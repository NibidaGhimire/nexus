Anti-Context-Free Languages
Carles Card´ o
Abstract. Context-free languages can be characterized in several ways.
This article studies projective linearisations of languages of simple de-
pendency trees, i.e., dependency trees in which a node can govern at
most one node with a given syntactic function. We prove that the projec-
tive linearisations of local languages of simple dependency trees coincide
with the context-free languages.
Simple dependency trees suggest alternative dual notions of local-
ity and projectivity, which permits defining a dual language for each
context-free language. We call this new class of languages anti-context-
free. These languages are related to some linguistic constructions ex-
hibiting the so-called cross-serial dependencies that were historically
important for the development of computational linguistics. We propose
that this duality could be a relevant linguistic phenomenon.
Mathematics Subject Classification. 68Q45, 03D05.
Keywords. anti-context-free languages, context-free languages, local tree
languages, projectivity, dependency grammar.
1. Introduction
1.1. Historical and linguistic motivation
From the beginning of the history of computational linguistics, there was a
suspicion that, although context-free grammars could describe a large part
of natural languages, they were insufficient [13]. After some discussions over
the next decades (see [32] for state of the art at that time), it was confirmed
that some natural languages present non-context-free constructions [36]. In
this respect, let us consider a pair of formal languages for a fixed alphabet
Σ:
Lsqua={a2
1···a2
n|a1,...,an∈Σ,n≥0}, (1.1)
Lcopy={xx|x∈Σ∗}, (1.2)
To appear at Journal of Automata, Languages and Combinatorics.arXiv:2401.07815v1  [cs.FL]  15 Jan 20242 C. Card´ o
which here we will call the language of squares and the copy language , respec-
tively. The first is a context-free, indeed regular, language. It is a mathemat-
ical abstraction of a chain of subordinate clauses in English. For example:
(1) . . . that JohnasawaPeterbhelpbMarycreadc.
The second is not context-free and represents a chain of subordinate clauses
in Dutch [7]. For example:
(2) . . . dat
. . . thatJana
JanPietb
PietMariec
Mariezaga
sawhelpenb
helplezenc.
read
‘. . . that J. saw P. help M. read. ’
This example is said to exhibit cross-serial dependencies . The same crossing
configuration is found in Swiss-German. The links are visible since Swiss-
German has case marking:
(3) Jan
Johns¨ ait
saiddas
thatmer
thed’chind
children ACCem Hans
Hans DATes
thehuus
houseACCl¨ ond
let
h¨ alfe
helpaastriiche.
paint.
In the sentences (1) and (2), the abstraction into the formal language follows
from the argument structure; that is, we mark with the same letter the sub-
ject and the related verb. However, the case of Swiss-German can be formally
reduced into the copy language by intersecting the whole Swiss-German lan-
guage with a regular language and reducing by a homomorphism of monoids
according to the case mark [25]. Since context-free languages are closed under
intersections with regular languages and homomorphisms [22], Swiss-German
is not context-free.
Another pair of formal languages that we want to consider are the mul-
tipleabclanguage and the respectively abclanguage :
Lmult={(abc)n|n≥0}, (1.3)
Lresp={anbncn|n≥0}. (1.4)
The first is a context-free, indeed regular language, but not the second. The
first corresponds to simple coordination, as in sentence (4)a, while the sec-
ond is a mathematical idealization of the respectively construction, as in sen-
tence (4)b:
(4) a. Jeanaseems Germanbbut he is Frenchc, Pietroaseems Russianb
but he is Italiancand Peteraseems Belgianb, but he is Englishc.
b. Jeana, Pietroaand Peteraseem Germanb, Russianband Belgianb,
respectively, but they are Frenchc, Italiancand Englishc.
Respectively constructions, which also exhibit cross-serial dependencies, had
already been considered as initial counterexamples against context-freeness
of English with sentences as:1
1See [5] for (5)a, and [24] for (5)b.Anti-Context-Free Languages 3
(5) a. John, Mary and David are a widower, a widow and a widower
respectively.
b. This land and these woods can be expected to rend itself and sell
themselves respectively.
However, some linguists considered such constructions too artificial, the ex-
amples in Swiss-German being more transparent.
In any case, given linguistic data, formal grammars should be able to
generate such cross-serial dependencies. The strategy was to give just a lit-
tle extra power to context-free grammars to incorporate the copy language,
and similar constructions, which was called mildly context-sensitivity [23].
A little zoo of formal systems was proposed during the following years: tree
adjoining grammars TAG [23]; linear indexed grammars LIG [16];q-linear
context-free rewriting systems q-LCFRS [39];q-multiple context-free gram-
marsq-MCFG [35]. Some of these formalisms are equivalent; for example,
linear indexed grammars are equivalent to tree adjoining grammars, and q-
LCFRS are equivalent to q-MCFG, for any q. Moreover, some formalisms
are subsumed by others; for example, linear context-free rewriting systems
are more expressive than linear indexed grammars. See [25] for a survey on
the issue. Some linguists assume that natural language’s weak capacity could
be very close to MCFG, although the debate continues. On the one hand,
Kanazawa and Salvati [26] invite to refine this claim, since the mix language ,
Lmix={x∈{a,b,c}∗||x|a=|x|b=|x|c}, is generable by a 2-MCFG, but
such language is thought not pertinent for human language [4, 3]. On the
other hand, we have to consider the possibility of non-semi-linear construc-
tions, such as in Yoruba or Old-Georgian, which overpasses the generative
capacity of MCFG, see [11].
Historic considerations aside, let us examine the four languages above
Lcopy,Lsqua,Lmult, andLresp, and redefine them with the following notation.
For a string x=x1···xn∈Σ∗, where Σ is any alphabet and x1,...,xn∈Σ,
we writex↑m=xm(that is, the usual power of strings) and x↓m=xm
1···xm
n.
Consider the following table:
Context-Free Non-Context-Free
Lsqua={x↓2|x∈Σ∗}Lcopy={x↑2|x∈Σ∗}
Lmult={(abc)↑n|n≥0}Lresp={(abc)↓n|n≥0}
There is a duality between context-free and non-context-free languages under
the operations↑and↓. One of the goals of this article is to formalize that
duality giving a theoretical framework. Beyond these simple examples, we
will see that there exists a dual language for each context-free language.
Thus, it is natural to consider a dual class of context-free languages. We
call this class anti-context-free . Such languages were introduced in [10] in
the wider framework of Algebraic Dependency Grammar , while the notion
of duality as a linguistic phenomenon was already suggested in [9]. However,
and importantly, here we recast definitions, theorems, and proofs without the
heavy algebraic apparatus given earlier in [10].4 C. Card´ o
1.2. The dependency approach
The phenomenon of duality has only been described in the framework of
dependency grammar . Dependency structures are an alternative form of syn-
tactic analysis introduced by the French grammarian L. Tesni` ere [38]. Unlike
constituent analysis, a dependency structure is rather a relational structure
over the words of a sentence. For example, in the sentence the boy caught
frogs, the word caught governs the word frogs as object ( Ob), that forms the
dependence:
caught frogsOb
Joining together a set of dependencies we have a dependency structure:
the boy caught frogsSb ObDt
In what follows we will use the abbreviations Sbsubject, Dtdeterminer, Ob
object, Adadjective, Mdmodifier, Cjconjunction, Cocoordination. Other
authors use different nomenclature for such syntactic functions, but this is
not relevant for the examples in this article.
Dependency structures can adopt several topologies, such as networks
for semantics or directed acyclic graphs for some syntactic representations.
However, the most fundamental is the tree-shape [30]. There is not a unique
style of parse trees. Several treebanks allow a node to govern two or more chil-
dren with the same syntactic function. This configuration arises in coordina-
tion constructions, a particularly controversial issue in dependency grammar.
A possible style of analysis for coordination is:
John caught frogs dragonflies and butterfliesSb ObObOb
Cj
Another variant is:
John caught frogs dragonflies and butterfliesSbCo
Co CoOb
In the Russian linguistic tradition [29], a unique coordinator function Cois
needed to chain the coordinants. For example:Anti-Context-Free Languages 5
John caught frogs dragonflies and butterfliesSb ObCoCo
Cj
These are only some variants. See [33] for a comparison among styles of
parsing from different treebanks.
In this article, we will use simple dependency trees , defined as a partial
mapping from syntactic addresses to the vocabulary, rather than graphs, see
Definition 2.1, Section 2. This format is algebraically compact but only allows
at most one child governed by a function. In particular, simple dependency
trees only model a part of dependency trees. For coordination constructions,
the reader with linguistic interests should adopt, at least for this article,
the last commented style. However, leaving aside the linguistic motivation of
the current and the last section, this article focuses on the formal aspects
of duality and formal language theory. In-depth linguistic discussions are
beyond the scope of this article.
1.3. Projective word-orderings
The above dependency trees show the tree and the phrase simultaneously.
However, we can separate the structures. A word-ordering is a relationship
between the dependency tree and the places in the linear order of the sen-
tence. An important type of word-ordering is the projective word-ordering .
Projective word-orderings can be represented graphically by lines from the
dependency tree to the linear order without intersecting (this needs addi-
tional constraints that we do not address here). See Figure 1(a). There exist
several equivalent characterizations of projectivity. Here we adapt the follow-
ing from [28]: A word-ordering is projective if it transforms (total) subtrees
of the dependency tree into substrings . See an example in Figure 1(b).
Statistics from treebanks show that most word-orderings are projective.
There are, of course, remarkable exceptions. The Dutch sentence (2) is an
example:
. . . dat Jan Piet Marie zag helpen lezenOb ObSb Sb Sb
A major issue in dependency grammar is how to formalize these non-projective
word-orderings.
1.4. Structure of the article
Once we have fixed notation and preliminary definitions in Section 2, we in-
troduce in Section 3 the class of local simple dependency tree languages , and,
in Section 4, the class of recursive projective linearisations . Next, we prove
that the class of context-free languages is equivalent to the local simple de-
pendency tree languages projectively linearised. There exists a classical result
in this direction for rewriting systems: It is proved that the set of derivation6 C. Card´ o
Figure 1. (a) Dependency tree and a projective word-
ordering. (b) An example of subtree transformed into a sub-
string.
trees of a context-free grammar is local [12]. In terms of the so-called depen-
dency systems, a similar result was already proved [19, 20, 15, 34]. However,
here we offer a version for tree languages and projective linearisations as
independent objects.
The definition of simple dependency trees invites us to consider dual
alternatives. So we can define for each tree language a dual tree language,
and the same occurs in the case of linearisations, Section 5. We will prove that
by taking the dual class of local languages and the dual class of projections,
we obtain context-free languages again. However, by taking the dual form of
only one of these classes, we obtain the new class so-called anti-context-free
languages . Section 6 defines, shows examples, and studies the main properties
of those languages. We have, for instance, that the respectively language and
the copy language are anti-context-free. Finally, we resume the linguistic
discussion in Section 7.
2. Notation and Definitions
Given a finite set A,A∗denotes the free monoid over A, the elements of
which are called strings . The empty string is denoted by ε. The length of
a stringxis denoted by|x|, and|x|astands for the number of occurrences
of a letterain the string x. When necessary, we will use the multiplicative
notation,x·y=xy. Given the string x=x1···xn∈A∗, withx1,...,xn∈
A, the reversed string is defined as xR=xn···x1. A string yis said to
be a substring ofy′if there are strings y,zsuch thatxyz =y′. Given a
permutation ρ:{1,...,m}−→{ 1,...,m}, and given mstringsx1,...,xm,
we will write ρ(x1,x2,...,xn) for the string xρ(1)xρ(2)···xρ(m). Since we willAnti-Context-Free Languages 7
use simultaneously two free monoids ζ∗and Σ∗for different uses, we reserve
εfor the empty string of Σ∗, and 1 for the empty string of ζ∗. The domain
of a partial mapping fis denoted by dom f.
Definition 2.1. Let Σ and ζbe finite sets, called vocabulary and syntactic
functions , respectively. A simple dependency tree is a partial mapping S:
ζ∗−→Σ with finite domain. The set of simple dependency trees over ζand
Σ is denoted by Treeζ,Σ. When the context permits, we will write Tree ,
for short. The depth of a simple dependency tree is defined as depth( S) =
max{|x||x∈domS}. Simple dependency trees with depth zero, are called
atomic , writtena•, wherea∈Σ, that is, dom S={1}anda•(1) =a. We
accept the empty set as a simple dependency tree with depth( ∅) = 0. In what
follows, we will say treeto refer to a simple dependency tree according to this
definition when no confusion can arise.
Remark 2.2. If we take the prefix closure of the domain of S∈Tree , thenS
can be interpreted as a tree. Every vertex can be identified by a unique Gorn
addressx∈ζ∗. Whenx∈domS, we label the vertex as S(x). Prefixes not
in the domain are viewed as unlabelled vertices. Definition 2.1 implies that
given a syntactic function α∈ζ, and given a vertex xthere exists at most
one childysuch thatxgovernsybyα. So, the class of simple dependency
trees only captures a certain kind of dependency tree.
Example 2.3. Consider the set of syntactic functions ζ={Sb,Ob,Dt,Ad,Md},
recall the nomenclature given in Section 1.2. Let the vocabulary be Σ = {the,
boy, caught, frogs }. The tree:
S(x) =

caught if x= 1,
John if x=Sb,
frog if x=Ob,
a if x=Ob·Dt,
big if x=Ob·Ad,
very if x=Ob·Ad·Md,
very if x=Ob·Ad·Md·Md.
can be represented as in Figure 1(a). Gorn addresses must be read in reverse
order to make linguistic sense. For example, “the determiner of the object”
is the path Ob·Dt.
Definition 2.4. Given a tree S, we introduce the operators [ S]p,S, S|φ, φ|S:
Treeζ,Σ−→Treeζ,Σ, wherep≥0 is an integer and φ∈ζ∗, defined as fol-
lows:
(i) [S]p(x) =/braceleftigg
S(x) if|x|≤p,
not defined if|x|>p.
(ii)S(x) =S(xR) for eachx∈domS.
(iii)/parenleftbig
S|φ/parenrightbig
(x) =S(φx) for eachφx∈domS.
(iv)/parenleftbig
φ|S/parenrightbig
(x) =S(xφ) for eachxφ∈domS.8 C. Card´ o
Proposition 2.5. LetSbe a tree,p≥0an integer and φ∈ζ∗:
(i)S=S;
(ii) [S]p=[S]p;
(iii) Ifdepth(S)≤1, thenS=S;
(iv) (S|φ)|ψ=S|(φψ),ψ|(φ|S) = (ψφ)|S;
(v)S|φ=φR|S,φ|S=S|φR.
Proof. (i) is trivial. For (ii) we use |xR|=|x|. (iii) If|x|≤1, thenx=xR,
wherebyS(x) =S(xR) =S(x). (iv) ((S|φ)|ψ)(x) = (S|φ)(ψx) =S(φψx) =
(S|(φψ))(x). The second equality in (iv) is similar. (v) S|φ(x) =S|φ(xR) =
S(φxR) =S(((φxR)R)R) =S((xφR)R) =S(xφR) = (φR|S)(x). The second
equality in (v) is similar. □
Example 2.6. Let us show some examples of the operators from Definition 2.4.
Let the sets be ζ={α,β}, and Σ ={a,a′,b,b′,c,c′}. Figure 2(a) depicts the
tree:
S(x) =

a ifx= 1,
a′ifx=α,
b ifx=β,
b′ifx=βα,
c ifx=ββ,
c′ifx=ββα.
Or, equivalently, Sis the set:
S={(1,a),(α,a′),(β,b),(βα,b′),(β2,c),(β2α,c′)}.
The operator [ S]ptakes the top of Suntil depth pand trims the rest of
the tree. See, for example, [ S]1in Figure 2(b). The operator S|φtakes the
subtree ofSwith the root at φ. For example, S|βis the subtree depicted in
Figure 2(c). The operator Sconsists in reversing the addresses:
S={(1,a),(α,a′),(β,b),(αβ,b′),(β2,c),(αβ2,c′)},
depicted in Figure 2(d). Finally, by the property Proposition 2.5(v), the op-
eratorα|Sis the subtree with root at αR=αof the treeS.
Sometimes the operator Syields a tree whose representation can con-
tains some gaps. Consider the following tree, with Σ = {a,b,c,d}andζ=
{α,β,γ}:
S′={(1,a),(α,b),(β,c),(βγ,d ),(β2,e)}.
Figure 2(a’) depicts S′, Figure 2(b’) depicts [ S′]1, Figure 2(c’) depicts S′|β,
Figure 2(d’) depicts S′, and Figure 2(e’) depicts γ|S′.
Definition 2.7. We say that a string x∈Σ∗is an ordering of the treeS∈Tree
if|x|a=|S−1(a)|for eacha∈Σ, where|S−1(a)|is the cardinality of the set
S−1(a).
Alinearisation is a mapping Π : Treeζ,Σ−→Σ∗such that Π( S) is an
ordering of S, for eachS∈Treeζ,Σ.Anti-Context-Free Languages 9
Figure 2. Example of operators from Example 2.6.
Definition 2.8. Astring language is a subset of Σ∗for some alphabet. A tree
language is a subset of Treeζ,Σ. By class here we mean simply “set” and we
reserve this denomination for class of string languages ,class of tree languages ,
andclass of linearisations .
Given a class of tree languages Xand a class of linearisations Y, we
define the class of string languages:2
X
Y={Π(W)|W∈X,Π∈Y}. (2.1)
3. Locality
Locality is the mathematical notion according to which an object is identified
by examining just a neighbourhood of its components. An string language L
is said to be p-locally testable [42, 41],p-local for short, if there is a set of
prefix strings U1, a set of internal strings U2and a set of suffix strings U3
such thatx∈Lif and only the prefix of xof lengthpis inU1, every substring
ofxof lengthpis inU2, and the suffix of xof lengthpis inU3. A language
islocal if it isp-local for some positive integer p. This notion is naturally
translated to trees as described by Knuutila [27]. We adapt the definition to
simple dependency trees.
Definition 3.1. LetSandS′be trees and let pbe an integer p≥0. We say
that:
(i)S′is the topp-subtree of SifS′= [S]p.
(ii)S′is ap-subtree of Sif [S|φ]p=S′for someφ∈domS.
(iii)S′is aterminalp-subtree of SifS|φ=S′for someφ∈domSsuch
that for all x∈ζ∗with|x|>pwe have that φx̸∈domS.
2More formally,X
Y={Π(W)|W∈X,Π∈Ysuch that Π : Treeζ,Σ−→ Σ∗,W⊆
Treeζ,Σ}.10 C. Card´ o
Figure 3. The tree language Wsqua.
Definition 3.2. Given an integer p > 0, we say that a tree language Wis
p-local if there exist three sets U1,U2,U3such thatS∈Wif and only if the
topp-subtree of Sis inU1, everyp-subtree of Sis inU2and every terminal
p-subtree of Sis inU3. We will write Loc(U1,U2,U3) =W. We say that a
tree language is local if it isp-local for some integer p. The class of local tree
languages is denoted by LC.
Remark 3.3. It follows from the definition that if a local tree language
Loc(U1,U2,U3) is not empty, then U1,U3⊆U2.
Example 3.4. Consider the set of trees Wsqua={∅,Qa,Qb,Qa,a,Qa,b,...}
⊆Treeζ,Σ, whereζ={α,β}and Σ ={a,b}, displayed in Figure 3. Wsqua=
Loc(U1,U2,U3), where:
•U1is the set of the trees of the form:
∅,
,
 ,
 ,
withx,y,z∈{a,b}.
•U2=U1.
•U3is the set of the trees of the form:
∅, x•,
,
 ,
withx,y,z∈{a,b}.
Example 3.5. Consider the set of trees Wmult ={∅,M1,M2,M3...} ⊆
Treeζ,Σ, whereζ={α,β,γ}and Σ ={a,b,c}, displayed in Figure 4.
Wmult=Loc(U1,U2,U3), where:
•U1is the set of the trees:
∅,
 ,
 .
•U2=U1.Anti-Context-Free Languages 11
Figure 4. The tree language Wmult.
•U3is the set of the trees:
∅,
 .
4. Projectivity
This section characterises the class of context-free languages as local trees
projectively arranged, Theorem 4.4. As described in Section 1.3, a word-
ordering of a given dependency tree is projective if each subtree is a substring
of the word-ordering. That is a broad notion, among other reasons, because
the same tree can exhibit different projective orders in the same natural
language. However, a definition that associates a single string with each tree
will suffice for the mentioned characterisation.
Definition 4.1. Letζ={α1,...,αm}be a set of syntactic functions and Σ a
vocabulary. Fix a permutation ρ. We say that a mapping Π : Tree−→Σ∗is
arecursive projective linearisation if for eachS∈Tree we have:
Π(∅) =ε, (4.1)
Π(S) =ρ(root(S),Π(S|α1),..., Π(S|αm)), (4.2)
where root( S) =S(1) if 1∈domS, otherwise root( S) =ε. The class of such
linearisations is denoted by PR.
Remark 4.2. Notice that the condition Π( ∅) =εstops the recursion. For an
atomic tree a•we have Π(a•) = Π(∅)···Π(∅)aΠ(∅)···Π(∅) =ε···εaε···ε=
a.
Notice, in addition, that Π takes the root of Sand the strings Π( S|α1),
...,Π(S|αm), and puts them in some order given by ρ. Since every S|αiis
a subtree, necessarily Π transforms subtrees of Sinto substrings of Π( S).
Therefore, Π( S) is a projective ordering of S(see Section 1.3).12 C. Card´ o
Example 4.3. We consider the tree set Wsquaand the recursive projective
linearisation:
Πsqua(S) = Π squa(S|α)·root(S)·Πsqua(S|β). (4.3)
Let us see an example of linearisation for a tree Qa,b,a∈Wsqua.
Πsqua(Qa,b,a) = Π squa(Qa,b,a|α)aΠsqua(Qa,b,a|β)
= Π squa(a•)aΠsqua(Qb,a)
=aa/parenleftbig
Πsqua(Qb,a|α)bΠsqua(Qb,a|β)/parenrightbig
=aa/parenleftbig
Πsqua(b•)bΠsqua(Qa)/parenrightbig
=aa(bb(Πsqua(a•)a))
=aa(bb(aa))
=a2b2a2.
By generalizing this case to other trees in Wsqua, we obtain the square lan-
guage Π squa(Wsqua) =Lsqua. For the case of the tree language Wmulttake
the linearisation:
Πmult(S) = Π mult(S|α)·Πmult(S|β)·root(S)·Πmult(S|γ). (4.4)
We have, for example, for the tree M3:
Πmult(M3) = Π mult(M3|α)Πmult(M3|β)cΠmult(M3|γ)
= Π mult(a•)Πmult(b•)cΠmult(M2)
=abcΠmult(M2)
=abc/parenleftbig
Πmult(M2|α)Πmult(M2|β)cΠmult(M2|γ)/parenrightbig
=abc/parenleftbig
Πmult(a•)Πmult(b•)cΠmult(M1)/parenrightbig
=abc/parenleftbig
abcΠmult(M1)/parenrightbig
=abc/parenleftbig
abc/parenleftbig
Πmult(M1|α)Πmult(M1|β)cΠmult(M1|γ)/parenrightbig/parenrightbig
=abc/parenleftbig
abc/parenleftbig
Πmult(a•)Πmult(b•)cΠmult(ε•)/parenrightbig/parenrightbig
=abc(abc(abcε))
= (abc)3.
Thus, Π mult(Wmult) =Lmult.
Theorem 4.4. LetCFstand for the class of context-free languages:
LC
PR=CF. (4.5)
Proof. (⊆) Consider a p-local tree language W=Locp(U1,U2,U3)⊆Treeζ,Σ
and a projective recursive linearisation Π. We are going to prove that Π( W) is
a context-free language by constructing a context-free grammar G= (Σ,S,V,R),
the language of which L(G) is Π(W), where Σ is the alphabet, Sis the start
symbol,Vis the set of variables, and Ris the set of rewriting rules.Anti-Context-Free Languages 13
Letζ={α1,...,αm}and letρbe the permutation associated to the
linearisation Π. We can assume that W̸=∅, otherwise we construct an empty
context-free grammar. Recall that if W=Loc(U1,U2,U3) is not empty, then
U1,U3⊆U2. We take the set of variables, V={XT|T∈U2}, and we define
the set of rulesRas the union of the following sets:
•A set of start rules given by:
S::=XT (4.6)
for eachT∈U1.
•A set of body rules:
XT::=ρ(root(T),XT1,...,XTm) (4.7)
for each set of trees T,T1,...,Tm∈U2such that [ Ti]p−1=T|αifor
eachi= 1,...,m .
•A set of terminal rules of the form:
XT::= Π(T) (4.8)
for eachT∈U3.
With all this, we have that Π( W) =L(G)∈CF.
(⊇) For each context-free grammar there exists a grammar in Greibach
normal form that generates the same language [21, 22, 8]. Consider the gram-
mar in Greibach normal form with rules:

A1::=a1B1,1···Br1,1
...
As::=asB1,s···Brs,s(4.9)
whereAj,Bi,j∈Vare variables and aj∈Σ are terminals symbols. We are
going to construct a local set of trees that imitates the rules. Two trees can be
chained if and only if the rules which represent invoke one to other. However,
some of the variables Bi,jcan be repeated in the same rule. For this reason
we introduce r1+···+rsnew variables Ci,j, all different, that is, Ci,j=Ci′,j′
if and only if i=i′,j=j′. Consider the system:

A1::=a1C1,1···Cr1,1
...
As::=asC1,s···Crs,s
C1,1::=B1,1
...
Crs,s::=Brs,s(4.10)
This grammar generates the same language, and the rules does not contain
repeated variables. Without loss of generality, we will assume a grammar
G= (Σ,S,V,R) in the above form (4.10), where V={Aj|j= 1,...,s}
∪{Bi,j|i= 1,...,rj,andj= 1,...,s}∪{Ci,j|i= 1,...,rj,andj=
1,...,s}, where all the variables Ci,jare different. Next, we construct the set14 C. Card´ o
of subtrees, initial trees, and terminal trees, in order to define a local tree
language.
•(Subtrees) Let U2be stand for the set of following trees. For each subset
ofRformed by 1 + n+/summationtextn
j=1kjrules of the form:


X::=aX1···Xn
Xj::=bjY1,j···Ykj,jj= 1,...,n
Yi,j::=ci,jZi,ji= 1,...,kj,j= 1,...,n(4.11)
whereX,Xi,Y1,i···Yni,i∈Vanda,bi,ci,j∈Σ∪{ε}, for each subscript
and whereZi,j∈V∗, we construct a tree of depth 2 with Σ as vocabulary
andζ=Vas syntactic functions as follows:
(4.12)
For the superfluous rules of the form Ci,j=Bi,j, some letters a,bi,ci,j
can be the empty string. In this case, we do not define the tree at that
Gorn adress.
•(Initial trees) We do the same for the start rules. U1stands for all the
following trees.
–For each set of rules of the form:

S::=aX1···Xn
Xj::=bjY1,j···Ykj,jj= 1,...,n
Yi,j::=ci,jZi,ji= 1,...,kj,j= 1,...,n(4.13)
we construct trees as in the case (4.12).
–For each set of rules of the form:/braceleftigg
S::=aX1···Xn
Xj::=bjj= 1,...,n(4.14)
whereX,Xi,∈Vanda,bi∈Σ, for each subscript, we construct a
tree of depth 1 as follows:
(4.15)
–For each rule of the form:
S::=a (4.16)
we construct an atomic tree a•.Anti-Context-Free Languages 15
•(Terminal trees) Finally we construct trees for the terminal trees. U3
stands for all the following trees.
–For each set of rules of the form:

X::=aX1···Xn
Xj::=bjY1,j···Ykj,jj= 1,...,n
Yi,j::=ci,j i= 1,...,kj,j= 1,...,n(4.17)
we construct a tree of the same form in the case (4.12).
–For each subset of rules of the form:/braceleftigg
X::=aX1···Xn
Xj::=bj j= 1,...,n(4.18)
we construct trees as in the case (4.15).
–For each rule of the form:
X::=a (4.19)
whereX∈Vanda∈Σ, we construct an atomic tree a•.
Next, define the recursive projective linearisation:
Π(S) = root(S)·s/productdisplay
j=1rj/productdisplay
i=1Π(S|Ci,j)·s/productdisplay
j=1ri/productdisplay
i=1Π(S|Bi,j) (4.20)
which summarizes all the possibles orderings of each rewriting rule. In par-
ticular, notice that when the subtrees S|C1,j,...,S|Crj,jare not empty for
somej, all the other subtrees are empty. This yields:
Π(S) = root(S)·ε···ε·rj/productdisplay
i=1Π(S|Ci,j)·ε···ε= root(S)rj/productdisplay
i=1Π(S|Ci,j),
(4.21)
which reproduces the order of the rule Aj::=ajC1,j···Crj,j. When the
subtreeS|Bi,jis not empty for some iandj, root(S) =εand the rest of
subtrees are empty. This yields:
Π(S) =ε···ε·Π(S|Bi,j)·ε···ε= Π(S|Bi,j), (4.22)
which reproduces the order of the superfluous rule Ci,j::=Bi,j. With all
this, we have that Π( Loc(U1,U2,U3)) =L(G), which proves that CF⊆
LC/PR. □
Remark 4.5. Notice that in the proof of Theorem 4.4 it is only required a
depth two of locality.
5. Anti-classes
We have used the operator S|φto extract subtrees of Swith the root at φ.
Definitions of tree local languages and recursive projections use the subtree
notion. Taking the alternative operator φ|S, we obtain dual definitions of
locality and projectivity.16 C. Card´ o
Figure 5. The dual tree language Wsquafrom the Example 5.3.
Definition 5.1. LetSandS′be trees and let pbe an integer p≥0.
(i) We say that S′is ap-anti-subtree of Sif [φ|S]p=S′for someφ∈domS.
(ii) We say that S′is aterminalp-anti-subtree of Sifφ|S=S′for some
φ∈domSsuch that for all x∈ζ∗with|x|> p we have that xφ̸∈
domS.
Definition 5.2. Given an integer p > 0, we say that a tree language Wis
p-anti-local if there are three sets U1,U2,U3such thatS∈Wif and only if
the topp-subtree of Sis inU1, everyp-anti-subtree of Sis inU2, and every
terminalp-anti-subtree of Sis inU3. We write Loc(U1,U2,U3) =W. A tree
language is anti-local if it isp-anti-local for some integer p>0. The class of
anti-local tree languages is denoted by −LC.
Example 5.3. Consider the following anti-local tree language Loc(V1,V2,V3) =
Wsqua, depicted in Figure 5, where:
•V1is the set of the trees of the form:
∅,
,
 ,
 ,
withx,y,z∈{a,b}.
•V2=V1.
•V3is the set of the trees of the form:
∅, x•,
,
 ,
withx,y,z∈{a,b}.
Definition 5.4. We say that a mapping Π :Tree−→Σ∗is arecursive anti-
projective linearisation if for eachS∈Tree we have
Π(∅) =ε, (5.1)
Π(S) =ρ(root(S),Π(α1|S),..., Π(αm|S)). (5.2)
for some fixed permutation ρ. The class of such linearisations is denoted by
−PR.Anti-Context-Free Languages 17
Lemma 5.5. We have the following commutations:
(i)Loc(U1,U2,U3) =Loc(U1,U2,U3);
(ii)Π(S) = Π(S).
Proof. (i) First, we see the following equivalences from Proposition 2.5.
•S′is the topp-subtree of SiffS′is the topp-subtree of S. We just use
the equivalence S′= [S]piffS′=[S]p= [S]p.
•S′is ap-subtree of SiffS′is ap-anti-subtree of S. We just use the
equivalence S′= [S|φ]piffS′=[S|φ]p= [S|φ]p= [φR|S] and the
equivalence φ∈domSiffφR∈domS.
•S′is a terminal p-subtree ofSiffS′is a terminal p-anti-subtree of S. We
use the equivalence S′=S|φiffS′=S|φ=φR|Sand the equivalences
|x|>piff|xR|>pandφx̸∈domSiffxRφR̸∈domS.
Now we notice that:
S∈Loc(U1,U2,U3)⇐⇒S∈Loc(U1,U2,U3).
The right-hand expression is equivalent to say that:
•ifS′is the topp-subtree of SthenS′∈U1, or equivalently, if S′is the
topp-subtree of S, thenS′∈U1.
•ifS′is ap-subtree of S, thenS′∈U2, or equivalently, if S′is ap-anti-
subtree ofSthenS′∈U2.
•ifS′is a terminal p-subtree of S, thenS′∈U2, or equivalently, if S′is
a terminalp-anti-subtree of S, thenS′∈U3.
Therefore:
S∈Loc(U1,U2,U3)⇐⇒S∈Loc(U1,U2,U3)⇐⇒S∈Loc(U1,U2,U3).
(ii) By induction on the depth of the tree. If depth( S)≤1, thenS=S,
and trivially Π(S) = Π(S) = Π(S). Assume that the statement is true for any
tree with depth less than n, and take a tree with depth( S) =n. On the one
hand,S(1) =S(1R) =S(1). Thus, root( S) = root(S). On the other hand,
we notice that for any αi∈ζwe have depth( α|S)<n. Then, by hypothesis
of induction: Π(αi|S) = Π(αi|S) = Π(S|αR
i) = Π(S|αi), sinceαR
i=αi∈ζ.
Thus:
Π(S) =ρ/parenleftbig
root(S),Π(α1|S),..., Π(αm|S)/parenrightbig
=ρ/parenleftbig
root(S),Π(S|α1),..., Π(S|αm)/parenrightbig
= Π(S).
□
Theorem 5.6. The following equalities hold:
LC
PR=−LC
−PR, (5.3)
−LC
PR=LC
−PR. (5.4)18 C. Card´ o
Proof.L∈LC
PRif and only if L= Π( Loc(U1,U2,U3)) for some tree sets
U1,U2,U3. By Lemma 5.5, L= Π(Loc(U1,U2,U3)) = Π(Loc(U1,U2,U3))
=Π(Loc(U1,U2,U3)).Thus,L∈LC
PRif and only if L∈−LC
−PR. This proves
the first equality. For the second equality, we suppose L∈−LC
PR. And then,
L= Π(Loc(U1,U2,U3)) = Π( Loc(U1,U2,U3)) = Π(Loc(U1,U2,U3)). Thus,
L∈−LC
PRif and only if L∈LC
−PR. □
Example 5.7. We can see that Π squa(Wsqua) =Lcopy. We write Tx1,...,x n,
withx1,...,xn∈Σ for the trees of the form:
x1β−→x2β−→···β−→xn
Then, we have the following ordering for the tree Qa,b,a:
Πsqua(Qa,b,a) = Π squa(Qa,b,a|α)aΠsqua(Qa,b,a|β)
= Π squa(Ta,b,a)aΠsqua(Tb,a)
=/parenleftbig
Πsqua(Ta,b,a|α)aΠsqua(Ta,b,a|β)/parenrightbig
a/parenleftbig
Πsqua(Tb,a|α)bΠsqua(Tb,a|β)/parenrightbig
=/parenleftbig
Πsqua(∅)aΠsqua(Tb,a)/parenrightbig
a/parenleftbig
Πsqua(∅)bΠsqua(a•)/parenrightbig
=/parenleftbig
εa/parenleftbig
Πsqua(Tb,a|α)bΠsqua(Tb,a|β)/parenrightbig/parenrightbig
a/parenleftbig
εba/parenrightbig
=/parenleftbig
a/parenleftbig
Πsqua(∅)bΠsqua(a•)/parenrightbig/parenrightbig
a/parenleftbig
ba/parenrightbig
=/parenleftbig
a/parenleftbig
εba/parenrightbig/parenrightbig
a/parenleftbig
ba/parenrightbig
= (a(ba))a(ba)
= (aba)2.
Similarly, one can see that Π mult(Wmult) =Lresp.
6. Anti-Context-Free Languages
Theorem 5.6 suggests a new class of languages:
Definition 6.1. We call
−CF=−LC
PR=LC
−PR(6.1)
the class of anti-context-free languages . Given two languages L∈CFand
L′∈−CF, we say that they form a dual pair , writtenL⊥L′, ifL= Π(W)
andL′=Π(W) = Π(W) for some local tree language Wand some projective
linearisation Π. A language Lis self-dual if L⊥L.
Remark 6.2. The relation⊥is symmetric but not in general reflexive nor
transitive.
Example 6.3. We know that Wsqua∈LCand Π squa∈PR. Since Π squa(Wsqua) =
Lcopy, we have that Lcopy∈−CF. Similarly, we have that Lresp∈−CF. Thus,
we have the dual pairs Lsqua⊥LcopyandLmult⊥Lresp.Anti-Context-Free Languages 19
Example 6.4. A language does not have a unique dual language. This is
because for a language L∈CFit is possible to find different W,W′,Π,Π′
such that Π( W) = Π′(W′) =L. Let us examine the following case. First
we consider another classical language called the Dyck language ,LDyck⊆
(Σ∪/tildewideΣ)∗, where Σ ={(,[,{,[ [}is the alphabet of left parenthesis and /tildewideΣ =
{),],},] ]}the alphabet of right parenthesis. LDyck consists in the strings
ofwell balanced parentheses [6]. This language is representative of context-
freeness in the sense stated in the Chomsky-Sch¨ utzenberger Theorem [14, 6,
2].
Consider a pair of languages very similar to LsquaandLcopy. Define the
operator/tildewider(·) : Σ−→/tildewideΣ by˜( = ),˜[ = ],˜{=}and˜[ [ = ] ]. With this, we define:
]Lsqua={a1˜a1···an˜an|a1,...,an∈Σ,n≥0} (6.2)
]Lcopy={a1···an˜a1···˜an|a1,...,an∈Σ,n≥0} (6.3)
These languages are, respectively, context-free (indeed regular) and non-
context-free.
For the Dyck language, we can construct a local tree language for which
the following Figure 6(a) shows a tree S. The function αopens a parenthesis,
γcloses the parenthesis, βputs material inside the parenthesis, while δputs
material at the right. We take the linearisation:
Π(S) = root(S)·Π(S|α)·Π(S|β)·Π(S|γ)·Π(S|δ) (6.4)
For example, for the tree of Figure 6(a) we have the string: Π( S) = [ ( ) ]{[ [ ] ]}.
Figure 6(b) depicts the reversed tree Swhich, when it is linearized, yields the
string Π(S) = [ ( [ [{] ) ] ]}∈]Lcopy. And in general we have that LDyck⊥]Lcopy.
Now we consider the following Figures. Figure 6(c) depicts a tree for a
tree language defined locally with which the linearisation:
Π(S) = Π(S|α)·root(S)·Π(S|β) (6.5)
yields the language ]Lsqua. In particular the tree from the figure yields the
string [ ] ( ) [ [ ] ]{}. Figure 6(d) shows the reversed tree with which the same
linearisation yields the string [ ( [ [ {] ) ] ]}∈]Lcopy. In sum, ]Lcopy⊥LDyck and
]Lcopy⊥]Lsqua, whereby a language can have more than one dual language.
Let us see some basic properties of anti-context-free languages. We write
UNfor the class of unary languages, that is, languages defined over an al-
phabet with only one letter; FN, for the class of finite languages; RGfor the
class of regular languages; and LSthe class of local string languages.
Definition 6.5. Given a subclass X⊆CF∪−CFwe define the ‘ anti’ operator:
−X={L′∈CF∪−CF|L′⊥L,L∈X}. (6.6)
We say that a class X⊆CF∪−CFis involutive if and only −(−X) =X.
It is easily seen that X⊆−(−X), since⊥is a symmetric relation, but
in general, the ‘anti’ operator is not involutive. CFand−CFare trivially20 C. Card´ o
Figure 6. Trees from Example 6.4.
involutive classes. For some classes, we have that −X=X, and then the class
is trivially involutive.
Proposition 6.6. We have the following properties for the ‘anti’ operator:
(i)−CF̸=CF;
(ii)−(CF∩UN) =−(RG∩UN) =RG∩UN=CF∩UN;
(iii)−FN=FN;
(iv)RGis not involutive.
(v)L={anbn|n≥0}is self-dual. In particular, L∈CF∩−CF.
(vi) Every local string language is self-dual. In particular, LS⊆−LS.
(vii)Lmult is self-dual.
(viii) LSis not involutive.
(ix)CF∩−CFis not involutive.
Proof. (i) By Example 6.3, Lsqua⊥Lcopy. SinceLcopy̸∈CF, the classes
must be different.
(ii) It is easily seen that RG∩UN=CF∩UN. Consider a unary language
L= Π(W) inCF. SinceLis unary, for any linearisation Π, if S∈W,
then Π(S) =Π(S). And, then L= Π(W) =Π(W) = Π(W). Hence, for
any unary language, L∈CFif and only if L∈−CF.
(iii) Π(W) is a finite language if and only if Wis finite if and only if Π(W)
is finite.
(iv) recall from the Example 6.4 that ]Lsqua⊥]Lcopy and]Lcopy⊥LDyck.
]Lsqua is regular, LDyck is context free but not regular. Thus, RG⊊
−(−RG).Anti-Context-Free Languages 21
(v) LetWbe the local tree language:
By taking the linearisation Π( S) = Π(S|α)·Π(S|β)·root(S) we have
thatL={anbn|n≥0}= Π(W) =Π(W), and therefore, L⊥L, and
L∈CF∩−CF.
(vi) A string x=x1x2···xn, withx1,x2,...,xn∈Σ can be encoded as a
tree with only one syntactic function α:
Tx=x1α−→x2α−→···α−→xn
A local string languages Lis defined by a set of prefixes, a set of strings
and a set of sufixes. Encoding these strings we obtain a set of top trees,
a set of subtrees, and a set of terminal subtrees that give us a local tree
languageWL. When we linearize the trees by Π( S) = root(S)·Π(S|α),
we obtain the local string language in question, Π( WL) =L. Now we
only need to notice that Tx=Tx. This means that L= Π(WL) =
Π(WL). Therefore, L⊥L.
(vii) We take the set of prefix strings {a,ab,abc}, the set of internal strings
{abc}and the set of suffix strings {abc,bc,c}. The local string language
defined by these sets is Lmult. By (vi) the language is self-dual.
(viii) By (vii) and (vi), Lmultis self-dual. However, we saw that Lmult⊥Lresp.
ButLrespis not context-free, neither a local string language.
(ix) The same reasoning of (viii).
□
We set Σ ={a1,...,ak}. The Parikh mapping is the mapping p: Σ∗−→
Nkdefined by p(x) = (|x|a1,...,|x|ak). A set in Nkissemi-linear if it is a
finite union of sets of the form {p1A1+···+pnAn+B|p1,...,pn∈N}for
somen≥0 andA1,...,An,B∈Nk. A string language L⊆Σ∗is semi-linear
ifp(L) is a semi-linear set [25]. The Parikh theorem states that context-free
languages are semi-linear [31].
Proposition 6.7. Languages in−CFare semilinear.
Proof. First we notice that the notion of semi-linear string language can be
directly translated to tree languages. We define the Parikh mapping for trees
P:Treeζ,Σ−→NkasP(S) = (|S−1(a1)|,...,|S−1(ak)|). A tree language
W⊆Treeζ,Σis said to be semi-linear if P(W) is a semi-linear set.
LetWbe a tree language and let Φ : Treeζ,Σ−→Σ∗be a linearisation.
We have that Φ( W) is a semi-linear string language if and only if Wis a
semi-linear tree language. This is because P(S) =p(Φ(S)).
Finally, we consider L′∈−CF. There is a L∈CFsuch thatL⊥L′.
That is, there is a projective linearisation Π and a local tree language Wsuch22 C. Card´ o
that Π(W) =LandΠ(W) =L′. By the above comment, L′is semi-linear
if and only if Wis semi-linear if and only if Lis semi-linear. By Parikh’s
theorem,Lis semi-linear, so L′is semi-linear. □
7. Discussion
Since context-free languages seem to form a fundamental class for under-
standing the human language, although insufficient in terms of weak capac-
ity, the early strategy slightly increased the descriptive power of the context-
free grammars. Although this ancient recipe may be formally appropriate,
it ignores the phenomenon of duality, explained here, which has, we believe,
linguistic relevance.
As an illustration, let us resume the first sentences (1) . . . that John saw
Peter help Mary read ; and the Dutch version (2) . . . dat Jan Piet Marie zag
helpen lezen , which exhibits cross-serial dependencies. We assume that both
sentences must have the same tree shape. Therefore the linearisations must
be different for each language. Indeed, consider the two linearisations:
ΠEng(S) = Π Eng(S|Sb)·root(S)·ΠEng(S|Ob), (7.1)
ΠDut(S) = Π Dut(Sb|S)·root(S)·ΠDut(Ob|S). (7.2)
Figure 7(a) and Figure 7(b) show the derivations. The first is a projective
linearisation, while the second is anti-projective. We only need to swap the
tree operators to obtain one from the other. We have, indeed, the relations:
ΠDut=ΠEng, or equivalently, Π Eng=ΠDut. All this indicates that word-
ordering in both languages should be considered at the same level of com-
plexity. In the same way, recall sentences (4)a, (4)b, (5)a, and (5)b, we can
construct a linearisation for coordination in the standard order, say Π sta,
which is projective, and other linearisation, Π resp, anti-projective, for respec-
tively construction. The same relation of duality holds Π sta=Πresp.3
The class of anti-context-free languages opens some questions. It can
be proved that n-copy languages, Ln-copy ={xn|x∈Σ∗}, are anti-context-
free, and it is known that TAG only can generate the copy language L2 -copy .
We do not know whether some well-known formalisms can generate the class
−CF. We notice that dual languages are equal up to reordering the letters
of each string. That makes us suspect that the mix language could be not in
−CF.
It is worth considering a straightforward extension of the class CF∪−CF
with more linguistic interest. Let us denote ±PRthe class of linearisations
that can combine subtrees and anti-subtrees. Then, CF∪−CF⊆LC/±PR.
However, this class could still fall short of natural languages.4
3[9] considered the possibility of changing the syntactic structure but not the linearisation,
that is, taking locally defined dependency trees for English and anti-locally for Dutch,
and a projective linearisation for both. According to Theorem 5.6, there is an algebraic
equivalence.
4There are constructions whose dependency trees are not local. Consider the English sen-
tences: She is an actress ,She wants to be an actress ,She wants to try to be an actress ,Anti-Context-Free Languages 23
Figure 7. (a) Linearization for subordination in English.
(b) Linearization for subordination in Dutch.
Another question is whether ‘anti’ operators (anti-subtree, anti-class,
. . . ) can be generalized to more general dependency trees, i.e., trees where
a node can govern two or more children with the same syntactic function.
Further work should tackle this methodological aspect jointly with the issue
of coordination.
A final idea, a little more philosophical, is suggested by the very notion
of duality. The fact that two objects are dual entails that neither is privi-
leged over the other. From an algebraic perspective, this is the case for the
classes CFand−CF. Why has human language favoured one of the forms?
To discover the causes of this symmetry breaking (frequent in other physical
and biological systems), we should evaluate other linguistic aspects beyond
the algebraic point of view, such as the efficiency of parsing or evolutionary
syntax. In this respect, some literature [37, 18, 17] considers, as a universal
principle, that natural languages have evolved to minimise the length of de-
pendencies in a sentence. If two vertices form a dependency, its dependency
length is the distance of the vertices in the linear order, and the total length is
the sum of all dependency lengths. The sentence (1) has total length 7, while
the sentence (2), 11. We can formalise a bit more that question in our own
terms. Fix a tree language Wand a linearisation Π. Given x= Π(S), letℓ(x)
denote the total dependency length of the string xwith dependencies given
byS∈W. It is easily seen that ℓgrows linearly for Lsqua= Π squa(Wsqua):
ℓ(x) =3|x|
2−2,
and so on. Since the subject sheand the predicate actress must agree on gender, and one
can put more and more material between them, the agreement is not local in the tree.24 C. Card´ o
(a)aabbcc12
12
1
(b)abcabc1 13 3 3
Figure 8. (a) A string in Lsqua= Π squa(Wsqua) with total
dependency length 7 (b) A string in Lcopy=Πsqua(Wsqua)
with total dependency length 11.
and that it grows quadratically for Lcopy=Πsqua(Wsqua):
ℓ(x) =|x|2
4+|x|
2−1.
See Figures 8 (a) and (b). A direction of further research is to study the
dependency length growth concerning the classes of tree languages and lin-
earisations viewed in this article.
References
1. Aho, A. V.: Indexed grammars, an extension of context-free grammars, Journal
of the ACM (JACM), 15, 4, 647–671, 1968.
2. Autebert, J.M., Berstel, J. and Boasson, L.: Context-free languages and push-
down automata, Handbook of formal languages: Word, Language, Grammar, 1,
111–174, Springer, 1997.
3. Bach, E.: Discontinuous constituents in generalized categorial grammar, Pro-
ceedings of the 11th Annual Meeting of the North Eastern Linguistics Society,
1–12, 1981.
4. Bach, E.: Categorial grammars as theories of language, Studies in Linguistics
and Philosophy, 32, 17–34, 1988.
5. Bar-Hillel, Y. and Shamir, E.: Finite-state languages: formal representations
and adequacy problems, Bull. of Res. Council of Israel, 8F, 155–166, 1960.
6. Berstel, J.: Transductions and context-free languages, Leitf¨ aden der ange-
wandten Mathematik und Mechanik, Teubner Studienb¨ ucher: Informatik, 38,
Teubner, 2013.
7. Savitch, W. J., Bach, E., Marsh, W. and Safran-Naveh, G.: Cross-serial depen-
dencies in Dutch, The formal complexity of natural language, 286–319, 1987.
8. Blum, N. and Koch, R.: Greibach normal form transformation revisited, Infor-
mation and Computation, 150, 1, 112–118, 1999.
9. Card´ o, C.: Algebraic Governance and Symmetry in Dependency Grammars,
20th and 21st International Conferences on Formal Grammar, (FG’15), revised
Selected Papers, Barcelona, Catalunya, 60–76, 2016.Anti-Context-Free Languages 25
10. Card´ o, C.: Algebraic dependency grammar, Universitat Polit` ecnica de
Catalunya 2018
11. Clark, A. and Yoshinaka, R.: Beyond Semilinearity: Distributional Learning of
Parallel Multiple Context-free Grammars, Proceedings of the 11th International
Conference on Grammatical Inference (ICGI’12), Washington, D.C., USA, 84–
96, 2012.
12. Comon, H., Dauchet, M., Gilleron, R., L¨ oding, C., Jacquemard, F., Lugiez,
D., Tison, D., and Tommasi, M.: Tree automata techniques and applications,
year=2007, http://tata.gforge.inria.fr/ [Accessed: 05/6/2020].
13. Chomsky, N.: Syntactic structures , Janua linguarum, Series minor, 4, Mouton
& Co., 1957.
14. Chomsky, N. and Sch¨ utzenberger, M. P.: The algebraic theory of context-free
languages, Studies in Logic and the Foundations of Mathematics, 35, 118–161,
1963.
15. Gaifman, H.: Dependency systems and phrase-structure systems, Information
and control, 8, 3, 304–337, 1965.
16. Gazdar, G.: Applicability of indexed grammars to natural languages. Natural
language parsing and linguistic theories , Studies in Linguistics and Philosophy,
35, 69–94, Springer, 1988.
17. G´ omez-Rodr´ ıguez, C. and Ferrer-i-Cancho, R.: Scarcity of crossing dependen-
cies: A direct outcome of a specific constraint?, Physical Review E, 96, 6, 2017.
18. G´ omez-Rodr´ ıguez, C. , Morten H. and Ferrer-i-Cancho, R.: Memory limitations
are hidden in grammar, arXiv preprint arXiv:1908.06629, 2019.
19. Gross, M.: On the equivalence of models of language used in the fields of
mechanical translation and information retrieval, Information storage and re-
trieval, 2, 1, 43–57, 1964.
20. Hays, D. G.: Dependency theory: A formalism and some observations, Lan-
guage, 511–525, 1964.
21. Hopcroft, J. and Ullman, J. D.: Introduction to automata theory, languages,
and computation, Addison Wesley,1979.
22. Hopcroft, J. E., Motwani R., and Ullman, J. D.: Introduction to automata
theory, languages, and computation, Addison Wesley, Pearson education, 2nd,
2001.
23. Joshi, A. K.: Tree adjoining grammars: How much context-sensitivity is required
to provide reasonable structural descriptions?, Natural Language Parsing, 206–
250, 1985.
24. Kac, M. B., Manaster-Ramer, A., and Rounds, W. C.: Simultaneous-distributive
coordination and context-freeness, Computational Linguistics, 13, 1-2, 25–30,
1987.
25. Kallmeyer, L.: Parsing beyond context-free grammars , publisher=Springer,
Berlin, Heilderberg,2010
26. Kanazawa, M. and Salvati, S.: MIX is not a tree-adjoining language, Proceed-
ings of the 50th Annual Meeting of the Association for Computational Linguis-
tics (ACL’12), Jeju, Korea. 1,666–674, 2012.
27. Knuutila, T.: Inference of k-testable tree languages, Advances in Structural
and Syntactic Pattern Recognition: proceedings of the international workshop26 C. Card´ o
(SSPR’92), Bern, Switzerland, Machine perception artificial intelligence, 5, 109–
120, 1993.
28. Kuhlmann, M.: Dependency Structures and Lexicalized Grammars: An Alge-
braic Approach, 6270, Published PhD thesis, Springer, 2010.
29. Melˇ cuk, I. A.: Dependency Syntax: Theory and practice, sunny series in lin-
guistics, State University of New York Press, 1998.
30. Nivre, J.: Dependency grammar and dependency parsing, V¨ axj¨ o University:
School of Mathematics and Systems Engineering, MSI 05133, 5133, 1–32, 2005.
31. Parikh, R. J.: On context-free languages, Journal of the ACM (JACM), 13, 4,
570–581, 1966.
32. Pullum, G. K. and Gazdar, G.: Natural languages and context-free languages.
Linguistics and Philosophy, 4, 471–504, 1982.
33. Popel, M., Marecek, D., Step´ anek, J., Zeman, D. and Zabokrtsk` y, Z.: Coor-
dination Structures in Dependency Treebanks, Proceedings of the 13rd confer-
ences of the association for computational linguistics (ACL’13), Sofia, Bulgaria,
pages=517–527, year=2013.
34. Robinson, J. J.: Dependency structures and transformational rules, Language,
pages=259–285, 46, 1970.
35. Seki, H., Matsumura, T., Fujii, M. and Kasami, T.: On multiple context-free
grammars, Theoretical Computer Science, 88, 2, 191–229, 1991.
36. Shieber, S. M.: Evidence against the context-freeness of natural language. Lin-
guistics and philosophy, 8, 333–343, 1985.
37. Temperley, D. and Gildea, D.: Minimizing syntactic dependency lengths: Ty-
pological/cognitive universal, Annual Review of Linguistics, 4, 1, 67–80, 2018.
38. Tesni` ere, L.: El´ ements de syntaxe structurale, Klincksieck, Paris, 1959.
39. Vijay-Shanker, K., Weir, D. J. and Joshi, A. K.: Characterizing structural de-
scriptions produced by various grammatical formalisms, Proceedings of the 25th
annual meeting on Association for Computational Linguistics, 104–111, 1987.
40. Weir, D. and Joshi, A.: Combinatory categorial grammars: Generative power
and relationship to linear context-free rewriting systems, 26th Annual Meeting
of the Association for Computational Linguistics, 278–285, 1988.
41. Yokomori, T.: On polynomial-time learnability in the limit of strictly determin-
istic automata, Machine Learning, 19, 2, 153–179, 1995.
42. Zalcstein, Y.: Locally testable languages, Journal of Computer and System
Sciences, 6, 2, 151–167, 1972.
Carles Card´ o
Departament de Ci` encies de la Computaci´ o,
Campus Nord, Edifici Omega, Jordi Girona Salgado 1-3. 08034
Universitat Polit` ecnica de Catalunya
Barcelona,
Catalonia (Spain)
URL: http://www.cs.upc.edu
e-mail: cardocarles@gmail.com