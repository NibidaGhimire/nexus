Rehabilitation Exercise Repetition Segmentation
and Counting using Skeletal Body Joints
Ali Abedi1, Paritosh Bisht2, Riddhi Chatterjee2, Rachit Agrawal2, Vyom Sharma2,
Dinesh Babu Jayagopi2, Shehroz S. Khan1,3,
1KITE Research Institute‚Äì Toronto Rehabilitation Institute, University Health Network, Canada
2Multimodal Perception Lab, International Institute of Information Technology Bangalor, India
3Institute of Biomedical Engineering, University of Toronto, Canada
fali.abedi, shehroz.khan g@uhn.ca, fparitosh.bisht, riddhi.chatterjee, rachit.agrawal, vyom.sharma, jdinesh g@iiitb.ac.in
Abstract ‚ÄîPhysical exercise is an essential component of re-
habilitation programs that improve quality of life and reduce
mortality and re-hospitalization rates. In AI-driven virtual re-
habilitation programs, patients complete their exercises indepen-
dently at home, while AI algorithms analyze the exercise data
to provide feedback to patients and report their progress to
clinicians. To analyze exercise data, the Ô¨Årst step is to segment it
into consecutive repetitions. There has been a signiÔ¨Åcant amount
of research performed on segmenting and counting the repetitive
activities of healthy individuals using raw video data, which raises
concerns regarding privacy and is computationally intensive. Pre-
vious research on patients‚Äô rehabilitation exercise segmentation
relied on data collected by multiple wearable sensors, which are
difÔ¨Åcult to use at home by rehabilitation patients. Compared to
healthy individuals, segmenting and counting exercise repetitions
in patients is more challenging because of the irregular repetition
duration and the variation between repetitions. This paper
presents a novel approach for segmenting and counting the
repetitions of rehabilitation exercises performed by patients,
based on their skeletal body joints. Skeletal body joints can be
acquired through depth cameras or computer vision techniques
applied to RGB videos of patients. Various sequential neural
networks, including many-to-many models (with binary sequence
output and density map output) and many-to-one models (with a
single output), are designed to analyze the sequences of skeletal
body joints and perform repetition segmentation and counting.
Extensive experiments on three publicly available rehabilitation
exercise datasets, KIMORE, UI-PRMD, and IntelliRehabDS,
demonstrate the superiority of the proposed method compared
to previous methods. The proposed method enables accurate ex-
ercise analysis while preserving privacy, facilitating the effective
delivery of virtual rehabilitation programs.
Index Terms ‚Äîexercise segmentation, exercise repetition count-
ing, skeletal body joints, LSTM, transformer, convolutional
neural network, virtual rehabilitation
I. I NTRODUCTION
Referral of patients to rehabilitation programs following
a stroke, cardiac event, or injury is a common practice
aimed at improving patients‚Äô quality of life and reducing re-
hospitalization and death rates [1]. Central to these programs
are regular and repetitive exercises that enable patients to re-
gain mobility and strength [1]. Recently, ArtiÔ¨Åcial Intelligence
(AI)-driven virtual rehabilitation has emerged as a promising
approach to delivering rehabilitation programs remotely to
patients in their homes [2]. This approach involves the use of
various sensors to capture patients‚Äô movements and the use ofAI algorithms to analyze patients‚Äô movements during exercise
[2], [3]. The analysis results can be used to provide patients
with feedback on the quality or completion of their exercises
[3], [4]. Additionally, clinicians can also use the analysis
results to monitor patients‚Äô progress and take appropriate
interventions.
In rehabilitation programs, patients are typically prescribed
speciÔ¨Åc exercises with designated numbers of sets and rep-
etitions [1], [5]‚Äì[8]. Evaluating exercise performance relies
on objective criteria such as compliance with the prescribed
number of sets and repetitions, repeating exercises in a con-
stant manner, proper technique and quality of movements,
and correct posture of various body parts [5], [6]. Therefore,
repetition (temporal) segmentation, the process of dividing
a continuous sequence of movement data into individual
repetitions, is the Ô¨Årst step in an AI-driven exercise evaluation
pipeline [5]. Exercise repetition counting can either be derived
from the segmentation process or executed as a separate task.
A variety of data modalities were used for repetition seg-
mentation and counting [20], including Inertial Measurement
Unit (IMU) sensor data [9]‚Äì[15], video data [16]‚Äì[18], and
skeletal body joints [19]. Existing algorithms for segment-
ing human movement can be divided into unsupervised and
supervised algorithms [20]. Unsupervised algorithms, which
do not require labeled data to develop, include thresholding,
template matching, and exemplar-based approaches [20], [21].
Supervised algorithms requiring labeled data to be trained on
include Support Vector Machines (SVM) [11], Hidden Markov
Model (HMM) [13], Convolutional Neural Networks (CNNs)
[10], and the combination of CNNs and Finite State Machines
(FSMs) [9]. The existing works on repetition counting [15],
[18], [19], [28] are not capable of segmenting movement
data into individual repetitions, that is, they are not capable
of determining the start and end timestamps of individual
repetitions. Video-based approaches are not privacy-preserving
and are computationally prohibitive [16]‚Äì[18]. The IMU-based
approaches require wearing multiple IMU sensors while exer-
cising [9]‚Äì[15], which is challenging in the real world, i.e., in
rehabilitation patients‚Äô homes. The previous works on healthy
individuals [16]‚Äì[19] cannot be directly used on rehabilitation
patients due to the fact that segmenting and counting exercise
repetitions is more challenging in patients due to irregularitiesarXiv:2304.09735v1  [cs.CV]  19 Apr 2023in the duration and completion of exercises resulting from their
respective impairments [5], [6].
This paper presents novel methods for rehabilitation exer-
cise repetition segmentation and counting from the skeletal
body joints of patients. Various many-to-many and many-to-
one deep sequential neural network architectures, including
Long Short Term Memory (LSTM) and a combination of
LSTM and CNN, are designed to analyze the sequences of
skeletal body joints and perform repetition segmentation and
counting. Our primary contributions are as follows:
This is the Ô¨Årst work on rehabilitation exercise repe-
tition segmentation using skeletal body joints collected
by depth cameras or extracted from RGB video using
advanced computer-vision techniques.
We developed neural network architectures capable of
analyzing sequences of body joints, i.e., multivariate time
series.
We conducted extensive experiments on three publicly
available rehabilitation exercise datasets and demon-
strated the effectiveness of the proposed method com-
pared to the previous methods.
As a point of clariÔ¨Åcation, the purpose of this paper is not
rehabilitation exercise recognition/classiÔ¨Åcation nor rehabilita-
tion exercise quality/correctness assessment. SpeciÔ¨Åcally, this
paper focuses on the temporal (not spatial) segmentation of
rehabilitation exercises into individual repetitions and counting
the number of repetitions.
II. R ELATED WORK
This section reviews existing research on repetitive action
segmentation and counting, with an emphasis on rehabilitation
exercise segmentation and counting. The literature review is
organized according to the data modality used for segmenta-
tion and counting.
A. Rehabilitation Exercise Segmentation from IMU sensors
Lin et al. [11] proposed an approach for segmenting reha-
bilitation exercises into individual repetitions using the data
collected by IMU wearable sensors worn on the hip, knee,
and ankle. Segmentation was deÔ¨Åned as a binary classiÔ¨Åcation
problem in which movement data at consecutive timestamps
are classiÔ¨Åed into segment points or non-segment points. To
perform segmentation, after several steps of preprocessing,
including Ô¨Åltering, down-sampling, and windowing, the IMU
signal is classiÔ¨Åed into segment or non-segment classes by an
SVM. Lin et al. [13] proposed a two-stage approach in which
Ô¨Årst, segment point candidates are identiÔ¨Åed by analyzing
the velocity features extracted from the IMU signal. Then,
HMMs are used to recognize segment locations from segment
point candidates. The method was evaluated on three publicly
available IMU datasets and achieved high accuracy. Brennan
et al. [9], [10] proposed a two-stage approach in which
Ô¨Årst, a CNN classiÔ¨Åes sliding windows of the IMU signals
into either ‚Äùdynamic‚Äù or ‚Äùdormant‚Äù classes. The classiÔ¨Åcation
results are then streamed into an FSM that keeps track of the
classes of consecutive windows and outputs the starting andending points of repetitions. They achieved high accuracy in
shoulder and knee exercises. Bevilacqua et al. [14] proposed
a joint rehabilitation exercise motion primitive segmentation
and classiÔ¨Åcation using a mixture of LSTMs and boosting
aggregation. Their method was evaluated on accelerometer
and gyroscope data and achieved high exercise primitive
classiÔ¨Åcation accuracy.
B. Rehabilitation Exercise Repetition Counting from Skeletal
Body Joints
Hsu et al. [19] proposed a rehabilitation exercise repetition
counting using Skeletal body joints data. Initially, the pairwise
cosine similarity of the skeleton time series data is calculated.
A spectrogram is then constructed based on the pairwise cosine
similarity. A repetition count is then calculated by integrating
from the spectrogram. The method was evaluated on the UI-
PRMD rehabilitation exercise dataset [7] and the MM-Fit
Ô¨Åtness exercise dataset [34] and achieved low Mean Absolute
Error (MAE).
C. Repetitive Action Segmentation from Videos
Hu et al. [16] introduced a large-scale repetitive action
counting dataset, named RepCount, containing 1451 videos
with about 20000 annotations of the start and end of rep-
etitions. The dataset contains in-the-wild videos of healthy
individuals exercising. They have also proposed a deep neural
network architecture, named TransRAC [16], for repetition
counting that was trained and evaluated on the RepCount
dataset. In TransRAC, with step sizes of 1, 2, and 4, multi-
scale video sequences are generated from the input video.
After extracting features from the multi-scale video sequences
by an encoder neural network, temporal correlations are calcu-
lated between the extracted features, and correlation matrices
are created. The concatenation of the correlation matrices is
input to a transformer to output a density map as a prediction.
The ground-truth density maps are generated by approximating
a Gaussian distribution between the start and end of each
repetition [16]. To overcome a major limitation of TransRAC,
its inability to handle long videos, Chung et al. [17] proposed
a video transformer equipped with class token distillation and
marginally improved the repetition counting results of Tran-
sRAC. Zhang et al. [18] proposed an approach for repetition
counting from videos incorporating the corresponding sound
of the video. An S3D-based architecture was used for repe-
tition counting from video, while a ResNet-18-based neural
network was used for repetition counting from audio. The
results on an audiovisual dataset showed improvements when
audio data is added to video data for repetition counting. To
learn more about repetitive action segmentation and counting
from videos using deep learning algorithms, please refer to
[18], [24]‚Äì[28].
The major disadvantage of the IMU-based methods [9]‚Äì
[11], [13], [14] is that they require the determination of various
parameters, including window sizes and thresholds. Addition-
ally, these methods are capable of analyzing multivariate time
series with a small number of variables, corresponding to thenumber of IMU sensors worn during exercise, but are unable
to analyze multivariate time series with high dimensionalities,
such as all the joints of the skeletal system. Moreover, it may
be infeasible for rehabilitation patients to wear multiple IMU
sensors while exercising independently at home. The method
proposed by Hsu et al. [19] was designed for rehabilitation
exercise counting and is not capable of segmenting rehabilita-
tion exercises. Exercise temporal segmentation into individual
repetitions is required for accurate exercise assessment [5].
A limitation of video-based methods [16]‚Äì[18], [24]‚Äì[28] is
their complexity and their large number of training parameters,
in addition to privacy concerns. There is no previous method
for segmenting rehabilitation exercises based on skeletal body
joints [5]. In order to Ô¨Åll the gap in the literature, this
paper proposes deep-learning algorithms for segmenting and
counting repetitions in rehabilitation exercises performed by
patients. The proposed method facilitates exercise assessment
and feedback generation for patients in virtual rehabilitation
programs without the need for body-worn sensors.
III. M ETHOD
The input to the proposed method is the sequence of skeletal
body joints of a patient doing rehabilitation exercises. The
output is the segmented sequence into individual exercise
repetitions and the total number of repetitions. The sequence
of features extracted from the sequence of skeletal body joints
is fed to a sequential neural network for exercise repetition
segmentation and counting.
The proposed method can work with either a depth camera
capable of capturing skeletal body joints or a regular RGB
camera. In the latter case, an additional module is required in
order to extract skeletal body joints from RGB video frames.
Several libraries are available for extracting body joints from
videos, including MediaPipe [22] and OpenPose [23].
The sequential neural networks in the proposed method
can be provided with three types of data, raw body joints,
exercise-speciÔ¨Åc features extracted from the body joints, and
their concatenation. Inspired by Guo and Khan [29] worked
on the KIMORE dataset, exercise-speciÔ¨Åc features are those
calculated based on the angles between the body joints that are
moving in speciÔ¨Åc exercises. For instance, for upper-extremity
rehabilitation exercises for stroke patients, the shoulder-wrist
angle is an exercise-speciÔ¨Åc feature [6], [29]. The body joints
or features, extracted from each frame of the input data, are
provided for each timestamp of a sequential neural network.
A. Sequential Neural Network
Three sequential neural networks are used to analyze the
sequence of features and perform rehabilitation exercise seg-
mentation and counting. The core of all the networks is an
LSTM trailed by a 1D CNN followed by a fully connected
neural network.
1) Many-to-many with Binary Sequence Output: The se-
quential neural network is trained to output a binary sequence.
Corresponding to the input at every timestamp (the body joints
or extracted feature vector from every frame), the networkgenerates one or zero. The output of one indicates that the
input frame at a particular timestamp is a frame after a
repetition of an exercise is complete or before the beginning
of the next repetition, whereas an output of zero indicates that
the input frame is during a repetition of an exercise. Using the
generated output binary sequence, repetition segmentation is
performed based on the occurrences of outputs of one among
outputs of zero. Repetition counting is done by counting the
number of segmented repetitions.
2) Many-to-many with Density Map Output: The sequential
neural network is trained to output a density map. A density
map is a vector consisting of the same number of elements
as the number of frames in the input, i.e., the number of
timestamps in the sequential neural network. The ground-
truth density maps are generated by approximating a Gaussian
distribution between the start and end of each repetition [16].
Repetition segmentation is performed by Ô¨Ånding the peaks
in the predicted density map. Repetition counting is done by
counting the number of segmented repetitions.
3) Many-to-one with Repetition Counts Output: By sum-
ming the outputs of the sequential neural network at con-
secutive timestamps, it will be converted to a many-to-one
sequential neural network and is trained to output the number
of repetitions. Unlike the two previous architectures, this
architecture can only perform repetition counting and not
segmentation.
The details of the parameters of the sequential neural
networks are explained in Section IV-C.
IV. E XPERIMENTS
This section evaluates the performance of the proposed
method on three publicly available datasets using different
evaluation metrics and in comparison to previous methods.
A. Datasets
1) KIMORE: The KIMORE dataset [6] contains RGB and
depth videos along with body joint position and orientation
data captured by the Kinect camera. The data were collected
from 78 subjects, including 44 healthy subjects and 34 patients
with motor dysfunction (stroke, Parkinson‚Äôs disease, and low
back pain). Each data sample in this dataset is composed
of one subject performing multiple repetitions of one of the
Ô¨Åve exercises: (1) lifting of the arms, (2) lateral tilt of the
trunk with the arms in extension, (3) trunk rotation, (4) pelvis
rotations on the transverse plane, and (5) squatting. The data
samples were annotated in terms of exercise quality and
technique.
The focus of this paper is on repetition segmentation and
counting. KIMORE, however, does not contain such annota-
tions. Two co-authors of this paper annotated the start and
end of repetitions in each data sample using RGB video
playback and the Aegisub software [35]. The annotations
were veriÔ¨Åed by another two co-authors. The annotations
were used as ground-truth labels for training and evaluation
of neural network models. The annotations are available
at https://github.com/abedicodes/repetition-segmentation. Themean (standard deviation) of the number of repetitions across
the total 353 samples in the dataset is 4.70 (2.21).
In this paper, two modalities of data from the KIMORE
dataset were used. See Section III. In the Ô¨Årst setting, RGB
videos were used as input data. The body joints were extracted
by OpenPose [23] and then inputted into the sequential models
for analysis. In the second setting, the body joints captured
by Kinect were directly considered as input to the sequential
models.
2) UI-PRMD: UI-PRMD [7] is a dataset of physical
therapy rehabilitation collected from 10 healthy individuals
who performed 10 physical therapy rehabilitation exercises
correctly and incorrectly to represent the performance of
patients. Each data sample in this dataset is composed of
one subject performing multiple repetitions of one of the 10
exercises, (1) deep squat, (2) hurdle step, (3) inline lunge, (4)
side lunge, (5) sit to stand, (6) standing active straight leg
raise, (7) standing shoulder abduction, (8) standing shoulder
extension, (9) standing shoulder internal-external rotation, and
(10) standing shoulder scaption. The annotations for repetition
segmentation were available in UI-PRMD and were used in
our experiments. The mean (standard deviation) of the number
of repetitions across the total 200 samples in the dataset is
10.00 (0.00). This dataset contains body joints captured by
Vicon and Kinect cameras. The Kinect data was used in our
experiments.
3) IntelliRehabDS: IntelliRehabDS [8] is a dataset of body
joints captured by the Kinect camera collected from 29 sub-
jects, 15 patients and 14 healthy subjects. Each data sample in
this dataset is composed of one subject performing multiple
repetitions of one of the 9 physical rehabilitation exercises, (1)
elbow Ô¨Çexion left, (2) elbow Ô¨Çexion right, (3) shoulder Ô¨Çexion
left, (4) shoulder Ô¨Çexion right, (5) shoulder abduction left,
(6) shoulder abduction right, (7) shoulder forward elevation,
(8) side tap left, and (9) side tap right. The annotations for
repetition segmentation were available in IntelliRehabDS and
were used in our experiments. The mean (standard deviation)
of the number of repetitions across the total 361 samples in
the dataset is 5.69 (2.48).
B. Evaluation Metrics
In accordance with the literature, Off-By-One count Accu-
racy (OBOA) and MAE were used as evaluation metrics for
repetition counting [16], [17], [19]. Intersection-Over-Union
(IOU) [31] was used for repetition segmentation. A new metric
for evaluating repetition segmentation was developed in this
study, Mean Average Error in Frames (MAE-F). First, for
every data sample, the average number of frames by which
the start and end points of the predicted repetition deviate
from those of the ground truth repetition is calculated. Then,
these deviations are averaged over all data samples.
C. Experimental Settings
The dimension of the input to the sequential neural networks
is determined based on the dimension of the feature vectors
extracted from frames of the data samples. This dimension is75, 43, and 118, when using the Kinect body joints, exercise-
speciÔ¨Åc features [29], and their concatenation, respectively.
This dimension is the number of neurons in the input layer
of the LSTM. The number of neurons in the hidden layers
of LSTM is set to two times the dimension of the feature
vectors. In different exercises of different datasets, the number
of LSTM layers varies from one to three layers.
For comparison, we implemented a modiÔ¨Åed version of the
TransRAC model [16], described in Section II-C. In place of
using the encoder in TransRAC for feature extraction from
multi-scale videos, multi-scale body joints were replaced.
The other parts of the network remained unchanged and
the network was trained from scratch to predict the density
map. As a video-based method, the pre-trained TransRAC on
RepCount TransRAC was also used to predict the density map
for the RGB videos of exercises in the KIMORE dataset [6].
Due to the fact that none of the datasets determined separate
training and test sets, Ô¨Åve-fold cross-validation was used. The
loss function was a linear combination of the Kullback-Leibler
divergence loss and L1 loss minimized by the Adam optimizer
[32]. The experiments were implemented in PyTorch [32]
and scikit-learn [33] on a server with 128 GB of RAM and
NVIDIA 2080 Ti 12 GB GPU. The code of our implemen-
tation is available at https://github.com/abedicodes/repetition-
segmentation.
D. Experimental Results
1) KIMORE: Tables I (a)-(b), and (c)-(d), respectively
present the results of repetition counting, and segmentation
on the KIMORE dataset [6] through Ô¨Åve-fold cross-validation.
The body joints captured by the Kinect camera in the dataset
(rows #4-11 in tables), the raw RGB videos in the dataset
(#2), and the body joints extracted from the RGB videos by
OpenPose (#1 and #3) [23] were analyzed in the experiments
in Table I. It should be noted that repetition counting was
carried out in rows #1 and #3-10 of Tables I (a)-(b) as a result
of repetition segmentation. First, repetition segmentation was
undertaken, and the number of repetitions was counted. The
repetition counting was performed directly in rows #2 and #11
of Tables I (a)-(b).
The results of the many-to-many model with density map
output (#8) are superior to those of the many-to-many model
with binary sequence output (#10). The ground-truth density
maps were generated by approximating a Gaussian distribution
between the start and end of each repetition [16]. The model
that was trained on and predicted based on density maps was
more robust to noise and Ô¨Çuctuations in body joints, as well
as irregularities and imperfections in completing exercises in
the patients in the KIMORE dataset.
There is a signiÔ¨Åcant difference between the results of the
two models described above (#8 and #10) and those of the
many-to-one model with repetition counts output (#11). One
reason for this is that more ground-truth information was
available for the training of the Ô¨Årst two models. The Ô¨Årst two
models described above were provided with labels for everyTABLE I: (a) Mean absolute error and (b) off-by-one accuracy of repetition counting, and (c) intersection over union and (d)
mean average error in frames of repetition segmentation on the KIMORE dataset [6] through Ô¨Åve-fold cross-validation. Density
Map: many-to-many model with density map output, General: One single model was trained and evaluated on all the samples
in the dataset, Pre-trained TransRAC: Pre-trained TransRAC model on RepCount video dataset [16], ModiÔ¨Åed TransRAC: A
modiÔ¨Åed version of the TransRAC model [16] by removing the autoencoder feature extractor in TransRAC and replacing the
body joints as the features, Exercise SpeciÔ¨Åc: Exercise-speciÔ¨Åc models were trained and evaluated on the samples of speciÔ¨Åc
exercises in the dataset, Density Map (LSTM Only): many-to-many model without CNN with density map output, Binary
sequence: many-to-many model with binary sequence output, Counts: many-to-one model with repetition counts output.
(a) Mean absolute error ‚Üì 
# Data  Features  Model  Ex. 1  Ex. 2  Ex. 3  Ex. 4  Ex. 5  Ex. 1 -5 (Total)  
1 Video  Body joints (OpenPose)  Density Map ‚Äì General  1.8143  3.3538  3.5152  7.7465  2.7432  3.8873  
2 Video  - Pre-trained TransRAC [16]  3.5972  2.0909  2.2794  4.7917  2.4800  3.0690  
3 Video  Body joints (OpenPose)  Modified TransRAC  [16] 3.6143  7.4769  7.4545  7.8143  3.1081  5.8203  
4 Kinect  Body joints  Modified TransRAC  [16] 3.4487  7.27400  7.4118  8.6667  3.7895  6.1011  
5 Kinect  Body joints  Density Map ‚Äì Exercise Specific  0.1944  0.4000  0.5147  1.6389  0.0933  0.5682  
6 Kinect  Features  Density Map ‚Äì General  0.3611  0.4462  0.4706  1.9722  0.5333  0.7642  
7 Kinect  Body joints + Features  Density Map ‚Äì General  0.3056  0.3385  0.4559  1.9583  0.3600  1.5739  
8 Kinect  Body joints  Density Map ‚Äì General  0.4306  0.4000  0.5147  1.1944  0.1333  0.5313  
9 Kinect  Body joints  Density Map (LSTM only) ‚Äì General  0.6944  0.3333  0.6323  0.5417  0.8649  0.6193  
10 Kinect  Body joints  Binary Sequence ‚Äì General  1.5972  1.5231  1.6912  3.1389 2.2533  1.9205  
11 Kinect  Body  joints  Counts ‚Äì General  0.5000 0.7846  1.0000 2.0555  0.7867  2.5085  
 
(b) Off-by-one accuracy ‚Üë 
# Data  Features  Model  Ex. 1  Ex. 2  Ex. 3  Ex. 4  Ex. 5  Ex. 1 -5 (Total)  
1 Video  Body joints (OpenPose)  Density Map ‚Äì General  0.6143  0.2308  0.1667  0.0845  0.3919  0.3006  
2 Video  - Pre-trained TransRAC [16]  0.0694  0.4545  0.3676  0.1944  0.3600  0.2861  
3 Video  Body joints (OpenPose)  Modified TransRAC  [16] 0.1714  0.0462  0.0758  0.0429  0.2838  0.1275  
4 Kinect  Body joints  Modified TransRAC  [16] 0.1923  0.0548  0.0294  0.0123  0.1447  0.0878  
5 Kinect  Body joints  Density Map ‚Äì Exercise Specific  0.9861  0.9692  0.8971  0.6250  0.9867  0.8920  
6 Kinect  Features  Density Map ‚Äì General  0.9306  0.9231  0.9412  0.5833  0.8800  0.8494  
7 Kinect  Body joints + Features  Density Map ‚Äì General  0.9583  0.9538  0.9265  0.5278  0.9200  0.8665  
8 Kinect  Body joints  Density Map ‚Äì General  0.9444  0.9846  0.9117  0.7778  0.9867  0.9233  
9 Kinect  Body joints  Density Map (LSTM only) ‚Äì General  0.8611  0.9546  0.8970  0.9028  0.8378  0.8890  
10 Kinect  Body joints  Binary Sequence ‚Äì General  0.5417  0.6462  0.6176  0.3750  0.4933  0.5313  
11 Kinect  Body joints  Counts ‚Äì General  0.9444  0.8308  0.7647  0.4444  0.9200  0.8011  
 
(c) Intersection over union ‚Üë 
# Data  Features  Model  Ex. 1  Ex. 2  Ex. 3  Ex. 4  Ex. 5  Ex. 1 -5 (Total)  
1 Video  Body joints (OpenPose)  Density Map ‚Äì General  0.4352  0.2485  0.2454  0.2368  0.3618  0.3075  
2 Video  - Pre-trained TransRAC [16]  - - - - - - 
3 Video  Body joints (OpenPose)  Modified TransRAC  [16] 0.0707  0.0510  0.0616  0.0549  0.0603  0.0513  
4 Kinect  Body joints  Modified TransRAC  [16] 0.0940  0.1481  0.2773  0.0732  0.1211  0.1154  
5 Kinect  Body joints  Density Map ‚Äì Exercise Specific  0.9083  0.5244  0.6291  0.4426  0.8812  0.6825  
6 Kinect  Features  Density Map ‚Äì General  0.5347  0.4721  0.4070  0.3357  0.2862  0.3606  
7 Kinect  Body joints + Features  Density Map ‚Äì General  0.6941  0.5236  0.4507  0.5406  0.3799  0.4410  
8 Kinect  Body joints  Density Map ‚Äì General  0.7072  0.6715  0.9107  0.2163  0.8375  0.6886  
9 Kinect  Body joints  Density Map (LSTM only) ‚Äì General  0.6359  0.7070  0.6946  0.6393  0.6699  0.6680  
10 Kinect  Body joints  Binary Sequence ‚Äì General  0.3377  0.2638  0.3080  0.1807  0.2910  0.2760  
11 Kinect  Body joints  Counts ‚Äì General  - - - - - - 
 
(d) Mean average error in frames  ‚Üì 
# Data  Features  Model  Ex. 1  Ex. 2  Ex. 3  Ex. 4  Ex. 5  Ex. 1 -5 (Total)  
1 Video  Body joints (OpenPose)  Density Map ‚Äì General  57 137 99 108 59 91 
2 Video  - Pre-trained TransRAC [16]  - - - - - - 
3 Video  Body joints (OpenPose)  Modified TransRAC  [16] 126 124 93 117 104 109 
4 Kinect  Body joints  Modified TransRAC  [16] 111 62 88 128 81 87 
5 Kinect  Body joints  Density Map ‚Äì Exercise Specific  13 45 46 38 8 30 
6 Kinect  Features  Density Map ‚Äì General  33 26 40 36 40 35 
7 Kinect  Body joints + Features  Density Map ‚Äì General  31 34 32 37 33 33 
8 Kinect  Body joints  Density Map  ‚Äì General  36 25 28 24 28 28 
9 Kinect  Body joints  Density Map (LSTM only) ‚Äì General  40 22 22 25 32 28 
10 Kinect  Body joints  Binary Sequence ‚Äì General  72 97 83 95 65 82 
11 Kinect  Body joints  Counts ‚Äì General  - - - - - - 
 timestamp; however, the many-to-one model was provided
with only one label for the entire network.
The results based on body joints captured by the Kinect
camera (#8) were far better than those extracted from RGB
videos (#1). Due to the low quality of videos in the KIMORE
dataset, the high distance of the subjects to the camera, im-
perfect lighting conditions, and blurred faces in the dataset to
preserve privacy, OpenPose [23] had difÔ¨Åculty extracting body
joints from the dataset, resulting in low-quality body joints
compared to Kinect body joints. Therefore, the OpenPose
[23] input to the models was noisy, resulting in suboptimal
performance.
Comparing the results of the many-to-many model with
density map output (#8) as described in Section III-A2 (an
LSTM trailed by a 1D CNN and a linear layer) with the results
of the many-to-many model with density map with only an
LSTM trailed by a linear layer and without 1D CNN (#9)
shows the importance of including 1D CNN in the proposed
model.
Across all settings, the proposed method outperformed
the modiÔ¨Åed TransRAC method (#3-4), Section IV-C. The
reason for this can be attributed to two factors: the extreme
complexity of the modiÔ¨Åed TransRAC model in comparison to
the simple yet effective sequential models used in this paper, as
well as the limited number of training samples in the KIMORE
dataset. It is common for healthcare and rehabilitation datasets
to have a small number of training samples. In this regard,
it illustrates the necessity of taking the amount of data into
account when selecting and designing the architecture of deep
neural networks.
The RGB videos of exercises included in the KIMORE
dataset were input into the original TransRAC model which
was pre-trained on the RepCount video dataset [16] (#2). The
results are much inferior to those predicted by the proposed
architectures. The RepCount dataset contains videos of healthy
subjects performing repetitive actions perfectly, while the
KIMORE dataset contains rehabilitation exercises conducted
by patients with imperfect repetitive actions.
Comparing the results of different features provided to the
many-to-many model with density map output, body joints
(#8), features extracted from body joints (#6), and their
concatenation (#7) indicates that for the repetition counting
and segmentation tasks, there is no need to extract handcrafted
features from body joints [29] and the body joints as the input
to the models result in the best performance.
According to the results of the general model trained and
evaluated on all the samples in the dataset (#8), as compared to
the exercise-speciÔ¨Åc models trained and evaluated on speciÔ¨Åc
exercises (#5), exercise-speciÔ¨Åc models are not needed for rep-
etition counting and segmentation tasks, and general models
perform marginally better than exercise-speciÔ¨Åc models.
Comparing the performance of the models for speciÔ¨Åc exer-
cises reveals that almost all methods had difÔ¨Åculty segmenting
and counting repetitions for Ex. 4 in the KIMORE dataset.
This is due to the fact that Ex. 4 involved pelvis rotations,
i.e., movements along the z axis, which is relatively difÔ¨Åcultto capture by depth cameras and analyze by models. As the
movements in Ex. 4 differed from those in other exercises,
which involved movements of the hands and feet along the x
and y axes, it was difÔ¨Åcult to generalize models to the samples
in Ex. 4.
There is a similar overall trend in the performance of
different methods in different settings across all four evaluation
metrics, MAE, OBO, IOU, and MAE-F. Overall, the general
many-to-many model with density map output trained with
Kinect body joints achieved superior results with the lowest
total MAE (0.5313) and the highest total OBO (0.9233) for
repetition counting, as well as the highest total IOU (0.6886)
and the lowest total MAE-F (28) for repetition segmentation.
2) UI-PRMD: Table II (a) shows the results of repetition
segmentation and counting on the UI-PRMD dataset [7] using
the general many-to-many model with density map output and
the general many-to-one model with repetition counts output
both trained and evaluated on Kinect body joints through Ô¨Åve-
fold cross-validation. Table II (a) illustrates that the repetition
counting results of the many-to-many model with density map
output are very similar to those of the many-to-one model with
repetition counts output having zero errors in counting and
outperforming the previous repetition counting method [19].
It is important to note that the number of repetitions across
samples in the UI-PRMD dataset has a very low standard
deviation. Using the proposed method, repetition segmentation
is successfully performed with a total IOU of 0.82 and a total
MAE-F of 12.
3) IntelliRehabDS: Table II (b) shows the results of repeti-
tion segmentation and counting on the IntelliRehabDS dataset
[8] using the general many-to-many model with density map
output and the general many-to-one model with repetition
counts output both trained and evaluated on Kinect body joints
through Ô¨Åve-fold cross-validation. In Table II (b), the results of
the many-to-many model with density map output are superior
to those of the many-to-one model with repetition counts
output. Using the proposed method, repetition segmentation
is successfully performed with a total IOU of 0.68 and a total
MAE-F of 39.
Figure 1 illustrates the ground truth values and predictions
for repetition counts of the proposed many-to-many model
with density map output on (a) healthy subjects and (b)
patients in all the exercises in the KIMORE dataset using Ô¨Åve-
fold cross-validation. The predictions closely follow ground-
truth values in both populations, however, there are some
deviations in patients as a result of irregularities in the duration
and completion of exercise repetitions in patients. Figure 1 (b)
shows large deviations in predictions for samples with a large
number of repetitions, such as samples with 12 and 13 repeti-
tions. This can be attributed to the imbalanced distribution of
samples in the dataset, i.e., having a small number of samples
with large numbers of repetitions.
V. C ONCLUSION AND FUTURE WORKS
The purpose of this study was to develop a learning-based
method for segmenting and counting repetitions in rehabil-TABLE II: Mean Absolute Error (MAE) and Off-By-One count accuracy (OBO) of repetition counting and Intersection-Over-
Union (IOU) and Mean Average Error in Frames (MAE-F) of repetition segmentation on (a) the UI-PRMD [7] and (b) the
IntelliRehabDS [8] dataset through Ô¨Åve-fold cross-validation. Density Map ‚Äì General: many-to-many model with density map
output trained and evaluated on all the samples in the dataset, Counts ‚Äì General: many-to-one model with repetition counts
output trained and evaluated on all the samples in the dataset.
(a) 
 Density Map  
‚Äì General  Counts  
‚Äì General  Density Map 
‚Äì General  Counts ‚Äì 
General  Density Map  
‚Äì General  Density Map  
‚Äì General  VI [19]  
 MAE ‚Üì MAE ‚Üì OBO ‚Üë OBO ‚Üë IOU ‚Üë MAE -F ‚Üì MAE ‚Üì 
Ex. 1  0.00 0.00 1.00 1.00 0.91 4 0.04 
Ex. 2  0.10 0.00 1.00 1.00 0.82 8 0.06 
Ex. 3  0.15 0.00 1.00 1.00 0.89 5 0.05 
Ex. 4  0.05 0.00 1.00 1.00 0.86 7 0.04 
Ex. 5  0.00 0.00 1.00 1.00 0.92 4 0.05 
Ex. 6  0.25 0.00 0.95 1.00 0.77 10 0.10 
Ex. 7  0.05 0.00 1.00 1.00 0.89 4 0.03 
Ex. 8  0.80 0.00 0.80 1.00 0.77 10 0.14 
Ex. 9  4.4 0.00 0.40 1.00 0.47 61 0.05 
Ex. 10  0.00 0.00 1.0 1.00 0.86 5 0.05 
Ex. 1 -10 (Total)  0.58 0.00 0.92 1.00 0.82 12 0.06 
 
(b) 
 Density Map  
‚Äì General  Counts  
‚Äì General  Density Map 
‚Äì General  Counts ‚Äì 
General  Density Map  
‚Äì General  Density Map  
‚Äì General   
 MAE ‚Üì MAE ‚Üì OBO ‚Üë OBO ‚Üë IOU ‚Üë MAE -F ‚Üì  
Ex. 1  1.05 1.03 0.88 0.68 0.67 24  
Ex. 2  0.32 0.73 0.93 0.78 0.73 24  
Ex. 3  1.42 2.04 0.75 0.63 0.59 94  
Ex. 4  0.73 1.51 0.86 0.61 0.67 80  
Ex. 5  0.85 0.93 0.92 0.74 0.65 26  
Ex. 6  0.37 0.97 0.95 0.83 0.81 7  
Ex. 7  0.50 0.92 0.95 0.70 0.71 37  
Ex. 8  0.53 0.93 0.85 0.70 0.67 28  
Ex. 9  0.62 1.06 0.92 0.69 0.65 33  
Ex. 1 -9 (Total)  0.72 1.13 0.89 0.80 0.68 39  
 
  
(a) (b) 
 3579111315
0 50 100 150 200Repetition Counts
Sample IndexGround truth
Prediction
3579111315
0 20 40 60 80 100 120 140Repetition Counts
Sample IndexGround truth
Prediction
Fig. 1: The ground truth values and predictions for repetition counts of the proposed many-to-many model with density map
output on (a) healthy subjects and (b) patients in all the exercises in the KIMORE dataset [6].itation exercises. On three publicly available rehabilitation
exercise datasets, the proposed method successfully segmented
and counted repetitions, and outperformed previous works, in-
cluding a video-based method. This study represented the Ô¨Årst
work on repetition segmentation using skeletal body joints and
data collected from patients, which is more challenging due
to irregularities in exercise duration and completion. Despite
being much lighter than video-based models [16]‚Äì[18], [24]‚Äì
[28], our sequential models require an initial stage of body-
joint extraction from videos or directly by the use of depth
cameras. Our body-joint-based method has the advantage of
being more interpretable and capable of being used in a
framework providing patients with actionable feedback on
their exercises. The successful segmentation and counting of
rehabilitation exercises using the proposed method is the Ô¨Årst
step in the development of an automated virtual rehabilitation
platform capable of assessing exercise quality and providing
feedback to patients and reports to clinicians. Future research
may involve incorporating the attention mechanism [30], [36]
into the current sequential models and extending the models to
facilitate multi-task learning to perform exercise segmentation
and assessment jointly.
REFERENCES
[1] Dibben, Grace O., et al. ‚ÄùExercise-based cardiac rehabilitation for
coronary heart disease: a meta-analysis.‚Äù European Heart Journal 44.6
(2023): 452-469.
[2] Ferreira, Ricardo, Rubim Santos, and Andreia Sousa. ‚ÄùUsage of Auxil-
iary Systems and ArtiÔ¨Åcial Intelligence in Home-Based Rehabilitation: A
Review.‚Äù Exploring the Convergence of Computer and Medical Science
Through Cloud Healthcare (2023): 163-196.
[3] Sangani, Samir, et al. ‚ÄùReal-time avatar-based feedback to enhance
the symmetry of spatiotemporal parameters after stroke: instantaneous
effects of different avatar views.‚Äù IEEE Transactions on Neural Systems
and Rehabilitation Engineering 28.4 (2020): 878-887.
[4] Fernandez-Cervantes, Victor, et al. ‚ÄùVirtualGym: A kinect-based system
for seniors exercising at home.‚Äù Entertainment Computing 27 (2018):
60-72.
[5] Liao, Yalin, et al. ‚ÄùA review of computational approaches for evaluation
of rehabilitation exercises.‚Äù Computers in biology and medicine 119
(2020): 103687.
[6] Capecci, Marianna, et al. ‚ÄùThe KIMORE dataset: KInematic assessment
of MOvement and clinical scores for remote monitoring of physical RE-
habilitation.‚Äù IEEE Transactions on Neural Systems and Rehabilitation
Engineering 27.7 (2019): 1436-1448.
[7] Vakanski, Aleksandar, et al. ‚ÄùA data set of human body movements for
physical rehabilitation exercises.‚Äù Data 3.1 (2018): 2.
[8] Miron, Alina, et al. ‚ÄùIntelliRehabDS (IRDS)‚ÄîA dataset of physical
rehabilitation movements.‚Äù Data 6.5 (2021): 46.
[9] Bevilacqua, Antonio, et al. ‚ÄùRehabilitation exercise segmentation for
autonomous biofeedback systems with ConvFSM.‚Äù 2019 41st Annual
International Conference of the IEEE Engineering in Medicine and
Biology Society (EMBC). IEEE, 2019.
[10] Brennan, Louise, et al. ‚ÄùSegmentation of shoulder rehabilitation ex-
ercises for single and multiple inertial sensor systems.‚Äù Journal
of Rehabilitation and Assistive Technologies Engineering 7 (2020):
2055668320915377.
[11] Lin, Jonathan Feng-Shun, Vladimir Joukov, and Dana Kuli ¬¥c.
‚ÄùClassiÔ¨Åcation-based segmentation for rehabilitation exercise monitor-
ing.‚Äù Journal of rehabilitation and assistive technologies engineering 5
(2018): 2055668318761523.
[12] Lin, Jonathan Feng-Shun, Vladimir Joukov, and Dana Kulic. ‚ÄùHuman
motion segmentation by data point classiÔ¨Åcation.‚Äù 2014 36th Annual
International Conference of the IEEE Engineering in Medicine and
Biology Society. IEEE, 2014.[13] Lin, Jonathan Feng-Shun, and Dana Kuli ¬¥c. ‚ÄùOnline segmentation of
human motion for automated rehabilitation exercise analysis.‚Äù IEEE
Transactions on Neural Systems and Rehabilitation Engineering 22.1
(2013): 168-180.
[14] Bevilacqua, Antonio, et al. ‚ÄùCombining real-time segmentation and
classiÔ¨Åcation of rehabilitation exercises with lstm networks and point-
wise boosting.‚Äù Proceedings of the AAAI Conference on ArtiÔ¨Åcial
Intelligence. V ol. 34. No. 08. 2020.
[15] Soro, Andrea, et al. ‚ÄùRecognition and repetition counting for complex
physical exercises with deep learning.‚Äù Sensors 19.3 (2019): 714.
[16] Hu, Huazhang, et al. ‚ÄùTransRAC: Encoding Multi-scale Temporal Cor-
relation with Transformers for Repetitive Action Counting.‚Äù Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recog-
nition. 2022.
[17] Chung, Kevin, Alec Kerrigan, and Ishan Dave. ‚ÄùLong Action Repetition
Counting with Staged Layer Distillation.‚Äù
[18] Zhang, Yunhua, Ling Shao, and Cees GM Snoek. ‚ÄùRepetitive activity
counting by sight and sound.‚Äù Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition. 2021.
[19] Hsu, Yu Cheng, et al. ‚ÄùViewpoint-Invariant Exercise Repetition Count-
ing.‚Äù arXiv preprint arXiv:2107.13760 (2021).
[20] Lin, Jonathan Feng-Shun, Michelle Karg, and Dana Kuli ¬¥c. ‚ÄùMovement
primitive segmentation for human motion modeling: A framework for
analysis.‚Äù IEEE Transactions on Human-Machine Systems 46.3 (2016):
325-339.
[21] SarsÔ¨Åeld, Joe, et al. ‚ÄùSegmentation of exercise repetitions enabling real-
time patient analysis and feedback using a single exemplar.‚Äù IEEE
Transactions on Neural Systems and Rehabilitation Engineering 27.5
(2019): 1004-1019.
[22] Lugaresi, Camillo, et al. ‚ÄùMediapipe: A framework for building percep-
tion pipelines.‚Äù arXiv preprint arXiv:1906.08172 (2019).
[23] Pavllo, Dario, et al. ‚Äù3d human pose estimation in video with tem-
poral convolutions and semi-supervised training.‚Äù Proceedings of the
IEEE/CVF conference on computer vision and pattern recognition. 2019.
[24] Dwibedi, Debidatta, et al. ‚ÄùCounting out time: Class agnostic video rep-
etition counting in the wild.‚Äù Proceedings of the IEEE/CVF conference
on computer vision and pattern recognition. 2020.
[25] Ferreira, Bruno, et al. ‚ÄùDeep learning approaches for workout repetition
counting and validation.‚Äù Pattern Recognition Letters 151 (2021): 259-
266.
[26] Cheng, Sheng-Hsien, et al. ‚ÄùPeriodic Physical Activity Information
Segmentation, Counting and Recognition from Video.‚Äù IEEE Access
(2023).
[27] Jacquelin, Nicolas, Romain Vuillemot, and Stefan Duffner. ‚ÄùPeriodicity
counting in videos with unsupervised learning of cyclic embeddings.‚Äù
Pattern Recognition Letters 161 (2022): 59-66.
[28] Yu, Qingtian, et al. ‚ÄùDeep Learning-Enabled Multitask System for
Exercise Recognition and Counting.‚Äù Multimodal Technologies and
Interaction 5.9 (2021): 55.
[29] Guo, Qingyang, and Shehroz S. Khan. ‚ÄùExercise-SpeciÔ¨Åc Feature Ex-
traction Approach for Assessing Physical Rehabilitation.‚Äù 4th IJCAI
Workshop on AI for Aging, Rehabilitation and Intelligent Assisted
Living. IJCAI. 2021.
[30] S. S. Khan and A. Abedi, ‚ÄùStep Counting with Attention-based
LSTM,‚Äù 2022 IEEE Symposium Series on Computational Intel-
ligence (SSCI), Singapore, Singapore, 2022, pp. 559-566, doi:
10.1109/SSCI51031.2022.10022210.
[31] Jiang, Zhengkai, et al. ‚ÄùSTC: spatio-temporal contrastive learning for
video instance segmentation.‚Äù Computer Vision‚ÄìECCV 2022 Work-
shops: Tel Aviv, Israel, October 23‚Äì27, 2022, Proceedings, Part IV .
Cham: Springer Nature Switzerland, 2023.
[32] Paszke, Adam, et al. ‚ÄùPytorch: An imperative style, high-performance
deep learning library.‚Äù Advances in neural information processing sys-
tems 32 (2019).
[33] Pedregosa, Fabian, et al. ‚ÄùScikit-learn: Machine learning in Python.‚Äù the
Journal of machine Learning research 12 (2011): 2825-2830.
[34] Str ¬®omb¬®ack, David, Sangxia Huang, and Valentin Radu. ‚ÄùMm-Ô¨Åt: Mul-
timodal deep learning for automatic exercise logging across sensing
devices.‚Äù Proceedings of the ACM on Interactive, Mobile, Wearable and
Ubiquitous Technologies 4.4 (2020): 1-22.
[35] https://github.com/Aegisub/Aegisub
[36] Nikpour, Bahareh, and Narges Armanfard. ‚ÄùSpatio-Temporal Hard
Attention Learning for Skeleton-based Activity Recognition.‚Äù Pattern
Recognition (2023): 109428.