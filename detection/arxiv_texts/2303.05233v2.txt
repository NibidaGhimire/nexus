This work has been accepted for publication in 2023 IEEE International Conference on Communications (ICC): SAC Aerial Communications Track
Dual-Attention Deep Reinforcement Learning for
Multi-MAP 3D Trajectory Optimization
in Dynamic 5G Networks
Esteban Catt ¬¥e, Mohamed Sana and Mickael Maman
CEA-Leti, Universite Grenoble Alpes, F-38000 Grenoble, France
festeban.catte, mohamed.sana, mickael.maman g@cea.fr
Abstract ‚Äî5G and beyond networks need to provide dynamic
and efÔ¨Åcient infrastructure management to better adapt to time-
varying user behaviors (e.g., user mobility, interference, user
trafÔ¨Åc and evolution of the network topology). In this paper, we
propose to manage the trajectory of Mobile Access Points (MAPs)
under all these dynamic constraints with reduced complexity.
We Ô¨Årst formulate the placement problem to manage MAPs
over time. Our solution addresses time-varying user trafÔ¨Åc
and user mobility through a Multi-Agent Deep Reinforcement
Learning (MADRL). To achieve real-time behavior, the proposed
solution learns to perform distributed assignment of MAP-user
positions and schedules the MAP path among all users without
centralized user‚Äôs clustering feedback. Our solution exploits a
dual-attention MADRL model via proximal policy optimization
to dynamically move MAPs in 3D. The dual-attention takes into
account information from both users and MAPs. The cooperation
mechanism of our solution allows to manage different scenarios,
without a priory information and without re-training, which
signiÔ¨Åcantly reduces complexity.
I. I NTRODUCTION
In 5G and beyond networks, it is important to ensure that
equal opportunity is offered to users regardless of their location
and mobility with a dynamic and efÔ¨Åcient management of the
infrastructure. This Ô¨Çexible infrastructure can be implemented
as a service [1] by using Mobile Access Points (MAPs) and
will better adapt to time-varying user behavior. In recent years,
the deployment of these MAPs has been widely studied to
improve the Ô¨Çexibility of self-managed infrastructures [2].
However, their management is still poorly studied in highly
dynamic networks, taking into account i)user mobility, ii)
interference, iii)time-varying user trafÔ¨Åc and iv)changing
scenarios. This paradigm needs to balance the real-time place-
ment of MAPs by tracking the evolution of each user‚Äôs state to
improve the experience with long-term behavior to optimize
network resources without adding much complexity to network
operation. Our objective is to manage the trajectory of MAPs
under all these dynamic constraints with reduced complexity.
In the literature, the MAP management Ô¨Årstly followed
centralized and combinatorial approaches. Authors in [3] pre-
sented a mixed integer linear problem solved iteratively taking
into account user‚Äôs mobility. To include user‚Äôs demand, authors
in [4] designed a successive convex optimization algorithm.
It maximizes the average throughput and place MAPs over
time. Authors in [5] tackled the co-channel interference and
MAPs completion time while maximizing average throughput
using a particle swarm optimization. These approaches Ô¨Ånds
optimal solutions for simpliÔ¨Åed models but fails to handle all
constraints at the same time. These methods resolve successivedeployment problem over time to create a continuous behavior
and adapt to different scenarios with high complexity.
By considering each MAP as an agent, Reinforcement
Learning (RL) and Deep Reinforcement Learning (DRL) ap-
proaches reduce the complex solution to a single Q-function
method for creating complex behaviors. The authors of [6]
address 3D placement with moving users for one MAP. Then,
the approach was extended to multiple MAPs in [7] but they
consider a short time scale execution via Q-learning. The
authors of [8] designed a DRL model where each agent acts
according to heterogeneous users trafÔ¨Åc distribution. Authors
in [9] proposed a DRL model with Ô¨Åltered actions to optimize
the sum-rate of moving users while considering interference.
The authors of [10] and [11] proposed a dual-clip Proximal
Policy Optimization (PPO) algorithm and an actor-critic DRL
framework, respectively, to optimize MAP placement and user
association at the same time. These solutions deal with a
small evolution of scenarios and not all dynamic constraints
at the same time, as the function becomes non-trivial for large
parameter sets. To handle time-varying scenarios, they need to
be re-trained, which makes it difÔ¨Åcult to apply them to highly
dynamic cases without much complexity.
To handle the large amount of data and factors, a good
solution is decentralized RL. Each agent learns a reduced local
problem, decreasing all possible combinations and increasing
the number of supported scenarios. Since each agent makes
an autonomous decision and computes its own local observa-
tion, the model has a higher training diversity with reduced
complexity to achieve self-management. The authors of [12]
and [13] proposed a Multi-Agent DRL (MADRL) model with
an additional target neural network to stabilize it. Authors in
[14] proposed a hybrid solution to optimize user association
and MAP trajectory with static clustered users. With pre-
deployed MAPs on user clusters, the proposed decentralized
DRL is more likely to converge. However, the movement and
distribution to the cluster centers remain centralized and must
be recomputed after large changes or for each new scenario.
Our approach will exploit distributed DRL to handle time-
varying variables and free itself from centralized clustering by
including the deployment phase. In this case, agents must learn
to cooperate to achieve near-optimal solutions. The distributed
cooperation will be achieved through the attention mechanism.
It allows the model to build its own representation of the
input data to handle non-stationary scenarios. It creates a
comprehensive context to encourage cooperation and transmit
complex messages [15].arXiv:2303.05233v2  [cs.NI]  14 Mar 2023In this paper, we formulate the placement problem to
manage MAPs over time. Our solution tackles time-varying
user trafÔ¨Åc and user mobility through a multi-agent DRL.
To achieve real-time behavior, the proposed solution learns
to perform a distributed MAP-user positions assignment and
schedules the path of MAPs among all users without central-
ized clustering feedback. Our solution exploits a dual-attention
multi-agent DRL model via proximal policy optimization to
dynamically move MAPs in 3D. The dual-attention takes into
account both users and MAPs information. The cooperation
mechanism of our solution allows to manage different sce-
narios, without a priory information and without re-training,
which greatly reduces the complexity. The paper is organized
as follows. Section II presents the system model and formu-
lates the addressed problem. Then, Section III describes our
proposed solution, whereas Section IV provides our numerical
results. Finally, Section V concludes the paper.
II. S YSTEM MODEL & P ROBLEM FORMULATION
A. System Model
We consider a downlink network composed of MMAPs
operating at mmWave frequency and a grounded sub-6GHz
Macro-Base-Station (MBS), jointly providing services to K(t)
UEs at time t. LetU(t) =f1;:::;K (t)gdenote the set
of user equipments (UE) and M=f1;:::;Mgthe set of
MAPs. In our system model, we assume that each UE is
equipped with two antennas and can communicate either with
the sub-6GHz MBS or a mmWave MAP. We assume that each
MAP coverage range is determined by its antenna aperture
angle#i. We assume that each UE is associated with the
Access Point (AP) providing the maximum signal to noise
ratio (called max-SNR algorithm). In addition, we assume that
the backhaul network interconnecting the MAPs with the core
network is fully provisioned ( i.e.has sufÔ¨Åcient capacity). Thus,
we do not optimize backhaul links. However, even with such
assumptions, MAP trajectory optimization, which is the focus
of this paper, is a crucial task to improve the spectral efÔ¨Åciency
of the network by dynamically adapting the location of MAPs
w.r.t. grounded UEs dynamics while limiting interference. In
this work, we aim at maximizing total network sum-rate. Let
Ri;j(t)be the rate experienced by UE jcommunicating with
its serving AP i, which is given by the Shannon capacity.
Ri;j(t) =Blog2(1 + SINR i;j(t)): (1)
Here,Bis the total system bandwidth and SINRi;j(t)is the
signal-to-interference-plus-noise ratio experienced by the link
i!j, which includes intra-cell and inter-cell interference. In
particular, the SINR is affected by the channel path losses,
which in turn vary according to several factors, including the
3D location `i(t)of MAPiand`j(t)of UEj.
We distinguish between ground-to-ground sub-6GHz path
loss and air-to-ground mmWave path loss. The air-to-ground
mmWave path loss depends on Line-of-Sight (LoS) conditions
and the euclidean distance di;j(t) =k`i(t) `j(t)kbetween
MAPiand UEjat timet[16].
PL(T)
i;j(t) =p(t)PL(Los)
i;j(t) + (1 p(t))PL(NLos)
i;j (t);(2)
MAP i
MAP k
œëk‚Ñìk(t)
‚Ñìj(t)Œ∏i,j(t)‚Ñìi(t)
ck(t)
N(UE)
i(t)UEj MBSTrajectoryFig. 1. System model with 2 moving MAPs, a Ô¨Åxed MBS and 7 UEs. The
MAPiis receiving message from UE jand MAPk.
wherepis the LoS probability. Here, PL(Los)
i;j , and PL(NLos)
i;j
are the LoS and NLoS path loss respectively, given by:
PL(l)
i;j(t) = 20log(4fcdi;j(t)
c) +l; l2fLos;NLosg(3)
wherefcis the carrier frequency, cis the light‚Äôs velocity,
lcaptures the large scale shadowing effect with a standard
deviationl. The LoS probability depends on the relative
elevation angle i;j(t)(in radians) between MAP iand UE
j(as shown in Figure 1):
p(t) =1
1 +e (i;j(t) ): (4)
Here, parameters anddepend on the radio environment
(e.g. buildings‚Äô height and Los condition) and are given in
[16].
The ground-to-ground sub-6GHz path-loss [17] is given as:
PL(T)
i;j(t) = 10log10(di;j(t)) ++ 10log10(fc) +l;(5)
where,anddepends on radio environment [17] and
Los condition, fcis the carrier frequency and lcaptures
the shadowing effect. The details of path loss parameters are
given in Table I.
B. Problem Formulation
LetDj(t)refer to the time-varying data demand of UE jat
timet. We usexi;j(t)to indicate whether UE jis associated
with APiat timet, in which case xi;j(t) = 1 , otherwise
xi;j(t) = 0 . In particular, since we assume max-SNR policy
for user association, xi;j(t)depends on the location `i(t)of
MAPiw.r.t. UEjlocation`j(t). Thus, the effective network
sum-rateR(t)reads as:
R(t) =X
i2MX
j2U(t)xi;j(t) min (Dj(t);Ri;j(t)): (6)
We now formulate the following problem to maximize the
long-term sum-rate subject to instantaneous constraints:max
	(t)lim
T!+11
TTX
t=1E[R(t)]; (P1)
s:t: xi;j(t)2f0;1g;8i2M;j2U(t);(C1)X
i2Mxi;j(t) = 1; 8j2U(t);(C2)
X
j2U(t)xi;j(t)P; 8i2M;(C3)
`i(t)2L R3; 8i2M;(C4)
k`i(t+ 1) `i(t)k`;8i2M;(C5)
where 	(t) =f`i(t);8igand the expectation in ( P1) is
taken w.r.t the random trafÔ¨Åc requests and channels realiza-
tion, whose statistics are unknown. For the MAPs mobility
management problem ( P1), the constraint ( C1) deÔ¨Ånesxi;j
as a binary variable. Eq. ( C2) guarantees that each UE is
associated with exactly one AP. If a UE is not connected to
a MAP, a connection to the MBS is guaranteed. Constraint
(C3) ensures that every MAP cannot accept more than P
connections simultaneously. Constraint ( C4) restricts MAPs
mobility within an operation zone deÔ¨Åned by LR3. This
restricted zone can for e.g.deÔ¨Åne the maximum and minimum
Ô¨Çying altitude. Finally, constraint ( C5) ensures that a MAP
cannot move more than a distance `at a time. In particular,
`can be Ô¨Åxed based on the maximum authorized MAP‚Äôs
Ô¨Çight speed. EfÔ¨Åciently solving Problem ( P1) is challenging.
Indeed, the optimal solution of this non-convex combinatorial
problem strongly depends on UEs mobility, the dynamics of
UEs trafÔ¨Åc demands and channel variations. As we jointly
optimize the trajectories of multiple MAPs, solutions based
on a centralized exhaustive search are unfeasible in practice.
To solve this issue, we propose a model-free approach based
on MADRL.
III. P ROPOSED SOLUTION VIA DISTRIBUTED MADRL
Our proposed solution is based on MADRL and models
each MAP as an RL agent, which learns to make autonomous
decisions based only on local observations and some messages
received from its neighboring UEs. Agents Ô¨Årst autonomously
assign themselves a set (cluster) of UEs based on a common
ground obtained by a message passing between the MAPs.
Then, each agent learns its optimal trajectory, successively
deciding its optimal location over time. To do so, at each
instant, an agent can make an action from a predeÔ¨Åned
setA=fforward, backward, up, down, left, right, hover g,
corresponding to a movement along the xyz-axis, with a
Ô¨Åxed step-size `. Our proposed solution is distributed and
speciÔ¨Åcally addresses three challenges: i) a model-free ap-
proach, which does not require a priori information about the
radio environment, channel statistics and UE data demands; ii)
efÔ¨Åcient representation of agents state observations and design
of reward signals to effectively establish a common ground
between agents iii) Ô¨Çexibility to support size-varying networks,
including changes in topology, number and positions of UEs.
A. Background on MADRL
In a fully observable environment, the decision making
process of an agent ican be formalized as a Markov De-cision Process (MDP). Formally, a MDP can be deÔ¨Åned as
a tuple (S;A;T;R)in whichSis the true state space, A
denotes the action space, T(si(t);ai(t);si(t+1)) =P(si(t+
1)jsi(t);ai(t))is the probability of transitioning to state
si(t+1) after making action ai(t)in statesi(t), which results
in an immediate reward ri(t) =R(si(t);ai(t)). The problem
for agentiin a MDP is to Ô¨Ånd an optimal policy i(t) :
S !A that maximizes the expected sum of perceived ( -
discounted) rewards E[PTe
=t tri()]over a time horizon
Te, where2[0;1). In MADRL, such policy is modeled as
a neural network (NN) and is learned by the interaction of
several agents with a shared environment. Major challenges
appear in this context: the non stationarity of the environment
due to the simultaneous interactions of agents. In addition, in
our work, an agent has access to only a partial observation of
its true state, which is either unknown or difÔ¨Åcult to obtain
through computation or signals. To efÔ¨Åciently represent agent
states with limited complexity, we propose a novel approach
based on neural attention mechanism [18].
B. Dual-Attention Mechanism for Effective Representation
Learning
To better represent the true states of the agents, we al-
low information exchange between communicating entities.
SpeciÔ¨Åcally, MAP ican collect information about the locations
of neighboring MAPs (deÔ¨Åned as N(MAP)
i (t)) and neighboring
UEs (deÔ¨Åned asN(UE)
i(t)), where card
N(MAP)
i (t)
< M
andcard
N(UE)
i(t)
< K to limit the complexity of the
information exchange. Then, the agent ilearns its actual
state representation by encoding the received messages using
amessage encoder . However, in this dynamic environment
where the number and position of UEs may change over time,
the size ofN(r)
i(t);8r2fUE;MAPgand the order of mes-
sages may vary accordingly. In this context, the architecture
of the message encoder must not only be invariant to the
varying size of the neighborhood, but also to the permutations
of the observed messages. To solve this problem, we adopt
the idea from neural attention mechanism [18] (see Figure
2). SpeciÔ¨Åcally,8j2 N(r)
i(t), agentideÔ¨Ånes k(r)
i;j(t) =
w(r)
i;k(`i(t) `j(t))T2Rn,v(r)
i;j(t) =w(r)
i;v(`i(t) `j(t))T2
Rn, referred to as the relative keyandvalue associated with
message of entity j. In addition, agent icomputes its query
vector q(r)
i(t) = w(r)
i;q`i(t)T2Rn. Here, the encoding
matrices wi;k;wi;v;wi;q2Rn3are learnable parameters
(e.g., a hidden layer of dimension n). Then, the value of the
messages are aggregated independently to compute agent i
state representation (r)
i2Rn:
(r)
i(t) =X
j2N(r)
i(t)i;j(t)v(r)
i;j(t); (7)
wherei;j(t)deÔ¨Ånes the interaction vector of agent iw.r.t. en-
tityjand reads as follows:
i;j(t) = softmax0
B@2
4q(r)
ik(r)
i;pT
pn3
5
p2N(r)
i(t)1
CA
j: (8)Fig. 2. Dual-Attention-PPO architecture. Agent icollects message from its
neighborhoodN(r)
i.
Here, softmax is the normalized exponential function. By
construction, the size of (r)
iis constant and equal to nand
does not vary with the size of the neighborhood N(r)
i(t)
as desired. In our solution, (MAP)
i captures the relative
perception of MAP iw.r.t. to others neighboring MAPs. In
contrast,(UE)
i captures the relative perception of MAP i
w.r.t. to neighboring UEs. Next, as shown in Figure 2, we
combine these two dual representations to serve as input to an
actor-critic framework, which we optimize in an end-to-end
manner using the well-known proximal policy optimization
(PPO). SpeciÔ¨Åcally, we minimize the (1;2)-clipped proximal
loss proposed in our previous work [19].
C. Designing Effective Reward Function
To effectively learn a common ground between agent, we
adopt a hierarchical approach for designing the reward signal.
Our goal is Ô¨Årst to maximize user coverage, then maximize
network sum-rate. To do so, during the training phase, each
agentilearns to maximize the following reward function:
ri(t) = (i(t) 1)di(t) +i(t)(R(t) d0): (9)
Here,i(t) =1(di(t)d0), whered0is a reference distance
anddi(t) =k`i(t) `
i(t)kis the distance of MAP ito
its optimal location `
i(t)(w.r.t. (P1)). Since this location is
not known a priori, we approximate it during the training
phase with the location of the nearest assigned centroid ci(t)
obtained after clustering UEs using e.g. Kmeans algorithm
(Algorithm 1). In particular, we obtain the z-coordinate of
centroid by computing the minimal altitude at which a MAP
with an aperture angle of #iwould cover the UE cluster. In
this way, we push agent ito Ô¨Årst maximize user coverage
(Ô¨Årst term in Equation 9) and then network sum-rate (second
term in Equation 9). It is worth noting that the clustering
operation only serves during the training phase for learning a
common ground . In contrast, no central coordinator is required
during testing and MAPs autonomously coordinate themselves
to optimize (P1) (Algorithm 2).
Remark 1. In practice, we normalize di(t)by a predeÔ¨Åned
maximum distance and R(t)by to its average value (w.r.t. en-
vironment randomness). Also, instead of the step function
i(t), we use a smooth Gompertz function (see Figure 3), a
generalized logistic function i(t) = 1 0:9e e 0:06(di(t) d0).Algorithm 1: Training algorithm
Input: Init model weights w, environment state S
1forrun= 1:::run do
2 Randomly deploy agents in the cell
3 Agents receive UE centroids location ci(0)
4 Agents compute their neighborhood N(r)
i(0)
5 Agents gather messages from N(r)
i(0)
6 Agents computes their state representation (r)
i(0)
7 fort= 1:::T edo
8 Agents select and execute ai(t)
9 Agents receive their nearest assigned centroid ci(t)
10 Agents receive rewards ri(t)
11 Update the environment state S
12 Agents compute their neighborhood N(r)
i(t)
13 Agents gather messages from N(r)
i(t)
14 Agents compute their state representation (r)
i(t)
15 end
16 Compute (1;2)-clipped proximal loss
17 Performs gradient descent step with Adam
18 Update policies i;8i2 M
19end
Output: policiesi
Algorithm 2: Testing algorithm
Input: Load model weights w.
Trained agent policies i
1forrun= 1:::run do
2 fort= 1:::T edo
3 Agents computes their neighborhood N(r)
i(t)
4 Agents gather messages from N(r)
i(t)
5 Agents computes their state representation (r)
i(t)
6 Agents select and execute ai(t)
7 Update the environment state S
8 ComputeR(t)
9 end
10end
Output: E[R(t)]
IV. N UMERICAL RESULTS
We perform the training of our proposed MADRL algorithm
for10000 Monte-Carlo runs. For each run, we deploy M= 3
MAPs moving with a step size `= 5 m ; we also deploy K=
25UEs inMcentroids of radius 25 m , randomly sampled in a
200 m by200 m area. During the training phase, the UEs are
static forTe= 300 iterations while they follow a random way-
point centroid mobility at 0:8m=sduring the testing phase. As
the model has not been trained with speciÔ¨Åc mobility model,
it is able to support time-varying constraints. We set the agent
maximum neighborhood size max
card
N(UE)
i(t)
= 15
andmax
card
N(MAP)
i (t)
= 3 composed of nearest
entities. The UEs trafÔ¨Åc follows a Poisson distribution of
parameterk= 1000 Mbps . We consider a Nakagami fast-
fading model of parameter = 1 for each channel and other
channel parameters are given in Table I. We train each model
with a learning rate equals to 1e 4and= 0:6. We set
PPO-clips (1;2) = (0:01;0:5), and compose the message
encoders with one multi-layer perceptron (MLP) of n= 128
neurons. The actor and critic comprises also one MLP of 2n
neurons.TABLE I
SIMULATION PARAMETERS
Channel Parameters MBS MAP
Carrier Frequency fc 2 GHz 28 GHz
BandwidthB 10 MHz 500 MHz
Thermal Noise N0 174 dBm=Hz
Shadowing Variance 2
l3 dB 12 dB
Antenna Gain 17 dBi Directive [19]
Antenna Aperture Angle 180 90
LoS Path Loss Parameter = 2= 10:37
= 31:4= 0:05
= 2:1
NLoS Path Loss Parameter = 3:5= 35:85
= 24:4= 0:04
= 1:9
Benchmarks. We deÔ¨Åne two benchmark solutions. The Ô¨Årst
benchmark (referred to as Centralized Ben. ) pre-computes the
centroids of UE clusters using a centralized Kmeans algorithm.
Then, each centroid is assigned to the closest MAP, whose
trajectory is planned using Dijkstra‚Äôs algorithm. The second
benchmark (referred to as SA-PPO ) is similar to our proposed
solution, in which we employ a single attention mechanism
w.r.t. UEs without any cooperation between MAPs.
Learning convergence . We Ô¨Årst compare the rewards of SA-
PPO and our proposed solution. As shown in Figure 3, both
models converge for the complex ( P1) problem. Indeed, each
method obtains a positive reward for all agents, which means
that each agent achieves a placement at a distance less than
d0to the centroid center (Equation 9). Especially, our solution
ends with a higher reward for each agent compared to the
SA-PPO training. The difference comes from the lack of
explicit cooperation among SA-PPO agents, resulting in a
lower reward. Indeed, if two MAPs end up serving the same
set of UEs, the sum-rate, which is a global performance,
drops sharply. In contrast, by using our proposed dual-attention
mechanism, agents converge to the same behavior and coop-
eratively distribute MAPs to UE clusters.
Fig. 3. Proposed solution and single-attention training rewards ri(t)for each
agent and associated step and Gompertz functions i().
Distributed cooperation . In this step, we validate that our
solution is able to deploy and place agents with Static and
Moving UEs thanks to cooperation and we compare our solu-
tion with the centralized benchmark for a c= 200 . UE trafÔ¨Åc
requests are always dynamic and moving MAPs introduce
Ô¨Çuctuating level of interference. For the Static scenario, Figure
4 shows that our model matches the sum-rate performance of
the centralized approach. Therefore, our solution achieves afast location negotiation explained by the 10 iterations latency
on the deployment phase. The Ô¨Årst iterations are used to
exchange messages to discover and allocate agents to the
negotiated positions. Furthermore, it proves that our model
is able to compute and focus a virtual point via message
exchange, which leads to a distributed clustering behavior.
Concerning the Moving scenario, our solution renegotiates its
positions to follow network dynamics robustly. Figure 4 shows
the importance of not considering only the MAP deployment
phase, due to the change of network during MAPs time of
Ô¨Çight, preventing a performance drop. It is now important to
extend the time window to ensure that our model does not
also suffer from a performance drop.
Fig. 4. Instantaneous sum-rate comparison between our solution and the
benchmark for static and mobile UEs over time. Results are averaged over
50random deployments.
Robustness to dynamic networks . In this step, we investigate
how the centralized benchmark and our solution support highly
dynamic networks with or without centralized user clustering
feedback for a long time period ( t= [0;1000] ). Figure 5 shows
that the centralized benchmark is attractive when the feedback
on clustering occurs every time slot ( c= 1). This comes
with a very high complexity. When the feedback becomes less
frequent (c= 200 ), the benchmark is not robust to network
dynamics and the performance drops after 30time slots. This
is due to the fact that the clustering information is outdated
because of major changes in the network. Figure 5 shows
that our solution is able to guarantee a sum-rate and load
level without any drop when the entire network conÔ¨Åguration
changes several times and then achieve real-time behavior.
Dual-attention agents trade 20% of the network loadLi(t),
representing the proportion of users connected to a MAP,
against outlier UEs with isolated behavior, with a guaranteed
expected sum-rate. At this time scale, the few iterations used
to the beginning of the negotiation become negligible and this
scenario demonstrates the importance of the parameter c.
Trade-off on clustering . Figure 6 compares our solution with
the centralized benchmark for different clustering periods c.
The benchmark performance strongly depends on cwhich
deÔ¨Ånes the age of information. In contrast, our solution does
not depend on clustering updates coming from a centralFig. 5. Instantaneous network sum-rate evolution over time with c=
f1;200g. Results are averaged over 50random deployments.
coordinator. The centralized approach must cluster the UEs
everyc= 10 time slots to match our solution.
Fig. 6. Average sum-rate Rfor our solution and the centralized benchmark
for different cclustering periods.
V. C ONCLUSION
In this paper, we study the optimization of multi-MAP
3D trajectory for dynamic 5G networks. To this end, we ad-
dressed the mobility management of MAPs under time-varying
user trafÔ¨Åc, user mobility and interference. We proposed a
dual-attention MADRL solution capable of self-managing a
Ô¨Çexible infrastructure using cooperative MAPs. The proposed
solution learns to perform distributed assignment of MAP-
user positions and schedules the MAP path among all users
without centralized user clustering feedback. Agents converge
to the same behavior and cooperatively distribute MAPs to UE
clusters. The cooperation mechanism also allows to manage
different scenarios, without a priory information and without
re-training, which signiÔ¨Åcantly reduces complexity. Our so-
lution does not depend on clustering updates coming from
a central coordinator and is robust to network dynamics. In
future work, the solution will be extended to handle more
dynamic parameters and imperfections such as the backhaul
connectivity or imperfect beamforming. Moreover, our so-
lution will include additional metrics such as energy and
deployment cost.ACKNOWLEDGMENT
This work was supported by the European Union H2020
Project DEDICAT 6G under grant no. 101016499. The con-
tents of this publication are the sole responsibility of the
authors and do not in any way reÔ¨Çect the views of the EU.
REFERENCES
[1] M. Maman, E. Catte, M. Sana, M. Girmay, V . Maglogiannis, D. Naudts,
H. Lee, F. Carrez, A. Anttonen, Y . Fernandez, J. Moreno, V . Lamprousi,
and V . Stavroulaki, ‚ÄúCoverage Extension as a Service for Flexible
6G Networks Infrastructure,‚Äù in 2022 IEEE Globecom Workshops (GC
Wkshps) , pp. 1329‚Äì1334, 2022.
[2] E. Catt ¬¥e, M. Sana, and M. Maman, ‚ÄúCost-EfÔ¨Åcient and QoS-Aware User
Association and 3D Placement of 6G Aerial Mobile Access Points,‚Äù in
2022 Joint European Conference on Networks and Communications &
6G Summit (EuCNC/6G Summit) , pp. 357‚Äì362, 2022.
[3] A. Alsharoa, H. Ghazzai, M. Yuksel, A. Kadri, and A. E. Kamal,
‚ÄúTrajectory Optimization for Multiple UA Vs Acting as Wireless Relays,‚Äù
in2018 IEEE International Conference on Communications Workshops
(ICC Workshops) , pp. 1‚Äì6, 2018.
[4] Q. Wu, Y . Zeng, and R. Zhang, ‚ÄúJoint Trajectory and Communication
Design for Multi-UA V Enabled Wireless Networks,‚Äù IEEE Transactions
on Wireless Communications , vol. 17, no. 3, pp. 2109‚Äì2121, 2018.
[5] Y . Pan, X. Da, H. Hu, Y . Huang, M. Zhang, K. Cumanan, and
O. A. Dobre, ‚ÄúJoint Optimization of Trajectory and Resource Allocation
for Time-Constrained UA V-Enabled Cognitive Radio Networks,‚Äù IEEE
Transactions on Vehicular Technology , vol. 71, no. 5, 2022.
[6] R. Ghanavi, E. Kalantari, M. Sabbaghian, H. Yanikomeroglu, and
A. Yongacoglu, ‚ÄúEfÔ¨Åcient 3D aerial base station placement considering
users mobility by reinforcement learning,‚Äù in 2018 IEEE Wireless
Communications and Networking Conference (WCNC) , pp. 1‚Äì6, 2018.
[7] X. Liu, Y . Liu, and Y . Chen, ‚ÄúReinforcement Learning in Multiple-UA V
Networks: Deployment and Movement Design,‚Äù IEEE Transactions on
Vehicular Technology , vol. 68, no. 8, pp. 8036‚Äì8049, 2019.
[8] V . Saxena, J. Jald ¬¥en, and H. Klessig, ‚ÄúOptimal UA V Base Station Tra-
jectories Using Flow-Level Models for Reinforcement Learning,‚Äù IEEE
Transactions on Cognitive Communications and Networking , vol. 5,
no. 4, pp. 1101‚Äì1112, 2019.
[9] W. Zhang, Q. Wang, X. Liu, Y . Liu, and Y . Chen, ‚ÄúThree-Dimension
Trajectory Design for Multi-UA V Wireless Network With Deep Re-
inforcement Learning,‚Äù IEEE Transactions on Vehicular Technology ,
vol. 70, no. 1, pp. 600‚Äì612, 2021.
[10] J. Ji, K. Zhu, and L. Cai, ‚ÄúTrajectory and Communication Design for
Cache-Enabled UA Vs in Cellular Networks: A Deep Reinforcement
Learning Approach,‚Äù IEEE Transactions on Mobile Computing , 2022.
[11] R. Ding, F. Gao, and X. S. Shen, ‚Äú3D UA V Trajectory Design and
Frequency Band Allocation for Energy-EfÔ¨Åcient and Fair Communica-
tion: A Deep Reinforcement Learning Approach,‚Äù IEEE Transactions
on Wireless Communications , vol. 19, no. 12, pp. 7796‚Äì7809, 2020.
[12] N. Zhao, Z. Liu, and Y . Cheng, ‚ÄúMulti-Agent Deep Reinforcement
Learning for Trajectory Design and Power Allocation in Multi-UA V
Networks,‚Äù IEEE Access , vol. 8, pp. 139670‚Äì139679, 2020.
[13] Z. Qin, Z. Liu, G. Han, C. Lin, L. Guo, and L. Xie, ‚ÄúDistributed
UA V-BSs Trajectory Optimization for User-Level Fair Communication
Service With Multi-Agent Deep Reinforcement Learning,‚Äù IEEE Trans-
actions on Vehicular Technology , vol. 70, no. 12, 2021.
[14] S. Zhou, Y . Cheng, X. Lei, Q. Peng, J. Wang, and S. Li, ‚ÄúResource
Allocation in UA V-assisted Networks: A Clustering-Aided Reinforce-
ment Learning Approach,‚Äù IEEE Transactions on Vehicular Technology ,
pp. 1‚Äì16, 2022.
[15] A. Das, T. Gervet, J. Romoff, D. Batra, D. Parikh, M. G. Rabbat,
and J. Pineau, ‚ÄúTarmac: Targeted multi-agent communication,‚Äù CoRR ,
vol. abs/1810.11187, 2018.
[16] A. Al-Hourani, S. Kandeepan, and S. Lardner, ‚ÄúOptimal LAP Altitude
for Maximum Coverage,‚Äù IEEE Wireless Communications Letters , vol. 3,
no. 6, pp. 569‚Äì572, 2014.
[17] S. Sun, T. S. Rappaport, et al. , ‚ÄúPropagation Path Loss Models for 5G
Urban Micro- and Macro-Cellular Scenarios,‚Äù in Proc. IEEE Vehicular
Technology Conference (VTC Spring) , pp. 1‚Äì6, 2016.
[18] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
L. Kaiser, and I. Polosukhin, ‚ÄúAttention is all you need,‚Äù in Proceedings
of the 31st International Conference on Neural Information Processing
Systems , NIPS‚Äô17, p. 6000‚Äì6010, Curran Associates Inc., 2017.
[19] M. Sana, N. di Pietro, and E. Calvanese Strinati, ‚ÄúTransferable and
Distributed User Association Policies for 5G and Beyond Networks,‚Äù in
Proc. IEEE International Symposium on Personal, Indoor and Mobile
Radio Communications (PIMRC) , pp. 966‚Äì971, 2021.