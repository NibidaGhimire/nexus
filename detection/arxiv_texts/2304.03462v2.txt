Quantumness and Learning Performance
in Reservoir Computing with a Single Oscillator
Arsalan Motamedi,1,∗Hadi Zadeh-Haghighi,2, †and Christoph Simon2, ‡
1Institute for Quantum Computing, Department of Physics & Astronomy University of Waterloo, Waterloo, ON, N2L 3G1, Canada
2Department of Physics and Astronomy, Institute for Quantum Science and Technology,
Quantum Alberta, and Hotchkiss Brain Institute, University of Calgary, Calgary, AB T2N 1N4, Canada
(Dated: March 19, 2024)
We explore the power of reservoir computing with a single oscillator in learning time series using
quantum and classical models. We demonstrate that this scheme learns the Mackey–Glass (MG)
chaotic time series, a solution to a delay differential equation. Our results suggest that the quan-
tum nonlinear model is more effective in terms of learning performance compared to a classical
non-linear oscillator. We develop approaches for measuring the quantumness of the reservoir dur-
ing the process, proving that Lee-Jeong’s measure of macroscopicity is a non-classicality measure.
We note that the evaluation of the Lee-Jeong measure is computationally more efficient than the
Wigner negativity. Exploring the relationship between quantumness and performance by examining
a broad range of initial states and varying hyperparameters, we observe that quantumness in some
cases improves the learning performance. However, our investigation reveals that an indiscriminate
increase in quantumness does not consistently lead to improved outcomes, necessitating caution in
its application. We discuss this phenomenon and attempt to identify conditions under which a high
quantumness results in improved performance.
I. INTRODUCTION
The theory of quantum information processing has
been thriving over the past few decades, offering vari-
ous advantages, including efficient algorithms for break-
ing Rivest–Shamir–Adleman (RSA) encryption, exponential
query complexity speed-ups, improvement of sensors and ad-
vances in metrology, and the introduction of secure commu-
nication protocols [1–8]. Nevertheless, the challenge of er-
ror correction and fault-tolerant quantum computing is still
the biggest obstacle to the realization of a quantum com-
puter. Despite threshold theorems giving the hope of fault-
tolerant computation on quantum hardware [9–12], a success-
ful realization of such methods is only recently accomplished
on intermediate-size quantum computers [13], and the im-
plementation of a large scale quantum computer is yet to be
achieved. Moreover, today’s quantum hardware contain only
a few tens of qubits. Hence we are in the noisy intermediate-
scale quantum (NISQ) era, and it is of interest to know what
tasks could be performed by such limited noisy devices that
are hard to do with classical computers [14–17].
In the past few years, and on the classical computing side,
neuromorphic (brain-inspired) computing has shown promis-
ing results [18–21], most notably the celebrated artificial neu-
ral networks used in machine learning. Neuromorphic com-
puting uses a network of neurons to access a vast class of
parametrized non-linear functions. Despite being very suc-
cessful in accuracy, these models are hard to train due to
the need to optimize many parameters. Another obstacle in
the training of such models is the vanishing gradient problem
[22, 23].
∗arsalan.motamedi@uwaterloo.ca
†hadi.zadehhaghighi@ucalgary.ca
‡csimo@ucalgary.caA subfield of brain-inspired computing, derived from re-
current neural networks, is reservoir computing, where the
learning is to be performed only at the readout. Notably, this
simplification - optimizing over a smaller set of parameters -
allows for circumventing the problem of barren plateaus en-
countered in the training of recurrent neural networks. Despite
such simplifications, reservoir computing still shows remark-
able performance [24–29]. Reservoir computing methods are
often applied to temporal categorization, regression, and also
time series prediction [30, 31]. Notably, there have been suc-
cessful efforts on the physical implementation of (classical)
reservoir computing [26, 29, 32].
More recently, the usefulness of quantum computing in the
field of machine learning has been studied [33–35]. In addi-
tion to that, there are novel attempts to introduce an appropri-
ate quantum counterpart for classical neuromorphic (in par-
ticular reservoir) computing. There have been different reser-
voir models considered, which could mostly be categorized
as spin-based or oscillator-based [36–40] (corresponding to
finite and infinite dimensional Hilbert spaces).
On the quantum reservoir computing front, there have been
efforts such as [37], where one single Kerr oscillator is ex-
ploited for fundamental signal processing tasks. The approach
used in [40] for quantum reservoir computing introduces non-
linearity through the encoding of input signals in Gaussian
states. Their approach has been proven to be universal, mean-
ing that it can accurately approximate fading memory func-
tions with arbitrary precision. [41] predicts time series using
a spin-based model. [42] exploits a network of interacting
quantum reservoirs for tasks like quantum state preparation
and tomography. [43] proposes heuristic approaches for op-
timized coupling of two quantum reservoirs. An analysis of
the effect of quantumness is performed in [37], where the au-
thors consider dimensionality as a quantum resource. The ef-
fects of quantumness have been studied more concretely in
[44], where they consider an Ising model as their reservoirarXiv:2304.03462v2  [quant-ph]  16 Mar 20242
FIG. 1: A schematic representation of the computation model, either classical or
quantum. In the learning process, we find the proper Athat is to predict the sample set
G, based on the outputs of the reservoir. The dynamics of the reservoir is controlled via
sample set F.
and show that the dimension of the phase space used in the
computation is linked to the system’s entanglement. Also,
[41] demonstrates that quantum entanglement might enhance
reservoir computing. Specifically, they show that a quantum
model with a few strongly entangled qubits could perform as
well as a classical reservoir with thousands of perceptions, and
moreover, performance declines when the reservoir is decom-
posed into separable subsets of qubits.
In this work, we explore how well a single quantum non-
linear oscillator performs time series predictions. In partic-
ular, we are focused on the prediction of the Mackey-Glass
(MG) time series [45], which is often used as a benchmark
task in reservoir computing. We then investigate the role of
quantumness in the quantum learning model. We use Lee-
Jeong measure [46] as a measure of quantumness. This mea-
sure was originally introduced for macroscopicity, but here we
demonstrate that it is a non-classicality measure as well. We
highlight that Lee-Jeong measure can be computed efficiently,
as opposed to Wigner negativity. Using our approaches, we
observe that quantumness can enhance performance, mea-
sured in terms of test error. Nevertheless, more quantumness
does not always lead to improved performance; caution is nec-
essary. In particular, our results demonstrate that there exists
a pattern in the interplay of hyperparameters (with a proper
choice of initial state), indicating optimal performance within
specific parameter ranges, for which we provide an intuitive
reason. Moreover, it is the case that certain states consistently
yield poor learning performance across a broad spectrum of
hyperparameters.
The paper is organized as follows. In Section II we intro-
duce the reservoir computing method used in this work. Sec-
tion III A shows the performance of the method. Section III B
analyzes the effect of quantumness measures on performance.
Finally, Section IV provides a discussion of the findings.
II. RESERVOIR COMPUTING
In this work, we use the approach introduced by Govia et
al. [37] to feed the input signal to the reservoir by manip-
ulating the Hamiltonian. We describe the state of our quan-
tum and classical systems using ˆρanda, respectively. Let us
consider a time interval of length ∆tthat has been discretized
with Nequidistant points t1<···<tN. We can expandthis set by considering Mfuture values tN+1<···<tN+M,
which are also equidistantly distributed. It is worth noting
that the interval tN−t1is equal to ∆t. Our objective is to
estimate a set of future values of the function f, which is
denoted by G= (f(ti))M
i=N+1, given the recent observations
F= (f(ti))N
i=1. To this end, we evolve our system so that ˆρ(tj)
(respectively, a(tj)) depends on f(t1),···,f(tj). We then ob-
tain observations s(ti) =⟨ˆOˆρ(ti)⟩for an observable ˆO(respec-
tively, s(ti) = ( h◦a)tifor a function h). Finally, we perform
a linear regression on (s(ti))N
i=1to predict G. In what follows
we elaborate on the system’s evolution in both classical and
quantum cases.
For the classical reservoir, we consider the following evo-
lution
˙a=−iK(1+2|a|2)a−κ
2a−iαf(t) (1)
with K,κ, andαbeing the reservoir’s natural frequency, dissi-
pation rate, and the amplifier of the input signal, respectively.
We let s(t) =tanh(Re{a(t)}). The quantum counterpart of
the evolution described by Eq. (1) is the following Markovian
dynamics [37, 47]
d
dtˆρ(t) =−i[ˆH(t),ˆρ(t)]+κDa(ˆρ)
where ˆH(t) =KˆN2+αf(t)ˆX,
andDa(ˆρ) =ˆaˆρˆa†−{ˆN,ˆρ}/2.(2)
Here ˆ arefers to the annihilation operator and is related to
ˆXand ˆNthrough ˆX=ˆa+ˆa†and ˆN=ˆa†ˆa. Note that we
use the properly scaled parameters, such that the uncertainty
principle becomes ∆x∆p≥1/2 (we have employed the nota-
tion∆A=⟨(ˆA−⟨ˆA⟩)2⟩for an observable ˆA). The parameters
(α,κ,K)are the same as above (in (1)). The operators ˆX, ˆa,
and ˆNrepresent the position, annihilation, and the number op-
erator respectively. We let s(t) =tr ˆρ(t)tanh ˆX
. Utilizing the
non-linear quantum evolution (2) as our quantum reservoir,
we perform the learning of the MG time series. Throughout,
in time series prediction, both fandgrefer to the same series,
that is generally denoted by x.
Let us now provide details on the process of linear regres-
sion. Our objective is to find the predictor Athat satisfies the
relationship G≈As(note that we think of Gandsas column
vectors). To this end, we conduct the experiment Ttimes and
collect the resulting column vectors as− →s1,− →s2,···,− →sT. We
then define a matrix Sas the concatenation of these column
vectors
S:= s1|s2| ···| sT
(3)
Similarly, we define the matrix Gas
G:= G1|G2| ··· | GT
. (4)
Finally, we choose Aby applying Tikhonov regularization
[48], which results in the following choice of A
A=GST(SST+ηI)−1(5)3
0 20 40 60 80 100 120
timestep/10-0.6-0.4-0.200.20.40.6x;^xProduced	by	reservoir
Actual	test	data
Separating	train	from	test	data
(a) The oscillator is trained to reproduce the Mackey-Glass time series,
and then predict, given initial values, which are the data points before
the dashed line.
-0.5 0 0.5
x[n!17]-0.5-0.4-0.3-0.2-0.100.10.20.30.4x[n]
-0.5 0 0.5
^x[n!17]-0.6-0.5-0.4-0.3-0.2-0.100.10.20.30.4^x[n](b) Comparing the delayed embedding of the actual Mackey-Glass series
(left) with that of the reservoir’s output (right). See Section III A for
definitions.
FIG. 2: Performance of the trained quantum model. Here ndenotes the time step or equivalently the index of the sampled point from the function, xand ˆxrefer to the actual and the
predicted series, respectively.
withηandIbeing a regularization parameter, and the identity
matrix, respectively. One should note that GandSwritten
in (5) correspond to Ttraining samples that we take. The
matrix Aevaluated above is then used for the prediction of the
test data. Fig. 1 provides a schematic representation of the
reservoir training.
Overall, to predict time series, the reservoir is initially
trained to determine Aby using equation (5). After training, a
set of initial values outside of the training data is inputted into
the oscillator. The oscillator uses Ato predict future values,
which are then used as initial values for further predictions.
III. RESULTS
This section investigates the performance of a single Kerr
non-linear oscillator trained on the MG chaotic time series,
as well as the effect of quantumness on the performance of
the reservoir. Specifically, in Section III A, we discuss how
well the non-linear oscillator learns chaotic time series, and in
Section III B, we examine the impact quantumness on the per-
formance. Lastly, we outline further investigations, including
the effects of noise.
To simulate quantum dynamics in the Fock space, we trun-
cate every operator in the number basis, making them dt-
dimensional. Notably, the simulation results in this work use
a dimension dt≥20, which is sufficiently large as most of the
states considered have a significant overlap with the subspace
spanned by the number states |n⟩forn≤10 (For instance, we
use the coherent state |α⟩withα=1+i, the overlap of which
with the first 20 Fock states is larger than 1 −6.5×10−15).
A. Learning Time Series
In what follows we report the results obtained by training
our single non-linear oscillator. Here, we consider the predic-tion of the chaotic MG series, which is formally defined as the
solution to the following delay differential equation
˙x(t) =βx(t−τ)
1+x(t−τ)m−γx(t). (6)
We use the parameters β=0.2,γ=0.1,m=10, and τ=17
throughout. In Section III of our Supplementary Material, we
discuss that τcontrols the complexity of the series, and we re-
peat the training for different values of τ. We demonstrate that
learning a series with a larger τis still feasible if we increase
the input length N(as defined in Section II). The performance
of the trained reservoir on the test data is presented in Fig. 2a.
Fig. 2b shows the delayed embedding of the predicted MG,
which is compared to the actual diagram. One can readily
observe that this model is successful in learning MG. We em-
phasize that we are using the units in which h=2π. We then
integrate the evolution of the reservoir with time step ∆t=0.1
in that unit (this is identical to ∆tintroduced in Section II).
Moreover, using (6), we generated the Mackey–Glass series,
and sampled it at the integer points in time t. This provides us
with a discrete set of data that is then fed to the reservoir.
Table I summarizes the parameters employed in the training
of the model, by which we generated Fig. 2a and Fig. 2b.
B. Quantumness
In this section, we introduce our quantumness measure and
study the effect of quantumness on that basis on the accuracy
of the learning model.
Our main goal here is to determine if there is a relation
between the quantumness of the system and the accuracy of
the learning process. We aim to explore whether quantum-
ness can serve as a valuable resource for reservoir computing.
To this end, we need to quantify the quantumness of a state
in Fock space. We point out that there has been extensive
research done on the quantification of quantumness [49–51].4
TABLE I: Learning parameters and values that are used in obtaining Fig. 2a and
Fig. 2b. Please refer to Section II for a detailed explanation of these parameters. The
units of the parameters is consistent with the formulation of (2).
Parameter Description Value
K non-linearity strength 0 .05
κ dissipation rate 0 .1
α input coefficient 1 .2
N input length 200
M output length 100
∆t time step 0 .1
η regularization parameter 0 .01
|ψinit⟩ initial state of reservoir |6⟩
T number of training rounds 248
50 0.1 0.2 0.3 0.4 0.5 0.6 0.7Q u a n t u m n e s s ( Q )
00.050.10.150.20.250.30.350.40.45
ket 6
Cat
Mixed
Coherent
FIG. 3: Average quantumness ( Q) during the evolution is shown to be decreasing as κ
(the photon loss rate) increases. The states used as initial states are the ‘Coherent’ state
|α⟩, the mixed state (labeled as ‘mix’) being proportional to |α⟩⟨α|+|−α⟩⟨−α|, the
corresponding ‘cat’ state being proportional to |α⟩+|−α⟩, and the |6⟩(‘ket 6’). The
parameters are (α,λ,K) = ( 1.2,0.1,0.05), where λis the pumping parameters, used to
keep all the quantumness values in a similar range. We use α=1+ifor the coherent,
the mixed, and the cat states. Please refer to Section II of the Supplementary Material
for the definition of the noise model corresponding to the parameter λ.
Furthermore, there has been a line of research in the study of
the macroscopicity of quantum states and their effective size
[52–55]. One such measure in the Fock space is defined by
Lee and Jeong [46] as follows:
I(ˆρ):=πZ 
∂xW(x,p)2
+ 
∂pW(x,p)2−2W(x,p)2
dxdp(7)
where x,pare dimensionless space and momentum com-
ponents (in such scale, the uncertainty principle becomes
∆x∆p≥1/2). Here W(x,p)refers to the Wigner function
W(x,p) =1
πZ∞
−∞⟨x−y
2|ˆρ|x+y
2⟩eipydy. (8)
The above formulation of I(ˆρ)can also be found in [56]. The
following identities are pointed out in [46]:•I(|α⟩⟨α|) =0, for any coherent state |α⟩.
•∀n∈N:I(∑n−1
i=01
n|i⟩⟨i|) =0, where |i⟩are the Fock states
(i.e., the eigen-vectors of the number operator).
It is worth mentioning that this measure can obtain negative
values [56]. Intuitively, one could think of I(ˆρ)as the fineness
of the Wigner function associated with ˆρ. The aforementioned
results may suggest that the positiveness of Iindicates non-
classicality, as the coherent state and a diagonal density matrix
in the Fock basis are considered as classical states. In the
following theorem, we prove that if I(ˆρ)>0, then ˆρcannot
be written as a mixture of coherent states, hence being non-
classical.
In what follows, we prove that if ˆρis a mixture of coherent
states, then I(ˆρ)≤0. For any coherent state |α⟩, one has
Wα=1
πe−
(x−Re(α))2+(p−Im(α))2
.
Let us consider a set of Kcoherent states, namely {ˆρi=
|αi⟩⟨αi|:i=1,2,···,K}, and define xi:=Re(αi),pi:=
Im(αi). Also let (qi)i∈[K]be a probability distribution over K
objects. We can then consider the mixture of coherent states
as
ˆρ=K
∑
i=1qi|αi⟩⟨αi|
since the Wigner function is linear with respect to the density
matrix, one has Wˆρ(x,p) =∑K
i=1qiWαi(x,p).Hence, we get
1
πI(ˆρ)
=Z 
∑
i∈[K]qi∂xWi!2
+ 
∑
i∈[K]qi∂pWi!2
−2 
∑
i∈[K]qiWi!2
dxdp
=∑
i,j∈[k]qiqjZ
∂xWi∂xWj+∂pWi∂pWj−2WiWj
dxdp
note that the terms in the summation above with i=jcould
be rewritten as q2
iI(ˆρi) =0 since any cohrent state ˆρihas zero
quantumness i.e., I(ˆρi) =0. Furthermore, our explanation be-
low guarantees that for any choice of i,jthe expression in the
parenthesis is non-positive, and hence, the proof is complete.
All that is left, is to prove that for any two coherent states,
say|α0⟩and|α1⟩, both of the following inequalities hold
Z
∂xW0∂xW1−W0W1
dxdp≤0,
Z
∂pW0∂pW1−W0W1
dxdp≤0(9)
We start by writing
W0=1
πe−
(x−x0)2+(p−p0)25
hence
∂xW0=−2(x−x0)
πe−
(x−x0)2+(p−p0)2
,
∂pW0=−2(p−p0)
πe−
(x−x0)2+(p−p0)2
,
and similar expressions for W1and its derivatives. Let us now
prove the first inequality. Define
A:=Z
∂xW0∂xW1−W0W1
dxdp
then, by direct substitution one gets
A=1
π2Z 
4(x−x0)(x−x1)−1
×e−
(x−x0)2+(x−x1)2+(p−p0)2+(p−p1)2
dxdp,
we may now use the elementary identities (x−x0)(x−
x1) = 
x−x0+x1
22− x0−x1
22and(x−x0)2+ (x−x1)2= 
x−x0+x1
22+ x0−x1
22and further, letting ∆x:=x0−x1,
∆p=p0−p1, and x:=x0+x1
2, and p=p0+p1
2to conclude
A=e−1
2(∆x2+∆p2)
π2Z
4 
x−x2−(∆x)2−1
e−2(x−x)2−2(p−p)2
=−e−1
2(∆x2+∆p2)
2π(∆x)2≤0,
where the last equality follows from elementary Gaussian in-
tegrals. A similar argument gives the second inequality of (9).
One should also note that Iis computable in a much shorter
time since it can be reformulated as ([46])
I(ˆρ) =−tr(ˆρDa(ˆρ)). (10)
On the other hand, the computation of Wigner negativity with
the current algorithms is costly, as it requires the computation
of the entire Wigner function. We hence use the following
quantumness measure Q
Q(ˆρ) =(
I(ˆρ)ifI(ˆρ)>0,
0 o.w.(11)
We make this choice as we do not want our quantumness mea-
sure to obtain negative values. We observe that this measure
is consistent with some intuitions regarding the quantumness
of reservoir computing, which have been previously used in
[37]. In particular, the intuition that by increasing κwe should
reach a classical limit, which is illustrated in Fig. 3. Finally,
we point out the fact that the quantumness of the state changes
during evolution. This is indeed observed in Fig. 4.
C. Exploring parameter space
The set of hyperparameters define the dynamics of an os-
cillator through (2), and therefore, the quantumness exhibited
(a) Two snapshots of Wigner functions of the reservoir’s state, starting
from different states. The plots correspond to the initial state being the
mixed state (i.e. proportional to |α⟩⟨α|+|−α⟩⟨−α|), cat state (i.e.
proportional to |α⟩+|−α⟩), a coherent state |α⟩, and the number state
|6⟩. Here, Xrefers to position and Prefers to momentum (see (8) for the
definition).
t ( s )05101520Q u a n t u m n e s s ( Q )
0123456
t ( s )05101520W i g n e r N e g a t i v i t y ( W )
050100150200250300350400450500
coherent
mixed
cat
ket6
(b) Quantumness (Q) during the evolution. The label ’mix’ corresponds
to the case where the initial state is |α⟩⟨α|+|−α⟩⟨−α|, the label ’cat’
corresponds to |α⟩+|−α⟩, and the label ’ket6’ corresponds to |6⟩(Note
that the correspondences are up to a normalization factor). The values
are normalized by the absolute maximum in each diagram.
FIG. 4: Quantumness and Wigner plots of the reservoir’s state evolution. The oscillator
parameters are (α,κ,K) = ( 1.2,0.1,0.05).
by the model depends on these parameters. As previously dis-
cussed (e.g. see Fig. 3), a large value for κyields a classi-
cal oscillator, while high quantumness is expected when κis
significantly smaller than K. It is worth noting that αin (2)
dictates the responsiveness to input data. In this section, we
maintain αat a constant value of 1 .2, consistent with the ex-
periments conducted in preceding sections, while exploring6
-1 0 1 2 3 4 5 6
log(Quantumness)0.0550.060.0650.070.0750.080.0850.090.095TestErrorRandomStateTaining
FIG. 5: All data points obtained from the task of Section III C. Each blue point
corresponds to the outcome of the training on a particular state with a particular set of
reservoir parameters (K,κ). The solid red and green curves are piece-wise linear
estimators to the worst and best achieved test error at each quantumness value,
respectively. The solid black shows the average test error at each quantumness value.
We have employed the natural logarithm in the scaling of the horizontal axis.
the impact of other parameters ( Kandκ) on the learning pro-
cess.
The simulation methodology is structured as fol-
lows: We select pairs of (K,κ)where Ktakes val-
ues from {0.02,0.05,0.07,0.1,0.12}and κ from
{0.02,0.03,0.05,0.1,0.2,0.3}. For each pair, the oscil-
lator is initialized at a state from a collection of 35 random
states, one at a time. It is important to note that the same set
of random states is utilized across all pairs of (K,κ). For the
initial random states, we fix a dimension d(which determines
its support on the Fock basis), then pick a state according to
the Haar random measure on H(Cd)[57]. To this end, we
use [58, Proposition 7.17]. In particular, we consider a set of
2dindependent and identically distributed (i.i.d.) standard
Gaussian random variables ζ1,···,ζ2dto construct the state
|ψ⟩=1q
∑2d−1
m=0ζ2md−1
∑
n=0(ζ2n+iζ2n+1)|n⟩ (12)
where |n⟩are the Fock basis states. We highlight that due to
[58, Proposition 7.17], the state |ψ⟩is a Haar random state in
H(Cd), meaning that its distribution is invariant under the ac-
tion of the unitaries acting on H(Cd). Through this method,
we generate 5 random states for each dimension d, ranging
from 4 to 10, resulting in a total of 35 states.
The described approach yields the results presented in
Fig. 5. Furthermore, Table II provides comprehensive de-
tails regarding this simulation. Our first observation is that
quantumness has the potential to improve performance, as
measured by test error. However, an increase in quantum-
ness does not guarantee a corresponding improvement in per-
formance. In addition to the quantumness-performance rela-
tionship, there are other aspects to consider, such as identify-
ing the optimal parameter regime (K,κ)for achieving peakperformance. From the minimum error values in Table II
and also from Fig. 6, the optimal performance occurs when
κ≪K≪α. This outcome is intuitively explained by the fact
thatκ≪Kcorresponds to high quantumness, which is the
regime with best performance. Also κ,K≪αindicates the
dominance of dynamics generated by the input data, facilitat-
ing the reservoir’s learning process. It is evident that if the
reservoir’s dependency on input data is obfuscated by other
dynamics (even in the high quantumness regime), its ability
to predict the series diminishes.
The quest for both the best and worst performing states is
also noteworthy, as it aids in determining suitable initial con-
ditions for the reservoir. We have provided this analysis in
Section IV of the Supplementary Material.
2 2.5 3 3.5 4 4.5 5 5.5
log(quantumness)0.020.040.060.080.10.120.140.160.180.20.22Kand5BestKand5values
K
kappa
FIG. 6: Values of K∈ {0.02,0.05,0.07,0.1,0.12}and
κ∈ {0.02,0.03,0.05,0.1,0.2,0.3}which correspond to the best performing reservoir,
as a function of quantumness. Note that the absolute best reservoir corresponds to
K=0.05,κ=0.02. The logarithm for the x-axis scaling is the natural logarithm, i.e.,
base e.
D. Further investigation
There are other interesting aspects of learning a time series
with such a model. Here we discuss some related questions
that are addressed in our Supplementary Material.
One may ask if there are other families of chaotic time se-
ries that can be learned by this model. We affirmatively an-
swer this question in Section I in the Supplementary Material.
Furthermore, we study the resilience of this model against
noise in Section II of the Supplementary Material, where we
observe that the reservoir is surprisingly well-robust against a
variety of sources of error. Finally, we illustrate a few sam-
ples of the trajectories of the reservoir, starting from different
initial states in Section V of the Supplementary Material.7
TABLE II: This table presents a summary of simulation results outlined in Section III C. Each cell corresponds to fixed values of Kandκ. All values in the table, including the test
error (best, average, and worst) as the first entry, the average quantumness of states with standard deviation as the second entry, and the indices denoting the best and worst performing
states in the form (best, worst) as the third entry, are obtained by subjecting 35 random states to the training process under the specified hyperparameters.
K=0.02 K=0.05 K=0.07 K=0.1 K=0.12
κ=0.02(0.0632 ,0.0658 ,0.0694)
143.6502±16.5434
(16,22)(0.0592 ,0.0652 ,0.0685)
154.5753±28.1046
(28,16)(0.0573 ,0.0655 ,0.0810)
162.0891±16.6570
(20,19)(0.0595 ,0.0705 ,0.0845)
168.6424±15.7672
(2,19)(0.0639 ,0.0759 ,0.0948)
170.5078±14.8454
(1,13)
κ=0.03(0.0633 ,0.0661 ,0.0692)
100.9440±27.6376
(13,9)(0.0605 ,0.0653 ,0.0684)
107.8367±16.6010
(28,22)(0.0579 ,0.0651 ,0.0746)
112.7951±16.6570
(20,19)(0.0577 ,0.0687 ,0.0832)
117.3331±15.7672
(2,19)(0.0598 ,0.0728 ,0.0855)
118.9255±14.8454
(1,13)
κ=0.05(0.0634 ,0.0666 ,0.0693)
60.8518±8.6175
(6,22)(0.0628 ,0.0659 ,0.0685)
64.2209±16.6010
(28,22)(0.0601 ,0.0654 ,0.0713)
67.1316±16.6570
(20,19)(0.0603 ,0.0674 ,0.0788)
70.1166±15.7672
(2,19)(0.0590 ,0.0691 ,0.790)
71.2287±14.8454
(1,13)
κ=0.1(0.0644 ,0.0671 ,0.0695)
30.6725±4.5540
(14,19)(0.0628 ,0.0664 ,0.0693)
31.5354±8.6175
(26,12)(0.0640 ,0.0665 ,0.0693)
32.3377±8.5992
(7,12)(0.0618 ,0.0663 ,0.0693)
33.5689±8.3105
(14,19)(0.0628 ,0.0672 ,0.0693)
34.1637±7.9619
(28,19)
κ=0.2(0.0658 ,0.0681 ,0.0698)
15.9071±3.2411
(7,22)(0.0654 ,0.0676 ,0.0693)
16.1002±4.5540
(14,19)(0.0653 ,0.0674 ,0.0693)
16.2353±4.5746
(7,12)(0.0647 ,0.0671 ,0.0693)
16.4535±4.5242
(14,19)(0.0643 ,0.0668 ,0.0698)
16.6154±4.4510
(28,19)
κ=0.3(0.0663 ,0.0681 ,0.0698)
11.0579±3.2411
(27,5)(0.0666 ,0.0676 ,0.0697)
11.1314±4.5540
(7,5)(0.0660 ,0.0674 ,0.0697)
11.1876±4.5746
(13,5)(0.0659 ,0.0671 ,0.0699)
11.2706±4.5242
(26,5)(0.0658 ,0.0668 ,0.0695)
11.3266±4.4510
(13,27)
IV . DISCUSSION
In this study, we focused on evaluating the effectiveness of
a quantum non-linear oscillator in making time-series predic-
tions and examining how quantumness impacts the quantum
learning model. We utilize the Lee-Jeong measure [46] as our
quantumness metric, which was initially introduced to mea-
sure macroscopicity and in this work is shown to be a quan-
tumness measure as well. Through our methodologies, we
discovered that quantumness provides us with a broader set of
outcomes, and that the best performance observed is achieved
at high quantumness, but that high quantumness alone does
not guarantee good performance. Overall, our findings con-
tribute to a deeper understanding of the role of quantumness
in continuous-variable reservoir computing and highlight its
potential for enhancing the performance of this computational
model.
Our work raises a number of important questions. Firstly,
we aim to determine what specific structures within a reservoir
computing model will lead to quantum speed-ups. Indeed,
there are other measures of quantumness in the Fock space
(c.f., [59]). It would be interesting to explore the relationship
of these measures with learning performance. Additionally,
one can investigate the impact of quantumness for a network
of oscillators in future research. Notably, when dealing with a
network of continuous variable oscillators, entanglement as a
measure of quantumness could also be examined. It is worth
mentioning that our method has potential for implementation
on actual quantum hardware, and may even be feasible withcurrent limited devices due to the strong Kerr non-linearity
present in models for a transmon superconducting qubit [60].
CODE A VAILABILITY
The codes used for the generation of the
plots of this manuscript are publicly available at
https://github.com/arsalan-motamedi/QRC.
ACKNOWLEDGEMENT
We acknowledge Wilten Nicola for fruitful discussions and
for reading and commenting on an earlier version of the
manuscript. The authors acknowledge an NSERC Discovery
Grant, the Alberta Major Innovation Fund, Alberta Innovates,
Quantum City, and NRC CSTIP grant AQC 007.
CONTRIBUTIONS
All authors contributed extensively to the presented work.
H.Z-H. and C.S. conceived the original ideas and super-
vised the project. A.M. performed analytical studies and nu-
merical simulations and generated different versions of the
manuscript. H.Z-H. and C.S. verified the calculations, pro-
vided detailed feedback on the manuscript, and applied many
insightful updates.
[1] P. W. Shor, Polynomial-time algorithms for prime factorization
and discrete logarithms on a quantum computer, SIAM review
41, 303 (1999).[2] E. R. MacQuarrie, C. Simon, S. Simmons, and E. Maine, The
emerging commercial landscape of quantum computing, Nature
Reviews Physics 2, 596 (2020).8
[3] A. W. Harrow, A. Hassidim, and S. Lloyd, Quantum algorithm
for linear systems of equations, Physical Review Letters 103,
10.1103/physrevlett.103.150502 (2009).
[4] M. A. Nielsen and I. Chuang, Quantum computation and quan-
tum information (2002).
[5] C. H. Bennett and G. Brassard, Quantum cryptography:
Public key distribution and coin tossing, arXiv preprint
arXiv:2003.06557 (2020).
[6] C. L. Degen, F. Reinhard, and P. Cappellaro, Quantum sensing,
Reviews of modern physics 89, 035002 (2017).
[7] C. Simon, Towards a global quantum network, Nature Photon-
ics11, 678 (2017).
[8] R. L. Rivest, A. Shamir, and L. M. Adleman, Crypto-
graphic communications system and method (1983), uS Patent
4,405,829.
[9] D. Aharonov and M. Ben-Or, Fault-tolerant quantum compu-
tation with constant error, in Proceedings of the twenty-ninth
annual ACM symposium on Theory of computing (1997) pp.
176–188.
[10] E. Knill, R. Laflamme, and W. H. Zurek, Resilient quantum
computation, Science 279, 342 (1998).
[11] A. Y . Kitaev, Fault-tolerant quantum computation by anyons,
Annals of Physics 303, 2 (2003).
[12] P. W. Shor, Fault-tolerant quantum computation, in Proceedings
of 37th conference on foundations of computer science (IEEE,
1996) pp. 56–65.
[13] R. Acharya, I. Aleiner, R. Allen, T. I. Andersen, M. Ansmann,
F. Arute, K. Arya, A. Asfaw, J. Atalaya, R. Babbush, et al. ,
Suppressing quantum errors by scaling a surface code logical
qubit, arXiv preprint arXiv:2207.06431 (2022).
[14] K. Temme, S. Bravyi, and J. M. Gambetta, Error mitigation
for short-depth quantum circuits, Physical review letters 119,
180509 (2017).
[15] K. Bharti, A. Cervera-Lierta, T. H. Kyaw, T. Haug, S. Alperin-
Lea, A. Anand, M. Degroote, H. Heimonen, J. S. Kottmann,
T. Menke, et al. , Noisy intermediate-scale quantum algorithms,
Reviews of Modern Physics 94, 015004 (2022).
[16] A. Kandala, K. Temme, A. D. C ´orcoles, A. Mezzacapo, J. M.
Chow, and J. M. Gambetta, Error mitigation extends the com-
putational reach of a noisy quantum processor, Nature 567, 491
(2019).
[17] J. Preskill, Quantum computing in the nisq era and beyond,
Quantum 2, 79 (2018).
[18] E. Farquhar, C. Gordon, and P. Hasler, A field programmable
neural array, in 2006 IEEE international symposium on circuits
and systems (IEEE, 2006) pp. 4–pp.
[19] J. J. Hopfield, Neural networks and physical systems with emer-
gent collective computational abilities., Proceedings of the na-
tional academy of sciences 79, 2554 (1982).
[20] J. Schmidhuber, Deep learning in neural networks: An
overview, Neural networks 61, 85 (2015).
[21] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-
Farley, S. Ozair, A. Courville, and Y . Bengio, Generative adver-
sarial networks, Communications of the ACM 63, 139 (2020).
[22] R. Pascanu, T. Mikolov, and Y . Bengio, On the difficulty of
training recurrent neural networks, in International conference
on machine learning (PMLR, 2013) pp. 1310–1318.
[23] S. Basodi, C. Ji, H. Zhang, and Y . Pan, Gradient amplification:
An efficient way to train deep neural networks, Big Data Mining
and Analytics 3, 196 (2020).
[24] W. Maass, T. Natschl ¨ager, and H. Markram, Real-time comput-
ing without stable states: A new framework for neural com-
putation based on perturbations, Neural computation 14, 2531
(2002).[25] H. Jaeger and H. Haas, Harnessing nonlinearity: Predicting
chaotic systems and saving energy in wireless communication,
science 304, 78 (2004).
[26] G. Tanaka, T. Yamane, J. B. H ´eroux, R. Nakane, N. Kanazawa,
S. Takeda, H. Numata, D. Nakano, and A. Hirose, Recent ad-
vances in physical reservoir computing: A review, Neural Net-
works 115, 100 (2019).
[27] A. R ¨ohm and K. L ¨udge, Multiplexed networks: reservoir com-
puting with virtual and real nodes, Journal of Physics Commu-
nications 2, 085007 (2018).
[28] W. Nicola and C. Clopath, Supervised learning in spiking neural
networks with force training, Nat Commun 8, 10.1038/s41467-
017-01827-3 (2017).
[29] K. Nakajima, Reservoir computing: Theory, physical imple-
mentations, and applications, IEICE Technical Report; IEICE
Tech. Rep. 118, 149 (2018).
[30] B. Schrauwen, D. Verstraeten, and J. Van Campenhout, An
overview of reservoir computing: theory, applications and im-
plementations, in Proceedings of the 15th european symposium
on artificial neural networks. p. 471-482 2007 (2007) pp. 471–
482.
[31] Y . D. Mammedov, E. U. Olugu, and G. A. Farah, Weather fore-
casting based on data-driven and physics-informed reservoir
computing models, Environmental Science and Pollution Re-
search 29, 24131 (2022).
[32] S. Kan, K. Nakajima, T. Asai, and M. Akai-Kasaya, Physical
implementation of reservoir computing through electrochemi-
cal reaction, Advanced Science 9, 2104076 (2022).
[33] J. Biamonte, P. Wittek, N. Pancotti, P. Rebentrost, N. Wiebe,
and S. Lloyd, Quantum machine learning, Nature 549, 195
(2017).
[34] F. P. Maria Schuld, Ilya Sinayskiy, An introduction
to quantum machine learning, Contemporary Physics
10.1080/00107514.2014.964942.
[35] M. Schuld, I. Sinayskiy, and F. Petruccione, An introduction
to quantum machine learning, Contemporary Physics 56, 172
(2015).
[36] K. Fujii and K. Nakajima, Quantum reservoir computing: a
reservoir approach toward quantum machine learning on near-
term quantum devices, in Reservoir computing (Springer, 2021)
pp. 423–450.
[37] L. C. G. Govia, G. J. Ribeill, G. E. Rowlands, H. K. Krovi, and
T. A. Ohki, Quantum reservoir computing with a single nonlin-
ear oscillator, Phys. Rev. Research 3, 013077 (2021).
[38] I. Luchnikov, S. Vintskevich, H. Ouerdane, and S. Filippov,
Simulation complexity of open quantum dynamics: Connec-
tion with tensor networks, Physical review letters 122, 160401
(2019).
[39] R. Mart ´ınez-Pe ˜na, J. Nokkala, G. L. Giorgi, R. Zambrini, and
M. C. Soriano, Information processing capacity of spin-based
quantum reservoir computing systems, Cognitive Computation
, 1 (2020).
[40] J. Nokkala, R. Mart ´ınez-Pe ˜na, G. L. Giorgi, V . Parigi, M. C. So-
riano, and R. Zambrini, Gaussian states of continuous-variable
quantum systems provide universal and versatile reservoir com-
puting, Communications Physics 4, 53 (2021).
[41] P. Pfeffer, F. Heyder, and J. Schumacher, Quantum reser-
voir computing of thermal convection flow, arXiv preprint
arXiv:2204.13951 (2022).
[42] S. Ghosh, K. Nakajima, T. Krisnanda, K. Fujii, and T. C.
Liew, Quantum neuromorphic computing with reservoir com-
puting networks, Advanced Quantum Technologies 4, 2100053
(2021).9
[43] S. Vintskevich and D. Grigoriev, Computing with two quan-
tum reservoirs connected via optimized two-qubit nonselective
measurements, arXiv preprint arXiv:2201.07969 (2022).
[44] N. G ¨otting, F. Lohof, and C. Gies, Exploring quantum me-
chanical advantage for reservoir computing, arXiv preprint
arXiv:2302.03595 (2023).
[45] M. C. Mackey and L. Glass, Oscillation and chaos in physio-
logical control systems, Science 197, 287 (1977).
[46] C.-W. Lee and H. Jeong, Quantification of macroscopic quan-
tum superpositions within phase space, Physical review letters
106, 220401 (2011).
[47] C. Gardiner, P. Zoller, and P. Zoller, Quantum noise: a hand-
book of Markovian and non-Markovian quantum stochastic
methods with applications to quantum optics (Springer Science
& Business Media, 2004).
[48] S. Shalev-Shwartz and S. Ben-David, Understanding machine
learning: From theory to algorithms (Cambridge university
press, 2014).
[49] B. Groisman, D. Kenigsberg, and T. Mor, ” quantumness”
versus” classicality” of quantum states, arXiv preprint quant-
ph/0703103 (2007).
[50] H. Ollivier and W. H. Zurek, Quantum discord: a measure of the
quantumness of correlations, Physical review letters 88, 017901
(2001).
[51] K. Takahashi, Wigner and husimi functions in quantum me-
chanics, Journal of the Physical Society of Japan 55, 762
(1986).
[52] F. Fr ¨owis, P. Sekatski, W. D ¨ur, N. Gisin, and N. Sangouard,
Macroscopic quantum states: Measures, fragility, and imple-
mentations, Reviews of Modern Physics 90, 025004 (2018).
[53] S. Nimmrichter and K. Hornberger, Macroscopicity of mechan-
ical quantum superposition states, Physical review letters 110,
160403 (2013).
[54] A. J. Leggett and A. Garg, Quantum mechanics versus macro-
scopic realism: Is the flux there when nobody looks?, Physical
Review Letters 54, 857 (1985).
[55] A. J. Leggett, Note on the” size” of schroedinger cats, arXiv
preprint arXiv:1603.03992 (2016).
[56] J. Gong, Comment on ”quantification of macroscopic quantum
superpositions within phase space” (2011).
[57] A. Haar, Der massbegriff in der theorie der kontinuierlichen
gruppen, Annals of mathematics , 147 (1933).
[58] J. Watrous, The theory of quantum information (Cambridge uni-
versity press, 2018).
[59] O. Steuernagel and R.-K. Lee, Quantumness measure from
phase space distributions, arXiv preprint arXiv:2311.17399
(2023).
[60] P. Bertet, F. Ong, M. Boissonneault, A. Bolduc, F. Mallet,
A. Doherty, A. Blais, D. Vion, and D. Esteve, Circuit quantum
electrodynamics with a nonlinear resonator (Oxford University
Press, 2012).
[61] O. E. R ¨ossler, An equation for continuous chaos, Physics Let-
ters A 57, 397 (1976).
[62] O. Rossler, An equation for hyperchaos, Physics Letters A 71,
155 (1979).
[63] T. Kubota, Y . Suzuki, S. Kobayashi, Q. H. Tran, N. Yamamoto,
and K. Nakajima, Quantum noise-induced reservoir computing
(2022).
[64] D. Fry, A. Deshmukh, S. Y .-C. Chen, V . Rastunkov, and
V . Markov, Optimizing quantum noise-induced reservoir com-
puting for nonlinear and chaotic time series prediction, arXiv
preprint arXiv:2303.05488 (2023)1
Supplementary Material for: Correlations Between Quantumness and Learning Performance in
Reservoir Computing with a Single Oscillator
I. R ¨OSSLER ATTRACTOR
This section provides the performance in the training of R ¨ossler attractor. The R ¨ossler attractor [S61, S62] is a three-
dimensional motion following the dynamics


dx
dt=−y−z
dy
dt=x+ay
dz
dt=b+z(x−c)(S1)
where (a,b,c)∈R3are constant parameters, which we set to (0.2,0.2,5.7)in our experiment. Fig. S1a and Fig. S1b show the
model’s learning results for this chaotic time series. We highlight that each component of the oscillator is learned independently
from the others.
102030405060708090
timestep/10-0.500.51x;^xProduced	by	reservoir
Actual	test	data
Separating	train	from	test	data
(a) The oscillator is trained to reproduce the R ¨ossler time series, and
then asked to perform prediction, given initial values, which are the data
points before the dashed line. The parameters are
(α,κ,K) = ( 1.0,0.2,0.01).
X-1-0.500.511.5Y
-1-0.8-0.6-0.4-0.200.20.40.60.81
^ X-1-0.500.511.5^ Y
-1-0.8-0.6-0.4-0.200.20.40.60.81(b) Comparing the phase diagrams X−Yof the R ¨ossler attractor of the
actual R ¨ossler attractor with the predicted by the reservoir (left) with that
of the reservoir’s output (right).
FIG. S1: R ¨ossler attractor training with a quantum oscillator. Here ndenotes the time step or equivalently the index of the sampled point from the function, (x,y)and(ˆx,ˆy)refer to the
actual and the predicted series, respectively.
II. NOISE
Noise is an inevitable factor in quantum devices, and it is of profound importance for a quantum computing approach to be
robust to noise. We show the robustness of this approach by considering different noise models as explained below.
a. Noisy MG learning Here we consider adding noise to the learning process of an MG time series, and will examine
the performance. We introduce a dephasing error by considering the Lindbladian operators Ln=λ|n⟩⟨n|for all n∈Z≥0.
Furthermore, the incoherent pumping error is simulated by considering the Lindbladian corresponding to λa†. On top of those,
we add white noise to the input of the reservoir. This is performed via changing the equations of evolution (2) through the
substitution f(t)→f(t) +λ′n(t). Here, n(t)is the white noise of unit power, and λ′controls its strength. Fig. S2 shows the
performance of the reservoir’s output under both incoherent pumping, dephasing error, and white noise (see Eq. (2)). We have
set(K,κ,α,λ,λ′) = ( 0.05,0.15,1.2,0.05,0.02).
b. Other time series Here we consider the effect of noise on several other time series. We consier simple periodic functions,
but we add a much larger white noise to the input. Fig. S4 shows the outcome of this learning task. Despite significant
signal distortion caused by noise, the oscillator demonstrates the ability to learn the underlying periodic functions. We further
investigated the effect of training sawtooth signal with different noise levels, which resulted in Fig. S3. A similar experiment,
this time with the MG, resulted in a training error of 0 .053. Noting that the training error in the noiseless case results in an
error of 0 .047, we conclude that the model is robust to this noise model for a variety of prediction tasks. We made the choice of
parameters α=0.1,κ=K=0.05 in obtaining the results.2
0 20 40 60 80 100 120
timestep/10-0.5-0.4-0.3-0.2-0.100.10.20.30.40.5x;^xProduced	by	reservoir
Actual	test	data
Separating	train	from	test	data
(a) Prediction of Mackey-Glass under the effects of noise. The noise models
considered are dephasing incoherent pumping, and white noise on the input.
-0.5 0 0.5
x[n!17]-0.5-0.4-0.3-0.2-0.100.10.20.30.4x[n]
-0.5 0 0.5
^x[n!17]-0.5-0.4-0.3-0.2-0.100.10.20.30.4^x[n](b) Comparing the delayed embedding of the actual Mackey-Glass (left) with the
reservoir’s output (right) in the noisy settings.
FIG. S2: Noisy reservoir learning MG. The MG training process is performed when there are a variety of noises applied to the reservoir.
It is worth mentioning that noise in the context of reservoir computing is shown to be useful in certain cases [S63, S64].
00.020.040.060.080.10.120.140.160.180.2T e s t E r r o r
00.020.040.06S e t t i n g t h e c a t s t a t e a s t h e i n i t i a l s t a t e
N o i s e V a r i a n c e00.020.040.060.080.10.120.140.160.180.2T e s t E r r o r
00.020.040.06S e t t i n g t h e m i x e d s t a t e a s t h e i n i t i a l s t a t e
FIG. S3: Test error of training noisy sawtooth function. The initial states are the cat state and its classical mixture i.e., the normalized |α⟩+|−α⟩and|α⟩⟨α|+|−α⟩⟨−α|. The noise
model in this example is an additive white noise to the input.
III. MACKEY–GLASS WITH DIFFERENT PARAMETERS
In this section we investigate the hardness of training a Mackey–Glass series with different τvalues. Note that τsignificanly
changes the structure of the chaotic series. A small τcorresponds to a simple series, while a large τbring in much chaos. We
trained our model on series with τ∈ {10,17,40}and provided the traninig result in . We highlight that all hyperparameters
used in the training of these series are the same except for the input length, N(as defined in Section II). We have used N=110,
N=200, and N=400 for τ=10,τ=17, and τ=40, respectively. Notably, using an input length as large as N=200 results
in the poor training for the series with τ=40, while an input length as small as N=110 is sufficient for learning the series with
τ=10.
IV . RANDOM STATES USED IN SECTION III C
We have presented the Wigner function of the 35 randomly selected states that are sent through the training process in
Section III C, in Fig. S6. We have also explored the performance of the state in Fig. S7. Notably, states indexed as 22, 19,
and 5 should be avoided during training, as they consistently rank among the worst performers and never appear among the best3
2030405060f 1 ( t )
-202N o i s y i n p u t t o t h e r e s e r v o i r
60708090100^ f 1 ( t )
-101R e s e r v o i r ' s o u t p u t
2030405060f 1 ( t )
-202
60708090100^ f 1 ( t )
-101
2030405060f 2 ( t )
-202
60708090100^ f 2 ( t )
-101
t2030405060f 3 ( t )
-2024
t60708090100^ f 3 ( t )
0123
FIG. S4: Learning noisy periodic functions. The input to the reservoir is contaminated by white noise. However, the reservoir is still able to learn the input signal.
0 20 40 60 80 100 120-0.500.5x(t);^x(t)==10
0 20 40 60 80 100 120-0.500.5x(t);^x(t)==17
0 20 40 60 80 100 120
timestep/10-0.500.5x(t);^x(t)==40
FIG. S5: Training MG series with different τvalues. We have used the values τ∈ {10,17,40}. The blue curve in each panel demonstrates the output of the reservoir and the red curve
corresponds to the actual data. As we observe, the behavior of the series get more complex and chaotic as we increase τ. We also highlight that all the hyperparameters of the
reservoirs we used in this simulation are the same, except for the training length N(as defined in Section II). We use N=110,200,400 for the generation of the top, middle, and the
bottom panel respectively.
ones in Fig. S7. However, it is noteworthy that the best-performing states are dispersed and are not well-concentrated on a few
instances.
V . EVOLUTION ANIMATIONS
Animations showing the evolution of the Wigner function throughout the process are prepared and made available online at
https://github.com/arsalan-motamedi/QRC/tree/main/EvolutionAnimations.4
0
100Index = 1
 Index = 8
 Index = 15
 Index = 22
 Index = 29
0
100Index = 2
 Index = 9
 Index = 16
 Index = 23
 Index = 30
0
100Index = 3
 Index = 10
 Index = 17
 Index = 24
 Index = 31
0
100Index = 4
 Index = 11
 Index = 18
 Index = 25
 Index = 32
0
100Index = 5
 Index = 12
 Index = 19
 Index = 26
 Index = 33
0
100Index = 6
 Index = 13
 Index = 20
 Index = 27
 Index = 34
0 1000
100Index = 7
0 100Index = 14
0 100Index = 21
0 100Index = 28
0 100Index = 35
0.100
0.075
0.050
0.025
0.0000.0250.0500.0750.100
valuesAll states
FIG. S6: The Wigner function of all the 35 randomly selected states that were used in the training process described in Section III C.
Worst1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
Indexofstate012345678Frequency
Best1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
Indexofstate0123456Frequency
FIG. S7: The histogram that shows how many times (i.e., for how many of (K,κ)pairs) does a state appears as the worst (top panel) and the best (bottom panel) performing state.
Please refer to Fig. S6, where we represent the Wigner function of these randomly selected states.