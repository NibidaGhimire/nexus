1
Optimum Output Long Short-Term Memory Cell
for High-Frequency Trading Forecasting
Adamantios Ntakaris, Moncef Gabboujy,Fellow, IEEE , and Juho Kanniaineny
Business School, University of Edinburgh, Edinburgh, EH8 9JS, UK
yDepartment of Computing Sciences, Tampere University, Tampere, 33720, Finland
Abstract —High-frequency trading requires fast data pro-
cessing without information lags for precise stock price
forecasting. This high-paced stock price forecasting is usually
based on vectors that need to be treated as sequential and
time-independent signals due to the time irregularities that
are inherent in high-frequency trading. A well-documented
and tested method that considers these time-irregularities
is a type of recurrent neural network, named long short-
term memory neural network. This type of neural network
is formed based on cells that perform sequential and stale
calculations via gates and states without knowing whether
their order, within the cell, is optimal. In this paper, we
propose a revised and real-time adjusted long short-term
memory cell that selects the best gate or state as its ﬁnal
output. Our cell is running under a shallow topology, has
a minimal look-back period, and is trained online. This
revised cell achieves lower forecasting error compared to
other recurrent neural networks for online high-frequency
trading forecasting tasks such as the limit order book mid-
price prediction as it has been tested on two high-liquid US
and two less-liquid Nordic stocks.
Index Terms —Limit order book, high-frequency trading,
long short-term memory cell, online learning, stock forecast-
ing.
I. I NTRODUCTION
HIGH-FREQUENCY trading (HFT), which represents
more than 50% of the trading activity in the US
stock market, is the process where signals and trades are
analyzed and executed in a fraction of a second (i.e.,
double-digit nanoseconds). This speed race creates several
opportunities for the participants who can capitalize on
their infrastructure, like ﬁber cables and microwave sig-
nals, and also on their scientiﬁc and technical expertise.
One way to organize this trading activity is the so-called
limit order book (LOB) where participants in the form of
liquidity providers and liquidity takers, with the former
ones being more active in the trading scene(s), form its
order ﬂow dynamics. This symbiotic relationship between
these two parties creates competition based on the order
ﬂow information analysis and trading strategy execution.
In this paper, we focus on the ﬁrst part which is the
fast order ﬂow information analysis for forecasting tasks
such as the prediction of the next LOB’s mid-price (i.e.,
the average of the best ask and bid LOB’s price level)
and is the main objective of our experimental protocol.The forecasting of the next mid-price, as part of the fast
order ﬂow information analysis, is equivalent to online or
else tick-by-tick (i.e., forecasting based on every possible
trading event disconnected from the time) prediction.
Every forecasting task in the HFT LOB universe is
a challenge in terms of data size digestion (i.e., several
million trading events per trading day) and time irregu-
larities inspection. These two factors create a non-linear
environment that can be handled properly by speciﬁc types
of neural networks (NN). A NN type that has exhibited
good predictive power is a variant that belongs to the recur-
rent neural network (RNN) family, named long short-term
memory neural network (LSTM) [1]. LSTM ﬁlters current
and look-back/past information of the input data. The
effectiveness of LSTMs is quite extensive in several ﬁelds
such as acoustics [2]–[4], natural language processing [5]–
[7], biology and medicine [8]–[10], and computer vision
[11]–[13] among others. One particular area of research,
where LSTMs have been applied extensively, is ﬁnance
and algorithmic trading [14]–[18], [20].
The study of HFT LOB datasets requires the modeler
to parse all the relevant information without any process-
ing lags. There are several examples where the machine
learning (ML) experimental protocol was organized around
NNs that had to predict lagged or hidden lagged supervised
classiﬁcation labels [19], [21]. The term hidden lagged
supervised classiﬁcation task refers to the process where
the forecasting seems to be based on tick data but the ML
HFT trader has to wait until the next change in the bid
(or ask) price is going to take place without any parallel
countdown mechanism in place like in [22].
An additional line of challenge in building real online
machine learning experimental protocols is not just the
engineering of the objective (i.e., LOB’s mid-price fore-
casting) but also the development of dynamically-adjusted
NNs that are suitable for short training based on fewer
training epochs. To address the challenge of creating real
online forecasting models following the RNN architecture
we have to better understand the internal computational
processes of their cells, such as the LSTM cell. The LSTM
cell is a wrapper system that contains several gates and
states that ﬁlter the input vector(s). These gates and states
are combinations of activation functions that act as thearXiv:2304.09840v3  [cs.LG]  15 May 20232
source of information for the next LSTM cell. The order of
these calculations is ﬁxed, stale during training/learning,
and their position within the cell is not justiﬁed by any
speciﬁc metric. That means that the LSTM cell and the
LSTM NN in general exhibit a static behavior against
a dynamic and ultra-fast trading setting (i.e., HFT LOB
universe). A critical question that the ML HFT trader has
to answer is whether the LSTM NN should be utilized
as it is (i.e., forecasting will be based on NN’s stale
behavior and online prediction will be based on processing
data using a sliding window) or an autonomous and
dynamic cell intervention will allow a more systematic
and computationally cheaper, in terms of a shallow NN
topology, forecasting outcome.
In this paper, we address these issues (i.e., truly dynamic
and shallow NN topology) by re-arranging and selecting
online the optimal LSTM’s cell ﬁnal output. We do that
by evaluating online the importance of the internal gates
and states. This internal (i.e., within the LSTM cell)
feature importance mechanism has been developed around
two critical components: a simple optimization method
that follows the gradient descent (GD) learning algorithm
and a non-forecasting supervised regression problem that
has similar but not the same targets/labels as the main
objective of our experimental protocol. The GD method
and the non-forecasting supervised regression problem
are combined internally (i.e., within the LSTM cell) and
the optimized outcome is passed to the next LSTM cell.
Since we capitalize on the existing LSTM cell architecture
without adding, removing, or changing the core LSTM cell
calculations we name our cell architecture as Optimum
Output LSTM (OPTM-LSTM) cell.1
The rest of this paper is organized as follows. In
Section II, we expand on related work about different RNN
formations. In Section III, we provide the technical details
of the OPTM-LSTM cell and in Section IV we provide
the experimental details and the comparative performance
results. Section V , summarizes our ﬁndings and discusses
limitations and future lines of research.
II. R ELATED WORK
So far, RNNs such as LSTM NN (e.g., shallow or deep
topology) have been applied extensively in the LOB liter-
ature for several different forecasting tasks. For instance,
authors in [24], [26] utilized, among several ML and deep
learning (DL) methods, LSTMs for mid-price movements
forecasting with LSTMs being among the best performers
in terms of predictive power. LSTMs also exhibit good
forecasting performance for ask and bid future movements
as highlighted by the authors in [21]. Authors in [27]
enhanced the LSTM’s predictive power by attaching an
attention mechanism. In an earlier study authors in [28]
1The coding part is based on TensorFlow [49] and Keras [48] libraries.
Code available at https://github.com/DeQmE/OPTMLSTM.combined LSTM NNs with an attention mechanism for
stock jump price prediction. LSTMs have been recently
implemented for market-making strategies as this can be
seen in [29].
The LSTM cell architecture is based on a pre-deﬁned
sequence of operations that were set based on ad-hoc
assumptions about the ordering of the internal calcula-
tions, the number of gates/states, and the cell’s output
information. This is a common approach for other RNN
variants such as standard recurrent cells, gated recurrent
units (GRU), and several other LSTM variants. There
are only a few studies, outside the HFT LOB universe,
where the internal gates were adjusted according to the
information ﬂow but with a few shortcomings. For in-
stance, the authors in [30] evaluated over ten thousand
RNN architectures with their ﬁndings suggesting that a
combination of some of the calculations, similar to a GRU
development, performed better but close to the perfor-
mance of the standard LSTM NN. Although the authors
offered an impressive number of different architectures,
for the problems of arithmetic predictions, XML, and
language modelling, their results are not suitable for HFT
LOB forecasting tasks. The extensive topological grid
search is time-consuming until a suitable sequence and
type of linear algebra calculations have to be used. Another
problem is that the re-arrangement or the elimination of
the RNN’s gate(s) is not directly connected to the ﬁnal
objective(s). That means that the RNN’s weights will be
updated ﬁrst on the full batch (or mini-batch) sequence and
then a different architecture will be tested. On a different
note, several other authors suggested variants of RNN
cells that exhibited good predicting performance but their
architecture remained unchanged when the training pro-
cess started. For instance, authors in [31] suggested three
simpliﬁed LSTM cells with similar or better performance
to the original LSTM cell structure. In the same fashion
authors in [32] and [33] proposed modiﬁcations to the
most important LSTM’s cell gates. Lighter RNN cells are
also suggested by authors in [34]–[38].
More complicated structures were also proposed. For
instance, authors in [39] attached an additional term to
several internal LSTM gates, known as peephole LSTM
NN. Other variants of such complicated cells can be found
in [41]–[46]. Few additional RNN variants exist, such as
bidirectional LSTM, which were utilized effectively in
[50] for predicting the open, high, low, and close stock
prices. Similarly, authors in [51], and [52] also employed
bidirectional LSTMs architectures for daily stock price
forecasting. An additional line of research of LSTMs
is their use as hybrid models since there are several
examples, such as [53], [54], [55], and [56], that they
combined with convolutional neural networks (CNN) for
the task of stock price forecasting. These models, despite
their forecasting efﬁcacy, are not agile in terms of high-
paced information ﬂow analysis since the cells remain3
stale during training and also during learning and are
disconnected from their predicting task.
III. P ROPOSED METHOD
The behavior of an HFT LOB is characterized by the
rapid changes in its stock inventory which affects the stock
price. As a result, the LSTM NN needs to be ready to
identify, in a fraction of a second, these dynamics and
provide the most optimized suggestion/information. This
can happen effectively if we improve, for instance, the
original mechanics of the LSTM cell. The cell, as part
of the LSTM NN, so far was over-engineered, either by
adding or removing complexity in terms of the number
and order of its internal states and gates. For this reason,
we propose a real adaptive LSTM cell architecture, named
OPTM-LSTM cell, that is different from the existing ones
(i.e., RNN cell mechanics) in two key areas:
LSTM’s cell gates and states are treated as fea-
tures and we measure their ability to handle the
online information ﬂow via an internal feature im-
portance mechanism. More speciﬁcally, the LSTM
cell is equipped with this internal (i.e., within the
cell) feature importance mechanism which acts as a
non-forecasting supervised regression problem. That
means that this non-forecasting supervised regression
relies on labels that represent the current (i.e., already
known) LOB’s mid-price and act as a guarantor for
the importance of the internal LSTM’s cell gates and
states. The guarantor will attach trained weights, via
the GD learning algorithm, to every LSTM’s cell
internal gates and states.
The guarantor mechanism, which is an internal block
within the LSTM cell, operates based on labels that
represent the current (i.e., already known) LOB’s
mid-price. The fact that these labels represent the
current mid-price means that the OPTM-LSTM cell
is equipped with a robust internal instructing method
that is connected, conceptually (i.e., current and al-
ready known mid-price), to the overall forecasting
objective of the present paper which is the prediction
of the next mid-price. In other words, this instructing
method is using a lagged version of the labels (i.e.,
current mid-price labels as part of the internal feature
importance mechanism) that are utilized in the pre-
diction of the next mid-price. It can also be stated that
we have two supervised regression problems, the ﬁrst
one is part of the guarantor mechanism (i.e., use of the
current mid-price as a label) within the OPTM-LSTM
cell and the other one is part of the overall forecasting
objective in our ML experimental protocol which is
the prediction of the next LOB mid-price (i.e., use of
the next mid-price as a label).
This way, we do not add or remove any arbitrary calcula-
tions within the LSTM cell but instead, we capitalize onits existing formation. Thus, we can train fast without any
extensive grid search calculations. The cell will be updated
constantly based on the tick-by-tick/current mid-price (i.e.,
every incoming trading event that forms the current mid-
price) inﬂow. That means that the internal gates and states
will be evaluated according to the GD method with respect
to the mean squared error (MSE) and this mechanism will
measure the features’ (i.e., gates and states) importance.
Before we delve into the details of the method let us
deﬁne the LSTM cell ﬁrst together with some background
information in terms of learning and training.
In Fig. 1 we can see that every LSTM cell considers
only one time-step input. An example of this input can be
just the present full LOB level (i.e., the continual operation
of a minimal time-step input length) or the current and
previous full LOB levels (i.e., a sliding window based on
larger historical input LOB data). These time-step inputs
are sequential trading events of a speciﬁc length (i.e., usu-
ally of arbitrary length but authors in [22] speciﬁed, based
on a hypothesis test, a logical time-steps vector length) and
we can call it a look-back period. The look-back period
will determine whether the number of sequential LSTM
cells can retain any useful information in their memory.
The LSTM’s cell memory is based on nine components
(i.e., four internal gates, four states, and one input feature
vector), which are:
ft2R1U, forget gate
it2R1U, input gate
~ct2R1U, cell input gate
ot2R1U, output gate
ct2R1U, cell state
ht2R1U, hidden state
ht 12R1U, previous hidden state
ct 12R1U, previous cell state
xt2R1D, input vector
where the ﬁrst dimension of the tensors above represents
the current or the previous time-step of the look-back
period, Uare the number of LSTM units (e.g., under
the TensorFlow framework), and Dthe number of input
features. We also include the bias terms bf,bi,b~c, and b0
2R1U(which are activated by default in TensorFlow).
Each of these gates and states represents the following
transformations:
ft=(Wfxt+Ufht 1+bf) (1)
it=(Wixt+Uiht 1+bi) (2)
~ct=tanh (W~cxt+U~cht 1+b~c) (3)
ot=(Woxt+Uoht 1+bo) (4)
ct=ftct 1+it~ct (5)4
Fig. 1: LSTM Cell: This is a three-dimensional interpretation of the core LSTM cell operations. There are four internal
gates (i.e., ft,it,~ct, and ot) and four states divided into two groups named previous (i.e., ct 1andht 1) and next
states (i.e., ctandht), respectively. There are three input tensors and these are: the feature vector xt2R1dwhich
represents just one time-step of the look-back period and the full range of the feature space, the previous LSTM cell
output ht 12R1Uwhere its dimensions stem from the previous time-step and the number of LSTM units (i.e., the
units are based on Tensorﬂow’s notation where the ML user usually selects 8, 16, 32, 64, 128, 256 or 512 units),
and the third input is the previous cell state ct 12R1Uwith a similar dimension proﬁle as the previous hidden
state ht 1. Note: The LSTM cell is designed in a three-dimensional space. More speciﬁcally, every gate and state are
presented as three-dimensional tensors but this is only for convenience to demonstrate the different building blocks. In
reality, these are two-dimensional tensors with a batch size of 1.
ht=ottanh (ct) (6)
whereis the sigmoid function, tanh is the hyperbolic
tangent function, the recurrent weights Wf,Wi,W~c,Wo,
Uf,Ui,U~c, andUoare multiplied with their corresponding
inputxtand previous hidden state ht 1vectors based
on the dot product algebraic operation, and ﬁnally, the
algebraic operation is the Hadamard product. The ﬁrst
dimension of the tensors represents the input time-step
which in our case will be one, otherwise will be equal
to the selected batch size. We need to mention here the
importance of the relationship between the batch size and
the look-back period in an online learning experimental
protocol. The batch or mini-batch setting updates the
NN weights with a lag equal to the length of the look-
back period and this creates several contradictory facts
about the online training scenarios. These scenarios are
the sliding window (i.e., a length-speciﬁc rolling window)
or the continual learning (i.e., the rolling window has a
minimum length of one and represents the current LOB
state) approaches. More speciﬁcally, if the length of the
look-back period is selected based on some experimental
criteria and not randomly, then the batch or mini-batch
size will be less important and only then the attention will
be paid to the topology of the LSTM NN. This is the
main motivation that enables us to scrutinize the LSTM
cell topology under an online continual learning protocol.
The main idea of our OPTM-LSTM cell is to enable theML trader to focus only on the selection of the look-back
period and discard the selection of the batch or mini-batch
size per training iteration. This can happen by creating
a dynamically adjusted LSTM cell that can capitalize
on its existing internal gates and states and process the
information ﬂow without any lags. In Fig. 2 we see an
overview of the OPTM-LSTM cell. This cell contains the
same number of gates and states as the original LSTM cell
with the only difference that just before the generation of
the two output tensors (i.e., hidden and cell states at time
t) we employ a feature importance mechanism based on an
internal non-forecasting supervised regression which acts
as a feature importance guarantor. The guarantor considers
only the current and already known mid-price and it will
highlight the most important feature among ft,it,~ct,ot,
ct, andht. This mid-price will be the label/target for the
internal non-forecasting supervised problem and it will
deﬁne the importance/order of the LSTM’s cell states and
gates. This importance mechanism is utilizing the GD as
the learning weights optimizer.
The internal supervised problem is not a forecasting
problem but a calibration problem. That means that the
provided labels represent the current LOB mid-price.
This approach offers a robust calibration of the feature
repository (or else Feature Repo in Fig. 2) selection
mechanism based on GD. The GD learning algorithm will
converge after a few iterations (i.e., we tested that seven
to ten iterations sufﬁce) with the input data vector being
the current full LOB data state. The feature repository5
Fig. 2: OPTM-LSTM Cell: The optimum output LSTM cell contains the same number of internal gates and states and
the same number of outputs as the LSTM prototype. The only difference is that just before we release the two output
tensors (i.e., hidden and cell states) we group (i.e., feature repository or Feature Repo) all the internal gates and states
and perform a feature importance calculation based on a non-forecasting supervised regression task. We then select the
most important feature according to the lowest MSE. The graph does not include any biases intentionally since we did
not want to overload the visuals. Note: The OPTM-LSTM is designed in a three-dimensional space. More speciﬁcally,
every gate and state are presented as three-dimensional tensors but this is only for convenience to demonstrate the
different building blocks. In reality, these are two-dimensional tensors with a batch size of 1.
then extracts the best gate or state and this will be the
ﬁnal/optimal output named hidden state ht. The cell state
ctstays intact. We observed that the replacement of the cell
statectby the second most important feature (according to
the MSE score) did not exhibit signiﬁcant improvements
to the overall ﬁnal objective, which is the prediction of the
next mid-price (i.e., overall regression objective outside the
OPTM-LSTM cell).
A closer look inside the Feature Repo (i.e., feature
repository) block can be seen in Fig. 3. The Feature
Repo block contains several critical components which
are: the collection of the six internal gates and states,
the gradient weights which are updated based on the GD
learning algorithm, the labels that reﬂect only the current
mid-price (i.e., non-forecasting regression problem), and
the input time-step tensor xt. The combination of these
components will highlight which state or gate is the most
important feature by averaging the highest price in terms
of the learned weights per internal gate or state. We
need to mention that the GD weights are not part of the
backpropagation through time training method (BPTT) -
the mechanism that optimizes the LSTM’s trainable parts
Wf,Wi,W~c,Wo,Uf,Ui,U~c, andUo- see Appendix A
for a detailed derivation of the BPTT process.
The mechanics of the Feature Repo block are expressed
mathematically based on the following three steps:
1) For the input xt2R1D, where the ﬁrst dimension
represents the batch size and Dthe number of input
features (e.g., current LOB state) and Uthe number of
hidden units, we concatenate all the internal LSTM gates
and states which are ft,it,~ct,ot,ct, and ht, in one vectorrt2R1C, whereC= 6Uis the total number of
the most recently calculated internal gates and states in
relation to the number of hidden units, as follows:
rt= [ftjitj~ctjotjctjht] (7)
2) For the non-forecasting label yt2R11that represents
the present and already known mid-price and the vector rt
the iterative process of the GD algorithm (see Algorithm
1) unfolds, as follows:
Algorithm 1 Online Gradient Descent Algorithm
Require: Learning rate , number of iterations I, initial
parameter, non-forecasting labels yt
1:fori= 1 toIdo
2: Compute the predicted labels ^yt=Trt
3: Compute the error =^yt yt
4: Compute the gradient rJ()=2(rT
terror )
5: Update the parameter   rJ()
6:end for
Ensure: The optimized parameter vector 
with the parameter 2RC1.
3) The trained parameter is the Importance Weights
Vector orIWV (see Fig. 3) and it will be partitioned
based on the six gates and states. Each of the components
of this partition represents one of the six gates and states
with respect to the number of hidden units U. We then6
Fig. 3: From left to right: the Feature Repo is a block within the OPTM-LSTM cell and contains the mechanism that
will decide which state (or gate) will replace the default LSTM’s cell output. It is developed in three stages: the ﬁrst
stage is the GD method that contains the features (i.e., LSTM’s cell states and gates) and the guarantor (i.e., the current
and already known mid-price that acts as a non-forecasting label) which are calculated based on matrix multiplications
and vector differences. After a few iterations, the GD will achieve convergence and the ﬁnal vector of the learned
weights, which we call importance vector, will then be stored for the next phase. The next phase is taking place outside
the GD algorithm and is related to the importance vector which contains the learned weights per number of units for
every LSTM’s states and gates. Every state (or gate) has 1Unumber of weights and by calculating their average
we can identify which state (or gate) is more important. This leads to the ﬁnal step which is OPTM-LSTM’s cell ﬁnal
output. Now that we know which gate (or state) is the most important based on their average weight we will replace
the ﬁnalhtstate with the htnewwhich can be any of the ft,it,~ct,ot,ct, andht. The ﬁnal cell state ct(see bottom
right corner) remains the same since we did not notice any signiﬁcant overall forecasting improvements by replacing it
with the second most important state or gate. Note: The Feature Repo is designed in a three-dimensional space. More
speciﬁcally, every gate (or state), the gradient weights block, the label, or the importance weights vector, are presented
as three-dimensional tensors but this is only for convenience to demonstrate the different building blocks. In reality,
these are two-dimensional tensors with a batch size of 1.
calculate the average importance per component AIc, as
follows:
AIc=1
UcUX
j=(cU) (U 1)IWV j (8)
wherec=1, 2, 3, 4, 5, 6 and then we select the component
with the highest AIcvalue. This component will be the
optimum output (or htnew) of the OPTM-LSTM cell.
A. Complexity
The LSTM cell is very efﬁcient in terms of time and
space complexity. More speciﬁcally, the LSTM cell is
local in time and space, which means that every update
per timestep is independent of the input sequence length.
LSTM’s cell time and space complexity per timestep is
O(W) where Wis the number of its parameters and it is
equal to W= 4U2+ 4UI+UO+ 3U, where
Uis the number of hidden cells, Iis the number of input
units, and Ois the number of the output units. The newly
introduced OPTM-LSTM cell contains, in terms of com-
putational complexity, an additional term that representsthe GD’s online updates. As a result, OPTM-LSTM’s time
and space complexities are equal to O(W+ND)and
O(W+N), respectively, where D= 6UandNis the
number of input sample points.
The input sample points are based on a two-dimensional
tensor with dimensions equal to the number of hidden cell
unitsthe six gates/states for the ﬁrst dimension and
the number of LOB features for the second dimension.
An additional critical point is that GD’s complexity anal-
ysis is based on a batch size with a single item since
the OPTM-LSTM performs online reading of the input
data. An additional consideration for the OPTM-LSTM
complexity analysis is the complexity (time and space)
of the backward pass which is based on the BPTT (see
Appendix A) and has O(N2L)andO(NL)time and
space complexities, respectively, with N being the number
of network units and Lbeing the full timestep length. The
BPTT is clear from the GD step since the Feature Repo
block (see Fig. 2) is not participating in the backward
learning process.7
B. Motivation
Now that we have a deeper understanding of what the
OPTM-LSTM does, we will provide the motivate behind
the development of the optimum output LSTM cell. Two
key components led us to the creation of the OPTM-
LSTM, one is theoretical and the other one is practical:
Theoretical : The main advantage of the LSTM archi-
tecture is to retain the provided information and delay
(or avoid) the vanishing and exploding gradients. If
we want to reduce even further the probability of the
vanishing (or exploding) gradients we have to pay
attention to the derivative of the cell state ~cat the
trading event t(see Eq. 38 in Appendix A) which
contains the forget gate with a range between 0and
1. The vanishing and exploding gradient problem
refer to the case where the RNN/LSTM during the
BPTT process is governed by small or large derivative
elements - a process that is directly connected to the
length of the input sequence (i.e., look-back period).
As a result, the forget gate just slows down [59], but
does not eliminate, the vanishing of gradients phe-
nomenon where the rest of the cell state’s gradients
are responsible for the exploding gradients. There are
always methods that can hold the gradients within
boundaries (e.g., gradient clipping [57] or restricted
initialization [58]) but these methods are not native to
the LSTM cell and further experimental calibration is
required. The OPTM-LSTM by shufﬂing strategically
LSTM’s cell ﬁnal output delays (or avoids) even
further the possibility of uncontrolled gradients.
Experimental : An in vitro experimental approach of
LSTM’s cell internal gates and states exhibited a
remarkable characteristic of their behaviour. More
speciﬁcally, by extracting the internal states and gates
from their LSTM cell (i.e., in vitro approach) per
trading event and treating them as trained features,
we noticed a constant feature importance alternation
while trying to forecast, in an online manner, LOB’s
mid-price target price. More speciﬁcally, we extracted
LSTM’s cell components and we considered them
as trading signals. Instead of relying on the full
input feature vector length, we performed a feature
importance approach based on the GD learning al-
gorithm. The result was that the feature importance
(i.e., LSTM’s internal gates and states) frequency
alternation was constant and independent of the stock
proﬁle. Based on that evidence, we developed the
Feature Repo block (see Fig. 3) within the existing
LSTM cell.
IV. E XPERIMENTS
In this section, we present details and performance re-
sults of the OPTM-LSTM cell based on an online HFT
forecasting experimental protocol. In that protocol, wepredict the value of the next mid-price. The OPTM-LSTM
will compete against several key RNN developments such
as prototype LSTM, bidirectional LSTMs, LSTMs with
attention layer, GRUs, a hybrid model based on LSTM
and CNN, and two naive models (i.e., naive/baseline
regressor and the persistence algorithm which considers
a ﬂat tick-by-tick mid-price prediction). More speciﬁcally,
the naive/baseline regressor is based on the idea that the
targets in the training set will be a constant value and
that value will be utilized for the MSE calculation for the
targets in the test set. The inputs to this naive model are
the full LOB data but they do not have any contribution
to the forecasting process. The second baseline model
(i.e., persistence algorithm) is developed according to a
forecasting function that predicts (i.e., ﬂat prediction) that
the next target price is the same as the current target
priceSt+1=St, whereSrepresents the target price.
The input values to this model will be just the mid-
price (i.e., single feature) that we will formally deﬁne in
the following section. The experimental section unfolds
as follows: Initially, we introduce some general concepts
about the input data which is based on HFT LOBs, then
we describe details of the online experimental protocol and
we conclude the section based on tables with performance
measures in terms of MSE scores. We present MSE score
results based on raw data and two normalization settings
(i.e., MinMax and Zscore - two methods that are based
on the minimum and maximum values and zero mean and
unit standard deviation per input, respectively).
A. HFT LOB
LOB is a vital component for the ML trader that wants to
extract information about the interplay between liquidity
and price for both the ask and bid sides. Every trading
side (i.e., ask or bid) is divided into several price levels
(e.g., in our case every LOB has 10 price levels for every
side) that change asynchronously. The most sensitive and
desirable prices, in terms of the current trading activity,
are the best ask and bid prices that form the so-called
spread. The spread is the distance between the highest bid
and the lowest ask price. Averaging these two prices (i.e.,
best ask and best bid price) we form the mid-price (MP),
which is an artiﬁcial indicator (i.e., it is not a traded index)
but can be utilized as a sensitive proxy for forecasting
tasks. The mid-price is extracted for every trading event.
Formally, the mid-price at trading event tcan be calculated
as follows:
MPt=PAt+PBt
2; (9)
whereMPtis the mid-price at the trading event t,PAt
andPBtare the best ask and bid prices, respectively. A
critical point here is the perception of the trading event t.
More speciﬁcally, the time perception in the HFT space
should be avoided and instead, only the trading events8
should be considered as measurements of sequential trad-
ing progression in an intra-day trading setting. This is due
to the time-frequency abnormalities in the HFT universe
(i.e., trading events that arrive simultaneously or exhibit
extensive trading inactivity for several milliseconds). To
tackle these time irregularities we perceive every trading
event as independent information that is part of the LOB
inventory. LOB’s current ask and bid price levels and
their corresponding inventory states (i.e., volume per LOB
level) will be the forces that will trigger the changes in the
OPTM-LSTM output information. That information will
be utilized to predict the next mid-price at the trading event
t+ 1.
B. Experimental Protocol and Datasets
The main objective of our experimental protocol is to
forecast the next LOB’s mid-price based on tick-level
HFT data without any information lag. This is an online
regression forecasting task that considers every trading
event in LOB. That means that we do not sample the
data and the forecasting of the next mid-price considers
only the latest information arrival (i.e., only the current
state of the LOB). Although the ML trader will perform
only the aforementioned forecasting regression task, an
additional internal and fully-automated non-forecasting
regression task (i.e., the guarantor mechanism) is taking
place within the OPTM-LSTM cell. This internal non-
forecasting regression problem will arrange and verify the
importance order of the cell’s internal gates and states. Our
experimental ML protocol is based on HFT LOB datasets
tested on two high-liquid US stocks, Amazon and Google,
and two less-liquid Nordic stocks, Kesko and Wartsila
obtained from NASDAQ and followed the ITCH protocol
- a protocol that offers advanced visibility of NASDAQ’s
equities markets, and it ensures ultra-low latency for the
market data feeds. To avoid memory loss due to decimal
points we multiplied the LOB price levels by 10,000. The
historical period that we cover is the ﬁrst two trading
months of the year 2015 for the two US stocks and two
trading months of the year 2010 (i.e., from the 1st of June
until the end of July) for the two Nordic stocks.
We use a progressive training scenario which is up to
20,000,000 trading events for the selected US stocks (i.e.,
this is approximately two months’ worth of trading data for
the two high-liquid stocks US stocks) and up to 5,000,000
trading events for the two Nordic stocks (i.e., 5,000,000
trading events is approximately two months worth of
trading data for the two less-liquid Nordic stocks). The
motivation for selecting a training set of up to 20,000,000
for the two US stocks and 2,000,000 trading events for
the two Nordic stocks (i.e., despite the availability of
5,000,000 trading events for Wartsila) is the fact that the
selected models reached a plateau or in some cases, there
was a performance drop after that speciﬁc input data size,
respectively. Since we deal with ultra-fast phase executionswe need to ﬁnd every model’s plateau performance. For the
testing set, we use 1,000 trading events that we also eval-
uate progressively. The motivation for the selected testing
length is the fact that the span of these 1,000 trading events
can vary from several events that arrive simultaneously
(i.e., same timestamp registration) to events that are up
to eight minutes apart. The progressive testing framework
covers 1,000 trading events with every iteration (i.e., new
testing trading event) acting as the latest training event
in the next forecasting step. In simple terms, the training
set absorbs progressively/sequentially every new testing
event. The testing performance is measured based on a
sequentially-stored MSE score that we average at the end
of that testing period.
The reported models’ performance results are based on
an extensive topological and hyper-parameter grid search
for optimal MSE scores. More speciﬁcally, we limited the
grid search to up to four hidden layers for the ﬁve RNN
competitors (i.e., prototype LSTM, LSTM with attention,
bidirectional, GRU, and hybrid model) and up to two
hidden layers for OPTM-LSTM. The reason for these
depth limitations stems from the fact that we operate under
an HFT setting and the topological grid search for the
ﬁve RNN competitors is quite extensive. This grid search
considers as hyperparameters numerous combinations in
terms of the number of hidden units, optimizers, dropout
levels, a look-back period, and batch sizes. The topological
grid search limitations to the OPTM-LSTM model were
motivated by the reported MSE performance of competi-
tors’ results. That means that we applied an early stopping
performance mechanism when we achieved an overall (i.e.,
against the ﬁve RNN) lower MSE score. We report in
detail the optimal topologies in Section IV-C. The results
reported progressively based on several data size scenarios
The motivation for this type of performance reporting is
that HFT requires ultra-fast data digestion/learning and
we need to know what is the optimal minimum data size
per model. Based on the concept of ultra-fast data diges-
tion/learning we provide an additional layer of reporting
which is related to the number of training epochs and we
name them Long Training (i.e., training up to 60 epochs)
and Short Training (i.e., training up to 5 epochs). The
Long Training and the Short Training settings will give
us an insight into every model’s behaviour with respect to
data requirements and learning time.
Lastly, to scrutinize the effectiveness of the OPTM-
LSTM model we provide MSE scores based on raw data
(i.e., for several data size scenarios) and two normalization
settings (i.e., MinMax and Zscore based on speciﬁc and
critical for the MSE performance data size scenarios) for
two different input feature sets: the ﬁrst feature set is the
full LOB data (i.e., 40 features which are the ask and bid
price levels and their corresponding volume levels) and
the second feature set considers only the mid-price. These
two different features groups are suitable for employing9
two naive regressors such as the naive/baseline regressor
(i.e., based on the full LOB data input) and the persistence
algorithm (i.e., based only on the mid-price) for a ﬂat
tick-by-tick forecasting comparison against the ﬁve RNN
competitors and the OPTM-LSTM model. We conclude
our experimental setting with a self-comparison of the
OPTM-LSTM performance. That means that we compare
the Long Training with the Short Training MSE scores
for our model and check its ability to learn fast and based
on smaller datasets. We complement our result tables with
plots for a visual interpretation of our ﬁndings.
The motivation for these two progressive training set-
tings is to investigate whether the OPTM-LSTM cell
is robust across higher and lower trading volume stock
examples and also to check whether it provides better
performance compared to the other RNN developments.
Additionally, this two-phased experimental protocol tries
to answer a critical question for the HFT ML trader: can
a small (i.e., in terms of size), but well-trained (i.e., a
high number of epochs), dataset compete with a larger,
but less trained, dataset? The restrictions in the previous
questions stem from the fact that the HFT ML trader
requires the fastest forecasting ability (i.e., model’s per-
formance plateau) together with the shortest training time
(i.e., minimum optimal data size). In simple terms, we
want to cover the minimum distance between the model’s
performance, input data size, and the number of epochs.
C. Results
The experimental objective of our OPTM-LSTM cell is
to ﬁnd its best/plateau performance by relying only on a
very small portion of the given data and achieve lower
MSE scores against its well-established competitors (i.e.,
ﬁve RNN models based on the prototype LSTM, LSTM
with attention, bidirectional LSTM, GRU and a hybrid
model based on an LSTM-CNN architecture) and the
two naive forecasting models (i.e., naive/baseline regressor
and persistence algorithm). We also offer an additional
self-comparison between the Short and Long Training
settings for the OPTM-LSTM model. The Short and Long
Training experiments refer to the process where we use
just ﬁve epochs (i.e., Short Training) against a longer
training process of up to 60 epochs (i.e., Long Training).
The batch size is selected to be minimal for the OPTM-
LSTM and equal to 1. This is possible since our optimum
output cell allows for optimized communication between
the connected OPTM-LSTM cells. This optimized cell
communication gives us an additional line of customiza-
tion which is the selection of the look-back period length.
In our experiments, we also select the look-back period to
be 1 (i.e., only the current LOB data).
We employ these ﬁve RNN competitors against our
new cell and we curate them based on a wide and fully-
automated topological grid search range. That means that
we run an extensive topological grid search for thesemodels and report the best candidates in terms of the
lowest MSE score. The reported models consider the
following as hyper-parameters: the depth which is limited
to four hidden layers, the number of hidden units that
varies from 8 to 512, the type of optimizer (e.g., Adam,
Nadam, RMSProp, and Stochastic Gradient Descent or
SGD), the batch size which varies from 1 to 64, the use
(or not) of Dropout percentage (e.g., 0.20 or 0.50), and a
look-back period of 1, 5 or 10 past trading LOB states.
Our OPTM-LSTM model hyper-parametrization considers
the following, as hyperparameters: the depth is limited to
two hidden layers, the number of hidden units varies from
4 to 512, and the type of optimizer (e.g., Adam, Nadam,
RMSProp, and SGD). Our model considers a batch size
and a look-back period of 1. We need to mention that
the optimal models’ topologies are stock speciﬁc and that
can be seen in Table I and Table IV. The ﬁve RNN
models and the baseline/naive models can be found in the
tables as: ’LSTM’ for the prototype LSTM, ’Attention’
for the LSTM with the attention layer, ’Bidirectional’
for the bidirectional LSTM, ’GRU’ for the GRU RNN,
’Hybrid’ for the hybrid LSTM-CNN model, ’Baseline’ for
the naive/baseline model based on the LOB data input, and
’Persistence’ for the persistence model based on the mid-
price as input.
We see that the OPTM-LSTM requires fewer building
blocks compared to its well-established competitors. The
effectiveness of the OPTM-LSTM cell is highlighted based
on the development of a two-phased experimental setting
that unfolds, as follows: the ﬁrst phase, based on Short
Training, our optimum output LSTM cell competes with
the best candidate models (see Table I and Table IV)
via a progressive training process which incrementally
goes from 1,000 trading events up to 20,000,000 trading
events for the two high-liquid stocks (i.e., Amazon and
Google) and from 1,000 trading events up to 2,000,000
trading events for the less-liquid Nordic stocks. The second
phase is based on Long Training equipped with an active
Early Stopping mechanism, the same models, and the same
progressive training process that goes from 1,000 to 15,000
trading events for both sets of stocks.
Considering this question, we present our ﬁndings (i.e.,
MSE scores) per stock in Table II - Table XIV in the
following order:
Two MSE score tables (i.e., Short and Long ex-
perimental protocols) that contain several data size
scenarios based on raw LOB data as input for the
OPTM-LSTM and the ﬁve RNN competitors. It is
important to mention that these data size scenarios
will highlight the optimal minimal sample length that
is required for the models to operate effectively in
HFT.
According to the optimal minimal sample length
above, two additional multidimensional MSE score
tables (i.e., Benchmark Training and Benchmark10
TABLE I: Best-performing candidates based on a topological grid search for the US stocks.
Stock Model Topology Stock Model Topology
Amazon LSTM LSTM layer with 32 units Google LSTM LSTM layer with 32 units
Dropout 50% Dropout 50%
Dense layer with 1 unit Dense layer with 1 unit
RMSProp optimizer Adam optimizer
Batch size of 32 samples Batch size of 32 samples
LSTMLSTM layer with 40 units LSTM LSTM layer with 64 units
withPReLU with PReLU
AttentionAttention layer Attention Attention layer
Dense layer with 40 units Dense layer with 32 units
Dense layer with 1 unit Dense layer with 1 unit
Nadam optimizer Nadam optimizer
Batch size of 64 samples Batch size of 64 samples
BidirectionalBidirectional LSTM layer with 32 units Bidirectional Bidirectional LSTM layer with 32 units
RNNDense layer with 32 units RNN Dense layer with 32 units
Dropout 50% Dropout 50%
Dense layer with 4 units Dense layer with 4 units
Dense layer with 1 unit Dense layer with 1 unit
Adam optimizer Adam optimizer
Batch size of 32 samples Batch size of 64 samples
GRUGRU with 32 units GRU GRU with 32 units
Dense layer with 32 units GRU with 32 units
Dense layer with 1 unit Dense layer with 32 units
Adam Optimizer Dense layer with 1 unit
Batch size of 32 samples Adam Optimizer
Batch size of 32 samples
Hybrid1D Convolution layer with with 60 ﬁlters, 6 as kernel size Hybrid 1D Convolution layer with with 60 ﬁlters, 6 as kernel size
RelU activation function MaxPooling1D
LSTM layer with 64 units with Tanh activation function LSTM layer with 64 units with Tanh activation function
LSTM layer with 64 units with Tanh activation function LSTM layer with 64 units with Tanh activation function
Dense layer with 30 units with ReLU activation function Dense layer with 30 units with ReLU activation function
Dense layer with 10 units with ReLU activation function Dense layer with 10 units with ReLU activation function
Dense layer with 1 unit Dense layer with 1 unit
Nadam Optimizer Nadam Optimizer
Batch size of 32 samples Batch size of 64 samples
OPTM-LSTMOPTM-LSTM layer with 64 units OPTM-LSTM OPTM-LSTM layer with 8 units
Dense layer with 4 units Dense layer with 4 units
Dense layer with 1 unit Dense layer with 1 unit
Adam optimizer Adam optimizer
Batch size of 1 sample Batch size of 1 sample
Testing) will summarize/compare the OPTM-LSTM
against the two baseline models and the ﬁve RNN
competitors, under two different input settings (i.e.,
LOB data and mid-price) and two normalization
settings (i.e., MinMax and Zscore).
For space economy, we present the case of Google
in Table II and Table III in the the main body of the
text and the rest of the stocks (i.e., Amazon, Kesko, and
Wartsila) can be found in Appendix B. The Short and Long
experimental results tables are combined with time-series
plots (e.g., Fig. 4 and Fig. 5).
Table II and Table III and their corresponding time-
series plots reveal several critical performance advantages
of the OPTM-LSTM cell. The ﬁrst point is that this new
cell achieves better performance across all the selected
stock examples. Its performance is stable for the majority
of the provided data sizes for both stock categories (i.e.,
high-liquid and less-liquid stocks). More speciﬁcally, there
were very few cases for Amazon stock (see Table VI)
that the hybrid model, despite its huge score ﬂuctuations,
approached the performance of the OPTM-LSTM NN.
This performance similarity (i.e., the similarity between
OPTM-LSTM and the hybrid model) stems from the fact
that the hybrid model was oscillating heavily across many
MSE scores. That oscillation was probably due to the fact
that CNNs are non-linear functions which means that smallchanges in the input source can cause signiﬁcant changes
in the output. In terms of testing performance, the OPTM-
LSTM NN achieved better results compared to the rest of
the models in both experimental protocol (i.e., protocols
based on a lower and higher number of epochs) settings
and all the selected stocks. The majority of the models
reached their plateau performance before the 20,000,000
trading events for the high-liquid stocks and 2,000,000 for
the less-liquid stocks, respectively.
OPTM-LSTM was able to achieve lower MSE scores
compared to its competitors because it can immediately
recognize every next price or volume ﬂuctuation by the
internal guarantor (i.e., non-forecasting internal mecha-
nism) that the cell is equipped and it is able to online
re-adjust/optimize its cell output. Another important point
is that the OPTM-LSTM NN has a very simple topology.
The winning topology was based on just one hidden layer,
with only a few units, with minimal look-back history and
batch size. This is important since the HFT ML trader
will utilize a simple and light NN structure. Despite the
fact that the OPTM-LSTM has a higher computational and
time complexity per cell unit compared to the prototype
LSTM, it processes the data ﬂow in a similar or even
faster manner since its structural elements (i.e., the look-
back period, number of layers, and units) require less
training parameters. Additionally, the internal optimization11
TABLE II: Google MSE scores under the Short experimental protocol.
Stock Size Model MSE - Train Stock Size Model MSE - Train Stock Size Model MSE - Train
Google 1,000 OPTM-LSTM 1.86157E+13 Google 2,000 OPTM-LSTM 6.81980E+04 Google 3,000 OPTM-LSTM 2.82656E+14
LSTM 3.04603E+13 LSTM 5.34948E+14 LSTM 3.69276E+14
Attention 5.94603E+13 Attention 4.34950E+14 Attention 3.69279E+14
Bidirectional 3.94602E+13 Bidirectional 5.44951E+14 Bidirectional 3.69279E+14
GRU 7.94603E+13 GRU 5.34951E+14 GRU 3.69279E+14
Hybrid 2.42938E+13 Hybrid 9.79887E+14 Hybrid 9.00659E+14
4,000 OPTM-LSTM 2.75419E+14 5,000 OPTM-LSTM 2.26636E+14 6,000 OPTM-LSTM 1.93647E+14
LSTM 2.86605E+14 LSTM 2.37062E+14 LSTM 2.04036E+14
Attention 2.86606E+14 Attention 2.37062E+14 Attention 2.04038E+14
Bidirectional 2.86606E+14 Bidirectional 2.37062E+14 Bidirectional 2.04038E+14
GRU 2.86606E+14 GRU 2.37062E+14 GRU 2.04038E+14
Hybrid 1.70827E+15 Hybrid 9.83038E+14 Hybrid 1.22734E+15
7,000 OPTM-LSTM 1.68468E+14 10,000 OPTM-LSTM 1.13063E+14 15,000 OPTM-LSTM 9.39333E+13
LSTM 2.10449E+14 LSTM 1.38019E+14 LSTM 1.16983E+14
Attention 1.80449E+14 Attention 1.38012E+14 Attention 1.16973E+14
Bidirectional 1.80449E+14 Bidirectional 1.38012E+14 Bidirectional 1.16973E+14
GRU 1.80449E+14 GRU 1.38012E+14 GRU 1.16973E+14
Hybrid 2.06190E+15 Hybrid 1.09899E+15 Hybrid 8.66925E+14
20,000 OPTM-LSTM 8.79208E+13 35,000 OPTM-LSTM 4.04241E+13 50,000 OPTM-LSTM 2.03210E+13
LSTM 8.85471E+13 LSTM 6.73828E+13 LSTM 5.88844E+13
Attention 8.85388E+13 Attention 6.73811E+13 Attention 5.88940E+13
Bidirectional 8.85380E+13 Bidirectional 6.73812E+13 Bidirectional 5.88937E+13
GRU 8.85379E+13 GRU 6.73805E+13 GRU 5.88918E+13
Hybrid 2.13339E+15 Hybrid 2.18678E+15 Hybrid 1.51111E+15
100,000 OPTM-LSTM 1.09730E+13 400,000 OPTM-LSTM 5.00414E+12 800,000 OPTM-LSTM 1.55909E+12
LSTM 4.88444E+13 LSTM 4.08671E+13 LSTM 3.91895E+13
Attention 4.88383E+13 Attention 4.01958E+13 Attention 3.62271E+13
Bidirectional 4.88300E+13 Bidirectional 3.98711E+13 Bidirectional 3.70604E+13
GRU 4.88339E+13 GRU 3.97324E+13 GRU 3.95741E+13
Hybrid 1.81421E+15 Hybrid 2.15967E+15 Hybrid 1.73484E+15
1,000,000 OPTM-LSTM 1.25909E+12 10,000,000 OPTM-LSTM 1.05909E+12 20,000,000 OPTM-LSTM 9.59093E+11
LSTM 3.99895E+13 LSTM 3.99995E+13 LSTM 4.06759E+13
Attention 3.60522E+13 Attention 3.60009E+13 Attention 3.62271E+13
Bidirectional 3.70007E+13 Bidirectional 3.67804E+13 Bidirectional 3.64675E+13
GRU 3.97990E+13 GRU 3.92581E+13 GRU 3.90696E+13
Hybrid 1.63679E+15 Hybrid 1.56326E+15 Hybrid 1.85684E+15
Stock Size Model MSE - Test Stock Size Model MSE - Test Stock Size Model MSE - Test
Google 1,000 OPTM-LSTM 1.04441E+15 Google 2,000 OPTM-LSTM 3.32290E+13 Google 3,000 OPTM-LSTM 2.60755E+13
LSTM 1.05494E+15 LSTM 3.94267E+13 LSTM 3.93324E+13
Attention 1.05495E+15 Attention 3.94266E+13 Attention 3.93324E+13
Bidirectional 1.05495E+15 Bidirectional 3.94266E+13 Bidirectional 3.93324E+13
GRU 1.05494E+15 GRU 3.94267E+13 GRU 3.93324E+13
Hybrid 1.11003E+15 Hybrid 1.12736E+15 Hybrid 1.26887E+15
4,000 OPTM-LSTM 2.02484E+13 5,000 OPTM-LSTM 1.39428E+13 6,000 OPTM-LSTM 1.75926E+13
LSTM 3.93318E+13 LSTM 3.92138E+13 LSTM 3.91251E+13
Attention 3.93315E+13 Attention 3.92136E+13 Attention 3.91260E+13
Bidirectional 3.93316E+13 Bidirectional 3.92136E+13 Bidirectional 3.91256E+13
GRU 3.93318E+13 GRU 3.92139E+13 GRU 3.91256E+13
Hybrid 1.58088E+15 Hybrid 5.37368E+14 Hybrid 1.52489E+15
7,000 OPTM-LSTM 4.60990E+12 10,000 OPTM-LSTM 5.14485E+10 15,000 OPTM-LSTM 3.40823E+11
LSTM 3.91154E+13 LSTM 3.91224E+13 LSTM 3.91048E+13
Attention 3.91166E+13 Attention 3.91234E+13 Attention 3.91057E+13
Bidirectional 3.91162E+13 Bidirectional 3.91234E+13 Bidirectional 3.91058E+13
GRU 4.91163E+13 GRU 3.91234E+13 GRU 3.91058E+13
Hybrid 4.78848E+14 Hybrid 3.71959E+14 Hybrid 5.93297E+14
20,000 OPTM-LSTM 5.39592E+09 35,000 OPTM-LSTM 2.05396E+12 50,000 OPTM-LSTM 3.13280E+10
LSTM 3.90233E+13 LSTM 3.91767E+13 LSTM 3.89640E+13
Attention 3.90265E+13 Attention 3.91775E+13 Attention 3.89657E+13
Bidirectional 9.90264E+13 Bidirectional 3.91775E+13 Bidirectional 3.89653E+13
GRU 7.90257E+13 GRU 3.91765E+13 GRU 3.89625E+13
Hybrid 7.41977E+14 Hybrid 2.33358E+14 Hybrid 5.49750E+14
100,000 OPTM-LSTM 1.20071E+09 400,000 OPTM-LSTM 3.62520E+09 800,000 OPTM-LSTM 4.74797E+09
LSTM 3.84952E+13 LSTM 3.83070E+13 LSTM 3.82016E+13
Attention 3.84821E+13 Attention 3.70009E+13 Attention 3.31423E+13
Bidirectional 3.84672E+13 Bidirectional 3.63139E+13 Bidirectional 3.44671E+13
GRU 3.84753E+13 GRU 3.60069E+13 GRU 2.86685E+13
Hybrid 1.83510E+09 Hybrid 1.44407E+15 Hybrid 1.41009E+15
1,000,000 OPTM-LSTM 4.85674E+09 10,000,000 OPTM-LSTM 4.94797E+09 20,000,000 OPTM-LSTM 5.67455E+09
LSTM 3.82005E+13 LSTM 3.80002E+13 LSTM 3.82016E+13
Attention 3.20579E+13 Attention 3.20015E+13 Attention 3.21794E+13
Bidirectional 3.35623E+13 Bidirectional 3.14658E+13 Bidirectional 3.02094E+13
GRU 2.51040E+13 GRU 2.27702E+13 GRU 2.05398E+13
Hybrid 1.57883E+15 Hybrid 1.35003E+15 Hybrid 1.22934E+15
mechanism and the gates/states selection are part only of
the forward step and no additional computational cost is
attached to the backpropagation step since these parame-
ters are trained locally (i.e., they do not participate in the
overall chained partial derivation rule).
An additional interpretation of the lower MSE scores
that the OPTM-LSTM achieved arrives from the fact that
it can capitalize on the LOB structure better compare to
the rest of the RNN family. In particular, every LOB
time instance is a collection of current and previous
price and volume levels. For instance, we utilized for our
experiments LOBs with 10 price levels, which means that
there are 10 price and volume levels for the bid and ask
sides, respectively. The best levels, which form the mid-price, are more informed compared to the levels deeper in
the book which potentially carry past information. These
deeper LOB levels can act as a look-back period for our
OPTM-LSTM cell. The same information is also available
to the other models but they have already trained and
developed a stale topology that may carry noise.
On a different note, the OPTM-LSTM cell performs an
internal cell feature selection based on the current and pre-
vious LOB’s state without exposure to the disadvantages
of the vanishing and exploding gradients. The gradient
problem is a problem that RNNs suffer from due to the use
of sequential inputs (i.e., time-steps or look-back period).
LSTMs, despite the common perception that they do not
suffer from that problem, are exposed to that risk since12
TABLE III: Google MSE scores under the Long experimental protocol.
Size Stock Model MSE - Train Size Stock Model MSE - Train Size Stock Model MSE - Train
Google 1,000 OPTM-LSTM 7.13393E+12 Google 2,000 OPTM-LSTM 1.95355E+13 Google 3,000 OPTM-LSTM 3.62966E+14
LSTM 1.92849E+13 LSTM 1.95598E+13 LSTM 3.69272E+14
Attention 1.94585E+13 Attention 1.95592E+13 Attention 3.69269E+14
Bidirectional 1.94585E+13 Bidirectional 1.97603E+13 Bidirectional 3.69270E+14
GRU 1.94583E+13 GRU 1.96591E+13 GRU 3.69269E+14
Hybrid 5.15419E+13 Hybrid 6.22681E+13 Hybrid 6.43307E+14
4,000 OPTM-LSTM 2.18488E+14 5,000 OPTM-LSTM 2.70888E+14 6,000 OPTM-LSTM 1.80689E+14
LSTM 2.86598E+14 LSTM 2.37052E+14 LSTM 2.04031E+14
Attention 2.86591E+14 Attention 2.37038E+14 Attention 2.03984E+14
Bidirectional 2.86589E+14 Bidirectional 2.37032E+14 Bidirectional 2.03993E+14
GRU 2.86549E+14 GRU 2.37033E+14 GRU 2.03987E+14
Hybrid 7.24462E+14 Hybrid 1.12468E+15 Hybrid 1.36507E+15
7,000 OPTM-LSTM 1.67232E+14 10,000 OPTM-LSTM 1.13227E+14 15,000 OPTM-LSTM 5.04511E+13
LSTM 1.80438E+14 LSTM 1.37966E+14 LSTM 1.16907E+14
Attention 1.80375E+14 Attention 1.37812E+14 Attention 8.07426E+13
Bidirectional 1.80392E+14 Bidirectional 1.37815E+14 Bidirectional 1.10559E+14
GRU 1.80364E+14 GRU 1.37761E+14 GRU 1.70163E+14
Hybrid 1.89508E+15 Hybrid 1.99974E+15 Hybrid 2.19058E+15
Stock Size Model MSE - Test Stock Size Model MSE - Test Stock Size Model MSE - Test
Google 1,000 OPTM-LSTM 1.02565E+15 Google 2,000 OPTM-LSTM 2.21936E+13 Google 3,000 OPTM-LSTM 2.42504E+13
LSTM 1.04581E+15 LSTM 1.05493E+15 LSTM 3.93302E+13
Attention 1.05495E+15 Attention 1.05495E+15 Attention 3.93206E+13
Bidirectional 1.05495E+15 Bidirectional 1.05495E+15 Bidirectional 8.93262E+13
GRU 1.05494E+15 GRU 1.05493E+15 GRU 3.69269E+14
Hybrid 1.03878E+15 Hybrid 1.14449E+15 Hybrid 1.33994E+15
4,000 OPTM-LSTM 6.48726E+12 5,000 OPTM-LSTM 7.15408E+12 6,000 OPTM-LSTM 1.67019E+12
LSTM 3.93236E+13 LSTM 3.92020E+13 LSTM 3.91175E+13
Attention 3.93125E+13 Attention 3.91765E+13 Attention 3.90502E+13
Bidirectional 3.93107E+13 Bidirectional 3.91813E+13 Bidirectional 3.90623E+13
GRU 3.93068E+13 GRU 3.91734E+13 GRU 3.90501E+13
Hybrid 1.33515E+15 Hybrid 4.39233E+14 Hybrid 9.10956E+14
7,000 OPTM-LSTM 2.64051E+12 10,000 OPTM-LSTM 3.48733E+11 15,000 OPTM-LSTM 1.74703E+09
LSTM 3.90955E+13 LSTM 3.90843E+13 LSTM 3.90295E+13
Attention 3.90155E+13 Attention 3.88844E+13 Attention 2.17723E+13
Bidirectional 3.90497E+13 Bidirectional 3.88841E+13 Bidirectional 3.01027E+13
GRU 3.90018E+13 GRU 3.88263E+13 GRU 3.94140E+13
Hybrid 2.82755E+14 Hybrid 1.38452E+15 Hybrid 1.46894E+15
they just prevent or delay the gradients from a fast-paced
vanishing (see Eq. 5 and Eq. 38). It is also very difﬁcult
to identify the ideal length of the look-back period but
the OPTM-LSTM cell solves that problem by viewing the
input space in different dimensions and free for the batch
size. In simple terms, it discards the previous timesteps
and focuses on the present LOB state (i.e., present ask
and bid LOB levels). That way every batch of size 1
is updated according to the present LOB’s supply and
demand and hedges the HFT ML trader from a potential
vanishing gradient risk. As a result, the backpropagation
process treats every OPTM-LSTM cell as a separate NN
and not as a part of a chain rule calculation.
We also noticed that all RNN models, including the
OPTM-LSTM, are sensitive to the higher mid-price vari-
ance. For instance, for Google stock, we see in Fig. 4 and
Fig. 5 that all models exhibit higher MSE scores between
2,000 and 3,000 data samples compare to Amazon (see
Fig. 6 and Fig. 7) that for the same data size samples the
models performed better. This is due to a combination of
two things: the short data size length and the variability
of the mid-price. More speciﬁcally, Google and Amazon
exhibit different mid-price variances under the same short
data size. For instance, the mid-price variance for 3,000
data samples for Google and Amazon are 7.37E+14 and
5.78E+12 (without any data transformation), respectively.
Under the same comparison for 5,000 data samples, the
variances are 2.03E+14 (Google) and 5.66E+12 (Amazon)
which is a drop in variance of 72% and 2%, respectively.
The variance acts as a threshold for all the models’
performance during training, especially in the case ofshorter trading horizons (i.e., smaller data samples). This
rapid variance drop plays a signiﬁcant role in the testing
performance which follows the training set. That means
that in the case of Google stock, the models will be tested
in a set with 72% less mid-price variability (the rest of the
LOB price levels follow a similar variance proﬁle).
Next, Table V contains the MSE score of the OPTM-
LSTM NN against two baseline methods and the ﬁve RNN
models under two normalization settings and the raw data.
The OPTM-LSTM exhibited the same stable behaviour
as in the Short Training and Long Training experimental
protocols. The newly introduced cell was also able to
achieve better results against the two baseline models. To
get the reported MSE scores we run again a smaller scale
topological grid search for all the RNN and the OPTM-
LSTM models. Among the three different input settings
(i.e., raw, MinMax, and Zscore) we found that the Zscore
normalization was the most challenging setting. Under that
setting the differences across the majority of the models
were very close to each other but the OPTM-LSTM cell
performed better than the rest. The reason is that the cell
is very adaptive in terms of learning and in particular,
we refer to the learning rate within the cell. The cell is
equipped with an internal GD method with a ﬁxed learning
rate that can be adjusted according to the selected stock.
For instance, in the case of Google, the learning rate was
adjusted to 0.0001 and that hyperparameter tuning helped
us to achieve better results.
The last part of our experimental analysis belongs to
the self-comparison between the Short Training against
the Long Training experimental protocols. To get a better13
understanding of that comparison we utilize again the case
of Google and we isolate the MSE scores per data size for
both protocols - Fig. 12. It is obvious from the MSE scores
that the higher the number of epochs the lower the MSE
scores. Amazon, Kesko, and Wartsila exhibited a similar
response (see Appendix B). If the ML trader can use up to
15,000 trading events and additional time capacity for the
higher number of epochs (e.g., approximately 0.4 seconds
per epoch for the 15,000 trading events according to a
single GPU machine based on the NVIDIA Ampere A100)
then the Long Training is more suitable for the forecasting
task otherwise if there is a capacity for additional data
samples but limited time then the Short Training protocol
is better.
Finally, we should mention that the overall forecasting
behaviour, apart from the OPTM-LSTM which achieved
better results, of the ﬁve RNN competitors and the two
baseline models follows the literature in terms of forecast-
ing performance. For instance, authors in [38] conclude
that the prototype LSTM is robust across a wide range
of datasets and any modiﬁcations to its internal topology
(e.g., removal of gates) did not improve the performance
of the prototype LSTM. This is something that we also
noticed in the behaviour of the ﬁve RNN competitors. The
performance of the RNN family models exhibited similar
performance behaviour across a wide range of data size,
normalization, and feature scenarios to the baseline models
and in particular the persistence algorithm which can be
identiﬁed as a reliable predictor. We noticed a similar be-
haviour of this baseline model against the prototype LSTM
as the authors in [60]. Similarly, authors in [61] compared
the prototype LSTM against the persistence algorithm with
the LSTM outperforming the baseline model.
D. Limitations and Future Research
Although our OPTM-LSTM NN offers better forecast-
ing performance in the online HFT universe there are a
few limitations that we need to mention. One limitation
is the restricted number of stocks and trading horizons.
Despite the fact that we utilized indicative examples of
high-liquid and less-liquid stocks we believe that a wider
selection of stocks will provide more insights into the
behaviour of the OPTM-LSTM model. Another limitation
of our study is the selected trading horizon. We limited the
selected number of trading events only when we realize a
steady decrease in the MSE score performance. Potentially,
an even lower MSE score could have been achieved by
incorporating a larger trading horizon. Last but not least,
we believe that a more advanced optimization method
can be utilized as a part of the OPTM-LSTM cell. Apart
from the aforementioned limitations, the development of
the OPTM-LSTM cell opens additional research avenues.
For instance, the same internal cell architecture can be
extended for classiﬁcation tasks. The current internal re-
gression supervised problem is connected directly to theﬁnal forecasting objective which is a regression problem.
It will be very interesting to utilize several classiﬁcation
scenarios as the cell’s internal supervised problem and
connect them to a ﬁnal classiﬁcation objective. The present
work is classiﬁed as a narrow artiﬁcial intelligence (AI)
application. We believe that our research developments
here can be utilized for other online tasks such as vision
and fully autonomous or full-self driving tasks.
V. C ONCLUSION
The task of online learning and forecasting in the HFT
space is challenging and we believe to the best of our
knowledge that the existing literature has not addressed
that challenge fully. In particular, we propose an optimum
output LSTM cell architecture for the task of HFT LOB’s
mid-price forecasting via an online experimental protocol.
The motivation for developing this optimum output cell is
that several machine learning and deep learning models
utilize a stale approach to training and learning. That
means that the ML trader builds a predictive model which
has a stale topology during the training/learning process
and is disconnected from the actual forecasting objective.
We named the new cell Optimum Output LSTM (OPTM-
LSTM) cell because we capitalize on the existing LSTM
cell architecture by reshufﬂing strategically and online the
ﬁnal cell’s output. This new cell is equipped with an
internal supervised and non-forecasting regression task,
which acts as a guarantor for the ranking of the internal
states and gates. This process is taking place online which
means that the optimum output cell is directly connected
to the latest information in relation to the predicted value.
We tested the effectiveness of our new cell on two high-
liquid stocks (i.e., Amazon and Google) and two less-
liquid stocks (i.e., Kesko and Wartsila) via several data
size scenarios and training routines. Our method achieved
better performance in terms of MSE scores against a wide
range of RNN and baseline models. We believe that our
work opens additional research avenues not only in the
space of HFT online AI learning but hopefully in other
ﬁelds such as vision and fully-autonomous driving.
ACKNOWLEDGMENTS
The authors would like to thank CSC-IT Center for Sci-
ence, Finland, for the generous computational resources.
REFERENCES
[1] S. Hochreiter, and J. Schmidhuber, “Long short-term memory”,
Neural Computation , vol. 9, issue 8, pp. 1735-1780, Nov 1997.
[2] T. Zia, and U. Zahid (2019), ”Long short-term memory recurrent
neural network architectures for Urdu acoustic modeling”, Interna-
tional Journal of Speech Technology , vol. 22, issue 1, pp. 21-30,
Mar 2019
[3] A. Zeyer, P. Doetsch, P. V oigtlaender, R. Schl ¨uter, and H. Ney,“
Comprehensive study of deep bidirectional LSTM RNNs for acous-
tic modeling in speech recognition”, In 2017 IEEE international
conference on acoustics, speech and signal processing (ICASSP) ,
pp. 2462-2466, Mar 2017.14
[4] Q. Wang, C. Downey, L. Wan, P. A. Mansﬁeld, and I. L. Moreno,
“Speaker diarization with LSTM”, In IEEE International Con-
ference on Acoustics, Speech and Signal Processing (ICASSP) ,
Calgary, 2018, pp. 5239-5243.
[5] P. Zhou, W. Shi, J. Tian, Z. Qi, B. Li, H. Hao, and B. Xu,
“Attention-based bidirectional long short-term memory networks
for relation classiﬁcation” In Proceedings of the 54th annual
meeting of the association for computational linguistics , vol. 2
Short papers, pp. 207-212, Aug 2016.
[6] O. Melamud, J.. Goldberger, and I. Dagan, (2016, August). “con-
text2vec: Learning generic context embedding with bidirectional
lstm”, In Proceedings of the 20th SIGNLL conference on compu-
tational natural language learning , Berlin, 2016, pp. 51-61.
[7] G. Liu, and J. Guo, (2019), “Bidirectional LSTM with attention
mechanism and convolutional layer for text classiﬁcation” Neuro-
computing , vol. 337, pp. 325-338, Apr 2019.
[8] K. M. Tsiouris, V . C. Pezoulas, M. Zervakis, S. Konitsiotis, D. D.
Koutsouris, and D. I. Fotiadis, “A long short-term memory deep
learning network for the prediction of epileptic seizures using EEG
signals”, Computers in biology and medicine , vol. 99, pp. 24-37,
Aug 2018.
[9] ¨O. Yildirim, “A novel wavelet sequence based on deep bidirectional
LSTM network model for ECG signal classiﬁcation”, Computers
in biology and medicine , vol 96, pp. 189-202, May 2018.
[10] S. L. Oh, E. Y . Ng, R. San Tan, and U. R. Acharya, “Automated
diagnosis of arrhythmia using combination of CNN and LSTM
techniques with variable length heart beats”, Computers in biology
and medicine , vol. 102, pp. 278-287, Nov 2018.
[11] R. R. Varior, B. Shuai, J. Lu, D. Xu, and G. Wang, “A siamese
long short-term memory architecture for human re-identiﬁcation””,
In European conference on computer vision , Cham, 2016, pp. 135-
153.
[12] X. Liang, X. Shen, J. Feng, L. Lin, and S. Yan,(2016, October),
“Semantic object parsing with graph lstm”, In European Confer-
ence on Computer Vision, Springer , Cham, 2016, pp. 125-143.
[13] H. Xue, D. Q. Huynh, and M. Reynolds, “SS-LSTM: A hierarchical
LSTM model for pedestrian trajectory prediction”, In 2018 IEEE
Winter Conference on Applications of Computer Vision (WACV),
IEEE , Nevada, 2018, pp. 1186-1194.
[14] T. Fischer, and C. Krauss, “Deep learning with long short-term
memory networks for ﬁnancial market predictions”, European
Journal of Operational Research , vol. 270, issue 2, pp. 654-669,
Oct 2018.
[15] W. Bao, J. Yue, and Y . Rao, “A deep learning framework for
ﬁnancial time series using stacked autoencoders and long-short term
memory”, PloS One , vol. 12, issue 7, Jul 2017.
[16] R. Akita, A. Yoshihara, T. Matsubara, and K. Uehara, “Deep learn-
ing for stock prediction using numerical and textual information”,
In 2016 IEEE/ACIS 15th International Conference on Computer
and Information Science (ICIS), IEEE , Okayama, 2016, pp. 1-6.
[17] D. M. Nelson, A. C. Pereira, and R. A. De Oliveira, “Stock market’s
price movement prediction with LSTM neural networks”, In 2017
International joint conference on neural networks (IJCNN), IEEE ,
Anchorage, 2017, pp. 1419-1426.
[18] M. Fabbr, and G. Moro, “Dow Jones Trading with Deep Learning:
The Unreasonable Effectiveness of Recurrent Neural Networks”,
Data , pp. 142-153, Jul 2018.
[19] J. A. Sirignano, “Deep learning for limit order books”, Quantitative
Finance , vol. 19, issue 4, pp. 549-570, Apr 2019.
[20] J. Sirignano, and R. Cont, “Universal features of price formation in
ﬁnancial markets: perspectives from deep learning”, Quantitative
Finance , vol 19, issue 9, pp. 1449-1459, Sep 2019.
[21] Z. Zhang, S. Zohren, and S. Roberts, “Deeplob: Deep convolutional
neural networks for limit order books”, IEEE Transactions on
Signal Processing , vol 67, issue 11, 3001-3012, Mar 2019.
[22] A. Ntakaris, G. Mirone, J. Kanniainen, M. Gabbouj and A. Iosiﬁdis,
“Feature Engineering for Mid-Price Prediction With Deep Learn-
ing”, IEEE Access , vol. 7, pp. 82390-82412, Jun 2019
[23] A. Ntakaris, M. Magris, J. Kanniainen, M. Gabbouj, A. and Iosi-
ﬁdis. “Benchmark dataset for mid-price forecasting of limit order
book data with machine learning methods”, Journal of Forecasting,
vol. 37, issue 8, pp.852-866, Dec 2018.[24] A. Tsantekidis, N. Passalis, A. Tefas, J. Kanniainen, M. Gabbouj
and A. Iosiﬁdis,“Using deep learning to detect price change indica-
tions in ﬁnancial markets, ” 2017 25th European Signal Processing
Conference (EUSIPCO), 2017, pp. 2511-2515.
[25] M. Dixon, “Sequence classiﬁcation of the limit order book using
recurrent neural networks”, Journal of computational science, vol.
24, pp. 277-286, Jan 2018.
[26] A. Tsantekidis, N. Passalis, A. Tefas, J. Kanniainen, M. Gabbouj,
and A. Iosiﬁdis, “Using deep learning for price prediction by
exploiting stationary limit order book features””, Applied Soft
Computing, vol. 93, Aug 2020.
[27] Y. Li, L. Li, X. Zhao, T. Ma, Y. Zou, and M. Chen, “An Attention-
Based LSTM Model for Stock Price Trend Prediction Using Limit
Order Books”, In Journal of Physics: Conference Series, vol. 1575,
issue. 1, pp. 012124, Jun 2020.
[28] Y. M ¨akinen, J. Kanniainen, M. Gabbouj, and A. Iosiﬁdis, “Fore-
casting jump arrivals in stock prices: new attention-based network
architecture using limit order book data”, Quantitative Finance,
vol. 19, issue 12, pp. 2033-2050, Dec 2019.
[29] T. Sun, D. Huang, and J. Yu, “Market Making Strategy Optimization
via Deep Reinforcement Learning”, IEEE Access, vol. 10, pp. 9085-
9093, Jan 2022.
[30] R. Jozefowicz, W. Zaremba, and I. Sutskever, “An empirical ex-
ploration of recurrent network architectures”, In International
conference on machine learning, PMLR, 2015, pp. 2342-2350.
[31] Y. Lu, and F . M. Salem, “Simpliﬁed gating in long short-term
memory (lstm) recurrent neural networks”, In 2017 IEEE 60th
International Midwest Symposium on Circuits and Systems (MWS-
CAS) Medford, 2017, pp. 1601-1604.
[32] C. Tallec, and Y. Ollivier, “Can recurrent neural networks warp
time?”, arXiv preprint arXiv:1804.11188, Mar 2013.
[33] A. Gu, C. Gulcehre, T. Paine, M. Hoffman, and R. Pascanu,
“Improving the gating mechanism of recurrent neural networks”,
In International Conference on Machine Learning, PMLR, 2020,
pp. 3800-3809).
[34] S. Zhang, Y. Wu, T. Che, Z. Lin, R. Memisevic, R. R. Salakhutdinov,
and Y. Bengio, “Architectural complexity measures of recurrent
neural networks”, Advances in neural information processing sys-
tems, vol. 29, 2016
[35] M. Arjovsky, A. Shah, and Y. Bengio, “Unitary evolution recurrent
neural networks”, In International Conference on Machine Learn-
ing, PMLR, New York, 2016 pp. 1120-1128.
[36] A. G. Ororbia II, T. Mikolov and D. Reitter, “Learning Simpler
Language Models with the Differential State Framework”, in Neu-
ral Computation, vol. 29, no. 12, pp. 3327-3352, Dec. 2017.
[37] Q. V . Le, N. Jaitly, and G. E. Hinton, “A simple way to initial-
ize recurrent networks of rectiﬁed linear units”, arXiv preprint
arXiv:1504.00941, Apr 2015
[38] K. Greff, R. K. Srivastava, J. Koutn ´ık, B. R. Steunebrink, and J.
Schmidhuber, J., “LSTM: A search space odyssey”, Transactions
on neural networks and learning systems, IEEE, vol. 28, issue 10,
pp. 2222-2232, Jul 2016.
[39] F . A. Gers, J. Schmidhuber, and F . Cummins, “Learning to forget:
Continual prediction with LSTM”, Neural computation, vol 12,
issue 10, pp. 2451-2471, Oct 2000.
[40] D. T. Tran, A. Iosiﬁdis, J. Kanniainen and M. Gabbouj,“Temporal
Attention-Augmented Bilinear Network for Financial Time-Series
Data Analysis”, Transactions on Neural Networks and Learning
Systems, vol. 30, no. 5, pp. 1407-1418, May 2019.
[41] Z. He, S. Gao, L. Xiao, D. Liu, H. He, and D. Barber, “Wider
and deeper, cheaper and faster: Tensorized lstms for sequence
learning”, in Advances in neural information processing systems,
NIPS, California, 2017.
[42] D. Krueger, T. Maharaj, J. Kram ´ar, M. Pezeshki, N. Ballas, N. R.
Ke, N.R., A. Goyal, Y. Bengio, A. Courville, A. and C. Pal, “Zone-
out: Regularizing rnns by randomly preserving hidden activations”,
arXiv preprint arXiv:1606.01305, June 2016.
[43] D. Neil, M. Pfeiffer, and S. C. Liu, “Phased lstm: Accelerating
recurrent network training for long or event-based sequences”, Ad-
vances in neural information processing systems,NIPS, Barcelona,
2016.
[44] M. Fraccaro, S. K. Sønderby, U. Paquet, and O. Winther, “Se-
quential neural models with stochastic layers”, Advances in neural
information processing systems, NIPS, Barcelona, 2016.15
[45] K. Yao, T. Cohn, K. Vylomova, K. Duh, and C. Dyer, “Depth-gated
recurrent neural networks”, arXiv preprint arXiv:1508.03790, Aug.
2015.
[46] R. Dangovski, L. Jing, P . Nakov, M. Tatalovi ´c, and M. Solja ˇci´c,
“Rotational unit of memory: a novel representation unit for RNNs
with scalable applications”, Transactions of the Association for
Computational Linguistics, vol. 7, pp. 121-138, Aug 2019.
[47] Y. LeCun, L. Bottou, Y. Bengio, and P . Haffner 1998. “Gradient-
based learning applied to document recognition”. In of Proceedings
of the IEEE, 86(11), pp.2278-2324, Nov 1998. Vancouver
[48] F . Chollet, 2015, “keras”, Github, GitHub repository, https://
github.com/fchollet/keras, commit: 5bcac37
[49] M. Abadi, A. Agarwal, P . Barham, E. Brevdo, Z. Chen, C. Citro,
G.S. Corrado, A. Davis, J. Dean, M. Devin, and S. Ghemawat,
2016. Tensorﬂow: “Large-scale machine learning on heterogeneous
distributed systems”. arXiv preprint arXiv:1603.04467.
[50] S. Mootha, S. Sridhar, R. Seetharaman, and S. Chitrakala, 2020.
“Stock price prediction using bi-directional LSTM based sequence
to sequence modeling and multitask learning”. In 11th IEEE An-
nual Ubiquitous Computing, Electronics & Mobile Communication
Conference, UEMCON, New York, 2020.
[51] M.A.I. Sunny, M.M.S.Maswood, and A.G., Alharbi, 2020, October.
“Deep learning-based stock price prediction using LSTM and
bi-directional LSTM model”. In Novel Intelligent and Leading
Emerging Sciences Conference”, (NILES), Egypt, 2020.
[52] K.A. Althelaya, E.S.M. El-Alfy, and S. Mohammed. “Evaluation of
bidirectional LSTM for short-and long-term stock market predic-
tion”. In 9th international conference on information and commu-
nication systems, (ICICS), France, 2018.[53] W. Lu,, J. Li, Y. Li, A. Sun, and J. Wang, ”A CNN-LSTM-based
model to forecast stock prices. Complexity, 2020.
[54] S., Mehtab, and J. Sen. “Stock price prediction using CNN and
LSTM-based deep learning models. In International Conference on
Decision Aid Sciences and Application (DASA), IEEE, Bahrain,
2020.
[55] J.M.T. Wu, Z. Li, N. Herencsar, B. Vo, and J.C.W. Lin. “A graph-
based CNN-LSTM stock price prediction algorithm with leading
indicators. Multimedia Systems, pp.1-20, Feb. 2021.
[56] Z. Nourbakhsh, and N. Habibi. “Combining LSTM and CNN
methods and fundamental analysis for stock price trend prediction.
Multimedia Tools and Applications, 2022.
[57] T. Mikolov, “Statistical Language Models Based on Neural Net-
works”, PhD Thesis, Department of Computer Graphics and Mul-
timedia, BRNO University of Technology, Brno, Czechia, 2012
[58] M. Mehdipour Ghazi, M. Nielsen, A. Pai, M. Modat, M.J. Cardoso,
S. Ourselin, S. and L. Sorensen. “On the initialization of long short-
term memory networks”. In International Conference on Neural
Information Processing (pp. 275-286). Springer, Cham, Dec. 2019
[59] H.Y.S. Chien, J.S. Turek, N. Beckage, V .A. Vo, C.J. Honey, and
T.L. Willke. “Slower is Better: Revisiting the Forgetting Mech-
anism in LSTM for Slower Information Decay”. arXiv preprint
arXiv:2105.05944, May 2021.
[60] B. Laperre, J. Amaya, and G. Lapenta. “Dynamic time warping as
a new evaluation for dst forecast with machine learning”. Frontiers
in Astronomy and Space Sciences, , p.39. 2020
[61] G. G ¨urses-Tran, and A. Monti. “Advances in Time Series Fore-
casting Development for Power Systems” Operation with MLOps.
Forecasting, 4(2), pp.501-524. May 2022.16
APPENDIX A
LSTM’s backward pass, named Backpropagation Through
Time (BPTT), is the learning/update mechanism of all the
trainable parameters which are Wf,Wi,W~c,Wo,Uf,
Ui,U~c, andUoincluding biases. Despite the fact that we
mainly utilize a many-to-one approach in our experimental
protocol we explain below the general idea that the loss
is calculated and then summed up for the entire look-back
period. In the many-to-one case, the remaining steps of
each individual timestep that the temporal is not consid-
ered can be assumed by the ML trader that they are equal
to zero. We will explain the BPTT process based on the
partial derivatives w.r.t. (i.e., with respect to): (1) internal
states (i.e., candidate gate, cell state, and hidden state),
(2) internal gates (i.e., output gate, forget gate, and input
gate), and (3) shared weights (i.e., V,W, andU). At this
stage we should introduce the notation V which represents
the shared weights between LSTM’s individual timestep
prediction and the selected loss function - in our case will
be the MSE. We will also use for our partial derivations the
linear activation transformation which is suitable for our
regression task and comes as a default option for the Dense
layer in TensorFlow. Dimensions of the internal gates and
states and their equivalent partial derivatives retain their
dimensional proﬁle throughout BPTT.
Hidden state ht:
@Lt
@ht=@Lt
@^yt@^yt
@ht; (10)
@Lt
@^yt=@
@^yt[1
2(yt ^yt)2] = ^yt yt; (11)
@^yt
@ht=@
@ht[Vht] =V; (12)
@Lt
@ht= (^yt yt)VT: (13)
Cell statect:
@Lt
@ct=@Lt
@^yt@^yt
@ht@ht
@ct; (14)
where:
@ht
@ct=@
@ct[ottanh (ct)] =ot(1 tanh2(ct));(15)
and based on (13) and (15) the loss Lw.r.t. the cell
statecat timetis:
@Lt
@ct= (^yt yt)VTot(1 tanh2(ct)):(16)
Candidate gate ~ct:
@Lt
@~ct=@Lt
@^yt@^yt
@ht@ht
@ct@ct
@~ct; (17)
where:
@ct
@~ct=@
@~ct[ftct 1+it~ct] =it; (18)and based on (13) and (14) the loss Lw.r.t. the
candidate gate ~cat timetis:
@Lt
@~ct= (^yt yt)2VTot(1 tanh2(ct))it;(19)
Output gate ot:
@Lt
@ot=@Lt
@^yt@^yt
@ht@ht
@ot; (20)
where:
@ht
@ot=@
@ot[ottanh (ct)] =tanh (ct); (21)
and based on (13) and (21) the loss Lw.r.t. the output
gateoat timetis:
@Lt
@ot= (^yt yt)2VTtanh (ct): (22)
Input gate it:
@Lt
@it=@Lt
@^yt@^yt
@ht@ht
@ct@ct
@it; (23)
where:
@ct
@it=@
@it[ftct 1+it~ct] = ~ct; (24)
and based on (17) and (24) the loss Lw.r.t. the input
gateiat timetis:
@Lt
@it= (^yt yt)VTot(1 tanh2(ct))~ct:(25)
Forget gate ft:
@Lt
@ft=@Lt
@^yt@^yt
@ht@ht
@ct@ct
@ft; (26)
where:
@ct
@ft=@
@ft[ftct 1+it~ct] =ct 1; (27)
and based on (17) and (27) the loss Lw.r.t. the forget
gatefat timetis:
@Lt
@ft= (^yt yt)VTot(1 tanh2(ct))ct 1:
(28)
whereLtis the temporal loss function MSE =1
2(yt 
^yt)2at timestep t, ytis the provided label, ^ytis the
predicted value, and is the dot product.
Next, we present the partial derivatives w.r.t. the trainable
parameters/weights V , W, and U, where V represents the
shared weights between LSTM’s cell output htand the
predicted value ^ytat time instance, Wrepresents the
weights between the input tensor xtand the internal gates,
andUrepresents the weights between the previous hidden
stateht 1and the internal gates at time instance t. More
speciﬁcally:17
Weights V : For the entire length of the look-back
period the loss function w.r.t. Vis:
@L
@V=TX
t=1@Lt
@^yt@^yt
@V=TX
t=1hT
t(^yt yt):(29)Weights U :W is a concatenation of the
LSTM’s cell matrices Wi,Wf,Wo, and
W~c. The loss function Lwill be updated
w.r.t. these individual weights, as follows:
@L
@Ui=TX
t=1@Lt
@it@it
@Ui=TX
t=1(h
(^yt yt)VTot(1 tanh2(ct))~ct)iT
it(1 it))
ht 1; (30)
@L
@Uf=TX
t=1@Lt
@ft@ft
@Uf=TX
t=1(h
(^yt yt)VTot(1 tanh2(ct))ct 1)iT
ft(1 ft))
ht 1;(31)
@L
@Uo=TX
t=1@Lt
@ot@ot
@Uo=TX
t=1(h
(^yt yt)VT(tanh (ct))iT
ot(1 ot))
ht 1; (32)
@L
@U~ct=TX
t=1@Lt
@~ct@~c
@U~c=TX
t=1(h
(^yt yt)VTot(1 tanh2(ct))itiT
(1 ~c2
t))
ht 1: (33)
Weights W : The partial derivative of the loss function
Lw.r.t. the components of the weight matrix Wis:
@L
@Wi=TX
t=1@Lt
@it@it
@Wi=TX
t=1(
xT
t(^yt yt)VTot(1 tanh2(ct))~ct)
it(1 it); (34)
@L
@Wf=TX
t=1@Lt
@ft@ft
@Wf=TX
t=1(
xT
t(^yt yt)VTot(1 tanh2(ct))ct 1)
ft(1 ft); (35)
@L
@Wo=TX
t=1@Lt
@ot@ot
@Wo=TX
t=1(
xT
t(^yt yt)2VTtanh (ct))
ot(1 ot); (36)
@L
@W~c=TX
t=1@Lt
@~ct@~ct
@W~c=TX
t=1(
xT
t(^yt yt)2VTot(1 tanh2(ct))it)
~ct(1 ~ct): (37)
The generalized temporal loss Lw.r.t. toUandWat
any time instance tcell in the BPTT process, is:
@Lt
@U=@Lt
@^yt@^yt
@ht@ht
@ct"TY
t=2@ct
@ct 1#
@c1
@U;and@Lt
@W=@Lt
@^yt@^yt
@ht@ht
@ct"TY
t=2@ct
@ct 1#
@c1
@W: (38)
APPENDIX B18
TABLE IV: Best-performing candidates based on a topological grid search for the Finnish stocks.
Stock Model Topology Stock Model Topology
Kesko LSTM LSTM layer with 32 units Wartsila LSTM LSTM layer with 32 units
Dense layer with 1 unit Dense layer with 1 unit
Adam optimizer Adam optimizer
Batch size of 32 samples Batch size of 32 samples
LSTMLSTM layer with 32 units LSTM LSTM layer with 32 units
withPReLU with PReLU
AttentionAttention layer Attention Attention layer
Dense layer with 16 units Dense layer with 16 units
Dense layer with 1 unit Dense layer with 1 unit
Adam optimizer Adam optimizer
Batch size of 32 samples Batch size of 32 samples
BidirectionalBidirectional LSTM layer with 32 units Bidirectional Bidirectional LSTM layer with 32 units
RNNDense layer with 32 units RNN Dense layer with 32 units
Dropout 50% Dropout 50%
Dense layer with 4 units Dense layer with 4 units
Dense layer with 1 unit Dense layer with 1 unit
Adam optimizer Adam optimizer
Batch size of 32 samples Batch size of 32 samples
GRUGRU with 32 units GRU GRU with 32 units
Dense layer with 32 units GRU with 32 units
Dense layer with 1 unit Dense layer with 32 units
Adam Optimizer Dense layer with 1 unit
Batch size of 32 samples Adam Optimizer
Batch size of 32 samples
Hybrid1D Convolution layer with with 8 ﬁlters, 6 as kernel size Hybrid 1D Convolution layer with with 8 ﬁlters, 6 as kernel size
RelU activation function RelU activation function
LSTM layer with 64 units with Tanh activation function LSTM layer with 64 units with Tanh activation function
LSTM layer with 64 units with Tanh activation function LSTM layer with 64 units with Tanh activation function
Dense layer with 10 units with ReLU activation function Dense layer with 10 units with ReLU activation function
Dense layer with 1 unit Dense layer with 1 unit
Adam Optimizer Adam Optimizer
Batch size of 32 samples Batch size of 32 samples
OPTM-LSTMOPTM-LSTM layer with 4 units OPTM-LSTM OPTM-LSTM LSTM layer with 4 units
Dense layer with 4 units Dense layer with 4 units
Dense layer with 1 unit Dense layer with 1 unit
Adam optimizer Adam optimizer
Batch size of 1 sample Batch size of 1 sample
TABLE V: Google Benchmark Training (left) and Benchmark Testing (right). Data sample
is 35,000 trading events.
Input Normalization Model MSE - Train Input Normalization Model MSE - Test
LOB Data Raw OPTM-LSTM 4.04241E+13 LOB Data Raw OPTM-LSTM 2.05396E+12
LSTM 6.73657E+13 LSTM 3.91887E+13
Attention 6.72123E+13 Attention 3.91715E+13
Bidirectional 6.73657E+13 Bidirectional 3.91545E+13
GRU 6.73774E+13 GRU 3.91948E+13
Hybrid 2.18984E+15 Hybrid 2.33644E+14
Baseline 5.76034E+15 Baseline 1.38235E+14
MinMax OPTM-LSTM 1.88756E-05 MinMax OPTM-LSTM 1.31094E-05
LSTM 3.79995E-05 LSTM 3.99546E-05
Attention 5.71432E-05 Attention 4.94253E-05
Bidirectional 3.72352E-05 Bidirectional 3.59546E-05
GRU 4.15867E-05 GRU 3.30894E-05
Hybrid 6.56392E-05 Hybrid 6.78675E-05
Baseline 6.81553E-05 Baseline 6.92356E-05
Zscore OPTM-LSTM 1.61746E+02 Zscore OPTM-LSTM 1.59013E+02
LSTM 1.63453E+02 LSTM 2.34154E+02
Attention 1.64778E+02 Attention 6.94510E+02
Bidirectional 1.64456E+02 Bidirectional 4.56984E+02
GRU 1.63756E+02 GRU 5.65567E+02
Hybrid 4.66898E+02 Hybrid 6.72002E+02
Baseline 6.32211E+02 Baseline 6.40224E+02
Mid-price Raw OPTM-LSTM 4.31765E+13 Mid-price Raw OPTM-LSTM 4.36219E+13
LSTM 6.74887E+13 LSTM 7.22948E+13
Attention 6.74443E+13 Attention 7.42857E+13
Bidirectional 6.70889E+13 Bidirectional 7.88225E+13
GRU 6.70366E+13 GRU 7.90564E+13
Hybrid 6.68133E+13 Hybrid 8.98765E+13
Persistence 9.32445E+13 Persistence 9.92332E+13
MinMax OPTM-LSTM 5.72756E-03 MinMax OPTM-LSTM 5.95884E-03
LSTM 2.50001E-02 LSTM 2.51443E-02
Attention 5.05911E-02 Attention 5.21542E-02
Bidirectional 5.45328E-02 Bidirectional 6.12984E-02
GRU 3.42819E-02 GRU 3.01012E-02
Hybrid 7.69334E-02 Hybrid 8.95335E-02
Persistence 5.75984E-01 Persistence 6.36985E-01
Zscore OPTM-LSTM 1.19774E+02 Zscore OPTM-LSTM 1.20403E+02
LSTM 1.22889E+02 LSTM 1.25776E+02
Attention 1.22980E+02 Attention 1.26503E+02
Bidirectional 1.24435E+02 Bidirectional 1.28245E+02
GRU 1.22025E+02 GRU 1.27121E+02
Hybrid 1.29885E+02 Hybrid 2.35335E+02
Persistence 2.90325E+02 Persistence 2.99894E+0219
(a) OPTM-LSTM training MSE scores
 (b) OPTM-LSTM testing MSE scores
(c) LSTM training MSE scores
 (d) LSTM testing MSE scores
(e) Attention LSTM training MSE scores
 (f) Attention LSTM testing MSE scores
(g) Bidirectional training MSE scores
 (h) Bidirectional testing MSE scores
(i) GRU training MSE scores
 (j) GRU testing MSE scores
(k) Hybrid training MSE scores
 (l) Hybrid testing MSE scores
Fig. 4: Google Short MSE scores based on Table II.20
(a) OPTM-LSTM training MSE scores
 (b) OPTM-LSTM testing MSE scores
(c) LSTM training MSE scores
 (d) LSTM testing MSE scores
(e) Attention LSTM training MSE scores
 (f) Attention LSTM testing MSE scores
(g) Bidirectional training MSE scores
 (h) Bidirectional testing MSE scores
(i) GRU training MSE scores
 (j) GRU testing MSE scores
(k) Hybrid training MSE scores
 (l) Hybrid testing MSE scores
Fig. 5: Google Long MSE scores based on Table III.21
TABLE VI: Amazon MSE scores under the Short experimental protocol.
Size Stock Model MSE - Train Size Stock Model MSE - Train Size Stock Model MSE - Train
Amazon 1,000 OPTM-LSTM 2.37755E+13 Amazon 2,000 OPTM-LSTM 1.91119E+13 Amazon 3,000 OPTM-LSTM 2.11565E+13
LSTM 2.43245E+13 LSTM 2.23752E+13 LSTM 2.18732E+13
Attention 2.43227E+13 Attention 2.23726E+13 Attention 2.18664E+13
Bidirectional 2.43231E+13 Bidirectional 2.08533E+13 Bidirectional 2.18677E+13
GRU 2.43227E+13 GRU 2.08469E+13 GRU 2.18628E+13
Hybrid 2.94457E+13 Hybrid 2.49406E+13 Hybrid 3.04949E+13
4,000 OPTM-LSTM 2.20394E+13 5,000 OPTM-LSTM 2.10221E+13 6,000 OPTM-LSTM 2.08704E+13
LSTM 2.26918E+13 LSTM 2.21002E+13 LSTM 2.19382E+13
Attention 2.26825E+13 Attention 2.20843E+13 Attention 2.19207E+13
Bidirectional 2.26816E+13 Bidirectional 2.20871E+13 Bidirectional 2.19159E+13
GRU 2.26779E+13 GRU 2.20792E+13 GRU 2.19059E+13
Hybrid 3.09348E+13 Hybrid 2.02846E+13 Hybrid 2.30043E+13
7,000 OPTM-LSTM 2.16789E+13 10,000 OPTM-LSTM 2.13176E+13 15,000 OPTM-LSTM 2.06974E+13
LSTM 2.21532E+13 LSTM 2.30184E+13 LSTM 2.37194E+13
Attention 2.21188E+13 Attention 2.29426E+13 Attention 2.35976E+13
Bidirectional 2.21234E+13 Bidirectional 2.29432E+13 Bidirectional 2.36162E+13
GRU 2.21123E+13 GRU 2.29194E+13 GRU 2.35377E+13
Hybrid 2.30576E+13 Hybrid 3.08916E+13 Hybrid 1.98247E+13
20,000 OPTM-LSTM 1.93105E+13 35,000 OPTM-LSTM 1.61412E+13 50,000 OPTM-LSTM 8.65632E+12
LSTM 2.52202E+13 LSTM 2.68694E+13 LSTM 2.76307E+13
Attention 2.49042E+13 Attention 2.56509E+13 Attention 2.40673E+13
Bidirectional 2.47769E+13 Bidirectional 2.51290E+13 Bidirectional 2.44334E+13
GRU 2.47721E+13 GRU 2.51239E+13 GRU 2.39928E+13
Hybrid 1.89910E+13 Hybrid 1.58455E+13 Hybrid 1.99902E+13
100,000 OPTM-LSTM 4.42847E+12 400,000 OPTM-LSTM 7.48735E+11 800,000 OPTM-LSTM 7.09238E+11
LSTM 2.84573E+13 LSTM 2.86594E+13 LSTM 2.84907E+13
Attention 1.72512E+13 Attention 1.67204E+12 Attention 1.22332E+12
Bidirectional 1.79991E+13 Bidirectional 1.11909E+12 Bidirectional 1.21469E+12
GRU 1.79168E+13 GRU 1.08493E+12 GRU 1.10944E+12
Hybrid 1.36274E+13 Hybrid 3.13572E+13 Hybrid 2.12974E+13
1,000,000 OPTM-LSTM 5.57388E+11 10,000,000 OPTM-LSTM 5.09864E+11 20,000,000 OPTM-LSTM 4.70387E+11
LSTM 2.85123E+13 LSTM 2.86813E+13 LSTM 2.87918E+13
Attention 1.20907E+12 Attention 1.10192E+12 Attention 1.03817E+12
Bidirectional 1.23501E+12 Bidirectional 1.24928E+12 Bidirectional 1.25935E+12
GRU 1.24944E+12 GRU 1.17564E+12 GRU 1.19487E+12
Hybrid 3.56355E+13 Hybrid 2.94568E+13 Hybrid 2.75774E+13
Size Stock Model MSE - Test Stock Size Model MSE - Test Stock Size Model MSE - Test
Amazon 1,000 OPTM-LSTM 1.80046E+13 Amazon 2,000 OPTM-LSTM 2.07877E+13 Amazon 3,000 OPTM-LSTM 2.38613E+13
LSTM 2.04199E+13 LSTM 2.08535E+13 LSTM 2.52190E+13
Attention 2.04171E+13 Attention 2.08495E+13 Attention 2.52077E+13
Bidirectional 2.04172E+13 Bidirectional 2.23727E+13 Bidirectional 2.52091E+13
GRU 2.43227E+13 GRU 2.23705E+13 GRU 2.52008E+13
Hybrid 1.97743E+13 Hybrid 2.99633E+13 Hybrid 2.99564E+13
4,000 OPTM-LSTM 1.91248E+13 5,000 OPTM-LSTM 1.96493E+13 6,000 OPTM-LSTM 1.92065E+13
LSTM 1.97085E+13 LSTM 2.13228E+13 LSTM 2.35344E+13
Attention 1.96953E+13 Attention 2.12964E+13 Attention 2.35050E+13
Bidirectional 1.96927E+13 Bidirectional 2.13016E+13 Bidirectional 2.35021E+13
GRU 1.96876E+13 GRU 2.12849E+13 GRU 2.34807E+13
Hybrid 2.57867E+13 Hybrid 1.86085E+13 Hybrid 1.96575E+13
7,000 OPTM-LSTM 2.44086E+13 10,000 OPTM-LSTM 2.34885E+13 15,000 OPTM-LSTM 2.22071E+13
LSTM 2.54747E+13 LSTM 2.65046E+13 LSTM 2.71849E+13
Attention 2.54255E+13 Attention 2.63515E+13 Attention 2.69329E+13
Bidirectional 2.54299E+13 Bidirectional 2.63566E+13 Bidirectional 2.69567E+13
GRU 2.54086E+13 GRU 2.63118E+13 GRU 2.68626E+13
Hybrid 2.83514E+13 Hybrid 3.44564E+13 Hybrid 2.15106E+13
20,000 OPTM-LSTM 2.01197E+13 35,000 OPTM-LSTM 1.53563E+13 50,000 OPTM-LSTM 1.28607E+13
LSTM 2.69286E+13 LSTM 2.94158E+13 LSTM 2.94260E+13
Attention 2.63352E+13 Attention 2.72802E+13 Attention 2.43106E+13
Bidirectional 2.61174E+13 Bidirectional 2.68018E+13 Bidirectional 2.45215E+13
GRU 2.60337E+13 GRU 2.64865E+13 GRU 2.46317E+13
Hybrid 2.11629E+13 Hybrid 2.79983E+13 Hybrid 1.46543E+13
100,000 OPTM-LSTM 2.46977E+12 400,000 OPTM-LSTM 6.29513E+09 800,000 OPTM-LSTM 2.50453E+07
LSTM 2.92036E+13 LSTM 2.84972E+13 LSTM 2.87144E+13
Attention 1.23114E+13 Attention 2.34635E+10 Attention 2.18847E+09
Bidirectional 1.54173E+13 Bidirectional 1.05435E+10 Bidirectional 3.53215E+10
GRU 1.41743E+13 GRU 9.97630E+09 GRU 1.57717E+09
Hybrid 7.21288E+12 Hybrid 9.82100E+10 Hybrid 8.09596E+08
1,000,000 OPTM-LSTM 1.90265E+07 10,000,000 OPTM-LSTM 1.21373E+07 20,000,000 OPTM-LSTM 1.13554E+07
LSTM 2.89144E+13 LSTM 2.79154E+13 LSTM 2.76699E+13
Attention 2.00338E+09 Attention 1.98734E+09 Attention 1.93428E+09
Bidirectional 3.43883E+10 Bidirectional 3.13243E+10 Bidirectional 3.38476E+10
GRU 1.64125E+09 GRU 1.77395E+09 GRU 1.96355E+09
Hybrid 1.46733E+09 Hybrid 1.54599E+09 Hybrid 9.93424E+0822
(a) OPTM-LSTM training MSE scores
 (b) OPTM-LSTM testing MSE scores
(c) LSTM training MSE scores
 (d) LSTM testing MSE scores
(e) Attention LSTM training MSE scores
 (f) Attention LSTM testing MSE scores
(g) Bidirectional training MSE scores
 (h) Bidirectional testing MSE scores
(i) GRU training MSE scores
 (j) GRU testing MSE scores
(k) Hybrid training MSE scores
 (l) Hybrid testing MSE scores
Fig. 6: Amazon Short MSE scores based on Table VI.23
TABLE VII: Amazon MSE scores under the Long experimental protocol.
Stock Size Model MSE - Train Stock Size Model MSE - Train Stock Size Model MSE - Train
Amazon 1,000 OPTM-LSTM 2.28218E+13 Amazon 2,000 OPTM-LSTM 2.14712E+13 Amazon 3,000 OPTM-LSTM 2.03532E+13
LSTM 2.33984E+13 LSTM 2.23702E+13 LSTM 2.18672E+13
Attention 2.33777E+13 Attention 2.17337E+13 Attention 2.13392E+13
Bidirectional 2.33644E+13 Bidirectional 2.17895E+13 Bidirectional 2.12358E+13
GRU 2.30463E+13 GRU 2.16431E+13 GRU 2.12181E+13
Hybrid 6.37836E+13 Hybrid 2.98727E+13 Hybrid 2.12181E+13
4,000 OPTM-LSTM 2.15148E+13 5,000 OPTM-LSTM 2.01779E+13 6,000 OPTM-LSTM 1.43550E+13
LSTM 2.26837E+13 LSTM 2.20931E+13 LSTM 2.19235E+13
Attention 2.17953E+13 Attention 2.11484E+13 Attention 2.05555E+13
Bidirectional 2.17825E+13 Bidirectional 2.08829E+13 Bidirectional 2.06374E+13
GRU 2.16035E+13 GRU 2.08893E+13 GRU 2.06077E+13
Hybrid 2.56088E+13 Hybrid 2.41016E+13 Hybrid 2.45141E+13
7,000 OPTM-LSTM 1.92793E+13 10,000 OPTM-LSTM 1.87029E+13 15,000 OPTM-LSTM 1.00104E+13
LSTM 2.21432E+13 LSTM 2.29951E+13 LSTM 2.36865E+13
Attention 2.09194E+13 Attention 2.29951E+13 Attention 2.08998E+13
Bidirectional 2.08263E+13 Bidirectional 2.11482E+13 Bidirectional 2.07893E+13
GRU 2.12098E+13 GRU 2.10835E+13 GRU 2.07266E+13
Hybrid 2.04333E+13 Hybrid 3.49398E+13 Hybrid 1.94942E+13
Stock Size Model MSE - Test Stock Size Model MSE - Test Stock Size Model MSE - Test
Amazon 1,000 OPTM-LSTM 1.94055E+13 Amazon 2,000 OPTM-LSTM 2.00711E+13 Amazon 3,000 OPTM-LSTM 2.11361E+13
LSTM 2.04158E+13 LSTM 2.00849E+13 LSTM 2.52133E+13
Attention 1.96952E+13 Attention 2.03613E+13 Attention 2.46697E+13
Bidirectional 1.97491E+13 Bidirectional 2.04502E+13 Bidirectional 2.45197E+13
GRU 1.98757E+13 GRU 2.02619E+13 GRU 2.43652E+13
Hybrid 4.92951E+13 Hybrid 3.05593E+13 Hybrid 5.91284E+13
4,000 OPTM-LSTM 1.55827E+13 5,000 OPTM-LSTM 2.00073E+13 6,000 OPTM-LSTM 7.56541E+12
LSTM 1.97012E+13 LSTM 2.13159E+13 LSTM 2.35177E+13
Attention 1.89802E+13 Attention 2.02218E+13 Attention 2.16751E+13
Bidirectional 1.88544E+13 Bidirectional 2.02142E+13 Bidirectional 2.21063E+13
GRU 1.86681E+13 GRU 2.01437E+13 GRU 2.21322E+13
Hybrid 2.17871E+13 Hybrid 2.33849E+13 Hybrid 2.28543E+13
7,000 OPTM-LSTM 2.19539E+13 10,000 OPTM-LSTM 2.10338E+13 15,000 OPTM-LSTM 5.28577E+12
LSTM 2.54651E+13 LSTM 2.64762E+13 LSTM 2.71402E+13
Attention 2.42306E+13 Attention 2.37283E+13 Attention 2.24987E+13
Bidirectional 2.42153E+13 Bidirectional 2.42298E+13 Bidirectional 2.33562E+13
GRU 2.43058E+13 GRU 2.39311E+13 GRU 2.27396E+13
Hybrid 2.24018E+13 Hybrid 4.06392E+13 Hybrid 2.14205E+13
TABLE VIII: Amazon Benchmark Training (left) and Benchmark Testing (right).
Data sample is 100,000 trading events.
Input Normalization Model MSE - Train Input Normalization Model MSE - Test
LOB Data Raw OPTM-LSTM 4.42854E+12 LOB Data Raw OPTM-LSTM 2.46979E+12
LSTM 2.84573E+13 LSTM 2.92032E+13
Attention 1.72512E+13 Attention 1.23114E+13
Bidirectional 1.79991E+13 Bidirectional 1.54175E+13
GRU 1.79168E+13 GRU 1.41743E+13
Hybrid 1.36274E+13 Hybrid 7.21285E+12
Baseline 5.76229E+15 Baseline 5.98276E+15
MinMax OPTM-LSTM 2.22887E-05 MinMax OPTM-LSTM 2.02887E-05
LSTM 5.12674E-05 LSTM 4.69676E-05
Attention 4.75122E-05 Attention 4.20884E-05
Bidirectional 5.71983E-05 Bidirectional 4.80998E-05
GRU 4.13893E-05 GRU 4.08778E-05
Hybrid 8.92443E-05 Hybrid 9.57998E-05
Baseline 9.99776E-05 Baseline 1.00998E-04
Zscore OPTM-LSTM 6.85887E-02 Zscore OPTM-LSTM 5.20034E-02
LSTM 8.57663E-02 LSTM 8.00990E-02
Attention 8.48443E-02 Attention 8.17477E-02
Bidirectional 8.95367E-02 Bidirectional 8.03909E-02
GRU 8.22114E-02 GRU 7.99188E-02
Hybrid 4.75702E-01 Hybrid 6.06009E-01
Baseline 9.86855E-01 Baseline 9.85114E-01
Mid-price Raw OPTM-LSTM 5.94990E+12 Mid-price Raw OPTM-LSTM 6.20004E+12
LSTM 2.76338E+13 LSTM 2.94290E+13
Attention 2.76912E+13 Attention 2.94904E+13
Bidirectional 2.73117E+13 Bidirectional 2.90581E+13
GRU 2.73403E+13 GRU 2.90554E+13
Hybrid 7.57040E+13 Hybrid 7.57452E+13
Persistence 8.00184E+14 Persistence 8.01993E+14
MinMax OPTM-LSTM 1.04280E-06 MinMax OPTM-LSTM 1.02934E-06
LSTM 1.59140E-06 LSTM 1.57829E-06
Attention 1.77829E-06 Attention 2.41392E-06
Bidirectional 3.58820E-06 Bidirectional 3.58151E-06
GRU 3.64600E-06 GRU 4.52590E-06
Hybrid 4.51500E-06 Hybrid 5.00910E-06
Persistence 2.05157E-06 Persistence 2.36094E-06
Zscore OPTM-LSTM 7.79740E-03 Zscore OPTM-LSTM 7.94824E-03
LSTM 6.90000E-02 LSTM 7.50000E-02
Attention 6.94000E-02 Attention 6.98876E-02
Bidirectional 7.00000E-02 Bidirectional 7.94008E-02
GRU 7.13324E-02 GRU 7.66547E-02
Hybrid 1.83000E-01 Hybrid 1.63776E-01
Persistence 7.05157E-02 Persistence 7.96094E-0224
(a) OPTM-LSTM training MSE scores
 (b) OPTM-LSTM testing MSE scores
(c) LSTM training MSE scores
 (d) LSTM testing MSE scores
(e) Attention LSTM training MSE scores
 (f) Attention LSTM testing MSE scores
(g) Bidirectional training MSE scores
 (h) Bidirectional testing MSE scores
(i) GRU training MSE scores
 (j) GRU testing MSE scores
(k) Hybrid training MSE scores
 (l) Hybrid testing MSE scores
Fig. 7: Amazon Long MSE scores based on Table VII.25
TABLE IX: Kesko MSE scores under the Short experimental protocol.
Stock Size Model MSE - Train Stock Size Model MSE - Train Stock Size Model MSE - Train
Kesko 1,000 OPTM-LSTM 5.67480E+10 Kesko 2,000 OPTM-LSTM 5.09428E+10 Kesko 3,000 OPTM-LSTM 4.35003E+10
LSTM 6.88025E+10 LSTM 6.84285E+10 LSTM 6.82272E+10
Attention 6.88067E+10 Attention 6.84241E+10 Attention 6.82374E+10
Bidirectional 6.88056E+10 Bidirectional 6.84235E+10 Bidirectional 6.82452E+10
GRU 6.88063E+10 GRU 6.84220E+10 GRU 6.82457E+10
Hybrid 1.45781E+11 Hybrid 1.31784E+11 Hybrid 1.59131E+11
4,000 OPTM-LSTM 3.88949E+10 5,000 OPTM-LSTM 5.93965E+10 6,000 OPTM-LSTM 1.87280E+10
LSTM 6.81911E+10 LSTM 6.82883E+10 LSTM 6.83506E+10
Attention 6.82022E+10 Attention 6.82883E+10 Attention 6.83441E+10
Bidirectional 6.81910E+10 Bidirectional 6.82882E+10 Bidirectional 6.83413E+10
GRU 6.82025E+10 GRU 6.82887E+10 GRU 6.83589E+10
Hybrid 8.82731E+10 Hybrid 8.06607E+10 Hybrid 8.02223E+10
7,000 OPTM-LSTM 7.01066E+09 10,000 OPTM-LSTM 3.36665E+08 15,000 OPTM-LSTM 1.23322E+05
LSTM 6.83350E+10 LSTM 6.84798E+10 LSTM 6.84651E+10
Attention 6.83902E+10 Attention 6.85087E+10 Attention 6.84350E+10
Bidirectional 6.83704E+10 Bidirectional 6.85370E+10 Bidirectional 6.84671E+10
GRU 6.83900E+10 GRU 6.85051E+10 GRU 6.84191E+10
Hybrid 1.33378E+11 Hybrid 2.38552E+11 Hybrid 2.94083E+11
20,000 OPTM-LSTM 3.53800E+03 35,000 OPTM-LSTM 3.08900E+03 50,000 OPTM-LSTM 1.28150E+04
LSTM 6.83821E+10 LSTM 6.92214E+10 LSTM 7.00089E+10
Attention 6.83587E+10 Attention 6.90948E+10 Attention 7.00074E+10
Bidirectional 6.83818E+10 Bidirectional 6.92188E+10 Bidirectional 6.97331E+10
GRU 6.84508E+10 GRU 6.90434E+10 GRU 7.00073E+10
Hybrid 2.22008E+11 Hybrid 2.01700E+11 Hybrid 1.62985E+11
100,000 OPTM-LSTM 3.00368E+05 400,000 OPTM-LSTM 3.01665E+05 800,000 OPTM-LSTM 3.09889E+05
LSTM 7.11851E+10 LSTM 7.14898E+10 LSTM 7.16874E+10
Attention 7.01533E+10 Attention 7.01758E+10 Attention 7.10329E+10
Bidirectional 7.11849E+10 Bidirectional 7.19002E+10 Bidirectional 7.21223E+10
GRU 7.11846E+10 GRU 7.12240E+10 GRU 7.17981E+10
Hybrid 1.55687E+11 Hybrid 2.08874E+11 Hybrid 1.59847E+11
1,000,000 OPTM-LSTM 3.10878E+05 2,000,000 OPTM-LSTM 3.12093E+05
LSTM 7.20847E+10 LSTM 7.29091E+10
Attention 7.11739E+10 Attention 7.19023E+10
Bidirectional 7.20324E+10 Bidirectional 7.25881E+10
GRU 7.19423E+10 GRU 7.18201E+10
Hybrid 2.88737E+11 Hybrid 2.94437E+11
Stock Size Model MSE - Test Stock Size Model MSE - Test Stock Size Model MSE - Test
Kesko 1,000 OPTM-LSTM 5.13944E+10 Kesko 2,000 OPTM-LSTM 4.30635E+10 Kesko 3,000 OPTM-LSTM 3.12937E+10
LSTM 6.80401E+10 LSTM 6.78792E+10 LSTM 6.80535E+10
Attention 6.80458E+10 Attention 6.78726E+10 Attention 6.80665E+10
Bidirectional 6.80445E+10 Bidirectional 6.78721E+10 Bidirectional 6.80767E+10
GRU 6.80452E+10 GRU 6.78698E+10 GRU 6.80774E+10
Hybrid 1.43664E+11 Hybrid 1.33300E+11 Hybrid 1.58883E+11
4,000 OPTM-LSTM 2.37974E+10 5,000 OPTM-LSTM 5.06404E+10 6,000 OPTM-LSTM 2.20000E+09
LSTM 6.86076E+10 LSTM 6.87101E+10 LSTM 6.85633E+10
Attention 6.86343E+10 Attention 6.87102E+10 Attention 6.85558E+10
Bidirectional 6.86187E+10 Bidirectional 6.87102E+10 Bidirectional 6.85517E+10
GRU 6.86339E+10 GRU 6.87101E+10 GRU 6.85764E+10
Hybrid 8.81820E+10 Hybrid 7.00176E+10 Hybrid 8.01716E+10
7,000 OPTM-LSTM 3.78775E+05 10,000 OPTM-LSTM 1.00700E+03 15,000 OPTM-LSTM 1.37600E+03
LSTM 6.90179E+10 LSTM 6.82803E+10 LSTM 6.82137E+10
Attention 6.90947E+10 Attention 6.83169E+10 Attention 6.81673E+10
Bidirectional 6.90667E+10 Bidirectional 6.83577E+10 Bidirectional 6.82134E+10
GRU 6.90950E+10 GRU 6.83124E+10 GRU 6.81444E+10
Hybrid 1.35000E+11 Hybrid 4.36224E+11 Hybrid 4.94433E+11
20,000 OPTM-LSTM 3.93000E+02 35,000 OPTM-LSTM 4.44000E+02 50,000 OPTM-LSTM 9.99000E+02
LSTM 6.88736E+10 LSTM 7.25105E+10 LSTM 7.20523E+10
Attention 6.88337E+10 Attention 7.23182E+10 Attention 7.20518E+10
Bidirectional 6.88669E+10 Bidirectional 7.25108E+10 Bidirectional 7.16165E+10
GRU 6.89682E+10 GRU 7.22375E+10 GRU 7.20519E+10
Hybrid 5.98547E+11 Hybrid 2.10906E+11 Hybrid 1.67994E+11
100,000 OPTM-LSTM 1.93400E+03 400,000 OPTM-LSTM 1.35000E+03 800,000 OPTM-LSTM 1.26600E+03
LSTM 7.57863E+10 LSTM 7.49856E+10 LSTM 7.53487E+10
Attention 7.39432E+10 Attention 7.39998E+10 Attention 7.45662E+10
Bidirectional 7.57872E+10 Bidirectional 7.23200E+10 Bidirectional 7.05777E+10
GRU 7.57864E+10 GRU 7.38821E+10 GRU 7.16384E+10
Hybrid 1.65579E+11 Hybrid 1.90002E+11 Hybrid 2.88704E+11
1,000,000 OPTM-LSTM 1.00900E+03 2,000,000 OPTM-LSTM 1.00100E+03
LSTM 7.38745E+10 LSTM 7.09273E+10
Attention 7.49993E+10 Attention 7.10024E+10
Bidirectional 6.99432E+10 Bidirectional 7.02131E+10
GRU 7.01342E+10 GRU 7.00800E+10
Hybrid 1.87748E+11 Hybrid 2.44905E+1126
(a) OPTM-LSTM training MSE scores
 (b) OPTM-LSTM testing MSE scores
(c) LSTM training MSE scores
 (d) LSTM testing MSE scores
(e) Attention LSTM training MSE scores
 (f) Attention LSTM testing MSE scores
(g) Bidirectional training MSE scores
 (h) Bidirectional testing MSE scores
(i) GRU training MSE scores
 (j) GRU testing MSE scores
(k) Hybrid training MSE scores
 (l) Hybrid testing MSE scores
Fig. 8: Kesko Short MSE scores based on Table IX.27
TABLE X: Kesko MSE scores under the Long experimental protocol.
Stock Size Model MSE - Train Stock Size Model MSE - Train Stock Size Model MSE - Train
Kesko 1,000 OPTM-LSTM 4.68200E+04 Kesko 2,000 OPTM-LSTM 2.21950E+04 Kesko 3,000 OPTM-LSTM 1.73180E+04
LSTM 6.88089E+10 LSTM 6.81480E+10 LSTM 6.82454E+10
Attention 6.88093E+10 Attention 6.81174E+10 Attention 6.82453E+10
Bidirectional 6.88091E+10 Bidirectional 6.81027E+10 Bidirectional 6.78117E+10
GRU 6.88090E+10 GRU 6.84291E+10 GRU 6.82455E+10
Hybrid 1.72457E+11 Hybrid 2.05373E+11 Hybrid 1.21925E+11
4,000 OPTM-LSTM 1.49260E+04 5,000 OPTM-LSTM 1.23690E+04 6,000 OPTM-LSTM 9.92900E+03
LSTM 6.75009E+10 LSTM 6.82883E+10 LSTM 6.77034E+10
Attention 6.71903E+10 Attention 6.64124E+10 Attention 6.55445E+10
Bidirectional 6.70497E+10 Bidirectional 6.63702E+10 Bidirectional 6.83595E+10
GRU 6.82024E+10 GRU 6.67256E+10 GRU 6.83590E+10
Hybrid 2.14246E+11 Hybrid 8.93922E+10 Hybrid 2.01985E+11
7,000 OPTM-LSTM 1.04300E+04 10,000 OPTM-LSTM 6.87700E+03 15,000 OPTM-LSTM 5.94700E+03
LSTM 6.83884E+10 LSTM 6.64836E+10 LSTM 6.58317E+10
Attention 6.47468E+10 Attention 5.61866E+10 Attention 6.58317E+10
Bidirectional 6.83898E+10 Bidirectional 6.06804E+10 Bidirectional 4.84266E+10
GRU 6.83906E+10 GRU 6.85372E+10 GRU 4.47933E+10
Hybrid 1.27098E+11 Hybrid 2.48311E+11 Hybrid 1.13078E+11
Stock Size Model MSE - Test Stock Size Model MSE - Test Stock Size Model MSE - Test
Kesko 1,000 OPTM-LSTM 3.26100E+03 Kesko 2,000 OPTM-LSTM 1.22400E+03 Kesko 3,000 OPTM-LSTM 2.64000E+03
LSTM 6.80489E+10 LSTM 6.76086E+10 LSTM 6.80769E+10
Attention 6.80492E+10 Attention 6.75622E+10 Attention 6.80767E+10
Bidirectional 6.80488E+10 Bidirectional 6.75473E+10 Bidirectional 6.76332E+10
GRU 6.80489E+10 GRU 6.78792E+10 GRU 6.80770E+10
Hybrid 1.73009E+11 Hybrid 2.81E115+11 Hybrid 1.22093E+11
4,000 OPTM-LSTM 1.25600E+03 5,000 OPTM-LSTM 4.85000E+02 6,000 OPTM-LSTM 1.40800E+03
LSTM 6.79221E+10 LSTM 6.87101E+10 LSTM 6.79125E+10
Attention 6.75923E+10 Attention 6.67740E+10 Attention 6.56693E+10
Bidirectional 6.74470E+10 Bidirectional 6.67399E+10 Bidirectional 6.85769E+10
GRU 6.86338E+10 GRU 6.70961E+10 GRU 6.85764E+10
Hybrid 2.27355E+11 Hybrid 8.95668E+10 Hybrid 3.84335E+11
7,000 OPTM-LSTM 5.98900E+03 10,000 OPTM-LSTM 1.04000E+03 15,000 OPTM-LSTM 4.38700E+03
LSTM 6.90948E+10 LSTM 6.62728E+10 LSTM 6.55495E+10
Attention 6.53064E+10 Attention 5.55827E+10 Attention 6.82135E+10
Bidirectional 6.90945E+10 Bidirectional 6.02120E+10 Bidirectional 4.75339E+10
GRU 6.90956E+10 GRU 6.83574E+10 GRU 4.38358E+10
Hybrid 1.27009E+11 Hybrid 3.48221E+11 Hybrid 1.13889E+11
TABLE XI: Kesko Benchmark Training (left) and Benchmark Testing (right). Data
sample is 50,000 trading events.
Input Normalization Model MSE - Train Input Normalization Model MSE - Test
LOB Data Raw OPTM-LSTM 1.28150E+04 LOB Data Raw OPTM-LSTM 9.99000E+02
LSTM 7.00089E+10 LSTM 7.20523E+10
Attention 7.00074E+10 Attention 7.20518E+10
Bidirectional 6.97331E+10 Bidirectional 7.16165E+10
GRU 7.00073E+10 GRU 7.20519E+10
Hybrid 1.62985E+11 Hybrid 1.67990E+11
Baseline 9.41815E+10 Baseline 9.44458E+10
MinMax OPTM-LSTM 2.85920E-05 MinMax OPTM-LSTM 9.98282E-05
LSTM 1.04000E-04 LSTM 1.14617E-04
Attention 1.24770E-04 Attention 2.50912E-04
Bidirectional 1.30000E-04 Bidirectional 1.22839E-04
GRU 1.36740E-04 GRU 1.46407E-04
Hybrid 3.60000E-04 Hybrid 2.65749E-04
Baseline 4.35225E-04 Baseline 4.12883E-04
Zscore OPTM-LSTM 2.13500E-01 Zscore OPTM-LSTM 3.99179E-01
LSTM 1.27620E+00 LSTM 1.00865E+00
Attention 1.98440E+00 Attention 1.85445E+00
Bidirectional 1.96900E+00 Bidirectional 1.13340E+00
GRU 2.09650E+00 GRU 1.45136E+00
Hybrid 2.20460E+00 Hybrid 1.32600E+00
Baseline 3.24333E+00 Baseline 3.14158E+00
Mid-price Raw OPTM-LSTM 1.15897E+05 Mid-price Raw OPTM-LSTM 9.80490E+04
LSTM 6.80624E+07 LSTM 6.80760E+07
Attention 6.82317E+07 Attention 6.82315E+07
Bidirectional 6.39776E+07 Bidirectional 6.32714E+07
GRU 6.38784E+07 GRU 6.31658E+07
Hybrid 1.10363E+08 Hybrid 1.10018E+08
Persistence 7.99125E+07 Persistence 7.90350E+07
MinMax OPTM-LSTM 1.15280E-05 MinMax OPTM-LSTM 1.26656E-05
LSTM 9.49250E-05 LSTM 9.28870E-05
Attention 8.32390E-05 Attention 7.98481E-05
Bidirectional 7.62860E-04 Bidirectional 8.46251E-04
GRU 7.70360E-04 GRU 8.89751E-04
Hybrid 1.43340E-04 Hybrid 2.48957E-04
Persistence 7.05425E-04 Persistence 7.74876E-04
Zscore OPTM-LSTM 1.05100E-01 Zscore OPTM-LSTM 1.32550E-01
LSTM 2.33020E+00 LSTM 1.87150E+00
Attention 1.96070E+00 Attention 1.96074E+00
Bidirectional 1.98860E+00 Bidirectional 1.98863E+00
GRU 2.57540E+00 GRU 1.72275E+00
Hybrid 2.56590E+00 Hybrid 2.68000E+00
Persistence 2.68474E+00 Persistence 2.54835E+0028
(a) OPTM-LSTM training MSE scores
 (b) OPTM-LSTM testing MSE scores
(c) LSTM training MSE scores
 (d) LSTM testing MSE scores
(e) Attention LSTM training MSE scores
 (f) Attention LSTM testing MSE scores
(g) Bidirectional training MSE scores
 (h) Bidirectional testing MSE scores
(i) GRU training MSE scores
 (j) GRU testing MSE scores
(k) Hybrid training MSE scores
 (l) Hybrid testing MSE scores
Fig. 9: Kesko Long MSE scores based on Table X.29
TABLE XII: Wartsila MSE scores under the Short experimental protocol.
Stock Size Model MSE - Train Stock Size Model MSE - Train Stock Size Model MSE - Train
Wartsila 1,000 OPTM-LSTM 1.09804E+11 Wartsila 2,000 OPTM-LSTM 8.39142E+10 Wartsila 3,000 OPTM-LSTM 9.66711E+10
LSTM 1.21563E+11 LSTM 1.21051E+11 LSTM 1.20788E+11
Attention 1.21562E+11 Attention 1.21045E+11 Attention 1.20792E+11
Bidirectional 1.21559E+11 Bidirectional 1.21051E+11 Bidirectional 1.20793E+11
GRU 1.21562E+11 GRU 1.21044E+11 GRU 1.20779E+11
Hybrid 1.21395E+12 Hybrid 1.10232E+11 Hybrid 5.69594E+11
4,000 OPTM-LSTM 5.06770E+10 5,000 OPTM-LSTM 5.37683E+10 6,000 OPTM-LSTM 1.60922E+10
LSTM 1.20691E+11 LSTM 1.20673E+11 LSTM 1.20643E+11
Attention 1.20690E+11 Attention 1.20673E+11 Attention 1.20642E+11
Bidirectional 1.20676E+11 Bidirectional 1.20673E+11 Bidirectional 1.20642E+11
GRU 1.20674E+11 GRU 1.20654E+11 GRU 1.20642E+11
Hybrid 2.28451E+11 Hybrid 1.95861E+11 Hybrid 3.97941E+11
7,000 OPTM-LSTM 3.48147E+10 10,000 OPTM-LSTM 3.92906E+09 15,000 OPTM-LSTM 5.63909E+07
LSTM 1.19545E+11 LSTM 1.20258E+11 LSTM 1.20148E+11
Attention 1.19504E+11 Attention 1.20258E+11 Attention 1.20096E+11
Bidirectional 1.19511E+11 Bidirectional 1.20257E+11 Bidirectional 1.20151E+11
GRU 1.19507E+11 GRU 1.20209E+11 GRU 1.20151E+11
Hybrid 1.79659E+11 Hybrid 1.35232E+11 Hybrid 4.92045E+11
20,000 OPTM-LSTM 4.79600E+03 35,000 OPTM-LSTM 3.12200E+03 50,000 OPTM-LSTM 2.68100E+03
LSTM 1.19779E+11 LSTM 1.18918E+11 LSTM 1.18318E+11
Attention 1.19783E+11 Attention 1.18639E+11 Attention 1.18008E+11
Bidirectional 1.19663E+11 Bidirectional 1.18918E+11 Bidirectional 1.18325E+11
GRU 1.19783E+11 GRU 1.18707E+11 GRU 1.18325E+11
Hybrid 1.95015E+11 Hybrid 1.31175E+12 Hybrid 4.00368E+11
100,000 OPTM-LSTM 1.75200E+03 400,000 OPTM-LSTM 3.02000E+03 800,000 OPTM-LSTM 1.92900E+04
LSTM 1.17305E+11 LSTM 1.21255E+11 LSTM 1.22637E+11
Attention 1.15883E+11 Attention 1.21319E+11 Attention 1.22882E+11
Bidirectional 1.17321E+11 Bidirectional 8.40267E+10 Bidirectional 1.22881E+11
GRU 1.16074E+11 GRU 8.43822E+10 GRU 1.22881E+11
Hybrid 8.94838E+11 Hybrid 4.94263E+11 Hybrid 2.17551E+11
1,000,000 OPTM-LSTM 2.01330E+04 2,000,000 OPTM-LSTM 2.07880E+04
LSTM 1.29683E+11 LSTM 1.48478E+11
Attention 1.25959E+11 Attention 1.27544E+11
Bidirectional 1.22556E+11 Bidirectional 1.21888E+11
GRU 1.22669E+11 GRU 1.21567E+11
Hybrid 4.88503E+11 Hybrid 2.50929E+11
Stock Size Model MSE - Test Stock Size Model MSE - Test Stock Size Model MSE - Test
Wartsila 1,000 OPTM-LSTM 1.04203E+11 Wartsila 2,000 OPTM-LSTM 6.88044E+10 Wartsila 3,000 OPTM-LSTM 8.27156E+10
LSTM 1.20541E+11 LSTM 1.20273E+11 LSTM 1.20388E+11
Attention 1.20540E+11 Attention 1.20265E+11 Attention 1.20389E+11
Bidirectional 1.20536E+11 Bidirectional 1.20272E+11 Bidirectional 1.20374E+11
GRU 1.20541E+11 GRU 1.20263E+11 GRU 1.20374E+11
Hybrid 1.50009E+12 Hybrid 6.71980E+11 Hybrid 5.68991E+11
4,000 OPTM-LSTM 2.17475E+10 5,000 OPTM-LSTM 2.19227E+10 6,000 OPTM-LSTM 1.64676E+08
LSTM 1.20608E+11 LSTM 1.20489E+11 LSTM 1.20262E+11
Attention 1.20606E+11 Attention 1.20491E+11 Attention 1.20262E+11
Bidirectional 1.20588E+11 Bidirectional 1.20489E+11 Bidirectional 1.20262E+11
GRU 1.20586E+11 GRU 1.20464E+11 GRU 1.20262E+11
Hybrid 2.28250E+11 Hybrid 1.95808E+11 Hybrid 3.96597E+11
7,000 OPTM-LSTM 1.69590E+09 10,000 OPTM-LSTM 2.17000E+02 15,000 OPTM-LSTM 2.15600E+03
LSTM 1.19545E+11 LSTM 1.19716E+11 LSTM 1.19533E+11
Attention 1.19504E+11 Attention 1.19716E+11 Attention 1.19454E+11
Bidirectional 1.19511E+11 Bidirectional 1.19716E+11 Bidirectional 1.19532E+11
GRU 1.19507E+11 GRU 1.19648E+11 GRU 1.19533E+11
Hybrid 1.78198E+11 Hybrid 1.34849E+11 Hybrid 4.89492E+11
20,000 OPTM-LSTM 1.72960E+04 35,000 OPTM-LSTM 3.03000E+02 50,000 OPTM-LSTM 2.08000E+02
LSTM 1.17931E+11 LSTM 1.16948E+11 LSTM 1.17274E+11
Attention 1.17933E+11 Attention 1.16533E+11 Attention 1.16784E+11
Bidirectional 1.17762E+11 Bidirectional 1.16951E+11 Bidirectional 1.17281E+11
GRU 1.17935E+11 GRU 1.16633E+11 GRU 1.17281E+11
Hybrid 1.92199E+11 Hybrid 1.29029E+12 Hybrid 3.97123E+11
100,000 OPTM-LSTM 2.44200E+03 400,000 OPTM-LSTM 1.13400E+03 800,000 OPTM-LSTM 1.26000E+02
LSTM 1.16278E+11 LSTM 1.30016E+11 LSTM 1.17921E+11
Attention 1.13813E+11 Attention 1.30117E+11 Attention 1.18251E+11
Bidirectional 1.16301E+11 Bidirectional 5.85243E+10 Bidirectional 1.18250E+11
GRU 1.16301E+11 GRU 5.93584E+10 GRU 1.18251E+11
Hybrid 8.87207E+11 Hybrid 5.30482E+11 Hybrid 2.10445E+11
1,000,000 OPTM-LSTM 1.15000E+02 2,000,000 OPTM-LSTM 1.01000E+02
LSTM 1.16884E+11 LSTM 1.17877E+11
Attention 1.18254E+11 Attention 1.20885E+11
Bidirectional 1.17566E+11 Bidirectional 1.20701E+11
GRU 1.15771E+11 GRU 1.17881E+11
Hybrid 3.89878E+11 Hybrid 1.33446E+1130
(a) OPTM-LSTM training MSE scores
 (b) OPTM-LSTM testing MSE scores
(c) LSTM training MSE scores
 (d) LSTM testing MSE scores
(e) Attention LSTM training MSE scores
 (f) Attention LSTM testing MSE scores
(g) Bidirectional training MSE scores
 (h) Bidirectional testing MSE scores
(i) GRU training MSE scores
 (j) GRU testing MSE scores
(k) Hybrid training MSE scores
 (l) Hybrid testing MSE scores
Fig. 10: Wartsila Short MSE scores based on Table XII.31
TABLE XIII: Wartsila MSE scores under the Long experimental protocol.
Stock Size Model MSE - Train Stock Size Model MSE - Train Stock Size Model MSE - Train
Wartsila 1,000 OPTM-LSTM 5.67870E+04 Wartsila 2,000 OPTM-LSTM 3.08010E+04 Wartsila 3,000 OPTM-LSTM 2.41600E+04
LSTM 1.21562E+11 LSTM 1.20695E+11 LSTM 1.20276E+11
Attention 1.21443E+11 Attention 1.21050E+11 Attention 1.19978E+11
Bidirectional 1.21562E+11 Bidirectional 1.21055E+11 Bidirectional 1.19986E+11
GRU 1.21562E+11 GRU 1.21051E+11 GRU 1.20798E+11
Hybrid 1.46884E+11 Hybrid 4.05923E+11 Hybrid 1.57073E+11
4,000 OPTM-LSTM 1.90670E+04 5,000 OPTM-LSTM 1.66770E+04 6,000 OPTM-LSTM 1.15560E+04
LSTM 1.19263E+11 LSTM 1.19089E+11 LSTM 1.20608E+11
Attention 1.20693E+11 Attention 1.20672E+11 Attention 1.17067E+11
Bidirectional 1.20695E+11 Bidirectional 1.18731E+11 Bidirectional 1.17356E+11
GRU 1.19233E+11 GRU 1.20673E+11 GRU 1.17483E+11
Hybrid 1.61632E+11 Hybrid 4.01191E+11 Hybrid 7.68799E+11
7,000 OPTM-LSTM 1.08690E+04 10,000 OPTM-LSTM 8.25900E+03 15,000 OPTM-LSTM 9.63900E+03
LSTM 1.20589E+11 LSTM 1.20257E+11 LSTM 1.20087E+11
Attention 1.16692E+11 Attention 1.20258E+11 Attention 1.20151E+11
Bidirectional 1.15956E+11 Bidirectional 1.08053E+11 Bidirectional 9.66286E+10
GRU 1.16061E+11 GRU 1.20258E+11 GRU 1.20151E+11
Hybrid 4.69414E+11 Hybrid 2.20184E+11 Hybrid 2.94234E+11
Stock Size Model MSE - Test Stock Size Model MSE - Test Stock Size Model MSE - Test
Wartsila 1,000 OPTM-LSTM 3.01000E+02 Wartsila 2,000 OPTM-LSTM 7.29000E+02 Wartsila 3,000 OPTM-LSTM 1.98800E+03
LSTM 1.20539E+11 LSTM 1.19912E+11 LSTM 1.19869E+11
Attention 1.20419E+11 Attention 1.20272E+11 Attention 1.19560E+11
Bidirectional 1.20541E+11 Bidirectional 1.20272E+11 Bidirectional 1.19569E+11
GRU 1.20541E+11 GRU 1.20272E+11 GRU 1.20389E+11
Hybrid 1.46654E+11 Hybrid 4.04110E+11 Hybrid 1.56225E+11
4,000 OPTM-LSTM 5.51000E+02 5,000 OPTM-LSTM 5.68000E+02 6,000 OPTM-LSTM 3.11260E+04
LSTM 1.19163E+11 LSTM 1.18888E+11 LSTM 1.20227E+11
Attention 1.20606E+11 Attention 1.20489E+11 Attention 1.16579E+11
Bidirectional 1.20606E+11 Bidirectional 1.18492E+11 Bidirectional 1.16879E+11
GRU 1.19112E+11 GRU 1.20491E+11 GRU 1.17007E+11
Hybrid 1.61739E+11 Hybrid 4.00412E+11 Hybrid 7.66015E+11
7,000 OPTM-LSTM 2.17700E+03 10,000 OPTM-LSTM 5.62000E+02 15,000 OPTM-LSTM 4.02000E+02
LSTM 1.19545E+11 LSTM 1.19716E+11 LSTM 1.19469E+11
Attention 1.15535E+11 Attention 1.19716E+11 Attention 1.19532E+11
Bidirectional 1.14774E+11 Bidirectional 1.07105E+11 Bidirectional 9.52940E+10
GRU 1.14889E+11 GRU 1.19716E+11 GRU 1.19533E+11
Hybrid 4.65431E+11 Hybrid 2.19444E+11 Hybrid 2.92892E+11
TABLE XIV: Wartsila Benchmark Training (left) and Benchmark Testing (right).
Data sample is 100,000 trading events.
Input Normalization Model MSE - Train Input Normalization Model MSE - Test
LOB Data Raw OPTM-LSTM 1.75200E+03 LOB Data Raw OPTM-LSTM 2.44200E+03
LSTM 1.17305E+11 LSTM 1.16278E+11
Attention 1.15883E+11 Attention 1.13813E+11
Bidirectional 1.17321E+11 Bidirectional 1.16301E+11
GRU 1.16074E+11 GRU 1.16301E+11
Hybrid 8.94838E+11 Hybrid 8.87207E+11
Baseline 6.81022E+11 Baseline 6.76325E+11
MinMax OPTM-LSTM 1.10690E-04 MinMax OPTM-LSTM 1.09039E-04
LSTM 1.92230E-04 LSTM 1.87033E-04
Attention 2.17910E-04 Attention 1.95100E-04
Bidirectional 3.06550E-04 Bidirectional 4.05580E-04
GRU 2.85060E-04 GRU 3.79129E-04
Hybrid 8.86610E-04 Hybrid 8.74760E-04
Baseline 7.14814E-04 Baseline 5.60481E-04
Zscore OPTM-LSTM 1.76000E-02 Zscore OPTM-LSTM 3.22796E-02
LSTM 1.32100E-01 LSTM 2.80338E-01
Attention 3.74500E-01 Attention 7.54337E-01
Bidirectional 3.69200E-01 Bidirectional 7.03396E-01
GRU 3.32800E-01 GRU 7.49759E-01
Hybrid 1.56070E+00 Hybrid 1.86654E+00
Baseline 7.06162E-01 Baseline 6.26625E-01
Mid-price Raw OPTM-LSTM 2.15000E+03 Mid-price Raw OPTM-LSTM 2.05800E+03
LSTM 1.17975E+11 LSTM 1.16916E+11
Attention 1.18014E+11 Attention 1.16952E+11
Bidirectional 1.02525E+11 Bidirectional 1.19952E+11
GRU 1.02197E+11 GRU 1.21952E+11
Hybrid 2.24065E+11 Hybrid 2.22092E+11
Persistence 2.23065E+11 Persistence 2.23065E+11
MinMax OPTM-LSTM 1.28090E-04 MinMax OPTM-LSTM 2.45917E-04
LSTM 3.36300E-04 LSTM 4.08478E-04
Attention 5.03490E-04 Attention 6.09052E-04
Bidirectional 7.65230E-04 Bidirectional 8.37438E-04
GRU 9.88666E-04 GRU 9.72481E-04
Hybrid 5.27550E-04 Hybrid 5.23988E-04
Persistence 6.06519E-04 Persistence 6.04269E-04
Zscore OPTM-LSTM 2.44100E-01 Zscore OPTM-LSTM 2.55842E-01
LSTM 5.65200E-01 LSTM 6.04132E-01
Attention 5.82000E-01 Attention 8.90469E-01
Bidirectional 6.79000E-01 Bidirectional 7.98851E-01
GRU 6.62600E-01 GRU 9.50139E-01
Hybrid 8.81800E-01 Hybrid 1.15688E+00
Persistence 1.08168E+00 Persistence 1.09168E+0032
(a) OPTM-LSTM training MSE scores
 (b) OPTM-LSTM testing MSE scores
(c) LSTM training MSE scores
 (d) LSTM testing MSE scores
(e) Attention LSTM training MSE scores
 (f) Attention LSTM testing MSE scores
(g) Bidirectional training MSE scores
 (h) Bidirectional testing MSE scores
(i) GRU training MSE scores
 (j) GRU testing MSE scores
(k) Hybrid training MSE scores
 (l) Hybrid testing MSE scores
Fig. 11: Wartsila Long MSE scores based on Table XIII.33
(a) Google: Self-comparison Training Performance
 (b) Google: Self-comparison Testing Performance
(c) Amazon: Self-comparison Training Performance
 (d) Amazon: Self-comparison Testing Performance
(e) Kesko: Self-comparison Training Performance
 (f) Kesko: Self-comparison Testing Performance
(g) Wartsila: Self-comparison Training Performance
 (h) Wartsila: Self-comparison Testing Performance
Fig. 12: OPTM-LSTM cell self-comparison performance between the Short and Long experimental settings.