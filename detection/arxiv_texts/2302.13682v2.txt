arXiv:2302.13682v2  [cond-mat.dis-nn]  28 Jun 2023A deep learning approach to the measurement of long-lived me mory kernels from generalised
Langevin dynamics
Max Kerr Winter,∗Ilian Pihlajamaa, Vincent E. Debets, and Liesbeth M. C. Jans sen∗
Department of Applied Physics, Eindhoven University of Tec hnology, P .O. Box 513, 5600 MB Eindhoven, The Netherlands
(Dated: June 29, 2023)
Memory effects are ubiquitous in a wide variety of complex ph ysical phenomena, ranging from glassy dy-
namics and metamaterials to climate models. The Generalise d Langevin Equation (GLE) provides a rigorous
way to describe memory effects via the so-called memory kern el in an integro-differential equation. However,
the memory kernel is often unknown, and accurately predicti ng or measuring it via e.g. a numerical inverse
Laplace transform remains a herculean task. Here we describ e a novel method using deep neural networks
(DNNs) to measure memory kernels from dynamical data. As pro of-of-principle, we focus on the notoriously
long-lived memory effects of glass-forming systems, which have proved a major challenge to existing methods.
Speciﬁcally, we learn the operator mapping dynamics to memo ry kernels from a training set generated with
the Mode-Coupling Theory (MCT) of hard spheres. Our DNNs are remarkably robust against noise, in con-
trast to conventional techniques. Furthermore, we demonst rate that a network trained on data generated from
analytic theory (hard-sphere MCT) generalises well to data from simulations of a different system (Brownian
Weeks-Chandler-Andersen particles). Finally, we train a n etwork on a set of phenomenological kernels and
demonstrate its effectiveness in generalising to both unse en phenomenological examples as well as supercooled
hard-sphere MCT data. We provide a general pipeline, Kernel Learner, for training networks to extract mem-
ory kernels from any non-Markovian system described by a GLE . The success of our DNN method applied to
noisy glassy systems suggests deep learning can play an impo rtant role in the study of dynamical systems with
memory.
I. INTRODUCTION
Non-Markovian systems, i.e. those that exhibit memory ef-
fects, pose a number of major challenges to both analytic and
computational analysis. This issue is of particular import ance
as such systems occur across many different areas of mod-
ern physics, for example climate models [1], gene interacti on
networks [2], quantum-classical simulations [3, 4], and th e be-
haviour of supercooled liquids and glasses [5, 6], among man y
others.
A common, and very general, framework for describing
non-Markovian dynamics in continuous time is the Gener-
alised Langevin Equation (GLE), where memory effects are
included via a so-called memory kernel. GLEs are a common
occurrence across statistical physics and beyond as they ar e
produced by the Mori-Zwanzig projection operator formalis m
[7–9]. This formalism starts with a memoryless (e.g. Hamil-
tonian) system in a very high dimensional space, and project s
the dynamics onto a lower dimensional space consisting of
degrees of freedom that are of theoretical interest or are ex -
perimentally accessible. The price paid for this dimension al-
ity reduction is the emergence of memory effects in the low
dimensional dynamics. Memory kernels are often unknown
or highly non-trivial and so measuring them from data is an
important step in developing and testing non-Markovian the -
ories. Furthermore, including memory effects can be a highl y
computationally efﬁcient method of describing complex dy-
namics [10], hence it is desirable to have quick, accurate te ch-
niques for measuring memory kernels.
∗To whom correspondence should be addressed: m.j.kerr.wint er@tue.nl and
l.m.c.janssen@tue.nlGlasses and supercooled liquids are particularly challeng -
ing non-Markovian systems to study as they experience com-
plex dynamics over a wide range of length and time scales,
with memory effects lasting multiple orders of magnitude in
time. The Mori-Zwanzig method has proved to be very popu-
lar in the ﬁeld of glassy physics, and in particular forms the ba-
sis of Mode-Coupling Theory (MCT), a ﬁrst-principles, self -
consistent framework of the glass transition [5, 7, 11–14]. To
date, there is no complete theory of the glass transition, an d
MCT is no exception. Although the Mori-Zwanzig method
is exact, it results in an intractable expression for the mem -
ory kernel which must then be approximated in a number of
ways, varying in complexity depending on the ﬂavour of MCT
[12, 15–19]. This fact emphasises the memory kernel as an
object of particular importance to glassy physics, as it is t he
point at which an exact theory is abandoned in favour of ap-
proximations.
For both glassy and other non-Markovian systems, a GLE
of some autocorrelation function, y, can be written in the over-
damped limit as
y′(t)+Ωy(t)+/integraldisplayt
−∞dτK(τ)y′(t−τ) = 0, (1)
whereKis the memory kernel, and Ω, the so-called frequency
term, is a parameter that is in general known, which describe s
the memoryless evolution of y. In general GLEs also contain
a random force term, which is removed from Eq. 1by taking
the correlation of a variable with itself to get the autocorr ela-
tiony. In MCT, yis the autocorrelation function of density
ﬂuctuations. Equations with the form of Eq. 1are also called
memory equations. The question of how to study Kgiveny
at ﬁrst appears simple. By applying a Laplace transform to
Eq.1the kernel can be disentangled from its convolution with2
y′, resulting in an explicit expression for K,
K(t) =L/axisshort1/bracketleftbiggy(0)−(s+Ω)L[y]
sL[y]−y(0)/bracketrightbigg
, (2)
whereL[f(t)](s) =/integraltext∞
0e−stf(t)dtis the Laplace transform
of a function, f(t). The complexity arises from the fact that
in practice performing an inverse Laplace transform is a maj or
challenge.
The difﬁculties of performing a numerical inverse Laplace
transform are well known [20–22], and the extent of this prob -
lem is nicely summarised by Epstein and Schotland: “Our re-
sults give cogent reasons for the general sense of dread most
mathematicians feel about inverting the Laplace transform ”
[23]. Laplace inversion is an example of an ill-posed invers e
problem, where information is lost during any numerical im-
plementation of the forward transform, making the inverse
difﬁcult if not impossible. This effect can be demonstrated
by considering the forward Laplace transform of a function
expressed as a Fourier series, L[f(t)] =L[/summationtext
iaisin(ωit) +/summationtext
jbjcos(ωjt)]. The Laplace transforms of sinandcosare
L[sin(ωt)](τ) =ω/(ω2+τ2), andL[cos(ωt)](τ) =τ/(ω2+
τ2)respectively. As such, the amplitude of high-frequency
components in f(t)are suppressed by the ω2in the denomi-
nators of the respective Laplace transforms. Above some cri ti-
calω∗, the high-frequency components become indistinguish-
able from noise (either experimental or numerical) and henc e
are unrecoverable by the inverse transform. Consequently, nu-
merical inverse Laplace transforms are very sensitive to no ise,
with even numerical round-off errors potentially overwhel m-
ing the signal.
Many authors in the soft matter and glassy physics commu-
nities have taken an alternative approach to determining th e
memory kernel, whereby an implicit V olterra integral equa-
tion forKis constructed from pair-wise correlation functions
[10, 24–27]. Such correlation functions are constructed by
taking an ensemble average over a large number of trajectori es
of particle-based simulations. A variety of different nume rical
methods have been used to successfully solve such equations
for the short time memory effects of various systems [28–31] .
An alternative to both explicit Laplace inversion, and solv -
ing a V olterra equation, is to use a minimisation approach to
approximate the kernel. The problem of performing the in-
verse Laplace transform in Eq. 2can be reformulated as ﬁnd-
ing some ¯Ksuch that |L[¯K]−L[K]|< ǫ, for some ǫ∈Rthat
can be set arbitrarily small [21]. As the high-frequency par ts
of¯KandKare lost in the forward transform, this minimisa-
tion problem does not have a unique solution. Consequently,
it is helpful to include a regularisation functional, R, resulting
in the minimisation problem |L[¯K]−L[K]|+|λR[¯K]|< ǫ,
whereλis a parameter to control the strength of the regu-
larisation. A simple regularisation technique is to penali se
low order derivatives in the solution, i.e. R[¯K] =¯K′′, result-
ing in a smooth ¯K. In some cases, e.g. astronomical image
restoration, an entropy can be calculated for the image func -
tion, which is then maximised by a regularisation functiona l
[32]. Recent successes in modelling systems with memory ef-
fects over a wide range of timescales have also been achievedby moving away from the GLE completely in favour of time-
local non-Markovian methods with attractive computationa l
properties [33].
Complementary to conventional physical and mathematical
approaches, machine learning techniques are rapidly emerg -
ing as powerful and computationally efﬁcient tools for the
study of glasses [34–43] and complex soft matter systems
more generally [44, 45]. As such, the recent explosion of
interest in deep learning suggests a new route to extracting
memory kernels from GLEs following the minimisation phi-
losophy above. Deep Neural Networks (DNNs) have very
attractive generalisation and expressivity properties [4 6–48],
and so it is reasonable to ask whether a DNN could learn the
mapping from the function yto the kernel Kin Eq. 1. Previ-
ous authors have made signiﬁcant progress by parameterisin g
the kernel with manually curated sets of functions [49–51].
Our neural network parameterisation follows a similar phil os-
ophy while taking advantage of the very broad approximation
properties of DNNs. Such an approach has shown impressive
results when applied to analytic problems with rapidly deca y-
ing memory kernels [52]. By learning a mapping between
function spaces, our approach falls within the rapidly expa nd-
ing ﬁeld of deep operator learning, which was popularised by
the publication of DeepONet [53].
In this work we present a novel machine-learning based
method for kernel extraction from GLE data. As a demon-
stration of effectiveness, we train and apply DNNs to
the challenging problem of extracting memory kernels in
glass-forming systems that have very long-lasting memory
effects and signiﬁcant levels of noise. The networks are
far more robust to noise in the input signal than traditional
methods. Furthermore, we show that a network trained on
data generated from analytic theory (hard-sphere MCT)
generalises well to data from particle-based simulations
of a different system. MCT provides an analytic, though
approximate, memory kernel from which a training set
can be generated, whereas with simulations we study
the true dynamics, but with an unknown kernel. Our
network that has been trained on MCT data is available at
https://zenodo.org/record/7603275#.Y_vAxS8w1pQ .
While we focus on glass-forming materials here to es-
tablish proof of principle, our method is not limited to
glassy systems per se. In fact we hope it will be used to
study the form of kernels across a wide variety of non-
Markovian phenomena, and in particular systems that have
a less well developed body of theory than glassy materials.
Furthermore, memory effects play an important role in
coarse-grained or reduced order models (ROM) of complex
systems [54–56]. As such, being able to measure kernels
from a minimum of high resolution data is important.
Consequently, we have written a pipeline for users to train
networks on their GLE system of choice, which is available at
https://github.com/mkerrwinter/KernelLearner .3
II. PROBLEM DEFINITION AND NUMERICAL SETUP
Our general method can be brieﬂy summarised as follows.
Starting from a set of memory kernels similar to those we wish
to measure, training and testing sets are generated by solv-
ing the GLE using the same method as in [57], subjecting the
solutions to many noise realisations, and using the (soluti on,
kernel) pairs as input and output respectively. Multiple ne t-
works are trained over a range of hyperparameters, and an op-
timum network is selected which achieves the minimum test
loss. Finally, a novel memory kernel can be measured from
unseen input data, and validation can be performed by solv-
ing the GLE with this measured kernel to compare with the
input. The “ﬁrst guess” kernels could be derived from theory ,
generated to a low level of accuracy with existing kernel mea -
surement methods, measured from a similar system, or simply
be informed guesswork.
Our goal is to extract a memory kernel, K(t), from the
density autocorrelation function, F(t), of a glassy system de-
scribed by a GLE like Eq. 1. We useF(t)curves generated by
numerically solving MCT, as well as curves measured from
particle-based simulations. The neural networks are train ed
on MCT data, and validation is performed on both unseen
MCT data and simulation data. For comparison, we also ex-
tract the kernel by applying conventional (i.e. non-networ k)
methods.
As the variable of interest, MCT typically employs the au-
tocorrelation function
F(k,t) =1
N/an}bracketle{tρ−k(0)ρk(t)/an}bracketri}ht, (3)
whereρk=/summationtext
jeik·rj(t)is the microscopic density in Fourier
space, ka wavevector, Nthe number of particles, and /an}bracketle{t·/an}bracketri}htde-
notes an ensemble average. The overdamped MCT GLE has
the form
˙F(k,t)+D0k2
S(k)F(k,t)+
/integraldisplayt
0dτK MCT(k,τ)˙F(k,t−τ) = 0,(4)
wherek=|k|is the wavenumber, D0is the self-diffusion
coefﬁcient, S(k) =F(k,0)is called the static structure factor,
andKMCTis the kernel. The MCT kernel is given by
KMCT(k,t) =ρD0
16π3/integraldisplay
dqV2
q,k−qF(q,t)F(|k−q|,t),(5)
whereρis the average number density, the vertex term
Vq,k−q=k−1[k·qc(q)+k·(k−q)c(|k−q|)], andc(k) =
ρ−1[1−1/S(k)](for more details see e.g. [5]). In general,
F(t)is a function of the wavenumber, and the MCT kernel
couples different wavenumbers together. Equation 4is sub-
ject to the initial condition S(k) =F(k,0).
To numerically solve the MCT equation we use the Percus-
Yevick closure for a system of hard spheres [58, 59]. This is
an analytic approximation for the static two-point correla tion
functionS(k)of a system of hard spheres at a given density.For simplicity, we test our method on the F(k,t)behaviour
at wavenumber k=k∗, corresponding to the main peak of
S(k). From here on we will omit the explicit kdependence
and useF(t) =F(k∗,t)for brevity. Note that by restricting
ourselves to the peak wavenumber we only achieve accurate
network predictions at k∗, however the training set can easily
be extended to include more wavenumbers.
The simulation data we use is from a system of particles in-
teracting via the Weeks-Chandler-Andersen (WCA) potentia l
[60]. Details of the simulations are given in Appendix B. Note
that the type of particle in the simulation data (soft sphere s) is
different from the MCT data used to train the networks (hard
spheres).
A. Dataset, network and training process
Both the training dataset and MCT testing dataset are pro-
duced in the same way. First, Eq. 4is solved numerically
at volume fractions φ∈ {0.45,0.451,0.452...,0.58}, that are
symmetric about the MCT glass transition φg≈0.516[11], to
produce a set of analytic curves denoted FA(t). Each curve is
then subjected to 1000 different realisations of Gaussian n oise
like
F(t) =FA(t)+µ(max(FA)−min(FA))ξ(t), (6)
whereξ(t)is Gaussian noise with unit variance and zero
mean, and /an}bracketle{tξ(t)ξ(t′)/an}bracketri}ht=δ(t−t′). The parameter µcontrols
the strength of the noise. By adding noise to the training set
the DNN learns the mapping from noisy dynamics to a clean
memory kernel, and hence is able to handle real data measured
from simulations or experiments. The dimension of the set of
noisy curves, {F(t)}, is then reduced with Principal Compo-
nent Analysis (PCA) where only the ﬁrst 15 PCA components
are retained [61]. Note that PCA is a generic and automated
method for ﬁnding a suitable lower-dimensional representa -
tion of data, i.e. it does not rely on any domain speciﬁc knowl -
edge about how best to encode the data, and so allows this
pre-processing method to be broadly applicable to many dif-
ferent systems. Concretely, our original dataset consisti ng of
Pexamples, each deﬁned on a time grid of 4352 points (with
the grid spacing doubling as in [19]), is transformed to Pex-
amples in a 15-D space which serves as our DNN-input. The
explained variance per principal component is shown in Fig.
1(b), which levels off at the 15th component. As well as the
PCA components, we include the frequency term, Ω =D0k2
S(k),
and the value of the autocorrelation function at the ﬁnal tim e-
point,F(tmax), in order to clearly distinguish between liquids
(F(tmax) = 0 ) and glasses ( F(tmax)>0). The dimensionally
reduced set of noisy curves, Ω, andF(tmax)form the input of
the network, of dimension 17. The target is the KMCTused to
produce a given F(t), discretised on a logarithmically spaced
time grid of 100 points. Consequently, the output of the neur al
network is also deﬁned on this time grid. Both the testing and
the training set consist of P= 65500 such examples.
We use a fully connected feed-forward network with L+14
036912 15 18 
PCA index100
104
107Explained variance 
KMCT (t)
00.0010.010.050.120 100300LTest(a)
(b) (c)[FPCA ,Ω,F (tmax )] 
FIG. 1.(a)A schematic of the network structure. The input consists
of the ﬁrst 15 PCA components of a pairwise correlation funct ion
F(t), the frequency term Ω, and the ﬁnal correlation value F(tmax).
The output is an approximation of the memory kernel used to ge ner-
ateF(t)via Eq. 4 [62]. (b)The explained variance of consecutive
PCA components of the training set, which display a kink betw een
components 12 and 15. (c)Minimum test loss values for a subset of
networks in the hyperparameter search. These networks have a width
factorω= 8, and were trained with a batch size of 2500. The plot
shows the variability between networks with different regu larisation
λ, and different initial conditions.
layers, with input x, and output given by
fα(x,Θ) =aL+1
α, (7)
ai
β=/summationdisplay
αWi
αβσ(ai−1
α)−Bi
β,2≤i≤L+1
(8)
a1
β=/summationdisplay
αW1
αβxα−B1
β. (9)
Greek letters index the matrices of weights between each
layer, whereas Latin letters index the layers themselves, h ence
Wi
αβare the elements of the matrix of weights between layer
i−1andi. The biases are given by Bi
α. The variable Θ
is a vector containing all weights and biases. For the acti-
vation functions we choose the popular Rectiﬁed Linear Unit
(ReLU),σ(z) = max(0,z). We use the dropout method to
reduce overﬁtting [63], so that at each training step nodes
are temporarily dropped from the network with probability
p= 0.5. The network structure consists of L= 6 hid-
den layers that gradually increase in width. The hidden laye rwidths are [50ω,100ω,150ω,200ω,250ω,300ω], where the
width factor ωis an integer hyperparameter controlling the
width of the network. We have borrowed this triangular net-
work structure from the authors of [52], who address a simila r
problem. We chose a fully connected network (as opposed to
e.g. a convolutional neural network) as it is the most genera l
feed forward architecture, and is determined by a relativel y
small set of hyperparameters. The structure of the network,
input, and output data is shown schematically in Fig. 1(a).
We use a weighted mean square error loss function between
network output at neuron j,fj, and the true kernel in adimen-
sional form ˜K(tj), with L2 regularisation on the network pa-
rameters, Θk, to prevent overﬁtting [64],
L=1
PP/summationdisplay
i=11
jmaxjmax/summationdisplay
j=1αj(fj−˜K(tj))2+λM/summationdisplay
k=1Θ2
k,(10)
where the ﬁrst sum is over examples in the training set, and
the second is over the time points tjat which ˜Kis discretised.
The training set is produced using natural units. Space is me a-
sured in terms of the particle diameter, d, and time is measured
ind2/D, whereDis the diffusion constant. The units are cho-
sen such that d=D= 1, and˜K=Kd2/D. The weights
increase linearly with the time grid, αj=j/j max, so that long-
time behaviour is given more importance. This is because the
kernel at long times affects the dynamics to a greater extent
than at short times. The parameter λcontrols the strength of
L2 regularisation over the Mnetwork parameters. Its effect
on the test loss is demonstrated in Appendix C. Training is
performed using the Adam method, a popular stochastic gra-
dient descent algorithm [65]. We use early stopping to avoid
overﬁtting, i.e. we select the optimum network state that gi ves
the minimum test loss, as demonstrated in Appendix C.
B. Non-network methods
For comparison with the above network-based method, we
also implement two traditional kernel extraction methods. The
ﬁrst applies an inverse Laplace transform to Eq. 2. We use
the De Hoog algorithm to evaluate L−1using a Fourier series
with accelerated convergence [66, 67]. This method outper-
formed other common inversion algorithms (Talbot [68] and
Stehfest [69]) on our data. To mitigate the effect of noise we
ﬁrst smooth the F(t)curves with a Savitzky-Golay ﬁlter [70]
before applying the De Hoog inverter. The second method is
to construct and solve a V olterra integral equation for K, the
details of which are given in Appendix G.
III. RESULTS
A. Hyperparameter search
We ﬁrst search for optimum network hyperparameters. As
there are a large number of hyperparameters that can be op-
timised a full grid search is unfeasible. Instead, we choose5
the L2 regularisation strength λ, the batch size of the Adam
method, and the width factor ωas the most important hyper-
parameters, and perform a search over reasonable intervals for
each. This process is shown for λin Fig. 1(c), where multiple
initial conditions have been included for each λ. As well as
demonstrating the effect of λon the test loss, this ﬁgure illus-
trates the signiﬁcant randomness introduced by using diffe rent
initial conditions for Θ, and hence the importance of training
multiple networks with different initial conditions. We se lect
the network with the lowest test loss across the whole hyper-
parameter search.
B. Performance on MCT Percus-Yevick hard spheres
We apply our optimum network to the task of extracting
memory kernels from unseen F(t)curves generated by hard
sphere MCT and subjected to noise according to Eq. 6. For
comparison, we use the De Hoog algorithm to extract the ker-
nel by means of Eq. 2applied to noisy F(t)curves. It is
reasonable to ask whether a simple smoothing procedure to
mitigate the effect of noise would be sufﬁcient to achieve re a-
sonable performance without resorting to deep learning. To
investigate this, we also use the De Hoog algorithm on F(t)
curves that have been smoothed by a Savitzky-Golay ﬁlter.
The left-hand panels of Fig. 2show both the noisy and
smoothed F(t). The two right-hand panels show the ker-
nel measured by our neural network, Laplace inversion of
Eq.2, and Laplace inversion of Eq. 2using the smoothed
F(t). We measure kernels in both the liquid (volume frac-
tionφ= 0.475) and glass ( φ= 0.52) regimes, subjected to
both high ( µ= 10−2) and low ( µ= 10−5) levels of noise.
In all cases the network reproduces the true MCT kernel from
F(t)to a very high degree of accuracy. In contrast, Laplace
inversion fails to produce an accurate (or indeed even vague ly
reasonable) kernel across all times. Although the smoothin g
process on F(t)in the left-hand panel of Fig. 2(a)and(c)
looks very effective by eye, it does not signiﬁcantly improv e
the accuracy of the measured kernel. As can be seen in the
low noise plots, Fig. 2(b)and(d), the noise on F(t)must
be reduced to the point where it is no longer visible by eye
before conventional Laplace inversion can measure a reason -
able kernel over multiple decades. Even then, this approach
fails at short times. Interestingly, when noise is this low t he
smoothing process actually decreases performance.
In Fig. 3we demonstrate that the DNN hugely outperforms
De Hoog Laplace inversion once again, this time in the super-
cooled regime, very close to the glass transition point. Thi s is
a particularly challenging region of parameter space as F(t)
becomes very sensitive to small changes in the initial condi -
tions. Fig. 8in Appendix Dshows how the network extrapo-
lates well to regions of phase space not included in the train ing
set. The ability to generalise is crucial for the usefulness of the
DNN method. The caveat to this extrapolation is that deep in
the glass phase, at volume fractions higher than those in the
training set, the dynamics become only weakly dependent on
the kernel and the performance of the network decreases.
The results in Fig. 2demonstrate the extreme susceptibility103101105
t0.000.250.500.751.00FSG filter
103101105
t0200400KMCT
DNN
103101105
t0200400KMCT
De Hoog
SG De HoogLiquid phase, µ= 10 −2
103101105
t0.000.250.500.751.00FSG filter
103101105
t0200400KMCT
DNN
103101105
t0200400KMCT
De Hoog
SG De HoogLiquid phase, µ= 10 −5
103101105
t0.900.951.00FSG filter
103101105
t0200400600800KMCT
DNN
103101105
t0200400600800KMCT
De Hoog
SG De Hoog
Glass phase, µ= 10 −5(c)
103101105
t0.900.951.00FSG filterGlass phase, µ= 10 −2
103101105
t0200400600800KMCT
DNN
103101105
t0200400600800KMCT
De Hoog
SG De Hoog(d)(a)
(b)
FIG. 2. Examples of kernel extraction from a density autocor relation
function using a trained network and the De Hoog algorithm. (a)
The liquid phase of Percus-Yevick Hard Sphere MCT with volum e
fractionφ= 0.475. The left-hand panel shows the noisy F(t)curve,
withµ= 10−2as deﬁned in Eq. 6, as well as the smoothed curve
produced by the Savitzky-Golay ﬁlter. The middle panel cont ains
the memory kernel as measured by a network and the true MCT ker -
nel. The right-hand panel contains the kernel measured by th e De
Hoog algorithm from the smoothed F(t)(‘SG De Hoog’), the kernel
measured from the raw noisy curve (‘De Hoog’), and the true MC T
kernel.(b)The same curves measured from F(t)atφ= 0.475with
µ= 10−5.(c)and(d)are the same measurements again but in the
glass phase with φ= 0.52.
of Laplace inversion to noise, as well as the huge improve-
ment achieved by deep learning. Furthermore, as shown in
Fig. 3and Appendix D, our network method is highly effec-
tive in difﬁcult regions of parameter space, and in regions n ot
included in the training set. Finally, the trained DNN is hun -
dreds of times faster than the De Hoog algorithm. Our deep
learning method comprehensively outperforms conventiona l
Laplace inversion for measuring the memory kernel of hard
sphere MCT.6
104101102105
t0200400600800K
104101102105
t0.00.20.40.60.81.0F
SG filter
104101102105
t0200 400 KMCT 
De Hoog 
SG De Hoog
104101102105
t0200 400 KMCT 
DNN
FIG. 3. Left panel: density correlation function F(t)in the super-
cooled regime as predicted by MCT for hard spheres at volume f rac-
tionφ= 0.515, which is very close to the MCT glass transition
0.515< φg<0.516. Middle panel: the corresponding MCT mem-
ory kernel and the kernel predicted by DNN. Right panel: the c or-
responding memory kernel as obtained by the De Hoog algorith m,
applied to both noisy and smoothed F(t)data.
C. Performance on simulated soft spheres
To test the limits of the network’s performance, we apply it
to data from a different system than the one it was trained on.
We run Brownian dynamics simulations of WCA monodis-
perse spheres (see Appendix B), and measure F(t)both from
single trajectories, and an ensemble average of several hun -
dred trajectories. These simulations are in the liquid regi me, at
temperatures just above the solid-liquid binodal below whi ch
the system crystallises. We measure kernels by two methods,
namely from our neural network, and by constructing an im-
plicit V olterra integral equation for K(see Appendix G). The
neural network has not been retrained, i.e. it has only seen t he
MCT hard sphere training set of the previous section. This
is in order to study the ability of the network to generalise t o
an unseen system. For validation, we solve Eq. 4using the
kernels measured by the network or V olterra method in place
ofKMCT, resulting in a new F(t)curve which we can com-
pare with F(t)from the simulations. In Fig. 4(a)the noisy
F(t)of a single simulation trajectory (left-hand panel) is used
as input. The network is able to reproduce dynamics that are
very close to the true dynamics of the system calculated by
averaging many trajectories (right-hand panel). In contra st,
the V olterra method, which is at ﬁrst more accurate than the
DNN, begins to oscillate wildly and soon diverges. The same
measurements are repeated at a lower temperature (but still in
the normal liquid regime) in Fig. 4(c). Similar to the hard-
sphere case, the DNN method hugely outperforms traditional
methods in the presence of noise. In Fig. 4(b)and(d)the in-
put data is the ensemble-averaged F(t). The right-hand pan-
els compare the dynamics using the network kernel, and the
V olterra kernel, to the true dynamics from the left-hand pan el.
In this low-noise context the V olterra method performs very
well. At high Tthe network is able to produce dynamics that
are very similar to the ground truth, whereas at low Tthe net-
work is less accurate, but not catastrophically so. This dis -
crepancy is due to the greater role Markovian dynamics (con-
trolled by the frequency term, Ω) play at high temperatures.
An explicit comparison between the simulated dynamics, net -
work prediction, and MCT is given in Appendix E. Impor-
tantly, the network-predicted dynamics exhibit an incorre ct
non-zero plateau at long times. The long-time behaviour of(a)
(c)(b)
(d)107105103101101
t0.20.40.60.81.0F
105103101101
t0.00.20.40.60.81.0F
107105103101101
t0.20.40.60.81.0F
105103101101
t0.00.20.40.60.81.0F
107105103101101
t0.2 0.4 0.6 0.8 1.0 F
105103101101
t0.00.20.40.60.81.0F
107105103101101
t0.2 0.4 0.6 0.8 1.0 F
105103101101
t0.00.20.40.60.81.0FT= 3 ,single trajectory 
T= 3 ,multiple trajectories 
T= 1 ,multiple trajectories T= 1 ,single trajectory 107105103101101
t0.0 0.2 0.4 0.6 0.8 1.0 FSim
DNN 
Volterra 
107105103101101
t0.0 0.2 0.4 0.6 0.8 1.0 FSim
DNN 
Volterra 
107105103101101
t0.0 0.2 0.4 0.6 0.8 1.0 FSim
DNN 
Volterra 
107105103101101
t0.0 0.2 0.4 0.6 0.8 1.0 FSim
DNN 
Volterra 
FIG. 4. Reproducing F(t)from WCA simulations. In the left-hand
panels of (a)and(c),F(t)is calculated from a single simulation tra-
jectory and hence exhibits signiﬁcant levels of noise. In th e left-hand
panels of (b)and(d)and in the right-hand panels clean F(t)curves
are produced by averaging over many trajectories (‘ /angbracketleftSim/angbracketright’). Ker-
nels are measured from F(t)with the network (‘DNN’) and V olterra
(‘V olterra’) methods, which are then used to solve Eq. 4. The result-
ingF(t)curves are plotted in the right-hand panels. A perfect kerne l
measurement would result in exactly the same curve as ‘ /angbracketleftSim/angbracketright’. In
the low-noise plots the V olterra line is almost indistingui shable from
the data. Temperatures are in Lennard-Jones units, deﬁned i n Ap-
pendix B.
F(t)is very sensitive to the long-time behaviour of K, hence
small non-zero values in the tail of K(a likely outcome of any
minimisation routine) can result in a non-zero tail in F(t).
Let us now discuss the computational efﬁciency of the DNN
approach as compared to that of the V olterra method. Af-
ter generating the HS-MCT training set (which took approx-7
imately 35 minutes on a 2020 Macbook Pro), the DNN was
trained to its minimum test loss state on a single Xeon E5
(2019) CPU core in 24 hours. Measuring a kernel with the
trained network subsequently takes less than a second. The
dynamics shown in Fig. 4(b)and4(d)(V olterra curves) each
required approximately 100 hours of computing time, again
on a single Xeon E5 core. As such, the DNN method is signif-
icantly faster in this instance. However, a note of caution i s re-
quired. Many factors can dramatically effect the efﬁciency of
the DNN method, both slowing it down (performing hyperpa-
rameter searches, using more data, using larger networks) a nd
speeding it up (using a learning rate schedule, using GPUs,
transfer learning), and it is important to note that minimis ing
computing time was not a priority in this work. Similarly for
the V olterra method, simulating different systems, of diff er-
ent sizes, and for different amounts of time, will have a larg e
impact on the efﬁciency.
As well as demonstrating that the DNN can accurately mea-
sure kernels in a system that differs from the training set, F ig.
4is also an example of how kernel extraction can be used in
reduced order modelling. The ‘DNN’ curves are generated by
the relatively cheap process of training a network on HS-MCT
theory, measuring the kernel from a single simulation traje c-
tory, then solving a GLE with this kernel. The resulting dy-
namics closely approximate the much more expensive ‘ /an}bracketle{tSim/an}bracketri}ht’
curves, which were generated by running many repeats of the
full particle resolved simulations.
It is important to recall that the DNN has not been retrained
on the soft sphere simulations. The input data in Fig. 4dif-
fers from the training set in how it was generated (simulatio n
vs theory), the nature of the noise on F(t), and in the sys-
tem itself (WCA vs hard spheres). Despite these multiple dif -
ferences, our results demonstrate how well our DNN method
generalises to new systems. Furthermore, the network can re -
produce ensemble-averaged dynamics from noisy data, unlik e
conventional methods, allowing a clean measurement to be
made from a single simulation trajectory.
D. A phenomenological training set
In many situations there is no established theory from
which to construct a training set of GLE solutions and mem-
ory kernels. In such a case it is instead possible to use physi cal
intuition and educated guesswork to generate training data . As
a proof of principle, we demonstrate this process on a train-
ing set of phenomenological kernels that exhibit liquid-li ke,
supercooled liquid-like, and glass-like behaviour, i.e. s imple
fast relaxation, two step relaxation, and persistent memor y ef-
fects respectively.
Our phenomenological memory kernels are parameterised
as
K(t) =a
(1+btc)d+fe−(t
10g)h, (11)
where the parameters a,b, etc. are chosen manually to mimic
the behaviour of the three regimes (speciﬁc values are given in
Appendix F). The form of Eq. 11was chosen to capture bothfast and slow relaxation regimes. Furthermore, it is known
that the approach to the supercooled plateau is a power law,
and the long time relaxation can be well ﬁtted to a stretched
exponential [5, 7, 71]. The ﬁrst and second terms of Kare
chosen to reﬂect this. Next, we solve the GLE using these
memory kernels to obtain corresponding F(t)curves, add
noise using the procedure in Eq. 6withµ= 10−2, and con-
struct a training set of (solution, kernel) pairs. The perfo r-
mance of the network on unseen, noisy F(t)curves gener-
ated from kernels parameterised by Eq. 11is shown in Fig. 5
for liquid-like, supercooled liquid-like and glass-like k ernels.
The network predictions in all three regimes are accurate, a nd
hence demonstrate the ability of the network to learn a train ing
set of kernels with the complex parameterisation of Eq. 11.
107103101105
t0200 400 600 800 KDNN
Ground truth Liquid Supercooled Glass 
107103101105
t0200 400 600 800 KDNN
Ground truth 
107103101105
t0200 400 600 800 KDNN 
Ground truth 
FIG. 5. Kernel predictions made by the DNN trained on phenome no-
logical kernels when applied to unseen, noisy F(t)in the liquid-like,
supercooled liquid-like and glass-like regimes.
Having trained the DNN on this phenomenological dataset,
we then apply it to a noisy F(t)of hard sphere MCT in the
supercooled regime, where the dynamics exhibit a typical tw o
step relaxation. The result is shown in Fig. 6, where the DNN
makes a very accurate prediction of the true MCT kernel de-
spite never having seen MCT data. Fig. 6demonstrates the
ability of our deep learning approach to generalise beyond
phenomenological training data to physically realistic un seen
examples.
107103101105
t0.00 0.25 0.50 0.75 1.00 F
103101105
t0200 400 600 800 KMCT 
DNN 
FIG. 6. The performance of a network trained on phenomenolog ical
data, applied to hard sphere MCT in the supercooled regime. L eft
panel: the input noisy F(t)calculated from hard sphere MCT. Note
this curve exhibits the two step relaxation that is typical o f super-
cooled liquids. Right panel: the true memory kernel (‘MCT’) and
the kernel predicted by the network (‘DNN’).8
IV . CONCLUSION
In this work we develop a novel deep learning method for
measuring the memory kernel of GLEs. Our method is gener-
ally applicable to any GLE system where a training set can be
constructed. We demonstrate its effectiveness on MCT hard
sphere data, and show that the DNN measures highly accurate
kernels on unseen inputs, with particular robustness to noi se.
This is in stark contrast with existing Laplace inversion me th-
ods that are highly sensitive to noise. Furthermore, the DNN
method generalises well to a system different from that used
for training, and can even be used to make accurate predic-
tions without training data generated from rigorous theory .
Our DNN method has several attractive features. It is com-
putationally efﬁcient as the time to generate a training set and
train the network can be signiﬁcantly less than the correspo nd-
ing simulation time required by conventional methods, and
making predictions with a trained network requires negligi ble
computational resources. It also makes no distinction betw een
short or long memory effects. This is in contrast to existing it-
erative techniques where errors accumulate over long times ,
posing a particular problem with long-lived glassy kernels .
The ability of the DNN to map noisy single trajectory inputs
to clean, ensemble averaged outputs is particularly powerf ul,
allowing it to be used in situations where it is difﬁcult to me a-
sure multiple trajectories for averaging, e.g. costly simu lations
or experiments.
As is the case with any machine learning technique, the
network performs less well when presented with data that
is dissimilar to that used during training, as seen with the
low-temperature WCA data with low noise in Fig. 4(d).
However, this shortcoming can be addressed by including
more diverse examples in the training set. As such, we
present our code as a pipeline for training networks on data
of the user’s choosing, as well as our own trained network
for the case of hard spheres. Giving users the tools to train
networks on their speciﬁc problem will result in signiﬁcant ly
smaller, yet better performing, networks than attempting t o
train a general-purpose GLE kernel extractor. The success
with which neural networks can learn the highly non-trivial
mapping between F(t)andKsuggests that deep learning
techniques should be considered for a diverse range of inver se
problems where a training set can be generated by solving the
simpler forward problem.
V . ACKNOWLEDGEMENTS
It is a pleasure to thank Sonja Georgievska, Meiert Grootes,
and Jisk Attema of the Netherlands eScience Center for many
valuable discussions in the context of the Small-Scale Ini-
tiative on Machine Learning. We acknowledge the Dutch
Research Council (NWO) for ﬁnancial support through a
START-UP grant (MKW, VED, and LMCJ) and Vidi grant
(IP and LMCJ).Appendix A: Hyperparameter search
A hyperparameter search was performed over the L2
regularisation parameter, λ, the batch size, B, and width
factor,ω. For each combination of parameters, 5 networks
with different initial conditions were trained. The initia l
condition was set by drawing weights and biases from a
random uniform distribution U(−√
k,√
k), where1/kis the
number of input features to the layer. The hyperparameter
values in the search were
•λ: 0.1, 0.05, 0.01, 0.001, 0
•B: 300, 2500, 10000
•ω: 2, 4, 8,
resulting in a total of 5×3×3×5 = 225 networks. The
depth of the network (6 hidden layers) is somewhat arbitrary ,
and was chosen by balancing the greater expressivity of deep
networks with the increase in training time as the network ge ts
larger. The Adam optimizer parameters, referred to as β1and
β2in [65], were kept as their default values of 0.9 and 0.999
respectively. The learning rate, µLR, was set to 10−3. It would
have been preferable to include the hyperparameters β1,β2,
andµLRin our grid search, however this was computationally
infeasible.
Appendix B: Simulation details
Simulations were performed of a set of N= 2000 Brow-
nian particles in 3D, with periodic boundary conditions and a
number density ρ= 0.95. The position of particle i,ri, obeys
the overdamped Langevin equation,
˙ri=ζ−1Fi+ξi(t), (B1)
whereζ= 1 is a friction coefﬁcient, Fiis a force acting on
particleidue to the inter-particle potential, and ξiis a ran-
dom noise term obeying /an}bracketle{tξi(t)/an}bracketri}ht=0, and/an}bracketle{tξi(t)·ξj(t′)/an}bracketri}ht=
6D0δijδ(t−t′), whereD0=kBT/ζ is the diffusion con-
stant,kBthe Boltzmann constant, and Tthe temperature. The
interaction force comes from the Weeks-Chandler-Andersen
potential,
U(r) = 4ǫ/bracketleftbigg/parenleftBigσ
r/parenrightBig12
−/parenleftBigσ
r/parenrightBig6/bracketrightbigg
+ǫ, (B2)
whereris the inter-particle distance. We use Lennard-Jones
units such that σ= 1 is the particle diameter, and ǫ= 1
a parameter determining the strength of the interaction. A
dimensionless temperature can be deﬁned as T∗=kBT/ǫ.
U(r)is truncated such that the potential is purely repulsive.
The simulations were performed with the LAMMPS molec-
ular dynamics software [72] with timestep ∆t= 10−5. The
system is left to evolve for 107timesteps to equilibrate, then
measurements are taken over a further 107timesteps.9
Appendix C: Avoiding overﬁtting
Overﬁtting is a common problem in deep learning, partic-
ularly when the size of the network is much larger than the
size of the dataset, as is the case in this work. Several meth-
ods have been suggested for avoiding overﬁtting, including
but not limited to regularisation, early stopping, and the u se
of dropout layers. We use all three, and also perform a pa-
rameter search over the regularisation parameter λto ﬁnd the
optimum level of regularisation. In Fig. 7(a)we demonstrate
our use of early stopping, where we use the state of the net-
work when it achieves the minimum test loss, not at the end
of training. In Fig. 7(b)we show the effect of varying λ.
Due to limited computational resources we did not perform a
parameter search over the dropout probability, instead set ting
it to 0.5 for all networks.
020000 40000 60000 
Time steps 010000 20000 Loss Test loss 
NaN 
00.001 0.01 0.05 0.1 20 100 300 LTestOptimum test loss (a) (b)
FIG. 7.(a)The test loss of the DNN during training. Beyond a cer-
tain time point the system begins to overﬁt and the test loss i ncreases.
Consequently, we use the state of the network at the minimum t est
loss, as indicated by the red arrow. Note that the x axis is in t ime
steps, not epochs. 1 epoch = P/B timesteps. (b)The minimum
test loss achieved by networks with varying λ(andω= 8, batch
size=2500). We select the λthat results in the minimum test loss, as
indicated by the red arrow.
Appendix D: Exploring unseen phase space
The network is trained on a set of F(t)curves generated at
volume fractions φ∈ {0.45,0.451,0.452...,0.58}, and sub-
jected to multiple noise realisations. How does the network
perform on volume fractions that are not in the training set?
This question is addressed in Fig. 8. Fig. 8(a)shows the per-
formance of the network at a volume fraction below the range
of the training set. The left-hand panel is the input noisy F(t),
and the right-hand panel shows the measured K, which agrees
with the true MCT kernel to a high degree of accuracy, with a
small discrepancy at very short times. Fig. 8(b)is at a volume
fraction that is not in the training set, but is within the ran ge of
trainingφvalues. In this case the network measures a highly
accurate kernel, as it does at φvalues in the training set. Fig.
8(c)and(d)are at volume fractions above the upper end of
the training set. Here the measured kernel is inaccurate. Th e
reason for this can be seen in the F(t)curves in the left-hand
panels. At φ= 0.59andφ= 0.62we are deep in the glass
phase, and the system decorrelates very little (i.e. the asy mp-
tote ofF(t)at long times is greater than 0.99 in both cases).As such, the dynamics become less and less sensitive to the
exact form of the memory kernel. These plots demonstrate
that the network can generalise well to regions of phase spac e
beyond the training set, however care must be taken. For some
parameter values (e.g. φdeep in the glass phase) the network
is insufﬁciently sensitive to the weak relationship betwee nK
andF(t).
103101105
t0.00 0.25 0.50 0.75 1.00 F
103101105
t0100 200 300 400 K(a)φ= 0 .44 
103101105
t0.00 0.25 0.50 0.75 1.00 F
103101105
t0200 400 Kφ= 0 .4655 (b)
103101105
t0.9900 0.9925 0.9950 0.9975 1.0000 F
103101105
t0500 1000 1500 2000 K(c)φ= 0 .59 
103101105
t0.9900 0.9925 0.9950 0.9975 1.0000 F
103101105
t01000 2000 Kφ= 0 .62 (d)104101102105
t0200 400 KMCT 
DNN 
104101102105
t0200 400 KMCT 
DNN 
104101102105
t0200 400 KMCT 
DNN 
104101102105
t0200 400 KMCT 
DNN 
FIG. 8.F(t)curves and the corresponding kernels, ground truth
given by MCT and approximated by the network, at volume fract ions
not in the training set. (a)Below the lowest φin the training set. (b)
Between values in the training set. (c)and(d)Above the highest
value in the training set.
Appendix E: Performance on soft spheres
Here we look in more detail at the performance of the DNN
that has been trained on Percus-Yevick hard sphere MCT, and
then applied to data from simulations of Weeks-Chandler-10
Andersen spheres. In Fig. 9we plot the dynamics predicted by
the DNN, along with the ensemble averaged dynamics of the
simulations, the prediction of the V olterra method, and the dy-
namics as predicted by both HS and WCA MCT. In 9(a), the
comparison is done at high temperature. In this plot, the DNN
signiﬁcantly outperforms HS MCT, despite being trained on
this theory. This is because the network also uses as input
the frequency term, Ω, calculated from the simulation static
structure factor. It is to be expected that at high temperatu re
the Markovian dynamics (determined by Ω) play a large role.
In Fig. 9(b)the comparison is made at a lower temperature,
hence the performance of the DNN is worse, though still bet-
ter than HS MCT. In this example the WCA MCT predic-
tion is also inaccurate, highlighting the approximate natu re of
the theory. These plots demonstrate the importance of know-
ing the limitations of the training set when applying machin e
learning methods. Better performance would be achieved by
using a training set of either theoretical or phenomenologi cal
kernels that more accurately describe the low temperature b e-
haviour of WCA particles.
107105103101101
t0.0 0.2 0.4 0.6 0.8 1.0 FSim
DNN 
Volterra 
WCA MCT
PYHS MCT 
107105103101101
t0.00.20.40.60.81.0FSim
DNN 
Volterra 
WCA MCT 
PYHS MCT (a) (b)
FIG. 9. A comparison of the dynamics from averaging many simu la-
tion trajectories ( /angbracketleftSim/angbracketright), solving the GLE with the kernel measured
by the DNN method from a single simulation trajectory (‘DNN’ ),
the V olterra kernel (‘V olterra’), the Weeks-Chandler-And ersen MCT
kernel, and the Percus-Yevick Hard Sphere MCT kernel. All me th-
ods are presented at T= 3in(a), and T=1 in (b).
Appendix F: Generating a phenomenological dataset
The phenomenological kernels are parameterised according
to
K(t) =a
(1+btc)d+fe−(t
10g)h. (F1)
The training set was generated by combining three regions of
parameter space: liquid, supercooled liquid, and glass. Th e
liquid kernels used all combinations of parameter values in
the following table:
a{240,275,305}
b{15,12,10}
c{0.65,0.6,0.55}
d{1.86,1.73,1.6}
f{200,240,280}
g{−3.5,−3.25,−3.1}
h{0.85,0.83,0.8}The supercooled kernels were from all combinations of the
following parameters:
a{660,710,760}
b{12000,30000,100000}
c{1.16,1.26,1.36}
d{0.45,0.35,0.28}
f{105,90,80}
g{0.02,1.0,3.1}
h{0.43,0.49,0.52}
And ﬁnally the glass kernels were from all combinations
of the following parameters:
a{780,795,810}
b{2800,9000,15000}
c{0.98,1.08,1.16}
d{0.5,0.45,0.4}
f{5800,2000,140}
g{−300,−140,−8}
h{0.002,−0.06,−0.114}
These values were selected by ﬁrst ﬁtting the form of Kto
the hard sphere MCT kernel at φ= 0.45,0.515and0.52(in
the liquid, supercooled, and glass regimes respectively) t hen
varying the parameters about their ﬁtted values. Each kerne l
was used to solve the GLE, and the resulting F(t)was sub-
jected to four realisations of Gaussian noise. The (solutio n,
kernel) pairs were then randomly shufﬂed and split 50:50 int o
training and testing sets.
Appendix G: Volterra Method
Our goal is to ﬁnd an expression for the irreducible mem-
ory kernel K(k,t) =1
Nk2D0/angbracketleftBig
R∗
keΩ†Q′QtRk/angbracketrightBig
. Here,Rk=
QΩ†ρkis the ﬂuctuating force, Ω†is the conjugate of the
Smoluchowski operator, and Q= 1− P is the projector on
the space orthogonal to that spanned by the density modes,
given by P=ρk/an}bracketri}ht/an}bracketle{tρ∗
kρk/an}bracketri}ht−1/an}bracketle{tρ∗
k. Additionally, we have de-
ﬁned a second projection P′=ρk/an}bracketri}ht/angbracketleftbig
ρ∗
kΩ†ρk/angbracketrightbig−1/angbracketleftbig
ρ∗
kΩ†, and
its complement Q′= 1− P′(for more details see [73] and
[74]).
Since the evolution operator eΩ†Q′Qtis hard to deal with,
we apply the Dyson decomposition identity,
eΩ†Q′Qt=eΩ†t−/integraldisplayt
0dτeΩ†Q′Q(t−τ)Ω†(1−Q′Q)eΩ†t,
(G1)
which yields a V olterra equation for the memory kernel
K(k,t) =KΩ†(k,t)+/integraldisplayt
0dτK(k,t−τ)W(k,τ).(G2)
Here we have introduced the function
KΩ†(k,t) = (Nk2D)−1/angbracketleftBig
R∗
keΩ†tRk/angbracketrightBig
, (G3)11
and the correlation
W(k,t) = (Nk2D)−1/angbracketleftBig
ρ∗
kΩ†eΩ†tRk/angbracketrightBig
, (G4)
which both evolve with standard Brownian dynamics.
In order to solve the integral equation ( G2), we ﬁrst com-
puteKΩ†(k,t)andW(k,t)atk= 7.0from the simulation
trajectories. This we do by evaluating their deﬁnitions ( G3)
and ( G4), averaging over 50 independently initialised simula-tion trajectories, a small number of time origins, and all al -
lowed wave vectors in the range k∈(7.0±0.1). We refer to
averaging over both independent simulation trajectories a nd
time origins as an ensemble average. For Fig. 4(a)and(c),
we omit the ensemble and time-origin average, in order to in-
troduce more noise.
Either the single trajectory or ensemble-averaged KΩ†(k,t)
andW(k,t)are inserted in a discretised version of integral
equation ( G2). The memory kernel is subsequently found by
solving the resulting system of equations [75].
[1]C. L. E. Franzke, T. J. O’Kane, J. Berner, P. D. Williams, and
V . Lucarini, Wiley Interdisciplinary Reviews: Climate Change
6(2015), 10.1002/wcc.318 ,arXiv:1409.0423 .
[2]E. Herrera-Delgado, R. Perez-Carrasco, J. Briscoe, and P. S ol-
lich, PLoS Computational Biology 14(2018), 10.1371/jour-
nal.pcbi.1006003 ,arXiv:1708.09312 .
[3]A. Kelly, A. Montoya-Castillo, L. Wang, and T. E. Mark-
land, Journal of Chemical Physics 144, 184105 (2016) ,
arXiv:1603.01905 .
[4]W. C. Pfalzgraff, A. Montoya-Castillo, A. Kelly, and T. E.
Markland, Journal of Chemical Physics 150, 244109 (2019) ,
arXiv:1903.09608 .
[5]L. M. C. Janssen, Frontiers in Physics 6(2018),
10.3389/fphy.2018.00097 ,arXiv:1806.01369 .
[6]P. G. Debenedetti and F. H. Stillinger, Nature 410(2001) .
[7]D. R. Reichman and P. Charbonneau, Journal of Statistical Me-
chanics: Theory and Experiment , P05013 (2005) .
[8]R. Zwanzig, Physical Review 124 (1961), 10.1103/Phys-
Rev.124.983 .
[9]H. Mori, Progress of Theoretical Physics 33(1965).
[10] S. Cao, A. Montoya-Castillo, W. Wang, T. E. Markland, and
X. Huang, Journal of Chemical Physics 153, 014105 (2020) .
[11] U. Bengtzelius, W. G¨ otze, and A. Sj¨ olander, Jour-
nal of Physics C: Solid State Physics 17 (1984),
10.1080/01411599408200364 .
[12] E. Leutheusser, Physical Review A 29(1984), 10.1103/Phys-
RevA.29.2765 .
[13] S. P. Das, Reviews of Modern Physics 76 (2004),
10.1103/RevModPhys.76.785 .
[14] W. G¨ otze, Complex Dynamics of Glass-Forming Liquids: A
Mode-Coupling Theory (Oxford University Press, 2008).
[15] G. Szamel, Physical Review Letters 90, 228301 (2003) ,
arXiv:0305626 .
[16] S. P. Das and G. F. Mazenko, Physical Review A 34, 2265
(1986).
[17] W. G¨ otze and L. Sj¨ ogren, Zeitschrift f¨ ur Physik B Condensed
Matter 65, 415 (1987) .
[18] C. Luo and L. M. C. Janssen, Journal of Chemical Physics 153
(2020), 10.1063/5.0026969 ,arXiv:1909.00428 .
[19] L. M. C. Janssen and D. R. Reichman, Physical Review Letters
115, 205701 (2015) ,arXiv:1507.01947 .
[20] J. G. McWhirter and E. R. Pike, Journal of Physics A: Mathe-
matical and General 11(1978), 10.1088/0305-4470/11/9/007 .
[21] I. J. D. Craig and A. M. Thompson, Computers in Physics 8
(1994), 10.1063/1.4823347 .
[22] R. Satija and D. E. Makarov, Journal of Physical Chemistry B
123, 802 (2019) .
[23] C. L. Epstein and J. Schotland, SIAM Review 50(2008),
10.1137/060657273 .[24] M. Baity-Jesi and D. R. Reichman, Journal of Chemical Physics
151, 084503 (2019) ,arXiv:1906.05818 .
[25] H. Vroylandt and P. Monmarch´ e, Journal of Chemical Physics
156, 244105 (2022) ,arXiv:2201.02457 .
[26] Y . Han, J. Jin, and G. A. V oth, Journal of Chemical Physics
154, 084122 (2021) .
[27] A. Obliger, The Journal of Chemical Physics 158, 144101
(2023) .
[28] G. Jung, M. Hanke, and F. Schmid, Journal of Chemical Theory
and Computation 13, 2481 (2017) ,arXiv:1709.07805 .
[29] B. Kowalik, J. O. Daldrop, J. Kappler, J. C. F. Schulz,
A. Schlaich, and R. R. Netz, Physical Review E 100, 012126
(2019) .
[30] M. Berkowitz, J. D. Morgan, D. J. Kouri, and J. A. McCam-
mon, The Journal of Chemical Physics 75, 2462 (1981) .
[31] Q. Shi and E. Geva, Journal of Chemical Physics 120, 10647
(2004) .
[32] R. Narayan and R. Nityananda, Annual Review of Astronomy
and Astrophysics 24(1986), 10.1146/annurev.astro.24.1.127 .
[33] A. J. Dominic, T. Sayer, S. Cao, T. E. Markland, X. Huang, and
A. Montoya-Castillo, Proceedings of the National Academy of
Sciences 120, e2221048120 (2023) .
[34] S. S. Schoenholz, E. D. Cubuk, D. M. Sussman, E. Kaxiras,
and A. J. Liu, Nature Physics 12(2016), 10.1038/nphys3644 ,
arXiv:1506.07772 .
[35] I. Tah, S. A. Ridout, and A. J. Liu, The Journal of Chemical
Physics 157, 124501 (2022) ,arXiv:2205.07187 .
[36] R. M. Alkemade, E. Boattini, L. Filion, and F. Smallenburg,
Journal of Chemical Physics 156, 204503 (2022) .
[37] D. Coslovich, R. L. Jack, and J. Paret, Journal of Chemical
Physics 157, 204503 (2022) ,arXiv:2211.01904 .
[38] J. Paret, R. L. Jack, and D. Coslovich, Journal of Chemical
Physics 152, 144502 (2020) ,arXiv:2002.02726 .
[39] V . Bapst, T. Keck, A. Grabska-Barwi´ nska, C. Donner, E. D.
Cubuk, S. S. Schoenholz, A. Obika, A. W. Nelson, T. Back,
D. Hassabis, and P. Kohli, Nature Physics 16, 448 (2020) .
[40] H. Shiba, M. Hanai, T. Suzumura, and T. Shimokawabe, Jour-
nal of Chemical Physics 158(2023), 10.1063/5.0129791 .
[41] E. Boattini, S. Mar´ ın-Aguilar, S. Mitra, G. Fofﬁ, F. Smalle n-
burg, and L. Filion, Nature Communications 11, 5479 (2020) ,
arXiv:2003.00586 .
[42] F. S. Pezzicoli, G. Charpiat, and F. P. Landes, arXiv (2022) ,
arXiv:2211.03226 .
[43] G. Jung, G. Biroli, and L. Berthier, arXiv (2022) ,
arXiv:2210.16623 .
[44] G. Campos-Villalobos, E. Boattini, L. Filion, and M. Dijkst ra,
Journal of Chemical Physics 155, 174902 (2021) .
[45] Z. She, P. Ge, and H. Lei, The Journal of Chemical Physics
158, 034102 (2023) ,arXiv:2210.05814 .12
[46] S. L. Smith, E. Elsen, and S. De, Proceedings of the
37th International Conference on Machine Learning (2020) ,
arXiv:2006.15081 .
[47] D. S. Park, J. Sohl-Dickstein, Q. V . Le, and S. L. Smith, 36th
International Conference on Machine Learning, ICML 2019 ,
8857 (2019), arXiv:1905.03776 .
[48] K. Hornik, M. Stinchcombe, and H. White, Neural Networks 2
(1989), 10.1016/0893-6080(89)90020-8 .
[49] H. Lei, N. A. Baker, and X. Li, Proceedings of the National
Academy of Sciences of the United States of America 113,
14183 (2016) ,arXiv:1606.02596 .
[50] F. Grogan, H. Lei, X. Li, and N. A. Baker, Journal of Compu-
tational Physics 418, 109633 (2020) ,arXiv:1912.00265 .
[51] C. Ayaz, L. Tepper, F. N. Br¨ unig, J. Kappler, J. O. Daldrop, a nd
R. R. Netz, Proceedings of the National Academy of Sciences
of the United States of America 118, 20 (2021) .
[52] R. Fournier, L. Wang, O. V . Yazyev, and Q. S. Wu, Physical
Review Letters 124, 056401 (2020) ,arXiv:1810.00913 .
[53] L. Lu, P. Jin, G. Pang, Z. Zhang, and G. E. Karniadakis, Nature
Machine Intelligence 3, 218 (2021) .
[54] A. Gouasmi, E. J. Parish, and K. Duraisamy, Proceedings of
the Royal Society A: Mathematical, Physical and Engineerin g
Sciences 473, 20170385 (2017) .
[55] S. E. Ahmed, S. M. Rahman, O. San, A. Rasheed, and
I. M. Navon, Physics of Fluids 31(2019), 10.1063/1.5128374 ,
arXiv:1910.07649 .
[56] Z. Li, X. Bian, X. Li, and G. E. Karniadakis, Journal of Chem-
ical Physics 143(2015), 10.1063/1.4935490 .
[57] V . E. Debets, C. Luo, S. Ciarella, and L. M. C. Janssen, Physi-
cal Review E 104, 065302 (2021) ,arXiv:2108.06829 .
[58] J. K. Percus and G. J. Yevick, Physical Review 110, 1 (1958) .
[59] M. S. Wertheim, Physical Review Letters 10, 321 (1963) .[60] J. D. Weeks, D. Chandler, and H. C. Andersen, The Journal of
Chemical Physics 54, 5237 (1971) .
[61] I. T. Jolliffe, Principal Component Analysis , 2nd ed. (Springer
New York, 2002).
[62] A. Lenail, “alexlenail.me/NN-SVG/,” (2023).
[63] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and
R. Salakhutdinov, Journal of Machine Learning Research 15
(2014).
[64] X. Ying, Journal of Physics: Conference Series 1168 (2019).
[65] D. P. Kingma and J. L. Ba, 3rd International Conference on
Learning Representations (2015), arXiv:1412.6980 .
[66] F. R. de Hoog, J. H. Knight, and A. N. Stokes, SIAM Journal
on Scientiﬁc and Statistical Computing 3, 357 (1982) .
[67] K. L. Kuhlman, Numerical Algorithms 63, 339 (2013) ,
arXiv:1204.4754 .
[68] A. Talbot, IMA Journal of Applied Mathematics 23, 97 (1979).
[69] H. Stehfest, Communications of the ACM 13, 47 (1970) .
[70] A. Savitzky and M. J. E. Golay, Analytical Chemistry 36, 1627
(1964) .
[71] M. D. Ediger, C. A. Angell, and S. R. Nagel, Journal of Physi-
cal Chemistry 100, 13200 (1996) .
[72] A. P. Thompson, H. M. Aktulga, R. Berger, D. S. Bolintineanu,
W. M. Brown, P. S. Crozier, P. J. in ’t Veld, A. Kohlmeyer, S. G.
Moore, T. D. Nguyen, R. Shan, M. J. Stevens, J. Tranchida,
C. Trott, and S. J. Plimpton, Computer Physics Communica-
tions 271, 108171 (2022) .
[73] G. N¨ agele, Physics Report 272 (1996),
10.1016/0370-1573(95)00078-X .
[74] G. N¨ agele, J. Bergenholtz, and J. K. Dhont, Journal of Chemi-
cal Physics 110, 7037 (1999) .
[75] I. Pihlajamaa, V . E. Debets, C. C. L. Laudicina, and L. M. C.
Janssen, Soon to be published .