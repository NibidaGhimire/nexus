Rethinking the Video Sampling and Reasoning Strategies for
Temporal Sentence Grounding
Jiahao Zhu1, Daizong Liu2y, Pan Zhou1y, Xing Di3, Yu Cheng4, Song Yang5,
Wenzheng Xu6, Zichuan Xu7, Yao Wan8, Lichao Sun9, Zeyu Xiong1
1Huazhong University of Science and Technology3ProtagoLabs Inc4Microsoft Research
2Peking University5Beijing Institute of Technology6School of Sichuan University
7Dalian University of Technology9Lehigh University
8School of Computer Sci. & Tech., Huazhong University of Science and Technology
{jiahaozhu, panzhou, wanyao, zeyuxiong}@hust.edu.cn, dzliu@stu.pku.edu.cn
xing.di@protagolabs.com, yu.cheng@microsoft.com, S.Yang@bit.edu.cn,
wenzheng.xu@scu.edu.cn, z.xu@dlut.edu.cn, lis221@lehigh.edu
Abstract
Temporal sentence grounding (TSG) aims to
identify the temporal boundary of a speciﬁc
segment from an untrimmed video by a sen-
tence query. All existing works ﬁrst uti-
lize a sparse sampling strategy to extract a
ﬁxed number of video frames and then con-
duct multi-modal interactions with query sen-
tence for reasoning. However, we argue that
these methods have overlooked two indispens-
able issues: 1) Boundary-bias: The annotated
target segment generally refers to two spe-
ciﬁc frames as corresponding start and end
timestamps. The video downsampling process
may lose these two frames and take the adja-
cent irrelevant frames as new boundaries. 2)
Reasoning-bias: Such incorrect new bound-
ary frames also lead to the reasoning bias
during frame-query interaction, reducing the
generalization ability of model. To alleviate
above limitations, in this paper, we propose a
novel Siamese Sampling and Reasoning Net-
work (SSRN) for TSG, which introduces a
siamese sampling mechanism to generate ad-
ditional contextual frames to enrich and re-
ﬁne the new boundaries. Speciﬁcally, a rea-
soning strategy is developed to learn the inter-
relationship among these frames and generate
soft labels on boundaries for more accurate
frame-query reasoning. Such mechanism is
also able to supplement the absent consecutive
visual semantics to the sampled sparse frames
for ﬁne-grained activity understanding. Exten-
sive experiments demonstrate the effectiveness
of SSRN on three challenging datasets.
1 Introduction
Temporal sentence grounding (TSG) is an impor-
tant yet challenging task in natural language pro-
Equal contributions.yCorresponding author.
(a) An example of the temporal sentence grounding  (TSG) .
Sentence Query:  The woman then adds ginger ale, and shakes the drink in a tumbler .
Ground Truth | | 63.74s 80.57s
(a) An example of the temporal sentence grounding  (TSG) .
Sentence Query:  The woman then adds ginger ale, and shakes the drink in a tumbler .
Ground Truth | | 63.74s 80.57s
VideoOriginal segment
... ...
(b) An illustration of the video feature sampling of existing TSG methods .VideoOriginal segment
... ...
(b) An illustration of the video feature sampling of existing TSG methods .
(c) An overview of our siamese sampling strategy . (c) An overview of our siamese sampling strategy .New segmentNew segment
refinerefineSiamese SamplingSiamese SamplingOriginal segmentOriginal segment
Contextual FramesContextual FramesenrichenrichContextual FramesContextual FramesenrichenrichSparse  Sampling Sparse  Sampling
......VideoVideo
......
...... ......New segmentNew segment
Refined  segment Refined  segmentFigure 1: (a) An example of temporal sentence ground-
ing task. (b) All existing TSG methods generally utilize
a downsampling process to evenly extract a ﬁxed num-
ber of frames from a long video. However, the new tar-
get segment is obtained by rounding operation and may
introduces boundary bias since some original bound-
ary frames are lost. (c) We propose a siamese sam-
pling strategy to extract additional adjacent frames to
enrich and reﬁne the information of the sampled frames
for generating more accurate boundary of the new seg-
ment.
cessing, which has drawn increasing attention over
the last few years due to its vast potential applica-
tions in information retrieval (Dong et al., 2019;
Yang et al., 2020) and human-computer interaction
(Singha et al., 2018). It aims to ground the most rel-
evant video segment according to a given sentence
query. As shown in Figure 1 (a), video and query
information need to be deeply incorporated to dis-
tinguish the ﬁne-grained details of adjacent frames
for determining accurate boundary timestamps.
Previous TSG methods (Gao et al., 2017; Chen
et al., 2018; Zhang et al., 2019b; Yuan et al., 2019a;arXiv:2301.00514v1  [cs.CV]  2 Jan 2023Zhang et al., 2020b; Liu et al., 2018a; Zhang et al.,
2019a; Liu et al., 2018b, 2021a) generally follow
an encoding-then-interaction framework that ﬁrst
extracts both video and query features and then con-
duct multi-modal interactions for reasoning. Since
many videos are overlong while corresponding tar-
get segments are short, these methods simply uti-
lize a sparse sampling strategy shown in Figure1
(b), which samples a ﬁxed number of frames from
each video to reconstruct a shorter video, and then
learn frame-query relations for segment inferring.
We argue that existing learning paradigm suffers
from two obvious limitations: 1) Boundary-bias:
Each video has a query-related segment, which
refers to two speciﬁc frames as its start and end
timestamps. Traditional sparse downsampling strat-
egy extracts frames from videos with a ﬁxed in-
terval. A rounding operation is then applied to
map the annotated segment to the sampled frames
by keeping the same proportional length in both
original and new videos. As a result, the ground-
truth boundary frames may be ﬁltered out and the
query-irrelevant frames will be regarded as the ac-
tual boundaries, generating wrong labels for latter
training. 2) Reasoning-bias: The query-irrelevant
boundary frames in the newly reconstructed seg-
ment will also lead to incorrect frame-query interac-
tion and reasoning in the training process, reducing
the generalization ability of model.
To alleviate these two issues, a straightforward
idea is to ﬁlter out the sampled boundary frames in
the new segment if they are query-irrelevant. How-
ever, this will destroy the true segment length when
we transfer the downsampled segment back to the
original one during the inference process. Another
straightforward idea is to directly keep the appropri-
ate segment length (by ﬂoat values) in the newly re-
constructed video and then reason the query content
in the new boundary to determine what percentage
of this boundary is correct. However, the query-
irrelevant boundaries lack sufﬁcient query-related
information for boundary reasoning. Based on the
above considerations, we aim to extract additional
frames adjacent to the sampled frames to enrich
and reﬁne their information for supplementing the
consecutive visual semantics. In this way, the new
boundary frames are well semantic-correlated to
its original adjacent boundaries. Based on the re-
ﬁned boundary frames, we can keep and learn the
appropriate segment length of the downsampled
video for query reasoning. Moreover, other innerframes are also enriched by their neighbors, captur-
ing more consecutive visual appearances for fully
understanding the entire activity.
Therefore, in this paper, we propose a novel
Siamese Sampling and Reasoning Network (SSRN)
for temporal sentence grounding task to generate
additional contextual frames to enrich and reﬁne
the new boundaries. Speciﬁcally, we treat the
sparse sampled video frames as anchor frames, and
additionally extract several frames adjacent to each
anchor frame as the siamese frames for semantic
sharing and enriching. A siamese knowledge ag-
gregation module is designed to explore internal
relationships and aggregate contextual information
among these frames. Then, a siamese reasoning
module supplements the ﬁne-grained contexts of
siamese frames into the anchor frames for enrich-
ing their semantics. In this way, the query-related
information are added into the new boundaries thus
we can utilize an appropriate ﬂoat value to rep-
resent the new segment length for query reason-
ing, addressing both boundary- and reasoning-bias.
Moreover, other sampled frames are also equipped
with more consecutive visual semantics from their
original neighbors, which further beneﬁts more
ﬁne-grained learning process.
Our contributions are summarized as follows:
•We propose a novel SSRN model which can
sparsely extract multiple relevant frames from
original videos to enrich the anchor frames
for more accurate boundary prediction. To
the best of our knowledge, we are the ﬁrst to
propose and address both boundary-bias and
reasoning-bias in TSG task.
•We propose an effective siamese aggregation
and reasoning method to correlate and inte-
grate the contextual information of siamese
frames to reﬁne the anchor frames.
•Extensive experiments are conducted on three
challenging public benchmarks, including Ac-
tivityNet Captions, TACoS and Charades-
STA, demonstrating the effectiveness of our
proposed SSRN method.
2 Related Work
Temporal sentence grounding (TSG) is a new task
introduced recently (Gao et al., 2017; Anne Hen-
dricks et al., 2017), which aims to localize the
most relevant video segment from a video withsentence descriptions. All existing methods fol-
low an encoding-then-interaction framework that
ﬁrst extracts video/query features and then conduct
multi-modal interactions for segment inferring.
Based on the interacted multi-modal features,
traditional methods follow a propose-and-rank
paradigm to make predictions. Most of them (Ge
et al., 2019; Qu et al., 2020; Xiao et al., 2021; Liu
et al., 2021a,c, 2020a; Liu and Hu, 2022a,b; Liu
et al., 2022c; Fang et al., 2022; Liu et al., 2022f)
typically utilize a proposal-based grounding head
that ﬁrst generates multiple candidate segments as
proposals, and then ranks them according to their
similarity with the query semantic to select the best
matching one. Some of them (Gao et al., 2017;
Anne Hendricks et al., 2017) directly utilize multi-
scale sliding windows to produce the proposals and
subsequently integrate the query with segment rep-
resentations via a matrix operation. To improve the
quality of the proposals, latest works (Wang et al.,
2020; Yuan et al., 2019a; Zhang et al., 2019b; Cao
et al., 2021; Liu et al., 2021b, 2020b, 2022d,e,a) in-
tegrate sentence information with each ﬁne-grained
video clip unit, and predict the scores of candidate
segments by gradually merging the fusion feature
sequence over time.
Recently, some proposal-free works (Yuan et al.,
2019b; Wang et al., 2019; Rodriguez et al., 2020;
Chen et al., 2020; Mun et al., 2020; Zeng et al.,
2020; Zhang et al., 2020a, 2021; Nan et al., 2021)
directly predict the temporal locations of the tar-
get segment without generating complex proposals.
These works directly select the starting and end-
ing frames by leveraging cross-modal interactions
between video and query. Speciﬁcally, they either
regress the start/end timestamps based on the en-
tire video representation (Yuan et al., 2019b; Mun
et al., 2020), or predict at each frame to determine
whether this frame is a start or end boundary (Ro-
driguez et al., 2020; Chen et al., 2020; Zeng et al.,
2020; Zhang et al., 2020a, 2021).
Although the above two types of methods have
achieved great performances, their video sampling
strategy in encoding part is unreasonable that can
lead to both boundary and reasoning bias. Speciﬁ-
cally, the boundary bias is deﬁned as the incorrect
boundary of the new segment reconstructed by the
video sparse sampling. The reasoning bias is de-
ﬁned as the incorrect correlation learning between
the query-irrelevant frames and query. In this pa-
per, we aim to reduce the above bias by proposinga new siamese sampling and reasoning strategy to
enrich the sampled frames and further reﬁne the
reconstructed segment boundary.
3 The Proposed Method
Given an untrimmed video and a sentence query,
we represent the video as Vwith a frame number
ofT. Similarly, the query with Nwords is denoted
asQ. Temporal sentence grounding (TSG) aims
to localize a segment (s;e)starting at timestamp
sand ending at timestamp ein videoV, which
corresponds to the same semantic as query Q.
The overall architecture of the proposed Siamese
Sampling and Reasoning Network (SSRN) method
is illustrated in Figure 2. The SSRN framework
contains four main components: (1) Siamese sam-
pling and encoding: We sparsely downsample
each long video into the anchor frames, and a
new siamese sampling strategy additionally sam-
ples their adjacent frames as siamese frames. A
video/query encoder then extracts visual/query fea-
tures from all sampled video frames and query sen-
tence respectively. (2) Multi-modal interaction: Af-
ter that, we interact the query features with the vi-
sual features for cross-modal interaction. (3) Multi-
modal reasoning: Next, to supplement the knowl-
edge of siamese frames into the anchor frames, a
siamese knowledge aggregation module is devel-
oped to determine how much the information of
siamese frames are needed to inject into the an-
chor ones. Then, a reasoning module is utilized
to enrich the anchor frames with the aggregated
semantic knowledge. In this way, the contexts of
both new boundaries and other sparse frames are
enriched and can better represent the full and con-
secutive visual semantics. (4) Grounding heads
with soft labels: At last, we employ the ground-
ing heads with soft label to predict more accurate
boundaries via ﬂoat value to keep the appropriate
segment length. We illustrate the details of each
component in the following subsections.
3.1 Siamese Sampling and Encoding
Given the dense video input V, previous works gen-
erally downsample each video into a new video
of ﬁxed length to address the problem of overlong
video. Considering the existing boundary-bias, we
propose a siamese sampling strategy to additionally
extract contextual adjacent frames nearby each sam-
pled frame to enrich its query-related information
for better determining the accurate new boundary.Xi Video InputMultimodal 
InteractionRounded Boundary
PredictorFloat Boundary
Predictor
Siamese
Knowledge
AggregationSiamese
Knowledge
ReasoningVideo Frames
Anchor frames
Query Input
Query EncoderVideo EncoderMulti -Modal 
Reasoning 
𝑃𝑃𝑠𝑠,𝑃𝑃𝑒𝑒
𝑂𝑂𝑠𝑠,𝑂𝑂𝑒𝑒
Timeline𝒱𝒱𝑎𝑎
{𝒱𝒱𝑠𝑠,𝑘𝑘}𝑘𝑘=1𝐾𝐾−1
𝑭𝑭𝑎𝑎
{𝑭𝑭𝑠𝑠,𝑘𝑘}𝑘𝑘=1𝐾𝐾−1Q
“The woman then adds ginger ale, 
and shakes the drink in a tumbler.”
Samping
...
...Siamese frames...
...
...
length of  Tlength of  M length of  M
𝒱𝒱
𝑄𝑄Siamese
Knowledge Enriched anchor feature �𝑭𝑭𝑎𝑎
̂𝜏𝜏𝑠𝑠′̂𝜏𝜏𝑒𝑒′
𝑂𝑂𝑠𝑠(̂𝜏𝜏𝑠𝑠′)𝑂𝑂𝑒𝑒(̂𝜏𝜏𝑒𝑒′)Soft
Label
length of  K𝑽𝑽𝑎𝑎
{𝑽𝑽𝑠𝑠,𝑘𝑘}𝑘𝑘=1𝐾𝐾−1
𝑪𝑪Figure 2: Overview of our Siamese Sampling and Reasoning Network. Given a dense video, the anchor frames
and siamese frames are ﬁrst extracted by sparse sampling and siamese sampling, respectively. Then a video/query
encoder and a multimodal interaction module are utilized to generate multimodal features. Next, a siamese knowl-
edge generation module is proposed to model contextual relationship between anchor frames and siamese ones
from the same video. After that, the siamese knowledge reasoning module exploits the siamese knowledge to en-
rich the information of the anchor frames for more accurate boundary prediction. At last, in the grounding heads,
we utilize a soft label to learn more ﬁne-grained boundaries of ﬂoat value in addition to the rounded one.
Here, we call the downsampled frames and their
contextual frames as anchor frames and siamese
frames, respectively. Speciﬁcally, as shown in
Figure 1 (c), following previous works, we di-
rectly construct the anchor video Vaby sparsely
and evenly sampling Mframes from dense video
frames of length T(Tis usually much greater
thanM). The new siamese videos are then cap-
tured at different beginning indices in the original
video but next to the frames of the anchor video.
The same sample interval is utilized for all frames.
After siamese sampling, we can obtain multiple
siamese videos with same length and similar global
semantics as the anchor video. We denote the new
siamese videos as fVs;kgK
k=1whereKmeans the
siamese sample number.
Since we utilize the sampling strategy to pro-
cess the dense video frames, the start/end time
of the target segment in original video sequence
needs to be accurately mapped to the correspond-
ing boundaries in the new video sequence of M
frames. Following almost all previous TSG meth-
ods (Zhang et al., 2019b, 2020a; Liu et al., 2021a),
the new start/end index is generally calculated by
^s(e)=bs(e)=TMc, wherebcdenotes the
rounding operator. During the inference, the pre-
dicted segment boundary index can be easily con-
verted to the corresponding time in the dense videovias(e)= ^s(e)=MT. However, the rounding
operation may produce boundary bias that the new
boundary frames are not semantically correlated to
the query semantic. Therefore, we further generate
a soft label ~s(e)=hs(e)=TMias an additional
supervision to keep the appropriate segment length
during training, where hidenotes the ﬂoat result.
Video encoder For video encoding, we ﬁrst ex-
tract frame features by a pre-trained C3D network
(Tran et al., 2015), and then add a positional en-
coding (Vaswani et al., 2017) to provide positional
knowledge. Such position encoding plays a crucial
role in distinguishing semantics at diverse temporal
locations. Considering the sequential characteristic
in videos, a Bi-GRU (Chung et al., 2014) is further
applied to incorporate the contextual information
along time series. We denote the extracted video
features of both anchor video and siamese video as
Va;fVs;kgK
k=12RMD, respectively.
Query encoder For query encoding, we ﬁrst ex-
tract word embeddings by the Glove model (Pen-
nington et al., 2014). We also apply positional
encoding and Bi-GRU to integrate the sequential
information within the sentence. The ﬁnal feature
of the query is denoted as Q2RND.3.2 Multi-Modal Interaction
After obtaining the video features Va;fVs;kgK
k=1
and query feature Q, we utilize a co-attention mech-
anism (Lu et al., 2019) to capture the cross-modal
interactions between them. Speciﬁcally, for each
video feature V2fVag[fVs;kgK
k=1, we ﬁrst
calculate the similarity between VandQas:
S=V(QWS)>2RMN; (1)
whereWS2RDDprojects the query features
into the same latent space as the video. Then, we
compute two attention weights as:
A=Sr(QWS)2RMD;
B=SrST
cV2RMD;(2)
whereSrandScare the row- and column-wise
softmax results of S, respectively. We compose the
ﬁnal query-guided video representation by learning
its sequential features as follows:
F=Bi-GRU ([V;A;VA;VB])2RMD;
(3)
where Bi-GRU ()denotes the Bi-GRU layers, [; ]
is the concatenate operation, and is the element-
wise multiplication. The output F2 fFag[
fFs;kgK
k=1encodes visual features with query-
guided attention.
3.3 Multi-Modal Reasoning Strategy
Note that the query-irreverent new boundary
frames encoded in the anchor video feature Fahas
insufﬁcient query-guided visual information for lat-
ter boundary prediction. To address this issue, we
propose a new multi-modal reasoning strategy to
enrich the query-related knowledge in anchor fea-
turesFareferring to the contextual information in
siamese featuresfFs;kgK
k=1. In detail, the multi-
modal reasoning strategy consists of two compo-
nents: a siamese knowledge aggregation module
and a siamese knowledge reasoning module.
Siamese knowledge aggregation Intuitively, fea-
tures with close visual-query correlation are ex-
pected to generate more consistent predictions of
segment probabilities. To this end, we utilize a
siamese knowledge aggregation module to generate
interdependent knowledge from siamese features
to anchor ones to enrich the contexts of anchor
features and reﬁne the prediction.
We propose to propagate and integrate knowl-
edge between the query-guided visual features FaandfFs;kgK
k=1. Speciﬁcally, we ﬁrst obtain their
semantic similarities by calculating their pairwise
cosine similarity scores as:
C(i;k) =(Fa
i)(Fs;k
i)>
kFa
ik2kFs;k
ik2; (4)
whereC2RMKis interdependent similarity
matrix,k  k 2isl2-norm,i2 f1;2;:::;Mgis
the indices of features and k2f1;2;:::;Kgis the
indices of siamese videos. Here, each anchor frame
is needed to be enriched by only its siamese frames.
We employ a softmax function to each row of the
similarity matrix Cas:
C(i;k) =exp(C(i;k))Pexp(C(i;k)); (5)
where the new Cindicates the contextual afﬁnities
between each anchor feature and its corresponding
siamese features.
Siamese knowledge reasoning After that, we
propose to adaptively propagate and merge the
siamese knowledge into the anchor features for
enriching the query-aware information. This is es-
pecially helpful when we determine more accurate
boundaries for the downsampled video. Speciﬁ-
cally, The integration process can be formulated as:
eFa=KX
k=1C(:;k)(Fs;kW1)2RMD;(6)
whereeFais the propagated semantic vector in an-
chor video. In order to avoid over propagation and
involves in irrelevant noisy information, we further
exploit a residual design with a learnable weight to
enrich the anchor video as:
eFa=KX
k=1C(:;k)(Fs;kW1) + (1 )FaW2;
(7)
whereW1;W22RDDare projection matrices,
weighting factor 2[0;1]is a hyper-parameter.
With the above formulations, the knowledge of
the siamese samples within the same video can be
propagated and integrated to the anchor one.
3.4 Grounding Heads with Soft Label
For the ﬁnal segment boundary prediction, we ﬁrst
follow the span predictor in (Zhang et al., 2020a) to
utilize two stacked-LSTM with two corresponding
feed-forward layers to predict the start/end scoresof each frame. In details, we send the contextual
multi-modal feature eFa2RMDinto this span
predictor and apply the softmax function on its
two outputs to produce the probability distributions
Ps;Pe2RMof start and end boundaries. We
utilize the rounded boundary ^s(e)to generate the
coarse label vectors Ys(e)to supervise Ps;Peas:
L1=fCE(Ps;Ys) +fCE(Pe;Ye); (8)
wherefCErepresents cross-entropy loss function.
The predicted timestamps ( ^s0;^e0)are obtained
from the maximum scores of start and end predic-
tionsPs(e)of frames as:
( ^s0;^e0) =argmax
^s0;^e0Ps( ^s0)Pe( ^e0);(9)
where 0^0
s^0
eM.
Since the above predictions are coarse on the seg-
ment boundaries with boundary-bias, we further
utilize a parallel prediction head on eFato predict
more ﬁne-grained ﬂoat boundaries on the down-
sampled boundary frames. Speciﬁcally, we utilize
the ﬂoat boundary ~s(e)to generate the soft labels
Y0
s(e), andeFais fed into a single feed-forward layer
to predict the ﬂoat boundaries Os(e)supervised by
our designed soft labels Y0
s(e)as follows:
L2=R1(Os(e) Y0
s(e)); (10)
whereR1is the smooth L1 loss. The ﬁnal predicted
segment is calculated by:
(~0
s;~0
e) = (^0
s+1 Os(^0
s);^0
e 1+Os(^0
e)):(11)
4 Experiments
4.1 Datasets and Evaluation
ActivityNet Captions This dataset (Krishna et al.,
2017) contains 20000 untrimmed videos from
YouTube with 100000 textual descriptions. The
videos are 2 minutes on average, and the annotated
video clips have signiﬁcant variation of length,
ranging from several seconds to over 3 minutes.
Following public split, we use 37417, 17505, and
17031 sentence-video pairs for training, validation,
and testing.
TACoS TACoS (Regneri et al., 2013) contains
127 videos. The videos from TACoS are collected
from cooking scenarios, thus lacking the diversity.
They are around 7 minutes on average. We use
the same split as (Gao et al., 2017), which includesMethod FeatureR@1, R@1, R@5, R@5
IoU=0.5 IoU=0.7 IoU=0.5 IoU=0.7
TGN C3D 28.47 - 43.33 -
CTRL C3D 29.01 10.34 59.17 37.54
QSPN C3D 33.26 13.43 62.39 40.78
CBP C3D 35.76 17.80 65.89 46.20
GDP C3D 39.27 - - -
VSLNet C3D 43.22 26.16 - -
CMIN C3D 43.40 23.88 67.95 50.73
DRN C3D 45.45 24.36 77.97 50.30
2DTAN C3D 44.51 26.54 77.13 61.96
APGN C3D 48.92 28.64 78.87 63.19
MGSL C3D 51.87 31.42 82.60 66.71
SSRN C3D 54.49 33.15 84.72 68.48
Table 1: Performance compared with the state-of-the-
art TSG models on ActivityNet Captions dataset.
Method FeatureR@1, R@1, R@5, R@5,
IoU=0.3 IoU=0.5 IoU=0.3 IoU=0.5
TGN C3D 21.77 18.90 39.06 31.02
CTRL C3D 18.32 13.30 36.69 25.42
QSPN C3D 20.15 15.23 36.72 25.30
CBP C3D 27.31 24.79 43.64 37.40
GDP C3D 24.14 - - -
VSLNet C3D 29.61 24.27 - -
CMIN C3D 24.64 18.05 38.46 27.02
DRN C3D - 23.17 - 33.36
2DTAN C3D 37.29 25.32 57.81 45.04
APGN C3D 40.47 27.86 59.98 47.12
MGSL C3D 42.54 32.27 63.39 50.13
SSRN C3D 45.10 34.33 65.26 51.85
Table 2: Performance compared with the state-of-the-
art TSG models on TACoS datasets.
Method FeatureR@1, R@1, R@5, R@5,
IoU=0.5 IoU=0.7 IoU=0.5 IoU=0.7
2DTAN VGG 39.81 23.25 79.33 51.15
APGN VGG 44.23 25.64 89.51 57.87
SSRN VGG 46.72 27.98 91.37 59.64
CTRL C3D 23.63 8.89 58.92 29.57
QSPN C3D 35.60 15.80 79.40 45.40
CBP C3D 36.80 18.87 70.94 50.19
GDP C3D 39.47 18.49 - -
APGN C3D 48.20 29.37 89.05 58.49
SSRN C3D 50.39 31.42 90.68 59.94
DRN I3D 53.09 31.75 89.06 60.05
APGN I3D 62.58 38.86 91.24 62.11
MGSL I3D 63.98 41.03 93.21 63.85
SSRN I3D 65.59 42.65 94.76 65.48
Table 3: Performance compared with the state-of-the-
art TSG models on Charades-STA datasets.
10146, 4589, 4083 query-segment pairs for training,
validation and testing.
Charades-STA Charades-STA is built on the Cha-
rades dataset (Sigurdsson et al., 2016), which fo-
cuses on indoor activities. The video length of
Charades-STA dataset is 30 seconds on average,CTRL TGN 2DTAN CMIN DRN APGN SSRN
VPS"0.45 1.09 1.75 81.29 133.38 146.67 158.12
Para.#22 166 363 78 214 91 184
Table 4: Efﬁciency comparison in terms of video per
second (VPS) and parameters (Para.).
and there are 12408 and 3720 moment-query pairs
in the training and testing sets, respectively.
Evaluation Following previous works (Gao et al.,
2017; Liu et al., 2021a), we adopt “R@n, IoU=m”
as our evaluation metrics. The “R@n, IoU=m” is
deﬁned as the percentage of at least one of top-n
selected moments having IoU larger than m, which
is the higher the better.
4.2 Implementation Details
For video encoding, we apply C3D (Tran et al.,
2015) to encode the videos on all three datasets,
and also extract the I3D (Carreira and Zisserman,
2017) and VGG (Simonyan and Zisserman, 2014)
features on Charades-STA dataset for fairly com-
paring with other methods. Following previous
works, we set the length Mof the sampled anchor
video sequences to 200 for ActivityNet Captions
and TACoS datasets, 64 for Charades-STA dataset,
respectively. As for sentence encoding, we utilize
Glove word2vec (Pennington et al., 2014) to embed
each word to a 300-dimension feature. The hidden
state dimensions of Bi-GRU and Bi-LSTM are set
to 512. The number Kof the sampled siamese
frames for each anchor frame is set to 4. We train
our model with an Adam optimizer with leaning
rate810 4,310 4,410 4for ActivityNet
Captions, TACoS, and Charades-STA datasets, re-
spectively. The batch size is set to 64.
4.3 Comparison with State-of-the-Arts
Compared methods We compare our SSRN with
state-of-the-art methods, including: (1) propose-
and-rank methods: TGN (Chen et al., 2018), CTRL
(Gao et al., 2017), QSPN (Xu et al., 2019), CBP
(Wang et al., 2020), CMIN (Zhang et al., 2019b),
2DTAN (Zhang et al., 2020b), APGN (Liu et al.,
2021a), MGSL (Liu et al., 2022b). (2) proposal-
freemethods: GDP (Chen et al., 2020), VSLNet
(Zhang et al., 2020a), DRN (Zeng et al., 2020).
Quantitative comparison As shown in Table 1, 2
and 3, our SSRN outperforms all the existing meth-
ods by a large margin. Speciﬁcally, on ActivityNet
Captions dataset, compared to the previous best
method MGSL, we outperform it by 2.62%, 1.73%,Model Anchor Siamese SKA SKR SLR@1, R@1,
IoU=0.5 IoU=0.7
¬ X    42.78 26.35
­ X   X 43.64 26.81
® X X   45.50 27.93
¯ X XX 48.97 29.36
° X X X X  51.26 31.02
± X X X X X 54.49 33.15
Table 5: Main ablation studies on ActivityNet Captions
dataset, where “Anchor" and “Siamese" denote the an-
chor and siamese frames, “SKA" and “SKR" denote
the siamese knowledge aggregation and siamese knowl-
edge reasoning, “SL" denotes the usage of soft label.
2.12%, 1.77% in all metrics, respectively. Al-
though TACoS dataset suffers from similar kitchen
background and cooking objects among the videos,
it is worth noting that our SSRN still achieves sig-
niﬁcant improvements. Compared to the previ-
ous best method MGSL, our method brings sig-
niﬁcant improvement of 2.06% and 1.72% in the
strict “R@1, IoU=0.5” and “R@5, IoU=0.5” met-
rics, respectively. On Charades-STA dataset, for
fair comparisons with other methods, we perform
experiments with same features (i.e., VGG, C3D,
and I3D) reported in their papers. It shows that our
SSRN reaches the highest results over all metrics.
Efﬁciency comparison To compare the efﬁciency
of our SSRN with previous methods, we make a
fair comparison on a single Nvidia TITAN XP GPU
on the TACoS dataset. As shown in Table 4, it can
be observed that we achieve much faster processing
speeds with a competitive model sizes.
4.4 Ablation Study
Effect of the siamese learning strategy As
shown in Table 5, we set the network without both
siamese sampling/reasoning and soft label train-
ing as the baseline (model ¬). Compared with
the baseline, the model ®additionally extracts
siamese frames for contextual learning, and can
apparently improve the accuracy. It directly utilizes
average operation to aggregate siamese knowledge
and exploit concatenation for knowledge reasoning,
which validates that multiple frames from same
videos can really bring some strong knowledge
to enhance the network. When further applying
the SKR module on model ®, the model ¯per-
forms better, demonstrating the effectiveness of our
SKR module. When we further add the SKG mod-
ule, our model °can reach a higher performance,
which can demonstrate the effectiveness of buildingAnchor Frames
sampling
Sentence Query: The person turns around to look out of a window.
Ground Truth3.4s11.7s||
Sentence Query: The person turns around to look out of a window.
Ground Truth3.4s11.7s||
GT3.4s11.7s||GT3.4s11.7s||VSLNet4.8s9.9s||Ours3.6s11.6s||
Siamese Frames
leftrightQuery-related“turns around”“look”enrichenrichAnchor Frames
sampling
Sentence Query: The person turns around to look out of a window.
Ground Truth3.4s11.7s||
GT3.4s11.7s||VSLNet4.8s9.9s||Ours3.6s11.6s||
Siamese Frames
leftrightQuery-related“turns around”“look”enrichenrichAnchor FramessamplingSentence Query: A person is snuggling with a pillow on a chair.Ground Truth4.9s18.9s||
GT4.9s18.9s||GT4.9s18.9s||VSLNet3.7s15.6s||Ours4.9s18.4s||Siamese Frames
leftrightQuery-related“pillow”“snuggling”enrichenrich
Figure 3: The visualization examples to show the beneﬁts from the siamese frames. Due to the boundary-bias
during the sparse sampling process, previous VSLNet method ﬁlters out the true-positive boundary frames and
fails to predict the accurate boundaries. Instead, our siamese learning strategy supplements the query-related
information of the adjacent frames into the ambiguous downsampled boundary-frames for predicting more precise
boundaries.
Number K=1 K=2 K=4 K=8
R@1, IoU=0.5 50.45 52.10 54.49 54.62
R@1, IoU=0.7 29.64 30.78 33.15 33.27
Table 6: The effect of the number Kof the sampled
siamese frames on ActivityNet Captions dataset.
the interdependent knowledge (i.e., siamese knowl-
edge) for integrating the samples. It can also prove
that adaptively reasoning by our siamese knowl-
edge is better than the purely average operation. We
think that the siamese knowledge not only serves
as the knowledge-routed representation, but also
implicitly constrains the semantic consistency of
frames in the space of frame-text features.
Effect of the usage of soft label We also inves-
tigate whether our soft label (ﬂoat value) of the
segment boundary contributes to the performance
of our model. As shown in Table 5, directly apply-
ing the soft label learning to the baseline does not
bring signiﬁcant performance improvement (model
­). This is mainly because that the boundary frame
may be query-irrelevant and its feature is not able
to be accurately matched with the query. Instead,
comparing model ±with model °, model ±en-
riches the boundary frames with siamese contexts
and supplements them with the neighboring query-
related visual information. Therefore, it brings
signiﬁcant improvement by using the soft label in
training process.
Effect of the number of siamese frames We
compare our method with various number of
siamese frames as shown in Table 6. When adding
the siamese sample number Kfrom 1 to 8, our
method dynamically promotes the accuracy. Such
improvement can demonstrate that more siamese
samples can bring richer knowledge, which makes
our network beneﬁted from it. Although the ac-
curacy is increasing with the number of siameseMethods VariantR@1, R@1,
IoU=0.5 IoU=0.7
VSLNetOrigin 43.22 26.16
+siamese 50.38 30.06
CBLNOrigin 48.12 27.60
+siamese 56.86 30.79
MGSLOrigin 51.87 31.42
+siamese 58.77 33.41
Table 7: We apply our siamese learning strategy to ex-
isting TSG models on ActivityNet Captions dataset.
frames, we observe that the improvement from the
number 4 to 8 is slight. We think the reason is
the saturation of knowledge, i.e., the model has
enough knowledge to learn the task on this dataset.
Hence, it is almost meaningless to purely increase
the siamese frames. To balance the training time
and accuracy, we assign K= 4 in our ﬁnal version.
Plug-and-Play Our proposed siamese learning
strategy is ﬂexible and can be adopted to other
TSG methods for anchor feature enhancement. As
shown in Table 7, we directly apply siamese learn-
ing strategy into existing module for anchor feature
enriching without using soft label training. It shows
that our siamese learning strategy can provide more
contextual and ﬁne-grained information for anchor
feature encoding, bringing large improvement.
4.5 Qualitative Results
In Figure 3, we show two visualization examples to
qualitatively analyze what kind of knowledge does
the siamese frames bring to the anchor frames. It
is unavoidable to lose some visual contents when
sparsely sampling from the video. Especially for
the boundary frames that are easily to be ﬁltered
out by sampling, the visual content of the newly
sampled boundary may lose query-relevant infor-
mation ( e.g., brown words in ﬁgure). However, we
can obtain the absent contents from their siameseframes due to different sampling indices and du-
ration. Hence, our siamese frames can enrich and
supplement the sampled frames with more con-
secutive query-related visual semantics to make a
ﬁne-grained video comprehension, keeping the ap-
propriate segment length of the sampled video for
more accurate boundary prediction.
5 Conclusion
In this paper, we propose a novel Siamese Sampling
and Reasoning Network (SSRN) to alleviate the
limitations of both boundary-bias and reasoning-
bias in existing TSG methods. In addition to the
original anchor frames, our model also samples a
certain number of siamese frames from the same
video to enrich and reﬁne the visual semantics of
the anchor frames. A soft label is further exploited
to supervise the enhanced anchor features for pre-
dicting more accurate segment boundaries. Exper-
imental results show both effectiveness and efﬁ-
ciency of our SSRN on three challenging datasets.
Limitations
This work analyzes an interesting problem of how
to learn from inside to address the limitation of the
boundary-bias on the temporal sentence ground-
ing. Since our method targets on the issue of long
video sampling, it may be not helpful to handle the
short video processing but still can improve the con-
textual representation learning for the short video.
Besides, our sampled siamese frames would bring
extra burden ( e.g., computation, memory and pa-
rameters) during the training and testing. Therefore,
a more light way to ease the siamese knowledge
extraction is a promising future direction.
6 Acknowledgments
This work was supported by National Natu-
ral Science Foundation of China (No.61972448,
No.62272328, No.62172038 and No.62172068).
References
Lisa Anne Hendricks, Oliver Wang, Eli Shechtman,
Josef Sivic, Trevor Darrell, and Bryan Russell. 2017.
Localizing moments in video with natural language.
InProceedings of the IEEE International Confer-
ence on Computer Vision (ICCV) .
Meng Cao, Long Chen, Mike Zheng Shou, Can Zhang,
and Yuexian Zou. 2021. On pursuit of designing
multi-modal transformer for video grounding. In
Proceedings of the 2021 Conference on EmpiricalMethods in Natural Language Processing (EMNLP) ,
pages 9810–9823.
Joao Carreira and Andrew Zisserman. 2017. Quo vadis,
action recognition? a new model and the kinetics
dataset. In proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition (CVPR) ,
pages 6299–6308.
Jingyuan Chen, Xinpeng Chen, Lin Ma, Zequn Jie, and
Tat-Seng Chua. 2018. Temporally grounding natural
sentence in video. In Proceedings of the 2018 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP) , pages 162–171.
Long Chen, Chujie Lu, Siliang Tang, Jun Xiao, Dong
Zhang, Chilie Tan, and Xiaolin Li. 2020. Rethink-
ing the bottom-up framework for query-based video
localization. In Proceedings of the AAAI Conference
on Artiﬁcial Intelligence .
Junyoung Chung, Caglar Gulcehre, KyungHyun Cho,
and Yoshua Bengio. 2014. Empirical evaluation of
gated recurrent neural networks on sequence model-
ing. In Advances in Neural Information Processing
Systems (NIPS) .
Jianfeng Dong, Xirong Li, Chaoxi Xu, Shouling Ji, and
Xun Wang. 2019. Dual encoding for zero-example
video retrieval. In Proceedings of the IEEE Confer-
ence on Computer Vision and Pattern Recognition
(CVPR) .
Xiang Fang, Daizong Liu, Pan Zhou, and Yuchong
Hu. 2022. Multi-modal cross-domain alignment net-
work for video moment retrieval. IEEE Transac-
tions on Multimedia .
Jiyang Gao, Chen Sun, Zhenheng Yang, and Ram Neva-
tia. 2017. Tall: Temporal activity localization via
language query. In Proceedings of the IEEE In-
ternational Conference on Computer Vision (ICCV) ,
pages 5267–5275.
Runzhou Ge, Jiyang Gao, Kan Chen, and Ram Nevatia.
2019. Mac: Mining activity concepts for language-
based temporal localization. In IEEE Winter Confer-
ence on Applications of Computer Vision (WACV) ,
pages 245–253.
Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei,
and Juan Carlos Niebles. 2017. Dense-captioning
events in videos. In Proceedings of the IEEE In-
ternational Conference on Computer Vision (ICCV) ,
pages 706–715.
Daizong Liu, Xiang Fang, Wei Hu, and Pan Zhou.
2022a. Exploring optical-ﬂow-guided motion and
detection-based appearance for temporal sentence
grounding. arXiv preprint arXiv:2203.02966 .
Daizong Liu and Wei Hu. 2022a. Learning to focus on
the foreground for temporal sentence grounding. In
Proceedings of the 29th International Conference on
Computational Linguistics , pages 5532–5541.Daizong Liu and Wei Hu. 2022b. Skimming, locating,
then perusing: A human-like framework for natural
language video localization. In Proceedings of the
30th ACM International Conference on Multimedia ,
pages 4536–4545.
Daizong Liu, Xiaoye Qu, Xing Di, Yu Cheng,
Zichuan Xu Xu, and Pan Zhou. 2022b. Memory-
guided semantic learning network for temporal sen-
tence grounding. In Proceedings of the AAAI Con-
ference on Artiﬁcial Intelligence .
Daizong Liu, Xiaoye Qu, Jianfeng Dong, and Pan
Zhou. 2020a. Reasoning step-by-step: Temporal
sentence localization in videos via deep rectiﬁcation-
modulation network. In Proceedings of the 28th In-
ternational Conference on Computational Linguis-
tics, pages 1841–1851.
Daizong Liu, Xiaoye Qu, Jianfeng Dong, and Pan
Zhou. 2021a. Adaptive proposal generation network
for temporal sentence localization in videos. In
Proceedings of the 2021 Conference on Empirical
Methods in Natural Language Processing (EMNLP) ,
pages 9292–9301.
Daizong Liu, Xiaoye Qu, Jianfeng Dong, Pan Zhou,
Yu Cheng, Wei Wei, Zichuan Xu, and Yulai Xie.
2021b. Context-aware biafﬁne localizing network
for temporal sentence grounding. In Proceedings of
the IEEE/CVF Conference on Computer Vision and
Pattern Recognition .
Daizong Liu, Xiaoye Qu, and Wei Hu. 2022c. Re-
ducing the vision and language bias for tempo-
ral sentence grounding. In Proceedings of the
30th ACM International Conference on Multimedia ,
pages 4092–4101.
Daizong Liu, Xiaoye Qu, Xiao-Yang Liu, Jianfeng
Dong, Pan Zhou, and Zichuan Xu. 2020b. Jointly
cross-and self-modal graph attention network for
query-based moment localization. In Proceedings
of the 28th ACM International Conference on Multi-
media , pages 4070–4078.
Daizong Liu, Xiaoye Qu, Yinzhen Wang, Xing Di, Kai
Zou, Yu Cheng, Zichuan Xu, and Pan Zhou. 2022d.
Unsupervised temporal video grounding with deep
semantic clustering. In Proceedings of the AAAI
Conference on Artiﬁcial Intelligence .
Daizong Liu, Xiaoye Qu, and Pan Zhou. 2021c. Pro-
gressively guide to attend: An iterative alignment
framework for temporal sentence grounding. In
Proceedings of the 2021 Conference on Empirical
Methods in Natural Language Processing (EMNLP) ,
pages 9302–9311.
Daizong Liu, Xiaoye Qu, Pan Zhou, and Yang Liu.
2022e. Exploring motion and appearance informa-
tion for temporal sentence grounding. In Proceed-
ings of the AAAI Conference on Artiﬁcial Intelli-
gence .Daizong Liu, Pan Zhou, Zichuan Xu, Haozhao Wang,
and Ruixuan Li. 2022f. Few-shot temporal sen-
tence grounding via memory-guided semantic learn-
ing. IEEE Transactions on Circuits and Systems for
Video Technology .
Meng Liu, Xiang Wang, Liqiang Nie, Xiangnan He,
Baoquan Chen, and Tat-Seng Chua. 2018a. Atten-
tive moment retrieval in videos. In Proceedings of
the 41nd International ACM SIGIR Conference on
Research and Development in Information Retrieval
(SIGIR) , pages 15–24.
Meng Liu, Xiang Wang, Liqiang Nie, Qi Tian, Bao-
quan Chen, and Tat-Seng Chua. 2018b. Cross-
modal moment localization in videos. In Proceed-
ings of the 26th ACM international conference on
Multimedia , pages 843–851.
Chujie Lu, Long Chen, Chilie Tan, Xiaolin Li, and Jun
Xiao. 2019. DEBUG: A dense bottom-up ground-
ing approach for natural language video localization.
InProceedings of the 2019 Conference on Empiri-
cal Methods in Natural Language Processing and
the 9th International Joint Conference on Natural
Language Processing (EMNLP-IJCNLP) .
Jonghwan Mun, Minsu Cho, and Bohyung Han. 2020.
Local-global video-text interactions for temporal
grounding. In Proceedings of the IEEE Confer-
ence on Computer Vision and Pattern Recognition
(CVPR) , pages 10810–10819.
Guoshun Nan, Rui Qiao, Yao Xiao, Jun Liu, Sicong
Leng, Hao Zhang, and Wei Lu. 2021. Interventional
video grounding with dual contrastive learning. In
Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR) .
Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for word rep-
resentation. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP) , pages 1532–1543.
Xiaoye Qu, Pengwei Tang, Zhikang Zou, Yu Cheng,
Jianfeng Dong, Pan Zhou, and Zichuan Xu. 2020.
Fine-grained iterative attention network for tempo-
ral language localization in videos. In Proceedings
of the 28th ACM International Conference on Multi-
media , pages 4280–4288.
Michaela Regneri, Marcus Rohrbach, Dominikus Wet-
zel, Stefan Thater, Bernt Schiele, and Manfred
Pinkal. 2013. Grounding action descriptions in
videos. Transactions of the Association for Compu-
tational Linguistics , 1:25–36.
Cristian Rodriguez, Edison Marrese-Taylor, Fate-
meh Sadat Saleh, Hongdong Li, and Stephen Gould.
2020. Proposal-free temporal moment localization
of a natural-language query in video using guided
attention. In The IEEE Winter Conference on Ap-
plications of Computer Vision (WACV) , pages 2464–
2473.Gunnar A Sigurdsson, Gül Varol, Xiaolong Wang, Ali
Farhadi, Ivan Laptev, and Abhinav Gupta. 2016.
Hollywood in homes: Crowdsourcing data collec-
tion for activity understanding. In European Confer-
ence on Computer Vision (ECCV) , pages 510–526.
Karen Simonyan and Andrew Zisserman. 2014. Very
deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556 .
Joyeeta Singha, Amarjit Roy, and Rabul Hussain
Laskar. 2018. Dynamic hand gesture recognition us-
ing vision-based approach for human–computer in-
teraction. Neural Computing and Applications .
Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Tor-
resani, and Manohar Paluri. 2015. Learning spa-
tiotemporal features with 3d convolutional networks.
InProceedings of the IEEE International Confer-
ence on Computer Vision (ICCV) , pages 4489–4497.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems (NIPS) , pages 5998–6008.
Jingwen Wang, Lin Ma, and Wenhao Jiang. 2020. Tem-
porally grounding language queries in videos by con-
textual boundary-aware prediction. In Proceedings
of the AAAI Conference on Artiﬁcial Intelligence .
Weining Wang, Yan Huang, and Liang Wang. 2019.
Language-driven temporal activity localization: A
semantic matching reinforcement learning model.
InProceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR) , pages
334–343.
Shaoning Xiao, Long Chen, Jian Shao, Yueting
Zhuang, and Jun Xiao. 2021. Natural language
video localization with learnable moment proposals.
InProceedings of the 2021 Conference on Empiri-
cal Methods in Natural Language Processing , pages
4008–4017.
Huijuan Xu, Kun He, Bryan A Plummer, Leonid Si-
gal, Stan Sclaroff, and Kate Saenko. 2019. Multi-
level language and vision integration for text-to-clip
retrieval. In Proceedings of the AAAI Conference on
Artiﬁcial Intelligence , volume 33, pages 9062–9069.
Xun Yang, Jianfeng Dong, Yixin Cao, Xun Wang,
Meng Wang, and Tat-Seng Chua. 2020. Tree-
augmented cross-modal encoding for complex-
query video retrieval. In Proceedings of the 43rd
International ACM SIGIR Conference on Research
and Development in Information Retrieval (SIGIR) ,
pages 1339–1348.
Yitian Yuan, Lin Ma, Jingwen Wang, Wei Liu, and
Wenwu Zhu. 2019a. Semantic conditioned dy-
namic modulation for temporal sentence grounding
in videos. In Advances in Neural Information Pro-
cessing Systems (NIPS) , pages 534–544.Yitian Yuan, Tao Mei, and Wenwu Zhu. 2019b. To ﬁnd
where you talk: Temporal sentence localization in
video with attention based location regression. In
Proceedings of the AAAI Conference on Artiﬁcial In-
telligence , volume 33, pages 9159–9166.
Runhao Zeng, Haoming Xu, Wenbing Huang, Peihao
Chen, Mingkui Tan, and Chuang Gan. 2020. Dense
regression network for video grounding. In Pro-
ceedings of the IEEE Conference on Computer Vi-
sion and Pattern Recognition (CVPR) , pages 10287–
10296.
Da Zhang, Xiyang Dai, Xin Wang, Yuan-Fang Wang,
and Larry S Davis. 2019a. Man: Moment alignment
network for natural language moment retrieval via
iterative graph adjustment. In Proceedings of the
IEEE Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 1247–1257.
H. Zhang, A. Sun, W. Jing, L. Zhen, and Rsm Goh.
2021. Parallel attention network with sequence
matching for video grounding. In Findings of the
Association for Computational Linguistics: ACL-
IJCNLP 2021 .
Hao Zhang, Aixin Sun, Wei Jing, and Joey Tianyi Zhou.
2020a. Span-based localizing network for natural
language video localization. In Proceedings of the
58th Annual Meeting of the Association for Compu-
tational Linguistics , pages 6543–6554.
Songyang Zhang, Houwen Peng, Jianlong Fu, and
Jiebo Luo. 2020b. Learning 2d temporal adjacent
networks for moment localization with natural lan-
guage. In Proceedings of the AAAI Conference on
Artiﬁcial Intelligence .
Zhu Zhang, Zhijie Lin, Zhou Zhao, and Zhenxin Xiao.
2019b. Cross-modal interaction networks for query-
based moment retrieval in videos. In Proceedings of
the 42nd International ACM SIGIR Conference on
Research and Development in Information Retrieval
(SIGIR) , pages 655–664.