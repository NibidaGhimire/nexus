QUEUE SCHEDULING WITH ADVERSARIAL BANDIT LEARNING
A P REPRINT
Jiatai Huang
Tsinghua University
hjt18@mails.tsinghua.edu.cnLeana Golubchik
University of Southern California
leana@usc.eduLongbo Huang
Tsinghua University
longbohuang@tsinghua.edu.cn
March 6, 2023
ABSTRACT
In this paper, we study scheduling of a queueing system with zero knowledge of instantaneous net-
work conditions. We consider a one-hop single-server queueing system consisting of Kqueues, each
with time-varying and non-stationary arrival and service rates. Our scheduling approach builds on
an innovative combination of adversarial bandit learning and Lyapunov drift minimization, with-
out knowledge of the instantaneous network state (the arrival and service rates) of each queue.
We then present two novel algorithms SoftMW (SoftMaxWeight) and SSMW (Sliding-window Soft-
MaxWeight), both capable of stabilizing systems that can be stablized by some (possibly unknown)
sequence of randomized policies whose time-variation satisÔ¨Åes a mild condition. We further gen-
eralize our results to the setting where arrivals and departures only have bounded moments instead
of being deterministically bounded and propose SoftMW+ andSSMW+ that are capable of stabilizing
the system. As a building block of our new algorithms, we also extend the classical EXP3.S [3]
algorithm for multi-armed bandits to handle unboundedly large feedback signals, which can be of
independent interest.
Keywords Scheduling, Queueing Bandit LearningLyapunov Analysis
1 Introduction
Stochastic network scheduling is concerned with a fundamental problem of allocating resources to serving demand in
dynamic environments, and it has found wide applicability in modeling real-world networked systems, including data
communication [19, 37], cloud computing and server farms [25, 10, 31, 5], smart grid management [14, 18, 24], supply
chain management [32, 4], and control of transportation networks [39, 7, 6]. One basic requirement of most existing
scheduling solutions is having knowledge of the instantaneous network state ‚Äì i.e., the amount of arrival trafÔ¨Åc and
the amount of service under any feasible control action, e.g., the power allocation among all links ‚Äì before taking a
new scheduling action. Given this information, there have been many successful network scheduling algorithms, with
various aspects of theoretical performance guarantees, including queue stability [38, 34, 23], delays [26, 27, 15], and
utilities [16, 15, 29].
However, in many real-world scenarios, such network-state knowledge may not always be available if its measurement
or estimation is too difÔ¨Åcult or costly to obtain. Even when such knowledge is available, it can be biased and imper-
fect. For instance, in an IoT system, due to sensors‚Äô temperature-drift or device malfunction, unexpected changes
in trafÔ¨Åc and channel patterns can occur at any time [11]. In an underwater communication system, it is extremely
challenging to perform perfect channel state estimation [17]. Moreover, in applications where the communicating
parties can move rapidly, e.g., self-driving vehicles [1], or in an arbitrary manner, e.g., wireless AR/VR devices [8],
channel conditions can also change rapidly and thus difÔ¨Åcult to estimate accurately. Therefore, scheduling policies
relying on precise network-state knowledge may not be applicable to many real-world tasks; relying on such policies
can result in signiÔ¨Åcant performance degradation due to inaccurate information. Hence, network scheduling without
instantaneous knowledge and accurate estimation of the network state is important both, in theory and in practice, i.e.,
it can signiÔ¨Åcantly improve robustness and availability of large-scale networked systems while reducing operational
and maintenance costs.arXiv:2303.01745v1  [math.OC]  3 Mar 2023Queue Scheduling with Adversarial Bandit Learning A P REPRINT
To this end, in this paper, we focus on a novel scheduling without network-state knowledge formulation. SpeciÔ¨Åcally,
we focus on a one-hop scheduling task, where a single-server serves Kqueues, each corresponding to a job type. The
server chooses a single queue to serve at each time slot. The network dynamics, i.e., arrival and service rates, evolve
in an oblivious adversarial manner and are unknown before the scheduling decision. Moreover, the service outcome
is only observed after the action with bandit feedback, i.e., only the served queue produces an observation. Our goal
is to seek an efÔ¨Åcient scheduling policy to stabilize the network.
To solve this problem, we introduce novel learning-augmented scheduling algorithms, inspired by the celebrated
MaxWeight queue scheduling algorithm [28] and the success of the EXP3 family of algorithms on non-stationary Multi-
Armed Bandits (MAB) problems [3]. The proposed algorithms are capable of stabilizing a non-stationary system, as
long as the system can be stabilized by a randomized policy whose total variation of probabilities to serve each type of
job is not too large. Perhaps surprisingly, our algorithms rely on neither knowing the network statistics before-hand,
nor on complicated explicit real-time estimation of the system. As a result, compared to its network-state knowledge
dependent counterparts, our algorithms are naturally more robust to jitter and unexpected trafÔ¨Åc/service patterns in the
system. Indeed, Appendix A gives a numerical comparison of our algorithms with their accurate knowledge dependent
counterparts. From this comparison, we show that the presented algorithms do give superior performance on systems
with service state noise, as depicted in Figure 1.
Figure 1: Numerical evaluation on a non-stationary
system (see Appendix A for details)Our work differs from the existing learning-augmented net-
work control literature, e.g., [9, 21, 13, 40], in the following as-
pects. [9, 13, 21] study the scheduling or load-balancing tasks
on stationary systems with rate statistics unknown before-
hand, while in our setting the system can be time-varying and
adversarial. [40] also considers non-stationary systems, but
they assume smoothly time-varying service rates and explicitly
estimate the instantaneous service rates using exponential av-
erage and discounted UCB bonus. Compared to these works,
our approach requires neither to explicitly optimize off-line
problems nor to explicitly probe and estimate the instantaneous
channel states, but rather uses adversarial bandit learning tech-
niques to coherently explore and stabilize the system at the
same time.
Our contributions in this work can be summarized as follows:
‚Ä¢ We propose two novel scheduling algorithms SoftMW (Algorithm 2) and SSMW (Sliding-window SoftMW , Algo-
rithm 3) that are capable of scheduling one-hop queueing systems without channel state knowledge, while stablizing
the systems under mild conditions on the time-variation of the reference randomized policy.
‚Ä¢ In designing these two algorithms, we carefully combine techniques from online bandit learning and Lyapunov drift
based scheduling approaches and analysis. The bandit part can guarantee that our algorithms‚Äô ‚Äúregret‚Äù against an
unknown time-varying randomized policy over a Ô¨Ånite time-horizon is small. The regret guarantee can be coupled
(in an innovative manner) with Lyapunov drift analysis to develop the stability result (see Sections 5.3 and 6.1).
‚Ä¢ We extend the EXP3.S algorithm [3], originally designed for adversarial MAB problems with bounded rewards,
such that time-varying learning rates and exploration rates are applicable to handling unboundedly large feedback
(see Section 5.1, Algorithm 1). This extended EXP3.S algorithm (we call EXP3.S+ ) is used as a building block in
SoftMW andSSMW . However, it is also of independent interest beyond the scope of queueing.
‚Ä¢ We further generalize our results to the setting where arrivals and departures have bounded moments instead of
being deterministically bounded (see Section 7). We present SoftMW+ (Algorithm 4) and SSMW+ (Algorithm 5) that
are capable of stabilizing the system.
Table 1 provides a comparison summary between our proposed algorithms and closely related efforts. To our knowl-
edge, our work is the Ô¨Årst to utilize adversarial MAB algorithms with dynamic regret guarantees in queueing systems
scheduling. Most prior work is based on epsilon-greedy or Upper ConÔ¨Ådence Bounds (UCB), where the assumption is
needed that the system is either stationary or non-stationary but with arrival (departure) rates having adequate smooth-
2Queue Scheduling with Adversarial Bandit Learning A P REPRINT
ness. Hence, our algorithms can apply to more general and complex settings. We believe our approach can facilitate
novel and interesting insights to MaxWeight -type as well as other queueing scheduling algorithm design problems.
Table 1: Overview of Our Algorithms and Closely Related Work
Algorithm Systems Stabilizable Average Queue Length
MaxWeight [36]Homogeneous Jobs (KM2
)
Assumption 1 + service rate forecasts (CWKM2
)
MaxWeight with
Discounted UCB[40]Assumption 1,
Service rates have smoothness matching the discounting factor1 
MK 1(1=)
SoftMW
(Ours , Algorithm 2)Assumption 1,
Assumption 2 ( (T1
2 )reference policy total variation)(CWKM2
)
SSMW
(Ours , Algorithm 3)Assumption 1,
Assumption 3 ( (T1 )reference policy time-homogeneous total variation) 
(1 +CV)MK 1(1=)
SoftMW+
(Ours , Algorithm 4)Assumption 1, Assumption 2 ( (T1
2 )reference policy total variation),
Arrivals and departures can be unbounded, but have bounded -th moment,>7(CWKM2
)
SSMW+
(Ours , Algorithm 5)Assumption 1,
Assumption 3 ( (T1 )reference policy time-homogeneous total variation),
Arrivals and departures can be unbounded, but have bounded 2nd moment 
(1 +CV)MK 1(1=)
1[40] uses similar assumption where the one-step service rate drift of each channel is universally upper-bounded by some polynomial of (1 ) 1.
Hereis a hyper-parameter of their algorithm, namely the discounting factor in UCB.
2 Notation
Throughout this paper, for n1, we denote the set f1;2;:::;ngby[n]and the (n 1)-dimensional probability
simplex over [n]by4[n]. We use bold English letters (e.g., Qt,St) and Greek letters with arrows above (e.g., ~ t,
~t) to denote vector-valued variables. We use 0to denote the all-zero vector, and 1to denote the all-one vector. We
use1ito denote the one-hot vector with 1on thei-th coordinate, i.e., (1i)j= 1 ifi=jand0otherwise. We use
1[statement ]to denote the indicator of a given statement; its value is taken as 1if the statement holds and 0otherwise.
We use xyto denote the element-wise product of two vectors xandy.
Letfbe a strictly convex function deÔ¨Åned on some convex domain ARK. For any x;y2A, ifrf(x)exists, we
write the Bregman divergence between yandxinduced byfas
Df(y;x),f(y) f(x) hrf(x);y xi
We usef(y),supx2RKfhy;xi f(x)gto denote the convex conjugate of f.
We useeO,e
oreto suppress poly-logarithmic factors in T(the length of the decision horizon) and K(the number
of queues). Unless stated otherwise, we use
Ft=(a1;:::;at;Q0;:::;Qt;A1;:::;At;S1;a1;:::;St;at)
for anyt0to denote the Ô¨Åltration of -algebra when studying random quantities indexed by time, i.e., Ftis
generated by all decisions and quantities visible to a scheduling policy at the end of t-th time slot.
3 Problem Setting
We consider the problem of scheduling Kjob types on a single work-conserving server with a slotted time system.
Each arriving job Ô¨Årst joins a queue associated with its type i, which we denote by Qi. Denote by At;ithe amount
of arriving jobs of type iin thet-th time slot, and by St;ithe maximum amount of jobs of type ithe server can serve
in thet-th time slot. At the beginning of each time slot t, the server chooses exactly one type of a job at2[K]to
serve. Denote by Qt;ithe queue length of type ijobs at the end of time slot t. Then, each Qt;ievolves according to
the following equation:
Qt;i= maxfQt 1;i+At;i St;i 1[i=at];0g
3Queue Scheduling with Adversarial Bandit Learning A P REPRINT
where Q0= (Q0;1;:::;Q 0;K) =0. At the beginning of each time slot t, the latest queue lengths Qt 1;1;:::;Qt 1;K
are available to the server for making new decisions. The maximum service amount of past actions S0;a0;:::;St 1;at
are also visible to the server.
We assume that there are two sequences of distributions fA1;A2;:::gandfS1;S2;:::g, all Ô¨Åxed before the queue
process starts, and their statistics are known to the scheduler before-hand. All distributions Ats andSts are supported
on[0;M]K, whereMis a constant known before-hand. We further assume that each Atis randomly sampled from
At, each Stis sampled fromSt, and all Ats andSts are independent random vectors. We denote by ~tthe mean of
At, and by~ tthe mean ofSt.
Our objective is to design a scheduling policy, under which we have the following upper-bound on the average expected
queue lengths:
1
TT 1X
t=0KX
i=1E[Qt;i] = (1):
We say a scheduling policy stabilizes the system, or the system is stable under some scheduling policy, if the above
bound holds.
Classical scheduling tasks on stationary systems (e.g., [9, 21]) correspond to the case where At=A1(St=S1),
i.e., the distributions are time-invariant in our setting. In our problem, it is complicated to explore and estimate the
time-varying service distributions subject to the queue stability.
4 A SufÔ¨Åcient Condition for Stabilizing the System
In our paper, we make the following assumption on the system, which is analogous to the capacity region deÔ¨Ånition in
stationary network scheduling [28], and can be viewed as a generalized stability condition for scheduling in adversarial
environments.
Assumption 1 (Piecewise Stabilizability) .There existCW0,>0,~1;~2;2 [K]and a partition of N+into
intervalsW0;W1;, such that for any T1we have
X
i:mint2Wit<T(jWij 1)2CWT (1)
and for anyi0andj2[K]we have
1
jWijX
t2Wit;jt;j+1
jWijX
t2Wit;j: (2)
Assumption 1 can be regarded as a generalizition of the (W;)-constrained dynamics in [22]. It essentially assumes
that the time horizon can be divided into intervals, within which there exist stationary policies that can stablize the
network (Eq. (2)). As a quick sanity check, for stationary instances where the arrival rate vector is in the interior of
the capacity region, Assumption 1 is automatically satisÔ¨Åed with CW= 0 (hence allWis are singleton sets) and all
~is are equal to some Ô¨Åxed element ~2[K], which is a randomized policy capable of stabilizing the system.
Remark. In fact, under the above assumption, by a quadratic Lyapunov drift argument (see Theorem 5.5), we can also
show that a policy, in which at each time step twe serve a type of job atindependently at random according to the
distribution indicated by ~t, can stabilize the system as well (require knowing ~tbeforehand). We call f~t:t1gthe
reference mixed action sequence , and refer to the above randomized policy induced by f~t:t1gasthe reference
randomized policy .
With Assumption 1, in general, it is still a challenging problem to scheduling the system. For our main results in
Section 5, we need another technical assumption presented below.
Assumption 2 (Reference Policy Stationarity) .For the reference mixed action sequence f~tgin Assumption 1, there
exist some>0andCV>0such that
T 1X
t=1k~t+1 ~tk1CVT1
2 
for anyT1.
4Queue Scheduling with Adversarial Bandit Learning A P REPRINT
Intuitively speaking, Assumption 2 says that the sequence f~tg(and hence the environment) does not change in a very
abrupt way. Similar smooth assumptions have also been made in existing results, e.g., [40].1In Section 6, we will
also study when can we handle problems where the reference policy has signiÔ¨Åcantly larger variation.
5 Queue Scheduling with only Bandit Feedback
In contrast to the setting with perfect network state knowledge, in our case, there is no such accurate channel condition
for the scheduler. SpeciÔ¨Åcally, the server only receives a bandit feedback for each time step‚Äôs actual service, i.e., only
St;atis known after the service decision atis made.
In this section, we present a novel algorithm, which is capable of stabilizing the system using only bandit feedback,
St;at. Our core idea is to embed a suitable Multi-Armed Bandit algorithm into the MaxWeight scheduler [36], so
that the term E[PT
t=1Qt 1;atSt;at], which is the key ingredient of MaxWeight , is guaranteed to be not too far from
E[PT
t=1hQt 1St;~ti]. Given access to ~ t,MaxWeight achieves this by greedily choosing at= arg maxiQt 1;it;i
at each time step t. However, when ~ tis unknown and time-varying, it is hard to guarantee that each summand
Qt 1;atSt;atis large. Thus, we focus on optimizing the whole sum E[PT
t=1Qt 1;atSt;at].
In the remainder of this section, we will Ô¨Årst present EXP3.S+ , an extended version of the EXP3.S [3] algorithm
for adversarial MAB (Section 5.1). EXP3.S+ has adequate Ô¨Çexibility to serve as an important building block of
our novel scheduling algorithm SoftMW (Section 5.2). We also present its performance guarantee, as it is key for
understanding our later analysis. Finally, in Section 5.3, we outline analysis of SoftMW and describe several important
novel techniques to relate adversarial MAB learning to Lyapunov drift analysis.
5.1 EXP3.S+ : An Extended Version of EXP3.S
We Ô¨Årst present EXP3.S+ , which extends the EXP3.S algorithm [3], designed originally for solving adversarial Multi-
Armed Bandit (MAB) problems, to address the potentially unbounded queue lengths in queueing systems, which
cannot be directly handled by existing bandit algorithms.
More formally, EXP3.S+ applies to the following scenario: there is an agent and an adversary simultaneously making
decisions on a Ô¨Ånite-length time-horizon t= 1:::T . At each time t, the agent chooses an xt2[K]deterministically
based on observed history, then samples at2[K]according to xt. Simultaneously (at time t), the adversary chooses
gt2RKdeterministically, based on observed history. Then, gt;atis revealed to the agent. The high-level objective for
the agent is to maximize the cumulative feedbackPT
t=1gt;at. The details of our EXP3.S+ are described in Algorithm 1.
Algorithm 1: EXP3.S+
Input: Number of actions K, time-horizon length T, initial mixed action x12[K]
Output: A sequence of actions a1;a2;:::;aT2[K]
Intermediate Variables: A sequence of learning rates 1;2;:::;T2R+, a sequence of implicit exploration
rates1;2;:::;T2[0;1=K], a sequence of explicit exploration rates
1;2;:::;T2[0;1=2], a sequence of explicit exploration normal vectors
e1;e2;:::;eT2[K]
1	(x),PK
i=1(xilnxi xi)
2fort= 1;2;:::;T do
3 Chooset,t,etandi
4 Below denote by [K];t,fx2[K]:xit8i2[K]g
5pt (1 t)xt+tet
6 Sampleatpt, take action at, observegt;at
7egt gt;at=pt;attheat-th coordinate
0 the other coordinates
8xt+1 arg minx024[K];th tegt;x0i+D	(x0;xt)
1Strictly speaking, [40] introduces a smoothness assumption on the arrival and service rate rather than the reference randomized
policy.
5Queue Scheduling with Adversarial Bandit Learning A P REPRINT
Remark. The amplitude of feedback value gt;atin Line 6 is crucial to the correctness of EXP3.S . The original EXP3.S
algorithm in [3] uses a constant learning rate and a constant exploration rate across allTtime steps. However, the
algorithm can only support problems with feedback value no more than  1, and does not apply to our setting, where
the queue length (which is the reward of EXP3.S ) can go unbounded. For our purpose, in the presented algorithms, we
feedQt 1;atSt;atintoEXP3.S+ as the reward value, which is a quantity that can be arbitrarily large (since Qt 1;atcan
be arbitrarily large). In EXP3.S+ , the learning rates and exploration rates can both be time-varying, and the exploration
rates can even be action-dependent (it allows specifying any et2[K]rather than 1=K).
The formal performance guarantee of EXP3.S+ forPT
t=1gt;atis given in Theorem 5.1 below.
Theorem 5.1 (EXP3.S+ Dynamic Regret Guarantee) .During the execution of Algorithm 1, for any Ô¨Åxed sequence
~1;:::;~T2[K], if w.p.1 the following events happen,
(i)x12[K];1,
(ii)gt 1
ttetfor all 1tT,
(iii)12T,
(iv)12T,
(v)~t2[K];tfor all 1tT,
then let
V,T 1X
t=1k~t+1 ~tk1;
we will have
E"TX
t=1hgt;~ti#
 E"TX
t=1gt;at#
(1 +V)E
 1
Tln1
T
+eE"TX
t=1tkgtk2
2#
+E"TX
t=1thgt;eti#
:
In Appendix B, we provide a formal proof for Theorem 5.1 using an analysis based on Online Mirror Descent [12],
which is much more suitable for handling time-varying learning rates compared to the classical sum-of-exp potential
function approach in [3]. We also discuss a practical implementation of the arg max calculation (at Line 8) in Ap-
pendix C. We note that Algorithm 1 and its analysis can be of independent interest and applied to problems other than
stochastic network scheduling.
5.2 Soft Max-Weight Scheduling using EXP3.S+
We now present our novel scheduling algorithm, SoftMW , in Algorithm 2. SoftMW is based on carefully designed
feedback signals as well as parameters and learning rates in EXP3.S+ . Its name refers to the computation in EXP3.S+
(Algorithm 1) that is heavily based on the softmax operation (see Appendix C).
The intuitive reason why Algorithm 2 works is as follows. We use EXP3.S+ in a carefully designed way to drive the
scheduling process, so that the effect of Algorithm 2 is very closed to (or better than) the reference randomized policy
given by Assumption 1, in the sense that under Algorithm 2, the queues‚Äô total quadratic Lyapunov drift is only slightly
larger (or even smaller) than that under the reference randomized policy. Therefore, Algorithm 2 has similar (or even
stronger) capability of stabilizing the system.
Algorithm 2‚Äôs average queue length bound on any Ô¨Ånite time-horizon is given in Theorem 5.2.
Theorem 5.2. For problem instances satisfying Assumptions 1 and 2, SoftMW (Algorithm 2) guarantees
1
TE"TX
t=1kQtk1#
2(K+ 1)M2+ 4CW(KM2+KM )
+o(1):
In particular, the system is stable.
Remark. As a quick sanity check, for stationary problem instances, Theorem 5.2 gives (KM2=)average queue
length bound, which coincides with the classical result we can achieve in stationary problems ([28] Sec. 3.1 ). In
fact, one can show that for both (i) pretending to have accurate one-step forecasts for service rates and running vanilla
6Queue Scheduling with Adversarial Bandit Learning A P REPRINT
Algorithm 2: SoftMW (Soft M axWeight)
Input: One-step arrival/service upper-bound M > 0, Number of job types K, Problem instance smoothness
parameter¬ø 0
Output: A sequence of job types to serve a1;a2;:::2[K]
1Initialize an EXP3.S+ instance with Kavailable actions and x1=1=K
2fort= 1;2;:::do
3 Pick the following parameters of EXP3.S+ for time slot t:
4t t 3=K
5t=
t (1
4 
2)Mq
86M2K6t3
2+Pt 1
s=0kQsk2
2 1
6 et=Qt 1=kQt 1k1
7t=MtkQt 1k1=kQt 1k1
t (1
4 
2)q
86M2K6t3
2+Pt 1
s=0kQsk2
2 1
8 Take a new action decision output atfrom EXP3.S+ , serve theat-th queue , regard Qt 1;atSt;atas a new
feedbackgt;atand feed it into the current EXP3.S+ instance
MaxWeight , and (ii) running the reference randomized policy f~tgspeciÔ¨Åed in Assumption 1, the average queue
length bounds via a standard quadratic Lyapunov analysis are 
(CWK+K+1)M2

. Therefore, informally, in terms
of queue length bound, the overhead due to SoftMW on problem instances satisfying Assumption 2 is insigniÔ¨Åcant.
5.3 Queue Stability Analysis Outline
In this section, we give a brief outline of how to formally establish the queue stability result (Theorem 5.2). We
Ô¨Årst review the general procedure from quadratic Lyapunov drift analysis. Then, we show that EXP3.S+ scheduling
can lead to terminal Lyapunov function values close to the reference policy in Assumption 1, differing by a term
proportional topPkQtk2
2. Finally, we relate thispPkQtk2
2term with the queue lengths (PkQtk1) we want to
bound, and show that the Lyapunov terminal value bound leads to an average queue length bound.
5.3.1 Recap of Lyapunov Drift Analysis
In our analysis, we use standard results from quadratic Lyapunov drift analysis [28]. Conventionally, we deÔ¨Åne
Lt,1
2kQtk2
2=1
2KX
i=1Q2
t;i;
as the quadratic Lyapunov function of the queue lengths. We Ô¨Årst have the following standard lemma regarding the
drift upper bound.
Lemma 5.3 (General quadratic Lyapunov Drift Upper-bound [28]) .Consider any scheduling policy for this queueing
system and suppose that the policy randomly picks a job type ataccording to a probability distribution pt(which may
depend on the system‚Äôs history, i.e., ptis anFt 1-measurable random vector supported on [K]). Let Qtdenote the
queue length vector under that policy. We have
E[Lt Lt 1jFt 1](k+ 1)M2
2+hQt 1;~t ~ tpti
=(k+ 1)M2
2+hQt 1;~ti E[Qt 1;atSt;atjFt 1]
for anyt1. By summing the inequalities over 1tT, taking total expectation and then rearranging the terms,
we get
E"TX
t=1Qt 1;atSt;at hQt 1;~ti#
(K+ 1)M2T
2(3)
for any time horizon length T1.
7Queue Scheduling with Adversarial Bandit Learning A P REPRINT
Next, we have Lemma 5.4 regarding the drift value under the reference policies. As in the standard Lyapunov drift
analysis [28], this bound will be useful for deriving queue length results for queue-based policies.
Lemma 5.4 (Negative Lyapunov Drift under Reference Policy) .Suppose Assumption 1 holds. Consider any schedul-
ing policy for this queueing system, under which the queue length vectors are denoted by fQtg. Letf~t:t1g
be the sequence of probabilities to serve each queue as deÔ¨Åned in Assumption 1. Then, for any time horizon length
T1, we can Ô¨Ånd a constant TTthat depends only on T, such thatTTTT+q
T
CW+ 1and
E"TTX
t=1hQt 1;~ t~t ~ti#
E"TTX
t=1kQt 1k1#
 (KM2+KM )CWTT
E"TX
t=1kQt 1k1#
 (KM2+KM )CWTT:
Hereis the element-wise product, i.e., ~ a~b= (a1b1;:::;aKbK), andCWis the constant deÔ¨Åned in Assumption 1.
Proof. See Appendix D.
Combining Theorem 5.3 and Theorem 5.4, we obtain the following important proposition for our analysis.
Proposition 5.5 (SufÔ¨Åciently-Large-Weight Implies Queue Stability) .Suppose Assumption 1 holds, also suppose a
scheduling policy guarantees the following.
E"TX
t=1Qt 1;atSt;at#
E"TX
t=1hQt 1;~ t~ti#
 f(T)
for allTmaxf4
CW;CWg, wheref(T)is some non-negative, increasing function of T. Then, we have
1
TE"TX
t=1kQtk1#
(K+ 1)M2+ 2CW(KM2+KM )
+f(2T)
T:
In particular, if f(T)is(T), then this policy stabilizes the system.
Proof. See Appendix D.
Remark. Theorem 5.5 implies that for problem instances satisfying Assumption 1, serving the queue accord-
ing to eitherf~tg(the reference randomized policy) or the vanilla MaxWeight algorithm (assuming that service
rate forecasts are available to the algorithm at the time of decision making), the average queue length will be no
more than(K+1)M2+2CW(KM2+KM )
, as claimed earlier in Section 4. This is because in both cases, we have
E[Qt 1;atSt;atjFt 1]hQt 1;~ t~ti. Hence, the condition in Theorem 5.5 holds with f(T) = 0 for these
two policies.
In the remaining of the analysis, we will derive the corresponding f(T)forSoftMW+ , so that we can conclude the
queue stability via an argument similar to Theorem 5.5.
5.3.2 From EXP3.S+ Regret Bound to Lyapunov Function Value Bound
To build the queue stability result for SoftMW (Algorithm 2), our high-level idea is to develop the required condition
in Theorem 5.5 such that f(T)can also be properly controlled. Since SoftMW makes decisions based on EXP3.S+ ,
intuitively, we should utilize the regret upper-bound result Theorem 5.1. In order to do that, we need to verify that the
required conditions (i)-(iv)2in Theorem 5.1 hold.
In fact, in SoftMW , our choices of ts andts are obviously decreasing, hence condition (iii) and (iv) hold. We choose
x1= (1=K;:::; 1=K)thus condition (i) also holds; the choice of tandetalso guarantees condition (ii). The real
issue is whether ‚Äôs exceed1
2. This is established in the following proposition.
2The reference policy f~tgitself may not satisÔ¨Åes condition (v), but we will project each ~tonto[K];tas~0
t, and only use
Theorem 5.1 to obtain a regret bound against the action sequence f~0
tg.
8Queue Scheduling with Adversarial Bandit Learning A P REPRINT
Proposition 5.6 (Feasibility of the Exploration Rates in SoftMW ).For allt1, we havet1
2inSoftMW .
Appendix E gives a detailed proof of Theorem 5.6. Having conÔ¨Årmed that the algorithm is feasible, we can now safely
apply Theorem 5.1, resulting in the following property of SoftMW .
Lemma 5.7 (SoftMW Large-Weight Guarantee ) .Suppose Assumptions 1 and 2 hold; then, running Algorithm 2
guarantees
TX
t=1Eh
hQt 1;St~ti Qt 1;atSt;ati
E8
<
:9M(1 +CV)T1
4 
2(3 lnT+ lnK)vuut86M2K6T3
2+TX
t=1kQt 1k2
2+ 4M29
=
;(4)
for any time horizon length T1. Heref~tgis the reference policy in Assumptions 1 and 2.
Proof. See Appendix E.
Theorem 5.7 gives an upper-bound for EhPQt 1;atSt;at PhQt 1;~ t~tii
, which is closely related to the con-
dition required by Theorem 5.5. However, this upper-bound is not yet a quantity that depends solely on T; it still has a
factor ofp
E[PkQtk2
2], depending on the actual queueing trajactory. Therefore, we are unable to apply Theorem 5.5
directly to claim queue stability. Rather, we need to work with thep
E[PkQtk2
2]factor, to convert it to the cumulative
queue length E[PkQtk1], just as we did in Theorem 5.4 to convert EhPhQt 1;~ t~t ~tii
to queue lengths.
5.3.3 Relate Regrets inp
E[PkQtk2
2]to Queue Lengths E[PkQtk1]
Plugging Eq. (4) into Eq. (3) in Theorem 5.3, after further applying Theorem 5.4 and rearranging terms, we get the
following proposition, which offers an inequality connectingp
E[PkQtk2
2]andE[PkQtk1].
Proposition 5.8. Given Assumptions 1 and 2, Algorithm 2 gives us
E"TTX
t=1kQt 1k1#
(5)
(K+ 1)M2+ 2CW(KM2+KM )
TT+4M2
+g(TT)
vuut86M2K6T3
2
T+E"TTX
t=1kQt 1k2
2#
(K+ 1)M2+ 2CW(KM2+KM )
TT+4M2
+p
86MK3T3
4
Tg(TT)
+g(TT)
vuutE"TTX
t=1kQt 1k2
2#
for anyTmaxf4
CW;CWg, whereTTis some constant no more than 2T, and
g(T) = 9M(1 +CV)T1
4 
2(3 lnT+ lnK) =e(T1
4 
2):
Recall that all arrivals and departures are assumed to be bounded by a constant M. Therefore, each dimension of the
queue length vectors fQtgis a sequence of non-negative numbers, where the difference between any two adjacent
terms is withinM. We may then make use of the following lemma for such bounded-difference sequences.
Lemma 5.9. Supposex1= 0,x2;:::;xn0,jxi+1 xij1for all 1i<n . Denote by S=Pn
i=1xi; then we
havenX
i=1x2
i4S3
2:
Proof. See Appendix E.
9Queue Scheduling with Adversarial Bandit Learning A P REPRINT
For our purposes, Theorem 5.9 guarantees that
TX
t=1kQt 1k2
24p
MKX
i=1 TX
t=1Qt 1;i!3
2
4p
M TX
t=1kQt 1k1!3
2
: (6)
Then, plugging Eq. (6) into Eq. (5), we obtain the following inequality that depends entirely onEhPTT
t=1kQt 1k1i
:
E"TTX
t=1kQt 1k1#
h(TT) +g(TT) 
E"TTX
t=1kQt 1k1#!3
4
(7)
where
g(T) = 18M5
4(1 +CV)T1
4 
2(3 lnT+ lnK) =e 
T1
4 
2
!
;
h(T) =(K+ 1)M2+ 2CW(KM2+KM )
T+e(T1 
2):
It remains to solve Eq. (7), in order to obtain an upper bound for EhPTT
t=1kQt 1k1i
. To do so, we utilize the following
lemma.
Lemma 5.10. Lety;f;g :R+![1;1)be three non-decreasing functions. If
y(x)f(x) +y(x)1
4g(x)
for allx0, then we have
y(x)
f(x)1
4+g(x)4
:
Proof. See Appendix E.
Finally, according to Theorem 5.10, the solution of Eq. (7) gives us:
E"TTX
t=1kQt 1k1#

h(TT)1
4+g(TT)4
(K+ 1)M2+ 2CW(KM2+KM )
TT+o(TT):
Thus,
1
TE"TX
t=1kQt 1k1#
1
TE"TTX
t=1kQt 1k1#
(K+ 1)M2+ 2CW(KM2+KM )
TT
T+o(TT=T)
2(K+ 1)M2+ 4CW(KM2+KM )
+o(1)
as desired.
6 Taming Time-Homogeneous (T1 )Refernce Policy Total Variation
In this section, we propose another novel algorithm capable of stabilizing our adversarial queueing system. Specif-
ically, this algorithm is stable under a reference randomized policy with O(T1 )total variation, as long as that
much total variation is to some extent ‚Äúevenly‚Äù distributed throughout the inÔ¨Ånite time horizon. This new condition is
formalized as follows.
Assumption 3 (Time-Homogeneous Reference Policy Stationarity) .For the sequencef~tgin Assumption 1, there
exist some>0andCV>0such that
T0+T 1X
t=T0+1kt+1 tk1CVT1 
for anyT00andT1.
10Queue Scheduling with Adversarial Bandit Learning A P REPRINT
Remark. Assumption 3 can be viewed as a shift-invariant version of Assumption 2, with the degree of Trelaxed from
1
2 to1 . Roughly speaking, this assumption holds as long as there is only a Ô¨Ånite number of time periods on
which the reference policy variation accumulates at a linear rate. For example, ifPT
t=0kt+1 tk1= (T1 ), then
Assumption 3 is satisÔ¨Åed.
For problem instances where Assumptions 1 and 3 hold, we present a new algorithm to stabilize the system, namely
Sliding SoftMW (SSMW ), which is detailed in Algorithm 3.
Figure 2: Comparison between SSMW andSoftMW onEXP3.S+ invocations
Algorithm 3: SSMW (Sliding-window SoftMW )
Input: One-step arrival/service moment upper-bound parameter M > 0, Number of job types K, Problem
instance smoothness parameter ¬ø 0
Output: A sequence of job types to serve a1;a2;:::2[K]
1while truedo
2T0 the latest time index tat which we have made a new decision at // for the first iteration, we
should have T0= 0
3m maxn
dkQT0k1
2Me;1o
4 Run a fresh EXP3.S+ instance for mtime steps with the following conÔ¨Åguration (below denotes the time
index within the epoch of length m, 1-based):
5=m 2=K
6 x1can be any element in [K];,fx2[K]:xi8i2[K]g
7=
6M2Km1+
2 1
8 e=QT0+ 1=kQT0+ 1k1
9=MkQT0+ 1k1=1
6K 1M 1m 1 
2kQT0+ 1k1
10 Take a new action decision output from the current EXP3.S+ instance, serve this type of jobs (recall we are at
the(T0+)-th time step of the whole time horizon), regard QT0+ 1;aT0+ST0+;aT0+as a new feedback
g;aand feed it into EXP3.S+
Compared to SoftMW (Algorithm 2), SSMW (Algorithm 3) does not use historical queue lengths at the beginning to
tune the EXP3.S+ learning rates. Instead, SSMW starts with new EXP3.S+ instances of lengths proportional to the
current queue lengths (Line 3). As a result, SSMW initiates many more EXP3.S+ instances throughout its execution,
though each EXP3.S+ period is likely to be short (demonstrated in Fig. 2). In this sense, SSMW is more similar to
MaxWeight , since MaxWeight always uses the current queue length vector for making new decisions, and disregards
how the system arrived at the current state. Theorem 6.1 gives the queue stability result for SSMW .
Theorem 6.1. For problem instances satisfying Assumptions 1 and 3, SSMW (Algorithm 3) guarantees
1
TE"TX
t=1kQtk1#

3KM2m0+(K+ 1)M2
2+ (KM2+KM )CW+ 6M2
10

for any time horizon of length T4
CW+CW. In particular, the system is stable. Here m0is deÔ¨Åned as
m0,infn
m:m2;f(m0)
28m0mo
 
(1 +CV)MK lnK 1(1=)
11Queue Scheduling with Adversarial Bandit Learning A P REPRINT
where
f(m) = 88(1 +CV)MKm 
2(2 lnm+ lnK):
Remark. Compared to the ( 1)queue length bound of SoftMW (Theorem 5.2), Theorem 6.1 only gives an (1=)
queue length guarantee. Nevertheless, the simulation results in Appendix A show that the empirical performance of
SSMW is comparable or even better than that of SoftMW .
6.1 Queue Stability Analysis Outline for SSMW
In this section, we provide a proof outline for Theorem 6.1. First, applying Theorem 5.1, we can obtain the result
in Theorem 6.2 regarding the performance of SSMW compared to the corresponding reference poilcy in each EXP3.S+
instance of SSMW .
Lemma 6.2. Suppose Assumptions 1 and 3 hold, then, let T0be some time step at which we start a new EXP3.S+
instance of length min Algorithm 3; then we have
1[T0ends an EXP3.S+ instance, and the new EXP3.S+ instance is of length m]
mX
t=1Eh
hQT0+t 1ST0+t;~T0+ti QT0+t 1;aT0+tST0+t;aT0+tFT0i
6 (1 +CV)M2Km2 
2(2 lnm+ lnK) +K 1m 1 
2E"mX
=1kQT0+ 1k2
2FT0#
+ 6M2: (8)
Proof. See Appendix F.
Similar to what we have done after obtaining Theorem 5.7 when analysing SoftMW , we will relate this regret upper-
bound in Theorem 6.2 to the actual cumulative queue length E[PkQtk1]. To achieve this, we need several lemmas on
sums of bounded-increment sequences.
Lemma 6.3. For anyT01, supposeM0,kQT0k14M, then for any T0+ 1tT0+kQT0k1
2M+ 1, we
have1
4kQT0k11
2kQT0k1 MkQt 1k13
2kQT0k1+M2kQT0k1:
LetT=dkQT0k1
2Me; then we have
1
4MTkQT0+t 1k14MT
for any 1tT. Moreover,
1
16M2T3T0+TX
t=T0+1kQt 1k2
216KM2T3;
1
4MT2T0+TX
t=T0+1kQt 1k14KMT2:
Proof. See Appendix F.
We can now apply Theorem 6.3 to relate the regret upper-bound in Theorem 6.2 to the queue-length sumsPkQtk1)
to obtain the result in Theorem 6.4.
Lemma 6.4. Suppose Assumptions 1 and 3 hold. Let T0be a time step at which we start a new EXP3.S+ instance of
lengthmin Algorithm 3. Then, we have
1
T0ends an EXP3.S+ instance, and the new EXP3.S+ instance is of length m=kQT0k1
2M
;m2
mX
t=1Eh
hQT0+t 1ST0+t;~T0+ti QT0+t 1;aT0+tST0+t;aT0+tFT0i
12Queue Scheduling with Adversarial Bandit Learning A P REPRINT
6M2+E"
f(m)mX
t=1kQT0+t 1k1FT0#
;
where
f(m) = 88(1 +CV)MKm 
2(2 lnm+ lnK):
Proof. See Appendix F.
Compared to SoftMW , anSSMW execution contains multiple EXP3.S+ executions, and their starting times and ending
times are all stochastic quantities. To rigorously handle these stochastic EXP3.S+ epochs in the analysis, we will
introduce a few more notations. Denote by i(i0) the time at which the i-thEXP3.S+ instance Ô¨Ånishes. Then,
0= 0 andfigis a sequence of non-decreasing fFtg-adapted stopping-times. Furthermore, each i+1isFi-
measurable. Fix any T1and deÔ¨Åne
0
i,8
<
:0 ifi= 0
i ifi>0and0
i 1<T
0
i 1otherwise;
i.e.,0
ican be regarded as the epoch end time i, but truncated at T, and it will be more convenient than iwhen we
consider the cumulative regret up to time T. Then,f0
igis also a sequence of non-decreasing fFtg-adapted stopping-
times, each 0
i+1isF0
i-measurable, and 0
i+1=0
iif any only if 0
iT. Thus, we can restate Theorem 6.4 as
Theorem 6.5.
Lemma 6.5. Suppose Assumptions 1 and 3 hold. Then, we have
1
kQ0
ik14M0
i+1 0
iX
t=1Eh
hQ0
i+t 1S0
i+t;~0
i+ti Q0
i+t 1;a0
i+tS0
i+t;a0
i+tF0
ii
h(0
i+1 0
i) +g(0
i+1 0
i)0
i+1 0
iX
t=1E
kQ0
i+t 1k1F0
i
for anyi0, where
g(m) = 88(1 +CV)MKm 
2(2 lnm+ lnK); (9)
h(m) = 1[m> 0]6M2: (10)
Compared to Theorem 6.4, now Theorem 6.5 allows us to sum the regret bounds for each epoch freely without caring
about the subtleties caused by the Ô¨Ånite time-horizon length T.
Next, we will make use of Lemma 6.5 to bound E[PT1
t=1kQt 1k1]. To begin, Ô¨Åx some T1, letT0,supfi:i
0;i<Tg, andT1,inffi:i0;iTg. We see thatT0andT1are bothfFtg-adapted stopping-times, and T1is
FT0-measurable. Note that T0<TT1. Furthermore, since T1 T0is the length of the last epoch in the Ô¨Årst Ttime
steps, we haveT1 T0kQT0k1
2M+ 1T0M
2M+ 1 =T0
2+ 1T0
2+T, thus we can seeT15
2T.
In the remainder of this section, we combine Theorem 5.3 and Theorem 6.5 to bound E[PT1
t=1kQt 1k1]in(E[T1]) =
(T)in order to conclude that E[PT
t=1kQt 1k1]is also (T).
To this end, recall that >0is the lower-bound of the ‚Äúaverage advantage of departure against arrival‚Äù of the reference
policyf~tgin Assumption 1. DeÔ¨Åne
m0,infn
m:m2;g(m0)
28m0mo
:
Thenm0is a constant that only depends on and; in fact, it solves to
m0 
(1 +CV)MK lnK 1(1=):
By considering whether each epoch length 0
i+1 0
iis greater than m0or not, we conclude from Theorem 6.5 that for
alli0,
13Queue Scheduling with Adversarial Bandit Learning A P REPRINT
0
i+1 0
iX
t=1Eh
hQ0
i+t 1S0
i+t;~0
i+ti Q0
i+t 1;a0
i+tS0
i+t;a0
i+tF0
ii
h(0
i+1 0
i) +
20
i+1 0
iX
t=1E
kQ0
i+t 1k1F0
i
| {z }
when0
i+1 0
i>m0, apply Theorem 6.5+ 1[0
i+1 0
im0]0
i+1 0
iX
t=1Eh
hQ0
i+t 1S0
i+t;~0
i+tiF0
ii
| {z }
when0
i+1 0
im0, simply drop the minus-signed term
(a)
h(0
i+1 0
i) +
20
i+1 0
iX
t=1E
kQ0
i+t 1k1F0
i
+ 1[0
i+1 0
im0]M0
i+1 0
iX
t=1E
kQ0
i+t 1k1F0
i
(b)
h(0
i+1 0
i) +
20
i+1 0
iX
t=1E
kQ0
i+t 1k1F0
i
+ 1[0
i+1 0
im0]KM0
i+1 0
iX
t=1E
kQ0
ik1+Mm 0F0
i
(c)
h(0
i+1 0
i) +
20
i+1 0
iX
t=1E
kQ0
i+t 1k1F0
i
+ 1[0
i+1 0
im0]KM0
i+1 0
iX
t=1E
3Mm 0jF0
i
h(0
i+1 0
i) +
20
i+1 0
iX
t=1E
kQ0
i+t 1k1F0
i
+ 3KM2m0(0
i+1 0
i) (11)
Hereh(m)is the function in (10). In steps (a)and(b)we simply leverage the assumption of bounded queue length
increments. Step (c)is due tokQ0
ik12M(0
i+1 0
i)2Mm 0as long as0
i+1 0
i>0, i.e.,iis not the index
of an epoch after the Ô¨Årst Ttime slots.
Summing Eq. (11) over all i0and then taking total expectations, we obtain
E"T1X
t=1hQt 1St;~ti Qt 1;atSt;at#

2E"T1X
t=1kQt 1k1#
+ 3KM2m0E[T1] +E"1X
i=0h(0
i+1 0
i)#
:
From (10), we see that in any sample path,P1
i=0h(0
i+1 0
i)6M2T1. Therefore,
E"T1X
t=1hQt 1St;~ti Qt 1;atSt;at#

2E"T1X
t=1kQt 1k1#
+ (3KM2m0+ 6M2)E[T1]: (12)
According to Theorem 5.4, we can also Ô¨Ånd a constant T2depending onT1, such thatT2T1+q
T1
CW+ 1, and
 E"T2X
t=1hQt 1;~ t~t ~ti#
 E"T1X
t=1kQt 1k1#
+ (KM2+KM )CWE[T2]: (13)
IfT4
CW+CW, we haveT1maxfCW;4
CWg; henceT22T15T. Also, Theorem 5.3 guarantees that
E"T2X
t=1Qt 1;atSt;at hQt 1;~ti#
(K+ 1)M2E[T2]
2: (14)
Combining Eqs. (12) to (14) by simply summing them up, we obtain

2E"T1X
t=1kQt 1k1#
(K+ 1)M2
2+ (KM2+KM )CW+ 3KM2m0+ 6M2
E[T2]
(K+ 1)M2
2+ (KM2+KM )CW+ 3KM2m0+ 6M2
5T:
Thus, when T4
CW+CW, we have
1
TE"TX
t=1kQt 1k1#
1
TE"T1X
t=1kQt 1k1#

3KM2m0+(K+ 1)M2
2+ (KM2+KM )CW+ 6M2
10
;
which completes the proof of Theorem 6.1.
14Queue Scheduling with Adversarial Bandit Learning A P REPRINT
7 Relaxing the Boundedness Assumption for Queue Increments
In this section, we further extend SoftMW andSSMW to settings where the queue lengths increments (individual arrivals
and departures) are not necessarily bounded in a known range, but have bounded moments. Formally, we will replace
the bounded-arrival-and-service assumption in our problem setting by the following new assumption.
Assumption 4 (Queue length increments with bounded moments) .The arrival and service distributions fA1;A2;:::g
andfS1;S2;:::gare supported on RK
+, but there exists constants 2andM > 0, both known to the system
scheduler before-hand, such that
E
A
t;iFt 1
;E
A
t;iFt 1
M
for allt1andi2[K]. As an immediate implication, we also have for all t1andi2[K]that
E[jQt;i Qt 1;ijjFt 1]2M:
Note that while this assumption is often not difÔ¨Åcult in the standard Lyapunov analysis [28], it poses a new challenge
in the learning-augmented control analysis, especially when the algorithm uses UCB bonuses (e.g., [9, 13, 21, 40]),
primarily due to the impact it brings in estimation.3
Below, we present new variants of SoftMW andSSMW that are capable of stabilizing the system under Assumption 4.
We explain the high-level design ideas in Appendix G, and we put detailed queue stability proofs in Appendices H
and I.
Algorithm 4: SoftMW for queue-length increments with bounded moments ( SoftMW+ )
Input: Queue-length increment moment upper-bound parameter M > 0,>14, Number of job types K,
Problem instance smoothness parameter 0<1
2Output: A sequence of job types to serve a1;a2;:::2[K]
1L0 M
2Initialize an extended EXP3.S+ instance.
3fort= 1;2;:::do
4 Pick the following parameters of EXP3.S+ for time slot t:
5t t 4=K
6t=
t (1
4 
2)Lt 1q
86L2
t 1K6t3
2+Pt 1
s=0kQsk2
2 1
7 et=Qt 1=kQt 1k1
8t=Mt
4tkQt 1k1=t
4ML 1
t 1kQt 1k1
t (1
4 
2)q
86L2
t 1K6t3
2+Pt 1
s=0kQsk2
2 1
9 Take a new action decision output atfrom EXP3.S+ , serve theat-th queue, receive feedback St;at
10S0
t,(
St;atifSt;atMt
4
0 otherwise
11 RegardQt 1;atS0
tas a new feedback gt;atand feed it into the current EXP3.S+ instance
12Lt maxfLt 1;kQt Qt 1k1g
SoftMW+ (Algorithm 4) is a generalized version of SoftMW for Assumption 4. Compared to SoftMW (Algorithm 2),
SoftMW+ (Algorithm 4) explicitly tracks Lt(the maximum queue length increment we have encountered up to time
t) and most occurrences of the queue length increment that upper-bound Min Algorithm 2 are replaced by Ltin the
new algorithm. We also increase the explicit exploration rate by t
4times (Line 8) and clip the service feedback before
we feed it into EXP3.S+ (Line 10).
Theorem 7.1 below gives the corresponding average queue length bound on any Ô¨Ånite time-horizon.
Theorem 7.1. For problem instances satisfying Assumptions 1, 2 and 4, SoftMW+ (Algorithm 4) guarantees
1
TE"TX
t=1kQtk1#
2(K+ 1)M2+ 4CW(KM2+KM )
+o(1)
as long as>7. In particular, the system is stable.
3For example, UCB-based estimators usually require the distributions to be sub-Gaussian, which is a much more restricted
assumption compared to our Assumption 4.
15Queue Scheduling with Adversarial Bandit Learning A P REPRINT
Proof. See Appendix H.
We next present SSMW+ (Algorithm 5), the generalized version of SSMW . Compared to SoftMW+ (Algorithm 4), SSMW+
does not require the actual value of in Assumption 4, and can stabilize any problem instance where new arrivals and
service have bounded second moments (any 2in Assumption 4 also implies this). By contrast, Algorithm 4 needs
to knowbeforehand, and requires that the product of andis not too small.
Algorithm 5: SSMW for queue-length increments with bounded moments ( SSMW+ )
Input: Queue-length increment moment upper-bound parameter M > 0, Number of job types K, Problem
instance smoothness parameter >0
Output: A sequence of job types to serve a1;a2;:::2[K]
1while truedo
2T0 the latest time index tat which we have made a new decision at // for the first iteration, we
should have T0= 0
3m maxnlkQT0k1
2Mm
;1o
4 Run a fresh EXP3.S+ instance for mtime steps with the following conÔ¨Åguration (below denotes the time
index within the epoch of length m, 1-based):
5=m 3=K
6 x1can be any element in [K];,fx2[K]:xi8i2[K]g
7=
4M3Km1+2
3 1
8 e=1=K
9=m
3KMkQT0k1=1
4M 2m 1 
3kQT0k1
10 Take a new action decision output atfrom EXP3.S+ , serve theat-th queue, receive feedback ST0+;aT0+
11gT0+,(
QT0+ 1;aT0+ST0+;aT0+ifQT0+ 1;aT0+ST0+;aT0+m
3MQT0;aT0+
0 otherwise
12 RegardgT0+as a new feedback and feed it into the EXP3.S+ instance
Theorem 7.2 gives the corresponding average queue length bound on any Ô¨Ånite time-horizon.
Theorem 7.2. For problem instances satisfying Assumptions 1, 3 and 4, SSMW+ (Algorithm 5) guarantees
1
TE"TX
t=1kQtk1#

3KM2m0+(K+ 1)M2
2+ (KM2+KM )CW+ 4M2
10

for any time horizon length T4
CW+CW. In particular, the system is stable. Here m0is deÔ¨Åned as
m0,infn
m:m2;f(m0)
28m0mo
 
(1 +CV)M2KlnK 1(1=)
where
f(m) = 42(1 +CV)M2Km 
3(3 lnm+ lnK):
Proof. See Appendix I.
8 Related Work
Recent literature includes learning-based scheduling policies that require little prior-knowledge and can gather channel
statistics at run-time.
Learning-based approaches to scheduling queueing systems without perfect channel state knowledge require substan-
tial exploration, to probe for more information of all the channels inside the system instead of merely exploiting the
statistics at hand (e.g., via a MaxWeight style planning). Typical ways to introduce adequate exploration include
epsilon-greedy, which explicitly allocates a small probability to serve each channel unconditionally [30, 20, 21]; here
the exploration is blind to the queue sizes and historical channel statistics and thus almost decoupled from exploitation.
By contrast, optimistic exploration works by adding bonus terms to current channel statistics, so that exploration and
16Queue Scheduling with Adversarial Bandit Learning A P REPRINT
exploitation are naturally coupled during scheduling [9, 21, 35, 40]. Upper conÔ¨Ådence bound (UCB) [2] is a classical
method for designing a bonus term.
Existing works on scheduling in non-stationary queueing systems include [40], which uses discounted UCB estimators
for an up-to-date service rate of each link to replace the actual mean services rate in classical MaxWeight . The resulting
policy can stabilize problem instances where the difference of each link‚Äôs arrival (and service) rates between any two
time steps in any time window of length Wis sufÔ¨Åciently small, and this window length Wneeds to match with the
discounting factor used in discounted UCB estimators. Compared to [40], our smoothness assumption is on the
reference randomized policies rather than the true service rates.
9 Conclusions and Future Work
In this paper, we propose a novel approach to apply adversarial bandit learning techniques to schedule queueing
systems with unknown, time-varying channel states. The presented new algorithms SoftMW and SSMW are capable
of stabilizing the system whenever the system can be stabilized by some (possibly unknown) sequence of randomized
policies, and their time-variation satisÔ¨Åes some mild condition. We further generalize our results to the setting where
arrivals and departures only have bounded moments and develop two stablizing algorithms SoftMW+ andSSMW+ .
We believe our approach can be generalized to more complex stochastic networks (e.g., multi-hop networks), and to
achieve other tasks such as utility optimization subject to queue stability. It is also an interesting future work to design
distributed network scheduling algorithms using adversarial bandit learning techniques.
References
[1] Mohammad Ashjaei, Lucia Lo Bello, Masoud Daneshtalab, Gaetano Patti, Sergio Saponara, and Saad Mubeen.
2021. Time-Sensitive Networking in automotive embedded systems: State of the art and research opportunities.
Journal of systems architecture 117 (2021), 102137.
[2] Peter Auer. 2002. Using conÔ¨Ådence bounds for exploitation-exploration trade-offs. Journal of Machine Learning
Research 3, Nov (2002), 397‚Äì422.
[3] Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. 2002. The nonstochastic multiarmed
bandit problem. SIAM journal on computing 32, 1 (2002), 48‚Äì77.
[4] Oussama Ben-Ammar, Belgacem Bettayeb, and Alexandre Dolgui. 2019. Optimization of multi-period supply
planning under stochastic lead times and a dynamic demand. International Journal of Production Economics
218 (2019), 106‚Äì117.
[5] Benjamin Berg, Mor Harchol-Balter, Benjamin Moseley, Weina Wang, and Justin Whitehouse. 2020. Optimal
resource allocation for elastic and inelastic jobs. In Proceedings of the 32nd ACM Symposium on Parallelism in
Algorithms and Architectures . 75‚Äì87.
[6] Anton Braverman, Jim G Dai, Xin Liu, and Lei Ying. 2017. Fluid-model-based car routing for modern rideshar-
ing systems. In Proceedings of the 2017 ACM SIGMETRICS/International Conference on Measurement and
Modeling of Computer Systems . 11‚Äì12.
[7] Anton Braverman, Jim G Dai, Xin Liu, and Lei Ying. 2019. Empty-car routing in ridesharing systems. Opera-
tions Research 67, 5 (2019), 1437‚Äì1452.
[8] Jiangong Chen, Feng Qian, and Bin Li. 2022. Enhancing Quality of Experience for Collaborative Virtual Real-
ity with Commodity Mobile Devices. In 2022 IEEE 42nd International Conference on Distributed Computing
Systems (ICDCS) . IEEE, 1018‚Äì1028.
[9] Tuhinangshu Choudhury, Gauri Joshi, Weina Wang, and Sanjay Shakkottai. 2021. Job dispatching policies for
queueing systems with unknown service rates. In Proceedings of the Twenty-second International Symposium on
Theory, Algorithmic Foundations, and Protocol Design for Mobile Networks and Mobile Computing . 181‚Äì190.
[10] Said El Kafhali and Khaled Salah. 2017. Stochastic modelling and analysis of cloud computing data center. In
2017 20th Conference on Innovations in Clouds, Internet and Networks (ICIN) . IEEE, 122‚Äì126.
[11] Anuroop Gaddam, Tim Wilkin, Maia Angelova, and Jyotheesh Gaddam. 2020. Detecting sensor faults, anomalies
and outliers in the internet of things: A survey on the challenges and solutions. Electronics 9, 3 (2020), 511.
[12] Elad Hazan et al. 2016. Introduction to online convex optimization. Foundations and Trends¬Æ in Optimization
2, 3-4 (2016), 157‚Äì325.
17Queue Scheduling with Adversarial Bandit Learning A P REPRINT
[13] Wei-Kang Hsu, Jiaming Xu, Xiaojun Lin, and Mark R Bell. 2022. Integrated online learning and adaptive control
in queueing systems with uncertain payoffs. Operations Research 70, 2 (2022), 1166‚Äì1181.
[14] Shuyan Hu, Xiaojing Chen, Wei Ni, Xin Wang, and Ekram Hossain. 2020. Modeling and analysis of energy har-
vesting and smart grid-powered wireless communication networks: A contemporary survey. IEEE Transactions
on Green Communications and Networking 4, 2 (2020), 461‚Äì496.
[15] Longbo Huang, Scott Moeller, Michael J Neely, and Bhaskar Krishnamachari. 2012. LIFO-backpressure
achieves near-optimal utility-delay tradeoff. IEEE/ACM Transactions On Networking 21, 3 (2012), 831‚Äì844.
[16] Longbo Huang and Michael J Neely. 2011. Utility optimal scheduling in processing networks. Performance
Evaluation 68, 11 (2011), 1002‚Äì1021.
[17] Md Rizwan Khan, Bikramaditya Das, and Bibhuti Bhusan Pati. 2020. Channel estimation strategies for underwa-
ter acoustic (UWA) communication: An overview. Journal of the Franklin Institute 357, 11 (2020), 7229‚Äì7265.
[18] Tae Hyun Kim, Hansol Shin, Kyuhyeong Kwag, and Wook Kim. 2020. A parallel multi-period optimal schedul-
ing algorithm in microgrids with energy storage systems using decomposed inter-temporal constraints. Energy
202 (2020), 117669.
[19] Xiangqi Kong, Ning Lu, and Bin Li. 2019. Optimal scheduling for unmanned aerial vehicle networks with
Ô¨Çow-level dynamics. IEEE Transactions on Mobile Computing 20, 3 (2019), 1186‚Äì1197.
[20] Subhashini Krishnasamy, PT Akhil, Ari Arapostathis, Rajesh Sundaresan, and Sanjay Shakkottai. 2018. Aug-
menting max-weight with explicit learning for wireless scheduling with switching costs. IEEE/ACM Transactions
on Networking 26, 6 (2018), 2501‚Äì2514.
[21] Subhashini Krishnasamy, Rajat Sen, Ramesh Johari, and Sanjay Shakkottai. 2021. Learning unknown service
rates in queues: A multiarmed bandit approach. Operations research 69, 1 (2021), 315‚Äì330.
[22] Qingkai Liang and Eytan Modiano. 2018. Minimizing queue length regret under adversarial network models.
Proceedings of the ACM on Measurement and Analysis of Computing Systems 2, 1 (2018), 1‚Äì32.
[23] Shihuan Liu, Lei Ying, and R Srikant. 2011. Throughput-optimal opportunistic scheduling in the presence of
Ô¨Çow-level dynamics. IEEE/ACM Transactions on Networking 19, 4 (2011), 1057‚Äì1070.
[24] Lingling Lv, Chan Zheng, Lei Zhang, Chun Shan, Zhihong Tian, Xiaojiang Du, and Mohsen Guizani. 2021.
Contract and lyapunov optimization-based load scheduling and energy management for UA V charging stations.
IEEE Transactions on Green Communications and Networking 5, 3 (2021), 1381‚Äì1394.
[25] Siva Theja Maguluri, Rayadurgam Srikant, and Lei Ying. 2012. Stochastic models of load balancing and schedul-
ing in cloud computing clusters. In 2012 Proceedings IEEE Infocom . IEEE, 702‚Äì710.
[26] Michael J Neely. 2008. Order optimal delay for opportunistic scheduling in multi-user wireless uplinks and
downlinks. IEEE/ACM Transactions on Networking 16, 5 (2008), 1188‚Äì1199.
[27] Michael J Neely. 2009. Delay analysis for max weight opportunistic scheduling in wireless systems. IEEE Trans.
Automat. Control 54, 9 (2009), 2137‚Äì2150.
[28] Michael J Neely. 2010. Stochastic network optimization with application to communication and queueing sys-
tems. Synthesis Lectures on Communication Networks 3, 1 (2010), 1‚Äì211.
[29] Michael J Neely. 2012. Delay-based network utility maximization. IEEE/ACM Transactions on Networking 21,
1 (2012), 41‚Äì54.
[30] Michael J Neely, Scott T Rager, and Thomas F La Porta. 2012. Max weight learning algorithms for scheduling
in unknown environments. IEEE Trans. Automat. Control 57, 5 (2012), 1179‚Äì1191.
[31] Konstantinos Psychas and Javad Ghaderi. 2021. A Theory of Auto-Scaling for Resource Reservation in Cloud
Services. ACM SIGMETRICS Performance Evaluation Review 48, 3 (2021), 27‚Äì32.
[32] Mohammad Rahdar, Lizhi Wang, and Guiping Hu. 2018. A tri-level optimization model for inventory control
with uncertain demand and lead time. International Journal of Production Economics 195 (2018), 96‚Äì105.
[33] Ralph Tyrell Rockafellar. 2015. Convex analysis . Princeton university press.
[34] Bilal Sadiq and Gustavo De Veciana. 2009. Throughput optimality of delay-driven MaxWeight scheduler for a
wireless system with Ô¨Çow dynamics. In 2009 47th Annual Allerton Conference on Communication, Control, and
Computing (Allerton) . IEEE, 1097‚Äì1102.
[35] Thomas Stahlbuhk, Brooke Shrader, and Eytan Modiano. 2018. Learning algorithms for scheduling in wireless
networks with unknown channel statistics. In Proceedings of the Eighteenth ACM International Symposium on
Mobile Ad Hoc Networking and Computing . 31‚Äì40.
18Queue Scheduling with Adversarial Bandit Learning A P REPRINT
[36] Leandros Tassiulas and Anthony Ephremides. 1993. Dynamic server allocation to parallel queues with randomly
varying connectivity. IEEE Transactions on Information Theory 39, 2 (1993), 466‚Äì478.
[37] Christos Tsanikidis and Javad Ghaderi. 2021. On the power of randomization for scheduling real-time trafÔ¨Åc in
wireless networks. IEEE/ACM Transactions on Networking 29, 4 (2021), 1703‚Äì1716.
[38] Vagelis Tsibonis, Leonidas Georgiadis, and Leandros Tassiulas. 2003. Exploiting wireless channel state infor-
mation for throughput maximization. In IEEE INFOCOM 2003. Twenty-second Annual Joint Conference of the
IEEE Computer and Communications Societies (IEEE Cat. No. 03CH37428) , V ol. 1. IEEE, 301‚Äì310.
[39] Hua Wei, Chacha Chen, Guanjie Zheng, Kan Wu, Vikash Gayah, Kai Xu, and Zhenhui Li. 2019. Presslight:
Learning max pressure control to coordinate trafÔ¨Åc signals in arterial network. In Proceedings of the 25th ACM
SIGKDD International Conference on Knowledge Discovery & Data Mining . 1290‚Äì1298.
[40] Zixian Yang, R Srikant, and Lei Ying. 2022. MaxWeight With Discounted UCB: A Provably Stable Scheduling
Policy for Nonstationary Multi-Server Systems With Unknown Statistics. arXiv preprint arXiv:2209.01126
(2022).
19Queue Scheduling with Adversarial Bandit Learning A P REPRINT
Supplementary Materials
A Simulation Results 20
B Proofs for EXP3.S+ (Algorithm 1) 23
C On the Implementation of EXP3.S+ (Algorithm 1) 26
D Proofs for General Qurdratic Lyapunov Analysis 27
E Proofs for SoftMW (Algorithm 2) Stability Analysis 30
F Proofs for SSMW (Algorithm 3) Stability Analysis 34
G High-Level Ideas to Handle Queue Length Increments with Bounded Moments 36
H Detailed Analysis for Algorithm 4 40
I Detailed Analysis for Algorithm 5 48
A Simulation Results
In this section, we evaluate performance of the presented algorithms SoftMW (Algorithm 2) and SSMW (Algorithm 3)
based on synthetic problem instances. In our experiments, there are K= 5queues, the arrival and departure processes
of each queue are all Bernoulli. SpeciÔ¨Åcally, we assign the arrival rates and departure rates as follows:
‚Ä¢ The arrival processes of each queue are Ô¨Åxed Bernoulli distributions, i.e., ~t=~(0)for allt1for some Ô¨Åxed
~(0)2[0;1]K.
‚Ä¢ We set the Bernoulli service rates for each queue to
~ t= minfmaxf~ (0)+~t;0g;1g,
where~ (0)2[0;1]Kis a Ô¨Åxed vector, and ~tis generated from some K-dimensional stochastic process such that
E[~t] =0, i.e., they are marginally zero-mean.
Therefore, each considered problem instance can be regarded as the superposition of a stationary Bernoulli arrival and
service process pair and a 0-mean noise process on the service.
The Pre-noising Stationary Problem
In the experiments, we consider the following stationary problem instance:
~= (0:25;0:2;0:15;0:1;0:05);
~ = (0:9;0:85;0:8;0:59;0:39): (15)
For this stationary problem, we solve the following linear programming problem
max
~2RK; (16)
s.t.~2[K]
+iii8i2[K]
to get~, referred to as the LP-based randomized policy (see e.g., [28]) and , the distance from the problem instance
to the boundary of the capacity region. The solution to Eq. (16) is
~= (0:27802;0:23556;0:18778;0:16987;0:12877);
= 2:220710 4:
20Queue Scheduling with Adversarial Bandit Learning A P REPRINT
Figure 3: Generated service noise sequence for Queue 1
The Noise Process
We use the following AR(1)autoregressive process to generate the noise to be added to the stationary service rates:
t;i=0 t= 0
0:999t 1;i+N(0;0:0052)t1; (17)
for allt1andi2[K]. i.e., the noise value at time tis the noise at t 1times a discounting factor 0:999, then add
an independently sampled 0-mean normal-distributed random variable.
For this noise mechanism, we generate one trajectory of f~t: 0t2106g(i.e., generate K= 5i.i.d. samples of
the1-dimensional AR(1) process deÔ¨Åned in Eq. (17)). We save this trajectory, and use it in the subsequent repeated
queueing simulations. In other words, when we simulate the system repeatedly, the noise vectors ~t‚Äôs will notbe
resampled, hence the post-noising service rates ~ t‚Äôs are Ô¨Åxed before-hand, only the Bernoulli trial outcomes will be
resampled in each new run. In Figure 3, we plot the Ô¨Årst 50000 items of the generated service noise for the Ô¨Årst queue.
Algorithms Evaluated
In the experiment, we evaluate the following algorithm instances:
‚Ä¢ (Baseline) The MaxWeight algorithm, taking the pre-noising service rate vector ~ (0)as input parameters. It sched-
ules the system according to
at= arg maxi2[K]n
Qt 1;i(0)
io
;
‚Ä¢ (Oracle baseline when there are noises) The MaxWeight algorithm, taking all post-noising service rate vectors
f~ t: 1tTgas as input parameters, referred to as MaxWeightGT . It schedules the system according to
at= arg maxi2[K]fQt 1;it;ig;
‚Ä¢ (Baseline) The LP-based randomized policy (referred to as Randomized ). It schedules the system by sampling at
according to the probability vector ~independently at each time step t.
‚Ä¢ (Ours) SoftMW (Algorithm 2) with parameters M= 1,= 0:5(referred to as SoftMW-0.5 );
‚Ä¢ (Ours) SoftMW (Algorithm 2) with parameters M= 1,= 0:1(referred to as SoftMW-0.1 );
‚Ä¢ (Ours) SoftMW (Algorithm 2) with parameters M= 1,= 0(referred to as SoftMW-0 );
‚Ä¢ (Ours) SSMW (Algorithm 3) with parameters M= 1,= 0:5(referred to as SSMW-0.5 );
‚Ä¢ (Ours) SSMW (Algorithm 3) with parameters M= 1,= 0:1(referred to as SSMW-0.1 );
‚Ä¢ (Ours) SSMW (Algorithm 3) with parameters M= 1,= 0(referred to as SSMW-0 ).
21Queue Scheduling with Adversarial Bandit Learning A P REPRINT
Remark. Our main theoretical results, Theorems 5.2 and 6.1, cannot give queue stability guarantee for both algo-
rithms running with = 0. Nevertheless, with = 0, both algorithms are proper and feasible queueing policies.
Therefore, we also include them in the list of algorithm instances to evaluate, to illustrate the limit behavior of SoftMW
andSSMW whenis sufÔ¨Åciently small.
When we implement SSMW , we take the xvalue of the last step in the previous EXP3.S+ epoch as the initial mixed
action x1in the new EXP3.S+ instance.
Simulation Results
We do the numerical evaluation on two problem instances, both based on the stationary problem speciÔ¨Åed in Eq. (15).
In one instance, we set all service noise vectors ~ts to0; in the other instance, we set ~taccording to Eq. (17). In both
problem instances, we simulate for T= 2106time steps.
In Figure 4, we plot the simulation results for the two problem instances, on which there are 8and9algorithm instances
being evaluated (in the no-noise problem MaxWeightGT andMaxWeight are identical), the curve for each algorithm
is obtained by taking average over 20independent simulations. Below, we brieÔ¨Çy summarize the simulation results.
‚Ä¢ In the no-noise problem instance (Figure 4a), MaxWeight leads to very small average queue lengths, surpassing other
evaluated algorithms. This is not suprising, since MaxWeight is known to perform well given the precise channel
condition. It is worth noting that our algorithms also stablize the network, despite not having such information.
‚Ä¢ In the noisy problem instance (Figure 4b), MaxWeight experiences signiÔ¨Åcant performance degrade, and the system
seems unstable. While the oracle baseline MaxWeightGT still gives rather small queue lengths. Our SoftMW and
SSMW algorithms all perform better than Randomized . In particular, SSMW-0.1 andSSMW-0 lead to average queue
lengths close to MaxWeightGT .
(a) The Problem without noise. Here MaxWeight has access to the accurate channel information.
22Queue Scheduling with Adversarial Bandit Learning A P REPRINT
(b) The Problem with AR(1) noise. Here MaxWeightGT has access to the accurate channel information, MaxWeight only has access
to imperfect channel information that does not count in noises.
Figure 4: Total Queue Lengths Plot
B Proofs for EXP3.S+ (Algorithm 1)
Unless stated otherwise, for any strictly convex function f, we usefto denote its restriction on some 4[K];, where
should be inferred from the context. SpeciÔ¨Åcally,
f(x),f(x);x24[K];
1;x=24[K];:
We list some important properties of Legendre functions and Bregman divergences below. The proof can be found in
many literature on convex analysis, e.g., [33].
Lemma B.1. LetCRnbe a convex set, f:C!Rbe a Legendre function. Then,
1.rfis a bijection between int(C)andint(dom(f))with the inverse (rf) 1=rf.
2.Df(y;x) =Df(rf(x);rf(y))for all x;y2int(C).
3. The convex conjugate fis Legendre.
4. (Generalized Pythagorean Theorem) Let Wbe a closed convex subset of C, for any x2Cdenote by f(x;W),
arg minx02WDf(x0;x). The minimizer is guaranteed to exist uniquely, and for any y2Wwe haveDf(y;x)
Df(y;f(x;W)) +Df(f(x;W);x).
Below is a technical lemma relating the single-step regret to single-step OMD update.
Lemma B.2. For any2[0;1=K],>0,x;y24[K];,g2RK
+and Legendre function 	 :RK
+!R, we have
hg;y xi 1D	(y;x)  1D	(y;z) + 1D	(x;ez) (18)
where
z= arg min
x024[K];h g;x0i+D	(x0;x);ez= arg min
x02RK
+h g;x0i+D	(x0;x); (19)
or equivalently,
ez=r	(r	(x) +g) (20)
Proof. We Ô¨Årst prove that Eq. (19) and Eq. (20) are equivalent. 	is Legendre hence r	explodes on @RK
+, and
the minimizer ezdeÔ¨Åned by Eq. (19) will lie in int(RK
+), furthermore, we must have@
@ez0[h g;ezi+D	(ez;x)] =
23Queue Scheduling with Adversarial Bandit Learning A P REPRINT
 g+r	(ez) r	(x) =0, thusr	(ez) =r	(x) +g. Then the bijection property in Theorem B.1 states that
ez=r	(r	(ez)) =r	(r	(x) +g), which is just Eq. (20).
We know from Eq. (20) that
g= 1(r	(ez) r	(x)) (21)
The Ô¨Årst order optimality condition of ezin Eq. (19) implies that h g+r	(z) r	(x);y zi 0for any
y24[K];, thus
hg;y zi 1hr	(z) r	(x);y zi (22)
Therefore, we can write
hg;y xi=hg;y zi+hg;z xi
(a)
 1hr	(z) r	(x);y zi+ 1hr	(ez) r	(x);z xi
(b)= 1(D	(y;z) +D	(z;x) D	(y;x)) + 1(D	(z;x) +D	(x;ez) D	(z;ez))
= 1(D	(y;x) D	(y;z) +D	(x;ez) D	(z;ez))
 1(D	(y;x) D	(y;z) +D	(x;ez)):
Here in step (a), we plug in Eq. (22) for the Ô¨Årst term and Eq. (21) for the second term, in step (b), we use the
following ‚Äúthree-point identity‚Äù of Bregman divergences:
D	(a;b) +D	(b;c) D	(a;c) =hr	(c) r	(b);a bi;
which can be veriÔ¨Åed by expanding all Bregman divergences.
Intuitively, Theorem B.2 gives an upper-bound in Bregman divergences for the sample-path single step regret of an
mixed action xagainst a base-line mixed action y, where the reward vector is g. Our high-level idea towards Theo-
rem 5.1 is to sum up the bound in Theorem B.2 and then appropriately take expectations and control the expectations.
Firstly, we can get a sample-path bound for the total dynamic regret.
Lemma B.3. For any Ô¨Åxed sequence ~12[K];1;:::;~T2[K];T, Algorithm 1 guarantees that
TX
t=1hegt;~t xti 
1 +T 1X
t=1k~t+1 ~tk1!
 1
Tln1
T  1
TD	(~T;zT) +TX
t=1 1
tD	(xt;ezt) (23)
where 	(x),PK
i=1(xilnxi xi),ezt,r	(r	(xt) +tegt),zt,arg minx024[K];th tegt;x0i+D	(x0;xt).
Proof. We will prove Eq. (23) by induction on T. Eq. (23) trivially holds when T= 0 (here we use the convention
that 1
0= 0). Then it sufÔ¨Åce to verify Theorem B.3, while assuming that
T 1X
t=1hegt;~t xti 
1 +T 2X
t=1k~t+1 ~tk1!
 1
T 1ln1
T 1  1
T 1D	(~T 1;zT 1) +T 1X
t=1 1
tD	(xt;ezt):
In Algorithm 1, the 	function is chosen to the negative entropy, and we have
D	(y;x) =KX
t=1yilnyi
xi: (24)
Hence for any y2[K];x2[K];, sincexifor alli2[K], we have
D	(y;x)KX
i=1yiln1
xiln1
: (25)
Note that the choice of xtin Algorithm 1 is just zt 1, hence we can write
 1
TD	(~T;xT)
= 1
TD	(~T;zT 1)
24Queue Scheduling with Adversarial Bandit Learning A P REPRINT
= 1
T 1D	(~T 1;zT 1) + 1
T 1
D	(~T;zT 1) D	(~T 1;zT 1)
+ ( 1
T  1
T 1)D	(~T;zT 1)
(a)
 1
T 1D	(~T 1;zT 1) + 1
T 1KX
t=1
T;ilnT;i
zT 1;i T 1;ilnT 1;i
zT 1;i
+ ( 1
T  1
T 1) ln1
T 1
(b)
 1
T 1D	(~T 1;zT 1) + 1
T 1KX
i=1
(1 zT 1;i) ln1
zT 1;ijT;i T 1;ij
+ ( 1
T  1
T 1) ln1

 1
T 1D	(~T 1;zT 1) + 1
T 1k~T ~T 1k1ln1
T 1+ ( 1
T  1
T 1) ln1
T 1;
where in step (a)we plug in Eq. (24) for the second term and Eq. (25) for the third term, in step (b)we use the
following fact: let f(x) =xlnx
y, thenf0(x) = (1 y) lnx
y(1 y) ln1
yas long asx;y2(0;1).
Then, we can apply Theorem B.2 for the T-th time step and write
hegT;~T xTi 1
TD	(~T;xT)  1
TD	(~T;zT) + 1
TD	(xT;ezT)
 1
T 1D	(~T 1;zT 1)  1
TD	(~T;zT) + 1
TD	(xT;ezT)
+ 1
T 1k~T ~T 1k1ln1
T 1+ ( 1
T  1
T 1) ln1
T 1;
which is adequate for the induction step, recalling that t‚Äôs andt‚Äôs are both decreasing.
Lemma B.4. For each term  1
tD	(xt;ezt)deÔ¨Åned in Theorem B.3, we have
E
 1
tD	(xt;ezt)Ft 1
etkgtk2
2:
Proof. In fact, for any choice of 	, we have
 1
tD	(xt;ezt)(a)= 1
tD	(r	(ezt);r	(xt))
= 1
tD	(r	(xt) +tegt;r	(xt))
= 1
t(	(r	(xt) +tegt) 	(r	(xt)) hxt;tegti)
(b)=t
2kegtk2
r2	(wt); (26)
where in step (a)we use the duality property in Theorem B.1, in step (b)we regard the Bregman divergence as a second
order Lagrange remainder, wtis some element inside the line segment connecting r	(xt) +tegtandr	(xt). i.e.,
wt;i=r	(xt)ifor alli6=at,wt;at2[r	(xt)at;r	(xt)at+tgt;at
pt;at]. Since we have assumed that  1
ttet;igt;i
for alli, and we use an explicit exploration mechanism to guarantee that pt;i= (1 t)xt;i+tet;itet;i, hence
now we have tgt;at
pt;at1.
Now for the particular choice of 	being the negative entropy function, we have r	(x) = (lnx1;:::; lnxK)and
r2	(x) = Diag(x 1
1;:::;x 1
K). Using the property (1) in Theorem B.1 we can see that
r2	(wt) = 
r2	(r	(wt)) 1
= Diag(exp( wt;1) 1;:::; exp(wt;K) 1) 1
= Diag(exp( wt;1);:::; exp(wt;K));
therefore, we have
Diag(xt;1;:::;xt;K)r2	(wt)eDiag(xt;1;:::;xt;K) (27)
We can then plug Eq. (27) into Eq. (26) to get
 1
tD	(xt;ezt)et
2eg2
t;atxt;at
=et
2g2
t;at
p2
t;atxt;at:
25Queue Scheduling with Adversarial Bandit Learning A P REPRINT
Taking expectation, we can see
E
 1
tD	(xt;ezt)Ft 1
et
2KX
i=1g2
t;ixt;i
pt;i
etKX
i=1g2
t;i
where the last step is due to t1
2, hencept;i(1 i)xt;i1
2xt;i.
Now we are ready to prove Theorem 5.1.
Proof of Theorem 5.1. Take expectaion on both sides of Eq. (23) in Theorem B.3, note that
Eh
hegt;~t xtiFt 1i
=hgt;~t xti;
and
E[gt;atjFt 1] =hgt;pti
=hgt;xti+hgt;pt xti
=hgt;xti+hgt; txt+teti
hgt;xti+thgt;eti:
Then apply Theorem B.4.
C On the Implementation of EXP3.S+ (Algorithm 1)
The core operation when implementing our EXP3.S+ (Algorithm 1) is the arg min expression in Line 8. In other
words, we need to solve the following optimization problem:
min
y2RK hg;yi+D	(y;x) (28)
s.t.yi81iK
KX
i=1yi= 1
where g2RK
+andx2[K]are two Ô¨Åxed vectors, 0 <1
Kis a constant. For this optimization problem, we can
introduce a set of Lagrange multipliers 1;:::;K;and the Lagrangian
L(y;~;), KX
i=1giyi+D	(y;x) +KX
i=1i(yi ) + KX
i=1yi 1!
:
Lety2[K];be an optimizer for the optimization problem Eq. (28), and let ~;be the corresponding multipli-
ers. Then we can write down the KKT condition for y:
@L
@yi
y;~;= gi+ ln(y
i) ln(xi) +
i+= 081iK;
y
i>)
i= 081iK:
Denote byI,fi2[K] :y
i>g, then the KKT conditions implies that
ln(y
i) ln(xi) gi= ln(y
j) ln(xj) gj (29)
for alli;j2I. On the other hand, we know that
X
i2Iy
i= 1 (K jIj ) (30)
26Queue Scheduling with Adversarial Bandit Learning A P REPRINT
since we have assumed that y
i=for alli2[K]nI. In fact, Eqs. (29) and (30) have an explicit ‚Äúsoft-max‚Äù solution
y
i/xiexp(gi)
and thus
y
i= [1 (K jIj )]xiexp(gi)P
j2Ixjexp(gj)
for alli2I. Therefore, it remains to determine Ito complete depict y. Note that the optimization problem Eq. (28)
satisÔ¨Åes Slater‚Äôs condition, hence its optimizers are completely determined by the KKT conditions. One can verify
that the following problem
min
y2RKD	(y;x0) (31)
s.t.yi81iK
KX
i=1yi= 1
wherex0
i=xiexp(gi), has the exactly same KKT conditions as Eq. (28). Furthermore, without loss of generality, we
assume that x0
1x0
2x0
Kin Eq. (31) (otherwise we can permute and rearrange the coordinates of xsince
bothD	(;)and the constraints are symmetric in all coordinates). Then, we can claim that y
1y
2y
K, i.e.,
the coordinates of yare also in a sorted order. The reason is, we can expand the objective of Eq. (31) as:
OBJ = 	(y) 	(x0) hr 	(x0);y x0i
=KX
i=1yiln(yi) KX
i=1x0
iln(x0
i) KX
i=1ln(x0
i)(yi x0
i) 1 +kx0k1
=KX
i=1yi( ln(x0
i)) + some permutation-invariant quantity of y
Then, one can see if yhas a pair of coordinates i < j butyi> yj, then swapping yiandyjmakes the objective
function smaller. Therefore, after reducing the optimization problem to Eq. (31) with ascending x0
i‚Äôs, there exists
0iKsuch that=y
1==y
i= <y
i+1y
i+2y
K. To solve Eq. (31), it sufÔ¨Åces to enumerate
this boundary index i= 0:::K , then calculate y
i+1;:::;y
Kby
y
j= (1 i)x0
jPK
k=i+1x0
k:
Ify
i+1, we can accept the current ias the optimal boundary index, and conclude that (;:::;;y
i+1;:::;y
K)is
the optimizer of Eq. (31).
D Proofs for General Qurdratic Lyapunov Analysis
Proof of Theorem 5.3. Recall that in this paper we denote by Qtthe queue lengths at the end of t-th time slot, by At,
Stthe arrivals and (maximum) services respectively at time t. DeÔ¨Åne
Ut= maxf Qt 1 At+St1at;0g
to be the unused services at time t, then it is easy to see we have
Qt=Qt 1+At St1at+Ut (32)
and
hQt;Uti= 0; (33)
hQt 1+At St1at;Uti0 (34)
for allt1.
Using Eq. (32) to express Lt, we can see
Lt Lt 1=1
2kQt 1+At St1at+Utk2
2 1
2kQt 1k2
2
27Queue Scheduling with Adversarial Bandit Learning A P REPRINT
=1
2kQt 1+At St1atk2
2+1
2kUtk2
2+hQt 1+At St1at;Uti 1
2kQt 1k2
2
(a)
1
2kQt 1+At St1atk2
2+1
2kUtk2
2 1
2kQt 1k2
2
=1
2kAt St1atk2
2+1
2kUtk2
2+hQt 1;At St1ati
(b)
(K+ 1)M2
2+hQt 1;At St1ati;
where step (a)is due to equation Eq. (34), (b)is due tokAt St1atk1;kUtk1M, andUtcan have at most
one positive entry. Thus
E[Lt Lt 1jFt 1](K+ 1)M2
2+E[hQt 1;At St1atijFt 1]
(K+ 1)M2
2+hQt 1;~t ~ tpti;
where the last step is because Qt 1isFt 1-measurable, and At;Stare both independent to atandFt 1,P[at=
ijFt 1] =pt;i.
Proof of Theorem 5.4. For each interval of time steps Wjin Assumption 1, denote by T0the index of the Ô¨Årst time
slot inWj, then we can write
X
t2WjhQt 1;~t~ t ~ti=X
t2WjhQT0 1;~t~ t ~ti+X
t2WjhQt 1 QT0 1;~t~ t ~ti;
then bound the two sums one by one. First the sum with QT0 1factors, we have
X
t2WjhQT0 1;~t~ t ~ti(a)
jWjjkQT0 1k1
X
t2Wj(kQt 1k1 kQt 1 QT0 1k1)
(b)
X
t2Wj(kQt 1k1 KM(t T0))
X
t2WjkQt 1k1 KM (jWjj 1)2
where step (a)is due to Eq. (2) in Assumption 1, (b)is because the queue length increments are bounded by M.
As for the sum with Qt 1 QT0 1, we have
X
t2WjhQt 1 QT0 1;~t~ t ~ti X
t2WjkQt 1 QT0 1k1k~t~ t ~tk1
 X
t2WjKM(t T0)M
 KM2(jWjj 1)2:
Sum up the two parts, we get
X
t2WjhQt 1;~t~ t ~tiX
t2WjkQt 1k1+ (KM2+KM ) (jWjj 1)2: (35)
For any given time horizon T1, we can Ô¨Ånd a smallest nsuch thatSn
j=0Wj[T], then the left most endpoint of
Wnis smaller than T. According to Eq. (1) in Assumption 1, we have (jWnj 1)2CWT, hencejWnjq
T
CW+1.
LetTT,mint2Wnbe the rightmost endpoint of Wn, thenW0;:::;Wnis a partition of [TT], and we have
TTT+jWnjT+r
T
CW+ 1:
28Queue Scheduling with Adversarial Bandit Learning A P REPRINT
Summing Eq. (35) over j= 0:::n , we get
TTX
t=1hQt 1;~t~ t ~tiTTX
t=1kQt 1k1+ (KM2+KM )nX
j=0(jWjj 1)2
(a)
TTX
t=1kQt 1k1+ (KM2+KM )CWTT
TX
t=1kQt 1k1+ (KM2+KM )CWTT
where in (a)we applied Eq. (1).
Proof of Theorem 5.5. If sufÔ¨Åces to note that when Tmaxf4
CW;CWg, we haveq
T
CWq
CW
CW= 1. Also, we
haveT24T
CWhenceT2q
T
CW. Therefore, the constant TTin Theorem 5.4 satisÔ¨Åes
TTT+r
T
CW+ 1
T+ 2r
T
CW
2T:
Then, applying Theorem 5.3 with the time horizon length TT, we get
E"TTX
t=1Qt 1;atSt;at hQt 1;~ti#
(K+ 1)M2TT
2: (36)
By rearranging the terms in the bound given by Theorem 5.4, we get
E"TX
t=1kQt 1k1#
E"TTX
t=1hQt 1;~ t~t ~ti#
+ (KM2+KM )CWTT
=E"TTX
t=1Qt 1;atSt;at hQt 1;~ti#
+ (KM2+KM )CWTT
 E"TTX
t=1hQt 1;~ t~ti Qt 1;atSt;at#
E"TTX
t=1Qt 1;atSt;at hQt 1;~ti#
+ (KM2+KM )CWTT+f(TT); (37)
where the last inequality is due to the assumption that EhPT
t=1Qt 1;atSt;ati
EhPT
t=1hQt 1;~ t~tii
 f(T).
Plugging Eq. (36) into Eq. (37), we can see that
E"TX
t=1kQt 1k1#
(K+ 1)M2TT
2+ (KM2+KM )CWTT+f(TT)
and thus
1
TE"TX
t=1kQtk1#
(K+ 1)M2+ 2CW(KM2+KM )
2TT
T+f(TT)
T
(K+ 1)M2+ 2CW(KM2+KM )
+f(2T)
T
where the last step is due to TT2Tand the assumption that f()is increasing.
29Queue Scheduling with Adversarial Bandit Learning A P REPRINT
E Proofs for SoftMW (Algorithm 2) Stability Analysis
Here is a lemma similar to Theorem 5.9 but in the opposite direction.
Lemma E.1. Supposex1= 0,x2;:::;xn0,jxi+1 xij1for all 1i<n , then we have
nX
i=1x2
i1
3x3
n:
Proof. For0idxne, we havexn ixn i0, thus
nX
i=1x2
idxneX
i=0x2
n idxneX
i=0(xn i)2
=1
6xn(xn+ 1)(2xn+ 1) 1
6fxng(fxng+ 1)(2fxng+ 1)
1
3x3
n
wherefxng,xn dxne.
Corollary E.2. LetfQtgbe queue lengths where Q0=0, and the increments are bounded by M, then
T 1X
t=0kQtk2
21
3MK3kQT 1k3
1
for anyT1.
Proof. Leti2arg maxi2[K]QT 1;i, then
Q0;i
M;Q1;i
M;:::;QT 1;i
M
is a non-negative sequence starting from 0with differences bounded by 1. Applying Theorem E.1 to this sequence,
we can see
T 1X
t=0kQtk2
2M2T 1X
t=0Qt;i
M2
M2
3QT 1;i
M3
=1
3MQ3
T 1;i
1
3MkQT 1k1
K3
=1
3MK3kQT 1k3
1
as claimed.
Proof of Theorem 5.6. It sufÔ¨Åces to verify that at each time step
2kQt 1k1t (1
4 
2)vuut86M2K6t3
2+t 1X
s=0kQsk2
2;
m
4kQt 1k2
1t 1
2+ 
86M2K6t3
2+t 1X
s=0kQsk2
2!
;
30Queue Scheduling with Adversarial Bandit Learning A P REPRINT
*(a)
4kQt 1k2
1t 1
2+
86M2K6t3
2+1
3MK3kQt 1k3
1
m
4kQt 1k2
1t 1
2+1
3258M2K6t3
2+2
31
2MK3kQt 1k3
1
;
*(b)
4kQt 1k2
1t 1
2+2581
32 2
3t1
2kQt 1k2
1
m
4t258
41
3
;
and the last statement trivially holds. Here in step (a)we apply Theorem E.2, in step (b)we apply AM-GM inequality
1
3x+2
3yx1
3y2
3.
Proof of Theorem 5.7. For each~t, we can Ô¨Ånd a ~0
t2[K];tso thatk~t ~0
tk12Ktandk~0
s ~0
tk1k~s ~tk1.
For example, we can choose
~0
t= (1 t)~t+t1:
Then, we can write
hQt 1;St~ti=hQt 1;St~0
ti+hQt 1St;~t ~0
ti
hQt 1;St~0
ti+kQt 1Stk1k~t ~0
tk1
hQt 1;St~0
ti+M2t2Kt
=hQt 1;St~0
ti+ 2M2t 2:
Then, one can see the quantity
TX
t=1Eh
hQt 1;St~0
ti Qt 1;atSt;ati
=TX
t=1Eh
hQt 1St;~0
ti (Qt 1St)ati
satisÔ¨Åes the condition to apply Theorem 5.1. Theorem 5.1 asserts that
TX
t=1Eh
hQt 1;St~0
ti Qt 1;atSt;ati
 
1 +T 1X
t=1k~0
t+1 ~0
tk1!
E
 1
Tln1
T
+eE"TX
t=1tkQt 1Stk2
2#
+E"TX
t=1t
Qt 1St;Qt 1
kQt 1k1#
: (38)
Below, we bound each term in the RHS of Eq. (38). Firstly, we havePT 1
t=1k~0
t+1 ~0
tk1PT 1
t=1k~t+1 ~tk1
CVT1
2 according to Assumption 2, hence
 
1 +T 1X
t=1k~0
t+1 ~0
tk1!
E
 1
Tln1
T

1 +CVT1
2 
E
 1
Tln1
T

1 +CVT1
2 
(3 lnT+ lnK)E2
4T (1
4 
2)Mvuut86M2K6T3
2+T 1X
s=0kQsk2
23
5
31Queue Scheduling with Adversarial Bandit Learning A P REPRINT
(1 +CV)T1
4 
2(3 lnT+ lnK)ME2
4vuut86M2K6T3
2+T 1X
s=0kQsk2
23
5:
For the second term, we have
TX
t=1tkQt 1Stk2
2
M2TX
t=1tkQt 1k2
2
=MTX
t=1t1
4 
20
@vuut86M2K6t3
2+t 1X
s=0kQsk2
21
A 1
kQt 1k2
2
MTX
t=1t1
4 
20
@vuut1 +t 1X
s=0kQsk2
21
A 1
kQt 1k2
2
MT1
4 
2TX
t=10
@vuut1 +t 1X
s=0kQsk2
21
A 1
kQt 1k2
2
2MT1
4 
2vuut1 +T 1X
s=0kQsk2
2;
where in the last step, we use the fact that
nX
i=1xiq
1 +Pi
j=1xj2vuut1 +nX
i=1xi
for non-negative x1:::;xn.
For the third term, we have
TX
t=1t
Qt 1St;Qt 1
kQt 1k1
MTX
t=1tkQt 1k 1
1kQt 1k2
2
=MTX
t=1t1
4 
20
@vuut1 +t 1X
s=0kQsk2
21
A 1
kQt 1k2
2
then it can be bounded in the way exactly same as the second term, hence it is also no more than
2MT1
4 
2vuut1 +T 1X
s=0kQsk2
2:
Combining everything together then taking expectation, we can see that
TX
t=1Eh
hQt 1;St~0
ti Qt 1;atSt;ati
T1
4 
2ME2
4vuut86M2K6T3
2+T 1X
s=0kQsk2
23
5f(1 +CV) (3 lnT+ lnK) + 2e+ 2g+ 2M22
6
32Queue Scheduling with Adversarial Bandit Learning A P REPRINT
(2e+ 3)(1 +CV)T1
4 
2(3 lnT+ lnK)ME2
4vuut86M2K6T3
2+T 1X
s=0kQsk2
23
5+ 4M2
9(1 +CV)T1
4 
2(3 lnT+ lnK)ME2
4vuut86M2K6T3
2+T 1X
s=0kQsk2
23
5+ 4M2:
Proof of Theorem 5.9. Sortx1;:::;xnin acsending order to y1y2yn. We then claim that y1;:::;ynis
also a sequence of non-negative numbers with increments bounded by 1, i.e.,jyi+1 yij1for all 1i<n .
Let: [n]![n]be a permutation over [n]such thatyi=x(i)for any 1in. For any 1i<n , without loss
of generality, assume (i+ 1)>(i), let
j,min
k:(i)<k(i+ 1);xkx(i)	
:
Since we have assumed (i+ 1)>(i),jis guaranteed to be properly deÔ¨Åned. According to the deÔ¨Ånition of jand
the fact thatfyigis a sorted list offxig, we have
xj 1(a)
x(i)=yiyi+1=x(i+1)(b)
xj;
where the inequality (a)is due to the deÔ¨Ånition of j(jis the index of the Ô¨Årst element in fxigafterx(i)that is above
yi), and (b)is due toxjyi, andyi+1is the smallest element among the numbers above yi. Therefore,
jyi+1 yij=yi+1 yixj xj 1jxj xj 1j1:
The case where (i+ 1)<(i)can be similarly veriÔ¨Åed.
DeÔ¨Åne
s,dyne=
max
1inxi
;
then we have yn iyn i0for all 0i<s , hence
S=nX
i=1yis 1X
i=0yn is 1X
i=0(yn i) =s(2yn s+ 1)
2y2
n
2;
where the last inequality is due to ynsyn+ 1, hence 2yn s+ 1yn. Thereforeynp
2S. Then, it is easy
to conclude that
nX
i=1x2
iynnX
i=1xip
2SS=p
2S3
2:
Proof of Theorem 5.10. Letz(x),y(x)1
4, then we have
z(x)4f(x) +z(x)g(x)
for allx0. Note that for any Ô¨Åxed choice of x0, the function
h(z),z4 g(x)z f(x)
is increasing on
g(x)
41
3;1
. In order to prove Theorem 5.10, it sufÔ¨Åces to show that the particular choice of
z=f(x)1
4+g(x)guaranteesz
g(x)
41
3andh(z)0. The condition z
g(x)
41
3. First, the condition
z
g(x)
41
3holds trivially since we have assumed that f(x)andg(x)are both no less than 1. Then, we can do a
direct calculation
z4=
f(x)1
4+g(x)4
33Queue Scheduling with Adversarial Bandit Learning A P REPRINT
=f(x) + 4f(x)3
4g(x) + 6f(x)1
2g(x)2+ 4f(x)1
4g(x)3+g(x)4;
thus,
h(z) =z4 g(x)
f(x)1
4+g(x)
 f(x)
=
4f(x)3
4g(x) f(x)1
4g(x)
+
6f(x)1
2g(x)2 g(x)2
+ 4f(x)1
4g(x)3+g(x)4
where we can see all terms are non-negative since f(x);g(x)1.
F Proofs for SSMW (Algorithm 3) Stability Analysis
Proof of Theorem 6.2. Similar to the proof of Theorem 5.7, it sufÔ¨Åces to apply Theorem 5.1, but we need to verify
that all‚Äôs are no more than1
2Ô¨Årst.
Our choice of mguarantees that mkQT0k1
2M, hencekQT0k12Mm , and for any inside this epoch, we have
kQT0+k1kQT0k1+M3Mm . Therefore
=1
6K 1M 1m 1 
2kQT0+ 1k1
1
6K 1M 1m 1 
23KMm
=1
2m 
2
1
2:
For each~T0+, we can Ô¨Ånd a ~0
T0+2[K];so thatk~T0+ ~0
T0+k12Kandk~0
s ~0
tk1k~T0+s ~T0+tk1
for any 1s;tm. For example, we can choose
~0
T0+= (1 )~T0++1:
Then, we can write
hQT0+ 1;ST0+~T0+i=hQT0+ 1;ST0+~0
T0+i+hQT0+ 1ST0+;~T0+ ~0
T0+i
hQT0+ 1;ST0+~0
T0+i+kQT0+ 1ST0+k1k~T0+ ~0
T0+k1
hQT0+ 1;ST0+~0
T0+i+ 3MmM2K
=hQT0+ 1;ST0+~0
T0+i+ 6M2m 1:
Then, one can see the quantity
mX
=1Eh
hQT0+ 1;ST0+~0
T0+i QT0+ 1;aT0+ST0+;aT0+FT0i
=mX
=1Eh
hQT0+ 1ST0+;~0
T0+i (QT0+ 1ST0+)aT0+FT0i
satisÔ¨Åes the condition to apply Theorem 5.1. Theorem 5.1 asserts that
mX
=1Eh
hQT0+ 1;ST0+~0
T0+i QT0+ 1;aT0+ST0+;aT0+FT0i
 
1 +m 1X
=1k~0
T0++1 ~0
T0+k1!
E
 1
mln1
mFT0
+eE"mX
=1kQT0+ 1ST0+k2
2FT0#
+E"mX
=1
QT0+ 1ST0+;QT0+ 1
kQT0+ 1k1FT0#
34Queue Scheduling with Adversarial Bandit Learning A P REPRINT
 
1 +m 1X
=1k~0
T0++1 ~0
T0+k1!
6M2Km1+
2(2 lnm+ lnK)
+ (e+ 1)1
6K 1m 1 
2E"mX
=1kQT0+ 1k2
2FT0#
6 
1 +CVm1 
M2Km1+
2(2 lnm+ lnK)
+ (e+ 1)1
6K 1m 1 
2E"mX
=1kQT0+ 1k2
2FT0#
;
where in the last step, we use the bound forPm 1
=1k~0
T0++1 ~0
T0+k1in Assumption 3.
Therefore, we have
mX
=1Eh
hQT0+ 1;ST0+~T0+i QT0+ 1;aT0+ST0+;aT0+FT0i
6 (1 +CV)M2Km2 
2(2 lnm+ lnK) +K 1m 1 
2E"mX
=1kQT0+ 1k2
2FT0#
+ 6M2:
Proof of Theorem 6.3. These inequalities are all direct implications of the bounded-increment assumption. Denote by
i2arg maxi2[K]QT0;i, then we have
Qt;i(a)
QT0;i (t T0)M
QT0;i kQT0k1
2M+ 1
M
=kQT0k1
2 M
(b)
kQT0k1
4
for allT0+ 1tT0+kQT0k1
2M+ 1. Here step (a)is due to the boundedness of queue length increments, step (b)
is due to the assumption that kQT0k14M. Thus, for these t‚Äôs we have
kQtk1Qt;ikQT0k1
4:
Also, for any T0+ 1tT0+kQT0k1
2M+ 1andi2[K], we have
Qt;i(a)
QT0;i+ (t T0)M
kQT0k1+kQT0k1
2M+ 1
M
=3
2kQT0k1+M
(b)
2kQT0k1:
Here similarly, step (a)is due to the boundedness of queue length increments, step (b)is due to the assumption that
kQT0k14M. Therefore,
kQtk1= max
i2[K]Qt;i2kQT0k1:
35Queue Scheduling with Adversarial Bandit Learning A P REPRINT
Proof of Theorem 6.4. According to Theorem 6.3, when m=dkQT0k1
2Mewe have
Mm24mX
=1kQT0+ 1k1 (39)
andmX
=1kQT0+ 1k2
216KM2m3= 16KMmMm264KMmmX
=1kQT0+ 1k1: (40)
Then, we can apply the two inequalities to Eq. (8) of Theorem 6.2, to obtain an upper-bound inPm
=1kQT0+ 1k1.
SpeciÔ¨Åcally, for the 6 (1 +CV)M2Km2 
2(2 lnm+ lnK)term in the RHS of Eq. (8), we can apply Eq. (39) to
upper-bound one Mm2factor by 4Pm
=1kQT0+ 1k1, hence the term is no more than
24 (1 +CV)MKm 
2(2 lnm+ lnK)E"mX
=1kQT0+ 1k1FT0#
:
For theK 1m 1 
2EPm
=1kQT0+ 1k2
2FT0
term, we can upper-bound thepPm
=1kQT0+ 1k2
2factor (in each
sample path) by 64KMmPm
=1kQT0+ 1k1. Hence, the term is no more than
64Mm 
2E"mX
=1kQT0+ 1k1FT0#
:
Putting the upper-bounds for the two terms, we can get the claimed upper-bound.
G High-Level Ideas to Handle Queue Length Increments with Bounded Moments
Here, we explain the intuition behind the design of SoftMW+ andSSMW+ . Recall that in the design and analysis of
SoftMW andSSMW , the original assumption of bounded increments is mainly utilized in the following two steps:
‚Ä¢(EXP3.S+ feedback signal scale) In order to apply Theorem 5.1, it must be guaranteed that each feedback value
gt;atto be fed into EXP3.S+ is no more than  1
ttet;at. This is the main reason for us to set etto the normalized
current queue length vector, and set t=tMkQtk1in the bounded increment case.
‚Ä¢(Bounding EXP3.S+ regret byPkQtk1)After successfully applying Theorem 5.1 (and possibly summing over
EXP3.S+ instances), we obtain an upper-bound on the expected quadratic Lynapunov function value at the end of
timeT. Compared to the bound we can obtain in the stationary setting with Max-Weight via a standard Lyapunov
analysis, our bound contains an additional term with apPkQtk2
2factor. We then apply Theorem 5.9 for SoftMW
and Theorem 6.3 for SSMW to upper-bound thepPkQtk2
2by expressions inPkQtk1.
Therefore, if we want to handle potentially unbounded queue length increments with only bounded moments, we need
to generalize the arguments of the above two steps. Below, we introduce the high-level ideas to overcome the challenge
introduced by unbounded increments.
Skipping large feedback values from EXP3.S+
Suppose we are now at time tand choose et=Qt 1=kQt 1k1andt=CtMtkQt 1k1in the next EXP3.S+ step,
whereCt>0is some constant to be determined at the end of time t 1. Then EXP3.S+ can handle a new feedback
signal no more than CtMQt 1;at, whereatis the index of the queue we Ô¨Ånally choose to serve in this time step. In
other words, we can feed the signal to EXP3 only ifSt;atCtM. Since our goal of applying EXP3.S+ is to make the
sum ofhQt 1St;~ti Qt 1;atSt;atsmall, we can write
hQt 1St;~ti Qt 1;atSt;at 1[St;at>CtM]hQt 1St;~ti+hQt 1S0
t;~ti Qt 1;atS0
t;at
where
S0
t;i=St;iifSt;iCtM
0 otherwise
is a truncation of St;i. Then,hQt 1S0
t;~ti Qt 1;atS0
t;atis a quantity with an expected cumulative sum that can
be regarded as the regret of a MAB problem, and thus can be controlled by EXP3.S+ , if we takeQt 1;atS0
t;atas the
36Queue Scheduling with Adversarial Bandit Learning A P REPRINT
feedback value. Now there will be a multiplicative factor (max 1tTCt)in the obtained regret upper-bound. Also
note that
Eh
1[St;at>CtM]hQt 1St;~tiFt 1i
=E2
4 1[St;at>CtM]X
i6=atQt 1;iSt;it;iFt 13
5+E[ 1[St;at>CtM]Qt 1;atSt;att;atjFt 1]
(a)=E2
4P[St;at>CtMjFt 1;at]X
i6=atQt 1;it;it;iFt 13
5+E[ 1[St;at>CtM]Qt 1;atSt;att;atjFt 1]
P[St;at>CtMjFt 1]hQt 1~ t;~ti+E[ 1[St;at>CtM]Qt 1;atSt;att;atjFt 1]
P[St;at>CtMjFt 1]hQt 1~ t;~ti+kQt 1k1E[ 1[St;at>CtM]St;atjFt 1]
(b)
C 
thQt 1~ t;~ti+C +1
tMkQt 1k1
2C +1
tMkQt 1k1:
Here in step (a), we take the conditional expectation of 1[St;at> CtM]P
i6=atQt 1;iSt;it;iwith respect to
atbefore taking the conditional expectation with respect to Ft 1. In step (b), we make use of the fact that
P[St;at>CtMjFt 1]C 
tandE[ 1[St;at>CtM]St;atjFt 1]E[ 1[St;at>CtM]S
t;at=(CtM) 1jFt 1]
(CtM) +1E[S
t;atjFt 1]MC +1
t . Thus, the expected cumulative sum of 1[St;at>CtM]hQt 1St;~tican
also be well-controlled as long as we choose sufÔ¨Åciently small Cts.
By choosing appropriate Cts, we can trade-off the clipping errorPEh
1[St;at>CtM]hQt 1St;~tii
and the
post-clipping MAB regretPEh
hQt 1S0
t;~ti Qt 1;atS0
t;ati
well to make their sum, i.e., the pre-clipping MAB
regretPEh
hQt 1St;~ti Qt 1;atSt;ati
, not too large.
General conversion lemmas betweenPkQtk1andPkQtk2
2
So far we have dealt with issues of unbounded feedback values when applying regret bounds of EXP3.S+ . It remains
to boundPkQtk2
2byPkQtk1so that it can Ô¨Ånally solve to an upper-bound ofPkQtk1on the whole time-horizon.
In fact, Theorem 5.9 and Theorem 6.3 are both sample-path bounds forPkQtk2
2which hold under arbitrary
scheduling policies. SpeciÔ¨Åcally, Theorem 5.9 states thatPkQtk2
2is(PkQtk1)3
2on the entire time-horizon,
whereas Theorem 6.3 describes howPkQtk1andPkQtk2
2can be used to bound each other on any successive
m= (kQT0k1=M)time steps (where T0is the Ô¨Årst time step of the mtime steps considered). On the other hand,
our analysis actually only needs a relationship between the expectations EhpPkQtk2
2i
andE[PkQtk1]. Therefore,
there may be room for generalizing the result for queue size increments with bounded moments.
Surprisingly, when one tries to develop expectation versions of Theorem 5.9 and Theorem 6.3, they behave quite
differently. Theorem 6.3 has a direct expectation alternative but this does not seem to be the case for Theorem 5.9.
SpeciÔ¨Åcally, we can generalize Theorem 6.3 to Theorem G.1.
Lemma G.1. Under Assumption 4, for any T00, letm=dkQT0k1
2Me; further, suppose that kQT0k18M; then
M
2m2E"mX
t=1kQT0+t 1k1FT0#
3KMm2;
M2m3E"mX
t=1kQT0+t 1k2
2FT0#
5KM2m3:
Thus, we have
E2
4vuutTX
t=1kQT0+t 1k2
2FT03
5vuutE"TX
t=1kQT0+t 1k2
2FT0#
2p
5KpmE"TX
t=1kQT0+t 1k1FT0#
:
37Queue Scheduling with Adversarial Bandit Learning A P REPRINT
Proof. See Appendix H.
However, we cannot expect to generalize Theorem 5.9 in the same way. Formally speaking, now our goal would be to
obtain
E2
4vuutTX
t=1kQt 1k2
23
50
B@Ta0
@E2
4vuutTX
t=1kQt 1k13
51
Ab1
CA (41)
for somea+b<1. Recall that to obtain the last statement in Theorem G.1, we simply apply Jensen‚Äôs inequality to get
EhpPkQtk2
2i
p
E[PkQtk2
2]. But now we want Eq. (41) to hold under any scheduling policy; if we similarly
apply Jensen‚Äôs inequality, the task would reduce to bounding EPkQtk2
2
unconditionally , which is impossible since
under trivial scheduling policies that cannot stabilize the system, we can have kQtk2
2= (t)and thus EPkQtk2
2
is(T2).
It seems quite difÔ¨Åcult to directly obtain an expectation bound forpPkQtk2
2without interchanging the square root
and the expectation. Nevertheless, we can still utilize Theorem 5.9 in a somewhat naive way. Let
L, max
1tT;i2[K]jQt;i Qt 1;ij;
i.e.,Lis the maximum queue length increment in this sample path; hence, it is a random variable. Then, we can write
E2
4vuutTX
t=1kQt 1k2
23
5E2
4 1[L>C ]vuutTX
t=1kQt 1k2
23
5+E2
4 1[LC]vuutTX
t=1kQt 1k2
23
5
Eh
1[L>C ]p
KT3L2i
+E2
4 1[LC]vuutTX
t=1kQt 1k2
23
5
p
KT3
2E[ 1[L>C ]L] +E2
4 1[LC]vuutTX
t=1kQt 1k2
23
5:
For the E[ 1[L>C ]L]term, recall that Lis the maximum of KT random variables (and they are martingale dif-
ferences) with bounded -th moments; it can be shown that L‚Äôs-th moment is no more than 2KTM, and
E[ 1[L>C ]L]2KTMC1 . As for the other term, we can apply Theorem 5.9 to obtain a sample-path
bound 1[LC]qPT
t=1kQt 1k2
2(C1
4(PT
t=1kQt 1k1)3
4). Therefore, when is sufÔ¨Åciently large, we can
chooseCas some power of Tto conclude EqPT
t=1kQt 1k2
2
 
Ta
EqPT
t=1kQt 1k1b!
+Tcwhere
0<a;b;c< 1anda+b<1.
Proof of Theorem G.1. From the bouned -th moment assumption for queue length increments, we can also get a
bound for the queue length increment itself:
E[jQt;i Qt 1;ijjFt 1] =Eh
(jQt;i Qt 1;ij)1
Ft 1i
(a)
(E[jQt;i Qt 1;ijjFt 1])1

(M)1

=M;
where (a)is due to the concavity of x7!x1
. Similarly, we have
Eh
(Qt;i Qt 1;i)2Ft 1i
M2:
Then, for any tT0andi2[K], we can write
E[Qt;ijFT0]QT0;i+E"t 1X
s=T0E[jQs+1;i Qs;ijjFs]FT0#
38Queue Scheduling with Adversarial Bandit Learning A P REPRINT
QT0;i+ (t T0)M:
IfT0tT0+m, then we have E[Qt;ijFT0]QT0;i+Mm , hence
T0+m 1X
t=T0E[kQtk1jFT0]T0+m 1X
t=T0X
i2[K](QT0;i+Mm)
=mkQT0k1+KMm2
(a)
m2KMm +KMm2
3KMm2;
where step (a)is due to our choice m=lkQT0k1
2Mm
.
Similarly, for any T0tT0+mandi2[K], we have
E[Qt;ijFT0]QT0;i E"t 1X
s=T0E[jQs+1;i Qs;ijjFs]FT0#
QT0;i (t T0)M
=QT0;i Mm;
thus, denoting by i= arg maxi2[K]QT0;i, we have
T0+m 1X
t=T0E[kQtk1jFT0]T0+m 1X
t=T0Qt;i
T0+mX
t=T0+1(QT0;i Mm)
=mkQT0k1 Mm2
m(2Mm 2M) Mm2
=Mm2 2Mm
1
2Mm2;
where the last step uses the assumption that kQT0k18M, which implies that m4. Therefore, we conclude that
M
2m2E"mX
t=1kQT0+t 1k1FT0#
3KMm2:
For the bound forPkQtk2
2, we have
E
Q2
t;iFT0
Q2
T0;i+E2
4t 1X
s=T0Eh
(Qs+1;i Qs;i)2Fsi
+ 2X
T0s<s0<tE[jQs+1;i Qs;ijjFs]E[jQs0+1;i Qs0;ijjFs0]FT03
5
Q2
T0;i+ (t T0)M2+ 2t T0
2
M2
=Q2
T0;i+ (t T0)2M2
Q2
T0;i+M2m2
for allT0tT0+m. Thus, we get
T0+m 1X
t=T0E
kQtk2
2FT0
T0+m 1X
t=T0X
i2[K] 
Q2
T0;i+M2m2
39Queue Scheduling with Adversarial Bandit Learning A P REPRINT
=mkQT0k2
2+KM2m3
(a)
mK(2Mm)2+KM2m3
5KM2m3;
where step (a)is due to our choice m=lkQT0k1
2Mm
. For the other direction, we have
E
Q2
t;iFT0
Q2
T0;i E2
4t 1X
s=T0Eh
(Qs+1;i Qs;i)2Fsi
 2X
T0s<s0<tE[jQs+1;i Qs;ijjFs]E[jQs0+1;i Qs0;ijjFs0]FT03
5
Q2
T0;i (t T0)M2 2t T0
2
M2
=Q2
T0;i (t T0)2M2
Q2
T0;i M2m2:
and thus denoting by i= arg maxi2[K]QT0;i, we have
T0+m 1X
t=T0E
kQtk2
2FT0
T0+m 1X
t=T0 
Q2
T0;i M2m2
=mkQT0k2
1 M2m3
(a)
m(2Mm 2M)2 M2m3
(b)
m3
2Mm2
 M2m3
M2m3;
where step (a)is due to our choice m=lkQT0k1
2Mm
, step (b)uses the assumption that m4hence 2M1
2Mm .
Therefore,
M2m3E"mX
t=1kQT0+t 1k2
2FT0#
5KM2m3:
H Detailed Analysis for Algorithm 4
The analysis for Algorithm 4 is similar to Algorithm 2. First of all, we need to verify that the chosen exploration rates
t‚Äôs are guaranteed not to exceed1
2.
Proposition H.1. For allt1, we havet1
2in Algorithm 4.
Proof. Note that
t=t
4ML 1
t 1kQt 1k10
@t (1
4 
2)vuut86L2
t 1K6t3
2+t 1X
s=0kQsk2
21
A 1
t
4kQt 1k10
@t (1
4 
2)vuut86L2
t 1K6t3
2+t 1X
s=0kQsk2
21
A 1
;
sinceLtMfor allt0. Thus it sufÔ¨Åces to verify that
2t
4kQt 1k1t (1
4 
2)vuut86L2
t 1K6t3
2+t 1X
s=0kQsk2
2;
40Queue Scheduling with Adversarial Bandit Learning A P REPRINT
m
4kQt 1k2
1t 1
2+
2 
86L2
t 1K6t3
2+t 1X
s=0kQsk2
2!
;
*(a)
4kQt 1k2
1t 1
2+
2
86L2
t 1K6t3
2+1
3Lt 1K3kQt 1k3
1
m
4kQt 1k2
1t 1
2+
21
3258L2
t 1K6t3
2+2
31
2Lt 1K3kQt 1k3
1
;
*(b)
4kQt 1k2
1t 1
2+
22581
32 2
3t1
2kQt 1k2
1
m
4t
2258
41
3
;
and the last statement trivially holds. Here in step (a)we apply Theorem E.2 (it is applicable as long as we replace M
byLt 1), in step (b)we apply AM-GM inequality1
3x+2
3yx1
3y2
3.
Having veriÔ¨Åed that t1
2, also notice that we choose t=Mt
4tkQt 1k1in Algorithm 4, which allows EXP3.S+
to handle a new feedback gt;atas large asQt 1;atMt
4. Compared to Algorithm 2, in Algorithm 4, we clip the new
serviceSt;atatMt
4toS0
t, and instead feed Qt 1;atS0
tintoEXP3.S+ , therefore, Theorem 5.1 is now applicable, and
we can get
Lemma H.2. Suppose Assumption 1, 2 and 4 hold, then, running Algorithm 4 guarantees
TX
t=1Eh
hQt 1;S0
t~ti Qt 1;atS0
t;ati
E2
4LT 1(1 +CV)T1
4 
2(4 lnT+ lnK)vuut86L2
T 1K6T3
2+TX
t=1kQt 1k2
23
5
+E2
48MT1
4 
4vuut1 +TX
t=1kQt 1k2
2+ 4MLT 13
5 (42)
for any time horizon length T1. HereSt;iis deÔ¨Åned as
S0
t;i,(
St;iifSt;iMt
4
0 otherwise;
andf~tgis the reference policy in Assumption 1 and 2.
Proof. For each~t, we can Ô¨Ånd a ~0
t2[K];tso thatk~t ~0
tk12Ktandk~0
s ~0
tk1k~s ~tk1. For example,
we can choose
~0
t= (1 t)~t+t1:
Then, we can write
hQt 1;S0
t~ti=hQt 1;S0
t~0
ti+hQt 1S0
t;~t ~0
ti
hQt 1;S0
t~0
ti+kQt 1S0
tk1k~t ~0
tk1
hQt 1;S0
t~0
ti+Lt 1tMt
42Kt
=hQt 1;S0
t~0
ti+ 2Lt 1Mt
4 3
41Queue Scheduling with Adversarial Bandit Learning A P REPRINT
hQt 1;S0
t~0
ti+ 2Lt 1Mt 2:
Then, one can see the quantity
TX
t=1Eh
hQt 1;S0
t~0
ti Qt 1;atS0
t;ati
=TX
t=1Eh
hQt 1S0
t;~0
ti (Qt 1S0
t)ati
satisÔ¨Åes the condition to apply Theorem 5.1. On the other hand, the total difference of the regret to f~tgand the regret
tof~0
tgis within
TX
t=12Lt 1Mt 22MLT 1TX
t=1t 2
2
3MLT 1:
Theorem 5.1 asserts that
TX
t=1Eh
hQt 1;S0
t~0
ti Qt 1;atS0
t;ati
 
1 +T 1X
t=1k~0
t+1 ~0
tk1!
E
 1
Tln1
T
+eE"TX
t=1tkQt 1S0
tk2
2#
+E"TX
t=1t
Qt 1S0
t;Qt 1
kQt 1k1#
: (43)
Below, we bound each term in the RHS of Eq. (43). Firstly, we havePT 1
t=1k~0
t+1 ~0
tk1PT 1
t=1k~t+1 ~tk1
CVT1
2 according to Assumption 2, hence
 
1 +T 1X
t=1k~0
t+1 ~0
tk1!
E
 1
Tln1
T

1 +CVT1
2 
E
 1
Tln1
T

1 +CVT1
2 
(4 lnT+ lnK)E2
4T (1
4 
2)LT 1vuut86L2
T 1K6T3
2+T 1X
s=0kQsk2
23
5
(1 +CV)T1
4 
2(4 lnT+ lnK)LT 1E2
4vuut86L2
T 1K6T3
2+T 1X
s=0kQsk2
23
5:
For the second term, we have
TX
t=1E
tkQt 1S0
tk2
2Ft 1
=TX
t=1tX
i2[K]Q2
t i;iE
S02
t;iFt 1
(a)
M2TX
t=1tkQt 1k2
2
=MTX
t=1ML 1
t 1t1
4 
20
@vuut86L2
t 1K6t3
2+t 1X
s=0kQsk2
21
A 1
kQt 1k2
2
42Queue Scheduling with Adversarial Bandit Learning A P REPRINT
(b)
MTX
t=1t1
4 
20
@vuut1 +t 1X
s=0kQsk2
21
A 1
kQt 1k2
2
MT1
4 
2TX
t=10
@vuut1 +t 1X
s=0kQsk2
21
A 1
kQt 1k2
2
(c)
2MT1
4 
2vuut1 +T 1X
s=0kQsk2
2;
where in step (a), we apply the second-order moment bound for St;iinstead of the clipping threshold Mt
4. Step (b)
is due toLtMfor allt. In step (c), we use the fact that
nX
i=1xiq
1 +Pi
j=1xj2vuut1 +nX
i=1xi
for non-negative x1:::;xn.
The third term can be bounded very similarly to the second term, to be speciÔ¨Åc, we have
TX
t=1E
t
Qt 1S0
t;Qt 1
kQt 1k1Ft 1
(a)
MTX
t=1tkQt 1k 1
1kQt 1k2
2
MTX
t=1ML 1
t 1t
4t1
4 
20
@vuut1 +t 1X
s=0kQsk2
21
A 1
kQt 1k2
2
MTX
t=1t1
4 
40
@vuut1 +t 1X
s=0kQsk2
21
A 1
kQt 1k2
2
2MT1
4 
4vuut1 +T 1X
s=0kQsk2
2;
where in step (a)we apply the Ô¨Årst-order moment bound for St;i.
Combining everything together then taking expectation, we can see that
TX
t=1Eh
hQt 1;S0
t~0
ti Qt 1;atS0
t;ati
(1 +CV)T1
4 
2(4 lnT+ lnK)LT 1E2
4vuut86L2
T 1K6T3
2+T 1X
s=0kQsk2
23
5
+ 2MT1
4 
2E2
4vuut1 +T 1X
s=0kQsk2
23
5+ 2eMT1
4 
4E2
4vuut1 +T 1X
s=0kQsk2
23
5+2
3ME[LT 1]
(1 +CV)T1
4 
2(4 lnT+ lnK)LT 1E2
4vuut86L2
T 1K6T3
2+T 1X
s=0kQsk2
23
5
+ 8MT1
4 
4E2
4vuut1 +T 1X
s=0kQsk2
23
5+ 4ME[LT 1]:
43Queue Scheduling with Adversarial Bandit Learning A P REPRINT
Then, we can turn Theorem H.2, the upper-bound for (Qt 1S0
t)-objective regret to a (Qt 1St)-objective one:
Lemma H.3. Suppose Assumption 1, 2 and 4 hold, then, running Algorithm 4 guarantees
TX
t=1Eh
hQt 1;St~ti Qt 1;atSt;ati
E2
4LT 1(1 +CV)T1
4 
2(4 lnT+ lnK)vuut86L2
T 1K6T3
2+TX
t=1kQt 1k2
23
5
+E2
48MT1
4 
4vuut1 +TX
t=1kQt 1k2
2+ 4MLT 13
5+o(T) (44)
for any time horizon length T1. Heref~tgis the reference policy in Assumption 1 and 2.
Proof. Note that for each t, the single-step regret hQt 1;St~ti Qt 1;atSt;atcan be upper-bounded by
hQt 1;St~ti Qt 1;atSt;athQt 1;S0
t~ti Qt 1;atS0
t;at| {z }
(Qt 1S0
t)-objective single step regret+ 1h
St;at>Mt
4i
hQt 1;St~ti
where
S0
t;i,(
St;iifSt;iMt
4
0 otherwise:
Therefore, it sufÔ¨Åces to upper-boundPT
t=1Eh
1h
St;at>Mt
4i
hQt 1;St~tii
. In fact,
Eh
1h
St;at>Mt
4i
hQt 1;St~tiFt 1i
kQt 1k1Eh
1h
St;at>Mt
4i
kStk1i
=kQt 1k10
@E2
4 1h
St;at>Mt
4iX
i2[K]:i6=atSt;i3
5+Eh
1h
St;at>Mt
4i
St;ati1
A:
For the Ô¨Årst term Eh
1h
St;at>Mt
4iP
i2[K]:i6=atSt;ii
, we can write
E2
4 1h
St;at>Mt
4iX
i2[K]:i6=atSt;i3
5=E2
4Ph
St;at>Mt
4ati
E2
4X
i2[K]:i6=atSt;iat3
53
5
=E2
4Ph
St;at>Mt
4ati
X
i2[K]:i6=att;i3
5
KMPh
St;at>Mt
4i
KMM(Mt
4) 
=KMt 
4;
where the last inequality is due to the -th moment upper-bound for St;atand Chebyshev‚Äôs inequality. For the other
termEh
1h
St;at>Mt
4i
St;ati
, we have
Eh
1h
St;at>Mt
4i
St;ati
Eh
S
t;at(Mt
4) ( 1)i
Mt ( 1)
4:
44Queue Scheduling with Adversarial Bandit Learning A P REPRINT
Therefore,
Eh
1h
St;at>Mt
4i
hQt 1;St~tiFt 1i
KMt ( 1)
4kQt 1k1;
TX
t=1Eh
1h
St;at>Mt
4i
hQt 1;St~tii
KMTX
t=1t ( 1)
4E[kQt 1k1]
KMTX
t=1t1 ( 1)
4E[Lt]
KMTX
t=1t1 ( 1)
4E[L
t]1

2K1+1
M2TX
t=1t1 ( 1)
4t1

o(T)
where the last step is due to the assumption of >7.
Compared to Theorem 5.7, the RHS of the bound in Theorem H.3 now involves LT 1, the sample-path maximum
queue length increment. Now we will do a further calculation to get rid of the LT 1factors.
Recall thatLtis deÔ¨Åned as
Lt,max
M; max
1st;i2[K]jQs;i Qs 1;ij
in Algorithm 4, the bounded-moment-queue-length increment assumption gives an upper-bound for the -th moment
ofLt
E[L
t]max(
M;tX
s=1E[jQs;i Qs 1;ij])
maxfM;2KTMg
= 2KTM:
This moment bound for Ltenables us to deal with the LT 1factor in Theorem H.3. For example, we have
E[LT 1]E
L
T 11
2K1
MT1
=o(T):
To bound the E
LT 1(1 +CV)T1
4 
2(4 lnT+ lnK)q
86L2
T 1K6T3
2+PT
t=1kQt 1k2
2
term, we Ô¨Årst simply
write
E2
4LT 1(1 +CV)T1
4 
2(4 lnT+ lnK)vuut86L2
T 1K6T3
2+TX
t=1kQt 1k2
23
5
E2
4LT 1(1 +CV)T1
4 
2(4 lnT+ lnK)0
@q
86L2
T 1K6T3
2+vuutTX
t=1kQt 1k2
21
A3
5
=Eh
K3(1 +CV)T1 
2(4 lnT+ lnK)L2
T 1i
+E2
4(1 +CV)T1
4 
2(4 lnT+ lnK)LT 1vuutTX
t=1kQt 1k2
23
5
45Queue Scheduling with Adversarial Bandit Learning A P REPRINT
and then bound the two terms one by one. The L2
T 1term is easy:
Eh
K3(1 +CV)T1 
2(4 lnT+ lnK)L2
T 1i
K3(1 +CV)T1 
2(4 lnT+ lnK)E
L
T 12

K3(1 +CV)T1 
2(4 lnT+ lnK)2K2
M2T2

(T1 
2+2
)
o(T)
where the last step is due to the assumption that >7. For the other term, we can write
LT 1vuutTX
t=1kQt 1k2
2
 1h
LT 1MT
4i
LT 1vuutTX
t=1kQt 1k2
2+ 1h
LT 1>MT
4i
LT 1vuutTX
t=1kQt 1k2
2: (45)
For the term with factor 1h
LT 1MT
4i
in Eq. (45), we can apply Theorem 5.9 to get
1h
LT 1MT
4i
LT 1vuutTX
t=1kQt 1k2
2 1h
LT 1MT
4i
2L5
4
T 1 TX
t=1kQt 1k1!3
4
2M5
4T5
16 TX
t=1kQt 1k1!3
4
:
For the other term with factor 1h
LT 1>MT
4i
, we can write
1h
LT 1>MT
4i
LT 1vuutTX
t=1kQt 1k2
2 1h
LT 1>MT
4i
LT 1vuutTX
t=1K(LT 1T)2
p
KT3
2 1h
LT 1>MT
4i
L2
T 1;
besides, we can bound its expectation by
Eh
1h
LT 1>MT
4i
L2
T 1i
Eh
L
T 1=(MT
4) 2i
2KTM(MT
4) ( 2)
2KM2T1  2
4;
hence
E2
4 1h
LT 1>MT
4i
LT 1vuutTX
t=1kQt 1k2
23
5p
KT3
22KM2T1  2
4
= 2K3
2M2T5
2  2
4
= 2K3
2M2T10+2 
4
2K3
2M2T11 
4
o(T)
where the last inequality is due to the assumption that >7.
Combining bounds for all terms, we can conclude that
E2
4(1 +CV)T1
4 
2(4 lnT+ lnK)LT 1vuutTX
t=1kQt 1k2
23
5
46Queue Scheduling with Adversarial Bandit Learning A P REPRINT
o(T) +E2
4(1 +CV)T1
4 
2(4 lnT+ lnK) 1h
LT 1MT
4i
LT 1vuutTX
t=1kQt 1k2
23
5
o(T) + (1 +CV)T1
4 
2(4 lnT+ lnK)2M5
4T5
16E2
4 TX
t=1kQt 1k1!3
43
5
=o(T) + 2(1 +CV)M5
4T1
4 3
16(4 lnT+ lnK)E2
4 TX
t=1kQt 1k1!3
43
5:
It remains to bound the E
8MT1
4 
4q
1 +PT
t=1kQt 1k2
2
term in Theorem H.2. Since
T1
4 
4q
1 +PT
t=1kQt 1k2
2+T1
4 
4qPT
t=1kQt 1k2
2=o(T) +T1
4 
4qPT
t=1kQt 1k2
2, it sufÔ¨Åces to
bound E
MT1
4 
4qPT
t=1kQt 1k2
2
. Similarly, we begin by writing
T1
4 
4vuutTX
t=1kQt 1k2
2 1h
LT 1>MT
4i
T1
4 
4vuutTX
t=1kQt 1k2
2+ 1h
LT 1MT
4i
T1
4 
4vuutTX
t=1kQt 1k2
2
and bound the two terms‚Äô expectations one by one. For the term with 1h
LT 1>MT
4i
factor, we have
E2
4 1h
LT 1>MT
4i
T1
4 
4vuutTX
t=1kQt 1k2
23
5T1
4 
4p
KT3
2Eh
1h
LT 1>MT
4i
LT 1i
T1
4 
4p
KT3
22KTMT ( 1)
4
= 2qK3
2MT11
4 
4
o(T)
where the last step is due to the assumption that >7. For the term with 1h
LT 1>MT
4i
factor, we can bound
it by
E2
4 1h
LT 1MT
4i
T1
4 
4vuutTX
t=1kQt 1k2
23
5
T1
4 
4E2
4 1h
LT 1MT
4i
2L1
4
T 1 TX
t=1kQt 1k1!3
43
5
2M1
4T1
4 3
16E2
4 TX
t=1kQt 1k1!3
43
5:
Therefore, we can conclude that
TX
t=1Eh
hQt 1;St~ti Qt 1;atSt;ati
o(T) + 2(1 +CV)M5
4T1
4 3
16(4 lnT+ lnK)E2
4 TX
t=1kQt 1k1!3
43
5
+ 16M5
4T1
4 3
16E2
4 TX
t=1kQt 1k1!3
43
5
47Queue Scheduling with Adversarial Bandit Learning A P REPRINT
o(T) + 18(1 +CV)M5
4T1
4 3
16(4 lnT+ lnK)E2
4 TX
t=1kQt 1k1!3
43
5
o(T) + 18(1 +CV)M5
4T1
4 3
16(4 lnT+ lnK) 
E"TX
t=1kQt 1k1#!3
4
: (46)
Plugging Eq. (46) into Eq. (3) in Theorem 5.3, we get
Proposition H.4. With Assumption 1, 2 and 4, if >7, then running Algorithm 4 guarantees that
E"TTX
t=1kQt 1k1#
f(TT) +h(TT) 
E"TTX
t=1kQt 1k1#!3
4
(47)
for anyTmaxf4
CW;CWg, whereTTis some constant no more than 2T,
f(T) =(K+ 1)M2+ 2CW(KM2+KM )
T+o(T);
and
h(T) =18(1 +CV)M5
4T1
4 3
16(4 lnT+ lnK)
=e(T1
4 3
16 1):
Applying Theorem 5.10 to Eq. (47), Eq. (47) solves to
E"TTX
t=1kQt 1k1#

h(TT)1
4+f(TT)4
(K+ 1)M2+ 2CW(KM2+KM )
TT+o(TT);
thus
1
TE"TX
t=1kQt 1k1#
1
TE"TTX
t=1kQt 1k1#
(K+ 1)M2+ 2CW(KM2+KM )
TT
T+o(TT=T)
2(K+ 1)M2+ 4CW(KM2+KM )
+o(1)
as desired.
I Detailed Analysis for Algorithm 5
Similar to the analysis of Algorithm 3, we Ô¨Årst build generalized version of the regret bound lemma of each EXP.3
epoch (Theorem 6.2):
Lemma I.1. Suppose Assumption 1, 3 and 4 hold, then, let T0be some time step on which we start a new EXP3.S+
instance of length min Algorithm 5, we have
1[T0ends an EXP3 instance, and the new EXP3 instance is of length m;m2]
mX
t=1Eh
hQT0+t 1ST0+t;~T0+ti QT0+t 1;aT0+tST0+t;aT0+tFT0i
21(1 +CV)M3Km2 
3(3 lnm+ lnK) + 4M2: (48)
Proof. The high level idea is to apply Theorem 5.1, but we need to verify that all ‚Äôs are no more than1
2Ô¨Årst.
Our choice of mguarantees that mkQT0k1
2M, hencekQT0k12Mm , therefore
=1
4M 2m 1 
3kQT0k1
48Queue Scheduling with Adversarial Bandit Learning A P REPRINT
1
4M 2m 1 
32Mm
=1
2M 1m 
3
1
2:
For any 1m, deÔ¨Åne
gT0+;i,(
QT0+ 1;iST0+;i ifQT0+ 1;iST0+;im
3MQT0;i
0 otherwise:
Below, we Ô¨Årst bound
E"mX
=1D
gT0+;~T0+E
 gT0+;aT0+FT0#
;
i.e., the gT0+-objective regret, then derive a bound for the original QT0+ 1ST0+-objective regret.
For each~T0+, we can Ô¨Ånd a ~0
T0+2[K];so thatk~T0+ ~0
T0+k12Kandk~0
s ~0
tk1k~T0+s ~T0+tk1
for any 1s;tm. For example, we can choose
~0
T0+= (1 )~T0++1:
Then, we can write
hgT0+;~T0+i=hgT0+;~0
T0+i+hgT0+;~T0+ ~0
T0+i
hgT0+;~0
T0+i+kgT0+k1k~T0+ ~0
T0+k1
hgT0+;~0
T0+i+m
3MkQT0k1k~T0+ ~0
T0+k1
hgT0+;~0
T0+i+m
3M2Mmk~T0+ ~0
T0+k1
hgT0+;~0
T0+i+ 2M2m1+
32K
=hgT0+;~0
T0+i+ 4M2m
3 2
hgT0+;~0
T0+i+ 4M2m 1:
Then, one can see the quantity
mX
=1Eh
hgT0+;~0
T0+i gT0+;aT0+FT0i
satisÔ¨Åes the condition to apply Theorem 5.1. Theorem 5.1 asserts that
mX
=1Eh
hgT0+;~0
T0+i gT0+;aT0+FT0i
 
1 +m 1X
=1k~0
T0++1 ~0
T0+k1!
E
 1
mln1
mFT0
+eE"mX
=1kgT0+k2
2FT0#
+E"mX
=1
gT0+;1
KFT0#
 
1 +m 1X
=1k~0
T0++1 ~0
T0+k1!
4M3Km1+2
3(3 lnm+ lnK)
+e1
4M 1K 1m 1 2
3E"mX
=1kgT0+k2
2FT0#
+1
4K 1m 1 1
3kQT0k1E"mX
=1kgT0+k1FT0#
4 
1 +CVm1 
M3Km1+2
3(3 lnm+ lnK)
49Queue Scheduling with Adversarial Bandit Learning A P REPRINT
+e
4M 1K 1m 1 2
3E"mX
=1kgT0+k2
2FT0#
+1
4K 1m 1 1
3kQT0k1E"mX
=1kgT0+k1FT0#
;
where in the last step, we use the bound forPm 1
=1k~0
T0++1 ~0
T0+k1in Assumption 3. To bound the two expectation
terms involving gT0+factors, recall that gT0+still enjoys the moment bound before clipping:
E
g2
T0+;iFT0
E
Q2
T0+ 1;iS2
T0+;iFT0
=E
Q2
T0+ 1;iFT0
E
S2
T0+;iFT0
(a)
(Q2
T0;i+M2( 1)2)M2
((2Mm)2+M2m2)M2
= 5M4m2;
where in step (a), we bound E
Q2
T0+ 1;iFT0
byQ2
T0;i+M2( 1)2in the same way when we prove Theorem G.1.
Similarly,
E[gT0+;ijFT0]E[QT0+ 1;iST0+;ijFT0]
=E[QT0+ 1;ijFT0]E[ST0+;ijFT0]
(QT0;i+M( 1))M
(2Mm +Mm)M
= 3M2m:
Thus the two expectation factors can be bounded by
E"mX
=1kgT0+k2
2FT0#
5KM4m3;
kQT0k1E"mX
=1kgT0+k1FT0#
2Mm3KM2m
= 6KM3m2:
Therefore, we can conclude that
mX
=1Eh
hgT0+;~0
T0+i gT0+;aT0+FT0i
4 
1 +CVm1 
M3Km1+2
3(3 lnm+ lnK)
+5e
4M3m2 2
3+3
2M3m2 1
3
4 
1 +CVm1 
M3Km1+2
3(3 lnm+ lnK)
+7
2M3m2 2
3+3
2M3m2 1
3
9(1 +CV)M3Km2 
3(3 lnm+ lnK):
Finally, note that for each , the single-step regret hQT0+ 1;ST0+~T0+i QT0+ 1;aT0+ST0+;aT0+can be
upper-bounded by
hQT0+ 1;ST0+~T0+i QT0+ 1;aT0+ST0+;aT0+
hgT0+;~T0+i gT0+;aT0+|{z }
gT0+-objective single step regret+ 1h
QT0+ 1;aT0+ST0+;aT0+>Mm
3kQT0k1i
hQT0+ 1;ST0+~T0+i:
And each difference term can be controlled by
Eh
1h
QT0+ 1;aT0+ST0+;aT0+>Mm
3kQT0k1i
hQT0+ 1;ST0+~T0+iFT0i
50Queue Scheduling with Adversarial Bandit Learning A P REPRINT
E"
1h
QT0+ 1;aT0+ST0+;aT0+>Mm
3kQT0k1iKX
i=1QT0+ 1;iST0+;iFT0#
=E2
4 1h
QT0+ 1;aT0+ST0+;aT0+>Mm
3kQT0k1iX
i2[K]:i6=atQT0+ 1;iST0+;iFT03
5
+Eh
1h
QT0+ 1;aT0+ST0+;aT0+>Mm
3kQT0k1i
QT0+ 1;atST0+;atFT0i
:
For the Ô¨Årst term Eh
1h
QT0+ 1;aT0+ST0+;aT0+QT0;at>Mm
3kQT0k1iP
i2[K]:i6=atQT0+ 1;iST0+;iFT0i
,
we can write
E2
4 1h
QT0+ 1;aT0+ST0+;aT0+>Mm
3kQT0k1iX
i2[K]:i6=atQT0+ 1;iST0+;iFT03
5
=E2
4Ph
QT0+ 1;aT0+ST0+;aT0+>Mm
3kQT0k1FT0;ati
E2
4X
i2[K]:i6=atQT0+ 1;iST0+;iFT0;at3
5FT03
5:
Recall that for each 1mandi2[K], we can bound the second monent of QT0+ 1;iST0+;iby
E
Q2
T0+ 1;iS2
T0+;iFT0
=E
Q2
T0+ 1;iFT0
E
S2
T0+;iFT0
(a)
(Q2
T0;i+M2( 1)2)M2
((2Mm)2+M2m2)M2
= 5M4m2;
therefore, by Chebyshev‚Äôs inequality, we have
Ph
QT0+ 1;aT0+ST0+;aT0+>Mm
3kQT0k1FT0;ati
5M4m2(Mm
3kQT0k1) 2
(a)
5M4m2(Mm
3Mm) 2
= 5m 2
3;
where step (a)is due to the assumption that m2, thuskQT0k12M(m 1)Mm . For succeeding factor
EhP
i2[K]:i6=atQT0+ 1;iST0+;iFT0;ati
, we have
E2
4X
i2[K]:i6=atQT0+ 1;iST0+;iFT0;at3
5(K 1)p
5M2m:
For the other term Eh
1h
QT0+ 1;aT0+ST0+;aT0+>Mm
3kQT0k1i
QT0+ 1;atST0+;atFT0i
, we have
Eh
1h
QT0+ 1;aT0+ST0+;aT0+>Mm
3kQT0k1i
QT0+ 1;atST0+;atFT0i
E
Q2
T0+ 1;atS2
T0+;atFT0

Mm
3kQT0k1 1
5M4m2(Mm
3Mm) 1
= 5M2m1 
3:
Therefore, combining the different parts of bounds, we get
mX
=1Eh
1h
QT0+ 1;aT0+ST0+;aT0+>Mm
3kQT0k1i
hQT0+ 1;ST0+~T0+iFT0i
5p
5(K 1)M2m2 2
3+ 5M2m2 
3
51Queue Scheduling with Adversarial Bandit Learning A P REPRINT
12KM2m2 
3:
Putting this regret difference bound with the gT0+-objective regret bound, we get
mX
t=1Eh
hQT0+t 1ST0+t;~T0+ti QT0+t 1;aT0+tST0+t;aT0+tFT0i
9(1 +CV)M3Km2 
3(3 lnm+ lnK) + 4M2+ 12KM2m2 
3
21(1 +CV)M3Km2 
3(3 lnm+ lnK) + 4M2:
Combining Theorem I.1 and Theorem G.1, we can get a regret upper-bound inPm
=1kQT0+ 1k1:
Lemma I.2. Suppose Assumption 1, 3 and 4 hold, then, let T0be some time step on which we start a new EXP3.S+
instance of length min Algorithm 5, we have
1[T0ends an EXP3 instance, and the new EXP3 instance is of length m;m2]
mX
t=1Eh
hQT0+t 1ST0+t;~T0+ti QT0+t 1;aT0+tST0+t;aT0+tFT0i
42(1 +CV)M2Km 
3(3 lnm+ lnK)E"mX
=1kQT0+ 1k2
2FT0#
+ 4M2: (49)
With Theorem I.2, the remaining steps are the same as the analysis of Algorithm 3.
Denote byi(i0) the time on which the i-thEXP3.S+ instance Ô¨Ånishes. Then, 0= 0,figis a sequence of
non-decreasingfFtg-adapted stopping-times. Further more, each i+1isFi-measurable. Fix any T1, deÔ¨Åne
0
i,8
<
:0 ifi= 0
i ifi>0and0
i 1<T
0
i 1otherwise;
thenf0
igis a sequence of non-decreasing fFtg-adapted stopping-times, each 0
i+1isF0
i-measurable, 0
i+1=0
iif
any only if0
iT. Thus, we can restate Theorem I.2 as the following:
Lemma I.3. Suppose Assumption 1, 3 and 4 hold, then we have
1
kQ0
ik14M0
i+1 0
iX
t=1Eh
hQ0
i+t 1S0
i+t;~0
i+ti Q0
i+t 1;a0
i+tS0
i+t;a0
i+tF0
ii
h(0
i+1 0
i) +f(0
i+1 0
i)0
i+1 0
iX
t=1E
kQ0
i+t 1k1F0
i
for anyi0, where
f(m) = 42(1 +CV)M2Km 
3(3 lnm+ lnK);
h(m) = 1[m> 0]4M2:
Fix someT1, letT0,supfi:i0;i< Tg,T1,inffi:i0;iTg, thenT0andT1are bothfFtg-
adapted stopping-time, T1isFT0-measurable. Note that T0<TT1. Furthermore, since T1 T0kQT0k1
2M+ 1
T0M
2M+ 1 =T0
2+ 1T0
2+T, hence we haveT15
2T.
Below, we will combine Theorem 5.3 and Theorem I.3 to bound E[PT1
t=1kQt 1k1]in(E[T1]) = (T)so that we
can conclude that E[PT
t=1kQt 1k1]is also (T).
Recall>0is the lower-bound of the ‚Äúaverage advantage of departure against arrival‚Äù of the reference policy f~tgin
Assumption 1, deÔ¨Åne
m0,infn
m:m2;f(m0)
28m0mo
;
52Queue Scheduling with Adversarial Bandit Learning A P REPRINT
thenm0is a constant that only depends on and, in fact,
m0 
(1 +CV)M2KlnK 1(1=):
By discussing whether each epoch length 0
i+1 0
iis greater than m0or not, we conclude from Theorem I.3 that
0
i+1 0
iX
t=1Eh
hQ0
i+t 1S0
i+t;~0
i+ti Q0
i+t 1;a0
i+tS0
i+t;a0
i+tF0
ii
h(0
i+1 0
i) +
20
i+1 0
iX
t=1E
kQ0
i+t 1k1F0
i
+ 1[0
i+1 0
im0]0
i+1 0
iX
t=1Eh
hQ0
i+t 1S0
i+t;~0
i+tiF0
ii
h(0
i+1 0
i) +
20
i+1 0
iX
t=1E
kQ0
i+t 1k1F0
i
+ 1[0
i+1 0
im0]M0
i+1 0
iX
t=1E
kQ0
i+t 1k1F0
i
(a)
h(0
i+1 0
i) +
20
i+1 0
iX
t=1E
kQ0
i+t 1k1F0
i
+ 1[0
i+1 0
im0]M3KM(0
i+1 0
i)2
h(0
i+1 0
i) +
20
i+1 0
iX
t=1E
kQ0
i+t 1k1F0
i
+ 3KM2m0(0
i+1 0
i) (50)
for alli0. Here in step (a)we apply Theorem G.1.
Summing Eq. (50) over all i0and then taking total expectations, we get
E"T1X
t=1hQt 1St;~ti Qt 1;atSt;at#

2E"T1X
t=1kQt 1k1#
+ 3KM2m0E[T1] +E"1X
i=0h(0
i+1 0
i)#
:
In any sample path, we haveP1
i=0h(0
i+1 0
i)4M2T1, therefore
E"T1X
t=1hQt 1St;~ti Qt 1;atSt;at#

2E"T1X
t=1kQt 1k1#
+ (3KM2m0+ 4M2)E[T1]: (51)
According to Theorem 5.4, we can also Ô¨Ånd a constant T2depending onT1, such thatT2T1+q
T1
CW+ 1and
 E"T2X
t=1hQt 1;~ t~t ~ti#
 E"T1X
t=1kQt 1k1#
+ (KM2+KM )CWE[T2]: (52)
IfT4
CW+CW, we haveT1maxfCW;4
CWghenceT22T15T. Also, Theorem 5.3 guarantees that
E"T2X
t=1Qt 1;atSt;at hQt 1;~ti#
(K+ 1)M2E[T2]
2: (53)
Combining Eqs. (51) to (53) together, we get

2E"T1X
t=1kQt 1k1#
(K+ 1)M2
2+ (KM2+KM )CW+ 3KM2m0+ 4M2
E[T2]
(K+ 1)M2
2+ (KM2+KM )CW+ 3KM2m0+ 4M2
5T:
Thus, when T4
CW+CW, we have
1
TE"TX
t=1kQt 1k1#
1
TE"T1X
t=1kQt 1k1#

3KM2m0+(K+ 1)M2
2+ (KM2+KM )CW+ 4M2
10
:
53