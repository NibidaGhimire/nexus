Heterogeneous Social Value Orientation Leads to Meaningful
Diversity in Sequential Social Dilemmas
Udari Madhushani
Princeton University
udarim@princeton.eduKevin R. McKee
DeepMind
kevinrmckee@google.comJohn P. Agapiou
DeepMind
jagapiou@google.com
Joel Z. Leibo
DeepMind
jzl@google.comRichard Everett
DeepMind
reverett@google.comThomas Anthony
DeepMind
twa@google.com
Edward Hughes
DeepMind
edwardhughes@google.comKarl Tuyls
DeepMind
karltuyls@google.comEdgar A. DuÃ©Ã±ez-GuzmÃ¡n
DeepMind
duenez@google.com
ABSTRACT
In social psychology, Social Value Orientation (SVO) describes an
individualâ€™s propensity to allocate resources between themself and
others. In reinforcement learning, SVO has been instantiated as
an intrinsic motivation that remaps an agentâ€™s rewards based on
particular target distributions of group reward. Prior studies show
that groups of agents endowed with heterogeneous SVO learn
diverse policies in settings that resemble the incentive structure
of Prisonerâ€™s dilemma. Our work extends this body of results and
demonstrates that (1) heterogeneous SVO leads to meaningfully
diverse policies across a range of incentive structures in sequential
social dilemmas, as measured by task-specific diversity metrics; and
(2) learning a best response to such policy diversity leads to better
zero-shot generalization in some situations. We show that these
best-response agents learn policies that are conditioned on their
co-players, which we posit is the reason for improved zero-shot
generalization results.
1 INTRODUCTION
In psychology research, Social Value Orientation (SVO) is a cog-
nitive construct reflecting a personâ€™s preference for resource al-
location between themselves and others [ 7,15,22]. While some
individuals may solipsistically focus on maximizing their personal
success, others demonstrate different motivations, including maxi-
mizing the difference between their own and othersâ€™ outcomes (a
competitive orientation), maximizing collective welfare (a prosocial
orientation), or maximizing other peoplesâ€™ benefit (an altruistic ori-
entation). In artificial intelligence research, various algorithms draw
inspiration from these insights in their design or implementation
[19,27]. In reinforcement learning, SVO is an intrinsic motivation
that transforms an agentâ€™s reward based on a parameterized target
distribution between its reward and the reward of others. Recently,
studies have investigated the role of SVO in social dilemmas, sit-
uations where a group of agents or players interact in ways that
involve trade-offs between their self-interest and the collective in-
terest of the group. This research has generated insight into the
Proc. of the Adaptive and Learning Agents Workshop (ALA 2023), Cruz, Hayes, Wang,
Yates (eds.), May 9-10, 2023, Online, https://ala2023.github.io/ . 2023.impact of SVO on the emergence of diverse behaviors and cooper-
ation [ 19,20], and partner choice [ 18]. SVO research has focused
primarily on social dilemmas with underlying incentive structures
resembling the Prisonerâ€™s dilemma [26], wherein each player has
an incentive to defect, even though they would be better off if they
both cooperated.
Sequential social dilemmas are a class of social dilemmas in
which the decision-making process of the interacting agents is
temporally and spatially extended [ 13]. Performing well in a se-
quential social dilemma can be accomplished by considering of
long-term consequences, interdependence, and cooperation among
group members. Sequential social dilemmas have been widely stud-
ied in the context of emergence and maintenance of cooperation
[14,25], inequity aversion [ 11], partner choice [ 4,18], and general-
ization [ 1,20] wherein agents interact with novel co-players in test
scenarios.
While environments provide an extrinsic reward that can be
used to learn a policy, it is often useful to provide agents with
anintrinsic reward to shape their behavior towards a policy with
desirable properties. Intrinsic reward has be used analogously to
social preferences in human decision making. In most research
on sequential social dilemmas, all players either have no intrinsic
reward , or they all have the same function (i.e. they have homoge-
neous social preferences) [ 14,31]. However, it has been observed
that having a population of agents who differ in their intrinsic
reward function (i.e. they have heterogeneous social preferences)
can lead to higher levels of cooperation [ 11]. In [ 18â€“20], the au-
thors showed that heterogeneity can produce behavioral diversity
in group dilemmas, and in games with incentive structures similar
to the Prisonerâ€™s dilemma. Other incentive structures have not yet
been explored. In addition, the precise interplay between diversity
in social preferences and in agent policies, particularly on the mech-
anisms that enable generalization to novel social partners, remains
under-explored.
Diversity in policies has been demonstrated to improve various
aspects of agent performance, such as exploration [ 33], adaptation
to environmental changes [ 3], positive group outcomes [ 19,30],
generalization to novel co-players [ 17], and collaboration with hu-
mans [ 29]. One way to quantify diversity is to examine the reward
an agent obtains when interacting with different co-players (oftenarXiv:2305.00768v1  [cs.MA]  1 May 2023called strategic diversity ) [2,6]. To complement these methods, diver-
sity can also be evaluated through state-action variation, which mea-
sures the distribution of state-action pairs that an agent traverses.
State-action diversity can be assessed by measuring differences
in the state visitation frequency [ 33], action selection frequency
in a given state [ 20], or differences between state-action trajecto-
ries starting from a specific state [ 17]. Defining an environment
agnostic metric based on state-action variation that captures mean-
ingful diversityâ€”that is, diversity that has a broader effect on group
trajectoriesâ€”can be challenging. An alternative is to instead use
environment-specific measures of diversity, which the researcher
can design using their knowledge of specific environment features.
Zero-shot generalization [ 9,10,12,20,29] seeks to develop gen-
eral agents that are capable of successfully interacting with novel
agents during test time (i.e., agents not seen during training). In
such situations, the policies of the novel agents encountered at
test time can be out-of-distribution for the agents, leading to poor
coordination in purely cooperative settings [ 10,17], and getting ex-
ploited in competitive settings [ 24]. In mixed-motive games, failure
to generalize to novel agents can lead to deadweight loss by missing
an opportunity to cooperate [ 12]. Learning a best response to part-
ners/opponents with diverse policies has emerged as a promising
approach to zero-shot generalization [ 29]. The intuition behind this
approach is that training with a set of diverse policies decreases
the likelihood of encountering out-of-distribution policies at test
time. Despite this promise these best response techniques have not
yet been applied in a wide range of incentive structures.
In this work, we assess heterogeneous SVO in a range of incentive
structures in sequential social dilemmas. We include temporally
and spatially extended environments with an underlying structure
that resembles several different matrix games: Prisonerâ€™s dilemma ;
Chicken , where both players have an incentive to choose a risky
behavior, but where the worst outcome is if both choose the high
risk; and Stag hunt wherein players have a safe choice, and an
incentive to coordinate on a high-reward strategy that carries a risk
of costly miscoordination. Chicken and Stag hunt are equilibrium
selection social dilemmas.
We extend the observation that heterogeneous SVO leads to
diverse policies to the Chicken- and Stag hunt-like incentive struc-
tures. We also show that this diversity, when leveraged via best
response, can improve zero-shot generalization in equilibrium se-
lection sequential social dilemmas. We found that best-response
agents adapted to partners/opponents with diverse behaviors by
learning a conditional policy. However, when the sequential social
dilemma was not an equilibrium-selection problem, the learned
best response collapsed to one unconditional policy, leading to poor
zero-shot generalization
The paper is organized as follows. Section 2 outlines the method-
ology employed in the paper. In Subsection 2.1, we present the
formulation of the ğ‘-agent partially observable Markov process
used in the paper. Subsection 2.2 describes the Social Value Orien-
tation (SVO) framework and its implementation. In Subsection 2.3,
we discuss the various environments used in the study and their
characteristics. Subsection 2.4 details the procedure for generating
diverse policies in sequential social dilemmas. In Subsection 2.5, we
present the process for training a best response agent with a popula-
tion of agents and evaluating zero-shot generalization performance.Furthermore, we provide a description of the agentâ€™s architecture
in Subsection 2.5. Section 3 presents the results of the work. In
Subsection 3.1 and 3.2, we present the results obtained from gen-
erating diverse policies in environments with different incentive
structures. In Subsection 3.3, we present the results of zero-shot
generalization performance evaluation. Finally, in Section 4, we
provide additional discussions and conclusions. The section sum-
marizes the main contributions of the work and discuss potential
societal impacts.
2 METHODS
Train diverse 
bots Train a best 
response agent Evaluate ZSG 
performance of BR agent 
Train a population of bots 
with heterogeneous SVO Train a best response agent 
with diverse bots Test with a held out test 
population 
Trained Training Diverse bots BR agent Test population 
Figure 1: Overview of the methodology. Blue shapes show
agents that are actively being trained, whereas gray ones
denote frozen agents (bots). Circles represent the agents
trained with diverse SVO, triangles denote a best response
agent, and squares denote a held-out set of co-players. Evalu-
ation is zero-shot, meaning the best response agent is frozen
(gray triangle) and is evaluated against the held-out bots.
2.1ğ‘-agent POMDP
We consider a multi-agent partially observable Markov decision
process defined by the tupleD
ğ‘,S,A,R,ğ‘ƒ,ğ›¾E
,whereğ‘is the num-
ber of agents,Sis the joint state space, A=Ã—ğ‘
ğ‘–=1Ağ‘–is the joint
action space, ğ‘ƒis the state transition probability distribution, Ris
the reward function and ğ›¾is the discount factor. This can also be
referred to as a partially observable Markov game [ 16] or a partially
observable stochastic game [ 28]. At each time step ğ‘¡, each agent
ğ‘–âˆˆ1,...,ğ‘ observes a private (local) observation ğ‘œğ‘–
ğ‘¡and takes an
actionğ‘ğ‘–
ğ‘¡from a set of actions Ağ‘–. The joint action of all agents
at time step ğ‘¡is denoted as ğ‘ğ‘¡=(ğ‘1
ğ‘¡,...,ğ‘ğ‘
ğ‘¡). The stateğ‘ ğ‘¡is not
observed directly by the agents, instead the partial observation ğ‘œğ‘–
ğ‘¡
depends on the current state of the environment ğ‘ ğ‘¡and the agentâ€™s
observation function. The observation function for agent ğ‘–is de-
noted asğ‘‚ğ‘–(ğ‘œğ‘–
ğ‘¡|ğ‘ ğ‘¡). Each agent ğ‘–receives a reward ğ‘Ÿğ‘–
ğ‘¡which is a
function of the joint action ğ‘ğ‘¡and the state ğ‘ ğ‘¡of the environment.
The state of the environment transitions according to a probability
distribution ğ‘ƒ(ğ‘ ğ‘¡+1|ğ‘ ğ‘¡,ğ‘ğ‘¡).
The objective of each agent ğ‘–is to maximize their cumulative ex-
pected discounted reward, over a given finite time horizon, defined
asğ½ğ‘–=EÃğ‘‡
ğ‘¡=0ğ›¾ğ‘¡ğ‘Ÿğ‘–
ğ‘¡
, whereğ›¾âˆˆ[0,1]balances the importance of
immediate and future rewards. The agentsâ€™ policies are defined as
the mapping from the agentâ€™s observation history to an action, i.e.,ğœ‹ğ‘–(ğ‘ğ‘–
ğ‘¡|ğ‘œğ‘–
1,Â·Â·Â·,ğ‘œğ‘–
ğ‘¡). The policies are updated using a multi-agent rein-
forcement learning algorithm that maximizes the agentsâ€™ objective
functions.
2.2 Social Value Orientation
Omitting the dependence on ğ‘¡, letğ‘Ÿğ‘–be the reward of agent ğ‘–.Let
Â¯ğ‘Ÿâˆ’ğ‘–be the average reward of all the agent except agent ğ‘–.Then we
have
Â¯ğ‘Ÿâˆ’ğ‘–=1
ğ‘âˆ’1ğ‘âˆ‘ï¸
ğ‘—=1,ğ‘—â‰ ğ‘–ğ‘Ÿğ‘—.
Letğœƒğ‘–denote the SVO target angle of agent ğ‘–. Following the
definition given in [ 18], we define the effective reward Ë†ğ‘Ÿğ‘–of agentğ‘–
as
Ë†ğ‘Ÿğ‘–=ğ‘Ÿğ‘–cos(ğœƒğ‘–)+Â¯ğ‘Ÿâˆ’ğ‘–sin(ğœƒğ‘–).
While sometimes intrinsic rewards are temporally smoothed (e.g.[ 11]),
in this work, effective reward does not include any temporal smooth-
ing. Reintroducing the time step ğ‘¡from the previous section the
objective function agent ğ‘–optimizes for is
Ë†ğ½ğ‘–=E"ğ‘‡âˆ‘ï¸
ğ‘¡=0ğ›¾ğ‘¡Ë†ğ‘Ÿğ‘–
ğ‘¡#
.
2.3 Environments
We provide a brief description of the environments. For all exper-
iments in this paper, we use environments from Melting Pot 2.0
without modifications [1].
â€œin the matrixâ€ repeated games: The â€œin the matrixâ€ repeated
games are a family of sequential social dilemmas in Melting Pot 2.0
where two-players interact. In the beginning of each episode the
environment is initialized according to a given resource layout, and
a set of fixed points where players can spawn. The map consists of
two types of resources which can be distinguished by their colour;
red corresponds to defection and green corresponds to cooperation
(see Figure 3). Players can pick up resources by walking over them,
and these resources go into a player inventory. Players spawn with
one of each resource type in their inventory. After spawning, each
player can move around the map, collect resources, and interact
with the co-player by firing an interaction beam. When players
interact (by one player hitting the other using their interaction
beam), each player gets a reward equal to the expected payoff
calculated from the inventory counts and environment-specific
payoff matrix. The agent who zaps the other agent is considered
as the row player. The inventory count of each player defines a
mixed strategy where the probability of playing each pure strategy
is equivalent to the percentage of the corresponding resource. Let
ğ‘ğ‘–ğ‘Ÿandğ‘ğ‘–ğ‘”denote the inventory count, number of red resources
and green resources respectively, for agent ğ‘–âˆˆ1,2.For each agent
ğ‘–their mixed strategy is given as
ğ‘=hğ‘ğ‘–ğ‘Ÿ
ğ‘ğ‘–ğ‘Ÿ+ğ‘ğ‘–ğ‘”,ğ‘ğ‘–ğ‘”
ğ‘ğ‘–ğ‘Ÿ+ğ‘ğ‘–ğ‘”i
Letğ´be the payoff matrix for the game. Let ğ‘Ÿğ‘Ÿğ‘œğ‘¤andğ‘Ÿğ‘ğ‘œğ‘™be the
reward of row player and column player respectively. Let ğ‘ğ‘Ÿğ‘œğ‘¤andğ‘ğ‘ğ‘œğ‘™be the mixed strategy probability vector of row player and
column player respectively. Then the rewards can be defined as
ğ‘Ÿğ‘Ÿğ‘œğ‘¤=ğ‘ğ‘‡
ğ‘Ÿğ‘œğ‘¤ğ´ğ‘ğ‘ğ‘œğ‘™, ğ‘Ÿğ‘ğ‘œğ‘™=ğ‘ğ‘‡
ğ‘ğ‘œğ‘™ğ´ğ‘‡ğ‘ğ‘Ÿğ‘œğ‘¤
These reward calculations correspond to those used in game theory
for matrix games and iterated social dilemmas [32].
4 0
2 23 2
5 03 0
5 1Stag hunt Chicken Prisonerâ€™s dilemma 
Figure 2: Payoff matrices for Stag hunt, Chicken and Pris-
onerâ€™s dilemma. The values shown correspond to the payoff
of the row player. The payoff of the column player is the
transpose of the shown matrix (i.e. the games are symmet-
ric games). Cooperation corresponds to the first row and col-
umn. Defection corresponds to the send row and column.
The payoff matrices ğ´used are given in Figure 2. After inter-
acting, players receive their reward from interaction, freeze for 16
steps, and have their inventory counts reset (to one of each resource
type). And the end of the 16 steps players disappear and get re-
spawned after 5 steps. Players can have multiple interactions within
an episode. Once a resource is picked up, it begins to regenerate
after a delay of 10steps, with a 20% chance of regenerating on each
subsequent step. As is standard in Melting Pot 2.0, in each game,
there is a 10% chance that the episode will end after every 100steps,
with a minimum of 1000 steps per episode.
Externality mushrooms: Externality mushrooms is a sequential
social dilemma where players are immediately affected from proso-
cial or antisocial behaviors of their co-players. This is a 5-player
game where players eat mushrooms in order to receive rewards.
Four types of mushrooms grow (in different amounts) on the map:
red, green, blue, and orange. Eating a red (â€œfizeâ€: full internality
zero externality) mushroom gives a reward of 1to the player who
consumed the mushroom. Eating a green (â€œhiheâ€: half internality
half externality) mushroom gives a total reward of 2/5to all players .
Eating a blue (â€œzifeâ€: zero internality full externality) mushroom
gives a total reward of 3/4divided equally among all players exclud-
ing the player who consumed it . Eating an orange (â€œnizeâ€: negative
internality zero externality) mushroom causes red fize mushrooms
to be destroyed, each with probability 0.25, and gives a reward
ofâˆ’0.1to the player who consumed it. After eating a mushroom,
the player who consumed it freezes for the mushroomâ€™s digestion
time: 0(red), 10(green), 15(blue), and 15steps (orange). After
spawning, a mushroom is removed from the map after its perishing
time, i.e. the time it takes for the mushroom to disappear: 200(red),
100(green), and 75steps (blue). Orange mushrooms never perish.
Mushrooms respawn from spores depending on consumption of
other mushrooms. Eating a red, green, or blue mushroom releases 3
spores for red mushrooms, each spore will spawn a mushroom with
probability 0.25. Eating a green or blue mushrooms also releasesFigure 3: â€œIn the matrixâ€ repeated games. This is a 2-player
game where agents can gather 2 types of resources (green
corresponding to cooperation, red corresponding to defec-
tion). When agents interact (using an interaction beam) they
get rewards according to their inventory counts and a game
specific payoff matrix. The payoff matrix can be Stag hunt,
Chicken or Prisonerâ€™s dilemma type payoff matrix
3spores for green mushrooms which spawn with probability 0.4.
Eating a blue mushroom also releases a blue spore which spawn
with probability 0.6. Eating an orange mushroom releases a spore
for a new orange mushroom which spawns with probability 1. Sim-
ilar to â€œin the matrixâ€œ repeated games, in Externality mushrooms
each episode runs for at least 1000 steps. Following that the episode
terminates with probability 0.2at every 100steps.
Externality mushrooms has an incentive structure similar to
Chicken, where reward is maximized selfishly by consuming red
mushrooms while the others are consuming blue or green mush-
rooms. But if everyone else is eating red mushrooms, the selfish
strategy is to eat green mushrooms, as otherwise all mushrooms
would be eventually depleted.
2.4 Generating diverse policies in sequential
social dilemmas
In the beginning of the training process we define distinct SVO
angles for each agent. Each environment has a fixed number of
players. We train the agents in a distributed asynchronous manner
by initializing â€™arenasâ€™ to train a population of agents. Arenas run
in parallel and each arena is a copy of the environment with the
number of players specified for that environment. This is a multi-
agent version of A3C [ 21] that is commonly used for multi-agent
reinforcement learning [ 1]. The Melting Pot evaluation protocol
requires sampling of policies with replacement. Training in pure
self-play introduces skewed reward incentives by playing with
copies of oneself. To alleviate this issue, we set players in each
arena to play the game for one episode either in self-play or in
Figure 4: Externality Mushrooms. This is a 5-player sequen-
tial social dilemma game with immediate feedback. Agents
instantaneously share rewards with others depending on
the mushroom they are picking.
population-play (with equal probability). During population-play
we sample agents without replacement. We train each agent for
109learner frames.
2.5 Training a best-response agent and
zero-shot generalization performance
evaluation
We train a selfish naive learner without intrinsic reward, to best
respond against the policies generated using heterogeneous SVO.
In order to avoid confusion we use the term best-response agent
for the training agent, and SVO bots for the pre-trained diverse
agents trained with heterogeneous SVO values. In each episode the
best-response agent plays with a set of SVO bots sampled with-
out replacement. We train the best-response agent for 109learner
frames.
Melting Pot 2.0 [ 1] provides a protocol for evaluating generaliza-
tion to novel social partners, which are packaged with the suite as
a held-out set of co-players in a suite of test scenarios. We measure
the performance of the best-response agent using the Melting Pot
test protocol.
We use the Melting Pot test scenarios for evaluation in Stag hunt,
Chicken, Prisonersâ€™ dilemma â€œin the matrix â€œ repeated games and
Externality mushrooms. Test scenario details are provided below.
Test scenarios for â€œin the matrixâ€ repeated.
Focal player (our best response agent) encounters:
S0:(cooperator + defector) either a cooperator or a defector
with 0.5probability each
S1:(cooperator ) a cooperator
S2:(defector) a defector
S3:(grim strike 1) a player who starts by cooperating and
defect for the rest the episode when focal player defects onceS4:(grim strike 2) a player who starts by cooperating and
defect for the rest the episode when focal player defects
twice
S5:(tit-for-tat) a player who plays tit-for-tat
S6:(tit-for-tat tremble) a player who a player who plays
tit-for-tat and occasionally unconditionally defect. (noisy
tit-for-tat)
S7:(flipping) a player who cooperate during the first 3 inter-
actions and defect for the rest of the episode
S8:(corrigible tit-for-tat) a player who starts with defection
and switch to tit-for-tat strategy when best-response agent
defects
S9:(corrigible tit-for-tat tremble) a player who starts with
defection and switch to noisy tit-for-tat strategy when best-
response agent defects
Test scenarios for Externality mushrooms:
Focal player (our best response agent) encounters:
S0:(visiting cooperators) 4 cooperators
S1:(visiting defectors) 4 defectors
2 focal players (in our case 2 copies of best response agent) en-
counter:
S2:(resident cooperators) 3 cooperators
S3:(resident cooperators) 3 defectors
We provide an overview of the end to end methodological pipeline
in Figure 1.
2.6 Agent architecture
We trained the agents using the well-established Actor-Critic base-
line algorithm proposed in [ 5], building on the earlier work in [ 21]
named Asynchronous Advantage Actor Critic or A3C.
The neural network of the agent consists of two convolutional
layers, a two-layer perceptron, and an LSTMâ€”all separated by ReLU
activation functions. The convolutional layers have 16and32output
channels, kernel shapes of 8and4, and strides of 8and1. The
perceptron layers are 64neurons each, and the LSTM layer has 128
units. The policy and baseline for the critic are created by multilayer
perceptrons ( 256hidden units with ReLU activations) connected to
the output of the LSTM.
Representation shaping is achieved through the use of an auxil-
iary loss and contrastive predictive coding [ 23], which is used to
differentiate between nearby time points via LSTM state represen-
tations. PopArt [ 8] is used to adjust for the different reward scales
of the different environments. The optimization method used is
RMSProp with a learning rate of 4Ã—10âˆ’4, epsilon of 10âˆ’5, zero
momentum, decay of 0.99, and batch size of 256. The baseline cost
for the critic is 0.5, and the entropy regularization cost for the policy
is0.003.
1 2 3 4
Agent id0246Resource count 
(selfish baseline)
1 2 3 4
Agent id1 2 3 4
Agent id
1 
 (-15)
2 
 (0)
3 
 (60)
4 
 (75)
Agent id and SVO0246Resource count 
(SVO)
(a) Stag hunt1 
 (-15)
2 
 (0)
3 
 (60)
4 
 (75)
Agent id and SVO
(b) Chicken1 
 (-15)
2 
 (0)
3 
 (60)
4 
 (75)
Agent id and SVO
(c) Prisoner's dilemmadefective resource cooperative resourceFigure 5: â€œIn the matrixâ€ repeated games. Diversity of
policies of selfish-baseline bots and SVO bots. Each subfig-
ure shows average inventory counts during evaluation for
4 agents, trained with 50% self-play and 50% population
play. The bottom row corresponds to SVO bots with ğœƒğ‘–âˆˆ
{âˆ’15Â°,0Â°,60Â°,75Â°}and the top row corresponds to selfish-
baseline bots. Green and red represents cooperative and de-
fective resource counts respectively. Error bars show the
standard deviation of results over 3 random seeds.
3 RESULTS
3.1 Experiment 1: Generating diverse policies
in â€œin the matrixâ€ repeated games
Experimental setup: We consider Stag hunt, Chicken and Pris-
onersâ€™ dilemma â€œin the matrixâ€ repeated games. For each game we
average the results over 3 random seeds. We train four agents with
SVO values ofâˆ’15Â°,0Â°,60Â°, and 75Â°, respectively. These values were
chosen to cluster around the incentives of competition ( âˆ’15Â°), self-
ishness ( 0Â°) and pro-sociality ( 60Â°,75Â°). The â€œin the matrixâ€ repeated
games are 2-player games. In addition to SVO bots we also train
and freeze a set of selfish-baseline bots (i.e., no intrinsic reward)
using the same procedure for comparison.
Finding 1: Heterogeneous SVO bots learn meaningfully di-
verse policies
We use the inventory count of the bots at the time of interaction
as an environment-specific diversity measure. Since the inventory
counts define the mixed strategy probability vectors, sufficiently
distinct ratios of inventory counts indicate distinct mixed strategies.
During evaluation agents play in population-play.
Figure 5 shows the inventory counts for the 4bots averaged
over 500interactions during evaluation after the completion of
training. Top and bottom rows correspond to resource counts of
selfish-baseline bots and SVO bots respectively. Figures 5(a), 5(b)
and 5(c) correspond to Stag hunt, Chicken and Prisonersâ€™ dilemma
respectively. The error bars presented in the figure correspond to
the average results of 3independent runs. The results demonstrate
that in each game, all 4selfish-baseline bots have comparable in-
ventory count ratios, suggesting that their policies lack diversity.1 2 3 4 5
Agent id0.00.20.40.60.81.0Mushroom count 
(selfish baseline)
1 
 (-15)
2 
 (0)
3 
 (60)
4 
 (75)
5 
 (90)
Agent id and SVO0.00.20.40.60.81.0Mushroom count 
(SVO)fize hihe zife nizeFigure 6: Externality mushrooms. Diversity of policies of
selfish-baseline bots and SVO bots. Each plot shows average
fraction of mushrooms consumed by 5 agents during evalu-
ation, trained with 50% self-play and 50% population play in
Externality mushrooms dense game. The bottom row corre-
sponds to SVO agents with ğœƒğ‘–âˆˆ{âˆ’ 15Â°,0Â°,60Â°,75Â°,90Â°}and the
top row corresponds to selfish-baseline agents. Error bars
show the standard deviation of results over 3 random seeds.
Conversely, the 4SVO bots exhibit varied inventory count ratios, in-
dicating diverse behaviors. For each â€œin the matrixâ€œ repeated game,
resource counts correspond to SVO bots with ğœƒ=[âˆ’15Â°,0Â°,60Â°,75Â°],
whereğœƒğ‘–=ğœƒ[ğ‘–],forğ‘–âˆˆ{1,2,3,4}.We denote the cooperative re-
source counts and defective resource counts using green and red
respectively. As the SVO angles increase from âˆ’15Â°to75Â°, the ratio
between the red and green resource counts increases, indicating
more altruistic behavior.
3.2 Experiment 2: Generating diverse policies
in Externality Mushrooms
Experimental setup: Similar to the training process in â€œin the
matrixâ€œ repeated game we average the results from 3 random seeds.
For each seed we train 5agents with SVO values of âˆ’15Â°,0Â°,60Â°,75Â°,
and90Â°, respectively in 50%self-play and 50%population-play. In
addition to SVO bots we also train a set of selfish-baseline bots,
using the same procedure for comparison.
Finding 2: The results extends to multi-player games with
more than 2players
We show that our method scales to games with more than 2
players. Figure 6 shows that in Externality Mushrooms, agents
trained using heterogeneous SVO learn diverse policies. We use the
count of mushrooms consumed of each type as the environment-
specific diversity metric. The selfish-baseline bots tend to consumemushrooms at similar ratios across different types, whereas the
SVO bots consume varying ratios of different mushroom types
exhibiting meaningfully diverse behaviors. Agents with low (or
negative) SVO consume the selfish mushroom (red), and even the
spiteful mushroom (orange), whereas those with high SVO, tend to
consume more of the prosocial mushrooms (green and blue).
3.3 Experiment 3: Zero-shot generalization
evaluation
We evaluate the zero-shot generalization performance of a learned
best response to the SVO bots trained using heterogeneous SVO.
Baselines: We compare the performance of a learned best response
policy for SVO bots with a best response to selfish-baseline bots,
Fictitious co-play (FCP, a type of best response that includes also
earlier checkpoints of the agents to best respond to) [ 29], and ex-
ploiters (i.e., a best response agent trained on the test scenario
directly) [ 1]. We train one exploiter for each test scenario. To train
FCP agents we train a naive learning agent with 3checkpoints for
each bot from a bot population. Here we use the first checkpoint,
mid checkpoint and last checkpoint. The mid checkpoint is the time
during training where the agent first obtains half of its final reward,
of the policies of the bots. We report results for FCP applied to the
heterogeneous SVO bots FCP(SVO), as well as to selfish baselines
FCP(selfish-baseline). To evaluate zero-shot generalization, we also
compare the performance of best response agents with the perfor-
mance of selfish-baseline agents and random agents.
Experimental setup: We train best-response agents for the selfish-
baseline bots and SVO bots. Recall that we trained each type of
bots, i.e., selfish-baseline or SVO, for 3 random seeds in this setup.
We train a best-response agent for bots from each seed. For each
type of test bots we show the average performance evaluation runs
correspond to these 3training runs.
0.0 0.2 0.4 0.6 0.8 1.0
T est bot's cooperation probability0.00.20.40.60.81.0Best response agent's 
 cooperation probability
BR (svo)
FCP (svo)
BR (selfish-baseline)
FCP (selfish-baseline)
Figure 7: Comparing how well best-response agents learn con-
ditional policies in Stag hunt in the matrix.
Finding 3: Best-response agents learn a conditional behaviour
In order to get a better understanding about the learned policies0.0 0.2 0.4 0.6 0.8 1.0
T est bot's cooperation probability0.00.20.40.60.81.0Best response agent's 
 cooperation probability
BR (svo)
FCP (svo)
BR (selfish-baseline)
FCP (selfish-baseline)Figure 8: Comparing how well best-response agents learn con-
ditional policies in Chicken in the matrix.
of the best-response agents we analyze the behaviour of the best-
response agents during test time. For each test bot, Figures 7 and
8 show the fraction of interactions where the best-response agent
cooperated with a bot with respect to the fraction of interactions
where the bot cooperated with the best-response agent. Figure 7
corresponds to Stag hunt â€œin the matrixâ€œ repeated and 8 corresponds
to Chicken â€œin the matrixâ€ repeated.
In this analysis we define the best-response agentâ€™s interaction
as a cooperation when they have higher number of cooperative
resources than defective resources in their inventory at the time of
interaction. In Stag hunt in the matrix, both agents cooperating, i.e.,
both agents playing Stag, yields a higher reward, but it is a riskier
strategy. Defecting, yields a secure payoff. Both agents cooperating
or both defecting are Nash equilibria, that is, there is no incentive
to unilaterally deviate from that strategy. An agent who cooper-
ates with a defector gets 0 reward. When trained in Stag hunt in
the matrix, selfish-baseline bots learn to defect. The best response
to unconditional defectors is defecting. Hence the best-response
agents trained with selfish-baseline bots learn to unconditionally
defect. In contrast the heterogeneous SVO bot population consists
of both defectors and cooperators with different levels of cooper-
ation and defection. Best-response agents training with SVO bots
encounter both cooperators and defectors and subsequently learn
a conditional policy that tends to cooperate with cooperators and
defect with defectors.
In Chicken in the matrix, the two Nash equilibria are for one
agent to cooperate (swerve) and the other agent to defect (straight).
In this case selfish-baseline agents learn to do both defection and
cooperation. Hence the best-response agents trained with selfish-
baseline bots also learn to defect and cooperate. However in Figure
8 we see that this behaviour is not conditional. In contrast best-
response agents training with SVO bots encounter mostly coop-
erative and mostly defective bots, leading to best-response agents
learning a conditional behavior where they tend to cooperate with
defectors and defect against cooperators.Finding 4: Failure case with Prisonerâ€™s dilemma In Prisonerâ€™s
dilemma in the matrix, the Nash equilibrium is both agents de-
fecting, as a result selfish-baseline agents learn to unconditionally
defect. Thus, the best response agents that are trained with self-
ish agents also learn to defect. Moreover, defection is also a best
response to unconditional cooperation. Because SVO bots learned
only unconditional strategies (either cooperate or defect), the best
response to SVO bots is also to unconditionally defect. Figure 9 il-
lustrates this showing that all the best-response agents are learning
to defect regardless of the level of cooperation of their partners.
0.0 0.2 0.4 0.6 0.8 1.0
T est bot's cooperation probability0.00.20.40.60.81.0Best response agent's 
 cooperation probability
BR (svo)
FCP (svo)
BR (selfish-baseline)
FCP (selfish-baseline)
Figure 9: Comparing how well best-response agents learn con-
ditional policies in Prisonerâ€™s dilemma in the matrix.
Finding 5: Best response agents perform better in zero-shot
generalization Zero-shot generalization performance of the best
response agents, selfish-baseline agent, random agent, and exploiters
are given in Table 1. Each agent type is run on each test scenario and
their average returns are calculated. The score is normalized across
agents for each scenario where the best agent receives a score of
1, and the worst a score of 0. The final score of an agent is their
average over all scenarios. This is the same method used in Melting
Pot [ 1]. The exploiters and random agent are intended to provide
approximate upper and lower bounds for performance across all en-
vironments. As expected the table shows that the exploiters achieve
the best performance, while the random agent performs the worst.
Across all environments at least one best response agent performs
better than the selfish-baseline agent indicating that learning a best
response improves zero-shot generalization.
On average in the Stag hunt in the matrix scenarios, BR(SVO)
outperforms other agents. From Figure 7 we see that BR(SVO) and
FCP(SVO) cooperate with unconditional defectors with a small
probability. However, in Stag hunt in the matrix, an agent coop-
erating with a defector or defecting with a defector receives the
same reward. Thus when encountering defectors and test bots that
are more likely to defect BR(SVO), FCP(SVO) receives comparable
rewards to BR(selfish-baseline) and FCP(selfish-baseline). When
encountering more cooperative test bots, best response agents thatBR(SVO) FCP(SVO) BR(selfish-baseline) FCP(selfish-baseline) selfish-baseline random exploiter
Stag hunt ITMR 0.876 0.830 0.856 0.847 0.850 0.000 0.988
Chicken ITMR 0.696 0.668 0.745 0.723 0.723 0.000 0.958
Prisonerâ€™s dilemma ITMR 0.738 0.702 0.777 0.783 0.754 0.000 1.000
Externality mushrooms 0.619 0.764 0.612 0.846 0.660 0.000 0.900
Table 1: Zero-shot generalization performance of best response agents, selfish-baseline agent, random agent and exploiter.
The score is calculated by first re scaling the rewards received by each agent such that in each scenario the agent with high-
est(lowest) reward gets score 1(0) and then averaging over all scenarios for each environment.
are able to adapt to partner behaviours and cooperate with cooper-
ators receive a higher reward. This leads to the higher score of the
BR(SVO) agent in Stag hunt in the matrix.
Table 1 shows that in Chicken in the matrix scenarios, BR(selfish-
baseline) outperforms other agents. Note that in Chicken in the
matrix, an agent cooperating with a defector receives a higher re-
ward than an agent defecting against a defector. From results in
Figure 8 we see that when test bots defect with a probability close
to1all the best response agents cooperate with similar probabili-
ties. Thus in scenarios where test bots are unconditionally defect-
ing all the best response agents obtain comparable performance.
However, when test bots are cooperating with about 0.4 probabil-
ity, BR(selfish-baseline) and FCP(selfish-baseline) cooperate with a
higher matching probability compared to BR(SVO) and FCP(SVO)
thus leading to better performance for BR(selfish-baseline) and
FCP(selfish-baseline). In scenarios where best response agents en-
counter unconditional cooperators BR(SVO) and FCP(SVO) defect
with a probability close to 1obtaining better performance compared
to BR(selfish-baseline) and FCP(selfish-baseline). Since most of the
test scenarios consist of defectors or test bots that are more likely
to defect, this leads to BR(selfish-baseline) outperforming BR(SVO)
and BR(FCP) agents.
Recall that Figure 9 illustrates that all the best-response agents
are defecting against all test bots. Thus we expect the performance
score of best response agents for Prisonerâ€™s dilemma in the matrix
given in Table 1 to be similar. However, surprisingly BR(selfish-
baseline) and FCP(selfish-baseline) perform better than BR(SVO)
and FCP(SVO). We leave investigating this as future work.
In Externality mushrooms, FCP type best response agents per-
form better than best response agents trained with only final poli-
cies of the co-players. This indicates that best response agents that
encounter less proficient agents as well as more proficient agents
perform better than the best response agents that only encounter
proficient agents during training time.
4 DISCUSSION
In this paper we investigated the impact of heterogeneous Social
Value Orientation on different incentive structures in sequential
social dilemmas. We tested whether the presence of heterogeneous
SVO leads to diverse policies and if learning a best response to these
policies improves zero-shot generalization. The study found that
the presence of heterogeneous SVO does indeed lead to measurable
diversity in policies, and this diversity sometimes results in better
zero-shot generalization for agents that best respond to them.The best-response agents achieve better performance by learn-
ing a conditional policy that adapts to novel agents during test time.
The study also revealed that when the sequential social dilemma
is not an equilibrium-selection problem, this method still gener-
ates meaningful diversity in policies, but it fails to achieve better
zero-shot generalization performance. This occurs because the best
response to a diverse set of policies collapses to one unconditional
policy that performs poorly when encountering conditional policies
during test time.
Our findings have implications for understanding how heteroge-
neous SVO impacts incentive structures and policy diversity, and
how agents can learn to adapt to diverse policies during test time to
achieve better zero-shot generalization performance. Our findings
provide new insights into the behavior of agents in sequential social
dilemmas and highlights the importance of considering the role of
heterogeneity in SVO in the design of incentive structures.
We observed that SVO agents were able to learn cooperative
policies in all of the environments we tested. This hints at the
potential value of using SVO to capture at least some of the aspects
necessary to align agents with human values.ACKNOWLEDGEMENT
Authors thank Daniel Hennes, DJ Strouse, Marc Lanctot, Alexander
(Sasha) Vezhnevets, Julien Perolat, Andrea Tacchetti and Ian Gemp
for helpful discussion.
REFERENCES
[1]John P Agapiou, Alexander Sasha Vezhnevets, Edgar A DuÃ©Ã±ez-GuzmÃ¡n, Jayd
Matyas, Yiran Mao, Peter Sunehag, Raphael KÃ¶ster, Udari Madhushani, Kavya
Kopparapu, Ramona Comanescu, et al .2022. Melting Pot 2.0. arXiv preprint
arXiv:2211.13746 (2022).
[2]David Balduzzi, Marta Garnelo, Yoram Bachrach, Wojciech Czarnecki, Julien
Perolat, Max Jaderberg, and Thore Graepel. 2019. Open-ended learning in sym-
metric zero-sum games. In International Conference on Machine Learning . PMLR,
434â€“443.
[3]Kenneth Derek and Phillip Isola. 2021. Adaptable agent populations via a gen-
erative model of policies. Advances in Neural Information Processing Systems 34
(2021), 3902â€“3913.
[4]Edgar A DuÃ©Ã±ez-GuzmÃ¡n, Kevin R McKee, Yiran Mao, Ben Coppin, Silvia Chi-
appa, Alexander Sasha Vezhnevets, Michiel A Bakker, Yoram Bachrach, Suzanne
Sadedin, William Isaac, et al .2021. Statistical discrimination in learning agents.
arXiv preprint arXiv:2110.11404 (2021).
[5]Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Vlad Mnih, Tom
Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, et al .2018. Impala:
Scalable distributed deep-rl with importance weighted actor-learner architectures.
InInternational conference on machine learning . PMLR, 1407â€“1416.
[6]Marta Garnelo, Wojciech Marian Czarnecki, Siqi Liu, Dhruva Tirumala, Junhyuk
Oh, Gauthier Gidel, Hado van Hasselt, and David Balduzzi. 2021. Pick Your
Battles: Interaction Graphs as Population-Level Objectives for Strategic Diversity.
InProceedings of the 20th International Conference on Autonomous Agents and
MultiAgent Systems . 1501â€“1503.
[7]Donald W Griesinger and James W Livingston Jr. 1973. Toward a model of
interpersonal motivation in experimental games. Behavioral science 18, 3 (1973),
173â€“188.
[8]Matteo Hessel, Hubert Soyer, Lasse Espeholt, Wojciech Czarnecki, Simon Schmitt,
and Hado van Hasselt. 2019. Multi-task deep reinforcement learning with popart.
InProceedings of the AAAI Conference on Artificial Intelligence , Vol. 33. 3796â€“3803.
[9]Hengyuan Hu, Adam Lerer, Brandon Cui, Luis Pineda, Noam Brown, and Jakob
Foerster. 2021. Off-belief learning. In International Conference on Machine Learn-
ing. PMLR, 4369â€“4379.
[10] Hengyuan Hu, Adam Lerer, Alex Peysakhovich, and Jakob Foerster. 2020. â€œOther-
Playâ€ for Zero-Shot Coordination. In International Conference on Machine Learn-
ing. PMLR, 4399â€“4410.
[11] Edward Hughes, Joel Z Leibo, Matthew Phillips, Karl Tuyls, Edgar DueÃ±ez-
Guzman, Antonio GarcÃ­a CastaÃ±eda, Iain Dunning, Tina Zhu, Kevin McKee,
Raphael Koster, et al .2018. Inequity aversion improves cooperation in intertem-
poral social dilemmas. Advances in neural information processing systems 31
(2018).
[12] Joel Z Leibo, Edgar A DueÃ±ez-Guzman, Alexander Vezhnevets, John P Agapiou,
Peter Sunehag, Raphael Koster, Jayd Matyas, Charlie Beattie, Igor Mordatch, and
Thore Graepel. 2021. Scalable evaluation of multi-agent reinforcement learning
with melting pot. In International Conference on Machine Learning . PMLR, 6187â€“
6199.
[13] Joel Z Leibo, Vinicius Zambaldi, Marc Lanctot, Janusz Marecki, and Thore Graepel.
2017. Multi-agent Reinforcement Learning in Sequential Social Dilemmas. In
Proceedings of the 16th Conference on Autonomous Agents and MultiAgent Systems .
464â€“473.
[14] Adam Lerer and Alexander Peysakhovich. 2017. Maintaining cooperation in
complex social dilemmas using deep reinforcement learning. arXiv preprint
arXiv:1707.01068 (2017).
[15] Wim BG Liebrand and Charles G McClintock. 1988. The ring measure of social
values: A computerized procedure for assessing individual differences in infor-
mation processing and social value orientation. European journal of personality
2, 3 (1988), 217â€“230.
[16] Michael L Littman. 1994. Markov games as a framework for multi-agent rein-
forcement learning. In Machine learning proceedings 1994 . Elsevier, 157â€“163.
[17] Andrei Lupu, Brandon Cui, Hengyuan Hu, and Jakob Foerster. 2021. Trajectory
diversity for zero-shot coordination. In International Conference on Machine
Learning . PMLR, 7204â€“7213.
[18] Kevin R McKee, Xuechunzi Bai, and Susan T Fiske. 2022. Warmth and Competence
in Human-Agent Cooperation. In Proceedings of the 21st International Conference
on Autonomous Agents and Multiagent Systems . 898â€“907.
[19] Kevin R McKee, Ian Gemp, Brian McWilliams, Edgar A DuÃ¨Ã±ez-GuzmÃ¡n, Edward
Hughes, and Joel Z Leibo. 2020. Social Diversity and Social Preferences in
Mixed-Motive Reinforcement Learning. In Proceedings of the 19th International
Conference on Autonomous Agents and MultiAgent Systems . 869â€“877.[20] Kevin R McKee, Joel Z Leibo, Charlie Beattie, and Richard Everett. 2022. Quanti-
fying the effects of environment and population diversity in multi-agent rein-
forcement learning. Autonomous Agents and Multi-Agent Systems 36, 1 (2022),
1â€“16.
[21] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Tim-
othy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. 2016. Asyn-
chronous methods for deep reinforcement learning. In International conference
on machine learning . PMLR, 1928â€“1937.
[22] Ryan O Murphy, Kurt A Ackermann, and Michel JJ Handgraaf. 2011. Measuring
social value orientation. Judgment and Decision making 6, 8 (2011), 771â€“781.
[23] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learning
with contrastive predictive coding. arXiv preprint arXiv:1807.03748 (2018).
[24] Nicolas Perez-Nieves, Yaodong Yang, Oliver Slumbers, David H Mguni, Ying Wen,
and Jun Wang. 2021. Modelling behavioural diversity for learning in open-ended
games. In International Conference on Machine Learning . PMLR, 8514â€“8524.
[25] Alexander Peysakhovich and Adam Lerer. 2018. Consequentialist conditional
cooperation in social dilemmas with imperfect information (short workshop
version). In Workshops at the Thirty-Second AAAI Conference on Artificial Intelli-
gence .
[26] Anatol Rapoport. 1974. Prisonerâ€™s dilemmaâ€”recollections and observations. In
Game Theory as a Theory of a Conflict Resolution . Springer, 17â€“34.
[27] Wilko Schwarting, Alyssa Pierson, Javier Alonso-Mora, Sertac Karaman, and
Daniela Rus. 2019. Social behavior for autonomous vehicles. Proceedings of the
National Academy of Sciences 116, 50 (2019), 24972â€“24978.
[28] Lloyd S Shapley. 1953. Stochastic games. Proceedings of the national academy of
sciences 39, 10 (1953), 1095â€“1100.
[29] DJ Strouse, Kevin McKee, Matt Botvinick, Edward Hughes, and Richard Everett.
2021. Collaborating with humans without human data. Advances in Neural
Information Processing Systems 34 (2021), 14502â€“14515.
[30] Zhenggang Tang, Chao Yu, Boyuan Chen, Huazhe Xu, Xiaolong Wang, Fei Fang,
Simon Shaolei Du, Yu Wang, and Yi Wu. 2021. Discovering Diverse Multi-Agent
Strategic Behavior via Reward Randomization. In International Conference on
Learning Representations .
[31] Jane X Wang, Edward Hughes, Chrisantha Fernando, Wojciech M Czarnecki,
Edgar A DuÃ©Ã±ez-GuzmÃ¡n, and Joel Z Leibo. 2018. Evolving intrinsic motivations
for altruistic behavior. arXiv preprint arXiv:1811.05931 (2018).
[32] JÃ¶rgen W Weibull. 1997. Evolutionary game theory . MIT press.
[33] Tom Zahavy, Yannick Schroecker, Feryal Behbahani, Kate Baumli, Sebastian
Flennerhag, Shaobo Hou, and Satinder Singh. 2022. Discovering policies with
domino: Diversity optimization maintaining near optimality. arXiv preprint
arXiv:2205.13521 (2022).