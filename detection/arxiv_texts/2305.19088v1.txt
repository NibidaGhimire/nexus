TrueDeep: A systematic approach of crack detection with less data
Ramkrishna Pandeya, Akshit Acharaa
aGE Research, Bangalore, India
ARTICLE INFO
Keywords :
Cracks
Semantic Segmentation
Deep Learning
Computer Vision
Image ProcessingABSTRACT
Supervised and semi-supervised semantic segmentation algorithms require significant amount of
annotated data to achieve a good performance. In many situations, the data is either not available
or the annotation is expensive. The objective of this work is to show that by incorporating domain
knowledgealongwithdeeplearningarchitectures,wecanachievesimilarperformancewithlessdata.
Wehaveusedpubliclyavailablecracksegmentationdatasetsandshownthatselectingtheinputimages
using knowledge can significantly boost the performance of deep-learning based architectures. Our
proposed approaches have many fold advantages such as low annotation and training cost, and less
energy consumption. We have measured the performance of our algorithm quantitatively in terms of
meanintersectionoverunion(mIoU)andFscore.Ouralgorithms,developedwith23%oftheoverall
data; have a similar performance on the test data and significantly better performance on multiple
blind datasets.
1. Introduction
Structural health monitoring is crucial for ensuring the
safety and longevity of infrastructures such as roads, build-
ings, bridges, gas turbines, wind turbines, and pipelines.
One of the key aspects of this monitoring is the detection
and characterization of cracks, which can be a sign of po-
tential failures in these structures. Traditionally, crack de-
tection involves manual inspection of structures, which can
be time-consuming, labor-intensive, expensive, and error-
prone. With the rapid development of computer vision and
machinelearningalgorithms,automaticcrackdetectionsys-
temshaveemergedasanefficientandreliablealternative.In
this manuscript, we present a systematic approach of crack
detection with less data using computer vision techniques,
with a focus on segmentation-based methods. We discuss
the challenges and opportunities in this field, provide a
critical analysis of existing techniques, and highlight poten-
tial directions for future research. The ultimate goal of this
manuscript is to provide a useful resource for researchers,
engineers, and practitioners interested in automatic crack
detection for structural health monitoring.
Image processing based techniques have emerged as
one of the most effective methods for crack detection due
to their ability to analyze large datasets quickly and ac-
curately. Among these approaches, thresholding techniques
have gained popularity as they enable the extraction of
crack pixels from an image by setting a suitable threshold.
Dynamic thresholding is often used to adjust the threshold
value,makingthealgorithmadaptabletovariationsinimage
quality and crack size and shape.
The Canny edge detector algorithm [1] is widely used
in image processing based crack detection due to its high
accuracy in detecting edges. However, the algorithm’s sen-
sitivity to noise can cause issues with detecting edges in
noisy images. As a result, noise reduction techniques are
typically applied before the Canny algorithm to improve its
ramp@alum.iisc.ac.in (R. Pandey);
f2016953p@alumni.bits-pilani.ac.in (A. Achara)performance. Despite these measures, challenges remain in
using the Canny algorithm for crack detection, particularly
when dealing with variations in image quality and texture
[2].
Energy minimization methods, such as active contours,
are commonly used for image segmentation tasks like con-
tourdetection,whichcanalsobeappliedtocrackdetection.
In[3],aninitialestimateofcrackregionswasobtainedfrom
a crack saliency map. This map was generated by applying
a steerable matched filter to the image, which enhanced the
contrast between the cracks and the surrounding pavement.
This initial estimate was fed into an active contour-based
method, resulting in a better estimate of the cracks.
However,theperformanceofthesemethodsdependson
an initial estimate of the crack regions and the choice of
multiple parameters such as regularization terms and focus
parameters to converge towards the inside or outside of
the region. The output of steerable matched filtering also
depends on the choice of parameters such as filter size and
scale tuning, which can be difficult to choose for varying
images and complex features.
It should be noted that image processing based ap-
proaches depend heavily on the selection of appropriate
parameters for each specific image. These parameters are
typically selected based on factors such as image texture,
color, brightness, scale, and noise levels. Proper selection
of these parameters is essential to achieving accurate and
reliable results.
Machine learning models like random forests [4], sup-
port vector machines [5], etc can be trained to identify
crackanditscharacteristicsusingdata,whichinvolveshand-
crafting features and using these features for classifying
into crack vs non-crack categories. These approaches are
howeversensitivetonoiseanddifferentbrightnesslevels[6].
Recently, deep learning based algorithms have gained
popularity as they perform well on multiple tasks such as
classification, segmentation, instance segmentation, object
detection etc. Crack detection algorithms fall usually fall
in one of these categories. One of the recent works in this
Pandey et al.: Preprint submitted to Elsevier Page 1 of 9arXiv:2305.19088v1  [cs.CV]  30 May 2023TrueDeep
directionwhichshowpromisingresultsisadeephierarchical
CNN based approach (DeepCrack) for crack segmentation
[7]. The method achieves an F-Score of 86.5 and outper-
forms the previous methods by a significant margin. The
latestresultsontheirsourcerepository[8]showanimproved
F-Scoreof86.84usingguidedfiltersalongwithDeepCrack
architecture (DeepCrack-GF). In [9], the authors address
a very relevant problem of dealing with partially accu-
rate ground truths (specificially for thin cracks where the
"All Black" phenomenon occurs) by using crack-patch-only
(CPO)supervisedgenerativeadversariallearningforend-to-
endtrainingtogeneratedata.Itcomparestheperformanceof
crackGAN with several other CNN based models including
DeepCrack[7] and mentions that the DeepCrack[7] training
relies on accurate GTs and the method fails when the GTs
are biased. In can be seen in the tables 2a, 2b, 3a and 3b
that the performance of DeepCrack is better on thick cracks
(crack500)ascomparedtothincracks(cracktree200).How-
ever,theabovemethodrequireslargeamountoftrainingdata
tobeabletogenerateimagesandthefocusoftheworkison
thin cracks.
Overall, there is a scope of further improvements in the
domain of crack detection that calls for further research.
Many of the deep learning methods used for crack seg-
mentation use localized parameters like dataset dependent
thresholdswhichhassomelimitationsonthegeneralization
and automation. The amount to data that is required to be
annotationinorderforreachsufficientmodelgeneralization
also calls for some research.
In our work, we address the above issues by proposing
a method that requires less data and annotations, along
with several proposed augmentation approaches (discussed
insection2.4)forbettergeneralization(seetables2a,2b,3a
and 3b).
1.1. Contributions
Following are our main contributions:
•Proposed an approach to select a subset of images
from a given dataset to train a model that achieves
a similar or better performance as compared to the
same model trained using the entire dataset (see sec-
tion 2.2.1 for further details).
•We have trained models with almost1
4of the overall
DeepCrack[5] data that achieve better or comparable
results than the state of the art crack detection tech-
niques on the same dataset. (see table 1)
•We have shown the effectiveness of our models by
testingonmultipleblindtestdatasets[10].Ourresults
are better or comparable to the state of the art tech-
niques (trained on the same DeepCrack[7] dataset as
ourmodels)intheliteratureonthesedatasets[10].See
tables 2a, 2b, 3a and 3b.
•We have also proposed data augmentation strate-
gies(sw, sl, ss, mix) that improve the model perfor-
mance on the blind datasets/unseen data. For more
details, refer section 2.4.2. Dataset
DeepCrack [7] dataset is used for all our experiments
which has 300 train and 237 test images. This is a public
benchmark dataset with cracks in multiple scales and back-
grounds. The training images are of size 384×544 and the
test images are of sizes 384×544 and544×384 . We used
this dataset for building our models.
2.1. Blind Test Dataset
The kaggle crack-segmentation-dataset [10] which con-
tains the datasets listed in the Table 2a (except for the ’non-
crack’and’deepcrack’imagesthatarepresentintheoverall
kaggle dataset). It is to be noted that we have used the test
split of all these datasets. These datasets are used to blindly
testthemodelperformanceandcontainimagesofsize 448×
448with varying texture, color, crack characteristics, etc.
2.2. TrueSet
We followed a strategy discussed in the section 2.2.1
to sub-sample 63 train and 7 validation images from the
overalldataset[7]whichcontains300trainimages.Thesub-
sampled dataset is hereafter referred as trueset. The model
obtained after training on trueset is hereafter referred as
truecrack. The model trained on trueset is able to perform
comparable with the state of the art methods in literature.
2.2.1. True Image Selection
We have used prinical component analysis (PCA) to
obtain our trueset (containing 70 images from 300 Deep-
Crack images) which approximates the distribution of over-
all DeepCrack data. We have used the encoder outputs
obtained from a 2D UNet [11] with an EfficientNetB0 [12]
backbonepretrainedonimagenet[13].PCAisappliedonall
the outputs to get the most significant dimension using the
algorithm1.Weseparatetheimagesintodifferentbinsusing
hist function from matplotlib [14] and calculate the number
of images to pick from each bin using the algorithm 2. We
finally select the train and validation images from each bin
with a 90-10 train-validation ratio using the algorithm 3.
Figures 2 and 3 suggest that trueset follows similar
distributiontotheoverallDeepCrackimagesvisuallywhere
the former is a plot of images using the first component
obtained from PCA and the latter is a plot of images using
the first 2 components obtained from PCA.
It can be observed in Fig. 4 that the number of trueset
train images and trueset validation images present in each
bin are proportional to the number of overall DeepCrack
train images. This proportionality demonstrates the sim-
ilarity in the distribution of original training dataset i.e.
DeepCrack train set and trueset.
2.2.2. Feature Representation
The proposed true image selection approach uses the
outputs from EfficientNetB0 backbone that is pretrained
on imagenet, to obtain a feature represenation of the input
images. Since the backbone was pretrained to learn fea-
ture representation of vast number of images with varying
Pandey et al.: Preprint submitted to Elsevier Page 2 of 9TrueDeep
Algorithm 1 Encoder Outputs to Coordinates (Coordinate
Mapping): The algorithm returns a mapping of images and
the corresponding first coordinate as obtained by PCA.
procedure ENCODEROUTPUTS(𝑖𝑚𝑎𝑔𝑒𝑠)
𝑜𝑢𝑡𝑝𝑢𝑡𝑠 ←Φ
for𝑖𝑚𝑎𝑔𝑒in𝑖𝑚𝑎𝑔𝑒𝑠 do
𝑜𝑢𝑡𝑝𝑢𝑡 ←𝑒𝑛𝑐𝑜𝑑𝑒𝑟(𝑖𝑚𝑎𝑔𝑒)
𝑜𝑢𝑡𝑝𝑢𝑡_𝑓𝑙𝑎𝑡←𝑐𝑜𝑛𝑣𝑒𝑟𝑡_𝑡𝑜_𝑓𝑙𝑎𝑡(𝑜𝑢𝑡𝑝𝑢𝑡)
⊳map image to flat encoder output
𝑜𝑢𝑡𝑝𝑢𝑡𝑠 ←𝑜𝑢𝑡𝑝𝑢𝑡𝑠∪{𝑜𝑢𝑡𝑝𝑢𝑡_𝑓𝑙𝑎𝑡}
end for
⊳apply PCA to get the set of first dimension coordinates
𝑓𝑖𝑟𝑠𝑡_𝑐𝑜𝑜𝑟𝑑𝑖𝑛𝑎𝑡𝑒𝑠 ←𝑃𝐶𝐴(𝑜𝑢𝑡𝑝𝑢𝑡𝑠,𝑑𝑖𝑚𝑠 =1)
⊳𝑖𝑚𝑎𝑔𝑒𝑠is a set of all input images
⊳𝑖𝑚𝑎𝑔𝑒𝑠maps to𝑓𝑖𝑟𝑠𝑡_𝑐𝑜𝑜𝑟𝑑𝑖𝑛𝑎𝑡𝑒𝑠 (one-to-one mapping)
𝑐𝑜𝑜𝑟𝑑_𝑚𝑎𝑝∶𝑖𝑚𝑎𝑔𝑒𝑠 →𝑓𝑖𝑟𝑠𝑡_𝑐𝑜𝑜𝑟𝑑𝑖𝑛𝑎𝑡𝑒𝑠
return𝑐𝑜𝑜𝑟𝑑_𝑚𝑎𝑝
end procedure
Start
Input Images Algorithm 1
Coordinate Mapping Algorithm 2
Select Mapping, Bin Mapping Algorithm 3
trueset
Stop
Figure 1: The flowchart shows the process of True Image
Selection. The Input Images are the 300 DeepCrack train
images used as an input for algorithm 1 which results in a
mapping of the image and its corresponding coordinate of
the first dimension on applying PCA. The image-coordinate
mapping is the input for algorithm 2 which results in bins
containingmutuallyexclusiveimagesetsobtainedbasedonthe
distance of an image from the mean distance over all images
and the number of images to select from each of those bins.
The algorithm 3 takes in these results from algorithm 2 as
inputs to produce a new train and validation set defined as
trueset (see section 2.2).
backgrounds and classes, the output feature representations
capture both the local and global contexts from varying the
input crack images with different local and global features.
This representation is then used as an input to PCA to
further process the representation as shown in the previous
section. The proposed algorithms applied on the condensedAlgorithm 2 DistanceBasedSelection:Thealgorithmsorts
the images in order of its distances from the mean first
coordinate and number of images to select from each bin.
procedure GETSELECTION PARAMETERS (𝑐𝑜𝑜𝑟𝑑_𝑚𝑎𝑝)
⊳𝑐𝑜𝑜𝑟𝑑_𝑚𝑎𝑝∶𝑖𝑚𝑎𝑔𝑒𝑠 →𝑓𝑖𝑟𝑠𝑡_𝑐𝑜𝑜𝑟𝑑𝑖𝑛𝑎𝑡𝑒𝑠 (algorithm 1)
𝑛𝑢𝑚_𝑖𝑚𝑎𝑔𝑒𝑠 ←|𝑖𝑚𝑎𝑔𝑒𝑠|
⊳average coordinate value of all the images
𝑎𝑣𝑔←1
𝑛𝑢𝑚_𝑖𝑚𝑎𝑔𝑒𝑠∑𝑛𝑢𝑚_𝑖𝑚𝑎𝑔𝑒𝑠
𝑖=1𝑓𝑖𝑟𝑠𝑡_𝑐𝑜𝑜𝑟𝑑𝑖𝑛𝑎𝑡𝑒𝑠
𝑑𝑖𝑠𝑡𝑎𝑛𝑐𝑒𝑠 ←Φ
⊳calculate the distance from 𝑎𝑣𝑔for each image
for all𝑐𝑜𝑜𝑟𝑑∈𝑓𝑖𝑟𝑠𝑡_𝑐𝑜𝑜𝑟𝑑𝑖𝑛𝑎𝑡𝑒𝑠 do
𝑑𝑖𝑠𝑡𝑎𝑛𝑐𝑒𝑠 ←𝑑𝑖𝑠𝑡𝑎𝑛𝑐𝑒𝑠∪{√
(𝑐𝑜𝑜𝑟𝑑−𝑎𝑣𝑔)2)}
end for
⊳we have selected a value of 10 for our experiments
𝑛_𝑏𝑖𝑛𝑠←10
⊳𝑛is a list of count of the values in each bin
⊳𝑏𝑖𝑛𝑠contains the edges of the bins. (Contains the lower
⊳and upper limit of each bin)
𝑛,𝑏𝑖𝑛𝑠 ←ℎ𝑖𝑠𝑡(𝑑𝑖𝑠𝑡𝑎𝑛𝑐𝑒𝑠,𝑛 _𝑏𝑖𝑛𝑠)
⊳𝑖𝑑𝑥is a set of indices of bins
𝑖𝑑𝑥←{0,1,2,3,4,5,6,7,8,9}
𝑖𝑚𝑔_𝑏𝑖𝑛𝑠←𝑔𝑒𝑡_𝑖𝑚𝑎𝑔𝑒𝑠_𝑖𝑛_𝑒𝑎𝑐ℎ_𝑏𝑖𝑛(𝑏𝑖𝑛𝑠)
⊳A map of bin index and set of images in that bin
𝑏𝑖𝑛_𝑚𝑎𝑝𝑝𝑖𝑛𝑔∶𝑖𝑑𝑥→𝑖𝑚𝑔_𝑏𝑖𝑛𝑠
⊳A list of bin indices in descending order of number of
⊳images in the bins
𝑖𝑑𝑥_𝑑𝑒𝑠𝑐𝑒𝑛𝑑𝑖𝑛𝑔 ←𝑠𝑜𝑟𝑡_𝑑𝑒𝑠𝑐𝑒𝑛𝑑𝑖𝑛𝑔(𝑛)
⊳selection parameter ∈[0,1](0.5 was used for this paper)
𝑠←0.5
⊳Reduction fraction to reduce the images to select
𝑑𝑒𝑐←𝑠
𝑛_𝑏𝑖𝑛𝑠−1
⊳A map of bin index and |𝑖𝑚𝑎𝑔𝑒𝑠|to be selected from that bin
𝑠𝑒𝑙𝑒𝑐𝑡_𝑚𝑎𝑝𝑝𝑖𝑛𝑔 ←ℝ1↦ℝ1
𝑠𝑒𝑙𝑒𝑐𝑡 ←⌊𝑛𝑢𝑚_𝑖𝑚𝑎𝑔𝑒𝑠×𝑠
𝑛_𝑏𝑖𝑛𝑠−1⌉
for𝑏𝑖𝑛𝑖𝑑𝑥∈𝑖𝑑𝑥_𝑑𝑒𝑠𝑐𝑒𝑛𝑑𝑖𝑛𝑔 do
if𝑠>0then
𝑠𝑒𝑙𝑒𝑐𝑡_𝑚𝑎𝑝𝑝𝑖𝑛𝑔[𝑏𝑖𝑛𝑖𝑑𝑥]←𝑠𝑒𝑙𝑒𝑐𝑡
𝑠←𝑠−𝑑𝑒𝑐
𝑠𝑒𝑙𝑒𝑐𝑡 ←⌊𝑛𝑢𝑚_𝑖𝑚𝑎𝑔𝑒𝑠×𝑠
𝑛_𝑏𝑖𝑛𝑠−1⌉
end if
end for
return𝑏𝑖𝑛_𝑚𝑎𝑝𝑝𝑖𝑛𝑔,𝑠𝑒𝑙𝑒𝑐𝑡 _𝑚𝑎𝑝𝑝𝑖𝑛𝑔
end procedure
outputs obtained from PCA result in separate groups of
similar crack images. It is to be noted that there are other
variants of EfficientNet which are more complex and give
richer feature representations but since the task at hand was
crack segmentation which doesn’t include multiple objects,
we have chosen a ligther variant.
2.3. AllSet
We split the DeepCrack train images into 270 train and
30validationimagesusingthe90-10ratiohereafterreferred
Pandey et al.: Preprint submitted to Elsevier Page 3 of 9TrueDeep
Algorithm 3 True Image Selection: Returns trueset which
hasadistributionsimilartothatofDeepCracktraindataset.
procedure SELECTTRUEIMAGES(𝑏𝑖𝑛_𝑚𝑎𝑝,𝑠𝑒𝑙𝑒𝑐𝑡 _𝑚𝑎𝑝)
⊳Φrefers to empty set
𝑖𝑚𝑎𝑔𝑒𝑠𝑡𝑟𝑎𝑖𝑛,𝑖𝑚𝑎𝑔𝑒𝑠𝑣𝑎𝑙←Φ
for(𝑏𝑖𝑑𝑥,𝑏𝑖𝑚𝑔)∈𝑏𝑖𝑛_𝑚𝑎𝑝 do
𝑐𝑙𝑒𝑎𝑛𝑒𝑑 ←Φ
𝑡𝑟𝑎𝑖𝑛 ←Φ
𝑣𝑎𝑙←Φ
if𝑏_𝑖𝑚𝑔=Φthen
do nothing
else
𝑠𝑒𝑙𝑒𝑐𝑡 ←𝑠𝑒𝑙𝑒𝑐𝑡_𝑚𝑎𝑝[𝑏𝑖𝑑𝑥]
if𝑠𝑒𝑙𝑒𝑐𝑡> |𝑏𝑖𝑚𝑔|then
𝑡𝑜𝑡𝑎𝑙 ←|𝑏𝑖𝑚𝑔|
else
𝑡𝑜𝑡𝑎𝑙 ←|𝑠𝑒𝑙𝑒𝑐𝑡|
end if
𝑛𝑡𝑟𝑎𝑖𝑛←⌊𝑡𝑜𝑡𝑎𝑙×0.90)⌉
𝑛𝑣𝑎𝑙←(𝑡𝑜𝑡𝑎𝑙−𝑛𝑡𝑟𝑎𝑖𝑛)
if𝑛𝑣𝑎𝑙>0then
𝑗𝑢𝑚𝑝 ←⌊|𝑏𝑖𝑚𝑔|
𝑛𝑣𝑎𝑙⌉
for𝑧←0to𝑛_𝑣𝑎𝑙do
𝑣𝑎𝑙𝑖𝑑𝑥←⌊𝑧×𝑗𝑢𝑚𝑝+𝑗𝑢𝑚𝑝
2⌉
𝑣𝑎𝑙←𝑣𝑎𝑙∪{𝑏𝑖𝑚𝑔[𝑣𝑎𝑙𝑖𝑑𝑥]}
𝑐𝑙𝑒𝑎𝑛𝑒𝑑 ←𝑏𝑖𝑚𝑔−{𝑏𝑖𝑚𝑔[𝑣𝑎𝑙𝑖𝑑𝑥]}
end for
end if
if𝑛𝑡𝑟𝑎𝑖𝑛>0then
𝑗𝑢𝑚𝑝 ←⌊|𝑐𝑙𝑒𝑎𝑛𝑒𝑑 |
𝑛𝑡𝑟𝑎𝑖𝑛⌉
else
𝑗𝑢𝑚𝑝 ←0
end if
if𝑗𝑢𝑚𝑝=1then
𝑡𝑟𝑎𝑖𝑛 ←𝑡𝑟𝑎𝑖𝑛∪𝑐𝑙𝑒𝑎𝑛𝑒𝑑
else if𝑗𝑢𝑚𝑝>1then
for𝑧←0to𝑛𝑡𝑟𝑎𝑖𝑛do
𝑡𝑟𝑎𝑖𝑛𝑖𝑑𝑥←⌊𝑧×𝑗𝑢𝑚𝑝+𝑗𝑢𝑚𝑝
2⌉
𝑡𝑟𝑎𝑖𝑛 ←𝑡𝑟𝑎𝑖𝑛∪{𝑐𝑙𝑒𝑎𝑛𝑒𝑑[𝑡𝑟𝑎𝑖𝑛𝑖𝑑𝑥]}
end for
end if
𝑖𝑚𝑎𝑔𝑒𝑠𝑡𝑟𝑎𝑖𝑛←𝑖𝑚𝑎𝑔𝑒𝑠𝑡𝑟𝑎𝑖𝑛∪𝑡𝑟𝑎𝑖𝑛
𝑖𝑚𝑎𝑔𝑒𝑠𝑣𝑎𝑙←𝑖𝑚𝑎𝑔𝑒𝑠𝑣𝑎𝑙∪𝑣𝑎𝑙
end if
end for
𝑡𝑟𝑢𝑒𝑠𝑒𝑡 ←𝑖𝑚𝑎𝑔𝑒𝑠𝑡𝑟𝑎𝑖𝑛∪𝑖𝑚𝑎𝑔𝑒𝑠𝑣𝑎𝑙
return𝑡𝑟𝑢𝑒𝑠𝑒𝑡
end procedure
as allset.The modeltrained onallset ishereafter referredas
allcrack.
2.4. Augmentations
We performed some augmentations on the trueset 2.2
(discussedinsubsections2.4.1,2.4.2,2.4.3,and2.4.4)using
(i) All the deepcrack
train images.
(ii) AllSet 2.3
 (iii) TrueSet 2.2
Figure 2: Shows the plots of first component of encoder
outputs obtained after applying PCA. See algorithm 1 for more
details.
(i) All the deepcrack
train images.
(ii) AllSet 2.3
 (iii) TrueSet 2.2
Figure 3: Shows the plots of first 2 components of encoder
outputs obtained after applying PCA.
Figure 4: Figure shows the plot of the number of overall
DeepCrack train images, trueset train images and trueset
validation images present in each bin. The blue line shows
the number of images present in each bin of the train split
of the overall DeepCrack dataset (see section 2); the orange
line shows the number of trueset train images selected from
each bin; the green line shows the number of trueset validation
images selected from each bin. See section 2.2 and 2.2.1 for
more details.
domain knowledge before training the models. No augmen-
tation was performed on the validation images.
2.4.1. Stochastic width
Stochastic width augmentation is performed by dilating
the ground truth using three different kernels of sizes 3×3,
5 × 5and8 × 8respectively as shown in the figure 5.
The augmented dataset contains 63(trueset images) + 3×
63(augmented images) = 252training images along with 7
validation images. For more details, refer [15]. The model
trained on this dataset is hereafter referred as swcrack.
2.4.2. Stochastic Length
In this approach, we perform the masking on ground
truth as described in the algorithm 4. We selected points
Pandey et al.: Preprint submitted to Elsevier Page 4 of 9TrueDeep
Figure 5: Shows (a) original image, (b) ground truth mask, (c)
ground truth mask dilated with kernel of size 3 ×3, (d) ground
truth mask dilated with kernel of size 5 ×5 and (e) ground truth
mask dilated with kernel of size 8 ×8.
Algorithm 4 Random Masking
C←𝑙𝑖𝑠𝑡𝑜𝑓𝑐𝑜𝑛𝑛𝑒𝑐𝑡𝑒𝑑𝑐𝑜𝑚𝑝𝑜𝑛𝑒𝑛𝑡𝑠
𝑚𝑎𝑠𝑘 ←𝑔𝑟𝑜𝑢𝑛𝑑_𝑡𝑟𝑢𝑡ℎ
for all𝑐∈𝐶do
𝑎←𝑎𝑟𝑒𝑎(𝑐)
if𝑎≤𝑡0then
do nothing
else
⊳Get the range of sqaure mask side
𝑟𝑎𝑛𝑔𝑒 ←𝑔𝑒𝑡𝑠𝑖𝑑𝑒𝑟𝑎𝑛𝑔𝑒 (c)
⊳Estimate a polygon around the component 𝑐
𝑝𝑜𝑙𝑦𝑔𝑜𝑛 ←𝑃𝑜𝑙𝑦𝑔𝑜𝑛(c)
if𝑎≤𝑡1then
𝑛𝑝𝑡𝑠∈{1,2,3}
else if𝑎≤𝑡2then
𝑛𝑝𝑡𝑠∈{2,3,4,5}
else
𝑛𝑝𝑡𝑠∈{5,6,7,8}]
end if
⊳randomly get 𝑛𝑝𝑡𝑠within the polygon
𝑝𝑡𝑠←𝑟𝑝𝑤𝑖𝑡ℎ𝑖𝑛𝑝𝑜𝑙𝑦𝑔𝑜𝑛 (𝑝𝑜𝑙𝑦𝑔𝑜𝑛,𝑛𝑝𝑡𝑠 )
⊳Remove the square mask pixels for each point
for all𝑝∈𝑝𝑡𝑠do
𝑥←𝑝.𝑥
𝑦←𝑝.𝑦
𝑠𝑖𝑑𝑒∈[𝑟𝑎𝑛𝑔𝑒.𝑠𝑡𝑎𝑟𝑡..𝑟𝑎𝑛𝑔𝑒.𝑒𝑛𝑑 ]
𝑔𝑡_𝑚𝑎𝑠𝑘 ←𝑔𝑒𝑡_𝑠𝑞𝑢𝑎𝑟𝑒(𝑠𝑖𝑑𝑒)
𝑚𝑎𝑠𝑘 ←𝑚𝑎𝑠𝑘−𝑔𝑡_𝑚𝑎𝑠𝑘
end for
end if
end for
(rangingfrom1to3forcomponentsofareabetween 𝑡0=50
and𝑡1= 100, 2 to 5 for compontents of area between 𝑡1
and𝑡2= 200and 5 to 8 for components of area greater
than𝑡2) randomly within each connected component in
the ground truth and apply square masks with those points
as the center of the squares. The augmentation results in
two masks for each input image as shown in figure 6.
The augmented dataset contains 63(trueset images) + 63
(augmentedimages) =126trainingimagesandmasksalong
with7validationimagesandmasks.Themodeltrainedusing
this dataset is hereafter referred as slcrack.
Figure 6: Shows (a) original image, (b) ground truth mask,
and (c) randomly masked ground truth.
2.4.3. Stochastic Scale Space
We resized the ground truth mask 4 times the initial
dimensions using the ’INTER_CUBIC’ [16] interpolation.
Post that, we performed three dilation with kernels of sizes
3 × 3,5 × 5and8 × 8respectively as shown in the fig-
ure 7. Then resized back each of the dilated images to
the original dimension using the ’INTER_NEAREST’ [16]
(nearest-neighbour) interpolation as shown in the figure 8.
The dilation is applied in a different scale to preserve the
sharpness in the crack boundaries. The method allows us to
use a dilation kernel of fractional values on downscaling to
the initial size of the image.
The augmented dataset contains 63(trueset images) +
3 × 63(augmented images) = 252training images and
masksalongwith7validationimagesandmasks.Themodel
trained on this dataset will be referred as sscrack in the
further sections.
Figure 7: Shows (a) original image, (b) ground truth, (c)
upscaled and dilated ground truth with a kernel of 3 ×3, (d)
upscaled and dilated ground truth with a kernel of 5 ×5 and
(e) upscaled and dilated ground truth with a kernel of 8 ×8.
2.4.4. Combining length and width stochasticity: Mix
Based on our experiments, we have observed that the
stochasticwidthmethodachieveshighrecallwhereasstochas-
tic length method high precision (see table 1 and ROC
figure 14). We have tried to balance the precision and recall
by combining stochastic width and length approaches. We
have obtained 3 augmentations from the input ground truth
mask; the mask obtained after dilating the original mask
with a kernel of sizes 3 × 3and5 × 5respectively (for
width stochasticity), and a randomly masked ground truth
as discussed in section 2.4.2 (for length stochasticity). The
augmenation is shown in the figure 9.
Pandey et al.: Preprint submitted to Elsevier Page 5 of 9TrueDeep
Figure 8: Shows (a) original image, (b) ground truth, (c)
Scaled-Dilated GT 3 from fig. 7 downscaled to the same size as
ground truth, (d) Scaled-Dilated GT 5 from fig. 7 downscaled
to the same size ground truth and (e) Scaled-Dilated GT 8
from fig. 7 downscaled to the same size as ground truth. See
section 2.4.3 for more details.
The augmented dataset contains 63(trueset images)
+3 × 63(augmented images) = 252training images and
masksalongwith7validationimagesandmasks.Themodel
trained on this dataset is hereafter referred as mixcrack.
Figure 9: Shows (a) original image, (b) ground truth, (c)
groundtruthmaskdilatedwithakernelofsize3 ×3,(d)ground
truth mask dilated with a kernel of size 5 ×5 and (e) randomly
masked ground truth.
3. Training
We have used 2D UNet architecture [11] with a back-
bone, wherein weights are initialized with EfficientNetB0
[12] which is pretrained with imagenet [13] dataset.
Thelossfunctionusedisbinaryfocaldiceloss(equation
3) which is a sum of binary focal loss [17] (equation 1) and
dice loss [18] (equation 2).
𝐿1(𝐺,𝑃)=−𝐺𝛼(1−𝑃)𝛾log(𝑃)−(1−𝐺)𝛼𝑃𝛾log(1−𝑃)
(1)
𝐿2(𝑝,𝑟)=1−(1+ 𝛽2)𝑝⋅𝑟
𝛽2⋅𝑝+𝑟(2)
𝐿𝑡𝑜𝑡𝑎𝑙=𝐿1(𝐺,𝑃)+𝐿2(𝑝,𝑟) (3)
𝐺,𝑃,𝛼,and𝛾arethegroundtruth,prediction,weighting
factor and the parameter to decide the downweight amount
respectively.Highervaluesof 𝛾indicateslowerdownweight.
Wehaveused 𝛼=0.5and𝛾=3.33inourlossfunction. 𝑝=
𝑇𝑃
𝑇𝑃+𝐹𝑃and𝑟=𝑇𝑃
𝑇𝑃+𝐹𝑁areprecisionandrecallrespectively
where𝑇𝑃,𝐹𝑃and𝐹𝑁are true positives, false postives
and false negatives respectively. We have used 𝛽=1in our
experiments.
Figure 10: Shows architecture of the 2D UNet with Efficient-
NetB0 backbone used for all our experiments. (zoom to see
details)
The batch size used for training is 8 and validation is 4.
The augmentations used during training are: flip, rotate,
shift-scale-rotate, shear, translate, downscale, clahe, gaus-
sian blur, median blur and sharpen using albumentations
[19]. We randomly select one augmentation from the list of
augmentations mentioned above to apply on each input im-
ageduringthetrainingprocess.Additionalknowledgebased
augmentations performed before training are discussion in
the section 2.4.
Initial learning rate of 1𝑒− 3is reduced using ’Re-
duceLROnPlateau’ from keras [20] by a factor of 0.5 till
1𝑒−6whenever the validation loss is not reducing for 50
continuous epochs.
4. Experiments
We evaluated all our models on the DeepCrack test
dataset and blind test datasets (refer section 2.1) with-
out any preprocessing on images. We compared our re-
sults with DeepCrack-BN and DeepCrack-GF models (pre-
trained model downloaded from here [8]) using the testing
scripts and framework provided in [8]. For more details on
DeepCrack-GF, see Figure 11 The evaluation metrics used
are global accuracy (G), class average accuracy (C), mean
intersection over union (mIoU) over all classes, Precision
Pandey et al.: Preprint submitted to Elsevier Page 6 of 9TrueDeep
Figure 11: Show the process of obtaining the DeepCrack-GF
output as discussed in [7].
(P), Recall (R) and F-score (F) same as in [7]. Threshold is
termed as T [7].
5. Results and Discussions
5.1. Qualitative Analysis
Fig. 12i is obtained at the best thresholds and fig. 12ii
is obtained at a threshold of 0.5, show comparison of our
results (e to j) with DeepCrack-BN and DeepCrack-GF ap-
proaches (c and d) on the DeepCrack dataset. It can be
observed that our approaches have better predictions of
cracks (zoom to see the finer details (top portion) of fig-
ure 12i, cracks are detected better in our techniques). Fig-
(a) Image
 (b) GT
 (c) DeepCrack-GF
 (d) DeepCrack-BN
 (e) AllCrack
 (f) TrueCrack
 (g) SW
 (h) SL
 (i) SS
 (j) MIX
(i) The results are calculated by applying a threshold on the
predictions that results in the highest F score.
(a) Image
 (b) GT
 (c) DeepCrack-GF
 (d) DeepCrack-BN
 (e) AllCrack
 (f) TrueCrack
 (g) SW
 (h) SL
 (i) SS
 (j) MIX
(ii)Theresultsarecalculatedbyapplyingathresholdof0.5onthe
predictions.
Figure 12: Figures show (a) original image, (b) ground truth,
(c) DeepCrack-GF output, (d) DeepCrack-BN output, (e)
allcrack output, (f) truecrack output, (g) swcrack output, (h)
slcrack output, (i) sscrack output and (j) mixcrack output on
the DeepCrack [7] dataset. Zoom to see the details.
ures13i,13ii,13iiiand13ivshowcomparisonofourresults
(e to j) with DeepCrack-BN and DeepCrack-GF approaches
(candd)onablinddataset(seesection2.1).Ourtechniques
are more robust to background variations and have better
crack detectability at both thresholds whereas DeepCrack
results are having noisy predictions (when using the best
thresholds)andaremissing(whenusingastandardthreshold
of 0.5).
(a) Image
 (b) GT
 (c) DeepCrack-GF
 (d) DeepCrack-BN
 (e) AllCrack
 (f) TrueCrack
 (g) SW
 (h) SL
 (i) SS
 (j) MIX(i) The results are calculated by applying a threshold on the
predictions that results in the highest F score (see table 3a).
(a) Image
 (b) GT
 (c) DeepCrack-GF
 (d) DeepCrack-BN
 (e) AllCrack
 (f) TrueCrack
 (g) SW
 (h) SL
 (i) SS
 (j) MIX
(ii)Theresultsarecalculatedbyapplyingathresholdof0.5onthe
predictions.
(a) Image
 (b) GT
 (c) DeepCrack-GF
 (d) DeepCrack-BN
 (e) AllCrack
 (f) TrueCrack
 (g) SW
 (h) SL
 (i) SS
 (j) MIX
(iii) The results are calculated by applying a threshold on the
predictions that results in the highest F score (see table 3a).
(a) Image
 (b) GT
 (c) DeepCrack-GF
 (d) DeepCrack-BN
 (e) AllCrack
 (f) TrueCrack
 (g) SW
 (h) SL
 (i) SS
 (j) MIX
(iv)Theresultsarecalculatedbyapplyingathresholdof0.5onthe
predictions.
Figure 13: Figures show (a) original image, (b) ground truth,
(c) DeepCrack-GF output, (d) DeepCrack-BN output, (e)
allcrack output, (f) truecrack output, (g) swcrack output, (h)
slcrack output, (i) sscrack output and (j) mixcrack output on
the kaggle crack-segmentation dataset (Volker).
5.2. ROC and PR Curves
ToplotReceiverOperatingCharacteristics(ROC)(fig.14i)
we have used sklearn [21] library and the Precision-Recall
(PR) (fig. 14ii) curve is plotted manually using thresholds
ranging from 0 to 0.99 with a step size of 0.01. The area
under curve (AUC) for DeepCrack-BN is more than other
techniques (see captions for more details).
In the PR-curve plots, the curve for DeepCrack-GF is
plotted by using a threshold1 (see figure 11) value of 0.31
that results in the highest F-score for the DeepCrack-BN
approach (see table 1a). For a better comparison, we have
alsoplotted thecurveforDeepCrack-GF isplottedbyusing
athreshold1valueof 0.5.ThisplotislabelledasDeepCrack-
GF-T1_5.
Pandey et al.: Preprint submitted to Elsevier Page 7 of 9TrueDeep
(i) DeepCrack-BN has the highest AUC value of 0.984. It is to be
notedthatswcrackhasabetterAUCthantheDeepCrack-GF-T1_5
method. The scale on x-axis is 0-0.2 for clear visibility.
0.0 0.2 0.4 0.6 0.8 1.0
Recall0.00.20.40.60.81.0Precision[F=.837], [F_5=.837] sscrack
[F=.812], [F_5=.791] swcrack
[F=.868], [F_5=.868] DeepCrack-GF
[F=.839], [F_5=.815] slcrack
[F=.863], [F_5=.850] DeepCrack-GF-T1_5
[F=.852], [F_5=.829] DeepCrack-BN
[F=.821], [F_5=.819] mixcrack
[F=.864], [F_5=.851] truecrack
[F=.854], [F_5=.834] allcrack
(ii) Precision-Recall curve comparison of our techniques with
DeepCrack approaches on the DeepCrack dataset. F value is ob-
tainedatthethresholdthatresultsinthehighestF-scoreandF_5is
obtained at the threshold of 0.5. The truecrack approach achieves
a F-score of 85.1 which is comparable to DeepCrack-GF-T1_5.
Figure 14: ROC and PR curves on the DeepCrack dataset. We
haveusedathreshold1valueof0.5(seefigure11)forobtaining
DeepCrack-GF-T1_5 curve(see section 5.2 for details).
The ROC curve shown in figure 14i suggests that our
models generate less false positives consistently. Our false
positive rate changes marginally with different thresholds.
This suggests that all the crack probabilities generated by
ourmodelsarestrongcandidatesforcrackpixels(canalsobe
validatedfromtheresultsshownintables3aand3bwherein
thehighestF-scoresareobtainedaroundathresholdof 0.0).5.3. Quantitative Analysis
Tables1band1ashowthecomparisonofourtechniques
with DeepCrack-BN and DeepCrack-GF on the DeepCrack
dataset. The proposed truecrack approach has the highest
F-score of 85.1 %when thresholds of 0.5 were used on the
predictions. The proposed swcrack approach achieves the
highestrecallof88 %whereastheallcrackapproachachieves
the highest precision of 94.3 %. See captions for more de-
tails on the performance of different approaches using the
thresholdsresultingthebestF-scores.Tables2aand2bshow
the comparison of our techniques with DeepCrack-BN and
DeepCrack-GF on blind datasets(see section 2.1). It can be
observed that the proposed swcrack model F-score is better
than all the other methods on 5 datasets namely, Rissbilder,
SylvieChambon[22],Volker,GAPs[23]andEugenMuller.
See captions of tables 2a, 2b, 3a and 3b.
6. Conclusion
Imagesegmentationisoneofthecommonlyusedsuper-
visedtechniquesforcrackdetectionthatrequirelabellingof
each crack pixel by human experts which is expensive and
timeconsuming.Figures(12i-13iv)andtables(1a-3b)qual-
itativelyandquantitativelyshowthatourresultsarecompa-
rabletothestateofthearttechniquesevenafterreducingthe
trainingdatasignificantly.Ourtrueimageselectionapproach
discussed in section 2.2.1 has many fold advantages (a)
helps in reducing the number of images to be annotated for
supervised machine learning, (b) reducing training cost and
time, (c) reducing carbon consumption due to less resource
utilization [24]. Using knowledge based augmentation, we
have further improved our model performance significantly
as discussed in section 5.
7. Acknowledgments
TheauthorswouldliketothankGEResearchforprovid-
ing the resources to enable us to do this work.
Pandey et al.: Preprint submitted to Elsevier Page 8 of 9TrueDeep
References
[1] Canny, John. "A computational approach to edge de-
tection." IEEE Transactions on pattern analysis and
machine intelligence 6 (1986): 679-698.
[2] Zhao, Huili, Guofeng Qin, and Xingjian Wang. "Im-
provement of canny algorithm based on pavement
edge detection." 2010 3rd international congress on
image and signal processing. Vol. 2. IEEE, 2010.
[3] Li, Shuai, Yang Cao, and Hubo Cai. "Automatic
pavement-crack detection and segmentation based
on steerable matched filtering and an active contour
model." Journal of Computing in Civil Engineering
31.5 (2017): 04017045.
[4] Chen, Jieh-Haur, et al. "A self organizing map op-
timization based image recognition and processing
model for bridge crack inspection." Automation in
Construction 73 (2017): 58-66.
[5] Hoang,Nhat-Duc,Quoc-LamNguyen,andDieuTien
Bui. "Image processing–based classification of as-
phalt pavement cracks using support vector machine
optimized by artificial bee colony." Journal of Com-
puting in Civil Engineering 32.5 (2018): 04018037.
[6] Hamishebahar, Younes, et al. "A comprehensive re-
viewofdeeplearning-basedcrackdetectionapproaches."
Applied Sciences 12.3 (2022): 1374.
[7] Liu, Yahui, et al. "DeepCrack: A deep hierarchical
feature learning architecture for crack segmentation."
Neurocomputing 338 (2019): 139-153.
[8] "DeepSegmentor".https://github.com/yhlleo/DeepSegmentor,
accessed 2021
[9] Zhang, Kaige, Yingtao Zhang, and Heng-Da Cheng.
"CrackGAN:Pavementcrackdetectionusingpartially
accurategroundtruthsbasedongenerativeadversarial
learning." IEEE Transactions on Intelligent Trans-
portation Systems 22.2 (2020): 1306-1319.
[10] ’Kaggle Crack Segmentation Dataset’,
https://www.kaggle.com/datasets/lakshaymiddha/crack-
segmentation-dataset, accessed 2020
[11] Ronneberger,Olaf,PhilippFischer,andThomasBrox.
"U-net: Convolutional networks for biomedical im-
age segmentation." Medical Image Computing and
Computer-AssistedIntervention–MICCAI2015:18th
InternationalConference,Munich,Germany,October
5-9, 2015, Proceedings, Part III 18. Springer Interna-
tional Publishing, 2015.
[12] Tan, Mingxing, and Quoc Le. "Efficientnet: Rethink-
ingmodelscalingforconvolutionalneuralnetworks."
Internationalconferenceonmachinelearning.PMLR,
2019.
[13] Deng, Jia, et al. "Imagenet: A large-scale hierarchical
imagedatabase."2009 IEEEconferenceoncomputer
vision and pattern recognition. Ieee, 2009.
[14] Hunter, John D. "Matplotlib: A 2D graphics envi-
ronment." Computing in science & engineering 9.03
(2007): 90-95.[15] Pandey, R. K., & Achara, A. (2022). CoreDeep: Im-
proving Crack Detection Algorithms Using Width
Stochasticity. arXiv preprint arXiv:2209.04648.
[16] Bradski, Gary. "The openCV library." Dr. Dobb’s
Journal:SoftwareToolsfortheProfessionalProgram-
mer 25.11 (2000): 120-123.
[17] Lin, Tsung-Yi, et al. "Focal loss for dense object
detection."ProceedingsoftheIEEEinternationalcon-
ference on computer vision. 2017.
[18] Sudre, C. H., Li, W., Vercauteren, T., Ourselin, S., &
Jorge Cardoso, M. (2017). Generalised dice overlap
asadeeplearninglossfunctionforhighlyunbalanced
segmentations. In Deep learning in medical image
analysisandmultimodallearningforclinicaldecision
support (pp. 240-248). Springer, Cham.
[19] Buslaev, Alexander, et al. "Albumentations: fast and
flexibleimageaugmentations."Information11.2(2020):
125.
[20] Chollet,F.,&others."Keras".GitHub.Retrievedfrom
https://github.com/fchollet/keras, accessed 2015
[21] Pedregosa,Fabian,etal."Scikit-learn:MachineLearn-
ing in Python, Journal of Machine Learning Re-
search, 12." (2011): 2825.
[22] Amhaz, Rabih, et al. "Automatic crack detection
on two-dimensional pavement images: An algorithm
based on minimal path selection." IEEE Transactions
on Intelligent Transportation Systems 17.10 (2016):
2718-2729.
[23] Eisenbach, Markus, et al. "How to get pavement dis-
tress detection ready for deep learning? A system-
aticapproach."2017internationaljointconferenceon
neural networks (IJCNN). IEEE, 2017.
[24] Patterson,D.,Gonzalez,J.,Le,Q.,Liang,C.,Munguia,
L.M., Rothchild, D., So, D., Texier, M. & Dean, J.
(2021). Carbon emissions and large neural network
training. arXiv preprint arXiv:2104.10350.
8. Appendices
The tables below demonstrate the detailed quantitative
results of the study(discussed in section 5.3). See captions
of the respective tables for more details.
Pandey et al.: Preprint submitted to Elsevier Page 9 of 9TrueDeep
T G C mIoU P R F
DeepCrack-GF 0.48 0.989 0.928 0.878 0.879 0.858 0.868
DeepCrack-BN 0.31 0.987 0.92 0.864 0.858 0.846 0.852
allcrack 0.01 0.988 0.913 0.866 0.879 0.831 0.854
truecrack 0.03 0.988 0.920 0.874 0.883 0.845 0.864
swcrack 0.99 0.984 0.898 0.833 0.820 0.804 0.812
slcrack 0.01 0.987 0.901 0.854 0.873 0.807 0.839
sscrack 0.25 0.986 0.905 0.853 0.858 0.817 0.837
mixcrack 0.92 0.986 0.905 0.853 0.858 0.817 0.837
(a) DeepCrack-GF has the highest Recall and our truecrack has the highest Precision whereas DC-GF and truecrack have a comparable
F-score. we have used a threshold1 of 0.31 and threshold2 of 0.48 (see figure 11) for obtaining DeepCrack-GF outputs.
G C mIoU P R F
DeepCrack-GF 0.988 0.889 0.863 0.930 0.782 0.850
DeepCrack-BN 0.987 0.874 0.847 0.925 0.751 0.829
allcrack 0.987 0.873 0.851 0.943 0.748 0.834
truecrack 0.988 0.891 0.864 0.930 0.784 0.851
swcrack 0.980 0.932 0.817 0.719 0.880 0.791
slcrack 0.986 0.859 0.837 0.939 0.720 0.815
sscrack 0.986 0.900 0.853 0.870 0.806 0.837
mixcrack 0.984 0.906 0.838 0.818 0.819 0.819
(b) Precision, Recall and F-score are better in our techiques; we have used a threshold1 and threshold2 of 0.5 (see figure 11) for obtaining
DeepCrack-GF outputs.
Table 1
Table (a) shows quantitative comparison of results obtained using our techniques with that of the DeepCrack approaches; these
results are obtained using a threshold that maximizes the F-score and is obtained using grid search(from 0.0 to 0.99 with a
step-size of 0.01) on the DeepCrack test dataset as reported in [5], whereas in table (b) we have used a fixed threshold of 0.5
(standard threshold for binary classification) to obtain all the results.
Dataset DeepCrack-GF DeepCrack-BN allcrack truecrack swcrack slcrack sscrack mixcrack
Rissbilder 0.278 0.154 0.267 0.255 0.417 0.304 0.380 0.411
Sylvie Chambon 0.134 0.070 0.086 0.082 0.215 0.069 0.145 0.187
Volker 0.319 0.220 0.331 0.318 0.586 0.326 0.403 0.492
cfd 0.573 0.392 0.512 0.455 0.570 0.409 0.595 0.576
crack500 0.593 0.561 0.450 0.193 0.460 0.237 0.376 0.403
GAPs 0.207 0.170 0.199 0.287 0.322 0.188 0.308 0.281
Eugen Muller 0.138 0.067 0.184 0.122 0.496 0.151 0.273 0.326
forest 0.615 0.478 0.522 0.505 0.589 0.406 0.607 0.573
cracktree200 0.215 0.171 0.276 0.240 0.206 0.226 0.246 0.166
(a)TheallcrackapproachhasthehighestF-scoreoncracktree200dataset;swcrackon5datasets;sscrackapproachoncfddataset;overall,
our techniques are better on 7 datasets and DeepCrack-GF is better on 2 datasets namely, crack500 and forest.
DeepCrack-GF DeepCrack-BN allcrack truecrack swcrack slcrack sscrack mixcrack
Rissbilder 0.567 0.528 0.565 0.561 0.620 0.577 0.606 0.618
Sylvie Chambon 0.498 0.480 0.487 0.486 0.528 0.482 0.505 0.518
Volker 0.577 0.543 0.582 0.577 0.694 0.580 0.610 0.648
cfd 0.696 0.616 0.667 0.642 0.692 0.623 0.707 0.696
crack500 0.687 0.673 0.622 0.526 0.625 0.540 0.591 0.602
GAPs 0.547 0.538 0.548 0.577 0.588 0.544 0.583 0.575
Eugen Muller 0.513 0.493 0.530 0.511 0.648 0.519 0.559 0.578
forest 0.717 0.652 0.672 0.664 0.702 0.622 0.713 0.695
cracktree200 0.555 0.543 0.577 0.565 0.552 0.561 0.565 0.536
(b)TheallcrackapproachhasthehighestmIoUoncracktree200dataset;swcrackapproachon5datasets;sscrackapproachoncfddataset;
overall, our techniques are better on 7 datasets and DeepCrack-GF is better on 2 datasets namely, crack500 and forest.
Table 2
Tables (a) and (b) show the F1 and mIoU comparision of our techniques with that of DeepCrack approaches respectively. The
scores have been calculated at a threshold of 0.5 over multiple blind datasets (see 2.1).
Pandey et al.: Preprint submitted to Elsevier Page 10 of 9TrueDeep
DeepCrack-GF DeepCrack-BN allcrack truecrack swcrack slcrack sscrack mixcrack
Rissbilder (0.02, 0.431) (0.06, 0.449) (0.0, 0.422) (0.0, 0.42) (0.0, 0.463) (0.0, 0.402) (0.0, 0.487) (0.01, 0.471)
Sylvie Chambon (0.0, 0.372) (0.03, 0.335) (0.0, 0.195) (0.0, 0.183) (0.0, 0.349) (0.0, 0.183) (0.0, 0.27) (0.0, 0.363)
Volker (0.01, 0.542) (0.04, 0.549) (0.0, 0.506) (0.0, 0.476) (0.0, 0.695) (0.0, 0.476) (0.0, 0.549) (0.0, 0.637)
cfd (0.2, 0.618) (0.18, 0.639) (0.0, 0.661) (0.0, 0.611) (0.92, 0.576) (0.0, 0.55) (0.01, 0.619) (0.2, 0.577)
crack500 (0.26, 0.601) (0.25, 0.597) (0.0, 0.563) (0.0, 0.316) (0.0, 0.541) (0.0, 0.384) (0.0, 0.482) (0.0, 0.503)
GAPs (0.14, 0.227) (0.18, 0.225) (0.0, 0.338) (0.0, 0.345) (0.02, 0.332) (0.21, 0.189) (0.02, 0.33) (0.01, 0.292)
Eugen Muller (0.0, 0.376) (0.03, 0.401) (0.0, 0.399) (0.0, 0.285) (0.0, 0.635) (0.0, 0.317) (0.0, 0.418) (0.0, 0.537)
forest (0.26, 0.641) (0.21, 0.655) (0.0, 0.667) (0.0, 0.619) (0.66, 0.59) (0.0, 0.568) (0.01, 0.64) (0.07, 0.58)
cracktree200 (0.65, 0.22) (0.34, 0.207) (0.05, 0.287) (0.06, 0.256) (0.98, 0.222) (0.04, 0.239) (0.98, 0.27) (0.99, 0.223)
(a) Each tuple has the form (F-score, Threshold). The allcrack approach achieves the highest F-score on 3 datasets; truecrack on GAPs;
swcrack on Volker and Eugen Muller; sscrack on Rissbilder; overall, our techniques are better on 7 datasets and DeepCrack-GF (column2
thresholds are threshold1 and column1 thresholds are threshold2, see Figure 11) is better on crack500 and Sylvie Chambon.
DeepCrack-GF DeepCrack-BN allcrack truecrack swcrack slcrack sscrack mixcrack
Rissbilder (0.02, 0.621) (0.06, 0.628) (0.0, 0.623) (0.0, 0.621) (0.0, 0.637) (0.0, 0.611) (0.0, 0.649) (0.01, 0.641)
Sylvie Chambon (0.0, 0.577) (0.03, 0.555) (0.0, 0.52) (0.0, 0.517) (0.0, 0.575) (0.0, 0.517) (0.0, 0.547) (0.0, 0.58)
Volker (0.01, 0.67) (0.04, 0.672) (0.0, 0.655) (0.0, 0.641) (0.0, 0.755) (0.0, 0.641) (0.0, 0.675) (0.0, 0.721)
cfd (0.2, 0.718) (0.18, 0.729) (0.0, 0.742) (0.0, 0.714) (0.92, 0.696) (0.0, 0.684) (0.01, 0.718) (0.2, 0.696)
crack500 (0.26, 0.689) (0.25, 0.688) (0.0, 0.674) (0.0, 0.566) (0.0, 0.659) (0.0, 0.593) (0.0, 0.635) (0.0, 0.643)
GAPs (0.14, 0.55) (0.18, 0.548) (0.0, 0.594) (0.0, 0.595) (0.02, 0.591) (0.21, 0.544) (0.02, 0.59) (0.01, 0.578)
Eugen Muller (0.0, 0.59) (0.03, 0.598) (0.0, 0.606) (0.0, 0.563) (0.0, 0.718) (0.0, 0.575) (0.0, 0.613) (0.0, 0.667)
forest (0.26, 0.73) (0.21, 0.738) (0.0, 0.745) (0.0, 0.719) (0.66, 0.703) (0.0, 0.693) (0.01, 0.73) (0.07, 0.698)
cracktree200 (0.65, 0.557) (0.34, 0.552) (0.05, 0.58) (0.06, 0.57) (0.98, 0.559) (0.04, 0.564) (0.98, 0.575) (0.99, 0.558)
(b) The allcrack approach has the highest mIoU on 3 datasets; truecrack on GAPs; swcrack on Volker and Eugen Muller; sscrack on
Rissbilder; mixcrack on Sylvie Chambon; overall, our techniques are better on 8 datasets and DeepCrack-GF (column2 thresholds are
threshold1 and column1 thresholds are threshold2, see Figure 11) is better on crack500.
Table 3
Tables (a) and (b) show the F-score and mIoU comparision of our techniques with that of DeepCrack approaches respectively.
The scores have been calculated at the best thresholds (grid search from 0.0 to 0.99 at a step of 0.01) over multiple blind datasets
(see 2.1).
Pandey et al.: Preprint submitted to Elsevier Page 11 of 9