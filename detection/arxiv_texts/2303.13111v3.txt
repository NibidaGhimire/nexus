1
Boosting Convolution with Efficient MLP-Permutation for
V olumetric Medical Image Segmentation
Yi Lin, Xiao Fang, Dong Zhang, Kwang-Ting Cheng Fellow, IEEE , Hao Chen*, Senior Member, IEEE
Abstract ‚ÄîRecently, the advent of vision Transformer (ViT) has
brought substantial advancements in 3D dataset benchmarks,
particularly in 3D volumetric medical image segmentation (Vol-
MedSeg). Concurrently, multi-layer perceptron (MLP) network
has regained popularity among researchers due to their compa-
rable results to ViT, albeit with the exclusion of the resource-
intensive self-attention module. In this work, we propose a
novel permutable hybrid network for Vol-MedSeg, named PHNet,
which capitalizes on the strengths of both convolution neural net-
works (CNNs) and MLP. PHNet addresses the intrinsic isotropy
problem of 3D volumetric data by employing a combination of
2D and 3D CNNs to extract local features. Besides, we propose
an efficient multi-layer permute perceptron (MLPP) module
that captures long-range dependence while preserving positional
information. This is achieved through an axis decomposition
operation that permutes the input tensor along different axes,
thereby enabling the separate encoding of the positional informa-
tion. Furthermore, MLPP tackles the resolution sensitivity issue
of MLP in Vol-MedSeg with a token segmentation operation,
which divides the feature into smaller tokens and processes them
individually. Extensive experimental results validate that PHNet
outperforms the state-of-the-art methods with lower computa-
tional costs on the widely-used yet challenging COVID-19-20 and
Synapse benchmarks. The ablation study also demonstrates the
effectiveness of PHNet in harnessing the strengths of both CNNs
and MLP1.
I. I NTRODUCTION
Computer-aided diagnosis (CAD) systems have gained pop-
ularity in the healthcare sector, assisting radiologists in di-
agnosing and treating patients. Convolution neural networks
(CNNs) have shown remarkable advancements, improving the
performance of CAD, particularly in medical image segmen-
tation (MedSeg) [1-3]. Over the past decades, substantial re-
search efforts have focused on developing efficient and robust
MedSeg methodologies. One of the most popular architectures
for this task is UNet [4], which employs an encoder-decoder
structure and skip connections to reserve both contextual and
semantic information. Building upon the success of UNet, nu-
merous variants have been proposed with various convolution-
based blocks and different skip connections strategies, includ-
ing ResUNet [5], Y-Net [6], and V-Net [7], etc.
Recently, Transformers with attention mechanism have
shown promising superiority in the realm of natural lan-
guage processing [8]. Subsequent studies, such as ViT [9]
and DeiT [10], have demonstrated remarkable capabilities in
achieving state-of-the-art performance on versatile computer
vision tasks. Given the notable strides of Transformers in
natural image recognition tasks, researchers have investigated
The first two authors contributed equally and asterisk indicates the corre-
sponding author.
1The code will be accessible to the public upon acceptance.
6669727578818487
11.21.41.61.82Dice (%)on Synapse dataset
Throughput (samples/second)MLPTransformerCNNOursMixerShiftWaveSwinUNETRUNETRCoTr
VNetAtt-UNetUNetPHNet (Ours)
7980818283848586
8509009501000Dice (%)on Synapse dataset
FLOPs (G)MLPCNNTr ansformerFig. 1. Left: Performance v.s.throughput of PHNet and other SOTA methods.
Right: performance v.s.FLOPs of CNN, Transformers, and MLP, in different
model capacity.
the effectiveness of these neural networks for MedSeg. To
name a few, TransUNet [3] proposed to employ a Transformer
in the bottleneck of a UNet architecture for global informa-
tion communication. Similarly, UNETR [11], CoTr [12], and
SwinUNet [1] designed a hierarchical fusion of Transformer
and CNNs architecture.
Despite the developments in MedSeg domain via Trans-
former methods, their heavy computational costs caused by the
self-attention limit their practical application, particularly for
3D volumetric medical images, which necessitate a substantial
number of forward and backward passes [13]. Consequently,
multi-layer perceptron (MLP) has regained interest in the com-
munity, as it has demonstrated comparable performance with
both CNNs and Transformers (in Figure 1), without requiring
the heavy self-attention mechanism [14]. For instance, MLP-
Mixer [14] enabled information communication by a series of
MLPs, capturing long-range dependencies in the input data.
However, the effectiveness of MLP in volumetric MedSeg
remains understudied.
In this paper, we propose PHNet, a novel Permutable
Hybrid Network that integrates the strengths of CNN and MLP
for volumetric medical image segmentation. As illustrated
in Figure 2, PHNet embodies an encoder-decoder paradigm.
Notably, the encoder utilizes a 2.5D CNN structure that
capitalizes on the inherent isotropy of medical images, while
avoiding information loss in shallow layers by capturing
the varying information density in different directions of
volumetric medical images. In PHNet, we further proposes
MLPP, a Multi-Layer Permute Perceptron module that can
maintain the positional information with the axial decom-
position operation while integrating global interdependencearXiv:2303.13111v3  [eess.IV]  24 Aug 20232
in a computationally-efficient manner. To enhance computa-
tional efficiency, token-group operation is introduced, which
efficiently aggregates feature maps at a token level, reducing
the number of computations required. PHNet is evaluated on
two publicly available datasets, COVID-19 Lung CT Lesion
Segmentation Challenge-2020 [15] and Synapse Multi-Organ
Segmentation [16]. Extensive experimental results validate that
PHNet achieves state-of-the-art performance on both datasets,
surpassing the winner in MICCAI COVID-19-20 challenge.
II. R ELATED WORK
A. CNN-Based Networks.
Since the groundbreaking introduction of the UNet [4],
CNN-based networks have achieved state-of-the-art results on
various 2D and 3D MedSeg tasks [17-20]. These methods use
CNNs as the backbone to extract image features, and com-
bine some elaborate tricks ( e.g., skip connection, multi-scale
representation, feature interaction) for feature enhancement.
Compared to 2D methods, 3D approaches directly utilize the
full volumetric image represented by a sequence of 2D slices
or modalities. Despite their success, CNN-based networks
exhibit a constraint in effectively learning global context
and long-range spatial dependencies, resulting in sub-optimal
performance for challenging tasks.
B. Transformer-Based Networks.
Vision Transformers have been applied in MedSeg to estab-
lish long-range dependence and capture context information.
For example, UNETR [11] leverages Transformer as the
encoder to learn sequence representations, which captures the
global multi-scale information. Due to high computational
costs, many architectures have been proposed to reduce the
cost. Peiris et al. introduces VT-UNet [21] that leverages
a hierarchical vision Transformer which gradually decreases
the resolution of features in Transformer layers and uti-
lizes sub-sampled attention modules. Chen et al. proposes
TransUNet [3] which uses CNN to efficiently extract local
information and employs a Transformer to model global
features. Tang et.al proposes SwinUNETR [22] that calculates
the attention weight between different tokens in shifted local
windows, reducing the computational cost from quadratic to
linear complexity. However, the self-attention mechanism is
still computationally expensive and relatively slow on GPUs.
C. MLP-Based Networks.
Acknowledging the substantial computational cost of atten-
tion blocks in Transformer, simple and efficient modules that
consist of only MLPs are proposed. Notably, MLP-Mixer [14]
uses token-mixing MLP and channel-mixing MLP to capture
the relationship between tokens and channels, respectively.
CycleMLP [23] introduces a hierarchical cycle fully connected
layer to aggregate spatial context and improve performance.
WaveMLP [24] represents each token as a wave function
with amplitude and phase to dynamically aggregate features
according to the semantic contents of input images. In MedSeg
field, UNeXt [13] shifts tokens along vertical and horizontal
¬©¬©¬©EncoderDecoderIP-featureTP-featureGlobal-dependency2D CNN3D CNNTransformerMLP
(a) UNet(b) UNETR(c) TransUNet(d) PHNet(Ours)Fig. 2. Overview of the proposed PHNet.
directions to get an axial receptive field of 2D images to
increase efficiency. However, the effects of MLPs in V ol-
MedSeg remains unexplored. To the best of our knowledge, it
is the first attempt to investigate the effectiveness of integrating
CNNs and MLPs in V ol-MedSeg.
III. M ETHODOLOGY
PHNet adopts an encoder-decoder paradigm, as exemplified
in Figure 2. The encoder consists of a 2.5D convolution mod-
ule and an MLPP module. The 2.5D convolution is responsible
for extracting local features, and the output feature maps are
subsequently forwarded to MLPP to capture global features.
The decoder processes the hierarchical features for prediction.
In the following, we will detail each component.
A. 2.5D Convolution
Drawing upon previous research on bias in medical image
analysis [25] and the anisotropic nature of volumetric medical
images, we incorporate convolutional layers in the shallow
layers of the encoder to extract local features. V olumetric
images, such as CT and MRI scans, are often affected by
anisotropic problems due to their thick-slice scanning [26],
resulting in high in-plane (IP) resolution and low through-
plane (TP) resolution [18, 20]. This discrepancy is particularly
pronounced in COVID-19-20 [15], where the IP resolution
is 0.74mm on average, while the TP resolution plunges to a
mere 5mm. To mitigate this issue, we use 2D conv-blocks to
capture the IP information until the feature is reformulated in
approximately uniform resolution across all three axes: axial,
coronal, and sagittal. Then, we apply 3D conv-blocks to handle
the volumetric information. Each encoder layer consists of
two residual convolution blocks, with each block comprising
two sequential Convolution-Instance Normalization-Rectified
Linear Unit (Conv-IN-ReLU) operations [27]. The residual
addition takes place before the final ReLU activation.
B. Multi-Layer Permute Perceptron (MLPP)
Although CNNs are capable of modeling long-range de-
pendencies through deep stacks of convolution layers, stud-
ies [13, 14, 28] have highlighted the superior ability of MLP-
based networks to learn global context. Motivated by this,
we design MLPP as depicted in Figure 3 to acquire global
information in deep layers of the encoder. MLPP decomposes3
AA-MLP
IP-MLP
‚Ä¶
‚Ä¶
‚Ä¶‚Ä¶
HWC
D‚Ä¶AxialDecompositionHorizontalVerticalChannel
‚Ä¶
‚Ä¶‚Ä¶Lg1
1g
L‚Ä¶‚Ä¶Token SegmentationùêøùëîFC Layer
ùêøùëîFC Layerùê∂FC Layer
L1ùêø!FC Layerflattenflattenflattenùêª√óùëä√óùê∂permute
permuteùêª√óùëä√óùê∂
ùêª√óùëä√óùê∂permuteHWpartition intowindows
TP-MLP
CD‚Ä¶
‚Ä¶permute
‚Ä¶Token SegmentationùêøùëîFC LayerHW
DC‚Ä¶‚Ä¶Lg1flatten
ùêª√óùëä√óùëá√óùê∂
‚Ä¶
Lflatten‚Ä¶
Fig. 3. Illustration of multi-layer permute perceptron (MLPP) module.
the training of IP feature and TP feature in sequential order.
We denote these two blocks as IP-MLP and TP-MLP , re-
spectively. To facilitate communication of cross-axis tokens,
we further propose an auxiliary attention branch in IP-MLP,
denoted as AA-MLP . We provide a PyTorch-style pseudo code
of MLPP in the supplementary material.
1)IP-MLP .:Given one slice of input feature maps X‚àà
RH√óW√óC, conventional MLP-based methods [14, 29] typ-
ically flatten Xinto a 1D vector, which loses the spatial
information of the original conv-features [30] and results in
large computational complexity which escalates quadratically
with the size of X. To this end, as depicted in Figure 3, we
introduce an axial decomposition operation in triplet pathways
that separately process Xin horizontal ( W), vertical ( H),
and channel ( C) axis. It is achieved by permuting dimensions
ofXsuch that voxels within the same pathway across all
channels are grouped, thus enabling the preservation of pre-
cise positional information along other axes when encoding
information along one axis. It also reduces the computational
complexity, which scales linearly with the size of X.
To strike a balance between local feature and long-distance
interactions, and alleviate image resolution sensitivity prob-
lem [31] wherein the MLP-based model is sensitive to the
input resolution2, we present a token segmentation operation
that splits the feature vector into multiple tokens, which can
be efficiently processed by the following fully-connected (FC)
layers. We take the horizontal axis as an example. Instead
of encoding the entire dimension, we split Xinto non-
overlapping segments along the horizontal direction. Each
segment, denoted as Xi‚ààRL√óC, where i‚àà1, ..., HW/L ,
has a segment length of L. Similarly, we divide each Xiinto
multiple non-overlapping groups along channel dimension,
where each group has g=C/L channels. This yields split
segments, and each individual segment is Xk
i‚ààRLg, where
k‚àà {1, ..., C/g }. Next, we flatten each segment and map
RLg7‚ÜíRLgby an FC layer to transform each segment, pro-
ducingYk
i. To recover the original dimension, we permute all
segments back to YW‚ààRH√óW√óC. Similarly, we conduct the
2Further discussion is provided in supplementary material.same operations in the vertical pathway as above to permute
tokens along the vertical direction, yielding YH. To facilitate
communication among groups along channel dimension, we
design a parallel branch that contains an FC layer that maps
RC7‚ÜíRCto process each token individually, yielding YC.
Finally, we feed the element-wise summation of horizontal,
vertical, and channel features into a new FC layer to attain
the output, which can be formulated as:
YIP= (YH+YW+YC)W, (1)
whereW‚ààRC√óCdenotes an FC weight matrix.
2)AA-MLP .:The MLPP, despite its strengths, possesses
two limitations that may have a negative impact on segmen-
tation performance. First, the axial decomposition operation
restricts direct interaction between tokens that are not in
the same horizontal or vertical position. Second, the token
segmentation operation would suffer from a smaller local
reception field compared with the vanilla MLP [14]. To
overcome these limitations, we design an auxiliary branch to
enable intra-axis token communication, which serves as an
attention mechanism via a lightweight yet effective MLP-like
architecture. Specifically, given an input slice of feature maps
X‚ààRH√óW√óC, we partition Xinto non-overlapping win-
dows. We set window size to Land thus obtain Xi‚ààRL√óL,
where i‚àà { 1, ..., HWC/L2}. Subsequently, we apply an
FC matrix W‚ààRL2√óL2to transform each window and
getYi‚ààRL√óL. The final attention map YA‚ààRH√óW√óC
is obtained by permuting all windows back to the original
dimension. Finally, the feature maps FIPof IP-MLP are
obtained by performing residual attention [32] of YIPandYA
as follows:
FIP= (1 + YA)‚äôYIP, (2)
where ‚äôdenotes element-wise multiplication.
3)TP-MLP .:Upon obtaining the in-plane information from
the IP-MLP module, we proceed to apply the TP-MLP module
to capture the long-term through-plane features. Similarly,
given input feature maps FIP‚ààRH√óW√óD√óC, we first split
X=FIPalong the depth dimension into non-overlapping
segments with a segment length of L. We thus obtain Xi‚àà4
RL√óC, where i‚àà {1, ..., HWD/L }. Subsequently, we divide
Xinto several non-overlapping groups along the channel
dimension, with each group containing g=C/L channels.
This results in Xk
i‚ààRLg, where k‚àà {1, ..., C/g }. Subse-
quently, we flatten each segment and map RLg7‚ÜíRLgby
an FC layer, yielding Yk
i. Finally, we permute all segments
Yk
i‚ààRLgback to the original dimension, yielding the output
FTP‚ààRH√óW√óD√óC.
C. Decoder Network
The decoder in our proposed method utilizes a slim CNN
architecture, employing transpose convolution to progressively
upsample the feature maps to match the input image res-
olution. Within the decoder, we separate an isotropic 3D
convolution with kernel size 3√ó3√ó3into a 3√ó3√ó1in-
plane convolution and a 1√ó1√ó3through-plane convolution
to efficiently fuse the feature [33]. We further include skip
connections between the encoder and decoder, allowing for
the preservation of low-level details.
IV. E XPERIMENTS
A. Datasets and Evaluation Metrics
We conduct experiments on two publicly available datasets:
COVID-19-20 [15] and Synapse [16]. COVID-19-20 is com-
prised of 249 unenhanced chest CT scans, with 199 samples
designated for training and 50 samples for testing. All samples
are positive for SARS-CoV-2 RT-PCR. Synapse consists of
30 cases of CT scans, with 14, 4, and 12 cases designated
for training, validation, and testing, respectively [1]. For
COVID-19-20, we use the official evaluation metrics from the
challenge [15], including Dice coefficient (Dice), Intersection
over Union (IoU), Surface Dice coefficient (SD), Normalized
V olume Difference (NVD), and Hausdorff Distance (HD). For
Synapse, we follow [3] to adopt Dice and HD as evaluation
metrics. The computational cost is measured with an input size
of192√ó192√ó48and a batch size of 1, in terms of FLOPS
(G), Parameters (M), Peak Memory (G) and Throughput
(samples/s). For details, please refer to [28].
B. Implementation Details
PHNet is implemented using PyTorch and MONAI [34]
framework and trained on an NVIDIA RTX 3090 GPU.
For COVID-19-20, all images are interpolated into the voxel
spacing of 0.74√ó0.74√ó5.00mm3. Three sub-volumes of
224√ó224√ó28are sampled from each scan. We train PHNet
for a total of 250 epochs on Synapse and 450 epochs on
COVID-19-20. For all experiments we adopt the AdamW op-
timizer [35] with an initial learning rate lr= 10‚àí3√óbatch size
1024,
as suggested by [28]. The objective function is the summation
of Dice loss and cross-entropy loss. Except for the above, we
follow baseline from [15] and [36] for the COVID-19-20 and
Synapse datasets, respectively.C. Comparisons with State-of-the-Arts
1) Comparison methods.: We compare the proposed PHNet
with CNN-based, Transformer-based, MLP-based methods,
and foundation model. We list the details below.
‚Ä¢CNN-based methods , including V-Net [7], UNet [4],
nnUNet [36], and attention-UNet [37]. These architec-
tures feature a hierarchical contracting path for context
aggregation and a symmetric expanding path for resolu-
tion recovery and precise localization.
‚Ä¢Transformer-based methods , including ViT [9], UN-
ETR [11], SwinUNETR [22], TransUNet [3], Swin-
UNet [1], CoTr [12], and CTO-Net [38]. These architec-
tures can be classified into three categories: 1) classical
Transformer ( i.e., ViT), 2) encoder-decoder framework
with pure Transformer blocks ( i.e., SwinUNet), and 3)
hybrid architectures with CNN and Transformer ( i.e.,
UNETR, SwinUNETR, TransUNet, CoTr, CTO-Net).
‚Ä¢MLP-based methods , including UNext [13],
CycleMLP [23], MLP-Mixer (Mixer) [14], ShiftMLP
(Shift) [39], and WaveMLP (Wave) [24]. We only replace
the MLPP module in PHNet with these alternatives for
a fair comparison.
‚Ä¢SAMed [40] is built upon SAM [41] which is a foundation
model for semantic segmentation. SAMed retains the
same architecture as SAM, which features a ViT-based
encoder, a prompt module, and a mask decoder. SAMed
applies the LoRA [42] finetuning strategy on target tasks.
2) Quantitative comparisons.: We begin by evaluating the
performance of our method in multi-organ segmentation on
Synapse [16] in Table I. Result shows that our method achieves
the highest average Dice score of 85.62 %and lowest HD
of 11.75, outperforming the SOTA methods. Specifically, our
method achieves 1.64 %and 8.77 %Dice improvements over
nnUNet [36] and UNet [19], whose backbones are built
upon 3D CNNs. Compared to top-performing competitors
of Transformer-based CTO-Net [38], UNETR [11], MLP-
based UNext [13], Wave [24], and SAMed [40], our method
also achieves better performance with 4.52 %, 6.05 %, 18.55 %,
0.53%, 3.74 %Dice gains, respectively. These results conform
to our argument that PHNet obtains satisfying segmentation
performance through effective local-to-global modeling.
We further evaluate the performance on COVID-19-20 [15]
and the official evaluation result is presented in Table II. Com-
pared to CNN-based methods, our method attains the highest
scores in all metrics. This suggests that CNN-based approaches
have limitations in long-distance context fusion. Compared
to Transformer-based methods, PHNet also achieves better
performance, suggesting that our proposed hybrid structure
design facilitates more adept feature learning. Compared to
MLP-based methods, PHNet outperforms by a remarkable
margin. This is partly because existing MLP-based methods
are designed to segment on 2D slices which lose through-
plane features, resulting in severe performance deterioration
in challenging volumetric image segmentation tasks. The large
performance gap between SAMed [40] and PHNet indicates
the constrained generalization capacity of foundation mod-
els within specific segmentation tasks. Additionally, follow-5
TABLE I
RESULT COMPARISONS WITH THE STATE -OF-THE-ART METHODS ON SYNAPSE [16].
Methods mDice ‚ÜëHD‚ÜìAorta Gallb .Kid(L) Kid(R) Liver Panc .Spleen Stom .CNN-basedUNet [4] 76.85 39.70 89.07 69.72 77.77 68.60 93.43 53.98 86.67 75.58
V-Net [7] 68.81 - 75.34 51.87 77.10 80.75 87.84 40.05 80.56 56.98
Attention-UNet [37] 77.77 36.02 89.55 68.88 77.98 71.11 93.57 58.04 87.30 75.75
nnUNet [36] 83.98 12.56 91.73 66.22 87.30 84.41 96.15 76.04 94.81 75.20
ViT [9] 71.29 32.87 73.73 55.13 75.80 72.20 91.51 45.99 81.99 73.95
TransUNet [3] 77.48 31.69 87.23 63.13 81.87 77.02 94.08 55.86 85.08 75.62
CoTr [12] 78.08 27.38 85.87 61.38 84.83 79.36 94.28 57.65 87.74 73.55
UNETR [11] 79.57 23.87 89.99 60.56 85.66 84.80 94.46 59.25 87.81 73.99
SwinUNETR [22] 77.09 34.21 88.73 60.07 78.10 80.99 93.81 51.97 89.72 73.29
SwinUNet [1] 79.13 21.55 85.47 66.53 83.28 79.61 94.29 56.58 90.66 76.60Transformer-basedCTO-Net [38] 81.10 18.75 87.72 66.44 84.49 81.77 94.88 62.74 90.60 80.20MLP-basedMixer [14] 82.80 21.12 87.40 67.16 85.37 83.96 95.16 70.53 94.53 78.25
UNeXt [13] 67.07 40.47 76.43 51.64 74.54 67.94 91.11 34.95 79.20 60.70
CycleMLP [23] 70.82 24.47 75.07 61.75 77.25 70.71 92.83 42.29 81.44 65.26
Shift [39] 83.69 16.44 91.35 64.50 86.68 84.15 96.01 73.62 95.22 78.01
Wave [24] 85.09 12.49 91.74 72.48 86.37 84.63 96.03 77.02 95.25 77.23
SAMed [40] 81.88 20.64 87.77 69.11 80.45 79.95 94.80 72.17 88.72 82.06
PHNet (Ours) 85.62 11.75 90.31 74.08 87.46 85.69 96.16 76.94 95.23 79.11
ing [15], we perform five-fold cross-validation and model
ensemble using our proposed method. The result demonstrates
that our method achieves the highest dice score of 77.18%,
surpassing the performance of the top-12 solutions in this
challenge3.
Additionally, our method is subjected to further evaluation
on two other public datasets, namely the Liver Tumor Seg-
mentation dataset (LiTS) [43] and the Medical Segmentation
Decathlon (MSD) brain tumor segmentation (BraTS) [44]
dataset. The details are presented in the supplementary mate-
rial. Results show that our method outperforms other state-of-
the-art methods on both datasets, demonstrating the credibility
and wide-ranging applicability of our proposed method.
3) Qualitative comparisons.: Visual comparisons on
Synapse [16] are shown in Figure 4. PHNet achieves the
best visual segmentation results compared to nnUNet [36],
TransUNet [3], UNext [13], and Mixer [14]. As shown in the
first row, the comparison indicates that our method achieves a
better segmentation where the under-segmentation of pancreas
is observed in nnUNet, TransUNet, and UNext. Moreover,
as shown in the second row, PHNet can accurately delin-
eate boundaries between liver and stomach, while the over-
segmentation of liver is observed in both UNeXt and Mixer.
Similarly, PHNet delineates a more precise boundary between
pancreas and stomach as shown in the last row. The primary
3https://covid-segmentation.grand-challenge.org/evaluation/challenge/
leaderboardfactor could be PHNet helps extract accurate contours by
effective local-to-global modeling and aggregation of inter-
axis and intra-axis token features. Overall, PHNet attains better
segmentation results and mitigates the issues associated with
under- and over-segmentation contours.
D. Ablation Study
We conduct ablation studies on Synapse to validate the
effectiveness and efficiency of each component in our method.
We use the same decoder architecture for all variations.
1) Effectiveness of each components.: To verify the effec-
tiveness of core components in our approach, we increase
each essential component gradually based on the vanilla MLP
network (abbreviated as ‚Äúbaseline‚Äù), as shown in Table III.
Compared with the baseline, the integration of axial decom-
position yields a 1.49% enhancement in Dice and a note-
worthy improvement in efficiency. This is due to positional
information encoding and complexity reduction. There is an
additional improvement of 0.65% Dice through decomposition
into in-plane and through-plane features, which is attributed
to the amplified learning capacity of the module as it contains
distinct learning parameters for in-plane and through-plane
features [45]. Furthermore, AA-MLP contributes to 0.68%
performance gains with slightly increasing computational cost,
and the whole framework achieves an 85.62% Dice score.
This is because intra-axis token communication enables better
learning representation from larger receptive field, as shown6
TABLE II
RESULT COMPARISONS WITH THE STATE -OF-THE-ART METHODS AND TOP -12 SOLUTIONS ON COVID-19-20 [15].
Methods Dice‚Üë IoU‚Üë SD‚Üë NVD‚Üì HD‚Üì Method Dice ‚ÜëCNNUNet [19] 69.09 55.27 62.83 40.79 134.76 Rank 1 77.09
nnUNet [36] 72.51 59.40 69.04 27.87 123.65 Rank 2 76.87
TransUNet [3] 18.64 11.70 11.93 65.06 279.03 Rank 3 76.87
CoTr [12] 59.63 45.65 52.97 44.57 172.78 Rank 4 76.78
UNETR [11] 57.18 43.10 51.20 44.64 174.40 Rank 5 76.77
SwinUNETR [22] 63.65 50.13 58.19 37.20 141.42 Rank 6 76.64Transformer
SwinUNet [1] 32.82 22.08 20.71 67.62 221.46 Rank 7 76.47MLPShift [39] 72.18 59.32 68.58 25.27 132.01 Rank 8 76.45
Wave [24] 74.08 60.92 70.76 22.12 120.59 Rank 9 76.28
UNeXt [13] 21.90 14.64 10.79 81.52 328.83 Rank 10 76.27
CycleMLP [23] 27.53 17.91 15.55 79.67 228.07 Rank 11 76.27
SAMed [40] 15.43 09.52 7.66 96.36 2265.73 Rank 12 76.16
PHNet (Ours) 76.34 63.36 72.10 20.60 108.16 Ours 77.18
(a) Image(b) Ground Truth(c) PHNet(d) nnUNet(e) TransUNet(f) UNeXt(g) Mixer
aortagallbladderleft kidneyright kidneyliverpancreasspleenstomach
Fig. 4. Qualitative visualizations of different methods on the Synapse [16]. Regions of evident improvements are enlarged to show better details.
TABLE III
RESULT COMPARISONS WITH DIFFERENT EFFECTS OF EACH COMPONENT
ONSYNAPSE [16]. ‚ÄúB ASELINE ‚ÄùDENOTES THE VANILLA MLP NETWORK .
‚ÄúIP-‚Äù DENOTES THE PROPOSED AXIAL DECOMPOSITION OPERATION IN
IP-MLP MODULE . ‚ÄúTP-‚Äù DENOTES THE PROPOSED OPERATION OF
DECOMPOSITION OF IN -PLANE FEATURE AND THROUGH -PLANE FEATURE
INTP-MLP. ‚ÄúAA-‚Äù DENOTES OUR PROPOSED AA-MLP MODULE .
Baseline IP- TP- AA- Dice ‚Üë HD‚Üì FLOPs Thro.
‚úì 82.80 21.12 1249 1.04
‚úì ‚úì 84.29 19.05 938 1.73
‚úì ‚úì 83.39 19.66 971 1.62
‚úì ‚úì ‚úì 84.94 18.29 947 1.75
‚úì ‚úì ‚úì ‚úì 85.62 11.75 953 1.73
in Figure 5. A similar trend can be observed in the evaluation
of HD metric. These results underscore the effectiveness of
each component in our approach.2) Impact of segment length.: In Figure 6(a), we investigate
the impact of different segment lengths Lin PHNet. Expressly,
the segment length is set to various ratios of the width ( W),
i.e.,1,1
2,1
3, and1
4, respectively. With a larger segment length,
long-range dependencies can be more effectively captured
in deep layers. Conversely, smaller segment length implies
fewer adjacent tokens are grouped, emphasizing more on local
information. The best performance is achieved when L=1
2W,
which provides a good balance between local information and
long-range dependencies.
3) Impact of MLP layers.: In Figure 6(b), we study the
influence of a different number of MLP layers Kin our
PHNet. Results show that the best performance is achieved
when the number of MLP layers is 2. This observation may
vary across datasets. Our method consistently outperforms the
baseline method across different configurations of MLP layers,7
(a) (b) (c) (d) (e) (f)
aorta gallbladder leftkidney right kidney liver pancreas spleen stomach
Fig. 5. Segmentation visualizations of ablation variations on Synapse [16],
including (a) original image, (b) ground truth, predictions of (c) baseline +
IP + TP + AA, (d) baseline + IP + TP, (e) baseline + IP, and (f) baseline.
Regions of evident improvements are enlarged to show better details. Better
viewed with zooming in.
84.2684.4385.6283.8182838485860123K
83.2684.5785.6284.678283848586W/4W/3W/2WL(a)(b)baseline
Fig. 6. Impact of (a) different segment length L; (b) different number of
MLP layers Kon Synapse dataset.
affirming the robustness and stability of our method.
CNNTrans.MLPCNN84.2685.4785.62Trans.70.7975.9775.13MLP73.8574.4676.88(b)Shallow DeepHDDice(%)Throughput(sample/s)
FLOPs(G)Parameters(M)PeakMemory(G)1.851.731.2885.6285.4784.2616.0713.4111.7549.8913.5711.081086103895333.9724.3722.93CNNTransformerMLP(a)
Fig. 7. (a) Comparisons between conventional architectures in terms of ef-
fectiveness and efficiency; (b) Comparison between architecture combinations
in Dice (%).
E. Discussion
1) Comparisons with conventional architectures.: We con-
duct experiments to validate our analysis as shown in Fig. 7(a).
Specifically, we replace the MLPP module with 3D CNN and
Transformer blocks to assess their impact on efficiency and
effectiveness. For the Transformer component, we opt for the
advanced Swin Transformer [22] due to the promising perfor-
mance and notable reduction in computational cost through the
use of shift windows. Compared to CNN, our method achieves
1.36% improvement in Dice, indicating that our proposed
MLPP module more effectively encodes the global informa-
tion, leading to enhanced segmentation accuracy. Concerning
computational efficiency, our method achieves a profile of953G FLOPs, 24.37M parameters, 13.57G peak memory, and
1.73 samples/second throughput. These efficiency outcomes
are comparable to those of 3D CNN, indicating that aug-
ment CNN performance while upholding efficiency. Although
Transformer showcases competitive performance when con-
figured with stacked blocks, our method shows significant
superiority in terms of efficiency. This suggests that our MLPP
is capable of capturing global information while eliminating
the exhaustiveness of token comparison seen in self-attention.
We also provide theoretical analysis for the computational cost
in the supplementary material.
2) Comparisons with architecture combinations.: We fur-
ther conduct a comprehensive experimental analysis of various
combinations of CNN, Transformer and MLP in both shallow
and deep layers of the encoder. As depicted in Figure 7(b), the
combination of CNN in shallow layers and MLP in deep layers
achieves the best performance with an 85.62% Dice score. This
finding supports our argument that CNN excels in capturing
local features, while MLP is more effective in modeling long-
range dependencies. Interestingly, the inverse configuration,
which employs MLP in shallow layers and CNN in deep
layers, results in a significant decline in performance. This
suggests that local information can be effectively extracted
in shallow layers, while global information is better captured
in deeper layers. We also observe that the performance of
Transformer architectures is underwhelming, which may be
due to their high model complexity leading to overfitting on
small datasets.
V. C ONCLUSION
This paper introduced a novel permutable hybrid network,
PHNet, specifically designed for volumetric medical image
segmentation. By integrating 2D CNN, 3D CNN, and MLP,
PHNet effectively captures both local and global features.
Additionally, we proposed a permutable MLP block to address
spatial information loss and alleviate computational burden.
Experimental results on four public datasets demonstrate the
superiority of PHNet over state-of-the-art approaches. Future
research will explore extending the framework to other medi-
cal image analysis tasks, such as disease diagnosis and local-
ization, and further examine the interactions and effectiveness
of CNN, Transformer, and MLP.
REFERENCES
[1] H. Cao, Y . Wang, J. Chen, D. Jiang, X. Zhang, Q. Tian, and M. Wang,
‚ÄúSwin-Unet: UNet-like pure transformer for medical image segmenta-
tion,‚Äù in European Conference on Computer Vision , 2023.
[2] D. Zhang, Y . Lin, H. Chen, Z. Tian, X. Yang, J. Tang, and K. T. Cheng,
‚ÄúDeep learning for medical image segmentation: tricks, challenges and
future directions,‚Äù arXiv , 2022.
[3] J. Chen, Y . Lu, Q. Yu, X. Luo, E. Adeli, Y . Wang, L. Lu, A. L. Yuille, and
Y . Zhou, ‚ÄúTransUNet: Transformers make strong encoders for medical
image segmentation,‚Äù arXiv , 2021.
[4] O. Ronneberger, P. Fischer, and T. Brox, ‚ÄúU-Net: Convolutional net-
works for biomedical image segmentation,‚Äù in International Conference
on Medical Image Computing and Computer Assisted Intervention .
Springer, 2015, pp. 234‚Äì241.
[5] K. He, X. Zhang, S. Ren, and J. Sun, ‚ÄúDeep residual learning for image
recognition,‚Äù in Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition , 2016, pp. 770‚Äì778.8
[6] S. Mehta, E. Mercan, J. Bartlett, D. Weaver, J. G. Elmore, and
L. Shapiro, ‚ÄúY-Net: joint segmentation and classification for diagnosis
of breast biopsy images,‚Äù in International Conference on Medical Image
Computing and Computer Assisted Intervention . Springer, 2018, pp.
893‚Äì901.
[7] F. Milletari, N. Navab, and S.-A. Ahmadi, ‚ÄúV-Net: Fully convolutional
neural networks for volumetric medical image segmentation,‚Äù in Inter-
national Conference on 3D Vision . Ieee, 2016, pp. 565‚Äì571.
[8] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
≈Å. Kaiser, and I. Polosukhin, ‚ÄúAttention is all you need,‚Äù Advances in
Neural Information Processing Systems , vol. 30, 2017.
[9] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,
T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,
J. Uszkoreit, and N. Houlsby, ‚ÄúAn image is worth 16x16 words:
Transformers for image recognition at scale,‚Äù ICLR , 2021.
[10] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and
H. J ¬¥egou, ‚ÄúTraining data-efficient image transformers & distillation
through attention,‚Äù in International Conference on Machine Learning .
PMLR, 2021, pp. 10 347‚Äì10 357.
[11] A. Hatamizadeh, Y . Tang, V . Nath, D. Yang, A. Myronenko, B. Land-
man, H. R. Roth, and D. Xu, ‚ÄúUNETR: Transformers for 3D medical
image segmentation,‚Äù in Proceedings of the IEEE/CVF Winter Confer-
ence on Applications of Computer Vision , 2022, pp. 574‚Äì584.
[12] Y . Xie, J. Zhang, C. Shen, and Y . Xia, ‚ÄúCoTr: Efficiently bridging cnn
and transformer for 3D medical image segmentation,‚Äù in International
Conference on Medical Image Computing and Computer Assisted Inter-
vention . Springer, 2021, pp. 171‚Äì180.
[13] J. M. J. Valanarasu and V . M. Patel, ‚ÄúUNeXt: MLP-based rapid medical
image segmentation network,‚Äù in International Conference on Medical
Image Computing and Computer Assisted Intervention . Springer, 2022,
pp. 23‚Äì33.
[14] I. O. Tolstikhin, N. Houlsby, A. Kolesnikov, L. Beyer, X. Zhai,
T. Unterthiner, J. Yung, A. Steiner, D. Keysers, J. Uszkoreit et al. ,
‚ÄúMLP-mixer: An all-MLP architecture for vision,‚Äù Advances in Neural
Information Processing Systems , vol. 34, pp. 24 261‚Äì24 272, 2021.
[15] H. R. Roth, Z. Xu, C. Tor-D ¬¥ƒ±ez, R. S. Jacob, J. Zember, J. Molto,
W. Li, S. Xu, B. Turkbey, E. Turkbey et al. , ‚ÄúRapid artificial intelligence
solutions in a pandemic‚Äîthe covid-19-20 lung ct lesion segmentation
challenge,‚Äù Medical Image Analysis , vol. 82, p. 102605, 2022.
[16] B. Landman, Z. Xu, J. Igelsias, M. Styner, T. Langerak, and A. Klein,
‚ÄúMiccai multi-atlas labeling beyond the cranial vault‚Äìworkshop and
challenge,‚Äù in Proc. MICCAI Multi-Atlas Labeling Beyond Cranial
Vault‚ÄîWorkshop Challenge , vol. 5, 2015, p. 12.
[17] Q. Dou, H. Chen, Y . Jin, L. Yu, J. Qin, and P.-A. Heng, ‚Äú3d deeply
supervised network for automatic liver segmentation from ct volumes,‚Äù
inMedical Image Computing and Computer-Assisted Intervention‚Äì
MICCAI 2016: 19th International Conference, Athens, Greece, October
17-21, 2016, Proceedings, Part II 19 . Springer, 2016, pp. 149‚Äì157.
[18] S. Liu, D. Xu, S. K. Zhou, O. Pauly, S. Grbic, T. Mertelmeier,
J. Wicklein, A. Jerebko, W. Cai, and D. Comaniciu, ‚Äú3D anisotropic
hybrid network: Transferring convolutional features from 2D images to
3D anisotropic volumes,‚Äù in International Conference on Medical Image
Computing and Computer Assisted Intervention . Springer, 2018, pp.
851‚Äì858.
[19] T. Falk, D. Mai, R. Bensch, ¬®O. C ¬∏ ic ¬∏ek, A. Abdulkadir, Y . Marrakchi,
A. B ¬®ohm, J. Deubner, Z. J ¬®ackel, K. Seiwald et al. , ‚ÄúU-Net: deep learning
for cell counting, detection, and morphometry,‚Äù Nature Methods , vol. 16,
no. 1, pp. 67‚Äì70, 2019.
[20] Z. Dong, Y . He, X. Qi, Y . Chen, H. Shu, J.-L. Coatrieux, G. Yang,
and S. Li, ‚ÄúMNet: Rethinking 2D/3D networks for anisotropic medical
image segmentation,‚Äù in International Joint Conferences on Artificial
Intelligence , 2022.
[21] H. Peiris, M. Hayat, Z. Chen, G. Egan, and M. Harandi, ‚ÄúA robust
volumetric transformer for accurate 3d tumor segmentation,‚Äù in Medical
Image Computing and Computer Assisted Intervention‚ÄìMICCAI 2022:
25th International Conference, Singapore, September 18‚Äì22, 2022,
Proceedings, Part V . Springer, 2022, pp. 162‚Äì172.
[22] Y . Tang, D. Yang, W. Li, H. R. Roth, B. Landman, D. Xu, V . Nath, and
A. Hatamizadeh, ‚ÄúSelf-supervised pre-training of swin transformers for
3D medical image analysis,‚Äù in Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition , 2022, pp. 20 730‚Äì20 740.
[23] S. Chen, E. Xie, C. GE, R. Chen, D. Liang, and P. Luo, ‚ÄúCycleMLP:
A MLP-like architecture for dense prediction,‚Äù in International
Conference on Learning Representations , 2022. [Online]. Available:
https://openreview.net/forum?id=NMEceG4v69Y
[24] Y . Tang, K. Han, J. Guo, C. Xu, Y . Li, C. Xu, and Y . Wang, ‚ÄúAn
image patch is a wave: Phase-aware vision MLP,‚Äù in Proceedings of theIEEE Conference on Computer Vision and Pattern Recognition , 2022,
pp. 10 935‚Äì10 944.
[25] P. W. Battaglia, J. B. Hamrick, V . Bapst, A. Sanchez-Gonzalez, V . Zam-
baldi, M. Malinowski, A. Tacchetti, D. Raposo, A. Santoro, R. Faulkner
et al. , ‚ÄúRelational inductive biases, deep learning, and graph networks,‚Äù
arXiv , 2018.
[26] G. Wang, J. Shapey, W. Li, R. Dorent, A. Dimitriadis, S. Bisdas,
I. Paddick, R. Bradford, S. Zhang, S. Ourselin et al. , ‚ÄúAutomatic
segmentation of vestibular schwannoma from T2-weighted mri by deep
spatial attention with hardness-weighted loss,‚Äù in International Confer-
ence on Medical Image Computing and Computer Assisted Intervention .
Springer, 2019, pp. 264‚Äì272.
[27] D. Ulyanov, A. Vedaldi, and V . Lempitsky, ‚ÄúInstance normalization: The
missing ingredient for fast stylization,‚Äù arXiv , 2016.
[28] Q. Hou, Z. Jiang, L. Yuan, M.-M. Cheng, S. Yan, and J. Feng, ‚ÄúVision
permutator: A permutable MLP-like architecture for visual recogni-
tion,‚Äù IEEE Transactions on Pattern Analysis and Machine Intelligence ,
vol. 45, no. 1, pp. 1328‚Äì1334, 2022.
[29] H. Liu, Z. Dai, D. So, and Q. V . Le, ‚ÄúPay attention to MLPs,‚Äù Advances
in Neural Information Processing Systems , vol. 34, pp. 9204‚Äì9215,
2021.
[30] D. J. Zhang, K. Li, Y . Wang, Y . Chen, S. Chandra, Y . Qiao, L. Liu, and
M. Z. Shou, ‚ÄúMorphMLP: An efficient mlp-like backbone for spatial-
temporal representation learning,‚Äù in European Conference on Computer
Vision . Springer, 2022, pp. 230‚Äì248.
[31] R. Liu, Y . Li, L. Tao, D. Liang, and H.-T. Zheng, ‚ÄúAre we ready for
a new paradigm shift? a survey on visual deep MLP,‚Äù Patterns , vol. 3,
no. 7, p. 100520, 2022.
[32] F. Wang, M. Jiang, C. Qian, S. Yang, C. Li, H. Zhang, X. Wang,
and X. Tang, ‚ÄúResidual attention network for image classification,‚Äù in
Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition , 2017, pp. 3156‚Äì3164.
[33] F. Zhang, Y . Wang, and H. Yang, ‚ÄúEfficient context-aware network for
abdominal multi-organ segmentation,‚Äù arXiv , 2021.
[34] M. J. Cardoso, W. Li, R. Brown, N. Ma, E. Kerfoot, Y . Wang, B. Murrey,
A. Myronenko, C. Zhao, D. Yang et al. , ‚ÄúMonai: An open-source
framework for deep learning in healthcare,‚Äù arXiv , 2022.
[35] I. Loshchilov and F. Hutter, ‚ÄúDecoupled weight decay regularization,‚Äù
inInternational Conference on Learning Representations , 2019.
[36] F. Isensee, P. F. Jaeger, S. A. Kohl, J. Petersen, and K. H. Maier-Hein,
‚ÄúnnU-Net: a self-configuring method for deep learning-based biomedical
image segmentation,‚Äù Nature Methods , vol. 18, no. 2, pp. 203‚Äì211, 2021.
[37] J. Schlemper, O. Oktay, M. Schaap, M. Heinrich, B. Kainz, B. Glocker,
and D. Rueckert, ‚ÄúAttention gated networks: Learning to leverage salient
regions in medical images,‚Äù Medical Image Analysis , vol. 53, pp. 197‚Äì
207, 2019.
[38] Y . Lin, D. Zhang, X. Fang, Y . Chen, K.-T. Cheng, and H. Chen, ‚ÄúRe-
thinking boundary detection in deep learning models for medical image
segmentation,‚Äù in International Conference on Information Processing
in Medical Imaging , 2023.
[39] D. Lian, Z. Yu, X. Sun, and S. Gao, ‚ÄúAs-mlp: An axial shifted
mlp architecture for vision,‚Äù in International Conference on Learning
Representations (ICLR) , 2022.
[40] K. Zhang and D. Liu, ‚ÄúCustomized segment anything model for medical
image segmentation,‚Äù arXiv , 2023.
[41] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson,
T. Xiao, S. Whitehead, A. C. Berg, W.-Y . Lo et al. , ‚ÄúSegment anything,‚Äù
arXiv , 2023.
[42] E. J. Hu, P. Wallis, Z. Allen-Zhu, Y . Li, S. Wang, L. Wang, W. Chen
et al. , ‚ÄúLora: Low-rank adaptation of large language models,‚Äù in Inter-
national Conference on Learning Representations , 2021.
[43] P. Bilic, P. Christ, H. B. Li, E. V orontsov, A. Ben-Cohen, G. Kaissis,
A. Szeskin, C. Jacobs, G. E. H. Mamani, G. Chartrand et al. , ‚ÄúThe liver
tumor segmentation benchmark (lits),‚Äù Medical Image Analysis , vol. 84,
p. 102680, 2023.
[44] M. Antonelli, A. Reinke, S. Bakas, K. Farahani, A. Kopp-Schneider,
B. A. Landman, G. Litjens, B. Menze, O. Ronneberger, R. M. Summers
et al. , ‚ÄúThe medical segmentation decathlon,‚Äù Nature communications ,
vol. 13, no. 1, p. 4128, 2022.
[45] G. Bertasius, H. Wang, and L. Torresani, ‚ÄúIs space-time attention all
you need for video understanding?‚Äù in ICML , vol. 2, no. 3, 2021, p. 4.