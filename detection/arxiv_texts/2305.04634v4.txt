Neural Likelihood Surfaces for Spatial
Processes with Computationally Intensive or
Intractable Likelihoods
Julia Walchessen
Department of Statistics and Data Science, Carnegie Mellon University
and
Amanda Lenzi
School of Mathematics, University of Edinburgh
and
Mikael Kuusela
Department of Statistics and Data Science, Carnegie Mellon University
Abstract
In spatial statistics, fast and accurate parameter estimation, coupled with a re-
liable means of uncertainty quantification, can be challenging when fitting a spatial
process to real-world data because the likelihood function might be slow to evaluate
or wholly intractable. In this work, we propose using convolutional neural networks
to learn the likelihood function of a spatial process. Through a specifically designed
classification task, our neural network implicitly learns the likelihood function, even
in situations where the exact likelihood is not explicitly available. Once trained on
the classification task, our neural network is calibrated using Platt scaling which
improves the accuracy of the neural likelihood surfaces. To demonstrate our ap-
proach, we compare neural likelihood surfaces and the resulting maximum likelihood
estimates and approximate confidence regions with the equivalent for exact or ap-
proximate likelihood for two different spatial processes—a Gaussian process and a
Brown–Resnick process which have computationally intensive and intractable likeli-
hoods, respectively. We conclude that our method provides fast and accurate pa-
rameter estimation with a reliable method of uncertainty quantification in situations
where standard methods are either undesirably slow or inaccurate. The method is
applicable to any spatial process on a grid from which fast simulations are available.
Keywords: deep neural networks, likelihood-free inference, parameter estimation, simulation-
based inference, spatial extremes, uncertainty quantification
1arXiv:2305.04634v4  [stat.ME]  29 Dec 20241 Introduction
In spatial statistics, parametric spatial models used to describe real-world phenomena typ-
ically have intractable or computationally intensive likelihood functions. This is generally
due to the large number of spatial locations the spatial model must cover. Classical meth-
ods of statistical inference rely strongly on the likelihood function to provide parameter
estimation, hypothesis testing, and uncertainty quantification (Casella and Berger, 2001).
Due to this reliance, past research in the field of spatial statistics has focused on providing
approximations for the likelihood that sidestep the need to evaluate the full likelihood (Sun
et al., 2012). Examples include composite likelihood, low-rank approximations, and Vec-
chia approximation (Padoan et al., 2010; Sun et al., 2012). Yet, these approximations suffer
in terms of the accuracy of parameter estimation and uncertainty quantification compared
to the equivalent for exact likelihood (Sun et al., 2012).
Thanks to the advent of modern machine learning and the ability to quickly simulate
from many spatial processes, recent research has focused on neural estimation—efficient
parameter estimation using neural networks—for these processes. To address the compu-
tational inefficiency of Gaussian process parameter estimation, Gerber and Nychka (2021)
trained a neural network to predict the parameters of the covariance function and achieved
similar accuracy yet significant computational efficiency when compared to directly com-
puting the maximum likelihood estimator (MLE). Lenzi et al. (2023) demonstrated that
neural estimation can be extended to other spatial processes by training a neural network
to predict the parameters of several max-stable processes.
Lenzi et al. (2023); Sainsbury-Dale et al. (2023) also proposed an uncertainty quantifica-
tion method for neural estimation via bootstrapping, a computationally intensive approach
with unclear theoretical and practical properties in this context. Unfortunately, neural
estimation does not easily lend itself to more traditional, better-understood methods of
quantifying uncertainty. Another limitation of neural estimation is the reliance on a prior
over the parameter space. In this context, the prior corresponds to the distribution that was
used to simulate the parameters for training the neural network. Depending on the prior
selected, the neural network may produce biased estimates and unreliable bootstrapped
2spatial field
y∈Rs×sspatial field
y∈Rs×sparameter θ∈Rk
NN NN
Parameter Point
Estimator ˆθ∝ L(θ|y)
Neural Likelihood SurfaceParameter Point
Estimator ˆθ
Approximate
Confidence
Region C1−α
Figure 1: The basic structure for traditional neural estimation (left) and learning the likelihood
via our proposed method (right) where NN refers to neural network.
uncertainty quantification. The prior dependence of neural estimation can be established
with a simple argument: Since the neural estimator ˆθis the minimizer of the mean squared
error, it can be understood as a regularized finite-sample estimator of the conditional ex-
pectation E(θ|y) with respect to the conditional distribution p(θ|y)∝p(y|θ)p(θ)
which inescapably depends on the prior p(θ). Finding a way to learn the actual likelihood
function instead of point predictions has the potential to address both of these limitations
of neural estimation.
In this paper, we propose a method to learn the likelihood functions of spatial processes
for which fast simulation is possible using a specifically constructed binary classification
task and apply this method to processes with intractable or computationally intensive
likelihoods. For the selected spatial process, fast simulation is necessary because we use
simulations to form two classes consisting of pairs of a parameter θand a spatial field
realization y. The parameter θand spatial field yare dependent in the first class and
independent in the second class. Yet, the marginal distributions for yandθin both classes
are the same due to a permutation trick in which we form the data for the second class
from the first class by permuting the parameters assigned to the realizations in the first
class. We train a classifier to discriminate between these two classes and show that the
resulting classifier is related to the likelihood of the spatial process via a closed-form trans-
formation. Using the classifier and this transformation, we can produce learned likelihood
3surfaces, parameter estimates, and confidence regions. This method of learning the like-
lihood via classification is well-known in the field of simulation-based inference (Cranmer
et al., 2016; Hermans et al., 2020; Dalmasso et al., 2020). The novelty of the present work
is in introducing this method to spatial statistics and demonstrating its benefits in that
context.
Since convolutional neural networks (CNNs, Lecun et al. (1998)) have excellent empir-
ical performance in tasks involving image data, we use a CNN as our classifier. For neural
estimation, Lenzi et al. (2023); Gerber and Nychka (2021); Sainsbury-Dale et al. (2023);
Lenzi and Rue (2023); Richards et al. (2023) used CNNs as well. However, a key distinc-
tion between neural estimation and our method is the inputs for the CNN–a single input,
a spatial field y, for neural estimation and two inputs, a spatial field yand parameter θ,
for our method–and the outputs of the CNN–a point estimator ˆθfor neural estimation
and a scalar which is proportional to the likelihood up to a known transformation for our
method. See Figure 1 to better understand the basic differences between neural estimation
and our method.
Learning the likelihood accurately requires our CNN to be well calibrated (i.e., to
produce accurate class probabilities) so we apply post-hoc calibration to it using Platt
scaling (Guo et al., 2017). Calibration significantly improves the predicted probabilities
from the CNN and hence the learned likelihood. As such, calibration is an integral part
of this method. We refer to the likelihood function that we compute based on the final
trained and calibrated CNN as neural likelihood.
As a proof of concept, we first apply our method to Gaussian processes, the most
popular stochastic process for modeling spatial data, and show that the neural likelihood
is comparable to the exact likelihood in terms of parameter estimation and uncertainty
quantification with a significant speedup in computation. Since this method of learning
the likelihood is primarily intended for spatial processes with intractable likelihoods, we
next test our method on a Brown–Resnick process, a model for spatial extremes with an
intractable likelihood even for a moderate number of spatial locations, and demonstrate that
neural likelihood is comparable or better than standard methods for approximating this
4likelihood in terms of parameter estimation, uncertainty quantification, and computational
efficiency.
A major benefit of neural likelihood over prediction-based approaches is that it is inde-
pendent of the prior over the parameter space used to generate training data for the neural
network. As mentioned above, prediction-based estimators have the potential to suffer
from bias depending on the selected prior over the parameter space. For our proposed
method, the prior over the parameter space only affects how quickly or to what detail the
neural network implicitly learns the likelihood not whether bias is introduced to the neural
likelihood.
Our proposed method produces parameter estimates grounded in classical statistical
inference. To produce parameter estimates, we evaluate the neural likelihood on a fine
grid over the parameter space and find the maximum likelihood estimator (MLE) over this
grid. Neural likelihood is particularly well suited for grid evaluations like this in terms of
computational efficiency. Since we use grid search, our method is likely to find the global
maximizer even in the case of a non-concave likelihood surface whereas other gradient-based
methods might not.
By learning the likelihood, we can provide a means of uncertainty quantification using
approximate confidence regions derived from the shape of the likelihood surface. These
confidence regions are constructed using likelihood ratios as shown in Section 3.5.3. While
these confidence regions only provide approximate guarantees for finite spatial domains,
empirically we find well-calibrated coverage for these regions for both Gaussian and Brown–
Resnick process parameters.
Finally, the last two benefits of our method are computational efficiency and the ability
to easily handle multiple realizations. First, compared to traditional methods of comput-
ing the exact or approximate likelihood, our method is significantly faster by potentially
several orders of magnitude depending on the spatial process in question and the number
of observed spatial locations because neural networks consist of simple non-linear and lin-
ear functions composed together. In spatial statistics, a long line of research has focused
on methods to speed up likelihood evaluation for tractable spatial processes, especially
5the Gaussian process. Yet, as explained in Section 2.2, these methods can be efficient
but are often inaccurate due to loss of information. In contrast, our method can provide
significant computational speed-ups with accuracy that remains comparable to exact like-
lihood. Lastly, since the likelihood of multiple independent realizations is the product of
the single-realization likelihoods, we can handle an arbitrary number of realizations with-
out retraining the neural network unlike the estimators proposed in Lenzi et al. (2023);
Gerber and Nychka (2021). Although, Sainsbury-Dale et al. (2023) partially addresses the
multiple independent realizations case for neural estimation with a permutation-invariant
neural network.
The structure of the rest of this paper is as follows. Section 2 details how simulation-
based inference has evolved with the aid of modern machine learning and how the field of
spatial statistics has previously addressed inference for spatial processes with intractable
or computationally intensive likelihoods. Section 3 describes our method of learning the
likelihood including how the classifier relates to the likelihood, how the training data for
the classification task is generated using simulation, why we use a neural network as the
classifier, and how to produce neural likelihood surfaces, parameter estimates, and approx-
imate confidence regions from the classifier. In Section 4, we study our method on two
spatial processes–Gaussian and Brown–Resnick processes–and compare the resulting neu-
ral likelihood surfaces, parameter estimates, and confidence regions to those resulting from
conventional methods. Finally in Section 5, we discuss the limitations of our method and
possible future extensions.
2 Background and Related Work
Thanks to advances in computing power and data storage, there is great interest in the
collection, storage, and analysis of increasingly large spatial datasets. As a result, recent
developments in the field of spatial statistics have focused on adapting classical statistical
methods such as likelihood inference for large spatial data (Sun et al., 2012). In conjunction,
the field of simulation-based inference (SBI) has seen tremendous development in methods
for learning surrogate likelihoods, likelihood ratios, and posteriors for high-dimensional data
6with the help of machine learning (Cranmer et al., 2020). In this section, we briefly describe
relevant developments in both spatial statistics and SBI which provide the backdrop for
this work.
2.1 Simulation-Based Inference
In simulation-based inference, there are two classical approaches. The first is Approximate
Bayesian Computation (ABC) in which simulated and observed data are compared using a
distance metric and typically a summary statistic to obtain samples from an approximate
posterior (Sisson et al., 2018). The second approach is density estimation (approximate fre-
quentist computation) in which the distribution of the simulated data or summary statistics
of the simulated data is approximated using traditional density estimation methods (Cran-
mer et al., 2020). Both methods suffer from the curse of dimensionality: in some cases, the
number of simulations necessary for sufficient approximation grows exponentially with the
dimension. The efficiency of both classical approaches to SBI can be improved by utilizing
machine learning which can more easily handle high-dimensional data (Cranmer et al.,
2020).
Beyond improving traditional approaches of SBI, machine learning has ushered in a
new approach in which a surrogate for the likelihood or likelihood ratio is learned using
training data generated from the simulator (Cranmer et al., 2016; Dalmasso et al., 2020,
2023; Brehmer and Cranmer, 2022; Brehmer et al., 2018). Our proposed method is directly
derived from this approach of learning a likelihood surrogate via training a neural network
on simulated data. As far as we are aware, there are three main approaches to learning
the surrogate using machine learning (Brehmer and Cranmer, 2022). The first approach is
neural density estimators which are flexible probabilistic models with tractable likelihoods
that can be combined in such a way as to provide a sufficient surrogate for the likelihood.
This approach differs from our proposed method by additionally approximating the nor-
malizing constant p(y) where yis the observed data. Since likelihood-based parameter
estimation and uncertainty quantification do not depend on the normalizing constant p(y),
our proposed method is potentially much faster and less tricky to train than neural density
7estimation.
The second approach is CARL, a method of learning the likelihood ratio via classifica-
tion and a transformation utilizing the likelihood ratio trick (Cranmer et al., 2016). Our
proposed method is derived from this approach and shares some key similarities. The first
similarity between CARL and our proposed method is the classifier output transformation
to produce the likelihood ratio. The second similarity lies in the classification task. Yet,
the classification task in CARL relies on a reference distribution whereas ours does not.
Additionally, CARL utilizes a different calibration method which may significantly impact
the resulting learned likelihood.
Subsequent work inspired by CARL includes Kaji and Roˇ ckov´ a (2023) and Hermans
et al. (2020) in which a similar process of learning the likelihood ratio is employed to facil-
itate quick computation in Markov chain Monte Carlo algorithms. Additionally, Dalmasso
et al. (2020, 2023) use the learned likelihood ratio in test inversion to obtain confidence
sets and Rizvi et al. (2024) analyze how the loss function and classifier structure affect the
accuracy of the learned likelihood ratio.
Finally, the third approach involves incorporating knowledge about the inner workings
of the simulator, such as latent variables, into learning the surrogate (Brehmer et al.,
2018; Brehmer and Cranmer, 2022). This third approach is most ideal for physical models
in which there are typically latent variables and some knowledge of the simulator. This
approach could also be helpful for simulation-based inference for hierarchical statistical
models. However, in the absence of a hierarchical model and in the interest of full generality,
we will focus in this paper on simulation-based inference that does not attempt to make
explicit use of latent variables.
To the best of our knowledge, simulation-based inference for learning the likelihood has
not been previously applied to spatial statistics. As such, we believe that our approach is
novel in the field of spatial statistics, yet it is well-grounded in recent developments within
the field of simulation-based inference, especially in high-energy physics.
82.2 Spatial Statistics
In spatial statistics, work on likelihood-based inference methods focuses on either expe-
diting the process of evaluating the exact likelihood for spatial processes with tractable
likelihoods or providing an approximation of the likelihood for spatial processes with in-
tractable likelihoods such as composite likelihood (Varin et al., 2011).
Often, when evaluating exact likelihoods of spatial processes over a large number of
spatial locations, the covariance matrix must be inverted, as is the case for Gaussian pro-
cesses (Sun et al., 2012). The time complexity and memory required for matrix inversion
areO(n3) andO(n2), respectively, where nis the number of spatial locations (Sun et al.,
2012, p. 56). To reduce the time complexity of matrix inversion, the original covariance
matrix is replaced with either a low-rank or sparse matrix (Heaton et al., 2017). Low-rank
and sparse matrices are much faster to invert than the original covariance matrix. Yet,
these representations of the original covariance matrix contain less information about the
spatial process which can lead to reduced accuracy in parameter estimation and uncertainty
quantification and a lack of statistical guarantees (Sun et al., 2012).
For Gaussian random fields (GRFs), Lindgren et al. (2011) modeled the spatial field as a
solution of an Stochastic Partial Differential Equation (SPDE) and showed that inference is
of the order O(n3/2) compared to the O(n3) complexity for estimating covariance functions.
These computational benefits are due to estimating the generally sparse precision matrices
(inverse covariance matrices) rather than dense covariance matrices. However, there are
several important classes of spatial processes, such as non-Gaussian models for spatial
extremes, that cannot be handled using the SPDE approach.
Another popular method for efficiently evaluating the likelihood of a Gaussian process
is Vecchia approximation—a method to approximate the joint density of a spatial process
observed at multiple locations as a product of conditional densities according to some
ordering of a subset of locations (Sun et al., 2012; Katzfuss and Guinness, 2021). Due to
the decision to reduce the number of conditioned spatial locations, Vecchia approximation
loses information about the full likelihood and requires design choices as to how to order
the spatial locations and which spatial locations to not include in the conditioning.
9While we cover composite likelihood for Brown–Resnick processes in greater depth in
Section 4, we provide a general description of composite likelihood here. Composite likeli-
hood (Varin et al., 2011) is applicable to many max-stable processes for which the likelihood
is tractable only up to a certain number of spatial locations (Castruccio et al., 2016). For
these spatial processes, the full likelihood involves a summation indexed by the number
of partitions for nspatial locations which grows more than exponentially with respect to
n. Due to this more than exponential growth, the full likelihood is intractable for large
and even moderate n. Yet, if we approximate the likelihood with a product of m−variate
densities for m << n , such as the bivariate density, we reduce the number of terms to
consider. However, even this reduction in terms can be too intensive to compute, and con-
sequentially, only a subset of spatial locations are typically used in the m−variate densities
(Padoan et al., 2010). As with Vecchia approximation, composite likelihood contains less
information about the full likelihood and requires a design choice in terms of which subsets
of spatial locations to include. A poor design choice may lead to a significant deterioration
in the quality of inference as shown in Section S5 of the Supplementary Material for the
Brown–Resnick process.
In all the techniques outlined in this (non-exhaustive) overview of methods for evalu-
ating or approximating exact likelihoods for large spatial data, there are common themes
in terms of loss of information and nontrivial design choices. Our proposed method learns
a surrogate of the exact likelihood that becomes increasingly accurate the more training
data is available and only requires design choices in terms of the neural network structure
and training hyperparameters, choices that can be optimized using a validation dataset.
Finally, many of these previous techniques are specific to certain types of spatial processes,
whereas our method of learning the likelihood is generalizable to any spatial process on
a grid (in fact, any statistical model with regular outputs) for which fast simulation is
possible.
103 Methodology
This section describes our framework for learning the likelihood function from a specifically-
designed classification task using simulated data from a given spatial process. Our proposed
methodology is derived from previous work in simulation-based inference on learning the
likelihood ratio via classification (Cranmer et al., 2016; Hermans et al., 2020; Dalmasso
et al., 2020). Specifically, our task is to infer the parameter θfrom a realization yof a
spatial process evaluated on a finite set of spatial locations:
f∼SpatialProcess( θ),
y:=f(S),(1)
where Sis a set of spatial locations on which we observe a realization ffrom a spatial
process depending on an unknown parameter θ. A key assumption throughout this work
is that for any θ, it is easy to simulate realizations yfrom SpatialProcess( θ) according to
Equation (1).
3.1 Connection between Simulated Data, Classification Task, and
Likelihood
Our methodology hinges on four key observations. First, simulation from many spatial
processes with intractable likelihoods is simple and fast. Fast simulation enables us to
apply likelihood-free inference techniques to these intractable spatial processes in order to
learn the likelihood (Cranmer et al., 2020). Second, the realizations from spatial processes
with different parameters may be distinguishable by the well-trained eye, and as such, one
would expect an image classification model such as a convolutional neural network (CNN)
to be able to distinguish between realizations generated by different parameters. Third,
modern classification algorithms are not necessarily affected by the curse of dimensionality
to the same extent as other methods of directly learning the likelihood might be. Finally,
following Cranmer et al. (2016); Hermans et al. (2020); Dalmasso et al. (2020), we can
draw the connection below between a specifically-designed classification task and the like-
lihood function of a given spatial process using a permutation-based approach described in
11Section 3.2.
Consider the space D×Θ, where Dis the space of realizations for some spatial process
evaluated on spatial locations Sgoverned by parameters from the parameter space Θ which
we assume to be bounded. We will relate the likelihood function L(θ|y) =p(y|θ) for
parameter θ∈Θ and realization y∈Dto the output of a probabilistic binary classifier
h:D×Θ→[0,1],(y,θ)7→h(y,θ). The first class ( C= 1) for the binary classifier is the
class in which pairs of realizations and parameters are dependent. Specifically, the given
realization yis generated from the spatial process with the given parameter θ. The second
class ( C= 2) is the class in which the pairs of realizations and parameters are independent
yet have the same marginal distributions as in the first class. The conditional probabilities
for the two classes have the following form:
P 
(y,θ)|C= 1
=p(y,θ) and P 
(y,θ)|C= 2
=p(y)p(θ), (2)
where p(y) and p(θ) are the marginal distributions implied by p(y,θ),
p(y) =Z
p(y,θ) dθand p(θ) =Z
p(y,θ) dy. (3)
We can now draw the connection between our constructed classifier h(y,θ) and the likeli-
hood function L(θ|y):
h(y,θ) =P 
C= 1|(y,θ)
=P 
(y,θ)|C= 1
P(C= 1)
P 
(y,θ)|C= 1
P(C= 1) + P 
(y,θ)|C= 2
P(C= 2)
=P 
(y,θ)|C=1
P 
(y,θ)|C=2
P 
(y,θ)|C=1
P 
(y,θ)|C=2+ 1=p(y,θ)
p(y)p(θ)
p(y,θ)
p(y)p(θ)+ 1=p(y)−1p(y|θ)
p(y)−1p(y|θ) + 1=p(y)−1L(θ|y)
p(y)−1L(θ|y) + 1.
(4)
On the first line of (4), we apply Bayes’ Rule in order to describe the classifier output
h(y,θ) in terms of the conditional probabilities for the two classes. In the second line, we
use (2) and assume the two classes are balanced, i.e., P(C= 1) = P(C= 2) = 1 /2, for
ease of exposition.
To more clearly connect the classifier and the likelihood function, we define a function
ψ:D×Θ→Rin the following way:
ψ(y,θ) =p(y)−1L(θ|y). (5)
12Substituting the function ψinto (4), we obtain
h(y,θ) =ψ(y,θ)
ψ(y,θ) + 1. (6)
We can solve (6) for ψwhich is proportional to the likelihood function L(θ|y) for a fixed
realization y:
L(θ|y)∝ψ(y,θ) =h(y,θ)
1−h(y,θ). (7)
This shows that the likelihood function is proportional to a simple transformation of the
classifier output hin the case of balanced classes. The same holds for the case of unbalanced
classes; the right side of (7) is simply multiplied by the fixed ratioP(C=2)
P(C=1).
If desired, we can also use the same classifier hto evaluate the likelihood (up to a
multiplicative constant) for an arbitrary number of i.i.d. realizations y1, . . . ,ynfrom a
common spatial process in the following manner:
L(θ|y1, . . . ,yn) =nY
i=1L(θ|yi)∝nY
i=1ψ(yi,θ) =nY
i=1h(yi,θ)
1−h(yi,θ). (8)
Notice that there is no need to change the classifier hifnchanges.
In the following section, we explain how to construct the two classes for training a
classifier ˆhwhich estimates the exact classifier hfrom finite simulated data. With the
estimator ˆh, we can use (5) and (7) to obtain the estimated neural likelihood,
LNN(θ|y) :=p(y)ˆψ(y,θ)∝ˆψ(y,θ) =ˆh 
y,θ
1−ˆh 
y,θ. (9)
3.2 Generating the two specifically constructed classes via simu-
lation
In order to train the classifier ˆh:D×Θ→[0,1] which is an estimator of the exact classifier
hin (4), we need to first obtain samples from the two classes described above. To produce
the first class ( C= 1), we simulate realizations yfrom the spatial process for different
parameters θ∈Θ and pair the simulated realizations with the parameter which generated
the realization. To produce the second class ( C= 2), we simply permute the parameters
assigned to the simulated realizations in the first class to break the dependency between the
13two while maintaining the marginal distributions. In the following paragraphs, we describe
in detail how the classes are constructed using simulation and permutation and provide
pseudocode for the construction process in Algorithms 1 and 2.
First, we sample mparameters from the bounded parameter space Θ in such a way
that the sampled parameters are guaranteed to uniformly cover Θ. It is key to note that Θ
must be bounded in order to obtain samples which accurately represent the whole parameter
space. It is also important to note that the sampling method for Θ is not a prior over Θ
in the Bayesian sense. Our method of learning the likelihood does not require any priors
over Θ in the Bayesian sense. Yet, a poor sampling method that does not provide sufficient
coverage of Θ will result in a neural likelihood surface with less information about the true
likelihood in certain regions of Θ. To avoid this, we utilize Latin hypercube sampling (LHS)
using the LHS R package (Carnell, 2022). In Latin hypercube sampling, Θ is partitioned
intomequal sized regions, and a parameter θiis sampled from each of these mregions and
forms the sampled parameters {θ}i∈[m]. As such, Latin hypercube sampling guarantees
that the sampled parameters {θ}i∈[m]provide uniform coverage of Θ.
Next, for each sampled parameter, we simulate nrealizations from the spatial process
evaluated at locations S. We will specify what Sis in Section 3.3. The n·mpairs of
realizations and the sampled parameters which generated the realizations will form the
first class. The sampling process for the first class is summarized in Algorithm 1.
After simulating data for the first class, we can construct the independent pairs of real-
izations and parameters from the dependent pairs of the first class using permutations of the
sampled parameters. Since there are msampled parameters and nrealizations per sampled
parameter, we randomly sample npermutations of the indices 1 through mwhich we refer
to as π1, . . . , π n. For the jth realizations of the spatial fields, we apply the permutation πjto
the sampled parameters and assign each of the permuted parameters to the corresponding
spatial field realization to obtain pairs of the following form: {(yi,j,θπj(i))|i∈[m]}.
We repeat this process for all nvalues of the index j. The spatial field realizations
and permuted parameters are now independent of each other with unchanged marginal
distributions by construction, as required to satisfy Equation (4). Due to this process
14Algorithm 1 Generating Data for First
Class
foriin 1 : mdo
θi∼LHS(Θ)
forjin 1 : ndo
fi,j∼SpatialProcess( θi)
yi,j=fi,j(S)
end for
end for
The first class is
C1={(yi,j,θi)}i∈[m],j∈[n].Algorithm 2 Constructing Second
Class from First Class
Sample uniformly at random n
permutations π1···πnof indices
1. . . m
Initialize C2=∅
forjin 1 : ndo
C2←C2∪ {(yi,j,θπj(i))|i∈[m]}
end for
The second class is
C2={(yi,j,θπj(i))|i∈[m], j∈[n]}.

(y1,1,θ1). . . (y1,n,θ1)
.........
(ym,1,θm). . . (ym,n,θm)
First Class
(y1,1,θπ1(1)). . . (y1,n,θπn(1))
.........
(ym,1,θπ1(m)). . . (ym,n,θπn(m))
Second Class
Figure 2: A visualization of the pairs of spatial field realizations yand parameters θin the first
and second classes. For both classes, the realizations yi,jin the ith row are generated using
the same θi. In the first class, all realizations yi,jare paired with the θiwhich generated the
realization. In the second class, the realizations yi,jin the jth column are paired with parameters
permuted according to the jth permutation of the total number of parameters m.
of permuting the sampled parameters, we avoid simulating and storing new data for the
second class. Pseudocode for the process of constructing the second class from the first
class is shown in Algorithm 2. See Figure 2 for an illustration of the first and second classes
to more fully understand the permutation process.
3.3 Classifier Architecture
Now that we have described how to learn the likelihood using specifically constructed
classes, we need to select a classifier that can be flexible both in input shape and structure.
As of yet, we have not specified the set of spatial locations Sfor the evaluation of the
spatial process. In this paper, we restrict the set of spatial locations Sto a regular grid
15of fixed size and discuss the potential to relax this restriction in Section 5. The classifier
needs to be flexible enough in structure to accommodate many different spatial processes
since our method of learning the likelihood through classification is in principle applicable
to any spatial process.
Since neural networks are both flexible in input shape and structure and flexible in the
complexity of the functions they can approximate, we use a convolutional neural network
(CNN). CNNs have previously been used by Lenzi et al. (2023); Gerber and Nychka (2021);
Sainsbury-Dale et al. (2023); Richards et al. (2023) to predict parameters of spatial fields.
We continue this practice here, with appropriate modifications as detailed below, because
CNNs are excellent at handling images and the spatial field input yis effectively an image.
See Section S1 of the Supplementary Material for an overview of CNNs.
Since the input for our classifier is both a matrix, the spatial field y, and a vector,
the parameter θ, we need to modify the structure of a typical CNN to accommodate this
type of input. For our CNN classifier, we first transform the spatial field into a flattened
vector of reduced dimension via the convolutional part of the CNN classifier. Since the
flattened vector and the parameter θare both in vector form, we can concatenate the two
and process the concatenated vector with the fully connected part of the CNN classifier
where the last layer outputs the desired class probability. The CNN classifier output is
ˆh(y,θ) =ˆP 
C= 1|(y,θ)
which we obtain by using the sigmoid activation function and
binary cross-entropy loss (Goodfellow et al., 2016, p. 130). Figure 3 illustrates this CNN
architecture. We implement this CNN classifier using the Keras interface (Chollet et al.,
2015) for Tensorflow and train the network using the Adam optimizer (Kingma and Ba,
2015).
3.4 Calibration
Due to growth in model complexity, modern neural networks are not always well-calibrated
(Guo et al., 2017). A classification model is well-calibrated when the estimated class prob-
ability for a given input is sufficiently close to the true probability of the input belonging
16spatial field
y∈Rs×sparameter
θ∈Rk
CNNCNN output
CNN( y)∈Rlconcatenated vector
(CNN( y),θ)∈Rl+kNNfinal output
ˆh(y,θ)∈R
ˆψ∝ L NN(θ|y)
Figure 3: The basic structure of our CNN. In this illustration, CNN refers to the convolutional
and pooling layers and NN refers to the fully connected layers.
to the given class. For binary classification, perfect calibration is defined as
P(C= 1|ˆh(x) =p) =p,∀p∈[0,1], (10)
where xis the input and ˆh(x) =ˆP(C= 1|x) is the estimated probability of the input
belonging to class one (Guo et al., 2017). For neural networks, model capacity is determined
by depth, the number of layers in a neural network, and width, the complexity of each layer.
Our CNN has the potential to suffer from miscalibration depending on the complexity of
the convolutional layers.
To correct this potential miscalibration, we utilize a calibration method called Platt
scaling (Platt, 1999). In Platt scaling, the original class probabilities from a classifier
are used as features for a logistic regression model whose response variables are the true
class labels. The fitted values of this regression are the calibrated class probabilities. The
calibration training data is independent from the data used to train the CNN. Our logistic
regression model has the following form:
logπ(p)
1−π(p)
=β0+β1logp
1−p
, (11)
where pis the uncalibrated probability for class one and π(p) is the corresponding calibrated
probability. In (11), the domain of pis transformed from [0 ,1] toRwhich improves the
logistic regression model. More details on calibration are given in Section 4. From here on,
we use ˆhto denote the calibrated classifier.
173.5 Likelihood Inference with Neural Likelihood
In this section, we describe how to construct likelihood surfaces, parameter estimators, and
approximate confidence regions using the neural likelihood. It is worth noting that the
neural likelihood LNN(θ|y) =p(y)ˆψ(y,θ) is only known up to a multiplicative constant
because p(y) is unknown. In what follows, we evaluate ˆψ(y,θ) over the parameter space Θ.
This surface, which we refer to as the neural likelihood surface, is proportional to the
likelihood LNN(θ|y). As shown below, parameter estimators and approximate confidence
regions are fortunately not affected by the unknown proportionality constant.
3.5.1 Constructing Neural Likelihood Surfaces
First, we explain how to obtain the neural likelihood surface over the parameter space Θ
from the classifier ˆhfor a fixed realization y∈D. Since the parameter space Θ is bounded,
we evaluate the classifier ˆh(y,·) on a regular grid
ΘL={(θ1+α1·i1, . . . , θ k+αk·ik)T|ij∈[sj], j∈[k]} (12)
where α1, . . . , α k>0 are the fixed lengths between points for the respective dimensions,
sj∈Nare the number of points in each dimension, and θjandθj+αj·sjare the endpoints
of Θ for the respective dimension. Then, we transform the classifier outputs ˆh(y,θ) for
θ∈ΘLvia the transformation given in (9). This produces a regular grid of values which
are proportional to the neural likelihood function LNN(· |y) evaluated on the regular grid
ΘLfor the fixed realization y.
Due to modern software packages such as the Keras (Chollet et al., 2015) interface to
Tensorflow, we can quickly construct the neural likelihood surface using our CNN in the
following manner. For a given realization y∈D, we evaluate the CNN with the vectorized
input of the realization yand parameters in the regular grid, {(y,θl)|θl∈ΘL}, to obtain
the neural likelihood surface. The CNN can handle this vectorized input at once, and
thus, we can efficiently evaluate the neural likelihood surface. Moreover, once trained and
calibrated, the CNN is amortized and thus, we can efficiently evaluate the neural likelihood
surface for multiple realizations y. To demonstrate our method, we compare the neural
18likelihood surface to the exact or approximate likelihood surface constructed using the same
process on the same regular grid Θ L.
Finally, the surfaces we construct are truly the log neural and log exact and log ap-
proximate likelihood surfaces. As shown in Section 3.1, the transformed classifier output
ˆψis equivalent to the neural likelihood up to a multiplicative constant. By applying a log
transformation to the logit-transformed classifier output in (9), the multiplicative constant
becomes an additive constant. Consequentially, the ranges of the log neural and log exact
likelihood surfaces should have the same range of values yet shifted by some unknown con-
stant. By constructing log neural and log exact or log approximate likelihood surfaces, we
facilitate comparisons between the surfaces.
3.5.2 Constructing Parameter Estimators from Neural Likelihood Surfaces
Parameter estimation for the neural and exact or approximate likelihoods is a discrete
version of maximum likelihood estimation (MLE) in which we select the parameter on the
grid ΘLwhich maximizes the given surface,
ˆθNN= arg max
θ∈ΘLLNN(θ|y) = arg max
θ∈ΘLp(y)ˆψ(y,θ) = arg max
θ∈ΘLˆψ(y,θ). (13)
In the rest of the paper, we refer to the estimator in (13) as the neural parameter estimator
ˆθNN. Following the same process of determining the parameter which maximizes the surface
over ΘL, we produce exact likelihood estimators ˆθand approximate likelihood estimators
ˆθapprox :
ˆθ= arg max
θ∈ΘLL(θ|y) and ˆθapprox = arg max
θ∈ΘLLapprox (θ|y). (14)
There are two major reasons why we use a grid-based approach rather than a gradient-
based approach for parameter estimation via maximum likelihood. First, due to the way we
construct the neural likelihood, its exact gradient is only available via automatic differentia-
tion; see Section 5 for further discussion. To fairly compare neural and exact or approximate
likelihood, we use the same grid-based maximization approach for each function in order
to produce a fair comparison between parameter estimators. Second, a gradient-based
maximization approach may suffer from a local maxima problem whereas a grid-based
19maximization approach does not. If the exact likelihood were non-convex, a gradient-based
approach might find a local maximum rather than a global maximum and hence potentially
select a parameter far from the global maximizer. Yet, the grid-based approach would se-
lect a parameter which is reasonably close to the global maximizer over Θ depending on
the resolution of the grid. Hence, the grid-based approach enables global maximization of
the likelihood within Θ.
3.5.3 Constructing Approximate Confidence Regions from Neural Likelihood
Surfaces
Next, we explain how to construct approximate confidence regions from the neural, exact,
and approximate likelihood surfaces. Specifically, we construct confidence regions from an
inversion of the likelihood ratio test because these confidence regions tend to have better
coverage accuracy than those constructed using inversion of other asymptotic methods
(Casella and Berger, 2001; Vaart, 1998).
In the likelihood ratio test, we reject the null hypothesis that the true parameter is θ0
if the−2×log-likelihood ratio test statistic is greater than some critical value cα:
reject H0if−2 log( λ(θ0))> cα,where λ(θ0) =L(θ0|y)
L(ˆθ|y), (15)
withcαchosen such that the test has significance level αfor large enough sample sizes (Shao,
2003, p. 429). Depending on the spatial process, the distribution of the likelihood ratio
test statistic −2 log( λ(θ)) converges under the null hypothesis to a chi-squared distribution
with degrees of freedom equal to the dimension of the parameter space Θ under increasing
spatial domain asymptotics (Gelfand et al., 2010, p. 50). Therefore, we set cα=χ2
k,1−α,
the 1−αquantile of the χ2
kdistribution.
Mardia and Marshall (1984) showed that the maximum likelihood estimator for the co-
variance parameters of a Gaussian process is normally distributed with unbiased mean and
the inverse of the Fisher information as the covariance matrix under increasing-domain
asymptotics. As a consequence, the likelihood ratio statistic converges to a chi-squared
distribution as the spatial domain increases (Davison, 2003). Therefore, we expect the ap-
proximate confidence regions constructed from the likelihood ratio test to have the desired
20coverage in our first case study using Gaussian processes in Section 4.1 because the spatial
domain is sufficiently large.
Our second case study in Section 4.2 involves Brown–Resnick processes whose asymp-
totic properties are less well-studied. There is some work indicating that the asymptotic
distribution of the maximum likelihood estimator in the multiple realizations case has the
aforementioned properties which implies that the likelihood ratio statistic is asymptoti-
cally chi-squared distributed (Dombry et al., 2017; Davison, 2003). Consequently, if we
can connect the asymptotic theory for multiple realizations with the asymptotic theory
for increasing spatial domain, we have shown the likelihood ratio statistic has the desired
asymptotic properties. Ergodicity provides such a connection and, indeed, under certain
conditions, max-stable processes, including Brown-Resnick processes, are ergodic (Stoev,
2008). As such, there is some indication that our constructed approximate confidence re-
gions should have the intended coverage for Brown–Resnick processes, a claim that will be
evaluated in detail in our experiments in Section 4.2.
We can construct a 1 −αconfidence set C1−α(Θ) by inverting the likelihood ratio test
(Casella and Berger, 2001). The 1 −αconfidence set C1−α(Θ) consists of all those parameters
in Θ for which the null hypothesis in (15) is not rejected:
C1−α(Θ) = {θ∈Θ| −2 log( λ(θ))≤cα}
={θ∈Θ| −2
log(L(θ|y))−log(L(ˆθ|y))
≤cα}
={θ∈Θ|2
log(L(ˆθ|y))−log(L(θ|y))
≤cα}.(16)
To accommodate the evaluation of the likelihood on a regular grid, we restrict (16) to
parameters on the grid ΘL⊂Θ:
C1−α(ΘL) ={θ∈ΘL|2
log(L(ˆθ|y))−log(L(θ|y))
≤cα}. (17)
For the approximate likelihood, we simply replace L(θ|y) with Lapprox (θ|y) in (17) to
obtain Capprox ,1−α(ΘL). Finally, we can replace L(θ|y) with ˆψto obtain CNN,1−α(ΘL),
CNN,1−α(ΘL) ={θ∈ΘL|2 
log(LNN(ˆθ|y))−log(LNN(θ|y))
≤cα}
={θ∈ΘL|2 
log(p(y)ˆψ(y,ˆθ))−log(p(y)ˆψ(y,θ))
≤cα}
={θ∈ΘL|2 
log(ˆψ(y,ˆθ))−log(ˆψ(y,θ))
≤cα},(18)
21because the proportionality constant p(y) cancels out in (18).
There are two reasons why we construct confidence regions. First, we would like to
understand how reliable the neural likelihood surfaces are in terms of providing accurate
uncertainty quantification for parameter estimation as compared to the exact or approx-
imate likelihoods. Often, methods utilizing neural networks for statistical inference focus
purely on parameter point estimation as a prediction task and generally do not provide
a measure of uncertainty with their point estimates. If shown to be reliable, confidence
regions constructed from the neural likelihood provide a principled method of uncertainty
quantification for parameter estimation. In Section 4, we demonstrate the reliability of
these confidence regions by analyzing their empirical coverage and size across the parame-
ter space Θ. Second, these confidence regions provide a sanity check for whether the neural
likelihood is an accurate representation of the exact likelihood when the latter is unavail-
able. If the empirical coverage of the confidence sets derived from the neural likelihood
surfaces for a sufficiently large number of realizations across the parameter space matches
the expected coverage, then that increases our confidence that the neural likelihood is an
accurate representation of the exact likelihood.
4 Case Studies
While our method of learning the likelihood function is applicable to any spatial process
for which fast simulation is possible, we are most interested in providing inference for
spatial processes with intractable likelihoods. Yet, in the intractable case, we can not
directly compare the neural and exact likelihood surfaces. To address this issue, we first
demonstrated that our method works as expected using a popular spatial process with a
tractable likelihood, a Gaussian process, in our first case study. In our second case study,
we applied our method to a spatial process with an intractable likelihood, a Brown–Resnick
process, and compared the neural likelihood to the pairwise likelihood, a well-established
approximation for the exact likelihood.
224.1 First Case Study: Gaussian Process with Computationally
Intensive Likelihood
The primary purpose of this case study is to validate our method in a case where the
exact likelihood is available. A secondary purpose is to demonstrate the computational
benefits of our method. Gaussian processes are often used to model spatial data, yet the
time complexity of exact likelihood scales cubically with the number of spatial locations
as described in Section 2.2. As such, developing a more computationally efficient method
that provides a comparable quality of parameter estimation and uncertainty quantification
as exact likelihood is an important issue in of itself. As shown in this section, our method
of learning the likelihood provides such an approach.
4.1.1 Description of Gaussian Process and Its Exact likelihood
We applied our method to a Gaussian process with a zero mean function and an exponential
covariance function with two positive parameters, length scale ℓand variance ν. Hence,
the model is
f∼GP(m(x), k(x,x′)),where m(x) = 0 and k(x,x′) =νexp(−∥x−x′∥2
ℓ). (19)
For a realization y=f(S), where Sis a regular finite grid Sofslocations with covariance
matrix Σ= 
k(xi,xj)i,j
∈Rs×s, the log likelihood is
log(L(θ|y)) =−1
2y⊺Σ−1y−s
2log(2 π)−1
2log(det( Σ)), (20)
where θ= (ν, ℓ) (Rasmussen and Williams, 2006, pp. 81–96).
The two most computationally intensive operations in (20) are computing the inverse
and the determinant of the covariance matrix Σeach with a time complexity of O(s3).
To efficiently compute both of these operations, we utilize the Cholesky decomposition
of the symmetric, positive-definite matrix Σ(Rasmussen and Williams, 2006, pp. 202–
203). While Cholesky decomposition is computationally expensive with a time complexity
ofO(s3), substituting the Cholesky decomposition for the covariance matrix reduces the
number of O(s3) operations from two to one.
234.1.2 Experiments
Input Details Since we are using a CNN as our classifier, the input shape for the
spatial field and parameter must be fixed. The spatial field is a realization y∈R25×25
of the spatial process for a given parameter on a 25 ×25 regular grid Swith spatial
domain D= [−10,10]×[−10,10]. The parameter θis two-dimensional, and the evaluation
parameter space Θ is Θ = (0 ,2)×(0,2). The training parameter space ˜Θ was extended
beyond the evaluation parameter space Θ to ˜Θ = (0 ,2.5)×(0,2.5) to better learn the
likelihood and prevent potential biases at the boundary. For more training details, see
Section S2.2 of the Supplementary Material.
Simulating Training and Validation Data To generate the training data, we sim-
ulated n= 500 spatial field realizations for each of the m= 3000 parameters sampled
from the training parameter space ˜Θ via Latin hypercube sampling as explained in Section
3.2. Separately from the training data, we simulated n= 500 spatial field realizations for
each of m=300 parameters sampled from ˜Θ via Latin hypercube sampling to serve as the
validation data.
Architecture Here we provide details about the CNN architecture described in Section
3.3. An ideal architecture is an architecture flexible enough to learn the likelihood for a
large variety of spatial processes. To achieve such versatility, the CNN architecture must
be complex enough to be able to learn a potentially rough, non-convex likelihood yet not
too complex to overfit when learning a smooth, convex likelihood. While the ability to
learn the likelihood for any spatial process is most likely too ambitious, the architecture
we propose does have the flexibility to learn the likelihoods for both Gaussian and Brown–
Resnick processes. This architecture is motivated by the architecture of the CNN trained
for neural estimation in Lenzi et al. (2023).
In the first part of the architecture, there are three convolutional layers with a decreasing
number of filters starting with 128 filters for the first, 128 filters for the second, and 16 for
the third. All the convolutional layers have the same kernel size and activation function
which are 3 ×3 and ReLU, respectively. Between these convolutional layers are max pooling
24layers with the same kernel size of 2 ×2. The convolutional part outputs a flatten vector
of size 64 to which we concatenate the parameter θ∈R2. In the fully connected part
of the architecture, there are 4 layers with a decreasing number of neurons and the same
ReLU activation function except for the last layer which has a softmax activation function
to handle probabilistic binary classification. The output is a 2-d vector in which the first
entry is the predicted probability ˆh(y,θ) = ˆP(C= 1|(y,θ)). See Table S1 in the
Supplementary Materials for the specific architecture.
Training and Model Selection We trained several models with varying configurations
of batch size and learning rate schedule for twenty epochs. We selected a model with the
lowest validation loss amongst the versions in which there was little to no indication of
overfitting in the plot of training and validation loss. The selected model was trained with
a batch size of 30000 and a learning rate schedule in which the the learning rate started
at 0.001 for the first five epochs and decreased by a multiplicative factor of e−0.1for each
epoch after the first five. Due to the large batch size, we distributed training over four
NVIDIA Tesla K80 GPUs at the most to address memory requirements for such a large
matrix (i.e., 30000 ×25×25).
Calibration First, we generated training data by simulating n= 50 realizations per each
of the m= 3000 parameters sampled via Latin hypercube sampling from the evaluation
parameter space Θ. With the same process, we generated test data: n= 50 realizations per
each of the m= 300 sampled parameters. The training and test data for calibration were
independent of the data used to train the CNN. After constructing the two classes for both
training and test data, we used the uncalibrated CNN to obtain the corresponding classifier
outputs. These classifier outputs coupled with the corresponding class labels formed the
training and test data for the logistic regression model in (11).
To determine the effectiveness of calibration, we examined reliability diagrams as well
as the neural likelihood surfaces before and after calibration for the test data. The relia-
bility diagram depicts the empirical class probability as a function of the predicted class
probability (Guo et al., 2017). For a perfectly calibrated model, the reliability diagram
25Figure 4: Reliability diagram for class one before calibration (left) and after calibration (right)
for Gaussian process.
should show the identity function. Thus, the closer the calibrated probabilities are to the
identity function, the more effective the calibration.
The reliability diagram after calibration in Figure 4 shows great improvement in how
closely the predicted and true class probabilities match. Before calibration, the empirical
class one probability is generally lower than expected for any given predicted class one
probability. Calibration fixes this tendency for the classifier to overestimate the probability
a given realization and parameter is in the first class, the class in which the parameter θ
and realization yare dependent. Since learning the likelihood relies on estimating this
probability correctly, fixing this tendency produces calibrated neural likelihood surfaces in
which the area of high likelihood is larger and improves approximate confidence regions
and empirical coverage as we demonstrate later in this section.
Evaluation We discuss now in detail how we evaluated the neural likelihood’s perfor-
mance as compared to standard methods. First, we created an evaluation dataset by
generating n= 200 spatial field realizations per parameter on a 9 ×9 regular grid over
the evaluation parameter space Θ = (0 ,2)×(0,2) for both spatial processes. If there
are certain regions in which the CNN underperforms, we can detect these regions when
evaluating surfaces, parameter estimators, approximate confidence regions, and empirical
coverage with this evaluation data.
26Figure 5: Surfaces for exact log exact likelihood (left), neural likelihood before calibration (cen-
ter), and neural likelihood after calibration (right) for a realization of a Gaussian process with
parameters ν= 0.8 and λ= 0.8. In each figure, the color scale ranges from the maximum value
of the surface to ten units less than the maximum value.
Evaluation: Surfaces Using the process described in Section 3.5.1, we constructed the
neural and exact likelihood surfaces with a 40 ×40 regular grid ΘLin (12) over the evaluation
parameter space Θ = (0 ,2)×(0,2). As mentioned in Section 3.5.1, the surfaces are truly
the log likelihood surfaces which ensures the ranges of the exact and neural likelihoods
differ by an additive constant rather than a multiplicative constant. For visualizations, we
utilized a color scale in which the maximum values of both the color scale and the particular
surface match to deal with the shifted range due to the additive constant. We selected a
range of ten units for the color scale because an approximate confidence region in a two-
dimensional parameter space has 99% coverage probability for a cut-off value C.01= 9.21,
the 99% quantile of a χ2distribution with two degrees of freedom in (16).
Across the evaluation parameter space Θ, the areas of high likelihood for the neural and
exact likelihood surfaces are similar in shape and location yet differ in size. In correcting the
classifier’s tendency to overestimate the class one predicted probability, calibration spreads
out the high-likelihood region. Consequentially, the neural likelihood surfaces more closely
resemble the exact likelihood surfaces in terms of the size of the high-likelihood region.
Yet, the area of high likelihood is slightly too large after calibration. See Figure 5 for a
visualization of these observations which hold true for all surfaces of spatial field realizations
generated for any parameter in Θ.
27Figure 6: Parameter estimates for exact and neural likelihood for a Gaussian process. Each
of the 16 plots contains the true parameter (black star) which generated the 200 spatial field
realizations and the corresponding parameter estimates for exact likelihood (purple) and neural
likelihood (green) with mean squared error (MSE) in the legend. The true parameter increases
in variance from bottom to top and in length scale from left to right.
Evaluation: Parameter Estimation To compare the parameter estimates for neural
and exact likelihood, we display a 4 ×4 grid of plots in which each plot contains the param-
eter estimates from both methods for 200 realizations generated from a single parameter
in the evaluation data. The parameter estimates for the neural likelihood before and after
calibration are the same because Platt scaling is a monotonic transformation. Since the
parameter estimates are all on the same 40 ×40 grid, the estimates are jittered with a
small amount of noise in Figures 6 and 9 to distinguish the individual estimates. The
parameters which generated the realizations range from 0 .4 to 1 .6 by increments of 0 .4 for
28both parameters.
From Figure 6, we conclude that the parameter estimates for both methods are com-
parable across the evaluation parameter space Θ. This indicates that the neural likelihood
serves as a great surrogate for the exact likelihood in terms of point estimation. The neural
likelihood estimator ˆθNNhas greater variance than the exact likelihood estimator ˆθ, yet the
patterns of behavior are the same for both estimators across Θ. For instance, as the true
length scale ℓincreases, both exact and neural likelihood estimators increase in variance
along the length scale axis.
Evaluation: Approximate Confidence Regions We constructed 95% approximate
confidence regions for each realization in the evaluation data. As shown in Figure 5,
the neural likelihood approximate confidence regions become larger yet retain their shape
and location after calibration because Platt scaling is a monotonic transformation. After
calibration, the neural and exact likelihood confidence regions are more comparable in
shape, location, and size because the neural likelihood confidence regions increase in size
from 60% to 120% of the size of the exact likelihood confidence regions on average.
Evaluation: Empirical Coverage and Confidence Region Area We computed
empirical coverage and confidence region area for the evaluation data. For each of the two
hundred realizations {yj,l}j∈[200]per parameter θlon the 9 ×9 grid, we determined whether
the true parameter θlis in the 95% approximate confidence region as well as the area of
the region. The empirical coverage for the parameter θlis
1
nnX
j=11 
θl∈ CNN,.95(ΘL,yj,l)
, (21)
where n= 200. We visualize the empirical coverage and confidence region area with a heat
map over the evaluation parameter space Θ to determine whether the coverage and area
vary over Θ.
As shown in Figure 8, calibration increases average confidence region area for neural
likelihood from 60% to 120% of the average area for exact likelihood. Confidence region
area for exact and neural likelihood become more comparable after calibration, and thus,
29Figure 7: Empirical coverage for 95% approximate confidence regions for the exact likelihood (left)
and neural likelihood before calibration (center) and after calibration (right) for 200 realizations
per parameter on a 9 ×9 grid over Θ.
Figure 8: 95% approximate confidence region area for exact likelihood (left) and neural likelihood
before calibration (center) and after calibration (right) for 200 realizations per parameter on a
9×9 grid over Θ.
empirical coverage for neural likelihood increases from 80% to approximately 93% across
Θ as shown in Figure 7. Additionally, the coverage for both exact and neural is essentially
uniform across Θ.
Evaluation: Timing Study We conducted a timing study for evaluating the neural
and exact likelihood surfaces using an Intel Core i7-10875H processor with eight cores, each
with two threads. For fifty realizations yi∈R25×25, we recorded the average elapsed time
and standard deviation in evaluating the neural or exact likelihood surface on a 40 ×40
grid over the evaluation parameter space Θ. As mentioned in Section 3.5.1, the CNN can
handle vectorized inputs which significantly accelerates evaluation of the neural likelihood
surface. To understand the impact of vectorization, we timed the evaluation of the neural
30Table 1: Time to produce neural and exact likelihood surfaces on a 40 ×40 grid over Θ for 50
realizations of a Gaussian process on a 25 ×25 grid with a spatial domain of [ −10,10]×[−10,10].
Type of surface and method average (sec) std. dev. (sec)
exact likelihood 71.67 1.02
unvectorized neural likelihood 13.46 0.26
vectorized neural likelihood 2.26 0.34
likelihood when using both vectorized and unvectorized inputs.
As far as we are aware, there is no prepackaged way to vectorize the evaluation of the
exact likelihood. As such, when conducting a similar study for the exact likelihood, we
used Cholesky factorization and multicore processing but no vectorization when evaluating
the exact likelihood for the same fifty realizations using the same processor.
As shown in Table 1, vectorizing reduces the time to compute the neural likelihood
surface by a factor of approximately 6. Due to a discrepancy in the ability to handle vec-
torized computations, the time difference between constructing neural and exact likelihood
surfaces is significant. The vectorized neural likelihood surfaces are approximately thirty
times faster to produce than exact likelihood surfaces. Since the computational cost of
exact likelihood increases cubically with the number of spatial locations, we expect that
the computational efficiency of vectorized neural likelihood will only increase in comparison
to exact likelihood as the number of locations increases.
We did not include the upfront cost of neural network training in the recorded times
of evaluating the neural likelihood surface as we are considering the ideal use case for this
method—evaluating the amortized neural likelihood repeatedly under the same inference
framework in which case the upfront cost of training diminishes in importance. However,
for reference, training the neural network on a cluster of 4 NVIDIA Tesla K80 GPUs with
the given data and batch sizes took less than ten hours.
Evaluation: Multiple Realizations As explained in Section 3.1, our method of learn-
ing the likelihood is applicable to the case of an arbitrary number of i.i.d. realizations
31y1. . .ynfrom the spatial process without needing to retrain the single-realization neural
network. To demonstrate this, we generated evaluation data for five i.i.d. spatial field re-
alizations in the same manner as we did in the single realization case and evaluated neural
likelihood according to (8) for the evaluation data. We display the 4 ×4 plot of the resulting
parameter estimates as we do for the single realization case.
The parameter estimators for exact and neural likelihood in the multiple i.i.d. realiza-
tions case in Figure 9 are more accurate and have less variance than equivalent estimators
for the single realization case in Figure 6, as expected. As in the single realization case, the
neural likelihood estimator ˆθNNhas slightly greater variance than the exact likelihood esti-
mator ˆθ, yet the patterns of behavior are similar for both estimators across the evaluation
parameter space Θ.
Summary of Results From this case study, we demonstrated that our method of learn-
ing the likelihood is both accurate and computationally efficient for Gaussian processes.
Once calibrated, the neural likelihood surfaces, parameter estimates, and approximate con-
fidence regions are comparable to the equivalent for exact likelihood. From the evaluation
of neural likelihood surfaces, confidence regions, and empirical coverage before and after
calibration, it is clear that calibration is essential to our method in order to achieve results
comparable to exact likelihood. Finally, the vectorized neural likelihood surfaces are sig-
nificantly faster to evaluate than exact likelihood surfaces because neural networks are fast
to evaluate and our CNN, once trained, is amortized. Thus, we have both validated our
method in a case where the exact likelihood is available and shown this method is more
computationally efficient than exact likelihood.
4.2 Second Case Study: Brown–Resnick Process with an In-
tractable Likelihood
We next apply our method to a Brown–Resnick process which has an intractable likelihood.
Yet, there is composite likelihood, an approximation for the exact likelihood (Castruccio
et al., 2016), to which we can compare neural likelihood. Our selection of a Brown–Resnick
32Figure 9: Parameter estimates for exact and neural likelihood in the case of 5 i.i.d. spatial field
realizations for a Gaussian process. Each of the 16 plots contains the true parameter (black star)
which generated the realizations and the corresponding parameter estimates for exact likelihood
(purple) and neural likelihood (green) with mean squared error (MSE) in the legend. The true
parameter increases in variance from bottom to top and in length scale from left to right.
Process for this case study is in part due to its use as a case study for neural estimation in
Lenzi et al. (2023).
4.2.1 Description of Brown-Resnick Process and its Approximate Likelihood
The Brown–Resnick process has two parameters– λ∈R+, a range parameter which deter-
mines the degree to which the spatial process at locations a fixed distance away impact the
spatial process at a given location, and ν∈(0,2], a smoothness parameter which deter-
mines the overall degree of smoothness of the process across the spatial domain. As such,
33the parameter space Θ for which we are interested in learning the likelihood is a bounded
subset of R+×(0,2].
The likelihood involves a summation over the set of all possible partitions of the spatial
locations {si}i∈[n]at which the Brown-Resnick process is observed (Castruccio et al., 2016).
This set has cardinality equal to the bell number Bnwhich grows more than exponentially
with respect to n, the number of spatial locations. Since the number of spatial locations
is generally large in practice, computing the likelihood for a Brown–Resnick process is
intractable in many practical cases.
Yet, in the bivariate case of only two spatial locations, the likelihood has a simple closed
form. Using the bivariate case, we can provide an approximation for the full likelihood of
a Brown–Resnick process called pairwise likelihood, a form of composite likelihood. In
pairwise log likelihood, the summands are the bivariate log likelihoods between two spatial
locations sj1andsj2forj1, j2∈[n], and involve only select pairs ( sj1,sj2) because even a
summation over alln(n−1)
2pairs is computationally intensive for a large number of locations.
Since nearby locations contain the most information about a given location, the selec-
tion criterion generally is determined by the distance between locations. When we compute
pairwise likelihood in Section 4.2.2, all pairs of observations whose locations are within a
certain cut-off distance δof each other are included. The cut-off distance δis a tuning
parameter that must be appropriately selected in order to obtain reasonable results. In
practice, if the cut-off distance is too small or too large, the maximum pairwise likelihood
estimates can be highly inaccurate, and the pairwise likelihood surfaces tend to be un-
informative as shown in Section S5 of the Supplementary Material. In contrast, neural
likelihood does not involve such tuning parameters.
Additionally, the asymptotic behavior of the maximum pairwise likelihood estimator
and, subsequently, the pairwise likelihood ratio statistic is different than the equivalent for
the full likelihood. To enable asymptotic inference, Chandler and Bate (2007) proposed
adjusting the pairwise likelihood such that the asymptotic distribution of the adjusted
pairwise likelihood ratio statistic is the same as that of the full likelihood. See Sections
S3 and S4 of the Supplementary Material for further technical details on adjusting the
34pairwise likelihood.
4.2.2 Experiments
Unless otherwise stated, the details of this case study are the same as the Gaussian process
case study in Section 4.1.
Training and Model Selection As in the Gaussian process case, the evaluation pa-
rameter space is Θ = (0 ,2)×(0,2). For the Brown–Resnick process, the training parameter
space ˜Θ is the same as evaluation because training difficulties arise if the training param-
eter space is extended. The selected model was trained with a batch size of 50 and a
learning rate schedule in which the learning rate started at 0 .002 for the first five epochs
and decreased by a multiplicative factor of e−0.1for each of the fifteen epochs after the first
five. See Section S2.3 of the Supplementary Material for more detail as to why the given
batch size and learning rate were selected and what training issues we encountered when
extending the training parameter space.
Calibration The reliability diagrams before and after calibration in Figure 10 show
great improvement in how closely the predicted and true class probabilities match after
calibration. Before calibration, the classifier either underestimates or overestimates the
probability of a given parameter and spatial field pair belonging to class one depending
on the predicted class one probability. Calibration largely fixes these tendencies, and the
resulting calibrated neural likelihood surfaces and approximate confidence regions are more
accurate as shown later in this section.
Pairwise Likelihood We compare neural likelihood to both unadjusted and adjusted
pairwise likelihood for distance cut-off δ= 2. See Section S5 of the Supplementary Ma-
terial for comparisons of neural and pairwise likelihood for various distance cut-offs δ. To
compute pairwise likelihood, we used the fitmaxstab function in the R package SpatialEx-
tremes (Ribatet, 2020). The fitmaxstab function fits data to a max-stable process such
as a Brown–Resnick process and returns a list of objects including the function nllh, the
35Figure 10: Reliability diagrams for class one before calibration (left) and after calibration (right)
for Brown–Resnick process.
negative log pairwise likelihood function for the given data and weighting scheme. For each
realization yin the evaluation data, we produced a pairwise likelihood surface, parameter
point estimate and approximate confidence region by evaluating the nllh function at each
parameter on the 40 ×40 grid over Θ.
Adjusted Pairwise Likelihood We adjusted the pairwise likelihood surfaces for δ= 2
according to the process described in Sections S3 and S4 of the Supplementary Material.
Adjusting the pairwise surfaces for δ= 1 case failed. See Section S4 of the Supplementary
Material for details as to why the adjustment failed in this case. We do not display sepa-
rate parameter estimates for the unadjusted and adjusted pairwise likelihoods because the
pairwise likelihood parameter estimates are unchanged after the adjustment. As described
in Section S4 of the Supplementary Material, there are two methods available to perform
the adjustment—Cholesky factorization and eigendecomposition. In this paper, we dis-
play only results using Cholesky factorization as this method produced the better results
between the two adjustments in terms of empirical coverage and confidence region area.
Evaluation: Surfaces We compare the neural likelihood surface before and after cali-
bration to both the unadjusted and adjusted pairwise likelihood surfaces for distance cut-off
δ= 2 for the same realization yin the evaluation data in Figure 11. We do not expect
the neural and pairwise likelihood surfaces, whether adjusted or not, to exactly mirror
36Figure 11: The pairwise likelihood surface for distance cut-off δ= 2 before adjustment (far left)
and after adjustment (center left) and neural likelihood surface before calibration (center right)
and after calibration (far right) for a realization of a Brown–Resnick process with parameters
ν= 0.8 and λ= 0.8. In each figure, the color scale ranges from the maximum value of the surface
to ten units less than the maximum value.
each other because pairwise likelihood is an approximation of the exact likelihood. In gen-
eral, the area of high likelihood in the pairwise surface increases after adjustment yet, as
expected, the point estimate is unchanged. The neural and pairwise likelihood surfaces,
whether adjusted or not, exhibit very different behavior in shape, location, and size of the
area of high likelihood. Calibration increases the area but maintains the shape and location
of the high likelihood region in the neural likelihood surface.
Evaluation: Parameter Estimation In Figure 12, we compare parameter estimates
for neural and pairwise likelihood for cut-off δ= 2, the optimal cut-off in terms of pairwise
parameter estimation. To determine the optimal cut-off, we varied δand observed that the
accuracy of pairwise estimates increases as δincreases from 0 to approximately 2 and then
decreases as δincreases beyond 2 as shown in Table 2. See Section S5 of the Supplementary
Material for pairwise estimates with different cut-offs δ. From Figure 12, we observe that
the neural and pairwise estimates are similar in terms of accuracy and behavior except in
certain areas of the evaluation parameter space Θ. For large smoothness and small range,
the pairwise estimates are significantly more accurate than the neural estimates because
the pairwise estimates are less biased and have less variance. For small smoothness and
large range, the neural estimates are more accurate than the pairwise estimates.
37Figure 12: Parameter estimates for neural likelihood and pairwise likelihood with δ= 2. Each
of the 16 plots contains the true parameter (black star) which generated the 200 spatial field
realizations and the corresponding parameter estimates for pairwise likelihood (purple) and neural
likelihood (green) with mean squared error (MSE) in the legend. The true parameter increases
in range from bottom to top and in smoothness from left to right.
Evaluation: Approximate Confidence Regions As shown in Figure 11, the ap-
proximate confidence regions for pairwise likelihood increase in size after adjustment. The
neural and pairwise confidence regions have remarkably different shapes and sizes no matter
whether the pairwise likelihood is adjusted or not. As in the Gaussian process case, cali-
bration increases the size while preserving the shape and location of the neural likelihood
confidence region.
Evaluation: Empirical Coverage and Confidence Region Area After adjust-
ment, confidence region area and thus empirical coverage increase for the 95% approximate
38Table 2: Root mean squared error (rmse), mean absolute error (mae), and median of the median
absolute error (mmae) for all parameter estimates (200 estimates per parameter on a 9 ×9 grid
over Θ) for a Brown–Resnick process for pairwise likelihood with different distance cut-offs δand
neural likelihood. The best results for each metric are in bold.
Type of surface rmse mae mmae
pairwise likelihood ( δ= 1) 0.51 0.74 0.65
pairwise likelihood ( δ= 2) 0.25 0.30 0 .20
pairwise likelihood ( δ= 5) 0.28 0.37 0.25
neural likelihood 0.24 0 .30 0 .20
confidence regions constructed from pairwise likelihood surfaces as shown in Figures 14
and 13 respectively. Before adjustment, the confidence regions are unrealistically small,
and thus, empirical coverage is low. After adjustment, the confidence regions are reason-
ably sized and achieve decent empirical coverage as a result. Yet, empirical coverage suffers
near the boundaries of the parameter space Θ for adjusted pairwise likelihood and even
fails for a few parameters near the boundary (indicated by crosses in Figures 13 and 14).
See Section S4 of the Supplementary Material for details on this failure at the boundary.
As in the Gaussian process case, calibration increases confidence region area for the
neural likelihood and thus increases empirical coverage close to the intended 95% coverage.
Empirical coverage for neural likelihood is uniform over the evaluation parameter space
Θ in contrast to both variants of pairwise likelihood in which coverage varies. Where
adjusted pairwise and calibrated neural likelihood have comparable coverage, the former
tends to have larger confidence region area. To conclude, neural likelihood provides better
empirical coverage than both unadjusted and adjusted pairwise likelihood for distance cut-
offδ= 2 while keeping confidence region area relatively small and coverage across Θ
relatively uniform.
Evaluation: Timing Study As mentioned earlier in this section, we used the function
nllh from the fitmaxstab function to evaluate the pairwise likelihood surfaces. Since
39Figure 13: Empirical coverage for 95% approximate confidence regions for unadjusted pairwise
likelihood with distance cut-off δ= 2 (far left) and adjusted pairwise likelihood with δ= 2 (center
left) and neural likelihood before calibration (center right) and after calibration (far right).
Figure 14: 95% approximate confidence region area for unadjusted pairwise likelihood with a
distance cut-off of δ= 2 (far left) and adjusted pairwise likelihood with δ= 2 (center left) and
neural likelihood before calibration (center right) and after calibration (far right).
thefitmaxstab function involves finding the parameters that best fit the spatial field via
L-BFGS, we limited the number of optimization iterations to zero to only count the time
necessary to construct the pairwise likelihood surface. We used parallel processing with the
same processor in the Gaussian process case study to evaluate the function nllh at each
parameter of the 40 ×40 grid over the evaluation parameter space Θ. Importantly, our
timing study only involves unadjusted pairwise likelihood surfaces. The time to produce
adjusted pairwise likelihood surfaces is similar if the computation of the linear transforma-
tion for the adjustment is not considered. Additionally, as in the Gaussian process case,
the time to train the neural network is not included in the recorded times to evaluate the
neural likelihood surfaces. The time to train the neural network in the Brown–Resnick case
is similar to the Gaussian process case.
As the distance cut-off δincreases, the time to construct the corresponding pairwise
40Table 3: Time to produce neural and unadjusted pairwise likelihood surfaces on a 40 ×40 grid
over Θ for 50 realizations of a Brown–Resnick process on a 25 ×25 grid on spatial domain
[−10,10]×[−10,10].
Type of surface and method average (sec) standard deviation (sec)
pairwise likelihood ( δ= 1) 5.05 0.34
pairwise likelihood ( δ= 2) 5.38 0.27
pairwise likelihood ( δ= 5) 5.86 0.28
pairwise likelihood ( δ= 10) 7.33 0.16
vectorized neural likelihood 2.24 0.13
unvectorized neural likelihood 14.39 0.03
likelihood surface should increase and indeed does increase as shown in Table 3 because the
number of spatial location pairs for which the bivariate likelihood is computed increases.
Yet, the computation time only increases slightly as δincreases. Depending on δ, the
vectorized neural likelihood method is approximately two to three times faster than pairwise
likelihood. However, the unvectorized method is at least twice as slow as computing the
pairwise likelihood surface. Thus, the efficiency of neural likelihood surfaces is due to the
ability to evaluate the CNN at multiple inputs simultaneously.
Evaluation: Multiple Realizations The parameter estimators for pairwise and neural
likelihood for the five i.i.d. realizations case in Figure 15 are more accurate and have less
variance than the equivalent for the single realization case in Figure 12, as expected. As
in the single realization case, the neural and pairwise estimates are similar in terms of
accuracy and behavior except in certain areas of the evaluation parameter space Θ. For
large smoothness and small range, the pairwise estimates are significantly more accurate
than the neural estimates. Yet, for small smoothness and large range, the neural estimates
are slightly more accurate than the pairwise estimates.
41Figure 15: Parameter estimates for neural likelihood and pairwise likelihood with δ= 2 in the
case of 5 i.i.d. spatial field realizations for a Brown–Resnick process. Each of the 16 plots contains
the true parameter (black star) which generated the realizations and the corresponding parameter
estimates for pairwise likelihood (purple) and neural likelihood (green) with mean squared error
(MSE) in the legend. The true parameter increases in range from bottom to top and in smoothness
from left to right.
Summary of Results In this case study, we demonstrated that in terms of parame-
ter estimation and uncertainty quantification, our method of learning the likelihood has
significant advantages over pairwise likelihood, a well-established approximation for the
intractable exact likelihood. The neural likelihood parameter estimates are generally com-
parable or significantly better than the pairwise likelihood parameter estimates depending
on the distance cut-off δ. Yet, there are some cases for which the pairwise estimates are
more accurate depending on the location in the parameter space Θ. This discrepancy may
42be due to either the CNN not sufficiently learning the likelihood in this particular region or
it could be that pairwise likelihood is truly better than exact likelihood in terms of param-
eter estimation in this region. The approximate confidence regions for neural likelihood
provide better uncertainty quantification than pairwise likelihood whether adjusted or not:
the neural confidence regions achieve empirical coverage comparable to the intended cover-
age with sufficiently small confidence region area. From the evaluation of neural likelihood
surfaces, confidence regions, and empirical coverage before and after calibration, it is clear
that calibration is essential to our method in order to achieve the intended coverage level
of the approximate confidence regions. Finally, the neural likelihood surfaces are signifi-
cantly faster to evaluate than pairwise likelihood surfaces because neural networks are fast
to evaluate and our CNN, once trained, is amortized.
5 Discussion and Conclusions
In this paper, we have proposed a new method to learn the likelihood function of spatial
processes using a specifically designed classification task. This classification task involves
generating simulated data from the spatial process to construct two classes which consist
of pairs of dependent and independent spatial fields yand parameters θ. Due to the
construction of the classes, the resulting classifier is equivalent to the likelihood up to a
multiplicative constant and a known transformation. Once we calibrate the classifier, we
can transform the classifier outputs to produce neural likelihood surfaces, approximate
confidence regions, and parameter estimates.
We demonstrated that the neural likelihood produces comparable results to the exact
likelihood in terms of parameter estimation and uncertainty quantification for a Gaussian
process, a spatial process with a computationally intensive yet tractable likelihood. How-
ever, evaluating the neural likelihood surface is faster than evaluating the exact likelihood
surface by potentially orders of magnitude depending on the number of observed spatial
locations. Additionally, the approach is well-suited to determining parameter estimates us-
ing a grid-based approach which ensures that the resulting estimator is close to the global
maximizer of the neural likelihood. Altogether, the neural likelihood provides comparable
43parameter estimation and uncertainty quantification to the exact likelihood, when one is
available, along with much improved computational efficiency and guarantees for finding
parameter estimators close to the global maximizer.
We have provided compelling evidence that our method produces a neural likelihood
which approximates well the exact likelihood of the Brown–Resnick process, a spatial pro-
cess with an intractable likelihood. We compared neural likelihood to pairwise likelihood,
a common approximation for the exact likelihood of this process. Pairwise likelihood has
a tuning parameter, the distance cut-off δ, which is hard to choose optimally, and an ad-
justment, which can ensure asymptotic guarantees of the maximum pairwise likelihood
estimator yet is difficult to apply in practice. Due to this tuning parameter, there is often a
trade-off in terms of obtaining reasonable surfaces, parameter estimates, and approximate
confidence regions. Neural likelihood, on the other hand, does not have such a tuning
parameter and does not suffer from this trade-off between good parameter estimation and
reasonable approximate confidence regions. Adjusting pairwise likelihood can improve the
quality of the surfaces and increase empirical coverage, yet it is tricky and computationally
intensive to implement especially for different tuning parameters δ. In contrast, neural
likelihood has no such adjustment, unless one considers calibration an adjustment, and
performs better than adjusted pairwise likelihood in terms of surfaces, approximate confi-
dence regions, empirical coverage, and confidence region area across most of the parameter
space, although in certain parts of the parameter space, parameter estimation via pairwise
likelihood was more accurate than neural likelihood. Additionally, neural likelihood sur-
faces are much quicker to evaluate than pairwise likelihood. Thus, we conclude that for
this particular spatial process with an intractable likelihood, neural likelihood generally
performs much better than pairwise likelihood.
We conclude with a discussion of the limitations and potential extensions of our method
of learning the likelihood via classification. Currently, our method learns the likelihood over
a predetermined bounded parameter space which is a common assumption in contemporary
simulation-based inference (e.g., Dalmasso et al., 2023) since simulating training parameters
from an unbounded space is not feasible in practice. An important question for future work
44is to address the misspecification situation where the neural likelihood is evaluated for a
realization yof the spatial process generated by a parameter θoutside the parameter space
used to train the classifier. With active learning (Murphy, 2022, 646–647), we may be able
to adaptively extend the parameter space during training in such a way that even in the
misspecified case we eventually include with high probability the region that contains the
true parameter θ.
Another limitation is the assumption that the spatial observations are on a fully ob-
served regular grid of fixed size. This motivates two extensions that we plan to address
in future work: First, it would be important to extend the method to partially observed
grids which can probably be done by considering the unobserved locations on the grid
as latent variables. Second, there are many significant examples of irregular spatial data
which our method cannot currently handle. One such example is Argo float data (Wong
et al., 2020; Kuusela and Stein, 2018) consisting of irregularly sampled ocean temperature
and salinity profiles throughout the global ocean. In the future, we would like to extend
neural likelihood to such irregular spatial data, which is a nontrivial extension since the
neural network and its training would need to be adapted to handle irregular inputs. One
possible method of extension is graph convolutional neural networks which Sainsbury-Dale
et al. (2023) utilized to extend neural estimation to irregular spatial data.
Another limitation of this work is our focus on low-dimensional parameter spaces.
Likelihood-free inference methods similar to the one considered here have been shown
to work with at least up to ten parameters (Dalmasso, 2021, pp. 43–48). However, as
the parameter space grows in dimension, sampling from this space to generate a sufficient
amount of training data to adequately learn the likelihood becomes exponentially more
computationally demanding. This training issue may potentially be addressed by finding
ways to adaptively sample the parameter space so that the training sample is concentrated
in areas of high likelihood. Additionally, once the neural likelihood is obtained over a
high-dimensional parameter space, the grid-based approach to parameter estimation and
confidence region construction will become computationally expensive. A possible solu-
tion is a gradient-based approach in which the neural likelihood gradient and Hessian are
45computed using automatic differentiation.
Another important topic for future work will be to investigate the robustness of our
method to input data whose distribution deviates slightly from the stochastic model used
to train the network (Drenkow et al., 2022). Real-data applications may require developing
techniques for addressing this distribution shift if it turns out to have a substantial impact
on the performance of the network.
Finally, in this paper, we have only presented our method in terms of learning the
likelihood for spatial processes. Yet, this method of learning the likelihood can extend with
simple modifications beyond spatial processes to other complex statistical models. The
only true requirement for the statistical model of interest is the ability to easily simulate
observations from it. Altogether, this method may eventually enable efficient and accurate
likelihood-based parameter estimation and uncertainty quantification for a broad range
of statistical models where only approximate or computationally inefficient inference has
previously been possible.
Competing interests
No competing interests are declared.
Data availability statement
The code used to produce these results is publicly available at https://github.com/
jmwalchessen/neural_likelihood . The simulation experiments in this paper can be
reproduced with the given code.
Acknowledgments
We are grateful to the members of the CMU Statistical Methods for the Physical Sciences
(STAMPS) Research Group for insightful discussions and feedback throughout this work.
We would also like to thank Raphael Huser and L´ eo Belzile for helpful discussions about
46spatial extremes. We would like to acknowledge Microsoft for providing Azure computing
resources for this work. J.W. and M.K. were supported in part by NSF grants DMS-
2053804 and PHY-2020295, NOAA grant NA21OAR4310258 and a grant from C3.ai Digital
Transformation Institute.
References
Brehmer, J. and K. Cranmer (2022). Simulation-Based Inference Methods for Particle
Physics. In Artificial Intelligence for High Energy Physics , Chapter 16, pp. 579–611.
World Scientific.
Brehmer, J., G. Louppe, J. Pavez, and K. Cranmer (2018). Mining Gold from Implicit
Models to Improve Likelihood-Free Inference. Proceedings of the National Academy of
Sciences 117 , 5242 – 5249.
Carnell, R. (2022). lhs: Latin Hypercube Samples . R package version 1.1.5.
Casella, G. and R. Berger (2001, June). Statistical Inference . Duxbury Resource Center.
Castruccio, S., R. Huser, and M. G. Genton (2016). High-Order Composite Likelihood
Inference for Max-Stable Distributions and Processes. Journal of Computational and
Graphical Statistics 25 (4), 1212–1229.
Chandler, R. E. and S. Bate (2007, 03). Inference for clustered data using the independence
loglikelihood. Biometrika 94 (1), 167–183.
Chollet, F. et al. (2015). Keras. https://keras.io .
Cranmer, K., J. Brehmer, and G. Louppe (2020). The Frontier of Simulation-Based Infer-
ence. Proceedings of the National Academy of Sciences 117 (48), 30055–30062.
Cranmer, K., J. Pavez, and G. Louppe (2016). Approximating likelihood ratios with cali-
brated discriminative classifiers.
47Dalmasso, N. (2021, 8). Uncertainty Quantification in Simulation-based Inference . Ph. D.
thesis, Carnegie Mellon University.
Dalmasso, N., R. Izbicki, and A. Lee (2020, 13–18 Jul). Confidence sets and hypothesis
testing in a likelihood-free inference setting. In H. D. III and A. Singh (Eds.), Proceedings
of the 37th International Conference on Machine Learning , Volume 119 of Proceedings
of Machine Learning Research , pp. 2323–2334. PMLR.
Dalmasso, N., L. Masserano, D. Zhao, R. Izbicki, and A. B. Lee (2023). Likelihood-
free frequentist inference: Bridging classical statistics and machine learning for reliable
simulator-based inference. arXiv:2107.03920.
Davison, A. C. (2003). Likelihood , pp. 94–160. Cambridge Series in Statistical and Proba-
bilistic Mathematics. Cambridge University Press.
Dombry, C., S. Engelke, and M. Oesting (2017). Asymptotic properties of the maximum
likelihood estimator for multivariate extreme value distributions.
Drenkow, N., N. Sani, I. Shpitser, and M. Unberath (2022). A Systematic Review of
Robustness in Deep Learning for Computer Vision: Mind the Gap? arXiv:2112.00639.
Gelfand, A., M. Fuentes, P. Guttorp, and P. Diggle (2010). Handbook of Spatial Statistics .
Chapman & Hall/CRC Handbooks of Modern Statistical Methods. Taylor & Francis.
Gerber, F. and D. Nychka (2021). Fast Covariance Parameter Estimation of Spatial Gaus-
sian Process Models using Neural Networks. Stat 10 (1), e382.
Goodfellow, I. J., Y. Bengio, and A. Courville (2016). Deep Learning . Cambridge, MA,
USA: MIT Press. http://www.deeplearningbook.org .
Guo, C., G. Pleiss, Y. Sun, and K. Q. Weinberger (2017). On Calibration of Modern Neural
Networks. In Proceedings of the 34th International Conference on Machine Learning -
Volume 70 , ICML’17, pp. 1321–1330. JMLR.org.
48Heaton, M. J., A. Datta, A. O. Finley, R. Furrer, J. Guinness, R. Guhaniyogi, F. Gerber,
R. B. Gramacy, D. M. Hammerling, M. Katzfuss, F. Lindgren, D. W. Nychka, F. Sun, and
A. Zammit-Mangion (2017). A Case Study Competition Among Methods for Analyzing
Large Spatial Data. Journal of Agricultural, Biological, and Environmental Statistics 24 ,
398 – 425.
Hermans, J., V. Begy, and G. Louppe (2020, 13–18 Jul). Likelihood-free MCMC with
amortized approximate ratio estimators. In H. D. III and A. Singh (Eds.), Proceedings
of the 37th International Conference on Machine Learning , Volume 119 of Proceedings
of Machine Learning Research , pp. 4239–4248. PMLR.
Huser, R. and A. Davison (2013). Composite Likelihood Estimation for the Brown–Resnick
Process. Biometrika 100 (2), 511–518.
Kabluchko, Z., M. Schlather, and L. de Haan (2009). Stationary Max-Stable Fields Asso-
ciated to Negative Definite Functions. The Annals of Probability 37 (5), 2042 – 2065.
Kaji, T. and V. Roˇ ckov´ a (2023). Metropolis–Hastings via classification. Journal of the
American Statistical Association 118 (544), 2533–2547.
Katzfuss, M. and J. Guinness (2021). A General Framework for Vecchia Approximations
of Gaussian Processes. Statistical Science 36 (1), 124 – 141.
Kingma, D. and J. Ba (2015). Adam: A Method for Stochastic Optimization. Proceedings
of the 3rd International Conference on Learning Representations (ICLR 2015) .
Kuusela, M. and M. L. Stein (2018). Locally Stationary Spatio-Temporal Interpolation of
Argo Profiling Float Data. Proceedings of the Royal Society A 474 (2220), 20180400.
Lecun, Y., L. Bottou, Y. Bengio, and P. Haffner (1998). Gradient-Based Learning Applied
to Document Recognition. Proceedings of the IEEE 86 (11), 2278–2324.
Lenzi, A., J. Bessac, J. Rudi, and M. L. Stein (2023). Neural Networks for Parameter
Estimation in Intractable Models. Computational Statistics &Data Analysis 185 ,
107762.
49Lenzi, A. and H. Rue (2023). Towards black-box parameter estimation.
Lindgren, F., H. Rue, and J. Lindstr¨ om (2011). An Explicit Link Between Gaussian Fields
and Gaussian Markov Random Fields: The Stochastic Partial Differential Equation Ap-
proach. Journal of the Royal Statistical Society: Series B (Statistical Methodology) 73 (4),
423–498.
Mardia, K. V. and R. J. Marshall (1984). Maximum likelihood estimation of models for
residual covariance in spatial regression. Biometrika 71 (1), 135–146.
Murphy, K. P. (2022). Probabilistic Machine Learning: An Introduction . MIT Press.
Padoan, S. A., M. Ribatet, and S. A. Sisson (2010). Likelihood-based Inference for Max-
Stable Processes. Journal of the American Statistical Association 105 (489), 263–277.
Platt, J. (1999). Probabilistic Outputs for Support Vector Machines and Comparisons to
Regularized Likelihood Methods. Advances in Large Margin Classifiers 10 (3), 61–74.
Rasmussen, C. E. and C. K. I. Williams (2006). Gaussian Processes for Machine Learning.
Adaptive computation and machine learning. MIT Press.
Ribatet, M. (2009). A User’s Guide to the SpatialExtremes Package .
Ribatet, M. (2020). SpatialExtremes: Modelling Spatial Extremes . R package version 2.0-9.
Richards, J., M. Sainsbury-Dale, A. Zammit-Mangion, and R. Huser (2023). Likelihood-free
neural bayes estimators for censored inference with peaks-over-threshold models.
Rizvi, S., M. Pettee, and B. Nachman (2024). Learning likelihood ratios with neural network
classifiers. Journal of High Energy Physics 2024 (2), 1–41.
Sainsbury-Dale, M., J. Richards, A. Zammit-Mangion, and R. Huser (2023). Neural bayes
estimators for irregular spatial data using graph neural networks.
Sainsbury-Dale, M., A. Zammit-Mangion, and R. Huser (2023). Neural Point Estimation
for Fast Optimal Likelihood-Free Inference. arXiv:2208.12942.
50Shao, J. (2003). Mathematical Statistics (2nd ed.). Springer.
Sisson, S. A., Y. Fan, and M. A. Beaumont (2018). Handbook of Approximate Bayesian
Computation. Taylor and Francis.
Stoev, S. A. (2008). On the ergodicity and mixing of max-stable processes. Stochastic
Processes and their Applications 118 (9), 1679–1705.
Sun, Y., B. Li, and M. G. Genton (2012). Geostatistics for Large Datasets. In Advances
and Challenges in Space-time Modelling of Natural Events , Chapter 3, pp. 55–77. Berlin,
Heidelberg: Springer.
Vaart, A. W. v. d. (1998). Asymptotic Statistics . Cambridge Series in Statistical and
Probabilistic Mathematics. Cambridge University Press.
Varin, C., N. Reid, and D. Firth (2011). An overview of composite likelihood methods.
Statistica Sinica 21 (1), 5–42.
Wong, A., S. Wijffels, S. Riser, S. Pouliquen, S. Hosoda, D. Roemmich, J. Gilson, G. John-
son, K. Martini, D. Murphy, M. Scanderbeg, T. Udaya Bhaskar, J. Buck, F. Merceur,
T. Carval, G. Maze, C. Cabanes, X. Andre, N. Poffa, and H.-M. Park (2020). Argo Data
1999–2019: Two Million Temperature-Salinity Profiles and Subsurface Velocity Obser-
vations From a Global Array of Profiling Floats. Frontiers in Marine Science 7 (700).
51S1 Supplementary Material
S1 Background Information on Convolutional Neural Networks
The following offers a brief overview of CNNs as used in our methodology. For those inter-
ested in a more detailed description, we refer the reader to Lecun et al. (1998), the original
paper introducing CNNs, and Chapter 9 of Goodfellow et al. (2016). A CNN is a type of
neural network which takes as input a matrix of fixed size and uses convolutions to ex-
tract spatial patterns between matrix entries pertinent to the task in question (Goodfellow
et al., 2016, pp. 329–341). Specifically, filters in each convolutional layer of the CNN ex-
tract different spatial patterns that may be of use in the learning task. Due to their ability
to extract spatial patterns, CNNs are often used to process visual data such as photos in
which pixels can be depicted as constituting a fixed-size matrix. As such, CNNs are well
suited to dealing with the spatial field input which is effectively an image. Outputs of a
CNN can vary from a scalar in the case of binary classification to a multidimensional vector
in the case of multi-class classification or even another matrix in the case of image-on-image
regression (Goodfellow et al., 2016, pp. 352–353).
In a CNN, convolutional layers are interspersed with pooling layers in order to distill
the spatial information contained in an image or other matrix-type object to the most
important spatial patterns. Pooling layers reduce the output size of the previous layer by
combining multiple neurons into a single neuron in the next layer (Goodfellow et al., 2016,
pp. 335–339). The output of the convolutional and pooling layers is a reduced-size matrix
containing spatial patterns of interest to the learning task.
In the case of classification, this reduced matrix can be flattened to a manageable-size
vector which is then processed in the second part of the network—a collection of fully
connected layers with multiple hidden layers ending with an output layer. For binary
classification, the output layer is either a binary classification output or a class probability,
a real value between zero and one, depending on the activation function as well as the loss
function.
1S2 Training Details
S2.1 Architecture
Table S1 shows the specific CNN architecture used in both case studies in Section 4.
Table S1: CNN architecture
layer type output shape filters kernel size activation weights
2D convolution [ -, 23, 23, 128] 128 3 ×3 ReLU 1280
2D convolution [ -, 10, 10, 128] 128 3 ×3 ReLU 147584
2D convolution [ -, 3, 3, 16] 16 3 ×3 ReLU 18448
flatten and concatenate [-, 66] 0
Dense [-,64] ReLU 4288
Dense [-,16] ReLU 1040
Dense [-,8] ReLU 136
Dense [-,2] ReLU 18
S2.2 Gaussian Process
When experimenting with different batch sizes and learning rate schedules, we observed
that as batch size decreases, overfitting tends to increase for a fixed learning rate schedule.
Decreasing initial learning rate reduces but does not eliminate overfitting. We did not
observe a similar tendency for overfitting during training for the Brown–Resnick process
and speculate that the CNN architecture might be too complex for learning the relatively
simple likelihood of a Gaussian process unless care is taken when selecting batch size and
learning rate schedule.
S2.3 Brown–Resnick Process
We had some difficulty training the CNN for various combinations of learning rate sched-
ule and batch size. The training and validation loss would generally plateau after a few
2epochs. Simply training the CNN with the same batch size and learning rate schedule for
various initializations of the weights eventually produced a model in which the training and
validation loss did not plateau early in training. The selected model for which plateauing
did not occur was trained using a batch size of 50 and a learning rate schedule in which
the learning rate started at 0 .002 for the first five epochs and decreased by a multiplicative
factor of e−0.1for each of the fifteen epochs after the first five.
As in the Gaussian process case, the evaluation parameter space is Θ = (0 ,2)×(0,2).
We experimented with a training parameter space ˜Θ = (0 ,2)×(0,2.5) in which the domain
of the smoothness parameter νis not extended because this parameter is bounded from zero
to two as described in Section 4.2.1. Yet, extending ˜Θ beyond the evaluation parameter
space Θ caused problems during training. The validation loss fluctuated while the training
loss smoothly decreased as the number of epochs increased. In contrast, the validation
loss smoothly decreased with the training loss for the CNN trained on data from Θ =
(0,2)×(0,2). Thus, the model we selected was trained on data from the training parameter
space ˜Θ = (0 ,2)×(0,2) = Θ with the batch size and learning rate schedule described earlier.
S3 More Description of Brown–Resnick Process and its Approx-
imate Likelihood
In general, we can construct a max-stable process from other stochastic processes in the
following way. First, consider the points ηiof a positive Poisson point process with intensity
function dΛ( η) =η−2dηand i.i.d. realizations Wi(s) of a non-negative stochastic process
with a mean of one. For independent realizations of the Poisson process and the non-
negative process, a max-stable process has the following form:
Z(s) = max
i>0ηiWi(s) for a location son a domain D ⊂Rd(S1)
with marginal P(Z(s)≤z) = exp( −1
z), the unit Fr´ echet distribution (Kabluchko et al.,
2009).
Depending on what non-negative stochastic process we select for Wi(s), we obtain
different max-stable processes (Kabluchko et al., 2009). We can construct the Brown–
3Resnick process using (S1) with Wi(s) = exp( ϵi(s)−γ(s)), where ϵi(s) are realizations
of an intrinsically stationary Gaussian process such that ϵi(s) = 0 almost surely with
semivariogram γ(h) = (∥h∥
λ)νwhere his the spatial separation between two locations,
λ∈R+is a range parameter, and ν∈(0,2] is a smoothness parameter (Castruccio et al.,
2016). As such, the parameter space Θ for which we are interested in learning the likelihood
is a bounded subset of R+×(0,2].
The density is
f(z1, . . . , z n) = exp 
−V(z1, . . . , z n)X
P∈P zY
S∈P−VS(z1, . . . , z n) (S2)
where Pzis the set of all partitions Pof the values z1, . . . , z nandVSis the partial derivative
of
V(z1, . . . , z n) =E(max{W(s1)
z1, . . . ,W(sn)
zn}) (S3)
with respect to the values z1, . . . , z nindexed by S∈ P(Castruccio et al., 2016; Huser and
Davison, 2013). The number of terms in the summation in (S2) is |Pz|=Bn, the Bell
number for n, which grows more than exponentially (Castruccio et al., 2016).
Since the number of spatial locations is generally large in practice, the computation of
the likelihood for a Brown–Resnick process is intractable in many practical cases. Yet, in
the bivariate case of only two spatial locations, the likelihood has a simple closed form.
From (S2), the bivariate log likelihood is
ℓ(λ, ν) = log
V1(z1, z2)V2(z1, z2)−V12(z1, z2)
−V(z1, z2) (S4)
in which V and its partial derivatives V1, V2andV12are tractable (Huser and Davison,
2013). Using the bivariate case of the full likelihood, we can provide an approximation
for the full likelihood of a Brown–Resnick process called pairwise likelihood, a form of
composite likelihood. (Padoan et al., 2010).
In pairwise log likelihood, the summands are the bivariate likelihoods between two spa-
tial locations sj1andsj2forj1, j2∈[n], and involve only select pairs of spatial locations
(sj1,sj2) because even a summation over alln(n−1)
2pairs of spatial locations is computa-
tionally intensive for a large number of spatial locations. The pairwise log likelihood has
the following form (Huser and Davison, 2013; Castruccio et al., 2016):
4log
Lapprox 
(λ, ν)|z
=X
j2>j1nX
j1=1wj1,j2
log
V1(zj1, zj2)V2(zj1, zj2)
−V12(zj1, zj2)
−V(zj1, zj2) (S5)
where the weights wj1,j2are between zero and one depending on whether the bivariate
likelihood for the pair of spatial locations ( sj1,sj2) is included in the summation. Inclusion
is based on a criterion such as the following:
wj1,j2=

1,if∥sj1−sj2∥ ≤δ,
0,otherwise .(S6)
Since nearby locations contain the most information about a given location, the selection
criterion generally is determined by the distance between spatial locations. In (S6), all
pairs of observations whose locations are within a certain cut-off distance δof each other
are included.
When computing the pairwise likelihood, the cut-off distance δis a tuning parameter
that must be appropriately selected in order to obtain reasonable results. In practice, if the
cut-off distance is too small or too large, the maximum pairwise likelihood estimates can be
highly inaccurate, and the pairwise likelihood surfaces tend to be uninformative as shown
in Section S5. In contrast, neural likelihood does not involve such tuning parameters.
Additionally, the asymptotic behavior of the maximum likelihood estimator and, sub-
sequently, the likelihood ratio statistic is different for pairwise likelihood. Under certain
conditions, the maximum likelihood estimator for pairwise likelihood has the following
asymptotic distribution (Padoan et al., 2010):
ˆθpwl∼ N(θ∗, H(θ∗)−1J(θ∗)H(θ∗)−1), (S7)
where H(θ∗) =−E ∂2ℓpwl(θ)
∂θ∂θ⊺
θ=θ∗is the negative expectation of the Hessian of the log-
pairwise likelihood at θ∗andJ(θ∗) = Cov( ∇ℓpwl(θ∗)) is the covariance matrix of the
gradient. For the full likelihood, the maximum likelihood estimator, on the other hand,
has the following asymptotic distribution (Davison, 2003):
ˆθMLE∼ N(θ∗, I(θ∗)−1), (S8)
5where I(θ∗) =−E
∂2ℓ(θ)
∂θ∂θ⊺
θ=θ∗is the Fisher information for the full likelihood. The
difference between (S7) and (S8) is the covariance of the asymptotic normal distribution
of the estimators. The asymptotic covariance matrix in (S7) does not match the curvature
of the pairwise likelihood surface at the true parameter θ∗which affects the asymptotic
distribution of the likelihood ratio statistic.
To enable asymptotic inference, Chandler and Bate (2007) proposed adjusting the pair-
wise likelihood such that the asymptotic distribution of the adjusted pairwise likelihood
ratio statistic is the same as that of the full likelihood. This method involves adjusting the
parameters that are evaluated by the pairwise likelihood function via an affine transforma-
tion. The resulting adjusted log-pairwise likelihood ℓapwlis given by (Chandler and Bate,
2007)
ℓapwl(θ) =ℓpwl(˜θ), ˜θ=ˆθpwl+C(θ∗)(θ−ˆθpwl), (S9)
where C(θ∗) is a square matrix that has the effect of adjusting the curvature of the pair-
wise likelihood surface at ˆθpwl. Specifically, C(θ∗) is formed from H(θ∗), the unadjusted
curvature, and Hadj(θ∗) =H(θ∗)J(θ∗)−1H(θ∗), the intended curvature, via
C(θ∗) =M(θ∗)−1Madj(θ∗), (S10)
such that M(θ∗) and Madj(θ∗) are matrix square roots of H(θ∗) and Hadj(θ∗), respectively.
By construction of C(θ∗), the adjusted pairwise likelihood has the required curvature at its
maximizer. Consequently, the adjusted pairwise likelihood ratio statistic has an asymptotic
chi-squared distribution (Chandler and Bate, 2007). See Section S4 for further technical
details on adjusting the pairwise likelihood.
S4 Technical Details for Adjusting the Pairwise Likelihood
Section S3 provides a broad overview of how to adjust the pairwise likelihood surface
using the linear transformation C(θ∗) of the parameters at which we evaluate the pairwise
likelihood in (S9). In this section, we focus on the technical details of computing C(θ∗)
in which θ∗is the true parameter that generated the spatial field realizations of interest.
The linear transformation C(θ∗) is formed from the negative expectation of the Hessian
6H(θ∗) and the covariance of the gradient J(θ∗). As such, we first need to compute the
Hessian D(θ∗|y) =∂2ℓpwl(θ|y)
∂θ∂θ⊺
θ=θ∗and gradient ∇ℓpwl(θ∗|y) for multiple realizations
y∼p(y|θ∗). In practice, we use finite differencing to obtain numerical approximations
˜D(θ∗|y) and ˜∇ℓpwl(θ∗|y) ofD(θ∗|y) and∇ℓpwl(θ∗|y), respectively.
Specifically, for a given parameter θ∗on the 9 ×9 grid over the evaluation space Θ =
(0,2)×(0,2), we simulated n= 5000 spatial field realizations yi. Then, we computed
˜D(θ∗|yi) and ˜∇ℓpwl(θ∗|yi) for all i∈[n] using finite differencing. For ˜∇ℓpwl(θ∗|yi), we
used forward differencing with h= 0.05, the fineness of the 40 ×40 grid over Θ on which
we evaluate the neural and pairwise likelihood surfaces. Thus, the estimator of J(θ∗) is
ˆJ(θ∗) =1
nnX
i=1˜∇ℓpwl(θ∗|yi)˜∇ℓpwl(θ∗|yi)⊺. (S11)
Finite differencing for ˜D(θ∗|yi) is more difficult because H(θ∗) must be positive
definite for the entries of the square root matrix to be real. To ensure the estimator is
positive definite, we compute four different finite-differenced approximations of the Hessian
via upper right, lower left, lower right, and upper left finite differencing, in this order, with
h= 0.05. After computing each difference, we check whether the given approximate Hessian
is negative definite. If so, we used the Hessian in our estimation of H(θ∗) and halted
computing the remaining finite differences. Otherwise, we continued computing finite-
differenced Hessians in the given order. If none of the four finite-differenced approximations
were negative definite, no Hessian for the given realization yiwas included in the estimation
ofH(θ∗). Thus, the estimator of H(θ∗) is
ˆH(θ∗) =−1
˜nnX
i=11 ˜D(θ∗|yi)≺0˜D(θ∗|yi) where ˜ n=nX
i=11 ˜D(θ∗|yi)≺0
.(S12)
Since the parameter θ∗is unknown in practice, a practitioner would instead have to use
certain data-driven estimators of ˆJ(θ∗) and ˆH(θ∗) that do not rely on the knowledge of θ∗
(Ribatet, 2009). For simplicity, in this paper, we focus on the oracle estimators ˆJ(θ∗) and
ˆH(θ∗) defined in Eqs. (S11) and (S12). As such, the adjusted pairwise likelihood surfaces,
approximate confidence regions, empirical coverage, and confidence region area presented
in Section 4.2.2 are optimistic versions of what would be available in practice for pairwise
likelihood.
7In certain situations, obtaining a negative definite Hessian ˜D(θ∗|yi) using our par-
ticular method of finite differencing turned out to be impossible for all n= 5000 spatial
field realizations. As a result, ˆH(θ∗) cannot be computed, and the adjustment cannot be
performed. We attempted to adjust the pairwise likelihood for all parameters on the 9 ×9
grid over the evaluation parameter space Θ for δ= 1,2. Only for δ= 2 were we able to
compute ˆH(θ∗) for most parameters on the 9 ×9 grid, except for three. In Figures 13
and 14, the empirical coverage and confidence region area for these three parameters are
masked out. In the case of δ= 1, the adjustment was unsuccessful for large portions of the
parameter space. For this reason, we present adjusted pairwise likelihood results only for
δ= 2 in Section 4.2.2.
With positive definite ˆH(θ∗) and ˆJ(θ∗), the square roots of ˆH(θ∗) and ˆHadj(θ∗) =
ˆH(θ∗)ˆJ(θ∗)−1ˆH(θ∗) can be computed to obtain the transformation C(θ∗). These square
roots are not unique for k >1, where kis the number of parameters. As such, Chandler and
Bate (2007) compute the square roots using either Cholesky factorization or eigendecom-
position (spectral method). We implemented both methods and found that the behavior
of the resulting adjusted pairwise likelihoods is similar in the local area around ˆθ, yet can
substantially differ in the rest of the parameter space Θ. In Figures 11, 13 and 14, we show
the results for Cholesky factorization which, in our experiments, had greater empirical
coverage than eigendecomposition over the entire parameter space and similar confidence
region area.
S5 Additional Results
As mentioned in Section 4.2, depending on the distance cut-off δfor pairwise likelihood,
the resulting likelihood surfaces and parameter estimates can vary dramatically in accuracy
and usefulness. Here, we show additional results to illustrate how δaffects the pairwise
estimates and surfaces.
Figures S1 and S2 show pairwise parameter estimates for δ= 1 and δ= 5 for a single
realization of the Brown–Resnick process. The pairwise estimates are significantly worse in
terms of variance and bias across the parameter space than the estimates for δ= 2 displayed
8in Figure 12. Figures S3 and S4 show the analogous results for 5 i.i.d. realizations of the
Brown–Resnick process. While the variance of the pairwise estimates decreases in the case
of multiple realizations, the estimates retain the same qualitative patterns as in the single
realization case and are overall less accurate than the pairwise estimates for the multiple
realization case when δ= 2 shown in Figure 15.
We compare the unadjusted pairwise likelihood surfaces for δ= 1,2 to the uncalibrated
and calibrated neural likelihood surfaces in Figure S5. For δ= 1, the surface is highly
uninformative because the area of high likelihood is large. As δincreases, the area of
high likelihood decreases and the surface can become more informative, yet more biased,
as shown by the surface for δ= 2. We do not display the unadjusted pairwise likelihood
surface for δ= 5 because the area of high likelihood is so sharply concentrated that it
carries little meaningful information about the uncertainty of the parameter.
9Figure S1: 4 ×4 plot of neural and pairwise parameter estimates for δ= 1. Each of the 16
plots contains the true parameter (black star) which generated the 200 spatial field realizations
and the corresponding parameter estimates for pairwise likelihood (purple) and neural likelihood
(green) with mean squared error (MSE) in the legend. The true parameter increases in range
from bottom to top and in smoothness from left to right.
10Figure S2: 4 ×4 plot of neural and pairwise parameter estimates for δ= 5. Each of the 16
plots contains the true parameter (black star) which generated the 200 spatial field realizations
and the corresponding parameter estimates for pairwise likelihood (purple) and neural likelihood
(green) with mean squared error (MSE) in the legend. The true parameter increases in range
from bottom to top and in smoothness from left to right.
11Figure S3: 4 ×4 plot of neural and pairwise parameter estimates for δ= 1 in the case of 5
i.i.d. spatial field realizations for a Brown–Resnick process. Each of the 16 plots contains the
true parameter (black star) which generated the realizations and the corresponding parameter
estimates for pairwise likelihood (purple) and neural likelihood (green) with mean squared error
(MSE) in the legend. The true parameter increases in range from bottom to top and in smoothness
from left to right.
12Figure S4: 4 ×4 plot of neural and pairwise parameter estimates for δ= 5 in the case of 5
i.i.d. spatial field realizations for a Brown–Resnick process. Each of the 16 plots contains the
true parameter (black star) which generated the realizations and the corresponding parameter
estimates for pairwise likelihood (purple) and neural likelihood (green) with mean squared error
(MSE) in the legend. The true parameter increases in range from bottom to top and in smoothness
from left to right.
13Figure S5: The unadjusted pairwise likelihood surfaces for distance cut-off δ= 1 (far left) and δ=
2 (center left) and neural likelihood surface before calibration (center right) and after calibration
(far right) for a realization of a Brown–Resnick process with parameters ν= 0.8 and λ= 0.8. In
each figure, the color scale ranges from the maximum value of the surface to ten units less than
the maximum value.
14