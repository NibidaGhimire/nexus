Gauge-equivariant pooling layers for preconditioners in lattice QCD
C. Lehner1,and T. Wettig1
1Department of Physics, University of Regensburg, 93040 Regensburg, Germany
(Dated: April 21, 2023)
We demonstrate that gauge-equivariant pooling and unpooling layers can perform as well as tradi-
tional restriction and prolongation layers in multigrid preconditioner models for lattice QCD. These
layers introduce a gauge degree of freedom on the coarse grid, allowing for the use of explicitly
gauge-equivariant layers on the coarse grid. We investigate the construction of coarse-grid gauge
elds and study their eciency in the preconditioner model. We show that a combined multi-
grid neural network using a Galerkin construction for the coarse-grid gauge eld eliminates critical
slowing down.
I. INTRODUCTION
Numerical simulations of quantum eld theories such
as Quantum Chromodynamics (QCD) continue to be our
best systematically improvable method to obtain infor-
mation on the nonperturbative features of the theory.
These simulations are done on a nite space-time lattice
on large supercomputers. In most cases, the run time of
the simulations is dominated by the solution of the Dirac
equation for a xed gauge eld. This is usually done by
iterative algorithms whose iteration count is determined
by the condition number of the matrix representing the
linear system, in our case the Dirac operator. As we ap-
proach the interesting regions of parameter space, i.e.,
physical quark mass and continuum limit, the condition
number of the Dirac matrix becomes very large, leading
to critical slowing down. To deal with this problem, very
sophisticated algorithms have been developed over the
years. In particular, suitably constructed multigrid pre-
conditioners have been shown to reduce or even eliminate
critical slowing down [1{11]. Multigrid algorithms use
restriction and prolongation operators to transfer elds
from a ne to a coarse grid and back. In a recent paper
[12] we discussed the construction of such multigrid pre-
conditioners in the language of gauge-equivariant neural
networks and showed that the multigrid paradigm can be
learned eciently by such networks. However, in Ref. [12]
the restriction and prolongation layers were not learned
but computed by hand. The aim of the present paper is
to demonstrate that these two layers can also be learned
by gauge-equivariant neural networks and perform as well
as the traditional construction. We show that both the
model of Ref. [12] as well as the new models discussed in
the current work eliminate critical slowing down.
The construction of multigrid algorithms for lattice
eld theory has a long history, addressing both the
Markov chain Monte Carlo sampling of elds and the
computation of propagators. In the late 1980s and
early 1990s a number of groups devised several multigrid
schemes aimed at eliminating critical slowing down for
dierent lattice eld theories, gauge groups, and fermion
discretizations [13{39]. There was even an early attempt
to use neural networks in this context [40]. These works
used gauge-equivariant constructions of restriction andprolongation operators to address high-frequency noise
from the gauge degrees of freedom. Note that in these
papers gauge equivariance is referred to as gauge covari-
ance, as is common in quantum eld theory. Another
important guiding principle is the approximate preserva-
tion of the space spanned by the low eigenmodes of the
Dirac operator on the coarse grid. The observation of
\local coherence" of the low modes [2] implies that this
space can be approximated locally by a relatively small
number of suitable vectors. State-of-the-art multigrid al-
gorithms make use of this observation in the construc-
tion of the restriction and prolongation operators. Our
explicit construction of these operators in [12] was also
based on this observation. Here, we replace this construc-
tion by gauge-equivariant pooling and unpooling layers
but are still guided by the same objectives. These layers
are parametrized by gauge-invariant spin matrices which
are learned in the present work. In future work, we will
construct models that, for a given gauge conguration,
provide these matrices as output features.
There is a growing body of related work. Several au-
thors have constructed multigrid algorithms, or elements
thereof, using neural networks [41{46], but gauge equiv-
ariance did not play a role in these papers. Gauge equiv-
ariance of neural networks was rst discussed in [47, 48].
In short, gauge symmetry can be built into the model
by requiring that the map implemented by the neural
network commutes with local gauge transformations. As
a result, the neural network does not need to learn this
symmetry and can achieve the same expressivity with
fewer weights. Gauge-equivariant neural networks were
constructed to generate gauge-eld ensembles in several
lattice eld theories in [49{51]. Neural networks that do
not explicitly preserve gauge equivariance were used as
preconditioners in a two-dimensional U(1) lattice gauge
theory in [52]. Reference [53] demonstrated that gauge-
equivariant neural networks can approximate any gauge-
equivariant function on the lattice. In Ref. [54] the equiv-
ariance of neural networks was extended to global lattice
symmetries, and group-equivariant pooling layers were
discussed.
This paper is structured as follows. In Sec. II we de-
scribe our coarsening approach using gauge-equivariant
pooling and unpooling layers. In Sec. III we provide de-arXiv:2304.10438v1  [hep-lat]  20 Apr 20232
tails of the Wilson-clover Dirac spectrum on a gauge con-
guration with nonzero topological charge. In Sec. IV we
discuss the training strategy for the (un)pooling layers,
and in Sec. V we show that the models resolve critical
slowing down. In Sec. VI we summarize our results and
provide an outlook on our future research program.
II. GAUGE-EQUIVARIANT COARSENING
In the following, we build on notation dened in
Ref. [12] but introduce an explicitly gauge-equivariant
coarsening procedure using gauge-equivariant pooling
and unpooling layers that are combined with subsam-
pling layers.
A. Review of notation and coarse-grid vector space
We consider a d-dimensional space-time lattice, the
ne grid, and denote the set of its sites by S. We de-
ne a eld ':S!VI,x7!'(x) on the ne grid with
internal vector space
VI=VG
VG; (1)
whereVGis a gauge vector space and VGis a non-
gauge vector space, respectively. The set of such elds
is denoted byF'. Under a gauge transformation 
 :
S!End(VG),x7!
(x), the elds transform as
'(x)!
(x)'(x). Furthermore, we consider gauge elds
U:S!End(VG),x7!U(x), where2f1;:::;dg. In
the case of QCD, U(x)2SU(3)End(VG). We will use
Uas a short-hand notation for the tuple ( U1;:::;Ud).
We also consider a d-dimensional coarse grid with set
of sites ~S. We dene elds on the coarse grid ~ ':~S!~VI,
y7!~'(y) with internal vector space ~VI. The set of such
coarse elds is denoted by F~'. In contrast to Ref. [12]
~VI=VG
~VG (2)
i.e., in the current work the local gauge space on the
coarse grid is the same as on the ne grid.
As in Ref. [12], we dene a block map B:~S!P(S),
wherePdenotes the power set. We also dene a map
Br:~S!S; y7!Br(y) (3)
that selects for each site yon the coarse grid a reference
siteBr(y) on the ne grid. In the following, we only
consider maps Brfor whichBr(y)2B(y). The coarse
elds shall transform as
~'(y)!~
(y) ~'(y) (4)
with
~
(y) = 
(Br(y)) (5)
under gauge transformations 
. For a related discussion
of gauge-equivariant blocking schemes, see, e.g., Ref. [25].
RL
PLFIG. 1. Graphical representation of restriction layer (left) and
prolongation layer (right) for a single feature. The input and
output features are represented by the planes, and the layers
are represented by the paths drawn and the arrow mapping
the input to the output feature. The reference site is drawn
in black.
B. Restriction and prolongation layers
The restriction layer (RL) can be written as the com-
position of a pooling layer (Pool) and a subsampling layer
(SubSample),
RL = SubSample Pool: (6)
The pooling layer Pool: F'!F','7!Pool'is given
by
Pool'(x) =X
q2QWq(x)Tq'(x): (7)
In the following we describe the elements of this equa-
tion in detail. The sum is over couples (i.e., two-tuples)
q= (p;U) that consist of a path pand a gauge eld U.
A pathpis dened as a sequence of hops without refer-
ence to a starting or end point. A set of paths Pshall
be called \complete" if it connects every site in B(y) to
Br(y) exactly once. A complete set of paths therefore
always hasjB(y)jelements, wherejXjdenotes the cardi-
nality of a set X. In the current work, we only consider
couples withjQj=njB(y)jandn2N+such thatnpre-
scriptions to construct the gauge eld are combined with
nprescriptions to construct a complete set of paths.
The pooling layer is parametrized by weights Wq(x)2
End(VG). In the context of the current paper the Wq(x)
are spin matrices.
Finally, the operator Tqforq= (p;U) is the parallel-
transport operator Tp:F'!F','7!Tp'dened in
Ref. [12] with gauge elds Ureplaced by U.1The gauge
elds UenteringTpdo not have to be the original ne-
grid gauge links Uas long as they transform in the usual
way, i.e., as
U(x)!
(x)U(x)
y(x+ ^) (8)
under gauge transformations 
. We will make use of this
freedom in this work.
1In Eq. (7), Tq'(x) means that the eld Tq'is evaluated at x.3
FIG. 2. The two-level multigrid model studied in this work. The model is similar to the one studied in Ref. [12], but explicitly
gauge-equivariant pooling and unpooling layers are used in the current work for the restriction and prolongation layers. The
coarse-grid layer is limited by the blue features. This layer and the last four layers are LPTC layers introduced in Ref. [12].
The subsampling layer SubSample: F'!F ~','7!
SubSample'is dened by
SubSample'(y) ='(Br(y)) (9)
for a given choice of reference-point map Brdened in
Eq. (3). This construction therefore satises Eq. (4) with
~'= RL'for a given'2F'. For a discussion of a general
group-equivariant pooling layer, see Ref. [54].
The prolongation layer (PL) is simply dened as
PL = PoolySubSampley; (10)
where the dagger of an operator Ois dened in the usual
way by requiring 'y
1O'2= ('y
2Oy'1)for arbitrary '1
and'2. Note that the couples and weights of a restric-
tion and prolongation layer can in principle be chosen
independently. The models studied in this work, how-
ever, use the same couples and weights for both RL and
PL so that PL = RLy.2
A graphical representation of the restriction and pro-
longation layers is given in Fig. 1. The pooling layer is
a generalization of the local parallel-transport convolu-
tion (LPTC) layer introduced in Ref. [12]. However, one
would typically implement the combined RL directly to
avoid unnecessary computation of feature elements that
will be discarded by the subsequent subsampling layer.
This can be done eciently by precomputing, for each
complete set of paths, a eld S!End(VG) that is used
in combination with a reduction operation within each
block. We provide such implementations of both RL and
PL in the Grid Python Toolkit (GPT) [56].
We note that the construction of similar restriction
and prolongation operations has a long history, see, e.g.,
[20, 25, 35].
2In the context of a multigrid solver, Ref. [55] calls this the vari-
ational choice because it follows from a variational principle.C. Coarsening of the gauge elds
In the current work, we preserve the general model
structure introduced in Ref. [12]. However, we replace
the restriction and prolongation layers with ones based
on gauge-equivariant pooling and unpooling layers, see
Fig. 2. This replacement introduces an explicit gauge
degree of freedom on the coarse grid so that the coarse-
grid layer can be constructed in an explicitly gauge-
equivariant manner. For this layer we need coarse gauge
elds ~U.
The gauge transformation property of coarse elds
given in Eq. (4) is consistent with gauge elds on the
coarse grid that perform a parallel transport between ref-
erence sites Br(y) andBr(y0) on the ne grid, where y
andy0are neighboring sites on the coarse grid. Such
gauge elds must transform as
~U(y)!~
(y)~U(y)~
y(y+ ^) (11)
under gauge transformations. We investigate two choices
for the ~Uin this work.
The rst choice is to connect Br(y) andBr(y0) using
the shortest path on the ne grid connecting both points.
In this work, we use a block map Bsuch thatB(y) is
given by a Cartesian product of neighboring sites in each
dimension, and a xed reference site Brwithin each block
so that the shortest path is unique and aligns with a
coordinate axis. We then always have
Br(y0) Br(y) =b^ (12)
with unit vector ^ in direction andb2N+. The coarse-
grid gauge eld ~U(y) corresponding to this pair of ref-
erence points is then simply
~U(y) =U(Br(y))U(Br(y) + (b 1)^) (13)
with ne-grid gauge links U. We will refer to this choice
as the \plain coarse-link model."4
The second choice is based on the Galerkin coarse-grid
operator
~D= RLDPL (14)
with gauge-equivariant ne-grid operator D. For the pur-
pose of the current paper, Dis the Wilson-clover Dirac
operator (for the precise denition see Ref. [12]). We
then simply dene
~U(y) =~D(y;y+ ^); (15)
which transforms as in Eq. (11) since ~D(y;y0) transforms
to~
(y)~D(y;y0)~
y(y0) under gauge transformations 
.
We refer to this choice as the \Galerkin model." Note
that in the Galerkin model the coarse gauge links will
depend on the weights in the RL and PL. In the Galerkin
model ~U(y)2End( ~VI), while ~U(y)2End(VG) in the
plain coarse-link model. Both are acceptable in the con-
text of the gauge-equivariant coarse-grid LPTC layer in
Fig. 2 as long as Eq. (11) is satised.
We again note that there is a rich history of related
work, see, e.g., Refs. [21, 38, 57{60]. As in these works,
our coarse gauge elds dened by Eq. (15) are, in gen-
eral, no longer elements of the original gauge group.
While this is not a problem of principle, Refs. [21, 27]
found better performance of the multigrid algorithm if
the coarse gauge elds are projected back to the original
gauge group. We plan to investigate this possibility in
future work. We also note that there is an alternative
way to dene the coarse gauge elds using the pooling
and subsampling layers introduced in Sec. II B and ap-
plying them to the gauge links between the blocks, see,
e.g., Ref. [38]. We did not implement this alternative be-
cause it does not increase the expressivity of the model
compared to Eq. (15).
III. DIRAC SPECTRUM AND TOPOLOGY
As in Ref. [12], we have generated quenched Wilson
gauge congurations with 8316 lattice sites for = 6
and attempt to precondition the Dirac equation for the
Wilson-clover Dirac operator with csw= 1. In order to
provide an even more challenging setup for the precondi-
tioner models, we select gauge congurations with topo-
logical charge Q= 1 dened via the ve-loop enhanced
denition of Ref. [61] after cooling the gauge elds by ap-
plying the Wilson ow [62] with ow time t= 10.3The
Dirac operator has an eigenvalue with vanishing imagi-
nary part and real part very close to the lower edge of
the spectrum, see Fig. 3. In this case, we expect critical
slowing down to be clearly visible as the quark mass is
tuned to criticality.
3The measured value for the conguration used in this work is
Q= 0:998.
−0.1 0.0 0.1 0.2 0.3 0.4
Reλ−0.8−0.6−0.4−0.20.00.20.40.60.8Imλ
FIG. 3. Smallest eigenvalues of the Wilson-clover Dirac
operator with mass m= 0:5645 andcsw= 1 on a pure-
Wilson-gauge conguration with topological charge Q= 1,
= 6, and 8316 lattice sites. The mass mis tuned to near
criticality for the experiments in this work.
IV. TRAINING STRATEGY
In the following we describe our training strategy for
the preconditioner model shown in Fig. 2. We perform
the training in two steps.
In the rst step, we train only the restriction and pro-
longation layers. One may naturally consider to train
PLRL as an autoencoder with training vectors sam-
pled from the low-mode space of D. We nd that this
strategy by itself is not sucient to obtain an ecient
model. Instead, we also train PL RL to act as a projec-
tor onto the low-mode space, i.e., it should project high
modes to zero. Furthermore, we found that it is bene-
cial to approximately preserve the property RL PL = 1.
We also found that restricting PL = RLyby using the
same couples q= (p;U) and the same weights Wq(x)
for the restriction and prolongation layers did not reduce
the performance of the model, and therefore we adopt
this choice for simplicity.
We implement this strategy by using the cost function
C=jPLRLv` v`j2+jPLRLvh P`vhj2
+jRLPLvc vcj2(16)
with two ne-grid vectors v`andvhand one coarse-grid
vectorvc. For each training step new random vectors
v`;vh;vcare chosen according to the following proce-
dure. Forv`we select a random element of fu1;:::;usg
of the near-null space vectors uidened in Ref. [12] with
s2N+. Forvhandvcwe take random vectors with el-
ements normally distributed about zero. The low-mode5
projector
P`=WyW (17)
withWdened in Eq. (31) of [12] is using the same set
of near-null vectors fu1;:::;usg. All vectors v`,vh, and
vcare normalized to unit length before being used in
the cost function. Note that P`v`=v`by construction
so that we can also write the cost function in the more
symmetric way
C=jPLRLv` P`v`j2+jPLRLvh P`vhj2
+jRLPLvc vcj2: (18)
This training procedure provides the gauge-invariant
spin matrices Wq(x) for a given gauge conguration.
While the current training strategy does not reduce the
overall cost compared to the multigrid model studied in
Ref. [12], we will study constructing the gauge-invariant
Wq(x) directly from a given gauge eld Uusing gauge-
invariant models [53] in future work. The local features
ofWq(x) may be related to features of the local energy
density, topological charge density, and general Wilson
loops so that no retraining may be needed for a dierent
gauge conguration of the same ensemble.
In the second step, we use the trained RL and PL in
the modelMof Fig. 2 and train the model with frozen
pooling-layer weights using the same cost function as in
Ref. [12],
C=jMbh uhj2+jMb` u`j2(19)
withbh=Dv1,uh=v1,b`=v2, andu`=D 1v2.
Here,v1andv2are random vectors normalized such that
jbhj=jb`j= 1. After this procedure, we can also con-
tinue to train the model without freezing the pooling-
layer weights. However, no benet was observed from
this renement.
V. MODEL DETAILS AND RESULTS
In this section we demonstrate the performance of the
models we studied with focus on removing the critical
slowing down in solving the Dirac equation when the
mass parameter mis tuned towards criticality.
For concreteness, we use a coarse grid of size 234
such thatjB(y)j= 44, ands= 4. Note that in Ref. [12]
we useds= 12. However, for the case at hand s= 4 was
sucient to obtain a well-performing model.
For the pooling layers, we found that using gauge elds
which are smeared dierently depending on the set of
paths works well. Concretely, we use 9 dierent gauge
elds U(i)withi= 1;:::; 9. We construct the U(i)by
applyingi(i 1)=2 steps of= 0:1 stout smearing [63]
to the unsmeared gauge elds U. For xed i, we dene
pathsp(ij)that connect all elements of B(y), enumerated
byj= 1;:::;jB(y)j, to the reference site Br(y). For dif-
ferentiwe use dierent prescriptions for the paths p(ij),
−0.564−0.562−0.560−0.558
m010002000300040005000Iteration Count
Unpreconditioned
Smoother-only model
Gauge-equivariant Galerkin modelFIG. 4. Outer iteration count of unpreconditioned and pre-
conditioned solvers as a function of the quark mass. The
gauge-equivariant Galerkin model completely removes the
critical slowing down as the mass is tuned to criticality.
and then use the couples qij= (p(ij);U(i)) in Eq. (7).
We dene four dierent prescriptions ^ p1;:::; ^p4in the
following and set p(ij)= ^p(j)
imod 4.
For all prescriptions we select the reference site to be
the origin of each block. For the rst block the reference
site corresponds to coordinate (1 ;1;1;1). The starting
site for path pis denoted by ( x1+1;x2+1;x3+1;x4+1).
Then the rst prescription to construct the paths is
to useTp=Hx4
 4Hx3
 3Hx2
 2Hx1
 1with hopping operator
Hdened in Ref. [12]. The second prescription is
Tp=Hx1
 1Hx2
 2Hx3
 3Hx4
 4. The third and fourth prescrip-
tions modify the rst and second prescription, respec-
tively, by permuting the hops in a way that, to the degree
possible, at most one hop in one direction is performed at
a time. A concrete example for x1= 2,x2= 2,x3= 1,
x4= 1 isTp=H 2H 1H 4H 3H 2H 1for the third
prescription and Tp=H 1H 2H 1H 2H 3H 4for the
fourth prescription.
We investigated many additional choices for reference
sites, paths, and gauge elds to be used in the pooling-
layer construction. However, the setup just described
proved to perform well while still being relatively simple.
We then solve the Dirac equation with and without
preconditioning and study the iteration count of the outer
FGMRES [64] solver to 10 8precision as a function of the
quark mass m. In Fig. 4 we compare the outer iteration
count of the unpreconditioned solver with the smoother-
only model of Ref. [12] and the new gauge-equivariant
Galerkin model. We nd that in the smoother-only
model critical slowing down is still visible, while it is com-
pletely absent in the gauge-equivariant Galerkin model.
In Fig. 5 we compare the original multigrid model of6
−0.564−0.562−0.560−0.558
m050100150200250300350400Iteration Count
Original multi-grid model
Gauge-equivariant plain coarse-link model
Gauge-equivariant Galerkin model
FIG. 5. Comparison of multigrid models studied in this work
and the original multigrid model of Ref. [12]. The gauge-
equivariant Galerkin model performs very well even for masses
near criticality. The plain coarse-link model shows a mild
increase in outer iteration count near criticality.
Ref. [12] with the gauge-equivariant Galerkin model and
with the gauge-equivariant plain coarse-link model. We
nd that the original model and the gauge-equivariant
Galerkin model perform best, while the plain coarse-link
model indicates a small remaining signature of critical
slowing down. Note that there is some randomness in the
training procedure that explains the performance uctua-
tions between neighboring mass points for a given model.
VI. SUMMARY AND OUTLOOK
The current work is part of a larger research program
based on gauge-equivariant multigrid neural networks.In our rst paper [12] we demonstrated that a state-of-
the-art multigrid preconditioner can be learned eciently
by gauge-equivariant neural networks. The restriction
and prolongation layers of Ref. [12] were, however, man-
ually constructed by traditional methods to nd near-
null-space vectors.
In the current work, we replaced this construction by
gauge-equivariant pooling and unpooling layers that are
learned for a given gauge conguration. We demon-
strated that such models can eliminate critical slowing
down and perform as well as traditional multigrid mod-
els. The pooling and unpooling layers are parametrized
by gauge-invariant spin matrices, which in turn can be
learned by models such as discussed in Ref. [53]. The
construction of such models, including a detailed study
of transfer learning, is left for future work. If successful,
such models promise to drastically reduce the setup cost
in multigrid preconditioners and may therefore play an
important role in improving the performance of gauge-
generation algorithms such as HMC [65] or ow-based
models [49{51, 66, 67].
Another important topic left for future studies is
the construction of multigrid models for operators with
a more challenging spectrum, such as domain-wall
fermions.
Finally, gauge-equivariant multigrid models should
also be able to learn to directly approximate complex
hadronic correlation functions without constructing them
from intermediate approximations of propagators. Such
direct approximations can then be used to reduce statis-
tical noise without introducing bias [68]. We will explore
this application of gauge-equivariant multigrid models in
future work as well.
Acknowledgments. This work was funded in part
by the Deutsche Forschungsgemeinschaft (DFG, German
Research Foundation), project number 460248186.
Corresponding author; christoph.lehner@ur.de
[1] M. L uscher, Solution of the Dirac equation in lattice
QCD using a domain decomposition method, Comput.
Phys. Commun. 156, 209 (2004), arXiv:hep-lat/0310048.
[2] M. L uscher, Local coherence and deation of the
low quark modes in lattice QCD, JHEP 07, 081,
arXiv:0706.2298 [hep-lat].
[3] J. Brannick, R. C. Brower, M. A. Clark, J. C. Os-
born, and C. Rebbi, Adaptive Multigrid Algorithm for
Lattice QCD, Phys. Rev. Lett. 100, 041601 (2008),
arXiv:0707.4018 [hep-lat].
[4] R. Babich, J. Brannick, R. C. Brower, M. A. Clark,
T. A. Manteuel, S. F. McCormick, J. C. Osborn, and
C. Rebbi, Adaptive multigrid algorithm for the lattice
Wilson-Dirac operator, Phys. Rev. Lett. 105, 201602(2010), arXiv:1005.3043 [hep-lat].
[5] A. Frommer, K. Kahl, S. Krieg, B. Leder, and
M. Rottmann, Adaptive Aggregation Based Domain
Decomposition Multigrid for the Lattice Wilson Dirac
Operator, SIAM J. Sci. Comput. 36, A1581 (2014),
arXiv:1303.1377 [hep-lat].
[6] P. A. Boyle, Hierarchically deated conjugate gradient,
(2014), arXiv:1402.2585 [hep-lat].
[7] J. Brannick, A. Frommer, K. Kahl, B. Leder,
M. Rottmann, and A. Strebel, Multigrid Precondition-
ing for the Overlap Operator in Lattice QCD, Numer.
Math. 132, 463 (2016), arXiv:1410.7170 [hep-lat].
[8] A. Yamaguchi and P. Boyle, Hierarchically deated
conjugate residual, PoS LATTICE2016 , 374 (2016),
arXiv:1611.06944 [hep-lat].7
[9] R. C. Brower, M. A. Clark, A. Strelchenko, and E. Wein-
berg, Multigrid algorithm for staggered lattice fermions,
Phys. Rev. D 97, 114513 (2018), arXiv:1801.07823 [hep-
lat].
[10] R. C. Brower, M. A. Clark, D. Howarth, E. S. Weinberg,
Multigrid for chiral lattice fermions: Domain wall, Phys.
Rev. D 102, 094517 (2020), arXiv:2004.07732 [hep-lat].
[11] P. Boyle and A. Yamaguchi, Comparison of Domain Wall
Fermion Multigrid Methods (2021), arXiv:2103.05034
[hep-lat].
[12] C. Lehner and T. Wettig, Gauge-equivariant neural
networks as preconditioners in lattice QCD (2023),
arXiv:2302.05419 [hep-lat].
[13] J. Goodman and A. D. Sokal, Multigrid Monte Carlo
Method for Lattice Field Theories, Phys. Rev. Lett. 56,
1015 (1986).
[14] A. Hulsebos, J. Smit, and J. C. Vink, Multigrid Monte
Carlo in an external U(1) gauge eld, Nucl. Phys. B Proc.
Suppl. 9, 512 (1989).
[15] A. Hulsebos, J. Smit, and J. C. Vink, Multigrid Monte
Carlo for a Bose Field in an External Gauge Field, Nucl.
Phys. B 331, 531 (1990).
[16] A. Hulsebos, J. Smit, and J. C. Vink, Multigrid inver-
sion of the staggered fermion matrix, Nucl. Phys. B Proc.
Suppl. 20, 94 (1991).
[17] A. Hulsebos, J. Smit, and J. C. Vink, Multigrid inversion
of the staggered fermion matrix with U(1) and SU(2)
gauge elds, Int. J. Mod. Phys. C 3, 161 (1991).
[18] J. C. Vink, Multigrid inversion of staggered and Wilson
fermion operators with SU(2) gauge elds in two dimen-
sions, Phys. Lett. B 272, 81 (1991).
[19] A. Hulsebos, J. Smit, and J. C. Vink, Multigrid inver-
sion of lattice fermion operators, Nucl. Phys. B 368, 379
(1992).
[20] R. Ben-Av, A. Brandt, and S. Solomon, The Fermionic
Matrix, Instantons, Zero Modes and Multigrid, Nucl.
Phys. B 329, 193 (1990).
[21] R. Ben-Av, A. Brandt, M. Harmatz, E. Katznelson, P. G.
Lauwers, S. Solomon, and K. Wolowesky, Fermion simu-
lations using parallel transported multigrid, Phys. Lett.
B253, 185 (1991).
[22] M. Harmatz, P. G. Lauwers, R. Ben-Av, A. Brandt,
E. Katznelson, S. Solomon, and K. Wolowesky, Par-
allel transported multigrid and its application to the
Schwinger model, Nucl. Phys. B Proc. Suppl. 20, 102
(1991).
[23] P. G. Lauwers, R. Ben-Av, and S. Solomon, Inverting the
Dirac matrix for SU(2) lattice gauge theory by means of
parallel transported multigrid, Nucl. Phys. B 374, 249
(1992).
[24] S. Solomon and P. G. Lauwers, Parallel transported
multigrid beats conjugate gradient, Int. J. Mod. Phys.
C3, 149 (1992).
[25] R. Ben-Av, M. Harmatz, P. G. Lauwers, and S. Solomon,
Parallel transported multigrid for inverting the Dirac op-
erator: Variants of the method and their eciency, Nucl.
Phys. B 405, 623 (1993).
[26] M. Harmatz, S. Solomon, P. Lauwers, and T. Wittlich,
Visual study of zero modes role in PTMG convergence,
Nucl. Phys. B Proc. Suppl. 30, 192 (1993).
[27] P. G. Lauwers and T. Wittlich, Inversion of the fermion
matrix in lattice QCD by means of parallel transported
multigrid (PTMG), Int. J. Mod. Phys. C 4, 609 (1993).
[28] P. G. Lauwers and T. Wittlich, Parallel transportedmultigrid (PTMG) for inverting the Dirac operator in
SU(3) lattice gauge theory, Nucl. Phys. B Proc. Suppl.
30, 261 (1993).
[29] R. C. Brower, K. J. M. Moriarty, C. Rebbi, and E. Vicari,
Multigrid propagators in the presence of disordered U(1)
gauge elds, Phys. Rev. D 43, 1974 (1991).
[30] R. C. Brower, C. Rebbi, and E. Vicari, Projective multi-
grid method for propagators in lattice gauge theory,
Phys. Rev. D 43, 1965 (1991).
[31] R. C. Brower, C. Rebbi, and E. Vicari, Non-Abelian Pro-
jective Multigrid for Lattice Gauge Theory, Phys. Rev.
Lett. 66, 1263 (1991).
[32] R. C. Brower, R. G. Edwards, C. Rebbi, and E. Vicari,
Projective multigrid for Wilson fermions, Nucl. Phys. B
366, 689 (1991).
[33] T. Kalkreuter, Projective block spin transformations in
lattice gauge theories, Nucl. Phys. B 376, 637 (1992).
[34] T. Kalkreuter, Ground state projection multigrid for
propagators in four-dimensional SU(2) gauge elds, Phys.
Lett. B 276, 485 (1992).
[35] T. Kalkreuter, G. Mack, and M. Speh, Blockspin and
multigrid for staggered fermions in nonAbelian gauge
elds, Int. J. Mod. Phys. C 3, 121 (1992).
[36] T. Kalkreuter, Improving multigrid and conventional re-
laxation algorithms for propagators, Int. J. Mod. Phys.
C3, 1323 (1992), arXiv:hep-lat/9207016.
[37] T. Kalkreuter, Multigrid for propagators of staggered
fermions in four-dimensional SU(2) gauge elds, Nucl.
Phys. B Proc. Suppl. 30, 257 (1993), arXiv:hep-
lat/9211008.
[38] T. Kalkreuter, Multigrid methods for the computation of
propagators in gauge elds, Int. J. Mod. Phys. C 5, 629
(1994), arXiv:hep-lat/9212021.
[39] T. Kalkreuter, Idealized multigrid algorithm for stag-
gered fermions, Phys. Rev. D 48, R1926 (1993),
arXiv:hep-lat/9304004.
[40] M. B aker, T. Kalkreuter, G. Mack, and M. Speh, Neural
multigrid for gauge theories and other disordered sys-
tems, Int. J. Mod. Phys. C 4, 239 (1993), arXiv:hep-
lat/9209021.
[41] A. Katrutsa, T. Daulbaev, and I. Oseledets, Deep Multi-
grid: learning prolongation and restriction matrices
(2017), arXiv:1711.03825 [math.NA].
[42] D. Greenfeld, M. Galun, R. Basri, I. Yavneh, and R. Kim-
mel, Learning to Optimize Multigrid PDE Solvers, in
Proceedings of the 36th International Conference on Ma-
chine Learning (2019) pp. 2415{2423, arXiv:1902.10248
[math.NA].
[43] I. Luz, M. Galun, H. Maron, R. Basri, and I. Yavneh,
Learning algebraic multigrid using graph neural net-
works, in Proceedings of the 37th International Con-
ference on Machine Learning (2020) pp. 6489{6499,
arXiv:2003.05744 [cs.LG].
[44] M. Eliasof, J. Ephrath, L. Ruthotto, and E. Treister,
MGIC: Multigrid-in-Channels Neural Network Architec-
tures (2020), arXiv:2011.09128 [cs.CV].
[45] R. Huang, R. Li, and Y. Xi, Learning optimal multigrid
smoothers via neural networks, SIAM J. Sci. Comput. ,
S199 (2021), arXiv:2102.12071 [math.NA].
[46] A. van Betteray, M. Rottmann, and K. Kahl, MGiaD:
Multigrid in all dimensions. Eciency and robustness by
coarsening in resolution and channel dimensions (2022),
arXiv:2211.05525 [cs.CV].
[47] T. Cohen and M. Welling, Group Equivariant Convolu-8
tional Networks, in Proceedings of The 33rd International
Conference on Machine Learning (2016) pp. 2990{2999,
arXiv:1602.07576 [cs.LG].
[48] T. S. Cohen, M. Weiler, B. Kicanaoglu, and M. Welling,
Gauge Equivariant Convolutional Networks and the
Icosahedral CNN, in Proceedings of the 36th Interna-
tional Conference on Machine Learning (2019) pp. 1321{
1330, arXiv:1902.04615 [cs.LG].
[49] G. Kanwar, M. S. Albergo, D. Boyda, K. Cranmer,
D. C. Hackett, S. Racani ere, D. J. Rezende, and P. E.
Shanahan, Equivariant ow-based sampling for lattice
gauge theory, Phys. Rev. Lett. 125, 121601 (2020),
arXiv:2003.06413 [hep-lat].
[50] D. Boyda, G. Kanwar, S. Racani ere, D. J. Rezende, M. S.
Albergo, K. Cranmer, D. C. Hackett, and P. E. Shana-
han, Sampling using SU(N) gauge equivariant ows,
Phys. Rev. D 103, 074504 (2021), arXiv:2008.05456 [hep-
lat].
[51] R. Abbott et al. , Gauge-equivariant ow models for sam-
pling in lattice eld theories with pseudofermions, Phys.
Rev. D 106, 074506 (2022), arXiv:2207.08945 [hep-lat].
[52] S. Cal , D. C. Hackett, Y. Lin, P. E. Shanahan,
and B. Xiao, Neural-network preconditioners for solv-
ing the Dirac equation in lattice gauge theory, (2022),
arXiv:2208.02728 [hep-lat].
[53] M. Favoni, A. Ipp, D. I. M uller, and D. Schuh, Lat-
tice Gauge Equivariant Convolutional Neural Networks,
Phys. Rev. Lett. 128, 032003 (2022), arXiv:2012.12901
[hep-lat].
[54] J. Aronsson, D. I. M uller, and D. Schuh, Geometrical
aspects of lattice gauge equivariant convolutional neural
networks (2023), arXiv:2303.11448 [hep-lat].
[55] J. Goodman and A. D. Sokal, Multigrid Monte Carlo
method: Conceptual foundations, Phys. Rev. D 40, 2035
(1989).
[56] C. Lehner et al., Grid Python Toolkit (GPT).[57] H. B. Nielsen and A. Patkos, Eective Dielectric Theory
from QCD, Nucl. Phys. B 195, 137 (1982).
[58] T. Balaban, Regularity and Decay of Lattice Green's
Functions, Commun. Math. Phys. 89, 571 (1983).
[59] G. Mack, Dielectric lattice gauge theory, Nucl. Phys. B
235, 197 (1984).
[60] A. D. Sokal, Some comments on multigrid methods for
computing propagators, Phys. Lett. B 317, 399 (1993),
arXiv:hep-lat/9307020.
[61] P. de Forcrand, M. Garcia Perez, and I.-O. Stamatescu,
Topology of the SU(2) vacuum: A Lattice study us-
ing improved cooling, Nucl. Phys. B 499, 409 (1997),
arXiv:hep-lat/9701012.
[62] M. L uscher, Properties and uses of the Wilson ow in
lattice QCD, JHEP 08, 071, [Erratum: JHEP 03, 092
(2014)], arXiv:1006.4518 [hep-lat].
[63] C. Morningstar and M. J. Peardon, Analytic smearing of
SU(3) link variables in lattice QCD, Phys. Rev. D 69,
054501 (2004), arXiv:hep-lat/0311018.
[64] Y. Saad, A Flexible Inner-Outer Preconditioned GMRES
Algorithm, SIAM J. Sci. Comput. 14, 461 (1993).
[65] S. Duane, A. D. Kennedy, B. J. Pendleton, and
D. Roweth, Hybrid Monte Carlo, Phys. Lett. B 195, 216
(1987).
[66] R. Abbott et al. , Aspects of scaling and scalabil-
ity for ow-based sampling of lattice QCD, (2022),
arXiv:2211.07541 [hep-lat].
[67] R. Abbott et al. , Sampling QCD eld congurations with
gauge-equivariant ow models, PoS LATTICE2022 ,
036 (2023), arXiv:2208.03832 [hep-lat].
[68] E. Shintani, R. Arthur, T. Blum, T. Izubuchi, C. Jung,
and C. Lehner, Covariant approximation averaging,
Phys. Rev. D 91, 114511 (2015), arXiv:1402.0244 [hep-
lat].