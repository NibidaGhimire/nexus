1
Efficient Evaluation Methods for Neural
Architecture Search: A Survey
Xiaotian Song, Xiangning Xie, Zeqiong Lv, Gary G. Yen, Fellow, IEEE ,
Weiping Ding, Senior Member, IEEE , Jiancheng Lv, Senior Member, IEEE , and Yanan Sun, Senior Member, IEEE
Abstract —Neural Architecture Search (NAS) has received
increasing attention because of its exceptional merits in automat-
ing the design of Deep Neural Network (DNN) architectures.
However, the performance evaluation process, as a key part of
NAS, often requires training a large number of DNNs. This
inevitably makes NAS computationally expensive. In past years,
many Efficient Evaluation Methods (EEMs) have been proposed
to address this critical issue. In this paper, we comprehensively
survey these EEMs published up to date, and provide a detailed
analysis to motivate the further development of this research
direction. Specifically, we divide the existing EEMs into four
categories based on the number of DNNs trained for constructing
these EEMs. The categorization can reflect the degree of efficiency
in principle, which can in turn help quickly grasp the method-
ological features. In surveying each category, we further discuss
the design principles and analyze the strengths and weaknesses
to clarify the landscape of existing EEMs, thus making easily
understanding the research trends of EEMs. Furthermore, we
also discuss the current challenges and issues to identify future
research directions in this emerging topic. In summary, this
survey provides a convenient overview of EEM for interested
users, and they can easily select the proper EEM method for the
tasks at hand. In addition, the researchers in the NAS field could
continue exploring the future directions suggested in the paper.
Impact Statement —NAS, a key subfield of AutoML, helps
users (e.g., stock analysts and clinicians) easily obtain promising
DNN architectures in practice without requiring extensive DNN
domain knowledge. However, the performance evaluation process
for NAS often proves time-consuming, hindering its widespread
application. To address this, many EEMs have been developed
to accelerate the performance evaluation process. This paper
provides a systematic review of state-of-the-art EEMs, including
a novelty categorization method for EEMs, a comprehensive
comparative analysis of EEMs, and a summary of the issues
in existing EEMs. Moreover, the future research direction and
challenges are discussed in detail. By offering a bird’s-eye survey
of EMMs, this paper facilitates the development of efficient NAS
and their broader real-world applications.
Index Terms —Deep neural network (DNN), neural architecture
search (NAS), performance predictors, weight sharing, efficient
evaluation
Xiaotian Song, Xiangning Xie, Zeqiong Lv, Jiancheng Lv, and
Yanan Sun are with the College of Computer Science, Sichuan
University, Chengdu 610065, China (e-mails: songxt@stu.scu.edu.cn;
xnxie@stu.scu.edu.cn; zq lv@stu.scu.edu.cn; lvjiancheng@scu.edu.cn;
ysun@scu.edu.cn). (corresponding author: Yanan Sun.)
Gary G. Yen is with the School of Electrical and Computer Engi-
neering, Oklahoma State University, Stillwater, OK 74078 USA (e-mail:
gyen@okstate.edu).
Weiping Ding is the School of Artificial Intelligence and Computer Sci-
ence, Nantong University, Nantong, 226019, China, and also the Faculty
of Data Science, City University of Macau, Macau 999078, China (e-mail:
dwp9988@163.com).I. I NTRODUCTION
NEURAL Architecture Search (NAS) aims to automati-
cally discover high-performance Deep Neural Network
(DNN) architectures, thus allowing researchers without or with
rare expertise to conveniently benefit from the success of
DNNs. The architectures designed by NAS have shown to
even outperform those manually designed in some tasks [1],
[2], and have become increasingly popular in the field of
deep learning [3]. Mathematically, NAS formulates the design
process of DNN architectures as an optimization problem [4].
In particular, NAS first defines the search space containing all
candidates. Then, it adopts a well-designed search strategy
to search for the optimal architecture. During the search
process, NAS must evaluate the performance of every searched
architecture to effectively guide the running of the search
strategy. Generally, the NAS problem is difficult to be solved
because of facing multiple optimization challenges, such as
the prohibitive computational cost, and with multi-conflicting
objectives [5].
The Evolutionary Computation (EC) [6], [7], the Reinforce-
ment Learning (RL) [2], [8], and the gradient algorithm [9]
are currently the three mainstream optimization techniques
for addressing NAS. The EC-based NAS regards the DNN
architectures as the individuals, and iteratively generates the
population by applying genetic operators and eliminates poorly
performed individuals by selection operators. When the stop-
ping condition is satisfied, the best architecture from the
surviving population is picked up for use. In the RL-based
NAS, a controller is trained to guide the search process. It uses
the performance of the architecture as the reward to update the
itself to search for a better architecture in the next iteration.
The gradient-based NAS generally relaxes the discrete search
space to be continuous and uses the gradient descent algorithm
to search for a promising architecture. Please note that the
gradient-based NAS has been inappropriately claimed to be
more efficient than others. This misunderstanding is mainly
caused by the representative in this category, i.e., DARTS [10],
which was collectively designed with an Efficient Evaluation
Method (EEM) named weight-sharing (will be discussed in
Section III-C). In principle, all NAS algorithms have similar
computation complexity if they maintain the same perfor-
mance evaluation techniques.
Generally, whatever optimization algorithm is used, many
DNN architectures need to be evaluated during the search
process. This is because these optimization algorithms are iter-arXiv:2301.05919v2  [cs.NE]  9 Oct 20242
2017 2018 2019 2020 2021 2022 2023
Year020406080100120140The number of submissions
Fig. 1. The number of “submissions” refers to EEMs. The statistical results
are searched on Google Scholar with the following steps: (1) Select a specific
year; (2) Search with the keywords “early stopping” OR “learning curve” OR
“network morphism” OR “weight sharing” OR “one-shot” OR “DARTS” OR
“differentiable” OR “predictor” OR “population memory” OR “zero-cost” OR
“zero-shot” OR “training-free” AND “architecture search” OR “architecture
design” OR “CNN” OR “deep learning” OR “deep neural network”; (3) Check
the selected papers in detail to verify if they belong to EEMs.
ative, and the current performance must be known in advance,
thus effectively guiding the next-step iteration search [4]. For
the Traditional Evaluation Method (TEM), the performance
in NAS is evaluated by fully training the corresponding
architecture(s) searched in each iteration. Commonly, training
a DNN from scratch until converging on a small-scale dataset
such as CIFAR-10 [11] may take hours or even days depending
on the scale of the DNN. Consequently, since there are often
thousands of DNNs to be trained in NAS, the whole NAS
algorithm becomes prohibitively computation-intensive and
time-consuming. For example, on CIFAR-10, the LargeEvo
algorithm [6] consumed 250 Graphic Processing Units (GPUs)
for 11 days. The NAS-RL algorithm [8] took 800 GPUs for
28 days. Even more, the RegularizedEvo algorithm [1] ran
on 450 GPUs for 7 days. In practice, buying or renting such
scales of GPU resources is commonly unaffordable for most
researchers [12]. As a result, how to accelerate the TEM to
reduce the prohibitive computational overhead is essential,
which results in the research topic of EEMs in the NAS
community [3]. Specifically, EEM refers to the performance
evaluation method that consumes less time than TEM in the
entire performance evaluation process of NAS.
To the best of our knowledge, the first work of EEMs is
designed in the LargeEvo algorithm [6] preprintly available in
Arxiv in 2016, although there are also some earlier works
that potentially can achieve the same goal [13]. With the
development of NAS, the research of EEMs has received
much attention and is becoming one of the hottest topics in
the current artificial intelligence community. As evidenced by
Fig. 1 which reports the submissions of EEMs from 2017
to 2023, the number of submissions increases year by year.
Despite the popularity and criticality of the EEMs, there is a
lack of a survey to systematically review these works. This will
make it difficult for researchers with rarely relative knowledge
to quickly grasp the current situation and landmark works in
the research topic.
Related surveys [3]–[5], [14] primarily focus on the devel-
opment of NAS, with little discussion on EEMs. However, the
efficient evaluation is a crucial component of NAS methods,
and a rough number of 300 publications, as we have reportedabove, has been devoted to EEMs for efficient NAS during
past years. To the best of our knowledge, only Liu et al. [15]
provided an overview of efficient NAS. However, their survey
lacks systematic research on EEMs. First, they miss some
important EEMs such as the zero-shot methods discussed in
Subsection III-D, and do not discuss the evaluation criteria
of EEMs. Second, their categorization method only considers
the consumed time of EEMs during the running of NAS, but
ignores the pre-computation time which is also crucial for con-
structing the EEMs. In this survey, we aim to systematically
encompass all existing EEMs and subsequently classify them
into four distinct categories, employing a novel categorization
method to provide readers with a clearer and more effective
way of comprehending the variety of available EEMs. In
summary, the contributions of our paper are as follows:
•This survey provides a comprehensive and systematic
discussion of EEMs. With this survey, the interested re-
searchers could be easily informed about the development
and the taxonomy of EMMs which is one of the hottest
research topics in the community of artificial intelligence.
•We have conducted a thorough analysis to summarize the
advantages and disadvantages of various EEMs through
an exhaustive investigation of close to 200 references in
recent years. This work can provide valuable insights
to NAS researchers, facilitating the swift selection of
appropriate EEMs and the development of efficient NAS.
•We analyze the current status and future development di-
rections of each specific EEM, pointing out the challenges
that need to be addressed in the future of the EEMs
field. This will serve as a guideline for researchers to
investigate impactful EEMs and promote the development
of automated machine learning.
In the following, we show the detail of the categorization
and organization in Section II. After that, the details of these
EEMs are surveyed in Sections III to V, including the method
designs, evaluation metrics, and future directions. Finally, we
make a conclusion in Section VI.
II. C ATEGORIZATION AND ORGANIZATION
In this section, we first give the formulation of NAS in Sub-
section II-A. Then, we describe the notation and terminology
used in this paper in Subsection II-B. After that, the catego-
rization for the EEMs is detailed in Subsection II-C. Finally,
the organization of this survey is discussed in Section II-D.
A. Formulation of NAS
As discussed in Section I, the optimization problem of NAS
can be formulated as Equation (1):
arg max
A∈AL(A,Dtrain,Dvalid) (1)
where Arepresents the search space that filled with a set of
neural architectures, L(·)denotes the performance of neural
architecture Aon the validation dataset Dvalid after being
trained on the training dataset Dtrain. As for the DNNs
on image classification tasks, for example, L(·)denotes the
classification accuracy.3
Search Space 𝒜 Search StrategyPerformance
EvaluationArchitecture
{𝐴1,…,𝐴𝑛}∈𝒜
Performance
of {𝐴1,…,𝐴𝑛}...
 ... ...
Fig. 2. The workflow of the NAS algorithm.
The optimization problem in Equation (1) can be solved
with three stages, as shown in Fig. 2: search space, search
strategy, and performance evaluation. Specifically, the search
process occurs in the predefined search space, and the search
strategy is used to select the promising architecture in the
search space. However, the architectures selected by the search
strategy are typically unlabeled, i.e., the performance of the
architectures is unknown. Thus, we cannot verify whether the
architecture is good or not. Performance evaluation aims to
address this, which can get the L(·)for each architecture
to guide the search process. Typically, the search-evaluation
process iterates in a loop and needs to evaluate numerous
architectures (more than hundreds). This process for the TEMs
requires evaluating each architecture on the GPUs. It is com-
putationally extensive and time-consuming. EEMs, therefore,
try to reduce and even eliminate the requirements of the GPUs,
which can significantly accelerate the NAS algorithm.
B. Notation and Terminology
As introduced in Subsection II-A, NAS aims to find the
architecture Awith promising performance. Thus, the evalu-
ation process of NAS needs to get the performance of every
searched architecture, as indicated in Equation (1). Note that
we use Asto denote the architectures searched in all iterations
of the NAS algorithm.
Based on the time consumption of the evaluation process,
existing evaluation methods can be divided into TEM and
EEM. To better describe them, we first define the initialization
time,evaluation time , and runtime . Specifically, the initial-
ization time is the time used to evaluate the architecture on
GPUs to pre-build the evaluation method, while the evaluation
time refers to the overall time for directly evaluating the
performance of the architecture on GPUs. The runtime is the
sum of the initialization time and the evaluation time, which
can be represented as Equation (2):
T=Ttrain(Ain) +T∗
train(Aev) (2)
where Ttrain(·)represents the time for training the architec-
tures and building the evaluation method, and T∗
train(·)denotes
the time to only evaluate architectures on GPUs. In addition,
Ainrepresents the architectures used to build the evaluation
method, and Aevdenotes architectures that need to directly
obtain their performance on GPUs.
TEM needs to fully train each searched architecture on
GPUs, and its initialization time is zero since there is no pre-
computation. Specifically, ATEMrepresents the architectures
that require evaluation of TEM, which is equal to As. Thus,
the runtime of TEM is TTEM =T∗
train(Aev), subject toATEM=Aev. This indicates that each searched architecture
requires training, resulting in TEM being time-consuming.
EEM consumes less runtime than TEM for the same NAS
algorithm. It is achieved through different working principles.
One is directly reducing the evaluation time with training
acceleration methods. The other part involves evaluating the
performance of few searched architectures and building evalu-
ation methods based on them, which takes initialization time.
In particular, AEEMrepresents the architectures utilized in
EEM. Therefore, the runtime of EEM can be represented as
TEEM =Ttrain(Ain) +T∗
train(Aev), subject to AEEM=
Ain+Aev. Note that AEEMdiffers from Asand different
EEMs can be categorized based on the different AEEM.
C. Categorization
Existing categorization methods have some limitations.
Specifically, White et al. [16] divided existing EEMs into
four categories: model-based, learning curve-based, zero-cost
proxies, and weight sharing. However, they categorize the
existing EEMs based on their working principles. This means
once an EEM with a new working principle appears, this
categorization method may no longer apply. In fact, the
downscaled datasets methods and the downscaled model meth-
ods mentioned in Section III-A do not belong to the four
categories. Liu et al. [15] proposed to categorize existing
EEMs into proxy-based and surrogate-assisted methods. The
proxy-based methods need the weights of the neural network
in the evaluation process, while the surrogate-assisted methods
are just the opposite. The reason for this categorization is
that the proxy-based methods require additional computing
resources during the running of the NAS algorithm, while the
surrogate-assisted methods do not. Although this method can
stably classify EEMs, it only considers the evaluation time and
ignores the initialization time which is also crucial. As a result,
existing categorization methods cannot accurately measure the
true time complexity of EEMs.
In this survey, we categorize the existing EEMs based on
|AEEM|. The categorization method in this survey is inspired
by the concept of “ K-shot learning” [17] from the field of
machine learning. Specifically, “ K-shot learning” refers to a
machine learning paradigm where the model is trained with
“K” annotated samples per class. This term serves as a gener-
alization that encompasses various research topics, including
zero-shot learning, one-shot learning, and few-shot learning.
Considering this concept has been accepted a convention in the
machine learning community, in this survey, we also adopt
the term “shot” to describe the number of annotated DNNs
during the NAS process and categorize EEMs into zero-shot,
one-shot, few-shot, and N-shot methods. In general, the more
annotated samples used in the training process, the more
complex of the corresponding machine learning algorithm
will be. In this regard, this categorization method can also
intuitively infer the time complexity of the target EEMs, which
in turn helps the corresponding researchers easily choose the
EEMs with the proper complexity for the use. Specifically,
this results in four different categories shown below:
•N-shot evaluation method . For the N-shot evaluation
method, the number of trained architectures is greater4
than or equal to the number of searched architectures,
i.e.,|AEEM| ≥ |As|. The N-shot evaluation methods
still need to train every searched architecture, and mainly
accelerate the training of these architectures to consume
less runtime than TEM.
•Few-shot evaluation method . For the few-shot evaluation
method, the number of trained architectures is less than
the number of searched architectures and greater than
one, i.e., |As|>|AEEM|>1. Because the number of
trained architectures of the few-shot evaluation method
is less than the number of searched architectures, the
runtime is naturally less than TTEM .
•One-shot evaluation method . For the one-shot evaluation
method, the number of trained architectures is equal to
one, i.e., |AEEM|= 1. The one-shot evaluation method
can consume less runtime than TEM because only one
architecture needs to be trained.
•Zero-shot evaluation method . For the zero-shot evaluation
method, the number of trained architectures is equal to
zero, i.e., AEEM=∅. This method involves no training,
thus resulting in an extremely low cost.
We hereby acknowledge that the proposed categorization
method has certain limitations and potential bias. In particular,
the efficiency of EEMs is not solely determined by the
time spent training DNNs in AEEM. For zero-shot methods
where training neural networks is unnecessary, the process
still requires feeding one or more batches of data into each
network for multiple forward and backward passes, which
consumes time. If, in future EEMs, non-training processes
account for the majority of runtime, the interpretability of this
categorization method could be compromised.
D. Organization
TheN-shot evaluation methods, the few-shot evaluation
methods, the one-shot evaluation methods, and the zero-shot
evaluation methods are discussed in Section III, with the
focus on design principle, strength and weakness analysis
of these methods. In Section IV, we detail the evaluation
for these EEMs, including the evaluation metrics, benchmark
dataset, and the comparison results on benchmark datasets. In
Section V, we discuss the challenges and future directions. For
the convenience of quickly navigating to the interested part,
an illustration of above organization is shown in Fig. 4.
III. M ETHODS
In this section, we discuss the working principle and re-
search status of various EEMs under the four categories. For
each method, we first give its main operations or workflow.
Then, detailed literature examples are provided. Finally, we
discuss the advantages and disadvantages of the method.
A.N-shot Evaluation Methods
As shown in Fig. 3, for all searched architectures, the N-
shot evaluation methods still need to be trained and validated
on the dataset using GPUs. Different subcategories of N-
shot methods functioned at different stages to accelerate the
evaluation process.
AllSearched 
Architectures
Few GPUsTrain & Valid
DatasetPerformanceDownscale dataset:
reduce Data.Downscale model: 
reduce Arch.Network morphism & 
Learning curve 
extrapolation: reduce 
no. of iterationsFig. 3. The flowchart of N-shot evaluation method and its relationship to
subcategory methods.
For the purpose of better discussion, we first describe the
training process for a DNN. During the training process,
all samples in the dataset are sent into the model (i.e.,
architecture) to complete one forward and backpropagation,
and the process repeats Nepoch times. The training time mainly
depends on three factors: the dataset scale, the model size, and
the number of epochs. Specifically, the time of completing one
forward and backpropagation depends on the dataset scale and
the model size. If the dataset scale or the model size is large,
the computational complexity becomes higher, which leads to
much time spent on the computation. Furthermore, the training
time is also positively correlated with the number of epochs
(Nepoch ). The larger Nepoch , the longer the training time.
TheN-shot evaluation methods mainly consist of down-
scaled dataset methods ,downscaled model methods ,network
morphism , and learning curve extrapolation . Please note that
AEEMis equal to As(i.e.,AEEM=As) for all N-shot eval-
uation methods except for the learning curve extrapolation. As
the name says, the downscaled dataset methods aim to reduce
the dataset scale to accelerate the training. The downscaled
model methods focus on reducing the model size. Furthermore,
network morphism and learning curve extrapolation aim to
reduce the number of epochs.
1) Downscaled Dataset Methods: Similar to TEM, the
downscaled dataset methods train every searched architecture
to obtain their performance to guide the search process. The
difference is that they use a downscaled dataset to replace
the original one to accelerate the training of architectures.
Naturally, the runtime of the downscaled dataset methods is
less than that of TEM. The downscaled dataset methods can
be divided into three categories: (1) samples a subset of the
original dataset; (2) downsamples each sample in the original
dataset; and (3) uses a different and smaller proxy dataset
with similar properties to the original dataset. All of them do
not rely on pre-build evaluation methods, and the runtime of
them only contains evaluation time. Please note that the “proxy
data”, “proxy dataset”, and “data proxy” are the keywords to
select specific methods.
Category (1): This category samples a subset of the original
dataset as the downscaled dataset. Because the number of
samples in the dataset is reduced, the overall dataset scale is
reduced. The simplest way to obtain the subset is to randomly
select some samples from the original dataset. Literature
examples: Liu et al. [18] randomly selected a small num-
ber of medical images from the entire dataset to train the
searched architectures. However, random sampling may lead
to the removal of some representative samples, thus greatly
weakening the generalization of the architecture trained in the
subset [19], [20]. This will lead to an inaccurate evaluation
of the performance. To alleviate the problem, some works
designed new sampling methods. For example, Park et al. [19]5
Methods
(Section III )
Evaluation
(Section IV)
Challenges and future directions
(Section V)Efficient Evaluation 
Methods (EEMs)N-shot evaluation method
(Section III-A)
Few-shot evaluation method
(Section III-B)
One-shot evaluation method
(Section III-C)
Zero -shot evaluation method
(Section III-D)
Evaluation metrics
(Section IV-A)
Benchmark datasets
(Section IV-B)
Comparisons on Benchmark 
Datasets (Section IV-C)Cross -domain prediction
(Section V -B)
Multi -task prediction
(Section V -C)
Uniform representation
(Section V -D)Effectiveness validation
(Section V -A)Parameter -level methods
Architecture -level methodsPath -based methods
Gradient -based methodsPerformance predictors
Population memoryNetwork morphism
Learning curve extrapolationDown -scaled dataset methods
Down -scaled model methodsRefs. [2], [18], [21], [32], [77]–[82]
Refs. [1], [2], [10]
Refs. [23]–[26], [8 3]–[85]
Refs. [2], [13], [28], [29], [86], [87]
Refs. [140]–[146]Refs. [137]–[139]Refs. [9], [10], [59]–[61], [6 5]–[75], [118] –[136]Refs. [52]–[57], [103]–[117]Refs. [12], [50], [51], [101], [102]Refs. [28], [2 9], [31], [32], [34], [37]–[44], [4 6], [88]–[100]
Fig. 4. The organization of the description for EEMs.
designed a probe network to measure the impact of every
sample on the performance ranking of the architectures. Then,
they removed the samples that have a small impact on the
performance of architectures. Na et al. [20] used the data
entropy to analyze existing sampling methods, and proposed
a new sampling method that prefers samples in the tail ends
of the data entropy distribution.
Category (2): This category downsamples each sample in
the original dataset to reduce the dataset scale. Because the
size of every sample becomes smaller by the downsampling,
the overall dataset scale naturally becomes smaller. A typical
way is to reduce the resolution of the images in the dataset
for image classification. Specifically, as the resolution of the
image is reduced, the pixels in the image become fewer,
and the size of the image naturally becomes smaller. As a
result, reducing the image resolution in a dataset can reduce
the dataset scale, thus speeding up the training. Literature
examples: Chrabaszcz et al. [21] proposed the variants of
ImageNet [22], i.e., ImageNet16x16, ImageNet32x32, and Im-
ageNet64x64. Specifically, ImageNet16x16, ImageNet32x32,
and ImageNet64x64 used the box technique to downsample all
images in ImageNet to 16x16, 32x32, and 64x64, respectively.
Category (3): This category uses a different and smaller
proxy dataset with similar properties to the original dataset to
train the searched architectures. The proxy dataset may have
fewer samples and/or smaller sample size compared with the
original dataset. Literature examples: Zoph et al. [2] used
CIFAR-10 as the proxy for ImageNet because CIFAR-10 not
only has both fewer samples and a smaller size than ImageNet
but also targets the same image classification task as ImageNet.
Advantages: The downscaled dataset method is straight-
forward and easy to implement. It can reduce one to several
orders of magnitude computational costs depending on the size
of the downscaled dataset and the original dataset.
Disadvantages: In practice, the performance rankings of
DNNs trained on the original dataset may not be consistent
with their performance rankings when trained on a downscaled
dataset. This inconsistency is caused by two main factors.
(1) Information Loss: The downscaled datasets will inevitably
lose some information present in the original dataset. Thelost information may contain important features and patterns
that impact the performance. Therefore, the abilities of certain
models that perform well on the original dataset may not be
adequately reflected on the downscaled dataset. (2) Class Im-
balance: Downscaling can introduce the imbalance proportions
of the samples in various classes. If the relative proportions
of classes are not preserved during downscaling, certain class
samples may be excessively reduced, affecting the learning
ability from those classes. This can lead to performance
degradation, particularly in minority classes, when trained on
the downscaled dataset, deviating from the performance on the
original dataset. In order to avoid these problems as much as
possible, the downscaled methods should be carefully designed
to maintain class balance and to avoid the loss of information
with a significant impact on performance.
2) Downscaled Model Methods: The downscaled model
methods reduce the model size during the search process (e.g.,
search one cell rather than the whole architecture), and the
optimal cell found is repeated after the search process. Similar
to downscaled dataset methods, the runtime of the downscaled
model methods only needs evaluation time.
Literature examples: Many works have used this method
to accelerate the performance evaluation process, and we use
the keywords “cell repeat” and “repeated module” to choose
the papers mentioned below. For example, Zoph et al. [2]
proposed a cell-based search space (i.e., NASNet search space)
which repeatedly stacked the same cell structure to build the
architecture, and only the cell structure needs to be found
during the search process. To reduce the model size, they
transformed each model to one with a fewer number of cell
repeats and filters in the initial convolutional cell during the
search. Because of its efficiency, many works [1], [10] used
the NASNet search space or its variants and followed its
downscaled model method to accelerate the training.
Advantages: The downscaled model method is also easy
to implement, and it can save GPU memory. In addition,
the training time becomes less and the runtime spent on
performance evaluation is less than TTEM .
Disadvantages: Downscaling methods usually assume that
the same downscaling action has the same effect on the per-6
formance of different models. This is because the performance
ranking of the downscaled model and the original model will
remain consistent only if the degree of the performance change
is the same for all models. However, this assumption does not
always hold. For example, removing a convolution operation
from a high-complexity model may improve its performance
because the operation may be redundant. Removing the same
convolution operation from a low-complexity model may
reduce its performance because of the removal of the learnable
operation. As a result, these methods may result in a lower
correlation between the performance of the downscaled model
and that of the original model. In the future, this method
should theoretically demonstrate the correlation between the
performance ranking of the original model and the downscaled
model. This will help in the development of a more rigorous
downscaled model method, thereby avoiding the sole reliance
on intuition when downsizing the original model.
3) Network Morphism: Network morphism can accelerate
the evaluation process of most searched architectures by
reducing the number of epochs, so the evaluation time is the
only cost of the runtime of network morphism. Please note
that not every training process of the searched architecture
is accelerated, the reason for which is explained in the third
paragraph of this subsection. Because the training time for
most searched architectures is reduced, the runtime of network
morphism is significantly less than that of TEM. Network
morphism is first proposed in the background of transfer
learning to rapidly transfer knowledge from one fully-trained
network (i.e., a parent network) into another network (i.e., a
child network). It is soon applied in the field of NAS because
it can effectively accelerate the training of the child network.
For the sake of understanding, we first explain the acceleration
principle of network morphism and then discuss how it is
applied in NAS as an EEM.
The term morphism mathematically means a structure-
preserving map from one structure to another of the same
type. Network morphism refers to a function-preserving action
to transform a fully-trained parent network into a child one
that can completely preserve the function of the parent net-
work [23]. Specifically, the parent network is first transformed
into a different child network by predefined morphing actions.
Then, the child network directly inherits the weights from
the parent network and has the same function and output as
the parent network. After that, the child network is generally
trained on the dataset. Because the child network does not
require being trained from scratch, the number of epochs
needed to train to convergence is naturally reduced. As the
inheriting process does not need intensive resources, the
network morphism can accelerate the training time of the
child network. The morphing actions (i.e., function-preserving
actions) mainly include width morphing, deep morphing, ker-
nel size morphing, etc. Different types of morphing change
different parts of the parent network to generate a new child
network. For the convenience of understanding, Fig. 5 shows
an example of width morphing. As the name says, the child
network is wider than the parent network. However, the
functions performed by both the child network and the parent
network remain the same.
2
61
234
5a
d
eb/2c
d
e b/2
The parent network The child networkMorph
ea
d1
34
5bc22Fig. 5. An example of width morphing, where different types of nodes
indicate distinct operations (e.g., convolution layer, pooling layer). The labels
on the edges represent the corresponding weight values. The parent network
is a fully-trained network, and the child network is morphed from the parent
network by a width morphing operation. Specifically, in the child network,
node #6 is a copy of node #3, and the weights of node #3 are also transferred to
node #6. To preserve the function of the two networks, the value of weight bis
divided by two, while the values of weights a,c,d, anderemain unchanged.
In this way, if the same input is fed to the parent network and the child
network, respectively, the output of both networks will be the same, which
means the function of the parent network is preserved in the child network.
Generally, network morphism cannot be used to accelerate
the evaluation in NAS because there must exist fully-trained
parent networks for using network morphism. To gap this, the
architectures searched in the first iteration of the search strat-
egy are generally fully trained as the parent of the architectures
searched in the next iteration. Specifically, the architectures
searched in the first iteration of NAS are fully trained as the
parent network. Then, various morphing operations are applied
to these parent networks to generate child networks. These
child networks inherit weights from the parent network and
are trained to obtain their performance with fewer epochs.
These child networks are then regarded as the parent network,
and the above process is iteratively performed. Because all
searched architectures except the ones searched in the first
iteration are trained with fewer epochs compared with TEM,
network morphism consumes less runtime than TTEM .
Literature examples: Many works have used network mor-
phism to accelerate the performance evaluation process, and
the keywords of them are “network morphism” and “network
transformation”. For example, Cai et al. [24] adopted the RL-
based search strategy, and used the controller to generate a
morphing action to apply in the current network for the search.
The designed morphing actions include a width morphing
called Net2Wider and a deep morphing called Net2Deeper.
Net2Wider can replace a layer with a wider layer such as more
filters for convolutional layers, and Net2Deeper inserts a new
layer that is initialized as an identity mapping between two lay-
ers. Then, the newly searched architectures inherited weights
from the current network and were trained with fewer epochs
to obtain their performance. Cai et al. [25] proposed the path-
level morphing actions because the previous work adopted
layer-level morphing which can only add filters or add layers.
The path-level morphing can modify the path topologies of
the parent networks and preserve their functions. Noting that
the previous morphing actions can only increase the size of
the networks because the function-preserving property is not
guaranteed when decreasing the size of the networks. This is
not suitable for some computationally constrained scenarios.
To solve the problem, Elsken et al. [26] designed the approx-
imate network morphism that could decrease the network size
and roughly preserve the functions. Then, they adopted the
EC-based search strategy and used the approximate network7
Parent networkNetwork #1Transform
Network #2x xTransform
Parent network Network #1Morph
Network #2x xMorph
InConv
OutConv
Conv ConvIn Conv OutInPool
OutPool
Pool Pool
Fig. 6. An example to show the limit of the network morphism. Network
#1, Network #2 and the parent network are from the same search space, and
the parent network is a fully-trained network. Network #1 and Network #2
cannot be morphed from the parent network. Specifically, Network #1 cannot
be morphed from a parent network with different operations. This is because
the operation types (except for input and output nodes) are different between
the parent network and Network #1. Network morphism only modifies the
topological architecture of neural networks without changing the operation
types of existing neurons. Network #2 cannot be morphed from the parent
network because the function of the parent network with four convolution
layers cannot be the same as Network #2 with only one convolution layer.
morphism as a mutation operator to search for architectures.
Advantages: The network morphism method can leverage
much knowledge from parent networks, and it has more
theoretical foundations.
Disadvantages: Not all the networks in the search space
can be transformed from a fully-trained parent network. In
this case, the network morphism method will fail. Specifically,
the possible neural architectures morphed by the existing
morphism operation (e.g., depth morphing, width morphing,
kernel size morphing, and subnet morphing) are limited. As
shown in Fig. 6, not all potential architectures can be explored
during the search process, thereby restricting the diversity
and innovation of searched architectures. In addition, network
morphism may lead to the search process getting trapped in
local optima. This is because the global optimal may not be
obtained by morphing the parent network, which causes the
algorithm to fall into a local optimal. Specifically, assuming
that Network #2 in Fig. 6 represents a globally optimal
solution and the parent network is the current solution, it
becomes evident that relying solely on morphing operations
cannot reach the ultimate global optimum. Such limitations
can restrict the exploration of the search space and prevent
NAS from finding the optimal network architecture. Overall,
the limitations of network morphism stem from the lack of
diversity of existing morphing operations. One future direction
is to explore more types of morphing operations to promote
the diversity of morphed networks. In this way, the inflexibility
of network morphism can be alleviated to some extent.
4) Learning Curve Extrapolation: Different from other N-
shot evaluation methods that AEEMis equal to As,AEEM
in the learning curve extrapolation methods composed of As
andAf, i.e.,AEEM=As∪Af, where Afis an architecture
set that includes some fully trained architectures for learning
curve extrapolation. The learning curve extrapolation trains
a model on Afto predict the actual performance of the
architecture trained after only a few epochs. In this way, the
training time of the searched architectures is largely reduced
because the epochs become fewer. Since the time saved by
training the architectures in Asis generally greater than the
time spent on fully training the architectures in Af, it can still
spend less time on performance evaluation than TEM.
To illustrate the principle of learning curve extrapolation,
we first introduce three terms: learning curve ,partial learningcurve , and final learning curve . Specifically, the term learning
curve generally refers to the function of performance with a
growing number of iterations for an iterative machine learning
algorithm. We use the term final learning curve to represent
the entire learning curve ft= (p1, p2, . . . , p t) of an algorithm
from the beginning to the end of the training, where pi
represents the performance at the iteration of iandptis
the final performance. Furthermore, we use the term partial
learning curve to refer to the learning curve observed as of
epoch l, i.e., fl= (p1, p2, . . . , p l)(l < t).
The workflow of the learning curve extrapolation method
is as the following. First, some architectures are sam-
pled from the predefined search space, and then are
fully trained to construct the architecture set Af=
{(x1, f1
t),(x2, f2
t), . . . , (xk, fk
t)}where xidenotes the i-
th architecture and fi
trepresents its final learning curve.
Then, the architecture set is transformed to another form
Af={(x1, f1
l, p1
t),(x2, f2
l, p2
t), . . . , (xk, fk
l, pk
t)}where fi
l
represents the partial learning curve and pi
tdenotes the final
performance of architecture xi. The learning curve extrapola-
tion method builds a model P′(xi, fi
l)by training it on Af.
The model P′(·)can be used to predict the final performance
of the searched architectures by feeding their architectures
and partial learning curves. Finally, the model is used to
predict the final performance of the searched architectures
after obtaining their partial learning curve. The runtime of
the learning curve extrapolation method includes initialization
time (to build the predicted model) and evaluation time (to get
the partial learning curve).
It is worth mentioning that the early-stopping strategy, a
popular EEM, is considered a special case of learning curve
extrapolation. Specifically, the early-stopping strategy directly
regarded the obtained performance after just a few epochs as
the final performance. We can view that the early-stopping
strategy builds a function pt=fl(t) = plfor a partial
learning curve fl= (p1, p2, . . . , p l)(l < t ) to predict the final
performance pt. Because no training architecture is needed to
build pt,AEEMis equal to Asfor the early-stopping strategy.
Literature examples: The keywords we used to select the
studies of the learning curve extrapolation are “partial learning
curves,” “partial training,” and “early stop”. The learning
curve extrapolation was first proposed to automatically find
a well-performing hyperparameter configuration for neural
networks [27]. Because both hyperparameter optimization and
NAS are faced with the problem of the high cost of network
evaluation, the learning curve extrapolation is soon applied to
NAS. For example, Rawal et al. [28] developed a Long Short
Term Memory (LSTM) consisting of an encoder RNN and a
decoder RNN to predict the final performance of a partially
trained model. They fed the validation loss at the first 10
epochs to the encoder and obtained the final validation loss
by the decoder. Baker et al. [29] used a set of support vector
machines to predict the architecture performance by feeding
the features of model architectures, training hyper-parameters,
and partial learning curve. Furthermore, they model the es-
timation process as a gaussian perturbation to calculate the
probability p(ˆy≤ybest)where ˆyrepresents the estimated
final performance, ybestdenotes the performance of the best8
EpochPerformance
𝑡1 𝑡2architecture #1
architecture #2
Fig. 7. An example of the early stopping strategy causing inaccurate
prediction. The figure shows the performance values of architecture #1 and
architecture #2 at every epoch. The performance ranking at t1of architecture
#1 and architecture #2 is not consistent with the final performance ranking
att2. This is because architecture #2 is close to convergence at t1, while
architecture #1 still requires further training. Hence, when both architectures
converge, architecture #1 has higher performance than architecture #2.
Few Searched
Architectures
 Fewer GPUsTrain & Valid
DatasetPre-built
Eval. Method
Other Searched
ArchitecturesPre-built
Eval. MethodPerformancePerformance Predictor: 
build regression model
Population memory: 
record in the cache
Fig. 8. The flowchart of few-shot evaluation method and its relationship to
subcategory methods. “Eval.” is the abbreviation of “Evaluation”.
architecture. When p(ˆy≤ybest)≥∆where ∆is used to
balance the trade-off between increased speedups and risk
of prematurely terminating good architecture, the training of
an architecture terminates. Wistuba et al. [30] built a neural
network to rank the architecture with the input of architecture,
partial learning curve, dataset, and hyper-parameters. In this
way, they can search for the optimal architecture. As for
the early-stopping strategy, it can be easily embedded in the
NAS algorithms. For example, Zoph et al. [2] trained every
architecture for only 20 epochs to reduce the search cost.
Advantages: The learning curve extrapolation can decrease
the number of epochs to accelerate the training process. It
is easy to combine with different NAS algorithms without
modifying data or models. Thus, it is highly flexible.
Disadvantages: The learning curve extrapolation needs to
fully train some architectures as the dataset for building the
prediction models. The amount of data should not be too small,
otherwise, the generalization ability of the prediction model
may be poor. Thus, it still requires consuming much time
to collect the dataset. For example, Baker et al. [29] built
three architecture datasets for ResNets, MetaQNN, and LSTM
with the data numbers 1,000, 1,000, and 300, respectively. The
training of these architectures is time-consuming. In addition,
the early-stopping strategy assumes that the performance rank-
ing obtained by the partially-trained architectures is consistent
with the actual performance ranking. However, the assumption
is not always held because the convergence speed of different
networks is not consistent as shown in Fig. 7. Accordingly, the
early-stopping strategy may offer an inaccurate performance
estimation. In fact, Zoph et al. [2] also retrain the top- 250
architectures with the highest performance until convergence
on CIFAR-10 to discover the best architecture after searching.
This confirmed the inaccuracy of the early-stopping strategy.
B. Few-shot Evaluation Methods
As shown in Fig. 8, the few-shot evaluation methods first
need to train a few architectures (i.e., |AEEM|<|As|) to
pre-build the evaluation method. Then, the performance of
other architectures is obtained by the evaluation method. Thiscan consume less runtime than TEM does. According to the
evaluation method utilized, existing few-shot methods can be
divided into performance predictor and population memory .
All these methods need to build the evaluation method and
then use it to evaluate the performance of the searched
architecture. Thus, their runtime contains initialization time
and evaluation time.
1) Performance Predictors: The performance predictor is
a kind of regression model, and can directly predict the per-
formance of the architectures in Asafter being trained on the
architecture set AEEM={(Ai, Pi)}|AEEM|
i=1where Aidenotes
thei-th architecture and Pirepresents its actual performance.
Because |AEEM|is generally less than |As|and the time
of predicting the performance of the architectures in Asis
negligible, the performance predictors would consume less
runtime than TEM. The main steps of building performance
predictors are as follows:
Step 1 Sample and train some architectures in the predefined
search space to serve as the architecture set;
Step 2 Encode the architectures in the architecture set;
Step 3 Train a performance predictor to map the encoding of
architectures to the corresponding performance values;
Literature examples for each Step: We will separately
discuss the existing works about steps 1-3 in this part, and the
keywords of the selected papers are “performance predictor”
or “neural predictor”.
Step 1: The researchers often randomly sample a lot of
architectures from the search space and train them from
scratch to construct the architecture set. However, the set
obtained by random sampling may not cover all representative
architectures, which may result in the weak generalization
of the performance predictor. Consequently, some works use
other sampling methods to address this aspect. For example,
Daiet al. [31] used Quasi Monte-Carlo to sample architectures
from the search space. Hassantabar et al. [32] chose Sobol
sequences to sample architectures from the search space
because it can generate more evenly distributed samples. The
aims of these works are all to generate a small architecture
set that can sufficiently represent the entire search space, thus
minimizing the number of architectures required to be trained.
Step 2: The architecture set gained in Step 1 cannot be
directly fed into the regression model and must be encoded
into a form that the regression model can tackle. In practice,
there are two popular ways to encode the architectures: the
sequence-based scheme and the graph-based scheme [33].
The sequence-based scheme encodes the specific serialized
information of the architectures and flattens the entire architec-
tures to the strings. This kind of encoding scheme is broadly
used in various search spaces. For example, Deng et al. [34]
employed a layer-based search space, proposed a uniform
layer code to encode each layer to numerical vectors, and
used the LSTM which is effective in processing the sequential
data to integrate the information along a sequence of layers
into a final string. Sun et al. [35] proposed a block-based
search space that is composed of the ResNet block, DenseNet
block, and pooling block, and encoded each block based on
its kernel size, input channel, and output channel by sequence,9
and obtained a string at last. For the cell-based search space,
Luo et al. [36] used the identifier of the input layers and
the name of the operation applied to encode each layer to
a string token. Then, a sequence of the discrete tokens was
used to describe the given architectures. The sequence-based
scheme is easy to implement and straightforward. However, it
can only implicitly model the topological information of the
given architectures. This makes performance predictors hard
to capture topological information from the encoding. As a
result, this scheme may lead to the reduction of the prediction
ability of performance predictors for some architectures with
rich topology information.
The graph-based scheme generally regards the architecture
as the Directed Acyclic Graph (DAG). It explicitly represents
the topological information of the architecture. For example,
Wen et al. [37] used the adjacency matrix to describe the
connections between operations and the one-hot codes to
represent the operation type. To encode more information, Xu
et al. [38] obtained the vectors of the operations, the FLating-
point Operations Per second (FLOPs), and the parameter size
for each operation. Then, these vectors were broadcasted
into the adjacency matrix to generate the type matrix, the
FLOP matrix, and the parameter matrix, respectively. At last,
these matrices were concatenated as the final encoding. Ning
et al. [33] modeled the operations as the transformation of
the propagating information to mimic the data processing
of the architecture. Although the graph-based scheme can
better encode the topological information, it is not suitable
for the layer-based and block-based search spaces which are
almost the linear structure. The introduction of topological
information may bring redundant information for them [39].
Step 3: The performance predictor can be essentially treated
as the regression model, and its training can be formulated as
Equation (3):
min
TpL(R(Tp, Encoder (X)), y) (3)
where Tpis the trainable parameters of the regression model
R, andL(·)denotes the loss function. Encoder (·)denotes
the encoding method. Xandycorrespond to the architecture
and its performance in the architecture set, respectively. Then,
we introduce the training process of the performance predictor
from two aspects: the regression model and loss function.
In terms of the regression model, we illustrate it based
on the encoding scheme because different models are good
at handling different types of data. First, for the sequenced-
based schemes, the models suitable for the sequential data
are broadly used such as LSTM [34], [40], RNN [41], Trans-
former. Furthermore, some non-neural models are also used
such as random forest and gradient boosting decision tree
because of their ability to distinguish feature importance [39].
As for the graph-based scheme, GNNs are commonly used
because of its superiority in processing graph data [37].
With regard to the loss function, it can be mainly divided
into two types. The first is the element-wise loss function that
calculates the distance between the predicted label and the
ground-truth label for every sample. The most commonly usedis Mean Square Error (MSE) shown as Equation (4):
LMSE =1
NNX
i=1(yi−f(xi))2(4)
where xiandyirefer to the architecture and its corresponding
performance, Nrepresents the number of samples. f(·)repre-
sents the performance predictor. Huber loss is also a popular
element-wise loss function as shown by Equation (5). It can
prevent the model from being dominated by outliers [31].
LHuber =1
2(yi−f(xi))2If|yi−f(xi)| ≤δ
δ|yi−f(xi)| −1
2δ2otherwise(5)
In Equation (5), δcan be set according to the specific situation.
When the prediction deviation is less than δ,LHuber uses
the squared error. Otherwise, it uses linear error. The second
type of loss function is the pair-wise ranking loss function.
It is firstly used in the performance predictor in [38]. This
is because the performance ranking is more important than
the absolute performance of every architecture for NAS. The
pair-wise ranking loss function just cares more about the
performance ranking. Specifically, a typical hinge pair-wise
ranking loss is represented by Equation (6):
Lpairwise =1
NNX
i=11
|Ti|X
j∈TiMax (0 , m−(yi−yj)) (6)
where Ti={j|yi> yj}.mis the comparison margin.
Literature examples based on the categorization: Ac-
cording to whether the performance predictors are retrained
for updating during the process of NAS, the performance
predictors can be divided into online and offline predictors.
For the offline predictor, the architecture-performance pairs
have been completely collected before training the predictors.
Note that the architecture set AEEMof offline performance
predictor is not sampled from the searched architecture As.
Once the predictors begin to work, there will be no new sample
added to the training set. Many works employ this way to train
the predictors owing to its simplicity. For example, Wen et
al.[37] directly trained a small number of the architectures to
train a GCN model as the performance predictor. However, the
performance of offline predictors largely relies on the quality
of the collected data. If the quality is poor, the predictor
may also result in poor performance. Furthermore, once new
samples are added to the dataset, the offline predictor has to
be retrained, which makes the offline predictor inflexible.
Different from offline predictors, online predictors can add
new samples to improve their performance after they have
been trained and used. As a result, it is more flexible and
practical than offline predictors. Most online predictors first
train a small number of architectures to build the predictor.
Then, they choose some untrained architectures and predict
their performance by predictors. Finally, the architectures with
the top- khighest predicted performance are trained and added
to the architecture set to retrain the predictor. As a result,
the architectures in AEEMare generally partially sampled
fromAsbecause the newly added architecture is usually
found by the NAS algorithm. Please note the runtime of the
online predictor also includes the update time, in addition10
to the initialization time and evaluation time mentioned in
Subsection II-B. Specifically, the update time refers to the time
for retraining the offline predictor. Many works adopted online
performance predictors. For instance, Liu et al. [41] started
the training of the predictor with the simple architectures, and
continuously used the predictor to evaluate more complex ar-
chitectures. Then, they picked and trained the most promising
architectures as the new samples to finetune the predictor. Wei
et al. [42] proposed an EC-based search strategy, and trained
the predictor with the architecture-performance pairs of the
initial population. Then, they trained the architectures in the
next population and added them into the architecture set to
retrain the performance predictor.
Depending on if the architectures are fully labeled, the
existing works can also be classified into supervised and semi-
supervised performance predictors.
The supervised performance predictors completely use the
labeled architectures to train the predictors. Most predictors
belong to the supervised one because it is simple. However, the
number of labeled architectures is generally limited because
the annotation of architectures is expensive. As a result, the su-
pervised performance predictors generally try to extract more
meaningful features from the limited architectures to improve
their performance. For example, Chen et al. [43] proposed an
operation-adaptive attention module in the predictor to capture
the relative significance between operations. Ning et al. [33]
designed an encoding method that can model the calculation
process of neural architectures to extract more meaningful
information from architectures.
Unlike the supervised predictors, the semi-supervised ones
not only use the labeled architectures but also utilizes the
unlabeled architectures. This is because the massive unlabeled
architectures can provide invaluable information to optimize
the performance predictor. For example, Tang et al. [44] used
an AutoEncoder to learn meaningful features from both the
labeled and unlabeled architectures and constructed a related
graph to capture the inner similarities. Then, the related graph
and the acquired features were mapped into the performance
value by a GCN predictor. Luo et al. [45] trained the perfor-
mance predictor with a small set of annotated architectures.
Then, they used the predictor to predict the performance of
unlabeled architectures. These predicted architectures were
then used to retrain the performance predictors.
Advantages: Performance predictor has emerged as one of
the research hotspots in EEMs in recent years [34], [41], [46].
It has fast evaluation speed since the searched architecture
does not need to be evaluated on GPUs.
Disadvantages: One critical problem for the performance
predictor is that many fully-trained architectures are required
to build the performance predictor. The contradiction of this
problem lies in that training a large number of architectures
may conflict with the design intention for EEMs, but a
small number of architectures may lead to the overfitting
problem [47]. In practice, the performance predictors always
face a lack of data. The existing works solve the problem
mainly from three aspects: encoding method, model, and
data. In terms of the encoding method, researchers focus on
representing more useful information about the architecture by
Architecture 𝐴𝑖
Train𝐴𝑖𝐴𝑖in Population 
Memory ?
Performance 𝑃𝑖Architecture Performance
𝐴1 𝑃1
𝐴2 𝑃2
… …
𝐴𝑁 𝑃𝑁Yes
Add 
(𝐴𝑖, 𝑃𝑖)Population memory
NoFig. 9. A workflow of the population memory. Specifically, for each searched
architecture Ai, if it is in the population memory that stores architectures and
their corresponding performance, we can directly gain its performance Pi.
Otherwise, it will be trained to obtain the performance Pi, and ( Ai,Pi) will
be subsequently stored in population memory.
the encoding method. For example, Xu et al. [38] proposed
a graph-based encoding method to extract more valuable
information about the architecture. Specifically, they encoded
not only the topological information and operation type of
each node but also the FLOPs and parameters of each node.
As for the model, many works focus on how to extract
more meaningful features from the encoding. For example,
Wang et al. [48] proposed a multi-stage MLP model that
used different models to predict the architectures in different
ranges of performance values. With regard to the data, Liu
et al. [49] proposed a homogeneous augmentation method for
the architectures to generate a large amount of labeled data in
an affordable way. Although many solutions are proposed, as
a common problem for machine learning, the lack of labeled
architectures cannot be solved from the root. As large language
models continue to advance, the exploration of performance
predictors can shift its focus toward harnessing self-supervised
learning methods to create a large prediction model for neural
architecture. This study will largely improve the generalization
of the performance predictor and eliminate the necessity of
constructing separate performance predictors for each search
space through independent data collection.
2) Population Memory: Population memory is popular to
accelerate fitness evaluation in EC. It is then used in the
EC-based NAS to avoid evaluating the same architecture
repeatedly. This is because the individuals in the previous
populations may appear in the latter populations, and it is
a waste of computational resources to reevaluate these in-
dividuals. The population memory can be seen as a cache
system, and it mainly shortens time by reusing the architectural
information that has appeared. For the population memory,
the trained architecture set AEEMis a subset of the searched
architecture set As(i.e.,AEEM∈ As). The workflow of the
population memory is shown in Fig. 9. In this way, we can
avoid evaluating the architectures repeatedly when facing the
same architectures as the previous architectures. Many EC-
based NAS adopted the method to avoid unnecessary costs.
Literature examples: We utilize the “population memory”
and “fitness memory” as keywords to choose the specific
papers. Fujino et al. [50] used a population memory to store
the performance value of the individuals, and retrieved from
memory when the architectures with the same encoding as
the previous architectures appear. Similarly, Sun et al. [51]
designed a global cache system to record the hash code
and the performance of the architectures. They obtained the11
performance when facing an architecture whose encoding has
been stored in the cache.
Advantages: Compared with other EEMs, population mem-
ory allows for a more accurate evaluation of performance
because the obtained performance by population memory is
equal to the actual performance.
Disadvantages: The shortcoming of the population memory
method is its lack of flexibility. Specifically, it can not be used
to evaluate the architectures that have not been trained before.
However, in practice, the repetitive architectures only account
for a small portion of all architectures searched [51].
C. One-shot Evaluation Methods
As indicated in Fig. 10, the one-shot method only re-
quires training one neural architecture during the whole search
process (i.e., |AEEM|= 1 ). It is also called the weight-
sharing method in the field of NAS. Because only one network
(also called “supernet”) needs to be trained, the method is
cost-saving. Depending on whether architecture search and
supernet training are coupled, the existing one-shot methods
can be classified into path-based methods andgradient-based
methods . All of them do not rely on the pre-build models, and
the runtime only comprises evaluation time.
1) Path-based Methods: The path-based method trains a
supernet that contains all candidate architectures (i.e., subnet).
The weights of subnets can directly be extracted from the
supernet. Each path refers to a subnet in the supernet. The
workflow of evaluating subnets is shown in Fig. 11. The main
workflow of the path-based method is as below:
Step 1 Design the supernet subsuming all the candidate ar-
chitectures;
Step 2 Train the supernet from scratch by a path sampling
strategy;
Step 3 Predict the searched architectures by inheriting the
weights from the supernet and inferencing the perfor-
mance on the validation dataset.
Among these steps, the training of the supernet (i.e., Step 2)
is critical. This is because the weights of the supernet are
obtained through training and the weights of the subnet are
extracted from the supernet. In practice, the weights of the
supernet are almost not directly optimized. The reason is
One Supernet
Architecture
 Fewer GPUsTrain & Valid
DatasetSupernet
PerformanceCandidate Architecture
Performance
Path-based: extract weight from 
supernet based on specific path
Gradient -based: optimize 
architecture and weight together
Fig. 10. The flowchart of one-shot evaluation method and its relationship to
subcategory methods.
Performance
Supernet SubnetValidation
datasetweights infer
Fig. 11. The workflow of evaluating subnets for the path-based method. The
grey lines and nodes in the subnet represent inactive connections and nodes,
respectively. Only the weights of the supernet are obtained by training. The
weights of the subnet are extracted from the supernet. Then, the performance
of these subnets can be obtained by inferring on the validation dataset.that the weights in the supernet are deeply coupled with the
architecture of the supernet. Accordingly, the supernet is not
robust to the changes in the architecture [52], [53]. This may
lead to a low correlation between the performance obtained
by inheriting the weights and the actual performance.
Literature examples: Many path sampling strategies are
proposed to decide which path(s) to train in every epoch of
the supernet training to decouple the weights&architecture of
the supernet, and the keywords we used to select the studies are
“one-shot” and “single path”. For example, Bender et al. [52]
proposed to randomly drop a subset of the operations in each
iteration. The probability of the dropout is determined by a
parameter called dropout rate. However, experiments in [52]
indicated the training is sensitive to the dropout rate, which
makes the training of supernet complicated. Guo et al. [53]
proposed to randomly sampled a subnet in the supernet, and
only the weights of the subnet are updated in each iteration.
This method is hyperparameter-free and thus avoids the defect
of the path dropout strategy which is sensitive to the dropout
rate. Chu et al. [54] pointed out that the inherent unfairness
in the method of Guo et al. may lead to biased evaluation.
They thus proposed strict fairness sampling strategy. This
strategy samples m(mis the number of choice blocks per
layer) at each epoch. In this strategy, all choice blocks are
activated only once and are optimized only on one batch of
data. Zhang et al. [55] designed the novelty-driven sampling
strategy. Only the weights of the architectures sampled by
novelty search were optimized. You et al. [56] proposed a
multi-path sampling strategy with rejection to filter the weak
paths by evaluating them on a portion of the validation dataset.
During the training of supernet, only those potential paths
were trained. Zhang et al. [57] recognized that subnets with
varying computational resources might require distinct training
strategies. To address this, they proposed the probability shift,
which is learned based on the training sufficiency of each
subnet. This probability shift enables the automatic adjustment
of sampling probabilities, optimizing the training process of
the supernet.
Advantages: The path-based method only needs to train
one architecture (i.e., |AEEM|= 1) and the inference on the
validation dataset is quick, thus it can largely accelerate the
performance evaluation.
Disadvantages: Although the prediction accuracy of the
path-based methods is improved with the help of the path
sampling strategies, they still face the multi-model forgetting
problem [58]. Specifically, the multi-model forgetting problem
refers to the performance decline of the previous-trained
subnets when training subsequent subnets. This is because of
the overwriting of shared weights when training the subsequent
subnets with partially-shared weights one by one. An example
of this problem is shown in Fig. 12. The multi-model forget-
ting problem will deteriorate the prediction ability. As a result,
although the path-based methods can greatly reduce computa-
tional time, they may result in unreliable performance ranking.
Many solutions are proposed to alleviate the multi-model
forgetting problem. For example, Benyahia et al. [58] proposed
a statistically-justified weight plasticity loss to regularize the
optimization of the shared parameters. In this way, they can12
wA1 wA2 wA3 wA4
Supernet Model A Model AStep 1: Train model A to convergence.
EpochPerformanceModel A
Model BwB1 wB2 wB3 wB4wB6
wB5
Step 2: Train model B which shares weights with A .wA1 wA2 wA3 wA4
Model B Model B
Fig. 12. An example of the multi-model forgetting problem. Model A and
Model B are two different subnets that share part of the weights in the
supernet. During the training of the supernet, model A is firstly trained in
Step 1 and then the model B is trained in Step 2. This will cause the multi-
model forgetting problem. Specifically, the weights of model A are optimized
in the supernet at Step 1. Then, model B which shares partial weights with
model A is trained at Step 2. During the training of model B, the performance
of model A declines. The phenomenon is called multi-model forgetting.
preserve the shared parameters that were important for the
previous model. Zhang et al. [59] formulated the supernet
training as a constrained continual learning optimization prob-
lem. During the training of the current subnet, they constrained
the training loss of all previous subnets is less than that in the
last step. However, the number of constraints increases linearly
as the increase of previous subnets. Thus, they proposed a
greedy novelty search method to select a subset of existing
constraints from previous subsets to overcome the multi-model
forgetting problem. Although the above works can alleviate the
phenomenon of multi-model forgetting, they cannot solve the
problem from the root. This is because the problem is caused
by the mechanism the path-based method trains the supernet to
evaluate subnets. As a result, the ranking correlation between
the performance obtained by the path-based method and the
actual performance may be poor.
2) Gradient-based Methods: Unlike the path-based meth-
ods, the gradient-based methods couple the supernet training
and architecture searching. Specifically, they relax the discrete
search space to be continuous, and jointly optimize the archi-
tecture and the supernet weights by the gradient-based search
strategy. The most famous work falling into this category is
DARTS [10] proposed in 2019. Then, the DARTS approach
became the dominant approach to gradient-based methods
owing to its effectiveness. Specifically, DARTS used a cell-
based search space and regarded the supernet as a DAG with
Nnodes. Each node x(i)is a latent representation such as the
feature map, and each edge (i, j)refers to the operation o(i,j)
that can transform xi. Each intermediate node is computed
based on all of its predecessors as shown in Equation (7):
x(j)=X
i<jo(i,j)
x(i)
(7)
To relax the discrete search space to be continuous, the
categorical choice of a particular operation is continuously
relaxed by employing a softmax function over all the possible
operations as shown in Equation (8):
¯o(i,j)(x) =X
o∈Oexp
α(i,j)
o
P
o′∈Oexp
α(i,j)
o′o(x) (8)where Ois a set of candidate operations. α(i,j)denotes the
operation mixing weights (i.e., architecture parameter) for
a pair of nodes (i, j). An architecture can be obtained by
replacing each mixed operation o(i, j)with the most likely
operation, i.e., o(i,j)= argmaxo∈Oα(i,j)
o. Then, DARTS aims
to jointly learn the architecture parameters αand the weights
ωwhich determine the training loss Ltrain and validation loss
Lval. This can be formulated as a bilevel optimization with α
as the upper-level variable while ωas the lower-level variable
problem, modeled by Equation (9):
minαLval(w∗(α), α)
s.t.w∗(α) = argminwLtrain(w, α)(9)
By this way, we can find α∗that minimize the Lval(w∗(α), α)
where the weights ω∗minimize the Ltrain(w, α).
Literature examples: Many gradient-based methods are
proposed to improve effectiveness, and we utilize “differen-
tiable NAS,” “differentiable architecture search,” and “differ-
entiable neural architecture search” as the keywords to choose
the papers. These papers mainly focus on the three research
directions as follows:
•Reduce memory: Many works try reduce the memory
of gradient-based methods. For example, Cai et al. [60]
leveraged binarized architecture parameters to guarantee
only one path of activation is active in memory at runtime.
Xuet al. [61] randomly sampled a subset of channels
as the proxy of all the channels to reduce memory
consumption. In addition, they used edge normalization,
which adds a new set of edge-level parameters to improve
the stability of the search. Similarly, Xue et al. [62] saved
the memory by adopting the subset of the channels. The
difference is that they proposed an attention mechanism,
and selected the channels with higher attention weights.
The method can better transmit important feature infor-
mation into the search space and prevent the instability
of the search.
•Enhance generalization: The gradient-based method
generally suffers from poor generalization on various
datasets of the searched architectures. To solve this, Li
et al. [63] introduced the idea of domain adaptation.
They improved the generalizability of architectures by
minimizing the generalization gap between domains. Liu
et al. [64] proposed a novel method to mix the data from
multiple tasks and domains into a composited dataset.
Yeet al. [65] proposed a regularization method that can
maintain a small Lipschitz of the searched architectures.
This is because the architectures have good generalization
ability if its Lipschitz is small.
•Improve stability: The gradient-based method lacks sta-
bility because of performance collapse. Existing works to
alleviate this problem can be divided into five categories.
The first category typically uses dropout or stop opera-
tions to limit the number of skip connections, such as
P-Darts [66] and DARTS+ [67]. However, they would
reject some potentially high-performance architectures
with multiple skip connections. The second category
refers to regularizing relevant indicators, such as Hes-
sian eigenvalues [68], [69] and layer alignment [70], to13
indicate the performance collapse of the network. The
main limitation of these methods is their reliance on the
quality of the indicator. The third category is known as
modifying the cell, which often involves auxiliary skip
connection [71] and additional sigmoid function [72].
Thus, they are computationally expensive. The fourth
category is regarded as dropping out unimportant opera-
tions based on the group during the training process [73],
[74]. Unfortunately, they highly rely on the manually set
hyperparameters, e.g., group number. The fifth catagory
is based on the regularization method, such as Beta-
decay [65] and BatchNorm of the supernet [75]. However,
they may lead to overly conservative updates, preventing
the ability to explore the search space.
Advantages: The gradient-based method combines supernet
training and architecture searching, which is more effective
than the path-based method and achieves great performance
on a large-scale dataset.
Disadvantages: There are three disadvantages of the
gradient-based method: (1) Huge memory: The gradient-based
methods always suffer from large memory because of the joint
optimization which not only trains a supernet but also searches
for the architectures. (2) Low generalization: The architecture
search by radient-based method cannot perform as well as the
dataset used in the search on other datasets. (3) Performance
collapse: The gradient-based methods, e.g., DARTS, tend to
accumulate parameter-free operations (i.e., skip connection)
which lead to rapid gradient descent [68], [71]. However,
learnable operations such as convolution are the better choice
for improving the expression ability of DNNs. As mentioned
above, existing works can only alleviate these problems but
cannot address them.
In summary, the one-shot method can largely reduce the
computational time by only training one supernet. However,
the path-based methods experience the phenomenon of multi-
path forgetting, and the gradient-based method faces the
problem of performance collapse. These problems may lead
to inaccurate ranking. Hence, the one-shot method may fail to
reflect the true ranking of architectures. This also affects the
performance of the discovered architecture. The evidence is
that the architecture searched by the one-shot method performs
sometimes worse than those discovered by random search [76].
D. Zero-shot Evaluation Methods
For all searched architectures, they do not need to be trained
on GPUs (i.e., AEEM=∅), instead of evaluating them by
scoring function as shown in Fig. 13. Based on the type
of scoring function, the zero-shot methods can be mainly
classified into parameter-level methods andarchitecture-level
methods . In addition, the scoring function is typically a math-
ematical formula that does not rely on pre-built evaluation
AllSearched
ArchitecturesScoring Function
PerformanceParameter -level: score  the 
saliency of parameters
Architecture -level: score the 
properties of architecture
Fig. 13. The flowchart of zero-shot evaluation method and its relationship
to subcategory methods.methods. Thus, the runtime of the zero-shot method is the eval-
uation time to obtain the score. Because getting the score of
architecture is often time-saving, it has an extremely low cost.
Please note that zero-shot methods cannot directly evaluate the
absolute performance of a given architecture. It can only rank
the architectures based on the score of the performance.
1) Parameter-level Methods: The parameter-level methods
first use the indicator to measure the saliency (i.e., impor-
tance) of certain parameters. Then, they score for the entire
architecture by aggregating the saliencies of certain parame-
ters. The methods only required a minibatch of data and a
forward/backward propagation pass to calculate the indicators
for certain parameters. Because it involves no training, it
can consume much less runtime than TEM. Generally, the
saliency indicators generally come from the network pruning
literature. They are originally used to evaluate the importance
of parameters and thus remove the unimportant ones.
Literature examples: There have been some works pro-
posed in this category, and the keywords of them are “zero-
shot”, and “parameter saliency”. For example, Abdelfattah et
al.[137] adopted a series of pruning-at-initialization metrics
includes grad norm, snip [147], grasp [148], fisher [149],
[150], synflow [151] to measure the saliency of parameters.
Then, they summed up the metric value of all parame-
ters to score for the architecture performance. Specifically,
grad norm uses the Euclidean norm of the gradient after a
forward/backward propagation pass of a minibatch of training
data to measure the parameters. Snip calculates the saliency
of a specific parameter by approximating the change in cross-
entropy loss when the parameter is pruned. Synflow is a
modified version of snip with the change that the loss in
synflow is the product of all parameters in the network.
Grasp calculates the saliency by measuring the approximate
change in gradient norm when the parameter is removed.
Fisher measures the approximate loss change when activation
channels are removed. Li et al. [139] discovered that a high-
performance network tends to possess high absolute mean
values and low standard deviation values for the gradient.
Thus, they introduced the zero-shot inverse coefficient of
variation, which considers both the absolute mean and standard
deviation values of each layer.
Advantages: The parameter-level method has a low com-
putational complexity because only a few simple parameters
need to be computed.
Disadvantages: Despite the cost-saving of parameter-level
methods, they are proven inaccurate. Simply summing up the
saliencies of each parameter to measure the performance of
architectures is problematic. This is because the saliency only
reflects the impact of the parameter on the architecture. In
fact, a recent work [152] also pointed out that the existing
parameter-level methods are not suitable for ranking archi-
tectures. Their ranking qualities even cannot surpass those of
parameter size or FLOPs. This proves the unreliability of the
parameter-level methods on the other hand.
2) Architecture-level Methods: The architecture-level meth-
ods score the architectures by measuring the properties pos-
itively related to architecture performance. Because they do
not need to train any architecture, they can largely save time14
TABLE I
SUMMARY OF DIFFERENT EEM S IN NAS INCLUDING THEIR PRINCIPLE AND RELATIVE REFERENCES . THE FIRST COLUMN REPRESENTS THE
CATEGORIES OF EEM S. THE SECOND COLUMN ILLUSTRATES THE SUBCATEGORIES OF EACH CATEGORY WITH THEIR RUNTIME COMPOSITION ,WHERE
‘IT’ AND ‘ET’ DENOTE THE INITIALIZATION TIME AND EVALUATION TIME ,RESPECTIVELY . THE FOURTH AND FIFTH COLUMNS PROVIDE THE
ADVANTAGES AND DISADVANTAGES OF EACH METHOD ,RESPECTIVELY . THE SIXTH COLUMN DESCRIBES THE WORKING PRINCIPLES ASSOCIATED WITH
EACH SUBCATEGORY . THE FIFTH COLUMN INDICATES THE RELEVANT REFERENCES CORRESPONDING TO EACH SUBCATEGORY .
Category Subcategory Working Principle Advantages Disadvantages References
N-shotDownscaled dataset
(ET)Training time reduced by training
on the down-sampled dataset.Easy to implementRanking mismatch on the
downscaled datasets[2], [18], [21], [32],
[77]–[82]
Downscaled model
(ET)Training time reduced by down-
scaling architectures.Save GPU memoryLow correlation between
original and proxy models[1], [2], [10]
Network morphism
(ET)Training time reduced by inheriting
weights from the parent model.Leverage existing
knowledgeFailed when child network
not exist in parent network[23]–[26], [83]–[85]
Learning curve
extrapolation (IT, ET)The performance is extrapolated
after only training for a few epochs.Easily combine with
various NAS algorithmsNeed trained network to
build the prediction models[2], [13], [28], [29],
[86], [87]
Few-shotPerformance predictor
(IT, ET)Use a regression model to directly
predict the performance of DNNsFast GPU-freed evalua-
tion speedNeed fully-trained architec-
tures to build the regression
model[28], [29], [31], [32],
[34], [37]–[44], [46],
[88]–[100]
Population memory
(IT, ET)Store the trained DNNs and di-
rectly match them when used.Loss-less evaluation per-
formanceLack of flexibility cannot
evaluate unseen architecture[12], [50], [51],
[101], [102]
One-shotPath-based
(ET)Only the supernet requires being
trained, where the weights of can-
didate architectures are from it.Fast, inference to get the
performance of searched
networkMulti-model forgetting prob-
lem deteriorates the predic-
tion ability[52]–[57], [103]–
[117]
Gradient-based
(ET)Jointly optimize the architecture
parameter and the supernet weights
by bilevel optimization.Effectively, suitable for
large-scale datasets(1)Huge memory; (2) Low
generalization; (3) Perfor-
mance collapse[9], [10], [59]–[61],
[65]–[75], [118]–
[136]
Zero-shotParameter-level
(ET)Aggregate the saliencies of all
parameters to measure the entire
DNN without training.Low computational
complexity Unreliability and inaccurate[137]–[139]
Architecture-level
(ET)Measure the properties positively
related to architecture performance
without training.Consider the global
characteristics of the
architectureInaccurate performance ranking[140]–[146]
for performance evaluation.
Literature examples: Many architecture-level methods
have been proposed to efficiently evaluate the performance
of the architecture, and we use the keywords “zero-shot,”
“scoring at initialization,” and “pruning at initialization” to
select them. For example, Mellor et al. [140], [141] proposed
to quantify the activation overlap of different inputs for a
network to score the untrained networks. The learnability of
architectures is more strong when the activation for different
inputs is well separated. They proposed an indicator based
on input Jacobian correlation in [140]. Furthermore, they also
devised using the Hamming distance to judge how dissimilar
the two inputs are in another version of this paper [141]. Xu et
al.[145] assumed that the gradients can be used to evaluate the
random-initialized networks. This is based on the observation
that gradients can directly decide the convergence and general-
ization results. They presented the gradient kernel to take each
layer as a basic unit. After that, the mean of the Gram matrix
for each layer was computed to score the network. Lin et
al.[142] averaged the Gaussian complexity of linear function
in each linear region to measure the network expressivity.
Zhou et al. [146] observed the synaptic diversity of multi-head
self-attention in Vision Transformer (ViT) affects the perfor-
mance notably. Thus, they proposed an indicator to rank ViT
architectures from the perspectives of synaptic diversity and
synaptic saliency. Chen et al. [143] measured the architecture
performance by analyzing the spectrum of the neural tangent
kernel and the number of linear regions in the input space. Shu
et al. [144] proposed an efficient approximation of the trace
norm of NTK to alleviate the prohibitive cost of computingNTK and estimate the performance of neural architectures.
Advantages: The architecture-level method can comprehen-
sively evaluate the global characteristics of the architecture,
such as hierarchy and connectivity. This helps capture the
impact of architecture on performance.
Disadvantages: The architecture-level methods are usually
motivated by some theoretic studies on neural networks.
They proposed indicators to judge the trainability, learnability,
generalization, or expressivity of the architectures and ranked
the architectures. Despite their efficiency, the architecture-level
methods always lead to inaccurate performance ranking. This
is because they only roughly measure the properties of archi-
tectures that are positively related to architecture performance.
In summary, the zero-shot methods can further reduce the
computational time compared with other methods. However,
the performance of the zero-shot method is usually not good
enough in practice. Moreover, the robustness of the zero-shot
methods can not be guaranteed. The performance fluctuated
dramatically among different tasks.
So far, all categorizations of EEMs have been described in
detail, and we have summarized these EEMs in Table. I. It is
worth noting that there is a method with more than one EEM,
also known as the hybrid method. For example, Zoph et al. [2]
combined a downsampled dataset and a downsampled model
to decrease the evaluation time. Based on this, Zoph et al. [2]
even added the learning curve extrapolation methods to further
improve the evaluation efficiency. In addition, Rawal et al. [28]
introduced a performance predictor with the early stopping
strategy to accelerate the evaluation process. Moreover, EEMs
also have the potential to be combined with each other, e.g.,15
performance predictors and population memory, and users
could use them in practice as they need.
IV. E VALUATION
In this section, we first discuss the evaluation metrics in Sec-
tion IV-A. Then, we introduce the commonly-used benchmark
datasets in Section IV-B. Finally, we report the performance
of various EEMs on the benchmark datasets in Section IV-C.
A. Evaluation Metrics
Based on the no-free-lunch theory, the EEMs may perform
inaccurately compared with TEM though their speed is much
faster. As a result, we mainly measure the EEMs in two folds:
runtime and prediction ability. The runtime of an EEM is
generally measured by GPU day (GPU day = The number of
GPUs×The number of days) [35]. As for the measurement of
prediction ability, the commonly used metrics can be mainly
divided into correlation-based, ranking-based, and top-based.
For the convenience of discussion, we denote the ground-truth
performance and the predicted performance of architectures
{Ai}N
i=1as{yi}N
iand{ˆyi}N
i, respectively. The real perfor-
mance ranking of the architecture Ai(i.e., the ranking of yi
among {yi}N
i) isri, while the predicted performance ranking
forAi(i.e., the ranking of ˆyiamong {ˆyi}N
i) isˆri.
The correlation-based metric mainly calculates the correla-
tion between the ground-truth performance and the predicted
performance. It mainly includes the Pearson coefficient and
the Coefficient of Determination (i.e., R2). The Pearson coef-
ficient is formulated by Equation (10):
Mpearson =PN
i=1(yi−¯y) 
ˆyi−¯ˆy
qPN
i=1(yi−¯y)2PN
i=1 
ˆyi−¯ˆy2(10)
where yiandˆyirepresent the ground-truth and predicted
performance, respectively. ¯yand¯ˆyrepresent the average
of all ground-truth performance and predicted performance,
respectively. The R2is formulated by Equation (11):
MR2= 1−PN
i=1(yi−ˆyi)2
PN
i=1(yi−¯yi)2(11)
For both metrics, the closer to one the value is, the more
accurate the EEM is.
The ranking-based metric calculates the correlation between
the real performance ranking and the predicted one. Kendall’s
Tau ranking correlation (KTau) [153] and Spearman’s ranking
correlation (SpearmanR) [154] are frequently-used ranking-
based metrics. The KTau is the relative difference of concor-
dant and discordant pairs, and is shown in Equation (12):
MKTau = 2×number of concordant pairs
N(N−1)/2−1 (12)
where the concordant pair means that the predicted rankings
and the ground-truth rankings of a given pair are the same.
On the other hand, the SpearmanR is calculated based on the
difference between the predicted ranking and the real ranking.
It is shown in Equation (13):
MSpearmanR = 1−6PN
i=1(ri−ˆri)2
N(N2−1)(13)For both KTau and SpearmanR, the value closer to one means
that the EEM is more accurate.
The top-based metric measures the ability of EEMs to
discover the best architecture, which is important for NAS.
The top-based metric is mainly composed of N@K and
Precision@K. In specific, N@K refers to the best ground-truth
ranking among the predicted top-K architectures. The value
of N@K is a positive integer, and greater than or equal to
one. A lower value of N@K means that the EEM is more
capable of finding the best architecture. The Precision@K
is the proportion of the ground-truth top-K architectures in
the predicted top-K proportion architectures. Different from
N@K, the value closer to one means the EEM is better at
discovering best architectures.
B. Benchmark Datasets
EEMs are often evaluated on public benchmark datasets
to perform fair comparisons. Specifically, the benchmark
datasets include a lot of architecture-performance pairs in
the specific search spaces. The architecture-performance pairs
in these benchmark datasets can be used to evaluate the
prediction accuracy of EEMs. The commonly used bench-
mark datasets includes NAS-Bench-101 [155], NAS-Bench-
201 [155], NAS-Bench-301 [156] for image classification,
NAS-Bench-NLP [157] for natural language processing, and
NAS-Bench-ASR [158] for automatic speech recognition.
These benchmark datasets all use the cell-based search
space. Specifically, they stack cells to form the architecture,
and only the structure of the cell can be searched. Each cell can
be treated as a DAG. Based on the position of the operation in
DAG, the cell-based search spaces can be divided into Opera-
tion on Node (OON) and Operation on Edge (OOE) search
spaces. Specifically, the node is regarded as the operation
while the edge is treated as the connection between operations
for OON. The edge represents the operation while the node
is regarded as the connection for OOE. We summarized the
mentioned benchmarks in Table. II.
C. Comparisons on Benchmark Datasets
In order to make audiences more intuitively compare the
performance of various EEMs, we collect the KTau of them
on NAS-Bench-101, NAS-Bench-201, and NAS-Bench-301.
Please note the results are collected from the original paper or
some papers that report the results of these EEMs. We do not
report the results of the downscaled model method, network
morphism, gradient-based method, and population memory.
For the the downscaled model method, we do not have
enough computing resources to re-evaluate the downscaled
models in these benchmark datasets. For network morphism,
gradient-based method, and population memory, they cannot
be used to accelerate all architectures in these benchmark
datasets. Specifically, network morphism can only be applied
to accelerate the networks morphed from the parent network.
The gradient-based method is also a search strategy. It can only
be used to accelerate the searched architectures. Performance
memory can only be applied to query the performance of archi-
tectures that have been evaluated. As a result, the experimental
results of these methods are not reported.16
TABLE II
THE CHARACTERISTICS OF NAS BENCHMARKS .|V|REFERS TO THE NUMBER OF NODES ,AND|E|DENOTES THE NUMBER OF EDGES . PLEASE NOTE
THAT NAS-B ENCH -301 PROVIDES 60,000 ANNOTATED ARCHITECTURES AND A PERFORMANCE PREDICTOR WHICH CAN PREDICT ALL 1018
ARCHITECTURE IN THE DARTS SEARCH SPACE .
Benchmark Year Task Dataset Search space OON/OOE #Architecture
NAS-Bench-101 2019 Image classification CIFAR-10 |V| ≤7,#ops=3, |E| ≤9 OON 423,624
NAS-Bench-201 2020 Image classificationCIFAR-10, CIFAR-100,
ImageNet-16-120|V|= 4,#ops=5 OOE 15,625
NAS-Bench-301 2020 Image classification CIFAR-10 |V| ≤7,#ops=7 OOE 60,000/ 1018
NAS-Bench-NLP 2020 Natural language processing Penn Tree Bank |V| ≤24,#ops=7 OOE 14,322
NAS-Bench-ASR 2021 Automatic Speech Recognization TIMIT audio dataset |V| ≤4,#ops=7, |E| ≤9 OOE 8,242
TABLE III
THEKENDALL ’STAU(KT AU)VALUE OF N-SHOT ,FEW-SHOT ,ONE-SHOT AND ZERO -SHOT METHODS ON NAS-B ENCH -101 (NB101), NAS-B ENCH -201
(NB201), AND NAS-B ENCH -301 (NB301). S PECIFICALLY ,THE FIRST COLUMN SHOWS THE CATEGORIES OF EEM S,AND THE SECOND COLUMN SHOWS
THE SUBCATEGORIES OF EACH CATEGORY . THE THIRD COLUMN SHOWS THE SPECIFIC METHOD ,AND THE FIFTH ,SIXTH AND SEVENTH COLUMNS SHOW
THE KTAU OF EACH SPECIFIC METHOD ON NB101, NB201 AND NB301 RESPECTIVELY . THE EIGHTH COLUMN IS THE EXPERIMENTAL NOTES FOR
OBTAINING KTAU VALUE .†DENOTES THAT THE BENCHMARK DOES NOT PROVIDE SUFFICIENT DATA TO PRODUCE THE EXPERIMENTS .‡INDICATES THE
CORE CODE OF THE METHOD IS NOT OPEN -SOURCE AND THE RESULTS IS ABSENCE IN THE ORIGINAL PAPER .∗INDICATES NB301 DOES NOT PROVIDE
THE REAL ACCURACY VALUE ,SO THE COMMUNITY OF PERFORMANCE PERFORMANCE WOULD NOT PROVIDE THE KTAU VALUE .
Category Subcategory Method NB101 NB201 NB301 Note
N-shotDownscaled datasetProxy dataset -†0.827 -†CIFAR-10 is proxy dataset, and ImageNet is original
dataset.†NB101 and NB201 only have CIFAR10 results.
Subset of dataset -†0.867 -†Use CIFAR-10 as the subset of CIFAR-100.†NB101 and
NB201 only provide CIFAR10 results.
Learning curve
extrapolationEarly stopping(1/4) -†0.602 0.614 Terminate training when the epoch is one quarter of the
fully trained epoch.†NB101 lacks 1/4 epoch results.
Early stopping(1/2) 0.438 0.663 0.662 Terminate training when the epoch is one half of the fully
trained epoch.
Few-shot Performance predictorNeuralPredictor [37] 0.679 0.646 -∗424 and 1,564 samples are used as the training data in
NAS-Bench-101 and NAS-Bench-201, respectively.
Peephole [34] 0.4556 -‡-∗424 samples are used as the training data.
E2EPP [46] 0.5038 0.7669 -∗424 samples are used as the training data.
ReNAS [38] 0.657 -‡-∗424 samples are used as the training data.
HOP [43] 0.813 0.897 -∗381 and 781 samples are used as the training data in NAS-
Bench-101 and NAS-Bench-201, respectively.
TNASP [97] 0.722 0.726 -∗424 and 1,564 samples are used as the training data in
NAS-Bench-101 and NAS-Bench-201, respectively.
One-shot Path-basedOSNAS [152] 0.446 0.744 0.548 Using MC sample in supernet training.
FairNAS [54] -‡0.706 0.527 -
Zero-shotParameter-levelsynflow [137] -0.063 0.573 0.201 -
grad norm [137] -0.276 0.401 0.070 -
snip [137] -0.206 0.402 0.050 -
grasp [137] -0.266 0.348 0.365 -
fisher [137] -0.202 0.362 -0.158 -
plain [137] 0.240 0.311 0.394 -
Architecture-leveljacob cov [140] 0.066 0.608 0.230 -
relu logdet [141] 0.290 0.611 0.539 -
The experimental results are presented in Table. III, and
we will discuss each-shot method sequentially. (1) N-shot
methods: These methods still need to evaluate all search
architectures, which typically consume more time compared
to others. Specifically, learning curve extrapolation methods
achieve inferior results than downscaled dataset methods. This
is because the early-stop learning curve cannot properly reflect
the final performance, while CIFAR10 has similar features as
ImageNet and is widely used as a proxy pair. In addition, the
performance of early stopping at 1/2 is superior to stopping
at 1/4. This means more training iterators can match the
final learning curve. (2) Few-shot methods: The performance
predictor needs additional initialization time, but it has the
best performance among all methods. Specifically, HOP [43]
achieves 0.813 and0.891 KTau values, which present the
optimal results on NB101 and NB201, respectively. (3) One-shot methods: These methods require evaluation time to get the
performance of a big supernet, and the path-bathed methods
can get similar results as the early stop strategy. (4) Zero-
shot methods: These methods are regarded as the most time-
saving ones because they do not rely on any GPU-based
evaluation process. However, this category of methods does
not perform as well as other categories. Since they evaluate
some statistical metrics, which cannot reflect the real accuracy
of the architecture. Note that parameter-level methods even get
the negative KTau value on NB101 and NB301.
The performance of each-shot methods is affected by var-
ious factors. Specifically, the similarity between the proxy
and original dataset and the representativity of the sub-dataset
can highly affect the performance of the downscaled dataset
method. In addition, the ability of the performance predictor
can be affected by the encoding strategy of the architecture and17
regression model. Moreover, the path sampling strategy would
influence the performance of the path-based methods. Finally,
as for the zero-shot method, their performance relies on the
designed scoring functions. If the function can precisely reflect
the real accuracy of the architecture, the zero-shot methods can
acquire great performance.
V. C HALLENGES AND FUTURE DIRECTIONS
In summary, current research efforts are primarily focused
on designing EEMs with high prediction accuracy and ef-
ficiency on some benchmark datasets like NAS-Bench-101,
NAS-Bench-201, and NAS-Bench-301, and have achieved
considerable success. Firstly, the performance of neural archi-
tectures in existing architecture benchmark datasets is obtained
on some small-scale datasets such as CIFAR10. These bench-
mark datasets cannot reflect the performance of EEMs in real-
world scenarios. Secondly, the current EEMs face difficulties
in achieving cross-domain prediction of neural architectures
across multiple different search spaces. This increases the time
complexity because the EEM has to be redesigned for each
specific search space. Moreover, the current EEMs lack the
capability to evaluate the performance of a neural architecture
on multiple datasets within a single search space. This has
increased the time required to build EEMs on multiple datasets
even for architectures within the same search space. Lastly, the
existing EEM methods also struggle with representing multiple
types of neural architectures uniformly, which affects their
ability to effectively mine valuable architecture data. In this
section, we will discuss these challenges in detail.
A. Effectiveness Validation
The current effectiveness validation for EEMs is not con-
vincing for most researchers. Specifically, the benchmarking
datasets are used for effectiveness validation and fair com-
parisons. However, the existing architecture dataset cannot
conform to the practical sceneries because of their small
scales. For example, NAS-Bench-101 only involves three types
of operations (i.e., convolution 1 ×1, convolution 3 ×3, and
max pooling). It limits the max number of nodes and edges
to seven and nine, respectively. The NAS-Bench-101 search
space is so small that it only consists of 423,624architectures.
NAS-Bench-201 only includes four different operations, and
the architecture only has four vertices. The NAS-Bench-201
search space only includes 15,625 architectures, which is even
smaller than NAS-Bench-101. In contrast, the search spaces
applied in real application scenarios, such as the search space
of NASNet, MobileNet, or Transformer, are more complex.
They are generally several orders of magnitude more than
NAS-Bench-101 and NAS-Bench-201 in quantity. For exam-
ple, the NASNet search space designs an operation set that
contains 15operations and five nodes excluding the input and
output nodes. The MobileNet search space, as a block-based
search space, contains multiple choices for each block. The
search space size is about 1039with a block size of five. As a
result, the results on the existing benchmarking datasets cannot
reflect the effect of the EEMs on real scenarios. There exists
the need to construct a larger architecture dataset to assist in
the validation of the proposed EEMs.B. Cross-domain Prediction
Cross-domain prediction mainly refers to predicting the
performance of the architectures in different search spaces.
In existing EEMs, only the downscaled dataset method, the
downscaled model method and population memory have the
ability of cross-domain prediction. This is because these
methods do not care about the domains of the architectures.
Other EEMs are specific to a target search space and lack
cross-domain ability. For example, the designed function-
preserving operations in network morphism only can be used
in a specific structure. If this structure is not included in
the architecture, the network morphism method cannot be
used. The performance predictors generally design encoding
methods and regression models for specific search spaces.
As a result, they cannot be applied or have poor results in
other search spaces. The one-shot method can only predict
the performance of the subnets contained in the supernet.
The cross-domain ability of zero-shot methods also cannot
be guaranteed because the measurement of the properties for
different types of architectures is different. This means that we
have to rebuild an EEM once the search space changes. This
is labor-intensive and computationally expensive. There are
works that explore the methods of cross-domain prediction.
For example, Han et al. [159] represented the candidate
CNNs as a computation graph that consists of only primi-
tive operators. Then, they proposed a semi-supervised graph
representation learning procedure to predict the architectures
from multiple families. However, the method is limited to the
cell-based search space. How to develop a cross-domain EEM
is still a challenging issue.
C. Multi-task Prediction
Multi-task prediction refers that EEMs require predicting
the multiple performance values in multiple tasks of the same
architecture. Moreover, there is an inner correlation between
the multiple labels of the same architecture. The multi-task
situation often appears in multi-task learning. Specifically,
multi-task learning leverages the useful information between
different but related tasks to improve the generalizability of
networks [160]. Multi-task learning has become a hot topic
because it can save computational overhead by applying one
network to multiple tasks. Furthermore, training one network
on multiple tasks can also improve generalization. In multi-
task learning, we need to estimate the performance of the
architectures on multiple tasks. Some researchers have ob-
served this problem and work on it. For example, Huang et
al.[89] embedded the tasks as a part of the input to achieve
the prediction of multiple labels. Shala et al. [161] proposed a
transferable performance predictor with deep-kernel Gaussian
processes to predict the performance architectures on different
datasets. To promote the development of multi-task prediction,
Duan et al. [162] proposed a benchmarking dataset (called
TransNAS-Bench-101). Specifically, TransNAS-Bench-101 in-
volves the performance of 51,464 architectures across seven
tasks such as classification, regression, pixel-level prediction,
and self-supervised tasks. However, the work on this is fairly
limited. With the success of multi-task learning in real-world18
deployment scenarios, we believe that the EEMs for multi-task
are a promising future research direction.
D. Uniform Representation
The performance of the architecture largely depends on
the design of the architecture. As a result, how to extract
meaningful information from the architectures is critical for
the prediction of NAS. To mine the architectures, we first
need to provide the representation method to describe the
architectures. Although many powerful representation methods
have been proposed, they can only represent the architecture in
a specific type of search space. For example, the commonly-
used adjacency matrix encodings [163] can only be used
to represent the cell-based architectures. This prevents the
researchers from learning knowledge of various architectures
at the same time. Furthermore, this is not conducive to
improving the transferability of the EEM. To overcome the
problem, Sun et al. [164] designed a unified text method
to describe the CNN. Concretely, it designed four units to
describe the detailed information of each layer in CNNs.
Moreover, it provided a unique order of layers to make the
topology information constant. However, this method cannot
describe other types of architectures in addition to CNN. The
research on the uniform expression for various architectures
is still in the early stage for the field of NAS.
VI. C ONCLUSION
This paper gives a comprehensive survey of EEMs. Specif-
ically, based on the number of architectures trained, we cate-
gorize the EEMs into the N-shot methods, few-shot methods,
one-shot methods, and zero-shot methods. The N-shot meth-
ods require training every searched architecture and mainly
consist of the downscaled dataset methods, downscaled model
methods, learning curve extrapolation methods, and network
morphisms. The few-shot methods only need to train a smaller
number of architectures than TEM. It mainly includes the
performance predictor and the population memory. The one-
shot methods merely require training one architecture, and
mainly include the path-based methods and the gradient meth-
ods. The zero-shot methods involve no training, thus further
reducing the runtime. They can be divided into the parameter-
level method and architecture-level method. Furthermore, we
review the evaluation metrics and benchmark datasets for
EEMs, and report the results on these benchmark datasets to
intuitively show the performance of various EEMs. Lastly, we
summarize the challenges and future research directions of
the existing EEMs. Specifically, the challenges mainly include
effectiveness validation, cross-domain prediction, multi-task
prediction, and uniform representation.
REFERENCES
[1] E. Real, A. Aggarwal, Y . Huang, and Q. V . Le, “Regularized evolution
for image classifier architecture search,” in Proc. the AAAI Conf. on
Artif. Intell. , vol. 33, no. 01, 2019, pp. 4780–4789.
[2] B. Zoph, V . Vasudevan, J. Shlens, and Q. V . Le, “Learning transferable
architectures for scalable image recognition,” in Proc. IEEE/CVF Conf.
Comput. Vis. Pattern Recognit. , 2018, pp. 8697–8710.[3] P. Ren, Y . Xiao, X. Chang, P.-Y . Huang, Z. Li, X. Chen, and X. Wang,
“A comprehensive survey of neural architecture search: Challenges and
solutions,” ACM Comput. Surv. , vol. 54, no. 4, pp. 1–34, 2021.
[4] T. Elsken, J. H. Metzen, and F. Hutter, “Neural architecture search: A
survey,” J. Mach. Learn. Res. , vol. 20, no. 1, pp. 1997–2017, 2019.
[5] Y . Liu, Y . Sun, B. Xue, M. Zhang, G. G. Yen, and K. C. Tan, “A survey
on evolutionary neural architecture search,” IEEE Trans. Neural Netw.
and Learn. Syst. , vol. 34, no. 2, pp. 550–570, 2023.
[6] E. Real, S. Moore, A. Selle, S. Saxena, Y . L. Suematsu, J. Tan, Q. V .
Le, and A. Kurakin, “Large-scale evolution of image classifiers,” in
Proc. Int. Conf. Mach. Learn. , vol. 70, 2017, pp. 2902–2911.
[7] L. Xie and A. Yuille, “Genetic CNN,” in Proc. IEEE/CVF Int. Conf.
Comput. Vis. , 2017, pp. 1379–1388.
[8] B. Zoph and Q. Le, “Neural architecture search with reinforcement
learning,” in Proc. Int. Conf. Learn. Represent. , 2016, [Online]. Avail-
able: https://openreview.net/pdf?id=r1Ue8Hcxg.
[9] M. Fil, B. Ru, C. Lyle, and Y . Gal, “DARTS without a validation set:
Optimizing the marginal likelihood,” arXiv preprint arXiv:2112.13023 ,
2021.
[10] H. Liu, K. Simonyan, and Y . Yang, “Darts: Differentiable architecture
search,” in Proc. Int. Conf. Learn. Represent. , 2018, [Online]. Avail-
able: https://openreview.net/pdf?id=S1eYHoC5FX.
[11] A. Krizhevsky and G. Hinton, “Learning multiple layers of features
from tiny images,” Syst. Autoimmun Dis. , vol. 1, no. 4, 2009.
[12] X. Xie, Y . Liu, Y . Sun, G. G. Yen, B. Xue, and M. Zhang,
“BenchENAS: A benchmarking platform for evolutionary neural archi-
tecture search,” IEEE Trans. Evol. Comput. , vol. 26, no. 6, pp. 1473–
1485, 2022.
[13] T. Domhan, J. T. Springenberg, and F. Hutter, “Speeding up automatic
hyperparameter optimization of deep neural networks by extrapolation
of learning curves,” in Proc. Int. Joint Conf. Artif. Intell. , no. 9, 2015,
pp. 3460–3468.
[14] D. Baymurzina, E. Golikov, and M. Burtsev, “A review of neural
architecture search,” Neurocomputing , vol. 474, pp. 82–93, 2022.
[15] S. Liu, H. Zhang, and Y . Jin, “A survey on computationally efficient
neural architecture search,” J. Autom. Intell. , vol. 1, no. 1, p. 100002,
2022.
[16] C. White, A. Zela, R. Ru, Y . Liu, and F. Hutter, “How powerful are
performance predictors in neural architecture search?” in Proc. Adv.
Neural Inf. Process. Syst. , vol. 34, 2021, pp. 28 454–28 469.
[17] Y . Wang, Q. Yao, J. T. Kwok, and L. M. Ni, “Generalizing from a
few examples: A survey on few-shot learning,” ACM Comput. Surv. ,
vol. 53, no. 3, pp. 1–34, 2020.
[18] P. Liu, M. D. El Basha, Y . Li, Y . Xiao, P. C. Sanelli, and R. Fang, “Deep
evolutionary networks with expedited genetic algorithms for medical
image denoising,” Med. Image Anal. , vol. 54, pp. 306–315, 2019.
[19] M. Park, “Data proxy generation for fast and efficient neural architec-
ture search,” arXiv preprint arXiv:1911.09322 , 2019.
[20] B. Na, J. Mok, H. Choe, and S. Yoon, “Accelerating neural architecture
search via proxy data,” in Proc. Int. Joint Conf. Artif. Intell. , 2021, pp.
2848–2854.
[21] P. Chrabaszcz, I. Loshchilov, and F. Hutter, “A downsampled variant
of ImageNet as an alternative to the CIFAR datasets,” arXiv preprint
arXiv:1707.08819 , 2017.
[22] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Ima-
geNet: A large-scale hierarchical image database,” in Proc. IEEE/CVF
Conf. Comput. Vis. Pattern Recognit. , 2009, pp. 248–255.
[23] T. Wei, C. Wang, Y . Rui, and C. W. Chen, “Network morphism,” in
Proc. Int. Conf. Mach. Learn. , 2016, pp. 564–572.
[24] H. Cai, T. Chen, W. Zhang, Y . Yu, and J. Wang, “Efficient architecture
search by network transformation,” in Proc. the AAAI Conf. on Artif.
Intell. , vol. 32, no. 1, 2018.
[25] H. Cai, J. Yang, W. Zhang, S. Han, and Y . Yu, “Path-level network
transformation for efficient architecture search,” in Proc. Int. Conf.
Mach. Learn. , 2018, pp. 678–687.
[26] T. Elsken, J. H. Metzen, and F. Hutter, “Efficient multi-objective
neural architecture search via lamarckian evolution,” arXiv preprint
arXiv:1804.09081 , 2018.
[27] J. N. v. Rijn, S. M. Abdulrahman, P. Brazdil, and J. Vanschoren, “Fast
algorithm selection using learning curves,” in Proc. Int. Symposium
Intell. Data Anal. , 2015, pp. 298–309.
[28] A. Rawal and R. Miikkulainen, “From nodes to networks: Evolving
recurrent neural networks,” arXiv preprint arXiv:1803.04439 , 2018.
[29] B. Baker, O. Gupta, R. Raskar, and N. Naik, “Accelerating neu-
ral architecture search using performance prediction,” arXiv preprint
arXiv:1705.10823 , 2017.19
[30] M. Wistuba and T. Pedapati, “Learning to rank learning curves,” in
Proc. Int. Conf. Mach. Learn. , 2020, pp. 10 303–10 312.
[31] X. Dai, A. Wan, P. Zhang, B. Wu, Z. He, Z. Wei, K. Chen, Y . Tian,
M. Yu, P. Vajda et al. , “FBNetV3: Joint architecture-recipe search using
predictor pretraining,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern
Recognit. , 2021, pp. 16 276–16 285.
[32] S. Hassantabar, X. Dai, and N. K. Jha, “CURIOUS: Efficient neural
architecture search based on a performance predictor and evolutionary
search,” IEEE Trans. Computer-Aided Design Integr. Circuits Syst. ,
vol. 41, no. 11, pp. 4975–4990, 2022.
[33] X. Ning, Y . Zheng, T. Zhao, Y . Wang, and H. Yang, “A generic graph-
based neural architecture encoding scheme for predictor-based nas,” in
Proc. Eur. Conf. Comput. Vis. , 2020, pp. 189–204.
[34] B. Deng, J. Yan, and D. Lin, “Peephole: Predicting network perfor-
mance before training,” arXiv preprint arXiv:1712.03351 , 2017.
[35] Y . Sun, B. Xue, M. Zhang, and G. G. Yen, “Completely automated
CNN architecture design based on blocks,” IEEE Trans. Neural Netw.
and Learn. Syst. , vol. 31, no. 4, pp. 1242–1254, 2019.
[36] R. Luo, F. Tian, T. Qin, E. Chen, and T.-Y . Liu, “Neural architecture
optimization,” in Proc. Adv. Neural Inf. Process. Syst. , vol. 31, 2018,
pp. 7816–7827.
[37] W. Wen, H. Liu, Y . Chen, H. Li, G. Bender, and P.-J. Kindermans,
“Neural predictor for neural architecture search,” in Proc. Eur. Conf.
Comput. Vis. , 2020, pp. 660–676.
[38] Y . Xu, Y . Wang, K. Han, Y . Tang, S. Jui, C. Xu, and C. Xu,
“ReNAS: Relativistic evaluation of neural architecture search,” in Proc.
IEEE/CVF Conf. Comput. Vis. Pattern Recognit. , 2021, pp. 4411–4420.
[39] R. Luo, X. Tan, R. Wang, T. Qin, E. Chen, and T.-Y . Liu, “Accuracy
prediction with non-neural model for neural architecture search,” arXiv
preprint arXiv:2007.04785 , 2020.
[40] R. Istrate, F. Scheidegger, G. Mariani, D. Nikolopoulos, C. Bekas, and
A. C. I. Malossi, “TAPAS: Train-less accuracy predictor for architecture
search,” in Proc. the AAAI Conf. on Artif. Intell. , vol. 33, no. 01, 2019,
pp. 3927–3934.
[41] C. Liu, B. Zoph, M. Neumann, J. Shlens, W. Hua, L.-J. Li, L. Fei-Fei,
A. Yuille, J. Huang, and K. Murphy, “Progressive neural architecture
search,” in Proc. Eur. Conf. Comput. Vis. , 2018, pp. 19–34.
[42] C. Wei, C. Niu, Y . Tang, Y . Wang, H. Hu, and J. Liang, “NPENAS:
Neural predictor guided evolution for neural architecture search,” IEEE
Trans. Neural Netw. and Learn. Syst. , vol. 34, no. 11, pp. 8441–8455,
2022.
[43] Z. Chen, Y . Zhan, B. Yu, M. Gong, and B. Du, “Not all operations
contribute equally: Hierarchical operation-adaptive predictor for neural
architecture search,” in Proc. IEEE/CVF Int. Conf. Comput. Vis. , 2021,
pp. 10 508–10 517.
[44] Y . Tang, Y . Wang, Y . Xu, H. Chen, B. Shi, C. Xu, C. Xu, Q. Tian, and
C. Xu, “A semi-supervised assessor of neural architectures,” in Proc.
IEEE/CVF Conf. Comput. Vis. Pattern Recognit. , 2020, pp. 1810–1819.
[45] R. Luo, X. Tan, R. Wang, T. Qin, E. Chen, and T.-Y . Liu, “Semi-
supervised neural architecture search,” in Proc. Adv. Neural Inf. Pro-
cess. Syst. , vol. 33, 2020, pp. 10 547–10 557.
[46] Y . Sun, H. Wang, B. Xue, Y . Jin, G. G. Yen, and M. Zhang, “Surrogate-
assisted evolutionary deep learning using an end-to-end random forest-
based performance predictor,” IEEE Trans. Evol. Comput. , vol. 24,
no. 2, pp. 350–364, 2019.
[47] C. Shorten and T. M. Khoshgoftaar, “A survey on image data augmen-
tation for deep learning,” J. Big Data , vol. 6, no. 1, pp. 1–48, 2019.
[48] L. Wang, Y . Zhao, Y . Jinnai, Y . Tian, and R. Fonseca, “Alphax:
Exploring neural architectures with deep neural networks and monte
carlo tree search,” arXiv preprint arXiv:1903.11059 , 2019.
[49] Y . Liu, Y . Tang, and Y . Sun, “Homogeneous architecture augmentation
for neural predictor,” in Proc. IEEE/CVF Int. Conf. Comput. Vis. , 2021,
pp. 12 249–12 258.
[50] S. Fujino, N. Mori, and K. Matsumoto, “Deep convolutional networks
for human sketches by means of the evolutionary deep learning,” in
Proc. Int. Fuzz. Syst. Assoc. Int. Conf. Soft Comput. Intell. Syst. , 2017,
pp. 1–5.
[51] Y . Sun, B. Xue, M. Zhang, G. G. Yen, and J. Lv, “Automatically
designing CNN architectures using the genetic algorithm for image
classification,” IEEE Trans. Cybern. , vol. 50, no. 9, pp. 3840–3854,
2020.
[52] G. Bender, P.-J. Kindermans, B. Zoph, V . Vasudevan, and Q. Le,
“Understanding and simplifying one-shot architecture search,” in Proc.
Int. Conf. Mach. Learn. , 2018, pp. 550–559.
[53] Z. Guo, X. Zhang, H. Mu, W. Heng, Z. Liu, Y . Wei, and J. Sun, “Single
path one-shot neural architecture search with uniform sampling,” in
Proc. Eur. Conf. Comput. Vis. , 2020, pp. 544–560.[54] X. Chu, B. Zhang, and R. Xu, “FairNAS: Rethinking evaluation fairness
of weight sharing neural architecture search,” in Proc. IEEE/CVF Int.
Conf. Comput. Vis. , 2021, pp. 12 239–12 248.
[55] M. Zhang, H. Li, S. Pan, T. Liu, and S. W. Su, “One-shot neural
architecture search via novelty driven sampling.” in Proc. Int. Joint
Conf. Artif. Intell. , 2020, pp. 3188–3194.
[56] S. You, T. Huang, M. Yang, F. Wang, C. Qian, and C. Zhang,
“GreedyNAS: Towards fast one-shot NAS with greedy supernet,” in
Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. , 2020, pp. 1999–
2008.
[57] M. Zhang, X. Yu, H. Zhao, and L. Ou, “Shiftnas: Improving one-shot
nas via probability shift,” in Proc. IEEE/CVF Int. Conf. Comput. Vis. ,
2023, pp. 5919–5928.
[58] Y . Benyahia, K. Yu, K. B. Smires, M. Jaggi, A. C. Davison, M. Salz-
mann, and C. Musat, “Overcoming multi-model forgetting,” in Proc.
Int. Conf. Mach. Learn. , 2019, pp. 594–603.
[59] M. Zhang, H. Li, S. Pan, X. Chang, C. Zhou, Z. Ge, and S. Su,
“One-shot neural architecture search: Maximising diversity to over-
come catastrophic forgetting,” IEEE Trans. Pattern Anal. Mach. Intell. ,
vol. 43, no. 9, pp. 2921–2935, 2020.
[60] H. Cai, L. Zhu, and S. Han, “ProxylessNAS: Direct neural
architecture search on target task and hardware,” in Proc.
Int. Conf. Learn. Represent. , 2018, [Online]. Available:
https://openreview.net/pdf?id=HylVB3AqYm.
[61] Y . Xu, L. Xie, X. Zhang, X. Chen, G.-J. Qi, Q. Tian, and H. Xiong,
“PC-DARTS: Partial channel connections for memory-efficient archi-
tecture search,” in Proc. Int. Conf. Learn. Represent. , 2019, [Online].
Available: https://openreview.net/forum?id=BJlS634tPr.
[62] Y . Xue and J. Qin, “Partial connection based on channel attention for
differentiable neural architecture search,” IEEE Trans. Ind. Inform. ,
vol. 19, no. 5, pp. 6804–6813, 2022.
[63] Y . Li, Z. Yang, Y . Wang, and C. Xu, “Adapting neural architectures
between domains,” in Proc. Adv. Neural Inf. Process. Syst. , vol. 33,
2020, pp. 789–798.
[64] L. Liu, Z. Wen, S. Liu, H.-Y . Zhou, H. Zhu, W. Xie, L. Shen, K. Ma,
and Y . Zheng, “Mixsearch: Searching for domain generalized medical
image segmentation architectures,” arXiv preprint arXiv:2102.13280 ,
2021.
[65] P. Ye, B. Li, Y . Li, T. Chen, J. Fan, and W. Ouyang, “Beta-DARTS:
Beta-decay regularization for differentiable architecture search,” arXiv
preprint arXiv:2203.01665 , 2022.
[66] X. Chen, L. Xie, J. Wu, and Q. Tian, “Progressive differentiable archi-
tecture search: Bridging the depth gap between search and evaluation,”
inProc. IEEE/CVF Int. Conf. Comput. Vis. , 2019, pp. 1294–1303.
[67] H. Liang, S. Zhang, J. Sun, X. He, W. Huang, K. Zhuang, and
Z. Li, “DARTS+: Improved differentiable architecture search with early
stopping,” arXiv preprint arXiv:1909.06035 , 2019.
[68] T. E. Arber Zela, T. Saikia, Y . Marrakchi, T. Brox, and F. Hutter,
“Understanding and robustifying differentiable architecture search,”
inProc. Int. Conf. Learn. Represent. , 2020, [Online]. Available:
https://openreview.net/forum?id=H1gDNyrKDS.
[69] X. Chen and C.-J. Hsieh, “Stabilizing differentiable architecture search
via perturbation-based regularization,” in Proc. Int. Conf. Mach. Learn. ,
2020, pp. 1554–1565.
[70] S. Movahedi, M. Adabinejad, A. Imani, A. Keshavarz, M. De-
hghani, A. Shakery, and B. N. Araabi, “ Λ-DARTS: Mitigating per-
formance collapse by harmonizing operation selection among cells,”
inProc. Int. Conf. Learn. Represent. , 2023, [Online]. Available:
https://openreview.net/forum?id=oztkQizr3kk.
[71] X. Chu, X. Wang, B. Zhang, S. Lu, X. Wei, and J. Yan, “DARTS-
: Robustly stepping out of performance collapse without indicators,”
arXiv preprint arXiv:2009.01027 , 2020.
[72] X. Chu, T. Zhou, B. Zhang, and J. Li, “Fair DARTS: Eliminating unfair
advantages in differentiable architecture search,” in Proc. Eur. Conf.
Comput. Vis. , 2020, pp. 465–480.
[73] W. Hong, G. Li, W. Zhang, R. Tang, Y . Wang, Z. Li, and Y . Yu,
“DropNAS: Grouped operation dropout for differentiable architecture
search,” arXiv preprint arXiv:2201.11679 , 2022.
[74] Y .-C. Gu, L.-J. Wang, Y . Liu, Y . Yang, Y .-H. Wu, S.-P. Lu, and M.-
M. Cheng, “Dots: Decoupling operation and topology in differentiable
architecture search,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern
Recognit. , 2021, pp. 12 311–12 320.
[75] X. Zhang, Y . Li, X. Zhang, Y . Wang, and J. Sun, “Differentiable
architecture search with random features,” in Proc. IEEE/CVF Conf.
Comput. Vis. Pattern Recognit. , 2023, pp. 16 060–16 069.20
[76] A. Yang, P. M. Esperanc ¸a, and F. M. Carlucci, “NAS evaluation
is frustratingly hard,” in Proc. Int. Conf. Learn. Represent. , 2019,
[Online]. Available: https://openreview.net/forum?id=HygrdpVKvr.
[77] A. Klein, S. Falkner, S. Bartels, P. Hennig, and F. Hutter, “Fast bayesian
optimization of machine learning hyperparameters on large datasets,”
inProc. Artif. Intell. Statist. , 2017, pp. 528–536.
[78] H. Shu and Y . Wang, “Automatically searching for U-Net image
translator architecture,” arXiv preprint arXiv:2002.11581 , 2020.
[79] D. Sapra and A. D. Pimentel, “Constrained evolutionary piecemeal
training to design convolutional neural networks,” in Proc. Int. Conf.
Ind., Eng. and Other Appli. of Appl. Intell. Syst. , 2020, pp. 709–721.
[80] B. Wang, B. Xue, and M. Zhang, “Particle swarm optimisation for
evolving deep neural networks for image classification by evolving
and stacking transferable blocks,” in Proc. IEEE Evol. Comput. , 2020,
pp. 1–8.
[81] Y . Xu, L. Xie, W. Dai, X. Zhang, X. Chen, G.-J. Qi, H. Xiong, and
Q. Tian, “Partially-connected neural architecture search for reduced
computational redundancy,” IEEE Trans. Pattern Anal. Mach. Intell. ,
vol. 43, no. 9, pp. 2953–2970, 2021.
[82] J. Liu, M. Gong, Q. Miao, X. Wang, and H. Li, “Structure learning
for deep neural networks based on multiobjective optimization,” IEEE
Trans. Neural Netw. and Learn. Syst. , vol. 29, no. 6, pp. 2450–2463,
2017.
[83] T. Chen, I. Goodfellow, and J. Shlens, “Net2net: Accelerating learning
via knowledge transfer,” arXiv preprint arXiv:1511.05641 , 2015.
[84] T. Elsken, J.-H. Metzen, and F. Hutter, “Simple and efficient ar-
chitecture search for convolutional neural networks,” arXiv preprint
arXiv:1711.04528 , 2017.
[85] Z. Zhang, Y . Chen, and C. Zhou, “Self-growing binary activation
network: A novel deep learning model with dynamic architecture,”
IEEE Trans. Neural Netw. and Learn. Syst. , vol. 35, no. 1, pp. 624–633,
2022.
[86] A. Klein, S. Falkner, J. T. Springenberg, and F. Hutter,
“Learning curve prediction with bayesian neural networks,” in
Proc. Int. Conf. Learn. Represent. , 2016, [Online]. Available:
https://openreview.net/forum?id=S11KBYclx.
[87] Y . Kim, W. J. Yun, Y . K. Lee, and J. Kim, “Two-stage architectural fine-
tuning with neural architecture search using early-stopping in image
classification,” arXiv preprint arXiv:2202.08604 , 2022.
[88] Y . Sun, X. Sun, Y . Fang, G. G. Yen, and Y . Liu, “A novel training
protocol for performance predictors of evolutionary neural architecture
search algorithms,” IEEE Trans. Evol. Comput. , vol. 25, no. 3, pp.
524–536, 2021.
[89] M. Huang, Z. Huang, C. Li, X. Chen, H. Xu, Z. Li, and
X. Liang, “Arch-graph: Acyclic architecture relation predictor for task-
transferable neural architecture search,” in Proc. IEEE/CVF Conf.
Comput. Vis. Pattern Recognit. , 2022, pp. 11 881–11 891.
[90] C. White, W. Neiswanger, and Y . Savani, “Bananas: Bayesian optimiza-
tion with neural architectures for neural architecture search,” in Proc.
the AAAI Conf. on Artif. Intell. , vol. 35, no. 12, 2021, pp. 10 293–
10 301.
[91] L. Dudziak, T. Chau, M. Abdelfattah, R. Lee, H. Kim, and N. Lane,
“Brp-NAS: Prediction-based nas using gcns,” in Proc. Adv. Neural Inf.
Process. Syst. , vol. 33, 2020, pp. 10 480–10 490.
[92] Y . Chen, Y . Guo, Q. Chen, M. Li, W. Zeng, Y . Wang, and M. Tan,
“Contrastive neural architecture search with neural architecture com-
parators,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. ,
2021, pp. 9502–9511.
[93] L. Mauch, S. Tiedemann, J. A. Garcia, B. N. Cong, K. Yoshiyama,
F. Cardinaux, and T. Kemp, “Efficient sampling for predictor-based
neural architecture search,” arXiv preprint arXiv:2011.12043 , 2020.
[94] C.-H. Liu, Y .-S. Han, Y .-Y . Sung, Y . Lee, H.-Y . Chiang, and K.-C.
Wu, “FOX-NAS: Fast, on-device and explainable neural architecture
search,” in Proc. IEEE/CVF Int. Conf. Comput. Vis. , 2021, pp. 789–
797.
[95] Y . Li, C. Hao, P. Li, J. Xiong, and D. Chen, “Generic neural architecture
search via regression,” in Proc. Adv. Neural Inf. Process. Syst. , vol. 34,
2021, pp. 20 476–20 490.
[96] Z. Li, T. Xi, J. Deng, G. Zhang, S. Wen, and R. He, “GP-NAS: Gaussian
process based neural architecture search,” in Proc. IEEE/CVF Conf.
Comput. Vis. Pattern Recognit. , 2020, pp. 11 933–11 942.
[97] S. Lu, J. Li, J. Tan, S. Yang, and J. Liu, “TNASP: A transformer-based
NAS predictor with a self-evolution framework,” in Proc. Adv. Neural
Inf. Process. Syst. , vol. 34, 2021, pp. 15 125–15 137.
[98] Y . Peng, A. Song, V . Ciesielski, H. M. Fayek, and X. Chang, “Pre-
nas: Predictor-assisted evolutionary neural architecture search,” arXiv
preprint arXiv:2204.12726 , 2022.[99] R. Wang, X. Chen, M. Cheng, X. Tang, and C.-J. Hsieh, “Rank-
nosh: Efficient predictor-based architecture search via non-uniform
successive halving,” in Proc. IEEE/CVF Int. Conf. Comput. Vis. , 2021,
pp. 10 377–10 386.
[100] J. Wu, X. Dai, D. Chen, Y . Chen, M. Liu, Y . Yu, Z. Wang, Z. Liu,
M. Chen, and L. Yuan, “Stronger NAS with weaker predictors,” in Proc.
Adv. Neural Inf. Process. Syst. , vol. 34, 2021, pp. 28 904–28 918.
[101] F. M. Johner and J. Wassner, “Efficient evolutionary architecture search
for CNN optimization on gtsrb,” in IEEE Proc. Int. Conf. Mach. Learn.
And Applications , 2019, pp. 56–61.
[102] E. Miahi, S. A. Mirroshandel, and A. Nasr, “Genetic neural architecture
search for automatic assessment of human sperm images,” Exp. Syst.
Appl. , vol. 188, p. 115937, 2022.
[103] Y . Chen, T. Yang, X. Zhang, G. Meng, X. Xiao, and J. Sun, “DetNAS:
Backbone search for object detection,” in Proc. Adv. Neural Inf.
Process. Syst. , vol. 32, 2019, pp. 6642–6652.
[104] H. Peng, H. Du, H. Yu, Q. Li, J. Liao, and J. Fu, “Cream of the crop:
Distilling prioritized paths for one-shot neural architecture search,” in
Proc. Adv. Neural Inf. Process. Syst. , vol. 33, 2020, pp. 17 955–17 964.
[105] S.-Y . Huang and W.-T. Chu, “Searching by generating: Flexible and
efficient one-shot NAS with architecture generator,” in Proc. IEEE/CVF
Conf. Comput. Vis. Pattern Recognit. , 2021, pp. 983–992.
[106] X. Xia, X. Xiao, X. Wang, and M. Zheng, “Progressive automatic
design of search space for one-shot neural architecture search,” in Proc.
IEEE/CVF Winter Conf. Appl. Comput. Vis. , 2022, pp. 2455–2464.
[107] R. Guo, C. Lin, C. Li, K. Tian, M. Sun, L. Sheng, and J. Yan, “Powering
one-shot topological NAS with stabilized share-parameter proxy,” in
Proc. Eur. Conf. Comput. Vis. , 2020, pp. 625–641.
[108] T. Liang, Y . Wang, Z. Tang, G. Hu, and H. Ling, “OpaNAS: One-
shot path aggregation network architecture search for object detection,”
inProc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. , 2021, pp.
10 195–10 203.
[109] M. Chen, J. Fu, and H. Ling, “One-shot neural ensemble architecture
search by diversity-guided search space shrinking,” in Proc. IEEE/CVF
Conf. Comput. Vis. Pattern Recognit. , 2021, pp. 16 530–16 539.
[110] X. Dong and Y . Yang, “One-shot neural architecture search via self-
evaluated template network,” in Proc. IEEE/CVF Int. Conf. Comput.
Vis., 2019, pp. 3681–3690.
[111] H. Cai, C. Gan, T. Wang, Z. Zhang, and S. Han, “Once for
all: Train one network and specialize it for efficient deployment,”
inProc. Int. Conf. Learn. Represent. , 2020, [Online]. Available:
https://openreview.net/forum?id=HylxE1HKwS.
[112] B. Yan, H. Peng, K. Wu, D. Wang, J. Fu, and H. Lu, “Lighttrack:
Finding lightweight neural networks for object tracking via one-shot
architecture search,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern
Recognit. , 2021, pp. 15 180–15 189.
[113] O. T.-C. Chen, Y . C. Zhang, Y .-X. Chang, and Y .-L. Chang, “Iterative
multiple-path one-shot NAS for the optimized performance,” in IEEE
Int. Symp. Circuits Syst. , 2021, pp. 1–5.
[114] X. Li, C. Lin, C. Li, M. Sun, W. Wu, J. Yan, and W. Ouyang,
“Improving one-shot NAS by suppressing the posterior fading,” in
Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. , 2020, pp.
13 836–13 845.
[115] H. Shi, R. Pi, H. Xu, Z. Li, J. Kwok, and T. Zhang, “Bridging the gap
between sample-based and one-shot neural architecture search with
bonas,” in Proc. Adv. Neural Inf. Process. Syst. , vol. 33, 2020, pp.
1808–1819.
[116] D. Wang, M. Li, C. Gong, and V . Chandra, “Attentivenas: Improving
neural architecture search via attentive sampling,” in Proc. IEEE/CVF
Conf. Comput. Vis. Pattern Recognit. , 2021, pp. 6418–6427.
[117] K. Liu, R. Ding, Z. Zou, L. Wang, and W. Tang, “A comprehensive
study of weight sharing in graph networks for 3d human pose estima-
tion,” in Proc. Eur. Conf. Comput. Vis. , 2020, pp. 318–334.
[118] X. Wang, J. Lin, J. Zhao, X. Yang, and J. Yan, “EAutoDet: Efficient
architecture search for object detection,” in Proc. Eur. Conf. Comput.
Vis., 2022, pp. 668–684.
[119] K. Xu and G. He, “DNAS: A decoupled global neural architecture
search method,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recog-
nit., 2022, pp. 1979–1985.
[120] H. Yu, H. Peng, Y . Huang, J. Fu, H. Du, L. Wang, and H. Ling, “Cyclic
differentiable architecture search,” IEEE Trans. Pattern Anal. Mach.
Intell. , vol. 45, no. 1, pp. 211–228, 2022.
[121] M. Zhang, S. Pan, X. Chang, S. Su, J. Hu, G. R. Haffari, and B. Yang,
“BaLeNAS: Differentiable architecture search via the bayesian learning
rule,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. , 2022,
pp. 11 871–11 880.21
[122] T. Huang, S. You, F. Wang, C. Qian, C. Zhang, X. Wang, and C. Xu,
“GreedyNASv2: Greedier search with a greedy path filter,” in Proc.
IEEE/CVF Conf. Comput. Vis. Pattern Recognit. , 2022, pp. 11 902–
11 911.
[123] X. Dong and Y . Yang, “Searching for a robust neural architecture
in four GPU hours,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern
Recognit. , 2019, pp. 1761–1770.
[124] M. Zhang, H. Li, S. Pan, X. Chang, and S. Su, “Overcoming multi-
model forgetting in one-shot NAS with diversity maximization,” in
Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. , 2020, pp. 7809–
7818.
[125] J. Pan, C. Sun, Y . Zhou, Y . Zhang, and C. Li, “Distribution consistent
neural architecture search,” in Proc. IEEE/CVF Conf. Comput. Vis.
Pattern Recognit. , 2022, pp. 10 884–10 893.
[126] Y . Akimoto, S. Shirakawa, N. Yoshinari, K. Uchida, S. Saito, and
K. Nishida, “Adaptive stochastic natural gradient method for one-shot
neural architecture search,” in Proc. Int. Conf. Mach. Learn. , 2019, pp.
171–180.
[127] Q. Zhou, X. Zheng, L. Cao, B. Zhong, T. Xi, G. Zhang, E. Ding, M. Xu,
and R. Ji, “EC-DARTS: Inducing equalized and consistent optimization
into DARTS,” in Proc. IEEE/CVF Int. Conf. Comput. Vis. , 2021, pp.
11 986–11 995.
[128] B. Wu, X. Dai, P. Zhang, Y . Wang, F. Sun, Y . Wu, Y . Tian, P. Vajda,
Y . Jia, and K. Keutzer, “FBNet: Hardware-aware efficient convnet
design via differentiable neural architecture search,” in Proc. IEEE/CVF
Conf. Comput. Vis. Pattern Recognit. , 2019, pp. 10 734–10 742.
[129] A. Wan, X. Dai, P. Zhang, Z. He, Y . Tian, S. Xie, B. Wu, M. Yu,
T. Xu, K. Chen et al. , “FBNetV2: Differentiable neural architecture
search for spatial and channel dimensions,” in Proc. IEEE/CVF Conf.
Comput. Vis. Pattern Recognit. , 2020, pp. 12 965–12 974.
[130] N. Nayman, Y . Aflalo, A. Noy, and L. Zelnik, “HardCoRe-NAS: Hard
constrained differentiable neural architecture search,” in Proc. Int. Conf.
Mach. Learn. , 2021, pp. 7979–7990.
[131] M. Zhang, S. W. Su, S. Pan, X. Chang, E. M. Abbasnejad, and
R. Haffari, “iDARTS: Differentiable architecture search with stochastic
implicit gradients,” in Proc. Int. Conf. Mach. Learn. , 2021, pp. 12 557–
12 566.
[132] S. Xue, R. Wang, B. Zhang, T. Wang, G. Guo, and D. Doermann,
“IDARTS: Interactive differentiable architecture search,” in Proc.
IEEE/CVF Int. Conf. Comput. Vis. , 2021, pp. 1163–1172.
[133] S. Tang, M. S. Hosseini, L. Chen, S. Varma, C. Rowsell, S. Damask-
inos, K. N. Plataniotis, and Z. Wang, “Probeable DARTS with ap-
plication to computational pathology,” in Proc. IEEE/CVF Int. Conf.
Comput. Vis. , 2021, pp. 572–581.
[134] C. Xue, X. Wang, J. Yan, Y . Hu, X. Yang, and K. Sun, “Rethinking
bi-level optimization in neural architecture search: A gibbs sampling
perspective,” in Proc. the AAAI Conf. on Artif. Intell. , vol. 35, no. 12,
2021, pp. 10 551–10 559.
[135] P. Hou, Y . Jin, and Y . Chen, “Single-DARTS: Towards stable archi-
tecture search,” in Proc. IEEE/CVF Int. Conf. Comput. Vis. , 2021, pp.
373–382.
[136] Y . Li, Z. Wen, Y . Wang, and C. Xu, “One-shot graph neural architecture
search with dynamic search space,” in Proc. the AAAI Conf. on Artif.
Intell. , vol. 35, no. 10, 2021, pp. 8510–8517.
[137] M. S. Abdelfattah, A. Mehrotra, Ł. Dudziak, and N. D.
Lane, “Zero-cost proxies for lightweight NAS,” in Proc.
Int. Conf. Learn. Represent. , 2020, [Online]. Available:
https://openreview.net/forum?id=0cmMMy8J5q.
[138] Y . Shu, Z. Dai, Z. Wu, and B. K. H. Low, “Unifying and boosting
gradient-based training-free neural architecture search,” arXiv preprint
arXiv:2201.09785 , 2022.
[139] G. Li, Y . Yang, K. Bhardwaj, and R. Marculescu, “Zico: Zero-
shot nas via inverse coefficient of variation on gradients,” in
Proc. Int. Conf. Learn. Represent. , 2023, [Online]. Available:
https://openreview.net/forum?id=rwo-ls5GqGn.
[140] J. Mellor, J. Turner, A. Storkey, and E. J. Crowley, “Neural architecture
search without training,” arXiv preprint arXiv:2006.04647v1 , 2020.
[141] ——, “Neural architecture search without training,” in Proc. Int. Conf.
Mach. Learn. , 2021, pp. 7588–7598.
[142] M. Lin, P. Wang, Z. Sun, H. Chen, X. Sun, Q. Qian, H. Li, and R. Jin,
“Zen-NAS: A zero-shot NAS for high-performance image recognition,”
inProc. IEEE/CVF Int. Conf. Comput. Vis. , 2021, pp. 347–356.
[143] W. Chen, X. Gong, and Z. Wang, “Neural architecture search on
ImageNet in four GPU hours: A theoretically inspired perspective,”
inProc. Int. Conf. Learn. Represent. , 2021, [Online]. Available:
https://openreview.net/forum?id=Cnon5ezMHtu.[144] Y . Shu, S. Cai, Z. Dai, B. C. Ooi, and B. K. H. Low, “NASI:
Label-and data-agnostic neural architecture search at initialization,”
inProc. Int. Conf. Learn. Represent. , 2021, [Online]. Available:
https://openreview.net/forum?id=v-v1cpNNK v.
[145] J. Xu, L. Zhao, J. Lin, R. Gao, X. Sun, and H. Yang, “KNAS: Green
neural architecture search,” in Proc. Int. Conf. Mach. Learn. , 2021, pp.
11 613–11 625.
[146] Q. Zhou, K. Sheng, X. Zheng, K. Li, X. Sun, Y . Tian, J. Chen, and R. Ji,
“Training-free transformer architecture search,” in Proc. IEEE/CVF
Conf. Comput. Vis. Pattern Recognit. , 2022, pp. 10 894–10 903.
[147] N. Lee, T. Ajanthan, and P. H. Torr, “Snip: Single-shot
network pruning based on connection sensitivity,” in Proc.
Int. Conf. Learn. Represent. , 2019, [Online]. Available:
https://openreview.net/forum?id=B1VZqjAcYX.
[148] C. Wang, G. Zhang, and R. Grosse, “Picking winning
tickets before training by preserving gradient flow,” in Proc.
Int. Conf. Learn. Represent. , 2019, [Online]. Available:
https://openreview.net/forum?id=SkgsACVKPH.
[149] L. Theis, I. Korshunova, A. Tejani, and F. Husz ´ar, “Faster gaze
prediction with dense networks and fisher pruning,” arXiv preprint
arXiv:1801.05787 , 2018.
[150] J. Turner, E. J. Crowley, M. O’Boyle, A. Storkey, and G. Gray,
“Blockswap: Fisher-guided block substitution for network compression
on a budget,” in Proc. Int. Conf. Learn. Represent. , 2020, [Online].
Available: https://openreview.net/forum?id=SklkDkSFPB.
[151] H. Tanaka, D. Kunin, D. L. Yamins, and S. Ganguli, “Pruning neural
networks without any data by iteratively conserving synaptic flow,” in
Proc. Adv. Neural Inf. Process. Syst. , vol. 33, 2020, pp. 6377–6389.
[152] X. Ning, C. Tang, W. Li, Z. Zhou, S. Liang, H. Yang, and Y . Wang,
“Evaluating efficient performance estimators of neural architectures,” in
Proc. Adv. Neural Inf. Process. Syst. , vol. 34, 2021, pp. 12 265–12 277.
[153] P. K. Sen, “Estimates of the regression coefficient based on kendall’s
tau,” J. Am. Stat. Assoc. , vol. 63, no. 324, pp. 1379–1389, 1968.
[154] J. Hauke and T. Kossowski, “Comparison of values of pearson’s and
spearman’s correlation coefficients on the same sets of data,” Quaest.
Geogr. , vol. 30, no. 2, pp. 87–93, 2011.
[155] C. Ying, A. Klein, E. Christiansen, E. Real, K. Murphy, and F. Hutter,
“NAS-Bench-101: Towards reproducible neural architecture search,” in
Proc. Int. Conf. Mach. Learn. , 2019, pp. 7105–7114.
[156] J. Siems, L. Zimmer, A. Zela, J. Lukasik, M. Keuper, and F. Hutter,
“NAS-Bench-301 and the case for surrogate benchmarks for neural
architecture search,” arXiv preprint arXiv:2008.09777 , 2020.
[157] N. Klyuchnikov, I. Trofimov, E. Artemova, M. Salnikov, M. Fedorov,
A. Filippov, and E. Burnaev, “NAS-Bench-NLP: Neural architecture
search benchmark for natural language processing,” IEEE Access ,
vol. 10, pp. 45 736–45 747, 2022.
[158] A. Mehrotra, A. G. C. Ramos, S. Bhattacharya, Ł. Dudziak, R. Vip-
perla, T. Chau, M. S. Abdelfattah, S. Ishtiaq, and N. D. Lane,
“NAS-Bench-ASR: Reproducible neural architecture search for speech
recognition,” in Proc. Int. Conf. Learn. Represent. , 2020, [Online].
Available: https://openreview.net/forum?id=CU0APx9LMaL.
[159] F. X. Han, K. G. Mills, F. Chudak, P. Riahi, M. Salameh, J. Zhang,
W. Lu, S. Jui, and D. Niu, “A general-purpose transferable predictor
for neural architecture search,” in Proc. SIAM Int. Conf. Data Min. ,
2023, pp. 721–729.
[160] Y . Zhang and Q. Yang, “A survey on multi-task learning,” IEEE Trans.
Knowl. Data Eng. , vol. 34, no. 12, pp. 5586–5609, 2022.
[161] G. Shala, T. Elsken, F. Hutter, and J. Grabocka, “Transfer
nas with meta-learned bayesian surrogates,” in Proc.
Int. Conf. Learn. Represent. , 2022, [Online]. Available:
https://openreview.net/forum?id=paGvsrl4Ntr.
[162] Y . Duan, X. Chen, H. Xu, Z. Chen, X. Liang, T. Zhang, and Z. Li,
“TransNAS-Bench-101: Improving transferability and generalizability
of cross-task neural architecture search,” in Proc. IEEE/CVF Conf.
Comput. Vis. Pattern Recognit. , 2021, pp. 5251–5260.
[163] C. White, W. Neiswanger, S. Nolen, and Y . Savani, “A study on
encodings for neural architecture search,” in Proc. Adv. Neural Inf.
Process. Syst. , vol. 33, 2020, pp. 20 309–20 319.
[164] Y . Sun, G. G. Yen, B. Xue, M. Zhang, and J. Lv, “ArcText: A unified
text approach to describing convolutional neural network architectures,”
IEEE Trans. Artif. Intell. , vol. 3, no. 4, pp. 526–540, 2022.