Enhancing Contrastive Learning with
Efficient Combinatorial Positive Pairing
Jaeill Kim1∗, Duhun Hwang1∗, Eunjung Lee3†,
Jangwon Suh1,Jimyeong Kim1,Wonjong Rhee1,2
1Department of Intelligence and Information, Seoul National University,
2Interdisciplinary Program in Artificial Intelligence (IPAI), Seoul National University,
3Artificial Intelligence Cardiovascular Innovation Research, Mayo Clinic
jaeill0704@snu.ac.kr ,yelobean@snu.ac.kr ,lee.eunjung@mayo.edu ,
rxwe5607@snu.ac.kr ,wlaud1001@snu.ac.kr ,wrhee@snu.ac.kr
Abstract
In the past few years, contrastive learning has played a central role for the success
of visual unsupervised representation learning. Around the same time, high-
performance non-contrastive learning methods have been developed as well. While
most of the works utilize only two views, we carefully review the existing multi-
view methods and propose a general multi-view strategy that can improve learning
speed and performance of any contrastive or non-contrastive method. We first
analyze CMC’s full-graph paradigm and empirically show that the learning speed
ofK-views can be increased by KC2times for small learning rate and early
training. Then, we upgrade CMC’s full-graph by mixing views created by a
crop-only augmentation, adopting small-size views as in SwA V multi-crop, and
modifying the negative sampling. The resulting multi-view strategy is called
ECPP (Efficient Combinatorial Positive Pairing). We investigate the effectiveness
of ECPP by applying it to SimCLR and assessing the linear evaluation performance
for CIFAR-10 and ImageNet-100. For each benchmark, we achieve a state-of-the-
art performance. In case of ImageNet-100, ECPP boosted SimCLR outperforms
supervised learning.
1 Introduction
The early development of visual unsupervised representation learning was typically based on pretext
learning techniques applied to a single-view [ 10,11,30,32]. Recent works have achieved an
impressive advancement based on contrastive and non-contrastive learning techniques, where typically
multi-views are adopted instead of a single-view. Most of the multi-view works, however, are limited
in that they focus on two views only. Among the very few works that consider more than two views,
CMC [ 38] and SwA V [ 3] are the most widely known methods. A summary of multi-view methods
with more than two views is provided in Table 1.
CMC [ 38] investigates the classic hypothesis that a powerful representation is one that models
view-invariant factors, and studies the framework of multi-view contrastive learning. Because
they specifically consider the settings that require more than two views, they have extended the
two-view framework to a general multiple-view framework. For handling more than two views,
they suggest core view paradigm and full graph paradigm. Between the two, full graph paradigm
considers combinatorial pairing of Kviews. Following CMC, SimCLR [ 4] was developed where
*Equal contribution.
†Work performed while at Seoul National University.
This research was conducted in 2022, and the content of this version is the same as in 2022.arXiv:2401.05730v1  [cs.CV]  11 Jan 2024Table 1: Summary of positive pairing strategies. Kviews of an image are generated by a stochastic
augmentation function. As in the full graph strategy of CMC [ 38], ECPP utilizes the maximum
number of positive pairs, KC2=K(K−1)
2. As in the multi-crop of SwA V [ 3], ECPP reduces
the computational burden by employing small additional views. ECPP adopts three additional
improvement techniques that are explained in Section 4. Note that the computational burden is
proportional to the number and size of the views.
Method Augmentation design LossNumber of positive
pairsComputation
Baseline (2 views)
 LV1,V2 N 2N
CMC (core view)
P
1<j≤KLV1,Vj (K−1)N KN
CMC (full graph)
P
1≤i<KP
i<j≤KLVi,Vj K(K−1)
2NKN
Multi-crop
P
1≤i≤2P
i<j≤KLVi,Vj (2K−3)N2N+ (K−2)N·small image size
large image size
ECPP×K(Ours)
P
1≤i<KP
i<j≤KLVi,Vj K(K−1)
2N 2N+ (K−2)N·small image size
large image size
self-augmentation of an image is used for generating multiple views. In SimCLR and most of
the following works, use of two views turns out to be simple and powerful as long as a proper
augmentation function is chosen. The core view and full graph paradigms of CMC, however, have
been mostly forgotten and researchers have been focusing on two-view methods.
SwA V [ 3] is another important prior-art that utilizes more than two views. SwA V does not require
pairwise comparisons, but instead enforces consistency between cluster assignments produced for
different augmentation views of the same image. They also propose a new data augmentation strategy,
multi-crop , that uses a mix of views with different resolutions in place of full-resolution views for all.
This has the advantage of minimizing the memory and computation overheads that are inevitably
created by the additional views. The main purpose of introducing multi-views in SwA V is to have
many views that are known to belong to the same cluster. The design of two full-resolution views with
many small additional views allows an increase in the number of views while incurring a relatively
small increase in the memory and computation overheads. The two full-resolution views are created
with the standard SimCLR augmentation, and the small additional views are created with a slight
modification in the crop setting. Multi-crop only considers pairing between one of global views and
one of small views, and therefore utilizes less pairs than the combinatorial pairing of CMC’s full
graph. As can be seen in Table 1, CMC’s full graph generates O(K2)of positive pairs and SwA V’s
multi-crop generates O(K)of positive pairs. Recently, ReLICv2 [ 39] has adopted multi-crop from
SwA V and surpassed the supervised learning performance of the ImageNet-1K classification task.
In this work, we focus on generalizing and improving the previous works on multi-views. To be
specific, we first focus on understanding the fundamental benefits of K-views and then we develop
an efficient method called Efficient Combinatorial Positive Pairing (ECPP) for fully and efficiently
utilizing K-views. ECPP can be combined with any existing contrastive learning or non-contrastive
learning algorithm as long as the base algorithm utilizes positive pairing. In our study, we focus
on applying ECPP with SimCLR as the base algorithm because of SimCLR’s simplicity and broad
applicability. As for the notation, we denote SimCLR combined with K-view ECPP as SimCLR×K.
Similarly, we denote BYOL combined with K-view ECPP as BYOL×K.
We first focus on understanding a fundamental benefit of K-views. In CMC [ 38], it is explained that
the enhanced performance, that improves as Kincreases, can be attributed to the Mutual Informa-
tion (MI) maximization between views and to the information minimization that discards nuisance
factors that are not shared across the views. The explanations, however, have been challenged
in [20,41] for a few reasons including the fact that unsupervised contrastive learning can be success-
fully performed with an invertible network that has no effect on mutual information. Success of many
2non-contrastive methods that are not based on InfoNCE (or any other information-theoretic loss) is
another reason to suspect the MI based explanations. Instead of the MI explanations, we investigate if
the increase in the number of positive pairing terms can provide a simple yet fundamental explanation.
Empirically, we show that the performance improvement can be purely dependent on the number of
positive pairing terms shown in Table 1. Then, the result has an implication that CMC full graph and
ECPP can be exactly KC2=K(K−1)
2times faster in learning when compared to a 2-view baseline.
Figure 1: The number of positive pairs (equivalent
to the number of loss terms LVi,Vj) processed by
multi-view representation learning frameworks.With a better understanding on K-views, we
build up on the previously developed methods
of CMC’s full graph and SwA V’s multi-crop.
The design of ECPP starts by combining the
K(K−1)
2benefit of CMC with the memory and
computation efficiency of multi-crop. As the
result, ECPP can process more positive pair-
ings than any other methods for a given amount
of computation budget as shown in Figure 1.
As we will show later, the number of loss terms
LVi,Vjthat are applied during the learning is
equal to the number of positive pairs. To fur-
ther improve the performance and complete
the design of ECPP, we carefully investigate
the choice of augmentations for the small ad-
ditional views, global view vs. local view, and
negative sampling scheme.
2 Related works
The recent progress in unsupervised representation learning is primarily driven by improved loss
design, efficient optimization, intensive investigations on augmentation design, and selective negative
sampling.
Loss design: In most cases, a loss is designed to learn invariant representation while preventing
representation collapse. To learn invariant representation, most methods maximize the cosine sim-
ilarity between the two embeddings of a positive pair [ 4,7,16] or cosine similarity’s equivalents
such as normalized dot product [ 13], alignment [ 43], and tolerance [ 42]. Barlow Twins [ 50] max-
imizes correlation, VICReg [ 2] maximizes covariance, and SwA V [ 3] maximizes cross entropies
between two positive embeddings. A naive implementation of learning invariant representation can
end up with a representation collapse. Prevention of representation collapse can be achieved with
negative samples [ 4,16], equal clustering constraint [ 3], stop gradient with predictor [ 7,13], feature
de-correlation [2, 50], or uniformity among features on a unit hypersphere [43].
Optimization improvement: It has been widely observed that the quality of unsupervised represen-
tation learning benefits from longer training, large batch sizes, and larger models [ 2–5,7,13,16,50].
Therefore, a substantial effort has been made to deal with the associated computational challenges.
When training with multiple GPUs via distributed data parallelism [ 24], it has been found that smaller
working batch sizes of batch normalization (BN) can be a problem [ 46] and should be converted to
synchronized cross-GPU batch normalization (SyncBN) [ 51]. To explicitly reduce the computing
FLOPs, mixed precision training [ 27] has become a default choice. Training with a mix of small
and large size images can be helpful for improving the performance [ 40]. For a faster convergence,
a large learning rate with cyclical learning rate scheduling can improve speed and performance of
the training [ 35,36]. Following such works, use of a large learning rate with cosine learning rate
decay [ 25] has been widely adopted for accelerating the rate of convergence. Also, linear learning
rate scaling rule with a warm-up [ 12] and Layer-wise Adaptive Rate Scaling (LARS) [ 49] optimizer
have been adopted to resolve the unstable optimization difficulties when training with large learning
rates and large batch sizes [ 26]. In our work, we utilize combinatorial positive pairing of multiple
views to improve the computational efficiency of the optimization.
Augmentation design: Most of the recent methods rely on two views generated by a carefully
chosen augmentation function. Augmentation function design was heavily studied when developing
SimCLR [ 4], and an excellent stochastic composition was suggested where cropping with resizing,
3horizontal flipping, color jittering, gray scaling, and Gaussian blurring are utilized. The SimCLR
augmentation has become the base setting for many subsequent works [ 2,3,7,8,13,39,50],
where slight modifications such as crop size/scale adjustment and inclusion of solarization/saliency-
masking [ 29] have been considered. For more than two views, however, hardly anything has been
studied regarding multi-view specific aspects. [ 3] is the only study where the base two views are
expanded by small additional views in its multi-crop. The additional views are created by cropping
small parts in the anchor image. In our work, we investigate the augmentation design for the additional
views when the first two views are generated with SimCLR augmentation.
Selective negative sampling: In contrastive learning, the characteristics of negative examples have
been found to be influential to the representation learning. A hard negative is a negative example
whose embedding is similar to the embedding of the anchor example. Using hard negatives for
learning was found to be beneficial for deep metric learning [ 14,19,31,37,44,48,53] and also
for contrastive learning [ 21,33,42,45]. A false negative is a negative example whose contextual
information match the anchor’s contextual information. Using false negatives for learning was
found to be harmful and should be removed for contrastive learning [ 9,18,33]. For contrastive
learning, however, it is generally impossible to distinguish false negatives from hard negatives in an
unsupervised manner [ 33]. In our work, we show that the multiple views of an anchor image can be
interpreted as false negatives and removing them can be beneficial for multi-view contrastive learning.
3 A fundamental benefit of exploiting more than two views
As explained in Section 1, ECPP can be combined with any representation learning algorithm that
relies on positive pairing through instance augmentation. In this section, we investigate SimCLR as
the base algorithm and a vanilla version of SimCLR×Kas its K-view extension. For SimCLR×K,
we show how the number of positive pairings can affect the performance and speed of learning. We
first define the loss functions.
3.1 Loss of SimCLR ( LSimCLR )
Given an encoding network f(·), a projection network g(·), stochastic augmentation functions t∼ T
andt′∼ T′, and a mini-batch of Nexamples, SimCLR generates Npairs of embedding vectors. For
an example xn, the embedding vectors are zn(=g◦f◦t(xn))andz′
n(=g◦f◦t′(xn)). Therefore,
we end up with 2Nembeddings for the mini-batch. The loss function of SimCLR for an ordered
positive pair (zn, z′
n)can be described as
ℓ(zn,z′n)=−logexp(sim(zn, z′
n)/τ)PN
m=11[m̸=n]exp(sim(zn, zm)/τ) +PN
m=1exp(sim(zn, z′m)/τ), (1)
where τdenotes the temperature parameter and sim(zn, z′
n)is the cosine similarity that is equivalent
to the normalized dot product. Note that ℓ(zn,z′n)̸=ℓ(z′n,zn). Finally, the losses of all 2Nembeddings
are summed to form the SimCLR loss as below.
LSimCLR =NX
n=1ℓ(zn,z′n)+ℓ(z′n,zn) (2)
3.2 Loss of SimCLR with K-views (vanilla version; LVanilla
SimCLR×K)
SimCLR is based on two views that are generated by a stochastic augmentation function. We can
name the two views as V1andV2. To reflect the views in the notation, we can slightly modify the
notation LSimCLR in Eq. (2)intoLV1,V2
SimCLR . With this updated notation, the loss of SimCLR with
K-views can be defined as below.
LVanilla
SimCLR×K=X
1≤i<j≤KLVi,Vj
SimCLR (3)
Strictly speaking, the loss in Eq. (3)is simply LSimCLR extended with K-views and CMC’s full
graph paradigm. Therefore, it is different from the full ECPP implementation ( LSimCLR×K) that
requires additional modifications that will be explained in Section 4. Therefore, we denote the loss as
LVanilla
SimCLR×Kinstead of LSimCLR×K. In this section, we want to focus on the effect of combinatorial
4(a) Epoch
 (b) Number of positive pairs
Figure 2: Linear evaluation performance of ResNet-18 with 2, 4, and 8 views. (a) SimCLR with more
views learns faster. In fact, SimCLR with K-views learns KC2times faster for any given iteration
number. (b) By adjusting the X-axis to the number of processed positive pairs (i.e., the number of
LVi,Vj
SimCLR terms used for back-propagation), it can be seen that the learning speed is about the same as
long as the processed number of positive pairs is the same.
positive pairing alone without being affected by the other modifications of ECPP, and thus investigate
LVanilla
SimCLR×Kinstead of LSimCLR×K. With this caution, we provide the results of empirical investigations
in the following Section 3.3.
3.3 Analysis on the speed and performance of learning
For convolutional neural networks, training with batch size of Nforksteps has been shown to
perform about the same as the training with batch size of N·kfor one step [ 22,49]. Here, we
investigate if a similar result can be obtained for the K-view loss in Eq. (3). As summarized in Table 1
and as can be checked by comparing Eq. (2)and Eq. (3), learning with K-views can be considered as
learning with KC2times more examples. This can be verified by confirming that the loss in Eq. (3)is
formed by summing KC2ofLVi,Vj
SimCLR terms. Note that KC2corresponds to the number of positive
paring that is possible with Kviews.
To verify if the learning speed improvement is indeed KC2, we performed a contrastive learning
using the loss in Eq. (3)and the results are shown in Figure 2. At the end of each epoch of contrastive
learning, we have performed a linear evaluation to assess the quality of the learned representations.
To decouple other learning effects, we used a constant learning rate of 0.0004 without any learning
rate scheduling such as warm-up and learning rate decay. We have also used the full-resolution with
SimCLR augmentations for all Kviews. Therefore, the learning speed improvement is due to the
use of combinatorial positive pairing alone. From Figure 2(a), the performance improvement for the
same epoch can be confirmed. From Figure 2(b), the speed improvement of KC2can be confirmed.
Surprisingly, the learning speed increase of exploiting Kviews is quite large. The speed increase
comes at the cost of processing more views. The computational burden increases linearly with K
while the speed benefit increases with KC2.
The full ECPP implementation utilizes additional techniques such as the standard acceleration with
large learning rate with learning rate scheduling, small additional views, and a loss modification to
handle negative pairing. With the additional techniques, the learning dynamics become sophisticated
and we will provide a further analysis in Section 6.
4 Efficient combinatorial positive pairing (ECPP)
4.1 Designing augmentation of the additional views
[4,28] noted that crop plays a crucial role when designing augmentations. To confirm the previous
findings and quantitatively investigate the importance of each individual augmentation, we performed
a new ablation study for CIFAR-10 and ImageNet-100. The results are shown in Table 2 where crop
has the largest impact on the linear evaluation performance.
ForK-view ECPP, the original two views are kept intact by applying the original augmentation
scheme. For the additional views, however, there is no clear reason to apply the same augmentation
scheme. It might be better to apply heterogeneous augmentation schemes to increase the diversity in
learning. Motivated by the superiority of crop, we investigate mixing SimCLR augmentation with a
new augmentation scheme named crop-only augmentation. The results are shown in Table 3. Based
on the results, we design ECPP to apply SimCLR augmentation to the half of the views and crop-only
augmentation to the other half.
5Table 2: Effectiveness of each augmentation technique. For the baseline 2-view SimCLR on ResNet-
50, we measure linear evaluation performance (top-1% validation accuracy). The first set in the table
shows how much change is observed by excluding a technique from the default augmentation of
SimCLR. The second set in the table shows the same for including only a single augmentation but
nothing else. Crop is most sensitive in both sets, indicating that it is the most influential.
Augmentation CIFAR-10 ImageNet-100
Baseline augmentation 89.88 78.66
(except) Crop 67.10 40.56
(except) Flip 89.64 76.94
(except) ColorJitter 85.14 74.40
(except) GrayScale 86.90 77.20
(except) GaussianBlur - 78.82
(except) Solarization - 78.20
No augmentation
(include) Crop 74.23 50.10
(include) Flip 25.22 5.76
(include) ColorJitter 55.99 23.98
(include) GrayScale 22.86 4.70
(include) GaussianBlur - 6.48
(include) Solarization - 7.92
Table 3: The effect of mixed views created by applying the standard SimCLR augmentation or
crop-only augmentation. Top-1% linear evaluation results for ResNet-50 and 100 epochs are shown.
Mixing views helps when compared to using SimCLR views only.
Augmentation design Dataset
Total number of views Number of SimCLR views Number of crop-only views CIFAR-10 ImageNet-100
44 0 91.94 83.14
3 1 92.27 83.50
2 2 93.07 84.46
66 0 92.40 84.30
5 1 93.02 84.68
4 2 93.05 83.90
3 3 93.50 84.30
2 4 93.39 85.10
88 0 93.00 84.18
7 1 93.13 84.04
6 2 93.55 84.24
5 3 93.29 84.90
4 4 93.55 84.62
3 5 93.72 84.56
2 6 93.17 83.86
4.2 Applying small views of multi-crop to the additional views for efficiency
The downside of using K-views is in the computational side. By creating additional views, the
amount of forward computation is increased from 2Nfor the base algorithm (e.g., SimCLR) to
KN for the ECPP combined algorithm (e.g., SimCLR×K). The increase in computational burden
is by a factor ofKN
2N=K
2. Multi-crop in SwA V [ 3] wisely mitigates this burden by choosing a
smaller image size for the additional views while keeping the size of the original two views intact.
By reducing each of height and width of an image by a factor of r, the reduction in image’s area
becomes a factor of r2.
We explored the effect of image size and the hyperparameters of crop, and the results are shown in
Table 4. As for the size, we choose small views as in the multi-crop. Then, the computational burden
Table 4: The effect of controlling the size of K−2additional views and the hyperparameters of the
K
2crop views. For ECPP, we choose small views not only for the computational efficiency but also
for a better performance. Also, we choose the crop scale of [0.20, 1.00] (global views are created)
instead of the multi-crop’s [0.05, 0.14] (local views are created). Top-1% linear evaluation results for
ResNet-50 on ImageNet-100 with 100 epochs are shown.
Crop design and size of the additional views Pairing strategy 4-views 6-views 8-views
Default large view [0.20, 1.00] (224 ×224) KC2(baseline) 83.56 83.88 83.86
Local small view [0.05, 0.14] (96 ×96)2K−3(multi-crop) 82.28 84.00 84.26
KC2 81.16 82.52 82.44
Global small view [0.20, 1.00] (96 ×96)2K−3 83.02 84.06 84.44
KC2(ECPP) 84.12 84.30 84.58
6is reduced from KN to2N+ (K−2)N·small image size
large image size. For ImageNet,small image size
large image size=962
2242= 0.18
which is 82% reduction in the overhead that is created by the additional views. In fact, the use of
small views allows a better performance than the baseline. Related to this, it has been already reported
that training with a mix of small size images and full size images in multi-crop can improve the
classification performance in [40].
As for the hyperparameters of crop, we depart from multi-crop’s local view design and use global
views that cover almost the entire image using a weak crop scale and a strong resize. Using local
views caused performance degradation in our study, especially when combinatorial positive pairing
(KC2) is used. This is most likely due to the reduced probability of the views sharing the same
contextual information.
4.3 A slight modification on negative sampling
The loss of ECPP is almost the same as the loss in Eq. (3), except for one modification. For the loss
termℓ(zn,z′n)in Eq. (1),znserves as the anchor. While znis not included as a negative sample in the
denominator, z′
nis kept according to Eq. (1). However, z′
nis a self-augmented version of zn, and we
find it helpful to remove it from the set of negative samples as shown below.
ℓ′
(zn,z′n)=−logexp(sim(zn, z′
n)/τ)PN
m=11[m̸=n](exp(sim(zn, zm)/τ) +exp(sim(zn, z′m)/τ))(4)
In conclusion, our final design of ECPP loss, LSimCLR×K, is the same as LVanilla
SimCLR×Kexcept that
ℓ(zn,z′n)is replaced with ℓ′
(zn,z′n). Performance comparison results are shown in Table 5 where the
use of ℓ′
(zn,z′n)always performs better.
Table 5: Comparison between the loss LVanilla
SimCLR×Kin Eq. (3)and ECPP’s loss LSimCLR×K. ECPP
uses a slightly difference loss where ℓ′
(zn,z′n)in Eq. (4)is used instead of ℓ(zn,z′n)in Eq. (1). Top-1%
linear evaluation performance is shown for networks trained for 100 epochs.
CIFAR-10 ImageNet-100
ResNet-18 ResNet-50 ResNet-18 ResNet-50
Loss 2-views 4-views 2-views 4-views 2-views 4-views 2-views 4-views
ℓ(Eq.(1)) 85.87 89.43 86.31 91.88 73.44 76.62 77.28 82.68
ℓ′(Eq.(4)) 86.12 89.78 87.20 91.94 73.78 77.18 78.30 83.56
5 Experimental results
We pre-train multi-view representation learning models without labels by implementing the same
architecture and training protocols as in SimCLR [ 4] with three datasets, CIFAR-10 [ 23], ImageNet-
100 (the subset of randomly chosen 100 classes of ImageNet-1K as in [ 38]), and ImageNet-1K [ 34].
The default baseline augmentation sets for CIFAR-10 and ImageNet-1K (and ImageNet-100) follow
the standard SimCLR augmentation sets for CIFAR-10 and ImageNet-1K, respectively. For ImageNet-
1K (and ImageNet-100), solarization is applied as the final step. Unless otherwise stated, the default
batch size is 64 and the base encoding network is the standard ResNet-50 [ 15] where the representation
is the output of the penultimate layer of 2048 dimensions. When training with CIFAR-10, we make
the standard modifications to the encoding network for tiny images where the first convolution layer
is replaced with 3x3 Conv of stride 1 and the first maxpool layer is removed. When evaluating the
models, we follow the standard linear evaluation practice [ 1,3,4,13,28] where the linear classifier
on top of the frozen pre-trained model is trained with training dataset in a supervised manner and the
classification performance is evaluated with the validation dataset. A complete set of implementation
details are provided in Appendix A.
5.1 Comparison with prior arts
Table 6 shows the comparison results of the linear evaluation. SimCLR×Knot only outperforms
the other methods for CIFAR-10 and ImageNet-100 but also surpasses the supervised baseline for
ImageNet-100. For ImageNet-1K, SimCLR×Kimproves the performance of SimCLR baseline by
6.8%. All of our results in the table, for all three datasets, were obtained with a single hyperparameter
setting (see Appendix A). For ImageNet-1K, it should be possible to improve the performance of
SimCLR×Kwith an adequate amount of tuning.
7Table 6: Comparison between the best known prior arts and SimCLR combined with K-view ECPP
(i.e., SimCLR×K). Top-1% linear evaluation of classification is shown. All of our ECPP results are
obtained with a single hyperparameter setting (see Appendix A). For ImageNet-1K, a proper tuning
of hyperparameters will very likely improve the performance of SimCLR×K. For ImageNet-100,
SimCLR×4outperforms the supervised learning performance.
(a) CIFAR-10
Method Epoch Top-1
Supervised 95.0
Decoupled NT-Xent [6] 800 94.0
SWD [6] 800 94.1
SimCLR×2800 93.9
SimCLR×4800 94.4
SimCLR×6800 94.3
SimCLR×8200 94.4(b) ImageNet-100
Method Epoch Top-1
Supervised 86.2
Align Uniform [43] 240 74.6
CMC (K=1) [52] 200 75.8
CMC (K=4) [52] 200 78.8
CACR(K=1) [52] 200 79.4
CACR(K=4) [52] 200 80.5
LooC++ [47] 500 82.2
MoCo-v2+MoCHi [21] 800 84.5
SimCLR×2800 84.5
SimCLR×4800 87.0
SimCLR×6400 86.6
SimCLR×8100 84.6
SimCLR×8200 85.6(c) ImageNet-1K
Method Epoch Top-1
Supervised 76.5
InstDisc [17] 200 58.5
LocalAgg [17] 200 58.8
MoCo [17] 200 60.8
CPC v2 [17] 200 63.8
Shuffled-DBN [17] 200 65.2
MoCo v2 [17] 200 67.5
PCL v2 [17] 200 67.6
PIC [17] 200 67.6
MoCHi [17] 200 68.0
AdCo [17] 200 68.6
SwA V [17] 200 72.7
SimCLR [4] 200 64.3
SimCLR×6200 71.1
5.2 The effect of training length (maximum epoch value)
For a more careful analysis of ECPP, we have evaluated 4 different views with 4 different training
lengths, and the results are shown in Figure 3. In (a), it can be seen that 2-views and 4-views continue
to improve their performance even at 800 epochs. But, the performance saturates earlier for 6-views
and 8-views. When the same results are plotted with the number of positive pairs as the X-axis, a
strong pattern can be observed. For any number of views, the best performance seems to occur in
the red dotted box area in Figure 3(b). This indicates that it can be harmful to perform unsupervised
learning for too long. Additional discussions are provided in Section 6.2.
(a) Maximum epoch configuration
 (b) Number of positive pairs
Figure 3: The effect of maximum epoch value. Because of the cosine learning rate decay [ 25], we
have evaluated the SimCLR×Kperformance for a range of maximum epoch configurations. Each
point in the plot corresponds to an independent evaluation. Results for ImageNet-100.
6 Discussion
6.1 Applying ECPP to non-contrastive learning (BYOL)
ECPP can also be combined with any non-contrastive learning algorithm as long as a positive pairing
with instance augmentation is used. BYOL [ 13] is a very popular non-contrastive model, and we
have combined ECPP with BYOL to demonstrate ECPP’s broad applicability. In Figure 4, it can be
confirmed that BYOL×Kperforms in a similar manner as the SimCLR×Kin Figure 2. Performance
evaluation results are shown in Table 7, and it can be confirmed that performance can be improved by
using K-views.
6.2 Performance and learning speed for a full training
In Section 3.3, we have shown in Figure 2 that the learning speed of ECPP is improved by KC2
times when the learning rate is a small constant and only combinatorial positive pairing is applied.
Obviously, the speed and performance gain in Figure 2 is not maintained when cosine learning rate
8(a) Epoch
 (b) Number of positive pairs
Figure 4: Linear evaluation performance of ResNet-18 with 2, 4, and 8 for BYOL×K. The figures
are generated in the same way as in Figure 2.
Table 7: Top-1% linear evaluation of classification for BYOL×Kwith ResNet-50.
(a) CIFAR-10
Method Epoch Performance
BYOL×2100 85.0
BYOL×4100 89.0
BYOL×6100 92.0
BYOL×8100 93.3(b) ImageNet-100
Method Epoch Performance
BYOL×2100 75.8
BYOL×4100 81.9
BYOL×6100 81.8
BYOL×8100 81.7
decay and other enhancement techniques are applied to ECPP for a full training. As an evidence, the
performance curves in Figure 3(b) do not overlap as much as in the performance curves in Figure 2(b).
We investigated an actual full training of ECPP and the results are shown in Figure 5. Note that
this is different from Figure 3 because we are showing a performance curve of a single full training
for each K. It can be confirmed from Figure 5(a) that larger Kis still helpful for a given number
of epoch. The results in Figure 5(b), however, show that the order is reversed, i.e., smaller views
perform better for a given number of positive pairs. Most likely, this reduction in efficiency is due to
the correlations between the loss terms. With more views, some of the loss terms in the set {LVi,Vj}
inevitably become correlated and can reduce the speed gain. It is also interesting to note another
limitation of ECPP shown in Figure 3 – the performance does not improve forever, and it peaks
around a certain number of positive pairs.
(a) Epochs
 (b) Number of positive pairs
Figure 5: Linear evaluation performance of SimCLR×Kfor ResNet-50 on CIFAR-10 with 2, 4, 6,
and 8 views. The figure (a) and (b) are generated in the similar way as in Figure 2.
7 Conclusion
In this work, we carefully study the fundamental benefits of K-views and propose Efficient Com-
binatorial Positive Pairing (ECPP), a simple add-on method that can enhance the learning speed
and efficiency of contrastive learning and non-contrastive learning methods. While contrastive and
non-contrastive learning is widely adopted for a variety of unsupervised representation learning tasks,
our experiments are limited to the vision tasks. Our method can train a high-performance network
with a relatively small computation. This property can be helpful to the research community.
9Acknowledgements
This work was supported by a National Research Foundation of Korea (NRF) grant funded by the
Korea government (MSIT) (No. NRF-2020R1A2C2007139).
References
[1]Guillaume Alain and Yoshua Bengio. Understanding intermediate layers using linear classifier
probes. arXiv preprint arXiv:1610.01644 , 2016.
[2]Adrien Bardes, Jean Ponce, and Yann LeCun. Vicreg: Variance-invariance-covariance regular-
ization for self-supervised learning. arXiv preprint arXiv:2105.04906 , 2021.
[3]Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin.
Unsupervised learning of visual features by contrasting cluster assignments. Advances in Neural
Information Processing Systems , 33:9912–9924, 2020.
[4]Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework
for contrastive learning of visual representations. In International conference on machine
learning , pages 1597–1607. PMLR, 2020.
[5]Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey E Hinton. Big
self-supervised models are strong semi-supervised learners. Advances in neural information
processing systems , 33:22243–22255, 2020.
[6]Ting Chen, Calvin Luo, and Lala Li. Intriguing properties of contrastive losses. Advances in
Neural Information Processing Systems , 34, 2021.
[7]Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 15750–15758,
2021.
[8]Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum
contrastive learning. arXiv preprint arXiv:2003.04297 , 2020.
[9]Ching-Yao Chuang, Joshua Robinson, Yen-Chen Lin, Antonio Torralba, and Stefanie Jegelka.
Debiased contrastive learning. Advances in neural information processing systems , 33:8765–
8775, 2020.
[10] Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised visual representation learning
by context prediction. In Proceedings of the IEEE international conference on computer vision ,
pages 1422–1430, 2015.
[11] Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by
predicting image rotations. arXiv preprint arXiv:1803.07728 , 2018.
[12] Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola,
Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training
imagenet in 1 hour. arXiv preprint arXiv:1706.02677 , 2017.
[13] Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena
Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar,
et al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in Neural
Information Processing Systems , 33:21271–21284, 2020.
[14] Ben Harwood, Vijay Kumar BG, Gustavo Carneiro, Ian Reid, and Tom Drummond. Smart
mining for deep metric learning. In Proceedings of the IEEE International Conference on
Computer Vision , pages 2821–2829, 2017.
[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition ,
pages 770–778, 2016.
10[16] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for
unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition , pages 9729–9738, 2020.
[17] Tianyu Hua, Wenxiao Wang, Zihui Xue, Sucheng Ren, Yue Wang, and Hang Zhao. On
feature decorrelation in self-supervised learning. In Proceedings of the IEEE/CVF International
Conference on Computer Vision , pages 9598–9608, 2021.
[18] Tri Huynh, Simon Kornblith, Matthew R Walter, Michael Maire, and Maryam Khademi.
Boosting contrastive self-supervised learning with false negative cancellation. In Proceedings of
the IEEE/CVF Winter Conference on Applications of Computer Vision , pages 2785–2795, 2022.
[19] Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, and Ond ˇrej Chum. Mining on manifolds: Metric
learning without labels. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition , pages 7642–7651, 2018.
[20] Jörn-Henrik Jacobsen, Arnold Smeulders, and Edouard Oyallon. i-revnet: Deep invertible
networks. arXiv preprint arXiv:1802.07088 , 2018.
[21] Yannis Kalantidis, Mert Bulent Sariyildiz, Noe Pion, Philippe Weinzaepfel, and Diane Larlus.
Hard negative mixing for contrastive learning. Advances in Neural Information Processing
Systems , 33:21798–21809, 2020.
[22] Alex Krizhevsky. One weird trick for parallelizing convolutional neural networks. arXiv
preprint arXiv:1404.5997 , 2014.
[23] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009.
[24] Shen Li, Yanli Zhao, Rohan Varma, Omkar Salpekar, Pieter Noordhuis, Teng Li, Adam Paszke,
Jeff Smith, Brian Vaughan, Pritam Damania, et al. Pytorch distributed: Experiences on
accelerating data parallel training. arXiv preprint arXiv:2006.15704 , 2020.
[25] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv
preprint arXiv:1608.03983 , 2016.
[26] Dominic Masters and Carlo Luschi. Revisiting small batch training for deep neural networks.
arXiv preprint arXiv:1804.07612 , 2018.
[27] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia,
Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed precision
training. arXiv preprint arXiv:1710.03740 , 2017.
[28] Ishan Misra and Laurens van der Maaten. Self-supervised learning of pretext-invariant rep-
resentations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 6707–6717, 2020.
[29] Tam Nguyen, Maximilian Dax, Chaithanya Kumar Mummadi, Nhung Ngo, Thi Hoai Phuong
Nguyen, Zhongyu Lou, and Thomas Brox. Deepusps: Deep robust unsupervised saliency
prediction via self-supervision. Advances in Neural Information Processing Systems , 32, 2019.
[30] Mehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving
jigsaw puzzles. In European conference on computer vision , pages 69–84. Springer, 2016.
[31] Hyun Oh Song, Yu Xiang, Stefanie Jegelka, and Silvio Savarese. Deep metric learning via lifted
structured feature embedding. In Proceedings of the IEEE conference on computer vision and
pattern recognition , pages 4004–4012, 2016.
[32] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A Efros. Context
encoders: Feature learning by inpainting. In Proceedings of the IEEE conference on computer
vision and pattern recognition , pages 2536–2544, 2016.
[33] Joshua Robinson, Ching-Yao Chuang, Suvrit Sra, and Stefanie Jegelka. Contrastive learning
with hard negative samples. arXiv preprint arXiv:2010.04592 , 2020.
11[34] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual
recognition challenge. International journal of computer vision , 115(3):211–252, 2015.
[35] Leslie N Smith. Cyclical learning rates for training neural networks. In 2017 IEEE winter
conference on applications of computer vision (WACV) , pages 464–472. IEEE, 2017.
[36] Leslie N Smith and Nicholay Topin. Super-convergence: Very fast training of neural networks
using large learning rates. In Artificial intelligence and machine learning for multi-domain
operations applications , volume 11006, page 1100612. International Society for Optics and
Photonics, 2019.
[37] Yumin Suh, Bohyung Han, Wonsik Kim, and Kyoung Mu Lee. Stochastic class-based hard
example mining for deep metric learning. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , pages 7251–7259, 2019.
[38] Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. In European
conference on computer vision , pages 776–794. Springer, 2020.
[39] Nenad Tomasev, Ioana Bica, Brian McWilliams, Lars Buesing, Razvan Pascanu, Charles
Blundell, and Jovana Mitrovic. Pushing the limits of self-supervised resnets: Can we outperform
supervised learning without labels on imagenet? arXiv preprint arXiv:2201.05119 , 2022.
[40] Hugo Touvron, Andrea Vedaldi, Matthijs Douze, and Hervé Jégou. Fixing the train-test
resolution discrepancy. Advances in neural information processing systems , 32, 2019.
[41] Michael Tschannen, Josip Djolonga, Paul K Rubenstein, Sylvain Gelly, and Mario Lucic. On
mutual information maximization for representation learning. arXiv preprint arXiv:1907.13625 ,
2019.
[42] Feng Wang and Huaping Liu. Understanding the behaviour of contrastive loss. In Proceedings
of the IEEE/CVF conference on computer vision and pattern recognition , pages 2495–2504,
2021.
[43] Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through
alignment and uniformity on the hypersphere. In International Conference on Machine Learning ,
pages 9929–9939. PMLR, 2020.
[44] Chao-Yuan Wu, R Manmatha, Alexander J Smola, and Philipp Krahenbuhl. Sampling matters
in deep embedding learning. In Proceedings of the IEEE International Conference on Computer
Vision , pages 2840–2848, 2017.
[45] Mike Wu, Milan Mosse, Chengxu Zhuang, Daniel Yamins, and Noah Goodman. Condi-
tional negative sampling for contrastive learning of visual representations. arXiv preprint
arXiv:2010.02037 , 2020.
[46] Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the European conference
on computer vision (ECCV) , pages 3–19, 2018.
[47] Tete Xiao, Xiaolong Wang, Alexei A Efros, and Trevor Darrell. What should not be contrastive
in contrastive learning. arXiv preprint arXiv:2008.05659 , 2020.
[48] Hong Xuan, Abby Stylianou, Xiaotong Liu, and Robert Pless. Hard negative examples are hard,
but useful. In European Conference on Computer Vision , pages 126–142. Springer, 2020.
[49] Yang You, Igor Gitman, and Boris Ginsburg. Large batch training of convolutional networks.
arXiv preprint arXiv:1708.03888 , 2017.
[50] Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and Stéphane Deny. Barlow twins: Self-
supervised learning via redundancy reduction. In International Conference on Machine Learn-
ing, pages 12310–12320. PMLR, 2021.
[51] Hang Zhang, Kristin Dana, Jianping Shi, Zhongyue Zhang, Xiaogang Wang, Ambrish Tyagi,
and Amit Agrawal. Context encoding for semantic segmentation. In Proceedings of the IEEE
conference on Computer Vision and Pattern Recognition , pages 7151–7160, 2018.
12[52] Huangjie Zheng, Xu Chen, Jiangchao Yao, Hongxia Yang, Chunyuan Li, Ya Zhang, Hao Zhang,
Ivor Tsang, Jingren Zhou, and Mingyuan Zhou. Contrastive attraction and contrastive repulsion
for representation learning. arXiv preprint arXiv:2105.03746 , 2021.
[53] Wenzhao Zheng, Zhaodong Chen, Jiwen Lu, and Jie Zhou. Hardness-aware deep metric learning.
InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition ,
pages 72–81, 2019.
13Supplementary materials for the paper
“Enhancing Contrastive Learning with
Efficient Combinatorial Positive Pairing”
A Implementation Details
In this work, we use 4 ×RTX 3090 GPUs as the default device for pre-training the encoding network
and its evaluation.
Encoder pre-training: For all three datasets(CIFAR-10, ImageNet-100, and ImageNet-1K) and
all view sizes, we use base learning rate of 0.4 where the learning rate is linearly scaled with batch
size (lr= base learning rate ×batch size / 256) and is scheduled by cosine learning rate decay with
10-epoch warm-up and without restarts [ 12,25]. We use SGD optimizer with momentum of 0.9 that
is commonly adopted for ImageNet training [ 12]. In this work, we do not use LARS optimizer [ 49],
because our default batch sizes is small (e.g., 64).
We use global weight decay of 0.0001 for CIFAR-10 and ImageNet-100 and 0.00002 for ImageNet-
1K. Weight decay is not applied to the biases and batch normalization parameters. We use a 2-layer
MLP projection head for CIFAR-10 and a 3-layer MLP projection head for ImageNet-100 and
ImageNet-1K. Each projection head’s hidden-layer dimension is chosen to be the same as the
encoder’s representation dimension. The projection head’s output dimension is always chosen to be
256.
Linear evaluation: We train a single layer linear classifier on top of the frozen encoder using SGD
optimizer (batch size of 256 for 100 epochs). For CIFAR-10, we use learning rate of 0.25, momentum
of 0.9, and no weight decay. For ImageNet-100 and ImageNet-1K, we use learning rate of 0.3,
momentum of 0.995, and weight decay of 0.000001. The learning rate is scheduled by a cosine
learning rate decay without warm-up and restarts.
Temperature ( τ):This work uses the temperature value of 0.2 following Wang and Liu [42] where
it was shown that SimCLR can achieve a better performance with 0.2 instead of the original 0.1 that
was used in SimCLR [4].
Crop scale: As the default crop scale, SimCLR [ 4] and BYOL [ 13] use [0.08,1] and SimSiam [ 7]
and MoCo-v2 [ 8] use [0.2,1]. We chose [0.2,1] because it performs better for the most cases shown
in Table 8.
Table 8: Comparison between two different crop scale options over a variety of temperature values.
Top-1% linear evaluation results for ResNet-50, batch size of 128, and 200 epochs are shown. The
results indicate that the crop scale of [0.2,1] can achieve a better performance in most of the cases
and confirms that the temperature of 0.2 performs the best.
(a) CIFAR-10
τ 0.10 0.15 0.20 0.25 0.30
(0.08,1) 87.7 89.4 90.0 89.7 89.4
(0.2,1) 86.9 89.8 90.4 90.1 90.0(b) ImageNet-100
τ 0.10 0.15 0.20 0.25 0.30
(0.08,1) 76.3 77.6 78.7 78.3 77.9
(0.2,1) 75.1 78.0 78.7 78.5 78.3
14Augmentation sets: The augmentation settings are shown below.
import torchvision.transforms as T
from PIL import ImageOps
class Solarization(object):
def __init__(self):
pass
def __call__(self, img):
return ImageOps.solarize(img)
transform_imagenet_default_large = \
T.Compose([
T.RandomResizedCrop(224, scale=(0.2, 1.0)),
T.RandomHorizontalFlip(),
T.RandomApply([T.ColorJitter(0.8, 0.8, 0.8, 0.2)], p=0.8),
T.RandomGrayscale(p=0.2),
T.RandomApply([T.GaussianBlur(23, sigma=(0.1, 2.0))], p=0.5),
T.RandomApply([Solarization()], p=0.1),
T.ToTensor(),
T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
transform_imagenet_global_small = \
T.Compose([
T.RandomResizedCrop(96, scale=(0.2, 1.0)),
T.RandomHorizontalFlip(),
T.RandomApply([T.ColorJitter(0.8, 0.8, 0.8, 0.2)], p=0.8),
T.RandomGrayscale(p=0.2),
T.RandomApply([T.GaussianBlur(23, sigma=(0.1, 2.0))], p=0.5),
T.RandomApply([Solarization()], p=0.1),
T.ToTensor(),
T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
transform_imagenet_global_small_crop_only = \
T.Compose([
T.RandomResizedCrop(96, scale=(0.2, 1.0)),
T.ToTensor(),
T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
# For CIFAR-10, we do not use small additional views.
transform_cifar_default = T.Compose([
T.RandomResizedCrop(32, scale=(0.2, 1.0)),
T.RandomHorizontalFlip(),
T.RandomApply([T.ColorJitter(0.4, 0.4, 0.4, 0.1)], p=0.8),
T.RandomGrayscale(p=0.2),
T.ToTensor(),
T.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010]),
])
transform_cifar_global_croponly = T.Compose([
T.RandomResizedCrop(32, scale=(0.2, 1.0)),
T.ToTensor(),
T.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010]),
])
15