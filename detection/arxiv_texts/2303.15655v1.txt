Joint embedding in Hierarchical distance and semantic representation
learning for link prediction
Jin Liu, Fengyu Zhou<, Jianye Chen and Chongfeng Fan
School of control science and Engineering, Shandong university, Jinan 250011, China
ARTICLE INFO
Keywords :
Knowledge graph embedding
Hierarchy information
Distance measurement
Semantic measurement
Knowledge graph representationABSTRACT
The link prediction task aims to predict missing links in the knowledge graph and is essential for the
downstream application. Existing well-known models deal with this task by mainly focusing on rep-
resenting knowledge graph triplets in the distance space or semantic space. However, they can not
fully capture the information of head and tail entities, nor even make good use of hierarchical level
information. Thus, in this paper, we propose a novel knowledge graph embedding model for the link
predictiontask,namely,HIE,whichmodelseachtriplet(h,r,t)intodistancemeasurementspaceand
semantic measurement space simultaneously. Moreover, to leverage the hierarchical information of
entitiesand relations,HIEisintroduced intohierarchical-awarespace forbetterrepresentationlearn-
ing. Speciﬁcally, we apply distance transformation operation on the head entity in distance space to
obtain the tail entity instead of translation-based or rotation-based approaches. Experimental results
ofHIEonfourreal-worlddatasetsshowthatHIEoutperformsseveralexistingstate-of-the-artknowl-
edge graph embedding methods on link prediction task and deals with complex relations accurately.
1. Introduction
Knowledgegraphs(KGs)havebeenanessentialcompo-
nentinartiﬁcialintelligence(AI)andappliedtomanydown-
stream applications, such as question answering[1][2], sen-
timent analysis[3] and image caption[4]. Numerous well-
known large-scale knowledge graphs, such as WordNet[5],
Freebase[6]andYAGO[7],composedofstructuralinforma-
tion from manifold human knowledge, stored in a directed
graph,havebeenwidelyemployedforvariousdomaininthe
pastdecades. Theformofknowledgefactistypicallytriplet,
i.e. (head entity, relation, tail entity) or (subject entity, rela-
tion,objectentity),or( h,r,t)inshort. Speciﬁcally,thehead
or tail entity denotes the concepts or deﬁnitions, and the re-
lation represents the relationship between entities. Taking a
triplet(Allen,Workmate,Wangkai)fromFig.1asanexam-
ple, it describes the fact that Wang kai is the workmate of
Allen.
AlthoughpreviousKGs,suchasWordnet,Freebaseand
YAGOarecomposedofmassivefacts,theyinevitablysuﬀer
from the incompleteness of the existing knowledge graphs.
As the KG shown in Fig.1, we can get the corresponding
facts of the triplets that are circled in black and connected
toAllen. However,conditionedontheKG,wedonotknow
what relationship between Allen and Joe Chen, nor the re-
lationships among those are connect to Allen. Thus, ac-
curately predicting the missing links between known enti-
ties, i.e., link prediction task, has gained much more atten-
tion. Many knowledge graph embedding (KGE) methods
are proposed to learn the continuous low-dimensional em-
bedding expressions of entities and relations to fulﬁll this
task. According to the form of score functions, previous
KGEmethodscanberoughlyclassiﬁedintothreecategories:
<Corresponding author
zhoufengyu@sdu.org (F. Zhou)
ORCID(s):
AllenWang ShashaQiao xin
Song xiaoqiang Nick WangPeng Sanyuan
Relation=?Relation= ？
Joe ChenFigure 1: IllustrationofasmallKGforAllen, wheretherelation
between Joe Chen and Allen, Peng Sanyuan and Allen are
needed to infer via KG embedding methods.
(1)Translation-basedmodels(2)Semanticmatchingmodels
(3) Deep neural network-based models. These approaches
achieve promising results to some extent. However, since
thesemethodscanonlycapturestructuralinformationorse-
manticinformation,theylacktheabilitytoextracthierarchi-
calfeaturesandtakeadvantageoftwokindsofinformation,
which potentially limits their performance on the link pre-
diction task.
AstheseminalworkinKGEs,TransE[8]projectsknowl-
edge graphs into low-dimensional space eﬃciently and ob-
tainssatisfactoryresultsin1-to-1tasks. Themodelingabil-
ityofTransEmaybebecauseitcouldcapturealotofstruc-
tural information in the distance measurement space. How-
ever, TransE fails to model other complex relation proper-
tiesof1-to-N,N-to-1andN-to-N.Tosolvesuchaproblem,
TransR[9] utilizes the relationship-speciﬁc mapping matrix
to project head and tail entities into relation-speciﬁc space.
Nevertheless, there may be a little diﬀerences between tail
entities and head entities under the same relationship. Be-
sides, TransR ignores the structural information of the dis-
Jin Liu et al.: Preprint submitted to Elsevier Page 1 of 12arXiv:2303.15655v1  [cs.CL]  28 Mar 2023HIE for the link prediction
tance measurement space and the latent semantic informa-
tion between entity-relations, which can also impair the ex-
pression ability of the model. Another exciting work M-
DCNtodealwithsuchaproblemwasproposedin[10]. The
model uses convolutional kernels and embedding combina-
tion to capture triplet-level semantic information. However,
the structural features are weakened, and the way of entity
combination in M-DCN can omit some original embedding
information.
Thus, to build an expressive KGE model and deal with
thelinkpredictiontask,weproposeanovelknowledgegraph
embedding model named HIE. Unlike the previous works,
weprojecteachentry(h,r,t)intothedistancemeasurement
space and semantic measurement space to keep structural
and entity-relation’s semantic information through the cor-
responding mapping matrix. Consequently, the model can
leverage more complex features between entities and their
relationships. Taking the relationship play asin Fig.1 as an
example, in the distance measurement space, models may
get Emb(Qiao xin) ùEmb(Song xiaoqiang). While in the
semanticmeasurementspace,theprojectedtailentities(i.e.,
QiaoXinandSongxiaoqiang)maybecloseviatheplay_as-
speciﬁcmatrixbyusingTransRorM-DCNandtherelation
maynotbesuitablefortheprojectedentities. Thus,themod-
eling ability of methods may be limited by only projecting
theentitiesandrelationsindistancemeasurementorseman-
ticspace. Comparatively,HIEprojectstheheadentity,rela-
tionandtailentityintoﬁne-grainedsemanticspace,whichis
shown in Fig.2. That’s to say, we may get the two plausible
triplets(Allen(musician),play_in_urban_ﬁlm,Qiaoxin(rock
teenagers))and(Allen(worker),play_in_narrative_ﬁlm,Song
xiaoqiang(covid-19 ﬁghter)). As for the relationship Class-
mate,theprojectedembeddinginthedistancemeasurement
spacekeepsthestructuralinformationoftheoriginaltriplet.
Therefore, HIE can distinguish the triplets easily under the
diﬀerentrelations. Inaddition,motivatedbytheTKRLmodel[11],
whichmodelsentitieswithhierarchicaltypes. Wejointlyex-
tracthierarchicalgeometricinformation(fromdistancemea-
surement) and semantic information (from semantic mea-
surement) to capture more triplet features, which can also
be treated as a multi-view way to use knowledge to solve
the link prediction task. In the empirical experiments, we
compareHIEwithseveralstate-of-the-artKGEmethods,in-
cludingtranslation-based,neuralnetwork-basedandseman-
tic matching-based models. The results on the benchmark
datasets show that our proposed HIE outperforms the base-
line model in the link prediction task and outperforms sev-
eral complex relations modeling task.
Inconclusion,ourmaincontributionsaresummarizedas
follows:
•WeproposeanovelKGEmodel,namely,HIE,which
keeps structural geometric information and captures
semantic information by projecting each triplet (h, r,
t) into both distance measurement and semantic mea-
surementspacetolearntheknowledgerepresentation
jointly.
AllenQiao xin
Song xiaoqiangMusician Play_In_ Urban _Film Rock teenagers
Worker Play_In_ Narrative _FilmCOVID -19 
FighterOrigin space  semantic space Semantic spaceFigure 2: Illustration of the projection from origin space to
entity-relation semantic space.
•We use distance transformation operation instead of
translation-based or rotation-based projection to ob-
tain tail entity. And adopt the semantic extractor to
learn ﬁne-grained entity and relation representations
indiﬀerentsemanticspaces. Moreover,weextracthi-
erarchical geometric information and semantic infor-
mationfortherepresentationlearningprocesstocap-
ture more information without extra data.
•Tothebestofourknowledge,wearetheﬁrsttocom-
bine distance measurement with semantic measure-
ment and further employ the hierarchical information
for the model to address the link prediction task.
•We conduct our experiments extensively on four rep-
resentativebenchmarkdatasetsWN18[8],WN18RR[12],
FB15k-237[13] and YAGO3-10[14]. The evaluation
results illustrate that HIE outperforms the compared
severalstate-of-the-artmethodsonthelinkprediction
task. More importantly, HIE shows its robustness for
empirical results via comprehensive ablation studies.
The rest of this article is organized as follows. Related
work is brieﬂy reviewed in Sec.2. In Sec.3, the proposed
HIE is presented in detail. In Sec.4, extensive experiments
are conducted on four real-world datasets. Also, the experi-
mental protocols, evaluation metrics and the results on link
prediction and complex relations are reported. The conclu-
sion and future work are discussed in Sec.5.
2. Related work
Knowledge graph embedding has been widely studied
as one of the most critical parts for solving the link pre-
diction task in recent years. According to the score func-
tions and representation approaches, existing KGE models
can be roughly divided into three categories: Translation-
based models, semantic matching models, deep neural net-
work models. Table.1 reports several state-of-the-art KGE
models and their parameters.
2.1. Translation-based models
TransE[8]isregardedasthemostrepresentativetranslation-
basedmodel,interpretingrelationshipsasatranslationfrom
Jin Liu et al.: Preprint submitted to Elsevier Page 2 of 12HIE for the link prediction
Table 1
Summary of several KGE models’ score function: where hdenotes the conjugate oper-
ation overh,< h;t > denotes the Hermitian dot product and equals hTt.ýrepresents
the Hadamard product. fDdenotes the score function of discriminator. p(h,r;c)=(cýh)?r,
where?denotes the circular correlation, c is a ﬁxed vector. <denotes convolution opera-
tion,ârepresentstheconcatenateoperation, èdenotesthedepthwisecircularconvolution
operation.
Category Model name Score function Parameters
Translation-based modelsTransE - ððh+r-tððL1_L2h,r,tËRd
TransD - ðð(rphT
p+I)h+r-(rptT
p+I)tðð2
2h,t,wh,wtËRd,r,wrËRk
TransR -ððMrh+r-Mrtðð2
2h,t,rËRd,MrËRdd
ComplEx Re( <r, h,t>) h,r,tËCd
RotatE ððhýr - tðð h,t,r,wrËRd
TorusE min.x;y/Ë.[h]+[r]/[t]ððx - yðði [h],[r],[t]ËTn
Semantic matching modelsDistMult hTdiag(Mr)t h,r,t ËRd
RESCAL hTMrt h,t ËRd,MrËRdd
HolEx³l
j=0p(h,r;cj)t h,r,tËRd
SimplE1
2(hýrt+týr¨t) h,r,t,r¨ËRd
Deep neural network modelsKBGAN fD(h,r,t) h,r,tËRd
ConvE f(vec( f([h;r]<
))W)t h,r,t ËRd
ConvKB concat( f([h,r,t]<
))W h,r,tËRd
M-DCN (f(vec( f([hâr]<wi
r))W)t+b) h,r,tËRd, wi
rËR12
InteractE g(vec( f(([(h,r)])èw))W)h h,r,t ËRd,wËRkk
the head entity to the tail entity. For a triplet ( h,r,t), it at-
tempts that h+rùtand employs fr(h,t)= -ððh+r-tððL1_L2as the corresponding score function under L1orL2con-
straints. Despiteitssimplicityandeﬃciency,TransEcouldn’t
obtain well results on complex relations, such as 1-to-N, N-
to-1 and N-N. In order to address such issues, TransH[15]
projects the entities into the relation hyperplane and per-
formsatranslationoperationonthetransformedembeddings.
TransR[9] employs projection spaces for entities and rela-
tionsviaarelation-speciﬁcmappingmatrixMrandperforms
translation into the relation-speciﬁc space. To improve the
performance of TransR, Ji et al.[16] proposed a more ﬁne-
grained model TransD, in which entities and relations are
composed of two basic vectors. One is for capturing the
semantic meanings, and the other one for constructing dy-
namicmappingmatricesMrh=rphT
p+ImnandMrt=rptT
p+Imn
via the projection vectors hp, tpandrp.
Recently,therehavebeensomevariantsofthetranslation-
based model by representing the head entities and relations
into the complex vector space and manifold space. Com-
plEx[17]ﬁrstlyappliescomplexvalueembeddingapproach
toentitiesandrelations,whichattemptstomodelsymmetric
and antisymmetric relations via Hermitian dot product be-
tween real vectors and its conjugate. However, it can’t deal
withthecompositionrelationpattern. Tosolvethisproblem,
RotatE[14]modelstherelationsasanelement-wiserotation
rather than a translation from head entities to tail entities in
complex space, and the corresponding score function is de-
ﬁnedasððhýr-tðð. Moreover,sinceTransEexiststheprob-
lems of regularization and an unchangeable negative sam-plingratio,TorusE[18]projectstheentitiesandrelationson
acompactLiegroupintorusspace,andthescorefunctionis
deﬁned asmin.x;y/Ë.[h]+[r]/[t]ððx - yððiwith the projection:
RnTn,x­[x], where [ h], [r], [t]ËTn. Additionally,
TorusE adopts the similar relation translation, i.e., [ h] + [r]
ù[t]. However, the above models simply project both en-
tities and relations into the same embedding space. Thus,
they couldn’t model the hierarchical characteristics for en-
tities and relations, which leads to weak improvements for
link prediction task.
2.2. Semantic matching models
Another way to obtain the embeddings is to match the
latent semantic similarity between entities and relations in
vector space. SE[19], which is regarded as a linear model,
projectsthespeciﬁcrelationasasquarematrixpairM=(M1,
M2)andusesthesimilarityfunction fr(h,t)=-ððM1h-M2tððL1_L2to determine a plausible triplet. As for the bilinear model,
RESCAL[20] is a representative tensor factorization model
to capture the latent structure information, where each re-
lation is factored as a dense square matrix Mrto associ-
ated with the entity vector embedding. However, due to the
three-way rank-r factorization, it has quadratic growth pa-
rameters and tends to overﬁt[21]. DistMult[22], which ex-
tendsRESCALbymodifyingthebilinearfunction,proposes
asimpleembeddingapproachthroughabilinearproductand
replacesthecomplexrelationmatrixwithaneasy-to-traindi-
agonal matrix. However, when dealing with the symmetric
relations, i.e., (h,r,t) and (t,r,h), DistMult exhibits the same
results for triplets and lacks adequate expressive power. By
Jin Liu et al.: Preprint submitted to Elsevier Page 3 of 12HIE for the link prediction
employingcircularcorrelationofentityandrelationembed-
dings, HolE[23] can capture rich embedding interaction in-
formation and maintain the computing eﬃciency as Dist-
Mult. TofurtherimprovetherepresentationabilityofHolE,
Xueetal.[24]proposesHolExthatinterpolatesthefullprod-
uct matrices to achieve a lower dimensionality. SimplE[25]
introduces a simple tensor factorization approach by calcu-
lating the average score of the inverse triplets to be full-
expressive, i.e., ( h, r, t) and ( t, r*1, h), where r¨represents
the inverse relation embeddings.
Although these semantic models can capture semantic
information between entities and relations, they are prone
to suﬀering overﬁtting due to the model redundancy in low
dimension embedding space. Further, the structural infor-
mation of the triplets can not be fully utilized.
2.3. Deep neural network-based models
Recently,deepneuralnetworkshavebeenappliedinvar-
ious domains successfully and attracted more and more at-
tention in solving link prediction task. Representative neu-
ral tensor work (NTN)[26] takes entity embeddings and re-
lation vectors as input and measures the plausibility of a
triplet associated with a particular relationship by combin-
ing multi-layer perceptrons (MLPs) with bilinear models.
ConvE[12]andConvKB[13]adoptaconvolutionneuralnet-
worktomodelinteractionsbetweenentitiesandrelationsfor
thelinkpredictiontask. Thediﬀerencebetweenthemisthat
ConvE uses 2D convolution on a reshaped entity and rela-
tionembeddingmatrixwhileConvKBencodestheconcate-
nationofentityandrelationembeddingswithoutreshaping.
ApartfromCNN,Nguyenetal.[27]proposesCapsE,which
takes each triplet as a 3-column matrix and adopts Capsule
Net to capture the information of embeddings [28]. How-
ever,thesemodelssuﬀerfromthreeproblems: (1)itisinsuf-
ﬁcient in dealing with complex relations; (2) the number of
interactionsthattheycancaptureislimited;(3)thestructural
information between entities and relations can be ignored
easily. Thus, Zhang et al.[10] presents a multi-scale dy-
namic convolutional network to explore the entity and rela-
tionembeddingcharacteristicsfortheﬁrstproblem,namely,
M-DCN.Asforthesecondproblem,InteractE[29]increases
the number of additional interactions via feature permuta-
tion,anovelfeaturereshaping,andcircularconvolution. For
the purpose of modeling various relational patterns, captur-
ing structural information and improving the computation
eﬃciency of deep neural networks, Liu et al.[30] proposes
AprilE by introducing a triple-level self-attention approach.
Graph convolution networks (GCNs) have been used as an-
otherwaytocapturethegraph-structuredandembeddingin-
formationsimultaneously. R-GCN[31]usesrelation-speciﬁc
GCN to model the directed knowledge graph. KBGAN[32]
combinesKGEwithgenerativeadversarialnetworks(GANs)
to model the entities and relations, and the score function is
deﬁned in Table.1.
Although neural network-based models can capture se-
mantic information of triplet more easily than translation-
based and semantic matching models, they still suﬀer fromTable 2
Notations and Explanations of HIE which are used in this pa-
per.
Notations Short explanations
G Knowledge graph
E;R;T entities, relations and triplets
h,r,t head entity, relationship and tail entity
fr(h,t) score function
1;2,5the weights of diﬀerent hierarchical level
ý Hadamard product
ððððL1_L2L1orL2norm
diag() the diagonal projection matrix
large model parameters, and the inability to make good use
of the global structure.
2.4. models with hierarchical structural
information
Tomakeuseofauxiliaryinformationfromexternalknowl-
edge for the link prediction task, several models have intro-
ducedhierarchicalstructuresinrecentyears. TKRLprojects
entities by type information matrices, which are composed
of weighted and recursive hierarchical type encoders[11].
HA-RotatEprojectsentitieswithdiﬀerentmoduliinthedif-
ferenthierarchicallevels[33]. MuRPtakesfulladvantageof
hyperbolic embeddings to represent hierarchical relational
graphdataaccordingtotherelation-speciﬁcparameters[34].
Comparatively,HAKElearnsthehierarchicalentityembed-
dings in the polar coordinate system.
Theabovehierarchical-basedlearningmethodsonlycap-
tureentityorrelationfeaturesintheirembeddingspace. How-
ever,theyfailtokeepthegeometricinformationorsemantic
information simultaneously. Thus, they can only show ad-
vantages in dealing with complex relation properties or re-
lation patterns.
3. Our Model
This section presents our proposed model HIE for the
link prediction task, which can capture the position and se-
manticinformationinhierarchicallevelrepresentationspaces.
Before introducing the HIE model, the basic mathematical
notions of model are summarized in Table.2.
3.1. Problem formulation
A knowledge graph consists of triplets, which can be
expressed as G= (E;R;T), where h0;5;ðEð,t0;5;ðEðËEand
r0;5;ðRðËR.ðEðandðRðdenotethenumberofentitiesand
relations, respectively. For a link prediction task, the model
aimstopredicttheheadentitywithagiventailentity(?, r,t)
or the tail entity with a given head entity ( h,r,?) to get a
valid triplet ( h,r,t). Moreover, the model needs to learn the
low-dimension embeddings for entities and relations.
Jin Liu et al.: Preprint submitted to Elsevier Page 4 of 12HIE for the link prediction
3.2. HIE model
3.2.1. Basic structure space
UnliketheTKRL,whichcaptureshierarchicalentitytype
information,andTranR,whichprojectsentitiesintorelation-
speciﬁc space, comparatively, we model the triplet into dis-
tance measurement and semantic measurement space. The
basic distance measurement space and semantic measure-
ment space structure are shown in Fig.3. For each entity or
relation,weconsiderthatitcarriestwopartsofinformation.
One is the geometric information for the distance measure-
mentofentitiesindistancespaceunderthespeciﬁcrelation.
The other one is used to identify the sub-semantic restric-
tionsinsemanticspace. It’sworthnotingthattheconceptof
distancemeasurementisageneralwaytodeterminethepo-
sition change degree in the embedding space. Take a triplet
fromFig.1,(Allen,classmate,Wangshasha),the"classmate"
is1-1relationshipinthissub-graph. Thus,entitiesandrela-
tionshipembeddingindistancemeasurementspacecaneas-
ilydistinguishthetriplet,theeﬀectofthesemanticisslightly
low. While we choose the 1-N relation "play as", the sub-
semanticplaysamoreimportantrolethandistancemeasure-
ment space. Also, the distance measurement space keeps
the geometric for the origin triplet as auxiliary information,
which can be useful for determining a plausible long path
triplet in dealing with complex relation properties[35].
(a) Distance measurement
(b) Semantic measurement
1
ph
1
pt
1
pr
1
st
1
sr
1
sh
1
,htr
1
,spt
t
h
?rOrigin space
1Mhp
1Mhs
1Mts
1Mtp
Figure 3: The basic structure of HIE.
Thestepsoftheproposedmethodinbasicdistancemea-
surementspaceandsemanticmeasurementspacearedescribed
as follows.
Firstly,theentitiesandrelationsarerandomlyinitialized
ask-dimensionrepresentation. Then,allofthemareequally
segmented into two parts. For example head entity h is di-
vided into h0andh1. Here, we consider that h0consists of
geometricinformationand h1consistsofsemanticinforma-
tion.
Secondly,weprojectthe h0andh1indistancemeasure-
ment space and semantic measurement space, respectively.
Distance measurement space In this geometric space,
we utilize the corresponding matrices to obtain h1
p,t1
pandr1
p,respectively. Theelement-wiseprojectioninthedistance
measurement space is deﬁned as:
h
n
l
njh1
p=diag.M1
hp/ýh0
t1
p=diag.M1
tp/ýt0
r1
p=diag.M1
rp/ýr0(1)
where h0,t0andr0denote the ﬁrst segment representation
ofh,r, and t, respectively. diag.Ms
hp/is a diagonal matrix
thatprojectsh0intogeometricdistancemeasurementspace,
diag.Ms
tp/anddiag.Ms
rp/denote the same operation on t0
andr0.
Insteadofusingrotationortranslationoperationsforknowl-
edge graph representation, a ﬂexible transformation matrix
Ms
ris used to capture the geometric features among entities
and relations. As is shown in Fig.3, the light blue straight
linemodelstherelationasatranslation,andthepurplecurve
dotted line treats the relation as a rotation. However, these
modelingapproachesarenotﬂexible,norcapturemoretrans-
formed information between entities and relations for deal-
ingwithcomplexrelations. Thus,asofttransformmatrixis
proposed to overcome the problems, which is shown as the
red curve solid line. In this way, HIE stores all the possible
formoftransformationandmovementoperationparameters
throughthematrix M1
r. Moreover,thisapproach canensure
that the projecting procedure becomes more ﬂexible.
M1
r=M0r1
p(2)
where M0is used to form transformation operation matrix
and the dimension of M0isk
2*1.
Hence,thepositiondistancefunctionintheﬁrstlevelof
HIE is shown in Eq.3.
dp1
r=ððh1
pM1
r*t1
pððL1_L2(3)
Semantic measurement space In this semantic space,
we tendto use sub-semantic toproject entities and relations
in a ﬁne-grained way. Similarly, the projection in this space
is deﬁned as Eq.4.
h
n
l
njh1
s=diag.M1
hs/ýh1
t1
s=diag.M1
ts/ýt1
r1
s=diag.M1
rs/ýr1(4)
where h1, t1,andr1denotethesecondsegmentofh,r,andt,
respectively. diag.Ms
hs/isasemanticextractorthatprojects
h1intoﬁne-grainedsub-semanticmeasurementspace, diag.Ms
ts/
anddiag.Ms
rs/denote the same operation on t1andr1.
AsisillustratedinFig.3(b), t1
s;pisthepredictedtailentity
andr1
h;tistheidealrelationembedding. Similarto[36][37],
wealsotrytohavethefollowingequation: h1
s+r1
sùt1
s,i.e.
r1
sùr1
h;t. And r1
sisaprojectedcompositionrepresentationin
semanticspace,and r1
sheredenotesamixedsemanticinfor-
mation representation, including types, concepts, and other
Jin Liu et al.: Preprint submitted to Elsevier Page 5 of 12HIE for the link prediction
Mpe0
1Distance 
measurement 
space
Semantic
measurement 
space
1Mp
1Ms
0M
=
+
=Basic structure space
Mpe
+
=
1M Hierarchical level
Mpe
Mpe
+
=
MkFeature score 
fusion
htrx
=h
r
th0
h1r0t0
r1t1
Figure 4: The whole architecture of the proposed HIE.
semantic information. The following Eq.5 is used to obtain
the score of triplet.
ds1
r=ððh1
s+r1
s*t1
sððL2(5)
Thirdly, according to the above projections in two spaces,
thejointscorefunctioninthebasicstructurallevelisdeﬁned
as Eq.6.
d1
r=dp1
r+.1*/ds1
r(6)
whereisalearnableweightparameterusedtotakeadvan-
tage of distance measurement space and semantic measure-
ment space.
3.2.2. Hierarchical structure representation
In order to capture hierarchical information from diﬀer-
entspacelevels,weencoderthebasicstructurespacerepre-
sentationinhierarchicalway,whichisshowninFig.4. Also,
the semantic and position extraction matrix are deﬁned as
follows:
h
n
n
n
n
l
n
n
n
njh2
p=h1
pMpe+h0
t2
p=t1
pMpe+t0
r2
p=r1
pMpe+r0
h2
s=h1
sMse+h1
t2
s=t1
sMse+t1
r2
s=r1
sMse+r1(7)
whereMpedenotesthegeometricextractionmatrixthatcan
obtain latent information from the shallow level, Mseis the
semantic extraction matrix. The deeper information can be
captured via the above matrices.
Also,throughsimilaroperationsonthenewembeddings
in the deep level representation space, the position distance
function can be deﬁned as:
dp2
r=ððh2
pM2
r*t2
pððL1_L2(8)
where h2
pandt2
pdenotecorrespondingelement-wiseprojec-
tion in distance measurement space, M2
ris the transforma-
tion operation parameter matrix as M1
rvia multiplying M1,
which is also ak
2*1 dimensional matrix.
Similarly,atdiﬀerenthierarchicallevelspace,thecorre-
sponding measurement score can be obtained as Eq.9.dpk
r=ððhk
pMk
r*tk
pððL1_L2(9)
Correspondingly,thesemanticfunctionofHIEatdiﬀer-
ent deep level is deﬁned as:
dsk
r=ððhk
s+rk
s*tk
sððL2(10)
And the total score function in deep level is deﬁned as:
dk
r=dpk
r+.1*/dsk
r(11)
Finally,accordingtoEq.6andEq.11,theﬁnalscorefunc-
tion of HIE is deﬁned as follows.
fr.h;t/=NÉ
k=1kdk
r(12)
wherekistheweightsforthetradeoﬀbetweenhierarchical
level embedding scores, under the constrain 1=³k.
3.3. Loss Function
Following [14], we use the negative sampling loss with
self-adversarialtrainingastheﬁnallossandtrytominimize
it. The ﬁnal loss function is illustrated as Eq.13.
L=*log.*fr.h,t//*nÉ
j=1p.h¨
j;r;t¨
j/log.fr.h¨
j;t¨
j/*/
(13)
whereis a ﬁxed margin, denotes the sigmoid function,
.h¨
j;r;t¨
j/isthej-thnegativetriplet,and p.h¨
j;r;t¨
j/istheneg-
ative triples sampling probability distribution, which is de-
ﬁned in Eq.14.
p.h¨
i;r;t¨
ið^hj;rj;tj`/=expfr.h¨
i;t¨
i/
³
jexpfr.h¨
j;t¨
j/(14)
whereis the temperature of sampling, and treated as the
weight of negative samples.
3.4. Connection to other KGE models
AlthoughsomemathematicalformsofourproposedHIE
are similar to RotatE[37], in which the author uses modu-
lus and phase information to learn entity and relation em-
beddings, the aims of the two models are diﬀerent indeed.
Jin Liu et al.: Preprint submitted to Elsevier Page 6 of 12HIE for the link prediction
Table 3
The statistics of datasets in the experiment. Rels denotes the
number of relations. Ents is the number of entities. Train,
Valid and Test denote the number of training, validation and
test triples, respectively.
Datasets Rels EntsTriples
Train Valid Test
WN18 18 40,943 141,442 5,000 5,000
WN18RR 11 40,943 86,835 3,034 3,134
FB15k-237 237 14,541 272,115 17,535 20,466
YAGO3-10 37 123,182 1,079,040 5,000 5,000
RotatE tries to model and infer all relation patterns, while
HIEaimstoobtainmoreexpressiveembeddingsviaextract-
ing information from position distance space and semantic
space. Moreover, RotatE can not deal with complex rela-
tionswellduetotheangle-representation. Morespeciﬁcally,
RotatEisonlyanangle-speciﬁcrepresentationofHIEbyre-
placing the transformation matrix with a rotation angle in
distance measurement space. Moreover, HIE utilizes hier-
archical level information to imitate people’s cognitive be-
havior to improve the expressive ability of the embedding
process.
4. Experiments
4.1. Datasets
Our proposed model HIE was evaluated on the follow-
ing public benchmark datasets: WN18[8], WN18RR[12],
FB15k-237[13], YAGO3-10[14]. All of them are extracted
from the real-world knowledge graphs and contain massive
entities and relations. The statistics of them are reported in
Table.3, and detailed information is listed as follows.
(1) WN18 is extracted from WordNet[38], a lexical knowl-
edge graph for English, consisting of 10 relations and
40,943 entities. Its entities represent the word sense,
andtherelationsdenotethelexicalrelationshipbetween
entities. Moreover, most the relation patterns are com-
posed of symmetry_antisymmetry andinversion .
(2) WN18RR is a subset of WN18, where all the inverse
relations are deleted. It consists of 40,943 entities with
11diﬀerentrelations. Andthemainrelationpatternsare
symmetry_antisymmetry andcomposition .
(3) FB15k-237 is a subset of FB15k with 14,541 entities
and 237 relations. Besides, all the inverse relations are
deleted. Thus,themainrelationpatternsare composition
andsymmetry_antisymmetry .
(4) YAGO3-10 is extracted from YAGO3[39], which con-
tainsapproximately123,182entitiesand37relations. It
describestherelationsamongpeopleandtheirattributes
and has a minimum of 10 relations per.
4.2. Evaluation Metrics
Linkpredictionisanessentialtaskforknowledgegraph
completion, aiming to predict the missing entity accordingto a speciﬁc relation, i.e. ( h, r, ?) or (?, r, t). The real triplet
can be obtained by ranking the results of score function fr.
Following the experimental settings in TransE[8], for each
given triplet xi=(h,r,t)ËTtest, we replace the head entity h
orthetailentitytbyanyotherentityintheknowledgegraph
to generate a set of corrupted triplets, i.e.  xs
i=(hcor,r,t)ÌT
or xo
i=(h,r,tcor)ÌT respectively, where hcorandtcordenote
a set of corresponding candidate entities. Then we check
whethertheproposedmodelHIEobtainsahighscoreforthe
test triplet and a low score for corrupted triplets. The right
ranksranko
iandleftranksranks
iofthei*thtesttripletxiare
each associated with corrupting either head or tail entities
corresponding to the score function Eq.12, i.e. fr.h;t/, and
are deﬁned as follows:
ranko
i=1+É
 xo
iÌTI['.xi/<'. xo
i/]
ranks
i=1+É
 xs
iÌTI['.xi/<'. xs
i/](15)
whereI[P]denotes the indicator function. If the condition
Pis true, the function returns 1 and returns 0 otherwise.
In the experiment, the following three popular ranking met-
ricsareutilizedasourevaluationmetrics: Meanrank(MR),
Meanreciprocalrank(MRR)andHits@k(k=1,3,and10in
this paper), which are deﬁned in Eq.16. MR and MRR de-
note the average ranks and the average inverse ranks for all
test triplets, respectively. Hits@k is the percentage of ranks
lower than or equal to k. In the above three metrics, higher
MRRandHits@kmeanbetterperformance,whilethelower
MR indicates better performance.
MR:1
2GÉ
xiËT.ranko
i+ranks
i/
MRR:1
2GÉ
xiËT.1
ranko
i+1
ranks
i/
Hits@k:1
2GÉ
xiËTI[ranko
ifk]+I[ranks
ifk](16)
whereG=ðTtestðdenotes the size of Ttest.
4.3. Implementation details
For this experiment, we implement our proposed model
by Pytorch[40] with an adam optimizer[41], and ﬁne-tune
the hyperparameters to obtain the optimal conﬁguration via
grid search approach. The hyperparameter settings are as
follows: entityandrelationembeddingdimension Ë[250,500],
batch sizeË[256,512], ﬁxed margin Ë[3, 6, 9, 12, 18],
self-adversarial sampling temperature [0.5, 1.0],l1-norm
orl2-norm, and test batch size Ë[8, 16],1and2Ë[ 0.2,
0.4, 0.6, 0.8].
Moreover, we compare the proposed model HIE with
threecategoriesofstate-of-the-artbaselines: (1)translation-
based models including TransE, TransR, TransD(unif), Ro-
tatE, TorusE and ComplEx. (2) semantic matching models,
Jin Liu et al.: Preprint submitted to Elsevier Page 7 of 12HIE for the link prediction
12 3 4 0.700.750.800.850.900.951
2 3 4 0.300.350.400.450.500.550.60WN18RRW N18T
he number of hierarchical levels MRR 
HITS@1 
HITS@3 
HITS@10 MRR 
HITS@1 
HITS@3 
HITS@10
Figure 5: The results of diﬀerent hierarchical level size on
WN18 and WN18RR
includingDistMult. (3)deepneuralnetworkmodelsinclud-
ingM-DCN,ConvE,ConvKB,KBGAN,R-GCN,HARN[42]
andInteractE.Therearetwocriteriafortheabovealgorithms
to be selected, i.e., they achieve high performance within a
certainrangeofcomparisonandprovidethesourcecode,or
their architecture is easy for reproducibility.
4.4. Optimal hierarchical level
Due to the limited computing resources, we need to de-
termine thenumber ofhierarchical levelsat ﬁrst. In thisex-
periment,thelinkpredictionresultsforhierarchicallevelsof
HIE on WN18RR and WN18 are displayed in Fig.5. As the
hierarchical level grows, the corresponding batch size and
embeddingdimensiondecrease,andthehierarchicalweights
areinitializedequally. HIEwithfourdiﬀerentlevelsareim-
plemented, as shown in Fig.5.
As can be observed from Fig.5, all the metrics are opti-
mal when two levels are set. This is mainly due to HIE can
obtainstructuralinformationandsemanticinformationfrom
joint spaces eﬃciently. However, only one level for HIE is
not adequate to capture enough information for modeling.
Too many levels could also introduce noises, and may lead
to poor performance for link prediction. Although the met-
rics in four level spaces are higher than that of three level
spaces, the values of four evaluation metrics are still lower
than that of two levels. In the next experiments, HIE with
two levels are expressed as shallow level (level 1) and deep
level (level 2). In conclusion, to balance the accuracy and
resources utilization, HIE with two levels are chosen in the
following experiments. Also, we can get the corresponding
score function as deﬁned in Eq.17.
fr.h;t/=1d1
r+2d2
r(17)
4.5. Link prediction
In this part, the link prediction results of HIE on four
publicdatasetsaresummarizedinTable.4andTable.5. The
bestscoreisin boldandthesecond-bestscoreisunderlined .
TheevaluationresultsonWN18andYGAO3-10arereported
in Table.4 and the results on WN18RR and FB15k-237 are
shown in Table.5. Moreover, all the compared results are
based on the optimal performance of the algorithm.
The observations from Table.4 can be summarized as
follows:•Ourproposedmodelcanobtainsigniﬁcantoveralleval-
uationscoresfromtheempiricalresultscomparedwith
threekindsofKGEmodelsonWN18andYAGO3-10.
Duetothemassiveinversionrelationpatternsexisting
in WN18 where the ratio reaches 94%[43]. To meet
the inversion equation, the transformation matrix in
HIEwillbeanidentitymatrix. That’stosay,morein-
formationondistancemeasurementisomitted. Thus,
theproposedHIEdoesnotachievebestscoresofMRR
and Hits@1 on WN18.
•On WN18, the proposed HIE outperforms the com-
pared algorithms and achieves the best MR, Hits@3
and Hits@10. In terms of MRR and Hits@1, HIE
is worse than that of several state-of-the-art models,
whileitobtainsa30%higherHits@1thanR-GCN[31]
which uses more additional neighbor information.
•OnYAGO3-10,intermsofMRR,Hits@3andHits@10,
HIEachievesthebestscore. AsfortheMRandHits@1,
HIE is slightly worse than the state-of-the-art RotatE
and InteractE in the corresponding aspects. Notably,
HIEobtainsa7%higherMRRthanthatofneuralnetwork-
based model M-DCN, 59% higher MRR than that of
semanticmatching-basedmodelDistMult,and9%higher
thanMRRthatoftranslationbasedmodelRotatE.Strik-
ingly, our proposed HIE achieves better performance
scores on Hits@3 and Hits@10 than that of ConvE,
RotatE and DistMult.
From Table.5, we can conclude that:
•From the comparison of the experimental results on
WN18RRandFB15k-237,comparedwithneuralnetwork-
based models and semantic matching based models,
wecanobservethatHIEachievessigniﬁcantimprove-
ments for all the evaluation metrics. Also, it outper-
forms most translation-based models and obtains the
best scores on MRR and Hits@3. That’s to say. Our
proposedHIEcancapturemoredeepinformationthan
thepreviousneuralnetwork-basedmodelsandseman-
ticinformationthantranslation-basedandsemanticmatch-
ing based models.
•OnWN18RR,HIEachievespromisingscoresforeach
evaluationcriteria. Comparedwiththeneuralnetwork-
basedmodels,HIEobtainsa118%higherMRR,14%
higherHits@10thanConvKB,respectively. Strikingly,
itobtainsa123%higherMRR,23.5%higherHits@10
than KBGAN. Also, HIE outperforms than DistMult
andobtainsa18.2%higherHits@10. Intermsofstate-
of-the-art translation-based algorithm RotatE, HIE is
slightlyworseinHits@1. Incontrast,HIEobtainsbet-
ter scores on MRR, MR, Hits@3 and Hits@10 than
RotatE.
•OnFB15k-237,HIEisthebestmodelintermsofMRR,
Hits@1 and Hits@3. Besides, HIE obtains a 2.4%
higherMRR,6%higherHits@1,and1.9%higherHits@3
Jin Liu et al.: Preprint submitted to Elsevier Page 8 of 12HIE for the link prediction
Table 4
Results of the link prediction on WN18 and YAGO3-10 datasets. Results [*] are taken from
[10]. Results [ ] are from [12]. Other results are taken from the corresponding original
papers.
MethodWN18 YAGO3-10
MRR ~MRHits@ ~MRR ~MRHits@ ~
1 3 10 1 3 10
TransE[<]0.454 - 0.089 0.823 0.934 0.238 - 0.212 0.361 0.447
TransR[<]0.605 - 0.335 0.876 0.940 0.256 - 0.223 0.356 0.478
TransD(unif) - 229 - - 0.925 - - - - -
RotatE 0:949309 0.944 0.952 0:959 0.495 17670.402 0.550 0.670
DistMult[]0.822 902 0.728 0.914 0.936 0.340 5926 0.237 0.379 0.540
ComplEx[]0.941 - 0.936 0.945 0.947 0.355 6351 0.258 0.399 0.547
M-DCN 0:950 -0:946 0:9540.958 0.505 - 0.423 0:5870.682
InteractE - - - - - 0:5412375 0:462 -0:687
ConvE 0.942 504 0:9550:9470.935 0.523 2792 0.448 0.564 0.658
R-GCN 0.819 - 0.697 0.929 0.964 - - - - -
HIE 0.930 1310.913 0:954 0:970 0:54220420:452 0:593 0:695
Table 5
Results of the link prediction on WN18 and YAGO3-10 datasets. Results [ ] are taken
from [14]. Results [ ] are from the author’s re-publication. Other results are taken from
the corresponding original papers.
MethodWN18RR FB15k-237
MRR ~MRHits@ ~MRR ~MRHits@ ~
1 3 10 1 3 10
TransE[]0.226 - - - 0.501 0.294 357 - - 0.465
MuRP 0.477 - 0:4380.489 0.555 0.324 - 0.235 0.356 0.506
RotatE 0:4763340 0.428 0:4920:5710:338 1770:2410:375 0:533
DistMult 0.430 5110 0.390 0.440 0.490 0.241 254 0.155 0.263 0.419
TorusE 0.452 - 0.422 0.464 0.512 0.305 - 0.219 0.337 0.485
ConvKB[]0.220 2741 - - 0.508 0.302 196 - - 0.483
KBGAN 0.215 - - - 0.469 0.277 - - - 0.458
R-GCN - - - - - 0.249 - 0.151 0.264 0.417
HIE 0:48028210:430 0:499 0:580 0:346215 0:255 0:3800:523
than that of RotatE. As for MR and Hits@10, HIE is
slightly worse than RotatE.
Inashortsummary,withtheadvantagesofstructuraland
semantic information, HIE can outperform the state-of-the-
art algorithms on most of metrics.
4.6. Complex Relations Modeling
In this part, the performance of HIE on several typical
complexrelationsisreported. AccordingtoTransE[8],rela-
tions can be classiﬁed into 1-to-1, N-to-1, 1-to-N and N-to-
N.Foreachrelation,itneedstocomputetheaveragenumber
of head entities that are connected with speciﬁc tail entity
hcorand the average number of tail entities that are con-
nected with speciﬁc head entity tcsr. Also, the complex re-
lation patterns can be calculated as follows:h
n
n
l
n
njhcor<and tcsr<1*to*1
hcor<and tcsrÎN*to*1
hcorÎand tcsr<1*to*N
hcorÎand tcsrÎN*to*N(18)
where= 1.5 is adopted the same as TransE[8].
Speciﬁcally,weextractfourtypicalrelationshipsintotal
corresponding to the speciﬁc relation patterns from WN18,
i.e.,similar_to(1-to-1),member_meronym (N-to-1),part_of
(1-to-N),also_see(N-to-N).TheresultsofMRRontheabove
four diﬀerent relationships are displayed in Fig.6.
OurproposedHIEisrepresentedbyorange,andtheother
baseline models are by light orange. Representative results
can also be reported as follows:
Jin Liu et al.: Preprint submitted to Elsevier Page 9 of 12HIE for the link prediction
HIETransETransDDistMultTorusEConvEHARNComplEx0.00.20.40.60.81.0fitered MRR on WN18R
el.1-1 similar toHIETransETransDDistMultTorusEConvEHARNComplEx0.00.20.40.60.81.0R
el.1-N member meronymH
IETransETransDDistMultTorusEConvEHARNComplEx0.00.20.40.60.81.0fitered MRR on WN18R
el.N-1 part ofHIETransETransDDistMultTorusEConvEHARNComplEx0.00.10.20.30.40.50.60.7R
el.N-N also see
Figure 6: The results of part complex relations on WN18
•As can be seen from Fig.6, HIE can model the four
kinds of relations on WN18. Compared with other
models,HIEoutperformsthecomparedalgorithmswith
four relation categories.
•In addition, HIE obtains a 0.758 higher MRR than
TransEonsimilar_to,a19.3%higherMRRthanDist-
Mult onmember_meronym, a 7.6% higher MRR than
ConvEonpart_ofanda0.05higherMRRthanCom-
plEx onalso_see.
•Translation-basedandsemanticmatching-basedmod-
els can not achieve promising results due to the lack
of semantic information. However, neural network-
based methods can capture more features of entities
and relations via the convolution process. Thus, it
achieves better performance than other baselines ex-
cept for HIE. Take also_seeas an example. HARN
obtains a 0.383 higher MRR and 0.07 higher MRR
thanTransEandDistMult,respectively. However,with
theadvantageofdeeplevelinformationandsemantic
information, HIE outperforms HARN and other neu-
ral network-based models as well.
4.7. Parameter Sensitivity Analysis
4.7.1. study of 
Thecoeﬃcient 1and2areutilizedtobalancetheshal-
lowanddeeplevelinformationandmeetthecondition 1+
2= 1. Thus, we only investigated the eﬀect of 1for our
proposed HIE on MRR and Hits@10 on WN18RR. The re-
sults are illustrated in Fig.7. The orange dotted line repre-
sentstheMRRvalueofTorusE,andthegreendottedlineis
the Hits@10 value of RotatE. We can observe that:
•When the1= 0:5, HIE achieves the best perfor-
manceonMRRandHits@10,andthevaluesare0.472
and 0.579, respectively.
•Compared with TorusE, HIE obtains slightly higher
MRRandHits@10whenthe 1=0:2;0:4;0:6and0:8.
In terms of 1= 0:5, HIE outperforms TorusE by
5.5%. In addition, HIE performs as good as RotatE
when the1= 0:2;0:4and0:6, and slightly worse
thanRotatE.Also,HIEobtainsa1.4%higherHits@10
than that of RotatE.
/s48/s46/s50 /s48/s46/s52 /s48/s46/s53 /s48/s46/s54 /s48/s46/s56/s48/s46/s48/s48/s46/s49/s48/s46/s50/s48/s46/s51/s48/s46/s52/s48/s46/s53/s32/s77/s82/s82
/s32/s72/s73/s84/s83/s64/s49/s48
/s49/s77/s82/s82
/s48/s46/s48/s48/s46/s49/s48/s46/s50/s48/s46/s51/s48/s46/s52/s48/s46/s53/s48/s46/s54/s48/s46/s55/s48/s46/s56
/s32/s72/s73/s84/s83/s64/s49/s48Figure 7: The results of diﬀerent 1on MRR and Hits@10 for
WN18RR
641 282 565 121 0240.350.400.450.500.550.60b
atch size MRR 
HITS@10 
HITS@3
Figure 8: The results of diﬀerent batch size on MRR for
WN18RR
•Thechangeof 1performsaslighteﬀectonMRRand
Hits@10 of HIE. The reason is that HIE can capture
moreinformationfromdeepandshallowlevelssimul-
taneously and ensure the stability and accuracy.
4.7.2. study of batch size
To explore the inﬂuence of batch size, we set diﬀerent
sizesas64,128,256,512and1024with 1=0:5onWN18RR.
The results are reported in Fig.8. The red dotted line repre-
sentsthebestscoreonMRR,Hits@3andHits@10. Wecan
see that the outstanding results can be achieved with batch
size512. Strikingly,asthebatchsizeincreases,HIEcanob-
tainmoreinformationtoimproveitself. However,whenthe
batch size is 1024, the MRR, Hits@3 and Hits@10 of HIE
are worse than that of batch size 512, which is mainly be-
causetheproposedHIEcapturemoreandmoreinformation,
including useful information as well as invalid information,
even harmful information, to guide the optimization of the
model. Simultaneously,largerinformationmaylimittheop-
timization capacity of HIE. Thus, batch size 512 is utilized
inHIEtoprojectentitiesandrelationsintolow-dimensional
space under the premise of measuring resources and accu-
racy.
Jin Liu et al.: Preprint submitted to Elsevier Page 10 of 12HIE for the link prediction
/s77/s82/s82 /s72/s73/s84/s83/s64/s49 /s72/s73/s84/s83/s64/s51 /s72/s73/s84/s83/s64/s49/s48/s48/s46/s48/s48/s46/s49/s48/s46/s50/s48/s46/s51/s48/s46/s52/s48/s46/s53/s48/s46/s54
/s101/s118/s97/s108/s117/s97/s116/s105/s111/s110/s32/s109/s101/s116/s114/s105/s99/s115/s32/s72/s73/s69
/s32/s45/s110/s111/s95/s100/s105/s115/s116/s97/s110/s99/s101
/s32/s45/s110/s111/s95/s115/s101/s109/s97/s110/s116/s105/s99
Figure 9: The ablation results of distance and semantic mea-
surement
4.8. Ablation study
Inthispart,severalablationstudiesonWN18RRforHIE
are carried out empirically.
4.8.1. The impact of distance and semantic
measurement
Fig.9 shows the ablation study results on WN18RR of
the impact of distance and semantic measurement when we
onlyomitthedistancemeasurement(-no_distance)andomit
thesemanticmeasurement(-no_semantic)fromHIE.Ascan
be seen from Fig.9, three results can be reported as follows:
•Comparedwiththeablatedmodel,theresultsdemon-
strate HIE achieves the best performance on all the
evaluation metrics. Speciﬁcally, HIE obtains a 0.373
higher Hits@1 and a 0.21 higher Hits@10 than HIE-
no_distance and HIE-no_semantic, respectively.
•In terms of MRR and Hits@1, distance measurement
signiﬁcantly impacts the performance of HIE. Also,
HIE-no_distanceobtainsa16.2%lowerHits@3anda
2% lower Hits@10 than HIE-no_semantic.
•From the comparison of the empirical results, we can
see that HIE-no_semnatic or HIE-no_distance could
achieve promising scores on Hits@3 and Hits@10,
which is mainly because of the use of deep level in-
formation. However, due to the lack of too much po-
sition information, HIE-no_distance can not be well
constrainedindistancespace. Thus,HIE-no_distance
has poor performance on Hits@1 and MRR. In com-
parison,HIE-no_semanticisslightlyworsethanHIE.
4.8.2. The cross eﬀects of both improvements
Fig.10 shows the ablation study results on WN18RR of
the cross eﬀects of both improvements, i.e., distance mea-
surement and its deep level information, and semantic mea-
surementanditsdeeplevelinformation. Astheresultsillus-
trated in Fig.10, we can also see that:
•Distancemeasurementanddeeplevelinformationplay
a crucial role in link prediction task. In contrast, se-
/s77/s82/s82 /s72/s73/s84/s83/s64/s49 /s72/s73/s84/s83/s64/s51 /s72/s73/s84/s83/s64/s49/s48/s48/s46/s48/s48/s46/s49/s48/s46/s50/s48/s46/s51/s48/s46/s52/s48/s46/s53/s48/s46/s54
/s101/s118/s97/s108/s117/s97/s116/s105/s111/s110/s32/s109/s101/s116/s114/s105/s99/s115/s32/s72/s73/s69
/s32/s45/s110/s111/s95/s100/s105/s115/s116/s97/s110/s99/s101/s95/s100/s101/s101/s112
/s32/s45/s110/s111/s95/s115/s101/s109/s97/s110/s116/s105/s99/s95/s100/s101/s101/s112Figure 10: The ablation results of cross eﬀects of both im-
provements
manticmeasurementcanfurtherminethesemanticin-
formation and improve the ability of the model.
•In terms of MRR, HIE achieves 9% higher than HIE-
no_distance_deep, while only 5.6% higher than HIE-
no_distance. Also, without deep level information,
HIE-no_distance is even worse.
•Moreover, HIE obtains a 5.8% higher Hits@10 than
HIE-no_semantic_deep,whileonlya3.7%higherthan
thatofHIE-no_semantic,whichcanalsodemonstrate
thatdeeplevelinformationcouldhelpimprovetheabil-
ity of HIE at a higher level.
5. Conclusion
In this paper, we propose a novel KGE model to solve
thelinkpredictiontask,namely,HIE.Eachsegmentedentry
of triplet (h, r, t) is projected into the distance and semantic
spaceviathecorrespondingextractor. Inaddition,tocapture
moreﬂexiblestructuralinformation,amoregeneraltransfor-
mation operation matrix is utilized to obtain tail entity in-
stead of translation-based operation. More importantly, we
introduce hierarchical level information to learn knowledge
representationsfromhierarchicalspacesjointly. Ourexperi-
mentsonWN18,WN18RR,FB15k-237andYAGO3-10for
the link prediction task show that HIE outperforms several
state-of-the-artmethods. Besides,theresultsofthecomplex
relation modeling illustrate that HIE can better model the
fourtypicalrelationshipsthanexistingneuralnetwork-based
models. Speciﬁcally, the study of HIE further proves that
hierarchical level information can signiﬁcantly improve the
performance of HIE, and both distance and semantic mea-
surement could aﬀect the expression ability of the model.
In the future work, we will focus more on multi-task learn-
ingmethodstocapturemoreinteractiveinformationbetween
entitiesandrelationssoastoimprovetheeﬃciencyandper-
formance of HIE.
Jin Liu et al.: Preprint submitted to Elsevier Page 11 of 12HIE for the link prediction
References
[1] H. Xiong, S. Wang, M. Tang, L. Wang, X. Lin, Knowledge graph
questionansweringwithsemanticorientedfusionmodel,Knowledge-
Based Systems 221 (2021) 106954–106954.
[2] Y. Chen, H. Li, Dam: Transformer-based relation detection for ques-
tionansweringoverknowledgebase,Knowledge-BasedSystems201
(2020) 106077–106077.
[3] A. Zhao, Y. Yu, Knowledge-enabled bert for aspect-based sentiment
analysis, Knowledge-Based Systems 227 (2021) 107220–107220.
[4] Y.Zhang,X.Shi,S.Mi,X.Yang,Imagecaptioningwithtransformer
andknowledgegraph,Knowledge-BasedSystems143(2021)43–40.
[5] G. A. Miller, Wordnet: a lexical database for english, Vol. 38, ACM
New York, USA, 1995, pp. 39–41.
[6] K. Bollacker, C. Evans, P. Paritosh, T. Sturge, J. Taylor, Freebase: a
collaboratively created graph database for structuring human knowl-
edge,in: Proceedingsofthe2008ACMSIGMODinternationalcon-
ference on Management of data, 2008, pp. 1247–1250.
[7] F. M. Suchanek, G. Kasneci, G. Weikum, Yago: a core of semantic
knowledge, in: Proceedings of the 16th international conference on
World Wide Web, 2007, pp. 697–706.
[8] A. Bordes, N. Usunier, A. Garcia-Duran, J. Weston, O. Yakhnenko,
Translating embeddings for modeling multi-relational data, in: Neu-
ral Information Processing Systems (NIPS), 2013, pp. 1–9.
[9] Y. Lin, Z. Liu, M. Sun, Y. Liu, X. Zhu, Learning entity and relation
embeddings for knowledge graph completion, in: Proceedings of the
AAAI Conference on Artiﬁcial Intelligence, Vol. 29, 2015.
[10] Z. Zhang, Z. Li, H. Liu, N. N. Xiong, Multi-scale dynamic convolu-
tional network for knowledge graph embedding, IEEE Transactions
on Knowledge and Data Engineering (2020) 1–1.
[11] R. Xie, Z. Liu, M. Sun, et al., Representation learning of knowledge
graphs with hierarchical types., in: IJCAI, 2016, pp. 2965–2971.
[12] T. Dettmers, P. Minervini, P. Stenetorp, S. Riedel, Convolutional 2d
knowledge graph embeddings, in: Proceedings of the AAAI Confer-
ence on Artiﬁcial Intelligence, Vol. 32, 2018.
[13] D. Q. Nguyen, T. D. Nguyen, D. Q. Nguyen, D. Phung, A novel
embedding model for knowledge base completion based on convo-
lutional neural network, arXiv preprint arXiv:1712.02121 (2017).
[14] Z.Sun,Z.-H.Deng,J.-Y.Nie,J.Tang,Rotate: Knowledgegraphem-
beddingbyrelationalrotationincomplexspace,in: SeventhInterna-
tional Conference on Learning Representations, 2019.
[15] Z.Wang,J.Zhang,J.Feng,Z.Chen,Knowledgegraphembeddingby
translating on hyperplanes, in: Proceedings of the AAAI Conference
on Artiﬁcial Intelligence, Vol. 28, 2014.
[16] G.Ji,S.He,L.Xu,K.Liu,J.Zhao,Knowledgegraphembeddingvia
dynamicmappingmatrix,in: Proceedingsofthe53rdannualmeeting
of the association for computational linguistics and the 7th interna-
tional joint conference on natural language processing, Vol. 1, 2015,
pp. 687–696.
[17] T.Trouillon,J.Welbl,S.Riedel,É.Gaussier,G.Bouchard,Complex
embeddings for simple link prediction, in: International Conference
on Machine Learning, 2016, pp. 2071–2080.
[18] T. Ebisu, R. Ichise, Toruse: Knowledge graph embedding on a lie
group, in: Proceedings of the AAAI Conference on Artiﬁcial Intelli-
gence, Vol. 32, 2018.
[19] A. Bordes, J. Weston, R. Collobert, Y. Bengio, Learning structured
embeddings of knowledge bases, in: Proceedings of the AAAI Con-
ference on Artiﬁcial Intelligence, 2011.
[20] M. Nickel, V. Tresp, H.-P. Kriegel, A three-way model for collective
learning on multi-relational data, in: Icml, 2011.
[21] S. Amin, S. Varanasi, K. A. Dunﬁeld, G. Neumann, Lowfer: Low-
rankbilinearpoolingforlinkprediction,in: InternationalConference
on Machine Learning, 2020, pp. 257–268.
[22] B.Yang,W.-t.Yih,X.He,J.Gao,L.Deng,Embeddingentitiesandre-
lations for learning and inference in knowledge bases, arXiv preprint
arXiv:1412.6575 (2014).
[23] M.Nickel,L.Rosasco,T.Poggio,Holographicembeddingsofknowl-
edge graphs, in: Proceedings of the AAAI Conference on Artiﬁcial
Intelligence, 2016.[24] Y. Xue, Y. Yuan, Z. Xu, A. Sabharwal, Expanding holographic em-
beddings for knowledge completion., in: NeurIPS, 2018, pp. 4496–
4506.
[25] D.Poole,Simpleembeddingforlinkpredictioninknowledgegraphs,
in: NeurIPS, 2018.
[26] R. Socher, D. Chen, C. D. Manning, A. Ng, Reasoning with neural
tensornetworksforknowledgebasecompletion,in: Advancesinneu-
ral information processing systems, 2013, pp. 926–934.
[27] T. Vu, T. D. Nguyen, D. Q. Nguyen, D. Phung, et al., A capsule
network-based embedding model for knowledge graph completion
and search personalization, in: Proceedings of the 2019 Conference
oftheNorthAmericanChapteroftheAssociationforComputational
Linguistics: Human Language Technologies, Volume 1 (Long and
Short Papers), 2019, pp. 2180–2189.
[28] Sabour, Dynamic routing between capsules, in: 31st Proceedings of
the AAAI Conference on Artiﬁcial Intelligence (NIPS), 2017.
[29] S. Vashishth, S. Sanyal, V. Nitin, N. Agrawal, P. Talukdar, Interacte:
Improving convolution-based knowledge graph embeddings by in-
creasing feature interactions, in: Proceedings of the AAAI Confer-
ence on Artiﬁcial Intelligence, Vol. 34, 2020, pp. 3009–3016.
[30] Y.Liu,P.Wang,Y.Li,Y.Shao,Z.Xu,Aprile: Attentionwithpseudo
residualconnectionforknowledgegraphembedding,in: Proceedings
of the 28th International Conference on Computational Linguistics,
2020, pp. 508–518.
[31] M. Schlichtkrull, T. N. Kipf, P. Bloem, R. Van Den Berg, I. Titov,
M. Welling, Modeling relational data with graph convolutional net-
works, in: European semantic web conference, Springer, 2018, pp.
593–607.
[32] L. Cai, W. Y. Wang, Kbgan: Adversarial learning for knowledge
graph embeddings, in: Proceedings of the 2018 Conference of the
North American Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, 2018, pp. 1470–1480.
[33] S. Wang, K. Fu, X. Sun, Z. Zhang, S. Li, L. Jin, Hierarchical-aware
relation rotational knowledge graph embedding for link prediction,
Neurocomputing 458 (2021) 259–270.
[34] I.Balazevic,C.Allen,T.Hospedales,Multi-relationalpoincarégraph
embeddings, Vol. 32, Advances in Neural Information Processing
Systems, 2019, pp. 4463–4473.
[35] S. Zhang, Y. Tay, L. Yao, Q. Liu, Quaternion knowledge graph em-
beddings, arXiv preprint arXiv:1904.10281 (2019).
[36] Y. Zhao, A. Zhang, H. Feng, Q. Li, P. Gallinari, F. Ren, Knowledge
graphentitytypingvialearningconnectingembeddings,Knowledge-
Based Systems 196 (2020) 105808–105808.
[37] G. Niu, B. Li, Y. Zhang, S. Pu, J. Li, Autoeter: Automated entity
type representation for knowledge graph embedding, arXiv preprint
arXiv:2009.12030 (2020).
[38] G. A. Miller, Wordnet: a lexical database for english, Vol. 38, ACM
New York, NY, USA, 1995, pp. 39–41.
[39] F. Mahdisoltani, J. Biega, F. Suchanek, Yago3: A knowledge base
from multilingual wikipedias, in: 7th biennial conference on innova-
tive data systems research, CIDR Conference, 2014.
[40] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,
T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, et al., Pytorch: An im-
perativestyle,high-performancedeeplearninglibrary,arXivpreprint
arXiv:1912.01703 (2019).
[41] D. P. Kingma, J. Ba, Adam: A method for stochastic optimization,
arXiv preprint arXiv:1412.6980 (2014).
[42] Z. Li, H. Liu, Z. Zhang, T. Liu, N. N. Xiong, Learning knowledge
graph embedding with heterogeneous relation attention networks,
IEEETransactionsonNeuralNetworksandLearningSystems(2021)
1–1.
[43] Q. Zhang, R. Wang, J. Yang, L. Xue, Knowledge graph embedding
by translating in time domain space for link prediction, Knowledge-
Based Systems 212 (2021) 106564–106564.
Jin Liu et al.: Preprint submitted to Elsevier Page 12 of 12