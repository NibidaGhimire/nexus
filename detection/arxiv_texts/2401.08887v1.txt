arXiv:2401.08887v1  [cs.SD]  16 Jan 2024NOTSOFAR-1 Challenge:
New Datasets, Baseline, and Tasks for Distant Meeting Trans cription
Alon Vinnikov, Amir Ivry, Aviv Hurvitz, Igor Abramovski, Sh aron Koubi, Ilya Gurvich, Shai Pe‘er,
Xiong Xiao, Benjamin Martinez Elizalde, Naoyuki Kanda, Xia ofei Wang, Shalev Shaer, Stav Yagev,
Yossi Asher, Sunit Sivasankaran, Yifan Gong, Min Tang, Huam ing Wang, Eyal Krupka
Microsoft
notsofar challenge@microsoft.com
Abstract
We introduce the ﬁrst Natural Ofﬁce Talkers in Settings of Fa r-
ﬁeld Audio Recordings (“NOTSOFAR-1”) Challenge1along-
side datasets and baseline system2. The challenge focuses on
distant speaker diarization and automatic speech recognit ion
(DASR) in far-ﬁeld meeting scenarios, with single-channel and
known-geometry multi-channel tracks, and serves as a launc h
platform for two new datasets: First, a benchmarking datase t
of 315 meetings, averaging 6 minutes each, capturing a broad
spectrum of real-world acoustic conditions and conversati onal
dynamics. It is recorded across 30 conference rooms, featur -
ing 4-8 attendees and a total of 35 unique speakers. Second,
a 1000-hour simulated training dataset, synthesized with e n-
hanced authenticity for real-world generalization, incor porat-
ing 15,000 real acoustic transfer functions. The tasks focu s on
single-device DASR, where multi-channel devices always sh are
the same known geometry. This is aligned with common setups
in actual conference rooms, and avoids technical complexit ies
associated with multi-device tasks. It also allows for the d e-
velopment of geometry-speciﬁc solutions. The NOTSOFAR-1
Challenge aims to advance research in the ﬁeld of distant con -
versational speech recognition, providing key resources t o un-
lock the potential of data-driven methods, which we believe are
currently constrained by the absence of comprehensive high -
quality training and benchmarking datasets.
Index Terms : Speech recognition, speaker diarization, speech
separation, far-ﬁeld, close-talk annotation, NOTSOFAR.
1. Introduction
Conversational speech recognition remains a formidable ch al-
lenge, particularly in meeting scenarios where distant arr ay
devices are commonly employed for capturing speech. The
distance between speakers and the microphones brings about
substantial acoustic complexities including reverberati on, varia-
tions in speaker distances and volumes, and background nois es.
These complexities are further ampliﬁed by the ever-changi ng
positions of speakers and environmental conditions, alter ing the
acoustic transfer functions (ATFs) [1] between speakers an d mi-
crophones. Compounding these acoustic challenges are the d y-
namic aspects of conversations, which include speech overl ap,
interruptions, rapid changes in speakers, and non-verbal v ocal-
izations [2].
With the emergence of Large Language Models (LLMs),
conversational speech recognition has been infused with un -
precedented practical importance. LLMs fed by speech recog -
nition engines are redeﬁning user-experience with capabil ities
1https://www.chimechallenge.org/current/
task2/index .
2A GitHub link will be shared by Feb. 1st.ranging from meeting summary, note taking and sentiment
analysis, to personalized and context-aware responses to u ser
queries. The foundation of these innovative features, part icu-
larly in meeting scenarios, lies in distant speaker diariza tion and
automatic speech recognition (DASR) [3].
The ﬁeld of conversational speech recognition has seen sig-
niﬁcant advancements thanks to numerous datasets such as AM I
[4], ICSI [5], Shefﬁeld Wargames [6], ASpiRE [7], DIRHA [8],
VOiCES [9], DiPCo [10], Ego4D [11], AliMeeting [12], Lib-
riCSS [13], and challenges such as CHiME [14, 15, 16, 17, 18,
19, 20]. However, they suffer from various limitations, par tic-
ularly with respect to advancing DASR. The three most-simil ar
corpora to our work, targeting general meeting scenarios wi th
unsegmented recordings from a single device, are LibriCSS,
AMI, and AliMeetings.
LibriCSS makes a signiﬁcant step forward by including un-
segmented semi-real meetings and proposing to measure word
error rate (WER) directly rather than signal-based metrics .
They also provide matching fully simulated training data fo r
speech separation. However, since their evaluation data ha s
been obtained by playing LibriSpeech utterances via loudsp eak-
ers, it does not account for natural conversational dynamic s or
real-world acoustic complexities, such as time-varying AT Fs.
AMI makes an impressive contribution collecting 100 hours
of real meeting recordings with close-talk annotations. Ho w-
ever, its capacity to fully represent real-world complexit ies is
hampered by several limitations. The recordings, conﬁned t o
just three rooms and no more than four attendees, offer lim-
ited acoustic variety. Additionally, the development and e valu-
ation sets consist of only 18 and 16 sessions, respectively. Such
a small sample size introduces uncertainty in predicting re al-
world performance. For instance, a severe diarization erro r in
just one session could signiﬁcantly impact the overall scor e. Al-
iMeetings collects a substantial number of real meeting rec ord-
ings involving a large number of unique speakers. Yet, its fo cus
on Mandarin recordings shifts the challenge more towards la n-
guage and automatic speech recognition (ASR) aspects, rath er
than far-ﬁeld acoustics and conversational dynamics. Furt her-
more, their test set consists of only 20 sessions, facing the same
small-sample issue as AMI. Notably, neither AMI nor AliMeet -
ings include a matching simulated training set, which hinde rs
the application of data-driven front-ends.
Addressing these gaps and several others, our main contri-
butions are as follows:
•A meeting dataset for benchmarking and training : Con-
sisting of roughly 315 distinct meetings and carefully de-
signed for benchmarking purposes. Meetings were recorded
across a variety of rooms, while capturing a wide spectrum
of real-world scenarios, covering both typical and rare one s,
with up to 8 attendees per meeting. Advancing beyond pastpractices, we emphasize transcription accuracy with a mult i-
judge annotation process and avoid the use of machine-aid
to mitigate related biases. We also include detailed meta-
data with each meeting for deep-dive analysis, documenting
acoustic events and conversational aspects such as the pres -
ence of extra overlap. To the best of our knowledge, this
dataset is the ﬁrst to offer this kind of annotation. Further -
more, in our recordings we utilize both single-channel and
multi-channel devices, reﬂecting the range of typical com-
mercial recording equipment.
•A matching simulated training dataset : This approxi-
mately 1000-hour simulated training set, intended for spee ch
separation and enhancement, improves the authenticity of t he
simulation process to facilitate the application of data-d riven
methods. It incorporates 15,000 real ATFs recorded with a
geometry matching the multi-channel devices from the real
meeting dataset, clean speech is carefully ﬁltered for qual -
ity, and the mixing process is designed to exhibit real-worl d
patterns. Additionally, the supervision signals distingu ish be-
tween the direct and early reﬂections, and reverb component s
of an ATF, offering more opportunity for exploration.
•An open-source baseline system : To assist participants in
getting started, we provide a fully functional baseline sys -
tem, complete with inference, training, data handling, and
evaluation code. The system is written entirely in Python an d
consists of continuous speech separation (CSS), ASR, and
diarization modules [21, 22].
•NOTSOFAR-1 Challenge : We are launching the
NOTSOFAR-1 Challenge to pursue fundamental questions
in the ﬁeld. It features single-channel and known-geometry
multi-channel tracks, centered on single-device DASR.
The rest of this paper is organized as follows: Sections 2
and 3 discuss the challenge tracks, metrics, and scientiﬁc g oals.
Section 4 explores the benchmarking and training datasets a nd
their design considerations in detail. Section 5 presents t he
baseline system. In Section 6, we conclude.
2. Tracks and Metrics
The challenge features two main tracks centered on the use of
a single device: the “single-channel track” and the “known-
geometry multi-channel track”, with participants able to s ubmit
entries to either one or both. The use of a single device and
ﬁxed, known geometry in the multi-channel track are key dis-
tinctions from CHiME-8’s DASR challenge3. Participants re-
ceive unsegmented recordings from each meeting and are aske d
to generate time-marked, speaker-attributed transcripts . In line
with the notations in [20], each meeting is annotated with a s et
of segments, denoted as ri= (∆,s,v), where∆ = (ts,te)
are start and end times, s∈Z+is the speaker label, and
v∈Σis the transcript for the segment, with vocabulary Σ.
A meeting’s reference is fully deﬁned by the Nreference seg-
mentsR={r1,...,r N}. The participants’ task is to esti-
mate this reference through a generated hypothesis, denote d by
H={h1,...,h ˆN}. Note that the estimated speaker labels are
not required to be identical to the reference labels.
In each track, systems will be ranked according to two
metrics: (1) a speaker-attributed metric to evaluate the im -
pact of both speaker diarization errors and word errors, and
(2) a speaker-agnostic metric to evaluate word errors. For t he
ﬁrst metric we align with CHiME-8’s DASR and choose the
3https://www.chimechallenge.org/current/
task1/indextime-constrained minimum-permutation word-error rate (t cp-
WER) [23]. This is an enhanced version of the concatenated
minimum-permutation word-error rate (cpWER) [24] that in-
corporates temporal information to prevent matching words that
are temporally distant, which better reﬂects temporal accu racy
and speeds up computation compared to cpWER. However,
speaker attributed metrics such as tcpWER are highly sensit ive
to speaker diarization errors: an incorrectly attributed s egment
will be penalized even if the words within it are accurately r ec-
ognized. Thus, we include a second speaker-agnostic metric to
isolate word error impact, highlighting systems that excel in this
aspect. Additionally, we aim to promote the development of i n-
novative, practical systems rather than performance-sque ezing
approaches that are more brute-force in nature. We encour-
age participants to submit their system’s runtime and hardw are
speciﬁcations. Systems deemed practical and efﬁcient will be
featured on a dedicated leaderboard.
3. Scientiﬁc Goals
We aim to address fundamental questions in the ﬁeld of distan t
conversational speech recognition:
• How much of an advantage does a multi-channel and
geometry-speciﬁc algorithm have over a single-channel al-
gorithm?
• How much of an advantage, if any, does a geometry-speciﬁc
algorithm have over a geometry-agnostic algorithm?4
• Can the introduced simulated training dataset lead to data -
driven front-end solutions that generalize well to realist ic
acoustic scenarios?
• How can the various available supervision signals for trai n-
ing be leveraged to improve algorithms? Namely, the
separated speech components within the simulated training
dataset, along with the close-talk recordings, transcript ions,
and speaker labels found in the real meeting dataset.
• Can the data verticals analysis enabled by the metadata rev eal
potential avenues for progress?
4. Training and Benchmarking Datasets
High-quality and high-quantity datasets are critical reso urces in
advancing machine learning research. Particularly in far- ﬁeld
speech processing, we believe the progression of data-driv en
methods is currently hindered by the absence of comprehensi ve
training and benchmarking datasets. These datasets are cru cial
for unlocking the full potential of such methods and measur-
ing progress. To address this need, we introduce three key re -
sources: First, a dataset of natural meeting recordings car efully
designed for benchmarking purposes. Second, to foster inno -
vation and encourage the adoption of data-driven solutions for
speech separation and enhancement, we offer a matched sim-
ulated training dataset that bridges several gaps between t rain-
ing and testing conditions. Third, to supplement the simula ted
training set and to allow training all components of a speech
processing pipeline, we allocate a portion of the meeting da taset
for training purposes. Both datasets encompass multi-chan nel
and single-channel settings.
4We work closely with CHiME-8’s DASR challenge that offers
a geometry-agnostic multi-channel track: www.chimechallenge.
org/current/task1/index . Every geometry-agnostic system
submitted to their track will automatically yield a result i n our
geometry-speciﬁc track on the NOTSOFAR meeting dataset.Table 1: NOTSOFAR Meeting Dataset.
Feature Training Set Development Set Evaluation Set (blind)
Number of Meetings 110 meetings 35 meetings 170 meetings
Meeting Duration 6 minutes 6 minutes 6 minutes
Number of SC streams 5 streams 5 streams 6 streams
Number of MC devices 4 devices 4 devices 4 devices
Total Duration of SC
Audio Recordings55 hours
(110 meetings ×
6 minutes ×
5 SC streams)17.5 hours
(35 meetings ×
6 minutes ×
5 SC streams)102 hours
(170 meetings ×
6 minutes ×
6 SC streams)
Total Duration of MC
Audio Recordings44 hours
(110 meetings ×
6 minutes ×
4 MC devices)14 hours
(35 meetings ×
6 minutes ×
4 MC devices)68 hours
(170 meetings ×
6 minutes ×
4 MC devices)
Total Number of Rooms 20 rooms 5 rooms 13 rooms
Number of Participants 22 Participants 11 Participants 13 Participants
Language English
4.1. NOTSOFAR Recorded Meeting Dataset
This dataset comprises approximately 315 unique meetings,
each lasting on average 6 minutes, featuring authentic, mul ti-
participant English conversations recorded in about 30 dif ferent
conference rooms at Microsoft ofﬁces. To increase acoustic di-
versity, each meeting was captured with several devices, ea ch
positioned differently. This setup, per meeting, involved around
5 single-channel (SC) devices producing a single internall y pro-
cessed stream each, and 4 multi-channel (MC) devices produc -
ing 7 raw streams each. In total, this resulted in roughly 170
hours of SC data and 130 hours of MC data across all devices
and meetings. We divide the dataset into training (55 SC hour s,
44 MC hours), development (17.5 SC hours, 14 MC hours),
and evaluation sets (102 SC hours, 68 MC hours). The latter,
a subset of which will be used as the fully blind evaluation se t
in the NOTSOFAR-1 Challenge, is entirely disjoint from the
other sets, with no overlap in speakers or rooms. Tables 1 and
2 give a compact view of this dataset. Meticulously designed
and collected, the dataset possesses unique characteristi cs that
we believe make it an excellent benchmark for conversationa l
scenarios.
Capturing the full spectrum of real-world complexities :
As many practitioners observe, to truly evaluate a system’s ac-
curacy, it is essential to test it not only in typical scenari os but
also in rare events that lie at the borders of, or even beyond, its
standard operational envelope. In line with this, the recor dings
encompass a wide array of acoustic situations, including sp eak-
ers at varying distances and volumes, and different types of
transient interference and noises at low and high levels. Im por-
tantly, the dataset includes acoustic situations that some times
substantially and frequently modify the ATF between the act ive
speaker and the device. These range from in-seat movement,
walking, standing or sitting, to speaking near a whiteboard , and
entering or leaving the room. Conversational dynamics incl ude
overlapping speech from multiple speakers, interruptions , rapid
speaker change, laughter, coughing, and ﬁllers. The meetin gs
span 30 different rooms of various dimensions, layouts, con -struction materials and acoustic characteristics. Attend ees for
each meeting typically vary between 4-8 adults, involving 2 2
unique speakers in the development and training sets, and a d is-
tinct group of 13 speakers in the evaluation set, while ensur ing
balanced gender representation.
Metadata for deep-dive analysis : The errors of speech
recognition systems in conversational settings are notori ously
hard to investigate. The multitude of factors impacting the
speech pipeline are highly context-dependent and are often un-
detectable without specialized tooling or manual tagging b y
human-in-the-loop. To facilitate scalable error analysis and un-
covering a system’s blind spots, each meeting is accompanie d
by metadata, including activities and acoustic events. Som e
meetings are purposefully dominated by rare events which ar e
documented in the metadata. For example, meetings tagged as
extra noisy, or having extra overlapping speech are useful f or
testing algorithms under those conditions in an isolated ma n-
ner. Meetings where speakers are in motion or are speaking
near sound-reﬂective whiteboards while turning their head s are
useful to test the robustness of an algorithm to changes in th e
ATF. In the NOTSOFAR-1 Challenge we intend to share the
performance evaluations of submissions both averaged acro ss
all meetings and within each data vertical.
Prioritizing meetings over hours : Speech datasets often
highlight the duration of their recordings in hours, yet the util-
ity of a benchmark is determined by the distribution it spans and
the sample size it provides. Recognizing that prolonged rec ord-
ings from the same meeting offer diminishing returns in term s
of yielding new, independent information, our approach foc uses
on maximizing the diversity of the dataset by including a lar ger
number of distinct meetings. Our dataset consists of approx i-
mately 315 relatively short meetings, each lasting roughly six
minutes. Since each meeting offers a unique mix of acoustics ,
speaker identities, and conversational dynamics, we obtai n a
highly diverse sample. This allows us to assume each meeting is
approximately an independent and identically distributed (i.i.d.)
element, facilitating the computation of meaningful conﬁd ence
intervals, a vital aspect of algorithm comparison [25]. Our base-Table 2: Recording Devices for NOTSOFAR Meeting Dataset.
Device Position Array GeometrySingle-
channel
streamMulti-
channel
streamDataset
Yealink SmartVision 60 Center of table Circular 7 Microphones Yes Yes All sets
Yealink Intelligent
SpeakerCenter of table Circular 7 Microphones Yes Yes All sets
EPOS Expand Capture
5Center of table Circular 7 Microphones Yes Yes All sets
Sennheiser
TeamConnect
Intelligent SpeakerCenter of table Circular 7 Microphones Yes Yes All sets
Logitech MeetUp Front of room Linear Yes No All sets
Poly Studio Front of room Linear Yes No Evaluation set
Jabra PanaCast 50 Front of room Linear Yes NoEvaluation set
(partial)
Shure WH20 +
TASCAM DR-40X
Close TalkHead mounted
microphoneN/a Yes NoAll sets (except
evaluation set)
line code features a simple method for calculating conﬁdenc e
intervals, both in absolute terms and relative to the baseli ne al-
gorithm. Notably, the narrowness of these conﬁdence interv als
is determined by the number of meetings rather than the total
recorded hours, and beneﬁts from the large volume of meeting s
in our dataset.
Bias-free transcription process : Accurate transcription is
crucial for training and evaluating speech recognition sys tems.
One concern in this area is the potential bias toward machine -
generated transcriptions. This bias can occur when the anno ta-
tors are inﬂuenced by the output of automated systems, leadi ng
to transcriptions that are inadvertently biased toward a pa rtic-
ular system’s errors. Consider a machine-aided process whe re
a machine ﬁrst transcribes the recordings, and a human con-
ducts a second pass, deciding to accept some words and cor-
rect others. Due to the inherent ambiguities of speech, even in
close-talk recordings, and especially in situations with h eated
discussions, overlapping speech, and rapid speaker turns, some
erroneous machine suggestions may seem reasonable and henc e
may be accepted by the human annotator. While this process is
relatively efﬁcient, it perpetuates errors of the speciﬁc t ranscrip-
tion algorithm used. The implications of this bias are profo und:
competing algorithms will be penalized for deviating from t he
transcription algorithm, potentially concealing the prog ress of
new algorithms. This is particularly problematic when impr ove-
ments are incremental, as is often the case in research. Sinc e
even subtler forms of machine aid can introduce bias, our tra n-
scription process for the development and evaluation sets i s
based on close-talk recordings and conducted entirely with out
human access to machine-generated transcriptions [26].
Multi-judge annotation for accurate segmentation and
transcription : To ensure high-quality ground-truth transcrip-
tion and segmentation, attendees were equipped with close- talk
microphones. Spectral subtraction between close-talk cha nnels
was used as pre-processing for an automatic segmentation al -
gorithm, followed by human annotators who reviewed and ﬁne-
tuned any segmentation errors. Throat microphones were use dfor quality control and tuning of the algorithms. Professio nal
transcribers carried out the transcription process. Most m eet-
ings were annotated independently by two professionals, wi th a
third judge resolving non-consensus cases, except for abou t 50
meetings in the training set that involved only two annotato rs.
Close-talk recordings of the development and training sets are
also released as part of the challenge to encourage explorat ion
of creative methods, such as using pseudo labels. However,
these close-talk recordings may still contain a small amoun t of
speech leakage. Hence, to prevent any impact on ASR perfor-
mance, we recommend masking the close-talk data of each par-
ticipant by their segmented speech.
Distinguishing single-channel from single-microphone :
When evaluating speech recognition systems that process a
one-dimensional stream of audio, it is common to use audio
recorded from one of the microphones in an array. However,
in practice, speech recognition systems often deal with aud io
from existing conference room devices. These devices are ty pi-
cally equipped with microphone arrays and employ proprieta ry
on-device algorithms to process the audio, yielding what we re-
fer to in this paper as a “single-channel” output. This outpu t
is usually the result of processing all the microphones in th e
array and using a pipeline that includes echo cancellation, de-
reverberation, beamforming, and noise suppression. This s ig-
niﬁcantly differs from the single-microphone outcome. To e val-
uate performance in the single-channel setting, our record ings
include a variety of commercially available single-channe l de-
vices. Although single-microphone setups are of great scie ntiﬁc
interest, particularly when compared to multiple micropho nes,
the NOTSOFAR-1 Challenge’s single-channel track focuses o n
the practical scenario of processing audio from single-cha nnel
devices to bridge the gap to real-world applications.
A variety of recording devices : Each meeting was
recorded by multiple commercially available conference ro oms
devices, as listed in Table 2. Each device produced a single-
channel stream and most recorded an additional, multi-chan nel
stream. The multi-channel streams are recorded in raw for-mat directly from the microphone arrays without processing
and their geometry is of one central and six surrounding mi-
crophones, matching the simulated training set described i n the
next section. The purely single-channel devices have linea r
microphone-arrays with different numbers of microphones. In
the NOTSOFAR-1 Challenge, although multiple devices were
used for recording, processing during inference is restric ted
to just one of these devices. The head-mounted close-talk
microphones featured a unidirectional cardioid pickup pat tern
that provides some isolation of each speaker from their near by
speakers, allowing for more faithful transcription even in the
presence of loud overlapped speech. In every conference roo m,
we played chirps (frequency sweeps) with a loudspeaker and
recorded them with each of the multi-channel devices. These
recordings are supplied as well and may be used to compute
reverberation times, e.g., RT60 and T30 [27].
Oriented toward far-ﬁeld : The primary challenge in far-
ﬁeld speech transcription lies in creating an algorithm tha t is ro-
bust to the acoustic complexities introduced by the distanc e be-
tween the microphone and the speaker, as well as the surround -
ing environmental conditions. Some aspects of our dataset a re
designed for far-ﬁeld evaluation. Namely, we deliberately steer
away from challenges related to different languages, accen ts,
and domain-speciﬁc jargon. The recordings are exclusively in
English, spoken by individuals who are either native speake rs
or possess near-native proﬁciency. The language employed i s
predominantly semi-professional. While the meetings enco m-
pass a wide range of topics, care was taken to avoid specializ ed
jargon or obscure terminology, thereby focusing more on the
acoustic challenges rather than linguistic diversity.
A multitude of topics and interaction styles : Most meet-
ings featured semi-professional topics, in which particip ants
role-played as professionals discussing a work-related is sue.
For example, a cruise ship company planning an event, admini s-
trators planning a city park, or users complaining about IT p rob-
lems. Some meetings featured non-work-related topics, suc h as
favorite TV shows, debating whether to raise kids as vegetar i-
ans, or friends sharing recipes. We expect this role-playin g to
create a range of conversational dynamics akin to those foun d
in actual meetings in terms of speech overlap, turn-taking d y-
namics, speech disﬂuencies, interruptions and so on.
4.2. NOTSOFAR Simulated Dataset
The simulated training set consists of about 1000 hours simu -
lated with the same microphone-array geometry of one centra l
and six surrounding microphones, matching the multi-chann el
devices in the NOTSOFAR meeting dataset. It provides sepa-
rated speech and noise components as supervision signals fo r
training data-driven speech separation and enhancement me th-
ods. It is designed to foster generalization to realistic se ttings by
identifying and closing major train-test gaps prevalent in simu-
lated datasets in prior works.
Real-room acoustic transfer functions : Simulated
datasets for speech separation typically rely on the Image
Method [28] to generate simulated ATFs, which are then con-
volved with clean speech to further simulate in-room utter-
ances. Despite advancements in simulation techniques, sim u-
lated ATFs do not take into account many elements present in
the real-world, such as: the room’s layout and the unique reﬂ ec-
tive properties of its surfaces, the dependency of the reﬂec tion
coefﬁcients on frequency and angle of incidence, the reﬂect ions
from adjacent speakers, the directivity of the speech sourc e, and
the acoustic effects of the speciﬁc microphone array casing . Thelimitations of the Image Method in this context have been ex-
plored in several studies, e.g., [29, 30, 31]. To address the se
shortcomings, our dataset features ATFs recorded in actual con-
ference rooms, in an acoustic setup arranged to closely repl i-
cate authentic meeting environments. We collected a total o f
15,000 real ATFs, measured in various positions and rooms by
multiple devices sharing the same geometry. The ATFs were
reconstructed from chirps emitted by a mouth simulator.
MOS-ﬁltered clean speech : For clean speech we use the
Librivox corpus [32], which contains recordings of varying
quality. Many of these recordings are of excellent speech qu al-
ity, but others exhibit poor quality in the form of speech dis tor-
tion, background noise and reverberation. This quality var iance
compromises simulation authenticity, since the ATF convol u-
tion operation assumes true clean speech. Hence, we follow
the work of [33] and choose the upper quartile with respect to
the subjective mean-opinion score (MOS) as our clean speech
corpus, amounting to 500 hours of speech.
Augmented clean speech for realistic overlap patterns :
We observe that the Librivox clean speech corpus, comprisin g
recordings of individuals reading audiobooks, typically c onsists
of rather continuous segments of speech lasting several sec onds,
without signiﬁcant pauses. This makes it harder to achieve a
mixture reﬂecting realistic overlap patterns which includ e short
interruptions and rapid speaker turns. We therefore apply a sig-
nal processing algorithm that detects drops in speech power to
identify short pauses in speech and subsequently insert ran dom-
length silence breaks at these points. By augmenting clean
speech in this manner, we facilitate a more realistic simula tion
of conversational dynamics during the mixing stage.
Mixing up to three speakers : During the mixing process
we ﬁrst convolve the augmented clean speech utterances with
different ATFs selected from the same room. The resulting in -
dividual utterances are roughly 40 seconds long. We then app ly
a small random shift before summing utterances from 3 dis-
tinct speakers. This strategy results in mixtures that aver age
50 seconds in length and possess two attractive characteris tics.
First, the mixtures display real-world patterns such as com plex
overlaps, and rapid speaker turns featuring up to three dist inct
speakers. Qualitatively we observe that randomly sampled s eg-
ments of 2-10 seconds, as commonly utilized in speech separa -
tion training, effectively capture patterns that are typic al in real
meetings. Second, these segments tend to be more challengin g,
serving as a form of hard sample mining, which is beneﬁcial fo r
training data-driven methods. Finally, transient and stat ionary
noises recorded in real rooms with the matching geometry wer e
added to the mixture.
Separating direct and early reﬂections from late rever-
berations : Previous studies have highlighted that speech sig-
nals captured by a distant microphone are generally smeared
by reverberation, which severely degrades automatic ASR pe r-
formance [34]. Motivated by this as well as internal experi-
mentation ﬁndings, we have designed the supervision signal s in
this training set to facilitate not only speech separation t raining
but also dereverberation. We assume a room impulse response
(RIR) consists of three parts: a direct-path response, earl y re-
ﬂections, and late reverberations. Since late reverberati ons are
known to be a major cause of ASR performance degradation, we
divide the recorded ATFs (now viewed as RIRs) into two com-
ponents: direct and early reﬂections, and late reverberati ons.
This division is accomplished by applying a smoothed-out cu t-
off at the 50-millisecond mark. The mixed signal at time t, withiindexing speakers, is then represented as follows:
mixture( t) =/summationdisplay
i/bracketleftBig
spkdirect+early
i(t)+spkreverb
i(t)/bracketrightBig
+noise(t),
The individual ground truth (GT) components are provided as
supervision signals in our dataset. The baseline model in th e
NOTSOFAR-1 Challenge employs the direct and early reﬂec-
tions component as target labels for its speech separation m od-
ule. However, since the sensitivity to late reverberations de-
pends on the speciﬁc ASR system, participants are encourage d
to construct their own target labels, possibly incorporati ng the
full RIR.
Limitations : A notable limitation of the simulated train-
ing set is the static nature of the ATFs, which, despite being
recorded in real rooms, remain ﬁxed throughout each simu-
lated utterance. This fails to mirror common real-world sit -
uations where the ATF varies in time as the speaker or envi-
ronment moves. To evaluate an algorithm’s robustness again st
time-varying ATFs, one can leverage the metadata provided i n
the NOTSOFAR meeting dataset. Furthermore, we hope that
the training portion of these real recordings, which includ es in-
stances of time-varying ATFs, will contribute signiﬁcantl y to
improving an algorithm’s robustness to such variations.
5. Baseline System
The baseline recipe of NOTSOFAR-1, inspired by [21, 22], con -
sists of three steps: continuous speech separation (CSS), A SR,
and speaker diarization. In this section, we will brieﬂy des cribe
each step.
CSS: The objective of CSS is to receive an input audio
stream, consisting of either 1 or 7 channels, and convert it i nto a
set of overlap-free streams. In our case, we generate N= 3
distinct speech streams, which can support up to 3 speakers
overlapping at the same time. An effective CSS essentially
monitors the input audio stream, and when overlapping utter -
ances are found, it distributes them to the different output chan-
nels. We follow the conventional CSS framework, ﬁrst traini ng
a speech separation network with permutation invariant tra ining
(PIT) loss, which takes ﬁxed-length audio segments as input and
outputsNspeech masks and one noise mask. The masks are in
STFT (short-time Fourier transform) domain [35]. For the ne t-
work architecture we select the conformer model [13, 36].
In the inference phase, the network is applied in a block-
wise streaming fashion to overlapping ﬁxed-length segment s.
Since the order of the Nspeech mask outputs may not be con-
sistent across segments, we align every pair of adjacent seg -
ments. To estimate the best order, we consider all permuta-
tions and select the one with the lowest mean squared error
(MSE) between the masked magnitude spectrograms, calcu-
lated over the frames shared by the two segments. After stitc h-
ing theNspeech masks and one noise mask over time, we pro-
ceed to generate each of the N output streams. For the single-
channel variant, this consists of multiplying the input mix ture
by the target speech mask. For the multi-channel variant, we
rely on mask-based minimum variance distortionless respon se
(MVDR) beamforming [37]. As part of this scheme, the spatial
covariance matrices (SCMs) of the target and interference s ig-
nals are computed, where the interference signal is deﬁned a s
the sum of all non-target speakers and the background noise.
ASR : For automatic speech recognition (ASR), we em-
ploy Whisper “large-v2” [38], which supports word-level ti mes-
tamps. ASR is applied independently to each audio stream pro -
duced by CSS.Speaker Diarization : The task of the diarization module is
to assign a speaker label to every word transcribed by the ASR
module. Speaker labels are unique identiﬁers such as ‘spk0’ ,
‘spk1’, etc., and they are not required to be identical to the ref-
erence speakers. We ﬁrst apply an ofﬂine diarization method to
diarize each of the audio streams produced by CSS. Then, we
assign each word to its speaker label based on its word bounda ry
information and the diarization output from the source stre am.
For ofﬂine diarization, we adopted the diarization recipe
of the NeMo toolkit [39]. There are two conﬁgurations sup-
ported, including “nmesc” which performs ofﬂine clusterin g us-
ing the normalized maximum eigengap-based spectral cluste r-
ing (NME-SC) algorithm [40], and “nmesc-msdd” which per-
forms NME-SC followed by the multi-scale diarization decod er
(MSDD) [41].
To assign a speaker label to an ASR word, we look up the
active speakers within its time boundaries in the diarizati on out-
put from the corresponding audio stream. In most cases, ther e
is only one active speaker within the word’s boundaries, and
it is assigned to the word. If there is no active speaker withi n
the word’s boundaries (i.e. diarization didn’t detect spee ch), the
speaker label of the nearest word in time is assigned. If ther e are
multiple active speakers within the word’s boundaries (i.e . di-
arization detected overlapped speech), the speaker who is a ctive
for the longest duration is assigned.
Note that the sole distinction between the single-channel
and multi-channel variants of our system is in the CSS mod-
ule which processes either 1 channel or 7 channels as its inpu t,
applying mask multiplication or MVDR respectively.
6. Conclusions
The NOTSOFAR-1 Challenge marks a signiﬁcant stride in the
ﬁeld of DASR. It provides a 315-meeting benchmarking datase t
that captures the nuances and complexities of real-world me et-
ings. The benchmark includes a large volume of distinct meet -
ings, enabling reliable performance analysis of algorithm s. An-
notations were collected by human listeners, providing gro und-
truth annotations with low WER. The challenge also includes
a 1000-hour simulated training data set, crafted to leverag e
known microphone array geometry, and mitigate the mismatch
between training and testing conditions. These contributi ons
collectively aim to stimulate further research and innovat ion,
and to push the boundaries of what is scientiﬁcally possible in
the ﬁeld of DASR.
7. Acknowledgements
We express our profound gratitude to Hadas Eilon Carmi, Ross
Cutler, Prem Premkumar, Solomiya Branets, and the IC3-AI
team at Microsoft for their invaluable assistance.
8. References
[1] J. Chung and D. Blaser, “Transfer function method of meas uring
in-duct acoustic properties. i. theory,” The Journal of the Acousti-
cal Society of America , vol. 68, no. 3, pp. 907–913, 1980.
[2] E. Skudrzyk, The foundations of acoustics: basic mathematics
and basic acoustics . Springer Science & Business Media, 2012.
[3] A. Sellen and E. Horvitz, “The rise of the AI Co-Pilot:
Lessons for design from aviation and beyond,” arXiv preprint
arXiv:2311.14713 , 2023.
[4] W. Kraaij, T. Hain, M. Lincoln, and W. Post, “The AMI meet-
ing corpus,” in Proc. International Conference on Methods and
Techniques in Behavioral Research , 2005.[5] R. Dhillon, S. Bhagat, H. Carvey, and E. Shriberg, “Meeti ng
recorder project: Dialog act labeling guide,” ICSI Technic al Re-
port TR-04, Tech. Rep., 2004.
[6] C. Fox, Y . Liu, E. Zwyssig, and T. Hain, “The Shefﬁeld warg ames
corpus,” in Proc. Interspeech . ISCA, 2013.
[7] M. Harper, “The automatic speech recogition in reverber ant en-
vironments (ASpIRE) challenge,” in Proc. 2015 IEEE Workshop
on Automatic Speech Recognition and Understanding (ASRU) .
IEEE, 2015, pp. 547–554.
[8] L. Cristoforetti, M. Ravanelli, M. Omologo, A. Sosi, A. A bad,
M. Hagm¨ uller, P. Maragos et al. , “The DIRHA simulated corpus,”
inProc. LREC , 2014, pp. 2629–2634.
[9] C. Richey, M. A. Barrios, Z. Armstrong, C. Bartels, H. Fra nco,
M. Graciarena, A. Lawson, M. K. Nandwana, A. Stauffer, J. van
Hout et al. , “V oices obscured in complex environmental settings
(voices) corpus,” arXiv preprint arXiv:1804.05053 , 2018.
[10] M. Van Segbroeck, A. Zaid, K. Kutsenko, C. Huerta, T. Ngu yen,
X. Luo, B. Hoffmeister, J. Trmal, M. Omologo, and R. Maas,
“DiPCo–dinner party corpus,” arXiv preprint arXiv:1909.13447 ,
2019.
[11] K. Grauman, A. Westbury, E. Byrne, Z. Chavis, A. Furnari ,
R. Girdhar, J. Hamburger, H. Jiang, M. Liu, X. Liu et al. , “Ego4d:
Around the world in 3,000 hours of egocentric video,” in Proc.
IEEE/CVF Conference on Computer Vision and Pattern Recogni -
tion, 2022, pp. 18 995–19 012.
[12] F. Yu, S. Zhang, Y . Fu, L. Xie, S. Zheng, Z. Du, W. Huang, P. Guo,
Z. Yan, B. Ma et al. , “M2MeT: The ICASSP 2022 multi-channel
multi-party meeting transcription challenge,” in Proc. ICASSP .
IEEE, 2022, pp. 6167–6171.
[13] Z. Chen, T. Yoshioka, L. Lu, T. Zhou, Z. Meng, Y . Luo, J. Wu ,
X. Xiao, and J. Li, “Continuous speech separation: Dataset a nd
analysis,” in Proc. ICASSP . IEEE, 2020, pp. 7284–7288.
[14] N. Ma, J. Barker, H. Christensen, and P. Green, “A hearin g-
inspired approach for distant-microphone speech recognit ion in
the presence of multiple sources,” Computer Speech & Language ,
vol. 27, no. 3, pp. 820–836, 2013.
[15] E. Vincent, J. Barker, S. Watanabe, J. Le Roux, F. Nesta, and
M. Matassoni, “The second ‘CHiME’ speech separation and
recognition challenge: Datasets, tasks and baselines,” in Proc.
ICASSP . IEEE, 2013, pp. 126–130.
[16] J. Barker, R. Marxer, E. Vincent, and S. Watanabe, “The t hird
‘CHiME’ speech separation and recognition challenge: Anal ysis
and outcomes,” Computer Speech & Language , vol. 46, pp. 605–
626, 2017.
[17] E. Vincent, S. Watanabe, J. Barker, and R. Marxer, “The 4 th
CHiME speech separation and recognition challenge,” URL:
http://spandh. dcs. shef. ac. uk/chime challenge {Last Accessed
on 1 August, 2018 }, 2016.
[18] J. Barker, S. Watanabe, E. Vincent, and J. Trmal, “The ﬁf th
’CHiME’ speech separation and recognition challenge: data set,
task and baselines,” arXiv preprint arXiv:1803.10609 , 2018.
[19] S. Watanabe, M. Mandel, J. Barker, E. Vincent, A. Arora,
X. Chang, S. Khudanpur, V . Manohar, D. Povey, D. Raj et al. ,
“CHiME-6 challenge: Tackling multispeaker speech recogni tion
for unsegmented recordings,” arXiv preprint arXiv:2004.09249 ,
2020.
[20] S. Cornell, M. Wiesner, S. Watanabe, D. Raj, X. Chang, P. Gar-
cia, Y . Masuyama, Z.-Q. Wang, S. Squartini, and S. Khudan-
pur, “The CHiME-7 DASR challenge: Distant meeting transcri p-
tion with multiple devices in diverse scenarios,” arXiv preprint
arXiv:2306.13734 , 2023.
[21] T. Yoshioka, I. Abramovski, C. Aksoylar, Z. Chen, M. Dav id,
D. Dimitriadis, Y . Gong, I. Gurvich, X. Huang, Y . Huang et al. ,
“Advances in online audio-visual meeting transcription,” inProc.
2019 IEEE Automatic Speech Recognition and Understanding
Workshop (ASRU) . IEEE, 2019, pp. 276–283.[22] D. Raj, P. Denisov, Z. Chen, H. Erdogan, Z. Huang, M. He,
S. Watanabe, J. Du, T. Yoshioka, Y . Luo et al. , “Integration of
speech separation, diarization, and recognition for multi -speaker
meetings: System description, comparison, and analysis,” inProc.
2021 IEEE spoken language technology workshop (SLT) . IEEE,
2021, pp. 897–904.
[23] T. von Neumann, C. Boeddeker, M. Delcroix, and R. Haeb-
Umbach, “MeetEval: A toolkit for computation of word error
rates for meeting transcription systems,” in Proc. CHiME-2023
Workshop, Dublin, England , 2023.
[24] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembe k,
N. Goel, M. Hannemann, P. Motlicek, Y . Qian, P. Schwarz et al. ,
“The Kaldi speech recognition toolkit,” in Proc. IEEE 2011 work-
shop on automatic speech recognition and understanding , no.
CONF. IEEE Signal Processing Society, 2011.
[25] M. E. Payton, M. H. Greenstone, and N. Schenker, “Overla p-
ping conﬁdence intervals or standard error intervals: what do they
mean in terms of statistical signiﬁcance?” Journal of Insect Sci-
ence, vol. 3, no. 1, p. 34, 2003.
[26] M. Levit, Y . Huang, S. Chang, and Y . Gong, “Don’t count on ASR
to transcribe for you: Breaking bias with two crowds.” in Proc.
Interspeech , 2017, pp. 3941–3945.
[27] A. Matt and D. Stowell, “Estimating & mitigating the imp act
of acoustic environments on machine-to-machine signallin g,” in
Proc. 2019 27th European Signal Processing Conference (EU-
SIPCO) . IEEE, 2019, pp. 1–5.
[28] J. B. Allen and D. A. Berkley, “Image method for efﬁcient ly sim-
ulating small-room acoustics,” The Journal of the Acoustical So-
ciety of America , vol. 65, no. 4, pp. 943–950, 1979.
[29] L. Savioja and U. P. Svensson, “Overview of geometrical room
acoustic modeling techniques,” The Journal of the Acoustical So-
ciety of America , vol. 138, no. 2, pp. 708–730, 2015.
[30] F. Brinkmann, L. Asp¨ ock, D. Ackermann, S. Lepa, M. V orl ¨ ander,
and S. Weinzierl, “A round robin on room acoustical simulati on
and auralization,” The Journal of the Acoustical Society of Amer-
ica, vol. 145, no. 4, pp. 2746–2760, 2019.
[31] E. De Sena, N. Antonello, M. Moonen, and T. Van Waterscho ot,
“On the modeling of rectangular geometries in room acoustic sim-
ulations,” IEEE/ACM Transactions on Audio, Speech, and Lan-
guage Processing , vol. 23, no. 4, pp. 774–786, 2015.
[32] J. Kearns, “Librivox: Free public domain audiobooks,” Reference
Reviews , vol. 28, no. 1, pp. 7–8, 2014.
[33] C. K. Reddy, V . Gopal, R. Cutler, E. Beyrami, R. Cheng,
H. Dubey, S. Matusevych, R. Aichner, A. Aazami, S. Braun
et al. , “The interspeech 2020 deep noise suppression challenge:
Datasets, subjective testing framework, and challenge res ults,”
arXiv preprint arXiv:2005.13981 , 2020.
[34] K. Lebart, J.-M. Boucher, and P. N. Denbigh, “A new metho d
based on spectral subtraction for speech dereverberation, ”Acta
Acustica united with Acustica , vol. 87, no. 3, pp. 359–366, 2001.
[35] L. Durak and O. Arikan, “Short-time Fourier transform: two fun-
damental properties and an optimal implementation,” IEEE Trans-
actions on Signal Processing , vol. 51, no. 5, pp. 1231–1242, 2003.
[36] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y . Zhang, J. Yu, W. Han,
S. Wang, Z. Zhang, Y . Wu et al. , “Conformer: Convolution-
augmented transformer for speech recognition,” arXiv preprint
arXiv:2005.08100 , 2020.
[37] E. A. Habets, J. Benesty, S. Gannot, and I. Cohen, “The
MVDR beamformer for speech enhancement,” in Speech Pro-
cessing in Modern Communication: Challenges and Perspecti ves.
Springer, 2010, pp. 225–254.
[38] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, an d
I. Sutskever, “Robust speech recognition via large-scale w eak su-
pervision,” 2022.
[39] O. Kuchaiev, J. Li, H. Nguyen, O. Hrinchuk, R. Leary, B. G ins-
burg, S. Kriman, S. Beliaev, V . Lavrukhin, J. Cook, P. Caston guay,
M. Popova, J. Huang, and J. M. Cohen, “NeMo: a toolkit for
building ai applications using neural modules,” 2019.[40] T. J. Park, K. J. Han, M. Kumar, and S. Narayanan, “Auto-t uning
spectral clustering for speaker diarization using normali zed max-
imum eigengap,” IEEE Signal Processing Letters , vol. 27, pp.
381–385, 2019.
[41] T. J. Park, N. R. Koluguri, J. Balam, and B. Ginsburg, “Mu lti-scale
speaker diarization with dynamic scale weighting,” arXiv preprint
arXiv:2203.15974 , 2022.