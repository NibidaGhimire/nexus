Optimal Sparse Sliced Inverse Regression via Random
Projection
Jia Zhang∗Runxiong Wu†Xin Chen‡
August 4, 2023
Abstract
We propose a novel sparse sliced inverse regression method based on random pro-
jections in a large psmall nsetting. Embedded in a generalized eigenvalue framework,
the proposed approach finally reduces to parallel execution of low-dimensional (gener-
alized) eigenvalue decompositions, which facilitates high computational efficiency. The-
oretically, we prove that this method achieves the minimax optimal rate of convergence
under suitable assumptions. Furthermore, our algorithm involves a delicate reweighting
scheme, which can significantly enhance the identifiability of the active set of covariates.
Extensive numerical studies demonstrate high superiority of the proposed algorithm in
comparison to competing methods.
Keywords: Sufficient dimension reduction; Sparse sliced inverse regression; Random pro-
jection; High-dimensional statistics; Minimax
∗School of Statistics, Southwestern University of Finance and Economics, Chengdu 611130, China.
†Co-first author. College of Engineering, University of California, Davis, Davis, CA 95616, USA
‡Corresponding author. Department of Statistics and Data Science, Southern University of Science and
Technology, Shenzhen 518055, China; Email: chenx8@sustech.edu.cn .
1arXiv:2305.05141v2  [stat.ME]  3 Aug 20231 Introduction
Faced with a large number of covariates in various modern applications, Sufficient Dimen-
sion Reduction (SDR) provides a statistical framework to reduce the dimension of the prob-
lem without loss of information through seeking low-dimensional linear combinations of the
original predictors. In a regression problem involving a univariate response Y∈Rand a
p-dimensional predictor vector X= (X1, . . . , X p)⊤∈Rp, SDR aims to find a dimension
reduction subspace of Rpwith a basis Bsuch that
Y⊥ ⊥X|B⊤X, (1.1)
where ⊥ ⊥stands for statistical independence. Dimension reduction subspaces are generally
not unique, so the primary interest of SDR lies in the intersection of all the dimension re-
duction subspaces, which enjoys minimum dimensionality and is itself a dimension reduction
subspace under mild conditions (Cook, 1996). We call the intersection the central subspace
and denote it as SY|X. Let β∈Rp×dbe a basis of SY|X, and then β⊤Xcaptures the full
regression relationship of YonX. Since the dimension of the central subspace dis usually
much smaller than p, then we can use the low-dimensional βTXto predict Ywithout loss
of any information.
Many SDR methods have been proposed to estimate the central subspace SY|X1, among
which the pioneering Sliced Inverse Regression (SIR) (Li, 1991) enjoys high popularity due
to its simplicity, generality and computational efficiency. Like most SDR methods, SIR has
been proved to be successful in traditional settings where the dimension of the predictors p
is fixed or diverges slowly with the sample size n(Li, 1991, Hsing and Carroll, 1992, Zhu
and Ng, 1995, Zhu et al., 2006). However, when p≍norp≫n, which is quite common
in modern datasets, SIR breaks down in both theoretical and computational aspects (Hung
and Huang, 2019, Lin et al., 2018, Lin et al., 2019). Indeed, Lin et al. (2018) showed that
the SIR estimator of the central subspace is consistent if and only if p/ngoes to zero under
mild conditions.
To remedy this situation, and to further facilitate interpretability and model parsimony,
1See Yin (2011) and Ma and Zhu (2013) for through reviews of SDR methods.
2a reasonable sparsity condition is imposed to restrict the number sof the active predictors
in the regression. Lin et al. (2018) proved that the optimal rate for estimating the central
subspace should be (slogp)/nup to some constants in a setting where Cov(X) =Ip, and
proposed a diagonal thresholding algorithm for the single index model which achieves the
optimal rate when Cov(X) =Ip. In a follow-up work, Lin et al. (2019) introduced an
efficient Lasso variant of SIR for multiple index models with a general covariance matrix of
X. However, this method was shown to be rate optimal when pis of order o(n2c2), where c
is the generalised signal-to-noise ratio. To fulfil the theoretical gap as well as the restriction
on the covariance matrix, Tan et al. (2020) proposed an adaptive estimation scheme for
sparse SIR, which was computationally tractable and rate optimal even in the case where
log(p) =o(n).
In this paper, we propose a novel efficient Sparse SIR method via Random Projection
(SSIRvRP) for large psmall nproblems. This approach can attain the minmax optimal rate
when log(p) =o(n)under mild assumptions and enjoys high computational efficiency. Com-
pared with existing algorithms, our SSIRvRP algorithm exhibits marked simplicity, robust-
nesstopoorinitialization, andscalabilitythrougheasyparallelization. Indeed, SSIRvRPcan
be implemented through the parallel execution of low-dimensional (generalized) eigenvalue
decompositions, thereby avoiding directly computing the inverse of the sample covariance
matrix or the eigenvectors of the sample conditional covariance matrix and selecting multi-
ple tuning parameters appeared in existing sparse SIR methods. Additionally, it does not
even require computing the (conditional) sample covariance matrix itself, since it suffices
to extract its principal submatrices, which can be computed from the low-dimensional pro-
jected data. Finally, the algorithm can be readily extended to other SDR methods via the
generalized eigenvalue formulation (Li, 2007, Hung and Huang, 2019). Extensive numerical
experiments demonstrate the superior performance of SSIRvRP in comparison to competing
approaches.
When Cov(X) =Ip, the proposed algorithm would reduce to the sparse PCA algorithm
proposed by Gataric et al. (2020). However, our algorithm is absolutely not a simple and
straightforward extension from eigenvalue decomposition to generalized eigenvalue decompo-
sition, which can be reflected from the following three aspects. Firstly and most importantly,
3the theoretical evidences behind the proposed algorithm are drastically different from that
of Gataric et al. (2020). Our work fulfils the theoretical gap between the standard and
generalized eigenvalue decompositions, at least in terms of random projection based algo-
rithms, mainly via wisely utilizing the Cholesky transformation. Secondly, as a generalized
eigenvalue problem, the sparse SIR has some unique features, especially in calculation. See
Section 2.2 for details. Lastly, we embed a reweighting scheme into the proposed algorithm,
which significantly improves the ability to identify the active covariates.
Related literature . Random projection techniques have played a role in designing
dimension reduction methods. To name a few, Qi and Hughes (2012) and Anaraki and
Hughes (2014) proposed computing the leading eigenvector of the sample covariance matrix
through an ensemble of low-dimensional random projections of the data. Gataric et al.
(2020) considered sparse principal component analysis via axis-aligned random projections
in lager psmall nsettings, from where we got inspiration for our approach. Hung and
Huang(2019)introducedaso-calledintegratedrandom-partitionSDRmethodbyintegrating
multiple sketches of the central subspace obtained from random partitions of the covariates.
Recently, Liu et al. (2023) proposed a random projection approach to hypothesis tests in
high-dimensional single index models.
We note that Tan et al. (2018) suggested tackling the sparse generalized eigenvalue prob-
lem via a truncated Rayleigh flow method. However, this method can only be employed
to estimate the leading generalized eigenvector, which limits its application as an SDR ap-
proach. For more discussion on sparse SDR methods, please refer to Li et al. (2020) for a
through review.
Notation . We introduce some notation used throughout the paper. For an integer
n > 0, let [n] :={1,2, . . . , n }andEn(X) := n−1Pn
i=1Xifor a random vector Xwith a
sample {Xi}n
i=1. For a vector α∈Rp, denote its j-th component by α(j), and for S⊂[p], let
α(S)denote a subvector of αwith components indexed in S. Write α’s Euclidean norm by
∥α∥2. For a matrix U∈Rp×d, letU(i,j)denote its (i, j)-th entry, U(i,·)denote its i-th row,
andU(·,j)itsj-th column. For S⊆[p]andS′⊆[d], write U(S,S′)be the |S|×|S′|submatrix
with row indexes in Sand column indexes in S′, and simplify U([p],S′)andU(S,[d])byU(·,S′)
andU(S,·), respectively. Let ∥U∥Fand∥U∥opdenote its Frobenius norm and operator norm,
4respectively.
For any index set J⊆[p],PJsignifies the projection matrix which is a p×pdiagonal
matrix with the j-th diagonal entry being 1{j∈J}. For a real symmetric matrix pair (A,B)
inRp×p×Rp×p, let λ1(A,B)≥λ2(A,B)≥. . .≥λp(A,B)be generalized eigenvalues in
decreasing order, and v1(A,B), . . . ,vp(A,B)be the corresponding eigenvectors such that
Avi(A,B) =λi(A,B)Bvi(A,B)
for any i∈[p].
Organization of the paper . The rest of the paper is organized as the follows. In
Section 2, we propose a sparse SIR estimator via random projection, whose theoretical
properties are investigated in Section 3. A reweighting scheme is added to improve the
efficiency of the proposed algorithm in Section 4, and Section 5 discusses the selection of
the hyperparameters for the improved algorithm. Numerical experiments are conducted in
Section 6, and in Section 7 we applied the proposed method to analyse a gene expression
data. Section 8 concludes the paper. All the technical proofs are deferred to the Appendix.
2 Method
2.1 Sparse SIR revisited
Define the discretized version of Yas
˜Y=HX
h=1h·I{Y∈Jh},
where {J1, J2, . . . , J H}is a measurable partition of the sample space of Y. IfH≥d+ 1,
we know that ˜Ycan be used to identify SY|Xinstead of Ywith no loss of information
(Bura and Cook, 2001, Cook and Forzani, 2009). Then the SIR procedure is actually a
generalized eigenvalue decomposition problem of the kernel matrix ΣE(X|Y)= Cov {E(X|˜Y)}
5with respect to Σ= Cov( X),i.e.,
ΣE(X|Y)βi=λiΣβiwithβ⊤
iΣβj=1{i=j}, (2.1)
where i, j= 1,···, p, and λ1≥ ··· ≥ λd>0 =λd+1=···=λp. The first dgeneralized
eigenvectors {β1, . . . ,βd}corresponding to the nonzero generalized eigenvalues λ1≥ ··· ≥
λdform a basis of SY|X. Thus, solving the following optimization problem yields a basis
β= (β1, . . . ,βd)ofSY|X:
β= argmax
B∈Rp×dtr
B⊤ΣE(X|Y)B	
s.t.B⊤ΣB=Id. (2.2)
To interpret the extracted components well, it is often encouraged to perform variable
selection for SIR. The goal of variable selection is to seek the smallest subset of the predictors
X(A), with partition X={(X(A))⊤,(X(Ac))⊤}⊤, such that
Y⊥ ⊥X|X(A), (2.3)
where A ⊆ [p]denotes the truly relevant predictor set and Acdenotes the irrelevant predictor
set (Li et al., 2005, Bondell and Li, 2009). Following the partition of X, one can partition
βaccordingly as
β=
β(A,·)
β(Ac,·)
,β(A,·)∈R|A|×d,β(Ac,·)∈R(p−|A| )×d,
where |A|is the cardinality of A. Then (2.3)implies that β(Ac,·)=0(Bondell and Li, 2009).
Letting supp (β) ={j∈[p] :β(j,·)̸=0⊤}be the row support of β, then supp (β) =Ain
the above partition. Assuming |A| ≤ s, sparse SIR is further defined based on (2.2)through
seeking βsuch that
β= argmax
B∈Rp×dtr
B⊤ΣE(X|Y)B	
,
s.t.B⊤ΣB=Idand|supp(B)| ≤s .(2.4)
6A natural sparse SIR estimator can be obtained by solving
ˇβ= argmax
B∈Rp×dtrn
B⊤bΣE(X|Y)Bo
,
s.t.B⊤bΣB=Idand|supp(B)| ≤s ,(2.5)
where bΣE(X|Y)andbΣare the sample covariance matrices of the conditional expectation
E(X|˜Y)andX, respectively. Tan et al. (2020) proved that this natural estimator is rate
optimal under various commonly used loss functions. However, solving the problem (2.5)
directly is computationally infeasible as it would require exhaustive search over all B∈Rp×d
subjecttothesparsityconstraint. Toremedythisproblem, Tanetal.(2020)furtherproposed
a refined three-steps estimator based on the work of Gao et al. (2017). In the following, we
draw inspiration from Gataric et al. (2020) and develop another computationally feasible
and much simpler estimator based on random projections that achieves optimal statistical
rate.
2.2 A sparse SIR estimator via random projection
Fork∈[p], letPk:={PS:S⊆[p],|S|=k}be the set of k-dimensional projections. Our
method is described as follows. For two fixed integers A, B∈N, we independently and
uniformly generate A×Bprojections {Pa,b:a∈[A], b∈[B]}fromPk. We can treat these
projections as Agroups, each with cardinality B. For each a∈[A], let
b∗(a) :=sargmax
b∈[B]dX
i=1λi(Pa,bbΣE(X|Y)Pa,b, Pa,bbΣPa,b)
denote the index of the selected projection within the a-th group, where sargmax denotes
the smallest element in the lexicographic ordering for those argmax values.
Consider the oracle case where Pa,b∗(a)=PAwithA= supp( β). Then (2.4) reduces to
seeking βsuch that
β= argmax
B∈Rp×dtr
B⊤PAΣE(X|Y)PAB	
,
s.t.B⊤PAΣPAB=Id,(2.6)
7which implies that βcan be formed by the leading generalized eigenvectors of the pair
(PAΣE(X|Y)PA, PAΣPA). Hence, the basic idea is to construct a set ˆSsuch that A ⊆ ˆSwith
high probability. This explains why b∗(a)is defined as stated above, to a certain extent.
Next,inordertoaggregatetheinformationofall Aestimators,wecomputeanimportance
score ˆw(j)for the j-th variable, which is defined as
ˆw(j):=1
AAX
a=1dX
i=1(ˆλa,b∗(a);i−ˆλa,b∗(a);d+1)(ˆv(j)
a,b∗(a);i)2,
where ˆλa,b∗(a);iis the i-th generalized eigenvalue of the matrix pair (Pa,b∗(a)bΣE(X|Y)Pa,b∗(a),
Pa,b∗(a)bΣPa,b∗(a)),and ˆv(j)
a,b∗(a);iisthe j-thelementofthecorrespondinggeneralizedeigenvector.
This means that we take account, not just of the frequency with which each co-ordinate is
chosen, but also their corresponding magnitudes in the selected eigenvector, as well as an
estimate of the signal strength. The estimation procedure is summarized as the following
algorithm and we name it as SSIRvRP (Sparse SIR via Random Projections).
Algorithm 1: pseudocode of the SSIRvRP algorithm for central subspace estima-
tion
Input: bΣE(X|Y),bΣ, A, B ∈N, k, l∈[p], d∈[k]
1Generate {Pa,b:a∈[A], b∈[B]}independently and uniformly from Pk
2fora= 1,···, Ado
3 forb= 1,···, Bdo
4 fori∈[d+ 1], compute ˆλa,b;i:=λi(Pa,bbΣE(X|Y)Pa,b, Pa,bbΣPa,b)and the
corresponding generalized eigenvector ˆva,b;i=vi(Pa,bbΣE(X|Y)Pa,b, Pa,bbΣPa,b)
with ˆλa,b;k+1= 0
5 end
6Compute b∗(a) :=sargmaxb∈[B]Pd
i=1ˆλa,b;i
7end
8Compute bw= ( ˆw(1),···,ˆw(p))⊤with
ˆw(j):=1
AAX
a=1dX
i=1(ˆλa,b∗(a);i−ˆλa,b∗(a);d+1)(ˆv(j)
a,b∗(a);i)2, j∈[p]
9LetˆS⊆[p]be the index set of the llargest components of bw
Output: bβ= (ˆv1, . . . , ˆvd), where ˆv1, . . . , ˆvdare the top dgeneralized eigenvectors of
(PˆSbΣE(X|Y)PˆS, PˆSbΣPˆS)
8In Algorithm 1, bΣ=En[{X−En(X)}{X−En(X)}⊤]and
bΣE(X|Y)=HX
h=1ˆph{Eh(X|˜Y=h)−En(X)}{Eh(X|˜Y=h)−En(X)}⊤,(2.7)
where ˆph=En[1{˜Y=h}]andEh(X|˜Y=h) = ˆp−1
hn−1Pn
i=1Xi1{˜Yi=h}. The positive
integers A,B,k,landdare hyperparameters of the proposed algorithm, whose choices will
be analysed in the following theoretical and numerical studies.
Remark 1.Another method to estimate bΣE(X|Y)is to utilize the identity Cov{E(X|Y)}=
Cov(X)−E{Cov(X|Y)}. Then, we can estimate bΣE(X|Y)=bΣ−bT, where
bT=1
HHX
h=11
nhX
i∈Sh(Xi−¯XSh)(Xi−¯XSh)⊤
, (2.8)
where S1, . . . , S Hcontains the sample indexes associated with the partitioned Yaccording
to its scale, nhdenotes the sample size of the slice Sh, and ¯XShdenotes the sample mean of
this slice. This estimator works as well as the one given above in our numerical experiments.
Notice that if Σ=Ip, Algorithm 1 reduces to the algorithm proposed by Gataric et al.
(2020) for sparse principal component analysis. However, our algorithm is absolutely not
a simple and straightforward extension from eigenvalue decomposition to generalized eigen-
value decomposition, which can be reflected from the following aspects. Firstly and most
importantly, the theoretical evidences behind the proposed algorithm are drastically different
from that of Gataric et al. (2020). It is well known that the generalized eigenvalue problem
is, in principle, more difficult than the standard one, in both theoretical and computational
sides. Several frequently used techniques in standard eigenvalue decompositions, like spec-
tral decomposition, have no counterparts in generalized eigenvalue decomposition problems,
which brings great challenges to the theoretical analysis of Algorithm 1. Our work fulfils
the gap between the standard and generalized eigenvalue decompositions, at least in terms
of random projection based algorithms. See Sections 3.2 and 9.2 for theoretical details.
Secondly, as a generalized eigenvalue problem, the sparse SIR has some unique features,
especially in calculation. See the following discussion.
Step 4 of Algorithm 1 involves solving a generalized eigenvalue problem with a non-
9negative definite matrix pair (Pa,bbΣE(X|Y)Pa,b, Pa,bbΣPa,b), which is different from common
generalized eigenvalue problems where the second matrix in the pair is positive definite (Li,
2007, Tan et al., 2018, Hung and Huang, 2019). The nonnegativeness of the second matrix
Pa,bbΣPa,bwould lead to multiple solutions of generalized eigenvectors. To see this clearly,
letSdenote the set of the krow indexes corresponding to the nonzero diagonal elements of
Pa,b. Then without loss of generality, the matrix pair turns out to be (PSbΣE(X|Y)PS, PSbΣPS)
with
PSbΣE(X|Y)PS=
bΣ(S,S)
E(X|Y)0
0 0
, PSbΣPS=
bΣ(S,S)0
0 0
,
and the generalized eigenvalue problem in Step 4 with respect to this pair reduces to
bΣ(S,S)
E(X|Y)ˆv(S)
i=ˆλibΣ(S,S)ˆv(S)
iwith ˆv(S),⊤
ibΣ(S,S)ˆv(S)
j=1{i=j} (2.9)
fori, j= 1, . . . , k, where ˆλi=λi(PSbΣE(X|Y)PS, PSbΣPS)and ˆvi= (ˆv(S),⊤
i,ˆv(Sc),⊤
i )⊤=
vi(PSbΣE(X|Y)PS, PSbΣPS). Since (2.9) does not impose any restriction for ˆv(Sc)
i, then the
leading kgeneralized eigenvectors of the original problem have infinitely many solutions. To
remedy this situation, we choose ˆvi= (ˆv(S),⊤
i,0⊤)⊤fori∈[k]for the generalized eigenvalue
problem in Step 4. This point is quite different from the eigenvalue decomposition of PSbΣPS,
which would naturally yield sparse eigenvectors as shown in Gataric et al. (2020).
It remains to solve the reduced low-dimensional generalized eigenvalue problem (2.9),
which can be solve quickly by traditional algorithms. Notice that if the submatrix bΣ(S,S)
is invertible, then (2.9) would further reduce to a low-dimensional eigenvalue decompo-
sition problem. Indeed, bΣ(S,S)is invertible with high probability for a properly chosen
k. Recall that bΣ(S,S)=En[{X(S)−En(X(S))}{X(S)−En(X(S))}⊤]. Then it is invert-
ible with probability approaching to 1provided that k≪nandΣhas full rank, which
contributes to the computational efficiency of the proposed algorithm. Consequently, the
computationally intractable sparse SIR problem (2.5) can be efficiently solved by conduct-
ing low-dimensional eigenvalue decompositions through our proposed SSIRvRP algorithm.
Moreover, since {Pa,b:a∈[A], b∈[b]}are generated randomly from Pk, the A×Blow-
10dimensional eigenvalue decompositions can be executed in parallel, which further facilitates
faster computation. Finally, the matrix pair (bΣ(S,S)
E(X|Y),bΣ(S,S))can either be extracted from
the pair (bΣE(X|Y),bΣ)or be estimated from the projected covariates X(S). The latter option
would be preferable when pis sufficiently large.
It is worthy noting that, different from the convex relaxation algorithms for sparse SIR
or SDR (Tan et al., 2018, Lin et al., 2019, Tan et al., 2020), the proposed algorithm is not
iterative and thus robust to poor initialization.
Another notable advantage of our method lies in its scalability to other SDR methods.
It is well known that the class of inverse regression based SDR methods, including the
sliced average variance estimation (Cook and Weisberg, 1991), principal Hessian directions
(Li, 1992, Cook, 1996), directional regression (Li and Wang, 2007), among others, can be
formulated as a generalized eigenvalue problem. Hence, the proposed SSIRvRP algorithm, as
a solution to sparse generalized eigenvalue problems, can be readily applied to these methods
to obtain a sparse estimator of the central subspace.
3 Theoretical analysis
3.1 Upper bound for isotropic covariance
We consider the setting where (Xi,˜Yi)n
i=1are i.i.d. such that Xi|(˜Yi=h)∼ N p(µh,Σh)
forh∈[H], which is also assumed in Cook and Yin (2001), Cook (2007), Cook and
Forzani (2009) and Tan et al. (2020). To simplify the theoretical analysis, we firstly as-
sume Cov (X) =Ip. Then, the generalized eigenvalue problem (2.1) reduces to the following
eigenvalue problem:
ΣE(X|Y)βi=λiβi,withβ⊤
iβj=1{i=j},
where ΣE(X|Y)is the covariance matrix of the conditional expectation E(X|˜Y),i, j=
1,···, p, and λ1≥ ··· ≥ λd>0 = λd+1=···=λp. Let Λ= diag( λ1, . . . , λ d)and
recall that β= (β1, . . . ,βd)collects the first deigenvectors corresponding to the nonzero
eigenvalues λ1≥ ··· ≥ λd. Therefore, we can decompose ΣE(X|Y)=βΛβ⊤.
The following technical conditions are needed.
11(A1) κλ≥λ1≥ ··· ≥ λd≥λ >0for some constant κ >1.
(A2) β∈Θp,d,s(µ), where
Θp,d,s(µ) :=
V∈Θp,d,supp(V)≤s,maxj:∥V(j,·)∥2̸=0∥V(j,·)∥2
minj:∥V(j,·)∥2̸=0∥V(j,·)∥2≤µ
,
andΘp,ddenotes the set of real p×dmatrices with orthonormal columns.
Assumption (A1) can be seen as a coverage condition (Cook, 2004, Yu et al., 2016),
and similar assumptions can be found in the literature (Cai et al., 2013, Gao et al., 2015,
Tan et al., 2020). Assumption (A2) is an incoherence condition by which we require the
parameter of interest βdo not have too many non-zero rows, and the non-zero rows should
have comparable Euclidean norms. For any β∈Θp,d,s(µ), sinceP
j∈A∥β(j,·)∥2
2=∥β∥2
F=d,
Assumption (A2) implies
d
sµ2≤ ∥β(j,·)∥2
2≤dµ2
s,∀j∈ A. (3.1)
ForU,V∈Θp,d, following Gataric et al. (2020), we use the loss function
L(U,V) :=∥sin{D(U,V)}∥F (3.2)
to evaluate the distance between UandV, where the sine function acts elementwise,
D(U,V)is the d×ddiagonal matrix whose j-th diagonal entry is the j-th principal angel
between UandV,i.e.,cos−1(σj), where σjis the j-th singular value of U⊤V.
In the following theoretical analysis, sandpare allowed to depend on the sample size n,
while λ,µ,dandHare treated as fixed constants. Recall that bβis the output of Algorithm
1 with inputs bΣE(X|Y),bΣ,A,B,d,kandl. Theorem 1 gives an upper bound of the proposed
SSIRvRP estimator.
Theorem 1. Suppose Assumptions (A1)-(A2) hold. If k≥max{d+ 1, s},l≥s, and
16Kr
klogp
n≤λd
sµ2, (3.3)
12then it holds that
P
L(bβ,β)≤2Ks
dllog(p)
nλ2
d
≥1−cp−3−pexp
−Aλ2
d
50p2µ8λ2
1
,
where K, c > 0are some constants.
Theorem 1 is a generalization of Theorem 1 of Gataric et al. (2020), where they consider
theestimationoftheprincipalsubspaceunderarestrictedcovarianceconcentrationcondition
that was introduced in Wang et al. (2016a). We extend their results to the problem of central
subspace estimation, and the results of Lemma 1 in the proof of Theorem 1 mimic the
restricted covariance concentration condition. Tan et al. (2020) also considered the problem
of the central subspace estimation and they proposed an adaptive estimation scheme for
sparse SIR, which achieves an upper boundp
slog(p)/nunder the loss function defined in
(3.2). If ℓ≍s, our SSIRvRP estimator also achieves the same bound up to some constants.
3.2 Upper bound for general covariance
We go further to the real generalized eigenvalue problem for sparse SIR specified in (2.1),
where the covariance Σof the covariates Xis not restricted to special structures. It is
well known that the generalized eigenvalue problem is, in principle, more difficult than the
standardone, especiallyintermsoftheoreticalanalysis. Thus, severalhigh-levelassumptions
would be imposed to highlight the technical difference between the generalized eigenvalue
problem tackled in this section and the standard one for the simplified setting where Σ=Ip
addressed in the above section. The data (Xi,˜Yi)n
i=1are also assumed to be i.i.d.
To be consistent with the logic behind Algorithm 1 and to utilize standard properties
of eigenvalue decomposition, for the sake of theoretical analysis, we hope to find a proper
transformation to reduce the generalized eigenvalue decomposition (2.1) to a standard one,
while retaining the row sparsity structure of the basis β. It turns out that the Cholesky
decomposition of Σplays a crucial role. For a positive-definite covariance matrix Σ, using
the Cholesky decomposition, we can decompose Σas
Σ=LL⊤,
13where Lis a real lower triangular matrix with positive diagonal entries. Let βi= (L⊤)−1γi
fori∈[p]. Then, (2.1) implies that
{L−1ΣE(X|Y)(L−1)⊤}γi=λiγiwithγ⊤
iγj=1{i=j}, (3.4)
where i, j∈[p], and λ1≥ ··· ≥ λd>0 =λd+1=···=λp. Denote M=L−1ΣE(X|Y)(L−1)⊤
andγ= (γ1, . . . ,γd), and then Mis a real symmetric matrix to which the spectral decom-
position can be readily applied, i.e.,M=γΛγ⊤withΛ= diag( λ1, . . . , λ d). In addition,
the eigenvalues of Mare just the generalized eigenvalues of the matrix pair (ΣE(X|Y),Σ),
and, more importantly, the leading eigenvectors γ= (γ1, . . . ,γd)ofMprecisely retain the
row sparsity structure of the generalized eigenvectors β= (β1, . . . ,βd)of(ΣE(X|Y),Σ). To
see this clearly, recall that β⊤= (β(A,·),⊤,0⊤)andβi= (L⊤)−1γiwhere Arepresents the
set of the active covariates and Lis a lower triangular matrix. Then, it holds that2
γ=L⊤β=
L(A,A),⊤L(Ac,A),⊤
0⊤L(Ac,Ac),⊤

β(A,·)
0
=
L(A,A),⊤β(A,·)
0
,
where
L=
L(A,A)0
L(Ac,A)L(Ac,Ac)
.
The transformation from the generalized eigenvalue problem (2.1) to the standard one (3.4)
with the row sparsity structure of the leading eigenvectors unchanged makes tractable the
theoretical analysis of Algorithm 1 under general covariance.
The following assumptions are required.
(A1’) ιθ≥θ1≥ ··· ≥ θp≥θ >0for some constant ι >1, where θiis the eigenvalue of Σ.
(A2’) γ∈Θp,d,s(µ), where Θp,d,s(µ)is defined in Assumption (A2).
2Cholesky decomposition is dependent on the order in which the variables appear in the random vector
X, and it works when the variables have a natural ordering.
14(A3)Define Sk:={S⊂[p] :|S|=k}for any k∈[p]. Then,
P
sup
S∈Skmax{∥bL(S,S)−L(S,S)∥op,∥cM(S,S)−M(S,S)∥op} ≤Kr
klogp
n
≥1−c1p−c2,
where bΣ=bLbL⊤is the Cholesky decomposition of bΣ,cM=bL−1bΣE(X|Y)(bL−1)⊤and
K, c 1, c2>0are constants.
(A4)For any j∈[p]and any ordering of X, there exists some τ∈(0,1]such that τ≤
(L(j,j))−2≤τ−1.
Assumption (A1’) is a regularity condition for Σ, whose eigenvalues are required to be
bounded away from zero and infinity. Assumption (A2’) is an incoherence condition for the
transformed basis γby which we require γdo not have too many non-zero rows, and the
non-zero rows should have comparable Euclidean norms. Recall that γ=L⊤βandLis a
lower triangular matrix. Hence, Assumption (A2’) is equivalent to requiring that βdo not
have too many non-zero rows and that the rows of L(A,A),⊤β(A,·)have comparable Euclidean
norms. For any γ∈Θp,d,s(µ), sinceP
j∈A∥γ(j,·)∥2
2=∥γ∥2
F=d, Assumption (A2’) implies
d
sµ2≤ ∥γ(j,·)∥2
2≤dµ2
s,∀j∈ A. (3.5)
Assumption (A3) is indeed a high-level condition imposed on the sample estimates of Σ
andΣE(X|Y). When Σis a diagonal matrix, Assumption (A3) holds under mild conditions
similar to those in Tan et al. (2020); see their Lemma 1 for details. Assumption (A4) is a
technical condition imposed to guarantee that the active variables enjoy higher weights than
the inactive ones in the population level. Since all the diagonal elements of Lare positive,
Assumption (A4) seems quite mild. Moreover, this assumption is implied by Assumption
(A1’) provided that Σis a diagonal matrix. Especially, for the simplified case where Σ=Ip,
the above assumptions naturally hold.
ForU,V∈Θp,d(Σ) :={V∈Rp×d:V⊤ΣV=Id}, we use the popular general loss
function
L(U,V) :=∥UU⊤−V V⊤∥F (3.6)
15to evaluate the distance between linear subspaces; see Cai et al. (2013), Gao et al. (2015)
and Tan et al. (2020). We note that, when Σ=Ip, this loss function reduces to the one
defined in (3.2) up to a constant√
2.
In the following, sandpare allowed to depend on the sample size n, while θ,λ,µ,τ,
dandHare treated as fixed constants. Theorem 2 gives an upper bound of the proposed
SSIRvRP estimator for general covariance.
Theorem 2. Suppose Assumptions (A1)-(A4) hold. If k≥max{d+1, s},l≥s,Kp
klog(p)/n≤
min{λ1/(4d),√θ1, θp/(6√θ1)},Kp
llog(p)/n≤min{λd/(2√
2),√θ1, θp/(6√θ1)}and
Cr
klogp
n≤τλd
sµ2(3.7)
for some sufficiently large constant C >0, then it holds that
P
L(bβ,β)≤C′r
llog(p)
n
≥1−c′p−c2−pexp
−Aτ4λ2
d
50p2µ8λ2
1
,
where C′, c′>0are some constants.
Theorem 2 generalizes Theorem 1 to the general covariance case, showing an upper bound
p
slog(p)/nforl≍s, which echoes latest research results for central subspace estimation
(Lin et al., 2019, Tan et al., 2020). Although seemingly similar to Theorem 1, the proof
of Theorem 2 is quite different from that of Theorem 1, which wisely utilizes the Cholesky
decomposition and several nice properties of triangular matrices; see Section 9.2 for details.
3.3 Lower bound
Theorem 1 and Theorem 2, together with the following Theorem 3, indicate that the
SSIRvRP estimator is minmax optimal up to some logarithmic factor, over all possible
sparse SIR estimators, provided that l≍s. Theorem 3 establishes a minmax lower bound
among all possible sparse SIR estimators eβ, which is similar to the lower bound established
in Tan et al. (2020). Different from their methods, we require the central subspace satisfy
an incoherence condition. However, we show that this kind of restriction on the parameter
16space does not make the estimation any easier from the mimnax perspective. For simplicity,
we assume (Xi,˜Yi)n
i=1are i.i.d. such that Xi|(˜Yi=h)∼ N p(µh,Σh)forh∈[H]andΣ=Ip.
Theorem 3. Assume that 4(s−1)≤p−1,(s−1) log{(p−1)/(s−1)} ≥ 6and5n≥
4s(s−1) log{(p−1)/(s−1)}. Then
inf
eβsup
β∈Θp,d,s(3)EPβ{L(eβ,β)}≳r
slog(p/s)
n,
where the expectation is with respect to (Xi, Yi)∼i.i.d.Pβ.
The bound in Theorem 3 is similar to the one established for the estimation of the
principal eigenspace (Gataric et al., 2020). We generalized their conclusion to the setting
of the estimation of the central subspace. Despite similar conclusions, the technical proofs
of Theorem 3 are quite different from theirs. Moreover, the lower bound established here
actually holds beyond the normality assumption of X|˜Yand the isotropic assumption of the
covariance matrix.
4 Improved algorithm
In this section, in order to enhance the identification ability of the active set of covariates,
we add a reweighting step to Algorithm 1 to refine the weights corresponding to the largest
values of bw. Specifically, let ˆS′be the index set of the l′largest components of bwproduced
in Algorithm 1. We suggest recomputing the weights of these variables in ˆS′by repeating
Steps 1-8 in Algorithm 1 with the submatrix pair (bΣ(ˆS′,ˆS′)
E(X|Y),bΣ(ˆS′,ˆS′)). Then we select l
indices corresponding to the largest values of bw′to form a set ˆS, and output an estimate
bβas the first dgeneralized eigenvectors of (PˆSbΣE(X|Y)PˆS, PˆSbΣPˆS). Pseudo code for this
modified SSIRvRP algorithm is given in Algorithm 2. We find that the new algorithm with
a reweighting step really works well, as shown in the numerical studies.
By similar techniques of Theorem 2, we can prove that the estimate of Algorithm 2
has the same upper boundp
llog(p)/nas that of Theorem 2. However, it seems that the
reweighting scheme in Step 9 of Algorithm 2 helps identify A, the row support of β, more
accurately in the finite sample. There is no surprise if we consider the formation of ˆSas a
17Algorithm 2: pseudocode of the SSIRvRP algorithm with reweighting
Input: bΣE(X|Y),bΣ, A1, A2, B1, B2∈N, k, l, l′∈[p], d∈[k]
1Generate {Pa,b:a∈[A1], b∈[B1]}independently and uniformly from Pk
2fora= 1,···, A1do
3 forb= 1,···, B1do
4 fori∈[d+ 1], compute ˆλa,b;i:=λi(Pa,bbΣE(X|Y)Pa,b, Pa,bbΣPa,b)and the
corresponding generalized eigenvector ˆva,b;i=vi(Pa,bbΣE(X|Y)Pa,b, Pa,bbΣPa,b)
with ˆλa,b;k+1= 0
5 end
6Compute b∗(a) :=sargmax
b∈[B1]Pd
i=1ˆλa,b;i
7end
8Compute bw= ( ˆw(1),···,ˆw(p))⊤with
ˆw(j):=1
A1A1X
a=1dX
i=1(ˆλa,b∗(a);i−ˆλa,b∗(a);d+1)(ˆv(j)
a,b∗(a);i)2, j∈[p]
9LetˆS′be the index set of the l′largest components of bw. Recompute the
l′-dimensional vector of weights bw′by repeating Steps 1−8with the submatrix
pair(bΣ(ˆS′,ˆS′)
E(X|Y),bΣ(ˆS′,ˆS′)), the projection parameters A2, B2and the newly defined
Pk:={PS:S⊆[l′],|S|=k}
10Denote by ˆSthe index set of the llargest components of bw′
Output: bβ= (ˆv1, . . . , ˆvd), where ˆv1, . . . , ˆvdare the top dgeneralized eigenvectors of
(PˆSbΣE(X|Y)PˆS, PˆSbΣPˆS)
process of variable selection. From this angle, the design of Algorithm 2 mimics a two-round
selection, which offers an opportunity to reassess the importance of the variables and retrieve
the mistakenly deleted ones, often accompanied by weak signals, in the first round.
Remark 2.Notice that the estimate of SSIRvRP naturally satisfies the orthogonal con-
straint bβ⊤bΣbβ=Id. However, the refined three-steps estimator (Tan et al., 2020) is op-
timized by relaxing original constraints and thus can not meet the orthogonal constraint
without a normalization step. The Lasso-SIR (Lin et al., 2019) also does not satisfy this
constraint. This can be seen another advantage of the proposed algorithm.
185 Choice of hyperparameters
In the SSIRvRP algorithm, there are several hyperparameters to be selected before the
implementationofthealgorithm. Wefindthattheproposedalgorithmbehavesquiterobustly
to a wide ranges of combinations of (A, B),(A1, B1),kandl′, as shown in the following
numerical experiments. For the choice of the dimension of the central subspace d, several
existing methods can be applied to the sparse SIR setting (Chen et al., 2010, Lin et al.,
2019), and thus we treat das a known constant.
The sparsity level lis a key tuning parameter in the proposed method. To choose the
tuning parameter, we minimize the following criterion, which has been used in Chen et al.
(2010):
−logn
tr(bβ⊤
lbΣE(X|Y)bβl)o
+δ·dfl,
wherebβldenotes the solution for βgiven the sparsity level l, dfldenotes the effective number
of parameters, and δ= 2/nfor the AIC-type criterion and δ= log( n)/nfor the BIC-type
criterion. Since the number of nonzero rows of bβlis just l, we can estimate df lby(l−d)·d.
Thus, we choose the best lby minimizing
−logn
tr(bβ⊤
lbΣE(X|Y)bβl)o
+δ·(l−d)·d .
One advantage of the tuning process of lis that it is conducted only in Step 10 of
Algorithm 2 and Steps 1-9 are computed only once. Furthermore, since lis an integer and
its parameter space is countable and finite, the tuning process enjoys higher computational
efficiency compared with those tuned on a continuous parameter space.
6 Simulation studies
Inthissection, weconductextensivenumericalexperimentstocomparetheproposedmethod
with several competitive methods, show the effect of the reweighting step in Algorithm 2,
and present some empirical instruction for choice of the hyperparameters in the proposed
algorithm.
196.1 Comparison with existing methods
We compare our method (Algorithm 2) with the Refined Three-Steps estimator (RTS, here-
after) in Tan et al. (2020) which was shown to be rate optimal when logp=o(n), the
Lasso-SIR in Lin et al. (2019) and DT-SIR in Lin et al. (2017). Lasso-SIR was proved to
be rate optimal when p=o(n2), and DT-SIR was rate optimal when the covariance matrix
ofXis identity. We describe our method as three types: the first one works with the true
sparsity level (SSIRvRP), the second one with the sparsity level tuned by the BIC crite-
rion (SSIRvRP-BIC), and the last one with the sparsity level tuned by the AIC criterion
(SSIRvRP-AIC).
To be fair, we copy the simulation settings in Tan et al. (2020) and recall the results
therein. Five models are considered, i.e.,
Model I : Y=β⊤X+ sin( β⊤X) +ϵ,
Model II : Y= 2 arctan( β⊤X) +ϵ,
Model III : Y= (β⊤X)3+ϵ,
Model IV : Y= sinh( β⊤X) +ϵ,
Model V : Y= exp( β⊤
1X)·sign(β⊤
2X) + 0.2ϵ,
where X∼ N p(0,Σ),ϵ∼ N(0,1), andXandϵare independent. The covariance matrix Σ
follows the following four structures:
(1)Identity covariance: Σ=Ip,
(2)Dense covariance: Σ= (σij)1≤i,j≤pwith σii= 1andσij= 0.6fori̸=j,
(3)Toplitz covariance: Σ= (σij)1≤i,j≤pwith σij= 0.5|i−j|,
(4)Sparse inverse covariance: Σis the correlation matrix of Σ0, and Σ−1
0= (wij)1≤i,j≤p
with wij=1{i=j}+ 0.5×I{|i−j|=1}+ 0.4×I{|i−j|=2}.
The regression coefficients β∈Rp×1and(β1,β2)∈Rp×2are set to have 5nonzero rows
whose indexes are randomly chosen from the set [p]. And the entries in these nonzero rows
are random numbers drawn from the uniform distribution on the finite set {−2,−1,0,1,2}.
20Forthehyperparametersoftheproposedalgorithm,weset (A1, B1) = (900 ,300),(A2, B2) =
(600,200),k= 20,l′= 50anddas its true value. Since the true parameter does not satisfy
the constraint β⊤Σβ=Id, we can not compare these sparse SIR estimates directly under
the loss function given in (3.2) or (3.6). Consequently, we use the correlation loss (Li, 1991,
Tan et al., 2020), defined as
Lρ(bβ,β) = 1−1
dtrn
(bβ⊤Σbβ)−1(bβ⊤Σβ)(β⊤Σβ)−1(β⊤Σbβ)o
,
to measure the efficiency of the estimation and to compare our method with its competing
methods. Each simulation is repeated 100 times, and we summarize the average correlation
loss across all combinations of five models and four covariance structures under various
(n, p)configurations in Tables 1-4. For each model setting, the average loss of the optimal
algorithm is highlighted by a bold font, and the average loss of the sub-optimal algorithm is
highlighted by an italic font.
It is clear that the proposed method performs generally better than the other methods
under various model settings. Specifically, SSIRvRP, SSIRvRP-BIC and SSIRvRP-AIC per-
form much better than the other methods in Models I, III and IV, and the average loss is
decreased by almost an order of magnitude. However, in Models II and V, RTS sometimes
performs the best, when the sample size and dimension are both low, i.e.,(n, p) = (100 ,200)
and(100,400). With the increase of the sample size and dimension, our SSIRvRP tends to
catch up quickly from behind, and the decrease of average loss is significant. Although RTS
was proved to be optimal when log(p) =o(n), which is the same as our method, its finite
sample performance is generally inferior to ours. The reason may be that the implementa-
tion of our method does not involve any choice of initial points, while other approaches are
gradient-based and thus vulnerable to a possible bad choice of initialization.
Additionally,whencomparingSSIRvRP-AICandSSIRvRP-BIC,itwasfoundthatSSIRvRP-
BICperformedslightlybetterinModelsIIandIII.However,whenconsideringallfivemodels,
neither method was found to be superior. Overall, they demonstrated similar performance.
Therefore, either method can be chosen as a suitable working algorithm.
Finally, the average correlation loss decreases as nincreases which can be seen from the
21Table 1: The averages of correlation loss with the identity covariance structure.
(n, p)Model DT-SIR Lasso-SIR RTS SSIRvRP SSIRvRP-BIC SSIRvRP-AIC
(100,200)I 0.939 0.230 0.070 0.015 0.024 0.027
II 0.926 0.389 0.046 0.052 0.053 0.122
III 0.955 0.163 0.021 0.002 0.009 0.002
IV 0.923 0.395 0.085 0.002 0.008 0.002
V 0.769 0.404 0.066 0.090 0.120 0.112
(100,400)I 0.954 0.435 0.033 0.011 0.017 0.032
II 0.949 0.448 0.015 0.092 0.081 0.162
III 0.863 0.355 0.039 0.002 0.007 0.004
IV 0.888 0.611 0.120 0.002 0.005 0.003
V 0.683 0.527 0.091 0.138 0.150 0.170
(100,600)I 0.902 0.587 0.109 0.023 0.022 0.039
II 0.879 0.675 0.175 0.109 0.099 0.196
III 0.968 0.489 0.021 0.006 0.009 0.005
IV 0.875 0.756 0.215 0.003 0.009 0.003
V 0.701 0.561 0.179 0.140 0.142 0.179
(200,600)I 0.997 0.094 0.012 0.003 0.003 0.018
II 0.930 0.233 0.039 0.010 0.014 0.091
III 0.944 0.058 0.008 0.001 0.001 0.001
IV 0.970 0.212 0.020 0.001 0.001 0.001
V 0.851 0.242 0.046 0.032 0.039 0.100
(400,600)I 0.014 0.014 0.009 0.002 0.002 0.010
II 0.924 0.027 0.013 0.004 0.005 0.047
III 0.971 0.004 0.004 0.000 0.000 0.000
IV 0.989 0.021 0.009 0.000 0.000 0.001
V 0.874 0.045 0.019 0.008 0.009 0.051
22Table 2: The averages of correlation loss with the dense covariance structure.
(n, p)Model DT-SIR Lasso-SIR RTS SSIRvRP SSIRvRP-BIC SSIRvRP-AIC
(100,200)I 0.472 0.106 0.143 0.045 0.054 0.053
II 0.766 0.283 0.160 0.130 0.129 0.175
III 0.904 0.483 0.074 0.003 0.024 0.005
IV 0.902 0.697 0.298 0.004 0.028 0.010
V 0.439 0.338 0.129 0.173 0.199 0.184
(100,400)I 0.856 0.509 0.074 0.057 0.057 0.054
II 0.828 0.887 0.019 0.135 0.124 0.194
III 0.256 0.026 0.012 0.004 0.023 0.008
IV 0.245 0.104 0.081 0.006 0.033 0.010
V 0.432 0.410 0.139 0.207 0.208 0.218
(100,600)I 0.883 0.600 0.301 0.062 0.059 0.080
II 0.340 0.159 0.131 0.170 0.160 0.235
III 0.928 0.926 0.284 0.008 0.027 0.019
IV 0.849 0.544 0.453 0.007 0.028 0.018
V 0.612 0.425 0.245 0.195 0.197 0.215
(200,600)I 0.854 0.187 0.023 0.007 0.012 0.024
II 0.507 0.085 0.096 0.036 0.034 0.088
III 0.637 0.028 0.031 0.002 0.007 0.004
IV 0.946 0.944 0.045 0.002 0.009 0.005
V 0.548 0.307 0.086 0.078 0.073 0.101
(400,600)I 0.822 0.050 0.005 0.002 0.003 0.010
II 0.287 0.027 0.020 0.010 0.010 0.038
III 0.472 0.006 0.019 0.001 0.001 0.001
IV 0.591 0.035 0.038 0.001 0.001 0.002
V 0.363 0.126 0.027 0.024 0.026 0.054
cases (n, p) = (100 ,600),(200,600), and (400,600)), and increases as pincreases seen from
the cases (n, p) = (100 ,200),(100,400), and (100,600)). These results are fairly consistent
with our theoretical findings.
Table 5 reports similar results as Table 3, where we use equation (2.8) in Remark 1 to
calculate the sample covariance matrix of E(X|˜Y). Both estimates works similarly to each
other, so you can choose whichever one you prefer.
6.2 The reweighting step
Compared with Algorithm 1, we add a reweighting step to Algorithm 2 to help retrieve true
signals. Figure 1 displays the effect of the reweighting step in terms of averaged correlation
23Table 3: The averages of correlation loss with the Toplitz covariance structure.
(n, p)Model DT-SIR Lasso-SIR RTS SSIRvRP SSIRvRP-BIC SSIRvRP-AIC
(100,200)I 0.945 0.186 0.073 0.013 0.019 0.022
II 0.943 0.297 0.046 0.056 0.058 0.099
III 0.951 0.088 0.022 0.002 0.007 0.002
IV 0.915 0.323 0.092 0.002 0.006 0.002
V 0.794 0.345 0.061 0.109 0.119 0.117
(100,400)I 0.952 0.283 0.033 0.013 0.024 0.034
II 0.919 0.368 0.017 0.089 0.081 0.155
III 0.917 0.254 0.042 0.003 0.005 0.002
IV 0.907 0.477 0.141 0.002 0.006 0.002
V 0.763 0.496 0.099 0.127 0.133 0.152
(100,600)I 0.927 0.474 0.105 0.022 0.026 0.036
II 0.874 0.581 0.187 0.114 0.103 0.188
III 0.935 0.340 0.021 0.005 0.014 0.006
IV 0.868 0.692 0.202 0.003 0.010 0.003
V 0.765 0.528 0.201 0.142 0.140 0.178
(200,600)I 0.932 0.070 0.010 0.004 0.004 0.018
II 0.926 0.150 0.034 0.012 0.016 0.084
III 0.831 0.030 0.008 0.001 0.001 0.001
IV 0.938 0.151 0.017 0.001 0.001 0.002
V 0.603 0.179 0.040 0.028 0.035 0.079
(400,600)I 0.372 0.014 0.008 0.002 0.002 0.009
II 0.289 0.021 0.012 0.004 0.005 0.043
III 0.176 0.004 0.003 0.000 0.000 0.000
IV 0.577 0.021 0.010 0.001 0.001 0.001
V 0.199 0.039 0.018 0.007 0.009 0.045
loss and signal rate. Here, signal rate summarizes the frequency that the algorithm identifies
all the true signals exactly. It is clear that the reweighting step significantly decreases the
correlation loss and increases the signal rate under various model settings. Therefore, we
suggest Algorithm 2 for solving sparse SIR problems.
6.3 Choice of hyperparameters
In this subsection, we conduct numerical experiments to give some instructions to the choice
of the hyperparameters in Algorithm 2. Generally speaking, the performance of the proposed
method seems quite robust to a wide range of combinations of the hyperparameters.
Figure 2 plots the relationship of the correlation loss and A,B, and combinations of
24Table 4: The averages of correlation loss with the sparse inverse covariance structure.
(n, p)Model DT-SIR Lasso-SIR RTS SSIRvRP SSIRvRP-BIC SSIRvRP-AIC
(100,200)I 0.876 0.153 0.084 0.015 0.023 0.023
II 0.906 0.265 0.060 0.100 0.086 0.127
III 0.848 0.056 0.024 0.002 0.004 0.002
IV 0.895 0.280 0.108 0.002 0.010 0.002
V 0.557 0.267 0.068 0.129 0.142 0.129
(100,400)I 0.930 0.203 0.040 0.029 0.026 0.031
II 0.943 0.238 0.016 0.108 0.088 0.159
III 0.851 0.133 0.039 0.003 0.009 0.003
IV 0.873 0.327 0.118 0.003 0.004 0.003
V 0.685 0.550 0.087 0.151 0.154 0.170
(100,600)I 0.930 0.388 0.147 0.036 0.032 0.046
II 0.930 0.431 0.183 0.119 0.105 0.172
III 0.926 0.192 0.023 0.003 0.008 0.002
IV 0.893 0.576 0.241 0.002 0.006 0.006
V 0.778 0.433 0.159 0.191 0.184 0.201
(200,600)I 0.452 0.033 0.011 0.005 0.004 0.017
II 0.704 0.082 0.038 0.012 0.016 0.069
III 0.255 0.010 0.007 0.001 0.001 0.001
IV 0.730 0.086 0.021 0.001 0.001 0.001
V 0.319 0.146 0.048 0.031 0.038 0.073
(400,600)I 0.328 0.016 0.012 0.002 0.002 0.006
II 0.228 0.023 0.012 0.003 0.004 0.034
III 0.334 0.007 0.011 0.001 0.001 0.001
IV 0.248 0.017 0.008 0.001 0.001 0.001
V 0.257 0.051 0.018 0.008 0.010 0.045
(A, B)with the other parameters staying unchanged. When Bis fixed, increasing Awould
not significantly decrease the correlation loss, and similar trend occurs when Ais fixed.
When both AandBare increased simultaneously while keeping B=A/3, the figure shows
an obvious downward trend for small As. As Agets large to 400, the decreasing trend of
correlation loss becomes flat. Moreover, the loss seems robust to all choices of A,Band
(A, B)when the sample size increases to 400and600.
For the choice of k, Figure 3 shows that their may exist some optimal kfor small sample
size, while there is no significant trend when ngets large. Figure 4 indicates that the loss is
robust to the choice of l′when n= 200. For the cases where n= 100, the figures shows a
downward trend when l′is small, and the trend gets flattened as l′gets large.
25Table 5: The averages of correlation loss with the Toplitz covariance structure using (2.8)
for estimating ΣE(X|Y).
(n, p)Model DT-SIR Lasso-SIR RTS SSIRvRP SSIRvRP-BIC SSIRvRP-AIC
(100,200) I 0.945 0.186 0.073 0.011 0.017 0.022
II 0.943 0.297 0.046 0.060 0.055 0.120
III 0.951 0.088 0.022 0.002 0.009 0.002
IV 0.915 0.323 0.092 0.002 0.008 0.003
V 0.794 0.345 0.061 0.125 0.135 0.157
(100,400) I 0.952 0.283 0.033 0.016 0.018 0.036
II 0.919 0.368 0.017 0.119 0.111 0.166
III 0.917 0.254 0.042 0.003 0.006 0.003
IV 0.907 0.477 0.141 0.003 0.008 0.003
V 0.763 0.496 0.099 0.142 0.157 0.177
(100,600) I 0.927 0.474 0.105 0.024 0.026 0.038
II 0.874 0.581 0.187 0.104 0.098 0.186
III 0.935 0.340 0.021 0.003 0.004 0.003
IV 0.868 0.692 0.202 0.002 0.007 0.006
V 0.765 0.528 0.201 0.125 0.135 0.157
(200,600) I 0.932 0.070 0.010 0.004 0.003 0.018
II 0.926 0.150 0.034 0.012 0.016 0.081
III 0.831 0.030 0.008 0.001 0.001 0.002
IV 0.938 0.151 0.017 0.001 0.001 0.001
V 0.603 0.179 0.040 0.031 0.038 0.093
(400,600) I 0.372 0.014 0.008 0.002 0.002 0.008
II 0.289 0.021 0.012 0.003 0.005 0.039
III 0.176 0.004 0.003 0.000 0.000 0.001
IV 0.577 0.021 0.010 0.000 0.000 0.000
V 0.199 0.039 0.019 0.009 0.012 0.047
To summarize, when a moderately large number of samples are collected, there is no need
to worry about the choice of hyperparameters. However, when the sample size is limited, we
suggest lager values for (A, B)andl′and a medium scale for k.
7 Real data study
In this section, we consider a gene expression data from the international “HapMap” project
(Thorisson et al., 2005). The data includes 90 samples, 45 Chinese and 45 Japanese, and
reports the gene expression levels from 47293 probes. The gene named CHRNA6 is the
subject of many nicotine addiction studies (Thorgeirsson et al., 2010), and we treat its
26Figure 1: Effect of the reweighting step under the Toplitz covariance structure. (a1)-(e1)
and (a2)-(e2) correspond to (n, p)settings: (100,200)-(400,600).
mRNA expression as the response Yas done in Fan et al. (2018) and Lin et al. (2019). The
expressions of other 47292 genes are treated as the covariates. Consequently, we are now
faced with a problem where p= 47292 ≫n= 90.
27Figure 2: Choice of A and B for Model I with Identity covariance.
Figure 3: Choice of k.
Our goal is to identify several combinations of a few genes that represent the regression
relationship of Yregarding X, and the proposed SSIRvRP method is well-suited for this
28Figure 4: Choice of l′.
task. Following Lin et al. (2019), we set d= 1andl= 13and other hyperparameters as
those in the simulation. Based on the estimated coefficients bβ, we define Z=bβ⊤Xand plot
Yagainst Zin Figure 5. We then find that there exists a moderate quadratic patten between
YandZ. Motivated by this finding, we go further to fit a regression model between Yand
Z, Z2, and obtain an adjusted R-squared 0.620 and a p-value 0.000. The mean squared error
of the fitted model is 0.040. These results seem a little better than those in Lin et al. (2019),
where the authors obtained an R-squared 0.578 and a mean squared error 0.044.
For comparison, we also employ the sparse PCA algorithm proposed by Gataric et al.
(2020) to estimate the coefficients eβwhere the information of Yis absent, and calculate
Z′=eβ⊤X. The scatter plot of Yagainst Z′does not indicate any interesting patterns
between the two variables. Running a linear regression model between YandZ′gives us
an R-squared 0.001 and a p-value 0.792, indicating that there is no linear patten between
the two variables. This result seems less meaningful than those achieved by the proposed
method which is supervised by Y.
8 Discussion
In this paper, we propose a random projection method to estimate the central subspace in
sparse SIR when p≫n. Compared with existing methods, the proposed algorithm is compu-
29−79 −78 −77 −76 −756.0 6.5 7.0 7.5
zyFigure 5: Quadratic relationship.
tationally simpler and more efficient. Theoretically, we proved that the proposed estimator
achieves the minmax optimal rate under suitable assumptions. It is notable that the ran-
dom projection technique is introduced to solve a generalize eigenvalue-eigenvector problem.
Hence, it can also be applied to sparse Fisher’s discriminant analysis for classification and
sparse canonical correlation analysis for exploring the relationship of two high-dimensional
random vectors. We leave it for further study.
9 Appendix
9.1 Proof of Theorem 1
The following lemma is needed, whose proof is given in Section 9.4.1.
Lemma 1. Under Assumption (A1), there is an event Ωwith probability at least 1−cp−3
such that on Ω, it holds that
supu∈Bp−1
0(k)|u⊤(bΣE(X|Y)−ΣE(X|Y))u| ≤Kr
klog(p)
n
30provided that klogp≪n, where Bp−1
0(k) ={v= (v(1), . . . , v(p))⊤∈Rp:∥v∥2= 1,Pp
j=11{v(j)̸=
0} ≤k}fork∈[p]andKis a constant depending on H.
We begin the proof. The proof is similar to that of Theorem 1 of Gataric et al. (2020),
but some details should be adjusted, cause we now have a sliced estimator of the covariance
matrix. For notation simplicity, we drop the subscript E(X|Y)ofΣ, andbΣ. Define Sk:=
{S⊂[p] :|S|=k}and recall A={j∈[p] :β(j,·)̸=0}which denotes the set of non-zero
rows of β. For any S∈ Sk, notice that Σ(S,S)=β(S,·)Λ(β(S,·))⊤. Hence,
dX
i=1λi(Σ(S,S)) =kX
i=1λi(Σ(S,S))−kX
i=d+1λi(Σ(S,S)) = tr( Σ(S,S)) =dX
i=1X
j∈S∩Aλi(β(j,i))2.(9.1)
Then, by Lemma 1, we know there is an event Ωwith probability at least 1−cp3such that
onΩwe have for any S∈ Sk,
∥bΣ(S,S)−Σ(S,S)∥op≤Kr
klogp
n,
where K > 0is some constant independent of S. On the event Ωdefined above, by Weyl’s
inequality, we have that
dX
i=1λi(bΣ(S,S))−dX
i=1X
j∈S∩Aλi(β(j,i))2(9.1)=dX
i=1{λi(bΣ(S,S))−λi(Σ(S,S))}
≤Kdr
klogp
n(3.3)
≤dλd
16sµ2.
Thus, on the event Ω, if(S∩ A)⊊(S′∩ A), then
dX
i=1λi(bΣ(S,S))−dX
i=1λi(bΣ(S′,S′))
≤dX
i=1X
j∈S∩Aλi(β(j,i))2−dX
i=1X
j∈S′∩Aλi(β(j,i))2+dλd
8sµ2
≤ −dX
i=1X
j∈(S′\S)∩Aλi(β(j,i))2+dλd
8sµ2. (9.2)
31Notice that expression (3.1) implies that
dX
i=1λi(β(j,i))2≥λd∥β(j,·)∥2
2≥dλd
sµ2(9.3)
for any j∈ A. Then, combining (9.2) and (9.3), we have
dX
i=1λi(bΣ(S,S))<dX
i=1λi(bΣ(S′,S′)). (9.4)
Given the above expression, we now prove that on the event Ω, for some fixed j∈ Aand
j′/∈ A, it holds that
qj≥qj′, (9.5)
where qk=P(k∈Sa,b∗(a)|X)for any k∈[p]and some fixed a∈[A]. We begin the proof.
Define for ˜j∈ {j, j′}andb∈[B]the following sets:
Sb,˜j:={(Sa,1, . . . , S a,B) :b∗(a) =b,˜j∈Sa,b},and
Sb:={(Sa,1, . . . , S a,B) :b∗(a) =b}.
Let the map ψ:Sk→ S kbe defined such that
ψ(S) :=(S\ {j′})∪ {j}ifj′∈Sandj /∈S,
S otherwise .
Then for every S∈ Sk, either ψ(S) =SorS∩A⊊ψ(S)∩A. Hence, inequality (9.4) implies
that
dX
i=1λi(bΣ(S,S))≤dX
i=1λi(bΣ(ψ(S),ψ(S))). (9.6)
Moreover, by the definition of ψ(·), we know that if j′∈Sfor some S∈ Sk, then j∈ψ(S).
Thus, for any (Sa,1, . . . , S a,b, . . . , S a,B)∈ Sb,j′, there exists (Sa,1, . . . , ψ (Sa,b), . . . , S a,B)∈ Sb,j.
32To see this clearly, since (Sa,1, . . . , S a,b, . . . , S a,B)∈ Sb,j′, then b∗(a) =bandj′∈Sa,b. Notice
thatb∗(a) =bimplies that
max
b∈[B]dX
i=1λi(bΣ(Sa,b,Sa,b))≤dX
i=1λi(bΣ(Sa,b,Sa,b)).
Together with (9.6), we know that
max
b∈[B]dX
i=1λi(bΣ(Sa,b,Sa,b))≤dX
i=1λi(bΣ(ψ(Sa,b),ψ(Sa,b))),
which implies that (Sa,1, . . . , ψ (Sa,b), . . . , S a,B)satisfies b∗(a) =b. Combining the fact that
j∈ψ(Sa,b), we obtain that (Sa,1, . . . , ψ (Sa,b), . . . , S a,B)∈ Sb,j. Hence, |Sb,j′| ≤ |S b,j|. Then,
onΩ, it holds for all b∈[B]that
P{j∈Sa,b∗(a)|X, b∗(a) =b}
=P{j∈Sa,b∗(a), b∗(a) =b|X}
P{b∗(a) =b|X}=|Sb,j|
|Sb|
≥|Sb,j′|
|Sb|=P{j′∈Sa,b∗(a), b∗(a) =b|X}
P{b∗(a) =b|X}
=P{j′∈Sa,b∗(a)|X, b∗(a) =b}.
Therefore, we have proved qj≥qj′as shown in (9.5).
Notice that
X
˜j∈[p]q˜j=X
˜j∈[p]P{˜j∈Sa,b∗(a)|X}=1
BX
b∈[B]X
˜j∈[p]P{˜j∈Sa,b∗(a)|b∗(a) =b,X}=X
˜j∈[p]k
p=k .
Thus, by (9.5) we obtain on Ωthat
qj≥P
˜j∈([p]\A∪{ j})q˜j
p−s+ 1=k−P
˜j∈A\{ j}q˜j
p−s+ 1≥k−s+ 1
p−s+ 1≥1
p. (9.7)
33Remark 3.Since Cov(X) =Ip,b∗(a)defined in Algorithm 1 reduces to
b∗(a) :=sargmaxb∈[B]dX
i=1ˆλa,b;i,
where ˆλa,b;i=λi(Pa,bbΣPa,b), the i-th largest eigenvalue of the matrix Pa,bbΣPa,b.
Define λa,b;i:=λi(Pa,bΣPa,b)and the corresponding eigenvector va,b;i:=vi(Pa,bΣPa,b)for
b∈[B]andi∈[p]. Notice that Σhasdnone-zero eigenvalues, so does Pa,bΣPa,b. Then
λa,b;d+1=. . .=λa,b;p= 0. Write Va,b:= (va,b;1, . . . ,va,b:d),bVa,b:= (ˆva,b;1, . . . , ˆva,b:d),Λa,b=
diag( λa,b;1, . . . , λ a,b;d)andbΛa,b= diag( ˆλa,b;1−ˆλa,b;d+1, . . . , ˆλa,b;d−ˆλa,b;d+1). For ˜j∈ {j, j′}, let
ˆw(˜j)
a:= (bVa,b∗(a)bΛa,b∗(a)bV⊤
a,b∗(a))(˜j,˜j)=dX
i=1(ˆλa,b∗(a);i−ˆλa,b∗(a);d+1)(ˆv(˜j)
a,b∗(a);i)2.
We now give upper bound and lower bound of ˆw(˜j)
a.
Notice that
Va,b∗(a)Λa,b∗(a)V⊤
a,b∗(a)=pX
i=1λa,b∗(a);iva,b∗(a);iv⊤
a,b∗(a);i=Pa,b∗(a)ΣPa,b∗(a),
whichimpliesthat (Va,b∗(a)Λa,b∗(a)V⊤
a,b∗(a))(˜j,˜j)= Σ(˜j,˜j)for˜j∈Sa,b∗(a)and(Va,b∗(a)Λa,b∗(a)V⊤
a,b∗(a))(˜j,˜j)=
0otherwise. By Lemma 2 in Appendix A.5 of Gataric et al. (2020), on Ωwe have
|(bVa,b∗(a)bΛa,b∗(a)bV⊤
a,b∗(a))(˜j,˜j)−(Va,b∗(a)Λa,b∗(a)V⊤
a,b∗(a))(˜j,˜j)|
(1)
≤ ∥bVa,b∗(a)bΛa,b∗(a)bV⊤
a,b∗(a)−Va,b∗(a)Λa,b∗(a)V⊤
a,b∗(a)∥op≤4d∥Pa,b∗(a)(bΣ−Σ)Pa,b∗(a)∥op
(2)
≤4Kdr
klog(p)
n(3.3)
≤dλd
4sµ2, (9.8)
where inequality (1)is obtained by the definition of the operator norm of a matrix and (2)
is implied by Lemma 1. Thus, on Ω∩ {j∈Sa,b∗(a)}, we have
ˆw(j)
a= (bVa,b∗(a)bΛa,b∗(a)bV⊤
a,b∗(a))(j,j)−(Va,b∗(a)Λa,b∗(a)V⊤
a,b∗(a))(j,j)+ (Va,b∗(a)Λa,b∗(a)V⊤
a,b∗(a))(j,j)
≥Σ(j,j)− |(bVa,b∗(a)bΛa,b∗(a)bV⊤
a,b∗(a))(j,j)−(Va,b∗(a)Λa,b∗(a)V⊤
a,b∗(a))(j,j)|(9.8)
≥Σ(j,j)−dλd
4sµ2
34≥λd∥V(j,·)∥2
2−dλd
4sµ2(3.1)
≥3dλd
4sµ2. (9.9)
Similarly, on Ω∩ {j∈Sa,b∗(a)}, we have
ˆw(j)
a≤5dλ1µ2
4s. (9.10)
Furthermore, on Ω∩ {j′∈Sa,b∗(a)}, it holds that
−dλd
4sµ2≤ˆw(j′)
a≤dλd
4sµ2(9.11)
by noting that j′∈ AcandΣ(j′,j′)=Pd
i=1λi(β(j′)
i)2= 0. Finally, for all j∈[p], ifj /∈Sa,b∗(a),
then by the definition of ˆva,b∗(a);ifori∈[d], we know that (ˆva,b∗(a);i)(j)= 0forisatisfying
ˆλa,b∗(a);i>0. Thus, by the definition of ˆw(j)
a, it holds that ˆw(j)
a= 0forj /∈Sa,b∗(a). Hence, we
have given upper bound and lower bound of ˆw(˜j)
a. Using the lower bound and upper bound
given above. we obtain on Ωthat
E( ˆw(j)
a−ˆw(j′)
a|X)
=E[ ˆw(j)
a(1{j∈Sa,b∗(a)}+1{j /∈Sa,b∗(a)})|X]−E[ ˆw(j′)
a(1{j′∈Sa,b∗(a)}+1{j′/∈Sa,b∗(a)})|X]
=E( ˆw(j)
a1{j∈Sa,b∗(a)} −ˆw(j′)
a1{j′∈Sa,b∗(a)}|X)
≥3qjdλd
4sµ2−qj′dλd
4sµ2(9.5)
≥qjdλd
2sµ2(9.7)
≥dλd
2psµ2. (9.12)
Now we let a,jandj′freely vary again. Define Ω1:={min j∈Aˆw(j)>max j∈Acˆw(j)}.
Since l≥s, onΩ1, it holds that A ⊆ ˆS, which implies that the dleading eigenvectors of
PˆSΣPˆSare the same as those of Σ. Hence, by Lemma 1 and Theorem 2 of Yu et al. (2015),
onΩ∩Ω1,
L(bβ,β)≤2d1/2∥PˆS(bΣ−Σ)PˆS∥op
λd≤2Ks
dllog(p)
nλ2
d.
Thenitsufficestoderivativethelowerboundof P(Ω∩Ω1). Observethat ˆw(j)=A−1PA
a=1ˆw(j)
a
35for any j∈[p]. Then, for any j∈ Aandj′∈ Ac, onΩ, it holds that
ˆw(j)−ˆw(j′)
={ˆw(j)−E( ˆw(j)|X)} − { ˆw(j′)−E( ˆw(j′)|X)}+E( ˆw(j)−ˆw(j′)|X)
(9.12)
≥dλd
2psµ2+{ˆw(j)−E( ˆw(j)|X)} − { ˆw(j′)−E( ˆw(j′)|X)},
which implies
Ωc
1⊆[
j∈A
ˆw(j)−E( ˆw(j)|X)≤dλd
4psµ2
∪[
j′/∈A
ˆw(j′)−E( ˆw(j′)|X)≥dλd
4psµ2
.
Notice that conditioned on X,{ˆw(j)
a}a∈[A]are i.i.d. random variables, and (9.9), (9.10) and
(9.11) implies that ˆw(j)
ais bounded on Ωfor all j∈[p]. Thus, by the union bound and the
Hoeffding’s inequality conditioned on X, onΩwe obtain that
P(Ωc
1|X)≤pexp
−Aλ2
d
50p2µ8λ2
1
.
We complete the proof by noting that
P(Ω∩Ω1)≥1−cp−3−pexp
−Aλ2
d
50p2µ8λ2
1
.
□
9.2 Proof of Theorem 2
Recall Sk:={S⊂[p] :|S|=k}andA={j∈[p] :β(j,·)̸=0}. For any S∈ Sk, notice that
M(S,S)=γ(S,·)Λ(γ(S,·))⊤. Hence,
dX
i=1λi(Σ(S,S)
E(X|Y),Σ(S,S)) =dX
i=1λi(M(S,S)) (9.13)
=kX
i=1λi(M(S,S))−kX
i=d+1λi(M(S,S)) = tr( M(S,S)) =dX
i=1X
j∈S∩Aλi(γ(j,i))2,(9.14)
36where equation (9.13) is obtained by the following lemma.
Lemma 2. For any S∈ S k, letΣ(S,S)=L(S,S)L⊤
(S,S)andΣ=LL⊤be the Cholesky
decomposition of Σ(S,S)andΣ, respectively. It then holds that L(S,S)=L(S,S)and
L−1
(S,S)Σ(S,S)
E(X|Y)(L−1
(S,S))⊤={L−1ΣE(X|Y)(L−1)⊤}(S,S)=M(S,S)
for some ordering of the random vector X.
Then, by similar arguments of (3.4), we get
λi(Σ(S,S)
E(X|Y),Σ(S,S)) =λi(L−1
(S,S)Σ(S,S)
E(X|Y)(L−1
(S,S))⊤) =λi(M(S,S)),
which gives (9.13).
By Assumption (A3), we know there is an event Ωwith probability at least 1−c′p−c2
such that on Ωwe have
sup
S∈Skmax{∥bL(S,S)−L(S,S)∥op,∥cM(S,S)−M(S,S)∥op} ≤Kr
klogp
n,
and
sup
S∈Slmax{∥bL(S,S)−L(S,S)∥op,∥cM(S,S)−M(S,S)∥op} ≤Kr
llogp
n,
where c′>0is some constant independent of S. On the event Ωdefined above, by Weyl’s
inequality, we have that
dX
i=1λi(cM(S,S))−dX
i=1X
j∈S∩Aλi(γ(j,i))2(9.14)=dX
i=1{λi(cM(S,S))−λi(M(S,S))}
≤Kdr
klogp
n(3.7)
≤dλd
16sµ2.
Thus, on the event Ω, if(S∩ A)⊊(S′∩ A), then
dX
i=1λi(cM(S,S))−dX
i=1λi(cM(S′,S′))
37≤dX
i=1X
j∈S∩Aλi(γ(j,i))2−dX
i=1X
j∈S′∩Aλi(γ(j,i))2+dλd
8sµ2
≤ −dX
i=1X
j∈(S′\S)∩Aλi(γ(j,i))2+dλd
8sµ2. (9.15)
Notice that expression (3.5) implies that
dX
i=1λi(γ(j,i))2≥λd∥γ(j,·)∥2
2≥dλd
sµ2(9.16)
for any j∈ A. Then, combining (9.15) and (9.16), we have
dX
i=1λi(cM(S,S))<dX
i=1λi(cM(S′,S′)). (9.17)
Given the above expression, we now prove that on the event Ω, for some fixed j∈ Aand
j′/∈ A, it holds that
qj≥qj′, (9.18)
where qk=P(k∈Sa,b∗(a)|X)for any k∈[p]and some fixed a∈[A]. We begin the proof.
Define for ˜j∈ {j, j′}andb∈[B]the following sets:
Sb,˜j:={(Sa,1, . . . , S a,B) :b∗(a) =b,˜j∈Sa,b},and
Sb:={(Sa,1, . . . , S a,B) :b∗(a) =b}.
Let the map ψ:Sk→ S kbe defined such that
ψ(S) :=(S\ {j′})∪ {j}ifj′∈Sandj /∈S ,
S otherwise .
Then for every S∈ S k, either ψ(S) =SorS∩ A⊊ψ(S)∩ A. Hence, inequality (9.17)
38implies that
dX
i=1λi(cM(S,S))≤dX
i=1λi(cM(ψ(S),ψ(S))). (9.19)
Moreover, by the definition of ψ(·), we know that if j′∈Sfor some S∈ Sk, then j∈ψ(S).
Thus, for any (Sa,1, . . . , S a,b, . . . , S a,B)∈ Sb,j′, there exists (Sa,1, . . . , ψ (Sa,b), . . . , S a,B)∈ Sb,j.
To see this clearly, since (Sa,1, . . . , S a,b, . . . , S a,B)∈ Sb,j′, then b∗(a) =bandj′∈Sa,b. Notice
thatb∗(a) =bimplies that
max
b∈[B]dX
i=1λi(bΣ(Sa,b,Sa,b)
E(X|Y),bΣ(Sa,b,Sa,b))Lemma 2= max
b∈[B]dX
i=1λi(cM(Sa,b,Sa,b))
≤dX
i=1λi(bΣ(Sa,b,Sa,b)
E(X|Y),bΣ(Sa,b,Sa,b)) =dX
i=1λi(cM(Sa,b,Sa,b)).
Together with (9.19), we know that
max
b∈[B]dX
i=1λi(cM(Sa,b,Sa,b))≤dX
i=1λi(cM(ψ(Sa,b),ψ(Sa,b))),
which implies that (Sa,1, . . . , ψ (Sa,b), . . . , S a,B)satisfies b∗(a) =b. Combining the fact that
j∈ψ(Sa,b), we obtain that (Sa,1, . . . , ψ (Sa,b), . . . , S a,B)∈ Sb,j. Hence, |Sb,j′| ≤ |S b,j|. Then,
onΩ, it holds for all b∈[B]that
P{j∈Sa,b∗(a)|X, b∗(a) =b}
=P{j∈Sa,b∗(a), b∗(a) =b|X}
P{b∗(a) =b|X}=|Sb,j|
|Sb|
≥|Sb,j′|
|Sb|=P{j′∈Sa,b∗(a), b∗(a) =b|X}
P{b∗(a) =b|X}
=P{j′∈Sa,b∗(a)|X, b∗(a) =b}.
Therefore, we have proved qj≥qj′as shown in (9.18).
39Notice that
X
˜j∈[p]q˜j=X
˜j∈[p]P{˜j∈Sa,b∗(a)|X}=1
BX
b∈[B]X
˜j∈[p]P{˜j∈Sa,b∗(a)|b∗(a) =b,X}=X
˜j∈[p]k
p=k .
Thus, by (9.18) we obtain on Ωthat
qj≥P
˜j∈([p]\A∪{ j})q˜j
p−s+ 1=k−P
˜j∈A\{ j}q˜j
p−s+ 1≥k−s+ 1
p−s+ 1≥1
p. (9.20)
Define λa,b;i:=λi(Σ(Sa,b,Sa,b)
E(X|Y),Σ(Sa,b,Sa,b))andˆλa,b;i:=λi(bΣ(Sa,b,Sa,b)
E(X|Y),bΣ(Sa,b,Sa,b))andva,b;i:=
vi(Σ(Sa,b,Sa,b)
E(X|Y),Σ(Sa,b,Sa,b))andˆva,b;i:=vi(bΣ(Sa,b,Sa,b)
E(X|Y),bΣ(Sa,b,Sa,b))forb∈[B]andi∈Sa,b. No-
tice that the pair (Σ(Sa,b,Sa,b)
E(X|Y),Σ(Sa,b,Sa,b))hasdnone-zero generalized eigenvalues. Then
λa,b;d+1=. . .=λa,b;k= 0. Write Va,b:= (va,b;1, . . . ,va,b:d),bVa,b:= (ˆva,b;1, . . . , ˆva,b:d),
Λa,b= diag( λa,b;1, . . . , λ a,b;d)andbΛa,b= diag( ˆλa,b;1−ˆλa,b;d+1, . . . , ˆλa,b;d−ˆλa,b;d+1). For
˜j∈Sa,b∗(a), let
ˆw(˜j)
a:= (bVa,b∗(a)bΛa,b∗(a)bV⊤
a,b∗(a))(˜j,˜j)=dX
i=1(ˆλa,b∗(a);i−ˆλa,b∗(a);d+1)(ˆv(˜j)
a,b∗(a);i)2.
We now give upper bound and lower bound of ˆw(˜j)
a.
Notice that
Va,b∗(a)Λa,b∗(a)V⊤
a,b∗(a)
=dX
i=1λa,b∗(a);iva,b∗(a);iv⊤
a,b∗(a);i(a)=kX
i=1λa,b∗(a);iva,b∗(a);iv⊤
a,b∗(a);i
(b)=L−1,⊤
(Sa,b∗(a),Sa,b∗(a))kX
i=1λa,b∗(a);iγa,b∗(a);iγ⊤
a,b∗(a);i
L−1
(Sa,b∗(a),Sa,b∗(a))
(c)=L−1,⊤
(Sa,b∗(a),Sa,b∗(a))(L−1
(Sa,b∗(a),Sa,b∗(a))Σ(Sa,b∗(a),Sa,b∗(a))
E(X|Y)L−1,⊤
(Sa,b∗(a),Sa,b∗(a)))L−1
(Sa,b∗(a),Sa,b∗(a))
(d)= (L(Sa,b∗(a),Sa,b∗(a)))−1,⊤M(Sa,b∗(a),Sa,b∗(a))(L(Sa,b∗(a),Sa,b∗(a)))−1
(e)= (L(Sa,b∗(a),Sa,b∗(a)))−1,⊤(γ(Sa,b∗(a),·)Λγ(Sa,b∗(a),·),⊤)(L(Sa,b∗(a),Sa,b∗(a)))−1, (9.21)
where Σ(Sa,b∗(a),Sa,b∗(a))=L(Sa,b∗(a),Sa,b∗(a))L⊤
(Sa,b∗(a),Sa,b∗(a))is the cholesky decomposition of
40Σ(Sa,b∗(a),Sa,b∗(a))andγa,b∗(a);i=vi(L−1
(Sa,b∗(a),Sa,b∗(a))Σ(Sa,b∗(a),Sa,b∗(a))
E(X|Y)L−1,⊤
(Sa,b∗(a),Sa,b∗(a))). Equation
(a)is implied by the fact that λa,b∗(a);d+1=. . .=λa,b∗(a);k= 0, (b) is obtained by a transfor-
mation similar to (3.4), (c) is given by the spectral decomposition, (d) uses Lemma 2, and
(e) is implied by the fact that M=γΛγ⊤.
LetS0=Sa,b∗(a)∩ AandSc
0=Sa,b∗(a)∩ Ac. Without loss of generality, we write
L(Sa,b∗(a),Sa,b∗(a))=
L(S0,S0)0
L(Sc
0,S0)L(Sc
0,Sc
0)
.
Then, recalling that the transformed basis γretains the row sparsity of β, we have by
equation (9.21) that
Va,b∗(a)Λa,b∗(a)V⊤
a,b∗(a)=
L(S0,S0),−1,⊤∗
0⊤∗

γ(S0,·)Λγ(S0,·),⊤0
0 0

L(S0,S0),−10
∗ ∗

=
L(S0,S0),−1,⊤(γ(S0,·)Λγ(S0,·),⊤)0
0 0

L(S0,S0),−10
∗ ∗

=
L(S0,S0),−1,⊤(γ(S0,·)Λγ(S0,·),⊤)L(S0,S0),−10
0 0
,
which implies that (Va,b∗(a)Λa,b∗(a)V⊤
a,b∗(a))(˜j,˜j)= (L(˜j,˜j))−2M(˜j,˜j)for˜j∈S0under some or-
dering of Xand(Va,b∗(a)Λa,b∗(a)V⊤
a,b∗(a))(˜j,˜j)= 0otherwise. Noting that Kp
klog(p)/n≤
min{λ1/(4d),√θ1, θp/(6√θ1)}, onΩwe have
|(bVa,b∗(a)bΛa,b∗(a)bV⊤
a,b∗(a))(˜j,˜j)−(Va,b∗(a)Λa,b∗(a)V⊤
a,b∗(a))(˜j,˜j)|
(1)
≤ ∥bVa,b∗(a)bΛa,b∗(a)bV⊤
a,b∗(a)−Va,b∗(a)Λa,b∗(a)V⊤
a,b∗(a)∥op
=(bL(Sa,b∗(a),Sa,b∗(a)))−1,⊤dX
i=1(ˆλa,b∗(a);i−ˆλa,b∗(a);d+1)bγa,b∗(a);ibγ⊤
a,b∗(a);i
(bL(Sa,b∗(a),Sa,b∗(a)))−1−
(L(Sa,b∗(a),Sa,b∗(a)))−1,⊤dX
i=1(λa,b∗(a);i−λa,b∗(a);d+1)γa,b∗(a);iγ⊤
a,b∗(a);i
(L(Sa,b∗(a),Sa,b∗(a)))−1
op
(2)
≤4−1Cdr
klog(p)
n(3.7)
≤dτλ d
4sµ2, (9.22)
41where bγa,b∗(a);i=vi((bL(Sa,b∗(a),Sa,b∗(a)))−1bΣ(Sa,b∗(a),Sa,b∗(a))
E(X|Y)(bL(Sa,b∗(a),Sa,b∗(a)))−1,⊤),bΣ=bLbL⊤is
the Cholesky decomposition of bΣfor some ordering of X. Inequality (1)is obtained by the
definition of the operator norm of a matrix, and (2) is implied by Assumptions (A1), (A1’),
(A3), the triangle inequality, and Lemma 2 of Gataric et al. (2020). Thus, on Ω∩ {j∈
Sa,b∗(a)}, we have by Assumption (A4) that
ˆw(j)
a= (bVa,b∗(a)bΛa,b∗(a)bV⊤
a,b∗(a))(j,j)−(Va,b∗(a)Λa,b∗(a)V⊤
a,b∗(a))(j,j)+ (Va,b∗(a)Λa,b∗(a)V⊤
a,b∗(a))(j,j)
≥(L(j,j))−2M(j,j)− |(bVa,b∗(a)bΛa,b∗(a)bV⊤
a,b∗(a))(j,j)−(Va,b∗(a)Λa,b∗(a)V⊤
a,b∗(a))(j,j)|
(9.22)
≥(L(j,j))−2M(j,j)−dτλ d
4sµ2≥τλd∥γ(j,·)∥2
2−dτλ d
4sµ2(3.5)
≥3dτλ d
4sµ2. (9.23)
Similarly, on Ω∩ {j∈Sa,b∗(a)}, we have
ˆw(j)
a≤5dλ1µ2
4τs. (9.24)
Furthermore, on Ω∩ {j′∈Sa,b∗(a)}, it holds that
−dτλ d
4sµ2≤ˆw(j′)
a≤dτλ d
4sµ2. (9.25)
Finally, for all j∈[p], ifj /∈Sa,b∗(a), then by the definition of ˆva,b∗(a);ifori∈[d]in Algorithm
1, we know that (ˆva,b∗(a);i)(j)= 0. Thus, by the definition of ˆw(j)
a, it holds that ˆw(j)
a= 0for
j /∈Sa,b∗(a). Hence, we have given upper bound and lower bound of ˆw(˜j)
a. Using the lower
bound and upper bound given above, we obtain on Ωthat
E( ˆw(j)
a−ˆw(j′)
a|X)
=E[ ˆw(j)
a(1{j∈Sa,b∗(a)}+1{j /∈Sa,b∗(a)})|X]−E[ ˆw(j′)
a(1{j′∈Sa,b∗(a)}+1{j′/∈Sa,b∗(a)})|X]
=E( ˆw(j)
a1{j∈Sa,b∗(a)} −ˆw(j′)
a1{j′∈Sa,b∗(a)}|X)
≥3qjdτλ d
4sµ2−qj′dτλ d
4sµ2(9.18)
≥qjdτλ d
2sµ2(9.20)
≥dτλ d
2psµ2. (9.26)
Now we let a,jandj′freely vary again. Define Ω1:={min j∈Aˆw(j)>max j∈Acˆw(j)}.
Letγ(ˆS,·)=Vd(M(ˆS,ˆS))andbγ(ˆS,·)=Vd(cM(ˆS,ˆS))denote the dleading eigenvectors of M(ˆS,ˆS)
42andcM(ˆS,ˆS), respectively. Then, by Assumption (A3) and Theorem 2 of Yu et al. (2015), on
Ω∩Ω1,
L(bγ(ˆS,·),γ(ˆS,·))≤2d1/2∥cM(ˆS,ˆS)−M(ˆS,ˆS)∥op
λd≤2Ks
dllog(p)
nλ2
d.
Since l≥s, on Ω1, it holds that A ⊆ ˆS, which implies that γ= (γ(ˆS,·),⊤,0⊤)⊤. Let
bγ= (bγ(ˆS,·),⊤,0⊤)⊤. We note that the true parameter β= (γ(ˆS,·),⊤L(ˆS,ˆS),−1,0⊤)andbβ=
(bγ(ˆS,·),⊤bL(ˆS,ˆS),−1,0⊤)for some ordering of X. Then, by Assumptions (A1), (A1’) and (A3),
noting that Kp
llog(p)/n≤min{λd/(2√
2),√θ1, θp/(6√θ1)}, onΩwe have
L(β,bβ) =L(β(ˆS,·),bβ(ˆS,·)) =∥β(ˆS,·)β(ˆS,·),⊤−bβ(ˆS,·)bβ(ˆS,·),⊤∥F≤Cs
dllog(p)
nλ2
d
for some sufficiently large constant C >0.
It suffices to derivative the lower bound of P(Ω∩Ω1). Observe that ˆw(j)=A−1PA
a=1ˆw(j)
a
for any j∈[p]. Then, for any j∈ Aandj′∈ Ac, onΩ, it holds that
ˆw(j)−ˆw(j′)
={ˆw(j)−E( ˆw(j)|X)} − { ˆw(j′)−E( ˆw(j′)|X)}+E( ˆw(j)−ˆw(j′)|X)
(9.26)
≥dτλ d
2psµ2+{ˆw(j)−E( ˆw(j)|X)} − { ˆw(j′)−E( ˆw(j′)|X)},
which implies
Ωc
1⊆[
j∈A
ˆw(j)−E( ˆw(j)|X)≤dτλ d
4psµ2
∪[
j′/∈A
ˆw(j′)−E( ˆw(j′)|X)≥dτλ d
4psµ2
.
Notice that conditioned on X,{ˆw(j)
a}a∈[A]are i.i.d. random variables, and (9.23), (9.24) and
(9.25) implies that ˆw(j)
ais bounded on Ωfor all j∈[p]. Thus, by the union bound and the
Hoeffding’s inequality conditioned on X, onΩwe obtain that
P(Ωc
1|X)≤pexp
−Aτ4λ2
d
50p2µ8λ2
1
.
43We complete the proof by noting that
P(Ω∩Ω1)≥1−c′p−c2−pexp
−Aτ4λ2
d
50p2µ8λ2
1
.
□
9.3 Proof of Theorem 3
Consider a specific case where d= 1andH= 2. In this case, the parameter of interest
β∈Θp,d,s(3)reduces to a 1-dimensional vector β1∈Θp,1,s(3). We consider the following
structure:
X|(˜Y= 1)∼ N p((1−α)β1,Ip−M),P(˜Y= 1) = α ,
X|(˜Y= 2)∼ N p(−αβ1,Ip−M),P(˜Y= 2) = 1 −α , (9.27)
where α∈(0,1)andMis a proper positive definite matrix to be defined later. Under this
structure, we have the following three results:
(i)EX=E{E(X|˜Y)}=0;
(ii) Define µh=E{X|(˜Y=h)}andph=P(˜Y=h)forh∈[H]. Then Cov{E(X|˜Y)}=
PH
h=1phµhµ⊤
h−(PH
h=1phµh)(PH
h=1phµh)⊤=PH
h=1phµhµ⊤
h=α(1−α)β1β⊤
1;
(iii) Therefore, Cov(X) =E(XX⊤) = Cov {E(X|˜Y)}+E{Cov(X|˜Y)}=α(1−α)β1β⊤
1+
Ip−M.
Letting M= Cov {E(X|˜Y)}=α(1−α)β1β⊤
1, we have Cov(X) =Ip. Hence, the
nonzero generalized eigenvalues of the pair (Cov{E(X|˜Y)},Cov(X))reduce to the nonzero
eigenvaluesof Cov{E(X|˜Y)}. Recallthat Cov{E(X|˜Y)}=α(1−α)β1β⊤
1,whichimpliesthat
Cov{E(X|˜Y)}has only one nonzero eigenvalue λ1=α(1−α)and the leading eigenvector,
which is the parameter of our interest, corresponds to β1.
We will use the Fano’s lemma (Yu, 1997) to derive the lower bound. To apply this lemma,
we go in three steps: (i) to construct some β∈Θp,1,s(3); (ii) to derive the Kullback-Leiber
44divergence between data distributions of interest, i.e., the constructed mixture Gaussian
distribution; (iii) to find a subset of Θp,1,s(3)with a proper packing number.
Proof of Step (i).We consider a specific setting of Gataric et al. (2020) (Proposition 1)
where the parameter of interest is now a vector, not a matrix. Define u= (s−1/21⊤
s,0⊤)⊤∈
Rp, and let ϵ∈ 
0,p
1/(16s)
. For any J∈Θp−1,1,s−1:={V∈Θp−1,1: supp( V)≤s−1},
sinceu⊤u= 1, we can select eU∈Θp,psuch that the first column of eUisuand the indexes
of the nonzero components of βJ=eU(√
1−ϵ2, ϵJ⊤)⊤are a subset of [s]. Notice that
βJ=eU
√
1−ϵ2
ϵJ
=u+eU
√
1−ϵ2−1
ϵJ
=:u+eU∆J.
Due to ∥eU∆J∥∞≤ ∥eU∆J∥2≤√
2ϵandϵ≤p
1/(16s),|β(k)
J| ∈[0.64/√s,1.36/√s]for any
k∈[s], which implies that βJ∈Θp,1,s(3).
Proof of Step (ii).Forany J, J′∈Θp−1,1,s−1, letMJ=λ1βJβ⊤
JandMJ′=λ1βJ′β⊤
J′, and
then denote by P(βJ,MJ),P(βJ′,MJ′)the corresponding mixture Gaussian distributions
specified in (9.27). Write DKL(P∥Q)for the KL divergence from a distribution PtoQ. By
the convexity of KL divergence, we have
DKL 
P(βJ,MJ)∥P(βJ′,MJ′)
≤αDKL 
Np((1−α)βJ,Ip−MJ)∥Np((1−α)βJ′,Ip−MJ′)
(9.28)
+ (1−α)DKL 
Np(−αβJ,Ip−MJ)∥Np(−αβJ′,Ip−MJ′)
.(9.29)
Hence, it suffices to bound the KL divergence between two Gaussian distributions. For the
first KL divergence in (9.28), we have
DKL 
Np((1−α)βJ,Ip−MJ)∥Np((1−α)βJ′,Ip−MJ′)
=1
2
[Tr{(Ip−MJ′)−1(Ip−MJ)} −p]| {z }
T1+ logdet(Ip−MJ′)
det(Ip−MJ)
| {z }
T2
+ (1−α)2(βJ′−βJ)⊤(Ip−MJ′)−1(βJ′−βJ)| {z }
T3
.
45For the first term, it holds that
T1= Tr{(Ip−MJ′)−1(MJ′−MJ)}
= Tr
Ip+λ1
1−λ1βJ′β⊤
J′
(λ1βJ′β⊤
J′−λ1βJβ⊤
J)
=λ2
1
1−λ1{1−Tr(βJ′β⊤
J′βJβ⊤
J)}
≤2λ2
1
1−λ1∥βJ−βJ′∥2
2.
For the term T2, noting that Ip−MJandIp−MJ′have the same eigenvalues, we then
obtain T2= 0. For the term T3, we have T3≤(1−α)2λmax{(Ip−MJ′)−1}∥βJ−βJ′∥2
2=
(1−α)2(1−λ1)−1∥βJ−βJ′∥2
2. Combining the upper bounds for the three terms, we finally
obtain
DKL 
Np((1−α)βJ,Ip−MJ)∥Np((1−α)βJ′,Ip−MJ′)
≤2λ2
1+ (1−α)2
2(1−λ1)∥βJ−βJ′∥2
2.
(9.30)
The second KL divergence in (9.29) has a similar upper bound
DKL 
Np(−αβJ,Ip−MJ)∥Np(−αβJ′,Ip−MJ′)
≤2λ2
1+α2
2(1−λ1)∥βJ−βJ′∥2
2.(9.31)
Combining (9.28)-(9.31), we have
DKL 
P(βJ,MJ)∥P(βJ′,MJ′)
≤2λ2
1+λ1
2(1−λ1)∥βJ−βJ′∥2
2=2λ2
1+λ1
1−λ1(1−β⊤
JβJ′).
Recall βJ=eU(√
1−ϵ2, ϵJ⊤)⊤for any J∈Θp−1,1,s−1. Then βJβ⊤
J′= 1−ϵ2+ϵ2J⊤J′≥
1−ϵ2−ϵ2∥J∥2∥J′∥2= 1−2ϵ2. Hence,
DKL 
P(βJ,MJ)∥P(βJ′,MJ′)
≤2ϵ2(2λ2
1+λ1)
1−λ1. (9.32)
Proof of Step (iii).LetJ ⊂ Θp−1,1,s−1such that min J,J′∈JL(βJ,βJ′)≥cϵfor some
universal constant c >0and3≤ |J | ≤ exp{n/(4s)}, which will be specified later. Let ˜βbe
46any possible sparse SIR estimator for β. Then by (9.32) and Fano’s lemma (Yu, 1997), we
obtain
inf
˜βsup
β∈Θp−1,1,s−1(3)EPβ{L(˜β,β)} ≥inf
˜βmax
J∈JEPβJ{L(˜β,βJ)}
≥cϵ
2
1−2nϵ2(2λ2
1+λ1)/(1−λ1) + log 2
log|J |
≥cϵ
21
3−nϵ2
log|J |
, (9.33)
where we employed the fact that |J | ≥ 3andλ1=α(1−α)≤1/4. Noting that log|J | ≤
n/(4s), we can select ϵ2= log( |J |)/(4n)≤1/(16s). Combining (9.33), we obtain
inf
˜βsup
β∈Θp,1,s(3)EPβ{L(˜β,β)}≳r
log|J |
n. (9.34)
It remains to construct J. LetS={S∈[p−1] :|S|=s−1}. For any S∈ S, define
JS∈Rp−1such that J(S)
S= (s−1)−1/21s−1andJ(Sc)
S=0. Then JS∈Θp−1,1,s−1. By the
Varshamov-Gilbert’s lemma (Lemma 4.10 in Massart (2007)), since 4(s−1)≤p−1, there
exists a subset TofSsuch that
log|T |=1
5(s−1) logp−1
s−1
(9.35)
and|T∩T′| ≤(s−1)/2for any T, T′∈ Twith T̸=T′. LetJ={JT:T∈ T }, and then
|J |=|T |. Moreover, the fact that |T∩T′| ≤(s−1)/2for any T, T′∈ Twith T̸=T′
implies that
min
J,J′∈J:J̸=J′L(J, J′) = min
J,J′∈J:J̸=J′p
1−(J⊤J′)2≥s
1−1
22
≥√
3
2. (9.36)
Notice that
L(βJ,βJ′) =1√
2∥βJβ⊤
J−βJ′β⊤
J′∥F=q
ϵ4L2(J, J′) +ϵ2(1−ϵ2)∥J−J′∥2
2≥ϵL(J, J′)
47for any J∈Θp−1,1,s−1. Combining (9.36), we have
min
J,J′∈J:J̸=J′L(βJ,βJ′)≥√
3
2ϵ .
Finally, since (s−1) log{(p−1)/(s−1)} ≥6and5n≥4s(s−1) log{(p−1)/(s−1)}, we
have 3≤ |J | ≤ exp{n/(4s)}. Therefore, (9.34) and (9.35) implies that
inf
˜βsup
β∈Θp,1,s(3)EPβ{L(˜β,β)}≳r
slog(p/s)
n.
We complete the proof. □
9.4 Proof of auxiliary lemmas
9.4.1 Proof of Lemma 1
Without loss of generality, we assume EX=0. Then, by definition
Σ:=ΣE(X|Y)= Cov {E(X|˜Y)}=E[{E(X|˜Y)}{E(X|˜Y)}⊤]
=HX
h=1P(˜Y=h){E(X|˜Y=h)}{E(X|˜Y=h)}⊤.
Notice that
E[X1{˜Y=h}] =E(E[X1{˜Y=h}|1{˜Y=h}])
=E(1{˜Y=h}E[X|1{˜Y=h}])
=P[1{˜Y=h}= 1]·E(X|1{˜Y=h}= 1)
=P(˜Y=h)]·E(X|˜Y=h)
=:ph·E(X|˜Y=h),
which implies that
Σ=HX
h=1E[X1{˜Y=h}](E[X1{˜Y=h}])⊤
ph=:HX
h=1uhu⊤
h
ph.
48By definition, we also have
bΣ:=bΣE(X|Y)=HX
h=1ˆuhˆu⊤
h
ˆph,
where ˆph=n−1Pn
i=11{˜Yi=h}andˆuh=n−1Pn
i=1Xi1{˜Yi=h}. Therefore, we obtain
bΣ−Σ=HX
h=1ˆuhˆu⊤
h
ˆph−uhu⊤
h
ph
=HX
h=1ˆuhˆu⊤
h
ˆph−uhu⊤
h
ˆph+uhu⊤
h
ˆph−uhu⊤
h
ph
=HX
h=1{ˆp−1
h(ˆuhˆu⊤
h−uhu⊤
h) + (ˆp−1
h−p−1
h)uhu⊤
h}
=HX
h=1{p−1
h(ˆuhˆu⊤
h−uhu⊤
h) + (ˆp−1
h−p−1
h)(ˆuhˆu⊤
h−uhu⊤
h) + (ˆp−1
h−p−1
h)uhu⊤
h}.
For any unit vector α∈Rp, it holds that
α⊤(bΣ−Σ)α
=HX
h=1{p−1
h|α⊤(ˆuh−uh)|2+ 2p−1
hα⊤uh(ˆuh−uh)⊤α+ (ˆp−1
h−p−1
h)|α⊤(ˆuh−uh)|2
+ 2(ˆp−1
h−p−1
h)α⊤uh(ˆuh−uh)⊤α+ (ˆp−1
h−p−1
h)|α⊤uh|2}. (9.37)
Now we claim that ∥uh∥2can be bounded by a constant for all h∈[H]. By definition,
Σ=HX
h=1uhu⊤
h
ph=βΛβ⊤,
where β= (β1, . . . ,βd)⊤andΛ= diag {λ1, . . . , λ d}. Taking trace on both sides, we obtain
PH
h=1p−1
h∥uh∥2
2=Pd
i=1λi≤Cby Assumption (A1), where the constant C >0depends on
d,κandλ. Observing that 0< ph<1, (9.37) leads to
|α⊤(bΣ−Σ)α|
49≤HX
h=1p−1
h|α⊤(ˆuh−uh)|2+ 2HX
h=1p−1
h|(ˆuh−uh)⊤α| · |uh|2+HX
h=1|ˆp−1
h−p−1
h| · |α⊤(ˆuh−uh)|2
+ 2HX
h=1|ˆp−1
h−p−1
h| · |(ˆuh−uh)⊤α| · |uh|2+HX
h=1|ˆp−1
h−p−1
h| · |uh|2
2
≲HX
h=1|α⊤(ˆuh−uh)|2+HX
h=1|ˆp−1
h−p−1
h| · |α⊤(ˆuh−uh)|2
+HX
h=1|(ˆuh−uh)⊤α|+HX
h=1|ˆp−1
h−p−1
h| · |(ˆuh−uh)⊤α|+HX
h=1|ˆp−1
h−p−1
h|. (9.38)
Consider the term α⊤(ˆuh−uh). Let Zi(h) =α⊤Xi1{˜Yi=h} −E[α⊤Xi1{˜Yi=h}]for
any unit vector α∈Rpandh∈[H]. Then, α⊤(ˆuh−uh) =n−1Pn
i=1Zi(h). Notice that
Xiis mixture-Gaussian distributed, which implies that {Zi(h)}n
1=1are i.i.d. sub-exponential
variables. Because {Xi,˜Yi}are i.i.d. random variables and Zi(h)are functions of Xiand
˜Yi, we know that {Zi(h)}n
1=1are also i.i.d. random variables. By the Bernstein inequality
(Vershynin, 2010), we have
P{|α⊤(ˆuh−uh)| ≥t} ≤2 exp{−Cn(t2∧t)}. (9.39)
For the term ˆp−1
h−p−1
h, it is easy to check that ˆph−ph=n−1Pn
i=11{˜Yi=h}−E[1{˜Yi=
h}],|1{˜Yi=h} −E[1{˜Yi=h}]| ≤1andVar(1{˜Yi=h} −E[1{˜Yi=h}])≤1/4. By
the Bernstein inequality (van der Vaart and Wellner, 1996), we have P(|ˆph−ph| ≥t)≤
2 exp{−Cn(t2∧t)}. Let pmin= min {p1, . . . , p H}. Due to |ˆp−1
h−p−1
h|=|ˆph−ph|/(phˆph), it
holds that
P(|ˆp−1
h−p−1
h| ≥2p−2
mint)
≤P(|ˆph−ph| ≥t) +P(phˆph≤p2
min/2)
≤P(|ˆph−ph| ≥t) +P(|ˆph−ph| ≥pmin/2)
≤4 exp{−Cn(t2∧t)} (9.40)
provided that pmin≥2t.
50Combing (9.38)-(9.40) and using the union bound, we have
P(|α⊤(bΣ−Σ)α| ≥t)≲Cexp{−C′n(t2∧t1/4)}.
By Lemma 2 of Wang et al. (2016b), if η >0satisfies klog(p/η)/n≤1, it then holds that
P
sup
u∈Bp−1
0(k)|α⊤(bΣ−Σ)α| ≥2Cr
klog(p/η)
n
≲k1/2p
k128√
255k−1
P
|α⊤(bΣ−Σ)α| ≥Cr
klog(p/η)
n
≲k1/2p
k128√
255k−1
exp{−C′C2klog(p/η)}
≲k1/2p
k128√
255k−1
p−kηk(choose C2= (C′)−1)
≲η .
Similarly, if klog(p/η)/n > 1, we have
P
sup
u∈Bp−1
0(k)|α⊤(bΣ−Σ)α| ≥2Cklog(p/η)
n4
≲η .
Combining the above two equations, we obtain
P
sup
u∈Bp−1
0(k)|α⊤(bΣ−Σ)α| ≥2Cmaxklog(p/η)
n4
,r
klog(p/η)
n
≤C′η .
Choose η=p−3. Noting that klogp≪nand (9.40) holds for sufficiently large n, we
complete the proof. □
9.4.2 Proof of Lemma 2
Without loss of generality, write
Σ=
Σ(S,S)Σ(S,Sc)
Σ(Sc,S)Σ(Sc,Sc)
andL=
L(S,S)0
L(Sc,S)L(Sc,Sc)
.
51Then,

Σ(S,S)Σ(S,Sc)
Σ(Sc,S)Σ(Sc,Sc)
=
L(S,S)0
L(Sc,S)L(Sc,Sc)

L(S,S),⊤L(Sc,S),⊤
0⊤L(Sc,Sc),⊤
=
L(S,S)L(S,S),⊤∗
∗ ∗
,
which implies that Σ(S,S)=L(S,S)L(S,S),⊤. Since Σ(S,S)=L(S,S)L⊤
(S,S)andL(S,S)is unique,
we obtain L(S,S)=L(S,S).
By the inverse of a block matrix, it holds that
L−1ΣE(X|Y)(L−1)⊤=
L(S,S)0
L(Sc,S)L(Sc,Sc)
−1
Σ(S,S)
E(X|Y)Σ(S,Sc)
E(X|Y)
Σ(Sc,S)
E(X|Y)Σ(Sc,Sc)
E(X|Y)

L(S,S)0
L(Sc,S)L(Sc,Sc)
−1,⊤
=
L(S,S),−10
−L(Sc,Sc),−1L(Sc,S)L(S,S),−1L(Sc,Sc),−1

Σ(S,S)
E(X|Y)Σ(S,Sc)
E(X|Y)
Σ(Sc,S)
E(X|Y)Σ(Sc,Sc)
E(X|Y)


L(S,S),−1,⊤−(L(Sc,Sc),−1L(Sc,S)L(S,S),−1)⊤
0⊤L(Sc,Sc),−1,⊤

=
L(S,S),−1Σ(S,S)
E(X|Y)L(S,S),−1Σ(S,Sc)
E(X|Y)
∗ ∗

L(S,S),−1,⊤∗
0⊤∗

=
L(S,S),−1Σ(S,S)
E(X|Y)L(S,S),−1,⊤∗
∗ ∗
.
Clearly, {L−1ΣE(X|Y)(L−1)⊤}(S,S)=L(S,S),−1Σ(S,S)
E(X|Y)L(S,S),−1,⊤. Combining L(S,S)=L(S,S),
we have {L−1ΣE(X|Y)(L−1)⊤}(S,S)=L−1
(S,S)Σ(S,S)
E(X|Y)(L−1
(S,S))⊤=M(S,S). We complete the
proof. □
References
Anaraki, F. P., and Hughes, S. (2014), “Memory and computation efficient PCA via very
sparse random projections,” in International Conference on Machine Learning , PMLR,
pp. 1341–1349.
Bondell, H. D., and Li, L. (2009), “Shrinkage inverse regression estimation for model-free
variable selection,” Journal of the Royal Statistical Society: Series B (Statistical Method-
ology), 71, 287–299.
52Bura, E., and Cook, R. D. (2001), “Extending sliced inverse regression: The weighted chi-
squared test,” Journal of the American Statistical Association , 96, 996–1003.
Cai, T.T., Ma, Z., andWu, Y.(2013), “SparsePCA:Optimalratesandadaptiveestimation,”
The Annals of Statistics , 41, 3074–3110.
Chen, X., Zou, C., and Cook, R. (2010), “Coordinate-Independent Sparse Sufficient Dimen-
sion Reduction and Variable Selection,” The Annals of Statistics , 38, 3696–3723.
Cook,R.(1996),“GraphicsforRegressionswithaBinaryResponse,” Journal of the American
Statistical Association , 91, 983–992.
Cook, R. D. (2004), “Testing predictor contributions in sufficient dimension reduction,” The
Annals of Statistics , 32, 1062–1092.
— (2007), “Fisher lecture: Dimension reduction in regression,” Statistical Science , 22, 1–26.
Cook, R. D., and Forzani, L. (2009), “Likelihood-based sufficient dimension reduction,” Jour-
nal of the American Statistical Association , 104, 197–208.
Cook, R. D., and Weisberg, S. (1991), “Discussion of sliced inverse regression for dimension
reduction,” Journal of the American Statistical Association , 86, 328–332.
Cook, R. D., and Yin, X. (2001), “Theory & methods: special invited paper: dimension
reduction and visualization in discriminant analysis (with discussion),” Australian & New
Zealand Journal of Statistics , 43, 147–199.
Fan, J., Shao, Q.-M., and Zhou, W.-X. (2018), “Are discoveries spurious? Distributions of
maximum spurious correlations and their applications,” Annals of statistics , 46, 989.
Gao, C., Ma, Z., Ren, Z., and Zhou, H. H. (2015), “Minimax estimation in sparse canonical
correlation analysis,” The Annals of Statistics , 43, 2168–2197.
Gao, C., Ma, Z.— (2017), “Sparse CCA: Adaptive Estimation and Computational Barriers,”
The Annals of Statistics , 45, 2074–2101.
Gataric, M., Wang, T., and Samworth, R. J. (2020), “Sparse Principal Component Analysis
via Axis-Aligned Random Projections,” Journal of the Royal Statistical Society: Series B
(Statistical Methodology) , 82, 329–359.
Hsing, T., and Carroll, R. J. (1992), “An asymptotic theory for sliced inverse regression,”
The Annals of Statistics , 1040–1061.
Hung, H., and Huang, S.-Y. (2019), “Sufficient dimension reduction via random-partitions
for the large-p-small-n problem,” Biometrics , 75, 245–255.
Li, B., and Wang, S. (2007), “On Directional Regression for Dimension Reduction,” Journal
of the American Statistical Association , 102, 997–1008.
53Li, K.(1991), “SlicedInverseRegressionforDimensionReduction,” (withdiscussion) Journal
of the American Statistical Association , 86, 316–327.
Li, K.-C. (1992), “On principal Hessian directions for data visualization and dimension re-
duction: Another application of Stein’s lemma,” Journal of the American Statistical As-
sociation , 87, 1025–1039.
Li, L. (2007), “Sparse sufficient dimension reduction,” Biometrika , 94, 603–613.
Li, L., Dennis Cook, R., and Nachtsheim, C. J. (2005), “Model-free variable selection,”
Journal of the Royal Statistical Society: Series B (Statistical Methodology) , 67, 285–299.
Li, L., Wen, X. M., and Yu, Z. (2020), “A Selective Overview of Sparse Sufficient Dimension
Reduction,” (with discussion) Statistical Theory and Related Fields , 4, 121–133.
Lin, Q., Li, X., Huang, D., and Liu, J. S. (2017), “On the optimality of sliced inverse
regression in high dimensions,” arXiv preprint arXiv:1701.06009 .
Lin, Q., Zhao, Z., and Liu, J. (2018), “On Consistency and Sparsity for Sliced Inverse Re-
gression in High Dimensions,” The Annals of Statistics , 46, 580–610.
Lin, Q., Zhao, Z., and Liu, J. S. (2019), “Sparse Sliced Inverse Regression via Lasso,” Journal
of the American Statistical Association , 114, 1726–1739.
Liu, C., Zhao, X., and Huang, J. (2023), “A Random Projection Approach to Hypothe-
sis Tests in High-Dimensional Single-Index Models,” Journal of the American Statistical
Association , 1–11.
Ma, Y., and Zhu, L. (2013), “A review on dimension reduction,” International Statistical
Review, 81, 134–150.
Massart, P. (2007), Concentration inequalities and model selection: Ecole d’Eté de Probabil-
ités de Saint-Flour XXXIII-2003 , Springer.
Qi, H., and Hughes, S. M. (2012), “Invariance of principal components under low-dimensional
random projection of the data,” in 2012 19th IEEE International Conference on Image
Processing , IEEE, pp. 937–940.
Tan, K., Shi, L., and Yu, Z. (2020), “Sparse SIR: Optimal Rates and Adaptive Estimation,”
The Annals of Statistics , 48, 64–85.
Tan, K. M., Wang, Z., Liu, H., and Zhang, T. (2018), “Sparse generalized eigenvalue prob-
lem,”Journal of the Royal Statistical Society. Series B (Statistical Methodology) , 80, 1057–
1086.
Thorgeirsson, T. E., Gudbjartsson, D. F., Surakka, I., Vink, J. M., Amin, N., Geller, F.,
Sulem, P., Rafnar, T., Esko, T., Walter, S. et al. (2010), “Sequence variants at CHRNB3–
CHRNA6 and CYP2A6 affect smoking behavior,” Nature genetics , 42, 448–453.
54Thorisson, G. A., Smith, A. V., Krishnan, L., and Stein, L. D. (2005), “The international
HapMap project web site,” Genome research , 15, 1592–1593.
van der Vaart, A. W., and Wellner, J. (1996), Weak convergence and empirical processes:
with applications to statistics , Springer Science & Business Media.
Vershynin, R. (2010), “Introduction to the non-asymptotic analysis of random matrices,”
arXiv preprint arXiv:1011.3027 .
Wang, T., Berthet, Q., and Samworth, R. J. (2016a), “Statistical and computational trade-
offsinestimationofsparseprincipalcomponents,” The Annals of Statistics , 44, 1896–1930.
Wang, T., Berthet, Q.— (2016b), “Statistical and computational trade-offs in estimation of
sparse principal components,” The Annals of Statistics , 44, 1896–1930.
Yin, X. (2011), “Sufficient dimension reduction in regression,” in High-dimensional Data
Analysis, World Scientific, pp. 257–273.
Yu, B. (1997), “Assouad, fano, and le cam,” in Festschrift for Lucien Le Cam , Springer, pp.
423–435.
Yu, Y., Wang, T., andSamworth, R.J.(2015), “AusefulvariantoftheDavis–Kahantheorem
for statisticians,” Biometrika , 102, 315–323.
Yu, Z., Dong, Y., and Shao, J. (2016), “On marginal sliced inverse regression for ultrahigh
dimensional model-free feature selection,” The Annals of Statistics , 44, 2594–2623.
Zhu, L., Miao, B., and Peng, H. (2006), “On sliced inverse regression with high-dimensional
covariates,” Journal of the American Statistical Association , 101, 630–643.
Zhu, L.-X., and Ng, K. W. (1995), “Asymptotics of sliced inverse regression,” Statistica
Sinica, 727–736.
55