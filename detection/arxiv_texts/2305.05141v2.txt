Optimal Sparse Sliced Inverse Regression via Random
Projection
Jia Zhang‚àóRunxiong Wu‚Ä†Xin Chen‚Ä°
August 4, 2023
Abstract
We propose a novel sparse sliced inverse regression method based on random pro-
jections in a large psmall nsetting. Embedded in a generalized eigenvalue framework,
the proposed approach finally reduces to parallel execution of low-dimensional (gener-
alized) eigenvalue decompositions, which facilitates high computational efficiency. The-
oretically, we prove that this method achieves the minimax optimal rate of convergence
under suitable assumptions. Furthermore, our algorithm involves a delicate reweighting
scheme, which can significantly enhance the identifiability of the active set of covariates.
Extensive numerical studies demonstrate high superiority of the proposed algorithm in
comparison to competing methods.
Keywords: Sufficient dimension reduction; Sparse sliced inverse regression; Random pro-
jection; High-dimensional statistics; Minimax
‚àóSchool of Statistics, Southwestern University of Finance and Economics, Chengdu 611130, China.
‚Ä†Co-first author. College of Engineering, University of California, Davis, Davis, CA 95616, USA
‚Ä°Corresponding author. Department of Statistics and Data Science, Southern University of Science and
Technology, Shenzhen 518055, China; Email: chenx8@sustech.edu.cn .
1arXiv:2305.05141v2  [stat.ME]  3 Aug 20231 Introduction
Faced with a large number of covariates in various modern applications, Sufficient Dimen-
sion Reduction (SDR) provides a statistical framework to reduce the dimension of the prob-
lem without loss of information through seeking low-dimensional linear combinations of the
original predictors. In a regression problem involving a univariate response Y‚ààRand a
p-dimensional predictor vector X= (X1, . . . , X p)‚ä§‚ààRp, SDR aims to find a dimension
reduction subspace of Rpwith a basis Bsuch that
Y‚ä• ‚ä•X|B‚ä§X, (1.1)
where ‚ä• ‚ä•stands for statistical independence. Dimension reduction subspaces are generally
not unique, so the primary interest of SDR lies in the intersection of all the dimension re-
duction subspaces, which enjoys minimum dimensionality and is itself a dimension reduction
subspace under mild conditions (Cook, 1996). We call the intersection the central subspace
and denote it as SY|X. Let Œ≤‚ààRp√ódbe a basis of SY|X, and then Œ≤‚ä§Xcaptures the full
regression relationship of YonX. Since the dimension of the central subspace dis usually
much smaller than p, then we can use the low-dimensional Œ≤TXto predict Ywithout loss
of any information.
Many SDR methods have been proposed to estimate the central subspace SY|X1, among
which the pioneering Sliced Inverse Regression (SIR) (Li, 1991) enjoys high popularity due
to its simplicity, generality and computational efficiency. Like most SDR methods, SIR has
been proved to be successful in traditional settings where the dimension of the predictors p
is fixed or diverges slowly with the sample size n(Li, 1991, Hsing and Carroll, 1992, Zhu
and Ng, 1995, Zhu et al., 2006). However, when p‚âçnorp‚â´n, which is quite common
in modern datasets, SIR breaks down in both theoretical and computational aspects (Hung
and Huang, 2019, Lin et al., 2018, Lin et al., 2019). Indeed, Lin et al. (2018) showed that
the SIR estimator of the central subspace is consistent if and only if p/ngoes to zero under
mild conditions.
To remedy this situation, and to further facilitate interpretability and model parsimony,
1See Yin (2011) and Ma and Zhu (2013) for through reviews of SDR methods.
2a reasonable sparsity condition is imposed to restrict the number sof the active predictors
in the regression. Lin et al. (2018) proved that the optimal rate for estimating the central
subspace should be (slogp)/nup to some constants in a setting where Cov(X) =Ip, and
proposed a diagonal thresholding algorithm for the single index model which achieves the
optimal rate when Cov(X) =Ip. In a follow-up work, Lin et al. (2019) introduced an
efficient Lasso variant of SIR for multiple index models with a general covariance matrix of
X. However, this method was shown to be rate optimal when pis of order o(n2c2), where c
is the generalised signal-to-noise ratio. To fulfil the theoretical gap as well as the restriction
on the covariance matrix, Tan et al. (2020) proposed an adaptive estimation scheme for
sparse SIR, which was computationally tractable and rate optimal even in the case where
log(p) =o(n).
In this paper, we propose a novel efficient Sparse SIR method via Random Projection
(SSIRvRP) for large psmall nproblems. This approach can attain the minmax optimal rate
when log(p) =o(n)under mild assumptions and enjoys high computational efficiency. Com-
pared with existing algorithms, our SSIRvRP algorithm exhibits marked simplicity, robust-
nesstopoorinitialization, andscalabilitythrougheasyparallelization. Indeed, SSIRvRPcan
be implemented through the parallel execution of low-dimensional (generalized) eigenvalue
decompositions, thereby avoiding directly computing the inverse of the sample covariance
matrix or the eigenvectors of the sample conditional covariance matrix and selecting multi-
ple tuning parameters appeared in existing sparse SIR methods. Additionally, it does not
even require computing the (conditional) sample covariance matrix itself, since it suffices
to extract its principal submatrices, which can be computed from the low-dimensional pro-
jected data. Finally, the algorithm can be readily extended to other SDR methods via the
generalized eigenvalue formulation (Li, 2007, Hung and Huang, 2019). Extensive numerical
experiments demonstrate the superior performance of SSIRvRP in comparison to competing
approaches.
When Cov(X) =Ip, the proposed algorithm would reduce to the sparse PCA algorithm
proposed by Gataric et al. (2020). However, our algorithm is absolutely not a simple and
straightforward extension from eigenvalue decomposition to generalized eigenvalue decompo-
sition, which can be reflected from the following three aspects. Firstly and most importantly,
3the theoretical evidences behind the proposed algorithm are drastically different from that
of Gataric et al. (2020). Our work fulfils the theoretical gap between the standard and
generalized eigenvalue decompositions, at least in terms of random projection based algo-
rithms, mainly via wisely utilizing the Cholesky transformation. Secondly, as a generalized
eigenvalue problem, the sparse SIR has some unique features, especially in calculation. See
Section 2.2 for details. Lastly, we embed a reweighting scheme into the proposed algorithm,
which significantly improves the ability to identify the active covariates.
Related literature . Random projection techniques have played a role in designing
dimension reduction methods. To name a few, Qi and Hughes (2012) and Anaraki and
Hughes (2014) proposed computing the leading eigenvector of the sample covariance matrix
through an ensemble of low-dimensional random projections of the data. Gataric et al.
(2020) considered sparse principal component analysis via axis-aligned random projections
in lager psmall nsettings, from where we got inspiration for our approach. Hung and
Huang(2019)introducedaso-calledintegratedrandom-partitionSDRmethodbyintegrating
multiple sketches of the central subspace obtained from random partitions of the covariates.
Recently, Liu et al. (2023) proposed a random projection approach to hypothesis tests in
high-dimensional single index models.
We note that Tan et al. (2018) suggested tackling the sparse generalized eigenvalue prob-
lem via a truncated Rayleigh flow method. However, this method can only be employed
to estimate the leading generalized eigenvector, which limits its application as an SDR ap-
proach. For more discussion on sparse SDR methods, please refer to Li et al. (2020) for a
through review.
Notation . We introduce some notation used throughout the paper. For an integer
n > 0, let [n] :={1,2, . . . , n }andEn(X) := n‚àí1Pn
i=1Xifor a random vector Xwith a
sample {Xi}n
i=1. For a vector Œ±‚ààRp, denote its j-th component by Œ±(j), and for S‚äÇ[p], let
Œ±(S)denote a subvector of Œ±with components indexed in S. Write Œ±‚Äôs Euclidean norm by
‚à•Œ±‚à•2. For a matrix U‚ààRp√ód, letU(i,j)denote its (i, j)-th entry, U(i,¬∑)denote its i-th row,
andU(¬∑,j)itsj-th column. For S‚äÜ[p]andS‚Ä≤‚äÜ[d], write U(S,S‚Ä≤)be the |S|√ó|S‚Ä≤|submatrix
with row indexes in Sand column indexes in S‚Ä≤, and simplify U([p],S‚Ä≤)andU(S,[d])byU(¬∑,S‚Ä≤)
andU(S,¬∑), respectively. Let ‚à•U‚à•Fand‚à•U‚à•opdenote its Frobenius norm and operator norm,
4respectively.
For any index set J‚äÜ[p],PJsignifies the projection matrix which is a p√ópdiagonal
matrix with the j-th diagonal entry being 1{j‚ààJ}. For a real symmetric matrix pair (A,B)
inRp√óp√óRp√óp, let Œª1(A,B)‚â•Œª2(A,B)‚â•. . .‚â•Œªp(A,B)be generalized eigenvalues in
decreasing order, and v1(A,B), . . . ,vp(A,B)be the corresponding eigenvectors such that
Avi(A,B) =Œªi(A,B)Bvi(A,B)
for any i‚àà[p].
Organization of the paper . The rest of the paper is organized as the follows. In
Section 2, we propose a sparse SIR estimator via random projection, whose theoretical
properties are investigated in Section 3. A reweighting scheme is added to improve the
efficiency of the proposed algorithm in Section 4, and Section 5 discusses the selection of
the hyperparameters for the improved algorithm. Numerical experiments are conducted in
Section 6, and in Section 7 we applied the proposed method to analyse a gene expression
data. Section 8 concludes the paper. All the technical proofs are deferred to the Appendix.
2 Method
2.1 Sparse SIR revisited
Define the discretized version of Yas
ÀúY=HX
h=1h¬∑I{Y‚ààJh},
where {J1, J2, . . . , J H}is a measurable partition of the sample space of Y. IfH‚â•d+ 1,
we know that ÀúYcan be used to identify SY|Xinstead of Ywith no loss of information
(Bura and Cook, 2001, Cook and Forzani, 2009). Then the SIR procedure is actually a
generalized eigenvalue decomposition problem of the kernel matrix Œ£E(X|Y)= Cov {E(X|ÀúY)}
5with respect to Œ£= Cov( X),i.e.,
Œ£E(X|Y)Œ≤i=ŒªiŒ£Œ≤iwithŒ≤‚ä§
iŒ£Œ≤j=1{i=j}, (2.1)
where i, j= 1,¬∑¬∑¬∑, p, and Œª1‚â• ¬∑¬∑¬∑ ‚â• Œªd>0 =Œªd+1=¬∑¬∑¬∑=Œªp. The first dgeneralized
eigenvectors {Œ≤1, . . . ,Œ≤d}corresponding to the nonzero generalized eigenvalues Œª1‚â• ¬∑¬∑¬∑ ‚â•
Œªdform a basis of SY|X. Thus, solving the following optimization problem yields a basis
Œ≤= (Œ≤1, . . . ,Œ≤d)ofSY|X:
Œ≤= argmax
B‚ààRp√ódtr
B‚ä§Œ£E(X|Y)B	
s.t.B‚ä§Œ£B=Id. (2.2)
To interpret the extracted components well, it is often encouraged to perform variable
selection for SIR. The goal of variable selection is to seek the smallest subset of the predictors
X(A), with partition X={(X(A))‚ä§,(X(Ac))‚ä§}‚ä§, such that
Y‚ä• ‚ä•X|X(A), (2.3)
where A ‚äÜ [p]denotes the truly relevant predictor set and Acdenotes the irrelevant predictor
set (Li et al., 2005, Bondell and Li, 2009). Following the partition of X, one can partition
Œ≤accordingly as
Œ≤=Ô£´
Ô£≠Œ≤(A,¬∑)
Œ≤(Ac,¬∑)Ô£∂
Ô£∏,Œ≤(A,¬∑)‚ààR|A|√ód,Œ≤(Ac,¬∑)‚ààR(p‚àí|A| )√ód,
where |A|is the cardinality of A. Then (2.3)implies that Œ≤(Ac,¬∑)=0(Bondell and Li, 2009).
Letting supp (Œ≤) ={j‚àà[p] :Œ≤(j,¬∑)Ã∏=0‚ä§}be the row support of Œ≤, then supp (Œ≤) =Ain
the above partition. Assuming |A| ‚â§ s, sparse SIR is further defined based on (2.2)through
seeking Œ≤such that
Œ≤= argmax
B‚ààRp√ódtr
B‚ä§Œ£E(X|Y)B	
,
s.t.B‚ä§Œ£B=Idand|supp(B)| ‚â§s .(2.4)
6A natural sparse SIR estimator can be obtained by solving
ÀáŒ≤= argmax
B‚ààRp√ódtrn
B‚ä§bŒ£E(X|Y)Bo
,
s.t.B‚ä§bŒ£B=Idand|supp(B)| ‚â§s ,(2.5)
where bŒ£E(X|Y)andbŒ£are the sample covariance matrices of the conditional expectation
E(X|ÀúY)andX, respectively. Tan et al. (2020) proved that this natural estimator is rate
optimal under various commonly used loss functions. However, solving the problem (2.5)
directly is computationally infeasible as it would require exhaustive search over all B‚ààRp√ód
subjecttothesparsityconstraint. Toremedythisproblem, Tanetal.(2020)furtherproposed
a refined three-steps estimator based on the work of Gao et al. (2017). In the following, we
draw inspiration from Gataric et al. (2020) and develop another computationally feasible
and much simpler estimator based on random projections that achieves optimal statistical
rate.
2.2 A sparse SIR estimator via random projection
Fork‚àà[p], letPk:={PS:S‚äÜ[p],|S|=k}be the set of k-dimensional projections. Our
method is described as follows. For two fixed integers A, B‚ààN, we independently and
uniformly generate A√óBprojections {Pa,b:a‚àà[A], b‚àà[B]}fromPk. We can treat these
projections as Agroups, each with cardinality B. For each a‚àà[A], let
b‚àó(a) :=sargmax
b‚àà[B]dX
i=1Œªi(Pa,bbŒ£E(X|Y)Pa,b, Pa,bbŒ£Pa,b)
denote the index of the selected projection within the a-th group, where sargmax denotes
the smallest element in the lexicographic ordering for those argmax values.
Consider the oracle case where Pa,b‚àó(a)=PAwithA= supp( Œ≤). Then (2.4) reduces to
seeking Œ≤such that
Œ≤= argmax
B‚ààRp√ódtr
B‚ä§PAŒ£E(X|Y)PAB	
,
s.t.B‚ä§PAŒ£PAB=Id,(2.6)
7which implies that Œ≤can be formed by the leading generalized eigenvectors of the pair
(PAŒ£E(X|Y)PA, PAŒ£PA). Hence, the basic idea is to construct a set ÀÜSsuch that A ‚äÜ ÀÜSwith
high probability. This explains why b‚àó(a)is defined as stated above, to a certain extent.
Next,inordertoaggregatetheinformationofall Aestimators,wecomputeanimportance
score ÀÜw(j)for the j-th variable, which is defined as
ÀÜw(j):=1
AAX
a=1dX
i=1(ÀÜŒªa,b‚àó(a);i‚àíÀÜŒªa,b‚àó(a);d+1)(ÀÜv(j)
a,b‚àó(a);i)2,
where ÀÜŒªa,b‚àó(a);iis the i-th generalized eigenvalue of the matrix pair (Pa,b‚àó(a)bŒ£E(X|Y)Pa,b‚àó(a),
Pa,b‚àó(a)bŒ£Pa,b‚àó(a)),and ÀÜv(j)
a,b‚àó(a);iisthe j-thelementofthecorrespondinggeneralizedeigenvector.
This means that we take account, not just of the frequency with which each co-ordinate is
chosen, but also their corresponding magnitudes in the selected eigenvector, as well as an
estimate of the signal strength. The estimation procedure is summarized as the following
algorithm and we name it as SSIRvRP (Sparse SIR via Random Projections).
Algorithm 1: pseudocode of the SSIRvRP algorithm for central subspace estima-
tion
Input: bŒ£E(X|Y),bŒ£, A, B ‚ààN, k, l‚àà[p], d‚àà[k]
1Generate {Pa,b:a‚àà[A], b‚àà[B]}independently and uniformly from Pk
2fora= 1,¬∑¬∑¬∑, Ado
3 forb= 1,¬∑¬∑¬∑, Bdo
4 fori‚àà[d+ 1], compute ÀÜŒªa,b;i:=Œªi(Pa,bbŒ£E(X|Y)Pa,b, Pa,bbŒ£Pa,b)and the
corresponding generalized eigenvector ÀÜva,b;i=vi(Pa,bbŒ£E(X|Y)Pa,b, Pa,bbŒ£Pa,b)
with ÀÜŒªa,b;k+1= 0
5 end
6Compute b‚àó(a) :=sargmaxb‚àà[B]Pd
i=1ÀÜŒªa,b;i
7end
8Compute bw= ( ÀÜw(1),¬∑¬∑¬∑,ÀÜw(p))‚ä§with
ÀÜw(j):=1
AAX
a=1dX
i=1(ÀÜŒªa,b‚àó(a);i‚àíÀÜŒªa,b‚àó(a);d+1)(ÀÜv(j)
a,b‚àó(a);i)2, j‚àà[p]
9LetÀÜS‚äÜ[p]be the index set of the llargest components of bw
Output: bŒ≤= (ÀÜv1, . . . , ÀÜvd), where ÀÜv1, . . . , ÀÜvdare the top dgeneralized eigenvectors of
(PÀÜSbŒ£E(X|Y)PÀÜS, PÀÜSbŒ£PÀÜS)
8In Algorithm 1, bŒ£=En[{X‚àíEn(X)}{X‚àíEn(X)}‚ä§]and
bŒ£E(X|Y)=HX
h=1ÀÜph{Eh(X|ÀúY=h)‚àíEn(X)}{Eh(X|ÀúY=h)‚àíEn(X)}‚ä§,(2.7)
where ÀÜph=En[1{ÀúY=h}]andEh(X|ÀúY=h) = ÀÜp‚àí1
hn‚àí1Pn
i=1Xi1{ÀúYi=h}. The positive
integers A,B,k,landdare hyperparameters of the proposed algorithm, whose choices will
be analysed in the following theoretical and numerical studies.
Remark 1.Another method to estimate bŒ£E(X|Y)is to utilize the identity Cov{E(X|Y)}=
Cov(X)‚àíE{Cov(X|Y)}. Then, we can estimate bŒ£E(X|Y)=bŒ£‚àíbT, where
bT=1
HHX
h=11
nhX
i‚ààSh(Xi‚àí¬ØXSh)(Xi‚àí¬ØXSh)‚ä§
, (2.8)
where S1, . . . , S Hcontains the sample indexes associated with the partitioned Yaccording
to its scale, nhdenotes the sample size of the slice Sh, and ¬ØXShdenotes the sample mean of
this slice. This estimator works as well as the one given above in our numerical experiments.
Notice that if Œ£=Ip, Algorithm 1 reduces to the algorithm proposed by Gataric et al.
(2020) for sparse principal component analysis. However, our algorithm is absolutely not
a simple and straightforward extension from eigenvalue decomposition to generalized eigen-
value decomposition, which can be reflected from the following aspects. Firstly and most
importantly, the theoretical evidences behind the proposed algorithm are drastically different
from that of Gataric et al. (2020). It is well known that the generalized eigenvalue problem
is, in principle, more difficult than the standard one, in both theoretical and computational
sides. Several frequently used techniques in standard eigenvalue decompositions, like spec-
tral decomposition, have no counterparts in generalized eigenvalue decomposition problems,
which brings great challenges to the theoretical analysis of Algorithm 1. Our work fulfils
the gap between the standard and generalized eigenvalue decompositions, at least in terms
of random projection based algorithms. See Sections 3.2 and 9.2 for theoretical details.
Secondly, as a generalized eigenvalue problem, the sparse SIR has some unique features,
especially in calculation. See the following discussion.
Step 4 of Algorithm 1 involves solving a generalized eigenvalue problem with a non-
9negative definite matrix pair (Pa,bbŒ£E(X|Y)Pa,b, Pa,bbŒ£Pa,b), which is different from common
generalized eigenvalue problems where the second matrix in the pair is positive definite (Li,
2007, Tan et al., 2018, Hung and Huang, 2019). The nonnegativeness of the second matrix
Pa,bbŒ£Pa,bwould lead to multiple solutions of generalized eigenvectors. To see this clearly,
letSdenote the set of the krow indexes corresponding to the nonzero diagonal elements of
Pa,b. Then without loss of generality, the matrix pair turns out to be (PSbŒ£E(X|Y)PS, PSbŒ£PS)
with
PSbŒ£E(X|Y)PS=Ô£´
Ô£≠bŒ£(S,S)
E(X|Y)0
0 0Ô£∂
Ô£∏, PSbŒ£PS=Ô£´
Ô£≠bŒ£(S,S)0
0 0Ô£∂
Ô£∏,
and the generalized eigenvalue problem in Step 4 with respect to this pair reduces to
bŒ£(S,S)
E(X|Y)ÀÜv(S)
i=ÀÜŒªibŒ£(S,S)ÀÜv(S)
iwith ÀÜv(S),‚ä§
ibŒ£(S,S)ÀÜv(S)
j=1{i=j} (2.9)
fori, j= 1, . . . , k, where ÀÜŒªi=Œªi(PSbŒ£E(X|Y)PS, PSbŒ£PS)and ÀÜvi= (ÀÜv(S),‚ä§
i,ÀÜv(Sc),‚ä§
i )‚ä§=
vi(PSbŒ£E(X|Y)PS, PSbŒ£PS). Since (2.9) does not impose any restriction for ÀÜv(Sc)
i, then the
leading kgeneralized eigenvectors of the original problem have infinitely many solutions. To
remedy this situation, we choose ÀÜvi= (ÀÜv(S),‚ä§
i,0‚ä§)‚ä§fori‚àà[k]for the generalized eigenvalue
problem in Step 4. This point is quite different from the eigenvalue decomposition of PSbŒ£PS,
which would naturally yield sparse eigenvectors as shown in Gataric et al. (2020).
It remains to solve the reduced low-dimensional generalized eigenvalue problem (2.9),
which can be solve quickly by traditional algorithms. Notice that if the submatrix bŒ£(S,S)
is invertible, then (2.9) would further reduce to a low-dimensional eigenvalue decompo-
sition problem. Indeed, bŒ£(S,S)is invertible with high probability for a properly chosen
k. Recall that bŒ£(S,S)=En[{X(S)‚àíEn(X(S))}{X(S)‚àíEn(X(S))}‚ä§]. Then it is invert-
ible with probability approaching to 1provided that k‚â™nandŒ£has full rank, which
contributes to the computational efficiency of the proposed algorithm. Consequently, the
computationally intractable sparse SIR problem (2.5) can be efficiently solved by conduct-
ing low-dimensional eigenvalue decompositions through our proposed SSIRvRP algorithm.
Moreover, since {Pa,b:a‚àà[A], b‚àà[b]}are generated randomly from Pk, the A√óBlow-
10dimensional eigenvalue decompositions can be executed in parallel, which further facilitates
faster computation. Finally, the matrix pair (bŒ£(S,S)
E(X|Y),bŒ£(S,S))can either be extracted from
the pair (bŒ£E(X|Y),bŒ£)or be estimated from the projected covariates X(S). The latter option
would be preferable when pis sufficiently large.
It is worthy noting that, different from the convex relaxation algorithms for sparse SIR
or SDR (Tan et al., 2018, Lin et al., 2019, Tan et al., 2020), the proposed algorithm is not
iterative and thus robust to poor initialization.
Another notable advantage of our method lies in its scalability to other SDR methods.
It is well known that the class of inverse regression based SDR methods, including the
sliced average variance estimation (Cook and Weisberg, 1991), principal Hessian directions
(Li, 1992, Cook, 1996), directional regression (Li and Wang, 2007), among others, can be
formulated as a generalized eigenvalue problem. Hence, the proposed SSIRvRP algorithm, as
a solution to sparse generalized eigenvalue problems, can be readily applied to these methods
to obtain a sparse estimator of the central subspace.
3 Theoretical analysis
3.1 Upper bound for isotropic covariance
We consider the setting where (Xi,ÀúYi)n
i=1are i.i.d. such that Xi|(ÀúYi=h)‚àº N p(¬µh,Œ£h)
forh‚àà[H], which is also assumed in Cook and Yin (2001), Cook (2007), Cook and
Forzani (2009) and Tan et al. (2020). To simplify the theoretical analysis, we firstly as-
sume Cov (X) =Ip. Then, the generalized eigenvalue problem (2.1) reduces to the following
eigenvalue problem:
Œ£E(X|Y)Œ≤i=ŒªiŒ≤i,withŒ≤‚ä§
iŒ≤j=1{i=j},
where Œ£E(X|Y)is the covariance matrix of the conditional expectation E(X|ÀúY),i, j=
1,¬∑¬∑¬∑, p, and Œª1‚â• ¬∑¬∑¬∑ ‚â• Œªd>0 = Œªd+1=¬∑¬∑¬∑=Œªp. Let Œõ= diag( Œª1, . . . , Œª d)and
recall that Œ≤= (Œ≤1, . . . ,Œ≤d)collects the first deigenvectors corresponding to the nonzero
eigenvalues Œª1‚â• ¬∑¬∑¬∑ ‚â• Œªd. Therefore, we can decompose Œ£E(X|Y)=Œ≤ŒõŒ≤‚ä§.
The following technical conditions are needed.
11(A1) Œ∫Œª‚â•Œª1‚â• ¬∑¬∑¬∑ ‚â• Œªd‚â•Œª >0for some constant Œ∫ >1.
(A2) Œ≤‚ààŒòp,d,s(¬µ), where
Œòp,d,s(¬µ) :=
V‚ààŒòp,d,supp(V)‚â§s,maxj:‚à•V(j,¬∑)‚à•2Ã∏=0‚à•V(j,¬∑)‚à•2
minj:‚à•V(j,¬∑)‚à•2Ã∏=0‚à•V(j,¬∑)‚à•2‚â§¬µ
,
andŒòp,ddenotes the set of real p√ódmatrices with orthonormal columns.
Assumption (A1) can be seen as a coverage condition (Cook, 2004, Yu et al., 2016),
and similar assumptions can be found in the literature (Cai et al., 2013, Gao et al., 2015,
Tan et al., 2020). Assumption (A2) is an incoherence condition by which we require the
parameter of interest Œ≤do not have too many non-zero rows, and the non-zero rows should
have comparable Euclidean norms. For any Œ≤‚ààŒòp,d,s(¬µ), sinceP
j‚ààA‚à•Œ≤(j,¬∑)‚à•2
2=‚à•Œ≤‚à•2
F=d,
Assumption (A2) implies
d
s¬µ2‚â§ ‚à•Œ≤(j,¬∑)‚à•2
2‚â§d¬µ2
s,‚àÄj‚àà A. (3.1)
ForU,V‚ààŒòp,d, following Gataric et al. (2020), we use the loss function
L(U,V) :=‚à•sin{D(U,V)}‚à•F (3.2)
to evaluate the distance between UandV, where the sine function acts elementwise,
D(U,V)is the d√óddiagonal matrix whose j-th diagonal entry is the j-th principal angel
between UandV,i.e.,cos‚àí1(œÉj), where œÉjis the j-th singular value of U‚ä§V.
In the following theoretical analysis, sandpare allowed to depend on the sample size n,
while Œª,¬µ,dandHare treated as fixed constants. Recall that bŒ≤is the output of Algorithm
1 with inputs bŒ£E(X|Y),bŒ£,A,B,d,kandl. Theorem 1 gives an upper bound of the proposed
SSIRvRP estimator.
Theorem 1. Suppose Assumptions (A1)-(A2) hold. If k‚â•max{d+ 1, s},l‚â•s, and
16Kr
klogp
n‚â§Œªd
s¬µ2, (3.3)
12then it holds that
P
L(bŒ≤,Œ≤)‚â§2Ks
dllog(p)
nŒª2
d
‚â•1‚àícp‚àí3‚àípexp
‚àíAŒª2
d
50p2¬µ8Œª2
1
,
where K, c > 0are some constants.
Theorem 1 is a generalization of Theorem 1 of Gataric et al. (2020), where they consider
theestimationoftheprincipalsubspaceunderarestrictedcovarianceconcentrationcondition
that was introduced in Wang et al. (2016a). We extend their results to the problem of central
subspace estimation, and the results of Lemma 1 in the proof of Theorem 1 mimic the
restricted covariance concentration condition. Tan et al. (2020) also considered the problem
of the central subspace estimation and they proposed an adaptive estimation scheme for
sparse SIR, which achieves an upper boundp
slog(p)/nunder the loss function defined in
(3.2). If ‚Ñì‚âçs, our SSIRvRP estimator also achieves the same bound up to some constants.
3.2 Upper bound for general covariance
We go further to the real generalized eigenvalue problem for sparse SIR specified in (2.1),
where the covariance Œ£of the covariates Xis not restricted to special structures. It is
well known that the generalized eigenvalue problem is, in principle, more difficult than the
standardone, especiallyintermsoftheoreticalanalysis. Thus, severalhigh-levelassumptions
would be imposed to highlight the technical difference between the generalized eigenvalue
problem tackled in this section and the standard one for the simplified setting where Œ£=Ip
addressed in the above section. The data (Xi,ÀúYi)n
i=1are also assumed to be i.i.d.
To be consistent with the logic behind Algorithm 1 and to utilize standard properties
of eigenvalue decomposition, for the sake of theoretical analysis, we hope to find a proper
transformation to reduce the generalized eigenvalue decomposition (2.1) to a standard one,
while retaining the row sparsity structure of the basis Œ≤. It turns out that the Cholesky
decomposition of Œ£plays a crucial role. For a positive-definite covariance matrix Œ£, using
the Cholesky decomposition, we can decompose Œ£as
Œ£=LL‚ä§,
13where Lis a real lower triangular matrix with positive diagonal entries. Let Œ≤i= (L‚ä§)‚àí1Œ≥i
fori‚àà[p]. Then, (2.1) implies that
{L‚àí1Œ£E(X|Y)(L‚àí1)‚ä§}Œ≥i=ŒªiŒ≥iwithŒ≥‚ä§
iŒ≥j=1{i=j}, (3.4)
where i, j‚àà[p], and Œª1‚â• ¬∑¬∑¬∑ ‚â• Œªd>0 =Œªd+1=¬∑¬∑¬∑=Œªp. Denote M=L‚àí1Œ£E(X|Y)(L‚àí1)‚ä§
andŒ≥= (Œ≥1, . . . ,Œ≥d), and then Mis a real symmetric matrix to which the spectral decom-
position can be readily applied, i.e.,M=Œ≥ŒõŒ≥‚ä§withŒõ= diag( Œª1, . . . , Œª d). In addition,
the eigenvalues of Mare just the generalized eigenvalues of the matrix pair (Œ£E(X|Y),Œ£),
and, more importantly, the leading eigenvectors Œ≥= (Œ≥1, . . . ,Œ≥d)ofMprecisely retain the
row sparsity structure of the generalized eigenvectors Œ≤= (Œ≤1, . . . ,Œ≤d)of(Œ£E(X|Y),Œ£). To
see this clearly, recall that Œ≤‚ä§= (Œ≤(A,¬∑),‚ä§,0‚ä§)andŒ≤i= (L‚ä§)‚àí1Œ≥iwhere Arepresents the
set of the active covariates and Lis a lower triangular matrix. Then, it holds that2
Œ≥=L‚ä§Œ≤=Ô£´
Ô£≠L(A,A),‚ä§L(Ac,A),‚ä§
0‚ä§L(Ac,Ac),‚ä§Ô£∂
Ô£∏Ô£´
Ô£≠Œ≤(A,¬∑)
0Ô£∂
Ô£∏=Ô£´
Ô£≠L(A,A),‚ä§Œ≤(A,¬∑)
0Ô£∂
Ô£∏,
where
L=Ô£´
Ô£≠L(A,A)0
L(Ac,A)L(Ac,Ac)Ô£∂
Ô£∏.
The transformation from the generalized eigenvalue problem (2.1) to the standard one (3.4)
with the row sparsity structure of the leading eigenvectors unchanged makes tractable the
theoretical analysis of Algorithm 1 under general covariance.
The following assumptions are required.
(A1‚Äô) ŒπŒ∏‚â•Œ∏1‚â• ¬∑¬∑¬∑ ‚â• Œ∏p‚â•Œ∏ >0for some constant Œπ >1, where Œ∏iis the eigenvalue of Œ£.
(A2‚Äô) Œ≥‚ààŒòp,d,s(¬µ), where Œòp,d,s(¬µ)is defined in Assumption (A2).
2Cholesky decomposition is dependent on the order in which the variables appear in the random vector
X, and it works when the variables have a natural ordering.
14(A3)Define Sk:={S‚äÇ[p] :|S|=k}for any k‚àà[p]. Then,
P
sup
S‚ààSkmax{‚à•bL(S,S)‚àíL(S,S)‚à•op,‚à•cM(S,S)‚àíM(S,S)‚à•op} ‚â§Kr
klogp
n
‚â•1‚àíc1p‚àíc2,
where bŒ£=bLbL‚ä§is the Cholesky decomposition of bŒ£,cM=bL‚àí1bŒ£E(X|Y)(bL‚àí1)‚ä§and
K, c 1, c2>0are constants.
(A4)For any j‚àà[p]and any ordering of X, there exists some œÑ‚àà(0,1]such that œÑ‚â§
(L(j,j))‚àí2‚â§œÑ‚àí1.
Assumption (A1‚Äô) is a regularity condition for Œ£, whose eigenvalues are required to be
bounded away from zero and infinity. Assumption (A2‚Äô) is an incoherence condition for the
transformed basis Œ≥by which we require Œ≥do not have too many non-zero rows, and the
non-zero rows should have comparable Euclidean norms. Recall that Œ≥=L‚ä§Œ≤andLis a
lower triangular matrix. Hence, Assumption (A2‚Äô) is equivalent to requiring that Œ≤do not
have too many non-zero rows and that the rows of L(A,A),‚ä§Œ≤(A,¬∑)have comparable Euclidean
norms. For any Œ≥‚ààŒòp,d,s(¬µ), sinceP
j‚ààA‚à•Œ≥(j,¬∑)‚à•2
2=‚à•Œ≥‚à•2
F=d, Assumption (A2‚Äô) implies
d
s¬µ2‚â§ ‚à•Œ≥(j,¬∑)‚à•2
2‚â§d¬µ2
s,‚àÄj‚àà A. (3.5)
Assumption (A3) is indeed a high-level condition imposed on the sample estimates of Œ£
andŒ£E(X|Y). When Œ£is a diagonal matrix, Assumption (A3) holds under mild conditions
similar to those in Tan et al. (2020); see their Lemma 1 for details. Assumption (A4) is a
technical condition imposed to guarantee that the active variables enjoy higher weights than
the inactive ones in the population level. Since all the diagonal elements of Lare positive,
Assumption (A4) seems quite mild. Moreover, this assumption is implied by Assumption
(A1‚Äô) provided that Œ£is a diagonal matrix. Especially, for the simplified case where Œ£=Ip,
the above assumptions naturally hold.
ForU,V‚ààŒòp,d(Œ£) :={V‚ààRp√ód:V‚ä§Œ£V=Id}, we use the popular general loss
function
L(U,V) :=‚à•UU‚ä§‚àíV V‚ä§‚à•F (3.6)
15to evaluate the distance between linear subspaces; see Cai et al. (2013), Gao et al. (2015)
and Tan et al. (2020). We note that, when Œ£=Ip, this loss function reduces to the one
defined in (3.2) up to a constant‚àö
2.
In the following, sandpare allowed to depend on the sample size n, while Œ∏,Œª,¬µ,œÑ,
dandHare treated as fixed constants. Theorem 2 gives an upper bound of the proposed
SSIRvRP estimator for general covariance.
Theorem 2. Suppose Assumptions (A1)-(A4) hold. If k‚â•max{d+1, s},l‚â•s,Kp
klog(p)/n‚â§
min{Œª1/(4d),‚àöŒ∏1, Œ∏p/(6‚àöŒ∏1)},Kp
llog(p)/n‚â§min{Œªd/(2‚àö
2),‚àöŒ∏1, Œ∏p/(6‚àöŒ∏1)}and
Cr
klogp
n‚â§œÑŒªd
s¬µ2(3.7)
for some sufficiently large constant C >0, then it holds that
P
L(bŒ≤,Œ≤)‚â§C‚Ä≤r
llog(p)
n
‚â•1‚àíc‚Ä≤p‚àíc2‚àípexp
‚àíAœÑ4Œª2
d
50p2¬µ8Œª2
1
,
where C‚Ä≤, c‚Ä≤>0are some constants.
Theorem 2 generalizes Theorem 1 to the general covariance case, showing an upper bound
p
slog(p)/nforl‚âçs, which echoes latest research results for central subspace estimation
(Lin et al., 2019, Tan et al., 2020). Although seemingly similar to Theorem 1, the proof
of Theorem 2 is quite different from that of Theorem 1, which wisely utilizes the Cholesky
decomposition and several nice properties of triangular matrices; see Section 9.2 for details.
3.3 Lower bound
Theorem 1 and Theorem 2, together with the following Theorem 3, indicate that the
SSIRvRP estimator is minmax optimal up to some logarithmic factor, over all possible
sparse SIR estimators, provided that l‚âçs. Theorem 3 establishes a minmax lower bound
among all possible sparse SIR estimators eŒ≤, which is similar to the lower bound established
in Tan et al. (2020). Different from their methods, we require the central subspace satisfy
an incoherence condition. However, we show that this kind of restriction on the parameter
16space does not make the estimation any easier from the mimnax perspective. For simplicity,
we assume (Xi,ÀúYi)n
i=1are i.i.d. such that Xi|(ÀúYi=h)‚àº N p(¬µh,Œ£h)forh‚àà[H]andŒ£=Ip.
Theorem 3. Assume that 4(s‚àí1)‚â§p‚àí1,(s‚àí1) log{(p‚àí1)/(s‚àí1)} ‚â• 6and5n‚â•
4s(s‚àí1) log{(p‚àí1)/(s‚àí1)}. Then
inf
eŒ≤sup
Œ≤‚ààŒòp,d,s(3)EPŒ≤{L(eŒ≤,Œ≤)}‚â≥r
slog(p/s)
n,
where the expectation is with respect to (Xi, Yi)‚àºi.i.d.PŒ≤.
The bound in Theorem 3 is similar to the one established for the estimation of the
principal eigenspace (Gataric et al., 2020). We generalized their conclusion to the setting
of the estimation of the central subspace. Despite similar conclusions, the technical proofs
of Theorem 3 are quite different from theirs. Moreover, the lower bound established here
actually holds beyond the normality assumption of X|ÀúYand the isotropic assumption of the
covariance matrix.
4 Improved algorithm
In this section, in order to enhance the identification ability of the active set of covariates,
we add a reweighting step to Algorithm 1 to refine the weights corresponding to the largest
values of bw. Specifically, let ÀÜS‚Ä≤be the index set of the l‚Ä≤largest components of bwproduced
in Algorithm 1. We suggest recomputing the weights of these variables in ÀÜS‚Ä≤by repeating
Steps 1-8 in Algorithm 1 with the submatrix pair (bŒ£(ÀÜS‚Ä≤,ÀÜS‚Ä≤)
E(X|Y),bŒ£(ÀÜS‚Ä≤,ÀÜS‚Ä≤)). Then we select l
indices corresponding to the largest values of bw‚Ä≤to form a set ÀÜS, and output an estimate
bŒ≤as the first dgeneralized eigenvectors of (PÀÜSbŒ£E(X|Y)PÀÜS, PÀÜSbŒ£PÀÜS). Pseudo code for this
modified SSIRvRP algorithm is given in Algorithm 2. We find that the new algorithm with
a reweighting step really works well, as shown in the numerical studies.
By similar techniques of Theorem 2, we can prove that the estimate of Algorithm 2
has the same upper boundp
llog(p)/nas that of Theorem 2. However, it seems that the
reweighting scheme in Step 9 of Algorithm 2 helps identify A, the row support of Œ≤, more
accurately in the finite sample. There is no surprise if we consider the formation of ÀÜSas a
17Algorithm 2: pseudocode of the SSIRvRP algorithm with reweighting
Input: bŒ£E(X|Y),bŒ£, A1, A2, B1, B2‚ààN, k, l, l‚Ä≤‚àà[p], d‚àà[k]
1Generate {Pa,b:a‚àà[A1], b‚àà[B1]}independently and uniformly from Pk
2fora= 1,¬∑¬∑¬∑, A1do
3 forb= 1,¬∑¬∑¬∑, B1do
4 fori‚àà[d+ 1], compute ÀÜŒªa,b;i:=Œªi(Pa,bbŒ£E(X|Y)Pa,b, Pa,bbŒ£Pa,b)and the
corresponding generalized eigenvector ÀÜva,b;i=vi(Pa,bbŒ£E(X|Y)Pa,b, Pa,bbŒ£Pa,b)
with ÀÜŒªa,b;k+1= 0
5 end
6Compute b‚àó(a) :=sargmax
b‚àà[B1]Pd
i=1ÀÜŒªa,b;i
7end
8Compute bw= ( ÀÜw(1),¬∑¬∑¬∑,ÀÜw(p))‚ä§with
ÀÜw(j):=1
A1A1X
a=1dX
i=1(ÀÜŒªa,b‚àó(a);i‚àíÀÜŒªa,b‚àó(a);d+1)(ÀÜv(j)
a,b‚àó(a);i)2, j‚àà[p]
9LetÀÜS‚Ä≤be the index set of the l‚Ä≤largest components of bw. Recompute the
l‚Ä≤-dimensional vector of weights bw‚Ä≤by repeating Steps 1‚àí8with the submatrix
pair(bŒ£(ÀÜS‚Ä≤,ÀÜS‚Ä≤)
E(X|Y),bŒ£(ÀÜS‚Ä≤,ÀÜS‚Ä≤)), the projection parameters A2, B2and the newly defined
Pk:={PS:S‚äÜ[l‚Ä≤],|S|=k}
10Denote by ÀÜSthe index set of the llargest components of bw‚Ä≤
Output: bŒ≤= (ÀÜv1, . . . , ÀÜvd), where ÀÜv1, . . . , ÀÜvdare the top dgeneralized eigenvectors of
(PÀÜSbŒ£E(X|Y)PÀÜS, PÀÜSbŒ£PÀÜS)
process of variable selection. From this angle, the design of Algorithm 2 mimics a two-round
selection, which offers an opportunity to reassess the importance of the variables and retrieve
the mistakenly deleted ones, often accompanied by weak signals, in the first round.
Remark 2.Notice that the estimate of SSIRvRP naturally satisfies the orthogonal con-
straint bŒ≤‚ä§bŒ£bŒ≤=Id. However, the refined three-steps estimator (Tan et al., 2020) is op-
timized by relaxing original constraints and thus can not meet the orthogonal constraint
without a normalization step. The Lasso-SIR (Lin et al., 2019) also does not satisfy this
constraint. This can be seen another advantage of the proposed algorithm.
185 Choice of hyperparameters
In the SSIRvRP algorithm, there are several hyperparameters to be selected before the
implementationofthealgorithm. Wefindthattheproposedalgorithmbehavesquiterobustly
to a wide ranges of combinations of (A, B),(A1, B1),kandl‚Ä≤, as shown in the following
numerical experiments. For the choice of the dimension of the central subspace d, several
existing methods can be applied to the sparse SIR setting (Chen et al., 2010, Lin et al.,
2019), and thus we treat das a known constant.
The sparsity level lis a key tuning parameter in the proposed method. To choose the
tuning parameter, we minimize the following criterion, which has been used in Chen et al.
(2010):
‚àílogn
tr(bŒ≤‚ä§
lbŒ£E(X|Y)bŒ≤l)o
+Œ¥¬∑dfl,
wherebŒ≤ldenotes the solution for Œ≤given the sparsity level l, dfldenotes the effective number
of parameters, and Œ¥= 2/nfor the AIC-type criterion and Œ¥= log( n)/nfor the BIC-type
criterion. Since the number of nonzero rows of bŒ≤lis just l, we can estimate df lby(l‚àíd)¬∑d.
Thus, we choose the best lby minimizing
‚àílogn
tr(bŒ≤‚ä§
lbŒ£E(X|Y)bŒ≤l)o
+Œ¥¬∑(l‚àíd)¬∑d .
One advantage of the tuning process of lis that it is conducted only in Step 10 of
Algorithm 2 and Steps 1-9 are computed only once. Furthermore, since lis an integer and
its parameter space is countable and finite, the tuning process enjoys higher computational
efficiency compared with those tuned on a continuous parameter space.
6 Simulation studies
Inthissection, weconductextensivenumericalexperimentstocomparetheproposedmethod
with several competitive methods, show the effect of the reweighting step in Algorithm 2,
and present some empirical instruction for choice of the hyperparameters in the proposed
algorithm.
196.1 Comparison with existing methods
We compare our method (Algorithm 2) with the Refined Three-Steps estimator (RTS, here-
after) in Tan et al. (2020) which was shown to be rate optimal when logp=o(n), the
Lasso-SIR in Lin et al. (2019) and DT-SIR in Lin et al. (2017). Lasso-SIR was proved to
be rate optimal when p=o(n2), and DT-SIR was rate optimal when the covariance matrix
ofXis identity. We describe our method as three types: the first one works with the true
sparsity level (SSIRvRP), the second one with the sparsity level tuned by the BIC crite-
rion (SSIRvRP-BIC), and the last one with the sparsity level tuned by the AIC criterion
(SSIRvRP-AIC).
To be fair, we copy the simulation settings in Tan et al. (2020) and recall the results
therein. Five models are considered, i.e.,
Model I : Y=Œ≤‚ä§X+ sin( Œ≤‚ä§X) +œµ,
Model II : Y= 2 arctan( Œ≤‚ä§X) +œµ,
Model III : Y= (Œ≤‚ä§X)3+œµ,
Model IV : Y= sinh( Œ≤‚ä§X) +œµ,
Model V : Y= exp( Œ≤‚ä§
1X)¬∑sign(Œ≤‚ä§
2X) + 0.2œµ,
where X‚àº N p(0,Œ£),œµ‚àº N(0,1), andXandœµare independent. The covariance matrix Œ£
follows the following four structures:
(1)Identity covariance: Œ£=Ip,
(2)Dense covariance: Œ£= (œÉij)1‚â§i,j‚â§pwith œÉii= 1andœÉij= 0.6foriÃ∏=j,
(3)Toplitz covariance: Œ£= (œÉij)1‚â§i,j‚â§pwith œÉij= 0.5|i‚àíj|,
(4)Sparse inverse covariance: Œ£is the correlation matrix of Œ£0, and Œ£‚àí1
0= (wij)1‚â§i,j‚â§p
with wij=1{i=j}+ 0.5√óI{|i‚àíj|=1}+ 0.4√óI{|i‚àíj|=2}.
The regression coefficients Œ≤‚ààRp√ó1and(Œ≤1,Œ≤2)‚ààRp√ó2are set to have 5nonzero rows
whose indexes are randomly chosen from the set [p]. And the entries in these nonzero rows
are random numbers drawn from the uniform distribution on the finite set {‚àí2,‚àí1,0,1,2}.
20Forthehyperparametersoftheproposedalgorithm,weset (A1, B1) = (900 ,300),(A2, B2) =
(600,200),k= 20,l‚Ä≤= 50anddas its true value. Since the true parameter does not satisfy
the constraint Œ≤‚ä§Œ£Œ≤=Id, we can not compare these sparse SIR estimates directly under
the loss function given in (3.2) or (3.6). Consequently, we use the correlation loss (Li, 1991,
Tan et al., 2020), defined as
LœÅ(bŒ≤,Œ≤) = 1‚àí1
dtrn
(bŒ≤‚ä§Œ£bŒ≤)‚àí1(bŒ≤‚ä§Œ£Œ≤)(Œ≤‚ä§Œ£Œ≤)‚àí1(Œ≤‚ä§Œ£bŒ≤)o
,
to measure the efficiency of the estimation and to compare our method with its competing
methods. Each simulation is repeated 100 times, and we summarize the average correlation
loss across all combinations of five models and four covariance structures under various
(n, p)configurations in Tables 1-4. For each model setting, the average loss of the optimal
algorithm is highlighted by a bold font, and the average loss of the sub-optimal algorithm is
highlighted by an italic font.
It is clear that the proposed method performs generally better than the other methods
under various model settings. Specifically, SSIRvRP, SSIRvRP-BIC and SSIRvRP-AIC per-
form much better than the other methods in Models I, III and IV, and the average loss is
decreased by almost an order of magnitude. However, in Models II and V, RTS sometimes
performs the best, when the sample size and dimension are both low, i.e.,(n, p) = (100 ,200)
and(100,400). With the increase of the sample size and dimension, our SSIRvRP tends to
catch up quickly from behind, and the decrease of average loss is significant. Although RTS
was proved to be optimal when log(p) =o(n), which is the same as our method, its finite
sample performance is generally inferior to ours. The reason may be that the implementa-
tion of our method does not involve any choice of initial points, while other approaches are
gradient-based and thus vulnerable to a possible bad choice of initialization.
Additionally,whencomparingSSIRvRP-AICandSSIRvRP-BIC,itwasfoundthatSSIRvRP-
BICperformedslightlybetterinModelsIIandIII.However,whenconsideringallfivemodels,
neither method was found to be superior. Overall, they demonstrated similar performance.
Therefore, either method can be chosen as a suitable working algorithm.
Finally, the average correlation loss decreases as nincreases which can be seen from the
21Table 1: The averages of correlation loss with the identity covariance structure.
(n, p)Model DT-SIR Lasso-SIR RTS SSIRvRP SSIRvRP-BIC SSIRvRP-AIC
(100,200)I 0.939 0.230 0.070 0.015 0.024 0.027
II 0.926 0.389 0.046 0.052 0.053 0.122
III 0.955 0.163 0.021 0.002 0.009 0.002
IV 0.923 0.395 0.085 0.002 0.008 0.002
V 0.769 0.404 0.066 0.090 0.120 0.112
(100,400)I 0.954 0.435 0.033 0.011 0.017 0.032
II 0.949 0.448 0.015 0.092 0.081 0.162
III 0.863 0.355 0.039 0.002 0.007 0.004
IV 0.888 0.611 0.120 0.002 0.005 0.003
V 0.683 0.527 0.091 0.138 0.150 0.170
(100,600)I 0.902 0.587 0.109 0.023 0.022 0.039
II 0.879 0.675 0.175 0.109 0.099 0.196
III 0.968 0.489 0.021 0.006 0.009 0.005
IV 0.875 0.756 0.215 0.003 0.009 0.003
V 0.701 0.561 0.179 0.140 0.142 0.179
(200,600)I 0.997 0.094 0.012 0.003 0.003 0.018
II 0.930 0.233 0.039 0.010 0.014 0.091
III 0.944 0.058 0.008 0.001 0.001 0.001
IV 0.970 0.212 0.020 0.001 0.001 0.001
V 0.851 0.242 0.046 0.032 0.039 0.100
(400,600)I 0.014 0.014 0.009 0.002 0.002 0.010
II 0.924 0.027 0.013 0.004 0.005 0.047
III 0.971 0.004 0.004 0.000 0.000 0.000
IV 0.989 0.021 0.009 0.000 0.000 0.001
V 0.874 0.045 0.019 0.008 0.009 0.051
22Table 2: The averages of correlation loss with the dense covariance structure.
(n, p)Model DT-SIR Lasso-SIR RTS SSIRvRP SSIRvRP-BIC SSIRvRP-AIC
(100,200)I 0.472 0.106 0.143 0.045 0.054 0.053
II 0.766 0.283 0.160 0.130 0.129 0.175
III 0.904 0.483 0.074 0.003 0.024 0.005
IV 0.902 0.697 0.298 0.004 0.028 0.010
V 0.439 0.338 0.129 0.173 0.199 0.184
(100,400)I 0.856 0.509 0.074 0.057 0.057 0.054
II 0.828 0.887 0.019 0.135 0.124 0.194
III 0.256 0.026 0.012 0.004 0.023 0.008
IV 0.245 0.104 0.081 0.006 0.033 0.010
V 0.432 0.410 0.139 0.207 0.208 0.218
(100,600)I 0.883 0.600 0.301 0.062 0.059 0.080
II 0.340 0.159 0.131 0.170 0.160 0.235
III 0.928 0.926 0.284 0.008 0.027 0.019
IV 0.849 0.544 0.453 0.007 0.028 0.018
V 0.612 0.425 0.245 0.195 0.197 0.215
(200,600)I 0.854 0.187 0.023 0.007 0.012 0.024
II 0.507 0.085 0.096 0.036 0.034 0.088
III 0.637 0.028 0.031 0.002 0.007 0.004
IV 0.946 0.944 0.045 0.002 0.009 0.005
V 0.548 0.307 0.086 0.078 0.073 0.101
(400,600)I 0.822 0.050 0.005 0.002 0.003 0.010
II 0.287 0.027 0.020 0.010 0.010 0.038
III 0.472 0.006 0.019 0.001 0.001 0.001
IV 0.591 0.035 0.038 0.001 0.001 0.002
V 0.363 0.126 0.027 0.024 0.026 0.054
cases (n, p) = (100 ,600),(200,600), and (400,600)), and increases as pincreases seen from
the cases (n, p) = (100 ,200),(100,400), and (100,600)). These results are fairly consistent
with our theoretical findings.
Table 5 reports similar results as Table 3, where we use equation (2.8) in Remark 1 to
calculate the sample covariance matrix of E(X|ÀúY). Both estimates works similarly to each
other, so you can choose whichever one you prefer.
6.2 The reweighting step
Compared with Algorithm 1, we add a reweighting step to Algorithm 2 to help retrieve true
signals. Figure 1 displays the effect of the reweighting step in terms of averaged correlation
23Table 3: The averages of correlation loss with the Toplitz covariance structure.
(n, p)Model DT-SIR Lasso-SIR RTS SSIRvRP SSIRvRP-BIC SSIRvRP-AIC
(100,200)I 0.945 0.186 0.073 0.013 0.019 0.022
II 0.943 0.297 0.046 0.056 0.058 0.099
III 0.951 0.088 0.022 0.002 0.007 0.002
IV 0.915 0.323 0.092 0.002 0.006 0.002
V 0.794 0.345 0.061 0.109 0.119 0.117
(100,400)I 0.952 0.283 0.033 0.013 0.024 0.034
II 0.919 0.368 0.017 0.089 0.081 0.155
III 0.917 0.254 0.042 0.003 0.005 0.002
IV 0.907 0.477 0.141 0.002 0.006 0.002
V 0.763 0.496 0.099 0.127 0.133 0.152
(100,600)I 0.927 0.474 0.105 0.022 0.026 0.036
II 0.874 0.581 0.187 0.114 0.103 0.188
III 0.935 0.340 0.021 0.005 0.014 0.006
IV 0.868 0.692 0.202 0.003 0.010 0.003
V 0.765 0.528 0.201 0.142 0.140 0.178
(200,600)I 0.932 0.070 0.010 0.004 0.004 0.018
II 0.926 0.150 0.034 0.012 0.016 0.084
III 0.831 0.030 0.008 0.001 0.001 0.001
IV 0.938 0.151 0.017 0.001 0.001 0.002
V 0.603 0.179 0.040 0.028 0.035 0.079
(400,600)I 0.372 0.014 0.008 0.002 0.002 0.009
II 0.289 0.021 0.012 0.004 0.005 0.043
III 0.176 0.004 0.003 0.000 0.000 0.000
IV 0.577 0.021 0.010 0.001 0.001 0.001
V 0.199 0.039 0.018 0.007 0.009 0.045
loss and signal rate. Here, signal rate summarizes the frequency that the algorithm identifies
all the true signals exactly. It is clear that the reweighting step significantly decreases the
correlation loss and increases the signal rate under various model settings. Therefore, we
suggest Algorithm 2 for solving sparse SIR problems.
6.3 Choice of hyperparameters
In this subsection, we conduct numerical experiments to give some instructions to the choice
of the hyperparameters in Algorithm 2. Generally speaking, the performance of the proposed
method seems quite robust to a wide range of combinations of the hyperparameters.
Figure 2 plots the relationship of the correlation loss and A,B, and combinations of
24Table 4: The averages of correlation loss with the sparse inverse covariance structure.
(n, p)Model DT-SIR Lasso-SIR RTS SSIRvRP SSIRvRP-BIC SSIRvRP-AIC
(100,200)I 0.876 0.153 0.084 0.015 0.023 0.023
II 0.906 0.265 0.060 0.100 0.086 0.127
III 0.848 0.056 0.024 0.002 0.004 0.002
IV 0.895 0.280 0.108 0.002 0.010 0.002
V 0.557 0.267 0.068 0.129 0.142 0.129
(100,400)I 0.930 0.203 0.040 0.029 0.026 0.031
II 0.943 0.238 0.016 0.108 0.088 0.159
III 0.851 0.133 0.039 0.003 0.009 0.003
IV 0.873 0.327 0.118 0.003 0.004 0.003
V 0.685 0.550 0.087 0.151 0.154 0.170
(100,600)I 0.930 0.388 0.147 0.036 0.032 0.046
II 0.930 0.431 0.183 0.119 0.105 0.172
III 0.926 0.192 0.023 0.003 0.008 0.002
IV 0.893 0.576 0.241 0.002 0.006 0.006
V 0.778 0.433 0.159 0.191 0.184 0.201
(200,600)I 0.452 0.033 0.011 0.005 0.004 0.017
II 0.704 0.082 0.038 0.012 0.016 0.069
III 0.255 0.010 0.007 0.001 0.001 0.001
IV 0.730 0.086 0.021 0.001 0.001 0.001
V 0.319 0.146 0.048 0.031 0.038 0.073
(400,600)I 0.328 0.016 0.012 0.002 0.002 0.006
II 0.228 0.023 0.012 0.003 0.004 0.034
III 0.334 0.007 0.011 0.001 0.001 0.001
IV 0.248 0.017 0.008 0.001 0.001 0.001
V 0.257 0.051 0.018 0.008 0.010 0.045
(A, B)with the other parameters staying unchanged. When Bis fixed, increasing Awould
not significantly decrease the correlation loss, and similar trend occurs when Ais fixed.
When both AandBare increased simultaneously while keeping B=A/3, the figure shows
an obvious downward trend for small As. As Agets large to 400, the decreasing trend of
correlation loss becomes flat. Moreover, the loss seems robust to all choices of A,Band
(A, B)when the sample size increases to 400and600.
For the choice of k, Figure 3 shows that their may exist some optimal kfor small sample
size, while there is no significant trend when ngets large. Figure 4 indicates that the loss is
robust to the choice of l‚Ä≤when n= 200. For the cases where n= 100, the figures shows a
downward trend when l‚Ä≤is small, and the trend gets flattened as l‚Ä≤gets large.
25Table 5: The averages of correlation loss with the Toplitz covariance structure using (2.8)
for estimating Œ£E(X|Y).
(n, p)Model DT-SIR Lasso-SIR RTS SSIRvRP SSIRvRP-BIC SSIRvRP-AIC
(100,200) I 0.945 0.186 0.073 0.011 0.017 0.022
II 0.943 0.297 0.046 0.060 0.055 0.120
III 0.951 0.088 0.022 0.002 0.009 0.002
IV 0.915 0.323 0.092 0.002 0.008 0.003
V 0.794 0.345 0.061 0.125 0.135 0.157
(100,400) I 0.952 0.283 0.033 0.016 0.018 0.036
II 0.919 0.368 0.017 0.119 0.111 0.166
III 0.917 0.254 0.042 0.003 0.006 0.003
IV 0.907 0.477 0.141 0.003 0.008 0.003
V 0.763 0.496 0.099 0.142 0.157 0.177
(100,600) I 0.927 0.474 0.105 0.024 0.026 0.038
II 0.874 0.581 0.187 0.104 0.098 0.186
III 0.935 0.340 0.021 0.003 0.004 0.003
IV 0.868 0.692 0.202 0.002 0.007 0.006
V 0.765 0.528 0.201 0.125 0.135 0.157
(200,600) I 0.932 0.070 0.010 0.004 0.003 0.018
II 0.926 0.150 0.034 0.012 0.016 0.081
III 0.831 0.030 0.008 0.001 0.001 0.002
IV 0.938 0.151 0.017 0.001 0.001 0.001
V 0.603 0.179 0.040 0.031 0.038 0.093
(400,600) I 0.372 0.014 0.008 0.002 0.002 0.008
II 0.289 0.021 0.012 0.003 0.005 0.039
III 0.176 0.004 0.003 0.000 0.000 0.001
IV 0.577 0.021 0.010 0.000 0.000 0.000
V 0.199 0.039 0.019 0.009 0.012 0.047
To summarize, when a moderately large number of samples are collected, there is no need
to worry about the choice of hyperparameters. However, when the sample size is limited, we
suggest lager values for (A, B)andl‚Ä≤and a medium scale for k.
7 Real data study
In this section, we consider a gene expression data from the international ‚ÄúHapMap‚Äù project
(Thorisson et al., 2005). The data includes 90 samples, 45 Chinese and 45 Japanese, and
reports the gene expression levels from 47293 probes. The gene named CHRNA6 is the
subject of many nicotine addiction studies (Thorgeirsson et al., 2010), and we treat its
26Figure 1: Effect of the reweighting step under the Toplitz covariance structure. (a1)-(e1)
and (a2)-(e2) correspond to (n, p)settings: (100,200)-(400,600).
mRNA expression as the response Yas done in Fan et al. (2018) and Lin et al. (2019). The
expressions of other 47292 genes are treated as the covariates. Consequently, we are now
faced with a problem where p= 47292 ‚â´n= 90.
27Figure 2: Choice of A and B for Model I with Identity covariance.
Figure 3: Choice of k.
Our goal is to identify several combinations of a few genes that represent the regression
relationship of Yregarding X, and the proposed SSIRvRP method is well-suited for this
28Figure 4: Choice of l‚Ä≤.
task. Following Lin et al. (2019), we set d= 1andl= 13and other hyperparameters as
those in the simulation. Based on the estimated coefficients bŒ≤, we define Z=bŒ≤‚ä§Xand plot
Yagainst Zin Figure 5. We then find that there exists a moderate quadratic patten between
YandZ. Motivated by this finding, we go further to fit a regression model between Yand
Z, Z2, and obtain an adjusted R-squared 0.620 and a p-value 0.000. The mean squared error
of the fitted model is 0.040. These results seem a little better than those in Lin et al. (2019),
where the authors obtained an R-squared 0.578 and a mean squared error 0.044.
For comparison, we also employ the sparse PCA algorithm proposed by Gataric et al.
(2020) to estimate the coefficients eŒ≤where the information of Yis absent, and calculate
Z‚Ä≤=eŒ≤‚ä§X. The scatter plot of Yagainst Z‚Ä≤does not indicate any interesting patterns
between the two variables. Running a linear regression model between YandZ‚Ä≤gives us
an R-squared 0.001 and a p-value 0.792, indicating that there is no linear patten between
the two variables. This result seems less meaningful than those achieved by the proposed
method which is supervised by Y.
8 Discussion
In this paper, we propose a random projection method to estimate the central subspace in
sparse SIR when p‚â´n. Compared with existing methods, the proposed algorithm is compu-
29‚àí79 ‚àí78 ‚àí77 ‚àí76 ‚àí756.0 6.5 7.0 7.5
zyFigure 5: Quadratic relationship.
tationally simpler and more efficient. Theoretically, we proved that the proposed estimator
achieves the minmax optimal rate under suitable assumptions. It is notable that the ran-
dom projection technique is introduced to solve a generalize eigenvalue-eigenvector problem.
Hence, it can also be applied to sparse Fisher‚Äôs discriminant analysis for classification and
sparse canonical correlation analysis for exploring the relationship of two high-dimensional
random vectors. We leave it for further study.
9 Appendix
9.1 Proof of Theorem 1
The following lemma is needed, whose proof is given in Section 9.4.1.
Lemma 1. Under Assumption (A1), there is an event ‚Ñ¶with probability at least 1‚àícp‚àí3
such that on ‚Ñ¶, it holds that
supu‚ààBp‚àí1
0(k)|u‚ä§(bŒ£E(X|Y)‚àíŒ£E(X|Y))u| ‚â§Kr
klog(p)
n
30provided that klogp‚â™n, where Bp‚àí1
0(k) ={v= (v(1), . . . , v(p))‚ä§‚ààRp:‚à•v‚à•2= 1,Pp
j=11{v(j)Ã∏=
0} ‚â§k}fork‚àà[p]andKis a constant depending on H.
We begin the proof. The proof is similar to that of Theorem 1 of Gataric et al. (2020),
but some details should be adjusted, cause we now have a sliced estimator of the covariance
matrix. For notation simplicity, we drop the subscript E(X|Y)ofŒ£, andbŒ£. Define Sk:=
{S‚äÇ[p] :|S|=k}and recall A={j‚àà[p] :Œ≤(j,¬∑)Ã∏=0}which denotes the set of non-zero
rows of Œ≤. For any S‚àà Sk, notice that Œ£(S,S)=Œ≤(S,¬∑)Œõ(Œ≤(S,¬∑))‚ä§. Hence,
dX
i=1Œªi(Œ£(S,S)) =kX
i=1Œªi(Œ£(S,S))‚àíkX
i=d+1Œªi(Œ£(S,S)) = tr( Œ£(S,S)) =dX
i=1X
j‚ààS‚à©AŒªi(Œ≤(j,i))2.(9.1)
Then, by Lemma 1, we know there is an event ‚Ñ¶with probability at least 1‚àícp3such that
on‚Ñ¶we have for any S‚àà Sk,
‚à•bŒ£(S,S)‚àíŒ£(S,S)‚à•op‚â§Kr
klogp
n,
where K > 0is some constant independent of S. On the event ‚Ñ¶defined above, by Weyl‚Äôs
inequality, we have that
dX
i=1Œªi(bŒ£(S,S))‚àídX
i=1X
j‚ààS‚à©AŒªi(Œ≤(j,i))2(9.1)=dX
i=1{Œªi(bŒ£(S,S))‚àíŒªi(Œ£(S,S))}
‚â§Kdr
klogp
n(3.3)
‚â§dŒªd
16s¬µ2.
Thus, on the event ‚Ñ¶, if(S‚à© A)‚ää(S‚Ä≤‚à© A), then
dX
i=1Œªi(bŒ£(S,S))‚àídX
i=1Œªi(bŒ£(S‚Ä≤,S‚Ä≤))
‚â§dX
i=1X
j‚ààS‚à©AŒªi(Œ≤(j,i))2‚àídX
i=1X
j‚ààS‚Ä≤‚à©AŒªi(Œ≤(j,i))2+dŒªd
8s¬µ2
‚â§ ‚àídX
i=1X
j‚àà(S‚Ä≤\S)‚à©AŒªi(Œ≤(j,i))2+dŒªd
8s¬µ2. (9.2)
31Notice that expression (3.1) implies that
dX
i=1Œªi(Œ≤(j,i))2‚â•Œªd‚à•Œ≤(j,¬∑)‚à•2
2‚â•dŒªd
s¬µ2(9.3)
for any j‚àà A. Then, combining (9.2) and (9.3), we have
dX
i=1Œªi(bŒ£(S,S))<dX
i=1Œªi(bŒ£(S‚Ä≤,S‚Ä≤)). (9.4)
Given the above expression, we now prove that on the event ‚Ñ¶, for some fixed j‚àà Aand
j‚Ä≤/‚àà A, it holds that
qj‚â•qj‚Ä≤, (9.5)
where qk=P(k‚ààSa,b‚àó(a)|X)for any k‚àà[p]and some fixed a‚àà[A]. We begin the proof.
Define for Àúj‚àà {j, j‚Ä≤}andb‚àà[B]the following sets:
Sb,Àúj:={(Sa,1, . . . , S a,B) :b‚àó(a) =b,Àúj‚ààSa,b},and
Sb:={(Sa,1, . . . , S a,B) :b‚àó(a) =b}.
Let the map œà:Sk‚Üí S kbe defined such that
œà(S) :=(S\ {j‚Ä≤})‚à™ {j}ifj‚Ä≤‚ààSandj /‚ààS,
S otherwise .
Then for every S‚àà Sk, either œà(S) =SorS‚à©A‚ääœà(S)‚à©A. Hence, inequality (9.4) implies
that
dX
i=1Œªi(bŒ£(S,S))‚â§dX
i=1Œªi(bŒ£(œà(S),œà(S))). (9.6)
Moreover, by the definition of œà(¬∑), we know that if j‚Ä≤‚ààSfor some S‚àà Sk, then j‚ààœà(S).
Thus, for any (Sa,1, . . . , S a,b, . . . , S a,B)‚àà Sb,j‚Ä≤, there exists (Sa,1, . . . , œà (Sa,b), . . . , S a,B)‚àà Sb,j.
32To see this clearly, since (Sa,1, . . . , S a,b, . . . , S a,B)‚àà Sb,j‚Ä≤, then b‚àó(a) =bandj‚Ä≤‚ààSa,b. Notice
thatb‚àó(a) =bimplies that
max
b‚àà[B]dX
i=1Œªi(bŒ£(Sa,b,Sa,b))‚â§dX
i=1Œªi(bŒ£(Sa,b,Sa,b)).
Together with (9.6), we know that
max
b‚àà[B]dX
i=1Œªi(bŒ£(Sa,b,Sa,b))‚â§dX
i=1Œªi(bŒ£(œà(Sa,b),œà(Sa,b))),
which implies that (Sa,1, . . . , œà (Sa,b), . . . , S a,B)satisfies b‚àó(a) =b. Combining the fact that
j‚ààœà(Sa,b), we obtain that (Sa,1, . . . , œà (Sa,b), . . . , S a,B)‚àà Sb,j. Hence, |Sb,j‚Ä≤| ‚â§ |S b,j|. Then,
on‚Ñ¶, it holds for all b‚àà[B]that
P{j‚ààSa,b‚àó(a)|X, b‚àó(a) =b}
=P{j‚ààSa,b‚àó(a), b‚àó(a) =b|X}
P{b‚àó(a) =b|X}=|Sb,j|
|Sb|
‚â•|Sb,j‚Ä≤|
|Sb|=P{j‚Ä≤‚ààSa,b‚àó(a), b‚àó(a) =b|X}
P{b‚àó(a) =b|X}
=P{j‚Ä≤‚ààSa,b‚àó(a)|X, b‚àó(a) =b}.
Therefore, we have proved qj‚â•qj‚Ä≤as shown in (9.5).
Notice that
X
Àúj‚àà[p]qÀúj=X
Àúj‚àà[p]P{Àúj‚ààSa,b‚àó(a)|X}=1
BX
b‚àà[B]X
Àúj‚àà[p]P{Àúj‚ààSa,b‚àó(a)|b‚àó(a) =b,X}=X
Àúj‚àà[p]k
p=k .
Thus, by (9.5) we obtain on ‚Ñ¶that
qj‚â•P
Àúj‚àà([p]\A‚à™{ j})qÀúj
p‚àís+ 1=k‚àíP
Àúj‚ààA\{ j}qÀúj
p‚àís+ 1‚â•k‚àís+ 1
p‚àís+ 1‚â•1
p. (9.7)
33Remark 3.Since Cov(X) =Ip,b‚àó(a)defined in Algorithm 1 reduces to
b‚àó(a) :=sargmaxb‚àà[B]dX
i=1ÀÜŒªa,b;i,
where ÀÜŒªa,b;i=Œªi(Pa,bbŒ£Pa,b), the i-th largest eigenvalue of the matrix Pa,bbŒ£Pa,b.
Define Œªa,b;i:=Œªi(Pa,bŒ£Pa,b)and the corresponding eigenvector va,b;i:=vi(Pa,bŒ£Pa,b)for
b‚àà[B]andi‚àà[p]. Notice that Œ£hasdnone-zero eigenvalues, so does Pa,bŒ£Pa,b. Then
Œªa,b;d+1=. . .=Œªa,b;p= 0. Write Va,b:= (va,b;1, . . . ,va,b:d),bVa,b:= (ÀÜva,b;1, . . . , ÀÜva,b:d),Œõa,b=
diag( Œªa,b;1, . . . , Œª a,b;d)andbŒõa,b= diag( ÀÜŒªa,b;1‚àíÀÜŒªa,b;d+1, . . . , ÀÜŒªa,b;d‚àíÀÜŒªa,b;d+1). For Àúj‚àà {j, j‚Ä≤}, let
ÀÜw(Àúj)
a:= (bVa,b‚àó(a)bŒõa,b‚àó(a)bV‚ä§
a,b‚àó(a))(Àúj,Àúj)=dX
i=1(ÀÜŒªa,b‚àó(a);i‚àíÀÜŒªa,b‚àó(a);d+1)(ÀÜv(Àúj)
a,b‚àó(a);i)2.
We now give upper bound and lower bound of ÀÜw(Àúj)
a.
Notice that
Va,b‚àó(a)Œõa,b‚àó(a)V‚ä§
a,b‚àó(a)=pX
i=1Œªa,b‚àó(a);iva,b‚àó(a);iv‚ä§
a,b‚àó(a);i=Pa,b‚àó(a)Œ£Pa,b‚àó(a),
whichimpliesthat (Va,b‚àó(a)Œõa,b‚àó(a)V‚ä§
a,b‚àó(a))(Àúj,Àúj)= Œ£(Àúj,Àúj)forÀúj‚ààSa,b‚àó(a)and(Va,b‚àó(a)Œõa,b‚àó(a)V‚ä§
a,b‚àó(a))(Àúj,Àúj)=
0otherwise. By Lemma 2 in Appendix A.5 of Gataric et al. (2020), on ‚Ñ¶we have
|(bVa,b‚àó(a)bŒõa,b‚àó(a)bV‚ä§
a,b‚àó(a))(Àúj,Àúj)‚àí(Va,b‚àó(a)Œõa,b‚àó(a)V‚ä§
a,b‚àó(a))(Àúj,Àúj)|
(1)
‚â§ ‚à•bVa,b‚àó(a)bŒõa,b‚àó(a)bV‚ä§
a,b‚àó(a)‚àíVa,b‚àó(a)Œõa,b‚àó(a)V‚ä§
a,b‚àó(a)‚à•op‚â§4d‚à•Pa,b‚àó(a)(bŒ£‚àíŒ£)Pa,b‚àó(a)‚à•op
(2)
‚â§4Kdr
klog(p)
n(3.3)
‚â§dŒªd
4s¬µ2, (9.8)
where inequality (1)is obtained by the definition of the operator norm of a matrix and (2)
is implied by Lemma 1. Thus, on ‚Ñ¶‚à© {j‚ààSa,b‚àó(a)}, we have
ÀÜw(j)
a= (bVa,b‚àó(a)bŒõa,b‚àó(a)bV‚ä§
a,b‚àó(a))(j,j)‚àí(Va,b‚àó(a)Œõa,b‚àó(a)V‚ä§
a,b‚àó(a))(j,j)+ (Va,b‚àó(a)Œõa,b‚àó(a)V‚ä§
a,b‚àó(a))(j,j)
‚â•Œ£(j,j)‚àí |(bVa,b‚àó(a)bŒõa,b‚àó(a)bV‚ä§
a,b‚àó(a))(j,j)‚àí(Va,b‚àó(a)Œõa,b‚àó(a)V‚ä§
a,b‚àó(a))(j,j)|(9.8)
‚â•Œ£(j,j)‚àídŒªd
4s¬µ2
34‚â•Œªd‚à•V(j,¬∑)‚à•2
2‚àídŒªd
4s¬µ2(3.1)
‚â•3dŒªd
4s¬µ2. (9.9)
Similarly, on ‚Ñ¶‚à© {j‚ààSa,b‚àó(a)}, we have
ÀÜw(j)
a‚â§5dŒª1¬µ2
4s. (9.10)
Furthermore, on ‚Ñ¶‚à© {j‚Ä≤‚ààSa,b‚àó(a)}, it holds that
‚àídŒªd
4s¬µ2‚â§ÀÜw(j‚Ä≤)
a‚â§dŒªd
4s¬µ2(9.11)
by noting that j‚Ä≤‚àà AcandŒ£(j‚Ä≤,j‚Ä≤)=Pd
i=1Œªi(Œ≤(j‚Ä≤)
i)2= 0. Finally, for all j‚àà[p], ifj /‚ààSa,b‚àó(a),
then by the definition of ÀÜva,b‚àó(a);ifori‚àà[d], we know that (ÀÜva,b‚àó(a);i)(j)= 0forisatisfying
ÀÜŒªa,b‚àó(a);i>0. Thus, by the definition of ÀÜw(j)
a, it holds that ÀÜw(j)
a= 0forj /‚ààSa,b‚àó(a). Hence, we
have given upper bound and lower bound of ÀÜw(Àúj)
a. Using the lower bound and upper bound
given above. we obtain on ‚Ñ¶that
E( ÀÜw(j)
a‚àíÀÜw(j‚Ä≤)
a|X)
=E[ ÀÜw(j)
a(1{j‚ààSa,b‚àó(a)}+1{j /‚ààSa,b‚àó(a)})|X]‚àíE[ ÀÜw(j‚Ä≤)
a(1{j‚Ä≤‚ààSa,b‚àó(a)}+1{j‚Ä≤/‚ààSa,b‚àó(a)})|X]
=E( ÀÜw(j)
a1{j‚ààSa,b‚àó(a)} ‚àíÀÜw(j‚Ä≤)
a1{j‚Ä≤‚ààSa,b‚àó(a)}|X)
‚â•3qjdŒªd
4s¬µ2‚àíqj‚Ä≤dŒªd
4s¬µ2(9.5)
‚â•qjdŒªd
2s¬µ2(9.7)
‚â•dŒªd
2ps¬µ2. (9.12)
Now we let a,jandj‚Ä≤freely vary again. Define ‚Ñ¶1:={min j‚ààAÀÜw(j)>max j‚ààAcÀÜw(j)}.
Since l‚â•s, on‚Ñ¶1, it holds that A ‚äÜ ÀÜS, which implies that the dleading eigenvectors of
PÀÜSŒ£PÀÜSare the same as those of Œ£. Hence, by Lemma 1 and Theorem 2 of Yu et al. (2015),
on‚Ñ¶‚à©‚Ñ¶1,
L(bŒ≤,Œ≤)‚â§2d1/2‚à•PÀÜS(bŒ£‚àíŒ£)PÀÜS‚à•op
Œªd‚â§2Ks
dllog(p)
nŒª2
d.
Thenitsufficestoderivativethelowerboundof P(‚Ñ¶‚à©‚Ñ¶1). Observethat ÀÜw(j)=A‚àí1PA
a=1ÀÜw(j)
a
35for any j‚àà[p]. Then, for any j‚àà Aandj‚Ä≤‚àà Ac, on‚Ñ¶, it holds that
ÀÜw(j)‚àíÀÜw(j‚Ä≤)
={ÀÜw(j)‚àíE( ÀÜw(j)|X)} ‚àí { ÀÜw(j‚Ä≤)‚àíE( ÀÜw(j‚Ä≤)|X)}+E( ÀÜw(j)‚àíÀÜw(j‚Ä≤)|X)
(9.12)
‚â•dŒªd
2ps¬µ2+{ÀÜw(j)‚àíE( ÀÜw(j)|X)} ‚àí { ÀÜw(j‚Ä≤)‚àíE( ÀÜw(j‚Ä≤)|X)},
which implies
‚Ñ¶c
1‚äÜ[
j‚ààA
ÀÜw(j)‚àíE( ÀÜw(j)|X)‚â§dŒªd
4ps¬µ2
‚à™[
j‚Ä≤/‚ààA
ÀÜw(j‚Ä≤)‚àíE( ÀÜw(j‚Ä≤)|X)‚â•dŒªd
4ps¬µ2
.
Notice that conditioned on X,{ÀÜw(j)
a}a‚àà[A]are i.i.d. random variables, and (9.9), (9.10) and
(9.11) implies that ÀÜw(j)
ais bounded on ‚Ñ¶for all j‚àà[p]. Thus, by the union bound and the
Hoeffding‚Äôs inequality conditioned on X, on‚Ñ¶we obtain that
P(‚Ñ¶c
1|X)‚â§pexp
‚àíAŒª2
d
50p2¬µ8Œª2
1
.
We complete the proof by noting that
P(‚Ñ¶‚à©‚Ñ¶1)‚â•1‚àícp‚àí3‚àípexp
‚àíAŒª2
d
50p2¬µ8Œª2
1
.
‚ñ°
9.2 Proof of Theorem 2
Recall Sk:={S‚äÇ[p] :|S|=k}andA={j‚àà[p] :Œ≤(j,¬∑)Ã∏=0}. For any S‚àà Sk, notice that
M(S,S)=Œ≥(S,¬∑)Œõ(Œ≥(S,¬∑))‚ä§. Hence,
dX
i=1Œªi(Œ£(S,S)
E(X|Y),Œ£(S,S)) =dX
i=1Œªi(M(S,S)) (9.13)
=kX
i=1Œªi(M(S,S))‚àíkX
i=d+1Œªi(M(S,S)) = tr( M(S,S)) =dX
i=1X
j‚ààS‚à©AŒªi(Œ≥(j,i))2,(9.14)
36where equation (9.13) is obtained by the following lemma.
Lemma 2. For any S‚àà S k, letŒ£(S,S)=L(S,S)L‚ä§
(S,S)andŒ£=LL‚ä§be the Cholesky
decomposition of Œ£(S,S)andŒ£, respectively. It then holds that L(S,S)=L(S,S)and
L‚àí1
(S,S)Œ£(S,S)
E(X|Y)(L‚àí1
(S,S))‚ä§={L‚àí1Œ£E(X|Y)(L‚àí1)‚ä§}(S,S)=M(S,S)
for some ordering of the random vector X.
Then, by similar arguments of (3.4), we get
Œªi(Œ£(S,S)
E(X|Y),Œ£(S,S)) =Œªi(L‚àí1
(S,S)Œ£(S,S)
E(X|Y)(L‚àí1
(S,S))‚ä§) =Œªi(M(S,S)),
which gives (9.13).
By Assumption (A3), we know there is an event ‚Ñ¶with probability at least 1‚àíc‚Ä≤p‚àíc2
such that on ‚Ñ¶we have
sup
S‚ààSkmax{‚à•bL(S,S)‚àíL(S,S)‚à•op,‚à•cM(S,S)‚àíM(S,S)‚à•op} ‚â§Kr
klogp
n,
and
sup
S‚ààSlmax{‚à•bL(S,S)‚àíL(S,S)‚à•op,‚à•cM(S,S)‚àíM(S,S)‚à•op} ‚â§Kr
llogp
n,
where c‚Ä≤>0is some constant independent of S. On the event ‚Ñ¶defined above, by Weyl‚Äôs
inequality, we have that
dX
i=1Œªi(cM(S,S))‚àídX
i=1X
j‚ààS‚à©AŒªi(Œ≥(j,i))2(9.14)=dX
i=1{Œªi(cM(S,S))‚àíŒªi(M(S,S))}
‚â§Kdr
klogp
n(3.7)
‚â§dŒªd
16s¬µ2.
Thus, on the event ‚Ñ¶, if(S‚à© A)‚ää(S‚Ä≤‚à© A), then
dX
i=1Œªi(cM(S,S))‚àídX
i=1Œªi(cM(S‚Ä≤,S‚Ä≤))
37‚â§dX
i=1X
j‚ààS‚à©AŒªi(Œ≥(j,i))2‚àídX
i=1X
j‚ààS‚Ä≤‚à©AŒªi(Œ≥(j,i))2+dŒªd
8s¬µ2
‚â§ ‚àídX
i=1X
j‚àà(S‚Ä≤\S)‚à©AŒªi(Œ≥(j,i))2+dŒªd
8s¬µ2. (9.15)
Notice that expression (3.5) implies that
dX
i=1Œªi(Œ≥(j,i))2‚â•Œªd‚à•Œ≥(j,¬∑)‚à•2
2‚â•dŒªd
s¬µ2(9.16)
for any j‚àà A. Then, combining (9.15) and (9.16), we have
dX
i=1Œªi(cM(S,S))<dX
i=1Œªi(cM(S‚Ä≤,S‚Ä≤)). (9.17)
Given the above expression, we now prove that on the event ‚Ñ¶, for some fixed j‚àà Aand
j‚Ä≤/‚àà A, it holds that
qj‚â•qj‚Ä≤, (9.18)
where qk=P(k‚ààSa,b‚àó(a)|X)for any k‚àà[p]and some fixed a‚àà[A]. We begin the proof.
Define for Àúj‚àà {j, j‚Ä≤}andb‚àà[B]the following sets:
Sb,Àúj:={(Sa,1, . . . , S a,B) :b‚àó(a) =b,Àúj‚ààSa,b},and
Sb:={(Sa,1, . . . , S a,B) :b‚àó(a) =b}.
Let the map œà:Sk‚Üí S kbe defined such that
œà(S) :=(S\ {j‚Ä≤})‚à™ {j}ifj‚Ä≤‚ààSandj /‚ààS ,
S otherwise .
Then for every S‚àà S k, either œà(S) =SorS‚à© A‚ääœà(S)‚à© A. Hence, inequality (9.17)
38implies that
dX
i=1Œªi(cM(S,S))‚â§dX
i=1Œªi(cM(œà(S),œà(S))). (9.19)
Moreover, by the definition of œà(¬∑), we know that if j‚Ä≤‚ààSfor some S‚àà Sk, then j‚ààœà(S).
Thus, for any (Sa,1, . . . , S a,b, . . . , S a,B)‚àà Sb,j‚Ä≤, there exists (Sa,1, . . . , œà (Sa,b), . . . , S a,B)‚àà Sb,j.
To see this clearly, since (Sa,1, . . . , S a,b, . . . , S a,B)‚àà Sb,j‚Ä≤, then b‚àó(a) =bandj‚Ä≤‚ààSa,b. Notice
thatb‚àó(a) =bimplies that
max
b‚àà[B]dX
i=1Œªi(bŒ£(Sa,b,Sa,b)
E(X|Y),bŒ£(Sa,b,Sa,b))Lemma 2= max
b‚àà[B]dX
i=1Œªi(cM(Sa,b,Sa,b))
‚â§dX
i=1Œªi(bŒ£(Sa,b,Sa,b)
E(X|Y),bŒ£(Sa,b,Sa,b)) =dX
i=1Œªi(cM(Sa,b,Sa,b)).
Together with (9.19), we know that
max
b‚àà[B]dX
i=1Œªi(cM(Sa,b,Sa,b))‚â§dX
i=1Œªi(cM(œà(Sa,b),œà(Sa,b))),
which implies that (Sa,1, . . . , œà (Sa,b), . . . , S a,B)satisfies b‚àó(a) =b. Combining the fact that
j‚ààœà(Sa,b), we obtain that (Sa,1, . . . , œà (Sa,b), . . . , S a,B)‚àà Sb,j. Hence, |Sb,j‚Ä≤| ‚â§ |S b,j|. Then,
on‚Ñ¶, it holds for all b‚àà[B]that
P{j‚ààSa,b‚àó(a)|X, b‚àó(a) =b}
=P{j‚ààSa,b‚àó(a), b‚àó(a) =b|X}
P{b‚àó(a) =b|X}=|Sb,j|
|Sb|
‚â•|Sb,j‚Ä≤|
|Sb|=P{j‚Ä≤‚ààSa,b‚àó(a), b‚àó(a) =b|X}
P{b‚àó(a) =b|X}
=P{j‚Ä≤‚ààSa,b‚àó(a)|X, b‚àó(a) =b}.
Therefore, we have proved qj‚â•qj‚Ä≤as shown in (9.18).
39Notice that
X
Àúj‚àà[p]qÀúj=X
Àúj‚àà[p]P{Àúj‚ààSa,b‚àó(a)|X}=1
BX
b‚àà[B]X
Àúj‚àà[p]P{Àúj‚ààSa,b‚àó(a)|b‚àó(a) =b,X}=X
Àúj‚àà[p]k
p=k .
Thus, by (9.18) we obtain on ‚Ñ¶that
qj‚â•P
Àúj‚àà([p]\A‚à™{ j})qÀúj
p‚àís+ 1=k‚àíP
Àúj‚ààA\{ j}qÀúj
p‚àís+ 1‚â•k‚àís+ 1
p‚àís+ 1‚â•1
p. (9.20)
Define Œªa,b;i:=Œªi(Œ£(Sa,b,Sa,b)
E(X|Y),Œ£(Sa,b,Sa,b))andÀÜŒªa,b;i:=Œªi(bŒ£(Sa,b,Sa,b)
E(X|Y),bŒ£(Sa,b,Sa,b))andva,b;i:=
vi(Œ£(Sa,b,Sa,b)
E(X|Y),Œ£(Sa,b,Sa,b))andÀÜva,b;i:=vi(bŒ£(Sa,b,Sa,b)
E(X|Y),bŒ£(Sa,b,Sa,b))forb‚àà[B]andi‚ààSa,b. No-
tice that the pair (Œ£(Sa,b,Sa,b)
E(X|Y),Œ£(Sa,b,Sa,b))hasdnone-zero generalized eigenvalues. Then
Œªa,b;d+1=. . .=Œªa,b;k= 0. Write Va,b:= (va,b;1, . . . ,va,b:d),bVa,b:= (ÀÜva,b;1, . . . , ÀÜva,b:d),
Œõa,b= diag( Œªa,b;1, . . . , Œª a,b;d)andbŒõa,b= diag( ÀÜŒªa,b;1‚àíÀÜŒªa,b;d+1, . . . , ÀÜŒªa,b;d‚àíÀÜŒªa,b;d+1). For
Àúj‚ààSa,b‚àó(a), let
ÀÜw(Àúj)
a:= (bVa,b‚àó(a)bŒõa,b‚àó(a)bV‚ä§
a,b‚àó(a))(Àúj,Àúj)=dX
i=1(ÀÜŒªa,b‚àó(a);i‚àíÀÜŒªa,b‚àó(a);d+1)(ÀÜv(Àúj)
a,b‚àó(a);i)2.
We now give upper bound and lower bound of ÀÜw(Àúj)
a.
Notice that
Va,b‚àó(a)Œõa,b‚àó(a)V‚ä§
a,b‚àó(a)
=dX
i=1Œªa,b‚àó(a);iva,b‚àó(a);iv‚ä§
a,b‚àó(a);i(a)=kX
i=1Œªa,b‚àó(a);iva,b‚àó(a);iv‚ä§
a,b‚àó(a);i
(b)=L‚àí1,‚ä§
(Sa,b‚àó(a),Sa,b‚àó(a))kX
i=1Œªa,b‚àó(a);iŒ≥a,b‚àó(a);iŒ≥‚ä§
a,b‚àó(a);i
L‚àí1
(Sa,b‚àó(a),Sa,b‚àó(a))
(c)=L‚àí1,‚ä§
(Sa,b‚àó(a),Sa,b‚àó(a))(L‚àí1
(Sa,b‚àó(a),Sa,b‚àó(a))Œ£(Sa,b‚àó(a),Sa,b‚àó(a))
E(X|Y)L‚àí1,‚ä§
(Sa,b‚àó(a),Sa,b‚àó(a)))L‚àí1
(Sa,b‚àó(a),Sa,b‚àó(a))
(d)= (L(Sa,b‚àó(a),Sa,b‚àó(a)))‚àí1,‚ä§M(Sa,b‚àó(a),Sa,b‚àó(a))(L(Sa,b‚àó(a),Sa,b‚àó(a)))‚àí1
(e)= (L(Sa,b‚àó(a),Sa,b‚àó(a)))‚àí1,‚ä§(Œ≥(Sa,b‚àó(a),¬∑)ŒõŒ≥(Sa,b‚àó(a),¬∑),‚ä§)(L(Sa,b‚àó(a),Sa,b‚àó(a)))‚àí1, (9.21)
where Œ£(Sa,b‚àó(a),Sa,b‚àó(a))=L(Sa,b‚àó(a),Sa,b‚àó(a))L‚ä§
(Sa,b‚àó(a),Sa,b‚àó(a))is the cholesky decomposition of
40Œ£(Sa,b‚àó(a),Sa,b‚àó(a))andŒ≥a,b‚àó(a);i=vi(L‚àí1
(Sa,b‚àó(a),Sa,b‚àó(a))Œ£(Sa,b‚àó(a),Sa,b‚àó(a))
E(X|Y)L‚àí1,‚ä§
(Sa,b‚àó(a),Sa,b‚àó(a))). Equation
(a)is implied by the fact that Œªa,b‚àó(a);d+1=. . .=Œªa,b‚àó(a);k= 0, (b) is obtained by a transfor-
mation similar to (3.4), (c) is given by the spectral decomposition, (d) uses Lemma 2, and
(e) is implied by the fact that M=Œ≥ŒõŒ≥‚ä§.
LetS0=Sa,b‚àó(a)‚à© AandSc
0=Sa,b‚àó(a)‚à© Ac. Without loss of generality, we write
L(Sa,b‚àó(a),Sa,b‚àó(a))=Ô£´
Ô£≠L(S0,S0)0
L(Sc
0,S0)L(Sc
0,Sc
0)Ô£∂
Ô£∏.
Then, recalling that the transformed basis Œ≥retains the row sparsity of Œ≤, we have by
equation (9.21) that
Va,b‚àó(a)Œõa,b‚àó(a)V‚ä§
a,b‚àó(a)=Ô£´
Ô£≠L(S0,S0),‚àí1,‚ä§‚àó
0‚ä§‚àóÔ£∂
Ô£∏Ô£´
Ô£≠Œ≥(S0,¬∑)ŒõŒ≥(S0,¬∑),‚ä§0
0 0Ô£∂
Ô£∏Ô£´
Ô£≠L(S0,S0),‚àí10
‚àó ‚àóÔ£∂
Ô£∏
=Ô£´
Ô£≠L(S0,S0),‚àí1,‚ä§(Œ≥(S0,¬∑)ŒõŒ≥(S0,¬∑),‚ä§)0
0 0Ô£∂
Ô£∏Ô£´
Ô£≠L(S0,S0),‚àí10
‚àó ‚àóÔ£∂
Ô£∏
=Ô£´
Ô£≠L(S0,S0),‚àí1,‚ä§(Œ≥(S0,¬∑)ŒõŒ≥(S0,¬∑),‚ä§)L(S0,S0),‚àí10
0 0Ô£∂
Ô£∏,
which implies that (Va,b‚àó(a)Œõa,b‚àó(a)V‚ä§
a,b‚àó(a))(Àúj,Àúj)= (L(Àúj,Àúj))‚àí2M(Àúj,Àúj)forÀúj‚ààS0under some or-
dering of Xand(Va,b‚àó(a)Œõa,b‚àó(a)V‚ä§
a,b‚àó(a))(Àúj,Àúj)= 0otherwise. Noting that Kp
klog(p)/n‚â§
min{Œª1/(4d),‚àöŒ∏1, Œ∏p/(6‚àöŒ∏1)}, on‚Ñ¶we have
|(bVa,b‚àó(a)bŒõa,b‚àó(a)bV‚ä§
a,b‚àó(a))(Àúj,Àúj)‚àí(Va,b‚àó(a)Œõa,b‚àó(a)V‚ä§
a,b‚àó(a))(Àúj,Àúj)|
(1)
‚â§ ‚à•bVa,b‚àó(a)bŒõa,b‚àó(a)bV‚ä§
a,b‚àó(a)‚àíVa,b‚àó(a)Œõa,b‚àó(a)V‚ä§
a,b‚àó(a)‚à•op
=(bL(Sa,b‚àó(a),Sa,b‚àó(a)))‚àí1,‚ä§dX
i=1(ÀÜŒªa,b‚àó(a);i‚àíÀÜŒªa,b‚àó(a);d+1)bŒ≥a,b‚àó(a);ibŒ≥‚ä§
a,b‚àó(a);i
(bL(Sa,b‚àó(a),Sa,b‚àó(a)))‚àí1‚àí
(L(Sa,b‚àó(a),Sa,b‚àó(a)))‚àí1,‚ä§dX
i=1(Œªa,b‚àó(a);i‚àíŒªa,b‚àó(a);d+1)Œ≥a,b‚àó(a);iŒ≥‚ä§
a,b‚àó(a);i
(L(Sa,b‚àó(a),Sa,b‚àó(a)))‚àí1
op
(2)
‚â§4‚àí1Cdr
klog(p)
n(3.7)
‚â§dœÑŒª d
4s¬µ2, (9.22)
41where bŒ≥a,b‚àó(a);i=vi((bL(Sa,b‚àó(a),Sa,b‚àó(a)))‚àí1bŒ£(Sa,b‚àó(a),Sa,b‚àó(a))
E(X|Y)(bL(Sa,b‚àó(a),Sa,b‚àó(a)))‚àí1,‚ä§),bŒ£=bLbL‚ä§is
the Cholesky decomposition of bŒ£for some ordering of X. Inequality (1)is obtained by the
definition of the operator norm of a matrix, and (2) is implied by Assumptions (A1), (A1‚Äô),
(A3), the triangle inequality, and Lemma 2 of Gataric et al. (2020). Thus, on ‚Ñ¶‚à© {j‚àà
Sa,b‚àó(a)}, we have by Assumption (A4) that
ÀÜw(j)
a= (bVa,b‚àó(a)bŒõa,b‚àó(a)bV‚ä§
a,b‚àó(a))(j,j)‚àí(Va,b‚àó(a)Œõa,b‚àó(a)V‚ä§
a,b‚àó(a))(j,j)+ (Va,b‚àó(a)Œõa,b‚àó(a)V‚ä§
a,b‚àó(a))(j,j)
‚â•(L(j,j))‚àí2M(j,j)‚àí |(bVa,b‚àó(a)bŒõa,b‚àó(a)bV‚ä§
a,b‚àó(a))(j,j)‚àí(Va,b‚àó(a)Œõa,b‚àó(a)V‚ä§
a,b‚àó(a))(j,j)|
(9.22)
‚â•(L(j,j))‚àí2M(j,j)‚àídœÑŒª d
4s¬µ2‚â•œÑŒªd‚à•Œ≥(j,¬∑)‚à•2
2‚àídœÑŒª d
4s¬µ2(3.5)
‚â•3dœÑŒª d
4s¬µ2. (9.23)
Similarly, on ‚Ñ¶‚à© {j‚ààSa,b‚àó(a)}, we have
ÀÜw(j)
a‚â§5dŒª1¬µ2
4œÑs. (9.24)
Furthermore, on ‚Ñ¶‚à© {j‚Ä≤‚ààSa,b‚àó(a)}, it holds that
‚àídœÑŒª d
4s¬µ2‚â§ÀÜw(j‚Ä≤)
a‚â§dœÑŒª d
4s¬µ2. (9.25)
Finally, for all j‚àà[p], ifj /‚ààSa,b‚àó(a), then by the definition of ÀÜva,b‚àó(a);ifori‚àà[d]in Algorithm
1, we know that (ÀÜva,b‚àó(a);i)(j)= 0. Thus, by the definition of ÀÜw(j)
a, it holds that ÀÜw(j)
a= 0for
j /‚ààSa,b‚àó(a). Hence, we have given upper bound and lower bound of ÀÜw(Àúj)
a. Using the lower
bound and upper bound given above, we obtain on ‚Ñ¶that
E( ÀÜw(j)
a‚àíÀÜw(j‚Ä≤)
a|X)
=E[ ÀÜw(j)
a(1{j‚ààSa,b‚àó(a)}+1{j /‚ààSa,b‚àó(a)})|X]‚àíE[ ÀÜw(j‚Ä≤)
a(1{j‚Ä≤‚ààSa,b‚àó(a)}+1{j‚Ä≤/‚ààSa,b‚àó(a)})|X]
=E( ÀÜw(j)
a1{j‚ààSa,b‚àó(a)} ‚àíÀÜw(j‚Ä≤)
a1{j‚Ä≤‚ààSa,b‚àó(a)}|X)
‚â•3qjdœÑŒª d
4s¬µ2‚àíqj‚Ä≤dœÑŒª d
4s¬µ2(9.18)
‚â•qjdœÑŒª d
2s¬µ2(9.20)
‚â•dœÑŒª d
2ps¬µ2. (9.26)
Now we let a,jandj‚Ä≤freely vary again. Define ‚Ñ¶1:={min j‚ààAÀÜw(j)>max j‚ààAcÀÜw(j)}.
LetŒ≥(ÀÜS,¬∑)=Vd(M(ÀÜS,ÀÜS))andbŒ≥(ÀÜS,¬∑)=Vd(cM(ÀÜS,ÀÜS))denote the dleading eigenvectors of M(ÀÜS,ÀÜS)
42andcM(ÀÜS,ÀÜS), respectively. Then, by Assumption (A3) and Theorem 2 of Yu et al. (2015), on
‚Ñ¶‚à©‚Ñ¶1,
L(bŒ≥(ÀÜS,¬∑),Œ≥(ÀÜS,¬∑))‚â§2d1/2‚à•cM(ÀÜS,ÀÜS)‚àíM(ÀÜS,ÀÜS)‚à•op
Œªd‚â§2Ks
dllog(p)
nŒª2
d.
Since l‚â•s, on ‚Ñ¶1, it holds that A ‚äÜ ÀÜS, which implies that Œ≥= (Œ≥(ÀÜS,¬∑),‚ä§,0‚ä§)‚ä§. Let
bŒ≥= (bŒ≥(ÀÜS,¬∑),‚ä§,0‚ä§)‚ä§. We note that the true parameter Œ≤= (Œ≥(ÀÜS,¬∑),‚ä§L(ÀÜS,ÀÜS),‚àí1,0‚ä§)andbŒ≤=
(bŒ≥(ÀÜS,¬∑),‚ä§bL(ÀÜS,ÀÜS),‚àí1,0‚ä§)for some ordering of X. Then, by Assumptions (A1), (A1‚Äô) and (A3),
noting that Kp
llog(p)/n‚â§min{Œªd/(2‚àö
2),‚àöŒ∏1, Œ∏p/(6‚àöŒ∏1)}, on‚Ñ¶we have
L(Œ≤,bŒ≤) =L(Œ≤(ÀÜS,¬∑),bŒ≤(ÀÜS,¬∑)) =‚à•Œ≤(ÀÜS,¬∑)Œ≤(ÀÜS,¬∑),‚ä§‚àíbŒ≤(ÀÜS,¬∑)bŒ≤(ÀÜS,¬∑),‚ä§‚à•F‚â§Cs
dllog(p)
nŒª2
d
for some sufficiently large constant C >0.
It suffices to derivative the lower bound of P(‚Ñ¶‚à©‚Ñ¶1). Observe that ÀÜw(j)=A‚àí1PA
a=1ÀÜw(j)
a
for any j‚àà[p]. Then, for any j‚àà Aandj‚Ä≤‚àà Ac, on‚Ñ¶, it holds that
ÀÜw(j)‚àíÀÜw(j‚Ä≤)
={ÀÜw(j)‚àíE( ÀÜw(j)|X)} ‚àí { ÀÜw(j‚Ä≤)‚àíE( ÀÜw(j‚Ä≤)|X)}+E( ÀÜw(j)‚àíÀÜw(j‚Ä≤)|X)
(9.26)
‚â•dœÑŒª d
2ps¬µ2+{ÀÜw(j)‚àíE( ÀÜw(j)|X)} ‚àí { ÀÜw(j‚Ä≤)‚àíE( ÀÜw(j‚Ä≤)|X)},
which implies
‚Ñ¶c
1‚äÜ[
j‚ààA
ÀÜw(j)‚àíE( ÀÜw(j)|X)‚â§dœÑŒª d
4ps¬µ2
‚à™[
j‚Ä≤/‚ààA
ÀÜw(j‚Ä≤)‚àíE( ÀÜw(j‚Ä≤)|X)‚â•dœÑŒª d
4ps¬µ2
.
Notice that conditioned on X,{ÀÜw(j)
a}a‚àà[A]are i.i.d. random variables, and (9.23), (9.24) and
(9.25) implies that ÀÜw(j)
ais bounded on ‚Ñ¶for all j‚àà[p]. Thus, by the union bound and the
Hoeffding‚Äôs inequality conditioned on X, on‚Ñ¶we obtain that
P(‚Ñ¶c
1|X)‚â§pexp
‚àíAœÑ4Œª2
d
50p2¬µ8Œª2
1
.
43We complete the proof by noting that
P(‚Ñ¶‚à©‚Ñ¶1)‚â•1‚àíc‚Ä≤p‚àíc2‚àípexp
‚àíAœÑ4Œª2
d
50p2¬µ8Œª2
1
.
‚ñ°
9.3 Proof of Theorem 3
Consider a specific case where d= 1andH= 2. In this case, the parameter of interest
Œ≤‚ààŒòp,d,s(3)reduces to a 1-dimensional vector Œ≤1‚ààŒòp,1,s(3). We consider the following
structure:
X|(ÀúY= 1)‚àº N p((1‚àíŒ±)Œ≤1,Ip‚àíM),P(ÀúY= 1) = Œ± ,
X|(ÀúY= 2)‚àº N p(‚àíŒ±Œ≤1,Ip‚àíM),P(ÀúY= 2) = 1 ‚àíŒ± , (9.27)
where Œ±‚àà(0,1)andMis a proper positive definite matrix to be defined later. Under this
structure, we have the following three results:
(i)EX=E{E(X|ÀúY)}=0;
(ii) Define ¬µh=E{X|(ÀúY=h)}andph=P(ÀúY=h)forh‚àà[H]. Then Cov{E(X|ÀúY)}=
PH
h=1ph¬µh¬µ‚ä§
h‚àí(PH
h=1ph¬µh)(PH
h=1ph¬µh)‚ä§=PH
h=1ph¬µh¬µ‚ä§
h=Œ±(1‚àíŒ±)Œ≤1Œ≤‚ä§
1;
(iii) Therefore, Cov(X) =E(XX‚ä§) = Cov {E(X|ÀúY)}+E{Cov(X|ÀúY)}=Œ±(1‚àíŒ±)Œ≤1Œ≤‚ä§
1+
Ip‚àíM.
Letting M= Cov {E(X|ÀúY)}=Œ±(1‚àíŒ±)Œ≤1Œ≤‚ä§
1, we have Cov(X) =Ip. Hence, the
nonzero generalized eigenvalues of the pair (Cov{E(X|ÀúY)},Cov(X))reduce to the nonzero
eigenvaluesof Cov{E(X|ÀúY)}. Recallthat Cov{E(X|ÀúY)}=Œ±(1‚àíŒ±)Œ≤1Œ≤‚ä§
1,whichimpliesthat
Cov{E(X|ÀúY)}has only one nonzero eigenvalue Œª1=Œ±(1‚àíŒ±)and the leading eigenvector,
which is the parameter of our interest, corresponds to Œ≤1.
We will use the Fano‚Äôs lemma (Yu, 1997) to derive the lower bound. To apply this lemma,
we go in three steps: (i) to construct some Œ≤‚ààŒòp,1,s(3); (ii) to derive the Kullback-Leiber
44divergence between data distributions of interest, i.e., the constructed mixture Gaussian
distribution; (iii) to find a subset of Œòp,1,s(3)with a proper packing number.
Proof of Step (i).We consider a specific setting of Gataric et al. (2020) (Proposition 1)
where the parameter of interest is now a vector, not a matrix. Define u= (s‚àí1/21‚ä§
s,0‚ä§)‚ä§‚àà
Rp, and let œµ‚àà 
0,p
1/(16s)
. For any J‚ààŒòp‚àí1,1,s‚àí1:={V‚ààŒòp‚àí1,1: supp( V)‚â§s‚àí1},
sinceu‚ä§u= 1, we can select eU‚ààŒòp,psuch that the first column of eUisuand the indexes
of the nonzero components of Œ≤J=eU(‚àö
1‚àíœµ2, œµJ‚ä§)‚ä§are a subset of [s]. Notice that
Œ≤J=eUÔ£´
Ô£≠‚àö
1‚àíœµ2
œµJÔ£∂
Ô£∏=u+eUÔ£´
Ô£≠‚àö
1‚àíœµ2‚àí1
œµJÔ£∂
Ô£∏=:u+eU‚àÜJ.
Due to ‚à•eU‚àÜJ‚à•‚àû‚â§ ‚à•eU‚àÜJ‚à•2‚â§‚àö
2œµandœµ‚â§p
1/(16s),|Œ≤(k)
J| ‚àà[0.64/‚àös,1.36/‚àös]for any
k‚àà[s], which implies that Œ≤J‚ààŒòp,1,s(3).
Proof of Step (ii).Forany J, J‚Ä≤‚ààŒòp‚àí1,1,s‚àí1, letMJ=Œª1Œ≤JŒ≤‚ä§
JandMJ‚Ä≤=Œª1Œ≤J‚Ä≤Œ≤‚ä§
J‚Ä≤, and
then denote by P(Œ≤J,MJ),P(Œ≤J‚Ä≤,MJ‚Ä≤)the corresponding mixture Gaussian distributions
specified in (9.27). Write DKL(P‚à•Q)for the KL divergence from a distribution PtoQ. By
the convexity of KL divergence, we have
DKL 
P(Œ≤J,MJ)‚à•P(Œ≤J‚Ä≤,MJ‚Ä≤)
‚â§Œ±DKL 
Np((1‚àíŒ±)Œ≤J,Ip‚àíMJ)‚à•Np((1‚àíŒ±)Œ≤J‚Ä≤,Ip‚àíMJ‚Ä≤)
(9.28)
+ (1‚àíŒ±)DKL 
Np(‚àíŒ±Œ≤J,Ip‚àíMJ)‚à•Np(‚àíŒ±Œ≤J‚Ä≤,Ip‚àíMJ‚Ä≤)
.(9.29)
Hence, it suffices to bound the KL divergence between two Gaussian distributions. For the
first KL divergence in (9.28), we have
DKL 
Np((1‚àíŒ±)Œ≤J,Ip‚àíMJ)‚à•Np((1‚àíŒ±)Œ≤J‚Ä≤,Ip‚àíMJ‚Ä≤)
=1
2
[Tr{(Ip‚àíMJ‚Ä≤)‚àí1(Ip‚àíMJ)} ‚àíp]| {z }
T1+ logdet(Ip‚àíMJ‚Ä≤)
det(Ip‚àíMJ)
| {z }
T2
+ (1‚àíŒ±)2(Œ≤J‚Ä≤‚àíŒ≤J)‚ä§(Ip‚àíMJ‚Ä≤)‚àí1(Œ≤J‚Ä≤‚àíŒ≤J)| {z }
T3
.
45For the first term, it holds that
T1= Tr{(Ip‚àíMJ‚Ä≤)‚àí1(MJ‚Ä≤‚àíMJ)}
= Tr
Ip+Œª1
1‚àíŒª1Œ≤J‚Ä≤Œ≤‚ä§
J‚Ä≤
(Œª1Œ≤J‚Ä≤Œ≤‚ä§
J‚Ä≤‚àíŒª1Œ≤JŒ≤‚ä§
J)
=Œª2
1
1‚àíŒª1{1‚àíTr(Œ≤J‚Ä≤Œ≤‚ä§
J‚Ä≤Œ≤JŒ≤‚ä§
J)}
‚â§2Œª2
1
1‚àíŒª1‚à•Œ≤J‚àíŒ≤J‚Ä≤‚à•2
2.
For the term T2, noting that Ip‚àíMJandIp‚àíMJ‚Ä≤have the same eigenvalues, we then
obtain T2= 0. For the term T3, we have T3‚â§(1‚àíŒ±)2Œªmax{(Ip‚àíMJ‚Ä≤)‚àí1}‚à•Œ≤J‚àíŒ≤J‚Ä≤‚à•2
2=
(1‚àíŒ±)2(1‚àíŒª1)‚àí1‚à•Œ≤J‚àíŒ≤J‚Ä≤‚à•2
2. Combining the upper bounds for the three terms, we finally
obtain
DKL 
Np((1‚àíŒ±)Œ≤J,Ip‚àíMJ)‚à•Np((1‚àíŒ±)Œ≤J‚Ä≤,Ip‚àíMJ‚Ä≤)
‚â§2Œª2
1+ (1‚àíŒ±)2
2(1‚àíŒª1)‚à•Œ≤J‚àíŒ≤J‚Ä≤‚à•2
2.
(9.30)
The second KL divergence in (9.29) has a similar upper bound
DKL 
Np(‚àíŒ±Œ≤J,Ip‚àíMJ)‚à•Np(‚àíŒ±Œ≤J‚Ä≤,Ip‚àíMJ‚Ä≤)
‚â§2Œª2
1+Œ±2
2(1‚àíŒª1)‚à•Œ≤J‚àíŒ≤J‚Ä≤‚à•2
2.(9.31)
Combining (9.28)-(9.31), we have
DKL 
P(Œ≤J,MJ)‚à•P(Œ≤J‚Ä≤,MJ‚Ä≤)
‚â§2Œª2
1+Œª1
2(1‚àíŒª1)‚à•Œ≤J‚àíŒ≤J‚Ä≤‚à•2
2=2Œª2
1+Œª1
1‚àíŒª1(1‚àíŒ≤‚ä§
JŒ≤J‚Ä≤).
Recall Œ≤J=eU(‚àö
1‚àíœµ2, œµJ‚ä§)‚ä§for any J‚ààŒòp‚àí1,1,s‚àí1. Then Œ≤JŒ≤‚ä§
J‚Ä≤= 1‚àíœµ2+œµ2J‚ä§J‚Ä≤‚â•
1‚àíœµ2‚àíœµ2‚à•J‚à•2‚à•J‚Ä≤‚à•2= 1‚àí2œµ2. Hence,
DKL 
P(Œ≤J,MJ)‚à•P(Œ≤J‚Ä≤,MJ‚Ä≤)
‚â§2œµ2(2Œª2
1+Œª1)
1‚àíŒª1. (9.32)
Proof of Step (iii).LetJ ‚äÇ Œòp‚àí1,1,s‚àí1such that min J,J‚Ä≤‚ààJL(Œ≤J,Œ≤J‚Ä≤)‚â•cœµfor some
universal constant c >0and3‚â§ |J | ‚â§ exp{n/(4s)}, which will be specified later. Let ÀúŒ≤be
46any possible sparse SIR estimator for Œ≤. Then by (9.32) and Fano‚Äôs lemma (Yu, 1997), we
obtain
inf
ÀúŒ≤sup
Œ≤‚ààŒòp‚àí1,1,s‚àí1(3)EPŒ≤{L(ÀúŒ≤,Œ≤)} ‚â•inf
ÀúŒ≤max
J‚ààJEPŒ≤J{L(ÀúŒ≤,Œ≤J)}
‚â•cœµ
2
1‚àí2nœµ2(2Œª2
1+Œª1)/(1‚àíŒª1) + log 2
log|J |
‚â•cœµ
21
3‚àínœµ2
log|J |
, (9.33)
where we employed the fact that |J | ‚â• 3andŒª1=Œ±(1‚àíŒ±)‚â§1/4. Noting that log|J | ‚â§
n/(4s), we can select œµ2= log( |J |)/(4n)‚â§1/(16s). Combining (9.33), we obtain
inf
ÀúŒ≤sup
Œ≤‚ààŒòp,1,s(3)EPŒ≤{L(ÀúŒ≤,Œ≤)}‚â≥r
log|J |
n. (9.34)
It remains to construct J. LetS={S‚àà[p‚àí1] :|S|=s‚àí1}. For any S‚àà S, define
JS‚ààRp‚àí1such that J(S)
S= (s‚àí1)‚àí1/21s‚àí1andJ(Sc)
S=0. Then JS‚ààŒòp‚àí1,1,s‚àí1. By the
Varshamov-Gilbert‚Äôs lemma (Lemma 4.10 in Massart (2007)), since 4(s‚àí1)‚â§p‚àí1, there
exists a subset TofSsuch that
log|T |=1
5(s‚àí1) logp‚àí1
s‚àí1
(9.35)
and|T‚à©T‚Ä≤| ‚â§(s‚àí1)/2for any T, T‚Ä≤‚àà Twith TÃ∏=T‚Ä≤. LetJ={JT:T‚àà T }, and then
|J |=|T |. Moreover, the fact that |T‚à©T‚Ä≤| ‚â§(s‚àí1)/2for any T, T‚Ä≤‚àà Twith TÃ∏=T‚Ä≤
implies that
min
J,J‚Ä≤‚ààJ:JÃ∏=J‚Ä≤L(J, J‚Ä≤) = min
J,J‚Ä≤‚ààJ:JÃ∏=J‚Ä≤p
1‚àí(J‚ä§J‚Ä≤)2‚â•s
1‚àí1
22
‚â•‚àö
3
2. (9.36)
Notice that
L(Œ≤J,Œ≤J‚Ä≤) =1‚àö
2‚à•Œ≤JŒ≤‚ä§
J‚àíŒ≤J‚Ä≤Œ≤‚ä§
J‚Ä≤‚à•F=q
œµ4L2(J, J‚Ä≤) +œµ2(1‚àíœµ2)‚à•J‚àíJ‚Ä≤‚à•2
2‚â•œµL(J, J‚Ä≤)
47for any J‚ààŒòp‚àí1,1,s‚àí1. Combining (9.36), we have
min
J,J‚Ä≤‚ààJ:JÃ∏=J‚Ä≤L(Œ≤J,Œ≤J‚Ä≤)‚â•‚àö
3
2œµ .
Finally, since (s‚àí1) log{(p‚àí1)/(s‚àí1)} ‚â•6and5n‚â•4s(s‚àí1) log{(p‚àí1)/(s‚àí1)}, we
have 3‚â§ |J | ‚â§ exp{n/(4s)}. Therefore, (9.34) and (9.35) implies that
inf
ÀúŒ≤sup
Œ≤‚ààŒòp,1,s(3)EPŒ≤{L(ÀúŒ≤,Œ≤)}‚â≥r
slog(p/s)
n.
We complete the proof. ‚ñ°
9.4 Proof of auxiliary lemmas
9.4.1 Proof of Lemma 1
Without loss of generality, we assume EX=0. Then, by definition
Œ£:=Œ£E(X|Y)= Cov {E(X|ÀúY)}=E[{E(X|ÀúY)}{E(X|ÀúY)}‚ä§]
=HX
h=1P(ÀúY=h){E(X|ÀúY=h)}{E(X|ÀúY=h)}‚ä§.
Notice that
E[X1{ÀúY=h}] =E(E[X1{ÀúY=h}|1{ÀúY=h}])
=E(1{ÀúY=h}E[X|1{ÀúY=h}])
=P[1{ÀúY=h}= 1]¬∑E(X|1{ÀúY=h}= 1)
=P(ÀúY=h)]¬∑E(X|ÀúY=h)
=:ph¬∑E(X|ÀúY=h),
which implies that
Œ£=HX
h=1E[X1{ÀúY=h}](E[X1{ÀúY=h}])‚ä§
ph=:HX
h=1uhu‚ä§
h
ph.
48By definition, we also have
bŒ£:=bŒ£E(X|Y)=HX
h=1ÀÜuhÀÜu‚ä§
h
ÀÜph,
where ÀÜph=n‚àí1Pn
i=11{ÀúYi=h}andÀÜuh=n‚àí1Pn
i=1Xi1{ÀúYi=h}. Therefore, we obtain
bŒ£‚àíŒ£=HX
h=1ÀÜuhÀÜu‚ä§
h
ÀÜph‚àíuhu‚ä§
h
ph
=HX
h=1ÀÜuhÀÜu‚ä§
h
ÀÜph‚àíuhu‚ä§
h
ÀÜph+uhu‚ä§
h
ÀÜph‚àíuhu‚ä§
h
ph
=HX
h=1{ÀÜp‚àí1
h(ÀÜuhÀÜu‚ä§
h‚àíuhu‚ä§
h) + (ÀÜp‚àí1
h‚àíp‚àí1
h)uhu‚ä§
h}
=HX
h=1{p‚àí1
h(ÀÜuhÀÜu‚ä§
h‚àíuhu‚ä§
h) + (ÀÜp‚àí1
h‚àíp‚àí1
h)(ÀÜuhÀÜu‚ä§
h‚àíuhu‚ä§
h) + (ÀÜp‚àí1
h‚àíp‚àí1
h)uhu‚ä§
h}.
For any unit vector Œ±‚ààRp, it holds that
Œ±‚ä§(bŒ£‚àíŒ£)Œ±
=HX
h=1{p‚àí1
h|Œ±‚ä§(ÀÜuh‚àíuh)|2+ 2p‚àí1
hŒ±‚ä§uh(ÀÜuh‚àíuh)‚ä§Œ±+ (ÀÜp‚àí1
h‚àíp‚àí1
h)|Œ±‚ä§(ÀÜuh‚àíuh)|2
+ 2(ÀÜp‚àí1
h‚àíp‚àí1
h)Œ±‚ä§uh(ÀÜuh‚àíuh)‚ä§Œ±+ (ÀÜp‚àí1
h‚àíp‚àí1
h)|Œ±‚ä§uh|2}. (9.37)
Now we claim that ‚à•uh‚à•2can be bounded by a constant for all h‚àà[H]. By definition,
Œ£=HX
h=1uhu‚ä§
h
ph=Œ≤ŒõŒ≤‚ä§,
where Œ≤= (Œ≤1, . . . ,Œ≤d)‚ä§andŒõ= diag {Œª1, . . . , Œª d}. Taking trace on both sides, we obtain
PH
h=1p‚àí1
h‚à•uh‚à•2
2=Pd
i=1Œªi‚â§Cby Assumption (A1), where the constant C >0depends on
d,Œ∫andŒª. Observing that 0< ph<1, (9.37) leads to
|Œ±‚ä§(bŒ£‚àíŒ£)Œ±|
49‚â§HX
h=1p‚àí1
h|Œ±‚ä§(ÀÜuh‚àíuh)|2+ 2HX
h=1p‚àí1
h|(ÀÜuh‚àíuh)‚ä§Œ±| ¬∑ |uh|2+HX
h=1|ÀÜp‚àí1
h‚àíp‚àí1
h| ¬∑ |Œ±‚ä§(ÀÜuh‚àíuh)|2
+ 2HX
h=1|ÀÜp‚àí1
h‚àíp‚àí1
h| ¬∑ |(ÀÜuh‚àíuh)‚ä§Œ±| ¬∑ |uh|2+HX
h=1|ÀÜp‚àí1
h‚àíp‚àí1
h| ¬∑ |uh|2
2
‚â≤HX
h=1|Œ±‚ä§(ÀÜuh‚àíuh)|2+HX
h=1|ÀÜp‚àí1
h‚àíp‚àí1
h| ¬∑ |Œ±‚ä§(ÀÜuh‚àíuh)|2
+HX
h=1|(ÀÜuh‚àíuh)‚ä§Œ±|+HX
h=1|ÀÜp‚àí1
h‚àíp‚àí1
h| ¬∑ |(ÀÜuh‚àíuh)‚ä§Œ±|+HX
h=1|ÀÜp‚àí1
h‚àíp‚àí1
h|. (9.38)
Consider the term Œ±‚ä§(ÀÜuh‚àíuh). Let Zi(h) =Œ±‚ä§Xi1{ÀúYi=h} ‚àíE[Œ±‚ä§Xi1{ÀúYi=h}]for
any unit vector Œ±‚ààRpandh‚àà[H]. Then, Œ±‚ä§(ÀÜuh‚àíuh) =n‚àí1Pn
i=1Zi(h). Notice that
Xiis mixture-Gaussian distributed, which implies that {Zi(h)}n
1=1are i.i.d. sub-exponential
variables. Because {Xi,ÀúYi}are i.i.d. random variables and Zi(h)are functions of Xiand
ÀúYi, we know that {Zi(h)}n
1=1are also i.i.d. random variables. By the Bernstein inequality
(Vershynin, 2010), we have
P{|Œ±‚ä§(ÀÜuh‚àíuh)| ‚â•t} ‚â§2 exp{‚àíCn(t2‚àßt)}. (9.39)
For the term ÀÜp‚àí1
h‚àíp‚àí1
h, it is easy to check that ÀÜph‚àíph=n‚àí1Pn
i=11{ÀúYi=h}‚àíE[1{ÀúYi=
h}],|1{ÀúYi=h} ‚àíE[1{ÀúYi=h}]| ‚â§1andVar(1{ÀúYi=h} ‚àíE[1{ÀúYi=h}])‚â§1/4. By
the Bernstein inequality (van der Vaart and Wellner, 1996), we have P(|ÀÜph‚àíph| ‚â•t)‚â§
2 exp{‚àíCn(t2‚àßt)}. Let pmin= min {p1, . . . , p H}. Due to |ÀÜp‚àí1
h‚àíp‚àí1
h|=|ÀÜph‚àíph|/(phÀÜph), it
holds that
P(|ÀÜp‚àí1
h‚àíp‚àí1
h| ‚â•2p‚àí2
mint)
‚â§P(|ÀÜph‚àíph| ‚â•t) +P(phÀÜph‚â§p2
min/2)
‚â§P(|ÀÜph‚àíph| ‚â•t) +P(|ÀÜph‚àíph| ‚â•pmin/2)
‚â§4 exp{‚àíCn(t2‚àßt)} (9.40)
provided that pmin‚â•2t.
50Combing (9.38)-(9.40) and using the union bound, we have
P(|Œ±‚ä§(bŒ£‚àíŒ£)Œ±| ‚â•t)‚â≤Cexp{‚àíC‚Ä≤n(t2‚àßt1/4)}.
By Lemma 2 of Wang et al. (2016b), if Œ∑ >0satisfies klog(p/Œ∑)/n‚â§1, it then holds that
P
sup
u‚ààBp‚àí1
0(k)|Œ±‚ä§(bŒ£‚àíŒ£)Œ±| ‚â•2Cr
klog(p/Œ∑)
n
‚â≤k1/2p
k128‚àö
255k‚àí1
P
|Œ±‚ä§(bŒ£‚àíŒ£)Œ±| ‚â•Cr
klog(p/Œ∑)
n
‚â≤k1/2p
k128‚àö
255k‚àí1
exp{‚àíC‚Ä≤C2klog(p/Œ∑)}
‚â≤k1/2p
k128‚àö
255k‚àí1
p‚àíkŒ∑k(choose C2= (C‚Ä≤)‚àí1)
‚â≤Œ∑ .
Similarly, if klog(p/Œ∑)/n > 1, we have
P
sup
u‚ààBp‚àí1
0(k)|Œ±‚ä§(bŒ£‚àíŒ£)Œ±| ‚â•2Cklog(p/Œ∑)
n4
‚â≤Œ∑ .
Combining the above two equations, we obtain
P
sup
u‚ààBp‚àí1
0(k)|Œ±‚ä§(bŒ£‚àíŒ£)Œ±| ‚â•2Cmaxklog(p/Œ∑)
n4
,r
klog(p/Œ∑)
n
‚â§C‚Ä≤Œ∑ .
Choose Œ∑=p‚àí3. Noting that klogp‚â™nand (9.40) holds for sufficiently large n, we
complete the proof. ‚ñ°
9.4.2 Proof of Lemma 2
Without loss of generality, write
Œ£=Ô£´
Ô£≠Œ£(S,S)Œ£(S,Sc)
Œ£(Sc,S)Œ£(Sc,Sc)Ô£∂
Ô£∏andL=Ô£´
Ô£≠L(S,S)0
L(Sc,S)L(Sc,Sc)Ô£∂
Ô£∏.
51Then,
Ô£´
Ô£≠Œ£(S,S)Œ£(S,Sc)
Œ£(Sc,S)Œ£(Sc,Sc)Ô£∂
Ô£∏=Ô£´
Ô£≠L(S,S)0
L(Sc,S)L(Sc,Sc)Ô£∂
Ô£∏Ô£´
Ô£≠L(S,S),‚ä§L(Sc,S),‚ä§
0‚ä§L(Sc,Sc),‚ä§Ô£∂
Ô£∏=Ô£´
Ô£≠L(S,S)L(S,S),‚ä§‚àó
‚àó ‚àóÔ£∂
Ô£∏,
which implies that Œ£(S,S)=L(S,S)L(S,S),‚ä§. Since Œ£(S,S)=L(S,S)L‚ä§
(S,S)andL(S,S)is unique,
we obtain L(S,S)=L(S,S).
By the inverse of a block matrix, it holds that
L‚àí1Œ£E(X|Y)(L‚àí1)‚ä§=Ô£´
Ô£≠L(S,S)0
L(Sc,S)L(Sc,Sc)Ô£∂
Ô£∏‚àí1Ô£´
Ô£≠Œ£(S,S)
E(X|Y)Œ£(S,Sc)
E(X|Y)
Œ£(Sc,S)
E(X|Y)Œ£(Sc,Sc)
E(X|Y)Ô£∂
Ô£∏Ô£´
Ô£≠L(S,S)0
L(Sc,S)L(Sc,Sc)Ô£∂
Ô£∏‚àí1,‚ä§
=Ô£´
Ô£≠L(S,S),‚àí10
‚àíL(Sc,Sc),‚àí1L(Sc,S)L(S,S),‚àí1L(Sc,Sc),‚àí1Ô£∂
Ô£∏Ô£´
Ô£≠Œ£(S,S)
E(X|Y)Œ£(S,Sc)
E(X|Y)
Œ£(Sc,S)
E(X|Y)Œ£(Sc,Sc)
E(X|Y)Ô£∂
Ô£∏
Ô£´
Ô£≠L(S,S),‚àí1,‚ä§‚àí(L(Sc,Sc),‚àí1L(Sc,S)L(S,S),‚àí1)‚ä§
0‚ä§L(Sc,Sc),‚àí1,‚ä§Ô£∂
Ô£∏
=Ô£´
Ô£≠L(S,S),‚àí1Œ£(S,S)
E(X|Y)L(S,S),‚àí1Œ£(S,Sc)
E(X|Y)
‚àó ‚àóÔ£∂
Ô£∏Ô£´
Ô£≠L(S,S),‚àí1,‚ä§‚àó
0‚ä§‚àóÔ£∂
Ô£∏
=Ô£´
Ô£≠L(S,S),‚àí1Œ£(S,S)
E(X|Y)L(S,S),‚àí1,‚ä§‚àó
‚àó ‚àóÔ£∂
Ô£∏.
Clearly, {L‚àí1Œ£E(X|Y)(L‚àí1)‚ä§}(S,S)=L(S,S),‚àí1Œ£(S,S)
E(X|Y)L(S,S),‚àí1,‚ä§. Combining L(S,S)=L(S,S),
we have {L‚àí1Œ£E(X|Y)(L‚àí1)‚ä§}(S,S)=L‚àí1
(S,S)Œ£(S,S)
E(X|Y)(L‚àí1
(S,S))‚ä§=M(S,S). We complete the
proof. ‚ñ°
References
Anaraki, F. P., and Hughes, S. (2014), ‚ÄúMemory and computation efficient PCA via very
sparse random projections,‚Äù in International Conference on Machine Learning , PMLR,
pp. 1341‚Äì1349.
Bondell, H. D., and Li, L. (2009), ‚ÄúShrinkage inverse regression estimation for model-free
variable selection,‚Äù Journal of the Royal Statistical Society: Series B (Statistical Method-
ology), 71, 287‚Äì299.
52Bura, E., and Cook, R. D. (2001), ‚ÄúExtending sliced inverse regression: The weighted chi-
squared test,‚Äù Journal of the American Statistical Association , 96, 996‚Äì1003.
Cai, T.T., Ma, Z., andWu, Y.(2013), ‚ÄúSparsePCA:Optimalratesandadaptiveestimation,‚Äù
The Annals of Statistics , 41, 3074‚Äì3110.
Chen, X., Zou, C., and Cook, R. (2010), ‚ÄúCoordinate-Independent Sparse Sufficient Dimen-
sion Reduction and Variable Selection,‚Äù The Annals of Statistics , 38, 3696‚Äì3723.
Cook,R.(1996),‚ÄúGraphicsforRegressionswithaBinaryResponse,‚Äù Journal of the American
Statistical Association , 91, 983‚Äì992.
Cook, R. D. (2004), ‚ÄúTesting predictor contributions in sufficient dimension reduction,‚Äù The
Annals of Statistics , 32, 1062‚Äì1092.
‚Äî (2007), ‚ÄúFisher lecture: Dimension reduction in regression,‚Äù Statistical Science , 22, 1‚Äì26.
Cook, R. D., and Forzani, L. (2009), ‚ÄúLikelihood-based sufficient dimension reduction,‚Äù Jour-
nal of the American Statistical Association , 104, 197‚Äì208.
Cook, R. D., and Weisberg, S. (1991), ‚ÄúDiscussion of sliced inverse regression for dimension
reduction,‚Äù Journal of the American Statistical Association , 86, 328‚Äì332.
Cook, R. D., and Yin, X. (2001), ‚ÄúTheory & methods: special invited paper: dimension
reduction and visualization in discriminant analysis (with discussion),‚Äù Australian & New
Zealand Journal of Statistics , 43, 147‚Äì199.
Fan, J., Shao, Q.-M., and Zhou, W.-X. (2018), ‚ÄúAre discoveries spurious? Distributions of
maximum spurious correlations and their applications,‚Äù Annals of statistics , 46, 989.
Gao, C., Ma, Z., Ren, Z., and Zhou, H. H. (2015), ‚ÄúMinimax estimation in sparse canonical
correlation analysis,‚Äù The Annals of Statistics , 43, 2168‚Äì2197.
Gao, C., Ma, Z.‚Äî (2017), ‚ÄúSparse CCA: Adaptive Estimation and Computational Barriers,‚Äù
The Annals of Statistics , 45, 2074‚Äì2101.
Gataric, M., Wang, T., and Samworth, R. J. (2020), ‚ÄúSparse Principal Component Analysis
via Axis-Aligned Random Projections,‚Äù Journal of the Royal Statistical Society: Series B
(Statistical Methodology) , 82, 329‚Äì359.
Hsing, T., and Carroll, R. J. (1992), ‚ÄúAn asymptotic theory for sliced inverse regression,‚Äù
The Annals of Statistics , 1040‚Äì1061.
Hung, H., and Huang, S.-Y. (2019), ‚ÄúSufficient dimension reduction via random-partitions
for the large-p-small-n problem,‚Äù Biometrics , 75, 245‚Äì255.
Li, B., and Wang, S. (2007), ‚ÄúOn Directional Regression for Dimension Reduction,‚Äù Journal
of the American Statistical Association , 102, 997‚Äì1008.
53Li, K.(1991), ‚ÄúSlicedInverseRegressionforDimensionReduction,‚Äù (withdiscussion) Journal
of the American Statistical Association , 86, 316‚Äì327.
Li, K.-C. (1992), ‚ÄúOn principal Hessian directions for data visualization and dimension re-
duction: Another application of Stein‚Äôs lemma,‚Äù Journal of the American Statistical As-
sociation , 87, 1025‚Äì1039.
Li, L. (2007), ‚ÄúSparse sufficient dimension reduction,‚Äù Biometrika , 94, 603‚Äì613.
Li, L., Dennis Cook, R., and Nachtsheim, C. J. (2005), ‚ÄúModel-free variable selection,‚Äù
Journal of the Royal Statistical Society: Series B (Statistical Methodology) , 67, 285‚Äì299.
Li, L., Wen, X. M., and Yu, Z. (2020), ‚ÄúA Selective Overview of Sparse Sufficient Dimension
Reduction,‚Äù (with discussion) Statistical Theory and Related Fields , 4, 121‚Äì133.
Lin, Q., Li, X., Huang, D., and Liu, J. S. (2017), ‚ÄúOn the optimality of sliced inverse
regression in high dimensions,‚Äù arXiv preprint arXiv:1701.06009 .
Lin, Q., Zhao, Z., and Liu, J. (2018), ‚ÄúOn Consistency and Sparsity for Sliced Inverse Re-
gression in High Dimensions,‚Äù The Annals of Statistics , 46, 580‚Äì610.
Lin, Q., Zhao, Z., and Liu, J. S. (2019), ‚ÄúSparse Sliced Inverse Regression via Lasso,‚Äù Journal
of the American Statistical Association , 114, 1726‚Äì1739.
Liu, C., Zhao, X., and Huang, J. (2023), ‚ÄúA Random Projection Approach to Hypothe-
sis Tests in High-Dimensional Single-Index Models,‚Äù Journal of the American Statistical
Association , 1‚Äì11.
Ma, Y., and Zhu, L. (2013), ‚ÄúA review on dimension reduction,‚Äù International Statistical
Review, 81, 134‚Äì150.
Massart, P. (2007), Concentration inequalities and model selection: Ecole d‚ÄôEt√© de Probabil-
it√©s de Saint-Flour XXXIII-2003 , Springer.
Qi, H., and Hughes, S. M. (2012), ‚ÄúInvariance of principal components under low-dimensional
random projection of the data,‚Äù in 2012 19th IEEE International Conference on Image
Processing , IEEE, pp. 937‚Äì940.
Tan, K., Shi, L., and Yu, Z. (2020), ‚ÄúSparse SIR: Optimal Rates and Adaptive Estimation,‚Äù
The Annals of Statistics , 48, 64‚Äì85.
Tan, K. M., Wang, Z., Liu, H., and Zhang, T. (2018), ‚ÄúSparse generalized eigenvalue prob-
lem,‚ÄùJournal of the Royal Statistical Society. Series B (Statistical Methodology) , 80, 1057‚Äì
1086.
Thorgeirsson, T. E., Gudbjartsson, D. F., Surakka, I., Vink, J. M., Amin, N., Geller, F.,
Sulem, P., Rafnar, T., Esko, T., Walter, S. et al. (2010), ‚ÄúSequence variants at CHRNB3‚Äì
CHRNA6 and CYP2A6 affect smoking behavior,‚Äù Nature genetics , 42, 448‚Äì453.
54Thorisson, G. A., Smith, A. V., Krishnan, L., and Stein, L. D. (2005), ‚ÄúThe international
HapMap project web site,‚Äù Genome research , 15, 1592‚Äì1593.
van der Vaart, A. W., and Wellner, J. (1996), Weak convergence and empirical processes:
with applications to statistics , Springer Science & Business Media.
Vershynin, R. (2010), ‚ÄúIntroduction to the non-asymptotic analysis of random matrices,‚Äù
arXiv preprint arXiv:1011.3027 .
Wang, T., Berthet, Q., and Samworth, R. J. (2016a), ‚ÄúStatistical and computational trade-
offsinestimationofsparseprincipalcomponents,‚Äù The Annals of Statistics , 44, 1896‚Äì1930.
Wang, T., Berthet, Q.‚Äî (2016b), ‚ÄúStatistical and computational trade-offs in estimation of
sparse principal components,‚Äù The Annals of Statistics , 44, 1896‚Äì1930.
Yin, X. (2011), ‚ÄúSufficient dimension reduction in regression,‚Äù in High-dimensional Data
Analysis, World Scientific, pp. 257‚Äì273.
Yu, B. (1997), ‚ÄúAssouad, fano, and le cam,‚Äù in Festschrift for Lucien Le Cam , Springer, pp.
423‚Äì435.
Yu, Y., Wang, T., andSamworth, R.J.(2015), ‚ÄúAusefulvariantoftheDavis‚ÄìKahantheorem
for statisticians,‚Äù Biometrika , 102, 315‚Äì323.
Yu, Z., Dong, Y., and Shao, J. (2016), ‚ÄúOn marginal sliced inverse regression for ultrahigh
dimensional model-free feature selection,‚Äù The Annals of Statistics , 44, 2594‚Äì2623.
Zhu, L., Miao, B., and Peng, H. (2006), ‚ÄúOn sliced inverse regression with high-dimensional
covariates,‚Äù Journal of the American Statistical Association , 101, 630‚Äì643.
Zhu, L.-X., and Ng, K. W. (1995), ‚ÄúAsymptotics of sliced inverse regression,‚Äù Statistica
Sinica, 727‚Äì736.
55