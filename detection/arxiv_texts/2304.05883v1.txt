arXiv:2304.05883v1  [cs.DS]  12 Apr 2023On Parallel k-Center Clustering
Sam Coy∗
University of WarwickArtur Czumaj†
University of WarwickGopinath Mishra‡
University of Warwick
Abstract
Weconsider theclassic k-centerproblem in aparallel setting, on thelow-local-spa ce Massively Parallel
Computation (MPC) model, with local space per machine of O(nδ), where δ∈(0,1) is an arbitrary
constant. As a central clustering problem, the k-center problem has been studied extensively. Still,
until very recently, all parallel MPC algorithms have been r equiring Ω( k) or even Ω( knδ) local space per
machine. While this setting covers the case of small values o fk, for a large number of clusters these
algorithms require large local memory, making them poorly s calable. The case of large k,k≥Ω(nδ),
has been considered recently for the low-local-space MPC mo del by Bateni et al. (2021), who gave
anO(loglogn)-round MPCalgorithm that produces k(1 +o(1)) centers whose cost has multiplicative
approximation of O(logloglog n). In this paper we extend the algorithm of Bateni et al. and de sign a
low-local-space MPC algorithm that in O(loglogn) rounds returns a clustering with k(1+o(1)) clusters
that is an O(log∗n)-approximation for k-center.
1 Introduction
Clustering large data is a fundamental primitive extensively studied b ecause of its numerous applications
in a variety of areas of computer science and data science. It is a ce ntral type of problem in modern data
analysis, including the ﬁelds of data mining, pattern recognition, mac hine learning, networking and social
networks, and bioinformatics. In a typical clustering problem, the goal is to partition the input data into
subsets (called clusters) such that the points assigned to the sam e cluster are “similar” to one another, and
data points assigned to diﬀerent clusters are “dissimilar”.
The most extensively studied clustering problems are k-means,k-median,k-center, various notions of
hierarchical clustering, and also variants of these problems with so me additional constraints (e.g., fairness
or balance).
While originally the clustering problems have been studied in the contex t of classical sequential computation,
most recently a large amount of research has been devoted to the non-sequential computational settings such
as distributed and parallel computing, mainly because these are the only settings capable of performing
computations in a reasonable time on large inputs, and because data is frequently collected on diﬀerent sites
and clustering needs to be performed in a distributed manner with low communication.
In this paper we consider one of the most fundamental clustering p roblems, the k-center problem, on the
Massively Parallel Computation (MPC) model.MPC is a modern theoretical model of parallel computation,
∗E-mail: S.Coy@warwick.ac.uk. Department ofComputer Scie nce, UniversityofWarwick, UK.Research supported inpartb y
the Centre for Discrete Mathematics and its Applications (D IMAP), by an EPSRC studentship, and by the Simons Foundation
Award No. 663281 granted to the Institute of Mathematics of t he Polish Academy of Sciences for the years 2021–2023.
†E-mail: A.Czumaj@warwick.ac.uk. Department of Computer S cience and Centre for Discrete Mathematics and its
Applications, University of Warwick, UK. Research support ed in part by the Centre for Discrete Mathematics and its
Applications (DIMAP), by EPSRC award EP/V01305X/1, by a Wei zmann-UK Making Connections Grant, by an IBM Award,
and by the Simons Foundation Award No. 663281 granted to the I nstitute of Mathematics of the Polish Academy of Sciences
for the years 2021–2023.
‡E-mail: Gopinath.Mishra@warwick.ac.uk. Department of Co mputer Science, University of Warwick, UK. Research
supported in part by the Centre for Discrete Mathematics and its Applications (DIMAP), by EPSRC award EP/V01305X/1,
and by the Simons Foundation Award No. 663281 granted to the I nstitute of Mathematics of the Polish Academy of Sciences
for the years 2021–2023.
1inspired by frameworks such as MapReduce [DG08], Hadoop [Whi15], Dr yad [IBY+07], and Spark [ZCF+10].
Introduced just over a decade ago by Karloﬀ et al. [KSV10] (and lat er reﬁned, e.g., in [ANOY14, BKS17,
C/suppress LM+18, GSZ11]), the model has been the subject of an increasing quan tity of fundamental research in
recent years, becoming nowadays the standard theoretical par allel model of algorithmic study.
MPC is a parallel system with m machines , each with s words of local memory . (We also consider the
global space g, which is the total space used across all machines, g = s ·m.) Computation takes place in
synchronous rounds: in each round, each machine may perform ar bitrary computation on its local memory,
and then exchange messages with other machines. Each message is sent to a single machine speciﬁed by the
machine sending the message. Machines must send and receive at mo st s words each round. The messages
are processed by recipients in the next round. At the end of the co mputation, machines collectively output
the solution. The goal is to design an MPC algorithm that solves a given task in as few rounds as possible.
If the input is of size n, then one wants s to be sublinear in n(for if s≥nthen a single machine can solve
any problem without any communication, in a single round), and the to tal space across all the machines to
be at leastn(in order for the input to ﬁt onto the machines) and ideally not much la rger. In this paper, we
focus on the low-local-space MPC setting, where the local space of each machine is strongly sublinear in the
input size, i.e., s = O(nδ) for some arbitrarily constant δ∈(0,1). This low-local-space regime is especially
attractive because of its scalability. At the same time, this setting is particularly challenging in that it
requires extensive inter-machine communication to solve clustering problems for the input data scattered
over many machines.
In recent years we have seen a number of very eﬃcient, often con stant-time, parallel clustering algorithms
that have been relying on a combination of a core-set and a “reduce-and-merge” approach. In this setting,
one gradually ﬁlters the data set by typically reducing its size on ever y machine to /tildewideO(k), continuing until
all the data can be stored on a single machine, at which point the prob lem is solved locally. Observe that
this approach has an inherent bottleneck that requires that any m achine must be able to store Ω( k) data
points. Intuitively, this follows from the fact that if a machine sees kdata points that are all very far away
from each other, it needs to keep track of all kof them, for otherwise it might miss all the information
about one of the clusters, which in turn could lead to a large miscalcula tion of the objective value. Similar
arguments could be also used to argue that each machine needs to c ommunicate Ω( k) points to the others
(see [CSWZ16] for a formalization of such intuition for a worst-case partition of points for k-median,k-means,
andk-center problems, though the worst-case partition assumption m eans that this bound does not extend
directly to MPC ). Because of that, most of the earlier clustering MPC algorithms, especially those working
in a constant number of rounds (see, e.g., [EIM11, MKC+15]), require Ω( k) or even Ω( k)·nΩ(1)local space.
Therefore in the setting considered in this paper, of MPC with local space per machine of s = O(nδ), the
approach described above cannot be applied when the number of clu sters is large, when k=ω(s). This
naturally leads to the main challenge in the design of clustering algorith ms forMPC with low-local-space:
how to eﬃciently partition the data into kgood quality clusters on an MPCwith local space s≪k. We
believe that this setting is quantitatively diﬀerent (and more challeng ing) from the setting when kis smaller
(or even comparable to) the amount of local space s.
In this paper, we focus on the k-center clustering problem , a standard, widely studied, and widely used
formulation of metric clustering. The problem is, given a set of ninput points, to ﬁnd a subset of size kof
these points (called centers ) such that that maximum distance of a point to its nearest center is minimized.
Speciﬁcally, in this work, we focus on the case where k≫s and hence, when kis quite large relative
ton: one can think of these problem instances as “compressing” the inp ut set ofnpoints into kpoints.
Very recently this problem has been addressed by Bateni et al. [BE FM21], who showed one can design an
O(log logn)-roundMPC algorithm, with local space s = O(nδ) and global space g = /tildewideO(n1+δ),1that returns
anO(log log logn)-approximate solution with k+o(k) centers. Our main result is an improved bound in the
MPC model:
Theorem 1.1 (Main result stated informally ).InO(log logn)rounds on an MPC, we can compute
anO(log∗n)approximate solution to the k-centers problem using k(1 +o(1))centers.
1/tildewideO(f) hides a polynomial factor in log f.
2TheMPChas local space s =O(nδ)and global space g =/tildewideO(n1+ρ)for any constant ρ > 0. Then
input points are in Rdfor some constant dand we assume that k= Ω(logcn)for a suitable constant c. Our
algorithm succeeds with high probability.
The algorithmic framework is based on a repeated application of locally sensitive sampling : sampling a set
of “hub” points, assigning all other points to a nearby hub, and the n adding new hubs to well-approximate
the point set. We improve the approximation factor by a careful ex amination of the progress of clusters in
some ﬁxed optimal clustering over the course of the algorithm. Due to the depth of our iteration, clusters no
longer satisfy certain properties with high probability, and carefully bounding the size of the clusters that
fail to meet certain checks is an important challenge to overcome in o ur analysis. Additionally, we provide
a more ﬂexible guarantee on the global space, providing an accurac y parameter which can be set to reduce
global space used at the expense of a larger approximation ratio (o r vice versa).2This is possible because of
the way we implement locally-sensitive hashing (LSH) inMPC . We believe our implementation of LSH in
MPC could potentially see further applications, e.g., for other geometric problems.
1.1 Related work
There has been a large amount of work on various variants of the clu stering problems (see, e.g., [XW05]
for a survey of research until 2005), including some extensive stu dy of thek-center clustering problem. The
k-center problem is well known to be NP-hard and simple algorithms are known to achieve a 2-approximation
[DF85, Gon85, HS85]; this approximation ratio is tight unless P = NP [HN7 9].
The study of clustering in the context of parallel computing is extre mely well-motivated: as the size of
typical data sets continue to increase, it becomes infeasible to sto re input data on a single machine, let alone
iterate over it many times (as greedy sequential algorithms require (see, e.g., [Gon85])). It comes therefore
as no surprise that there has been a considerable amount of work o nk-center clustering algorithms in MPC .
In particular, several constant-round, constant-approximat ion algorithms in the MPC setting were given
recently for general metric k-center clustering, see, e.g., [EIM11, MKC+15, CPP19]. Much of this work used
coresets or similar techniques as a means of approximating the structure of t he underlying data, naturally
implying a requirement that the local space satisﬁes s = Ω( k) per machine and global space is g = Ω( nk) or
Ω(nǫk2) [EIM11, MKC+15, CPP19]. Speciﬁcally, Ene et al. [EIM11] gave a O(1) round 10-approximation
MPC algorithm that uses local space s = O(k2nΘ(1)), Malkomes et al. [MKC+15] obtained a 2-round 4-
approximation MPC algorithm with local space s = Ω(√
nk), and Ceccarello et al. [CPP19] obtained a
2-round (2 + ε)-approximation MPC algorithm that uses local space s = Od,ε(√
nk) for the problem in
metric spaces with doubling dimension d. As mentioned earlier, these algorithms are not scalable if kis large
relative ton(for example, when k=n1/3), making the case of large kparticularly challenging. Furthermore,
as argued by Bateni et al. [BEFM21], the case of large kappears naturally in some applications of k
clustering, including label propagation used in semi-supervised learn ing, or same-meaning query clustering
for online advertisement or document search [WLF+09]. Unfortunately, we do not know of any O(1)-round,
O(1)-approximation MPC algorithm that would use local space s = o(k).
In order to address the case of large k, Bateni et al. [BEFM21] considered a relaxed version of k-center
clustering for low dimensional Euclidean spaces with constant dimens ion. The goal of that work was to design
a scalable MPC algorithm for the k-center clustering problem with a sublogarithmic number of rounds of
computation, sublinear space per machine , and small global space. Bateni et al. [BEFM21] showed that in
O(log logn) rounds on an MPC with s =O(nδ), one can compute an O(log log logn)-approximate solution
to constant-dimension Euclidean k-center with k(1 +o(1)) centers. Their algorithm uses /tildewideO(n1+δ·log ∆)
global space. Bateni et al. [BEFM21] complemented their analysis by some empirical study to demonstrate
that the designed algorithm performs well in practice.
Finally, in the related PRAM model of parallel computation Blelloch and T angwonsan gave a 2-approximation
algorithm for k-center [BT10]. However, their algorithm requires Ω( n2) processors and it is therefore diﬃcult
to translate the approach to our setting.
2In particular, the constant in O(log∗n) of Theorem 1.1 depends on ρ.
31.2 Technical contributions
Our main result in Theorem 1.1 is an extension of the approach develop ed in Bateni et al. [BEFM21]
that signiﬁcantly improves the quality of the approximation guarant ee. To present these two results in the
right context, we will brieﬂy describe the main diﬀerences between t hese two works at a high level.
The approach of Bateni et al. [BEFM21] starts with the entire point setPas a set of potential centers
(solution), and reﬁnes it to P=P0⊇···⊇Pτ, until|Pτ|=k+o(k). The ﬁnal set Pτis reported as
the output. It is not diﬃcult to see that if we take an optimal cluster ingC∗forP(i.e.,C∗is the optimal
solution to the k-center problem for P), then the number of potential centers in any cluster C∈C∗reduces
over rounds (that is, |Pi+1∩C|≤|Pi∩C|). Let us deﬁne a cluster C∈C∗to beirreducible from round
i, ifiis the minimum index such that |C∩Pi| ≤ 1. Two central properties of the cluster reﬁnement
due to Bateni et al. [BEFM21] are that after O(log logn) rounds the size of each cluster in C∗reduces to
/tildewideO(logn), and that after that, the total number of the points in the redu cible clusters inC∗reduces after
each round by a constant factor, implying that another O(log logn) rounds suﬃce to ensure the desired
number of centers (at most kdue to the irreducible clusters and o(k) due to the reducible clusters) and
henceτ=O(log logn). This is then complemented by the analysis of the quality of the reﬁn ements which
guarantees that each new reﬁnement adds an additive term of O(opt) to the cost of the solution, giving
in total a double logarithmic approximation ratio. They further gave a sketch of the analysis to get an
approximation ratio of O(log log logn).
In our paper we substantially improve the approximation factor to O(log∗n) by extending the framework
in the following sense. We show that, after O(log logn) rounds, the size of each cluster in C∗reduces to
/tildewideO(logn) such that the reﬁnement in each round adds an additive error of O(opt
loglogn) to the cost of the
solution. Then, we show that after additional O(log log logn) rounds, the sizes of almost all (but not all)
clusters inC∗reduce to a/tildewideO(log logn) such that the reﬁnement in each round adds an additive error of
O(opt
logloglog n) to the cost of the solution. Next, we show that after another O(log log log log n) rounds, the
sizes ofalmost all clusters inC∗reduce to a/tildewideO(log log logn) such that the reﬁnement in each round adds
O(opt
loglogloglog n) to the cost of the solution, and so on. We continue this until the siz es of almost all clusters
inC∗reduce toO(log∗n). Observe that the total number of rounds taken so far is bound ed byO(log logn),
and we can argue that the current solution has an approximation ra tio ofO(log∗n). An important challenge
in analyzing this approach is that not all clusters satisfy these size guarantees with high pro bability . Indeed,
we cannot obtain a high probability guarantee by cluster reﬁnement relying on random sampling of the
already small clusters; we can ensure only that most of the clusters are getting small . LetC∗∗⊆C∗be the
clusters that satisfy the reduction property as discussed above , that is, such that the number of points in
each cluster ofC∗∗is bounded byO(log∗n) currently. We argue that the total number of points in the
reducible clusters in C∗∗reduces by a constant factor after each successive round, add ing an additive error
ofO(opt) each time. This implies that another O(log(log∗n)) rounds are good enough to ensure that we
have the desired number of centers at the end. To bound the tota l number of centers, we also need to show
that the number of centers in clusters in C∗\C∗∗(that is, the set of clusters which fail to adhere to a size
guarantee at some point during the algorithm) is bounded. Note tha t we cannot track which clusters succeed
or fail (doing so would require us to know an optimal clustering), and so we useC∗andC∗∗only for the
analysis. In summary, the approach sketched above will reduce th e number of clusters to k(1 +o(1)), and
will ensure that the total number of rounds spent by our algorithm isO(log logn) and the approximation
ratio of our solution is O(log∗n). A more detailed overview is in Section 2.
Our approach relies heavily on the use of LSH (locality sensitive hashin g), and we provide a ﬂexible
implementation of LSH in MPC which one can conﬁgure with an appropriate parameter ρ. Reducing the
value ofρdecreases the amount of global space used by the algorithm (globa l space used is /tildewideO(n1+ρ)) while
increasing the approximation ratio.
1.3 Notation and preliminaries
We now introduce the notation used through the paper.
4First, we present the setting of the parameters of our MPC . Thek-center algorithm in this paper works
for any local space s = O(nδ) for a constant 0 <δ< 1: the setting of δhas only a constant factor impact on
the running time. Similarly, the MPC can have any global space g = /tildewideO(n1+ρ) for some constant ρ >0:ρ
can be made arbitrarily small, and its setting has a constant factor im pact on the approximation ratio. We
sometimes refer to MPC with these choices of s and g simply as “ MPC ” in the rest of this paper.
Let us recall that certain operations, particularly sorting and pre ﬁx sum ofnelements, and broadcasting
a value of size <s, can be computed deterministically in O(1) rounds (see [GSZ11]).
The input to our problem is a set Pofnpoints in Rd, wheredis a constant, and an integer parameter
k<n . We deﬁne d(p,q) as the Euclidean distance between points pandqinRd. We generalize this notation
to the distance between a point and a set: d(p,S) := min
q∈Sd(p,q) is the minimum distance from pto a point
inS. We deﬁne cost (P,S) := max
p∈Pd(p,S) as the distance of the point in Pwhich is “furthest away” from
any point in S. Without loss of generality, we assume that the input set is re-scale d so that the minimum
distance between any two points in Pis 1; then we let ∆ to be the maximum distance between any two
points inP.
We denote the set {1,...,t}by [t] and log(i)n:= log...log/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
inthe iterated logarithm of n. By convention
log(0)n:=n. The notations /tildewideO(f) and/tildewideΘ(f) hide polynomial factors in log f.
We now deﬁne formally the k-center clustering problem.
Deﬁnition 1.2. LetPbe a set of points in Rd. AclusteringCofPis a partition of Pinto nonempty clusters
C1,...,C t. Theradius of clusterCiis min
x∈Cimax
y∈Cid(x,y), and the cost of the clusteringCis the maximum of
the radii of the clusters C1,...,C t.
Deﬁnition 1.3 (k-center clustering problem ).Letk,n,d∈Nwithk≤n, andPbe a set ofnpoints in
Rd. Thek-center problem for Pis to ﬁnd a set S∗⊆Psuch that
S∗= arg min
S⊆P:|S|=kcost (P,S).
Moreover, cost (P,S∗) is deﬁned as the (optimal) cost of the k-center problem for P.
We assume throughout the paper that k >s. However, our algorithms work as described provided that
k= Ω((logn)c) for a suitable constant c(which is also the main focus of the work).
1.4 Our results — detailed bounds
We now present in details the main result of this paper:
Theorem 1.4 (Main result). LetPbe any set of npoints in Rdand letoptdenote the optimal cost of the
k-center clustering problem for P. There exists an MPCalgorithm that in O(log logn)rounds determines
with high probability a set T⊆Pofk+o(k)centers, such that cost (P,T) =O(log∗n)·opt. TheMPC
uses local space s =O(nδ)and global space g =/tildewideO(n1+ρ·log2∆).
Theorem 1.4 follows directly from a more general theorem.
Theorem 1.5 (Generalization of Theorem 1.4). Letαbe an arbitrary integer, 1≤α≤log∗n−c0for
some suitable constant c0. LetPbe any set of npoints in Rdand letoptdenote the optimal cost of the
k-center clustering problem for P. There exists an MPCalgorithm that in O(log logn)rounds determines
with high probability a set T⊆Pof centers, such that cost (P,T) =O((α+ log(α+1)n))·optand|T|≤k·/parenleftig
1 +1
/tildewideΘ(log(α)n)/parenrightig
+/tildewideΘ((log(α)n)3). TheMPCuses local space s =O(nδ)and global space g =/tildewideO(n1+ρ·log2∆).
Observe that in Theorem 1.5 we have |T|=k+o(k), since we are assuming k= Ω((logn)c) for some
suitable constant c.
5Letα0be the solution to the equation α= log(α+1)n; observe that α0= Θ(log∗n). Then Theorem 1.4
is a corollary of Theorem 1.5 when we choose α=α0.
Theorem 1.5 can be seen as a ﬁne-grained version of Theorem 1.4: as αincreases the cost of the solution
decreases and number of center increases (with the number of ro unds always being O(log logn)). Therefore
Theorem 1.5 is more amiable in practical scenarios in the following sense :αin Theorem 1.5 can be set to
trade oﬀ between the quality of the solution and the number of cent ers in the solution. We would also like
to highlight that the result of Bateni et al. [BEFM21] is a special cas e of Theorem 1.5 when α= 1 andα= 2
to obtainO(log logn) andO(log log logn) approximation, respectively.
1.5 Organization of the paper
In Section 2 we give a proof of our main result predicated on the corr ectness of our main algorithm,
and then give an overview of the subroutines which our main algorithm contains. In Section 3 we explain
how LSH (locality-sensitive hashing) on MPC can be implemented to assign each point p∈Pto a hub
inH⊆Pwhich is within a constant factor of the closest hub to p. In Sections 4 and 5 we prove critical
properties of subroutines used in our main algorithm, and then in Sec tion 6 we prove the correctness of our
main algorithm. Finally, Section 7 contains some conclusions.
2 Technical overview
Recall that Theorem 1.4 is our main result and Theorem 1.5 is its parame terized generalization. Our
proof of Theorem 1.5 (and hence of Theorem 1.4) relies on the followin g main technical theorem.
Theorem 2.1 (Main technical theorem proved in this paper). Letαbe an arbitrary integer, 1≤α≤
log∗n−c0for some suitable constant c0. Letrbe an arbitrary positive real. Let Pbe any set of npoints in Rd
and letCrbe a clustering of Pthat has the minimum number of centers among all clusterings ofPwith cost
at mostrand|Cr|= Ω((logn)c)for a suitable constant c. There exists an MPCalgorithm Ext-k-Center
(Algorithm 5) that with probability at least 1−1
(log(α−1)n)Ω(1), inO(log logn)rounds determines a set T⊆P
of centers, such that cost (P,T) =O(r·(α+ log(α+1)n))and|T|≤|Cr|·/parenleftig
1 +1
/tildewideΘ(log(α)n)/parenrightig
+/tildewideΘ((log(α)n)3).
TheMPCuses local space s =O(nδ)and global space g =/tildewideO(n1+ρ·log ∆).
Algorithm Ext-k-Center used in Theorem 2.1 takes two parameters: an accuracy paramete rαand a
cost parameter r, and produces the output in a form similar to that required in Theore m 1.5, except that
the number of clusters is equal to the number of centers in an optim al clustering of Pwith cost at most r.
This is in contrast with a standard clustering setting where the numb er of clusters is given as input, with
no relationship to the cost of the solution. Therefore, if we knew a c onstant factor approximation to the
optimal cost to the k-center problem, then setting it to be rin Theorem 2.1, we would get a desired solution
as required in Theorem 1.5. This naturally suggests to run Ext-k-Center multiple times in parallel in order
to obtain Theorem 1.5. Note that the success probability of Theore m 2.1 is not high. Hence we ﬁrst run
Ext-k-Center a suitable number of times in parallel to get an algorithm Ext-k-Center′whose output
and space requirements are same as that of Ext-k-Center , but the success probability is high. Then we run
Ext-k-Center′forO(log ∆) choices of r(starting with r= ∆ and decreasing a constant factor each time)
in parallel to get algorithm Ext-k-Center′′(the algorithm of Theorem 1.5). Moreover, Ext-k-Center′′
reports the output of Ext-k-Center′for the minimum rfor which we get the number of centers equals to
k+o(k). The details are in Appendix B.
2.1 Overview of the proof of Theorem 2.1
The idea to prove Theorem 2.1 is based on the framework which we call locally sensitive sampling . We
generate a set H⊆Pof points (called hubs) by sampling each point in Pindependently with a suitable
probability, and assign all other points to one of the hubs based on it slocality . LetBhbe thebag of the
6hubh—-the set of points associated to a hub h∈H. We run a variation of a well known greedy algorithm
[Gon85] (for k-center in the sequential setting) for each bag in parallel to ﬁnd a s et of intermediate centers
Chfor hubhsuch that cost (Bh,Ch) =O(r). We again repeat the procedure by setting/uniontext
h∈HChas the
point set. We continue this process a particular number of times with a particular choice of probability and
radius parameters, and report the centers, at that point of time , as the ﬁnal solution.
This framework was recently used by Bateni et al. [BEFM21] to give anO(log logn)-round MPC
algorithm with local space s = O(nδ) and global space g = /tildewideO(n1+δ), which computes an O(log log logn)-
approximate solution to k-center with k(1 +o(1)) centers, with high probability. We extend their framework
and generalize the analysis to give an O(log∗n) approximate solution as stated in Theorem 1.4. Note that
Theorem 2.1 takes care of Theorem 1.4 via Theorem 1.5.
The algorithm corresponding to Theorem 2.1 is Ext-k-Center (Algorithm 5 in Section 6). Before
describing Ext-k-Center , we describe and contextualize the three subroutines which it uses (Nearest-
Hub-Search ,Sample-And-Solve andUniform -Center ). The main algorithm of Bateni et al. [BEFM21]
uses subroutine Sample-And-Solve andUniform -k-center . We use analogous subroutines Sample-
And-Solve andUniform -Center in our algorithm corresponding to Sample-And-Solve andUniform -
k-center in Bateni et al. [BEFM21], respectively, to achieve the desired result . But there are some
diﬀerences which we will discuss when we describe Sample-And-Solve andUniform -Center . Due to our
implementation of Nearest-Hub-Search , we are able to give a more ﬂexible bound on global space. We can
improve the approximation ratio mainly due to generalizing their Uniform -k-Center toUniform -Center
in our case and using sophisticated analysis in our main algorithm that c allsUniform -Center .
Let us discuss ﬁrst at a high level what these subroutines achieve in the context of the framework of
locally sensitive sampling (discussed at the beginning of this section). Intuitively, the purpose of Sample-
And-Solve is to sparsify dense regions of points: it samples nodes with a given pr obability and iteratively
adds centers in order to ensure that the cost of the centers rem ains low. Uniform -Center repeatedly uses
Sample-And-Solve : its main purpose is to guarantee that the number of centers in eac h cluster of some
ﬁxed optimal clustering decreases in a certain way over time.
Nearest-Hub-Search (Q,H)
Takes as input a set Qof at mostnpoints and a set of hubs H⊆Q. For all points q∈Q\H, it ﬁnds
a point close( q)∈Hsuch thatd(q,close(q)) =O(d(q,H)), with probability at least 1 −1
nΩ(1).Nearest-
Hub-Search can be implemented in MPC with local space s = O(nδ) and global space g = /tildewideO(n1+ρ·log ∆)
inO(1) rounds. Nearest-Hub-Search useslocally sensitive hashing [HIM12] and its implementation in
MPC . For details on Nearest-Hub-Search , see Section 3.
Sample-And-Solve (Q,p,r)
Takes a set of Qof at most npoints, a sampling parameter p, and a radius parameter r. It produces
some set of centers S⊆Qsuch that cost (Q,S) =O(r).3Importantly, this can be implemented in an MPC
with local space s = O(nδ) and global space /tildewideO(n1+ρ·log ∆) inO(1) rounds (Lemma 4.1) as, aside from
usingNearest-Hub-Search to assign points to hubs, the computation is all done locally. Sample-And-
Solve ﬁrst samples each point in Q(independently) with probability p: letH⊆Qbe the set of sampled
points called hubs. Then Sample-And-Solve callsNearest-Hub-Search with input point set Qand
hub setH. After getting close( q) for eachq∈Q\H,Sample-And-Solve collects all points Bh⊆Q
assigned to a hub h∈H(including hub h) and selects a set of centers ChfromBhgreedily using a variation
of the sequential algorithm of [Gon85], such that cost (Bh,Ch) =O(r). Finally, the algorithm outputs
S=/uniontext
h∈HCh. However, there is a diﬃculty to overcome: note that |Bh|may beω(nδ). SoBhmay not
ﬁt into the local memory of a machine. We show that this can be handle d by distributing the points in
Bhinto multiple machines, duplicating hacross all such machines. See Section 4 for more details about
Sample-And-Solve .
3The constant inside O(·) depends on ρ.
7Algorithm Sample-And-Solve in our paper serves essentially the same purpose as the correspon ding
algorithm due to Bateni et al. [BEFM21]. The approximation guarante e and number of rounds performed
are the same in both cases. However, the global space used by our algorithm Sample-And-Solve is more
ﬂexible in the following sense: reducing the value of ρdecreases the amount of global space used by the
algorithm (global space used is /tildewideO(n1+ρ)·log ∆) while increasing the approximation ratio.
Uniform -Center (Vt,r,t)
Takes a set Vtof at mostnpoints, a radius parameter r, and an additional parameter t≤n.It produces a
setSof centers, by calling Sample-And-Solve τ= Θ(log log t) times.Si−1is the input to the i-th call and
Siis the output of the i-th call: overall we have S0=VtandSτ=S(the output of Uniform -Center ). The
probability and radius parameters to the calls to Sample-And-Solve are setsuitably . From the guarantees
we have from Sample-And-Solve , we have the following guarantee for Uniform -Center : (i) it can be
implemented in an MPC with local space s = O(nδ) and global space g = /tildewideO(n1+ρ·log ∆) inO(log logt)
rounds (Lemma 5.1), and (ii) cost (Vt,S) =O(r·τ) =O(rlog logt) (Lemma 5.2). Uniform -Center
guarantees a reduction in cluster sizes in an optimal clustering in the following sense. Consider a ﬁxed
clusteringCt
rofVtthat has cost at most r. ForC∈Ct
r: if|C∩Vt|≤t, then|C∩S|=O(logt·(log logt)2),
with probability at least 1 −1
tΩ(1). This is formally stated in Lemma 5.3: note that this ceases to be high
probability when t∈o(n). This guarantee on the size reduction plays a crucial role when pro ving the number
of centers reported by Ext-k-Center in Section 6. For more details on Uniform -Center , see Section 5.
OurUniform -Center is a full generalization of the analagous Uniform -k-center in Bateni et al.
[BEFM21]. In particular, Uniform -k-Center is a special case of our Uniform -Center whent=n. This
generalization plays a crucial role in the correctness of Ext-k-Center when we call Uniform -Center
multiple times. Uniform -k-Center is not robust enough to be called from Ext-k-Center multiple times
to achieve the desired result.
Ext-k-Center
The algorithm consists of two phases, where Phase 1 consists ofαsubphases and Phase 2 consists of
β= Θ(log(α+1)n) subphases. In the j-th subphase of Phase 1 , that is, in Phase 1.j ,Ext-k-Center calls
Uniform -Center (Tj−1,rj−1,tj−1), whereT0=P,r0=r
loglogn,t0=n,tj=/tildewideΘ(log(j)n),andrj=r
loglogtj.
Observe that the guarantees of Uniform -Center ensure the following:
(i)Phase 1 can be implemented in an MPC with local space s = O(nδ) and global space g = /tildewideO(n1+ρ·
log ∆) inα/summationtext
j=1log logtj−1=O(log logn) rounds;
(ii)cost (Tj−1,Tj) =O(rj−1log logtj−1) =O(r) for eachj∈[α]. Hence, cost (P,Tα) =O(rα).
Now consider Phase 2 ofExt-k-Center .
In thei-th subphase of Phase 2 , that isPhase 2.i ,Ext-k-Center callsSample-And-Solve (Tα+i−1,1
2,r),
whereT=Tα+βis the ﬁnal output of Ext-k-Center . From the guarantee of Sample-And-Solve , we
have
(i)Phase 2 can be implemented in an MPC with local space s = O(nδ) and global space g = /tildewideO(n1+δ·
log ∆) inO(β) =O(log(α+1)n) rounds;
(ii)cost (Tα+i−1,Tα+1) =O(r) for eachi∈[β]. Hence,
cost (P,T) =cost (P,Tα+β) =cost (P,Tα) +O(βr)
=O(r·(α+ log(α+1)n)).
8Combining the guarantees concerning the round complexity, global space and approximation guarantee of
Phase 1 andPhase 2 , we get the claimed guarantees on round complexity, global space a nd approximation
guarantees in Theorem 2.1 (see Lemma 6.1 for round and global spac e guarantee and Lemma 6.2 for the
guarantee on approximation factor).
Now, we discuss how we bound the number of centers that Ext-k-Center outputs, that is,|T|. Consider
an optimal clustering CrofPwith cost at most r. A cluster C∈Cris said to be active (afterPhase 1 )
if|C∩Tj|≤tjfor eachjwith 1≤j≤α. We sayCisinactive , otherwise. Using the guarantee given by
Uniform -Center concerning the reduction in cluster sizes, we can show that the tot al number of centers in
Tα, that are in inactive clusters, is O/parenleftig
|Cr|
(log(α)n)Ω(1)/parenrightig
, with probability at least 1 −α/summationtext
i=11
tΩ(1)
i−1(see Lemma 6.6).
Note thatTαdenotes the set of intermediate centers we have after Phase 1 . So, for any cluster C∈Cr
that is active after Phase 1 , it satisﬁes|C∩Tα|≤tα=/tildewideΘ(log(α)n). That is, with probability at least
1−α/summationtext
i=11
tΩ(1)
i−1, we have the following:
|Tα|≤|Cr|·tα+O/parenleftigg
|Cr|
(log(α)n)Ω(1)/parenrightigg
.
We deﬁne an active cluster C∈Crisi-large if|C∩Tα+i−1|≥2. We show that the total number of
intermediate centers in any large clusters reduces by a constant f actor in Phase 2.i , with probability at
least 1−1
tΩ(1)
α−1. Note that the total number of intermediate centers in all active la rge clusters, just before
Phase 2 , is at most|Cr|·tα=|Cr|·/tildewideΘ(log(α)n), and we are executing β= Θ(log(α+1)n) many sub-phases
inPhase 2 . We can show that the total number of centers in the active large c lusters, after Phase 2 , is at
most|Cr|
/tildewideΘ(log(α)n)+/tildewideΘ((log(α)n)3), with probability at least 1 −1
tΩ(1)
α−1(Lemma 6.7). Combined with the fact
the number of active small clusters can be at most |Cr|with the bound on number of inactive clusters in
Phase 2 , we have the desired bound on |T|. Full details of Ext-k-Center and its analysis are presented
in Section 6.
3 Nearest Hub Search
Recall that our Nearest-Hub-Search algorithm takes a set Qof points and a set H⊆Qof hubs.
For eachq∈(Q\H), we want to ﬁnd a hub h∈Hsuch that the distance between qandhis only a
constant-factor more than the distance between qand the closest hub to qinH: informally, his “almost”
the closest hub to qinH.
In this section, we use locally sensitive hashing (LSH) [HIM12] to implement algorithm Nearest-Hub-
Search (Q,H ) onMPC . Our implementation of locally sensitive hashing is parameterizable: by setting the
parameterρappropriately, one can reduce the global space while increase the a pproximation ratio, or vice
versa.
First, we begin by recalling the deﬁnition of locally sensitive hashing, int roduced in [HIM12]:
Deﬁnition 3.1 (Locally sensitive hashing [HIM12]) .Letr∈R+,c>1 andp1,p2∈(0,1) be such that
p1> p2. A hash familyH={h:Rd→U}is said to be a ( r,cr,p 1,p2)-LSH family if for all x,y∈Rdthe
following hold:
•Ifd(x,y)≤r, then Pr h∈H(h(x) =h(y))≥p1;
•Ifd(x,y)≥cr, then Pr h∈H(h(x) =h(y))≤p2.
Consider the following proposition that talks about the existence of a particular hash family, which will
be useful to describe and analyse Nearest-Hub-Search (Q,H ) in Algorithm 1.
Proposition 3.2 ([HIM12]) .Letr,n∈Nandρ∈(0,1). There exists a (r,cρr,(1/n)ρ,1/n)-LSH family,
wherecρis a constant depending only on ρ.
9InNearest-Hub-Search (Q,H ),Qis a set of at most npoints and H⊆Qis the set of hubs. Our
objective is to ﬁnd a hub for each point which is at most some constan t factor further away than the nearest
hub, rather than ﬁnding the hub which is the closest. We do this by ma king log ∆ guesses about the distance
to the nearest hub, and for each guess trying to ﬁnd a hub within th at distance.
For our log ∆ guesses for r(the distance to the closest hub), we take (independently and unif ormly at
random)L= Θ(nρ) many hash functions from a ( r,cρr,(1/n)ρ,1/n)-LSH family and use them to hash all
the points, including the hubs.4Then we gather all points with the same hash value on consecutive ma chines.
We then need to ﬁnd, for each point, a hub that is close to it. This is diﬃ cult if the number of hubs mapped
to a given hash value is large: if hhubs andmpoints are mapped to the same hash value, then we have
to perform h·mdistance checks, which is potentially prohibitive if h·m > s. To overcome this we show
that, if many hubs are mapped to the same hash-value, we are able t o discard all but a constant number
of them, and retain for each point a hub that is within a constant fac tor of the distance of the closest hub.
This works because of the choice of our hash function and by the de ﬁnition of LSH. The full algorithm
Nearest-Hub-Search (Q,H ) is described in Algorithm 1, and its correctness is proved in Lemma 3.3 .
Lemma 3.3 (Nearest hub search). LetQbe a set of at most npoints in Rd,H⊆Qdenote the set of
hubs, andcρbe a suitable constant depending on ρ. There exists an MPCalgorithm Nearest-Hub-Search
(Q,H) (as described in Algorithm 1) that with high probability, i nO(1)rounds, for all q∈Q\H, ﬁnds a
hub close (q)∈Hsuch thatd(q,close (q))<2cρ·d(q,H). TheMPCuses local space s =O(nδ)and global
space g =O(n1+ρ·log2n·log ∆).
Algorithm 1: Nearest-Hub-Search (Q,H )
Input: A setQof at mostnpoints and a set of hubs H⊆Q.
Output: For each point in Q, report close( p)∈Hsuch thatd(p,close(p))≤2cρ·d(p,H), wherecρ
is a suitable constant depending only on ρ.
1begin
2for(i= 1toI= Θ(logn))do
3 for(j=0 to log ∆)do
4 Setr= 2j
5 TakeL= Θ(nρ) many hash function f1,...,f L(independently and uniformly at random)
from a (r,cρr,(1/n)ρ,1/n)-LSH family.
6 for(ℓ= 1toL)do
7 Determinefℓ(q) for eachq∈Q.
8 Find the distance of each q∈Qwith at most a constant (say 10) number of hubs
h∈Hsuch thatfl(q) =fl(h). If we get such a h∈Hsuch thatd(q,h)≤cρ·r, then
we set close ijℓ(q) =handnull , otherwise.
9 end
10 Set close ij(q) =null if close ijℓ(q) =null for allℓ∈[L]. Otherwise, set
closeij(q) = close ijℓ(q) for someℓ∈L.
11 end
12 Set close i(q) =null if close ij(q) =null for allj∈[log ∆]. Otherwise, close i(q) = close ij∗(q)
such thatj∗is minimum among all jfor which close ij(q) is notnull .
13end
14 If there exists a q∈Qsuch that close i(q) isnull for alli∈[logn], then report Fail.
15 Otherwise, set close( q) = close i(q) for somei∈I.
16end
From Algorithm 1, note that we repeat a procedure (lines 3–12 that ﬁnd an almost closest hub with
probability 2 /3)I= Θ(logn) times, and report the output we get from any of the instances. C onsider
4The constant ρis the same constant as in the exponent of nin the global space bound. Recall our tradeoﬀ: choosing a
smallerρdecreases the amount of global space needed, but increases t he approximation ratio.
10Lemma 3.4, that says that, in Nearest-Hub-Search each point q∈Qﬁnds close( q)∈Hsatisfying the
required property with high probability. This will immediately imply the co rrectness of Lemma 3.3. We
then discuss the MPC implementation of Nearest-Hub-Search .
Note that close i(q) (which is either null or a point in Hsuch thatd(q,closei(q)) =O(d(q,H))) denotes
the output of Nearest-Hub-Search for pointq∈Q\Hand the instance i∈I.
Lemma 3.4. For a particular q∈Q\Handi∈I, close i(q)∈His notnullandd(q,closei(q))≤
2cρ·d(q,H), with probability at least 2/3.
Proof. Considerj∗such thatd(q,H)≤r= 2j∗≤2·d(q,H), andqh∈Hbe such that d(q,qh)≤2·d(q,H).
As eachfi,i∈L,is a function chosen from ( r,cρr,(1/n)ρ,1/n)-LSH family, Pr( fi(q) =fi(qh))≥1
nρ. As
L= Θ(nρ), there exists an ℓ∗∈Lsuch thatfℓ∗(q) =fℓ∗(qh) with probability at least 9 /10. But our algorithm
may not ﬁnd this particular qhwhile considering the hubs h∈Hsuch thatfℓ∗(q) =fℓ∗(qh) =fℓ∗(h) (See line
8 ofNearest-Hub-Search ). Again, as fℓ∗is chosen from ( r,cρr,(1/n)ρ,1/n)-LSH family, the expected
number of hubs h∈H, withd(q,h)> cρrbutfℓ∗(q) =fℓ∗(h), is at most 1. By Markov’s Inequality,
the probability that the number of such hubs is more than 10 is at mos t 1/10. So, with probability at
least 2/3,Nearest-Hub-Search , sets close ij∗ℓ∗=hfor someh∈Hsuch thatd(q,h)≤cρr, that is,
d(q,h)≤2cρ·d(q,H). Now considering the way we set close ij(q) from close ijℓ(q)’s (ℓ∈L) in line 10 and
closei(q) from close ij(q)’s (0≤j≤log ∆) in line 12, we have that Nearest-Hub-Search sets close i(q)∈H
such thatd(q,closei(q))≤2cρ·d(q,H) with probability at least 2 /3.
Now, consider the way Nearest-Hub-Search sets the value of close( q) in lines 14–15 from close i(q)’s.
By Lemma 3.4, we have close( q) such that it is not null andd(q,close(q))≤2cρ·d(q,H) with probability
at least 1−1
nΩ(1).This is because I= Θ(logn). Applying the union bound over all points in Q\H, we see
that Lemma 3.3 is implied by Lemma 3.4, except the details of MPC implementation.
MPC implementation of Nearest-Hub-Search
Without loss of generality, we assume that ρ<δ as otherwise we can set ρ=δ. First, notice that, if we
can implement lines 4-10 of Nearest-Hub-Search inMPC with local space s = O(nδ) and global space
g =O(n1+ρ·logn), then we can run these lines in parallel for each possible value of iandj(adding a factor
ofO(logn·log ∆) to the global space). Then the results can be aggregated in O(1) rounds using sorting and
preﬁx sum [GSZ11].
It suﬃces then to show that lines 4–10 of Nearest-Hub-Search can be implemented in the desired
rounds and space. The hash functions in line 5 can be generated loca lly by some “leader” machine and
broadcast to the other machines in O(1) rounds, since we assume ρ < δ . We again perform lines 6–9 in
parallel, giving each f∈Lits own set of machines to use.
We next consider the implementation of lines 7–8 given a speciﬁc f∈L. Machines can compute f
locally and without communication. Each point q∈Qis now represented by a tuple ( f(q),hub(q),q), where
hub(q) = 1 ifq∈Hand 0, otherwise. Machines then sort these tuples lexicographically and remove (using
preﬁx sum) all but 10 hubs for each value in range( f). For each v∈range(f), we now have to compute the
distance between each point q∈Qsuch thatf(q) =v, and each hub h∈Hsuch thatf(h) =vandhwas not
removed. It might be the case that some points in Qare not located on the same machine as the hubs which
are hashed to the same value (and in general, these points might not all ﬁt on one machine, see discussion in
Section 4). However, all that is required in this case is that the hubs can be sent to all machines containing
points hashed to the same value: this can be done using preﬁx sum in a constant number of rounds; since
there are at most 10 hubs for each value in range( f), each machine receives at most 10 hubs. Now machines
have the information necessary to locally compute close ijℓ(q) for all points that they contain, and for each
pointq∈Qthe tuple (q,closeijℓ(q)) is generated.
Finally, observe that line 10 can be implemented in O(1) rounds using sorting and preﬁx sum.
114 Sample and Solve
In this section, we describe Sample-And-Solve (Q,p,r ), which is a subroutine in Uniform -Center
andExt-k-Center in Sections 5 and 6, respectively. Sample-And-Solve (Q,p,r ) takes a set Qof at most
npoints, a sampling parameter p, and a radius parameter r, it relies on Nearest-Hub-Search discussed
in Section 3, and produces a set of centers S⊆Qsuch that cost (Q,S) =O(r).
Algorithm 2: Greedy (R,h,r )
Input: SetRof at mostnpoints; radius parameter r∈R+.
Output: A setG⊆Rof centers.
1begin
2 SetG←{h}.
3while(∃x∈Rwithd(x,G) = min y∈Rd(x,y)>4cρr)do
4 // Herecρis the constant as in Lemma 3.3.
5 Letx∈Rbe the point furthest from G; addxtoG.
6end
7 Report the set Gof centers.
8end
Sample-And-Solve (Q,p,r ) calls algorithm Greedy (R,h,r ) as a subroutine, which produces a set of
centersG⊆Rsuch that cost (R,G) =O(r).Greedy (R,h,r ) is a variation of a classic 2-approximation
algorithm for k-center in the sequential setting [Gon85]. In Sample-And-Solve (Q,p,r ), the idea is to
sample each point in Q(independently) with probability pto form a set of hubs H. Then each point q∈Q
will be assigned to some hub h∈Hby using Nearest-Hub-Search (as described in Algorithm 1). For
h∈H, letBhbe the set of points assigned to h(includinghitself). We run Greedy for the points in Bh,
to produce a set of centers Sh. Finally,/uniontext
h∈HShis the output reported by Sample-And-Solve . There are
other technicalities – |Bh|may be much larger than s. In that case, we distribute the points in Bh\{h}across
a number of machines, but we send hto each machine, ensuring that the total number of points assigne d to
a machine (including h) is less than s—and then we apply Greedy to the points on each of these machines.
The formal algorithm for Sample-And-Solve is presented in Algorithm 3. The approximation guarantee,
round complexity and space complexity of Sample-And-Solve are stated in Lemma 4.1. An additional
property of Sample-And-Solve is stated in Lemma 4.4 which will be useful in both Section 5 and Section 6 .
Lemma4.1 (Approximation guarantee, round complexityand space compl exityof Sample-And-Solve ).
Consider Sample-And-Solve (Q,p,r), as described in Algorithm 3. With probability at least 1−min/braceleftig
e−Ω(p·nδ),1
nΩ(1)/bracerightig
,
it does not report Fail, and moreover:
(i) It produces a set of centers S⊆Qsuch that cost (Q,S)≤4cρr=O(r), wherecρis the constant as in
Lemma 3.3;
(ii) It takesO(1)MPCrounds with local space s =O/parenleftbig
nδ/parenrightbig
and global space g =/tildewideO(n1+ρ·log ∆).
Remark4.2. We callSample-And-Solve fromUniform -Center (Algorithm 4) with probability parameter
p= Ω/parenleftig
logn
nδ/parenrightig
. Therefore, the success probability of Sample-And-Solve in our case is always at least
1−1
nΩ(1).
Proof of Lemma 4.1. Note that Sample-And-Solve (Algorithm 3) crucially calls subroutine Greedy (Algorithm 2)
multiple times, particularly in line numbers 2, 10 and 14. We start the pr oof with the following observation
(about algorithm Greedy (R,h,r )) that follows from the description of Algorithm 2.
Observation 4.3. The output G⊆Rproduced by Greedy (R,h,r ) (as described in Algorithm 2) satisﬁes
cost (R,G)≤4cρr.
12Algorithm 3: Sample-And-Solve (Q,p,r )
Input: SetQof at mostnpoints; probability parameter p∈(0,1); radius parameter r∈R+.
Output: A setS⊆Qof centers.
1begin
2if/parenleftbig
|Q|≤s =O(nδ)/parenrightbig
then
3 CallGreedy (Q,q,r ) for some arbitrary q∈Q, and report the set of centers output by it as
S.
4end
5 Sample each point in Qindependently with probability p. Points which are sampled form the set
of hubsH.
6 IfH=∅,reportFail.
7 For each point qinQ, assign it to the closest hub in Hby calling
Nearest-Hub-Search (Q,H,ρ ). We call the set of points assigned to a hub h∈Hthebag
corresponding to h, and denote it as Bh. Note that Bhincludesh.
8for(eachh∈H)do
9 if(|Bh|≤s)then
10 CollectBhon a single machine.
11 Sh←Greedy (Bh,h,r).
12 end
13 else
14 Form bags Bh1,...,B hw, keepinghin everyBhi(i∈[w]) and putting other point in
Bh\{h}into exactly one of the Bhi’s, such that|Bhi|≤s =O(nδ) for eachi∈[w].
15 Shi←Greedy (Bhi,h,r), wherei∈[w].
16 Sh←w/uniontext
i=1Shi.
17 end
18end
19 Report set of centers S=/uniontext
h∈HSh.
20end
Note that both (i) and (ii) of Lemma 4.1 are direct if |Q|≤s. As in that case, we executes Greedy (Q,q,r )
for someq∈Qin one machine locally, and report its output as S. By Observation 4.3, we have cost (Q,S)≤
4cρr.
Now consider the case when |Q|>s. Note that Sample-And-Solve reportsFail only when the set
of hubsHis∅. As every point in Qis added to Hwith probability pindependently, the probability that
Sample-And-Solve reportsFail is at most (1−p)|Q|≤e−Ω(p·nδ). Now, we argue (i) and (ii) separately.
Recall the description of Algorithm 3 from Line 7–17.
(i) By Observation 4.3, for h∈H,cost (Bh,Sh)≤4cρr. AsS=/uniontext
h∈HShandQ=/uniontext
h∈HBh,cost (Q,S)≤
4cρr.
(ii) From Lemma 3.3, Nearest-Hub-Search can be implemented in MPC with local space s = O(nδ)
and global space g = /tildewideO(n1+ρ·log ∆) inO(1) rounds. After Nearest-Hub-Search is performed, each
point knows its assigned hub. Using sorting, we can place all points wit h the same hubs on consecutive
machines inO(1) rounds, and using preﬁx sum, we can count the number of point s assigned to each
hub inO(1) rounds. Now, we consider two cases:
If|Bh|≤s (that is: the bag could ﬁt on a single machine) then Greedy onBhcan be performed on
a single machine without communication, that is, in 0 rounds.5
5A minor technical matter is moving Bhto a single machine if it is big enough to ﬁt but originally sto red on two consecutive
13If|Bh|>s (that is: the bag could not ﬁt on a single machine) then we arbitrarily partition the bag and
performSample-And-Solve on each part. Speciﬁcally, we send hto each of the consecutive machines
on whichBhis stored, and these machines perform Greedy on the subset of the bag that they hold
locally. This can be performed in O(1) rounds.
Lemma 4.4 (An additional guarantee of Sample-And-Solve ).LetCrbe a clustering of Qhaving cost
at mostr. Then, with high probability, the following holds for any C∈Cr: if at least one hub is selected
fromC, then no further point in C\His selected as a center, that is, |S∩C|=|H∩C|.
Proof. Consider any point q∈C\H. As at least one hub is selected from C,d(q,H)≤2r. By the guarantee
fromNearest-Hub-Search (see Lemma 3.3), with probability at least 1 −1
nΩ(1),qis assigned to some hub
h∈Hsuch thatd(q,h)≤2cρd(q,H)≤4cρr. So, when we call Greedy (Bh,h,r), asd(q,h)≤4cρr,qwill
not be selected as a center. This implies that |S∩C|≤|H∩C|. The claim follows as H⊆S.
5 Uniform Center algorithm
In this section, we describe Uniform -Center (Vt,r,t), which iteratively reﬁnes a set of centers to a
smaller set of centers, by calling Sample-And-Solve on a quadratically-increasing probability schedule. It
callsSample-And-Solve Θ(log logt) times. The i-th call to Sample-And-Solve isSample-And-Solve
(Si−1,pi−1,r) (in particular): it produces a set Si⊆Si−1of centers as the output, where S0=Vtand
the probability parameters are set suitably. The formal algorithm is described in Algorithm 4. The round
complexity, space complexity and approximation guarantee are sta ted in Lemmas 5.1 and 5.2 — they follow
from the guarantees we have for Sample-And-Solve in Lemma 4.1 and the fact that Uniform -Center
(Vt,r,t) callsSample-And-Solve O(log logt) times. Uniform -Center has an additional guarantee as
stated in Lemma 5.3 relating to the reduction of cluster sizes, which p lays a crucial role in proving the
correctness of Ext-k-Center in Section 6. In particular it is useful in bounding the number of cente rs
output by Ext-k-Center .
Algorithm 4: Uniform -Center (Vt,r,t)
Input: A set of points Vtof at mostnpoints, a radius parameter r∈R+, and an additional
parametert≤n.
Output: A setS⊆Vtof centers.
1begin
2p0= Θ/parenleftig
logn
nδ/parenrightig
,s0=t, andS0←Vt.
3fori= 1toτ= Θ(log log t)do
4Si←Sample-And-Solve (Si−1,pi−1,r).
5si←√si−1andpi=1
si.
6end
7 ReportS=Sτ.
8end
Lemma 5.1 (Round complexity and global space of Uniform -Center).Consider Uniform -Center
(Vt,r,t), as described in Algorithm 4. The number of rounds taken by t he algorithm isO(log logt)and the
global space used by the algorithm is g =/tildewideO/parenleftbig
n1+ρ·log ∆/parenrightbig
.
machines. This can be done in 1 round if we have ≥2s/nmachines: when sorting, use only the ﬁrst s /nmachines, then if Bh
was originally stored on machines iandi+1, move it to machine i+s/n.
14Proof. AsUniform -Center (Vt,r,t) callsSample-And-Solve O(log logt) times, this follows directly from
Lemma 4.1 (ii) .
Lemma 5.2 (Approximation guarantee of Uniform -Center).Consider Uniform -Center (Vt,r,t)
as described in Algorithm 4. It produces output Ssuch that Cost (Vt,S) =O(r·log logt).
Proof. In thei-th iteration of Uniform -Center (Vt,r,t), we call Sample-And-Solve (Si−1,pi−1,r), and
getSias the centers. By Lemma 4.1 (i), cost (Si−1,Si) =O(r), where 1≤i≤τ. Hence,
cost (Vt,S)≤τ/summationdisplay
i=1cost (Si−1,Si) =τ·O(r) =O(rlog logt).
Lemma5.3 (Reduction in clustersizes). Consider Uniform -Center (Vt,r,t) as described in Algorithm 4,
and a ﬁxed clustering Ct
rofVtthat has cost r. It produces output S⊆Vtsuch that the following holds for
anyC∈Ct
r: if|C∩Vt|≤t, then with probability at least 1−1
tΩ(1), we have|C∩S|=O/parenleftbig
logt·(log logt)2/parenrightbig
.
Proof. Letbi= (1 +η)isilogt·(log logt)2, whereiis an non-negative integer and η= Θ/parenleftig
1
loglogt/parenrightig
. Observe
that
bτ−1= (1 +η)τ−1sτ−1logt(log logt)2=O/parenleftig
logt·(log logt)2/parenrightig
.
Using induction on i(i∈N), we will show that |C∩Si|≤bi−1for eachiwith 1≤i≤τ, with probability
at least 1−1
tΩ(1). This will imply the desired result as Siis the output after the i-th iteration, and S=Sτ.
Hence,|C∩S|≤bτ−1=O/parenleftbig
logt·(log logt)2/parenrightbig
.
Fori= 1,
|C∩S1|≤|C∩S0|=|C∩Vt|≤t≤b0.
The ﬁrst inequality follows as S1⊆S0; the second equality follows as S0=Vt; the third inequality follows
from the given condition that |C∩Vt|≤t; and the fourth one holds by the deﬁnition of b0.
Suppose the statement holds for each iwith 1≤i≤ℓ−1, that is,|C∩Si|≤bi−1for eachiwith
1≤i≤ℓ−1. Now we argue for i=ℓ. If|C∩Sℓ−1|≤bℓ−1, then|C∩Sℓ|≤bℓ−1, asSℓ⊆Sℓ−1. So, let us
assume that|C∩Sℓ−1|>bℓ−1.
Consider the ℓ-th iteration of Uniform -Center : it calls algorithm Sample-And-Solve (Sℓ−1,pℓ−1,r),
and produces Sℓas the set of intermediate centers. Let Hℓ⊆Sℓ−1be the set of hubs sampled in the
call ofSample-And-Solve (Sℓ−1,pℓ−1,r), where each point in Sℓ−1(independently) included in Hℓwith
probability pℓ−1.
Before proceeding, we make two observations:
Observation 5.4. The probability, that at least one point from C∩Sℓ−1is inHℓ, is at least 1−1
tΩ(1).
Proof. The probability, that no point in C∩Sℓ−1(|C∩Sℓ−1|>bℓ−1) is included in Hℓ, is at most
(1−pℓ−1)bℓ−1=/parenleftbigg
1−1
sℓ−1/parenrightbiggbℓ−1
≤1
tΩ(1).
Here, we have used that bℓ−1= (1 +η)ℓ−1sℓ−1logt·(log logt)2.
Observation 5.5. With probability at least 1 −1
tΩ(1), the number of points in C∩Sℓ−1that are in Hℓis at
mostbℓ−1.
15Proof. By induction hypothesis, |C∩Sℓ−1|≤bℓ−2. The expected number of points of C∩Sℓ−1that are in
Hℓis
|C∩Sℓ−1|·1
sℓ−1≤bℓ−2
sℓ−1
=(1 +η)ℓ−2sℓ−2logt·(log logt)2
sℓ−1
= (1 +η)ℓ−2sℓ−1logt·(log logt)2=µ.
By using Chernoﬀ bound (Lemma A.1), the probability, that the numb er of points of C∩Sℓ−1that are
inHℓis more than (1 + η)µ=bℓ−1, is at most e−η2µ
3≤1
tΩ(1).
Observations 5.4 and 5.5, along with Lemma 4.4, give us that |C∩Sℓ|=|Hℓ|≤bℓ−1with probability at
least 1−1
tΩ(1).
6 The main algorithm
In this section, we present our main algorithm Ext-k-Center . Recall the overall description of Ext-
k-Center in Section 2. Ext-k-Center has two phases. In Phase 1 , it calls Uniform -Centerαtimes,
and inPhase 2 , it calls Sample-And-Solve βtimes, where αis the input precision parameter and β=
Θ(log(α+1)n). The formal algorithm is described in Algorithm 5. We prove the roun d complexity and space
complexity of Ext-k-Center in Lemma 6.1, the approximation guarantee in Lemma 6.2 and the bound on
the number of centers in Lemma 6.3.
Algorithm 5: Ext-k-Center (P,r)
Input: SetPofnpoints; tradeoﬀ parameter α; radius parameter r∈R+.
Output: A setT⊆Pof centers.
1begin
2Phase 1:
3T0←P,t0=n, andr0= log logn.
4for(j= 1toα)do
5 Phase 1.j:
6Tj←Uniform -Center (Tj−1,rj−1,tj−1).
7tj= Θ(logtj−1·(log logtj−1)d+2).
8 // Note that tj=/tildewideΘ(logtj−1) =/tildewideΘ(log(j)n).
9rj=r
loglogtj.
10end
11Phase 2:
12for (i= 1toβ= Θ(log(α+1)n))do
13 Phase 2.i:
14Tα+i←Sample-And-Solve (Tα+i−1,1
2,r).
15end
16 ReportT=Tα+β.
17end
Lemma 6.1 (Round complexity and global space of Ext-k-Center).Consider Ext-k-Center (P,t),
as described in Algorithm 5. The number of rounds taken by the algorithm isO(log logn)and the global space
used by the algorithm is g =/tildewideO/parenleftbig
n1+ρ·log ∆/parenrightbig
.
16Proof. For anyjwith 1≤j≤α, inPhase 1.j ,Ext-k-Center (P,r) callsUniform -Center (Tj−1,rj−1,tj−1).
By Lemma 5.1 (i), the total number of rounds spent by Ext-k-Center (P,r) inPhase 1 is/summationtextα
j=1O(log logtj−1) =
O(log logn). This is because t0=n,tj=/tildewideΘ (logtj−1) for anyj≥1, that is,tj=/tildewideΘ/parenleftig
log(j)n/parenrightig
. InPhase
2,Ext-k-Center (P,r) callsSample-And-Solve forβ= Θ/parenleftig
log(α+1)n/parenrightig
times. By Lemma 4.1 (ii),
the total number of rounds spent by Ext-k-Center (P,r) inPhase 2 isO(β) =O/parenleftig
log(α+1)n/parenrightig
. So,
the round complexity of Ext-k-Center follows. The global space complexity of Ext-k-Center follows
from the global space complexities of Uniform -Center andSample-And-Solve (see Lemma 5.1 and
Lemma 4.1(ii), respectively).
Lemma 6.2 (Approximation guarantee of Ext-k-Center).Let us consider Ext-k-Center (P,r) as
described in Algorithm 5. It produces output T⊆Psuch that Cost (P,T) =O(r·(α+ log(α+1)n)).
Proof. Observe that
cost (P,T) =cost (T0,Tα+β)≤cost (T0,Tα) +cost (Tα,Tα+β).
It therefore suﬃces to show that cost (T0,Tα) andcost (Tα,Tα+β) are bounded by O(rα) andO(r·
log(α+1)n), respectively.
For anyjwith 1≤j≤α, note that Ext-k-Center (P,r) callsUniform -Center (Tj−1,rj−1,tj−1) in
Phase 1.j and produces Tjas the output. So, by Lemma 5.2, cost (Tj−1,Tj) =O(rj−1·log logtj−1), which
isO(r). Hence,
cost (T0,Tα)≤α/summationdisplay
j=1cost (Tj−1,Tj) =α·O(r) =O(r·α).
For anyiwith 1≤i≤β, note that Ext-k-Center (P,r) callsSample-And-Solve (Tα+i−1,1/2,r) in
Phase 2.i and produces Tα+ias the output. So, by Lemma 4.1 (i), cost (Tα+i−1,Tα+i) =O(r).Hence, as
β= Θ(log(α+1)n),
cost (Tα,Tα+β)≤β/summationdisplay
i=1cost (Tα+i−1,Tα+i) =O(r·log(α+1)n).
Lemma 6.3 (Number of centers reported by Ext-k-Center).Consider Ext-k-Center (P,r)as
described in Algorithm 5. It produces output Tsuch that, with probability at least 1−1
(log(α−1)n)Ω(1),
|T|≤|Cr|/parenleftigg
1 +1
/tildewideΘ(log(α)n)/parenrightigg
+/tildewideΘ((log(α)n)3).
Here,Cris a clustering of Pthat has the minimum number of centers among all possible clu stering ofP
with cost at most rsuch that|Cr|= Ω((logn)c), wherecis a suitable constant.
Now, we introduce the notion of active andinactive clusters in the following deﬁnition, which is useful
in proving Lemma 6.3. Inactive clusters are clusters which, at some p oint during Phase 1 , fail to reduce
in size suﬃciently. After the sub-phase during which they fail to red uce in size suﬃciently, we assume that
they never reduce in size again (since this is the worst case). We are then able to bound the total number
of centers in inactive clusters (Lemma 6.6). Active clusters, by con trast, always reduce in size as we expect:
the number of centers in active clusters is therefore easy to boun d.
Deﬁnition 6.4. LetCrbe an optimal clustering with cost at most r. For eachC∈Crandjwith 1≤j≤α,
we sayCisinactive inPhase 1.j if|C∩Ti|>tifor someiwith 1≤i<j . Otherwise, if|C∩Ti|≤tifor
everyiwith 1≤i<j ,Cis called active inPhase 1.j .
17LetC′
r⊆Crbe the set of clusters that are active after Phase 1 , that is, Phase 1.α. By the deﬁnition
of active clusters, for each C∈C′
r,|C∩Tα|≤tα. Note that Ext-k-Center goes overβsub-phases in
Phase 2 . AfterPhase 1 and before the start of Phase 2 , it hasTαas the set of intermediate centers.
For 1≤i≤β, inPhase 2.i , we call Sample-And-Solve/parenleftbig
Tα+i−1,1
2,r/parenrightbig
, and getTα+ias the intermediate
centers. For 0≤i≤β; a cluster C∈C′
ris said to be i-large if|C∩Tα+i|≥2. Let Γ i⊆C′
rdenote
the set ofi-large clusters, and let Yidenote the total number of points that are in i-large clusters, that is,
Yi=/summationtext
C∈Γi|C∩Tα+i|.
Note that, in Lemma 6.3, we want to bound the number of centers in T=Tα+β. We ﬁrst observe that
|T|can be expressed as the sum of three quantities:
Observation 6.5. |T|=|Tα+β|=|Cr|+Yβ+/summationtext
C∈Cr\C′r|C∩Tα|.
Proof. Observe that since Tα+β⊆Tα, we obtain,
Tα+β=/summationdisplay
C∈Cr\C′r|C∩Tα+β|+/summationdisplay
C∈C′r|C∩Tα+β|
≤/summationdisplay
C∈Cr\C′r|C∩Tα|+/summationdisplay
C∈C′r|C∩Tα+β|.
To bound the second inequality by |Cr|+Yβ, observe that
/summationdisplay
C∈C′r|C∩Tα+β|=/summationdisplay
C∈C′r:|C∩Tα+β|=1|C∩Tα+β|+/summationdisplay
C∈C′r:|C∩Tα+β|≥2|C∩Tα+β|
≤|C′
r|+Yβ≤|Cr|+Yβ,
which used thatC′
r⊆Cr. This yields Observation 6.5.
In the following lemmas, we bound/summationtext
C∈Cr\C′
r|C∩Tα|andYβ, and (with Observation 6.5) the result of
Lemma 6.3 immediately follows from these bounds. Lemmas 6.6 and 6.7 ar e technical that we will prove
later.
Lemma 6.6. With probability at least 1−/summationtextα
i=11
tΩ(1)
i−1,/summationtext
C∈Cr\C′r|C∩Tα|isO/parenleftig
|Cr|
(log(α)n)Ω(1)/parenrightig
, that is, the
number of points in Tαthat are present in clusters that are inactive after Phase 1 isO/parenleftig
|Cr|
(log(α)n)Ω(1)/parenrightig
.
Lemma 6.7. With probability at least 1−1
tΩ(1)
α−1, we haveYβ=O/parenleftig
|Cr|
tα+t3
α·logtα/parenrightig
.
Proof of Lemma 6.3 using Lemma 6.6 and Lemma 6.7. From the above two lemmas along with Observation 6.5
and the fact tj=/tildewideΘ(log(j)n), we have the following bound on |T|with probability at least 1 −α/summationtext
i=11
tΩ(1)
i−1≥
1−1
(log(α−1)n)Ω(1):
|T|≤|Cr|/parenleftigg
1 +1
/tildewideΘ(log(α)n)/parenrightigg
+/tildewideΘ/parenleftig
(log(α)n)3/parenrightig
.
Hence, we are done with the proof of Lemma 6.3.
Proof of Lemma 6.6
We prove Lemma 6.6 by using the following lemma, which we prove later.
18Lemma 6.8. Consider Ext-k-Center (P,r) as described in Algorithm 5 and Phase 1 . With probability
at least 1−/summationtextα
i=11
tΩ(1)
i−1, the following holds: the number of clusters C∈Cr, such that Cis active in Phase
1.ibut inactive after Phase 1.i , is at mostO/parenleftbigg
|Cr|
tΩ(1)
i−1/parenrightbigg
, where 1≤i≤α.
Proof of Lemma 6.6 using Lemma 6.8. Let us partition the clusters in C′
rintoA1,...,Aα, whereAiis the
set of clusters that are active in Phase 1.i but inactive after Phase 1.i , where 1≤i≤α. Consider a cluster
C∈Ai. By the deﬁnition of Ai,|C∩Ti−1|≤ti−1. That is, for each C∈Ai,|C∩Tα|≤ti−1, because
T0⊇T1⊇...⊇Tα.
Applying Lemma 6.8 , we have |Ai|=O/parenleftbigg
|Cr|
tΩ(1)
i−1/parenrightbigg
for eachi(1≤i≤α), with probability at least
1−/summationtextα
i=11
tΩ(1)
i−1. Hence, with the same probability,
/summationdisplay
C∈Cr\C′r|C∩Tα|≤α/summationdisplay
i=1/summationdisplay
C∈Ai|Tα∩C|=α/summationdisplay
i=1|Ai|ti−1
=O/parenleftiggα/summationdisplay
i=1|Cr|
tΩ(1)
i−1/parenrightigg
=O
|Cr|
/parenleftig
log(α)n/parenrightigΩ(1)
.
The last step uses that ti=/tildewideΘ/parenleftig
log(i)n/parenrightig
.
Proof of Lemma 6.8. Consideriwith 1≤i≤α. LetAibe the set of clusters that were active in Phase 1.i
but inactive after Phase 1.i , and letBi⊇Aibe the set of clusters that were active in Phase 1.i . It suﬃces
to show that|Ai|=O/parenleftbigg
|Cr|
tΩ(1)
i−1/parenrightbigg
holds with probability at least 1 −1
tΩ(1)
i−1.
ForC∈Bi, letXCbe the random variable deﬁned as
XC=/braceleftigg
1 if|C∩Ti|>ti
0 otherwise.
Observe that|Ai|=/summationtext
C∈BiXC.
Claim 6.9. The probability that XC= 1isO/parenleftbigg
1
tΩ(1)
i−1/parenrightbigg
.
Using the above claim, we prove Lemma 6.8 separately for i= 1 andi≥1.
Fori= 1, applying the union bound over all C∈Bi=B1, the probability that there exists a C∈B1
such thatXC= 1 is at most|B1|
tΩ(1)
0<1
tΩ(1)
0. It is because|B1|≤nandt0=n. This implies that |A1|= 0
with probability at least 1 −1
tΩ(1)
0. Note that we are done for the case i= 1.
For 2≤i≤α,E[|Ai|] =O/parenleftbigg
|Bi|
tΩ(1)
i−1/parenrightbigg
=O/parenleftbigg
|Cr|
tΩ(1)
i−1/parenrightbigg
. Asi≥2,ti−1≤t1=/tildewideO(logn). Recall that, we consider
|Cr|= Ω((logn)c) for a suitable constant c. So, we can assume that|Cr|
tΩ(1)
i−1≥logti−1. Using a Chernoﬀ bound
(Lemma A.1), Pr/parenleftbigg
|Ai|≥c1·|Cr|
tΩ(1)
i−1/parenrightbigg
≤1
tΩ(1)
i−1,wherec1is a suitable large constant. So, we are done with the
proof of Lemma 6.8, except for the proof of Claim 6.9.
Proof of Claim 6.9. Consider the clustering Cti−1ri−1ofTi−1as follows. For each C∈Cr, consider the partition
of clusterCinto at most z=O/parenleftig
(log logti−1)d/parenrightig
many clusters C1....,C zsuch that the radius of each
19Ciis at most ri−1. For each Ci(i∈[z]), the corresponding cluster in Cti−1ri−1isC′
i=Ci∩Ti−1. So,/vextendsingle/vextendsingle/vextendsingleCti−1ri−1/vextendsingle/vextendsingle/vextendsingle=|Cr|·O/parenleftig
(log logti−1)d/parenrightig
.
Consider the particular C∈Bi. By Deﬁnition 6.4, |C∩Ti−1|≤ti−1. If we consider the partition of C
intoC1,...,C zinCti−1ri−1, then|Cy∩Ti−1|≤ti−1for eachy∈[z]. LetB′
ibe the set of clusters in Cti−1ri−1that
are formed due to the partition of some C∈Bi. So, for each C′∈B′
i, we have|C′∩Ti−1|≤ti−1.
Consider Phase 1.i ofExt-k-Center : we getTias the current set of centers by calling Uniform -Center (Ti−1,ri−1,ti−1).
Let us apply Lemma 5.3 with t=ti−1,Vt=Ti−1,r=ri−1,S=Ti, andCt
r=Ctk−1rk−1. Fory∈[z], with
probability at least 1 −1
tΩ(1)
i−1, we have|Cy∩Ti|=O/parenleftig
logti−1·(log logti−1)2/parenrightig
. Applying union bound for all
y∈[z], with probability at least 1 −1
tΩ(1)
i−1, we have
|C∩Ti|=/summationdisplay
y∈[z]|Cy∩Ti|=O/parenleftig
logti−1·(log logti−1)d+2/parenrightig
=ti.
This is because z=O/parenleftbig
(log logti−1)d+2/parenrightbig
, and we are done with the proof of the claim.
This completes the proof of Lemma 6.6.
Proof of Lemma 6.7
We prove Lemma 6.7 by using the following lemma, which we prove later.
Lemma 6.10. Letζ∈(0,1)be a suitable constant, let ibe such that 1≤i≤βwithYi−1= Ω/parenleftbig
t3
αlogtα/parenrightbig
.
With probability 1−1
tΩ(1)
α,Yi≤/summationtext
C∈Γi−1|C∩Tα+i|≤ζ·Yi−1.
Applying the union bound over all i’s in 1 toβ, with probability at least 1 −1
tΩ(1)
α, we haveYβ=
ζβY0+t3
αlogtα.AsY0=O(|C′
r|·tα) =O(|Cr|·tα),β= Θ/parenleftig
log(α+1)n/parenrightig
andζis chosen suitably, we are
done with the proof of Lemma 6.7.
Proof of Lemma 6.10. AsYi=/summationtext
C∈Γi|C∩Tα+i|and Γi⊆Γi−1,Yi≤/summationtext
C∈Γi−1|C∩Tα+i|follows.
For the other part,/summationtext
C∈Γi−1|C∩Tα+i|≤ζ·Yi−1, letZC=|C∩Tα+i|andZ=/summationtext
C∈Γi−1ZC. Now
consider the following claim, which we will prove right after proving Lem ma 6.10.
Claim 6.11. LetC∈Γi−1. The probability that ZC=|C∩Tα+i|≤3
4·|C∩Tα+i−1|is at least a constant
κ∈(0,1).
Note thatTα+i⊆Tα+i−1. So,|C∩Tα+i|≤|C∩Tα+i−1|always. For C∈Γi−1, note that|Tα+i−1|≤tα.
From the above claim
E[ZC] =E[|C∩Tα+i|]
≤κ·3
4|C∩Tα+i−1|+ (1−κ)·|C∩Tα+i−1|
≤ζ′|C∩Tα+i−1|.
whereζ′is a suitable constant. Recalling the deﬁnition of Yi−1, we have
E[Z]≤ζ′/summationdisplay
C∈Γi−1|C∩Tα+i−1|=ζ′Yi−1.
Moreover, 0≤ZC≤tαfor eachC∈Γi−1. Hence, applying a Hoeﬀding bound (Lemma A.2), we have
Pr (Z≥ζYi−1) = Pr (Z≥E[Z] +ζ1Yi−1)
≤e−ζ2
1Y2
i−1
|Γi−1|t2α≤e−ζ2
1Y2
i−1
|Γi−1|t2α≤1/tΩ(1)
α−1.
20The last inequality folllows from Yi−1≥2|Γi−1|andYi−1= Ω(t3
α·logtα). This concludes the proof of
Lemma 6.10 since we have Z=/summationtext
C∈Γi−1|C∩Tα+i|.
We are left with only the proof of Claim 6.11.
Proof of Claim 6.11. We prove
(i) Pr/parenleftbig
ZC≤3
4|C∩Tα+i−1|/parenrightbig
≥Pr(ZC= 1) =|C∩Tα+i−1|
2|C∩Tα+i−1|;
(ii) Pr/parenleftbig
ZC≤3
4|C∩Tα+i−1|/parenrightbig
≥1−1
2Ω(|C∩Tα+i−1|).
From the above two statements, we are done with the claim by settin gκ= max/braceleftig
|C∩Tα+i−1|
2|C∩Tα+i−1|,1−e−Ω(|C∩Tα+i−1|)/bracerightig
,
which is Ω(1).
Note that Ext-k-Center callsSample-And-Solve/parenleftbig
Tα+i−1,1
2,r/parenrightbig
inPhase 2.i . InSample-And-Solve/parenleftbig
Tα+i−1,1
2,r/parenrightbig
,
letHi⊆Tα+i−1be the set of hubs sampled, where each point in Tα+i−1is (independently) included in Hi
with probability1
2.
For (i), Pr/parenleftbig
ZC≤3
4|C∩Tα+i−1|/parenrightbig
≥Pr(ZC= 1) is direct as |C∩Tα+i−1| ≥ 2. From Lemma 4.4,
ZC=|C∩Tα+i|= 1 if|Hi|= 1. So,
Pr(ZC= 1) =|C∩Tα+i−1|
2|C∩Tα+i−1|.
Now, we will prove (ii). From Lemma 4.4, ZC=|C∩Tα+i|=|Hi|if|Hi|>0. Observe that Pr( Hi>
0) = 1−1
2|C∩Tα+i−1|. The expected number of points in Hiis|C∩Tα+i−1|
2. Using Chernoﬀ bound (Lemma A.1),
Pr/parenleftbig
|Hi|>3
4|C∩Tα+i−1|/parenrightbig
≤e−Ω(|C∩Tα+i−1|).Hence, putting things together,
Pr/parenleftbigg
ZC≤3
4|C∩Tα+i−1|/parenrightbigg
≥Pr(|Hi|>0)·Pr/parenleftbigg
|Hi|≤3
4|C∩Tα+i−1|/parenrightbigg
≥/parenleftbigg
1−1
2|C∩Tα+i−1|/parenrightbigg
·/parenleftbigg
1−1
eΩ(|C∩Tα+i−1|)/parenrightbigg
≥1−1
2Ω(|C∩Tα+i−1|).
7 Conclusions
In this paper we show that even for large values of k, the classic k-center clustering problem in low-
dimensional Euclidean space can be eﬃciently and very well approxima ted in the parallel setting of low-
local-space MPC . While some earlier works (see, e.g., [EIM11, MKC+15, CPP19]) were able to obtain
constant-round MPC algorithms, they were relying on a large local space s ≫kallowing to successfully
apply the core-set approach, which permits only limited communicatio n. On the other hand, the low-local-
space setting considered in this paper seems to require extensive c ommunication between the machines
to achieve any reasonable approximation guarantees. Therefore we believe (without any evidence) that the
number of rounds of order O(log logn) may be almost as good as it gets. Also, we concede that our algorith m
does not achieve a constant approximation guarantee, but we fee l the approximation bound of O(log∗n) is
almost as good. Finally, our algorithm does not resolve the perfect s etting of the k-center clustering in that
it allows in the solution slightly more centers, k+o(k) centers. Improving on these three parameters is the
main open problem left by our work.
We believe that solely using the technique in this paper, improving the a pproximation factor and/or
number of rounds may not be possible (a detailed explanation is in Appe ndix C), but the approach may
be useful for related problems in MPC or other models. We remark that the extra space in global space
21complexity is mainly due to the use of LSH; note that, even in the RAM m odel setting, the use of LSH
requires some extra space.
Our work naturally suggests some open directions for future rese arch:
•Can we improve the approximation factor beyond O(log∗n) and/or the number of rounds beyond
O(log logn)?
•In the large kregime, can we design an eﬃcient algorithm that uses (almost) linear g lobal space?
•In the large kregime, can we design an eﬃcient algorithm that reports exactly kcenters?
•Are similar results possible for the related k-means and k-medians problems for large kinMPC ?
•WhatMPC results are possible when the points are in high-dimensional Euclidean space or in a general
metric space? Our work has a limitation to go beyond constant dimens ion as we are not aware of any
eﬃcient LSH for high dimension.
References
[ANOY14] Alexandr Andoni, Aleksandar Nikolov, Krzysztof Onak, an d Grigory Yaroslavtsev. Parallel
algorithms for geometric graph problems. In Proceedings of the 46th Annual ACM Symposium
on Theory of Computing (STOC) , pages 574–583, 2014.
[BEFM21] MohammadHossein Bateni, Hossein Esfandiari, Manuela Fisc her, and Vahab Mirrokni. Extreme
k-center clustering. In Proceedings of the 35th AAAI Conference on Artiﬁcial Intell igence(AAAI) ,
pages 3941–3949, 2021.
[BKS17] Paul Beame, Paraschos Koutris, and Dan Suciu. Communica tion steps for parallel query
processing. Journal of the ACM , 64(6):40:1–40:58, 2017.
[BT10] Guy E Blelloch and Kanat Tangwongsan. Parallel approximation algorithms for facility-location
problems. In Proceedings of the twenty-second annual ACM symposium on Pa rallelism in
algorithms and architectures , pages 315–324, 2010.
[C/suppress LM+18] Artur Czumaj, Jakub /suppress L¸ acki, Aleksander M¸ adry, Slobodan M itrovi´ c, Krzysztof Onak, and Piotr
Sankowski. Round compression for parallel matching algorithms. In Proceedings of the 50th
Annual ACM Symposium on Theory of Computing (STOC) , pages 471–484, 2018.
[CPP19] Matteo Ceccarello, Andrea Pietracaprina, and Geppino Puc ci. Solving k-center clustering (with
outliers) in MapReduce and streaming, almost as accurately as sequ entially. Proceedings of the
VLDB Endowment , 12(7):766–778, 2019.
[CSWZ16] Jiecao Chen, He Sun, David P. Woodruﬀ, and Qin Zhang. Com munication-optimal distributed
clustering. In Proceedings of the 29th Annual Conference on Neural Informa tion Processing
Systems (NIPS) , pages 3720–3728, 2016.
[DF85] Martin E. Dyer and Alan M. Frieze. A simple heuristic for the p-centre problem. Operations
Research Letters , 3(6):285–288, 1985.
[DG08] Jeﬀrey Dean and Sanjay Ghemawat. MapReduce: Simpliﬁed da ta processing on large clusters.
Communications of the ACM , 51(1):107–113, January 2008.
[EIM11] Alina Ene, Sungjin Im, and Benjamin Moseley. Fast clustering using MapReduce. In Proceedings
of the 17th ACM SIGKDD International Conference on Knowledg e Discovery and Data Mining
(KDD) , pages 681–689, 2011.
22[Gon85] Teoﬁlo F. Gonzalez. Clustering to minimize the maximum interclu ster distance. Theoretical
Computer Science , 38:293–306, 1985.
[GSZ11] Michael T. Goodrich, Nodari Sitchinava, and Qin Zhang. Sor ting, searching, and simulation in
the MapReduce framework. In Proceedings of the 22nd International Symposium on Algorit hms
and Computation (ISAAC) , pages 374–383, 2011.
[HIM12] Sariel Har-Peled, Piotr Indyk, and Rajeev Motwani. Appro ximate nearest neighbor: Towards
removing the curse of dimensionality. Theoretical Computer Science , 8(1):321–350, 2012.
[HN79] Wen-Lian Hsu and George L. Nemhauser. Easy and hard bott leneck location problems. Discrete
Applied Mathematics , 1(3):209–215, 1979.
[HS85] Dorit S. Hochbaum and David B. Shmoys. A best possible heuris tic for thek-center problem.
Mathematics of Operations Research , 10(2):180–184, 1985.
[IBY+07] Michael Isard, Mihai Budiu, Yuan Yu, Andrew Birrell, and Dennis F etterly. Dryad: Distributed
data-parallel programs from sequential building blocks. SIGOPS Operating Systems Review ,
41(3):59–72, March 2007.
[KSV10] Howard J. Karloﬀ, Siddharth Suri, and Sergei Vassilvitskii. A model of computation for
MapReduce. In Proceedings of the 21st Annual ACM-SIAM Symposium on Discre te Algorithms
(SODA) , pages 938–948, 2010.
[MKC+15] Gustavo Malkomes, Matt J. Kusner, Wenlin Chen, Kilian Q. Weinber ger, and Benjamin Moseley.
Fast distributed k-center clustering with outliers on massive data. In Proceedings of the 28th
Annual Conference on Neural Information Processing System s (NIPS) , pages 1063–1071, 2015.
[Whi15] Tom White. Hadoop: The Deﬁnitive Guide: Storage and Analysis at Intern et Scale . O’Reilly
Media, Sebastopol, CA, 4th edition, 2015.
[WLF+09] Haofen Wang, Yan Liang, Linyun Fu, Gui-Rong Xue, and Yong Yu. Eﬃcient query expansion for
advertisement search. In Proceedings of the 32nd Annual International ACM SIGIR Conf erence
on Research and Development in Information Retrieval (SIGI R), pages 51–58, 2009.
[XW05] Rui Xu and Donald Wunsch. Survey of clustering algorithms. IEEE Transactions on Neural
Networks , 16(3):645–678, 2005.
[ZCF+10] Matei Zaharia, Mosharaf Chowdhury, Michael J. Franklin, Sco tt Shenker, and Ion Stoica. Spark:
Cluster computing with working sets. In Proceedings of the 2nd USENIX Workshop on Hot
Topics in Cloud Computing (HotCloud) , 2010.
Appendix
A Concentration inequalities
In our analysis we use some basic and standard concentration inequ alities, which we present here for the
sake of completeness.
Lemma A.1 (Chernoﬀ bound). LetX1,...,X nbe independent random variables such that Xi∈[0,1].
ForX=/summationtextn
i=1Xiandµl≤E[X]≤µh, the following hold for any ε>0.
Pr(X≥(1 +ε)µh)≤e−µhε2/3.
23Lemma A.2 (Hoeﬀding bound). LetX1,...,X nbe independent random variables such that ai≤Xi≤bi
andX=/summationtextn
i=1Xi. Then, for all ε>0,
Pr(|X−E[X]|≥ε)≤2 exp/parenleftigg
−2ε2/n/summationdisplay
i=1(bi−ai)2/parenrightigg
.
B Proof of Theorem 1.5
In this section we prove Theorem 1.5 using Theorem 2.1.
Let us consider algorithm Ext-k-Center′that runsψinstances,ψ=O(log(max{n,log ∆})), of the
algorithm Ext-k-Center in parallel. Let T(1),...,T (ψ) be the outputs of the runs of Ext-k-Center .
Ext-k-Center′reportsT′=T(i) with the minimum cardinality as the output. Therefore by Theorem 2 .1,
with probability at least 1 −1
(max{n,log∆})Ω(1), the following is true for Ext-k-Center′:
(i) the number of rounds spent by Ext-k-Center′isO(log logn) and the global space used by Ext-k-Center′
is/tildewideO(n1+ρlog ∆);
(ii)cost (P,T′) =O(r·(α+ log(α+1)n)); and
(iii)|T′|≤|Cr|(1 +1
/tildewideΘ(log(α)n)) +/tildewideΘ((log(α)n)3).
Next, consider the following observation about Ext-k-Center′:
Observation B.1. Letopt be the optimal cost to the k-center problem for P. If one runs Ext-k-Center′
with radius parameter rwithr≥opt, then the number of centers reported by Ext-k-Center′is at most
k/parenleftig
1 +1
/tildewideΘ(log(α)n)/parenrightig
+/tildewideΘ((log(α)n)3), with probability at least 1 −1
(max{n,log∆})Ω(1).
Observation B.1 follows from the bound |Cr|≤kforr≥opt.
Now we describe the algorithm algorithm Ext-k-Center′′, which is the algorithm corresponding to
Theorem 1.5. Ext-k-Center′′runsφ=O(log ∆) instances of Ext-k-Center′, with radius parameters
r(1) = ∆,r(2) =∆
2,r(3) =∆
4,...,r (φ) =O(1), in parallel. Let T′(1),...,T′(φ) be the corresponding
outputs of the runs of Ext-k-Center′.Ext-k-Center′′reportsT′′=T′(i) as the output such that
|T′(i)|≤k(1 +1
/tildewideΘ(log(α)n)) +/tildewideΘ((log(α)n)3) andr(i) is the minimum. So, the round complexity and space
complexity of Theorem 1.5 follow from the round and space complexity guarantee of Ext-k-Center′,
respectively. From the guarantee of algorithm Ext-k-Center′about the set of centers returned by it, we
havecost (P,T′′) =r(i)·(α+(log(α+1)n)), and|T′′|≤k(1+1
/tildewideΘ(log(α)n)) +/tildewideΘ((log(α)n)3), with probability at
least 1−1
(max{n,log∆})Ω(1). From Observation B.1, r(i)≤2·optwith probability at least 1 −1
(max{n,log∆})Ω(1).
This yields the proof of guarantee on approximation factor and the number of centers of Theorem 1.5.
C Limitation of the techniques
Our algorithm is an iterative algorithm that reﬁnes the set of center s starting with the entire point set
as the set of centers. As with many distributed algorithms, the iter ative approach usually does not lead to
a constant round algorithm. In particular, roughly speaking, our a lgorithm ﬁrst samples points with inverse
polynomial probability and then increases probability in a square root fashion. So, to go from ncenters to
kor evenO(k) centers, one needs Ω(log log n)) rounds. Increasing the probability along a faster schedule
is unlikely to allow us to suﬃciently bound the number of centers while inc reasing the probability along a
slower schedule will increase the running time. Note also that the cos t of the solution gets added over the
rounds in our algorithm. The way our algorithm works, the cost of th e solution in the ﬁrst phase is O(r).
After that, each optimal cluster has at most /tildewideO(log∗n) centers. In phase 2, we apply Sample-And-Solve
24forO(log∗n) rounds with ras the radius that leads to approximation ratio O(log∗n). One may think to
applySample-And-Solve in Phase 2 with a radius parameter less than rin Phase 2. But in that case,
guaranteeing the total number of centers to be k+o(k) seems unlikely.
25