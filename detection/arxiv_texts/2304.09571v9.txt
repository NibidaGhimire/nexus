IEEE TRANSACTIONS ON MULTIMEDIA 1
LLIC: Large Receptive Field Transform Coding with
Adaptive Weights for Learned Image Compression
Wei Jiang , Peirong Ning , Jiayu Yang , Yongqi Zhai , Feng Gao ,Member, IEEE ,
Ronggang Wang ,Member, IEEE
Abstract —The effective receptive field (ERF) plays an important
role in transform coding, which determines how much redundancy
can be removed during transform and how many spatial
priors can be utilized to synthesize textures during inverse
transform. Existing methods rely on stacks of small kernels, whose
ERFs remain insufficiently large, or heavy non-local attention
mechanisms, which limit the potential of high-resolution image
coding. To tackle this issue, we propose Large Receptive Field
Transform Coding with Adaptive Weights for Learned Image
Compression (LLIC). Specifically, for the first time in the learned
image compression community, we introduce a few large kernel-
based depth-wise convolutions to reduce more redundancy while
maintaining modest complexity. Due to the wide range of image
diversity, we further propose a mechanism to augment convolution
adaptability through the self-conditioned generation of weights.
The large kernels cooperate with non-linear embedding and
gate mechanisms for better expressiveness and lighter point-
wise interactions. Our investigation extends to refined training
methods that unlock the full potential of these large kernels.
Moreover, to promote more dynamic inter-channel interactions,
we introduce an adaptive channel-wise bit allocation strategy
that autonomously generates channel importance factors in a
self-conditioned manner. To demonstrate the effectiveness of
the proposed transform coding, we align the entropy model
to compare with existing transform methods and obtain models
LLIC-STF, LLIC-ELIC, and LLIC-TCM. Extensive experiments
demonstrate that our proposed LLIC models have significant
improvements over the corresponding baselines and reduce the
BD-Rate by 9.49% ,9.47% ,10.94% on Kodak over VTM-17.0
Intra, respectively. Our LLIC models achieve state-of-the-art
performances and better trade-offs between performance and
complexity.
Index Terms —Transform coding, Learned image compression.
I. I NTRODUCTION
Learned image compression [1]–[6] has become an active
research area in recent years. Several models [7]–[15] have sur-
passed the advanced non-learned image codec Versatile Video
This work is financially supported for Outstanding Talents Training
Fund in Shenzhen, Shenzhen Science and Technology Program-Shenzhen
Cultivation of Excellent Scientific and Technological Innovation Talents
project (Grant No. RCJC20200714114435057) , Shenzhen Science and
Technology Program-Shenzhen Hong Kong joint funding project (Grant
No. SGDX20211123144400001), National Natural Science Foundation of
China U21B2012, R24115SG MIGU-PKU META VISION TECHNOLOGY
INNOV ATION LAB.
Wei Jiang, Peirong Ning, Jiayu Yang, Yongqi Zhai, Ronggang Wang are with
School of Electronic and Computer Engineering, Peking University, 518055
Shenzhen, China (email: wei.jiang1999@outlook.com).
Jiayu Yang, Yongqi Zhai, Ronggang Wang are with Peng Cheng Laboratory,
518000 Shenzhen, China (email: rgwang@pkusz.edu.cn).
Feng Gao is with School of Arts, Peking University, 100871, Beijing, China.
Ronggang Wang is the corresponding author.
Digital Object Identifier 10.1109/TMM.2024.3416831
2.5 5.0 7.5 10.0 12.5 15.0
Peak GPU Memory Consumption [GB]10
5
0
5
10BD-Rate over VTM-17.0 Intra [%]
Cheng'20(CVPR'20)Qian'21(ICLR'21)
Xie'21(ACMMM'21)
NeuralSyntax(CVPR'22)WACNN(CVPR'22)
STF(CVPR'22)ELIC(CVPR'22)LIC-TCM Large
    (CVPR'23)LLIC-ELIC(Ours)
Real-time lineThe Performance vs. Peak GPU Memory ConsumptionFig. 1. BD-Rate-peak GPU Memory Consumption during testing on CLIC
Pro Valid [19] with 2K resolution. Our LLIC-ELIC achieves a better trade-off
between performance and GPU memory consumption.
Coding (VVC) Intra [16]. Most learned image compression
models are based on variational autoencoders (V AEs) [17]. In
this framework, an analysis transform first converts the input
image to a latent representation, which is then quantized for
entropy coding. A synthesis transform subsequently maps the
quantized latent representation back to pixels. The advantages
of learned image compression over non-learned codecs are the
end-to-end optimization and non-linear transform coding [18].
Recent improvements in the rate-distortion performance of
learned image compression can be attributed to advancements
in nonlinear transform coding [18]. The inherent non-linearity
of such coding empowers the conversion of an input image to
a more compact latent representation, typically requiring fewer
bits for compression. The receptive field plays a crucial role in
transform coding because it determines the maximum amount
of redundancy that can be removed and the compactness of
the latent representation. Furthermore, a large receptive field
facilitates the generation of more accurate textures from more
spatial priors during the synthesis transform. The effective
receptive field is particularly important for high-resolution
image coding due to the presence of more spatial correlations.
For instance, in VVC, the largest size of coding unit is 128×128
(64×64in High Efficiency Video Coding), ensuring that long-
range dependencies can be captured in high-resolution images.
Most earlier nonlinear transform coding techniques [1], [3]–
[5], [9], [11]–[14], [20]–[23] rely on stacks of small kernelsarXiv:2304.09571v9  [cs.CV]  21 Jun 2024IEEE TRANSACTIONS ON MULTIMEDIA 2
(e.g., 3×3,5×5) to enlarge the receptive field. However,
the effective receptive fields (ERFs) [24] of stacks of small
kernels remain limited, as illustrated in Fig. 8. To enlarge the
receptive field, non-local attention [15], [25], [26] has been
employed; however, its inherent quadratic complexity limits the
potential for high-resolution image coding. Overall, enlarging
the effective receptive field of nonlinear transform coding with
acceptable complexity remains a challenge.
To our knowledge, large kernel design with the aim of
achieving a large receptive field for transform coding has
been overlooked. Could the application of a few larger kernels
potentially confers advantages over the conventional approach
of integrating numerous smaller kernels in transform coding?
Additionally, due to the wide range of image diversity, the
optimal transform coding for each image may differ. Compared
with existing attention-based or transformer-based transform
coding techniques [7], [8], [12], [15], [20], [26], the fixed
convolutional weights used during testing are less flexible and
adaptive. Based on above considerations, we introduce a novel
Spatial Transform Block (STB) with large receptive fields
and adaptability. Specifically, for the first time in the learned
image compression community, we propose the application
of depth-wise large kernels whose sizes range from 9×9
to11×11to achieve a good trade-off between performance
and efficiency. To enhance the adaptability of convolution
weights, we propose generating depth-wise kernel weights by
using the input feature as the condition with a progressive
down-sampling strategy. To argument the non-linearity of the
transform, we propose the depth-wise residual block for non-
linear embedding. Moreover, a gate block is proposed for low-
complexity point-wise interactions. We also propose a simple
yet effective training technique that uses large patches to fully
exploit the potential of large kernels. Thanks to the proposed
spatial transform with large receptive fields and adaptability,
our proposed transform coding achieves much larger effective
receptive fields with smaller depth, as illustrated in Fig. 8. A
more widely distributed green area indicates a larger ERF. A
larger ERF indicates that more spatial contextual information
is captured and utilized during the transform, thus making
the latent representation more compact, which means that
it requires fewer bits to compress an image. Moreover, the
complexity of the proposed transform coding method is still
modest.
Large adaptive depth-wise kernels are effective at reducing
spatial redundancy; however, the adaptive interactions among
channels are limited. Given the heterogeneity of information
carried by different channels in a latent representation, a more
targeted approach that dynamically allocates more bits to more
informative channels can enhance coding efficiency. To address
this issue, inspired by Channel Attention [27], we propose
a novel adaptive channel transform block (CTB). The CTB
shares the macro architecture of the STB, where non-linear
embedding and gate block are also employed. Specifically, it
generates channel importance factors through a condition-based,
progressive down-sampling process, subsequently applying
these factors to the latent representation to modulate channel-
wise importance dynamically.
To validate the effectiveness of the proposed transformcoding, we combine the proposed transform coding techniques
with the entropy models of STF [8], ELIC [12], and LIC-
TCM [15] and obtain the learned image compression models
LLIC-STF, LLIC-ELIC, and LLIC-TCM for fair comparisons
with existing state-of-the-art transform coding methods. Ex-
tensive experiments demonstrate that our models significantly
improve upon the corresponding baselines, especially on high-
resolution images. Our LLIC models achieve state-of-the-art
performance regarding rate-distortion performance and model
complexity (Fig. 1, 10, Table I).
Our contributions are as follows:
•We introduce the Spatial Transform Block, utilizing 11×11
and9×9large receptive field transforms to reduce spatial
redundancy. To our knowledge, this is the first time that
large kernels have been employed in the learned image
compression community.
•We propose the generation of depthwise convolutional
weights in a progressive down-sampling manner, using
the input itself as a condition, making CNNs adaptive.
•We propose the Channel Transform Block, which employs
importance factors for self-conditioned adaptive channel-
wise bit allocation in a progressive down-sampling manner,
using the input itself as a condition.
•We propose the use of the Depth-wise Residual Bottleneck
to enhance the non-linearity and the Gate block for
efficient point-wise interaction.
•Extensive experiments demonstrate that our proposed
LLIC-STF, LLIC-ELIC, and LLIC-TCM have significant
improvements over corresponding baselines and reduce
BD-Rate by 9.49%,9.47%, and 10.94% on Kodak over
VTM-17.0 Intra, respectively. Our LLIC models achieve
state-of-the-art performances and better trade-off between
performance and complexity.
II. R ELATED WORKS
A. Learned Image Compression
Toderici et al. [28], [29] propose the first learned image
compression model based on recurrent neural networks, which
encodes the residual between the reconstruction and ground-
truth iteratively. Currently, most of the end-to-end learned
image compression models [1]–[5], [12], [13], [20] are based
on auto-encoders, where the input image is first transformed to
the latent space for entropy coding, and the decoded latent is
inversely transformed to the RGB color space. To enhance the
rate-distortion performance, Ball ´eet al. [3] introduce a hyper-
prior for more accurate entropy coding. Context modeling [4]
is also utilized to explore the correlations between current
symbols and decoded symbols. Minnen et al. [4] employ
pixel-cnn [30] for serial context prediction. He et al. [31]
propose checkerboard context partition for parallel context
modeling. Minnen et al. [5] propose conducting serial context
modeling along the channel dimension for faster decoding.
In addition, global context modeling [13], [14], [32] is also
introduced to explore the correlations among distant symbols.
Multi-dimensional context modeling is recently developed. He
el al. [12] combine the checkerboard partition and channel-wiseIEEE TRANSACTIONS ON MULTIMEDIA 3
Q─AEAD+
Entropy ModelDownsampling
Downsampling
Downsampling
DownsamplingStage 1Basic block×1Stage 2Basic block×1Stage 3Basic block×3Stage 4Basic block×1Upsampling
Upsampling
Upsampling
UpsamplingStage 8Inverse Basic block×1Stage 7Inverse Basic block×1Stage 6Inverse Basic block×3Stage 5Inverse Basic block×1
Fig. 2. Network architecture of our LLIC-STF, LLIC-ELIC, and LLIC-TCM. gais the analysis transform. gsis the synthesis transform. Qis the quantization.
µandσare the estimated mean and scale of latent ˆyfor probability estimation. Following baseline models, the latent representation ysubtracts the means µ
for quantization before arithmetic encoding (AE), and the decoded residual Q(y−µ)adds the means µafter arithmetic decoding (AD). N= 192 , M= 320 .
context modeling. Jiang et al. [13], [14] propose the multi-
reference entropy modeling to capture the local, global, and
channel-wise correlations in an entropy model.
B. Learned Transform Coding
The advantage of end-to-end learned image compression
over non-learned codecs is its non-linear transform coding [18].
The transform plays an important role in improving the rate-
distortion performance. For enhanced non-linearity, techniques
such as generalized divisive normalization (GDN) [33] and
Residual Bottleneck [34] have been utilized in learned image
compression. Moreover, innovative normalization methods [35]
have been proposed in recent years. Furthermore, Cheng et al.
[20] suggest employing pixel shuffle for improved up-sampling.
Various approaches [21], [36], [37] have been proposed for
better interactions between high-frequency and low-frequency
features. For example, Akbari et al. [36] propose employing
octave convolution [38] to preserve more spatial structure of the
information. Gao et al. [37] decompose the images into several
layers with different frequency attributes for greater adaptability.
Xie et al. [22] propose the adoption of invertible neural
networks to reduce information loss during transform. Inspired
by non-learned codecs, wavelet-like transform [39] is also
introduced in recent years. In addition, non-local attention [26],
simplified attention [20], and group-separated attention [32] are
employed to reduce more spatial redundancy. However, such
attention mechanisms are much heavier and lead to greater
complexity. Transformers [40], [41] have been employed in
several works [7], [8], [15], [42]–[45]. For example, Zou et
al.[8], Lu et al. [44], Zhu et al. [7], and Wang et al. [45]
stack swin-transformer [41] layers in transforms to reduce more
redundancy. Liu et al. [15] employ mixed CNN-Transformer
architectures for enhanced local and non-local interactions.
The dynamic weights and large receptive field of transformers
contribute significantly to the overall performance enhancement.
However, to our knowledge, the large kernels are still
not explored in learned image compression. Employing large
kernels may lead to larger effective receptive fields, which
implies that more redundancy can be reduced. Furthermore,
how to utilize large kernels without leading to high complexity
isnon-trivial .III. M ETHOD
A. Problem Formulation
The formulation of end-to-end optimized image compres-
sion is first introduced. The fundamental architecture of
this paradigm comprises three key components: an analysis
transform ga, a synthesis transform gs, and an entropy model p.
The entropy model pcomputes the means µand scales σfor
probability estimation. The input image xis first transformed
to the latent representation yvia analysis transform ga. The
latent representation is quantized to ˆyfor entropy coding. ˆy
is inverse transformed to the reconstructed image ˆxvia the
synthesis transform gs. To ensure the differentiability of the
model during the training phase, the quantization operation is
replaced by either the addition of uniform noise (AUN) [3]
or the straight-through estimator (STE) [2]. Notably, adding
uniform noise u∼ U(−0.5,0.5)[3] to latent representation
makes optimization of the compression model for rate-distortion
performance equivalent to the minimization of the KL diver-
gence (in V AEs [17]):
L=−(R+λ× D)
=Eq(˜y|x)"
logp(x|˜y)|{z}
distortion+ log p(˜y)|{z}
rate−logq(˜y|x|{z}
0)#
.(1)
logp(x|˜y)is considered to be distortion because
p(x|˜y) =N(x|˜x,(2λ)−11), (2)
which is the mean square error (MSE). Eq(˜y|x)logp(˜y)is the
cross entropy, which is the theoretical bound of entropy coding.
q(˜y|x) =q(˜y|y) =U(˜y|y−0.5,y+ 0.5) = 1 . (3)
Currently, most of the learned image compression models [3]–
[5], [8], [12], [13], [20], [22] also incorporate hyper-priors [3].
The side information zis extracted from yusing a hyper-prior
network. The side information ˜zorˆzhelps to estimate theIEEE TRANSACTIONS ON MULTIMEDIA 4
entropy of latent ˜yorˆy. The loss function during training
becomes
L=−(R+λ× D)
=Eq(˜y,˜z|x)"
logp(x|˜y)|{z}
distortion+ log p(˜y|˜z) + log p(˜z)| {z }
rate
−logq(˜y|x) + log q(˜z|y| {z }
0)#
.(4)
Compared with traditional image codecs [16], [46]–[48],
the advantage of the learned image compression is its non-
linear transform coding [18]. In such a framework, the analysis
transform and synthesis transform play important roles.
First, the transform is employed to de-correlate the input
image, effectively reducing correlations and thus enabling a
more compact latent representation. This, in turn, contributes
to a reduced bit-rate necessary for compression. Despite
these advancements, the receptive fields of most current
learned image compression models remain somewhat limited,
leading to the persistence of certain redundancies within the
latent representation. Moreover, the inflexibility of transform
module weights during inference, due to their fixed nature,
significantly impairs content adaptability, thereby constraining
overall performance.
Second, from the perspective of generative models, the
synthesis transform is conceptualized as a generator that plays
a pivotal role in influencing the quality of the reconstructed
image. A powerful and expressive synthesis transform has
the capacity to produce finer details from the input latent
representation ˆy, enhancing the visual quality of the output.
Third, most of the analysis transforms down-sample the
input four times, which makes the resolution of the latent
representation ymuch smaller than that of the input image
x. The complexity of image compression models is primarily
attributable to transform coding. For instance, as demonstrated
by Cheng’20 [20], the forward MACs (MACs) required by
their model for an input image of size 768×512 reach
415.61GMACs, in stark contrast to the mere 0.53GMACs
necessitated by the entropy model. Thus, the development of a
more potent transform module that can minimize redundancies
while maintaining a reasonable model complexity presents a
considerable challenge.
These challenges, along with the potential to further enhance
the rate-distortion performance of learned image compression,
motivate us to design large receptive field transform coding with
self-conditioned adaptability for learned image compression.
B. Overview of Our Approach
The overall architecture of the proposed LLIC-STF, LLIC-
ELIC, and LLIC-TCM is presented in Fig. 2 and Fig. 3. Our
baseline models employ three types of state-of-the-art transform
coding techniques. STF [8] employs transformers [41] for
more compact latent representations. ELIC [12] employs
residual convolutional layers and attention techniques [20] to
enhance the non-linearity. LIC-TCM [15] employs mixed CNN-
transformer architectures to focus on the local and non-localredundancies. For fair comparisons with existing transform
coding techniques, the entropy models of LLIC-STF, LLIC-
ELIC, and LLIC-TCM are aligned with the entropy models of
STF [8], ELIC [12], and LIC-TCM [15], respectively.
Aligning with our baseline models and established meth-
ods [3]–[5], [13], [20], the proposed analysis and synthesis
transforms in our work encompass four stages. Within each
stage, the input undergoes either down-sampling or up-sampling
through dedicated blocks. These down-sampling and up-
sampling blocks incorporate a convolutional layer and a depth-
wise residual bottleneck [12]. Subsequent to the sampling
operations, the resultant features are propagated through
basic blocks or their inverse counterparts. The basic block
comprises two fundamental components: a Spatial Transform
Block (STB) and a Channel Transform Block (CTB). In the
inverse basic block, the architectural arrangement is reversed,
with the positions of the STB and CTB interchanged. The
structures of STB and CTB follow the classic architecture of
Transformers [40], [41]. To ensure stable training, both the STB
and CTB integrate layer normalization [49]. Internally, the STB
and CTB employ a Depth-wise Residual Block (DepthRB) for
non-linear embedding, coupled with the core transform blocks
and a gated block to facilitate point-wise interactions.
C. Proposed Spatial Transform Block (STB)
The architecture of STB is shown in Figure 3. Depth Residual
Block (DepthRB) is proposed for non-linear embedding. A
Gate Block is proposed for efficient point-wise interactions
with low complexity. We propose the Self-Conditioned Spatial
Transform (SCST) to effectively reduce spatial redundancy.
The overall process is formulated as:
Fstb
ebd=DepthRB (Norm (Fstb
in)),
Fstb
scst=SCST (Fstb
ebd),
Fstb
skip=Conv 1×1 
Fscst
+Fstb
in,
Fstb
out=Gate(Norm (Fstb
skip)) +Fstb
skip,(5)
whereFstb
inis the input feature and Fstb
outis the output feature.
1) Nonlinear Embedding: Previous works [8], [15], [40],
[41] adopt one linear convolutional layer for embedding.
In contrast to previous works, our STB uses a non-linear
embedding method. We propose employing DepthRB. The
architecture of DepthRB is presented in Figure 3, which
contains a 1×1convolutional layer, a 3×3depth-wise
convolutional layer, and a 1×1convolutional layer. The use of
depth-wise convolution helps to minimize complexity. The 3×3
depth-wise convolutional layer helps to aggregate more spatial
information, and non-linearity enhances the expressiveness of
the network.
2) Self-Conditioned Spatial Transform (SCST): During the
transform, the elimination of redundancies directly correlates
with the compactness of the latent representation obtained.
However, current state-of-the-art learned image compression
models often fall short in terms of the size of their effective
receptive fields, as depicted in Fig. 8. They [4], [5], [12], [20]
usually employ 3×3or5×5kernels. This limitation in receptive
field size leaves unaddressed redundancies, thereby hamperingIEEE TRANSACTIONS ON MULTIMEDIA 5
Conv1×1Gate Split, Channel / 2·Conv1×1Self-Conditioned Channel Transform (SCCT)Self-Conditioned Spatial Transform (SCST)
Conv1×1Average PoolingConv3×3Conv1×1Channel-wise MultiplicationSelf-ConditionedChannel-wise Scaling FactorCondition BranchConv1×1Average PoolingConv3×3Conv1×1ReshapeDW Convolution OperationLarge Receptive Self-ConditionedWeightsCondition BranchConv1×1DWConv3×3Conv1×1DepthRBLayerNorm
GateSelf-Conditioned Spatial Transform(SCST)Conv1×1++DepthRBconditionSTB
LayerNorm
LayerNorm
GateSelf-Conditioned Channel Transform(SCCT)Conv1×1++DepthRBconditionCTB
LayerNormBasic Block
Fig. 3. Architecture of the proposed basic block. STB is the proposed Spatial Transform Block. CTB is the proposed Channel Transform Block. DepthRB is
the depth-wise residual block for non-linear embedding. Gate is the proposed Gate Block. Fstb
inis the input of STB. Fctb
inis the input of CTB. In STB, we
employ large kernels to capture more spatial contexts, and the kernel size Kis set to 11or9in our method.
the efficiency of compression. To achieve larger effective
receptive fields, we propose employing large depth-wise kernels
for analysis and synthesis transform. To our knowledge, this is
thefirst attempt in the learned image compression community
to apply large kernels for transform. In our proposed transform
coding method, the kernel size is enlarged to {11,11,9,9}for
different stages of analysis transform. The kernel sizes of the
first two stages are larger because of the larger resolutions of
the input features in the first two stages. The large-resolution
features contain more spatial redundancies.
Although large kernels are employed in our proposed LLIC,
the complexity is still modest due to the depth-wise connectivity.
It is assumed that the large kernel size of a depth-wise
convolution is KL, the small kernel size of a vanilla convolution
isKS, and the input and output channels are N. The complexity
of the large depth-wise convolution is K2
L×N, and the
complexity of the small vanilla convolution is K2
S×N2.
K2
L×N≥K2
S×N2,
KL≥KS√
N.(6)
Nis192in our LLIC models and our baselines [3]–[5], [8],
[12], [20], and√
192≈13.86, indicating that a 11×11depth-
wise convolution is lighter than a 1×1vanilla convolution.
In contrast to the existing methods that rely on transformer-
based or attention-based transform coding techniques, the static
convolutional layer weights present a limitation in harnessing
the characteristics of the input image or features. To this end,
we propose generating convolutional weights by treating the
input itself as the condition in a progressive down-sampling
manner. The condition branch in SCST is utilized to generate
the self-conditioned adaptive weights. Specifically, the input
featureFscst
in∈Rc×h×wis first average pooled to Fscst
pool∈
Rc×3×3, where c, h, w are the channel number, height, andwidth of Fscst
in, respectively. The self-conditioned adaptive
weights Wscst∈Rc×K2are computed via the convolution
between the average pooled feature Fscst
pooland the weights of
the condition branch, where Kis the kernel size.
The overall process of the proposed SCST is formulated as
follows:
Fscst
pool=AvgPool (Fscst
in),
Wscst=Conv 1×1(Conv 3×3(Fstb
pool)),
Fscst
out=Wscst⊗Conv1 ×1(Fscst
in),(7)
whereFscst
out is the output feature, ⊗is the convolution
operation.
3) Gate Mechanism: In previous works [7], [8], the feed
forward network (FFN) [40] is adopted for channel-wise
interactions, which contains two linear layers and a GELU [50]
activation function. The overall process of an FFN is
Fgate
inc=Conv 1×1(Fgate
in),
Fgate
act=1
2Fgate
inc 
1 +tanh"r
2
π
0.044715 
Fgate
inc3#!
,
Fgate
out=Conv 1×1(Fgate
act) +Fgate
in,
(8)
whereFgate
in∈Rc×h×wis the input feature and Fgate
out∈
Rc×h×wis the output feature. Following existing methods, the
channel number of Fgate
inc andFgate
act is2c. The first 1×1
convolutional layer increases the dimension of the input, and
the second 1×1convolutional layer decreases the dimension
to the original dimension. GELU [50] is much more complex
than ReLU [51] or LeakyReLU. The GELU function can be
simplified as Fgate
incΦ(Fgate
inc)[52], which is quite similar to
the gate mechanism. Replacing GELU with a gate mechanism
leads to lower complexity, which inspired us to employ a
gate block instead of an FFN. The architecture of our gateIEEE TRANSACTIONS ON MULTIMEDIA 6
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Bit-Rate[Bpp]3032343638PSNR[dB]
Kodak
STF (CVPR'22, Baseline)
LLIC-STF (Ours)
0.1 0.2 0.3 0.4 0.5 0.6
Bit-Rate[Bpp]32343638PSNR[dB]
T ecnick
STF (CVPR'22, Baseline)
LLIC-STF (Ours)
0.1 0.2 0.3 0.4 0.5 0.6
Bit-Rate[Bpp]32343638PSNR[dB]
CLIC Pro Valid
STF (CVPR'22, Baseline)
LLIC-STF (Ours)
30 31 32 33 34 35 36 37
PSNR[dB]024681012Rate Saving [%] over VTM-17.0
Kodak
STF (CVPR'22, Baseline)
LLIC-STF (Ours)
31 32 33 34 35 36 37 38
PSNR[dB]246810121416Rate Saving [%] over VTM-17.0
T ecnick
STF (CVPR'22, Baseline)
LLIC-STF (Ours)
31 32 33 34 35 36 37 38
PSNR[dB]2
024681012Rate Saving [%] over VTM-17.0
CLIC Pro Valid
STF (CVPR'22, Baseline)
LLIC-STF (Ours)
Fig. 4. PSNR-Bit-Rate curves and Rate saving-PSNR curves of our proposed LLIC-STF and its baseline STF [8]. The relative rate-saving curves are generated
by first interpolating the discrete RD points with a cubic spline and then comparing the bitrates of different models at a fixed PSNR.
block is illustrated in Figure 3. Specifically, Fgate
mid is split
into two features Fgate
1∈Rc×h×wandFgate
2∈Rc×h×w
along the channel dimension. The activated feature Fgate
act is
obtained via the Hadamard product between Fgate
1 andFgate
2.
The gate mechanism causes non-linearity, which is similar to
GELU. The learnable 1×1convolutional layer also makes the
proposed gate mechanism more flexible. The overall process
of the proposed gate block is
Fgate
inc=Conv 1×1(Fgate
in),
Fgate
1,Fgate
2=Split(Fgate
inc),
Fgate
act=Fgate
1⊙Fgate
2,
Fgate
out=Conv 1×1(Fgate
act) +Fgate
in.(9)
The complexity of an FFN is approximately O(4c2hw+2chw),
while the complexity of the proposed gate block is O(3c2hw+
chw). The gate block is more efficient.
D. Proposed Channel Transform Block (CTB)
In the proposed Spatial Transform Block (STB), the kernel
weights are connected in a depth-wise manner, necessitating
the enhancement of interactions among channels. Compared
to baseline models and existing learned image compression
approaches, STB employs significantly larger kernels, enabling
a more expansive receptive field at the same depth. This
increased receptive field capacity provides an opportunity
to further strengthen the interactions between channels. Our
proposed CTB is illustrated in Figure 3. The architecture of
CTB is similar to that of STB. The SCST in STB is replaced
with Self-Conditioned Channel Transform (SCCT) to build the
CTB. DepthRB for non-linear embedding and gate block arealso employed. The overall process is formulated as
Fctb
ebd=DepthRB (Norm (Fctb
in)),
Fctb
scst=SCCT (Fctb
ebd),
Fctb
skip=Conv 1×1
Fctb
scst
+Fctb
in,
Fctb
out=Gate(Norm (Fctb
skip)) +Fctb
skip,(10)
whereFctb
inis the input feature and Fctb
outis the output feature.
1) Self-Conditioned Channel Transform (SCCT): In STB,
large receptive field kernels with self-conditioned adaptability
are employed to reduce spatial redundancy. Due to the limited
interactions among channels, Self-Conditioned Channel Trans-
form (SCCT) is introduced to reduce channel-wise redundancy.
It is important to recognize that different channels within a
feature map carry varying levels of information. Some channels
contain crucial information for reconstruction, while others may
be less informative. By allocating more bits to critical channels
and fewer bits to non-critical channels, we can achieve more
efficient utilization of the available bit-rate. To address this
issue and inspired by Channel Attention [27], we propose
generating adaptive channel factors to modify channel weights.
The proposed SCCT contains a condition branch and a main
branch. In the condition branch, the progressive down-sampling
strategy is also adopted, which reduces information loss. The
input feature Fscct
inis employed as a condition. Fscct
inis average
pooled to reset the resolution and obtain Fscct
pool∈Rc×3×3. The
self-conditioned channel scaling factor Wscct∈Rc×1×1is
computed via the convolution between Fscct
pooland the weights
of the condition branch. The process is formulated as follows:
Fscct
pool=AvgPool (Fscct
in),
Wscct=Conv 1×1(Conv 3×3(Fscct
pool)),
Fscct
out=Wscct⊙Conv 1×1(Fscct
in),(11)IEEE TRANSACTIONS ON MULTIMEDIA 7
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Bit-Rate[Bpp]3032343638PSNR[dB]
Kodak
ELIC (CVPR'22, Baseline)
LLIC-ELIC (Ours)
0.1 0.2 0.3 0.4 0.5 0.6
Bit-Rate[Bpp]32343638PSNR[dB]
T ecnick
ELIC (CVPR'22, Baseline)
LLIC-ELIC (Ours)
0.1 0.2 0.3 0.4 0.5 0.6
Bit-Rate[Bpp]32343638PSNR[dB]
CLIC Pro Valid
ELIC (CVPR'22, Baseline)
LLIC-ELIC (Ours)
30 31 32 33 34 35 36 37
PSNR[dB]4681012Rate Saving [%] over VTM-17.0
Kodak
ELIC (CVPR'22, Baseline)
LLIC-ELIC (Ours)
31 32 33 34 35 36 37 38
PSNR[dB]81012141618Rate Saving [%] over VTM-17.0
T ecnick
ELIC (CVPR'22, Baseline)
LLIC-ELIC (Ours)
31 32 33 34 35 36 37 38
PSNR[dB]2468101214Rate Saving [%] over VTM-17.0
CLIC Pro Valid
ELIC (CVPR'22, Baseline)
LLIC-ELIC (Ours)
Fig. 5. PSNR-Bit-Rate curves and Rate saving-PSNR curves of our proposed LLIC-ELIC and its baseline ELIC [12].
whereFscct
outis the output feature. Because the channel factor
is dependent on the condition, our proposed channel transform
is adaptive.
E. Improved Training Techniques
In previous works, the kernel sizes are 3×3and5×5,
which is not sufficient for redundancy reduction. In our STB,
the kernel size is scaled up to 11×11and9×9, which further
enlarge the receptive field. However, it is non-trivial to fully
exploit the potential of large kernels. Vanilla training strategies
(using 256×256patches) adopted by previous works [8], [12],
[20] cannot fully utilize the large convolutional kernel. When
using 256×256 patches, the resolutions of features during
the analysis transform are {128×128,64×64,32×32, and
16×16}, which are too small, especially 32×32and16×16for
the11×11and9×9kernels. To address this issue, we propose
training using 512×512patches. To mitigate the overhead,
we adopt a two-stage training approach. First, we train the
models on 256×256patches to establish a solid foundation.
Subsequently, we train the models on 512×512patches. This
allows them to refine their understanding of larger-scale spatial
relationships and fully exploit the potential of the large kernels.
This approach is quite simple yet effective.
IV. E XPERIMENTS
A. Implementation Details
1) Training dataset Preparation: The proposed LLIC-STF,
LLIC-ELIC, and LLIC-TCM are trained on 98939 images
from COCO2017 [53], ImageNet [54], DIV2K [55], and
Flicker2K [56]. The initial resolutions of these training images
are greater than 512×512. Following Ball ´eet al [3], in order
to reduce compression artifacts that may be present in JPEG
format images, JPEG images are further down-sampled with a
randomized factor using the PIL library. This down-sampling
process ensures that the minimum height or width of the images
falls within the range of 512 to 584 pixels.2) Training Strategy: Our proposed LLIC-STF, LLIC-
ELIC, and LLIC-TCM are built on PyTorch 2.1 [57] and
CompressAI 1.2.0b3 [58]. Equation 4 is employed as the loss
function. Following existing methods [3]–[5], [12], [13], [20],
the MSE and Multi-Scale Structural Similarity (MS-SSIM)
are employed as distortion metrics during training for rate-
distortion optimization. Following CompressAI [58], λin
Equation 4 is set to {18,35,67,130,250,483}×10−4for MSE
and{2.4,4.58,8.73,16.64,31.73,60.5}for MS-SSIM. The
training process is conducted on 2Intel(R) Xeon(R) Platinum
8260 CPUs and 8Tesla V100-32G GPUs under the PyTorch
distributed data parallel setting. During training, the batch size
is set to 16. Each model is trained for 2M steps. The initial
learning rate is 10−4. The learning rate decreases to 3×10−5
at1.7M steps, decreases to 10−5at1.8M steps, decreases
to3×10−6at1.9M steps, and finally decreases to 1.95M
steps. The training images are randomly cropped to 256×256
patches during the first 1.2M steps and randomly cropped to
512×512patches during the remaining steps. Large patches
are employed to improve the performance of large receptive
field transform coding.
B. Test settings
1) Performance Test Settings: For models optimized for
MSE, Peak Signal-to-Noise Ratio (PSNR) serves as the primary
metric for quantifying distortion. The bits per pixel (Bpp) are
utilized to measure bit-rate. The rate-distortion performances
are measure on 5widely used datasets, including
•Kodak [59], which contains 24uncompressed images, is
widely used in learned image compression community [1],
[3]–[5], [7], [8], [12], [13], [20], [22], [36], [39], [60]–[63].
The resolution of the images in Kodak is 768×512.
•Tecnick [64], which contains 100uncompressed images,
is utilized for performance evaluation in many previous
works [4], [5], [15], [22], [65]. The resolution of images
in Tecnick is 1200×1200 .IEEE TRANSACTIONS ON MULTIMEDIA 8
•CLIC Pro Valid [19] and CLIC 2021 Test [66] are the
validation dataset of the 3rd Challenge on Learned Image
Compression and the test dataset of the 4th Challenge on
Learned Image Compression, respectively. CLIC Pro Valid
contains 41high-resolution images, and CLIC 2021 Test
contains 60high-resolution images. Due to the impact of
Challenge on Learned Image Compression, CLIC datasets
are widely employed for rate-distortion evaluation [8], [9],
[11], [12], [15], [20]. The average resolution of images
in CLIC datasets is approximately 2048×1370 .
•JPEG AI Test [67] is the test dataset of the MMSP 2020
Learning-based Image Coding Challenge and contains 16
images. The largest resolution of the images in JPEGAI
Test is 3680×2456 .
These test datasets with different resolutions (from 768×512
to3680×2456 ) offer a comprehensive evaluation of learned
image compression models.
The Bjøntegaard delta rate (BD-Rate) [68] is utilized to rank
the performance of the learned image compression models.
2) Complexity Test Settings: To comprehensively evaluate
the complexities of various learned image compression models,
16images with resolutions greater than 3584×3584 from
the LIU4K test split [69] are selected as the complexity
test images. These 16images are further center cropped to
{512×512,768×768,1024×1024,1536×1536,2048×
2048,2560×2560,3072×3072,3584×3584}patches to cover
the various resolutions of images that can be encountered
in reality. The model complexities are evaluated from four
perspectives, including the peak GPU memory during encoding
and decoding, model Forward inference MACs, encoding time,
and decoding time. The encoding and decoding times include
the entropy coding and decoding times to better match practical
applications and realistic scenarios.
C. Rate-Distortion Performance
1) Quantitative Results: We compare our proposed LLIC-
STF, LLIC-ELIC, and LLIC-TCM with their baseline models
STF [8], ELIC [12], and LIC-TCM [15], recent learned
image compression models [5], [7], [9]–[11], [20], [22], [23],
[63], [65], [70], and the non-learned image codec VTM-17.0
Intra [16]. The rate-distortion curves and rate-savings-distortion
curves are illustrated in Fig. 4, Fig. 5, and Fig. 6. The BD-
Rate reductions are presented in Table I. VTM-17.0 under the
encoder intra vtm.cfg in the YUV444 color space is employed
as an anchor.
Compared with the baseline model STF [8], our LLIC-
STF achieves superior performance across all bit-rate tiers.
Specifically, LLIC-STF achieves average enhancements of
0.33,0.47,0.40,0.50,and0.62dB in PSNR over STF across
Kodak [59], Tecnick [64], CLIC Pro Valid [19], CLIC 2021 Test,
and JPEGAI Test [67], respectively. Our LLIC-STF decreases
the bitrate by 7.2%,10.88%,9.81%,11.01%,12.34% on these
datasets when the anchor is STF.
Compared with baseline model ELIC [12], LLIC-ELIC
shows markedly enhanced efficacy across all evaluated bit-
rates. Our proposed LLIC-ELIC yields average improvements
of0.19,0.25,0.29,0.21,0.39dB PSNR compared to ELICon Kodak [59], Tecnick [64], CLIC Pro Valid [19], CLIC
2021 Test, and JPEGAI Test [67], respectively. Our LLIC-
ELIC decreases the bitrate by 4.25%,5.66%,7.11%,4.80%,
and8.54% on these datasets when the anchor is ELIC.
When comparing LLIC-TCM with LIC-TCM models, it is
pertinent to highlight the significant performance uplift over the
LIC-TCM Middle, noting that the MACs of our LLIC-TCM is
321.93G and the MACs of the LIC-TCM Middle is 415.2G.
The MACs of LIC-TCM Large is 717.08G. Our LLIC-TCM
performs much better than LIC-TCM Middle and slightly better
than LLIC-TCM Large on Kodak. Our proposed LLIC-TCM
decreases the bitrate by 3%more than LIC-TCM Middle and
1%more than LIC-TCM Large. Our LLIC-TCM performs
much better than LIC-TCM models on high-resolution images.
Our LLIC-TCM decreases the bitrate by 6%,4%more than
LIC-TCM Middle and 3.5%, and 2.4%more than LIC-TCM
Large on Tecnick and CLIC Pro Val, respectively. Our LLIC-
TCM outperforms LIC-TCM Large with only half the MACs
of LIC-TCM Large.
2) Qualitative Results: Fig. 7 illustrates the example of
reconstructed Kodim07 of our proposed models, WACNN,
STF [8], Xie’21 [22], and VTM-17.0 Intra. The PSNR value
of the image reconstructed by our proposed models is 1dB
higher than that of the image reconstructed by VTM-17.0 Intra.
Compared with STF, VTM-17.0 Intra and other codecs, our
proposed models can retain more details. Images reconstructed
by our proposed LLIC have higher subjective quality.
D. Computational Complexity
GPU Memory Consumption-Image Resolution, Forward
MACs-Image Resolution, Encoding Time-Image Resolution,
and Decoding Time-Image Resolution curves are presented in
Fig. 10. We compare our proposed LLIC-STF, LLIC-ELIC, and
LLIC-TCM with their baseline models STF [8], ELIC [12], and
LIC-TCM Large [15] and recent learned image compression
models [20], [22], [23], [63], [70]. Overall, our proposed
transform coding method enhances the model performance
while exhibiting modest GPU memory, forward inference
MACs, and fast encoding and decoding speeds.
1) Testing GPU Memory Consumption Comparison: Com-
pared with existing recent learned image compression models
Xie’21 [22], Entroformer [70], and NeuralSyntax [71], our
proposed LLIC transform coding consumes much less GPU
memory. When compressing 3072×3072 images, the peak
GPU memory of our proposed LLIC models is approximately
10GB, while the GPU memory consumption of Xie’21 [22]
is over 20GB. Compared to the baseline models STF [8]
and ELIC [12], our proposed LLIC-STF and LLIC-ELIC do
not add much GPU memory overhead when compressing low-
resolution images. The GPU memory consumptions of STF,
ELIC, LLIC-STF, and LLIC-ELIC are very similar for low-
resolution images. It should be emphasized that our LLIC-STF
and LLIC-ELIC have great performance improvements over
STF [8] and ELIC [12], which illustrates the superiority of
our approach. Compared to the baseline model LIC-TCM
Large [15], our LLIC-TCM significantly reduces the GPU
memory consumption with better rate-distortion performance.IEEE TRANSACTIONS ON MULTIMEDIA 9
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Bit-Rate[Bpp]3032343638PSNR[dB]
Kodak
LIC-TCM Middle (CVPR'23, Baseline)
LIC-TCM Large (CVPR'23, Baseline)
LLIC-TCM (Ours)
0.1 0.2 0.3 0.4 0.5 0.6
Bit-Rate[Bpp]32343638PSNR[dB]
T ecnick
LIC-TCM Middle (CVPR'23, Baseline)
LIC-TCM Large (CVPR'23, Baseline)
LLIC-TCM (Ours)
0.1 0.2 0.3 0.4 0.5 0.6
Bit-Rate[Bpp]32343638PSNR[dB]
CLIC Pro Valid
LIC-TCM Middle (CVPR'23, Baseline)
LIC-TCM Large (CVPR'23, Baseline)
LLIC-TCM (Ours)
30 31 32 33 34 35 36 37
PSNR[dB]68101214Rate Saving [%] over VTM-17.0
Kodak
LIC-TCM Middle (CVPR'23, Baseline)
LIC-TCM Large (CVPR'23, Baseline)
LLIC-TCM (Ours)
32 33 34 35 36 37 38
PSNR[dB]4681012141618Rate Saving [%] over VTM-17.0
T ecnick
LIC-TCM Middle (CVPR'23, Baseline)
LIC-TCM Large (CVPR'23, Baseline)
LLIC-TCM (Ours)
31 32 33 34 35 36 37 38
PSNR[dB]246810121416Rate Saving [%] over VTM-17.0
CLIC Pro Valid
LIC-TCM Middle (CVPR'23, Baseline)
LIC-TCM Large (CVPR'23, Baseline)
LLIC-TCM (Ours)
Fig. 6. PSNR-Bit-Rate curves and Rate saving-PSNR curves of our proposed LLIC-TCM and its baseline LIC-TCM [15]. We highlight the performance
improvements over LIC-TCM Middle because the MACs of our LLIC-TCM is 321.93G and the MACs of LIC-TCM Middle is 415.2G on Kodak. The
MACs of LIC-TCM Large is 717.08G on Kodak. Our LLIC-TCM also outperforms LIC-TCM Large with only half the MACs of LIC-TCM Large.
Ground TruthBpp/ PSNRLLIC-STFLLIC-ELICLLIC-TCMSTFXie’21VTM-17.0 IntraWACNN0.112 / 31.3430.104 / 31.3200.104 / 31.2930.098 / 30.7400.111 / 30.5390.109 / 31.0620.107 / 30.290
Fig. 7. Visualization of the reconstructed Kodim07 from the Kodak dataset. The metrics are [bpp ↓/PSNR ↑]. We compare our LLIC-STF, LLIC-ELIC, and
LLIC-TCM with STF [8], WACNN [8], Xie’21 [22], and VTM-17.0 Intra [16].
When compressing and decompressing 2048×2048 images,
the GPU memory consumption of our proposed LLIC-TCM is
only1
3of the GPU memory consumption of LIC-TCM Large.
When compressing 3072×3072 images, the GPU memory
consumption of our proposed LLIC-TCM is only1
4of the GPU
memory consumption of LIC-TCM Large. The curve of the
GPU memory consumed by our LLIC-TCM as the resolution
grows is much flatter. The GPU memory overhead is a very
important measure of model complexity because, once the
amount of GPU memory required for encoding or decoding
exceeds the GPU memory capacity of the machine, it is no
longer possible to encode and decode images. The lower GPU
memory consumption indicates that our proposed LLIC-STF,
LLIC-ELIC, and LLIC-TCM have lower complexity and are
highly suitable for high-resolution image compression.
2) Forward Inference MACs Comparison: The forward
inference MACs of the learned image compression models are
computed through the fvcore, DeepSpeed, and Ptflops libraries.
Because the flops evaluation libraries are not very accurate,
when evaluating one model, we compute the output values
of fvcore, Deepspeed, and Ptflops, and we take the averageof the two closest values as the final result. Compared with
the recent learned image compression models Cheng’20 [20],
Xie’21 [22], Entroformer [70], and NeuralSyntax [23], our
proposed LLIC-STF, LLIC-ELIC and LLIC-TCM demonstrate
remarkable reductions in forward inference MACs. Notably,
the forward MACs required for our LLIC models to compress
images of dimensions 2048×2048 are approximately one-
fourth of those reported for NeuralSyntax. Compared with the
baseline models STF [8] and ELIC [12], our LLIC-STF and
LLIC-ELIC have slightly higher forward MACs. Considering
the performance gains of our proposed LLIC-STF and LLIC-
ELIC over STF [8], and ELIC [12], it is worthwhile to increase
the forward MACs a bit. In addition, the forward MACs of our
proposed LLIC models are approximately 74% of the forward
MACs of Cheng’20 [20] when compressing 2048×2048
images. Compared to the baseline model LIC-TCM Large, our
proposed LLIC-TCM significantly reduces the forward MACs.
The forward MACs of our proposed LLIC-TCM is only 45%
of the forward MACs of LIC-TCM Large when compressing
2048×2048 images and compressing 3072×3072 images.
Compared with the mixed CNN-Transformer-based transformIEEE TRANSACTIONS ON MULTIMEDIA 10
TABLE I
BD-R ATE(%) COMPARISON FOR PSNR ( DB)AND MS-SSIM. T HE ANCHOR IS VTM-17.0 I NTRA .
Methods VenueBD-Rate (%) w.r.t. VTM-17.0 Intra
Kodak [59] Tecnick [64] CLIC Pro Valid [19] CLIC 2021 Test [66] JPEGAI Test [67]
PSNR MS-SSIM PSNR PSNR PSNR PSNR
Cheng’20 [20] CVPR’20 +5 .58−44.21 +7 .57 +11 .71 +9 .40 +11 .95
Minnen’20 [5] ICIP’20 +3 .23 − −0.88 − − −
Qian’21 [63] ICLR’21 +10 .05−39.53 +7 .52 +0 .02 − −
Xie’21 [22] ACMMM’21 +1 .55−43.39 −0.80 +3 .21 +0 .99 +2 .35
Entroformer [70] ICLR’22 +4 .73−42.64 +2 .31 −1.04 − −
SwinT-Charm [7] ICLR’22 −1.73 − − − − −
NeuralSyntax [23] CVPR’22 +8 .97−39.56 − +5 .64 − −
McQuic [9] CVPR’22 −1.57−47.94 − +6 .82 − −
Contextformer [10] ECCV’22 −5.77−46.12 −9.05 − − −
Pan’22 [65] ECCV’22 +7 .56−36.2 +3 .97 − − −
NVTC [11] CVPR’23 −1.04 − − − − 3.61 −
STF as Baseline
STF [8] CVPR’22 −2.48−47.72 −2.75 +0 .42 −0.16 +1 .54
LLIC-STF Ours −9.49−49.11−13.06 −9.32 −11.44 −11.15
ELIC as Baseline
ELIC [12] CVPR’22 −5.95−44.60 −9.14 −3.45 −7.52 −3.21
LLIC-ELIC Ours −9.47−49.25−14.68 −10.35 −12.32 −11.24
LIC-TCM as Baseline
LIC-TCM Middle [15] CVPR’23 −7.43 − − 8.99 −6.35 − −
LIC-TCM Large [15] CVPR’23 −10.14−48.94 −11.47 −8.04 − −
LLIC-TCM Ours −10.94−49.73−14.99 −10.41 −13.14 −12.30
STF (CVPR’22)LIC-TCM Large (CVPR’23)LLIC-STF (Ours)
WACNN (CVPR’22)
ELIC (CVPR’22)
LLIC-ELIC (Ours)LLIC-TCM (Ours)
Fig. 8. Effective Receptive Fields (ERF) of analysis transforms gaof WACNN, STF, ELIC, LIC-TCM Large and our proposed models on 24 Kodak images
center-cropped to 512×512. The ERF is visualized as the absolute gradients of the center pixel in the latent ( dy/dx) with respect to the input image. Darker
green colors represent larger gradients. A more widely distributed green area indicates a larger ERF. A larger ERF indicates that more spatial contextual
information is captured and utilized during the transform.
coding of LIC-TCM Large, our proposed large receptive field
transform coding is more powerful and light-weight.
3) Encoding and Decoding Time Comparison: Because
our LLIC-STF, LLIC-ELIC, and LLIC-TCM employ parallel
entropy models, they encode and decode much faster than
Cheng’20 [20], Xie’21 [22], Qian’21 [63], Entroformer [70] and
NeuralSyntax [23]. Specifically, when compressing a 768×768
image, the encoding times of Cheng’20 and Xie’21 exceed
3s, the encoding times of Qian’21 and Entroformer exceed
40s, the decoding times of Cheng’20 and Xie’21 exceed 7s,
and the decoding times of Qian’21 and Entroformer exceed
50s; meanwhile, the encoding and decoding times of our
proposed LLIC models are approximately 0.06∼0.1s and0.1∼0.12s, respectively. Compared to the baseline model
STF [8], our proposed LLIC-STF is approximately as fast
as STF [8]. Compared to the baseline model ELIC [12], our
proposed LLIC-ELIC is approximately as fast as ELIC [12] on
low-resolution images. Our proposed LLIC-ELIC requires 20%
more time and requires 7%to encode a 2048×2048 image. The
added time to encode and decode the high-resolution images is
worthwhile considering the performance improvement brought
about by our proposed transform coding. Compared with LIC-
TCM Large, our LLIC-TCM encodes and decodes much faster
than the baseline model LIC-TCM Large. Overall, our proposed
transform coding enhances model performance while achievingIEEE TRANSACTIONS ON MULTIMEDIA 11
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 160 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 160.79 0.78 0.82 0.81 0.82 0.81 0.82 0.85 0.87 0.86 0.85 0.81 0.79 0.79 0.78 0.82 0.81
0.79 0.81 0.8 0.8 0.83 0.83 0.88 0.88 0.85 0.84 0.81 0.79 0.8 0.83 0.79 0.81 0.82
0.8 0.81 0.83 0.81 0.83 0.84 0.85 0.85 0.84 0.84 0.84 0.8 0.82 0.83 0.82 0.84 0.85
0.84 0.84 0.84 0.83 0.81 0.83 0.82 0.88 0.85 0.85 0.83 0.79 0.84 0.84 0.83 0.86 0.87
0.78 0.83 0.85 0.82 0.81 0.8 0.83 0.87 0.86 0.85 0.8 0.78 0.82 0.83 0.84 0.86 0.87
0.77 0.81 0.81 0.84 0.84 0.82 0.86 0.87 0.88 0.85 0.81 0.81 0.81 0.81 0.83 0.84 0.85
0.78 0.79 0.81 0.86 0.85 0.84 0.85 0.88 0.89 0.84 0.84 0.81 0.84 0.82 0.82 0.82 0.82
0.78 0.8 0.8 0.88 0.87 0.81 0.82 0.87 0.92 0.88 0.86 0.83 0.86 0.86 0.85 0.85 0.86
0.78 0.77 0.85 0.86 0.84 0.83 0.82 0.88 1 0.89 0.86 0.84 0.84 0.88 0.85 0.87 0.87
0.82 0.8 0.83 0.85 0.84 0.86 0.81 0.83 0.89 0.89 0.85 0.84 0.83 0.86 0.84 0.87 0.89
0.82 0.8 0.82 0.81 0.84 0.84 0.87 0.86 0.88 0.87 0.84 0.84 0.86 0.87 0.86 0.87 0.88
0.82 0.82 0.8 0.77 0.82 0.85 0.88 0.82 0.9 0.88 0.84 0.84 0.85 0.85 0.83 0.83 0.81
0.8 0.82 0.81 0.8 0.81 0.87 0.87 0.82 0.87 0.86 0.82 0.81 0.8 0.8 0.78 0.74 0.79
0.8 0.8 0.8 0.81 0.83 0.85 0.86 0.85 0.87 0.82 0.81 0.79 0.79 0.79 0.79 0.77 0.8
0.79 0.81 0.81 0.82 0.82 0.83 0.83 0.86 0.84 0.82 0.8 0.82 0.83 0.8 0.8 0.79 0.82
0.78 0.81 0.82 0.83 0.84 0.86 0.86 0.87 0.86 0.84 0.83 0.83 0.81 0.83 0.82 0.8 0.79
0.79 0.77 0.79 0.81 0.81 0.83 0.86 0.85 0.87 0.85 0.84 0.82 0.82 0.81 0.82 0.82 0.82
0.00.20.40.60.81.0
(a) STF
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 160 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 160.42 0.44 0.43 0.43 0.38 0.43 0.47 0.46 0.42 0.39 0.43 0.44 0.46 0.5 0.51 0.49 0.45
0.45 0.42 0.37 0.45 0.43 0.46 0.5 0.52 0.43 0.37 0.44 0.44 0.4 0.46 0.5 0.45 0.48
0.44 0.42 0.46 0.41 0.45 0.55 0.53 0.53 0.5 0.43 0.52 0.47 0.45 0.45 0.49 0.47 0.45
0.44 0.48 0.45 0.41 0.45 0.53 0.51 0.54 0.52 0.44 0.47 0.54 0.51 0.44 0.49 0.51 0.49
0.43 0.47 0.49 0.42 0.46 0.53 0.53 0.5 0.48 0.52 0.53 0.56 0.5 0.45 0.51 0.47 0.45
0.4 0.51 0.53 0.44 0.48 0.51 0.53 0.51 0.53 0.55 0.59 0.49 0.5 0.46 0.46 0.49 0.49
0.43 0.48 0.5 0.45 0.57 0.58 0.53 0.5 0.54 0.49 0.55 0.52 0.5 0.5 0.45 0.44 0.47
0.45 0.33 0.42 0.47 0.58 0.56 0.52 0.53 0.65 0.58 0.58 0.52 0.51 0.47 0.47 0.47 0.51
0.45 0.36 0.48 0.51 0.57 0.59 0.66 0.66 1 0.6 0.54 0.58 0.49 0.56 0.53 0.51 0.5
0.48 0.42 0.47 0.5 0.51 0.55 0.63 0.56 0.7 0.63 0.59 0.53 0.52 0.57 0.45 0.46 0.53
0.44 0.43 0.46 0.49 0.5 0.54 0.57 0.52 0.64 0.6 0.55 0.48 0.54 0.54 0.45 0.47 0.51
0.51 0.44 0.45 0.47 0.47 0.53 0.55 0.53 0.58 0.55 0.52 0.44 0.49 0.45 0.47 0.49 0.49
0.49 0.49 0.45 0.48 0.49 0.59 0.59 0.58 0.6 0.59 0.57 0.49 0.49 0.48 0.5 0.44 0.43
0.51 0.49 0.48 0.46 0.42 0.51 0.55 0.61 0.57 0.54 0.55 0.5 0.44 0.44 0.5 0.49 0.44
0.42 0.48 0.44 0.48 0.52 0.52 0.57 0.59 0.55 0.55 0.56 0.51 0.42 0.47 0.47 0.46 0.43
0.41 0.48 0.48 0.52 0.54 0.59 0.6 0.58 0.58 0.58 0.52 0.49 0.52 0.47 0.54 0.54 0.51
0.44 0.45 0.44 0.45 0.47 0.6 0.61 0.58 0.59 0.59 0.57 0.55 0.53 0.49 0.49 0.52 0.48
0.00.20.40.60.81.0 (b) ELIC
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 160 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 160.5 0.51 0.5 0.52 0.45 0.47 0.49 0.49 0.47 0.44 0.5 0.44 0.48 0.51 0.54 0.52 0.46
0.51 0.44 0.44 0.5 0.5 0.44 0.47 0.53 0.45 0.43 0.46 0.45 0.39 0.42 0.51 0.46 0.43
0.48 0.4 0.44 0.42 0.48 0.56 0.51 0.52 0.48 0.38 0.51 0.47 0.37 0.43 0.44 0.45 0.43
0.38 0.42 0.43 0.39 0.46 0.54 0.51 0.56 0.5 0.43 0.49 0.53 0.49 0.42 0.48 0.49 0.48
0.35 0.44 0.46 0.43 0.47 0.51 0.55 0.47 0.47 0.46 0.44 0.48 0.46 0.43 0.46 0.4 0.45
0.38 0.46 0.5 0.45 0.43 0.51 0.46 0.49 0.45 0.49 0.49 0.44 0.45 0.45 0.47 0.44 0.46
0.38 0.47 0.5 0.41 0.57 0.53 0.51 0.48 0.5 0.42 0.44 0.48 0.45 0.47 0.42 0.45 0.49
0.46 0.27 0.38 0.41 0.55 0.53 0.55 0.5 0.6 0.47 0.52 0.51 0.42 0.44 0.42 0.41 0.48
0.43 0.35 0.38 0.43 0.49 0.55 0.6 0.59 1 0.57 0.48 0.54 0.44 0.51 0.48 0.46 0.45
0.46 0.41 0.41 0.48 0.48 0.46 0.57 0.52 0.62 0.59 0.57 0.51 0.53 0.53 0.4 0.39 0.47
0.4 0.41 0.43 0.41 0.44 0.49 0.47 0.47 0.57 0.57 0.53 0.47 0.46 0.5 0.45 0.38 0.5
0.47 0.4 0.42 0.4 0.39 0.45 0.53 0.49 0.53 0.51 0.53 0.45 0.43 0.45 0.47 0.42 0.48
0.46 0.45 0.45 0.45 0.48 0.5 0.51 0.53 0.57 0.54 0.54 0.5 0.48 0.46 0.47 0.47 0.42
0.52 0.48 0.49 0.42 0.41 0.49 0.5 0.55 0.55 0.51 0.53 0.49 0.48 0.4 0.45 0.48 0.41
0.47 0.46 0.48 0.46 0.52 0.45 0.54 0.56 0.53 0.55 0.53 0.49 0.44 0.46 0.51 0.52 0.4
0.45 0.51 0.48 0.42 0.48 0.53 0.55 0.5 0.54 0.54 0.54 0.52 0.39 0.49 0.53 0.51 0.49
0.45 0.47 0.47 0.38 0.42 0.53 0.55 0.55 0.54 0.55 0.55 0.51 0.49 0.53 0.51 0.55 0.5
0.00.20.40.60.81.0
(c) LIC-TCM Large
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 160 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 160.42 0.41 0.41 0.4 0.36 0.38 0.42 0.43 0.38 0.34 0.42 0.38 0.43 0.49 0.47 0.46 0.39
0.44 0.39 0.34 0.39 0.4 0.44 0.47 0.49 0.41 0.36 0.41 0.39 0.38 0.42 0.47 0.4 0.43
0.43 0.37 0.43 0.35 0.42 0.5 0.47 0.5 0.48 0.38 0.48 0.41 0.4 0.42 0.4 0.41 0.4
0.37 0.42 0.4 0.35 0.38 0.44 0.42 0.47 0.46 0.4 0.46 0.47 0.45 0.4 0.44 0.45 0.46
0.35 0.43 0.42 0.37 0.41 0.46 0.47 0.41 0.4 0.41 0.43 0.47 0.45 0.41 0.45 0.45 0.43
0.35 0.45 0.48 0.39 0.42 0.44 0.44 0.45 0.43 0.46 0.5 0.43 0.46 0.43 0.43 0.45 0.44
0.37 0.41 0.43 0.36 0.53 0.51 0.47 0.44 0.48 0.4 0.46 0.44 0.43 0.44 0.4 0.42 0.44
0.42 0.27 0.39 0.38 0.51 0.5 0.48 0.47 0.59 0.52 0.55 0.48 0.4 0.42 0.43 0.44 0.45
0.44 0.35 0.4 0.45 0.5 0.51 0.57 0.57 1 0.54 0.45 0.53 0.45 0.52 0.47 0.46 0.44
0.41 0.36 0.43 0.44 0.48 0.46 0.56 0.52 0.64 0.58 0.54 0.5 0.47 0.52 0.38 0.4 0.45
0.4 0.4 0.42 0.41 0.41 0.47 0.48 0.45 0.58 0.54 0.47 0.43 0.47 0.5 0.42 0.41 0.46
0.43 0.39 0.4 0.41 0.4 0.47 0.51 0.45 0.5 0.47 0.49 0.41 0.43 0.44 0.44 0.43 0.44
0.42 0.43 0.41 0.4 0.44 0.53 0.54 0.53 0.55 0.51 0.51 0.46 0.46 0.44 0.46 0.4 0.4
0.45 0.44 0.44 0.38 0.38 0.45 0.52 0.52 0.54 0.49 0.52 0.46 0.47 0.41 0.46 0.47 0.41
0.36 0.43 0.41 0.41 0.45 0.41 0.48 0.55 0.53 0.51 0.49 0.48 0.41 0.43 0.4 0.42 0.39
0.36 0.45 0.45 0.43 0.45 0.52 0.52 0.51 0.49 0.5 0.46 0.5 0.41 0.44 0.47 0.47 0.43
0.38 0.41 0.4 0.37 0.4 0.52 0.51 0.49 0.49 0.49 0.51 0.48 0.46 0.45 0.42 0.46 0.45
0.00.20.40.60.81.0 (d) LLIC-STF
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 160 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 160.42 0.42 0.42 0.41 0.36 0.4 0.43 0.44 0.43 0.36 0.41 0.4 0.42 0.48 0.46 0.46 0.4
0.46 0.39 0.35 0.38 0.4 0.44 0.46 0.51 0.41 0.34 0.4 0.41 0.39 0.44 0.46 0.4 0.43
0.44 0.39 0.45 0.35 0.4 0.49 0.46 0.5 0.47 0.39 0.49 0.42 0.42 0.42 0.41 0.39 0.41
0.41 0.43 0.43 0.37 0.4 0.48 0.45 0.48 0.47 0.41 0.44 0.49 0.48 0.41 0.45 0.44 0.46
0.37 0.42 0.44 0.38 0.41 0.47 0.48 0.43 0.43 0.44 0.44 0.46 0.45 0.42 0.46 0.45 0.44
0.36 0.45 0.48 0.39 0.44 0.46 0.46 0.46 0.45 0.47 0.52 0.42 0.43 0.42 0.43 0.45 0.45
0.39 0.42 0.44 0.38 0.53 0.52 0.48 0.48 0.5 0.43 0.49 0.44 0.44 0.43 0.41 0.41 0.43
0.41 0.28 0.38 0.41 0.54 0.51 0.48 0.48 0.6 0.5 0.53 0.48 0.43 0.44 0.41 0.43 0.46
0.41 0.33 0.41 0.44 0.51 0.51 0.57 0.56 1 0.54 0.48 0.53 0.44 0.53 0.46 0.47 0.45
0.41 0.36 0.41 0.44 0.45 0.49 0.56 0.5 0.63 0.58 0.52 0.51 0.43 0.52 0.38 0.41 0.47
0.41 0.4 0.41 0.42 0.43 0.47 0.49 0.44 0.56 0.55 0.48 0.43 0.45 0.49 0.42 0.41 0.46
0.43 0.39 0.39 0.42 0.39 0.45 0.5 0.44 0.53 0.51 0.48 0.4 0.42 0.42 0.46 0.41 0.43
0.42 0.43 0.41 0.41 0.43 0.53 0.51 0.52 0.55 0.53 0.52 0.44 0.44 0.44 0.45 0.38 0.37
0.45 0.44 0.43 0.41 0.37 0.44 0.49 0.52 0.53 0.49 0.49 0.45 0.45 0.41 0.46 0.44 0.4
0.37 0.45 0.4 0.43 0.47 0.44 0.49 0.54 0.52 0.49 0.49 0.49 0.4 0.43 0.4 0.43 0.38
0.36 0.44 0.44 0.46 0.47 0.52 0.53 0.5 0.51 0.51 0.45 0.46 0.43 0.42 0.49 0.48 0.45
0.36 0.4 0.38 0.4 0.39 0.53 0.54 0.5 0.51 0.51 0.51 0.47 0.48 0.46 0.45 0.47 0.43
0.00.20.40.60.81.0
(e) LLIC-ELIC
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 160 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 160.41 0.44 0.41 0.47 0.42 0.45 0.44 0.46 0.46 0.39 0.44 0.42 0.41 0.46 0.48 0.45 0.39
0.44 0.42 0.36 0.46 0.45 0.47 0.46 0.47 0.4 0.37 0.41 0.42 0.44 0.45 0.5 0.4 0.44
0.44 0.42 0.44 0.36 0.4 0.49 0.45 0.46 0.46 0.36 0.46 0.4 0.4 0.42 0.43 0.38 0.41
0.4 0.45 0.39 0.36 0.37 0.46 0.42 0.46 0.42 0.36 0.42 0.45 0.45 0.39 0.45 0.41 0.47
0.39 0.38 0.45 0.39 0.39 0.47 0.47 0.41 0.37 0.42 0.42 0.46 0.4 0.37 0.45 0.38 0.41
0.36 0.41 0.48 0.42 0.42 0.45 0.44 0.44 0.45 0.45 0.5 0.39 0.4 0.41 0.37 0.45 0.44
0.36 0.41 0.43 0.34 0.49 0.51 0.45 0.39 0.47 0.36 0.47 0.41 0.42 0.41 0.39 0.39 0.44
0.41 0.24 0.36 0.37 0.5 0.47 0.46 0.44 0.57 0.46 0.48 0.42 0.37 0.44 0.38 0.39 0.48
0.41 0.31 0.37 0.4 0.47 0.46 0.54 0.52 1 0.49 0.43 0.48 0.42 0.5 0.43 0.43 0.44
0.42 0.35 0.39 0.43 0.43 0.44 0.53 0.47 0.64 0.55 0.51 0.46 0.45 0.47 0.36 0.35 0.48
0.39 0.39 0.4 0.41 0.41 0.43 0.49 0.43 0.55 0.51 0.47 0.4 0.44 0.44 0.38 0.38 0.47
0.45 0.39 0.4 0.43 0.38 0.44 0.45 0.45 0.51 0.46 0.47 0.36 0.43 0.39 0.43 0.39 0.47
0.42 0.42 0.44 0.44 0.44 0.53 0.5 0.54 0.52 0.51 0.53 0.45 0.46 0.43 0.43 0.4 0.38
0.48 0.47 0.45 0.39 0.39 0.43 0.49 0.52 0.51 0.49 0.49 0.46 0.4 0.37 0.46 0.44 0.38
0.39 0.45 0.4 0.42 0.45 0.41 0.47 0.51 0.49 0.51 0.54 0.47 0.38 0.42 0.43 0.42 0.35
0.43 0.43 0.45 0.42 0.48 0.5 0.53 0.49 0.53 0.52 0.47 0.44 0.42 0.41 0.48 0.46 0.43
0.4 0.45 0.43 0.4 0.39 0.53 0.52 0.51 0.52 0.54 0.51 0.49 0.46 0.43 0.44 0.47 0.42
0.00.20.40.60.81.0 (f) LLIC-TCM
Fig. 9. Absolute value of the cosine similarity among latents. Note that the
forward MACs of our LLIC models are only half of the forward MACs of
LIC-TCM Large. Please zoom in for a better view.
fast encoding and decoding speed.
E. Ablation Studies
1) Settings: Ablation studies are conducted on LLIC-STF.
When conducting ablation experiments, we train each model
for1.7M steps from scratch. The batch size is set to 16. We
adopt the training strategy in section IV-A 2. The results of
ablation studies are in Table II. In Table II, “w/o Gate” means
that the gate block is replaced by an FFN [40].
2) Influence of Self-Conditioned Spatial Transform: Al-
though it is possible to increase the theoretical receptive field
by continuously stacking network layers, it is likely that a
small effective receptive field [24] will be obtained. Following
Zhu et al [7], Effective Receptive Field (ERF) [24], [72]TABLE II
ABLATION STUDIES ON KODAK DATASET .
Settings BD-Rate (%) Forward MACs (G)
LLIC-STF Basic Block
STB + STB -6.05 331.55
CTB + CTB -6.19 318.61
only CTB -5.05 214.90
only STB -4.19 221.29
LLIC-STF w/o CTB
static weight -2.58 221.26
w/o DepthRB -1.01 188.09
w/o DepthRB & Gate -0.18 198.83
LLIC-STF w/o CTB
K={5, 5, 5, 5 } +0.05 216.28
K={7, 7, 7, 7 } -2.46 217.62
K={9, 9, 9, 9 } -3.38 219.29
K={11, 11, 9, 9 } -4.19 221.29
K={11, 11, 11, 11 } -4.42 221.62
LLIC-STF
256×256 patch -4.64 325.00
512×512 patch -7.85 325.00
VTM-17.0 Intra 0.00 –
is employed to evaluate the influence of the proposed self-
conditioned spatial transform, which utilizes large depth-wise
convolutions. The ERF is visualized as the absolute gradient
of the center pixel in the latent ( dy/dx) with respect to
the input image. The ERFs of our baseline models STF [8],
ELIC [12], and LIC-TCM Large [15] , our proposed LLIC-
STF, LLIC-ELIC and LLIC-TCM are visualized in Fig. 8.
Baseline model STF employs swin-transformer-based transform
coding, ELIC employs CNN-Attention-based transform coding,
and the recent LIC-TCM employs mixed CNN-Transformer-
based transform coding. Compared with these different types of
transform coding methods, it is obvious that our proposed LLIC-
STF, LLIC-ELIC, and LLIC-TCM achieve much larger ERFs
than their baselines. Larger ERFs indicate that our proposed
large receptive field transform coding is able to remove more
redundancy during the analysis transform, which makes our
proposed models perform better than our baseline models.
To further demonstrate the advantages of large receptive
fields, we compute the cosine similarity between the center
latent and other latents in y. The cosine similarity computation
is performed on a 17×17window obtained by center-cropping
the latent representation. We compute cosine similarity on
the Kodak dataset. The cosine similarities are averaged on 24
images from the Kodak dataset. The absolute cosine similarity
map is visualized in Figure 9. According to Figure 9, our
proposed method can reduce a large range of redundancies.
For example, compared to the baseline methods STF, ELIC,
and LIC-TCM, the similarity values of the 16-th rows of the
similarity maps of the proposed method are lower. The lower
similarity among distant latents can be attributed to larger
effective receptive fields. A lower similarity indicates that a
larger receptive field can reduce more spatial redundancy.IEEE TRANSACTIONS ON MULTIMEDIA 12
512×512768×7681024×10241536×15362048×20482560×25603072×30723584×3584
Image Resolution0510152025303540GPU Memory Comsuption [GB]
Cheng'20(CVPR'20)
Qian'21(ICLR'21)
Xie'21(ACMMM'21)
Entroformer(ICLR'22)
NeuralSyntax(CVPR'22)
WACNN(CVPR'22)
STF(CVPR'22)
ELIC(CVPR'22)
LIC-TCM Large(CVPR'23)
LLIC-STF(Ours)
LLIC-ELIC(Ours)
LLIC-TCM(Ours)
512×512768×7681024×10241536×15362048×20482560×25603076×30763584×3584
Image Resolution051015202530Forward TMACs
Cheng'20(CVPR'20)
Qian'21(CVPR'22)
Xie'21(ACMMM'21)
Entroformer(ICLR'22)
NeuralSyntax(CVPR'22)
WACNN(CVPR'22)
STF(CVPR'22)
ELIC(CVPR'22)
LIC-TCM Large(CVPR'23)
LLIC-STF(Ours)
LLIC-ELIC(Ours)
LLIC-TCM(Ours)
512×512768×7681024×10241536×15362048×20482560×25603072×30723584×3584
Image Resolution0.00.51.01.52.02.53.03.54.04.5Encoding Time
Cheng'20(CVPR'20)
Xie'21(ACMMM'21)
WACNN(CVPR'22)
STF(CVPR'22)
ELIC(CVPR'22)
LIC-TCM Large(CVPR'23)
LLIC-STF(Ours)
LLIC-ELIC(Ours)
LLIC-TCM(Ours)
512×512768×7681024×10241536×15362048×20482560×25603072×30723584×3584
Image Resolution0.00.51.01.52.02.53.03.54.04.5Decoding Time
Cheng'20(CVPR'20)
Xie'21(ACMMM'21)
WACNN(CVPR'22)
STF(CVPR'22)
ELIC(CVPR'22)
LIC-TCM Large(CVPR'23)
LLIC-STF(Ours)
LLIC-ELIC(Ours)
LLIC-TCM(Ours)
Fig. 10. GPU Memory Consumption-Image Resolution, Forward MACs-Image Resolution, Encoding Time-Image Resolution, Decoding Time-Image Resolution
curves.
TABLE III
INFLUENCE OF KERNEL SIZE ON VARIOUS RESOLUTIONS .
Kernel SizeBD-Rate (%) w.r.t. VTM-17.0 Intra
Kodak ( 768×512) [59] Tecnick ( 1200×1200 ) [64] CLIC Pro Valid ( 2048×1370 ) [19] JPEGAI Test ( 3680×2456 ) [67]
K={5,5,5,5} +0.05 −1.17 +1 .72 +1 .13
K={7,7,7,7} −2.46 −4.27 −1.63 −2.91
K={9,9,9,9} −3.38 −6.89 −3.47 −5.12
K={11,11,9,9} −4.19 −8.17 −4.33 −7.76
To further investigate the contribution of the proposed spatial
transform block, the spatial transform block is removed or
replaced in our ablation studies. Specifically, removing the
STB leads to significant performance degradation. If the STB
is replaced by CTB, the rate-distortion performance is still not
as good as the performance of the model utilizing STBs and
CTBs. This performance degradation indicates the necessity of
employing the proposed spatial transform blocks for a more
compact latent representation to enhance the rate-distortion
performance.
In our models and baseline models, the analysis transform
gaand synthesis transform gsinvolves four stages. The kernel
size of each stage is denoted as K={k1, k2, k3, k4}. To
evaluate the contribution of large kernel, we set the kernel
sizeK={5,5,5,5},K={7,7,7,7},K={9,9,9,9},
K={11,11,9,9}, and K={11,11,11,11}. The rate-
distortion performances when utilizing various kernel sizes
are shown in Table II. Clearly, increasing the size of the
convolutional kernel keeps improving the rate-distortion perfor-mance of the model, However, the gains decrease. Increasing
the kernel size Kfrom{5,5,5,5}to{7,7,7,7}leads to
the largest performance enhancement. The differences in the
performance when utilizing {11,11,9,9}kernels and when
utilizing {11,11,11,11}kernels are quite negligible. Therefore,
in our proposed LLIC models, K={11,11,9,9}is employed.
To analyze the relationship between the kernel size and
compression performance with different resolutions, we con-
duct ablation studies on Kodak, Tecnick [64], CLIC Pro
Valid [19], and JPEGAI Test [67]. The results are presented
in Table III. As the resolution of the input image increases,
the gap between the BD-Rate values of different kernels
gradually increases. For example, when compressing 512×768
images (Kodak), the difference between K={7,7,7,7}and
K={5,5,5,5}is−2.51%; when compressing 1200×1200
(Tecnick) images, 2048×1370 (CLIC Pro Valid) images,
and3680×2456 (JPEGAI Test) images, the differences are
−3.10%,−3.35%,−4.04%, respectively. The increased BD-
Rate gap indicates that large kernel sizes is beneficial for high-IEEE TRANSACTIONS ON MULTIMEDIA 13
resolution image coding. Compared with low-resolution images,
high-resolution images contain more spatial redundancy . For
high-resolution images, pixels in a larger area are correlated
with each other. In this case, the receptive field of the image
compression model is crucial. If the receptive field is small,
redundancy beyond its receptive field will be difficult to remove,
especially when compressing high-resolution images. Therefore,
we employ large depth-wise kernels to overcome the drawbacks
of the previous methods while maintaining modest complexity.
The large kernel leads to better performance when compressing
high-resolution images.
Context adaptability also plays an important role in boosting
the rate-distortion performance of learned image compression
models. When the kernel weights are independent of the input
feature, the rate-distortion becomes worse. The rate-distortion
performance loss is approximately 1.6%. The performance
degradations demonstrate the effectiveness of the proposed
self-conditioned weight generation.
3) Influence of Self-Conditioned Channel Transform: The
channel transform block is proposed for self-conditioned adap-
tive channel adjustment. In ablation studies, channel transform
blocks are removed or replaced. Specifically, removing CTBs
leads to significant performance degradation, and the rate-
distortion performance of replacing CTBs with STBs is still
not as good as the performance of the model utilizing CTBs
and STBs. The performance degradation in ablation studies
demonstrates the effectiveness and necessity of employing
channel transform blocks in transform coding.
4) Influence of DepthRB and Gate Mechanism: We use
LLIC-STF without CTBs to evaluate the effectiveness of the
proposed DepthRB for nonlinear embedding. Compared with
linear embedding, which employs a linear layer, our proposed
DepthRB for non-linear embedding further enhances the rate-
distortion performance. Non-linear embedding is more flexible
than linear embedding. We also evaluate the effectiveness
of the proposed gate block. Replacing the gate blocks with
vanilla FFNs increases the complexity of the model but leads
to performance degradation, which demonstrates the superiority
of the proposed gate block.
5) Influence of Large Patches for Training: The256×256
patches are insufficient for training. To fully exploit large
kernels, the large training strategy is employed. Large patch
training results in a performance increment of approximately
3%, thereby substantiating the efficacy of the large patch
training strategy.
V. C ONCLUSION
In this paper, we propose large receptive-field transform
coding with self-conditioned adaptability for learned image
compression, which effectively captures more spatial corre-
lations. To reduce channel-wise redundancy, we propose the
self-conditioned channel transform to adjust the weight of each
channel. To evaluate our proposed transform method, we align
the entropy model with existing advanced non-linear transform
coding techniques and obtain the models LLIC-STF, LLIC-
ELIC, and LLIC-TCM. Extensive experiments demonstrate
the superiority of our proposed large receptive field learningwith self-conditioned adaptability. Our LLIC-STF, LLIC-ELIC,
and LLIC-TCM achieve state-of-the-art performance and they
reduce the BD-Rate by 9.49%,9.47%,10.94% on Kodak over
VTM-17.0 Intra, respectively. To further enhance the perfor-
mance, it is promising to integrate with more advanced entropy
models [13], [14]. However, there are several limitations to
be addressed. First, the complexities of the proposed LLIC
models are not low enough, which means that they cannot be
directly employed on mobile devices. Second, the learned image
compression models are trained on natural images, which means
that the generalization ability on out-of-distribution images (e.g.,
screen content) may be limited. The decoding complexity of
LLIC models can be further reduced by employing asymmetric
encoder-decoder structure [73], where the encoder could be
heavy while the decoder is light. To enhance the generalization
ability of learned image compression models, we suggest
fine-tuning the encoder or latent representation for a specific
input, which will increase the encoding time, but the decoding
complexity will still be low. In addition, fine-tuning the encoder
or latent representation will also improve the performance on
natural images. We will investigate these techniques in the
future.
REFERENCES
[1]J. Ball ´e, V . Laparra, and E. P. Simoncelli, “End-to-end optimized image
compression,” in International Conference on Learning Representations ,
2017.
[2]L. Theis, W. Shi, A. Cunningham, and F. Husz ´ar, “Lossy image
compression with compressive autoencoders,” in International Conference
on Learning Representations , 2017.
[3]J. Ball ´e, D. Minnen, S. Singh, S. J. Hwang, and N. Johnston, “Variational
image compression with a scale hyperprior,” in International Conference
on Learning Representations , 2018.
[4]D. Minnen, J. Ball ´e, and G. D. Toderici, “Joint autoregressive and
hierarchical priors for learned image compression,” in Advances in Neural
Information Processing Systems , 2018, pp. 10 771–10 780.
[5]D. Minnen and S. Singh, “Channel-wise autoregressive entropy models
for learned image compression,” in 2020 IEEE International Conference
on Image Processing (ICIP) . IEEE, 2020, pp. 3339–3343.
[6]S. Ma, X. Zhang, C. Jia, Z. Zhao, S. Wang, and S. Wang, “Image and
video compression with neural networks: A review,” IEEE Transactions
on Circuits and Systems for Video Technology , vol. 30, no. 6, pp. 1683–
1698, 2019.
[7]Y . Zhu, Y . Yang, and T. Cohen, “Transformer-based transform coding,”
inInternational Conference on Learning Representations , 2022.
[8]R. Zou, C. Song, and Z. Zhang, “The devil is in the details: Window-
based attention for image compression,” in In Proceedings of the IEEE
conference on computer vision and pattern recognition , 2022.
[9]X. Zhu, J. Song, L. Gao, F. Zheng, and H. T. Shen, “Unified multivariate
gaussian mixture for efficient neural image compression,” in Proceed-
ings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , 2022, pp. 17 612–17 621.
[10] A. B. Koyuncu, H. Gao, A. Boev, G. Gaikov, E. Alshina, and E. Steinbach,
“Contextformer: A transformer with spatio-channel attention for context
modeling in learned image compression,” in European Conference on
Computer Vision , 2022, pp. 447–463.
[11] R. Feng, Z. Guo, W. Li, and Z. Chen, “Nvtc: Nonlinear vector transform
coding,” in Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , June 2023, pp. 6101–6110.
[12] D. He, Z. Yang, W. Peng, R. Ma, H. Qin, and Y . Wang, “Elic:
Efficient learned image compression with unevenly grouped space-
channel contextual adaptive coding,” in Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition , June 2022, pp.
5718–5727.
[13] W. Jiang, J. Yang, Y . Zhai, P. Ning, F. Gao, and R. Wang, “Mlic: Multi-
reference entropy model for learned image compression,” in Proceedings
of the 31st ACM International Conference on Multimedia , 2023, pp.
7618–7627.IEEE TRANSACTIONS ON MULTIMEDIA 14
[14] W. Jiang and R. Wang, “Mlic++: Linear complexity multi-reference
entropy modeling for learned image compression,” in ICML 2023
Workshop Neural Compression: From Information Theory to Applications ,
2023. [Online]. Available: https://openreview.net/forum?id=hxIpcSoz2t
[15] J. Liu, H. Sun, and J. Katto, “Learned image compression with
mixed transformer-cnn architectures,” in Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition , 2023, pp.
14 388–14 397.
[16] B. Bross, Y .-K. Wang, Y . Ye, S. Liu, J. Chen, G. J. Sullivan, and J.-R.
Ohm, “Overview of the versatile video coding (vvc) standard and its
applications,” IEEE Transactions on Circuits and Systems for Video
Technology , vol. 31, no. 10, pp. 3736–3764, 2021.
[17] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,” in
International Conference on Learning Representations , 2014.
[18] J. Ball ´e, P. A. Chou, D. Minnen, S. Singh, N. Johnston, E. Agustsson, S. J.
Hwang, and G. Toderici, “Nonlinear transform coding,” IEEE Journal of
Selected Topics in Signal Processing , vol. 15, no. 2, pp. 339–353, 2020.
[19] G. Toderici, W. Shi, R. Timofte, L. Theis, J. Ball ´e, E. Agustsson,
N. Johnston, and F. Mentzer, “Workshop and challenge on learned
image compression (clic2020),” CVPR, 2020. [Online]. Available:
https://data.vision.ee.ethz.ch/cvl/clic/professional valid 2020.zip
[20] Z. Cheng, H. Sun, M. Takeuchi, and J. Katto, “Learned image compres-
sion with discretized gaussian mixture likelihoods and attention modules,”
inProceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , June 2020.
[21] F. Chen, Y . Xu, and L. Wang, “Two-stage octave residual network for
end-to-end image compression,” in Proceedings of the AAAI Conference
on Artificial Intelligence , vol. 36, 2022, pp. 3922–3929.
[22] Y . Xie, K. L. Cheng, and Q. Chen, “Enhanced invertible encoding for
learned image compression,” in Proceedings of the ACM International
Conference on Multimedia , 2021, pp. 162–170.
[23] D. Wang, W. Yang, Y . Hu, and J. Liu, “Neural data-dependent transform
for learned image compression,” in Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition , 2022, pp.
17 379–17 388.
[24] W. Luo, Y . Li, R. Urtasun, and R. Zemel, “Understanding the effective
receptive field in deep convolutional neural networks,” Advances in neural
information processing systems , vol. 29, 2016.
[25] Y . Zhang, K. Li, K. Li, B. Zhong, and Y . Fu, “Residual non-local attention
networks for image restoration,” in International Conference on Learning
Representations , 2018.
[26] T. Chen, H. Liu, Z. Ma, Q. Shen, X. Cao, and Y . Wang, “End-to-end learnt
image compression via non-local attention optimization and improved
context modeling,” IEEE Transactions on Image Processing , vol. 30, pp.
3179–3191, 2021.
[27] J. Hu, L. Shen, and G. Sun, “Squeeze-and-excitation networks,” in
Proceedings of the IEEE conference on computer vision and pattern
recognition , 2018, pp. 7132–7141.
[28] G. Toderici, S. M. O’Malley, S. J. Hwang, D. Vincent, D. Minnen,
S. Baluja, M. Covell, and R. Sukthankar, “Variable rate image compres-
sion with recurrent neural networks,” in International Conference on
Learning Representations , 2016.
[29] G. Toderici, D. Vincent, N. Johnston, S. Jin Hwang, D. Minnen, J. Shor,
and M. Covell, “Full resolution image compression with recurrent neural
networks,” in Proceedings of the IEEE conference on Computer Vision
and Pattern Recognition , 2017, pp. 5306–5314.
[30] A. Van den Oord, N. Kalchbrenner, L. Espeholt, O. Vinyals, A. Graves
et al. , “Conditional image generation with pixelcnn decoders,” Advances
in neural information processing systems , vol. 29, 2016.
[31] D. He, Y . Zheng, B. Sun, Y . Wang, and H. Qin, “Checkerboard context
model for efficient learned image compression,” in Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition ,
2021, pp. 14 771–14 780.
[32] Z. Guo, Z. Zhang, R. Feng, and Z. Chen, “Causal contextual prediction
for learned image compression,” IEEE Transactions on Circuits and
Systems for Video Technology , 2021.
[33] J. Ball ´e, V . Laparra, and E. P. Simoncelli, “Density modeling of
images using a generalized normalization transformation,” arXiv preprint
arXiv:1511.06281 , 2015.
[34] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in Proceedings of the IEEE conference on computer vision
and pattern recognition , 2016, pp. 770–778.
[35] C. Shin, H. Lee, H. Son, S. Lee, D. Lee, and S. Lee, “Expanded adaptive
scaling normalization for end to end image compression,” in European
Conference on Computer Vision , 2022, pp. 390–405.[36] M. Akbari, J. Liang, J. Han, and C. Tu, “Learned multi-resolution
variable-rate image compression with octave-based residual blocks,” IEEE
Transactions on Multimedia , vol. 23, pp. 3013–3021, 2021.
[37] G. Gao, P. You, R. Pan, S. Han, Y . Zhang, Y . Dai, and H. Lee,
“Neural image compression via attentional multi-scale back projection and
frequency decomposition,” in Proceedings of the IEEE/CVF International
Conference on Computer Vision , 2021, pp. 14 677–14 686.
[38] Y . Chen, H. Fan, B. Xu, Z. Yan, Y . Kalantidis, M. Rohrbach, S. Yan,
and J. Feng, “Drop an octave: Reducing spatial redundancy in convo-
lutional neural networks with octave convolution,” in Proceedings of
the IEEE/CVF international conference on computer vision , 2019, pp.
3435–3444.
[39] H. Ma, D. Liu, R. Xiong, and F. Wu, “iwave: Cnn-based wavelet-like
transform for image compression,” IEEE Transactions on Multimedia ,
vol. 22, no. 7, pp. 1667–1679, 2019.
[40] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” Advances in
neural information processing systems , vol. 30, pp. 5998–6008, 2017.
[41] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang, S. Lin, and B. Guo,
“Swin transformer: Hierarchical vision transformer using shifted windows,”
inProceedings of the IEEE/CVF International Conference on Computer
Vision (ICCV) , October 2021, pp. 10 012–10 022.
[42] Y . Bai, X. Yang, X. Liu, J. Jiang, Y . Wang, X. Ji, and W. Gao,
“Towards end-to-end image compression and analysis with transformers,”
inProceedings of the AAAI conference on artificial intelligence , vol. 36,
no. 1, 2022, pp. 104–112.
[43] A. A. Jeny, M. S. Junayed, and M. B. Islam, “An efficient end-to-end
image compression transformer,” in 2022 IEEE International Conference
on Image Processing (ICIP) . IEEE, 2022, pp. 1786–1790.
[44] M. Lu, P. Guo, H. Shi, C. Cao, and Z. Ma, “Transformer-based image
compression,” in Data Compression Conference , 2022, pp. 469–469.
[45] M. Wang, K. Zhang, L. Zhang, Y . Li, J. Li, Y . Wang, and S. Wang,
“End-to-end image compression with swin-transformer,” in 2022 IEEE In-
ternational Conference on Visual Communications and Image Processing
(VCIP) . IEEE, 2022, pp. 1–5.
[46] W. B. Pennebaker and J. L. Mitchell, JPEG: Still image data compression
standard . Springer Science & Business Media, 1992.
[47] M. Rabbani, “Jpeg2000: Image compression fundamentals, standards and
practice,” Journal of Electronic Imaging , vol. 11, no. 2, p. 286, 2002.
[48] G. J. Sullivan, J.-R. Ohm, W.-J. Han, and T. Wiegand, “Overview of
the high efficiency video coding (hevc) standard,” IEEE Transactions on
circuits and systems for video technology , vol. 22, no. 12, pp. 1649–1668,
2012.
[49] J. L. Ba, J. R. Kiros, and G. E. Hinton, “Layer normalization,” arXiv
preprint arXiv:1607.06450 , 2016.
[50] D. Hendrycks and K. Gimpel, “Gaussian error linear units (gelus),” arXiv
preprint arXiv:1606.08415 , 2016.
[51] V . Nair and G. E. Hinton, “Rectified linear units improve restricted
boltzmann machines,” in Proceedings of the 27th international conference
on machine learning , 2010, pp. 807–814.
[52] L. Chen, X. Chu, X. Zhang, and J. Sun, “Simple baselines for image
restoration,” in European Conference on Computer Vision . Springer,
2022, pp. 17–33.
[53] T.-Y . Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,
P. Doll ´ar, and C. L. Zitnick, “Microsoft coco: Common objects in context,”
inEuropean Conference on Computer Vision , 2014, pp. 740–755.
[54] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet:
A large-scale hierarchical image database,” in 2009 IEEE conference on
computer vision and pattern recognition . Ieee, 2009, pp. 248–255.
[55] E. Agustsson and R. Timofte, “Ntire 2017 challenge on single image
super-resolution: Dataset and study,” 2017 IEEE Conference on Computer
Vision and Pattern Recognition Workshops , pp. 1122–1131, 2017.
[56] B. Lim, S. Son, H. Kim, S. Nah, and K. Mu Lee, “Enhanced deep
residual networks for single image super-resolution,” in Proceedings
of the IEEE conference on computer vision and pattern recognition
workshops , 2017, pp. 136–144.
[57] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,
T. Killeen, Z. Lin, N. Gimelshein, L. Antiga et al. , “Pytorch: An
imperative style, high-performance deep learning library,” in Advances
in Neural Information Processing Systems , 2019, pp. 8024–8035.
[58] J. B´egaint, F. Racap ´e, S. Feltman, and A. Pushparaja, “Compressai:
a pytorch library and evaluation platform for end-to-end compression
research,” arXiv preprint arXiv:2011.03029 , 2020.
[59] E. Kodak, “Kodak lossless true color image suite (photocd pcd0992),”
1993. [Online]. Available: http://r0k.us/graphics/kodak/IEEE TRANSACTIONS ON MULTIMEDIA 15
[60] Z. Cheng, H. Sun, M. Takeuchi, and J. Katto, “Energy compaction-based
image compression using convolutional autoencoder,” IEEE Transactions
on Multimedia , vol. 22, no. 4, pp. 860–873, 2020.
[61] W. Yin, Y . Shi, W. Zuo, and X. Fan, “A co-prediction-based compression
scheme for correlated images,” IEEE Transactions on Multimedia , vol. 22,
no. 8, pp. 1917–1928, 2020.
[62] Y . Mei, L. Li, Z. Li, and F. Li, “Learning-based scalable image
compression with latent-feature reuse and prediction,” IEEE Transactions
on Multimedia , vol. 24, pp. 4143–4157, 2022.
[63] Y . Qian, Z. Tan, X. Sun, M. Lin, D. Li, Z. Sun, L. Hao, and R. Jin,
“Learning accurate entropy model with global reference for image
compression,” in International Conference on Learning Representations ,
2020.
[64] N. Asuni and A. Giachetti, “Testimages: a large-scale archive for testing
visual devices and basic image processing algorithms,” in STAG: Smart
Tools & Apps for Graphics (2014) , 2014.
[65] G. Pan, G. Lu, Z. Hu, and D. Xu, “Content adaptive latents and decoder
for neural image compression,” in European Conference on Computer
Vision , 2022, pp. 556–573.
[66] G. Toderici, W. Shi, R. Timofte, L. Theis, J. Ball ´e, E. Agustsson,
N. Johnston, and F. Mentzer, “Workshop and challenge on learned
image compression (clic2021),” CVPR, 2021. [Online]. Available: https:
//storage.googleapis.com/clic2021 public/professional test 2021.zip
[67] JPEG-AI, “Jpeg-ai test images,” https://jpegai.github.io/test images/,
2020.
[68] G. Bjontegaard, “Calculation of average psnr differences between rd-
curves,” VCEG-M33 , 2001.
[69] J. Liu, D. Liu, W. Yang, S. Xia, X. Zhang, and Y . Dai, “A comprehensive
benchmark for single image compression artifact reduction,” IEEE
Transactions on image processing , vol. 29, pp. 7845–7860, 2020.
[70] Y . Qian, M. Lin, X. Sun, Z. Tan, and R. Jin, “Entroformer: A transformer-
based entropy model for learned image compression,” in International
Conference on Learning Representations , 2022.
[71] Z. Wang, E. P. Simoncelli, and A. C. Bovik, “Multiscale structural
similarity for image quality assessment,” in The Thrity-Seventh Asilomar
Conference on Signals, Systems & Computers, 2003 , vol. 2. Ieee, 2003,
pp. 1398–1402.
[72] X. Ding, X. Zhang, J. Han, and G. Ding, “Scaling up your kernels to
31x31: Revisiting large kernel design in cnns,” in Proceedings of the
IEEE/CVF conference on computer vision and pattern recognition , 2022,
pp. 11 963–11 975.
[73] Y . Yang and S. Mandt, “Computationally-efficient neural image com-
pression with shallow decoders,” in Proceedings of the IEEE/CVF
International Conference on Computer Vision (ICCV) , 2023, pp. 530–540.
Wei Jiang received the B.S. degree in Vehicle
Engineering from Chongqing University, China, in
2021. He is currently pursuing the Ph.D. degree in
computer application technology at Peking University.
His research interests include learned image / video
coding and implicit neural representation.
Peirong Ning received his Master’s degree in Com-
puter Science from Peking University in 2023. He
is currently working at Xiaohongshu Company, Ltd.,
where he focuses on learning-based codecs and the
Hybrid Video Coding framework.
Jiayu Yang received the B.E. degree in electronic
information engineering from Hefei University of
Technology, China, in 2019. He is currently pursuing
the Ph.D. degree in the School of Electronic and
Computer Engineering at Peking University, China.
His research interests include video coding and
immersive media coding.
Yongqi Zhai received the B.S. degree in electronic
information engineering from Jilin University, China,
in 2020. He is currently pursuing the Ph.D. degree in
computer application technology at Peking University.
His research interests include traditional and learning-
based image/video coding.
Feng Gao (Member, IEEE) received his B.S. degree
in Computer Science from University College London
in 2007, and Ph.D. degree in Computer Science from
Peking University in 2018. He was a post-doctoral
research fellow at the Future Laboratory,Tsinghua
University, from 2018 to 2020. He joins Peking
University as Assistant Professor since 2020. His
research interesting is working on the intersection of
Computer Science and Art, including but not limit on
artificial intelligence and painting art, deep learning
and painting robot, etc.
Ronggang Wang (Member, IEEE) received the Ph.D.
degree from the Institute of Computing Technology,
Chinese Academy of Sciences, in 2006. He is
currently a Professor with the School of Electronic
and Computer Engineering, Peking University Shen-
zhen Graduate School. His research interests include
immersive video coding and processing. He has made
over 100 technical contributions to ISO/IEC MPEG,
IEEE 1857 and China A VS. He has authored more
than 150 papers and held more than 100 patents. He
has been serving as the IEEE 1857.9 Immersive video
coding standard sub-group Chair and China A VS virtual reality sub-group
Chair since 2016.