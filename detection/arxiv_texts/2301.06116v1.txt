Maximally Compact and Separated Features with Regular Polytope Networks
Federico Pernici, Matteo Bruni, Claudio Baecchi and Alberto Del Bimbo
MICC – Media Integration and Communication Center
University of Florence – Italy
ffederico.pernici, matteo.bruni, claudio.baecchi, alberto.delbimbo g@unifi.it
Abstract
Convolutional Neural Networks (CNNs) trained with the
Softmax loss are widely used classiﬁcation models for sev-
eral vision tasks. Typically, a learnable transformation (i.e.
the classiﬁer) is placed at the end of such models return-
ing class scores that are further normalized into probabil-
ities by Softmax. This learnable transformation has a fun-
damental role in determining the network internal feature
representation.
In this work we show how to extract from CNNs fea-
tures with the properties of maximum inter-class separa-
bility and maximum intra-class compactness by setting the
parameters of the classiﬁer transformation as not train-
able (i.e. ﬁxed). We obtain features similar to what can
be obtained with the well-known “Center Loss” [1] and
other similar approaches but with several practical advan-
tages including maximal exploitation of the available fea-
ture space representation, reduction in the number of net-
work parameters, no need to use other auxiliary losses be-
sides the Softmax.
Our approach uniﬁes and generalizes into a common ap-
proach two apparently different classes of methods regard-
ing: discriminative features, pioneered by the Center Loss
[1] and ﬁxed classiﬁers, ﬁrstly evaluated in [2].
Preliminary qualitative experimental results provide
some insight on the potentialities of our combined strategy.
1. Introduction
Convolutional Neural Networks (CNNs) together with
the Softmax loss have achieved remarkable successes in
computer vision, improving the state of the art in image
classiﬁcation tasks [3, 4, 5, 6]. In classiﬁcation all the pos-
sible categories of the test samples are also present in the
training set and the predicted labels determine the perfor-
mance. As a result, the Softmax with Cross Entropy loss
is widely adopted by many classiﬁcation approaches due to
its simplicity, good performance and probabilistic interpre-
DCNN𝐟𝐰𝑖
DCNN𝐟𝐰𝑖
DCNN𝐟𝐰𝑖ℝ𝑑ℝ𝑑ℝ𝑑
𝜑𝜑𝜑
Figure 1: Margin Regular Polytope Networks (Margin-
RePoNets). Features with maximal inter-class separability
and intra-class compactness are shown (light blue). These
are determined combining ﬁxed classiﬁers derived from
regular polytopes [9] with a recently developed margin loss
[10]. Maximal features separation is obtained by setting the
classiﬁer weights wiaccording to values following the sym-
metrical of conﬁguration regular polytopes (red). Maximal
compactness is obtained by setting the margin between the
features at the maximum allowed (i.e. ').
tation. In other applications like face recognition [7] or hu-
man body reidentiﬁcation [8] test samples are not known in
advance and recognition at test time is performed according
to learned features based on their distance.
The underlying assumption in this learning scenario is
that images of the same identity (person) are expected to
be closer in the representation space, while different identi-
ties are expected to be far apart. Or equivalently, the learned
features having low intra-class distance and large inter-class
distance are successful at modeling novel unseen identities
and for this reason such features are typically deﬁned “dis-
criminative”. Speciﬁcally, the Center Loss, ﬁrstly proposed
in [1], has been proved to be an effective method to com-
pute discriminative features. The method learns a center
determined as the average of features belonging to the same
class. During training, the centers are updated by minimiz-
1arXiv:2301.06116v1  [cs.CV]  15 Jan 2023ing the distances between the deep features and their corre-
sponding class centers. The CNN is trained under the joint
supervision of the Softmax loss and the Center Loss by bal-
ancing the two supervision signals. Intuitively, the Softmax
loss forces the deep features of different classes to be sepa-
rable while the Center Loss attracts the features of the same
class to their centers achieving compactness.
Despite its usefulness, the Center Loss has some limita-
tions: the feature centers are extra parameters stored outside
the network that are not jointly optimized with the network
parameters. Indeed, they are updated with an autoregres-
sive mean estimator that tracks the underlying representa-
tion changes at each step. Moreover, when a large num-
ber of classes must be learned, mini-batches do not provide
enough samples for a correct estimation of the mean. Center
Loss also requires a balancing between the two supervision
losses which typically requires a search over the balancing
hyper-parameter.
Some works have successfully addressed all the issues
described above importing intra-class feature compactness
directly into the Softmax loss. This class of methods, in-
cluding [11, 12, 10, 13, 14], avoids the need of an auxiliary
loss (as in the Center Loss) with the possibility of includ-
ing a margin between the class decision boundaries, all in a
single Softmax loss.
Other successful works follow a nearly opposite strategy
by removing the ﬁnal classiﬁcation layer and learn directly a
distance evaluated on image pairs or image triplets as shown
in [15] and in [16] respectively. Despite the performance re-
sults, carefully designed pair and triplet selection is required
to avoid slow convergence and instability.
Except for few recent cases [17, 18, 9] inter-class sep-
arability and compactness are always enforced in a local
manner without considering global inter-class separability
and intra-class compactness. For this purpose, the work
[18] uses an auxiliary loss for enforcing global separabil-
ity. The work [17] use an auxiliary loss similar to [18] for
enforcing global separability and a further margin loss to
enforce compactness. The work [9] uses a ﬁxed classiﬁer
in which the parameters of the ﬁnal transformation imple-
menting the classiﬁer are notsubjected to learning and are
set with values taken from coordinate vertices of a regular
polytope. This avoids optimizing for maximal separation as
in [17] and [18] since regular polytopes naturally provide
distributed vertices (i.e. the classiﬁer weights) at equal an-
gles maximizing the available space.
In this paper we address allthose limitations including
global inter-class separability and compactness in a maxi-
mal sense without the need of any auxiliary loss. This is
achieved by exploiting the Regular Polytope ﬁxed classi-
ﬁers (RePoNets) proposed in [9] and improving their fea-
ture compactness according to the additive angular margin
described in [10]. As illustrated in Fig. 1, the advantageof the proposed combination is the capability of generating
global maximally separated and compact features (shown in
light blue) angularly centered around the vertices of poly-
topes (i.e. the classiﬁer ﬁxed weights shown in red). The
same ﬁgure further illustrates the three basic types of fea-
tures that can be learned. Although, there are inﬁnite regu-
lar polygons in R2and 5 regular polyedra in R3, there are
only three regular polytopes in Rdwithd≥5, namely the
d-Simplex, the d-Cube and the d-Orthoplex.
In particular, the angle 'subtended between a class
weight and its connected class weights is constant and max-
imizes inter-class separability in the available space. The
angle'is further exploited to obtain the maximal compact-
ness by setting the angular margin between the features to
'(i.e. the maximum allowed margin). The advantage of
our formulation is that the margin is no longer an hyperpa-
rameter that have to be searched since it is obtained from a
closed form solution.
2. Related Work
Fixed Classiﬁers . Empirical evidence, reported in [19],
ﬁrstly shows that a CNN with a ﬁxed classiﬁcation layer
does not worsen the performance on the CIFAR10 dataset.
A recent paper [2] explores in more detail the idea of ex-
cluding the classiﬁcation parameters from learning. The
work shows that a ﬁxed classiﬁer causes little or no re-
duction in classiﬁcation performance for common datasets
(including ImageNet) while allowing a noticeable reduc-
tion in trainable parameters, especially when the number
of classes is large. Setting the last layer as not trainable also
reduces the computational complexity for training as well
as the communication cost in distributed learning. The de-
scribed approach sets the classiﬁer with the coordinate ver-
tices of orthogonal vectors taken from the columns of the
Hadamard1matrix. Although the work uses a ﬁxed clas-
siﬁer, the properties of the generated features are not ex-
plored. A major limitation of this method is that, when the
number of classes is greater than the dimension of the fea-
ture space, it is not possible to have mutually orthogonal
columns and therefore some of the classes are constrained
to lie in a common subspace causing a reduction in classiﬁ-
cation performance.
Recently [9] improves in this regard showing that a novel
set of unique directions taken from regular polytopes over-
comes the limitations of the Hadamard matrix. The work
further shows that the generated features are stationary at
training time and coincide with the equiangular spaced ver-
tices of the polytope. Being evaluated for classiﬁcation the
method does not enforce feature compactness. We extend
this work by adding recent approaches to explicitly enforce
1The Hadamard matrix is a square matrix whose entries are either +1
or -1 and whose rows are mutually orthogonal.feature compactness by constraining features to lie on a hy-
persphere [12] and to have a margin between other features
[10].
Fixed classiﬁers have been recently used also for not dis-
criminative purposes. The work [20] trains a neural net-
work in which the last layer has ﬁxed parameters with pre-
deﬁned points of a hyper-sphere (i.e. a spherical lattice).
The work aims at learning a function to build an index that
maps real-valued vectors to a uniform distribution over a d-
dimensional sphere to preserve the neighborhood structure
in the input space while best covering the output space. The
learned function is used to make high-dimensional indexing
more accurate.
Softmax Angular Optimization . Some papers train
DCNNs by direct angle optimization [14, 21, 12]. From a
semantic point of view, the angle encodes the required dis-
criminative information for class recognition. The wider
the angles the better the classes are separated from each
other and, accordingly, their representation is more discrim-
inative. The common idea of these works is that of con-
straining the features and/or the classiﬁer weights to be unit
normalized. The works [22], [23] and [12] normalize both
features and weights, while the work [11] normalizes the
features only and [14] normalizes the weights only. Speciﬁ-
cally, [11] also proposes adding a scale parameter after fea-
ture normalization based on the property that increasing the
norm of samples can decrease the Softmax loss [24].
From a statistical point of view, normalizing weights and
features is equivalent to considering features distributed on
the unit hypersphere according to the von Mises-Fisher dis-
tribution [23] with a common concentration parameter (i.e.
features of each class have the same compactness). Under
this model each class weight represents the mean of its cor-
responding features and the scalar factor (i.e. the concen-
tration parameter) is inversely proportional to their standard
deviations. Several methods implicitly follow this statisti-
cal interpretation in which the weights act as a summarizer
or as parameterized prototype of the features of each class
[12, 14, 25, 26, 27]. Eventually, as conjectured in [12] if all
classes are well-separated, they will roughly correspond to
the means of features in each class.
In [9] the ﬁxed classiﬁers based on regular polytopes pro-
duce features exactly centered around their ﬁxed weights as
the training process advances. The work globally imposes
the largest angular distances between the class features be-
fore starting the learning process without an explicit opti-
mization of the classiﬁer or the requirement of an auxiliary
loss as in [18] and [17]. The works [18, 17] add a regular-
ization loss to speciﬁcally force the classiﬁer weights dur-
ing training to be far from each other in a global manner.
These works including [9] draw inspiration from a well-
known problem in physics – the Thomson problem [28] –
where given Kcharges conﬁned to the surface of a sphere,one seeks to ﬁnd an arrangement of the charges which min-
imizes the total electrostatic energy. Electrostatic force re-
pels charges each other inversely proportional to their mu-
tual distance. In [18] and [17] global equiangular features
are obtained by adding to the standard categorical Cross-
Entropy loss a further loss inspired by the Thomson prob-
lem while [9] builds directly an arrangement for global sep-
arability and compactness by considering that minimal en-
ergies are often concomitant with special geometric conﬁg-
urations of charges that recall the geometry of regular poly-
topes in high dimensional spaces [29].
3. Regular Polytope Networks with Additive
Angular Margin Loss
In Neural Networks the representation for an input sam-
ple is the feature vector fgenerated by the penultimate
layer, while the last layer (i.e. the classiﬁer) outputs score
values according to the inner product as:
zi=w/uni22BA
i⋅f (1)
for each class i, where wiis the weight vector of the clas-
siﬁer for the class i. In the ﬁnal loss, the scores are further
normalized into probabilities via the Softmax function.
Since the values of zican be also expressed as
zi=w/uni22BA
i⋅f=/divides.alt0/divides.alt0wi/divides.alt0/divides.alt0/divides.alt0/divides.alt0f/divides.alt0/divides.alt0cos(), whereis the angle between
wiandf, the score for the correct label with respect to the
other labels is obtained by optimizing /divides.alt0/divides.alt0wi/divides.alt0/divides.alt0,/divides.alt0/divides.alt0f/divides.alt0/divides.alt0and. Ac-
cording to this, feature vector directions and weight vector
directions align simultaneously with each other at training
time so that their average angle is made as small as possi-
ble. In [9] it is shown that if classiﬁer weights are excluded
from learning, they can be regarded as ﬁxed angular ref-
erences to which features align. In particular, if the ﬁxed
weights are derived from the three regular polytopes avail-
able in Rdwithd≥5, then their symmetry creates angular
references to which class features centrally align. More for-
mally, let X={(xi;yi)}N
i=1be the training set containing
Nsamples, where xiis the image input to the CNN and
yi∈{1;2;/uni22EF;K}is the label of the class that supervises the
output of the DCNN. Then, the Cross Entropy loss can be
written as:
L=−1
NN
/summation.disp
i=1log/uni239B
/uni239Dew/uni22BA
yifi+byi
∑K
j=1ew/uni22BA
jfi+bj/uni239E
/uni23A0; (2)
where W={wj}K
j=1are the ﬁxed classiﬁer weight vectors
for theKclasses. Only three polytopes exist in every di-
mensionality and are: the d-Simplex, the d-Orthoplex and
thed-Cube from which three classiﬁers can be deﬁned as
follow:
Ws=/braceleft.alt2e1;e2;:::;ed−1;d−1
/summation.disp
i=1ei/braceright.alt2; (3)Figure 2: The distribution of features learned from the MNIST dataset using the RePoNet classiﬁers. Features are shown
(from left to right) with a scatter plot matrix for the d-Simplex,d-Orthoplex and d-Cube classiﬁer respectively. It can
be noticed that features are distributed following the symmetric vertex conﬁguration of polytopes. Although features are
maximally separated, their compactness is limited.
Wo={±e1;±e2;:::;±ed}; (4)
Wc=/uni23A7/uni23AA/uni23AA/uni23A8/uni23AA/uni23AA/uni23A9w∈Rd∶/bracketleft.alt4−1√
d;1√
d/bracketright.alt4d/uni23AB/uni23AA/uni23AA/uni23AC/uni23AA/uni23AA/uni23AD; (5)
where=1−√
d+1
din Eq.3 and eiwithi∈{1;2;:::;d−1}
in Eqs.3 and 4 denotes the standard basis in Rd−1. The ﬁnal
weights in Eq.3 are further shifted about the centroid, the
other two are already centered around the origin. Such sets
of weights represent the vertices of the generalization of the
tetrahedron, octahedron and cube respectively, to arbitrary
dimensiond. The weights are further unit normalized ( ^wj=
wj
/divides.alt0/divides.alt0wj/divides.alt0/divides.alt0) and the biases are set to zero ( bj=0). According to
this, Eq. 2 simpliﬁes to:
L=−1
NN
/summation.disp
i=1log/uni239B
/uni239De^w/uni22BA
yifi
∑K
j=1e^w/uni22BA
j^fi/uni239E
/uni23A0: (6)
Although, Eq. 6 directly optimizes for small angles, only
partial intra-class compactness can be enforced. Fig.2
shows (from left to right) the distribution of features learned
from the MNIST dataset with the three different classiﬁers.
The features are displayed as a collection of points, each
having the activation of one feature coordinate determin-
ing the position on the horizontal axis and the value of the
other feature coordinate activation determining the position
on the vertical axis. All the pairwise scatter plots of the
feature activation coordinates are shown and feature classes
are color coded. The size of the scatter plot matrices fol-
lows the size of the feature dimensionality dof each ﬁxed
classiﬁer which can be determined according to the numberof classesKas:
d=K−1; d=/uni2308log2(K)/uni2309; d=/uni2308.alt2K
2/uni2309.alt2;(7)
respectively. The scatter plot matrices therefore result in the
following dimensions: 9×9,5×5and4×4respectively. As
evidenced from the ﬁgure, the features follow the symmet-
ric and maximally separated vertex conﬁgurations of their
corresponding polytopes. This is due to the fact that each
single pairwise scatter plot is basically a parallel projection
onto the planes deﬁned by pairs of multidimensional axes.
According to this, features assume a /upY,+, and×shaped
conﬁguration for the d-Simplex,d-Orthoplex and d-Cube
respectively. Although maximal separation is achieved, the
intra-class average distance is large and therefore not well
suited for recognition purposes.
The plotted features are obtained training the so called
LeNet++ architecture [1]. The network is a modiﬁcation
of the LeNet architecture [30] to a deeper and wider net-
work including parametric rectiﬁer linear units (pReLU)
[31]. The network is learned using the Adam optimizer [32]
with a learning rate of 0:0005 , the convolutional parameters
are initialized following [33] and the mini-batch size is 512.
To improve compactness keeping the global maximal
feature separation we follow [13, 12] normalizing the fea-
tures and multiplying them by a scalar :^fi=fi
/divides.alt0/divides.alt0fi/divides.alt0/divides.alt0. The
loss in Eq.2 can be therefore rewritten as:
L=−1
NN
/summation.disp
i=1log/uni239B
/uni239De^w/uni22BA
yi^fi
∑K
j=1e^w/uni22BA
j^fi/uni239E
/uni23A0
=−1
NN
/summation.disp
i=1log/uni239B
/uni239Decos(yi)
∑K
j=1ecos(j)/uni239E
/uni23A0(8)The equation above minimizes the angle yibetween the
ﬁxed weight corresponding to the label yiand its associ-
ated feature. The equation can be interpreted as if features
are realizations from a set of Kvon Mises-Fisher distribu-
tions having a common concentration parameter . Under
this parameterization ^wis the mean direction on the hyper-
sphere andis the concentration parameter. The greater
the value of the higher the concentration of the distribu-
tion around the mean direction ^wand the more compact the
features. This value has already been discussed sufﬁciently
in several previous works [12, 11]. In this paper, we directly
ﬁxed it to 30and will not discuss its effect anymore.
To obtain maximal compactness the additive angular
margin loss described in [10] is exploited. According to
this, Eq.8 becomes:
L=−1
NN
/summation.disp
i=1log/uni239B
/uni239Decos(yi+m)
ecos(yi+m)+n
∑
j=1
j≠yiecos(j)/uni239E
/uni23A0; (9)
where the scalar value mis an angle in the normalized
hypersphere introducing a margin between class decision
boundaries. The loss of Eq. 9 together with the ﬁxed clas-
siﬁer weights of Eqs. 3, 4, 5 allows learning discriminative
features without using any auxiliary loss other than the Soft-
max.
The advantage of our formulation is that mis no longer
an hyperparameter that have to be searched. Indeed, the loss
above when used with RePoNet classiﬁers is completely in-
terpretable and the margin mcan be set according to the an-
gle'subtended between a class weight and its connected
class weights as illustrated in Fig.1. For each of the three
RePoNet ﬁxed classiﬁers the angle 'can be analytically
determined as [9]:
's=arccos/parenleft.alt4−1
d/parenright.alt4; (10)
'o=
2; (11)
'c=arccos/parenleft.alt4d−2
d/parenright.alt4; (12)
respectively, where dis the feature space dimension size.
Fig. 3 shows the effect of setting:
m=':
In the ﬁgure we draw a schematic 2D diagram to show the
effect of the margin mon pushing the class decision bound-
ary to achieve feature compactness. In the standard case of
a learnable classiﬁer, as shown in Fig. 3 (left) , the value'
is not known in advance, it varies from class to class and
features are not guaranteed to distribute angularly centered
around their corresponding weights. Therefore, mcannot
𝑚=𝜑
𝑚
𝐰1
𝐰2𝐰1
𝐰2𝜑=?Softmax
Boundary
(learnable)Softmax
Boundary
(fixed)
𝜑/2𝜑Figure 3: Maximally compact feature learning with
RePoNet ﬁxed classiﬁers and the angular margin loss. Left:
In a standard learnable classiﬁer the decision boundaries
(dashed lines) deﬁned by the angular margin mdo not push
features to their respective weights uniformly (red arrows).
Right : In RePoNet classiﬁers the margin can be analyti-
cally determined ( m=') so that the decision boundaries
maximally push the features closer to their respective ﬁxed
weight.
be set in an interpretable way. Contrarily, in the case pro-
posed in this paper and shown in Fig. 3 (right) , the value'is
constant and known in advance, therefore by setting m=',
the class decision boundaries are maximally pushed to com-
pact features around their ﬁxed weights. This because the
Softmax boundary (from which the margin is added) is ex-
actly in between the two weights w1andw2. According to
this, the features generated by the proposed method are not
only maximally separated but also maximally compact (i.e.
maximally discriminative).
4. Exploratory Results
Experiments are conducted with the well-known MNIST
and EMNIST [34] datasets. MNIST contains 50;000train-
ing images and 10;000 test images. The images are in
grayscale and the size of each image is 28×28pixels. There
are 10 possible classes of digits. The EMNIST dataset (bal-
anced split) holds 112;800training images, 18;800test im-
ages and has 47classes including lower/upper case letters
and digits.
Fig. 4 shows a visual comparison between the features
generated by the RePoNet ﬁxed classiﬁers ( left column ) and
by a standard CNN baseline with learnable classiﬁers ( right
column ). Both approaches are trained according to the loss
of Eq. 9 and have exactly the same architecture, training
settings and embedding feature dimension used in Fig. 2.
Results are presented with a scatter plot matrix. Although
the two methods achieve substantially the same classiﬁca-
tion accuracy (i.e. 99:45% and99:47% respectively), it can
be noticed that the learned features are different. Speciﬁ-
cally, Margin-RePoNet follows the exact conﬁguration ge-
ometry of their related polytopes. Features follow very pre-
cisely their relative /upY,+, and×shapes therefore achieving
maximal separability. The standard baselines with learn-able classiﬁers (Fig. 4 left column ) achieve good but non
maximal separation between features. However, as the em-
bedding dimension decreases, as in Fig. 4(c), the separation
worsens.
This effect is particularly evident in more difﬁcult
datasets. Fig.5 shows the same visual comparison using the
EMNIST dataset where some of the 47classes are difﬁcult
to be correctly classiﬁed due to their inherent ambiguity.
Fig. 5 shows the scatter plot matrix of the d-Cube classiﬁer
(left) compared with its learnable classiﬁer baseline ( right )
in dimension d=6. Although also in this case they both
achieved the same classiﬁcation accuracy (i.e. 88:31% and
88:39%), the features learned by the baseline are neither
well separated nor compact.
Finally, in Fig. 6 we show the L2normalized features
(typically used in recognition) of both the training ( top) and
test set ( bottom ) for the same experiment shown in Fig. 5.
Class features in this case correctly follow the vertices of the
six-dimensional hypercube since all the parallel projections
deﬁned by each pairwise scatter plot result in the same unit
square centered at the origin.
5. Conclusion
We have shown how to extract features from Convo-
lutional Neural Networks with the desirable properties of
maximal separation and maximal compactness in a global
sense. We used a set of ﬁxed classiﬁers based on regular
polytopes and the additive angular margin loss. The pro-
posed method is very simple to implement and preliminary
exploratory results are promising.
Further implications may be expected in large face
recognition datasets with thousands of classes (as in
[7]) to obtain maximally discriminative features with a
signiﬁcant reduction in: the number of model param-
eters, the feature size and the hyperparameters to be
searched.
References
[1] Yandong Wen, Kaipeng Zhang, Zhifeng Li, and
Yu Qiao. A discriminative feature learning approach
for deep face recognition. In European Conference on
Computer Vision , pages 499–515. Springer, 2016. 1,
4
[2] Elad Hoffer, Itay Hubara, and Daniel Soudry. Fix
your classiﬁer: the marginal value of training the last
weight layer. In International Conference on Learning
Representations (ICLR) , 2018. 1, 2
[3] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hin-
ton. Imagenet classiﬁcation with deep convolutional
neural networks. In Advances in neural information
processing systems , pages 1097–1105, 2012. 1
(a)
(b)
(c)Figure 4: The distribution of MNIST learned features us-
ing the proposed method ( Left) and learned using a stan-
dard trainable classiﬁer ( Right ). The scatter plot highlights
the maximal separability and compactness of the extracted
features for the (a) d-Simplex, (b) d-Orthoplex and (c) d-
Cube classiﬁers. Class features are color coded. As the
feature space dimension decreases standard baselines have
difﬁculty in obtaining inter-class separation.
[4] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. Deep residual learning for image recognition.
InProceedings of the IEEE conference on computer
vision and pattern recognition , pages 770–778, 2016.
1
[5] G. Huang, Z. Liu, L. v. d. Maaten, and K. Q. Wein-
berger. Densely connected convolutional networks.
In2017 IEEE Conference on Computer Vision and
Pattern Recognition (CVPR) , pages 2261–2269, July
2017. 1Figure 5: The distribution of EMNIST (balanced split)
learned features. Left: Features learned using the d-Cube
ﬁxed classiﬁer with the additive angular margin loss. Right :
Features learned using a standard trainable classiﬁer with
the additive angular margin loss. In both cases the feature
dimension is 6and the classiﬁcation accuracy is compara-
ble. Maximal separability and compactness are evident in
our approach.
[6] Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and
Quoc V . Le. Learning transferable architectures for
scalable image recognition. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR) ,
June 2018. 1
[7] Q. Cao, L. Shen, W. Xie, O. M. Parkhi, and A. Zis-
serman. Vggface2: A dataset for recognising faces
across pose and age. In International Conference on
Automatic Face and Gesture Recognition , 2018. 1, 6
[8] Mahdi M Kalayeh, Emrah Basaran, Muhittin
G¨okmen, Mustafa E Kamasak, and Mubarak Shah.
Human semantic parsing for person re-identiﬁcation.
InProceedings of the IEEE Conference on Computer
Vision and Pattern Recognition , pages 1062–1071,
2018. 1
[9] Federico Pernici, Matteo Bruni, Claudio Baecchi, and
Alberto Del Bimbo. Fix your features: Stationary
and maximally discriminative embeddings using regu-
lar polytope (ﬁxed classiﬁer) networks. arXiv preprint
arXiv:1902.10441 , 2019. 1, 2, 3, 5
[10] Jiankang Deng, Jia Guo, and Stefanos Zafeiriou. Ar-
cface: Additive angular margin loss for deep face
recognition. The IEEE Conference on Computer Vi-
sion and Pattern Recognition (CVPR) , 2019. 1, 2, 3,
5
[11] Rajeev Ranjan, Carlos D Castillo, and Rama Chel-
lappa. L2-constrained softmax loss for discriminative
face veriﬁcation. arXiv preprint arXiv:1703.09507 ,
2017. 2, 3, 5
Figure 6: The distribution of the EMNIST normalized
learned features shown in 5 (Left) . (Top) training-set. ( Bot-
tom) test-set (best viewed in electronic version).
[12] Feng Wang, Xiang Xiang, Jian Cheng, and Alan Lod-
don Yuille. Normface: l 2 hypersphere embedding
for face veriﬁcation. In Proceedings of the 2017 ACM
on Multimedia Conference , pages 1041–1049. ACM,
2017. 2, 3, 4, 5
[13] Hao Wang, Yitong Wang, Zheng Zhou, Xing Ji, Di-
hong Gong, Jingchao Zhou, Zhifeng Li, and Wei
Liu. Cosface: Large margin cosine loss for deep facerecognition. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition , pages
5265–5274, 2018. 2, 4
[14] Weiyang Liu, Yandong Wen, Zhiding Yu, Ming Li,
Bhiksha Raj, and Le Song. Sphereface: Deep hy-
persphere embedding for face recognition. In CVPR ,
2017. 2, 3
[15] S Chopra, R Hadsell, and Y LeCun. Learning a simi-
larity metric discriminatively, with application to face
veriﬁcation. In 2005 IEEE Computer Society Con-
ference on Computer Vision and Pattern Recognition
(CVPR’05) , volume 1, pages 539–546. IEEE, 2005. 2
[16] Florian Schroff, Dmitry Kalenichenko, and James
Philbin. Facenet: A uniﬁed embedding for face recog-
nition and clustering. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition ,
pages 815–823, 2015. 2
[17] Ming-Ming Cheng Kai Zhao, Jingyi Xu. Regularface:
Deep face recognition via exclusive regularization. In
Proceedings of the IEEE Conference on Computer Vi-
sion and Pattern Recognition , 2019. 2, 3
[18] Weiyang Liu, Rongmei Lin, Zhen Liu, Lixin Liu,
Zhiding Yu, Bo Dai, and Le Song. Learning towards
minimum hyperspherical energy. NIPS , 2018. 2, 3
[19] Moritz Hardt and Tengyu Ma. Identity matters in deep
learning. International Conference on Learning Rep-
resentations (ICLR) , 2017. 2
[20] Alexandre Sablayrolles, Matthijs Douze, Cordelia
Schmid, and Herv ´e J´egou. Spreading vectors for sim-
ilarity search. International Conference on Learning
Representations (ICLR) , 2019. 3
[21] Weiyang Liu, Zhen Liu, Zhiding Yu, Bo Dai, Rongmei
Lin, Yisen Wang, James M. Rehg, and Le Song. De-
coupled networks. In The IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR) , 2018.
3
[22] Yu Liu, Hongyang Li, and Xiaogang Wang. Learning
deep features via congenerous cosine loss for person
recognition. arXiv preprint: 1702.06890 , 2017. 3
[23] Md Hasnat, Julien Bohn ´e, Jonathan Milgram,
St´ephane Gentric, Liming Chen, et al. von mises-
ﬁsher mixture model-based deep learning: Ap-
plication to face veriﬁcation. arXiv preprint
arXiv:1706.04264 , 2017. 3
[24] Yuhui Yuan, Kuiyuan Yang, and Chao Zhang. Feature
incay for representation regularization. arXiv preprint
arXiv:1705.10284 , 2017. 3[25] Feng Wang, Jian Cheng, Weiyang Liu, and Hai-
jun Liu. Additive margin softmax for face veriﬁca-
tion. IEEE Signal Processing Letters , 25(7):926–930,
2018. 3
[26] Hang Qi, Matthew Brown, and David G. Lowe. Low-
shot learning with imprinted weights. In The IEEE
Conference on Computer Vision and Pattern Recogni-
tion (CVPR) , June 2018. 3
[27] Zhirong Wu, Alexei A. Efros, and Stella X. Yu.
Improving generalization via scalable neighborhood
component analysis. In The European Conference on
Computer Vision (ECCV) , September 2018. 3
[28] Joseph John Thomson. XXIV. On the structure of the
atom: an investigation of the stability and periods of
oscillation of a number of corpuscles arranged at equal
intervals around the circumference of a circle; with
application of the results to the theory of atomic struc-
ture. The London, Edinburgh, and Dublin Philosophi-
cal Magazine and Journal of Science , 7(39):237–265,
1904. 3
[29] J Batle, Armen Bagdasaryan, M Abdel-Aty, and S Ab-
dalla. Generalized thomson problem in arbitrary di-
mensions and non-euclidean geometries. Physica A:
Statistical Mechanics and its Applications , 451:237–
250, 2016. 3
[30] Yann LeCun, L ´eon Bottou, Yoshua Bengio, and
Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE ,
86(11):2278–2324, 1998. 4
[31] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and
Jian Sun. Delving deep into rectiﬁers: Surpassing
human-level performance on imagenet classiﬁcation.
InProceedings of the IEEE international conference
on computer vision , pages 1026–1034, 2015. 4
[32] Diederik P. Kingma and Jimmy Ba. Adam: A method
for stochastic optimization. In International Confer-
ence on Learning Representations (ICLR) , 2015. 4
[33] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. Delving deep into rectiﬁers: Surpassing human-
level performance on imagenet classiﬁcation. In 2015
IEEE International Conference on Computer Vision,
ICCV , pages 1026–1034, 2015. 4
[34] Gregory Cohen, Saeed Afshar, Jonathan Tapson, and
Andr ´e van Schaik. EMNIST: extending MNIST to
handwritten letters. In 2017 International Joint Con-
ference on Neural Networks, IJCNN 2017, Anchorage,
AK, USA, May 14-19, 2017 , pages 2921–2926, 2017.
5