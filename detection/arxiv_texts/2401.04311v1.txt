arXiv:2401.04311v1  [cs.LG]  9 Jan 2024Private Truly-Everlasting Robust-Prediction
Uri Stemmer∗
January 9, 2024
Abstract
PrivateEverlastingPrediction(PEP),recentlyintroducedbyNaor etal.[2023],isamodelfor
diﬀerentiallyprivatelearninginwhichthelearnerneverpubliclyrelease sahypothesis. Instead,it
providesblack-boxaccesstoa“predictionoracle”thatcanpredic tthe labelsofan endless stream
of unlabeled examples drawn from the underlying distribution. Impor tantly, PEP provides
privacy both for the initial training set and for the endless stream o f classiﬁcation queries. We
present two conceptual modiﬁcations to the deﬁnition of PEP, as w ell as new constructions
exhibiting signiﬁcant improvements over prior work. Speciﬁcally, our contributions include:
•Robustness. PEP only guarantees accuracy provided that allthe classiﬁcation queries
are drawn from the correct underlying distribution. A few out-of- distribution queries
might break the validity of the prediction oracle for future queries, even for future queries
which are sampled from the correct distribution. We incorporate ro bustness against such
poisoning attacks into the deﬁnition of PEP, and show how to obtain it .
•Dependence of the privacy parameter δin the time horizon. We present a relaxed
privacy deﬁnition, suitable for PEP, that allows us to disconnect the privacy parameter δ
from the number of total time steps T. This allows us to obtain algorithms for PEP whose
sample complexity is independent from T, thereby making them “truly everlasting”. This
is in contrast to prior work where the sample complexity grows with po lylogT.
•New constructions. Prior constructions for PEP exhibit sample complexity that is
quadratic in the VC dimension of the target class. We present new constructio ns of PEP
for axis-alignedrectanglesand fordecision-stumps that exhibit sa mple complexity linearin
the dimension (instead of quadratic). We show that our construct ions satisfy very strong
robustness properties.
1 Introduction
The line of work on private learning , introduced by Kasiviswanathan et al. [2011], aims to under -
stand the computational and statistical aspects of PAC lear ning while providing strong privacy
protections for the training data. Recall that a (non-priva te) PAC learner is an algorithm that
takes a training set containing classiﬁed random examples a nd returns a hypothesis that should
be able to predict the labels of fresh examples from the same d istribution. A privatePAC learner
must achieve the same goal while guaranteeing diﬀerential pr ivacy w.r.t. the training data. This
means that the produced hypothesis should not be signiﬁcant ly aﬀected by any particular labeled
example. Formally, the deﬁnition of diﬀerential privacy is a s follows.
∗Tel Aviv University and Google research. u@uri.co.il . Partially supported by the Israel Science Foundation
(grant 1871/19) and by Len Blavatnik and the Blavatnik Famil y foundation.
1Deﬁnition 1.1 (Dwork et al. [2006]) .LetA:X∗→Ybe a randomized algorithm whose input is
a dataset D∈X∗. AlgorithmAis(ε,δ)-diﬀerentially private (DP) if for any two datasets D,D′
that diﬀer on one point (such datasets are called neighboring ) and for any outcome set F⊆Yit
holds that Pr[A(D)∈F]≤eε·Pr[A(D′)∈F]+δ.
Unfortunately, it is now known that there are cases where pri vate learning requires signiﬁcantly
more resources than non-private learning, which can be very prohibitive. I n particular, there are
simple cases where private learning is known to be impossible , even though they are trivial without
the privacy constraint. Once such example is the class of all 1-dimensional threshold functions
over the real line [Bun et al., 2015, Alon et al., 2022]. In add ition, there are cases where private
learning is possible, but provably requires signiﬁcantly m ore runtime than non-private learning
(under cryptographic assumptions) [Bun and Zhandry, 2016] .
The line of work on private prediction , introduced by Dwork and Feldman [2018], oﬀers a dif-
ferent perspective on private learning that circumvents so me these barriers. Speciﬁcally, in private
prediction we consider a setting where the private learning algorithm d oesnotoutput a hypoth-
esis. Instead, it provides a black-box access to a “predicti on oracle” that can be used to predict
the labels of fresh (unlabeled) examples from the underlyin g distribution. Works on private pre-
diction1showed that, informally, for any concept class Cthere is a private prediction algorithm
that can answer m“prediction queries” given a training set of size ≈VC(C)·√m. Note, how-
ever, that the number of “prediction queries” here is at most quadratic in the size of the initial
training set. This hinders the acceptance of private predic tion as a viable alternative to private
learning, as with the latter we obtain a privatized hypothes is that could be used to predict the
labels inﬁnitely many points. There are also empirical resu lts showing that when the number of
required predictions is large, then private prediction can be inferior to classical private learning
[van der Maaten and Hannun, 2020]. To tackle this, the recent work of Naor et al. [2023] intro-
duced the notion of private everlasting prediction (PEP) that supports an unbounded number of
prediction queries. To achieve this, Naor et al. [2023] allo wed the content of the black-box to con-
stantlyevolveas a function of the given queries, while guaranteeing priva cy both for the initial
training set and for the queries. Furthermore, they proved t hat such an “evolving” black-box is
necessary in order to achieve this. Informally, Naor et al. [ 2023] established the following theorem.
Theorem 1.2 (Naor et al.[2023], informal) .For every concept class Cthere is a private everlasting
predictor using training set of size ≈1
α·ε2·VC2(C), whereαis the accuracy parameter and εis
the privacy parameter (ignoring the dependence on all other parameters). The algorithm is not
necessarily computationally eﬃcient.
1.1 Our Contributions
Theorem 1.2 shows that PEP could be much more eﬃcient than cla ssical private learning (in
terms of sample complexity). Speciﬁcally, it shows that the sample complexity of PEP is at most
quadratic in the non-private sample complexity, while in th e classical model for private learning the
sample complexity can sometimes be arbitrarily far from tha t (if at all ﬁnite). However, a major
limitation of PEP is that it only guarantees accuracy provid ed thatallthe classiﬁcation queries are
drawn from the correct underlying distribution. Speciﬁcal ly, the algorithm initially gets a labeled
training set sampled according to some ﬁxed (but unknown) di stributionD, and then answers an
1See, e.g., [Dwork and Feldman, 2018, Bassily et al., 2018, Na ndi and Bassily, 2020, Dagan and Feldman, 2020].
2inﬁnite sequence of prediction queries, provided that all of these prediction queries are sampled
from the same distribution D. This is required in order to be able to “refhresh” the conten t of
the prediction oracle for supporting inﬁnitely many querie s without exhausting the privacy budget.
This could be quite limiting, where after investing eﬀorts in training the everlasting predictor, a
few out-of-distribution queries might completely contami nate the prediction oracle, rendering it
useless for future queries (even if these future queries wou ld indeed be sampled from the correct
distribution). We incorporate robustness against such poi soning attacks into the deﬁnition, and
introduce a variant of PEP which we call private everlasting robust prediction (PERP) . Informally,
the requirement is that the predictor should continue to pro vide utility, even if only a γfraction
of its queries are sampled from the correct distribution, an d all other queries are adversarial . We
do not require the algorithm to provide accurate prediction s on adversarial queries, only that these
adversarial queries wouldnotbreakthevalidity ofthepred ictoron“legitimate” queries. Weobserve
that the construction of Naor et al. [2023] could be adjusted to satisfy our new deﬁnition of PERP.
Informally,
Observation 1.3 (informal) .For every concept class Cthere is a private everlasting robustpre-
dictor using training set of size ≈1
α2·γ·ε2·VC2(C), whereαis the accuracy parameter, γis the
robustness parameter, and εis the privacy parameter (ignoring the dependence on all oth er param-
eters). The algorithm is not necessarily computationally eﬃ cient.
Note that the “price of robustness” in this observation is ro ughly1
αγ, as compared to the non-
robust construction of Theorem 1.2. We leave open the possib ility of a generic construction for
PERP with sample complexity linear in1
α, or with sample complexity that increases slower than
1
γ. We show that these two objectives are indeed achievable in s peciﬁc cases (to be surveyed later).
Disconnecting the privacy parameter δfrom the time horizon. Itiswidelyagreedthatthe
deﬁnition of diﬀerential privacy only provides meaningful g uarantees when the privacy parameter δ
is much smaller than1
n, wherenis the size of the dataset. The reason is that it is possible to satisfy
the deﬁnition while leaking about δnrecords in the clear (in expectation), so we want δn≪1 to
prevent such a leakage. In the context of PEP, this means that δshould be much smaller than
the total number of time steps (or queries), which we denote a sT. The downside here is that the
sample complexity of all known constructions for PEP grows w ith polylog(1
δ), which means that
the sample complexity actually grows with polylog( T).2This means that current constructions
for private everlasting prediction are not really “everlas ting”, as a ﬁnite training set allows for
supporting only a bounded number of queries (sub-exponenti al in the size of the training set). It
is therefore natural to ask if PEP could be made “truly everla sting”, having sample complexity
independentof thetimehorizon. We answer thisquestion in t heaﬃrmative, by presentingarelaxed
privacy deﬁnition (suitable for PEP) that allows us to disco nnect the privacy parameter δfrom the
time horizon T.
New constructions for PEP. Two shortcomings of the generic construction of Naor et al.
[2023], as well as our robust extension to it, are that (1) it i s computationally ineﬃcient , and
(2) it exhibits sample complexity quadratic in the VC dimension of the target class. We present a
computationally eﬃcientconstructionfortheclassofallaxis-aligned rectanglest hatexhibitssample
2This excludes cases where (oﬄine) private PAC learning is ea sy, such as learning point functions.
3complexity linearin the dimension. Furthermore, our constructing achieves v ery strong robustness
properties, wherethesamplecomplexity growsveryslowlya safunctionoftherobustnessparameter
γ. Speciﬁcally,
Theorem 1.4 (informal) .There exists a computationally eﬃcient PERP for axis aligned rectangles
inddimensions that uses sample complexity ≈d
α·ε2·log2(1
γ), whereαis the accuracy parameter, γ
is the robustness parameter, and εis the privacy parameter (ignoring the dependence on all oth er
parameters).
Via a simple reduction, we show that our construction can be u sed to obtain a robust predictor
also for the class of d-dimensional decision-stumps . (The VC dimension of this concept class is
known to be Θ(log d).) Speciﬁcally,
Theorem 1.5 (informal) .There exists a computationally eﬃcient PERP for d-dimensional decision-
stumps that uses sample complexity ≈logd
α·ε2·log2(1
γ), whereαis the accuracy parameter, γis the
robustness parameter, and εis the privacy parameter (ignoring the dependence on all oth er param-
eters).
For both of these classes, classical private learning is kno wn to be impossible (when deﬁned
over inﬁnite domains, such as the reals). Thus, Theorems 1.4 and 1.5 provide further evidence that
PEP could become a viable alternative to the classical model private learning: These theorems
provide examples where classical private learning is impos sible, while PEP is possible eﬃciently
with sample complexity that almost matches the non-private sample complexity (while satisfying
strong robustness properties).
2 Preliminaries
Notation. Two datasets SandS′are called neighboring if one is obtained from the other by
adding or deleting one element, e.g., S′=S∪{x′}. For two random variables Y,Zwe write
X≈(ε,δ)Yto mean that for every event Fit holds that Pr[ X∈F]≤eε·Pr[Y∈F] +δ, and
Pr[Y∈F]≤eε·Pr[X∈F]+δ. Throughout the paper we assume that the privacy parameter ε
satisﬁesε=O(1), but our analyses extend to larger values of epsilon.
2.1 Preliminaries from private prediction
We now provide the formal deﬁnitions for private everlastin g prediction. Before considering the
utility and privacy requirements, the following deﬁnition speciﬁes the interface, or the syntax,
required from a prediction algorithm.
Deﬁnition 2.1 (Prediction Oracle Naor et al. [2023]) .A prediction oracle is an algorithm Awith
the following properties:
1. At the beginning of the execution, algorithm Areceives a dataset S∈(X×{0,1})ncontaining
nlabeled examples and selects a (possibly randomized) hypot hesish0:X→{0,1}.
2. Then, in each round r∈N, algorithmAgets a query, which is an unlabeled element xr∈X,
outputshr−1(xr)and selects a (possibly randomized) hypothesis hr:X→{0,1}.
4The following deﬁnition speciﬁes the utility requirement f rom a prediction algorithm. Infor-
mally, the requirement is that allof the hypotheses it selects throughout the execution have l ow
generalization error w.r.t. the target distribution and ta rget concept.
Deﬁnition 2.2 (Everlasting Prediction Naor et al. [2023]) .LetAbe a prediction oracle. We say
thatAis an(α,β,n)-everlasting predictor for a concept class Cover a domain Xif the following
holds for every concept c∈Cand for every distribution DoverX. If the points in Sare drawn
i.i.d. fromDand labeled by c, and if all the queries x1,x2,...are drawn i.i.d. from D, then
Pr[∃r≥0s.t.errorD(c,hr)> α]≤β.
The following deﬁnition speciﬁes the privacy requirement f rom a prediction algorithm. Infor-
mally, in addition to requiring DP w.r.t. the initial traini ng set, we also require that an adversary
that completely determines A’sinputs, exceptforthe ithquery, andgets toseeall of A’spredictions
except for the predicted label for the ith query, cannot learn much about the ith query.
Deﬁnition 2.3 (Private Prediction Oracle Naor et al. [2023]) .A prediction oracle Ais a(ε,δ)-
privateif for every adversary Band every T∈N, the random variables View0
B,Tand View1
B,T
(deﬁned in Figure 1) are (ε,δ)-indistinguishable.
Finally, we can deﬁne private everlasting prediction to be algorithms that satisfy both Deﬁni-
tion 2.2 (the utility requirement) and Deﬁnition 2.3 (the pr ivacy requirement):
Deﬁnition 2.4 (PrivateEverlastingPredictionNaor et al. [2023]) .AlgorithmAis an(α,β,ε,δ,n )-
Private Everlasting Predictor (PEP) if it is an (α,β,n)-everlasting predictor and (ε,δ)-private.
As we mentioned, Naor et al. [2023] presented a generic const ruction for PEP with the following
properties:
Theorem 2.5 (Naor et al. [2023]) .For every concept class Cand every α,β,ε,δ there is an
(α,β,ε,δ,n )-PEP for Cwheren=˜O/parenleftBig
VC2(C)
αε2·polylog/parenleftBig
1
βδ/parenrightBig/parenrightBig
.The algorithm is not necessarily
eﬃcient.
3 Conceptual Modiﬁcations to the Deﬁnition of PEP
In this section we present the two conceptual modiﬁcation we suggest to the deﬁnition of private
everlasting prediction.
3.1 Robustness to out-of-distribution queries
We introduce a modiﬁed utility requirement for PEP in which o nly aγfraction of the queries are
assumed to be sampled from the target distribution, while al l other queries could be completely
adversarial. This modiﬁes only the utility requirement of P EP (Deﬁnition 2.2), and has no eﬀect
on the privacy requirement (Deﬁnition 2.3).
Deﬁnition 3.1 (Everlasting robust prediction) .LetAbe a prediction oracle (as in Deﬁnition 2.1).
We say thatAis an(α,β,γ,n )-everlasting robust predictor for a concept class Cover a domain
Xif the following holds for every concept c∈C, every distribution DoverX, and every adversary
F. Suppose that the points in the initial dataset Sare sampled i.i.d. from Dand labeled correctly by
5Parameters: b∈{0,1},T∈N.
Training Phase:
1. The adversary Bchooses two labeled datasets S0,S1∈(X×{0,1})∗which are either
neighboring or equal. % IfS0,S1are neighboring, then one of them can be obtained by the other
by adding/removing one labeled point.
2. IfS0=S1then set c0= 0. Otherwise set c0= 1. % We refer to the round rin which cr= 1
as the “challenge round”. If c0= 1 then the adversary chooses to place the challenge already in the initial
dataset. Otherwise, the adversary will have the chance to po se a (single) challenge round in the following
prediction phase.
3. AlgorithmAgetsSb.
Prediction phase:
4. For round r= 1,2,...,T:
(a) The adversary Boutputs a point xr∈Xand a bit cr∈{0,1}, under the restriction
that/summationtextr
j=0cj≤1.
(b) If (cr= 0 orb= 1) then algorithm Agetsxrand outputs a prediction ˆ yr.
(c) If (cr= 1 and b= 0) then algorithm Agets⊥.% Here⊥is a special symbol denoting
“no query”.
(d) Ifcr= 0 then the adversary Bgets ˆyr.% The adversary does not get ˆ yrifcr= 1.
Let Viewb
B,TbeB’s entire view of the execution, i.e., the adversary’s rando mness and the
sequence of predictions it received.
Figure 1: Deﬁnition of View0
B,tand View1
B,t.
c. Furthermore, suppose that each of the query points xiis generated as follows. With probability
γ, the point xiis sampled fromD. Otherwise (with probability 1−γ), the adversaryFchoosesxi
arbitrarily, based on all previous inputs and outputs of A. Then,Pr[∃r≥0s.t.errorD(c,hr)> α]≤
β.
Remark 3.2. The robustness parameter γcould actually be allowed to decrease throughout the exe-
cution. That is, as the execution progresses, the algorithm c ould potentially withstand growing rates
of adversarial queries. For simplicity, in Deﬁnition 3.1 we treatγas remaining ﬁxed throughout
the execution.
We now deﬁne private everlasting robustprediction as a combination of the privacy deﬁnition
of Naor et al. [2023] (Deﬁnition 2.3) with our modiﬁed utilit y deﬁnition (Deﬁnition 3.1).
Deﬁnition 3.3 (Private Everlasting Robust Prediction) .AlgorithmAis an(α,β,γ,ε,δ,n )-Private
6Everlasting Robust Predictor (PERP) if it is an (α,β,γ,n )-everlasting robust predictor (as in Def-
inition 3.1) and it is (ε,δ)-private (as in Deﬁnition 2.3).
Armed with our new deﬁnition of PERP, we observe that the cons truction of Naor et al. [2023]
extends from PEP to PERP, at the cost of inﬂating the sample co mplexity by roughly a1
αγfactor.
Speciﬁcally,
Observation 3.4. For every concept class Cand every α,β,γ,ε,δ there is an (α,β,γ,ε,δ,n )-
PERP for Cwheren=˜O/parenleftBig
VC2(C)
α2·γ·ε2·polylog/parenleftBig
1
βδ/parenrightBig/parenrightBig
.The algorithm is not necessarily eﬃcient.
This observation follows from essentially the same analysi s as in [Naor et al., 2023]. Here we
only ﬂesh out the reason for the1
αγblowup to the sample complexity. Informally, the generic
construction of [Naor et al., 2023] is based on the following subroutine:
1. Take a training set Scontaining nlabeled samples.
2. Partition SintoT≈αn
VC(C)chunks of size≈VC(C)
αeach.
3. For each chunk i∈[T], identify a hypothesis hi∈Cthat agrees with the ith chunk.
4. For≈ε2T2
αsteps: take an unlabeled query xand label it using a noisy majority vote among
theThypotheses.
% We can support ≈ε2T2
αqueries as in roughly a (1 −α)-fraction of them there will be a strong consensus
among the Thypotheses, to the extent that these labels would come “for f ree” in terms of the privacy analysis.
So we only need to “pay” for about ε2T2queries, which is standard using composition theorems.
5. At the end of this process, we have ≈ε2T2
αnew labeled examples, which is more than n
provided that n/greaterorsimilarVC2(C)
α·ε2. This allows us to repeat the same subroutine with these new
labeled examples as our new training set.
% This is actually not accurate, as this new training set cont ains errors while the original training set did not.
In the full construction additional steps are taken to ensur e that the error does not accumulate too much from
one phase to the next, making their construction ineﬃcient.
Based on this, Naor et al. [2023] presented a generic constru ction for PEP exhibiting sample
complexity n=˜O/parenleftBig
VC2(C)
α·ε2·polylog/parenleftBig
1
βδ/parenrightBig/parenrightBig
.Now supposethat we wouldlike tobeable to withstand
(1−γ)-fraction of adversarial queries. First, this prevents us from optimizing the number of
supported queries in Step 4, and we could only answer ≈ε2T2queries. The reason is that with
adversarial queries we can no longer guarantee that the vast majority of the queries would be in
consensus among the hypotheses we maintain. Second, only a γ-fraction of these queries would be
“true samples”, and we would need to ensure that the number of such new “true samples” is more
than what we started with. This translate to a requirement th atn/greaterorsimilarVC2(C)
α2·γ·ε2.
3.2 Truly everlasting private prediction
As we mentioned in the introduction, all current constructi ons for PEP exhibit sample complexity
that scale as polylog(1
δ). In addition, the deﬁnition of diﬀerential privacy is only c onsidered ade-
quate in this case provided that δ≪1
T, which means that current constructions for PEP are not
“truly everlasting” as they require a training set whose siz e scales with the bound on the number
of queries T. Formally,
7Deﬁnition 3.5 (Leakage attack) .
•LetAbe an (oﬄine) algorithm that operates on a dataset D∈[0,1]T. We say thatAisleaking
if when running it on a uniformly random dataset D, its outcome can be post-processed to
identify a point ythat with probability at least 1/2 satisﬁes y∈D.
•LetAbe an (interactive) prediction oracle, and consider an adve rsaryBthat interacts with A
forTtime steps, and adaptively decides on k≤Tsteps at which the algorithm gets a random
uniform sample from [0,1]without the adversary learning these points directly. We re fer to
these points as “challenge points”. The adversary arbitrari ly chooses all other inputs to the
algorithm and sees all of its outputs (the predictions it ret urns). We say that Aisleakingif
there is an adversary Bthat, with probability at least 1/2, at the end of the executi on outputs
a pointythat is identical to one of the challenge points.
Fact 3.6. LetAbe an algorithm that takes a dataset D∈[0,1]Tand outputs every input point
independently with probability δ. This algorithm satisﬁes (0,δ)-DP. Furthermore, if T≥1
δ, thenA
is leaking.
We put forward a simple twist to the privacy deﬁnition that al lows us to exclude this unde-
sired behaviour, without inﬂating the sample complexity of our algorithms by a polylog( T) factor.
Speciﬁcally, we require δtodecrease over time . Intuitively, instead of allowing every record ito
be leaked with probability δ, we allow the ith record to be leaked with probability at most δ(i),
such that/summationtext
iδ(i)/definesδ∗. In the context of PEP, we show that the sample complexity onl y needs to
grow with1
δ∗and not with max i{1
δ(i)}. This is a signiﬁcant improvement: While max i{1
δ(i)}must
be larger than Tto prevent the leakage attack mentioned above, this is not th e case with1
δ∗which
can be taken to be a small constant. This still suﬃces in order to ensure that the algorithm is not
leaking.
As a warmup, let us examine this in the context of the standard (oﬄine) deﬁnition of diﬀerential
privacy, i.e., in the context of Deﬁnition 1.1.
Deﬁnition 3.7. Letε≥0be a ﬁxed parameter and let δ:N→[0,1]be a function. Let A:X∗→Y
be a randomized algorithm whose input is a dataset. Algorith mAis(ε,δ)-DPif for any index i,
any two datasets D,D′that diﬀer on the ith entry, and any outcome set F⊆Yit holds that
Pr[A(D)∈F]≤eε·Pr[A(D′)∈F]+δ(i).
We now turn to the adaptive setting, as is required by the PEP m odel. Intuitively, the compli-
cation over the non-adaptive case is that the index iin which the adversary poses its challenge is
itself a random variable. This prevents us from directly ext ending Deﬁnition 3.7 to the adaptive
case, because Deﬁnition 3.7 uses the index ito quantify the closeness between the two “neighbor-
ing distributions”, and if iis a random variable then these two “neighboring distributi ons” are not
well-deﬁned until iis set. A direct approach for handling with this is to require that a similar
deﬁnition holds for all conditioning on i. This is captured in the following deﬁnition, where we use
the the same terminology as in Deﬁnition 2.3 (the privacy deﬁ nition for PEP), and use r∗to denote
the challenge round, i.e., the round rin which cr= 1.
Deﬁnition 3.8. Letε≥0be a ﬁxed parameter and let δ:N→[0,1]be a function. A prediction
oracleAis a(ε,δ)-private if for every adversary B, everyT∈N, and every i∈[T], conditioned
onr∗=ithen the random variables View0
B,Tand View1
B,T(deﬁned in Figure 1) are (ε,δ(i))-
indistinguishable.
8Note that this strictly generalizes Deﬁnition 2.3. Indeed, by taking δto be ﬁxed, i.e., δ(i) =δ∗
for alli, we get that View0
B,Tand View1
B,Tare (ε,δ∗)-indistinguishable (without the conditioning).
To see this, note that for any event Fwe have
Pr[View0
B,T∈F] =/summationdisplay
iPr[r∗=i]·Pr[View0
B,T∈F|r∗=i]
≤eε/parenleftBigg/summationdisplay
iPr[r∗=i]·Pr[View1
B,T∈F|r∗=i]/parenrightBigg
+δ∗
=eε·Pr[View1
B,T∈F]+δ∗.
So Deﬁnition 3.8 is not weaker than Deﬁnition 2.3. The other d irection is not true. Speciﬁcally,
consider an algorithm Athat with probability δdeclares (upon its instantiation) that it is going to
publish the ﬁrst record in the clear. Otherwise the algorith m publishes nothing. This is typically
something that would not be considered as a “privacy violati on”, as this catastrophic event hap-
pens only with probability δ. Indeed, this algorithm would satisfy Deﬁnition 2.3. Howev er, with
Deﬁnition 3.8, an adaptive attacker might decide to pose its challenge on the ﬁrst input if and only
ifthe algorithm has issued such a declaration. Otherwise the a ttacker never poses its challenge
on the ﬁrst record. In this case, in the conditional space whe rer∗= 1, we have that the attacker
succeeds with probability one , and so this algorithm cannot satisfy Deﬁnition 3.8. This be havior
makes Deﬁnition 3.8 somewhat harder to work with. We propose the following relaxed variant of
Deﬁnition 3.8 that still rules out leaking algorithms.
Deﬁnition 3.9. Letε≥0be a ﬁxed parameter and let δ:N→[0,1]be a function. A prediction
oracleAis a(ε,δ)-private if for every adversary B, everyT∈N, everyi∈[T], and every event F
it holds that
Pr[View0
B,T∈Fandr∗=i]≤eε·Pr[View1
B,T∈Fandr∗=i]+δ(i),
and vice versa.
This deﬁnition is strictly weaker (provides less privacy) t han Deﬁnition 3.8. In particular, via
the same calculation as above, when taking δ≡δ∗it only implies ( ε,/summationtext
iδ∗)-DP rather than ( ε,δ∗)-
DP. Nevertheless, when/summationtext
iδ(i)≪1, this deﬁnition still prevents the algorithm from leaking , even
ifTis much larger than (/summationtext
iδ(i))−1.
Observation 3.10. LetAbe an(ε,δ)-private algorithm (as in Deﬁnition 3.9), where/summationtext
iδ(i)<1
8.
ThenAis not leaking.
Proof.Assume towards contradiction that Ais leaking, and let Bbe an appropriate adversary (as
in Deﬁnition 3.5). Then, there must exist an index isuch that with probability at least 4 δ(i) it
holds that (1) this coordinate is chosen to be a challenge poi nt, and (2) the adversary recovers this
point. Otherwise, by a union bound, the probability of Bsucceeding in its leakage attack would be
at most/summationtext
i4δ(i)<1
2. Letibe such an index, and consider a modiﬁed adversary Bthat interacts
withA(as in Figure 1) while simulating B. More speciﬁcally, the adversary Bﬁrst samples T
uniformly random points x1,...,x T∈[0,1] and runsBinternally. Whenever Bspeciﬁes an input
thenBpasses this input to A. In rounds jwhereBasks to giveAa random point, then Bpassesxj
toA. Recall that (as in Figure 1) the adversary Bneeds to choose one round as its challenge round;
9this is chosen to be round i. In that round the adversary Bdoes not get to see A’s prediction but
it still needs to pass the prediction to Bin order to continue the simulation. It passes a random
bityitoBas if it is the prediction obtained from A(in all other rounds, Bpasses the predictions
obtained fromAtoB). Now, if the bit bin the experiment speciﬁed by Figure 1 is 1, and if yiis
equal to the prediction returned by Afor theith bit (which happens with probability 1 /2), then
this experiment perfectly simulates the interaction betwe enBandA. Hence, whenever b= 1 then
at the end of the execution Bguessesxiwith probability at least 2 δ(i). On the other hand, if the
bitbin the experiment speciﬁed by Figure 1 is 0, then Bgets no information about xiand hence
guesses it with probability exactly 0. This contradicts the deﬁnition of ( ε,δ)-privacy.
4 New Constructions for PEP
In this section we present constructions for PEP for speciﬁc concept classes that outperform the
generic construction of Naor et al. [2023] (and our robust ex tension of it) on three aspects: (1)
our constructions are computationally eﬃcient; (2) our con structions exhibit sample complexity
linearin the VC dimension rather than quadratic; and (3) our constr uctions achieve signiﬁcantly
stronger robustness guarantees than our extension to the co nstruction of Naor et al. [2023]. We
ﬁrst introduce additional preliminaries that are needed fo r our constructions.
4.1 Additional preliminaries
The Reorder-Slice-Compute (RSC) paradigm. Consider a case in which we want to apply
several privacy-preserving computations to our dataset, w here each computation is applied to a
disjoint part (slice) of the dataset. If the slices are selected in a data-i ndependent way, then a
straightforward analysis shows that our privacy guarantee s do not deteriorate with the number of
slices. The following algorithm (Algorithm RSC) extends this to the more complicated case where
we need to select the slices adaptively in a way that depends o n the outputs of prior steps.
Algorithm 1 RSC (Reorder-Slice-Compute) [Cohen et al., 2023]
Input:DatasetD∈Xn, integer τ≥1, and privacy parameters 0 < ε,δ < 1.
Fori= 1,...,τdo:
1. Receive a number mi∈N, an ordering≺(i)overX, and an ( ε,δ)-DP protocolAi.
2. ˆmi←mi+Geom(1−e−ε)
3.Si←the largest ˆ mielements in Dunder≺(i)
4.D←D\Si
5. InstantiateAonSiand provide external access to it (via its query-answer inte rface).
Theorem 4.1 (Cohen et al. [2023]) .For every ˆδ >0, Algorithm RSCis(O(εlog(1/ˆδ)),ˆδ+2τδ))-
DP.
Algorithm Stopper.The following algorithm (Algorithm Stopper) and its corresponding theo-
rem(Theorem4.2)followasaspecialcaseoftheclassical AboveThreshold algorithmofDwork et al.
[2009], also known as the Sparse Vector Technique. It allows us to continually monitor a bit stream,
and to indicate when the number of ones in this stream (roughl y) crosses some threshold.
10Algorithm 2 Stopper
Input:Privacy parameters ε,δ, threshold t, and a dataset Dcontaining input bits.
1.In update time: Obtain an input bit x∈{0,1}and add xtoD.
2.In query time:
(a) Let/tildewidestsumi←Lap/parenleftbig8
εlog(2
δ)/parenrightbig
+/summationtext
x∈Dx
(b) If/tildewidestsumi≥tthen output⊤and HALT
(c) Else output⊥and CONTINUE
Theorem 4.2. Algorithm Stopper is(ε,δ)-DP w.r.t. input bits (both in the initial dataset Dand
in query times).
Algorithm BetweenThresholds .The following algorithm (Algorithm BetweenThresholds ) al-
lows for testing a sequence of low-sensitivity queries to le arn whether their values are (roughly)
above or below some predeﬁned thresholds. The properties of this algorithm are speciﬁed in The-
orem 4.3.
Algorithm 3 BetweenThresholds [Bun et al., 2017]
Input:DatasetS∈X∗, privacyparameters ε,δ, numberof“medium”reports kwherek≥4log(2
δ),
thresholds tl,thsatisfying th−tl≥16
ε/radicalBig
klog(2
δ), and an adaptively chosen stream of queries
fi:X∗→Rwith sensitivity 1.
1. Letc= 0
2. In each round i, when receiving a query fi, do the following:
(a) Letˆfi←fi(S)+Lap/parenleftBig
4
ε/radicalBig
klog(2
δ)/parenrightBig
(b) Ifˆfi< tlthen output “low”
(c) Else if ˆfi> ththen output “high”
(d) Else output “medium” and set c←c+1. Ifc=kthen HALT.
Theorem 4.3 (Bun et al. [2017], Cohen and Lyu [2023]) .Algorithm BetweenThresholds is(ε,δ)-
DP
Algorithm ChallengeBT .Note that algorithm BetweenThresholds halts exactly after the kth
time that a “medium” answer is returned. In our application w e would need a variant of this
algorithm in which the halting time leaves some ambiguity as to the exact number of “medﬁum”
answers obtained so far, and to whether the last provided ans wer was a “medium” answer or not.
Consider algorithm ChallengeBT , which combines Algorithm BetweenThresholds with Algorithm
Stopper.
11Algorithm 4 ChallengeBT
Input:DatasetS∈X∗, privacyparameters ε,δ, numberof“medium”reports kwherek≥4log(4
δ),
thresholds tl,thsatisfying th−tl≥32
ε/radicalBig
klog(4
δ), a bound on the number of steps T, and an
adaptively chosen stream of queries fi:X∗→Rwith sensitivity 1.
1. Instantiate Stopper with privacy parameters ( ε,δ), and threshold t=kon the empty
dataset.
2. Instantiate a modiﬁed version of BetweenThresholds onSwith parameters ( ε,δ
2),k′=
k+8
εlog(2
δ)log(T
δ),tl,th. The modiﬁcation to the algorithm is that the algorithm neve r
halts (an so it does not need to maintain the counter c).
3. Denote Flag = 1.
4. Support the following queries:
(a)Stopping query: Set Flag = 1. Query Stopper to get an answer a, and output a. If
Stopper halted during this query then HALT the execution.
(b)BT query: Obtain a sensitivity-1 function f. If Flag = 0 then ignore this fand do
nothing. Otherwise do the following:
i. Set Flag = 0.
ii. FeedftoBetweenThresholds and receive an answer a. Output a.
iii. Ifa=“medium” then update Stopper with 1 and otherwise update it with 0.
Theorem 4.4. ChallengeBT is(ε,δ)-DP w.r.t. the input dataset S.
Proof.First note that if in Step2 we were to execute BetweenThresholds without modiﬁcations,
then the theorem would follow the privacy guarantees of BetweenThresholds , asChallengeBT
simply post-processes its outcomes. As we explain next, thi s modiﬁcation introduces a statistical
distance of at mostδ
2, and thus the theorem still holds. To see this, observe that a lgorithm Stopper
tracks the number of “medium” answers throughout the execut ion. Furthermore, note that the
valuek′with which we instantiate algorithm BetweenThresholds is noticeably bigger than k(the
threshold given to Stopper), so that with probability at least 1 −δ
2algorithm Stopper haltsbefore
thek′time in which a “medium” answer is observed. When that happen s, the modiﬁcation we
introduced to BetweenThresholds has no eﬀect on the execution.3This completes the proof.
4.2 Axis-aligned rectangles
We are now ready to present our construction for axis-aligne d rectangles in the Euclidean space
Rd. A concept in this class could be thought of as the product of dintervals, one on each axis.
Formally,
Deﬁnition 4.5 (Axis-Aligned Rectangles) .Letd∈Ndenote the dimension. For every w=
(a1,b1,...,ad,bd)∈R2ddeﬁne the concept recw:Rd→{0,1}as follows. For x= (x1,...,x d)∈Rd
3Formally, let Edenote the even that all of the noises sampled by Stopper throughout the execution are smaller
thank′−k. We have that Pr[ E]≥1−δ/2. Furthermore, conditioned on E, the executions with and without the
modiﬁcation are identical. This shows that this modiﬁcatio n introduces a statistical distance of at most δ/2.
12we haverecw(x) = 1iﬀ for every i∈[d]it holds that xi∈[ai,bi]. Deﬁne the class of all axis-aligned
rectangles over RdasRECd={recw:w∈R2d}.
4.2.1 A simpliﬁed overview of our construction
As we mentioned in Section 3.1, the generic construction of N aor et al. [2023] operates by main-
tainingk≫1 independent hypothesis, and privately aggregating their predictions for each given
query. After enough queries have been answered, then Naor et al. [2023] re-trains (at least) knew
models, treating the previously answered queries as the new training set. However, maintaining
k≫1complete hypotheses can be overly expensive in terms of the sample com plexity. This is the
reason why their construction ended up with sample complexi tyquadratic in the VC dimension.
We show that this can be avoided for rectangles in high dimens ions. Intuitively, our gain comes
fromnotmaintaining many complete hypotheses, but rather a single “ evolving” hypothesis.
To illustrate this, let us consider the case where d= 1. That is, for some a≤b, the target
function c∗is anintervalof the form c∗(x) = 1 iﬀ a≤x≤b. Let us also assume that the target
distributionDis such that 0≪Prx∼D[c∗(x) = 1]≪1, as otherwise the all 0 or all 1 hypothesis
would be a good solution.
Now suppose we get a sample Sfrom the underlying distribution D, labeled by c∗, and let
Sleft⊆SandSright⊆Sbe two datasets containing the m≈1
αεsmallest positive points Sand
themlargest positive points S(respectively). We can use Sleft,Srightto privately answer queries
as follows: Given a query x∈R, ifxis smaller than (almost) all of the points in Sleft, or larger
than (almost) all of the points in Sright, then we label xas 0. Otherwise we label it as 1. By the
Chernoﬀ bound, this would would have error less than α. Furthermore, in terms of privacy, we
could support quite a few queries using the “sparse vector te chnique” (or the BetweenThresholds
algorithm; see Section 4.1). Informally, queries xs.t.
1. max{a∈Sleft}≤x≤min{b∈Sright}; or
2.x≤min{a∈Sleft}; or
3. max{b∈Sright}≤x
would not incur a privacy loss. Thus, in the privacy analysis , we would only need to account for
queriesxthat are “deep” inside Sleftor “deep” inside Sright. Recall that we aim to operate in the
adversarial case, where the vast majority of the queries can be adversarial. Thus, it is quite possible
that most of the queries would indeed incur such a privacy los s. However, the main observation
here is that if xgenerated a privacy loss w.r.t. (say) Sleft, then we can use xto replace one of the
points in Sleft, as it is “deep” inside it. So essentially every time we incur a privacy loss w.r.t. one
of the “boundary datasets” we maintain, we get a new point to a dd to them in exchange. We show
that this balances out the privacy loss all together.
More precisely, it is actually nottrue that every query that generates a privacy loss w.r.t.
some “boundary dataset” could be added to it. The reason is th at we want our construction to be
“everlasting”, supporting an unboundednumber of queries. In this regime the BetweenThresholds
algorithm would sometimes err and falsely identify a query xas being “deep” inside one of our
“boundary datasets”. There would not be too many such cases, so that we can tolerate the privacy
loss incurred by such errors, but we do not want to add unrelat ed/adversarial queries to the
“boundary datasets” we maintain as otherwise they will get c ontaminated over time. In the full
13construction we incorporate measures to protect against th is. Our formal construction is given in
algorithm RectanglesPERP .
4.2.2 Privacy analysis of RectanglesPERP
Theorem 4.6. RectanglesPERP is(ε,δ)-private, as in Deﬁnition 3.9, where/summationtext
iδ(i)≤δ∗.
Proof.Using the notations of RectanglesPEP , we deﬁne the function δ:N→[0,1] where δ(i) =
δp(i)and where p(i) denote the phase to which ibelongs. When the time ior the phase pis clear
from the context, we write δpinstead of δp(i)=δ(i) to simplify the notations. Fix T∈N, ﬁx an
adversaryBfor interacting with RectanglesPERP as in Figure 1, and consider the random variables
View0
B,Tand View1
B,T. Now ﬁx an index i∈[T] and let p=p(i). We need to show that for every
eventFit holds that
Pr[View0
B,T∈Fandci= 1]≈(ε,δp)Pr[View1
B,T∈Fandci= 1].
To simplify notations, for b∈{0,1}let us deﬁne the random variable
Viewb
B,T=/braceleftBigg
Viewb
B,Tifci= 1
⊥ else
This is well-deﬁned as ciis included in the view of the adversary. Using this notation , our goal is
to show that
View0
B,T≈(ε,δp)View1
B,T.
To show this, we leverage the simulation proof-technique of Cohen et al. [2023]. Speciﬁcally, we
will show that Viewb
B,Tcan be perfectly simulated by post-processing the outcome o f an (ε,δp)-DP
computation on the bit b. To this end, we describe two interactive mechanisms: a simu lator Sim
and a helper Helper. The goal of the simulator is to simulate B’s view in the game speciﬁed in
Figure 1. This simulator does not know the value of the bit b. Still, it executes Band attempts
to conduct as much of the interaction as it can without knowin gb. Only when it is absolutely
necessary for the simulation to remain faithful, our simula tor will pose queries to Helper(who
knows the value of the bit b), and Helperresponds with the result of a DP computation on b. We
will show the following two statements, which imply the theo rem by closure to post-processing:
1.Helperis (ε,δp)-DP w.r.t. the bit b,
2.Simperfectly simulates Viewb
B,T.
The simulator Simbegins by instantiating the adversary B(who is expecting to interact with
RectanglesPERP through the game speciﬁed in Figure 1). We analyze the execut ion in cases,
according to the time in which the adversary sets cr= 1. At any case, if Bposes its challenge in
any round other than ithen the simulator outputs ⊥, as in the deﬁnition of Viewb
B,T.
Easy case: i= 0 and c0= 1.That is, the adversary Bspeciﬁes two neighboring datasets
S0/\e}atio\slash=S1in the beginning of the execution. In this case, Simasks Helperto execute Step 3 of
RectanglesPERP . That is, to execute RSConSband to provide Simwith the querying interface to
the resulting instantiations of ChallengeBT . This preserves ( ε,δp)-DP by the privacy guarantees of
14Algorithm 5 RectanglesPERP
Initial input: Labeled dataset S∈(Rd×{0,1})nand parameters ε,δ∗,α,β,γ.
Notations and parameters: Forp∈Ndenoteαp=α/2p, andβp=β/2p, andδp=
˜Θ/parenleftBig
δ∗γαε2β
d·8p/parenrightBig
, andmp= Θ/parenleftBig
1
ε2log5/parenleftBig
d
αp·βp·γ·ε·δp/parenrightBig/parenrightBig
, andkp= 2·mp, andtp= Θ/parenleftBig
d·mp
γ·αp/parenrightBig
, and
∆p= Θ/parenleftBig
log(1/δp)
ε/radicalBig
kplog(d
δp)log/parenleftBig
d·tp
βp/parenrightBig/parenrightBig
.
1. Letp= 1. %pdenotes the current “phase” of the execution.
2. Instantiate algorithm RSConS.
3. Forj= 1,2,...,d: Use RSCto run a copy of ChallengeBT on the≈mppositive examples
inSwith largest jth coordinate, and another copy on the ≈mppositive examples in Swith
smallest jth coordinate. Each execution with kpas the number of “medium” answers, with/parenleftBig
ε
log(1/δp),δp
d/parenrightBig
as the privacy parameters, and with thresholds tl= ∆pandth= 2∆p. Denote
these executions as RightjandLeftj, respectively.
4. LetD=∅and for every j∈[d] letDleft
j=Dright
j=∅.
5. For time i= 1,2,3,...do the following: % Inﬁnite loop for answering queries.
(a) For all j∈[d], pose a “stopping query” to Leftjand to Rightj.
(b) If any of LeftjorRightjhas halted during Step 5a then re-execute it on (a copy of)
the corresponding dataset Dleft
jorDright
j, respectively, and then empty this dataset.
(c) Obtain an unlabeled query point xi∈X
(d) Set ˆyi←0.% The default label is zero; this might change in the followin g steps.
(e) Ifxi=⊥the GOTO Step 5j. % Here⊥is a special symbol denoting “no query”.
(f) Forj= 1,2,...,ddo:
i. Query Leftjfor the number of points in its dataset whose jth coordinate are
bigger than xi[j], and obtain an answer a. Ifa=“high” then GOTO Step 5h. If
a=“medium” then add xitoDleft
jand GOTO Step 5i.
ii. Query Rightjfor the number of points in its dataset whose jth coordinate are
smaller than xi[j], and obtain an answer a. Ifa=“high” then GOTO Step 5h. If
a=“medium” then add xitoDright
jand GOTO Step 5i.
(g) Set ˆyi←1% Changing the label to 1 if all our tests succeeded.
(h) Add ( xi,ˆyi) toD
(i) Output the predicted label ˆ yi
(j) Ifi=/summationtext
q∈[p]tqthen: % The current phase ends.
i. Letp←p+1.
ii. Stop all copies of LeftjandRightj, and instantiate algorithm RSConD.
iii. Use RSCto obtain new executions RightjandLeftjforj∈[d] (as in Step 3).
iv. LetD=∅and for every j∈[d] letDleft
j=Dright
j=∅.
15RSC. The continuation of the execution can be done without any fu rther access to the bit b. That
is, the simulator Simcontinues to query the instantiations of ChallengeBT produces by Helper,
and whenever a new instantiation is required it can run it its elf since there are no more diﬀerences
between the execution with b= 0 or with b= 1.
More involved case: i=r≥1 andcr= 1.In this case, the simulator Simcan completely
simulate the execution of RectanglesPERP until the rth round. In particular, at the beginning of
therth round, the simulator is running (by itself) the existing c opies of ChallengeBT , referred to
asLeftjandRightjin the code of the algorithm. During the rth round, the simulator obtains
from the adversary the query point xr, but as it does not know the bit b, it does not know who
among{xr,⊥}needs to befed to algorithm RectanglesPERP .Either way, the simulator can execute
Steps 5a till 5d, as they do not depend on the current query. Ne xt, the simulator runs Step 5f
assuming that the query is xr.We proceed according to the following two sub-cases:
1. A “medium” answer is not encountered during the simulation of Step 5f. Recall
that a BT-query to algorithm ChallengeBT only changes its inner state if the answer is
“medium”. Hence, in this case, the execution of Step 5f with xrdid not aﬀect the inner
states of any of the executions of ChallengeBT , and the simulator can continue using them.
However, the executions with xrand with⊥are still not synchronized, as xrwould be added
to the dataset Din Step 5h whereas ⊥would not. This means that from this moment until
a new phase begins (in Step 5j), there are two possible option s for the dataset D: either
withxror without it. Thus, the next time that this dataset needs to b e used by algorithm
RectanglesPERP , which happens in Step 5(j)iii, the simulator could not do it itself. Instead,
it would ask Helperto conduct this step for it, similarly to the easy case. This p reserves
(ε,δp)-DP. From that moment on, the simulator does not need to acce ss the Helperanymore.
2. A “medium” answer is encountered, say when querying Leftj.In this case, the
executions with xrand with⊥diﬀer on the following points:
•The executions diﬀer in the inner state of Leftj. Speciﬁcally, let mdenote the number
of “middle” answers obtained from the current copy of Leftjbefore time r. After time
rthe internal state of Leftjdiﬀers between the two executions in that if b= 0 then
the copy of Stopper inside Leftjhas a dataset with mones, while if b= 1 then it has
m+1 ones. The execution of BetweenThresholds inside Leftjis not aﬀected. Thus, to
simulate this, the simulator Simasks Helperto instantiate a copy of algorithm Stopper
on a dataset containing m+bones. This satisﬁes ( ε,δp)-DP and allows Simto continue
the simulation of Leftjexactly.
•The executions also diﬀer in that xrwould be added to the dataset Dleft
jduring Step 5f
while⊥would not. In other words, from this moment until Dleft
jis used (in Step 5b) or
erased (in Step 5(j)iv), there are to options for this datase t: either with xror without
it. Hence, if Simneed to execute ChallengeBT on this dataset (in Step 5b) then instead
of doing it itself it ask Helperto do it. This also satisﬁes ( ε,δp)-DP. From that moment
on, the simulator does not need to access the Helperanymore.
Overall, we showed that Simcan perfectly simulate the view of the adversary Bwhile asking
Helpertoexecuteatmosttwoprotocols, eachsatisfying( ε,δp)-DP.Thisshowsthat RectanglesPERP
16is (ε,δ)-private (follows from parallel composition theorems). T he fact that/summationtext
iδ(i) =/summationtext
ptp·δp
converges to at most δ∗follows from our choices for δpandtp.
4.2.3 Utility analysis of RectanglesPERP
Fix a target distribution DoverRdand ﬁx a target rectangle c. Recall that cis deﬁned as the
intersection of dintervals, one on every axis, and denote these intervals as/braceleftBig
[cleft
j,cright
j]/bracerightBig
j∈[d]. That
is, for every x∈Rdwe have c(x) = 1 if and only if ∀j∈[d] we have cleft
j≤x[j]≤cright
j. We
writeRECTANGLE (c) ={x∈Rd:c(x) = 1}⊆Rdto denote the positive region of c. We assume for
simplicity that Pr x∼D[c(x) = 1]> α(note that if this is not the case then the all-zero hypothesi s
is a good solution).
Algorithm 6 Deﬁning sub-regions of RECTANGLE (c)
1. Denote R=RECTANGLE (c). Forp∈Nwe denote αp=α/2pandβp=β/2p, whereαandβ
are the utility parameters.
2. Forp= 1,2,3,...
(a) For axis j= 1,2,3,...,d:
•DeﬁneStripeleft
p,jto be the rectangular strip along the inside left side of Rwhich
encloses exactly weight αp/dunderD.
•SetR←R\Stripeleft
p,j
•DeﬁneStriperight
p,jto be the rectangular strip along the inside right side of Rwhich
encloses exactly weight αp/dunderD.
•SetR←R\Striperight
p,j
Deﬁnition 4.7. Forp∈Nandj∈[d]we deﬁne Stripeleft
p,jandStriperight
p,jaccording to Algo-
rithm 6.
Remark 4.8. In Algorithm 6 we assume that the stripes we deﬁne has weight exactlyαp/d. This
might not be possible (e.g., if Dhas atoms), but this technicality can be avoided using stand ard
techniques. For example, by replacing every sample x∈Rwith a pair (x,z)wherezis sampled
uniformly from [0,1]. We ignore this issue for simplicity.
Deﬁnition 4.9. LetR⊆Rdand letj∈[d]. For a point x∈Rwe write x∈jRif the projection
ofRon thejth axis contains x. For a vector x∈Rdwe write x∈jRifx[j]∈jR. For a set of
points (or vectors) Swe write S⊆jRif for every x∈Sit holds that x∈jR.
Deﬁnition 4.10. Given an (interactive) algorithm Awhose input is a dataset, we write data(A)
to denote the dataset on which Awas executed.
Deﬁnition 4.11. Forp∈N, letE(p)denote the following event:
1. All the geometric RV’s sampled during phase p(viaRCS) are at most1
εlog(1
δp)log(4d
βp).
172. All the Laplace RV’s sampled during phase p(viaChallengeBT ) are at most ∆pin absolute
value, where ∆p=4log(1/δp)
ε/radicalBig
kplog(2d
δp)log/parenleftBig
8dtp
βp/parenrightBig
.
Lemma 4.12. For every p∈Nwe have Pr[E(p)]≥1−βp.
Proof.In the beginning of phase pwe sample 2 dgeometric RV’s (via the RCSparadigm) with
parameter/parenleftbig
1−e−ε/log(1/δp)/parenrightbig
. By the properties of the geometric distribution, with prob ability at
least 1−βp
2, all of these samples are at most1
εlog(1
δp)log(4d
βp).
Throughout phase pthere are at most tptime steps. In every time steps we have (at most) 2
draws from the Laplace distribution in every copy of ChallengeBT , and there are 2d such copies.
By the properties of the Laplace distribution, with probabi lity at least 1−βp
2, all of these samples
are at most4log(1/δp)
ε/radicalBig
kplog(2d
δp)log/parenleftBig
8dtp
βp/parenrightBig
in absolute value.
Lemma 4.13. If the size of the initial labeled dataset satisﬁes n= Ω/parenleftBig
d
α·ε2polylog(d
α·β·γ·ε·δ∗)/parenrightBig
, then
for every p∈N, the following holds with probability at least/parenleftBig
1−2/summationtext
q∈[p]βq/parenrightBig
.
1. Event E(p′)holds for all p′≤p.
2. For every p′≤p, at any moment during phase p′and for every j∈[d]it holds that
data/parenleftbig
Leftj/parenrightbig
∈j/uniondisplay
q∈[p′]Stripeleft
q,j,anddata/parenleftbig
Rightj/parenrightbig
∈j/uniondisplay
q∈[p′]Striperight
q,j.
3. At the end of phase p, for every j∈[d]ands∈{left,right}the dataset Dcontains at least
2mp+1points from Stripes
p+1,jwith the label 1, and all positively labeled points in Dbelong
toRECTANGLE (C).
Proof.The proof of is by induction on p.4To this end, we assume that Items 1-3 hold for p−1,
and show that in this case they also hold for p, except with probability at most 2 βp. First, by
Lemma 4.12, Item 1 holds with probability at least 1 −βp. We proceed with the analysis assuming
that this is the case.
By assumption, at the end of phase p−1, the dataset Dcontains at least 2 mppoints from every
Stripes
p,j. As we assume that the noises throughout phase pare bounded (i.e., that Item 1 of the
lemma holds), and asserting that mp≥1
εlog(1
δp)log(4d
βp), we get that Item 2 of the lemma holds
in the beginning of phase p(in Step 5(j)iii, when the copies of ChallengeBT are executed by the
RSC paradigm). Throughout the phase, if we ever re-execute a ny of the copies of ChallengeBT
in Step 5b, say the copy Leftj, then this happens because Leftjhas produced≈kp“medium”
answers. By our assumption that the noise is bounded, and by a sserting that kp≥2∆p, we get
that there were at least kp/2 “medium” answers. (Here ∆ pbounds the Laplace noise throughout
the phase, see Deﬁnition 4.11.) Note that every query xthat generated such a “medium” answer
is added to Dleft
j. Again by our assumption that the noise is bounded, and by ass erting that
mp≥4∆p, every such query xsatisﬁesx∈jdata( Leftj). Hence, if we re-execute ChallengeBT in
Step 5b, then the projection of its new dataset on the jth axis is contained within the projection
4The base case of this induction (for p= 1) follows from similar arguments to those given for the ind uction step,
and is omitted for simplicity.
18of its previous dataset. Furthermore, by asserting that kp≥2mpwe get that the new dataset
contains at least mppoints. This implies (by induction) that Item 2 of the lemma c ontinues to
hold throughout all of the phase.
We now proceed with the analysis of Item 3. We have already est ablished that, conditioned on
Items 1,2,3 holding for p−1, then with probability at least 1 −βpwe have that Items 1,2 hold also
forp. Furthermore, this probability is over the sampling of the n oises throughout phase p. The
query points which are sampled throughout phase pfrom the target distribution Dare independent
of these events. Recall that there are tptime steps throughout phase p, and that roughly γportion
of them are sampled from D. Recall that each Stripes
p+1,jhas weight αp+1/dunderD. Thus, by
the Chernoﬀ bound (and a union bound over j,s), iftp≥8d
γαpln/parenleftBig
2d
βp/parenrightBig
, then with probability at
least 1−βp, throughout phase pwe receive at leastγ·αp·tp
2dpoints from every Stripes
p+1,j. By the
assumption that the noises are bounded (Item 1), each of thes e points is added to the dataset D
with the label 1. Thus, provided that tp≥4d
γ·αp·mp+1, then the dataset Dat the end of phase
pcontains at least 2 ·mp+1points from every Stripes
p+1,j. Note that this dataset might contain
additional points:
•Dmight contain additional points with label 0, but these do no t aﬀect the continuation of
the execution.
•Dmight contain additional points with label 1. However, agai n by the assumption that the
noises are bounded, we have that our labeling error is one sided , meaning that if a point is
included in Dwith the label 1 then its true label mustalso be 1.
Tosummarize, thedataset Dcontains at least 2 ·mp+1(distinct) points fromevery Stripes
p+1,jwith
the label 1, and all positively labeled points it contains be long toRECTANGLE (C). This completes
the proof.
Theorem 4.14. For every α,β,γAlgorithm RectanglesPERP is an(α,β,γ,n )-everlasting robust
predictor where n= Θ/parenleftBig
d
α·ε2polylog(d
α·β·γ·ε·δ∗)/parenrightBig
.
Proof.Attimeioftheexecution, considerthehypothesisdeﬁnedbySteps5c –5iof RectanglesPERP .
(This hypothesis takes an unlabeled point xiand returns a label ˆ yi.) We now claim that whenever
Items 1 and 2 deﬁned by Lemma 4.13 hold (which happens with pro bability at least 1 −β), then
all of these hypotheses has error at most αw.r.t. the target distribution Dand the target concept
c. To see this, observe that whenever Items 1 and 2 hold, then al gorithm RectanglesPERP never
errs on point outside of/uniondisplay
p∈N,j∈[d]
s∈{left,right}Stripes
p,j,
which has weight at most αunderD.
Remark 4.15. For constant d, the analysis outlined above holds even if the target concep tcis
chosen adaptively based on the initial sample S(and then Sis relabeled according to the chosen
conceptc). The only modiﬁcation is that in the base case of Lemma 4.13, w e cannot use the
Chernoﬀ bound to show that each Stripes
1,jcontains at least 2·m1points. Instead we would rely
on standard uniform convergence arguments for VC classes, s howing that w.h.p. over sampling the
unlabeled points in S, the probability mass of everypossible rectangle is close to its empirical weight
19inSup to a diﬀerence of O(α). This argument does not extend directly to super-constant va lues
ford, because in the analysis we argued about “stripes” with weig ht≈α/d, rather than α, but this
does not change much when d=O(1).
4.3 Decision-stumps
A decision stump could be thought of as a halfspace which is al igned with one of the principal
axes. Thus, decision-stumps in ddimensions could be privately predicted using our construc tion
for rectangles from Section 4.2. But this would be extremely wasteful as the VC dimension of
decision stump is known to be Θ(log d) instead of Θ( d). We brieﬂy describe a simple PERP for this
class which is computationally-eﬃcient and exhibits sampl e complexity linear in log( d). Formally,
Deﬁnition 4.16 (Decision-Stumps) .Letd∈Ndenote the dimension. For every j∈[d],σ∈{±1},
andt∈R, deﬁne the concept desision j,σ,t:Rd→{0,1}as follows. For x= (x1,...,x d)∈Rd
we have desision j,σ,t(x) = sign( σ·(xj−t)). Deﬁne the class of all decision-stumps over Rdas
DECISION d={desision j,s,t:j∈[d],σ∈{±1},t∈R}.
Algorithm 7 DecisionPERP
Initial input: Labeled dataset S∈(Rd×{0,1})nand parameters ε,δ,α,β,γ .
1. Use the exponential mechanism with privacy parameterε
4to select a pair ( j,σ)∈[d]×{±1}
for which there is a decision stump with low empirical error o nS.
2. Letpdenote the number of positively labeled points in S, and let ˆ p←p+Lap(4
ε).
3. LetDbe a dataset containing the projection of the points in Sonto the jth axis. Sort Din
an ascending order if σ= 1, and in descending order otherwise. Relabel the ﬁrst ˆ pexamples
inDas 1 and the other examples as 0.
4. Execute algorithm RectanglesPERP onD(as a multiset) for predicting thresholds in 1
dimension. Use parametersε
4,δ
2,α
2,β
2,γ. During the execution, pass only the jcoordinate
of the given queries to RectanglesPERP .
Theorem 4.17. Algorithm DecisionPERP is an(α,β,γ,ε,δ,n )-PERP for the class of d-dimensional
decision-stumps, where n= Θ/parenleftBig
1
αεlog(d)+1
α·ε2polylog(1
α·β·γ·δ)/parenrightBig
.
Proof.Theprivacyanalysisisstraightforward; followsfromcomp ositionandfromthefactthatonce
j,σ,ˆpare set, then changing one element of Saﬀects at most two elements in D. As for the utility
analysis, letDdenote the target distribution and let ( j∗,σ∗,t∗) denote the target concept. By the
properties of the exponential mechanism, with probability at least 1−β
6, the pair ( j,σ) selected in
Step 1 is such that there is a number twith which ( j,σ,t) misclassiﬁes at most O(1
εlogd
β) points
inS. Now let ( j,σ,ˆt) be a concept that agrees with the relabeled dataset D. By the properties
of the Laplace distribution, with probability at least 1 −β
6, these two concepts ( j,σ,t),(j,σ,ˆt)
disagree on at most O(1
εlogd
β) points in S. By the triangle inequality, we hence get that ( j,σ,ˆt)
disagrees with the target concept ( j∗,σ∗,t∗) on at most O(1
εlogd
β) points in S. Asserting that
|S|= Ω/parenleftBig
1
αεlogd
β/parenrightBig
, by standard generalization arguments, with probability a t least 1−β
6, the
concept ( j,σ,ˆt) has error at mostα
2w.r.t. the target distribution and the target concept. Fina lly,
20by the properties of RectanglesPERP , with probability at least (1 −β
2) it guaranteesα
2-accuracy
w.r.t. (j,σ,ˆt), even if (1−γ) fraction of the queries are adversarial. The theorem now fo llows by
the triangle inequality.
Acknowledgments
The author would like to thank Amos Beimel for helpful discus sions about Deﬁnitions 3.8 and 3.9.
References
Noga Alon, Mark Bun, Roi Livni, Maryanthe Malliaris, and Sha y Moran. Private and online
learnability are equivalent. J. ACM, 69(4):28:1–28:34, 2022.
Raef Bassily, Abhradeep Guha Thakurta, and Om Dipakbhai Tha kkar. Model-agnostic private
learning. In NeurIPS , 2018.
Mark Bun and Mark Zhandry. Order-revealing encryption and t he hardness of private learning. In
Theory of Cryptography - 13th International Conference, TCC 2016- A, Tel Aviv, Israel, January
10-13, 2016, Proceedings, Part I , pages 176–206, 2016.
Mark Bun, Kobbi Nissim, Uri Stemmer, and Salil P. Vadhan. Diﬀe rentially private release and
learning of threshold functions. In FOCS, pages 634–649, 2015.
Mark Bun, Thomas Steinke, and Jonathan Ullman. Make up your m ind: Theprice of online queries
in diﬀerential privacy. In Proceedings of the twenty-eighth annual ACM-SIAM symposium o n
discrete algorithms , pages 1306–1325. SIAM, 2017.
Edith Cohen and Xin Lyu. The target-charging technique for p rivacy accounting across interactive
computations. In NeurIPS , 2023.
Edith Cohen, Xin Lyu, Jelani Nelson, Tam´ as Sarl´ os, and Uri Stemmer. Optimal diﬀerentially
private learning of thresholds and quasi-concave optimiza tion. In STOC, pages 472–482. ACM,
2023.
Yuval Dagan and Vitaly Feldman. PAC learning with stable and private predictions. In COLT,
2020.
Cynthia Dwork and Vitaly Feldman. Privacy-preserving pred iction. In COLT, 2018.
Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith . Calibrating noise to sensitivity
in private data analysis. In TCC, pages 265–284, 2006.
Cynthia Dwork, Moni Naor, Omer Reingold, Guy N. Rothblum, an d Salil P. Vadhan. On the
complexity of diﬀerentially private data release: eﬃcient a lgorithms and hardness results. In
STOC, pages 381–390, 2009.
Shiva Prasad Kasiviswanathan, Homin K. Lee, Kobbi Nissim, S ofya Raskhodnikova, and Adam
Smith. What can we learn privately? SIAM J. Comput. , 40(3):793–826, 2011.
21Anupama Nandi and Raef Bassily. Privately answering classi ﬁcation queries in the agnostic PAC
model. In ALT, 2020.
Moni Naor, KobbiNissim, Uri Stemmer, andChaoYan. Private e verlasting prediction. In NeurIPS ,
2023.
Laurens van der Maaten and Awni Y. Hannun. The trade-oﬀs of pri vate prediction. CoRR,
abs/2007.05089, 2020.
22