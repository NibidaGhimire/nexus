arXiv:2305.06936v1  [cs.LG]  10 May 2023An Option-Dependent Analysis of Regret Minimization Algor ithms
in Finite-Horizon Semi-Markov Decision Processes
Gianluca Drappo1Alberto Maria Metelli1Marcello Restelli1
1DEIB, Politecnico di Milano, Milan, 20133, Italy.
Abstract
A large variety of real-world Reinforcement
Learning (RL) tasks is characterized by a complex
and heterogeneous structure that makes end-to-
end (or ﬂat) approaches hardly applicable or even
infeasible. Hierarchical Reinforcement Learning
(HRL) provides general solutions to address these
problems thanks to a convenient multi-level de-
composition of the tasks, making their solution
accessible. Although often used in practice, few
works provide theoretical guarantees to justify this
outcome effectively. Thus, it is not yet clear when
to prefer such approaches compared to standard
ﬂat ones. In this work, we provide an option-
dependent upper bound to the regret suffered by
regret minimization algorithms in ﬁnite-horizon
problems. We illustrate that the performance im-
provement derives from the planning horizon re-
duction induced by the temporal abstraction en-
forced by the hierarchical structure. Then, focus-
ing on a sub-setting of HRL approaches, the op-
tions framework, we highlight how the average
duration of the available options affects the plan-
ning horizon and, consequently, the regret itself.
Finally, we relax the assumption of having pre-
trained options to show how in particular situ-
ations, learning hierarchically from scratch could
be preferable to using a standard approach.
1 INTRODUCTION
Hierarchical Reinforcement Learning [HRL, Pateria et al.,
2021] is a learning paradigm that decomposes a long-
horizon Reinforcement Learning [RL, Sutton and Barto,
2018] task into a sequence of potentially shorter and sim-
pler sub-tasks. The sub-tasks themselves could be further
divided, generating a hierarchical structure organized in anarbitrary number of levels. Each of these deﬁnes a different
problem, where the original action space is replaced by the
set of sub-tasks available on the lower level, and the same
could be replicated for multiple levels. Although, the ac-
tual state transition is induced only once the control reach es
the leaf nodes, where the policies choose among the prim-
itive actions (i.e., actions of the original MDP on top of
which the hierarchy is constructed). For the higher levels,
once a sub-task is selected, the control passes to the relat-
ive internal policy until its termination. This introduces the
concept of temporal abstraction [Precup and Sutton, 1997],
for what concerns the high-level policy, the action persist s
for a certain time, resulting in an actual reduction of the
original planning horizon.
Several algorithms demonstrate outstanding performance
compared to standard RL approaches in several long-
horizon problems [Levy et al., 2019, Vezhnevets et al.,
2017, Bacon et al., 2017, Nachum et al., 2018]. However,
such evidence is mainly emerging in practical applications ,
and the theoretical understanding of the inherent reasons
for these advantages is still underdeveloped. Only a few pa-
pers tried to justify these advantages theoretically, focu s-
ing on different aspects. For instance, Mann et al. [2015]
studies the convergence of an algorithm that uses tempor-
ally extended actions instead of primitive ones. Fruit et al .
[2017] and the extension Fruit and Lazaric [2017] focus
on the exploration beneﬁt of using options in average re-
ward problems. More recently, Wen et al. [2020] show how
the MDP structure affects regret. In this paper, we seek to
further bridge this theory-practice gap, following the int u-
ition that a hierarchical structure in a ﬁnite-horizon prob -
lem would positively affect the sample complexity by redu-
cing the planning horizon. This could help to discriminate
among situations in which a hierarchical approach could be
more effective than a standard one for this particular famil y
of problems.
Contributions The contributions of the paper can be sum-
marized as follows. (1) We propose a new algorithm for
the ﬁnite-horizon setting that exploits a set of ﬁxed options[Sutton et al., 1999] to solve the HRL problem. (2) We
conducted a regret analysis of this algorithm, providing an
option-dependent upper bound, which, to the best of our
knowledge, is the ﬁrst in the Finite Horizon setting. This
result could be used to deﬁne new objective functions for
options discovery methods that would search for options
that minimize this regret. (3) For the sake of our analysis,
we formulate the notion of Finite Horizon SMDP and a per-
formance difference lemma for this setting. (4) Lastly, we
provide an algorithm to relax the assumption of having op-
tions with ﬁxed policies, and we demonstrate that there are
situations in which such an approach could provide better
guarantees in terms of sample complexity.
Outline In the following sections, we ﬁrst introduce the
problem and the notion used. Then, Section 3 describes
the main motivation behind this work and the focus on
the ﬁnite-horizon settings, and Section 4 describes the new
formalism introduced. The algorithm and its extension are
described in Section 5, and the main result and its deriva-
tion are discussed in Sections 6 and 7. Finally, we describe
in detail the related works (Section 8) and discuss some
further directions of research beyond the present work.
2 PRELIMINARIES
In this section, we provide the necessary background that
will be employed in the remainder of the paper.
Finite-Horizon MDPs A Finite Horizon Markov De-
cision Process [MDP, Puterman, 2014] is deﬁned as a tuple
M= (S,A,p,r,H), whereSandAare the ﬁnite state
and the primitive action spaces, respectively, p(s′|s,a,h)
is the transition probability function deﬁning the probabi l-
ity of transitioning to state s′∈ S by taking action a∈ A
in states∈ S at stageh∈[H].r(s,a,h)is the reward
function that evaluates the quality of action a∈ A when
taken in state s∈ Sat stageh∈[H], andHis the horizon,
which deﬁnes the duration of each episode of interaction
with the environment. The behavior of an agent is modeled
by a deterministic policy π:S×[H]→ A that maps states
s∈ S and stages h∈[H]to actions.
Semi-MDP A Semi-Markov Decision Process [SMDP,
Baykal-Gürsoy, 2010, Cinlar, 2013] is a generalization of
the MDP formalism. It admits temporally extended actions,
which, contrary to primitive ones (i.e., actions that execute
for a single time step), can execute for a certain time during
which the agent has no control over the decision process. A
usual notion when treating SMDP is the duration orhold-
ing time ,τ(s,a,h), which is the number of primitive steps
taken inside a temporally extended action.
HRL builds upon the theory of Semi-MDPs, characterizing
the concept of temporally extended action with basically
two main formalisms [Pateria et al., 2021]: sub-tasks [Diet -terich, 2000] and options [Sutton et al., 1999]. For the sake
of this paper, we focus on the options framework.
Options An option [Sutton et al., 1999] is a possible form-
alization of a temporally extended action. It is character-
ized by three components o= (Io,βo,πo).Io⊆ S ×[H]
is the subset of states and stages pairs (s,h)in which the
option can start, βo:S ×[H]→[0,1]deﬁnes the probab-
ility that an option terminates in a speciﬁc state-stage pai r,
πo:S ×[H]→ A is the policy executed until its termina-
tion. An example of an option could be a pre-trained policy
to execute a speciﬁc task in a control problem, such as pick-
ing up an object.
Exactly as stated by Sutton et al. [1999, Theorem 1] an
MDP in which primitive actions Aare replaced by options
O, becomes an SMDP . In this paper, we consider a hier-
archical approach working for a two-level hierarchy. On
the top, the goal is to ﬁnd the optimal policy µ:S → O ,
which determines the optimal option for each state-instant
pair. Once an option is selected, out of the SMDP’s scope,
its policy is executed until its termination, and the contro l
returns to the high-level policy. An assumption is needed
on the set of the given options [Fruit et al., 2017].
Assumption 2.1 (Admissible options) .The set of options
Ois assumed admissible, i.e., all options terminate in ﬁnite
time with probability 1 and, {∀o∈ O, s∈ S,andh∈
[H],∃o′∈ O:βo(s,h)>0and(s,h)∈ Io′}.
Lastly, we introduce an essential quantity for our analysis .
Regret Theregret is a performance metric for algorithms
frequently used in provably efﬁcient RL. For any starting
states∈ S, and up to the episode K, it is deﬁned as:
Regret(K)def=K/summationdisplay
k=1V∗(s,1)−Vµk(s,1) (1)
and evaluates the performance of the policy learned until
episodek,Vµkcompared to the value of an optimal policy
V∗.
2.1 NOTATION
In the following, we will use ˜O(·)to indicate quantities that
depend on (·)up to logarithmic terms. /BD(x=a)deﬁnes the
indicator function/BD(x=a)def=/braceleftBigg
0,ifx∝ne}ationslash=a
1,ifx=a
In the analysis, we denote optimistic terms with ∼and em-
pirical ones with ∧, e.g.,˜pandˆrare, respectively, the op-
timistic transition model and the estimated reward functio n.3 MOTIV ATION AND INTUITION
Usually, in Reinforcement Learning, the complexity of a
problem is highly correlated to the planning horizon, which
is even more natural in ﬁnite-horizon MDPs. The regret
analysis in the literature provides results for both the low er
and upper bound on the regret paid by an algorithm in this
setting, where there is a dependency on H.
Here comes our intuition, by using a hierarchical approach,
we can intrinsically reduce the planning horizon because
the number of decisions taken in Htime steps is scaled by
a term closely related to the average duration of each action ,
and thus, also the complexity scales with this quantity. In
addition, if the sub-tasks themselves need to be learned be-
cause policies are not provided, simpliﬁcation can also be
induced in these new problems. Under certain assumptions,
they could have shorter horizons, and an agent during the
training can focus just on smaller regions of the entire stat e
space. Furthermore, the learning could be further guided by
an additional reward that could better specify the singular
sub-problem.
Starting from this intuition, we analyze the performance of
an algorithm in a Finite Horizon Semi-MDP, considering
a set of pre-trained options and, afterward, its extension,
which incorporates a ﬁrst phase of options learning. Lastly ,
we provide a comparative study with its ﬂat counterpart.
4 FINITE HORIZON SMDP
In this section, we present a new formalism, Finite-Horizon
Semi-Markov Decision Processes , that combines the notion
used in FH-MDP with the concept of temporal abstraction.
A ﬁnite-horizon semi-MDP is deﬁned as a tuple SM=
(S,O,p,r,H), where SandOare the ﬁnite state
and the temporally extended action spaces, respectively,
p(s′,h′|s,o,h)is the probability of ending to state s′∈ S,
after(h′−h)steps, by playing the temporally extended ac-
tiono∈ O in states∈ S at stageh∈[H]. On the other
hand,r(s,o,h)is the expected reward accumulated until
the termination of the temporally extended action o∈ O
played in state s∈ S at stageh∈[H]of the episode.
Finally,His the horizon of interaction, and still τ(s,o,h)
is the number of primitive steps taken inside the tempor-
ally extended action. The agent’s behavior is modeled by
a deterministic policy µ:S ×[H]→ O mapping a state
s∈ S and a stage h∈[H]to a temporally extended ac-
tion. The goal of the agent is to ﬁnd a policy µ∗that max-
imizes the value function, deﬁned as the expected sum of
the rewards collected over the horizon of interaction and
recursively deﬁned as:
Vµ(s,h) = E
(s′,h′)∼p(·|s,µ(s,h),h)/bracketleftBig
r(s,µ(s,h),h)+Vµ(s′,h′)/bracketrightBig
,
with the convention that Vµ(s,H) = 0 . The value functionof any optimal policy is denoted by V∗(s,h):=Vµ∗(s,h).
5 ALGORITHMS
FH-SMDP-UCRLis a variant of the algorithm presented
in Fruit and Lazaric [2017], which in turn is inspired by
UCRL2 [Auer et al., 2008] and adapted for FH-SMDPs.
This family of algorithms implements the principle of " op-
timism in the face of uncertainty ", which states that when
interacting in unknown environments—with an unknown
model—the decisions have to be guided by a trade-off
between what we believe is the best option and by a term
representing the uncertainty on our estimates. More form-
ally, it is introduced the so-called exploration bonus , which
quantiﬁes the uncertainty level on our estimations of the
model, computed from the observed samples. This explor-
ation bonus is used to regulate the exploration-exploitation
dilemma, inducing the algorithm to explore regions of the
space with high uncertainty instead of sticking to what
seems to be the optimal solution and to overcome situations
in which the optimal solution resides in a region not yet dis-
covered.
However, a direct application of these algorithms in our set -
ting is unfeasible, as they are designed for inﬁnite-horizo n
average reward settings. Due to the lack of methods operat-
ing in these settings, we need to design a new algorithm for
ﬁnite-horizon SMDPs following the same paradigm.
As displayed by Algorithm 1, at each episode k, we com-
pute an estimate of the SMDP model, by computing, from
the collected samples up to episode k, the empirical trans-
ition probability ˆpkand the reward function ˆrk.
ˆpk(s′,h′|s,o,h) =/summationtextk−1
i=1
/BD((s,o,s′,h,h′)i= (s,o,h,s′,h′))
nk(s,o,h)(2)
ˆrk(s,o,h) =/summationtextk−1
i=1ri(s,o,h)
nk(s,o,h)(3)
We then redeﬁne the conﬁdence intervals of these two
quantities, βp
kandβr
k, respectively as
βr
k(s,o,h)∝/radicalBigg
2ˆVar(r)ln2/δ
nk(s,o,h)+7ln2/δ
3(nk(s,o,h−1),
(4)
βp
k(s,o,h)∝/radicalBigg
Slog/parenleftbignk(s,o,h)
δ/parenrightbig
nk(s,o,h), (5)
whereˆVar(r)is the sample variance of the reward func-
tion. From the estimates and the conﬁdence intervals just
deﬁned, we can build the conﬁdence sets Bp
kandBr
k, which
contain, with high probability, the true model. Being SMk
the set of plausible SMDPs, characterized by rewards andtransition within the conﬁdence sets, with SMkand an ad-
aptation of extended value iteration [Auer et al., 2008], for
FH-SMDP (Algorithm 2), we can compute the optimistic
policy˜µkand the relative optimistic value function ˜Vµk.
Then, by playing this policy for an entire episode, we col-
lect new samples and restart the process for the next epis-
odek+1.
5.1 OPTION LEARNING
By relaxing the assumption of having a set of pre-trained
options, considering known just their initial state set and
termination conditions, we can enhance characteristics of
problems more suited to be solved with a hierarchical ap-
proach even when no pre-trained policies are provided.
We present a model-based algorithm divided into two
phases, which initially learns each option policy individu -
ally, and then exploits them to solve the SMDP with
FH-SMDP-UCRL. As Algorithm 3 shows, each option
is considered as a single FH-MDPs, deﬁned based on
its initial-state set and termination probability, as Mo=
(So,Ao,p,ro,Ho)whereSo⊆S,Ao⊆A,Ho≤H,
which means that each option operates on a restricted por-
tion of the original problem, for a certain ﬁxed horizon Ho.
The option’s optimal policy is the policy of the relative sub -
FH-MDP computed until episode Ko, which is the number
of episodes assigned to each option.
Nevertheless, if no assumption on the reward function is
deﬁned, the options’ optimal policies could be sub-optimal
regarding the optimal policy computed by a standard ap-
proach for that portion of the MDP, being the option’s scope
limited to a certain part of the MDP with the impossibility
of having feedback on what happens after its termination.
Therefore we need to state:
Assumption 5.1. Given an MDP M= (S,A,p,r,H)and
a set of options o∈ O. Deﬁneπ∗
oas the optimal policy of
the option olearned individually on the sub-MDP Mo=
(So,Ao,p,ro,Ho)withSo⊆S,Ao⊆A, andHo≤H.
The reward function roof the sub-MDP Mo, which could
differ from r, ensure that
π∗(s) =π∗
o(s)∀s∈So
withπ∗(s)the optimal policy on M.
This assumption guarantees that the computed option’s op-
timal policy equals the optimal policy of the entire problem
in the option’s region.
6 MAIN RESULTS
In this section, we present the main contributions of the pa-
per, which in particular are an upper bound on the regretAlgorithm 1 UCRL-FH-SMDP
Require: S,Owith ﬁxed policies, H
Initialize µ0at random and Q1(s,o,h) = 0 for all
(s,o,h)∈ S ×O× [H]
1:Execute µ0forH steps and collect tuples
(s,o,h,s′,h′)andr(s,o,h)to store in D1
2:fork= 1,...,K do
3: Compute nk(s,o,h)
4: Estimate empirical SMDP /hatwidestSMk= (S,O,ˆpk,ˆrk)
with equations 2, 3.
5: Compute the conﬁdence sets Br
k(s,o,h)and
Bp
k(s,o,h)using the conﬁdence interval (Eq. 4, 5)
6: Planning with Backward Induction for µk, using an
adaptation to ﬁnite horizon of Extended Value Itera-
tion[Auer et al., 2008] (Algorithm 2)
7: forh= 1,...,H do
8: Executeo=µk(s,h)until it terminates
9: Observe(s′,h′)andr(s,o,h)
10: Add the tuple (s,o,h,s′,h′)kandrk(s,o,h)to
Dk+1
11: Seth=h′
12: end for
13:end for
of FH-SMDP-UCRL that highlights particular problem-
dependent features and, an upper bound on the regret of
its extension including a ﬁrst phase of options learning.
Theorem 6.1. Considering a non-stationary Finite Hori-
zon SMDP SM and a set of options O, with bounded
primitive reward r(s,a)∈[0,1]. The regret suffered by al-
gorithm FH-SMDP-UCRL, in Kepisodes of horizon His
bounded as:
Regret(K) ≤˜O/parenleftBigg/parenleftBig√
SOKd2/parenrightBig/parenleftbigg
T+√
SH/parenrightbigg/parenrightBigg
with probability 1−δ.
Where:
T= max
s,o,h/radicalbig
E[τ(o,s,h)2]
= max
s,o,h/radicalbig
E[τ(o,s,h)]2+Var[τ(s,o,h)],
τis the holding time, and ddescribes the expected number
of decisions taken in one episode that is d≈H/¯τ, with¯τ
the average duration of the set of options.
This result introduces one of the main contributions, an
option-dependent upper bound on the regret in FH-MDP
with options, not worst-case as in Fruit and Lazaric [2017].
A dependency on the properties of an option set is intro-
duced, embodied into both Tandd. The former gives the
same interesting consideration already underlined by Frui tAlgorithm 2 Extended Value Iteration for FH-SMDP
1:Input:S,O,Br
k,Bp
k
2:SetQH+1(s,o) = 0 for all(s,o)∈ S ×O
3:forh=H...1do
4: for(s,o)∈ S ×O do
5: forh′=h+1...H+1do
6: Compute
Qhk(s,o) = max
r∈Br
k(s,o,h)r(s,o,h)+ max
p∈Bp
k(s,o,h)E
s′,h′∼p(·,·|s,o,h)[Vh′,k(s′)]
Vh′k(s) = min{H−(h′−1),max
o∈OQh′k(s,o)}
7: end for
8: end for
9:end for
10:Output:µk(s,h) = argmaxo∈OQhk(s,o)
Algorithm 3 Option Learning
1:Input:So,Ao,Ho,Ko, andBr
k(s,a),Bp
k(s,a)which
are respectively the conﬁdence sets of the ﬂat model.
2:SetQHo+1(s,a) = 0 for all(s,a)∈ S≀×A≀
3:fork= 1,...,K odo
4: Compute nk(s,a,h)
5: Estimate empirical MDP /hatwiderMk= (So,Ao,ˆpk,ˆrk),
with an adaptation of eq. 2 and 3 for the ﬂat model.
6: Planning by backward induction for πohkwith Ex-
tended Value Iteration for FH-MDPs [Ghavamzadeh
et al., 2020], in the horizon Ho.
7: Playπokfor an episode to collects new samples.
8:end for
9:Output:πKoo.
and Lazaric [2017], whereby the extra cost of having ac-
tions with random duration is only partially additive rathe r
than multiplicative. On the other hand, the latter emphas-
izes the real beneﬁt of using a hierarchical approach over
a ﬂat one. The longer the expected duration of the set of
options, the more the effective horizon of the SMDP, d,
decreases. Indeed, d≈H
¯τ, with¯τthe average holding
time of the options. Notice that there is a√
dworsening
factor, which comes from the fact that we consider a non-
stationary MDP in the analysis. This outcome is common
in ﬁnite-horizon literature [Azar et al., 2017, Dann et al.,
2017, Zanette and Brunskill, 2018], where, instead, the re-
gret increases by a factor of√
H.
Let’s now analyze the regret suffered by the two-phase al-
gorithm that ﬁrst learns each option policy and then sub-
sequently solves the relative SMDP.
Theorem 6.2. The regret paid by the two-phase learning
algorithm until the episode K is:
Regret(K)≤˜O/parenleftBig
K2
33/radicalbig
H5oS2oAoO+H2S
Ho√
OK/parenrightBigwithHothe ﬁxed horizon of each option o∈ O,So, andAo
the upper bounds on the cardinality of the state and action
space of the sub-FH-MDPs.
As mentioned before, we consider a situation where we al-
locateKoepisodes for each option learning, hence T1=/summationtext
o∈OKoHo, andK2episodes for the SMDP policy learn-
ing at ﬁxed options policies. Being the algorithm divided
into two phases, we expected a regret that scales with K2/3,
a study of a more efﬁcient algorithm is left for future works.
We now would like to understand if there are any situ-
ations in which such an approach could produce more be-
neﬁt compared to a standard one, to examine if there are
any classes of problems in which learning using a hierarch-
ical approach almost from scratch should be preferred to
a standard one. Therefore we conduct a comparison of re-
grets paid by this algorithm and a standard ﬂat-one, UCRL2
in particular.
We recall that the regret of UCRL2 adapted for non-
stationary FH-MDPs [Ghavamzadeh et al., 2020] is
Regret(UCRL2−CH)≤˜O(H2S√
AK), thus the ratio
between the two regret bounds, R, is
R=Regret SMDP
Regret MDP≤K2/3(H5
oS2
oAoO)1/3
H2S√
AK
By considering particular relations between the option-
MDP and the original one, where Ao=αA,So=αS
andHo=αH, we can rewrite this ratio as:
R ≤K1/6α3/8O1/3
(HS)1/3A1/6(6)
WhenR ≤1is clearly beneﬁcial to use this approach in-
stead of a standard one. Thus, by imposing this assumption,
we can ﬁnd the maximum number of episodes for which
this constraint is satisﬁed.
K≤H2S2A
α16O2
Remark While in general, a comparison of upper bounds
is potentially loose, in this case, the two upper bounds have
been derived with similar techniques; hence they would be
similarly loose .
Final Sub-Optimality Remark. In both results, we need
to consider that, by using a deﬁned option set, we are intro-
ducing a bias. It could be that the optimal policy on the
ﬂat problem is irreproducible by a concatenation of the
policies of the options chosen by the optimal high-level
policyµ∗. This is because the structure introduced by the
options can cause some states to become inaccessible for
the high-level SMDP. This issue is also treated by Fruit and
Lazaric [2017] and produces an additional term on the re-
gret equal to V∗(M)−V∗(MO), whereMis the primitive
MDP, and MOthe same MDP with options.6.1 RENEWAL PROCESS
The expected number of options played in one episode d,
clearly depends on the random duration of each of these
options; hence it is itself a random variable, and we would
like to bound it with some quantity. Resorting to the Re-
newal Theory [Smith, 1958], this corresponds to the Re-
newal Function m(t).
Deﬁnition 6.3 (Renewal Process) .LetS1,S2...be a se-
quence of i.i.d. random variables with ﬁnite and non-zero
mean, representing the random time elapsed between two
consecutive events, deﬁned as the holding time. For each
n >0we deﬁne Jn=/summationtextn
i=1Si,as the time at which the
nthevent of the sequence terminates. Then, the sequence
of random variables Xt, characterized as
Xt=∞/summationdisplay
nI{Jn≤t}= sup{n:Jn≤t} (7)
constitutes a Renewal Process (Xt)t≥0, representing the
number of consecutive events that occurred up to time t.
Deﬁnition 6.4 (Renewal Function) .Considering a renewal
process(Xt)t≥0, the renewal function m(t)is the expected
number of consecutive events that occurred by time t.
m(t) =E[Xt]
Hence, it is possible to take inspiration from a bound of the
renewal function to bound the expected number of options
played in one episode.
Lemma 6.5. [Bound on number of options played in one
episode] Considering a Finite Horizon SMDP SM with
horizon H and, Ooptions with duration τmin≤τ≤τmax
andmino(E[τo])the expected duration of the shorter op-
tion. The expected number of options played in one episode
dcan be seen as the renewal function m(t)of a renewal
process up to the instant H. With probability 1−δthis
quantity is bounded by
d </radicalBigg
32(τmax−τmin)H(ln2−lnδ)
(mino(E[τo]))3+H
mino(E[τo])
Refer to the appendix for detailed proof of this result.
6.2 FIXED OPTION LENGTH
Let’s now analyze a particular case to clarify the claim in-
troduced with Theorem 6.1. A scenario in which the given
options are deterministic with ﬁxed length.
Corollary 6.6. Considering a non-stationary Finite Hori-
zon SMDP SM and a set of deterministic option Owithﬁxed duration ¯τ, the regret payed by FH-SMDP-UCRL,
after K steps is upper bounded by:
Regret(K) ≤˜O/parenleftBigg
H
¯τ/parenleftBig√
SOK/parenrightBig/parenleftbigg
¯τ+√
SH/parenrightbigg/parenrightBigg
Proof. The result is trivially derived by substituting dwith
the actual number of decisions taken in one episode, which
now is a deﬁned number equal to H/¯τ. Then, the same ap-
plied forTthat, considering options of length ¯τ, is exactly
¯τ.
This bound clearly shows a dependency on the choice of the
options set. The second term, which is the dominant one, is
mitigated by the√¯τ, thus reducing the sample complexity
as expected. The other/radicalBig
H
¯τ, as for Theorem 6.1, comes
from the non-stationarity of the SMDP.
6.3 DERIV ATION OF FH-MDP AND
PARALLELISM WITH AR-SMDP.
To further strengthen the obtained result, we can show
that considering some assumptions, we can derive the up-
per bound by Auer et al. [2008] adapted to FH-MDPs
[Ghavamzadeh et al., 2020].
Finite Horizon MDP. Referring to the result provided
by Auer et al. [2008] adapted to the ﬁnite horizon case
[Ghavamzadeh et al., 2020], the regret in Finite Horizon
MDPs scales with ˜O(HS√
AT), or with ˜O(H3
2S√
AT)
when the MDP in non-stationary. If, in our upper bound,
we substitute the cardinality of the option set O, with the
primitive-action space A. This leads to having T= 1 and
d=Hbecause the primitive actions, by deﬁnition, termin-
ate after a single time step. Thus, the average duration of
these single-step options is 1, and the number of decisions
taken in one episode is exactly H. Then, having bounded
primitive reward r(s,a)∈[0,1], we can write our res-
ult as˜O(H3
2S√
AKH)and considering the deﬁnition of
T=KH [Dann et al., 2017, Azar et al., 2017, Zanette and
Brunskill, 2018], we obtain the same equation.
Remark We are aware of the tighter upper bounds in
the Finite Horizon literature by Azar et al. [2017], that
get rid of a√
HS, by tightly bounding the estimation er-
ror(˜p−p)˜Vµkand their exploration bonus in terms of the
variance of V∗at the next state, and by using empirical
Bernstein and Freedman’s inequalities [Maurer and Pontil,
2009, Freedman, 1975]. However, with this work, our main
focus is to emphasize the role played by the options set’s
composition instead of providing a very tight analysis. We
still think the same tricks could be used in our analysis to
tighten the bound, but we leave that for future work.
Parallelism with Average Reward Setting. Fruit and
Lazaric [2017] showed that the regret in SMDPs withoptions when considering bounded holding times, and
Rmax= 1 scales with ˜O(DOSO√
On+Tmax√SOOn).
On the other hand, considering the same assumptions, our
result becomes of order ˜O(HS√
OKd2+Tmax√
SOKd2).
It is clearly impossible to derive one setting from the other .
Nevertheless, we can spot some similarities between the
two bounds. We can say that for ﬁnite-horizon problems,
the diameter Dcoincides with the horizon H[Ghavamz-
adeh et al., 2020]. Besides, Kdis exactly equal to n, the
number of decisions made up to episode K. The state space
Sis the state space of the SMDP in our formalism, which is
the deﬁnition provided for SOin Fruit and Lazaric [2017].
Consider, again, that the additional√
dcomes from the fact
that we refer to a non-stationary FH-SMDP.
Thus, we prove that our result is a generalization of the case
of FH-MDP and closely relates to the result presented for
the Average Reward Setting.
7 PROOFS SKETCH
In this section, we provide the sketch proofs of theorem 6.1
and theorem 6.2. Please refer to the appendix for all the
details.
7.1 SKETCH PROOF OF THEOREM 6.1
We deﬁned the regret in ﬁnite horizon problems as in eq.
1. Optimistic algorithms work by ﬁnding an optimistic es-
timation of the model of the environment to compute the
optimistic value function and the optimistic policy. Consi d-
ering how the conﬁdence sets are constructed, we can state
that˜p≥pand˜r≥r, where terms without tilde are the real
one, hence, V∗(s,h)≤˜Vµk(s,h)for allh. Thus, we can
bound eq. 1 with
Regret(K)opt
≤K/summationdisplay
k=1˜Vµk(s,1)−Vµk(s,1) (8)
Let’s now introduce a Performance Difference Lemma for
FH-SMDPs.
Lemma 7.1. [Performance Difference Lemma for FH-
SMDP] Given two FH-SMDPs ˆMand˜Mwith horizon H,
and respectively rewards ˆr,˜rand transition probabilities ˆp,
˜p. The difference in the performance of a policy µkis:
˜Vµk(s,1)−ˆVµk(s,1)
=ˆE/bracketleftbiggH/summationdisplay
i=1/parenleftBig/parenleftbig
˜r(si,oi,hi)−ˆr(si,oi,hi)/parenrightbig
+/parenleftbig
˜p(si+1,hi+1|si,oi,hi)−ˆp(si+1,hi+1|si,oi,hi)/parenrightbig
˜Vµk(si+1,hi+1)/parenrightBig/BD{hi< H}/bracketrightbigg
whereˆEis the expectation taken w.r.t. ˆpandµk.Note that the summation steps are not unitary but skip ac-
cording to the length of the transitions h′−h. The deriva-
tion of this lemma follows the one provided by Dann et al.
[2017] for FH-MDPS that is commonly used in literature
[Azar et al., 2017, Zanette and Brunskill, 2018]. Check the
appendix for further details.
Now we can use lemma 7.1 to substitute the difference of
value function in eq. 8 and we can upper bound both the
difference of randp, with 2 times their conﬁdence intervals
and the optimistic value ˜Vµk(si+1,hi+1)with the horizon
H- we consider bounded primitive reward r(s,a)∈[0,1].
Regret(K) ≤K/summationdisplay
k=1E/bracketleftbiggH/summationdisplay
i=1/parenleftBig
2βr
k+2βp
kH/parenrightBig/BD{hi< H}/bracketrightbigg
In the Finite-Horizon literature[Dann et al., 2017, Zanett e
and Brunskill, 2018], two terms are commonly used in the
proofs: (1) wk(s,o,h)that is the probability of taking the
optiono, in state sat time step h, which clearly depends
on the policy µkand the transition probability of the real
SMDP, (2) Lk, which deﬁnes the set of episodes visited
sufﬁciently often, and the set of (s,o,h)that were not vis-
ited often enough to cause high regret. Therefore, for using
the same approach to conduct the proof, we can substitute
the expectation E, which is taken w.r.t. the policy µkand
the real transition probability p(s′,h′|s,o,h), with
/summationdisplay
(s,o,h)∈Lkwk(s,o,h)
We deﬁned the conﬁdence intervals of randp, as in the
equations 4, 5, respectively using Empirical Bernstein In-
equality [Maurer and Pontil, 2009], Hoeffding [1963] and
Weissman et al. [2003].
By substituting these deﬁnitions and the term just intro-
duced, we get, up to numerical constants, that the regret
is bounded by
/summationdisplay
k/summationdisplay
i∈[H]/summationdisplay
(s,o,h)∈Lkwk(si,oi,hi)/radicalbig
nk(si,oi,hi)/parenleftbigg/radicalBig
ˆVar(r)+√
SH+1/radicalbig
nk(s,o,h)/parenrightbigg
Lemma 7.2. Considering a non-stationary MDP M with
a set of options as an SMDP MO[Sutton et al., 1999]. In
MOthe number of decisions taken in the kth-episode is a
random variable dand
/summationdisplay
i∈H/summationdisplay
(s,o)∈Lkwk(si,oi,hi) /BD{hi< H}=dwith{∀k:d≤H}
Therefore, the following holds true:
/summationdisplay
k/summationdisplay
i∈H/summationdisplay
(s,o)∈Lkwk(si,oi,hi)/radicalBigg
1
nk(si,oi,hi)=˜O/parenleftbigg√
SOKd2/parenrightbigg
or, using the same notation used in Fruit and Lazaric
[2017],˜O(√
SOKd2), withn=Kdthe number of de-
cisions taken up to episode K.Substituting the result of Lemma 7.2 in the equation of the
regret, we get
Regret(K) ≤˜O/parenleftBigg/parenleftBig√
dSOn/parenrightBig/parenleftbigg/radicalBig
ˆVar(r)+√
SH/parenrightbigg
+dSO/parenrightBigg
where as mentioned above, dis the expected number of de-
cision steps taken in one episode, nis the total number of
decisions taken up to episode k, andˆVar(r)is the empir-
ical variance of the reward that emerged from the use of
Empirical Bernstein inequality. A dependency on the vari-
ance of the reward is not that explainable for what we want
to show; hence we upper bound this term by the square root
of the empirical variance Tof the duration of the options
seen up to episode k, and this complete the proof.
7.2 SKETCH PROOF OF THEOREM 6.2
In order to prove the regret paid by the two-phase algorithm,
we ﬁrst consider that we can write the regret as the sum
of the regret paid in the ﬁrst phase and the regret paid in
the second one, plus an additional bias term. In the ﬁrst
phase, we pay full regret for each option learning, then the
maximum average regret considering the option learning as
a ﬁnite horizon MDP with horizon Ho, and the regret of the
SMDP learning with ﬁxed options
Regret(K)≤/summationdisplay
o∈OKoHo+K2max
o∈O1
KoH2
oSo/radicalbig
AoKo+HS/radicalbig
Od2K2(9)
WhereK2are the episodes used for the SMDP learning,
andK=/summationtext
o∈OKo+K2. Then considering that we alloc-
ateKoepisodes for each option learning, and AoandSo
are respectively the upper bounds on the action-space car-
dinality and state-space cardinality of the options set, we
can get rid of the maxo∈O. Now bounding K2≤K, we
can ﬁnd the optimal Koin closed form, and substituting it
in Equation 9, we conclude the proof.
8 RELATED WORKS
In the FH-MDP literature, several works provide an ana-
lysis of the regret of different algorithms. Osband and
Van Roy [2016] present a lower bound that scales with
Ω(√
HSAT). On the other hand, many other works pro-
pose various upper bounds for their algorithm. The most
common upper bound is the adaptation of Auer et al. [2008]
proposed by Ghavamzadeh et al. [2020], which is of the or-
der ofO(HS√
AT). This result has then been improved
in the following papers. An example is Azar et al. [2017],
which proposes a method with an upper bound on the regret
ofO(√
HSAT)that successfully matches the lower bound.
As mentioned above, both upper and lower bounds depend
onH.
Nevertheless, few works focused on theoretically under-
standing the beneﬁts of hierarchical reinforcement learni ngapproaches, and, to the best of our knowledge, this is the
ﬁrst to analyze these aspects in FH-SMDPs. To conduct
our analysis, we take inspiration from the paper by Fruit
and Lazaric [2017], in which they propose an adaptation of
UCRL2 [Auer et al., 2008] for SMDPs. They ﬁrst study
the regret of the algorithm for general SMDPs and then
focus on the case of MDP with options, providing both
a lower bound and a worst-case upper bound. This work
was the ﬁrst that theoretically compares the use of options
instead of primitive actions to learn in SMDPs. Nonethe-
less, it focuses on the average reward setting to study how
it is possible to induce a more efﬁcient exploration by us-
ing options, and it assumes ﬁxed options. Differently, we
aim to analyze the advantages of using options to reduce
the sample complexity of the problem, resorting to the in-
tuition that temporally extended actions can intrinsicall y
reduce the planning horizon in FH-SMDPs. Furthermore,
we provide an option-dependent upper bound, instead of
a worst-case one, that better quantiﬁes the impact of the
option duration on the regret. Other works providing a the-
oretical analysis of hierarchical reinforcement learning ap-
proaches are Fruit et al. [2017], which is an extension of
the aforementioned work in which the need for prior know-
ledge of the distribution of cumulative reward and duration
of each option is relaxed. Even in this case, they consider
the average reward setting, and the objective is identical.
Then, Mann et al. [2015] study the convergence property
of Fitted Value Iteration (FVI) using temporally extended
actions, showing that a longer duration of options and pess-
imistic estimates of the value function lead to faster conve r-
gence. Finally, Wen et al. [2020] demonstrate how patterns
and substructures in the MDP provide beneﬁts in terms
of planning speed and statistical efﬁciency. They present
a Bayesian approach exploiting this information, and they
analyze how sub-structure similarities and sub-problems’
complexity contribute to the regret of their algorithm.
9 CONCLUSIONS
In conclusion, we propose a new algorithm for Finite Ho-
rizon Semi Markov decision processes called FH-SMDP-
UCRL, and we provide theoretical evidence that supports
our original claim. Using hierarchical reinforcement lear n-
ing, it is provably possible to reduce the problem complex-
ity of a Finite Horizon problem when using a well-deﬁned
set of options. This analysis is the ﬁrst for FH-SMDP and
provides a form of option-dependent analysis for the regret
that could be used to deﬁne objectives for options discovery
methods better. Furthermore, by relaxing the assumption
of having a set of ﬁxed options’ policies, we were able to
provide insights on classes of problems in which a hierarch-
ical approach from scratch would still be beneﬁcial com-
pared to a ﬂat one. In the future, we would like to improve
the algorithm proposed for options learning to tighten thetheoretical guarantees and further characterize this fami ly
of problems. Finally, we would like to investigate, follow-
ing the ideas of Wen et al. [2020], how the structure of the
MDP could appear in our bound, which, in our opinion, is
a fundamental point to put another brick in the direction of
total understanding on the promising power of HRL.
References
Peter Auer, Thomas Jaksch, and Ronald Ortner. Near-
optimal regret bounds for reinforcement learning. Ad-
vances in neural information processing systems , 21,
2008.
Mohammad Gheshlaghi Azar, Ian Osband, and Rémi
Munos. Minimax regret bounds for reinforcement learn-
ing. In International Conference on Machine Learning ,
pages 263–272. PMLR, 2017.
Pierre-Luc Bacon, Jean Harb, and Doina Precup. The
option-critic architecture. In Proceedings of the AAAI
Conference on Artiﬁcial Intelligence , volume 31, 2017.
Melike Baykal-Gürsoy. Semi-markov decision processes.
Wiley Encyclopedia of Operations Research and Man-
agement Science , 2010.
Erhan Cinlar. Introduction to stochastic processes . Courier
Corporation, 2013.
Christoph Dann, Tor Lattimore, and Emma Brunskill. Uni-
fying pac and regret: Uniform pac bounds for episodic
reinforcement learning. Advances in Neural Information
Processing Systems , 30, 2017.
Thomas G Dietterich. Hierarchical reinforcement learning
with the maxq value function decomposition. Journal of
artiﬁcial intelligence research , 13:227–303, 2000.
Aryeh Dvoretzky, Jack Kiefer, and Jacob Wolfowitz.
Asymptotic minimax character of the sample distribution
function and of the classical multinomial estimator. The
Annals of Mathematical Statistics , pages 642–669, 1956.
David A Freedman. On tail probabilities for martingales.
the Annals of Probability , pages 100–118, 1975.
Ronan Fruit and Alessandro Lazaric. Exploration-
exploitation in mdps with options. In Artiﬁcial intelli-
gence and statistics , pages 576–584. PMLR, 2017.
Ronan Fruit, Matteo Pirotta, Alessandro Lazaric, and
Emma Brunskill. Regret minimization in mdps with op-
tions without prior knowledge. Advances in Neural In-
formation Processing Systems , 30, 2017.
Mohammad Ghavamzadeh, Alessandro Lazaric, and
Matteo Pirotta. Exploration in reinforcement
learning. Tutorial at AAAI’20, 2020. URL
https://rlgammazero.github.io/ .Wassily Hoeffding. Probability inequalities for sums of
bounded random variables. Journal of the American Stat-
istical Association , 58(301):13–30, 1963.
Andrew Levy, George Konidaris, Robert Platt, and Kate
Saenko. Learning multi-level hierarchies with hindsight.
InProceedings of International Conference on Learning
Representations , 2019.
Timothy A Mann, Shie Mannor, and Doina Precup. Ap-
proximate value iteration with temporally extended ac-
tions. Journal of Artiﬁcial Intelligence Research , 53:
375–438, 2015.
Andreas Maurer and Massimiliano Pontil. Empirical bern-
stein bounds and sample variance penalization. arXiv
preprint arXiv:0907.3740 , 2009.
Oﬁr Nachum, Shixiang Shane Gu, Honglak Lee, and
Sergey Levine. Data-efﬁcient hierarchical reinforcement
learning. Advances in neural information processing sys-
tems, 31, 2018.
Ian Osband and Benjamin Van Roy. On lower bounds
for regret in reinforcement learning. arXiv preprint
arXiv:1608.02732 , 2016.
Shubham Pateria, Budhitama Subagdja, Ah-hwee Tan, and
Chai Quek. Hierarchical reinforcement learning: A com-
prehensive survey. ACM Computing Surveys (CSUR) , 54
(5):1–35, 2021.
Iosif Pinelis. Dkw type inequality for re-
newal processes. MathOverﬂow, 2019. URL
https://mathoverflow.net/q/326434 .
Doina Precup and Richard S Sutton. Multi-time models for
temporally abstract planning. Advances in neural inform-
ation processing systems , 10, 1997.
Martin L Puterman. Markov decision processes: discrete
stochastic dynamic programming . John Wiley & Sons,
2014.
Walter L Smith. Renewal theory and its ramiﬁcations.
Journal of the Royal Statistical Society: Series B (Meth-
odological) , 20(2):243–284, 1958.
Richard S Sutton and Andrew G Barto. Reinforcement
learning: An introduction . MIT press, 2018.
Richard S Sutton, Doina Precup, and Satinder Singh.
Between mdps and semi-mdps: A framework for tem-
poral abstraction in reinforcement learning. Artiﬁcial in-
telligence , 112(1-2):181–211, 1999.
Alexander Sasha Vezhnevets, Simon Osindero, Tom
Schaul, Nicolas Heess, Max Jaderberg, David Silver, and
Koray Kavukcuoglu. Feudal networks for hierarchical
reinforcement learning. In International Conference on
Machine Learning , pages 3540–3549. PMLR, 2017.Tsachy Weissman, Erik Ordentlich, Gadiel Seroussi, Ser-
gio Verdu, and Marcelo J Weinberger. Inequalities for
the l1 deviation of the empirical distribution. Hewlett-
Packard Labs, Tech. Rep , 2003.
Zheng Wen, Doina Precup, Morteza Ibrahimi, Andre Bar-
reto, Benjamin Van Roy, and Satinder Singh. On ef-
ﬁciency in hierarchical reinforcement learning. Ad-
vances in Neural Information Processing Systems , 33:
6708–6718, 2020.
Andrea Zanette and Emma Brunskill. Problem dependent
reinforcement learning bounds which can identify ban-
dit structure in mdps. In International Conference on
Machine Learning , pages 5747–5755. PMLR, 2018.A DETAILED PROOF OF THEOREM 6.1
A.1 NOTATION AND SETTING
First, we need to deﬁne the value function of an SMDP. In Sutto n et al. [1999] it is deﬁned as a formalism for MDP with
options, that itself, by the demonstration presented in the same article, is an SMDP.
In our case, however, for the SMDP model, we are considering a n additional dependency on h∈[0,H].
Notation used:
•His the horizon
•µpolicy over options {µ:S×O×H→[0,1]}
•r(s,o,h)is the discounted cumulative reward gained by selecting the optiono, in states, in the instant hof the horizon
H
•p(s′,h′|s,o,h)is a new transition model that characterizes both the state d ynamic and the time the option executes.
•w(s,o,h)is the probability of playing option obeing in state sat time-step h
The value function is deﬁned as:
Vµ(s,h) =/summationdisplay
o∈Osµ(s,o,h)/bracketleftBig
r(s,o,h)+/summationdisplay
s′,h′>hp(h′,s′|s,o,h)Vµ(s′,h′)/bracketrightBig
(10)
withVµ(s,H) = 0 .
A.2 PERFORMANCE DIFFERENCE LEMMA
Difference in value of a policy µin two different SMDPs ( ˜and¯)
˜Vµk(s,1)−¯Vµk(s,1)
=ˆE/bracketleftbiggH/summationdisplay
i=1/parenleftBig/parenleftbig
˜r(si,oi,hi)−¯r(si,oi,hi)/parenrightbig
+/parenleftbig
˜p(si+1,hi+1|si,oi,hi)−¯p(si+1,hi+1|si,oi,hi)/parenrightbig˜Vµk(si+1,hi+1)/parenrightBig/BD{hi< H}/bracketrightbigg
ˆEis the expectation taken w.r.t. the policy µand the transition probability ˆp(s′,h′|s,o,h), and can be rewrite as:
H/productdisplay
i=1µk(si,oi,hi)ˆpk(si+1,hi+1|si,oi,hi) /BD{hi< H}
This quantity is the distribution of visits for the policy µkin the "ˆ" SMDP and it is equivalent to wtkfor the FH-MDP
case.
Proof. The result follows by unrolling equation 10. Lemma E.5 Dann e t al. [2017] for an example in FH-MDPs.
A.3 CONFIDENCE INTERV ALS
The conﬁdence sets are deﬁned as:
Br
k(s,o,h) := [ˆrk(s,o,h)−βr
k(s,o,h),ˆrk(s,o,h)+βr
k(s,o,h)]
Bp
k(s,o,h) :=/braceleftbig
pk(·,·|s,o,h)∈ ∇(s) :∝ba∇dbl˜pk(·,·|s,o,h)−ˆpk(·,·|s,o,h)∝ba∇dbl1≤βp
k(s,o,h)/bracerightbig
and the relative conﬁdence bounds βr
k(s,o,h)andβp
k(s,o,h)using Empirical Bernstein bound [Maurer and Pontil, 2009],
Hoeffding [1963] and Weissman et al. [2003].
βr
k(s,o,h)∝/radicalBigg
2ˆVar(r)ln2/δ
n(s,o,h)+7ln2/δ
3(n−1)(11)
βp
k(s,o,h)∝/radicalBigg
Slog/parenleftbignk(s,o,h)
δ/parenrightbig
nk(s,o,h)(12)withˆVar(r)be the sample variance of r.
ˆVar(r) =1
n(n−1)/summationdisplay
1≤i≤j≤n(ri−rj)2(13)
A.4 ACTUAL PROOF
Theorem 6.1. Considering a non-stationary Finite Horizon SMDP SM and a set of options O, with bounded primitive
rewardr(s,a)∈[0,1]. The regret suffered by algorithm FH-SMDP-UCRL, in Kepisodes of horizon His bounded as:
Regret(K) ≤˜O/parenleftBigg/parenleftBig√
SOKd2/parenrightBig/parenleftbigg
T+√
SH/parenrightbigg/parenrightBigg
with probability 1−δ.
Where:
T= max
s,o,h/radicalbig
E[τ(o,s,h)2]
= max
s,o,h/radicalbig
E[τ(o,s,h)]2+Var[τ(s,o,h)],
τis the holding time, and ddescribes the expected number of decisions taken in one epis ode that is d≈H/¯τ, with¯τthe
average duration of the set of options.
Proof.
Regret(K) =K/summationdisplay
k=1V∗(s,1)−¯Vµk(s,1)
Opt.
≤K/summationdisplay
k=1˜Vµk(s,1)−¯Vµk(s,1)
=K/summationdisplay
k=1¯E/bracketleftbiggH/summationdisplay
i=1/parenleftBig/parenleftbig
˜r(si,oi,hi)−¯r(si,oi,hi)/parenrightbig
+/parenleftbig
˜p(si+1,hi+1|si,oi,hi)−¯p(si+1,hi+1|si,oi,hi)/parenrightbig˜Vµk(si+1,hi+1)/parenrightBig/BD{hi< H}/bracketrightbigg
a=/summationdisplay
k/summationdisplay
i∈[H]/summationdisplay
(s,o,h)∈Lkwk(si,oi,hi)/parenleftbigg/parenleftbig
˜r(si,oi,hi)−¯r(si,oi,hi)/parenrightbig
+/parenleftbig
˜p(si+1,hi+1|si,oi,hi)−¯p(si+1,hi+1|si,oi,hi)/parenrightbigT˜Vµk(si+1,hi+1)/parenrightbigg
b
≤/summationdisplay
k/summationdisplay
i∈[H]/summationdisplay
(s,o,h)∈Lkwk(si,oi,hi)/parenleftBig
2βr
k(si,oi,hi)+2βp
k(si,oi,hi)TH/parenrightBig
c∝/summationdisplay
k/summationdisplay
i∈[H]/summationdisplay
(s,o,h)∈Lkwk(si,oi,hi)/parenleftbigg/radicalBigg
ˆVar(r)
nk(s,o,h)+1
nk(s,o,h)−1+/radicalBigg
S
nk(s,o,h)H/parenrightbigg
d
≤/summationdisplay
k/summationdisplay
i∈[H]/summationdisplay
(s,o,h)∈Lkwk(si,oi,hi)/radicalbig
nk(si,oi,hi)/parenleftbigg/radicalBig
ˆVar(r)+√
SH/parenrightbigg
+/summationdisplay
k/summationdisplay
i∈[H]/summationdisplay
(s,o,h)∈Lkwk(si,oi,hi)
nk(s,o,h)
e
≤˜O/parenleftBigg/parenleftBig√
dSOn/parenrightBig/parenleftbigg/radicalBig
ˆVar(r)+√
SH/parenrightbigg
+dSO/parenrightBigg
f
≤˜O/parenleftBigg/parenleftBig√
dSOn/parenrightBig/parenleftbigg
RmaxT+√
SH/parenrightbigg
+dSO/parenrightBiggwith
T= max
s,o,h/radicalbig
E[τ(oi,si,hi)2] = max
s,o,h/radicalbig
E[τ(oi,si,hi)]2+Var[τ(si,oi,hi)] (14)
withτrepresenting the average duration of the set of options seen so far.
The ﬁrst passage is a standard inequality when proving the re gret in frameworks adopting optimism in face of uncertainty .
(a) The expectation with respect to the policy µkand the transition model ¯pcan be replaced with a more common
formulation used in the Finite Horizon literature [Dann et a l., 2017, Zanette and Brunskill, 2018],/summationtext
(s,o,h)∈Lk.
Where,Lkis deﬁned as the good set [Dann et al., 2017, Zanette and Bruns kill, 2018], which is the number of episodes
in which the triple (s,o,h)is seen sufﬁciently often, and this equation is valid for all the tuples (s,o,h)being part of
this set.
(b) We upper bound the difference of rewards and transition p robabilities with two times their relative conﬁdence inter vals,
and, the Value function at the next step with the horizon leng thH.
(c) We substitute the conﬁdence intervals with their deﬁnit ions (eq. 12) neglecting logarithmic terms.
(d) We divide the summation in two, to upper bound the terms se parately
(e) Using the adaptation of lemma 16 of Zanette and Brunskill [2018] for SMDPs, lemma 7.2, for the ﬁrst term. Using
passage (b) and (c) in the proof of lemma E.1
(f) Upperbounding the sample variance of r, withRmaxT. WhereTis the sample variance of the duration.
B SPECIAL CASE OF FIXED-LENGTH OPTIONS
Let’s consider the same ﬁnite horizon MDP with options with ﬁ xed length M:=< S,O,R h,Ph,H,¯τ >where each option
o:=< I,πo,β > has a ﬁxed initial set Iand ﬁxed termination condition β,Rh(s,o)∈[0,¯τRmax]is the expectation of
the reward function distribution, Ph(·|s,o)is the transition distribution, His the horizon, and ¯τ≤His the options ﬁxed
length. The solution of the MDP will be a policy πH:S→Othat maximizes the cumulative return choosing among
options’ optimal policies πoi. The reward function over state−options pairs relates to the ﬂat-MDP’s reward as:
Rh(s,o) = Es0=s
ai∼πo(·|si)
si+1∼ph+1(·|si,ai)/bracketleftbigg¯τ−1/summationdisplay
i=0rh+i(ai,si)/bracketrightbigg
Denote with VπH
n(s)the state value function associated with a hierarchical pol icyπH(with Hierarchical policy we deﬁne
a policy that chooses among options).
VπH
n(s) = Es0=s
oj∼πH(·|sj)
sj+1∼P(·|sj,oj)h/bracketleftbiggN/summationdisplay
j=0Rh×¯τ(sj,oj)/bracketrightbigg
withN=H
¯τthe number of decision steps that occurs during the Horizon.
In this way, we can exploit the same performance difference lemma of Dann [Dann et al., 2017] Lemma E.15, where,
instead of actions we have ﬁxed length options, and we sum ove rNdecision steps. Hence, we can write:
Regret(K)Opt.
≤K/summationdisplay
k=1˜VπH
k
1(s)−¯VπH
k
1(s) (15)
a=/summationdisplay
k/summationdisplay
n∈[N]/summationdisplay
(s,o)∈Lkwnk(s,o)/parenleftbigg/parenleftbig˜Rn(s,o)−¯Rn(s,o)/parenrightbig
+/parenleftbig˜Pn(s,o)−¯Pn(s,o)/parenrightbigT˜Vπk
n+1/parenrightbigg
(16)
+term considering the state-options pairs inside the failur e event (17)herewnk(s,o)is the probability of visiting state sand choosing option othere at the decision step n in the k-th episode.
Then, we consider a new formulation of the conﬁdence sets:
BR
nk(s,o) := [ˆRnk(s,o)−βR
nk(s,o),ˆRnk(s,o)+βR
nk(s,o)]
BP
nk(s,o) :=/braceleftbig
Pnk(·|s,o)∈ ∇(s) :∝ba∇dbl˜Pnk(·|s,o)−ˆPnk(·|s,o)∝ba∇dbl1≤βP
nk(s,o)/bracerightbig
using Hoeffding [1963] and Weissman et al. [2003] the conﬁde nce bounds are:
βR
nk(s,o)∝Rmax¯τ/radicalBigg
log/parenleftbignnk(s,o)
δ/parenrightbig
nnk(s,o)(18)
βP
nk(s,o)∝/radicalBigg
Slog/parenleftbignnk(s,o)
δ/parenrightbig
nnk(s,o)(19)
After the deﬁnition of the conﬁdence sets we can bound the pre vious equation as follow:
K/summationdisplay
k=1˜VπH
k
1(s)−¯VπH
k
1(s) =/summationdisplay
k/summationdisplay
n∈[N]/summationdisplay
(s,o)∈Lkwnk(s,o)/parenleftbigg/parenleftbig˜Rnk(s,o)−¯Rnk(s,o)/parenrightbig
+/parenleftbig˜Pnk(s,o)−¯Pnk(s,o)/parenrightbigT˜Vπk
h+1/parenrightbigg
+term considering the state-options pairs inside the failur e event
a
≤/summationdisplay
k/summationdisplay
n∈[N]/summationdisplay
(s,o)∈Lkwnk(s,o)/parenleftbigg
2βR
nk+2βP
nkT(H−¯τ)/parenrightbigg
b∝/summationdisplay
k/summationdisplay
n∈[N]/summationdisplay
(s,o)∈Lkwnk(s,o)/parenleftbiggRmax¯τ√nnk+/radicalbigg
S
nnk(H−¯τ)/parenrightbigg
=/summationdisplay
k/summationdisplay
n∈[N]/summationdisplay
(s,o)∈Lkwnk(s,o)√nnk/parenleftbigg
Rmax¯τ+√
S(H−¯τ)/parenrightbigg
c
≤˜O/parenleftBig
N√
SOK/parenleftbig
Rmax¯τ+√
SH−√
S¯τ/parenrightbig/parenrightBig
d
≤˜O/parenleftBig
N√
SOK/parenleftbig
Rmax¯τ+√
SH)/parenrightBig
(a) substituting the ˜Rn(s,o)−¯Rn(s,o)and˜Pn(s,o)−¯Pn(s,o)with double the relative conﬁdence interval and consid-
ering˜Vπk
h+1≤(H−¯τ). The second term will be omitted for ease of notation.
(b) replacing the conﬁdence intervals with their deﬁnition
(c) Lemma E.3
(d) considering the worst case, where there isn’t the negati ve term
comparing it with the bound of Fruit and Lazaric [2017] for bo unded holding time:
˜O/parenleftBig/parenleftbig
Do√
S+Tmax+(Tmax−Tmin)/parenrightbig
Rmax/radicalbig
SOOn/parenrightBig
that having options with ﬁxed duration ¯τ, and considering Rmax= 1reduces to:
˜O/parenleftBig
DS√
On+ ¯τ√
SOn/parenrightBig
we have the same bound where instead of the diameter we have th e Horizon H, and where NK is exactly equal to the
number of the decisions up to episode k, which is nin their notation. We have:
˜O/parenleftBig
HS√
ON2K+ ¯τ√
SON2K/parenrightBig
Important : Note that we have an additional√
Nterms because we considered non-stationary MDP. This is a we ll-known
penalty term when considering non-stationarity in the proc ess.C PROOF OF THEOREM 6.2
Theorem 6.2. The regret paid by the two-phase learning algorithm until th e episode K is:
Regret(K)≤˜O/parenleftBig
K2
33/radicalbig
H5oS2oAoO+H2S
Ho√
OK/parenrightBig
withHothe ﬁxed horizon of each option o∈ O,So, andAothe upper bounds on the cardinality of the state and action
space of the sub-FH-MDPs.
Proof. The regret of the two-phase algorithm can be written in this f orm
Regret(K) =K1/summationdisplay
k=1V∗
∗(s,1)−Vµ
(πk)(s,1)+/summationdisplay
k=k1V∗
∗(s,1)−VµkπK1
=K1/summationdisplay
k=1V∗
∗(s,1)−Vµ
(πk)(s,1)
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
Options learning Regret+K/summationdisplay
k=K1V∗
∗(s,1)−V∗
(πK1)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
Bias+V∗
(πK1)−Vµk
(πK1)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
Regret SMDP with ﬁxed options
The regret is the sum of the regret paid in the ﬁrst phase and th e regret paid in the second one, plus an additional bias
term. By assuming all options with equal samples Koand the options’ policies learning as Oﬁnite horizon MDPs for
whichSo,Ao,Hoare the upper bounds of the option’s state space dimension, t he option’s action space dimension and the
option’s horizon, and K1=/summationtext
o∈OKoand
K=/summationdisplay
o∈OKo+K2 (20)
we can write the regret as
Regret(K)≤/summationdisplay
o∈OKoHo+K2max
o∈O1
KoH2
oSo/radicalbig
AoKo+HS/radicalbig
Od2K2
In which we pay full regret for each option learning, then the maximum average regret considering the option learning as
a ﬁnite horizon MDP with horizon Ho, and the regret of the SMDP learning with ﬁxed options.
However, considering the options with equal samples and wit hSo,Ao,Hothe upper bounds of the relative quantities we
can get rid of the maximization in the second term, and d=H
Hoand the regret became
Regret(K)≤OKoHo+K2
KoH2
oSo/radicalbig
AoKo+H2
HoS/radicalbig
OK2
Now, by substituting K2with eq. 20, and upper bounding (K−OK)≤Kwe can solve in closed form to ﬁnd Koto
minimize the regret.
Regret(K)≤OKoHo+K
KoH2
oSo/radicalbig
AoKo+H2
HoS√
OK
Ko=3/radicalbigg
K2S2oH2oAo
O24
Therefore, by substituting Koin the original equation we have
Regret(K)≤˜O/parenleftBig
K2
3(H5
oS2
oAoO)1
3+H2S
Ho√
OK/parenrightBig
Now we can compare the regret of this algorithm compared to th e regret of UCRL2 adapted for non-stationary FH-MDPs
[Ghavamzadeh et al., 2020].
Regret(UCRL2−CH)≤˜O(H2S√
AK)Regret SMDP
Regret MDP≤K1/6α3/8O1/3
(HS)1/3A1/6≤1 (21)
K≤H2S2A
α16O2(22)
D RENEWAL PROCESSES
Lemma D.1 (Renewal Function Bound) .Considering a Renewal process, (Xt)t≥0, and a sequence S1,S2...of ran-
dom variables, characterizing the random duration of an eve nt, alternatively deﬁned as holding time, with supp(Si)∈
{1,...,H}. We can bound, with probability 1−δ, the expected number of random events that occurred up to tim et,Xt,
with:
Xt</radicalbigg
ln2−lnδ
cK+t
µ
withc=µ3
32σ2Twhereµis the mean of the r.v.s and σ2the variance.
Proof. Based on the proof presented on Pinelis [2019], which apply D KW type inequalities to renewal processes [Dvoret-
zky et al., 1956]
Pr/parenleftbigg
sup
0≤t≤T/vextendsingle/vextendsingle/vextendsingle/vextendsingleXnt
n−t
µ/vextendsingle/vextendsingle/vextendsingle/vextendsingle≥ǫ/parenrightbigg
≤2e−cnǫ2
Now we can equal 2e−cnǫ2toδand ﬁndǫ.
ǫ=/radicalbigg
ln2−lnδ
cn
Thus with probability 1−δ
Xt≤/radicalbigg
ln2−lnδ
cn+t
µ
that completes the proof
The following lemma is lemma 6.5 in the main paper.
Lemma D.2. [Bound on number of options played in one episode] Consideri ng a Finite Horizon SMDP SM with horizon
H and,Ooptions with duration τmin≤τ≤τmax andmino(E[τo])the expected duration of the shorter option. The
expected number of options played in one episode dcan be seen as the renewal function m(t)of a renewal process up to
the instant H. With probability 1−δthis quantity is bounded by
d </radicalBigg
32(τmax−τmin)H(ln2−lnδ)
(mino(E[τo]))3+H
mino(E[τo])
Proof. The proof followed the one of lemma D.1 and the fact that we are considering T=H,n= 1,t=H,µ= ¯τ,
σ2= (τmax−τmin), andXt=d.E USEFULL LEMMAS
Lemma E.1 (lemma 16 [Zanette and Brunskill, 2018] for non stationary M DPs) .The following holds true:
/summationdisplay
k/summationdisplay
h∈[H]/summationdisplay
(s,a)∈Lkwhk(s,a)/radicalBigg
1
nk(s,a,h)=˜O(√
HSAT)
where the extra√
His due to the non-stationarity of the environment
Proof.
/summationdisplay
k/summationdisplay
h∈[H]/summationdisplay
(s,a)∈Lkwhk(s,a)/radicalBigg
1
nk(s,a,h)
a
≤/radicalBigg/summationdisplay
k/summationdisplay
h∈[H]/summationdisplay
(s,a)∈Lkwhk(s,a)/radicaltp/radicalvertex/radicalvertex/radicalbt/summationdisplay
k/summationdisplay
h∈[H]/summationdisplay
(s,a)∈Lkwhk(s,a)1
nk(s,a,h)
b=√
KH/radicaltp/radicalvertex/radicalvertex/radicalbt/summationdisplay
k/summationdisplay
h∈[H]/summationdisplay
(s,a)∈Lkwhk(s,a)1
nk(s,a,h)
e
≤˜O(√
HSAT)
Then:
/summationdisplay
k/summationdisplay
h∈[H]/summationdisplay
(s,a)∈Lkwhk(s,a)
nk(s,a,h)
c
≤/summationdisplay
h∈[H]/summationdisplay
(s,a)∈Lk/summationdisplay
kwhk(s,a)
1
4/summationtext
j≤kwhj(s,a)
d
≤4HSAlog/parenleftbiggKe
wmin/parenrightbigg
∼∝HSA
(a) by Cauchy-Schwartz
(b)/summationtext
t∈[H]/summationtext
(s,a)∈Lkwtk(s,a) =Hlemma 17 ( b) Zanette and Brunskill [2018]
(c) lemma 2 Zanette and Brunskill [2018] adapted to the non-s tationary case
(d) lemma E.5 Dann et al. [2017] considering that being (s,a) part of the good set Lk, then we are assuming (Appendix
E.3 Dann et al. [2017]) that wk(s,a)≥wmin.
(e) substituting (f) we get the upper bound, and we conclude t he proof.
Lemma E.2 (lemma 16 [Zanette and Brunskill, 2018] for SMDPs (Lemma 7.2 main paper)) .Considering a non-stationary
MDP M with a set of options as an SMDP MO[Sutton et al., 1999]. In MOthe number of decisions taken in the kth-episode
is a random variable dand
/summationdisplay
i∈H/summationdisplay
(s,o)∈Lkwk(si,oi,hi) /BD{hi< H}=dkwith{∀k:dk≤H}
with mean d. Therefore, the following holds true:
/summationdisplay
k/summationdisplay
i∈H/summationdisplay
(s,o)∈Lkwk(si,oi,hi)/radicalBigg
1
nk(si,oi,hi)=˜O/parenleftbigg√
SOKd2/parenrightbiggor, more generally, using the same notation used in Fruit and Lazaric [2017]
/summationdisplay
k/summationdisplay
i∈H/summationdisplay
(s,o)∈Lkwk(si,oi,hi)/radicalBigg
1
nk(si,oi,hi)=˜O/parenleftbigg√
dSOn/parenrightbigg
withnnumber of decisions taken up to episode k.
Proof. Due to the stochasticity of the option’s duration, dis a random variable expressing the number of decisions take n
in a step. Thus, ﬁrst, we can rewrite passage (b)of the proof of lemma 17 Zanette and Brunskill [2018] then, we change
lemma E.1 considering the same notion of good set considered in the appendix of Zanette and Brunskill [2018] and the
validity of lemma 2 of Zanette and Brunskill [2018], in the op tions framework(replacing owitha). If all the aforementioned
assumptions hold, thus the derivation of the new lemma follo ws the derivation of lemma E.1
Lemma E.3 (lemma 16 [Zanette and Brunskill, 2018] for MDPs with option s of ﬁxed lenght) .For an MDP with Ooptions,
with a ﬁxed lenght ¯τ, where the horizon is divided in N=H
¯τdecision steps, the following holds true:
/summationdisplay
k/summationdisplay
n∈N/summationdisplay
(s,o)∈Lkwnk(s,o)/radicalBigg
1
nk(s,o)=˜O/parenleftbigg
N√
SOK/parenrightbigg
Proof. In this MDP the control returns to the hierarchical policy af ter exactly ¯τtime steps (the length of an option), thus,
we can have at most N=H
¯τactions in the horizon H. For this reason, passage (b) of the proof of lemma E.1 become
/summationdisplay
n∈N/summationdisplay
(s,o)∈Lkwnk(s,o) =N
The rest results for the same passage of the proof of lemma E.1 .
To have a more complete analysis we need also to consider the t riples (s, o, h) which aren’t inside the good set. To do that,
we can adapt Lemma 3 of Zanette and Brunskill [2018], for the F H-SMDP setting.
Lemma E.4 (Outside the good set) .It holds that:
K/summationdisplay
k=1d/summationdisplay
h=1/summationdisplay
(s,o,h)/∈Lkwk(s,o,h) =˜O(SOd)
The proof follows from the one of lemma 3 of Zanette and Brunsk ill [2018].