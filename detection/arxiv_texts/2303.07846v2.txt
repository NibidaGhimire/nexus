Journal of Machine Learning Research 25 (2024) 1-32 Submitted 3/23; Revised 12/23; Published 1/24
Sample-efficient Adversarial Imitation Learning
Dahuin Jung annajung0625@snu.ac.kr
Electrical and Computer Engineering
Seoul National University, Seoul 08826, Republic of Korea
Hyungyu Lee rucy74@snu.ac.kr
Electrical and Computer Engineering
Seoul National University, Seoul 08826, Republic of Korea
Sungroh Yoon∗sryoon@snu.ac.kr
Electrical and Computer Engineering
Interdisciplinary Program in Artificial Intelligence
Seoul National University, Seoul 08826, Republic of Korea
Editor: Scott Niekum
Abstract
Imitation learning, in which learning is performed by demonstration, has been studied
and advanced for sequential decision-making tasks in which a reward function is not
predefined. However, imitationlearningmethodsstillrequirenumerousexpertdemonstration
samples to successfully imitate an expert’s behavior. To improve sample efficiency, we utilize
self-supervised representation learning, which can generate vast training signals from the
given data. In this study, we propose a self-supervised representation-based adversarial
imitation learning method to learn state and action representations that are robust to
diverse distortions and temporally predictive, on non-image control tasks. In particular, in
comparison with existing self-supervised learning methods for tabular data, we propose a
different corruption method for state and action representations that is robust to diverse
distortions. We theoretically and empirically observe that making an informative feature
manifold with less sample complexity significantly improves the performance of imitation
learning. The proposed method shows a 39 %relative improvement over existing adversarial
imitation learning methods on MuJoCo in a setting limited to 100 expert state-action
pairs. Moreover, we conduct comprehensive ablations and additional experiments using
demonstrations with varying optimality to provide insights into a range of factors.
Keywords: imitation learning, adversarial imitation learning, self-supervised learning,
data efficiency
1. Introduction
Imitation learning (IL) is widely used in sequential decision-making tasks, where the design
of a reward function is complicated or uncertain. When a reward is sparse (Reddy et al.,
2019) or an optimal reward function is unknown, IL finds an optimal policy that relies only
on expert demonstrations. Owing to recent development in deep neural networks, the range
∗. Corresponding Author
A preliminary version of this manuscript was presented at Deep RL Workshop, NeurIPS 2022.
©2024 Dahuin Jung, Hyungyu Lee, and Sungroh Yoon.
License: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/ . Attribution requirements are provided at
http://jmlr.org/papers/v25/23-0314.html .arXiv:2303.07846v2  [cs.LG]  23 Jan 2024Jung, Lee, and Yoon
of behaviors, which can be imitated, has expanded. There are two main learning approaches
for IL. The first approach trains a policy by following actions from an expert in a supervised
manner called behavioral cloning (BC) (Sutton and Barto, 1998; Pomerleau, 1989). However,
error accumulation limits BC because it greedily imitates the demonstrated actions. For
behavior cloning, it talks about the problem of accumulating errors, but there are alternative
IL approaches like DAgger that deals with that. The second approach is inverse reinforcement
learning (IRL) (Arora and Doshi, 2021), inferring a cost function based on given expert
demonstrations. The IRL implements adversarial learning (Goodfellow et al., 2014) to infer
the cost function. Therefore, an agent learns the policy to imitate expert demonstrations,
whereas a discriminator learns to differentiate between the expert’s behavior and that of the
agent. The learned discriminator is used as the cost function in the reinforcement learning
(RL) phase. However, there are also several IRL methods that do not leverage adversarial
training, e.g., Max-Margin Planning (Ratliff et al., 2006) and Max-Ent IRL (Ziebart et al.,
2008b).
Although IRL has led an advance in IL, it has key challenges. First, adversarial learning
is known to be delicate in practice. The min-max computational formulation of adversarial
imitation learning (AIL) often involves brittle approximation techniques. Second, the IL
requires many demonstration trajectories to recover an expert policy. Although IRL requires
fewer demonstrations than BC, it still requires considerable trajectories. Recently, many
algorithms or techniques have been proposed to address the first challenge (Fu et al., 2018;
Peng et al., 2019; Reddy et al., 2019; Qureshi et al., 2018; Chen et al., 2019; Wang et al., 2022);
however, little work has been done to improve the sample efficiency of expert demonstrations
required (Zhang et al., 2020; Barde et al., 2020).
Self-supervised representation learning (SSL) has advanced sample efficiency in the image
and language domains (Chen et al., 2020b; Grill et al., 2020; Mikolov et al., 2013). It applies
various transformations to the data and uses the transformed data itself as supervision.
Thus, it increases the sample efficiency by obtaining training signals from auxiliary tasks or
objectives that do not rely on labels. Specifically, InfoNCE (Chen et al., 2020b; He et al.,
2020) and asymmetric twin-based (Grill et al., 2020; Chen and He, 2021) SSL approaches
are known to be effective for learning robust feature representations for different distortions
of identical inputs. Recently, SSL has been utilized in image-based RL algorithms (Srinivas
et al., 2020; Schwarzer et al., 2020; Li et al., 2022) and has shown significant improvement
in performance. However, transformation techniques applied to image-based RL are not
directly adaptable to non-image control benchmarks. This is because these approaches rely
upon the semantic/spatial properties of data that may generate either out-of-distribution
examples or examples that supply only the same view when directly applied to a continuous
control (tabular) domain.
In this study, we propose a sample-efficient AIL method for non-image control benchmarks.
The proposed method leverages auxiliary training signals for learning state and action
representations that are temporally predictive and robust to diverse distortions. Based on the
characteristics of each domain and benchmark, an auxiliary task that can learn informative
feature representations is different. For RL, to address sequential decision-making tasks, the
feature representation of a state and action should contain temporally predictive information.
To address this, we add an auxiliary task that predicts the next state representation from the
given current state and action representations. This marks the initial attempt to enhance
2Sample-efficient Adversarial Imitation Learning
the sample efficiency of expert demonstrations in non-image imitation learning by employing
SSL.
Moreover, learning representations that discard information regarding nuisance variables
improves generalization and decreases required sample complexity. Previous transformation
techniquesfortabulardata(Yoonetal.,2020;Bahrietal.,2021)generatetransformedsamples
far from real samples. Therefore, we propose a simple, effective corruption method that
generates transformed samples showing diverse distortions that are possible in-distribution.
Empirically, we demonstrate that promoting temporally predictive feature representations
with robustness against diverse distortions significantly improves sample efficiency.
First, we theoretically observe that IL performs substantially better when an informative
feature manifold is created with less sample complexity. We evaluate the proposed method
on MuJoCo (Todorov et al., 2012) and Atari RAM of OpenAI Gym (Brockman et al., 2016),
where each benchmark is allowed less than 100 expert state-action pairs. The proposed
method outperforms the previous AIL methods by a significant margin in scenarios with a
small number of perfect or imperfect expert demonstrations. We discover that the proposed
corruption method generates various transformed samples that are not out-of-distribution.
We perform comprehensive ablation studies, detailing the intuitions and effects of various
design choices and factors. Through runs with various hyperparameters on non-image control
tasks, we demonstrate that the corruption rate and loss function should be carefully chosen.
2. Related Work
2.1 Data-efficient Reinforcement Learning
In deep RL, studies have been conducted to improve sample efficiency. For continuous control,
several studies have suggested the use of reconstruction loss (Lee et al., 2020; Hafner et al.,
2019). However, most of the suggested methods are RL benchmarks, which have a sparse
reward or image state. Methods using a self-supervised error as an intrinsic reward have been
proposed to improve the sample efficiency in a sparse reward scenario (Pathak et al., 2017;
Mohamed and Jimenez Rezende, 2015; Simmons-Edler et al., 2019; Wu et al., 2022). For
the image state, various image augmentation techniques and self-supervised objectives have
been applied to reduce environmental interactions (Srinivas et al., 2020; Schwarzer et al.,
2020; Kielak, 2020; Yarats et al., 2020; Laskin et al., 2020; Mazoure et al., 2020; Hansen
et al., 2020, 2021; Zadaianchuk et al., 2022). Additionally, there are some works that apply
self-supervised learning to train latent space or feature embedding for reward learning (Brown
et al., 2020; Bobu et al., 2023). Different from most existing approaches, we utilize SSL to
improve the sample efficiency of expert demonstrations for non-image imitation learning.
2.2 Self-supervised Representation Learning
Currently, SSL is divided into three approaches. The first is a pretext task (Noroozi and
Favaro, 2016), which creates a pre-task that can learn useful feature representations and use
the learned representations on downstream tasks. The second is UCL. Contrastive learning
works on a simple push-pull principle, and it can be a sample or cluster level (Caron et al.,
2020). The contrastive loss contrasts the neighboring instances with non-neighboring ones
(Chen et al., 2020b; He et al., 2020). The third is asymmetric twin-based SSL. Unlike UCL,
3Jung, Lee, and Yoon
these methods do not use negative samples during training. Asymmetric twin-based SSL
methods learn robust representations in such a way that differently transformed versions
of input have the same representation. As representative methods, BYOL (Grill et al.,
2020) and Simsiam (Chen and He, 2021) used Siamese networks with weight sharing and
stop-gradient techniques to avoid collapse. Barlow twins (Zbontar et al., 2021) utilized a
correlation matrix between the representations of a differently transformed same input to
maximize the similarity between them while minimizing redundancy. VICReg (Bardes et al.,
2021) is effective for making the two representations similar and reducing the embedding of
non-informative factors.
There are two SSL methods for tabular data, VIME (Yoon et al., 2020) and SCARF
(Bahri et al., 2021), which can be directly applied to non-image control benchmarks. VIME
uses a random corruption method and SCARF suggests a method that replaces each feature
dimension by a random draw from that feature dimension’s empirical marginal distribution.
The difference between our work is the corrupted data of swapping is a mixture of only two
state-action pairs because the replaced features are sampled from another single state-action
pair. Meanwhile, the corrupted data of SCARF is a mixture of varying state-action pairs,
resulting in possible out-of-distribution data. In the experiments, the proposed swapping
corruption method showed higher performance compared to the other two methods. We
measured the variance and outlier scores of the corrupted samples produced by VIME,
SCARF, and the proposed method, to confirm that the proposed method qualitatively
generates more realistic and still varied samples.
2.3 Inverse Reinforcement Learning
Although IRL (Abbeel and Ng, 2004; Ziebart et al., 2008a) has made significant advances in
IL, it encounters some challenges. First, there is an unstable training issue for adversarial
learning; improved algorithms have been proposed to overcome this problem. GAIL (Ho and
Ermon, 2016) is the first study drawing an analogy between IL and generative adversarial
networks (Goodfellow et al., 2014). AIRL (Fu et al., 2018) proposed an AIL method that is
robust to changes in dynamics. VAIL (Peng et al., 2019) improves the stability problem by
constraining the information flow in the discriminator. EAIRL (Qureshi et al., 2018) reduces
the overfitting problem using empowerment (the information gain on action entropies). These
algorithms have been suggested to improve the stability and scalability of AIL.
Second, studiesonthesampleefficiencyofexpertdemonstrationshavenotbeensufficiently
conducted. SAILfO (Torabi et al., 2019) covers the necessity of studying the sample efficiency
of expert demonstrations, and proposes a simple model-based algorithm. f-GAIL (Zhang
et al., 2020) showed that finding an appropriate discrepancy measure during training is better
than using a predetermined measure to improve sample efficiency. ASAF (Barde et al., 2020)
is an algorithm in which training the discriminator could perform the role of policy and
showed that it helps improve sample efficiency. However, these methods require at least five
full trajectories to recover the expert policy on continuous control benchmarks such as the
MuJoCo physics engine. Unlike those methods, the proposed method successfully imitates
the expert’s behavior with less than one full trajectory.
In practice, it is difficult to collect perfectly optimal demonstrations because demon-
strations are commonly collected by crowdsourcing (Hu et al., 2018) or multiple experts.
4Sample-efficient Adversarial Imitation Learning
The collected data from external sources are normally imperfect—a mixture of optimal and
non-optimal demonstrations. To address these problems, algorithms for IL from imperfect
demonstrations have been proposed (Wu et al., 2019; Zhang et al., 2021). We demon-
strate that combining the proposed method with other algorithms for IL from imperfect
demonstrations further improves them, thus, verifying the scalability of the proposed method.
3. Theoretical Motivation
This section aims to provide the theoretical intuition for designing our cost function and
feature space aimed at reducing the generalization gap.
In a general classification learning process, the data and label spaces are denoted by X
andY, respectively, and P(x, y)is the joint distribution of the data and label. The primary
objective is to learn a classifier f:X → Ythat minimizes the expected value of a loss term
ℓover the joint distribution P(x, y). The classifier fis composed of a feature extractor h
and a fully connected layer W. More formally, when P(x, y)is known, the expected risk can
be defined as follows:
R(f|P) =Z
ℓ(W⊤h(x), y)dP(x, y). (1)
However, in practice, the data distribution is unknown. Empirical risk minimization (ERM)
is commonly utilized to address this problem. Then, the minimization can be defined as
follows:
ˆR(f|P) =1
MMX
i=1ℓ(W⊤h(xi), yi), (2)
where Mis the number of training data points. The generalization performance of a
practical classification learning process is highly dependent on the volume of training data.
However, in many real-world scenarios, acquiring sufficient training data is a challenging and
time-consuming task.
The generalization of ERM has been theoretically justified based on Vapnik-Chervonenkis
(VC) theory (Vapnik, 1999). The upper bound of the expected risk of the classifier fis
formulated by
R(f|P)⩽ˆR(f|P) +O |F|VC−logδ
Mξ!
, (3)
where 0≤δ≤1,0.5≤ξ≤1, and|F|VCis the finite VC dimension of a classifier.
LetH ⊆ RD, where Drepresents latent dimension, be the feature space of hand
W∈RD×C, where Cdenotes the number of classes, be the last fully connected layer of f.
Assumption 1 Consider a perfectly trained fully connected layer W∗. We can then split
the features as class-relevant or class-irrelevant based on their mutual information with
a class label Y. Class-relevant features have high mutual information between hd(x)for
1≤d≤DandY, while class-irrelevant features have low mutual information between hd(x)
for1≤d≤DandY.
Definition 1 (He et al., 2019) To minimize a change in classification results by varia-
tion of class-irrelevant features, wd,iandwd,jfor1≤d≤Dand1≤i, j≤Cshould
5Jung, Lee, and Yoon
be similar for class-irrelevant features in the empirical risk on original data ˆR(f|P) =
1
MPM
i=1ℓ(W⊤h(xi), yi).
Definition 1 from He et al. shows that in ˆR(f|P),wd,ifor all ifor class-irrelevant features
are not forced to be 0, that is, f∗preserves class-irrelevant features.
Eq. 3 highlights three key factors that can improve the generalization performance of
a classifier: 1) minimizing the empirical risk ˆR(f|P), 2) increasing the size of the training
data, and 3) reducing the VC dimension of the classifier. To improve the generalization, we
propose an approach that leverages self-supervised learning. By utilizing transformed data as
its own supervision, self-supervised learning can learn feature representations that filter out
information about nuisance variables. This enables the model to capture more meaningful
and generalizable features that can improve the overall performance of the classifier.
To begin with, we aim to reduce the upper bound of the expected risk by minimizing the
discrepancy between the original and transformed samples in the feature space H. Formally,
we utilize the mean squared error (MSE), the most widely-used discrepancy loss, as follows:
L=1
M×GM×GX
i=1h(xi)−h(x′
i)2
2, (4)
where x′represents the transformed version of xandM×Gis the number of training data
with a finite constant number of augmentations G.
By minimizing Eq. 4, we can effectively learn a feature space that does not encode the
distortions present in the input samples due to transformations. In other words, the resulting
feature representation is invariant to such distortions, leading to improved generalization
performance. Given the feature extractor with Eq. 4 as hMSEand the corresponding feature
space as HMSE, the classifier fhMSEconsisting of hMSEholds the following empirical risk
equation:
R(fhMSE|P)⩽ˆR(fhMSE|P) +O |FhMSE|VC−logδ
Mξ!
, (5)
where |FhMSE|VCis the finite VC dimension of the classifier with the feature space HMSE
trained with Eq. 4 simultaneously.
Corollary 1 hMSEis effective in reducing the VC dimension, however, the problem is that
the upper bound of R(f|P)we seek to is based on ˆR(f|P)instead of ˆR(fhMSE|P). As a
result, fhMSEcannot capture all the properties preserved in ˆR(f|P). Additionally, due to
the distribution gap between the feature spaces handhMSE, the optimal function f∗
hMSEof
ˆR(fhMSE|P)is not guaranteed to be a minimum of ˆR(f|P).
Proof.The proof can be found in Appendix A.1.
The feature extractor fhtypically preserves both class-relevant and class-irrelevant
features, whereas the embedding learned by fhMSEis more heavily regularized due to the
intensive transformation applied to the input samples. This regularization can be beneficial
for reducing the upper bound of the expected risk of the classifier, but it can also introduce
a distribution shift. To mitigate this issue, unsupervised contrastive learning can be utilized.
We assume that MSE and contrastive learning share the feature space.
6Sample-efficient Adversarial Imitation Learning
𝑠𝑡,𝑠𝑡′𝑠𝑡+1𝜋𝐸𝑠𝐸
𝑎𝐸
Ƹ𝑧𝑡+1𝑠
𝑉𝑎𝑙𝑢𝑒
𝐹𝑢𝑛𝑐𝑡𝑖𝑜𝑛(𝑧𝐸𝑠, 𝑧𝐸𝑎)
𝑟𝐀𝐠𝐞𝐧𝐭𝑆𝑡𝑎𝑡𝑒
𝐸𝑛𝑐𝑜𝑑𝑒𝑟
𝐴𝑐𝑡𝑖𝑜𝑛
𝐸𝑛𝑐𝑜𝑑𝑒𝑟
𝑃𝑜𝑙𝑖𝑐𝑦𝑎𝑡,𝑎𝑡′
𝐹𝑜𝑟𝑤𝑎𝑟𝑑
𝐷𝑦𝑛𝑎𝑚𝑖𝑐𝑠
𝑀𝑜𝑑𝑒𝑙𝐷𝑖𝑠𝑐𝑟𝑖𝑚𝑖𝑛𝑎𝑡𝑜𝑟𝑧𝐸𝑠
𝑧𝑡𝑠
𝑧𝑡+1𝑠𝑧𝐸𝑎
𝑧𝑡𝑎(𝑧𝑡𝑠, 𝑧𝑡𝑎)
𝐅𝐨𝐫𝐰𝐚𝐫𝐝
𝐋𝐨𝐬𝐬
(ℒ𝐹)(𝑧𝑡𝑠,𝑧𝑡𝑠′)𝐂𝐨𝐫𝐫𝐮𝐩𝐭𝐢𝐨𝐧
𝐋𝐨𝐬𝐬
(ℒ𝑆𝐶,ℒ𝐴𝐶)(𝑧𝑡𝑎,𝑧𝑡𝑎′)𝒩
expert agent(𝑡) reward agent(𝑡+1)
Figure1: Overviewoftheproposedmodel. Ourproposedmodelcomprisessixnetworksduring
IRL training. 1) The policy generates actions abased on states saccording to a specified
policy; 2) The value function evaluates the current policy πθ. It is trained using rewards
rderived from an estimated cost function (discriminator); 3) The state encoder extracts a
feature representation of raw states s, 4) The action encoder extracts a feature representation
of actions a, 5) The forward dynamics model predicts the feature representation of the
distorted version of next states ˆzs
t+1based on the feature representations of the current state
and action, zs
tandza
t, along with Gaussian noises N, and 6) The discriminator discriminates
agent demonstrations from expert demonstrations. The input is zs⊕za. The discriminator
is also referred to as a cost function. More details of each component and loss in the figure
are described in Section 4.
Definition 2 (Wang and Isola, 2020) Consider a perfectly trained W∗via contrastive
learning. The feature space HMSEbecomes maximally informative.
Claim 1 Class-irrelevant yet informative features are preserved via contrastive learning. It
reduces the inconsistency between ˆR(fhMSE|P)andˆR(f|P).
Proof.The proof with assumptions can be found in Appendix A.2.
By combining MSE (Eq. 4) with contrastive learning in the feature space, we anticipate a
notable enhancement in generalization, even when dealing with a modest amount of training
data. Our suggested algorithm expands this theoretical foundation rooted in classification
to AIL. Importantly, we capture temporally significant features crucial in RL by creating a
transformed version based on temporal features for contrastive learning.
4. Method
The core of RL is an agent and environment. An agent receives a reward from the environment
based on the actions determined by a policy. RL learns the optimal policy of a Markov
decision process. For IL, the agent learns the optimal policy where a pre-defined reward
function does not exist by relying only on given expert demonstrations. We define the IL
scenario from a small number of expert demonstrations. The expert demonstrations DEare
sampled from a state-action density of an expert, ρO, defined as follows:
DE={(sn, an)}NE
n=1i.i.d.∼ρO(s, a), (6)
7Jung, Lee, and Yoon
where NEis the number of state-action pairs from ρO. We assume a scenario in which NE
is less than the number of one full trajectory. We denote a state-action pair by x= (s, a)
where x∈XandX=S×A.
This study comprises the following six networks, as illustrated in Figure 1.
•A policy πθ(·)parameterized by θthat generates actions agiven states sbased on a
policy.
•A value function V(·)that evaluates a current policy πθ.Vis trained with rewards
rfrom an estimated cost function.
•A state encoder SE(·)that extracts a feature representation of raw states s.zs=
SE(s)∈Zs.
•An action encoder AE(·)that extracts a feature representation of actions a.za=
AE(a)∈Za.
•A forward dynamics model F(·)that predicts the feature representation of the
distorted version of next states ˆzs
t+1from the feature representations of the current
state and action, zs
tandza
t, and Gaussian noises N.ˆzs
t+1=F(zs
t⊕za
t⊕ N)∈ˆZt+1,
where ⊕is concatenation.
•A discriminator Dω(·)parameterized by ωthat discriminates agent demonstrations
from expert demonstrations DE.The input of Dωiszs⊕za.Dωis also called a cost
function. In the RL phase, Dωis the estimated cost function ( r=−log(Dω(zs⊕za))∈
R).
AsshowninAlgorithm1, theproposedmethodcomprisesthreemajorparts. InSection4.1,
we explain how to train the cost function Dωusing expert demonstrations DE(GAIL in
Algorithm 1). In Section 4.2, we describe how to use SSL in a non-image environment (REPR
in Algorithm 1). We implement trust region policy optimization (Schulman et al., 2015) to
train the agent policy πθby following the use in Ho and Ermon (2016) (TRPO in Algorithm
1).
4.1 Generative Adversarial Imitation Learning
Our method is based on generative adversarial imitation learning (GAIL) (Ho and Ermon,
2016). GAIL finds an optimal policy by matching an occupancy measure between expert
Eand the agent. The optimization equation of GAIL can be derived in the form of the
Jensen-Shannon divergence, which is equal to the minimax equation of generative adversarial
networks (Goodfellow et al., 2014). The minimax optimization of GAIL is expressed as
follows:
min
θmax
ωE
x∼Dπ[logDω(x)] + E
x∼DE[log(1−Dω(x))], (7)
where DπandDEare the corresponding demonstrations from an agent policy πθand expert
policy πE, respectively. In GAIL, the raw state and action are input to the discriminator.
8Sample-efficient Adversarial Imitation Learning
Algorithm 1 Sample-efficient Adversarial Imitation Learning
1:input: Expert demonstrations DE≜{xi}NE
i=1,#of batches B, Training epochs T.
2:fork←1 to Tdo ▷Section 4
3:Obtain trajectories Dk={xk,i}N
i=1using πθ
4: πθ←TRPO( πθ, V, D ω,Dk) ▷Ho and Ermon (2016)
5: SE, AE ←Repr(SE, AE, F, Dk)
6: Dω←Gail(Dω, SE, AE, Dk,DE)
7:end for
8:function Repr(SE, AE, F, Dk) ▷Section 4.2
9:forb←1 toBdo
10: Generate X′
bby Eq. 11 ▷Section 4.2.2 Corruption Method
11: Obtain ZbfromDk,busing ( SE, AE)
12: Obtain Z′
bfrom X′
busing ( SE, AE)
13: Update SE, AE, andFby Eq. 14
14:end for
15:return SE, AE ▷ Learning state and action representations
16:end function
17:function Gail(Dω, SE, AE, Dk,DE) ▷Section 4.1
18:forb←1 toBdo
19: Obtain ZbfromDk,busing ( SE, AE)
20: Update SE, AE, andDωby Eq. 8
21:end for
22:return Dω ▷Learning cost function
23:end function
For the proposed discriminator, state and action representations embedded by state and
action encoders, SE(·)andAE(·), are input. The discriminator is expressed as follows:
max
ωE
x∼Dπ[logDω(z)] + E
x∼DE[log(1−Dω(z))], (8)
where z=zs⊕za. Here, zsis a state representation embedded by SE(s), and zais an action
representation embedded by AE(a).
4.2 State and Action Representations
4.2.1 Modeling forward Dynamics
Each domain requires different ways of generating self-supervision depending on the properties
of the data. For example, BERT (Devlin et al., 2018) leverages a training signal by predicting
future words from previous words. For RL, the prediction error of a forward dynamics model
has been used as an intrinsic reward (Pathak et al., 2017; Schwarzer et al., 2020). In tabular
data, in contrast to data from an image, it is difficult to create a distorted version of an
original input without losing semantic information. Therefore, we posit that maximizing the
agreement between the distorted and original ones is more suitable for learning meaningful
features than maximizing the agreement between the two distorted versions of input in
9Jung, Lee, and Yoon
tabular data. We propose a method that generates a distorted version of the input to learn
or discard the desired features and their corresponding loss function for RL.
The proposed method uses the forward dynamics model to predict the distorted version
of the next state representation from the given current state and action representations.
First, the forward dynamics model is mathematically expressed as follows:
ˆzs
t+1=F(zs
t⊕za
t⊕ N), (9)
where zs
t=SE(st),za
t=AE(at)at a time step t, and N(0,1)is the Gaussian noise.
The output ˆzs
t+1represents a distorted version of the observed future representations zt+1.
The choice of a transformation controls what the representation learns. Thus, we apply a
distortion by concatenating Gaussian noise rather than using a corrupted state-action pair
because corruption cannot guarantee consistency in information with respect to temporality.
We use a contrastive loss for training. InfoNCE-based unsupervised contrastive learn-
ing (UCL) methods learn a feature representation by maximizing the agreement between
differently transformed same input while minimizing that of the rest of the input (negative
samples). The learned representation from UCL is invariant in unnecessary details; however,
it contains maximal information by maximizing a lower bound on the mutual information
between the two views (Wang and Isola, 2020; Poole et al., 2019). We utilize the InfoNCE
loss to obtain as many temporally informative features as possible. The proposed forward
dynamics model is trained to maximize the agreement between the distorted and observed
next state representations while minimizing that of the rest of the state representations. This
is expressed as follows:
LF=−E"
logecs(ˆzs
i,t+1,zs
i,t+1)/τ
ζ#
, ζ=BSX
j=11j̸=iecs(ˆzs
i,t+1,zs
j,t)/τ+BSX
j=11j̸=iecs(ˆzs
i,t+1,zs
j,t+1)/τ,
(10)
where (zs
t, zs
t+1,ˆzs
t+1)∼(Zs
t, Zs
t+1,ˆZs
t+1)and cs( u, v)=u⊤v/||u||||v||(jindexes the state
or next state representation in the batch, and BSis the batch size). Following SimCLR (Chen
et al., 2020b), we use the other 2(BS−1)representations in the batch as negative samples.
4.2.2 Corruption Method
Learning representations that can discard nuisance features is preferable to reduce sample
complexity. To help with this, we propose a corruption method that creates a distorted
sample showing diverse views that are possible in-distribution. The proposed method swaps
the input features of each state-action pair with the input features of the same indices
of another state-action pair in a batch. For a batch of state-action pairs sampled from
the current policy, Xb, we generate a corrupted version x′
ifor each state-action pair xi.
The corrupted versions of state sand action aare generated, respectively. However, for
convenience of rationalization, we explain the method based on the state-action pair x.
First, we make a copy of XbasXc
band permute Xc
bby changing the order of each
state-action pair in the batch at random, perm (Xc
b). Second, we sample some indices of
the state-action pair without replacement, I∈ {0, ..., dim (s⊕a)−1}q.qis the number of
features to corrupt ( =⌊c·dim(s⊕a)⌋), where cis a corruption rate ( c=cs+ca). Third,
we duplicate Ias a shape of N(Xb)×q. Subsequently, we generate corrupted state-action
10Sample-efficient Adversarial Imitation Learning
pairs X′of a given batch as follows:
Xc
b[I] =perm (Xc
b)[I],
X′
b=Xc
b,(11)
where X′
b=⟨S′
b×A′
b⟩. For convenience, we omit subscripts bhereafter. We refer to this
method as the swapping corruption method. Empirically, we observed superior performance
compared to existing methods on non-image control benchmarks.
Numerous discrepancy measures can quantify the similarity between corrupted and
original inputs. For the state, we maximize the similarity between the representation of the
distorted and observed versions by minimizing the mean squared error (MSE) as follows:
LSC=Ezs−zs′2
2, (12)
where (zs, zs′)∼(Zs, Zs′)andZs=SE(S)andZs′=SE(S′). For the state representation,
temporally predictive features should be embedded well to minimize LFsimultaneously. Thus,
we observed the best performance with MSE compared to other indirect discrepancy measures,
as shown in Tab 7. For the action representation, because the gradients from the LFare
not sufficient to hinder collapse, we use the Barlow twins loss (Zbontar et al., 2021) that
is robust against the constant embedding problem. It has been proven through connection
with mutual information that the Barlow twins loss also learns the feature representation
which is invariant to the distortion of the sample (Zbontar et al., 2021), similar to the MSE.
The Barlow twins loss draws the cross-correlation matrix close to the identity matrix. This
is expressed as follows:
LAC=X
i(1− Cii)2−X
iX
j̸=iC2
ij,Cij=Za
iZa′
j
p
(Za
i)2q
(Za′
i)2(13)
where Za=AE(A)andZa′=AE(A′).i, jindex the vector dimensions of the action
representations. Therefore, the Barlow twins loss prevents collapse by maximizing the
similarity between the representation of the distorted and observed versions of action and
reducing entanglement between the components of the representations. Ablations that can
give more intuitions about losses are given in Section 6. Consequently, the total loss for SSL
is computed as follows.
LSS=λFLF+λSLSC+λALAC, (14)
where λF,λS, and λAare hyperparameters for each loss.
5. Experiments
The efficacy of the proposed approach is assessed using MuJoCo (Todorov et al., 2012) and
Atari RAM of OpenAI Gym (Brockman et al., 2016), with each benchmark having fewer
than 100 expert state-action pairs. In scenarios with a limited number of perfect or imperfect
expert demonstrations, our method surpasses previous Adversarial Imitation Learning (AIL)
approaches by a significant margin. We assessed the performance of the proposed method on
11Jung, Lee, and Yoon
Table 1: Final performance using 100 expert state-action pairs on Ant-v2, HalfCheetah-v2,
and Walker2d-v2 of MuJoCo. Best results are in bold. The proposed method outperforms
existing IRL methods by a significant margin. It succeeds at imitating the expert’s behavior
on all three benchmarks using only 100 expert state-action pairs.
BC GAIL AIRL VAIL EAIRL SQIL ASAF Ours
Ant 932.2 ±171.7 4198.2 ±72.6 3922.9 ±210.7 4216.8 ±31.0 3137.5 ±424.8 -141.4 ±427.6 1015.6 ±107.04554.8 ±162.6
HalfCheetah 1875.2 ±1623.3 2034.6 ±2384.6 -214.1 ±45.2 -1012.8 ±497.1 6.6 ±15.0 -238.0 ±22.5 1187.6 ±1935.95416.0 ±203.8
Walker2d 535.5 ±134.4 3513.4 ±172.9 909.7 ±695.8 3466.7 ±109.0 2084.9 ±2499.7 283.3 ±26.5 192.9 ±58.53527.6 ±131.4
Average 1114.3 ±688.1 3248.8 ±876.7 1539.5 ±317.2 2223.6 ±212.4 1743.0 ±979.8 -32.0 ±158.9 798.7 ±700.44499.4 ±165.9
Table 2: Final performance when using the proposed corruption method (Swapping) and
existing methods ( NE= 100), and variance and predicted local outliers (Breunig et al., 2000)
of corrupted states. For measuring the outlier factor of corrupted states, we make use of 10
neighbors from observed states.
Ant Random Mean Each dim Swapping
Cumulative rewards 4263.7 ±243.9 4482.0 ±127.4 4459.3 ±128.44554.8 ±162.6
Variance ↑ 0.765±0.05 0.756 ±0.030.843±0.02 0.843 ±0.02
Predicted local outliers ( %)↓90%±8%6%±5% 26%±8% 11%±6%
five continuous control benchmarks simulated by MuJoCo (Ant-v2, HalfCheetah-v2, Hopper-
v2, Swimmer-v2, and Walker2d-v2) in four distinct settings: using expert demonstrations
of less than one full trajectory with the optimality of 25 %, 50%, 75%, or 100 %. We tested
the sample efficiency of the proposed method in a scenario where optimal demonstration
samples of less than one full trajectory are available ( ≤100). Expert demonstrations with
optimalities of 25 %, 50%, and 75 %represent imperfect demonstrations - a mixture of optimal
and non-optimal demonstrations. Imperfect demonstrations DIare sampled from a noisy
state-action density ρ, expressed as follows: DI={(sn, an)}NI
n=1i.i.d.∼ρ(s, a), where NIis the
number of state-action pairs from ρ. The noisy state-action density ρcan be expressed as
follows:
ρ(s, a) =ψρO(s, a) +nX
i=1viρi(s, a)
=ψρO(s, a) + (1 −ψ)ρN(s, a),(15)
where ρOis the state-action density of an expert, ρiis the state-action density of a single
non-expert, and nis the number of non-experts. Furthermore, ψ, satisfying 0< ψ < 1, is
an unknown mixing coefficient of the optimal and non-optimal state-action densities, and
ψ+Pn
i=1vi= 1; that is, an optimality of 25 %indicates that ψ=0.25. More details on the
experimental setting, definition, and hyperparameters can be found in Appendix B.
Optimality of 100 %First, we evaluated the proposed method with a small number
of perfect expert demonstrations. We compared the method with seven existing IL methods:
BC, GAIL, AIRL, VAIL, EAIRL, SQIL, and ASAF. Table 1 shows that the proposed method
outperforms other IL methods on all three benchmarks. Particularly, the proposed method
succeeded in perfectly imitating the expert policy on HalfCheetah. Notably, GAIL and VAIL
show higher performance than the recently proposed ASAF in 100 state-action pairs. For
f-GAIL, we conducted experiments using the official GitHub; however, it failed to converge in
12Sample-efficient Adversarial Imitation Learning
less than one full trajectory. We surmise that this is because a reasonable number of expert
state-action pairs must be guaranteed to automatically learn an appropriate discrepancy
measure for the given pairs.
We also tested our method with varying expert data sizes. As provided in Table 12
in Appendix D, there is a relatively small decrease in the performance up to NE= 20 on
all three benchmarks. However, when NE= 10, the performance is decreased by a large
margin. To make the experimental results stronger, we tested the reliability of the reported
average using IQM (Agarwal et al., 2021). We obtained IQM 4555.8 on Ant, IQM 5420.3 on
HalfCheetah, and IQM 3527.9 on Walker2d, which are very close to the reported average.
In addition, we tested the proposed method (without LAC) on two discrete control
benchmarks of OpenAI Gym (Brockman et al., 2016; BeamRider-ram-v0, and SpaceInvaders-
ram-v0). We observed that the cumulative rewards of the proposed method are superior to
those of the GAIL. For the results on the two discrete control benchmarks, please refer to
Appendix F. Moreover, the average cumulative rewards of the expert policy that we obtained
can be found in Appendix B.1.
Corruption method We showed the performance of the swapping corruption method
by comparing it with the existing two corruption methods and additionally, a mean corruption
method, which replaces the features with the empirical marginal distribution’s mean, in
cumulative rewards, variance, and local outlier score. Table 2 shows that the proposed
method shows a higher performance compared with the three corruption methods. We
observed that the proposed method generated transformed samples that provide more
diverse views compared with random and mean methods and comparably diverse views
compared with the method, obtaining each replaced feature from varying state-action
pairs (Each dim). We measured its diversity on the corrupted states using variance σ2:
1
1000P1000
i=1Pdim(s)
j=1(s′
i,j−¯s′
i,j)2
where ¯s′denotes the average of {s′
i}1000
i=1.
Also, one potential concern about using corruption as a transformation technique is that
the corrupted samples are out-of-distribution, resulting in performance degradation. To
evaluate this, we computed the local outlier factor (Breunig et al., 2000) of the corrupted
states. We computed the percentage of corrupted samples that are local outliers with respect
to the observed states. Table 2 shows that for the random method, the most corrupted states
are predicted as outliers, despite the low variance. For the mean method, the corrupted states
are mostly realistic; however, the diversity is lower compared with the swapping method,
which limits the effectiveness of data augmentation. Replacing each feature with a feature
from a different combination has diversity, but it also creates more out-of-distribution data.
The proposed swapping method showed high variance and a relatively low percentage of local
outliers. As a result, we empirically confirmed that for control tasks, it can be beneficial to
create meaningful in-distribution data in the corruption process.
Optimality of 25 %, 50%, or 75 %We tested the performance of the proposed
method in a more practical scenario with imperfect demonstrations. In this environment,
we combined the proposed method with 2IWIL (Wu et al., 2019). 2IWIL showed stable
training compared to other IL algorithms for imperfect demonstrations because they predict
the confidence of given demonstrations in a pre-stage. Please refer to Appendix B.2 for
detailed explanations about 2IWIL, other comparison methods, and the collected imperfect
demonstrations. The pseudo-code of the combined algorithm can be found in Appendix C.
13Jung, Lee, and Yoon
Figure 2: Final performance on five continuous control benchmarks with different optimality
rates ψ. Vertical axes denote cumulative rewards acquired during the last 1000 training
iterations. Shaded regions denote standard errors over three runs. Ours* = Ours + MM
Table 3: Final performance using 25 optimal and 75 non-optimal state-action pairs ( ψ=
0.25) to test improvement in sample efficiency.
Ablation Cumulative Rewards
MM Ours Ant HalfCheetah Walker2d
478.4±159.7 4573.1 ±194.4 1981.8 ±989.7
✓ 776.3±1805.1 4728.3 ±172.83436.0 ±15.8
✓3764.6 ±241.2 5217.4 ±29.7 3039.6 ±295.6
✓ ✓ 3966.0 ±26.2 5221.0 ±75.13403.0 ±52.5
Figure 2 shows the cumulative rewards on the five continuous control MuJoCo benchmarks
with different optimality rates. The proposed method combined with 2IWIL outperforms
the other six comparisons by a large margin in all optimality rates. Particularly, the relative
improvement over 2IWIL and CAIL (Zhang et al., 2021) on Ant is 288 %and 208 %on average,
respectively. However, we observed a decrease in the cumulative rewards when the noise rate
was 0.75 on Ant and Walker2d benchmarks. We surmise that this was caused by a deficiency
in the number of locomotion movements from the optimal policy. The degree of improvement
in performance is dependent on a number of given optimal demonstrations to some extent
because SSL methods create an auxiliary training signal by leveraging the given data.
Manifold mixup We applied a widely-used sample efficiency technique, manifold
mixup (MM) (Verma et al., 2019b), to the combined method. In previous studies (Mangla
et al., 2020; Lee et al., 2021), it is shown that MM in the feature space enriched by SSL is
further effective to improve performance. Many mixup-based augmentation studies have been
proposed to improve the generalization of image domains (Yun et al., 2019; Hendrycks et al.,
2019; Kim et al., 2020; Dabouei et al., 2021) and sequential domains (Chen et al., 2020a;
Guo et al., 2019). However, for RL, the application of Mixup is only applicable to image
environments (Wang et al., 2020; Singh et al., 2019). Through this experiment, we would like
14Sample-efficient Adversarial Imitation Learning
Table 4: Ablation studies using 100 expert state-action pairs to test the importance of LF
and (LSC,LAC). FD = Forward dynamics, and CR = Corruption.
Ablation Cumulative Rewards
LFLSC,LAC Ant HalfCheetah Walker2d
4198.2 ±72.5 2034.6 ±2384.6 3513.4 ±172.9
✓ 3329.6 ±513.0 2330.3 ±3741.3 3524.5 ±17.3
✓ 2244.0 ±208.6 591.9 ±549.9 1028.6 ±15.4
✓ ✓ 4554.7 ±162.5 5415.9 ±203.8 3527.5 ±131.3
Table 5: Ablation studies using 50 optimal and 50 non-optimal state-action pairs to test
sensitivity to corruption rate of state and action.
ca0.2 0.5 <= 0.5
cs0.1 0.2 0.3 0.4 <= 0.5 <= 0.5 <= 0.5
Ant4384.71 ±49.214282.89 ±170.53 4206.76 ±502.76 4127.30 ±451.87 3546.28 ±487.71 2425.96 ±65.69 560.64 ±326.97
to compare the efficiency of 1) MM, 2) the proposed method, and 3) using both as a sample
efficiency technique. MM increases the diversity of expert demonstrations by interpolating
the feature space output of the input data pair. We performed MM on the feature space
as follows: (¯z,¯y) = (Mix λ(zi, zj),Mix λ(yi, yj)), whereMix λ(a, b) =λ·a+ (1−λ)·b. Here,
(zi, zj) are the feature representations of ( xi, xj), and ( yi, yj) are the estimated confidence
by 2IWIL. Table 3 shows that, when only MM was used, the performance on Walker2d
improves; however, the performance on Ant and HalfCheetah does not show a reasonable
improvement. This indicates that it is difficult to naturally learn representations that are
temporally predictive and robust to diverse distortions from training signals generated by
synthetic data from MM. Conversely, when only the proposed method was used, we observed a
relatively small increase on Walker2d due to a deficiency in the number of optimal locomotion
movements. Consequently, as shown in Figure 2 and Table 3, we observed near-optimal
performance on all the benchmarks with varying optimalities when both ours and MM were
used.
6. Building Intuitions with Ablations
We conducted ablations on the proposed factors to provide an intuition of each role.
Importance of both LFand (LSC,LAC)Table 4 shows that LFand (LSC,LAC)
are complementary to each other. When only LFis added to GAIL, we observed an increase
on HalfCheetah and Walker2d, not on Ant, and the gap in increase is small. Tian et al. (2020)
demonstrated that it is not always good to learn maximal information using contrastive
learning. Rather, it is important to minimize nuisance information as much as possible
using a strong transformation to maximize task-specific information. However, a strong
transformation without damaging semantic information is impossible in continuous control
data. Thus, LFmainly plays the role of maximally learning temporally predictive information
with a weak, reasonable transformation, and ( LSC,LAC) helps to suppress the nuisance
factors using the corruption method so as to maximize task-relevant features in the proposed
method.
15Jung, Lee, and Yoon
Table 6: Ablation studies us-
ing 50 optimal and 50 non-
optimal state-action pairs
on Ant-v2 to test the role of
a loss function of LF.
LF
MSE 2793.48 ±98.59
BYOL 752.19 ±51.08
Barlow twins 3602.71 ±1164.86
SimCLR 4384.71 ±49.21Table7: Ablationstudiesusing50optimaland50non-optimal
state-action pairs on Ant-v2 to test the role of a loss function
ofLSCandLAC. BT = Barlow twins.
LSC
MSE BYOL SimSiam BT VICReg Avg.
LACMSE 3658.97 -627.01 1516.06 2891.03 701.55 1628.12
BYOL 2668.32 1240.80 1315.78 265.13 902.04 1278.41
SimSiam 2717.81 4017.71 3854.55 3656.74 2943.91 3438.14
BT 4384.71 2871.37 2590.40 3474.28 4269.20 3517.99
VICReg 1927.16 2363.05 4027.86 3768.11 3458.10 3108.86
Avg. 3071.40 1973.18 2660.93 2811.06 2454.96
Sensitivity to corruption rate Recently, studies on the importance of the degree of
transformation have been proposed in SSL. According to Tian et al. (2020), using previously
suggested transformation techniques without searching can lead to an increase in learning
nuisance information. Jing et al. (2021) showed that a strong transformation can cause
dimensional collapse. They showed that a greedy search for finding appropriate corruption
rates is important in applying SSL to RL benchmarks. Table 5 shows that the action is very
vulnerable to a high corruption rate. For the state, the performance is maintained to some
extent below 0.4. Additionally, we confirmed that fixing the corruption rate is better than
providing a range.
Loss function of LFFor the discriminator Dω, temporally predictive features are
important information to distinguish the imitator (agent) from the expert. However, learning
maximal information can lead to learning nuisance information as well. Transformation
techniques should be used to suppress this. For LF, we tested the concatenation of Gaussian
noise and the corrupted state as transformation techniques to generate the distorted version.
The concatenation of Gaussian noise yields better results. Qualitatively, there was a 7.5 %
relative improvement when using the concatenation of Gaussian noise. We surmise that
this is because corruption can unavoidably change important input features with respect
to temporality. Also, we confirmed that appending noise dimensions rather than no noise
concatenation increases the performance (Appendix E). For the loss function of LF, in
addition to the InfoNCE loss of SimCLR, we tested MSE, and MSE with the stop-gradient of
BYOL, and the Barlow twins loss. Table 6 presents that SimCLR’s performance is superior
to other methods by a significant margin.
Loss function of LSCandLACAs shown in Table 7, we tested various SSL loss
functions for both LSCandLAC. Notably, for LSC, the MSE loss that is exposed to the
collapsing problem shows the highest performance on average. This is because LFcannot
be minimized if the state representation is only the same constant. Rather, the MSE loss
that can flow the gradients to both the input pair shows a higher performance than the loss
functions using a stop-gradient or different discrepancy measures. For LAC, the Barlow twins
and SimSiam losses showed the first- and second-best performance on average, respectively.
Because the role of the state is greater than that of the action when predicting the next state,
the action representation zais not completely free from the collapsing problem. Therefore,
unlike the trend of LSC, clearly, the loss functions that have a certain technique to prevent
it showed stable performance.
16Sample-efficient Adversarial Imitation Learning
7. Conclusion
Sample efficiency of expert demonstrations is desirable in imitation learning because obtaining
a large number of expert demonstrations is often costly. Motivated by successes in self-
supervised learning, we proposed a sample-efficient imitation learning method that promotes
learning feature representations that are temporally predictive and robust against diverse
distortions for continuous control. We evaluated our proposed method in various control tasks
with limited expert demonstration settings and showed superior performance compared to
existing methods. We analyzed the efficiency of the proposed method through both theoretical
motivation and extensive experiments on continuous and discrete-control benchmarks.
Despite the excellent performance with limited settings, the proposed method has some
limitations. There is an increase in model complexity and additional computational cost
during training since three additional networks and self-supervised losses are added.
8. Acknowledgement
This work was supported by the National Research Foundation of Korea (NRF) grant
funded by the Korea government (MSIT) (2022R1A3B1077720), Institute of Information
& communications Technology Planning & Evaluation (IITP) grant funded by the Korea
government (MSIT) [2021-0-01343: Artificial Intelligence Graduate School Program (Seoul
National University) and 2022-0-00959] and the BK21 FOUR program of the Education and
Research Program for Future ICT Pioneers, Seoul National University in 2023.
References
Pieter Abbeel and Andrew Y Ng. Apprenticeship learning via inverse reinforcement learning.
InProceedings of the twenty-first international conference on Machine learning , page 1,
2004.
Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron C Courville, and Marc
Bellemare. Deep reinforcement learning at the edge of the statistical precipice. Advances
in Neural Information Processing Systems , 34, 2021.
Saurabh Arora and Prashant Doshi. A survey of inverse reinforcement learning: Challenges,
methods and progress. Artificial Intelligence , page 103500, 2021.
Dara Bahri, Heinrich Jiang, Yi Tay, and Donald Metzler. Scarf: Self-supervised contrastive
learning using random feature corruption. arXiv preprint arXiv:2106.15147 , 2021.
Paul Barde, Julien Roy, Wonseok Jeon, Joelle Pineau, Chris Pal, and Derek Nowrouzezahrai.
Adversarial soft advantage fitting: Imitation learning without policy optimization. In
NeurIPS , 2020.
Adrien Bardes, Jean Ponce, and Yann LeCun. Vicreg: Variance-invariance-covariance
regularization for self-supervised learning. arXiv preprint arXiv:2105.04906 , 2021.
Andreea Bobu, Yi Liu, Rohin Shah, Daniel S Brown, and Anca D Dragan. Sirl: Similarity-
based implicit representation learning. In Proceedings of the 2023 ACM/IEEE International
Conference on Human-Robot Interaction , pages 565–574, 2023.
17Jung, Lee, and Yoon
Markus M Breunig, Hans-Peter Kriegel, Raymond T Ng, and Jörg Sander. Lof: identifying
density-based local outliers. In Proceedings of the 2000 ACM SIGMOD international
conference on Management of data , pages 93–104, 2000.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie
Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540 , 2016.
Daniel Brown, Russell Coleman, Ravi Srinivasan, and Scott Niekum. Safe imitation learning
via fastbayesianreward inference from preferences. In International Conference on Machine
Learning , pages 1165–1177. PMLR, 2020.
Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand
Joulin. Unsupervised learning of visual features by contrasting cluster assignments. arXiv
preprint arXiv:2006.09882 , 2020.
Jiaao Chen, Zichao Yang, and Diyi Yang. Mixtext: Linguistically-informed interpolation
of hidden space for semi-supervised text classification. In Proceedings of the 58th Annual
Meeting of the Association for Computational Linguistics , pages 2147–2157, 2020a.
Minshuo Chen, Yizhou Wang, Tianyi Liu, Zhuoran Yang, Xingguo Li, Zhaoran Wang, and
Tuo Zhao. On computation and generalization of generative adversarial imitation learning.
InInternational Conference on Learning Representations , 2019.
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework
for contrastive learning of visual representations. In International conference on machine
learning, pages 1597–1607. PMLR, 2020b.
XinleiChenandKaimingHe. Exploringsimplesiameserepresentationlearning. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 15750–
15758, 2021.
Ali Dabouei, Sobhan Soleymani, Fariborz Taherkhani, and Nasser M Nasrabadi. Supermix:
Supervising the mixing data augmentation. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 13794–13803, 2021.
Tri Dao, Albert Gu, Alexander Ratner, Virginia Smith, Chris De Sa, and Christopher Ré.
A kernel theory of modern data augmentation. In International Conference on Machine
Learning , pages 1528–1537. PMLR, 2019.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-
training of deep bidirectional transformers for language understanding. arXiv preprint
arXiv:1810.04805 , 2018.
Justin Fu, Katie Luo, and Sergey Levine. Learning robust rewards with adverserial inverse
reinforcement learning. In International Conference on Learning Representations , 2018.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil
Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in
neural information processing systems , 27, 2014.
18Sample-efficient Adversarial Imitation Learning
Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena
Buchatskaya, Carl Doersch, Bernardo Pires, Zhaohan Guo, Mohammad Azar, et al.
Bootstrap your own latent: A new approach to self-supervised learning. In Neural
Information Processing Systems , 2020.
Hongyu Guo, Yongyi Mao, and Richong Zhang. Augmenting data with mixup for sentence
classification: An empirical study. arXiv preprint arXiv:1905.08941 , 2019.
Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control:
Learning behaviors by latent imagination. In International Conference on Learning
Representations , 2019.
Nicklas Hansen, Rishabh Jangir, Yu Sun, Guillem Alenyà, Pieter Abbeel, Alexei A Efros,
Lerrel Pinto, and Xiaolong Wang. Self-supervised policy adaptation during deployment.
InInternational Conference on Learning Representations , 2020.
Nicklas Hansen, Hao Su, and Xiaolong Wang. Stabilizing deep q-learning with convnets and
vision transformers under data augmentation. Advances in Neural Information Processing
Systems, 34, 2021.
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for
unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 9729–9738, 2020.
Zhuoxun He, Lingxi Xie, Xin Chen, Ya Zhang, Yanfeng Wang, and Qi Tian. Data augmenta-
tion revisited: Rethinking the distribution gap between clean and augmented data. arXiv
preprint arXiv:1909.09148 , 2019.
Marti A. Hearst, Susan T Dumais, Edgar Osuna, John Platt, and Bernhard Scholkopf.
Support vector machines. IEEE Intelligent Systems and their applications , 13(4):18–28,
1998.
Dan Hendrycks, Norman Mu, Ekin Dogus Cubuk, Barret Zoph, Justin Gilmer, and Balaji
Lakshminarayanan. Augmix: A simple data processing method to improve robustness and
uncertainty. In International Conference on Learning Representations , 2019.
Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. Advances in
neural information processing systems , 29:4565–4573, 2016.
Zehong Hu, Yitao Liang, Jie Zhang, Zhao Li, and Yang Liu. Inference aided reinforcement
learning for incentive mechanism design in crowdsourcing. Advances in Neural Information
Processing Systems , 31:5507–5517, 2018.
Li Jing, Pascal Vincent, Yann LeCun, and Yuandong Tian. Understanding dimensional
collapse in contrastive self-supervised learning. arXiv preprint arXiv:2110.09348 , 2021.
Kacper Piotr Kielak. Do recent advancements in model-based deep reinforcement learning
really improve data efficiency?, 2020.
19Jung, Lee, and Yoon
JangHyun Kim, Wonho Choo, Hosan Jeong, and Hyun Oh Song. Co-mixup: Saliency
guided joint mixup with supermodular diversity. In International Conference on Learning
Representations , 2020.
Misha Laskin, Kimin Lee, Adam Stooke, Lerrel Pinto, Pieter Abbeel, and Aravind Srinivas.
Reinforcement learning with augmented data. Advances in Neural Information Processing
Systems, 33, 2020.
Alex Lee, Anusha Nagabandi, Pieter Abbeel, and Sergey Levine. Stochastic latent actor-critic:
Deep reinforcement learning with a latent variable model. Advances in Neural Information
Processing Systems , 33, 2020.
Kibok Lee, Yian Zhu, Kihyuk Sohn, Chun-Liang Li, Jinwoo Shin, and Honglak Lee. i-mix:
A domain-agnostic strategy for contrastive representation learning. 2021.
Xiang Li, Jinghuan Shang, Srijan Das, and Michael S Ryoo. Does self-supervised learning
really improve reinforcement learning from pixels? In Alice H. Oh, Alekh Agarwal, Danielle
Belgrave, andKyunghyunCho, editors, Advances in Neural Information Processing Systems ,
2022.
Puneet Mangla, Nupur Kumari, Abhishek Sinha, Mayank Singh, Balaji Krishnamurthy, and
Vineeth N Balasubramanian. Charting the right manifold: Manifold mixup for few-shot
learning. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer
Vision, pages 2218–2227, 2020.
Bogdan Mazoure, Remi Tachet des Combes, Thang Long DOAN, Philip Bachman, and
R Devon Hjelm. Deep reinforcement and infomax learning. Advances in Neural Information
Processing Systems , 33, 2020.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word
representations in vector space. arXiv preprint arXiv:1301.3781 , 2013.
Shakir Mohamed and Danilo Jimenez Rezende. Variational information maximisation for
intrinsically motivated reinforcement learning. Advances in Neural Information Processing
Systems, 28:2125–2133, 2015.
Krikamol Muandet, Kenji Fukumizu, Francesco Dinuzzo, and Bernhard Schölkopf. Learning
from distributions via support measure machines. In Proceedings of the 25th International
Conference on Neural Information Processing Systems-Volume 1 , pages 10–18, 2012.
Mehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving
jigsaw puzzles. In European conference on computer vision , pages 69–84. Springer, 2016.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas
Kopf, EdwardYang, ZacharyDeVito, MartinRaison, AlykhanTejani, SasankChilamkurthy,
Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style,
high-performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer,
F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing
Systems 32 , pages 8024–8035. Curran Associates, Inc., 2019.
20Sample-efficient Adversarial Imitation Learning
Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven
exploration by self-supervised prediction. In International conference on machine learning ,
pages 2778–2787. PMLR, 2017.
Xue Bin Peng, Angjoo Kanazawa, Sam Toyer, Pieter Abbeel, and Sergey Levine. Varia-
tional discriminator bottleneck: Improving imitation learning, inverse RL, and GANs by
constraining information flow. In International Conference on Learning Representations ,
2019.
Dean A Pomerleau. Alvinn: An autonomous land vehicle in a neural network. Technical
report, CARNEGIE-MELLON UNIV PITTSBURGH PA ARTIFICIAL INTELLIGENCE
AND PSYCHOLOGY ..., 1989.
BenPoole, SherjilOzair, AaronVanDenOord, AlexAlemi, andGeorgeTucker. Onvariational
bounds of mutual information. In International Conference on Machine Learning , pages
5171–5180. PMLR, 2019.
Ahmed H Qureshi, Byron Boots, and Michael C Yip. Adversarial imitation via variational
inverse reinforcement learning. In International Conference on Learning Representations ,
2018.
Nathan D Ratliff, J Andrew Bagnell, and Martin A Zinkevich. Maximum margin planning.
InProceedings of the 23rd international conference on Machine learning , pages 729–736,
2006.
Siddharth Reddy, Anca D Dragan, and Sergey Levine. Sqil: Imitation learning via reinforce-
ment learning with sparse rewards. In International Conference on Learning Representa-
tions, 2019.
Stéphane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning
and structured prediction to no-regret online learning. In Proceedings of the fourteenth
international conference on artificial intelligence and statistics , pages 627–635. JMLR
Workshop and Conference Proceedings, 2011.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust
region policy optimization. In International conference on machine learning , pages 1889–
1897. PMLR, 2015.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal
policy optimization algorithms. arXiv preprint arXiv:1707.06347 , 2017.
Max Schwarzer, Ankesh Anand, Rishab Goel, R Devon Hjelm, Aaron Courville, and Philip
Bachman. Data-efficient reinforcement learning with self-predictive representations. In
International Conference on Learning Representations , 2020.
Riley Simmons-Edler, Ben Eisner, Daniel Yang, Anthony Bisulco, Eric Mitchell, Sebastian
Seung, and Daniel Lee. Reward prediction error as an exploration objective in deep rl.
arXiv preprint arXiv:1906.08189 , 2019.
21Jung, Lee, and Yoon
Avi Singh, Larry Yang, Kristian Hartikainen, Chelsea Finn, and Sergey Levine. End-
to-end robotic reinforcement learning without reward engineering. arXiv preprint
arXiv:1904.07854 , 2019.
Aravind Srinivas, Michael Laskin, and Pieter Abbeel. Curl: Contrastive unsupervised
representations for reinforcement learning. arXiv preprint arXiv:2004.04136 , 2020.
Richard S Sutton and Andrew G Barto. Introduction to reinforcement learning, 1998.
Voot Tangkaratt, Nontawat Charoenphakdee, and Masashi Sugiyama. Robust imitation
learning from noisy demonstrations. In International Conference on Artificial Intelligence
and Statistics , pages 298–306. PMLR, 2021.
Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, and Phillip Isola.
What makes for good views for contrastive learning? arXiv preprint arXiv:2005.10243 ,
2020.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based
control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems ,
pages 5026–5033. IEEE, 2012.
Faraz Torabi, Sean Geiger, Garrett Warnell, and Peter Stone. Sample-efficient adversarial
imitation learning from observation. arXiv preprint arXiv:1906.07374 , 2019.
Vladimir N Vapnik. An overview of statistical learning theory. IEEE transactions on neural
networks , 10(5):988–999, 1999.
Vikas Verma, Alex Lamb, Christopher Beckham, Amir Najafi, Ioannis Mitliagkas, David
Lopez-Paz, and Yoshua Bengio. Manifold mixup: Better representations by interpolating
hidden states. In International Conference on Machine Learning , pages 6438–6447. PMLR,
2019a.
Vikas Verma, Alex Lamb, Christopher Beckham, Amir Najafi, Ioannis Mitliagkas, David
Lopez-Paz, and Yoshua Bengio. Manifold mixup: Better representations by interpolating
hidden states. In International Conference on Machine Learning , pages 6438–6447. PMLR,
2019b.
Kaixin Wang, Bingyi Kang, Jie Shao, and Jiashi Feng. Improving generalization in reinforce-
ment learning with mixture regularization. In NeurIPS , 2020.
Tianyu Wang, Nikhil Karnwal, and Nikolay Atanasov. Latent policies for adversarial imitation
learning. arXiv preprint arXiv:2206.11299 , 2022.
Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through
alignment and uniformity on the hypersphere. In International Conference on Machine
Learning , pages 9929–9939. PMLR, 2020.
Keyu Wu, Min Wu, Zhenghua Chen, Yuecong Xu, and Xiaoli Li. Generalizing reinforcement
learning through fusing self-supervised learning into intrinsic motivation. In Proceedings
of the AAAI Conference on Artificial Intelligence , volume 36, pages 8683–8690, 2022.
22Sample-efficient Adversarial Imitation Learning
Yueh-Hua Wu, Nontawat Charoenphakdee, Han Bao, Voot Tangkaratt, and Masashi
Sugiyama. Imitation learning from imperfect demonstration. In International Conference
on Machine Learning , pages 6818–6827. PMLR, 2019.
Denis Yarats, Ilya Kostrikov, and Rob Fergus. Image augmentation is all you need: Regular-
izing deep reinforcement learning from pixels. In International Conference on Learning
Representations , 2020.
Jinsung Yoon, Yao Zhang, James Jordon, and Mihaela van der Schaar. Vime: Extending
the success of self-and semi-supervised learning to tabular domain. Advances in Neural
Information Processing Systems , 33, 2020.
Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon
Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features.
InProceedings of the IEEE/CVF International Conference on Computer Vision , pages
6023–6032, 2019.
Andrii Zadaianchuk, Georg Martius, and Fanny Yang. Self-supervised reinforcement learning
with independently controllable subgoals. In Conference on Robot Learning , pages 384–394.
PMLR, 2022.
Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and Stephane Deny. Barlow twins: Self-
supervised learning via redundancy reduction. In Marina Meila and Tong Zhang, editors,
Proceedings of the 38th International Conference on Machine Learning , volume 139 of
Proceedings of Machine Learning Research , pages 12310–12320. PMLR, 18–24 Jul 2021.
Songyuan Zhang, Zhangjie Cao, Dorsa Sadigh, and Yanan Sui. Confidence-aware imitation
learning from demonstrations with varying optimality. In Thirty-Fifth Conference on
Neural Information Processing Systems , 2021.
Xin Zhang, Yanhua Li, Ziming Zhang, and Zhi-Li Zhang. f-gail: Learning f-divergence
for generative adversarial imitation learning. Advances in neural information processing
systems, 2020.
Brian D Ziebart, Andrew Maas, J Andrew Bagnell, and Anind K Dey. Maximum entropy
inverse reinforcement learning. In Proceedings of the 23rd national conference on Artificial
intelligence-Volume 3 , pages 1433–1438, 2008a.
Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, Anind K Dey, et al. Maximum entropy
inverse reinforcement learning. In Aaai, volume 8, pages 1433–1438. Chicago, IL, USA,
2008b.
23Jung, Lee, and Yoon
Appendix A. Proof for Section 3
A.1 Proof of Corollary 1
Using the first-order Taylor approximation, we can expand ˆR(fhMSE|P).
Theorem 1 Based on Dao et al. (2019), picking up z0= E
zi∼hMSE(Xi)[zi], the first-order
approximation of ˆR(fhMSE|P)is1
MPM
i=1ℓ
W⊤E
zi∼hMSE(Xi)[zi], yi
. Then, the first-order
effect is that it averages the distorted features that are not necessarily present in the original
dataset P.
Proof. For proof, we define a kernel classifier Kto map a feature space. The kernel is
always positive definite, and its space is expressed through a dot product. The kernel defines
various functions, and its space of such functions is called the reproducing kernel Hilbert
space (RKHS).
When we define that T(x)is the probability density that xinherently contains a distortion
that is not necessarily present in the original dataset and a kernel K′represents a new feature
space via Eq. 4, we can have the following proof.
K′(x,˜x) =
Ex∼T(x)[hMSE(x)],E˜x∼T(˜x)[hMSE(˜x)]
=Z
x∈RnZ
˜x∈Rn⟨hMSE(x), hMSE(˜x)⟩T(x)T(˜x)d˜xdx
=Z
x∈RnZ
˜x∈RnK(x,˜x)T(x)T(˜x)d˜xdx
= (TKT⊤)(x,˜x),
where xand˜xare two inputs. It shows that the proposed approach can be interpreted as a
linear classifier Won the new kernel K′. This proof is similar to how a support measure
machine (SMM) trains the model with a mean function in the RKHS (Muandet et al., 2012).
SMM is proved to be invariant on the distorted features that are not necessarily present
in the original dataset P. In the newly learned feature space through Eq. 4, the distorted
features, which are inherently contained in the original dataset, become non-informative.
As a result, the sample complexity required to learn the classifier fhMSEis reduced. More
formally, a support vector machine (SVM) (Hearst et al., 1998) is a special case of SMM.
SMM is effective at seeking a halfspace that separates a training set by a large margin. In
other words, our proposed approach similar to SMM is also effective in inducing the low VC
dimension.
Corollary 1 hMSEis effective in reducing the VC dimension, however, the major problem
is that the upper bound of R(f|P)we seek to is based on ˆR(f|P)instead of ˆR(fhMSE|P). As
a result, fhMSEcannot capture all the properties preserved in ˆR(f|P). Additionally, due to
the distribution gap between the feature spaces handhMSE, the optimal function f∗
hMSEof
ˆR(fhMSE|P)is not guaranteed to be a minimum of ˆR(f|P).
He et al. (2019) showed that the optimal function f∗ofˆR(f|P)leverages both class-
relevant and -irrelevant features ( wd,i≈wd,jfor1≤d≤Dand1≤i, j≤C). However, when
24Sample-efficient Adversarial Imitation Learning
finding f∗
hMSE, distorted, commonly class-irrelevant, features are restrained to be encoded
through Eq. 4. Thus, the optimal function f∗
hMSEofˆR(fhMSE|P)is not guaranteed to be a
minimum of ˆR(f|P)due to a distribution shift.
A.2 Proof of Claim 1
Suppose that the MSE (Eq. 4) and contrastive learning share its feature space H.
Definition 1 (Wang and Isola, 2020) Consider a perfectly trained h∗
MSEwith unsupervised
contrastive learning. A positive pair has similar features on HMSE(Alignment), and the
feature space HMSEbecomes maximally informative (Uniformity).
Definition 2 (Poole et al., 2019) Contrastive loss is defined using a critic function that
approximates density ratios (p(x|y)
p(x)=p(y|x)
p(y))of two random variables XandY. It is proved
that contrastive loss is a lower bound on mutual information (MI), I(X;Y). By minimizing
the loss, the lower bound on MI is maximized.
Unsupervised contrastive learning can be interpreted as an InfoMax principle, as shown
in Definitions 1 and 2. In our terms, minimizing contrastive loss is maximizing the lower
bound on I(hMSE(x);hMSE(x′))for a positive pair (x, x′)∼ P pos. MI between hMSE(x)and
hMSE(x′)can be re-written as follows:
I(hMSE(x);hMSE(x′)) =H(hMSE(x))−H(hMSE(x)|hMSE(x′)).
The first term of the right-hand side (RHS) shows that uniformity in Definition 1 is mathe-
matically favored by entropy H(hMSE(x)). The second term of RHS shows that alignment in
Definition 1 is mathematically favored by conditional entropy H(hMSE(x)|hMSE(x′)). As a
result, to maximally increase the closeness between all positive pairs in the feature space,
all informative features should be semantically embedded and utilized. That is, mapping
more informative features induces that more similar inputs have more similar features in
H. Through that, informative yet class-irrelevant features are also preserved for contrastive
learning.
Theorem 2 Based on Dao et al. (2019), with ψ= E
z∼hMSE(X)[z]andVar=E
(X−E[X])2
,
the second-order approximation of ˆR(fhMSE|P)is the first-order approximation plus variance
regularization of weight vectors wdfor1≤d≤D.
Proof.For cross entropy loss with softmax, ℓ′′is independent of yand positive semi-definite.
We can obtain a more exact expression for ˆR(fhMSE|P)by the second-order approximation
of the Taylor expansion.
25Jung, Lee, and Yoon
ˆR(fhMSE|P) =1
MMX
i=1ℓ 
W⊤ψi, yi
+1
2MMX
i=1E
zi∼hMSE (Xi)h 
W⊤(zi−ψi)2ℓ′′(W⊤zi, yi)i
=1
MMX
i=1ℓ 
W⊤ψi, yi
+
W⊤ 
1
2MMX
i=1E
zi∼hMSE (Xi)
∆i∆⊤
iℓ′′(W⊤zi, yi)!
W (∵∆i=zi−ψi)
=1
MMX
i=1ℓ 
W⊤ψi, yi
+
1
2MMX
i=1W⊤E
zi∼hMSE (Xi)
∆i∆⊤
i
ℓ′′(W⊤ψi)W(∵ℓ′′is independent of y)
The second-order approximation of ˆR(fhMSE|P)can be interpreted as the first-order
approximation plus variance regularization of weight vectors wdfor1≤d≤Dbecause ∆∆⊤
is equal to the variance of z. It represents that the weights of features having large variances
are forced to 0 restricted by regularization. We do not utilize data augmentation in the
classification loss because it further induces the generalization gap. When the feature space
is learned through only Eq. 4 and the classification loss, most class-irrelevant features cannot
be encoded in the feature manifold. We tackle this problem by mapping class-irrelevant yet
informative features learned from contrastive learning to the feature manifold. For contrastive
learning, the positive pair, which is created by two differently augmented same images, should
have very similar feature representation. It means informative features mapped through this
loss are robust to the variance via augmentation.
Claim 1 Class-irrelevant yet informative features are preserved via unsupervised contrastive
learning. It reduces the inconsistency between ˆR(fhMSE|P)andˆR(f|P).
Class-irrelevant yet informative features are preserved via unsupervised contrastive
learning. In other words, the feature manifold is enriched by unsupervised contrastive learning,
that is, both class-relevant and class-irrelevant yet informative features can be leveraged when
finding the optimal function f∗
hMSEofˆR(fhMSE|P). As a result, the inconsistency between
ˆR(fhMSE|P)andˆR(f|P)can be reduced.
Appendix B. Additional Implementation and Experimental Details
We use 5 continuous control benchmarks on Mujoco (Todorov et al., 2012) (Ant-v2, Hopper-
v2, HalfCheetah-v2, Swimmer-v2, and Walker2d-v2), and 2 discrete control benchmarks on
Atari RAM (BeamRider-ram-v0, and SpaceInvaders-ram-v0) of OpenAI Gym (Brockman
et al., 2016).
Overall, we reported the mean and standard error of the performance over 3 trials.
For experimental settings, we used GTX 1080 Ti for GPUs, Intel i7-6850K for CPUs, and
Ubuntu 18.04 for OS. The usage of GPU memory is approximately 3000MB for training,
26Sample-efficient Adversarial Imitation Learning
and training time for tested benchmarks was approximately 20 hours. Our code is based on
Pytorch (Paszke et al., 2019) and python libraries.
We make use of the same neural net architecture and hyperparameters for all benchmarks.
For the policy network, value network, discriminator, and state encoder, we use 3 hidden
layers with size 100 and Tanh as activation functions. For the action encoder, we use 6 1D
convolutional layers with sizes (64, 64, 64, 128, 256, and 256), 1 hidden layer with size 8 as
the output, and LeakyReLU as activation functions. For the forward dynamics model, we
use 1 hidden layer with size 114 and ReLU as activation functions.
For hyperparameters in all runs, the total epoch for Swimmer, and Hopper is 3,000,
for BeamRider, SpaceInvaders, HalfCheetah, and Walker2d is 5,000, and for Ant is 8,000.
Please refer to Table 8 for other hyperparameters. We set λF= 1,λS= 100, and λA= 1for
matching loss scale.
Table 8: Base hyperparameters used for all benchmarks.
Hyperparameters Value
γ 0.995
Generalized advantage estimation 0.97
N 5,000
Learning rate (all networks except for value network) 1e-3
Learning rate (value network) 3e-4
Batch size (RERP) 256
Batch size (TRPO) 128
Batch size (GAIL) 5,000
Optimizer (all networks) Adam
τ 0.1
λF 1
λS 100
λA 1
For the random corruption method (Yoon et al., 2020), we sampled an imputing value from
N(0,1)by considering the range of state values. The mean value used for imputation (Bahri
et al., 2021) is the mean vector of each Dkwhere kis the training iteration. We imputed
with a mean value of the corresponding dimension of the calculated mean vector.
B.1 Optimality of 100 %(Expert Demonstrations)
To train experts, we used a reinforcement learning algorithm, proximal policy optimization
(Schulman et al., 2017), uploaded in the official Github by the authors of CAIL (Zhang et al.,
2021). We selected the converged policy as the expert policy. The performance of the utilized
expert policy and other specifications related to the experiments on the main manuscript are
given in Table 9.
B.2 Optimality of 25 %, 50%, or 75 %(A Mixture of Optimal and Non-optimal
Demonstrations)
We also tested the effectiveness of the proposed method with imperfect demonstrations.
We combined the proposed method with the existing method for imperfect demonstrations
27Jung, Lee, and Yoon
Table 9: Specification and the number of used demonstrations of each continuous control
benchmark in the scenario of perfect expert demonstrations.
Benchmarks dim(S) dim( A)NEExpert’s Performance
HalfCheetah-v2 R17R6100 5455.49 ±74.26
Walker-v2 R17R6100 3685.27 ±57.99
Ant-v2 R111R8100 4787.23 ±115.72
and checked improvement in performance. We compared our combined method against the
following baselines: BC (Pomerleau, 1989), GAIL (Ho and Ermon, 2016), IC-GAIL (Wu
et al., 2019), 2IWIL (Wu et al., 2019), RIL-CO (Tangkaratt et al., 2021), and CAIL (Zhang
et al., 2021).
IL methods, especially variants of BC, require a large volume of expert demonstration
data for training (Ross et al., 2011). These methods struggle with a generalization problem
when using a small number of demonstrations. Empirically, we observed that BC almost
fails to converge on Ant, Walker2d, and Hopper with a small number of demonstrations. For
RIL-CO, they measure and optimize a classification risk with the symmetric loss. Basically,
they only assumed a scenario that the majority of demonstrations are obtained using an
optimal policy. IC-GAIL and 2IWIL were proposed in the same paper. Both methods are
confidence-based. In the case of 2IWIL, the confidence of each state-action pair is estimated
using the classification risk before training, and in the case of IC-GAIL, they implicitly utilize
the confidence score in a way of matching the occupancy measure of the imitator with the
expert. CAIL is the state-of-the-art work in IL algorithms for imperfect demonstrations.
They jointly learn the confidence score and policy using an outer loss. Because they update
two factors jointly, the training is unstable. Experimentally, there was no benchmark that
CAIL, which is most recently suggested, is superior to 2IWIL or RIL-CO with a small number
of imperfect demonstrations. We surmise that this is because CAIL is the method using a
full trajectory rather than each state-action pair when estimating the confidence score of
each pair. Consequently, we decided to combine our method with 2IWIL instead of CAIL.
Table 10: Specification and the number of used demonstrations of each continuous control
benchmark in the scenario of imperfect expert demonstrations.
Benchmarks dim(S) dim( A)NESuboptimal 1 Suboptimal 2 Suboptimal 3 Suboptimal 4 Expert’s Performance
HalfCheetah-v2 R17R61001051.91 ±50.17 2280.87 ±651.92 3533.07 ±79.47 4682.89 ±54.33 5455.49 ±74.26
Walker-v2 R17R6100691.14 ±96.12 1617.02 ±721.00 2579.41 ±512.19 2819.63 ±609.49 3685.27 ±57.99
Ant-v2 R111R8100789.13 ±170.45 2115.17 ±328.20 2947.49 ±191.72 3739.91 ±96.56 4787.23 ±115.72
Swimmer-v2 R8R22065.56±18.93 148.20 ±8.09 181.27 ±4.23 228.29 ±5.95 280.5 ±1.24
Hopper-v2 R11R3201262.34 ±296.32 1774.65 ±462.52 2185.33 ±996.92 2802.18 ±489.85 3531.03 ±23.51
For the experiments on the main manuscript, we collected a mixture of optimal and
non-optimal demonstrations with different optimalities. For collecting the imperfect demon-
strations, we used the official Github by the authors of CAIL. Following CAIL, We selected
four intermediate policies as sub-optimal policies and the converged policy as the optimal
policy. The performance of sub-optimal and optimal policies and other specifications for the
benchmarks are given in Table 10. Empirically, as shown in Table 10, because the dimension
28Sample-efficient Adversarial Imitation Learning
of the action space of Swimmer-v2 and Hopper-v2 is very small almost like discrete control,
we did not apply LAC.
The pseudo-code of the combined methods is given in Algorithms 2 and 3. To run the
combined methods, we need additional hyperparameters and their values are summarized in
Table 11.
Table 11: Additional hyperparameters for the combined methods.
Hyperparameters Value
αfor Mixup 4.0
Threshold for GMM 0.5
The number of non-experts 4
Ratio of labeled demonstrations 0.4
Appendix C. Pseudo-code of Combined Method
C.1 Combined Case #1: Ours + 2IWIL
Because labeling all state-action pairs from ρOorρNcan be expensive, following previous IL
works for imperfect demonstrations, we assumed that only a few demonstrations are labeled.
Then, the imperfect demonstrations are split into two demonstrations: a set of labeled
demonstrations DL={(xl,i, yl,i)}NL
i=1and a set of unlabeled demonstrations DU={xu,i}NU
i=1,
where NLandNUare the number of labeled demonstrations and unlabeled demonstrations,
respectively.
As shown in Algorithm 2, before training, 2IWIL (Wu et al., 2019) estimates pseudo labels
ˆyu,iofDUto utilize the set of unlabeled demonstrations DUwithDLusing the classification
risk proposed in their work as follows:
Rℓ(g)=Ex, y∼ Dl[y(ℓ(g(x))−ℓ(−g(x))) + (1 −β)ℓ(−g(x))] +Ex∼ Du[βℓ(−g(x))],(16)
where β=NU
NL+NU,g(·)is a neural network classifier, and ℓis a strictly proper composite loss.
Then, the predicted confidence score ˆyurepresents the probability that a given state-action
pair is optimal. The estimated confidence of each state-action pair is utilized as a sample
weight in their discriminator loss. As a result, the combined discriminator loss with our state
and action encoders is defined as follows:
max
ωE
x∼Dπ[logDω(z)] + E
(x,y)∼(˜DO∪˜DN)hy
ϵlog(1−Dω(z))i
, (17)
where ϵ=1
NLPNL
i=1yi,z=zs⊕za.zsis a state representation embedded by SE(s), and za
is an action representation embedded by AE(a).
C.2 Combined Case #2: Ours + 2IWIL + Manifold Mixup
For utilizing Manifold mixup (MM), we modeled the per-sample confidence score distribution
of(yl,ˆyu)with a two-component Gaussian mixture model to divide the imperfect demonstra-
tionsDIinto optimal demonstrations and non-optimal demonstrations, ˜DOand ˜DN. The
29Jung, Lee, and Yoon
Algorithm 2 Pseudo-code of Ours + 2IWIL (Wu et al., 2019)
1:input: Imperfect expert demonstrations DI={DL∪ DU}, Labeled demonstrations
DL≜{(xl,i, yl,i)}NL
i=1, Unlabeleddemonstrations DU≜{xu,i}NU
i=1,#ofbatchesB,Training
epochs T.
2:Train a probabilistic classifier by minimizing Eq. 16
3:Predict confidence scores {ˆyu,i}Nu
i=1for{xu,i}Nu
i=1
4:fork←1 to Tdo
5:Obtain trajectories Dk={xk,i}N
i=1using πθ
6: πθ←TRPO( πθ, V, D ω,Dk)
7: SE, AE ←Repr(SE, AE, F, Dk)
8: Dω←Gail(Dω, SE, AE, Dk,DI)
9:end for
10:function Repr(SE, AE, F, Dk)
11:forb←1 toBdo
12: Generate X′
bby Eq. 8
13: Obtain ZbfromDk,busing ( SE, AE)
14: Obtain Z′
bfrom X′
busing ( SE, AE)
15: Update SE, AE, andFby Eq. 14
16:end for
17:return SE, AE
18:end function
19:function Gail(Dω, SE, AE, Dk,DI)
20:forb←1 toBdo
21: Obtain ZbfromDk,busing ( SE, AE)
22: Update SE, AE, andDωby Eq. 17
23:end for
24:return Dω
25:end function
feature representation zo∼˜DOis interpolated with the feature representation zn∼˜DN.
More formally, for a batch of features (zo, zn)and corresponding confidence scores (yo, yn),
the mixed (¯z,¯y)can be computed by:
λ∼Beta(α, α), λ′=max(λ,1−λ),
¯z=λ′·zo+ (1−λ′)·zn,
¯y=λ′·yo+ (1−λ′)·yn,(18)
where z=zs⊕za.zsis a state representation embedded by SE(s),zais an action represen-
tation embedded by AE(a). Eq. 18 can ensure that ¯zare closer to optimal demonstrations
than non-optimal demonstrations.
30Sample-efficient Adversarial Imitation Learning
Algorithm 3 Pseudo-code of Ours + 2IWIL + Manifold Mixup (Verma et al., 2019a)
1:input: Imperfect expert demonstrations DI={DL∪ DU}, Labeled demonstrations
DL≜{(xl,i, yl,i)}NL
i=1, Unlabeleddemonstrations DU≜{xu,i}NU
i=1,#ofbatchesB,Training
epochs T.
2:Train a probabilistic classifier by minimizing Eq. 16
3:Predict confidence scores {ˆyu,i}NU
i=1for{xu,i}NU
i=1
4:˜DO,˜DN←GMM (DL,(DU,{ˆyu,i}NU
i=1))
5:fork←1 to Tdo
6:Obtain trajectories Dk={xk,i}N
i=1using πθ
7: πθ←TRPO( πθ, V, D ω,Dk)
8: SE, AE ←Repr(SE, AE, F, Dk)
9: Dω←Gail(Dω, SE, AE, Dk,˜DO,˜DN)
10:end for
11:function Repr(SE, AE, F, Dk)
12:forb←1 toBdo
13: Generate X′
bby Eq. 8
14: Obtain ZbfromDk,busing ( SE, AE)
15: Obtain Z′
bfrom X′
busing ( SE, AE)
16: Update SE, AE, andFby Eq. 14
17:end for
18:return SE, AE
19:end function
20:function Gail(Dω, SE, AE, Dk,˜DO,˜DN)
21:forb←1 toBdo
22: Obtain ZbfromDk,busing ( SE, AE)
23: Obtain ( Zo,Zn) from ( ˜DO,˜DN) using ( SE, AE)
24: Compute (¯Z,¯Y)by Eq. 18
25: Update SE, AE, andDωby Eq. 19
26:end for
27:return Dω
28:end function
As shown in Algorithm 3, by additionally including synthetic data through MM, the
discriminator loss is expressed as follows:
max
ωE
x∼Dπ[logDω(z)] + E
(x,y)∼(˜DO∪˜DN)hy
ϵlog(1−Dω(z))i
+
E
(¯z,¯y)∼(˜DO∪˜DN)[log((1−¯y)·Dω(¯z) + ¯y·(1−Dω(¯z)))].(19)
Appendix D. Expert Data Size
We assessed our method with varying expert data sizes. As shown in the table, there is a
relatively small or no decrease in the performance up to NE= 20. When NEis reduced
31Jung, Lee, and Yoon
to 20 for Ant and Walker2d and 10 for HalfCheetah, the performance is comparable to the
baseline AIL.
Table 12: Additional studies using 10, 20, or 50 expert state-action pairs on Ant-v2,
HalfCheetah-v2, and Walker2d-v2 of MuJoCo.
GAIL Ours
NE= 100 NE= 10 NE= 20 NE= 50 NE= 100
Ant 4198.2 ±72.6 2855.9 ±1139.7 4286.4 ±125.6 4432.4 ±41.1 4554.8 ±162.6
HalfCheetah 2034.6 ±2384.6 2787.6 ±3454.4 5381.6 ±81.3 5410.6 ±44.2 5416.0 ±203.8
Walker2d 3513.4 ±172.9 3023.1 ±439.6 3412.6 ±195.6 3520.9 ±108.5 3527.6 ±131.4
Appendix E. Role of Gaussian Noise
Appending noise dimensions increases the performance on all three tasks as shown in Table 13.
-71.09, -81.29, and –87.90 represent the decrease without appending the noise dimensions.
Table 13: Ablation studies on appending Gaussian noise using 100 expert state-action pairs
on Ant-v2, HalfCheetah-v2, and Walker2d-v2 of MuJoCo.
Ours Ant HalfCheetah Walker2d
w/o noise 4483.7 ±159.6 (-71.09) 5334.7 ±43.0 (-81.29) 3439.7 ±122.2 (-87.90)
w noise 4554.8 ±162.6 5416.0 ±203.8 3527.6 ±131.4
Appendix F. Discrete control Benchmarks
To test the scalability of the proposed method, we evaluated the performance of the proposed
method on 2 discrete control benchmarks: BeamRider-ram-v0, and SpaceInvaders-ram-v0 of
OpenAI Gym.
Table 14: Final performance using 20 expert state-action pairs on BeamRider-ram-v0, and
SpaceInvaders-ram-v0 of OpenAI Gym. Best results are in bold.
BeamRider SpaceInvaders
GAIL Ours GAIL Ours
399.43 ±55.63433.04 ±76.15166.39 ±107.88289.87 ±5.37
For comparison, we chose GAIL, which showed the second-best performance in the
experiments of continuous control benchmarks. To apply the proposed method on discrete
control benchmarks, we ignored the action encoder AEand its corresponding loss LACof
the proposed model. Nevertheless, as shown in Table 14, the proposed method still showed
better performance compared to GAIL on discrete control benchmarks.
32