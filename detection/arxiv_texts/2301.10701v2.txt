arXiv:2301.10701v2  [math.PR]  26 Jan 2023DISTRIBUTION OF THE THRESHOLD FOR THE SYMMETRIC
PERCEPTRON
ASHWIN SAH AND MEHTAAB SAWHNEY
Abstract. We derive an explicit distribution for the threshold sequen ce of the symmetric binary
perceptron with Gaussian disorder, proving that the critic al window is of constant width.
1.Introduction
We ﬁx a real number κ >0throughout the rest of this paper.
Deﬁnition 1.1. LetΣn={±1}n. Given an inﬁnite sequence M= (Mj)j≥1of vectors in Rn, let
Sj(M) :={x∈Σn:|/an}⌊ra⌋k⌉tl⌉{tx,Mj/an}⌊ra⌋k⌉tri}ht| ≤κ√n}, Sm(M) :=m/intersectiondisplay
j=1Sj(M).
Furthermore, deﬁne the threshold τ=τ(M)as the ﬁrst index such that
Sτ−1(M)/n⌉}ationslash=∅andSτ(M) =∅,
or+∞if it does not exist. Let G= (Gj)j≥1be a sequence of independent standard n-dimensional
Gaussian vectors, i.e., Gj∼ N(0,In). The threshold for the symmetric binary perceptron with
Gaussian disorder isτ=τ(G)(noteτ(G)<+∞almost surely).
We note that the threshold as deﬁned in Deﬁnition 1.1 is up to rescaling equivalent to the storage
capacity of the binary perceptron model. We have chosen to work with Ga ussian disorder as it
marginally simpliﬁes certain computations. The limiting d istribution can immediately be extended
to Rademacher disorder, although various quantitative err or terms will diﬀer and the mean and
variance of an associated log-normal distribution Z∗must be slightly changed.
We will require the following basic special functions.
Deﬁnition 1.2. LetZ,Z1,Z2∼ N(0,1)be independent standard Gaussians. Let p=P[|Z| ≤κ]
and deﬁne the critical value
αc:=−log2/logp.
We also deﬁne
µ2=E[Z2/BD|Z|≤κ]
p, β=−√αc
2(1−µ2).
Finally we deﬁne the pair probability function
q(γ) =P/bracketleftbig
|√γZ1+/radicalbig
1−γZ2| ≤κ∧|√γZ1−/radicalbig
1−γZ2| ≤κ/bracketrightbig
.
We note that −1/2< β <0follows from [ 3, (3.8)]. We now state our main theorem.
Theorem 1.3. LetGandτ=τ(G)be as in Deﬁnition 1.1 . There exists θ >0such that the
following holds for all suﬃciently large n. LetZ∗∼ N(1
4log(1−4β2),−1
2log(1−4β2))and choose
kwithk+αcn∈Z. We have/vextendsingle/vextendsingle/vextendsingle/vextendsingleP[τ≤k+αcn]−E/bracketleftbigg
exp/parenleftbigg−eZ∗pk
2/parenrightbigg/bracketrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤n−θ.
Sah and Sawhney were supported by NSF Graduate Research Fell owship Program DGE-1745302. Sah was sup-
ported by the PD Soros Fellowship. Sawhney was supported by t he Churchill Foundation.
1Remark 1.4.The polynomial rate speciﬁed by Theorem 1.3 is essentially the limit of our method.
Combining the statement of Theorem 1.3 with the tails bounds given by Altschuler [ 3, Theorem 2],
one can prove that there exists Csuch that for any ﬁxed q≥1andnlarge in terms of q:
E|τ−αcn|q≤Cq.
We also note that Theorem 1.3 disproves [ 3, Conjecture 1] since the upper tail P[τ≥k+αcn]is of
exponential type in k >0.
1.1.Previous work. A toy model for neural networks, the (asymmetric) perceptro n model has
been a model of substantial interest, in part stemming from c onjectures on the asymptotic threshold
due to Krauth and Mezard [ 13]. Despite progress on a number of related problems, progres s on the
perceptron has proved considerably more diﬃcult. In this di rection, groundbreaking work of Ding
and Sun [ 8] establishes the conjectural lower bound for the threshold and recent work of Xu [ 19]
has established an analog of Friedgut’s sharp threshold the orem [ 11] for such models (see work of
Nakajima and Sun [ 15] for an alternate proof). In particular these sharp thresho ld results establish
that for general perceptron models, τtypically lives within an interval of length o(n).
The symmetric perceptron model, introduced in work of Aubin , Perkins, and Zdeborová [ 4], has
proven to be considerably more tractable at a mathematical l evel due the availability of the moment
method applied directly to the number of solutions (while fo r the asymmetric perceptron model a
substantially more involved conditioning scheme is requir ed). It provides a test-bed for various con-
jectural phenomena since the solution space geometries of s ymmetric and asymmetric perceptron are
expected to behave similarly. In particular, work of Aubin, Perkins, and Zdeborová [ 4] established
that w.h.p. τ <(αc+ǫ)nand with positive probability that τ >(αc−ǫ)n. The determination of
the threshold (with scaling window o(n)) atαcnwith high probability was proven independently by
Abbe, Li, and Sly [ 2] and Perkins and Xu [ 17]1. Both the works of Abbe, Li, and Sly [ 2] and Perkins
and Xu [ 17] establish that log|Sm(G)|concentrates around logE[|Sm(G)|]. Furthermore, the work
of Abbe, Li, and Sly [ 2] establishes that Sm(G)has a log-normal distribution (when mis linearly
bounded away from the threshold) via a modiﬁcation of “small subgraph conditioning” made suitable
for dense models. Finally, in recent work Altschuler [ 3] substantially improved the concentration
window by proving that the ﬁrst and second moment of number of solutions at ⌈αcn⌉match up
to a constant. Adapting the argument for concentration give n by Perkins and Xu [ 17], Altschuler
derived that the window of concentration is O(logn)w.h.p. We refer the reader to [ 3, Section 2]
for a more extensive discussion of previous works on the thre shold, discussion of threshold for more
general constraint satisfaction problems, and related wor k.
We also note that there has recently been work on understandi ng eﬃcient algorithms for the
symmetric perceptron; in particular work of Abbe, Li, and Sl y [1] proves that eﬃcient algorithms can
ﬁnd exponentially rare clusters at suﬃciently low density a nd work of Gamarnik, Kızıldağ, Perkins,
and Xu [ 12] examines the limits of eﬃcient algorithms for the symmetri c perceptron through the
lens of the multi-Overlap Gap Property.
1.2.Stopping time and Poisson sampling heuristic. The key ingredient our in work is an early
stopping time argument, which combined with an understandi ng of the geometry of the remaining
solutions is suﬃcient to derive a distribution for the thres hold. More precisely, deﬁne the (non-
random) cutoﬀ time τpre=⌊αcn−ηlogn⌋for some suﬃciently small constant η >0. By mimicking
the techniques of Abbe, Li, and Sly [ 2] (for Gaussian disorder), one can prove that the number of
solutions to the perceptron at τprefollows a log-normal distribution; we note here that the fac t that
the second and ﬁrst moments match so close to the threshold is a critical insight found in the work
of Altschuler [ 3]. Furthermore, the moment computations of Altschuler (in p articular the “frozen”
1We note that the work of Aubin, Perkins, and Zdeborová [ 4] and Perkins and Xu [ 17] was conditional on a
numerical hypothesis which was veriﬁed in the work of Abbe, L i, and Sly [ 2].
2nature of typical solutions) can be used to prove that all pai rs of solutions are essentially orthogonal
(e.g. have dot product bounded in magnitude by n1/2logn).
The crucial idea at this point is to note that since w.h.p. at τprethere are a small polynomial
number of solutions, the pairwise dot product of solutions b eing near zero can be translated to
near-independence of the event of whether these solutions w ill survive the next slices. In particular,
the total variation distance between the true distribution and a natural product distribution is
polynomially small. The desired result then follows noting that there are only logarithmically more
steps and thus the computation of the threshold sequence red uces to a computation with product
distributions: we have some set Sof solutions at time τpreand each solution is retained for t−τpre
extra slices with probability pt−τpre, so an independence heuristic suggests that P[Bin(|S|,pt−τpre) =
0]is roughly the chance that we empty the solution set in tsteps, where Bindenotes a binomial
random variable. In our regime, this can be approximated via a Poissonian heuristic that gives rise
to the expression in Theorem 1.3 (after incorporating the fact that |S|/E|S|satisﬁes a log-normal
distribution). We note that the strange factor of 2arising in the distribution in Theorem 1.3 stems
from the fact that v∈Sif and only if −v∈S.
We note that in implementing the above sketch in order to give polynomial error rates, the
primary diﬃculty is quantifying the work of Abbe, Li, and Sly [2] (and adjusting it to both handle
when the number of hyperplanes is near the threshold and to al low for Gaussian disorder). In
order to provide a quantitative rate we transform the necess ary cycle count distributional claims
under conditional models to distributional questions abou t unconditioned Gaussian polynomials (via
orthogonal invariance of Gaussians and various expectatio n and variance computations) and then
provide quantitative rates via Stein’s method (in paricula r through the use of exchangeable pairs),
rather than the method of moments used in [ 2]. We note one curious feature is that the distribution of
solutions at τpre, while still being log-normal, does not have the same mean an d variance parameters
as the Rademacher case derived in [ 2]. The diﬀerence ultimately arises from the fact that if M∼
N(0,1), thenM2has nontrivial variance whereas M2is constant if M∼Unif({±1}). (This
diﬀerence manifests in the need to track the degenerate 2-cycle count corresponding to k= 1in
Deﬁnition 3.2 ; we note that the k= 1expression in the Rademacher case would be deterministic,
hence the diﬀerence.)
We note that the methods of this paper appear to more generall y applicable; in particular, our
approach can likely be adapted give a precise characterizat ion of the threshold for solutions in
constrained k-XORSAT which was shown to have an O(1)-size scaling window by Pittel and Sorkin
[18] (see also [ 7,9]). We intend to return to this subject in future work.
Organization. The remainder of the paper is organized as follows. In Section 2 , we prove Theorem 1.3
conditional on the “strong freezing” of the solution space a nd on the log-normality of the number
of solutions ( Proposition 2.1 ). This argument implements the main thrust of the paper outl ined
inSection 1.2 . We reduce Proposition 2.1 to a concrete moment computation ( Lemma 2.7 ) in
Section 2.1 . InSection 3 , we state preliminaries used to verify the moment computati on including
a suitable quantiﬁcation of cycle count convergence as deﬁn ed in the work of Abbe, Li, and Sly [ 2].
Finally, in Section 4 , we prove the necessary moment estimates to imply Proposition 2.1 , which in
turn closely follows the work of Altschuler [ 3] and Abbe, Li, and Sly [ 2].
Acknowledgements. We thank Will Perkins for bringing this problem to our attent ion and point-
ing out the work of Pittel and Sorkin [ 18], and for various useful comments on the draft. We also
thank Mark Sellke for ﬁnding a number of typos and Michael Ren for useful conversations.
Notation. All dependences of notation or asymptotics on κhave been suppressed, and we treat it
as a ﬁxed absolute constant throughout the paper. We write f=O(g)to mean that f≤Cgfor
some absolute constant C, andg= Ω(f)andf/lessorsimilargto mean the same. We write f=o(g)if for all
3c >0we have f≤cgonce the implicit growing parameter (typically n) grows large enough, and
g=ω(f)means the same. Furthermore throughout this paper all logar ithms are base e.
For positive semideﬁnite Σ∈Rd×dwe letN(0,Σ)be the Gaussian vector with covariance matrix
Σ. A common choice is Σ =Id, the standard Gaussian vector with identity covariance. Fo r a
discrete random variable Xwith nonzero probabilities p1,...,pmfor its atoms, its entropy is
H(X) =−m/summationdisplay
i=1pilogpi.
The Kullback–Leibler divergence between X,Y deﬁned on the same atom set with probabilities
qi,rirespectively is
KL(X/⌊ar⌈⌊lY) =m/summationdisplay
i=1qilogqi
ri
and we deﬁne the real function H(x) =H(Ber(x)) =−xlogx−(1−x)log(1−x). IfX,Y are
supported on Rdwith probability densities q,rtheir KL divergence is
KL(q/⌊ar⌈⌊lr) =/integraldisplay
Rdq(x)logq(x)
r(x)dx.
For real matrices AandB, let
/an}⌊ra⌋k⌉tl⌉{tA,B/an}⌊ra⌋k⌉tri}ht= tr(ATB),/⌊ar⌈⌊lA/⌊ar⌈⌊lHS=/radicalBig
tr(ATA) =/radicalbig
/an}⌊ra⌋k⌉tl⌉{tA,A/an}⌊ra⌋k⌉tri}ht.
Furthermore for an order ktensor deﬁne
/⌊ar⌈⌊lA/⌊ar⌈⌊lop= sup
|vi|=1|A(v1,v2,...,vk)|.
Finally for f∈ Ck(Rn)deﬁne the kthderivative (tensor) operators evaluated at u1,...,u k∈Rnas
/an}⌊ra⌋k⌉tl⌉{tDkf(x),(u1,...,u k)/an}⌊ra⌋k⌉tri}ht=/summationdisplay
i1,i2,...,ik∈[n]∂kf
∂xi1...∂xik(u1)i1...(uk)ik
and deﬁne
Mr(g) = sup
x∈Rn/⌊ar⌈⌊lDrg(x)/⌊ar⌈⌊lop.
For a matrix Adeﬁne/⌊ar⌈⌊lA/⌊ar⌈⌊l2
HS=/summationtext
i,j|aij|2.
2.Proof of Theorem 1.3
In order to derive our main result we will assume the followin g pair of structural properties of the
solution space and its size. Here and beyond, we ﬁx some η >0that will be chosen small in terms
of onlyκat the end of the argument. For now, we leave it unspeciﬁed in t he various statements
that appear, merely using that it is suﬃciently small in the p roofs.
Proposition 2.1. There exists γ >0such that the following holds. Let τpre=⌊αcn−ηlogn⌋.
Fornsuﬃciently large, we have the following pair of estimates on the size of the solution space
X(G) :=|Sτpre(G)|fornsuﬃciently large.
1. With probability at least 1−n−1there do not exist x1,x2∈Sτpre(G)such that |/an}⌊ra⌋k⌉tl⌉{tx1,x2/an}⌊ra⌋k⌉tri}ht| ∈
[n1/2logn,n−1].
2. LetZ∗=N(1
4log(1−4β2),−1
2log(1−4β2)). Then for all t∈R,
P/bracketleftbiggX(G)
EX(G)≥et/bracketrightbigg
=P[Z∗≥t]±n−γ.
We will also require the following tail bounds on the number o f solutions. The second appears
explicitly as [ 3, Theorem 3] which in turns builds closely on work of Perkins a nd Xu [ 17].
4Proposition 2.2 ([3, Theorem 3]) .We have the following.
1. For all t≥0we have
P[|St(G)| ≥1]≤2npt.
2. There exists a constant c=c2.2>0such that following holds. Fix H≥c−1. Suppose that
nis suﬃciently large in terms of H. Then
P[S⌊αcn−Hlogn⌋(G) =∅]≤exp(−cHlogn).
We require the following basic bound on the total variation d istance between a pair of normal
distributions.
Lemma 2.3. For positive deﬁnite symmetric matrices Σ1,Σ2∈Rd×d, we have
2TV(N(0,Σ1),N(0,Σ2))2≤KL(N(0,Σ1)/⌊ar⌈⌊l N(0,Σ2)) =1
2/parenleftbig
log(det(Σ 2Σ−1
1))−d+tr(Σ−1
2Σ1)/parenrightbig
.
Proof. The ﬁrst inequality is by Pinsker’s inequality (see e.g. [ 6, Lemma 2]), holding for general
distributions. The equality is a straightforward Gaussian integral (see e.g. [ 10, Page 13]). /square
Now we prove Theorem 1.3 given these inputs.
Proof of Theorem 1.3 .Letτpre,ηandX(G)be as in Proposition 2.1 . Let
E1={X(G)≥E[X(G)]·exp((log n)3/4)}∪{X(G)≤E[X(G)]·exp(−(logn)3/4)}
and
E2={|/an}⌊ra⌋k⌉tl⌉{tx1,x2/an}⌊ra⌋k⌉tri}ht| ∈[n1/2logn,n−1] :x1,x2∈Sτpre}.
We now reveal set S=Sτpre(G)∩{x:x1= 1}and note that E1,E2, andX(G)are deterministic given
Sτpre(G). We note that Proposition 2.1 implies that P[E1∪ E2]≤n−Ω(1)(since|Z∗| ≤(logn)3/4
occurs with super-polynomially good probability). We cond ition on a revelation of Ssuch that
Ec
1∩Ec
2holds.
We now consider Gtfort∈(τpre,τpre+2ηlogn]and consider the distribution induced by
{/an}⌊ra⌋k⌉tl⌉{tGt,xi/an}⌊ra⌋k⌉tri}ht/√n:xi∈S,t∈(τpre,τpre+2ηlogn]}.
Note that Gtare independent for t∈(τpre,τpre+2ηlogn]. LetMdenote a X(G)/2bynmatrix
where each row corresponds to an entry of S. Notice that for each t∈(τpre,τpre+⌈2ηlogn⌉]we
have that
{/an}⌊ra⌋k⌉tl⌉{tGt,xi/an}⌊ra⌋k⌉tri}ht/√n:xi∈S} ∼ N(0,MMT/n).
The crucial point is that since MMT/nis suﬃciently diagonally dominant we will be able to prove
that the distribution is close in total variation distance t oN(0,I|S|). To prove this, note that
/⌊ar⌈⌊lMMT/n−I|S|/⌊ar⌈⌊l2
op≤ /⌊ar⌈⌊lMMT/n−I|S|/⌊ar⌈⌊l2
F≤ |S|2max
x1,x2∈S
x1/\⌉}atio\slash=x2|/an}⌊ra⌋k⌉tl⌉{tx1,x2/an}⌊ra⌋k⌉tri}ht|2/n2
≤ |S|2·n−3/4≤n−1/2
due toEc
2holding and the fact that Sdoes not simultaneously contain any pair {x,−x}.
This immediately implies that all the eigenvalues of MMT/nare of the form 1±n−1/4and
therefore by Lemma 2.3 we have that
TV(N(0,I|S|),N(0,MMT/n))≤(Tr(MMT/n)−|S|+log(det( MMT/n)))/2
≤ |S|n−1/4+|S|log(1+n−1/4)≤n−1/5.
The last inequality comes from |S|=X(G)/2≤EX(G)·exp((log n)3/4)≤nO(η)by the choice of
τpre, and assuming ηis small enough.
5As we are restricting attention to t∈(τpre,τpre+⌈2ηlogn⌉]we have that
TV(({/an}⌊ra⌋k⌉tl⌉{tGt,xi/an}⌊ra⌋k⌉tri}ht/√n:xi∈S})t∈(τpre,τpre+⌈2ηlogn⌉],N(0,I|S|)⊗⌈2ηlogn⌉))≤n−1/6.
SinceSsatisﬁesEc
1∪Ec
2, this implies that for t∈(τpre,τpre+⌈2ηlogn⌉]we have
P[St=∅|Sτpre] = (1−pt−τpre)X(G)/2±n−1/6.
We now choose a constant ρ >0which is suﬃciently small with respect to η. We ﬁrst isolate the
case where t∈[⌈αcn−ρlogn⌉,⌈αcn+ρlogn⌉]. Notice that we therefore have that
P[St=∅] =P[St=∅∩(Ec
1∩Ec
2)]±P[E1∪E2]
=P[St=∅∩(Ec
1∩Ec
2)]±n−Ω(1)
=E[(1−pt−τpre)X(G)/2/BDEc
1∩Ec
2]±n−Ω(1)
=E/bracketleftbigg
exp/parenleftbigg−pt−τpre·X(G)
2/parenrightbigg/BDEc
1∩Ec
2/bracketrightbigg
±n−Ω(1).
The ﬁnal equality requires justiﬁcation: note that (1−p)t−τpre)X(G)≤1,1−x=e−x+O(x2)for all
|x| ≤1/2, and
exp(p2(t−τpre)·X(G))≤1+n−Ω(1)
by using that Ec
1holds and some basic computation.
LetZ∗be as in Proposition 2.1 . This implies that
E/bracketleftbigg
exp/parenleftbigg−pt−τpre·X(G)
2/parenrightbigg/BDEc
1∩Ec
2/bracketrightbigg
=/integraldisplay1
0P/bracketleftbigg
exp/parenleftbigg−pt−τpre·X(G)
2/parenrightbigg/BDEc
1∩Ec
2≥u/bracketrightbigg
du
=/integraldisplay1
0P/bracketleftbigg
exp/parenleftbigg−eZ∗E[X(G)]pt−τpre
2/parenrightbigg
≥u/bracketrightbigg
du±n−Ω(1)
=E/bracketleftbigg
exp/parenleftbigg−eZ∗E[X(G)]pt−τpre
2/parenrightbigg/bracketrightbigg
±n−Ω(1)
=E[exp(−eZ∗2n−1pt)]±n−Ω(1).
This implies the desired result for tin the speciﬁed interval.
Fort≤αcn−γlogn, we have that
P[|St|=∅]≤P[|S⌊αcn−γlogn⌋|=∅]≤n−Ω(1),
using what we have established for t=⌊αcn−γlogn⌋and explicitly computing Eexp(e−Z∗2n−1pt),
which implies the desired result in this range. Finally for t≥αcn+γlogn, we have that
P[|St| /n⌉}ationslash=∅]≤E[|St|]≤p(γlogn)/2≤n−Ω(1)
fromProposition 2.2 (1) which immediately implies the desired result in this ran ge. This ﬁnishes
the proof. /square
We also prove the remark following Theorem 1.3 .
Proof of Remark 1.4 .Fixq≥1. Letτ∗be the random variable such that P[τ∗≤k+αcn] =
E/bracketleftbig
exp/parenleftbig
−eZ∗pk
2/parenrightbig/bracketrightbig
for all choices of k+αcn∈Z. Taking H= 2qc−1and applying Proposition 2.2 (1,2),
we have that
E|τ−αcn|q≤E[|τ−αcn|q/BD|τ−αcn|≤Hlogn]+E[|τ−αcn|q/BD|τ−αcn|≥Hlogn]
≤E[|τ−αcn|q/BD|τ−αcn|≤Hlogn]+(2αcn)qexp(−2qlogn)+E[τq/BDτ≥2αcn]
≤E[|τ−αcn|q/BD|τ−αcn|≤Hlogn]+n−q/2
≤E[|τ∗−αcn|q/BD|τ−αcn|≤Hlogn]+n−Ω(1)
6≤E|τ∗−αcn|q+n−Ω(1)
≤Cq
Note that bounds derived on the upper tail for τare an immediate consequence of Proposition 2.2 (1)
and the ﬁnal inequality follows from noting that τ∗has subexponential tails by direct calculation. /square
2.1.Reduction to normalized second moment computation. We now formally state the
second moment computation that we perform in the remainder o f the paper and use these results to
prove Proposition 2.1 . We ﬁrst deﬁne the key random variables used for small graph c onditioning
used in the deﬁnition of the moments.
Deﬁnition 2.4. The2k-cycle count of a sequence of vectors M= (Mj)j≥1is
Ck(m,M) =/parenleftbigg1√n/parenrightbiggk/parenleftbigg1√m/parenrightbiggk/summationdisplay
i1,...,ik∈[n]distinct
j1,...,jk∈[m]distinctk/productdisplay
i=1Mjℓ,iℓMjℓ,iℓ+1− /BDk=1√mn,
where we let ik+1=i1. Then the K-weighted cycle count is
YK(m,M) =K/summationdisplay
k=12(2β)kCk(m,M)−(2β)2k
4k.
Remark 2.5.Notice that for k= 1, this is a shifted sum of squares of Gaussians. Such an expres sion
is trivial in the Rademacher setting and hence is not require d in the work of Abbe, Li, and Sly [ 2].
We also deﬁne certain planted conditional distributions th at are key in computing the moments.
Deﬁnition 2.6. Fort∈[−n,n]witht≡n(mod 2) deﬁnevt= (1,...,1,−1,...,−1)∈Rnwhere
there are (n+t)/2many1s. Let∆0∼ N(0,1)⊗n. Let∆1bex∼∆0conditional on the event
{|/an}⌊ra⌋k⌉tl⌉{tx,vn/an}⌊ra⌋k⌉tri}ht| ≤κ√n}. Let∆2(t)bex∼∆1conditional on the further event {|/an}⌊ra⌋k⌉tl⌉{tx,vt/an}⌊ra⌋k⌉tri}ht| ≤κ√n}. Then
letG(0)=G, which has its vectors drawn from ∆0, and let G(1)= (G(1)
j)j≥1have vectors drawn
independently from ∆1andG(2)(t) = (G(2)
j(t))j≥1have vectors drawn independently from ∆2(t).
We have the following weighted moment estimate.
Lemma 2.7. There exists γ >0such that the following holds. Let m=τpre=⌊αcn−ηlogn⌋and
supposenis large. With L=ηlogn,K=⌈ηlogn⌉, andδ=n−γwe have
E/bracketleftbiggX(G)
exp(YK(m,G) /BD[YK(m,G)≥ −L])/bracketrightbigg
≥(1−δ)EX(G) (2.1)
E/bracketleftbiggX(G)2
exp(2YK(m,G) /BD[YK(m,G)≥ −L])/bracketrightbigg
≤(1+δ)EX(G)2. (2.2)
Additionally, we have
/summationdisplay
t∈[−(n−1),−n1/2logn]∪[n1/2logn,n−1]2n/parenleftbiggn
(n+t)/2/parenrightbigg
P[vn,vt∈Sτpre(G)]≤exp(−(logn)3/2).(2.3)
To prove this moment estimate we will use Bayes’ theorem, requ iring an understanding of the
L-weighted cycle count when the vectors of Mare drawn from the planted conditional distributions.
Lemma 2.8. Letm=τpre=⌊αcn−ηlogn⌋,K=⌈ηlogn⌉, and|t| ≤√nlogn. We deﬁne
V(M) :=/parenleftbiggC1(m,M)√
2·1,...,CK(m,M)√
2K/parenrightbigg
, µ:=/parenleftbigg(2β)1
√
2·1,...,(2β)K
√
2K/parenrightbigg
.
7Forg∈ C3(RK)andnsuﬃciently large we have
|E[g(V(G(0)))]−EZ∼N(0,IK))[g(Z)]| ≤(M1(g)+M2(g)+M3(g))n−1/5, (2.4)
|E[g(V(G(1))−µ)]−EZ∼N(0,IK))[g(Z)]| ≤(M1(g)+M2(g)+M3(g))n−1/5, (2.5)
|E[g(V(G(2)(t))−2µ)]−EZ∼N(0,IK))[g(Z)]| ≤(M1(g)+M2(g)+M3(g))n−1/5, (2.6)
Now we prove Proposition 2.1 .
Proof of Proposition 2.1 .We prove each of the items of Proposition 2.1 in turn. For the ﬁrst item
notice by linearity of expectation that
E[|{x1,x2∈Sτpre(G):|/an}⌊ra⌋k⌉tl⌉{tx1,x2/an}⌊ra⌋k⌉tri}ht| ∈[−(n−1),−n2/3]∪[n2/3,n−1]}|]
=/summationdisplay
t∈[−(n−1),−n1/2logn]∪[n1/2logn,n−1]2n/parenleftbiggn
(n+t)/2/parenrightbigg
P[vn,vt∈Sτpre(G)]
≤exp(−(logn)3/2)
by using (2.3). The ﬁrst item then follows immediately by Markov’s inequal ity.
For the second item, let /tildewideX(G) =X(G)
exp(YK(m,G) /BD[YK(m,G)≥−L]). By(2.1)and(2.2)we have that
E/bracketleftbigg/parenleftbigg/tildewideX(G)
E[X(G)]−1/parenrightbigg2/bracketrightbigg
≤3n−γ.
Therefore with probability 1−n−Ω(1)we have that /tildewideX(G) = (1±n−Ω(1))E[X(G)]. Equivalently this
implies that with probability 1−n−Ω(1),
X(G)/E[X(G)] = (1±n−Ω(1))exp(YK(m,G) /BD[YK(m,G)≥ −L]).
The result then follows from Lemma 2.8 :Yk(m,G)is a function of the coordinates of V(G), which
is close in test function distance to N(0,IK). Therefore, the distribution of X(G)/EX(G)ought to
look like an exponential of an appropriate Gaussian with mea n/summationtextK
k=1−(2β)2k
4k≈(1/4)log(1−4β2)
and variance/summationtextK
k=1/parenleftbigg
(2β)k
√
2k/parenrightbigg2
≈ −(1/2)log(1−4β2)(these terms come from the linear expression
ofYK(m,G)in terms of V(G)).
To make this quantitative, we choose appropriate bump funct ionsgin(2.4)and use the fact that
Gaussian distributions are appropriately anticoncentrat ed; we omit the routine justiﬁcation (see
e.g. [5, Lemma 7.1]; see also Section 4.1 for a similar computation). /square
3.Preliminaries for moment computations
3.1.Estimates on Gaussian pair probabilities. We will require the following special function
estimates on the pair point probabilities derived in the wor k of Abbe, Li, and Sly [ 2] and in work
of Altschuler [ 3]. The ﬁrst bullet is equivalent to the ﬁrst bullet of [ 3, Lemma 3.1], the next three
bullets appear exactly as in [ 3, Lemma 3.1] (these were originally established in [ 2, Section 4.7]),
the ﬁfth appears as [ 3, (4.6)], and the sixth appears as [ 3, (4.3)].
Lemma 3.1 (From [ 3, Lemma 3.1], [ 3, (4.6)], [ 3, (4.3)]) .There exists ε=ε3.1>0such that the
following hold. Let F(γ) :=H(γ)+αclogq(γ).
S1q′′(1/2) =8κ2e−κ2
π
S2F′′(1/2)≤ −ε.
S3F(γ)is decreasing for γ∈[0,ε].
S4For any 0≤a≤b≤1/2,
max
β∈[a,b]F(γ) = max{F(a),F(b),F(1/2)−ε}.
8S5
F(1/n)≤F(0)−ε/√n.
S6
(1−µ2)p=/radicalbigg
2
πκe−κ2/2
3.2.Quantiﬁcation of Gaussian convergence. In order to prove the required Gaussian conver-
gence of the random variables YK, we will prove Lemma 2.8 in this section, essentially following the
approach of Abbe, Li, and Sly [ 2, Lemma 3.2]. We note their argument is written for Rademache r
disorder, and additionally in order to prove Remark 1.4 , we will need to adapt various arguments in
order to prove a stronger convergence result. To do so, we pro ceed indirectly via a linear algebraic
change using the Gaussian structure. Speciﬁcally, we will ﬁ rst show that in a sense, sampling the
vectors of Mfrom the conditional distributions ∆1or∆2(t)for|t|small (i.e., setting M=G(1)or
M=G(2)(t)) is very similar to sampling from ∆0=N(0,In)(i.e.,M=G(0)=G) up to a mean
shift. Then we will show the necessary multivariate central limit theorem for ∆0using multidimen-
sional Stein’s method machinery adapted from Meckes [ 14]. As our argument essentially serves as a
more eﬀective version of the bounds given in [ 2, Lemma 3.2] in the Gaussian case, we will be brief
with certain basic calculations.
To reduce to ∆0, we introduce a slightly more general cycle count which allo ws us to plug in
diﬀerent conditional distributions.
Deﬁnition 3.2. GivenMand−n < t < n witht≡n(mod 2) , we deﬁne
σj,i(t,M) =1
(n+t)/2(n+t)/2/summationdisplay
i′=1Mj,i′, /tildewiderMj,i(t) =Mj,i−σj,i(t,M),for1≤i≤n+t
2,
σj,i(t,M) =1
(n−t)/2n/summationdisplay
i′=(n+t)/2+1Mj,i′,/tildewiderMj,i(t) =Mj,i−σj,i(t,M),forn+t
2< i≤n.
If we are further given a sequence of Rnvectorsσ= (σj)j≥1, we deﬁne
/tildewideCk(m,t,M,σ ) =1
(mn)k/2/summationdisplay
i1,...,ik∈[n]distinct
j1,...,jk∈[m]distinctk/productdisplay
ℓ=1/parenleftbigg
/tildewiderMjℓ,iℓ(t)+σjℓ,iℓ/parenrightbigg/parenleftbigg
/tildewiderMjℓ,iℓ+1(t)+σjℓ,iℓ+1/parenrightbigg
− /BDk=1√mn.
The key point of this deﬁnition is that
Ck(m,M) =/tildewideCk(m,t,M,σ (t,M))
and the distributions of /tildewiderM(t)andσ(t,M)are independent in all relevant cases (namely M=
G(0),G(1),G(2)(t)). Furthermore, across all relevant cases we have that the di stribution of /tildewiderM(t)is
identical, so the distributions of Ck(m,M)and/tildewideCk(m,t,G,σ (t,M))are identical. Thus it suﬃces
to understand the diﬀerences in the models via the parameter sσ, which merely encode the average
of the ﬁrst (n+t)/2and last (n−t)/2elements of each vector. Therefore, we prove the following,
which essentially shows the resulting statistics V(M)are the same up to shifting of means.
Lemma 3.3. Letm=τpre=⌊αcn−ηlogn⌋andK=⌈ηlogn⌉. Given 1≤k≤Kand|t| ≤√nlogn, we have
E/bracketleftbig/tildewideCk(m,t,G,σ (t,G(1)))−Ck(m,G)/bracketrightbig
= (1+/tildewideO(n−1))(2β)k, (3.1)
E/bracketleftbig/tildewideCk(m,t,G,σ (t,G(2)(t)))−Ck(m,G)/bracketrightbig
= (1+/tildewideO(n−1/4))2·(2β)k, (3.2)
Var/bracketleftbig/tildewideCk(m,t,G,σ (t,G(1)))−Ck(m,G)/bracketrightbig
=O(n−5/6Var[Ck(m,G)]), (3.3)
Var/bracketleftbig/tildewideCk(m,t,G,σ (t,G(2)(t)))−Ck(m,G)/bracketrightbig
=O(n−5/6Var[Ck(m,G)]). (3.4)
9Proof. We deﬁne
ρ(0)
1=σ1,1(t,G(0)),ρ(0)
2=σ1,n(t,G(0))
and similar for G(1),G(2)(t). Note that σ1,ifor alli∈ {1,...,(n+t)/2}will give exactly ρ1
andσ1,ifor alli∈ {(n+t)/2 + 1,...,n}will give exactly ρ2. Furthermore, as we vary jin
σj,i, we do not have the exact same value but we have an independent value that is drawn from
the same distribution. For instance, note that ρ(0)
1= (G1,1+···+G1,(n+t)/2)/((n+t)/2)and
ρ(0)
2some Gaussians, while (ρ(1)
1,ρ(1)
2)have the distribution of (x,y) = (ρ(0)
1,ρ(0)
2)conditioned on
|(n+t)x/2+(n−t)y/2| ≤κ√n, and(ρ(2)
1,ρ(2)
2)have the distribution of (x,y) = (ρ(1)
1,ρ(1)
2)further
conditioned on |(n+t)x/2−(n−t)y/2| ≤κ√n.
We ﬁrst note that
Eρ(b)
1=Eρ(b)
2= 0 (3.5)
for allb∈ {0,1,2}, and
Eρ(b)
1ρ(b)
2= 0 (3.6)
forb∈ {0,2}. This can be seen from symmetry. Note that b= 1does have some covariance.
Additionally, we have
E(ρ(0)
1)2=1
(n+t)/2,E(ρ(0)
2)2=1
(n−t)/2.
Finally, we can compute
E(ρ(2)
1)2= (1+/tildewideO(n−1/4))2µ2
n+t,E(ρ(2)
2)2= (1+/tildewideO(n−1/4))2µ2
n−t,
using that the distribution of (ρ(2)
1,ρ(2)
2)is that of (ρ(0)
1,ρ(0)
2), which are independent Gaussians,
conditional on two explicit linear inequalities. (The erro r term comes from the possible deviation
int; ift= 0these are easily seen to be precise equalities.)
We ﬁrst consider (3.2)and(3.4). For the expectation, note that G,G(2)(t)are independent and
G,G(2)(t)are mean 0(the latter since we are in the symmetric perceptron), so for k >1
E[/tildewideCk(m,t,G,σ (t,G(2)(t)))−Ck(m,G)] =E[/tildewideCk(m,t,G,σ (t,G(2)(t)))]
=1
(mn)k/2/summationdisplay
i1,...,ik∈[n]distinct
j1,...,jk∈[m]distinctE/bracketleftbiggk/productdisplay
ℓ=1/parenleftbig/tildewideGjℓ,iℓ+σjℓ,iℓ(t,G(2)(t))/parenrightbig/parenleftbig/tildewideGjℓ,iℓ+1+σjℓ,iℓ+1(t,G(2)(t))/parenrightbig/bracketrightbigg
=1
(mn)k/2/summationdisplay
i1,...,ik∈[n]distinct
j1,...,jk∈[m]distinct/parenleftbiggk/productdisplay
ℓ=1E[/tildewideGjℓ,iℓ/tildewideGjℓ,iℓ+1+σjℓ,iℓ(t,G(2)(t))σjℓ,iℓ+1(t,G(2)(t))]/parenrightbigg
=(m)···(m−k+1)
(mn)k/2/summationdisplay
i1,...,ik∈[n]distinct/parenleftbiggk/productdisplay
ℓ=1E[/tildewideGjℓ,iℓ/tildewideGjℓ,iℓ+1+σjℓ,iℓ(t,G(2)(t))σjℓ,iℓ+1(t,G(2)(t))]/parenrightbigg
.
Now if any iℓ,iℓ+1are not in the same group {1,...,(n+t)/2},{(n+t)/2 + 1,...,n}then the
corresponding terms are 0by(3.6). Thus we must either choose all iℓto be in one group or the
other to have nonzero contribution. Additionally, the choi ce ofj1,...,jkdoes not impact the inner
term, which implies
E[/tildewideCk(m,t,G,σ (t,G(2)(t)))−Ck(m,G)] =E[/tildewideCk(m,t,G,σ (t,G(2)(t)))]
=(m)···(m−k+1)
(mn)k/2/parenleftbigg/parenleftbiggn+t
2/parenrightbigg
···/parenleftbiggn+t
2−k+1/parenrightbigg/parenleftbigg
−2
n+t+E(ρ(2)
1)2/parenrightbiggk
10+/parenleftbiggn−t
2/parenrightbigg
···/parenleftbiggn−t
2−k+1/parenrightbigg/parenleftbigg
−2
n−t+E(ρ(2)
2)2/parenrightbiggk/parenrightbigg
= (1+/tildewideO(n−1/4))αk/2
c(2(µ2−1)k) = (1+ /tildewideO(n−1/4))2(2β)k
We used that E/tildewideGj,1/tildewideGj,2=−2/(n+t)and similar for the other index group. This establishes (3.2)
fork >1. Fork= 1, we again see EC1(m,G) = 0 so we obtain
m√mn/parenleftbiggn/summationdisplay
i=1E/tildewideG2
1,i+Eσ1,1(t,G(2)(t))2+Eσ1,n(t,G(2)(t))2/parenrightbigg
−√mn
=m√mn/parenleftbiggn+t
2−1+n−t
2−1+(1+ /tildewideO(n−1/4))(µ2+µ2)/parenrightbigg
−√mn= (1+/tildewideO(n−1/4))2(2β).
For(3.4), we easily compute Var[Ck(m,G)] = (1 + /tildewideO(n−1))2k(using that cycle terms have
covariance 0unless they represent the exact same cycle, and using that th ere are2kways to
represent each particular cycle). Thus it suﬃces to bound th e desired variance by a smaller than
constant order. We expand and compute the order of magnitude of each covariance term that
appears. Writing X(0)
j,i=Y(0)
j,i=/tildewideGj,iandX(1)
j,i=σj,i(t,G(2)(t))andY(1)
j,i=σj,i(t,G), we have
Var[/tildewideCk(m,t,G,σ (t,G(2)(t)))−Ck(m,G)]
≤22k
(mn)k/summationdisplay
b∈{0,1}2kVar/bracketleftbigg/summationdisplay
i1,...,ik∈[n]
j1,...,jk∈[m]/parenleftbiggk/productdisplay
ℓ=1X(b2ℓ−1)
jℓ,iℓX(b2ℓ)
jℓ,iℓ+1−k/productdisplay
ℓ=1Y(b2ℓ−1)
jℓ,iℓY(b2ℓ)
jℓ,iℓ+1/parenrightbigg/bracketrightbigg
by the Cauchy–Schwarz inequality. Now we compute the order o f magnitude of a single contribution
corresponding to a ﬁxed b∈ {0,1}2k. We obtain
/summationdisplay
i,j,i′,j′E/bracketleftbigg/parenleftbiggk/productdisplay
ℓ=1X(b2ℓ−1)
jℓ,iℓX(b2ℓ)
jℓ,iℓ+1−k/productdisplay
ℓ=1Y(b2ℓ−1)
jℓ,iℓY(b2ℓ)
jℓ,iℓ+1−E[·]/parenrightbigg/parenleftbiggk/productdisplay
ℓ=1X(b2ℓ−1)
j′
ℓ,i′
ℓX(b2ℓ)
j′
ℓ,i′
ℓ+1−k/productdisplay
ℓ=1Y(b2ℓ−1)
j′
ℓ,i′
ℓY(b2ℓ)
j′
ℓ,i′
ℓ+1−E[·]/parenrightbigg/bracketrightbigg
(3.7)
where each ·is replaced with the preceding diﬀerence of products and whe rei,j,i′,j′are summing
over sequences of distinct values over the appropriate rang es.
To estimate this, we ﬁrst consider the order of magnitude of a n individual term. Any such term
is composed of values /tildewideGj,i,σj,i. Furthermore, to avoid the expectation being 0, we must have an
even number of σj,iterms for each possible index j, and an even number of /tildewideGj,ifor each j. Note
thatEσ2a
j,i=Oa(n−a)for either version of σ(coming from G(0),G(2)(t)). Additionally,
E/tildewideG1,1/tildewideG1,2=O(1/n),E/tildewideG1,1/tildewideG1,2/tildewideG1,3/tildewideG1,4=O(1/n2),E/tildewideG2
1,1/tildewideG1,3/tildewideG1,4=O(1/n)
follows from explicit computation.
Therefore, we easily ﬁnd that (using independence of the ran dom variables associated to diﬀerent
jand using that every jcan only appear once in (jℓ)ℓ∈[k],(j′
ℓ)ℓ∈[k]):
E/bracketleftbigg/parenleftbiggk/productdisplay
ℓ=1X(b2ℓ−1)
jℓ,iℓX(b2ℓ)
jℓ,iℓ+1−k/productdisplay
ℓ=1Y(b2ℓ−1)
jℓ,iℓY(b2ℓ)
jℓ,iℓ+1−E[·]/parenrightbigg/parenleftbiggk/productdisplay
ℓ=1X(b2ℓ−1)
j′
ℓ,i′
ℓX(b2ℓ)
j′
ℓ,i′
ℓ+1−k/productdisplay
ℓ=1Y(b2ℓ−1)
j′
ℓ,i′
ℓY(b2ℓ)
j′
ℓ,i′
ℓ+1−E[·]/parenrightbigg/bracketrightbigg
= (O(1/n))2k−t,
whereris the number of /tildewideG2terms that occur in the expanded product. We used that there
are4ktotal terms X,Y in any resulting product (the inner expectation terms can be handled
similarly). This means that the 2k-cycle formed by (iℓ)ℓ∈[k],(jℓ)ℓ∈[k]and the 2k-cycle formed by
(i′
ℓ)ℓ∈[k],(j′
ℓ)ℓ∈[k]must overlap in at least redges that form a /tildewideG2. In the case r= 2k, note that we
11must have bℓ= 0for allℓ∈[2k]and then the fact that we subtract X,Ymeans that the term is in
fact precisely 0. Thus we may assume r <2k. Therefore we may suppose these r≥0edges form
s≥0connected components which are all paths. The number of choi ces fori,j,i′,j′is then easily
seen to be (O(k))O(s)·(O(n))4k−r−s. Whens= 0, we have r= 0, and we can save an extra factor
ofncompared to the bound (O(n))4kfor the counting given above: if (jℓ)ℓ∈[k]and(j′
ℓ)ℓ∈[k]share
no values, then the two parts of the covariance are independe nce and we obtain a 0contribution,
so we can ignore the bulk of those terms. The number of ways to c hoosei,j,i′,j′to have such an
overlap is at most k2m2k−1n2k= (O(n))4k−1.
Therefore, we deduce
Var[/tildewideCk(m,t,G,σ (t,G(2)(t)))−Ck(m,G)]
≤/summationdisplay
r,s22k
(mn)k·22k·(O(1/n))2k−r·min{(O(k))O(s)(O(n))4k−r−s,(O(n))4k−1}
≤(O(1))k·min{(kO(1)/n)s,1/n} ≤n−5/6Var[Ck(m,G)].
We used k=ηlognwhere recall η >0will be chosen suﬃciently small. This completes the
justiﬁcation of (3.4)fork >1;k= 1is easily checked by hand.
For(3.1)and(3.3)the argument is similar, but the expectation computation is complicated by the
fact that Eρ(1)
1ρ(1)
2/n⌉}ationslash= 0. In fact, instead of conditioning on the values M1+···+M(n+t)/2,M(n+t)/2+1+
···+Mnthat appear, it is easier to only condition on the total sum M1+···+Mn(since we are
only conditioning on information related to it for this comp utation), and to modify Deﬁnition 3.2
appropriately. Additionally, since the value tplays no role if we compute the expectation this way,
we actually have the better error term listed in (3.1). However, we do omit these details here (note
that [ 2, Lemma 3.2] implies that our claimed mean shift is in fact wha t comes out of doing the
necessary computations). /square
Finally, in order to prove the Gaussian convergence of Ckwhen the matrix is sampled from ∆0,
we rely on an argument based on exchangeable pairs and Stein’ s method. Let us ﬁrst deﬁne the
notion of a pair of exchangeable random variables.
Deﬁnition 3.4. X′andXare exchangeable random variables if (X′,X)and(X,X′)have the same
distribution.
The key probability theoretic statement we will use is a mult ivariate version of the method of
exchangeable random variables for proving convergence to a Gaussian. This form is due to Meckes
[14].
Theorem 3.5 (Specialization of [ 14, Theorem 3]) .Let(X,X′)be an exchangeable pair of random
vectors in Rd. Suppose that there is an invertible matrix Λand a random matrix E′such that
•E/bracketleftbig
X′−X/vextendsingle/vextendsingleX/bracketrightbig
=−ΛX
•E/bracketleftbig
(X′−X)(X′−X)T/vextendsingle/vextendsingleX/bracketrightbig
= 2Λ+E/bracketleftbig
E′/vextendsingle/vextendsingleX/bracketrightbig
.
Then for g∈ C3(Rd),
/vextendsingle/vextendsingleEg(X)−Eg(Z)/vextendsingle/vextendsingle≤ /⌊ar⌈⌊lΛ−1/⌊ar⌈⌊lop/bracketleftBigg√
d
4M2(g)E/⌊ar⌈⌊lE′/⌊ar⌈⌊lHS+1
9M3(g)E/⌊ar⌈⌊lX′−X/⌊ar⌈⌊l3
2/bracketrightBigg
(3.8)
whereZis a standard Gaussian random vector in Rd.
We will need the following bound on moments of standard Gauss ian random variables (which is
a special case of a phenomenon called hypercontractivity ).
12Theorem 3.6 ([16, Theorem 9.21]) .Letfbe a polynomial in nvariables of degree at most d. Let
/vector x= (x1,...,x n)either be a vector of independent Rademacher random variabl es or a vector of
independent standard Gaussian random variables. Then for a ny real number q≥2, we have
E/bracketleftbig
|f(/vector x)|q/bracketrightbig1/q≤/parenleftbig/radicalbig
q−1/parenrightbigdE/bracketleftbig
f(/vector x)2/bracketrightbig1/2.
Proposition 3.7. Letm=τpre=⌊αcn−ηlogn⌋andK=⌈ηlogn⌉. We deﬁne V(M)as in
Lemma 2.8 and consider nsuﬃciently large. For g∈ C3(RK), we have
/vextendsingle/vextendsingleEg(V(G(0)))−EZ∼N(0,IK)g(Z)/vextendsingle/vextendsingle≤(M2(g)+M3(g))n−1/4.
Proof. We now deﬁne the setup for our exchangeable pairs. Let
G=G(0)= (gj,i)(j,i)∈[m]×[n]
and
G′= (gi,j)(j,i)∈([m]×[n])\I∪(gI)
whereIis an index sampled from [m]×[n]uniformly at random and the gIis an independent
standard Gaussian. The exchangeable pair of random variabl es we will consider is:
V=/parenleftbiggC1(m,G)√
2·1,...,CK(m,G)√
2K/parenrightbigg
, V′=/parenleftbiggC1(m,G′)√
2·1,...,CK(m,G′)√
2K/parenrightbigg
.
SinceIis a uniformly random element from [m]×[n]notice that
E[V′−V|V] =−diag/parenleftbigg2k− /BDk=1
mn/parenrightbigg
k∈[K]V
so deﬁne
Λ =diag/parenleftbigg2k− /BDk=1
mn/parenrightbigg
k∈[K].
In order to apply Theorem 3.5 , we take
E′=E[(V′−V)(V′−V)T−2Λ|G]
and note that this satisﬁes the required condition for E′asVis a measurable function of G.
We bound each of the terms appearing in (3.8). First,/⌊ar⌈⌊lΛ−1/⌊ar⌈⌊lop=mn. Second, note that
E/⌊ar⌈⌊lX′−X/⌊ar⌈⌊l3
2≤E/bracketleftbigg/parenleftbiggK/summationdisplay
k=1(Ck(m,G)−Ck(m,G′))2/parenrightbigg3/2/bracketrightbigg
≤K1/2·E/bracketleftbiggK/summationdisplay
k=1|Ck(m,G)−Ck(m,G′)|3/bracketrightbigg
≤n1/5·K/summationdisplay
k=1/parenleftbigg
E/bracketleftbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingleCk(m,G)−Ck(m,G′)/vextendsingle/vextendsingle/vextendsingle/vextendsingle2/bracketrightbigg/parenrightbigg3/2
=n1/5·K/summationdisplay
k=1(2k/(mn)E[Ck(m,G)2])3/2
≤n−11/4
where we have applied Holder’s inequality, Theorem 3.6 , and computed the variance using orthogo-
nality of various monomials in order to prove the desired res ult (note that all but at most 2k/(mn)
fraction of the monomials are canceled in Ck(m,G)−Ck(m,G′)).
Finally, we compute
E/⌊ar⌈⌊lE′/⌊ar⌈⌊lHS≤(E/⌊ar⌈⌊lE′/⌊ar⌈⌊l2
HS)1/2
13≤/parenleftbigg
2/summationdisplay
1≤k1<k2≤KE/bracketleftbig
E[(Ck1(m,G)−Ck1(m,G′))(Ck2(m,G)−Ck2(m,G′))|G]2/bracketrightbig
+/summationdisplay
1≤k≤KVar/bracketleftbig
E[(Ck(m,G)−Ck(m,G′))2|G]/bracketrightbig/parenrightbigg1/2
where we have used that 2k(E′)k,k=E[(Ck(m,G)−Ck(m,G′))2|G]−E[(Ck(m,G)−Ck(m,G′))2].
We now handle the oﬀ-diagonal and on-diagonal contribution separately. For ease of notation, given
an index I∈[m]×[n]we deﬁne
Ck(m,M,I) =/parenleftbigg1√n/parenrightbiggk/parenleftbigg1√m/parenrightbiggk /summationdisplay
i1,...,ik∈[n]distinct
j1,...,jk∈[m]distinct
I∈/uniontextℓ
k=1{(jℓ,iℓ),(jℓ,iℓ+1)}k/productdisplay
i=1Mjℓ,iℓMjℓ,iℓ+1,
C∗
k(m,M,I) =/parenleftbigg1√n/parenrightbiggk/parenleftbigg1√m/parenrightbiggk /summationdisplay
i1,...,ik∈[n]distinct
j1,...,jk∈[m]distinct
I∈/uniontextℓ
k=1{(jℓ,iℓ),(jℓ,iℓ+1)}/productdisplay
(j,i)∈/uniontextℓ
k=1{(jℓ,iℓ),(jℓ,iℓ+1)}\IMj,i.
We consider the oﬀ-diagonal contribution ﬁrst. We have
E/bracketleftbig
E[(Ck1(m,G)−Ck1(m,G′))(Ck2(m,G)−Ck2(m,G′))|G]2/bracketrightbig
=1
m2n2E/bracketleftbigg/parenleftbigg/summationdisplay
I∈[m]×[n]Ck1(m,G,I)Ck2(m,G,I)+C∗
k1(m,G,I)C∗
k2(m,G,I)/parenrightbigg2/bracketrightbigg
≤2
m2n2E/bracketleftbigg/parenleftbigg/summationdisplay
I∈[m]×[n]Ck1(m,G,I)Ck2(m,G,I)/parenrightbigg2
+/parenleftbigg/summationdisplay
I∈[m]×[n]C∗
k1(m,G,I)C∗
k2(m,G,I)/parenrightbigg2/bracketrightbigg
.
Note that as k1/n⌉}ationslash=k2, we have that Ck1(m,G,I)Ck2(m,G,I)andC∗
k1(m,G,I)C∗
k2(m,G,I)are mean
zero random variables. We will bound
E/bracketleftbigg/parenleftbigg/summationdisplay
I∈[m]×[n]Ck1(m,G,I)Ck2(m,G,I)/parenrightbigg2/bracketrightbigg
.
Expanding, each term is of the form Ck1(m,G,I 1)Ck2(m,G,I 1)Ck1(m,G,I 2)Ck2(m,G,I 2). A term
contributes precisely if the monomial in Ghas all even powers. Furthermore, any Gj,iappears at
most4times, which means any contribution is of size at most 34K= (O(1))K. Therefore, we are
primarily interested in counting the number of nonzero term s that arise. We have a multigraph H
coming from layering two cycles of length k1and two cycles of length k2, with possible overlaps
(including two guaranteed overlaps at edges I1,I2). Let there be vvertices, and let H′be the
multigraph formed by halving the multiplicities of H. Note that each vertex has at least two
outgoing edges to diﬀerent vertices in H′, and the total multiplicity of edges is 2k1+ 2k2. We
deduce that there are at most 2k1+2k2vertices, with equality precisely when H′is a disjoint union
of cycles. However, H′must be connected and can be seen to not be a single cycle since k1/n⌉}ationslash=k2.
Thus in fact v≤2k1+2k2−1.
Ifv≤2k1+2k2−8loglognthen there are at most (8k)8knv≤n2k1+2k2−4choices of term, using
k≤ηlogn. Otherwise let r= 2k1+2k2−vand note 1≤r≤8loglogn. We see that H′has at
mostO(r)many vertices of degree at least 3, where we include multiplicity (since there are 2k1+2k2
edges with multiplicity, 2k1+2k2−rvertices, and each vertex has at least two distinct neighbor s).
Notice that between such vertices, H′must have paths and there are at most Kr2ways to choose
14the sizes of all these paths. After doing so, there are 4O(K)ways to assign these edges to various
cycles. Finally, there are at most eO(K)ways to order these into cycles: outside of the rhigher
degree vertices, we have bounded degree so a bounded number o f choices for the next edge at every
stage, and among r≤8loglogn=O(logK)vertices we have at most O(K)choices, for KO(logK)
choices. Since Kr2=KO((logK)2)we easily ﬁnd a total of at most (O(1))Kn2k1+2k2−1total terms.
Plugging in, we ﬁnd the contribution to (E/⌊ar⌈⌊lE′/⌊ar⌈⌊lHS)2from these oﬀ-diagonal terms is at most
1
m2n2·(O(1))Kn2k1+2k2−1·/parenleftbigg1
(mn)k1/parenrightbigg/parenleftbigg1
(mn)k2/parenrightbigg
.
The total contribution is at most n−5+1/6. The situation for C∗is even simpler, and we can obtain
a bound of n−5+1/6analogously. Additionally, this analysis is easily seen to hold when k1= 1.
For the on-diagonal contribution, the analysis is quite sim ilar. For k= 1, straightforward com-
putation gives a contribution to (E/⌊ar⌈⌊lE′/⌊ar⌈⌊lHS)2of at most O(n−6).
Fork≥2, note that we have
Var/bracketleftbig
E[(Ck(m,G)−Ck(m,G′))2|G]/bracketrightbig
=1
m2n2Var/bracketleftbigg/summationdisplay
I∈[m]×[n](Ck(m,G,I)2+C∗
k(m,G,I)2)/bracketrightbigg
≤2
m2n2/parenleftbigg
Var/bracketleftbigg/summationdisplay
I∈[m]×[n]Ck(m,G,I)2/bracketrightbigg
+Var/bracketleftbigg/summationdisplay
I∈[m]×[n]C∗
k(m,G,I)2/bracketrightbigg/parenrightbigg
.
As before we consider the Cterm, since the C∗term can be analyzed analogously. We have
Var/bracketleftbigg/parenleftbigg/summationdisplay
I∈[m]×[n]Ck(m,G,I)2/parenrightbigg/bracketrightbigg
=E/bracketleftbigg/parenleftbigg/summationdisplay
I∈[m]×[n]Ck(m,G,I)2/parenrightbigg2/bracketrightbigg
−E/bracketleftbigg/summationdisplay
I∈[m]×[n]Ck(m,G,I)2/bracketrightbigg2
.
Again view this as an expansion into 4overlapping cycles as in the oﬀ-diagonal case. Here however
as the two cycles in each pair coming from k1,k2are isomorphic, one can overlap them perfectly,
which was not allowed in the prior analysis (which was used to argue that in fact v≤2k1+2k2−1,
ruling out v= 2k1+2k2). In this remaining case, we can see the contribution is exac tly cancelled
however by the subtracted term. The proof can thus proceed si milarly to the oﬀ-diagonal case, and
we obtain a bound n−5+1/6in this case as well.
Now we deduce
/⌊ar⌈⌊lΛ−1/⌊ar⌈⌊lop/parenleftbigg√
k
4E/⌊ar⌈⌊lE′/⌊ar⌈⌊lHS+1
9E/⌊ar⌈⌊lX′−X/⌊ar⌈⌊l3
2/parenrightbigg
≤mn(/radicalbig
logn(4n−5+1/6)1/2+n−11/4)≤n−1/3,
and therefore Theorem 3.5 ﬁnishes. /square
We are ﬁnally in position to deduce Lemma 2.8 .
Proof of Lemma 2.8 .The result is ultimately a straightforward combination of Lemma 3.3 and
Proposition 3.7 . We prove the statement for G(2)(t); the proof in the other two cases are iden-
tical. Note that
E/⌊ar⌈⌊lV(G(2)(t))−2µ−V(G(0))/⌊ar⌈⌊l2
≤(E/⌊ar⌈⌊lV(G(2)(t))−2µ−V(G(0))/⌊ar⌈⌊l2
2)1/2
≤/parenleftbigg
/⌊ar⌈⌊lE[V(G(2)(t))−2µ−V(G(0))]/⌊ar⌈⌊l2
2+/summationdisplay
1≤k≤KVar[/tildewideCk(m,t,G,σ (t,G(2)(t)))−Ck(m,G)]/parenrightbigg
≤n−1/5
15where we have applied Lemma 3.3 . This immediately implies that
|E[g(V(G(2)(t))−2µ)−g(V(G(0))]| ≤M1(g)·E[/⌊ar⌈⌊lV(G(2)(t))−2µ−V(G(0))/⌊ar⌈⌊l2]
≤M1(g)·n−1/5.
The desired result then follows from triangle inequality an dProposition 3.7 . /square
3.3.Miscellaneous estimates. Finally, we will additionally require the following basic e stimate
on the binomial coeﬃcient.
Claim 3.8. For1≤k≤n−1we have that
/parenleftbiggn
k/parenrightbigg
≤/radicalbig
n/(k(n−k))exp/parenleftbigg
−klog/parenleftbiggk
n/parenrightbigg
−(n−k)log/parenleftbiggn−k
n/parenrightbigg/parenrightbigg
.
Furthermore for |k| ≤√nlognandnsuﬃciently large such that (n+k)/2∈Z, we have that
/parenleftbiggn
(n+k)/2/parenrightbigg
=2n√
2√πnexp/parenleftbigg
−k2
2n±n−1/2/parenrightbigg
.
4.Moment computations
In this section we prove Lemma 2.7 using the tools we have developed so far. We separate the
ﬁrst and second moment computations.
4.1.First moment. We ﬁrst prove the ﬁrst moment.
Proof of (2.1).We choose a function h:R→[0,1]which is 1on[−L,∞],0on[−∞,−L−1]and
inC∞(R). Notice that linearity of expectation yields E[X(G)] = 2npmand
E[X(G)exp(−YK(m,G) /BD[YK(m,G)≥ −L])]≥E[X(G)exp(−YK(m,G)h(YK(m,G))]
= 2nE[ /BDvn∈Sτpre(G)exp(−YK(m,G)h(YK(m,G))]
= 2npmE[exp(−YK(m,G(1))h(YK(m,G(1)))].
(Recallvn∈Rnis all1s.) Note
YK(m,M) =K/summationdisplay
k=12(2β)kCk(m,M)−(2β)2k
4k=K/summationdisplay
k=1(2β)k
√
2k·Ck(m,M)√
2k−K/summationdisplay
k=1(2β)2k
4k.
Viewing exp(−YK(m,G(1))h(YK(m,G(1)))as a function of Ck(m,G(1))fork∈[K], by the product
rule we see that the function has ﬁrst, second, and third deri vatives of size O(n2η). LetZ∼
N(µ∗,σ2)with
µ∗=K/summationdisplay
k=1(2β)k
√
2k·(2β)k
√
2k−K/summationdisplay
k=1(2β)2k
4k=K/summationdisplay
k=1(2β)2k
4k, σ2=K/summationdisplay
k=1(2β)2k
2k.
Applying (2.5)ofLemma 2.8 , we ﬁnd that
E[X(G)exp(−YK(m,G) /BD[YK(m,G)≥ −L])]≥2npm(E[exp(−Zh(Z))]−n2η−1/6)
≥2npm(1−n−1/8)
where we have used that ηis a suﬃciently small constant in the ﬁnal bound, and that
Eexp(−Zh(Z)) = (1±n−2)Eexp(−Z) = (1±n−2)exp/parenleftbigg
−µ∗+σ2
2/parenrightbigg
= 1±n−1/2./square
164.2.Second moment. The second moment computation in Lemma 2.7 closely follows the com-
putation given in the work of Abbe, Li, and Sly [ 2]; we thus provide a proof tracking quantitative
aspects but omitting various routine calculations. We ﬁrst prove (2.3)which handles the majority
of the range of overlaps; here our approach is essentially id entical to that of Altschuler [ 3].
Proof of (2.3).ByClaim 3.8 , symmetry, and independence of the vectors of Gnote that
/summationdisplay
t∈[−(n−1),−n1/2logn]∪[n1/2logn,n−1]2n/parenleftbiggn
(n+t)/2/parenrightbigg
P[vn,vt∈Sτpre(G)]
≤2n/summationdisplay
t∈[n1/2logn,n−1]2nexp(nH(1/2+t/(2n)))P[vn,vt∈Sτpre(G)]
= 2n/summationdisplay
1≤t≤n/2−√nlogn2nexp(nH(t/n))(q(t/n))m
≤n2/summationdisplay
1≤t≤n/2−√nlogn2nexp(nH(t/n))(q(t/n))αcn
where we have used that by the Gaussian correlation inequali ty thatq(t)≥q(1/2) =P[|Z| ≤κ]2>0
and that ηis a suﬃciently small constant. We ﬁrst handle 1≤t≤ǫn/2. Notice that
/summationdisplay
1≤t≤ǫn/22nexp(nH(t/n))(q(t/n))αcn≤n·max
1/n≤t≤ǫ/22nexp(nH(t/n))·(q(t/n))αcn
≤n·2nexp(nH(1/n))·(q(1/n))αcn
≤n·exp(−Ω(√n)).
We have implicitly used S3andS5fromLemma 3.1 as well as the fact that q(0)αcn=pαcn= 2−n.
Next, notice that the function F(γ)is easily seen to have bounded derivatives away from the
endpoints of the interval [0,1]and therefore as F′(1/2) = 0 (by symmetry) and F′′(1/2)≤ −ε(S2),
we have that there exists δ >0such that
F(1/2+τ)≤F(1/2)−δτ2
for|τ| ≤δ. This implies that
/summationdisplay
(1/2−δ)n≤t≤n/2−√nlogn2nexp(nH(t/n))(q(t/n))αcn
≤n·2n(F(1/2)−δ(logn)2/n)n≤exp(−(logn)5/3)
given that nis suﬃciently large. Finally for the intermediate range, no te that
/summationdisplay
ǫn/2≤t≤(1/2−δ)n2nexp(−nH(t/n))(q(t/n))αcn≤n·2n·max
ǫ/2≤β≤1/2−δF(β)n
≤exp(−Ω(n))
where we have used S4witha=ε,b= 1/2−δ(the above analysis demonstrates that F(ε)< F(0)
andF(1/2−δ)≤F(1/2)−δ3). /square
We now handle the second moment in full generality.
Proof of (2.2).Notice that linearity of expectation and conditioning give s
E[X(G)2exp(−2YK(m,G) /BD[YK(m,G)≥ −L])]
=/summationdisplay
−n≤t≤n
(n+t)/2∈Z2n/parenleftbiggn
(n+t)/2/parenrightbigg
P[vn,vt∈Sτpre(G)]E[exp(−2YK(m,G(2)(t)) /BD[YK(m,G(2)(t))≥ −L])].
17We ﬁrst handle the range when n1/2logn≤ |t| ≤n−1. By(2.3)we have that
/summationdisplay
n1/2logn≤|t|≤n−1
(n+t)/2∈Z2n/parenleftbiggn
(n+t)/2/parenrightbigg
P[vn,vt∈Sτpre(G)]E[exp(−2YK(m,G(2)(t)) /BD[YK(m,G(2)(t))≥ −L])]
≤exp(2L)·/summationdisplay
n1/2logn≤|t|≤n−1
(n+t)/2∈Z2n/parenleftbiggn
(n+t)/2/parenrightbigg
P[vn,vt∈Sτpre(G)]
≤n2ηexp(−(logn)3/2)≤exp(−(logn)4/3)
which is decaying faster than any polynomial. We next handle the case where t=±n; notice the
contribution in this case is
2·2nP[vn∈Sτpre(G)]E[exp(−2YK(m,G(2)(n)) /BD[YK(m,G(2)(n))≥ −L])]
= 2E[X(G)]·E[exp(−2YK(m,G(1)) /BD[YK(m,G(1))≥ −L])]
/lessorsimilarE[X(G)]≤n−Ω(1)E[X(G)]2.
We used (2.5)ofLemma 2.8 with an appropriate test function g(an exponential of a linear func-
tion with smooth cutoﬀ) to bound the quantity E[exp(−2YK(m,G(1)) /BD[YK(m,G(1))≥ −L])]by
a constant, similar to in the proof of (2.1). The ﬁnal inequality follows since τpreis such that
EX(G) =nΘ(η)is of polynomial size.
Finally we handle the case where |t| ≤n1/2logn. For the sake of brevity, we deﬁne /tildewideY(t) =
E[exp(−2YK(m,G(2)(t)) /BD[YK(m,G(2)(t))≥ −L])]. Now
/summationdisplay
|t|≤n1/2logn
(n+t)/2∈Z2n/parenleftbiggn
(n+t)/2/parenrightbigg
P[vn,vt∈Sτpre(G)]/tildewideY(t)
=/summationdisplay
|t|≤n1/2logn
(n+t)/2∈Z4n√
2√πnexp/parenleftbigg
−t2
2n±n−1/2/parenrightbigg
q/parenleftbiggn+t
2n/parenrightbiggm
/tildewideY(t)
=q(1/2)m−αcn/summationdisplay
|t|≤n1/2logn
(n+t)/2∈Z4n√
2√πnexp/parenleftbigg
−t2
2n±n−1/4/parenrightbigg
q/parenleftbiggn+t
2n/parenrightbiggαcn
/tildewideY(t)
=q(1/2)m−αcn/summationdisplay
|t|≤n1/2logn
(n+t)/2∈Z4n√
2√πnexp/parenleftbigg
−t2
2n±n−1/4/parenrightbigg/parenleftbigg
q/parenleftbigg1
2/parenrightbigg
+t2q′′(1/2)
8n2±n−5/4/parenrightbiggαcn
/tildewideY(t)
=q(1/2)m−αcn/summationdisplay
|t|≤n1/2logn
(n+t)/2∈Z4n√
2√πnexp/parenleftbigg
−t2
2n±n−1/6/parenrightbigg/parenleftbigg
q/parenleftbigg1
2/parenrightbigg
+t2q′′(1/2)
8n2/parenrightbiggαcn
/tildewideY(0).
We used Claim 3.8 in the ﬁrst line, used that q(·)has bounded ﬁrst, second, third derivatives in the
neighborhood of 1/2in the second and third lines, and that /tildewideY(t) = (1±n−2/11)/tildewideY(0)(which is an
immediate consequence of Lemma 2.8 and an argument identical to that in the proof of (2.1)) in
the ﬁnal line.
18Continuing,
q(1/2)m−αcn/summationdisplay
|t|≤n1/2logn
(n+t)/2∈Z4n√
2√πnexp/parenleftbigg
−t2
2n±n−1/6/parenrightbigg/parenleftbigg
q/parenleftbigg1
2/parenrightbigg
+t2q′′(1/2)
8n2/parenrightbiggαcn
/tildewideY(0)
= 4np2m/tildewideY(0)/summationdisplay
|t|≤n1/2logn
(n+t)/2∈Z√
2√πnexp/parenleftbigg
−t2
2n±n−1/6/parenrightbigg/parenleftbigg
1+t2q′′(1/2)
8n2p2/parenrightbiggαcn
= 4np2m/tildewideY(0)/summationdisplay
|t|≤n1/2logn
(n+t)/2∈Z√
2√πnexp/parenleftbigg
−t2
2n+t2q′′(1/2)αc
8np2±n−2/13/parenrightbigg
≤(1+n−1/7)E[X(G)]2/tildewideY(0)/summationdisplay
t∈Z1√
2πnexp/parenleftbigg
−t2
2n+t2q′′(1/2)αc
8np2/parenrightbigg
= (1+n−1/7)E[X(G)]2/tildewideY(0)/summationdisplay
t∈Z1√
2πnexp/parenleftbigg
−t2
2n+t2κ2e−κ2αc
πnp2/parenrightbigg
= (1+n−1/7)E[X(G)]2/tildewideY(0)/summationdisplay
t∈Z1√
2πnexp/parenleftbigg
−t2
2n+t2αc(1−µ2)2
2n/parenrightbigg
= (1+n−1/7)E[X(G)]2/tildewideY(0)/summationdisplay
t∈Z1√
2πnexp/parenleftbigg
−t2(1−4β2)
2n/parenrightbigg
= (1±n−1/8)E[X(G)]2/tildewideY(0)1√
2π/integraldisplay∞
−∞exp/parenleftbigg
−x2(1−4β2)
2/parenrightbigg
dx
= (1±n−1/8)E[X(G)]2/tildewideY(0)1/radicalbig
1−4β2
where we have used that q(1/2) =p2, the values of q′′(1/2),(1−µ2)pand identities relating this
value to βviaS1andS6ofLemma 3.1 , and ﬁnally using the Euler–Maclaurin formula to transfer
from the ﬁnal sum to an integral.
The ﬁnal remaining task is therefore to bound
/tildewideY(0) =E[exp(−2YK(m,G(2)(0)) /BD[YK(m,G(2)(0))≥ −L])].
Note that YK(m,G(2)(0))is approximately normally distributed as Z∼ N(µ∗,σ2)with
µ∗=K/summationdisplay
k=1(2β)k
√
2k·2(2β)k
√
2k−K/summationdisplay
k=1(2β)2k
4k=K/summationdisplay
k=13(2β)2k
4k, σ2=K/summationdisplay
k=1(2β)2k
2k.
Via an argument essentially identical to that in the proof of (2.1), we have that
/tildewideY(0) = (1±n−1/4)E[exp(−2Z)]
= (1±n−1/4)exp(−2µ∗+2σ2)
= (1±n−1/4)exp/parenleftbigg
−K/summationdisplay
k=1(2β)2k
2k/parenrightbigg
= (1±n−Ω(1))/radicalbig
1−4β2.
Finally, putting everything together we have
E[X(G)2exp(−2YK(m,G) /BD[YK(m,G)≥ −L])]≤n−Ω(1)EX(G)2+e−(logn)4/3+(1±n−Ω(1))EX(G)2,
19so the desired result follows immediately. /square
References
[1] Emmanuel Abbe, Shuangping Li, and Allan Sly, Binary perceptron: eﬃcient algorithms can ﬁnd solutions in a
rare well-connected cluster , STOC ’22—Proceedings of the 54th Annual ACM SIGACT Symposi um on Theory
of Computing, ACM, New York, [2022] ©2022, pp. 860–873. 2
[2] Emmanuel Abbe, Shuangping Li, and Allan Sly, Proof of the Contiguity Conjecture and Lognormal Limit for
the Symmetric Perceptron , 2021 IEEE 62nd Annual Symposium on Foundations of Computer Science—FOCS
2021, IEEE Computer Soc., Los Alamitos, CA, [2022] ©2022, pp . 327–338. 2,3,7,8,9,12,17
[3] Dylan J Altschuler, Critical Window of The Symmetric Perceptron , arXiv:2205.02319. 1,2,3,4,5,8,17
[4] Benjamin Aubin, Will Perkins, and Lenka Zdeborová, Storage capacity in symmetric binary perceptrons , J. Phys.
A52(2019), 294003, 32. 2
[5] Ross Berkowitz, Ashwin Sah, and Mehtaab Sawhney, Number of arithmetic progressions in dense random subsets
ofZ/nZ, Israel J. Math. 244(2021), 589–620. 8
[6] Clément L Canonne, A short note on an inequality between KL and TV , arXiv:2202.07198. 5
[7] Martin Dietzfelbinger, Andreas Goerdt, Michael Mitzen macher, Andrea Montanari, Rasmus Pagh, and Michael
Rink, Tight thresholds for cuckoo hashing via xorsat , International Colloquium on Automata, Languages, and
Programming, Springer, 2010, pp. 213–225. 3
[8] Jian Ding and Nike Sun, Capacity lower bound for the Ising perceptron , STOC’19—Proceedings of the 51st
Annual ACM SIGACT Symposium on Theory of Computing, ACM, New York, 2019, pp. 816–827. 2
[9] Olivier Dubois and Jacques Mandler, The 3-XORSAT threshold , C. R. Math. Acad. Sci. Paris 335(2002),
963–966. 3
[10] John Duchi, Derivations for linear algebra and optimization. ,http://web.stanford.edu/~jduchi/projects/general_no tes.pdf .
5
[11] Ehud Friedgut, Sharp thresholds of graph properties, and the k-sat problem , J. Amer. Math. Soc. 12(1999),
1017–1054, With an appendix by Jean Bourgain. 2
[12] David Gamarnik, Eren C Kızıldağ, Will Perkins, and Chan gji Xu, Algorithms and barriers in the symmetric
binary perceptron model , arXiv:2203.15667. 2
[13] Werner Krauth and Marc Mézard, Storage capacity of memory networks with binary couplings , Journal de
Physique 50(1989), 3057–3066. 2
[14] Elizabeth Meckes, On Stein’s method for multivariate normal approximation , High dimensional probability V: the
Luminy volume, Inst. Math. Stat. (IMS) Collect., vol. 5, Ins t. Math. Statist., Beachwood, OH, 2009, pp. 153–178.
9,12
[15] Shuta Nakajima and Nike Sun, Sharp threshold sequence and universality for Ising percep tron models ,
arXiv:2204.03469. 2
[16] Ryan O’Donnell, Analysis of Boolean functions , Cambridge University Press, New York, 2014. 13
[17] Will Perkins and Changji Xu, Frozen 1-RSB structure of the symmetric Ising perceptron , STOC ’21—Proceedings
of the 53rd Annual ACM SIGACT Symposium on Theory of Computin g, ACM, New York, [2021] ©2021,
pp. 1579–1588. 2,4
[18] Boris Pittel and Gregory B. Sorkin, The satisﬁability threshold for k-XORSAT , Combin. Probab. Comput. 25
(2016), 236–268. 3
[19] Changji Xu, Sharp threshold for the Ising perceptron model , Ann. Probab. 49(2021), 2399–2415. 2
Department of Mathematics, Massachusetts Institute of Tec hnology, Cambridge, MA 02139, USA
Email address :{asah,msawhney}@mit.edu
20