Exploring the Curious Case of Code Prompts
Li Zhang, Liam Dugan, Hainiu Xu, Chris Callison-Burch
University of Pennsylvania
{zharry,ldugan,seacow,ccb}@seas.upenn.edu
Abstract
Recent work has shown that prompting lan-
guage models with code-like representations
of natural language leads to performance
improvements on structured reasoning tasks.
However, such tasks comprise only a small
subset of all natural language tasks. In our
work, we seek to answer whether or not code-
prompting is the preferred way of interacting
with language models in general . We compare
code and text prompts across three popular
GPT models ( davinci ,code-davinci-002 ,
andtext-davinci-002 ) on a broader selec-
tion of tasks (e.g., QA, sentiment, summariza-
tion) and Ô¨Ånd that with few exceptions, code
prompts do not consistently outperform text
prompts. Furthermore, we show that the style
of code prompt has a large effect on perfor-
mance for some but not all tasks and that Ô¨Åne-
tuning on text instructions leads to better rela-
tive performance of code prompts.
1 Introduction
Recent work has shown that pre-training language
models (LMs) on a mixture of text and program
code (e.g., Python or Javascript) makes them more
capable of reasoning over natural language (Suz-
gun et al., 2022). Such program-trained language
models (PLMs) signiÔ¨Åcantly outperform text-only
LMs on tasks such as math problems and track-
ing shufÔ¨Çed objects despite such tasks lacking any
explicit code formulae (Liang et al., 2022).
Furthermore, prompting such PLMs with code-
like structures (e.g., Python, JSON, PDDL) instead
of text has been shown to lead to performance im-
provements on structured common sense reasoning
(Madaan et al., 2022), event argument extraction
(Wang et al., 2022), knowledge graph construction
(Bi et al., 2023), story understanding (Dong et al.,
2022), and causal reasoning (Zhang et al., 2023).
Equal contribution.
You are trying to draw a simple teddy
bear. You need to do two things:
(a) erase unnecessary lines
(b) draw a shirt for the bear
The first thing to do is¬†
instructions = "Given a goal and
two steps, predict the order to do
the steps to achieve the goal"
goal = "Draw a Simple Teddy Bear"
step0 = "erase unnecessary lines"
step1 = "draw a shirt for the bear"
order_of_execution =¬†
Code PromptText Prompt
code-davinci-002text-davinci-002(b) draw a shirt
for the bear
[step1, step0]Figure 1: For certain tasks, prompting program-trained
language models with code -like representations works
better than prompting with text.
Such results naturally lead us to ask whether
code-prompting is the preferred way of interact-
ing with PLMs in general . While previous work
is limited to reasoning tasks, in this work we an-
alyze a broad selection of tasks (e.g., QA, senti-
ment, summarization) and systematically compare
the performance of prompting PLMs with code vs.
prompting with text1. We Ô¨Ånd that:
‚Ä¢With the exception of some reasoning tasks,
code prompts do not outperform text prompts
‚Ä¢The style of code prompt has a large effect on
performance for some but not all tasks.
‚Ä¢Fine-tuning on text instructions leads to rela-
tive improvements when using code prompts.
2 Experimental Design
Model Selection For our text-based LM we use
the original 175 billion parameter davinci model
introduced by Brown et al. (2020). For our
PLM we use the newer code-davinci-002 model
which was explicitly trained on text and code.
Neither model underwent any supervised instruc-
tion Ô¨Åne-tuning. In addition, we analyze perfor-
mance on text-davinci-002 , which is a variant
1The code, prompts, and outputs for our experiments are
public at https://github.com/zharry29/codex_vs_gpt3arXiv:2304.13250v1  [cs.CL]  26 Apr 2023Dataset Task Category Num. Eval Examples Metric Origin
HellaSwag Commonsense Reasoning 1000 / 10042 Accuracy Zellers et al. (2019)
wikiHow Goal-Step Commonsense Reasoning 1000 / 1073 Accuracy Zhang et al. (2020)
wikiHow Temporal Commonsense Reasoning 1000 / 3100 Accuracy Zhang et al. (2020)
WinoGrande Commonsense Reasoning 1000 / 1767 Accuracy Sakaguchi et al. (2021)
OpenPI Commonsense Reasoning 111 / 111 ROUGE-F1 Tandon et al. (2020)
ANLI Natural Language Inference 1000 / 3000 Accuracy Nie et al. (2020)
Yelp Sentiment Analysis 1000 / 10000 Pearson‚Äôs r Zhang et al. (2015)
IMDb Sentiment Analysis 1000 / 25000 Accuracy Maas et al. (2011)
HotpotQA Question Answering 1000 / 7405 Macro-F1 Yang et al. (2018)
SQuAD Question Answering 1000 / 11873 Macro-F1 Rajpurkar et al. (2018)
CNN/Daily Mail Summarization 1000 / 13368 ROUGE-2 Nallapati et al. (2016)
XSUM Summarization 1000 / 11332 ROUGE-2 Narayan et al. (2018)
Table 1: The 12 evaluation tasks. Macro F1 is based on Rajpurkar et al. (2016). For each task, we randomly
sample a Ô¨Åxed set of 1000 examples from its validation or test set for evaluation. For OpenPI we are limited to 111
examples.
ofcode-davinci-002 trained explicitly on human
demonstrations using supervised Ô¨Åne-tuning2. We
include this model to help us determine whether or
not Ô¨Åne-tuning PLMs on text instructions affects
their ability to interpret code prompts. All three
models were queried through the OpenAI API3and
our experiments cost approximately $2700 in total
(see Appendix E for the full cost breakdown).
Task Selection Following the methodology of
Sanh et al. (2022) we select tasks in a top-down
fashion by Ô¨Årst choosing the categories of interest
(e.g. Question Answering, Sentiment Analysis,
Summarization) and then selecting datasets from
within those categories. We pay special attention to
common sense and causal reasoning tasks as PLMs
prompted with code have been shown to perform
well on such tasks. The resulting 12 tasks are listed
in Table 1 and include Commonsense Reasoning,
Natural Language Inference, Sentiment Analysis,
Question Answering, and Summarization. More
details on each task can be found in Appendix A.
Prompt Formulation We collect text prompts
for each task using the PromptSource dataset (Bach
et al., 2022), a publicly available collection of
crowd-sourced prompt templates. For tasks with
many prompts, we manually select one from those
provided in the dataset. For a few tasks absent on
PromptSource, we write the prompts ourselves.
For our code prompts, we manually write four
custom code prompts per task. The code prompt
types are as follows, from least to most Pythonic.
(i).Vanilla ( Vanilla ): instructions and inputs
2https://platform.openai.com/docs/
model-index-for-researchers
3https://openai.com/blog/openai-apiare given as variables with generic names;
(ii).Var IdentiÔ¨Åer ( VI): instructions and inputs
are given as variables with meaningful names;
(iii).Var IdentiÔ¨Åer + Comments ( VIC): instruc-
tions and inputs are given as variables with
meaningful names along with comments ex-
plaining their purpose;
(iv).Class + Var IdentiÔ¨Åer + Comments ( CVIC ):
instructions and inputs are given as a task-
speciÔ¨Åc class . Functionality is ‚Äúimple-
mented‚Äù as member functions.
Figure 2 shows an example of the different styles
of code prompts for the wikiHow temporal order-
ing task. Note that we attempt to write our code
prompts such that we match the wording of the text-
based PromptSource prompt as closely as possible.
At inference time, for each test example, we ran-
domly sample in-context examples from the train-
ing set and add them to the context window until the
maximum context length is reached. This process
circumvents the bias caused by static in-context
examples. We conduct an ablation study where we
vary the random seed and show that this process
produces consistent results (see Appendix C).
3 Results
What is the best type of code prompt? We
compare performance across the four code prompt
types from Section 2 on all 12 tasks using
code-davinci-002 and report our results in Fig-
ure 3. We Ô¨Ånd that no single type of code prompt
performs signiÔ¨Åcantly better than the others across
all tasks and that the relative difference in perfor-
mance between code prompts also varies signiÔ¨Å-
cantly across tasks. For example, on IMDb and
SQuAD all code prompts have roughly even perfor-You are trying to {goal}. You
need to do two things:
(a) {step0}
(b) {step1}
The first thing to do
is¬†{first}
input0 = "Given a goal and two steps,
predict the correct order to do the
steps to achieve the goal"
input1¬†= "{goal}"
step0 = "{step0}"
step1 = "{step1}"
label =¬†[{first},{second}]Code Prompt (vanilla) Text Prompt
instructions = "Given a goal and two
steps, predict the correct order to do
the steps to achieve the goal"
goal¬†= "{goal}"
step0 = "{step0}"
step1 = "{step1}"
order_of_exec¬†=¬†[{first},{second}]Code Prompt (VI - var identifier)
"""Given a goal and two steps, predict the correct
order to do the steps to achieve the goal"""
# The goal that someone is trying to achieve
goal¬†= "{goal}"
# One of the steps that needs to be taken
step0 = "{step0}"
# Another one of the steps that need be taken
step1 = "{step1}"
# The list of correct order of those two steps
order_of_exec¬†=¬†[{first},{second}]Code Prompt (VIC - var identifier + comments)
import order_steps
class Event:
¬† """Given a goal and two steps, predict the correct¬†
¬† order to do the steps to achieve the goal"""
¬† def __init__(self, goal, step0, step1):
¬† ¬† self.goal = goal # The goal someone is trying to accomplish
¬† ¬† self.step0 = step0 # One of the steps that need be taken
¬† ¬† self.step1 = step1 # Another step that need be taken
¬† def get_order_of_steps(self):
¬† ¬† # Output a list of correct order of the two steps to be taken
¬† ¬† return order_steps(self.goal, self.step0, self.step1)
event = Event(goal="{goal}",¬†step0="{step0}",¬†step1="{step1}")
assert(event.get_order_of_steps == [{first},{second}])Code Prompt (CVIC - class + var identifier + comments)Figure 2: An example of the four styles of manually written code prompts used in our analysis ( Vanilla ,VI,VIC,
andCVIC ) for the wikiHow temporal ordering task. At test time, variables in braces are replaced with information
from the dataset item (as shown in Figure 1). For this task, {goal} ,{step0} ,{step1} refer to the article title and
the steps to order while {first} and{second} refer to the true ordering of the steps.
HotpotQA ANLI SQuAD OpenPI0:40:50:6
CNN/DM XSUM1111:512
HellaSWAG wikiHow
goal-stepwikiHow
temporalWinoGrande Yelp IMDb0:50:60:70:80:9
Vanilla VI VIC CVIC
Figure 3: Comparison of code-davinci-002 across
the four types of code prompts. Figures are split to
allow for different y-axis scales. We see that different
prompts do better on different tasks and while some
tasks have high variance over prompt types, others do
not.
mance while for tasks such as wikiHow-Temporal
and WinoGrande we see a near 14% accuracy dif-
ference between the worst and best prompt.
In Appendix B, we calculate the average rank of
each code prompt type relative to each other and
Ô¨Ånd that the ‚ÄúVar IdentiÔ¨Åer + Comments‚Äù ( VIC)
prompt is the best across all tasks on average (2.25
avg. rank). We thus use this prompt type for our
comparison in all future sections.
How many in-context examples should we in-
clude in our code prompt? We would like to
also investigate how the number of in-context ex-
1 10 1000.00.10.20.30.40.50.60.70.80.91.0
Wikihow-GS
Wikihow-T
HotpotQA
WinoGrande
HellaSwag
CNN/DMOpenPI
SQuAD
XSUM
ANLI
IMDb
YelpFigure 4: Performance score (y-axis) vs number of
in-context examples (x-axis, in log scale) using code
prompts ( VIC) with code-davinci-002 . We see that
increasing number of examples does not always in-
crease performance and in some cases makes it worse.
amples in the prompt affects models‚Äô ability to
perform the task. We therefore conducted an ex-
periment where we Ô¨Ålled the context window of
code-davinci-002 with in-context examples up
to 2000 tokens, 4000 tokens, and 8000 tokens and
plotted the validation accuracy of the model with
respect to the number of examples in Figure 4.
Contrary to expectations, we Ô¨Ånd that the num-
ber of in-context examples has little effect on model
performance for most tasks and actually has a neg-
ative effect on some tasks. This is especially in-
teresting given that previous work on in-context
learning with text prompts Ô¨Ånds roughly mono-
tonic improvement from adding more in-context
examples (Liu et al., 2021). While further research
is necessary, it seems that code prompts may haveDataset Metric davinci code-002 text-002
+Text +Code  +Text +Code  +Text +Code 
Hellaswag Accuracy 0.321 0.307 -0.014 0.652 0.606 -0.046 0.717 0.773 +0.046
wikiHow goal-step Accuracy 0.347 0.302 -0.045 0.924 0.898 -0.026 0.919 0.915 -0.004
wikiHow temporal Accuracy 0.495 0.532 +0.037 0.622 0.727 +0.105 0.688 0.761 +0.073
Yelp Pearson  0.913 0.896 -0.017 0.924 0.907 -0.017 0.919 0.904 -0.015
IMDb Accuracy 0.872 0.935 +0.063 0.945 0.951 +0.006 0.940 0.952 +0.012
WinoGrande Accuracy 0.513 0.500 -0.013 0.607 0.716 +0.109 0.628 0.726 +0.098
ANLI Accuracy 0.333 0.360 +0.027 0.562 0.551 -0.011 0.504 0.557 +0.053
HotpotQA Macro-F1 - - - 0.470 0.449 -0.021 0.490 0.350 -0.140
SQuAD Macro-F1 0.482 0.466 -0.016 0.604 0.579 -0.025 0.670 0.656 -0.014
OpenPI ROUGE-F1 - - - 37.33 36.36 -0.970 35.60 31.30 -4.300
CNN/Daily Mail ROUGE-2 9.28 9.13 -0.150 11.74 11.67 -0.070 13.63 13.55 -0.080
XSUM ROUGE-2 9.38 6.83 -2.550 14.51 11.03 -3.580 14.48 13.26 -1.220
Table 2: Performance of the three LMs when using code prompts (+Code) vs. using text prompts (+Text). Blank
cells indicate tasks for which single test examples could not Ô¨Åt in the context window. Color indicates whether or
not code prompts are better, slightly better, slightly worse, or worse than text prompts. We see that while code
prompts outperform text prompts for certain tasks (such as wikiHow temporal and WinoGrande) text prompts are
better on average. We also Ô¨Ånd that instruction Ô¨Åne-tuning ( text-002 ) allows for better code prompt utilization.
different scaling behavior than text prompts when
used in in-context learning.
Which is better: code or text prompts? In our
main experiment we compare the performance of
the three GPT models on code prompts ( VICstyle)
and text prompts across the 12 datasets. Given the
results from Figure 4, we Ô¨Åll the context window
of all models with in-context examples up to 4000
tokens to serve as a middle ground for comparing
code and text prompts. We report the results of
our main experiment in Table 2 and see several
surprising trends.
First, we Ô¨Ånd that prompting PLMs with code
leads to substantial increases in performance for
certain few reasoning tasks but that this trend does
not hold across all tasks‚Äîor even all reasoning
tasks. For example, when using code prompts with
code-davinci-002 , we see a 10.5% accuracy in-
crease on wikiHow temporal ordering but a 2.6%
accuracy decrease on wikiHow goal-step inference
despite both being commonsense reasoning tasks
and having identical source material.
Second, we Ô¨Ånd that supervised instruc-
tion Ô¨Åne-tuning on natural language demon-
strations does not hurt model performance on
code. Rather, we instead observe that code
prompts outperform text prompts on more tasks
when using text-davinci-002 than when us-
ing code-davinci-002 despite the fact that
text-davinci-002 received no additional Ô¨Åne-
tuning on code instructions.
Finally, we Ô¨Ånd that LMs not explicitly trained
on code can also beneÔ¨Åt from code promptingon certain reasoning tasks. In particular, code
prompts outperform text prompts on davinci for
3 out of our 12 tasks‚Äîthe same proportion as
code-davinci-002 . The tasks that beneÔ¨Åt from
code prompts also seem to be largely consistent
across the three types of models tested, suggesting
some underlying trend as to which tasks systemati-
cally beneÔ¨Åt from structured input.
4 Conclusion
In this work we investigate whether or not there
exists a systematic performance difference between
prompting PLMs with code or with text. We con-
Ô¨Årm that there are indeed tasks for which code
prompting is signiÔ¨Åcantly more effective than text
prompting and that this Ô¨Ånding holds across differ-
ent types of models. However, for most tasks, we
Ô¨Ånd that text prompting is still the best method for
eliciting few-shot generalization from PLMs.
Given this result it seems reasonable to attempt
to predict which tasks will beneÔ¨Åt from code
prompts and which tasks will not. However, we
show that making such predictions based on sim-
ple heuristics such as domain and task category is
difÔ¨Åcult and that the larger trends remain unclear.
Future work should seek to investigate the core
mechanism behind what makes code prompting
effective for certain tasks.
Finally, concurrent to our work, a new line of re-
search has emerged wherein models generate code
andexecute that code to produce valid output (Chen
et al., 2022; Mishra et al., 2022; Gao et al., 2022;
Lyu et al., 2023). Future work should consider-ing whether or not the tasks that beneÔ¨Åt from ex-
ecutable code prompts and non-executable code
prompts have any overlap.
Limitations
One signiÔ¨Åcant limitation to our study is that, as of
March 23rd 2023, OpenAI has deprecated access
tocode-davinci-0024, thus rendering our results
non-replicable for any team not granted special
access to these models by OpenAI. We did not
anticipate this deprecation while conducting this
work and we believe this raises serious questions
about the usage of API-based language models in
scholarly work.
Another limitation is that the 12 tasks we se-
lected may not be representative of the broader
population of natural language tasks. Had we con-
ducted our experiments on a larger selection of
tasks there may have been larger-scale trends that
we would have been able to uncover.
The largest and most pressing limitation with
our work is that the models we are testing on
have closed-source pre-training datasets. Thus, we
are unable to verify the extent to which our task
datasets have been included in the training or in-
struction Ô¨Åne-tuning data. Given that the training
data for most of the models tested in this work cuts
off in late 2021, this is a very strong possibility.
Our results should be viewed with this limitation
strongly in mind.
Finally, while we experimented with different
code prompts, the search space of possible prompts
is very large. Thus, it is very likely that there
exists some prompt that outperforms our chosen
prompts for each task. Drawing conclusions based
on a limited sampling of prompts is tenuous and
while methods exist for searching the space of all
prompts, such techniques lack interpretability and
erase any distinction between code and text prompt
(Li and Liang, 2021).
Acknowledgements
The paper is dedicated to the late Prof. Dragomir
Radev, the Ô¨Årst mentor in NLP of the author Li
Zhang, for igniting his passion for research and
passing onto him much knowledge.
We thank Shuyan Zhou, Aman Madaan, and
Niket Tandon for valuable discussions about this
4https://platform.openai.com/docs/
model-index-for-researcherswork and we thank Alyssa Hwang for her contribu-
tions to the structure, presentation, and narrative of
the Ô¨Ånal paper.
This research is based upon work supported
in part by the DARPA KAIROS Program (con-
tract FA8750-19-2-1004), the DARPA LwLL Pro-
gram (contract FA8750-19-2-0201), the OfÔ¨Åce
of the Director of National Intelligence (ODNI)
via the IARPA HIATUS Program (contract 2022-
22072200005), the NSF (Award 1928631), and
gifts from Roblox and Salesforce. Approved for
Public Release, Distribution Unlimited. The views
and conclusions contained herein are those of the
authors and should not be interpreted as neces-
sarily representing the ofÔ¨Åcial policies, either ex-
pressed or implied, of DARPA, ODNI, IARPA,
NSF, the U.S. Government, or of Roblox or Sales-
force. The U.S. Government is authorized to re-
produce and distribute reprints for governmental
purposes notwithstanding any copyright annotation
therein.
References
Stephen H. Bach, Victor Sanh, Zheng-Xin Yong, Al-
bert Webson, Colin Raffel, Nihal V . Nayak, Ab-
heesht Sharma, Taewoon Kim, M Saiful Bari,
Thibault Fevry, Zaid Alyafeai, Manan Dey, An-
drea Santilli, Zhiqing Sun, Srulik Ben-David, Can-
wen Xu, Gunjan Chhablani, Han Wang, Jason Alan
Fries, Maged S. Al-shaibani, Shanya Sharma, Ur-
mish Thakker, Khalid Almubarak, Xiangru Tang,
Dragomir Radev, Mike Tian-Jian Jiang, and Alexan-
der M. Rush. 2022. Promptsource: An integrated
development environment and repository for natural
language prompts.
Zhen Bi, Jing Chen, Yinuo Jiang, Feiyu Xiong,
Wei Guo, Huajun Chen, and Ningyu Zhang. 2023.
Codekgc: Code language model for generative
knowledge graph construction.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry,
Amanda Askell, Sandhini Agarwal, Ariel Herbert-
V oss, Gretchen Krueger, Tom Henighan, Rewon
Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu,
Clemens Winter, Chris Hesse, Mark Chen, Eric
Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
Jack Clark, Christopher Berner, Sam McCandlish,
Alec Radford, Ilya Sutskever, and Dario Amodei.
2020. Language models are few-shot learners. In
Advances in Neural Information Processing Systems ,
volume 33, pages 1877‚Äì1901. Curran Associates,
Inc.
Wenhu Chen, Xueguang Ma, Xinyi Wang, and
William W Cohen. 2022. Program of thoughtsprompting: Disentangling computation from reason-
ing for numerical reasoning tasks. arXiv preprint
arXiv:2211.12588 .
Yijiang River Dong, Lara J. Martin, and Chris Callison-
Burch. 2022. Corrpus: Detecting story inconsisten-
cies via codex-bootstrapped neurosymbolic reason-
ing.
Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon,
Pengfei Liu, Yiming Yang, Jamie Callan, and Gra-
ham Neubig. 2022. Pal: Program-aided language
models. arXiv preprint arXiv:2211.10435 .
Karl Moritz Hermann, Tomas Kocisky, Edward Grefen-
stette, Lasse Espeholt, Will Kay, Mustafa Suleyman,
and Phil Blunsom. 2015. Teaching machines to read
and comprehend. In NIPS , pages 1693‚Äì1701.
Xiang Lisa Li and Percy Liang. 2021. PreÔ¨Åx-tuning:
Optimizing continuous prompts for generation.
Percy Liang, Rishi Bommasani, Tony Lee, Dimitris
Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian
Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Ku-
mar, et al. 2022. Holistic evaluation of language
models. arXiv preprint arXiv:2211.09110 .
Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,
Lawrence Carin, and Weizhu Chen. 2021. What
makes good in-context examples for gpt- 3?
Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang,
Delip Rao, Eric Wong, Marianna Apidianaki, and
Chris Callison-Burch. 2023. Faithful chain-of-
thought reasoning.
Andrew L. Maas, Raymond E. Daly, Peter T. Pham,
Dan Huang, Andrew Y . Ng, and Christopher Potts.
2011. Learning word vectors for sentiment analy-
sis. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies , pages 142‚Äì150, Port-
land, Oregon, USA. Association for Computational
Linguistics.
Aman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang,
and Graham Neubig. 2022. Language models of
code are few-shot commonsense learners. arXiv
preprint arXiv:2210.07128 .
Swaroop Mishra, Matthew Finlayson, Pan Lu, Leonard
Tang, Sean Welleck, Chitta Baral, Tanmay Ra-
jpurohit, Oyvind Tafjord, Ashish Sabharwal, Pe-
ter Clark, et al. 2022. Lila: A uniÔ¨Åed bench-
mark for mathematical reasoning. arXiv preprint
arXiv:2210.17517 .
Ramesh Nallapati, Bowen Zhou, Cicero dos Santos,
√áaÀòglar Gul√ßehre, and Bing Xiang. 2016. Abstrac-
tive text summarization using sequence-to-sequence
rnns and beyond. In Proceedings of the 20th
SIGNLL Conference on Computational Natural Lan-
guage Learning , pages 280‚Äì290.Shashi Narayan, Shay B Cohen, and Mirella Lapata.
2018. Don‚Äôt give me the details, just the summary!
topic-aware convolutional neural networks for ex-
treme summarization. In Proceedings of the 2018
Conference on Empirical Methods in Natural Lan-
guage Processing , pages 1797‚Äì1807.
Yixin Nie, Adina Williams, Emily Dinan, Mohit
Bansal, Jason Weston, and Douwe Kiela. 2020. Ad-
versarial nli: A new benchmark for natural language
understanding. In Proceedings of the 58th Annual
Meeting of the Association for Computational Lin-
guistics , pages 4885‚Äì4901.
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida,
Carroll L. Wainwright, Pamela Mishkin, Chong
Zhang, Sandhini Agarwal, Katarina Slama, Alex
Ray, John Schulman, Jacob Hilton, Fraser Kelton,
Luke Miller, Maddie Simens, Amanda Askell, Pe-
ter Welinder, Paul Christiano, Jan Leike, and Ryan
Lowe. 2022. Training language models to follow in-
structions with human feedback.
Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018.
Know what you don‚Äôt know: Unanswerable ques-
tions for squad. In Proceedings of the 56th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 2: Short Papers) , pages 784‚Äì789.
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. Squad: 100,000+ questions for
machine comprehension of text. In Proceedings of
the 2016 Conference on Empirical Methods in Natu-
ral Language Processing , pages 2383‚Äì2392.
Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavat-
ula, and Yejin Choi. 2021. Winogrande: An adver-
sarial winograd schema challenge at scale. Commu-
nications of the ACM , 64(9):99‚Äì106.
Victor Sanh, Albert Webson, Colin Raffel, Stephen
Bach, Lintang Sutawika, Zaid Alyafeai, Antoine
ChafÔ¨Ån, Arnaud Stiegler, Arun Raja, Manan Dey,
et al. 2022. Multitask prompted training enables
zero-shot task generalization. In International Con-
ference on Learning Representations .
Maarten Sap, Vered Shwartz, Antoine Bosselut, Yejin
Choi, and Dan Roth. 2020. Introductory tutorial:
Commonsense reasoning for natural language pro-
cessing. ACL 2020 , page 27.
Mirac Suzgun, Nathan Scales, Nathanael Sch√§rli, Se-
bastian Gehrmann, Yi Tay, Hyung Won Chung,
Aakanksha Chowdhery, Quoc V Le, Ed H Chi,
Denny Zhou, et al. 2022. Challenging big-bench
tasks and whether chain-of-thought can solve them.
arXiv preprint arXiv:2210.09261 .
Niket Tandon, Keisuke Sakaguchi, Bhavana Dalvi,
Dheeraj Rajagopal, Peter Clark, Michal Guerquin,
Kyle Richardson, and Eduard Hovy. 2020. A dataset
for tracking entities in open domain procedural text.
InProceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing (EMNLP) ,
pages 6408‚Äì6417.Xingyao Wang, Sha Li, and Heng Ji. 2022.
Code4struct: Code generation for few-shot
structured prediction from natural language. arXiv
preprint arXiv:2210.12810 .
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,
William Cohen, Ruslan Salakhutdinov, and Christo-
pher D Manning. 2018. Hotpotqa: A dataset for
diverse, explainable multi-hop question answering.
InProceedings of the 2018 Conference on Empiri-
cal Methods in Natural Language Processing , pages
2369‚Äì2380.
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali
Farhadi, and Yejin Choi. 2019. Hellaswag: Can a
machine really Ô¨Ånish your sentence? In Proceed-
ings of the 57th Annual Meeting of the Association
for Computational Linguistics , pages 4791‚Äì4800.
Li Zhang, Qing Lyu, and Chris Callison-Burch. 2020.
Reasoning about goals, steps, and temporal order-
ing with wikihow. In Proceedings of the 2020 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP) , pages 4630‚Äì4639.
Li Zhang, Hainiu Xu, Yue Yang, Shuyan Zhou, Weiqiu
You, Manni Arora, and Chris Callison-Burch. 2023.
Causal reasoning of entities and events in procedural
texts. In Findings of the Association for Computa-
tional Linguistics: EACL 2023 , Dubrovnik, Croatia.
Association for Computational Linguistics.
Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015.
Character-level convolutional networks for text clas-
siÔ¨Åcation. In Advances in Neural Information Pro-
cessing Systems , volume 28. Curran Associates, Inc.
A Detailed Task Description
Summarization is the task of composing a con-
cise description of a lengthy text. Given a long
narrative, the model is tasked with composing a
short summary that contains the salient events in
the original text.
For our study, we select the CNN/Daily Mail
(Hermann et al., 2015; Nallapati et al., 2016) and
XSUM (Narayan et al., 2018) datasets as both are
variants on the challenging abstractive summariza-
tion task. XSUM tasks models with generating
extremely concise 1 to 2 sentence summaries of
news articles and CNN/Daily Mail tasks models
with generating reasonably concise but longer ab-
stractive summaries. For both CNN/Daily Mail
andXSUM datasets, we use ROUGE-2 score for
evaluation.
Question Answering (QA) is the task of com-
posing answers given a question and an optional
context passage. When this context passage is pro-
vided the task is referred to as ‚Äúopen-book‚Äù QA andwhen it is not it is referred to as ‚Äúclosed-book‚Äù QA.
Open-book QA tasks examine language models‚Äô
ability to understand and extract information from
their context while Closed-book QA tasks evaluate
the amount of knowledge encapsulated in language
models during pre-training.
For our study we pick two open-book QA
datasets, SQuADv2 (Rajpurkar et al., 2018) and
HotpotQA (Yang et al., 2018), which allow us to
focus our evaluation on how structured prompts af-
fect models‚Äô ability to comprehend long text input.
For both SQuADv2 andHotpotQA , we evaluate
model performance based on the macro-averaged
F1 score as proposed in Rajpurkar et al. (2016).
This metric measures the average overlap between
the prediction and ground truth answer. It is cal-
culated by treating the prediction and ground truth
as bags of tokens, and Ô¨Årst computing their F1.
Then, the maximum F1 score is taken over all of
the ground truth answers for a given question, and
that score is averaged over all of the questions to
get the Ô¨Ånal result.
Commonsense Reasoning is a machine reason-
ing task that demands the use of commonsense
knowledge which is oftentimes implicitly present
in the text (Sap et al., 2020). The customary formu-
lation of commonsense reasoning tasks are Clas-
siÔ¨Åcation , where the input is a context, optionally
with candidate answers as choices, and the output
is a label from a pre-deÔ¨Åned label space, and Ques-
tion Answering (QA), where the input is a context
followed by a reasoning question and the output is
in free-form language.
In this study, we selected four ClassiÔ¨Åcation style
commonsense reasoning tasks: wikiHow Temporal
andwikiHow Goal-Step (Zhang et al., 2020), ANLI
(Nie et al., 2020), and HellaSwag (Zellers et al.,
2019). We also included one Question Answering
style task with OpenPI (Tandon et al., 2020). In
addition, we evaluate our models on WinoGrande a
comprehensive reasoning benchmark dataset (Sak-
aguchi et al., 2021).
ForwikiHow Goal-Step ,wikiHow Temporal ,
HellaSwag ,WinoGrande , and ANLI , we use clas-
siÔ¨Åcation accuracy as the evaluation metric. To
evaluate OpenPI , we use F1 score based on the
ROUGE metric as described in the original paper
(Tandon et al., 2020).
Sentiment Analysis is a task that is concerned
with judging emotion and its degree in text. GivenVanilla VI VIC CVIC
HellaSwag 3 2 1 4
wikiHow Goal-Step 4 2 1 3
wikiHow Temporal 4 3 2 1
Yelp 4 2 1 4
IMDb 1 3 1 4
WinoGrande 4 1 2 3
HotpotQA 4 3 2 1
ANLI 1 2 4 3
OpenPI 1 2 3 4
SQuAD 1 3 4 2
CNN/Daily Mail 4 2 3 1
XSUM 2 4 3 1
Mean 2.75 2.42 2.25 2.58
Standard Deviation 1.36 0.76 1.09 1.26
Table 3: Relative performance rank of the four code
prompt types from Section 2 across the 12 tasks. Ranks
are calculated based on the results reported in Figure 3.
We see that the ‚ÄúVariable IdentiÔ¨Åer + Comments‚Äù ( VIC)
style prompt performs the best out of all code prompt
types on average.
a passage, a language model is tasked with clas-
sifying the sentiment (positive, negative, neutral)
and/or its degree (strongly, weakly, moderately).
The selected datasets, namely IMDb (Maas et al.,
2011) and Yelp (Zhang et al., 2015), are both con-
structed using customer reviews. The IMDb dataset
proposes a binary classiÔ¨Åcation problem where
the input is a movie review and the label space
isfnegative;positive g. Yelp proposes a Ô¨Åve-way
classiÔ¨Åcation problem where the input is a restau-
rant review and the label space is the number of
stars (out of 5) the customers assigned to the restau-
rant.
ForIMDb , we use accuracy as the evaluation
metric and for Yelp, we use Pearson Correlation
between the predicted rating and the ground truth
rating as the evaluation metric.
B Ranking of Code Prompt Styles
In Table 3 we report the rank-based statistics of the
four code prompt types from Section 2 on our 12
tasks. Ranks are calculated based on the results re-
ported in Figure 3 of the main paper. The numbers
in a row reÔ¨Çect the relative standing of each code
prompt on the corresponding task. While we note
that all code prompts perform within 0.5 ranks of
each other on average, we see that on average the
VICprompt performs the best across all tasks and
theVanilla prompt performs the worst. Looking
to the standard deviation section, we see that the
VIprompt performs the most consistently acrossDataset Performance 
Hellaswag 0.65, 0.67, 0.69, 0.67, 0.67 0.01
wikiHow-GS 0.51, 0.51, 0.51, 0.50, 0.51 0.00
wikiHow-T 0.62, 0.65, 0.63, 0.63, 0.62 0.01
Yelp 0.92, 0.92, 0.92, 0.92, 0.92 0.00
IMDb 0.94, 0.94, 0.94, 0.94, 0.94 0.00
WinoGrande 0.62, 0.64, 0.61, 0.62, 0.62 0.01
HotpotQA 0.35, 0.33, 0.35, 0.35, 0.35 0.01
ANLI 0.59, 0.58, 0.57, 0.60, 0.61 0.01
OpenPI 36.3, 38.1, 38.3, 37.7, 39.9 1.16
SQuAD 0.60, 0.62, 0.61, 0.60, 0.63 0.01
CNN/DM 11.7, 12.0, 12.4, 12.3, 12.0 0.25
XSUM 14.5, 14.9, 15.5, 15.2, 15.4 0.36
Table 4: Comparison across 5 repeated runs of the
code-davinci-002 model with text prompts using dif-
ferent random seeds for sampling in-context examples.
We see minimal standard deviation ( ) between the
runs.
all tasks and that once again the Vanilla prompt
performs the least consistently.
C Ablation Study
To see whether the Ô¨Åndings in our Results sec-
tion could be attributed to variance in the ran-
dom sampling of in-context training examples per
test example, we conduct Ô¨Åve repeated runs using
code-davinci-002 with different random seeds
each time and calculated the standard deviation
across the Ô¨Åve runs. We report our results in Ta-
ble 4 and Ô¨Ånd that the choice of in-context exam-
ples accounts for very little of the observed vari-
ance across prompt type and context length. This
Ô¨Ånding is surprising as previous work has shown
that the selection and ordering of in-context exam-
ples has a very large effect on the performance of
models (Liu et al., 2021). However, it seems that
our approach of random sampling in-context ex-
amples per test item helps to lessen this inherent
variance.
D Evaluation on text-davinci-003
While conducting our research into the differ-
ences between code and text prompts, OpenAI
released the text-davinci-003 model. This
model differs from text-davinci-002 in that it
is trained using Reinforcement Learning with Hu-
man Feedback (RLHF) instead of supervised in-
struction Ô¨Åne-tuning (Ouyang et al., 2022). Out
of curiosity, to see the effect of this new train-
ing paradigm, we conducted experiments com-
paring this new text-davinci-003 model to the
other GPT-3.5 models ( text-davinci-002 andTask code-002 text-002 text-003
(base) (+IFT) (+RLHF)
HellaSwag 0.652 0.717 0.714
wikiHow GS 0.924 0.919 0.510
wikiHow T 0.622 0.688 0.815
Yelp 0.924 0.919 0.903
IMDb 0.945 0.940 0.938
WinoGrande 0.607 0.628 0.735
ANLI 0.562 0.504 0.549
HotpotQA 0.470 0.490 0.378
SQuAD 0.604 0.670 0.663
OpenPI 37.33 35.60 39.06
CNN/DM 11.74 13.63 12.64
XSUM 14.51 14.48 13.36
Table 5: Performance of the three GPT-3.5 models
across our 12 datasets with text prompts . (+IFT)
indicates the addition of supervised instruction Ô¨Åne-
tuning and (+RLHF) indicates the addition of training
using Reinforcement Learning from Human Feedback
(Ouyang et al., 2022). We see that RLHF does not
always improve performance and that for some tasks
(HotpotQA and wikiHow Goal-Step) it causes large
degradations in performance.
code-davinci-002 ). We report the results of our
comparison across the 12 evaluation tasks in Ta-
ble 5.
We see that while text-davinci-003 out-
performs all previous models on wikiHow Tem-
poral ,WinoGrande , and OpenPI , it does signif-
icantly worse than previous models on wikiHow
Goal-Step andHotpotQA . Such large reductions in
performance are to be somewhat expected when
using RLHF given the costly nature of collecting
human demonstrations. However, the magnitude of
the decreases (-50.1% for wikiHow and -11.2% for
HotpotQA ) is nonetheless surprising and such re-
sults raise important questions about exactly what
is being learned when conducting instruction Ô¨Åne-
tuning and whether or not this learned information
can generalize to tasks not seen during Ô¨Åne-tuning.
E Evaluation Cost
In this section we report the approximate cost
of conducting our experiments. In our study
we use four OpenAI models, namely davinci ,
code-davinci-002 ,text-davinci-002 and
text-davinci-003 . While code-davinci-002
is free to use at the time of this study, we report
the approximate cost of running the experiments
on the other three models5in Table 6. To
5The cost of querying davinci ,text-davinci-002 and
text-davinci-003 is $0.02/1,000 tokens at the time of study.
Seehttps://openai.com/pricing for more details.Dataset Num. Examples Est. Cost
HellaSwag 1000 / 10042 $240.48
wikiHow Goal-Step 1000 / 1073 $240.48
wikiHow Temporal 1000 / 3100 $240.48
WinoGrande 1000 / 1767 $240.48
OpenPI 111 / 111 $28.08
ANLI 1000 / 3000 $240.48
Yelp 1000 / 10000 $240.48
IMDb 1000 / 25000 $240.48
HotpotQA 1000 / 7405 $241.20
SQuAD 1000 / 11873 $241.08
CNN/Daily Mail 1000 / 13368 $257.91
XSUM 1000 / 11332 $246.66
Total Cost $2698.29
Table 6: The total estimated cost of running davinci ,
text-davinci-002 andtext-davinci-003 for 1000
data samples from each dataset (except for OpenPI).
estimate the cost of an experiment, we calculate
the approximate number of tokens necessary
for computing one dataset example and then
multiplied that by the number of examples in the
dataset. For classiÔ¨Åcation tasks, since we Ô¨Åll up
the context window to roughly 4000 tokens for
every test example, we estimate the number of
tokens to be 4000 (3999 tokens for the prompt
and 1 token for the label). To estimate cost for
generative tasks (OpenPI, HotpotQA, SQuAD,
CNN/Daily Mail, and XSUM), we compute the
average generation length from our generated
samples and assume the in-context examples take
up 3500 tokens. While this calculation results in
a fairly loose upper bound, we believe this to be
a good estimate of the total cost incurred by the
project as such overestimates help offset the cost
of other miscellaneous API queries done over the
course of the project.