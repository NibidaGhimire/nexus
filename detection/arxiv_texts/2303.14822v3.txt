MGTBench: Benchmarking Machine-Generated Text Detection
Xinlei He Xinyue Shen Zeyuan Chen Michael Backes Yang Zhang
CISPA Helmholtz Center for Information Security
Abstract
Nowadays, powerful large language models (LLMs) such
as ChatGPT have demonstrated revolutionary power in a
variety of natural language processing (NLP) tasks such
as text classification, sentiment analysis, language transla-
tion, and question-answering. Consequently, the detection
of machine-generated texts (MGTs) is becoming increas-
ingly crucial as LLMs become more advanced and preva-
lent. These models have the ability to generate human-like
language, making it challenging to discern whether a text is
authored by a human or a machine. This raises concerns re-
garding authenticity, accountability, and potential bias. How-
ever, existing methods for detecting MGTs are evaluated us-
ing different model architectures, datasets, and experimental
settings, resulting in a lack of a comprehensive evaluation
framework that encompasses various methodologies. Fur-
thermore, it remains unclear how existing detection methods
would perform against powerful LLMs.
In this paper, we fill this gap by proposing the first bench-
mark framework for MGT detection against powerful LLMs,
named MGTBench. Extensive evaluations on public datasets
with curated texts generated by various powerful LLMs such
as ChatGPT-turbo and Claude demonstrate the effectiveness
of different detection methods. Our ablation study shows
that a larger number of words in general leads to better per-
formance and most detection methods can achieve similar
performance with much fewer training samples. Addition-
ally, our findings reveal that metric-based/model-based de-
tection methods exhibit better transferability across different
LLMs/datasets. Furthermore, we delve into a more challeng-
ing task: text attribution, where the goal is to identify the
originating model of a given text, i.e., whether it is a spe-
cific LLM or authored by a human. Our findings indicate
that the model-based detection methods still perform well
in the text attribution task. To investigate the robustness
of different detection methods, we consider three adversar-
ial attacks, namely paraphrasing, random spacing, and ad-
versarial perturbations. We discover that these attacks can
significantly diminish detection effectiveness, underscoring
the critical need for the development of more robust detec-
tion methods. We envision that MGTBench will serve as a
benchmark tool to accelerate future investigations involving
the evaluation of powerful MGT detection methods on their
respective datasets and the development of more advancedMGT detection methods.1
1 Introduction
Large language models (LLMs), such as T5 [27], GPT3 [11],
PaLM [13], and more powerful LLMs such as ChatGPT [5]
and Claude [6], have been a significant breakthrough in the
field of natural language processing (NLP). With huge num-
bers of parameters and being trained with massive amounts
of data, LLMs have shown remarkable performance in var-
ious real-world applications, e.g., education, customer ser-
vice, and finance.
While powerful LLMs have shown impressive qualities in
terms of achieving remarkable performance in various tasks
and generating human-like texts, several limitations and eth-
ical concerns have to be taken into account. First, LLMs
may generate text that sounds realistic but may not be en-
tirely accurate or factual [21]. Second, the misuse of LLMs
may raise concerns in education, making a fair judgment im-
possible [32]. Also, it is difficult to trace back the machine-
generated text to its source, which raises concerns about ac-
countability, especially when the content is used to spread
misinformation or propaganda [37].
To address these issues, researchers have considered au-
tomatic detection methods that can identify the machine-
generated text (MGT) from the human-written text (HWT).
Concretely, those methods can be concluded into two cat-
egories, i.e., metric-based methods and model-based meth-
ods. For the metric-based methods [17, 23, 29], metrics such
as log-likelihood, word rank, and predicted distribution en-
tropy are used to determine whether a text belongs to MGT
or HWT. Regarding the model-based methods [3, 18, 29, 38],
classification models are trained by both MGTs and HWTs.
Overall, existing MGT detection methods have been stud-
ied under different LLMs and different datasets, albeit in
isolation. Also, powerful LLMs such as ChatGPT demon-
strate remarkable performance across diverse tasks, provid-
ing high-quality, human-like answers. This prompts the need
for a holistic evaluation of these methods against powerful
LLMs. To do this, we develop a comprehensive benchmark
of MGT detection, namely MGTBench. MGTBench follows
a modular design, consisting of the input module, the detec-
tion module, and the evaluation module. Until now, we have
1Source code and datasets are available at https://github.com/
xinleihe/MGTBench .
1arXiv:2303.14822v3  [cs.CR]  16 Jan 2024implemented ten MGT detection methods. We also take ad-
vantage of the powerful LLMs including ChatGLM, Dolly,
ChatGPT-turbo, GPT4All, StableLM, and Claude, which are
the most popular and powerful LLMs (thus far), to pro-
duce MGTs on existing HWT datasets. MGTBench en-
ables researchers to benchmark various methods on differ-
ent datasets. Its modular design facilitates the integration of
additional detection methods, as well as the plugging in of
datasets and models.
Evaluation. We perform an extensive measurement study
over the 13 detection methods against 6 LLMs. Our measure-
ment is performed on 3 benchmark datasets including Essay,
WP, and Reuters. Note that for each dataset, besides the texts
written by humans (labeled as HWTs), we also query each
LLM with the constructed prompt (see Table 1) to obtain the
MGT from the LLM (labeled as MGTs).
Extensive evaluations show that the LM Detector outper-
forms other detection methods and reaches the best perfor-
mance. For instance, to differentiate whether the texts are
generated by humans or ChatGPT-turbo on Essay, the detec-
tion F1-score is 0.993 with LM Detector while only 0.968
with Log-Likelihood. Our ablation study shows that an in-
creased word count can enhance the detection performance,
and 200 words is a sufficient length to achieve satisfac-
tory performance. We also find that most detection meth-
ods can achieve comparable performance with significantly
fewer training samples. For instance, with only 10 training
samples, Log-Likelihood achieves 0.967 F1-score to differ-
entiate HWTs from MGT generated by ChatGLM on Esay,
which is close to the performance with full training sam-
ples (0.970). Another interesting finding is that metric-based
methods can better transfer to new LLMs while model-based
methods are more capable of adapting to new datasets. For
example, on Essay, Log-Likelihood trained on MGTs gener-
ated by GPT4All can attain 0.983 F1-score in differentiating
MGTs generated by ChatGLM, and OpenAI Detector trained
on Essay can achieve 0.931 F1-score on WP when dealing
with MGTs generated by ChatGPT-turbo.
Later, we consider a more challenging task, i.e., text attri-
bution, where the goal is to identify the exact model to gener-
ate the text, e.g., either human or one of the LLMs. Our eval-
uation shows that the model-based detection methods per-
form much better than the metric-based detection method in
the text attribution task. For example, on Essay, LM Detector
achieves 0.927 F1-score while the F1-score is only 0.208 and
0.387 for Rank and GLTR.
Then, to quantify the robustness of different detection
methods, we consider three adversarial attacks, namely
paraphrasing, random spacing, and adversarial perturbation.
Evaluation results demonstrate that current detection meth-
ods are extremely vulnerable to adversarial attacks. Take
LRR as an example, with WP as the dataset and ChatGPT-
turbo as the MGT generator, the detection F1-score degrades
0.418, 0.413, and 0.469 in response to paraphrasing, random
spacing, and adversarial perturbation attacks (see Table 4 for
more details). This pronounced vulnerability underscores the
necessity to develop more robust methods for MGT detec-
tion. Our code and models will be made publicly available.In summary, we make the following contributions:
• We propose MGTBench, a benchmarking framework
for MGT detection/attribution against powerful LLMs.
• Our empirical evaluation shows that the LM Detector
outperforms other detection methods in both MGT de-
tection and text attribution tasks. Additionally, our ab-
lation studies shed light on the unique characteristics of
different detection methods.
• We systematically evaluate the robustness of different
detection methods by introducing three adversarial at-
tacks. Our findings indicate that those methods are
highly susceptible to such attacks, which highlights the
need for future research to develop more resilient MGT
detection methods.
2 Preliminary and Related Work
2.1 Text Generation with Large Language
Models
The recent advancements in LLMs can be traced back to
the transformer architecture proposed by Vaswani et al. [35],
which introduces the self-attention mechanisms to allow the
model to focus on different regions of the input sequence.
Later Radford et al. [25] develop the generative pre-trained
transformer (GPT) based on the transformer architectures.
Being trained with a large corpus of text data, GPT achieves
superior performance on a wide range of language genera-
tion tasks. GPT2 [26], a larger version of GPT that contains
more parameters and is trained on a larger corpus, is devel-
oped and achieves better performance than GPT. GPT3 [11]
is the third generation of GPT with over 175B parameters,
which has been shown to be capable of generating coherent
and contextually appropriate text even in situations where it
is provided with minimal input or guidance. Since Novem-
ber 2022, OpenAI releases ChatGPT [5], which is trained
based on the GPT-3.5 architectures and leverages Reinforce-
ment Learning from Human Feedback (RLHF) [14, 30] to
improve its generation ability. ChatGPT shows revolutionary
capabilities in generating coherent and relevant texts, which
can be integrated into various applications, such as chatbots,
customer service, and education.
Another notable model series in the field is the masked lan-
guage model. Bidirectional Encoder Representations from
Transformers (BERT) developed by Devlin et al. [15] is one
of the most representative models that is pre-trained using
the masked language modeling task. Liu et al. [22] develop
RoBERTa, which leverages a robustly optimized BERT pre-
training approach and can reach an even better performance
than BERT in various tasks. With the widespread use of
LLMs for generating texts, concerns about authenticity, ac-
countability, and potential bias have also been raised.
In this paper, we consider six representative power-
ful LLMs, including ChatGPT-turbo, ChatGLM, Dolly,
GPT4All, StableLM, and Claude.
ChatGPT-turbo [5]. ChatGPT is an advanced large lan-
guage model built upon the GPT-3.5 architecture and specif-
2ically designed to generate highly human-like responses. To
achieve this capability, ChatGPT is fine-tuned with Rein-
forcement Learning from Human Feedback (RLHF) [30]
where human trainers actively engage in conversations with
ChatGPT. Here we consider the latest iteration of ChatGPT,
namely ChatGPT-turbo.
ChatGLM [1]. ChatGLM is another large language model
based on the GLM (General Language Model) frame-
works [16]. The training process involved a combination of
supervised fine-tuning, feedback bootstrap, and RLHF, thus
enabling it to generate responses that are in accordance with
human-like patterns and preferences.
Dolly [2]. Dolly is an instruction-following large language
model released by Databricks. It is trained on around 15K
instruction/response records, generated by Databricks em-
ployees in capability domains such as brainstorming, classi-
fication, closed QA, generation, information extraction, open
QA, and summarization.
GPT4All [7]. GPT4All is an open-source assistant-style
large language model. It is trained over a massive curated
corpus including word problems, story descriptions, multi-
turn dialogue, and code.
StableLM [4]. StableLM is an auto-regressive language
model based on the NeoX transformer architecture [10]. It is
fine-tuned on various chat and instruction-following datasets.
Claude [6]. Claude, developed by Anthropic, is an AI lan-
guage model similar to OpenAI’s ChatGPT. It is designed
to engage in natural language conversations, provide helpful
responses, and assist users with a variety of tasks.
2.2 Machine-Generated Text Detection
To address the above-mentioned issues, researchers have de-
veloped various MGT detection methods [8, 17, 18, 20, 23,
24, 29, 33, 34, 38]. Current detection methods can be divided
into two categories, i.e., metric-based methods and model-
based methods. Generally speaking, metric-based methods
leverage pre-trained LLMs to process the text and extract dis-
tinguishable features from it, e.g., the rank or entropy of each
word in a text conditioned on the previous context. In this
paper, we consider eight metric-based detection methods, in-
cluding Log-Likelihood, Rank, Log-Rank, Entropy, GLTR,
DetectGPT, LRR, and NPR.
Log-Likelihood [29]. This approach leverages a language
model to measure the token-wise log probability. Concretely,
given a text, we average the token-wise log probability of
each word to generate a score for this text. Note that a larger
score denotes the text is more likely to be machine-generated.
Rank [17]. For each word in a text, given its previous con-
text, we can calculate the absolute rank of this word. Then,
for a given text, we compute the score of the text by averag-
ing the rank value of each word. Note that a smaller score
denotes the text is more likely to be machine-generated.
Log-Rank [23]. Slightly different from the Rank metric that
uses the absolute rank, the Log-Rank score is calculated by
first applying the log function to the rank value of each word.Entropy [17]. Similar to the Rank score, the Entropy score
of a text is calculated by averaging the entropy value of each
word conditioned with its previous context. As mentioned by
previous work [17, 23], the machine-generated text is more
likely to have a lower Entropy score.
GLTR [17]. GLTR is developed as a support tool to facilitate
the labeling process of whether a text is machine-generated.
In our evaluation, we follow the suggestion of Guo et al. [18]
and consider the Test-2 features (i.e., the fraction of words
that rank within 10, 100, 1,000, and others). Note that one
can easily implement other sets of features in MGTBench.
DetectGPT [23]. Mitchell et al. [23] propose DetectGPT
that measures the change of the model’s log probability func-
tion by adding minor perturbation to the original text. The
intuition is that the text derived from an LLM has a tendency
to be in the local optimal of the model’s log probability func-
tion. Therefore, any minor perturbation of model-generated
text tends to have a lower log probability under the model
than the original text, while minor perturbation of human-
written text may have a higher or lower log probability than
the original text.
LRR [31]. Su et al. [31] propose Log- Likelihood Log-
Rank Ratio (LRR), which combines Log-Likelihood and
Log-Rank as they provide complementary information for
the given text.
NPR [31]. Similar to DetectGPT, Normalized Perturbed
Log- Rank (NPR) also introduces perturbation to the origi-
nal text. The motivation for NPR is that both MGTs and
HWTs show vulnerability to minor disturbances, as indicated
by a rise in the Log-Rank score following such perturbations.
However, this effect is more pronounced in MGTs, as they
exhibit a greater increase in the Log-Rank score after distur-
bances, implying a higher NPR (noise-to-perturbation ratio)
score for MGTs compared to HWTs.
Regarding the model-based methods, a classification
model is usually trained using a corpus that contains both
HWTs and MGTs. By doing this, the classification model
is expected to have the capability in identifying MGTs from
a given corpus. In this paper, we consider five model-based
methods.
OpenAI Detector [29]. The OpenAI Detector is used to
detect GPT2-generated output, which was created by fine-
tuning a RoBERTa model using outputs from the largest
GPT2 model (i.e., with 1.5B parameters). This model is
capable of predicting whether a given text was machine-
generated or not.
ChatGPT Detector [18]. ChatGPT Detector is developed
by Guo et al. [18] to distinguish human-written texts from
ChatGPT-generated texts. The model was created by fine-
tuning a RoBERTa model using the HC3 [18] dataset. The
detector is based on the RoBERTa model. The authors pro-
vide two ways to train the RoBERTa model. The first one
only leverages the pure answered text, and the second one
leverages the question-answer text pair to jointly train the
model. In our evaluation, we consider the first one to be con-
sistent with other detection methods.
3ConDA [9]. Bhattacharjee et al. [9] develop Contrastive
Domain Adaptation (ConDA), which leverages the repre-
sentation power of contrastive learning to acquire domain-
invariant representations
GPTZero [3]. GPTZero is an MGT analyzer tool that uses
two main measures, i.e., perplexity and burstiness, to deter-
mine whether a text is machine-generated or written by a hu-
man.2GPTZero provides the publicly accessible API to pro-
duce a confidence score about how likely a text is generated
by the machine.
LM Detector. Besides the previous methods, the detec-
tor can also be built by fine-tuning the pre-trained language
model (LM) with an extra classification layer. Here we take
the BERT model as an example to evaluate its efficacy fol-
lowing Ippolito et al. [20].
Compared to previous work [24, 34], our study offers no-
table advancements in the field of MGT detection by integrat-
ing more detection methods into our benchmarking frame-
work. Moreover, we extend the evaluation to more power-
ful LLMs such as ChatGPT-turbo and Claude. This broader
scope enables us to gain deeper insights into the performance
and robustness of various detection methods when applied to
a wider range of LLMs. By incorporating these enhance-
ments, our work significantly contributes to the existing lit-
erature and paves the way for further advancements in MGT
detection research.
3 MGTBench
In this section, we introduce MGTBench, a modular frame-
work designed to benchmark MGT detection methods. Cur-
rently, we have provided reference implementations of the 8
metric-based detection methods and easy-to-use APIs for the
5 model-based methods we mentioned before.
3.1 Modular Design
MGTBench consists of three different modules, including the
input module ,detection module , and evaluation module .
Input Module. In the input module, we provide specific
dataset pre-processing functions for different datasets and
our code base is easy to cope with datasets from Hugging-
Face, which facilitates the future developments of different
users.
Detection Module. This module implements different
metric-based and model-based detection methods with a
standardized input/output format. Currently, we support ten
different detection methods.
Evaluation Module. This module is used to evaluate the per-
formance of different detection methods. Now, we provide
five different evaluation metrics, including accuracy, preci-
sion, recall, F1-score, and AUC, which are the commonly
used metrics to evaluate classification performance. We also
support sample-level logging for detailed analysis.
2https://www.makeuseof.com/gptzero-detect-ai-generated-
text/ .3.2 Using MGTBench
To be best of our knowledge, MGTBench is the most com-
prehensive benchmark tool for MGT detection against pow-
erful LLMs. Users can leverage MGTBench on their own
dataset for a comprehensive risk assessment of potential
MGTs in the dataset. On the other hand, researchers can
leverage MGTBench as a tool to evaluate new MGT detec-
tion/generation methods. As MGTBench follows a modular
design, its input and evaluation modules can be easily re-used
by new detection methods. Also, new detection methods can
be easily implemented within the standardized API provided
by MGTBench. Moreover, MGTBench integrates well with
HuggingFace, given the fact that many model-based detec-
tion methods have published or are willing to publish their
models into HuggingFace, MGTBench can be seamlessly up-
dated to fit the new model-based detection methods. MGT-
Bench is under continuous development and we will include
more detection methods as well as analysis tools in the fu-
ture.
4 Experimental Settings
4.1 Datasets
Table 1: The prompts we used to obtain MGTs from LLMs.
Following Verma et al. [36], we set Kto the number of words
in each HWT rounding to the nearest 100 and acquire the
<prompt >/<headline >by querying ChatGPT-turbo.
Dataset Prompt
Essay Write an essay in Kwords to the prompt
<prompt >
WP Write a story in Kwords to the prompt
<prompt >
Reuters Write a news article in Kwords with the
following headline <headline >
In this paper, we consider three datasets provided by
Verma et al. [36], namely Essay, WP, and Reuters. Note that
for each dataset, besides the HWTs, it also contains MGTs
generated by ChatGPT-turbo and Clude. And we addition-
ally generate MGTs from other LLMs as well.
Essay. This dataset has 1,000 samples extracted from es-
says available on IvyPanda, encompassing both high school
and university-level essays across various academic disci-
plines. The authors first query ChatGPT-turbo to generate
the<prompt >corresponding to the essay. Then, the con-
structed prompt would be leveraged to query different LLMs
and obtain the generated essays.
WP. This dataset has 1,000 samples that are derived from
the subreddit r/WritingPrompts where users share cre-
ative writing prompts and craft stories in response to these
prompts. Following the constructed prompts, we can then
query different LLMs to generate stories.
Reuters. This dataset is derived from the Reuters 50-50 au-
thorship identification dataset [19], which consists of 1,000
4(news) articles by 50 journalists (20 articles per journalist).
As the article does not have the headline, the authors first
prompt ChatGPT-turbo to generate a <headline >for each ar-
ticle. We also leverage this headline to construct the prompt
and query LLMs to obtain MGTs.
We show the prompt we used to query LLMs in Table 1.
For each entry in a dataset, we have the human text and six
LLM-generated texts (see Section 2.1 for more details about
the six LLMs). We only keep entries with more than 1 word
for human and LLM-generated texts. Then, we randomly
split 80% of the entries as the training set and the rest as the
testing set.
4.2 Tasks
In our evaluation, our primary focus is on the MGT detec-
tion task, which involves detecting whether a given text is
generated by a human or a machine (i.e., LLM). To accom-
plish this, we establish a binary classification task for texts
generated by humans and each LLM, such as Human vs.
ChatGPT-turbo, Human vs. ChatGLM, etc. Note that we
also consider a more complex task, namely text attribution
(see Section 5.2). This task seeks to pinpoint the precise
model responsible for generating the text. Essentially, when
presented with a text, our goal is to determine whether it is
generated by the human or one of the six LLMs. This task
can be viewed as a seven-category classification scenario.
4.3 Detection Methods
For metric-based methods, we use GPT2-medium as the base
model in our experiments since it can already reach good per-
formance with limited cost. Given the metrics extracted with
the GPT2-medium, we additionally build a logistic regres-
sion model on top of it to provide concrete predictions. For
model-based methods, we directly use the publicly available
pre-trained models from HuggingFace (OpenAI Detector,
ChatGPT Detector, and LM Detector) or GitHub (ConDA).
Concretely, for OpenAI Detector, we use the RoBERTa-base
version of it as it usually gives better detection performance.
For ChatGPT Detector, we leverage the provided RoBERTa-
base model of it. For LM Detector, we leverage the distilled
BERT-base model as it has superior performance and mod-
est expenditure. For ConDA, we leverage the RoBERTa-base
model targeted on ChatGPT.3Note that for OpenAI Detector,
ChatGPT Detector, and ConDA, the pre-trained models are
already optimized for the MGT detection task (binary classi-
fication), therefore, we do not further fine-tune them unless
otherwise mentioned. It is also worth mentioning that, in the
text attribution task, we fine-tune all detection methods as the
number of classes increases from 2 to 7.
4.4 Evaluation Metrics
MGTBench supports various metrics to evaluate perfor-
mance, including accuracy, precision, recall, F1-score, and
AUC (area under the ROC curve). In our evaluation, unless
otherwise mentioned, we use F1-score as the main evaluation
metric.
3https://github.com/AmritaBh/ConDA-gen-text-detection .5 Evaluation
We first present the experiment results on the MGT detec-
tion task. As shown in Table 2, we observe that metric-
based methods such as Log-Likelihood, Log-Rank, GLTR,
and LRR have relatively good performances on different
datasets as well. For instance, Log-Likelihood reaches 0.970,
0.980, and 0.972 F1-score on Essay, WP, and Reuters when
distinguishing ChatGLM-generated texts from HWTs. On
the other hand, NPR and DetectGPT reach less satisfying
performance. For instance, on WP, to distinguish MGTs gen-
erated by ChatGPT-turbo from HWTs, the F1-score is only
0.352 and 0.608, respectively. We suspect the reason is that
the metric changes less when the perturbation is conducted
on longer texts, which leads to lower performance.
Regarding other model-based methods, we observe that
LM Detector achieves the best performance across different
datasets/LLMs in general. For instance, on Essay, the LM de-
tector achieves 0.993 F1-score against ChatGPT-turbo. This
is expected as the LM detector is trained on the corpus with
both HWTs and MGTs, and the LM’s ability to capture the
context and coherence information greatly aids in differenti-
ating the pattern variations between these two types of texts.
We also observe that the ChatGPT detector shows better
performance in differentiating HWTs from MGTs when the
models are from the GPT family, i.e., ChatGLM, ChatGPT-
turbo, and GPT4All. For instance, on Essay, the F1-score
is 0.923, 0.742, and 0.815 for ChatGLM, ChatGPT-turbo,
and GPT4All, respectively. We suspect the reason is that the
ChatGPT detector is trained on the corpus with both human
answers and ChatGPT-generated answers, which can better
capture the “machine” pattern on the GPT model family. On
the other hand, we find that the OpenAI Detector’s perfor-
mance is less satisfying in the detection task (e.g., 0.751 F1-
score on Essay with Dolly-generated texts as MGTs). This
might be credited to the fact that this detector is trained on
MGTs produced by GPT2 while the MGTs produced by
larger LLMs such as ChatGPT-turbo or Claude have higher
quality, which makes it harder to be detected. Regarding
ConDA, it is the weakest detection method. One possible
reason is that ConDA is trained with corpus with shorter
length [9], which makes it harder to transfer to longer texts.
We later show that after fine-tuning, ConDA, OpenAI De-
tector, and ChatGPT Detector can achieve comparable or
even better performance than LM Detector (see Figure 3).
Lastly, we find that GPTZero, a commercial MGT detection
API, reaches good performance except for Dolly on WP and
Reuters as well as StableLM on all datasets. By checking
the distribution of #. words (shown in Figure 1), we find that
the MGTs has fewer #. words in these cases, which leads
to lower performance. It is also mentioned on GPTZero’s
website. Note that for GPTZero, we only sample 1/8 test-
ing data for evaluation, which roughly matches the monthly
word limit for the API subscription starting plan ($49.99).4
Detection Efficiency. We then quantify the time cost of
each detection method on different datasets. Here we con-
sider ChatGPT-turbo as the LLM for a case study since other
4https://app.gptzero.me/app/api-subscription .
5Table 2: The performance (F1-score) of different detection methods. Here OpenAI-D, ChatGPT-D, and LM-D denote the OpenAI
Detector, ChatGPT Detector, and LM Detector. We follow this naming rule in the following tables/figures in this paper. * means we
only sample part of the data (1/8) for testing.
Dataset Method ChatGLM Dolly ChatGPT-turbo GPT4All StableLM Claude
EssayLog-Likelihood 0.970 0.866 0.968 0.923 0.665 0.834
Rank 0.740 0.737 0.915 0.843 0.667 0.772
Log-Rank 0.983 0.865 0.966 0.923 0.692 0.814
Entropy 0.806 0.683 0.874 0.699 0.566 0.771
GLTR 0.988 0.848 0.954 0.925 0.756 0.806
LRR 0.982 0.810 0.925 0.904 0.748 0.746
NPR 0.956 0.865 0.218 0.927 0.740 0.238
DetectGPT 0.891 0.844 0.227 0.908 0.704 0.236
GPTZero* 0.923 0.880 0.980 0.943 0.486 0.870
ConDA 0.668 0.069 0.000 0.260 0.663 0.664
OpenAI-D 0.921 0.724 0.353 0.863 0.774 0.009
ChatGPT-D 0.923 0.630 0.742 0.815 0.491 0.057
LM-D 1.000 0.997 0.993 0.997 0.997 0.980
WPLog-Likelihood 0.980 0.794 0.841 0.934 0.786 0.773
Rank 0.840 0.760 0.797 0.891 0.781 0.709
Log-Rank 0.985 0.807 0.819 0.929 0.832 0.751
Entropy 0.800 0.662 0.770 0.766 0.644 0.731
GLTR 0.983 0.766 0.800 0.935 0.861 0.733
LRR 0.980 0.774 0.728 0.930 0.875 0.656
NPR 0.970 0.801 0.352 0.905 0.764 0.521
DetectGPT 0.812 0.719 0.608 0.808 0.695 0.517
GPTZero* 0.980 0.732 0.980 1.000 0.148 0.818
ConDA 0.585 0.039 0.075 0.674 0.667 0.000
OpenAI-D 0.980 0.776 0.093 0.948 0.937 0.029
ChatGPT-D 0.880 0.528 0.352 0.795 0.616 0.044
LM-D 0.998 0.950 0.990 0.983 0.966 0.970
ReutersLog-Likelihood 0.972 0.381 0.926 0.697 0.659 0.798
Rank 0.650 0.413 0.847 0.665 0.635 0.648
Log-Rank 0.990 0.373 0.944 0.735 0.701 0.785
Entropy 0.477 0.553 0.703 0.668 0.620 0.694
GLTR 0.987 0.556 0.946 0.742 0.750 0.772
LRR 0.992 0.590 0.948 0.796 0.766 0.715
NPR 0.950 0.790 0.284 0.843 0.751 0.560
DetectGPT 0.866 0.782 0.270 0.821 0.756 0.558
GPTZero* 0.980 0.485 0.936 0.980 0.611 0.750
ConDA 0.664 0.137 0.000 0.667 0.000 0.667
OpenAI-D 0.985 0.713 0.954 0.900 0.903 0.000
ChatGPT-D 0.968 0.650 0.931 0.898 0.617 0.019
LM-D 1.000 0.995 0.995 1.000 0.995 0.993
LLMs show similar time costs. As shown in Table 3, we can
observe that most of the detection methods have similar time
costs except NPR, DetectGPT, and GPTZero, which cost sig-
nificantly higher time than the others. This is because NPR
and DetectGPT require multiple rounds of perturbation to the
text to get a good estimation of the metric’s change. Also,
GPTZero needs to query the public API, where the internet
latency has to be taken into account.
In general, we consider LM Detector and Log-Rank as bet-
ter detection methods as they achieve the best detection per-
formance. Moreover, their associated time cost is relativelylow, making them comparable to other metric-based detec-
tion methods. Note that later we omit NPR, DetectGPT, and
GPTZero as they usually have less satisfying performance
and take much longer time/monetary costs than the others.
5.1 Ablation Studies
We now conduct several ablation studies to investigate how
different factors would affect detection performance.
Effect of Text Length. We first present the distribution of #.
words for HWTs and MGTs on different datasets (shown in
Figure 1). We observe that, although we consider to prompt
6HumanChatGLMDolly
ChatGPT-turboGPT4All StableLMClaude100101102103#. Words
(a) Essay
HumanChatGLMDolly
ChatGPT-turboGPT4All StableLMClaude100101102103#. Words
 (b) WP
HumanChatGLMDolly
ChatGPT-turboGPT4All StableLMClaude100101102103#. Words
 (c) Reuters
Figure 1: The distribution of #. words for HWTs and MGTs on different datasets.
10 20 50 100 200 500 All
#. Words0.00.20.40.60.81.0F1-score
Log-Likelihood
Rank
Log-Rank
Entropy
GLTRLRR
ConDA
OpenAI-D
ChatGPT-D
LM-D
(a) ChatGLM
10 20 50 100 200 500 All
#. Words0.00.20.40.60.81.0F1-score
Log-Likelihood
Rank
Log-Rank
Entropy
GLTRLRR
ConDA
OpenAI-D
ChatGPT-D
LM-D (b) Dolly
10 20 50 100 200 500 All
#. Words0.00.20.40.60.81.0F1-score
Log-Likelihood
Rank
Log-Rank
Entropy
GLTRLRR
ConDA
OpenAI-D
ChatGPT-D
LM-D (c) ChatGPT-turbo
10 20 50 100 200 500 All
#. Words0.00.20.40.60.81.0F1-score
Log-Likelihood
Rank
Log-Rank
Entropy
GLTRLRR
ConDA
OpenAI-D
ChatGPT-D
LM-D
(d) GPT4All
10 20 50 100 200 500 All
#. Words0.00.20.40.60.81.0F1-score
Log-Likelihood
Rank
Log-Rank
Entropy
GLTRLRR
ConDA
OpenAI-D
ChatGPT-D
LM-D (e) StableLM
10 20 50 100 200 500 All
#. Words0.00.20.40.60.81.0F1-score
Log-Likelihood
Rank
Log-Rank
Entropy
GLTRLRR
ConDA
OpenAI-D
ChatGPT-D
LM-D (f) Claude
Figure 2: The F1-score of different detection methods under texts with different maximum number of words on Essay.
Table 3: Time cost (seconds) to differentiate texts generated by
ChatGPT or humans.
Method Essay WP Reuters
Log-Likelihood 63 60 62
Rank 75 70 72
Log-Rank 75 70 73
Entropy 50 47 52
GLTR 88 83 85
LRR 138 130 134
NPR 2307 2257 2391
DetectGPT 2185 2142 2272
ConDA 16 16 16
OpenAI-D 16 16 16
ChatGPT-D 16 15 15
LM-D 59 59 59the LLM with #. words that are expected to be generated
(see Table 1), different LLMs demonstrate varying degrees
of proficiency in matching the specified word count. For in-
stance, ChatGPT-turbo and Claude exhibit the highest per-
formance, while Dolly and StableLM exhibit the lowest. We
then take a step further to understand how the number of
words in texts would affect the detection performance. Con-
cretely, we train each detection method with the full-length
texts and test it with the texts that have been truncated to have
maximum Kwords, where K∈[10,20,50,100,200,500 ].
The results on Essay are shown in Figure 2. First, we ob-
serve that, in general, larger Kleads to better detection per-
formance, which is in line with previous work [12, 29, 36].
For instance, the detection performance (F1-score) of Chat-
GLM with Entropy (Figure 2a) increases from 0.150 to 0.809
when Kincreases from 10 to 200. Also, we find that 200
words are generally enough to achieve the (nearly) best per-
710 20 50 100 All
#. Training Samples0.00.20.40.60.81.0F1-score
Log-Likelihood
Rank
Log-Rank
Entropy
GLTRLRR
ConDA
OpenAI-D
ChatGPT-D
LM-D(a) ChatGLM
10 20 50 100 All
#. Training Samples0.00.20.40.60.81.0F1-score
Log-Likelihood
Rank
Log-Rank
Entropy
GLTRLRR
ConDA
OpenAI-D
ChatGPT-D
LM-D (b) Dolly
10 20 50 100 All
#. Training Samples0.00.20.40.60.81.0F1-score
Log-Likelihood
Rank
Log-Rank
Entropy
GLTRLRR
ConDA
OpenAI-D
ChatGPT-D
LM-D (c) ChatGPT-turbo
10 20 50 100 All
#. Training Samples0.00.20.40.60.81.0F1-score
Log-Likelihood
Rank
Log-Rank
Entropy
GLTRLRR
ConDA
OpenAI-D
ChatGPT-D
LM-D
(d) GPT4All
10 20 50 100 All
#. Training Samples0.00.20.40.60.81.0F1-score
Log-Likelihood
Rank
Log-Rank
Entropy
GLTRLRR
ConDA
OpenAI-D
ChatGPT-D
LM-D (e) StableLM
10 20 50 100 All
#. Training Samples0.00.20.40.60.81.0F1-score
Log-Likelihood
Rank
Log-Rank
Entropy
GLTRLRR
ConDA
OpenAI-D
ChatGPT-D
LM-D (f) Claude
Figure 3: The F1-score of different detection methods with different numbers of training samples on Essay.
formance in most of the cases. For instance, for Claude (Fig-
ure 2f), Log-Likelihood reaches F1-score of 0.800 and 0.834
with 200-word texts and full-length texts. This discovery
provides valuable insights into the selection of HWTs and
MGTs for the development of MGT detection methods.
Fine-tune with Fewer Samples. We then investigate the de-
tection effectiveness with fewer training samples and the re-
sults on Essay are shown in Figure 3. Note that different from
the previous evaluation, here we also fine-tune ConDA, Ope-
nAI Detector, and the ChatGPT Detector with those training
samples.
We can observe that, in most cases, 10 training samples
are sufficient for metric-based methods to achieve good de-
tection performance. For instance, when the LLM is Chat-
GLM (Figure 3a), Log-Likelihood reaches 0.967 F1-score
with only 10 training samples, which is close to the perfor-
mance with full training data, i.e., 0.970. On the other hand,
for model-based methods, more training samples can better
facilitate the detection performance. For instance, the F1-
score of LM-D increases from 0.121 (with 10 training sam-
ples) to 0.984 (with full training data). This difference is ex-
pected, as metric-based methods primarily need to determine
a threshold to separate metric values, whereas model-based
methods require the optimization of a large number of pa-
rameters, typically benefiting from a larger training dataset.
Transfer Setting. Given the necessity of the training proce-
dure for most detection methods, we aim to investigate the
efficacy of transferring these methods to different datasets
and LLMs. Note that here we also fine-tune ConDA, Ope-
nAI Detector, and ChatGPT Detector using the same data
with LM Detector for a fair comparison.We first investigate whether the detection methods trained
on one dataset can be transferred to another dataset and the
performance is shown in Figure 4. We can observe that detec-
tion methods trained on different datasets may have different
transferability to the other datasets. Concretely, WP has the
highest transferability while Reuters has the lowest. For in-
stance, for Log-Likelihood trained on WP, the test F1-score
is 0.907 and 0.779 on Essay and Reuters. However, for Log-
Likelihood trained on Reuters, the test F1-score is 0.938 and
0.335 on Essay and WP. One possible reason is that Reuters
only contains news articles from 50 authors, representing a
relatively narrow domain. In contrast, WP covers a wide
range of topics, providing a more diverse dataset. This diver-
sity can better guide the detection method to acquire general
features that facilitate the differentiation between MGTs and
HWTs.
Also, we find that compared to the metric-based methods,
model-based methods like OpenAI Detector and LM Detec-
tor are relatively robust across different datasets. For exam-
ple, given the LM Detector trained on Essay (Figure 4j), the
test F1-score only drops 0.010 and 0.144 on WP and Reuters.
However, for Entropy trained on Essay (Figure 4d), the test
F1-score drops 0.285 and 0.139 on WP and Reuters. This is
because metric-based detection methods usually rely on one
specific metric to perform the detection, which might not be
robust enough for dataset distribution shifts. We take Log-
Likelihood as a case study and visualize its distributions for
HWTs and MGTs (generated by ChatGPT-turbo) across dif-
ferent datasets in Figure 6a. We can observe that for different
datasets, the best threshold to separate HWTs and MGTs is
usually different, making it hard to transfer. On the contrary,
8Essay WP Reuters
Test DatasetEssay WP ReutersTrain Dataset0.968 0.687 0.909
0.907 0.841 0.779
0.938 0.335 0.926(a) Log-Likelihood
Essay WP Reuters
Test DatasetEssay WP ReutersTrain Dataset0.915 0.744 0.843
0.863 0.797 0.782
0.899 0.717 0.847 (b) Rank
Essay WP Reuters
Test DatasetEssay WP ReutersTrain Dataset0.966 0.683 0.907
0.907 0.819 0.777
0.924 0.321 0.944 (c) Log-Rank
Essay WP Reuters
Test DatasetEssay WP ReutersTrain Dataset0.874 0.589 0.735
0.817 0.770 0.685
0.786 0.218 0.703 (d) Entropy
Essay WP Reuters
Test DatasetEssay WP ReutersTrain Dataset0.954 0.725 0.858
0.911 0.800 0.778
0.829 0.195 0.946 (e) GLTR
Essay WP Reuters
Test DatasetEssay WP ReutersTrain Dataset0.925 0.591 0.906
0.880 0.728 0.790
0.688 0.103 0.948
(f) LRR
Essay WP Reuters
Test DatasetEssay WP ReutersTrain Dataset0.998 0.662 0.948
0.703 0.995 0.699
0.668 0.686 0.998 (g) ConDA
Essay WP Reuters
Test DatasetEssay WP ReutersTrain Dataset0.966 0.931 0.911
0.818 0.973 0.925
0.708 0.871 0.968 (h) OpenAI-D
Essay WP Reuters
Test DatasetEssay WP ReutersTrain Dataset0.946 0.656 0.830
0.662 0.888 0.771
0.758 0.341 0.945 (i) ChatGPT-D
Essay WP Reuters
Test DatasetEssay WP ReutersTrain Dataset0.993 0.983 0.849
0.763 0.990 0.906
0.672 0.966 0.995 (j) LM-D
Figure 4: The F1-score of different detection methods when the training dataset and the testing dataset are different. Here The MGTs
are generated by ChatGPT-turbo.
ChatGLMDolly
ChatGPT-turboGPT4All StableLMClaude
Test LLMChatGLM
Dolly
ChatGPT-turbo
GPT4All
StableLM
ClaudeTrain LLM0.970 0.636 0.963 0.845 0.480 0.554
0.934 0.866 0.926 0.898 0.671 0.840
0.983 0.746 0.968 0.910 0.542 0.669
0.973 0.843 0.962 0.923 0.657 0.776
0.847 0.801 0.832 0.823 0.665 0.807
0.941 0.866 0.926 0.902 0.671 0.834
(a) Log-Likelihood
ChatGLMDolly
ChatGPT-turboGPT4All StableLMClaude
Test LLMChatGLM
Dolly
ChatGPT-turbo
GPT4All
StableLM
ClaudeTrain LLM0.740 0.763 0.861 0.835 0.645 0.779
0.727 0.737 0.833 0.814 0.661 0.765
0.774 0.707 0.915 0.832 0.586 0.715
0.761 0.741 0.882 0.843 0.628 0.762
0.667 0.667 0.667 0.667 0.667 0.667
0.737 0.764 0.863 0.835 0.647 0.772 (b) Rank
ChatGLMDolly
ChatGPT-turboGPT4All StableLMClaude
Test LLMChatGLM
Dolly
ChatGPT-turbo
GPT4All
StableLM
ClaudeTrain LLM0.983 0.612 0.952 0.858 0.534 0.521
0.936 0.865 0.926 0.889 0.721 0.816
0.985 0.783 0.966 0.925 0.617 0.671
0.973 0.830 0.959 0.923 0.672 0.749
0.856 0.819 0.847 0.829 0.692 0.802
0.934 0.863 0.924 0.889 0.721 0.814 (c) Log-Rank
ChatGLMDolly
ChatGPT-turboGPT4All StableLMClaude
Test LLMChatGLM
Dolly
ChatGPT-turbo
GPT4All
StableLM
ClaudeTrain LLM0.806 0.651 0.882 0.678 0.502 0.773
0.792 0.683 0.854 0.704 0.529 0.772
0.757 0.534 0.874 0.558 0.426 0.726
0.790 0.685 0.860 0.699 0.531 0.763
0.761 0.712 0.792 0.724 0.566 0.760
0.802 0.655 0.880 0.680 0.502 0.771 (d) Entropy
ChatGLMDolly
ChatGPT-turboGPT4All StableLMClaude
Test LLMChatGLM
Dolly
ChatGPT-turbo
GPT4All
StableLM
ClaudeTrain LLM0.988 0.570 0.908 0.826 0.485 0.432
0.936 0.848 0.926 0.894 0.733 0.809
0.975 0.805 0.954 0.919 0.662 0.683
0.975 0.828 0.949 0.925 0.699 0.751
0.898 0.832 0.889 0.870 0.756 0.795
0.932 0.849 0.922 0.895 0.726 0.806 (e) GLTR
ChatGLMDolly
ChatGPT-turboGPT4All StableLMClaude
Test LLMChatGLM
Dolly
ChatGPT-turbo
GPT4All
StableLM
ClaudeTrain LLM0.982 0.400 0.667 0.750 0.518 0.188
0.932 0.810 0.910 0.879 0.748 0.748
0.966 0.743 0.925 0.905 0.708 0.646
0.968 0.731 0.922 0.904 0.704 0.637
0.932 0.810 0.910 0.879 0.748 0.745
0.917 0.821 0.897 0.867 0.746 0.746
(f) LRR
ChatGLMDolly
ChatGPT-turboGPT4All StableLMClaude
Test LLMChatGLM
Dolly
ChatGPT-turbo
GPT4All
StableLM
ClaudeTrain LLM0.993 0.916 0.010 0.984 0.963 0.000
0.995 0.995 0.029 0.997 0.994 0.020
0.910 0.512 0.998 0.856 0.164 0.861
1.000 1.000 0.010 1.000 0.997 0.030
0.993 0.981 0.084 0.985 0.991 0.000
0.830 0.625 0.890 0.876 0.154 0.998 (g) ConDA
ChatGLMDolly
ChatGPT-turboGPT4All StableLMClaude
Test LLMChatGLM
Dolly
ChatGPT-turbo
GPT4All
StableLM
ClaudeTrain LLM0.993 0.959 0.901 0.995 0.934 0.276
0.993 0.979 0.955 1.000 0.988 0.803
0.975 0.956 0.966 0.965 0.876 0.941
0.988 0.973 0.978 0.977 0.973 0.724
0.995 0.973 0.790 0.997 0.994 0.228
0.888 0.599 0.967 0.746 0.444 0.998 (h) OpenAI-D
ChatGLMDolly
ChatGPT-turboGPT4All StableLMClaude
Test LLMChatGLM
Dolly
ChatGPT-turbo
GPT4All
StableLM
ClaudeTrain LLM0.992 0.676 0.667 0.901 0.580 0.020
0.911 0.893 0.822 0.892 0.734 0.588
0.964 0.785 0.946 0.939 0.559 0.612
0.988 0.951 0.651 0.980 0.846 0.317
0.975 0.915 0.449 0.967 0.968 0.116
0.946 0.907 0.951 0.947 0.797 0.919 (i) ChatGPT-D
ChatGLMDolly
ChatGPT-turboGPT4All StableLMClaude
Test LLMChatGLM
Dolly
ChatGPT-turbo
GPT4All
StableLM
ClaudeTrain LLM1.000 0.800 0.000 0.846 0.783 0.000
1.000 0.997 0.000 1.000 0.994 0.000
0.966 0.881 0.993 0.957 0.164 0.964
1.000 0.997 0.000 0.997 0.963 0.000
0.997 0.997 0.000 1.000 0.997 0.000
0.854 0.858 0.967 0.876 0.221 0.980 (j) LM-D
Figure 5: The F1-score of different detection methods on Essay when the train LLM and the test LLM are different.
model-based methods can extract various features related to
both LLM and human patterns. This feature extraction pro-
cess could enhance the detection performance and make it
more robust to the dataset distribution shift.
We then investigate whether the detection methods trained
on one LLM can be transferred to the other LLMs as well.
The performance on Essay is summarized in Figure 5. Other
datasets show similar trends. We first observe that the
OpenAI Detector in general has better performance when
the training and testing LLMs are different (Figure 5h).
For instance, the OpenAI Detector trained to detect MGTs
generated by ChatGPT-turbo can also detect MGTs gen-
erated by Claude with 0.941 F1-score, which only drops
0.034. Another interesting observation is that, compared tomodel-based methods, metric-based methods are more ro-
bust against LLM shifts, i.e., the variance across different
LLMs is smaller.
To delve deeper into the underlying reasons, we visualize
the log-likelihood distributions for HWTs and MGTs (gener-
ated by different LLMs) on Essay (shown in Figure 6b). We
can observe that, although different LLMs have varied log-
likelihood distributions, they have significantly higher log-
likelihood values than HWTs, and -3.0 log-likelihood is a
good split point to separate HWTs and MGTs from different
LLMs. This implies that MGTs generated by diverse LLMs
differ from HWTs when evaluated using carefully designed
metrics such as log-likelihood.
9−5−4−3−2−1
Log-Likelihood0.00.51.01.52.0DensityEssay, Human
Essay, ChatGPT-turbo
WP, Human
WP, ChatGPT-turbo
Reuters, Human
Reuters, ChatGPT-turbo(a) Different Datasets
−6−5−4−3−2−1 0
Log-Likelihood0.00.51.01.52.0DensityHuman
ChatGLM
Dolly
ChatGPT-turbo
GPT4All
StableLM
Claude
(b) Different LLMs
Figure 6: The log-likelihood distributions of HWTs and MGTs.
Note that (b) is derived from the Essay dataset.
Essay WP Reuters
Dataset0.00.20.40.60.81.0F1-scoreLog-Likelihood
Rank
Log-Rank
Entropy
GLTRLRR
ConDA
OpenAI-D
ChatGPT-D
LM-D
Figure 7: The F1-score of different detection methods on the
text attribution task.5.2 Text Attribution
Previous evaluations have shown the remarkable perfor-
mance of MGT detection, especially with the model-based
methods. We now explore a more difficult task, i.e., text
attribution. Here the goal is to examine whether human-
generated texts or those generated by different LLMs can be
accurately attributed to their respective source models.
We extend previous MGT detection methods to the text
attribution task by increasing the number of classes from 2 to
7 in the classification layer and further fine-tuning the model.
The performance of text attribution with different meth-
ods is summarized in Figure 7. We find that, compared to the
MGT detection task, metric-based detection methods have
less satisfying performance on the text attribution task. For
instance, on Essay, Log-Likelihood reaches 0.970 F1-score
in distinguishing ChatGLM-generated texts from HWTs (Ta-
ble 2). However, the F1-score of Log-Likelihood is only
0.374 in the text attribution task (Figure 7). This is expected
as the text attribution task not only needs to distinguish
whether the text is generated by humans or machines but also
requires the precise prediction of the source model. However,
the specific characteristics among texts generated by differ-
ent LLMs cannot be captured precisely by the metric-based
methods.
We also observe that model-based methods have signifi-
cantly better performance than metric-based methods. For
instance, on WP, ConDA achieves a 0.926 F1-score while
the F1-score is only 0.361, 0.386, and 0.378 for Log-Rank,
GLTR, and LRR, respectively. We take log-likelihood as
an example (Figure 6b), the log-likelihood distributions for
MGTs from different LLMs are hard to separate since they
are overlapped. Conversely, model-based methods can bet-
ter capture the semantic and syntactic relationships between
words and phrases, which greatly improves the attribution
performance on different LLMs.
Training Epochs. We then investigate how the #. training
epochs affects the text attribution performance. Note that
here we only consider the model-based methods as they have
significantly better performance than the metric-based meth-
ods. The results are shown in Figure 8.
We find that the performance keeps improving when the
training epoch increases from 1 to 5, but plateaus from 5 to
20. For instance, on WP, the F1-score of ChatGPT Detec-
tor increases from 0.444 to 0.760 when the training epoch
increases from 1 to 5, while the F1-score is 0.836 with 20
epochs. Therefore, we stop the training after 20 epochs as
the performance is satisfying and the training cost is reason-
able.
Class-Wise Performance. To better investigate the detec-
tion performance on different classes, we visualize the nor-
malized confusion matrix of different detection methods in
Figure 9. Note that here we only present the results on Essay
as other datasets share similar trends.
We can see that, although metric-based methods have ac-
ceptable performance in identifying HWTs and MGTs gen-
erated by ChatGLM, the performance on attributing MGTs
from other LLMs is largely limited. Take LRR as an example
101 2 5 10 20
Training Epochs0.00.20.40.60.81.0F1-score
ConDA
OpenAI-D
ChatGPT-D
LM-D(a) Essay
1 2 5 10 20
Training Epochs0.00.20.40.60.81.0F1-score
ConDA
OpenAI-D
ChatGPT-D
LM-D (b) WP
1 2 5 10 20
Training Epochs0.00.20.40.60.81.0F1-score
ConDA
OpenAI-D
ChatGPT-D
LM-D (c) Reuters
Figure 8: The F1-score of text attribution performance with different training epochs.
HumanChatGLMDolly
ChatGPT-turboGPT4All StableLMClaude
PredictionHuman
ChatGLM
Dolly
ChatGPT-turbo
GPT4All
StableLM
ClaudeTrue0.667 0.000 0.013 0.000 0.000 0.248 0.072
0.000 0.791 0.026 0.124 0.052 0.000 0.007
0.118 0.033 0.157 0.124 0.190 0.183 0.196
0.000 0.255 0.052 0.444 0.235 0.007 0.007
0.013 0.203 0.190 0.196 0.248 0.020 0.131
0.425 0.013 0.098 0.072 0.092 0.170 0.131
0.039 0.026 0.118 0.085 0.150 0.314 0.268
(a) Log-Likelihood
HumanChatGLMDolly
ChatGPT-turboGPT4All StableLMClaude
PredictionHuman
ChatGLM
Dolly
ChatGPT-turbo
GPT4All
StableLM
ClaudeTrue0.699 0.000 0.013 0.000 0.000 0.242 0.046
0.000 0.856 0.007 0.098 0.039 0.000 0.000
0.144 0.020 0.222 0.137 0.163 0.170 0.144
0.000 0.183 0.072 0.503 0.216 0.007 0.020
0.020 0.196 0.196 0.255 0.268 0.013 0.052
0.399 0.013 0.092 0.098 0.111 0.163 0.124
0.092 0.013 0.150 0.078 0.144 0.281 0.242 (b) Log-Rank
HumanChatGLMDolly
ChatGPT-turboGPT4All StableLMClaude
PredictionHuman
ChatGLM
Dolly
ChatGPT-turbo
GPT4All
StableLM
ClaudeTrue0.876 0.000 0.020 0.000 0.000 0.072 0.033
0.000 0.948 0.007 0.033 0.013 0.000 0.000
0.229 0.092 0.085 0.157 0.144 0.190 0.105
0.013 0.281 0.052 0.425 0.176 0.020 0.033
0.026 0.314 0.039 0.131 0.301 0.124 0.065
0.392 0.072 0.046 0.078 0.131 0.235 0.046
0.288 0.026 0.144 0.137 0.046 0.176 0.183 (c) GLTR
HumanChatGLMDolly
ChatGPT-turboGPT4All StableLMClaude
PredictionHuman
ChatGLM
Dolly
ChatGPT-turbo
GPT4All
StableLM
ClaudeTrue0.895 0.000 0.007 0.000 0.000 0.026 0.072
0.000 0.889 0.000 0.013 0.098 0.000 0.000
0.288 0.013 0.157 0.144 0.163 0.026 0.209
0.013 0.052 0.137 0.275 0.431 0.013 0.078
0.026 0.163 0.111 0.157 0.451 0.039 0.052
0.386 0.046 0.078 0.118 0.222 0.033 0.118
0.340 0.000 0.183 0.092 0.065 0.039 0.281 (d) LRR
HumanChatGLMDolly
ChatGPT-turboGPT4All StableLMClaude
PredictionHuman
ChatGLM
Dolly
ChatGPT-turbo
GPT4All
StableLM
ClaudeTrue0.967 0.000 0.000 0.020 0.000 0.013 0.000
0.000 1.000 0.000 0.000 0.000 0.000 0.000
0.000 0.020 0.654 0.000 0.275 0.052 0.000
0.000 0.000 0.000 0.993 0.007 0.000 0.000
0.000 0.013 0.000 0.000 0.980 0.007 0.000
0.000 0.000 0.013 0.000 0.007 0.980 0.000
0.000 0.000 0.000 0.000 0.007 0.000 0.993
(e) ConDA
HumanChatGLMDolly
ChatGPT-turboGPT4All StableLMClaude
PredictionHuman
ChatGLM
Dolly
ChatGPT-turbo
GPT4All
StableLM
ClaudeTrue0.928 0.000 0.007 0.046 0.000 0.007 0.013
0.000 1.000 0.000 0.000 0.000 0.000 0.000
0.000 0.052 0.503 0.000 0.392 0.052 0.000
0.000 0.000 0.000 0.987 0.013 0.000 0.000
0.000 0.163 0.000 0.000 0.830 0.007 0.000
0.000 0.020 0.000 0.000 0.033 0.948 0.000
0.000 0.000 0.000 0.052 0.007 0.000 0.941 (f) OpenAI-D
HumanChatGLMDolly
ChatGPT-turboGPT4All StableLMClaude
PredictionHuman
ChatGLM
Dolly
ChatGPT-turbo
GPT4All
StableLM
ClaudeTrue0.935 0.000 0.007 0.039 0.000 0.013 0.007
0.000 0.961 0.000 0.000 0.039 0.000 0.000
0.000 0.085 0.412 0.000 0.458 0.046 0.000
0.007 0.000 0.013 0.961 0.000 0.013 0.007
0.000 0.085 0.026 0.000 0.889 0.000 0.000
0.000 0.000 0.150 0.000 0.078 0.771 0.000
0.092 0.000 0.000 0.046 0.000 0.000 0.863 (g) ChatGPT-D
HumanChatGLMDolly
ChatGPT-turboGPT4All StableLMClaude
PredictionHuman
ChatGLM
Dolly
ChatGPT-turbo
GPT4All
StableLM
ClaudeTrue0.967 0.000 0.000 0.007 0.000 0.013 0.013
0.000 1.000 0.000 0.000 0.000 0.000 0.000
0.000 0.052 0.641 0.000 0.281 0.026 0.000
0.000 0.000 0.000 0.993 0.007 0.000 0.000
0.000 0.052 0.026 0.000 0.915 0.007 0.000
0.000 0.007 0.039 0.000 0.007 0.948 0.000
0.039 0.000 0.000 0.150 0.000 0.007 0.804 (h) LM-D
Figure 9: The normalized confusion matrix of text attribution with different methods on Essay. Note that the values in the diagonal
represent the class-wise accuracy. The first row presents 4 (best-performing) metric-based methods. The second row shows 4 model-
based methods.
(see Figure 9d), the prediction accuracy of Human and Chat-
GLM is 0.895 and 0.889, while the prediction accuracy of
Dolly, ChatGPT-turbo, and Claude is only 0.157, 0.275, and
0.281, respectively. This is expected due to potential overlap
in the distribution of the metric among various LLMs, which
introduces extra challenges in attribution.
On the other hand, model-based methods have almost per-
fect performance in identifying HWTs (over 0.928 accuracy),
and better performance in attributing the source model of
MGTs. For instance, ConDA achieves 0.993, 0.980, and
0.0.993 accuracy in attributing texts generated by ChatGPT-
turbo, GP4All, and Claude, respectively. This suggests that
model-based methods are more suitable for the text attribu-
tion task, as they excel in capturing context, coherence, and
long-range dependencies.
Broadly speaking, our findings suggest that the model-based methods excel in the text attribution task. However,
they do exhibit challenges in accurately classifying MGTs
within a specific category, e.g., Dolly. This underscores the
necessity for the development of more advanced and effec-
tive text attribution techniques.
5.3 Adversarial Attacks
Our previous evaluation demonstrates the effectiveness of
MGT detection methods. We then take a step further to eval-
uate whether those methods are robust against adversarial at-
tacks. Concretely, we consider three different attack strate-
gies:
•Paraphrasing. This attack replaces each sentence with
its paraphrase using a paraphrasing model.
11Table 4: The performance degradation (F1-score) caused by the three attack strategies. Each cell contains three values. The first,
second, and third values represent performance degradation caused by paraphrasing, random spacing, and adversarial perturbation,
respectively. The best strategy in each cell is highlighted in bold. Note that we round the value to two decimal places to ease the reading
process.
Dataset Method ChatGLM Dolly ChatGPT-turbo GPT4All StableLM Claude
EssayLog-Likelihood 0.37|0.56|0.77 0.24|0.54|0.65 0.22|0.75|0.72 0.29 |0.76|0.70 0.14 |0.30|0.22 0.24 |0.65|0.50
Rank -0.10|0.45|0.59 0.11|0.52|0.65 0.09|0.83|0.87 0.07|0.67|0.62 0.00 |0.00|0.00 0.04 |0.66|0.67
Log-Rank 0.47|0.41|0.76 0.23|0.51|0.63 0.22|0.70|0.71 0.28|0.61|0.70 0.18|0.27|0.22 0.21 |0.56|0.43
Entropy 0.05|0.37|0.59 0.20|0.37|0.45 0.26|0.61|0.71 0.21|0.48|0.55 0.18|0.26|0.16 0.28 |0.50|0.47
GLTR 0.61|0.21|0.68 0.21|0.43|0.50 0.19|0.52|0.57 0.27|0.50|0.67 0.21|0.22|0.22 0.23|0.44|0.37
LRR 0.83|0.27|0.77 0.22 |0.34|0.54 0.25|0.55|0.70 0.32|0.39|0.68 0.28 |0.28|0.27 0.20 |0.35|0.35
ConDA 0.99|0.00|0.47 0.00 |0.00|0.00 0.03 |0.01|0.54 0.00|0.01|0.48 0.00|0.00|0.10 0.32 |0.02|0.12
OpenAI-D 0.12|0.18|0.73 -0.00|0.10|0.57 0.00|0.13|0.84 0.00|0.31|0.68 0.00|0.20|0.16 0.21 |0.00|0.38
ChatGPT-D 0.30|0.94|0.89 0.00 |0.10|0.23 -0.01|0.86|0.33 0.01 |0.18|0.57 -0.01|0.17|0.12 -0.02 |0.23|0.17
LM-D 1.00|0.00|0.04 0.00 |0.07|0.88 0.00|0.00|0.97 0.00|0.01|0.69 0.00|0.00|0.16 0.00|0.01|0.94
WPLog-Likelihood 0.75|0.43|0.90 0.49|0.45|0.62 0.53|0.60|0.47 0.61 |0.63|0.82 0.50 |0.45|0.15 0.65|0.61|0.40
Rank 0.04|0.33|0.84 0.18|0.36|0.70 0.28|0.63|0.48 0.23 |0.55|0.86 0.25|0.33|0.50 0.26|0.48|0.48
Log-Rank 0.75|0.35|0.87 0.48|0.41|0.63 0.51|0.56|0.47 0.58 |0.44|0.77 0.53 |0.43|0.14 0.60|0.52|0.36
Entropy 0.34|0.25|0.56 0.29|0.25|0.43 0.44 |0.40|0.42 0.41 |0.40|0.58 0.38 |0.26|0.10 0.50|0.41|0.32
GLTR 0.73|0.16|0.60 0.42 |0.31|0.50 0.45|0.46|0.46 0.53|0.38|0.63 0.56 |0.34|0.09 0.56|0.44|0.27
LRR 0.79|0.20|0.78 0.44 |0.22|0.59 0.42|0.41|0.47 0.60|0.27|0.64 0.64 |0.33|0.14 0.48|0.31|0.21
ConDA 0.00|0.00|0.01 0.00|0.00|0.07 0.00|0.00|0.02 0.01|0.01|0.09 0.01 |0.00|0.00 0.86|0.12|0.18
OpenAI-D 0.01|0.03|0.69 0.01|0.10|0.59 0.00|0.00|0.36 0.00|0.08|0.58 0.01|0.11|0.09 0.18|0.00|0.05
ChatGPT-D -0.01|0.90|0.91 -0.01|0.16|0.21 -0.00|0.74|0.30 -0.00 |0.66|0.48 -0.01 |0.50|0.05 0.04 |0.37|0.07
LM-D 0.00|0.02|0.99 -0.00|0.04|0.89 0.00|0.00|0.55 0.00 |0.11|0.96 0.14|0.42|0.94 -0.00|0.06|0.97
ReutersLog-Likelihood 0.57|0.78|0.69 -0.23 |-0.38|-0.21 0.31|0.62|0.76 0.31|0.64|0.51 0.35 |0.53|0.34 0.44 |0.77|0.62
Rank 0.02|0.46|0.49 -0.09 |-0.40|-0.22 0.11 |0.80|0.82 0.13|0.56|0.46 0.12 |0.47|0.32 0.04 |0.62|0.52
Log-Rank 0.54|0.59|0.70 -0.21 |-0.37|-0.22 0.28 |0.49|0.76 0.33|0.63|0.52 0.37 |0.56|0.36 0.41 |0.74|0.62
Entropy 0.19|0.37|0.37 -0.16|-0.23|-0.14 0.32|0.63|0.61 -0.09|-0.16|-0.17 -0.11|-0.16|-0.15 0.42 |0.63|0.52
GLTR 0.52|0.32|0.68 0.13 |-0.02|-0.10 0.33 |0.33|0.67 0.30|0.56|0.50 0.33 |0.50|0.36 0.39 |0.65|0.61
LRR 0.54|0.32|0.70 0.22|0.33|0.19 0.35 |0.31|0.74 0.30|0.48|0.54 0.39|0.47|0.46 0.29 |0.54|0.57
ConDA 0.00|0.00|0.00 0.00 |0.00|0.01 0.00|0.00|0.00 0.00 |0.00|0.00 -0.01 |-0.01|0.06 0.00|0.00|0.00
OpenAI-D 0.01|0.10|0.51 0.00|0.23|0.13 -0.00 |0.18|0.80 0.00|0.22|0.35 0.00|0.18|0.14 0.05 |0.00|0.58
ChatGPT-D 0.01|0.90|0.79 -0.02 |0.07|0.15 0.00|0.74|0.53 0.00 |0.55|0.50 -0.00 |0.41|0.20 -0.08 |0.29|0.47
LM-D 0.00|0.00|0.71 0.00|0.00|0.20 -0.01|0.01|0.99 0.00|0.00|0.59 -0.01|0.01|0.36 -0.00|0.01|0.74
•Random Spacing. This attack randomly inserts spaces
into the given texts.
•Adversarial Perturbation. This attack optimizes per-
turbations to deceive a target detector.
we evaluate the attack performance on the MGT detection
task and we consider the fine-tuned version of all model-
based methods. For paraphrasing, we consider ChatGPT
Paraphraser5as the paraphrasing model. As to random spac-
ing, we consider inserting space in each position of a text
with 1% probability. Regarding adversarial perturbation, we
consider the attack proposed by Ren et al. [28] and leverage
the TextAttack6library to implement the attack. We consider
the LM Detector as the target detector for adversarial pertur-
bation. Note that the attack only perturbs MGTs by applying
one attack strategy. The HWTs remain unchanged.
We evaluate the effectiveness of the attack by measuring
the F1-score degradation after applying the attack. The re-
sults are summarized in Table 4. We observe that, in general,
adversarial perturbation is the most effective strategy. For
instance, on Essay with ChatGPT-turbo, the adversarial per-
turbation degrades 0.97 F1-score for LM Detector. Also, this
5https://huggingface.co/datasets/humarin/chatgpt-
paraphrases .
6https://github.com/QData/TextAttack/ .perturbation can transfer to other detection methods as well,
e.g., the F1-score degradation is 0.84, 0.54, and 0.70 for Ope-
nAI Detector, ConDA, and LRR on Essay with ChatGPT-
turbo. Another observation is that paraphrasing and ran-
dom spacing are more effective against metric-based meth-
ods than model-based methods. For example, on Reuters
with Claude, paraphrasing and random spacing degrade the
F1-score of Log-Likelihood by 0.44 and 0.77, while only
0.05 and 0.00 for OpenAI-D. This could be attributed to the
fact that paraphrasing or random spacing alters word dis-
tribution, potentially influencing the metrics calculated by
metric-based methods. However, since these methods do
not significantly change the semantic meaning, their effect
on model-based methods is comparatively lower.
Overall, our findings indicate that MGT detection methods
are vulnerable to potential adversarial attacks. This prompts
the need for developing more robust detection methods. Pos-
sible directions could be combining input filtering, data aug-
mentation, and adversarial training into existing detection
methods.
6 Limitations and Discussions
Choice of LLM/Methods/Datasets. In our current study, we
have concentrated on 6 representative LLMs and 13 detection
methods on 3 benchmarking datasets. We agree that experi-
12ments with new LLMs with billions of parameters would be
fantastic, but they would exhaust our technical capabilities
and provide little further insights. Regarding methods, our
implemented methods are mainly based on pre-defined met-
rics (metric-based) or pre-trained LMs (model-based). Nev-
ertheless, as LLMs continue to advance, we recognize the po-
tential of incorporating novel approaches such as prompt tun-
ing and in-context learning into MGTBench as well. These
methodologies have demonstrated remarkable efficacy and
efficiency in adapting to new tasks, suggesting a valuable
direction for future exploration and integration within our
framework. Our modular design of MGTBench is a signif-
icant strength, enabling seamless scalability and adaptabil-
ity. This flexibility allows for the incorporation of an array
of new detection methods and LLMs as they emerge, en-
suring that MGTBench remains at the forefront of develop-
ments in this rapidly evolving field. Regarding the datasets,
our current datasets cover aspects ranging from news articles
(Reuters) to creative prompts (WP). We acknowledge that
there are also other domains of datasets with varied topics,
word counts, and even languages. Expanding our datasets to
encompass this diversity would undoubtedly enrich the de-
velopment of MGT detection methods.
Security Implication. Designing and benchmarking MGT
detection methods has important security implications in the
following aspects. First, detecting MGT effectively can stifle
the spread of automated misinformation campaigns, which
can be used to manipulate public sentiment, sway elections,
or incite unrest. Second, Phishing emails or scam messages
can be tailored using generative AI. A robust MGT detec-
tion system can help in identifying and blocking such at-
tempts, safeguarding individuals and institutions from po-
tential financial losses. Also, for sectors like journalism,
academia, and legal processes, the authenticity of informa-
tion is paramount MGT detection ensures the integrity of
content in these fields, thereby preserving trust. Moreover, as
AI systems become more integral in various applications, ad-
versarial attacks, where AI-generated content is used to fool
other AI systems, will become more prevalent. An MGT de-
tection framework can act as a first line of defense against
such attacks. Another implication is that as users become
more aware of AI’s capabilities, there’s a growing skepticism
towards digital content. Effective MGT detection can allevi-
ate these concerns, ensuring users can trust the platforms they
engage with.
Broader Implication in Generative AI. MGTBench pro-
vides a uniform platform where different detection method-
ologies can be evaluated and compared. This standardiza-
tion can drive the development of more effective, efficient,
and universally accepted MGT detection techniques. Also,
knowing the strengths and weaknesses of generative mod-
els through a detection benchmark can guide researchers in
refining these models, potentially leading to more advanced
and indistinguishable MGTs. Moreover, our work signifies
the importance of the MGT detection issue, raising aware-
ness among the public. This can lead to better-informed con-
sumers of digital content and a more nuanced understanding
of generative AI’s capabilities and limitations.7 Conclusion
In this paper, we perform the first systematic quantification
of existing MGT detection methods under the representa-
tive powerful LLMs. Specifically, we consider 8 metric-
based detection methods and 5 model-based detection meth-
ods. Our analysis reveals that the LM Detector consistently
excels across various datasets. Additionally, our ablation
studies indicate a shortcoming of current methods in accu-
rately distinguishing MGTs with a limited number of words.
Interestingly, most detection methods demonstrate the abil-
ity to achieve satisfactory performance with considerably
fewer training samples. Moreover, we observe that metric-
based methods exhibit better adaptability to different LLMs,
whereas model-based methods show superior flexibility in
transitioning across diverse datasets.
We also explore the feasibility of applying existing MGT
detection methods to text attribution, a notably more complex
task. Our findings indicate that model-based methods signif-
icantly outperform their metric-based counterparts. This ef-
fectiveness is attributed to the model-based approaches’ pro-
ficiency in capturing semantic and syntactic nuances within
texts, whereas metric-based methods, dependent mainly on
specific metrics, struggle to differentiate between source
LLMs in the text attribution task.
Further, we assess the robustness of MGT detection meth-
ods by introducing three different adversarial attacks, namely
paraphrasing, random spacing, and adversarial perturbation.
Our evaluation uncovers a pronounced vulnerability across
various detection methods to these attacks. This assessment
highlights the need to develop more robust MGT detection
methods.
To facilitate research in this domain, we integrate the de-
tection methods as well as datasets into a modular-designed
framework named MGTBench. We envision that MGT-
Bench will serve as a benchmark tool to expedite future re-
search endeavors in enhancing MGT detection methodolo-
gies and refining the training processes of LLMs.
13References
[1] ChatGLM. https://github.com/THUDM/ChatGLM-
6B. 3
[2] Dolly. https://github.com/databrickslabs/
dolly . 3
[3] GPTZero. https://gptzero.me/ . 1, 4
[4] StableLM. https://huggingface.co/
stabilityai/stablelm-tuned-alpha-7b . 3
[5] ChatGPT. https://chat.openai.com/chat . 1, 2
[6] Claude. https://claude.ai . 1, 3
[7] Yuvanesh Anand, Zach Nussbaum, Brandon Duder-
stadt, Benjamin Schmidt, and Andriy Mulyar. Gpt4all:
Training an assistant-style chatbot with large scale
data distillation from gpt-3.5-turbo. https://github.
com/nomic-ai/gpt4all , 2023. 3
[8] Sameer Badaskar, Sachin Agarwal, and Shilpa Arora.
Identifying Real or Fake Articles: Towards better Lan-
guage Modeling. In International Joint Conference on
Natural Language Processing (IJCNLP) , pages 817–
822. ACL, 2008. 3
[9] Amrita Bhattacharjee, Tharindu Kumarage, Raha
Moraffah, and Huan Liu. Conda: Contrastive do-
main adaptation for ai-generated text detection. In An-
nual Meeting of the Association for Computational Lin-
guistics and International Joint Conference on Natural
Language Processing (ACL/IJCNLP) , pages 598–610.
ACL, 2023. 4, 5
[10] Sid Black, Stella Biderman, Eric Hallahan, Quentin
Anthony, Leo Gao, Laurence Golding, Horace He,
Connor Leahy, Kyle McDonell, Jason Phang, Michael
Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria
Reynolds, Jonathan Tow, Ben Wang, and Samuel Wein-
bach. Gpt-neox-20b: An open-source autoregressive
language model. CoRR abs/2204.06745 , 2022. 3
[11] Tom B. Brown, Benjamin Mann, Nick Ryder,
Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry,
Amanda Askell, Sandhini Agarwal, Ariel Herbert-
V oss, Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
Clemens Winter, Christopher Hesse, Mark Chen, Eric
Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
Jack Clark, Christopher Berner, Sam McCandlish, Alec
Radford, Ilya Sutskever, and Dario Amodei. Language
Models are Few-Shot Learners. In Annual Conference
on Neural Information Processing Systems (NeurIPS) .
NeurIPS, 2020. 1, 2
[12] Souradip Chakraborty, Amrit Singh Bedi, Sicheng Zhu,
Bang An, Dinesh Manocha, and Furong Huang. On
the possibilities of ai-generated text detection. CoRR
abs/2304.04736 , 2023. 7[13] Aakanksha Chowdhery, Sharan Narang, Jacob De-
vlin, Maarten Bosma, Gaurav Mishra, Adam Roberts,
Paul Barham, Hyung Won Chung, Charles Sutton, Se-
bastian Gehrmann, Parker Schuh, Kensen Shi, Sasha
Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker
Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prab-
hakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner
Pope, James Bradbury, Jacob Austin, Michael Isard,
Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm
Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk
Michalewski, Xavier Garcia, Vedant Misra, Kevin
Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito,
David Luan, Hyeontaek Lim, Barret Zoph, Alexan-
der Spiridonov, Ryan Sepassi, David Dohan, Shivani
Agrawal, Mark Omernick, Andrew M. Dai, Thanu-
malayan Sankaranarayana Pillai, Marie Pellat, Aitor
Lewkowycz, Erica Moreira, Rewon Child, Oleksandr
Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang,
Brennan Saeta, Mark Diaz, Orhan Firat, Michele
Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas
Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. PaLM:
Scaling Language Modeling with Pathways. CoRR
abs/2204.02311 , 2022. 1
[14] Paul F. Christiano, Jan Leike, Tom B. Brown, Miljan
Martic, Shane Legg, and Dario Amodei. Deep Re-
inforcement Learning from Human Preferences. In
Annual Conference on Neural Information Processing
Systems (NIPS) , pages 4299–4307. NIPS, 2017. 2
[15] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. BERT: Pre-training of Deep Bidi-
rectional Transformers for Language Understanding. In
Conference of the North American Chapter of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies (NAACL-HLT) , pages 4171–4186.
ACL, 2019. 2
[16] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding,
Jiezhong Qiu, Zhilin Yang, and Jie Tang. GLM: Gen-
eral Language Model Pretraining with Autoregressive
Blank Infilling. In Annual Meeting of the Association
for Computational Linguistics (ACL) , pages 320–335.
ACL, 2022. 3
[17] Sebastian Gehrmann, Hendrik Strobelt, and Alexan-
der M. Rush. GLTR: Statistical Detection and Visual-
ization of Generated Text. In Annual Meeting of the As-
sociation for Computational Linguistics (ACL) , pages
111–116. ACL, 2019. 1, 3
[18] Biyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang,
Jinran Nie, Yuxuan Ding, Jianwei Yue, and Yupeng
Wu. How Close is ChatGPT to Human Experts? Com-
parison Corpus, Evaluation, and Detection. CoRR
abs/2301.07597 , 2023. 1, 3
[19] John Houvardas and Efstathios Stamatatos. N-gram
feature selection for authorship identification. In Ar-
tificial Intelligence: Methodology, Systems, and Appli-
cations (AIMSA) , pages 77–86. Springer, 2006. 4
14[20] Daphne Ippolito, Daniel Duckworth, Chris Callison-
Burch, and Douglas Eck. Automatic Detection of Gen-
erated Text is Easiest when Humans are Fooled. In
Annual Meeting of the Association for Computational
Linguistics (ACL) , pages 1808–1822. ACL, 2020. 3, 4
[21] Stephanie Lin, Jacob Hilton, and Owain Evans. Truth-
fulQA: Measuring How Models Mimic Human False-
hoods. In Annual Meeting of the Association for Com-
putational Linguistics (ACL) , pages 3214–3252. ACL,
2022. 1
[22] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke
Zettlemoyer, and Veselin Stoyanov. RoBERTa: A Ro-
bustly Optimized BERT Pretraining Approach. CoRR
abs/1907.11692 , 2019. 2
[23] Eric Mitchell, Yoonho Lee, Alexander Khazatsky,
Christopher D. Manning, and Chelsea Finn. Detect-
GPT: Zero-Shot Machine-Generated Text Detection us-
ing Probability Curvature. CoRR abs/2301.11305 ,
2023. 1, 3
[24] Jiameng Pu, Zain Sarwar, Sifat Muhammad Abdul-
lah, Abdullah Rehman, Yoonjin Kim, Parantapa Bhat-
tacharya, Mobin Javed, and Bimal Viswanath. Deep-
fake Text Detection: Limitations and Opportunities.
InIEEE Symposium on Security and Privacy (S&P) .
IEEE, 2023. 3, 4
[25] Alec Radford, Karthik Narasimhan, Tim Salimans, and
Ilya Sutskever. Improving Language Understanding by
Generative Pre-Training. 2016. 2
[26] Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, and Ilya Sutskever. Language Mod-
els are Unsupervised Multitask Learners. OpenAI blog ,
2019. 2
[27] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei
Li, and Peter J. Liu. Exploring the Limits of Trans-
fer Learning with a Unified Text-to-Text Transformer.
Journal of Machine Learning Research , 2020. 1
[28] Shuhuai Ren, Yihe Deng, Kun He, and Wanxiang Che.
Generating Natural Language Adversarial Examples
through Probability Weighted Word Saliency. In An-
nual Meeting of the Association for Computational Lin-
guistics (ACL) , pages 1085–1097. ACL, 2019. 12
[29] Irene Solaiman, Miles Brundage, Jack Clark, Amanda
Askell, Ariel Herbert-V oss, Jeff Wu, Alec Radford, and
Jasmine Wang. Release Strategies and the Social Im-
pacts of Language Models. CoRR abs/1908.09203 ,
2019. 1, 3, 7
[30] Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M.
Ziegler, Ryan Lowe, Chelsea V oss, Alec Radford,
Dario Amodei, and Paul F. Christiano. Learn-
ing to summarize from human feedback. CoRR
abs/2009.01325 , 2020. 2, 3[31] Jinyan Su, Terry Yue Zhuo, Di Wang, and Preslav
Nakov. Detectllm: Leveraging log rank information for
zero-shot detection of machine-generated text. CoRR
abs/2306.05540 , 2023. 3
[32] Teo Susnjak. ChatGPT: The End of Online Exam In-
tegrity? CoRR abs/2212.09292 , 2022. 1
[33] Adaku Uchendu, Thai Le, Kai Shu, and Dongwon Lee.
Authorship Attribution for Neural Text Generation. In
Conference on Empirical Methods in Natural Language
Processing (EMNLP) , pages 8384–8395. ACL, 2020. 3
[34] Adaku Uchendu, Zeyu Ma, Thai Le, Rui Zhang, and
Dongwon Lee. TURINGBENCH: A Benchmark En-
vironment for Turing Test in the Age of Neural Text
Generation. In Conference on Empirical Methods in
Natural Language Processing (EMNLP) , pages 2001–
2016. ACL, 2021. 3, 4
[35] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. Attention is All you Need.
InAnnual Conference on Neural Information Process-
ing Systems (NIPS) , pages 5998–6008. NIPS, 2017. 2
[36] Vivek Verma, Eve Fleisig, Nicholas Tomlin, and Dan
Klein. Ghostbuster: Detecting text ghostwritten by
large language models. CoRR abs/2305.15047 , 2023.
4, 7
[37] Laura Weidinger, John Mellor, Maribeth Rauh, Conor
Griffin, Jonathan Uesato, Po-Sen Huang, Myra Cheng,
Mia Glaese, Borja Balle, Atoosa Kasirzadeh, Zac
Kenton, Sasha Brown, Will Hawkins, Tom Steple-
ton, Courtney Biles, Abeba Birhane, Julia Haas, Laura
Rimell, Lisa Anne Hendricks, William S. Isaac, Sean
Legassick, Geoffrey Irving, and Iason Gabriel. Ethical
and social risks of harm from Language Models. CoRR
abs/2112.04359 , 2021. 1
[38] Rowan Zellers, Ari Holtzman, Hannah Rashkin,
Yonatan Bisk, Ali Farhadi, Franziska Roesner, and
Yejin Choi. Defending Against Neural Fake News. In
Annual Conference on Neural Information Processing
Systems (NeurIPS) , pages 9051–9062. NeurIPS, 2019.
1, 3
15