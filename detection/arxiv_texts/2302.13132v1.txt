Hierarchical Needs-driven Agent Learning Systems:
From Deep Reinforcement Learning To Diverse Strategies
Qin Yang
The Department of Computer Science at the University of Georgia
Email: RickYang2014@gmail.com; qy03103@uga.edu
Abstract
The needs describe the necessities for a self-organizing sys-
tem to survive and evolve, which arouses an agent to ac-
tion toward a goal, giving purpose and direction to behav-
ior. Based on Maslow’s hierarchy of needs, an agent needs
to satisfy a certain amount of needs at the current level as a
condition to arise at the next stage – upgrade and evolution.
Especially, Deep Reinforcement Learning (DAL) can help
AI agents (like robots) organize and optimize their behav-
iors and strategies to develop diverse Strategies based on their
current state and needs (expected utilities or rewards). This
paper introduces the new hierarchical needs-driven Learning
systems based on DAL and investigates the implementation
in the single-robot with a novel approach termed Bayesian
Soft Actor-Critic (BSAC). Then, we extend this topic to
the Multi-Agent systems (MAS), discussing the potential re-
search ﬁelds and directions.
Introduction
In nature, the interaction between and within various ele-
ments of a system is complex (Chan 2001). Many natural
systems (e.g., brains, immune system, ecology, societies)
and artiﬁcial systems (robotic and AI systems) are charac-
terized by apparently complex behaviors that emerge as a
result of often nonlinear spatiotemporal interactions among
a large number of component systems at different levels of
the organization (Levin 1998). For example, to maximize
the chance of detecting predators, forage, and save more
energy while migrating to different locations, animals usu-
ally split into different swarms to minimize their aggregated
threat and maximize beneﬁts according to different scenar-
ios forming complex adaptive systems. Formally, accord-
ing to the expected utility hypothesis (V on Neumann and
Morgenstern 2007), considering the probabilities of agents’
needs in the speciﬁc situation, their decisions always want to
maximize expected utilities (needs) and also maximize the
probability of the decision’s consequences being preferable
to some uncertain threshold. Furthermore, as the combina-
tion of hierarchical needs, intelligent agents develop various
strategies and skills to adapt to different scenarios, satisfy-
ing or maximizing their current dominant needs based on
the corresponding rewards (expected utility/needs) mecha-
nism through learning from the interaction with other agents
and environments.
Joint Probability Distributions
Action Combination T  t
1
 t
4 t
2
 t
5Each Joint 
Action Space
ti, i  {1, …, 5}∈
PT = P(t1)P(t2 | t1)P(t3 | t1)P(t4 | t2)P(t5 | t3)
Hip 
(t1)
Left Knee
(t2)Right Knee
(t3)
Left Ankle
(t4)Right Ankle
(t5)Biped Robot
 t3Figure 1: An example biped robot model’s Bayesian Strat-
egy Network, showing a decomposed strategy based on ac-
tion dependencies.
In Artiﬁcial Intelligence (AI) methods, a strategy de-
scribes the general plan of an AI agent1achieving short-term
or long-term goals under conditions of uncertainty, which
involves setting sub-goals and priorities, determining action
sequences to fulﬁll the tasks, and mobilizing resources to ex-
ecute the actions (Freedman 2015). It exhibits the fundamen-
tal properties of agents’ perception, reasoning, planning,
decision-making, learning, problem-solving, and communi-
cation in interaction with dynamic and complex environ-
ments (Langley, Laird, and Rogers 2009; Yang et al. 2019).
Especially in the ﬁeld of real-time strategy (RTS) game
(Buro 2003; Yang and Parasuraman 2022) and real-world
implementation scenarios like robot-aided urban search and
rescue (USAR) missions (Murphy 2014; Yang and Parasur-
aman 2020b), agents need to dynamically change the strate-
gies adapting to the current situations based on the environ-
ments and their expected utilities or needs (Yang and Para-
suraman 2020a, 2021).
From the agent perspective, a strategy is a rule used by
agents to select an action to pursue goals, which is equiva-
lent to a policy in a Markov Decision Process (MDP) (Rizk,
Awad, and Tunstel 2018). In reinforcement learning (RL),
1This paper uses the terms agent and robot interchangeably.arXiv:2302.13132v1  [cs.AI]  25 Feb 2023Joint Probability Distributions
Final Strategy T t
1
 t
3 t
2
 t
4state 
   S
p
1
p
1
(at11, …, at1n) t1 Action Space
Probability
DistributionsP1 = P(t1)
t2 Action Space
Probability
DistributionsP2 = P(t2 | t1)
t3 Action Space
Probability
DistributionsP3 = P(t3 | t2 )
t4 Action Space
Probability
DistributionsP4 = P(t4 | t2)p2p2π1( · | s; θ1 )
p2
p4p3
T Joint 
ProbabilityT Policy Net
π( AT | s; θ  )
(Actor)(at21, …, at2n)
(at31, …, at3n)
(at41, …, at4n)π2( · | s; θ2 )
π3( · | s; θ3 )π4( · | s; θ4 )
Strategy TValue Net
q( s, AT; w)
(Critic)
(a) Bayesian Strategy Network (b) Actor-Critic Model ArchitecturePT = P(t1)P(t2 | t1)P(t3 | t2)P(t4 | t2)Figure 2: An overview of the proposed BSN based implementation of the Actor-Critic DRL architecture model
the policy dictates the actions that the agent takes as a func-
tion of its state and the environment, and the goal of the
agent is to learn a policy maximizing the expected cumu-
lative rewards in the process. With advancements in deep
neural network implementations, deep reinforcement learn-
ing (DRL) helps AI agents master more complex strate-
gies (policies) and represents a step toward building au-
tonomous systems with a higher-level understanding of the
visual world (Arulkumaran et al. 2017).
Speciﬁcally, reinforcement learning is a framework that
helps develop self-learning capability in robots, but it is lim-
ited to the lower-dimensional problem because of complex-
ity in memory and computation; Deep RL integrates the
deep neural network implementing function approximation
andrepresentation learning to overcome the limitation of
RL (Singh, Kumar, and Singh 2021). On the other hand,
current research and industrial communities have sought
more software-based control solutions using low-cost sen-
sors with less operating environment requirements and cali-
bration (Liu et al. 2021a). As the most promising algorithm,
DRL ideally suits robotic manipulation and locomotion be-
cause of no predeﬁned training data requirement. Further-
more, the control policy could be obtained by learning and
updating instead of hard-coding directions to coordinate all
the joints.
Background and Preliminaries
This section provides the essential background about Agent
Needs Hierarchy andDeep Reinforcement Learning When
describing a speciﬁc method, we use the notations and rela-
tive deﬁnitions from the corresponding papers.
Agent Needs Hierarchy
InAgent Needs Hierarchy (Yang and Parasuraman 2020a),
the abstract needs of an agent for a given task are prioritizedand distributed into multiple levels, each of them precondi-
tioned on their lower levels. At each level, it expresses the
needs as an expectation over the corresponding factors/fea-
tures’ distribution to the speciﬁc group (Yang and Parasura-
man 2020b).
Speciﬁcally, it deﬁnes ﬁve different levels of agent needs
similar to Maslow’s human needs pyramid (Yang and Para-
suraman 2021, 2023a). The lowest (ﬁrst) level is the safety
features of the agent (e.g., features such as collision detec-
tion, fault detection, etc., that assure safety to the agent,
human, and other friendly agents in the environment). The
safety needs (Eq. (1)) can be calculated through its innate
value and the probability of corresponding safety features
based on the agent’s current state. After satisfying safety
needs, the agent considers its basic needs (Eq. (2)), which
includes features such as energy levels and data commu-
nication levels that help maintain the basic operations of
that agent. Only after ﬁtting the safety and basic needs, an
agent can consider its capability needs (Eq. (3)), which are
composed of features such as its health level, computing
(e.g., storage, performance), physical functionalities (e.g.,
resources, manipulation), etc. At the next higher level, the
agent can identify its teaming needs (Eq. (4)) that account
for the contributions of this agent to its team through sev-
eral factors (e.g., heterogeneity, trust, actions) that the team
needs so that they can form a reliable and robust team to
perform a given mission successfully.
Ultimately, at the highest level, the agent learns skills/fea-
tures to improve its capabilities and performance in achiev-
ing a given task, such as Reinforcement Learning. The pol-
icy features (Q table or reward function) are accounted into
its learning needs expectation (Eq. (5)). The expectation of
agent needs at each level are given below:
Safety Needs :Nsj=sjX
i=1SiP(SijXj;T); (1)Hopper-v2
[
t1
t2
t3]
Figure 3: BSN 3P Model in Hopper
Walker2d-v2
t4t2[
t1
t3
t5] Figure 4: BSN 5P Model in Walker2d
Humanoid-v2
t4t2[
t3
t5 ]
t1 Figure 5: BSN 5P Model in Humanoid
Basic Needs :Nbj=bjX
i=1BiP(BijXj;T;Nsj); (2)
CapabilityNeeds :Ncj=cjX
i=1CiP(CijXj;T;Nbj);(3)
Teaming Needs :Ntj=tjX
i=1TiP(TijXj;T;Ncj);(4)
Learning Needs :Nlj=ljX
i=1LiP(LijXj;T;Ntj);(5)
Here,Xj=fPj;Cjg2	is the combined state of the agent
jwithPjbeing the perceived information by that agent and
Cjrepresenting the communicated data from other agents.
Tis the assigned task. Si,Bi,Ci,Ti, andLiare the utility
values of corresponding feature/factor iin the correspond-
ing levels - Safety, Basic, Capability, Teaming, and Learn-
ing, respectively. sj,bj,cj,tj, andljare the sizes of agent
j’s feature space on the respective levels of needs. The col-
lective need of an agent jis expressed as the union of needs
at all the levels in the needs hierarchy as in Eq. (6)2.
Nj=Nsj[Nbj[Ncj[Ntj[Nlj (6)
More speciﬁcally, the set of agent needs in a multi-agent
system can be regarded as a kind of motivation or require-
ments for cooperation between agents to achieve a speciﬁc
group-level task (Yang 2022).
Deep Reinforcement Learning (DRL)
The essence of reinforcement learning (RL) is learning from
interaction based on reward-driven (like utilities and needs)
behaviors, much like natural agents. When an RL agent
interacts with the environment, it can observe the conse-
quence of its actions and learn to change its behaviors based
on the corresponding rewards received. Moreover, the the-
oretical foundation of RL is the paradigm of trial-and-error
learning rooted in behaviorist psychology (Sutton and Barto
2018). Furthermore, DRL trains deep neural networks to ap-
proximate the optimal policy and/or the value function. The
2Each category needs level is combined with various similar
needs (expected values) presenting as a set, consisting of individual
hierarchical and compound needs matrix Nj.deep neural network serving as a function approximator en-
ables powerful generalization, especially in visual domains,
general AI systems, robotics, and multiagent/robot systems
(MAS/MRS) (Hernandez-Leal, Kartal, and Taylor 2019).
The various DRL methods can be divided into three groups:
value-based methods, such as DQN (Mnih et al. 2015); pol-
icy gradient methods, like the PPO (Schulman et al. 2017);
and actor-critic methods, like the Asynchronous Advantage
Actor-Critic (A3C) (Mnih et al. 2016). From the determinis-
tic policy perspective, DDPG (Lillicrap et al. 2015) provides
a sample-efﬁcient learning approach. On the other hand,
from the entropy angle, SAC (Haarnoja et al. 2018) consid-
ers a more general maximum entropy objective retaining the
beneﬁts of efﬁciency and stability. Here, we brieﬂy discuss
SAC method as follows:
Soft Actor-Critic (SAC): SAC is an off-policy actor-critic
algorithm that can be derived from a maximum entropy vari-
ant of the policy iteration method. The architecture consider
a parameterized state value function V (st), soft Q-function
Q(st;at), and a tractable policy (atjst). It updates the
policy parameters by minimizing the Kullback-Leibler di-
vergence between the policy 0and the Boltzmann policy in
Eq. (7).
new= arg min
0DKL
0(jst)exp(Q(st;)
Z(st)
(7)
Hierarchical Needs-driven Agent learning
This section ﬁrst introduces the Bayesian Strategy Net-
work (BSN) (Yang and Parasuraman 2023b) based on the
Bayesian networks to decompose a complex strategy or in-
tricate behavior into several simple tactics or actions. Then,
from a single-agent perspective, we discuss the promising
potential of a new DRL model termed Bayesian Soft Actor-
Critic (BSAC) in robotics. An example of a BSN-based ac-
tion decomposition of a Biped Robot is shown in Fig. 1. Fur-
thermore, we extend this topic to MAS/MRS ﬁelds.
Single-Agent Systems
Bayesian Strategy Networks (BSN) Supposing that the
strategyTconsists ofmtactics (t1;:::;tm) and their spe-
ciﬁc relationships can be described as the BSN. We con-
sider the probability distribution Pias the policy for tactic ti.Low-level skills:
Running, Getting up
Single
Player
(1) Imitation 
of mocap dataMultiply
PlayerMid-level skills:
Dribbling, ShootingHigh-level skills:
Coordination, Passing, 
Blocking(2) Reinforcement learning
          in drill tasks
(4) Reinforcement learning
          in 2 vs 2 matches(3) Distillation
          of drill experts
          into policy priorsFigure 6: End-to-End learning of coordinated 2 vs 2 humanoid football in MuJoCo multi-agent soccer environment
Then, according to the Bayesian chain rule, the joint policy
(aT2T;s)can be described as the joint probability func-
tion (Eq. (8), m2Z+) through each sub-policy i(ti;s),
correspondingly. An overview of the example BSN imple-
mentation in actor-critic architecture is presented in Fig. 2.
T(t1;:::;tm) =1(t1)mY
i=2i(tijt1;:::;ti): (8)
Fig. 3, 4, and 5 provide three examples of decompos-
ing RL agents’ behaviors into different BSN models in the
standard continuous control benchmark domains – Hopper-
v2, Walker2d-v2, and Humanoid-v2 – at OpenAI’s Gym
MuJoCo environments. The source of the BSAC algorithm
is available on GitHub: https://github.com/RickYang2016/
Bayesian-Soft-Actor-Critic--BSAC.
Bayesian Soft Actor-Critic (BSAC) BSAC integrates the
Bayesian Strategy Networks (BSN) and the state-of-the-art
SAC method (Yang and Parasuraman 2023b). By building
several simple sub-policies organized as BSN, BSAC pro-
vides a more ﬂexible and suitable joint policy distribution
to adapt to the Q-value distribution, increasing sample efﬁ-
ciency and boosting training performance.
Speciﬁcally, BSAC incorporates the maximum entropy
concept into the actor-critic deep RL algorithm. According
to the additivity of the entropy, the system’s entropy can
present as the sum of the entropy of several independent sub-
systems (Wehrl 1978). For each step of soft policy iteration,
the joint policy will calculate the value to maximize the
sum of sub-systems’ ientropy in the BSN using the below
objective function Eq. (9). To simplify the problem, each
policy’s weight and corresponding temperature parameters
iare the same in each sub-system.
JV() =TX
t=0E(st;At)i"
r(st;At) +
mmX
i=1H(i(jst))#
(9)
Furthermore, the soft policy evaluation and the soft policy
improvement alternating execution in each soft sub-policy
iteration guarantees the convergence of the optimal maxi-
mum entropy among the sub-policies combination.Multi-Agent Systems (MAS)
When considering multi-agent systems (MAS) learning, an
individual agent must ﬁrst master essential skills to sat-
isfy low-level needs (safety, basic, and capability needs).
Then, it will develop effective strategies ﬁtting middle-level
needs (like teaming and collaboration) to guarantee the sys-
tems’ utilities. Through learning from interaction, MAS can
optimize group behaviors and present complex strategies
adapting to various scenarios and achieving the highest-level
needs, and fulﬁlling evolution. By cooperating to achieve a
speciﬁc task, gaining expected needs (rewards) , or against
the adversaries decreasing the threat, intelligent agents can
beneﬁt the entire group development or utilities and guar-
antee individual needs and interests. It is worth mentioning
that (Liu et al. 2021b) developed the end-to-end learning im-
plemented in the MuJoCo multi-agent soccer environment
(Tunyasuvunakool et al. 2020), which combines low-level
imitation learning, mid-level, and high-level reinforcement
learning, using transferable representations of behavior for
decision-making at different levels of abstraction. Figure 6
represents the training process.
Speciﬁcally, Agent Needs Hierarchy is the foundation for
a MAS learning from interaction. It surveys the system’s
utility from individual needs. Balancing the needs/rewards
between agents and groups for MAS through interaction and
adaptation in cooperation optimizes the system’s utility and
guarantees sustainable development for each group member,
much like human society does. Furthermore, by designing
suitable reward mechanisms and developing efﬁcient DRL
methods, MAS can effectively represent various group be-
haviors, skills, and strategies to adapt to uncertain, adversar-
ial, and dynamically changing environments.
Conclusions
This paper introduces the new hierarchical needs-driven
agent learning systems, discusses the implementation of the
BSAC in single-agent systems, and extends this topic to the
potential research ﬁelds in the MAS. It presents the promis-
ing potential that hierarchical needs-driven learning systems
can bridge the future gap between robotics and AI.References
Arulkumaran, K.; Deisenroth, M. P.; Brundage, M.; and
Bharath, A. A. 2017. Deep reinforcement learning: A brief
survey. IEEE Signal Processing Magazine , 34(6): 26–38.
Buro, M. 2003. Real-time strategy games: A new AI re-
search challenge. In IJCAI , volume 2003, 1534–1535.
Chan, S. 2001. Complex adaptive systems. In ESD. 83 re-
search seminar in engineering systems , volume 31, 1–9.
Freedman, L. 2015. Strategy: A history . Oxford University
Press.
Haarnoja, T.; Zhou, A.; Abbeel, P.; and Levine, S. 2018.
Soft actor-critic: Off-policy maximum entropy deep rein-
forcement learning with a stochastic actor. In International
conference on machine learning , 1861–1870. PMLR.
Hernandez-Leal, P.; Kartal, B.; and Taylor, M. E. 2019. A
survey and critique of multiagent deep reinforcement learn-
ing. Autonomous Agents and Multi-Agent Systems , 33(6):
750–797.
Langley, P.; Laird, J. E.; and Rogers, S. 2009. Cognitive ar-
chitectures: Research issues and challenges. Cognitive Sys-
tems Research , 10(2): 141–160.
Levin, S. A. 1998. Ecosystems and the biosphere as complex
adaptive systems. Ecosystems , 1(5): 431–436.
Lillicrap, T. P.; Hunt, J. J.; Pritzel, A.; Heess, N.; Erez, T.;
Tassa, Y .; Silver, D.; and Wierstra, D. 2015. Continuous
control with deep reinforcement learning. arXiv preprint
arXiv:1509.02971 .
Liu, R.; Nageotte, F.; Zanne, P.; de Mathelin, M.; and Dresp-
Langley, B. 2021a. Deep reinforcement learning for the
control of robotic manipulation: a focussed mini-review.
Robotics , 10(1): 22.
Liu, S.; Lever, G.; Wang, Z.; Merel, J.; Eslami, S.; Hennes,
D.; Czarnecki, W. M.; Tassa, Y .; Omidshaﬁei, S.; Abdol-
maleki, A.; et al. 2021b. From motor control to team
play in simulated humanoid football. arXiv preprint
arXiv:2105.12196 .
Mnih, V .; Badia, A. P.; Mirza, M.; Graves, A.; Lillicrap, T.;
Harley, T.; Silver, D.; and Kavukcuoglu, K. 2016. Asyn-
chronous methods for deep reinforcement learning. In In-
ternational conference on machine learning , 1928–1937.
PMLR.
Mnih, V .; Kavukcuoglu, K.; Silver, D.; Rusu, A. A.; Ve-
ness, J.; Bellemare, M. G.; Graves, A.; Riedmiller, M.; Fidje-
land, A. K.; Ostrovski, G.; et al. 2015. Human-level control
through deep reinforcement learning. nature , 518(7540):
529–533.
Murphy, R. R. 2014. Disaster robotics . MIT press.
Rizk, Y .; Awad, M.; and Tunstel, E. W. 2018. Decision mak-
ing in multiagent systems: A survey. IEEE Transactions on
Cognitive and Developmental Systems , 10(3): 514–529.
Schulman, J.; Wolski, F.; Dhariwal, P.; Radford, A.; and
Klimov, O. 2017. Proximal policy optimization algorithms.
arXiv preprint arXiv:1707.06347 .
Singh, B.; Kumar, R.; and Singh, V . P. 2021. Reinforce-
ment learning in robotic applications: a comprehensive sur-
vey. Artiﬁcial Intelligence Review , 1–46.Sutton, R. S.; and Barto, A. G. 2018. Reinforcement learn-
ing: An introduction . MIT press.
Tunyasuvunakool, S.; Muldal, A.; Doron, Y .; Liu, S.; Bohez,
S.; Merel, J.; Erez, T.; Lillicrap, T.; Heess, N.; and Tassa, Y .
2020. dm control: Software and tasks for continuous con-
trol. Software Impacts , 6: 100022.
V on Neumann, J.; and Morgenstern, O. 2007. Theory of
games and economic behavior . Princeton university press.
Wehrl, A. 1978. General properties of entropy. Reviews of
Modern Physics , 50(2): 221.
Yang, Q. 2022. Self-Adaptive Swarm System . Ph.D. thesis,
University of Georgia.
Yang, Q.; Luo, Z.; Song, W.; and Parasuraman, R. 2019.
Self-reactive planning of multi-robots with dynamic task
assignments. In 2019 International Symposium on Multi-
Robot and Multi-Agent Systems (MRS) , 89–91. IEEE.
Yang, Q.; and Parasuraman, R. 2020a. Hierarchical needs
based self-adaptive framework for cooperative multi-robot
system. In 2020 IEEE International Conference on Systems,
Man, and Cybernetics (SMC) , 2991–2998. IEEE.
Yang, Q.; and Parasuraman, R. 2020b. Needs-driven hetero-
geneous multi-robot cooperation in rescue missions. In 2020
IEEE International Symposium on Safety, Security, and Res-
cue Robotics (SSRR) , 252–259. IEEE.
Yang, Q.; and Parasuraman, R. 2021. How Can Robots Trust
Each Other For Better Cooperation? A Relative Needs En-
tropy Based Robot-Robot Trust Assessment Model. In 2021
IEEE International Conference on Systems, Man, and Cy-
bernetics (SMC) , 2656–2663. IEEE.
Yang, Q.; and Parasuraman, R. 2022. Game-theoretic Util-
ity Tree for Multi-Robot Cooperative Pursuit Strategy. ISR
Europe 2022; 54th International Symposium on Robotics,
2022 , 1–7.
Yang, Q.; and Parasuraman, R. 2023a. A game-theoretic
utility network for cooperative multi-agent decision-making
in adversarial environments. Proceedings of the 38th
ACM/SIGAPP Symposium on Applied Computing .
Yang, Q.; and Parasuraman, R. 2023b. A Strategy-Oriented
Bayesian Soft Actor-Critic Model. Procedia Computer Sci-
ence, the 14th International Conference on Ambient Sys-
tems, Networks and Technologies (ANT 2023) .