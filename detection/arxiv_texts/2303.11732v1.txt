Noname manuscript No.
(will be inserted by the editor)
Multi-modal Prompting for Low-Shot Temporal Action
Localization
Chen Ju Zeqian Li Peisen Zhao Ya Zhang Xiaopeng Zhang 
Qi Tian Yanfeng Wang Weidi Xie
Received: date / Accepted: date
Abstract In this paper, we consider the problem of
temporal action localization under low-shot (zero-shot
& few-shot) scenario, with the goal of detecting and
classifying the action instances from arbitrary categories
within some untrimmed videos, even not seen at train-
ing time. We adopt a Transformer-based two-stage ac-
tion localization architecture with class-agnostic action
proposal, followed by open-vocabulary classication. We
make the following contributions. First , to compensate
image-text foundation models with temporal motions,
we improve category-agnostic action proposal by ex-
plicitly aligning embeddings of optical ows, RGB and
texts, which has largely been ignored in existing low-
shot methods. Second , to improve open-vocabulary ac-
tion classication, we construct classiers with strong
discriminative power, i.e., avoid lexical ambiguities. To
be specic, we propose to prompt the pre-trained CLIP
text encoder either with detailed action descriptions (ac-
quired from large-scale language models), or visually-
conditioned instance-specic prompt vectors. Third ,
we conduct thorough experiments and ablation stud-
ies on THUMOS14 and ActivityNet1.3, demonstrating
the superior performance of our proposed model, out-
performing existing state-of-the-art approaches by one
signicant margin.
Keywords Vision-Language Foundation Models 
Prompt TuningLow-shot Video Understanding
Chen Ju, Zeqian Li, Ya Zhang, Yanfeng Wang, Weidi Xie
CMIC, Shanghai Jiao Tong University, Shanghai
E-mail: fjuchen, lzq0103, ya zhang g@sjtu.edu.cn
fwangyanfeng, weidi g@sjtu.edu.cn
Peisen Zhao, Xiaopeng Zhang, Qi Tian
Huawei Cloud & AI, Shenzhen, Guangdong
E-mail: fpszhao93, zxphistory, tianqi1 g@gmail.com1 Introduction
Temporal action localization (TAL) aims to localise and
classify action instances in untrimmed long videos and
is an indispensable part of video understanding [66,97,
26]. Despite great progress has been made by training
deep neural networks on large-scale datasets [4,89], ex-
isting research mainly considers a closed-set scenario,
where action categories remain identical at training and
inference stage. Such an assumption is clearly an over-
simplication for deployment scenarios, hence limiting
its practical uses.
In the recent literature, another line of research [20,
51] considers a more challenging problem, that requires
the vision system to handle both seen and unseen cat-
egories, with low-shot (zero or only few) examples at
inference time, this problem is often termed as open-
vocabulary temporal action localization. To tackle the
problem, existing studies [20,51,42] take inspiration from
large-scale foundational models [59,16,80], casting the
problem of action classication in the form of cross-
modal retrieval, i.e., for one action videos, searching its
closest category embedding in text form ( e.g., \an ac-
tion video of class"). However, such a design potentially
suers from the lexical ambiguities, as multiple actions
may share category names, despite its diering visual
appearance. For example in Figure 1, \fencing" could
either refer to \the game of stabbing with slender steel
swords in protective clothing" or \enclose or separate
something using paling or hedgerow", directly encod-
ing the category names would therefore be unable to
distinguish these two concepts.
To alleviate the above challenge, in this paper, we
consider two ideas to enrich the discriminative power
of action classiers constructed from a pre-trained text
encoder, via natural language descriptions, or visuallyarXiv:2303.11732v1  [cs.CV]  21 Mar 20232 Chen Ju et al.
Flow 
encoderRGB 
encoderClass Prompt
Tuning
Temporal LayerAlignText encoder
Temporal LayerAttribute
DescriptionVision -conditional
Prompt Tuning
enclose object with 
paling or hedgerow
FencingLexical Ambiguities in Vanilla Category Names
game of stabbing 
with slender swords 
Attribute Descriptions:
handspring, walkover, leap straddle, 
scissors, somersault, landing
RGB 
encoderText encoder
Align
Decompose Actions Learn from Vision
LLMs Questions
Model Video Input
Fig. 1 Left : Existing methods add prompt tuning and temporal layer to foundation models. Middle : The main challenge is
lexical ambiguities in vanilla category names. To disambiguate text-based category names, we decompose actions by prompting
large language models for various action attribute descriptions. For cases where it may be dicult to give comprehensive detail
descriptions, we further propose vision-conditional prompting to learn from input videos. Right : Our overall framework.
conditioned prompting. To be specic, (1) to disam-
biguate the text-based category names, we construct
an automatic pipeline to source detailed attribute de-
scriptions for action categories by prompting large-scale
language models (LLMs) [3,87]. Along this line of idea,
we propose three types of attribute question templates,
to encode the salient objects, event elds, and interac-
tions for the query actions. For instance, while prompt-
ing LLMs using the template: \What tools are needed
for [skiing]?", it outputs \ski board, ski stick, ski gog-
gles, helmet", hence giving additional cues to enhance
the discrimination of the subsequently generated ac-
tion classier; (2) in cases that could be dicult to
provide comprehensive detail descriptions, we further
propose vision-conditional prompting, that enables to
extract action details from RGB and Flow embeddings,
as instance-specic prompt vectors for the pre-trained
text encoder. As an example demonstrated in Figure 1,
to construct the action classier for \competitive artis-
tic gymnastics", we can either encode the detailed lan-
guage descriptions (handspring, walkover, leap strad-
dle, scissors, somersault, etc.), or simply calculate the
visual embeddings of the given video, and prompt the
text encoder along with action categories.
Inheriting empirical observations that optical ows
can generally bring impressive category-agnostic detec-
tion (action proposals), i.e., large ow normally indi-
cates the existence of actions, we also explicitly incor-
porate optical ows into the visual representations [5],
and then align RGB, Flow, and text modalities into one
shared embedding space. When evaluating on two stan-
dard benchmarks: THUMOS14 and ActivityNet1.3, ourmodel signicantly outperforms existing competitive meth-
ods, sometimes by over 10% across few-shot and zero-
shot scenarios. We also conduct thorough ablation stud-
ies to reveal the eectiveness of each component, both
quantitatively and qualitatively.
2 Related Work
Vision-Language Pre-training. In the early litera-
ture, [50,9,72] explore jointly training image-text em-
beddings using paired text documents. Recently, some
studies have further scaled up the training with large-
scale web data to form \the foundation models", e.g.,
CLIP [59], ALIGN [16], Florence [81], FILIP [80], Video-
CLIP [74], and LiT [84]. These foundation models usu-
ally contain one visual encoder and one textual encoder,
which are trained using simple noise contrastive learn-
ing for powerful cross-modal representations. They have
shown promising potential in many tasks, such as im-
age classication and detection, action recognition, and
retrieval. In this paper, we use CLIP for low-shot tem-
poral action localization, but the same technique should
be applicable to other foundation models as well.
Prompting refers to leveraging input instructions to
steer foundation models for desired outputs. In the NLP
domain, early papers [13,19,60,62] focus on handcrafted
prompt templates. To avoid labor and increase exibil-
ity, some studies [30,31,32] propose learnable prompt
tuning at the textual stream, showing strong low-shot
generalization. In the CV domain, some recent papers [94,
95,20] introduce such randomly initialized prompt tun-
ing to handle visual tasks, e.g., image understanding [96,Multi-modal Prompting for low-shot TAL 3
41,79,45] and video understanding [17,51,56]. However,
these studies ignore lexical ambiguity of category names,
and cases that are not easy to describe in text. This
paper designs novel conditional prompt tuning and lan-
guage descriptions from LLMs, to solve these issues.
Closed-set Temporal Action Localization consid-
ers to detect and classify action instances from one pre-
dened category list. Specically, existing methods can
be divided into two popular supervisions, i.e., strong [83,
33,58] and weak [69,21,24,82]. Strong supervision gives
precise boundary labels and category labels for training.
There are two detailed pipelines: the top-down frame-
work [65,63,12,7,35,73,67,98,70,75] pre-denes exten-
sive anchors, adopts xed-length sliding windows to
produce initial proposals, then regresses to rene bound-
aries; the bottom-up framework [92,36,34,68,90,1] learns
frame-wise boundary detectors for the boundary frames,
then groups extreme frames or estimates action lengths
for proposal generation. In addition, several works [10,
39,78] used various fusion strategies to complement these
frameworks. On the other hand, weak supervision trains
without boundary labels to alleviate annotation costs.
The video-level setting learns from category labels [57,
25], the CAS-based framework [38,23,49,52,28,29,91]
and attention-based framework [54,43,55,61,11,14,15,
44,46] have been well studied. To generate better results
from CAS or attention, some studies [64,40] improved
post-processing. To balance cost and performance, some
papers introduced single-frame annotations [22,47,27,
77,48] or instance-number annotations [53,76].
Nevertheless, all the above methods assume that ac-
tion categories remain identical for training and testing,
which is an over-simplication of real application sce-
narios, limiting practical uses of the vision system.
Low-Shot Temporal Action Localization consid-
ers more realistic scenarios: generalize TAL towards ac-
tion categories that are unseen (zero-shot) or with sev-
eral support samples (few-shot). Existing methods [20,
51,88,2] most rely on foundational models pre-trained
on large-scale image-caption pairs for help. Typically,
E-Prompt [20] is the rst to construct wide baselines
with popular prompt tuning [30,31] and vanilla tempo-
ral modeling. STALE [51] explores the one-stage frame-
work to further simplify usage. Although promising, all
above methods meet two main challenges: (1) For cate-
gory semantics, the denition may be vague, inaccurate,
or incomplete. (2) For visual motions, temporal model-
ing may be insucient. In this paper, for detailed cate-
gory understanding, we design novel language descrip-
tions from LLMs and vision-conditional prompt tuning;
for clearer motion understanding, we introduce optical
ows to provide explicit motion inputs.3 Method
In this paper, our goal is to tackle open-vocabulary tem-
poral action localization, by aligning multi-modal video
representations, extracted from RGB, Flows, and texts.
In Sec. 3.1, we start by introducing problem scenarios.
In Sec. 3.2, we describe the proposed architecture, con-
sisting of two vital modules, namely, category-agnostic
action proposal and open-vocabulary action classica-
tion via descriptions or visual instance prompts. Lastly,
we detail the training and inference procedure in Sec. 3.3.
3.1 Problem Scenario
Assuming we are provided one untrimmed video dataset
for training, e.g.,Dtrain =f(V1;Y1);:::; (VN;YN)g,
where Vi2RTiHW3denotes one video sequence
withTiframes, Yi2RTiCtrainrefers to the frame-
wise annotation for action categories, that fall into one
pre-dened vocabulary. Our goal is to train an open-
vocabulary temporal action localization model, that can
process testing videos from unseen categories, i.e., to
detect and classify actions beyond the seen ones at
training time:
Yj=open-TAL (Vj)2RTjCtest;VjD test; (1)
where Vjrefers to a video sampled from the testing set.
Under zero-shot settings, action categories for training
(base) and evaluation (novel) are disjoint, i.e.,Ctrain\
Ctest=;. Under few-shot settings, training categories
are included in the testing categories, i.e.,CtestCtrain.
As a comparison, in closed-set scenarios, categories for
training and testing are identical, i.e.,Ctest=Ctrain.
3.2 Architecture
We propose a multi-modal low-shot action localization
framework, containing two modules: category-agnostic
proposal (in Sec. 3.2.1), and open-vocabulary classi-
cation by descriptions or visual prompts (in Sec. 3.2.2).
Note that , both visual and textual encoders are pre-
trained and kept frozen, thus resembling the downstream
adaptations of foundation models [59,16,80] eectively.
3.2.1 Category-agnostic Action Proposal
As for the visual encoding, we here consider two widely-
used modalities for video understanding, namely, RGB
appearance and optical ow.
RGB Frame Encoder. Given one untrimmed video
withTframes, V=fI1;:::;ITg, we employ two types4 Chen Ju et al.
action
proposalsactionness
bound offset
Where[skiing] usually 
takes place ?How to decompose steps 
for[skiing] ?
Text 
Encoderpullpush push
pushcategory
agnostic 
detection
RGB 
EncoderFlow 
Encoder
Large Language ModelTemplates of 
Action Attributes
What tools are needed 
for[skiing] ?ski board, ski stick, 
helmet, goggles
stand in half -squat 
pose; put stick in 
front of toes; push 
arms back
snow mountain, 
alpine slope 
Text 
Encoderpullpush push
pushcategory
agnostic 
detection
RGB 
EncoderFlow 
Encoder
Vision -conditional
Prompt ModuleTOKENIZER
(skiing)action
proposalsactionness
bound offset
action
proposalsactionness
bound offsetaction
proposalsactionness
bound offset
(B)(A)
(A)text-based classifier by 
attribute description s
category -agnostic detection
Detector
RegressorTemporal
Aggregation(B)vision -based classifier  
from RGB/Flow -
conditional prompt
Fig. 2 Framework Overview. Given one input video, we rst encode RGB and Flow for appearance and motion embeddings,
then localize category-agnostic action proposals (detect actionness and regress boundary). For open-vocabulary classication,
we design (A) text-based classiers using attribute descriptions from Large Language Models, and (B) vision-based classiers
using (RGB & Flow)-conditional prompt tuning. Finally, the RGB-Flow-Text modalities are aligned for low-shot TAL.
of RGB pre-trained encoders to obtain frame-wise fea-
tures, i.e., CLIP image [59] and I3D RGB [5].
Frgb=CLIP-r (V) or Frgb=I3D-r(V); (2)
whereCLIP-r () is pre-trained on 400M image-text pairs,
with rich description of visual appearances; I3D-r()
is pre-trained on Kinetics-400 for action recognition,
with good representations of the temporal continuity.
For simplicity, we denote the above features as Frgb2
RTDrgb,Drgbdenotes the dimension of RGB features.
Flow Encoder. We rst use the TV-L1 algorithm [71]
to compute optical ows from the RGB frame sequence,
and then, feed them into the pre-trained I3D Flow en-
coder [5] to compute frame-wise motion features:
Fow=I3D-f(V)2RTDflow; (3)
whereI3D-f() is pre-trained on Kinetics-400, thus con-
taining rich motion details, Dowdenotes the dimension
of the Flow features.
Temporal Aggregation. Given visual features ( Frgb
&Fow) from the frozen encoders, we pass them through
one temporal aggregation module temp(), consisting
of standard Transformer encoder layers, for better ac-
tion modeling. As the action duration varies frequently,following recent methods [8,85], we construct a multi-
scale pyramid structure for temp, formulated as:
F=fF
1;:::;F
Lg=temp(F)2RT0D; (4)
whererefers to the RGB or Flow modality. The Trans-
former pyramid covers Llayers, and each layer consists
of Multi-head Self-attention, Layer Norm, and MLPs.
We perform \2" down-sampling between adjacent lay-
ers, to ensure that deeper layers focus on long actions,
while shallow layers handle short actions. As a result,
we obtain F
i2RT=2iDfor thei-th layer features.
Category-agnostic Proposals. Here, we design one
temporal action localizer, containing one detector det()
and one regressor reg() in parallel, to produce category-
agnostic action proposals. Concretely, when taking multi-
scale pyramid features Fas input,det() predicts
the frame-level action probability, i.e., performing the
binary classication for action and background; while
reg() regresses the left oset and right oset to the
nearest action instance for each frame, thus rening the
boundary.
bp
det=det(F)2RT0;bP
reg=reg(F)2RT02:(5)
Structurally, both the detector and the regressor are
composed of 1D convolutional networks. Hereafter, by
further post-processing bpdetandbPreg, we could obtainMulti-modal Prompting for low-shot TAL 5
Baction proposalsfbigB
i=1. And then, we compute vi-
sual embeddings for these category-agnostic proposals
by taking the mean pooling of RGB or Flow features
within the proposal intervals.
f
i=pool(F[bi])2RD: (6)
Note that , we do not claim novelty or contribution
on category-agnostic action detection [37,90,67,85,33],
which has been widely-studied in the community. And
our method is exible to these o-the-shelf detectors.
3.2.2 Open-vocabulary Action Classication
In this section, we introduce the procedure for classify-
ing action proposals towards both base and novel cat-
egories (only requested by user at inference time). To
produce such open-vocabulary classiers, the key is to
use a pre-trained text encoder of vision-language foun-
dation models. Specically, one vanilla solution [59] is
to combine category names with handcrafted prompts,
e.g., \one video offcategoryg", then feed into the CLIP
text encoder to generate action classier. However, such
a paradigm suers from lexical ambiguities, for exam-
ple, \fencing" can either refer to \the game of stabbing
with slender steel swords in protective clothing" or \en-
close or separate something with paling or hedgerow",
directly encoding vanilla category names is unable to
distinguish these concepts. Here, we present two novel
strategies to enhance the discriminative power of gener-
ated classiers with: detailed language descriptions or
vision-conditioned prompting.
Classier construction by language descriptions.
Here, we consider to decompose actions into \attribute"
that species one category from various aspects, hence
enriching the discriminative power of constructed clas-
siers. As shown in Figure 2 (A), for Caction cate-
gory names (denoting as Cname), we prompt one large-
scale language model (LLMs), for example, GPT-3 [3]
or OPT [87], with three attribute templates, to obtain
detailed descriptions of salient objects, event elds, and
motion interactions for query actions:
Mdesc=LLM(attr[Cname]); (7)
where three attribute templates attrare \what tools
are needed for [action]?", \where [action] usually takes
place?", and \how to decompose steps for [action]?",
respectively. Note that, here we only use some straight-
forward prompt templates, while other templates may
also bring similar attribute descriptions.
With the attribute descriptions Mdesc, the classier
embeddings for Caction categories can thus be gener-
ated using the CLIP text encoder:
Ftext=CLIP-t (tokenise (Mdesc))2RCDtext; (8)wheretokenise () refers to the language tokenizer that
converts words into vectors, and Dtextrefers to the di-
mension of the textual features.
Classier construction by conditioning on video
instance. For the cases where comprehensive text de-
scriptions are challenging to acquire, for instance, a
good description for gymnastics with intricate pose pat-
terns is prohibitively long, we consider one alternative
way for lexical disambiguation. In particular, we design
a vision-conditional prompt module prmp(), as shown
in Figure 2 (B). For any given video, prmp() takes its
RGB and Flow features, i.e.,FrgbandFow, as inputs,
and outputs Kconcrete prompt vectors Mcond.
M
cond=prmp(F)2RKDtext: (9)
whereprmp() refers to a trainable module (can be
either MLPs or Transformer encoder), and M
cond de-
notes the instance-specic prompt vectors, containing
rich visual details from the RGB or Flow stream.
Hereafter, we can generate the classier embeddings
forCactions, i.e., prepending / appending M
condwith
category name tokens, then inputting all these tokens
into the CLIP text encoder.
Ftext=CLIP-t (M
cond; toke(Cname);M
cond); (10)
where Ftext2RCDtextrefers to the classier embed-
dings. At training time, the gradients can ow through
the frozen encoder CLIP-t () to only optimize the con-
ditional prompt module prmp().
In zero-shot scenarios, prmp() is trained on seen
(base) categories, then directly applied to test videos
of novel categories, outputting vision-conditional, task-
specic prompt vectors Mcond; while in the few-shot
scenarios, Mcondcan be category-specic to learn from
the support video exemplars of novel categories. Intu-
itively,prmp() can be seen as implicitly decomposing
actions into \visual attributes", enriching the classier
obtained from only encoding category names.
Cross-modal alignment. For one video, the textual
stream ends up with the category-wise features Ftext=
fftext
1;:::;ftext
Cg2RCDtext; while the visual stream
ends up with RGB features frgb2RDrgband Flow fea-
tures fow2RDflowfor action proposals.
To alignment between language with appearance or
motion, we adopt a lightweight module ( align()) to
map RGB-Flow-Text embeddings into one shared space.
h=align(f)2RDalign; (11)
whererefers to any of the three modalities, and Dalign
refers to the aligned dimension of the embedding space.
Architecture-wise, align() is exible to o-the-shelf net-
works, such as MLPs or Transformer encoder.6 Chen Ju et al.
3.3 Training and Inference
Given one batch of (RGB, Flow, Text) training pairs,
the visual stream ends up with RGB and Flow embed-
dings for action proposals, namely, hrgbandhow; while
the textual stream ends up with classier embeddings
fhtext
1;:::;htext
Cg. In the following, we describe the op-
timization details for detection and classication.
Category-agnostic Proposal. Following previous
methods [37,67], to supervise the generation of action
proposals, we adopt the weighted cross-entropy loss for
action proposal [34,36] and use the DIoU loss Lregfor
distance regression [93]. Formally,
Ldet=X
t2
+H(p
t;bp
t) +X
t2
 H(p
t;bp
t); (12)
wherept2f0;1gandbpt2[0;1] are the label and prob-
ability for category-agnostic actions at t-th timestamp;
Handare the cross-entropy and the balancing weight;

+and
 are positive sets and negative sets.
Lreg= 1 IoU +D(bP
reg;P
reg); (13)
where IoU is the intersection over union between pre-
dicted proposals and ground-truth actions. Pregrefers
to the regression ground-truth, and Drefers to the nor-
malized euclidean distance.
Open-vocabulary Classication. After calculating
action proposals, we aim to encourage the paired (RGB,
Flow, Text) embeddings to emit the highest similarity
score among others, we use the infoNCE loss to super-
vise tri-modal alignment in the shared space, which can
be written as follows:
Lcls= X
i 
logexp(S(h
i;htext
i)=)P
jexp(S(h
i;htext
j)=)
; (14)
whereSrefers to the cosine similarity, and is one tem-
perature parameter. Using the well-aligned multi-modal
features, we can naturally achieve open-vocabulary clas-
sication, through evaluating the cosine similarity be-
tween the textual and visual modalities.
Total Loss. During training, we freeze encoders for
RGB-Flow-Text, and jointly optimize the detector, re-
gressor, vision-conditional prompt module, and corss-
modal aligner. Using two balancing ratios ( 1and2),
the total optimization loss Lallcan be formulated:
Lall=Ldet+1Lreg+2Lcls: (15)
Inference. At testing time, for one given video, we
could compute the frame-level action probability bp
det
from the binary classier, and the frame-level boundaryosetbP
regfrom the boundary regressor. For category-
agnostic proposal, we threshold bp
detthroughloc, con-
catenate consecutive snippets as action proposals, uti-
lizebP
regfor boundary renement, and eliminate pro-
posal redundancy with soft non-maximum suppression
(NMS). For low-shot classication, we calculate cosine
similarity between category textual embeddings and pro-
posal visual embeddings, then only retain the category
with probabilities greater than threshold cls.
4 Experiments
We experiment on two public datasets across four data
splits. In Sec. 4.3 and 4.4, we validate the eectiveness
of each component. In Sec. 4.5, we compare with state-
of-the-art low-shot methods. In Sec. 4.6, we visualize
the localization results.
4.1 Datasets & Metrics
THUMOS14 [18] has 413 untrimmed videos from 20
categories, with an average of 15 instances per video,
and 20 videos per category. ActivityNet1.3 [4] covers
20k videos from 200 categories, with an average of 1 :5
instances per video, and 100 videos per category.
Splits. Following literature [20,51], we adopt two types
of splits for zero-shot scenarios. The 75:25 split : train
on 75% base categories and test on 25% novel cate-
gories. The 50:50 split : train on 50% base categories
and test on 50% novel categories. The nal results are
calculated by averaging 10 random splits.
While for few-shot scenarios, as we are not aware of
any existing benchmarks, we initiate the N-shot eval-
uation, i.e., sampleNvideos from the training set for
each novel category, to form the few-shot support set,
and then measure on the standard testing set.
Metrics. To evaluate localization performance, we re-
port mean Average Precision (mAP) under dierent in-
tersections over union (IoU) thresholds, following stan-
dard protocols. To evaluate classication performance,
we report the TOP1 accuracy. Note that one proposal
is regarded as positive only if both the category predic-
tion is correct and the IoU exceeds set thresholds.
4.2 Implementation Details
Our framework is implemented with PyTorch, and all
experiments are conducted on one 24G GeForce RTX
3090 GPU. On all datasets, the models are optimized
with Adam, using a learning rate of 10 4, and a batch
size of 32 videos. We warm up the model in the rst
5 epochs for better convergence, and continue to trainMulti-modal Prompting for low-shot TAL 7
Text ModalitymAP@IoU AVG
(0.3-0.7)ACC0.3 0.7
CnameRGB 33.8 6.2 20.7 79.3
RGB+Flow 46.8 12.9 31.5 79.4
MdescRGB 47.4 11.7 31.2 85.3
RGB+Flow 54.3 16.7 37.8 86.7
Table 1 Text-based classiers from language descrip-
tions. For classier generation, decomposing vanilla category
names into attribute descriptions through LLMs, can provide
additional action details to enrich discriminative information
for classiers, thus alleviating lexical confusion.
Descriptions Fusion ModalityAVG
(0.3-0.7)ACC
How
{RGB
+
Flow33.3 80.3
Where 35.0 84.0
What 35.5 84.2
How+What
+Whereaverage 34.6 83.1
concat 36.8 84.3
weight 37.8 86.7
Table 2 Descriptions of attribute questions. We adopt
\what, where, how" to prompt LLMs for salient objects, event
elds, and motion interactions, respectively. The weighted fu-
sion of these three descriptions brings the best classier.
45 epochs for full optimization. To deal with the large
variety in video durations, we pad all videos with zeros
toTframes.Tis set to 192 on ActivityNet1.3, and
2304 on THUMOS14. For temporal resolution, we take
16 consecutive frames as one basic input unit, and the
stride of sliding windows is set to 4 frames, following
the literature [85,38,20]. For spatial resolution, we use
center crop on each video frame to get 224 224 image.
For the text stream, we employ the CLIP text encoder
Dtext= 512. For the Flow encoder, we utilize the I3D
networkDow= 1024. For the RGB stream, we explore
two solutions: the CLIP image encoder Drgb= 512 and
the I3D network Drgb= 1024. Architecture-wise, both
the CLIP image and text encoders are ViT-B/16. We
use GPT-3 [3] for LLM(); adopt fully convolutional
networks for detector detand regressor reg(); employ
MLPs for vision-conditional prompt mudule prmp().
All hyperparameters are set by the grid search: bal-
ancing ratios 1=2= 1, the temperature = 0:07,
detection threshold cls= 0:85, classication threshold
loc= 0:05, and the soft-NMS threshold is set to 0 :5.
The dimension of multi-modal shared space is Dalign=
1024, and the pyramid layer L= 6.
4.3 Ablation Study
We here ablate key components to evaluate their eec-
tiveness. Unless otherwise stated, experiments are con-
ducted under the 75:25 zero-shot split on THUMOS14,Prompt ModalitymAP@IoU AVG
(0.3-0.7)ACC0.3 0.7
7RGB 33.8 6.2 20.7 79.3
RGB+Flow 46.8 12.9 31.5 79.4
MrandRGB 39.7 9.7 26.0 81.7
RGB+Flow 50.3 16.1 35.5 82.3
McondRGB 46.8 11.5 30.7 84.0
RGB+Flow 56.3 17.3 39.2 87.6
Table 3 Vision-conditional prompt module. Compared
to the prompt tuning in the text stream ( Mrand),i.e., prompt
vectors are randomly initialized and learnt on seen categories,
our vision-conditional prompts ( Mcond) characterize more vi-
sual details, thus strengthening generalization.
ModalityOracle AVG
(0.3-0.7)AVG
(0.3-0.7)ACC
RGB-Text y 30.5 29.2 97.9
RGB-Text z 40.4 30.1 83.8
Flow-Text 46.5 32.3 76.7
RGB-Text-Flow y 47.4 43.7 98.0
RGB-Text-Flow z 48.9 39.2 87.6
Table 4 RGB-Flow-Text tri-modal alignment. `Oracle'
refers to detection using ground-truth category labels. yandz
refer to using RGB encoders from CLIP or I3D. By eectively
aligning to Text, RGB and Flow encoders can handle low-shot
tasks. The tri-modal alignment shows the best performance.
using the I3D encoders for both RGB and Flow modal-
ities, CLIP encoder for text modality.
Text-based classiers from detailed language de-
scriptions. To alleviate lexical confusion for vanilla
category namesCname, we decompose actions into at-
tribute descriptions Mdesc, with the help of Large-scale
Language Models (GPT-3). Table 1 compares the per-
formance resulting from these two options.
Comparing to only using category names for action
classication, incorporating detailed descriptions could
enrich discriminative information for classiers. As a
result, Mdesccould bring around 10.5% average mAP
gains and 6.0% accuracy gains over Cname, proving the
eectiveness of category completion. On the one hand,
attribute descriptions specify one category name from
various aspects, hence giving additional action details.
On the other hand, the descriptions from LLMs avoid
manually spending time to search external knowledge
sources, thus they are also ecient for use.
Optimal language descriptions. For descriptions
Mdescfrom LLMs, there are various prompt templates
available. For full attribute descriptions of various as-
pects, we use three types of templates, i.e., \what tools?",
\where takes place?", and \how to decompose steps?",
to obtain salient objects, event elds, and motion inter-
actions. Table 2 evaluates their eectiveness.8 Chen Ju et al.
Position ModalitymAP@IoU AVG
(0.3-0.7)ACC0.3 0.7
InputRGB46.8 11.5 30.7 84.0
Output 46.3 11.4 30.4 84.4
InputRGB+Flow56.3 17.3 39.2 87.6
Output 56.1 17.6 39.2 87.9
Table 5 Usage positions of vision-conditional prompt
vectors M cond.For the CLIP text encoder, putting Mcond
into the input or the output produces similar results.
4
Prompt Length8 16 32 64RGB & Flow
RGB
383940
37A VG mAP (0.3-0.7)
29303132A VG mAP (0.3-0.7)
Fig. 3 Eect of prompt length. The variance in prompt
length has only slight eects on localization performance.
Single prompt only brings trivial results, and \what
tools?" performs best among the three attributes. To
integrate the diversity, we propose three simple strate-
gies, i.e., concatenate descriptions then text encoding,
average their text encodings, and weighted sum the text
encodings. Overall, prompt fusion brings better perfor-
mance, e.g., 2.8% average mAP gains and 2.6% accu-
racy gains over any single prompt. We nally choose
weighted sum for its best results.
Vision-conditional prompt tuning. For the case
on building classiers with video-conditioned prompts,
i.e., feed RGB and Flow embeddings into the prompt
moduleprmp(), to generate vision-conditional prompt
vectors Mcond. Table 3 compares Mcond withMrand,
where the latter refers to the popular prompt tuning [96,
20,30], with the prompt vectors being randomly initial-
ized and learnt on the base (seen) categories.
Comparing to the case without prompt, Mrandhas
shown considerable improvements, for both detection
and classication. While our vision-conditional prompt
vectors Mcond further boost the performance. For in-
stance, when using only RGB modality, 2.3% accuracy
gains and 4.7% average mAP gains over Mrand. Our
prmp() enables the prompt vectors Mcondto be instance-
specic, that enables to characterize rich visual details,
eectively complementing the information acquired from
only encoding the action category names.
Moreover, the prompt vectors learned from RGB-
Flow dual modalities signicantly outperform those from
uni-modality. This is because, in terms of describing vi-
sual action details, the RGB modality focuses more on
appearance or context; while the Flow modality mainlyModality ShareAVG
(0.3-0.7)ACC
RGB-Text-Flow yNo43.7 98.0
RGB-Text-Flow z 39.2 87.6
RGB-Text-Flow yYes33.5 83.3
RGB-Text-Flow z 36.3 83.6
Table 6 Shared visual backbones for RGB and Flow.
yandzrefer to utilizing the RGB encoders from CLIP or I3D.
`Oracle' means detection using ground-truth category labels.
The shared backbones can harm performance somewhat.
Training ModalitymAP@IoU AVG
(0.3-0.7)ACC0.3 0.7
FreezeRGB+Flow56.3 17.3 39.2 87.6
End-to-End 41.7 12.8 29.3 71.3
Table 7 Optimization modes. Comparing to freezing all
encoders, end-to-end ne-tuning the entire model brings nu-
merous parameters that tend to specialize on the base cate-
gories, thereby damaging the model generalization.
consists of motion information, the dual-modal prompt
vectors could complement each other.
Eectiveness of optical ow. Table 4 validates the
ecacy of multi-modal alignment. For evaluation met-
rics, we also report the Oracle mAP using GT category
labels, to decouple proposal and classication.
In general, making alignment with Text gives RGB
or Flow the open-vocabulary ability, enabling them to
deal with zero-shot classication. Comparing to adopt-
ing RGB only, introducing Flow for tri-modal alignment
leads to impressive boosts in the performance of both
proposal and classication. For example, 8.5% gains on
oracle average mAP and 3.8% gains on TOP1 accuracy
over only I3D RGB pre-training, revealing the essence
of Flow. In terms of ecacy, RGB has more advantages
on zero-shot classication, mainly because it gives valu-
able appearance or context to distinguish actions; while
Flow is better at category-agnostic localization, consis-
tent with the closed-set scenarios, i.e., explicit motion
inputs are critical clues for action discovery.
Encoder generalization. Table 4 also attempts to
align two types of the RGB encoders with the Flow and
Text encoders. As is evident, our method shows promis-
ing performance boosts with these two RGB encoders,
validating the strong encoder generalization, i.e., the
same technique should be applicable to more pre-trained
encoders. As far as RGB encoder is concerned, the I3D
and CLIP have their own advantages. As CLIP is pre-
trained using 400M image-text pairs, it shows better
vision-language alignment, leading to strong zero-shot
classication. While I3D is pre-trained with Kinetics-Multi-modal Prompting for low-shot TAL 9
THUMOS14 (mAP@IoU) ActivityNet1.3 (mAP@IoU)
Setting Method Modality 0.3 0.4 0.5 0.6 0.7 AVG 0.5 0.75 0.95 AVG
Closed-Set
100% Seen
0% UnseenTALNET [6] RGB+Flow 53.2 48.5 42.8 33.8 20.8 39.8 38.2 18.3 1.3 20.2
BSN [37] RGB+Flow 53.5 45.0 36.9 28.4 20.0 36.8 46.5 30.0 8.0 30.0
BUTAL [90] RGB+Flow 53.9 50.7 45.4 38.0 28.5 43.3 43.5 33.9 9.2 30.1
A2NET [78] RGB+Flow 58.6 54.1 45.5 32.5 17.2 41.6 43.6 28.7 3.7 27.8
RTD-Net [67] RGB+Flow 68.3 62.3 51.9 38.8 23.7 49.0 47.2 30.7 8.6 30.8
AFSD [33] RGB+Flow 67.3 62.4 55.5 43.7 31.1 52.0 52.4 35.3 6.5 34.4
Aformer [85] RGB+Flow 82.1 77.8 71.0 59.4 43.9 66.8 53.5 36.2 8.2 35.6
75% Seen
25% UnseenI3D [51] RGB 28.5 20.3 17.1 10.5 6.9 16.6 32.6 18.5 5.8 19.6
CLIP [59] RGB 33.0 25.5 18.3 11.6 5.7 18.8 35.6 20.4 2.1 20.2
E-Prompt [20] RGB 39.7 31.6 23.0 14.9 7.5 23.3 37.6 22.9 3.8 23.1
STALE [51] RGB 40.5 32.3 23.5 15.3 7.6 23.8 38.2 25.2 6.0 24.9
OursRGB 46.3 39.0 29.5 18.3 8.7 28.4 42.0 25.8 3.2 25.9
RGB+Flow 64.3 56.9 46.0 32.6 18.5 43.7 43.9 27.3 3.8 27.5
50% Seen
50% UnseenI3D [51] RGB 21.0 16.4 11.2 6.3 3.2 11.6 25.3 13.0 3.7 12.9
CLIP [59] RGB 27.2 21.3 15.3 9.7 4.8 15.7 28.0 16.4 1.2 16.0
E-Prompt [20] RGB 37.2 29.6 21.6 14.0 7.2 21.9 32.0 19.3 2.9 19.6
STALE [51] RGB 38.3 30.7 21.2 13.8 7.0 22.2 32.1 20.7 5.9 20.5
OursRGB 42.3 34.7 25.8 16.2 7.5 25.3 34.3 20.8 3.0 21.0
RGB+Flow 55.7 48.9 39.6 28.5 16.1 37.8 36.5 22.3 3.5 22.5
Table 8 Comparison with state-of-the-art methods on zero-shot scenarios. AVG is the average mAP in [0.3:0.1:0.7]
on THUMOS14, and [0.5:0.05:0.95] on ActivityNet1.3. Using only RGB, our method has outperformed all zero-shot studies by
a large margin. By adding Flow, our method is given explicit motion inputs to be comparable with early closed-set methods.
400 for better temporal continuity, resulting in superior
category-agnostic detection, i.e., Oracle mAP.
4.4 Detailed Comparison & Module Choice
In this section, we make detailed comparisons to further
dissect model architectures and optimization designs.
Experiments are conducted under 75:25 zero-shot splits
on THUMOS14, using the I3D encoders for both RGB
and Flow, while the CLIP encoder for Text.
Prompt positions. For the learnable prompt vectors
Mcondfrom vision-conditional prompt module, they can
be fed to the input or output of the text encoder, serv-
ing as some visual contexts functionally. We make com-
parisons of these two usage positions (denoting as input
and output respectively) in Table 5, and observe similar
performance. In general, these two positions are almost
equivalent as they both extract ne-grained action in-
formation from the visual stream to textual stream.
Prompt length & format. For the learned prompt
vectors Mcond, we prepend or append them in the for-
mat of [ Mcond; tokenise (Cname);Mcond]. In practise, this
format is equivalent with [ Mcond; tokenize (Cname)] or
[tokenize (Cname);Mcond], as revealed by existing stud-
ies [20,95]. On the other hand, given Mcond2RKDtext,
to validate the eect of prompt length K, we also exper-
iment in Figure 3, i.e., gradually increase the prompt
number from 4 to 64. Overall, the variance in promptlength has marginal eects on performance. We there-
fore pick 32 prompt vectors for its good trade-o be-
tween model performance and parameter eciency.
Sharing visual backbone. In the closed-set action
localization, there are two public backbone strategies:
share one network (early fusion) or utilize separate net-
works (late fusion) for RGB and Flow. In Table 6, we
explore these two strategies for low-shot scenarios. Gen-
erally speaking, sharing a visual backbone damages the
performance to some extent, both for proposal and clas-
sication. This is possibly due to the premature fusion
of RGB and Flow, hindering the eective alignment be-
tween these two visual modalities.
Freeze encoders vs.end-to-end ne-tuning. To
avoid heavy computational burdens, we freeze the pre-
trained encoders for RGB-Flow-Text modalities, but
only optimize lightweight modules (detector, regressor,
etc.) in our proposed method. Here, in Table 7, we com-
pare with end-to-end ne-tuning of the entire model
on THUMOS14. Surprisingly, ne-tuning more param-
eters actually leads to lower performance. We conjec-
ture this is because the large models can be overtting
to the training data, thus damaging the generalization
towards unseen (novel) action categories.
4.5 Comparison with state-of-the-art methods
This section makes full comparisons with state-of-the-
art methods on both THUMOS14 and ActivityNet1.3.10 Chen Ju et al.
THUMOS14 (mAP@IoU) ActivityNet1.3 (mAP@IoU)
Setting Method Modality Shot 0.3 0.4 0.5 0.6 0.7 AVG Shot 0.5 0.75 0.95 AVG
75% Seen
25% UnseenE-Prompt RGB
144.6 36.2 27.6 17.7 8.2 26.8
539.8 23.2 1.5 23.2
Ours RGB 46.8 39.5 29.8 18.6 8.9 28.7 43.3 26.6 3.4 26.6
Ours RGB+Flow 64.6 57.0 46.2 33.1 18.6 43.9 45.9 28.3 4.0 28.5
E-Prompt RGB
245.5 36.9 28.2 18.0 8.3 27.4
1041.2 23.8 1.5 23.9
Ours RGB 47.6 40.1 30.1 19.1 9.1 29.2 44.2 27.1 3.6 27.1
Ours RGB+Flow 65.6 57.8 47.1 33.9 19.0 44.6 46.5 28.5 4.2 28.8
50% Seen
50% UnseenE-Prompt RGB
141.2 33.1 24.1 14.6 6.4 23.9
533.6 20.8 0.5 20.4
Ours RGB 42.6 34.9 25.9 16.3 7.6 25.5 37.3 23.1 3.2 23.0
Ours RGB+Flow 57.3 50.7 41.4 29.7 16.7 39.2 39.2 24.3 3.7 24.5
E-Prompt RGB
242.4 34.2 25.0 15.2 6.8 24.7
1034.9 21.1 0.6 21.0
Ours RGB 43.6 35.7 26.6 16.7 7.7 26.0 38.2 23.5 3.4 23.3
Ours RGB+Flow 60.1 53.2 43.5 31.2 17.7 41.2 40.3 24.9 3.8 25.1
Table 9 Comparison with state-of-the-art methods on few-shot scenarios. We retrain and report few-shot results of
E-Prompt with their released codes. Although only several support samples are given for novel categories, few-shot scenarios
obtain considerable gains over zero-shot scenarios. Flow can further boost the RGB performance on both datasets.
025272931A VG mAP (0.3-0.7)
Shot Number1 2 3ViGAOurs
+4.1
Weak SupervisionFull Supervision
PartialSingle -Frame
Short -Clip (2s)
Short -Clip (4s)
5RGB & Flow
RGB
384144
35A VG mAP (0.3-0.7)
Fig. 4 Eect of shot number. More shots for novel cate-
gories bring greater gains, but also heavier annotation costs.
For the sake of fairness, we employ the CLIP encoders
for both RGB and Text modalities. While for the Flow
modality, we adopt the I3D network pre-trained on the
Kinetics-400 dataset [5]. For low-shot classiers, we here
use vision-conditional prompt tuning ( Mcond).
The zero-shot performance is reported in Table 8.
We respectively list the results of RGB and Flow for
clear understanding. In general, on all benchmarks, our
framework achieves new state-of-the-art under most IoU
regimes, using the single RGB modality, for example,
comparing to concurrent work, we signicantly surpass
competitive methods by over 5% average mAP on THU-
MOS14. Overall, existing zero-shot TAL methods usu-
ally adopt prompt tuning in the text stream, lacking
the understanding of visual details. Instead, our vision-
conditional method uses RGB or Flow embeddings to
enrich the classier generation, showing better general-
ization to the novel action categories.
Moreover, adding optical Flows for explicit motion
inputs brings immediate improvements, proving the ef-
fectiveness of tri-modal alignment. Delightfully, our zero-
shot results with RGB-Flow inputs are even compara-
ble with several early methods [6,37,63] from closed-set
scenarios, demonstrating the superiority. Additionally,comparing to the 75:25 data splits, the 50:50 data splits
pose bigger challenges, yet our method still yields ex-
cellent results, showing powerful generalization.
Note that , comparing to THUMOS14, ActivityNet
has lower requirements for classication and detection,
as there are only 1 :5 action instances per video, and
most videos contain only one category; while for THU-
MOS14, there are an average of 15 action instances per
video, with signicant variations. This phenomenon has
become a consensus [6,86,44] of temporal localization,
and could somewhat limit our gains on ActivityNet.
Few-shot performance. For few-shot scenarios, sev-
eral video exemplars are annotated for novel (unseen)
categories as the support set. Since we are not aware
of any existing benchmarks, we initiate evaluation set-
tings based on data splits of zero-shot scenarios. Specif-
ically, we label 1 or 2 videos (shots) each category for
THUMOS14, while label 5 or 10 videos per category
for ActivityNet1.3. The shot number is set according
to the dataset scale. We conduct 3 trials and report the
average results to ensure statistical signicance.
As shown in Table 9, we also retrain E-Prompt [20]
with its released codes to get few-shot results for com-
parison. In all settings of both datasets, these few-shot
video exemplars bring considerable gains over the zero-
shot counterparts, by providing explicit distribution of
novel categories. In addition, more shots naturally bring
higher performance gains, and optical ows still have
immediate improvements in performance.
To further evaluate the ecacy of shot number, Fig-
ure 4 reports some results under the 50:50 data splits
on THUMOS14. As can be seen, more shots for novel
categories indeed bring greater gains, and also enhance
the model generalization. However, there is a price to
pay, i.e., annotation overheads continue to rise.Multi-modal Prompting for low-shot TAL 11
RGB & FlowRGBGT
RGB & FlowRGBGT
RGB & FlowRGBGT
Fig. 5 Qualitative zero-shot results on THUMOS14. For various videos from novel categories, our method all outputs
good detection results, although action number and action duration vary frequently. Single RGB sometimes has large deviations,
or even omits action instances. By bringing motion details, Flow could further correct or complete the RGB results.
Eectiveness of optical ow. On both datasets, we
can observe that adding Flow consistently improves the
performance by a large margin, for both zero-shot and
few-shot tasks. Especially on THUMOS14, introducing
optical Flow boosts the average mAP for more than
10%. The same phenomenon could be observed under
various data splits, thus reecting the signicance and
eectiveness of tri-modal alignment.
4.6 Qualitative Localization Results
We visualize several detection results of novel categories
in Figure 5, under the 75:25 zero-shot splits on THU-
MOS14. Note that one proposal is shown only if the
predicted category is correct. As is evident, for various
novel categories, both action number and action dura-
tion could vary frequently in these videos, posing great
challenges to TAL models. Nevertheless, our method
obtains good results in most cases, again proving the
gratifying eectiveness. Besides, the single RGB modal-
ity sometimes outputs large deviations, or even omits
action instances. By adding motion details, the Flow
modality can further correct or complete results.
5 Conclusion
This paper considers low-shot temporal action local-
ization, and handles one main challenge, i.e., the lex-
ical ambiguities in vanilla category names. To enrich
the discriminative power of models, we decompose cat-
egories into adaptive attribute descriptions, by prompt-
ing large-scale language models, making text-based clas-
siers more detailed. Moreover, to tackle cases where it
is dicult to give comprehensive descriptions, we designone novel vision-conditional prompt module, inputting
RGB & Flow embeddings to generate prompting with
rich visual details, for powerful vision-based classiers.
Besides, we also inject optical ows for explicit motion
inputs, bringing impressive category-agnostic detection.
Extensive experiments and thorough ablations demon-
strate the eectiveness of core components, and our su-
perior performance over state-of-the-art methods.
References
1. Bai, Y., Wang, Y., Tong, Y., Yang, Y., Liu, Q., Liu,
J.: Boundary content graph neural network for tempo-
ral action proposal generation. In: Proceedings of the
European Conference on Computer Vision, pp. 121{137
(2020)
2. Bao, W., Yu, Q., Kong, Y.: Opental: Towards open set
temporal action localization. In: Proceedings of the IEEE
Conference on Computer Vision and Pattern Recogni-
tion, pp. 2979{2989 (2022)
3. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan,
J.D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry,
G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger,
G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu,
J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M.,
Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S.,
Radford, A., Sutskever, I., Amodei, D.: Language models
are few-shot learners. In: Advances in Neural Information
Processing Systems, pp. 1877{1901 (2020)
4. Caba Heilbron, F., Escorcia, V., Ghanem, B., Car-
los Niebles, J.: Activitynet: A large-scale video bench-
mark for human activity understanding. In: Proceedings
of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 961{970 (2015)
5. Carreira, J., Zisserman, A.: Quo vadis, action recogni-
tion? a new model and the kinetics dataset. In: Proceed-
ings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 6299{6308 (2017)
6. Chao, Y.W., Vijayanarasimhan, S., Seybold, B., Ross,
D.A., Deng, J., Sukthankar, R.: Rethinking the faster12 Chen Ju et al.
r-cnn architecture for temporal action localisation. In:
Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pp. 1130{1139 (2018)
7. Chao, Y.W., Vijayanarasimhan, S., Seybold, B., Ross,
D.A., Deng, J., Sukthankar, R.: Rethinking the faster
r-cnn architecture for temporal action localization. In:
Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pp. 1130{1139 (2018)
8. Dai, R., Das, S., Kahatapitiya, K., Ryoo, M.S., Bremond,
F.: Ms-tct: Multi-scale temporal convtransformer for ac-
tion detection. In: Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pp. 20041{
20051 (2022)
9. Frome, A., Corrado, G.S., Shlens, J., Bengio, S., Dean,
J., Ranzato, M.A., Mikolov, T.: Devise: A deep visual-
semantic embedding model. In: Advances in Neural In-
formation Processing Systems (2013)
10. Gao, J., Chen, K., Nevatia, R.: Ctap: Complementary
temporal action proposal generation. In: Proceedings of
the European Conference on Computer Vision, pp. 68{83
(2018)
11. Gao, J., Chen, M., Xu, C.: Fine-grained temporal con-
trastive learning for weakly-supervised temporal action
localization. In: Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 19999{
20009 (2022)
12. Gao, J., Yang, Z., Chen, K., Sun, C., Nevatia, R.: Turn
tap: Temporal unit regression network for temporal ac-
tion proposals. In: Proceedings of the International Con-
ference on Computer Vision, pp. 3628{3636 (2017)
13. Gao, T., Fisch, A., Chen, D.: Making pre-trained lan-
guage models better few-shot learners. In: Association
for Computational Linguistics (2021)
14. He, B., Yang, X., Kang, L., Cheng, Z., Zhou, X., Shri-
vastava, A.: Asm-loc: Action-aware segment modeling for
weakly-supervised temporal action localization. In: Pro-
ceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pp. 13925{13935 (2022)
15. Huang, L., Wang, L., Li, H.: Foreground-action consis-
tency network for weakly supervised temporal action lo-
calization. In: Proceedings of the International Confer-
ence on Computer Vision, pp. 8002{8011 (2021)
16. Jia, C., Yang, Y., Xia, Y., Chen, Y.T., Parekh, Z., Pham,
H., Le, Q.V., Sung, Y., Li, Z., Duerig, T.: Scaling up
visual and vision-language representation learning with
noisy text supervision. In: Proceedings of the Interna-
tional Conference on Machine Learning, pp. 4904{4916
(2021)
17. Jia, M., Tang, L., Chen, B.C., Cardie, C., Belongie, S.,
Hariharan, B., Lim, S.N.: Visual prompt tuning. In: Pro-
ceedings of the European Conference on Computer Vi-
sion, pp. 709{727 (2022)
18. Jiang, Y.G., Liu, J., Zamir, A.R., Toderici, G., Laptev,
I., Shah, M., Sukthankar, R.: Thumos challenge: Action
recognition with a large number of classes. URL http:
//crcv.ucf.edu/THUMOS14/
19. Jiang, Z., Xu, F.F., Araki, J., Neubig, G.: How can we
know what language models know? Transactions of the
Association for Computational Linguistics pp. 423{438
(2020)
20. Ju, C., Han, T., Zheng, K., Zhang, Y., Xie, W.: Prompt-
ing visual-language models for ecient video understand-
ing. In: Proceedings of the European Conference on Com-
puter Vision, pp. 105{124. Springer (2022)
21. Ju, C., Wang, H., Liu, J., Ma, C., Zhao, P., Zhang, Y.,
Chang, J., Tian, Q.: Constraint and union for partially-
supervised temporal sentence grounding. arXiv preprint
arXiv:2302.09850 (2023)22. Ju, C., Zhao, P., Chen, S., Zhang, Y., Wang, Y., Tian,
Q.: Divide and conquer for single-frame temporal action
localization. In: Proceedings of the International Confer-
ence on Computer Vision, pp. 13455{13464 (2021)
23. Ju, C., Zhao, P., Chen, S., Zhang, Y., Zhang, X., Tian,
Q.: Adaptive mutual supervision for weakly-supervised
temporal action localization. IEEE Transactions on Mul-
timedia (2022)
24. Ju, C., Zhao, P., Zhang, Y., Wang, Y., Tian, Q.:
Point-level temporal action localization: Bridging fully-
supervised proposals to weakly-supervised losses. arXiv
preprint arXiv:2012.08236 (2020)
25. Ju, C., Zheng, K., Liu, J., Zhao, P., Zhang, Y., Chang,
J., Wang, Y., Tian, Q.: Distilling vision-language pre-
training to collaborate with weakly-supervised tempo-
ral action localization. arXiv preprint arXiv:2212.09335
(2022)
26. Ke, Y., Sukthankar, R., Hebert, M.: Volumetric features
for video event detection. International Journal of Com-
puter Vision 88, 339{362 (2010)
27. Lee, P., Byun, H.: Learning action completeness from
points for weakly-supervised temporal action localiza-
tion. In: Proceedings of the International Conference on
Computer Vision, pp. 13648{13657 (2021)
28. Lee, P., Uh, Y., Byun, H.: Background suppression net-
work for weakly-supervised temporal action localization.
In: Proceedings of the AAAI Conference on Articial In-
telligence, pp. 11320{11327 (2020)
29. Lee, P., Wang, J., Lu, Y., Byun, H.: Weakly-supervised
temporal action localization by uncertainty modeling. In:
Proceedings of the AAAI Conference on Articial Intel-
ligence, pp. 1854{1862 (2021)
30. Lester, B., Al-Rfou, R., Constant, N.: The power of scale
for parameter-ecient prompt tuning. In: Proceedings
of the Conference on Empirical Methods in Natural Lan-
guage Processinng (2021)
31. Li, X.L., Liang, P.: Prex-tuning: Optimizing continuous
prompts for generation. In: Association for Computa-
tional Linguistics (2021)
32. Li, X.L., Liang, P.: Prex-tuning: Optimizing continuous
prompts for generation. In: Association for Computa-
tional Linguistics (2021)
33. Lin, C., Xu, C., Luo, D., Wang, Y., Tai, Y., Wang, C., Li,
J., Huang, F., Fu, Y.: Learning salient boundary feature
for anchor-free temporal action localization. In: Proceed-
ings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 3320{3329 (2021)
34. Lin, T., Liu, X., Li, X., Ding, E., Wen, S.: Bmn:
Boundary-matching network for temporal action pro-
posal generation. In: Proceedings of the International
Conference on Computer Vision, pp. 3889{3898 (2019)
35. Lin, T., Zhao, X., Shou, Z.: Single shot temporal action
detection. In: Proceedings of ACM International Confer-
ence on Multimedia, pp. 988{996 (2017)
36. Lin, T., Zhao, X., Su, H., Wang, C., Yang, M.: Bsn:
Boundary sensitive network for temporal action proposal
generation. In: Proceedings of the European Conference
on Computer Vision, pp. 3{19 (2018)
37. Lin, T., Zhao, X., Su, H., Wang, C., Yang, M.: BSN:
Boundary sensitive network for temporal action proposal
generation. In: Proceedings of the European Conference
on Computer Vision, pp. 3{19 (2018)
38. Liu, D., Jiang, T., Wang, Y.: Completeness modeling and
context separation for weakly supervised temporal ac-
tion localization. In: Proceedings of the IEEE Confer-
ence on Computer Vision and Pattern Recognition, pp.
1298{1307 (2019)Multi-modal Prompting for low-shot TAL 13
39. Liu, Y., Ma, L., Zhang, Y., Liu, W., Chang, S.F.: Multi-
granularity generator for temporal action proposal. In:
Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pp. 3604{3613 (2019)
40. Liu, Z., Wang, L., Zhang, Q., Gao, Z., Niu, Z., Zheng, N.,
Hua, G.: Weakly supervised temporal action localization
through contrast based evaluation networks. In: Proceed-
ings of the International Conference on Computer Vision,
pp. 3899{3908 (2019)
41. Lu, Y., Liu, J., Zhang, Y., Liu, Y., Tian, X.: Prompt
distribution learning. In: Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pp.
5206{5215 (2022)
42. Luo, H., Ji, L., Zhong, M., Chen, Y., Lei, W., Duan, N.,
Li, T.: Clip4clip: An empirical study of clip for end to
end video clip retrieval and captioning. Neurocomputing
pp. 293{304 (2022)
43. Luo, W., Zhang, T., Yang, W., Liu, J., Mei, T., Wu,
F., Zhang, Y.: Action unit memory network for weakly
supervised temporal action localization. In: Proceedings
of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 9969{9979 (2021)
44. Luo, Z., Guillory, D., Shi, B., Ke, W., Wan, F., Darrell,
T., Xu, H.: Weakly-supervised action localization with
expectation-maximization multi-instance learning. In:
Proceedings of the European Conference on Computer
Vision, pp. 729{745 (2020)
45. Ma, C., Yang, Y., Ju, C., Zhang, F., Liu, J., Wang, Y.,
Zhang, Y., Wang, Y.: Diusionseg: Adapting diusion
towards unsupervised object discovery. arXiv preprint
arXiv:2303.09813 (2023)
46. Ma, F., Zhu, L., Yang, Y.: Weakly supervised moment lo-
calization with decoupled consistent concept prediction.
International Journal of Computer Vision 130(5), 1244{
1258 (2022)
47. Ma, F., Zhu, L., Yang, Y., Zha, S., Kundu, G., Feiszli,
M., Shou, Z.: Sf-net: Single-frame supervision for tempo-
ral action localization. In: Proceedings of the European
Conference on Computer Vision, pp. 420{437. Springer
(2020)
48. Mettes, P., Snoek, C.G.: Pointly-supervised action local-
ization. International Journal of Computer Vision 127,
263{281 (2019)
49. Min, K., Corso, J.J.: Adversarial background-aware loss
for weakly-supervised temporal activity localization. In:
Proceedings of the European Conference on Computer
Vision, pp. 283{299 (2020)
50. Mori, Y., Takahashi, H., Oka, R.: Image-to-word transfor-
mation based on dividing and vector quantizing images
with words. In: First International Workshop on Mul-
timedia Intelligent Storage and Retrieval Management
(ACM Multimedia Conference), pp. 1{9 (1999)
51. Nag, S., Zhu, X., Song, Y.Z., Xiang, T.: Zero-shot tempo-
ral action detection via vision-language prompting. In:
Proceedings of the European Conference on Computer
Vision, pp. 681{697. Springer (2022)
52. Narayan, S., Cholakkal, H., Hayat, M., Khan, F.S., Yang,
M.H., Shao, L.: D2-net: Weakly-supervised action local-
ization via discriminative embeddings and denoised acti-
vations. In: Proceedings of the International Conference
on Computer Vision, pp. 13608{13617 (2021)
53. Narayan, S., Cholakkal, H., Khan, F.S., Shao, L.: 3c-net:
Category count and center loss for weakly-supervised ac-
tion localization. In: Proceedings of the International
Conference on Computer Vision, pp. 8679{8687 (2019)
54. Nguyen, P., Liu, T., Prasad, G., Han, B.: Weakly su-
pervised action localization by sparse temporal poolingnetwork. In: Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 6752{
6761 (2018)
55. Nguyen, P.X., Ramanan, D., Fowlkes, C.C.: Weakly-
supervised action localization with background model-
ing. In: Proceedings of the International Conference on
Computer Vision, pp. 5502{5511 (2019)
56. Ni, B., Peng, H., Chen, M., Zhang, S., Meng, G., Fu,
J., Xiang, S., Ling, H.: Expanding language-image pre-
trained models for general video recognition. In: Proceed-
ings of the European Conference on Computer Vision, pp.
1{18 (2022)
57. Paul, S., Roy, S., Roy-Chowdhury, A.: W-talc: Weakly-
supervised temporal activity localization and classica-
tion. In: Proceedings of the European Conference on
Computer Vision, pp. 563{579 (2018)
58. Qing, Z., Su, H., Gan, W., Wang, D., Wu, W., Wang, X.,
Qiao, Y., Yan, J., Gao, C., Sang, N.: Temporal context
aggregation network for temporal action proposal rene-
ment. In: Proceedings of the International Conference on
Computer Vision, pp. 485{494 (2021)
59. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh,
G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P.,
Clark, J., Krueger, G., Sutskever, I.: Learning transfer-
able visual models from natural language supervision. In:
Proceedings of the International Conference on Machine
Learning, pp. 8748{8763 (2021)
60. Schick, T., Sch utze, H.: Exploiting cloze questions for
few shot text classication and natural language infer-
ence. In: In Proceedings of the 16th Conference of the
European Chapter of the Association for Computer Lin-
guistics (2021)
61. Shi, B., Dai, Q., Mu, Y., Wang, J.: Weakly-supervised
action localization by generative attention modeling. In:
Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pp. 1009{1019 (2020)
62. Shin, T., Razeghi, Y., IV, R.L.L., Wallace, E., Singh, S.:
AutoPrompt: Eliciting knowledge from language models
with automatically generated prompts. In: Proceedings
of the Conference on Empirical Methods in Natural Lan-
guage Processinng (2020)
63. Shou, Z., Chan, J., Zareian, A., Miyazawa, K., Chang,
S.F.: Cdc: Convolutional-de-convolutional networks for
precise temporal action localization in untrimmed videos.
In: Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 5734{5743 (2017)
64. Shou, Z., Gao, H., Zhang, L., Miyazawa, K., Chang, S.F.:
Autoloc: Weakly-supervised temporal action localization
in untrimmed videos. In: Proceedings of the European
Conference on Computer Vision, pp. 154{171 (2018)
65. Shou, Z., Wang, D., Chang, S.F.: Temporal action lo-
calization in untrimmed videos via multi-stage cnns. In:
Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pp. 1049{1058 (2016)
66. Shu, T., Xie, D., Rothrock, B., Todorovic, S., Chun Zhu,
S.: Joint inference of groups, events and human roles in
aerial videos. In: CVPR, pp. 4576{4584 (2015)
67. Tan, J., Tang, J., Wang, L., Wu, G.: Relaxed transformer
decoders for direct action proposal generation. In: Pro-
ceedings of the International Conference on Computer
Vision, pp. 13526{13535 (2021)
68. Vo, K., Truong, S., Yamazaki, K., Raj, B., Tran, M.T.,
Le, N.: Aoe-net: Entities interactions modeling with
adaptive attention mechanism for temporal action pro-
posals generation. International Journal of Computer
Vision 131(1), 302{323 (2023)14 Chen Ju et al.
69. Wang, L., Xiong, Y., Lin, D., Van Gool, L.: Untrimmed-
nets for weakly supervised action recognition and detec-
tion. In: Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, pp. 4325{4334
(2017)
70. Wang, Q., Zhang, Y., Zheng, Y., Pan, P.: Rcl: Recurrent
continuous localization for temporal action detection. In:
Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pp. 13566{13575 (2022)
71. Wedel, A., Pock, T., Zach, C., Bischof, H., Cremers, D.:
An improved algorithm for tv-l 1 optical ow. In: Statisti-
cal and geometrical approaches to visual motion analysis,
pp. 23{45. Springer (2009)
72. Weston, J., Bengio, S., Usunier, N.: WSABIE: Scaling
up to large vocabulary image annotation. In: Proceed-
ings of the International Joint Conference on Articial
Intelligence (2011)
73. Xu, H., Das, A., Saenko, K.: R-c3d: Region convolutional
3d network for temporal activity detection. In: Proceed-
ings of the International Conference on Computer Vision,
pp. 5783{5792 (2017)
74. Xu, H., Ghosh, G., Huang, P.Y., Okhonko, D., Agha-
janyan, A., Metze, F., Zettlemoyer, L., Feichtenhofer, C.:
Videoclip: Contrastive pre-training for zero-shot video-
text understanding. arXiv preprint arXiv:2109.14084
(2021)
75. Xu, M., Zhao, C., Rojas, D.S., Thabet, A., Ghanem, B.:
G-tad: Sub-graph localization for temporal action detec-
tion. In: Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, pp. 10156{10165
(2020)
76. Xu, Y., Zhang, C., Cheng, Z., Xie, J., Niu, Y., Pu, S., Wu,
F.: Segregated temporal assembly recurrent networks for
weakly supervised multiple action detection. In: Proceed-
ings of the AAAI Conference on Articial Intelligence,
pp. 9070{9078 (2019)
77. Yang, L., Han, J., Zhao, T., Lin, T., Zhang, D., Chen,
J.: Background-click supervision for temporal action lo-
calization. IEEE Transactions on Pattern Analysis and
Machine Intelligence pp. 9814{9829 (2021)
78. Yang, L., Peng, H., Zhang, D., Fu, J., Han, J.: Revisit-
ing anchor mechanisms for temporal action localization.
IEEE Transactions on Image Processing pp. 8535{8548
(2020)
79. Yang, X., Zhang, H., Gao, C., Cai, J.: Learning to collo-
cate visual-linguistic neural modules for image caption-
ing. International Journal of Computer Vision pp. 1{19
(2022)
80. Yao, L., Huang, R., Hou, L., Lu, G., Niu, M., Xu, H.,
Liang, X., Li, Z., Jiang, X., Xu, C.: Filip: Fine-grained
interactive language-image pre-training. In: Proceedings
of the International Conference on Learning Representa-
tions (2022)
81. Yuan, L., Chen, D., Chen, Y.L., Codella, N., Dai, X.,
Gao, J., Hu, H., Huang, X., Li, B., Li, C., et al.: Florence:
A new foundation model for computer vision. arXiv
preprint arXiv:2111.11432 (2021)
82. Yudistira, N., Kavitha, M.S., Kurita, T.: Weakly-
supervised action localization, and action recognition us-
ing global{local attention of 3d cnn. International Jour-
nal of Computer Vision 130(10), 2349{2363 (2022)
83. Zeng, R., Huang, W., Tan, M., Rong, Y., Zhao, P.,
Huang, J., Gan, C.: Graph convolutional networks for
temporal action localization. In: Proceedings of the Inter-
national Conference on Computer Vision, pp. 7094{7103
(2019)84. Zhai, X., Wang, X., Mustafa, B., Steiner, A., Keysers,
D., Kolesnikov, A., Beyer, L.: Lit: Zero-shot transfer with
locked-image text tuning. In: Proceedings of the IEEE
Conference on Computer Vision and Pattern Recogni-
tion, pp. 18123{18133 (2022)
85. Zhang, C., Wu, J., Li, Y.: Actionformer: Localizing mo-
ments of actions with transformers. In: Proceedings of
the European Conference on Computer Vision, pp. 492{
510. Springer (2022)
86. Zhang, C., Xu, Y., Cheng, Z., Niu, Y., Pu, S., Wu, F.,
Zou, F.: Adversarial seeded sequence growing for weakly-
supervised temporal action localization. In: Proceedings
of ACM International Conference on Multimedia, pp.
738{746 (2019)
87. Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M.,
Chen, S., Dewan, C., Diab, M., Li, X., Lin, X.V., et al.:
Opt: Open pre-trained transformer language models. In:
Proceedings of the Conference on Empirical Methods in
Natural Language Processinng (2022)
88. Zhang, Y., Zhang, X.Y., Shi, H.: Ow-tal: Learning un-
known human activities for open-world temporal action
localization. Pattern Recognition p. 109027 (2022)
89. Zhao, H., Torralba, A., Torresani, L., Yan, Z.: Hacs: Hu-
man action clips and segments dataset for recognition
and temporal localization. In: Proceedings of the Inter-
national Conference on Computer Vision, pp. 8668{8678
(2019)
90. Zhao, P., Xie, L., Ju, C., Zhang, Y., Wang, Y., Tian, Q.:
Bottom-up temporal action localization with mutual reg-
ularization. In: Proceedings of the European Conference
on Computer Vision, pp. 539{555. Springer (2020)
91. Zhao, T., Han, J., Yang, L., Wang, B., Zhang, D.: Soda:
Weakly supervised temporal action localization based on
astute background response and self-distillation learning.
International Journal of Computer Vision 129(8), 2474{
2498 (2021)
92. Zhao, Y., Xiong, Y., Wang, L., Wu, Z., Tang, X., Lin,
D.: Temporal action detection with structured segment
networks. In: Proceedings of the International Conference
on Computer Vision, pp. 2914{2923 (2017)
93. Zheng, Z., Wang, P., Liu, W., Li, J., Ye, R., Ren, D.:
Distance-iou loss: Faster and better learning for bounding
box regression. In: Proceedings of the AAAI Conference
on Articial Intelligence, pp. 12993{13000 (2020)
94. Zhou, K., Yang, J., Loy, C.C., Liu, Z.: Learning to prompt
for vision-language models. In: International Journal of
Computer Vision, pp. 2337{2348 (2019)
95. Zhou, K., Yang, J., Loy, C.C., Liu, Z.: Conditional
prompt learning for vision-language models. In: Proceed-
ings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 16816{16825 (2022)
96. Zhu, B., Niu, Y., Han, Y., Wu, Y., Zhang, H.: Prompt-
aligned gradient for prompt tuning. arXiv preprint
arXiv:2205.14865 (2022)
97. Zhu, X., Loy, C.C., Gong, S.: Learning from multiple
sources for video summarisation. International Journal
of Computer Vision 117, 247{268 (2016)
98. Zhu, Z., Tang, W., Wang, L., Zheng, N., Hua, G.: Enrich-
ing local and global contexts for temporal action local-
ization. In: Proceedings of the International Conference
on Computer Vision, pp. 13516{13525 (2021)