IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, 2023 1
Progressive Transfer Learning for Dexterous
In-Hand Manipulation with Multi-Fingered
Anthropomorphic Hand
Yongkang Luo1,Member, IEEE , Wanyi Li1,Member, IEEE , Peng Wang1, 2, 3,Member, IEEE , Haonan Duan1,2,
Wei Wei1,2,Student Member, IEEE , and Jia Sun1
Abstract ‚ÄîDexterous in-hand manipulation for a multi-Ô¨Ångered
anthropomorphic hand is extremely difÔ¨Åcult because of the
high-dimensional state and action spaces, rich contact patterns
between the Ô¨Ångers and objects. Even though deep reinforcement
learning has made moderate progress and demonstrated its
strong potential for manipulation, it is still faced with certain
challenges, such as large-scale data collection and high sample
complexity. Especially, for some slight change scenes, it always
needs to re-collect vast amounts of data and carry out numerous
iterations of Ô¨Åne-tuning. Remarkably, humans can quickly trans-
fer learned manipulation skills to different scenarios with little su-
pervision. Inspired by human Ô¨Çexible transfer learning capability,
we propose a novel dexterous in-hand manipulation progressive
transfer learning framework (PTL) based on efÔ¨Åciently utilizing
the collected trajectories and the source-trained dynamics model.
This framework adopts progressive neural networks for dynamics
model transfer learning on samples selected by a new samples
selection method based on dynamics properties, rewards and
scores of the trajectories. Experimental results on contact-rich
anthropomorphic hand manipulation tasks show that our method
can efÔ¨Åciently and effectively learn in-hand manipulation skills
with a few online attempts and adjustment learning under the
new scene. Compared to learning from scratch, our method can
reduce training time costs by 95%.
Index Terms ‚ÄîDeep reinforcement learning, transfer learning,
in-hand manipulation, experience replay, progressive neural net-
works.
I. I NTRODUCTION
DEXTEROUS manipulation of objects is a fundamental
task in human daily life, it is still one type of the most
complex and largely unsolved control problems in robotics.
The goal of dexterous in-hand manipulation is to control object
movement with precise control of forces and motions for a
multi-Ô¨Ångered anthropomorphic hand that has high degrees of
freedom [1]. Dexterous in-hand manipulation not only needs
This work was supported in part by the National Natural Science Foundation
of China under Grants (91748131, 62006229, and 61771471), in part by the
InnoHK Project, and in part by the Strategic Priority Research Program of
Chinese Academy of Science under Grant XDB32050100. (Corresponding
author: Peng Wang.)
E-mail:fyongkang.luo, wanyi.li, peng wang, duanhaonan2021,
wei.wei2018, jia.sun g@ia.ac.cn.
1State Key Laboratory of Multimodal ArtiÔ¨Åcial Intelligence Systems,
Institute of Automation, Chinese Academy of Sciences, Beijing, 100190,
China.
2School of ArtiÔ¨Åcial Intelligence, University of Chinese Academy of
Sciences, Beijing, 100190, China
3Centre for ArtiÔ¨Åcial Intelligence and Robotics, Hong Kong Institute of
Science and Innovation, Chinese Academy of Sciences, Hong Kong, 999077,
Chinato effectively coordinate the high degree-of-freedom Ô¨Ångers,
but also stabilize objects through contacts [2].
As dexterous manipulation has been an active area of re-
search for decades, many approaches have been proposed [3]‚Äì
[7]. These methods use open-loop planning after computing a
trajectory, which is based on exact models of the robot hand
and objects. Since accurate models are not easy to obtain, it is
arduous for such methods to realize satisfactory performance.
Some methods adopt a closed-loop approach for dexterous
manipulation with sensor feedback during execution [8], [9].
These methods can correct the failure cases at runtime, but
they still need reasonable models about the kinematics and
dynamics of the robot, which are difÔ¨Åcult for an under-actuated
multi-Ô¨Ångered anthropomorphic hand with high degrees of
freedom.
The development of deep reinforcement learning [10]‚Äì
[13] in recent years has provided a new way for dexterous
manipulation learning [14], [15]. Learning complex dexterous
in-hand manipulation with deep reinforcement learning has
obtained impressive performance. [16] proposed a distributed
distributional deep deterministic policy gradient algorithm for
dexterous manipulation tasks including catch, pick-up-and-
orient, and rotate-in-hand on a simulated model of the Johns
Hopkins Modular Prosthetic Limb hand with a total of 22
degrees of freedom [17]. [18] designed a multi-goal reinforce-
ment learning framework for in-hand object manipulation with
a Shadow Dexterous Hand [19] that has Ô¨Åve Ô¨Ångers with a
total of 24 degrees of freedom. [14] proposed a deep dynam-
ics model for learning dexterous manipulation on a 24-DoF
anthropomorphic hand, which demonstrates that a learning-
based method can efÔ¨Åciently and effectively learn contact-
rich dexterous manipulation skills. [15] used reinforcement
learning to learn dexterous in-hand manipulation policies,
which are trained in a simulated environment and transferred to
a physical Shadow Dexterous Hand. The learning processing
is conducted on a distributed RL system with a pool of 384
worker machines, each with 16 CPU cores, which can generate
about 2 years of simulated experience per hour. It shows that
in-hand manipulation learning with reinforcement learning in
a simulator can achieve an unprecedented level of dexterity
on a multi-Ô¨Ångered anthropomorphic hand. However, deep
reinforcement learning for in-hand manipulation needs a large-
scale dataset, which is expensive to gather. Furthermore, even
the minor changes in the manipulation scenario will cause the
performance sharp reduction of the source-trained model thatarXiv:2304.09526v1  [cs.RO]  19 Apr 2023IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, 2023 2
Fig. 1. Motivation of progressive transfer learning for in-hand manipulation with multi-Ô¨Ångered anthropomorphic hand. a) Manipulation learning based on
model-based reinforcement learning is time-consuming. b) New scenes with changed manipulation objects. c) Using the source-trained model directly in the
new scenes often fails. d) Progressive transfer learning in the new scenes through leveraging the collected trajectories and the source-trained dynamics model
performs well.
has been trained with a large amount of data and time, as
shown in Figure 1.
Due to the transfer learning ability, human is able to quickly
transfer learned dexterous manipulation skills to different
scenarios with minimal supervision [20]. Transfer learning
utilizes external expertise from the related well-trained task,
which has been shown to effectively accelerate the learning
process. Fine-tuning method is a simple transfer learning
method, which Ô¨Åne-tunes the pre-trained model weight on a
small amount of data for the target task. This method can be
used for in-hand manipulation policy learning. However, the
Ô¨Åne-tuning method just utilizes the pre-trained model weights,
which is not enough for the complicated in-hand manipulation
task. In the reinforcement learning procedure, the agent needs
to interact with the environment to collect enough experiencesfor improving performance. Furthermore, at the initial training
stage in the target scene, the interaction experiences include
unsuccessful manipulation experiences. Fine-tuning the model
with these experiences may mislead the model training to arise
catastrophic forgetting, which may increase the training time
and decrease the performance.
In order to quickly learn the complicated in-hand manip-
ulation skills in new and similar scenarios, it is better to
effectively leverage the collected trajectories and the source-
trained dynamics model. Based on this consideration, we
propose a new dexterous in-hand manipulation progressive
transfer learning framework (PTL) for the multi-Ô¨Ångered an-
thropomorphic hand. The proposed method adopts progressive
neural networks for dexterous manipulation dynamics model
transfer learning, which uses lateral connection structural withIEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, 2023 3
the pre-trained model. To effectively selected experiences from
the trajectory replay buffer, we designed a new transfer learn-
ing samples selection method based on dynamics properties,
rewards and scores of trajectories.
Experimental results on contact-rich anthropomorphic hand
manipulation tasks show that the proposed method can efÔ¨Å-
ciently and effective learn the in-hand manipulation skills with
a few online attempts and adjustment learning under the new
scene in which the manipulation objects have been changed.
The proposed method reduces the data collection time and
obtains better performance than learning from scratch and Ô¨Åne-
tuning methods.
Our main contributions can be summarized as follows:
We propose a novel dexterous in-hand manipulation
progressive transfer learning framework (PTL) based
on leveraging the collected trajectories and the source-
trained dynamics model. The proposed framework can
efÔ¨Åciently learn the in-hand manipulation skills with a
few online attempts in the new scenes.
We propose a new efÔ¨Åcient transfer learning samples
selection method which is based on the dynamics prop-
erties, rewards and scores of the collected trajectories.
Using this method, our framework can effectively select
trajectories from the source domain collected trajectory
replay buffer, and reduce storage requirements.
We design three transfer learning tasks of dexterous in-
hand manipulation to evaluate the performance. Extensive
experimental results for comparison and ablation study
demonstrate the effectiveness of the proposed method.
II. R ELATED WORK
This section provides a brief summary of several related
works, including in-hand dexterous manipulation, deep trans-
fer reinforcement learning, and experience replay.
A. In-hand Dexterous Manipulation
The goal of in-hand manipulation with a dexterous hand is
to operate the objects to the desired pose by controlling Ô¨Ångers
dynamically over time [21]. Common in-hand manipulation
tasks include rotation, regrasping, and rolling through Ô¨Ånely
controlled Ô¨Ångers contacts [4], [22]‚Äì[24]. In recent years,
many model-based control methods have been explored for in-
hand manipulation with underactuated hands [25]‚Äì[28]. The
performance of these model-based methods depends on the
accuracy of modeling manipulation scenarios.
In order to make robot manipulators work gracefully in
practical applications, the robot manipulators must be adaptive
to variations of properties of objects and the robot [29].
Machine learning is an important approach for intelligent
robotics to deal with the model variation of manipulation
environment [30]. Such as, [31]‚Äì[33] use machine learning
methods to deal with sensing information for multi-Ô¨Ångered
in-hand manipulation. Deep reinforcement learning has been
widely used for dexterous manipulation, which has shown
great potential [14], [15], [34]‚Äì[36]. These methods obtain
good performance based on a large amount of dataset and atime-consuming training process. If the manipulation scenarios
have been changed, the trained models will not work well.
Learning a single strategy that works well in all envi-
ronments is very difÔ¨Åcult. It requires lots of training data
as the methods adopted in [15], [37]‚Äì[39], which generate
many and diversity of training data with extensive randomized
simulation environments. Due to the complexity of the in-hand
manipulation task, acquiring sufÔ¨Åcient interaction samples is
not easy to implement. Therefore, in this study, we focus
on the problem of how to effectively and efÔ¨Åciently utilize
external expertise to speed up dexterous in-hand manipulation
learning processing with better performance.
B. Deep Transfer Reinforcement Learning
Transfer learning in reinforcement learning transfers knowl-
edge from some source tasks to a target task for addressing
sequential decision-making problems [40]. The transferred
knowledge can speed up the learning process and improve
performance by reducing data collection and training time
consumption [41], [42].
There are many kinds of transfer learning methods. For
example, forward transfer methods that train a model on one
task and transfer it to a new task, include Ô¨Åne-tuning method
[43]‚Äì[45], domain adaptation method [46]‚Äì[48], and random-
ization method [49], [50]. These methods obtain impressive
performance, while they require a large amount of dataset,
computational cost, and training time. Multi-task transfer
methods, which train a model on many tasks and transfer it
to a new task, include policy distillation methods [51], [52],
and contextual policy method [53]. Transfer learning methods
based on models and policies, include transferring with learned
models [54], [55], learned policy [56], [57], and successor
representation [58], [59].
Transfer reinforcement learning for dexterous manipulation
on a multi-Ô¨Ångered anthropomorphic hand is very challenging,
which involves dealing with the high-dimensionality of manip-
ulation dynamics. Prior works focus on sim-to-real transfer
learning for dexterous manipulation [15], [60] with domain
randomization strategies. Different from previous works, we
focus on using collected experiences and trained models for
faster learning and better performance in the new domain task.
C. Experience Replay
Experience replay [61] is used popularly for stabilizing
learning in modern deep reinforcement learning [10], [62]‚Äì
[64], which improves sample efÔ¨Åciency by reusing the stored
uncorrelated experiences. Collecting diverse exploratory data
can effectively improve the performance of the ofÔ¨Çine rein-
forcement learning algorithm, as proved in [65].
For transfer learning in reinforcement learning, experience
replay is beneÔ¨Åcial for the target task that only has a small
number of interaction samples. In the transfer learning pro-
cedure, not all the experience samples in the source task
are good for the target task learning, which due to the
differences in dynamics characteristics between the source task
and target task. In order to reduce the learning complexity, [66]
selects samples from the source task that are most relevantIEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, 2023 4
Fig. 2. Overview of the proposed progressive transfer learning framework for multi-Ô¨Ångered dexterous in-hand manipulation.
to the target task for transfer reinforcement learning. [67],
[68] estimate the importance weight of transfer samples from
source task to target task, and assign their contribution to
the learning process proportional to their importance weight.
These methods achieve better transfer learning performance.
[69] adopts hierarchical experience replay to transfer knowl-
edge for deep reinforcement learning by selecting experiences
from the replay memories, which can effectively accelerate
learning with good performance.
In our progressive transfer learning framework, we measure
the action smoothness of the trajectories and combine them
with rewards and scores of trajectories to select transfer
experiences from the source task.
III. M ETHOD
In this section, we introduce the proposed progressive trans-
fer learning method for dexterous in-hand manipulation with
multi-Ô¨Ångered anthropomorphic hand, which includes prob-
lem statement, framework, transfer learning samples selection
method, progressive dynamics model, and online planning
with model predictive control.
A. Problem Statement
In this work, we focus on the problem of dexterous in-
hand manipulation with faster learning and better perfor-
mance based on collected experiences and trained model.
We deÔ¨Åne the dynamics with a Markov decision process
M= (S;A;p;R ): state in manipulation environment s2S,
robot action a2A, state transition distribution p(s0js;a), and
Rcorresponds to the reward function r(s;a). The goal of
manipulation policy is to select actions with maximizing the
expected total rewards over a trajectory.B. Dexterous In-Hand Manipulation Progressive Transfer
Learning Framework
Dexterous in-hand object manipulation task is complex, and
learning-based methods always require large amounts of data.
Our goal is to speed up transfer learning processing with better
performance. Sample efÔ¨Åciency is an important factor that
we consider. Model-based reinforcement learning approaches
leverage the autonomous learning ability from data-driven
methods, that makes them succeed on complex dexterous ma-
nipulation tasks. As proved in [14], for challenging dexterous
in-hand manipulation, such as Baoding Balls task, the model-
free methods were unable to succeed at this task. Therefore,
we adopt the model-based reinforcement learning with online
planning for dexterous in-hand object manipulation tasks as a
baseline, which is proposed in [14].
In order to effectively and efÔ¨Åciently utilize the collected
experience and the learned dynamics model, we extend the
baseline method with progressive neural networks for dexter-
ous in-hand manipulation progressive transfer learning based
on selected samples from source task. The overall framework
(PTL) is shown in Figure 2. The beneÔ¨Åt of this framework is
that it decouples the dynamics model learning and manipula-
tion policy planning, allowing it to focus more on how well
source tasks may transfer to target tasks in dynamics during
transfer learning. As a result, the framework can simplify
transfer learning while improving sample efÔ¨Åciency.
C. Transfer Learning Samples Selection
Learning dexterous manipulation with reinforcement learn-
ing requires a large amount of dataset, and collecting this
dataset is time-consuming. In the source task, after a long
time of training, it can acquire a well-trained model and collect
large-scale interaction data in the manipulation environment.
When switching to a new task, it is better to make fullIEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, 2023 5
use of the collected experience data to speed up the transfer
learning process and performance. Using all the source task
datasets can be highly suboptimal [70], due to the negative
transfer when transferred samples from sources being much
different with the target task [71], [72]. Moreover, using all
the source samples needs more storage and computing cost.
Therefore, we design a new transfer sample selection strategy
that is inspired by the computational principles of movement
neuroscience.
The state of the position and velocity of the hand always
changes continuously within a movement [73]. Based on
maximizing smoothness of the hand trajectory, [74] proposed
an optimal control model of movements. Inspired by the above,
we select the trajectories (rollouts) of which the states change
smoothly with high total reward and Ô¨Ånal score from the
source domain collected trajectory buffer. This is a prioritized
experience replay strategy. Thus, we specify a priority value
PViof each trajectory for the transfer sample selection strat-
egy, which is deÔ¨Åned as
PVi= (1Rewardi+(1 1)MeanScore i)e StateTD i;
(1)
whereRewardiis the total reward, MeanScore iis the mean
Ô¨Ånal score.1is a hyperparameter to balance the contribution
of each term. We set 1= 0:4experimentally. StateTDiis
the mean state transition divergence of anthropomorphic hand
in each trajectory, which is deÔ¨Åned as
StateTDi=1
TT 1X
t=0jjshand
t+1 shand
tjj2; (2)
whereTis the total steps of each trajectory. The smaller value
ofStateTDimeans the state changes more smoothly.
In order to select trajectories with higher priority value for
target model training, we adopt the method proposed in [75]
to design prioritized sampling probability pibased on priority
value as follows:
pi=8
<
:1
Nrollouts(1 +)ifPVi>median (PV)
1
NrolloutsifPVi=median (PV)
1
Nrollouts(1 )ifPVi<median (PV);(3)
where2[0;1]is an interpolation hyperparameter, we set
= 0:1experimentally. median (PV)is the median of all
trajectory priority values PV.Nrollouts is the total number of
the collected trajectories (rollouts). We normalize the above
value with
pi=piPNrollouts
ipi: (4)
Then, we randomly sample transfer learning trajectories
Dselected fromDsource with prioritized sampling probability
piat sample ratio number = 0:1. With these selected trajec-
tories, we can generate dynamics quadruple (st;at;rt;st+1)
for progressive dynamics model learning. The algorithm of
transfer learning samples selection is shown in Algorithm 1.
Furthermore, we collect random trajectories
Dtarget random in the target scene, and get the transfer
learning datasetDtarget by combing selected trajectories
Dselected with collected random target scene trajectories
Dtarget random . During robot hand manipulation, we can getthe target trajectory data, and store it into trajectory buffer
for experience replay. Meanwhile, we delete some outdated
trajectories that come from the source task scene. With this
iteratively updating transferring samples strategy, we can
make the dynamics model learning procedure progressively
focus on the interaction data in the target scene.
Algorithm 1: Transfer Learning Samples Selection.
Input: Collected Trajectories: Dsource
Sample Ratio: number
Interpolation Hyperparameter: 
Output: Selected Trajectories: Dselected
1foreach trajectory inDsource do
2 Get total reward of trajectory i:Rewardi
3 Get mean Ô¨Ånal of trajectory i:MeanScore i
4 Compute robot state transition divergence:
StateTDi
5 Compute priority value PViwith Equation 1
6 Compute prioritized sampling probability piwith
Equation 3 and 4
7end
8Sample transfer learning trajectories Dselected from
Dsource with prioritized sampling probability piat
sample ration number .
D. Progressive Dynamics Model
Inspired by [14], we use neural networks to learn an
approximate model ^p(s0js;a)for state transition distribution
p(s0js;a)in manipulation dynamics, which is parameterized
by. The parameterization form of state transition distribution
is based on Gaussian model with ^p(s0js;a) =N(^f(s;a);),
where the mean ^f(s;a)is given by the learned dynamics
model with neural networks, and denotes the covariance of
conditional Gaussian distribution.
In the framework, we use progressive neural networks [56]
to learn the dynamics model of in-hand manipulation. Differ-
ent from the original networks proposed in [56], [57], some
speciÔ¨Åc changes have been made to make it more suitable
for in-hand manipulation transfer learning. First, considering
the sample efÔ¨Åciency problem, we use progressive neural
networks to learn the approximate state transition distribution
^p(s0js;a)for manipulation dynamics, instead of learning the
policy(ajs). Second, the lateral connection structure has
some changes, outputs of the Ô¨Årst and second hidden layers
in the source task column are laterally connected into the Ô¨Årst
and second layers of the target task column. It is different from
[56] where outputs of the Ô¨Årst and second hidden layers in the
source task column are laterally connected with outputs of Ô¨Årst
and second layers in the target task column. This change is
intended to fully leverage the feature representation capability
of the source-trained model. Through these modiÔ¨Åcations, our
framework can obtain better performance than the original
progressive neural networks [56], [57], which is veriÔ¨Åed in
the experimental results.
SpeciÔ¨Åcally, the progressive transfer learning model starts
with the source task column: it is a three-layer network withIEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, 2023 6
hidden activations hk
Source2Rnk
Source , wherenk
Source is the
number of units at layer k=f1;2g. The source task column
has parameters Source that are frozen in the transfer learning
process. The target task column has the same structure with the
source task column with hk
Target2Rnk
Target , wherenk
Target is
the number of units at layer k=f1;2g. The target task column
has parameters Target , which are initialized randomly in the
transfer learning process. The Ô¨Årst hidden layer gets input
from bothh0
Target (networks input) and h1
Source via lateral
connections. The second hidden layer gets input from both
h1
Target andh2
Source via lateral connections.
Based on the progressive dynamics model, we can get each
hidden layer output with:
hk
Target =F(Wk
Targethk 1
Target +Wk
lcF(k
lchk
Source ));(5)
whereWk
Target2Rnk
Targetnk 1
Target is the weight matrix of
layerkof the target task column. Wk
lc2Rnk
Targetnk
Source
denotes the lateral connection weight matrix. k
lcis the learn-
able scalar factor in lateral connection, which is initialized
randomly.F()denotes an element-wise non-linearity function
F(x) =max(0;x). And the output of the hidden layer in
source task column hk
Source is provided with:
hk
Source =F(Wk
Sourcehk 1
Source ); (6)
whereWk
Source2Rnk
Sourcenk 1
Source is the weight matrix of
layerkof the source task column. Note that the h0
Source and
h0
Target are the inputs of the networks which include the states
and actions.
We learn a progressive dynamics functions ^f(st;at)that
predicts change between standst+1. Thus, we can get the
next state with : ^st+1=^f(st;at) +st. And we train the
progressive dynamics model with the standard mean squared
error (MSE) loss as follows:
L=1
jDtargetjX
(st;at;st+1)2Dtarget1
2jj(st+1 st) ^f(st;at)jj2;
(7)
whereDtarget is the transfer learning dataset for the target
scene.
E. Online Planning with Model Predictive Control
Based on the learned dynamics model, we can predict
the state of taking action afrom states, and utilize this
prediction to select optimal actions with online planning based
on model predictive control (MPC) as the method in [14]. This
online planning method performs a short-horizon trajectory
optimization by using the model predictions of different action
sequences. It generates Ncandidate action sequences with Ô¨Ål-
tering and reward-weighted reÔ¨Ånement, which can reduces the
dimensionality of the search space, making it better scale with
dimensionality. Then, it selects the optimal action sequence
with the maximum reward.
F . Overall Algorithm
The overall algorithm of dexterous in-hand manipulation
progressive transfer learning is shown in Algorithm 2.Algorithm 2: Progressive Transfer Learning.
Input: Collected source domain trajectories: Dsource
Trained source domain dynamics model:
Modelsource =ffS
0;:::;fS
Mg
Output: Trained dynamics model for target domain
Modeltarget =ffT
0;:::;fT
Mg
1Initialize the source column network in progressive
dynamics model with the trained dynamics model
weightsModelsource
2Initialize the target column and lateral connection
network in progressive dynamics model randomly
3Get the selected trajectories Dselected fromDsource
with Algorithm 1
4Collect random trajectories Dtarget random in target
scene
5Get the transfer learning dataset Dtarget by combing
samples selectionDselected withDtarget random
6foriter in range( I)do
7 forrollout in range( RN)do
8 Reset environment s0
9 fort in range(T)do
10 at Online Planning with MPC
11 st+1,rt take action at
12Dtarget (st;at;rt;st+1)
13 end
14 end
15 UpdateDtarget by deleting outdated data from
Dselected
16 TrainModeltarget withDtarget
17end
IV. E XPERIMENTS
In this section, we will introduce the transfer learning
tasks, experimental setting, measurement methods, comparison
results, and ablation study.
A. Transfer Learning Tasks
We evaluate our method on some challenging tasks of
dexterous in-hand manipulation that include Baoding Balls
task and Cube Reorientation task, as shown in Figure 3.
The Baoding Balls task aims to rotate two balls around
the palm with a multi-Ô¨Ångered anthropomorphic hand. It is
a contact-rich task that includes not only contacts between
balls and hand, but also contacts between two balls. This leads
to discontinuous dynamics and difÔ¨Åculty to keep two balls
in the hand during rotating two balls. We design a transfer
learning problem by rotating different size balls, which is an
inter-task transfer learning problem [41]. The source task is to
rotate two big balls, and the target task is to rotate two small
balls. They have the same state-action space. However, their
transition dynamics are different due to different ball sizes.
Thus, the same action produces different inÔ¨Çuences on the
same states in the source and target tasks. As shown in Figure
5, the policies of rotating different size balls are different.IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, 2023 7
Fig. 3. Transfer learning task examples for dexterous in-hand manipulation.
When rotating big size balls, it always uses the little Ô¨Ånger to
push the big ball up in the source task, while it uses the ring
Ô¨Ånger to push the small ball up and the little Ô¨Ånger keeps the
ball from dropping in the target task. Even though these two
tasks are very similar, a well-trained model in the source task
is not work in the target task, which is as shown in Figure 5.
To further evaluate the transfer learning performance, we
design a new Baoding Balls task that aims to rotate one big
ball and one small ball simultaneously. Meanwhile, we design
a transfer learning problem that transfers rotating two balls
with the same size to different sizes, as shown in Figure 3.
Furthermore, we also evaluate the transfer learning perfor-
mance on Cube Reorientation task with different size cubes,
as shown in Figure 3. Cube Reorientation task aims to reorient
a cube to a goal pose in hand, which needs to avoid dropping
the cube from the palm.
B. Experimental Setting
In our experiments, the manipulation environment setting is
the same as in the baseline [14]. In Baoding Balls task, the
dimension of the state space is 40, and the dimension of the
action space is 24. The frequency of action control is 10 per
second. The reward function is deÔ¨Åned as:
r= 5jjobjectxyz targetxyzjj 500 1(isdrop )
 500 1(toohigh );(8)
whereobjectxyzandtargetxyzdenote the coordinate of the
balls at current and target respectively. The term  500
1(isdrop )means to penalize for the ball dropping from the
hand, and the term  500 1(toohigh )means to penalize wrist
angle for lifting up too much. Its score is deÔ¨Åned as:
Score = jjobjectxyz targetxyzjj: (9)In Cube Reorientation task, the dimension of the state space
is36, and the dimension of the action space is 24. The reward
function is deÔ¨Åned as:
r= 7jjcuberpy targetrpyjj 100 1(isdrop );(10)
wherecuberpyandtargetrpydenote the rotational orientation
of the cube at current and target respectively. For Cube
Reorientation task, its score is deÔ¨Åned as:
Score = jjcuberpy targetrpyjj: (11)
The hyperparameters for Baoding Balls and Cube Reorien-
tation tasks are set the same as in [14], including RN =
30;T= 100;H = 7;N = 700 (Balling Balls) ;N =
1000 (Cube Reorientation).
C. Measurement Methods
In order to measure the performance, we evaluate the total
rewards and mean Ô¨Ånal scores that are averaged over Ô¨Åve
random seeds. Meanwhile, we measure success rates ( SR)
with rollout total reward ( Rewardi) and mean Ô¨Ånal score
(Scorei) above some thresholds (reward threshold: RT, score
threshold:ST), which are deÔ¨Åned as follows:
SRReward =PNrollouts
i=11(Rewardi>=RT)
Nrollouts; (12)
SRScore =PNrollouts
i=11(Scorei>=ST)
Nrollouts; (13)
For Baoding Balls task, RT= 20;ST = 0:02. For Cube
Reorientation task, RT= 250;ST = 0:5.
D. Comparison Results
For the transfer learning problem on Baoding Balls task
with the same size change of the two balls, we compare
the proposed method with learning from scratch, Ô¨Åne-tuning
from scratch, Ô¨Åne-tuning with selected samples, PNN [56]
and PNN-V1 [57]. Note that PNN [56] and PNN-V1 [57] are
model-free approaches that are unable to succeed at Baoding
Balls task as proved in [14]. Therefore, we modify them into
our framework, which adopts the progressive neural networks
for transition dynamics learning instead of learning the policy
directly. As shown in Table I, PTL can reduce the training time
with better performance. Compare with methods that learn
or Ô¨Åne-tune from scratch, training with experience replay not
only saves time, but also obtains better performance. Learning
or Ô¨Åne-tuning from scratch may get stuck in pursuit of high
reward, while the scores have not been improved, which means
it is unable to complete the task. It shows that the successful
experiences on the source task can guide the training with
better performance on the target task. Figure 4 shows that
the proposed method can obtain more robust training perfor-
mance than Ô¨Åne-tuning method. The manipulation examples
are shown in Figure 5. We can Ô¨Ånd that the PTL model is able
to effectively coordinate the high degree-of-freedom Ô¨Ångers,
which can manipulate the two balls successfully.
For the transfer learning problem on Baoding Balls task
with big and small balls, we compare our method withIEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, 2023 8
TABLE I
TRANSFER LEARNING PERFORMANCE COMPARISONS ON BAODING BALLS TASK WITH THE SAME SIZE CHANGE OF THE TWO BALLS . SUCCEED RATE
WITH REWARD THRESHOLD (-0.20). S UCCEED RATE WITH SCORE THRESHOLD (-0.02).
Method Reward ScoreSucceed Rate
with Reward (%)Succeed Rate
with Score (%)Training
Time (H)Number of
Datapoints
Source task learning -28.920 -0.02 95.0 80.0 110 1,540,080
Learning from scratch  124:88365:948 0:0620:002 76.7 3.0 31 914,034
Fine-tuning from scratch  142:85130:345 0:0640:005 73.3 7.0 2.7 285,046
Fine-tuning  29:5535:078 0:0230:001 95.0 61.5 3.5 177,664
PNN [56]  9:8006:112 0:0150:005 98.8 82.0 4.4 389,001
PNN-V1 [57]  15:82210:152 0:0130:003 97.6 86.8 4.1 395,945
PTL (Our)  7:4562:035  0:0100:001 99.2 92.8 4.7 189,797
1Except Learning from scratch is the result of training with 300 iterations, source task learning is the result of training with 560 iterations, others are
the results of training with 50 iterations.
2Except noting ‚Äùfrom scratch‚Äù is training from scratch, others are trained with selected samples.
3Except source task learning is trained with one random seed, other results are averaged over Ô¨Åve random seeds.
(a)
 (b)
Fig. 4. Transfer learning performance of the training process on Baoding Balls task with the same size change scene. (a) Rewards for model performance
after each iteration of training. (b) Scores for model performance after each iteration of training.
Fig. 5. Quality results on Baoding Balls task with two small balls. a) First row shows the results of the source-trained model on the source task. b) Second
row shows the results of the source-trained model on the target task. c) Third row shows the results of the PTL model on the target task.IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, 2023 9
TABLE II
TRANSFER LEARNING PERFORMANCE COMPARISONS ON BAODING BALLS TASK WITH BIG AND SMALL BALLS . SUCCEED RATE WITH REWARD
THRESHOLD (-0.20). S UCCEED RATE WITH SCORE THRESHOLD (-0.02).
Method Reward ScoreSucceed Rate
with Reward (%)Succeed Rate
with Score (%)Training
Time (H)Number of
Datapoints
Learning from scratch -117.527 -0.064 78.0 1.0 10 747,097
Fine-tuning  61:92641:780 0:0270:006 88.6 64.8 3.1 735,030
PTL (Our)  31:16016:227  0:0240:005 94.6 65.8 3.0 751,592
1Except Learning from scratch is the result of training with 300 iterations, others are the results of training with 100 iterations.
2Fine-tuning and PTL are trained with selected samples.
3Except learning from scratch is trained with one random seed, other results are averaged over Ô¨Åve random seeds.
Fig. 6. Quality results on Baoding Balls task with big and small balls. a) First row shows the results of the trained model on the source task. b) Second row
shows the results of the source-trained model on the target task. c) Third row shows the results of the PTL model on the target task.
TABLE III
TRANSFER LEARNING PERFORMANCE COMPARISONS ON CUBE REORIENTATION TASK WITH DIFFERENT SIZE CUBES . SUCCEED RATE WITH REWARD
THRESHOLD (-250). S UCCEED RATE WITH SCORE THRESHOLD (-0.5).
Method Reward ScoreSucceed Rate
with Reward (%)Succeed Rate
with Score (%)Training
Time (H)Number of
Datapoints
Source task learning -405.013 -0.548 66 60 29 1,016,654
Learning from scratch 300  315:63347:151 0:3900:034 68.6 69.0 6.5 623,944
Learning from scratch 600  290:44929:549 0:3170:029 76.2 76.0 19.3 1,266,063
Fine-tuning  327:02045:184 0:3880:049 71.6 69.4 9 935,888
PTL learning from scratch  281:96031:900  0:2910:036 78.0 78.0 19.3 613,799
PTL (Our)  273 :64130:018 0:3010:060 78.2 77.8 12.5 1,286,745
1Except Learning from scratch 300 & 600 are the results of training with 300 and 600 iterations, source task learning is the result of training with 590
iterations, others are the results of training with 100 iterations.
2Fine-tuning and PTL are trained with selected samples.
3Except source task learning is trained with one random seed, other results are averaged over Ô¨Åve random seeds.
learning from scratch and Ô¨Åne-tuning methods. For this more
challenging problem, our method can also achieve better
performance, as shown in Table II. From the manipulation
examples shown in Figure 6, we can Ô¨Ånd that the source-
trained model cannot predict the state transition accuracy,
which makes it unable to select optimal actions with online
planning method. Based on our PTL framework, our method
can learn the dynamics of the new manipulation scenario more
accurately. The more precise dynamics prediction enables the
multi-Ô¨Ångered anthropomorphic hand to better complete the
dexterous in-hand manipulation with online planning.For the transfer learning problem on Cube Reorientation
task with different size cubes, we compare our method (with or
without selected samples) with learning from scratch and Ô¨Åne-
tuning methods. For this more challenging problem, progres-
sive transfer learning with selected samples from the source
task, can reduce the time of data collection and model training,
as shown in Table III. After progressive transfer learning, the
PTL model can achieve more accurate reorientation than the
source-trained model, which is shown in Figure 7.IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, 2023 10
Fig. 7. Quality results on Cube Reorientation task with different size cubes. a) First row shows the results of the trained model on the source task. b) Second
row shows the results of the source-trained model on the target task. c) Third row shows the results of the PTL model on the target task.
TABLE IV
ABLATION EXPERIMENTS ABOUT TRAINING SAMPLE SELECTION METHODS . SCRATCH : LEARNING FROM SCRATCH WITHOUT EXPERIENCE DATA .
UNIFORM : LEARNING WITH RANDOM UNIFORM SELECTED SAMPLES FROM SOURCE TASK . SELECT : LEARNING WITH SELECTED SAMPLES BY OUR
PROPOSED METHOD FROM SOURCE TASK . SUCCEED RATE WITH REWARD THRESHOLD (-0.20). S UCCEED RATE WITH SCORE THRESHOLD (-0.02).
MethodTraining ExamplesReward ScoreSucceed Rate
with Reward (%)Succeed Rate
with Score (%)Training
Time (H)Number of
Datapoints Scratch Uniform Select
FTp 142:85130:345 0:0640:005 73.3 7.0 2.7 285, 046
FTp 36:26018:789 0:0190:003 93.6 73.8 3.5 175, 366
FTp 29:5535:078 0:0230:001 95.0 61.5 3.5 177, 664
PTLp 17:8194:456 0:0190:006 97.25 73.5 20.3 624, 110
PTLp 26:04317:233 0:0140:003 95.0 85.0 4.3 185, 982
PTLp 7:4562:035  0:0100:001 99.2 92.8 4.7 189, 797
1Except PTL scratch is the result of training with 200 epochs, others are the results of training with 50 epochs.
2All the results are averaged over Ô¨Åve random seeds.
E. Ablation Study
In order to evaluate the effectiveness of each proposed com-
ponent, we conduct some ablation experiments on Baoding
Balls task with the same size change of the two balls. The
ablation experiments include: Scratch‚Äîlearning from scratch
without experience; Uniform‚Äîlearning with random uniform
samples from source task; Select‚Äîlearning with samples
selected by the proposed method.
From Table IV, we can Ô¨Ånd that progressive transfer learn-
ing is able to achieve better performance than Ô¨Åne-tuning
methods, and reduce the training time. Progressive transfer
learning can also avoid getting stuck in a low score with high
reward, which can complete the manipulation task better. It can
also Ô¨Ånd that using the experiences sampled from the source
task can effectively reduce the data collection time and guide
the model training toward better performance.
V. C ONCLUSION
We have proposed a new dexterous in-hand manipula-
tion progressive transfer learning framework for the multi-
Ô¨Ångered anthropomorphic hand. The proposed framework
exploits transfer learning to speed up learning for the newscenarios with a progressive dynamics model and experience
replay based on transferring sample selection. Experiments
on contact-rich anthropomorphic hand manipulation tasks
demonstrate that the proposed method can efÔ¨Åciently and
effectively learn the in-hand manipulation skills with some
online attempts and adjustment learning in the new scenarios.
In our current work, the transfer learning experiments are
only conducted on manipulation object size changed scene. In
the future work, we will focus on more challenging transfer
learning tasks with more domain differences scenarios. Mean-
while, we will apply the proposed method to the real multi-
Ô¨Ångered anthropomorphic hand platform.
REFERENCES
[1] A. M. Okamura, N. Smaby, and M. R. Cutkosky, ‚ÄúAn overview of dexter-
ous manipulation,‚Äù in Proceedings 2000 ICRA. Millennium Conference.
IEEE International Conference on Robotics and Automation. Symposia
Proceedings (Cat. No. 00CH37065) , vol. 1. IEEE, 2000, pp. 255‚Äì262.
[2] V . Kumar, Y . Tassa, T. Erez, and E. Todorov, ‚ÄúReal-time behaviour
synthesis for dynamic hand-manipulation,‚Äù in 2014 IEEE International
Conference on Robotics and Automation (ICRA) . IEEE, 2014, pp.
6808‚Äì6815.
[3] A. Bicchi, ‚ÄúHands for dexterous manipulation and robust grasping: A
difÔ¨Åcult road toward simplicity,‚Äù IEEE Transactions on robotics and
automation , vol. 16, no. 6, pp. 652‚Äì662, 2000.IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, 2023 11
[4] R. R. Ma and A. M. Dollar, ‚ÄúOn dexterity and dexterous manipulation,‚Äù
in2011 15th International Conference on Advanced Robotics (ICAR) .
IEEE, 2011, pp. 1‚Äì7.
[5] W. H. Huang and M. T. Mason, ‚ÄúMechanics, planning, and control
for tapping,‚Äù The International Journal of Robotics Research , vol. 19,
no. 10, pp. 883‚Äì894, 2000.
[6] M. Erdmann, ‚ÄúAn exploration of nonprehensile two-palm manipulation,‚Äù
The International Journal of Robotics Research , vol. 17, no. 5, pp. 485‚Äì
503, 1998.
[7] Y . Bai and C. K. Liu, ‚ÄúDexterous manipulation using both palm
and Ô¨Ångers,‚Äù in 2014 IEEE International Conference on Robotics and
Automation (ICRA) . IEEE, 2014, pp. 1560‚Äì1565.
[8] K. Tahara, S. Arimoto, and M. Yoshida, ‚ÄúDynamic object manipulation
using a virtual frame by a triple soft-Ô¨Ångered robotic hand,‚Äù in 2010
IEEE International Conference on Robotics and Automation . IEEE,
2010, pp. 4322‚Äì4327.
[9] M. Li, H. Yin, K. Tahara, and A. Billard, ‚ÄúLearning object-level
impedance control for robust grasping and dexterous manipulation,‚Äù
in2014 IEEE International Conference on Robotics and Automation
(ICRA) . IEEE, 2014, pp. 6784‚Äì6791.
[10] V . Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.
Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski
et al. , ‚ÄúHuman-level control through deep reinforcement learning,‚Äù
nature , vol. 518, no. 7540, pp. 529‚Äì533, 2015.
[11] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang,
A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton et al. , ‚ÄúMastering
the game of go without human knowledge,‚Äù nature , vol. 550, no. 7676,
pp. 354‚Äì359, 2017.
[12] J. Schrittwieser, I. Antonoglou, T. Hubert, K. Simonyan, L. Sifre,
S. Schmitt, A. Guez, E. Lockhart, D. Hassabis, T. Graepel et al. ,
‚ÄúMastering atari, go, chess and shogi by planning with a learned model,‚Äù
Nature , vol. 588, no. 7839, pp. 604‚Äì609, 2020.
[13] B. Belousov, H. Abdulsamad, P. Klink, S. Parisi, and J. Peters, Rein-
forcement Learning Algorithms: Analysis and Applications . Springer,
2021.
[14] A. Nagabandi, K. Konolige, S. Levine, and V . Kumar, ‚ÄúDeep dynamics
models for learning dexterous manipulation,‚Äù in Conference on Robot
Learning . PMLR, 2020, pp. 1101‚Äì1112.
[15] O. M. Andrychowicz, B. Baker, M. Chociej, R. Jozefowicz, B. McGrew,
J. Pachocki, A. Petron, M. Plappert, G. Powell, A. Ray et al. , ‚ÄúLearning
dexterous in-hand manipulation,‚Äù The International Journal of Robotics
Research , vol. 39, no. 1, pp. 3‚Äì20, 2020.
[16] G. Barth-Maron, M. W. Hoffman, D. Budden, W. Dabney, D. Horgan,
D. Tb, A. Muldal, N. Heess, and T. Lillicrap, ‚ÄúDistributed distributional
deterministic policy gradients,‚Äù arXiv preprint arXiv:1804.08617 , 2018.
[17] M. S. Johannes, J. D. Bigelow, J. M. Burck, S. D. Harshbarger, M. V .
Kozlowski, and T. Van Doren, ‚ÄúAn overview of the developmental
process for the modular prosthetic limb,‚Äù Johns Hopkins APL Technical
Digest , vol. 30, no. 3, pp. 207‚Äì216, 2011.
[18] M. Plappert, M. Andrychowicz, A. Ray, B. McGrew, B. Baker, G. Pow-
ell, J. Schneider, J. Tobin, M. Chociej, P. Welinder et al. , ‚ÄúMulti-goal
reinforcement learning: Challenging robotics environments and request
for research,‚Äù arXiv preprint arXiv:1802.09464 , 2018.
[19] P. TufÔ¨Åeld and H. Elias, ‚ÄúThe shadow robot mimics human actions,‚Äù
Industrial Robot: An International Journal , 2003.
[20] A. Karni, G. Meyer, C. Rey-Hipolito, P. Jezzard, M. M. Adams,
R. Turner, and L. G. Ungerleider, ‚ÄúThe acquisition of skilled motor
performance: fast and slow experience-driven changes in primary motor
cortex,‚Äù Proceedings of the National Academy of Sciences , vol. 95, no. 3,
pp. 861‚Äì868, 1998.
[21] D. Rus, ‚ÄúIn-hand dexterous manipulation of piecewise-smooth 3-d
objects,‚Äù The International Journal of Robotics Research , vol. 18, no. 4,
pp. 355‚Äì381, 1999.
[22] V . Kumar, E. Todorov, and S. Levine, ‚ÄúOptimal control with learned
local models: Application to dexterous manipulation,‚Äù in 2016 IEEE
International Conference on Robotics and Automation (ICRA) . IEEE,
2016, pp. 378‚Äì383.
[23] E. Psomopoulou, D. Karashima, Z. Doulgeri, and K. Tahara, ‚ÄúStable
pinching by controlling Ô¨Ånger relative orientation of robotic Ô¨Ångers with
rolling soft tips,‚Äù Robotica , vol. 36, no. 2, pp. 204‚Äì224, 2018.
[24] S. Cruciani, B. Sundaralingam, K. Hang, V . Kumar, T. Hermans, and
D. Kragic, ‚ÄúBenchmarking in-hand manipulation,‚Äù IEEE Robotics and
Automation Letters , vol. 5, no. 2, pp. 588‚Äì595, 2020.
[25] B. Calli and A. M. Dollar, ‚ÄúVision-based model predictive control
for within-hand precision manipulation with underactuated grippers,‚Äù
in2017 IEEE International Conference on Robotics and Automation
(ICRA) . IEEE, 2017, pp. 2839‚Äì2845.[26] M. V . Liarokapis and A. M. Dollar, ‚ÄúLearning task-speciÔ¨Åc models for
dexterous, in-hand manipulation with simple, adaptive robot hands,‚Äù
in2016 IEEE/RSJ International Conference on Intelligent Robots and
Systems (IROS) . IEEE, 2016, pp. 2534‚Äì2541.
[27] N. Rojas, R. R. Ma, and A. M. Dollar, ‚ÄúThe gr2 gripper: An un-
deractuated hand for open-loop in-hand planar manipulation,‚Äù IEEE
Transactions on Robotics , vol. 32, no. 3, pp. 763‚Äì770, 2016.
[28] A. Caldas, M. Grossard, M. Makarov, and P. Rodriguez-Ayerbe, ‚ÄúTask-
level dexterous manipulation with multiÔ¨Ångered hand under modeling
uncertainties,‚Äù Journal of Dynamic Systems, Measurement, and Control ,
vol. 144, no. 3, 2022.
[29] J. Cui and J. Trinkle, ‚ÄúToward next-generation learned robot manipula-
tion,‚Äù Science robotics , vol. 6, no. 54, p. eabd9461, 2021.
[30] O. Kroemer, S. Niekum, and G. D. Konidaris, ‚ÄúA review of robot
learning for manipulation: Challenges, representations, and algorithms,‚Äù
Journal of machine learning research , vol. 22, no. 30, 2021.
[31] S. Funabashi, T. Isobe, F. Hongyi, A. Hiramoto, A. Schmitz, S. Sugano,
and T. Ogata, ‚ÄúMulti-Ô¨Ångered in-hand manipulation with various object
properties using graph convolutional networks and distributed tactile
sensors,‚Äù IEEE Robotics and Automation Letters , 2022.
[32] G. Khandate, M. Haas-Heger, and M. Ciocarlie, ‚ÄúOn the feasibility
of learning Ô¨Ånger-gaiting in-hand manipulation with intrinsic sensing,‚Äù
arXiv preprint arXiv:2109.12720 , 2021.
[33] B. Sundaralingam and T. Hermans, ‚ÄúIn-hand object-dynamics inference
using tactile Ô¨Ångertips,‚Äù IEEE Transactions on Robotics , vol. 37, no. 4,
pp. 1115‚Äì1126, 2021.
[34] H. J. Charlesworth and G. Montana, ‚ÄúSolving challenging dexterous
manipulation tasks with trajectory optimisation and reinforcement learn-
ing,‚Äù in International Conference on Machine Learning . PMLR, 2021,
pp. 1496‚Äì1506.
[35] J. Ibarz, J. Tan, C. Finn, M. Kalakrishnan, P. Pastor, and S. Levine,
‚ÄúHow to train your robot with deep reinforcement learning: lessons we
have learned,‚Äù The International Journal of Robotics Research , vol. 40,
no. 4-5, pp. 698‚Äì721, 2021.
[36] A. Gupta, J. Yu, T. Z. Zhao, V . Kumar, A. Rovinsky, K. Xu, T. De-
vlin, and S. Levine, ‚ÄúReset-free reinforcement learning via multi-task
learning: Learning dexterous manipulation behaviors without human
intervention,‚Äù in 2021 IEEE International Conference on Robotics and
Automation (ICRA) . IEEE, 2021, pp. 6664‚Äì6671.
[37] T. Chen, J. Xu, and P. Agrawal, ‚ÄúA simple method for complex in-hand
manipulation,‚Äù in 5th Annual Conference on Robot Learning , vol. 3,
2021.
[38] W. Huang, I. Mordatch, P. Abbeel, and D. Pathak, ‚ÄúGeneralization in
dexterous manipulation via geometry-aware multi-task learning,‚Äù arXiv
preprint arXiv:2111.03062 , 2021.
[39] T. Chen, J. Xu, and P. Agrawal, ‚ÄúA system for general in-hand object
re-orientation,‚Äù in Conference on Robot Learning . PMLR, 2022, pp.
297‚Äì307.
[40] M. E. Taylor and P. Stone, ‚ÄúTransfer learning for reinforcement learning
domains: A survey,‚Äù Journal of Machine Learning Research , vol. 10,
no. 7, 2009.
[41] Q. Yang, Y . Zhang, W. Dai, and S. J. Pan, Transfer learning . Cambridge
University Press, 2020.
[42] H. Han, H. Liu, C. Yang, and J. Qiao, ‚ÄúTransfer learning algorithm with
knowledge division level,‚Äù IEEE Transactions on Neural Networks and
Learning Systems , pp. 1‚Äì15, 2022, doi:10.1109/TNNLS.2022.3151646.
[43] T. Haarnoja, H. Tang, P. Abbeel, and S. Levine, ‚ÄúReinforcement learning
with deep energy-based policies,‚Äù in ICML , 2017.
[44] C. Florensa, Y . Duan, and P. Abbeel, ‚ÄúStochastic neural networks for
hierarchical reinforcement learning,‚Äù ArXiv , vol. abs/1704.03012, 2017.
[45] S. Kumar, A. Kumar, S. Levine, and C. Finn, ‚ÄúOne solution is not all
you need: Few-shot extrapolation via structured maxent rl,‚Äù ArXiv , vol.
abs/2010.14484, 2020.
[46] E. Tzeng, C. Devin, J. Hoffman, C. Finn, P. Abbeel, S. Levine,
K. Saenko, and T. Darrell, ‚ÄúAdapting deep visuomotor representations
with weak pairwise constraints,‚Äù in WAFR , 2016.
[47] B. Eysenbach, S. Asawa, S. Chaudhari, R. Salakhutdinov, and S. Levine,
‚ÄúOff-dynamics reinforcement learning: Training for transfer with domain
classiÔ¨Åers,‚Äù ArXiv , vol. abs/2006.13916, 2021.
[48] J. Chen, X. Wu, L. Duan, and S. Gao, ‚ÄúDomain adversarial reinforce-
ment learning for partial domain adaptation,‚Äù IEEE Transactions on
Neural Networks and Learning Systems , vol. 33, no. 2, pp. 539‚Äì553,
2022.
[49] J. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and P. Abbeel,
‚ÄúDomain randomization for transferring deep neural networks from
simulation to the real world,‚Äù in 2017 IEEE/RSJ international conference
on intelligent robots and systems (IROS) . IEEE, 2017, pp. 23‚Äì30.IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, 2023 12
[50] X. B. Peng, M. Andrychowicz, W. Zaremba, and P. Abbeel, ‚ÄúSim-to-real
transfer of robotic control with dynamics randomization,‚Äù in 2018 IEEE
international conference on robotics and automation (ICRA) . IEEE,
2018, pp. 3803‚Äì3810.
[51] A. A. Rusu, S. G. Colmenarejo, C ¬∏ aglar G ¬®ulc ¬∏ehre, G. Desjardins,
J. Kirkpatrick, R. Pascanu, V . Mnih, K. Kavukcuoglu, and R. Hadsell,
‚ÄúPolicy distillation,‚Äù CoRR , vol. abs/1511.06295, 2016.
[52] G. Berseth, K. Xie, P. Cernek, and M. van de Panne, ‚ÄúProgressive
reinforcement learning with distillation for multi-skilled motion control,‚Äù
ArXiv , vol. abs/1802.04765, 2018.
[53] M. Gimelfarb, S. Sanner, and C.-G. Lee, ‚ÄúContextual policy transfer in
reinforcement learning domains via deep mixtures-of-experts,‚Äù in UAI,
2021.
[54] Y . Sun, K. Zhang, and C. Sun, ‚ÄúModel-based transfer reinforcement
learning based on graphical model representations,‚Äù IEEE Transac-
tions on Neural Networks and Learning Systems , pp. 1‚Äì14, 2021,
doi:10.1109/TNNLS.2021.3107375.
[55] R. Sasso, M. Sabatelli, and M. A. Wiering, ‚ÄúMulti-source transfer
learning for deep model-based reinforcement learning,‚Äù ArXiv , vol.
abs/2205.14410, 2022.
[56] A. A. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer, J. Kirkpatrick,
K. Kavukcuoglu, R. Pascanu, and R. Hadsell, ‚ÄúProgressive neural
networks,‚Äù arXiv preprint arXiv:1606.04671 , 2016.
[57] A. A. Rusu, M. Ve Àácer¬¥ƒ±k, T. Roth ¬®orl, N. Heess, R. Pascanu, and
R. Hadsell, ‚ÄúSim-to-real robot learning from pixels with progressive
nets,‚Äù in Conference on Robot Learning . PMLR, 2017, pp. 262‚Äì270.
[58] A. Barreto, D. Borsa, J. Quan, T. Schaul, D. Silver, M. Hessel, D. J.
Mankowitz, A. Z ¬¥ƒ±dek, and R. Munos, ‚ÄúTransfer in deep reinforcement
learning using successor features and generalised policy improvement,‚Äù
inICML , 2018.
[59] M. Abdolshah, H. Le, T. G. Karimpanal, S. Gupta, S. Rana, and
S. Venkatesh, ‚ÄúA new representation of successor features for transfer
across dissimilar environments,‚Äù ArXiv , vol. abs/2107.08426, 2021.
[60] A. Allshire, M. Mittal, V . Lodaya, V . Makoviychuk, D. Makoviichuk,
F. Widmaier, M. W ¬®uthrich, S. Bauer, A. Handa, and A. Garg, ‚ÄúTransfer-
ring dexterous manipulation from gpu simulation to a remote real-world
triÔ¨Ånger,‚Äù arXiv preprint arXiv:2108.09779 , 2021.
[61] L.-J. Lin, ‚ÄúSelf-improving reactive agents based on reinforcement learn-
ing, planning and teaching,‚Äù Machine learning , vol. 8, no. 3-4, pp. 293‚Äì
321, 1992.
[62] T. Schaul, J. Quan, I. Antonoglou, and D. Silver, ‚ÄúPrioritized experience
replay,‚Äù arXiv preprint arXiv:1511.05952 , 2015.
[63] T. De Bruin, J. Kober, K. Tuyls, and R. Babuska, ‚ÄúExperience selec-
tion in deep reinforcement learning for control,‚Äù Journal of Machine
Learning Research , vol. 19, 2018.
[64] W. Fedus, P. Ramachandran, R. Agarwal, Y . Bengio, H. Larochelle,
M. Rowland, and W. Dabney, ‚ÄúRevisiting fundamentals of experience
replay,‚Äù in International Conference on Machine Learning . PMLR,
2020, pp. 3061‚Äì3071.
[65] D. Yarats, D. Brandfonbrener, H. Liu, M. Laskin, P. Abbeel, A. Lazaric,
and L. Pinto, ‚ÄúDon‚Äôt change the algorithm, change the data: Ex-
ploratory data for ofÔ¨Çine reinforcement learning,‚Äù arXiv preprint
arXiv:2201.13425 , 2022.
[66] A. Lazaric, M. Restelli, and A. Bonarini, ‚ÄúTransfer of samples in
batch reinforcement learning,‚Äù in Proceedings of the 25th international
conference on Machine learning , 2008, pp. 544‚Äì551.
[67] A. Tirinzoni, A. Sessa, M. Pirotta, and M. Restelli, ‚ÄúImportance
weighted transfer of samples in reinforcement learning,‚Äù in International
Conference on Machine Learning . PMLR, 2018, pp. 4936‚Äì4945.
[68] A. Tirinzoni, M. Salvini, and M. Restelli, ‚ÄúTransfer of samples in policy
search via multiple importance sampling,‚Äù in International Conference
on Machine Learning . PMLR, 2019, pp. 6264‚Äì6274.
[69] H. Yin and S. J. Pan, ‚ÄúKnowledge transfer for deep reinforcement
learning with hierarchical experience replay,‚Äù in Thirty-First AAAI
conference on artiÔ¨Åcial intelligence , 2017.
[70] D. Kalashnikov, J. Varley, Y . Chebotar, B. Swanson, R. Jonschkowski,
C. Finn, S. Levine, and K. Hausman, ‚ÄúMt-opt: Continuous multi-task
robotic reinforcement learning at scale,‚Äù ArXiv , vol. abs/2104.08212,
2021.
[71] M. A. Wiering and M. Van Otterlo, ‚ÄúReinforcement learning,‚Äù Adapta-
tion, learning, and optimization , vol. 12, no. 3, p. 729, 2012.
[72] J. Pan, X. Wang, Y . Cheng, and Q. Yu, ‚ÄúMultisource transfer double
dqn based on actor learning,‚Äù IEEE transactions on neural networks
and learning systems , vol. 29, no. 6, pp. 2227‚Äì2238, 2018.
[73] D. M. Wolpert and Z. Ghahramani, ‚ÄúComputational principles of move-
ment neuroscience,‚Äù Nature neuroscience , vol. 3, no. 11, pp. 1212‚Äì1217,
2000.[74] T. Flash and N. Hogan, ‚ÄúThe coordination of arm movements: an ex-
perimentally conÔ¨Årmed mathematical model,‚Äù Journal of neuroscience ,
vol. 5, no. 7, pp. 1688‚Äì1703, 1985.
[75] B. Daley and C. Amato, ‚ÄúReconciling -returns with experience replay,‚Äù
Advances in Neural Information Processing Systems , vol. 32, 2019.