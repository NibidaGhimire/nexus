NeRF-GAN Distillation for Efficient 3D-Aware Generation with Convolutions
Mohamad Shahbazi1Evangelos Ntavelis1,3Alessio Tonioni2Edo Collins2
Danda Pani Paudel1Martin Danelljan1Luc Van Gool1
1Computer Vision Lab, ETH Z ¨urich2Google Z ¨urich3ML & Robotics, CSEM, Switzerland
{mshahbazi, entavelis, paudel, martin.danelljan, vangool }@vision.ee.ethz.ch,
{alessiot, edocollins }@google.com
Abstract
Pose-conditioned convolutional generative models
struggle with high-quality 3D-consistent image generation
from single-view datasets, due to their lack of sufficient
3D priors. Recently, the integration of Neural Radiance
Fields (NeRFs) and generative models, such as Generative
Adversarial Networks (GANs), has transformed 3D-aware
generation from single-view images. NeRF-GANs exploit
the strong inductive bias of neural 3D representations and
volumetric rendering at the cost of higher computational
complexity. This study aims at revisiting pose-conditioned
2D GANs for efficient 3D-aware generation at inference
time by distilling 3D knowledge from pretrained NeRF-
GANs. We propose a simple and effective method, based on
re-using the well-disentangled latent space of a pre-trained
NeRF-GAN in a pose-conditioned convolutional network
to directly generate 3D-consistent images corresponding
to the underlying 3D representations. Experiments on
several datasets demonstrate that the proposed method
obtains results comparable with volumetric rendering in
terms of quality and 3D consistency while benefiting from
the computational advantage of convolutional networks.
The code will be available at: https://github.com/
mshahbazi72/NeRF-GAN-Distillation
1. Introduction
Generative Adversarial Networks (GANs) [15] have un-
dergone outstanding progress in photo-realistic image gen-
eration and manipulation in a variety of applications [4, 27,
40, 22, 23, 9, 45, 44]. Recently, there has been an increasing
interest in extending GANs to the task of 3D-aware genera-
tion from single-view image datasets, with the goal of pro-
viding disentangled control over the content and the view-
point of the generated images.
V olumetric Rendering (EG3D) 
Our Convolutional Rendering 
Batch Size Batch Size Figure 1. Top: views of the same subject cat generated by a volu-
metric rendering generator (EG3D) and by our convolutional gen-
erator. Bottom: comparison of the inference memory consumption
and speed (on a fixed GPU budget) for the two methods.arXiv:2303.12865v3  [cs.CV]  24 Jul 2023Image GAN models have been historically based on con-
volutional architectures, enabling efficient training and gen-
eration for 2D tasks. However, pose-conditioned convolu-
tional GANs (pcGANs) struggle with 3D-consistent image
generation, due to their lack of sufficient 3D priors [35].
Therefore, some studies have previously attempted to disen-
tangle the pose from the content in pcGANs using explicit
3D supervision [52, 28, 12], which, however, is not read-
ily available for most datasets. As a result, later methods
moved away from fully convolutional GANs by incorpo-
rating 3D inductive biases in the architecture and training
pipeline, such as 3D neural representations and differen-
tiable rendering methods [35, 36, 48, 39].
The advent of Neural Radiance Fields (NeRFs) [31] has
recently transformed the neural 3D representation and the
task of novel-view synthesis [2, 3, 43, 32, 30, 17, 56]. For
this reason, NeRFs have been successfully integrated with
GANs to achieve promising results in 3D-aware genera-
tion [46, 6, 38, 16, 50, 37, 47]. Nerf-GANs, in their general
form, map a latent space to a 3D representation of objects
and generate images from queried viewpoints using volu-
metric rendering. However, volumetric rendering is compu-
tationally demanding due to its ray-casting process, making
high-resolution generation slow and memory-expensive.
Recent works have proposed different approaches to im-
prove the computational efficiency of NeRF-GANs using
more efficient 3D representations [37, 6, 47] and training
protocols [6, 38]. Nevertheless, volumetric rendering re-
mains an integral part of these models.
In recent NeRF-GANs [37, 6, 47, 38], convolutional net-
works have been reintroduced in the generator architecture
as super-resolution networks or as 3D-representation gener-
ators, in order to scale up NeRF-GANs for high-resolution
generation. In this study, we take a different approach to
integrating NeRF-GANs and convolutional GANs for 3D-
aware generation from single-view images. In particular,
we investigate the capacity of convolutional generators to
achieve 3D-consistent rendering with explicit pose control
when learning from a pretrained NeRF-GAN without any
additional explicit 3D supervision. A convolutional gener-
ator that fairly preserves the 3D consistency, image quality,
and the correspondence between the generated images and
the underlying 3D representation can be used for efficient
multi-view inference in setups where volumetric rendering
is not affordable, such as in mobile applications. However,
balancing and minimizing the trade-off between efficiency
and 3D consistency is a highly challenging task, which we
set out to explore in this work.
We propose a simple but effective method for distilling
a pretrained NeRF-GAN into a pose-conditioned fully con-
volutional generator. The main component of our approach
is based on exploiting the well-disentangled intermediate
latent space of the NeRF-GAN in the convolutional gen-erator. In particular, our convolutional generator learns to
map each latent code from the 3D generator, along with
the target viewpoint, to the corresponding obtained images
by explicit volumetric rendering. By doing so, we aim to
distill the NeRF-GAN’s underlying 3D knowledge into the
convolutional generator, as well as to establish a correspon-
dence between the images of the generator and the 3D rep-
resentation of the NeRF-GAN. As demonstrated in Fig. 1,
our experiments on three different datasets indicate that the
convolutional generator trained with our method is capable
of achieving results comparable to volumetric rendering in
terms of image quality and 3D-consistency, while benefiting
from the superior efficiency of convolutional networks.
Our contributions are summarized as follows:
• We propose a method to distill NeRF-GANs into con-
volutional generators for efficient 3D-aware inference.
• We provide a simple and effective method to condition
the convolutional generator on the well-disentangled
intermediate latent space of the NeRF-GAN.
• Through experiments on three different datasets, we
show that the generator trained by our distillation
method well preserves the 3D consistency, image qual-
ity, and semantics of the pretrained NeRF-GAN.
2. Related Works
3D-aware Generation from Single-View Images. Prior
works have attempted to create 3D awareness in 2D GANs
using explicit 3D supervision, such as 3D models [52, 12],
pose and landmark annotations [49, 21], and synthetic
data [28]. In many applications, obtaining such 3D super-
vision is not practical. As a result, later works aimed at
unsupervised methods by introducing 3D inductive biases
in GANs, including 3D neural representations and differen-
tiable rendering [35, 39, 48, 36] These methods, although
promising, lag far behind 2D GANs in terms of image qual-
ity or struggle with high-resolution generation due to the
additional computational complexity.
NeRF-GANs. NeRFS have shown outstanding potential in
compactly representing 3D scenes for novel view synthe-
sis. GRAF [46] and Pi-GAN [6] are the first works to in-
tegrate NeRFs and GANs. While achieving highly consis-
tent 3D-aware generation, the computational restrictions of
NeRF framework make these methods impractical for high-
resolution generation or environments with constrained re-
sources. In order to extend NeRF-GANs to higher reso-
lutions, convolutional super-resolution networks were used
in later studies [37, 16, 38] at the expense of some multi-
view inconsistencies. EpiGRAF [50], in contrast, adopts
an efficient multi-scale patch training protocol, but still
requires full high-resolution sampling rendering for infer-
ence, which makes it comparatively more computationally
demanding than competitors.Other studies aim at bringing the recent advances in the
efficiency of NeRFS to NeRF-GANs. Although there ex-
ist numerous works on efficient 3D representations [33, 43,
51, 8, 55, 54] and volumetric sampling [57, 14, 18, 34, 20]
in NeRFs, only a subset of them [5, 47, 58] have been suc-
cessfully applied to NeRF-GANs. This is because they are
mainly designed for the single-scene setup, making their
adaptation to the generative setup not trivial. The use of
sparse voxel grids in V oxGRAF [47] and multi-plane image
representations in [58] result in efficient and 3D-consistent
generation while compromising the image quality and 3D
geometry. EG3D [5] proposes using tri-planes to repre-
sent the geometry of the generated objects. Exploiting tri-
planes, coupled with carefully designed techniques to en-
force 3D consistency, allows EG3D to significantly improve
both computational efficiency and image quality. Live 3D
Portrait [53] is a concurrent work based on EG3D that aims
at real-time one-shot reconstruction of faces by estimat-
ing the canonical tri-planes of a pre-trained EG3D. How-
ever, Live 3D Portrait is computationally limited by the
underlying volumetric rendering of EG3D. Most similar to
our study, SURF-GAN [29] aims to discover directions for
pose control in a pretraind 2D GAN by generating multi-
view images using a pretrained NeRF-GAN. However, us-
ing NeRF-GANs only as multi-view supervision does not
fully exploit their underlying 3D knowledge. Moreover, the
2D generator obtained using this method does not preserve
any correspondence between the NeRF-GAN’s 3D repre-
sentations and the generated images. Different from SURF-
GAN, we exploit the intermediate latent space of retrained
NeRF-GANs to distill 3D knowledge into a 2D genera-
tor and establish correspondence between the convolutional
generator and the NeRF-GAN’s 3D representations.
3. Method
In this section, we first provide a brief overview of the
formulation of NeRF-GANs and then explain the proposed
formulation in detail.
3.1. Preliminaries
The general formulation of NeRF-GANs consists of a
3D-representation generator G3D(z), which maps a latent
variable z(usually drawn from a normal distribution) to a
3D representation of an object. Then, in order to render an
image Iz,cfrom the target viewpoint (camera parameters)
c∈R25, volumetric rendering is applied to the generated
3D representation. We base our method on EG3D [5], as it
provides a strong trade-off in image quality, 3D consistency,
and efficiency, among recent NeRF-GANs.
EG3D represents 3D scenes using tri-planes, which are
three axis-aligned orthogonal feature planes, each with a
size of N×N×C, where Nis spatial resolution and Cis the
number of channels. To represent a 3D position x∈R3,xis projected onto each of the three feature planes, retrieving
the corresponding feature vector (Fxy, Fxz, Fyz)via bilin-
ear interpolation, and aggregating the three feature vectors
via summation. To obtain the color and density at position
x, a lightweight MLP decodes the feature vector obtained
for the queried position to a density and color value.
The tri-plane generator G3D(z, c)in EG3D consists of
a mapping network M3D(z, c), which maps the input la-
tent code and the target viewpoint to an intermediate la-
tent variable w, namely the style code. The style code
then is used to modulate a convolutional synthesis network
S3D(w)to generate the tri-planes I3p
z,c. In order to render
an image Iz,cfrom the target viewpoint c, hierarchical vol-
umetric rendering is applied to the tri-planes. Since volu-
metric rendering at high resolutions is computationally too
expensive, EG3D does so at a lower resolution and uses a
convolutional super-resolution network to obtain a final im-
age. More specifically, the low-resolution output of vol-
umetric rendering in EG3D consists of a 32-channel fea-
ture map If
z,c, the first three of which represent the low-
resolution RGB image ILR
z,c), which is given as input to the
super-resolution network. EG3D is trained in an adversarial
fashion with a viewpoint-conditioned dual discriminator D
that ensures the photorealism of the generated images from
the target viewpoints, as well as the consistency between
high-resolution and low-resolution images.
3.2. Convolutional Rendering of Pretrained NeRF-
GANs
The aim of the proposed method is to distill a pre-trained
NeRF-GAN G3D
z,cinto a 2D image generator G2D
z,c, such
thatG2D
z,cdirectly predicts 3D-consistent multi-view images
I′
z,c, corresponding to the volumetric renderings obtained
by the underlying 3D representation of G3D
z,c. To this end,
we propose to exploit the well-disentangled style space of
G3Dto distill the underlying 3D representation into G2D.
Sharing the style space wof the pre-trained 3D generator
with the convolutional renderer is the first step towards es-
tablishing a correspondence between the 3D representations
and the generated images. Secondly, it allows training the
convolutional generator for 3D-consistent generation with-
out the need for generating multiple views of the same ob-
jects and enforcing multi-view consistency.
The overall architecture of EG3D and our convolutional
generator is visualized in 2. The convolutional generator
is based on StyleGAN architecture [24, 25], consisting of
a mapping network, a low-resolution convolutional feature
prediction, and a convolutional super-resolution network.
The mapping network transforms the style code wofG3D,
and the target viewpoint cto the style code w′ofG2D.
Then, the low-resolution feature predictor S2Destimates
the EG3D features and low-resolution image obtained by
the volumetric rendering. Estimated features and images areVolumetric 
Renderer 
w
w’z
c
Mapping 
Network 
Mapping 
Network 
S3D
S2DSuper 
Resolution
Super 
ResolutionLLR r e c LHR r e c 
La dv 
Frozen weights 
 Trained  Discr.
Discr.Iz,c 
I’z,c 
cFigure 2. Using a student-teacher framework, we distill 3D consistency from a frozen volumetric rendering based Nerf-GAN (top) to a 2D
convolutional renderer (bottom). A loss consisting of high- and low-resolution image reconstruction and an adversarial component allows
us to retain good image quality and 3D consistency.
then mapped to the high-resolution outputs using the super-
resolution network. In our setup, the super-resolution net-
work is initialized with EG3D’s super-resolution network
and is jointly optimized with the feature predictor network.
3.3. Training
In order to train the proposed convolutional renderer,
we use a teacher-student framework, where the volumet-
ric rendering is used to supervise G2Don the viewpoint-
conditioned mapping of (z, c)toI′
z,c. A schematic repre-
sentation of our training regime is reported in Fig. 2. Specif-
ically, for each training sample, we randomly sample ziand
ciand use the pretrained NeRF-GAN to obtain: the corre-
sponding style code wi, the low-resolution image ILR
zi,ci, fea-
ture maps If
zi,cirendered by volumetric rendering, as well
as, the high-resolution image IHR
zi,cigenerated by the super-
resolution network. These together form a training sample
ifor the proposed convolutional renderer.
We provide zandctoG2Dand compute a loss function
composed of three parts. We first add a reconstruction term
LLR
recbetween the low-resolution outputs of the volumetric
and convolutional renderers. A second reconstruction loss
LHR
recis applied between the super-resolved outputs of the
two renderers. Lastly, we apply an adversarial term Ladv.
In the following, we drop the subscripts zi, cito reduce
the clutter in notation. The low-resolution reconstruction
lossLLR
recconsists of a pixel-wise smooth L1 loss between
the two feature maps, as well as a perceptual loss between
the generated and target low-resolution images,
LLR
rec=λLR
Smooth L1∗Smooth L1(I′f, If)+
λLR
perc∗PerceptualLoss (I′LR, ILR).(1)Here, λLR
Smooth L1andλLR
perc are the weights for the low-
resolution smooth L1 and perceptual loss, respectively.
Similarly, the high-resolution reconstruction loss LHR
recis
defined as:
LHR
rec=λHR
Smooth L1∗Smooth L1(I′f, If)+
λHR
perc∗PerceptualLoss (I′HR, IHR).(2)
where λHR
rec andλHR
perc are the weights for the high-
resolution smooth L1 and perceptual loss, respectively.
The adversarial term Ladvis similar to the one used in
EG3D. We use the same dual-discriminator architecture D
as in EG3D to ensure the realism of the high-resolution
images, their consistency with the low-resolution version,
and the compliance of the generated image with the queried
viewpoints. The total loss Ltotal for training G2Dis,
Ltotal =LLR
rec+LHR
rec+λadv∗Ladv, (3)
where λadvis the weight for the adversarial loss.
Two-stage training: in practice, empirical experiments
show that training the convolutional renderer using the full
objective from the beginning will lead to high-quality but
3D-inconsistent images. Therefore, we instead propose a
2-stage training curriculum. In the first stage, G2Dis only
optimized by pure distillation of the volumetric rendering
G3Dusing LLR
recandLHR
recuntil the renderer achieves rea-
sonable generation quality. Then, the adversarial loss Ladv
is added to the training to further improve the performance.
By applying this 2-stage curriculum, we are able to counter
the 3D inconsistency induced by the adversarial training.
Pose-correlated dataset bias: As shown in EG3D [5],
adversarial training of the convolutional network is prone tolearning pose-correlated dataset biases, such as more smil-
ing in non-frontal viewpoints in FFHQ dataset [26], which
in turn results in 3D attribute inconsistencies. To mitigate
such biases in FFHQ dataset, we use both real images and
the images rendered from EG3D as the real examples shown
to the discriminator. The proportion of the EG3D-rendered
images shown to Dis controlled by the hyper-parameter
α(0≤α≤1), which is set to 0.5in our experiments. As
we will discuss in section 4.6, αcan be used to control the
trade-off between image quality and 3D consistency.
4. Experiments
In this section, we first describe our experimental setup
for the evaluation of our method. Then, we compare the
proposed method with baselines in terms of visual quality,
3D consistency, and computational efficiency. Moreover,
we provide an ablation study and a discussion on the bene-
fits and trade-offs of the proposed method.
4.1. Datasets
Following EG3D [6], we evaluate our method on three
different datasets:
Flickr-Faces-HQ (FFHQ) [26] : a collection of 70k high-
quality images of real-world human faces, as well as corre-
sponding approximate camera extrinsics estimated using an
of-the-shell pose estimator.
AFHQ Cats : a sub-category of the Animal-Face-HQ
(AFHQ) [10], consisting of around 5k high-quality images
of cat faces, as well as corresponding camera extrinsics es-
timated using an of-the-shell pose estimator.
ShapeNet Cars: a category of ShapeNet [7] consisting of
synthetic images of cars rendered from different viewpoints,
as well as the corresponding camera extrinsics annotations.
4.2. Baselines
We consider EG3D [5] and The method proposed in
SURF-GAN [29] as our main baselines for this study. For a
more complete evaluation, we also compare our method to
additional relevant baselines:
EG3D [5] : the NeRF-GAN used for distilling 3D knowl-
edge in the convolutional generator. EG3D serves as the up-
per bound for the 3D consistency of the proposed method.
Pose-Conditioned StyleGAN (PC-GAN) : a standard con-
ditional 2D GAN, conditioned on the pose annotations with-
out any knowledge distillation.
SURF : Inspired by the proposed method in SURF-
GAN [29], we create a baseline called SURF, where multi-
view images of EG3D are used to discover pose-control in a
2D StyleGAN (pretrained on FFHQ) based on the proposed
method in [29]. The details of the implementation can be
found in the supplementary material1.
1As the official implementation of SURF-GAN does not include theirLiftGAN [48] : a method predating EG3D and SURF base-
lines, based on differentiable rendering for distilling 2D
GANs to train a 3D generator.
4.3. Implementation and Evaluation Details
We implement and evaluate the proposed generator us-
ing both StyleGAN2 (ST2) [24] and StyleGAN3 (ST3) [25]
architectures. For the pretrained NeRF-GAN, we use the
official models from EG3D [5] (for Shapenet Cars, we re-
train the model as the official model does not match the re-
sults reported by EG3D). We train our experiments using a
batch size of 16. The rendering resolution and the final res-
olution are (128, 512) for FFHQ and AFHQ and (64, 128)
for Shapenet Cars. Both training and inference experiments
were conducted using NVidia RTX 3090 GPUs. In all ex-
periments, we set all of the weights of reconstruction loss
terms ( λLR
smooth l1,λLR
perc,λHR
perc) to the value 1 and the weight
of the adversarial loss ( λadv) to the value 0.1.
4.4. Metrics
We evaluate our method quantitatively in terms of visual
quality, and 3D consistency.
Fr´echet Inception Distance (FID) [19] : The most com-
mon metric to assess the quality and diversity of generation.
Kernel Inception Distance (KID) [19] : An unbiased alter-
native to FID for smaller datasets.
Pose Accuracy (PA) : following previous works [5], we
measure the ability of the model in generating images of the
query poses by calculating the mean squared error (MSE)
between the query poses and the pose of the generated im-
ages, estimated using an of-the-shelf pose estimator [13].
Identity Preservation (ID) : As a metric for 3D consistency,
we measure the degree of face identity preservation between
different viewpoints with respect to the canonical pose us-
ing ArcFace [11] cosine similarity for the FFHQ setup.
3D Landmark Consistency : As another 3D consistency
metric, we measure the change in facial landmarks between
different viewpoints in FFHQ using MSE. The 3D land-
marks are estimated using an off-the-shelf estimator [13].
4.5. Quantitative Comparison
In the following, we quantitatively compare the proposed
method with the baselines described in Sec. 4.2 in terms of
inference efficiency, visual quality, and 3D consistency.
4.5.1 Efficiency
The efficiency of fully-convolutional networks compared to
the rendering-based method is well-known. To better assess
the practical benefit of the proposed method, we provide a
comparison of inference efficiency between EG3D. Fig 1
method for pose control in 2D GANs, we provide our best attempt in im-
plementing their method.Table 1. Comparison of image quality on three datasets in terms of
FID and KID metrics. *The value is borrowed from [48].
Method FFHQ AFHQ ShapeNET Cars
FID↓KID↓FID↓KID↓FID↓ KID↓
EG3D [5] 5.0 0.0018 2.9 0.0003 3.5 0.0017
PC-GAN 19.3 0.0085 4.5 0.0009 6.1 0.0018
LiftGAN [48] 29.8* - - - - -
SURF 31.1 0.0153 - - - -
Ours (ST2) 6.6 0.0019 3.8 0.0011 3.1 0.0013
Ours (ST3) 6.8 0.0023 3.2 0.0007 3.1 0.0012
visualizes an example of the inference memory consump-
tion and speed of the two methods using different batch
sizes on a fixed GPU budget (in this case, on RTX 3090
GPU with 24G of memory). As shown, EG3D is restricted
to small batch sizes (a maximum of 14) due to its costly
memory consumption, where our method can scale up to
a maximum of 96 samples per batch. As for speed, our
convolutional generator achieves better frame-per-second,
especially when using StyleGAN2 as its backbone.
4.5.2 Image Quality
To assess the trade-off brought about by our convolutional
generator, we evaluate the quality of the generated images.
Table 1 shows the FID and KID scores for our method and
the baseliness on different datasets. Compared to the PC-
GAN and SURF baselines, our method constantly achieves
higher quality. This confirms that exploiting the style space
of the pretrained NeRF-GAN contributes to the ability of
the convolutional renderer in pose-conditioned generation.
Although our method does not fully match the visual quality
of EG3D, it is still able to fairly maintain high image qual-
ity and significantly reduce the compromise in the quality
compared to the other convolutional counterparts.
4.5.3 3D Consistency
While, unlike volumetric rendering, 2D convolutions do
not guarantee 3D consistency, we show that our approach
achieves a good performance in this regard. We assess the
3D consistency of generated images on FFHQ by measur-
ing the pose accuracy, 3D landmark consistency, and face
identity preservation, as discussed in Sec. 4.4, which are
provided in Table 2. Based on the results, Our method
achieves comparable 3D consistency with EG3D, while PC-
GAN and SURF struggle. Note that the high values for
identity preservation and 3D landmark consistency in SURF
are due to the limited pose variations, and hence generating
similar images regardless of the input pose, as reflected by
the pose accuracy (and the visual examples in Fig. 3).
4.6. Ablation
Ablation on loss functions: the proposed training objective
in section 3.3 consists of different loss terms to ensure bothTable 2. Comparison of 3D consistency metrics on FFHQ.
Method Pose Acc. ↓3D Landmark ↓ID↑
EG3D [5] 0.002 0.018 0.75
PC-GAN 0.009 0.062 0.56
SURF 0.044 0.014 0.86
Ours (ST2) 0.002 0.023 0.75
Ours (ST3) 0.002 0.022 0.75
image quality and consistency with the output of volumetric
rendering. In this section, we ablate the importance of these
components, by adding them one by one to form the final
objective of Eq. 3. Table 3 shows the FID scores for the
following experiments on the loss terms on AFHQ dataset:
•LR: Only the low-resolution reconstruction loss. the
super-resolution network is frozen.
•HR: Only the high-resolution reconstruction loss.
•LR + HR Full reconstruction loss on low-resolution
and high-resolution.
•HR + Adv : reconstruction and adversarial losses on
the high-resolution images.
•Full (LR + HR + ADV) Full training objective, in-
cluding the reconstruction and adversarial terms.
As shown by the ablation study, the combination of the pro-
posed loss terms leads to the best FID scores.
Single-stage v.s. two-stage training: As mentioned in 3.3,
we find out that single-stage training by jointly optimizing
for both reconstruction and adversarial losses results in sub-
tle inconsistencies such as color shifts and geometry warps,
which can be mitigated using the proposed 2-stage training
in section 3. As the observed inconsistencies are difficult
to capture using our quantitative 3D consistency metrics,
we provide a visual comparison between the examples of
single-stage and two-stage training on AFHQ in Fig. 6. To
better visualize the inconsistencies, we provide a video vi-
sualization and comparison in the supplementary material.
Mitigating Pose-Attribute Correlation: In Table 4, we
Table 3. Ablation on different loss functions for training the con-
volutional renderer on AFHQ Cats dataset.
Method FID ↓
LR 30.55
HR 10.39
LR + HR 9.1
HR + ADV 6.58
Full (LR + HR + ADV) 3.2
Table 4. The effect of mixing real images and EG3D-rendered im-
ages as real examples for adversarial training, controlled by the
parameter α, on FFHQ dataset.
Method Ours (ST2) Ours (ST3)
FID↓3D Landmark ↓FID↓3D Landmark ↓
α= 0 5.5 0.027 5.7 0.027
α= 0.5 6.8 0.022 6.6 0.023Frontal Yaw = 30 Yaw = 20 Yaw = -30 Yaw = 20 
PC-GAN EG3D Ours (ST2) Pitch = -30 Pitch = -20 Pitch = 20 Pitch = 30 
SURF 
 Ours (ST3) Figure 3. Qualitative examples of variations in yaw and pitch for FFHQ. Compared to the pose-conditioned GAN and SURF baseline, our
proposed method nearly matches the 3D consistency and image quality of volumetric rendering (EG3D).
PC-GAN EG3D Ours (ST3) 
Frontal Yaw = 30 Yaw = 20 Yaw = -30 Yaw = -20 Pitch = -30 Pitch = -20 Pitch = 20 Pitch = 30 
Ours (ST2) 
Figure 4. Qualitative examples of variations in yaw and pitch for AFHQ cats. In line with our quantitative experiments, the pose-conditioned
convolutional baseline (PC-GAN) fails to preserve the identity of the subject under different poses. In contrast, our method exhibits similar
preservation of identity to the volume rendering approach (EG3D), despite the difference in computational resources and time.
provide an ablation on the parameter αintroduced in sec-
tion 3.3 for FFHQ dataset. As shown, including EG3D-
generated images ( α= 0.5) improves the 3D consistency at
the cost of lower generation quality.
4.7. Qualitative Comparison
In this section, We provide a visual comparison of our
method with the baselines. In Fig. 3 and 4, we providevisual examples of variations in yaw and pitch for FFHQ
and AFHQ Cats. Compared to a PC-GAN and SURF, our
proposed method closely matches the 3D consistency and
maintains the image quality of volumetric rendering. Fig. 5
additionally provides examples of Shapenet Cars gener-
ated using our method and their corresponding images from
EG3D. Similarly, our method exhibits preservation of 3D
consistency and image quality, despite the difference in re-EG3D Ours (ST3) 
 Ours (ST2) Figure 5. Qualitative examples of different camera poses in Shapenet Cars for three different car models.
1-Stage 2-Stage 
Figure 6. one-stage training causes subtle color and geometry in-
consistencies (first row). Such inconsistencies can be resolved us-
ing our proposed 2-stage training (second row).
Style Mixing Interpolation 
 Inversion 
Target 
EG3D 
Ours 
Course Fine Fine Course 
Figure 7. First row: Inversion using PTI [42]) for EG3D and
our method; second row: interpolation in the latent space of our
method; third row: style mixing in the latent space of our method.
quired computational resources. We provide more visual
results in the supplementary material.
4.8. Inversion, Interpolation, and Style Mixing
As the proposed generator follows a StyleGAN archi-
tecture, it can easily benefit from most of the editing tech-
niques common in the GANs’ literature. Fig. 7 shows exam-
ples of inversion using Pivotal Tuning Inversion (PTI) [42],
latent space interpolation, and style mixing.
4.9. Discussion: StyleGAN2 V .S. StyleGAN3
StyleGAN2 is more computationally efficient than Style-
GAN3. Based on the provided quantitative evaluations,
our method reaches comparable image quality and 3D con-
sistency with both architectures. However, StyleGAN2
is known to suffer from more texture stitching and arti-
facts [25], which we also observe in the generated images
(visualizations are provided in the supplementary material).4.10. Correspondence between Convolutional and
Volumetric Renderering
As mentioned before, exploiting the style space of the
pretrained NeRF-GAN also provides an opportunity for es-
tablishing a direct correspondence between the 3D repre-
sentation of the 3D generator and the generated images us-
ing the convolutional generator. A close comparison of im-
ages generated using the convolutional and volumetric ren-
dering in Figures 3, 4, and, 5 indicates that the convo-
lutional render is able to infer and match many attributes
of the underlying 3D representation from the shared latent
space and generate images similar in content to those of vol-
umetric rendering. However, there still remains a gap in the
full correspondence of the two rendering methods, as se-
mantic and identity changes are visible between the corre-
sponding images generated by the two methods. Investigat-
ing more explicit approaches for enforcing correspondence
could be an interesting direction for improving the convolu-
tional rendering for NeRF-GAN models.
5. Conclusion
We presented a method to distill a pretrained NeRF-
GAN into a pose-conditioned convolutional generator. The
proposed method enables considerably higher efficiency,
which is crucial if 3D neural rendering is to become ubiqui-
tous and deployed at scale. To do so, we proposed exploit-
ing the intermediate latent space of the pretrained NeRF-
GAN as a conditioning input of the convolutional genera-
tor. We additionally provided a training protocol to further
improve the visual quality and 3D consistency of the im-
ages generated using our generator. Through our experi-
ments, we showed that our method maintains good image
quality and 3D consistency, significantly better than previ-
ous existing fully-convolutional methods and approaching
that of the baseline NeRF-GAN with volumetric rendering.
Finally, while our method takes steps toward achieving full
correspondence between the two rendering methods, there
remains a gap in terms of image semantics. Improving this
aspect remains a subject for further research.References
[1] Eg3d inversion projector. https://github.com/
oneThousand1000/EG3D-projector .
[2] H. Baatz, J. Granskog, M. Papas, F. Rousselle, and J. Nov ´ak.
Nerf-tex: Neural reflectance field textures. Computer Graph-
ics Forum , 41(6):287–301, 2022.
[3] Jonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter
Hedman, Ricardo Martin-Brualla, and Pratul P. Srinivasan.
Mip-nerf: A multiscale representation for anti-aliasing neu-
ral radiance fields. 2021 IEEE/CVF International Confer-
ence on Computer Vision (ICCV) , Oct 2021.
[4] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large
scale gan training for high fidelity natural image synthesis.
arXiv preprint arXiv:1809.11096 , 2018.
[5] Eric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano,
Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas J
Guibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient
geometry-aware 3d generative adversarial networks. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 16123–16133, 2022.
[6] Eric R Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu,
and Gordon Wetzstein. pi-gan: Periodic implicit generative
adversarial networks for 3d-aware image synthesis. In Pro-
ceedings of the IEEE/CVF conference on computer vision
and pattern recognition , pages 5799–5809, 2021.
[7] Angel X Chang, Thomas Funkhouser, Leonidas Guibas,
Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese,
Manolis Savva, Shuran Song, Hao Su, et al. Shapenet:
An information-rich 3d model repository. arXiv preprint
arXiv:1512.03012 , 2015.
[8] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and
Hao Su. Tensorf: Tensorial radiance fields. In European
Conference on Computer Vision (ECCV) , 2022.
[9] Yunjey Choi, Youngjung Uh, Jaejun Yoo, and Jung-Woo Ha.
Stargan v2: Diverse image synthesis for multiple domains.
InProceedings of the IEEE Conference on Computer Vision
and Pattern Recognition , 2020.
[10] Yunjey Choi, Youngjung Uh, Jaejun Yoo, and Jung-Woo Ha.
Stargan v2: Diverse image synthesis for multiple domains.
InProceedings of the IEEE/CVF conference on computer vi-
sion and pattern recognition , pages 8188–8197, 2020.
[11] Jiankang Deng, Jia Guo, Xue Niannan, and Stefanos
Zafeiriou. Arcface: Additive angular margin loss for deep
face recognition. In CVPR , 2019.
[12] Yu Deng, Jiaolong Yang, Dong Chen, Fang Wen, and Xin
Tong. Disentangled and controllable face image generation
via 3d imitative-contrastive learning. In IEEE Computer Vi-
sion and Pattern Recognition , 2020.
[13] Yu Deng, Jiaolong Yang, Sicheng Xu, Dong Chen, Yunde
Jia, and Xin Tong. Accurate 3d face reconstruction with
weakly-supervised learning: From single image to image
set. In IEEE Computer Vision and Pattern Recognition Work-
shops , 2019.
[14] Stephan J. Garbin, Marek Kowalski, Matthew Johnson,
Jamie Shotton, and Julien Valentin. Fastnerf: High-fidelity
neural rendering at 200fps. 2021.[15] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial nets. In Advances in
Neural Information Processing Systems , pages 2672–2680,
2014.
[16] Jiatao Gu, Lingjie Liu, Peng Wang, and Christian Theobalt.
Stylenerf: A style-based 3d aware generator for high-
resolution image synthesis. In International Conference on
Learning Representations , 2022.
[17] Yuan-Chen Guo, Di Kang, Linchao Bao, Yu He, and Song-
Hai Zhang. Nerfren: Neural radiance fields with reflec-
tions. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition (CVPR) , pages 18409–
18418, June 2022.
[18] Peter Hedman, Pratul P. Srinivasan, Ben Mildenhall,
Jonathan T. Barron, and Paul Debevec. Baking neural ra-
diance fields for real-time view synthesis. ICCV , 2021.
[19] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,
Bernhard Nessler, and Sepp Hochreiter. Gans trained by a
two time-scale update rule converge to a local nash equilib-
rium. In Advances in Neural Information Processing Sys-
tems, pages 6626–6637, 2017.
[20] Tao Hu, Shu Liu, Yilun Chen, Tiancheng Shen, and Jiaya Jia.
Efficientnerf efficient neural radiance fields. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition (CVPR) , pages 12902–12911, June 2022.
[21] Yibo Hu, Xiang Wu, Bing Yu, Ran He, and Zhenan Sun.
Pose-guided photorealistic face rotation. CVPR , 2018.
[22] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A
Efros. Image-to-image translation with conditional adver-
sarial networks. CVPR , 2017.
[23] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen.
Progressive growing of GANs for improved quality, stability,
and variation. In Proceedings of the International Confer-
ence on Learning Representations (ICLR) , 2018.
[24] Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine,
Jaakko Lehtinen, and Timo Aila. Training generative ad-
versarial networks with limited data. In Advances in Neural
Information Processing Systems , 2020.
[25] Tero Karras, Miika Aittala, Samuli Laine, Erik H ¨ark¨onen,
Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Alias-free
generative adversarial networks. Advances in Neural Infor-
mation Processing Systems , 34:852–863, 2021.
[26] Tero Karras, Samuli Laine, and Timo Aila. A style-based
generator architecture for generative adversarial networks.
pages 4396–4405, 06 2019.
[27] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,
Jaakko Lehtinen, and Timo Aila. Analyzing and improving
the image quality of StyleGAN. In Proc. CVPR , 2020.
[28] Marek Kowalski, Stephan J. Garbin, Virginia Estellers,
Tadas Baltru ˇsaitis, Matthew Johnson, and Jamie Shotton.
Config: Controllable neural face image generation. In Eu-
ropean Conference on Computer Vision (ECCV) , 2020.
[29] Jeong-gi Kwak, Yuanming Li, Dongsik Yoon, Donghyeon
Kim, David Han, and Hanseok Ko. Injecting 3d perception
of controllable nerf-gan into stylegan for editable portrait im-
age synthesis. In European Conference on Computer Vision ,
pages 236–253. Springer, 2022.[30] Ricardo Martin-Brualla, Noha Radwan, Mehdi S. M. Sajjadi,
Jonathan T. Barron, Alexey Dosovitskiy, and Daniel Duck-
worth. Nerf in the wild: Neural radiance fields for uncon-
strained photo collections, 2020.
[31] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,
Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:
Representing scenes as neural radiance fields for view syn-
thesis. In ECCV , 2020.
[32] Thomas M ¨uller, Alex Evans, Christoph Schied, and Alexan-
der Keller. Instant neural graphics primitives with a multires-
olution hash encoding. arXiv preprint arXiv:2201.05989 ,
2022.
[33] Thomas M ¨uller, Alex Evans, Christoph Schied, and Alexan-
der Keller. Instant neural graphics primitives with a multires-
olution hash encoding. ACM Trans. Graph. , 41(4):102:1–
102:15, July 2022.
[34] Thomas Neff, Pascal Stadlbauer, Mathias Parger, Andreas
Kurz, Joerg H. Mueller, Chakravarty R. Alla Chaitanya, An-
ton S. Kaplanyan, and Markus Steinberger. DONeRF: To-
wards Real-Time Rendering of Compact Neural Radiance
Fields using Depth Oracle Networks. Computer Graphics
Forum , 40(4), 2021.
[35] Thu Nguyen-Phuoc, Chuan Li, Lucas Theis, Christian
Richardt, and Yong-Liang Yang. Hologan: Unsupervised
learning of 3d representations from natural images. In The
IEEE International Conference on Computer Vision (ICCV) ,
Nov 2019.
[36] Thu Nguyen-Phuoc, Christian Richardt, Long Mai, Yong-
Liang Yang, and Niloy Mitra. Blockgan: Learning 3d object-
aware scene representations from unlabelled images. In Ad-
vances in Neural Information Processing Systems 33 , Nov
2020.
[37] Michael Niemeyer and Andreas Geiger. Giraffe: Represent-
ing scenes as compositional generative neural feature fields.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 11453–11464, 2021.
[38] Roy Or-El, Xuan Luo, Mengyi Shan, Eli Shecht-
man, Jeong Joon Park, and Ira Kemelmacher-Shlizerman.
StyleSDF: High-Resolution 3D-Consistent Image and Ge-
ometry Generation. In Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition (CVPR) ,
pages 13503–13513, June 2022.
[39] Xingang Pan, Bo Dai, Ziwei Liu, Chen Change Loy, and
Ping Luo. Do 2d gans know 3d shape? unsupervised 3d
shape reconstruction from 2d image gans. In International
Conference on Learning Representations , 2021.
[40] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan
Zhu. Semantic image synthesis with spatially-adaptive nor-
malization. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR) , 2019.
[41] Elad Richardson, Yuval Alaluf, Or Patashnik, Yotam Nitzan,
Yaniv Azar, Stav Shapiro, and Daniel Cohen-Or. Encoding
in style: a stylegan encoder for image-to-image translation.
InIEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , June 2021.
[42] Daniel Roich, Ron Mokady, Amit H Bermano, and Daniel
Cohen-Or. Pivotal tuning for latent-based editing of real im-ages. ACM Transactions on Graphics (TOG) , 42(1):1–13,
2022.
[43] Sara Fridovich-Keil and Alex Yu, Matthew Tancik, Qinhong
Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels:
Radiance fields without neural networks. In IEEE Confer-
ence on Computer Vision and Pattern Recognition (CVPR) ,
2022.
[44] Axel Sauer, Kashyap Chitta, Jens Muller, and Andreas
Geiger. Projected gans converge faster. In NeurIPS , 2021.
[45] Axel Sauer, Katja Schwarz, and Andreas Geiger. Stylegan-
xl: Scaling stylegan to large diverse datasets. volume
abs/2201.00273, 2022.
[46] Katja Schwarz, Yiyi Liao, Michael Niemeyer, and Andreas
Geiger. Graf: Generative radiance fields for 3d-aware image
synthesis. In Advances in Neural Information Processing
Systems (NeurIPS) , 2020.
[47] Katja Schwarz, Axel Sauer, Michael Niemeyer, Yiyi Liao,
and Andreas Geiger. V oxgraf: Fast 3d-aware image synthesis
with sparse voxel grids. arXiv preprint arXiv:2206.07695 ,
2022.
[48] Yichun Shi, Divyansh Aggarwal, and Anil Jain. Lifting 2d
stylegan for 3d-aware face generation. pages 6254–6262, 06
2021.
[49] Alon Shoshan, Nadav Bhonker, Igor Kviatkovsky, and
G´erard Medioni. Gan-control: Explicitly controllable gans.
InProceedings of the IEEE/CVF International Conference
on Computer Vision (ICCV) , pages 14083–14093, October
2021.
[50] Ivan Skorokhodov, Sergey Tulyakov, Yiqun Wang, and Pe-
ter Wonka. Epigraf: Rethinking training of 3d gans. arXiv
preprint arXiv:2206.10535 , 2022.
[51] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct voxel
grid optimization: Super-fast convergence for radiance fields
reconstruction. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 5459–
5469, 2022.
[52] Ayush Tewari, Mohamed Elgharib, Gaurav Bharaj, Florian
Bernard, Hans-Peter Seidel, Patrick Perez, Michael Zoll-
hofer, and Christian Theobalt. Stylerig: Rigging stylegan
for 3d control over portrait images. pages 6141–6150, 06
2020.
[53] Alex Trevithick, Matthew Chan, Michael Stengel, Eric R.
Chan, Chao Liu, Zhiding Yu, Sameh Khamis, Manmohan
Chandraker, Ravi Ramamoorthi, and Koki Nagano. Real-
time radiance fields for single-image portrait view synthesis.
InACM Transactions on Graphics (SIGGRAPH) , 2023.
[54] Richard Tucker and Noah Snavely. Single-view view syn-
thesis with multiplane images. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR) , June
2020.
[55] Suttisak Wizadwongsa, Pakkapon Phongthawee, Jiraphon
Yenphraphai, and Supasorn Suwajanakorn. Nex: Real-time
view synthesis with neural basis expansion. In IEEE Confer-
ence on Computer Vision and Pattern Recognition (CVPR) ,
2021.
[56] Liwen Wu, Jae Yong Lee, Anand Bhattad, Yuxiong Wang,
and David Forsyth. Diver: Real-time and accurate neural ra-diance fields with deterministic integration for volume ren-
dering, 2021.
[57] Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and
Angjoo Kanazawa. PlenOctrees for real-time rendering of
neural radiance fields. In ICCV , 2021.
[58] Xiaoming Zhao, Fangchang Ma, David G ¨uera, Zhile Ren,
Alexander G. Schwing, and Alex Colburn. Generative mul-
tiplane images: Making a 2d gan 3d-aware. In Proc. ECCV ,
2022.Supplementary Material
In what follows, we first provide an evaluation of the cor-
respondence between the generated images using volumet-
ric rendering and our convolutional generator in section A.
Moreover, in section B, we extend the efficiency analysis
of our method to the setup of the Shapenet Cars dataset. In
section C, we discuss some of the implementation details.
Then, we offer a discussion on the limitations of the pro-
posed method in Sec. D. Finally, we provide more visual
examples for our method in Sec. E.
A. Evaluation of Correspondence
To provide a baseline for future research, we measure
the correspondence between generated images using our
method and the corresponding images rendered from EG3D
using Peak Signal to Noise Ratio (PSNR), the result of
which is provided in Table 5.
B. Analysis of Efficiency on Shapenet Cars
In Fig. 8, we provide a comparison of the computational
efficiency of inference using our method and the baseline
NeRF-GAN (EG3D) on Shapenet Cars setup. As men-
tioned in section 4.3 of the main paper, EG3D performs vol-
umetric rendering at the resolution of 64 and super-resolves
the output to the resolution of 128. As it can be seen in
Fig. 8, Our method with StyleGAN2 as its backbone is more
efficient than EG3D in terms of both memory consumption
and speed. However, the StyleGAN3 backbone is only more
efficient in terms of memory consumption, but it is slower
than the baseline. The reason for this is that StyleGAN3
is more computationally demanding than the tri-plane gen-
erator in EG3D, which is based on StyleGAN2. In the
setup of Shapenet Cars, the computational complexity of the
StyleGAN3 convolutional generator outweighs that of the
lower-resolution volumetric rendering in EG3D, resulting
in a slower inference (Note that the memory consumption
is still lower for the StyleGAN3 convolutional generator).
Table 5. PSNR values ( ↑) for measuring the correspondence be-
tween the images of EG3D and our convolutional generator on 3
datasets. The values are calculated for 1k images. ST2 and ST3
correspond to the two architectures StyleGAN2 and StyleGAN3
used as the backbone of our method
Dataset ST2 ST3
FFHQ 28.73 28.65
AFHQ Cats 28.46 28.10
Shapenet Cars 36.60 36.57
Figure 8. Comparison of the inference memory consumption and
speed (on a fixed GPU budget) for our method and EG3D on
Shapenet Cars.
C. Implementation Details
C.1. SURF Baseline
In the main paper, we provided a baseline, which we re-
ferred to as “SURF”, inspired by the method proposed in the
SURF-GAN paper [29] for pose/content disentanglement
of a pre-trained 2D StyleGAN. As the code and the pre-
trained models for such disentanglement are not made pub-
licly available, we provided our best effort in implementing
the SURF baseline inspired by their approach.
The proposed method in [29] consists of two main com-
ponents: 1. a NeRF-GAN model called SURF-GAN for
portrait images 2. a training recipe to add pose conditioning
to a pre-trained StyleGAN2 using multi-view images gen-
erated from the 3D NeRF-GAN. Specifically, the part rel-
evant as a baseline for our proposed method is the second
component, which we tried to re-implement it for compari-
son with our method. To do so, for a fair comparison with
our method, we use EG3D pre-trained on FFHQ for gener-
ating multi-view image triplets from 3 different viewpoints
(source, canonical, and target views). Following [29], we
use pSp [41], a pretrained inversion encoder to invert the
generated multi-view images in the style space of the 2D
generator. Using the multi-view images and their corre-sponding style codes obtained using pSp, we train two map-
ping networks to:
1. map any arbitrary style code to the style code of the
canonical viewpoint, and
2. map the canonical style code to the style code of the
target viewpoint.
For the mapping networks, we use MLP networks with
the same architecture as StyleGAN2’s mapping network.
For the training objective, we include reconstruction losses
both on the images (MSE and Perceptual) and latent codes
(MSE), as used in [29]. This architecture and training
regime is inspired by the one in [29], but not exactly the
same. We have adapted the method to the 3D generator
network we are using (EG3D). Moreover, in the original
method, the mapping from the canonical latent code to the
target one is formulated as the linear combination of a set
of learnable pose vectors. In our implementation, we use a
more general mapping by using an MLP network instead.
Nevertheless, the implemented baseline provides a compar-
ison with a proven alternative strategy to enable pose con-
ditioning of a StyleGAN-like convolutional architecture.
C.2. Inversion
In Sec. 4.8 of the main paper, we provided examples of
inversion in our generator using Pivotal Tuning Inversion
(PTI) [42]. Following EG3D, inversion is performed given
a target image and its corresponding camera parameters. As
for the implementation, we use the adaptation to EG3D pro-
vided at [1]. We use 500 steps for optimizing the latent
code and 350 additional steps for fine-tuning the generator
according to the PTI method.
C.3. 3D Consistency Metrics
To calculate the 3D Landmark Consistency and Identity
Preservation (ID) in section 4.5.3 of the main paper, we vary
the yaw from -40 to +40 and the pitch from -30 to +30, fol-
lowing the evaluation setup of [29]. As an additional visu-
alization, Fig. 9 shows the comparison of Identitiy Preser-
vation for each angle individually.
D. Limitations
In our approach, the outputs of the volumetric render-
ing branch are used to supervise our convolutional renderer.
Thus, the visual quality and 3D consistency of our approach
is largely bound by the quality and consistency of the pre-
trained NeRF-GAN. However, our formulation is largely
agnostic to the volumetric generator used. Therefore, im-
provements in volumetric rendering in the context of GANs
will also transfer to the generated quality of our approach.
As an example, a close look at the videos provided in the
supplementary video reveals small changes in the degree
Figure 9. Comparison of identity preservation across different
camera poses. Our method performs much better than the pose-
conditioned GAN baseline (PC-GAN), approaching the consis-
tency of volumetric rendering (EG3D). The higher consistency
achieved by SURF is due to its limited pose variations.
of the smile in different viewpoints of some of the gener-
ated faces. Such a phenomenon, which is due to the dataset
bias, also exists in EG3D, as analyzed in Sec 1.1 of the sup-
plementary material of EG3D paper [5] and visible in our
results as well.
E. Visual Results
In Fig. 10, we provide more visual examples generated
using our method from FFHQ dataset. Fig. 11 shows ex-
amples of generated images from AFHQ Cats dataset using
the proposed convolutional generator. Finally, in Fig. 12,
we provide random samples from Shapenet Cars dataset
generated using the volumetric rendering (EG3D) and our
method. For a video comparison, please refer to the supple-
mentary video “supp.mp4”.Frontal Yaw = 30 Yaw = 20 Yaw = -30 Yaw = -20 Pitch = -30 Pitch = -20 Pitch = 20 Pitch = 30 EG3D Ours (ST2) Ours (ST3) EG3D Ours (ST2) Ours (ST3) EG3D Ours (ST2) Ours (ST3) EG3D Ours (ST2) Ours (ST3) 
Figure 10. Visual examples of pose control in our convolutional generator and their comparison to those of EG3D on FFHQ dataset.Frontal Yaw = 30 Yaw = 20 Yaw = -30 Yaw = -20 Pitch = -30 Pitch = -20 Pitch = 20 Pitch = 30 EG3D Ours (ST2) 
 Ours (ST3) EG3D Ours (ST2) Ours (ST3) EG3D Ours (ST2) Ours (ST3) EG3D Ours (ST2) Ours (ST3) Figure 11. Visual examples of pose control in our convolutional generator and their comparison to those of EG3D on AFHQ Cats dataset.EG3D Ours (ST2) Ours (ST3) 
 EG3D Ours (ST2) Ours (ST3) EG3D Ours (ST2) Ours (ST3) EG3D Ours (ST2) Ours (ST3) Figure 12. Visual examples of pose control in our convolutional generator and their comparison to those of EG3D on Shapenet Cars dataset.