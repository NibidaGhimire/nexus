SELF-SUPERVISED LEARNING OF DENSE HIERARCHICAL REPRESENTATIONS FOR
MEDICAL IMAGE SEGMENTATION
Eytan Kats⋆Jochen G. Hirsch†Mattias P . Heinrich⋆
⋆Institute of Medical Informatics, University of L ¨ubeck, Germany
†Fraunhofer Institute for Digital Medicine MEVIS, Bremen, Germany
ABSTRACT
This paper demonstrates a self-supervised framework for
learning voxel-wise coarse-to-fine representations tailored
for dense downstream tasks. Our approach stems from the
observation that existing methods for hierarchical represen-
tation learning tend to prioritize global features over local
features due to inherent architectural bias. To address this
challenge, we devise a training strategy that balances the con-
tributions of features from multiple scales, ensuring that the
learned representations capture both coarse and fine-grained
details. Our strategy incorporates 3-fold improvements: (1)
local data augmentations, (2) a hierarchically balanced archi-
tecture, and (3) a hybrid contrastive-restorative loss function.
We evaluate our method on CT and MRI data and demon-
strate that our new approach particularly beneficial for fine-
tuning with limited annotated data and consistently outper-
forms the baseline counterpart in linear evaluation settings.
Our code and pre-trained models will be publicly available
athttps://github.com/multimodallearning/
hierarchical-dense-ssl .
Index Terms —Self-supervised learning, voxel-wise em-
beddings, segmentation
1. INTRODUCTION
Annotation of medical images is a time-consuming and com-
plex task that requires expert knowledge. This hinders the de-
velopment of supervised algorithms. In contrast to supervised
methods, self-supervised approaches utilize unlabeled data to
extract robust representations that can be fine-tuned for vari-
ous downstream tasks using only a small amount of labeled
data. This makes self-supervised representation learning an
important area of research in the medical imaging domain.
Contrastive and restorative approaches are two promi-
nent techniques in self-supervised representation learning. In
the restorative approach, the model learns latent features by
reconstructing the original image from its augmented ver-
sion [1], [2], [3]. In the contrastive approach, the model
acquires robust representations by minimizing the distance
between encodings of related data points (positive pairs),
Fig. 1 . Fine-tuning performance for varying amounts of data.
The benefit of pre-training are particularly evident when only
limited labeled data is available.
while pushing apart encodings of unrelated data points (neg-
ative pairs) [4], [5], [6]. A data point can be represented by
an entire image [4], an image patch [5], or a single voxel
[6]. Several methods demonstrate the effectiveness of com-
bining contrastive and restorative approaches into a unified
self-supervised framework [7], [8], [9].
Both contrastive and restorative methods can be used to
extract voxel-wise representations [3], [6], which is particu-
larly beneficial for dense downstream tasks such as segmen-
tation. In this context, each voxel is represented by a unique
feature vector that ideally encodes all levels of information
for the specific location, from fine-grained details to global
context. This requires to generate feature maps in full reso-
lution with sufficient number of channels. However, due to
the memory limitations, learning dense representations that
effectively capture such comprehensive information poses a
challenge.
Recent studies, such as SAM [10] and vox2vec [11],
demonstrate that voxels can be effectively represented as a
combination of corresponding feature vectors from different
scales of a Feature Pyramid Network (FPN). SAM focuses
on the landmark detection task and represents voxel usingarXiv:2401.06473v2  [cs.CV]  26 May 2024only two separate feature vectors from the highest-resolution
and lowest-resolution scales. vox2vec encodes voxel as a
concatenation of features from all pyramid scales and demon-
strates the advantage of such representation for downstream
segmentation tasks. However, the concatenated vector is
characterized by a substantial imbalance between coarse and
fine-grained features. The feature vector extracted from the
highest-resolution scale is significantly smaller than the fea-
ture vector extracted from the lowest-resolution scale. The
imbalance could result in the self-supervised training process
prioritizing the optimization of low-resolution features over
the high-resolution features. This may have a negative impact
on the downstream segmentation task, where both capturing
fine-grained details and understanding the global context are
crucial for the accurate delineation.
In this paper we propose a self-supervised framework for
learning hierarchical balanced coarse-to-fine representations.
We adopt the vox2vec [11] approach as a baseline and intro-
duce 3-fold improvements designed to strengthen the high-
resolution component of the representation vector (Sec.2): (1)
local augmentations, (2) a hierarchically balanced architec-
ture, and (3) a hybrid contrastive-restorative loss function.
Our major contributions are as following: (1) We propose
a self-supervised framework for learning dense hierarchical
representations that effectively mitigates the intrinsic bias of
the FPN-based approach towards prioritizing global context
features over the fine-grained features. (2) We demonstrate
that our strategy is particularly beneficial for downstream seg-
mentation tasks with limited annotated data, and show that
our pre-trained model significantly outperforms the baseline
counterpart in linear evaluation setting. (3) We publicly re-
lease the pre-trained models for both CT and MRI modalities.
2. METHOD
The main objective of the proposed approach is to learn
voxel-wise representations that are beneficial for downstream
segmentation tasks. Inspired by recent advancements in
coarse-to-fine modeling of dense representations [10], [11],
we introduce a novel self-supervised training strategy (Fig.2)
that effectively addresses the inherent imbalance in FPN-
based hierarchical embeddings. Local region augmentations
guide the training process to learn robust fine-grained features
(Sec.2.1). The chosen architectural configuration balances
the impact of features from different scales on the contrastive
loss (Sec.2.2). The reconstruction component of the hybrid
contrastive-restorative loss function further emphasizes the
significance of high-resolution features in the training process
(Sec.2.3).
2.1. Local augmentations
The effectiveness of contrastive learning is significantly de-
pendent on the selection of suitable augmentations [12]. Toencourage the model to focus on fine-grained features, we in-
corporate local pixel shuffling and local region in-painting
augmentations within a hierarchical contrastive learning
framework.
First, two overlapping patches, XAandXB, are randomly
cropped from a 3D image X. Then, each patch is trans-
formed with a unique augmentation sequence t∼ T drawn
from the augmentation set T:Xt
A=t(XA),Xt
B=t(XB).
Apart from local pixel shuffling and local region in-painting,
the augmentation set Talso includes non-linear intensity aug-
mentations.
2.2. Balanced coarse-to-fine voxel-wise representations
Given two overlapping patches Xt
AandXt
B, the positive pair
vA∈Xt
A, vB∈Xt
Bis defined as a pair of voxels that corre-
spond to the same location in the original image X. Other
pairs of voxels sampled from the same patch or from different
patches are considered negative pairs. During each training
step we randomly sample npositive pairs from the overlap-
ping area of patches Xt
AandXt
B. Each sampled voxel from
the patch Xt
Aforms one positive pair with the corresponding
voxel from the patch Xt
Band2·(n−1)negative pairs with
other voxels sampled from Xt
AandXt
B.
FPN generates two feature pyramids that correspond to
the cropped patches: FAandFB(Fig.2). To mitigate the
imbalance in voxel representation, the feature maps from dif-
ferent scales of each pyramid are projected to have the same
number of channels resulting in balanced feature pyramids JA
andJB. Feature vectors are sampled from the corresponding
locations of all scales in JAandJBand concatenated to cre-
ate balanced voxel representations denoted as ji
A,ji
B, where
i= 1, .., n .
Comparing to the vox2vec baseline [11] we also reduced
the depth of the feature pyramid and doubled the number of
feature maps in the remaining feature pyramid levels to fur-
ther amplify the contribution of local features.
The proposed architecture ensures that high-resolution
features have an equal contribution to the voxel representa-
tion as low-resolution features. This strategy facilitates an
unbiased optimization process of both fine-grained details
and global semantics, leading to enhanced performance in
downstream segmentation tasks (Sec.3.3).
2.3. Hybrid loss function
On one hand, the contrastive loss excels in learning semanti-
cally meaningful representations when applied to embeddings
extracted from different levels of the feature pyramid [10],
[11]. On the other hand, the restorative approach is known
to be particularly useful in capturing fine-grained details that
proved to be beneficial for downstream segmentation tasks
[3]. Therefore, we propose to include a restorative component
in the hierarchical contrastive learning framework to amplify
the emphasis on local, fine-grained features.Fig. 2 . Pre-training pipeline. Two overlapping patches augmented and processed by the FPN. The feature maps from the dif-
ferent scale levels projected to the same channel size. Feature vectors sampled from the projected scale levels in corresponding
positions to form hierarchically balanced voxel representation vectors. V oxel representations projected to the latent space where
the InfoNCE loss is calculated.
As suggested in [4], we first project ji
A,ji
Bto 128-length
vectors hi
A,hi
Busing 3-layer fully-connected network and
normalize the result: zi
A=hi
A/||hi
A||,hi
A/||hi
A||,i= 1...n.
Then we apply InfoNCE loss [4] as the contrastive objective:
Lc=−X
ilogd(zi
A, zi
B)
d(zi
A, zi
B) +P
j̸=iP
l∈A,Bd(zi
k, zj
l),(1)
where d(zi
A, zi
B) = exp 
⟨zi
A, zi
B⟩/τ
,⟨·,·⟩is the inner prod-
uct operation and τis the temperature parameter. In our ex-
periments τ= 0.1.
The feature maps of the high-resolution pyramid level
processed by the reconstruction head comprising two convo-
lution layers to produce reconstructed image dXA,dXB. We
apply MSE loss as the reconstruction objective:
Lr= ((XA−dXA)2+ (XB−dXB)2)/2. (2)
The total loss can be formulated as:
L=Lc+λLr, (3)
where parameter λweights between contrastive and recon-
struction objectives. In our experiments λ= 10 .
3. EXPERIMENTS
To assess the effectiveness of our approach, we conduct eval-
uations on both MRI and CT modalities. First, we pre-train
the models on a large collection of images. Next, we use the
pre-trained weights to evaluate the strength of the learned rep-
resentations on downstream segmentation tasks under linear
and fine-tuning settings.3.1. Datasets
For the MRI pre-training, we employ 1087 image volumes
covering the abdomen region from the NAKO dataset [13].
Subsequently, for MRI downstream segmentation task we uti-
lize 60 images with annotations for 12 abdominal organs from
the publicly available AMOS MRI dataset [14]. The pre-
processing steps for MRI images include (1) interpolation to
the voxel spacing 1.5×1.5×1.5mm3, (2) clipping intensity
values to 0.01 and 99.9 percentile, and afterwards scaling to
the range between 0 and 1, (3) cropping to the minimal vol-
ume containing the voxels with the intensity greater than 0.3.
For the CT pre-training we use 2500 publicly available
image volumes from the abdomen domain: 500 images from
the AMOS CT dataset [14] and 2000 images from the FLARE
dataset [15]. The BTCV dataset [16] is utilised for the down-
stream task. It consists of 30 CT images with 13 annotated
abdominal organs. Preprocessing for the CT data comprises
of the following steps: (1) interpolation to the voxel spacing
1×1×2mm3, (2) cropping to the minimal volume contain-
ing all the voxels with the intensity greater than -500 HU, (3)
clipping intensity values to -1350 HU and 1000 HU, and af-
terwards scaling them to the range between 0 and 1.
3.2. Implementation details
We adopt vox2vec [11] as our baseline method, as it show the
state-of-the-art results for the hierarchical contrastive learning
and outperforms other methods on the downstream segmen-
tation task. This suggests its suitability as a strong baseline to
demonstrate an effectiveness of ours approach. Consequently,we adopt most of the the training parameters from vox2vec.
The models are pre-trained for 50K steps using the Adam
optimizer [17] with a learning rate of 3·10−4. Each train-
ing batch includes 10 pairs of overlapping patches, with 1K
voxel locations sampled per pair. Training is conducted on
a single A100-80GB GPU, and the process takes an average
of 35 hours. For downstream tasks we train models for 45K
steps with batch size 7 employing the Adam optimizer with a
learning rate of 3·10−4.
In the fine-tuning configuration the non-linear head is at-
tached to the FPN backbone. The backbone is initially frozen
for 15K steps, after which the learning rate is exponentially
increased for the backbone parameters from 3·10−5to3·10−4
during 1,200 steps.
In the linear evaluation setup we attach the linear head
to the FPN backbone. Each pyramid level is separately fed
to a dedicated convolutional layer with kernel size 1 which
maps the number of channels to the number of classes. Sub-
sequently, all the outputs are up-sampled to the full resolution
and summed up.
3.3. Results
For the experiments with the AMOS MRI dataset we employ
a 5-fold cross validation, and a 3 fold cross-validation for the
experiments with the BTCV CT dataset. Performance of all
models is evaluated using the Dice score. For all experiments
we report mean results of cross-validation for the abdominal
organs: spleen Sp, kidneys Kid, gallbladder Gb, esophagus
Es, liver Li, stomach St, aorta Aor, inferior vena cava IV C ,
pancreas Paand adrenal glands AG. We also report the mean
and standard deviation of the overall Dice score across folds.
aug LcLrarch Sp Kid Gb Es Li St Aor IVC Pa AG Overall√76.9 79.3 33.2 35.1 87.8 51.1 73.0 62.0 55.3 36.4 58.8±3.5√ √78.9 82.7 38.9 40.8 89.1 50.9 77.9 64.6 59.1 39.7 62.1±2.4√ √57.3 62.3 30.7 25.6 80.0 33.6 59.2 44.9 36.1 20.5 44.4±4.3√ √ √79.7 82.7 40.0 41.2 90.2 56.6 78.4 63.0 62.8 39.0 62.9±2.6√ √ √ √83.2 84.9 44.9 47.4 90.3 56.1 80.6 69.2 62.0 42.5 65.7±2.6
Table 1 . Ablation study of the proposed method in linear
evaluation settings with MRI data.
First, to assess the impact of individual components on
the final performance of our method we conduct ablation ex-
periments, employing linear evaluation settings and MRI data
(Table 1). We start from the baseline vox2vec [11] model that
contains only contrastive loss function Lcand evaluate differ-
ent combinations of the proposed modifications: local region
augmentations aug, restorative loss function Lrand architec-
tural changes arch . Remarkably, addition of each of these
individual components consistently lead to performance im-
provement. The results confirm the importance of each mod-
ification in mitigating the inherent imbalance of hierarchical
contrastive learning.
Next we evaluate our method in linear evaluation settings
for CT data (Table 2). Our method outperforms the state-of-
the-art baseline approach [11] for models pre-trained on equalamounts of data. Moreover, our model surpasses a model ini-
tialized with the publicly released vox2vec weights, which
was trained on 2.5 times more data.
method data Sp Kid Gb Es Li St Aor IVC PVC Pa AG Overall
vox2vec [11] 6550 80.9 81.0 30.6 58.3 90.4 62.3 83.3 74.0 52.8 48.1 47.2 64.4±1.3
vox2vec [11] 2500 79.3 79.0 24.2 53.4 88.6 57.9 82.6 72.7 50.5 46.8 46.0 62.0±1.0
ours 2500 81.4 83.1 32.5 58.4 89.5 59.5 83.6 75.6 54.8 54.5 48.4 65.6±0.8
Table 2 . Results of linear evaluation on CT data. We em-
ployed 2500 images to pre-train model with our method and
vox2vec method, and compared their performance with pub-
licly available vox2vec model which was pre-trained using
6550 images.
Finally, we conduct a comparison between model fine-
tuning and training from scratch with varying amounts of
annotated data (Table 3). We observe that fine-tuning con-
sistently outperforms training from scratch, while especially
beneficial in low-data scenarios. The result highlights the
strength of learned hierarchical representations, which can be
effectively adapted to specific tasks even with limited training
data. This property is particularly important in the medical
domain where annotated data remains scarce.
method Sp Kid Gb Es Li St Aor IVC Pa AG Overall
42 training images
from scratch 89.3 90.7 59.6 61.0 93.7 81.1 89.5 84.4 76.9 54.4 77.1±5.4
fine-tuning 90.8 91.7 67.0 62.3 94.5 81.9 89.6 84.9 79.1 56.2 78.8±4.3
8 training images
from scratch 81.3 80.6 34.5 47.0 90.7 65.7 80.8 69.3 57.5 35.6 63.3±6.7
fine-tuning 86.8 85.6 50.8 46.0 93.0 66.9 84.6 76.7 67.8 44.3 69.3±4.7
4 training images
from scratch 66.2 67.7 18.7 35.0 85.1 40.7 72.1 54.9 39.2 16.6 48.4±10.1
fine-tuning 81.2 78.6 20.8 41.0 90.9 52.7 78.3 63.6 51.1 27.5 57.7±7.8
Table 3 . Comparison of fine-tuning with training from
scratch on MRI data of various sizes: 42, 8, and 4 training
images.
4. CONCLUSIONS
We propose a self-supervised framework for learning hierar-
chically balanced voxel-wise representations from unlabeled
data. Our approach effectively mitigates the inherent im-
balance of FPN-based embeddings, ensuring that both high-
resolution and low-resolution features contribute equally to
the learned voxel representations. We demonstrate that our
method outperforms the baseline in linear evaluation set-
tings and demonstrate that it is particularly advantageous in
fine-tuning settings with limited labeled data.
5. COMPLIANCE WITH ETHICAL STANDARDS
The German National Cohort (NAKO) study is performed
with the approval of the relevant ethics committees, and is
in accordance with national law and with the Declaration of
Helsinki of 1975 (in the current, revised version).6. ACKNOWLEDGMENTS
We gratefully acknowledge the financial support by German
Research Foundation: DFG, HE 7364/10-1, project number
500498869.
7. REFERENCES
[1] K. He, X. Chen, S. Xie, Y . Li, P. Doll’ar, and R.B.
Girshick, “Masked autoencoders are scalable vision
learners,” 2022 IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR) , pp. 15979–
15988, 2021.
[2] Z. Chen, D. Agarwal, K. Aggarwal, W. Safta, M.M.
Balan, V . S. Sethuraman, and K. Brown, “Masked image
modeling advances 3d medical image analysis,” 2023
IEEE/CVF Winter Conference on Applications of Com-
puter Vision (WACV) , pp. 1969–1979, 2022.
[3] Z. Zhou, V . Sodha, M.M.R. Siddiquee, R. Feng,
N. Tajbakhsh, M.B. Gotway, and J. Liang, “Models gen-
esis: Generic autodidactic models for 3d medical im-
age analysis,” in International Conference on Medical
Image Computing and Computer-Assisted Intervention ,
2019.
[4] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, “A
simple framework for contrastive learning of visual rep-
resentations,” in International conference on machine
learning , 2020.
[5] Y . Huang, L. Lin, P. Cheng, J. Lyu, and X. Tang,
“Lesion-based contrastive learning for diabetic retinopa-
thy grading from fundus images,” in International Con-
ference on Medical Image Computing and Computer-
Assisted Intervention , 2021.
[6] X. Wang, R. Zhang, C. Shen, T. Kong, and L. Li,
“Dense contrastive learning for self-supervised visual
pre-training,” 2021 IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR) , pp. 3023–3032,
2020.
[7] Y . Jiang, M. Sun, H. Guo, X. Bai, K. Yan, L. Lu, and
M. Xu, “Anatomical invariance modeling and seman-
tic alignment for self-supervised learning in 3d medical
image analysis,” in Proceedings of the IEEE/CVF In-
ternational Conference on Computer Vision , 2023, pp.
15859–15869.
[8] Y . Tang, D. Yang, W. Li, H.R. Roth, B.A. Landman,
D. Xu, V . Nath, and A. Hatamizadeh, “Self-supervised
pre-training of swin transformers for 3d medical image
analysis,” 2022 IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR) , pp. 20698–
20708, 2021.[9] F. Haghighi, M.R.H. Taher, M.B. Gotway, and J. Liang,
“Dira: Discriminative, restorative, and adversarial learn-
ing for self-supervised medical image analysis,” 2022
IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pp. 20792–20802, 2022.
[10] K. Yan, J. Cai, D. Jin, S. Miao, A.P. Harrison, D. Guo,
Y . Tang, J. Xiao, J. Lu, and L. Lu, “Sam: Self-
supervised learning of pixel-wise anatomical embed-
dings in radiological images,” IEEE Transactions on
Medical Imaging , vol. 41, pp. 2658–2669, 2020.
[11] M. Goncharov, V . Soboleva, A. Kurmukov, M. Pisov,
and M. Belyaev, “vox2vec: A framework for self-
supervised contrastive learning of voxel-level represen-
tations in medical images,” in International Conference
on Medical Image Computing and Computer-Assisted
Intervention , 2023.
[12] J. Zhang and K. Ma, “Rethinking the augmentation
module in contrastive learning: Learning hierarchical
augmentation invariance with expanded views,” 2022
IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pp. 16629–16638, 2022.
[13] F. Bamberg, H.U. Kauczor, S. Weckbach, C.L. Schlett,
M. Forsting, K.H Greiser S.C. Ladd, M.A. Weber,
J. Schulz-Menger, T. Niendorf, et al., “Whole-body
magnetic resonance imaging in the german national co-
hort (nako): Design & current status,” The European
Journal of Public Health , vol. 32, 2022.
[14] Y . Ji, H. Bai, C. Ge, J. Yang, Y . Zhu, R. Zhang, Z. Li,
L. Zhanng, W. Ma, X. Wan, et al., “Amos: A large-scale
abdominal multi-organ benchmark for versatile medical
image segmentation,” Advances in Neural Information
Processing Systems , vol. 35, pp. 36722–36732, 2022.
[15] J. Ma, Y . Zhang, S. Gu, X. An, Z. Wang, C. Ge,
C. Wang, F. Zhang, Y . Wang, Y . Xu, et al., “Fast and
low-gpu-memory abdomen ct organ segmentation: the
flare challenge,” Medical Image Analysis , vol. 82, pp.
102616, 2022.
[16] B. Landman, Z. Xu, J. Igelsias, M. Styner, T. Langerak,
and A. Klein, “Miccai multi-atlas labeling beyond the
cranial vault–workshop and challenge,” in MICCAI
Multi-Atlas Labeling Beyond Cranial Vault—Workshop
Challenge , 2015.
[17] D.P. Kingma and J. Ba, “Adam: A method for stochastic
optimization,” CoRR , 2014.