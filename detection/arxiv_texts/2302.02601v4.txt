Learning Representations of Bi-level Knowledge Graphs for
Reasoning beyond Link Prediction*
Chanyoung Chung, Joyce Jiyoung Whang†
School of Computing, KAIST
{chanyoung.chung, jjwhang }@kaist.ac.kr
Abstract
Knowledge graphs represent known facts using triplets.
While existing knowledge graph embedding methods only
consider the connections between entities, we propose
considering the relationships between triplets. For exam-
ple, let us consider two triplets T1andT2where T1is
(Academy Awards, Nominates, Avatar) and T2is (Avatar,
Wins, Academy Awards). Given these two base-level triplets,
we see that T1is a prerequisite for T2. In this paper, we de-
fine a higher-level triplet to represent a relationship between
triplets, e.g., ⟨T1, PrerequisiteFor, T2⟩where Prerequisite-
For is a higher-level relation. We define a bi-level knowl-
edge graph that consists of the base-level and the higher-level
triplets. We also propose a data augmentation strategy based
on the random walks on the bi-level knowledge graph to aug-
ment plausible triplets. Our model called BiVE learns embed-
dings by taking into account the structures of the base-level
and the higher-level triplets, with additional consideration of
the augmented triplets. We propose two new tasks: triplet pre-
diction and conditional link prediction. Given a triplet T1and
a higher-level relation, the triplet prediction predicts a triplet
that is likely to be connected to T1by the higher-level rela-
tion, e.g., ⟨T1, PrerequisiteFor, ? ⟩. The conditional link pre-
diction predicts a missing entity in a triplet conditioned on an-
other triplet, e.g., ⟨T1, PrerequisiteFor, (Avatar, Wins, ?) ⟩. Ex-
perimental results show that BiVE significantly outperforms
all other methods in the two new tasks and the typical base-
level link prediction in real-world bi-level knowledge graphs.
Introduction
A knowledge graph represents the relationships between enti-
ties using triplets consisting of a head entity, a relation, and a
tail entity. Knowledge graph embedding aims to represent the
entities and relations as a set of embedding vectors that can
be utilized in many modern AI applications [17, 20]. Most ex-
isting knowledge graph embedding methods generate the em-
bedding vectors by focusing solely on how the entities are con-
nected by the relations [4, 6, 36]. Even though some methods
predict missing connections between the entities by rule min-
ing [27, 31] or rule-and-path-based learning [29], these exist-
*Published in the Proceedings of the 37th AAAI Conference on Artificial
Intelligence (AAAI 2023).
†Corresponding author.
Vice
PresidentJoe
BidenDavid 
BeckhamEngland 
National 
TeamGeorge
Clooney
EnglandBarack
ObamaPresidentGeorge
W. BushColumbia
UniversityOceans
Eleven ActorSucceededBy
WorksFo r PrerequisiteForImpliesProfessionGeorge W. Bush,
HoldsPosition,
President
Barack Obama,
HoldsPosition,
President
Joe Biden,
HoldsPosition,
Vice PresidentGeorge Clooney,
ActsIn,
Oceans Eleven
George Clooney,
IsA,
Actor
David Beckham,
PlaysFor,
England National TeamDavid Beckham,
HasNationality,
EnglandFigure 1: Example of a bi-level knowledge graph consisting of
base-level and higher-level triplets in the FBHE dataset. The
relation labels are omitted in the base-level triplets.
ing approaches only enable expanding the entity-level connec-
tions.
Each triplet in a knowledge graph can have a relationship
with another triplet. For example, let us consider two base-
level triplets T1andT2where T1is (Joe Biden, HoldsPosi-
tion, Vice President) and T2is (Barack Obama, HoldsPosi-
tion, President). To represent the fact that Joe Biden was a
vice president when Barack Obama was a president, we define
a higher-level triplet ⟨T1, WorksFor, T2⟩where WorksFor is a
higher-level relation. In this paper, we define a bi-level knowl-
edge graph that includes both the base-level and the higher-
level triplets, where the base-level triplets correspond to the
original triplets representing the relationships between enti-
ties, while the higher-level triplets represent the relationships
between the base-level triplets using the higher-level relations.
Based on well-known knowledge graphs, FB15K237 [34] and
DB15K [11], we create three real-world bi-level knowledge
graphs named FBH ,FBHE , and DBHE . Figure 1 shows a sub-
graph of a bi-level knowledge graph in FBHE where the base-
level triplets correspond to the original triplets in FB15K237
and the higher-level triplets are manually created by defining
1arXiv:2302.02601v4  [cs.LG]  23 Oct 2023the higher-level relationships between the base-level triplets.
We propose incorporating the base-level and the higher-
level triplets into knowledge graph embedding. Using the bi-
level knowledge graphs, we also propose a data augmentation
strategy that augments triplets by identifying plausible relation
sequences based on random walks. We develop a new knowl-
edge graph embedding method called BiVE (embedding of Bi-
leVel knowledg Egraphs) that computes embedding vectors
by reflecting the structures of the base-level and the higher-
level triplets simultaneously, where the augmented triplets are
further incorporated. Using the bi-level knowledge graphs, we
propose two new tasks: triplet prediction and conditional link
prediction. The triplet prediction predicts a triplet that is likely
to be connected to a given triplet using a higher-level relation,
e.g.,⟨T1, WorksFor, ? ⟩, whereas the conditional link predic-
tion predicts a missing entity in a triplet where another triplet
is provided as a condition, e.g., ⟨T1, WorksFor, (?, HoldsPo-
sition, President) ⟩. Experimental results show that BiVE sig-
nificantly outperforms other state-of-the-art knowledge graph
completion methods in real-world datasets.1
Our contributions can be summarized as follows:
• To the best of our knowledge, our work is the first work
that introduces the higher-level relationships between
triplets in knowledge graphs; we define bi-level knowl-
edge graphs and create three real-world datasets.
• We propose an efficient data augmentation strategy using
random walks on a bi-level knowledge graph.
• We develop BiVE to learn embeddings by effectively
incorporating the base-level triplets, the higher-level
triplets, and the augmented triplets.
• We propose two new tasks, triplet prediction and condi-
tional link prediction, which have never been studied.
• BiVE significantly outperforms 12 different state-of-the-
art knowledge graph completion methods.
Related Work
Some knowledge graph completion methods use multi-hop
paths between distant entities [7, 18, 24, 25, 29] and rule-
based or logic-based methods identify frequently observed
patterns [8, 13, 27, 28, 31, 39]. The main difference between
these methods and BiVE is that the existing methods only con-
sider the relationships between entities, whereas BiVE consid-
ers not only the relationships between entities but also the rela-
tionships between triplets. Also, the way of expressing the re-
lationships between entities or triplets in BiVE is not restricted
to the first-order-logic-like expression. For example, the rule-
based methods consider the relationships between connected
entities, e.g., ∀x, y, z : (x, r 1, y)∧(y, r 2, z)⇒(x, r 3, z)
where there should exist a path connecting x,y, and zin the
knowledge graph. On the other hand, BiVE represents rela-
tionships like (x, r 1, y)br=⇒(p, r 2, q)where x,yandp,qare
not necessarily connected by the base-level triplets, and also
1https://github.com/bdi-lab/BiVEr1,r2, andbr=⇒can be any relation not restricted to the first-
order-logic-like relation.
Even though there have been many attempts to discover
meaningful patterns in a knowledge graph and utilize them to
complete missing links [21], such attempts have rarely been
studied in the context of data augmentation. Recently, rule-
based data augmentation for knowledge graph embedding has
been proposed [23]2. While this method uses logical rules us-
ing the base-level triplets, our data augmentation employs ran-
dom walks on a bi-level knowledge graph.
To exploit enriched information about triplets, some knowl-
edge graph embedding methods utilize attributes of entities or
ontological concepts [15]. TransEA [37] considers numeric at-
tributes of entities, and LiteralE [19] considers information
from literals. HINGE [30] has been proposed to represent
hyper-relational facts where a triplet has additional key-value
pairs to present extra information about each triplet. Even
though these methods consider enriched information about
triplets, they do not consider the relationships between triplets.
In information retrieval, a neural fact contextualization
method has been proposed to rank a set of candidate facts for
a given triplet [35]. Also, a way of representing a triplet in an
embedding space is studied by considering the concept of a
line graph [10]. Recently, ATOMIC [32] has been proposed
to provide commonsense knowledge for if-then reasoning,
whereas ASER [41] has been proposed to construct an even-
tuality knowledge graph. Although these methods consider
triplet-level operations, the goal of their methods is different
from ours and none of these considers the bi-level knowledge
graphs.
Bi-Level Knowledge Graphs
Let us represent a knowledge graph as G= (V,R,E)whereV
is a set of entities, Ris a set of relations, and E={(h, r, t ) :
h∈ V, r∈ R, t∈ V} is a set of triplets. Let us call Ga
base-level knowledge graph and call (h, r, t )∈ Ea base-level
triplet. We formally define the higher-level triplets as follows.
Definition 1 (Higher-Level Triplets) Given a base-level
knowledge graph G= (V,R,E), a set of higher-level triplets
is defined by H={⟨Ti,br, Tj⟩:Ti∈ E,br∈bR, Tj∈ E}
where Eis a set of base-level triplets and bRis a set of
higher-level relations connecting the base-level triplets.
We define a bi-level knowledge graph as follows.
Definition 2 (Bi-Level Knowledge Graph) Given a base-
level knowledge graph G= (V,R,E), a set of higher-level
relations bR, and a set of higher-level triplets H, a bi-level
knowledge graph is defined as bG= (V,R,E,bR,H).
2We could not include this method as a baseline in our experiments be-
cause the authors of [23] could not provide their source codes due to some
confidentiality restrictions.
2br ⟨Ti,br, Tj⟩
FBHEPrerequisiteForTi: (BAFTA Award, Nominates, The King’s Speech)
Tj: (The King’s Speech, Wins, BAFTA Award)
ImpliesProfessionTi: (Liam Neeson, ActsIn, Love Actually)
Tj: (Liam Neeson, IsA, Actor)
WorksForTi: (Joe Biden, HoldsPosition, Vice President)
Tj: (Barack Obama, HoldsPosition, President)
SucceededByTi: (George W.Bush, HoldsPosition, President)
Tj: (Barack Obama, HoldsPosition, President)
DBHEImpliesTimeZoneTi: (Czech Republic, TimeZone, Central European)
Tj: (Prague, TimeZone, Central European)
NextAlmaMaterTi: (Gerald Ford, StudiesIn, University ofMichigan)
Tj: (Gerald Ford, StudiesIn, Yale University)
Table 1: Examples of Higher-Level Relations and Triplets.
To define a bi-level knowledge graph, we add the higher-
level triplets Hto the base-level knowledge graph Gby intro-
ducing the higher-level relations bR. We create real-world bi-
level knowledge graphs FBH andFBHE based on FB15K237
from Freebase [2] and DBHE based on DB15K from DB-
pedia [1]. Table 1 shows some examples of the higher-level
relations and triplets. FBH contains the higher-level rela-
tions that can be inferred inside the base-level knowledge
graph, e.g., PrerequisiteFor and ImpliesProfession, whereas
FBHE andDBHE contain some externally-sourced knowl-
edge, e.g., WorksFor and NextAlmaMater. For example,
we crawl Wikipedia articles to find information about the
(vice)presidents of the United States and the alma mater in-
formation of politicians. As a result, FBH contains six dif-
ferent higher-level relations, FBHE has ten higher-level rela-
tions, and DBHE has eight higher-level relations. Note that
the base-level knowledge graphs of FBH andFBHE are
FB15K237 .FBHE extends FBH by including the externally-
sourced higher-level relationships. The authors of this pa-
per manually defined the higher-level relations and added the
higher-level triplets to FB15K237 andDB15K , which took six
weeks.
Using a bi-level knowledge graph, we define the triplet pre-
diction problem as follows.
Definition 3 (Triplet Prediction) Given a bi-level knowledge
graphbG= (V,R,E,bR,H)where H={⟨Ti,br, Tj⟩:Ti∈
E,br∈bR, Tj∈ E} , the triplet prediction problem is defined as
⟨Ti,br,?⟩or⟨?,br, Tj⟩where the goal is to predict the missing
base-level triplet.
Also, we define the conditional link prediction as follows.
Definition 4 (Conditional Link Prediction) Given a bi-level
knowledge graph bG= (V,R,E,bR,H)where H=
{⟨Ti,br, Tj⟩:Ti∈ E,br∈bR, Tj∈ E} , letTi:= (hi, ri, ti)and
Tj:= (hj, rj, tj). The conditional link prediction problem is
to predict a missing entity in a base-level triplet conditioned on
another base-level triplet. Specifically, the problem is defined
as⟨Ti,br,(hj, rj,?)⟩or⟨Ti,br,(?, rj, tj)⟩or⟨(hi, ri,?),br, Tj⟩
or⟨(?, ri, ti),br, Tj⟩.Data Augmentation by Random Walks on a
Bi-Level Knowledge Graph
Consider a bi-level knowledge graph in the training set
bGtrain= (V,R,Etrain,bR,Htrain)where EtrainandHtrainare the
base-level and the higher-level triplets in the training set re-
spectively. We add reverse relations to Rand add reversed
triplets to Etrain, i.e., for every r∈ R, we add r−1that has the
reverse direction of rand for every (h, r, t )∈ E train, we add
(t, r−1, h)toEtrain. Similarly, for every br∈bR, we add br−1
and add the reversed higher-level triplets to Htrain. All these
reverse relations and reversed triplets are added only for data
augmentation.
From an entity h, we randomly visit one of its neighbors
by following a base-level or a higher-level triplet. To search
for diverse patterns, we do not allow going back to an en-
tity that has already been visited. Let us define a random
walk path to be the sequence of visited entities, visited rela-
tions, and visited higher-level relations. Consider two base-
level triplets Ti= (hi, ri, ti)andTj= (hj, rj, tj)and a
higher-level triplet ⟨Ti,br, Tj⟩. From any entity in Ti, we can
go to any entity in Tjand vice versa by following ri,br, andrj
or their reverse relations. For example, one possible random
walk path is (hi, ri,br, rj, tj). Another possible random walk
path is (tj, r−1
j,br−1, ri, ti). Assume that we have a base-level
triplet T0= (h0, r0, hi). Starting from h0, we can make a
longer path, e.g., (h0, r0, hi, ri,br, rj, tj). We define the length
of a random walk path to be the number of entities in the se-
quence except the starting entity.
Given the maximum length of a random walk path L, we
repeat the random walks by varying the length l= 2,···, L
and repeat the random walks ntimes for every l. In our ex-
periments, we set L=3 and n=50,000,000. Let wdenote the
sequence of a random walk path of all possible lengths, where
we randomly select a starting entity for every w. If there are
multiple identical random walk paths, we remove the dupli-
cates to prevent unexpected bias. Let pkbe the k-th unique
sequence of relations and higher-level relations extracted from
w, i.e., we make pkby removing all entities from w, e.g., if
w= (h0, r0, hi, ri,br, rj, tj)thenpk= (r0, ri,br, rj). We call
pkthe relation sequence. Since pkonly traces the relations,
different random walk paths can be mapped into the same pk.
Using pk, we rewrite a random walk path w= (h,···, t)to
w= (h, pk, t)where the relation sequence of the original path
wis mapped into pk,his the starting entity and tis the last
entity. Let Wdenote the multiset of all random walk paths of
all possible lengths. We define the confidence score of (pk, r)
as
c(pk, r):=|{(h, r, t ) : (h, pk, t)∈ W,(h, r, t )∈ E train}|
|{(h, pk, t) : (h, pk, t)∈ W}|.
We select the pairs of (pk, r)that satisfies c(pk, r)≥τ
where we set τ= 0.7. LetSkr:={(h, r, t ) : (h, pk, t)∈
W, c(pk, r)≥τ,(h, r, t )/∈ E train}where Skrindicates a set
of missing triplets (h, r, t )even though c(pk, r)≥τ. Then, let
3Base -Level Triplet Higher -Level TripletBreakfast 
on Pluto
Liam 
Neeson
Love 
ActuallyMusician
Songwriter
Hugh 
GrantUnited 
KingdomActorBryan
Ferry
HasNationality ActsInImplies
ProfessionActsInIsAImplies
ProfessionImplies
Profession
IsA
IsAIsAIsA
SpecializationOfActsIn
Augmented Triplet Relation SequenceActsIn-1Figure 2: Random walk path in a bi-level knowledge graph and
an augmented triplet in FBH . We add missing triplets whose
confidence scores are greater than a certain threshold.
S:=∪k∪rSkrwhere Sis a set of augmented triplets. We
add the triplets in Sto a bi-level knowledge graph to augment
triplets that are likely to be present. Figure 2 shows an exam-
ple of a random walk path of length 2and an augmented triplet
inFBH , where the walk starts from Bryan Ferry. Let p1=
(ActsIn, ActsIn−1, ImpliesProfession, IsA). Since the confi-
dence score of ( p1, IsA) is 0.99, we add a triplet (Bryan Ferry,
IsA, Actor) which was missing in the original training set.
Embedding of Bi-Level Knowledge Graphs
A knowledge graph embedding method defines a scoring func-
tionf(h,r,t)of a triplet (h, r, t ), where h,r, and tare em-
bedding vectors of h,r, and trespectively; a higher score in-
dicates a more plausible triplet. In BiVE, the loss incurred by
the base-level triplets, Lbase, is defined as follows:
Lbase:=X
(h,r,t )∈Etraing(−f(h,r,t)) +X
(h′,r′,t′)∈E′
traing(f(h′,r′,t′))
where g(x) =log(1 + exp(x))andE′
trainis a set of corrupted
triplets. We can use any knowledge graph embedding scor-
ing function for f(·). We implement BiVE with two differ-
ent scoring functions for f(·): QuatE [42] for BiVE-Q and
BiQUE [12] for BiVE-B.
Given Ti= (hi, ri, ti), letTidenote an embedding vector
ofTiwhere the dimension is ˆd. We define Ti:=W[hi;ri;ti]
where hi,ri, and tidenote the embedding vectors of hi,
ri, and tirespectively, the dimension of each of these em-
bedding vectors is d, andWis a projection matrix of size
ˆd×3dwhich projects the vertically concatenated vector to the
ˆd-dimensional space. Similarly, Tj=W[hj;rj;tj]where
Tj= (hj, rj, tj). We define the loss incurred by the higher-
level triplets, Lhigh, as follows:
Lhigh:=X
⟨Ti,br,Tj⟩g(−f(Ti,br,Tj)) +X
⟨Ti′,br′,Tj′⟩g(f(T′
i,br′,T′
j))
where ⟨Ti,br, Tj⟩ ∈ H train,⟨Ti′,br′, Tj′⟩ ∈ H′
train,bris the em-
bedding vector of br, the dimension of brisˆd, and⟨Ti′,br′, Tj′⟩
is a corrupted higher-level triplet made by randomly replacing
TiorTjwith one of the triplets in Etrain.
We define the loss of the augmented triplets, Laug, as|V| |R| |E| | bR| |H| | bE|
FBH 14,541 237 310,116 6 27,062 33,157
FBHE 14,541 237 310,116 10 34,941 33,719
DBHE 12,440 87 68,296 8 6,717 8,206
Table 2: Statistics of a bi-level knowledge graph bG=
(V,R,E,bR,H).|bE|is the number of base-level triplets which
are involved in the higher-level triplets.
Laug:=X
(h,r,t )∈Sg(−f(h,r,t)) +X
(h′,r′,t′)∈S′g(f(h′,r′,t′))
whereSis the set of the augmented triplets and S′is the set of
corrupted triplets.
Finally, our loss function of BiVE is defined by
LBiVE:=Lbase+λ1·Lhigh+λ2·Laug
where λ1is a hyperparameter indicating the importance of
the higher-level triplets and λ2indicates the importance of the
augmented triplets. By optimizing LBiVE, BiVE learns embed-
dings by considering the structures of the base-level triplets,
the higher-level triplets, and the augmented triplets.
Let us describe the scoring functions of BiVE for triplet
prediction and conditional link prediction. To solve a triplet
prediction problem, ⟨Ti,br,?⟩, we compute Ftp(X):=
f(Ti,br,X)for every base-level triplet X∈ E trainwhere
Xis a learned embedding vector of X. To solve a condi-
tional link prediction problem, ⟨Ti,br,(hj, rj,?)⟩, we compute
Fclp(x):=f(hj,rj,x)+λ1·f(Ti,br,W[hj;rj;x])for every
x∈ V where xis a learned embedding of x.
Experimental Results3
We use three real-world bi-level knowledge graphs presented
in Table 2, where |bE|is the number of base-level triplets in-
volved in the higher-level triplets. We split EandHinto train-
ing, validation, and test sets with a ratio of 8:1:1. We use three
standard evaluation metrics: the filtered MR (Mean Rank),
MRR (Mean Reciprocal Rank), and Hit@ 10[36]. Higher
MRR and Hit@ 10and a lower MR indicate better results. We
repeat experiments ten times for each method and report the
mean and the standard deviation. We set d= 200 andˆd=
200. We use 12 different baseline methods: ASER [41], MIN-
ERV A [7], Multi-Hop [24], Neural-LP [39], DRUM [31], Any-
BURL [27], PTransE [25], RPJE [29], TransD [16], ANAL-
OGY [26], QuatE [42] and BiQUE [12]. For TransD and
ANALOGY, we use the implementations in OpenKE [14].
More details of datasets and methods are described in Ap-
pendix.
Triplet Prediction
While BiVE solves a triplet prediction problem using the
scoring function Ftp(X), none of the baseline methods can
3Results with a new implementation of BiVE are provided in Appendix
Additional Experimental Results. Although the original implementation is
also correct, the new implementation improves the performance of BiVE.
4FBH FBHE DBHE
MR (↓) MRR ( ↑) Hit@10 ( ↑) MR ( ↓) MRR ( ↑) Hit@10 ( ↑) MR ( ↓) MRR ( ↑) Hit@10 ( ↑)
ASER 74541.7 ±0.0 0.011±0.000 0.015±0.000 57916.0 ±0.0 0.050±0.000 0.070±0.000 18157.6 ±0.0 0.042±0.000 0.075±0.000
MINERV A 109055.1 ±98.5 0.093±0.002 0.113±0.002 85571.5 ±768.3 0.220±0.008 0.300±0.005 20764.3 ±72.3 0.177±0.005 0.221±0.004
Multi-Hop 108731.7 ±43.2 0.105±0.001 0.117±0.000 83643.8 ±33.2 0.255±0.012 0.311±0.003 20505.8 ±9.3 0.191±0.001 0.230±0.002
Neural-LP 115016.6 ±0.0 0.070±0.000 0.073±0.000 90000.4 ±0.0 0.238±0.000 0.274±0.000 21130.5 ±0.0 0.170±0.000 0.209±0.000
DRUM 115016.6 ±0.0 0.069±0.001 0.073±0.000 90000.3 ±0.0 0.261±0.000 0.274±0.000 21130.5 ±0.0 0.166±0.001 0.209±0.000
AnyBURL 108079.6 ±0.0 0.096±0.000 0.108±0.000 83136.8 ±5.3 0.191±0.001 0.252±0.001 20530.8 ±0.0 0.177±0.000 0.214±0.000
PTransE 111024.3 ±855.0 0.069±0.000 0.071±0.000 86793.2 ±961.0 0.249±0.001 0.274±0.000 18888.7 ±457.3 0.158±0.001 0.195±0.002
RPJE 113082.0 ±945.2 0.070±0.000 0.072±0.000 89173.1 ±797.3 0.267±0.000 0.274±0.000 20290.4 ±417.2 0.166±0.001 0.206±0.002
TransD 74277.3 ±2907.8 0.052±0.001 0.104±0.002 52159.4 ±758.9 0.238±0.002 0.280±0.003 16698.1 ±370.2 0.116±0.004 0.189±0.009
ANALOGY 93383.4 ±20576.5 0.072±0.004 0.107±0.002 60161.5 ±3295.5 0.286±0.004 0.318±0.001 18880.0 ±1213.8 0.150±0.005 0.211±0.005
QuatE 145603.8 ±1114.4 0.103±0.001 0.114±0.001 94684.4 ±1781.7 0.101±0.009 0.209±0.011 26485.0 ±491.8 0.157±0.003 0.179±0.002
BiQUE 81687.5 ±603.2 0.104±0.000 0.115±0.000 61015.2 ±399.8 0.135±0.002 0.205±0.007 19079.4 ±389.7 0.163±0.002 0.185±0.002
BiVE-Q 18.7±1.2 0.748±0.007 0.853±0.004 33.1±17.4 0.531±0.106 0.683±0.114 56.6±10.2 0.315±0.024 0.523±0.034
BiVE-B 19.7 ±1.9 0.731±0.010 0.837±0.006 27.9±2.4 0.555±0.007 0.718±0.007 4.7±0.2 0.644±0.004 0.914±0.005
Table 3: Results of Triplet Prediction. The best scores are boldfaced and the second best scores are underlined. Our models,
BiVE-Q and BiVE-B, significantly outperform all other baseline methods in terms of all metrics on all datasets.
(Person A,ActsIn, Film B)
(Person A, IsA, Actor )
Figure 3: Embedding vectors of base-level triplets in ⟨Ti, Im-
pliesProfession, Tj⟩where Tiis (Person A, ActsIn, Film B)
andTjis (Person A, IsA, Actor) in FBH . Both TiandTjem-
bedding vectors from BiVE are well-clustered.
deal with the higher-level triplets. To feed the higher-level
triplets to the baseline methods, we create a new knowledge
graph GTwhere a base-level triplet is converted into an en-
tity and a higher-level triplet is converted into a triplet. Let
Ti= (hi, ri, ti)∈ E traindenote a base-level triplet. We define
GT:= (Etrain,bR,Htrain), where each Tiis considered as an
entity. If we train the baseline methods using GT, the triplet
prediction task can be considered as a link prediction prob-
lem on GT. However, in this case, it is not guaranteed that all
Tiinvolved in the triplets in Htestappear in Htrainbecause we
randomly split Hinto training, validation, and test sets. There-
fore, for the baseline methods, the problem becomes an induc-
tive setting instead of a transductive setting. Indeed, among the
baseline methods, Neural-LP and DRUM are inductive meth-
ods and we include these methods because they can conduct
inductive inference. We assume that the candidates of a triplet
prediction problem should be included in the training set of
the base-level knowledge graph, which aligns with a realistic
setting. By taking into account both the base-level knowledge
graph and the higher-level triplets simultaneously, the problembecomes a transductive setting for BiVE. This shows that sim-
ply converting the higher-level triplets into GTcannot replace
our model.
Table 3 shows the results of triplet prediction. We see that
BiVE-Q and BiVE-B significantly outperform all other state-
of-the-art baseline methods in terms of all the three metrics
on all three real-world datasets. Note that the number of can-
didates of a triplet prediction problem is equal to the num-
ber of base-level triplets in Etrain. Therefore, achieving the MR
of 18.7 on FBH , for example, is surprising because we have
248,095 candidates in Etrain. We visualize the embedding vec-
tors generated by BiVE-Q on FBH in Figure 3. We take all
higher-level triplets in the form of ⟨Ti, ImpliesProfession, Tj⟩
and visualize the embedding vectors of TiandTjusing Prin-
cipal Component Analysis. In Figure 3, we only highlight the
base-level triplets TiandTjwhere Tiis (Person A, ActsIn,
Film B) and Tjis (Person A, IsA, Actor). We see that both Ti
andTjembedding vectors are well-clustered, meaning that
BiVE generates embeddings by appropriately reflecting the
structure of the higher-level triplets.
Conditional Link Prediction
To solve a conditional link prediction problem, BiVE uses
the scoring function Fclp(x). On the other hand, the base-
line methods cannot directly solve the conditional link pre-
diction problem. To allow the baseline methods to solve
⟨Ti,br,(hj, rj,?)⟩4, we define a scoring function of the
baseline methods as follows: F(x):=f(hj,rj,x) +
f(Ti,br, z(hj, rj, x))for all x∈ V where the former is com-
puted on the original base-level knowledge graph, the latter is
computed on GT,z(hj, rj, x)returns an embedding vector of
(hj, rj, x), and f(·)is the scoring function of each baseline
method. We cannot get f(Ti,br, z(hj, rj, x))if(hj, rj, x)/∈
Etrain. In that case, we compute the score using the randomly
initialized vectors in PTransE, RPJE, TransD, ANALOGY,
4We consider all four problems by changing the position of ?.
5FBH FBHE DBHE
MR (↓) MRR ( ↑) Hit@10 ( ↑) MR ( ↓) MRR ( ↑) Hit@10 ( ↑) MR ( ↓) MRR ( ↑) Hit@10 ( ↑)
ASER 1183.9 ±0.0 0.251±0.000 0.316±0.000 970.7±0.0 0.289±0.000 0.382±0.000 1893.5 ±0.0 0.225±0.000 0.348±0.000
MINERV A 3817.8 ±58.9 0.328±0.013 0.415±0.009 3018.5 ±45.8 0.407±0.013 0.492±0.014 2934.1 ±32.2 0.362±0.007 0.433±0.014
Multi-Hop 1878.2 ±12.0 0.421±0.003 0.578±0.003 1447.3 ±11.9 0.443±0.002 0.615±0.002 1012.3 ±28.5 0.442±0.007 0.652±0.008
Neural-LP 185.9 ±1.3 0.433±0.002 0.648±0.004 146.2±1.0 0.466±0.002 0.716±0.007 32.2±1.9 0.517±0.006 0.756±0.004
DRUM 262.7 ±13.3 0.394±0.002 0.555±0.003 207.6±10.0 0.413±0.010 0.620±0.018 49.0±3.9 0.470±0.010 0.732±0.012
AnyBURL 228.5 ±11.8 0.380±0.004 0.563±0.013 166.0±7.9 0.418±0.002 0.607±0.008 81.7±4.0 0.403±0.002 0.594±0.004
PTransE 214.8 ±0.7 0.440±0.001 0.686±0.002 167.0±1.8 0.516±0.001 0.752±0.001 19.3±0.2 0.505±0.004 0.780±0.001
RPJE 212.5 ±0.1 0.440±0.001 0.686±0.001 159.0±0.0 0.528±0.001 0.753±0.001 19.3±0.1 0.504±0.004 0.779±0.002
TransD 190.1 ±18.0 0.300±0.003 0.496±0.005 165.6±8.0 0.363±0.003 0.529±0.006 35.5±1.0 0.436±0.006 0.708±0.005
ANALOGY 341.0 ±218.7 0.182±0.065 0.291±0.125 113.3±2.0 0.409±0.004 0.581±0.004 279.1±197.1 0.140±0.089 0.253±0.166
QuatE 163.7 ±3.6 0.346±0.006 0.494±0.011 1546.4 ±98.0 0.124±0.022 0.189±0.014 551.6±40.5 0.208±0.013 0.309±0.023
BiQUE 111.0 ±0.9 0.423±0.002 0.641±0.002 90.1±0.5 0.387±0.009 0.617±0.011 29.5±1.2 0.378±0.007 0.677±0.004
BiVE-Q 7.0 ±0.3 0.752±0.005 0.906±0.002 11.0±0.3 0.698±0.004 0.839±0.003 12.5±1.0 0.606±0.009 0.828±0.010
BiVE-B 6.6±0.3 0.762±0.007 0.911±0.002 12.8±0.4 0.696±0.005 0.834±0.002 3.2±0.1 0.801±0.003 0.958±0.002
Table 4: Results of Conditional Link Prediction. The best scores are boldfaced and the second best scores are underlined. Our
models, BiVE-Q and BiVE-B, significantly outperform all baseline methods in terms of all metrics on all datasets.
Problem Prediction by BiVE-Q
⟨(?, HasAFriendshipWith, Kelly Preston), EquivalentTo, (Kelly Preston, HasAFriendshipWith, George Clooney) ⟩ George Clooney
⟨(?, HasAFriendshipWith, Kelly Preston), EquivalentTo, (Kelly Preston, HasAFriendshipWith, Tom Cruise)⟩ Tom Cruise
⟨(Joe Jonas, IsA, ?), ImpliesProfession, (Joe Jonas, IsA, Actor) ⟩ V oice Actor
⟨(Joe Jonas, IsA, ?), ImpliesProfession, (Joe Jonas, IsA, Musician) ⟩ Singer-songwriter
⟨(Bucknell University, HasAHeadquarterIn, Pennsylvania), ImpliesLocation, ( ?, Contains, Bucknell University) ⟩ Pennsylvania
⟨(Bucknell University, HasAHeadquarterIn, United States), ImpliesLocation, ( ?, Contains, Bucknell University) ⟩ United States
⟨(Saturn Award forBest Director, Nominates, Avatar), PrerequisiteFor, (Avatar, Wins, ?)⟩ Saturn Award forBest Director
⟨(Academy Award forBest Visual Effects, Nominates, Avatar), PrerequisiteFor, (Avatar, Wins, ?)⟩ Academy Award forBest Visual Effects
Table 5: Examples of Conditional Link Prediction on FBHE . BiVE correctly predicts the answers for all the above problems.
FBHE DBHE
MR (↓) Hit@10 ( ↑) MR ( ↓) Hit@10 ( ↑)
ASER 1489.3 ±0.0 0.323 ±0.000 2218.8 ±0.0 0.197 ±0.000
MINERV A 3828.4 ±56.9 0.339 ±0.003 3530.7 ±50.1 0.297 ±0.006
Multi-Hop 2284.0 ±9.5 0.500 ±0.001 2489.4 ±15.3 0.404 ±0.004
Neural-LP 1942.5 ±0.5 0.486 ±0.001 2904.8 ±0.6 0.357 ±0.001
DRUM 1945.6 ±0.8 0.490 ±0.002 2904.7 ±0.7 0.359 ±0.001
AnyBURL 342.0 ±4.6 0.526 ±0.002 879.1 ±5.7 0.364 ±0.003
PTransE 2077.6 ±10.3 0.333 ±0.000 3346.0 ±20.0 0.277 ±0.002
RPJE 1754.6 ±7.5 0.368 ±0.001 2991.7 ±28.1 0.341 ±0.000
TransD 166.3 ±1.3 0.527 ±0.001 429.0 ±7.5 0.423 ±0.001
ANALOGY 227.3 ±8.3 0.486 ±0.002 621.5 ±20.9 0.323 ±0.008
QuatE 139.0 ±1.6 0.581 ±0.001 409.6 ±8.5 0.440 ±0.001
BiQUE 134.9 ±0.9 0.583 ±0.001 376.6±3.5 0.446 ±0.002
BiVE-Q 125.2 ±0.9 0.584±0.001 405.4±4.1 0.438 ±0.002
BiVE-B 123.5±1.0 0.586 ±0.001 377.3±6.7 0.444±0.001
Table 6: Results of Base-Level Link Prediction.
QuatE, and BiQUE, whereas we set f(Ti,br, z(hj, rj, x)) = 0
for the other baseline methods by considering the mechanisms
of how each of the baseline methods assigns scores. In Ta-
ble 4, BiVE-Q and BiVE-B significantly outperform all other
baseline methods in conditional link prediction on all three
real-world datasets. In Table 5, we show some example prob-
lems of conditional link prediction in FBHE and the predic-
tions made by BiVE-Q where it correctly predicts the answers.
When we consider a problem in the form of ⟨Ti,br,(hj, rj,?)⟩,
even though we have the same problem of (hj, rj,?), the an-
swer becomes different depending on Ti. This is the difference
between the typical base-level link prediction and the condi-
tional link prediction.Base-Level Link Prediction
We present the performance of the typical base-level link pre-
diction in Table 6. Since the base-level knowledge graphs of
FBH andFBHE are identical, the performance of all base-
line methods is the same on FBH andFBHE . The base-level
link prediction performance of BiVE on FBH andFBHE is
also very similar to each other. We observed that the MRR
scores of our BiVE models and the two best baselines are al-
most the same on FBHE andDBHE . On FBHE , the average
MRR scores of BiVE-Q and QuatE are both 0.354, and those
of BiVE-B and BiQUE are both 0.356. On DBHE , the aver-
age MRR score of BiVE-Q is 0.265 and that of QuatE is 0.264;
the average MRR score of BiVE-B is 0.275 and that of BiQUE
is 0.274. Overall, our BiVE models show comparable results
to the baseline methods for the typical link prediction task;
our BiVE models have the extra capability of dealing with the
triplet prediction and conditional link prediction tasks.
Data Augmentation of BiVE
We analyze the augmented triplets that are added by our data
augmentation strategy. In Table 7, we show some examples
of a relation sequence pk, a relation r, and the confidence
of(pk, r), the number of augmented triplets based on (pk, r)
denoted by |Skr|, and examples of the augmented triplets in
FBHE andDBHE . According to our random walk policy, we
do not allow going back to an entity that has already been vis-
6Relation Sequence pk Relation r c (pk, r)|Skr|Examples of the Augmented Triplets
FBHENominatesIn, NominatesIn−1, ActsIn, ImpliesProfession , IsA IsA 0.86 610 (Patty Duke, IsA, Actor)
ParticipatesIn, ParticipatesIn−1,ImpliesSports , Plays−1, ParticipatesIn ParticipatesIn 0.81 57 (Houston Rockets, ParticipatesIn, 2003 NBA Draft)
Plays, Plays−1,ImpliesSports−1, HasPosition HasPosition 0.78 295 (Bayer 04Leverkusen, HasPosition, Forward)
Contains, Contains−1,ImpliesLocation−1, HasAHeadquarterIn Contains 0.72 81 (United States, Contains, Charlottesville Virginia)
Program−1, Program, Language Language 0.70 148 (David Copperfield (Film), Language, English Language)
DBHEGenre, ImpliesGenre−1, Genre, Genre−1,ImpliesGenre−1, Genre Genre 0.78 120 (Kenny Rogers, Genre, Pop Rock)
IsPartOf, IsPartOf, ImpliesLocation , IsPartOf IsPartOf 0.76 69 (San Pedro LosAngeles, IsPartOf, California)
IsPartOf, IsPartOf−1,ImpliesLocation−1, IsPartOf−1, TimeZone TimeZone 0.75 122 (Brockton Massachusetts, TimeZone, Eastern Time Zone)
IsProducedBy−1, IsProducedBy, ImpliesProfession , IsA IsA 0.73 80 (Jim Wilson, IsA, Film Producer)
Region, Region−1, Country Country 0.70 41 (Pontefract, Country, England)
Table 7: Examples of the Augmented Triplets in FBHE andDBHE . The higher-level relations are boldfaced.
FBH FBHE DBHE
No. of unique (pk, r) 340,194 349,120 149,365
No. of (pk, r)withc(pk, r)≥0.735,803 39,727 7,030
No. of augmented triplets 16,601 17,463 2,026
|S ∩ E valid|+|S ∩ E test| 5,237 5,380 316
Table 8: Statistics of the Augmented Triplets.
FBH FBHE DBHE
TPLbase+Lhigh 19.2 28.1 65.4
Lbase+Lhigh+Laug 18.7 33.1 56.6
CLPLbase+Lhigh 8.3 12.5 12.4
Lbase+Lhigh+Laug 7.0 11.0 12.5
BLPLbase 139.0 139.0 409.6
Lbase+Lhigh 138.4 138.4 408.1
Lbase+Laug 124.7 125.2 404.9
Lbase+Lhigh+Laug 124.7 125.2 405.4
Table 9: Ablation study of BiVE with different combinations
of the loss terms. The average MR scores on triplet prediction
(TP), conditional link prediction (CLP), and the base-level link
prediction (BLP).
ited. Thus, even though a relation and its reverse relation are
consecutively appeared in a relation sequence in Table 7, it
does not mean that we return back to the previous entity; in-
stead, it means that the walk steps another entity adjacent to
the corresponding relation. In Table 8, we show statistics of
the augmented triplets. Among the diverse combinations of a
relation sequence pkand a relation r, we consider the (pk, r)
pairs whose confidence scores are greater than or equal to 0.7.
It is interesting to see that there exist considerable overlaps be-
tween the set Sof the augmented triplets and EvalidandEtest,
indicating that our augmented triplets include many ground-
truth triplets that are missing in the training set.
Ablation Study of BiVE
In BiVE, we have three different types of loss terms: Lbase,
Lhigh, and Laug. Using different combinations of these loss
terms, we measure the performance of BiVE to check the im-
portance of each loss term. Table 9 shows the average MR
scores of BiVE-Q with different combinations of the loss
terms in three tasks: triplet prediction (TP), conditional linkTriplet Prediction Conditional LP
br Freq. MR MRR Hit@10 MR MRR Hit@10
EquivalentTo 98 17.5 0.416 0.679 2.2 0.744 0.977
ImpliesLanguage 29 35.6 0.292 0.578 18.4 0.632 0.786
ImpliesProfession 210 71.3 0.427 0.569 11.5 0.704 0.844
ImpliesLocation 163 42.2 0.219 0.463 9.4 0.502 0.816
ImpliesTimeZone 44 20.6 0.354 0.631 17.8 0.604 0.707
ImpliesGenre 84 113.8 0.177 0.345 32.6 0.408 0.681
NextAlmaMater 14 71.0 0.161 0.379 2.5 0.651 0.971
TransfersTo 29 67.0 0.140 0.374 5.7 0.527 0.537
Table 10: Performance of BiVE per higher-level relation in
DBHE . Freq. indicates the number of higher-level triplets in
Htestassociated with br.
prediction (CLP), and base-level link prediction (BLP). Note
that TP and CLP require at least two terms, LbaseandLhigh.
Also, Table 10 shows the performance of BiVE-Q per higher-
level relation in DBHE , where Freq. indicates the number of
higher-level triplets in Htestassociated with br. Among the eight
higher-level relations in DBHE , NextAlmaMater and Trans-
fersTo require externally-sourced knowledge. While Equiva-
lentTo is the easiest one, the performance on the other higher-
level relations varies depending on the tasks and the metrics.
Conclusion and Future Work
We define a bi-level knowledge graph by introducing the
higher-level relationships between triplets. We propose BiVE,
which takes into account the structures of the base-level
triplets, the higher-level triplets, and the augmented triplets.
Experimental results show that BiVE significantly outper-
forms state-of-the-art methods in terms of the two newly de-
fined tasks: triplet prediction and conditional link prediction.
We believe our method can contribute to advancing many
knowledge-based applications, including conditional QA [33]
and multi-hop QA [9], with a special emphasis on mixing a
neural language model and structured knowledge [40].
We plan to analyze the pros and cons of our bi-level
knowledge graphs and compare them with other forms of ex-
tended knowledge graphs, such as hyper-relational knowledge
graphs [5]. Also, we will extend BiVE and the proposed tasks
to an inductive learning scenario where both entities and rela-
tions can be new at inference time [22].
7Acknowledgments
This research was supported by IITP grants funded by the Ko-
rean government MSIT 2022-0-00369, 2020-0-00153 (Pene-
tration Security Testing of ML Model Vulnerabilities and De-
fense) and NRF of Korea funded by the Korean Government
MSIT 2018R1A5A1059921, 2022R1A2C4001594.
References
[1] S. Auer, C. Bizer, G. Kobilarov, J. Lehmann, R. Cyga-
niak, and Z. Ives. Dbpedia: A nucleus for a web of open
data. In Proceedings of the 6th International Semantic
Web Conference and the 2nd Asian Semantic Web Con-
ference , pages 722–735, 2007.
[2] K. Bollacker, C. Evans, P. Paritosh, T. Sturge, and J. Tay-
lor. Freebase: A collaboratively created graph database
for structuring human knowledge. In Proceedings of the
2008 ACM SIGMOD International Conference on Man-
agement of Data , pages 1247–1250, 2008.
[3] A. Bordes, N. Usunier, A. Garcia-Dur ´an, J. Weston, and
O. Yakhnenko. Translating embeddings for modeling
multi-relational data. In Proceedings of the International
Conference on Neural Information Processing Systems ,
page 2787–2795, 2013.
[4] I. Chami, A. Wolf, D.-C. Juan, F. Sala, S. Ravi, and C. R ´e.
Low-dimensional hyperbolic knowledge graph embed-
dings. In Proceedings of the 58th Annual Meeting of the
Association for Computational Linguistics , pages 6901–
6914, 2020.
[5] C. Chung, J. Lee, and J. J. Whang. Representation learn-
ing on hyper-relational and numeric knowledge graphs
with transformers. In Proceedings of the 29th ACM
SIGKDD Conference on Knowledge Discovery and Data
Mining , pages 310–322, 2023.
[6] C. Chung and J. J. Whang. Knowledge graph embedding
via metagraph learning. In Proceedings of the 44th Inter-
national ACM SIGIR Conference on Research and De-
velopment in Information Retrieval , pages 2212–2216,
2021.
[7] R. Das, S. Dhuliawala, M. Zaheer, L. Vilnis, I. Durugkar,
A. Krishnamurthy, A. Smola, and A. McCallum. Go for
a walk and arrive at the answer: Reasoning over paths in
knowledge bases using reinforcement learning. In Pro-
ceedings of the 6th International Conference on Learning
Representations , 2018.
[8] T. Demeester, T. Rockt ¨aschel, and S. Riedel. Lifted rule
injection for relation embeddings. In Proceedings of the
2016 Conference on Empirical Methods in Natural Lan-
guage Processing , pages 1389–1399, 2016.[9] Y . Fang, S. Sun, Z. Gan, R. Pillai, S. Wang, and J. Liu.
Hierarchical graph network for multi-hop question an-
swering. In Proceedings of the 2020 Conference on Em-
pirical Methods in Natural Language Processing , pages
8823–8838, 2020.
[10] V . Fionda and G. Pirr `o. Learning triple embeddings from
knowledge graphs. In Proceedings of the 34th AAAI
Conference on Artificial Intelligence , pages 3874–3881,
2020.
[11] A. Garcia-Duran and M. Niepert. Kblrn: End-to-end
learning of knowledge base representations with latent,
relational, and numerical features. In Proceedings of the
34th Conference on Uncertainty in Artificial Intelligence ,
pages 372–381, 2018.
[12] J. Guo and S. Kok. Bique: Biquaternionic embeddings
of knowledge graphs. In Proceedings of the 2021 Con-
ference on Empirical Methods in Natural Language Pro-
cessing , page 8338–8351, 2021.
[13] S. Guo, Q. Wang, L. Wang, B. Wang, and L. Guo. Jointly
embedding knowledge graphs and logical rules. In Pro-
ceedings of the 2016 Conference on Empirical Methods
in Natural Language Processing , pages 192–202, 2016.
[14] X. Han, S. Cao, X. Lv, Y . Lin, Z. Liu, M. Sun, and J. Li.
OpenKE: An open toolkit for knowledge embedding. In
Proceedings of the 2018 Conference on Empirical Meth-
ods in Natural Language Processing: System Demon-
strations , pages 139–144, 2018.
[15] J. Hao, M. Chen, W. Yu, Y . Sun, and W. Wang. Univer-
sal representation learning of knowledge bases by jointly
embedding instances and ontological concepts. In Pro-
ceedings of the 25th ACM SIGKDD International Con-
ference on Knowledge Discovery and Data Mining , page
1709–1719, 2019.
[16] G. Ji, S. He, L. Xu, K. Liu, and J. Zhao. Knowledge
graph embedding via dynamic mapping matrix. In Pro-
ceedings of the 53rd Annual Meeting of the Associa-
tion for Computational Linguistics and the 7th Interna-
tional Joint Conference on Natural Language Process-
ing, pages 687–696, 2015.
[17] S. Ji, S. Pan, E. Cambria, P. Marttinen, and P. S. Yu. A
survey on knowledge graphs: Representation, acquisition
and applications. IEEE Transactions on Neural Networks
and Learning Systems , 33(2):494–514, 2022.
[18] Y . Jiang, X. Wang, H. Fan, Q. Liu, B. Du, and H. Zhu.
Modeling relation path for knowledge graph via dy-
namic projection. In The 32nd International Conference
on Software Engineering and Knowledge Engineering ,
pages 65–70, 2020.
8[19] A. Kristiadi, M. A. Khan, D. Lukovnikov, J. Lehmann,
and A. Fischer. Incorporating literals into knowledge
graph embeddings. In Proceedings of the 18th Interna-
tional Semantic Web Conference , pages 347–363, 2019.
[20] J. H. Kwak, J. Lee, J. J. Whang, and S. Jo. Seman-
tic grasping via a knowledge graph of robotic manipu-
lation: A graph representation learning approach. IEEE
Robotics and Automation Letters , 7(4):9397–9404, 2022.
[21] N. Lao and W. W. Cohen. Relational retrieval using a
combination of path-constrained random walks. Machine
Learning , 81:53–67, 2010.
[22] J. Lee, C. Chung, and J. J. Whang. InGram: Inductive
knowledge graph embedding via relation graphs. In Pro-
ceedings of the 40th International Conference on Ma-
chine Learning , pages 18796–18809, 2023.
[23] G. Li, Z. Sun, L. Qian, Q. Guo, and W. Hu. Rule-based
data augmentation for knowledge graph embedding. AI
Open , 2:186–196, 2021.
[24] X. V . Lin, R. Socher, and C. Xiong. Multi-hop knowl-
edge graph reasoning with reward shaping. In Proceed-
ings of the 2018 Conference on Empirical Methods in
Natural Language Processing , pages 3243–3253, 2018.
[25] Y . Lin, Z. Liu, H. Luan, M. Sun, S. Rao, and S. Liu.
Modeling relation paths for representation learning of
knowledge bases. In Proceedings of the 2015 Conference
on Empirical Methods in Natural Language Processing ,
pages 705–714, 2015.
[26] H. Liu, Y . Wu, and Y . Yang. Analogical inference for
multi-relational embeddings. In Proceedings of the 34th
International Conference on Machine Learning , page
2168–2178, 2017.
[27] C. Meilicke, M. W. Chekol, D. Ruffinelli, and H. Stuck-
enschmidt. Anytime bottom-up rule learning for knowl-
edge graph completion. In Proceedings of the 28th In-
ternational Joint Conference on Artificial Intelligence ,
pages 3137–3143, 2019.
[28] M. Nayyeri, C. Xu, M. M. Alam, J. Lehmann, and H. S.
Yazdi. Logicenn: A neural based knowledge graphs em-
bedding model with logical rules. IEEE Transactions on
Pattern Analysis and Machine Intelligence , 2021.
[29] G. Niu, Y . Zhang, B. Li, P. Cui, S. Liu, J. Li, and
X. Zhang. Rule-guided compositional representation
learning on knowledge graphs. In Proceedings of the
34th AAAI Conference on Artificial Intelligence , pages
2950–2958, 2020.
[30] P. Rosso, D. Yang, and P. Cudr ´e-Mauroux. Beyond
triplets: Hyper-relational knowledge graph embedding
for link prediction. In Proceedings of The Web Confer-
ence 2020 , page 1885–1896, 2020.[31] A. Sadeghian, M. Armandpour, P. Ding, and D. Z. Wang.
Drum: End-to-end differentiable rule mining on knowl-
edge graphs. In Proceedings of the 32nd International
Conference on Neural Information Processing Systems ,
pages 15347–15357, 2018.
[32] M. Sap, R. L. Bras, E. Allaway, C. Bhagavatula,
N. Lourie, H. Rashkin, B. Roof, N. A. Smith, and
Y . Choi. Atomic: An atlas of machine commonsense
for if-then reasoning. In Proceedings of the 33rd AAAI
Conference on Artificial Intelligence , pages 3027–3035,
2019.
[33] H. Sun, W. Cohen, and R. Salakhutdinov. Conditionalqa:
A complex reading comprehension dataset with condi-
tional answers. In Proceedings of the 60th Annual Meet-
ing of the Association for Computational Linguistics ,
pages 3627–3637, 2022.
[34] K. Toutanova and D. Chen. Observed versus latent fea-
tures for knowledge base and text inference. In Proceed-
ings of the 3rd Workshop on Continuous Vector Space
Models and their Compositionality , pages 57–66, 2015.
[35] N. V oskarides, E. Meij, R. Reinanda, A. Khaitan, M. Os-
borne, G. Stefanoni, P. Kambadur, and M. de Ri-
jke. Weakly-supervised contextualization of knowledge
graph facts. In Proceedings of the 41st International
ACM SIGIR Conference on Research and Development
in Information Retrieval , pages 765–774, 2018.
[36] Q. Wang, Z. Mao, B. Wang, and L. Guo. Knowledge
graph embedding: A survey of approaches and applica-
tions. IEEE Transactions on Knowledge and Data Engi-
neering , 29(12):2724–2743, 2017.
[37] Y . Wu and Z. Wang. Knowledge graph embedding with
numeric attributes of entities. In Proceedings of the 3rd
Workshop on Representation Learning for NLP , pages
132–136, 2018.
[38] W. Xiong, T. Hoang, and W. Y . Wang. Deeppath: A re-
inforcement learning method for knowledge graph rea-
soning. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing , pages 564–
573, 2017.
[39] F. Yang, Z. Yang, and W. W. Cohen. Differentiable learn-
ing of logical rules for knowledge base reasoning. In Pro-
ceedings of the 31st International Conference on Neu-
ral Information Processing Systems , pages 2316–2325,
2017.
[40] M. Yasunaga, H. Ren, A. Bosselut, P. Liang, and
J. Leskovec. Qa-gnn: Reasoning with language models
and knowledge graphs for question answering. In Pro-
ceedings of the 2021 Conference of the North American
Chapter of the Association for Computational Linguis-
tics: Human Language Technologies , pages 535–546,
2021.
9[41] H. Zhang, X. Liu, H. Pan, Y . Song, and C. W.-K. Leung.
Aser: A large-scale eventuality knowledge graph. In Pro-
ceedings of The Web Conference 2020 , pages 201–211,
2020.
[42] S. Zhang, Y . Tay, L. Yao, and Q. Liu. Quaternion knowl-
edge graph embeddings. In Proceedings of the 33rd In-
ternational Conference on Neural Information Process-
ing Systems , pages 2735–2745, 2019.
10Appendix
Experimental Settings
All experiments are conducted on machines equipped with Intel(R) Xeon(R) E5-2690 v4 CPUs and 512GB memory. We use
RTX A6000 GPUs unless otherwise stated.
Baseline Methods
In experiments, we use 12 different baseline methods which are ASER [41], MINERV A [7], Multi-Hop [24], Neural-LP [39],
DRUM [31], AnyBURL [27], PTransE [25], RPJE [29], TransD [16], ANALOGY [26], QuatE [42], and BiQUE [12]. We use
GeForce RTX 2080Ti GPUs to run MINERV A, Neural-LP, and DRUM because these methods use TensorFlow version 1.
For ASER, we implement the string matching based inference model described in [41]. The original program of MINERV A
is designed only to predict a tail entity. We add reversed triplets to also predict a head entity. In Multi-Hop, we set all the
addreverse edge options to be true. We run AnyBURL with different rule learning times: 10 seconds, 100 seconds, 1,000
seconds and 10,000 seconds. Since the results of running 10,000 seconds are similar to those of 1,000 seconds, we use the results
of 1,000 seconds. We use the best hyperparameters provided by the authors of each of the baseline methods except PTransE,
RPJE, TransD, ANALOGY, QuatE, and BiQUE; we tune the hyperparameters of PTransE, RPJE, TransD, ANALOGY, QuatE,
and BiQUE because the best hyperparameters are not provided for these methods.
For PTransE and RPJE, we tune the learning rate αusing the range of α={10−6,5·10−6,10−5,5·10−5,10−4,5·10−4}and
the margin γusing the range of γ={0.5,1.0,2.0}. We use TransD, ANALOGY, QuatE, and BiQUE, which are implemented
based on OpenKE [14]. For TransD, we tune the learning rate αusing the range of α={0.1,0.5,1.0,2.0,5.0}and the
margin γusing the range of γ={2.0,5.0,10.0}. For ANALOGY, we tune the learning rate αusing the range of α=
{0.001,0.005,0.01,0.05,0.1,0.5}and the regularization rate βusing the range of β={0.1,0.5,1.0}. For QuatE and BiQUE,
we tune the learning rate αusing the range of α={0.1,0.5,1.0,2.0,5.0}and the regularization rate βusing the range of
β={0.1,0.5,1.0}. For TransD, ANALOGY, QuatE, and BiQUE, validation is done every 50 epochs up to 500 epochs, and we
select the best epoch based on the validation results. Table 11 shows the best hyperparameters of these methods for the triplet
prediction and the base-level link prediction. For the conditional link prediction problem, we combine the scores of the triplet
prediction and the base-level link prediction as described in the main paper; we use the best hyperparameters of the triplet
prediction and the base-level link prediction.
Triplet Prediction Base-Level Link Prediction
PTransEFBH α= 5·10−6, γ= 0.5 α= 10−6, γ= 0.5
FBHE α= 5·10−6, γ= 0.5 α= 10−6, γ= 0.5
DBHE α= 10−6, γ= 0.5 α= 10−6, γ= 0.5
RPJEFBH α= 10−6, γ= 1.0 α= 10−6, γ= 1.0
FBHE α= 10−6, γ= 1.0 α= 10−6, γ= 1.0
DBHE α= 10−6, γ= 1.0 α= 10−6, γ= 1.0
TransDFBH α= 0.5, γ= 5.0 α= 1.0, γ= 5.0
FBHE α= 0.1, γ= 5.0 α= 1.0, γ= 5.0
DBHE α= 0.5, γ= 5.0 α= 0.1, γ= 5.0
ANALOGYFBH α= 0.01, β= 0.01 α= 0.01, β= 0.1
FBHE α= 0.01, β= 0.01 α= 0.01, β= 0.1
DBHE α= 0.05, β= 0.1 α= 0.01, β= 0.1
QuatEFBH α= 1.0, β= 0.05 α= 0.1, β= 0.05
FBHE α= 1.0, β= 0.01 α= 0.1, β= 0.05
DBHE α= 0.5, β= 0.05 α= 0.5, β= 0.1
BiQUEFBH α= 0.1, β= 0.01 α= 0.5, β= 0.05
FBHE α= 0.1, β= 0.01 α= 0.5, β= 0.05
DBHE α= 0.1, β= 0.05 α= 0.5, β= 0.1
Table 11: The best hyperparameters of PTransE, RPJE, TransD, ANALOGY, QuatE, and BiQUE. α, β, γ indicate the learning
rate, the regularization rate, and the margin, respectively.
11FBH FBHE DBHE
α β λ 1λ2α β λ 1λ2α β λ 1λ2
BiVE-QTriplet Prediction 0.1 0.01 0.5 1.0 0.1 0.01 1.0 0.2 0.5 0.05 0.2 1.0
Conditional Link Prediction 0.1 0.01 1.0 0.2 0.1 0.01 1.0 0.2 0.5 0.01 1.0 0.2
Base-Level Link Prediction 0.1 0.05 1.0 0.2 0.1 0.05 0.5 0.2 0.5 0.1 0.5 0.2
BiVE-BTriplet Prediction 0.1 0.01 0.5 0.5 0.1 0.01 0.5 0.2 0.1 0.01 1.0 0.2
Conditional Link Prediction 0.1 0.01 1.0 0.2 0.1 0.01 1.0 0.2 0.5 0.01 1.0 0.2
Base-Level Link Prediction 0.1 0.05 0.5 0.2 0.1 0.05 0.5 0.5 0.5 0.1 0.2 1.0
Table 12: The best hyperparameters of BiVE on validation.
Hyperparameters of BiVE
Within BiVE, we use the scoring function of QuatE [42] for BiVE-Q and BiQUE [12] for BiVE-B. On the validation set, we
tune the learning rate α, the regularization rate β, and the weights λ1andλ2inLBiVE. We use the search space of {0.2,0.5,1.0}
for both λ1andλ2. Validation is done every 50 epochs up to 500 epochs, and we select the best epoch based on the validation
results. Table 12 shows the best hyperparameters of BiVE on the validation sets.
Real-World Bi-Level Knowledge Graphs
We describe the details about how we create the three real-world bi-level knowledge graphs, FBH ,FBHE , and DBHE .
Base-Level Knowledge Graphs
We use FB15K237 [34] as the base-level knowledge graphs for FBH andFBHE .FB15K237 is a standard benchmark knowl-
edge graph dataset which is constructed by taking 401 most frequent relations and merging near-duplicate and inverse relations
inFB15K [3] from Freebase [2].
We use a filtered version of DB15K [11] which is constructed based on DBpedia [1]. By following the strategy used
in [38], we first remove the relations that are not in the form of DBpedia URL, such as ‘http://www.w3.org/2000/01/rdf-
schema#seeAlso’, since these types of relations do not have clear semantics. Then, we take the relations involved in more than
100 triplets and merge near-duplicate and inverse relations by following the strategy used in [34]. For example, (Chevrolet,
owningCompany, General Motors) and (Chevrolet, owner, General Motors) are merged.
Higher-Level Triplets
In Table 13, we show the higher-level relations and the corresponding examples of the higher-level triplets used to create FBH ,
FBHE , and DBHE . While FBHE contains all ten higher-level relations, FBH contains only the first six higher-level relations.
Among the higher-level relations in Table 13, WorksFor, SucceededBy, TransfersTo, and HigherThan in FBHE and NextAl-
maMater and TransfersTo in DBHE require externally-sourced knowledge. For example, we crawled Wikipedia articles to find
information about the (vice)presidents of the United States, the teams a player was playing for, and the alma mater of politi-
cians. Also, to create ⟨Ti, HigherThan, Tj⟩inFBHE , we used the most recent rankings of Fortune 1000 and Times University
Ranking. Table 14 and Table 15 show all types of higher-level triplets used to create FBH ,FBHE , and DBHE .
12br ⟨Ti,br, Tj⟩ Description
FBHEPrerequisiteForTi: (BAFTA Award, Nominates, The King’s Speech) For The King’s Speech to win BAFTA Award, BAFTA Award should nominate The
Tj: (The King’s Speech, Wins, BAFTA Award) King’s Speech.
EquivalentToTi: (Hillary Clinton, IsMarriedTo, Bill Clinton)The two triplets indicate the same information.Tj: (Bill Clinton, IsMarriedTo, Hillary Clinton)
ImpliesLocationTi: (Sweden, CapitalIsLocatedIn, Stockholm)‘The capital of Sweden is Stockholm’ implies ‘Sweden contains Stockholm’.Tj: (Sweden, Contains, Stockholm)
ImpliesProfessionTi: (Liam Neeson, ActsIn, Love Actually)‘Liam Neeson acts in Love Actually’ implies ‘Liam Neeson is an actor’.Tj: (Liam Neeson, IsA, Actor)
ImpliesSportsTi: (Boston Red Socks, HasPosition, Infield)‘Boston Red Socks has an infield position’ implies ‘Boston Red Socks plays baseball’.Tj: (Boston Red Socks, Plays, Baseball)
NextEventPlaceTi: (1932 Summer Olympics, IsHeldIn, Los Angeles) Summer Olympics in 1932 and 1936 were held in Los Angeles and Berlin, respectively.
Tj: (1936 Summer Olympics, IsHeldIn, Berlin) 1936 Summer Olympics is the next event of 1932 Summer Olympics.
WorksForTi: (Joe Biden, HoldsPosition, Vice President)Joe Biden was a vice president when Barack Obama was a president of the United States.Tj: (Barack Obama, HoldsPosition, President)
SucceededByTi: (George W.Bush, HoldsPosition, President)President Barack Obama succeeded President George W. Bush.Tj: (Barack Obama, HoldsPosition, President)
TransfersToTi: (David Beckham, PlaysFor, Real Madrid CF)David Beckham transferred from Real Madrid CF to LA Galaxy.Tj: (David Beckham, PlaysFor, LA Galaxy)
HigherThanTi: (Walmart, IsRankedIn, Fortune 500)Walmart is ranked higher than Bank of America in Fortune 500.Tj: (Bank ofAmerica, IsRankedIn, Fortune 500)
DBHEEquivalentToTi: (David Beckham, IsMarriedTo, Victoria Beckham)The two triplets indicate the same information.Tj: (Victoria Beckham, IsMarriedTo, David Beckham)
ImpliesLanguageTi: (Italy, HasOfficialLanguage, Italian Language) ‘The official language of Italy is the Italian language’ implies ‘The Italian language is
Tj: (Italy, UsesLanguage, Italian Language) used in Italy’.
ImpliesProfessionTi: (Psycho, IsDirectedBy, Alfred Hitchcock)‘Psycho is directed by Alfred Hitchcock’ implies ‘Alfred Hitchcock is a film producer’.Tj: (Alfred Hitchcock, IsA, Film Producer)
ImpliesLocationTi: (Mariah Carey, LivesIn, New York City)‘Mariah Carey lives in New York City’ implies ‘Mariah Carey lives in New York’Tj: (Mariah Carey, LivesIn, New York)
ImpliesTimeZoneTi: (Czech Republic, TimeZone, Central European) ‘Czech Republic is included in Central European Time Zone’ implies ‘Prague is included
Tj: (Prague, TimeZone, Central European) in Central European Time Zone’.
ImpliesGenreTi: (Pharrell Williams, Genre, Progressive Rock) ‘Pharrell Williams is a progressive rock musician’ implies ‘Pharrell Williams is a rock
Tj: (Pharrell Williams, Genre, Rock Music) musician’.
NextAlmaMaterTi: (Gerald Ford, StudiesIn, University ofMichigan)Gerald Ford studied in University of Michigan. Then, he studied in Yale University.Tj: (Gerald Ford, StudiesIn, Yale University)
TransfersToTi: (Ronaldo, PlaysFor, FC Barcelona)Ronaldo transferred from FC Barcelona to Inter Millan.Tj: (Ronaldo, PlaysFor, Inter Millan)
Table 13: The higher-level relations and the corresponding examples of the higher-level triplets used to create FBHE , and
DBHE .
13Example Frequency
⟨Ti, PrerequisiteFor, Tj⟩Ti: (Person A, DatesWith, Person B) (Bruce Willis, DatesWith, Demi Moore)222Tj: (Person A, BreaksUpWith, Person B) (Bruce Willis, BreaksUpWith, Demi Moore)
Ti: (Award A, Nominates, Work B) (BAFTA Award, Nominates, The King’s Speech)2,335Tj: (Work B, Wins, Award A) (The King’s Speech, Wins, BAFTA Award)
Ti: (Person A, HasNationality, Country B) (Neymar, HasNationality, Brazil)109Tj: (Person A, PlaysFor, National Team of B) (Neymar, PlaysFor, Brazil National Football Team)
⟨Ti, EquivalentTo, Tj⟩Ti: (Person A, IsASiblingTo, Person B) (Serena Williams, IsASiblingTo, Venus Williams)120Tj: (Person B, IsASiblingTo, Person A) (Venus Williams, IsASiblingTo, Serena Williams)
Ti: (Person A, IsMarriedTo, Person B) (Hillary Clinton, IsMarriedTo, Bill Clinton)352Tj: (Person B, IsMarriedTo, Person A) (Bill Clinton, IsMarriedTo, Hillary Clinton)
Ti: (Person A, HasAFriendshipWith, Person B) (Bob Dylan, HasAFriendshipWith, The Beatles)1,832Tj: (Person B, HasAFriendshipWith, Person A) (The Beatles, HasAFriendshipWith, Bob Dylan)
Ti: (Person A, IsAPeerOf, Person B) (Jimi Hendrix, IsAPeerOf, Eric Clapton)132Tj: (Person B, IsAPeerOf, Person A) (Eric Clapton, IsAPeerOf, Jimi Hendrix)
⟨Ti, ImpliesLocation, Tj⟩Ti: (Location A, Contains, Location B) (England, Contains, Warwickshire)2,415Tj: (Location containing A, Contains, Location in B) (United Kingdom, Contains, Birmingham)
Ti: (Organization A, Headquarter, Location B) (Kyoto University, Headquarter, Kyoto)820Tj: (Location B, Contains, Organization A) (Kyoto, Contains, Kyoto University)
Ti: (Country A, CapitalIsLocatedIn, City B) (Sweden, CapitalIsLocatedIn, Stockholm)83Tj: (Country A, Contains, City B) (Sweden, Contains, Stockholm)
⟨Ti, ImpliesProfession, Tj⟩Ti: (Person A, IsA, Specialized Profession of B) (Mariah Carey, IsA, Singer-songwriter)2,364Tj: (Person A, IsA, Profession B) (Mariah Carey, IsA, Musician)
Ti: (Rock&Roll Hall of Fame, Inducts, Person A) (Rock&Roll Hall ofFame, Inducts, Bob Dylan)66Tj: (Person A, IsA, Musician/Artist) (Bob Dylan, IsA, Musician)
Ti: (Film A, IsWrittenBy, Person B) (127 Hours, IsWrittenBy, Danny Boyle)893Tj: (Person B, IsA, Writer/Film Producer) (Danny Boyle, IsA, Film producer)
Ti: (Person A, ActsIn, Film B) (Liam Neeson, ActsIn, Love Actually)10,864Tj: (Person A, IsA, Actor) (Liam Neeson, IsA, Actor)
Ti: (Person A, HoldsPosition, Government Position B) (Barack Obama, HoldsPosition, President)120Tj: (Person A, IsA, Politician) (Barack Obama, IsA, Politician)
⟨Ti, ImpliesSports, Tj⟩Ti: (Team A, HasPosition, Position of B) (Boston Red Socks, HasPosition, Infield)2,936Tj: (Team A, Plays, Sports B) (Boston Red Socks, Plays, Baseball)
Ti: (League of A, Includes, Team B) (National League, Includes, New York Mets)824Tj: (Team B, Plays, Sports A) (New York Mets, Plays, Baseball)
Ti: (Team A, ParticipatesIn, Draft of B) (Atlanta Braves, ParticipatesIn, MLB Draft)528Tj: (Team A, Plays, Sports B) (Atlanta Braves, Plays, Baseball)
⟨Ti, NextEventPlace, Tj⟩Ti: (Event A, IsHeldIn, Location A) (1932 Summer Olympics, IsHeldIn, Los Angeles)47Tj: (Next Event of A, IsHeldIn, Location B) (1936 Summer Olympics, IsHeldIn, Berlin)
⟨Ti, WorksFor, Tj⟩Ti: (Person A, HoldsPosition, Vice President) (Joe Biden, HoldsPosition, Vice President)13Tj: (Person B, HoldsPosition, President) (Barack Obama, HoldsPosition, President)
⟨Ti, SucceededBy, Tj⟩Ti: (Person A, HoldsPosition, President/Vice President) (George W.Bush, HoldsPosition, President)30Tj: (Person B, HoldsPosition, President/Vice President) (Barack Obama, HoldsPosition, President)
⟨Ti, TransfersTo, Tj⟩Ti: (Person A, PlaysFor, Team B) (David Beckham, PlaysFor, Real Madrid CF)377Tj: (Person A, PlaysFor, Team C) (David Beckham, PlaysFor, LA Galaxy)
⟨Ti, HigherThan, Tj⟩Ti: (Item A, IsRankedIn, Ranking List C) (Walmart, IsRankedIn, Fortune 500)7,459Tj: (Item B, IsRankedIn, Ranking List C) (Bank ofAmerica, IsRankedIn, Fortune 500)
Table 14: All types of higher-level triplets to create FBH andFBHE .
14Example Frequency
⟨Ti, EquivalentTo, Tj⟩Ti: (Person A, IsMarriedTo, Person B) (Hillary Clinton, IsMarriedTo, Bill Clinton)314Tj: (Person B, IsMarriedTo, Person A) (Bill Clinton, IsMarriedTo, Hillary Clinton)
Ti: (Location A, UsesLanguage, Language B) (Brazil, UsesLanguage, Portuguese Language)120Tj: (Language B, IsSpokenIn, Location A) (Portuguese Language, IsSpokenIn, Brazil)
Ti: (Person A, Influences, Person B) (Baruch Spinoza, Influences, Immanuel Kant)394Tj: (Person B, IsInfluencedBy, Person A) (Immanuel Kant, IsInfluencedBy, Baruch Spinoza)
⟨Ti, ImpliesLanguage, Tj⟩Ti: (Location A, HasOfficialLanguage, Language B) (Italy, HasOfficialLanguage, Italian Language)196Tj: (Location A, UsesLanguage, Language B) (Italy, UsesLanguage, Italian Language)
Ti: (Location A, UsesLanguage, Language B) (United States, UsesLanguage, English Language)75Tj: (Location in A, UsesLanguage, Language B) (California, UsesLanguage, English Language)
⟨Ti, ImpliesProfession, Tj⟩Ti: (Work A, MusicComposedBy, Person B) (Forrest Gump, MusicComposedBy, Alan Silvestri)553Tj: (Person B, IsA, Musician/Composer) (Alan Silvestri, IsA, Composer)
Ti: (Work A, Starring, Person B) (Love Actually, Starring, Liam Neeson)737Tj: (Person B, IsA, Actor) (Liam Neeson, IsA, Actor)
Ti: (Work A, CinematographyBy, Person B) (Jurassic Park, CinematographyBy, Dean Cundey)299Tj: (Person B, IsA, Cinematographer) (Dean Cundey, IsA, Cinematographer)
Ti: (Work A, IsDirectedBy, Person B) (Psycho, IsDirectedBy, Alfred Hitchcock)295Tj: (Person B, IsA, Film Director/Television Director) (Alfred Hitchcock, IsA, Film Director)
Ti: (Work A, IsProducedBy, Person B) (King Kong, IsProducedBy, Merian C.Cooper)354Tj: (Person B, IsA, Film Producer/Television Producer) (Merian C.Cooper, IsA, Film Producer)
Ti: (Person A, AssociatesWithRecordLabel, Record B) (Bo Diddley, AssociatesWithRecordLabel, Atlantic Records)155Tj: (Person A, IsA, Record Producer) (Bo Diddley, IsA, Record Producer)
⟨Ti, ImpliesLocation, Tj⟩Ti: (Location A, IsPartOf, Location B) (Ann Arbor, IsPartOf, Washtenaw County Michigan)1,174Tj: (Location in A, IsPartOf, Location Containing B) (Ann Arbor, IsPartOf, Michigan)
Ti: (Organization A, IsLocatedIn, Location B) (Adobe Systems, IsLocatedIn, San Jose California)250Tj: (Organization A, IsLocatedIn, Location Containing B) (Adobe Systems, IsLocatedIn, California)
Ti: (Person A, LivesIn, Location B) (Mariah Carey, LivesIn, New York City)213Tj: (Person A, LivesIn, Location Containing B) (Mariah Carey, LivesIn, New York)
⟨Ti, ImpliesTimeZone, Tj⟩Ti: (Location A, TimeZone, Time Zone B) (Czech Republic, TimeZone, Central European Time)409Tj: (Location in A, TimeZone, Time Zone B) (Prague, TimeZone, Central European Time)
⟨Ti, ImpliesGenre, Tj⟩Ti: (Musician A, Genre, Genre B) (Pharrell Williams, Genre, Progressive Rock)767Tj: (Musician A, Genre, Parent Genre of B) (Pharrell Williams, Genre, Rock Music)
⟨Ti, NextAlmaMater, Tj⟩Ti: (Person A, StudiesIn, Institution B) (Gerald Ford, StudiesIn, University ofMichigan)112Tj: (Person A, StudiesIn, Institution C) (Gerald Ford, StudiesIn, Yale University)
⟨Ti, TransfersTo, Tj⟩Ti: (Person A, PlaysFor, Team B) (Ronaldo, PlaysFor, FC Barcelona)300Tj: (Person A, PlaysFor, Team C) (Ronaldo, PlaysFor, Inter Millan)
Table 15: All types of higher-level triplets to create DBHE .
15Additional Experimental Results
We provide additional experimental results using a different implementation of BiVE. The implementation of BiVE is based
on OpenKE [14]. Three loss functions, Lbase,Lhigh, andLaug, are implemented based on the Softplus loss provided in OpenKE,
which can be formulated as follows:
L=1
|E|X
(h,r,t )∈Eg(−f(h,r,t)) +1
|E′|X
(h′,r′,t′)∈E′g(f(h′,r′,t′)) (1)
We recently implemented a different version of the Softplus loss that averages the loss incurred by the positive and negative
triplets at once. The new implementation of the Softplus loss can be formulated as follows:
Lnew=1
|E|+|E′|
X
(h,r,t )∈Eg(−f(h,r,t)) +X
(h′,r′,t′)∈E′g(f(h′,r′,t′))

Using the new implementation of the loss in BiVE, we provide the experimental results of triplet prediction and conditional
link prediction in Table 16 and Table 17. Since ANALOGY, QuatE, and BiQUE also use the Softplus loss implemented in
OpenKE, the results of these methods are also changed. We see that the new implementation of the Softplus loss improves
the performance of BiVE. Also, Table 18 shows the performance of the typical base-level link prediction. In all experiments,
the conclusion remains the same; our BiVE models significantly outperform baseline methods for the triplet prediction and
conditional link prediction tasks while achieving comparable results to the baseline methods for the base-level link prediction
task. Table 19 shows the results of the ablation study of BiVE with the new implementation.
Note that the original implementation of BiVE is also correct; the loss term defined in (1) aims to make the scores of the
positive triplets higher than those of the negative triplets. Both implementations of BiVE can be found at https://github.com/bdi-
lab/BiVE.
FBH FBHE DBHE
MR (↓) MRR ( ↑) Hit@10 ( ↑) MR ( ↓) MRR ( ↑) Hit@10 ( ↑) MR ( ↓) MRR ( ↑) Hit@10 ( ↑)
ASER 74541.7 ±0.0 0.011±0.000 0.015±0.000 57916.0 ±0.0 0.050±0.000 0.070±0.000 18157.6 ±0.0 0.042±0.000 0.075±0.000
MINERV A 109055.1 ±98.5 0.093±0.002 0.113±0.002 85571.5 ±768.3 0.220±0.008 0.300±0.005 20764.3 ±72.3 0.177±0.005 0.221±0.004
Multi-Hop 108731.7 ±43.2 0.105±0.001 0.117±0.000 83643.8 ±33.2 0.255±0.012 0.311±0.003 20505.8 ±9.3 0.191±0.001 0.230±0.002
Neural-LP 115016.6 ±0.0 0.070±0.000 0.073±0.000 90000.4 ±0.0 0.238±0.000 0.274±0.000 21130.5 ±0.0 0.170±0.000 0.209±0.000
DRUM 115016.6 ±0.0 0.069±0.001 0.073±0.000 90000.3 ±0.0 0.261±0.000 0.274±0.000 21130.5 ±0.0 0.166±0.001 0.209±0.000
AnyBURL 108079.6 ±0.0 0.096±0.000 0.108±0.000 83136.8 ±5.3 0.191±0.001 0.252±0.001 20530.8 ±0.0 0.177±0.000 0.214±0.000
PTransE 111024.3 ±855.0 0.069±0.000 0.071±0.000 86793.2 ±961.0 0.249±0.001 0.274±0.000 18888.7 ±457.3 0.158±0.001 0.195±0.002
RPJE 113082.0 ±945.2 0.070±0.000 0.072±0.000 89173.1 ±797.3 0.267±0.000 0.274±0.000 20290.4 ±417.2 0.166±0.001 0.206±0.002
TransD 74277.3 ±2907.8 0.052±0.001 0.104±0.002 52159.4 ±758.9 0.238±0.002 0.280±0.003 16698.1 ±370.2 0.116±0.004 0.189±0.009
ANALOGY 152635.3 ±554.7 0.100±0.001 0.110±0.001 118023.1 ±337.9 0.284±0.003 0.310±0.001 23512.7 ±3265.1 0.160±0.004 0.199±0.008
QuatE 109954.7 ±2068.8 0.104±0.000 0.114±0.001 85021.3 ±1402.8 0.251±0.013 0.282±0.005 27548.3 ±304.0 0.163±0.001 0.191±0.002
BiQUE 79802.8 ±528.9 0.104±0.000 0.115±0.000 59997.8 ±519.0 0.293±0.002 0.319±0.000 18259.8 ±231.0 0.160±0.001 0.194±0.001
BiVE-Q 5.6±0.9 0.876±0.003 0.938±0.003 10.7±9.3 0.728±0.008 0.882±0.013 4.3±0.4 0.634±0.008 0.923±0.005
BiVE-B 7.9 ±1.2 0.862±0.008 0.931±0.005 17.7±23.0 0.708±0.008 0.863±0.012 11.6±5.7 0.629±0.018 0.867±0.021
Table 16: Results of Triplet Prediction with New Implementation. The best scores are boldfaced and the second best scores are
underlined. Our models, BiVE-Q and BiVE-B, significantly outperform all other baseline methods in terms of all metrics on all
datasets.
16FBH FBHE DBHE
MR (↓) MRR ( ↑) Hit@10 ( ↑) MR ( ↓) MRR ( ↑) Hit@10 ( ↑) MR ( ↓) MRR ( ↑) Hit@10 ( ↑)
ASER 1183.9 ±0.0 0.251±0.000 0.316±0.000 970.7±0.0 0.289±0.000 0.382±0.000 1893.5 ±0.0 0.225±0.000 0.348±0.000
MINERV A 3817.8 ±58.9 0.328±0.013 0.415±0.009 3018.5 ±45.8 0.407±0.013 0.492±0.014 2934.1 ±32.2 0.362±0.007 0.433±0.014
Multi-Hop 1878.2 ±12.0 0.421±0.003 0.578±0.003 1447.3 ±11.9 0.443±0.002 0.615±0.002 1012.3 ±28.5 0.442±0.007 0.652±0.008
Neural-LP 185.9 ±1.3 0.433±0.002 0.648±0.004 146.2±1.0 0.466±0.002 0.716±0.007 32.2±1.9 0.517±0.006 0.756±0.004
DRUM 262.7 ±13.3 0.394±0.002 0.555±0.003 207.6±10.0 0.413±0.010 0.620±0.018 49.0±3.9 0.470±0.010 0.732±0.012
AnyBURL 228.5 ±11.8 0.380±0.004 0.563±0.013 166.0±7.9 0.418±0.002 0.607±0.008 81.7±4.0 0.403±0.002 0.594±0.004
PTransE 214.8 ±0.7 0.440±0.001 0.686±0.002 167.0±1.8 0.516±0.001 0.752±0.001 19.3±0.2 0.505±0.004 0.780±0.001
RPJE 212.5 ±0.1 0.440±0.001 0.686±0.001 159.0±0.0 0.528±0.001 0.753±0.001 19.3±0.1 0.504±0.004 0.779±0.002
TransD 190.1 ±18.0 0.300±0.003 0.496±0.005 165.6±8.0 0.363±0.003 0.529±0.006 35.5±1.0 0.436±0.006 0.708±0.005
ANALOGY 130.6 ±11.6 0.331±0.014 0.486±0.033 122.4±21.8 0.335±0.036 0.501±0.055 67.3±83.9 0.391±0.129 0.600±0.224
QuatE 124.4 ±1.5 0.399±0.004 0.572±0.009 94.6±1.4 0.419±0.003 0.598±0.008 31.0±1.8 0.432±0.013 0.700±0.019
BiQUE 107.5 ±1.3 0.414±0.001 0.640±0.003 84.9±0.7 0.444±0.003 0.670±0.003 20.1±3.0 0.493±0.003 0.775±0.003
BiVE-Q 2.2±0.1 0.913±0.005 0.982±0.001 3.8±0.2 0.838±0.003 0.929±0.003 2.3±0.2 0.860±0.003 0.978±0.003
BiVE-B 2.7 ±0.3 0.907±0.004 0.978±0.002 4.3±0.3 0.833±0.002 0.928±0.003 3.3±0.4 0.845±0.012 0.960±0.005
Table 17: Results of Conditional Link Prediction with New Implementation. The best scores are boldfaced and the second best
scores are underlined. Our models, BiVE-Q and BiVE-B, significantly outperform all other baseline methods in terms of all
metrics on all datasets.
FBHE DBHE
MR (↓) Hit@10 ( ↑) MR ( ↓) Hit@10 ( ↑)
ASER 1489.3 ±0.0 0.323 ±0.000 2218.8 ±0.0 0.197 ±0.000
MINERV A 3828.4 ±56.9 0.339 ±0.003 3530.7 ±50.1 0.297 ±0.006
Multi-Hop 2284.0 ±9.5 0.500 ±0.001 2489.4 ±15.3 0.404 ±0.004
Neural-LP 1942.5 ±0.5 0.486 ±0.001 2904.8 ±0.6 0.357 ±0.001
DRUM 1945.6 ±0.8 0.490 ±0.002 2904.7 ±0.7 0.359 ±0.001
AnyBURL 342.0 ±4.6 0.526 ±0.002 879.1 ±5.7 0.364 ±0.003
PTransE 2077.6 ±10.3 0.333 ±0.000 3346.0 ±20.0 0.277 ±0.002
RPJE 1754.6 ±7.5 0.368 ±0.001 2991.7 ±28.1 0.341 ±0.000
TransD 166.3 ±1.3 0.527 ±0.001 429.0±7.5 0.423±0.001
ANALOGY 244.2 ±7.7 0.516 ±0.003 1049.5 ±47.2 0.332 ±0.006
QuatE 144.1 ±2.6 0.594 ±0.001 549.1 ±11.3 0.451 ±0.002
BiQUE 140.4 ±1.7 0.591 ±0.001 505.2 ±5.6 0.458±0.001
BiVE-Q 127.6 ±2.6 0.596±0.002 552.3±10.8 0.453 ±0.001
BiVE-B 124.8±1.6 0.598 ±0.001 524.1±8.6 0.448 ±0.002
Table 18: Results of Base-Level Link Prediction with New Im-
plementation.FBH FBHE DBHE
TPLbase+Lhigh 5.1 11.9 4.1
Lbase+Lhigh+Laug 5.6 10.7 4.3
CLPLbase+Lhigh 2.7 4.3 2.2
Lbase+Lhigh+Laug 2.2 3.8 2.3
BLPLbase 144.1 144.1 549.1
Lbase+Lhigh 141.4 143.3 563.5
Lbase+Laug 127.9 127.7 541.6
Lbase+Lhigh+Laug 126.0 127.6 552.3
Table 19: Ablation study of BiVE with different combinations
of the loss terms (with the new implementation of BiVE). The
average MR scores on triplet prediction (TP), conditional link
prediction (CLP), and the base-level link prediction (BLP).
17