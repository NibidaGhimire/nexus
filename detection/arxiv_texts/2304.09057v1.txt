Predicting the Electronic Density Response of Condensed-Phase Systems
to Electric Field Perturbations
Alan M. Lewis,1Paolo Lazzaroni,1and Mariana Rossi1
Max Planck Institute for the Structure and Dynamics of Matter, Luruper Chaussee 149, 22761 Hamburg,
Germany
(*Electronic mail: mariana.rossi@mpsd.mpg.de)
(*Electronic mail: alan.lewis@mpsd.mpg.de)
(Dated: 19 April 2023)
We present a local and transferable machine learning approach capable of predicting the real-space density
response of both molecules and periodic systems to external homogeneous electric ﬁelds. The new method,
SALTER, builds on the Symmetry-Adapted Gaussian Process Regression SALTED framework. SALTER
requiresonlyasmall, butnecessary, modiﬁcationtothedescriptorsusedtorepresenttheatomicenvironments.
We present the performance of the method on isolated water molecules, bulk water and a naphthalene crystal.
Root mean square errors of the predicted density response lie at or below 10% with barely more than 100
training structures. Derived quantities, such as polarizability tensors and even Raman spectra further derived
from these tensors show a good agreement with those calculated directly from quantum mechanical methods.
Therefore, SALTER shows excellent performance when predicting derived quantities, while retaining all of
the information contained in the full electronic response. This method is thus capable of learning vector ﬁelds
in a chemical context and serves as a landmark for further developments.
Machine-learning (ML) tools are now widely used
in computational chemistry to reduce the cost of sim-
ulations and aid the interpretation of data.1–4ML
models are often employed to predict energies and
forces,5–9which are then used to drive molecular dy-
namics simulations.10–13This has opened the door to ab
initioquality simulations on timescales and system sizes
which are completely inaccessible by conventional AIMD
approaches.13–17However, this approach comes at the
cost of losing access to other properties directly related
to the electronic density.
These must be obtained by some independent method,
which triggered the development of many recent
models.18–23We have recently developed SALTED, a
machine learning method which directly predicts the
real-space electronic density of molecular and condensed
phase systems.24,25SALTED may be applied in tan-
dem with machine-learned potentials, providing access
toeveryground-stateelectronic-structurepropertywhich
would be available in a traditional AIMD calculation.
In this Communication, we present SALTER
(Symmetry-Adapted Learning of Three-dimensional
Electron Responses). SALTER builds on SALTED to
predict the static real-space response of the electron
density of a molecule or material to a homogeneous
electric ﬁeld. This response deﬁnes the global dielectric
susceptibility of the system, which is needed to simulate
many ﬂavors of vibrational Raman and sum-frequency
spectra,26–32and deﬁnes the high-frequency electronic
screening which impacts a wide range of static and
transport properties of molecules and materials.33–37
The computational burden of calculating this elec-
tronic response is considerable – the most commonly
employed method, density-functional perturbation the-
ory (DFPT),38scales similarly to the underlying DFTcalculation with respect to system size, but with a 4-
10 times larger prefactor.37,39As such, this calculation
is a bottleneck in assessing the dielectric properties of
systems along long molecular dynamics trajectories and
large system sizes.
As in our previous work,24,25SALTER is based on an
atom-centred expansion of the density response, with the
coeﬃcients of the expansion predicted using the sym-
metry adapted Gaussian process regression (SA-GPR)
approach.40Thisproducesamethodwhichislocal,trans-
ferable, and capable of treating molecular and periodic
systems on the same footing.24The key modiﬁcation in-
troduced in this work is the inclusion of a “dummy” atom
in the representation of each atomic environment, which
encodes information about the direction of the applied
perturbation in a simple way, while retaining all of the
symmetry properties of the representation. This modiﬁ-
cation is signiﬁcantly simpler than formally including the
applied ﬁeld in the descriptor, which would require tak-
ing the tensor product of each symmetry-adapted kernel
with a rank-1 tensor describing the applied ﬁeld.
We start by brieﬂy presenting the method. We follow
the same general approach outlined in our previous work
learningtheelectrondensity, ρ,24,25andhighlightthekey
diﬀerences to the previous method. For a more detailed
account we refer the reader to Refs. 24 and 25. The
density response ρ(1)
βto a ﬁeld applied along a Cartesian
axisβcan be expanded using a set of auxiliary basis
functionsφ, such that
dρ(r)
deβ≡ρ(1)
β(r)
≈˜ρ(1)
β(r) =/summationdisplay
i,σ,Uciσβφiσ(r−Ri+T(U)).(1)
HereRiisthepositionofatom i, thebasisfunction φiσisarXiv:2304.09057v1  [physics.chem-ph]  18 Apr 20232
centeredonatom iandmaybewrittenastheproductofa
radialpartRn(r)andarealsphericalharmonic Yλµ(θ,φ);
for brevity we use a composite index σ≡(anλµ ), where
alabels the atomic species. T(U)is a translation vector
to a point removed from the central reference unit cell by
an integer multiple U= (Ux, Uy, Uz)of the lattice vec-
tors, present only when the system under consideration
is periodic. The optimal coeﬃcients for this expansion
can be found by minimising the loss function
/epsilon1(cRI
β) =/integraldisplay
u.c.dr/vextendsingle/vextendsingle/vextendsingle˜ρ(1)
β(r;cRI
β)−ρ(1)
β(r)/vextendsingle/vextendsingle/vextendsingle2
,(2)
which yields
cRI
β=S−1w(1)
β. (3)
Here Sis the overlap matrix of the periodic or non-
periodic basis functions, and w(1)
βis a vector of the pro-
jections of the self-consistent density response ρ(1),QM
β(r)
onto the basis,
w(1)
iσβ=Ucut/summationdisplay
U/angbracketleftBig
φiσ(r−Ri+T(U))/vextendsingle/vextendsingle/vextendsingleρ(1)
β(r)/angbracketrightBig
u.c.(4)
These optimal coeﬃcients cRI
βserve as the references
against which the accuracy of coeﬃcients predicted by
the ML model cML
βare evaluated.
In order to predict cML
β, we approximate them as a
linear combination of the regression weights bjσβassoci-
atedwithMreferenceenvironments, andacovariantker-
nelkσβ(Ai,Mj)which describes the similarity between
the reference environments {Mj}and the atomic envi-
ronments which comprise the target structure {Ai}:
cML
iσβ≈M/summationdisplay
jbjσβkσβ(Ai,Mj). (5)
Note that here cML
iσβandbjσβare vectors of length 2λ+1,
andkσ,β(Ai,Mj)is a square matrix of the same dimen-
sion. Deﬁning and minimizing a loss function analogous
to Eq. (2) allows us to determine the regression weights
bβ:
/epsilon1(bβ) =N/summationdisplay
A=1/integraldisplay
u.c.dr/vextendsingle/vextendsingle/vextendsingle˜ρ(1),ML
β(r;bβ)−ρ(1),RI
β(r)/vextendsingle/vextendsingle/vextendsingle2
+ηbTKMMb.(6)
Here the index Aruns over each of the Nstructures in
the training set, the matrix KMMcouples the set of ref-
erenceatomicenvironmentstooneanother, andweintro-
duce the hyperparamter ηto control the regularization of
the minimization. In all the examples presented here, we
use a regularization parameter of η= 10−8, ﬁnding only
a weak dependence of the observed error on η. Eq. (6)
is solved iteratively using the conjugate gradient algo-
rithm; for a detailed description of the implementation
the reader is referred to Ref. 25.
FIG. 1. An illustration of the modiﬁed λ-SOAP descriptors
and kernels. Environments Aiare deﬁned by a cutoﬀ radius
around a central atom i, labelled with a blue dot. The usual
SOAP environments ni(see Eq. (8)) are constructed from
a Gaussian function centred on every atom within the cut-
oﬀ radius. Our modiﬁed environments niβ(see Eq. (9)) add
a Gaussian corresponding to an additional “dummy” atom,
shown here in green, which encodes the direction of the ap-
plied ﬁeld in the descriptor. The λ-SOAP kernels are then
built from the symmetry-adapted overlap of these modiﬁed
environments, as in Eq. (7).
ThekeydiﬀerencebetweenSALTERandSALTEDlies
in the similarity kernel kσβ(Ai,Mj). In Refs. 24 and
25, kernels are constructed from the λ-SOAP represen-
tation of the atomic environments AiandMj.40This
captures the symmetry covariant transformations of the
spherical harmonics under rotation; the coeﬃcients must
also possess this symmetry since they expand auxiliary
basis functions whose angular parts are spherical har-
monics. As such, they are independent of the laboratory
frame. However, in the present case the similarity kernel
kσβ(Ai,Mj)must retain all of the properties exploited in
previous work, while also including the laboratory frame
direction along which the perturbing ﬁeld is applied.
Our solution is simple. An additional “dummy” atom
of atomic number zero is added to every atomic envi-
ronment on the βaxis, diﬀerentiating this axis from the
others. The kernels are then calculated using λ-SOAP
representations of these modiﬁed atomic environments in
the normal way. This dummy atom is then naturally in-
cluded in the kernels for all values of λ, ensuring that the
kernels retain all of the required symmetry properties,
while including (symmetry-adapted) information about
the laboratory frame direction of the applied ﬁeld. This
idea is illustrated in Fig. 1, and deﬁned mathematically
below.
The unmodiﬁed λ-SOAP kernel is deﬁned as40
kσ(Ai,Mj) =/integraldisplay
dˆRDλ(ˆR)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/integraldisplay
ni(r)nj(ˆRr)dr/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
,(7)
where the ﬁrst integral is performed over all rotations of
orderλ, andDλ(ˆR)is the Wigner matrix corresponding
to rotation ˆR. The atomic environment around atom i
is described as a sum of Gaussian functions of width ν
centredatthepositionofeveryatom Rjwhichlieswithin
some radius rcof the central atom:
ni(r) =/summationdisplay
jgν(r−Rj) Θ(rc−|Rj−Ri|).(8)3
Θ(x)is the Heaviside step function. To include informa-
tion about the direction of an applied ﬁeld, we include
a “dummy” atom in the atomic environment, adding an
additional term to Eq. (8):
niβ(r) =/summationdisplay
jgν(r−Rj)Θ(rc−|Rj−Ri|)+gζ(r−xˆrβrc).
(9)
Herexis a parameter between 0 and 1 specifying the po-
sition of the dummy atom and ˆrβis a unit vector in the
Cartesian direction β. Note that the width of the Gaus-
sian associated with the dummy atom is not required to
be the same as that associated with the physical atoms;
ingeneralζ/negationslash=ν. Theλ-SOAPkernel kσβwhichaccounts
for the direction of the applied ﬁeld is then obtained by
replacingni(r)withniβ(r)in Eq. (7).
Throughout this work, we deﬁne the % RMSE across
the datasets as
% RMSE
100=/radicaltp/radicalvertex/radicalvertex/radicalvertex/radicalbt1
N/summationtextN
A/integraltext
u.c.dr/vextendsingle/vextendsingle/vextendsingle˜ρ(1),ML
Aβ(r)−˜ρ(1),RI
Aβ(r)/vextendsingle/vextendsingle/vextendsingle2
1
N−1/summationtextN
A∆¯cT
Aβ∆¯w(1)
Aβ.
(10)
Here the sums run over all of the structures Ain the
training set, and the denominator is the standard devia-
tion of the density response across the training set, with
∆¯cA=cA−¯cand∆¯w(1)
A=w(1)
A−¯w(1);¯cand ¯w(1)are
the mean values for the vectors of coeﬃcients and density
projections in the data set.
In all of the following we used the FHI-aims software
package41to obtain the training data Sandw(1)
βand ref-
erencedensityresponses ρ(1),QM
β, usingdensityfunctional
theory42,43employing the LDA functional.42In all cases
“light” basis sets were used, and the auxiliary basis func-
tions introduced in Eq. (1) are constructed by taking all
possible products of these basis functions and then elimi-
nating linear dependencies from the resulting basis set.44
The molecular dynamics simulations were performed us-
ing the i-PI code.45We note that the “light” settings used
arenotthemostaccurateforthepredictionoftheseprop-
erties, but our purpose here is simply testing the quality
of the ML model, which can be achieved at this level of
accuracy. Increasing the model accuracy only requires
calculating new training data, which, as we shall see be-
low, is not very costly. The training and reference data
are available from the repositories listed in the SM.
First, wetacklethesimpleexampleofanisolatedwater
molecule. We learn the density response to an electric
ﬁeld for a dataset of 1000 distorted conﬁgurations taken
from Ref. 40, using both the usual kernels kσand the
modiﬁed kernels kσβ. Each molecule in the dataset is
aligned to a reference molecule, as shown in Fig. 2. We
use the SOAP parameters rc= 4.0Å,ν= 0.3Å, as
is commonly used to describe water,40and choseζ=ν
andx= 0.9to deﬁne the dummy atom. The choice of
these parameters will be discussed in more detail below,
and makes little diﬀerence to the results in this simple
Kernelx y z
kσ115.5 7.93 99.9
kσβ1.04 0.63 0.96
FIG. 2. Above: A visualisation of the 1000 water molecule
dataset in which all structures are overlayed. Each molecule
lies in thexyplane, with the O atom at the origin; the posi-
tions of the H atoms are varied around the equilibrium geom-
etry, which is highlighted in blue and whose centre of mass
lies on the yaxis. Below: The % RMSE across the testset
of 750 structures, using λ-SOAP kernels built using atomic
environments which exclude (ﬁrst row) and include (second
row) a dummy atom.
example. The dataset of was split into a training set of
250 and a testset of 750 structures; M= 300reference
environments were used.
The accuracy of SALTER density responses for this
simple dataset are summarised in the table in Fig. 2.
The ﬁrst row shows that while aligning the molecules
is suﬃcient to allow reasonably accurate learning of the
density response to a ﬁeld applied along the yaxis, the
learning entirely fails for ﬁelds applied along the xor
zaxis. This can be explained by symmetry: since the
molecules lie in the xyplane, the SOAP descriptors are
invariant to reﬂections in that plane. As a result, there
is no diﬀerentiation between a ﬁeld applied in the +z
and−zdirection, resulting in errors approaching 100%.
By contrast, once the dummy atom is introduced, this
ambiguity is removed; the direction along which the ﬁeld
is applied is clearly deﬁned. As a result, we see similar
errors regardless of the ﬁeld direction, which are around
1% in every case. Indeed, we also see an improvement in
the learning of the response to a ﬁeld in the ydirection
when using the modiﬁed kernels, since the applied ﬁeld
directionismademoreexplicitthansimplybeinginferred
from the aligned geometries.
This simple example also displays a feature of
SALTER. Since the molecules are aligned, the Cartesian
axes are nonequivalent. As a result, a separate machine-
learning model must be trained for each non-equivalent
direction. We argue that this additional cost is oﬀset
by the conceptual simplicity of the approach, the com-
putational simplicity of calculating the descriptors, and
that this drawback only applies in the speciﬁc case of
nonequivalent axes. We also stress that it is not neces-
sary to align systems for this approach to succeed; the
alignment here has a purely pedagogical purpose.
Having established that our approach works in princi-
ple, we turn to a more realistic example: a cubic wa-
ter cell of side 9.67 Å containing 32 water molecules.
Our dataset consists of 500 conﬁgurations, divided into a
training set of 400 structures and a test set of 100 struc-4
% RMSE
Training Set Size, Nx
y
z
481632
1 10 100
FIG. 3. Learning curves for the density response to a ﬁeld
along each Cartesian direction for the bulk water dataset,
M= 3000.
tures; this is the same dataset as used in Ref. 25. Be-
cause there are now physical atoms distributed through-
out each spherical atomic environment, it is not clear a
prioriwhere the dummy atom should be placed within
the environment, or what the optimal values for xandζ
in Eq. (9) would be. We performed a search of the pa-
rameterspaceusingasubsetof50structuresfromthefull
training set, ﬁnding the lowest errors when x= 0.3and
ζ= 1.0Å. Full details of this optimisation are provided
in the Supplementary Material.
The learning curves for the density response to a ﬁeld
applied along each Cartesian axis are shown in Fig. 3.
Learning improves monotonically with increasing train-
ing set size, with the error reaching a plateau of around
12% forN≥100. We have used M= 3000reference en-
vironments for these calculations (see convergence with
respect toMin the SM). As the Cartesian directions are
all equivalent, the machine learning model trained on the
density response to a ﬁeld applied along the xaxis is also
used to predict the response to a ﬁeld along the yand
zaxes; the errors observed are very similar regardless of
the ﬁeld direction, as would be expected by symmetry.
The learning curve for this dataset saturates at a rela-
tively large % RMSE: around three times larger than the
error in the predicted densities we reported in our previ-
ous work.25The reason for this could be a deﬁciency of
the descriptor, or a lack of quality in the training data.
Our descriptors are short ranged: any correlations in the
response of the density to a ﬁeld which are longer-ranged
than the 4 Å cutoﬀ radius of the atomic environment
will be lost, limiting the accuracy of predictions. Alter-
natively, more accurate predictions may be possible if the
training data was obtained using a larger basis set than
the “light” basis sets found in AIMS, since this both al-
lows a more accurate expansion of the density response,
and reduces the error in the training data which in turn
reduces the noise in the machine-learning model.
We then tested the performance of SALTER on a de-
rived quantity from the electronic response, namely theComponent xx yy zz xy xz yz
% RMSE 5.03 4.88 4.39 5.11 4.91 4.31
TABLE I. % RMSE in each component of the dielectric sus-
ceptibility derived from the density responses predicted by a
machine learning model trained using N= 400, M= 3000
for the bulk water test set.
dielectric susceptibility tensor. This quantity is related
to the polarizability of individual molecules and its ele-
ments are deﬁned by37
αγδ=−/integraldisplay
u.c.rγρ(1)
δ(r)dr. (11)
Sinceweareconsideringaperiodicsystem, whenevaluat-
ingEq.(11)weexploitthecommutationrelationbetween
the unperturbed Hamiltonian and the position operator
to avoid an ill-deﬁned result.37,38We denote the tensor
deﬁned by the density response found by a self-consistent
DFPT calculation as αQM, whileαMLis that derived
from the predicted density response, and the error in
αMLrelative toαQMprovides a measure of the accuracy
of the predicted density responses. Table I shows the
errors in diﬀerent elements αMLderived from the den-
sity response predicted by the machine learning model
withN= 400, M= 3000. In all cases errors are around
5%; thiscomparesfavourablytoprevious directmachine-
learning predictions of αML, which typically ﬁnd errors
>10%for models trained on a few hundred structures,
reducing to 5% only when trained on a few thousand
structures.20,46,47Therefore, for a given accuracy this in-
direct method of predicting αMLreduces the computa-
tional eﬀort required to generate training data by an or-
der of magnitude relative to a direct prediction of αML.
The smaller errors in αMLthan inρ(1),MLcan be un-
derstood from Eq. (11). If errors in the density response
accumulate in regions of space which do not contribute
signiﬁcantlytotheintegral, onewouldexpecttoseelower
errors inαMLthan the density response from which it is
derived. Indeed, we discussed a similar dependence of
the error in properties derived from electron density on
the spatial distribution of errors in the electron density
itself in Ref. 25.
Finally, we show the performance of SALTER on a
molecular crystal and on a quantity which is “twice re-
moved” from the target prediction of the model, namely
the vibrational Raman spectrum. We take the P2 1/a
naphthalene crystal at 80 K. We used ﬁve 10 ps NVE ab
initiomolecular dynamics trajectories of a 2×2×1crys-
tal supercell (8 molecules) using the PBE functional48
with many-body dispersion corrections.49,50To train the
machine learning model, we selected 100 structures from
each of these trajectories, dividing these structures into
a training set of 400 conﬁgurations and a test set of the
remaining 100. The learning curves for the density re-
sponse to a ﬁeld applied along each Cartesian direction
are shown in Fig. 4. These curves are converged with re-
spect toM, as demonstrated in the Supplementary Ma-5
% RMSE
Training Set Size, Nx
y
z
24816
1 10 100
FIG. 4. Learning curves for the density response to a ﬁeld
along each Cartesian direction for the naphthalene test set,
M= 2000.
Component xx yy zz xy xz yz
% RMSE 9.13 14.40 19.74 9.38 12.81 20.11
TABLE II. % RMSE in each component of the dielectric
susceptibility derived from the density responses predicted by
machine learning models trained using N= 400, M= 3000
for the naphthalene test set.
terial, and we see no signiﬁcant diﬀerence in performance
whenpredictingtheresponsetoaﬁeldappliedalongeach
Cartesian axis. We note that the dataset is relatively
homogeneous: a single training structure is suﬃcient to
produce a ML model accurate to within 12%, and no im-
provement is seen when the training set size is increased
beyond 100.
We then used the predicted density responses to calcu-
lateαMLfor each structure in the test set. Curiously, as
shown in Table II, the errors are signiﬁcantly larger than
those found in bulk water (Table I), despite the % RMSE
of the density response itself being signiﬁcantly lower for
naphthalene. Furthermore, the errors in the components
ofαMLderived from a response to a ﬁeld applied in the
zdirection are the largest, despite the errors in this re-
sponse being the lowest (see crystal cell orientation in
the inset of Fig. 4). This counter-intuitive result is again
due to the fact that the errors in αMLdepend not simply
on the magnitude of the error but also its spatial distri-
bution. Speciﬁcally, for a given magnitude of error, the
greater the distance between the regions of positive and
negative error, the greater the error in the dielectric sus-
ceptibility derived from the density response – and this
is what we observe along the zdirection in this crystal.
This is illustrated in more detail in the SM.
We compared a Raman spectrum obtained using αQM
with one obtained indirectly using SALTER predictions
ofρ(1),MLto obtainαML. The anharmonic Raman
spectrum of a disordered naphthalene sample (the pow-
der spectrum) was obtained through ensemble-averaged
time-correlation functions, as detailed in Ref. 27. The
spectra are shown in Fig. 5. Good agreement is seen at
Intensity (arb. units)
Frequency (cm−1)QM
ML
00.20.40.60.81
0 300 600 900 1200 1500FIG. 5. The powder Raman spectrum of the P2 1/a naphtha-
lene crystal at 80 K, calculated from time-correlation func-
tions of dielectric susceptibilities ab initio (QM) and via an
indirect SALTER prediction.
the polymorph-sensitive low frequency modes, with small
discrepancies in the peak intensities appearing at higher
frequencies. Therefore, by applying SALTER, we obtain
Ramanspectrumofcomparablequalitytothoseobtained
using ab initio methods and to those obtained from a di-
rect learning of αQM,20while dramatically reducing the
simulation cost and retaining access to all the informa-
tion included in the real-space distribution of ρ(1)(r).
Inconclusion,wepresentedaconceptuallysimplemod-
iﬁcation to the widely used λ-SOAP descriptors which al-
lows us to use machine learning to predict a vector ﬁeld;
speciﬁcally,theresponseoftheelectrondensitytoastatic
electricﬁeld. Byaddinga“dummy” atomwhichindicates
the direction of the applied ﬁeld to each atomic environ-
ment, we are able to retain all of the beneﬁcial proper-
ties of the hierarchy of λ-SOAP kernels, which allowed
us to predict the scalar electron density ﬁeld in previous
work,24,25while including (symmetry-adapted) informa-
tion about the laboratory frame direction of the applied
ﬁeld. As a result, we expect the method developed here
to be straightforward to generalise to the prediction of
other vector ﬁelds. To the best of our knowledge this
is the ﬁrst reported machine learning model capable of
predicting vector ﬁelds in a chemical context.
We applied SALTER to two condensed phase systems:
liquid water and the P2 1/a naphthalene crystal. As ex-
pected for a disordered system, a single machine-learning
model successfully predicted the density response of wa-
ter to a ﬁeld applied along each of the Cartesian axes; we
derived the dielectric susceptibility of water from these
predictions, ﬁnding values within 5% of the reference
DFPT calculations. For naphthalene, we constructed
machine learning models of similar accuracy regardless
of the direction of the applied ﬁeld. We found a counter-
intuitive relationship between the errors in the density
response, which were smallest when the ﬁeld was applied
along thezaxis, and the errors in the corresponding di-
electric susceptibilities, which were largest for the yzand
zzcomponent of the tensor. This apparent contradiction6
can be explained by analysing the spatial distribution of
the error in the predicted density response. Nevertheless,
the Raman powder spectrum obtained from these indi-
rectly predicted dielectric susceptibilities was very simi-
lar to that obtained using DFPT, and was obtained at
a fraction of the computational cost. In general, this
method produces accurate derived quantities while re-
tainingthefullinformationcontainedintheelectronden-
sity response to an applied ﬁeld.
I. ACKNOWLEDGEMENTS
A.M.L. acknowledges partial support from the Alexan-
der von Humboldt Foundation for this work. P.L and
M.R.acknowledgesupportthroughtheLiseMeitnerPro-
gram of the Max Planck Society and the UFAST Inter-
national Max Planck Research School.
II. DATA AVAILABILITY
The machine learning datasets, along with the MD
trajectories and associated polarizabilities used to calcu-
late the reference Raman powder spectrum, can be found
atgithub.com/sabia-group/SALTER-examples . In ad-
dition, the naphthalene training data can be found in
the NOMAD repository dx.doi.org/10.17172/NOMAD/
2023.04.13-1 . The code to build similarity kernels us-
ing the modiﬁed λ-SOAP descriptors can be found at
github.com/alanmlewis/TENSOAP .
1K. T. Butler, D. W. Davies, H. Cartwright, O. Isayev, and
A. Walsh, “Machine learning for molecular and materials sci-
ence,” Nature 2018 559:7715 559, 547–555 (2018).
2M. Ceriotti, “Unsupervised machine learning in atomistic simu-
lations, between predictions and understanding,” The Journal of
Chemical Physics 150, 150901 (2019), arxiv:1902.05158.
3G. Carleo, I. Cirac, K. Cranmer, L. Daudet, M. Schuld,
N. Tishby, L. Vogt-Maranto, and L. Zdeborová, “Machine learn-
ing and the physical sciences,” Reviews of Modern Physics 91,
045002 (2019), arxiv:1903.10563.
4V.L.Deringer, A.P.Bartók, N.Bernstein, D.M.Wilkins, M.Ce-
riotti, andG.Csányi,“GaussianProcessRegressionforMaterials
and Molecules,” Chemical Reviews 121, 10073–10141 (2021).
5A. P. Thompson, L. P. Swiler, C. R. Trott, S. M. Foiles, and
G. J. Tucker, “Spectral neighbor analysis method for automated
generation of quantum-accurate interatomic potentials,” Journal
of Computational Physics 285, 316–330 (2015).
6A. Kamath, R. A. Vargas-Hernández, R. V. Krems, T. Carring-
ton, and S. Manzhos, “Neural networks vs Gaussian process
regression for representing potential energy surfaces: A compar-
ativestudyofﬁtqualityandvibrationalspectrumaccuracy,” The
Journal of Chemical Physics 148, 241702 (2018).
7R. Drautz, “Atomic cluster expansion for accurate and trans-
ferable interatomic potentials,” Physical Review B 99, 014104
(2019).
8V. L. Deringer, M. A. Caro, G. V. Csányi L Deringer, G. Csányi,
V. L. Deringer, and M. A. Caro, “Machine Learning Interatomic
Potentials as Emerging Tools for Materials Science,” Advanced
Materials 31, 1902765 (2019).
9J. Behler, “Four Generations of High-Dimensional Neural Net-
work Potentials,” Chemical Reviews 121, 10037–10072 (2021).10F.Noé, A.Tkatchenko, K.R.Müller, andC.Clementi,“Machine
Learning for Molecular Simulation,” Annual Review of Physical
Chemistry 71, 361–390 (2020), arxiv:1911.02792.
11P. Friederich, F. Häse, J. Proppe, and A. Aspuru-Guzik,
“Machine-learned potentials for next-generation matter simula-
tions,” Nature Materials 20, 750–761 (2021).
12O. T. Unke, S. Chmiela, H. E. Sauceda, M. Gastegger,
I. Poltavsky, K. T. Schütt, A. Tkatchenko, and K. R. Müller,
“Machine Learning Force Fields,” Chemical Reviews 121, 10142–
10186 (2021), arxiv:2010.07067.
13A. Musaelian, S. Batzner, A. Johansson, L. Sun, C. J. Owen,
M. Kornbluth, and B. Kozinsky, “Learning local equivariant rep-
resentations for large-scale atomistic dynamics,” Nature Commu-
nications 14, 579 (2023).
14G. C. Sosso, D. Donadio, S. Caravati, J. Behler, and
M. Bernasconi, “Thermal transport in phase-change materi-
als from atomistic simulations,” Physical Review B 86, 104301
(2012).
15V. Botu, R. Batra, J. Chapman, and R. Ramprasad, “Ma-
chine learning force ﬁelds: Construction, validation, and out-
look,” Journal of Physical Chemistry C 121, 511–522 (2017),
arxiv:1610.02098.
16F. Maresca, D. Dragoni, G. Csányi, N. Marzari, and W. A.
Curtin, “Screw dislocation structure and mobility in body cen-
tered cubic Fe predicted by a Gaussian Approximation Poten-
tial,” npj Computational Materials 2018 4:1 4, 1–7 (2018).
17V. L. Deringer, N. Bernstein, A. P. Bartók, M. J. Cliﬀe,
R. N. Kerber, L. E. Marbella, C. P. Grey, S. R. Elliott, and
G. Csányi, “Realistic Atomistic Structure of Amorphous Silicon
from Machine-Learning-Driven Molecular Dynamics,” Journal of
Physical Chemistry Letters 9, 2879–2885 (2018).
18M. G. Darley, C. M. Handley, and P. L. A. Popelier, “Beyond
Point Charges: Dynamic Polarization from Neural Net Predicted
Multipole Moments,” Journal of Chemical Theory and Compu-
tation 4, 1435–1448 (2008).
19O. T. Unke and M. Meuwly, “PhysNet: A Neural Network
for Predicting Energies, Forces, Dipole Moments, and Partial
Charges,” Journal of Chemical Theory and Computation 15,
3678–3693 (2019).
20N. Raimbault, A. Grisaﬁ, M. Ceriotti, and M. Rossi, “Using
Gaussian process regression to simulate the vibrational Raman
spectraofmolecularcrystals,” NewJournalofPhysics 21,105001
(2019), arxiv:1906.07485.
21M. Veit, D. M. Wilkins, Y. Yang, R. A. DiStasio, and M. Ceri-
otti, “Predicting molecular dipole moments by combining atomic
partial charges and atomic dipoles,” The Journal of Chemical
Physics 153, 024113 (2020).
22G. A. Pinheiro, J. Mucelini, M. D. Soares, R. C. Prati, J. L. F.
Da Silva, and M. G. Quiles, “Machine Learning Prediction of
Nine Molecular Properties Based on the SMILES Representa-
tion of the QM9 Quantum-Chemistry Dataset,” The Journal of
Physical Chemistry A 124, 9854–9866 (2020).
23J. Sun, L. Cheng, and T. F. Miller, “Molecular dipole mo-
ment learning via rotationally equivariant derivative kernels
in molecular-orbital-based machine learning,” The Journal of
Chemical Physics 157, 104109 (2022).
24A. M. Lewis, A. Grisaﬁ, M. Ceriotti, and M. Rossi, “Learn-
ing Electron Densities in the Condensed Phase,” Journal of
Chemical Theory and Computation 17, 7203–7214 (2021),
arxiv:2106.05364.
25A. Grisaﬁ, A. M. Lewis, M. Rossi, and M. Ceriotti, “Electronic-
Structure Properties from Atom-Centered Predictions of the
Electron Density,” Journal of Chemical Theory and Computa-
tion (2022), 10.1021/ACS.JCTC.2C00850, arxiv:2206.14087.
26P. J. Hendra and P. M. Stratton, “Laser-raman spectroscopy,”
Chemical Reviews 69, 325–344 (1969).
27D. A. McQuarrie, Statistical Mechanics (Harper Collins, New
York, 1976).
28A. Morita and J. T. Hynes, “A theoretical analysis of the sum
frequency generation spectrum of the water surface,” Chemical7
Physics 258, 371–390 (2000).
29A. Morita and J. T. Hynes, “A theoretical analysis of the sum
frequency generation spectrum of the water surface. II. Time-
dependentapproach,” JournalofPhysicalChemistryB 106,673–
685 (2002).
30B. M. Auer and J. L. Skinner, “IR and Raman spectra of liquid
water: Theory and interpretation,” Journal of Chemical Physics
128(2008), 10.1063/1.2925258.
31J.Lee,K.T.Crampton,N.Tallarida, andV.A.Apkarian,“Visu-
alizing vibrational normal modes of a single molecule with atom-
ically conﬁned light,” Nature 2019 568:7750 568, 78–82 (2019).
32N.Raimbault,V.Athavale, andM.Rossi,“Anharmoniceﬀectsin
the low-frequency vibrational modes of aspirin and paracetamol
crystals,” Physical Review Materials 3, 053605 (2019).
33T. Sohier, M. Calandra, and F. Mauri, “Density-functional cal-
culation of static screening in two-dimensional materials: The
long-wavelength dielectric function of graphene,” Physical Re-
view B 91, 165428 (2015).
34P. Cudazzo, I. V. Tokatly, and A. Rubio, “Dielectric screening in
two-dimensional insulators: Implications for excitonic and impu-
rity states in graphane,” Physical Review B 84, 085406 (2011).
35J. H. Skone, M. Govoni, and G. Galli, “Self-consistent hybrid
functional for condensed systems,” Physical Review B 89, 195112
(2014).
36N. P. Brawand, M. Vörös, M. Govoni, and G. Galli, “Gener-
alization of Dielectric-Dependent Hybrid Functionals to Finite
Systems,” Physical Review X 6, 041002 (2016).
37H. Shang, N. Raimbault, P. Rinke, M. Scheﬄer, M. Rossi, and
C. Carbogno, “All-Electron, Real-Space Perturbation Theory
for Homogeneous Electric Fields: Theory, Implementation, and
Application within DFT,” New Journal of Physics 20, 073040
(2018), arxiv:1803.00924.
38S. Baroni, S. de Gironcoli, and A. Dal Corso, “Phonons and
related crystal properties from density-functional perturbation
theory,” Reviews of Modern Physics 73, 515 (2001), arxiv:hep-
ph/9702376.
39X. Andrade, S. Botti, M. A. Marques, and A. Rubio, “Time-
dependent density functional theory scheme for eﬃcient calcula-
tions of dynamic (hyper)polarizabilities,” The Journal of Chem-
ical Physics 126, 184106 (2007), arxiv:cond-mat/0701632.
40A. Grisaﬁ, D. M. Wilkins, G. Csányi, and M. Ceriotti,
“Symmetry-Adapted Machine Learning for Tensorial Proper-ties of Atomistic Systems,” Physical Review Letters 120, 36002
(2018), arxiv:1709.06757.
41V. Blum, R. Gehrke, F. Hanke, P. Havu, V. Havu, X. Ren,
K. Reuter, and M. Scheﬄer, “Ab initio molecular simulations
with numeric atom-centered orbitals,” Computer Physics Com-
munications 180, 2175 (2009).
42P. Hohenberg and W. Kohn, “Inhomogeneous Electron Gas,”
Physical Review 136, B864 (1964).
43W. Kohn and L. J. Sham, “Self-Consistent Equations Including
Exchange and Correlation Eﬀects,” Physical Review 140, A1133
(1965).
44X. Ren, P. Rinke, V. Blum, J. Wieferink, A. Tkatchenko, A. San-
ﬁlippo, K. Reuter, and M. Scheﬄer, “Resolution-of-identity ap-
proach to Hartree-Fock, hybrid density functionals, RPA, MP2
and GW with numeric atom-centered orbital basis functions,”
New Journal of Physics 14, 053020 (2012), arxiv:1201.0655.
45V. Kapil, M. Rossi, O. Marsalek, R. Petraglia, Y. Litman,
T. Spura, B. Cheng, A. Cuzzocrea, R. Meißner, D. M. Wilkins,
P. Juda, S. P. Bienvenue, W. Fang, J. Kessler, I. Poltavsky,
S.Vandenbrande, J.Wieme, C.Corminboeuf, T.D.Kühne, D.E.
Manolopoulos,T.E.Markland,J.O.Richardson,A.Tkatchenko,
G. A. Tribello, V. V. Speybroeck, and M. Ceriotti, “I-PI 2.0:
A Universal Force Engine for Advanced Molecular Simulations,”
Computer Physics Communications 236, 214–223 (2019).
46D. M. Wilkins, A. Grisaﬁ, Y. Yang, K. U. Lao, R. A. DiSta-
sio, and M. Ceriotti, “Accurate molecular polarizabilities with
coupled cluster theory and machine learning,” Proceedings of the
National Academy of Sciences of the United States of America
116, 3401–3406 (2019), arxiv:1809.05337.
47H. Shang and H. Wang, “Anharmonic Raman spectra simulation
of crystals from deep neural networks,” AIP Advances 11, 035105
(2021).
48J.P.Perdew, K.Burke, andM.Ernzerhof,“Generalizedgradient
approximation made simple,” Physical Review Letters 77, 3865–
3868 (1996).
49A. Tkatchenko, R. A. DiStasio, R. Car, and M. Scheﬄer, “Ac-
curate and Eﬃcient Method for Many-Body van der Waals In-
teractions,” Physical Review Letters 108, 236402 (2012).
50A. Ambrosetti, A. M. Reilly, R. A. DiStasio, and A. Tkatchenko,
“Long-range correlation energy calculated from coupled atomic
response functions,” The Journal of Chemical Physics 140,
18A508 (2014).Supplementary Material: Predicting the Electronic Density Response of
Condensed-Phase Systems to Electric Field Perturbations
Alan M. Lewis, Paolo Lazzaroni, and Mariana Rossi
Max Planck Institute for the Structure and Dynamics of Matter,
Luruper Chaussee 149, 22761 Hamburg, Germany
(*mariana.rossi@mpsd.mpg.de)
(*alan.lewis@mpsd.mpg.de)
1arXiv:2304.09057v1  [physics.chem-ph]  18 Apr 2023I. MACHINE LEARNING PARAMETERS FOR BULK WATER AND NAPHTHALENE
In bulk water, there are physical atoms distributed throughout each spherical atomic environ-
mentAi. As such, it is not clear a priori where the dummy atom should be placed within this
environment, or what the optimal width of the associated Gaussian should be. To determine these
values (xandζin Eq. (9) in the main text), we took a subset of 50 structures from the full training
set, selected 40 of those to form a training set and used the remainder as the test set, and calculated
the % RMSE error as a function of xandζ, usingM= 200. Our results are shown in Fig. 1; we
found that the error decreased monotonically with increasing ζat all values of x, approaching an
asymptote at large values, while for large values of ζwe found a broad minimum in xcentred at
around 0.3. Therefore, x= 0.3andζ= 1.0Å deﬁne the dummy atom in our calculations. Since
we did not observe a deep minimum in these results, we felt conﬁdent using the same parameters
for naphthalene without repeating this optimisation procedure.
The learning curves for the density response to a ﬁeld applied along the xaxis are shown in
Fig. 2, using the full dataset. As can be seen, learning improves monotonically with increasing
training set size, with the error reaching a plateau for N≥100. The value of this plateau is
reduced as the number of reference environments is increased, converging at M= 3000with errors
just below 12%. The equivalent learning curves for naphthalene are shown in Fig. 3, showing similar
behaviour, with errors converging for M= 2000at around 6%.
FIG. 1. The % RMSE in the density response to a ﬁeld applied along the xaxis for a test set of 10
water conﬁgurations, as a function of the parameters xandζwhich deﬁne the position and Gaussian width
associated with the dummy atom added to the atomic environment. In all cases the machine learning model
is trained with N= 40,M= 200.
2% RMSE
Training Set Size, NM=1000
M=2000
M=3000
481632
1 10 100FIG. 2. Learning curves for the predicted density response to an applied ﬁeld along the xaxis for the full
bulk water dataset, converging with respect to M.
% RMSE
Training Set Size, NM=500
M=1000
M=2000
24816
1 10 100
FIG. 3. Learning curves for the predicted density response to an applied ﬁeld along the xaxis for the
naphthalene dataset, converging with respect to M.
3II. EXPLAINING THE ERRORS IN THE DIELECTRIC SUSCEPTIBILITY OF NAPH-
THALENE
As noted in the main text, we observe a counter-intuitive relationship between the error in the
predicted density response of naphthalene and the error in the dielectric susceptibility derived from
that predicted response. Speciﬁcally, we observe the lowest errors in the predicted response to a
ﬁeld applied along the zaxis, but the largest errors in the dielectric susceptibility tensor for the
components derived from this response; the reverse is true for the predicted response to a ﬁeld
applied along the xaxis. This apparent paradox can be explained in general terms by noting that
the errors in properties derived from the predicted density response depend on the distribution of
the error of the density response, as well as its overall magnitude. We noted this phenomenon
in our previous work predicting the electron density – in that case we found that errors in the
electrostatic energy derived from the predicted density were particularly sensitive to errors in the
electron density located close to the nuclei.[1]
The errors in the predicted density response to a ﬁeld applied along the xandzaxes of a
representative naphthalene conﬁguration are shown in Figure 4. In both cases the error in the
density is localised primarily on the surfaces of the molecule normal to the applied ﬁeld, with a
region of positive error on one surface and of negative error on the opposing surface. Since the
surface area of the molecule is largest normal to the xaxis, we observe a larger overall error in the
density response when the ﬁeld is applied along the xaxis than when it is applied along the zaxis.
However, from Eq. (11) in the main text it can be shown that for a given magnitude of error in
the density response, the magnitude of the corresponding error in the polarizability is determined
by the separation between the regions of positive and negative error in the density response. To
illustrate this, we imagine a simple description of an error in the density response:
˜ρ(1)
β(r) =ρ(1)
β(r) +εδ(r/prime)−εδ(r/prime/prime). (1)
Here ˜ρ(1)
β(r)is an approximate density response, ρ(1)
β(r)is the exact density response, εis the
magnitude of the error found at two points in space r/primeandr/prime/prime, andδ(r)is the Dirac delta function.
There are two error terms of equal and opposite magnitude since the approximate density response
must integrate to zero. The error in the polarizability arising from this error is given by
˜αγβ−αγβ=−/integraldisplay
u.c.rγ/parenleftBig
˜ρ(1)
β(r)−ρ(1)
β(r)/parenrightBig
dr
=/integraldisplay
u.c.rγ/parenleftbig
εδ(r/prime)−εδ(r/prime/prime)/parenrightbig
dr
=ε/parenleftbig
r/prime
γ−r/prime/prime
γ/parenrightbig
.(2)
4FIG. 4. Isosurfaces at ±0.01 VÅ−2of the error in the predicted density response to a ﬁeld applied along
thexaxis (left) and zaxis (right) for a particular conﬁguration of naphthalene. Positive errors are shown
in blue, negative errors in red.
Therefore, for a ﬁxed value of the error ε, then the greater the spatial separation in the γdirection
between these regions of error, the larger the error in the polarizability and dielectric susceptibility.
Since the long axis of naphthalene lies approximately along close to the zaxis, the separation
between these regions is largest when a ﬁeld is applied along the zaxis, and we therefore see much
larger errors in the corresponding components of the dielectric susceptibility, despite the fact that
the actual magnitude of the error in the density response is lowest for this ﬁeld direction.
[1] A. Grisaﬁ, A. M. Lewis, M. Rossi, and M. Ceriotti, Journal of Chemical Theory and Computation
(2022), 10.1021/ACS.JCTC.2C00850, arxiv:2206.14087.
5