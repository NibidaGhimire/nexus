arXiv:2305.04312v4  [cs.AI]  2 Aug 2023The Human-or-Machine Matter:
Turing-Inspired Reﬂections on an Everyday Issue
David Harel and Assaf Marron
Department of Computer Science and Applied Mathematics, We izmann Institute of Science,
Rehovot, 76100, Israel
dharel@weizmann.ac.il, assaf.marron@weizmann.ac.il
Keywords:
Turing test, conversational agents, chatbots, autonomous systems, artiﬁcial intelligence, robots,
interaction, ﬁrst impression, compassion, humanness, Hum an-or-Machine.
Abstract:
In his seminal paper “Computing Machinery and Intelligence ”, Alan Turing introduced the “imi-
tation game” as part of exploring the concept of machine inte lligence. The Turing Test has since
been the subject of much analysis, debate, reﬁnement and ext ension. Here we sidestep the ques-
tion of whether a particular machine can be labeled intellig ent, or can be said to match human
capabilities in a given context. Instead, we ﬁrst draw atten tion to the seemingly simpler question
a person may ask themselves in an everyday interaction: “Am I interacting with a human or with
a machine?” . We then shift the focus from seeking a method for eliciting the answer, and, rather,
reﬂect upon the importance and signiﬁcance of this Human-or -Machine question and the use one
may make of a reliable answer thereto. Whereas Turing’s orig inal test is widely considered to
be more of a thought experiment, the Human-or-Machine matte r as discussed here has obvious
practical relevance. While it is still unclear if and when ma chines will be able to mimic human
behavior with high ﬁdelity in everyday contexts, we argue th at near-term exploration of the issues
raised here can contribute to reﬁnement of methods for devel oping computerized systems, and may
also lead to new insights into fundamental characteristics of human behavior.
1 Introduction
Turing’s 1950 paper (Turing, 1950) introduced
the famed “imitation game” as part of the
deﬁnition of the concept of machine intelli-
gence; i.e., as a means to determine whether a
computer can be labeled intelligent. Over the
years, the Turing test has been the subject of
much work, and has resulted in several vari-
ants (French, 2000; Hoﬀmann, 2022). Similar
tests have also been proposed in quite dif-
ferent contexts (Hagen-Zanker et al., 2023;
Harel, 2005; Beven et al., 2022;
Harel, 2016; Kalik and Prokhorov, 2007;
Rosemarin and Rosenfeld, 2019;
Bush et al., 2020).
Here, we completely sidestep the conceptual,
philosophical issue of deﬁning or measuring in-
telligence, as well as the practical question of
whether a machine can be built to replace, ormimic, a person in the performance of some spe-
ciﬁc task (Sifakis, 2023).
Instead, we look more broadly at a concept
that we term The Human-or-Machine Matter . In
a future world, when in some interactions ma-
chines will be able to mimic humans impressively,
new social, psychological, functional and techni-
cal issues are bound to become relevant. For ex-
ample:
1. Will humans care whether the agent they in-
teract with is a human or a machine, and if
yes, why?
2. How will a person’s behavior or emotional
state diﬀer between interactions with another
human and interactions with a machine whose
behavior is indistinguishable from a human’s?
3. How will the answer to the question of an
agent’s human-or-machine identity (hereafter,
theH-or-M question ) be elicited?4. Will human language and social practices
change in such cases?
5. Will machine-machine interactions change
when the behavior of one or both of the par-
ticipants is very close to a human’s?
6. will machines indeed be indistinguishable
from humans, or will this be a non-issue be-
cause openly taking advantage of machine ca-
pabilities will be prioritized over manifesting
human-like behavior?
In examining these issues, we discuss research,
opinions and predictions about diﬀerences be-
tween humans and machines, and diﬀerences be-
tween human-human and human-machine inter-
actions.
Of special interest here is the importance of
theHuman-or-Machine question , which stands
for the interest, or curiosity, of a person who is
engaged in an everyday interaction with an agent,
and is wondering whether the agent is a human
or a machine.
While in general, machines are at present
unable to perform non-trivial interactive tasks
in a way that conceals the fact that they are
machines and not humans, we expect that in
the future this will change dramatically. This
will be supported by a combination of the
growing presence of of machines performing
functions that were traditionally carried out by
humans, as in service centers with automated
chatbots, healthcare conversational agents,
service robots in stores, autonomous vehi-
cles (Sheth et al., 2019; Parmar et al., 2022;
Frank and Otterbring, 2023; BBC, 2021;
Sadeghian and Hassenzahl, 2022), and the
prevalence of interactions that hide the agent
identity as in text-only or voice-only interactions,
or when not being able to see the driver of
another vehicle.
Even when the agent with whom we are inter-
acting is known to be a human, an appropriate
variant of the question is still relevant, since that
human’s behavior may be dictated by a machine,
as in a person reading aloud some text composed
by a machine or driving a car by merely follow-
ing computer-generated operation and navigation
instructions.
Similarly, when the agent is conspicuously a
machine, the H-or-M question asks who is behind
thedecisions orchoices underlying the agent’s
role in the interaction. For example, when the
agent displays a piece of text as part of an inter-
action, we want to know if the act of composingthe text and the decision to display it right now
were carried out by a human or by the agent.
If one insists, the original Turing test and its
many variants can be viewed as a special case
of dealing with the H-or-M question, where, in a
controlled laboratory setting, a human acts as an
interrogator in attempting to reveal the human-
or-machine identity of an unidentiﬁed and hidden
agent, based on observing the agent’s responses
and actions to his or her prodding questions. Our
inspiration from the Turing test here is mani-
fested in our focus on conﬁned human-agent inter-
actions, rather than on the broader issues of the
role of new human-like machines in the world, or
on forensic issues like whether a stand-alone, non-
interactive artifact, like a piece of text, a picture,
a video clip, or a medical diagnosis, was created
by a person or a machine (Noll, 1966). Still, here
we diverge from classical treatments of the Turing
test in that: (i) we concentrate on everyday in-
teractions, rather than on a controlled lab setup;
(ii) we are interested in how the agent’s H-or-M
identity aﬀects the interaction at hand; (iii) the
question of whether it is possible at all to extract
the agent’s H-or-M identity from a sequence of ex-
changes is secondary, and we only brieﬂy discuss
here issues related to interrogation techniques for
extracting this answer. (iv) we are not interested
in the question of whether the agent can be la-
beled as being intelligent.
Parts of our discussion are presented as ques-
tions, some of which may justify separate, focused
research eﬀorts by scientists, philosophers, etc.
The Human-or-Machine issue is presented
here as a binary one. Clearly, there may be mixed
modes. For example: (i) the apparent agent is a
human physician, who, while consulting a human
patient, relies extensively on online search for in-
formation or is informed, openly or discreetly, by
an automated agent listening in on the conversa-
tion; (ii) the apparent agent is a vehicle, but while
in many aspects it acts autonomously, it is also
remotely supervised and occasionally even con-
trolled by humans (and we are interested in this
vehicle’s interactions with other road users); (iii)
setups like the above two, but the mixed-mode
agent depends on more than one machine and/or
more than one human; (iv) extending the above
one-to-one human-agent interactions to group in-
teractions, and potentially even without a clear
delineation of “the agent” . Such mixed-mode
agents may be treated by default in the same way
as a pure machine or a pure human. We defer
to future work discussion of the cases where the
2mixed mode is substantially diﬀerent from the bi-
nary one.
2 Are We Diﬀerent When We
Interact with Machines?
One kind of relevance the H-or-M question might
have, is to the way in which knowing the answer
could aﬀect human behavior.
The relationship between actual humans and
machines that present themselves as almost
human, has been explored in a variety of ways
in arts, science and philosophy. Suﬃces to
consider movies like “The Matrix”, “Blade
Runner”, “The Terminator” series, and “Her”,
and books like “Machines Like Me”, and “I,
Robot”1. Scientists have also researched human-
machine relations (Chaturvedi et al., 2023;
Reinkemeier and Gnewuch, 2022;
Milcent et al., 2022; Hindriks et al., 2022;
Heiser et al., 1979) and proposed that the
science of sociology should study AI-related
issues (Woolgar, 1985; Liu, 2021).
There are many studies of human interac-
tions with chatbots—text-based conversational
agents; see, for example, literature reviews
in (Rapp et al., 2021; Chaturvedi et al., 2023;
Nicolescu and Tudorache, 2022;
Van Pinxteren et al., 2020; Mariani et al., 2023)
and references therein. Research themes include
the analysis of chatbot functionality and its
relationship to certain success factors, such as
the ability to aﬀect user actions; aspects of the
interactions, such as the language used or length
of conversation; and human-chatbot relations,
such as acceptance and trust. Studies that focus
speciﬁcally on the diﬀerences between humans
and human-like machines in normal kinds of in-
teraction contexts (like a service robot in a store)
are also emerging(Frank and Otterbring, 2023).
In most of that work the H-or-M question itself
is not at the center of the research. In many,
the fact that the agent is a machine is disclosed
up front, and in others the researchers were
interested in whether or not the human users
ascribed humanness, or human-like behavior, to
a machine agent.
1Excluded from this paper is a comprehensive
summary of how these movies and books present
the relations and interactions between humans and
human-like machine agents, which we were able to
readily obtain with a few queries to OpenAI’s Chat-
GPT.Below are some examples of person-agent in-
teractions, where having an answer may aﬀect the
person’s behavior, even in everyday routine in-
teractions. In this discussion, one should note,
though, that while validating or refuting each
such candidate eﬀect on the user’s actions or emo-
tional state is an intriguing issue, the examples
appear here only to support the main claim of
the paper; i.e., that the H-or-M matter will fast
become relevant in many everyday contexts.
Some languages require distinguishing humans
from non-human agents, and in the case of a hu-
man agent often also detecting the gender. A
person conducting a text exchange with a ser-
vice center may be inclined to use diﬀerent pro-
nouns or verbs for humans and for machines, both
when addressing the representative and when dis-
cussing what another representative may have
communicated in a prior exchange. Furthermore,
special linguistic patterns may evolve for cases
where such a determination remains unknown.
In order to plan his or her next steps, a person
is more likely to try to ﬁgure out particular pat-
terns in the behavior of the conversing agent if the
agent were known to be a machine, rather than
a human, and to make more of an eﬀort to re-
late to those patterns. One reason for this is that
machines are expected to follow patterns more
consistently than humans. Also, when we know
that we are interacting with a machine, we may
be able to ﬁnd out more information about the
machine, like its make and model, the software
controlling it, etc., and then use shared knowledge
about what to expect or how to interact with this
particular type of agent.
As summarized in (Rapp et al., 2021), some
research on human-chatbot interactions as com-
pared with human-human ones suggests that
when interacting with a machine the human
may be briefer and less polite, more inclined to
abruptly stop or divert a conversation, or even
employ profanities (Hill et al., 2015). It is not
clear if this is because we do not expect the ma-
chine to be really oﬀended, or because we are not
concerned that our impolite behavior might bring
about repercussions.
One may wonder if we will be more accepting
of a machine agent’s formal, dry, or even rude at-
titude, knowing that machines will not normally
be considerate. Similarly, will people be more pa-
tient with "stupid" or repeated answers, or with
inconsiderate actions, such as when driving be-
hind an overly cautious and slow autonomous ve-
hicle (AV), knowing that machines are limited,
3and their behavior cannot be readily changed?
We expect that people will be less patient when
experiencing delayed responses, knowing that ma-
chines are usually fast and task oriented.
The present emphasis on prompt writing and
prompt engineering skills for interacting with
Large Language Models (LLMs) suggests that we
will make a stronger eﬀort to explain ourselves,
knowing that a machine is expected to be more
limited than a human in understanding our inten-
tions and needs. Also, one can expect that we will
be more inclined to report a machine’s dangerous
or undesired behavior to the machine itself, or
to its owner or manufacturer, or even to the po-
lice, seeing our actions as desired feedback and
expecting more productive reactions than when
criticizing or reporting a person.
The issues of trust building, the will-
ingness to disclose personal information,
developing a personal relationship with and
feeling empathy towards machine agents,
have all been discussed in the litera-
ture (Rapp et al., 2021; Chaturvedi et al., 2023;
Van Pinxteren et al., 2020; Mariani et al., 2023;
Cai et al., 2022). Some research shows diﬃcul-
ties in these areas, which may be partly related
to the agents being perceived as uncanny. Other
research has shown a much “warmer” attitude
by users. It is yet to be determined how this
aspect will evolve as the technology advances
and becomes more pervasive.
Will we be more open to learning from the
machine’s behavior? Consider for example the
following: When observing an AV negotiating a
certain class of complicated scenarios diﬀerently
from the way we would have dealt with it, we
might be inclined to mimic the AV, assuming that
much thought and serious design and testing had
been carried out to yield such behavior. However,
even that is far from obvious, and we might ac-
tually do quite the opposite: unlike the natural
tendency to “follow the crowd”, we may prefer to
make our own decisions in such cases, thinking
ourselves to be “smarter” and more experienced
than a typical machine.
It would also be interesting to identify ar-
eas in which having the answer to the H-or-M
question does notaﬀect human behavior. Would
we still be curious about the answer? Will
the question arise subconsciously, like the in-
evitable tendency to try to incorporate gender
perception in ﬁrst impressions (Signorella, 1992;
Ruble and Stangor, 1986)? What purpose would
this curiosity or knowledge serve? And if suchcuriosity does not emerge, will the indiﬀerence to
whether the agent with whom we are interacting
is a human or a machine aﬀect overall patterns of
social interaction in the relevant area?
Another facet of the H-or-M question is the
eﬀect of an incorrect determination. For exam-
ple, will a human agent be oﬀended when they
realize that a person they are interacting with
thinks they are a machine? How will that per-
son feel when they realize their mistake? How
embarrassed or angry will a person be when
they realize that the agent (perhaps even a co-
worker (Sadeghian and Hassenzahl, 2022)) whom
they thought was human, and with whom they
have developed a relationship, is a machine?
3 When and How Should
H-or-M be Easily Resolvable?
Given the relevance of the H-or-M identity of an
agent, when and how should this information be
made readily available.
In what contexts should the answer be pro-
vided to the person once, explicitly, either in ad-
vance, as is the case with some service chatbots,
or perhaps constantly and automatically, as is
done with "recording in progress" indicators in
phone calls and teleconferences?
Presently most chatbots disclose the fact that
they are machines. Should autonomous vehicles
be clearly marked as such? Should autonomous
drones be marked diﬀerently from remotely
controlled ones (Traboulsi and Barbeau, 2021)?
Should a human-like receptionist robot be clearly
marked as such, in order to not be mistakenly
thought to be human? And should interactions
with human agents be labeled as such, or should
this be the default?
Should there be standards for communicating
this information — using, say, text, icons or spo-
ken words? In what cases should the information
be provided in response to programming inter-
faces?
When should the H-or-M question be left for
the interested person to answer for themselves,
without a dedicated, explicit interface? One con-
text in which this is likely to be the case is
when the agent’s behavior is clearly a mixed-
mode as described above , a collaborative oper-
ation, partly human and partly machine. The
exact division of subtasks may be interesting to
humans, but may not be readily available.
Are there cases where the answer should actu-
4ally be hidden? We are interested here mainly in
ethical contexts, excluding situations of conﬂict,
oppression or fraud. Thus, for example, detecting
whether interactions in social media or in dating
websites are with a human or with a machine is
outside the scope of this paper. Note that in these
excluded cases, the issues may be not just with
regard to the H-or-M identity of the agent, but
more general, as when agents are not really who
they claim to be. Hence, we ask, are there ethical
cases where even searching for the answer should
be forbidden or blocked? Here are a few candi-
date scenarios:
• When the agent’s role is to help train a human
user in interacting with other humans, com-
plete with their errors and misunderstandings,
as in the training of a therapist, a dancer or a
sports person.
• When the agent is part of a show, or a meta-
discussion, where the enigma of the H-or-M
identity of the agent is part of the essence of
the interaction.
• When during development of a system that
needs to mimic human interactions with hu-
mans, the system is trained or tested through
interaction with a person who works under the
assumption that they are interacting with an-
other human.
• A variety of research situations studying the
behaviors of humans and machines.
4 Some Inherent Diﬀerences
Between Humans and
Machines
One cannot delve into the H-or-M matter with-
out considering the essential diﬀerences between
the behavior of human agents and that of ma-
chine agents—in general and in speciﬁc con-
texts. Turing himself dedicated a section to such
a discussion in his 1950 paper (Turing, 1950),
though clearly some distinctions have changed
dramatically over time; e.g., in the capabil-
ity to learn and to adapt to changing condi-
tions. Such diﬀerences between humans and ma-
chines are sometimes phrased as goals in achiev-
ing artiﬁcial intelligence in perception, cognition
and reasoning (see, for example, (Sifakis, 2022,
Ch. 6,7) and (Russell and Norvig, 2002, Ch. 1.1)
and references therein) and in achieving a sense
of humanness when interacting with machineagents (Rapp et al., 2021; Setlur and Tory, 2022;
Cai et al., 2022)
Gaining insights into these inherent diﬀer-
ences can help in studying their eﬀects on inter-
actions and in designing interrogation strategies.
Without attempting to get into a deep psycho-
logical, biological and philosophical discussion,
we list below some such tentative diﬀerences, as
they may be identiﬁed by typical people in the
context of everyday interactions. Our intention
is to draw attention to the existence of behav-
ioral diﬀerences between humans and machines
in interaction, and to invite discussion of their
consequences.
• Free will: “Machines are completely pre-
programmed, whereas humans have free will. ”
• Emotions: “Humans have emotions and feel
compassion, whereas machines do not. ”
• Context awareness: “Humans are sensitive to
context and to innumerable explicit and tacit
inputs, to which a typical machine is blind. ”
• Common sense and worldly familiarity: “A
human has more common sense and knowl-
edge regarding relations between entities and
cause-and-eﬀect patterns in the world than
any single average machine. ”
• Narrow specialties: “A human’s expertise in a
given domain is expected to be more focused
and limited than a machine’s. ” For example,
a human agent handling technical issues with
the website of a health provider is not ex-
pected to also discuss actual medical issues.
In general, machines do not have this limita-
tion.
• Learning: Turing claimed that humans re-
tain both long and short term memory and
learn from them, and machines often do not.
However, these days the opposite might be
the case. Machines can be equipped with
vast memories, can access voluminous repos-
itories of data, to which they can then apply
powerful machine learning algorithms,where
humans capabilities would be more limited.
Still, one may say that “humans can often be
taught new tasks more simply than machines,
sometimes by simple conversation” . For ex-
ample, if a person needs an agent (human or
machine) to carry out a task that the agent
does not know how to do (in the case of a hu-
man agent) or is not programmed for (in the
case of a machine agent), the person is more
likely to succeed in teaching a willing human
5agent to carry out the task than in teaching a
typical computerized agent.
• Collaboration: “Machines are becoming bet-
ter positioned to take advantage of multi-
agent collaboration. ” For example, car-to-car
communication for better coordination on a
highway is probably easier to implement tech-
nologically than establishing driver-to-driver
communication. Thus the H-or-M question re-
garding a group of cars, wondering if they are
all autonomous or are driven by multiple hu-
mans, may be partly answered by observing
their coordination practices. The use of the
idiom ’like a well-oiled machine’ to describe
the operation of an eﬃcient human organiza-
tion, hints at our intuition in this regard.
• Mistakes: “Humans make more mistakes than
machines. ”
• Diversity: “Human behavior involves more
randomness and arbitrary actions and is less
predictable than that of machines. ” Thus, dif-
ferent humans working on the same task ex-
hibit more diversity than diﬀerent machines
of the same model working on the same task.
Similarly, the performance of a human repeat-
ing a given task is more diverse than that of
a machine repeating the task.
Better understanding of these diﬀerences may
pave the way to bridging them, by endowing ma-
chines with certain desired human capabilities,
and to a lesser extent vice versa. This can be
valuable not only for concealing the H-or-M an-
swer in everyday tasks when appropriate, but for
improving the performance of both humans and
machines.
5 Some Aspects of H-or-M
Interrogation
As stated in the Introduction, in this paper we
do not seek a strategy or protocol for eliciting
the answer to the H-or-M question in everyday
situations. Still, it is worthwhile to discuss some
relevant issues brieﬂy, and to hint at a number of
tentative ingredients of a potential strategy.
In the classical Turing Test, only the inter-
rogator is proactive, and they are in control of
the interaction. The agent is expected to merely
react to the inquiries and statements coming its
way. Thus, this is an interrogation in the true
sense of the word.Some variants of the Turing test are
non-verbal in nature (Ciardo et al., 2022;
Avraham et al., 2012). The interrogator chal-
lenges the agent to act in certain ways, and
then analyzes the resulting behavior, including
seeking patterns therein. However, in this case
too the entire exchange is orchestrated as an
interrogation.
Related interrogations techniques
can be found in Captcha chal-
lenges (Von Ahn et al., 2003) that are
built around a cognitive task, and in hu-
man driven interrogations in a variety of
contexts ranging from psychiatric ther-
apy (Heiser et al., 1979) to reasoning about
drone behavior (Traboulsi and Barbeau, 2021).
Here, we turn our attention to situations
where (i) the agent does not oﬀer a direct answer
to the H-or-M question, and (ii) the interroga-
tion is passive, or implicit. For example, when
a person is interacting with a service center the
conversation is expected to be focused on the
service issue at hand, rather than on unmask-
ing the agent’s H-or-M identity. If the person
is interested in this information, they should de-
rive it from the agent’s communications on the
service issue only. Similarly, a human driver ob-
serving the non verbal behavior of a nearby ve-
hicle might be interested in determining whether
it is autonomous or driven by another human.
This determination would normally be carried
out through a combination of passive and unin-
volved observation and ordinary road behavior,
such passing the vehicle in question or getting
close to it.
This leads to another aspect of interrogation:
how does the person interact with the agent?
Clearly, even just seeing the agent in action may
provide some relevant clues. Hearing is another
important channel. The classical Turing test is
constrained to typewritten textual interaction.
However, while this limitation seems appropriate
for achieving fairness — since it masks gender dif-
ferences between human speakers and overcomes
technological constraints in speech synthesis — it
robs the interrogation of emotional aspects mani-
fested in speech prosody. This could be appropri-
ate for testing intelligence with less of a focus on
emotions, but it may be inappropriate if we are
interested in the H-or-M question in interactions
that normally involve speech. The same may also
apply to interactions where agent actions could
involve touch, smell, and possibly even taste.
What about other kinds of physical interac-
6tions? Can the interrogator ask to see the results
of a blood test of the agent? We leave this aspect
and other “limitations of imitation” for a future
discussion.
With the fast improvement in machine capa-
bilities and usage, we expect the importance of
the H-or-M issue to increase over time. Thus, it
is likely that techniques will evolve for conducting
proactive interrogations in a concealed manner,
within matter-of-fact verbal or non-verbal inter-
actions. In some cases, these techniques may be
crafted from the knowledge we have about dis-
tinctions between human and machine behavior,
and in others they may evolve naturally or sub-
consciously, and may even lead to better under-
standing of diﬀerences between humans and ma-
chines. Such techniques may be supported by
sharing historical information about interactions
and interrogation results.
The ability to conduct productive implicit H-
or-M interrogations may even become an algo-
rithmic/computational thinking skill, or perhaps
a social skill, that humans will be expected to ac-
quire. Furthermore, if the techniques can be for-
malized and generalized, we may see automated
tools that help humans, as well as other auto-
mated agents, in conducting such delicate inter-
rogations. And, if such interrogations can be for-
malized or automated, will humans and machines
eventually learn to detect them? Such detection
could trigger direct responses, in order to save
time and eﬀort, or perhaps drive redoubled eﬀorts
to conceal the answer. Would a human agent be
oﬀended if they notice that the person they are
interacting with is not sure that they are indeed
human? Will people use such interrogation to
tease agents, or perhaps to hint that the agent’s
behavior is too rigid?
Finally, it is possible that while the H-or-
M matter will be highly relevant, no speciﬁc
eﬀective interrogation protocols will evolve in
the foreseeable future. In fact, social norms
or judicial regulations may result in a practice
of routinely disclosing an agent’s H-or-M iden-
tity. Moreover, in some contexts, people may
just learn to live with not knowing and not
asking—as is the case with determining the gen-
der of one’s counterpart in a text-only interac-
tion, although this is known to be a primary
component of ﬁrst impressions (Signorella, 1992;
Ruble and Stangor, 1986).
Technological deﬁciencies in mimicking hu-
mans may render the entire issue moot, and, con-
versely, technological superiority over human per-formance in key aspects of the interaction may
cause developers to forego the eﬀort to mimic
humans in secondary aspects. Human agents in
roles that are also fulﬁlled by machines may limit
their own behaviors to purely professional and
bureaucratic, thus mimicking machines and re-
ducing the advantages (or the signiﬁcance of the
diﬀerences) of interacting with a human. Con-
versely, humans in such roles may emphasize be-
haviors that disclose their being human. Finally,
it is possible that while interrogation protocols
will be developed, both humans and machines will
learn to detect them and avoid playing along, ren-
dering the protocols useless.
6 Discussion
While the issues and questions we have raised
regarding the Human-or-Machine matter may
pique one’s curiosity, we may still ask, why are
they interesting now? Why do we want to know
now what people will do with answers to the
Human-or-Machine question in common interac-
tions? Can’t we just wait and see? Why do we
care about what will people do when they ﬁnd out
that the agent they thought was a human, was re-
ally a machine, or vice versa? Why do we need
to articulate diﬀerences between humans and ma-
chines with regard to observable behaviors in cer-
tain contexts?
We believe that better understanding of these
issues can advance science and technology in
many ways. Here are some examples.
First, in current methods for developing com-
puterized agents, the design of human interfaces
involves a delicate balance between the value of
friendly, intuitive human-oriented behavior (say,
through the use of natural language), and the
value of succinctness and predictability of typi-
cal machine behavior (say, using menu-based se-
lections). Understanding how human behaviors
and expectations diﬀer between interacting with
humans and machines can help in the design
of human-computer interfaces and business pro-
cesses. This may improve agent development pro-
ductivity and the quality of the ﬁnal products,
and could also contribute to the satisfaction and
even the well-being of human users.
For example, if it turns out that people use a
diﬀerent subset of the language when interacting
with machines that understand natural language,
the training of such agents may become more fo-
cused and more eﬃcient than when training on
7general natural language, and lead to fewer mis-
understandings in real-world interactions.
Second, a major factor in rich interactions is
trust. Understanding the diﬀerences between how
trust-building emerges in human-human interac-
tion as compared to human-machine interaction
may allow us to better understand this elusive
concept and to create protocols for enhancing and
accelerating trust-building in a variety of con-
texts.
Third, we are all familiar with cartoons depict-
ing people grumbling or getting angry with their
computers. For our own well-being, knowing that
we are interacting with a machine rather than
with a human may require us to channel our own
natural emotions diﬀerently. System developers
are already well aware of the fact that certain sys-
tem behaviors may evoke anger, frustration, and
other emotions. Translating such knowledge into
system design decisions will become even more
complicated in the case of agents that mimic hu-
mans. While there is a body of research about
various aspects of human emotions in the con-
text of interactions with chatbots, the challenge
here may be broader, both in terms of the dif-
ferent kinds of agents, and in connection to the
very fact that a growing portion of one’s inter-
actions may eventually be carried out with ma-
chines. Research and therapy methods related to
this area are emerging (Chaturvedi et al., 2023;
Heiser et al., 1979).
Fourth, the design of machine-machine inter-
faces may change in various ways. On a basic
level, two machine agents may use natural lan-
guage conversations to discover each other’s pro-
gramming interfaces and available data (mimick-
ing an engineer studying the documentation of
such interfaces), and then switch to more eﬃ-
cient or more extensive interactions. Such ﬂex-
ible interfaces will manifest enhancements over
technologies, such as those of web sites that are
designed for human use but also oﬀer special in-
formation and interfaces for web crawlers asso-
ciated with search engines, or of registry-based
service discovery in service-oriented architectures
(SOA). We might see human-like agents that are
interested in the H-or-M identity of whoever they
interact with, and based on the answer will then
act diﬀerently, or refrain from certain interactions
altogether, insisting on interacting only with hu-
mans, or only with certain machines. More ad-
vanced interfaces may exhibit new self-organizing
multi-agent collaboration, where agents inquire
about each other’s broader goals and learning ca-pabilities and then collaborate accordingly.
Carrying out research on human interaction
with agents who mimic human behavior with high
ﬁdelity in common, real world situations may not
be easy at all. Will researchers be able to cre-
ate the everyday nature of such interactions in
a controlled environment? Will lab experiments
with a limited number of kinds of machine agents
be representative? And conversely, when collect-
ing data from real-world interactions, will enough
ground truth information be available with regard
to whether the agents are humans or machines?
In summary, recent technological advances
give intelligent machines critical roles in our ev-
eryday lives. We do not know if such machines
will come to be treated as conventional objects,
like personal computers or ATMs, or as diﬀerent
kinds of living species, or perhaps, in the long
run and in particular cases, even become indis-
tinguishable from human professionals.
However that may turn out, we are convinced
that determining whether one is interacting with
a machine or with another human is likely to
become a central question. The insights to be
gained from studying the question and its ramiﬁ-
cations may have surprised even Turing.
Acknowledgements
The authors thank Joseph Sifakis for valuable
discussions and suggestions. This research was
funded in part by an NSFC-ISF grant issued
jointly by the National Natural Science Foun-
dation of China (NSFC) and the Israel Science
Foundation (ISF grant 3698/21). Additional sup-
port was provided by a research grant from the
Estate of Harry Levine, the Estate of Avraham
Rothstein, Brenda Gruss, and Daniel Hirsch, the
One8 Foundation, Rina Mayer, Maurice Levy,
and the Estate of Bernice Bernath.
Declarations
The authors contributed equally to the paper.
The authors have no competing interests.
REFERENCES
Avraham, G., Nisky, I., Fernandes, H. L., Acuna,
D. E., Kording, K. P., Loeb, G. E., and
8Karniel, A. (2012). Toward perceiving robots
as humans: Three handshake models face the
turing-like handshake test. IEEE Transac-
tions on Haptics , 5(3):196–207.
BBC (2021). How driverless
cars will change our world.
https://www.bbc.com/future/article/20211126-
how-driverless-cars-will-change-our-world .
Accessed 5/2023.
Beven, K., Lane, S., Page, T., Kretzschmar, A.,
Hankin, B., Smith, P., and Chappell, N.
(2022). On (in) validating environmental
models. 2. implementation of a turing-like
test to modelling hydrological processes. Hy-
drological Processes , 36(10):e14703.
Bush, J. T., Pogany, P., Pickett, S. D., Barker,
M., Baxter, A., Campos, S., Cooper, A. W.,
Hirst, D., Inglis, G., Nadin, A., et al. (2020).
A turing test for molecular generators. Jour-
nal of Medicinal Chemistry , 63(20):11964–
11971.
Cai, D., Li, H., and Law, R. (2022). Anthro-
pomorphism and ota chatbot adoption: a
mixed methods study. Journal of Travel &
Tourism Marketing , 39(2):228–255.
Chaturvedi, R., Verma, S., Das, R., and Dwivedi,
Y. K. (2023). Social companionship with ar-
tiﬁcial intelligence: Recent trends and future
avenues. Technological Forecasting and So-
cial Change , 193:122634.
Ciardo, F., De Tommaso, D., and Wykowska,
A. (2022). Human-like behavioral variability
blurs the distinction between a human and a
machine in a nonverbal turing test. Science
Robotics , 7(68):eabo1241.
Frank, D.-A. and Otterbring, T. (2023). Being
seen. . . by human or machine? acknowledg-
ment eﬀects on customer responses diﬀer be-
tween human and robotic service workers.
Technological Forecasting and Social Change ,
189:122345.
French, R. M. (2000). The turing test: the ﬁrst 50
years. Trends in cognitive sciences , 4(3):115–
122.
Hagen-Zanker, A., Yu, J., Hughes, S., and Santi-
tissadeekorn, N. (2023). A turing test of the
plausibility of model-generated urban expan-
sion scenarios. Findings .
Harel, D. (2005). A turing-like test for biological
modeling. Nature Biotechnology , 23(4):495–
496.Harel, D. (2016). Niépce–bell or turing: how
to test odour reproduction. Journal of The
Royal Society Interface , 13(125):20160587.
Heiser, J. F., Colby, K. M., Faught, W. S., and
Parkison, R. C. (1979). Can psychiatrists
distinguish a computer simulation of para-
noia from the real thing?: The limitations
of turing-like tests as measures of the ade-
quacy of simulations. Journal of Psychiatric
Research , 15(3):149–162.
Hill, J., Ford, W. R., and Farreras, I. G.
(2015). Real conversations with artiﬁcial in-
telligence: A comparison between human–
human online conversations and human–
chatbot conversations. Computers in human
behavior , 49:245–250.
Hindriks, K. V., Hagenaar, M., and Huckelba,
A. L. (2022). Eﬀects of robot clothing
on ﬁrst impressions, gender, human-likeness,
and suitability of a robot for occupations. In
2022 31st IEEE International Conference on
Robot and Human Interactive Communica-
tion (RO-MAN) , pages 428–435. IEEE.
Hoﬀmann, C. H. (2022). Is AI intelligent? an as-
sessment of artiﬁcial intelligence, 70 years af-
ter turing. Technology in Society , 68:101893.
Kalik, S. F. and Prokhorov, D. V. (2007). Au-
tomotive turing test. In Proceedings of the
2007 Workshop on Performance Metrics for
Intelligent Systems , pages 152–158.
Liu, Z. (2021). Sociological perspectives on arti-
ﬁcial intelligence: A typological reading. So-
ciology Compass , 15(3):e12851.
Mariani, M. M., Hashemi, N., and Wirtz, J.
(2023). Artiﬁcial intelligence empowered
conversational agents: A systematic litera-
ture review and research agenda. Journal of
Business Research , 161:113838.
Milcent, A.-S., Kadri, A., and Richir, S. (2022).
Using facial expressiveness of a virtual agent
to induce empathy in users. International
Journal of Human–Computer Interaction ,
38(3):240–252.
Nicolescu, L. and Tudorache, M. T. (2022).
Human-computer interaction in customer
service: The experience with ai chatbots—a
systematic literature review. Electronics ,
11(10):1579.
Noll, A. M. (1966). Human or machine: A sub-
jective comparison of piet mondrian’s “com-
position with lines”(1917) and a computer-
9generated picture. The psychological record ,
16(1):1–10.
Parmar, P., Ryu, J., Pandya, S., Sedoc, J., and
Agarwal, S. (2022). Health-focused conver-
sational agents in person-centered care: a re-
view of apps. NPJ digital medicine , 5(1):21.
Rapp, A., Curti, L., and Boldi, A. (2021). The
human side of human-chatbot interaction: A
systematic literature review of ten years of
research on text-based chatbots. Interna-
tional Journal of Human-Computer Studies ,
151:102630.
Reinkemeier, F. and Gnewuch, U. (2022). Match
or mismatch? how matching personality and
gender between voice assistants and users af-
fects trust in voice commerce. In Proceedings
of the 55th Hawaii International Conference
on System Sciences .
Rosemarin, H. and Rosenfeld, A. (2019). Play-
ing chess at a human desired level and style.
InProceedings of the 7th International Con-
ference on Human-Agent Interaction , pages
76–80.
Ruble, D. N. and Stangor, C. (1986). Stalking the
elusive schema: Insights from developmental
and social-psychological analyses of gender
schemas. Social Cognition , 4(2):227–261.
Russell, S. and Norvig, P. (2002). Prentice Hall
series in artiﬁcial intelligence . Prentice Hall,
Upper Saddle River, NJ:.
Sadeghian, S. and Hassenzahl, M. (2022). The”
artiﬁcial” colleague: Evaluation of work sat-
isfaction in collaboration with non-human
coworkers. In 27th International Conference
on Intelligent User Interfaces , pages 27–35.
Setlur, V. and Tory, M. (2022). How do you con-
verse with an analytical chatbot? revisiting
gricean maxims for designing analytical con-
versational behavior. In Proceedings of the
2022 CHI Conference on Human Factors in
Computing Systems , pages 1–17.
Sheth, A., Yip, H. Y., Iyengar, A., and Tepper,
P. (2019). Cognitive services and intelligent
chatbots: current perspectives and special is-
sue introduction. IEEE Internet Computing ,
23(2):6–12.
Sifakis, J. (2022). Understanding and Changing
the World: From Information to Knowledge
and Intelligence . Springer Nature.
Sifakis, J. (2023). Testing system intelligence.
arXiv preprint arXiv:2305.11472 .Signorella, M. L. (1992). Remembering gender-
related information. Sex Roles , 27:143–156.
Traboulsi, A. and Barbeau, M. (2021). A reverse
turing like test for quad-copters. In 2021
17th International Conference on Distributed
Computing in Sensor Systems (DCOSS) ,
pages 351–358. IEEE.
Turing, A. (1950). Computing machinery and in-
telligence. Mind , 59(236):433.
Van Pinxteren, M. M., Pluymaekers, M., and
Lemmink, J. G. (2020). Human-like com-
munication in conversational agents: a liter-
ature review and research agenda. Journal
of Service Management , 31(2):203–225.
Von Ahn, L., Blum, M., Hopper, N. J., and Lang-
ford, J. (2003). Captcha: Using hard ai prob-
lems for security. In Eurocrypt , volume 2656,
pages 294–311. Springer.
Woolgar, S. (1985). Why not a sociology of ma-
chines? the case of sociology and artiﬁcial
intelligence. Sociology , 19(4):557–572.
10