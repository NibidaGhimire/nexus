Benchmarking sparse system identiﬁcation with low-dimensional chaos
Alan A. Kaptanoglu,1, 2, *Lanyue Zhang,3Zachary G. Nicolaou,3Urban Fasel,4and Steven L. Brunton2
1IREAP , University of Maryland, College Park, MD, USA
2Department of Mechanical Engineering, University of Washington, Seattle, WA, USA
3Department of Applied Mathematics, University of Washington, Seattle, WA, USA
4Department of Aeronautics, Imperial College, London, UK
Sparse system identiﬁcation is the data-driven process of obtaining parsimonious differential equations that
describe the evolution of a dynamical system, balancing model complexity and accuracy. There has been rapid
innovation in system identiﬁcation across scientiﬁc domains, but there remains a gap in the literature for large-
scale methodological comparisons that are evaluated on a variety of dynamical systems. In this work, we sys-
tematically benchmark sparse regression variants by utilizing the dysts standardized database of chaotic systems
introduced by Gilpin [1]. In particular, we demonstrate how this open-source tool can be used to quantitatively
compare different methods of system identiﬁcation. To illustrate how this benchmark can be utilized, we per-
form a large comparison of four algorithms for solving the sparse identiﬁcation of nonlinear dynamics (SINDy)
optimization problem, ﬁnding strong performance of the original algorithm and a recent mixed-integer discrete
algorithm. In all cases, we used ensembling to improve the noise robustness of SINDy and provide statistical
comparisons. In addition, we show very compelling evidence that the weak SINDy formulation provides signif-
icant improvements over the traditional method, even on clean data. Lastly, we investigate how Pareto-optimal
models generated from SINDy algorithms depend on the properties of the equations, ﬁnding that the perfor-
mance shows no signiﬁcant dependence on a set of dynamical properties that quantify the amount of chaos,
scale separation, degree of nonlinearity, and the syntactic complexity.
Keywords: system identiﬁcation, dynamical systems, chaos, nonlinear systems, sparse regression, SINDy
I. INTRODUCTION
The governing equations of a dynamical system have tra-
ditionally been derived from ﬁrst principles or phenomenol-
ogy and conﬁrmed with experimentation. However, the avail-
ability of enormous volumes of data in the modern era is fa-
cilitating data-driven model discovery, which is now widely
applied to many areas such as mathematics, physics, chem-
istry, biology, engineering, and economics. A large num-
ber of such approaches have been developed in recent years,
such as the dynamic mode decomposition [2–4], Koopman
theory [5, 6], nonlinear autoregressive algorithms [7], neu-
ral networks [8–10], Gaussian process regression [11], opera-
tor inference and reduced-order modeling [12–14], symbolic
regression [15, 16], divide-and-conquer strategies [17], and
sparse regression [18]. Even within each of these approaches,
there has been proliﬁc methodological innovation.
There are several factors that must be considered when
choosing a data-driven modeling strategy, including the quan-
tity and quality of the data and the ultimate use of the model.
A key tradeoff for all of these methods is balancing accuracy
with model complexity, since a naíve determination of many
free, nonzero parameters will likely result in a model that is
overﬁt to the training data, especially as real-world data will
contain noise of various types. Despite the variety of new
methods for data-driven model discovery, there is a relative
lack of comparisons between such methods on a large set of
nontrivial dynamical systems. It is this gap in the literature
that we intend to address by providing a systematic compari-
son of sparse system identiﬁcation variants on a recently de-
*Corresponding author (akaptano@uw.edu).veloped large, curated database of known, chaotic, dynamical
systems [1].
Sparse system identiﬁcation is an umbrella term for system
identiﬁcation methods that address this key tradeoff between
model complexity and accuracy by promoting sparsity in the
parameterization of the model. This facilitates a-priori parsi-
monious descriptions that model the data with as few terms
as necessary. The sparse identiﬁcation of nonlinear dynamics
(SINDy) [18] is a leading method for parsimonious modeling,
and it and its variants will be investigated here. It is based
on sparsity-regularized linear regression, so it can be com-
puted quickly and robustly. Moreover, identiﬁed models and
optimization results can be readily understood by researchers,
as there is a wealth of literature on generalized linear regres-
sion and sparse optimization. Another compelling reason to
investigate variations of SINDy is that it has seen rapid ap-
plication and extension since its introduction in 2016. SINDy
has been widely applied for model identiﬁcation in applica-
tions such as chemical reaction dynamics [19], chemical pro-
cesses [20, 21], air pollutant modeling [22], biological trans-
port [23], stellar dynamics [24], disease transmission [25],
convective heat transfer [26], nonlinear optics [27], power
systems [28, 29], hydraulics [30], scientiﬁc computing [31],
human behavior models [32], ﬂuid dynamics [33–44], turbu-
lence modeling [45–50], plasma physics [51–53], structural
modeling [54], among others [30, 55–61]. It has also been
extended to handle more complex modeling scenarios such
as PDEs [62, 63], delay equations [64], stochastic differen-
tial equations [65–69], Bayesian modelling [70], dimensional
analysis [71], systems with inputs or control [72–74], systems
with implicit dynamics [75, 76], hybrid systems [77, 78], to
enforce physical constraints [33, 52, 79], to incorporate in-
formation theory [80] or group sparsity [81] or global stabil-
ity [82], to identify models from corrupt or limited data [83–arXiv:2302.10787v1  [cs.LG]  4 Feb 20232
87], to identify models with partial measurements of the state
space [88–91], to identify models with clever subsampling
strategies [92], and ensembles of initial conditions [93], to
perform cross-validation with ensemble methods [94, 95], and
extending to related objective functions [96], a weak or inte-
gral formulation [97–107], tensor representations [108, 109],
and stochastic forcing [110].
Most of the work in this ﬁeld demonstrates performance on
just a few canonical dynamical systems, such as the Lorenz63
system [111]. Moreover, there has been little systematic in-
vestigation of the comparative performance of different sparse
system identiﬁcation techniques. We posit two plausible rea-
sons for the relative lack of these investigations in the litera-
ture: (1) for more sophisticated variants of system identiﬁca-
tion, it can be computationally intensive to compute thousands
or hundreds of thousands of dynamical models to draw statis-
tical conclusions like those presented here, and (2) an analysis
of the performance of system identiﬁcation is more straight-
forward if the true governing equations are actually known.
This is clearly not an option for real experimental data. With
benchmarks, proper comparisons can be made between new
and existing methods to understand the way in which a new
algorithm produces novel beneﬁts and functionality. More-
over, essentially all system identiﬁcation methods require hy-
perparameter tuning and a proper comparison between meth-
ods necessitates that only the optimally tuned models from
each method are compared on a large set of examples.
At ﬁrst glance, there may seem to be too many confounding
variables for a thorough comparison of methods. In princi-
ple, sparse system identiﬁcation performance can depend on
the equation complexity, the searchable space of functional
forms, the size and diversity of the training data, the amount
of computing power available for hyperparameter scans, the
nonlinearity of the underlying dynamics, the amount and type
of noise in the data, the quality of the algorithm used to solve
the problem (and its associated hyperparameters), and poten-
tially many more factors.
A. Contributions of this work
To control for these parameters, we have carefully designed
our experiments in a number of ways. We use a ﬁxed func-
tional space for ﬁtting all systems, containing all possible
terms necessary for successful model identiﬁcation. More-
over, the dynamics and size of the training data are standard-
ized by using all of the polynomial dynamical systems in the
dysts database [1], and the amount of computing power is
negligible by using sparse regression algorithms with mini-
mal hyperparameters. For generating robust conclusions, the
following steps have been taken: d% noise is sampled from
a zero-mean Gaussian N(0;s=dkXtraink2=100)and added
to every data point in the training data Xtrain(and the noise
is subsequently ampliﬁed by computing ˙XtrainfromXtrain).
Moreover, a large ensemble of SINDy models are generated
by sub-sampling the same data [94], hyperparameter scans
are performed, and ﬁnally, only statistical model performance
metrics are reported. This work begins by detailing this stan-dardization and methodology.
The entirety of the methods and results in the present work
can be reproduced with the PySINDy code [112, 113]. More-
over, we have included a discussion in Appendix A about how
to effectively use this tool in future work, as researchers de-
sign new algorithms, libraries, databases, or other function-
ality. The present work illustrates the scope and ﬂexibility
of this tool by performing a few large-scale investigations re-
garding SINDy optimization and performance.
First, we generate a statistical comparison of different op-
timization algorithms for solving the SINDy model identiﬁ-
cation problem across a large database. The STLSQ [18],
SR3 [114], Lasso [115], MIOSR [116], and weak form [117]
algorithms are compared. To our knowledge, this is the ﬁrst
large-scale and statistical comparison of these algorithms. We
ﬁnd that the original STLSQ algorithm [18] holds up well for
performing SINDy optimization and the recent MIOSR algo-
rithm is high performance particularly for low-dimensional
or constrained problems. Moreover, we ﬁnd compelling ev-
idence that the weak formulation of the SINDy problem pro-
vides signiﬁcant performance improvements, even in the clean
data setting. Next, we classify the Pareto-optimal model per-
formance against properties of the true underlying dynamics.
We ﬁnd that the SINDy method has no signiﬁcant dependence
on the amount of chaos, scale separation, degree of nonlin-
earity, and even the syntactical complexity of the equations,
albeit with some interesting deviations that could merit addi-
tional future investigation.
II. METHODOLOGY
Robust conclusions about the performance of system iden-
tiﬁcation methods require that only standardized comparisons
are made. We now proceed by explaining the dataset, met-
rics for measuring sparse system identiﬁcation performance,
how we use hyperparameter scanning to ﬁnd Pareto optimal
models for each system, and explicitly deﬁne the dynamical
properties to be investigated. This section is intended to clar-
ify how we have carefully managed confounding variables in
the system identiﬁcation in order to draw conclusions with re-
spect to different optimization algorithms and with respect to
the dynamical properties of the database.
A. Dysts database
The dysts database [1], introduced by Gilpin in 2021, is
ideal for evaluating system identiﬁcation performance, be-
cause it is a large and highly standardized set of chaotic
systems with known equations of motion. Many of the
chaotic systems exhibit syntactically simple equations, e.g.
Lorenz63 is a three-state system with quadratic nonlinearity
and only seven terms in the differential equation. However,
this database generates far more complicated and dynami-
cal data than that of many existing databases, such as the
Nguyen symbolic regression database [118] used previously
for benchmarking symbolic regression algorithms [119]. The3
Aizawa Bouali2 BurkeShaw Chen ChenLee
Dadras DequanLi Finance GenesioT esi GuckenHolmes
Hadley Halvorsen HenonHeiles HyperBao HyperCai
HyperJha HyperLorenz HyperLu HyperPang HyperQi
HyperRossler HyperWang HyperXu HyperYan HyperYangChen
KawcStrizhak Laser Lorenz LorenzBounded LorenzStenflo
LuChen LuChenCheng MooreSpiegel NewtonLiepnik NoseHoover
NuclearQuad PehlivanWei Qi QiChen RabFabrikant
RayBenard RikiDynamo Rossler Rucklidge Sakarya
ShMorioka SprottA SprottB SprottC SprottD
SprottE SprottF SprottG SprottH SprottI
SprottJ SprottJerk SprottK SprottL SprottM
SprottN SprottO SprottP SprottQ SprottR
x1x2SprottS SprottT orus VallisElNino WangSun ZhouChen
FIG. 1: A training trajectory in the (x1;x2)subspace with 1%
added noise (black), along with a clean testing trajectory
(red) for the systems in the present work, taken from [1].
Some of the dynamical systems have multiple attractors, and
the data may not be sufﬁcient to fully “cover” the attractor.database also provides data, equations, and dynamical proper-
ties for over 100 chaotic systems exhibiting strange attractors
and coming from disparate scientiﬁc ﬁelds. It can be used
for testing sparse system identiﬁcation because the dynam-
ical properties have been pre-computed and there is a high
degree of standardization across the systems. For instance,
phase surrogate signiﬁcance testing is used to select optimal
integration timesteps and sampling rates for all the systems, so
that dynamics across systems are aligned with respect to their
smallest and largest timescales [120]. This is a critical step for
generating similar time series for each model; the minimum
signiﬁcant time scale in this dataset varies by more than four
orders of magnitude. Most importantly, the true governing
equations are available for evaluating the model performance.
To narrow the scope of our comparison, we consider 70
systems in the database, representing systems of ordinary dif-
ferential equations (ODEs) that have polynomial nonlineari-
ties, with degree no more than four. We exclusively consider
these systems and deﬁne a single “feature library” (space of
functions that can be used to ﬁt the data) for every model ﬁt.
Polynomial models are often the natural choice for capturing
leading-order dynamics for general nonlinear systems near an
attracting set or ﬁxed point, where the dynamics might be rea-
sonably expanded in a Taylor series.
Unlike some other dynamical systems, chaotic systems ex-
hibit a continuous spectrum of frequencies, making the dy-
namical data particularly challenging to systematically distin-
guish from noise, e.g. by using frequency ﬁlters. Our chosen
systems exhibit a speciﬁc subclass of chaos, as they are all
bounded and are characterized by strange attractor(s) with the
default system parameters used in the database. Moreover,
54 of the systems have a state space dimension of three, the
remaining 16 have a dimension of four, and there is a single
Hamiltonian system in the data. 51 of the systems are “hyper-
chaotic” systems that have been estimated to exhibit two or
more positive Lyapunov exponents. A training and testing tra-
jectory of 1000 data points are each visualized in Fig. 1 so that
the strange attractors are apparent.
To generate statistical conclusions, we generate ﬁve train-
ing trajectories and ﬁve testing trajectories representing ten
periods of motion, all sampled at 100 points per period for a
total of 1000 data points for each trajectory. Each trajectory is
generated from a different initial condition on the attractor.
To summarize, the underlying dynamics are chaotic, and
we assume that the dynamical equations can be represented as
systems of ODEs with polynomial nonlinearities up to fourth
order. Additionally, full state measurements are available that,
at various points in the coming analysis, contain added zero-
mean Gaussian noise.
B. The SINDy method
SINDy utilizes sparse regression to discover governing
equations from data [18]. The aim is to identify differential
equation models of the form
˙x=f(x)Q(x); (1)4
wheref(x)is a nonlinear function of the state x,is a li-
brary of nonlinear functions, and is a matrix of constant
coefﬁcients. Together  represents a linear combination of
nonlinear functions that are chosen to best approximate f(x).
The choice of nonlinear functions in is clearly important for
successful approximation. Before moving from continuous
state space to discrete measurement data, recall that high or-
der differential equations can always be reduced to ﬁrst order
coupled differential equations, so this form is fairly general.
Extensions of Eq. (1) for control inputs [72], partial differen-
tial equations [63], implicit terms [76], and other variations
are available with PySINDy [121].
In order to identify Eq. (1) from data, we need measure-
ments of the state x. First, we uniformly sample the full-
state data on a set of training trajectories and a data matrix
X2RMdis formed from the time-series data of the state,
x(t1);x(t2);:::;x(tM):
X=state
                    !2
6664x1(t1)x2(t1)xd(t1)
x1(t2)x2(t2)xd(t2)
............
x1(tM)x2(tM)xd(tM)3
7775???????????ytime
: (2)
Mis the number of time points and dis the size of the state
space, i.e. three or four for our dataset. A matrix of deriva-
tives in time, ˙X, is deﬁned similarly and can be numerically
computed from X, for instance by ﬁnite differences or more
sophisticated differentiation [86, 122, 123]. Then we form a
feature library (X)of possible terms that could describe
the evolution of the system, and attempt to ﬁnd a sparse vec-
tor of coefﬁcients representing the best linear combination
of terms in the library. In general, the feature library can
include a wide range of generalized functions, and designing
this library often requires domain-knowledge; however, the
systems of interest in the present work are all polynomial.
Therefore, we ﬁx to be a polynomial library in X, up to
fourth order. This is an important point; in general system
identiﬁcation methods rely on adequate functional forms in
the feature library, and here we have completely bypassed this
issue, isolating and disambiguating the optimization perfor-
mance from the choice of library. We know deﬁnitively that
the feature library contains the necessary terms in order to de-
scribe all 70 chaotic systems considered in the present work.
The vector of coefﬁcients is determined via the following
sparse optimization problem:
argmin
1
2k ˙Xk2+lkk0
: (3)
The ﬁrst term in the SINDy optimization problem in Eq. (3)
is a least-squares ﬁt of a system of ODEs to the given
data in ˙X. The l0loss,kk0, is a function that counts the
number of nonzero elements of . Using the l0loss or al-
ternative regularization to promote sparsity in the coefﬁcients
of an identiﬁed model tends to improve robustness to numer-
ical or experimental noise, addressing the tradeoff betweenaccuracy and generalizability and avoiding overﬁtting to the
training data. The l0loss is nonconvex and nonsmooth, and
therefore many convex relaxations of this problem have been
introduced [18, 63, 79]. The ﬁrst major result presented in this
work compares a number of different algorithms for solving
Eq. (3), including the sequentially-thresholded least-squares
(STLSQ) algorithm from the original SINDy paper [18], the
traditional convex relaxation of Eq. (3) using the Lasso [115],
the SR3 algorithm [79, 114], and a recent mixed-integer dis-
crete algorithm called MIOSR [116].
As much as is feasible, we use default PySINDy param-
eters to reduce the number of hyperparameters that must be
scanned. For instance, in the present work ˙Xis calculated
by simple ﬁnite differences. Improved performance is imme-
diately available for future work using more sophisticated dif-
ferentation methods [86, 122, 123], many of which are already
available in the PySINDy code. Weak or integral formula-
tions of SINDy [97] are also known to mitigate the noise am-
pliﬁcation inherent to ﬁnite differencing, and we illustrate in
Sec. III B that the weak formulation implemented in PySINDy
produces substantial performance beneﬁts.
C. Metrics for performance
Methodological comparisons require appropriate metrics
for determining the relative performance quality of different
methods. Unlike real-world data, we have access to the true
equations of motion of all 70 systems to be tested. There-
fore we can compute a number of useful metrics to character-
ize the performance of the resulting sparse symbolic models
on these examples. We deﬁne the normalized coefﬁcient and
root-mean-square errors
Ecoef=kTrue SINDyk2
kTruek2; (4)
ERMSE =k˙XTrue ˙XSINDykF
k˙XTruekF: (5)
Other works have also explored the kk 1andkk 0norm ver-
sions of Eq. (4) to quantify model mismatch [76, 119]. Ob-
taining very small values for E RMSE (or variants such as the
mean absolute error, regular mean absolute percent error, etc.)
is simple with a basic system identiﬁcation algorithm applied
to clean data from our chosen database. This is because mod-
els with many parameters can overﬁt using many small but
nonzero terms and potentially produce very accurate testing
trajectories if we only investigate regions that are nearby to
the strange attractor(s). Despite this caveat, for forecasting,
prediction, reconstruction of the strange attractor, calculation
of the Lyapunov spectrum, and other important metrics for
capturing the behavior of a dynamical system, models with
low E RMSE are often more than adequate. Moreover, the usual
situation of interest is one in which the dynamical equations
and therefore the coefﬁcient errors are unknown. We com-
ment further on errors and stability in Appendix B.
Given that we know the true coefﬁcients, we can check for
overﬁtting by considering the coefﬁcient errors, which can be5
incorrect even when E RMSE is extremely small. Indeed, on
the strange attractor, the dynamics may be signiﬁcantly re-
duced compared to the dynamics on the full state space. A
connection to invariant manifolds and the reduced effective
dimension and entropy of the dynamical system are discussed
in Appendix C.
D. Automated hyperparameter scanning with ensembles
Before any conclusions can be drawn about system identi-
ﬁcation performance, each algorithm’s hyperparameters must
be tuned. Sweeping the sparsity-related hyperparameters gen-
erates a spectrum ranging from very sparse models with lim-
ited accuracy to dense models with high prediction accu-
racy, which is typically referred to as a Pareto-front [124].
The dataset is too large to manually tune hyperparameters
for optimal system identiﬁcation performance on each chaotic
trajectory, so for hyperparameter scanning, we use the fol-
lowing procedure. For all the algorithms, we use ensem-
bling [94]. We subsample the ﬁve training trajectories by
choosing 50% of the total trajectory data without replacement
in order to generate 10 models for each value of the threshold
hyperparameter. For STLSQ, we then sweep over 300 val-
ues of increasingly large hard thresholds, and determine the
best threshold by the minimal, ﬁnite-sample-size-corrected,
Akaike information criterion (AIC) [80] averaged over the 10
models generated by subsampling,
AIC=Mlog(k˙XTrue ˙XSINDyk2
2)+2k+2k(k+1)
(M k 1);(6)
where k=kSINDyk0and ˙XTrueand ˙XSINDy are both com-
puted from the test trajectories. The AIC is a well-known sta-
tistical metric for comparing models while balancing sparsity
and accuracy [80]. This systematic process requires the iden-
tiﬁcation of 3001070=210;000 models. Despite the
large number of models to generate, the entire procedure for
STLSQ takes about 10 minutes to run on single Intel Core i9-
7920X 2.90GHz CPU. After the best threshold is determined
via this Pareto sweep, the corresponding ten models allow us
to characterize a distribution of identiﬁed coefﬁcients and er-
rors. The other SINDy algorithms are tuned similarly for their
respective hyperparameters. The SR3 algorithm has an extra
parameter nthat we set to either 1 or 0 :1 since these values
tend to work well on clean data and it avoids additional hy-
perparameter scanning.
The use of SINDy, rather than more sophisticated sys-
tem identiﬁcation methods, facilitates very fast calculation of
Another common metric is the recovery rate [119], which returns a discrete 0
or 1 value for each correctly identiﬁed term in the equations. However, small
errors may result from insufﬁcient hyperparameter tuning, noise in the data,
insufﬁcient optimization convergence, and many other sources. Moreover,
it is often useful to penalize large coefﬁcient errors more than small ones.
Thus, we omit the recovery rate metric in this work but mention it for future
investigation.many models. For instance, previous work [125] investigat-
ing mostly genetic programming algorithms for symbolic re-
gression required roughly 200 core-hours per algorithm per
dynamical system in order to generate a best model from the
Pareto front. In contrast, using STLSQ we require roughly 
10 3core-hours per dynamical system to generate our Pareto-
optimal model, consistent with ﬁndings elsewhere [1, 126].
This speed is required because the systematic process we have
outlined requires generating O(105)SINDy models per algo-
rithm.
E. Dynamical properties
Systems of differential equations can often be effectively
classiﬁed by various dynamical properties. Now that the dy-
namical systems used in the present work have been intro-
duced, we deﬁne a set of dynamical properties for testing
sparse system identiﬁcation performance: the degree of chaos,
the degree of scale separation, the syntactic complexity of the
underlying equations, and the amount of nonlinearity. In the
present work, the degree of chaos is measured by the largest
Lyapunov exponent. The degree of scale separation is taken
as the system’s dominant timescale divided by the system’s
smallest signiﬁcant timescale as determined by the method in
Gilpin [1]. The syntactic complexity is quantiﬁed as the de-
scription length, representing the number of bits to describe
every object in the equations. Lastly, the amount of nonlin-
earity is taken as sum of the total number of terms in the
equations, weighted by each term’s polynomial degree, so
it is a mix between the degree of nonlinearity and the num-
ber of equation terms. For more precise deﬁnitions and dis-
cussion of the metric choices, see Appendix C. A summary
of the nonlinearities appearing in the dynamical equations is
shown in Fig. 2, indicating that most of the systems exhibit
strong quadratic nonlinearity. Quadratic nonlinearity is com-
mon in reduced-order models of ﬂuids and many other dynam-
ical systems, e.g. the Lorenz or Rossler systems, because of
the quadratic nonlinearity coming from the convective term
of the material derivative. Ten systems exhibit up to cubic
nonlinearity and only two systems have quartic nonlinearity.
Fig. 3 summarizes the distribution of the remaining dynami-
cal properties, indicating a wide range of values taken by the
dataset.
III. RESULTS
We now demonstrate the utility of combining dysts with
PySINDy by illustrating a series of benchmark experiments
to better understand performance tradeoffs of sparse system
identiﬁcation variants. Now that we have explained the char-
acteristics of the database, the sparse system identiﬁcation al-
gorithm, the error metrics, and the dynamical properties of
interest, we illustrate the results of our investigations with
the standardized, Pareto-optimal SINDy models. We empha-
size that the entirety of these results are reproduced as ex-
amples in the PySINDy code and the following investigations6
Aizawa
Bouali2
BurkeShaw
Chen
ChenLee
Dadras
DequanLi
Finance
GenesioT esi
GuckenHolmes
Hadley
Halvorsen
HenonHeiles
HyperBao
HyperCai
HyperJha
HyperLorenz
HyperLu
HyperPang
HyperQi
HyperRossler
HyperWang
HyperXu
HyperYan
HyperYangChen
KawcStrizhak
Laser
Lorenz
LorenzBounded
LorenzStenflo
LuChen
LuChenCheng
MooreSpiegel
NewtonLiepnik
NoseHoover
NuclearQuad
PehlivanWei
Qi
QiChen
RabFabrikant
RayBenard
RikiDynamo
Rossler
Rucklidge
Sakarya
ShMorioka
SprottA
SprottB
SprottC
SprottD
SprottE
SprottF
SprottG
SprottH
SprottI
SprottJ
SprottJerk
SprottK
SprottL
SprottM
SprottN
SprottO
SprottP
SprottQ
SprottR
SprottS
SprottT orus
VallisElNino
WangSun
ZhouChen01234
Poly
 order
0515
FIG. 2: Total number of polynomial terms in each equation, for each polynomial degree. It can be seen that quadratic
nonlinearities are dominantly represented.
102
101
100101102103104105
Dynamical property values02468101214# of systemsMax Lyapunov exponent
Scale separation
Description length
FIG. 3: Distributions of the different properties of the
dynamical system used in this work.
use Pareto-optimal identiﬁed models that have been chosen by
producing the minimal AIC in a hyperparameter scan.
A. Comparison of SINDy algorithms
As an illustration of this methodology and database, we
compare Pareto-optimal models for a class of SINDy algo-
rithms: STLSQ, Lasso, SR3 (ﬁxing the nhyperparameter to
1 or 0 :1), and MIOSR. Additionally, we use the STLSQ op-
timizer with the weak form implementation in the PySINDy
code in order to compare with the traditional method. Al-
though not shown in this work, the weak formulation can also
be used with any of the other PySINDy optimizers with min-
imal modiﬁcation. Weak form runs were performed with the
other algorithms and similar trends were seen between algo-
rithms, so we have omitted these results. The weak form uses
200 subdomains, which becomes 200 points to use in the opti-
mization, rather than the 1000 trajectory points used in the tra-
ditional optimization. This many subdomains is probably ex-
cessive; it was found empirically that, after 100 subdomains,
there is essentially no further improvement (in fact just 10 sub-
domains is much faster to compute and still performs quite
well). The MIOSR optimizer is set to use no more than ﬁve
seconds to solve and prove optimality per state variable. The
limit of ﬁve seconds is rarely invoked with clean data but more
often invoked once the data has 1% added noise.The results were generated by the process outlined earlier
in Section II D. Figure 4a illustrates the Pareto-optimal model
performances for each of the SINDy algorithms. Results are
shown for both clean data and data with 1% added noise.
These results illustrate the known insufﬁciency of the Lasso
for generating high-performance system identiﬁcation results,
although the Lasso degrades less than some of the other opti-
mizers as noise is added. On the clean data, the remainder of
the optimizers regularly produce models with Ecoef<1% and
ERMSE <10%.
For additional evidence that these are accurate dynamical
models, Fig. 5 illustrates the Pareto-optimal STLSQ model
simulations of ten trajectories from new initial conditions
starting slightly off each of the strange attractors. An increas-
ingly grey-tinted background indicates an increasing number
of simulated trajectories that went unstable in the ODE solver.
Overall, the major conclusion is that, without noise, Pareto-
optimal SINDy models regularly produce new trajectories that
recreate the strange attractor of each chaotic system, meaning
that the dynamical features such as the Lyapunov spectrum
can be accurately computed.
B. Weak formulation results
The weak or integral form of SINDy [97, 98, 100] subdi-
vides the time span of the trajectories and considers integrals
of the ODEs over these subdomains. This method then avoids
the calculation of derivatives on noisy data and subsequent
noise ampliﬁcation by integrating by parts when appropriate.
The results in Figure 4a indicate that the weak formulation
performs extraordinarily well in producing the correct equa-
tion coefﬁcients. The weak form ERMSE results appear slightly
less impressive for a rather technical reason. We are comput-
ing the pointwise ERMSE deﬁned in Eq. (5) in order to com-
pare against the other optimizers. Unlike the other optimiz-
ers,ERMSE (of the training data) is not directly minimized in
the weak formulation of the problem; the weak form solves
its optimization problem by minimizing an error metric that
is integrated over the subdomains. Despite not directly mini-
mizing these quantities, the weak form still generally obtains
the smallest values of ERMSE .
These results provide compelling evidence for the consider-
able strength of the weak formulation. Interestingly, the typ-
ical justiﬁcation for the high performance of weak formula-
tions of sparse regression is that it allows the user to avoid am-7
1% noise dataClean data
(a)
1% noise dataClean data (b)
FIG. 4: (a) Summary of the average errors for each optimization algorithm, without noise (top two rows) and with 1% noise
(bottom two rows), on all 70 chaotic systems. STLSQ and MIOSR generally produce the best models of the traditional
optimizers. Lasso has weak performance, and SR3 seems to require additional hyperparameter scanning to avoid substantial
errors when noise is present. The weak formulation with STLSQ identiﬁes the coefﬁcients extremely well and demonstrates the
substantial advantages of using the weak form. (b) Algorithm average and median performance aggregated over all the
dynamical systems. Marker colors indicate different optimizers, and the marker shapes indicate the type of error. All runs were
performed serially with an Intel Core i9-7920X 2.90GHz CPU.
plifying noise when computing derivatives of the data. How-
ever, we ﬁnd signiﬁcant improvements with the weak formu-
lation even when there is no added noise – only the intrinsic
and unavoidable “noise” of ﬁnite sampling rates. Even further,
the weak formulation results are not truly Pareto-optimal; the
threshold value for STLSQ is still scanned but the additional
hyperparameters available in the weak form are just ﬁxed to a
set of plausible values. Combined, we consider these results
persuasive for using the weak formulation whenever possible.
For ﬂexible usage, the weak formulation is entirely integrated
into the feature library functionality of the PySINDy code,
and therefore can be used with any of the current or future
optimizers, or with any generic library terms of interest.
C. Greedy vs exact algorithms
MIOSR is an exact mixed-integer algorithm that can often
solve the l0-regularized (i.e. nonconvex) SINDy problem to
optimality. In contrast, the STLSQ algorithm solves this prob-
lem greedily, meaning it chooses the best model on a subset
of the coefﬁcients at each iteration, and it cannot recover from
thresholding mistakes that occur in earlier iterations.On the noisy data, the MIOSR algorithm seems to outper-
form the others, while with clean data all of the non-weak al-
gorithms perform similarly well except for Lasso. With clean
data, the SR3 algorithms very well. Some variation is seen
with SR3 between the n=1 and n=0:1 cases but a clear
trend is not distinguishable. The SR3 errors become signif-
icantly larger as noise is added and these results seem to in-
dicate that hyperparameter scans over nare required for high
performance when noise is present. All of the optimizer per-
formances degrade somewhat when noise is added. These
overall takeaways are summarized further in Figure 4b.
At 1% noise, STLSQ and MIOSR show performance ad-
vantages over the other optimizers. As measured by the aver-
age coefﬁcient errors, MIOSR leads the group. STLSQ at 1%
noise exhibits a small number of very large RMSE errors and
this tends to reduce the average performance. This is consis-
tent with the explanation that greedy methods can increasingly
make catastrophic mistakes as the noise levels increase. This
trend can also been seen to an extent in the weak formulation
results since the optimization is still being performed greedily
with STLSQ.
One takeaway is that the MIOSR algorithm performs well,
as we might expect for an exact mixed-integer optimization.8
Aizawa Bouali2 BurkeShaw Chen ChenLee
Dadras DequanLi Finance GenesioT esi GuckenHolmes
Hadley Halvorsen HenonHeiles HyperBao HyperCai
HyperJha HyperLorenz HyperLu HyperPang HyperQi
HyperRossler HyperWang HyperXu HyperYan HyperYangChen
KawcStrizhak Laser Lorenz LorenzBounded LorenzStenflo
LuChen LuChenCheng MooreSpiegel NewtonLiepnik NoseHoover
NuclearQuad PehlivanWei Qi QiChen RabFabrikant
RayBenard RikiDynamo Rossler Rucklidge Sakarya
ShMorioka SprottA SprottB SprottC SprottD
SprottE SprottF SprottG SprottH SprottI
SprottJ SprottJerk SprottK SprottL SprottM
SprottN SprottO SprottP SprottQ SprottR
x1x2SprottS SprottT orus VallisElNino WangSun ZhouChen
FIG. 5: Pareto-optimal SINDy STLSQ model simulations
(red) vs true trajectories (black) in the (x1;x2)subspace using
ten new initial conditions. Greyer backgrounds indicate more
unstable trajectories, which are otherwise not pictured.However, Fig. 4b shows that, at larger levels of noise, the
MIOSR algorithm requires much more computing time than
the other algorithms. This is true despite putting a time limit
on the computation, but it is reassuring that strong perfor-
mance is seen even if the optimizer does not ﬁnish ﬁnding
a solution to optimality. Indeed, as is pointed out in detail in
Bertsimas et al. [116], MIOSR runs slower when the regres-
sion is harder, e.g. in the noisy setting, but the computational
efﬁciency scales very well with the amount of data and addi-
tionally sees substantial speedups with parallelization or bet-
ter CPUs than used in this work. However, our results appear
to show MIOSR does not signiﬁcantly dominate the other al-
gorithms in our chosen performance metrics. This difference
with the results in Bertsimas et al. [116] may be because their
performance metric focus is primarily on the true positive rate
of the model coefﬁcients, and their MIOSR results are most
impressive in the low-data limit.
In fact, these results also highlight the strong performance
of the STLSQ algorithm; STLSQ appears relatively robust to
noise, very fast to compute (regardless of the noise level),
and, for the most part, generates models of comparable per-
formance to MIOSR. Even at 1% noise, the errors that ac-
cumulate from the greedy nature of STLSQ appear tolerable.
These optimizers were not tested at larger values of noise. It
is well known that weak or integral formulations of the opti-
mization problem are required for retaining high performance
in the high-noise setting [94, 97, 99]. This fact is clearly rein-
forced by the impressive weak form performance in Fig. 4a.
D. Dynamical properties
A pressing question for practitioners of system identiﬁca-
tion is whether certain types of dynamical systems are more
difﬁcult to identify than others. For instance, some meth-
ods are speciﬁcally designed to search for Hamiltonian sys-
tems [127–129]. Another example is that traditional RNNs
should be modiﬁed for modeling chaotic systems in order to
avoid unbounded gradients [130].
We now present an investigation into the importance of the
dynamical properties with respect to the performance of the
various SINDy optimizers. The required data and compu-
tations for this analysis have already been generated in the
previous section. For each chaotic system in our dataset, the
errors of the Pareto-optimal models are sorted by the dynam-
ical property in question, and some simple data ﬁts are at-
tempted: linear, log-linear, and log-log. Linear ﬁts between
log(ERMSE)and the scale separation seemed to produce the
largest R2coefﬁcient of determination for all the optimizers,
and similarly for log-log ﬁts between log (ERMSE)and the log-
arithm of the remaining dynamical properties. We make no
claims that these simple ﬁts to the data best capture the cor-
relations, but rather they are used to roughly quantify if there
are correlations between increasing dynamical property val-
ues and worsening system identiﬁcation performance.
Visual inspection of the different dynamical property ﬁts for
the weak form STLSQ optimizer at 0 :1% noise in Fig. 6 does
seem to indicate that, on average, the optimizer performance9
STLSQ (weak), noise 0.1%
MIOSR, noise 1%
FIG. 6: Two illustrations of the ERMSE results for the Pareto-optimal STLSQ optimizer (weak form) on data with 0 :1% noise
and for the MIOSR optimizer at 1% noise. The STLSQ result suggests that ERMSE grows quickly with the scale separation. The
MIOSR algorithm produces weak log-log trends with the other three dynamical properties tested in this work, although these
trends strengthen as noise increases.
rapidly drops as the scale separation increases. Similarly, vi-
sual inspection of the different dynamical property ﬁts for the
MIOSR optimizer at 1% noise in Fig. 6 indicates that log-
log ﬁts to the data weakly capture the optimizer performance
for all of the dynamical properties except the scale separation,
which shows no clear trend.
All of the best R2coefﬁcients of determination are illus-
trated in Fig. 7 for each optimizer, each dynamical property,
and for noise levels of 0, 0.1, and 1%. Most of the R2val-
ues are below 0.25, indicating that the dominant behavior is
that the optimizers produce results that are approximately in-
dependent of the dynamical properties. Nonetheless, there ap-
pear to be some intriguing trends; for instance, the weak for-
mulation ERMSE shows substantial correlation with the scale
separation. MIOSR seems to increasingly correlate with the
dynamical properties (except that of scale separation) at larger
values of noise.
These results suggest that there are some relatively persis-
tent but quite weak correlations with the underlying dynami-
cal properties of the equations. Moreover, which correlationsdominate appears to depend on the level of noise in the data
and the choice of optimization algorithm.
Our large experiments are a strong afﬁrmation of the “Task
4” result in Gilpin indicating that SINDy model results were
independent of the level of chaos [1]. Our metric for the de-
gree of chaos in the system is a rather coarse measure, but
similar results were found in both Gilpin and the present work
when alternative metrics for chaos were used. The lack of
correlation with the description length is an interesting and
potentially counter-intuitive discovery. To be more speciﬁc, if
the dynamical terms live in the subspace of the feature library,
the quality of Pareto-optimal models generated by sparse re-
gression onto data seems to be approximately uncorrelated
with the length or complexity of the underlying dynamical
equations. This could be seen as a positive result. If ade-
quate functional terms are available to describe the underly-
ing dynamics, and the data is high-quality, the complexity of
the equation factors out. This motivates large and expansive
libraries, although this will generally increase the condition
number of the feature library and therefore require additional10
STLSQ 0%STLSQ 0.1%STLSQ 1%
log(ERMSE) = a * (scale separation) + b
log(ERMSE) = a * log(chaos) + b
log(ERMSe) = a * log(syn. complexity) + b
log(ERMSE) = a * log(nonlinearity) + b
SR3 (=1) 0%
SR3 (=1) 0.1%
SR3 (=1) 1%
SR3 (=0.1) 0%
SR3 (=0.1) 0.1%
SR3 (=0.1) 1%
Lasso 0%Lasso 0.1%Lasso 1%
MIOSR 0%MIOSR 0.1%MIOSR 1%
0.0 0.1 0.2 0.3 0.4 0.5 0.6
R2STLSQ (weak) 0%STLSQ (weak) 0.1%STLSQ (weak) 1%
FIG. 7: Summary of the weak R2coefﬁcients of determination found between each optimizer’s Pareto-optimal ERMSE and the
dynamical properties, choosing the best ﬁt from linear, log-linear, and log-log regressions. Some differing trends can be
observed for each optimizer. Even weaker correlations are found between Ecoefand the dynamical properties (not pictured).
regularization and shift the Pareto-front.
At 1% noise levels, the quality of the Pareto-optimal mod-
els generally drops; for instance, a typical result with STLSQ
produces only a few models with less than 10% ERMSE and
eight of the models are not able to produce any new trajecto-
ries (starting from slightly off the attractor) that either stay
bounded or remotely match the qualitative behavior of the
strange attractor.
IV . CONCLUSIONS
The methodology and analysis in the present work has
shown that sparse system identiﬁcation can be used to identify
the dynamics of 70 polynomial chaotic systems, reproducing
the strange attractors and therefore reproducing the fundamen-
tal dynamics. We extensively used sub-sampling to generate
large ensembles of models, utilized the AIC to identify ensem-
bles at the optimal hyperparameter values, and relied on the
computational speed of sparse linear regression to compute
over one million dynamical models in just two days worth of
CPU time (mostly expended during the MIOSR algorithm).
This methodology was used to benchmark a variety of dif-
ferent SINDy optimizers, ﬁnding that: STLSQ & MIOSR
produce the best performance, Lasso is relatively robust to
noise but otherwise lower performance, and large-scale ﬁt-
ting with SR3 requires additional hyperparameter scanning for
high performance. As signiﬁcant noise is added to the data,STLSQ retains its fast computational speed but starts making
“greedy” mistakes that cannot be recovered from, while con-
versely, MIOSR slows down considerably but can retain its
strong performance for most systems. Lastly, we ﬁnd very
persuasive evidence that the weak formulation provides sig-
niﬁcant performance improvements across the database, even
in the zero-added-noise setting. The weak form can be used
with any of the PySINDy optimizers and we recommend us-
ing the weak formulation for almost all cases. Similarly, sub-
sampling the data to create an ensemble of SINDy models [94]
is highly recommended, especially in the presence of noise.
We have also presented one of the ﬁrst large-scale and quan-
titative analyses into how sparse regression depends on the
dynamical properties of the underlying governing equations.
Overall, we found very weak correlations between the dy-
namical properties and the performance of the Pareto-optimal
SINDy models, with some noteworthy deviations, providing
a foundation for more sophisticated investigations in future
work.
There are many opportunities for future work that builds
on the database and analysis presented here. Examples in-
clude further improvements to recent and important work ad-
dressing partial measurements [89, 131], the low-data high-
noise regime [94, 116], extended feature libraries (which can
already be built in PySINDy), and algorithm comparisons
with convex constraints. For instance, most of the optimizers,
STLSQ, SR3, Lasso, and MIOSR, can in principle accommo-
date general convex constraints. However, future PySINDy11
work is required to implement these options. Currently, con-
straints with STLSQ [33] and Lasso are not supported, while
SR3 constraints in PySINDy (except linear equalities) rely on
a CVXPY [132] backend which signiﬁcantly slows down the
algorithm. See e.g. Kaptanoglu et al. [133] for a much faster
SR3 algorithm with quadratic inequality constraints.
There are also numerous other dynamical metrics, sparse
system identiﬁcation algorithms, subsampling methods, types
of dynamical systems (i.e. not just those deﬁned by poly-
nomial dynamics), and other parameters to systematically
test. Although it is beyond the scope of this work, the dysts
database facilitates another very interesting line of inquiry –
building data-driven models for predicting chaotic bifurca-
tions in global dynamical behavior, e.g. between exhibiting a
strange attractor and a stable periodic solution. Additional fu-
ture work could perform a quantitative analysis of how invari-
ant manifolds affect the performance and usefulness of system
identiﬁcation; this topic is discussed in Appendix C. Lastly,
many dynamical or engineering-focused metrics, such as the
description length of the equations, can be directly minimized
during the optimization and this may change the qualitative
conclusions found in this work with the baseline SINDy op-
timization problem, especially since the present work found
that the correlations are weak between model performance
and the metrics tested.
V . ACKNOWLEDGEMENTS
AAK would like to acknowledge useful conversations with
William Gilpin, Chris Hansen, and Nathan Kutz. ZGN
is a Washington Research Foundation Postdoctoral Fellow.
The authors acknowledge support from the National Sci-
ence Foundation AI Institute in Dynamic Systems under grant
number 2112085.
Appendix A: How to use this benchmark
The goal of the present work is to provide a systematic ap-
proach to benchmark new and existing innovations in the ﬁeld
of sparse system identiﬁcation. In order to do so, we have
endeavored to provide simple, intuitive scripts, built on the
open-source PySINDy code. A script for performing the type
of Pareto-optimal scans in this work can be found in one of
the PySINDy code examples. It contains options for the user
to specify (1) the data (including the number of points, sam-
pling rates, etc. of the training and testing trajectories), (2) the
optimization algorithm, and (3) the feature library (including
whether to normalize the feature library, how to subsample the
data for making model ensembles, and the number of models
to generate for making statistical conclusions). For the dysts
database, it shows additionally how to aggregate all the equa-
tions and their associated dynamical properties. The entirety
of the results in this work can be reproduced by running the
run_all.py script in the same directory.
The PySINDy code is regularly updated with new methods
from the literature and now contains many SINDy variationsand advanced functionality. New algorithms, feature libraries,
subsampling strategies, and more can be readily added into
the PySINDy code and then immediately used with the scripts
mentioned above. Much of the backend for forming general
SINDy libraries and new optimizers is already pre-made in the
code and should signiﬁcantly reduce the overhead designing
and testing new algorithms.
Appendix B: Small model errors and associated instability
True sparse regression is a nonconvex problem because of
thel0loss term in the objective function. The l1loss term is
often used in place of the l0since, under certain conditions, it
will produce the same solution with high probability [134].
When these conditions are not met, the Lasso is known to
weakly choose irrelevant features [135]. Furthermore, even
the optimally-solved l0-regularized problem can fail to pre-
dict exactly the correct features when the data is sufﬁciently
corrupted by noise. In the context of system identiﬁcation,
small model errors can generate signiﬁcant issues.
For instance, very small errors in the model identiﬁcation
can cause ﬁnite-time and unphysical instability when gener-
ating new trajectories from the identiﬁed models (although
this is not an issue if the user merely seeks to ﬁnd an ap-
proximation of the equations rather than to usethem for gen-
erating new trajectories). In the case of fourth order poly-
nomials, we can use the following heuristic. We would ex-
pect that small coefﬁcient errors on the third and fourth order
terms will almost inevitably produce instability on some tra-
jectory since these terms will be negligible near the origin (or
near the attractor) but an initial condition can always be cho-
sen far enough from the origin in order to make high order
terms dominant. Only in the constrained [33] and stability-
promoted [82] settings should we expect that most (or all)
trajectories generated from the identiﬁed models are stable.
Finite-time blow up of the solution obviously prevents a rea-
sonable comparison of the RMSE error of X(rather than ˙X)
and additionally prevents calculation of the Lyapunov spec-
trum and other dynamical properties of interest.
There are two half-remedies: only test the model with tra-
jectories with initial conditions reasonably close to the strange
attractor(s) and avoid rescaling the SINDy matrix. The ﬁrst
remedy comes from the intuition that small coefﬁcient errors
on high-order terms will only become dominant in regions of
the state space sufﬁciently far from the origin. The latter rem-
edy is also intuitive if we consider the STLSQ algorithm. It
has been previously noted, e.g. in Delahunt et al. [85], that
ifx(t)10 on the strange attractor, the two terms x(t)and
0:1x2(t)will contribute similarly to the time evolution, yet
the latter term will be the ﬁrst term to be truncated by an al-
gorithm that relies on some version of hard-thresholding the
smallest coefﬁcients in the equations. Rather than seeing this
as a problem of the algorithm, we can view this instead as
an algorithmic bias towards describing each system with low
order polynomial terms, which we expect should contribute
indirectly to boundedness. Unfortunately, the opposite is true
ifx(t)0:1, so it can be useful to rescale the different dy-12
namical systems so that this thresholding mismatch is all in
the same direction for the entire database. Although not ex-
plored in this work, minimizing an objective function related
to the Nt-step SINDy model prediction as in e.g. Kaheman et
al. [87] or Bakarji et al. [89], rather than the traditional single
time-step prediction, should generally improve the model sta-
bility but at the expense of nonconvexity (it is no longer sparse
linear regression).
Lastly, even when the identiﬁed models are essentially per-
fect and reproduce the strange attractor(s), reporting the X
RMSE error for chaotic systems can often be profoundly mis-
leading, since inevitably there will be (initially exponentially
large and then bounded) errors in the trajectory RMSEs from
tiny errors in the integration, identiﬁed coefﬁcients, and so
on. We found it much more convincing to simply show vi-
sually that, with new initial conditions, the identiﬁed models
can approximately reproduce the correct attractors, and there-
fore accurate calculations of the Lyapunov spectrum and other
dynamical properties are possible.
Appendix C: Dynamical property deﬁnitions
In this section we provide precise deﬁnitions for the dynam-
ical properties computed and analyzed in the present work.
1.The amount of chaos can be measured in numerous
ways [1], and a common choice is the largest Lya-
punov exponent. The largest Lyapunov exponent mea-
sures the rate at which trajectories from two initially
inﬁnitesimally-close points exponentially separate as
time evolves [136]. This exponent can have very large
variation over a trajectory, so it is usually considered
as a global average. There is some evidence [1] that
deep symbolic regression [119] and recurrent neural
networks see degraded performance at higher levels of
chaos [130].
2.The degree of scale separation could be deﬁned through
a ratio of the largest timescale to smallest timescale in
the underlying equations. However, these timescales
are not typically well-deﬁned even for polynomially
nonlinear systems, so we could instead use the ratio
of the largest to smallest Lyapunov exponents. As be-
fore, the Lyapunov spectrum can often vary tremen-
dously over a trajectory and therefore this is quite a
coarse measure of scale separation. For a much im-
proved metric to measure scale separation, we use the
dominant timescale divided by the smallest signiﬁcant
timescale as deﬁned in Gilpin [1]. To see why this mea-
sure of scale separation might be dynamically relevant
to symbolic regression, consider the following simpli-
ﬁed model
˙x1=f(x1;x2); (C1)
˙x2=1
eg(x1;x2);
fore1 and assuming the existence of a strange at-tractor, as in the case for our dataset. Following the
standard derivation for invariant manifolds [137], on
long time intervals (compared to the etimescale) x2(t)
quickly ﬁnds an equilibrium parametrized as x2(t)
h(x1(t)), and is functionally controlled by the x1(t)
evolution. The conclusion is the existence of an invari-
ant manifold deﬁned by
˙x1=f(x1;h(x1)); (C2)
˙x2=¶x1h(x1)˙x1=¶x1h(x1)f(x1;h(x1));
which is a manifold in the (x1;x2)state space. If few
data points are available for the period when te(or,
equivalently, if the initial value x2(0)starts close to the
equilibrium or strange attractor), we cannot reasonably
expect that Eqs. (C1) and (C2) can be distinguished. In
other words, we might expect that for the best symbolic
regression results (i.e. results that correctly capture the
dynamical terms necessary to converge to the strange
attractor from initial conditions starting substantially
off the attractor) we need to use initial conditions far
from the equilibrium, and resolved at the smallest time
scale. This intuition is given some additional practical
and theoretical justiﬁcation in Bucci et al. [138]. It is
shown explicitly that trajectories from linearly unstable
ﬁxed points contain less entropy, i.e. less information,
than trajectories starting on the attractors, since near the
ﬁxed points only the linear terms are active and infor-
mation about the nonlinearities cannot be gleaned. Sim-
ilarly, on the attractor there is a balance of terms that
reduces the information available about the equation
terms, relative to some generic trajectory in the phase
space. Some chaotic systems can have effective dimen-
sions on the attractor that are signiﬁcantly smaller than
the state space dimension.
There is another important note to make here. We know
that Eqs. (C1) are discoverable with the pre-deﬁned,
fourth-order polynomial feature library used for sym-
bolic regression. This is not guaranteed for the equa-
tions deﬁning the invariant manifold, and thus we may
still ﬁnd the correct equations simply because the more
speciﬁc invariant manifold equations are not accessible
in the regression. Proceeding down this line of analysis
is complex and further complicated by the fact that of-
ten polynomial terms are very close to linearly depen-
dent and therefore multiple sparse models may ﬁt the
data well [85].
In order to sample trajectories while they are off the
attractor, i.e. when te, we need to sample each
period such thate 1points are generated, greatly
increasing the required sampling resolution. Alterna-
tively, a more sophisticated weighting strategy may be
employed. There has been some recent symbolic re-
gression work [139–142] connecting system identiﬁca-
tion and invariant manifolds. An interesting future in-
vestigation could attempt to identify the map h(x1(t))
by searching for Eq. (C2) with the constraint that library
terms for ˙ x2must be proportional to ˙ x1. These types of13
constraints are already implemented in PySINDy.
3.The syntactic complexity of the model has been consid-
ered in earlier work as useful for both direct optimiza-
tion and as a post-ﬁt metric [143]. There are many pos-
sible deﬁnitions of syntactic complexity, but we adopt
the approximate description-length metric [144] since
the description length has been used in a number of con-
texts for direct optimization and has a natural interpre-
tation as approximating the number of bits to describe
each object in the equations.
4.The amount of nonlinearity is also not typically well-
deﬁned. The highest degree of nonlinearity can be de-
ﬁned as the largest polynomial appearing in the equa-
tions governing each system, and previous work has
used this deﬁnition for a complexity measure in sparse
regression [145]. However, some systems have many
quadratic nonlinear terms, and we might consider such
systems very nonlinear despite the lack of higher-order
polynomials. As a basic metric to trade-off between
these considerations, we consider a weight vector wi=[1;2;3;4;5]and for each system simply report
Nnonlinearity =d
å
j=15
å
i=1wikxi 1terms in equation jk0:(C3)
In other words, we sum all of the terms present in the
coupled set of ODEs, weighted by the polynomial de-
gree of each term. This includes the linear and constant
terms, so it is a mix between the degree of nonlinearity
and the number of equation terms. Because this deﬁ-
nition takes into account the number of terms, there is
presumably some overlap with the information captured
by the description length metric.
We make no claims that any of these metrics are the most
compelling choice for capturing the particular dynamical in-
formation, but each metric seems to capture an important as-
pect. A substantial discussion on some of these points can be
found in Murdoch et al. [146]. Future work could investigate
more sophisticated metrics such as the stiffness or integrabil-
ity of the identiﬁed ODE systems, or investigating the corre-
sponding stability features of the models. For generic nonlin-
ear ODE systems, investigating stability seems intractable, but
there is a signiﬁcant volume of literature for ﬁnding Lyapunov
functions for polynomially nonlinear systems [147, 148]. Op-
timizing stiffness [149] and stability would be additionally
useful for ﬁnding models that can be used for applying chaos
control [150].
[1] W. Gilpin, Chaos as an interpretable benchmark for fore-
casting and data-driven modelling, Advances in Neural In-
formation Processing Systems (NeurIPS), arXiv:2110.05266
(2021).
[2] P. J. Schmid, Dynamic mode decomposition of numerical and
experimental data, Journal of Fluid Mechanics 656, 5 (2010).
[3] C. W. Rowley, I. Mezi ´c, S. Bagheri, P. Schlatter, and D. Hen-
ningson, Spectral analysis of nonlinear ﬂows, Journal of ﬂuid
mechanics 641, 115 (2009).
[4] J. N. Kutz, S. L. Brunton, B. W. Brunton, and J. L. Proctor, Dy-
namic Mode Decomposition: Data-Driven Modeling of Com-
plex Systems (SIAM, 2016).
[5] I. Mezi ´c, Spectral properties of dynamical systems, model
reduction and decompositions, Nonlinear Dynamics 41, 309
(2005).
[6] S. L. Brunton, M. Budiši ´c, E. Kaiser, and J. N. Kutz, Modern
Koopman theory for dynamical systems, SIAM Review 64,
229 (2022).
[7] S. A. Billings, Nonlinear system identiﬁcation: NARMAX
methods in the time, frequency, and spatio-temporal domains
(John Wiley & Sons, 2013).
[8] J. Pathak, B. Hunt, M. Girvan, Z. Lu, and E. Ott, Model-
free prediction of large spatiotemporally chaotic systems from
data: a reservoir computing approach, Physical review letters
120, 024102 (2018).
[9] P. R. Vlachas, W. Byeon, Z. Y . Wan, T. P. Sapsis,
and P. Koumoutsakos, Data-driven forecasting of high-
dimensional chaotic systems with long short-term memory
networks, Proc. R. Soc. A 474, 20170844 (2018).[10] M. Raissi, P. Perdikaris, and G. Karniadakis, Physics-
informed neural networks: A deep learning framework for
solving forward and inverse problems involving nonlinear par-
tial differential equations, Journal of Computational Physics
378, 686 (2019).
[11] M. Raissi, P. Perdikaris, and G. E. Karniadakis, Machine
learning of linear differential equations using Gaussian pro-
cesses, Journal of Computational Physics 348, 683 (2017).
[12] P. Benner, S. Gugercin, and K. Willcox, A survey of
projection-based model reduction methods for parametric dy-
namical systems, SIAM review 57, 483 (2015).
[13] B. Peherstorfer and K. Willcox, Data-driven operator infer-
ence for nonintrusive projection-based model reduction, Com-
puter Methods in Applied Mechanics and Engineering 306,
196 (2016).
[14] E. Qian, B. Kramer, B. Peherstorfer, and K. Willcox, Lift
& Learn: Physics-informed machine learning for large-scale
nonlinear dynamical systems, Physica D: Nonlinear Phenom-
ena406, 132401 (2020).
[15] J. Bongard and H. Lipson, Automated reverse engineering of
nonlinear dynamical systems, Proc. Natl. Acad. Sciences 104,
9943 (2007).
[16] M. Schmidt and H. Lipson, Distilling free-form natural laws
from experimental data, Science 324, 81 (2009).
[17] S.-M. Udrescu and M. Tegmark, AI Feynman: A physics-
inspired method for symbolic regression, Science Advances
6, eaay2631 (2020).
[18] S. L. Brunton, J. L. Proctor, and J. N. Kutz, Discovering gov-
erning equations from data by sparse identiﬁcation of nonlin-14
ear dynamical systems, Proceedings of the National Academy
of Sciences 113, 3932 (2016).
[19] M. Hoffmann, C. Fröhner, and F. Noé, Reactive SINDy: Dis-
covering governing reactions from concentration data, The
Journal of chemical physics 150, 025101 (2019).
[20] B. Bhadriraju, M. S. F. Bangi, A. Narasingam, and J. S.-
I. Kwon, Operable adaptive sparse identiﬁcation of systems:
Application to chemical processes, AIChE Journal 66, e16980
(2020).
[21] L. Scheffold, T. Finkler, and U. Piechottka, Gray-box system
modeling using symbolic regression and nonlinear model pre-
dictive control of a semibatch polymerization, Computers &
Chemical Engineering 146, 107204 (2021).
[22] J. Rubio-Herrero, C. O. Marrero, and W.-T. L. Fan, Model-
ing atmospheric data and identifying dynamics temporal data-
driven modeling of air pollutants, Journal of Cleaner Produc-
tion333, 129863 (2022).
[23] J. H. Lagergren, J. T. Nardini, G. Michael Lavigne, E. M. Rut-
ter, and K. B. Flores, Learning partial differential equations for
biological transport models from noisy spatio-temporal data,
Proceedings of the Royal Society A 476, 20190800 (2020).
[24] M. Pasquato, M. Abbas, A. A. Trani, M. Nori, J. A. Kwiecin-
ski, P. Trevisan, V . F. Braga, G. Bono, and A. V . Macciò,
Sparse identiﬁcation of variable star dynamics, The Astro-
physical Journal 930, 161 (2022).
[25] Y .-X. Jiang, X. Xiong, S. Zhang, J.-X. Wang, J.-C. Li, and
L. Du, Modeling and prediction of the transmission dynam-
ics of COVID-19 based on the SINDy-LM method, Nonlinear
Dynamics 105, 2775 (2021).
[26] V . Zucatti, H. F. Lui, D. B. Pitz, and W. R. Wolf, Assessment of
reduced-order modeling strategies for convective heat transfer,
Numerical Heat Transfer, Part A: Applications 77, 702 (2020).
[27] M. Sorokina, S. Sygletos, and S. Turitsyn, Sparse identiﬁ-
cation for nonlinear optical communication systems: SINO
method, Optics express 24, 30433 (2016).
[28] A. M. Stankovi ´c, A. A. Sari ´c, A. T. Sari ´c, and M. K.
Transtrum, Data-driven symbolic regression for identiﬁcation
of nonlinear dynamics in power systems, in 2020 IEEE Power
& Energy Society General Meeting (PESGM) (IEEE, 2020)
pp. 1–5.
[29] Y . Cai, X. Wang, G. Joos, and I. Kamwa, An online data-
driven method to locate forced oscillation sources from power
plants based on sparse identiﬁcation of nonlinear dynamics
(SINDy), IEEE Transactions on Power Systems (2022).
[30] A. Narasingam and J. S.-I. Kwon, Data-driven identiﬁcation
of interpretable reduced-order models using sparse regression,
Computers & Chemical Engineering 119, 101 (2018).
[31] S. Thaler, L. Paehler, and N. A. Adams, Sparse identiﬁcation
of truncation errors, J. Comput. Phys. 397(2019).
[32] R. Dale and H. S. Bhat, Equations of mind: Data science for
inferring nonlinear dynamics of socio-cognitive systems, Cog-
nitive Systems Research 52, 275 (2018).
[33] J.-C. Loiseau and S. L. Brunton, Constrained sparse Galerkin
regression, Journal of Fluid Mechanics 838, 42 (2018).
[34] J.-C. Loiseau, B. R. Noack, and S. L. Brunton, Sparse reduced-
order modeling: sensor-based dynamics to full-state estima-
tion, Journal of Fluid Mechanics 844, 459 (2018).
[35] J.-C. Loiseau, Data-driven modeling of the chaotic thermal
convection in an annular thermosyphon, Theoretical and Com-
putational Fluid Dynamics 34, 339 (2020).
[36] Y . El Sayed M, R. Semaan, and R. Radespiel, Sparse model-
ing of the lift gains of a high-lift conﬁguration with periodic
coanda blowing, in 2018 AIAA Aerospace Sciences Meeting
(2018) p. 1054.[37] H. Chang and D. Zhang, Machine learning subsurface ﬂow
equations from data, Computational Geosciences 23, 895
(2019).
[38] N. Deng, B. R. Noack, M. Morzy ´nski, and L. R. Pastur, Low-
order model for successive bifurcations of the ﬂuidic pinball,
Journal of Fluid Mechanics 884, A37 (2020).
[39] K. Fukami, T. Murata, K. Zhang, and K. Fukagata,
Sparse identiﬁcation of nonlinear dynamics with low-
dimensionalized ﬂow representations, Journal of Fluid Me-
chanics 926(2021).
[40] J. L. Callaham, S. L. Brunton, and J.-C. Loiseau, On the role
of nonlinear correlations in reduced-order modelling, Journal
of Fluid Mechanics 938(2022).
[41] Z. C. Khoo, C. H. Chan, and Y . Hwang, A sparse optimal clo-
sure for a reduced-order model of wall-bounded turbulence,
Journal of Fluid Mechanics 939(2022).
[42] N. Deng, B. R. Noack, M. Morzy ´nski, and L. R. Pastur,
Galerkin force model for transient and post-transient dynamics
of the ﬂuidic pinball, Journal of Fluid Mechanics 918(2021).
[43] Q. Xiao, J. Wang, X. Yang, and B. Jiang, Construction of
a reduced-order model of an electroosmotic micromixer and
discovery of attractors for petal structure, Physics of Fluids
(2023).
[44] J. A. Foster, J. Decuyper, T. De Troyer, and M. Runacres, Esti-
mating a sparse nonlinear dynamical model of the ﬂow around
an oscillating cylinder in a ﬂuid ﬂow using SINDy, in Confer-
ence on Noise and Vibration Engineering ISMA 2022 (ISMA
2022, 2022) p. tba.
[45] M. Schmelzer, R. P. Dwight, and P. Cinnella, Discovery of al-
gebraic Reynolds-stress models using sparse symbolic regres-
sion, Flow, Turbulence and Combustion 104, 579 (2020).
[46] S. Beetham and J. Capecelatro, Formulating turbulence clo-
sures using sparse regression with embedded form invariance,
Physical Review Fluids 5, 084611 (2020).
[47] S. Beetham and J. Capecelatro, Multiphase turbulence model-
ing using sparse regression and gene expression programming,
arXiv preprint arXiv:2106.10397 (2021).
[48] S. Beetham, R. O. Fox, and J. Capecelatro, Sparse identiﬁ-
cation of multiphase turbulence closures for coupled ﬂuid–
particle ﬂows, Journal of Fluid Mechanics 914(2021).
[49] J. L. Callaham, G. Rigas, J.-C. Loiseau, and S. L. Brunton, An
empirical mean-ﬁeld model of symmetry-breaking in a turbu-
lent wake, Science Advances 8, eabm4786 (2022).
[50] A. Sansica, J.-C. Loiseau, M. Kanamori, A. Hashimoto, and
J.-C. Robinet, System identiﬁcation of two-dimensional tran-
sonic buffet, AIAA Journal 60, 3090 (2022).
[51] M. Dam, M. Brøns, J. Juul Rasmussen, V . Naulin, and J. S.
Hesthaven, Sparse identiﬁcation of a predator-prey system
from simulation data of a convection model, Physics of Plas-
mas24, 022310 (2017).
[52] A. A. Kaptanoglu, K. D. Morgan, C. J. Hansen, and S. L.
Brunton, Physics-constrained, low-dimensional models for
magnetohydrodynamics: First-principles and data-driven ap-
proaches, Physical Review E 104, 015206 (2021).
[53] E. P. Alves and F. Fiuza, Data-driven discovery of reduced
plasma physics models from fully kinetic simulations, Phys-
ical Review Research 4, 033192 (2022).
[54] Z. Lai and S. Nagarajaiah, Sparse structural system identiﬁ-
cation method for nonlinear dynamic systems with hystere-
sis/inelastic behavior, Mechanical Systems and Signal Pro-
cessing 117, 813 (2019).
[55] B. M. de Silva, D. M. Higdon, S. L. Brunton, and J. N. Kutz,
Discovery of physics from data: universal laws and discrepan-
cies, Frontiers in artiﬁcial intelligence 3, 25 (2020).15
[56] S. Pan, N. Arnold-Medabalimi, and K. Duraisamy, Sparsity-
promoting algorithms for the discovery of informative
Koopman-invariant subspaces, Journal of Fluid Mechanics
917(2021).
[57] R. Subramanian, R. R. Moar, and S. Singh, White-box ma-
chine learning approaches to identify governing equations for
overall dynamics of manufacturing systems: A case study on
distillation column, Machine Learning with Applications 3,
100014 (2021).
[58] M. Brenner, F. Hess, J. M. Mikhaeil, L. F. Bereska, Z. Mon-
fared, P.-C. Kuo, and D. Durstewitz, Tractable dendritic RNNs
for reconstructing nonlinear dynamical systems, in Interna-
tional Conference on Machine Learning (PMLR, 2022) pp.
2292–2320.
[59] S. Zhang, F. Ahamed, and H.-S. Song, Knowledge-informed
data-driven modeling for sparse identiﬁcation of governing
equations for microbial inactivation processes in food, Fron-
tiers in Food Science and Technology , 29 (2022).
[60] M. Golden, R. Grigoriev, J. Nambisan, and A. Fernandez-
Nieves, Physically-informed data-driven modeling of active
nematics, arXiv preprint arXiv:2202.12853 (2022).
[61] C. Joshi, S. Ray, L. M. Lemma, M. Varghese, G. Sharp,
Z. Dogic, A. Baskaran, and M. F. Hagan, Data-driven discov-
ery of active nematic hydrodynamics, Physical review letters
129, 258001 (2022).
[62] H. Schaeffer, Learning partial differential equations via data
discovery and sparse optimization, in Proc. R. Soc. A , V ol. 473
(The Royal Society, 2017) p. 20160446.
[63] S. H. Rudy, S. L. Brunton, J. L. Proctor, and J. N. Kutz, Data-
driven discovery of partial differential equations, Science Ad-
vances 3, e1602614 (2017).
[64] A. Sandoz, V . Ducret, G. A. Gottwald, G. Vilmart, and K. Per-
ron, SINDy for delay-differential equations: application to
model bacterial zinc response, Proceedings of the Royal So-
ciety A 479, 20220556 (2023).
[65] A. Klimovskaia, S. Ganscha, and M. Claassen, Sparse regres-
sion based structure learning of stochastic reaction networks
from single cell snapshot time series, PLoS computational bi-
ology 12, e1005234 (2016).
[66] D. B. Brückner, P. Ronceray, and C. P. Broedersz, Inferring the
dynamics of underdamped stochastic systems, Physical review
letters 125, 058103 (2020).
[67] M. Dai, T. Gao, Y . Lu, Y . Zheng, and J. Duan, Detecting the
maximum likelihood transition path from data of stochastic
dynamical systems, Chaos: An Interdisciplinary Journal of
Nonlinear Science 30, 113124 (2020).
[68] J. L. Callaham, J.-C. Loiseau, G. Rigas, and S. L. Brun-
ton, Nonlinear stochastic modelling with Langevin regression,
Proceedings of the Royal Society A 477, 20210092 (2021).
[69] Y . Huang, Y . Mabrouk, G. Gompper, and B. Sabass, Sparse in-
ference and active learning of stochastic differential equations
from data, Scientiﬁc Reports 12, 21691 (2022).
[70] S. M. Hirsh, D. A. Barajas-Solano, and J. N. Kutz, Sparsi-
fying priors for Bayesian uncertainty quantiﬁcation in model
discovery, Royal Society Open Science 9, 211823 (2022).
[71] J. Bakarji, J. Callaham, S. L. Brunton, and J. N. Kutz, Di-
mensionally consistent learning with buckingham pi, Nature
Computational Science , 1 (2022).
[72] S. L. Brunton, J. L. Proctor, and J. N. Kutz, Sparse identiﬁ-
cation of nonlinear dynamics with control (SINDYc), IFAC-
PapersOnLine 49, 710 (2016).
[73] E. Kaiser, J. N. Kutz, and S. L. Brunton, Sparse identiﬁcation
of nonlinear dynamics for model predictive control in the low-
data limit, Proceedings of the Royal Society of London A 474(2018).
[74] E. Kaiser, J. N. Kutz, and S. L. Brunton, Discovering conser-
vation laws from data for control, in 2018 IEEE Conference
on Decision and Control (CDC) (IEEE, 2018) pp. 6415–6421.
[75] N. M. Mangan, S. L. Brunton, J. L. Proctor, and J. N. Kutz,
Inferring biological networks by sparse identiﬁcation of non-
linear dynamics, IEEE Transactions on Molecular, Biological,
and Multi-Scale Communications 2, 52 (2016).
[76] K. Kaheman, J. N. Kutz, and S. L. Brunton, SINDy-PI: a
robust algorithm for parallel implicit sparse identiﬁcation of
nonlinear dynamics, Proceedings of the Royal Society A 476,
20200279 (2020).
[77] N. M. Mangan, T. Askham, S. L. Brunton, J. N. Kutz, and
J. L. Proctor, Model selection for hybrid dynamical systems
via sparse regression, Proceedings of the Royal Society A 475,
20180534 (2019).
[78] G. Thiele, A. Fey, D. Sommer, and J. Krüger, System identiﬁ-
cation of a hysteresis-controlled pump system using SINDy, in
2020 24th International Conference on System Theory, Con-
trol and Computing (ICSTCC) (IEEE, 2020) pp. 457–464.
[79] K. Champion, P. Zheng, A. Y . Aravkin, S. L. Brunton, and
J. N. Kutz, A uniﬁed sparse optimization framework to learn
parsimonious physics-informed models from data, IEEE Ac-
cess 8, 169259 (2020).
[80] N. M. Mangan, J. N. Kutz, S. L. Brunton, and J. L. Proc-
tor, Model selection for dynamical systems via sparse regres-
sion and information criteria, Proceedings of the Royal Soci-
ety A: Mathematical, Physical and Engineering Sciences 473,
20170009 (2017).
[81] X. Dong, Y .-L. Bai, Y . Lu, and M. Fan, An improved sparse
identiﬁcation of nonlinear dynamics with Akaike information
criterion and group sparsity, Nonlinear Dynamics , 1 (2022).
[82] A. A. Kaptanoglu, J. L. Callaham, A. Aravkin, C. J. Hansen,
and S. L. Brunton, Promoting global stability in data-driven
models of quadratic nonlinear dynamics, Physical Review Flu-
ids6, 094401 (2021).
[83] G. Tran and R. Ward, Exact recovery of chaotic systems from
highly corrupted data, Multiscale Modeling & Simulation 15,
1108 (2017).
[84] H. Schaeffer, G. Tran, and R. Ward, Extracting sparse high-
dimensional dynamics from limited data, SIAM Journal on
Applied Mathematics 78, 3279 (2018).
[85] C. B. Delahunt and J. N. Kutz, A toolkit for data-driven dis-
covery of governing equations in high-noise regimes, IEEE
Access 10, 31210 (2022).
[86] J. Wentz and A. Doostan, Derivative-based SINDy (DSINDy):
Addressing the challenge of discovering governing equations
from noisy data, arXiv preprint arXiv:2211.05918 (2022).
[87] K. Kaheman, S. L. Brunton, and J. N. Kutz, Automatic dif-
ferentiation to simultaneously identify nonlinear dynamics
and extract noise probability distributions from data, Machine
Learning: Science and Technology 3, 015031 (2022).
[88] A. Somacal, Y . Barrera, L. Boechi, M. Jonckheere, V . Leﬁeux,
D. Picard, and E. Smucler, Uncovering differential equations
from data with hidden variables, Physical Review E 105,
054209 (2022).
[89] J. Bakarji, K. Champion, J. N. Kutz, and S. L. Brunton, Dis-
covering governing equations from partial measurements with
deep delay autoencoders, arXiv preprint arXiv:2201.05136
(2022).
[90] P. Conti, G. Gobat, S. Fresca, A. Manzoni, and A. Frangi,
Reduced order modeling of parametrized systems through au-
toencoders and SINDy approach: continuation of periodic so-
lutions, arXiv preprint arXiv:2211.06786 (2022).16
[91] L. Gao and J. N. Kutz, Bayesian autoencoders for data-driven
discovery of coordinates, governing equations and fundamen-
tal constants, arXiv preprint arXiv:2211.10575 (2022).
[92] Z. Zhao and Q. Li, Adaptive sampling methods for learning
dynamical systems, in Mathematical and Scientiﬁc Machine
Learning (PMLR, 2022) pp. 335–350.
[93] K. Wu and D. Xiu, Numerical aspects for approximating gov-
erning equations using data, Journal of Computational Physics
384, 200 (2019).
[94] U. Fasel, J. N. Kutz, B. W. Brunton, and S. L. Brunton,
Ensemble-SINDy: Robust sparse model discovery in the low-
data, high-noise limit, with active learning and control, Pro-
ceedings of the Royal Society A 478, 20210904 (2022).
[95] L. Gao, U. Fasel, S. L. Brunton, and J. N. Kutz, Convergence
of uncertainty estimates in ensemble and Bayesian sparse
model discovery, arXiv preprint arXiv:2301.12649 (2023).
[96] F. Jiang, L. Du, F. Yang, and Z.-C. Deng, Regularized least ab-
solute deviation-based sparse identiﬁcation of dynamical sys-
tems, Chaos: An Interdisciplinary Journal of Nonlinear Sci-
ence 33, 013103 (2023).
[97] H. Schaeffer and S. G. McCalla, Sparse model selection via
integral terms, Physical Review E 96, 023302 (2017).
[98] D. R. Gurevich, P. A. Reinbold, and R. O. Grigoriev, Ro-
bust and optimal sparse regression for nonlinear PDE models,
Chaos: An Interdisciplinary Journal of Nonlinear Science 29,
103113 (2019).
[99] P. A. Reinbold, D. R. Gurevich, and R. O. Grigoriev, Using
noisy or incomplete data to discover models of spatiotemporal
dynamics, Physical Review E 101, 010203 (2020).
[100] D. A. Messenger and D. M. Bortz, Weak SINDy for par-
tial differential equations, Journal of Computational Physics
, 110525 (2021).
[101] L. M. Kageorge, R. O. Grigoriev, and M. F. Schatz, Data-
driven detection of drifting system parameters, arXiv preprint
arXiv:2111.12114 (2021).
[102] P. A. Reinbold, L. M. Kageorge, M. F. Schatz, and R. O.
Grigoriev, Robust learning from noisy, incomplete, high-
dimensional experimental data via physically constrained
symbolic regression, Nature communications 12, 1 (2021).
[103] D. R. Gurevich, P. A. Reinbold, and R. O. Grigoriev, Learning
ﬂuid physics from highly turbulent data using sparse physics-
informed discovery of empirical relations (SPIDER), arXiv
preprint arXiv:2105.00048 (2021).
[104] B. Russo and M. P. Laiu, Convergence of weak-SINDy surro-
gate models, arXiv preprint arXiv:2209.15573 (2022).
[105] D. A. Messenger and D. M. Bortz, Asymptotic consistency of
the WSINDy algorithm in the limit of continuum data, arXiv
preprint arXiv:2211.16000 (2022).
[106] D. A. Messenger and D. M. Bortz, Learning mean-ﬁeld equa-
tions from particle data using WSINDy, Physica D: Nonlinear
Phenomena 439, 133406 (2022).
[107] D. A. Messenger, E. Dall’Anese, and D. Bortz, Online weak-
form sparse identiﬁcation of partial differential equations, in
Mathematical and Scientiﬁc Machine Learning (PMLR, 2022)
pp. 241–256.
[108] P. Gelß, S. Klus, J. Eisert, and C. Schütte, Multidimensional
approximation of nonlinear dynamical systems, Journal of
Computational and Nonlinear Dynamics 14(2019).
[109] A. Goeßmann, M. Götte, I. Roth, R. Sweke, G. Kutyniok, and
J. Eisert, Tensor network approaches for learning non-linear
dynamical laws, arXiv preprint arXiv:2002.12388 (2020).
[110] L. Boninsegna, F. Nüske, and C. Clementi, Sparse learning
of stochastic dynamical equations, The Journal of Chemical
Physics 148, 241723 (2018).[111] E. N. Lorenz, Deterministic nonperiodic ﬂow, Journal of at-
mospheric sciences 20, 130 (1963).
[112] B. de Silva, K. Champion, M. Quade, J.-C. Loiseau, J. N.
Kutz, and S. Brunton, PySINDy: A Python package for the
sparse identiﬁcation of nonlinear dynamical systems from
data, Journal of Open Source Software 5, 1 (2020).
[113] A. A. Kaptanoglu, B. M. de Silva, U. Fasel, K. Kaheman,
A. J. Goldschmidt, J. Callaham, C. B. Delahunt, Z. G. Nico-
laou, K. Champion, J.-C. Loiseau, J. N. Kutz, and S. L. Brun-
ton, PySINDy: A comprehensive Python package for robust
sparse system identiﬁcation, Journal of Open Source Software
7, 3994 (2022).
[114] P. Zheng, T. Askham, S. L. Brunton, J. N. Kutz, and A. Y .
Aravkin, A uniﬁed framework for sparse relaxed regularized
regression: SR3, IEEE Access 7, 1404 (2019).
[115] R. Tibshirani, M. Wainwright, and T. Hastie, Statistical learn-
ing with sparsity: the lasso and generalizations (Chapman and
Hall/CRC, 2015).
[116] D. Bertsimas and W. Gurnee, Learning sparse nonlinear dy-
namics via mixed-integer optimization, Nonlinear Dynamics ,
1 (2023).
[117] H. Schaeffer, G. Tran, and R. Ward, Learning dynamical
systems and bifurcation via group sparsity, arXiv preprint
arXiv:1709.01558 (2017).
[118] N. Q. Uy, N. X. Hoai, M. O’Neill, R. I. McKay, and E. Galván-
López, Semantically-based crossover in genetic programming:
application to real-valued symbolic regression, Genetic Pro-
gramming and Evolvable Machines 12, 91 (2011).
[119] B. K. Petersen, M. L. Larma, T. N. Mundhenk, C. P. Santiago,
S. K. Kim, and J. T. Kim, Deep symbolic regression: Recover-
ing mathematical expressions from data via risk-seeking pol-
icy gradients, in International Conference on Learning Repre-
sentations (2021).
[120] H. Kantz and T. Schreiber, Nonlinear time series analysis ,
V ol. 7 (Cambridge university press, 2004).
[121] A. Kaptanoglu, An Exploration of Data-Driven System Identi-
ﬁcation and Machine Learning for Plasma Physics (University
of Washington, 2021).
[122] F. Van Breugel, J. N. Kutz, and B. W. Brunton, Numerical
differentiation of noisy data: A unifying multi-objective opti-
mization framework, IEEE Access 8, 196865 (2020).
[123] F. Van Breugel, Y . Liu, B. W. Brunton, and J. N. Kutz,
Pynumdiff: A python package for numerical differentiation
of noisy time-series data, Journal of Open Source Software 7,
4078 (2022).
[124] X. Blasco, J. M. Herrero, J. Sanchis, and M. Martínez, A
new graphical visualization of n-dimensional Pareto front for
decision-making in multiobjective optimization, Information
Sciences 178, 3908 (2008).
[125] W. La Cava, P. Orzechowski, B. Burlacu, F. O. de Franca,
M. Virgolin, Y . Jin, M. Kommenda, and J. H. Moore, Contem-
porary symbolic regression methods and their relative perfor-
mance, in Thirty-ﬁfth Conference on Neural Information Pro-
cessing Systems Datasets and Benchmarks Track (Round 1) .
[126] P. Orzechowski, W. La Cava, and J. H. Moore, Where are we
now? A large benchmark study of recent symbolic regres-
sion methods, in Proceedings of the Genetic and Evolutionary
Computation Conference (2018) pp. 1183–1190.
[127] H. S. Bhat, Learning and interpreting potentials for classi-
cal Hamiltonian systems, in Joint European Conference on
Machine Learning and Knowledge Discovery in Databases
(Springer, 2019) pp. 217–228.
[128] H. K. Chu and M. Hayashibe, Discovering interpretable dy-
namics by sparsity promotion on energy and the Lagrangian,17
IEEE Robotics and Automation Letters 5, 2154 (2020).
[129] T. Bertalan, F. Dietrich, I. Mezi ´c, and I. G. Kevrekidis, On
learning Hamiltonian systems from data, Chaos: An Interdis-
ciplinary Journal of Nonlinear Science 29, 121107 (2019).
[130] J. M. Mikhaeil, Z. Monfared, and D. Durstewitz, On the difﬁ-
culty of learning chaotic dynamics with RNNs, arXiv preprint
arXiv:2110.07238 (2021).
[131] S. Ouala, S. L. Brunton, B. Chapron, A. Pascual, F. Collard,
L. Gaultier, and R. Fablet, Bounded nonlinear forecasts of par-
tially observed geophysical systems with physics-constrained
deep learning, Physica D: Nonlinear Phenomena , 133630
(2023).
[132] S. Diamond and S. Boyd, CVXPY: A Python-embedded mod-
eling language for convex optimization, The Journal of Ma-
chine Learning Research 17, 2909 (2016).
[133] A. A. Kaptanoglu, T. Qian, F. Wechsung, and M. Landreman,
Permanent-magnet optimization for stellarators as sparse re-
gression, Physical Review Applied 18, 044006 (2022).
[134] M. J. Wainwright, Sharp thresholds for high-dimensional and
noisy sparsity recovery using l 1-constrained quadratic pro-
gramming (Lasso), IEEE transactions on information theory
55, 2183 (2009).
[135] D. Bertsimas, J. Pauphilet, and B. V . Parys, Sparse Regres-
sion: Scalable Algorithms and Empirical Performance, Statis-
tical Science 35, 555 (2020).
[136] J. C. Sommerer and E. Ott, Particles ﬂoating on a moving
ﬂuid: A dynamically comprehensible physical fractal, Science
259, 335 (1993).
[137] G. Pavliotis and A. Stuart, Multiscale methods: averaging and
homogenization (Springer Science & Business Media, 2008).
[138] A. Bucci, O. Semeraro, A. Allauzen, S. Chibbaro, and
L. Mathelin, Curriculum learning for data-driven modeling of
dynamical systems (2021).
[139] J. J. Bramburger, D. Dylewsky, and J. N. Kutz, Sparse identi-
ﬁcation of slow timescale dynamics, Physical Review E 102,
022204 (2020).
[140] M. Cenedese, J. Axås, B. Bäuerlein, K. Avila, and G. Haller,Data-driven modeling and prediction of non-linearizable dy-
namics via spectral submanifolds, Nature communications 13,
1 (2022).
[141] R. Szalai, Data-driven reduced order models using invari-
ant foliations, manifolds and autoencoders, arXiv preprint
arXiv:2206.12269 (2022).
[142] J. Axås, M. Cenedese, and G. Haller, Fast data-driven model
reduction for nonlinear dynamical systems, Nonlinear Dynam-
ics , 1 (2022).
[143] S.-M. Udrescu, A. Tan, J. Feng, O. Neto, T. Wu, and
M. Tegmark, AI Feynman 2.0: Pareto-optimal symbolic re-
gression exploiting graph modularity, Advances in Neural In-
formation Processing Systems 33, 4860 (2020).
[144] P. D. Grünwald, I. J. Myung, and M. A. Pitt, Advances in mini-
mum description length: Theory and applications (MIT press,
2005).
[145] E. J. Vladislavleva, G. F. Smits, and D. Den Hertog, Order
of nonlinearity as a complexity measure for models generated
by symbolic regression via pareto genetic programming, IEEE
Transactions on Evolutionary Computation 13, 333 (2008).
[146] W. J. Murdoch, C. Singh, K. Kumbier, R. Abbasi-Asl, and
B. Yu, Deﬁnitions, methods, and applications in interpretable
machine learning, Proceedings of the National Academy of
Sciences 116, 22071 (2019).
[147] A. Tesi, F. Villoresi, and R. Genesio, On the stability domain
estimation via a quadratic Lyapunov function: convexity and
optimality properties for polynomial systems, IEEE Transac-
tions on Automatic Control 41, 1650 (1996).
[148] A. A. Ahmadi, A. Majumdar, and R. Tedrake, Complexity of
ten decision problems in continuous time dynamical systems,
in2013 American Control Conference (IEEE, 2013) pp. 6376–
6381.
[149] H. E. Dikeman, H. Zhang, and S. Yang, Stiffness-reduced neu-
ral ODE models for data-driven reduced-order modeling of
combustion chemical kinetics, in AIAA SCITECH 2022 Forum
(2022) p. 0226.
[150] E. Ott, C. Grebogi, and J. A. Yorke, Controlling chaos, Physi-
cal review letters 64, 1196 (1990).