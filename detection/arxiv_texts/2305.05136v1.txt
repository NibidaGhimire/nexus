Localisation of Mammographic masses by Greedy Backtracking of
Activations in the Stacked Auto-Encoders.
*Shamna Pootheri (Research Fellowa, Former Research Scholarb) and
V K Govindan (Professor Emeritusb)
aNanyang Technological University, Singapore-639798,bNational Institute of Technology Calicut, India-673 601
*Corresponding author: Email: shamnapootheri@gmail.com
Abstract
Mammographic image analysis requires accurate localisation of salient mammographic masses.
In mammographic computer-aided diagnosis, mass or Region of Interest (ROI) is often marked
by physicians and features are extracted from the marked ROI. In this paper, we present a novel
mammographic mass localisation framework, based on the maximal class activations of the stacked
auto-encoders. We hypothesize that the image regions activating abnormal classes in mammo-
graphic images will be the breast masses which causes the anomaly. The experiment is conducted
using randomly selected 200 mammographic images (100 normal and 100 abnormal) from IRMA
mammographic dataset. Abnormal mass regions marked by an expert radiologist are used as the
ground truth. The proposed method outperforms existing Deep Convolutional Neural Network
(DCNN) based techniques in terms of salient region detection accuracy. The proposed greedy
backtracking method is more ecient and does not require a vast number of labelled training
images as in DCNN based method. Such automatic localisation method will assist physicians to
make accurate decisions on biopsy recommendations and treatment evaluations.
Keywords: Salient Region Detection, Interpreting Deep Learned Features, Mammographic
Image Analysis, Auto-encoder
1. Introduction
Breast cancer is one of the noted reasons of cancer-related deaths among women. Mammo-
graphic image analysis is the widely accepted method for breast mass detection and diagnosis. ThearXiv:2305.05136v1  [cs.CV]  9 May 2023detection of mammographic masses is challenging due to the disparity of masses in shape, size,
and low contrast from surrounding tissue.
Deep learning techniques, especially transfer learning and ne-tuning methods, are often used
for mammographic image analysis. Ribil et al. [1] proposed an automatic mass localisation method
using Faster R-CNN based techniques. Faster R-CNN has an additional layer called Region Pro-
posal Network (RPN), on top of the last convolutional layer of the original network, which is
trained to detect and localize salient image regions. The authors used VGG 16 pre-trained DCNN
and ne-tuned the network with DDSM [2] Mammographic images. Even though the network
automatically detects lesions in the test image, the model required pixel-level annotated lesions to
train the network and the performance of the system substantially depends on the quality of the
training images [1]. Rafael et al. [3] proposed a deep transfer learning based mammographic patch
retrieval framework. Features from the last convolution layers of the pre-trained DCNN (ResNet,
Inception-V3, Nas-NetLarge) are used to estimate the similarity between images. The retrieval list
is improved by a query renement strategy using the Query-Point Movement approach (QPM).
The performance of the retrieval system is compared with handcrafted and deep learning based
features. The results of the initial iterations for both hands crafted and deep learned features are
low. The precisions of deep learned features improve after a few iterations using query renement.
The performance of the method is evaluated using manually extracted ROI, and it is reported that
the system requires multiple iterations with query renement to obtain better results.
Recent literature shows that a few works only are trying to interpret DCNN based features [4,
5, 6, 7] to locate salient regions in natural images. In [5], class activation of the initial convolution
layers of DCNN networks is used to visualize and locate the discriminating image regions. Zeiler
et al. [4] occluded the input image by small patches, and the inuence of each image regions on
the class activation is observed to detect the signicant image pixels. Simonyan et al. [6] located
salient image regions by back-propagating the gradients of selected neurons in each layer. During
back-propagation, some paths have a positive impact, and some have a negative impact, and these
paths combine to generate noisy gradients. Whereas, guided back-propagation [7] method keeps
the paths that lead to a positive impact on the class score, and suppress the ones that have a
negative impact, leading to better localisation. The localisation ability of the above methods isnot yet analysed using medical images.
As the DCNN used in the above techniques are trained with millions of natural images, we have
to further analyse the performance of such systems in the medical domain. The main obstacle to
use the DCNN in the medical domain is the unavailability of large collection of images containing
annotated anomaly to train the network. Moreover, deep-learned features are often described as
black boxes since it is dicult to trace a prediction back to the features causing that prediction.
Adapting such features in medical evaluation and diagnosis may have serious legal consequences.
In this paper we try to interpret the deep learned features to locate salient mammographic regions
using unsupervised stacked auto-encoders.
The remaining part of this paper is presented as follows: Section 2 describes methods and
algorithm of the proposed framework. Section 3 presents the experimental details and results of
the proposed system. Section 4 provides comparative discussions of the results of the proposed
method with state-of-the-art systems. Section 5 concludes the work.
2. Material and Method
2.1. Mass Localisation by backtracking the maximally activated neurons
In mammographic computer-aided diagnosis, masses or Region of Interest (ROI) is often marked
by physicians and features are extracted from the marked ROI [8, 3, 9, 10]. In the proposed work
we back-track the maximally activated neuron in each layer of a stacked auto-encoders to locate
ROI in the mammographic images. An auto-encoder is an unsupervised articial feed-forward
neural network used to learn potential image features by reproducing the inputs from the hidden
representations. We trained the auto-encoders consisting of one input layer, two hidden layers
and one output layer to evaluate the performance of the proposed method. Bias is ignored to nd
the direct inuence of input pixels in activations of the neurons in each layer. Each layer in the
auto-encoder is trained separately and combined to form the four layered stacked auto-encoder.
Let the size of the input images in the database be MNpixels. The input layer accepts
an input as the pixel intensities of xi=fxij; j=f1;2; : : : ; MNgg. Where xirepresents ith
input image and xijrepresents intensity value of jthpixel in the ithimage. The rst hidden layer
encodes the high-dimensional ( MN) input image intensity to low dimensional (R) compressedrepresentation h1. The second hidden layer consisting of Q neurons nd more interesting structures
h2of the input data from h1. The output layer consisting of neuron proportions to the number of
classes (C) present in the dataset. We used two neurons in the output layer representing normal
and abnormal classes.
h1r=fe(JX
j=1wrjxij) (1)
h2q=fe(RX
r=1wqrh1r) (2)
zc= (QX
q=1wcqh2q) (3)
Where xijis the intensity of the jthpixel in the ithimage, Jis the number of neurons in the
input layer, Ris the number of neurons the hidden layer one, Qis the number of neuron in the
hidden layer two, Cis the number of neurons in the output layer, and J > R > Q > C .
The h1rrepresents the activation of rthneuron in the hidden layer one (Equation 1), wrj
represents the weight between the rthneuron in the hidden layer one and jthinput in the input
layer ( wrjsignies the importance of jthimage pixel in activating the rthneuron in the hidden
layer one). Similarly h2q,wqr( in Equation 2) are the class activation and weights of qthneuron in
hidden layer two; and zc,wcq( in Equation 3) are the class activation and weights of output layer.
We used sigmoid activation function ( fe) given in Equation 4 to encode and decode the data.
The results from the output layer ( zc) is passed to a Soft-max function (Equation 5). Soft-max
function produces the probability ( Pc) of the input image in each class. Based on the probability
the input image is classied to normal or abnormal class.
Sigmoid (a) =exp(a)
exp(a) + 1(4)
Pc=Softmax (zc) =exp(zc)PC
c=1exp(zc)(5)Algorithm 1 :Greedy Backtracking.
Input : <xi, AE, S >
xi=fxijjj=f1;2; : : : ; MNgg
.xibe the ithimage in the database, xijbe the intensity of the jthpixel in image xi
AE(InputLayer; HiddenLayer 1; HiddenLayer 2; OutputLayer )
.Stacked auto-encoders with one input layer, two hidden layers and one output layer
Parameter's of AE:
J .Number of neurons in the InputLayer , J=M*N
R .Number of neurons in the HiddenLayer 1
Q .Number of neurons in the HiddenLayer 2
C .Number of neurons in the OutputLayer /Number of Classes
wrj .Weight between the jthneuron in the InputLayer andrthneuron in the HiddenLayer 1
h1r .Output from the rthneuron in the HiddenLayer 1
wqr .Weight between the rthneuron in the HiddenLayer 1 and qthneuron in the HiddenLayer 2
h2q .Output from the qthneuron in the HiddenLayer 2
wcq .Weight between the qthneuron in the HiddenLayer 2 and cthneuron in the OutputLayer
S :Count of Salient pixels
.Number of pixels considered for seed point localisation
Output : <SP(cx; cy)>
SP(cx; cy):Seed Point Coordinates
.Image cordinate representing the seedpoint to locate anomaly.
1:Input xiin to the AEand nd probability of the image in each class : fP(c)jc2f1;2; :::; Cgg
2:Find the class of the images based on the maximum probability of Softmax function in the
output layer.
c= argmax
cfP(c)jc2f1;2; :::; Cgg .Let the class of image be c
3:Backtrack the cthneuron in OutputLayer and nd the neuron in HiddenLayer 2 which causes
maximum activation in the predicted class c.
q=argmax
qf(wcqh2q)jq2f1;2; :::; Qgg
.Letqbe the back-tracked neuron in HiddenLayer 2
4:Backtrack the qthneuron in HiddenLayer 2 and nd the neuron in HiddenLayer 1 which causes
max activation in the qthneuron.
r= argmax
rf(wqrh1r)jr2f1;2; :::; Rgg
.Letrbe the back-tracked neuron in HiddenLayer 1
5:Sort the activations of the rthneuron in the HiddenLayer 1 in descending order.
SortedActivations =SortDescendingf(wrjxij)jj=f1;2; ::; MXNgg
6:Take top 'S' image pixels ( xij) from SortedActivations which causes highest class activations
in the rthneuron in the HiddenLayer1.
7:Collect the coordinates of the selected top `S' pixels in the input image xi
8:Cluster the coordinate in the denser region and eliminate the outliers.
9:The cluster centre cordinates represents the seedpoint ( SP(cx; cy)) to locate anomaly.Figure 1: (colour)The pictorial overview of the backtraking the trained autoencoder to locate salient image pixels
in the input image .
The proposed mass localisation procedure by backtracking is listed in Algorithm 1. An input
image is fed in to the stacked auto-encoders with one input layer, two hidden layers and one output
layer. The image class is predicted based on the probability of the soft-max layer. The output
layer neuron corresponding to the predicted class is back-tracked and the neuron in hidden layer
2, which causes the highest activation in the predicted class is identied. Similarly, the layer 1
neuron which causes maximal activation is pinpointed, by backtracking the identied neuron in
the layer 2. After locating the neuron in layer 1, the coordinates of the top `S' image pixels (Salient
image pixels) causing maximum activation in the located layer1 neuron are selected. The selected
co-ordinates in the denser regions are clustered and outliers are removed. If more than one cluster
is present the cluster centre with highest pixel intensity is selected as the seed point coordinates
of the given image (since the abnormal masses have higher intensity value compared to normal
masses). Figure 1 represents a pictorial overview of the stacked auto-encoder and the backtracking
mechanism to nd seed point location in the input image. After locating the seed point, the ROI
is extracted and segmented automatically using the region growing algorithm [11].3. Experimental Details and Results
The performance of the proposed framework is assessed using randomly selected 200 images
from IRMA mammographic dataset (100 normal mammograms and 100 abnormal images with
mammographic masses are selected). Seventy percentage of the images from each class are used
to train the auto-encoder. All images are resized to 256 128 pixels size, and the pectoral muscle
area is removed[12]. The mammographic masses are marked by an expert radiologist, which is
used as the ground truth for the experiment. The proposed method is implemented using Matlab
2018b software in a system with 8 GB RAM and Intel i7 6500U CPU. The DCNN based ROI
localisation is performed using [13]. We trained a stacked auto-encoders with one input layer, two
hidden layer and one output layer. The input layer consists of neuron corresponding to the image
size (J=256128) and the output layer consists of neuron corresponding to the number of classes
(C=2, representing normal and abnormal classes). The number of neuron in the hidden layer 1
(R=100), the number of neuron in the hidden layer 2 (Q=10), and the salient image pixels (S=20)
is determined empirically.
We evaluated the mass localisation ability of DCNN in the mammographic images using Oc-
clusion [4], Back-Propogation [6] and Guided Back-Propogation [7] method using pre-trained
GoogleNet [14]. We also analysed the localisation ability of ne-tuned DCNN ( Alexnet [15],
GoogleNet [14] and Resnet [16]), by examine the maximum class activation in the initial convolu-
tional layers [5]. The ROI marked by the radiologist (100 mammographic images in the abnormal
class with one suspicious area) is used as the ground truth in the mass localisation experiment.
Table 1 shows the count of correctly located and wrongly located masses in the mammographic
images by the proposed method and DCNN based method [4, 5, 6, 7]. Figure 2 depicts a mammo-
graphic image with located masses using the proposed method and DCNN based methods. The
results in Table 1 and Figure 2 clearly shows that the proposed method has good localisation
ability to detects masses in mammographic images compared to the DCNN based methods.Figure 2: (colour) (a)A sample mammographic image. (b)ROI marked by the radiologist. (c)ROI located by
Back propogation [6] (d)ROI located by Guided back propogration [7] (e)ROI located by Occlusion method [4].
[Red colour pixels indicates salient regions in (c-e ) ],(f)Class activations of Alexnet [15]. (g)Class activations
of Googlnet [14]. (h)Class activations of Resnet [16].[ High pixel intensity(white colour or light coloured pixels)
indicates salient regions in class activatios of (f-h) ],(i)the proposed method (Salient 20 pixels are represented in
red colour). (j)Selected seed point from the salient pixels (k)ROI extracted using region growing algorithm [11]
represented in the red bounding boxTable 1: Mass localisation in 100 abnormal Mammograms
Method #Correctly Located #Wrongly Located
Proposed Method III 87 13
AlexNet (2012) [15] 65 35
Occlusion (2014) [4] 58 42
GoogleNet(2015) [14] 34 66
ResNet (2016) [16] 27 73
Guided back-Propagation (2014) [7] 19 81
Back-Propagation (2014) [6] 17 83
We also compared the performance of the proposed method with an automatic mass localisation
method using Faster R-CNN proposed by Ribli et al. [1]. We used the CnnCAD [17] plug-in
provided by the author for evaluating the performance. The CnnCAD plug-in is implemented in
Horos in a mac computer with 4 GB RAM and High Sierra (10.13) operating system. Table 2
provides the localisation performance of the proposed method with Faster R-CNN proposed by the
Ribli et al. [1]. The experimental results in Table 2 indicate that the performance of supervised
Faster R-CNN [1] detecting system is not promising since its performance is substantially depends
on the quality of the training images [1]. Figure 3 provides the samples of ROI detected by the
proposed method and Faster R-CNN [1]. In Faster R-CNN [17] method ROI markers will appear
automatically on the image, if the model detects any abnormal image region.
Table 2: Performance comparison of the proposed method with Faster-RCNN method
Method #Correctly Located #Wrongly Located
Proposed Method III 87 13
Faster R-CNN (2014) [1] 11 89Figure 3: (colour) ROI detected by the proposed method and Faster R-CNN [1] in four sample images. First row
(a) Represents the ROI marked by the expert. Second row (b) Indicates the ROI located by the proposed method.
Third Row (c) Represents the ROI detected by the Faster R-CNN method [1].4. Discussion
The overall results show that the proposed greedy backtracking method is less complex com-
pared to the DCNN based method, and more ecient to locate salient mammographic masses as
shown in Table 1 and 2. The performance of the DCNN based methods heavily depends on the
quality and quantity of the labelled images used to train the network [18, 19]. Most of the medical
images are normally noisy due to the imaging conditions and the labelled images are not available
at aordable cost in the medical eld. The auto-encoder requires a limited number of unlabelled
training images to de-noise and represent the input image eciently compared to DCNN [20]. The
convolutional layers in DCNN [5] eectively capture discriminating colour and texture features of
natural images to detect salient image regions. Most of the medical images are grey coloured and
do not have sharp edged features as in natural images. The proposed greedy backtacking method
using auto-encoders can detect ROI in grey-coloured mammograms very eciently, compared to
state-of-the-art DCNN based methods [1, 6, 7, 5, 4].
The experimental results in Figure 3 show that, the Faster R-CNN [17] method is not able to
detect the ROI's in the mammographic images with signicant mammographic masses (Figure 3:
Image 2 ROI is wrongly detected, Figure 3: Image 3 and 4 not detected any ROI). This may be
mainly due to the image quality requirement for Faster R-CNN [1]. However, the proposed method
is able to detect the ROI's with the same quality images. Moreover, Faster R-CNN [1] required
pixel level annotated images to train the network, whereas the proposed back-tracking method
using unsupervised auto-encoder doesn't require any labelled data.
In this paper, we hypothesized that the image regions activating abnormal classes in mammo-
graphic images will be the breast masses which causes the anomaly and back-tracked the maximal
activations of stacked auto-encoders to locate ROI. Such interpretations can help to promote the
acceptance of deep learned features in clinical evaluation and patient treatment process. As it is
dicult to attain large collection of medical images with unusual masses, our aim was to analyse
the localisation ability of the deep network with a limited number of images. The unsupervised
auto-encoders do not require a huge number of labelled training images, so the proposed method
is apt for medical image application. Additionally, the proposed method is computationally less
complex in locating salient image regions compared to the DCNN based methods [1, 4, 5, 6, 7].We have to further analyse the performance of the system using more complex dataset, containing
mammographic images with multiple suspicious masses.
5. Conclusion
Mass localisation in the mammographic images is the critical step in computer-aided mammo-
graphic analysis methods. In this paper, we have proposed an automatic mammographic mass
localisation and retrieval framework using stacked auto-encoders. The proposed method back-
tracks the maximal class activations in the stacked auto-encoders. The DCNN based methods
have poor localisation ability due to the absence of coloured and sharp edged features in medical
images. The experimental results demonstrated that the proposed method is superior in localising
mammographic images with a limited number of unlabelled medical images compared to DCNN
based methods.
References
[1] D. Ribli, A. Horv ath, Z. Unger, P. Pollner, I. Csabai, Detecting and classifying lesions in
mammograms with deep learning, Scientic reports 8 (1) (2018) 4165.
[2] M. Heath, K. Bowyer, D. Kopans, R. Moore, W. P. Kegelmeyer, The digital database for
screening mammography, in: Proceedings of the 5th international workshop on digital mam-
mography, Medical Physics Publishing, 2000, pp. 212{218.
[3] R. S. Bressan, D. H. Alves, L. M. Valerio, P. H. Bugatti, P. T. Saito, Doctor: the role of deep
features in content-based mammographic image retrieval, in: 2018 IEEE 31st International
Symposium on Computer-Based Medical Systems (CBMS), IEEE, 2018, pp. 158{163.
[4] M. D. Zeiler, R. Fergus, Visualizing and understanding convolutional networks, in: European
conference on computer vision, Springer, 2014, pp. 818{833.
[5] J. Yosinski, J. Clune, A. Nguyen, T. Fuchs, H. Lipson, Understanding neural networks through
deep visualization, arXiv preprint arXiv:1506.06579 (2015).[6] K. Simonyan, A. Vedaldi, A. Zisserman, Deep inside convolutional networks: Visualising
image classication models and saliency maps, arXiv preprint arXiv:1312.6034 (2013).
[7] J. T. Springenberg, A. Dosovitskiy, T. Brox, M. Riedmiller, Striving for simplicity: The all
convolutional net, arXiv preprint arXiv:1412.6806 (2014).
[8] S. Aminikhanghahi, S. Shin, W. Wang, S. I. Jeon, S. H. Son, A new fuzzy gaussian mixture
model (fgmm) based algorithm for mammography tumor image classication, Multimedia
Tools and Applications 76 (7) (2017) 10191{10205.
[9] Q. Li, R. Xu, H. Zhao, L. Xu, X. Shan, P. Gong, Computer-aided diagnosis of mammographic
masses using local geometric constraint image retrieval, Optik 171 (2018) 754{767.
[10] N. S. Purwadi, H. T. Atay, K. K. Kurt, S. T urkeli, Assessment of content-based image retrieval
approaches for mammography based on breast density patterns., in: MIE, 2016, pp. 727{731.
[11] R. Adams, L. Bischof, Seeded region growing, IEEE Transactions on pattern analysis and
machine intelligence 16 (6) (1994) 641{647.
[12] K. S. Camilus, V. Govindan, P. Sathidevi, Computer-aided identication of the pectoral
muscle in digitized mammograms, Journal of digital imaging 23 (5) (2010) 562{580.
[13] F. Gr un, C. Rupprecht, N. Navab, F. Tombari, A taxonomy and library for visualizing learned
features in convolutional neural networks, arXiv preprint arXiv:1606.07757 (2016).
[14] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke,
A. Rabinovich, Going deeper with convolutions, in: Proceedings of the IEEE conference on
computer vision and pattern recognition, 2015, pp. 1{9.
[15] A. Krizhevsky, I. Sutskever, G. E. Hinton, Imagenet classication with deep convolutional
neural networks (2012) 1097{1105.
[16] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recognition, in: Proceedings
of the IEEE conference on computer vision and pattern recognition, 2016, pp. 770{778.[17] Computer aided detection with Faster-RCNN, https://github.com/riblidezso/frcnn_
cad, accessed: 2019-11-09.
[18] J. Cho, K. Lee, E. Shin, G. Choy, S. Do, How much data is needed to train a medical image
deep learning system to achieve necessary high accuracy?, arXiv preprint arXiv:1511.06348
(2015).
[19] N. Tajbakhsh, J. Y. Shin, S. R. Gurudu, R. T. Hurst, C. B. Kendall, M. B. Gotway, J. Liang,
Convolutional neural networks for medical image analysis: Full training or ne tuning?, IEEE
transactions on medical imaging 35 (5) (2016) 1299{1312.
[20] M. Chen, X. Shi, Y. Zhang, D. Wu, M. Guizani, Deep features learning for medical image
analysis with convolutional autoencoder neural network, IEEE Transactions on Big Data
(2017).