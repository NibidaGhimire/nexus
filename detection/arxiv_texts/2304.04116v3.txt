Marginal Thresholding in Noisy Image Segmentation
Marcus Nordström*
Department of Mathematics
KTH Royal Institute of Technology
Stockholm, Sweden
marcno@kth.seHenrik Hult
Department of Mathematics
KTH Royal Institute of Technology
Stockholm, Sweden
hult@kth.se
Atsuto Maki
Department of Computer Science
KTH Royal Institute of Technology
Stockholm, Sweden
atsuto@kth.se
Abstract
This work presents a study on label noise in medical im-
age segmentation by considering a noise model based on
Gaussian field deformations. Such noise is of interest be-
cause it yields realistic looking segmentations and because it
is unbiased in the sense that the expected deformation is the
identity mapping. Efficient methods for sampling and closed
form solutions for the marginal probabilities are provided.
Moreover, theoretically optimal solutions to the loss func-
tions cross-entropy and soft-Dice are studied and it is shown
how they diverge as the level of noise increases. Based on re-
cent work on loss function characterization, it is shown that
optimal solutions to soft-Dice can be recovered by thresh-
olding solutions to cross-entropy with a particular a priori
unknown threshold that efficiently can be computed. This
raises the question whether the decrease in performance
seen when using cross-entropy as compared to soft-Dice is
caused by using the wrong threshold. The hypothesis is vali-
dated in 5-fold studies on three organ segmentation problems
from the TotalSegmentor data set, using 4 different strengths
of noise. The results show that changing the threshold leads
the performance of cross-entropy to go from systematically
worse than soft-Dice to similar or better results than soft-
Dice.
1. Introduction
In medical image analysis it is often of interest to parti-
tion an image into foreground and background with respect
*Author is also affiliated with RaySearch Laboratories.to some region of interest. This problem is referred to as the
medical image segmentation problem. Since the introduction
of the U-net [26], state of the art methods for addressing the
problem are dominated by neural network methods trained
according to the supervised learning paradigm [6,28]. These
methods are remarkably effective in scenarios with abundant
and clean data, and some would argue that the segmentation
problem under such circumstances has been solved. How-
ever, manually delineating a region of interest accurately
and reliably can be a complex and time-consuming process
that often requires medical expertise. This result in high
costs associated with obtaining high-quality data sets for
training, and consequently, that data used in practice often is
imperfect in the sense that the manually provided labels are
contaminated by noise [1, 5, 20, 24].
A large amount of research has been devoted to medical
image segmentation with imperfect data and one important
question that has been addressed is how robust various seg-
mentation methods are to label noise. Typically, experiments
are conducted by first obtaining some data set, then cor-
rupting the training data with various types of noise and
finally training different models on the generated data sets.
The results are then evaluated for the different models using
some kind of evaluation metric and compared with a base-
line model trained without any added noise. Simple noise
models that have been studied include randomly chopping
off parts of the structure [33] and flipping labels or clusters
of labels in an independent manner [9]. A more realistic
noise model that has been studied is based on random defor-
mations, where deformations are formed by the interpolation
of low dimensional Gaussian arrays. These Gaussian defor-arXiv:2304.04116v3  [cs.CV]  8 Jul 2023mation models are unbiased in the sense that the expected
deformation is given by the identity deformation. It has ex-
perimentally been shown that machine learning models are
more robust to noise that has this properties as compared to
noise that does not have this property [31].
How label noise affects the performance of segmentation
models depend on the loss function that is used to train the
model. Arguably, the most classical loss function is cross-
entropy. This loss function is known to have a lot of nice
theoretical properties and to be stable in practice. Because of
the unbalanced nature of medical image segmentation caused
by the foreground typically being significantly smaller than
the background, and the widespread use of Dice as the evalu-
ation metric, an alternative loss based on a smoothed version
of the Dice metric was later proposed. This loss is referred
to as soft-Dice or the Dice loss, and has been shown to
yield better performance than cross-entropy in several exper-
imental settings at the cost of sometimes introducing some
stability issues [2, 21]. As a consequence, to get some im-
proved performance without introducing to much potential
stability issues, it is common to use linear combintations of
the losses. This is for instance the type of loss used in the
popular NNUnet [11].
This work takes a closer look on a noise model based on
Gaussian deformations. In particular, such a model is formal-
ized and efficient methods for simulation and closed form ex-
pression for marginal probabilities are derived. For the noise
model, optimal solutions to cross-entropy and soft-Dice are
then studied, and it is shown that soft-Dice and cross-entropy
yield the same class of optimal solutions when the labels are
noise-free, but that they diverge when noise is increased. It
is further shown that optimal solutions to soft-Dice can be
recovered by thresholding solutions to cross-entropy with a
particular a prior unknown but efficiently computable thresh-
old. This leads to the question whether cross-entropy with
the alternative threshold yields the performance of soft-Dice,
and it is experimentally shown that this indeed is the case.
Contributions: A family of noise models based on Gaus-
sian field deformations is formalized. An efficient method
for sampling noisy segmentations and a closed form expres-
sion for the marginal probabilities are derived. The noise
model is used to show that optimal solutions to cross-entropy
and soft-Dice diverge as noise increases, but that optimal
solutions to soft-Dice can be recovered by thresholding the
cross-entropy estimates with a computable threshold. This
is then validated experimentally on three organs from the
TotalSegmentor data set [32].
2. Related work
Label noise in medical image segmentation data can be a
consequence of time constraints imposed on the annotators
drawing the segmentations, differing opinions on where thelabel borders should be or something else. The fact that
label noise often is present in data used for model training
is widely known and have been pointed out in for exam-
ple [1,5,20,24]. Many methods have been proposed for han-
dling label noise. Examples include active learning methods,
where experts are iteratively probed for more information [8]
and re-weighting schemes where methods dynamically learn
to put different weights on different regions depending on
certainty [19]. For fairly recent reviews, see [12, 30].
The influence label noise has on various loss functions
have been studied in several papers. This include the influ-
ence it has on calibration and volume bias for predictions
generated with models trained using cross-entropy and soft-
Dice [3], re-calibration methods for improving performance
of soft-Dice [27] and the relationship between calibration
and volume bias [25]. Other experimental work showing
that the expected calibration error is lower for cross-entropy
than for soft-Dice include [18]. In the above, noise has been
analyzed by considering the expected score with respect to
some noisy segmentation. An alternative way to incorporate
noise is to make use of soft labels. Examples of work using
soft labels with soft-Dice include [7, 13]. Examples of work
on soft labels with other loss functions include [15, 16, 29].
Inspired by threshold classifiers studied in [17, 34], opti-
mal segmentations to Accuracy and Dice under label noise
was analyzed in [22]. A characterization of optimal segmen-
tations was provided, sharp bounds with respect to volume
bias of the optimal segmentations where shown and the
volume of the optimizers associated with the metrics was
compared. A similar analysis was later provided for the
loss function soft-Dice [23]. In particular, a characteriza-
tion of the optimal solutions and the associated volumes are
provided. Moreover, a calibration result showing that a se-
quence the converge to optimal soft-Dice, when thresholded
appropriately, converges to optimal Dice.
The robustness of medical image segmentation to noise
has been addressed in several work. In [33], robustness is
studied by randomly chopping off sub-structures in the train-
ing data. Of special importance to this work is segmentation
noise provided by Gaussian field deformations. Two works
have studied such noise in an experimental setting. This
includes [9] and [31]. In these works, the segmentations in
the training data is perturbed according to a low dimensional
Gaussian field interpolated to the pixel space. Models are
then trained for various noise levels and results obtained are
reported. Finally, in [14], a noise model for target delin-
eation in radiotherapy is discussed. The model is based on
taking the level set of Gaussian field and efficient methods
for sampling is provided.
23. Preliminaries
3.1. Notation
LetΩ = [0 ,1]n⊂Rnbe the unit cube of dimension n≥
1andλbe the standard Lebesgue measure associated with
Rnsuch that λ(Ω) = 1 . LetSbe the space of measurable
functions from Ωto{0,1},Mbe the space of measurable
functions from Ωto[0,1]andFbe the space of bounded
measurable functions from ΩtoR, where all of the spaces S,
MandFare equipped with their associated Borel σ-fields
and the Lp-norms ∥·∥p. For short, the notation ¯f(ω) =
1−f(ω), ω∈Ωwill be used for any f∈ F. The letter
s∈ S is generally used to denote a segmentation, where
s(ω) = 1 indicates foreground of the target structure of
interest and s(ω) = 0 indicates background. The letter
m∈ M is generally used to denote a marginal function, that
is,m(ω)∈[0,1]indicates the probability of a noisy label
occupying site ω∈Ω. The letter c∈ M is generally used to
denote the soft segmentation that is obtained by optimizing
the chosen loss function prior to any thresholding. The letter
f∈ F is generally used to denote a logit function usually
associated with a soft segmentation such that σ◦f(ω) =c(ω)
forω∈Ωa.e., where σ(x) = 1 /(1 +e−x)is the standard
sigmoid function. Composition of functions are denoted by
◦andIA(x)is used to denote the indicator function over
the set A, that is IA(x) = 1 ifx∈AandIA(x) = 0
ifx̸∈A. The indicator function is used in conjunction
with some element c∈ M to denote thresholding of a soft
segmentation. That is, I[t,1]◦cassigns 1to all ω∈Ωwhere
c(ω)≥tand0to all ω∈Ωwhere c(ω)< t.
3.2. Loss functions and evaluation metrics
The two most popular loss functions in the field of med-
ical image segmentation are cross-entropy and soft-Dice.
When noise is present, the label becomes a random variable
and the losses need to be extended to a functional over one
deterministic soft segmentation (prediction) and one random
segmentation (label). In this work, the soft labeling conven-
tion for this extension is adopted for the theoretical analysis
[7, 13, 15, 16, 29]. Formally, the loss functions are defined
with respect to a marginal function m∈ M which corre-
sponds to the mean of a noisy segmentation Ltaking values
inS, that is m(ω) =L(ω), ω∈Ω.
Definition 1. For any m∈ M , (binary) cross-entropy is
given by
CEm(c).=−/integraldisplay
Ω[m(ω) log( c(ω))+
¯m(ω) log(¯ c(ω))]λ(dω),c∈ M.(1)
Definition 2. For any m∈ M , soft-Dice is given by
SDm(c).= 1−2/integraltext
Ωc(ω)m(ω)λ(dω)
∥c∥1+∥m∥1, c∈ M.(2)Arguably, the most classical evaluation metric used is
Accuracy. This metric simply measures the fraction of the
domain that is correctly labeled. The most popular evaluation
metric on the other hand is the Sörensen-Dice coefficient,
or Dice for short. To handle label noise, the metrics are
generalized in the same way as the loss functions, that is, by
using soft labels.
Definition 3. For any m∈ M , Accuracy for is given by
Am(s).=/integraldisplay
Ω[s(ω)m(ω) + ¯s(ω) ¯m(ω)]λ(dω), s∈ S (3)
Definition 4. For any m∈ M , Dice is given by
Dm(s).=2/integraltext
Ωs(ω)m(ω)λ(dω)
∥s∥1+∥m∥1, s∈ S. (4)
When doing the experiments, only one label per image
is available and it is more accurate to think of the setup
as an empirical approximation of the expected score. For
cross-entropy and Accuracy, the definitions using expected
score and the definitions using soft labels are equal, that is
E[AL(s)] = A m(s)andE[CEL(s)] = CE m(s). For soft-
Dice and Dice, the definitions are equal provided that the
volume of the noisy label is constant, that is Var[∥L∥1] = 0 .
If this holds approximately which is often the case in medical
image segmentation, where noise often alters the details of
the boundary without significantly altering the volume of the
segmentation, then it follows that E[DL(s)]≈Dm(s)and
similarly that E[SDL(s)]≈SDm(s).
3.3. Gaussian fields
Amongst the most central objects in probability theory
and statistics are normally distributed random variables, also
referred to as Gaussian random variables. Such random vari-
ables appear all over science and engineering and numerous
generalizations have been greatly studied because of their
richness and interesting properties. One simple generaliza-
tion of the classical Gaussian variable is to the multidimen-
sional case for which one talks about multivariate Gaussian
variables. Such a variable, when centered and isotropic, has
the following probability density function
pσ2(ω) ˙ =1
(2πσ2)n/2exp/braceleftbigg
−∥ω∥2
2
2σ2/bracerightbigg
, ω∈Rn,(5)
where σ2denotes the variance in each of the dimensions. A
common further generalization that can be done is to replace
the index set with a subset of the real line, in which case
the resulting object is referred to as a Gaussian process.
Similarly, but less common, is when the index set is replaced
by a subset of a real euclidean space. In this case the resulting
object is referred to as a Gaussian field. Formally, a Gaussian
fieldY(ω), ω∈Rnis a random variable such that for any
3finite collection of elements {ω1, . . . , ω M} ⊂Ω, it holds
that
(Y(ω1), . . . , Y (ωm))∼normal (0,Σ). (6)
The covariance matrix is usually expressed using a
kernel function
Σm,m′=k(ωm, ωm′), m, m′∈ {1, . . . , M }.(7)
Many types of kernel functions with different properties have
been studied in the literature. Amongst the most common
are
ka,b(ω, ω′) =a2exp/parenleftbigg
−∥ω−ω′∥2
2
2b2/parenrightbigg
, ω, ω′∈Rn.(8)
This covariance function will also be central in this work. An
important property of Gaussian processes with such covari-
ance functions are that samples are almost surely continuous.
4. Theory
4.1. Optimal segmentations
When a label is free from noise, any reasonable loss
function or metric is optimized uniquely by the target label.
When however there is noise present, it is no longer clear
what the solution should be and the losses and metrics im-
plicitly assigns a set of them. As a consequence, it is natural
to investigate what the optimal solutions are to a chosen loss
or metric. For the case of the cross-entropy loss, it is well
known that the optimizers are uniquely given by the marginal
function, that is
CEm(c) = inf
c′∈MCEm(c′)⇐⇒ c(ω) =m(ω).(9)
λ-a.e. In other words, minimizing cross-entropy yields es-
timates of the marginal probability distribution associated
with the label. For soft-Dice it was shown in [23] that
SD(c) = inf
c′∈MSD(c′)⇐⇒
c(ω)∈

{1} ifm(ω)>sups′∈SDm(s′)/2,
[0,1] ifm(ω) = sups′∈SDm(s′)/2,
{0} ifm(ω)<sups′∈SDm(s′)/2,(10)
λ-a.e. Using the setup adopted in this paper, the optimal
segmentations for Accuracy and Dice was characterized
in [22]. For Accuracy, the solutions are given by
Am(s) = sup
s′∈SAm(s′)⇐⇒
s(ω)∈

{1} ifm(ω)>1/2,
{0,1}ifm(ω) = 1/2,
{0} ifm(ω)<1/2,(11)λ-a.e. Similarly, for Dice the optimal segmentations are
given by
Dm(s) = sup
s′∈SDm(s′)⇐⇒
s(ω)∈

{1} ifm(ω)>sups′∈SDm(s′)/2,
{0,1}ifm(ω) = sups′∈SDm(s′)/2,
{0} ifm(ω)<sups′∈SDm(s′)/2,(12)
λ-a.e. Ideally, the minimizing of a loss function leads to the
maximization of the desired target metric. That is, that the
soft segmentations, when thresholded with a 1/2-threshold,
provides segmentations that maximize the target metric. Us-
ing these characterizations, the 1/2-thresholded optimizers
to the losses can easily be connected to the optimizers of the
metrics. For cross-entropy it follows that
CEm(c) = inf
c′∈MCE(c′) =⇒
Am(I[1/2,1]◦c) = sup
s′∈SA(s′).(13)
Similarly, for soft-Dice it follows that
SDm(c) = inf
c′∈MSD(c′) =⇒
Dm(I[1/2,1]◦c) = sup
s′∈SD(s′).(14)
Based on the characterization of the optimizers to soft-
Dice and cross-entropy, it is clear that that an optimizer to
soft-Dice can be obtained by first optimizing cross-entropy
and then picking a particular threshold that is not known a
priori
tm= sup
s′∈SDm(s′)/2. (15)
It then then follows that
CEm(c) = inf
c′∈MCE(c′) =⇒
SDm(I[tc,1]◦c) = inf
c′∈MSD(c′).(16)
This method is a variation of the method presented in [17]
from the statistical learning literature. In the medical image
segmentation community, however, it has to the best of our
knowledge not been treated.
A natural question to address after making these obser-
vations is whether the difference in performance associated
with cross-entropy and soft-Dice can be related to what
threshold is used for cross-entropy. Such an hypothesis is
further strengthened by two papers that have addressed the
question of optimal thresholds [4,10]. The results from these
papers experimentally show that using 1/2as a common
threshold leads to sub-optimal segmentations for various
loss functions with respect to Dice. However, neither is a
method for computing thresholds uniquely for every image
4considered, nor is the connection to the theoretical optimal
threshold done (15).
In [21], it was proposed that the instability sometimes
observed for soft-Dice may be associated with the fact that
it is not convex (nor quasi-convex) when using sigmoid
functions to enforce the bounds, that is f∝⇕⊣√∫⊔≀→SDm(σ◦
f), f∈ F. This is in contrast to cross-entropy where the
corresponding f∝⇕⊣√∫⊔≀→CEm(σ◦f), f∈ F is known to be
convex. If cross-entropy with the alternative threshold yields
the same performance as soft-Dice, then using it may provide
the performance of soft-Dice without sacrificing the stability
of cross-entropy.
4.2. Label noise
In contrast to binary classification, where a lot of label
noise can be modeled by simple independent flipping of
binary variables, label noise in segmentation can in general
have very complicated dependency structures. Often labels
affected by noise have the appearance of being drawn by
someone with shaky hands . For such noise, the correlation
between border points will depend on the distance between
them. That is, if two points are close to each other, then the
correlation will be high and vice versa.
One way to model segmentations that look as if they have
been drawn by shaky hands is by means of random Gaussian
field deformations. In particular, let
X= (X1, . . . , X n)T(17)
be such that {Xi}n
i=1are independent Gaussian fields with
kernel function (8). This means that Xis a random vector
field taking values in Rn∝⇕⊣√∫⊔≀→Rn. With such a random vector
field, a random segmentation is formed by deforming a noise-
free segmentation l∈ S as follows
L(ω) =/braceleftigg
l(ω+X(ω)) ifω+X(ω)∈Ω,
0 ifω+X(ω)̸∈Ω., ω∈Ω.
(18)
How samples of the random segmentations look depend
on what parameters are chosen when specifying the kernel
function. The parameter acontrols how large the deforma-
tions are, where greater aleads to larger deformations and
vice versa. The parameter bcontrols how smooth the defor-
mations are, where greater bleads to smoother deformations
and vice versa. Modeling noisy segmentations with (18) is
convenient because it allows for the derivation of several
theoretical properties. In this work, three such important
properties are proved.
In Theorem 1, it is shown that the marginal probabilities
associated with a noisy segmentation can be computed by
convolving the associated noise-free segmentation label with
an appropriate Gaussian density.Theorem 1. LetlandLbe defined as in (18) andpσ2be
defined as in (5), then if follows that
E[L(ω)] =/integraldisplay
Ωl(ω′)pa2(ω−ω′)λ(dω′), ω∈Ω.(19)
In Theorem 2, the marginal representation is used to show
that the the volume of the noisy segmentation is approxi-
mately unbiased in the sense that the expected volume of a
noisy segmentation is approximately equal to the volume of
the corresponding noise-free segmentation. The reason why
this does not hold exactly is that there is some probability
for the target structure to deform in such a way that parts
of the structure leave the domain Ω. For most reasonable
choices of parameters, the probability of this happening is
negligible.
Theorem 2. LetlandLbe defined as in (18) andpσ2be
defined as in 5, then it follows that
E[∥L∥1] =∥l∥1−ξ, (20)
where
ξ=/integraldisplay
Ω/integraldisplay
Rn\Ωl(ω′)pa2(ω−ω′)λ(dω)λ(dω′). (21)
In Theorem 3, it is shown that X(17) is equal in distribu-
tion to a particular vector of stochastic integrals. In the case
n= 1, this could have been done using regular Ito-integrals,
but for the case n >1a multivariate counter part is needed.
For this scattered Gaussian measures are used.
Theorem 3. LetXbe as in (17),pa2be as in (5)and
W1, . . . , W nbe independent copies of independently scat-
tered Gaussian measures on Rnwith control measure λ.
Then it follows that
X(ω)d=
a(2πb2)n/4/integraltext
Rnpb2/2(ω−ω′)W1(dw′)
...
a(2πb2)n/4/integraltext
Rnpb2/2(ω−ω′)Wn(dw′)
.
(22)
Details on scattered Gaussian measures and the proof can
be found in the Supplementary Document. The important
consequence of this theorem is that when the domains are
discretized, it implies that noisy segmentations can be sam-
pled by drawing independent Gaussian distributed variables
associated with all of the voxels, and then smoothing the
associated random arrays with Gaussian filters. Both opera-
tions which are very effectively implemented. This allows
for efficient sampling of noisy segmentations, even when the
image is composed of million of voxels.
5Samplesa= 0.00
 a= 0.01
 a= 0.02
 a= 0.03
 a= 0.04
 a= 0.05
 a= 0.06
Marginal
 Accuracy
 Dice
Figure 1. Illustration of the noise model and its effects on a two dimensional segmentation problem, with the columns showing the effect
of different noise strengths. The first row shows the pixel-wise average obtained from five samples. The second row shows the marginal
function. The third row shows optimal segmentations associated with Accuracy (11), which in this and most natural cases is unique. The
forth row shows optimal segmentations associated with Dice (12), which in this and most natural cases is unique. In all of the reported cases
b= 0.15/√
2.
4.3. Optimal segmentations under label noise
A natural question to ask is in what way the parameters
to the noise model affects the optimal solutions obtained
for a loss or metric. Since the losses and metrics investi-
gated in this work only depend on the marginal function,
this question can be answered by firstly addressing in what
way the segmentation noise affects the marginal function
and secondly addressing in what way the marginal function
affects the optimal solutions for a loss or metric.
In Figure 1, these two steps are illustrated for a two dimen-
sional segmentation problem with varying noise intensity. In
the first row, pixel-wise averages of 5 samples are illustrated.
In the second row, the marginal functions are illustrated.
In the third row, theoretical optimal solutions to Accuracy
which in these and most cases are uniquely given are shown.
Note that by (13), this is also given by the minimizer to
cross-entropy thresholded by 1/2. In the forth row, theoreti-
cal optimal solutions to Dice which in these and most cases
are uniquely given are shown. By (14) and(16), this is also
given by the minimizer to soft-Dice thresholded by 1/2and
by the minimizer to cross-entropy thresholded by tm(15).
A first observation that can be made is that noise affectsthe optimal segmentations with respect to the two metrics
such that the shape is changed as compared to the noise-free
structure. This is an important observation that contradicts
the picture that unbiased noise (unbiased in the sense of our
model), yield unbiased optimal segmentations. In particular,
both metrics yield segmentations that are changed in such a
way that the corners of the shapes are smoothed and details
such as thin structures and holes disappear. Loosely speak-
ing, the higher the noise, the more ball shaped the optimal
segmentations become.
A second observation is that the two metrics yield very
similar looking segmentations for low noise, but that they
yield increasingly different segmentations as the noise in-
crease. Since cross-entropy with the 1/2threshold theoreti-
cally gives optimal segmentations with respect to Accuracy
and soft-Dice with the 1/2threshold theoretically gives opti-
mal segmentations with respect to Dice, the performance of
the two losses as measured by Dice should be very similar in
low noise situations but increasingly better for for soft-Dice
as the noise is increased.
6Figure 2. Illustration of the data from the TotalSegmentor data
set [32] used in the experiments. The first row shows the axial,
coronal and sagittal views passing the center of mass of the kidney
in one of the data points. The second row shows the axial, coronal
and sagittal views passing the center of mass of the aorta in one of
the data points. The third row shows the axial, coronal and sagittal
views passing the center of mass of the esophagus in one of the
data points.
5. Experiments
In this section the three discussed methods are experi-
mentally tested using the derived noise model. For short,
the notation CE(0)will be used to denote a model trained
with cross-entropy and thresholded with 1/2,SD(0)a model
trained with cross-entropy and thresholded with 1/2and
CE(∗)a model trained with cross-entropy and thresholded
according to (15).
5.1. Data
The experiments are conducted with respect to the To-
talSegmentor data set [32]. This data set contains 1204 CT
images with 104 anatomical structures (27 organs, 59 bones,
10 muscles, 8 vessels). To illustrate the effect noise may
have on organs with different shape, three different organs
are chosen. This includes the right kidney which in general
is pretty spherical, the aorta which is tubular and relatively
thick and the esophagus which is tubular and relatively thin.
For each of the organs, 400 cases is selected and split into 5
folds of 80 cases. Finally, the images are sub-sample to half
resolution and patches of 643voxels centered in each of the
structures are extracted.Organ a CE(0)SD(0)CE(∗)
Kidney 0.0000 0.9611 0.9634 0.9615
0.0100 0.8762 0.8794 0.8774
0.0200 0.7883 0.7914 0.7909
0.0300 0.6947 0.7080 0.7055
Aorta 0.0000 0.9525 0.9515 0.9524
0.0100 0.8639 0.8654 0.8653
0.0200 0.7557 0.7569 0.7600
0.0300 0.6215 0.6513 0.6560
Esophagus 0.0000 0.8552 0.8603 0.8602
0.0100 0.6671 0.6722 0.6814
0.0200 0.4168 0.4527 0.4829
0.0300 0.1441 0.3105 0.3489
Table 1. Table over the results from the experiments for each organ
and noise level a. The entries are average Dice scores obtained from
the average over the five folds. CE(0)indicate that cross-entropy
with1/2threshold has been used, SD(0)indicate that soft-Dice
with a 1/2-threshold has been used and CE(∗)indicate that cross-
entropy with the threshold described in (15) has been used.
5.2. Model
A standard 3D U-net composed of blocks of two con-
volutions with instance normalization and relu-activations
is implemented. The number of features used increases
for each down-sampling according to 16,32,64,128,256,
where 16corresponds to the input resolution and 256the
bottleneck. Convolutions with kernel 23stride 2are used for
down-sampling and and transposed convolutions with kernel
23stride 2are used for up-sampling.
5.3. Training
The models are trained with either cross-entropy or soft-
Dice, in both cases using l2regularization with a weight of
10−5. Adam with learning rate of 10−4is used for optimiza-
tion together with a scheduler that divides the learning rate
by 5 after 30 epochs of non-improvement. The number of
epochs computed is fixed to 100 and the order of the training
data is randomly shuffled. Batch sizes of 1is used, and with
probability 0.5the data is augmented. The augmentation
is composed of random shifts, random rotations, applying
Gaussian filters with random smoothing levels of smooth-
ing. Performance is measured with respect to the model that
achieves the highest metric score on the training data.
5.4. Results
The results from the experiments are shown in Table 1
and Figure 3. This includes the average Dice score obtained
for the five different folds and four different noise levels, for
each of the different methods of obtaining segmentations:
CE(0),SD(0)andCE(∗).
70.00 0.01 0.02 0.030.70.80.9
aDiceKidney
CE(0)
SD(0)
CE(∗)
0.00 0.01 0.02 0.030.60.70.80.9
aDiceAorta
CE(0)
SD(0)
CE(∗)
0.00 0.01 0.02 0.030.20.40.60.8
aDiceEsophagus
CE(0)
SD(0)
CE(∗)
Figure 3. Illustration of the results of the experiments presented in Table 1. To the left are the results from the kidney segmentation problem,
in the middle are the results from the aorta segmentation problem and to the right are the results from the esophagus segmentation problem.
In each of the figures, the average Dice value obtained from five fold experiments is plotted as a function of the noise level a. The legend
CE(0)indicate that cross-entropy with the 1/2-threshold has been used, SD(0)indicate that soft-Dice with the 1/2-threshold has been used
andCE(∗)indicate that cross-entropy with the threshold described in (15) has been used. In all of the reported cases b= 0.15/√
2.
The first thing to notice is that either all of the the meth-
ods perform very similarly, or SD(0)threshold performs
better than CE(0). Also, whenever SD(0)performs better
thanCE(∗), cross-entropy with the Dice optimal threshold
performs similarly or better than SD(0). Simply put, using
the alternative threshold (15) with cross-entropy yields the
performance of soft-Dice. The second thing to notice is that
the difference amongst the methods increases when noise
increases. That is, for small noise, the performance of the
methods are almost equivalent, but as the noise increases,
the associated scores start to diverge. This type of result is
expected from the theory and depicted in Figure 1.
6. Discussion
The experiments provided in this paper systematically
validated the theoretical observations. However, the score as
measured by the average Dice value, is very low before the
effects start to show. In previous work comparing soft-Dice
and cross-entropy the effect seems to follow this pattern
in the sense that the difference is larger when the score is
lower [2]. However, there are situations when the Dice
score is high but with a significant difference. This could
be for many reasons, but one possibility is that there exist
some other noise that has this property. In general however,
the results indicate that there is evidence for using cross-
entropy rather than soft-Dice, provided the right threshold is
used. This is also preferred for several other reasons, such
as the better stability properties of cross-entropy and that it
provides marginal estimates that can be useful.7. Conclusion
In this work, the optimal solutions to soft-Dice and cross-
entropy has been compared and it is illustrated that optimal
solutions to soft-Dice can be obtained by thresholding cross-
entropy using computable threshold in general different from
1/2. A realistic label noise model based on Gaussian fields
is proposed and explicit formulations for marginal functions
are derived. Also, an efficient method for sampling noisy seg-
mentations is derived. Using the noise model, it is illustrated
how1/2-thresholded optimal solutions to cross-entropy and
soft-Dice are affected by various noise levels, and that the
resulting segmentations diverge as the noise is increased. It
is shown that optimal solutions to soft-Dice can be found by
thresholding the optimal solutions to cross-entropy with a
computable a priori unknown threshold than 1/2. Finally,
the theoretical observations is verified on real data from the
TotalSegmentor data set [32].
Limitations: There are two main limitations in this work.
Firstly, the theoretical results only hold exactly when the
volume of the noisy label is constant. Secondly, it is assumed
that the noise can be modeled with the proposed Gaussian
field model. In practice, noise will not follow such a model
exactly since it will often be the case that the amount of
noise will differ for different regions of the target structure.
Acknowledgement: The work was partially funded by
RaySearch Laboratories AB.
8References
[1]Samuel G Armato III, Geoffrey McLennan, Luc Bidaut,
Michael F McNitt-Gray, Charles R Meyer, Anthony P Reeves,
Binsheng Zhao, Denise R Aberle, Claudia I Henschke, Eric A
Hoffman, et al. The Lung Image Database Consortium (LIDC)
and Image Database Resource Initiative (IDRI): A Completed
Reference Database of Lung Nodules on CT Scans. Medical
Physics , 38(2):915–931, 2011.
[2]Jeroen Bertels, Tom Eelbode, Maxim Berman, Dirk Van-
dermeulen, Frederik Maes, Raf Bisschops, and Matthew B
Blaschko. Optimizing the Dice Score and Jaccard Index for
Medical Image Segmentation: Theory and Practice. In In-
ternational Conference on Medical Image Computing and
Computer-Assisted Intervention , pages 92–100. Springer,
2019.
[3]Jeroen Bertels, David Robben, Dirk Vandermeulen, and Paul
Suetens. Theoretical Analysis and Experimental Validation of
V olume Bias of Soft Dice Optimized Segmentation Maps in
the Context of Inherent Uncertainty. Medical Image Analysis ,
67:101833, 2021.
[4]Noah Bice, Neil Kirby, Ruiqi Li, Dan Nguyen, Tyler Bahr,
Christopher Kabat, Pamela Myers, Niko Papanikolaou, and
Mohamad Fakhreddine. A sensitivity analysis of probabil-
ity maps in deep-learning-based anatomical segmentation.
Journal of applied clinical medical physics , 22(8):105–119,
2021.
[5]Pete Bridge, Andrew Fielding, Pamela Rowntree, and Andrew
Pullar. Intraobserver Variability: Should We Worry? Journal
of Medical Imaging and Radiation Sciences , 47(3):217–220,
2016.
[6]Getao Du, Xu Cao, Jimin Liang, Xueli Chen, and Yonghua
Zhan. Medical image segmentation based on u-net: A review.
Journal of Imaging Science and Technology , 64:1–12, 2020.
[7]Charley Gros, Andreanne Lemay, and Julien Cohen-Adad.
SoftSeg: Advantages of Soft Versus Binary Training for Im-
age Segmentation. Medical Image Analysis , 71:102038, 2021.
[8]Yun Gu, Mali Shen, Jie Yang, and Guang-Zhong Yang. Reli-
able label-efficient learning for biomedical image recognition.
IEEE Transactions on Biomedical Engineering , 66(9):2423–
2432, 2018.
[9]Nicholas Heller, Joshua Dean, and Nikolaos Papanikolopou-
los. Imperfect segmentation labels: How much do they mat-
ter? In Intravascular Imaging and Computer Assisted Stent-
ing and Large-Scale Annotation of Biomedical Data and Ex-
pert Label Synthesis , pages 112–120. Springer, 2018.
[10] Burhan Rashid Hussein, Cédric Meurée, Malo Gaubert,
Arthur Masson, Anne Kerbrat, Benoît Combès, and Francesca
Galassi. A study on loss functions and decision thresholds
for the segmentation of multiple sclerosis lesions on spinal
cord mri. hal preprint hal-03865212 , 2022.
[11] Fabian Isensee, Paul F Jaeger, Simon AA Kohl, Jens Petersen,
and Klaus H Maier-Hein. nnu-net: a self-configuring method
for deep learning-based biomedical image segmentation. Na-
ture methods , 18(2):203–211, 2021.
[12] Davood Karimi, Haoran Dou, Simon K Warfield, and Ali
Gholipour. Deep Learning With Noisy Labels: ExploringTechniques and Remedies in Medical Image Analysis. Medi-
cal Image Analysis , 65:101759, 2020.
[13] Eytan Kats, Jacob Goldberger, and Hayit Greenspan. Soft
Labeling by Distilling Anatomical Knowledge for Improved
MS Lesion Segmentation. In 2019 IEEE 16th International
Symposium on Biomedical Imaging (ISBI 2019) , pages 1563–
1566. IEEE, 2019.
[14] Matthieu Lê, Jan Unkelbach, Nicholas Ayache, and Hervé
Delingette. Sampling image segmentations for uncertainty
quantification. Medical image analysis , 34:42–51, 2016.
[15] Andreanne Lemay, Charley Gros, and Julien Cohen-Adad. La-
bel Fusion and Training Methods for Reliable Representation
of Inter-Rater Uncertainty. arXiv preprint arXiv:2202.07550 ,
2022.
[16] Hang Li, Dong Wei, Shilei Cao, Kai Ma, Liansheng Wang,
and Yefeng Zheng. Superpixel-Guided Label Softening for
Medical Image Segmentation. In International Conference
on Medical Image Computing and Computer-Assisted Inter-
vention , pages 227–237. Springer, 2020.
[17] Zachary C Lipton, Charles Elkan, and Balakrishnan
Naryanaswamy. Optimal Thresholding of Classifiers to Maxi-
mize F1 Measure. In Joint European Conference on Machine
Learning and Knowledge Discovery in Databases , pages 225–
239. Springer, 2014.
[18] Alireza Mehrtash, William M Wells, Clare M Tempany, Pu-
rang Abolmaesumi, and Tina Kapur. Confidence calibration
and predictive uncertainty estimation for deep medical im-
age segmentation. IEEE transactions on medical imaging ,
39(12):3868–3878, 2020.
[19] Zahra Mirikharaji, Yiqi Yan, and Ghassan Hamarneh. Learn-
ing to Segment Skin Lesions From Noisy Annotations. In
Domain adaptation and representation transfer and medical
image learning with less labels and imperfect data , pages
207–215. Springer, 2019.
[20] Guy Nir, Soheil Hor, Davood Karimi, Ladan Fazli, Brian F
Skinnider, Peyman Tavassoli, Dmitry Turbin, Carlos F Vil-
lamil, Gang Wang, R Storey Wilson, et al. Automatic Grad-
ing of Prostate Cancer in Digitized Histopathology Images:
Learning From Multiple Experts. Medical Image Analysis ,
50:167–180, 2018.
[21] Marcus Nordström, Han Bao, Fredrik Löfman, Henrik Hult,
Atsuto Maki, and Masashi Sugiyama. Calibrated Surrogate
Maximization of Dice. In International Conference on Med-
ical Image Computing and Computer-Assisted Intervention ,
pages 269–278. Springer, 2020.
[22] Marcus Nordstrom, Henrik Hult, Fredrik Löfman, and Jonas
Söderberg. On image segmentation with noisy labels: Char-
acterization and volume properties of the optimal solutions to
accuracy and dice. Advances in Neural Information Process-
ing Systems , 35:34321–34333, 2022.
[23] Marcus Nordstrom, Henrik Hult, Atsuto Maki, and Fredrik
Löfman. Noisy image segmentation with soft-dice. arXiv
preprint arXiv , 2023.
[24] Tufve Nyholm, Stina Svensson, Sebastian Andersson, Joakim
Jonsson, Maja Sohlin, Christian Gustafsson, Elisabeth Kjel-
lén, Karin Söderström, Per Albertsson, Lennart Blomqvist,
et al. MR and CT Data With Multiobserver Delineations of
9Organs in the Pelvic Area—Part of the Gold Atlas Project.
Medical Physics , 45(3):1295–1300, 2018.
[25] Teodora Popordanoska, Jeroen Bertels, Dirk Vandermeulen,
Frederik Maes, and Matthew B Blaschko. On the Relation-
ship Between Calibrated Predictors and Unbiased V olume
Estimation. In International Conference on Medical Image
Computing and Computer-Assisted Intervention , pages 678–
688. Springer, 2021.
[26] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net:
Convolutional Networks for Biomedical Image Segmenta-
tion. In International Conference on Medical Image Com-
puting and Computer-Assisted Intervention , pages 234–241.
Springer, 2015.
[27] Axel-Jan Rousseau, Thijs Becker, Jeroen Bertels, Matthew B
Blaschko, and Dirk Valkenborg. Post Training Uncertainty
Calibration of Deep Networks for Medical Image Segmenta-
tion. In 2021 IEEE 18th International Symposium on Biomed-
ical Imaging (ISBI) , pages 1052–1056. IEEE, 2021.
[28] Nahian Siddique, Sidike Paheding, Colin P Elkin, and Vijay
Devabhaktuni. U-net and its variants for medical image seg-
mentation: A review of theory and applications. Ieee Access ,
9:82031–82057, 2021.
[29] Joao Lourenço Silva and Arlindo L Oliveira. Using Soft
Labels to Model Uncertainty in Medical Image Segmentation.
arXiv preprint arXiv:2109.12622 , 2021.
[30] Nima Tajbakhsh, Laura Jeyaseelan, Qian Li, Jeffrey N Chi-
ang, Zhihao Wu, and Xiaowei Ding. Embracing Imperfect
Datasets: A Review of Deep Learning Solutions for Medical
Image Segmentation. Medical Image Analysis , 63:101693,
2020.
[31] Eugene V orontsov and Samuel Kadoury. Label noise in seg-
mentation networks: mitigation must deal with bias. In Deep
Generative Models, and Data Augmentation, Labelling, and
Imperfections: First Workshop, DGM4MICCAI 2021, and
First Workshop, DALI 2021, Held in Conjunction with MIC-
CAI 2021, Strasbourg, France, October 1, 2021, Proceedings
1, pages 251–258. Springer, 2021.
[32] Jakob Wasserthal, Manfred Meyer, Hanns-Christian Breit,
Joshy Cyriac, Shan Yang, and Martin Segeroth. Totalsegmen-
tator: robust segmentation of 104 anatomical structures in ct
images. arXiv preprint arXiv:2208.05868 , 2022.
[33] Shaode Yu, Mingli Chen, Erlei Zhang, Junjie Wu, Hang Yu,
Zi Yang, Lin Ma, Xuejun Gu, and Weiguo Lu. Robustness
Study of Noisy Annotation in Deep Learning Based Med-
ical Image Segmentation. Physics in Medicine & Biology ,
65(17):175007, 2020.
[34] Ming-Jie Zhao, Narayanan Edakunni, Adam Pocock, and
Gavin Brown. Beyond Fano’s Inequality: Bounds on the
Optimal F-score, BER, and Cost-Sensitive Risk and Their
Implications. The Journal of Machine Learning Research ,
14(1):1033–1090, 2013.
10— Supplementary Document —
Marginal Thresholding in Noisy Image Segmentation
Marcus Nordström*
Department of Mathematics
KTH Royal Institute of Technology
Stockholm, Sweden
marcno@kth.seHenrik Hult
Department of Mathematics
KTH Royal Institute of Technology
Stockholm, Sweden
hult@kth.se
Atsuto Maki
Department of Computer Science
KTH Royal Institute of Technology
Stockholm, Sweden
atsuto@kth.se
Contents
1. Proofs 2
1.1. Proof of Theorem 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
1.2. Proof of Theorem 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
1.3. Proof of Theorem 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
2. Experiments 4
2.1. Computing the marginals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
2.2. Sampling noisy segmentations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
2.3. Dice optimal segmentations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
*Author is also affiliated with RaySearch Laboratories.arXiv:2304.04116v3  [cs.CV]  8 Jul 20231. Proofs
1.1. Proof of Theorem 1
First, since the components of Xare independent Gaussian fields with covariance kernel
ka,b(ω, ω′) =a2exp/parenleftbigg
−∥ω−ω′∥2
2
2b2/parenrightbigg
, ω, ω′∈Rn, (1)
it follows that
Var((X1(ω), . . . , X n(ω))T) = (ka,b(ω, ω), . . . , k a,b(ω, ω))T= (a2, . . . , a2)T, (2)
and consequently that the marginal probability densities X(ω), ω∈Ωare given by
X(ω)∼pa2(x), x∈Rn. (3)
Now, let
ˆl(ω) =/braceleftigg
l(ω)ifω∈Ω,
0 otherwise ,(4)
which means that
L(ω)d=ˆl(ω+X(ω)), (5)
whered=denotes equal in distribution. This together with the law of the unconscious statistician and the symmetry pa2(x) =
pa2(−x), x∈Rnthen for any ω∈Ωyields
E[L(ω)] =E[ˆl(ω+X(ω))] (6)
=/integraldisplay
Rnˆl(ω+x)pa2(x)λ(dx) (7)
=/integraldisplay
Rnˆl(ω′)pa2(ω′−ω)λ(dω′) (8)
=/integraldisplay
Rnˆl(ω′)pa2(ω−ω′)λ(dω′) (9)
=/integraldisplay
Ωl(ω′)pa2(ω−ω′)λ(dω′). (10)
This completes the proof.
1.2. Proof of Theorem 2
By Theorem 1, changing the order of integration and recalling that/integraltext
Rnpa2(ω−ω′)λ(dω) = 1 for any ω′∈Rn, it follows that
2E[∥L∥1] =E/bracketleftbigg/integraldisplay
Ω|L(ω)|λ(dω)/bracketrightbigg
(11)
=E/bracketleftbigg/integraldisplay
ΩL(ω)λ(dω)/bracketrightbigg
(12)
=/integraldisplay
ΩE[L(ω)]λ(dω) (13)
=/integraldisplay
Ω/integraldisplay
Ωl(ω′)pa2(ω−ω′)λ(dω′)λ(dω) (14)
=/integraldisplay
Ωl(ω′)/bracketleftbigg/integraldisplay
Rnpa2(ω−ω′)λ(dω)/bracketrightbigg
λ(dω′)−/integraldisplay
Ωl(ω′)/bracketleftigg/integraldisplay
Rn\Ωpa2(ω−ω′)λ(dω)/bracketrightigg
λ(dω′) (15)
=/integraldisplay
Ωl(ω′)λ(dω′)−ξ (16)
=/integraldisplay
Ω|l(ω′)|λ(dω′)−ξ (17)
=∥l∥1−ξ. (18)
This completes the proof.
1.3. Proof of Theorem 3
Independently scattered measures are introduced more generally for α-stable distributions in [ 3, Section 3.3]. The Gaussian
case used in this work is simply the special case when α= 2. Let Wbe an independently scattered Gaussian measure
onRn, with control measure λ. That is, with B0being the Lebesgue measurable sets with finite measure, for any finite
collection, A1, . . . , A kof disjoint sets in B0, the random variables W(A1), . . . , W (Ak)are independent, and W(Ai)has
centered Gaussian distribution with variance λ(Ai). Forf∈L2(Rn)the stochastic integral I(f) =/integraltext
Rnf(ω)W(dω)is well
defined with centered Gaussian distribution with variance ∥f∥2
L2. In fact, {I(f), f∈L2(Rn)}is a Gaussian process indexed
byL2(Rn). In particular, for a given f∈L2(Rn), the process Y={Y(ω), ω∈Rn}given by
Y(ω) =/integraldisplay
Rnf(ω−ω′)W(dω′), (19)
is a centered Gaussian process with covariance kernel given by
k(ω, ω′) =/integraldisplay
Rnf(ω−u)f(ω′−u)λ(du). (20)
For the squared exponential kernel
k(ω, ω′) =a2exp/parenleftbigg
−∥ω−ω′∥2
2b2/parenrightbigg
(21)
we can identify fas
f(ω) =a
(πb2/2)n/4exp/parenleftbigg
−∥ω∥2
b2/parenrightbigg
. (22)
3Indeed, for any ω, ω′∈Rnit follows, after a completion of the square, that
k(ω, ω′) =/integraldisplay
Rnf(ω−u)f(ω′−u)λ(du) (23)
=a2
(πb2/2)n/2/integraldisplay
Rnexp/parenleftbigg
−∥ω−u∥2
2
b2/parenrightbigg
exp/parenleftbigg
−∥ω′−u∥2
2
b2/parenrightbigg
λ(du) (24)
=a2
(πb2/2)n/2exp/parenleftbigg
−∥ω−ω′∥2
2
2b2/parenrightbigg/integraldisplay
Rdexp/parenleftigg
−∥u−ω+ω′
2∥2
2
2(b/2)2/parenrightigg
λ(du) (25)
=a2(2π(b/2)2)d/2
(πb2/2)d/2exp/parenleftbigg
−∥ω−ω′∥2
2
2b2/parenrightbigg
(26)
=a2exp/parenleftbigg
−∥ω−ω′∥2
2
2b2/parenrightbigg
(27)
Now consider the isotropic normal density in dimension dwith variance b2/2
pb2/2(ω) =1
(πb2)d/2exp/braceleftbigg
−∥ω∥2
2
b2/bracerightbigg
(28)
and note that it can be used to rewrite fas follows
f(ω) =a(2πb2)n/4pb2/2(ω). (29)
Consequently, it follows that
Y(ω) =a(2πb2)d/4/integraldisplay
Rnpb2/2(ω−ω′)W(dw′). (30)
Now, let W1, . . . , W nbe independent copies of W, then it follows that
X(ω)d=
a(2πb2)n/4/integraltext
Rnpb2/2(ω−ω′)W1(dw′)
...
a(2πb2)n/4/integraltext
Rnpb2/2(ω−ω′)Wn(dw′)
, (31)
whered=denotes equal in distribution. This completes the proof.
2. Experiments
The experimental code is composed of two parts. The first part is for extracting the data. The second part is for training the
model. Most of the code is straight forward and similar to what would be found in any standard implementation of a UNet
trained with either cross-entropy or soft-Dice. There are however three methods based on the theory described in this paper
that needs clarification.
2.1. Computing the marginals
import numpy as np
import s c i p y . ndimage
def g e t _ n o i s y _ m a r g i n a l s ( l , a ) :
s c a l e d _ a = np . a r r a y ( a ) *np . a r r a y ( l . shape )
n o i s y _ m a r g i n a l s = s c i p y . ndimage . g a u s s i a n _ f i l t e r (m, s c a l e d _ a , mode= ’ c o n s t a n t ’ )
return n o i s y _ m a r g i n a l s
Listing 1. Python code for computing the marginals associated with a noisy segmentation that is formed by the noise free segmentation land
the noise strength parameter a. The code is a direct implementation of the Theorem 1.
4The first method is for computing the marginals associated with a particular noisy segmentation and is based on Theorem 1.
The code for this method is listed in Listing 1. The input is composed of: la discretized version of the noise-free reference
segmentation represented as a multidimensional numpy array and athe parameter to the noise model encoding the strength
of the noise represented as a floating point number. The output is a discretized version of the marginals represented as a
multidimensional numpy array.
2.2. Sampling noisy segmentations
import numpy as np
import s c i p y . ndimage
def g e t _ n o i s y _ s a m p l e ( l , a ) :
b = 0 . 1 5 *np . s q r t ( 2 )
s c a l e d _ a = np . a r r a y ( a ) *np . a r r a y ( l . shape )
s c a l e d _ b = np . a r r a y ( b ) *np . a r r a y ( l . shape )
w e i g h t = s c a l e d _ a *(2*np . p i *s c a l e d _ b **2)**(len( np . shape (m) ) / 4 )
p er b = np . a r r a y ( [ w e i g h t *s c i p y . ndimage . g a u s s i a n _ f i l t e r (
np . random . normal ( s i z e = l . shape ) , s c a l e d _ b [ i ] / np . s q r t ( 2 ) , mode= ’ c o n s t a n t ’ )
for iin range (len( l . shape ) ) ] )
grid_mesh = np . meshgrid ( *[range ( l . shape [ i ] ) for iin range (len( l . shape ) ) ] ,
i n d e x i n g = ’ i j ’ )
n o i s y _ s a m p l e = np . round ( s c i p y . ndimage . m a p _ c o o r d i n a t e s ( l , grid_mesh +perb , mode= ’
n e a r e s t ’ ) )
return n o i s y _ s a m p l e
Listing 2. Python code for computing a random sample associated with a noisy segmentation that is formed by the noise free segmentation l
and the noise strength parameter a. The code is a direct implementation of Theorem 3.
The second method is for sampling noisy segmentations and is based on Theorem 3. The code for this method is listed
in Listing 2. The input is composed of: la discretized version of the noise-free reference segmentation represented as a
multidimensional numpy array and athe parameter to the noise model encoding the strength of the noise represented as a
floating point number. The output is a discretized version of a random sample associated with the noisy segmentation and is
represented as a multidimensional numpy array. The method can be broken down into three steps. Firstly the constant used for
rescaling is computed. Secondly, the vector of Gaussian fields is generated, which in the numerical setting is approximated by
drawing i.i.d. Gaussian variables for each entry in land then processing the resulting array with a Gaussian filter. Thirdly, the
noise-free segmentation lis deformed with the resulting random deformation array.
2.3. Dice optimal segmentations
import numpy as np
def g e t _ o p t _ d i c e _ s e g (m) :
p s i = np . f l i p ( np . s o r t (m. f l a t t e n ( ) ) )
d = 2 *np . cumsum ( p s i ) / ( np . sum (m) +np . a r a n g e ( 1 , len( p s i ) +1) )
t = np . max ( d ) / 2
s = 1 . 0 *(m>= t )
return s
Listing 3. Python code for generating a Dice optimal segmentation from a marginal function m.
The third method is for computing the optimal segmentation with respect to Dice. The code for this method is listed in
Listing 3 and is taken from [ 2]. It is an efficient variation of a method proposed in binary classification [ 1]. The input is
composed of: ma discretized version of the the marginal function represented as a multidimensional numpy array. The output
is a discretized version of the optimal segmentation with respect to Dice represented as a multidimensional numpy array. The
idea is to sort the voxels in mfrom largest to smallest, and then compute the Dice score associated with the segmentations that
5are formed by taking the first set of voxels associated with this sorted list. This is done for every possible number of voxels.
The maximal score is used to form a threshold which is used to formulate the final segmentation.
References
[1]Zachary C Lipton, Charles Elkan, and Balakrishnan Naryanaswamy. Optimal Thresholding of Classifiers to Maximize F1 Measure. In
Joint European Conference on Machine Learning and Knowledge Discovery in Databases , pages 225–239. Springer, 2014.
[2]Marcus Nordstrom, Henrik Hult, Fredrik Löfman, and Jonas Söderberg. On image segmentation with noisy labels: Characterization and
volume properties of the optimal solutions to accuracy and dice. Advances in Neural Information Processing Systems , 35:34321–34333,
2022.
[3]Gennady Samoradnitsky and Murad S. Taqqu. Stable non-Gaussian random processes: stochastic models with infinite variance.
Chapman and Hall, 1994.
6