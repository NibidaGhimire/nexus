arXiv:2305.05573v2  [cs.LG]  15 Jun 2023An AlgorithmforAdversary AwareDecentralized Networked
MARL
Soumajyoti Sarkar1
California,United States
sarkar.soumajyoti@gmail.com
ABSTRACT
Decentralizedmulti-agentreinforcementlearning(MARL) algorithms
have become popular in the literature since it allows hetero ge-
neous agents to have their own reward functions as opposed to
canonicalmulti-agentMarkovDecisionProcess(MDP)setti ngswhich
assumecommonrewardfunctionsoverallagents.Inthiswork ,we
followtheexistingworkoncollaborativeMARLwhereagents ina
connectedtimevaryingnetworkcanexchangeinformationam ong
eachotherinordertoreachaconsensus.Weintroducevulner abili-
tiesintheconsensusupdatesofexistingdecentralizedMAR Lalgo-
rithmswhere someagents candeviatefromtheir usualconsen sus
update, who we term as adversarial agents. We then proceed to
provide an algorithm that allows non-adversarial agents to reach
aconsensusinthepresenceofadversariesunderaconstrain edset-
ting.
ACMReference Format:
Soumajyoti Sarkar2. 2023. An Algorithm for Adversary Aware Decentral-
ized Networked MARL. In Proceedings of ACM Conference (Conference’17).
ACM,NewYork,NY,USA,5pages.https://doi.org/10.1145/nnnnnnn.nnnnnnn
1 INTRODUCTION
Multi-agentreinforcementlearningconstitutesareinfor cementlearn-
ing scenario where multiple agents participate to jointly l earn an
optimal policy and where optimality is deﬁned in terms of som e
objective[2].Inacollaborativesetting,theagentshavet oworkto-
gether with a goal to optimizea shared reward metric [12]. Mu lti-
agent environments areinherently non-stationary since th eother
agents are free to change their behavior as they also learn an d
adapt and this makes designing algorithms for MARL more com-
plex that single agent systems. Our model diﬀers from the tra -
ditional collaborative and fully co-operative multi-agen t systems
in that the agents in our system are heterogeneous and can hav e
diﬀerent local reward functions. We work on sequential deci sion-
makingproblemsinwhichtheagentsrepeatedlyinteractwit htheir
Permission to make digital or hard copies of all or part of thi s work for personal or
classroomuseisgrantedwithoutfeeprovidedthatcopiesar enotmadeordistributed
for proﬁt or commercial advantage and that copies bear this n otice and the full cita-
tionontheﬁrstpage.Copyrightsforcomponents of thiswork owned byothersthan
ACM must be honored. Abstracting with credit is permitted. T o copy otherwise, or
republish,to post on serversor to redistributeto lists, re quirespriorspeciﬁc permis-
sionand/or afee. Request permissionsfrompermissions@ac m.org.
Conference’17,July 2017,Washington,DC,USA
© 2023 Associationfor Computing Machinery.
ACM ISBN978-x-xxxx-xxxx-x/YY/MM...$15.00
https://doi.org/10.1145/nnnnnnn.nnnnnnn
2Thisworkhasbeendone while the authorwasat ArizonaState U niversityand
priorto joining Amazonenvironmentandtrytojointlyoptimizethelong-termrewar dthey
receivefromthesystem,whichdependsonasequenceofdecis ions
made by them and the signals shared by other agents in the net-
work.
Existingvalue-factorizedbasedMARLapproachesperformw ell
in various multi-agent cooperative environments under the cen-
tralizedtraininganddecentralizedexecution(CTDE)sche me,where
all agents in a team are trained together by the centralized v alue
networkandeachagentexecutesitspolicyindependently[1 4,15].
Oneoftheexistingissuesinsuchsystemsisthatinthecentr alized
trainingprocess,theenvironment fortheteamis partially observ-
able and non-stationary. The observation and action inform ation
of all the agents cannot represent the global states. What fo llows
isthattheexistingmethodsperformpoorlyandarehighlysa mple
ineﬃcient. In these regimes, regret minimization is a promi sing
approachasitperformswellinpartiallyobservableandful lycom-
petitive settings. However, it tends to model others as oppo nents
and thus cannot work well under the CTDE scheme. In our work,
we set out to respect two constraints: ﬁrst, we want the agent s to
befullydecentralizedeven intheirtrainingandsecond,we follow
the model free actor critic type algorithms for our collabor ative
MARLsetup as opposedto Q-learning typealgorithms or centr al-
izedtraining with modelbased approaches [28].
Themajordiﬀerenceinthekindofalgorithmswefollowinthi s
paper with that of CTDE techniques is that our work follows th e
studies on consensus based distributed optimization [19] w here
theagentsexchangeparametersinsteadoftheiractionsand states
with other agents [14]. There is no centralized training and each
agentperformsthecriticstepindependently.Theseconsen susbased
systemshavebeenpopularinmanyﬁeldslikesensornetworks ,so-
cial learning, co-ordination of vehicles and co-ordinated synchro-
nous distributed optimization to name a few. However, most o f
these studies evade the risk of adversaries or agents that de viate
from consensus updates. For example, in studies where human s
participateindecisionmakingandsociallearningovertim e[4,23],
presence of adversarial nodes could disrupt the learning dy nam-
ics. The study in [8] showed that a single adversarial agent c an
persuade all the other agents in the network to implement pol i-
cies that optimize an objective that it desires. It then beco mes im-
portantto answer whether we can modify the existing consens us
based decentralized MARL algorithms to respond to the prese nce
of malicious agents who deviate from the consensus rule upda tes
or whether the non-malicious agents can still converge to th e op-
timalsolutioninthepresence ofadversaries.
Westudythesetupwheretherewardsorthedatafortheagents
arenot corruptedbutinstead where theadversarial agents d o not
follow the consensus updates to obtain a joint optimal polic y.Weconsider the scenario where there could exist more than a sin gle
adversarialagentinoursetup.Section2providesthetechn icalpre-
liminaries of our networked MARL setup and Section 3 discuss es
theadversarialsettingsandtheadversaryawareconsensus MARL
algorithm.
2 DECENTRALIZED NETWORKED MARL
Our work is heavily inﬂuenced by the research on decentraliz ed
multi-agent reinforcement learning done in [31]. The autho rs de-
velop a consensus based reinforcement learning algorithm i n a
fully decentralized and networked setting. We primarily ex tend
theirworkinanetworkedsettingwheremaliciousagentsare present
underaconstrainedsetup.OurnetworkedmultiagentMDPisc on-
structed as follows: we have a state space Sshared by all agents
Nin a network, such that A/u1D456,/u1D456∈|N|is the actionof agent /u1D456. Let
A=/producttext.1/u1D441
/u1D456=1A/u1D456be the joint action space of all agents. Following
this,R/u1D456:S×A→Ris the local reward function of agent /u1D456and
/u1D443:S×A×S→[ 0,1]is the state transition probability of the
MDP.Weassumethatthestatesareobservablegloballyinord erto
ensure co-operation among agents, but the rewards are obser ved
locally as a function of the neighbors of the agent. At time /u1D461, an
agent/u1D456choosesanaction /u1D44E/u1D456
/u1D461given state/u1D460/u1D461,followingitsownlocal
policy/u1D70B/u1D456:S×A/u1D456→[0,1], where/u1D70B/u1D456(/u1D460,/u1D44E/u1D456)represents the proba-
bilityofchoosingaction /u1D44E/u1D456atstate/u1D460.Inthissetup,thejointpolicy
of allagents is given by /u1D70B(/u1D460,/u1D44E)=/producttext.1
/u1D456∈N/u1D70B/u1D456(/u1D460,/u1D44E/u1D456).
Foranagent /u1D456,thelocalpolicyisparameterizedby /u1D70B/u1D456
/u1D703where/u1D703/u1D456∈
Θ/u1D456aretheparametersandthejointpolicyisgivenby Θ=/producttext.1/u1D441
/u1D456=1Θ/u1D456.
As is standard in the assumptions of actor-critic algorithm s with
function approximation, the policy function /u1D70B/u1D456
/u1D703(/u1D460,/u1D44E/u1D456)>0 for any
/u1D703/u1D456∈Θ/u1D456. We assume that /u1D70B/u1D456
/u1D703/u1D456(/u1D460,/u1D44E/u1D456)is continuously diﬀerentiable
with respectto theparameter /u1D703/u1D456overΘ/u1D456.For all/u1D703∈Θ, thetransi-
tionmatrixof theMarkovchain {/u1D460/u1D461}/u1D461≥0inducedbypolicy /u1D70B/u1D703,for
any/u1D460,/u1D460′∈Sis given by
/u1D443/u1D703(/u1D460′|/u1D460)=/summationdisplay.1
/u1D44E∈A/u1D70B/u1D703(/u1D460,/u1D44E)·/u1D443(/u1D460′|/u1D460,/u1D44E) (1)
Itcomesalongwiththeassumptionthat {/u1D460/u1D461}/u1D461≥0isirreducibleand
aperiodicunderany /u1D70B/u1D703,andthechainofthestate-actionpair {(/u1D460/u1D461,/u1D44E/u1D461)}
has a stationarydistribution /u1D451/u1D703(/u1D460)·/u1D70B/u1D703(/u1D460,/u1D44E)forallpairs.
Objective of the MARL: The collective objective of the agents
is to collaborate and ﬁnd a policy /u1D70B/u1D703such that it maximizes the
averagegloballongtermrewardswhileonlyutilizinginfor mation
that is local to the agents in the network. In that context, le t/u1D45F/u1D456
/u1D461+1
denote the reward received by agent /u1D456at time/u1D461. Then the goal of
all agents collectivelyis tooptimizethefollowingobject ives:
/u1D45A/u1D44E/u1D465/u1D703/u1D43D(/u1D703)=/u1D459/u1D456/u1D45A/u1D4471
/u1D447E/parenleftBig/u1D447−1/summationdisplay.1
/u1D461=01
/u1D441/summationdisplay.1
/u1D456∈N/u1D45F/u1D456
/u1D461+1/parenrightBig
=/summationdisplay.1
/u1D460∈S,/u1D44E∈A/u1D451/u1D703(/u1D460)/u1D70B/u1D703(/u1D460,/u1D44E)·/u1D445(/u1D460,/u1D44E).(2)
where/u1D445(/u1D460,/u1D44E)=1
/u1D441·/summationtext.1
/u1D456∈N/u1D445/u1D456(/u1D460,/u1D44E)is the globallyaveraged reward
function. Following this, we have /u1D445(/u1D460,/u1D44E)=E[/u1D45F/u1D461+1|/u1D460/u1D461=/u1D460,/u1D44E/u1D461=
/u1D44E]where/u1D45F/u1D461=1
/u1D441/summationtext.1
/u1D456∈N/u1D461/u1D456
/u1D461. In that context of the symbols, theglobal state value and action value functions can be denoted by
Q/u1D703(/u1D460,/u1D44E)=/summationtext.1
/u1D461E[/u1D45F/u1D461+1−/u1D43D(/u1D703)|/u1D4600=/u1D460,/u1D44E0=/u1D44E,/u1D70B/u1D703]andV/u1D703(/u1D460)=/summationtext.1
/u1D44E∈A/u1D70B/u1D703(/u1D460,/u1D44E)Q/u1D703(/u1D460,/u1D44E).
PolicyGradientwithMARL: TodevelopanalgorithmforMARL,
wewouldapplythepolicygradient theoremasmentioned inTh e-
orem3.1in[31].Forany /u1D703∈Θ,let/u1D70B/u1D703beapolicyand /u1D43D(/u1D703)denote
the globally averaged return, we deﬁne the local advantage f unc-
tion/u1D434/u1D456
/u1D703:S×A→Ras/u1D434/u1D456
/u1D703(/u1D460,/u1D44E)=Q/u1D703(/u1D460,/u1D44E)−˜/u1D449/u1D456
/u1D703(/u1D460,/u1D44E−1),whereQ/u1D703
and/u1D434/u1D703are the corresponding global action-value and advantage
functions and ˜/u1D449/u1D456
/u1D703(/u1D460,/u1D44E−1)is the local value function. The gradient
ofthepolicyis given by
∇/u1D703/u1D456/u1D43D(/u1D703)=E/u1D460∼/u1D451/u1D703,/u1D44E∼/u1D70B/u1D703[∇/u1D703/u1D456/u1D459/u1D45C/u1D454/u1D70B/u1D456
/u1D703/u1D456(/u1D460,/u1D44E/u1D456)·/u1D434/u1D456
/u1D703(/u1D460,/u1D44E)](3)
3 ADVERSARY AWARE MARLALGORITHM
3.1 Non Adversarial Setting
We ﬁrst present the existing work under a non-adversarial se t-
ting.Here,heactor-criticbasedconsensusalgorithmfort heMARL
setup is as follows: we consider an agent speciﬁc local versi on of
the global action-value function /u1D444/u1D703which we denote as Q(/u1D714/u1D456)
where we hide the state, action factors in the function which is
implicit and /u1D714∈R/u1D448, where the dimension /u1D448is signiﬁcantly less
thanthejointstateactionspace.Inordertousethepolicyg radient
theorem discussed in the previous section to improve an agen ts’
policy, each agent shares its local parameters /u1D714/u1D456with its neigh-
bors on the network in order to reach an estimate of Q/u1D703that is
consensualamongallagentsinthenetwork.Suchdistribute dcon-
sensus algorithms have been proposedearlier [19] that guar antee
convergence ofthelocalagent functions.
3.2 Adversary AwareDecentralized MARL
In order to introduce the setup behind the MARL environment
where malicious agents are a part of the network, we ﬁrst deﬁn e
certain notations based on the setup of consensus based adve r-
sarial attacks in [27]. We consider an undirected network N=
/u1D449(/u1D441),/u1D438(/u1D441), where/u1D449(/u1D441)={/u1D4631,1...,/u1D463/u1D45B}and/u1D438(/u1D441)denotes the
edgesconnectingpairsofnodes.Notethat /u1D463/u1D456isjustanexaggerated
notationtodenoteanagentinagraphasopposedto /u1D456thatwehave
usedintheprevious sectionsbutthey refer tothesameagent .De-
notingK/u1D456astheneighborhoodverticesof /u1D456∈/u1D449(/u1D441),forany/u1D45F∈N,
asubset/u1D446⊂/u1D449(/u1D441)issaidtobe r-local,if|K/u1D456∩/u1D446|≤/u1D45F∀/u1D463/u1D456∈/u1D449(/u1D441)\/u1D446.
That is to say, for r-localsubset, there are at most /u1D45Fnodes in the
neighborhoodofanyvertexfrom /u1D449(/u1D441)\/u1D446.Forthesetupofadver-
sarialattacksinthemulti-agentsystem,weconsiderthefo llowing:
we consider that there are randomly chosen adversarial node s in
thenetworksuchthatforeachnode /u1D463/u1D456,therecannotbemorethan
/u1D454fraction of its neighbors who are adversaries, where /u1D454∈[0,1).
This fraction /u1D454is known toall nodes in thenetwork, however the
regularnodesdonotknowwhichorifanyoftheirneighbornod es
areadversaries.Hereweassumethattheadversariesareres tricted
toforman F-localset ,where/u1D439is a non-negative integer.Algorithm 1 Adversary awareNetworked actor-criticalgorithm
1:Input:Initial values of the parameters /u1D7070/u1D456,/u1D714/u1D456
0, ˜/u1D714/u1D456
0,/u1D703/u1D456
0,∀/u1D456∈
N; theinitial state /u1D4600oftheMDPand stepsizes {/u1D6FD/u1D714,/u1D461}/u1D461≥0and
{/u1D6FD/u1D703,/u1D461}/u1D461≥0,/u1D439,/u1D449(/u1D441)
2:Repeat:
3:for all/u1D456∈Ndo
4: Observe state /u1D460/u1D461+1and reward /u1D45F/u1D456
/u1D461+1.
5: Update/u1D707/u1D456
/u1D456+1←(1−/u1D6FD/u1D714,/u1D461)·/u1D707/u1D456
/u1D461+/u1D6FD/u1D714,/u1D461·/u1D45F/u1D456
/u1D461+1.
6: Selectand executeaction /u1D44E/u1D456
/u1D461+1∼/u1D70B/u1D456
/u1D703/u1D456
/u1D461(/u1D460/u1D461+1,·).
7:end for
8:Observe joint actions /u1D44E/u1D461+1=(/u1D44E1
/u1D461+1,...,/u1D44E/u1D441
/u1D461+1).
9:for all/u1D456∈Ndo
10: Update/u1D6FF/u1D456
/u1D461←/u1D45F/u1D456
/u1D461+1-/u1D707/u1D456
/u1D461+Q/u1D461+1(/u1D714/u1D456
/u1D461)-Q/u1D461(/u1D714/u1D456
/u1D461).
11: CriticStep : ˜/u1D714/u1D456
/u1D461←/u1D714/u1D456
/u1D461+/u1D6FD/u1D714,/u1D461·/u1D6FF/u1D456
/u1D461·∇/u1D714Q/u1D461(/u1D714/u1D456
/u1D461).
12: UpdateA/u1D456
/u1D461←Q/u1D461(/u1D714/u1D456
/u1D461)-/summationtext.1
/u1D44E/u1D456∈A/u1D456/u1D70B/u1D703/u1D456
/u1D461(/u1D460/u1D461,/u1D44E/u1D456·Q(/u1D460/u1D461,/u1D44E/u1D456,
/u1D44E−1;/u1D714/u1D456
/u1D461).
13: Update/u1D713/u1D456
/u1D461←∇/u1D456
/u1D703log/u1D70B/u1D456
/u1D703/u1D456
/u1D461(/u1D460/u1D461,/u1D44E/u1D456
/u1D461).
14: Actor Step :/u1D703/u1D456
/u1D461+1←/u1D703/u1D456
/u1D461+/u1D6FD/u1D703,/u1D461·A/u1D456
/u1D461·/u1D713/u1D456
/u1D461.
15: Send/u1D714/u1D456
/u1D461totheneighbors{/u1D457∈N:(/u1D456,/u1D457)∈E/u1D461}over the
communicationnetwork G/u1D461.
16:end for
17:for all/u1D456∈Ndo
18: Gather neighbors /u1D43E/u1D456from/u1D449(/u1D441)
19: Gather/u1D70F/u1D456(/u1D461)byremoving thehighest and lowest
/u1D439states among /u1D43E/u1D456
20: ConsensusStep :/u1D714/u1D456
/u1D461+1←/summationtext.1
/u1D457∈/u1D70F/u1D456/u1D450/u1D461(/u1D456,/u1D457)·˜/u1D714/u1D457
/u1D461.
21:end for
22:Updatetheiterationcounter /u1D461←/u1D461+1.
23:Until Convergence
3.3 Algorithm
Theoverallactor-criticalgorithmisdetailedinAlgorith m1.Inthe
actor step, each agent, each agent improves it policy as show n in
Line14,where /u1D6FD/u1D703,/u1D461>0isthestepsize.Notethatboththeactorand
thecriticstepscanbeexecutedinadecentralizedfashionw ithout
any centralized training. For the consensus step, one impor tant
thing to note that since the agents aim to optimize the global ly
averaged reward function /u1D45F, the agents share their local parame-
ters/u1D714/u1D456withtheirneighbors andthisallowstheagents toimprove
theircurrentpolicyusingthepolicygradienttheorem.The param-
eter sharing involves a consensus updateusing theweight ma trix
/u1D436/u1D461=[/u1D450/u1D461(/u1D456,/u1D457)]|N|×|N|such that/u1D450/u1D461(/u1D456,/u1D457)is the weight on the mes-
sage sent by agent /u1D457to agent/u1D456at time/u1D461. An important restriction
in our model is that we only consider /u1D450/u1D461(/u1D456,/u1D457)>0 if agent/u1D456and/u1D457
are neighbors of each other in the network. Some discussions on
thechoiceof /u1D450/u1D461(/u1D456,/u1D457)is inthelaterpartof this section.
We modify Algorithm 1 in [31] to incorporate co-ordinated re -
sponses by regular nodes in the presence of adversaries. At e ach
time step, the regular nodes {/u1D463/u1D456}gather ˜/u1D714/u1D457
/u1D461for all/u1D463/u1D457∈K/u1D456and re-
movethehighest andlowest /u1D439statesandtheremaining nodesare
denoted by /u1D70F/u1D456(/u1D461)∈K/u1D456. The consensus action in the actor is then
toaggregatetheparametersfromtheneighborsin /u1D70F/u1D456(/u1D461).Notethat,these updates are only done by the regular nodes and the adver -
sarialnodesaggregatetheweights fromtheneighborsinany way
theywish.Thedynamics ofupdatesbytheregularnodesarelo cal
anddecentralizedsincetheydonotrequireregularnodesto know
anything beyond the signals sent from their neighbors. As me n-
tionedin[27],whenthenetwork Nistime-invariant,theeﬀective
neighbor set /u1D70F/u1D456(/u1D461)is only a function of the states of neighbors of
/u1D463/u1D456at time step /u1D461. This ﬁltering is also closely connected to bandit
based ranking of arms where the distribution of the means of t he
armsdecidewhicharmwouldbepicked,albeithereinsteadof pick-
ingonearm,theagentselectsoneormultiplearms[6].There stof
theconsensusstylealgorithminthisdecentralizedenviro nmentis
thesameas Algorithm1 in[31].
The standard assumptions for the consensus matrix is deﬁned
in Assumption 4.4 of [31] and while there can be many ways to
deﬁne the weight, one popular way is to consider the notion of
Metropolisweights [29]
/u1D450/u1D461(/u1D456,/u1D457)=/braceleftBig
1+/u1D45A/u1D44E/u1D465[/u1D458/u1D461(/u1D456),/u1D458/u1D461(/u1D457)]/bracerightBig−1
,∀(/u1D456,/u1D457)∈E/u1D461
/u1D450/u1D461(/u1D456,/u1D457)=1−/summationdisplay.1
/u1D457∈/u1D70F/u1D456/u1D450/u1D461(/u1D456,/u1D457)∀/u1D456∈N
whereE/u1D461denotes the time varying edges and /u1D458/u1D461(/u1D456)is the degree
oftheagent /u1D456in thetimevarying network.
4 RELATED WORK
Oneoftheassumptionsintheprocessofsocialdecisionmaki ngis
that individuals following a learning trajectory(despite each indi-
vidualhavingalimitedmemory)successfullyconvergestot hebest
optionfor thecollectivepopulation.Whileindividuals pa rticipate
indecisionmakingwheretheyassociatediﬀerentriskandre wards
inanuncertainenvironment, they alsotend toincorporateb eliefs
from their immediate neighbors in a networked environment, a
phenomenon that has played a critical role in co-operative m ulti-
agent systems [4].This begetsthequestionas tohowresilie nt are
theseconsensusalgorithmsandhowcanagentsadapttodecen tral-
izedtraininginthepresenceofmaliciousagentsoradversa ries.We
brieﬂyhighlysomenotablestudiesdonepreviouslyatthein tersec-
tionof MARLand
MARL: Theﬁeldof MARLhas evolved very rapidlyover thepast
few years. The collective goal of the multi-agent system is t o ei-
ther reach a stable and consensus state for all agents [7], or solve
a static optimization problem in a distributed fashion[19] . How-
ever, competition and collaboration always emerge between au-
tonomousagents that learnby reinforcement over ﬁnite hori zons.
Some of the common approaches on modeling and solving coop-
erative multi-agent reinforcement learning problems incl ude: (1)
independent learners [10, 13], (2) fully observable critic [16], (3)
value function factorization [26], (4) consensus based RL [ 3], and
(5)learning tocommunicate[17].
Decentralized Networked MARL : The challenge with fully co-
operative multi-agent systems is that fully cooperative sy stems
(Dec-POMDPs) are signiﬁcantly harder to solve than single a gent
RL problems due to the combinatorial explosion in the joint a c-
tionstatespaces combiningallagents.ForthisreasonCTDE havebecome popular. The prevailing MARL paradigm of centralise d
trainingwithdecentralisedexecution(CTDE)[25,29,21]a ssumesa
trainingstageduringwhichthelearningalgorithmcanacce ssdata
from all agents to learn decentralised (locally-executabl e) agent
policies.CTDEalgorithmssuchasCOMA[9]learnpowerfulcr itic
networksconditionedonjointobservationsandactionsofa llagents.
OtherextensionsofMADDPincludesharedexperienceactor- critic
frameworks [5] for eﬃcient MARL exploration by sharing expe -
rience amongst agents as opposed to MADDPG which only rein-
forcesanagent’sownexploredactions.Therehavealsobeen stud-
ies conducted on making these RL systems in networked system s
scalabale[20].
Decision making for social systems : There are two broad av-
enues ofresearch ondecisionmakingininteractiveenviron ments
consideringsocialsystems.Theﬁrstgroupofstudiesfocus ongame
theoreticenvironments whereagents havesimilar adaptati onand
learning abilities and so the actions of each agent aﬀect the task
achievementoftheotheragents[1,18,21].Thepayoﬀofthea gents
in these games depend on whether they are purely collaborati ve
orcompetitiveoramix.InarecentstudyoncombiningReinfo rce-
ment Learning with Agent Based Modeling [25], the authors ad -
dress the self-organizing dynamics of social segregation a nd ex-
plore the space of possibilities that emerge from consideri ng dif-
ferent types of rewards. The second group of studies focuses on
socialinﬂuencefromotheragentsastheintrinsicfactorfo rthede-
cision making. Social inﬂuence has been known to bean intrin sic
factorinagents’ choices [23,24],and inrecent studies,th erehave
beenattemptstoproposeauniﬁedmechanismforcoordinatio nin
MARLbyrewardingagentsforhavingcausalinﬂuenceoveroth er
agents’actions[11,30].Adversarialattacksduringtrain ingcanpo-
tentiallyimpactthechoicesmadebyagentsandimpacttheco nsen-
susalgorithmsasstudiedin[8]andthisisimportanttosolv esince
attacksthroughsocialinﬂuenceareacommontoolforadvers aries
[22].
5 CONCLUSION
In this work, we discussed a simple strategy for agents to mit i-
gatethenuancesofadversarialagentsinfullydecentraliz edMARL
where the agents are connected via a time varying network. We
relied on the algorithm of [31] and modiﬁed the actor-critic al-
gorithm in the presence of one or more adversaries. We did not
provide a formal argument for the convergence of the consens us
updateswithouradversarial framework whichweleaveasone of
the immediate next steps of this work. A second direction whe re
this kind of work can be adapted is to use attention based acto r
critic algorithms [ ?] that modify the action value function by pa-
rameterizingitwithagent speciﬁcattentionweights.Weca nthen
use reward values to optimize for the attention weights over the
timevarying network.
REFERENCES
[1] MichaelBowlingandManuelaVeloso.2000. Ananalysisofstochasticgametheory
for multiagent reinforcement learning . Technical Report. Carnegie-MellonUniv
Pittsburgh PaSchool of ComputerScience.
[2] LucianBusoniu,RobertBabuska,and Bart DeSchutter.20 08. Acomprehensive
surveyofmultiagentreinforcementlearning. IEEETransactionsonSystems,Man,
and Cybernetics,Part C(Applicationsand Reviews) 38,2 (2008), 156–172.[3] LucasCassano,KunYuan,andAliHSayed.2020. Multiagen tfullydecentralized
value function learning with linear convergence rates. IEEE Trans. Automat.
Control66,4 (2020), 1497–1512.
[4] LElisa Celis,Peter MKraﬀt,and Nisheeth KVishnoi. 2017 . A distributed learn-
ing dynamics in social groups. In Proceedings of the ACM Symposium on Princi-
ples of Distributed Computing .441–450.
[5] Filippos Christianos, Lukas Schäfer, and Stefano Albre cht. 2020. Shared expe-
rience actor-critic for multi-agent reinforcement learni ng.Advances in neural
information processingsystems 33 (2020), 10707–10717.
[6] Eyal Even-Dar, Shie Mannor, Yishay Mansour, and Sridhar Mahadevan. 2006.
Action Elimination and Stopping Conditions for the Multi-A rmed Bandit and
Reinforcement Learning Problems. Journal of machine learning research 7, 6
(2006).
[7] JAlexanderFaxandRichardMMurray.2004. Informationﬂ owandcooperative
controlofvehicleformations. IEEEtransactionsonautomaticcontrol 49,9(2004),
1465–1476.
[8] Martin Figura, Yixuan Lin, Ji Liu, and Vijay Gupta. 2021. Resilient Consensus-
based Multi-agent Reinforcement Learning. arXiv preprint arXiv:2111.06776
(2021).
[9] Jakob Foerster, Gregory Farquhar, Triantafyllos Afour as, Nantas Nardelli, and
Shimon Whiteson. 2018. Counterfactual multi-agent policy gradients. In Pro-
ceedings ofthe AAAIconference onartiﬁcial intelligence , Vol. 32.
[10] Jakob Foerster, Nantas Nardelli, Gregory Farquhar, Tr iantafyllos Afouras,
Philip HS Torr, Pushmeet Kohli, and Shimon Whiteson. 2017. S tabilising ex-
perience replay for deep multi-agent reinforcement learni ng. InInternational
conference onmachine learning . PMLR,1146–1155.
[11] Natasha Jaques, Angeliki Lazaridou, Edward Hughes, Ca glar Gulcehre, Pedro
Ortega, DJ Strouse, Joel Z Leibo, and Nando De Freitas. 2019. Social inﬂuence
asintrinsic motivation formulti-agent deep reinforcemen tlearning. In Interna-
tional conference onmachine learning . PMLR,3040–3049.
[12] Jelle R Kok and Nikos Vlassis. 2006. Collaborative mult iagent reinforcement
learning by payoﬀ propagation. Journal of Machine Learning Research 7 (2006),
1789–1828.
[13] Guillaume J Laurent, Laëtitia Matignon, Le Fort-Piat, et al. 2011. The world
of independent learners is not Markovian. International Journal of Knowledge-
based and Intelligent EngineeringSystems 15,1 (2011), 55–64.
[14] Ryan Lowe, Yi I Wu, Aviv Tamar, Jean Harb, OpenAI Pieter A bbeel, and Igor
Mordatch.2017. Multi-agent actor-criticfor mixed cooper ative-competitiveen-
vironments. Advances inneural informationprocessingsystems 30 (2017).
[15] Xueguang Lyu, Yuchen Xiao, Brett Daley, and Christophe r Amato. 2021. Con-
trastingcentralizedanddecentralizedcriticsinmulti-a gentreinforcementlearn-
ing.arXiv preprintarXiv:2102.04402 (2021).
[16] Hangyu Mao,Zhengchao Zhang, Zhen Xiao, and ZhiboGong. 2018. Modelling
thedynamicjointpolicyofteammateswithattentionmulti- agentDDPG. arXiv
preprint arXiv:1811.07029 (2018).
[17] Igor Mordatch and Pieter Abbeel. 2018. Emergence of gro unded compositional
language in multi-agent populations. In Proceedings of the AAAI conference on
artiﬁcial intelligence , Vol. 32.
[18] Kazuaki Nakayama, Masato Hisakado, and Shintaro Mori. 2017. Nash equilib-
rium of social-learning agents in a restless multiarmed ban dit game. Scientiﬁc
reports7,1 (2017), 1937.
[19] Angelia Nedic and Asuman Ozdaglar. 2009. Distributed s ubgradient methods
for multi-agent optimization. IEEETrans.Automat.Control 54,1 (2009), 48–61.
[20] GuannanQu,YihengLin,AdamWierman,andNaLi.2020. Sc alablemulti-agent
reinforcement learning for networked systems with average reward.Advances
in Neural Information ProcessingSystems 33 (2020), 2074–2086.
[21] PaulBReverdy,VaibhavSrivastava,andNaomiEhrichLe onard.2014. Modeling
humandecisionmakingingeneralizedGaussianmultiarmedb andits.Proc.IEEE
102,4 (2014), 544–571.
[22] Soumajyoti Sarkar, Paulo Shakarian, Mika Armenta, Dan ielle Sanchez, and Ki-
ranLakkaraju.2019. Cansocial inﬂuence be exploited to com promisesecurity:
Anonlineexperimentalevaluation.In Proceedingsofthe2019IEEE/ACMInterna-
tionalConferenceonAdvancesinSocialNetworksAnalysisa ndMining .593–596.
[23] SoumajyotiSarkar,PauloShakarian,DanielleSanchez ,MikaArmenta,andKiran
Lakkaraju.2020. Use of a controlled experiment and computa tional models to
measurethe impact of sequential peer exposures on decision making. Plos one
15,7 (2020), e0234875.
[24] Karl H Schlag. 1998. Why imitate, and if so, how?: A bound edly rational ap-
proachtomulti-armedbandits. Journalofeconomictheory 78,1(1998),130–156.
[25] EgemenSert,YaneerBar-Yam,andAlfredoJMorales.202 0. Segregationdynam-
icswith reinforcementlearning and agent basedmodeling. Scientiﬁc reports 10,
1(2020), 11771.
[26] Kyunghwan Son, Daewoo Kim, Wan Ju Kang, David Earl Hosta llero, and Yung
Yi.2019. Qtran:Learningtofactorizewithtransformation forcooperativemulti-
agent reinforcement learning. In International conference on machine learning .
PMLR, 5887–5896.
[27] ShreyasSundaramandBahmanGharesifard.2015. Consen sus-baseddistributed
optimization with malicious nodes. In 2015 53rd Annual Allerton Conference onCommunication,Control,and Computing(Allerton) . IEEE, 244–249.
[28] Daniël Willemsen, Mario Coppola, and Guido CHE de Croon . 2021. MAMBPO:
Sample-eﬃcient multi-robot reinforcement learning using learned world mod-
els. In2021 IEEE/RSJ International Conference on Intelligent Rob ots and Systems
(IROS). IEEE, 5635–5640.
[29] LinXiao, Stephen Boyd, and SanjayLall. 2005. A scheme f orrobustdistributed
sensor fusion based on average consensus. In IPSN 2005. Fourth InternationalSymposium on InformationProcessingin SensorNetworks,200 5.IEEE, 63–70.
[30] Annie Xie, Dylan Losey, Ryan Tolsma, Chelsea Finn, and D orsa Sadigh. 2021.
Learning latent representationsto inﬂuence multi-agent i nteraction. In Confer-
ence on robotlearning . PMLR,575–588.
[31] Kaiqing Zhang, Zhuoran Yang, Han Liu, Tong Zhang, and Ta mer Basar. 2018.
Fully decentralized multi-agent reinforcementlearning w ith networked agents.
InInternational Conference on MachineLearning . PMLR,5872–5881.