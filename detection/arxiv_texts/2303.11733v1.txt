DIPPM: a Deep Learning Inference Performance
Predictive Model using Graph Neural Networks
Karthick Panner Selvam and Mats Brorsson
SnT, University of Luxembourg, Luxembourg
fkarthick.pannerselvam,mats.brorsson g@uni.lu
Abstract. Deep Learning (DL) has developed to become a corner-stone
in many everyday applications that we are now relying on. However,
making sure that the DL model uses the underlying hardware eciently
takes a lot of eort. Knowledge about inference characteristics can help to
nd the right match so that enough resources are given to the model, but
not too much. We have developed a DL Inference Performance Predictive
Model (DIPPM) that predicts the inference latency ,energy , and memory
usage of a given input DL model on the NVIDIA A100 GPU. We also
devised an algorithm to suggest the appropriate A100 Multi-Instance
GPU prole from the output of DIPPM. We developed a methodology
to convert DL models expressed in multiple frameworks to a generalized
graph structure that is used in DIPPM. It means DIPPM can parse input
DL models from various frameworks. Our DIPPM can be used not only
helps to nd suitable hardware congurations but also helps to perform
rapid design-space exploration for the inference performance of a model.
We constructed a graph multi-regression dataset consisting of 10,508
dierent DL models to train and evaluate the performance of DIPPM,
and reached a resulting Mean Absolute Percentage Error (MAPE) as low
as 1.9%.
Keywords: Performance Prediction Â·Multi Instance GPU Â·Deep Learn-
ing Inference
1 Introduction
Many important tasks a now relying on Deep learning models, for instance in
computer vision and natural language processing domains [3,13]. In recent years,
researchers have focused on improving the eciency of deep learning models to
reduce the computation cost, energy consumption and increase the throughput of
them without losing their accuracy. At the same time, hardware manufacturers
like NVIDIA increase their computing power. For example, the NVIDIA A1001
GPU half-precision Tensor Core can perform matrix operations at 312 TFLOPS.
But all deep learning models will not fully utilize the GPU because the workload
and number of matrix operations will vary according to the problem domain.
1https://www.nvidia.com/en-us/data-center/a100/arXiv:2303.11733v1  [cs.PF]  21 Mar 20232 K. Panner Selvam et al.
DIPPMInput DNN Predicted
Latency (ms)
Ener gy (J)Memory (MB)
MIG Profile
Fig. 1: DIPPM can predict the Latency, Energy, Memory requirement, and MIG
Prole for inference on an NVIDIA A100 GPU without actually running on it.
For this reason, NVIDIA created the Multi-Instance GPU (MIG2) technology
starting from the Ampere architecture; they split the single physical GPU into
multi-isolated GPU instances, so multiple applications can simultaneously run
on dierent partitions of the same GPU, which then can be used more eciently.
However, determining the DL model's eciency on a GPU is not straight-
forward. If we could predict parameters such as inference latency, energy con-
sumption, and memory usage, we would not need to measure them on deployed
models which is a tedious and costly process. The predicted parameters could
then also support ecient Neural Architecture Search (NAS) [5], ecient DL
model design during development, and avoid job scheduling failures in data cen-
ters. According to Gao et al. [6], most failed deep learning jobs in data centers
are due to out-of-memory errors.
In order to meet this need, we have developed a novel Deep Learning Infer-
ence Performance Predictive Model (DIPPM) to support DL model developers
in matching their models to the underlying hardware for inference. As shown
in gure 1, DIPPM takes a deep learning model expressed in any of the frame-
works: PyTorch, PaddlePaddle, Tensorow, or ONNX, and will predict the la-
tency (ms), energy (J), memory requirement (MB), and MIG prole for inference
on an Nvidia A100 GPU without running on it. At the moment, the model is
restricted to inference and the Nvidia A100 architecture, but we aim to relax
these restrictions in future work. As far as we are aware, this is the rst predic-
tive model that can take input from any of the mentioned frameworks and to
predict all of the metrics above.
Our contributions include the following:
{We have developed, trained and evaluated a performance predictive model
which predicts inference latency, energy, memory, and MIG prole for A100
GPU with high accuracy.
{We have developed a methodology to convert deep learning models from
various deep learning frameworks into generalized graph structures for graph
learning tasks in our performance predictive model.
2https://docs.nvidia.com/datacenter/tesla/mig-user-guide/DIPPM: a Deep Learning Inference Performance Predictive Model 3
{We have devised an algorithm to suggest the MIG prole from predicted
Memory for the given input DL model.
{We have created an open-sourced performance predictive model dataset con-
taining 10,508 graphs for graph-level multi-regression problems.
Next, we discuss our work in relation to previous work in this area before
presenting our methodology, experiments, and results.
2 Related Work
Performance prediction of deep learning models on modern architecture is a
rather new research eld being attended to only since a couple of years back.
Bouhali et al. [2] and Lu et al. [14] have carried out similar studies where a
classical Multi-Layer Perceptron (MLP) is used to predict the inference latency
of a given input DL model. Their approach was to collect high-level DL model
features such as batch size, number of layers, and the total number of oating
point operations (FLOPS) needed. They then fed these features into an MLP
regressor as input to predict the latency of the given model. Bai et al. [1] used
the same MLP method but predicted both the latency and memory. However,
the classical MLP approach did not work very well due to the inability to capture
a detailed view of the given input DL model.
To solve the above problems, some researchers came up with a kernel addi-
tive method; they predict each kernel operation, such as convolution, dense, and
LSTM, individually and sum up all kernel values to predict the overall perfor-
mance of the DL model [8,15,18,20,22,24]. Yu et al. [23] used the wave-scaling
technique to predict the inference latency of the DL model on GPU, but this
technique requires access to a GPU in order to make the prediction.
Kaufman et al. and Dudziak et al. [9,4] used graph learning instead of MLP to
predict each kernel value. Still, they used the kernel additive method for inference
latency prediction. However, this kernel additive method did not capture the
overall network topology of the model, and instead it will aect the accuracy of
the prediction. To solve the above problem, Liu et al. [12] used a Graph level
task to generalize the entire DL model into node embeddings and predicted the
inference latency of the given DL model. However, they did not predict other
parameters, such as memory usage and energy consumption.
Li et al. [11] tried to predict the MIG proles on A100 GPU for the DL
models. However, their methodology is not straightforward; they used CUDA
Multi-Process Service (MPS) values to predict the MIG, So the model must run
at least on the target hardware once to predict the MIG Prole.
Most of the previous research work concentrated on parsing the input DL
model from only one of the following frameworks (PyTorch, TensorFlow, Pad-
dlePaddle, ONNX). As far as we are aware, none of the previous performance
prediction models predicted Memory usage, Latency, Energy, and MIG prole
simultaneously.
Our novel Deep Learning Inference Performance Predictive Model (DIPPM)
lls a gap in previous work; a detailed comparison is shown in Table 1. DIPPM4 K. Panner Selvam et al.
Table 1: Related Work comparison
Related Works A100 MIG GNNaMulti-SFbLatency Power Memory
Ours ( DIPPM )
Bai et al. [1] - - - - -
Bouhali et al. [2] - - - - - -
Dudziak et al. [4] - - - - -
Justus et al. [8] - - - - - -
Kaufman et al. [9] - - - - -
Li et al. [11] - - - - -
Liu et al. [12] - - - - -
Lu et al. [14] - - - -
Qi et al. [15] - - - - - -
Sponner et al. [18] - - -
Wang et al. [20] - - - - - -
Yang et al. [22] - - - - - -
Yu et. al. [23] - - - - -
Zhang et al. [24] - - - - - -
aUsing Graph Neural Network for performance prediction
bAble to parse DL model expressed in Multiple DL Software Framework
takes a deep learning model as input from various deep learning frameworks such
as PyTorch, PaddlePaddle, TensorFlow, or ONNX and converts it to generalize
graph with node features. We used a graph neural network and MIG predictor to
predict the inference latency (ms), energy (J), memory (MB), and MIG prole
for A100 GPU without actually running on it.
3 Methodology
The architecture of DIPPM consists of ve main components: Relay Parser,
Node Feature Generator, Static Feature Generator, Performance Model Graph
Network Structure (PMGNS), and MIG Predictor, as shown in Fig. 2. We will
explain each component individually in this section.
3.1 Relay Parser
The Relay Parser takes as input a DL model expressed in one of several sup-
ported DL frameworks, converts it to an Intermediate Representation (IR), and
passes this IR into the Node Feature Generator and the Static Feature Generator
components.
Most of the previously proposed performance models are able to parse the
given input DL model from a single DL framework, not from several, as we
already discussed in Section 2. To enable the use of multiple frameworks, we
used a relay, which is a high-level IR for DL models [16]. It has been used toDIPPM: a Deep Learning Inference Performance Predictive Model 5
+
+3X
Graph SAGEGraph SAGEGraph SAGE
Fully ConnectedRelu
ReluReluReluDr opoutDr opoutDr opout
Dr opoutPredictedMBmsJAdjacency Matrix Node Feature MatrixNode EmbeddingsV ector ConcatenateMemory (MB)Memory MIG Pr ofileInput DL
ModelInfer ence LatencyEnergy
Latency (ms)Ener gy (J)Static FeaturesTVM Relay 
ParserNode Featur e
GeneratorPMGNSPerformance Model Graph Network Structur e (PMGNS)MIG 
Pr edictorStatic Featur e 
GeneratorDIPPM
[1, 512][1, 5]
Fig. 2: Overview of DIPPM Architecture
compile DL models for inference in the TVM3framework. We are inspired by the
way they convert the DL model from various DL frameworks into a high-level IR
format and therefore used their technique in our DIPPM architecture. It allows
parsing given input DL models from various frameworks, but we have chosen
to limit ourselves to PyTorch, TensorFlow, ONNX, and PaddlePaddle. We pass
this DL IR to the subsequent components in our DIPPM architecture.
3.2 Node Feature Generator
The Node Feature Generator (NFG) converts the DL IR into an Adjacency
Matrix (A) and a Node feature matrix ( X) and passes this data to the PMGNS
component.
The NFG takes the IR from the relay parser component. The IR is itself
a computational data ow graph containing more information than needed for
our performance prediction. Therefore we lter and pre-process the graph by
post-order graph traversal to collect necessary node information. The nodes in
the IR contain useful features such as operator name, attributes, and output
shape of the operator, which after this rst ltering step are converted into a
suitable data format for our performance prediction. In the subsequent step, we
loop through the nodes and, for each operator node, generate node features with
a xed length of 32.
3https://tvm.apache.org/6 K. Panner Selvam et al.
Algorithm 1 Algorithm to convert DL model IR into a graph with node features
CreateGraph takes input IR and lters it by post-order traversal. Collect node features
for each node and generate a new graph Gwith node features, nally extract node
feature matrixXand adjacency matrix AfromG.
1:function CreatGraph (IR) .IR from Relay Parser Component
2:N filter andpreprocess (IR)
3:G ; .Create empty directed graph
4: for each node2Ndo .where node is node in node listN
5: ifnode:op2[operators] then .Check node is a operator
6:Foh onehotencoder (node:op )
7:Fattr ExtractAttributes (node)
8:Fshape ExtractOutshape (node)
9:Fnode F ohF attrF shape
10:G:add node(node:parent; node:id; Fnode).Nodes are added in sequence
11: end if
12: end for
13:A GetAdjacencyMatrix (G)
14:X GetNodeFeatureMatrix (G)
15: returnA;X
16:end function
The central part of the NFG is to generate an Adjacency Matrix (A) and
aNode feature matrix (X) as expressed in algorithm 1. Xhas the shape
of [Nop;Nfeatures ], whereNopis the number of operator nodes in the IR and
Nfeatures is the number of features. In order to create node features Fnfor each
node, rst, we need to encode the node operator name into a one hot encoding
as can be seen on line 6 in algorithm 1. Then extract the node attributes Fattr
and output shape Fshape into vectors. Finally, perform vector concatenation to
generateFnfor a node. We repeat this operation for each node and create the
G. From theG, we extractA,Xthat are passed to the main part of our model,
the Performance Model Graph Network Structure.
3.3 Static Feature Generator
The Static Feature Generator (SFG) takes the IR from the relay parser compo-
nent and generates static features Fsfor a given DL model and passes them into
the graph network structure.
For this experiment, we limited ourselves to ve static features. First, we
calculate theFmactotal multiply-accumulate (MACs) of the given DL model.
We used the TVM relay analysis API to calculate total MACs, but it is limited
to calculating MACs for the following operators (in TVM notation): Conv2D,
Conv2D transpose, dense, and batch matmul. Then we calculate the total num-
ber of convolutions FTconv , DenseFTdense , and Relu FTrelu operators from the
IR. We included batch size Fbatch as one of the static features because it gives
the ability to predict values for various batch sizes of a given model. Finally,DIPPM: a Deep Learning Inference Performance Predictive Model 7
we concatenate all the features into a vector Fsas expressed in equation 1. The
feature setFsis subsequently passed to the following graph network structure.
Fs F macF batchF TconvF TdenseF Trelu (1)
3.4 Performance Model Graph Network Structure (PMGNS)
The PMGNS takes the node feature matrix ( X), the adjacency matrix ( A) from
the Node Feature Generator component, and the feature set ( Fs) from the Static
feature generator and predicts the given input DL model's memory, latency, and
energy, as shown in Fig. 2.
The PMGNS must be trained before prediction, as explained in section 4.
The core idea of the PMGNS is to generate the node embedding zfromXand
Aand then to perform vector concatenation of zwithFs. Finally, we pass the
concatenated vector into a Fully Connected layer for prediction, as shown in
Fig. 2. In order to generate z, we used the graphSAGE algorithm suggested by
Hamilton et al. [7], because of its inductive node embedding, which means it can
generate embedding for unseen nodes without pretraining.
We already discussed that we generate node features of each node in the
section 3.2. The graphSAGE algorithm will convert node features into a node
embedding zwhich is more amenable for model training. The PMGNS contains
three sequential graphSAGE blocks and three sequential Fully connected (FC)
blocks as shown in Fig. 2. At the end of the nal graphSAGE block, we get the
generalized node embedding of given XandA, which we concatenate with Fs.
Then we pass the concatenated vector into FC to predict the memory (MB),
latency (ms), and energy (J).
3.5 MIG Predictor
The MIG predictor takes the memory prediction from PMGNS and predicts the
appropriate MIG prole for a given DL model, as shown in Fig. 2.
As mentioned in the introduction, the Multi-instance GPU (MIG) technology
allows to split an A100 GPU into multiple instances so that multiple applications
can use the GPU simultaneously. The dierent instances dier in their compute
capability and, most importantly, in the maximum memory limit that is allowed
to be used. The four MIG proles of the A100 GPU that we consider here
are: 1g.5gb, 2g.10gb, 3g.20gb, and 7g.40gb, where the number in front of "gb"
denotes the maximum amount of memory in GB that the application can use on
that instance. For example, the maximum memory limit of 1g.5gb is 5GB, and
7g.40gb is 40GB.
For a given input DL model, PMGNS predicts memory for 7g.40gb MIG
prole, which is the full GPU. We found that this prediction can be used as a
pessimistic value to guide the choice of MIG prole. Fig. 3 shows manual memory
consumption measurements of the same DL model inference on dierent proles.
The results show no signicant dierence in the memory allocation of DL in the8 K. Panner Selvam et al.
VGG16 Swin Base Densenet121
DL Model Architectures3000320034003600380040004200440046004800Memory Consumption (MB)1g5GB 2g10GB 3g20GB 7g40GB
Fig. 3: MIG Prole comparison of three dierent DL models memory consump-
tion on A100 GPU. We used batch size 16 for VGG16 and Densenet121 model
and batch size 8 for Swin base model.
dierent MIG proles even though the consumption slightly increases with the
capacity of the MIG prole. The memory consumption is always the highest
when running on the 7g.40gb MIG prole.
As mentioned, PMGNS predicts memory for 7g.40gb, so we claim that pre-
dicted memory will be an upper bound. Then we perform a rule-based prediction
to predict the MIG prole for the given input DL model, as shown in equation
2. Whereis predicted memory from PMGNS.
MIG() =8
>>>><
>>>>:1g.5gb;if 0gb<< 5gb
2g.10gb;if 5gb<< 10gb
3g.20gb;if 10gb<< 20gb
7g.40gb;if 20gb<< 40gb
None;otherwise(2)
4 Experiments & Results
4.1 The DIPPM Dataset
We constructed a graph-level multi-regression dataset containing 10,508 DL
models from dierent model families to train and evaluate our DIPPM. The
dataset distribution is shown in Table 2. Why do we need to create our own
dataset? To the best of our knowledge, the previous predictive performance
model dataset doesn't capture memory consumption, inference latency, and en-
ergy consumption parameters for wide-range DL models on A100 GPU.
Our dataset consists of DL models represented in graph structure, as gener-
ated by the Relay parser described in section 3.1. Each data point consists ofDIPPM: a Deep Learning Inference Performance Predictive Model 9
Table 2: DIPPM Graph dataset distribution
Model Family # of Graphs Percentage (%)
Ecientnet 1729 16:45
Mnasnet 1001 9:53
Mobilenet 1591 15:14
Resnet 1152 10:96
Vgg 1536 14:62
Swin 547 5:21
Vit 520 4:95
Densenet 768 7:31
Visformer 768 7:31
Poolformer 896 8:53
Total 10 508 100%
four variables:X,A,Y, andFs, whereXandAare the Node feature matrix
and Adjacency Matrix, respectively, as discussed in section 3.2, and Fsis the
static features of the DL model as discussed in section 3.3. We used the Nvidia
Management Library4and the CUDA toolkit5to measure the energy, memory,
and inference latency of each given model in the dataset. For each model, we
ran the inference ve times to warm up the architecture and then the inference
30 times, and then took the arithmetic mean of those 30 values to derive the Y,
whereYconsists of inference latency (ms), memory usage (MB), and energy (J)
for a given DL on A100 GPU.
We used a full A100 40GB GPU, or it is equivalent to using 7g.40gb MIG
prole to collect all the metrics.
4.2 Enviroment setup
We used an HPC cluster at the J ulich research centre in Germany called JUWELS
Booster for our experiments6. It is equipped with 936 nodes, each with AMD
EPYC 7402 processors, 2 sockets per node, 24 cores per socket, 512 GB DDR4-
3200 RAM and 4 NVIDIA A100 Tensor Core GPUs with 40 GB HBM.
The main software packages used in the experiments are: Python 3.10, CUDA
11.7 torch 1.13.1, torch-geometric 2.2.0, torch-scatter 2.1.0, and torch-sparse
0.6.16.
4.3 Evaluation
The Performance Model Graph Network Structure is the main component in
DIPPM, and we used the PyTorch geometric library to create our model, as
4https://developer.nvidia.com/nvidia-management-library-nvml
5https://developer.nvidia.com/cuda-toolkit
6https://apps.fz-juelich.de/jsc/hps/juwels/booster-overview.html10 K. Panner Selvam et al.
Table 3: Settings in GNN comparison.
Setting Value
Dataset partition Train (70%) / Validation (15%) / Test (15%)
Nr hidden layers 512
Dropout probability 0.05
Optimizer Adam
Learning rate 2:75410 5
Loss function Huber
Table 4: Comparison with dierent GNN algorithms and MLP with graphSAGE,
we trained all the models for 10 epochs and used Mean Average Percentage
Error for validation. The results indicate that DIPPM with graphSAGE performs
signicantly better than other variants.
Model Training Validation Test
GAT 0.497 0.379 0.367
GCN 0.212 0.178 0.175
GIN 0.488 0.394 0.382
MLP 0.371 0.387 0.366
(Ours) GraphSAGE 0.182 0.159 0.160
shown in Fig. 2. We split our constructed dataset into three parts randomly:
training set 70%, validation set 15%, and a test set 15%.
In order to validate that graphSAGE performs better than other GNN al-
gorithms and plain MLP, we compared graphSAGE with the following other
algorithms:, GAT [19], GCN [10], GIN [21], and nally, plain MLP without
GNN. Table 3 summarizes the settings used. The learning rate was determined
using a learning rate nder as suggested by Smith [17]. The Huber loss function
achieved a higher accuracy than mean square error, which is why we chose that
one.
For the initial experiment, we trained for 10 epochs and used Mean Average
Percentage Error (MAPE) as accuracy metric to validate DIPPM. A MAPE
value close to zero indicates good performance on regression prediction. Table 4
shows that graphSAGE gives a lower MAPE value in all of training, validation,
and test datasets. Without using a GNN, MLP gives 0.366 of MAPE. With
graphSAGE, MAPE is 0.160 on the test dataset which is a signicant improve-
ment on a multi-regression problem. We conclude that graphSAGE outperforms
other GNN algorithms, and MLP because of its inductive learning, as discussed
in section 3.4.
After this encouraging result we increased the number of epochs for training
our DIPPM with graphSAGE to increase the prediction accuracy. After 500
epochs, we attained MAPE of 0.041 on training and 0.023 on the validation
dataset. In the end, we attained 1.9% MAPE on the test dataset. Some of the
DIPPM predictions on the test dataset are shown in Fig. 4.DIPPM: a Deep Learning Inference Performance Predictive Model 11
VGG11 VGG16 Mnasnet Poolformer Vit Visformer
DL Model Architectures020406080100120140160Inference Latency (ms)Predicted Actual
(a) Inference latency (ms).
VGG11 VGG16 Mnasnet Poolformer Vit Visformer
DL Model Architectures01000020000300004000050000Energy (J)Predicted Actual (b) Energy (J).
VGG11 VGG16 Mnasnet Poolformer Vit Visformer
DL Model Architectures02000400060008000100001200014000Memory Consumption (MB)Predicted Actual
(c) Memory consumption (MB).
Fig. 4: Comparison of actual value with DIPPM predicted values on the test
dataset. Results show that DIPPM predictions are close to the actual predictions.
4.4 Prediction of MIG Proles
In order to verify the MIG prole prediction for a given DL model, we compared
the actual MIG prole value with the predicted MIG prole from the DIPPM, as
shown in table 5. To calculate the actual suitable MIG prole, we divide actual
memory consumption by the maximum memory limit of the MIG proles. The
higher the value is, the more appropriate prole for the given DL model.
For example, the predicted memory consumption for densenet121 at batch
size 8 is 2865 MB. The actual memory consumption for the 7g.40gb MIG prole
is 3272 MB. You can see that our DIPPM correctly predicted the MIG prole
1g.5gb for densenet121.
It is interesting to note that the densent121 models are from our test dataset
and the swin base patch4 model is not in our DIPPM dataset but a similar swin
base model family was used to train DIPPM. The convnext models are com-
pletely unseen to our DIPPM, but it's still predicting the MIG prole correctly.
4.5 DIPPM Usability aspects
DIPPM takes basic parameters like frameworks, model path, batch, and input
size, and nally, device type. As of now, we only considered A100 GPU; we
are working to extend DIPPM to various hardware platforms. With a simple
python API call, DIPPM predicts memory, latency, energy, and MIG prole for
the given model, as can be seen in Fig. 5.12 K. Panner Selvam et al.
Table 5: DIPPM MIG prole prediction for seen and unseen DL model architec-
tures. (densenet*: seen, swin*: partially seen, convnext*: unseen).
Predicted ActualModelBatch
size MIG Mem Mem 1g.5gb 2g.10gb 3g.20gb 7g.40gb
densenet121 8 1g.5gb 2865 3272 58% 30% 15% 8%
densenet121 32 2g.10gb 5952 6294 60% 30% 16%
swin base patch4 2 1g.5gb 2873 2944 52% 27% 14% 7%
swin base patch4 16 2g.10gb 6736 6156 59% 30% 15%
convnext base 4 1g.5gb 4771 1652 61% 31% 16% 8%
convnext base 128 7g.40gb 26439 30996 77%
Fig. 5: A sample code to use DIPPM for performance prediction of VGG16 DL
model developed by PyTorch framework.
5 Conclusion
We have developed a novel Deep Learning (DL) Inference Performance Pre-
dictive Model (DIPPM) to predict the inference latency, energy, and memory
consumption of a given input DL model on an A100 GPU without running on
it. Furthermore, We devised an algorithm to select the appropriate MIG prole
from the memory consumption predicted by DIPPM.
The model includes a methodology to convert the DL model represented
in various frameworks to a generalized graph structure for performance predic-
tion. To the best of our knowledge, DIPPM can help to develop an ecient DL
model to utilize the underlying GPU eectively. Furthermore, we constructed
and open-sourced7a multi-regression graph dataset containing 10,508 DL mod-
els for performance prediction. It can even be used to evaluate other graph-
based multi-regression GNN algorithms. Finally, we achieved 1.89% MAPE on
our dataset.
7The URL to the dataset will be provided with the camera-ready version of the paper.DIPPM: a Deep Learning Inference Performance Predictive Model 13
Acknowledgment
This work has been funded by EuroHPC JU under contract number 955513
and by the Luxembourg National Research Fund (FNR) under contract number
15092355.
References
1. Bai, L., Ji, W., Li, Q., et al.: Dnnabacus: Toward accurate computational cost
prediction for deep neural networks. CoRR abs/2205.12095 (2022)
2. Bouhali, N., Ouarnoughi, H., Niar, S., et al.: Execution time modeling for cnn in-
ference on embedded gpus. In: Proceedings of the 2021 Drone Systems Engineering
and Rapid Simulation and Performance Evaluation: Methods and Tools Proceed-
ings. p. 59{65. DroneSE and RAPIDO '21, Association for Computing Machinery,
New York, NY, USA (2021)
3. Brown, T.B., Mann, B., Ryder, N., et al.: Language models are few-shot learn-
ers. In: Proceedings of the 34th International Conference on Neural Information
Processing Systems. NIPS'20, Curran Associates Inc., Red Hook, NY, USA (2020)
4. Dudziak, L., Chau, T., Abdelfattah, M.S., et al.: Brp-nas: Prediction-based nas
using gcns. In: Proceedings of the 34th International Conference on Neural Infor-
mation Processing Systems. NIPS'20, Curran Associates Inc., Red Hook, NY, USA
(2020)
5. Elsken, T., Metzen, J.H., Hutter, F.: Neural architecture search: A survey. J. Mach.
Learn. Res. 20(1), 1997{2017 (mar 2021)
6. Gao, Y., Liu, Y., Zhang, H., et al.: Estimating GPU memory consumption of deep
learning models. In: Proceedings of the 28th ACM Joint Meeting on European
Software Engineering Conference and Symposium on the Foundations of Software
Engineering. pp. 1342{1352. ESEC/FSE 2020, Association for Computing Machin-
ery, New York, NY, USA (Nov 2020)
7. Hamilton, W.L., Ying, R., Leskovec, J.: Inductive representation learning on large
graphs. In: Proceedings of the 31st International Conference on Neural Information
Processing Systems. p. 1025{1035. NIPS'17, Curran Associates Inc., Red Hook,
NY, USA (2017)
8. Justus, D., Brennan, J., Bonner, S., et al.: Predicting the computational cost of
deep learning models. In: 2018 IEEE International Conference on Big Data (Big
Data). pp. 3873{3882. IEEE Computer Society, Los Alamitos, CA, USA (dec 2018)
9. Kaufman, S., Phothilimthana, P., Zhou, Y., et al.: A learned performance model
for tensor processing units. In: Smola, A., Dimakis, A., Stoica, I. (eds.) Proceedings
of Machine Learning and Systems. vol. 3, pp. 387{400 (2021)
10. Kipf, T.N., Welling, M.: Semi-supervised classication with graph convolutional
networks. In: International Conference on Learning Representations (ICLR) (2017)
11. Li, B., Patel, T., Samsi, S., et al.: Miso: Exploiting multi-instance gpu capability
on multi-tenant gpu clusters. In: Proceedings of the 13th Symposium on Cloud
Computing. p. 173{189. SoCC '22, Association for Computing Machinery, New
York, NY, USA (2022)
12. Liu, L., Shen, M., Gong, R., et al.: Nnlqp: A multi-platform neural network latency
query and prediction system with an evolving database. In: Proceedings of the
51st International Conference on Parallel Processing. ICPP '22, Association for
Computing Machinery, New York, NY, USA (2023)14 K. Panner Selvam et al.
13. Liu, Z., Lin, Y., Cao, Y., et al.: Swin transformer: Hierarchical vision transformer
using shifted windows. In: 2021 IEEE/CVF International Conference on Computer
Vision (ICCV). pp. 9992{10002. IEEE Computer Society, Los Alamitos, CA, USA
(oct 2021)
14. Lu, Z., Rallapalli, S., Chan, K., et al.: Augur: Modeling the resource requirements
of convnets on mobile devices. IEEE Transactions on Mobile Computing 20(2),
352{365 (2021)
15. Qi, H., Sparks, E.R., Talwalkar, A.: Paleo: A performance model for deep neu-
ral networks. In: 5th International Conference on Learning Representations, ICLR
2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenRe-
view.net (2017)
16. Roesch, J., Lyubomirsky, S., Weber, L., et al.: Relay: A new ir for machine learning
frameworks. In: Proceedings of the 2nd ACM SIGPLAN International Workshop
on Machine Learning and Programming Languages. p. 58{68. MAPL 2018, Asso-
ciation for Computing Machinery, New York, NY, USA (2018)
17. Smith, L.N.: Cyclical learning rates for training neural networks. In: 2017 IEEE
Winter Conference on Applications of Computer Vision (WACV). pp. 464{472
(2017)
18. Sponner, M., Waschneck, B., Kumar, A.: Ai-driven performance modeling for ai
inference workloads. Electronics 11(15) (2022)
19. Veli ckovi c, P., Cucurull, G., Casanova, A., et al.: Graph Attention Networks. In-
ternational Conference on Learning Representations (2018), accepted as poster
20. Wang, C.C., Liao, Y.C., Kao, M.C., et al.: Toward accurate platform-aware per-
formance modeling for deep neural networks. SIGAPP Appl. Comput. Rev. 21(1),
50{61 (jul 2021)
21. Xu, K., Hu, W., Leskovec, J., et al.: How powerful are graph neural networks? In:
International Conference on Learning Representations (2019)
22. Yang, C., Li, Z., Ruan, C., et al.: PerfEstimator: A Generic and Extensible Perfor-
mance Estimator for Data Parallel DNN Training. In: 2021 IEEE/ACM Interna-
tional Workshop on Cloud Intelligence (CloudIntelligence). pp. 13{18 (May 2021)
23. Yu, G.X., Gao, Y., Golikov, P., et al.: Habitat: A Runtime-Based Computational
Performance Predictor for Deep Neural Network Training. In: Proceedings of the
2021 USENIX Annual Technical Conference (USENIX ATC'21) (2021)
24. Zhang, L.L., Han, S., Wei, J., et al.: Nn-meter: Towards accurate latency predic-
tion of deep-learning model inference on diverse edge devices. In: Proceedings of
the 19th Annual International Conference on Mobile Systems, Applications, and
Services. p. 81{93. MobiSys '21, Association for Computing Machinery, New York,
NY, USA (2021)