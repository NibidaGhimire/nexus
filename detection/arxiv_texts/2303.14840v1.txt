On the Importance of Accurate Geometry Data for Dense 3D Vision Tasks
HyunJun Jung∗1, Patrick Ruhkamp∗1,2, Guangyao Zhai1, Nikolas Brasch1, Yitong Li1,
Yannick Verdie1,3, Jifei Song3, Yiren Zhou3, Anil Armagan3, Slobodan Ilic1,4,
Ales Leonardis3, Nassir Navab1, Benjamin Busam1,2
1Technical University of Munich,23Dwe.ai,3Huawei Noah’s Ark Lab,4Siemens AG,∗Equal Contribution
hyunjun.jung@tum.de, p.ruhkamp@tum.de, guangyao.zhai@tum.de, b.busam@tum.de
Abstract
Learning-based methods to solve dense 3D vision prob-
lems typically train on 3D sensor data. The respectively
used principle of measuring distances provides advantages
and drawbacks. These are typically not compared nor
discussed in the literature due to a lack of multi-modal
datasets. Texture-less regions are problematic for structure
from motion and stereo, reflective material poses issues for
active sensing, and distances for translucent objects are in-
tricate to measure with existing hardware. Training on in-
accurate or corrupt data induces model bias and hampers
generalisation capabilities. These effects remain unnoticed
if the sensor measurement is considered as ground truth
during the evaluation. This paper investigates the effect of
sensor errors for the dense 3D vision tasks of depth estima-
tion and reconstruction. We rigorously show the significant
impact of sensor characteristics on the learned predictions
and notice generalisation issues arising from various tech-
nologies in everyday household environments. For evalu-
ation, we introduce a carefully designed dataset1compris-
ing measurements from commodity sensors, namely D-ToF ,
I-ToF , passive/active stereo, and monocular RGB+P . Our
study quantifies the considerable sensor noise impact and
paves the way to improved dense vision estimates and tar-
geted data fusion.
1. Introduction
Our world is 3D. Distance measurements are essential
for machines to understand and interact with our environ-
ment spatially. Autonomous vehicles [23, 30, 50, 58] need
this information to drive safely, robot vision requires dis-
tance information to manipulate objects [15,62,72,73], and
AR realism benefits from spatial understanding [6, 31].
A variety of sensor modalities and depth predic-
1dataset available at https://github.com/Junggy/HAMMER-dataset
Figure 1. Other datasets for dense 3D vision tasks reconstruct the
scene as a whole in one pass [8,12,56], resulting in low quality and
accuracy (cf. red boxes). On the contrary, our dataset scans the
background and every object in the scene separately a priori and
annotates them as dense and high-quality 3D meshes. Together
with precise camera extrinsics from robotic forward-kinematics,
this enables a fully dense rendered depth as accurate pixel-wise
ground truth with multimodal sensor data, such as RGB with po-
larization, D-ToF, I-ToF and Active Stereo. Hence, it allows quan-
tifying different downstream 3D vision tasks such as monocular
depth estimation, novel view synthesis, or 6D object pose estima-
tion.
tion pipelines exist. The computer vision community
thereby benefits from a wide diversity of publicly available
datasets [23, 51, 52, 57, 60, 61, 65], which allow for evalua-
tion of depth estimation pipelines. Depending on the setup,
different sensors are chosen to provide ground truth (GT)
depth maps, all of which have their respective advantages
and drawbacks determined by their individual principle of
distance reasoning. Pipelines are usually trained on the data
without questioning the nature of the depth sensor used for
supervision and do not reflect areas of high or low confi-
dence of the GT.
Popular passive sensor setups include multi-view stereo
cameras where the known or calibrated spatial relationshiparXiv:2303.14840v1  [cs.CV]  26 Mar 2023between them is used for depth reasoning [51]. Correspond-
ing image parts or patches are photometrically or struc-
turally associated, and geometry allows to triangulate points
within an overlapping field of view. Such photometric cues
are not reliable in low-textured areas and with little ambient
light where active sensing can be beneficial [52,57]. Active
stereo can be used to artificially create texture cues in low-
textured areas and photon-pulses with a given sampling rate
are used in Time-of-Flight (ToF) setups either directly (D-
ToF) or indirectly (I-ToF) [26]. With the speed of light, one
can measure the distance of objects from the return time of
the light pulse, but unwanted multi-reflection artifacts also
arise. Reflective and translucent materials are measured at
incorrect far distances, and multiple light bounces distort
measurements in corners and edges. While ToF signals can
still be aggregated for dense depth maps, a similar setup is
used with LiDAR sensors which sparsely measure the dis-
tance using coordinated rays that bounce from objects in the
surrounding. The latter provides ground truth, for instance,
for the popular outdoor driving benchmark KITTI [23].
While LiDAR sensing can be costly, radar [21] provides
an even sparser but more affordable alternative. Multiple
modalities can also be fused to enhance distance estimates.
A common issue, however, is the inherent problem of warp-
ing onto a common reference frame which requires the in-
formation about depth itself [27, 37]. While multi-modal
setups have been used to enhance further monocular depth
estimation using self-supervision from stereo and temporal
cues [25, 60], its performance analysis is mainly limited to
average errors and restricted by the individual sensor used.
An unconstrained analysis of depth in terms of RMSE com-
pared against a GT sensor only shows part of the picture as
different sensing modalities may suffer from drawbacks.
Where are the drawbacks of current depth-sensing
modalities - and how does this impact pipelines trained
with this (potentially partly erroneous) data? Can self- or
semi-supervision overcome some of the limitations posed
currently? To objectively investigate these questions, we
provide multi modal sensor data as well as highly accurate
annotated depth so that one can analyse the deterioration
of popular monocular depth estimation and 3D reconstruc-
tion methods (see Fig. 1) on areas of different photometric
complexity and with varying structural and material prop-
erties while changing the sensor modality used for training.
To quantify the impact of sensor characteristics, we build a
unique camera rig comprising a set of the most popular in-
door depth sensors and acquire synchronised captures with
highly accurate ground truth data using 3D scanners and
aligned renderings. To this end, our main contributions can
be summarized as follows:
1. We question the measurement quality from commodity
depth sensor modalities and analyse their impact as
supervision signals for the dense 3D vision tasks ofdepth estimation and reconstruction.
2. We investigate performance on texture-varying mate-
rial as well as photometrically challenging reflec-
tive, translucent and transparent areas where learning
methods systematically reproduce sensor errors .
3. To objectively assess and quantify different data
sources, we contribute an indoor dataset compris-
ing an unprecedented combination of multi-modal
sensors , namely I-ToF, D-ToF, monocular RGB+P,
monochrome stereo, and active light stereo together
with highly accurate ground truth.
2. Related Work
2.1. Geometry from X
A variety of sensor modalities have been used to obtain
depth maps. Typical datasets comprise one ground truth
sensor used for all acquisitions, which is assumed to give
accurate enough data to validate the models:
Stereo Vision. In the stereo literature, early approaches [51]
use a pair of passive cameras and restrict scenes to piece-
wise planar objects for triangulation. Complex setups with
an industrial robot and structured light can yield ground
truth depth for stereo images [1]. Robots have also been
used to annotate keypoints on transparent household ob-
jects [36]. As these methods are incapable of retrieving
reliable depth in textureless areas where stereo matching
fails, active sensors are used to project patterns onto the
scenes to artificially create structures. The availability of
active stereo sensors makes it also possible to acquire real
indoor environments [52] where depth data at missing pix-
els is inpainted. Structure from motion (SfM) is used to
generate the depth maps of Sun3D [65] where a moving
camera acquires the scenes and data is fused ex post. A
temporally tracked handheld active sensor is further used
for depth mapping for SLAM evaluation in the pioneering
dataset of Sturm et al. [57]. While advancing the field, its
depth maps are limited to the active IR-pattern used by its
RGB-D sensor.
Time-of-Flight Sensors. Further advances in active depth
sensing emphasize ToF more. Initial investigations focus on
simulated data [26] and controlled environments with little
ambient noise [54]. The broader availability of ToF sensors
in commercial products (e.g. Microsoft Kinect series) and
modern smartphones (e.g. I-ToF of Huawei P30 Pro, D-
ToF in Apple iPhone 12) creates a line of research around
curing the most common sensor errors. These are multi-
path interference (MPI), motion artefacts and a high level
of sparsity and shot noise [27]. Aside of classical active and
passive stereo, we therefore also include D-ToF and I-ToF
modalities in all our experiments.Figure 2. Scanning Process Overview. To extract highly accurate geometry, we design a multi-stage acquisition process. At first, 3D
models are extracted with structured light 3D scanners (a). Scene objects (b) and mounted sensor rig (b) are calibrated towards a robot for
accurate camera pose retrieval [61]. A motion trajectory is recorded in gravity compensation mode (d) and repeated to record synchronized
images of all involved sensors (e). A partial digital twin of the 3D scene (f) is aligned to small (g) and larger (h) objects to retrieve an entire
in silico replica of the scene which can be rendered from the camera views of each sensor used (i) which results in highly accurate dense
depth maps that enable investigations of individual sensor components.
Polarimetric Cues. Other properties of light are used to in-
directly retrieve scene surface properties in the form of nor-
mals for which the amount of linearly polarized light and
its polarization direction provide information, especially for
highly reflective and transparent objects [17, 29]. Initial
investigations for shape from polarization mainly analyse
controlled setups [3,18,53,70]. More recent approaches in-
vestigate also sensor fusion methods [28] even in challeng-
ing scenes with strong ambient light [60]. We consequently
also acquire RGB+P data for all scenes.
Synthetic Renderings. In order to produce pixel-perfect
ground truth, some scholars render synthetic scenes [40].
While this produces the best possible depth maps, the
scenes are artificially created and lack realism, causing
pipelines trained on Sintel [7] or SceneFlow [40] to suffer
from a synthetic-to-real domain gap. In contrast, we fol-
low a hybrid approach and leverage pixel-perfect synthetic
data from modern 3D engines to adjust highly accurate 3D
models to real captures.
2.2. Monocular Depth Estimation
Depth estimation from a single image is inherently ill-
posed. Deep learning has enabled this task for real scenes.
Supervised Training. Networks can learn to predict depth
with supervised training. Eigen et al. [14] designed the
first monocular depth estimation network by learning to pre-
dict coarse depth maps, which are then refined by a second
network. Laina et al. [32] improved the latter model by
using only convolutional layers in a single CNN. The re-
quired ground truth often limits these methods to outdoor
scenarios [22]. A way of bypassing this is to use synthetic
data [39]. Narrowing down the resulting domain gap can
be realized [26]. MiDaS [47] generalizes better to unknownscenes by mixing data from 3D movies. To predict high-
resolution depth, most methods use multi-scale features or
post processing [41, 69] which complicates learning. If not
trained on a massive set of data, these methods show limited
generalization capabilities.
Self-Supervision. Self-supervised monocular methods try
to circumvent this issue. The first such methods [19, 66]
propose to use stereo images to train a network for depth
prediction. With it, the left image is warped into the right
where photometric consistency serves as training signal.
Monodepth [24] added a left-right consistency loss to mutu-
ally leverage warping from one image into the other. Even
though depth quality improves, it requires synchronized im-
age pairs. Monocular training methods are developed that
use only one camera where frames in a video are lever-
aged for the warping with simultaneously estimated poses
between them. This task is more intricate, however, Mon-
odepth2 [25] reduces the accuracy gap between the stereo
and monocular training by automasking and with a mini-
mum reprojection loss. A large body of work further im-
proves the task [10,33,46,47,55,68] and investigates tempo-
ral consistency [38,50,64]. To compare the effect of various
supervision signals for monocular depth estimation, we uti-
lized the ResNet backbone of the popular Monodepth2 [25]
together with its various training strategies.
2.3. Reconstruction and Novel View Synthesis
The 3D geometry of a scene can be reconstructed from
2D images and optionally their depth maps [43]. Scenes are
stored explicitly or implicitly. Typical explicit representa-
tion include point clouds or meshes [11] while popular im-
plicit representation are distance fields [71] which provide
the scene as a level set of a given function, or neural fieldsFigure 3. Data Quality. A full 3D reconstruction of the RGB scene (left) allows to render highly accurate depth maps from arbitrary
views. These serve as GT to study sensor errors of various depth sensors for different scene structures (right). E.g., due to the measurement
principle, the translucent glass becomes invisible for the ToF sensors.
where the scene is stored in the weights of a network [67].
NeRFs. Due to their photorealism in novel view synthesis,
recent advances around neural radiance fields ( NeRF ) [42]
experience severe attention. In this setup, one network is
trained on a posed set of images to represent a scene. The
method optimizes for the prediction of volume density and
view-dependent emitted radiance within a volume. Inte-
gration along query rays allows to synthesize novel views
of static and deformable [44] scenes. Most noticeable re-
cent advances extend the initial idea to unbounded scenes
of higher quality with Mip-NeRF 360 [5] or factor the rep-
resentation into low-rank components with TensoRF [9] for
faster and more efficient usage. Also robustness to pose
estimates and calibration are proposed [34, 63]. While
the initial training was computationally expensive, meth-
ods have been developed to improve inference and training.
With spherical harmonics spaced in a voxel grid structure,
Plenoxels [16] speed up processes even without a neural
network and interpolation techniques [59] accelerate train-
ing. Geometric priors such as sparse and dense depth maps
can regularize convergence, improve quality and training
time [13,49]. Besides recent works on methods themselves,
[48] propose to leverage real world objects from crowd-
sourced videos on a category level to construct a dataset
to evaluate novel view synthesis and category-centric 3D
reconstruction methods.
We make use of most recent NeRF advances and anal-
yse the impact of sensor-specific depth priors in [49] for the
task of implicit scene reconstruction. To neglect the influ-
ence of pose estimates and produce highly accurate data, we
leverage the robotic pose GT of our dataset.
3. Data Acquisition & Sensor Modalities
We set up scenes composed of multiple objects with
different shapes and materials to analyse sensor character-
istics. 3D models of photometrically challenging objects
with reflective or transparent surfaces are recorded with
high quality a priori and aligned to the scenes. Images are
captured from a synchronised multi-modal custom sensor
mounted at a robot end-effector to allow for precise pose
camera measurements [61]. High-quality rendered depth
can be extracted a posteriori from the fully annotated scenesfor the viewpoint of each sensor. The acquisition pipeline is
depicted in Fig. 2.
Previous 3D and depth acquisition setups [8,12,56] scan
the scene as a whole which limits the quality by the used
sensor. We instead separately scan every single object, in-
cluding chairs and background, as well as small household
objects a priori with two high-quality structured light object
scanners. This process significantly pushes the annotation
quality for the scenes as the robotic 3D labelling process
only has a point RMSE error of 0.80mm [61]. For compar-
ison, a Kinect Azure camera induces a standard deviation of
17mm in its working range [35]. The accuracy allows us
to investigate depth errors arising from sensor noise objec-
tively, as shown in Fig. 3, while resolving common issues
of imperfect meshes in available datasets (cf. Fig. 1, left).
3.1. Sensor Setup & Hardware Description
The table-top scanner (EinScan-SP, SHINING 3D Tech.
Co., Ltd., Hangzhou, China) uses a rotating table and is de-
signed for small objects. The other is a hand-held scan-
ner (Artec Eva, Artec 3D, Luxembourg) which we use for
larger objects and the background. For objects and areas
with challenging material, self-vanishing 3D scanning spray
(AESUB Blue) is used. For larger texture-less areas such as
tables and walls we temporarily attach small markers [20]
to the surface to allow for relocalization of the 3D scan-
ner. The robotic manipulator is a KUKA LBR iiwa 7 R800
(KUKA Roboter GmbH, Germany) with a position accu-
racy of ±0.1mm. We validated this during our pivot cal-
ibration stage (Fig. 2 b) by calculating the 3D location of
the tool tip (using forward kinematics and hand-tip cali-
bration) while varying robot poses. The position varied in
[−0.158,0.125] mm in line with this. Our dataset features
a unique multi-modal setup with four different cameras,
which provide four types of input images (RGB, polariza-
tion, stereo, Indirect ToF (I-ToF) correlation) and three dif-
ferent depth images modalities (Direct ToF (D-ToF), I-ToF,
Active Stereo). RGB and polarization images are acquired
with a Phoenix 5.0 MP Polarization camera (PHX050S1-
QC, LUCID Vision Labs, Canada) equipped with a Sony
Polarsens sensor (IMX264MYR CMOS, Sony, Japan). To
acquire stereo images, we use an Intel RealSense D435 (In-Figure 4. Camera Rig and 3D Sensor Data. The custom multi-
modal sensor rig comprises depth sensors for I-ToF (top left),
Stereo (lower left), D-ToF (lower right), and RGB-P (Polarization,
top right). It is fixed to a robot end-effector (top) and a Raspberry
Pi (right) triggers acquisition.
tel, USA) with switched off infrared projector. Depth is
acquired from an Intel RealSense L515 D-ToF sensor, an
Intel Realsense D435 active stereo sensor with infrared pat-
tern projection, and a Lucid Helios (HLS003S-001, LUCID
Vision Labs, Canada) I-ToF sensor. A Raspberry Pi trig-
gers each camera separately to remove interference effects
between infrared signals of depth sensors. The hardware is
rigidly mounted at the robot end-effector (see Fig. 4) which
allows to stop frame-by-frame for the synchronized acqui-
sition of a pre-recorded trajectory.
3.2. Scene Statistics & Data Comparison
We scanned 7 indoor areas, 6 tables, and 4 chairs, with
the handheld scanner as background and large objects. 64
household objects from 9 categories (bottle, can, cup, cut-
lery, glass, remote, teapot, tube, shoe) are scanned with the
tabletop structured light scanner. The data comprises 13
scenes split into 10 scenes for training and 3 scenes for test-
ing. Each scene is recorded with 2 trajectories of 200-300
frames with and without the objects. This sums up to 800-
1200 frames per scene, with a total of 10k frames for train-
ing and 3k frames for our test set. The 3 test scenes have
different background setups: 1) Seen background, 2) Seen
background with different lighting conditions and 3) Un-
seen background and table, with three different object se-
tups: 1) Seen objects 2) Unseen objects from the seen cat-
egory 3) Unseen objects from unseen categories (shoe and
tube). Table 1 compares our dataset with various existing
setups. To the best of our knowledge, our dataset is the only
multi-modal dataset comprising RGB, ToF, Stereo, Active
Stereo, and Polarisation modalities simultaneously with re-
liable ground truth depth maps.
4. Methodology
The dataset described above allows for the first time for
rigorous, in-depth analysis of different depth sensor modal-ities and a detailed quantitative evaluation of learning-based
dense scene regression methods when trained with vary-
ing supervision signals. We focus on the popular tasks of
monocular depth estimation and implicit 3D reconstruction
with the application of novel view synthesis.
4.1. Depth Estimation
To train the depth estimation from a single image, we
leverage the widely adopted architecture from [25]. We
train an encoder-decoder network with a ResNet18 encoder
and skip connections to regress dense depth. Using differ-
ent supervision signals from varying depth modalities al-
lows to study the influence and the characteristics of the
3D sensors. Additionally, we analyze whether complemen-
tary semi-supervision via information of the relative pose
between monocular acquisitions and consecutive image in-
formation of the moving camera can overcome sensor is-
sues.
We further investigate the network design influence on
the prediction quality for the supervised case. For this,
we train two high-capacity networks with transformer back-
bones on our data, namely DPT [46] and MIDAS [47].
Dense Supervision In the fully supervised setup, depth
modalities from the dataset are used to supervise the pre-
diction of the four pyramid level outputs after upsam-
pling to the original input resolution with: Lsupervised =
/summationtexti=4
i=1/vextenddouble/vextenddouble/vextenddouble/tildewideDi−D/vextenddouble/vextenddouble/vextenddouble
1, where Dis the supervision signal for
valid pixels of the depth map and /tildewideDithe predicted depth at
pyramid scale i.
Self-Supervision Depth and relative pose prediction be-
tween consecutive frames of a moving camera can be for-
mulated as coupled optimization problem. We follow es-
tablished methods to formulate a dense image reconstruc-
tion loss through projective geometric warping [25]. In this
process, a temporal image It′at time t′is projectively trans-
formed to the frame at time tvia:
It′→t=It′/angbracketleftig
proj(Dt, Tt→t′, K)/angbracketrightig
, where Dtis the pre-
dicted depth for frame t,Tt→t′the relative camera pose,
andKthe camera intrinsics. The photometric reconstruc-
tion error [25, 50, 64] between image IxandIy, given by:
Epe(Ix, Iy) = α1−SSIM (Ix,Iy)
2+ (1−α)∥Ix−Iy∥1is
computed between target frame Itand each source frame
Iswith s∈S. The pixel-wise minimum error is re-
trieved to finally define Lphoto overS= [t−F, t+F]as
Lphoto = min s∈SEpe(It, Is→t). The edge-aware smooth-
nessLsis applied [25] to encourage locally smooth depth
estimations with the mean-normalized inverse depth dtas
Ls=/vextendsingle/vextendsingle∂xdt/vextendsingle/vextendsinglee−|∂xIt|+/vextendsingle/vextendsingle∂ydt/vextendsingle/vextendsinglee−|∂yIt|. The final train-
ing loss for the self-supervised setup is: Lself-supervised =
Lphoto+λs· Ls.Table 1. Comparison of Datasets . Shown are differences between our dataset and previous multi-modal depth datasets for indoor
environments. Our dataset is the only one that provides highly accurate GT (Depth, Surface Normals, 6D Object Poses, Instance Masks,
Camera Poses, Dense Scene Mesh) together with varying sensor data for real scenes.
Dataset Acc.GT RGB D-ToF I-ToF Stereo Act.Stereo Polar. Indoor Real Video Frames
Agresti [2] - - - ✓ - - - ✓ ✓ - 113
CroMo [60] - - - ✓ ✓ ✓ ✓ (✓)✓ ✓ >10k
Zhu [75] - ( ✓) - - - - ✓ ✓ ✓ - 1
Sturm [57] - ✓ - - - - - ✓ ✓ ✓ >10k
[28]/ [45]/ [4] - ✓ - - - - ✓ ✓ ✓ - 1/40/300
Guo [26] ✓ - - ✓ - - - ✓ - - 2000
Ours ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ >10k
Semi-Supervision For the semi-supervised training, the
ground truth relative camera pose is leveraged. The pre-
dicted depth estimate is used to formulate the photometric
image reconstruction. We also enforce the smoothness loss
as detailed above.
Data Fusion Despite providing high accuracy ground
truth, our annotation pipeline is time-consuming. One may
ask whether this cannot be done with multi-view data aggre-
gation. We therefore compare the quality against the dense
structure from motion method Kinect Fusion [43] and an
approach for TSDF Fusion [74]. The synchronized sensor
availability allows also to investigate and improve sensor
fusion pipelines. To illustrate the impact of high quality GT
for this task, we also train the recent raw ToF+RGB fusion
network Wild-ToFu [27] on our dataset.
4.2. Implicit 3D Reconstruction
Recent work on implicit 3D scene reconstruction lever-
ages neural radiance fields (NeRF) [42]. The technique
works particularly well for novel view synthesis and allows
to render scene geometry or RGB views from unobserved
viewpoints. Providing additional depth supervision regu-
larizes the problem such that fewer views are required and
training efficiency is increased [13, 49]. We follow the mo-
tivation of [49] and leverage different depth modalities to
serve as additional depth supervision for novel view syn-
thesis. Following NeRF literature [42, 49], we encode the
radiance field for a scene in an MLP Fθto predict colour
C= [r, g, b ]and volume density σfor some 3D posi-
tionx∈R3and viewing direction d∈S2. We use
the positional encoding from [49]. For each pixel, a ray
r(t) =o+tdfrom the camera origin ois sampled through
the volume at location tk∈[tn, tf]between near and far
planes by querying Fθto obtain colour and density:
ˆC(r) =/summationtextK
k=1wkckwithwk=Tk(1−exp(−σkδk)),
Tk= exp/parenleftig
−/summationtextk
k′=1σk′δk′/parenrightig
andδk=tk+1−tk.
The NeRF depth ˆz(r)is computed by: ˆz(r) =/summationtextK
k=1wktk
and the depth regularization for an image with rays Ris:LD=/summationdisplay
r∈R|ˆz(r)−z(r)|
ˆz(r) +z(r), where z(r)is the depth of the
sensor. Using the mean squared error (MSE) loss Lcolour =
MSE (ˆC,C)for synthesized colours, the final training loss
is:LNeRF=Lcolour +λD· LD.
5. Sensor Impact for Dense 3D Vision Tasks
We train a series of networks for the task of monocular
depth estimation and implicit scene reconstruction.
5.1. Depth Estimation
Results for monocular depth estimation with varying
training signal are summarized in Table 2 and Fig. 5. We
report average results for the scenes and separate perfor-
mances for background, objects, and materials of different
photometric complexity. The error varies from background
to objects. Their varying photometric complexity can ex-
plain this. Not surprisingly, the ToF training is heavily in-
fluenced by reflective and transparent object material, where
the active stereo camera can project some patterns onto dif-
fusely reflective surfaces. Interestingly, the self- and semi-
supervised setups help to recover information in these chal-
lenging setups to some extent, such that these cases even
Table 2. Depth Prediction Results for Different Training Sig-
nals. Top: Dense supervision from different depth modali-
ties. Bottom: Evaluation of semi-supervised (pose GT) and self-
supervised (mono and mono+stereo) training. The entire scene
(Full), background (BG), and objects (Obj) are evaluated sepa-
rately. Objects material is further split into textured, reflective and
transparent. Best and 2nd best RMSE in mm are indicated.
Training Signal Full BG Obj Text. Refl. Transp.Sup.I-ToF 113.29 111.13 119.72 54.45 87.84 207.89
D-ToF 77.97 69.87 112.83 37.88 71.59 207.85
Active Stereo 72.20 71.94 61.13 50.90 52.43 87.24Sel/SemPose 154.87 158.67 65.42 57.22 37.78 61.86
M 180.34 183.65 85.51 84.26 48.80 49.62
M+S 159.80 161.65 82.16 71.24 63.92 66.48Figure 5. Fully Supervised Monocular Depth. Monocular depth
tends to overfit on the specific noise of the sensor the network
is trained on. Prediction from Active Stereo GT is robust on the
material while depth map is blurry, while both I-ToF and D-ToF
has strong material dependent artifact but sharp on the edges.
Figure 6. Dense SfM. A scene with our GT (left), Kinect [43]
(top) and TSDF [74] (bottom) fusion approaches. Inherent sensor
noise due to MPI (white), transparent objects (red), and diffuse
texture-less material (yellow) persists.
outperform the ToF supervision for photometrically chal-
lenging objects. In contrast, simpler structures (such as the
background) benefit from the ToF supervision. This indi-
cates that sensor-specific noise is learnt and reveals that sys-
tematic errors of learning approaches cannot be evaluated if
such 3D devices are used for ground truth evaluation with-
out critical analysis. This might ultimately lead to incor-
rect result interpretations, particularly if self-supervised ap-
proaches are evaluated against co-modality sensor data. The
table also discloses that the mutual prediction of inter-frame
poses in self-supervision indoor setups is challenging, and
accurate pose labels can have an immediate and significant
impact on the depth results (Pose vs. M).
Fig. 6 shows that multi-view data aggregation in the formof dense SfM fails to reproduce highly reliable 3D recon-
structions. In particular transparent and diffuse texture-
less objects pose challenges to both Active Stereo and D-
ToF. These can neither be recovered by the Kinect Fusion
pipeline [43] nor by the TSDF Fusion implementation of
Open3D [74] for which we use the GT camera poses. Inher-
ent sensor artefacts are present even if depth maps from dif-
ferent viewpoints are combined. This quality advantage jus-
tifies our expensive annotation setup. We further analysed
the results of training runs with DPT [46] and MIDAS [47],
which we train from scratch. While these more complex ar-
chitectures with higher capacity show the same trend and
also learn sensor noise, the training time is significantly
longer. More details are provided in the supplementary ma-
terial. From the previous results, we have seen that ToF
depth is problematic for translucent and reflective material.
Fig 7 illustrates that an additional co-modal input signal at
test time can cure these effects partly. It can be observed
that the use of additional RGB data in [27] reduces the in-
fluence of MPI and resolves some material-induced depth
artefacts. Our unique dataset also inspires cross-modal fu-
sion pipelines’ development and objective analysis.
5.2. Implicit 3D Reconstruction & View Synthesis
Our implicit 3D reconstruction generates novel views for
depth, normals and RGB with varying quality. If trained
with only colour information, the NeRF produces convinc-
ing RGB views with the highest PSNR (cf. Fig. 8 and Ta-
ble 3). However, the 3D scene geometry is not well recon-
structed. In line with the literature [13, 49], depth regular-
ization improves this (e.g. on texture-less regions). Reg-
ularising with different depth modalities makes the sensor
noise of I-ToF, AS, and D-ToF clearly visible. While the
RMSE behaves similarly to the monocular depth prediction
results with AS as best, followed by D-ToF and I-ToF. The
cosine similarity for surface normal estimates confirms this
trend. The overall depth and normal reconstruction for AS
are very noisy, but depth error metrics are more sensitive
for significant erroneous estimates for reflective and translu-
cent objects. Prior artefacts of the respective sensor influ-
ence the NeRF and translate into incorrect reconstructions
(e.g. errors from D-ToF and I-ToF for translucent material
or noisy background and inaccurate depth discontinuities at
edges for AS). Interestingly, the D-ToF prior can improve
Figure 7. Sensor Fusion. Scene (left) with I-ToF depth (cen-
tre) and ToF+RGB Fusion [27] (right). Fusion can help to resolve
some material induced artefacts (yellow) as well as MPI (blue).Figure 8. Reconstruction Results. The results of an implicit scene reconstruction with a Neural Radiance Field (NeRF) are shown. Images
are synthesised for depth, surface normals and RGB for an unseen view, which is shown together with the prediction errors. The columns
allow us to compare different methods where a NeRF [42] is trained solely on RGB (first column) and various depth maps for regularisation
as proposed in [49]. The last column illustrates synthesised results from training with GT depth for comparison. Differences are visible,
especially for the partly reflective table edges, the translucent bottle and around depth discontinuities.
the overall reconstruction for most of the scene but fails for
the bottle, where the AS can give better depth priors. This is
also visible in the synthesised depth. Leveraging synthetic
depth GT (last row) mitigates these issues and positively af-
fects the view synthesis with higher SSIM.
6. Discussion & Conclusion
This paper shows that questioning and investigating
commonly used 3D sensors helps to understand their im-
pact on dense 3D vision tasks. For the first time, we make it
possible to study how sensor characteristics influence learn-
ing in these areas objectively. We quantify the effect of
Table 3. Novel View Synthesis from Implicit 3D Reconstruc-
tion. Evaluation against GT for RGB, depth and surface nor-
mal estimates for different optimisation strategies (RGB-only for
supervision and +respective sensor depth). We indicate best,
2nd best and3rdbest. Depth metrics in mm.
RGB Depth Normal
Modality PSNR↑ SSIM↑ Abs.Rel. ↓ Sq.Rel.↓ RMSE↓σ < 1.25↑ Cos.Sim. ↓
RGB Only 32.406 0.889 0.328 111.229 226.187 0.631 0.084
+AS 17.570 0.656 0.113 16.050 94.520 0.853 0.071
+I-ToF 18.042 0.653 0.296 91.426 217.334 0.520 0.102
+D-ToF 31.812 0.888 0.112 24.988 119.455 0.882 0.031
+Syn. 32.082 0.894 0.001 0.049 3.520 1.000 0.001various photometric challenges, such as translucency and
reflectivity for depth estimation, reconstruction and novel
view synthesis and provide a unique dataset to stimulate re-
search in this direction. While obvious sensor noise is not
”surprising”, our dataset quantifies this impact for the first
time. For instance, interestingly, D-ToF supervision is sig-
nificantly better suited (13.02 mm) for textured objects than
AS, which in return surpasses I-ToF by 3.55 mm RMSE
(cf. 2). Same trend holds true on mostly texture-less back-
grounds where D-ToF is 37% more accurate than I-ToF. For
targeted analysis and research of dense methods for reflec-
tive and transparent objects, a quantitative evaluation is of
utmost interest - while our quantifiable error maps allow
specifying the detailed deviations. Although our dataset
tries to provide scenes with varying backgrounds, the pos-
sible location of the scene is restricted due to the limited
working range of the robot manipulator. Aside from our
investigations and the evaluation of sensor signals for stan-
dard 3D vision tasks, we firmly believe that our dataset can
also pave the way for further investigation of cross-modal
fusion pipelines.References
[1] Henrik Aanæs, Rasmus Jensen, George V ogiatzis, Engin
Tola, and Anders Dahl. Large-scale data for multiple-view
stereopsis. International Journal of Computer Vision , 120,
11 2016. 2
[2] Gianluca Agresti, Henrik Schaefer, Piergiorgio Sartor, and
Pietro Zanuttigh. Unsupervised domain adaptation for tof
data denoising with adversarial learning. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , June 2019. 6
[3] Gary A Atkinson and Edwin R Hancock. Recovery of sur-
face orientation from diffuse polarization. IEEE transactions
on image processing , 15(6):1653–1664, 2006. 3
[4] Yunhao Ba, Alex Gilbert, Franklin Wang, Jinfa Yang,
Rui Chen, Yiqin Wang, Lei Yan, Boxin Shi, and Achuta
Kadambi. Deep shape from polarization. In Computer
Vision–ECCV 2020: 16th European Conference, Glasgow,
UK, August 23–28, 2020, Proceedings, Part XXIV 16 , pages
554–571. Springer, 2020. 6
[5] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P
Srinivasan, and Peter Hedman. Mip-nerf 360: Unbounded
anti-aliased neural radiance fields. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 5470–5479, 2022. 4
[6] Benjamin Busam, Matthieu Hog, Steven McDonagh, and
Gregory Slabaugh. SteReFo: efficient image refocusing with
stereo vision. In Proceedings of the IEEE/CVF International
Conference on Computer Vision Workshops , 2019. 1
[7] D. J. Butler, J. Wulff, G. B. Stanley, and M. J. Black. A nat-
uralistic open source movie for optical flow evaluation. In
A. Fitzgibbon et al. (Eds.), editor, European Conf. on Com-
puter Vision (ECCV) , Part IV , LNCS 7577, pages 611–625.
Springer-Verlag, Oct. 2012. 3
[8] Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Hal-
ber, Matthias Niessner, Manolis Savva, Shuran Song, Andy
Zeng, and Yinda Zhang. Matterport3d: Learning from rgb-
d data in indoor environments. International Conference on
3D Vision (3DV) , 2017. 1, 4
[9] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and
Hao Su. Tensorf: Tensorial radiance fields. arXiv preprint
arXiv:2203.09517 , 2022. 4
[10] Po-Yi Chen, Alexander H Liu, Yen-Cheng Liu, and Yu-
Chiang Frank Wang. Towards scene understanding: Unsu-
pervised monocular depth estimation with semantic-aware
representation. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition , pages 2624–
2632, 2019. 3
[11] Brian Curless and Marc Levoy. A volumetric method for
building complex models from range images. In Proceedings
of the 23rd annual conference on Computer graphics and
interactive techniques , pages 303–312, 1996. 3
[12] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Hal-
ber, Thomas Funkhouser, and Matthias Nießner. Scannet:
Richly-annotated 3d reconstructions of indoor scenes. In
Proc. Computer Vision and Pattern Recognition (CVPR),
IEEE , 2017. 1, 4[13] Kangle Deng, Andrew Liu, Jun-Yan Zhu, and Deva Ra-
manan. Depth-supervised nerf: Fewer views and faster train-
ing for free. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 12882–
12891, 2022. 4, 6, 7
[14] David Eigen, Christian Puhrsch, and Rob Fergus. Depth map
prediction from a single image using a multi-scale deep net-
work. In Advances in neural information processing systems ,
pages 2366–2374, 2014. 3
[15] Hao-Shu Fang, Chenxi Wang, Minghao Gou, and Cewu Lu.
Graspnet-1billion: A large-scale benchmark for general ob-
ject grasping. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 11444–
11453, 2020. 1
[16] Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong
Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels:
Radiance fields without neural networks. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 5501–5510, 2022. 4
[17] Daoyi Gao, Yitong Li, Patrick Ruhkamp, Iuliia Skobleva,
Magdalena Wysocki, HyunJun Jung, Pengyuan Wang, Ar-
turo Guridi, and Benjamin Busam. Polarimetric pose pre-
diction. In European Conference on Computer Vision , pages
735–752. Springer, 2022. 3
[18] N Missael Garcia, Ignacio De Erausquin, Christopher Ed-
miston, and Viktor Gruev. Surface normal reconstruction us-
ing circularly polarized light. Optics express , 23(11):14391–
14406, 2015. 3
[19] Ravi Garg, Vijay Kumar Bg, Gustavo Carneiro, and Ian Reid.
Unsupervised cnn for single view depth estimation: Geom-
etry to the rescue. In European conference on computer vi-
sion, pages 740–756. Springer, 2016. 3
[20] Sergio Garrido-Jurado, Rafael Mu ˜noz-Salinas, Fran-
cisco Jos ´e Madrid-Cuevas, and Manuel Jes ´us Mar ´ın-
Jim´enez. Automatic generation and detection of highly
reliable fiducial markers under occlusion. Pattern Recogni-
tion, 47(6):2280–2292, 2014. 4
[21] Stefano Gasperini, Patrick Koch, Vinzenz Dallabetta, Nas-
sir Navab, Benjamin Busam, and Federico Tombari. R4dyn:
Exploring radar for self-supervised monocular depth estima-
tion of dynamic scenes. In 2021 International Conference
on 3D Vision (3DV) , pages 751–760. IEEE, 2021. 2
[22] A Geiger, P Lenz, C Stiller, and R Urtasun. Vision meets
robotics: The kitti dataset. The International Journal of
Robotics Research , 32(11):1231–1237, Aug 2013. 3
[23] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we
ready for autonomous driving? the kitti vision benchmark
suite. In 2012 IEEE conference on computer vision and pat-
tern recognition , pages 3354–3361. IEEE, 2012. 1, 2
[24] Clement Godard, Oisin Mac Aodha, and Gabriel J. Bros-
tow. Unsupervised monocular depth estimation with left-
right consistency. 2017 IEEE Conference on Computer Vi-
sion and Pattern Recognition (CVPR) , Jul 2017. 3
[25] Cl ´ement Godard, Oisin Mac Aodha, Michael Firman, and
Gabriel J. Brostow. Digging into self-supervised monocular
depth prediction. In The International Conference on Com-
puter Vision (ICCV) , 2019. 2, 3, 5[26] Qi Guo, Iuri Frosio, Orazio Gallo, Todd Zickler, and Jan
Kautz. Tackling 3d tof artifacts through learning and the flat
dataset. In The European Conference on Computer Vision
(ECCV) , September 2018. 2, 3, 6
[27] HyunJun Jung, Nikolas Brasch, Ale ˇs Leonardis, Nassir
Navab, and Benjamin Busam. Wild tofu: Improving range
and quality of indirect time-of-flight depth with rgb fusion in
challenging environments. In 2021 International Conference
on 3D Vision (3DV) , pages 239–248. IEEE, 2021. 2, 6, 7
[28] Achuta Kadambi, Vage Taamazyan, Boxin Shi, and Ramesh
Raskar. Depth sensing using geometrically constrained po-
larization normals. International Journal of Computer Vi-
sion, 125(1-3):34–51, 2017. 3, 6
[29] Agastya Kalra, Vage Taamazyan, Supreeth Krishna
Rao, Kartik Venkataraman, Ramesh Raskar, and Achuta
Kadambi. Deep polarization cues for transparent object
segmentation. In Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition , pages
8602–8611, 2020. 3
[30] Xin Kong, Xuemeng Yang, Guangyao Zhai, Xiangrui Zhao,
Xianfang Zeng, Mengmeng Wang, Yong Liu, Wanlong Li,
and Feng Wen. Semantic graph based place recognition for
3d point clouds. In 2020 IEEE/RSJ International Conference
on Intelligent Robots and Systems (IROS) , pages 8216–8223.
IEEE, 2020. 1
[31] Johannes Kopf, Xuejian Rong, and Jia-Bin Huang. Ro-
bust consistent video depth estimation. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 1611–1621, 2021. 1
[32] Iro Laina, Christian Rupprecht, Vasileios Belagiannis, Fed-
erico Tombari, and Nassir Navab. Deeper depth prediction
with fully convolutional residual networks. In 2016 Fourth
international conference on 3D vision (3DV) , pages 239–
248. IEEE, 2016. 3
[33] Sihaeng Lee, Janghyeon Lee, Byungju Kim, Eojindl Yi, and
Junmo Kim. Patch-wise attention network for monocular
depth estimation. In Proceedings of the AAAI Conference on
Artificial Intelligence , volume 35, pages 1873–1881, 2021. 3
[34] Chen-Hsuan Lin, Wei-Chiu Ma, Antonio Torralba, and Si-
mon Lucey. Barf: Bundle-adjusting neural radiance fields.
InProceedings of the IEEE/CVF International Conference
on Computer Vision , pages 5741–5751, 2021. 4
[35] Xingyu Liu, Shun Iwase, and Kris M Kitani. Stereobj-1m:
Large-scale stereo image dataset for 6d object pose estima-
tion. In Proceedings of the IEEE/CVF International Confer-
ence on Computer Vision , pages 10870–10879, 2021. 4
[36] Xingyu Liu, Rico Jonschkowski, Anelia Angelova, and
Kurt Konolige. Keypose: Multi-view 3d labeling and key-
point estimation for transparent objects. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 11602–11610, 2020. 2
[37] Adrian Lopez-Rodriguez, Benjamin Busam, and Krystian
Mikolajczyk. Project to adapt: Domain adaptation for depth
completion from noisy and sparse sensor data. In Proceed-
ings of the Asian Conference on Computer Vision , 2020. 2
[38] Xuan Luo, Jia-Bin Huang, Richard Szeliski, Kevin Matzen,
and Johannes Kopf. Consistent video depth estimation.ACM Transactions on Graphics (Proceedings of ACM SIG-
GRAPH) , 39(4):71–1, 2020. 3
[39] Nikolaus Mayer, Eddy Ilg, Philipp Fischer, Caner Hazir-
bas, Daniel Cremers, Alexey Dosovitskiy, and Thomas Brox.
What makes good synthetic training data for learning dispar-
ity and optical flow estimation? International Journal of
Computer Vision , 126(9):942–960, 2018. 3
[40] Nikolaus Mayer, Eddy Ilg, Philip Hausser, Philipp Fischer,
Daniel Cremers, Alexey Dosovitskiy, and Thomas Brox. A
large dataset to train convolutional networks for disparity,
optical flow, and scene flow estimation. In Proceedings of
the IEEE conference on computer vision and pattern recog-
nition , pages 4040–4048, 2016. 3
[41] S Mahdi H Miangoleh, Sebastian Dille, Long Mai, Sylvain
Paris, and Yagiz Aksoy. Boosting monocular depth estima-
tion models to high-resolution via content-adaptive multi-
resolution merging. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
9685–9694, 2021. 3
[42] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,
Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:
Representing scenes as neural radiance fields for view syn-
thesis. Communications of the ACM , 65(1):99–106, 2021. 4,
6, 8
[43] Richard A. Newcombe, Shahram Izadi, Otmar Hilliges,
David Molyneaux, David Kim, Andrew J. Davison, Push-
meet Kohi, Jamie Shotton, Steve Hodges, and Andrew
Fitzgibbon. Kinectfusion: Real-time dense surface mapping
and tracking. In 2011 10th IEEE International Symposium
on Mixed and Augmented Reality , pages 127–136, 2011. 3,
6, 7
[44] Keunhong Park, Utkarsh Sinha, Jonathan T Barron, Sofien
Bouaziz, Dan B Goldman, Steven M Seitz, and Ricardo
Martin-Brualla. Nerfies: Deformable neural radiance fields.
InProceedings of the IEEE/CVF International Conference
on Computer Vision , pages 5865–5874, 2021. 4
[45] Simeng Qiu, Qiang Fu, Congli Wang, and Wolfgang Hei-
drich. Polarization demosaicking for monochrome and
color polarization focal plane arrays. In Hans-J ¨org Schulz,
Matthias Teschner, and Michael Wimmer, editors, Vision,
Modeling and Visualization . The Eurographics Association,
2019. 6
[46] Ren ´e Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vi-
sion transformers for dense prediction. In Proceedings of
the IEEE/CVF International Conference on Computer Vi-
sion, pages 12179–12188, 2021. 3, 5, 7
[47] Ren ´e Ranftl, Katrin Lasinger, David Hafner, Konrad
Schindler, and Vladlen Koltun. Towards robust monocular
depth estimation: Mixing datasets for zero-shot cross-dataset
transfer. IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence , 44(3), 2022. 3, 5, 7
[48] Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler,
Luca Sbordone, Patrick Labatut, and David Novotny. Com-
mon objects in 3d: Large-scale learning and evaluation of
real-life 3d category reconstruction. In Proceedings of the
IEEE/CVF International Conference on Computer Vision
(ICCV) , pages 10901–10911, October 2021. 4[49] Barbara Roessle, Jonathan T Barron, Ben Mildenhall,
Pratul P Srinivasan, and Matthias Nießner. Dense depth pri-
ors for neural radiance fields from sparse input views. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 12892–12901, 2022. 4,
6, 7, 8
[50] Patrick Ruhkamp, Daoyi Gao, Hanzhi Chen, Nassir Navab,
and Benjamin Busam. Attention meets geometry: Geom-
etry guided spatial-temporal attention for consistent self-
supervised monocular depth estimation. In IEEE Interna-
tional Conference on 3D Vision (3DV) , December 2021. 1,
3, 5
[51] Daniel Scharstein and Richard Szeliski. A taxonomy and
evaluation of dense two-frame stereo correspondence algo-
rithms. International journal of computer vision , 47(1):7–
42, 2002. 1, 2
[52] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob
Fergus. Indoor segmentation and support inference from
rgbd images. In European conference on computer vision ,
pages 746–760. Springer, 2012. 1, 2
[53] William AP Smith, Ravi Ramamoorthi, and Silvia Tozza.
Height-from-polarisation with unknown lighting or albedo.
IEEE transactions on pattern analysis and machine intelli-
gence , 41(12):2875–2888, 2018. 3
[54] Kilho Son, Ming-Yu Liu, and Yuichi Taguchi. Learning to
remove multipath distortions in time-of-flight range images
for a robotic arm setup. In 2016 IEEE International Confer-
ence on Robotics and Automation (ICRA) , pages 3390–3397.
IEEE, 2016. 2
[55] Jaime Spencer, Richard Bowden, and Simon Hadfield.
Defeat-net: General monocular depth via simultaneous un-
supervised representation learning. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 14402–14413, 2020. 3
[56] Julian Straub, Thomas Whelan, Lingni Ma, Yufan Chen, Erik
Wijmans, Simon Green, Jakob J. Engel, Raul Mur-Artal,
Carl Ren, Shobhit Verma, Anton Clarkson, Mingfei Yan,
Brian Budge, Yajie Yan, Xiaqing Pan, June Yon, Yuyang
Zou, Kimberly Leon, Nigel Carter, Jesus Briales, Tyler
Gillingham, Elias Mueggler, Luis Pesqueira, Manolis Savva,
Dhruv Batra, Hauke M. Strasdat, Renzo De Nardi, Michael
Goesele, Steven Lovegrove, and Richard Newcombe. The
Replica dataset: A digital replica of indoor spaces. arXiv
preprint arXiv:1906.05797 , 2019. 1, 4
[57] J ¨urgen Sturm, Nikolas Engelhard, Felix Endres, Wolfram
Burgard, and Daniel Cremers. A benchmark for the evalua-
tion of rgb-d slam systems. In 2012 IEEE/RSJ international
conference on intelligent robots and systems , pages 573–580.
IEEE, 2012. 1, 2, 6
[58] Yongzhi Su, Yan Di, Guangyao Zhai, Fabian Manhardt, Ja-
son Rambach, Benjamin Busam, Didier Stricker, and Fed-
erico Tombari. Opa-3d: Occlusion-aware pixel-wise aggre-
gation for monocular 3d object detection. IEEE Robotics and
Automation Letters , 2023. 1
[59] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct voxel
grid optimization: Super-fast convergence for radiance fields
reconstruction. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition , pages 5459–
5469, 2022. 4
[60] Yannick Verdie, Jifei Song, Barnab ´e Mas, Busam Ben-
jamin, Ales Leonardis, , and Steven McDonagh. Cromo:
Cross-modal learning for monocular depth estimation. In
IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , 2022. 1, 2, 3, 6
[61] Pengyuan Wang, HyunJun Jung, Yitong Li, Siyuan Shen,
Rahul Parthasarathy Srikanth, Loranzo Garattoni, Sven
Meier, Nassir Navab, and Benjamin Busam. Phocal: A
multimodal dataset for category-level object pose estima-
tion with photometrically challenging objects. In IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR) , 2022. 1, 3, 4
[62] Pengyuan Wang, Fabian Manhardt, Luca Minciullo, Lorenzo
Garattoni, Sven Meier, Nassir Navab, and Benjamin Busam.
Demograsp: Few-shot learning for robotic grasping with hu-
man demonstration. In 2021 IEEE/RSJ International Confer-
ence on Intelligent Robots and Systems (IROS) , pages 5733–
5740. IEEE, 2021. 1
[63] Zirui Wang, Shangzhe Wu, Weidi Xie, Min Chen,
and Victor Adrian Prisacariu. Nerf–: Neural radiance
fields without known camera parameters. arXiv preprint
arXiv:2102.07064 , 2021. 4
[64] Jamie Watson, Oisin Mac Aodha, Victor Prisacariu, Gabriel
Brostow, and Michael Firman. The Temporal Opportunist:
Self-Supervised Multi-Frame Monocular Depth. In Com-
puter Vision and Pattern Recognition (CVPR) , 2021. 3, 5
[65] Jianxiong Xiao, Andrew Owens, and Antonio Torralba.
Sun3d: A database of big spaces reconstructed using sfm
and object labels. In Proceedings of the IEEE international
conference on computer vision , pages 1625–1632, 2013. 1,
2
[66] Junyuan Xie, Ross Girshick, and Ali Farhadi. Deep3d:
Fully automatic 2d-to-3d video conversion with deep convo-
lutional neural networks. In European Conference on Com-
puter Vision , pages 842–857. Springer, 2016. 3
[67] Yiheng Xie, Towaki Takikawa, Shunsuke Saito, Or Litany,
Shiqin Yan, Numair Khan, Federico Tombari, James Tomp-
kin, Vincent Sitzmann, and Srinath Sridhar. Neural fields in
visual computing and beyond. Computer Graphics Forum ,
2022. 4
[68] Zhenheng Yang, Peng Wang, Yang Wang, Wei Xu, and Ram
Nevatia. Lego: Learning edge with geometry all at once
by watching videos. In Proceedings of the IEEE conference
on computer vision and pattern recognition , pages 225–234,
2018. 3
[69] Wei Yin, Jianming Zhang, Oliver Wang, Simon Niklaus,
Long Mai, Simon Chen, and Chunhua Shen. Learning to
recover 3d scene shape from a single image. In Proc. IEEE
Conf. Comp. Vis. Patt. Recogn. (CVPR) , 2021. 3
[70] Ye Yu, Dizhong Zhu, and William AP Smith. Shape-from-
polarisation: a nonlinear least squares approach. In Proceed-
ings of the IEEE International Conference on Computer Vi-
sion Workshops , pages 2969–2976, 2017. 3
[71] Andy Zeng, Shuran Song, Matthias Nießner, Matthew
Fisher, Jianxiong Xiao, and Thomas Funkhouser. 3dmatch:Learning local geometric descriptors from rgb-d reconstruc-
tions. In CVPR , 2017. 3
[72] Guangyao Zhai, Dianye Huang, Shun-Cheng Wu, HyunJun
Jung, Yan Di, Fabian Manhardt, Federico Tombari, Nassir
Navab, and Benjamin Busam. Monograspnet: 6-dof grasping
with a single rgb image. In IEEE International Conference
on Robotics and Automation . IEEE, 2023. 1
[73] Guangyao Zhai, Yu Zheng, Ziwei Xu, Xin Kong, Yong Liu,
Benjamin Busam, Yi Ren, Nassir Navab, and Zhengyou
Zhang. Da2dataset: Toward dexterity-aware dual-arm grasp-
ing. IEEE Robotics and Automation Letters , 7(4):8941–
8948, 2022. 1
[74] Qian-Yi Zhou, Jaesik Park, and Vladlen Koltun. Open3D: A
modern library for 3D data processing. arXiv:1801.09847 ,
2018. 6, 7
[75] Dizhong Zhu and William AP Smith. Depth from a polarisa-
tion + rgb stereo pair. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
7586–7595, 2019. 6On the Importance of Accurate Geometry Data for Dense 3D Vision Tasks
–
Supplementary Material
HyunJun Jung∗1, Patrick Ruhkamp∗1,2, Guangyao Zhai1, Nikolas Brasch1, Yitong Li1,
Yannick Verdie1,3, Jifei Song3, Yiren Zhou3, Anil Armagan3, Slobodan Ilic1,4,
Ales Leonardis3, Nassir Navab1, Benjamin Busam1,2
1Technical University of Munich,23Dwe.ai,3Huawei Noah’s Ark Lab,4Siemens AG,∗Equal Contribution
hyunjun.jung@tum.de, p.ruhkamp@tum.de, guangyao.zhai@tum.de, b.busam@tum.de
1. Dense 3D Vision Tasks
1.1. Monocular Depth Estimation
Following the results on monocular depth estimation in
the main paper, we describe the implementation details of
the training, show additional results on different scenes and
provide additional metrics on different test scenes.
Implementation Details For all our depth estimation ex-
periments, we use PyTorch [ ?] and train for 20 epochs for
comparability using Adam [ ?]. Monocular approaches are
trained with a batch size of 12 on one NVIDIA RTX-3090
GPU. We chose λs= 10−3and sample SwithT= 10
frames offset due to small relative camera movement be-
tween frames and the high frame rate. The RGB inputs are
scaled to 480×320for supervised training and to 320×160
for self-supervised training, respectively. The depth net-
work regresses dense depth predictions on four pyramid
levels, each with half the resolution of the previous. Pose
network and augmentations follow [25]. We choose an ini-
tial learning rate of 1×10−4for 15 epochs, which we de-
crease to 1×10−5after 15 epochs in the self-supervised
setting. For the supervised case, we start with a learning
rate of 1×10−3, which we decrease every five epochs by a
factor of ten.
1.1.1 Quantitative evaluation
Test scenes. Table 1 summarizes the extensive quan-
titative evaluation of the supervised training with differ-
ent depth modalities as supervision signal for different test
scenes. Test scene 1 has a similar background compared to
the training scenes and includes additional unseen objects.
The scene is also observed from viewing angles that dif-
fer significantly from the training data. The background intest scene 2 is only partly observed in the training data and
it includes mostly unseen objects. Test scene 3 is similar
to test scene 2, but with a modified object layout and dif-
ficult lighting in the background from an additional bright
light source above the scene. The additional test set with
(partly) seen scenes is an additional test split which includes
the first 10 frames of each training sequence. Please note
that these frames have not been used during training. Here,
we first test all predictions against the rendered ground truth
(Top) and additionally on each individual respective modal-
ity (Bottom) to highlight the overfitting issue of invalid
ground truth from each modality. The results suggest that
overall the supervision with accurate rendered ground truth
achieves to generalize best for (mostly) unknown scenes.
It is noticeable, that the active stereo achieves to produce
good predictions for transparent objects and also performs
well for reflective ones. The I-ToF and D-ToF predictions
suffer from incorrect ground truth values for such objects.
Overfitting on (partly) seen scenes. The (partly) seen
scene shows generally lower overall errors for all modali-
ties as compared to the (mostly) unseen test scenes 1,2, and
3. Again, the active stereo can provide decent depth su-
pervision for reflective and transparent objects, where the
ToF sensors cannot provide valid depth. The prediction of
the background of the scene performs worst for the active
stereo, as the textureless wall is still problematic for the sen-
sor.
When testing on the respective modality itself, the over-
fitting issue due to incorrect depth values of the sensor be-
comes apparent. It can be noticed, that for objects where the
respective sensor cannot yield accurate depth values (e.g.
transparent objects for I-ToF or reflective objects for D-
ToF), the errors are significantly lower, indicating overfit-
ting to the specific sensor modality.
1arXiv:2303.14840v1  [cs.CV]  26 Mar 2023Table 1. Depth prediction comparison when training with different modalities and tested on different unseen scenes and seen
scenes. (Top) Evaluation against GT of depth predictions on the test set with dense supervision from different depth modalities. (Bottom)
Predictions evaluated on respective modality. Error is reported as Sq.Rel. and RMSE in mm.
Mask Full Scene Background All Objects Textured Reflective Transparent
Metric Sq.Rel. RMSE Sq.Rel. RMSE Sq.Rel. RMSE Sq.Rel. RMSE Sq.Rel. RMSE Sq.Rel. RMSE
Test 1 I-ToF 24.78 148.09 22.25 151.07 29.62 123.19 16.47 99.08 102.79 214.60 44.29 134.44
D-ToF 24.23 151.72 23.74 159.28 22.85 110.88 16.22 101.12 57.14 148.61 30.23 107.23
Active Stereo 32.15 173.72 33.84 184.16 22.23 116.57 19.55 114.07 64.27 167.71 12.92 69.49
Test 2 I-ToF 27.42 123.79 22.66 116.86 39.85 139.67 48.66 144.92 16.15 99.44 25.15 122.25
D-ToF 23.00 115.40 21.18 113.27 27.89 119.59 30.00 112.92 15.81 90.89 23.73 117.72
Active Stereo 25.94 124.17 25.50 126.28 27.18 117.04 32.81 121.24 16.40 101.86 15.73 95.27
Test 3 I-ToF 36.82 152.51 35.92 153.26 38.75 147.14 34.09 127.51 20.21 110.85 55.09 183.14
D-ToF 32.99 145.50 35.64 153.07 25.90 120.35 19.92 96.01 21.59 105.41 37.26 149.66
Active Stereo 31.63 141.77 35.24 151.37 22.44 110.42 23.47 106.63 14.49 94.51 21.21 109.53
T. SeenI-ToF 9.87 77.99 4.62 57.10 33.91 133.46 6.18 60.48 35.65 119.76 91.30 224.27
D-ToF 15.43 93.31 11.62 79.89 31.12 123.97 4.40 51.91 17.42 82.29 89.19 212.55
Active Stereo 9.43 88.30 9.28 88.24 9.11 75.21 6.32 65.54 12.98 65.73 16.62 98.75
Tested on Modality:Test SeenI-ToF 8.34 52.29 8.57 50.00 7.01 58.85 3.80 43.44 23.28 95.38 13.69 65.41
D-ToF 8.05 50.43 6.82 45.50 13.52 66.34 9.00 54.15 30.91 87.71 27.92 87.32
Active Stereo 39.25 101.76 40.87 102.29 30.32 90.00 32.24 90.49 23.36 72.21 37.25 101.23
GT 1.12 28.81 0.71 24.41 2.65 40.41 1.83 34.89 2.16 29.55 5.02 52.43
1.1.2 Qualitative predictions
Figures 1, 2 and 3 show predictions on exemplary frames
of the test scenes 1, 2 and 3, together with the different
sensor modalities and the error plot of the prediction com-
pared against the ground truth. The training with rendered
ground truth generally performs best. Both ToF sensors
show incorrect depth values for reflective or transparent ob-
jects which also translates to incorrect predictions in these
areas (compare Fig. 1. The predictions when training with
active stereo as supervision are more blurry and show less
distinct edges at depth boundaries when compared to other
modalities, which may arise from many depth pixels being
invalidated by the sensors around such boundaries (com-
pare Fig. 2). The very challenging test scene 3 with bright
lighting and many unseen objects is difficult to predict for
all training setups (compare Fig. 3. We can see similar ar-
tifacts as described above. Additionally, the unseen trophy
object with partly reflective and partly transparent material
shows large errors for the sensor inputs as well as for its
predictions. The desk surface is also incorrectly captured
by the D-ToF sensors due to large reflections and MPI from
the background.
1.2. Implicit Reconstruction
Implementation Details As mentioned in the main paper,
we follow NeRF [42] and build upon the work of [49] with-Table 2. Relative Pose Error of SLAM and SfM.
Error Direct (DSO) Dense dToF Dense iToF Dense AS SfM
rot [deg] 0.22 0.18 0.51 0.56 10.76
trans [cm] 0.27 0.31 0.68 0.62 2.86
out a depth completion network, but leverage the respective
sensor depth with a scale-invariant depth loss LD. We use
images with a resolution of 640×480and process batches of
1024 rays. We set λDto 0.1 and the learning rate to 0.0005
and optimize for 100k iterations with Adam optimizer [ ?].
1.3. Camera Pose Estimation
The analysis above focuses on dense monocular depth
estimation and novel view synthesis as recent and im-
portant approaches - for which pixelwise prediction and
evaluation are crucial. We add results for direct SLAM
(DSO) [ ?], KinectFusion [43] with different depth modal-
ities, and COLMAP SfM [ ?] in Fig. 4.
Tab. 2 summarizes the relative pose error for different
approaches (cf. Fig 4). Note the pose accuracy results for
KinectFusion [43] align with the depth results from Tab.2
in the main paper.Figure 1. Qualitative evaluation on test scene 1. Each depth modality, the network prediction when trained with supervision of each
modality, and the error, are shown as qualitative evaluation.Figure 2. Qualitative evaluation on test scene 2. Each depth modality, the network prediction when trained with supervision of each
modality, and the error, are shown as qualitative evaluation.Figure 3. Qualitative evaluation on test scene 3. Each depth modality, the network prediction when trained with supervision of each
modality, and the error, are shown as qualitative evaluation.Figure 4. Qualitative reconstruction results from SLAM and SfM.2. Dataset
2.1. Detailed Dataset Description
Sec. 3 of the main paper mentioned that our dataset uses
multiple images/depth sensors to collect the dataset with
highly accurate annotations of the scene using the robotic
arm in a synchronized manner. This section shows the de-
tailed description of data we include in our dataset.
2.1.1 Polarization Camera
Fig. 5 shows examples of images included for the polariza-
tion camera. As mentioned in Sec. 2 of the main paper, a
polarization camera provides images with different polar-
ization angles, which can extract cues like the surface nor-
mal by using the physical property of object material in the
scene. The polarization camera we used in our dataset (See
Sec. 3 in the main paper) provides polarized images at 4
different angles (0, 90, 180 270 degrees) which are saved
in a single 2x2 image (Fig. 5, (a)). A regular RGB im-
age is obtained by averaging the 4 images (Fig. 5, (b)). To
showcase the results of the depth map trained with different
depth cameras, we include warped depth images from each
depth camera into the polarization camera coordinates using
the extrinsic between the two cameras and its depth image
(Fig. 5, (d-g)). These can be additionally used for RGBD-
based depth completion research. On top of that, we include
extra information, such as instance map (Fig. 5, (c)) to help
train or validate pipelines for categorical level tasks, accu-
rate 6d pose of the camera as the 4x4 matrix obtained from
the robotic arm, extrinsic transformation between cameras
as 4x4 matrices and camera intrinsics as 3x3 matrix.
2.1.2 D-ToF Camera
Fig. 6 shows an example of images included for the D-ToF
camera. Direct ToF (D-ToF) camera senses the depth infor-
mation of its surrounding by emitting an infrared signal and
measuring the difference in time between the emitted and
received signal. The quality of this modality highly depends
on the reflection of the signal. It often suffers from spe-
cific physical noise such as Multi-Path-Interference (MPI)
or strong material dependent artefacts (Fig. 9). For the D-
ToF camera, we provide the depth map from the camera
(Fig. 6, (a)) as well as its rendered ground truth depth map
(Fig. 6, (b)) such that one can also research on D-ToF refine-
ment pipelines to reduce such errors. As in the polarization
camera, we include extra information such as instance label
map (Fig. 6, (c)), camera pose, intrinsic and extrinsics of
the camera as well.2.1.3 I-ToF Camera
Fig. 7 shows image examples for the I-ToF camera. Indi-
rect ToF (I-ToF) cameras sense the depth information of
their surrounding by emitting a frequency modulated sig-
nal and measuring the return signal. Unlike Direct ToF
(D-ToF), I-ToF cameras do not calculate the time differ-
ence to infer the depth. Instead, the camera correlates
the returning signal with phase-shifted emitting signals to
generate 4 different measurements, called correlation im-
ages. These are measured as sinus functions of distance
((sin(d),cos(d),−sin(d),−cos(d)) = ( c1, c2, c3, c4)in
Fig. 7, (a)). Either arc-tangent formula or convolutional
neural networks can be used to extract depth information
from the correlation images. As I-ToF modality also re-
lies on the reflection of the signal like in D-ToF, it suffers
from similar artefacts, such as MPI and material dependent
artefacts (compare qualitative results of the test scenes in
Figs. 1, 2 and 3). Here, we provide raw correlation im-
ages and depth map from the camera (see Fig. 7, (a,b)) as
well as its rendered ground truth depth (Fig. 7, (c)) such
that one can train I-ToF depth improvement pipelines either
from raw signal or from I-ToF depth itself. As the other
cameras, extras such as instance map (Fig. 7, (d)), camera
pose, intrinsic and extrinsics are included.
2.1.4 Active Stereo Camera
Fig. 8 shows the examples of images included for the Ac-
tive Stereo camera. Stereo depth estimation infers depth
using and photometric consistency and geometrical con-
straints from epipolar geometry and triangulates the depth
map from the disparity between left and right cameras. As
the disparity is calculated via matching on the image itself,
the stereo based depth estimation methods suffers less from
the specific material, but they suffer from other aspects such
as stereo occlusion and large texture-less regions. Active
projection (Active Stereo) is used to overcome this issue.
We provide both, active and passive stereo left / right im-
ages (Fig. 8, (a),(b)) and raw depth from the camera (active,
Fig. 8, (c)) as well as the rendered ground truth (Fig. 8, (d)).
This allows to use our dataset to improve stereo methods
from either passive or active stereo and also depth refine-
ment pipelines. Similar to the other cameras, extras such as
instance map (Fig. 8, (e)), camera pose, intrinsic and extrin-
sics are included.
2.2. Error Analysis on Different Modality
In this section, we show specific errors on each depth
modality to illustrate the implication of the depth quality
when the given modality is used as the ground truth, as
well as advantage of using our rendered depth as the ground
truth.Figure 5. Example of the images included for the polarization camera input (top) together with instance label map and depth estimates
warped onto the same coordinate reference frame.
Figure 6. Example of the images included for the D-ToF camera: its depth map (left), ground truth depth (centre) and an object instance
label map (right).
2.2.1 D-ToF Camera
As mentioned in Subsec. 2.1.2, D-ToF modality suffers
from its own reflection-based nature, such as MPI and mate-
rial dependent artefacts. When the angle of the surface nor-
mal of the scene is close to the incident angle of the infrared
signal, the strength of the reflected signal becomes weak
due to scattering effects (Fig. 9, (a) blue arrow) while multi-
ple scattered signals from the other surfaces which has more
traveling distance are received and with stronger strength
(Fig. 9, (a) red arrow) and interfere with the original signal
(MPI), producing a wrong measurement of the depth on the
area with further distance which looks like a reflection or
shadow of the object to the surface (Fig. 9, (b) red marker).
This effect can be intensified when the surface material is
reflective, which gives even stronger artefact as its reflec-
tive surface bounces even weaker and noisier signal withless attenuation (Fig. 9, (a,b) yellow arrow&marker). On
the other hands, when the surface material is transparent,
the emitted infrared signal rather goes through the object in
the both ways (Fig. 9, (a) green arrow) which at the end ig-
nores the object and the sensor produce the depth value as
similar level as its background (Fig. 9, (b) green marker -
material dependent artefact). Quality of the depth map de-
grades slightly around some boundaries after warping into
the RGB frame (Fig. 10, (b), red), while the invalid regions
actually helps to invalidate more area on wrong depth espe-
cially on the reflective objects (Fig. 10, (b), green) , which
might become beneficial when it is used in the training.
2.2.2 I-ToF Camera
As mentioned in Subsec. 2.1.3, I-ToF modality suffers by
its own reflection based nature as well similar to D-ToF,Figure 7. Example of the images included for the I-ToF camera.
Figure 8. Example of the images included for the Active Stereo camera.
such as MPI and material dependent artefact (Fig. 11. Al-
though the quality of depth itself seems better as the depth
itself is more dense (with less invalid region) and amount
of the artefacts are less, it is hard to say I-ToF modality is
better than D-ToF as these two camera are in different price
range and power level. Also less invalid area but rather with
wrong depth didn’t help invalidating depth (Fig. 12) not like
in D-ToF case, which could result in artefact in the predic-
tion when it is used as GT during the training.2.2.3 Active Stereo Camera
As the stereo camera uses left and right matching with pho-
toelectric cue, depth map suffers less on the challenging
material as the projection can be visible on the surface as
well as left-right check can be performed to invalidate re-
gion with the wrong depth. For this reason, depth on glass
or the reflective object is significantly more accurate com-
pared to either of ToF modality (Fig. 13, green arrow). OnFigure 9. Detailed ray paths with MPI and surface material induced error on D-ToF modality. While D-ToF produces dense and sharp
depth, its quality is highly dependent on the surface material and the incident angle.
Figure 10. Error after warping D-ToF into RGB view. Slight errors are introduced on some edges (red) while expansion of the invalid area
helps to invalidate on the reflective objects (green).
the other hands, due to its nature of pattern projection far
distance that depth quality gets worsen as the scene gets
further (Fig. 13, red arrow) the projection pattern gets at-
tenuated and spread in the far distance. Moreover, the depth
map in general is more blurry, jittery, sparse and has wrong
values on some regions without being invalidated (Fig. 13,
orange arrow) which can introduce negative influence when
it is used as GT, such as blurriness and depth jittering. Er-
ror introduced by warping is trivial (Fig. 14) as the original
depth map is already blurry and sparse.Figure 11. Depth quality from I-ToF camera. I-ToF modality suffers from same type of artefect as D-ToF. While depth map itself is more
sense and suffers less from MPI artefact on the table.
Figure 12. Error after warping I-ToF into RGB view. Not like D-ToF, most of depth error exists without being invalidated, which might
introduce more error when it used as GT during the training.Figure 13. Depth quality from Active Stereo camera. While depth map suffers less on the challenging material, quality of depth itself is far
behind either of ToF modality in multiple aspects, such as sharpness, variance, sparsity.
Figure 14. Error after warping Active Stereo into RGB view. Note that there isn’t significant change in the depth quality after the warping.2.3. Detailed Background and Objects Description
As described in Sec. 4 in the main paper, our dataset
comprises a total of 13 scenes divided into 10 scenes for
training and 3 testing scenes composed of a mixture of 4
different chairs, 6 different tables, 64 household objects
from 8 plus 4 different categories (i.e. cup, teapot, bot-
tle, remote, boxes, can, glass, cutlery and tube, shoe, plas-
tic kitchenware, trophy) and and 7 different indoor areas.
Test sets have 1 unseen background and 2 seen backgrounds
with and without different lighting and contain a mixture of
seen/unseen objects from seen/unseen categories. In this
section, we show detailed images of backgrounds, chairs,
tables, and other objects. Fig. 15 and 17 respectively show
images of 3 chairs and 6 tables used in the dataset and their
corresponding meshes. Fig. 18 and 19 show a collection
of household objects used in training and test set. Fig. 16
shows 9 backgrounds used in the dataset and their corre-
sponding meshes.
2.3.1 Detailed Scene Description
As described, our training set is composed of 10 scenes,
and the test set is composed of 3 scenes. For each scene, we
include 2 different trajectories. Each trajectory covers 2 se-
tups with and without objects (naked scene). This sums up
to 800-1200 frames per scene and a total of ca. 10k frames.
In this section, we show several sample images of the scenes
in Fig. 20, 21, and 22, 23. Each of them consists of an an-
notated mesh and RGB images with different types of ren-
dering, which show the diversity and quality of our dataset.
2.3.2 Partial Scanning of the Scene and Mesh Fitting
As mentioned in Sec. 3 in the main paper, we use partial
scanning and mesh fitting to annotate background, large
objects, and objects outside the robotic workspace. This
section shows images of partial scanning and the mesh fit-
ting from one of the scenes as an example. The green
box in Fig. 24, (a) shows annotated meshes of the objects
by the robotic arm. Once the objects are annotated, the
scene is partially scanned with multiple viewpoints to make
the scanning dense and cover multiple facets of the back-
ground. Note that the center of the scanning is not yet in
the robot base coordinates (Fig. 24, (a) blue box). Once
the partial scanning is done, the scanned mesh is then fit
onto the annotated objects, such that the partially scanned
mesh origin concides with the robot base (Fig. 24, (b)).
Once the scanned mesh is put to robot base coordinates,
we fit background, large objects, and distant objects meshes
also in robot base coordinates to annotate them (Fig. 25,
(a)). Fig. 25, (b-c) shows the result of the annotated mesh.
For the mesh fitting, we used Artec Studio 10 Professional(Artec 3D, Luxembourg) which runs a point correspon-
dence and ICP-based method to fit the meshes.Figure 15. Chairs used in the dataset. Chairs in group (a) are used for the training set and the chair in (b) is used for the test set.Figure 16. Backgrounds used in the dataset. Note that one of the background in the group (b) is also included in the training set, but we
varied the lighting condition to provide different various factors for evaluation.Figure 17. Tables used in the dataset. Tables in group (a) are used for the training set and the table in (b) is used for the test set. Note that,
unlike small objects or chairs, we decide not to scan some parts of the large tables (e.g. end of their legs) as the cameras cannot see the part
in their trajectories.
Figure 18. Collection of small household objects used in the training set. Objects from 8 household categories are used in the training set,
3 of which have photometrically challenging surface material - partially reflective (can), transparent (glass/plastic), reflective (cutlery).Figure 19. Collection of small household objects used in the test set. The test set comprises a mixture of seen (left column) and unseen
(mid column) objects from 8 seen categories and a few objects from unseen categories (right column - tube, slipper, plastic kitchenware,
trophy) are used.
Figure 20. Example images from Training Scene 1. The annotated mesh is shown on the left together with an RGB view from the
scene (second from left) with and without objects. The overlayed masks (second from right) and the rendered depth (right) illustrate the
annotation quality of our data.Figure 21. Example images from Training Scene 2-5. The annotated mesh for 4 different scenes is shown on the left together with an RGB
view from the scene (second from left) with and without objects. The overlayed masks (second from right) and the rendered depth (right)
illustrate the annotation quality of our data.Figure 22. Example images from Training Scene 6-9. The annotated mesh for four different scenes is shown on the left together with an
RGB view from the scene (second from left) with and without objects. The overlayed masks (second from right) and the rendered depth
(right) illustrate the annotation quality of our data.Figure 23. Example images from Training Scene 10 and Test scene 1-3. The annotated mesh is shown on the left together with an RGB
view from the scene (second from left) with and without objects. The overlayed masks (second from right) and the rendered depth (right)
illustrate the annotation quality of our data. Note that the test scene 2,3 are recorded in the exactly same pose and trajectory but with the
different lighting.Figure 24. Example of partial scanning of the scene before and after the fitting on scene 13. Note that the center of the partial scanned
mesh is aligned to robot base (xyz coordinate marker) after fitting it onto the mesh of the annotated objects.
Figure 25. Example of far objects and background fitting onto partially scanned mesh. Left: Background and objects are fit to partial scans.
Centre: All annotated meshes are shown without partial scans. Right: Corresponding scene from the camera viewpoint with augmented
object masks. Note that the annotation quality of meshes with partial scans and robot arm is similar. The annotated meshes via partial
scanning are marked with red arrows.References
[1] Henrik Aanæs, Rasmus Jensen, George V ogiatzis, Engin
Tola, and Anders Dahl. Large-scale data for multiple-view
stereopsis. International Journal of Computer Vision , 120,
11 2016.
[2] Gianluca Agresti, Henrik Schaefer, Piergiorgio Sartor, and
Pietro Zanuttigh. Unsupervised domain adaptation for tof
data denoising with adversarial learning. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , June 2019.
[3] Gary A Atkinson and Edwin R Hancock. Recovery of sur-
face orientation from diffuse polarization. IEEE transactions
on image processing , 15(6):1653–1664, 2006.
[4] Yunhao Ba, Alex Gilbert, Franklin Wang, Jinfa Yang,
Rui Chen, Yiqin Wang, Lei Yan, Boxin Shi, and Achuta
Kadambi. Deep shape from polarization. In Computer
Vision–ECCV 2020: 16th European Conference, Glasgow,
UK, August 23–28, 2020, Proceedings, Part XXIV 16 , pages
554–571. Springer, 2020.
[5] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P
Srinivasan, and Peter Hedman. Mip-nerf 360: Unbounded
anti-aliased neural radiance fields. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 5470–5479, 2022.
[6] Benjamin Busam, Matthieu Hog, Steven McDonagh, and
Gregory Slabaugh. SteReFo: efficient image refocusing with
stereo vision. In Proceedings of the IEEE/CVF International
Conference on Computer Vision Workshops , 2019.
[7] D. J. Butler, J. Wulff, G. B. Stanley, and M. J. Black. A nat-
uralistic open source movie for optical flow evaluation. In
A. Fitzgibbon et al. (Eds.), editor, European Conf. on Com-
puter Vision (ECCV) , Part IV , LNCS 7577, pages 611–625.
Springer-Verlag, Oct. 2012.
[8] Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Hal-
ber, Matthias Niessner, Manolis Savva, Shuran Song, Andy
Zeng, and Yinda Zhang. Matterport3d: Learning from rgb-
d data in indoor environments. International Conference on
3D Vision (3DV) , 2017.
[9] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and
Hao Su. Tensorf: Tensorial radiance fields. arXiv preprint
arXiv:2203.09517 , 2022.
[10] Po-Yi Chen, Alexander H Liu, Yen-Cheng Liu, and Yu-
Chiang Frank Wang. Towards scene understanding: Unsu-
pervised monocular depth estimation with semantic-aware
representation. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition , pages 2624–
2632, 2019.
[11] Brian Curless and Marc Levoy. A volumetric method for
building complex models from range images. In Proceedings
of the 23rd annual conference on Computer graphics and
interactive techniques , pages 303–312, 1996.
[12] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Hal-
ber, Thomas Funkhouser, and Matthias Nießner. Scannet:
Richly-annotated 3d reconstructions of indoor scenes. In
Proc. Computer Vision and Pattern Recognition (CVPR),
IEEE , 2017.[13] Kangle Deng, Andrew Liu, Jun-Yan Zhu, and Deva Ra-
manan. Depth-supervised nerf: Fewer views and faster train-
ing for free. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 12882–
12891, 2022.
[14] David Eigen, Christian Puhrsch, and Rob Fergus. Depth map
prediction from a single image using a multi-scale deep net-
work. In Advances in neural information processing systems ,
pages 2366–2374, 2014.
[15] Hao-Shu Fang, Chenxi Wang, Minghao Gou, and Cewu Lu.
Graspnet-1billion: A large-scale benchmark for general ob-
ject grasping. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 11444–
11453, 2020.
[16] Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong
Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels:
Radiance fields without neural networks. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 5501–5510, 2022.
[17] Daoyi Gao, Yitong Li, Patrick Ruhkamp, Iuliia Skobleva,
Magdalena Wysocki, HyunJun Jung, Pengyuan Wang, Ar-
turo Guridi, and Benjamin Busam. Polarimetric pose pre-
diction. In European Conference on Computer Vision , pages
735–752. Springer, 2022.
[18] N Missael Garcia, Ignacio De Erausquin, Christopher Ed-
miston, and Viktor Gruev. Surface normal reconstruction us-
ing circularly polarized light. Optics express , 23(11):14391–
14406, 2015.
[19] Ravi Garg, Vijay Kumar Bg, Gustavo Carneiro, and Ian Reid.
Unsupervised cnn for single view depth estimation: Geom-
etry to the rescue. In European conference on computer vi-
sion, pages 740–756. Springer, 2016.
[20] Sergio Garrido-Jurado, Rafael Mu ˜noz-Salinas, Fran-
cisco Jos ´e Madrid-Cuevas, and Manuel Jes ´us Mar ´ın-
Jim´enez. Automatic generation and detection of highly
reliable fiducial markers under occlusion. Pattern Recogni-
tion, 47(6):2280–2292, 2014.
[21] Stefano Gasperini, Patrick Koch, Vinzenz Dallabetta, Nas-
sir Navab, Benjamin Busam, and Federico Tombari. R4dyn:
Exploring radar for self-supervised monocular depth estima-
tion of dynamic scenes. In 2021 International Conference
on 3D Vision (3DV) , pages 751–760. IEEE, 2021.
[22] A Geiger, P Lenz, C Stiller, and R Urtasun. Vision meets
robotics: The kitti dataset. The International Journal of
Robotics Research , 32(11):1231–1237, Aug 2013.
[23] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we
ready for autonomous driving? the kitti vision benchmark
suite. In 2012 IEEE conference on computer vision and pat-
tern recognition , pages 3354–3361. IEEE, 2012.
[24] Clement Godard, Oisin Mac Aodha, and Gabriel J. Bros-
tow. Unsupervised monocular depth estimation with left-
right consistency. 2017 IEEE Conference on Computer Vi-
sion and Pattern Recognition (CVPR) , Jul 2017.
[25] Cl ´ement Godard, Oisin Mac Aodha, Michael Firman, and
Gabriel J. Brostow. Digging into self-supervised monocular
depth prediction. In The International Conference on Com-
puter Vision (ICCV) , 2019. 1[26] Qi Guo, Iuri Frosio, Orazio Gallo, Todd Zickler, and Jan
Kautz. Tackling 3d tof artifacts through learning and the flat
dataset. In The European Conference on Computer Vision
(ECCV) , September 2018.
[27] HyunJun Jung, Nikolas Brasch, Ale ˇs Leonardis, Nassir
Navab, and Benjamin Busam. Wild tofu: Improving range
and quality of indirect time-of-flight depth with rgb fusion in
challenging environments. In 2021 International Conference
on 3D Vision (3DV) , pages 239–248. IEEE, 2021.
[28] Achuta Kadambi, Vage Taamazyan, Boxin Shi, and Ramesh
Raskar. Depth sensing using geometrically constrained po-
larization normals. International Journal of Computer Vi-
sion, 125(1-3):34–51, 2017.
[29] Agastya Kalra, Vage Taamazyan, Supreeth Krishna
Rao, Kartik Venkataraman, Ramesh Raskar, and Achuta
Kadambi. Deep polarization cues for transparent object
segmentation. In Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition , pages
8602–8611, 2020.
[30] Xin Kong, Xuemeng Yang, Guangyao Zhai, Xiangrui Zhao,
Xianfang Zeng, Mengmeng Wang, Yong Liu, Wanlong Li,
and Feng Wen. Semantic graph based place recognition for
3d point clouds. In 2020 IEEE/RSJ International Conference
on Intelligent Robots and Systems (IROS) , pages 8216–8223.
IEEE, 2020.
[31] Johannes Kopf, Xuejian Rong, and Jia-Bin Huang. Ro-
bust consistent video depth estimation. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 1611–1621, 2021.
[32] Iro Laina, Christian Rupprecht, Vasileios Belagiannis, Fed-
erico Tombari, and Nassir Navab. Deeper depth prediction
with fully convolutional residual networks. In 2016 Fourth
international conference on 3D vision (3DV) , pages 239–
248. IEEE, 2016.
[33] Sihaeng Lee, Janghyeon Lee, Byungju Kim, Eojindl Yi, and
Junmo Kim. Patch-wise attention network for monocular
depth estimation. In Proceedings of the AAAI Conference on
Artificial Intelligence , volume 35, pages 1873–1881, 2021.
[34] Chen-Hsuan Lin, Wei-Chiu Ma, Antonio Torralba, and Si-
mon Lucey. Barf: Bundle-adjusting neural radiance fields.
InProceedings of the IEEE/CVF International Conference
on Computer Vision , pages 5741–5751, 2021.
[35] Xingyu Liu, Shun Iwase, and Kris M Kitani. Stereobj-1m:
Large-scale stereo image dataset for 6d object pose estima-
tion. In Proceedings of the IEEE/CVF International Confer-
ence on Computer Vision , pages 10870–10879, 2021.
[36] Xingyu Liu, Rico Jonschkowski, Anelia Angelova, and
Kurt Konolige. Keypose: Multi-view 3d labeling and key-
point estimation for transparent objects. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 11602–11610, 2020.
[37] Adrian Lopez-Rodriguez, Benjamin Busam, and Krystian
Mikolajczyk. Project to adapt: Domain adaptation for depth
completion from noisy and sparse sensor data. In Proceed-
ings of the Asian Conference on Computer Vision , 2020.
[38] Xuan Luo, Jia-Bin Huang, Richard Szeliski, Kevin Matzen,
and Johannes Kopf. Consistent video depth estimation.ACM Transactions on Graphics (Proceedings of ACM SIG-
GRAPH) , 39(4):71–1, 2020.
[39] Nikolaus Mayer, Eddy Ilg, Philipp Fischer, Caner Hazir-
bas, Daniel Cremers, Alexey Dosovitskiy, and Thomas Brox.
What makes good synthetic training data for learning dispar-
ity and optical flow estimation? International Journal of
Computer Vision , 126(9):942–960, 2018.
[40] Nikolaus Mayer, Eddy Ilg, Philip Hausser, Philipp Fischer,
Daniel Cremers, Alexey Dosovitskiy, and Thomas Brox. A
large dataset to train convolutional networks for disparity,
optical flow, and scene flow estimation. In Proceedings of
the IEEE conference on computer vision and pattern recog-
nition , pages 4040–4048, 2016.
[41] S Mahdi H Miangoleh, Sebastian Dille, Long Mai, Sylvain
Paris, and Yagiz Aksoy. Boosting monocular depth estima-
tion models to high-resolution via content-adaptive multi-
resolution merging. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
9685–9694, 2021.
[42] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,
Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:
Representing scenes as neural radiance fields for view syn-
thesis. Communications of the ACM , 65(1):99–106, 2021.
2
[43] Richard A. Newcombe, Shahram Izadi, Otmar Hilliges,
David Molyneaux, David Kim, Andrew J. Davison, Push-
meet Kohi, Jamie Shotton, Steve Hodges, and Andrew
Fitzgibbon. Kinectfusion: Real-time dense surface mapping
and tracking. In 2011 10th IEEE International Symposium
on Mixed and Augmented Reality , pages 127–136, 2011. 2
[44] Keunhong Park, Utkarsh Sinha, Jonathan T Barron, Sofien
Bouaziz, Dan B Goldman, Steven M Seitz, and Ricardo
Martin-Brualla. Nerfies: Deformable neural radiance fields.
InProceedings of the IEEE/CVF International Conference
on Computer Vision , pages 5865–5874, 2021.
[45] Simeng Qiu, Qiang Fu, Congli Wang, and Wolfgang Hei-
drich. Polarization demosaicking for monochrome and
color polarization focal plane arrays. In Hans-J ¨org Schulz,
Matthias Teschner, and Michael Wimmer, editors, Vision,
Modeling and Visualization . The Eurographics Association,
2019.
[46] Ren ´e Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vi-
sion transformers for dense prediction. In Proceedings of
the IEEE/CVF International Conference on Computer Vi-
sion, pages 12179–12188, 2021.
[47] Ren ´e Ranftl, Katrin Lasinger, David Hafner, Konrad
Schindler, and Vladlen Koltun. Towards robust monocular
depth estimation: Mixing datasets for zero-shot cross-dataset
transfer. IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence , 44(3), 2022.
[48] Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler,
Luca Sbordone, Patrick Labatut, and David Novotny. Com-
mon objects in 3d: Large-scale learning and evaluation of
real-life 3d category reconstruction. In Proceedings of the
IEEE/CVF International Conference on Computer Vision
(ICCV) , pages 10901–10911, October 2021.
[49] Barbara Roessle, Jonathan T Barron, Ben Mildenhall,
Pratul P Srinivasan, and Matthias Nießner. Dense depth pri-ors for neural radiance fields from sparse input views. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 12892–12901, 2022. 2
[50] Patrick Ruhkamp, Daoyi Gao, Hanzhi Chen, Nassir Navab,
and Benjamin Busam. Attention meets geometry: Geom-
etry guided spatial-temporal attention for consistent self-
supervised monocular depth estimation. In IEEE Interna-
tional Conference on 3D Vision (3DV) , December 2021.
[51] Daniel Scharstein and Richard Szeliski. A taxonomy and
evaluation of dense two-frame stereo correspondence algo-
rithms. International journal of computer vision , 47(1):7–
42, 2002.
[52] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob
Fergus. Indoor segmentation and support inference from
rgbd images. In European conference on computer vision ,
pages 746–760. Springer, 2012.
[53] William AP Smith, Ravi Ramamoorthi, and Silvia Tozza.
Height-from-polarisation with unknown lighting or albedo.
IEEE transactions on pattern analysis and machine intelli-
gence , 41(12):2875–2888, 2018.
[54] Kilho Son, Ming-Yu Liu, and Yuichi Taguchi. Learning to
remove multipath distortions in time-of-flight range images
for a robotic arm setup. In 2016 IEEE International Confer-
ence on Robotics and Automation (ICRA) , pages 3390–3397.
IEEE, 2016.
[55] Jaime Spencer, Richard Bowden, and Simon Hadfield.
Defeat-net: General monocular depth via simultaneous un-
supervised representation learning. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 14402–14413, 2020.
[56] Julian Straub, Thomas Whelan, Lingni Ma, Yufan Chen, Erik
Wijmans, Simon Green, Jakob J. Engel, Raul Mur-Artal,
Carl Ren, Shobhit Verma, Anton Clarkson, Mingfei Yan,
Brian Budge, Yajie Yan, Xiaqing Pan, June Yon, Yuyang
Zou, Kimberly Leon, Nigel Carter, Jesus Briales, Tyler
Gillingham, Elias Mueggler, Luis Pesqueira, Manolis Savva,
Dhruv Batra, Hauke M. Strasdat, Renzo De Nardi, Michael
Goesele, Steven Lovegrove, and Richard Newcombe. The
Replica dataset: A digital replica of indoor spaces. arXiv
preprint arXiv:1906.05797 , 2019.
[57] J ¨urgen Sturm, Nikolas Engelhard, Felix Endres, Wolfram
Burgard, and Daniel Cremers. A benchmark for the evalua-
tion of rgb-d slam systems. In 2012 IEEE/RSJ international
conference on intelligent robots and systems , pages 573–580.
IEEE, 2012.
[58] Yongzhi Su, Yan Di, Guangyao Zhai, Fabian Manhardt, Ja-
son Rambach, Benjamin Busam, Didier Stricker, and Fed-
erico Tombari. Opa-3d: Occlusion-aware pixel-wise aggre-
gation for monocular 3d object detection. IEEE Robotics and
Automation Letters , 2023.
[59] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct voxel
grid optimization: Super-fast convergence for radiance fields
reconstruction. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 5459–
5469, 2022.
[60] Yannick Verdie, Jifei Song, Barnab ´e Mas, Busam Ben-
jamin, Ales Leonardis, , and Steven McDonagh. Cromo:Cross-modal learning for monocular depth estimation. In
IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , 2022.
[61] Pengyuan Wang, HyunJun Jung, Yitong Li, Siyuan Shen,
Rahul Parthasarathy Srikanth, Loranzo Garattoni, Sven
Meier, Nassir Navab, and Benjamin Busam. Phocal: A
multimodal dataset for category-level object pose estima-
tion with photometrically challenging objects. In IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR) , 2022.
[62] Pengyuan Wang, Fabian Manhardt, Luca Minciullo, Lorenzo
Garattoni, Sven Meier, Nassir Navab, and Benjamin Busam.
Demograsp: Few-shot learning for robotic grasping with hu-
man demonstration. In 2021 IEEE/RSJ International Confer-
ence on Intelligent Robots and Systems (IROS) , pages 5733–
5740. IEEE, 2021.
[63] Zirui Wang, Shangzhe Wu, Weidi Xie, Min Chen,
and Victor Adrian Prisacariu. Nerf–: Neural radiance
fields without known camera parameters. arXiv preprint
arXiv:2102.07064 , 2021.
[64] Jamie Watson, Oisin Mac Aodha, Victor Prisacariu, Gabriel
Brostow, and Michael Firman. The Temporal Opportunist:
Self-Supervised Multi-Frame Monocular Depth. In Com-
puter Vision and Pattern Recognition (CVPR) , 2021.
[65] Jianxiong Xiao, Andrew Owens, and Antonio Torralba.
Sun3d: A database of big spaces reconstructed using sfm
and object labels. In Proceedings of the IEEE international
conference on computer vision , pages 1625–1632, 2013.
[66] Junyuan Xie, Ross Girshick, and Ali Farhadi. Deep3d:
Fully automatic 2d-to-3d video conversion with deep convo-
lutional neural networks. In European Conference on Com-
puter Vision , pages 842–857. Springer, 2016.
[67] Yiheng Xie, Towaki Takikawa, Shunsuke Saito, Or Litany,
Shiqin Yan, Numair Khan, Federico Tombari, James Tomp-
kin, Vincent Sitzmann, and Srinath Sridhar. Neural fields in
visual computing and beyond. Computer Graphics Forum ,
2022.
[68] Zhenheng Yang, Peng Wang, Yang Wang, Wei Xu, and Ram
Nevatia. Lego: Learning edge with geometry all at once
by watching videos. In Proceedings of the IEEE conference
on computer vision and pattern recognition , pages 225–234,
2018.
[69] Wei Yin, Jianming Zhang, Oliver Wang, Simon Niklaus,
Long Mai, Simon Chen, and Chunhua Shen. Learning to
recover 3d scene shape from a single image. In Proc. IEEE
Conf. Comp. Vis. Patt. Recogn. (CVPR) , 2021.
[70] Ye Yu, Dizhong Zhu, and William AP Smith. Shape-from-
polarisation: a nonlinear least squares approach. In Proceed-
ings of the IEEE International Conference on Computer Vi-
sion Workshops , pages 2969–2976, 2017.
[71] Andy Zeng, Shuran Song, Matthias Nießner, Matthew
Fisher, Jianxiong Xiao, and Thomas Funkhouser. 3dmatch:
Learning local geometric descriptors from rgb-d reconstruc-
tions. In CVPR , 2017.
[72] Guangyao Zhai, Dianye Huang, Shun-Cheng Wu, HyunJun
Jung, Yan Di, Fabian Manhardt, Federico Tombari, Nassir
Navab, and Benjamin Busam. Monograspnet: 6-dof graspingwith a single rgb image. In IEEE International Conference
on Robotics and Automation . IEEE, 2023.
[73] Guangyao Zhai, Yu Zheng, Ziwei Xu, Xin Kong, Yong Liu,
Benjamin Busam, Yi Ren, Nassir Navab, and Zhengyou
Zhang. Da2dataset: Toward dexterity-aware dual-arm grasp-
ing. IEEE Robotics and Automation Letters , 7(4):8941–
8948, 2022.
[74] Qian-Yi Zhou, Jaesik Park, and Vladlen Koltun. Open3D: A
modern library for 3D data processing. arXiv:1801.09847 ,
2018.
[75] Dizhong Zhu and William AP Smith. Depth from a polarisa-
tion + rgb stereo pair. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
7586–7595, 2019.