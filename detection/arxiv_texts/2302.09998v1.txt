Gesture Recognition with Keypoint and Radar
Stream Fusion for Automated Vehicles
Adrian Holzbock1, Nicolai Kern2, Christian Waldschmidt2, Klaus Dietmayer1,
and Vasileios Belagiannis3
1Institute of Measurement, Control and Microtechnology, Ulm University,
Albert-Einstein-Allee 41, 89081 Ulm, Germany
first name.last name@uni-ulm.de
2Institute of Microwave Engineering, Ulm University, Albert-Einstein-Allee 41,
89081 Ulm, Germany
first name.last name@uni-ulm.de
3Department of Simulation and Graphics, Otto von Guericke University Magdeburg,
Universit¨ atsplatz 2, 39106 Magdeburg, Germany
first name.last name@ovgu.de
Abstract. We present a joint camera and radar approach to enable au-
tonomous vehicles to understand and react to human gestures in every-
day traffic. Initially, we process the radar data with a PointNet followed
by a spatio-temporal multilayer perceptron (stMLP). Independently, the
human body pose is extracted from the camera frame and processed with
a separate stMLP network. We propose a fusion neural network for both
modalities, including an auxiliary loss for each modality. In our experi-
ments with a collected dataset, we show the advantages of gesture recog-
nition with two modalities. Motivated by adverse weather conditions, we
also demonstrate promising performance when one of the sensors lacks
functionality.
Keywords: Camera radar fusion, gesture recognition, automated driv-
ing.
1 Introduction
The safe interaction of traffic participants in urban environments is based on dif-
ferent rules like signs or the right of way. Besides static regulations, more dynamic
ones like gestures are possible. For example, a police officer manages the traf-
fic [37] by hand gestures, or a pedestrian waves a car through at a crosswalk [26].
Although the driver intuitively knows the meaning of human gestures, an au-
tonomous vehicle cannot interpret them. To safely integrate the autonomous
vehicle into urban traffic, it is essential to understand human gestures.
To detect human gestures, many related approaches only rely on camera
data [3,22]. Camera-based gesture recognition is not always reliable, for instance,
due to the weather sensitivity of the camera sensor [21]. One way to mitigate
the drawbacks of camera-based approaches is the augmentation of the gesturearXiv:2302.09998v1  [cs.CV]  20 Feb 20232 A. Holzbock et al.
recognition system by non-optical sensor types. A sensor with less sensitivity to
environmental conditions is a radar sensor [24] that has been shown to be suited
for gesture recognition. Furthermore, radar sensors are not limited to detecting
fine-grained hand gestures but can also predict whole body gestures [10]. Hence,
radar sensors are a promising candidate to complement optical sensors for more
reliable gesture recognition in automotive scenarios. This has been demonstrated
for gesture recognition with camera-radar fusion in close proximity to the sensors,
as presented in [17]. Furthermore, depending not only on one sensor can increase
the system’s reliability in case of a sensor failure. In contrast to the prior work,
we propose a method for whole-body gesture recognition at larger distances and
with a camera-radar fusion approach.
We present a two-stream neural network to realizing camera and radar fu-
sion. Our approach extracts independently a representation for each modality
and then fuses them with an additional network module. In the first stream,
the features of the unordered radar targets are extracted with a PointNet [23]
and further prepared for fusion with an spatio-temporal multilayer perceptron
(stMLP) [8]. The second stream uses only a stMLP to process the keypoint data.
The extracted features of each stream are fused for the classification in an addi-
tional stMLP. We train the proposed network with an auxiliary loss function for
each modality to improve the feature extraction through additional feedback. For
training, we use data containing radar targets of three chirp sequence (CS) radar
sensors and human keypoints extracted from camera images. The eight differ-
ent gestures are presented in Fig. 1 and represent common traffic gestures. The
performance of our approach is tested in a cross-subject evaluation. We improve
performance by 3.8 percentage points compared to the keypoint-only setting and
8.3 percentage points compared to the radar-only setting. Furthermore, we can
show robustness against the failure of one sensor.
Overall, we propose a two-stream neural network architecture to fuse key-
point and radar data for gesture recognition. To process the temporal context
in the model, we do not rely on recurrent models [28] but instead propose the
stMLP fusion. In the training of our two-stream model, we introduce an auxil-
iary loss for each modality. To the best of our knowledge, we are the first to fuse
radar and keypoint data for gesture recognition in autonomous driving.
2 Related Work
In the following, we discuss other methods regarding gesture recognition in gen-
eral as well as in autonomous driving. We give an overview of approaches relying
only on keypoint or radar data. Besides the single sensor gesture recognition, we
present methods using a combination of different sensors.
Gesture Recognition with Camera Data Gesture recognition is applied to
react to humans outside the vehicle [40,22] and to the passenger’s desires [8,36].
For gesture recognition of police officers, related work processes camera image
snippets [3] directly. Due to the advances in human body pose estimation [5,4],Gesture Recognition with Keypoint and Radar Stream Fusion 3
(a)
 (b)
 (c)
 (d)
(e)
 (f)
 (g)
 (h)
Fig. 1. Visualization of the characteristic poses of the eight gestures. (a) Fly. (b) Come
closer. (c) Slow down. (d) Wave. (e) Push away. (f) Wave through. (g) Stop. (h) Thank
you.
related approaches extract the body skeleton data from the images and perform
gesture recognition on it [35,22,15]. For processing skeletons to predict the ges-
ture, recurrent neural networks [35] or convolutional neural networks [22] are
applied. Besides the police officer gesture recognition, the actions of other hu-
man traffic participants like cyclists [40] or pedestrians [6] are also analyzed in
literature. Similar to pedestrian gesture recognition is the pedestrian intention
prediction [25,1], where the pedestrian’s intention to cross the street should be
recognized. Changing the view to the interior of the car, there are approaches
to recognize the driver’s activities in order to check if the driver is focused on
the traffic. For this purpose, methods like attention-based neural networks [36]4 A. Holzbock et al.
or models only built on multi-layer perceptrons [8] are developed. Compared to
our work, lighting and weather conditions influence the performance of camera-
based approaches. Due to the fusion with the radar sensor data, our method
mitigate these factors.
Gesture Recognition with Radar Data Besides radar sensors’ insensitivity
to adverse weather and lighting, they also evoke fewer privacy issues than cam-
eras. As a result, radar-based gesture recognition has received increased attention
in recent years, with research efforts mainly devoted to human-machine interac-
tion in the consumer electronics area [13]. For the control of devices with hand
gestures, gesture recognition algorithms based on a wide range of neural networks
have been proposed, involving, e.g., 2D-CNNs [12], 2D-CNNs with LSTMs [34],
or 3D-CNNs with LSTM [42]. These approaches exploit spectral information
in the form of micro-Doppler spectrograms [12] or range-Doppler spectra [34],
but it is also possible to distinguish between gestures [10] and activities [29]
using radar point clouds. The latter are a more compact representation of the
radar observations and are obtained by finding valid targets in the radar data.
Point clouds facilitate the application of geometrical transformations [29] as well
as the inclusion of additional information [10]. While most research considers
small-scale gesture recognition close to the radar sensor, reliable macro gesture
recognition at larger distances has been also shown to be feasible with radar
sensors for applications such as smart homes [18] or traffic scenarios [11,10].
While radar-only gesture recognition has shown promising results, augmenting
it by camera data can further improve classification accuracy, as demonstrated
in this paper. This is particularly important in safety-critical applications such
as autonomous driving.
Gesture Recognition with Sensor Fusion By combining data of multiple
sensors, sensor fusion approaches can overcome the drawbacks of the individual
sensor types, like the environmental condition reliance of the camera or its miss-
ing depth information. Sensor fusion has been applied e.g. for gesture-based hu-
man machine interaction in vehicles, where touchless control can increase safety
[16]. Besides the image information for gesture recognition, the depth data con-
tains essential knowledge. Therefore, other methods fuse camera data with the
data of a depth sensor [19] and process the data with a 3D convolutional neu-
ral network [16,41]. Molchanov et al. [17] run a short-range radar sensor next
to the RGB camera and depth camera and process the recorded data with a
3D convolutional neural network for a more robust gesture recognition. Another
approach fuses the short-range radar data with infrared sensor data instead of
RGB images [30]. Current fusion approaches are not only limited to short ranges
and defined environments but can also be applied in more open scenarios like
surveillance [9] or smart home applications [32]. Here, camera images and radar
data, converted to images, are used to classify the gesture, while we use the
skeletons extracted from the images and the radar targets described as unstruc-
tured point clouds. Moreover, contrary to [32], our fusion approach enhances theGesture Recognition with Keypoint and Radar Stream Fusion 5
gesture recognition accuracy not only in cases where one modality is impaired
but also in normal operation.
3 Fusion Method
We present a fusion technique for radar and keypoint data for robust gesture
recognition. To this end, we develop a neural network ˆ y=f((xR,xK), θ) de-
fined by its parameters θ. The output prediction ˆ yis defined as one-hot vector
ˆ yi∈ {0,1}C, such thatPC
c=1ˆ yi(c) = 1 for a C-category classification problem.
The radar input data is represented by xR∈RT×5×300, where T is the number
of time steps and 300 is the number of sampled radar targets in each time step,
each of which is described by 5 target parameters. For the keypoint stream, 17
2D keypoints are extracted and flattened in each time step, such that the key-
point data is described by xK∈RT×34. We use the ground truth class label y
for the training of f(.) to calculate the loss of the model prediction ˆ y. In the
following, we first present the architecture of our neural network and afterward
show the training procedure.
PointNet
stMLP stMLP
Concatenate
stMLP
Linear LayerLinear LayerLayer
NormalizationLayer
Normalization
Linear LayerKeypoint
InputRadar
Input 
Fused
PredictionKeypoint
PredictionRadar
PredictionLayer
Normalization
Fig. 2. Architecture of the proposed neural network for gesture recognition with radar
and keypoint data. The blue layers correspond to the inference neural network while
the red layers are only used to compute the auxiliary losses.6 A. Holzbock et al.
3.1 Neural Network Architecture
The neural network inputs are the radar data xRand the keypoint data xK.
The network consists of two different streams, one for each modality, to extract
the features of the different modalities. The information of the two streams is
concatenated and fed to a joint network which does the fusion and the gesture
classification. Additionally, auxiliary outputs are added to the model for the
training procedure. An overview of the proposed network architecture is given
in Fig. 2.
The feature extraction from the unordered radar data xRis done with a
PointNet [23] that extracts the features for each time step t∈ {1,2, . . . , T }. The
features of the time steps are concatenated to one feature tensor xR,PN∈RT×512.
The PointNet does not process the data in the temporal dimension but only in
the point dimension. For the temporal processing, we use the stMLP model [8],
which replaces a standard method for temporal data processing, e.g. a long short-
term memory model (LSTMs) [7]. Unlike LSTMs, the stMLP is based solely
on multilayer perceptrons (MPLs) and does not have any recurrent parts. The
stMLP processes the radar data xR,PN extracted with the PointNet in the tem-
poral and feature dimensions and outputs mixed radar features ˜ xR∈RT×H/2,
where His the hidden dimension. Due to the ordered structure of the key-
point data xK, the mixed keypoint features ˜ xK∈RT×H/2are extracted only
with an stMLP model, which processes the extracted keypoint features in both
dimensions, namely the spatial and the temporal dimension. The extracted fea-
tures of both modalities, ˜ xRand˜ xK, are concatenated to a single feature tensor
˜ x∈RT×Hand fed into another stMLP model. This model performs a spatial
and temporal fusion of the radar and keypoint features to produce a meaning-
ful representation for the gesture classification step. The gesture classification is
performed by a Layer Normalization [2] and a single linear layer for each time
step.
Additionally, we add for each modality an auxiliary output during train-
ing [31]. The auxiliary output of both modalities is built on a Layer Normal-
ization and a linear layer for the auxiliary classification. In Fig. 2, the parts of
the auxiliary outputs and the corresponding layers are drawn in red. For the
gesture prediction, each auxiliary output only uses one modality and no infor-
mation from the other. This means that the auxiliary radar output ˆ yRgets the
extracted radar features ˜ xRfor the prediction and the auxiliary keypoint output
ˆ yKthe keypoint data features ˜ xK. For the model, the auxiliary outputs give ad-
ditional specialized feedback for updating the network parameters in the radar
and the keypoint stream. After training, the layers of the auxiliary outputs can
be removed.
3.2 Model Training
For the training of the neural network introduced in Sec. 3.1 we utilize the data
presented in Sec. 4.1. During training, we use the fused output of the model ˆ y
and the auxiliary outputs derived from the radar ˆ yRand keypoint data ˆ yK. ForGesture Recognition with Keypoint and Radar Stream Fusion 7
each output, we calculate the cross entropy loss LCE, which can be formulated
as
LCE=−CX
c=0yclog(ˆyc), (1)
where yis the ground truth label, ˆ ythe network prediction, and Cthe number
of gesture classes. We use a weighted sum of the different sub-losses to get the
overall loss Lwhich can be expressed by
L=LF+µ∗(LR+LK). (2)
In the overall loss, LFis the loss of the fused output, LRof the auxiliary
radar output, and LKof the auxiliary keypoint output. The auxiliary losses
are weighted with the auxiliary loss weight µ.
4 Results
We evaluate our approach in two different settings. First, we test our approach
with two modalities, assuming no issue with the sensors and getting data from
both. Second, we use only one modality to evaluate the model. Only having one
modality can be motivated by adverse weather conditions, e.g. the camera sensor
is completely covered by snow or technical problems with one sensor.
4.1 Dataset
For the development of robust gesture recognition with the fusion of radar and
keypoint data in the context of autonomous driving, we need a dataset that
contains both modalities captured synchronously and at sufficient ranges. Con-
sequently, the custom traffic gesture dataset introduced in [10] with both camera
and radar data is used. Following, we specify the setup and the data processing
of the dataset.
Setup The gesture dataset comprises measurements of eight different gestures
shown in Fig. 1 for 35 participants. The measurements are conducted on a small
street on the campus of Ulm University as well as inside a large hall resembling
a car park. For each participant, data recording is repeated multiple times under
different orientations, and the measurements are labeled by means of the camera
data. For the measurements, a setup consisting of three chirp sequence (CS)
radar sensors and a RGB camera as illustrated by the sketch in Fig. 3 are used.
The radar sensors operate in the automotive band at 77 GHz. Each sensor has a
range and velocity resolution of 4 .5 cm and 10 .7 cm s−1, respectively, and eight
receive channels for azimuth angle estimation. The camera in the setup has a
resolution of 1240 ×1028, and keypoints obtained from the camera serve as input
to the keypoint stream of the proposed gesture recognition algorithm. All sensors
are mounted on a rail and synchronized with a common trigger signal with 30 fps.8 A. Holzbock et al.
Fig. 3. Measurement system consisting of the RGB camera and three CS radar sensors.
All sensors receive a common trigger signal for time synchronization.
Data Processing From the gesture recordings, per-frame radar point clouds
and 2D skeletal keypoints are computed. The radar responses recorded by the
sensors are processed sensor-wise to obtain range-Doppler maps [38]. The Or-
dered Statistics CFAR algorithm [27] is applied to extract valid targets and
thereby compress the information in the range-Doppler maps. For each radar
target, its azimuth angle is estimated by digital beamforming [33]. The tar-
get parameters are normalized by the radar sensors’ unambiguous ranges. Each
target in the resulting target lists is described by its range, velocity, azimuth an-
gle, its reflected power in dB, and the index in∈0,1,2 of the radar sensor that
detected it. Since the number of detected targets varies from frame to frame, tar-
get lists with a constant number NRof targets are sampled randomly for frames
whose target count exceeds NR. Contrary, for frames where the number of tar-
gets is less than NR, zero-padding is applied to fill the target lists. The target
lists of the radar sensors are stacked, such that the final target list contains 3 NR
targets. After repeating the processing for all TMframes in the measurement,
the radar observations are described by input data of shape xR∈RTM×5×300,
when setting NRto 100.
The camera data is processed by Detectron2 [39] to extract 17 keypoints
in the COCO keypoint format [14]. The extracted keypoints are normalized
with the image width and height to restrict the keypoint values for the neural
network to the range between 0 and 1. The keypoints’ 2D pixel positions over the
frames are summarized in the camera observation tensor oK∈RTM×2×17, which
is flattened to the keypoint data xK∈RTM×34. After the signal processing, the
radar and camera data of the measurements are downsampled to 15 fps and
segmented into smaller snippets with T= 30 time steps each, corresponding to
2 s of observation. Finally, 15700 samples are available for training the gesture
recognition model. In the cross-subject evaluation, we use the data of 7 subjects
for testing and the data of the remaining subjects as training and validation
data. We train the model 5 times, using different subjects for testing in each run
and average over the 5 runs for the final performance.Gesture Recognition with Keypoint and Radar Stream Fusion 9
4.2 Experimental Setup
The neural network is implemented with the PyTorch [20] framework, and we use
PointNet1and stMLP2as a base for our new fusion architecture. In the PointNet,
we apply the feature transformation and deactivate the input transformation. In
each stMLP structure we use 4 mixer blocks and set the stMLP hyperparam-
eters as follows: the hidden input dimension to 256, the hidden spatial-mixing
dimension to 64, and the hidden temporal-mixing dimension to 256. The loss is
calculated with the function defined in Eq. 2, where we set the auxiliary weight
µto 0.5. We train our model for 70 epochs with a batch size of 32 and calculate
the gradients with the SGD optimizer that uses a learning rate of 0.003, a mo-
mentum of 0.95, and a weight decay of 0.001. To get the best training result, we
check the performance during the training on the validation set and use the best
validation epoch for testing on the test set. The model performance is measured
with the accuracy metric. To prepare the neural network for missing input data,
we skip in 30% of the training samples the radar or the keypoint data, which we
call skipped-modality training (SM-training). When reporting the results with
a single-modality model, we remove the layers for the other modality.
4.3 Results
During testing with optimal data we assume that we get the keypoint data from
the camera sensor and the targets from the radar sensors, i.e., we have no samples
in the train and test set that only contain one modality’s data. The results are
shown in Tab. 1, where we compare with a model using an LSTM instead of an
stMLP for temporal processing. In the Single Modality part of the table, we show
the performance of our architecture trained and tested on only one modality. In
these cases, the layers belonging to the other modality are removed. As it can
be seen, the LSTM and the stMLP model are on par when training only with
the radar data, and the stMLP model is better than the LSTM model trained
only with the keypoint data.
In the Fusion part of Tab. 1, we show the fusion performance of our archi-
tecture. We first train the stMLP and the LSTM fusion with all the training
data and then apply SM-training with a ratio of 30%. The SM-training means
that we skip the radar or the keypoint data in 30% of the training samples. The
stMLP fusion performs better in both cases compared to the LSTM fusion. Fur-
thermore, skipping randomly one modality in 30% of the training data slightly
benefits the test performance. Overall, the stMLP architecture performs better
in the fusion of the radar and the keypoint data than the LSTM architecture.
Compared to the single-modality model, the fusion improves the performance in
both architectures (LSTM and stMLP) by over 4 percentage points. This shows
that the fusion of the keypoint and radar data for gesture recognition benefits
the performance compared to single-modality gesture recognition.
1https://github.com/fxia22/pointnet.pytorch
2https://github.com/holzbock/st mlp10 A. Holzbock et al.
Table 1. Performance of our model trained with both modalities. Single Modality is a
model that only contains layers for one modality and is trained and tested only with
this modality. The Fusion rows use our proposed model. SM-training means that we
skip one modality in 30% of the training samples during the training.
ModelTemporal Test data SM- Accuracy
Processing Radar Keypoint training in %
Single ModalityLSTM ✗ ✓ ✗ 86.2
stMLP ✗ ✓ ✗ 89.9
LSTM ✓ ✗ ✗ 85.7
stMLP ✓ ✗ ✗ 85.4
FusionLSTM ✓ ✓ ✗ 90.2
stMLP ✓ ✓ ✗ 93.7
LSTM ✓ ✓ ✓ 90.7
stMLP ✓ ✓ ✓ 93.8
4.4 Results with Single Modality
When we test our architecture with single modality, during testing only the radar
or the keypoint data are fed into the model, but not both. This can be compared
with one fully corrupted sensor due to adverse weather or a technical problem.
Contrary, during training we either use all training data (no SM-training) or set
the SM-training to a ratio of 30%, such that in 30% of samples one modality
is skipped. In the first part of Tab. 2, results are shown for the model trained
without the skipped modalities, which means that the model has not learned to
handle single-modality samples. Despite the missing modality in the evaluation,
the model can still classify the gestures, but with a decreased accuracy. Com-
paring the LSTM and stMLP variants, the LSTM variant performs better with
only the radar data and the stMLP with only the keypoint data.
In the second part of Tab. 2, we show the performance of our model trained
with skipped modalities in 30% of the training samples (SM-training is 30%).
When the model is trained with the skipped modalities, it learns better to handle
missing modalities. This results in a better performance in the single-modality
evaluation, and we can improve the accuracy by a minimum of 3 percentage
points compared to the model trained without the skipped modalities. Overall,
the fusion can improve the reliability of gesture recognition in cases where one
sensor fails e.g. due to adverse weather conditions or a technical sensor failure.
5 Ablation Studies
The performance of our model depends on different influence factors, which we
evaluate in the ablation studies. As standard setting, we choose the hyperpa-
rameters defined in Sec. 4.2 and change the ratio of the SM-training and the
auxiliary loss weight during the ablations.Gesture Recognition with Keypoint and Radar Stream Fusion 11
Table 2. Performance of our model with single-modality data. The Fusion rows use our
proposed model. SM-training means that we skip one modality in 30% of the training
samples during the training.
ModelTemporal Test data SM- Accuracy
Processing Radar Keypoint training in %
FusionLSTM ✗ ✓ ✗ 69.7
LSTM ✓ ✗ ✗ 82.6
stMLP ✗ ✓ ✗ 87.7
stMLP ✓ ✗ ✗ 78.3
FusionLSTM ✗ ✓ ✓ 80.8
LSTM ✓ ✗ ✓ 85.0
stMLP ✗ ✓ ✓ 91.2
stMLP ✓ ✗ ✓ 83.0
5.1 Amount of Single-Modality Training Samples
The training with missing data teaches the model to handle singe modality
input data, as shown in Sec. 4.4. In our standard setting for the SM-training,
we randomly skip one modality in 30% of the training samples. In this ablation
study, the SM-training ratio ranges from 0% to 60% during training and shows
the influence of this parameter on the performance.
We test the model with different SM-training ratios with the test data con-
taining both modalities and show the result in the Fusion row of Tab. 3. Addi-
tionally, we test with the data of only one modality and present the results in
theOnly Keypoints andOnly Radar row. The experiment shows that the SM-
training does not significantly influence the performance when testing with both
modalities. This is explainable because in the testing set no samples have lack-
ing modalities. Testing with only one modality, the amount of skipped-modality
samples during the training influences the accuracy. Here, the performance in-
creases until the SM-training ratio reaches 30% and then stays constant. In this
case, the amount of skipped-modality samples during training influences the
adaptation of the model to single-modality samples.
Table 3. Influence of the SM-training ratio on the overall accuracy.
Skip modality in % 0 10 20 30 40 50 60
Fusion 93.7 92.8 92.1 93.8 94.1 93.2 93.6
Only Keypoints 87.7 89.9 89.8 91.2 91.4 91.0 91.5
Only Radar 78.3 81.7 81.1 83.0 83.0 80.7 83.212 A. Holzbock et al.
5.2 Loss Function
Besides the ratio of the SM-training, the auxiliary loss weight µis an essential
parameter in training. In our standard evaluation, we set µto 0.5, while we vary
the loss weight in this ablation study from 0 to 3.
The results of the different µare shown in Tab. 4. In the first row, we deliver
the results when testing with both modalities ( Fusion ). In the Only Keypoints
and Only Radar row, we present the performance when evaluating only one
modality. As we can see in Tab. 4, the fusion results behave equally to the
single-modality results. For the Fusion as well as for the single modality, a higher
µincreases the performance, and the best accuracy is reached with µ= 0.8.
Further increasing µleads to a decreasing performance in gesture recognition.
Comparing the results of the best µ= 0.8 with the model without the auxiliary
loss ( µ= 0.0) shows that with the auxiliary loss, the fusion performance stays
equal but the single-modality accuracy increases. This indicates that the model
benefits from the additional feedback of the auxiliary loss during training.
Table 4. Influence of the loss weights on the overall performance.
Loss weight 0.0 0.2 0.5 0.8 1.0 2.0 3.0
Fusion 94.2 92.9 93.8 94.1 93.7 92.4 93.4
Only Keypoints 89.2 90.8 91.2 92.1 91.0 90.1 91.7
Only Radar 84.3 81.2 83.0 84.7 82.2 81.3 82.8
6 Conclusion
We present a novel two-stream neural network architecture for the fusion of radar
and keypoint data to reliably classify eight different gestures in autonomous driv-
ing scenarios. The proposed fusion method improves the classification accuracy
over the values obtained with single sensors, while enhancing the recognition ro-
bustness in cases of technical sensor failure or adverse environmental conditions.
In the model, we first process the data of each modality on its own and then
fuse them for the final classification. We propose a stMLP fusion which applies
besides the fusion of the features of both modalities also temporal processing.
Furthermore, for a better overall performance of our approach, we introduce an
auxiliary loss in the training that provides additional feedback to each modality
stream. The evaluation of our method on the radar-camera dataset, and we show
that even with missing modalities, the model can reach a promising classifica-
tion performance. In the ablation studies, we demonstrate the influence of the
SM-training ratio and the auxiliary loss weight.Gesture Recognition with Keypoint and Radar Stream Fusion 13
Acknowledgment
Part of this work was supported by INTUITIVER (7547.223-3/4/), funded by
State Ministry of Baden-W¨ urttemberg for Sciences, Research and Arts and the
State Ministry of Transport Baden-W¨ urttemberg.14 A. Holzbock et al.
References
1. Abughalieh, K.M., Alawneh, S.G.: Predicting pedestrian intention to cross the
road. IEEE Access 8, 72558–72569 (2020)
2. Ba, J.L., Kiros, J.R., Hinton, G.E.: Layer normalization. arXiv preprint
arXiv:1607.06450 (2016)
3. Baek, T., Lee, Y.G.: Traffic control hand signal recognition using convolution and
recurrent neural networks. Journal of Computational Design and Engineering 9(2),
296–309 (2022)
4. Belagiannis, V., Amann, C., Navab, N., Ilic, S.: Holistic human pose estimation
with regression forests. In: International Conference on Articulated Motion and
Deformable Objects. pp. 20–30. Springer (2014)
5. Bouazizi, A., Wiederer, J., Kressel, U., Belagiannis, V.: Self-supervised 3d human
pose estimation with multiple-view geometry. In: 2021 16th IEEE International
Conference on Automatic Face and Gesture Recognition (FG 2021). pp. 1–8 (2021).
https://doi.org/10.1109/FG52635.2021.9667074
6. Geng, K., Yin, G.: Using deep learning in infrared images to enable human gesture
recognition for autonomous vehicles. IEEE Access 8, 88227–88240 (2020)
7. Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural computation
9(8), 1735–1780 (1997)
8. Holzbock, A., Tsaregorodtsev, A., Dawoud, Y., Dietmayer, K., Belagian-
nis, V.: A spatio-temporal multilayer perceptron for gesture recognition.
In: 2022 IEEE Intelligent Vehicles Symposium (IV). pp. 1099–1106 (2022).
https://doi.org/10.1109/IV51971.2022.9827054
9. de Jong, R.J., de Wit, J.J., Uysal, F.: Classification of human activity using radar
and video multimodal learning. IET Radar, Sonar & Navigation 15(8), 902–914
(2021)
10. Kern, N., Grebner, T., Waldschmidt, C.: Pointnet+lstm for target
list-based gesture recognition with incoherent radar networks. IEEE
Transactions on Aerospace and Electronic Systems pp. 1–1 (2022).
https://doi.org/10.1109/TAES.2022.3179248
11. Kern, N., Steiner, M., Lorenzin, R., Waldschmidt, C.: Robust doppler-based ges-
ture recognition with incoherent automotive radar sensor networks. IEEE Sensors
Letters 4(11), 1–4 (2020)
12. Kim, Y., Toomajian, B.: Hand gesture recognition using micro-doppler signatures
with convolutional neural network. IEEE Access 4, 7125–7130 (2016)
13. Lien, J., Gillian, N., Karagozler, M.E., Amihood, P., Schwesig, C., Olson, E., Raja,
H., Poupyrev, I.: Soli: Ubiquitous gesture sensing with millimeter wave radar. ACM
Transactions on Graphics (TOG) 35(4), 1–19 (2016)
14. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll´ ar, P.,
Zitnick, C.L.: Microsoft coco: Common objects in context. In: European conference
on computer vision. pp. 740–755. Springer (2014)
15. Mishra, A., Kim, J., Cha, J., Kim, D., Kim, S.: Authorized traffic controller hand
gesture recognition for situation-aware autonomous driving. Sensors 21(23), 7914
(2021)
16. Molchanov, P., Gupta, S., Kim, K., Kautz, J.: Hand gesture recognition with 3d
convolutional neural networks. In: Proceedings of the IEEE conference on computer
vision and pattern recognition workshops. pp. 1–7 (2015)
17. Molchanov, P., Gupta, S., Kim, K., Pulli, K.: Multi-sensor system for driver’s hand-
gesture recognition. In: 2015 11th IEEE international conference and workshops
on automatic face and gesture recognition (FG). vol. 1, pp. 1–8. IEEE (2015)Gesture Recognition with Keypoint and Radar Stream Fusion 15
18. Ninos, A., Hasch, J., Zwick, T.: Real-time macro gesture recognition using effi-
cient empirical feature extraction with millimeter-wave technology. IEEE Sensors
Journal 21(13), 15161–15170 (2021)
19. Ohn-Bar, E., Trivedi, M.M.: Hand gesture recognition in real time for automotive
interfaces: A multimodal vision-based approach and evaluations. IEEE transactions
on intelligent transportation systems 15(6), 2368–2377 (2014)
20. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,
T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E.,
DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L.,
Bai, J., Chintala, S.: Pytorch: An imperative style, high-performance deep
learning library. In: Advances in Neural Information Processing Systems 32, pp.
8024–8035. Curran Associates, Inc. (2019), http://papers.neurips.cc/paper/
9015-pytorch-an-imperative-style-high-performance-deep-learning-library.
pdf
21. Pfeuffer, A., Dietmayer, K.: Robust semantic segmentation in adverse weather
conditions by means of sensor data fusion. In: 2019 22th International Conference
on Information Fusion (FUSION). pp. 1–8. IEEE (2019)
22. Pham, D.T., Pham, Q.T., Le, T.L., Vu, H.: An efficient feature fusion of graph
convolutional networks and its application for real-time traffic control gestures
recognition. IEEE Access 9, 121930–121943 (2021)
23. Qi, C.R., Su, H., Mo, K., Guibas, L.J.: Pointnet: Deep learning on point sets
for 3d classification and segmentation. In: Proceedings of the IEEE conference on
computer vision and pattern recognition. pp. 652–660 (2017)
24. Qian, K., Zhu, S., Zhang, X., Li, L.E.: Robust multimodal vehicle detection in
foggy weather using complementary lidar and radar signals. In: Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 444–453
(2021)
25. Quintero, R., Parra, I., Lorenzo, J., Fern´ andez-Llorca, D., Sotelo, M.: Pedestrian
intention recognition by means of a hidden markov model and body language.
In: 2017 IEEE 20th international conference on intelligent transportation systems
(ITSC). pp. 1–7. IEEE (2017)
26. Rasouli, A., Tsotsos, J.K.: Autonomous vehicles that interact with pedestrians:
A survey of theory and practice. IEEE transactions on intelligent transportation
systems 21(3), 900–918 (2019)
27. Rohling, H.: Radar CFAR thresholding in clutter and multiple target sit-
uations. IEEE Trans. Aerosp. Electron. Syst. AES-19 (4), 608–621 (1983).
https://doi.org/10.1109/TAES.1983.309350
28. Schreiber, M., Belagiannis, V., Gl¨ aser, C., Dietmayer, K.: Motion estimation in
occupancy grid maps in stationary settings using recurrent neural networks. In:
2020 IEEE International Conference on Robotics and Automation (ICRA). pp.
8587–8593 (2020). https://doi.org/10.1109/ICRA40945.2020.9196702
29. Singh, A.D., Sandha, S.S., Garcia, L., Srivastava, M.: Radhar: Human activity
recognition from point clouds generated through a millimeter-wave radar. In: Pro-
ceedings of the 3rd ACM Workshop on Millimeter-wave Networks and Sensing
Systems. pp. 51–56 (2019)
30. Skaria, S., Al-Hourani, A., Huang, D.: Radar-thermal sensor fusion methods for
deep learning hand gesture recognition. In: 2021 IEEE Sensors. pp. 1–4. IEEE
(2021)
31. Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D.,
Vanhoucke, V., Rabinovich, A.: Going deeper with convolutions. In: Proceedings
of the IEEE conference on computer vision and pattern recognition. pp. 1–9 (2015)16 A. Holzbock et al.
32. Vandersmissen, B., Knudde, N., Jalalvand, A., Couckuyt, I., Dhaene, T., De Neve,
W.: Indoor human activity recognition using high-dimensional sensors and deep
neural networks. Neural Computing and Applications 32(16), 12295–12309 (2020)
33. Vasanelli, C., Roos, F., Durr, A., Schlichenmaier, J., Hugler, P., Meinecke, B.,
Steiner, M., Waldschmidt, C.: Calibration and direction-of-arrival estimation of
millimeter-wave radars: A practical introduction. IEEE Antennas Propag. Mag.
62(6), 34–45 (2020). https://doi.org/10.1109/MAP.2020.2988528
34. Wang, S., Song, J., Lien, J., Poupyrev, I., Hilliges, O.: Interacting with soli: Ex-
ploring fine-grained dynamic gesture recognition in the radio-frequency spectrum.
In: Proceedings of the 29th Annual Symposium on User Interface Software and
Technology. pp. 851–860 (2016)
35. Wang, S., Jiang, K., Chen, J., Yang, M., Fu, Z., Yang, D.: Simple but effec-
tive: Upper-body geometric features for traffic command gesture recognition. IEEE
Transactions on Human-Machine Systems 52(3), 423–434 (2021)
36. Wharton, Z., Behera, A., Liu, Y., Bessis, N.: Coarse temporal attention network
(cta-net) for driver’s activity recognition. In: Proceedings of the IEEE/CVF Winter
Conference on Applications of Computer Vision. pp. 1279–1289 (2021)
37. Wiederer, J., Bouazizi, A., Kressel, U., Belagiannis, V.: Traffic control gesture
recognition for autonomous vehicles. In: 2020 IEEE/RSJ International Conference
on Intelligent Robots and Systems (IROS). pp. 10676–10683. IEEE (2020)
38. Winkler, V.: Range Doppler detection for automotive FMCW radars.
In: Eur. Radar Conf. pp. 166–169. IEEE, Piscataway, NJ (2007).
https://doi.org/10.1109/EURAD.2007.4404963
39. Wu, Y., Kirillov, A., Massa, F., Lo, W.Y., Girshick, R.: Detectron2. https://
github.com/facebookresearch/detectron2 (2019)
40. Xu, F., Xu, F., Xie, J., Pun, C.M., Lu, H., Gao, H.: Action recognition framework
in traffic scene for autonomous driving system. IEEE Transactions on Intelligent
Transportation Systems (2021)
41. Zengeler, N., Kopinski, T., Handmann, U.: Hand gesture recognition in automotive
human–machine interaction using depth cameras. Sensors 19(1), 59 (2018)
42. Zhang, Z., Tian, Z., Zhou, M.: Latern: Dynamic continuous hand gesture recogni-
tion using fmcw radar sensor. IEEE Sensors Journal 18(8), 3278–3289 (2018)