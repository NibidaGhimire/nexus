â€œOh, Sorry, I Think I Interrupted Youâ€: Designing Repair Strategies
for Robotic Longitudinal Well-being Coaching
Minja Axelsson
minja.axelsson@cl.cam.ac.uk
University of Cambridge
United KingdomMicol Spitaleâˆ—
micol.spitale@polimi.it
Politecnico di Milano
ItalyHatice Gunes
hatice.gunes@cl.cam.ac.uk
University of Cambridge
United Kingdom
ABSTRACT
Robotic well-being coaches have been shown to successfully pro-
mote peopleâ€™s mental well-being. To provide successful coaching, a
robotic coach should have the capability to repair the mistakes it
makes. Past investigations of robot mistakes are limited to game or
task-based, one-off and in-lab studies. This paper presents a 4-phase
design process to design repair strategies for robotic longitudinal
well-being coaching with the involvement of real-world stakehold-
ers: 1) designing repair strategies with a professional well-being
coach; 2) a longitudinal study with the involvement of experienced
users (i.e., who had already interacted with a robotic coach) to in-
vestigate the repair strategies defined in (1); 3) a design workshop
with users from the study in (2) to gather their perspectives on the
robotic coachâ€™s repair strategies; 4) discussing the results obtained
in (2) and (3) with the mental well-being professional to reflect on
how to design repair strategies for robotic coaching. Our results
show that users have different expectations for a robotic coach than
a human coach, which influences how repair strategies should be
designed. We show that different repair strategies (e.g., apologizing,
explaining, or repairing empathically) are appropriate in different
scenarios, and that preferences for repair strategies change during
longitudinal interactions with the robotic coach.
CCS CONCEPTS
â€¢Human-centered computing â†’User studies ;HCI design and
evaluation methods ;â€¢Computer systems organization â†’
Robotics .
KEYWORDS
socially assistive robotics, well-being, coaching, interaction rup-
tures, robot mistakes, human-robot interaction, design research
1 INTRODUCTION
Robot mistakes and failures are a very well-known problem in
the human-robot interaction (HRI) research community [ 15]. For
example, robots can hit objects, interrupt people speaking, mis-
understand human speech, not respond to people, and experience
Wi-Fi malfunctions [ 45,50]. Therefore, designing repair strategies
is extremely important for the success of the human-robot interac-
tion. Very recently, the HRI interest in applying and using robots
as mental well-being coaches has increased. Robotic coaches for
well-being have been examined for use at the workplace [ 2,48],
in public environments [ 3,35], in a lab context [ 1,6,10], and for
at-home use [ 24â€“26]. In these contexts, addressing the problem of
âˆ—This work was undertaken and finalised while Micol Spitale was a postdoctoral
researcher at the University of Cambridge.robot mistakes by designing repair strategies is even more relevant
for the success of the coaching practice and for making a step for-
ward towards the deployment of such robotic coaches in real-world
scenarios.
Past works have investigated the problem of repair strategies
[11,28,44,54] but they have several limitations. First, most of them
are limited to game-based (e.g., [ 45]) or task-based scenarios (e.g.,
[28]), without exploring the extent to which those results may be
applicable to other contexts, such as well-being coaching. Second, in
most of these studies, the users interacted with the robot in a one-off
interaction [ 45,59]. This constrained the design of repair strategies
to short-term interactions without considering longitudinal effects
(i.e., how the usersâ€™ perceptions change over time). Third, most
of the studies have been undertaken in the lab [ 28,59], making it
difficult to generalise these results to real-world settings.
In this study, we sought to overcome those limitations by de-
signing repair strategies for robotic longitudinal well-being coach-
ing with the involvement of real-world stakeholders. To this end,
we undertook a 4-phase design process in which we involved a
professional mental well-being coach, and users who had already
interacted with a robotic coach in their workplace. As a first step,
we designed a set of repair strategies together with the professional
human coach, utilizing their expertise and experience. In the second
phase, we deployed a robotic coach at a workplace and compared
the two sets of repair strategies over four weeks by involving 12
users who had already interacted with a robotic coach before. After
the study, we informed the users of the study set-up, and asked for
their feedback on the robotâ€™s mistakes in a design workshop. Finally,
we reflected on the user study results and user insights with the
professional coach from the first phase, in order to further inform
the design of repair strategies for robotic well-being coaches.
This paper contributes to: 1) designing repair strategies appropri-
ate for robotic well-being coaching; 2) investigating the longitudinal
effect of repair strategies; and 3) collecting user perspectives on
robot mistakes and repair strategies in a real-world context.
2 RELATED WORK
Well-being Coaching and Mistakes. While there is no single defini-
tion of well-being [ 36], we focus on the definition of well-being as
positive psychological functioning [ 42]. Mental well-being coach-
ing aims to support coachees to thrive in their life [ 21] and improve
positive psychological functioning [ 53]. Positive psychology coach-
ing in particular aims to help coachees to focus on positive aspects
of their life [ 46], through exercises where the coachee e.g. focuses
on experiences where they felt grateful, in order to enhance positive
memories and goal attainment [ 13,58]. During coaching, difficul-
ties with the coach, such as the coach being vague or not sensitivearXiv:2401.03794v1  [cs.RO]  8 Jan 2024Minja Axelsson, Micol Spitale, and Hatice Gunes
Phase 1
Repair strategy designPhase 2
User studyPhase 3
Design workshop with usersPhase 4
Coach reï¬‚ections on user insights
Future work: Design new repair strategies informed by design process
Session 1
Scenario : The robotÂ  interrupts youÂ 
(aÂ ï¬rst- time user) for theÂ  ï¬rst time.
Why:  The speech processingÂ 
thought you had stopped speakingÂ 
because you paused in yourÂ 
response.
EXAMPLE
Repair : Empathic, ask how personÂ 
is feeling
Why:  First time interruption so userÂ 
is feeling unsure
Name:  Minja
Scenario: The robotÂ doesn'tÂ 
respond  to you (aÂ  returning user)Â 
for 10 seconds for theÂ  third time inÂ 
this session .
Why:  The Wi- Fi is slow and theÂ 
messages the robot is passing toÂ 
generate a response are delayed.Repair options:
Do nothing: This mistake does not require a repair
Empathic: Ask how the person is feeling after the mistake and acknowledge that feeling
Technical explanation : Explain to the user the cause of a speci ï¬c error (e.g. Wi-  Fi causing delay in robot response)
Instructions on use: Tell the user how they can avoid the error (e.g. speak closer to the microphone)
Other: [Fill in what you would like the robot to do]
Repair: ?
Why: ?
Name: ?
Repair: Technical explanation (inÂ 
addition to brief apology)
Why:  It helps to understand thatÂ 
the robot isn't being malicious, andÂ 
that the problem is external to theÂ 
robot and neither the robot nor theÂ 
user can do anything about it.
Name:  LucySession 2 Session 3 Session 4 Session 8 Session 20
Scenario : The robotÂ  interrupts  youÂ 
(aÂ returning user),Â for the secondÂ 
time in the session, when you areÂ 
sharing an experience veryÂ 
enthusiastically.
Why:  The microphone is notÂ 
working well.
Repair : Technical explanation +Â 
opportunity to share again whatÂ 
you were saying
Why: Nice to get an explanationÂ 
but no need for me to beÂ 
distracted thinking about anotherÂ 
topic (what I feel about thisÂ 
interruption). I just want toÂ 
summarise my feelings again.
Name:  Michelle
Repair: Apologise and say 'pleaseÂ 
continue'
Why:  Don't want to forget theÂ 
story!  Easy to take the lazier optionÂ 
and minimise details when havingÂ 
to repeat so better to try to get meÂ 
to keep going.
Name:  Jen
Scenario : The robotÂ  doesn'tÂ 
respond to you for 20 seconds forÂ 
ï¬rst time in this session.
Why:Â  The connection to the serviceÂ 
generating the robot's response isÂ 
slow.
Repair : Simple apology
Why:  I expect to wait sometimes...Â 
so it's totally ï¬ne I don't need toÂ 
know why necessarily (vibe ofÂ 
making excuses for poorÂ 
performance?)
Name:  Michelle
Repair: Technical, Explain why itÂ 
happened in more detail andÂ 
mention that it will probablyÂ 
happen again
Why: Happening for the ï¬rst timeÂ 
so it was strange. Knowing how toÂ 
prevent it from happening wouldÂ 
have helped in future sessions
Name: Daren
Repair: Empathic + technicalÂ 
explanation + instructions for useÂ 
(if user can do anything about it)
Why:  If it's true, it's an impressiveÂ 
capability, and good for users toÂ 
know how to correct.
Name: Michelle
Repair: Technical - MainlyÂ 
acknowledge the mistake, then tryÂ 
again/move on
Why:  Mistakes happen, no reasonÂ 
to make it a big deal
Name: Paul
Repair: Technical Explain +Â 
Instructions
Why:  It's the ï¬rst time the user hasÂ 
seen this error, so they need toÂ 
understand and know how / why toÂ 
ï¬x.
Name: Joe
Repair : Balanced empathic andÂ 
non-  empathic (acknowledge theÂ 
error and keep going)
Why: Acknowledging the errorÂ 
shows the robot is on top ofÂ 
things/remains the expert in theÂ 
room, but also maintainsÂ 
momentum of the session.
Name: Jen
Repair : Technical explanation /Â 
instructions for use
Why: Since this is a ï¬rst time use,Â 
there, it's likely the set-  up isn'tÂ 
optimal so helping the user toÂ 
improve / ï¬x will help in theÂ 
future. It may also be worthÂ 
pointing out to the user in a semi- 
empathetic way that "these thingsÂ 
may happen, and if they happenÂ 
again I'll let you know" orÂ 
something like that, so that theÂ 
user doesn't get surprised by anyÂ 
future / repeated mistakes and toÂ 
set expectations.
Name:  Robert
Repair: TechnicalÂ 
Repair: Apologise brie ï¬‚y but noÂ 
explanation or other repair needed
Why:  Doing more than this causesÂ 
unnecessary interruption. A briefÂ 
apology and then moving on isÂ 
what i'd expect of a human in thisÂ 
scenario
Name: Lucy
Repair: Technical explanation
Why:  A ï¬rst error probably doesn'tÂ 
need an instruction to ï¬x yet -Â 
allows for the user to self-  correctÂ 
initially
Name: Martin
Repair :Technical
Why:  Explain there has been aÂ 
glitch and what to do to correct it
Name: Jason
Repair: Instructions and technical
Why:  Might be helpful at this stageÂ 
to give the user a reason and a fewÂ 
useful prompts of ways they couldÂ 
optimise the interaction, if there's aÂ 
physical/technical problem
Name:  Martin
Repair : Brief TechnicalÂ 
Explanation
Why:  The user knows what theÂ 
error is as it's happened multipleÂ 
times but their repair attemptsÂ 
haven't worked so moreÂ 
instructions to ï¬x are unlikely toÂ 
achieve the ï¬x - but shouldÂ 
acknowledge in a small way theÂ 
error.
Name: Joe
Repair: Technical explanation +Â 
apology
Why:  Set expectations for the restÂ 
of this current session. ThisÂ 
expectation probably won't carryÂ 
over to other sessions as it's aÂ 
temporary problem.
Name:  Michelle
Repair: Empathetic + Technical -Â 
Keep the user informed
Why:  At this point the user isÂ 
probably annoyed, so both keepÂ 
them informed allow them aÂ 
chance to voice their frustration
Name:  Paul
Repair: Empathetic and technicalÂ 
explanation
Why: I think at this point, since it hasÂ 
happened 3 times, I would start to getÂ 
annoyed. So empathy would make meÂ 
feel validated. A technical explanationÂ 
then would allow me to potentially haveÂ 
control over the solution and also haveÂ 
empathy for the robot.
Name: Emma
Repair: Technical explanation andÂ 
possibly (once issue is ï¬xed)Â 
something to "restart" the sessionÂ 
(the ï¬‚ow is probably broken)
Why: As a user, I'd probably beÂ 
quite worried that something hadÂ 
seriously gone wrong so someÂ 
pointers how to ï¬x it would beÂ 
good. 10s is quite long, so maybeÂ 
a "holding response" would alsoÂ 
help to avoid too much awkwardÂ 
silence
Name: Robert
Repair: Empathic and technical
Why:  To know that the robot isn'tÂ 
just ignoring what I'm saying, andÂ 
understand that there is a recurringÂ 
technical issue
Name:  Daren
Repair : Instructions on use
Why: This is a problem which isÂ 
ï¬xable by the user, but they mightÂ 
not realise that by themself, soÂ 
pointing this out could make theÂ 
rest of the session run moreÂ 
smoothly. This is also what i'dÂ 
expect of a human counsellor etcÂ 
in this situation.
Name: Lucy
Repair:  short apology
Why:  It's the 3rd time so don'tÂ 
need to get into a detailedÂ 
discussion
Name:  Jen
Repair : Empathic andÂ 
instructional
Why: Need to acknowledge thatÂ 
the user was enthusiasticallyÂ 
responding, and apologise for theÂ 
 interruption. Then giveÂ 
instructions to solve it so itÂ 
doesn't happen again. BUT NOTÂ 
ASKING HOW IT MADE YOU FEEL!
Also, maybe come back to theÂ 
same question later to recap andÂ 
ï¬ll in the  gaps
Name: Martin
Repair: Instructions + maybe aÂ 
quick mic test?
Why:Â No strong opinions, not reallyÂ 
sure what is best
Name:  Paul
Repair : Empathy + Technical ExplanationÂ 
+ instructions
Why: Again, it would provide validationÂ 
to the users feelings, provide insightÂ 
into the robots issues to allow the userÂ 
to have empathy, and instructionsÂ 
would give me a sense of control overÂ 
the situation.Â 
Name:  Emma
Repair: Instructions on use
Why: The user should know whatÂ 
the error is so doesn't need theÂ 
explanation - but it's aÂ 
straightforward ï¬x to improve theÂ 
quality of the rest (and majority of)Â 
the session.
Name: Joe
Repair : Technical explanation (inÂ 
addition to brief apology) plusÂ 
stating that they've reported theÂ 
problem to whoever controls theÂ 
robot
Why: It helps to understand thatÂ 
the robot isn't being malicious,Â 
and that the problem is externalÂ 
to the robot and neither the robotÂ 
nor the user can do anythingÂ 
about it. It is also reassuring to theÂ 
user that someone might be ableÂ 
to ï¬x the problem so it doesn'tÂ 
recur in future sessions.
Name: Lucy
cOULD BE A MIXED RSPONSE AS INÂ 
IF IT HAPPENS MORE THAT ONCEÂ 
SAY SORRY  AND ASK HOW THEÂ 
PERSON IS FEELING. THEY MAYÂ 
WANT TO VENT THEIRÂ 
FRUSTRATION
jASON
Probably use to the technical glitches byÂ 
Repair: Empathic, technical andÂ 
instructions
Why:  Understanding why it keepsÂ 
happening and how to remedyÂ 
would make the session better, theÂ 
interruptions can be annoying
Name:  Daren
Repair : Technical explanation andÂ 
apology / motivation if possible.
Why: This seems like somethingÂ 
that should be easily ï¬xable *if*Â 
the source of the fault is reliablyÂ 
known - e.g. "move theÂ 
microphone closer". Not sure howÂ 
to restore the "enthusiasm"Â 
though after the interruption,Â 
presumably the not- working micÂ 
meant the robot didn't knowÂ 
about the enthusiasm in the ï¬rstÂ 
place. But if there is anyÂ 
information, it might be good toÂ 
reference it, e.g. "can you pleaseÂ 
repeat what you said about such- 
and-  such"
Name: Robert
Repair : Technical explanation
Why:  It's an error that's outside ofÂ 
the user control and not reallyÂ 
something they can ï¬x - userÂ 
should decide if they want toÂ 
continue with the session.
Name:  Joe
Repair: Do nothing
Repair : Brief apology and technicalÂ 
explanation
Why:  It's ok to wait a few seconds,Â 
but knowing that it was a technicalÂ 
issue would help make it lessÂ 
awkward. User could think that theÂ 
robot is expecting to hear moreÂ 
before responding
Name: Daren
Probably use to this issues by now so aÂ 
quick appology and what ever troubleÂ 
shooting is necessary will do
JasonI AGREE (Michelle)
Yes, me too (Lucy) especially because in this application, it's not so critical that the robot hasÂ 
genuinely heard and understood everything the user has said. It's more important that theÂ 
user gets their story out in a way that is helpful for them.
Probably use to the technical glitches byÂ 
now  so whilst I might have lost my train ofÂ 
thought  just telling n=me to adjust theÂ 
mike would be ok.
Jason
Repair : Brief technical explanation
Why:  We know each other very wellÂ 
now... let's cut through the nicetiesÂ 
and get on with this!
Name:  Martin
Repair : Technical explanationÂ 
and/or go to an oï¬„ine routine?
Why:Â Not sure what's better,Â 
having someone wait a very longÂ 
time or doing a scripted routine
Name: Paul
Repair : Intermediate response /Â 
technical explanation
Why: 20s is a very long time, soÂ 
having some response beforeÂ 
then (e.g. "I'm thinking about this")Â 
could be helpful to avoid theÂ 
awkward silence. Again of course,Â 
if we know what the issue is andÂ 
how to prevent it fromÂ 
reoccurring, a technicalÂ 
explanation might help
Name: Robert
Scenario : The robot repeatedlyÂ 
asks to re- phrase what the userÂ 
was saying
Why:Â robot does not understandÂ 
the user's accent / sounds fromÂ 
microphone too quiet to processÂ 
properly / genuine issues withÂ 
extracting key information fromÂ 
the user's statement (e.g. due toÂ 
complexity of sentences,Â 
vocabulary used, etc)
Name:Â  Robert
Scenario : The robotÂ 
misunderstands  you (aÂ  returningÂ 
user) for theÂ  sixth time in thisÂ 
session
Why:Â The speech recognitionÂ 
system doesn't work for yourÂ 
voice/accent
Name:Â Lucy
Scenario : after an interruption,Â 
asked me to repeat my exampleÂ 
and then asked me to repeat again
Why: ?
Name: Jen
Scenario : Lag between robotÂ 
stopping speaking and starting toÂ 
listen (click!) - and user startsÂ 
speaking too soon
Why:Â Upload/download speed toÂ 
slow, and user not yet alert to theÂ 
behaviour
Name: Martin
Scenario : Takes a while (>30 sec) toÂ 
respond to user after user tellsÂ 
lengthy personal response.
Why: lag with query to chatgptÂ 
API.
Name: Emma
Scenario: Asked a really generic questionÂ 
"why do you feel this way?" or smth (veryÂ 
very small error compared to others but IÂ 
was actually a bit disappointed because byÂ 
then I was used to it asking me great,Â 
insightful questions like "what otherÂ 
dishes do you like to cook apart fromÂ 
chicken satay?")
Why:Â Idk, I guess it's just the way the APIÂ 
worked then
Name:Â Michelle
Scenario : As aÂ  returning user, t heÂ 
robotÂ goes oï¬€-  topic for theÂ ï¬rstÂ 
time  stating "As an AI LanguageÂ 
Model..."
Why:Â LLM losing context / givingÂ 
poor response
Name: Â Joe
Scenario : Robot doesn't repeatÂ 
itself
Why:Â Human wasn't payingÂ 
attention and asks the robot to sayÂ 
its question again
Name:Â Paul
Scenario :Robot pauses after userÂ 
spoke, then says that it did not hearÂ 
and asks user to repeat
Why:
Name: Daren
Scenario: repeatedly asked toÂ 
repeat the same information overÂ 
and over again and again as ifÂ 
stuck on a loop
Why:  My solution would be IT 101Â 
turn it oï¬€ and back on again to seeÂ 
if it reboots! Jason
Scenario : Robot asks questionÂ 
which throws the user oï¬€ train ofÂ 
thought or down less usefulÂ 
trajectory of discussion (e.g. "whatÂ 
wood did you use to make yourÂ 
workbench?")
Why:Â Language model picks up onÂ 
wrong sentiment is user speech
Name:Â Martin
Scenario : The robotÂ  interrupts Â 
you (aÂ  returning user ) for theÂ  thirdÂ 
time in this session, whilstÂ 
apologising for interrupting youÂ 
the previous time
Why:Â  idk Â¯\_( )_/Â¯
Name:Â Lucy
Goal: Deï¬ne two sets of repair strat-
egies to compare in a user study
Methdology: 2 discussions with a 
professional coach
Goal: Examine user perceptions of 
robotic coach, and empathic and 
non empathic repair strategies
Methdology: Between-subjects 
user study (n = 12) over four 
weeks at a workplace
Outcome:
Two sets of repair 
strategies: empathic 
and non-empathic
Outcome:
Coach reï¬‚ec-
tions on user 
insights to 
inform the 
design and 
use of speciï¬c 
repair strate-
gies in robotic 
well-being 
coaching
Outcome:
While quant. data 
shows preference 
for empathic strat -
egies, users had 
varied preferences 
which need more 
investigation
Outcome:
Insights on what 
mistakes users 
experienced, and 
what repair strat-
egies they prefer 
when those mis-
takes happen
Goal: Receive user feedback 
on when speciï¬c repair strate-
gies are appropriate
Methdology: Online design 
workshop with Miro board
Goal: Reï¬‚ect on user in-
sights with professional 
coach
Methdology: Online dis-
cussion with the coach
Figure 1: A timeline of the four study phases with their goals, methodologies, and outcomes that steered the next phase.
or supportive enough, not being flexible, and struggling with the
concepts of coaching, can disrupt the coach-coachee relationship
[8]. Such issues can negatively impact the alliance between the
coach and coachee [ 43], and as such disrupt the interaction [ 12].
Resolving such mistakes is important so that the coachee can gain
the maximum positive impact from coaching.
Robotic Coaching. In a workplace coaching context, robotic forms
have been compared, finding preference for a toy-like robot [ 48]. In
public settings, robots have been used to conduct group Mindfulness
sessions [ 3], finding that robots can be helpful but need to be more
responsive; and to conduct private deep-breathing sessions at a
health centre [ 35], finding that participants successfully completed
deep breathing exercises with the robot. Finally, in home contexts,
a robotic coach has been deployed in several at-home studies [ 24â€“
26], finding that usersâ€™ well-being improved, and that the robot
was perceived more positively when it related to the user as a
companion rather than a coach. While these studies all examined
robotic coaching in different contexts, none of them focused on
the examination and repair of robot mistakes during coaching.
Spitale et al. examined how users expressed their behaviour during
the mistakes a robotic coach makes [ 50], however they did not
examine what repair strategies robotic coaches should use when
they make mistakes. In this study, we present the first steps into
examining what repairs are applicable to robotic well-being
coach mistakes .
Robot Mistakes and Repairs in HRI. Within the context of social in-
teractions, past works have focused on understanding robot failures
to improve various aspects of human perception towards robots,
such as trust [ 20]. Correia et al. explored how technical failures of
an autonomous social robot affects trust during a HRI collaborative
scenario [ 11]. Their results showed that a faulty robot is perceived
as significantly less trustworthy. Analogously, van Wareven et al.
investigated the effect of robot failure severity on participantsâ€™ sub-
jective rating of the robot in a room-escape scenario [ 54] and found
that the severity affects the faith participants had on robots in future
scenarios. Salem et al. explored how robot mistakes affect trust-
worthiness and acceptance in human-robot collaboration [ 44], and
showed that subjective perceptions of the robot and assessments of
its reliability and trustworthiness has been significantly affected
by the robotâ€™s performance. Robot mistakes and repair strategies
have been largely examined in the context of task-based interac-
tions. Kontogiorgos et al. examined a Furhat robot simulating errorsduring Wizard-of-Oz cooking instructions [ 28â€“30], finding that par-
ticipants responded most positively through non-verbal responses
to a robotâ€™s explanations. Esterwood and Robert also found that
explanations were the most effective strategy for trust repair after
repeated mistakes in a box sorting task [ 14,16]. Additionally, Sebo
et al. found that a robotâ€™s apology was a more effective trust repair
strategy than denial during a game [ 45]. However, these studies
did not examine the longitudinal effects of different repair
strategies .
3 DESIGN PROCESS OVERVIEW
This work aims to understand how a robotic longitudinal well-being
coach could repair mistakes during coaching. We have undertaken
a design process that included four phases depicted at glance in Fig.
1: (Phase 1 ) repair strategy design, ( Phase 2 ) user study, ( Phase
3) design workshop with users, and ( Phase 4 ) coach reflections
on user insights. Each phase was informative for the following
one, and we set our goals every step of the way during the design
process.
Phase 1 aimed at designing two sets of repair strategies for
robotic longitudinal well-being coaches. To accomplish this, we
first discussed with a professional well-being coach the robot mis-
takes in well-being coaching (Discussion 1), and we then defined
with them potential repair strategies (Discussion 2), as detailed in
Section 4. The results of this design phase were the formulation of
empathic and non-empathic repair strategies. Building upon these
results, Phase 2 aimed at investigating the user perception towards
a robotic coach that used empathic and non-empathic repair strate-
gies in a user study, as described in Section 5. We found that users
had various opinions and preferences towards empathic and non-
empathic conditions, opening up further investigation. Hence, we
designed Phase 3 as a design workshop with users of the study to
better understand the appropriateness of each repair strategy in
different scenarios, as detailed in Section 6. The workshop results
showed that different repair strategies may be preferred depending
on what errors the users experienced, and personal preferences.
Finally, we involved the mental well-being coach from Phase 1 in
Phase 4 to gather their feedback and reflect on the findings from
Phase 2 and Phase 3, as reported in Section 7.
4 PHASE 1: REPAIR STRATEGY DESIGN
In Phase 1, we set out to understand how to design repair strategies
for robotic coaches, i.e., what a robot should say when it makes aâ€œOh, Sorry, I Think I Interrupted Youâ€: Designing Repair Strategies for Robotic Longitudinal Well-being Coaching
mistake. To do this, we had two discussions with a professional well-
being coach, in order to find out how they repair mistakes during a
coaching session, and to design appropriate repair strategies for a
robotic well-being coach with the professional coach.
DISCUSSION 1: Robot Mistakes
In this first 2-hour discussion with a professional well-being coach,
we followed a structure consisting of four parts: 1) show the coach
videos of a robotic coach interacting with participants, and in-
stances of interaction ruptures; 2) ask the coach what the robot
did wrong; 3) ask the coach how the robot should attempt to re-
pair those situations; and 4) how these repair strategies should
best be deployed in a user study. Below, we detail each part of the
discussion.
1)In our previous study [ 48], we collected video recordings of
coachees interacting with a robotic coach in a workplace setting,
and subsequently analyzed interaction ruptures, i.e. instances where
the robotic coach made mistakes and/or where coachees felt awk-
ward [50]. We used this video data, and selected short clips that
contained instances of interaction ruptures (as annotated in [ 50]).
We showed 6 clips in total, ranging from 22 seconds to 2 minutes
and 20 seconds.
2)The coach noted that the robotâ€™s main mistakes were interrupting
the coachee, and not providing a response for a long period of time.
Additionally, the robot did not respond appropriately to the content
of the userâ€™s speech. In the videos from our previous study [ 50],
the robot was responding in a pre-scripted manner, which was a
barrier to generating appropriate responses on the go.
3)The coach mentioned that initially, the robot should apologize
for the mistake it has made, and then identify what mistake has been
made (e.g., â€œOh, sorry, I think I interrupted you.â€), as that is what
they would do as a coach in a session. Robot apologies have also
been previously used to repair trust in human-robot interactions
[40]. The coach suggested that after the apology, the robot should
then acknowledge its limitations as a robot : the robot should explain
why a mistake happened, or give its best guess of the technical fault
if there is no definitive information (e.g., â€œThe Wi-Fi is not working
well. â€); and the robot should emphasize its intent to improve (e.g.,
â€œIâ€™m trying to improve.â€), since self-awareness of robot limitations
could put people at ease.
4)In order to examine usersâ€™ perceptions of a robotic coachâ€™s repair
strategies, we discussed with the coach whether the robot should
make intentional mistakes during a coaching session, and then
deploy the repair strategies defined above. We decided to examine
the most common mistakes of the robot, i.e., interrupting and not
responding, which we identified from the videos. In order to collect
repair strategies with the structure defined above, we scheduled a
second discussion with the coach.
Outcome: Identifying common robot mistakes during coaching
interactions (interrupting and not responding), and creating the
structure for repair strategies (apology, explanation, and intent to
improve).
DISCUSSION 2: Repair Strategies
In this 1.5-hour discussion, we used bodystorming [38], i.e., we
simulated an interaction with the robotic coach by asking the pro-
fessional coach to act as the robotic coach, and two researchersacting as coachees. We did this in order to experience the coach-
ing session, the mistakes, and the perception of the planned repair
strategies from the perspective of the coachees. In preparation for
the discussion, we adapted four existing well-being exercises with
the coach: (1) savouring , where the coach asks the coachee to reflect
on a positive memory in the recent past ([ 47]); (2) gratitude , where
the coach asks the coachee to think of things they were grateful
for ([ 18]); (3) accomplishments , where coachees reflect on accom-
plishments ([ 18]); and (4) one door closes one door opens , where
coachees think of a time when a door closed (i.e., they missed out
on an opportunity), and what doors opened as a result (i.e., new
opportunities arose), in order to cultivate optimism ([27]).
During the bodystorming, we asked the coach to act as a robot
and intentionally interrupt and not respond to the researchers, and
then use the repair strategy we designed (see Sec. 4) in Discussion
1 (i.e., apology, explanation, and intent to improve). We provided
the coach with a list of causes for robot errors such as interrupting
and not responding, to use in their explanation : microphone fault,
processing error, slow Wi-Fi, and error in speech understanding.
We structured the 1.5-hour session as follows: 1) 30 minutes of
coaching, mistakes and repair strategies with 15 minutes per each
researcher while the other took notes; 2) 15 minutes of discussion;
3) 30 minutes of coaching, mistakes and repair strategies with 15
minutes per researcher while the other took notes; and 4) 15 minutes
of discussion. We detail each part of the discussion below.
1)The coach conducted each exercise with the two researchers,
acting as the robotic coach, and asked for 2 instances of positive ex-
periences during each exercise (e.g., 2 moments when the coachee
felt grateful). During each instance, the coach made a mistake (not
responding or interrupting), and then used a repair strategy (apol-
ogy, explanation, and intent to improve). We collected 8 different
phrasings of repair strategies in this manner.
2)After the first half hour, both researchers and the coach had a
discussion on how the session went. Both researchers noted that
despite the employed repair strategies, they did not feel understood
or listened to , and that the coach did not understand how they felt .
The coach also mentioned that it could be helpful to ask the coachee
how they felt after the coach made a mistake. Both researchers noted
that the coaching interaction appeared awkward . We decided that
to resolve these issues (which have also been previously reported in
robotic coaching [ 1,3,48]), the robotic coach should be empathic .
In fact, empathic communication in a therapeutic context can help
a client feel listened to, understood and accepted, and have their
feelings validatedâ€”improving outcomes for well-being [ 23,56].
Empathy can also be conducive to resolving affective ruptures in
interpersonal interactions [ 17]. The coach suggested that we amend
the repair strategy structure to be empathic as follows (additions
in bold): apology, ask for user emotion ,empathize with user
emotion ,reassure user , explanation, and intent to improve.
3)For the second 30-minute coaching session, each researcher again
did two exercises with the coach. This time, the coach used the
empathic repair strategies as designed above. Again, the coach
administered each of the four exercises, with 2 instances of each,
resulting in 8 different phrasings of empathic repair strategies.
4)In the final discussion, we found that the coach felt the empathic
strategies better suited a robotic coach, and the researchers felt
listened to and heard , and that the coach understood how they felt .Minja Axelsson, Micol Spitale, and Hatice Gunes
The researchers also observed that the interaction appeared less
awkward . We concluded that in order to examine how empathy
impacts usersâ€™ perceptions of repair strategies , we should con-
duct a between-subjects study comparing the two different types of
repair strategies (empathic and non-empathic). We also discussed
that in order to investigate how usersâ€™ opinions of robotic coach
repair strategies evolve over time, the study should be longitudinal.
Outcome: Study design of comparing empathic and non-empathic
repair strategies, and 8 phrasings of each type of repair strategy.
5 PHASE 2: USER STUDY
In order to examine the empathic and non-empathic repair strate-
gies defined in Phase 1 (Sec. 4), we conducted a longitudinal user
study where the robot executed those repair strategies (Phase 2 in
Fig. 1). The study design, the experiment protocol, and the consent
forms were approved by the Ethics Committee of the Department
of Computer Science and Technology, University of Cambridge.
5.1 Protocol & Questionnaires
We conducted the study over four weeks, with one well-being
exercise (a maximum of 10 minutes) administered by the robotic
well-being coach per week. Users interacted with the robot at their
workplace, in a room reserved for the study, with the robot standing
on a table 1.5 meters away from the user, and the user sat at a chair
next to the table. After each interaction, users filled in a PANAS
questionnaire about their positive and negative emotions [ 55], RoPE
questionnaire about the userâ€™s perception of the robotâ€™s empathy [ 9],
RoSAS about the userâ€™s perception of the robotâ€™s social attributes
[7], and the MDMT questionnaire about trust in the robot [ 34]. At
the end of the study, users took part in a semi-structured interview
about their overall experience with the robot (15-20 minutes). This
interview had two parts: one with general questions about the robot,
its mistakes and repair strategies; and one after disclosing the study
protocol and the pre-planned mistakes to the user, followed by
questions about the robotâ€™s mistakes and repair strategies. Interview
questions are reported in the Supplementary Material (Sec. 1).
5.2 Users
The users (ğ‘›=12) were recruited from the host company called
Cambridge Consultants Inc., and had previously interacted with a
robotic well-being coach in a 4-week study [ 48], where they expe-
rienced commonly known robot errors (such as interruptions and
slow responses). We selected these experienced users in order to to
mitigate the novelty effect [4], so that we could engage users in a
critical discussion about the robotâ€™s mistakes and repair strategies
after the study, which would not be influenced by the novelty of
a robotic well-being coach. Users were also screened for anxiety
(GAD-7 questionnaire) [ 57] and depression (PHQ-9 questionnaire)
[31]. We chose this screening in order to not use a robotic coach
with a clinical population, which we do not consider ethical prior
to thorough examination with a non-clinical population. The users
were split into two groups ( ğ‘›=6per condition), one group experi-
encing the empathic repair strategy condition, and the other the
non-empathic repair strategy condition. 1 user was aged 18â€“25, 4
were aged 26â€“35, 3 were aged 36â€“45, and 4 were aged 46â€“55. 3 users
were female, 1 non-binary, and 8 male. Minority genders (femaleand non-binary) were balanced across conditions, with 2 in each
condition. Users rated their previous experience with social robots
as(ğ‘€=3,ğ‘†ğ·=1.044)on a scale from 1 (lowest) to 5 (highest).
All users were native or fluent speakers of English, and all had an
Undergraduate, Masterâ€™s, or PhD degree. The usersâ€™ demographics
reflect the demographics of the host company Cambridge Consul-
tants Inc., and as such have the distribution of the ages, genders,
and degree statuses as described.
5.3 Robot Platform and Architecture
We used the QTrobot by LuxAI S.p.A.1â€”a 90 cm tall, tabletop child-
like robot with static legs, 4 degrees of freedom (DOF) arms, 2 DOF
neck, and a screen face. We chose this robot as it has been previously
used successfully as a robotic coach [ 3,48]. The fully autonomous
robotic coach was implemented using our newly developed VITA
system [ 49], a multi-modal LLM-based system for longitudinal and
adaptive robotic mental well-being coaching, that is open source
leveraging on the HARMONI framework [51].
5.4 Exercises
The robotic coach administered a different Positive Psychology exer-
cise each week, one per week. The robotic coach delivered the same
four exercises used during Discussion 2 described in Section 4. Each
exercise consisted of the robot asking for two different examples
from the users, and two follow-up questions per example. Follow-
up questions were generated by sending the userâ€™s utterance in
response to the robotâ€™s questions to ChatGPT gpt-3.5-turbo model
via OpenAI APIs2. We used ChatGPT to generate follow-up ques-
tions, in order to minimize the impact of a pre-scripted robot not
responding appropriately, which was a mistake the coach identified
from the robotic coach videos in Phase 1 (Sec. 4).
5.5 Administered Robot Mistakes and Repairs
Each week, the robot was pre-programmed to make two mistakes,
each during one of its utterance turns during the interaction. While
the conversational flow itself was automated (as described in Sec.
5.4), this conversational flow was interrupted at pre-determined
times to administer the mistake and the repair. The timing and
type of mistake was counterbalanced across the sessions to avoid
repetitiveness (each timing and type is listed in Tables 1, 2 and 3
in the Supplementary Material). These mistakes were either inter-
rupting the user (3-7 seconds into the userâ€™s speaking turn), or not
responding to the user for a longer period of time (12-18 seconds).
These mistakes were chosen since they were the most common mis-
takes in robotic well-being coaching, as identified from watching
the robotic coaching videos with the professional coach in Phase 1
(Sec. 4). The robot made these mistakes at different stages (either
after initially explaining the exercise, or when asking the follow-up
questions). The mistakes were distributed as follows: (Session 1)
interrupting and not responding, (Session 2) interrupting twice,
(Session 3) not responding twice, (Session 4) not responding and
interrupting.
1https://luxai.com/
2https://platform.openai.com/â€œOh, Sorry, I Think I Interrupted Youâ€: Designing Repair Strategies for Robotic Longitudinal Well-being Coaching
5.6 Study Conditions
Our two study conditions for the between-subjects study were the
robot administering either empathic repair strategies, or non-
empathic repair strategies. The repair strategies were deployed
whenever the robot made a pre-planned mistake, and were con-
structed together with the professional well-being coach.
The basic structure of both conditions was defined together with
the professional well-being coach, to include an apology, a technical
explanation for why the error occurred (realistic explanations for
why each type of mistake typically occurs in HRIâ€”e.g., microphone
or Wi-Fi malfunction [ 50]), and intent to improve (to reassure the
user). In the empathic condition, the robot would also ask the user
about the emotion they were experiencing due to the mistake, cog-
nitively empathize with the emotion [ 5] (i.e. repeating the emotion
of the user to acknowledge it), and affectively reassure them [ 39]
(aiming to reduce worry and to reassure the user that the robot
is attempting to listen to them). To cognitively empathize and af-
fectively reassure the user, the userâ€™s utterance in response to the
robotâ€™s question about how they were feeling was sent to ChatGPT
gpt-3.5-turbo model via OpenAI APIs, and ChatGPT was prompted
to return the utteranceâ€™s emotional valence (positive, neutral, or
negative), and the specific emotion (repeated back to the user
by the robot when ChatGPT returned the utteranceâ€™s emotion as
negative, to acknowledge it).
5.6.1 Repair Strategy Construction. The repair strategies and exam-
ples of each were constructed (with differences between conditions
italicized) with the following structure. The specific wording of
each repair strategy was different, and was based on the profes-
sional coachâ€™s phrasing. A full list of all repair strategies is made
available in Supplementary Material (Sec. 2).
Non-empathic: Apology, explain technical error, intent to improve,
ask for repetition of userâ€™s previous utterance
Example: â€œOh, sorry, I think I interrupted you. My microphone
isnâ€™t working well today. Iâ€™m trying to do better. Could you repeat
what you were saying before I interrupted you?â€
Empathic: Apology, ask for user emotion ,cognitively empathize with
user emotion [5],affective reassurance [39], explain technical error,
intent to improve, ask for repetition of userâ€™s previous utterance.
Example: â€œOh, sorry, I think I interrupted you. How did me inter-
rupting you make you feel?â€
â†’Negative user feeling description:
â€œIâ€™m sorry, I understand it can make you feel <feeling [e.g., awk-
ward]> when I make mistakes. My intention is to listen to what you
are saying, but sometimes I experience errors. My microphone isnâ€™t
working well today. Iâ€™m trying to do better. Could you repeat what
you were saying before I interrupted you?â€
â†’Positive or neutral user feeling description:
â€œThanks for being understanding. My intention is to listen to what
you are saying, but sometimes I experience errors. My microphone
isnâ€™t working well today. Iâ€™m trying to do better. Could you repeat
what you were saying before I interrupted you?â€Questionnaire Measure Empathic Non-Empathic Average
WAI-SR [37] Goal-subscale â†‘ M=7.500, SD =1.225 (ğ‘€=5.667,ğ‘†ğ·=1.506) (ğ‘€=6.583,ğ‘†ğ·=1.621)
WAI-SR [37] Task-subscale â†‘ M=14.000, SD =2.098 (ğ‘€=8.167,ğ‘†ğ·=2.714) (ğ‘€=11.083,ğ‘†ğ·=3.825)
WAI-SR [37] Bond-subscale â†‘ M=31.333, SD =4.761 (ğ‘€=21.333,ğ‘†ğ·=7.340) (ğ‘€=26.333,ğ‘†ğ·=7.878)
WAI-SR [37] Alliance total â†‘ M=31.333, SD =4.761 (ğ‘€=21.333,ğ‘†ğ·=7.340) (ğ‘€=26.333,ğ‘†ğ·=7.878)
SUS [19] Usability total â†‘ M=3.500, SD =1.517 (ğ‘€=2.500,ğ‘†ğ·=0.837) (ğ‘€=3.000,ğ‘†ğ·=1.280)
(C) - Understand What I said â†‘ M=3.500, SD =1.517 (ğ‘€=2.500,ğ‘†ğ·=0.837) (ğ‘€=3.000,ğ‘†ğ·=1.280)
(C) - Understand How I felt â†‘ M=2.500, SD =1.643 (ğ‘€=1.500,ğ‘†ğ·=0.837) (ğ‘€=2.000,ğ‘†ğ·=1.349)
(C) - Understand Adapted â†‘ M=3.500, SD =1.055 (ğ‘€=3.000,ğ‘†ğ·=0.894) (ğ‘€=3.25,ğ‘†ğ·=1.055)
(C) - Mistakes Made mistakes â†‘ M=4.8.333, SD =0.408 (ğ‘€=4.500,ğ‘†ğ·=0.837) (ğ‘€=4.667,ğ‘†ğ·=0.651)
(C) - Mistakes I understood why â†‘ M=4.167, SD =1.170 (ğ‘€=3.667,ğ‘†ğ·=1.033) (ğ‘€=3.917,ğ‘†ğ·=1.084)
(C) - Mistakes Irritation â†“ ( ğ‘€=3.167,ğ‘†ğ·=1.329)M=4.167, SD =1.169 (ğ‘€=3.667,ğ‘†ğ·=1.303)
(C) - Mistakes Disruption â†“ ( ğ‘€=3.667,ğ‘†ğ·=1.506)M=4.500, SD =0.837 (ğ‘€=4.083,ğ‘†ğ·=1.240)
(C) - Mistakes Repaired â€“ M=3.167, SD =1.412 M =3.167, SD =0.983 M =3.167, SD =1.193
(C) - Repairs Appropriate â†‘ M=3.333, SD =1.211 (ğ‘€=3.000,ğ‘†ğ·=1.095) (ğ‘€=3.167,ğ‘†ğ·=1.115)
(C) - Repairs Appropriate amount â†‘M=3.333, SD =1.033 (ğ‘€=2.833,ğ‘†ğ·=0.753) (ğ‘€=3.083,ğ‘†ğ·=0.900)
(C) - Repairs Right time â†‘ M=2.833, SD =1.170 (ğ‘€=2.500,ğ‘†ğ·=1.049) (ğ‘€=2.667,ğ‘†ğ·=1.073)
(C) - Repairs Empathic â†‘ M=3.667, SD =1.366 (ğ‘€=2.833,ğ‘†ğ·=1.330) (ğ‘€=3.250,ğ‘†ğ·=1.357)
Table 1: Post-study quantitative measures (higher measures
bolded). (C) denotes a custom question. Arrows illustrate
where positive changes occurred in the empathic condition.
5.7 User Study Findings
5.7.1 Data Analysis. Due to the sample size for a between-subjects
study, we use descriptive statistics to describe the quantitative differ-
ences between user groupsâ€™ perceptions of the empathic and non-
empathic repair strategies, as well as the longitudinal perception
of the repair strategies. For qualitative analysis, we use Framework
Analysis [ 52], consisting of the steps of: (1) familiarization with
the data, (2) identifying a thematic framework, (3) indexing, (4)
charting the data, and (5) interpretation of the data.
5.7.2 Quantitative Results. We present these results to contextu-
alize our qualitative results, as well as the user design workshop
(Sec. 6) and coach feedback (Sec. 7) findings. As described in Sec.
5.1, we administered questionnaires after each session, and after
the four sessions of the study. We also measured usersâ€™ well-being
with Ryffâ€™s well-being questionnaire [ 42] pre- and post-study (min.:
18, max.: 108). The median of well-being was Empathic: ( (ğ‘ğ‘Ÿğ‘’=
100,ğ‘ğ‘œğ‘ ğ‘¡ =105)); Non-empathic: ( (ğ‘ğ‘Ÿğ‘’=90.5,ğ‘ğ‘œğ‘ ğ‘¡ =92.5)). These
results confirmed our expectations of no significant impact on well-
being, due the negative impact of the planned mistakes on the
coaching experience, as well as the screened user group with high
levels of well-being to begin with (see Sec. 5.2).
We present post-study quantitative measures in the Table 1. As
post-study measures, we administered the WAI-SR (Working Al-
liance Inventory Short) questionnaire [ 37] to measure user alliance
with the robotic coach, and the SUS (System Usability Scale) ques-
tionnaire [ 19] in order to examine usersâ€™ perceptions of the robotic
coach between conditions. Additionally, we administered custom
questions on a Likert scale from 1 (lowest) to 5 (highest), about
the robotâ€™s understanding of what the user said,felt, and how it
adapted to them ; the robotâ€™s mistakes in terms of whether it made
mistakes , whether the users understood why it made mistakes, were
irritated by the mistakes, the session was disrupted by the mistakes,
and whether the robot repaired the mistakes; and on the repairs in
terms of how appropriate they were, how appropriate the amount
of repairs was, whether the repairs were administered at the right
time, and whether the repairs were empathic . The data indicates
that the robot was perceived slightly more positively in the em-
pathic condition for alliance, usability, understanding, and success
of repairs than in the non-empathic condition. For mistakes, theMinja Axelsson, Micol Spitale, and Hatice Gunes
Empathic
Non-empathicPANAS (Mood) Median value
Session100
90
80
70
60
50
40
30
201       2          3     4
RoPE (Empathy) Median value
SessionEmpathic
Non-empathic100
90
80
70
60
50
40
30
201       2          3     4
RoSAS (Social attributes) 
SessionMedian valueEmpathic
Non-empathic90
80
70
60
50
40
30
20
1       2          3     4
MDMT (Trust) 
Empathic
Non-empathicMedian value
Session140
120
100
80
60
40
20
01       2          3    4
Figure 2: Longitudinal trends of quantitative measures in
usersâ€™ experiences of empathic andnon-empathic repairs.
robot was also perceived more positively, with users better under-
standing why the robot made mistakes, and feeling less disrupted
and irritated.
We find the longitudinal quantitative data from week-by-week
measures to be a useful indicator of usersâ€™ experience. We calcu-
lated the median for each measure (reported in Sec. 5.1) for each
week, within each condition group and across all users. Our data
show decreasing trends across all users and within both conditions
throughout the weeks (Fig. 2) for trust in the robot (MDMT), robotâ€™s
perceived empathy (RoPE), and a similar trend for the empathic con-
dition for the robotâ€™s social attributes (RoSAS). These data indicate
that over time, the usersâ€™ experience with the robot was decreasing
in quality. The qualitative data in the next section (Sec. 5.7.3) helps
understand why this may be the case. Mood (PANAS) measures
remained relatively stable across the weeks, with the non-empathic
group experiencing lower mood throughout. Together with the
other measures, this may indicate a more negative experience with
the robot in the non-empathic condition.
5.7.3 Qualitative Results. We analysed our qualitative results with
Framework Analysis [ 52], to examine usersâ€™ longitudinal experi-
ences, and the two condition groups (empathic and non-empathic).
We present quotes related to these themes in the Supplementary
Material (Sec. 3). For instance, some users called the empathic strate-
gies of the robot â€œcaringâ€ (P20) and that it â€œadapted to the sentimentâ€
they were feeling (P01), but that the empathy became less genuine
over time (P01) and was â€œa bit over the topâ€ (P05). Some users stated
a preference for not needing empathy in a robot: â€œit felt too much
like a machine and empathy doesnâ€™t applyâ€ (P02).
In terms of longitudinal experiences, users noted that in the ini-
tial weeks, they were trusting of the robot recognizing its mistakes,
and of its repairs. However, this trust decreased throughout the
weeks, as shown by quantitative (Fig. 2). For instance, explanations
were viewed as helpful to understand the robot was not â€œbrokenâ€
(P09), however users noted that the robotâ€™s explanations seemed less
genuine towards the end of the study and felt like â€œexcusesâ€ (P11).
In fact, some users noted that repairs â€œmade [the interaction] worseâ€
(P04), disrupting the interaction. The repetitiveness of the repairswas also viewed as disruptive, and that they were not â€œmeeting my
needs, as in not understandingâ€ (P06).
The challenges for repair strategy design found by this analysis
are 1) users become less receptive to the repair strategies over time,
2) usersâ€™ personal preferences for repairs (especially empathic vs
non-empathic repairs), and 3) repairs are sometimes helpful but
sometimes disruptive. Users had extensive feedback on how repair
strategies could be improved, and when and how often to apply
them. To investigate this further, we organized a design workshop
with the users as the next phase of the study.
6 PHASE 3: WORKSHOP WITH USERS
We conducted a design workshop with users from the study in
Phase 2 (ğ‘›=10, with P04 and P07 not available to attend), in order
to gather their opinions on how a robotic well-being coach should
deploy repair strategies in different scenarios during coaching. We
discussed the challenges identified in Phase 2 (Sec. 5), namely the
negative perception of repair strategies over time, usersâ€™ personal
preferences, and when repairs are helpful and when disruptive.
The workshop was conducted in an online video call, with a Miro
board for collaboration3. The workshop was structured into five
sections: (1) showing users videos of both repair strategy conditions
(empathic and non-empathic) to inform them of the robotâ€™s capabil-
ities, (2) warm-up exercise, (3) thinking through appropriate repairs
for pre-defined scenarios, and generating mistake scenarios that
each user experienced, (4) thinking through appropriate repairs
for those user-generated scenarios, and (5) distilling insights for
robotic well-being coach repairs for different contexts.
6.1 Repair Strategy Preferences for
User-generated Mistake Scenarios
We have included the 12 user-generated mistake scenarios in the
Supplementary Material (Sec. 4). In addition to the robotâ€™s pre-
programmed mistakes of interrupting and non-responding, users
generated other scenarios (referred to as â€œSc.â€ throughout this pa-
per) of robot mistakes, e.g. the robot misunderstanding them or
asking generic questions. Users could select multiple options for
repair strategies for their scenarios, from 5 categories (including the
categories â€œempathicâ€ and â€œtechnical explanationâ€ which they expe-
rienced in the study, and â€œinstructions on useâ€, â€œbrief apologyâ€, and
â€œdo nothingâ€, which they suggested in the post-study interviews)
and also add another suggestion in the â€œotherâ€ section. Overall, for
the 12 user-generated scenarios, user responses from the categories
tallied up as follows: do nothing (32), empathic (15), brief apology
(48), technical explanation (6), and instructions on use (10).
Do nothing or apologize briefly â€“ Users show an overall pref-
erence for the robot either doing nothing or giving a brief apology.
Reasons for these included â€œFor a small error just continue with the
sessionâ€ (P06, Sc. 4), â€œBetter to move onâ€ (P01, Sc. 4), â€œNot worth
dwelling over, but [use] quick apology for throwing the user off.â€
(P08, Sc. 5). Users explained that longer repairs may in some cases
distract from the session, and the robot should â€œcontinue momen-
tumâ€ (P01, Sc. 4) and â€œtry and get [the] conversation back on trackâ€
(P05, Sc. 5).
3https://miro.com/â€œOh, Sorry, I Think I Interrupted Youâ€: Designing Repair Strategies for Robotic Longitudinal Well-being Coaching
Empathic â€“ There were no scenarios where most users wanted
empathic repairs. In scenarios where users did want them (2-3
users), the reasons were e.g. â€œBeing empathetic, efficient and give
advice on improving user experience seems useful. â€ (P11, Sc. 2), and
â€œAny empathetic apology should be to validate user feelings and
be quick [..] Providing advice on how to avoid this error would
be useful again to continue conversation along.â€ (P11, Sc. 7). One
user noted that a repair â€œShould be an empathic apology but not
necessarily asking how user felt.â€ (P01, Sc.10), wanting an empathic
repair but in a brief format.
Technical explanation and Instructions on use â€“ Users pre-
ferred instructions over explanations. Reasons for instructions were
e.g. â€œI would like to know how to get robot to repeat question in
the future if it is my fault.â€ (P11, Sc. 9), â€œIf the robot canâ€™t adapt
to the user can the users adapt to the robotâ€ (P06, Sc. 10). Users
wanted instructions to interact with the robot when they could
be able to correct the error. In contrast, explanations should be
used e.g. â€œIf the reason for not understanding is known, try and
point it out to the user so they can address it.â€ (P05, Sc. 10) and
â€œGiven itâ€™s a multiple repeat Iâ€™d want to know that it has realised
itâ€™s mistake and by giving an explanation I would assume it can
actually move on.â€ (P03, Sc. 11). In these cases, the user wanted
the robot to inform them of the cause of the error to increase the
transparency of whether the robot is aware of the error and the
reason for it, as well as whether the user should further address
it (outside of the session). Technical explanations and instructions
could be used in tandem, e.g., â€œRobot should explain the technical
error, perhaps it did not hear properly and microphone should be
moved closer.â€ (P09, Sc. 7).
6.2 User Insights and Discussion
While the previous sections show that users had significant personal
preferences , we conducted a final discussion at the end of the design
workshop to shape some more general insights for robotic coach
repair strategies. These insights are detailed here.
To repair or not to repair? â€“ In general, users wanted the
robot to repair when the mistake was more disruptive to
the session than deploying a repair would be. Users noted the
robot should not repair minor mistakes, because paying attention
to the mistake could cause further disruption to the session. Minor
mistakes were e.g., â€œif it interrupts the user momentarilyâ€ (P11), â€œit
interrupts the user and the user keeps talkingâ€ (P08), â€œif itâ€™s likely a
one-off issue that will not impact furtherâ€ (P03). In these cases, the
users preferred the robot to either do nothing orgive a brief apology .
The robot should also not repair if it disrupts the session: â€œif the
user is in the flow and it would be detrimental to the session to
interrupt the userâ€ (P10), and â€œthe repair would be more interrupting
/ distracting than what is trying to be repairedâ€. Users noted that the
repairs would need to be selected to be less disruptive (i.e., in the
case of a minor mistake, a more detailed repair than a brief apology
might be disruptive). Scenarios where the robot should definitely
repair were â€œwhen the user needs to participate in the fixâ€ (P03),
or â€œif the robotâ€™s response depends on the userâ€ (P11). In terms of
longitudinally administering repairs, users noted that initially the
robot should focus on introducing its main functionalities. In the
next few sessions, it is important to administer repairs to introducethe repair capability to the user and improve their experience (P01),
and to give the user any necessary instructions to resolve issues
(P10). Users noted that over time they would expect less repairs,
since the robot is not a â€œstrangerâ€ anymore (P01), and that it â€œcan
cut through and carry onâ€ (P10).
When to use empathic repairs? â€“ In general, users wanted
empathic repairs when they were likely to feel frustrated .
This could be the case in â€œrepeated mistakes, especially when I have
been talking for a while alreadyâ€ (P05), â€œwhen it interrupts during
a story/detailed explanation of an experienceâ€ (P03), and â€œwhen the
error has affected the session in a way that could have negatively
impacted the flowâ€ (P10). However, one user noted that empathic
repairs could make the user â€œfeel more frustratedâ€ (P01). This indi-
cates that empathic repairs should be dynamically deployed based
on the userâ€™s response to the repair itself.
When to give technical explanations? â€“ In general, expla-
nations were wanted to increase transparency in the case of
severe technical errors. For example, â€œwhen the error has occurred
for the first time overall for rare errorsâ€ (P12), and â€œwhen an error
has occurred that is outside the userâ€™s controlâ€ (P10). However, the
explanation should be â€œsomething the user can understandâ€ (P03).
When to give instructions on use? â€“ In general, users wanted
instructions from the robot when the mistake was something
that the user can help correct, during the first few interaction
sessions with the robot. Examples were â€œwhen the user can actually
make a simple fix (like move the microphone closer)â€ (P03) and â€œif
the error has to do with the userâ€™s way of communicatingâ€ (P11).
7 PHASE 4: COACH REFLECTIONS
To reflect on the insights provided by the robotic coach users, we
took these insights back to the professional well-being coach who
had collaborated with us to design the robotic coach and its repair
strategies in Phase 1 (Sec. 4). In this discussion (1 hour), we asked
the coach to reflect on the results of our user study and the collected
user insights, and to give their opinions on them, as follows.
Robotic coach repair strategy design can not be solely based
on professional coach repair strategies â€” We can not con-
clude simply that â€œempathic repair strategies are better than non-
empathic onesâ€, as was the original expectation of the authors and
the well-being coach. Instead, our findings indicate that a robotic
well-being coach is fundamentally different in its capabilities when
compared to a human coach, and as such usersâ€™ expectations are dif-
ferent . The professional coach noted that repeated social mistakes
(interrupting or non-responding) in human coaching are rare, as hu-
mans have the capacity to better intuit social signals and thus make
fewer mistakes. As coachees do not expect many mistakes in human
coaching, empathic repairs may be helpful in the rare situations
where mistakes do occur. However, due to limited robot capabilities,
social mistakes occur often in robotic coaching (as shown by the
videos in Sec. 4 [ 50]). Due to repeated mistakes, while the coachâ€™s
and researchersâ€™ intuition was that empathic repairs would be simi-
larly applicable to a robotic coach, the user study (Sec. 5) and design
workshop with users (Sec. 6) contradicted this intuition. Some users
enjoyed empathic repairs in the first few interaction sessions, but
became averse when repeated in later interactions, viewing them
as disingenuous. Other users experienced the empathic repairs asMinja Axelsson, Micol Spitale, and Hatice Gunes
disingenuous already from initial interactions, due to viewing the
robot as a tool that empathic expressions are incongruous with. As
such, repair design needs to acknowledge the limited capabilities
and higher error rate of robotic coaches.
Repair strategies are not always necessary â€“ In some instances
users perceived the repair strategies as disruptive (see Sec. 5-6). The
professional coach agreed that repairs could be detrimental if they
were â€œlong-winded explanationsâ€ that may be perceived as excuses
rather than helpful. The coach agreed that users may â€œget used to a
robot and its errorsâ€, and due to this, repairs could be reduced over
time in the case of repetitive errors. Future research is needed in
to distinguish whether and when a user no longer requires repair
for a specific error, e.g. by detecting a userâ€™s behavioral signals.
Repairs should be utilized according to user preference â€“
There is no â€œone-size-fits-allâ€ with regards to non-/empathic (or
other) repairs in robotic well-being coaching. Some users appreci-
ated the empathic repairs and viewed them as increasing trust and
improving the interaction, while some viewed them as disingenuous
and even deceptive due to the fundamentally non-empathic nature
of a robot as a machine. The professional coach noted that at times
they adjust their level of empathic expression to their client, by
matching their expression to that observed from the client. Previous
research proposed the adaptation of a robotâ€™s empathic expressions
to a userâ€™s positive emotional signals [ 32].Future research is
needed to analyze user feedback during the administration of the
repair strategy itself (i.e., observing a userâ€™s response to a repair
via behavioral signals), and accordingly adjusting future repairs.
Robot-specific repair strategies include technical explana-
tions and instructions on use â€“ The professional coach noted
that technical error explanation is a robot specific behaviour, and
there is no direct comparison to human-to-human coaching, but
that if they were making a repeated error, they would want to
give an explanation to their client. Future research is needed to
develop robot awareness of mistake occurrence, cause, and to gen-
erate authentic technical explanations. In terms of instructions on
use, the coach compared this to the situation where their client
may have hearing loss, and the coach might in turn increase their
speech volume. Future research should focus on how robots may
detect and adapt to such personal requirements of each user, while
applying appropriate personalised instructions on use of the robot.
Repetitions of repair strategies are detrimental â€“ Due to so-
cial interaction timing issues (i.e., mistakes such as interruptions
and non-responding) that are inherent to social robots [ 22,33,50],
robotic coaching can have an awkward rhythm. When repair strate-
gies are continuously administered to repair these issues (even
with different phrasings across sessions, as we have done in our
user study), the effect can be detrimental to the user. The coach
compared this to a â€œphone helplineâ€, where the phrase â€œyour call is
important to usâ€ is often repeated, but â€œbecomes less true over timeâ€.
The repair strategies we have proposed here assume the current
level of disruption in state-of-the-art HRI, and they may not hold
as robot capabilities further develop. Future research should aim
to reduce such latency in robotic conversational interaction, so that
repair strategies will gradually become less needed.8 CONCLUSIONS WITH A CRITICAL LOOK
This paper contributes insights for designing repair strategies for
longitudinal robotic well-being coaching, informed by real-world
usersâ€™ and a professional coachâ€™s perspectives. We have shown how
our 4-phase design process and its outcomes can contribute toward
the real-world deployment of longitudinal robotic coaches that are
capable of repairing their mistakes and thus improving coaching
interactions. As part of this process, we designed our initial study
to compare empathic andnon-empathic repairs, based on two
discussions with a professional well-being coach (Sec. 4). However,
in our between-subjects study where a robotic coach administered
these repair strategies (Sec. 5), we found that users had detailed feed-
back beyond the question of whether empathic or non-empathic
repair strategies were better. We then designed a workshop with
users to give detailed feedback on repairs for a robotic coach (Sec.
6), and reflecting on these user insights with the professional coach
from Phase 1 (Sec. 7). We encourage researchers to include such
retrospective discussions with users and stakeholders in their re-
search, especially when their intuitions are not confirmed, to better
understand how robots are experienced in the real world.
We would also like to direct a critical eye toward our own study.
In this paper, we conducted a user study in which the robot made
intentional mistakes and repairs. However, intentional mistakes rely
on timing, which is difficult for social robots . Additionally, the timing
of a repair can have an impact on its success [ 41]. Thoroughly inves-
tigating the timing of the mistakes and repairs, and how this may
have impacted user perceptions is out of scope of this work. We will
investigate this in future research , and encourage the HRI field
to reflect on how intentional social mistakes can be investigated
taking into account the challenge of timing in interactions. Also, we
used a LLM (ChatGPT) to generate the robotâ€™s responses to users.
In some cases, the LLM spontaneously asked users for clarification
when it did not â€œunderstandâ€ the userâ€™s utterance. In this paper, we
asked users to recall the mistakes and repairs they experienced dur-
ing the interaction (including those spontaneously deployed by the
LLM), however, further analysis on spontaneous repairs and usersâ€™
perceptions of them is out of scope of this work. We invite future
research to investigate the impact of spontaneous LLM repairs on
usersâ€™ perceptions of robotic coaches. These repair strategies have
been designed specifically for well-being coaching . In previous HRI
literature on game- and task-based scenarios, repair strategies focus
on a robotâ€™s mistakes where a right vs wrong condition is clear due
to a set goal [ 14,16,28â€“30,45]. In conversational contexts such
as coaching, identifying the mistake itself, and consequently the
necessity for and appropriate type of repair, is more complex due to
factors such as user preferences. Despite this complexity, we have
attempted to distil relevant insights and reflections on such repair
strategies. Future research should further investigate longitudinal
coaching interactions, administering repair strategies according to
our insights, and further refine these in an iterative manner.
ACKNOWLEDGMENTS
We thank Cambridge Consultants Inc. and their employees for participating in this
study. Funding: M. Axelsson is funded by the Osk. Huttunen foundation and the
EPSRC under grant EP/T517847/1. M. Spitale and H. Gunes have been funded by the
EPSRC/UKRI under grant ref. EP/R030782/1 (ARoEQ). Open Access: For open access
purposes, the authors have applied a Creative Commons Attribution (CC BY) licence
to any Author Accepted Manuscript version arising. Data access: Raw data related to
this publication cannot be openly released due to anonymity and privacy issues.â€œOh, Sorry, I Think I Interrupted Youâ€: Designing Repair Strategies for Robotic Longitudinal Well-being Coaching
REFERENCES
[1]Minja Axelsson, Nikhil Churamani, Atahan Caldir, and Hatice Gunes. 2022.
Participant Perceptions of a Robotic Coach Conducting Positive Psychology
Exercises: A Systematic Analysis. arXiv preprint arXiv:2209.03827 (2022).
[2] Minja Axelsson, Micol Spitale, and Hatice Gunes. 2023. Adaptive robotic mental
well-being coaches. In Companion of the 2023 ACM/IEEE International Conference
on Human-Robot Interaction . 733â€“735.
[3]Minja Axelsson, Micol Spitale, and Hatice Gunes. 2023. Robotic Coaches De-
livering Group Mindfulness Practice at a Public Cafe. In Companion of the 2023
ACM/IEEE International Conference on Human-Robot Interaction . 86â€“90.
[4]Tony Belpaeme. 2020. Advice to new human-robot interaction researchers.
Human-robot interaction: Evaluation methods and their standardization (2020),
355â€“369.
[5] Christopher Birmingham, Ashley Perez, and Maja MatariÄ‡. 2022. Perceptions of
Cognitive and Affective Empathetic Statements by Socially Assistive Robots. In
2022 17th ACM/IEEE International Conference on Human-Robot Interaction (HRI) .
IEEE, 323â€“331.
[6] Indu P Bodala, Nikhil Churamani, and Hatice Gunes. 2021. Teleoperated robot
coaching for mindfulness training: A longitudinal study. In 2021 30th IEEE Inter-
national Conference on Robot & Human Interactive Communication (RO-MAN) .
IEEE, 939â€“944.
[7] Colleen M Carpinella, Alisa B Wyman, Michael A Perez, and Steven J Stroessner.
2017. The robotic social attributes scale (RoSAS) development and validation.
InProceedings of the 2017 ACM/IEEE International Conference on human-robot
interaction . 254â€“262.
[8] Alison Carter, Anna Blackman, Ben Hicks, Matthew Williams, and Rachel Hay.
2017. Perspectives on effective coaching by those who have been coached.
International Journal of Training and Development 21, 2 (2017), 73â€“91.
[9] Laurianne Charrier, Alisa Rieger, Alexandre Galdeano, AmÃ©lie Cordier, Mathieu
Lefort, and Salima Hassas. 2019. The rope scale: a measure of how empathic a
robot is perceived. In 2019 14th ACM/IEEE International Conference on Human-
Robot Interaction (HRI) . IEEE, 656â€“657.
[10] Nikhil Churamani, Minja Axelsson, Atahan CaldÄ±r, and Hatice Gunes. 2022.
Continual learning for affective robotics: A proof of concept for wellbeing. In 2022
10th International Conference on Affective Computing and Intelligent Interaction
Workshops and Demos (ACIIW) . IEEE, 1â€“8.
[11] Filipa Correia, Carla Guerra, Samuel Mascarenhas, Francisco S Melo, and Ana
Paiva. 2018. Exploring the impact of fault justification in human-robot trust.
InProceedings of the 17th international conference on autonomous agents and
multiagent systems . 507â€“513.
[12] Erik De Haan and Judie Gannon. 2017. The coaching relationship. The SAGE
handbook of coaching (2017), 195â€“217.
[13] Robert A Emmons and Anjali Mishra. 2011. Why gratitude enhances well-being:
What we know, what we need to know. Designing positive psychology: Taking
stock and moving forward 248 (2011), 262.
[14] Connor Esterwood and Lionel P Robert. 2021. Do you still trust me? human-
robot trust repair strategies. In 2021 30th IEEE International Conference on Robot
& Human Interactive Communication (RO-MAN) . IEEE, 183â€“188.
[15] Connor Esterwood and Lionel P Robert. 2022. A literature review of trust repair
in HRI. In 2022 31st IEEE International Conference on Robot and Human Interactive
Communication (RO-MAN) . IEEE, 1641â€“1646.
[16] Connor Esterwood and Lionel P Robert. 2023. Three Strikes and you are out!:
The impacts of multiple human-robot trust violations and repairs on robot
trustworthiness. Computers in Human Behavior (2023), 107658.
[17] Adam D Galinsky, Debra Gilin, and William W Maddux. 2011. Using both your
head and your heart: The role of perspective taking and empathy in resolving social
conflict . Vol. 13. Psychology Press New York.
[18] Tammy Gregersen, Peter D MacIntyre, Kate Hein Finegan, Kyle Talbot, and
Shelby Claman. 2014. Examining emotional intelligence within the context of
positive psychology interventions. (2014).
[19] Rebecca A Grier, Aaron Bangor, Philip Kortum, and S Camille Peres. 2013. The
system usability scale: Beyond standard usability testing. In Proceedings of the
human factors and ergonomics society annual meeting , Vol. 57. SAGE Publications
Sage CA: Los Angeles, CA, 187â€“191.
[20] Katherine Harrison, Giulia Perugia, Filipa Correia, Kavyaa Somasundaram, Sanne
van Waveren, Ana Paiva, and Amy Loutfi. 2023. The Imperfectly Relatable Robot:
An Interdisciplinary Workshop on the Role of Failure in HRI. In Companion of the
2023 ACM/IEEE International Conference on Human-Robot Interaction . 917â€“919.
[21] Vicki Hart, John Blattner, and Staci Leipsic. 2001. Coaching versus therapy: A
perspective. Consulting Psychology Journal: Practice and Research 53, 4 (2001),
229.
[22] Shanee Honig and Tal Oron-Gilad. 2018. Understanding and resolving failures
in human-robot interaction: Literature review and model development. Frontiers
in psychology 9 (2018), 861.
[23] Bhautesh Dinesh Jani, David N Blane, and Stewart W Mercer. 2012. The role of
empathy in therapy and the physician-patient relationship. Forschende Komple-
mentÃ¤rmedizin/Research in Complementary Medicine 19, 5 (2012), 252â€“257.[24] Sooyeon Jeong, Sharifa Alghowinem, Laura Aymerich-Franch, Kika Arias, Agata
Lapedriza, Rosalind Picard, Hae Won Park, and Cynthia Breazeal. 2020. A robotic
positive psychology coach to improve college studentsâ€™ wellbeing. In 2020 29th
IEEE International Conference on Robot and Human Interactive Communication
(RO-MAN) . IEEE, 187â€“194.
[25] Sooyeon Jeong, Laura Aymerich-Franch, Sharifa Alghowinem, Rosalind W Pi-
card, Cynthia L Breazeal, and Hae Won Park. 2023. A robotic companion for
psychological well-being: A long-term investigation of companionship and ther-
apeutic alliance. In Proceedings of the 2023 ACM/IEEE International Conference on
Human-Robot Interaction . 485â€“494.
[26] Sooyeon Jeong, Laura Aymerich-Franch, Kika Arias, Sharifa Alghowinem, Agata
Lapedriza, Rosalind Picard, Hae Won Park, and Cynthia Breazeal. 2023. Deploying
a robotic positive psychology coach to improve college studentsâ€™ psychological
well-being. User Modeling and User-Adapted Interaction 33, 2 (2023), 571â€“615.
[27] Carol Kauffman. 2006. Positive psychology: The science at the heart of coaching.
Evidence based coaching handbook: Putting best practices to work for your clients
219 (2006), 253.
[28] Dimosthenis Kontogiorgos, Andre Pereira, Boran Sahindal, Sanne van Waveren,
and Joakim Gustafson. 2020. Behavioural responses to robot conversational
failures. In Proceedings of the 2020 ACM/IEEE International Conference on Human-
Robot Interaction . 53â€“62.
[29] Dimosthenis Kontogiorgos, Minh Tran, Joakim Gustafson, and Mohammad So-
leymani. 2021. A systematic cross-corpus analysis of human reactions to robot
conversational failures. In Proceedings of the 2021 International Conference on
Multimodal Interaction . 112â€“120.
[30] Dimosthenis Kontogiorgos, Sanne Van Waveren, Olle Wallberg, Andre Pereira,
Iolanda Leite, and Joakim Gustafson. 2020. Embodiment effects in interactions
with failing robots. In Proceedings of the 2020 CHI conference on human factors in
computing systems . 1â€“14.
[31] Kurt Kroenke, Robert L Spitzer, and Janet BW Williams. 2001. The PHQ-9: validity
of a brief depression severity measure. Journal of general internal medicine 16, 9
(2001), 606â€“613.
[32] Iolanda Leite, Ginevra Castellano, AndrÃ© Pereira, Carlos Martinho, and Ana
Paiva. 2012. Modelling empathic behaviour in a robotic game companion for
children: an ethnographic study in real-world settings. In Proceedings of the
seventh annual ACM/IEEE international conference on Human-Robot Interaction .
367â€“374.
[33] Maitreyee and Helena Lindgren. 2021. Younger and Older Adultsâ€™ Perceptions on
Role, Behavior, Goal and Recovery Strategies for Managing Breakdown Situations
in Human-Robot Dialogues. In Proceedings of the 9th International Conference on
Human-Agent Interaction . 433â€“437.
[34] Bertram F Malle and Daniel Ullman. 2021. A multidimensional conception and
measure of human-robot trust. In Trust in human-robot interaction . Elsevier,
3â€“25.
[35] Kayla Matheus, Marnyel VÃ¡zquez, and Brian Scassellati. 2022. A social robot forÂ´
anxiety reduction via deep breathing. In 2022 31st IEEE International Conference
on Robot and Human Interactive Communication (RO-MAN) . IEEE, 89â€“94.
[36] Allan McNaught. 2011. Defining wellbeing. Understanding wellbeing: An intro-
duction for students and practitioners of health and social care (2011), 7â€“23.
[37] Thomas Munder, Fabian Wilmers, Rainer Leonhart, Hans Wolfgang Linster,
and JÃ¼rgen Barth. 2010. Working Alliance Inventory-Short Revised (WAI-SR):
psychometric properties in outpatients and inpatients. Clinical Psychology &
Psychotherapy: An International Journal of Theory & Practice 17, 3 (2010), 231â€“239.
[38] Antti Oulasvirta, Esko Kurvinen, and Tomi Kankainen. 2003. Understanding
contexts by being there: case studies in bodystorming. Personal and ubiquitous
computing 7 (2003), 125â€“134.
[39] Tamar Pincus, Nicola Holt, Steven Vogel, Martin Underwood, Richard Savage,
David Andrew Walsh, and Stephanie Jane Caroline Taylor. 2013. Cognitive and
affective reassurance and patient outcomes in primary care: a systematic review.
PainÂ®154, 11 (2013), 2407â€“2416.
[40] Babiche L Pompe, Ella Velner, and Khiet P Truong. 2022. The robot that showed
remorse: Repairing trust with a genuine apology. In 2022 31st IEEE International
Conference on Robot and Human Interactive Communication (RO-MAN) . IEEE,
260â€“265.
[41] Paul Robinette, Ayanna M Howard, and Alan R Wagner. 2015. Timing is key
for robot trust repair. In Social Robotics: 7th International Conference, ICSR 2015,
Paris, France, October 26-30, 2015, Proceedings 7 . Springer, 574â€“583.
[42] Carol D Ryff. 1989. Happiness is everything, or is it? Explorations on the meaning
of psychological well-being. Journal of personality and social psychology 57, 6
(1989), 1069.
[43] Jeremy D Safran and J Christopher Muran. 1996. The resolution of ruptures in
the therapeutic alliance. Journal of consulting and clinical psychology 64, 3 (1996),
447.
[44] Maha Salem, Gabriella Lakatos, Farshid Amirabdollahian, and Kerstin Dauten-
hahn. 2015. Would you trust a (faulty) robot? Effects of error, task type and
personality on human-robot cooperation and trust. In Proceedings of the tenth
annual ACM/IEEE international conference on human-robot interaction . 141â€“148.
[45] Sarah Strohkorb Sebo, Priyanka Krishnamurthi, and Brian Scassellati. 2019. â€œI
donâ€™t believe youâ€: Investigating the effects of robot trust violation and repair. InMinja Axelsson, Micol Spitale, and Hatice Gunes
2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI) .
IEEE, 57â€“65.
[46] Martin EP Seligman. 2007. Coaching and positive psychology. Australian Psy-
chologist 42, 4 (2007), 266â€“267.
[47] Jennifer L Smith and Agnieszka A Hanni. 2019. Effects of a savoring intervention
on resilience and well-being of older adults. Journal of Applied Gerontology 38, 1
(2019), 137â€“152.
[48] Micol Spitale, Minja Axelsson, and Hatice Gunes. 2023. Robotic mental well-being
coaches for the workplace: An in-the-wild study on form. In Proceedings of the
2023 ACM/IEEE International Conference on Human-Robot Interaction . 301â€“310.
[49] Micol Spitale, Minja Axelsson, and Hatice Gunes. 2023. VITA: A Multi-modal
LLM-based System for Longitudinal, Autonomous, and Adaptive Robotic Mental
Well-being Coaching. arXiv:2312.09740 [cs.RO]
[50] Micol Spitale, Minja Axelsson, Neval Kara, and Hatice Gunes. 2023. Longitudinal
Evolution of Coacheesâ€™ Behavioural Responses to Interaction Ruptures in Robotic
Positive Psychology Coaching. In 2023 32nd IEEE International Conference on
Robot and Human Interactive Communication (RO-MAN) . IEEE.
[51] Micol Spitale, Chris Birmingham, R Michael Swan, and Maja J MatariÄ‡. 2021.
Composing harmoni: An open-source tool for human and robot modular open
interaction. In 2021 IEEE International Conference on Robotics and Automation
(ICRA) . IEEE, 3322â€“3329.
[52] Aashish Srivastava and S Bruce Thomson. 2009. Framework analysis: a qualitative
methodology for applied policy research. (2009).[53] Dirk Van Dierendonck and Hodar Lam. 2023. Interventions to enhance eudae-
monic psychological well-being: A meta-analytic review with Ryffâ€™s Scales of
Psychological Well-being. Applied Psychology: Health and Well-Being 15, 2 (2023),
594â€“610.
[54] Sanne van Waveren, Elizabeth J Carter, and Iolanda Leite. 2019. Take one for the
team: The effects of error severity in collaborative tasks with social robots. In
Proceedings of the 19th ACM International Conference on Intelligent Virtual Agents .
151â€“158.
[55] David Watson, Lee Anna Clark, and Auke Tellegen. 1988. Development and
validation of brief measures of positive and negative affect: the PANAS scales.
Journal of personality and social psychology 54, 6 (1988), 1063.
[56] Jeanne C Watson, Patricia L Steckley, and Evelyn J McMullen. 2014. The role of
empathy in promoting change. Psychotherapy Research 24, 3 (2014), 286â€“298.
[57] Nerys Williams. 2014. The GAD-7 questionnaire. Occupational medicine 64, 3
(2014), 224â€“224.
[58] Alex M Wood, Jeffrey J Froh, and Adam WA Geraghty. 2010. Gratitude and
well-being: A review and theoretical integration. Clinical psychology review 30,
7 (2010), 890â€“905.
[59] Sean Ye, Glen Neville, Mariah Schrum, Matthew Gombolay, Sonia Chernova,
and Ayanna Howard. 2019. Human trust after robot mistakes: Study of the
effects of different forms of robot communication. In 2019 28th IEEE International
Conference on Robot and Human Interactive Communication (RO-MAN) . IEEE,
1â€“7.