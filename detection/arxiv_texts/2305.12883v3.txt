PREDICTION RISK AND ESTIMATION RISK OF THE RIDGELESS LEAST
SQUARES ESTIMATOR UNDER GENERAL ASSUMPTIONS ON REGRESSION
ERRORS
SUNGYOON LEE AND SOKBAE LEE
Abstract .In recent years, there has been a significant growth in research focusing on minimum
ℓ2norm (ridgeless) interpolation least squares estimators. However, the majority of these analyses
have been limited to an unrealistic regression error structure, assuming independent and identically
distributed errors with zero mean and common variance. In this paper, we explore prediction risk as
well as estimation risk under more general regression error assumptions, highlighting the benefits
of overparameterization in a more realistic setting that allows for clustered or serial dependence.
Notably, we establish that the estimation di fficulties associated with the variance components of both
risks can be summarized through the trace of the variance-covariance matrix of the regression errors.
Our findings suggest that the benefits of overparameterization can extend to time series, panel and
grouped data.
1. Introduction
Recent years have witnessed a fast growing body of work that analyzes minimum ℓ2norm
(ridgeless) interpolation least squares estimators (see, e.g., Bartlett et al., 2020; Hastie et al., 2022;
Tsigler and Bartlett, 2023, and references therein). Researchers in this field were inspired by
the ability of deep neural networks to accurately predict noisy training data with perfect fits, a
phenomenon known as “double descent” or “benign overfitting” (e.g., Belkin et al., 2018, 2019,
2020; Zou et al., 2021; Mei and Montanari, 2022, among many others). They discovered that to
achieve this phenomenon, overparameterization is critical.
In the setting of linear regression, we have the training data {(xi,yi)∈Rp×R:i=1,···,n},
where the outcome variable yiis generated from
yi=x⊤
i◁beta1+εi, i=1,...,n,
xiis a vector of features (or regressors), ◁beta1is a vector of unknown parameters, and εiis a regression
error. Here,nis the sample size of the training data and pis the dimension of the parameter vector
◁beta1.
In the literature, the main object for the theoretical analyses has been mainly on the out-of-sample
prediction risk. That is, for the ridge or interpolation estimator ˆ◁beta1, the literature has focused on
Eh
(x⊤
0ˆ◁beta1−x⊤
0◁beta1)2|x1,...,xni
,
Date : May 2024.
1arXiv:2305.12883v3  [math.ST]  13 Jun 20242 SUNGYOON LEE AND SOKBAE LEE
wherex0is a test observation that is identically distributed as xibut independent of the training
data. For example, Dobriban and Wager (2018); Wu and Xu (2020); Richards et al. (2021); Hastie
et al. (2022) analyzed the predictive risk of ridge(less) regression and obtained exact asymptotic
expressions under the assumption that p/n converges to some constant as both pandngo to
infinity. Overall, they found the double descent behavior of the ridgeless least squares estimator in
terms of the prediction risk. Bartlett et al. (2020); Kobak et al. (2020); Tsigler and Bartlett (2023)
characterized the phenomenon of benign overfitting in a di fferent setting.
To the best of our knowledge, a vast majority of the theoretical analyses have been confined to a
simple data generating process, namely, the observations are independent and identically distributed
(i.i.d.), and the regression errors have mean zero, have the common variance, and are independent
of the feature vectors. That is,
(yi,x⊤
i)⊤∼i.i.d. with E[εi]=0,E[ε2
i]=σ2<∞andεiis independent of xi. (1)
This assumption, although convenient, is likely to be unrealistic in various real-world examples. For
instance, Liao et al. (2023) adopted high-dimensional linear models to examine the double descent
phenomenon in economic forecasts. In their applications, the outcome variables include S&P firms’
earnings, U.S. equity premium, U.S. unemployment rate, and countries’ GDP growth rate. As in
their applications, economic forecasts are associated with time series or panel data. As a result, it is
improbable that (1) holds in these applications. As another example, Spiess et al. (2023) examined
the performance of high-dimensional synthetic control estimators with many control units. The
outcome variable in their application is the state-level smoking rates in the Abadie et al. (2010)
dataset. Considering the geographical aspects of the U.S. states, it is unlikely that the regression
errors underlying the synthetic control estimators adhere to (1). In short, it is desirable to go beyond
the simple but unrealistic regression error assumption given in (1).
To further motivate, we start with our own real-data example from American Community Survey
(ACS) 2018, extracted from IPUMS USA (Ruggles et al., 2024). The ACS is an ongoing annual
survey by the US Census Bureau that provides key information about the US population. To have
a relatively homogeneous population, the sample extract is restricted to white males residing in
California with at least a bachelor’s degree. We consider a demographic group defined by their age,
the type of degree, and the field of degree. Then, we compute the average of log hourly wages for
each age-degree-field group, treat each group average as the outcome variable, and predict group
wages by various group-level regression models where the regressors are constructed using the
indicator variables of age, degree, and field as well as their interactions. We consider 7 specifications
ranging from 209 to 2,182 regressors. To understand the role of non-i.i.d. regressor errors, we add
the artificial noise to the training sample. See Appendix A for details regarding how to generate
the artificial noise. In the experiment, the constant cvaries such that c=0corresponds to no
clustered dependence across observations but as a positive cgets larger, the noise has a larger shareRIDGELESS LEAST SQUARES ESTIMATOR 3
0 500 1000 1500 2000
p0.00.10.20.30.40.50.60.70.80.9error
train error (c=0)
train error (c=1/4)
train error (c=2/4)
train error (c=3/4)
test error (c=0)
test error (c=1/4)
test error (c=2/4)
test error (c=3/4)
Figure 1.Comparison of in-sample and out-of-sample mean squared error (MSE)
across various degrees of clustered noise. The vertical line indicates p=n(=
1,415).
of clustered errors but the variance of the overall regression errors remains the same regardless of
the value of c. Figure 1 shows the in-sample (train) vs. out-of-sample (test) mean squared error
(MSE) for various values of c∈{0,0.25,0.5,0.75}. It can be seen that the experimental results
are almost identical across di fferent values of cespecially when p>n , suggesting that the double
descent phenomenon might be universal for various degrees of clustered dependence, provided that
the overall variance of the regression errors remains the same. It is our main goal to provide a firm
foundation for this empirical phenomenon. To do so, we articulate the following research questions:
•How to analyze the out-of-sample prediction risk of the ridgeless least squares estimator
under general assumptions on the regression errors?
•Why does notthe prediction risk seem to be a ffected by the degrees of dependence across
observations?
To delve into the prediction risk, suppose that Σ:=E[x0x⊤
0] is finite and positive definite. Then,
Eh
(x⊤
0ˆ◁beta1−x⊤
0◁beta1)2|x1,...,xni
=Eh
(ˆ◁beta1−◁beta1)⊤Σ(ˆ◁beta1−◁beta1)|x1,...,xni
.
IfΣ =I(i.e., the case of isotropic features), where Iis the identity matrix, the mean squared error
of the estimator defined by E[∥ˆ◁beta1−◁beta1∥2], where∥·∥is the usual Euclidean norm, is the same as
the expectation of the prediction risk defined above. However, if Σ,I, the link between the two
quantities is less intimate. One may regard the prediction risk as the Σ-weighted mean squared error
of the estimator; whereas E[∥ˆ◁beta1−◁beta1∥2]can be viewed as an “unweighted” version, even if Σ,I.
In other words, regardless of the variance-covariance structure of the feature vector, E[∥ˆ◁beta1−◁beta1∥2]
treats each component of ◁beta1“equally.” The mean squared error of the estimator is arguably one
of the most standard criteria to evaluate the quality of the estimator in statistics. For instance,
in the celebrated work by James and Stein (1961), the mean squared error criterion is used to4 SUNGYOON LEE AND SOKBAE LEE
show that the sample mean vector is not necessarily optimal even for standard normal vectors
(so-called “Stein’s paradox”). Many follow-up papers used the same criterion; e.g., Hansen (2016)
compared the mean-squared error of ordinary least squares, James–Stein, and Lasso estimators in
an underparameterized regime. Both Σ-weighted and unweighted versions of the mean squared
error are interesting objects to study. For example, Dobriban and Wager (2018) called the former
“predictive risk” and the latter “estimation risk” in high-dimensional linear models; Berthier et al.
(2020) called the former “generalization error” and the latter “reconstruction error” in the context of
stochastic gradient descent for the least squares problem using the noiseless linear model. In this
paper, we analyze both weighted and unweighted mean squared errors of the ridgeless estimator
under general assumptions on the data-generating processes, not to mention anisotropic features.
Furthermore, our focus is on the finite-sample analysis, that is, both pandnare fixed but p>n .
Although most of the existing papers consider the simple setting as in (1), our work is not the
first paper to consider more general regression errors in the overparameterized regime. Chinot
et al. (2022); Chinot and Lerasle (2023) analyzed minimum norm interpolation estimators as
well as regularized empirical risk minimizers in linear models without any conditions on the
regression errors. Specifically, Chinot and Lerasle (2023) showed that, with high probability, without
assumption on the regression errors, for the minimum norm interpolation estimator, (ˆ◁beta1−◁beta1)⊤Σ(ˆ◁beta1−◁beta1)
is bounded from above by ∥◁beta1∥2P
i≥c·nλi(Σ)∨Pn
i=1ε2
i/n, wherecis an absolute constant and λi(Σ)
is the eigenvalues of Σin descending order. Chinot and Lerasle (2023) also obtained the bounds on
the estimation error (ˆ◁beta1−◁beta1)⊤(ˆ◁beta1−◁beta1). Our work is distinct and complements these papers in the sense
that we allow for a general variance-covariance matrix of the regression errors. The main motivation
of not making any assumptions on εiin Chinot et al. (2022) and Chinot and Lerasle (2023) is to
allow for potentially adversarial errors. We aim to allow for a general variance-covariance matrix
of the regression errors to accommodate time series and clustered data, which are common in
applications. See, e.g., Hansen (2022) for a textbook treatment (see Chapter 14 for time series and
Section 4.21 for clustered data).
The main contribution of this paper is that we provide exact finite-sample characterization of
the variance component of the prediction and estimation risks under the assumption that X=
[x1,x2,···,xn]⊤isleft-spherical (e.g.,xi’s can be i.i.d. normal with mean zero but more general);
εi’scan be correlated and have non-identical variances ; andεi’s are independent of xi’s. Specifically,
the variance term can be factorized into a product between two terms: one term depends only on
thetrace of the variance-covariance matrix, say Ω, ofεi’s; the other term is solely determined by
the distribution of xi’s. Interestingly, we find that although Ωmay contain non-zero o ff-diagonal
elements, only the trace of Ωmatters, as hinted by Figure 1, and further demonstrate our finding
via numerical experiments. In addition, we obtain exact finite-sample expression for the bias
terms when the regression coe fficients follow the random-e ffects hypothesis (Dobriban and Wager,
2018). Our finite-sample findings o ffer a distinct viewpoint on the prediction and estimation risks,RIDGELESS LEAST SQUARES ESTIMATOR 5
contrasting with the asymptotic inverse relationship (for optimally chosen ridge estimators) between
the predictive and estimation risks uncovered by Dobriban and Wager (2018). Finally, we connect
our findings to the existing results on the prediction risk (e.g., Hastie et al., 2022) by considering
the asymptotic behavior of estimation risk.
One of the limitations of our theoretical analysis is that the design matrix Xis assumed to be
left-spherical, although it is more general than i.i.d. normal with mean zero. We not only view
this as a convenient assumption but also expect that our findings will hold at least approximately
even ifXdoes not follow the left-spherical distribution. It is a topic for future research to formally
investigate this conjecture.
2. T heFramework under General Assumptions on Regression Errors
We first describe the minimum ℓ2norm (ridgeless) interpolation least squares estimator in the
overparameterized case ( p>n ). Define
y:=[y1,y2,···,yn]⊤∈Rn,
ε:=[ε1,ε2,···,εn]⊤∈Rn,
X⊤:=h
x1,x2,···,xni
∈Rp×n,
so thaty=X◁beta1+ε. The estimator we consider is
ˆ◁beta1:=arg min
b∈Rp{∥b∥:Xb=y}=(X⊤X)†X⊤y=X†y,
whereA†denotes the Moore–Penrose inverse of a matrix A.
The main object of interest in this paper is the prediction and estimation risks of ˆ◁beta1under the
data scenario such that the regression error εimay notbe i.i.d. Formally, we make the following
assumptions.
Assumption 2.1. (i)y=X◁beta1+ε, whereεis independent of X, andE[ε]=0. (ii) Ω:=E[εε⊤]is
finite and positive definite (but not necessarily spherical).
We emphasize that Assumption 2.1 is more general than the standard assumption in the literature
on benign overfitting that typically assumes that Ω≡σ2I. Assumption 2.1 allows for non-identical
variances across the elements of εbecause the diagonal elements of Ωcan be di fferent among each
other. Furthermore, it allows for non-zero o ff-diagonal elements in Ω. It is di fficult to assume that
the regression errors are independent among each other with time series or clustered data; thus, in
these settings, it is important to allow for general Ω,σ2I. Below we present a couple of such
examples.
Example 2.1 (AR(1) Errors) .Suppose that the regressor error follows an autoregressive process:
εi=ρεi−1+ηi, (2)6 SUNGYOON LEE AND SOKBAE LEE
whereρ∈(−1,1)is an autoregressive parameter, ηiis independent and identically distributed with
mean zero and variance σ2(0<σ2<∞) and is independent of X. Then, the (i,j) element of Ωis
Ωij=σ2
1−ρ2ρ|i−j|.
Note that Ωij,0as long asρ,0.
Example 2.2 (Clustered Errors) .Suppose that regression errors are mutually independent across
clusters but they can be arbitrarily correlated within the same cluster. For instance, students in
the same school may a ffect each other and also have the same teachers; thus it would be di fficult
to assume independence across student test scores within the same school. However, it might be
reasonable that student test scores are independent across di fferent schools. For example, assume
that (i) if the regression error εibelongs to cluster g, whereg=1,...,G andGis the number of
clusters, E[ε2
i]=σ2
gfor some constant σ2
g>0that can vary over g; (ii) if the regression errors
εiandεj(i,j)belong to the same cluster g,E[εiεj]=ρgfor some constant ρg,0that can be
different acrossg; and (iii) if the regression errors εiandεj(i,j)do not belong to the same cluster,
E[εiεj]=0. Then, Ωis block diagonal with possibly non-identical blocks.
For vectoraand square matrix A, let∥a∥2
A:=a⊤Aa. Conditional on Xand givenA, we define
BiasA(ˆ◁beta1|X) :=∥E[ˆ◁beta1|X]−◁beta1∥Aand Var A(ˆ◁beta1|X) :=Tr(Cov( ˆ◁beta1|X)A),
and we write Var =VarIand Bias =BiasIfor the sake of brevity in notation.
The mean squared prediction error for an unseen test observation x0with the positive definite
covariance matrix Σ:=E[x0x⊤
0](assuming that x0is independent of the training data X) and the
mean squared estimation error of ˆ◁beta1conditional on Xcan be written as:
RP(ˆ◁beta1|X) :=E(x⊤
0ˆ◁beta1−x⊤
0◁beta1)2|X=[Bias Σ(ˆ◁beta1|X)]2+Var Σ(ˆ◁beta1|X),
RE(ˆ◁beta1|X) :=E∥ˆ◁beta1−◁beta1∥2|X=[Bias( ˆ◁beta1|X)]2+Var(ˆ◁beta1|X).
In what follows, we obtain exact finite-sample expressions for prediction and estimation risks:
RP(ˆ◁beta1) :=EX[RP(ˆ◁beta1|X)] andRE(ˆ◁beta1) :=EX[RE(ˆ◁beta1|X)].
We first analyze the variance terms for both risks and then study the bias terms.
3. T heVariance Components of Prediction and Estimation Risks
3.1. The variance component of prediction risk. We rewrite the variance component of prediction
risk as follows:
Var Σ(ˆ◁beta1|X)=Tr(Cov( ˆ◁beta1|X)Σ)=Tr(X†ΩX†⊤Σ)=∥SX†T∥2
F, (3)
where positive definite symmetric matrices S:= Σ1/2andT:= Ω1/2are the square root matrices of
the positive definite matrices ΣandΩ, respectively. To compute the above Frobenius norm of theRIDGELESS LEAST SQUARES ESTIMATOR 7
matrixSX†T, we need to compute the alignment of the right-singular vectors of B:=SX†∈Rp×n
with the left-eigenvectors of T∈Rn×n. Here,Bis a random matrix while Tis fixed. Therefore, we
need the distribution of the right-singular vectors of the random matrix B.
Perhaps surprisingly, to compute the expected variance EX[Var Σ(ˆ◁beta1|X)], it turns out that we do
not need the distribution of the singular vectors if we make a minimal assumption (the left-spherical
symmetry ofX) which is weaker than the assumption that {xi}n
i=1is i.i.d. normal with E[x1]=0.
Definition 3.1 (Left-Spherical Symmetry (Dawid, 1977, 1978, 1981; Gupta and Nagar, 1999)) .
A random matrix Zor its distribution is called to be left-spherical ifOZandZhave the same
distribution ( OZd=Z) for any fixed orthogonal matrix O∈O(n) :={A∈Rn×n:AA⊤=A⊤A=I}.
Assumption 3.1. The design matrix Xis left-spherical.
For the isotropic error case ( Ω =I), we have EX[VarΣ(ˆ◁beta1|X)]=EX[Tr((X⊤X)†Σ)]directly from
(3) sinceX†X†⊤=(X⊤X)†. Moreover, for the arbitrary error, the left-spherical symmetry of X
plays a critical role to factor out the same EX[Tr((X⊤X)†Σ)]and the trace of the variance-covariance
matrix of the regression errors, Tr( Ω), from the variance after the expectation over X.
Lemma 3.1. For a subsetS⊂Rm×msatisfyingC−1∈Sfor allC∈S, if matrix-valued random
variablesZandAZhave the same distribution measure µZfor anyA∈S, then we have
EZ[f(Z)]=EZ[f(AZ)]=EZ[EA′∼ν[f(A′Z)]]
for any function f∈L1(µZ)and any probability density function νonS.
Theorem 3.2. Let Assumptions 2.1, and 3.1 hold. Then, we have
EX[Var Σ(ˆ◁beta1|X)]=1
nTr(Ω)EX[Tr((X⊤X)†Σ)].
Sketch of Proof. WithB= Σ1/2X†andT= Ω1/2, we can rewrite the variance as follows:
Var Σ(ˆ◁beta1|X)=∥BT∥2
F=∥UDV⊤UTDTV⊤
T∥2
F=∥DV⊤UTDT∥2
F
from the singular value decompositions B=UDV⊤andT=UTDTV⊤
Twith orthogonal matrices
U,V,UT,VT, and diagonal matrices D,DT. Then, we need to compute the alignment V⊤UTof the
right-singular vectors of Bwith the left-eigenvectors of Tbecause
∥DV⊤UTDT∥2
F=λ
(X⊤X)†Σ⊤Γ(X)λ(Ω)=a(X)⊤Γ(X)b,
wherev(i):=V:i,u(j):=(UT):j,γij:=⟨v(i),u(j)⟩2≥0,Γ(X) :=(γij)i,j∈Rn×nandλ(A)∈Rnis a
vector where its elements are the eigenvalues of A.
Now, we want to compute the expected variance. To do so, from Lemma 3.1 with S=O(n)and
the left-spherical symmetry of X, we can obtain
EX[a(X)⊤Γ(X)b]=EXEO∼ν[a(OX)⊤Γ(OX)b]=EXa(X)⊤EO∼ν[Γ(OX)]b,8 SUNGYOON LEE AND SOKBAE LEE
0.00 0.25 0.50 0.75 1.00
2
0.00.10.20.30.40.50.62
Prediction Risk - AR(1) Errors
0.3 0.6 0.9 1.21.51.82.12.42.73.0
0.00 0.25 0.50 0.75 1.00
2
0.00.10.20.30.40.50.62
Estimation Risk - AR(1) Errors
0.3 0.6 0.9 1.21.51.82.12.42.73.0
Figure 2.Our theory (dashed lines) matches the expected variances (solid lines)
of the prediction (left) and estimation risks (right) in Example 2.1 (AR(1) Errors).
Each point (σ2,ρ2)represents a di fferent noise covariance matrix Ω, but with the
same Tr( Ω) along each line{(σ2,ρ2) :σ2/κ2+ρ2=1}for someκ2>0, they have
the same expected variance. We set n=50,p=100, and evaluate on 100 samples
ofXand 100 samples of ε(for each realization of X) to approximate the expectations.
whereνis the unique uniform distribution (the Haar measure) over the orthogonal matrices O(n).
Here, we can show that EO∼ν[Γ(OX)]=1
nJ, whereJis the all-ones matrix with Jij=1(i,j=
1,2,···,n). Therefore, we have the expected variance as follows:
EX[Var Σ(ˆ◁beta1|X)]=EX
a(X)⊤1
nJb
=1
nnX
i,j=1EX[ai(X)]bj=1
nEX[Tr((X⊤X)†Σ)] Tr(Ω).
□
The proofs of Lemma 3.1 and Theorem 3.2 are in the supplementary appendix.
3.2. The variance component of estimation risk. For the expected variance EX[Var(ˆ◁beta1|X)]of
the estimation risk, a similar argument still holds if plugging-in B=X†instead ofB= Σ1/2X†.
Theorem 3.3. Let Assumptions 2.1, and 3.1 hold. Then, we have
EX[Var( ˆ◁beta1|X)]=1
npTr(Ω)EX[Tr(Λ†)],
whereXX⊤/p=UΛU⊤for some orthogonal matrix U∈O(n).
3.3. Numerical experiments. In this section, we validate our theory with some numerical exper-
iments of Examples 2.1 and 2.2, especially how the expected variance is related to the general
covariance Ωof the regressor error ε. In the both examples, we sample {xi}n
i=1fromN(0,Σ)with a
general feature covariance Σ =UΣDΣU⊤
Σfor an orthogonal matrix UΣ∈O(p)and a diagonal matrix
DΣ≻0. In this setting, we have rank( XX⊤)=nandΛ†= Λ−1almost everywhere.RIDGELESS LEAST SQUARES ESTIMATOR 9
0.1 0.2 0.3 0.4 0.5
2
1
0.10.20.30.40.52
2
Prediction Risk - Clustered Errors
0.150.200.250.300.350.400.450.500.550.600.65
0.1 0.2 0.3 0.4 0.5
2
1
0.10.20.30.40.52
2
Estimation Risk - Clustered Errors
0.150.200.250.300.350.400.450.500.550.60
Figure 3.Our theory (dashed lines) matches the expected variances (solid lines) of
the prediction (left) and estimation risks (right) in Example 2.2 (Clustered Errors).
Each point (σ2,ρ2)represents a di fferent noise covariance matrix Ω, but with the
same Tr(Ω)along each line{(σ2
1,σ2
2) :n1
nσ2
1+n2
nσ2
2=κ2}for someκ2>0, they
have the same expected variance. We set G=2,(n1=5,n2=15),n=20,p=
40,ρ1=ρ2=0.05, and evaluate on 100 samples of Xand 100 samples of ε(for
each realization of X) to approximate the expectations.
3.3.1. AR(1) Errors. As shown in Example 2.1, when the regressor error follows an autoregressive
process in (2), we have Ωij=σ2ρ|i−j|/(1−ρ2)andTr(Ω)/n=σ2/(1−ρ2). Therefore, for pairs of
(σ2,ρ2)with the same Tr(Ω)/n, they are expected to yield the same variances of the prediction and
estimation risk from Theorem 3.2 and 3.3 even though they have di fferent o ff-diagonal elements in
Ω. To be specific, the pairs (σ2,ρ2)on a line{(σ2,ρ2) :σ2/κ2+ρ2=1}have the same Tr(Ω)/n
and the same expected variance which gets larger for the line with respect to a larger κ2.
Figure 2 (left) shows the contour plots of EX[Var Σ(ˆ◁beta1|X)]and1
nTr(Ω)EX[Tr((X⊤X)†Σ)]for
different pairs of (σ2,ρ2)in Example 2.1. They have di fferent slopes−κ−2according to the value of
κ2=Tr(Ω)/n. The right panel shows equivalent contour plots for estimation risk.
3.3.2. Clustered Errors. Now consider the block diagonal covariance matrix Ω = diag(Ω1,Ω2,···,ΩG)
in Example 2.2, where Ωgis anng×ngmatrix with (Ωg)ii=σ2
gand(Ωg)ij=ρg(i,j) for
eachi,j=1,2,···,ngandg=1,2,···,G. Letn=PG
g=1ng. We then have Tr(Ω)/n=PG
g=1Tr(Ωg)/n=PG
g=1(ng/n)σ2
g. Therefore, given a partition {ng}G
g=1of thenobservations, the
covariance matrices Ωwith di fferent{σ2
g}G
g=1have the same Tr(Ω)/nif(σ2
1,σ2
2,···,σ2
G)∈RGare
on the same hyperplanen1
nσ2
1+n2
nσ2
2+···+nG
nσ2
G=κ2for someκ2>0.
Figure 3 (left) shows the contour plots of EX[Var Σ(ˆ◁beta1|X)]and1
nTr(Ω)EX[Tr((X⊤X)†Σ)]for
different pairs of (σ2
1,σ2
2)for a simple two-clusters example ( G=2) of Example 2.2 with (n1,n2)=
(5,15). Here, we use a fixed value of ρ1=ρ2=0.05, but the results are the same regardless of
their values, as shown in the appendix. Unlike Example 2.1, the hyperplanes are orthogonal to
v=[n1,n2]⊤regardless of the value of κ2=Tr(Ω)/n. Again, the right panel shows equivalent
contour plots for estimation risk.10 SUNGYOON LEE AND SOKBAE LEE
4. T heBiasComponents of Prediction and Estimation Risks
Our main contribution is to allow for general assumptions on the regression errors, and thus
the bias parts remain the same as they do not change with respect to the regression errors. For
completeness, in this section, we briefly summarize the results on the bias components. First, we
make the following assumption for a constant rank deficiency of X⊤Xwhich holds, for example,
eachxihas a positive definite covariance matrix and is independent of each other.
Assumption 4.1. rank(X)=nalmost everywhere.
4.1. The bias component of prediction risk. The bias term of prediction risk can be expressed as
follows:
[Bias Σ(ˆ◁beta1|X)]2=(S◁beta1)⊤lim
λ↘0λ2(S−1ˆΣS+λI)−2S◁beta1, (4)
where ˆΣ:=X⊤X/n. Now, in order to obtain an exact closed form solution, we make the following
assumption:
Assumption 4.2. E◁beta1[S◁beta1(S◁beta1)⊤]=r2
ΣI/p, wherer2
Σ:=E◁beta1[∥◁beta1∥2
Σ]<∞and◁beta1is independent of X.
A similar assumption (see Assumption 4.3) has been shown to be useful to obtain closed-form
expressions in the literature (e.g., Dobriban and Wager, 2018; Richards et al., 2021; Li et al., 2021;
Chen et al., 2023).
Under this assumption, since [Bias Σ(ˆ◁beta1|X)]2=Tr[S◁beta1(S◁beta1)⊤limλ↘0λ2(S−1ˆΣS+λI)−2]from (4),
we have the expected bias (conditional on X) as follows:
E◁beta1[Bias Σ(ˆ◁beta1|X)2|X]=r2
Σ
plim
λ↘0pX
i=1λ2
(˜si+λ)2=r2
Σ
p|{i∈[p] : ˜si=0}|=r2
Σp−n
p,
where ˜siare the eigenvalues of S−1ˆΣS∈Rp×pand rank(S−1ˆΣS)=rank(X)=nalmost everywhere
under Assumption 4.1. This bias is independent of the distribution of Xor the spectral density of
S−1ˆΣS, but only depending on the rank deficiency of the realization of X.
Finally, the prediction risk RP(ˆ◁beta1) can be summarized as follows:
Corollary 4.1. Let Assumptions 2.1, 3.1, 4.1, and 4.2 hold. Then, we have
RP(ˆ◁beta1)=r2
Σ 
1−n
p!
+Tr(Ω)
nEXh
Tr((X⊤X)†Σ)i
.
4.2. The bias component of estimation risk. For the bias component of estimation risk, we can
obtain a similar result with 4.1 as follows:
[Bias( ˆ◁beta1|X)]2=◁beta1⊤(I−ˆΣ†ˆΣ)◁beta1=lim
λ↘0◁beta1⊤λ(ˆΣ +λI)−1◁beta1.
Assumption 4.3. E◁beta1[◁beta1◁beta1⊤]=r2I/p, wherer2:=E◁beta1[∥◁beta1∥2]<∞and◁beta1is independent of X.RIDGELESS LEAST SQUARES ESTIMATOR 11
Under Assumption 4.3, we have the expected bias (conditional on X) as follows:
E◁beta1[Bias( ˆ◁beta1|X)2|X]=r2
plim
λ↘0pX
i=1λ
si+λ=r2
p|{i∈[p] :si=0}|=r2p−n
p, (5)
wheresiare the eigenvalues of ˆΣ∈Rp×pand rank( ˆΣ)=rank(X)=nunder Assumption 4.1.
Thanks to Theorem 3.3 and (5), we obtain the following corollary for estimation risk.
Corollary 4.2. Let Assumptions 2.1, 3.1, 4.1, and 4.3 hold. Then, we have
RE(ˆ◁beta1)=r2 
1−n
p!
+Tr(Ω)
nEX"Z1
sdFXX⊤/n(s)#
,
whereFA(s) :=1
nPn
i=11{λi(A)≤s}is the empirical spectral distribution of a matrix Aand
λ1(A),λ2(A),···,λn(A)are the eigenvalues of A.
The proof of Corollary 4.2 is in the appendix.
4.2.1. Asymptotic analysis of estimation risk. To study the asymptotic behavior of estimation risk,
we follow the previous approaches (Dobriban and Wager, 2018; Hastie et al., 2022). First, we define
the Stieltjes transform as follows:
Definition 4.1. The Stieltjes transform sF(z) of a dfFis defined as:
sF(z) :=Z1
x−zdF(x),forz∈C\supp(F).
100101102
=p/n
102
101
100101102Prediction Risk
variance
variance (theory)
variance (theory, iso.)
bias
bias (theory)
100101102
=p/n
102
101
100101102Estimation Risk
variance
variance (theory)
variance (theory, iso.)
bias
bias (theory)
Figure 4. The “descent curve” in the overparameterization regime for predic-
tion risk (left) and estimation risk (right). We test Ω’s with Tr(Ω)/n=1,2,4in
black, blue, red, respectively. For the anisotropic feature, the expected variance
(×) and its theoretical expression ( ) areΘTr(Ω)/n
γ−1
and larger than that in the high-
dimensional asymptotics for the isotropic Σ =I. For the isotropic Σ =I, the variance
terms (dotted) and the bias terms (dashed) in the high-dimensional asymptotics are
1
γ−1limn→∞Tr(Ω)
nandr2
1−1
γ
, respectively.
We are now ready to investigate the asymptotic behavior of the mean squared estimation error
with the following theorem:12 SUNGYOON LEE AND SOKBAE LEE
Theorem 4.3. (Silverstein and Bai, 1995, Theorem 1.1) Suppose that the rows {xi}n
i=1inXare
i.i.d. centered random vectors with E[x1x⊤
1]= Σ and that the empirical spectral distribution
FΣ(s)=1
pPp
i=11{τi≤s}ofΣconverges almost surely to a probability distribution function Has
p→∞ . Whenp/n→γ>0asn,p→∞ , then a.s.,FXX⊤/nconverges vaguely to a df Fand the
limits∗:=limz↘0sF(z)of its Stieltjes transform sFis the unique solution to the equation:
1−1
γ=Z1
1+τs∗dH(τ). (6)
This theorem is a direct consequence of Theorem 1.1 in Silverstein and Bai (1995). Then, from
Corollary 4.2, we can write the limit of estimation risk as follows:
Corollary 4.4. Let Assumptions 2.1, 3.1, 4.1, and 4.3 hold. Then, under the same assumption as
Theorem 4.3, as n,p→∞ andp/n→γ, where 1<γ<∞is a constant, we have
RE(ˆ◁beta1)=E∥ˆ◁beta1−◁beta1∥2→r2 
1−1
γ!
+s∗lim
n→∞Tr(Ω)
n.
Here, the limit s∗of the Stieltjes transform sFis highly connected with the shape of the spectral
distribution of Σ. For example, in the case of isotropic features ( Σ =I), i.e.,dH(τ)=δ(τ−1)dτ,
we haves∗
iso=(γ−1)−1from 1−1
γ=1
1+s∗
iso. In addition, if Ω =σ2I, then the limit of the mean
squared error is exactly the same as the expression for γ>1in equation (10) of Hastie et al. (2022,
Theorem 1). This is because prediction risk is the same as estimation risk when Σ =I.
Remark 4.1. Generally, if the support of His bounded within [cH,CH]⊂Rfor some positive
constants 0< cH≤CH<∞, then we can observe the double descent phenomenon in the
overparameterization regime with limγ↘1s∗=∞andlimγ→∞s∗=0withs∗= Θ
1
γ−1
from the
following inequalities:
C−1
H1
γ−1≤s∗≤c−1
H1
γ−1. (7)
In fact, a tighter lower bound is available:
s∗≥µ−1
H(γ−1)−1, (8)
whereµH:=Eτ∼H[τ], i.e., the mean of distribution H. The proofs of (7) and (8) are given in the
supplementary appendix.
We conclude this paper by plotting the “descent curve” in the overparameterization regime in
Figure 4. On one hand, the expected variance ( ×) perfectly matches its theoretical counterpart ( )
and goes to zero as γgets large. On the other hand, the bias term is bounded even if γ→∞ . The
appendix contains the experimental details for all the figures.RIDGELESS LEAST SQUARES ESTIMATOR 13
Appendix A. D etails for drawing Figure 1
To draw Figure 1, we use a sample extract from American Community Survey (ACS) 2018. To
have a relatively homogeneous population, the sample extract is restricted to white males residing in
California with at least a bachelor’s degree. We consider a demographic group defined by their age
in years (between 25 and 70), the type of degree (bachelor’s, master’s, professional, and doctoral),
and the field of degree (172 unique values). Then, we compute the average of log hourly wages
for each age-degree-field group (all together 7,073 unique groups in the sample). We treat each
group average as the outcome variable (say, ya,d,f) and predict group wages by various group-level
regression models where the regressors are constructed using the indicator variables of age, degree,
and field as well as their interactions: that is,
ya,d,f=x⊤
a,d,f◁beta1+εa,d,f.
For the regressors xa,d,f, we consider 7 specifications ranging from 209 to 2,183 regressors:
•Spec. 1 (p=209): dummy variables for age (say, xa)+dummy variables for the type of
degree (say,xd)+dummy variables for the field of degree (say, xf),
•Spec. 2 (p=391): Spec. 1 +all interactions between xdandxa,
•Spec. 3 (p=598): Spec. 1 +all interactions between xdandxf,
•Spec. 4 (p=778): Spec. 1 +all interactions between xdandxa+all interactions between
xdandxf,
•Spec. 5 (p=1640 ): Spec. 1 +all interactions between xdandxa+all interactions between
xaandxf,
•Spec. 6 (p=1754 ): Spec. 1 +all interactions between xdandxf+all interactions between
xaandxf,
•Spec. 7 (p=2182 ): Spec. 1 +all three-way interactions among xa,xdandxf.
Here, the dummy variable are constructed using one-hot encoding. We randomly split the sample
into the train and test samples with a ratio of 1:4. The resulting sample sizes are 1,415 and 5,658,
respectively. To understand the role of non-i.i.d. regressor errors, we add the artificial noise to the
training sample: that is, we compute the ridgeless least squares estimator using the training sample
of (˜ya,d,f,x⊤
a,d,f)⊤, where ˜ya,d,f=ya,d,f+ua,d,f. Here, the artificial noise ua,d,fhas the form
ua,d,f≡(1−c)ea,d,f+c·efp
(1−c)2+c2,
whereea,d,f∼N(0,σ2), independently across age ( a), degree (d) and field (f);efis the average of
another independent N(0,σ2)variable within f(hence,efis identical for each value of f) and thus
the source of clustered errors; and c∈{0,0.25,0.5,0.75}is a constant that will be varied across
the experiment. As cgets larger, the noise has a larger share of clustered errors but the variance14 SUNGYOON LEE AND SOKBAE LEE
of the overall regression errors ( ua,d,f) remains the same: in other words, var(ua,d,f)=σ2for each
value ofc. Figure 1 was generated with σ=0.5by generating the artificial noise only once.
Appendix B. D etails for drawing Figures 2, 3, and4
To draw Figure 2, 3, and 4, we sample {xi}n
i=1fromN(0,Σ)withΣ =UΣDΣU⊤
ΣwhereUΣis an
orthogonal matrix random variable, drawn from the uniform (Haar) distribution on O(p), andDΣ
is a diagonal matrix with its elements di=|zi|/Pp
i=1|zi|being sampled with zi∼N(0,1)for each
i=1,2,···,p. With this general anisotropic Σ, the term EX[Tr(Λ−1)]/pis somewhat larger than
µ−1
Hs∗
iso=(γ−1)−1which is 1in Figure 2 and 3 since µH=1andγ=2. For example, in Figure 2,
whenσ2=1,ρ2=0, we have Tr( Ω)/n=1but Tr( Ω)EX[Tr(Λ−1)]/(np)>1.
In Figure 4, we fix n=50and usep=nγforγ∈[1,100].
To compute the expectations of EX[Var(ˆ◁beta1|X)]andEX[Tr(Λ−1)]overX, we sampleNXsamples of
X’s,X1,X2,···,XNX. Moreover, to compute the expectation over εin
Var(ˆ◁beta1|Xi)≡Tr
Eε[ˆ◁beta1ˆ◁beta1⊤]−Eε[ˆ◁beta1]Eε[ˆ◁beta1]⊤
,
we sampleNεsamples ofε’s,ε1,ε2,···,εNεfor each realization Xi. To be specific,
EX[Var( ˆ◁beta1|X)]≈1
NXNXX
i=1Var(ˆ◁beta1|Xi)≈1
NXNXX
i=1Tr1
NεNεX
j=1ˆ◁beta1i,jˆ◁beta1⊤
i,j−1
NεNεX
j=1ˆ◁beta1i,j1
NεNεX
j=1ˆ◁beta1⊤
i,j
1
pEX[Tr(Λ−1)]≈1
NXNXX
i=1Tr((XiX⊤
i)−1)=1
NXNXX
i=1nX
k=11
λk(XiX⊤
i),
where ˆ◁beta1i,j=arg min◁beta1{∥b∥:Xib−yi,j=0},yi,j=Xi◁beta1+εj, andλk(XiX⊤
i)is thek-th eigenvalue of
XiX⊤
i. We can do similarly for the variance part of the prediction risk.
Figure 5 shows an additional experimental result.
0.1 0.2 0.3 0.4 0.5
2
1
0.10.20.30.40.52
2
Prediction Risk - Clustered Errors
0.200.250.300.350.400.450.500.550.600.650.70
0.1 0.2 0.3 0.4 0.5
2
1
0.10.20.30.40.52
2
Estimation Risk - Clustered Errors
0.200.250.300.350.400.450.500.550.600.65
0.65
Figure 5.We use the same setting as Figure 3, except uniformly sample each
ρifrom [0,0.05]for each experiment with the pairs (σ2
1,σ2
1). As expected, the
off-diagonal elements ρiofΩdo not a ffect the expected variances.RIDGELESS LEAST SQUARES ESTIMATOR 15
Appendix C. P roofs omitted in the main text
Proof of Lemma 3.1. For a givenA∈S, sinceA−1∈S, we haveZd=A−1Z:=˜Zand
EZ[f(Z)]=EA−1Z[f(Z)]=E˜Z[f(A˜Z)]=EZ[f(AZ)].
This naturally leads to
EZ[EA′∼ν[f(A′Z)]]=EA′∼ν[EZ[f(A′Z)]]=EA′∼ν[EZ[f(Z)]]=EZ[f(Z)]
where the first equality comes from Fubini’s theorem and the integrability of f. □
Proof of Theorem 3.2. Since ˆ◁beta1=X†y, we have Cov(ˆ◁beta1|X)=X†Cov(y|X)X†⊤=X†ΩX†⊤, which
leads to the following expression for the variance component of prediction risk:
VarΣ(ˆ◁beta1|X)=Tr(Cov( ˆ◁beta1|X)Σ)=Tr(X†ΩX†⊤Σ)=∥SX†T∥2
F=∥BT∥2
F,
whereS= Σ1/2,T= Ω1/2, andB=SX†. Using the singular value decomposition (SVD) of Band
T, respectively, we can rewrite this as follows:
∥BT∥2
F=∥UDV⊤UTDTV⊤
T∥2
F=∥DV⊤UTDT∥2
F,
whereB=UDV⊤andT=UTDTV⊤
Twith orthogonal matrices U,V,UT,VT, and diagonal matrices
D,DT. Now we need to compute the alignment V⊤UTof the right-singular vectors of Bwith the
left-eigenvectors of T.
∥DV⊤UTDT∥2
F=nX
i,j=1
DiiXn
k=1V⊤
ik(UT)kj(DT)jj2
=Xn
i,j=1λi(B)2λj(T)2γij
=Xn
i,j=1λi
(X⊤X)†Σ
λj(Ω)γij
=λ
(X⊤X)†Σ⊤
1×nΓ(X)
n×nλ(Ω)
n×1,
whereγij:=⟨V:i,(UT):j⟩2≥0,Γ(X) :=(γij)i,j∈Rn×nandλ(A)∈Rnis a vector with its element
λi(A) as thei-th largest eigenvalue of A.
Therefore, we can rewrite the variance as Var Σ(ˆ◁beta1|X)=a(X)⊤Γ(X)bwith
a(X) :=λ
(X⊤X)†Σ
∈Rn,
b:=λ(Ω)∈Rn,
Γ(X)ij=γij=⟨v(i),u(j)⟩2,
wherev(i):=V:iandu(j):=(UT):j. Note that the alignment matrix Γ(X)is a doubly stochastic matrix
sinceP
jγij=P
iγij=1and0≤γij≤1.16 SUNGYOON LEE AND SOKBAE LEE
Now, we want to compute the expected variance. To do so, from Lemma 3.1 with S=O(n), we
can obtain
EX[a(X)⊤Γ(X)b]=EXEO∼ν[a(OX)⊤Γ(OX)b]=EXa(X)⊤EO∼ν[Γ(OX)]b,
whereνis the unique uniform distribution (the Haar measure) over the orthogonal matrices O(n).
For an orthogonal matrix O∈O(n), we have
Γ(OX)ij=⟨Ov(i),u(j)⟩2=(v(i)⊤O⊤u(j))2,
sinceS(OX)†=SX†O⊤=BO⊤=UD(OV)⊤. Here, (OX)†=X†O⊤follows from the orthogonality
ofO∈O(n). Since the Haar measure is invariant under the matrix multiplication in O(n), if we
take the expectation over the Haar measure, then we have
¯Γ(X)ij:=EO∼ν[Γ(OX)ij]=EO∼ν[(v(i)⊤O⊤u(j))2]=EO∼ν[(v(i)⊤O⊤O(j)⊤u(j))2].
Here, for a given j, we can choose a matrix O(j)∈O(n)such that its first column is u(j)and
O(j)⊤u(j)=e1, then ¯Γ(X)ijis independent of j(say ¯Γ(X)ij=αi). Since Γ(X)is doubly stochastic,
so is ¯Γ(X)and we havePn
j=1¯Γ(X)ij=nαi=1which yields ¯Γ(X)ij=αi=1/n, regardless of the
distribution of V; thus, ¯Γ(X)=1
nJ, whereJij=1(i,j=1,2,···,n).
Therefore, we have the expected variance as follows:
EX[Var Σ(ˆ◁beta1|X)]=EX[a(X)⊤1
nJb]=1
nnX
i,j=1EX[ai(X)]bj=1
nEX[Tr((X⊤X)†Σ)] Tr(Ω).
□
Proof of Corollary 4.2. Note that
EX[Var( ˆ◁beta1|X)]=Tr(Ω)
pEX1
nX
i1
λi
=Tr(Ω)
pEX"Z1
sdFXX⊤/p(s)#
=Tr(Ω)
nEX"Z1
sdFXX⊤/n(s)#
.
Then, the desired result follows directly from (5). □RIDGELESS LEAST SQUARES ESTIMATOR 17
Proof of (4). The bias term of the prediction risk can be expressed as follows:
[Bias Σ(ˆ◁beta1|X)]2=∥E[ˆ◁beta1|X]−◁beta1∥2
Σ
=∥(ˆΣ†ˆΣ−I)◁beta1∥2
Σ
=◁beta1⊤(I−ˆΣ†ˆΣ)Σ(I−ˆΣ†ˆΣ)◁beta1
=◁beta1⊤lim
λ↘0λ(ˆΣ +λI)−1Σlim
λ↘0λ(ˆΣ +λI)−1◁beta1
=(S◁beta1)⊤lim
λ↘0λ2(S−1ˆΣS+λI)−2S◁beta1,
where ˆΣ =X⊤X/n. Here, the fourth equality comes from the equation
I−ˆΣ†ˆΣ =lim
λ↘0I−(ˆΣ +λI)−1ˆΣ
=lim
λ↘0I−(ˆΣ +λI)−1(ˆΣ +λI−λI)
=lim
λ↘0λ(ˆΣ +λI)−1.
□
Proof of (7). The RHS of (6) is bounded above byR
1
1+cHs∗dH(τ)=1
1+cHs∗, and thus 1−1
γ≤1
1+cHs∗,
which yields s∗≤c−1
H1
γ−1. We can similarly prove the other inequality in (7) with a lower bound
1
1+CHs∗on the RHS of (6). □
Proof of (8). To further explore the inequalities (7), we rewrite (6) from Theorem 4.3 as follows:
1−1
γ=Eτ∼H[g(τ;s∗)],whereg(t;s) :=1
1+tsfort,s> 0.
Here, sinceg(t;s)is convex with respect to t>0for a givens>0, by Jensen’s inequality, we then
have
Eτ∼H[g(τ;µ−1
Hs∗
iso)]≥g
µH;µ−1
Hs∗
iso
=g(1;s∗
iso)=1−γ−1
whereµH=Eτ∼H[τ]. Therefore, the limit Stieltjes transform s∗in the anisotropic case should
be larger than µ−1
Hs∗
isoof the isotropic case to satisfy Eτ∼H[g(τ;s∗)]=1−γ−1sinceg(t;s)is a
decreasing function with respect to s≥0whent >0. This leads to a tighter lower bound
s∗≥µ−1
Hs∗
iso=µ−1
H(γ−1)−1than (7) because µH≤CH. □
References
Abadie, A., A. Diamond, and J. Hainmueller (2010). Synthetic control methods for comparative case
studies: Estimating the e ffect of california’s tobacco control program. Journal of the American
Statistical Association 105 (490), 493–505. 2
Bartlett, P. L., P. M. Long, G. Lugosi, and A. Tsigler (2020). Benign overfitting in linear regression.
Proceedings of the National Academy of Sciences 117 (48), 30063–30070. 1, 218 SUNGYOON LEE AND SOKBAE LEE
Belkin, M., D. Hsu, S. Ma, and S. Mandal (2019). Reconciling modern machine-learning prac-
tice and the classical bias–variance trade-o ff.Proceedings of the National Academy of Sci-
ences 116 (32), 15849–15854. 1
Belkin, M., D. Hsu, and J. Xu (2020). Two models of double descent for weak features. SIAM
Journal on Mathematics of Data Science 2 (4), 1167–1180. 1
Belkin, M., S. Ma, and S. Mandal (2018). To understand deep learning we need to understand
kernel learning. In International Conference on Machine Learning , pp. 541–549. PMLR. 1
Berthier, R., F. Bach, and P. Gaillard (2020). Tight nonparametric convergence rates for stochastic
gradient descent under the noiseless linear model. Advances in Neural Information Processing
Systems 33 , 2576–2586. 4
Chen, X., Y . Zeng, S. Yang, and Q. Sun (2023). Sketched ridgeless linear regression: The role of
downsampling. In International Conference on Machine Learning , pp. 5296–5326. PMLR. 10
Chinot, G. and M. Lerasle (2023). On the robustness of the minimum ℓ2interpolator.
Bernoulli . forthcoming, available at https://www.bernoullisociety.org/publications/
bernoulli-journal/bernoulli-journal-papers . 4
Chinot, G., M. Lö ffler, and S. van de Geer (2022). On the robustness of minimum norm interpolators
and regularized empirical risk minimizers. The Annals of Statistics 50 (4), 2306 – 2333. 4
Dawid, A. (1977). Spherical matrix distributions and a multivariate model. Journal of the Royal
Statistical Society: Series B (Methodological) 39 (2), 254–261. 7
Dawid, A. (1978). Extendibility of spherical matrix distributions. Journal of Multivariate Analy-
sis 8(4), 559–566. 7
Dawid, A. (1981). Some matrix-variate distribution theory: Notational considerations and a
Bayesian application. Biometrika , 265–274. 7
Dobriban, E. and S. Wager (2018). High-dimensional asymptotics of prediction: Ridge regression
and classification. The Annals of Statistics 46 (1), 247 – 279. 2, 4, 5, 10, 11
Gupta, A. K. and D. K. Nagar (1999). Matrix variate distributions , V olume 104. CRC Press. 7
Hansen, B. (2022). Econometrics . Princeton University Press. 4
Hansen, B. E. (2016). The risk of James–Stein and Lasso shrinkage. Econometric Reviews 35 (8-10),
1456–1470. 4
Hastie, T., A. Montanari, S. Rosset, and R. J. Tibshirani (2022). Surprises in high-dimensional
ridgeless least squares interpolation. The Annals of Statistics 50 (2), 949–986. 1, 2, 5, 11, 12
James, W. and C. Stein (1961). Estimation with quadratic loss. In Proc. 4th Berkeley Sympos. Math.
Statist. and Prob., Vol. I , pp. 361–379. Univ. California Press, Berkeley, Calif. 3
Kobak, D., J. Lomond, and B. Sanchez (2020). The optimal ridge penalty for real-world high-
dimensional data can be zero or negative due to the implicit ridge regularization. The Journal of
Machine Learning Research 21 (1), 6863–6878. 2RIDGELESS LEAST SQUARES ESTIMATOR 19
Li, Z., C. Xie, and Q. Wang (2021). Asymptotic normality and confidence intervals for prediction
risk of the min-norm least squares estimator. In International Conference on Machine Learning ,
pp. 6533–6542. PMLR. 10
Liao, Y ., X. Ma, A. Neuhierl, and Z. Shi (2023). Economic forecasts using many noises. arXiv
preprint arXiv:2312.05593. 2
Mei, S. and A. Montanari (2022). The generalization error of random features regression: Precise
asymptotics and the double descent curve. Communications on Pure and Applied Mathemat-
ics 75 (4), 667–766. 1
Richards, D., J. Mourtada, and L. Rosasco (2021). Asymptotics of ridge (less) regression under
general source condition. In International Conference on Artificial Intelligence and Statistics , pp.
3889–3897. PMLR. 2, 10
Ruggles, S., S. Flood, M. Sobek, D. Backman, A. Chen, G. Cooper, S. Richards, R. Rodgers, and
M. Schouweiler (2024). IPUMS USA: Version 15.0 [dataset]. https://doi.org/10.18128/
D010.V15.0 . Minneapolis, MN: IPUMS. 2
Silverstein, J. W. and Z. Bai (1995). On the empirical distribution of eigenvalues of a class of large
dimensional random matrices. Journal of Multivariate analysis 54 (2), 175–192. 12
Spiess, J., G. Imbens, and A. Venugopal (2023). Double and single descent in causal inference with
an application to high-dimensional synthetic control. In Thirty-seventh Conference on Neural
Information Processing Systems . 2
Tsigler, A. and P. L. Bartlett (2023). Benign overfitting in ridge regression. Journal of Machine
Learning Research 24 (123), 1–76. 1, 2
Wu, D. and J. Xu (2020). On the optimal weighted ℓ2regularization in overparameterized linear
regression. Advances in Neural Information Processing Systems 33 , 10112–10123. 2
Zou, D., J. Wu, V . Braverman, Q. Gu, and S. Kakade (2021). Benign overfitting of constant-stepsize
SGD for linear regression. In Conference on Learning Theory , pp. 4633–4635. PMLR. 1
Department of Computer Science , Hanyang University , Seoul, Korea
Email address :sungyoonlee@hanyang.ac.kr
Department of Economics , Columbia University , NY , U nited States
Email address :sl3841@columbia.edu