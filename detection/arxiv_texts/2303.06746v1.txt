DNN-Alias: Deep Neural Network Protection
Against Side-Channel Attacks via Layer Balancing
Mahya Morid Ahmadiy, Lilas Alrahisz, Ozgur Sinanogluzand Muhammad ShaÔ¨Åquez
yTechnische Universit ¬®at Wien (TU Wien), Vienna, Austria
zDivision of Engineering, New York University Abu Dhabi (NYUAD), Abu Dhabi, United Arab Emirates
Email: mahya.ahmadi@tuwien.ac.at, flma387, ozgursin, muhammad.shaÔ¨Åque g@nyu.edu
Abstract ‚ÄîExtracting the architecture of layers of a given
deep neural network (DNN) through hardware-based side
channels allows adversaries to steal its intellectual property
and even launch powerful adversarial attacks on the target
system. In this work, we propose DNN-Alias , an obfuscation
method for DNNs that forces all the layers in a given network
to have similar execution traces, preventing attack models
from differentiating between the layers. Towards this, DNN-
Alias performs various layer-obfuscation operations, e.g., layer
branching, layer deepening, etc, to alter the run-time traces while
maintaining the functionality. DNN-Alias deploys an evolutionary
algorithm to Ô¨Ånd the best combination of obfuscation operations
in terms of maximizing the security level while maintaining a
user-provided latency overhead budget.
We demonstrate the effectiveness of our DNN-Alias technique
by obfuscating the architecture of 700 randomly generated and
obfuscated DNNs running on multiple Nvidia RTX 2080 TI
GPU-based machines. Our experiments show that state-of-the-
art side-channel architecture stealing attacks cannot extract the
original DNN accurately. Moreover, we obfuscate the architecture
of various DNNs, such as the VGG-11, VGG-13, ResNet-20,
and ResNet-32 networks. Training the DNNs using the standard
CIFAR10 dataset, we show that our DNN-Alias maintains the
functionality of the original DNNs by preserving the original
inference accuracy. Further, the experiments highlight that
adversarial attack on obfuscated DNNs is unsuccessful.
I. I NTRODUCTION
Deep neural networks (DNNs) have experienced rapid
advancements in the past decade, leading to their application
to many areas of human endeavor and various Ô¨Åelds of
science [1], [2]. The deployment of DNNs in mission-critical
applications, such as healthcare systems [3] and anomaly
detection in cyber-physical systems [4], raises concerns about
the safety and security of these networks. Moreover, building
and training DNNs require expert knowledge and costly
resources. Thus, DNNs for a certain application are considered
sensitive and expensive intellectual property (IP) that require
protection from malicious users and/or market competitors.
To facilitate the application of DNNs, model developers
offer machine learning as a service (MLaaS), which includes
machine learning (ML) models and services running on the
cloud or edge devices [5]. In the MLaaS business model, the
end-user receives input/output access to the DNN, i.e., black-
box access . An untrusted user would want to extract the IP
of the underlying DNN, for launching white-box adversarial
attacks on the target system and/or for stealing the IP without
Execution Trace DNN ModelAdversar y
Hidden Target DNNSide-channel 
Architecture Stealing
Adversarial 
Attack
GPUFig. 1. Hardware-based side-channel leakage facilities DNN architecture
stealing, leading to white-/grey-box adversarial attacks.
incurring high research and development costs.1Note, it is
difÔ¨Åcult to launch such attacks with only back-box access to
the system, e.g., it takes 30 GPU days to launch an adversarial
attack on Lenet+ (7 layers) using input/output queries [9].
Additional information about the target DNN could be
extracted through hardware-based side-channels , such as the
DNN architecture (i.e., the number, type, dimension, and
connectivity of layers), enabling advanced adversarial attacks.
For example, DeepSniffer [10] is a DNN side-channel-based
architecture stealing (SCAS) attack that learns the correlation
between the architecture hints (such as volumes of memory
writes/reads) and the DNN architecture.2DeepSniffer showed
that including the architecture information increases the
success rate of an adversarial attack by 3. Fig. 1 shows
an example of such SCAS attack Ô¨Çow.
To demonstrate the potent nature of such SCAS attacks,
we plot the run-time traces of the AlexNet DNN trained on
the CIFAR10 dataset in terms of computation latency (cycles)
and memory access time (read and write) per layer in Fig 2. It
can be observed that each type of layer has a unique execution
signature. As a result, SCAS attacks learn the execution pattern
of the layer functions and extract the layer sequence.
Researchers have studied SCAS attacks based on various
side-channels, e.g., power [12]. We focus on timing (execution
time and memory access) side-channels, which are more
stealthy, as they exploit an operational part of system (system
proÔ¨Ålers) for measuring and do not require extra equipment.
Researchers have developed several protection techniques
1An adversarial attack performs subtle perturbations to the input samples
of an ML model, causing the model to predict incorrect outputs [6], [7]. A
white-box attack model assumes access to the inputs, architecture, and internal
details, e.g., weights. Conversely, in the black-box model, the attacker lacks
access to these details. A gray-box attack trains a substitute model to generate
adversarial samples and attack the target model [8].
2SCAS attacks can be physical (edge) or remote (cloud) [11].arXiv:2303.06746v1  [cs.CR]  12 Mar 2023conv2d_relu
conv2d_relu
conv2d_relu
max_pool2d
conv2d_relu
conv2d_relu
conv2d_relu
max_pool2d
conv2d_relu
conv2d_relu
conv2d_relu
conv2d_relu
conv2d_relu
conv2d_relu
max_pool2d
conv2d_relu
conv2d_relu
conv2d_relu
conv2d_relu
conv2d_relu
conv2d_relu
max_pool2d
conv2d_relu
conv2d_relu
conv2d_relu
conv2d_relu
conv2d_relu
conv2d_relu
max_pool2d
fc_relu
fc_relu
fc
log_softmax
log_softmaxLatency 
(cycles)Mem_read
(sector)Mem_write
(sector)Kernels
Conv_ReLu
FC
Each layer function has a 
unique pattern in run -
time trace of the kernels.Fig. 2. The run-time trace of AlexNet. The unique pattern of the memory
access bytes and computational cycles per each kernel allows SCAS attacks
to learn these features and extract the target DNN architecture successfully.
TABLE I
COMPARISON OF THE SOTA METHODS THAT PROTECT THE DNN
ARCHITECTURE FROM MEMORY LEAKAGE SCAS ATTACKS ,EXPLAINED IN
SEC. I-A. NA INDICATES NOT APPLICABLE .
DefenseLow
LatencyThwarts
ML SCASThwarts
NeuroUnlock [17]
Hardware-based [14], [18], [19] 3 7 NA
Memory TrafÔ¨Åc Noise [10] 107 3 7
DNN Obfuscation [15] 3 3 7
Proposed DNN-Alias 1:43 3 3
to thwart SCAS attacks. Some hiding techniques decrease the
signal-to-noise ratio in side-channel traces, e.g., via dummy
memory operations [13]. Other methods necessitate hardware
modiÔ¨Åcations to encrypt memory and other side-channel
leakage [14]. More recently, DNN obfuscation has been
proposed to alter the run-time traces of a given DNN while
preserving its functionality, thwarting SCAS attacks [15], [16].
Nevertheless, these methods suffer at least from one of the
drawbacks discussed next and summarized in Table I.
A. State-of-the-Art (SOTA) and their Limitations
Overhead: Oblivious random access memory (ORAM)
schemes encrypt and shufÔ¨Çe the memory read/writes, reducing
memory leakage [18]. However, ORAM-protected designs
suffer from high overhead, e.g., 10latency cost [20].
Ineffectiveness: Introducing noise to the execution traces
thwarts statistical SCAS attacks, but fails to mitigate ML-
based SCAS attacks.3Such ML-based attacks are trained to
be resilient to noisy data. For example, the success rate of the
ML-based DeepSniffer [10] attack remains the same even with
30% of amplitude noise.
Limited and Hardware-SpeciÔ¨Åc Security: Current DNN
obfuscation methods offer limited security. It has been shown
that advanced attacks, such as NeuroUnlock [17], can learn
the obfuscation procedure and automatically revert it, thereby
recovering the original DNN architecture. Further, existing
obfuscation techniques, such as NeurObfuscator [15], depend
on extensive hardware proÔ¨Åling. This proÔ¨Åling step makes the
defense mechanism dependent on the underlying hardware.
3In Ô¨Årst, the attacker applies statistical methods, i.e., correlation analysis,
to distinguish the correct secret value among the hypotheses [12], while in
second, the attacker trains an ML model to classify the traces.
Original DNN Obfuscated DNN1) Measuring the deviation
of run -time traces 
2) Reducing the diversity of 
DNN layer models
3) Automatic obfuscation 
algorithm to optimize cost
Run-time trace Run-time traceFig. 3. Our contributions presented in this work are in blue box.
B. Key Research Challenges Targeted in this Work
The above discussion shows that there is still a gap in
designing secure DNN architectures. Developing an efÔ¨Åcient
and cost-effective defense mechanism imposes the following
important research questions and challenges.
1)What makes SCAS attacks successful? To design
secure DNNs, we need to identify the conditions that
make the SCAS attacks successful and eliminate them
during the development stage of the DNN.
2)Generic security metric. Defense mechanisms that focus
solely on reducing an attack-speciÔ¨Åc metric, such as the
layer error rate (LER),4cannot mitigate further attack
vectors. Thus, devising a generic security metric is
required to evaluate the security of DNNs at design time.
3)Performance overhead. A generic and adaptive defense
mechanism is required, which can be tailored per target
DNN, hardware implementation, and overhead.
C. Our Novel Concept and Contributions
To address the above challenges, we propose DNN-Alias ,
a DNN obfuscation methodology to protect the architecture
of DNNs against static and ML-based SCAS attacks. We
argue that SCAS attacks are successful because each layer
has a unique run-time trace signature (see Fig. 2). Therefore,
we demonstrate that if the signatures of the different layers
overlap, it will be difÔ¨Åcult for any SCAS attack to differentiate
between them. Deterministic DNN obfuscation alters the run-
time traces of the layers. However, it does not guarantee
overlapping signatures. Our proposed DNN-Alias employs, for
the Ô¨Årst time, a generic security metric, which measures the
overlap between layer signatures by computing the standard
deviation. DNN-Alias performs various DNN obfuscation
operations to minimize the standard deviation, i.e., resulting
in a more secure DNN architecture. Note that the security
objective does not depend on any speciÔ¨Åc attack output but
rather on the features of the DNN itself. Furthermore, DNN-
Alias does not proÔ¨Åle the underlying hardware. Hence, DNN-
Alias is a generic defense methodology that is applicable to
any DNN running on any hardware. Our novel contributions
are summarized in Fig. 3 and discussed below.
1)General solution to measure the diversity of DNN
layers (Sec. III-C): DNN-Alias deÔ¨Ånes a novel security
metric based on the distribution of run-time parameters
to measure the overlap between layer signatures.
4The editing distance between extracted and original layer sequence.SpeciÔ¨Åcally, DNN-Alias calculates the standard deviation
of features in the run-time trace and reduces it. By
utilizing this metric, DNN-Alias becomes independent of
the outcome of any particular attack.
2)Balancing the execution trace (Sec. III-B): DNN-
Alias presents a novel DNN obfuscation technique that
performs layer balancing, limiting information leakage.
3)EfÔ¨Åcient DNN obfuscation (Sec. III-D): DNN-Alias
employs a genetic algorithm to Ô¨Ånd an effective
combination of obfuscation operations. The reward
function of this algorithm minimizes the standard
deviation in run-time trace while maintaining cost budget.
Key Results: We have comprehensively evaluated the
efÔ¨Åcacy of DNN-Alias with a broad set of random and standard
DNNs on image classiÔ¨Åcation, running on the Nvidia RTX
2080 TI GPU. We demonstrate that DNN-Alias increases the
LER (between the original and obfuscated DNN) by 2:5
compared to SOTA techniques, resulting in higher obfuscation.
We evaluate the security of DNN-Alias by launching;
(i) an ML-based SCAS attack and (ii) NeuroUnlock on the
obfuscated DNNs. We measure the difference between the
architecture of the original DNN and the recovered DNN by
the attacks. The LER obtained on the DNN-Alias networks is
2 on average (an LER higher than 1 is considered secure).
Further, NeuroUnlock fails to de-obfuscate the networks,
reporting an LER of 1.1 on DNN-Alias networks.
Further, DNN-Alias preserves the training accuracy while
protecting the DNN against gray-box adversarial attacks.
II. B ACKGROUND
In this section, we provide the necessary background
required to protect DNNs against SCAS attacks.
A. Side-channel-based Architecture Stealing (SCAS) Attacks
Adversaries can compromise the security of a DNN system
by uncovering its conÔ¨Ådential model components, such as its
architecture and parameters. Extracting an exact copy of the
DNN is challenging in a black-box setup, where access to
the victim model is limited [21]. However, physical access
to the DNN‚Äôs hardware platform can lead to the exposure of
conÔ¨Ådential information through side-channel attacks, such as
power analysis or timing analysis [22] (see 1in Fig. 4).
Through a SCAS attack, the adversary builds a substitute
model using the extracted information and trains it by querying
the victim DNN or using a publicly available labeled dataset
(see 2and 3in Fig. 4). The attacker can then use the
substitute model to launch adversarial attacks against the
original DNN system (see 4in Fig. 4).
In this study, we focus on attacks that exploit memory,
cache, and timing-based information leaks to reveal the
architecture of DNNs running on GPU devices. For example,
the DeepSniffer attack [10], depicted in Fig. 5, employs a
long-short-term-memory (LSTM) model [24] to deduce the
layer arrangement of a targeted DNN based on its runtime
trace.
The run-time trace is a time-series collection of various
characteristics, such as execution time and dynamic random
Adversarial attackTarget model
Training dataset
1 2
3
4
Layer sequence 
identificationDNN model 
architectural hints 
from the kernel
Dimension size 
estimation
Architecture + ParametersFig. 4. Side-channel-based architecture stealing (SCAS) attack.
Run-time trace
(Kernel -wise 
architectural 
hints vectors)
Latency
Mem. Read
Mem. Write
‚Ä¶Layer 
sequence  
identificationModel 
constructionRun-time layer 
sequence identifier
Network 
topology
construction
Dimension 
estimationNoise and
information
Extracted modelReLU
Conv 1 -2
ReLU
Pool
Conv 2 -1
‚Ä¶Conv 1 -2 
Fig. 5. Flow of the DeepSniffer attack [23].
access memory (DRAM) access time. The training process for
the LSTM model involves proÔ¨Åling randomly generated DNNs
on the target GPU. The layer sequence of each generated
DNN is encoded as a vector and its runtime trace is extracted.
Subsequently, a runtime proÔ¨Åle dataset is constructed based
on kernel-aware architectural hint, and used to train the
LSTM. Attacker utilizes this LSTM as a predictor model, to
detect the layer sequence of the DNN from run-time traces.
Once the attacker obtains the layer sequence from predictions
and construct the model, the dimensions of each layer are
determined based on the predicted operation and its position
in the time-step and Ô¨Ånal model is extracted.
To evaluate the accuracy of the SCAS attack, the LER
metric has been adopted in literature to measure the difference
between the original and extracted DNN layer sequence, which
we also use in our analysis. LER is calculated as follows:
LER =ED(L;L)
jLj(1)
whereLrepresents the predicted layer sequence, Lrepresents
the ground-truth, and j:jdenotes the length of a sequence.
ED(p;q)denotes the edit distance between the pand
qsequences, i.e., the minimum number of insertions,
substitutions and deletions required to change pintoq(also
referred to as the Levenshtein distance [25]).
III. O URPROPOSED DEFENSE MECHANISM : DNN-A LIAS
We propose a DNN obfuscation method, DNN-Alias, to
thwart SCAS attacks. In this section, we explain the steps of
DNN-Alias in detail and summarize them in Fig. 6. Further,
we discuss the attack model and its assumptions in Sec. III-A.
DNN-Alias takes an unprotected DNN as input and modiÔ¨Åes
its architecture by applying layer obfuscation techniques
(Sec. III-B). The goal is to reduce the diversity in the run-time
behavior of layers in the given DNN. To measure the diversity
of the DNN‚Äôs run-time behavior, DNN-Alias automaticallyOriginal
DNN modelObfuscated 
DNN model
Obfuscated 
DNNsSelectionCrossover &
mutationConverged?
Calculate 
fitnessObfuscation algorithmDNN -Alias 
Cost 
budgetProfiling 
hardwareCandidate 
DNN modelFiltering
featuresStandard
deviationsScaling for 
the budgetFitness functionA
BCFig. 6. Proposed DNN-Alias methodology for DNN architecture protection.
analyzes its proÔ¨Åle during one inference execution (Sec. III-C).
DNN-Alias employs a genetic algorithm to guide the
obfuscation and balance the DNN layers, taking into account
the overhead budget (Sec. III-D).
A. Threat Model and Assumptions
Consistent with most recent related works [26], we
assume that the adversary has no prior knowledge of the
victim DNN architecture, parameters, training algorithms, or
hyper-parameters. We focus on edge security, in which the
attacker has (i) system privilege access to the GPU platform
encapsulating the victim DNN, (ii) the inputs and outputs
(labels) of the DNN, and (iii) a publicly available training
dataset. We show that even with such a powerful threat model,
attackers cannot steal the architecture of the DNNs obfuscated
via our proposed DNN-Alias.
B. DNN Layer Sequence Obfuscation Operations
DNN-Alias uses function-preserving obfuscation operations
to protect the architecture of DNNs. DNN-Alias carefully
apply one (or more) operation to each layer of the original
model, making the target DNN more difÔ¨Åcult to reverse
engineer or attack (Step Ain Fig. 6).
Please note that DNN obfuscation operations have been
used before to protect the DNN architecture [15], [27].
DNN-Alias applies the same obfuscation knobs to change
the run-time proÔ¨Åle of each layer function and hide the
architecture of original DNN. However, existing solutions
either randomly obfuscate the layers or focus on a speciÔ¨Åc
SCAS attack, resulting in weak protection. DNN-Alias guides
the obfuscation differently, thwarting all SCAS attacks. Next,
we explain the obfuscation operations used by DNN-Alias.
Let the matrix W(i)
k1;k2;c;jrepresent the ithconvolutional
layer to be modiÔ¨Åed. k1andk2represent the height and width
of the convolution kernel, respectively, while candjdenote
the input and output channel size, respectively. X(i)and()
denote the input of the layer and the activation function (e.g.,
ReLU ), respectively. Fig. 7 1illustrates the original operator.
Layer Branching. dividing a single layer operator into
smaller, partial operators, as demonstrated in Fig. 7 2. For
example, a 2-D convolution layer ( Conv2D )W(i)
k1;k2;c;jcan be
separated into two partial convolutions, as follows.
U(i)
k1;k2;c;j= 2=W(i)
k1;k2;c;mm2
0;bj
2c
;
V(i)
k1;k2;c;j= 2=W(i)
k1;k2;c;mm2
bj
2c;j (2)
X
YConv
ReLUX
YReLUOriginal DNN Layer Deepening Layer Skipping Layer BranchingConv‚Ä¶
ConvConv1 2 3X X
Conv
ReLUConv
Deepening 
Layer BN Skipping 
Layer+
Y Y4
ReLUFig. 7. Visualization of the employed obfuscation operations [15], [27].
The Ô¨Ånal output is obtained by combining the two partial
results.
U(i)
k1;k2;c;j= 2X(i)jjV(i)
k1;k2;c;j= 2X(i)(3)
The splitting can also be performed in the input channel
dimension. The Ô¨Ånal output in this case is the addition of
the two, as follows, where X(i)is sliced into A(i)andB(i).
U(i)
k1;k2;c=2;jA(i)+V(i)
k1;k2;c=2;jB(i)(4)
Layer Skipping. An additional Conv2D layer U(i+1)
k1;k2;j;j
with all its parameters set to 0is inserted to retain the original
functionality. An illustration of this is shown in Fig. 7 3. The
Conv2D layer can be expressed as follows, with  
X(i+1)
representing the activation output of ithoriginal layer.

X(i+1)
+
U(i+1)X(i+1)
=
X(i+1)
(5)
Layer Deepening. adds a new computational layer to the
sequence. The new layer is inserted after the activation of the
current layer and before the batch normalization (BN) step as
shown in Fig. 7 4. If the previous layer is linear, the newly
added layer U(i+1)is initialized as an identity matrix Ito
preserve the function of the model. Otherwise, U(i+1)
k1;k2;j;jcan
be generalized as:
U(i+1)
a;b;c;d=
1a=k1+1
2^b=k2+1
2^c=d
0 otherwise(6)
Layer deepening is effective as long as the activation function
satisÔ¨Åes the following condition, like the ReLU function.
8x:(x) =(I(x)) (7)
Post obfuscation, the computation graph is extracted and
fed to the TVM¬Æcompiler [28]. The compiler performs
optimizations at the graph and operator levels, generating low-
level optimized code for GPU execution.5
C. Measuring the difference of the layers
DNN-Alias guides the obfuscation algorithm to assemble
a conÔ¨Åguration of obfuscation knobs on a given DNN
(Sec. III-D). For each obfuscated DNN, DNN-Alias analyzes
its run-time execution trace in inference mode and computes
the difference of the levels in the trace using a generic
5DNN obfuscation can be categorized into sequence and dimension
obfuscation. We focus on sequence obfuscation since the sequence
identiÔ¨Åcation stage is the most fundamental step in SCAS attacks.0120000240000360000480000600000Memory Read
0200000040000006000000800000010000000
1
4
7
10
13
16
19
22
25
28
31
34
37
40
43
46
49
52
55
58
61
64
67
70
73
76
79
82
85
88
91
94
97
100Latency
Kernel ID08001600240032004000Memory Write
Original Model         Obfuscated Model80320.6
54447.65128.6
2685.1
Standard Deviation105.2
72.7Fig. 8. Comparison of the run-time trace of each kernel for memory access
time and computational latency. The DNN reads the inputs and weights of
each layer from the memory (Memory Read), executes the function of the
layer (#Cycles) and writes the output of the layer in memory (Memory Write).
DNN-Alias decreases the standard deviation in each trace.
measurement technique (See Bin Fig. 6). In this technique,
the run-time traces are collected and analyzed online and target
hardware is not required to be proÔ¨Åled in advance.
DNN-Alias measures the difference of the levels in a run-
time trace using the standard deviation ( St:D ) of values in a
kernel per each feature in the run-time proÔ¨Åle of the DNN.6
St:D =vuut1
N 1NX
i=1(xi x)2 (8)
Nis number of kernels, xiis the value in run-time trace
andxis the mean of the values. First, the original DNN is
conÔ¨Ågured by incorporating the obfuscation operations. Then,
the obfuscated DNN is executed on the hardware platform
andSt:D of the above features is calculated. DNN-Alias
uses a genetic algorithm to Ô¨Ånd the best conÔ¨Åguration of
the obfuscation operations, by decreasing the St:D value so
that the run-time traces for all layers are similar. Decreasing
theSt:D (difference) enables layer balancing and hides the
function of the kernel from the attacker‚Äôs predictors.
In Fig. 8, we present an example of DNN-Alias obfuscation
by comparing the execution trace of the unprotected (black)
and obfuscated (blue) ResNet-20. The execution of each
layer in the network involves reading weights and inputs
from memory through a kernel process, executing the layer‚Äôs
function, and writing the output back to memory. The
combination of these three values for each trace per kernel
creates a unique pattern, which reveals information to SCAS
attacks and allow them to predict the function of the layer. To
counter this, DNN-Alias reduces the difference between the
traces and forces all layers to have similar run-time traces.
6A standard deviation is a measure of how dispersed the data is in relation
to the mean. A low standard deviation means data are clustered around the
mean, and a high standard deviation indicates data are more spread out.Algorithm 1 DNN-Alias Genetic Algorithm-Based
Obfuscation
1:procedure OBFUSCATION (DNN ,budget ,size p,generations )
2: population generate size pvariations of DNN
3: forgeneration index in0:::generations do
4: listrewards empty list
5: forpopulation element inpopulation do
6: .proÔ¨Åling is done with nvprof on GPU /
7: prof profile (population element )
8: reward layerbalancing sum standard deviations in prof
9: scaling budget scaling (latency; budget )
10: reward reward layerbalancingscaling budget
11: listrewards append reward
12: end for
13: .population update /
14: population select top 50% based on listrewards
15: population crossover and mutate population
16: end for
17:end procedure
Further, in Fig. 8 we observe that in the trace of unprotected
DNN (shown in black), initially there are high values (related
to Convolution functions) that are noticeably distinct from the
two Ô¨Ånal (functions related to the fully connected) layers (red
arrow in Latency trace). However, in the trace of obfuscated
DNN (shown in blue), the processing of large inputs is split
into smaller functions (red box in Memory Read trace) while
some functions are not changed (red boxes in Memory Write
trace). This technique, reduces the differences ( St:D values
shown by yellow box) and makes these two functions almost
indistinguishable in the traces. For example, the St:D value
of the latency trace drops from 80320:6to54447:6, i.e., the
St:D was reduced by 32%, thwarting SCAS attacks.
D. EfÔ¨Åcient exploration of obfuscations
The objective of DNN-Alias is to identify the optimal
arrangement of obfuscation functions while staying within the
cost limit. The cost taken into account is the inference latency
of the DNN. This optimization challenge can be framed as a
limited discrete optimization problem that minimizes the St:D
of the layers with the termination condition of cost budget,
described below. Nis the number of features considered in
proÔ¨Ålers,Sis the set of obfuscation knobs, Tis the latency
with obfuscation, and Tis the latency of original DNN.
min
SNX
i=1St:D i(S)
s.t.T(1 +B)T(9)
To achieve this, we utilize a genetic algorithm to search
the space of potential solutions for random combinations of
obfuscation functions and determine the difference in the layer
kernel-wise values (as outlined in Algorithm 1).
Initial population. For a given DNN, Ô¨Årst, DNN-
Alias automatically creates a random starting population of
obfuscated DNNs with 16 candidates, i.e., population size
sizep= 16 , all with the same original DNN functionality.
The initial random obfuscation procedure is explained next.050000010000001500000
1 51 101 151 201 251 301
DNN models analyzed in the algorithm320Trend of fitness score in DNN -Alias genetic algorithm Fig. 9. Fitness score trend of DNN-Alias on ResNet-20 with a population of
16. Here, we have 320 elements and 20 mutations.
Random DNN obfuscation. A random selection process
is utilized to determine the insertion and conÔ¨Åguration of
obfuscation operations within each layer of the speciÔ¨Åed DNN.
For each obfuscation operation, a binary random decision is
made with a 50% likelihood to determine if the operation will
be utilized or disregarded. As a result, a layer may incur the
application of 1 and 2 (each 37:5%probability), 3 or none
(each 12:5%probability) obfuscation operations.
Fitness function. Then, we evaluate each member of the
population using the Ô¨Åtness score, which is determined by the
St:D and a scaled value of the cost budget, as deÔ¨Åned below.
Fitness =NX
i=1St:D i(S):T (1 +B)T
T2
(10)
Crossover and mutation. For the mating process, we rank
the population based on their Ô¨Åtness scores and select the top
half of individuals with the best scores as parents and add
them to the next candidate pool. Also, parents are brought
to the crossover process, where they combine to form an
equal number of offspring that are added to the pool of
candidates. The offspring are built from the crossover of the
parents‚Äô obfuscation list, then mutated by adding Gaussian
noise. These methods are known as 1-point crossover and
Gaussian mutation in literature. As shown in Fig. 9, the trend
of the Ô¨Åtness score gradually decreases with each pool, and
the best candidates are carried forward to the next generation.
Final solution. The mutation process continues until the
Ô¨Åtness score converges and stabilizes, which we found to occur
after 20 generations in our case.
IV. E XPERIMENTAL SETUP
In this section, we present our experimental setup in
evaluating the effectiveness and security of DNN-Alias.
Hardware. For our experiments, we utilized the Nvidia
RTX 2080 Ti GPU as our experimental platform. However,
our proposed method is generic and applicable to other GPUs
and hardware platforms. We employ Nsight¬ÆCompute [29] for
proÔ¨Åling the GPU and launching the ML-based SCAS attack,
which requires privileged access to the performance counters.
A dataset of randomly generated DNNs was generated to
proÔ¨Åle them on the GPU and train the attack model predictors.
Next, we explain how we create the dataset of random DNNs.
Adversarial 
Sample 
GenerationExtracted DNN Existing DNN
Inception
ResNetGoogleNet
Substitute Models
Target Model
 Dataset
1 2 3Fig. 10. Adversarial attack Ô¨Çow on the extracted DNN architecture.
Obfuscation Algorithm. We implement DNN-Alias using
the PyTorch deep learning framework [30]. The DNNs
(original and obfuscated) are described as Python model Ô¨Åles.
The genetic algorithm is implemented by using pymoo [31].
Random DNN Creation. To train the predictors of SCAS
attack, we use the following method for generating 5,000
different DNNs for image classiÔ¨Åcation, using the CIFAR-10
dataset as a reference. The number of Conv2D layers in each
DNN is randomly selected from the range [4, 12], and the
number of fully connected (FC) layers is randomly selected
from the range [1, 4]. The output channel sizes of Conv2D
layers and the dimensions of FC layers are also randomly
chosen from a range of predeÔ¨Åned values. Some Conv2D
layers are randomly replaced with blocks from the ResNet and
MobileNet networks, and some Conv2D layers are changed
to pooling layers. Batch normalization (BN) layers are added
after each Conv2D and FC layer. All DNNs have 3 input
channels, width and height of 32, and 10 output classes. Using
this technique, we also generate 700 random DNNs to analyze
the performance of DNN-Alias in obfuscating random DNNs.
SCAS-based Adversarial Attack. In an adversarial attack
scenario, the attacker manipulates the output of a DNN by
adding subtle, almost imperceptible alterations to the input
images. The objective of the attack is to Ô¨Ånd the smallest
possible changes in the input that can cause the DNN
to produce incorrect output, either arbitrarily (in case of
untargeted attack) or as pre-determined (in case of targeted
attack). To launch an adversarial attack on a DNN that operates
as a gray box, the attacker often develops a substitute model
by examining the input and output of the victim DNN. With
the help of the SCAS attack, the adversary has access to the
details of target DNN with high accuracy to build the substitute
model. Then, adversarial samples are created using the white-
box substitution technique. Finally, these adversarial samples
are utilized to disrupt the workings of the target DNN.
In summary, Fig. 10 depicts the transfer-based adversarial
attack Ô¨Çow, which includes the following steps:
1) Substitute Models: In this step, we train substitute
models to closely mimic the target model‚Äôs behavior. For
black-box adversarial attacks, the substitute models are
selected from publicly available DNN families. In SCAS-
based adversarial attacks, the substitute model is obtained from
runtime traces. We compare the success rate of adversarial
attacks in both scenarios.
2) Adversarial Sample Generation: The most advanced
methods utilize an ensemble approach to increase the
likelihood of a successful attack, based on the idea that if an
adversarial image is able to fool multiple models, it is more
likely to have a similar effect on the black-box model. We01234
0 100 200 300 400 500 600 700LER of Random DNN Models Obfuscated by DNN -Alias 
DNN Model NumberDNN -Alias
NeurObfuscator
RedLockDNN -Alias shows 
higher LER in a SCAS 
attack than SOTA. Fig. 11. LER of random DNNs obfuscated by DNN-Alias and SOTA
techniques.
follow the same procedure to produce adversarial images for
our target DNNs.
3) Deployment of Adversarial Samples: We pass the
generated adversarial examples as input data to launch an
attack on the gray-box DNN.
V. S ECURITY ANALYSIS AND OVERHEAD RESULTS
In this section, we present the security evaluation and
performance analysis of the DNN-Alias compared to SOTA.
A. Effectiveness of DNN-Alias
In Fig. 12, we show the obfuscation and SCAS attack
evaluation procedure. The original DNN is Ô¨Årst obfuscated by
DNN-Alias (step A). Next, the runtime traces are collected to
extract the DNN architecture via an ML-based SCAS attack
(step B). By comparing the LER between the original and
extracted DNNs, we can analyze the effectiveness of the
obfuscation method (step C). The best case scenario for the
SCAS attack is to obtain an LER close to 0.
1) Effectiveness on Random DNNs: We obfuscate 700
randomly generated DNNs (explained in Sec. IV) using DNN-
Alias, launch the SCAS attack, and measure the security in
terms of the LER (Extracted Obf, Original). These results
are shown in Fig. 11. The cost budget considered in this
experiment is set to 0:2. The minimum LER value observed
in this experiment is 0:3and the maximum is 3:5. The
FORECAST calculation of LER (predicts a future value by
using linear regression) shows 1:2. The average of LER
for SOTA techniques, is shown in Fig. 11, where LER for
NeurObfuscator [15] is 0:62and for ReDLock [17] is 0:73.
Therefore, DNN-Alias obfuscation is 2more resilient
against SCAS attacks compared to SOTA.
2) Effectiveness on Publicly Available DNNs: Further, we
analyzed the effectiveness of DNN-Alias on a set of real DNNs
as a case study. The results are shown in Fig. 13.
First, we launch the ML-based SCAS attack on the original
DNNs, i.e., without any obfuscation. The red bar for LER
(Extracted org, Original) shows the ML-based SCAS predictor
errors with an average of 0:05, indicating that the original
DNNs are completely vulnerable to SCAS attacks. Our goal
is to increase this LER value over 1.
Next, each DNN was obfuscated using DNN-Alias for two
cost budgets ( 0:2and0:6). We launch the same ML-bases
SCAS attack on the obfuscated DNNs.
Original DNN
Overhead 
budget
Obfuscation 
method
Obfuscated DNN
Extract from 
run-time traceSCAS 
predictors
observe 
the 
execution
Extracted DNN
Comparing each pair for LER of layer sequencesAB
CFig. 12. Measuring the accuracy of the SCAS attack, when the DNN is
protected by obfuscation methods.
The blue bar LER (Extracted obf, Original) represents the
difference between the extracted DNN by the SCAS attack
and the original DNN, which averages 1:8for DNN-Alias.
The increase of the LER from an average of 0:05to1:8
demonstrates that DNN-Alias is highly effective in protecting
the DNN against SCAS.
3) Obfuscation Overhead and Cost Budget: In all cases
presented in Fig. 13, we can see that increasing the cost budget
(from 0:2to0:6) increases the LER (from an average of 1:7
to1:96), i.e., leads to stronger obfuscation.
The gray bar LER (Obfuscated, Original) in Fig. 13
represents the difference between the original DNN and the
obfuscated DNN. The LER, in this case, has an average of
0:1, demonstrating that DNN-Alias effectively thwarts SCAS
attacks through minor changes in the original DNN , avoiding
high overhead costs.
4) NeuroUnlock Attack on DNN-Alias: We evaluated
the SOTA NeuroUnlock attack [17] on DNN-Alias and
NeurObfuscator [15]. NeuroUnlock attempts to reverse the
obfuscation of the extracted DNN from the SCAS attack using
sophisticated ML-based models. The green bar labeled ‚ÄúLER
(Recovered, Original)‚Äù in Fig.13 shows the difference between
the original DNN and the recovered DNN using NeuroUnlock
when DNN-Alias is in place. With an average LER of 0:9,
the results indicate that NeuroUnlock failed to accurately
recover the original DNN. In comparison, recovering the
obfuscated DNN using NeuroUnlock with NeurObfuscator
in place resulted in an average LER of 0:31. This suggests
that DNN-Alias is3more robust against de-obfuscation
techniques than the current SOTA methods.
5) Comparison with SOTA: We compare the effectiveness
of DNN-Alias to NeurObfuscator [15] considering the target
real DNNs. We obfuscate the DNNs with 0:2and0:6latency
budgets using DNN-Alias and NeurObfuscator (the code is
open-sourced). We apply the SCAS attack on the obfuscated
DNNs and compare each extracted DNN to the original DNN.
The results in Fig. 14 show that the LER of obfuscated
DNNs using DNN-Alias is 2:5(on average) more than
NeurObfuscator. In summary, DNN-Alias is more effective in
hiding the layer sequence of DNNs compared to SOTA.
B. Performance Analysis
In this section, we examine the effect of DNN-Alias on the
DNN training and the overhead involved in its design. Last
but not least, we study the success rate of adversarial attacks
against DNN-Alias networks.0.12
0.14
0.13
0.13
0.05
0.00
0.11
0.111.82
1.94
1.63
1.99
1.64
1.88
1.74
2.030.13
0.11
0.29
0.21
0.11
0.14
0.19
0.110.94
1.03
0.70
0.82
0.81
1.11
0.82
1.02
00.71.42.1
0.2 0.6 0.2 0.6 0.2 0.6 0.2 0.6Comparison on the performance of DNN -Alias obfuscated DNN models based on latency budget
LER (Extracted_org and Original) LER (Extracted_Obf, Original) LER (Obfuscated, Original) LER (Recovered, Original)LER
Vgg-11 ResNet -20 Vgg-13 ResNet -32Fig. 13. The LER for the DNN-Alias obfuscated DNNs.
LER
0.12
0.14
0.13
0.13
0.05
0.00
0.11
0.111.82
1.94
1.63
1.99
1.64
1.88
1.74
2.030.61
0.71
0.69
0.91
0.65
0.84
0.64
0.88
00.71.42.1
0.2 0.6 0.2 0.6 0.2 0.6 0.2 0.6Comparing the performance of obfuscation methods
Extracted_Org Extracted_obf (DNN-Alias)
Extracted_obf (NeurObfuscator)Vgg-11 ResNet -20 Vgg-13 ResNet -32
Fig. 14. The LER for the DNN-Alias and NeurObfuscator obfuscated DNNs
compared to the original model.
1) Training Performance: To assess the effect of DNN-
Alias on the functionality of the DNN, we compare the
validation accuracy of the original and obfuscated DNNs.
Further, we train the DNNs recovered by the SCAS attack
and by NeuroUnlock attack to see if the high LER values
map to loss in DNN performance. Fig. 15 shows the validation
accuracy of the VGG-11 DNN over 30epochs of training on
the CIFAR-10 dataset [32].
The results demonstrate that DNN-Alias (blue line)
maintains the functionality of the DNN and does not impact
the training performance. However, the recovered DNN by the
SCAS attack (green line) does not work and simply does not
converge, with a 75% drop in performance. Further, even if
NeuroUnlock was launched after the SCAS attack to revert
the obfuscation, the recovered model (black line) still shows a
lower validation accuracy, with a drop of 10%, and converges
about 8epochs later. Therefore, DNN-Alias forces the SCAS
attack to recover a DNN with worse performance compared
to the original DNN .
2) Adversarial Attack: We launch the SCAS-based
adversarial attacks (discussed in Sec. IV) on VGG-11 DNN
obfuscated by DNN-Alias as target model.
In this experiment, the adversarial samples are generated
from the CIFAR10 dataset and the label output of VGG-11.
We study this attack on the original (un-protected) DNN and
then compare it with the obfuscated DNN. Also, we show the
success rate of this attack on similar DNN families.
1. Original Model: To validate the adversarial attack
implementation, we tested adversarial samples generated from
the unprotected model extracted by SCAS. The results,
00.20.40.60.81
0 5 10 15 20 25 29Comparison on the training results
Original
NN-Alias_Obf
Recovered NN
Extracted_obf
Epoch numberValidation accuracyDNN -Alias_obfFig. 15. Validation accuracy after training the models.
represented by 1in Fig. 16, show a success rate of 98%
because the unprotected model closely resembles the original
model.
2. Obfuscated and Recovered Models: Next, we
generate the adversarial samples using the DNN obfuscated
by DNN-Alias and test the target model. The results in
2in Fig. 16 show that the adversarial attack on DNN-
Alias is unsuccessful (success rate 0:2%). Furthermore, we
launch NeuroUnlock [17] after the SCAS attack and we show
(3in Fig. 16) that the success rate increases to 51% on
average. Although NeuroUnlock enhances the performance of
the adversarial attack, the attack is still ineffective due to the
errors in the de-obfuscation process.
3. Public DNN Families: We report the success rate of the
adversarial attack on target DNN when the adversarial samples
are generated using standard DNN families. The success rate
for GoogleNet 4is14%, Inception-V3 5is48%, and ResNet-
34 6is88%. Since the attacker is unaware of the architecture
of the target DNN to choose a similar DNN family, the results
present that DNN-Alias successfully protects the DNN against
SCAS-based adversarial attacks.
C. Overhead Analysis
The results in Fig. 17 show that while DNN-Alias on
average, increases memory access time for both read ( 13%)
and write ( 40%) operations, the computation latency decreases
(25%). Thus, memory access time presents the primary
bottleneck for further increasing the obfuscation level. This
observation opens up opportunities for future research on the
optimization of obfuscation techniques through the use of
efÔ¨Åcient memory protocols.
VI. C ONCLUSION
In this paper, we present a novel obfuscation method called
DNN-Alias to protect deep neural networks (DNNs) against1
23
46
5Fig. 16. Comparison of the average success rate of adversarial attack on VGG-
11 model across various substitute models. Results show that only obfuscation
was effective in mitigating the attack.
Computation latency Memory read
Original 
ObfuscatedMemory write
4 8 12 100 200 300 1100 2200 3300
Timestamps (x10000)
Fig. 17. Effect of DNN-Alias on memory access time and computation
latency. The latency overhead of DNN-Alias is primarily attributed to
increased memory accesses. Computational latency was reduced in most cases.
side-channel attacks. Our proposed method forces all the layers
in a DNN to have similar execution traces, making it difÔ¨Åcult
for attackers to differentiate between the layers and extract
the architecture. DNN-Alias employs a genetic algorithm to
Ô¨Ånd the best combination of layer obfuscation operations to
maximize the security level while maintaining a user-speciÔ¨Åed
latency overhead budget.
The effectiveness of DNN-Alias is demonstrated through
experiments on various randomly generated and publicly
available DNNs. We show that DNN-Alias can successfully
prevent state-of-the-art side-channel architecture stealing
attacks and adversarial attacks while preserving the original
functionality of the DNNs. Our results highlight the potential
of DNN-Alias as a generic and hardware-independent defense
mechanism for DNNs against side-channel attacks.
REFERENCES
[1] ‚ÄúAutopilot.‚Äù [Online]. Available: https://www.tesla.com/autopilot
[2] A. Abdolrahmani, R. Kuber, and S. M. Branham, ‚Äú‚ÄúSiri talks at you‚Äù an
empirical investigation of voice-activated personal assistant (vapa) usage
by individuals who are blind,‚Äù in ASSETS , 2018, pp. 249‚Äì258.
[3] A. R. Rao and D. Clarke, ‚ÄúA comparison of models to predict medical
procedure costs from open public healthcare data,‚Äù in IJCNN . IEEE,
2018, pp. 1‚Äì8.
[4] Z. Ahmad et al. , ‚ÄúAnomaly detection using deep neural network for iot
architecture,‚Äù Applied Sciences , vol. 11, no. 15, p. 7050, 2021.
[5] M. Kesarwani, B. Mukhoty, V . Arya, and S. Mehta, ‚ÄúModel extraction
warning in mlaas paradigm,‚Äù in ACSAC , 2018, pp. 371‚Äì380.
[6] N. Akhtar and A. Mian, ‚ÄúThreat of adversarial attacks on deep learning
in computer vision: A survey,‚Äù IEEE Access , vol. 6, pp. 14 410‚Äì14 430,
2018.[7] F. Khalid et al. , ‚ÄúFaDec: A fast decision-based attack for adversarial
machine learning,‚Äù in IJCNN , 2020, pp. 1‚Äì8.
[8] Y . Xu, X. Zhong, A. J. Yepes, and J. H. Lau, ‚ÄúGrey-box adversarial
attack and defence for sentiment classiÔ¨Åcation,‚Äù arXiv preprint
arXiv:2103.11576 , 2021.
[9] S. J. Oh, M. Augustin, B. Schiele, and M. Fritz, ‚ÄúTowards reverse-
engineering black-box neural networks,‚Äù ICLR , 2018.
[10] X. Hu, L. Liang, S. Li, L. Deng, P. Zuo, Y . Ji, X. Xie, Y . Ding,
C. Liu, T. Sherwood, and Y . Xie, ‚ÄúDeepSniffer: A DNN model extraction
framework based on learning architectural hints,‚Äù in ASPLOS , 2020, p.
385‚Äì399.
[11] Y . Zhang, R. Yasaei, H. Chen, Z. Li, and M. A. A. Faruque, ‚ÄúStealing
neural network structure through remote fpga side-channel analysis,‚Äù
IEEE Transactions on Information Forensics and Security , vol. 16, pp.
4377‚Äì4388, 2021.
[12] M. M ¬¥endez Real and R. Salvador, ‚ÄúPhysical side-channel attacks on
embedded neural networks: A survey,‚Äù Applied Sciences , vol. 11, no. 15,
2021.
[13] H. Chabanne, J.-L. Danger, L. Guiga, and U. K ¬®uhne, ‚ÄúParasite:
Mitigating physical side-channel attacks against neural networks,‚Äù in
SPACE . Springer, 2021, pp. 148‚Äì167.
[14] E. Shi, T.-H. H. Chan, E. Stefanov, and M. Li, ‚ÄúOblivious RAM with
o ((logN) 3) worst-case cost,‚Äù in Advances in Cryptology . Springer,
2011, pp. 197‚Äì214.
[15] J. Li, Z. He, A. S. Rakin, D. Fan, and C. Chakrabarti, ‚ÄúNeurObfuscator:
A full-stack obfuscation tool to mitigate neural architecture stealing,‚Äù in
HOST . IEEE, 2021.
[16] Y . Che and R. Wang, ‚ÄúDnncloak: Secure dnn models against memory
side-channel based reverse engineering attacks,‚Äù in ICCD . IEEE, 2022,
pp. 89‚Äì96.
[17] M. M. Ahmadi, L. Alrahis, A. Colucci, O. Sinanoglu, and M. ShaÔ¨Åque,
‚ÄúNeuroUnlock: Unlocking the architecture of obfuscated deep neural
networks,‚Äù in IJCNN , 2022, pp. 01‚Äì10.
[18] O. Goldreich and R. Ostrovsky, ‚ÄúSoftware protection and simulation on
oblivious RAMs,‚Äù Journal of the ACM , vol. 43, no. 3, pp. 431‚Äì473,
1996.
[19] E. Karimi, Y . Fei, and D. Kaeli, ‚ÄúHardware/software obfuscation against
timing side-channel attack on a GPU,‚Äù in HOST . IEEE, 2020, pp. 122‚Äì
131.
[20] C. Liu, A. Harris, M. Maas, M. W. Hicks, M. Tiwari, and E. Shi,
‚ÄúGhostrider: A hardware-software system for memory trace oblivious
computation,‚Äù in ASPLOS , 2015, pp. 87‚Äì101.
[21] M. Jagielski, N. Carlini, D. Berthelot, A. Kurakin, and N. Papernot,
‚ÄúHigh accuracy and high Ô¨Ådelity extraction of neural networks,‚Äù in
USENIX , 2020, pp. 1345‚Äì1362.
[22] A. S. Rakin, M. H. I. Chowdhuryy, F. Yao, and D. Fan, ‚ÄúDeepSteal:
Advanced model extractions leveraging efÔ¨Åcient weight stealing in
memories,‚Äù in S&P , 2021.
[23] X. Hu, L. Liang, X. Chen, L. Deng, Y . Ji, Y . Ding, Z. Du, Q. Guo,
T. Sherwood, and Y . Xie, ‚ÄúA systematic view of model leakage risks
in deep neural network systems,‚Äù IEEE Transactions on Computers ,
vol. 71, no. 12, pp. 3254‚Äì3267, 2022.
[24] S. Hochreiter and J. Schmidhuber, ‚ÄúLong Short-Term Memory,‚Äù Neural
Computation , vol. 9, no. 8, pp. 1735‚Äì1780, Nov. 1997.
[25] G. Navarro, ‚ÄúA guided tour to approximate string matching,‚Äù ACM
Comput. Surv. , vol. 33, no. 1, p. 31‚Äì88, mar 2001.
[26] E. D. Cubuk, B. Zoph, J. Shlens, and Q. Le, ‚ÄúRandAugment: Practical
automated data augmentation with a reduced search space,‚Äù in NIPS ,
H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, Eds.,
vol. 33, 2020, pp. 18 613‚Äì18 624.
[27] H. Zhu, Z. An, C. Yang, K. Xu, E. Zhao, and Y . Xu, ‚ÄúEENA: efÔ¨Åcient
evolution of neural architecture,‚Äù in ICCVW , 2019, pp. 1891‚Äì1899.
[28] T. Chen, T. Moreau, Z. Jiang, L. Zheng, E. Yan, H. Shen, M. Cowan,
L. Wang, Y . Hu, L. Ceze et al. , ‚ÄúTVM: An automated end-to-end
optimizing compiler for deep learning,‚Äù in USENIX , 2018, pp. 578‚Äì594.
[29] Nvidia nsight systems: https://developer.nvidia.com/nsight-systems.
Accessed: 2022-02-10.
[30] A. e. a. Paszke, ‚ÄúPytorch: An imperative style, high-performance deep
learning library,‚Äù in NIPS , 2019, pp. 8024‚Äì8035.
[31] J. Blank and K. Deb, ‚Äúpymoo: Multi-objective optimization in python,‚Äù
IEEE Access , vol. 8, pp. 89 497‚Äì89 509, 2020.
[32] A. Krizhevsky, ‚ÄúLearning multiple layers of features from tiny images,‚Äù
2009.