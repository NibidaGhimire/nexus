A semi-smooth Newton method for general projection equations
applied to the nearest correlation matrix problem
Nicolas F. Armijoâˆ—Yunier Bello-Cruzâ€ Gabriel Haeserâ€¡
January 10, 2024
Abstract
In this paper, we extend and investigate the properties of the semi-smooth Newton method when
applied to a general projection equation in finite dimensional spaces. We first present results
concerning Clarkeâ€™s generalized Jacobian of the projection onto a closed and convex cone. We
then describe the iterative process for the general cone case and establish two convergence
theorems. We apply these results to the constrained quadratic conic programming problem,
emphasizing its connection to the projection equation. To illustrate the performance of our
method, we conduct numerical experiments focusing on semidefinite least squares, in particular
the nearest correlation matrix problem. In the latter scenario, we benchmark our outcomes
against previous literature, presenting performance profiles and tabulated results for clarity and
comparison.
Keywords: Conic programming, nearest correlation matrix, quadratic programming, semi-
smooth Newton method.
2010 AMS Subject Classification: 90C33, 15A48.
1 Introduction
We begin by considering the following special nonlinear system:
PK(x) +Tx=b, (1)
where K âŠ†Xis a non-nempty, closed and convex cone of a finite dimensional vector space X
with an inner product âŸ¨Â·,Â·âŸ©,PK(x) is the projection of xâˆˆXontoK,bâˆˆX, and T:Xâ†’X
is a linear operator. Some particular cases of equation (1) have been studied, for instance, in
[1â€“5,8,10,14,16,19]. Among those problems, particular attention has been given to the cases where
Kis the n-dimensional non-negative orthant or Lorentzâ€™s cone. For these cases, novel iterative
methods have been proposed; see, for instance, [1,2,4].
âˆ—Department of Applied Mathematics, University of SËœ ao Paulo, Brazil (e-mail: nfarmijo@ime.usp.br ). The
author was supported by Fapesp grant 2019/13096-2.
â€ Northern Illinois University, USA (e-mail: yunierbello@niu.edu ). The author was partially supported by the
NSF Grant DMS-2307328 and by an internal grant from NIU.
â€¡Department of Applied Mathematics, University of SËœ ao Paulo, Brazil (e-mail: ghaeser@ime.usp.br ). The author
was supported by CNPq and Fapesp grant 2018/24293-0.
1arXiv:2401.04657v1  [math.OC]  9 Jan 2024Equation (1) is closely related to the quadratic cone-constrained programming:
min1
2âŸ¨x, QxâŸ©+âŸ¨q, xâŸ©,
s.t. xâˆˆ K,(2)
for a linear operator Q:Xâ†’Xand a vector qâˆˆX. The particularly relevant case occurs when
X=RnandKis either the non-negative orthant or Lorentzâ€™s cone. The connection of (1) with
(2) is established by setting T= (Qâˆ’Id)âˆ’1(where Id is the nÃ—nidentity matrix) and b=âˆ’Tq.
Moreover, the projection onto Kof a solution of equation (1) satisfies the first order necessary
optimality conditions for problem (2). Here, we also prove that this property holds in the general
case, that is, for any closed and convex cone Kin a finite dimensional vector space Xand considering
any linear operator Q:Xâ†’X. Additional linear equality constraints are also considered in (2).
We here focus our attention on the semi-smooth Newton method for solving equation (1). This
method finds a zero of the mapping F:Xâ†’X,
F(x) =PK(x) +Txâˆ’b, x âˆˆX. (3)
By starting at a point x0âˆˆX, the classical semi-smooth Newton method iterates as follows:
xk+1=xkâˆ’h
Fâ€²(xk)iâˆ’1
F(xk), (4)
where Fâ€²(xk) is a generalized Jacobian of Fatxk. This iteration applying to (3) will take the
following simple form:
V(xk) +T
xk+1=b, k âˆˆN, (5)
where V(xk)âˆˆâˆ‚CPK(xk) is a Clarkeâ€™s generalized Jacobian of the projection PK(Â·) atxk. This is
a consequence of the relation V(xk)xk=PK(xk) which we will prove later.
Our approach allows us to consider the relevant case when K=Sn
+, the cone of positive semidef-
inite nÃ—nmatrices, where a subdifferential V(xk) can be computed by the spectral decomposition
ofxk. For this case, we will tackle the quadratic cone-constrained problem (2), where we also
include additional linear constraints; see problem (13) below. This is a well-known optimization
problem with several applications. The goal is to study further the properties of the semi-smooth
Newton method and prove the Q-linear global convergence of the iteration under some standard
conditions on the linear operator Tfor a general cone K, allowing a more detailed study for the
cone of positive semidefinite matrices.
The paper is organized as follows: we start by introducing our notation and presenting some
preliminary results needed in our analysis. In Section 2, we present general results about the
projection operator onto cones and its differentiability. In Section 3, we present the semi-smooth
Newton method and prove the previously mentioned global convergence theorems. Section 4 is
devoted to exploring the relationship between equation (1) and the quadratic conic programming
problem (2), with special emphasis on the case when K=Sn
+, including additional linear constraints.
Finally, we present some numerical experiments for the positive semidefinite cone Sn
+, focusing
particularly on semidefinite least squares problems. Specifically, we include a comparison with the
algorithm from [18] in the context of the nearest correlation matrix problem, which is a well studied
topic in economics [11].
1.1 Notations and preliminaries
In this section, we present some relevant results and definitions that are used in this paper. We
denote the nonnegative integers by Nand by Id the identity operator. The bracket notation âŸ¨Â·,Â·âŸ©is
2referred to the inner product in any finite dimensional space X. Given a linear operator T:Xâ†’X,
we use the notation âˆ¥Tâˆ¥for the operator norm of T, that is, âˆ¥Tâˆ¥:= max {âˆ¥Txâˆ¥ |xâˆˆX,âˆ¥xâˆ¥= 1},
where âˆ¥xâˆ¥:=p
âŸ¨x, xâŸ©is the norm associated with the inner product. We also denote by Tâˆ—its
adjoint linear operator Tâˆ—:Xâ†’X, that is, âŸ¨Tx, yâŸ©=âŸ¨x, Tâˆ—yâŸ©, for all x, yâˆˆX. When Tis self-
adjoint, that is, T=Tâˆ—, we say that Tis positive semidefinite (definite) if âŸ¨Tx, xâŸ© â‰¥0 (>0,
respectively), âˆ€xâˆˆX, xÌ¸= 0 and we denote by Î»min(T) and Î»max(T) the smallest and largest
eigenvalues of T, respectively. For a cone K âŠ†X, the dual of Kis denoted by Kâˆ—={yâˆˆX|
âŸ¨y, xâŸ© â‰¥0,âˆ€xâˆˆ K} . When X=Snis the space of symmetric nÃ—nmatrices with the inner product
âŸ¨A, BâŸ©= trace( AB), A, B âˆˆSn, we consider the self-dual cone K=Sn
+of positive semidefinite
matrices.
The projection of a point xonto a closed and convex cone K Ì¸=âˆ…is denoted by PK(x) and is
defined by PK(x) = argmin {âˆ¥yâˆ’xâˆ¥ |yâˆˆ K} . For a given mapping F:Xâ†’X, we denote the set
where it is differentiable by DFand the Jacobian at a point xâˆˆDFbyFâ€²(x). The set of Clarkeâ€™s
generalized Jacobians at a point xâˆˆXis denoted by âˆ‚CF(x) and it is defined by
âˆ‚CF(x) = conv
lim
kâ†’âˆžFâ€²(xk)|xkâ†’x, xkâˆˆDF
,
that is, the convex hull of all limits of Jacobians of Fnearby x. Throughout this paper, a Clarkeâ€™s
generalized Jacobian of the projection PK(x) will be denoted by V(x), so there is no confusion in
not referring to a particular mapping F.
We will make use of the well-known results below, which we state in the context of a general
finite dimensional inner product space Xas follows.
Theorem 1.1 (Mean Value Theorem [6], Proposition 2.6.5, Page 79) .LetF:Xâ†’Xbe a Lipschitz
mapping. Then, we have
F(y)âˆ’F(x)âˆˆconv ( âˆ‚CF([x, y])) (yâˆ’x),
that is, F(y)âˆ’F(x) =U(z)(yâˆ’x)where U(z)âˆˆâˆ‚CF(z)andzis a convex combination of xand
y.
Lemma 1.1 (Banachâ€™s Lemma [13], Page 351) .LetE:Xâ†’Xbe a mapping onto X. Ifâˆ¥Eâˆ¥<1,
then Eâˆ’Idis invertible and
âˆ¥(Eâˆ’Id)âˆ’1âˆ¥ â‰¤1
1âˆ’ âˆ¥Eâˆ¥.
Lemma 1.2 (Weylâ€™s inequality [13], Theorem 4.3.1, Page 239) .LetA, B:Xâ†’Xbe self-adjoint
linear operators. Then, it holds
Î»min(A) +Î»min(B)â‰¤Î»min(A+B)â‰¤Î»max(A+B)â‰¤Î»max(A) +Î»max(B).
Finally, an important result to ensure the existence and uniqueness of solutions of equation (1)
is the contraction mapping principle.
Theorem 1.2 (Contraction mapping principle [17], Thm. 8.2.2, page 153) .LetÎ¦:Xâ†’Xand
suppose that there exists Î»âˆˆ[0,1)such that âˆ¥Î¦(y)âˆ’Î¦(x)âˆ¥ â‰¤Î»âˆ¥yâˆ’xâˆ¥, for all x, yâˆˆX. Then,
there exists a unique xâˆˆXsuch that Î¦(x) =x.
32 On the projection mapping onto a closed and convex cone
In this section, we study some useful results that will be important in the well-definiteness and
global convergence of the semi-smooth Newton method for equation (1). We begin by presenting
the following result on the properties of generalized Jacobians.
Theorem 2.1. The projection operator PK(Â·)is differentiable almost everywhere. The Jacobian
Pâ€²
K(x)(when it exists) and any generalized Jacobian V(x)âˆˆâˆ‚CPK(x)for all xâˆˆX, are self-adjoint
and positive semidefinite operators. Moreover, the following properties hold:
(i)âˆ¥V(x)âˆ¥ â‰¤1,âˆ€V(x)âˆˆâˆ‚CPK(x)withxâˆˆX.
(ii)Pâ€²
K(x)x=PK(x),âˆ€xâˆˆDPK.
(iii) V(x)x=PK(x),âˆ€V(x)âˆˆâˆ‚CPK(x)withxâˆˆX.
(iv) For all xâˆˆX,
0â‰¤Î»min(V(x))â‰¤Î»max(V(x))â‰¤1,âˆ€V(x)âˆˆâˆ‚CPK(x).
Proof. The fact that the projection is differentiable almost everywhere is well-known due to its
non-expansiveness (that is, the projection is 1-Lipschitz). When Pâ€²
K(x) exists, it is self-adjoint and
positive semidefinite due to Proposition 2.2 of [9]. Now, let xâˆˆXandV(x)âˆˆâˆ‚CPK(x). By
definition we have that there exist V1(x), . . . , V m(x) and {xj
k} âŠ‚DPKsuch that lim kâ†’âˆžxj
k=x,
Pâ€²
K(xj
k)â†’Vj(x),âˆ€j= 1, . . . , m, andV(x) =Pm
j=1Î±jVj(x), withPm
j=1Î±j= 1 and Î±jâˆˆ[0,1] for
allj. By the continuity of the inner product, we can deduce that each Vj(x) is also self-adjoint and
positive semidefinite, and therefore, by linearity, the same holds true for V(x).
To prove item (i), note that âˆ¥Pâ€²
K(xj
k)âˆ¥ â‰¤1 for all jandkdue to non-expansiveness. Hence
âˆ¥V(x)âˆ¥=mX
j=1Î±jVj(x)â‰¤mX
j=1Î±jâˆ¥Vj(x)âˆ¥
=mX
j=1Î±jlim
kâ†’âˆžâˆ¥Pâ€²
K(xj
k)âˆ¥ â‰¤1.
For item (ii), note that since Kis a cone, PK(Â·) is positive homogeneous, that is, PK(tx) =
tPK(x),âˆ€tâ‰¥0 for any xâˆˆX. Let xâˆˆDPK. Ifx= 0, the equality is evident. Assume that xÌ¸= 0.
By the definition of Pâ€²
K(x), we have that
0 = lim
tâ†’0âˆ¥PK(x+tx)âˆ’PK(x)âˆ’tPâ€²
K(x)xâˆ¥
âˆ¥txâˆ¥=âˆ¥PK(x)âˆ’Pâ€²
K(x)xâˆ¥
âˆ¥xâˆ¥.
Hence Pâ€²
K(x)x=PK(x), which proves item (ii).
In order to prove item (iii), by noting that V(x)xâˆ’PK(x) =Pm
j=1Î±j(Vj(x)xâˆ’PK(x)), it is
enough to show that for all j= 1, . . . , m ,Vj(x)x=PK(x). Recalling that Pâ€²
K(xj
k)â†’Vj(x), we have
that
âˆ¥Vj(x)xâˆ’Pâ€²
K(xj
k)xj
kâˆ¥ â‰¤ âˆ¥ Vj(x)xâˆ’Vj(x)xj
kâˆ¥+âˆ¥Vj(x)xj
kâˆ’Pâ€²
K(xj
k)xj
kâˆ¥
â‰¤ âˆ¥Vj(x)âˆ¥âˆ¥xâˆ’xj
kâˆ¥+âˆ¥Vj(x)âˆ’Pâ€²
K(xj
k)âˆ¥âˆ¥xj
kâˆ¥
â†’0.
4Using item (ii) and the continuity of PK(Â·) we conclude that
âˆ¥Vj(x)xâˆ’PK(x)âˆ¥ â‰¤ âˆ¥ Vj(x)xâˆ’PK(xj
k)âˆ¥+âˆ¥PK(xj
k)âˆ’PK(x)âˆ¥
=âˆ¥Vj(x)xâˆ’Pâ€²
K(xj
k)xj
kâˆ¥+âˆ¥PK(xj
k)âˆ’PK(x)âˆ¥ â†’0.
Finally, for item (iv), it is enough to note that 0 â‰¤Î»min(Vj(x))â‰¤Î»max(Vj(x))â‰¤1 for all j=
1, . . . , m due to the fact that Vj(x) is self-adjoint and positive semidefinite with âˆ¥Vj(x)âˆ¥ â‰¤1. The
result now follows easily from Lemma 1.2.
We conclude the section with the following useful result.
Lemma 2.1. Letx, yâˆˆXandV(x)âˆˆâˆ‚CPK(x). Then âˆ¥PK(y)âˆ’PK(x)âˆ’V(x)(yâˆ’x)âˆ¥ â‰¤ âˆ¥ yâˆ’xâˆ¥.
Proof. By Theorem 1.1 we have that
PK(y)âˆ’PK(x)âˆ’V(x)(yâˆ’x) = (V(z)âˆ’V(x))(yâˆ’x),
with V(z)âˆˆâˆ‚CPK(z), where zis a convex combination of xandy. The result follows from the
fact that âˆ¥V(z)âˆ’V(x)âˆ¥ â‰¤1 due to Lemma 1.2 and Theorem 2.1 item (iv).
Item (ii) from Theorem 2.1 provides a foundation for introducing the semi-smooth Newton
method for solving equation (1). Specifically, since the projection can be expressed as
PK(x) =V(x)x,
where V(x)âˆˆâˆ‚CPK(x), we have that F(x) as defined in (3) can be written as
F(x) = (V(x) +T)xâˆ’b,
with V(x) +Tâˆˆâˆ‚CF(x). Then, iteration (4) can be expressed as
xk+1=xkâˆ’(V(xk) +T)âˆ’1[(V(xk) +T)xkâˆ’b] = (V(xk) +T)âˆ’1b.
In the next section, we explore the convergence properties of this iteration.
3 A semi-smooth Newton method for general projection equations
In this section, we define a semi-smooth Newton method for solving equation (1) and study the
convergence along with the sufficient conditions required to achieve it. Our goal is to extend the
application of the semi-smooth Newton method, previously studied in [2, 4], for the cases where
K âŠ†Rnis either the non-negative orthant or Lorentzâ€™s cone. This extension considers any closed
and convex cone K âŠ†X. First, we establish a sufficient condition to the existence and uniqueness
of the solution to the equation (1).
Theorem 3.1 (Sufficient condition for existence and uniqueness of a solution) .IfTis invertible
andâˆ¥Tâˆ’1âˆ¥<1, then equation (1)has a unique solution for any bâˆˆX.
5Proof. Equation (1) has a unique solution if and only if the mapping Î¦( x) =âˆ’Tâˆ’1PK(x) +Tâˆ’1b
has a unique fixed point. Hence, it is sufficient to prove that Î¦ is a contraction and use Theorem
1.2 to guarantee the existence and uniqueness of a fixed point. From the definition of Î¦, we have
Î¦(x)âˆ’Î¦(y) =âˆ’Tâˆ’1(PK(x)âˆ’PK(y)).
Sinceâˆ¥PK(x)âˆ’PK(y)âˆ¥ â‰¤ âˆ¥ xâˆ’yâˆ¥we deduce that âˆ¥Î¦(x)âˆ’Î¦(y)âˆ¥ â‰¤ âˆ¥ Tâˆ’1âˆ¥âˆ¥xâˆ’yâˆ¥concluding that
Î¦ is a contraction since âˆ¥Tâˆ’1âˆ¥<1.
We define the semi-smooth Newton method for the mapping F(x) =PK(x) +Txâˆ’bstarting on
an initial point x0âˆˆXas the iteration
(V(xk) +T)xk+1=b, kâˆˆN (6)
with V(x)âˆˆâˆ‚CPK(x).
Notice that if xkâ†’x, then xis a solution of equation (1). To see this, we subtract ( V(xk)+T)xk
from both sides of (6). Using that PK(xk) =V(xk)xk, we arrive at ( V(xk) +T)(xk+1âˆ’xk) =
bâˆ’PK(xk)âˆ’Txk. Since V(xk) +Tis bounded, the left-hand side converges to zero, while from
the continuity of the projection the right-hand side converges to bâˆ’PK(x)âˆ’Tx. Therefore, xis a
solution of (1).
We start by showing a sufficient condition for stopping the method (6) at a solution.
Proposition 3.1 (Stopping criterion) .IfV(xk+1) =V(xk), then xk+1is a solution of equation
(1).
Proof. From Theorem 2.1 item (iii) and (6), we have that
PK(xk+1) +Txk+1= (V(xk+1) +T)xk+1= (V(xk) +T)xk+1=b.
Now, we show a sufficient condition for the global convergence of iteration (6). Provided certain
conditions regarding the norm of the inverse of Tare met, we can guarantee the existence and
uniqueness of the solution of equation (1). In addition to that, by imposing an additional norm
condition, we obtain linear global convergence.
Theorem 3.2 (Sufficient condition for global Q-linear convergence) .LetbâˆˆXandT:Xâ†’Xbe an
invertible linear operator. Assume that âˆ¥Tâˆ’1âˆ¥<1. Then, equation (1)has a unique solution xand
for any initial point x0, the semi-smooth Newton sequence generated by equation (6)is well-defined.
Additionally, if âˆ¥Tâˆ’1âˆ¥<1
2then the sequence {xk}converges Q-linearly to xand satisfies
âˆ¥xk+1âˆ’xâˆ¥ â‰¤âˆ¥Tâˆ’1âˆ¥
1âˆ’ âˆ¥Tâˆ’1âˆ¥âˆ¥xkâˆ’xâˆ¥, kâˆˆN.
Proof. First we know from Theorem 2.1 that âˆ¥V(x)âˆ¥ â‰¤1 for any xâˆˆX. Since âˆ¥Tâˆ’1âˆ¥<1 we
deduce that âˆ¥Tâˆ’1V(x)âˆ¥<1 for every xâˆˆX. Lemma 1.1 implies that âˆ’Tâˆ’1V(x)âˆ’Id is invertible
and therefore V(x) +Tis also invertible. In particular the semi-smooth Newton method (6) is well
defined. Let xbe the only solution of problem (1) (which exists and is unique due to Theorem 3.1).
So, this point satisfies the relation ( V(x) +T)xâˆ’b= 0. Combining with (6) we deduce that
(V(xk) +T)(xk+1âˆ’x) = (V(x)âˆ’V(xk))x=V(x)xâˆ’V(xk)xkâˆ’V(xk)(xâˆ’xk).
6Since V(x)x=PK(x) and V(xk)xk=PK(xk), using Lemma 2.1, we obtain
âˆ¥xk+1âˆ’xâˆ¥ â‰¤ âˆ¥ (V(xk) +T)âˆ’1âˆ¥âˆ¥(PK(x)âˆ’PK(xk)âˆ’V(xk)(xâˆ’xk))âˆ¥ â‰¤ âˆ¥ (V(xk) +T)âˆ’1âˆ¥âˆ¥xâˆ’xkâˆ¥.
Butâˆ¥(V(xk) +T)âˆ’1âˆ¥=âˆ¥(T(Tâˆ’1V(xk) + Id))âˆ’1âˆ¥ â‰¤ âˆ¥ (Tâˆ’1V(xk)âˆ’Id)âˆ’1âˆ¥âˆ¥Tâˆ’1âˆ¥.Lemma 1.1 and
âˆ¥Tâˆ’1V(xk)âˆ¥<1 implies that
âˆ¥(Tâˆ’1V(xk)âˆ’Id)âˆ’1âˆ¥ â‰¤1
1âˆ’ âˆ¥Tâˆ’1V(xk)âˆ¥â‰¤1
1âˆ’ âˆ¥Tâˆ’1âˆ¥.
Thus, we have that âˆ¥xk+1âˆ’xâˆ¥ â‰¤âˆ¥Tâˆ’1âˆ¥
1âˆ’âˆ¥Tâˆ’1âˆ¥âˆ¥xkâˆ’xâˆ¥withâˆ¥Tâˆ’1âˆ¥
1âˆ’âˆ¥Tâˆ’1âˆ¥<1 due to the assumption that
âˆ¥Tâˆ¥<1
2. Hence, xkconverges Q-linearly to the unique solution x.
The previous result states that with only a norm condition on the operator Tâˆ’1, namely âˆ¥Tâˆ’1âˆ¥<
1
2, we can achieve Q-linear convergence of the method. We prove next that for the case where T
being a positive definite linear mapping, the weaker norm condition âˆ¥Tâˆ’1âˆ¥<1 is sufficient to
ensure Q-linear convergence of the iteration (6) to the unique solution of the problem (1).
Theorem 3.3. LetbâˆˆXandT:Xâ†’Xbe a positive definite linear operator. Then, equation (1)
has a unique solution xâˆˆXand for any initial point x0âˆˆX, the semi-smooth Newton sequence
generated by (6)is well-defined. Additionally, if âˆ¥Tâˆ’1âˆ¥<1then the sequence {xk}converges Q-
linearly to xand satisfies
âˆ¥xk+1âˆ’xâˆ¥ â‰¤ âˆ¥ Tâˆ’1âˆ¥âˆ¥xkâˆ’xâˆ¥, kâˆˆN.
Proof. First notice that from Lemma 1.2 and the positive definiteness of T, it follows that Id + Tis
invertible with âˆ¥(Id + T)âˆ’1âˆ¥<1. Using Moreauâ€™s decomposition [12, Theorem 3.2.5], we can write
anyxâˆˆXasx=PK(x)âˆ’PKâˆ—(âˆ’x). Now, it follows directly that xis a solution of equation (1) if
and only if xis a fixed point of Î¦( x) = (Id + T)âˆ’1(bâˆ’PKâˆ—(âˆ’x)). Since
Î¦(x)âˆ’Î¦(y) = (Id + T)âˆ’1(âˆ’PKâˆ—(âˆ’x) +PKâˆ—(âˆ’y)), x, y âˆˆX,
we deduce that Î¦ is a contraction due to the non-expansiveness of the projection. This gives
existence and uniqueness of a solution of problem (1).
By Theorem 2.1 item (iv) and Lemma 1.2, it follows that V(x) +Tis positive definite with
âˆ¥(V(x) +T)âˆ’1âˆ¥ â‰¤ âˆ¥ Tâˆ’1âˆ¥for all xâˆˆX. Hence, iteration (6) is well-defined for every starting point
x0âˆˆX. The Q-linear convergence when âˆ¥Tâˆ’1âˆ¥<1 now follows from the relation âˆ¥xk+1âˆ’xâˆ¥ â‰¤
âˆ¥(V(xk) +T)âˆ’1âˆ¥âˆ¥xkâˆ’xâˆ¥deduced in the proof of Theorem 3.2.
Note that although the assumption of positive definiteness is sufficient for existence and unique-
ness of the solution without imposing a condition on the norm of Tâˆ’1, it does not guarantee the
convergence of Newtonâ€™s method; see [1, Example 1].
In the following section, we show relevant applications of our results to quadratic conic program-
ming.
4 Application to quadratic conic programming
In this section, we connect equation (1) with the important quadratic conic programming problem
7min1
2âŸ¨x, QxâŸ©+âŸ¨q, xâŸ©,
s.t. xâˆˆ K.(7)
This problem has been widely studied and has multiple applications such as semidefinite least
squares and, in particular, the nearest correlation matrix problem which we will present next.
The Lagrangian of the problem is given by
L(x, Âµ) :=1
2âŸ¨x, QxâŸ©+âŸ¨q, xâŸ© âˆ’ âŸ¨Âµ, xâŸ©,
where Âµâˆˆ Kâˆ—. Then, the well-known complementary KKT conditions are given by
Qx+qâˆ’Âµ= 0,
âŸ¨Âµ,xâŸ©= 0.
Or equivalently,
âŸ¨Qx+q,xâŸ©= 0, Qx+qâˆˆ Kâˆ—,xâˆˆ K. (8)
In order to find a solution to the KKT system (8), we use the following modified projection
equation:
(Qâˆ’Id)PK(x) +x=âˆ’q. (9)
With this in mind, we have the following connection between the solutions of (9) and the ones
of the KKT conditions (8) above. The following theorem is a generalization of Proposition 4 in [4].
Theorem 4.1 (KKT points and solutions of a generalized projection equation) .Ifxis solution of
equation (9), then x=PK(x)is a solution of the KKT system (8). On the other hand, if xis a
solution of system (8), then x=xâˆ’(Qx+q)is a solution of (9).
Proof. For the first part, let xbe a solution of (9). Using Moreauâ€™s decomposition [12, Theorem
3.2.5] for x, we have
x=PK(x)âˆ’PKâˆ—(âˆ’x), (10)
and
âŸ¨PK(x), PKâˆ—(âˆ’x)âŸ©= 0. (11)
By hypothesis we get that
QPK(x) +q=PK(x)âˆ’x=PKâˆ—(âˆ’x)âˆˆ Kâˆ—,
where we used (10) in the last equality. Now using the previous equation and (11), we have
âŸ¨QPK(x) +q, PK(x)âŸ©=âŸ¨PKâˆ—(âˆ’x), PK(x)âŸ©= 0,
implying that PK(x)âˆˆ Ksolves (8).
For the second part, let xbe a solution of (8) and x:=xâˆ’(Qx+q). Since xâˆˆ K,Qx+qâˆˆ Kâˆ—
withâŸ¨Qx+q, xâŸ©= 0, it follows by Moreauâ€™s decomposition that x=PK(x). Thus, replacing xin
(9) we obtain
(Qâˆ’Id)PK(x) +x= (Qâˆ’Id)x+xâˆ’(Qx+q) =âˆ’q.
Therefore, xis a solution of (9), which concludes the proof.
8Theorem 4.1 establishes a significant connection between system (8) and equation (9). In partic-
ular, it asserts that if (9) does not have a solution, then the quadratic conic programming problem
(7) lacks points satisfying the complementary optimality conditions.
We now extend our previous results to the case of equation (9). The proofs of the next three
results use Theorem 3.1 and closely follow the ideas presented in [4], and therefore, we omit some
details for brevity. We begin by presenting two propositions regarding the existence and uniqueness
of solutions as follows:
Proposition 4.1. Ifâˆ¥Qâˆ’Idâˆ¥<1, then equation (9)has a unique solution for any qâˆˆX.
Proof. Similar to the proof of Theorem 3.1 but replacing Tby (Qâˆ’Id)âˆ’1.
Proposition 4.2. IfQis invertible and âˆ¥Qâˆ’1âˆ’Idâˆ¥<1, then equation (9)has a unique solution
for any qâˆˆX.
Proof. Similar to the proof of Theorem 3.1 but defining Î¦( x) = ( Qâˆ’1âˆ’Id)PKâˆ—(âˆ’x)âˆ’Qâˆ’1qand
using that x=PK(x)âˆ’PKâˆ—(âˆ’x) and the fact that the projection PKâˆ—(Â·) is non-expansive.
Next, we specialize our Q-linear convergence results to the case of equation (9).
Theorem 4.2. LetqâˆˆXandQ:Xâ†’Xa linear operator. Assume that Qâˆ’Idis invertible and
âˆ¥Qâˆ’Idâˆ¥<1. Then, (9)has a unique solution x, and for any initial point x0the semi-smooth
Newton sequence generated by (6)is well-defined. Additionally, if âˆ¥Qâˆ’Idâˆ¥<1
2then the sequence
{xk}converges Q-linearly to xand satisfies
âˆ¥xk+1âˆ’xâˆ¥ â‰¤âˆ¥Qâˆ’Idâˆ¥
1âˆ’ âˆ¥Qâˆ’Idâˆ¥âˆ¥xkâˆ’xâˆ¥, kâˆˆN.
Proof. Similar to the proof of Theorem 3.2.
Before stating a result analogous to Theorem 3.3, we need the following lemma.
Lemma 4.1. LetxâˆˆXandV(x)âˆˆâˆ‚CPK(x). IfQis a positive definite linear mapping, then
(Qâˆ’Id)V(x) + Id is invertible.
Proof. By contradiction let us suppose that there exists uÌ¸= 0 with (( Qâˆ’Id)V(x) + Id) u= 0, or
equivalently that
QV(x)u= (V(x)âˆ’Id)u. (12)
Since Qis positive definite and V(x) is self-adjoint, we have that
0â‰¤ âŸ¨QV(x)u, V(x)uâŸ©=âŸ¨V(x)âˆ—QV(x)u, uâŸ©=âŸ¨V(x)QV(x)u, uâŸ©=âŸ¨(V(x)2âˆ’V(x))u, uâŸ© â‰¤0,
where the last inequality is due to Theorem 2.1, item (iv). Then, âŸ¨QV(x)u, V(x)uâŸ©= 0, which
implies that V(x)u= 0. But by (12), we conclude that u= 0, which is a contradiction. Hence, the
operator ( Qâˆ’Id)V(x) + Id is invertible.
Finally, we present a sufficient condition for the convergence of the semi-smooth Newton method
which is analogous to Theorem 3.3 applied to the quadratic conic programming problem when Q
is positive definite.
9Theorem 4.3. LetqâˆˆXandQ:Xâ†’Xa positive definite linear operator. Then, for any initial
point x0âˆˆX, the semi-smooth Newton sequence generated by (6)is well-defined. Additionally, if
Qâˆ’Idis invertible, then equation (9)has a unique solution xâˆˆXand, if âˆ¥Qâˆ’Idâˆ¥<1then{xk}
converges Q-linearly to xsatisfying
âˆ¥xk+1âˆ’xâˆ¥ â‰¤ âˆ¥ Qâˆ’Idâˆ¥âˆ¥xkâˆ’xâˆ¥, kâˆˆN.
Proof. Using Lemma 4.1, the proof follows the lines of the proof of Theorem 3.3.
We next consider an extension of the quadratic conic programming problem (7) by including
additional linear constraints. That is, given an additional linear mapping A:Xâ†’Y, where Yis
also a finite dimensional inner product vector space, and given bâˆˆY, we consider the problem
min1
2âŸ¨x, QxâŸ©+âŸ¨q, xâŸ©,
s.t. Ax=b,
xâˆˆ K.(13)
The Lagrangian function associated with (13) is given by:
L(x, Î», Âµ ) =1
2âŸ¨x, QxâŸ©+âŸ¨q, xâŸ©+âŸ¨Î»,Axâˆ’bâŸ© âˆ’ âŸ¨Âµ, xâŸ©. (14)
where Î»âˆˆYandÂµâˆˆX. The complementary KKT conditions for problem (13) are given by:
Qx+q+Aâˆ—Î»âˆ’Âµ= 0,
Ax=b,
âŸ¨Âµ,xâŸ©= 0,
where the Lagrange multipliers are Âµâˆˆ Kâˆ—andÎ»âˆˆY. This system can be rewritten as the following
complementary system:
* 
Qx+Aâˆ—Î»+q
Axâˆ’b!
, 
x
Î»!+
= 0, 
Qx+Aâˆ—Î»+q
Axâˆ’b!
âˆˆKâˆ—, (x,Î»)âˆˆK, (15)
where K=K Ã—Yand its dual is given by Kâˆ—=Kâˆ—Ã—{0}. Thus, by Theorem 4.1, this system may
be solved by means of the following projection equation
QAâˆ—
A0
âˆ’Id
PK(x, Î») +x
Î»
=âˆ’q
b
, (16)
where a solution ( x,Î») of (16) is such that PK(x,Î») = (PK(x),Î») solves the complementary system
(15). Notice that equation (16) can be rewritten as
 
(Qâˆ’Id)PK(x) +Aâˆ—Î»+x
APK(x)!
= 
âˆ’q
b!
, (17)
and for any starting point x0âˆˆX, the correspondent semi-smooth Newton iteration can be rewritten
as follows:  
(Qâˆ’Id)V(xk)xk+1+xk+1+Aâˆ—Î»k+1
AV(xk)xk+1!
= 
âˆ’q
b!
, (18)
forkâˆˆN. The convergence of the sequence {(xk, Î»k)}generated by (18) is guaranteed by applying
Theorems 4.2 and 4.3 with respect to equation (16).
105 The nearest correlation matrix problem
In this section, we describe the application of the semi-smooth Newton method to the nearest
correlation matrix problem. This specific problem represents a special case of the broader positive
semidefinite least squares problem (referenced as (19) below), a well-studied area notable for its
significant applications and established algorithms; see [11,15].
LetX=Snbe the set of symmetric nÃ—nmatrices with real entries and K=Sn
+âŠ‚Snbe the
cone of positive semidefinite matrices. Given any finite dimensional inner product space Yand a
linear mapping A:Snâ†’Y, we consider the problem
min1
2âˆ¥Xâˆ’Gâˆ¥2,
s.t. A(X) =b,
XâˆˆSn
+,(19)
where GâˆˆSnandbâˆˆYare given and the Frobenius norm is defined as âˆ¥Aâˆ¥=p
âŸ¨A, AâŸ©, where
âŸ¨A, BâŸ©= trace( AB) for A, BâˆˆSn.
Since1
2âˆ¥Xâˆ’Gâˆ¥=1
2âŸ¨X, XâŸ©âˆ’âŸ¨X, GâŸ©, problem (19) is an instance of (13) with Q= Id and q=âˆ’G.
Thus, substituting in (18) we arrive at the following iteration for the semi-smooth Newton method
for finding a solution ( X,Î›)âˆˆSnÃ—Yof (16):
 
Xk+1+Aâˆ—(Î›k+1)
AV(Xk)Xk+1!
= 
G
b!
. (20)
The nearest correlation matrix problem is the particular case where Y=RnandA= diag: Snâ†’
Rnwhich maps a symmetric matrix to its diagonal vector. Its dual Aâˆ—= Diag: Rnâ†’Snmaps a
vector to a diagonal matrix with the given vector in its diagonal. The right-hand side vector bâˆˆRn
will be fixed at the vector eof all ones. Namely, let us consider problem
min1
2âˆ¥Xâˆ’Gâˆ¥2,
s.t. diag( X) =e,
XâˆˆSn
+.(21)
Thus, substituting in (17) we aim at solving the following projection equation for ( X,Î»)âˆˆSnÃ—Rn:
X+ Diag( Î») =G, (22)
diag( PSn
+(X)) =e, (23)
by means of the semi-smooth Newton iteration
Xk+1+ Diag( Î»k+1) =G, (24)
diag( V(Xk)Xk+1) =e, (25)
which is obtained from (20).
From (22) and (24) we deduce that the off-diagonal entries of XandXk+1are equal to the
correspondent off-diagonal entries of G. Thus, by defining Dk+1= Diag(diag( Xk+1)) and Ë†G=
Gâˆ’Diag(diag( G)) we obtain
Xk+1=Dk+1+Ë†G, (26)
Î»k+1= diag( G)âˆ’diag( Dk+1). (27)
11Now, a simple calculation using (26) gives
diag( V(Xk)Xk+1) = diag( V(Xk)(Dk+1+Ë†G)),
= diag( V(Xk)Dk+1) + diag( V(Xk)Ë†G),
= Diag(diag( V(Xk)) diag( Dk+1) + diag( V(Xk)Ë†G).
Thus, substituting in (25) we arrive at the following expression for the iteration Dk+1:
diag( Dk+1) = (Diag(diag( V(Xk)))âˆ’1[eâˆ’diag( V(Xk)Ë†G)], (28)
which gives a fully computable iteration for our Newton method applied to the nearest correlation
matrix problem. In particular, we compute V(X) = UDUTfrom the spectral decomposition
X=UÎ›UT, where Dis a diagonal matrix with Dii= 1 if Î› ii>0 and Dii= 0 if Î› iiâ‰¤0. When
iteration (28) is not defined, we use in our implementation the pseudoinverse, however this never
occurred in the tests we run. We prove next that the iteration is well defined when diag( Xk)>0:
Proposition 5.1. LetXâˆˆSn. Ifdiag( X)>0, then diag( V(X))>0.
Proof. LetX=UÎ›UTandiâˆˆ {1, . . . , n }. Denote by ujthej-th row of U, j= 1, . . . , n . We have
Xii=nX
j=1Î›jj(uj)2
i>0.
Then there exists ksuch that Î› kk(uk)2
i>0. In particular, Î› kk>0 and ( uk)2
i>0. Then,
V(X) =UDUTwith Dkk= 1. Therefore,
V(X)ii=nX
j=1Djj(uj)2
i=Dkk(uk)2
i+X
jÌ¸=kDjj(uj)2
iâ‰¥(uk)2
i>0,
proving the result.
It turns out that in [18], a method closely resembling ours was proposed for the nearest correlation
matrix problem (29). In their approach, a semi-smooth Newton method is applied to the following
function:
ËœF(y) :=APSn
+(G+Aâˆ—y)âˆ’e,
where A= diag. A significant contribution of their work is the provision of a formula for computing
the matrix-vector operation h7â†’Vyh, where Vybelongs to the (B-)subdifferential of ËœFaty,
whithout the need to compute Vyexplicitly. The drawback to this approach is that they must
then resort to an iterative procedure for computing each Newton iteration, without exploiting the
diagonal structure of the operator A. In our approach, we compute the subdifferential explicitly,
which allows us to solve explicitly the diagonal Newtonian linear system. We note that both
methods require computing the full spectral decomposition of an nÃ—nmatrix at each iteration.
The algorithm in [18] also adds a line search procedure in order to ensure global convergence. For
a fairer comparison in our study, we turned off this procedure, although its impact on the methodâ€™s
performance was minimal.
126 Numerical experiments
In order to observe the behavior of the method, we conducted experiments 5.5 through 5.8 as de-
scribed in [18]. In these four experiments the Nearest Correlation Problem is solved using randomly
generated data. We remind the definition of the problem
min1
2âˆ¥Xâˆ’Gâˆ¥2,
s.t. diag( X) =e,
XâˆˆSn
+.(29)
Experiment 5.5 involves generating the matrix GasG:=C+Î±R, where Cis a random correlation
matrix generated by the randcorr Matlab command, Rijâˆˆ[âˆ’1,1] is random, and Î±â‰¥0 is chosen.
In Experiment 5.6, Gis defined as a matrix of random numbers Gijâˆˆ[âˆ’1,1] with Giifixed at 1 for
i= 1, . . . , n . Experiment 5.7 is analogous to 5.6, but in this case Gijâˆˆ[0,2]. Finally, Experiment
5.8 defines Gas
G:=â„“
1âˆ’â„“(Eâ„“âˆ’Idâ„“) 0
0 0
+D+Î±R,
where 1 â‰¤â„“â‰¤n,Î±â‰¥0, Id â„“is the identity matrix of dimension â„“,Eâ„“is a matrix of 1â€™s of dimension
â„“,Dis a random diagonal matrix with Diiâˆˆ[âˆ’20000 ,20000], and Ris a random matrix such
that Rijâˆˆ[âˆ’1,1]. The comparison is made using performance profiles [7]. We built 10 random
problems for each choice of parameters. In Experiment 5.5, n= 3000 is fixed and we test all
values of Î±âˆˆ {0.01,0.1,1,10}. For Experiments 5.6 and 5.7 we test n= 1000 ,2000,3000, and for
Experiment 5.8 we test n= 5000 ,8000,10000 for Î±= 0.001 and â„“=n/2.
The experiments were ran in a 2.30 GHz Intel(R) Core(TM) i5-8300H processor, 16 GB of RAM
and operating system Windows 10 using Matlab 9.5.0.944444 (R2018b). We stopped the execution
of the methods when the Euclidean residual error is smaller than 10âˆ’5and we report the CPU time
required by both methods.
In Experiment 5.5, our method was in general slower than that of Qi and Sun [18]. For Î±= 0.01,
our method took on average 65% more CPU time. This percentage was 137% and 499% for Î±= 1
and 10, respectively. However, for Î±= 0.1, their method took on average 13% more CPU time
than ours. In Figure 1 we present a performance profile for Experiment 5.5 with Î±= 0.1.
In Experiments 5.6 and 5.7, we observed that our method was slower compared to [18]. For
Experiment 5.6 our method took on average 1 .6 times the CPU time required by the method
from [18] for n= 1000, 2 times for n= 2000 and 2 .5 times for n= 3000. The situation was worse
regarding Experiment 5.7 where we observe that our method took around 4 times the total CPU
time required by the method from [18] for n= 1000. It was 7 times slower for n= 2000 and 9
times slower for n= 3000. The situation is much more favorable in Experiment 5.8. In Figures 2a,
2b, and 2c, we present the performance profiles concerning n= 5000, n= 8000, and n= 10000,
respectively. In Table 1 we show the average total time and the average number of iterations for
both methods and all dimensions tested.
We observed that the method of Qi and Sun was faster in all problem with dimension 5000 and in
75% of the problems with dimension 8000, while our method was faster in all problems of dimension
10000. In Table 1 we can see that our method in general requires more iterations to converge, yet
the computation of the iteration is cheaper. This pattern was consistent across other experiments
we conducted. Despite this, the difference in the number of iterations was not significantly enough
in order for the better iteration cost to yield a better overall performance. In Experiment 5.8, we
noted that as the dimension increases, our method becomes more efficient compared to the method
13Figure 1: Performance profile for Experiment 5.5 with Î±= 0.1.
of Qi and Sun as the number of iterations becomes similar. We could not, however, replicate this
phenomenon for different values of Î±. This suggests that although we can exploit the diagonal
structure of the linear system in order to compute Newtonâ€™s direction, while Qi and Sunâ€™s method
resorts to a conjugate gradient method, the subgradient they compute is somewhat more efficient
towards solving the problem than the one we obtain in our approach. Specifically, in Experiments
5.6 and 5.7 where the matrix Gis far from being positive semidefinite, the better subgradient of
Qi and Sun pays off considerably. Nonetheless, our method shows potential superiority for the low
noise level regime ( Î±â‰ˆ0) and high values of nin Experiments 5.5 and 5.8.
(a)n= 5000
 (b)n= 8000
 (c)n= 10000
Figure 2: Performance profiles for Experiment 5.8.
n= 5000 n= 8000 n= 10000
Method time (s) it time (s) it time (s) it
Our 972,99 13 3677,27 10 6880,66 10
Qi-Sun 791,12 8 3588,47 8 7463,56 9
Table 1: Average total time and number of iterations for n= 5000, n= 8000, and n= 10000.
147 Concluding remarks
In this paper, we investigated the global convergence properties of the semi-smooth Newton method
when applied to a general projection equation within finite-dimensional spaces. We have further
highlighted the intrinsic connection between the solutions of this projection equation and the con-
strained quadratic conic programming problem. Comparative experiments on semidefinite least
squares problems, particularly the nearest correlation matrix problem, benchmarked against exist-
ing literature, underscore the efficacy of the proposed method. The methodology introduced herein
paves the way for future research into the versatility and performance of the semi-smooth Newton
method in addressing a broader conic constrained problems via generalized projection equations.
References
[1] F. Armijo, N., Bello Cruz, Y.J. and Haeser, G.: On the convergence of iterative schemes for
solving a piecewise linear system of equations. Linear Algebra and its Applications , 665:291â€“
314, (2023).
[2] Barrios, J.G., Bello Cruz, Y.J., Ferreira, O.P., and NÂ´ emeth, S.Z.: A semi-smooth Newton
method for a special piecewise linear system with application to positively constrained convex
quadratic programming. J. Comput. Appl. Math. , 301:91â€“100, (2016).
[3] Barrios, J.G., Ferreira, O.P., and NÂ´ emeth, S.Z.: Projection onto simplicial cones by Picardâ€™s
method. Linear Algebra Appl. , 480:27â€“43, (2015).
[4] Bello Cruz, Y.J., Ferreira, O.P., NÂ´ emeth, S.Z. and Prudente, L.F.: A semi-smooth Newton
method for projection equations and linear complementarity problems with respect to the
second order cone. Linear Algebra Its Appl. , 513:160â€“181, (2017).
[5] Chen, J. and Agarwal, R.P.: On Newton-type approach for piecewise linear systems. Linear
Algebra and its Applications , 433(7):1463â€“1471, (2010).
[6] Clarke, F.H.: Optimization and Nonsmooth Analysis . Society for Industrial and Applied
Mathematics, second edition edition, (1990).
[7] Dolan, E.D. and MorÂ´ e, J.J.: Benchmarking optimization software with performance profiles.
Mathematical programming , 91:201â€“213, (2002).
[8] Ferreira, O.P. and NÂ´ emeth, S.Z.: Projection onto simplicial cones by a semi-smooth Newton
method. Optim. Lett. , 9(4):731â€“741, (2015).
[9] Fitzpatrick, S. and Phelps, R.R.: Differentiability of the metric projection in hilbert space.
Transactions of the American Mathematical Society , 270(2):483â€“501, (1982).
[10] Griewank, A., Bernt, J.U., Radons, M. and Streubel, T.: Solving piecewise linear systems in
abs-normal form. Linear Algebra and its Applications , 471:500â€“530, (2015).
[11] Higham, N.J.: Computing the nearest correlation Matrixâ€“a problem from finance. IMA
Journal of Numerical Analysis , 22(3):329â€“343, (2002).
15[12] Hiriart-Urruty, J.B. and LemarÂ´ echal, C.: Convex Analysis and Minimization Algorithms II ,
volume 306 of Grundlehren Der Mathematischen Wissenschaften [Fundamental Principles of
Mathematical Sciences] . Springer-Verlag, Berlin, (1993).
[13] Horn, R.A. and Johnson, C.R.: Matrix Analysis . Cambridge University Press, 2 edition,
(2012).
[14] Bello Cruz, Y.J., Prudente, L.F. and Ferreira, O.P.: On the global convergence of the inexact
semi-smooth newton method for absolute value equation. Computational Optimization and
Applications , 65(1):93â€“108, (2016).
[15] Malick, J.: A Dual Approach to Semidefinite Least-Squares Problems. SIAM J. Matrix Anal.
Appl. , 26(1):272â€“284, (2004).
[16] Mangasarian, O.L.: A generalized Newton method for absolute value equations. Optim Lett ,
3(1):101â€“108, (2009).
[17] Ortega, J.: Numerical Analysis: A Second Course . Society for Industrial and Applied Math-
ematics, Philadelphia, (1987).
[18] Qi, H. and Sun, D.: A quadratically convergent Newton method for computing the nearest
correlation matrix. SIAM J. Matrix Anal. Appl. , 28(2):360â€“385, (2006).
[19] Sun, Z., Wu, L. and Liu, Z.: A damped semismooth Newton method for the Brugnanoâ€“Casulli
piecewise linear system. Bit Numer Math , 55(2):569â€“589, (2015).
16