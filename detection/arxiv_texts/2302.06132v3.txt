NNKGC: Improving Knowledge Graph Completion
with Node Neighborhoods
Irene Li1,*, Boming Yang1
1The University of Tokyo, Tokyo, Japan
Abstract
Knowledge graph completion (KGC) aims to discover missing relations of query entities. Current text-
based models utilize the entity name and description to infer the tail entity given the head entity and a
particular relation. Existing approaches also consider the neighborhood of the head entity. However,
these methods tend to model the neighborhood using a flat structure and are only restricted to 1-hop
neighbors. In this work, we propose a node neighborhood-enhanced framework for knowledge graph
completion. It models the head entity neighborhood from multiple hops using graph neural networks to
enrich the head node information. Moreover, we introduce an additional edge link prediction task to
improve KGC. Evaluation on two public datasets shows that this framework is simple yet effective. The
case study also shows that the model is able to predict explainable predictions.
Keywords
Knowledge Graph, BERT, Link Prediction, Graph Convolutional Networks
1. Introduction
A knowledge graph (KG) is a structured representation of entities, their properties, and their
relations [ 1]. It has become increasingly popular in recent years due to its ability to support
various applications, including question answering, recommendation systems, and semantic
search [ 2,3,4]. However, a KG is usually defined by domain experts, so a few relations might
be missing. Knowledge Graph Completion (KGC) aims to discover these missing relations in a
given KG [5].
The methods for KGC mainly fall into two groups: embedding-based and text-based.
Embedding-based methods consider the knowledge graph relations as graph structures, then
learn low-rank embeddings to represent entities and relations, such as TransE [ 6], TuckER
[7] and so on. However, such methods fail in any inductive scenarios. Text-based methods
infer relations based on entities and the corresponding descriptions through representation
learning to solve this problem [ 8,9,10]. KEPLER [ 11] utilizes pretrained language modeling
representation with knowledge embedding to integrate factual knowledge for KGC. The recent
SimKGC [ 12] model applies a contrastive learning approach with new methods for negative
sampling, outperforming several embedding-based methods.
ISWC 2023: Deep Learning for Knowledge Graph(DL4KG), November 06-10, 2023, Athens, Greece
*Corresponding author.
/envel⌢pe-⌢penireneli@ds.itc.u-tokyo.ac.jp (I. Li); boming@g.ecc.u-tokyo.ac.jp (B. Yang)
/orcid0000-0002-1851-5390 (I. Li); 0009-0004-6298-5711 (B. Yang)
©2023 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).
CEUR
Workshop
Proceedingshttp://ceur-ws.org
ISSN 1613-0073
CEUR Workshop Proceedings (CEUR-WS.org)arXiv:2302.06132v3  [cs.CL]  19 Oct 2023Most existing work typically models the head entity ℎand the query relation 𝑟together, then
focuses on the modeling for predicting the relation for the tuple (ℎ, 𝑟,?)to find the correct tail
entity 𝑡. The modeling for (ℎ, 𝑟)tends only to consider the head entity name or descriptions
and the query relation name. Besides, existing approaches also consider the neighborhood of
the head entity [ 13,14]. However, these methods tend to model the neighborhood using a flat
structure so that a simple attention-based method can be applied; or they are only restricted to
1-hop neighbors for computational consideration. In this work, we propose considering the
neighborhood from multiple hops to improve the richness of the head entity information for
further KGC predictions. By doing so, the head entity contains much explicit information that
may help the relation prediction, especially when the head entity name/description is short and
not informative. We model the neighborhood using a graph neural network, making it easy to
propagate information within multiple hops away. Besides, to further enhance the correlation
with these neighbors and the tail entity, we add an extra link prediction loss to the KGC task,
and we show this could improve the KGC performance.
Our main contribution of this work is to propose a node neighborhood-enhanced model
for knowledge graph completion by modeling the neighbors with graph neural networks
[15]. Moreover, the experiment shows that this framework is effective in two public datasets.
Additional case studies show that this model could provide explainable predictions. We release
our code at https://github.com/IreneZihuiLi/NNKGC.
2. Method
We propose the NodeNeighborhood-enhanced model for knowledge Graph Completion
(NNKGC), which consists of the node encoder and the relation prediction module, as shown in
Fig. 1. Following the setting of the previous work [ 12,8], we define the task to be the following:
given an incomplete knowledge graph 𝒢, we need to find the tail entity 𝑡by providing the head
entity ℎand the query relation 𝑟:(ℎ, 𝑟,?).
Node Encoder Typically, we model the head and tail entity nodes separately. In our case,
each entity node contains a node name in a short phrase or a keyword ( name ), and a node
description in a sentence ( desc). The relation 𝑟is usually represented by a phrase or a keyword
(rel). To incorporate the text features into the knowledge graph, we utilize the pre-trained
language models (PLMs), BERT [ 16], to encode the head entity and relations. Each entity node
is represented by:
𝑒𝑛𝑜𝑑𝑒=BERT [𝑛𝑎𝑚𝑒, 𝑑𝑒𝑠𝑐, [SEP] , 𝑟𝑒𝑙] (1)
To further consider the head node neighborhood to enrich the encoding, we apply a graph
neural network [ 17] to model the neighbors. We first collect all the neighbors within 𝑘-hop as a
list of nodes: 𝑛𝑏1, 𝑛𝑏2, ...𝑛𝑏𝑙 . Then we encode the head entity and relation as follows:
𝑋= [𝑒ℎ𝑒𝑎𝑑, 𝑒𝑛𝑏1, 𝑒𝑛𝑏2, ...𝑒 𝑛𝑏𝑙], (2)
𝑒ℎ𝑟=GNN (𝑋, 𝐴), (3)
In which, 𝑋denotes the node embeddings of the entities within the neighborhood, including
the head node, as well as the neighbors, and 𝐴is the adjacency matrix indicating the directedHead NodeTail NodeHead Node NeighborhoodNode  Encoder
GraphEncoderHead Node Neighborhood Embedding
Tail NodeEmbedding
Cosine Similarity
VGAE Loss
Relation PredictionKG LossFigure 1: NNKGC Model Illustration: we initially identify the neighborhood of the head node. Fol-
lowing this, we employ a graph encoder to encode both the head node and its corresponding relation.
Simultaneously, the tail node is encoded. For the final link prediction, we utilize cosine similarity. A
novel aspect of our methodology is the introduction of the VGAE loss. This is optimized in conjunction
with the knowledge graph task loss during the model’s training phase.
connections among the entities. We define the neighborhood of a head node to be the connected
entities, including incoming and outgoing connections. In Fig 1, we illustrate 1-hop (blue) and
2-hop (grey) neighbors and ignore outgoing neighbors for simplicity. There is a number of
choices for the graph encoder, we investigate three types: graph convolutional network (GCN)
[18], graph attention network (GAT) [19], and GraphSAGE [20].
Relation Prediction To make the prediction on the tail entities, we followed the recent
SimKGC [ 12] model. SimKGC introduces a contrastive learning approach to improve the
negative sampling and applies the InfoNCE loss as the loss function. The prediction is simply a
score based on cosine similarity between 𝑒ℎ𝑟and𝑒𝑡. Then the tail entities are ranked by this
score and the largest one is chosen as the prediction. The loss for the tail entity prediction is
denoted as 𝐿𝑘𝑔.
Neighborhood Edge Prediction To further strengthen the effect of neighborhood entities
for the KGC task, we ask the model to predict neighborhood edges (i.e., the highlighted edge
in the figure). In other words, we randomly mask some edges in matrix 𝐴, and let the model
reconstruct these masked relations. There are multiple ways to achieve this, and we apply a
variational graph autoencoder (VGAE) [ 21] to conduct the missing link prediction. It is shown
to be effective for several NLP tasks [ 21,22]. Specifically, we apply vanilla GCNs to model the
mean and standard deviation of the node features, then conduct dot-product to predict the
existence of a given node pair. Note that this is only done for the neighborhood. It will generate
a loss on the reconstructed edges ℒ𝑒𝑑𝑔𝑒. So the final loss becomes:
ℒ=𝜆ℒ𝑘𝑔+ (1−𝜆)ℒ𝑒𝑑𝑔𝑒 (4)
Here, 𝜆is an empirical constant to control the effect of the edge prediction, and it ranges
between 0 and 1.Table 1
Dataset statistics: vocab indicates the vocabulary size; node mean indicates the average number of
tokens for the node descriptions, and the node median indicates the median number of tokens.
dataset entity relation mean median train/valid/test vocab node mean node median
WN18RR 40,943 11 3.7 2 86, 835/3,034/3,134 18,741 17.1 15
FB15k-237 14,541 237 2.8 2 272,115/17,535/20,466 28,538 114.6 76
3. Experiment
Shown in Tab. 1, we conduct knowledge graph completion on two public datasets: FB15k-237
[23], and WN18RR [ 24]. We follow the previous work [ 11,12] to report the following evaluation
metrics: mean reciprocal rank (MRR), Hits@1,3,10.
Hyperparameters The BERT encoder is the pretrained bert-base-uncased model.1AdamW
optimizer [ 25] was applied for training. For modeling the neighborhood, we typically apply
two layers of neural networks, and the dimension for the graph node representation is 768. We
set the number of attention heads for the GAT encoder to 3. We set the 𝜆to 0.2. We conduct
experiments on 4 A100 GPUs (with 40GB memory). Training on WN18RR with 30 epochs took
about 3 hours; training on FB15k237 with 5 epochs took less than one hour. More experimental
hyperparameters can be found in the code URL.
Table 2
Main results on selected datasets. We report MRR, H@1, H@3 and H@10 scores.
FB15k-237 MRR H@1 H@3 H@10
TransE [6] 27.9 19.8 37.6 44.1
TuckER [7] 35.8 26.6 39.4 54.4
MTL-KGC [26] 26.7 17.2 29.8 45.8
SimKGC [12] 33.6 24.9 36.2 51.1
NNKGC 33.2 24.6 35.9 50.4
NNKGC 𝑒 33.8 25.2 36.5 51.5
WN18RR MRR H@1 H@3 H@10
TransE [6] 24.3 4.3 44.1 53.2
TuckER [7] 47.0 44.3 48.2 52.6
MTL-KGC [26] 33.1 20.3 38.3 59.7
SimKGC [12] 66.6 58.7 71.7 80.0
NNKGC 67.3 59.8 71.7 80.5
NNKGC 𝑒 67.4 59.6 72.2 81.2
Main Results We compare with two embedding-based methods: TransE [ 6], and TuckER
[7]. TransE learns low-dimensional embeddings of the entities, and TuckER applies Tucker
decomposition of the binary tensor representation for knowledge graph triples. We also include
text-based methods for baselines. MTL-KGC [ 26] is a multi-task learning method for KGC,
1https://huggingface.co/bert-base-uncasedand SimKGC [ 12]. We compare two settings of our NNKGC model: without the edge loss
ℒ𝑒𝑑𝑔𝑒(NNKGC) and with the edge loss (NNKGC 𝑒). Text-based methods performs better than
embedding-based methods. Without the VGAE loss, our NNKGC is competitive with the best
baseline, SimKGC. In general, our best model NNKGC 𝑒outperforms the selected baselines in
both datasets.
4. Ablation Study
Neighborhood Encoder We then study different neighborhood graph encoders in Eq. 3. We
compare three widely-used graph models: GCN, GAT, and GraphSAGE. In Fig. 2, we report
the average performance on the two datasets. We could observe that GraphSAGE and GAT are
slightly better than the vanilla GCN encoder.
WN FB0.20.40.60.8Avg.GCN
GraphSAGE
GAT
Figure 2: Comparison of multiple neighborhood graph modeling: GCN, GraphSAGE and GAT.
We provide a detailed evaluation of various graph encoders for modeling the head node
neighborhood, shown in Tab. 4 and 3. In general, the GCN encoder is worse than the other two.
In general, GraphSAGE shows better stability in both datasets.
Table 3
Comparison of multiple neighborhood graph modeling: GCN, GraphSAGE, and GAT on the FB15k-237
dataset.
Encoder MRR H@1 H@3 H@10
GCN 32.7 23.8 35.4 50.6
GraphSAGE 33.8 25.2 36.5 51.5
GAT 33.7 25.1 36.2 51.2
Neighborhood Hops As we focus on the neighborhood, we now study the effect of the
neighborhood scale. We conduct experiments on 1-hop, 2-hop and 3-top neighbors to the head
entity. Shown in Fig. 3, we compare the evaluation metrics as well as the average performance
(Avg. on the WN18RR dataset.
We also show the bar chart of how the number of hops affects the performance for FB15k-237
in Fig. 4. One may observe a similar trend to that of WN18RR. As more hops will bring more
neighboring entities, some noises may be added, which may make the performance worse.Table 4
Comparison of multiple neighborhood graph modeling: GCN, GraphSAGE, and GAT on the WN18RR
dataset.
Encoder MRR H@1 H@3 H@10
GCN 65.4 57.0 70.7 80.2
GraphSAGE 65.8 56.4 72.4 81.7
GAT 67.4 59.6 72.2 81.2
Table 5
An example from FB15k-237: we show the ground truth tail and predictions, as well as a selection of
neighbor entities. We highlight direct triggering tokens and other relevant tokens.
Head: Star Wars Episode IV: A New Hope
Head node description: Star Wars, later retitled Star Wars Episode IV: A New Hope, is a 1977
American epic space opera film written and directed by George Lucas...
Relation: nominated for (an award)
Ground Truth Tail: Academy Award for Best Sound Mixing
Predicted Tails: Academy Award for Best Sound Mixing (0.71), BAFTA Award for Best Special
Visual Effects (0.656), Golden Globe Award for Best Original Score (0.65)
Neighbor 1, Name: Academy Award for Best Production Design
Description :The Academy Awards are the oldest awards ceremony for achievements in motion
pictures. The Academy Award for Best Production Design recognizes achievement in art direction on a
film. The category’s original name was Best Art Direction, but was changed to its current name in 2012
for the 85th Academy Awards.
Node relation: nominated for (an award)
Neighbor 2, Name: Ben Burtt
Description :Benjamin Ben Burtt, Jr. is an American sound designer , film editor, director, screenwriter,
and voice actor. He has worked as sound designer on various films including: the Star Wars and Indiana
Jones film series...
Node relation: winner, won (an award)
Neighbor 3, Name: John Williams
Description :John Towner Williams is an American composer , conductor and pianist...
Node relation: music film
In general, integrating more neighbors is not contributing to the performance. Especially,
H@1 drops significantly when the hop number increases. A possible reason might be that more
neighbors may introduce more noises which confuses the knowledge graph completion task.
Moreover, doing so brings more computational burden to the model as more nodes participate
in the graph modeling part. We did not conduct experiments on 4 and more hops, as the total
training time is incredibly long.
Case Study We select an example from FB15k-237 in Tab. 5, and study how the neighbor
entities and corresponding relations help with the KGC task. Given the head entity to be theFigure 3: The effect of hop numbers, results on WN18RR.
1 2 30.60.8
Hop NumberMRR
H@1
H@3
H@10
Avg.
Figure 4: Effect of the Number of Hops, results on FB15k-237.
1 2 30.20.30.4
Number of HopMRR
H@1
H@3
H@10
AVG
Table 6
An example from WN18RR: we show the ground truth tail and predictions, as well as a selection of
neighbor entities. We highlight direct triggering tokens and other relevant tokens.
Head: grant (NN)
Head node description: any monetary aid
Relation: hypernym
Ground Truth Tail: financial aid (NN1)
Predicted Tails: financial aid (NN1) (0.81), grant (NN1) (0.68), foreign aid (NN1)(0.572),
Neighbor 1, Name: grant in aid (NN2)
Description :a grant to a person or school for some educational project
Node relation: hypernym
Neighbor 2, Name: grant (VB2)
Description :give as judged due or on the basis of merit; the referee awarded a free kick to the team; the
jury awarded a million dollars to the plaintiff; funds are granted to qualified researchers .
Node relation: derivationally related form
movie Star Wars IV , and the relation to be nominated for (an award) , our model predicts a ranked
possible tail entity list. The entity Academy Award for Best Sound Mixing achieves the highest
score, which is precisely the ground truth. We also randomly select three 1-hop neighbors,
and their relations with the head entity in the training set. As we can see, the first neighbor is
one of the Academy Award categories, and the relation is the same as the query relation. The
second neighbor is a sound designer. The description shows that this person worked on theTable 7
An example from FB15k-237: we show the ground truth tail and predictions, as well as a selection of
neighbor entities. We highlight direct triggering tokens and other relevant tokens.
Head: Jack London
Head node description: John Griffith Jack London was an American author, journalist, and social
activist. He was a pioneer in the then-burgeoning world of commercial magazine fiction and was one of
the first fiction writers to obtain worldwide celebrity and a large fortune from his fiction alone...
Relation: nationality person people
Ground Truth Tail: United States of America
Predicted Tails: United States of America (0.91), Confederate States of America (0.40), Union (0.34),
Neighbor 1, Name: San Francisco
Description :San Francisco, officially the City and County of San Francisco, is the leading financial and
cultural center of Northern California and the San Francisco Bay Area...
Node relation: place of birth person people
Neighbor 2, Name: Oakland
Description :Oakland, located in the U.S. state of California , is a major West Coast port city and the
busiest port for San Francisco Bay and all of Northern California. It is the third largest city in the San
Francisco Bay Area, the eighth-largest city in the state, and the 47th-largest city
Node relation: location place, lived places
Neighbor 3, Name: University of California, Berkeley
Description :The University of California, Berkeley, is a public research university located in Berkeley,
California, United States . The university occupies 1,232 acres on the eastern side of the San Francisco
Bay with the central campus resting on 178 acres. Berkeley is the flagship institution of the 10 campus
University of California
Node relation: students, graduates educational institution
Star Wars series. The third neighbor is a composer who participates in the music production
for the movie. We highlight some potential triggering tokens in the table. As we can see, some
tokens are highly related to the ground truth tail entity, i.e., neighbor 1, the name Academy
Award . Besides, the token composer andmusic are also semantically related to the ground truth
tail entity name Best Sound Mixing . From this case study, one can notice that the neighborhood
has some positive effects on the KGC task, which is consistent with our motivation. Moreover,
we show that our framework has the potential to explain the predictions, which is an essential
step for further fact verification.
We present two more case studies. Tab. 6 shows a random example from WN18RR. As we
can see, the entity description tends to be shorter, but we can still find text chunks that are
highly related to the ground truth tail entity ( financial aid andfunds ). Similarly, Tab. 7 gives an
example of finding a person’s nationality. We can observe that the neighbors are some relevant
locations about this head entity person (i.e., lives in Oakland ,U.S. state of California ), which
contributes to the final prediction on the tail entity ( United States of America ).5. Conclusion
In this work, we proposed a node neighborhood-enhanced model for knowledge graph comple-
tion, by modeling the neighbors with graph neural networks. We showed that the framework is
simple but effective. Case studies also show that it is possible to provide explainable results.
References
[1]L. Ehrlinger, W. Wöß, Towards a definition of knowledge graphs., SEMANTiCS (Posters,
Demos, SuCCESS) 48 (2016) 2.
[2]T. Steiner, R. Verborgh, R. Troncy, J. Gabarro, R. Van de Walle, Adding realtime coverage
to the google knowledge graph, in: 11th International Semantic Web Conference (ISWC
2012), volume 914, Citeseer, 2012, pp. 65–68.
[3]X. Wang, T. Huang, D. Wang, Y. Yuan, Z. Liu, X. He, T.-S. Chua, Learning intents be-
hind interactions with knowledge graph for recommendation, Proceedings of the Web
Conference 2021 (2021).
[4]L. Liang, Y. Li, M. Wen, Y. Liu, Kg4py: A toolkit for generating python knowledge graph
and code semantic search, Connection Science 34 (2022) 1384 – 1400.
[5]Y. Lin, Z. Liu, M. Sun, Y. Liu, X. Zhu, Learning entity and relation embeddings for
knowledge graph completion, in: AAAI Conference on Artificial Intelligence, 2015.
[6]A. Bordes, N. Usunier, A. García-Durán, J. Weston, O. Yakhnenko, Translating embeddings
for modeling multi-relational data, in: NIPS, 2013.
[7]I. Balazevic, C. Allen, T. Hospedales, TuckER: Tensor factorization for knowledge graph
completion, in: Proceedings of the 2019 Conference on Empirical Methods in Natural
Language Processing and the 9th International Joint Conference on Natural Language
Processing (EMNLP-IJCNLP), Association for Computational Linguistics, Hong Kong,
China, 2019, pp. 5185–5194. URL: https://aclanthology.org/D19-1522. doi: 10.18653/v1/
D19-1522 .
[8]L. Yao, C. Mao, Y. Luo, Kg-bert: Bert for knowledge graph completion, ArXiv abs/1909.03193
(2019).
[9]B. Kim, T. Hong, Y. Ko, J. Seo, Multi-task learning for knowledge graph completion with
pre-trained language models, in: Proceedings of the 28th International Conference on Com-
putational Linguistics, International Committee on Computational Linguistics, Barcelona,
Spain (Online), 2020, pp. 1737–1743. URL: https://aclanthology.org/2020.coling-main.153.
doi:10.18653/v1/2020.coling-main.153 .
[10] T. Safavi, D. Koutra, Relational world knowledge representation in contextual language
models: A review, ArXiv abs/2104.05837 (2021).
[11] X. Wang, T. Gao, Z. Zhu, Z. Zhang, Z. Liu, J. Li, J. Tang, KEPLER: A unified model for
knowledge embedding and pre-trained language representation, Transactions of the
Association for Computational Linguistics 9 (2021) 176–194. URL: https://aclanthology.
org/2021.tacl-1.11. doi: 10.1162/tacl_a_00360 .
[12] L. Wang, W. Zhao, Z. Wei, J. Liu, Simkgc: Simple contrastive knowledge graph completion
with pre-trained language models, ArXiv (2022).[13] J. Zhuo, Q. Zhu, Y. Yue, Y. Zhao, W. Han, A neighborhood-attention fine-grained entity
typing for knowledge graph completion, Proceedings of the Fifteenth ACM International
Conference on Web Search and Data Mining (2022).
[14] A. Borrego, D. Ayala, I. Hernández, C. R. Rivero, D. Ruiz, Cafe: Knowledge graph completion
using neighborhood-aware features, Eng. Appl. Artif. Intell. 103 (2021) 104302.
[15] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, P. S. Yu, A comprehensive survey on graph
neural networks, IEEE Transactions on Neural Networks and Learning Systems 32 (2019)
4–24.
[16] J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, BERT: Pre-training of deep bidirectional
transformers for language understanding, in: Proceedings of the 2019 Conference of
the North American Chapter of the Association for Computational Linguistics: Human
Language Technologies, Volume 1 (Long and Short Papers), Association for Computational
Linguistics, Minneapolis, Minnesota, 2019, pp. 4171–4186. URL: https://aclanthology.org/
N19-1423. doi: 10.18653/v1/N19-1423 .
[17] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, G. Monfardini, The graph neural
network model, in: IEEE International Joint Conference on Neural Networks, IEEE, 2009,
pp. 2187–2192.
[18] T. Kipf, M. Welling, Semi-supervised classification with graph convolutional networks,
ArXiv abs/1609.02907 (2016).
[19] P. Velickovic, G. Cucurull, A. Casanova, A. Romero, P. Lio’, Y. Bengio, Graph attention
networks, ArXiv abs/1710.10903 (2017).
[20] W. L. Hamilton, Z. Ying, J. Leskovec, Inductive representation learning on large graphs,
in: NIPS, 2017.
[21] I. Li, V. Yan, T. Li, R. Qu, D. Radev, Unsupervised cross-domain prerequisite chain learning
using variational graph autoencoders, in: Proceedings of the 59th Annual Meeting of the
Association for Computational Linguistics and the 11th International Joint Conference on
Natural Language Processing (Volume 2: Short Papers), 2021, pp. 1005–1011.
[22] Q. Xie, J. Huang, P. Du, M. Peng, J. Nie, Inductive topic variational graph auto-encoder
for text classification, in: North American Chapter of the Association for Computational
Linguistics, 2021.
[23] K. Toutanova, D. Chen, P. Pantel, H. Poon, P. Choudhury, M. Gamon, Representing text
for joint embedding of text and knowledge bases, in: Conference on Empirical Methods in
Natural Language Processing, 2015.
[24] T. Dettmers, P. Minervini, P. Stenetorp, S. Riedel, Convolutional 2d knowledge graph
embeddings, in: AAAI Conference on Artificial Intelligence, 2017.
[25] I. Loshchilov, F. Hutter, Decoupled weight decay regularization, in: International Confer-
ence on Learning Representations, 2017.
[26] B. Kim, T. Hong, Y. Ko, J. Seo, Multi-task learning for knowledge graph completion with
pre-trained language models, in: International Conference on Computational Linguistics,
2020.