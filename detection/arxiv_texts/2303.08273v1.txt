 Available online at www.sciencedirect.com  
ScienceDirect  
Procedia Computer Science 00 (2023) 000 –000  
www.elsevier.com/locate/procedia  
 
1877 -0509 © 2023  The Authors. Published by Elsevier B.V.  
This is an open access article under the CC BY -NC-ND license ( http://creativecommons.org/licenses/by -nc-nd/4.0/ ) 
Peer-review under re sponsibility of the Conference Program Chairs.   
The 14th International Conference on Ambient Systems, Net works and Technologies (ANT )  
March  15-17, 2023, Leuven , Belgium  
Towards a Deep Learning Pain -Level Detection Deployment at 
UAE for Patient -Centric -Pain Management and Diagnosis Support: 
Framework and Performance Evaluation   
Leila Ismaila,b,c,*, Member, IEEE and Muhammad Danish  Waseemb,c 
 
aClouds and Distributed Computing and Systems (CLOUDS) Lab, School of Computing and Information Systems, Faculty of 
Engineering and Information Technology, The University of Melbourne, Australia 
bIntelligent Distributed Computing and Systems (INDUCE) Research Laboratory , Department of Computer Science and Software 
Engineering, College of Information Technology, United Arab Emirates University, UAE  
cNational Water and Energy Center, United Arab Emirates University, UAE 
 
Abstract  
The outbreak of the COVID -19 pandemic revealed the criticality of timely intervention in a situation exacerbated by a shortage in 
medical staff and equipment .  Pain -level screening is the initial step toward identifying the severity of patient conditions.  Automatic 
recognition of state and feelings help in identifying patient symptoms to take immediate adequate acti on and providing a patient -
centric medical plan tailored to a patient 's state.  In this paper, we propose a framework for pain -level detection for deployment in 
the United Arab Emirates and  assess its performance using the most used approaches in the literature.  Our results show that a 
deployment of a pain -level deep learning detection framework is promising in identifying the pain level accurately.  
 
© 2023  The Authors. Published by Elsevie r B.V.  
This is an open access article under the CC BY -NC-ND license  (http://creativecommons.org/licenses/by -nc-nd/4.0/ ) 
Peer-review under responsibility of the Conference Program Chairs . 
Keywords:  Computer Vision; Deep Learning; eHealth; Image Processing; Machine Learning; Pain Detection ; Patient -Centric; Smart healthcare  
 
 
* Corresponding author. Tel.: +971 -3-7673333; fax: +971 -3-7134343.  
   E-mail address:  leila@uaeu.ac.ae  2 Author name / Procedia Computer Science  00 (2018) 000–000 
1.  Introduction  
The recent Covid -19 outbreak amplified the need for medical staff to take real -time actions due to the severity of 
COVID -19 infection and its rapid spread to the respiratory organs, which in many cases can be fatal [1]. Body language 
is an important way of non -verbal communication to assist in decoding and understanding people 's emotions and 
states.  It can involve many parts of the body, such as facial expressions, hands movement s, voice tone s, and body 
postures and gestures. In this paper, we focus on automated facia l expressions detection that reveal s pain intensity 
levels , as an objective tool to report  a patient’s  pain.  Facial expressions coded with Facial Action Units (FACS), such 
as eyes tightly shut or wrinkles around the nose can provide an objective measure o f pain intensity. Automatic pain -
level detection can help assess  the severity of patients’ condition s and take timely  action s.  It can also aid health allied 
professional s to put in place a patient -centric  approach to pain relief.  To date, pain -level dete ction relies on verbal 
communication from a patient who may not be able to express the severity of the pain or who speaks  a different 
language than the medical  health professional which may add to the ambiguity of the situation.  
The United Arab Emirates i s one of the top countries leading the deployment of smartness in every aspect of its 
residents'  life.  In particular, there is a focus on smart healthcare and smart hospitals  for a patient -centric approach and 
timely intervention  [2] [3] [4] [5] [6] [7] [8]. With the emergence of deep learning, several works evaluated the 
performance of different deep learning algorithms for pain -level detection. However, these algorithms use disparate 
experimental setups [ 4-16], making it difficult to achieve an  objective comparison among them . To the best of our 
knowledge, there exists no comprehensive framework that depicts the process of pain -level detection. In this paper, 
we propose an intelligent deep learning -based pain-level detection framework that would support allied health 
professionals (doctors, pathologists, therapists, and medical technologists) for better diagnosis and prognosis  [9].  This 
is an initial step towards deploying an end -to-end patient -centric smart hospital system that starts with a smart pain -
level screening. The proposed framework is evaluated using the two most used deep  learning approaches for pain -
level detection in the literature (Table 2), VGG -Face and Resnet, to provide a baseline  comparison in a unified setup.  
The rest of the paper is organized as follows: In section II, we present a summary of related work.  Section III 
explains our proposed pain detection framework.  The experimental setup, experiments, and performance evaluation  
are discussed in Section IV.  Section V concludes the paper.  
2. Related Work  
     Table 1. Datasets of Pain Detection from Facial Images and Videos  
Ref Data Name  Year Published  Dataset Size  Dataset Features  
[10] UNBC -McMaster 
Shoulder Pain  2011  25 participants  
200 videos , 48,399 total  frames  Pain Score (16 levels), FACS coded, 
AAM Landmarks, VAS, SEN  
[11] BioVid Heat Pain  2013  87 participants  
17,300 5sec videos with 25fps 4 Pain intensities  
GSR, ECG, and EMG at trapezius muscle  
[12] BP4D -Spontaneous  2014  41 Participants  
320 2D+3D  sequences  
365,000 total frames  2.6 TB of 
video  Happiness/Amusement, Sadness, Startle, 
Embarrassment, Fear, Physical pain, 
Anger, Disgust  
FACS coded, 3D and 2D videos  
[13] BP4D+  2016  140 participants  
10TB+  
1.4 million frames  Happiness/amusement, Sadness , Startle, 
Embarrassment, Fear, Physical pain, 
Anger, Disgust .  
3D, 2D, thermal, heart rate, blood 
pressure, skin conductance (EDA), 
respiration rate, facial features, and FACS 
codes.  
[14] MIntPAIN  2018  20 Participants  
9366 variable -length videos  
1,87,939 frames  5 pain levels  
RGB + thermal +Depth  
FACs=Facial Action Coding, AAM= Active Appearance Model, VAS=Visual Analogue Scale, SEN=Sensory Scale, GSR = Galvanic Skin 
Response, ECG= electrocardiogram, EMG= Electromyography, RGB=Red -Green -Blue  
  Author name / Procedia C omputer Science  00 (2018) 000–000  3 
     Table 2. Work on Deep Learning -based Pain -Level Detection in the Literature.  
Work  Dataset used  Models evaluated  Evaluation metrics  
[15] UNBC -McMaster 
Shoulder Pain  Fine-tuned VGG -Face for feature extraction. PCA for 
dimension reduction. CNN + biLSTM for  classification.  Accuracy, AUC, TP, F -score, 
Precision  
 
[16] UNBC -McMaster 
Shoulder Pain,  
BioVid  Used 3D -Resnet model for the spatiotemporal representation 
of faces in videos.  AUC, MSE  
[17] MIntPAIN,  
UNBC -McMaster 
Shoulder Pain  Fine-tuned VGG -Face for feature extraction, PCA for feature 
selection, and an ensemble of shallow 3 CNN + RNN 
networks for predicting Pain intensity class.  MSE, MAE, Accuracy, AUC,  
F-score  
[18] BioVid  Finetuned VGGFace to distinguish between pain, happiness, 
and disgust  Precision, Recall, F1 Score  
[19] UNBC -McMaster 
Shoulder Pain  MobileNet, GoogleNet, ResNeXt -50, ResNet18  
DenseNet -161 MSE  
[20] UNBC -McMaster 
Shoulder Pain  Joint training of two custom CNN: One for frame sequence 
and the second for facial landmarks  MSE, MAE, Accuracy  
[21] UNBC -McMaster 
Shoulder Pain  Used VGG -Face for estimating frame level PSPI and used a 
fully connected neural network for sequence level pain score 
with predicted PSPI.  AUC, MAE, Intraclass correlation 
coeffici ent 
[22] BioVid Heat Pain 
Dataset  Used Linear Regression, Support Vector Regression, Neural 
Networks Extreme, and Gradient Boosting.  MAE, RMSE  
[23] UNBC -McMaster 
shoulder pain  Used Attention network for pain intensity estimation.  MSE, Accuracy  
[24] UNBC -McMaster 
shoulder Pain,  
MIntPAIN  Used VGG -Face and PCA for feature extraction and 
Temporal Convolution Network for pain intensity 
classification  AUC, Accuracy, MSE, MAE  
[25] AffectNet,  
UNBC -McMaster 
shoulder pain  Trained VGG16 on different emot ion categories and then did 
transfer learning for pain recognition.  F1, Precision, Recall, Accuracy  
[26] UNBC -McMaster 
shoulder pain  ResNet -34 and SVM  Acc, Recall, Precision, F1  
[27] UNBC -McMaster 
shoulder pain,  
BioVid Heat Pain 
Database  Used Deit -Transformers for knowledge distillation, 
GoogleNet  
 Accuracy  
This 
paper  UNBC -McMaster 
Shoulder Pain  Fine -tuned VGG -Face and Resnet -18 to estimate frame -
level PSPI  Accuracy, MAE, MSE  
AUC=Area Under Curve, MAE=Mean Absolute Error, RMSE=Root Mean Squared Error, MSE=Mean Squared Error  
 
Table 1 lists the characteristics of publicly available pain detection datasets. Table 2 compares related work on 
deep learning -based pain -level detection.  It shows that these works use different datasets and performance metr ics for 
evaluation.  In this paper, we propose an intelligent pain -level prediction framework and evaluate it using the two 
most used deep learning algorithms in the literature, providing an objective comparison among them.  
VGG -Face is widely used as a feature extractor for facial images.  [15] extracted the features of the preprocessed 
images through VGG -face before passing them to the hybrid CNN - BiLSTM classifier. [28] proposed a 2 -phased 
ensemble in which, fine -tuned VGG -face is used in the early phase for feature extraction. [18] fine-tuned VGG -Face 
for the three -class problem of distinguishing pain from happiness and disgust and applied explainable AI methods to 
visualize feature explanation. [21] trained VGG -Face to jointly predict frame -level PSPI (Prkachin and Solomon Pain 
Intensity) and individual facial action units. [25] used HSV color spaces images instead of RGB to extract reduced 
features from VGG -Face. Several variants of Resnet are also used in the literature. [16] used the 3D v ariant of Resnet 
to model spatiotemporal features from videos. [19] compared the performance of several deep convolutional neural 
networks including, MobileNet, GoogleNet, ResNeXt -50, ResNet18, and DenseNet -16. 4 Author name / Procedia Computer Science  00 (2018) 000–000 
3. Proposed Deep -Learning Pain -level -Detection  Framework  
Fig. 1 presents the stages of our proposed automatic pain detection framework that is based on the data analytics 
lifecycle.  
 
Fig. 1. Stages of the proposed automatic pain detection framework  
3.1. Domain Conception  
In this stage, the problem of automatic pain detection is studied. Pain detection in images can be performed on full 
body images by analyzing posture s or on facial images by examining expressions. For a patient in bed, we can use 
other features  such as ECG  along  with the camera feed [11]. For accurate pain level detection, it is crucial to 
understand the ambient environment where the camera is placed. This is because m ost pain recognition datasets (Table 
1) provide frontal face images that are taken in a controlled environment. Close -up facial images may provide more 
pain-related features than images taken in an uncontrolled environment . However, we must consider the surrounding s 
and varying environment of the inference time which may differ from the train ing images.  
The pain score should be defined  as a class label, such as, how many pain levels or range of pain intensities would  
be modeled. Instead of predict ing the pain score directly from an image, we can also predict the Facial Action Units 
(FACs), which can help us identify other emotions as well in addition to pain. FACs is the coding system for  muscle 
movements that  define facial expression s in terms of Action Units (AUs) . Table  3 shows  examples of AUs. Some 
AUs have binary values while others have a range of values. The Prkachin and Solomon Pain Intensity (PSPI)  [29] 
is a metric for measuring pain intensity level by simply adding AUs  (Eq. 1). 
 
𝑃𝑎𝑖𝑛  𝑆𝑐𝑜𝑟𝑒  = 𝐴𝑈4 + (𝐴𝑈6 𝑜𝑟 𝐴𝑈7) + (𝐴𝑈9 𝑜𝑟 𝐴𝑈10) + 𝐴𝑈43      (1) 
 
 
 Author name / Procedia C omputer Science  00 (2018) 000–000  5 
Table 3. Example of Facial Action Units  
Action 
Unit Descriptor  
AU4  Brow Lowerer  
AU6  Cheek Raiser  
AU7  Lid Tightener  
AU9  Nose Wrinkler  
AU10  Upper Lip Raise  
AU43  Eyes Closed  
3.2. Data Collection  and Privacy  
We can collect data ourselves over time or use datasets from public repositories (Table 1).  The d ata collection 
process depends upon the domain conception step, which  defines what  features we will be using to train our model. 
In addition to the RGB data, cameras with depth sensors can provide a 3-dimensional understanding of the faces and 
environment [14]. Pain level can be associated with the frames  in 2 ways:  1) As a classification task that classifies 
pain intensities in several classes such as no pai n, less pain, and high pain [11] or 2) as a regression task such as a 
range of pain level from 0 to  15 [10]. Pain intensity can be assessed by the Visual Analogue Scale (VAS) [Fig. 2] 
reported by an expert or the patient.  
 
Fig. 2. Visual Analogue Scale for assessing pain intensity.  
Patient sensitive data is collected via informed consent  and other data privacy -preserving  systems can be used  [30]. 
Datasets shown in Table 1 can be used for academic research upon signing an agreement.  
3.3. Data Understan ding 
Table 1 shows the  publicly available data sets related to pain detection. These datasets provide different features 
and class labels. For example,  [10] provides 16 level s of pain intensities, while [11] has only 4 levels. The pain 
detection data o ften suffer s from the class imbalance problem since the pain is only induced on high intensities for a 
very short time, therefore most frames that have a pain score of zero or very low pain .  To predict the sequence -level 
pain score, the video is divided into smaller chunks and the pain score is estimated for the sequence of images. The 
number of frames per second to process  should be selected , based on available computing resources.  
3.4. Data Preprocessing  
In this stage, the dataset should be pre-processed  for training.  It consists of the  following steps : 
 
• Face detection and cropping : To avoid background noise, we can crop the faces from the images. Off -the-
shelf methods such as DNN or Haar -Cascade face detector from OpenCV  [31] can be used to detect  faces from 
the images. After face detection, the face is cropped  from the image, which will not only reduce the size of the 
image but also provide the most relevant features to be modeled.  
• Image Transformations : Most pre -trained models require an input size of 244 x 244 x 3. Therefore, the images 
need to be transformed into the size required by the model. Normalization of RGB values can also be 
performed . 
• Class Imbalance :  When a dataset is imbalanced, o ver-sampling  and under-sampling techniques can be used 
to reduce the adverse effects of the class imbalance problem  [32]. 
• Data Augmentation :  Several  image transformations  such as jitter, center crop, random erasing, etc.  can be 
used for data augmentation , which  helps the model generalize s better on unse en data  [33]. 
6 Author name / Procedia Computer Science  00 (2018) 000–000 
3.5. Model Building  
The preprocessed data is used to build the deep learning model. The dataset is divided  into training, validation, and 
testing parts. The v alidation set is used for hyperparameter tuning and testing set  for the evaluation of the model on 
the unseen data. Optimal hyperparameters’ values are used  for building the model to improve the prediction 
performance.   At this stage, feature extraction and classification are performed .  Several works  [15], [18], [21], [24], 
[28] use VGG -Face,  which is a CNN architecture  for feature extraction , and pass these features to another neural 
network or linear machine learning model, such as SVM, for classification. Before passing the features to the classifier, 
feature selection an d reduction techniques such as Principal Component Analysis (PCA) can also be applied  [15], 
[24], [28] .  An end -to-end CNN model can  also be trained for feature extraction and classification by adding a linear 
layer at the end of the network for classifi cation.  This linear layer will output the vector with dimension as the number 
of classes. This output is passed to a Softmax layer for obtaining the class probabilities. The c lass with the highest 
probability will be considered as the predicted class . 
3.6. Model Evaluation  
To evaluate a created model,  the k -fold cross -validation or percentage split approach  can be used . Since the number 
of subjects is generally limited in the pain datasets (Table 1), several works use leave -one-out or k -fold cross -
validation  [19], [28], [34] . During the training , the loss function can be weighted by the  number of observations in 
each class in the dataset  to give more weight to the higher pain intensities . Similarly, during the testing, evaluation 
metrics can be weighted as w ell to give more weight s to the minority classes i.e., higher pain levels.   MAE, MSE , F1-
score, Precision, Recall, and Accuracy are widely used as evaluation metrics for pain intensity estimation . 
3.7. Model Deployment  
The developed  validated model is used for automatic pain detection by allied healthcare professionals. Resources 
required for real -time pain detection from videos need to be considered. Continuous lea rning of the model can be done 
with supervised or unsupervised learning .  However, the model  shoul d be evaluated periodically to avoid performance 
degradation over time.  
4. Experiments and Performance Evaluation  
To evaluate the performance of our framework, we implemented and compared the 2 most used deep learning 
algorithms, i.e., VGG -Face and Resne t. 
 
               Table 4. Layers in VGG -16, Resnet -18, and Resnet -34 
Layer  VGG -16 Resnet -18 Resnet -34 
Conv   7 x 7, 64, stride 2  7 x 7, 64, stride 2  
Conv  [3×3,64]×2 [3×3,64
3×3,64] ×2 [3×3,64
3×3,64] ×3 
Conv  [3×3,128 ]×2 [3×3,128
3×3,128] ×2 [3×3,128
3×3,128] ×4 
Conv  [3×3,256 ]×3 [3×3,256
3×3,256] ×2 [3×3,256
3×3,256] ×6 
Conv  [3×3,512 ]×3 [3×3,512
3×3,512] ×2 [3×3,512
3×3,512] ×3 
FC 4096 -d FC    
FC 4096 -d FC    
FC 1000 -d FC  1000 -d FC  1000 -d FC  
FLOPS  1.55×1010 1.8×109 3.6×109 
Parameters  134.7 M  11.4 M  21.5 M  
Conv = Convolutional Layer, FC = Fully Connected Layer,  
FLOPS = Floating Point Operations, M = Million  
  Author name / Procedia C omputer Science  00 (2018) 000–000  7 
VGG -Face [35]: It has the same architecture as VGG16 [36], having 16 convolutional and fully connected layers. 
However, the difference between VGG16 and VG G-Face is that VGG -Face has been trained on a large -scale facial 
image dataset. Since VGG -Face is pre -trained on face images it is widely used as a feature extractor for facial images.  
 
Resnet [37]: As neural networks become deeper their performance sta rt to degrade due to vanishing gradient 
problem. Resnet uses skip connections between one or more layers which resolves the issue of vanishing gradients, 
which allows us to train deeper models. There are different variants of Resnet based on the number of layers used, 
such as, Resnet -18 having 18 layers and Resnet -34 having 34 layers.  Table 4 shows the architecture of these models 
along with the number of FLOPS and parameters.  
 
We used UNBC -McMaster shoulder pain  [10] datase t in which a camera was placed  facing the patients having 
pain.  Table 5 shows the hyperparameters we have used for model training. We have used 2 Nvidia RTX -5000 GPUs 
with total memory of 32 GB. The models were  trained to predict frame level PSPI score.  
Table 5. Value(s) of Hyperparam eters used in the Literature and our Experiments for the Algorithms Under Study.  
Algorithm  Hyperparameters  Value(s) used in the literature  Value(s) used in our experiments  
VGG -Face   Optimizer  Adam [12]  
Adam Optimizer  
Learning Rate = 0.001  
5-Fold Cross Validation  
100 epochs  
Batch size = 256  
 Cross Validation  10-Fold [14], 5 -fold [18]  
Epochs  50 [14]  
Resnet  Optimizer  SGD [13], Adam [16]  
Cross Validation  Leave -one-out [16]  
Epochs  200 [13] [16], 5,20,30 [23]  
Batch Size  64 [13], 8,32,128 [23]  
Learning rate  0.01 [13]  
 
For data pre -processing, we  have used 2 OpenCV [31] Haar -Cascade face detectors  for getting bounding boxes for 
faces from raw frames. One for frontal face images and one for side face images. If the face is not detected by the 
frontal face detector, then we pass it to side face detector. Images were then cropped to only retain the fa ce part and 
remove the background. We resized the cropped images into 244 x 244 x 3. We used Random Horizontal Flip for data 
augmentation. Lastly, before passing the image to the model we normalized the images.  
 
To validate the models under study , 5-fold c ross-validation is used.  The dataset contains total 25 subjects. In each 
fold we take 15 subjects for training, 5 for validation, and left 5 for testing. We used validation set for evaluating the 
model during training. If the MAE does not improve for 20 ep ochs, we did early stopping to avoid overfitting.  The 
model from epoch having minimum MAE was used for testing.  We evaluate d the performance  of the models in terms 
of MAE, MSE , and Accuracy.   We performed end -to-end training of both the models with weighte d Cross -Entropy 
Loss. Weighted loss is used to mitigate the effects of class imbalance on the models. The weights are calculated with 
Eq. 2. 
 
𝑊𝑒𝑖𝑔 ℎ𝑡𝑠 𝑜𝑓 𝐶𝑙𝑎𝑠𝑠  𝑐= (1
𝑠𝑎𝑚𝑝𝑙𝑒𝑠  𝑜𝑓 𝐶𝑙𝑎𝑠𝑠  𝑐)(𝑇𝑜𝑡𝑎𝑙  𝑆𝑎𝑚𝑝𝑙𝑒𝑠
𝑁𝑢𝑚𝑏𝑒𝑟  𝑜𝑓 𝐶𝑙𝑎𝑠𝑠𝑒𝑠)  (2) 
 
Table 6 shows the results from VGG -Face , Resnet18  and Resnet34 . The average Accuracy, MAE, and MSE for 5 
folds is reported.  
     Table 6. Results of VGGFace  and Resnet18 on UNBC -McMaster Dataset  
Model  MAE  MSE  Accuracy  
VGG -Face  0.3589  1.7273  82.14 
Resnet18  0.4073  2.5002  87.89  
Resnet34  0.5521  1.7727  78.04 
 8 Author name / Procedia Computer Science  00 (2018) 000–000 
 
Fig. 3. MAE, MSE, and Accuracy comparison of VGG -Face, Resnet18 , and Resnet34  
As shown in Fig. 3, Resnet18 has the highe st accuracy  among the algorithms under study. H owever, VGG -Face 
has lowe st MAE and MSE, which show how  far our predicted pain scores  are from the true values . This means on 
average, the wrongly predicted scores by Resnet18 are further  from the true value as compared to the predictions of 
VGG -Face  and Resnet34 . MSE penalizes large errors more than MAE.  MSE of Resnet18 is higher than VGG -Face  
and Resnet34, meaning that a patient who is in pain would not be immediately treated, impacting the patient health.  
Resnet -34 has a greater number of layers, but its accuracy is lower than the other 2 models. It shows that increasing 
the model size does not necessarily increase the performance. A larger model may tend to overfit given  the limited 
amount of data.  
5. Concl usion  
We presented a framework for pain detection from images and videos  to contribute to  the UAE vision of smart 
healthcare and smart hospitals  enabled by big data analytics and artificial intelligence . We evaluated  the framework 
on two widely used deep l earning models in the literature i.e., VGG -Face and Resnet. Our results show that this 
framework can provide health practitioners a preliminary checkup metric with very good  accuracy.  Accuracy should 
not be the sole performance metric. MSE must be consider ed, as incr ease in MSE  would mean the patients with high 
pain levels could be neglected.  In future work , we plan to deploy our framework in UAE hospitals emergency units . 
   
Acknowledgment  
This research was funded by the National Water and Energy Centre of  the United Arab Emirates University (Grant 
31R215).  
References  
[1] S. F. Lax et al. , “Pulmonary Arterial Thrombosis in COVID -19 With Fatal Outcome,” Ann Intern Med , vol. 173, no. 5, pp. 350 –361, 
Sep. 2020, doi: 10.7326/M20 -2566.  
[2] “MINISTERIAL FORWARD E XECUTIVE SUMMARY,” 2031, Accessed: Nov. 23, 2022. [Online]. Available: https://ai.gov.ae/  
[3] T. Chomutare et al. , “Artificial Intelligence Implementation in Healthcare: A Theory -Based Scoping Review of Barriers and 
Facilitators,” Int J Environ Res Public Health , vol. 19, no. 23, p. 16359, Dec. 2022, doi: 10.3390/ijerph192316359.  
[4] L. Ismail, A. Hennebelle, H. Materwala, J. al Kaabi, P. Ranjan, and R. Janardhanan, “Secure and Privacy -Preserving Automated End -
to-End Integrated IoT -Edge -Artificial Intellige nce-Blockchain Monitoring System for Diabetes Mellitus Prediction,” Nov. 2022, 
[Online]. Available: http://arxiv.org/abs/2211.07643  
[5] L. Ismail and R. Buyya, “Artificial Intelligence Applications and Self -Learning 6G Networks for Smart Cities Digital Eco systems: 
Taxonomy, Challenges, and Future Directions,” Sensors , vol. 22, no. 15, p. 5750, Aug. 2022, doi: 10.3390/s22155750.  
[6] L. Ismail, H. Materwala, Y. al Hammadi, F. Firouzi, G. Khan, and S. R. bin Azzuhri, “Automated artificial intelligence -enabled 
proactive preparedness real -time system for accurate prediction of COVID -19 infections — Performance evaluation,” Front Med 
(Lausanne) , vol. 9, Aug. 2022, doi: 10.3389/fmed.2022.871885.  
[7] L. Ismail, H. Materwala, M. Tayefi, P. Ngo, and A. P. Karduck, “Typ e 2 Diabetes with Artificial Intelligence Machine Learning: 
Methods and Evaluation,” Archives of Computational Methods in Engineering , vol. 29, no. 1, pp. 313 –333, Jan. 2022, doi: 
10.1007/s11831 -021-09582 -x. 
[8] L. Ismail and H. Materwala, “From Conception  to Deployment: Intelligent Stroke Prediction Framework using Machine Learning and 
Performance Evaluation,” in 2022 IEEE International Conference on Omni -layer Intelligent Systems (COINS) , Aug. 2022, pp. 1 –7. doi: 
10.1109/COINS54846.2022.9854961.  
[9] L. Ismail, H. Materwala, A. P. Karduck, and A. Adem, “Requirements of Health Data Management Systems for Biomedical Care and 
Research: Scoping Review,” J Med Internet Res , vol. 22, no. 7, 2020, doi: 10.2196/17508.  
[10] P. Lucey, J. F. Cohn, K. M. Prkachin, P. E . Solomon, and I. Matthews, “Painful data: The UNBC -McMaster shoulder pain expression 
archive database,” in 2011 IEEE International Conference on Automatic Face and Gesture Recognition and Workshops, FG 2011 , 
2011, pp. 57 –64. doi: 10.1109/FG.2011.5771462.  
[11] S. Walter et al. , “The biovid heat pain database: Data for the advancement and systematic validation of an automated pain recognition,” 
in 2013 IEEE International Conference on Cybernetics, CYBCONF 2013 , 2013, pp. 128 –131. doi: 10.1109/CYBConf.2013.66 17456.  
[12] X. Zhang et al. , “BP4D -Spontaneous: A high -resolution spontaneous 3D dynamic facial expression database,” Image Vis Comput , vol. 
32, no. 10, pp. 692 –706, 2014, doi: 10.1016/j.imavis.2014.06.002.  
                   
                             
             
                             
                    
                                   Author name / Procedia C omputer Science  00 (2018) 000–000  9 
[13] Z. Zhang et al. , “Multimodal Spontaneous Emotion Corpus for Human Behavior Analysis.” [Online]. Available: 
http://www.biopac.com  
[14] M. A. Haque et al. , “Deep multimodal pain recognition: A database and comparison of spatio -temporal visual modalities,” in 
Proceedings - 13th IEEE International Conference on Automatic Face and Gesture Recognition, FG 2018 , Jun. 2018, pp. 250 –257. 
doi: 10.1109/FG.2018.00044.  
[15] G. Bargshady, X. Zhou, R. C. Deo, J. Soar, F. Whittaker, and H. Wang, “Enhanced deep learning algorithm developm ent to detect pain 
intensity from facial expression images,” Expert Syst Appl , vol. 149, Jul. 2020, doi: 10.1016/j.eswa.2020.113305.  
[16] M. Tavakolian and A. Hadid, “A Spatiotemporal Convolutional Neural Network for Automatic Pain Intensity Estimation fro m Facial 
Dynamics,” Int J Comput Vis , vol. 127, no. 10, pp. 1413 –1425, Oct. 2019, doi: 10.1007/s11263 -019-01191 -3. 
[17] G. Bargshady, X. Zhou, R. C. Deo, J. Soar, F. Whittaker, and H. Wang, “Ensemble neural network approach detecting pain intens ity 
from fa cial expressions,” Artif Intell Med , vol. 109, p. 101954, Sep. 2020, doi: 10.1016/j.artmed.2020.101954.  
[18] K. Weitz, T. Hassan, U. Schmid, and J. U. Garbas, “Deep -learned faces of pain and emotions: Elucidating the differences of facial 
expressions with the help of explainable AI methods,” Technisches Messen , vol. 86, no. 7 –8, pp. 404 –412, Jul. 2019, doi: 
10.1515/teme -2019 -0024.  
[19] S. el Morabit, A. Rivenq, M. E. N. Zighem, A. Hadid, A. Ouahabi, and A. Taleb -Ahmed, “Automatic pain estimation from facial  
expressions: A comparative analysis using off -the-shelf cnn architectures,” Electronics (Switzerland) , vol. 10, no. 16, Aug. 2021, doi: 
10.3390/electronics10161926.  
[20] A. Semwal and N. D. Londhe, “Computer aided pain detection and intensity estimation u sing compact CNN based fusion network,” 
Appl Soft Comput , vol. 112, Nov. 2021, doi: 10.1016/j.asoc.2021.107780.  
[21] X. Xu et al. , “Pain Evaluation in Video using Extended Multitask Learning from Multidimensional Measurements,” 2020.  
[22] F. Pouromran, S. Radhakrishnan, and S. Kamarthi, “Exploration of physiological sensors, features, and machine learning models for 
pain intensity estimation,” PLoS One , vol. 16, no. 7, p. e0254108, Jul. 2021, doi: 10.1371/journal.pone.0254108.  
[23] X. Xin, X. Lin, S. Yang, and X. Zheng, “Pain intensity estimation based on a spatial transformation and attention CNN,” PLoS One , 
vol. 15, no. 8, p. e0232412, Aug. 2020, doi: 10.1371/journal.pone.0232412.  
[24] G. Bargshady , X. Zhou, R. C. Deo, J. Soar, F. Whittaker, and H. Wang, “The modeling of human facial pain intensity based on 
Temporal Convolutional Networks trained with video frames in HSV color space,” Applied Soft Computing Journal , vol. 97, Dec. 
2020, doi: 10.1016/ j.asoc.2020.106805.  
[25] P. Prajod, D. Schiller, T. Huber, and E. André, “Do Deep Neural Networks Forget Facial Action Units? -- Exploring the Effects of 
Transfer Learning in Health Related Facial Expression Recognition,” Apr. 2021, doi: 10.1007/978 -3-030-93080 -6_16.  
[26] K. Pikulkaew, W. Boonchieng, E. Boonchieng, and V. Chouvatut, “2D Facial Expression and Movement of Motion for Pain 
Identification with Deep Learning Methods,” IEEE Access , vol. 9, pp. 109903 –109914, 2021, doi: 10.1109/ACCESS.2021.3101396.  
[27] S. el Morabit and A. Rivenq, “Pain Detection From Facial Expressions Based on Transformers and Distillation,” in 2022 11th 
International Symposium on Signal, Image, Video and Communications (ISIVC) , May 2022, pp. 1 –5. doi: 
10.1109/ISIVC54825.2022.980 0746.  
[28] “3-Ensemble neural network approach detect..”, doi: https://doi.org/10.1016/j.artmed.2020.101954.  
[29] K. M. Prkachin and P. E. Solomon, “The structure, reliability and validity of pain expression: Evidence from patients with sh oulder 
pain,” Pain, vol. 139, no. 2, pp. 267 –274, Oct. 2008, doi: 10.1016/j.pain.2008.04.010.  
[30] L. Ismail, H. Materwala, and A. Hennebelle, “A Scoping Review of Integrated Blockchain -Cloud (BcC) Architecture for Healthcare: 
Applications, Challenges and Solutions,” Senso rs, vol. 21, no. 11, p. 3753, May 2021, doi: 10.3390/s21113753.  
[31] “OpenCV: Face Detection using Haar Cascades.” https://docs.opencv.org/3.4/d7/d8b/tutorial_py_face_detection.html (accessed No v. 
23, 2022).  
[32] A. Fernández, S. García, M. Galar, R. C. Pr ati, B. Krawczyk, and F. Herrera, Learning from imbalanced data sets . 2018.  
[33] C. Shorten and T. M. Khoshgoftaar, “A survey on Image Data Augmentation for Deep Learning,” J Big Data , vol. 6, no. 1, p. 60, Dec. 
2019, doi: 10.1186/s40537 -019-0197 -0. 
[34] “15-Exploration of physiological sensors, f..”.  
[35] O. M. Parkhi, A. Vedaldi, and A. Zisserman, “Deep Face Recognition,” in Procedings of the British Machine Vision Conference 2015 , 
2015, pp. 41.1 -41.12. doi: 10.5244/C.29.41.  
[36] K. Simonyan  and A. Zisserman, “Very Deep Convolutional Networks for Large -Scale Image Recognition,” Sep. 2014, [Online]. 
Available: http://arxiv.org/abs/1409.1556  
[37] K. He, X. Zhang, S. Ren, and J. Sun, “Deep Residual Learning for Image Recognition,” in 2016 IEEE C onference on Computer Vision 
and Pattern Recognition (CVPR) , Jun. 2016, pp. 770 –778. doi: 10.1109/CVPR.2016.90.  
  