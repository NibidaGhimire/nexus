Quark: An Integer RISC-V Vector Processor for
Sub-Byte Quantized DNN Inference
MohammadHossein AskariHemmat, Th´eo Dupuis, Yoan Fournier, Nizar El Zarif, Matheus Cavalcantey,
Matteo Perottiy, Frank G ¨urkaynaky, Luca Beniniy, Franc ¸ois Leduc-Primeau, Yvon Savaria, Jean-Pierre David
Electrical Engineering Department, ´Ecole Polytechnique de Montr ´eal, Qu ´ebec, Canada
yIntegrated Systems Laboratory, ETH Z ¨urich, Switzerland
Abstract —In this paper, we present Quark, an integer RISC-V
vector processor speciﬁcally tailored for sub-byte DNN inference.
Quark is implemented in GlobalFoundries’ 22FDX FD-SOI tech-
nology. It is designed on top of Ara, an open-source 64-bit RISC-
V vector processor. To accommodate sub-byte DNN inference,
Quark extends Ara by adding specialized vector instructions
to perform sub-byte quantized operations. We also remove the
ﬂoating-point unit from Quarks’ lanes and use the CV A6 RISC-
V scalar core for the re-scaling operations that are required
in quantized neural network inference. This makes each lane
of Quark 2 times smaller and 1.9 times more power efﬁcient
compared to the ones of Ara. In this paper we show that Quark
can run quantized models at sub-byte precision. Notably we show
that for 1-bit and 2-bit quantized models, Quark can accelerate
computation of Conv2d over various ranges of inputs and kernel
sizes.
Index Terms —RISC-V , Vector ISA, Quantization, Machine
Learning, Efﬁciency
I. I NTRODUCTION
Vector processors are gaining traction for machine learning
computations, thanks to their efﬁciency when executing tensor
operations, combined with the ﬂexibility of fully programmable
instruction processing. Many Deep Neural Network (DNN)
tasks can perform well with far lower precision than is
typically used for operands of generic digital signal processing
computations [1]–[4]. Quantized models operate on much
smaller processing and power budgets than their full precision
counterparts, making them appealing options for edge applica-
tions. Even if using sub-byte networks can help speeding up
computation in neural networks, to the best of our knowledge,
there are currently no other vector processors that can efﬁciently
store, load, and operate on sub-byte operands.
This paper makes the following contributions: ﬁrst, we
modiﬁed Ara [5], a RISC-V V compliant vector processor to
support operations on sub-byte operands. To achieve this, we
added custom instructions to accelerate low-bitwidth operations.
Indeed, the majority of quantized DNN inference is performed
only in the integer domain, hence, we removed the ﬂoating-
point unit from Ara. This resulted in each lane of Quark
being 2smaller and 1.9more power efﬁcient than the ones
in Ara. On the other hand, in quantized DNNs, to recover
accuracy, there is a re-scaling step after each convolution or
linear layer that requires ﬂoating-point operations. In Quark,
higher precision operations can be performed in the CV A6 [6]
scalar processor.Second, we developed a set of custom instructions, speciﬁ-
cally designed for running sub-byte DNNs. Then, using these
custom instructions, we developed a small vector DNN runtime
that provides common DNN kernels operating on sub-byte
data. We show that compared to Int8 andFP32 kernels,
the quantized kernels achieve higher performance. Finally, we
have implemented Quark in GlobalFoundries’ 22FDX Fully-
Depleted Silicon-on-Insolator (FD-SOI) technology and provide
its detailed power and area usage report.
The rest of the paper is organized as follows. In Section II,
we review the related works. In Section III, we describe the
architectural changes made to Ara to make it more efﬁcient for
running quantized neural networks. In Section IV, we provide
performance comparisons between Quark and Ara, and an
analysis of Quark’s implementation results. Finally, Section V
concludes this work.
II. B ACKGROUND AND RELATED WORKS
Quantized neural networks are a well-studied research
topic [7]. However, supporting sub-byte precision has not
been as popular. In the following, we have divided the related
works for these types of quantized neural networks into three
different groups depending on how they are implemented: on
custom accelerators, commodity CPUs and custom RISC-V
Microprocessors.
A. Custom Accelerators
Since the inception of quantized neural networks, many
custom accelerators based on ASIC and FPGA were proposed.
Loom [8] supports sub-byte computation through bit-serial
computation. Loom accelerates the inherent computation cost
of bit-serial by using a large number of parallel bit-serial units
that perform a 1-bit by 1-bit multiplication. Loom expects
the input data to these parallel units to be in a bit-stream
format. Because of that, Loom transposes data into a bit-
stream format at each output unit, with a signiﬁcant amount
of overhead in area and power consumption. In [9] a DNN
accelerator based on Loom was proposed and implemented
in 65 nm technology, achieving 805 GOPS when operating on
8-bit precision operands. BitFusion [10] and BitBlade [11] use
a set of parallel 2-bit by 2-bit multipliers followed by a shift
accumulation unit to support sub-byte computation. To further
optimize for area and power, BitBlade [11] replaces the shift
accumulation units with bitwise summation. RaPiD [12] is anarXiv:2302.05996v1  [cs.AR]  12 Feb 2023Fig. 1. This ﬁgure shows the content of 8 vector elements of v1andv2when vbitpack instructions are executed consecutively on the same input and
output vector registers. In this example, we are packing the content of v1intov2when 2 bit precision is used. and show different bit slices.
ultra low precision accelerator for AI training and inference.
RaPiD supports 16 and 8-bit ﬂoating-point and 4 and 2-bit
ﬁxed-point and was implemented in 7 nm technology. It delivers
a peak performance of 3.5 TOPS/W in its 8-bit ﬂoating-point
mode and 16.5 TOPS/W in its 4-bit ﬁxed-point mode.
There are examples of FPGA-based quantized neural network
accelerators as well. FINN [13] is a framework that was
originally designed for the acceleration of binary neural
networks. It has now evolved to a data-ﬂow style framework
with support for low precision operations for DNN acceleration.
FINN can process quantized neural networks in ONNX format.
Then, FINN’s front-end compiler optimizes the computational
graph for the FINN’s back-end compiler, which uses an
HLS hardware library to generate hardware supporting input
quantized models. However, FINN requires the entire model
to be implemented all at once on the FPGA, which limits its
ﬂexibility. BARVINN [14] is an FPGA based neural network
accelerator that was designed to support arbitrary precision
operations. It consists of a barrel RISC-V processor [15]
used as a controller and an array of Matrix Vector Units
(MVU) [16]. Each MVU has a set of conﬁguration registers that
can be programmed by the RISC-V processor through RISC-
V’s control status registers. This allows the MVU to perform a
set of neural network speciﬁc operations in a pipelined fashion
with any precision. The MVUs are connected to a crossbar
that allows them to send the results of one MVU to another.
Furthermore, BARVINN has a Python code generator library
that can consume a quantized DNN in ONNX format and
generates RISC-V code.
B. Sub-byte Precision on Commodity CPUs
In addition to custom hardware accelerators, there have been
efforts to support low-precision inference on commodity CPUs.
In [17], the authors added support for bit-serial convolution in
TVM to generate code for Arm devices. They exploit the vector
instructions of Arm devices to overcome the computationally
expensive task of formatting tensors in bit-stream format.
They show that on a Cortex-A72 CPU in a Raspberry Pi 4B
device, the relative speedup over 32-bit ﬂoating-point kernels
for 1-bit and 2-bit precision is 6.2 and 1.7, respectively.
With ULPPACK [18], the authors propose a novel packing
method that allows a commodity CPU to operate on sub-byte
precision. In this packing method, the quantized inputs are
packed into CPU’s registers with enough space to allow for
overﬂow. In [18], the authors show that on a Cortex-A72 inthe Raspberry Pi 4B CPU, compared to [17], ULPPACK has
a better performance for 3-bit or higher precision inputs.
C. Custom RISC-V MPUs
Dustin [19] is a 16-core RISC-V processor based on
OpenHW Group’s CV32E40P [20], [21] cores. Each core has
a special dot product unit that can be programmed through
RISC-V’s control status registers (CSRs) to perform a dot
product operation with different precision (2, 4, 8 or 16-bit
operations are supported). Dustin was taped-out in 65 nm
CMOS technology and achieves a peak performance of 58
GOPS and a peak efﬁciency of 1.15 TOPS/W. Darkside [22] is
a heterogeneous RISC-V compute cluster speciﬁcally designed
for DNN inference at the edge. It has 8 RISC-V cores enhanced
with 2-bit to 32-bit mixed-precision integer arithmetic. To
improve performance, Darkside is equipped with a specialized
datamover, a 16 bit ﬂoating point Tensor Product Engine and
an accelerator for computing depthwise convolution. Darkside
was implemented in 65 nm CMOS technology and achieves a
peak integer performance of 65 GOPS on 2-bit integer kernels.
Compared to the existing works, Quark is a vector processor
that extends the RISC-V ISA by adding new instructions to
accelerate packing methods for bit-serial computation. As we
will discuss in Section III, compared to Ara, Quark is a much
slimmer processor since it does not require any vectorized
ﬂoating-point operations. Compared to ASIC or FPGA based
accelerators and due to the nature of general processors, Quark
is very ﬂexible and more likely to support any type of neural
network that might come up in the literature in the future.
Finally, none of the previous works used a vector RISC-V to
provide sub-byte support for quantized DNNs.
III. A RCHITECTURE
Quark’s architecture is based on Ara, a modular open-source
vector processor recently updated to the RISC-V V 1.0 vector
ISA that is able to reach almost its theoretical peak performance
when running matrix multiplication between “large enough”
matrices [23]. The Ara system is a decoupled vector architecture
composed of a CV A6, the scalar RV64GC core, and Ara, its
vector engine. Each unit has independent access to the upper
level of the memory hierarchy via a shared AXI bus.
CV A6, being the only unit capable of accessing the instruc-
tion side of the memory, fetches both the scalar and vector
instructions from its private L1 instruction cache, executes the
former ones, and dispatches the latter ones to Ara. Since CV A6
is an in-order issue/commit processor that can still executeinstructions out-of-order by means of a scoreboard, the dispatch
of vector instructions to Ara happens non-speculatively as soon
as the instruction reaches the top of the scoreboard. Then,
CV A6 waits for Ara’s acknowledgment and answer, before
committing the instruction. This is a ﬁre-and-forget dispatch
since Ara, if there is no result to return, acknowledges CV A6
immediately after checking that no exceptions can occur. Ara’s
architecture matured through time following the RISC-V V
speciﬁcations, from RVV0.5 to RVV1.0.
Ara has been designed to accelerate a wide range of compu-
tations by means of vectorization. However, it only supports
standard data types. In quantized neural networks, we try to
avoid ﬂoating-point operations since they are computationally
costly. Therefore, our design focuses on maximizing the
throughput of integer computations, paying only a negligible
degradation in accuracy. This allowed us to remove Ara’s
ﬂoating-point units and to save precious area to make room for
our custom bit-serial computation instructions. In the following
sub-sections, we brieﬂy discuss these modiﬁcations.
A. Bit-Serial Computation
Figure 2 illustrates a typical computation ﬂow for quantized
neural network inference. One way to accelerate the inference
of neural networks is to accelerate convolutions or matrix
multiplications by using low precision operations. However, as
it can be seen, the rest of the computation has to happen in full
precision. Since the computation complexity of convolution
or matrix multiplication kernels is much higher than the rest
of the operations in Figure 2, it makes sense to accelerate
them ﬁrst. As discussed in Section II, there are many ways
to support low precision and particularly sub-byte operations.
With Quark, this is achieved by exploiting its vector processing
capacity.
Fig. 2. Typical computation graph in a forward path of a quantized neural
network. Here, we show the computation ﬂow for 2 bit inference.
As mentioned earlier, Quark extended the RISC-V vector
ISA with custom instructions to speed up bit-serial computation
and packing. To understand how these bit-serial computations
operate, let us consider a vector-vector multiplication:
wa=N 1X
n=0M 1X
m=02n+mpopcnt (wm^an) (1)
Equation (1)describes a vector-vector multiplication in bit-
serial format between two input vectors wanda, where m
andnare the precision used for wanda, respectively. To
perform this operation in a vector processor, we need logicalAND, popcount, and shift-and-accumulate operators. Moreover,
Equation (1)expects the data to be in bit-stream format, and
since this data transposition has to happen for each input of a
convolution or linear layer, this data transformation should be
fast to avoid making it a bottleneck to the whole computation.
Fig. 3. Per layer relative speed up when running Resnet18 on CIFAR100
with batch size of 1, on Quark with Int1 andInt2 over Ara with Int8 .
Fig. 4. Rooﬂine plot for Quark with 8 lanes and Ara with 4 lanes on Conv2d
with a 33kernel.
Although the base RISC-V vector ISA includes vectorized
AND, it does not support the other operators needed. To
accelerate the bit-serial computation, we added three custom
instructions as follows: vpopcnt ,vshacc andvbitpack .
Our popcount operator requires an instruction able to count
the number of bits at 1for every element of the input
vector; however, the base RISC-V vector ISA can only count
the global number of 1s over the entire vector length. Our
special vpopcnt instruction performs popcount on each
vector element. We also developed vshacc to fuse the shift
and accumulation operations in a single instruction. Finally,
we developed a custom instruction to accelerate bit-packing
operations. vbitpack slices the elements of a vector register
into bits and then packs these bits into an output register.
To accumulate the bit slices of previous elements, each call
tovbitpack shifts the target register to the left and then
performs the packing operations. Figure 1 illustrates this
operation in detail.
IV. R ESULTS AND PERFORMANCE ANALYSIS
A. Performance Analysis
To demonstrate the beneﬁts of adding our custom instruc-
tions, we benchmarked Ara and Quark with different kernel
sizes and shapes. To do this, we developed customized bit-serialAra An Ara Lane Quark A Quark Lane
Fig. 5. Ara and Quark system placed-and-routed designs. is vector register ﬁle related , is operand queue, is the vector ﬂoating-point unit, is the
vector register ﬁle and is the vector ALU.
TABLE I
QUANTIZATION OF RESNET 18 U SING LSQ [3] Q UANTIZATION METHOD .
Dataset Model Precision (W/A) Accuracy Size (MB)
CIFAR 100 Resnet18LSQ(1/1) 57.32 1.45
LSQ(2/2) 76.81 2.89
LSQ(8/8) 78.45 10.87
FP32 76.82 42.80
programs for conv2d, matrix multiplication, and other common
kernels needed to run typical neural networks on Quark.
We benchmarked the system performance by running
Resnet18 (with CIFAR100 dataset) on both Ara and Quark. To
simulate, we benchmarked both Ara and Quark with Questasim
10.7c. For each experiment, we used CV A6’s cycle CSRs to
know exactly how many clock cycles each kernel takes. To
preserve the accuracy of the model, we used full precision data
type for input and output layers. Figure 3 illustrates the per-
layer performance from running Resnet18 on Quark and Ara
under different precisions. We ran Resnet18 on Ara with Int8
andFP32 data types and on Quark with Int1 ,Int2 data
types. To show the importance of using a specialized packing
operation, we ran the Int2 experiment once with RISC-
V vector instructions and then with the custom vbitpack
instruction. As Figure 3 shows, running Resnet18 with Int1
precision on Quark outperforms Int8 on Ara in each layer.
However, as Table I shows, quantization of Resnet18 with 1-bit
precision comes with a signiﬁcant accuracy degradation. On
the other hand, with 2-bit quantization, the accuracy drop is
negligible, but the performance boost over Int8 is signiﬁcant.
For 2-bit quantization, we measured the performance of
each layer with and without vbitpack instruction. Finally,
although Int2 without the vbitpack instruction performs
better than Int8 on average, the improvement is not signiﬁcant.
On the other hand, Figure 3 shows that Int2 withvbitpack
instruction outperforms Int8 by an average of 5:67.
B. Physical Implementation Results
We implemented Quark and Ara with GlobalFoundries’
22FDX FD-SOI technology. For both Quark and Ara, we used
4 lanes, a vector length of 4096 bits (i.e., 16 KiB of Vector
Register File), and an equally-conﬁgured CV A6. We usedSynopsys Design Compiler 21.06 for synthesis and Cadence
Innovus 21.14 for physical implementation. In Table II, we
report Innovus’s static power analysis for both Quark and Ara,
in typical operating conditions. Figure 5 illustrates the placed
and routed design for both Ara and Quark. Based on the results
in Figure 5 and Table II, each Quark lane is about 2:3smaller
and consumes 1:9less power than Ara. Nevertheless, both
cores can operate at the same 1 GHz clock frequency. Finally,
Figure 4 illustrates the rooﬂine plot for Quark and Ara on
conv2d with a 33kernel. In this graph, considering the
same power and area budget for Ara and Quark as shown in
Table II, Quark outperforms Ara in all the input tensor sizes.
TABLE II
PHYSICAL IMPLEMENTATION OF ARA AND QUARK
Ara Quark
Number of Lanes 4 4 8
VRF Size [KiB] 16 16 32
Lane Cell Area [mm2] 0.120 0.051 0.046
Die Area [mm2] 1.09 0.69 1.09
TT Frequency [GHz] 1.05 1.05 1.00
Core power per lane [mW] 229 119 97
V. C ONCLUSION
This paper introduced Quark, an integer RISC-V vector
processor based on Ara. It extends the RISC-V vector ISA
by adding custom instructions for sub-byte computations
in DNNs. We modiﬁed Ara by removing the ﬂoating-point
unit and adding custom instructions to accelerate bit-serial
computations. Our simulation shows that on Resnet18 with
1-bit and 2-bit precision, Quark is 5.7x and 3.5x faster than Ara
with 8-bit precision on average for each kerenel. Finally, our
implementation results of Quark in GlobalFoundries’ 22 nm
technology show that Quark’s lanes are 2.3x smaller than the
corresponding ones in Ara.
VI. A CKNOWLEDGEMENTS
The authors thank CMC Microsystems and Global Foundries
for access to design tools and technologies. This research
was funded by CMC Microsystems, Mitacs and the OpenHW
Group.REFERENCES
[1]M. Courbariaux and Y . Bengio, “Binarynet: Training deep neural
networks with weights and activations constrained to +1 or -1,” CoRR ,
2016. [Online]. Available: http://arxiv.org/abs/1602.02830
[2]M. Rastegari, V . Ordonez, J. Redmon, and A. Farhadi, “XNOR-net:
ImageNet classiﬁcation using binary convolutional neural networks,”
inComputer Vision – ECCV 2016 , B. Leibe, J. Matas, N. Sebe, and
M. Welling, Eds. Cham: Springer International Publishing, 2016, pp.
525–542.
[3]S. K. Esser, J. L. McKinstry, D. Bablani, R. Appuswamy, and
D. S. Modha, “Learned step size quantization,” in International
Conference on Learning Representations , 2020. [Online]. Available:
https://openreview.net/forum?id=rkgO66VKDS
[4]Z. Xu, M. Lin, J. Liu, J. Chen, L. Shao, Y . Gao, Y . Tian, and R. Ji, “Recu:
Reviving the dead weights in binary neural networks,” in Proceedings
of the IEEE/CVF international conference on computer vision , 2021, pp.
5198–5208.
[5]M. Cavalcante, F. Schuiki, F. Zaruba, M. Schaffner, and L. Benini,
“Ara: A 1-GHz+ scalable and energy-efﬁcient RISC-V vector processor
with multiprecision ﬂoating-point support in 22-nm FD-SOI,” IEEE
Transactions on Very Large Scale Integration (VLSI) Systems , vol. 28,
no. 2, pp. 530–543, 2020.
[6]F. Zaruba and L. Benini, “The cost of application-class processing: Energy
and performance analysis of a Linux-ready 1.7-GHz 64-bit RISC-V core
in 22-nm FDSOI technology,” IEEE Transactions on Very Large Scale
Integration (VLSI) Systems , vol. 27, no. 11, pp. 2629–2640, Nov 2019.
[7]A. Gholami, S. Kim, Z. Dong, Z. Yao, M. W. Mahoney, and K. Keutzer,
“A survey of quantization methods for efﬁcient neural network inference,”
2021.
[8]S. Sharify, A. D. Lascorz, K. Siu, P. Judd, and A. Moshovos, “Loom:
Exploiting weight and activation precisions to accelerate convolutional
neural networks,” in 2018 55th ACM/ESDA/IEEE Design Automation
Conference (DAC) , 2018, pp. 1–6.
[9]S. Sharify, A. D. Lascorz, M. Mahmoud, M. Nikolic, K. Siu, D. M.
Stuart, Z. Poulos, and A. Moshovos, “Laconic deep learning inference
acceleration,” in Proceedings of the 46th International Symposium
on Computer Architecture , ser. ISCA ’19. New York, NY , USA:
Association for Computing Machinery, 2019, p. 304–317. [Online].
Available: https://doi.org/10.1145/3307650.3322255
[10] H. Sharma et al. , “Bit fusion: Bit-level dynamically composable
architecture for accelerating deep neural network,” in 2018 ACM/IEEE
45th Annual International Symposium on Computer Architecture (ISCA) .
IEEE, 2018, pp. 764–775.
[11] S. Ryu, H. Kim, W. Yi, and J.-J. Kim, “Bitblade: Area and energy-efﬁcient
precision-scalable neural network accelerator with bitwise summation,”
inProceedings of the 56th Annual Design Automation Conference 2019 ,
2019, pp. 1–6.
[12] S. Venkataramani, V . Srinivasan, W. Wang, S. Sen et al. , “RaPiD: AI accel-
erator for ultra-low precision training and inference,” in 2021 ACM/IEEE
48th Annual International Symposium on Computer Architecture (ISCA) ,
2021, pp. 153–166.
[13] Y . Umuroglu et al. , “FINN: A framework for fast, scalable binarized
neural network inference,” in Proceedings of the 2017 ACM/SIGDA
International Symposium on Field-Programmable Gate Arrays . ACM,
2017, pp. 65–74.
[14] M. AskariHemmat, O. Bilaniuk, S. Wagner, Y . Hariri, Y . Savaria, and
J.-P. David, “BARVINN: Arbitrary precision DNN accelerator controlled
by a RISC-V CPU,” in 28th Asia and South Paciﬁc Design Automation
Conference, ASP-DAC 2023 , 2022.
[15] M. AskariHemmat, O. Bilaniuk, S. Wagner, Y . Savaria, and J.-P. David,
“RISC-V barrel processor for deep neural network acceleration,” in 2021
IEEE International Symposium on Circuits and Systems (ISCAS) , 2021,
pp. 1–5.
[16] O. Bilaniuk, S. Wagner, Y . Savaria, and J.-P. David, “Bit-slicing FPGA
accelerator for quantized neural networks,” in 2019 IEEE International
Symposium on Circuits and Systems (ISCAS) , 2019, pp. 1–5.
[17] M. Cowan, T. Moreau, T. Chen, J. Bornholt, and L. Ceze, “Automatic
generation of high-performance quantized machine learning kernels,” in
Proceedings of the 18th ACM/IEEE International Symposium on Code
Generation and Optimization , 2020, pp. 305–316.
[18] J. Won, J. Si, S. Son, T. J. Ham, and J. W. Lee, “ULPPACK: Fast
sub-8-bit matrix multiply on commodity SIMD hardware,” Proceedings
of Machine Learning and Systems , vol. 4, pp. 52–63, 2022.[19] G. Ottavi, A. Garofalo, G. Tagliavini, F. Conti, L. Benini, and D. Rossi,
“A mixed-precision RISC-V processor for extreme-edge DNN inference,”
in2020 IEEE Computer Society Annual Symposium on VLSI (ISVLSI) ,
2020, pp. 512–517.
[20] M. Gautschi, P. D. Schiavone, A. Traber, I. Loi, A. Pullini, D. Rossi,
E. Flamand, F. K. G ¨urkaynak, and L. Benini, “Near-threshold RISC-
V core with DSP extensions for scalable iot endpoint devices,” IEEE
Transactions on Very Large Scale Integration (VLSI) Systems , vol. 25,
no. 10, pp. 2700–2713, 2017.
[21] P. Davide Schiavone, F. Conti, D. Rossi, M. Gautschi, A. Pullini,
E. Flamand, and L. Benini, “Slow and steady wins the race? a comparison
of ultra-low-power RISC-V cores for Internet-of-Things applications,”
in2017 27th International Symposium on Power and Timing Modeling,
Optimization and Simulation (PATMOS) , 2017, pp. 1–8.
[22] A. Garofalo, Y . Tortorella, M. Perotti, L. Valente, A. Nadalini, L. Benini,
D. Rossi, and F. Conti, “Darkside: A heterogeneous RISC-V compute
cluster for extreme-edge on-chip DNN inference and training,” IEEE
Open Journal of the Solid-State Circuits Society , pp. 1–1, 2022.
[23] M. Perotti, M. Cavalcante, N. Wistoff, R. Andri, L. Cavigelli, and
L. Benini, “A “New Ara” for vector computing: An open source highly
efﬁcient RISC-V V 1.0 vector processor design,” in 2022 IEEE 33rd
International Conference on Application-speciﬁc Systems, Architectures
and Processors (ASAP) , 2022, pp. 43–51.