Instance-Optimality in Interactive Decision Making:
Toward a Non-Asymptotic Theory
Andrew Wagenmaker
University of Washington
ajwagen@cs.washington.eduDylan J. Foster
Microsoft Research
dylanfoster@microsoft.com
Abstract
We consider the development of adaptive, instance-dependent algorithms for interactive decision
making (bandits, reinforcement learning, and beyond) that, rather than only performing well in the
worst case, adapt to favorable properties of real-world instances for improved performance. We aim for
instance-optimality , a strong notion of adaptivity which asserts that, on any particular problem instance,
the algorithm under consideration outperforms all consistent algorithms. Instance-optimality enjoys a rich
asymptotic theory originating from the work of Lai and Robbins (1985) and Graves and Lai (1997), but
non-asymptotic guarantees have remained elusive outside of certain special cases. Even for problems as
simple as tabular reinforcement learning, existing algorithms do not attain instance-optimal performance
until the number of rounds of interaction is doubly exponential in the number of states.
In this paper, we take the Ô¨Årst step toward developing a non-asymptotic theory of instance-optimal
decision making with general function approximation. We introduce a new complexity measure, the
Allocation-Estimation CoeÔ¨Écient (AEC), and provide a new algorithm, AE2, which attains non-asymptotic
instance-optimal performance at a rate controlled by the AEC. Our results recover the best known
guarantees for well-studied problems such as Ô¨Ånite-armed and linear bandits and, when specialized to
tabular reinforcement learning, attain the Ô¨Årst instance-optimal regret bounds with polynomial dependence
on all problem parameters, improving over prior work exponentially. We complement these results with
lower bounds that show that i) existing notions of statistical complexity are insuÔ¨Écient to derive non-
asymptoticguarantees, andii)undercertaintechnicalconditions, boundednessoftheAllocation-Estimation
CoeÔ¨Écient is necessary to learn an instance-optimal allocation of decisions in Ô¨Ånite time.
Contents
1 Introduction 2
1.1 Interactive Decision Making . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
1.2 Background: Asymptotic Instance-Optimality . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
1.3 A Motivating Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
1.4 The Allocation-Estimation CoeÔ¨Écient . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
1.5 Main Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
1.6 Concrete Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
1.7 Organization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
2 The AE2Algorithm: Regret Bounds and Examples 13
2.1 Regularity Conditions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
2.2 The AE2Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
2.3AE2Algorithm: Regret Bound for Uniformly Regular Classes . . . . . . . . . . . . . . . . . . 17
2.4 The AE2
?Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
2.5AE2
?Algorithm: Regret Bound without Uniform Regularity . . . . . . . . . . . . . . . . . . . 21
2.6 Application: Structured and Contextual Bandits . . . . . . . . . . . . . . . . . . . . . . . . . 21
2.7 Application: Tabular Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
2.8 Overview of Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
1arXiv:2304.12466v1  [cs.LG]  24 Apr 20233 Lower Bounds for Learning the Optimal Allocation 30
3.1 Learning the Optimal Allocation: Minimax Formulation . . . . . . . . . . . . . . . . . . . . . 30
3.2 Main Result . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
3.3 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
3.4 Discussion and Interpretation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
4 Additional Related Work 37
5 Discussion 38
A Additional Notation 45
B Technical Tools 48
B.1 Online Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
B.2 Properties of Graves-Lai Program . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50
C Proofs from Section 2 59
C.1 Regret Bound for Uniformly Regular Classes (Theorem 2.1) . . . . . . . . . . . . . . . . . . . 59
C.2 Regret Bound without Uniform Regularity (Theorem 2.2) . . . . . . . . . . . . . . . . . . . . 69
C.3 Estimation Guarantees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76
C.4 Supporting Lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79
D Proofs for Examples 84
D.1 Preliminaries: Regular Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85
D.2 Structured Bandits with Gaussian Noise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87
D.3 Contextual Bandits with Finitely Many Actions (Example 2.6) . . . . . . . . . . . . . . . . . 94
D.4 Informative Arms (Example 2.1) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96
D.5 Tabular Reinforcement Learning (Section 2.7) . . . . . . . . . . . . . . . . . . . . . . . . . . . 98
E Proofs and Additional Results from Section 3 109
E.1 Technical Lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109
E.2 Proof of Theorem 3.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112
E.3 Proof of Theorem 3.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113
E.4 Proofs for Lower Bound Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117
E.5 Lower Bound on Regret for Algorithms with Well-Behaved Tails . . . . . . . . . . . . . . . . 121
1 Introduction
We consider the development of adaptive, sample-eÔ¨Écient algorithms for interactive decision making , encom-
passing bandit problems and reinforcement learning with general function approximation. For decision making
in high-dimensional spaces with a long horizon, existing approaches (Lillicrap et al., 2015; Mnih et al., 2015;
Silver et al., 2016) are sample-hungry, which presents an obstacle for real-world deployment in settings where
data is scarce or high-quality simulators are not available. To overcome this challenge, algorithms should
both i) Ô¨Çexibly incorporate users‚Äô domain knowledge, as expressed via modeling and function approximation,
and ii) explore the environment in a deliberate, adaptive fashion, taking advantage of favorable structure
whenever possible.
Toward achieving these goals, a major area of research aims to develop algorithms with optimal sample
complexity and understand the fundamental limits for such algorithms (Russo and Van Roy, 2013; Jiang
et al., 2017; Sun et al., 2019; Wang et al., 2020; Du et al., 2021; Jin et al., 2021; Foster et al., 2021), and
the foundations are beginning to fall into place. In particular, focusing on minimax regret (that is, the
best regret that can achieved for a worst-case problem instance in a given class of problems), Foster et al.
(2021, 2022b, 2023) provide uniÔ¨Åed algorithm design principles and measures of statistical complexity that
are both necessary and suÔ¨Écient for low regret. However, minimax regret and other notions of worst-case
performance are inherently pessimistic, and may not be suÔ¨Écient to close the gap between theory and practice.
2For example, recent work has shown that algorithms that are optimal in the worst-case can be arbitrarily
suboptimal on ‚Äúeasier‚Äù instances (Wagenmaker et al., 2022b). To overcome these challenges and develop
algorithms that perform well on everyinstance, a promising approach is to develop algorithms that adaptto
the diÔ¨Éculty of the problem instance under consideration.
The performance of such adaptive algorithms can be quantiÔ¨Åed through instance-dependent regret bounds,
which become smaller (leading to low regret) when the underlying problem instance is favorable. Algorithms
with such guarantees have been studied throughout the literature on bandits and reinforcement learning;
basic examples include adapting to large gaps in value between alternative actions (Lai and Robbins, 1985)
or low noise or variance in bandit problems (Allenberg et al., 2006; Hazan and Kale, 2011; Foster et al.,
2016; Wei and Luo, 2018; Bubeck et al., 2018), and adapting to the diÔ¨Éculty of reaching certain states in
Markov Decision Processes (Zanette and Brunskill, 2019; Simchowitz and Jamieson, 2019; Dann et al., 2021;
Wagenmaker et al., 2022b).
While there are many notions of adaptivity and instance-dependence, they are generally incomparable. A
stronger notion of adaptivity is instance-optimality , which asserts that the performance of the algorithm on a
problem instance of interest exceeds that of any consistent algorithm (that is, any algorithm with sublinear
regret for all problem instances). Instance-optimality enjoys a rich theory originating with the work of Lai and
Robbins (1985) and Graves and Lai (1997), with a celebrated line of research developing sharp guarantees for
the special case of Ô¨Ånite-armed bandits (Burnetas and Katehakis, 1996; Garivier et al., 2016; Kaufmann et al.,
2016; Lattimore, 2018; Garivier et al., 2019). Beyond the Ô¨Ånite-armed bandit setting, however, development
has been largely asymptotic in nature, and existing algorithms either:
1.achieve instance-optimality only as T!1(or, to the extent that they are non-asymptotic, require
Tto be exponentially large with respect to problem-dependent parameters) (Graves and Lai, 1997;
Komiyama et al., 2015; Combes et al., 2017; Degenne et al., 2020b; Dong and Ma, 2022), or
2.achieve non-asymptotic guarantees, but require restrictive modeling assumptions such as linear function
approximation (Tirinzoni et al., 2020; Kirschner et al., 2021).
Indeed, even for the simple problem of tabular (Ô¨Ånite-state/action) reinforcement learning, existing algorithms
do not attain instance-optimal performance until the number of rounds of interaction is doubly exponential
in the number of states (Ok et al., 2018; Dong and Ma, 2022). In this paper, we address these challenges,
providing algorithms that i) accommodate Ô¨Çexible, general-purpose function approximation, and ii) attain
instance-optimality in Ô¨Ånite time, in a sense which is itself optimal.
Contributions. We take the Ô¨Årst steps toward building a non-asymptotic theory of instance-optimal
decision making. We observe that asymptotic characterizations for instance-optimal statistical complexity:
1.reÔ¨Çect the regret incurred by an allocation of decisions designed to optimally distinguish the ground
truth problem instance from a set of alternatives, but
2. do not capture the statistical complexity required to learnsuch an allocation.
To address this, we introduce a new complexity measure, the Allocation-Estimation CoeÔ¨Écient (AEC), which
aims to capture the statistical complexity of learning an optimal Graves-Lai allocation. We provide a new
algorithm, AE2, which attains non-asymptotic instance-optimal regret at a rate controlled by the AEC. We
complement this result with lower bounds that show that under certain technical conditions, boundedness
of the Allocation-Estimation CoeÔ¨Écient is not just suÔ¨Écient, but necessary to learn an instance-optimal
allocation in Ô¨Ånite time.
Our algorithm is simple, and can be applied to any hypothesis class in a generic fashion. It recovers the best
known guarantees for standard problems such as Ô¨Ånite-armed and linear bandits and, when specialized to
tabular reinforcement learning, achieves the Ô¨Årst instance-optimal regret bounds with polynomial dependence
on all problem parameters. We believe that our approach clariÔ¨Åes and elucidates many tradeoÔ¨Äs and statistical
considerations left implicit in prior work, and hope that it will serve as a foundation for further development
of instance-optimal algorithms.
31.1 Interactive Decision Making
We adopt the Decision Making with Structured Observations (DMSO) framework of Foster et al. (2021),
which is a general setting for interactive decision making that encompasses bandit problems (structured,
contextual, and so forth) and reinforcement learning with function approximation.
The DMSO framework is speciÔ¨Åed by a decision space ,reward spaceRR, andobservation space O. The
learner is given access to a (known) model classM (!4RO), and it is assumed there exists some true
modelM?2M, unknown to the learner, which represents the underlying environment. Formally, we make
the following assumption.
Assumption 1.1 (Realizability) .The learner has access to a model class Mcontaining the true model M?.
The learning protocol consists of Trounds. For each round t= 1;:::;T:
1. The learner selects a decisiont2.
2.The learner receives a reward rt2Rand observation ot2Osampled via (rt;ot)M?(t), and
observes (rt;ot).
We can think of the model class Mas representing the learner‚Äôs prior knowledge about the decision making
problem, and it allows one to appeal to estimation and function approximation. For structured bandit
problems, for example, models correspond to reward distributions, and Mencodes structure in the reward
landscape. For reinforcement learning problems, models correspond to Markov decision processes (MDPs),
andMtypically encodes structure in value functions or transition probabilities. We refer the reader to
Section 2.6 and Section 2.7 for concrete examples of how standard decision-making settings can be instantiated
within the DMSO framework, and refer to Foster et al. (2021) for further background.
For a model M2M,EM;[]denotes the expectation under the process (r;o)M(),fM():=EM;[r]
denotes the mean reward function, M2arg max2fM()denotes any optimal decision, and M=
arg max2fM()denotes the setof all optimal decisions. Similarly, when the algorithm is clear from
context, EM[]andPM[]refer to the expectation and probability measure, respectively, induced over histories,
Ht= (1;r1;o1);:::; (t;rt;ot), onM. We overload notation somewhat and use PM;[]to refer to the
conditional density over ROinduced by M. While we do not assume the randomrewards are strictly
bounded (allowing, for example, Gaussian rewards), we make the following assumption on the reward means.
Assumption 1.2 (Bounded Reward Means) .For eachM2Mand2, we havefM()2[0;1].
Throughout this work, we also make the following assumption on the uniqueness of the optimal action of M?.
Assumption 1.3 (Unique Optimal Action) .For the ground truth model M?2M, the optimal action M?
is unique.
Note that the latter assumption is standard in the literature on instance-optimality. We measure performance
in terms of regret, which is given by
Reg(T):=TX
t=1Etpt
fM?(M?) fM?(t)
; (1)
whereptis the learner‚Äôs randomization distribution for round t. In addition, we deÔ¨Åne M() =fM(M) 
fM()as thesuboptimality gap function for model Mand decision , and the minimum suboptimality gap as
M
min:=(
inf2:M()>0M(); Mis unique,
0; otherwise.(2)
Since by assumption M?is unique, we have M?
min>0. Throughout, we replace dependence on M?with ‚Äú?‚Äù
when the meaning is clear from context, for example: ?
min:= M?
min,f?() :=fM?(), or?=M?.
Further Notation. We letM+=fM: !4ROjfM()2[0;1]gdenote the space of all possible
modelsMwith rewards inRandfM()2[0;1]. We use4Xto refer to the set of probability distributions
over anyX. Throughout, we often abbreviate Ep[]withEp[].
41.2 Background: Asymptotic Instance-Optimality
Our aim is to develop algorithms that are instance-optimal in a strong sense: for every model M?2M,
the regret of the algorithm under M?is at least as good as that of any consistent algorithm; here, an
algorithm is said to be ‚Äúconsistent‚Äù if it ensures that EM[Reg(T)] =o(T)for allM2M. Instance-optimality
is a powerful notion of performance: no algorithm‚Äîeven one designed speciÔ¨Åcally with M?in mind‚Äîcan
achieve lower regret on M?without giving up consistency. For multi-armed bandits, a long line of work
initiated by Lai and Robbins (1985) characterizes the instance-optimal regret as a function of the instance
M?, and provides eÔ¨Écient algorithms that attain instance-optimality in Ô¨Ånite time (Garivier et al., 2016;
Kaufmann et al., 2016; Lattimore, 2018; Garivier et al., 2019). For the general decision making setting we
consider, the forward-looking work of Graves and Lai (1997) (see Dong and Ma (2022) for a contemporary
treatment) introduced a complexity measure we refer to as the Graves-Lai CoeÔ¨Écient , which asymptotically
characterizes the instance-optimal performance as a function of the instance M?and model classM. DeÔ¨Åne
the Kullback-Leibler divergence by
DKL(PkQ) =R
log dP
dQ
dP;PQ;
+1; otherwise.
For any classMand model M2M, the Graves-Lai CoeÔ¨Écient is the value of the program
glc(M;M):= inf
2R
+(X
2M()j8M02Malt(M) :X
2DKL(M()kM0())1)
;(3)
where, forMwith unique optimal decision M, we deÔ¨Åne
Malt(M):=fM02MjM62M0g; (4)
the set of ‚Äúalternative‚Äù models‚Äîthe models M02Mthat disagree with Mon the optimal decision1. When
Mis clear from context, we will abbreviate gM:=glc(M;M)(and g?:=glc(M;M?)). We also denote any
solution to Eq. (3) by M‚Äînote that this is not in general unique. The characterization of Graves and Lai
(1997) is as follows.
Proposition 1.1 (Graves and Lai (1997); Dong and Ma (2022)) .For any model class Mwithjj<1, any
algorithm that is consistent with respect to Mmust have
EM?[Reg(T)]glc(M;M?)log(T) o(log(T)) (5)
for anyM?2M, and there exists an algorithm which achieves2
EM?[Reg(T)]glc(M;M?)log(T) +o(log(T)) (6)
for anyM?2Msatisfying Assumption 1.3.
The interpretation of the Graves-Lai CoeÔ¨Écient of M?with respect toM,glc(M;M?), is simple. It asks,
ifM?is known to the learner (to be clear, M?is not known a-priori), what is the minimum regret that
must be incurred to gather enough information to rule out all possible alternatives M02Mwhich do not
haveM?as an optimal decision (i.e., M?62M0)? In other words, it aims to certifythatM?is indeed
the optimal decision while incurring the minimum regret possible. This intuition is explicitly encoded in
Eq. (3): the objective,P
2()M?()denotes the regret that an allocation 2R
+will incur on the
true instance M?, while the constraintP
2()DKL(M?()kM0())1ensures that M?andMare
statistically distinguishable under the allocation , for allM02Malt(M?).
Asasimpleexample, fortheÔ¨Ånite-armedbanditproblemwhere  = [A]andM=
M() =N(f();1)jf2RA	
,
a straightforward calculation shows that
glc(M;M?)/X
6=M?1
M?()
1ForM2Msuch thatMis not unique, we deÔ¨Åne Malt(M):=fM02Mj M\M0=?g, and deÔ¨Åne glc(M;M)as in
Eq. (3), with respect to this Malt(M). We also deÔ¨ÅneMalt() =fM2Mj62Mg.
2To be precise, rather than scaling directly with glc(M;M?), the upper bound given by Dong and Ma (2022) scales with a
quantity glcT(M;M?)such that glcT(M;M?)!Tglc(M;M?).
5and any optimal allocation has M?()/1
(M?())2for6=M?. In this case, Proposition 1.1 recovers the
well-known gap-dependent logarithmic regret bound
EM?[Reg(T)].X
6=M?log(T)
M?()
of Lai and Robbins (1985), and shows that it is instance-optimal. Note that this bound can oÔ¨Äer signiÔ¨Åcant
improvement over the minimax rate  p
AT
whenTis large.
The Graves-Lai CoeÔ¨Écient characterization is appealing in its simplicity, but the catch‚Äîat least when one
moves beyond Ô¨Ånite-armed bandits‚Äîis hiding in the lower-order terms, particularly for the upper bound (6).
For general model classes, the best known Ô¨Ånite-time regret bounds (Dong and Ma, 2022) take the form
EM?[Reg(T)]glc(M;M?)log(T) + poly(jj;(M?
min) 1)log1 c(T) (7)
wherec>0is a universal constant. While this indeed leads to instance-optimality as T!1, the ‚Äúlower-order‚Äù
term in Eq. (7) scales with the size of the decision space, which is intractably large for most problems of
interest. As an example, consider the problem of tabular reinforcement learning in an episodic MDP with S
states,Aactions, and horizon H. Here, we typically have glc(M;M?) =poly(S;A;H )(in particular, the
Graves-Lai CoeÔ¨Écient can be bounded in terms of suboptimality gaps for the optimal value function and
reachability for states of interest (Simchowitz and Jamieson, 2019; Dann et al., 2021; Wagenmaker et al.,
2022b)), yetjj=AHS. Consequently, the Graves-Lai CoeÔ¨Écient does not become the dominant term in (7)
untilexp(exp(S)). That is, for realistic time horizons, asymptotic instance-optimality does not tell the full
story.
Learning an optimal allocation. Given knowledge of an optimal Graves-Lai allocation M?solving Eq.
(3), a learner could simply take actions as speciÔ¨Åed by M?, and would achieve the instance-optimal rate
given in Proposition 1.1. However, this is typically infeasible, as the optimal allocation itself depends strongly
upon the ground truth model M?, and is therefore unknown to the learner. In light of this challenge, the
approach taken by essentially all existing algorithms (Burnetas and Katehakis, 1996; Graves and Lai, 1997;
Magureanu et al., 2014; Komiyama et al., 2015; Lattimore and Szepesvari, 2017; Combes et al., 2017; Hao
et al., 2019, 2020; Van Parys and Golrezaei, 2020; Degenne et al., 2020b; Tirinzoni et al., 2020; Kirschner
et al., 2021; Dong and Ma, 2022) is to Ô¨Årst learn an estimate for a Graves-Lai allocation M?, and then take
actions as speciÔ¨Åed by this estimate. In addition to being natural, this approach is necessary in a certain
(weak) sense: for any algorithm that achieves instance-optimality, the expected decision frequencies must
converge to an approximately optimal allocation as Tgrows (cf. Lemma 3.1).3
The presence of the lower-order term scaling with jjin Eq. (6) (and in similar regret bounds from most
existing work) reÔ¨Çects the sample complexity required to learn an optimal Graves-Lai allocation through
uniform exploration. SpeciÔ¨Åcally, one can estimate an allocation by uniformly exploring the decision space to
gather data, and then solving an empirical approximation to the Graves-Lai program (3). Naive exploration
of this type inevitably results in 
(jj)sample complexity, and it is natural to ask whether a more deliberate
exploration strategy, perhaps by exploiting the structure of the class M, could lead to better Ô¨Ånite-time
regret bounds. For the setting of linear bandits, where Rdand the mean reward function 7!fM()is
linear, this is indeed the case: a recent line of work (Tirinzoni et al., 2020; Kirschner et al., 2021) provides
regret bounds of the form
EM?[Reg(T)]glc(M;M?)log(T) + poly(d;(M?
min) 1)log1 c(T):
This bound replaces the size of the decision space in the lower-order term by the dimension d, reÔ¨Çecting the
fact that there are only d‚ÄúeÔ¨Äective‚Äù directions in which exploration is required. While this is an encouraging
start, the techniques used in these works are specialized to linear bandits, and it is unclear how to generalize
them beyond this setting. Recently, some progress has been made in understanding more general structured
bandit problems (Jun and Zhang, 2020), yet a complete understanding of such problems remains lacking, and
even less is known in complex decision making problems such as reinforcement learning.
3The connection between instance-optimal regret and learning an optimal allocation has many subtleties; we refer ahead to
Section 3 for extensive discussion.
61.3 A Motivating Example
As discussed in the prequel, for Ô¨Ånite-armed bandits and linear bandits, it is possible to achieve instance-
optimal regret bounds where the lower-order terms scale with reasonable problem-dependent quantities of
interest: the number of actions Afor bandits, and the dimension dfor linear bandits. Informally, these
quantities reÔ¨Çect the amount of exploration required to learn a near-optimal Graves-Lai allocation for the
underlying model M?. One might be tempted to ask whether this phenomenon is universal. That is, can we
always learn a near-optimal allocation with sample complexity no larger than, say, the minimax rate for M?
The starting point for our work is to recognize that in general, the answer is no: existing notions of statistical
complexity‚Äîincluding those proposed in the minimax framework (Foster et al., 2021, 2022b, 2023) and the
Graves-Lai CoeÔ¨Écient itself‚Äîare insuÔ¨Écient to capture the complexity of learning the Graves-Lai allocation
in Ô¨Ånite time. To highlight this, consider the following simple example.
Example 1.1 (Searching for an informative arm) .LetA;N2and2(0;1)be parameters, and consider
the classMof all models deÔ¨Åned as follows. First,  = [A][f
igi2[N]; decisions in [A]are ‚Äúbandit arms‚Äù, and
decisions inf
igi2[N]are ‚Äúinformative‚Äù (or, revealing) arms. Each model Mhas a unique optimal decision
M, and the following structure, with O= [A][f?g.
‚Ä¢For each bandit arm k2[A]we haverN(fM(k);1)forfM2[0;1]. There are no observations, i.e.
o=?almost surely.
‚Ä¢All informative arms 
kgive 0reward almost surely. There exists a unique informative arm 
M2
f
igi2[N]associated with M, so that if we play any 
k, we receive an observation
oUnif([A]); 
k6=
M;
IM+ (1 )Unif([A]); 
k=
M:
We takeMto consist of all possible models with this structure. The interpretation here is as follows. Suppose
for concreteness that = 9=10. If one were to ignore the revealing arms f
igi2[N], this would be a standard
Ô¨Ånite-armed bandit problem. In particular, if we were to consider a model M?withfM?() =1
2+ If=ig
fori2[A], a standard calculation would yield
gM?/A
: (8)
However, the presence of the informative arms makes the problem substantially easier. With = 9=10(for
concreteness), one can see that for the model M, pulling the informative arm 
M?will giveo=M?with
probability at least 9=10, meaning that we can identify that M?is optimal with high probability by pulling

M?a constant number of times. It follows that the optimal allocation is to ignore the bandit arms and set
M?()/If=
M?g. This gives gM?O(1), which is substantially better than gM?/A
ifis small orA
is large.
If one only is only concerned with asymptotic rates, this is the end of the story, but for non-asymptotic rates,
we need to consider the amount of exploration required to learn the optimal allocation . In particular, in order
to identify the informative arm 
M?, which is necessary to learn the optimal allocation, it is clear that in the
worst case, any algorithm needs to try all of the revealing arms, leading to
E[Reg(T)] = 
(N):
This shows that while the complexity of learning the optimal allocation is washed away by an asymptotic
analysis:
lim
T!1O(1)log(T) +N
log(T)=O(1);
it cannot be ignored for Ô¨Ånite T. In addition, the 
(N)factor cannot be explained away by standard
complexity measures:
‚Ä¢By the argument above, supM2MgM=O(1), so the Graves-Lai CoeÔ¨Écient‚Äîeven in a worst-case
sense‚Äîdoes not reÔ¨Çect the complexity of learning an optimal allocation M?.
7‚Ä¢The minimax rate for this problem is always bounded by O(p
AT)(since we can treat it as a multi-armed
bandit instance), which does not scale with N. Yet, 
(A)sample complexity does not suÔ¨Éce to learn an
optimal allocation. As a result, existing complexity measures such as the Decision-Estimation CoeÔ¨Écient
(Foster et al., 2021), Eluder dimension (Russo and Van Roy, 2013), and information ratio (Russo and
Van Roy, 2018), which are tailored to the minimax setting, cannot explain away the 
(N)factor above.
/
Example 1.1 shows that if we want to achieve instance-optimality in Ô¨Ånite time, new notions of problem
complexity for the class Mare required. This motivates the following central questions:
1.Can we develop algorithms for general model classes Mthat achieve non-asymptotic instance-optimal
regret bounds of the form
EM?[Reg(T)]glc(M;M?)log(T) + comp(M)log1 c(T); (9)
where comp (M)is a complexity measure that reÔ¨Çects the intrinsic diÔ¨Éculty of exploration for M‚Äîin
particular, the diÔ¨Éculty of exploring to learn a Graves-Lai optimal allocation for? Ideally, such a
complexity measure should be small for standard classes of interest, generalizing what is already known
for Ô¨Ånite-armed and linear bandits.
2. Can we understand when the presence of such lower-order terms is necessary ?
1.4 The Allocation-Estimation CoeÔ¨Écient
To answer these questions and capture the statistical complexity of learning an optimal Graves-Lai allocation
in Ô¨Ånite time, we provide a new complexity measure, the Allocation-Estimation CoeÔ¨Écient (AEC). To
describe the AEC, let us introduce some additional notation. For a model M2Mand parameter "2[0;1],
we deÔ¨Åne
(M;") =
24 j9n2R+s.t.E[M()](1 +")gM
n; inf
M02Malt(M)E[DKL(M()kM0())]1 "
n
(10)
the set of (normalized) allocations 24 which are"-optimal for the Graves-Lai program glc(M;M)in Eq.
(3)‚Äîboth in terms of achieving the optimal objective value and satisfying the information constraint. In
addition, for a distribution 24 , we deÔ¨Åne
Mgl
"() =fM2Mj2(M;")g: (11)
Informally,Mgl
"()represents the set of models for which the (normalized allocation) 24 is"-optimal
for the Graves-Lai program glc(M;M).
For areference model M: !4RO(not necessarily in M) and parameter ">0, the Allocation-Estimation
CoeÔ¨Écient is given by
aec"(M;M) = inf
;!24sup
M2MnMgl
"()(
1
E!
DKL 
M()kM())
; (12)
where we adopt the convention that the value is 0 if Mgl
"() =M. In addition, letting co(M)denote the
convex hull forM, we deÔ¨Åne aec"(M):= supM2co(M)aec"(M;M).
The Allocation-Estimation CoeÔ¨Écient is a game between a min-player choosing ;!24 and a max-player
choosing a model M2M(with the restriction that M =2Mgl
"()). Let us interpret the value. First:
‚Ä¢The distribution 24 represents a normalized Graves-Lai allocation, while !24 is anexploration
distribution used to gather information.
‚Ä¢The reference model Mshould be interpreted as a guess for the underlying model M?2M.
8When24 is Ô¨Åxed, the value
inf
!24sup
M2MnMgl
"()(
1
E!
DKL 
M()kM())
represents the time required to gather enough information to distinguish between the reference model M
and all alternative models M =2Mgl
"()for whichis not an"-optimal Graves-Lai allocation‚Äîprovided
that we explore optimally by minimizing over !24 . For intuition, consider the case when M2M. In
this case,mustbe chosen so that M2Mgl
"()(i.e.,must be a Graves-Lai optimal allocation for M), as
otherwise the value of the AEC will be inÔ¨Ånite, since E!
DKL 
M()kM()
= 0for all!. Therefore, in
such cases, the AEC reÔ¨Çects the diÔ¨Éculty of distinguishing Mfrom models that have diÔ¨Äerent Graves-Lai
optimal allocations . In particular, such models might have the same optimal decision as Mbut, if our goal is
to play a Graves-Lai optimal allocation for M, we must still distinguish Mfrom such models. The value
aec"(M;M)then reÔ¨Çects the least possible time required to distinguish such models, which is achieved by
minimizing over the best possible (normalized) Graves-Lai allocation, 24 .
The Allocation-Estimation CoeÔ¨Écient plays a natural role for deriving both upper and lower bounds on the
time required to learn an optimal allocation. For lower bounds, the signiÔ¨Åcance of the AEC is somewhat
immediate: it precisely quantiÔ¨Åes the time required to acquire enough information to learn an "-optimal
allocation for the best possible exploration strategy , and thus leads to a lower bound on time required to learn
such an allocation for any algorithm. Notably, the AEC serves as a lower bound for all possible model classes
M, and hence may be thought of as an intrinsic structural property of the class M.
For upper bounds, the Allocation-Estimation CoeÔ¨Écient acts as a mechanism to drive exploration. Given an
estimatecMtforM?, if we select the decision t2using an appropriate mixture of the distributions (;!)
that attain the value of aec"(M;cMt), we are guaranteed that one of two good outcomes occurs. Either:
1.the normalized allocation 24 is"-optimal for glc(M;M?), so that by playing the learner matches
the performance of the optimal allocation (up to a tolerance "), thereby incurring the minimum amount
of regret needed to gain information that allows it to distinguish M?fromM2Malt(M?), or
2.M?=2Mgl
"()(that is,is not an"-optimal Graves-Lai allocation), in which case‚Äîfrom the deÔ¨Ånition
of the Allocation-Estimation CoeÔ¨Écient‚Äîplaying !24 will allow us to better distinguish M?from
cMt.
As long as we can perform estimation in a consistent, online fashion, this reasoning will allow us to argue
thatM?2Mgl
"()the majority of the time, and that the total regret incurred through the learning process
will scale with aec"(M).
Generalized Allocation-Estimation CoeÔ¨Écient. For certain results, we make use of the following,
slightly more general variant of the AEC. For a reference model M: !4ROandsubset of models
M0M, we deÔ¨Åne
aecM
"(M0;M) = inf
;!24sup
M2M0nMgl
"()(
1
E!
DKL 
M()kM())
; (13)
where we adopt the convention that the value is 0 if M0nMgl
"() =?. HereM0denotes the set we take the
supremum over, while Mdenotes the set that Mgl
"()is deÔ¨Åned with respect to (i.e., the set with respect to
which the Graves-Lai allocation is deÔ¨Åned). When M0=M, we recover the AEC as deÔ¨Åned in Eq. (12):
aec"(M;M) =aecM
"(M;M).
1.5 Main Results
Building on the intuition above, our main results show that boundedness of the Allocation-Estimation
CoeÔ¨Écient is suÔ¨Écient to achieve instance-optimality in Ô¨Ånite time, and is also necessary in order to learn a
near-optimal allocation. Formal statements of our upper bounds are given in Section 2 and formal statements
for our lower bounds in Section 3.
9Upper Bound. Our upper bounds are based on a new algorithm, AE2(Allocation Estimation via Adaptive
Exploration) , achieves instance-optimality by using the Allocation-Estimation CoeÔ¨Écient to drive exploration.
Theorem 1.1 (Upper Bound‚ÄîInformal Version of Theorem 2.1) .For any model class Msatisfying certain
regularity conditions, the AE2algorithm ensures that for all ">0,M?2M, andT2N:
EM?[Reg(T)](1 +")glc(M;M?)log(T) +eO+ 
aec"=12(M) +aec1=2
"=12(M)log1=2(T)
;(14)
whereeO+()suppresses polynomial dependence on " 1, the log-covering number of M,supM2M1=M
min,
log logT, and several other measures of the regularity for the class M.
Theorem 1.1 shows that it is therefore possible to achieve instance-optimality in Ô¨Ånite time with lower-order
terms scaling (primarily) as the cost of learning the optimal allocation, as captured by the AEC. For multi-
armed bandits with  = [A], we have aec"(M) =eO+(poly(A)), and for linear bandits with Rd, we have
aec"(M) =eO+(poly(d)), so that the regret bound in (14) enjoys similar scaling as existing non-asymptotic
approaches (Tirinzoni et al., 2020; Kirschner et al., 2021). For tabular reinforcement learning, we have
aec"(M) =eO+(poly(H;S;A )), which leads to exponential improvement over prior work (Dong and Ma,
2022). Finally, for the instance in Example 1.1, in cases when NA;1=min,aec"(M) =O(N), soaec"(M)
captures the intuitive diÔ¨Éculty of learning a Graves-Lai allocation in this setting.
Remark 1.1 (Technical Conditions) .The technical conditions under which Theorem 1.1 is proven are
relatively mild, and include certain smoothness of the KL divergences, sub-Gaussian tail behavior for log-
likelihood ratios, and bounded covering number for Mwith standard parametric growth (note that Mmay be
inÔ¨Ånite), all of which can be shown to hold for standard classes. In addition, we requires that the amount of
information that can be gained by playing the optimal decision for M?is bounded (see Section 2.1 for precise
statements of our conditions).
Remark 1.2 (Asymptotic Performance) .Asymptotically as T!1, the regret bound given in Theorem 1.1
scales as (1 +")glc(M;M?)logT, which is a factor of (1 +")oÔ¨Ä of the lower bound. For all standard
classes, aec"=12(M)scales polynomially in 1="so, to obtain an optimal leading-order constant, it suÔ¨Éces to
choose"= 1=logaT, for small enough a>0.
AdaptingtoMinimumGap. Notethatthelower-ordertermgiveninTheorem1.1scaleswith supM2M1=M
min,
the minimum gap of the entire model class. In Section 2.5, we give a reÔ¨Ånement of the AE2algorithm ( AE2
?)
which attains an improved regret bound which replaces the term supM2M1=M
minwith 1=?
min, the minimum
gap of the underlying model; notably AE2
?requires no prior knowledge of ?
min(i.e., it is able to adapt to the
minimum gap of the underlying model). In addition, rather than scaling with aec"=12(M), the lower-order
term now scales with aecM
"=12(M?)for a subsetM?Mwhich, informally, restricts to models in Mfor
which the minimum gap is at least ?
min.
Theorem 1.2 (Upper Bound‚ÄîInformal Version of Theorem 2.2) .For any model class Msatisfying certain
regularity conditions, the AE2
?algorithm ensures that for all ">0,M?2M, andT2N:
EM?[Reg(T)](1 +")glc(M;M?)log(T) +eO+ 
(aecM
"=12(M?))3+ log6=7(T)
; (15)
whereeO+()suppresses polynomial dependence on " 1, the log-covering number of M,1=?
min,log logT, and
several other measures of the regularity for the class M.
Lower Bound. To provide lower bounds, we adopt a novel minimax framework which asks, for the model
classMunder consideration, what is the least value of T2Nfor which it is possible to learn an "-optimal
Graves-Lai allocation for any model in M. To state our result, we introduce the following notation, deÔ¨Åned
with respect to any M2M+:
Mopt(M) =
M2Mj MM; D KL 
M()kM()
= 082M	
:
The setMopt(M)represents the set of models where 1) the optimal decision coincides with that of Mand 2)
MandM2Mcannot be distinguished by playing the optimal decision.
10Our main lower bound provides a sort of converse to the upper bound in Eq. (14).
Theorem 1.3 (Lower Bound‚ÄîInformal Version of Theorem 3.2) .For any model class Mand">0, it holds
that unless
log(T)sup
M2M+e
+ 
aecM
"(Mopt(M);M)
; (16)
no algorithm can simultaneously achieve the following for all instances M2M:
1.attain Graves-Lai optimality on Mwithin a constant factor (i.e., ensure EM[Reg(T)]2glc(M;M)log(T)).
2.discover an "-optimal allocation for M(i.e., Ô¨ÅndwithM2Mgl
"()) with probability greater than e
+(1).
Here,e
+()hides polynomial dependence on regularity parameters of M.
Observe that the Graves-Lai CoeÔ¨Écient becomes the dominant term in the upper bound (14) as soon as
log(T)e
+(aec"=12(M)). The lower bound (16) shows that for any algorithm that aims to estimate the
Graves-Lai allocation (in particular, AE2), such scaling is necessary, and therefore the lower-order term in
Theorem 1.1 is in some sense unimprovable. To the best of our knowledge, this is the Ô¨Årst general approach to
quantifying the lower-order terms necessary in order to achieve instance-optimality. We make several remarks
on the lower bound.
Remark 1.3 (Scaling in AEC) .Our upper and lower bounds scale with a slightly diÔ¨Äerent version of the
AEC, as the lower bound restricts the AEC to Mopt(M). In Section 3, we show an additional lower bound
that scales directly with aec"(M), matching our upper bound, but which only provides a lower bound on T
rather than log(T)(see Theorem 3.1).
Remark 1.4 (Asymptotic Optimality and Learning Optimal Allocations) .Theorem 1.3 gives a lower bound
on the time needed to learn a near-optimal Graves-Lai allocation, but does not directly imply that it is necessary
that an asymptotically optimal algorithm learn such an allocation. As we have noted, the allocations played by
any asymptotically optimal algorithm must converge to an optimal allocation in expectation . However, showing
that this convergence is necessary with even constant probability (the condition under which Theorem 1.3 is
proved) is rather subtle. As we show in Section 3 (Theorem 3.3), if one assumes that, in addition to being
asymptotically optimal in expectation, the algorithm under consideration also has regret with appropriately
bounded second moment, then if EM[Reg(T)](1 +")glc(M;M)log(T)for allM2M, it is indeed
necessary that a burn-in time analogous to Eq. (16) is satisÔ¨Åed. A detailed discussion of this point is given in
Section 3.4.
Together, our upper and lower bounds represent an initial step toward building a sharp non-asymptotic
theory of instance-optimality, and lead to a number of new conceptual insights. Our results open the door for
further-development, and to this end we highlight a number of opportunities for improvement (Section 3.4),
as well as open problems (Section 5).
1.6 Concrete Examples
We next present several examples illustrating our upper and lower bounds. All results in this section are
informal‚Äîsee Sections 2.3, 2.6, 2.7 and 3.3 for formal results and additional examples.
Example 1.2 (Searching for an Informative Arm (revisited)) .We return to the example of Section 1.3. Some
calculation shows that, for the choice of Min Example 1.1, as long as is constant and NA=2, we have

(N)sup
M2M+aecM
"(Mopt(M);M)and aec"(M)O(N):
Theorem 1.1 then implies that AE2has regret on Example 1.1 of
EM?[Reg(T)](1 +")g?log(T) +Npoly(A;1
;1
";logN;log logT)log1=2(T):
Furthermore, Theorem 1.3 shows that a scaling of log(T)e
+(N)is necessary for any algorithm to learn a
Graves-Lai optimal allocation. It follows that, on this example, the AEC reÔ¨Çects a notion of problem diÔ¨Éculty
11not captured by any existing complexity measure, matching our intuitive understanding of what the correct
scaling should be. We remark that the scaling log(T)e
+(N)is natural (as compared to Te
+(N))
since, if an algorithm is instance-optimal as required by Theorem 1.3, it can allocate at most O+(log(T))
pulls to suboptimal decisions. To pull every informative arm (each of which is suboptimal) while achieving
instance-optimality, it follows that we must have log(T)e
+(N). /
Example 1.3 (Tabular Reinforcement Learning) .Consider the setting of tabular reinforcement learning.
Here we take Mto be a (tabular) episodic Markov Decision Processes (MDP) with Sstates,Aactions,
horizonH, probability transition kernels fPM
hgH
h=1, and Gaussian rewards; see Section 2.7 for a full deÔ¨Ånition
of this setting. Let Mdenote the set of all such tabular MDPs which, for each state-action-state triple
(s;a;s0)andh2[H], havePM
h(s0js;a)Pmin>0; that is, each transition can occur with some minimum
probability. Then it can be shown that:
aecM
"(M?)poly
S;A;H;1
";1
?
min;log1
Pmin
:
This implies that for any tabular MDP in M, the AE2
?algorithm has regret bounded as:
EM?[Reg(T)](1 +")g?log(T) + poly(S;A;H;1
?
min;1
";log1
Pmin;log logT)log1=2(T):
To the best of our knowledge, this is the Ô¨Årst regret bound in the setting of tabular reinforcement learning
which is instance-optimal with lower-order terms scaling only polynomially in problem parameters, an
exponential improvement over past work (Ok et al., 2018; Dong and Ma, 2022). Furthermore, one can also
show that supM2M+aecM
"(Mopt(M);M)e

1
"2SA
(?
min)2
, so that our lower bound, Theorem 1.3, implies
that a burn-in time scaling polynomially in S;A;1
", and1
?
minis necessary to learn an "-optimal Graves-Lai
allocation for every model in M.
We remark that the prior work of Dong and Ma (2022) does not require that PM
h(s0js;a)Pminas we
do, yet their bound scales polynomially in the inverse probability of observing the trajectory that occurs
with minimum non-zero probability (the work of Ok et al. (2018) only holds for ergodic MDPs, itself a
very strong assumption). Our Ô¨Ånite-time results are therefore, in general, signiÔ¨Åcantly stronger, scaling only
logarithmically in Pmin. Removing the Pminassumption while still achieving reasonable lower-order terms is
an interesting direction for future work. /
Example 1.4 (Linear Bandits) .Consider the setting of linear bandits in ddimensions with unit-variance
Gaussian noise. Let Mdenote the set of all linear bandit models deÔ¨Åned with respect to some arm set
XRdand parameter set Rd. Concretely, each model M2Mtakes the form
M() =N(h;xi;1);
for some2, wherex2Xis an embedding of . Let ?
mindenote the minimum gap of M?(which is
unknown to the algorithm). Then it can be shown that
aecM
"(M?)poly
d;1
";1
?
min
which implies that the AE2
?algorithm has regret bounded as
EM?[Reg(T)](1 +")g?log(T) + poly(d;1
?
min;1
";log logT)log6=7(T): (17)
We remark that the scaling of Eq. (17) matches the state-of-the-art instance-optimal bounds for linear bandits
(in that all have polynomial lower-order terms‚Äîour polynomial dependence on dis slightly worse as our
upper bound on the AEC is somewhat coarse) (Tirinzoni et al., 2020; Kirschner et al., 2021). Notably, it is a
simple corollary of a much more general result, while prior work relies on specialized algorithms tailored to
linear bandits. /
In Sections 2.3, 2.6, 2.7 and 3.3 we formalize these examples and present additional examples, including
structured bandits with bounded eluder dimension and Ô¨Ånite-action contextual bandits. In all cases, we
obtain lower-order terms scaling only polynomially with problem parameters, and in each setting either match
the best-known existing bound, or are the Ô¨Årst to provide any meaningful Ô¨Ånite-time bounds.
121.7 Organization
Section 2 presents our algorithm and main upper bounds, as well as examples. Section 3 presents complemen-
tary lower bounds. In Section 4 we review additional related work. We conclude with discussion of open
problems and future directions in Section 5. Proofs are deferred to the appendix.
Additional notation. For an integer n2N, we let [n]denote the setf1;:::;ng. We adopt standard
big-oh notation, and write f=eO(g)to denote that f=O(gmaxf1;polylog(g)g). We leteO+()and
e
+()additionally suppress problem-dependent and regularity terms, as, for example, in Theorem 1.1 or
Theorem 1.3. We use .only in informal statements to emphasize the most notable elements of an inequality.
We will let lin()denote a function multi-linear and poly-logarithmic in its arguments. For a decision 2,
we use I24 to denote the delta distribution which places probability mass 1on.R+denotes the set of
nonnegative real numbers.
2 The AE2Algorithm: Regret Bounds and Examples
This section presents our main algorithm and regret bounds. We begin by introducing the most basic variant
of our algorithm, AE2, and using it to provide instance-optimal regret bounds for simple settings (Sections 2.2
and 2.3); with preliminaries in Section 2.1. We then give a reÔ¨Åned variant of the algorithm, AE2
?, which adapts
to the minimum gap ?
minand leads to regret bounds under relaxed regularity conditions (Section 2.4 and
Section 2.5). We use this variant to provide applications to structured and contextual bandits (Section 2.6)
and tabular reinforcement learning (Section 2.7). We conclude with an overview of our analysis in Section 2.8.
For all results in this section, we assume that Assumptions 1.1 to 1.3 hold.
2.1 Regularity Conditions
To present our algorithm and results, we Ô¨Årst introduce several regularity conditions for the model class M.
Likelihood ratios. We next make two assumptions concerning smoothness of KL divergences and behavior
of log-likelihood ratios.
Assumption 2.1 (Smooth KL) .There exists LKL>0such that for all M;M0;M002Mand2,
jDKL(M()kM00()) DKL(M0()kM00())jLKLp
DKL(M()kM0()):
Assumption 2.2 (Sub-Gaussian Log-Likelihood) .There exists VM>0such that for all M;M0;M002M
and2,
P(r;o)M()logPM0;(r;o)
PM00;(r;o) E(r0;o0)M()
logPM0;(r0;o0)
PM00;(r0;o0)x
2 exp( x2=V2
M)
for allx0.
Assumption 2.1 and Assumption 2.2 facilitate Ô¨Ånite-sample estimation guarantees with respect to the KL
divergence. Both assumptions are met by standard problem classes, including general structured bandit
problems with Gaussian noise. Existing works that consider general model classes make similar assumptions
(Dong and Ma, 2022).
Estimation. To provide estimation guarantees that accommodate inÔ¨Ånite classes M, we assume certain
covering properties. We will consider the following notion of a cover.
DeÔ¨Ånition 2.1 ((;)-Cover).We say that a set McovMis a(;)-cover ofMif there exists some
eventEsuch that:4
4Note that we require that McovM, i.e. thatMcovis a propercover.
131.supM2Msup2PM;(Ec).
2. For each M2M, there exists some M02M covsuch that
logPM;(r;o) logPM0;(r;o)
for all (r;o)2RO with supM002MPM00;(r;ojE)>0.
We denote the size of the smallest such cover by Ncov(M;;).
DeÔ¨Ånition 2.1 states that the log-likelihoods are ‚Äúcovered‚Äù under some good event Ewhich occurs with high
probability: for any model in the class, we can Ô¨Ånd some model in the cover with log-likelihoods that are
‚Äúclose‚Äù onE. We assume that the covering number for the model class Mis bounded, and has reasonable
(‚Äúparametric‚Äù) growth.
Assumption 2.3 (Bounded Covering Number) .For some parameters dcov1;Ccov1, we have
logNcov(M;;)dcovlogCcov

:
Note that the rate of growth of the covering number required by Assumption 2.3 is the standard rate of
growth for parametric (e.g., linear) classes. Our results easily extend to accommodate general growth rates,
but we adopt Assumption 2.3 because it suÔ¨Éces for all of the examples we will consider, and simpliÔ¨Åes
presentation.
Information content of optimal decisions. As noted in the introduction, the Graves-Lai CoeÔ¨Écient
g?=glc(M;M?)can be thought of as the minimal regret needed to distinguish M?from all possible models
with diÔ¨Äerent optimal decisions. As playing the optimal decision, ?, incurs no regret, any allocation which
is optimal for the Graves-Lai program, Eq. (3), will still be optimal if we increase the number of plays of ?
arbitrarily. As we are interested in Ô¨Ånite-time behavior in this work, it is undesirable to consider allocations
for which the number of pulls of optimal decisions are arbitrarily large. Instead, we would like to consider
allocations which play optimal decisions only as long as they still provides useful information about models
in the alternate set. The following deÔ¨Ånition gives a formal quantiÔ¨Åcation of this.
DeÔ¨Ånition 2.2 (Information Content of Optimal Decision) .Fix2(0;1=2]. For a model M2M, we deÔ¨Åne
nM
">0as the minimum value such that, for any allocation 2R
+satisfying
(1 +")gMX
2()M()and inf
M02Malt(M)X
2()DKL(M()kM0())1 ";
we have
inf
M02Malt(M)X
2;62M()DKL(M()kM0()) +X
2MnM
"DKL(M()kM0())1 2":
We denote nM
":= supM2MnM
".
Intuitively, any allocation which is "-optimal for the Graves-Lai program Eq. (3) need not play any optimal
decisionM2Mmore than nM
"times. Therefore, for model M,nM
"can be thought of as a quantiÔ¨Åcation of
the extent to which playing optimal decisions provides useful information‚Äîno additional useful information
can be acquired on models in the alternate set Malt(M)by playing optimal decisions more than nM
"times.
As we will see, nM
"is bounded polynomially in problem parameters for many classes of interest.
Uniformly regular classes. We refer to a class as uniformly regular ifnM
"<1, and the following
assumption on the minimum gaps holds.
Assumption 2.4 (Lower-Bounded Minimum Gap) .We have infM2MM
min>0. We denote by min>0a
(known) lower bound on infM2MM
min.
14Note that Assumption 2.4 implies that for all M2M,Mis unique. For the results concerning the most
basic version of our algorithm, AE2(Sections 2.2 and 2.3), we assume for expositional purposes that the
classMis uniformly regular. Our more general algorithm, AE2
?(Section 2.5), achieves guarantees similar
to those of AE2, but without uniform regularity. In particular, AE2
?replaces dependence on minwith the
minimum gap ?
min:= M?
minfor the true model, and replaces dependence on nM
"with n?
":=nM?
". We note,
however, that in cases where a lower bound on the minimum gap ?
minof the true model is known a-priori,
Assumption 2.4 can be satisÔ¨Åed by restricting the model class to models with minimum gap at least .
2.2 The AE2Algorithm
We now present the most basic variant of our main algorithm, AE2(Algorithm 1). This will serve as the
starting point for the most general version of our algorithm, AE2
?(Section 2.5). To describe the algorithm, we
Ô¨Årst introduce the primitive of an online estimation oracle (Foster and Rakhlin, 2020; Foster et al., 2021).
Estimation oracles. Algorithm 1 makes use of an online estimation oracle , denoted by AlgKL, which is
an algorithm that, given knowledge of the class M, estimates the underlying model M?2Mfrom data in a
sequential fashion. When invoked at step s2Nwith the data (1;r1;o1);:::; (s 1;rs 1;os 1)observed so
far, the estimation oracle builds an estimate
cMs=AlgKL
(i;ri;oi)	s 1
i=1
which aims to approximate the true model M?. Following (Foster et al., 2021; Chen et al., 2022; Fos-
ter et al., 2022a), we make use of randomized estimation oracles that, at each step produces s=
AlgKL 
(i;ri;oi)	s 1
i=1
, wheres24Mis a randomization distribution, and draw cMs. We mea-
sure the oracle‚Äôs performance in terms of cumulative estimation error, deÔ¨Åned as follows.
DeÔ¨Ånition 2.3 (Cumulative Estimation Error) .Consider the process where, for each round i2N, given
(1;r1;o1);:::; (i 1;ri 1;oi 1)withipiand(ri;oi)M?(i), the estimation oracle returns i=
AlgKL 
(1;r1;o1);:::; (i 1;ri 1;oi 1)
. For anys2N, we deÔ¨Åne the oracle‚Äôs cumulative KL estimation
error under this process as:
Est KL(s) :=sX
i=1EcMih
Epih
DKL 
M?()kcM()ii
:
Algorithm 1 can be invoked with any oÔ¨Ä-the-shelf algorithm for estimation, but our main results make use of
the fact that under Assumption 2.2 and Assumption 2.3, there exists an estimation oracle AlgKL(Algorithm 6
in Appendix C.3) which ensures that with probability at least 1 , for alls2N:
Est KL(s).VMdcovlog3=2Ccovs

: (18)
That is, the estimation oracle ensures that the KL divergence between the true model M?and the estimates
returned scales at most poly-logarithmically in the exploration horizon. Note that on its own, this guarantee
does not necessarily imply that cMs=EMs[M]!M?‚Äîlow online estimation error only requires that cMs
is on average close to M?on the decisions we have actually played.
Algorithm overview. We are now ready to give an overview of Algorithm 1. The algorithm alternates
between exploitsteps and exploresteps, tracking the number of explore steps that have been performed
with a counter s2N. For each step t2N, the algorithm makes use of an estimator cMs=EcMs[cM],
wheres=AlgKL 
(i;ri;oi)	s 1
i=1
is computed by calling the estimation oracle with data gathered at
previous explore steps. Given the estimator, AE2performs a test based on likelihood ratios (Line 5) to check
whether it has collected enough information to rule out all models for which cMs2cMsis not an optimal
decision. If so, it exploits, and plays cMs(Line 6), as in this case cMs=?with high probability. If the
test fails, the algorithm must gather more information to eliminate alternatives, and it explores (Line 8).
15Algorithm 1 Allocation Estimation via Adaptive Exploration ( AE2)
1:input:optimality tolerance ", model classM.
2:Initializes 1,nmax nmax(M;"=6), andq 4nmax+"gM
4nmax+2"gMforgM:= infM2M:gM>0gM.
3:Compute1 AlgKL(f?g)andcM1 EM1[M].
4:fort= 1;2;3;:::do
5:if9cMs2cMss.t.8M2Malt(cMs),Ps 1
i=1EcMih
logPcM;i(ri;oi)
PM;i(ri;oi)i
log(tlogt)then // Exploit
6: PlaycMs.
7:else // Explore
8: Setps qs+ (1 q)!sfor
s;!s arg min
;!24sup
M2MnMgl
"=6(;nmax)1
EcMs
E!
DKL cM()kM(): (19)
9: Drawspsand observe reward rsand observation os.
10: Compute estimate s+1 AlgKL(f(i;ri;oi)gs
i=1)andcMs+1=EcMs+1[cM].
11:s s+ 1.
The key component of the explore phase is the choice of the exploration distribution in Eq. (19), which
is based on the Allocation-Estimation CoeÔ¨Écient program, but incorporates some small modiÔ¨Åcations: 1)
First,cMis randomized according to the distribution s, 2) Second, the set M?2Mgl
"()is replaced with a
smaller setM?2Mgl
"(;nmax), which requires that obeys certain normalization constraints; this is detailed
below. Using the distributions s(representing a normalized allocation) and !s(representing an exploration
distribution) returned in Eq. (19), the algorithm computes a mixture ps=qs+ (1 q)!s, whereq2(0;1)
is a carefully chosen parameter, and plays sps(Line 9). The reward and observation (rs;os)that result
from playing sare then used to update the estimation oracle for subsequent rounds (Line 10).
To understand the intuition behind the explore phase and why the Allocation-Estimation CoeÔ¨Écient plays a
useful role here, we can consider two cases. In the Ô¨Årst case, if sis an"-optimal Graves-Lai allocation for
M?(that is,M?2Mgl
"=6(s;nmax)), then playing swill optimize the tradeoÔ¨Ä between minimizing regret on
M?and collecting information that allows one to distinguish M?fromM2Malt(M?), and will therefore
match the optimal performance prescribed by the Graves-Lai CoeÔ¨Écient, incurring regret scaling as g?.
In the second case, if sis not an"-optimal Graves-Lai allocation for M?, we haveM?62Mgl
"=6(s;nmax), so by
thedeÔ¨ÅnitionoftheAEC(Eq. (19)), !swillplacemassonactionsthatensure EcMs[E![DKL cM()kM?()
]]
is large; exactly how large this quantity is will be quantiÔ¨Åed by the value of the AEC. Since psplays!swith
constant probability, the quantity
EMs[Eps[DKL(M?()kM())]]
will also be large, but if the estimation oracle is consistent in the sense of DeÔ¨Ånition 2.3, this can only happen
a small number of times. In particular, if Eq. (18) holds, the number of times in which we encounter this
second case is at most logarithmic in the number of exploration rounds. As such, we can show that smust
be a near-optimal Graves-Lai allocation for M?on all but a logarithmic number of exploration rounds, and
thatAE2achieves the optimal rate on such rounds.
Critically, rather than exploring in a naive fashion (e.g., by sampling decisions uniformly), AE2explores only
to the extent necessary to learn a Graves-Lai allocation for M?. There may exist instances M6=M?which
diÔ¨Äer signiÔ¨Åcantly from M?but have a similar Graves-Lai allocations‚Äî AE2will make no eÔ¨Äort to distinguish
such instances since, as long as it knows that one of these instances is correct, it can simply play their shared
Graves-Lai allocation. This notion of exploration, which is targeted toward distinguishing instances that have
diÔ¨Äerent Graves-Lai allocations, is precisely the notion captured by the Allocation-Estimation CoeÔ¨Écient.
16Normalization factor for allocations. As noted in Section 2.1, while an optimal Graves-Lai allocation
may place an arbitrarily large number of pulls on an optimal decision, for Ô¨Ånite-time guarantees it is useful
to restrict to allocations which place only Ô¨Ånite mass on optimal decisions. To this end, AE2restricts the
optimization problem based on the AEC in Eq. (19) to only consider normalized allocations for which the
normalization factor is at most
nmax(M;") :=64
2
min1
"+VMnM
"
max
M2MgM: (20)
where the normalization factor refers to the value nin the deÔ¨Ånition of (M;")(see Eq. (10)). In
particular, to enforce this restriction, the optimization problem in Eq. (19) restricts the max-player to
M2MnMgl
"=6(;nmax), where nmax:=nmax(M;"=6)andMgl
"=6(;nmax)is deÔ¨Åned identically to Mgl
"=6()
in Eq. (11), but with nrestricted to nnmax. As we show in Lemma B.4, for nmax(M;")deÔ¨Åned as in Eq.
(20), the optimal value in Eq. (19) can be bounded by the AEC.
Computational eÔ¨Éciency. The primary computational burden in AE2lies in solving the optimization
problem (19) to compute the exploration distributions. In general there is little hope of solving this eÔ¨Éciently
(i.e., in time sublinear in jjandjMj)‚Äîindeed, in some cases it may be that to even determine whether
M2Mgl
"()will require enumerating the model class M. However, for nicely structured problems, we
anticipate that this program can be solved, or at least approximated, eÔ¨Éciently. As the focus of our work is
primarily statistical, we leave further exploration as to when the algorithm can be implemented eÔ¨Éciently to
future work.
Simplicity. We emphasize the simplicity of AE2. Most existing algorithms which achieve instance-optimality
are quite complex, even in specialized settings such as linear bandits. In contrast, AE2is very simple and
intuitive, and relies only on three basic components: an explore-exploit test, an estimation oracle, and a
single optimization to compute the exploration distributions. Despite its simplicity, as we show, AE2obtains
comparable or better performance over existing approaches.
Relation to existing approaches. At a very high level, AE2bears some similarity to the E2Dalgorithm of
Foster et al. (2021), which achieves the minimax optimal rate for general classes Min the DMSO framework.
Both algorithms rely on online estimation algorithms, and both solve min-max programs based on the
output of the estimator to determine which allocations to play. However, the algorithm design and analysis
principles for the two algorithms, and in particular the motivation for the min-max programs they solve,
diÔ¨Äer signiÔ¨Åcantly.
2.3 AE2Algorithm: Regret Bound for Uniformly Regular Classes
We present upper bounds for AE2in the setting where our class Mis uniformly regular: Assumption 2.4
holds and nM
"<1; these assumptions are relaxed by the AE2
?algorithm in the sequel. To state the regret
bound for AE2in the tightest form possible, we introduce the following variant of the Allocation-Estimation
CoeÔ¨Écient, which incorporates randomized estimators 24M:
aecM
"(M0;) := inf
;!24sup
M2MnMgl
"()1
EM[E![DKL 
M()kM()
]]; (21)
with aec"(M;) := aecM
"(M;)and aec"(M) := sup24Maec"(M;). Note that one can always bound
aec"(M)aec"(M)due to the convexity of the KL divergence. In fact, these deÔ¨Ånitions are equivalent up
to dependence on problem-dependent parameters in Section 2.1 (indeed, our lower bounds in Section 3 scale
with the latter quantity), but the former can be simpler to bound for some of the examples we consider.
Our main theorem concerning the performance of AE2is as follows.
Theorem 2.1 (Regret Bound for AE2).For any"2(0;1=2], there exists a choice for the estimation oracle
AlgKLsuch that for all T2N, under Assumptions 2.1 to 2.4 and if g?>0, the expected regret of AE2is
17bounded by
EM?[Reg(T)](1 +")g?log(T) +aec"=12(M)Caeclog3=2(logT) +Clowlog1=2(T);(22)
where
Caec:=cV2
Mdcovlog(Ccov)maxM2MgM
"3
min
" 1+VMnM
"=6
log(Clow);
for a universal numerical constant c>0, andClowis a lower-order constant given by
Clow:= lin
max
M2MgM;aec1=2
"=12(M);1
"2;1
3
min;nM
"=6;L2
KL;V13=2
M;dcov;log(Ccov);log logT
;
where lin()denotes a function multi-linear and poly-logarithmic in its arguments.
We prove Theorem 2.1 in Appendix C.1, and give a proof sketch in Section 2.8. Theorem 2.1 shows that
AE2achieves the asymptotically optimal Graves-Lai rate for M?, as given in Proposition 1.1, up to a (1 +")
approximation factor. In more detail, if we label the terms in Eq. (22) as
EM?[Reg(T)](1 +")g?log(T)|{z}
(I)+aec"=12(M)Caeclog3=2(logT)| {z }
(II)+Clowlog1=2(T)|{z}
(III);
the regret bound can be seen to consist of:
‚Ä¢Theleading-order term (I) = (1 +")g?log(T). This is the only term that scales linearly with log(T),
as a consequence we have limT!1EM?[Reg(T)]
log(T)(1 +")g?, which matches the instance-optimal rate
given in Proposition 1.1 up to a factor of (1 +").
‚Ä¢A lower-order term (II) =aec"=12(M)Caeclog3=2(logT), which is polylogarithmic in log(T), and
scales with aec"=12(M), as well as regularity parameters from Section 2.1.
‚Ä¢A second lower-order term (III) =Clowlog1=2T. This term scales with log1=2(T) =o(log(T))and, like
the term (II), scales with the AEC and regularity parameters from Section 2.1. Compared to (II), this
term has worse dependence on log(T), but enjoys sublinear aec1=2
"=12(M)scaling with the AEC.
Critically, both of the o(logT)lower-order terms above do not scale with (often exponentially large) terms
such asjjorjMjfound in prior work, and instead scale principally with aec"(M), which, as we will show in
Section 3, is unavoidable in a certain sense. In particular, note that once Tis large enough that
log(T)e
+(aec"=12(M));
the leading-order term (I) = (1 +")g?log(T)term in Theorem 2.1 will dominate the regret. This is precisely
the time horizon given by the lower bound in Theorem 1.3, which is necessary for an algorithm to learn a
near-optimal allocation for the Graves-Lai program. We oÔ¨Äer a more thorough comparison of Theorem 2.1
with our lower bounds in Section 3.4. Below, we discuss the lower-order terms and asymptotic performance
in greater detail.
Remark 2.1 (Additional Lower-Order Terms) .The lower-order terms in Theorem 2.1 depend on the
model classMthrough the regularity, covering, and smoothness assumptions, as well as the minimum gap
(Section 2.1). For many of the examples we consider, the Allocation-Estimation CoeÔ¨Écient will dominate
these other terms, yet there may exist classes where this is not the case. Resolving the optimal dependence on
these problem-dependent parameters in the lower-order terms, as well as understanding when these parameters
are necessary, remains an interesting direction for future work.
In addition, let us mention that while both lower-order terms scale with o(logT)(note that the scaling is no
larger than O(plogTpolylog logT)), it is not clear what the optimal dependence on Tshould be for the
lower-order terms. For example, one might hope to replace the dependence on log1=2(T)with loga(T)for
some constant a<1=2, or even with polylog (log(T)). Precisely characterizing the optimal log(T)scaling for
lower-order terms remains an interesting open question. To this end, we remark that Jun and Zhang (2020)
show that in some cases, an 
(log logT)term is indeed necessary.
18Remark 2.2 (Asymptotic Performance) .Asymptotically, as T!1, the regret of AE2scales with (1 +
")g?log(T), which is a factor of (1 +")oÔ¨Ä from the asymptotic lower bound in Proposition 1.1. For any
Ô¨ÅxedT2Nof interest, as long as aec"(M) =poly(" 1), one can obtain an asymptotic constant of 1by
choosing"= 1=loga(T)for a suÔ¨Éciently small constant a >0. For example, when jj<1, it is always
possible to bound aec"(M).poly(jj)="4(see Proposition 2.1 below), so choosing "as above ensures that
the lower-order terms scale o(logT), while the leading-order term scales as g?log(T)asymptotically.
2.3.1 Example: Searching for an Informative Arm
We next provide an example of a uniformly regular class in order to illustrate a case where Theorem 2.1 holds.
In particular, we revisit the informative arm setting described in the introduction (Example 1.1). Recall that
we exhibited a model class for which the complexity of learning the Graves-Lai allocation is not governed by
existing complexity measures, and can be larger than the minimax optimal rate for learning with M. In
what follows, we show that on this example the Allocation-Estimation CoeÔ¨Écient correctly adapts to the
complexity of this model class. We emphasize that the main applications of our results, which take advantage
of the more general AE2
?algorithm, will be given in Section 2.6 and Section 2.7, and we also present additional
examples of uniformly regular classes in Appendix D.2.1.
Example 2.1 (Searching for an Informative Arm (revisited)) .LetMdenote the model class constructed
in Example 1.1, with parameters A;N5and2[4=A;9=10]. We additionally discretize the space so
that, for each M2Mand2[A], we havefM()2f0;min;2min;:::;b1
mincming,5and furthermore
restrictMso that it does not include instances with multiple optimal arms. For this class, one can show that
Assumptions 2.1 to 2.4 hold with LKL;VMO(logA)anddcov=O(A);Ccov=O(N)(see Appendix D.4).
Furthermore, we can bound nM
"2
2
min, and
aec"(M)64N
2+16A
2
min:
As a result, for this class, AE2has expected regret bounded as
EM?[Reg(T)](1 +")g?log(T) +Npoly(A;1
";1
min;logN;log logT)log1=2(T):
Note that here the only term that scales linearly with Nis the Allocation-Estimation CoeÔ¨Écient‚Äîevery other
class-dependent term appearing in the regret bound scales at most logarithmically in N. We are particularly
interested in situations where the cost of Ô¨Ånding the correct informative arm is much larger than any existing
complexity measures for the problem: that is, when is constant and NA;1
min. In this case, we have
aec"(M)O(N), and the Allocation-Estimation CoeÔ¨Écient correctly captures the intuitive complexity of
learning the optimal allocation. In particular, the dependence on NreÔ¨Çects the fact that we need to test each
informative arm at least once. Furthermore, as we show in Example 3.1, we can lower bound aec"(M)
(N)
as well, so in the regime where NA;1
min, the AEC is the dominant lower-order term. /
See Appendix D.2 for the proof of this example.
2.4 The AE2
?Algorithm
While it may be reasonable to assume that the minimum gap of M?,?
min, is bounded away from 0, and
that the amount of useful information playing ?provides is also bounded on M?, assuming that this is true
for every model in the model class (as in the prequel) is a signiÔ¨Åcantly stronger assumption. For example, if
we letMdenote the space of all multi-armed bandits with means in [0;1], the only possible value of minis
0, as we can always Ô¨Ånd some instance with minimum gap arbitrarily close to 0. In this section, we dispense
with the uniform regularity assumption: we relax Assumption 2.4, and additionally prove that it suÔ¨Éces if
only n?
":=nM?
"(as opposed to nM
") is bounded.
5The discretization is required to satisfy the uniform regularity assumption‚Äîwe include it here to provide a concrete example
of a uniformly regular class. However, it can be shown that, without this discretization assumption, Theorem 2.2 applies to the
original formulation given in Example 1.1 with the AEC again scaling as O(N).
19Algorithm 2 Adaptive Exploration for Allocation Estimation for classes without uniform regularity ( AE2
?)
1:input:Optimality tolerance ", estimation oracle AlgKL, growth parameters q,n,M0.
2:s 1,` 1, "
4+2",qs 1 s q,ns sn.
3:Compute1 AlgKL(f?g)andcM1 EM1[M].
4:fort= 1;2;3;:::do
5:ifs2`then // Form active set and cover
6:` `+ 1.
7: ` arg min0s.t. aecM
=2(M;1
)sM.
8:M` M`;1
`\
M2M :nM
"+1
M
min+4gM
M
min+2nM
"
gM+4
M
minp
ns	
.
9:M`
cov (`;`)-cover ofM`for` 2 `;` 2 5`,D` ?.
10:if9cMs2cMss.t.8M2Malt(cMs),Ps 1
i=1EcMih
logPcM;i(ri;oi)
PM;i(ri;oi)i
log(tlogt)then // Exploit
11: PlaycMs.
12:else // Explore
13: Setps qss+ (1 qs)!sfor
s;!s arg min
;!24sup
M2M`nMgl
"(;ns)1
EcMs[E![DKL cM()kM()
]]: (23)
14: Drawsps, observers;os, setD` D`[f(s;rs;os)g.
15: Compute estimate s+1 AlgKL(D`;M`
cov)andcMs+1=EMs+1[M].
16:s s+ 1.
Our main algorithm for this section, AE2
?, is given in Algorithm 2. It is very similar to AE2but to remove the
requirement of uniform regularity, the algorithm avoids solving Eq. (19) over the entire model class M, and
instead solves it over a carefully restricted model class. For x;y> 0, deÔ¨Åne
Mx;y:=fM2M : M
minx;nM
"yg: (24)
AE2
?breaks its explore rounds into doubling epochs. For each epoch `, Eq. (23) in Algorithm 2 solves an
AEC-like optimization problem over a restricted class M`M`;1
`, which is chosen in Line 9 to explicitly
ensure that the value of the optimization problem in Eq. (23), is bounded; this is guaranteed by the deÔ¨Ånition
of`in Line 8. Similar to AE2, the value of the optimization in Eq. (23) quantiÔ¨Åes how much information
we are gaining about the Graves-Lai allocation of M?, and the regret of the explore phase can be bounded in
terms of the value of this optimization. By restricting M`so that the value of Eq. (23) is always bounded,
we can therefore ensure that the regret during the exploration phase is bounded.
Intuitively, this restriction of M`reduces the space of models we must distinguish M?from in order to identify
its Graves-Lai allocation: rather than distinguishing M?from all models in M, we must only distinguish it
from models inM`, which could be signiÔ¨Åcantly easier. The caveat is that, since we do not know the value of
n?
"or?
min,M?may not always be in M`. In such cases, little can be said about the exploration phase‚Äîwe
are not able to provide any meaningful guarantees on how much information sand!sacquire about M?.
To mitigate this, as sincreases we gradually relax the criteria for inclusion in M`, ensuring that for large
enoughs,M?will be inM`. In particular, one can show that the number of exploration rounds needed to
guaranteeM?2M`scales with aecM
"(M?), for
M?:=
M2M : M
min?;nM
"=361=?	
for ?:= minf?
min;1=n?
"=36g: (25)
That is,M?is the restriction of Mto models with gap at least minf?
min;1=n?
"=36g(implying all models in
M?have a unique optimal decision), and for which the information content of the optimal decision is at most
maxf1=?
min;n?
"=36g.
Estimation oracle. While AE2simply requires that the estimation oracle AlgKLreturns randomized
estimators supported on 4M, forAE2
?, we wish to ensure that the estimators produced are instead only
20supported onM`. To this end, we restrict the estimator to M`
cov, the (`;`)-cover ofM`. We denote the
resulting estimation oracle with AlgKL(D`;M`
cov), where the Ô¨Årst argument represents the set of available
observations, and the second argument the set over which the estimation oracle must return an estimate.
Computational eÔ¨Éciency of AE2
?.Similar to AE2, it is not clear how to solve the main optimization
required by AE2
?, Eq. (23), in general. In addition, unlike AE2,AE2
?maintains a version space of models, M`,
which could increase the computational burden further. We emphasize that the focus of this work is primarily
statistical, and leave addressing the computational challenges for future work.
2.5 AE2
?Algorithm: Regret Bound without Uniform Regularity
The following theorem provides the main guarantee for AE2
?.
Theorem 2.2 (Regret Bound for AE2
?).For any"2(0;1=2], if Assumptions 2.1 to 2.3 hold and g?>0,AE2
?
(Algorithm 2) ensures that for all T2N, the expected regret is bounded as
EM?[Reg(T)](1 +")g?log(T) +
aecM
"=12(M?)3
Caeclog3=2(logT) +Clowlog6=7(T);
where
Caec:=eOV3
M(VM+LKL)dcovlog(Ccov)
"?
min
;
andClowis a lower-order constant given by
Clow:= poly
g?;1
?
min;n?
"=6;1
";VM;LKL;dcov;logCcov;log logT
:
The proof of Theorem 2.2 is given in Appendix C.2. As Theorem 2.2 illustrates, at the expense of a
slightly larger polynomial dependence on the Allocation-Estimation CoeÔ¨Écient, and slightly larger lower-order
terms, we can obtain near instance-optimal regret‚Äîmatching the instance-optimal lower bound given in
Proposition 1.1 up to a factor of (1 +")‚Äîwithout requiring any assumption on the minimum gap, or
boundedness of nM
". Rather than scaling with the minimum gap for the entire class, min, Theorem 2.2 scales
only with the minimum gap of the ground truth model, ?
min, which could be substantially larger than min.
An additional advantage of Theorem 2.2 is that it scales with aecM
"(M?)as opposed to aec"(M); for the
examples we consider in the sequel, the former quantity enjoys better dependence on problem-dependent
parameters. For example, we show in Section 3 that for standard classes, aec"(M)can scale with the minimum
gap amongst all models in the class. On the other hand aecM
"(M?)typically scales with ?
min.
We emphasize that AE2
?requires no prior knowledge of ?
minorn?
"‚Äîit is able to adapt to the minimum gap
and regularity of the underlying model.
Remark 2.3 (Dependence on n?
").As we show in the following examples, n?
"is typically bounded polynomially
in standard problem parameters, though in practice this needs to be veriÔ¨Åed for each problem instance. We
remark that some scaling in terms of n?
"seems unavoidable‚Äîif there is a signiÔ¨Åcant amount of information to
be gained playing the optimal decision, any algorithm which is nearly instance-optimal will play the optimal
decision at least n?
"times, and therefore the ‚ÄúeÔ¨Äective horizon‚Äù to eliminate alternate instances scales with
n?
". As we are the Ô¨Årst to formalize this notion of how informative the optimal decision is, we believe more
research in this direction is required.
2.6 Application: Structured and Contextual Bandits
We now instantiate Theorem 2.2 to give regret bounds for AE2
?in standard settings of interest, bounding the
Allocation-Estimation CoeÔ¨Écient for each setting. We begin by focusing on structured bandit settings and
contextual bandits, then turn to tabular reinforcement learning in the sequel. We recall that, to map bandit
problems to the DMSO framework, we take the decision space to be the set of ‚Äúarms‚Äù, the observation space
O=f?g, and the reward space Rto be the rewards from the bandit (while we do not explicitly include
the rewards in the observation space, we assume they are observed). We defer proofs for all examples to
Appendix D.
212.6.1 The Uniform Exploration CoeÔ¨Écient
For the main examples we consider, we proceed by Ô¨Årst bounding the Allocation-Estimation CoeÔ¨Écient in
terms of another, somewhat simpler parameter we refer to as the uniform exploration coeÔ¨Écient .
DeÔ¨Ånition 2.4 (Uniform Exploration CoeÔ¨Écient) .For a randomized estimator 24M, we deÔ¨Åne the
uniform exploration coeÔ¨Écient with respect to at scale">0as the value of the following program:
C
exp(") := min
C2R+;p24
C8M;M02M :maxM002fM;M0gEM[Ep[DKL 
M()kM00()
]]1=C
=)maxp024Ep0[DKL(M()kM0())]"
:
We deÔ¨Ånep
exp(")as any minimizing distribution for this program, and let
Cexp(M;") := sup
24MC
exp(")
denote the uniform exploration constant for class M.
Intuitively, the uniform exploration coeÔ¨Écient characterizes the extent to which it is possible to explore
by uniformly covering the decision space. In particular, one can always choose pto be uniform over ,
which gives Cexp(M;").jj=", but in cases where information is shared between actions, the parameter is
signiÔ¨Åcantly smaller, as we will show for familiar examples below. For example, in the case of linear bandits
with dimension d, we haveCexp(M;")eO(dlog 1="
").
The following result shows that the Allocation-Estimation CoeÔ¨Écient can be bounded in terms of the uniform
exploration coeÔ¨Écient.
Proposition 2.1 (Informal) .ForM0M, we can bound aecM
"(M0)Cexp(M0;)for any
p
min
M2M0min(
min1
81LKL;M
min
34VM
"
2gM=M
min+nM
"=36;M
min
3)
:
The full statement of Proposition 2.1 is given in Lemma B.6. Using Proposition 2.1, we obtain guarantees for
AE2
?on several familiar classes, beginning with several bandit settings. We remark that Proposition 2.1 is not in
general tight‚Äîit simply shows that the Allocation-Estimation CoeÔ¨Écient is bounded by a simple, general, and
interpretable notion of how easily a class can be explored. As, such the bounds on the Allocation-Estimation
CoeÔ¨Écient in the following examples can almost certainly be improved using more specialized tools.
2.6.2 Finite-Armed Bandits
We Ô¨Årst consider the simplest bandit setting: multi-armed bandits with Ô¨Ånite arms.
Example 2.2 (Finite-Armed Bandit) .FixA > 0, and consider the class of Ô¨Ånite-armed bandits with A
arms and unit-variance Gaussian noise:
M=
M() =N(fM();1)jfM2[0;1]A	
:
ItisstraightforwardtoverifythatAssumptions2.1to2.3holdwith LKL;VM4anddcov=O(A);Ccov=O(1),
and it can also be shown that, as long as fM?(?)<1, we can bound n?
"cA2
"(?
min)4. In addition, we can
boundCexp(M?;")4A=", so Proposition 2.1 gives the following result.
Proposition 2.2. For the Ô¨Ånite-armed bandit problem with Aactions, there exists a universal constant c>0
such that
aecM
"(M?)cA15
"8(?
min)24: (26)
We immediately obtain the following corollary to Theorem 2.2.
22Corollary 2.1. For Ô¨Ånite-armed bandits with Gaussian noise, as long as fM?(?)<1,AE2
?has regret
bounded by
EM?[Reg(T)](1 +")g?log(T) + poly(A;1
";1
?
min;log logT)log6=7(T):
With more reÔ¨Åned analyses, various works have achieved instance-optimal regret bounds for Ô¨Ånite-armed
bandits with tighter lower-order terms than Corollary 2.1 (Garivier et al., 2016; Kaufmann et al., 2016;
Lattimore, 2018; Garivier et al., 2019). We emphasize that Corollary 2.1 is a special case of a much more
general result. In particular, we proved the bound on the Allocation-Estimation CoeÔ¨Écient, Eq. (26), using
tools which hold for general classes (e.g. Proposition 2.1). An analysis of AE2
?specialized to Ô¨Ånite-armed
bandits would likely yield a tighter result. /
2.6.3 Structured Bandits
Many bandit problems exhibit richer structure than the multi-armed bandit setting, and the study of these
settings has been the focus of much of the recent work on instance-optimal learning. We next consider one
such setting, that of structured bandits with bounded eluder dimension (Russo and Van Roy, 2013).
Example 2.3 (Structured Bandits with Bounded Eluder Dimension) .Consider a bandit problem with
unit-variance Gaussian noise but where the means are now given by a general function class Fmapping from
to[0;1]:
M=fM() =N(f();1)jf2Fg: (27)
For such general settings, we might hope to capture the complexity of learning in terms of generalized notions
of dimension forF. We consider one such notion here: the eluder dimension.
DeÔ¨Ånition 2.5 (Eluder Dimension (Russo and Van Roy, 2013)) .Let dE(F;"0)denote the length of the
longest sequence of actions f1;:::;dgsuch that, for each nd, there exist functions f;f02FwithqPn 1
i=1(f(i) f0(i))2"0butf(n) f0(n)>"0. Theeluder dimension of function class Fat scale"
is then deÔ¨Åned as dE(F;") = sup"0"dE(F;"0)_1.
The eluder dimension can be thought of as quantifying how easily a function class can be ‚Äúexplored‚Äù: evaluating
a pair of functions on dE(F;")points allows one to determine whether they are nearly identical over the
entire space. It is known to be bounded for many standard classes‚Äîfor example, for linear function classes
with dimension d,dE(F;")eO(dlog1=")‚Äîand is also closely related to the disagreement coeÔ¨Écient (Foster
et al., 2020). Furthermore, it can be shown to be a suÔ¨Écient condition for bounded (worst-case) regret in
general bandit problems (Russo and Van Roy, 2013). The following result shows that the eluder dimension
bounds the Allocation-Estimation CoeÔ¨Écient.
Proposition 2.3. For the structured bandit class Mconsidered in (27), we have
Cexp(M?;)16dE(F;p
=2)
:
This implies that
aecM
"(M?)16dE(F;p
=2)
for scale=c"28
?
dE(F;1
2?)2and ?:= minn
?
min;1=n?
"=36o
;
wherec>0is a universal constant.
Proposition 2.3 highlights the ability of the Allocation-Estimation CoeÔ¨Écient to adapt to the inherent com-
plexity of ‚Äúexploring‚Äù for the model class under consideration. We henceforth abbreviate dE:=dE(F;p
=2).
It is straightforward to show that Assumptions 2.1 and 2.2 are met in this setting with LKL;VM4(see
Appendix D.2). Furthermore, Assumption 2.3 can be shown to hold with dcovscaling with the covering
number ofFin the distance d(f;f0) =sup2jf() f0()jandCcov=O(1). In general, it must be shown
that n?
"is bounded for each M?and class of interest; as we show in the following examples, it is bounded for
standard structured bandit settings such as linear bandits. We have the following corollary to Theorem 2.2.
23Corollary 2.2. In the structured bandit setting with bounded eluder dimension considered above, AE2
?has
regret bounded as
EM?[Reg(T)](1 +")g?log(T) + poly(dE;dcov;1
";1
?
min;n?
"=36;log logT)log6=7(T):
To the best of our knowledge, Corollary 2.2 is the Ô¨Årst result to show that it is possible to obtain the instance-
optimal rate in classes with bounded eluder dimension, with lower-order terms scaling only polynomially in
the eluder dimension. More generally, Corollary 2.2 illustrates that AE2
?can adapt to the structural properties
of the given model class, and achieve regret scaling with existing notions of intrinsic dimension. /
We next consider two examples of structured bandits where it is known that the eluder dimension is bounded:
linear bandits and generalized linear models. While these results are immediate given Corollary 2.2, the
additional structure present in these settings allows us to obtain somewhat more explicit results.
Example 2.4 (Linear Bandits) .Consider the class of linear bandits with unit-variance Gaussian noise
deÔ¨Åned as
M=fM() =N(h;xi;1)j2g;
where Rdis some convex set with `2diameterO(1)andX:=fx:2gRdis the arm set, which
we assume haskxk21for all2. As in Example 2.3, Assumptions 2.1 and 2.2 are met in this setting
withLKL;VM4; furthermore, Assumption 2.3 is also met with dcov=O(d)andCcov=O(1). We then have
the following bound on the AEC.
Proposition 2.4. For the linear bandit class MdeÔ¨Åned above, we have
aecM
"(M?)cd3
"21
?
min+n?
"=368
polylog
d;1
";1
?
min;n?
"=36
for a universal constant c>0.
Using Proposition 2.4, we obtain the following corollary to Theorem 2.2.
Corollary 2.3. In the linear bandit setting deÔ¨Åned above, AE2
?has regret bounded as
EM?[Reg(T)](1 +")g?log(T) + poly(d;1
";1
?
min;n?
"=36;log logT)log6=7(T):
Corollary 2.4 has lower-order terms scaling similarly to the best known lower-order terms in the linear bandit
setting (Tirinzoni et al., 2020; Kirschner et al., 2021). These works, however, develop algorithms which
are specialized to the linear bandit setting, while Corollary 2.4 is the instantiation of a more general result
designed for arbitrary general decision-making settings.
The following result shows that under general conditions, we can bound the parameter n?
"for linear bandits.
Proposition 2.5 (Informal) .For linear bandits satisfying certain regularity conditions, n?
"is bounded by a
polynomial function of problem parameters and a geometry-dependent term scaling with the structure of X
and.
The full statement of Proposition 2.5 is given in Proposition D.2. The regularity condition for Proposition 2.5
requires primarily that ?lies suÔ¨Éciently far within the interior of (see Appendix D.2.4 for further details).
We remark that the guarantees given in both Tirinzoni et al. (2020) and Kirschner et al. (2021) scale with
geometric parameters very similar to n?
".
/
Example 2.5 (Generalized Linear Models) .In the generalized linear model setting, we take the model class
to be
M=fM() =N(g(h;xi);1)j2g;
24where andXare as in Example 2.4, and g()is a known link function which is increasing and Lipschitz, but
potentially nonlinear. Let gmaxandgmindenote upper and lower bounds on the derivative of g, respectively:
gmax:= max
2;x2co(X)g0(h;xi)andgmin:= min
2;x2co(X)g0(h;xi):
As in the linear bandit setting, we can show that Assumptions 2.1 and 2.2 are both met with LKL;VM4,
and that Assumption 2.3 is also met with dcov=O(d)andCcov=O(gmax). Furthermore, under the same
conditions as for linear bandits, n?
"can be bounded for generalized linear models exactly as for linear bandits,
but with an additional scaling of (gmax
gmin)2. We then have the following.
Proposition 2.6. For the generalized linear model class MdeÔ¨Åned above, we have
aecM
"(M?)cd3g3
max
"2g3
min1
?
min+n?
"=368
polylog
d;1
";1
?
min;n?
"=36
for a universal constant c>0.
Using Proposition 2.4, we obtain the following corollary to Theorem 2.2.
Corollary 2.4. In the generalized linear model setting deÔ¨Åned above, AE2
?has regret bounded as
EM?[Reg(T)](1 +")g?log(T) + poly(d;gmax
gmin;1
";1
?
min;n?
"=36;log logT)log6=7(T):
To the best of our knowledge, this is the Ô¨Årst result to obtain Ô¨Ånite-time instance-optimality for generalized
linear models with lower-order terms polynomial in problem parameters. /
2.6.4 Contextual Bandits
The previous examples illustrate that AE2
?is able to learn eÔ¨Éciently in a variety of structured bandit settings.
We now show that it leads to new guarantees for Ô¨Ånite-action contextual bandits with general function
approximation.
Example 2.6 (Contextual Bandits with Finitely Many Arms) .Consider the contextual bandit setting with
context setX(which could be arbitrarily large) and action set Asuch thatA:=jAj<1. LetpXdenote
the context distribution, which we assume is known to the learner. The learning protocol is then, for step
t= 1;2;3;::::
1. Environment samples context xtpX.
2. Learner chooses action at2A, receives reward rt.
We assume that rt=f?(xt;at) +wtforwtN(0;1), for somef?:XA! [0;1]. We assume as well that
the learner is given access to a set of functions Fsuch thatf?2F.
To view this setting as a special case of the DMSO framework, we take the decision space to be the set
 = (X!A )of all policies mapping from XtoA, and takeO=Xas the observation space. The learner‚Äôs
decision at round tis a policy t, and they receive a reward-observation pair (rt;ot) = (rt;xt)under the
processxtpX,rtN(f?(xt;t(xt));1). The model class Mis the set of all instances of this form for
f?2F.
The following result shows that the Allocation-Estimation CoeÔ¨Écient is be bounded by the number of actions
A, and isindependent of the size of the context space. See Appendix D.3 for a proof.
Proposition 2.7. For the contextual bandit setting, we can bound
Cexp(M?;")4A
";
which implies that
aecM
"(M?)cA3
"21
?
min+n?
"=368
;
25for a universal constant c>0.
As in the cases of bandits with bounded eluder dimension, n?
"must be bounded for each M?and classFof
interest. It is straightforward to show, however, that Assumptions 2.1 and 2.2 are met in this setting with
LKL;VM4, and, furthermore, that Assumption 2.3 is also met with dcovscaling as the covering number of
Fin the distance d(f;f0) =supx2X;a2Ajf(x;a) f0(x;a)j, andCcov=O(1). We then have the following
corollary.
Corollary 2.5. In the Ô¨Ånit-action contextual bandit setting considered above, AE2
?has regret bounded as
EM?[Reg(T)](1 +")g?log(T) + poly(A;d cov;1
";1
?
min;n?
"=36;log logT)log6=7(T):
To the best of our knowledge, Corollary 2.5 is the Ô¨Årst instance-optimal guarantee in the contextual bandit
setting with general function approximation that obtains lower-order term scaling polynomially in problem
parameters. Notably, the lower-order term scales independently of the size of the context space, jXj. We
anticipate that extending this result to contextual bandit settings that have large action spaces, but which
exhibit additional structure allowing for eÔ¨Écient exploration (e.g., linearity), will be straightforward. /
2.7 Application: Tabular Reinforcement Learning
As a Ô¨Ånal application of our results, we turn to the setting of episodic tabular reinforcement learning.
Episodic Markov decision processes. Recall that episodic reinforcement learning is a special case of
the DMSO framework in which each model M2Mis an episodic Markov Decision Process (MDP) given by
the tupleM= (S;A;H;fPM
hgH
h=1;fRM
hgH
h=1;s1). HereSis a set of states, Aa set of actions, Hthe horizon,
PM
h:SA!4Sthe probability transition kernel at step h,RM
h:SA!4 Rthe reward distribution at
steph, ands1a deterministic initial state, which we take to be Ô¨Åxed across models. We assume that RM
h(s;a)
is unit-variance Gaussian, and that ErhRM
h(s;a)[rh]2[0;1=H].6
The decision space consists of non-stationary policies = (1;:::;H), whereh:S!A. For a Ô¨Åxed
policy, an episode proceeds in an MDP Mproceeds as follows. First, beginning from the initial state s1, we
take action a11(s1), receive reward r1RM
1(s1;a1), and transition to s2PM
1(js1;a1). This continues
forHsteps at which point the episode terminates and the process repeats. We deÔ¨Åne fM() :=EM;PH
h=1rh
as the expected reward achieved over the entire episode under this process.
Each round t2[T]in the DMSO framework corresponds to an episode in the underlying MDP M?. At each
round, the learner selects a policy t, and receives reward rt=PH
h=1rt
handot= (st
1;at
1;rt
1;:::;st
H;at
H;rt
H),
where (st
1;at
1;rt
1;:::;st
H;at
H;rt
H)is the trajectory that results from executing tinM?for a single episode.
Tabular model class. In the tabular RL setting, it is assumed that S:=jSjandA:=jAjare both Ô¨Ånite,
and we take to be the set of all deterministic policies. In addition to assuming that Mconsists of tabular
MDPs, we restrict to the following subclass:
Mtab(Pmin) :=
M= (S;A;H;fPM
hgH
h=1;fRM
hgH
h=1;s1) : min
s;a;s0;hPM
h(s0js;a)Pmin
:(28)
While the assumption that mins;a;s0;hPM
h(s0js;a)Pminmay be seen as restrictive, the guarantees we
provide scale only with log1
Pmin, soPmincan be taken to be extremely small without aÔ¨Äecting the result
signiÔ¨Åcantly.
Note that when our results are specialized to reinforcement learning, ?
mindenotes the gap between the
performance of the optimal policy, and the next-best deterministic policy. This quantity can be lower
bounded in terms of other standard quantities including gaps in the rewards at each state and the transition
probabilities.
6This assumption only serves to ensure that fM()2[0;1], in line with the convention for the rest of the paper. Our results
continue to hold up to poly(H)factors if ErhRM
h(s;a)[rh]2[0;1].
26Toward instantiating Theorem 2.2 in this tabular RL setting, we Ô¨Årst provide a bound on the Allocation-
Estimation CoeÔ¨Écient, which we establish by Ô¨Årst bounding the Uniform Exploration CoeÔ¨Écient.
Proposition 2.8. ForM M tab(Pmin), we can bound7
CH
exp(M?;")cSAH2log2H
"2
for a universal constant c>0, which implies that
aecM
"(M?)cS5A5H14log10H
"41
?
min+n?
"=3624
log41
Pmin
for a universal constant c>0.
Next, it can be shown that Assumptions 2.1 to 2.3 hold for M M tab(Pmin)with constants (see Ap-
pendix D.5):
LKL=VM=O(Hlog 1=Pmin); d cov=O(S2AH); C cov=O(H=P min):
We then obtain the following corollary to Theorem 2.2.
Corollary 2.6. ForM M tab(Pmin)and",AE2
?has regret bounded by
EM?[Reg(T)](1 +")g?log(T) + poly(S;A;H;1
";1
?
min;log1
Pmin;n?
"=36;log log(T))log6=7(T):
To our knowledge, Corollary 2.6 is the Ô¨Årst guarantee for tabular RL that achieves the instance-optimal rate
while obtaining lower-order terms that scale only polynomially in problem parameters. As noted previously,
existing approaches to instance-optimal regret in tabular RL (Ok et al., 2018; Dong and Ma, 2022) have
lower-order terms that scale exponentially in problem parameters, and as a result are truly asymptotic in
nature.
While Corollary 2.6 is stated in terms of n?
"for the sake of generality, as we show in Appendix D.5, if M?has
rewards that are suÔ¨Éciently small (in particular, if it satisÔ¨Åes ErhRM?
h(s;a)[rh]<1=H2for all (s;a;h )), then
we can bound
n?
"cg?
?
min
1 +g?
"(?
min)2
:
In this case, Corollary 2.6 scales polynomially in all standard problem parameters.
Let us remark that the prior work of Dong and Ma (2022) does not require that PM
h(s0js;a)Pminas we
do (the work of Ok et al. (2018) only holds for ergodic MDPs, itself a very strong assumption). However, the
lower-order term obtained in Dong and Ma (2022) scales polynomially in the inverse probability of observing
the trajectory that occurs with minimum (non-zero) probability. In general, this will scale exponentially in
H, and inversely with the probability of the transition with minimum (non-zero) probability occurring, that
ismins;a;s0;h:PM
h(s0js;a)>0PM
h(s0js;a). Thus, while we must impose the stronger condition that all transitions
occur with some probability Pmin, our bounds only scale logarithmically in this quantity, and polynomially in
S;A;andH, a signiÔ¨Åcant improvement over Dong and Ma (2022). Understanding whether it is possible to
remove the additional restrictions we impose while still obtaining reasonable Ô¨Ånite-time performance is an
interesting direction for future work.
As far as we are aware, there is no prior work on instance-optimal algorithms for RL settings with general
function approximation. While we have only instantiated Theorem 2.2 and AE2
?in the tabular RL setting,
the tools we have developed can also be applied to RL with general model classes. Exploring the application
ofAE2
?to, for example, bilinear classes (Du et al., 2021) is an exciting avenue for future work.
7HereCH
expdenote the uniform exploration coeÔ¨Écient as deÔ¨Åned in DeÔ¨Ånition 2.4, but with DKL(k)replaced with D2
H(;).
To prove Proposition 2.8, we show that a variant of Proposition 2.1 still holds with this alternate deÔ¨Ånition of Cexp(M;").
272.8 Overview of Analysis
To close this section we brieÔ¨Çy sketch the proof of the regret bound for AE2(Theorem 2.1); the proof of the
regret bound for AE2
?(Theorem 2.2) builds on these ideas, but is slightly more involved. See Appendix C for
full proofs.
Let us refer to the exploit phase as the subset of rounds tin which Line 6 of AE2is reached, and refer to
theexplore phase as the subset of rounds in which Line 8 is reached. We focus on bounding the regret
in the explore phase‚Äîit can be shown (Lemma C.1) that in the exploit phase, where the if statement on
Line 6 is true, cMs=?for all butO(log logT)rounds, so that the regret incurred in this phase is at most
O(log logT).
LetsTdenote the total number of rounds in the explore phase up to time T. Fix an explore round s2[sT].
We bound the regret ?(ps)by considering three cases.
Case 1:M?2MnMgl
"=6(s;nmax).In this case, sis not an optimal (normalized) allocation for M?, but
we can use the AEC to argue that the information gained by the algorithm is large. In particular, since ps
plays!swith probability at least 1 q, we can bound
1
EcMs[Eps[DKL cM()kM?()
]]1
1 q1
EcMs[E!s[DKL cM()kM?()
]]
(a)
1
1 qmin
;!24sup
M2MnMgl
"=6(;nmax)1
EcMs[E!s[DKL cM()kM()
]]
.1
1 qaec"=12(M)
as long as nmaxis chosen appropriately; here, (a)follows because M?2MnMgl
"(s;nmax)by assumption in
this case, and by the choice of sand!sgiven in Eq. (19). Rearranging this gives
1.1
1 qaec"=12(M)EcMs[Eps[DKL cM()kM?()
]]:
This reÔ¨Çects that, when M?2MnMgl
"=6(s;nmax), our choice of psensures that cMsandM?can be
distinguished, with the amount of information gained lower bounded by O(aec"=12(M) 1)
Adding and subtracting2
1 qaec"=12(M)EcMs[Eps[DKL cM()kM?()
]]to?(ps), and using that
?(ps)1always, we then have that the instantaneous regret in this case is bounded by
?(ps) = ?(ps) 2
1 qaec"=12(M)EcMs[Eps[DKL cM()kM?()
]]
+2
1 qaec"=12(M)EcMs[Eps[DKL cM()kM?()
]]
 1 +2
1 qaec"=12(M)EcMs[Eps[DKL cM()kM?()
]]:
Summing over s, it follows that the total regret in this case is bounded by
sTX
s=1?(ps)Ifsin Case 1g2
1 qaec"=12(M)Est KL(sT) sTX
s=1Ifsin Case 1g:
Furthermore, since regret is always non-negative‚Äîthat is, ?(p)0for allp‚Äîrearranging this inequality
leads to a bound on the total number of times this case can occur:
sTX
s=1Ifsin Case 1g2
1 qaec"=12(M)Est KL(sT):
Critically, as given in (18), Est KL(sT)scales at most poly-logarithmically in sT. Thus, as long as sTis at
mostO(logT), the total regret incurred in this case (as well as the total number of times this case can occur),
will be at most O(aec"=12(M)log logT).
28Case 2:M?2Mgl
"=6(s;nmax)and?2cMs.In this case, we have that sis a Graves-Lai optimal
allocation for M?. Thus, it follows that
?(s)(1 +"=6)g?=n?and inf
M2Malt(M?)Es[DKL(M?()kM())](1 "=6)=n?
for some n?nmax. This implies that, for any M2Malt(M?), we can bound
?(ps) = ?(ps) (1 +")g?Eps[DKL(M?()kM())] + (1 +")g?Eps[DKL(M?()kM())]
.(1 +"=6)g?=n? (1 +")(1 "=6)g?=n?+ (1 +")g?Eps[DKL(M?()kM())]
. "g?=n?+ (1 +")g?Eps[DKL(M?()kM())]
 "g?=nmax+ (1 +")g?Eps[DKL(M?()kM())]
Since this bound holds uniformly for all M2Malt(M?), it follows that the total regret in this case can be
bounded as
sTX
s=1?(ps)Ifsin Case 2g.(1 +")g?sTX
s=1inf
M2Malt(M?)Eps[DKL(M?()kM())]If?2cMsg
 "g?
nmaxsTX
s=1Ifsin Case 2g:
To bound this, the key observation is that, if we explore at round s, then it must be the case that, for all
cMs2cMs, there exists some M2Malt(cMs)such thatPs 1
i=1EcMi[logPcM;i(ri;oi)
PM;i(ri;oi)]log(TlogT). Using
Assumption 2.1 and Assumption 2.2 to move from cMitoM?and to relate the observed log-likelihood
ratios to the KL divergence, we can furthermore show that
inf
M2Malt(M?)sTX
s=1Eps[DKL(M?()kM())]If?2cMsg
. inf
M2Malt(M?)sTX
s=1EcMsh
logPcM;s(rs;os)
PM;s(rs;os)i
+psTEst KL(sT).log(TlogT) +psTEst KL(sT):
This allows us to bound
(1 +")g?sTX
s=1inf
M2Malt(M?)Eps[DKL(M?()kM())]If?2cMsg.(1 +")g?logT+g?psTEst KL(sT):
Thus, as long as sT=O(logT), we can bound the total regret incurred in Case by (1 +")g?logT+o(logT).
Using that regret is always lower bounded by 0 in the same fashion as Case 1, we can further use this to
bound the total number of times that Case 2 occurs by
sTX
s=1Ifsin Case 2gnmax
"g?
g?logT+g?psTEst KL(sT)
:
The intuition for this case is that, since we are playing a Graves-Lai allocation for M?, the regret will scale
with g?, the instance-optimal rate, and, furthermore, the allocation will allow us to distinguish M?from
alternatives M2Malt(M?). Using that the total estimation error is bounded, and that we only enter the
explore phase if there exists some M2Malt(M?)that we cannot distinguish from M?, this ultimately implies
that the total number of times this phase occurs, and therefore the total regret incurred by this phase, is
bounded.
Case 3:M?2 Mgl
"=6(s;nmax)and?62cMs.In this case, we bound ?(ps)by adding and sub-
tracting EcMs[Eps[DKL cM()kM?()
]]in the same fashion as Case 1. Since ?62cMs, it can be
shown that smust place 
( min)probability mass on M2 Malt(M?), allowing us to lower bound
EcMs[Eps[DKL cM()kM?()
]]using the same reasoning as in Case 2. In total, we can show that the
regret in this case is bounded by O(g?
minEst KL(sT)), and the number of times this case can occur is at most
O(nmax
"minEst KL(sT)).
29Concluding the Proof. Combining all three cases, we have shown that the regret of the explore phase is
bounded by
(1 +")g?logT+eO1
1 qaec"=12(M)Est KL(sT) +g?psTEst KL(sT) +g?
minEst KL(sT)
:
Furthermore, using our bounds on the number of times each case can occur, one can show that sT=O(logT).
Since Est KL(sT)is at most polylogarithmic in sT, it follows that the regret is bounded as
(1 +")g?logT+eO
aec"=12(M) +aec1=2
"=12(M)log1=2T
;
as stated in Theorem 2.1.
3 Lower Bounds for Learning the Optimal Allocation
TheAE2algorithm achieves instance-optimal regret by explicitly learning an "-optimal Graves-Lai allocation
for the underlying model M?. In this section, we introduce an abstract formulation for the problem of learning
an optimal allocation (Section 3.1), and provide lower bounds which show that the Allocation-Estimation
CoeÔ¨Écient is a fundamental limit for this task (Section 3.2). We then present several examples illustrating
lower bounds on the Allocation-Estimation CoeÔ¨Écient (Section 3.3), and discuss how our lower bounds relate
to the problem of minimizing regret (Section 3.4).
Additional Notation. Throughout this section we will also make use of the following deÔ¨Ånition:
(M;";nmax) (29)
:=
24 :9n2(0;nmax]s.t.M()(1 +")gM
n; inf
M02Malt(M)E[DKL(M()kM0())]1 "
n
:
That is, (M;";nmax)denotes the set of normalized allocations that are Graves-Lai optimal for Mwith
tolerance", and have normalization factor at most nmax. Unless otherwise stated, the results in this section
do not make use of Assumption 1.3 or Assumption 2.4.
3.1 Learning the Optimal Allocation: Minimax Formulation
We consider the following protocol, which captures the task of learning an optimal Graves-Lai allocation for
an unknown model M?.
‚Ä¢Fort= 1;:::;T, sampletptand observe (rt;ot).
‚Ä¢Based on the entire history HT= (1;r1;o1);:::; (T;rT;oT), output a normalized allocation b24 .
The allocation may be randomized according to a distribution q244.
We formalize an algorithm for this task as a pair A= (p;q), whereq(jHT)is the distribution over bgiven
the history, and p=fptgt2[T]is a sequence of exploration distributions of the form pt(jHt 1). We let
PM;A()denote the law of HTwhenMis the underlying model and Ais the algorithm, and let EM;A[]denote
the corresponding expectation. The goal of the algorithm is to ensure that bis an"-optimal allocation for
M?with high probability, i.e.
PM?;A
b2(M?;";nmax)
1 
for some failure probability >0and normalization factor nmax>0
Intuitively, learning an optimal Graves-Lai allocation is closely related to achieving instance-optimal regret,
but there are some subtle technical diÔ¨Äerences which we discuss in detail in the sequel. We study the former
task because we Ô¨Ånd it to be more amenable to non-asymptotic lower bounds, and because it captures the
behavior of ‚Äúnatural‚Äù algorithms such as AE2and essentially every existing asymptotically optimal algorithm
we are aware of.
30Minimax framework. To provide lower bounds on the complexity of learning an optimal Graves-Lai
allocation, we consider a minimax framework. Our main quantity of interest will be:
Tgl(M;";nmax;) = inf
Ainfn
T2NjPM;A
b2(M;";nmax)
1 ;8M2Mo
: (30)
This represents the earliest time T2Nfor which there exists an algorithm that learns an "-optimal allocation
with probability at least 1 , and does so uniformly for all M2M. Recall that for our upper bounds
(Theorem 2.1), the Allocation-Estimation CoeÔ¨Écient gives a bound on the lower-order terms in the regret
(reÔ¨Çecting the time required to learn an "-optimal allocation) that holds uniformly for all models in the class
M. To understand the optimality of uniform bounds of this type, a minimax framework is natural. This
framework also naturally complements recent non-asymptotic algorithms for linear models such as (Tirinzoni
et al., 2020; Kirschner et al., 2021), where the complexity of exploration is captured by problem-dependent
quantities such as the feature dimension, which are bounded uniformly for all models in the class. Nonetheless,
exploring other notions of optimality (for example, instance-dependent complexity) for learning the Graves-Lai
allocation is an interesting direction for future research.
Note that the quantity (30) does not place any constraint on the regret of the algorithm under consideration.
It will also be useful to consider the notion
Tgl(M;";nmax;;R)
= inf
Ainfn
T2NjPM;A
b2(M;";nmax)
1 ;EM;A[Reg(T)]Rlog(T);8M2Mo
;(31)
which captures the minimax complexity of learning the Graves-Lai allocation, subject to the constraint that
the algorithm achieves logarithmic regret throughout the learning process. This notion is particularly well
suited to complement the upper bounds achieved by algorithms such as AE2.
We remark that the restriction to allocations with normalization factor no more than nmaxin the deÔ¨Ånitions
above is natural for several reasons. First, without such a restriction, the returned allocation can place an
arbitrarily small amount of mass on informative actions, and an arbitrarily large amount of mass on the
optimal action, since any Graves-Lai optimal allocation is still Graves-Lai optimal if the amount of mass on
the optimal action is increased arbitrarily. While technically Graves-Lai optimal, such allocations do not
reÔ¨Çect an allocation one could play over a Ô¨Ånite time horizon in order to certify the optimal decision ?‚Äîas
any algorithm with Ô¨Ånite-time guarantees must do‚Äîand therefore do not reÔ¨Çect the cost such an algorithm
must pay to learn an allocation. Second, given knowledge of the optimal action, any Graves-Lai optimal
allocation which has a normalization factor larger than nmaxcan be transformed into a Graves-Lai optimal
allocation with normalization factor nmaxby adjusting the mass on the optimal action, assuming nmaxis
taken to be suÔ¨Éciently large (in particular, as large as nmax(M;"); cf. Eq. (20)). Therefore, if we restrict
our attention to the task to learning the Graves-Lai allocation for a subclass of models which agree on the
optimal action (as we do in Theorem 3.2), any lower bound for learning an allocation with normalization
nmaxalso applies to learning an unrestricted allocation, for large enough nmax. Finally, AE2itself plays
allocations with bounded normalization, which as we note in Section 2.2, does not aÔ¨Äect the optimality of its
performance‚Äîallocations with bounded normalization are always suÔ¨Écient.
3.2 Main Result
We state two lower bounds. The Ô¨Årst scales with the version of the Allocation-Estimation CoeÔ¨Écient appearing
in our upper bound (Theorem 2.1), but leads to a lower bound on Tgl, while the second lower bound scales
with the AEC for a restriction M, but is exponentially stronger in the sense that it provides a similar lower
bound on log(Tgl). We remark brieÔ¨Çy that the regularity conditions of Section 2.1 are not required to hold
here, unless otherwise stated.
Theorem 3.1 (Main lower bound‚Äîweak variant) .Let">0,nmax>0, andM0Mbe given, and set
:="
2minf1;infM2M0gM=nmaxg. Unless
T >
8sup
M2M+aecM
2"(M0;M);
31any algorithm must have, for some M2M 0:
PM;Ah
b =2(M;";nmax)i

6:
Stated equivalently, Theorem 3.1 implies that for any " >0, if we set =c"infM2M0gM=nmaxfor
suÔ¨Éciently small numerical constant c, then
Tgl(M;";nmax;)sup
M2M+aec2"(M0;M):
Remark 3.1 (Choice ofM0).Theorem 3.1 is stated with respect to an arbitrary subset, M0, ofM. While
one could simply choose M0 M, in some cases it is advantageous to choose M0(M. In particular, note
that our lower bound scale with infM2M0gM. For classesMwhere infM2MgM= 0‚Äîwhich will be the case,
for example, ifMcorresponds to a model class where reward means form a compact set‚Äîit is advantageous
to restrictM0so that it corresponds only to instances with gM>0.
We remark as well that the restriction of the lower bound to M0is somewhat analogous to our upper bound
Theorem 2.2, which provides guarantees in terms of the AEC of a restriction of M,M?. We may therefore
chooseM0 M?to obtain lower bounds matching the scaling of Theorem 2.2. In the following section we
provide several examples of how M0can be chosen to yield intuitive lower bounds.
Lastly, let us mention that M0may also be chosen to yield lower bounds that have a more instance-dependent
Ô¨Çavor. For example, given some instance M?, we could choose M0to correspond to all instances identical
toM?up to a permutation of the decisions, in which case Theorem 3.1 is will yield lower bounds on the
performance of any algorithm on a permutation of M?, rather than over the entire class.
Remark 3.2. The lower bound in Theorem 3.1, for M0=M, scales with the quantity
sup
M2M+aec"(M;M) sup
M2co(M)aec"(M;M)sup
24Maec"(M;) =aec"(M);
which at Ô¨Årst glance might appear to be larger than the version of the AEC appearing in our upper bounds.
However, there is no contradiction, because these quantities can be shown to be equivalent (up to problem-
dependent parameters) under the assumptions with which Theorem 2.1 is proven.
Our second lower bound yields a lower bound on log(Tgl)as opposed to Tgl‚Äîa signiÔ¨Åcantly stronger
result‚Äîbut scales with the AEC for a restricted class. To state the result, for M2M+andM0M, deÔ¨Åne
Mopt
0(M) =
M2M 0jMM; D KL 
M()kM()
= 082M	
:
This represents the set of models Mwhere 1) the optimal decisions for Mare also optimal for Mand 2)
playing the an optimal decision reveals no information that can distinguish MandM. Our second lower
bound scales with the AEC for Mopt
0(M), and is restricted to algorithms with low regret.
Theorem 3.2 (Main lower bound‚Äîstrong variant) .Let">0,nmax>0, andM0Mbe given, and deÔ¨Åne
="
2minf1;infM2M0gM=nmaxg. Unless
sup
M2M0gM
M
minlog(T)
(2)sup
M2M+aecM
2"(Mopt
0(M);M);
there is no algorithm that simultaneously ensures that
1.EM;A[Reg(T)]2gMlog(T);8M2M 0.
2.PM;Ah
b =2(M;";nmax)i

12;8M2M 0.
Equivalently, Theorem 3.2 implies that for any ">0, if we set=c"infM2M0gM=nmaxfor a suÔ¨Éciently
small numerical constant c, then forR:= 2 supM2M0gM,
log(Tgl(M;";nmax;;R))2infM2M0M
min
Rsup
M2M+aecM
2"(Mopt
0(M);M):
323.3 Examples
As we discuss in the sequel, the dependence on the Allocation-Estimation CoeÔ¨Écient in our lower bounds
(particularly Theorem 3.2) qualitatively matches the dependence on the AEC in our main upper bounds,
Theorems 2.1 and 2.2. We defer a detailed discussion comparing our upper and lower bounds for a moment,
and make matters concrete by considering three examples: the informative arm example from the introduction
(Example 1.1), multi-armed bandits, and tabular reinforcement learning. We defer proofs for all examples to
Appendix E.4.
Example 3.1 (Searching for an Informative Arm (revisited)) .LetMbe the class constructed in Example 1.1
with parameters N;A2Nand2(0;1=2). LetM0denote the restriction of Mto models with M
minfor
some 2(0;1=6)(so thatMis unique), and fM(M)<1. As long as N16andlog(1+A)2=(A 1),
we have
sup
M2M+aecM
"(Mopt
0(M);M)N
4: (32)
For this construction, we have M
minand
(1)gMO( 1)for allM2M 0. Thus, Theorem 3.2
implies that any algorithm with EM;A[Reg(T)]2gMlog(T)for allM2M 0must fail to learn an "-optimal
allocation with probability = 
("=nmax)unless
sup
M2MgM
M
minlog(T)&"2
n2maxN
;
which implies that log(Tgl(M0;";nmax;;R))&"2=n2
maxNforR= 2supM2MgM=O( 1). Any Graves-
Lai allocation need only take at most O( 1)pulls to eliminate alternative instances, so an appropriate
choice of nmaxisO( 1), yielding log(Tgl(M0;";nmax;;R))&2"2N. While the dependence on the
parameter nmax;";>0here is certainly loose, this formalizes the intuition sketched in the introduction:
any algorithm that learns an optimal allocation must explore 
(N)times, yet any algorithm that achieves
near-instance-optimal regret EM;A[Reg(T)]2gMlog(T). 1log(T)can play a sub-optimal decision no
more than roughly  1log(T)=times, leading to the constraint that
log(T)&N:
We can also apply Theorem 3.1 to show that any algorithm must fail to learn an "-optimal allocation with
probability = 
("=nmax)unlessT&"=nmaxN
. /
Example 3.2 (Finite-Armed Bandit) .LetA6and2(0;1=2)be given and set  = [A]. LetMbe the
set of all multi-armed bandit instances with Gaussian noise:
M=n
M() =N(fM();1=2)jfM2[0;1]Ao
(33)
and letM0=fM2M :jMj= 1;M()2[;2]for6=M;fM(M)<1gdenote the set of all bandit
instances where suboptimal arms have gaps on order . Then for all "2(0;1=32),
sup
M2MaecM
"(Mopt
0(M);M)c" 2A
2; (34)
wherec>0is an absolute constant.
It can be shown that, by the construction of M0,gM= (A=)for allM2M 0, so/"minf1;A
nmaxg.
Theorem 3.1 then implies that any algorithm must fail to learn an "-optimal allocation with probability
= 
("minf1;A
nmaxg)unless
T&min
1;A
nmax
A
"2:
Any Graves-Lai allocation need only take O(A
2)pulls to eliminate all alternate instances, so a reasonable
choice of nmaxis therefore O(A
2). With this choice of nmax, we haveTgl(M;";nmax;)&A
". We remark
33that the scaling on all parameters here is natural. Intuitively, we would expect that we need to pull each
arm at least once to learn a near-optimal allocation, yielding an 
(A)scaling. In addition, note that for
multi-armed bandits, the optimal allocation places mass /1
2on arms with gap . To correctly estimate
this proportion requires an accurate estimate of , which becomes increasingly diÔ¨Écult as becomes smaller,
yielding an 
(1
)scaling. Finally, as we decrease ", we require that the returned allocation becomes closer to
a truly optimal allocation, and we would therefore expect an 
(1
")scaling.
Note that the result derived by applying Theorem 3.1 above only gives a lower bound on T. To obtain a lower
bound on log(T), we combine Theorem 3.2 and Eq. (34) with nmax=O(A
2)as above, which implies that
any algorithm with EM;A[Reg(T)]2gMlog(T)for allM2M 0must fail to learn an "-optimal allocation
with probability = 
("minf1;A
nmaxg)unless
sup
M2M0gM
M
minlog(T)&A;
or equivalently log(Tgl(M;";nmax;;R ))&A
supM2M0gM=M
minforR= 2supM2MgM. To see why such scaling
is natural, note that any algorithm which has EM;A[Reg(T)]2gMlog(T)for each instance Mcan aÔ¨Äord
to explore (that is, play a suboptimal decision) at most 2gM
M
minlog(T)times, or their regret could exceed
2gMlog(T). However, no algorithm has any hope of learning an optimal allocation unless they play every
arm at least once, so taking at least Apulls of suboptimal arms seems unavoidable, and we therefore would
expect that we must have 2gM
M
minlog(T)&A, which is precisely the necessary scaling shown here.
We make two remarks on this log(T)lower bound. First, note that due to the presence of the 2term in Eq.
(34) (which we believe to be loose), the lower bound we derive by applying Theorem 3.2 does not scale with
the parameter " 1, as one might hope. Second, as noted, for M2M 0forM0chosen as in Example 3.2, we
have gM= 
(A=), in which cases the dependence on Acancels, and the lower bound becomes trivial. Note
that this is somewhat to be expected. Theorem 3.2 will give a trivial lower bound whenever supM2M0gMis
much larger than the AEC. Recall that in our upper bound, Theorem 2.2, the leading order g?logTterm
will dominate the lower-order terms once
g?logT&
(aec"(M)):
Therefore, if g?is much larger than the AEC, Theorem 2.2 simply gives an upper bound scaling as approxi-
mately g?logT, as long as logT= 
(1). The interpretation in this setting is that the complexity of learning
the Graves-Lai allocation is dominated by the regret incurred by playingthe Graves-Lai allocation, which we
know is necessary from Proposition 1.1, and therefore we would not expect lower-order terms to be signiÔ¨Åcant
components of the regret, as reÔ¨Çected by Theorem 2.2. However, as we have already shown, In settings such
as Example 3.1 where this is not the case and the AEC is much larger than g?, Theorem 3.2 will give a
non-trivial lower bound which reÔ¨Çects the diÔ¨Éculty of learning the optimal allocation. /
Example 3.3 (Tabular Reinforcement Learning) .LetS;A;H2Nand2(0;1=2)be given, and assume
thatSA24andHlog2(S=2). LetMbe the set of all tabular MDPs with 1) jSj=S,jAj=Aand
horizonH, and 2) Gaussian rewards with variance 2= 1=2(cf. Section 2.7). Let M0be the result of
restrictingMin the same fashion as Example 3.2. Then for all "2(0;1=32),
sup
M2MaecM
"(Mopt
0(M);M)c" 2SA
2; (35)
wherec>0is an absolute constant.
It can be shown that, by the construction of M0,gM
(SA=)for allM2M 0. Thus, analogous to the
multi-armed bandit example, Theorem 3.1 and Eq. (35) imply that any algorithm must fail to learn an
"-optimal allocation with probability = 
("minf1;SA
nmaxg)unless
T&min
1;SA
nmax
SA
"2:
34Choosing nmax=O(SA
2)givesTgl(M;";nmax;)&SA
". With this same choice of nmax, Theorem 3.2 implies
that any algorithm with EM;A[Reg(T)]2gMlog(T)for allM2M 0must fail to learn an "-optimal allocation
with probability = 
(")unless
sup
M2M0gM
M
minlog(T)&SA;
or equivalently log(Tgl(M;";nmax;;R))&SA
supM2M0gM=M
minforR= 2 supM2M0gM.
/
3.4 Discussion and Interpretation
Our lower bounds show that the Allocation-Estimation CoeÔ¨Écient serves as a fundamental limit on the
sample complexity required to learn an approximate Graves-Lai allocation. In particular, they capture
phenomena such as the necessity of searching for an informative arm in Example 1.1 that are missed by
purely asymptotic analyses. To the best of our knowledge, our lower bounds represent the Ô¨Årst attempt to
systematically understand the sample complexity of learning the Graves-Lai allocation in a general decision
making framework. As such, they are somewhat coarse (in particular, the dependence on parameters such
as",nmax, and M
minis almost certainly loose), and they are best thought of as a starting point for further
research. In what follows, we provide additional interpretation of the results, and highlight some of the most
interesting remaining questions.
Regret versus learning the optimal allocation. Theorems 3.1 and 3.2 lower bound the sample com-
plexity required to learn an "-optimal Graves-Lai allocation. Intuitively, this task is closely related to achieving
instance-optimal regret. Our analysis of AE2shows that it is suÔ¨Écient , and many prior works aim to directly
estimate the optimal allocation as well. However, it is unclear to what extent learning the optimal allocation
isnecessary to achieve instance-optimal regret.
In more detail, it is quite straightforward to show that if an algorithm achieves instance-optimal regret, its
empirical frequencies act as an optimal allocation in expectation .
Lemma 3.1. Let"2(0;2), and suppose that Assumption 2.4 holds. Fix T2Nand consider an algorithm A
such that for all M2M,
EM;A[Reg(T)](1 +")gMlog(T):
For eachM2M, deÔ¨ÅneM2R
+viaM() =EM;Ah
T()
log(T)i
, whereT()denotes the number of pulls of
decision, and deÔ¨Åne M=M=kMk1. Then if
log(T)6
"log
sup
M2M2gM
M
minlog(T)
;
we have that for all M2M,
M2(M;"): (36)
This result gives a guarantee on the expected frequencies of any instance-optimal algorithm, but does not
give any guarantee for the realized frequencies. As such, without further assumptions on the algorithm under
consideration, it is unclear whether instance-optimal regret implies that it is possible to learn an optimal
allocation with high or even constant probability. We cannot currently rule out the existence of pathological
algorithms for which Mis optimal in expectation, yet the empirical arm frequencies deviate from the mean
with moderate probability. Nonetheless, if one is willing to make stronger assumptions on the algorithm
under consideration‚Äîin particular, that the second moment of regret is controlled‚Äîthen it is possible to
derive lower bounds on regret directly.
Theorem 3.3 (SimpliÔ¨Åed version of Theorem E.3) .Let the time horizon T2Nand"2(0;1=2)be given,
and suppose that Assumptions 2.2 and 2.4 hold. Suppose there exists an algorithm Awith the property that
for allM2M: 1)EM;A[Reg(T)](1 +")gMlog(T), 2)p
EM;A[(Reg(T))2]2gMlog(T), and 3) for all
352, ifEM;A[T()]6= 0, then EM;A[T()]1. Then if we deÔ¨Åne ="minf1;infM2M0gM
3gM=M
min+nM"g, it
must be the case that
log3(T)2
Csup
M2M+aecM
4"(Mopt(M);M):
forCO
(supM2MgM
M
min)4V2
Mlog( 1)
"2
.
See Appendix E.5 for a full statement and details. The idea behind the proof is to 1) show (via robust mean
estimation) that any instance-optimal algorithm with well-behaved tails can be used to estimate the optimal
allocation with high probability (with a small blowup in time horizon), and then 2) appeal to Theorem 3.2.
More work is required to understand whether 1) we can prove lower bounds on regret directly, and 2) whether
it is possible to show that low regret and learning the optimal allocation are equivalent in a stronger sense.
Comparing upper and lower bounds. Keeping the diÔ¨Äerences between regret minimization and learning
the optimal allocation in mind, let us highlight that the lower bound on Tprovided by Theorem 3.2 seems to
qualitatively match the upper bound from Theorems 2.1 and 2.2. In particular, ignoring problem-dependent
parameters and polylogarithmic factors, the upper bound Theorem 2.1 scales, for every model M2M, as
EM[Reg(T)](1 +")gMlog(T) +eO+(aec"(M)):
In order for this bound to simplify to, say,
EM[Reg(T)](1 + 2")gMlog(T);
we need
gMlog(T)e
+(1)aec"(M)
";
which has similar scaling to the lower bound
log(T)&e
+(1)sup
M2M+aecM
"(Mopt
0(M);M)
from Theorem 3.2. As discussed in the prequel, the former result is concerned with regret, while the
latter considers the task of learning the optimal allocation, but the scaling log(T)&aec"(M)seems to be
fundamental for both. Of course, beyond the gap between regret and learning the allocation, there is still
much room to improve the dependence on problem-dependent parameters in both results.
Comparing Theorem 3.1 and Theorem 3.2. Theorem 3.1 and Theorem 3.2 exhibit an interest-
ing dichotomy: Theorem 3.1 places no constraints on the regret of the algorithm under consideration,
and gives a lower of the form T&e
+(1)supM2M+aecM
"(M0;M), while Theorem 3.2 gives a lower
bound of the form log(T)&e
+(1)supM2M+aecM
"(Mopt
0(M);M), or equivalently T&exp(e
+(1)
supM2M+aec"(Mopt
0(M);M)); the latter lower bound is exponentially stronger, with the caveat that 1) the
classM0is replaced with the subclass Mopt
0(M)and 2) Theorem 3.2 assumes that the algorithm achieve
nearly-instance optimal regret for every model in M0(EM;A[Reg]2gMlog(T)). In what follows, we argue
that this tradeoÔ¨Ä is fundamental.
‚Ä¢First, let us consider the role of the assumption EM;A[Reg]2gMlog(T). Without this assumption,
the lower bound from Theorem 3.1 is qualitatively tight: if the algorithm explores optimally for every
roundt2[T], it gains roughly (supM2M+aecM
"(M0;M)) 1units of information per round, which is
suÔ¨Écient to identify an optimal allocation as soon as T&supM2M+aecM
"(M0;M).
‚Ä¢On the other hand, if the require that EM;A[Reg]2gMlog(T), then for each M2M 0, the algorithm
can aÔ¨Äord to explore (i.e., play a non-optimal action) at most 2gM
M
minlog(T)times. This changes the
36‚ÄúeÔ¨Äective‚Äù time horizon for exploration to T0= 2gM
M
minlog(T), but only if we restrict to models for which
playing an optimal decision gives no information. This is precisely what the subclass Mopt
0(M)captures:
modelsM2M 0for which decisions that are optimal for Mlead to no information. Combining
these insights leads to the lower bound T0gM
M
minlog(T)&
(1)supM2M+aecM
"(Mopt
0(M);M)in
Theorem 3.2.
We remark in passing that the deÔ¨Ånition of Mopt
0(M), which places the constraint that DKL 
M()kM()
=
0;82Mis somewhat coarse. We expect that both Theorem 3.2 and Theorem 2.1/Theorem 2.2 can be
improved to scale with
Mopt
0(M;) =
M2M 0jMM; D KL 
M()kM()
282M	
for1=p
T; the intuition is that we get 
(T)rounds worth of information on Mfor ‚Äúfree‚Äù, which facilitates
accurate estimation.
Minimax versus instance-dependent lower bounds. As mentioned in the prequel, our lower bounds
have a (constrained) minimax Ô¨Çavor. SpeciÔ¨Åcally, Theorem 3.2 shows that if Tis not suÔ¨Éciently large, then for
any algorithm, there must exist a ‚Äúworst-case‚Äù model M2Mfor which the algorithm either 1) fails to achieve
(approximately) instance-optimal regret or 2) fails to learn an "-optimal Graves-Lai allocation. While this is
quite diÔ¨Äerent from a classical minimax analysis, and certainly is closely connected to instance-optimality, an
interesting direction for future work is to develop a fully instance-dependent understanding of the complexity
of learning Graves-Lai allocations.
4 Additional Related Work
In this section, we discuss further related work not already covered in detail.
Asymptotic guarantees for general decision making. For the general decision making framework we
consider, which allows for arbitrary model classes and subsumes structured bandits and reinforcement, the
only prior works we are aware of that achieve the instance-optimal lower bound from Graves and Lai (1997)
are Komiyama et al. (2015), which restricts to Ô¨Ånite observation spaces, and Dong and Ma (2022), which
restricts to Ô¨Ånite decision spaces; these works do not provide non-asymptotic guarantees.
Many works provide purely asymptotic instance-optimality guarantees for more specialized settings, including
multi-armed bandits (Lai and Robbins, 1985; Garivier et al., 2016; Lattimore, 2018; Garivier et al., 2019),
linear bandits (Lattimore and Szepesvari, 2017; Hao et al., 2019, 2020), and general structured bandits
(Burnetas and Katehakis, 1996; Magureanu et al., 2014; Combes et al., 2017; Van Parys and Golrezaei, 2020;
Degenne et al., 2020b). Some of these works do provide non-asymptotic bounds on regret, but these results
generally have lower-order terms that scale linearly in jj, which renders them vacuous until log(T)&jj;
we consider such results to be asymptotic in spirit. Along these lines, it is worth discussing Jun and Zhang
(2020), which provides non-asymptotic guarantees for structured bandits in which the lower-order terms scale
with a quantity K that aims to capture the number of ‚ÄúeÔ¨Äective arms‚Äù. While this quantity can improve
overjjin certain situations, it is not clear whether it is well behaved for standard classes of interest (e.g.,
linear bandits).
Non-asymptotic guarantees for linear bandits. For linear bandits, a number of recent works provide
non-asymptotic instance-optimal regret bounds in which lower order terms scale only with the dimension d
rather than the number of decisions jj(Tirinzoni et al., 2020; Kirschner et al., 2021). These results take
advantage of the specialized geometric structure of the linear bandit setting (e.g., existence of optimal design)
for exploration, and cannot be directly adapted to general function approximation, but our results can be
viewed as generalizing these guarantees.
37Reinforcement learning. For reinforcement learning, a number of works‚Äîmostly focusing on tabular
settings or linear function approximation‚Äîprovides non-asymptotic guarantees that are instance-dependent,
but not necessarily instance-optimal (Simchowitz and Jamieson, 2019; Al Marjani and Proutiere, 2021;
Dann et al., 2021; Al Marjani et al., 2021; Wagenmaker et al., 2022b; Wagenmaker and Jamieson, 2022;
Wagenmaker and Pacchiano, 2022). For instance-optimality, the results we are aware of are the classical work
of Agrawal et al. (1988) and very recent work Dong and Ma (2022), which provides asymptotic guarantees for
Ô¨Ånite-horizon tabular RL, Ok et al. (2018), which provides asymptotic guarantees for an inÔ¨Ånite-horizon setting
under ergodicity assumptions, and Tirinzoni et al. (2022) which provides PAC guarantees for deterministic
MDPs (though we note that the guarantee of Tirinzoni et al. (2022) is also achieved, up to Hfactors, by
Wagenmaker and Jamieson (2022)).
Instance-optimal PAC guarantees. Our discussion has largely centered on regret, which is the focus of
our work. For the PAC setting, where the goal is to identify the optimal decision (or a near-optimal decision)
as quickly as possible, a number of recent works have employed similar techniques to derive instance-optimal
algorithms for settings such as multi-armed and structured bandits (Kaufmann et al., 2016; Garivier and
Kaufmann, 2016; Russo, 2016; Degenne and Koolen, 2019; Degenne et al., 2019, 2020a). While many of these
works are asymptotic in nature, in specialized settings such as multi-armed bandits (Jamieson et al., 2014),
linear bandits (Fiez et al., 2019; Katz-Samuels et al., 2020), and linear dynamical systems (Wagenmaker
et al., 2021), recent work has shown that the optimal rates are achievable in Ô¨Ånite-time.
Complexity of learning the Graves-Lai allocation. The Allocation-Estimation CoeÔ¨Écient aims to
capture the sample complexity required to learn an "-optimal Graves-Lai allocation. To the best of our
knowledge, our work is the Ô¨Årst to study the complexity of learning the allocation with general function
approximation, but a small body of work has studied the complexity in simple settings such as top- kbandits
(Simchowitz et al., 2017; Chen et al., 2017), and graph bandits (Marinov et al., 2022a,b).
Minimax regret. While the focus of this work has been on instance-optimality, a large body of work exists
onminimax optimality, where the goal is to perform optimally on the hardestinstance within a class. This
line of work has established worst-case optimal (or nearly optimal) rates in settings such as multi-armed
bandits (Auer et al., 2002; Audibert and Bubeck, 2009), linear bandits (Dani et al., 2008; Abbasi-Yadkori
et al., 2011), tabular reinforcement learning (Dann et al., 2019; Zhang et al., 2021), and reinforcement
learning with function approximation (Zhou et al., 2021; Du et al., 2021). The recent line of work Foster
et al. (2021, 2022b, 2023) shows that, in the interactive decision-making setting considered in this work, the
minimax-optimal rates are governed by a quantity known as the Decision-Estimation CoeÔ¨Écient . While our
work takes inspiration and bears some similarity with this work, we remark that the techniques necessary to
establish instance-optimality are signiÔ¨Åcantly more intricate. It is also worth stating that it is always possible
to bound the DEC by the AEC; the converse, however, is not true.
5 Discussion
Our work initiates the systematic study of non-asymptotic instance-optimality in interactive decision making.
We close by highlighting a number of interesting open problems and future directions raised by our work. On
the technical side:
‚Ä¢Our upper bounds depend on a number of diÔ¨Äerent problem-dependent parameters, such as the minimum
gap in the lower-order terms. Can we improve the dependence on these parameters, or understand to
what extend they are necessary?
‚Ä¢Our lower bounds concern the problem of learning a near-optimal allocation. Like the upper bounds,
these results are likely loose in terms of dependence on various problem parameters, and new techniques
will be required to tighten them. Furthermore, it remains to develop a complete understanding of the
connections between this problem and the problem of minimizing regret. While our results show that
‚Äúwell-behaved‚Äù algorithms which achieve instance-optimal regret must pay a burn-in proportional to the
38cost of learning a near-optimal allocation, it remains unclear if this is truly necessary for algorithms
which only have optimal expected regret (but, for example, could exhibit heavy-tailed behavior).
‚Ä¢We show that boundedness of the Allocation-Estimation CoeÔ¨Écient is necessary to learn the optimal
allocation in a minimax sense, but a natural question for future research is to derive instance-dependent
guarantees for learning optimal allocations. The tools used to prove the Graves-Lai lower bound can
be applied similarly to prove an instance-dependent lower-bound on the cost of learning the optimal
allocation, but it is not clear that such lower bounds capture the true complexity of learning (since,
as with the Graves-Lai lower bound, realizing such a lower bound would itself require knowledge of
the ground truth instance). Is a minimax-style lower bound always necessary in order to capture the
performance of algorithms which do not assume initial knowledge of the ground truth instance?
‚Ä¢While our algorithm achieves the instance-optimal rate, its regret could scale linearly over shorter time
horizons, until it has learned a near-optimal allocation. Can we develop ‚Äúbest-of-both-worlds‚Äù algorithms
that achieve the same instance-optimal guarantees of AE2, yet also achieves the minimax-optimal rate
(for example, aO(p
T)-style guarantee) over shorter time horizons?
‚Ä¢The focus of this work is primarily on regret minimization, yet the challenge of learning the optimal
allocation also arises in the PAC setting. Does the AEC extend to the PAC setting, and can algorithms
be developed in the PAC setting which achieve the instance-optimal rate in the leading-order term,
while scaling with an AEC-like quantity in the lower-order term?
More broadly, it will be interesting to explore whether our framework and algorithm design ideas can be used
to develop practical and computationally eÔ¨Écient algorithms.
Acknowledgements
The authors would like to thank Johannes Kirschner for helpful discussions. The work of AW was supported
in part by NSF TRIPODS 62-2945 and NSF HDR 62-0221. A portion of this work was completed while AW
was an intern at Microsoft Research, and while visiting the Simons Institute for the Theory of Computing.
39References
Yasin Abbasi-Yadkori, D√°vid P√°l, and Csaba Szepesv√°ri. Improved algorithms for linear stochastic bandits.
InAdvances in Neural Information Processing Systems , 2011.
Rajeev Agrawal, Demosthenis Teneketzis, and Venkatachalam Anantharam. Asymptotically eÔ¨Écient adaptive
allocation schemes for controlled markov chains: Finite parameter space. Technical report, MICHIGAN
UNIV ANN ARBOR COMMUNICATIONS AND SIGNAL PROCESSING LAB, 1988.
Aymen Al Marjani and Alexandre Proutiere. Adaptive sampling for best policy identiÔ¨Åcation in markov
decision processes. In International Conference on Machine Learning , pages 7459‚Äì7468. PMLR, 2021.
Aymen Al Marjani, Aur√©lien Garivier, and Alexandre Proutiere. Navigating to the best policy in markov
decision processes. Advances in Neural Information Processing Systems , 34:25852‚Äì25864, 2021.
Chamy Allenberg, Peter Auer, L√°szl√≥ Gy√∂rÔ¨Å, and Gy√∂rgy Ottucs√°k. Hannan Consistency in On-Line Learning
in Case of Unbounded Losses Under Partial Monitoring , pages 229‚Äì243. Springer Berlin Heidelberg, Berlin,
Heidelberg, 2006. ISBN 978-3-540-46650-5. doi: 10.1007/11894841_20. URL http://dx.doi.org/10.1007/
11894841_20 .
Jean-Yves Audibert and S√©bastien Bubeck. Minimax policies for adversarial and stochastic bandits. In COLT,
volume 7, pages 1‚Äì122, 2009.
Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit problem.
Machine learning , 47(2-3):235‚Äì256, 2002.
S√©bastien Bubeck, Michael Cohen, and Yuanzhi Li. Sparsity, variance and curvature in multi-armed bandits.
InAlgorithmic Learning Theory , pages 111‚Äì127. PMLR, 2018.
Apostolos N Burnetas and Michael N Katehakis. Optimal adaptive policies for sequential allocation problems.
Advances in Applied Mathematics , 17(2):122‚Äì142, 1996.
Fan Chen, Song Mei, and Yu Bai. UniÔ¨Åed algorithms for RL with decision-estimation coeÔ¨Écients: No-regret,
PAC, and reward-free learning. arXiv preprint arXiv:2209.11745 , 2022.
Lijie Chen, Jian Li, and Mingda Qiao. Nearly instance optimal sample complexity bounds for top-k arm
selection. In ArtiÔ¨Åcial Intelligence and Statistics , pages 101‚Äì110. PMLR, 2017.
Richard Combes, Stefan Magureanu, and Alexandre Proutiere. Minimal exploration in structured stochastic
bandits. In Proceedings of the 31st International Conference on Neural Information Processing Systems ,
pages 1761‚Äì1769, 2017.
Varsha Dani, Thomas P Hayes, and Sham M Kakade. Stochastic linear optimization under bandit feedback.
InConference on Learning Theory (COLT) , 2008.
Christoph Dann, Tor Lattimore, and Emma Brunskill. Unifying pac and regret: Uniform pac bounds for
episodic reinforcement learning. Advances in Neural Information Processing Systems , 30, 2017.
Christoph Dann, Lihong Li, Wei Wei, and Emma Brunskill. Policy certiÔ¨Åcates: Towards accountable
reinforcement learning. In International Conference on Machine Learning , pages 1507‚Äì1516. PMLR, 2019.
Christoph Dann, Teodor Vanislavov Marinov, Mehryar Mohri, and Julian Zimmert. Beyond value-function
gaps: Improved instance-dependent regret bounds for episodic reinforcement learning. Advances in Neural
Information Processing Systems , 34:1‚Äì12, 2021.
R√©my Degenne and Wouter M Koolen. Pure exploration with multiple correct answers. Advances in Neural
Information Processing Systems , 32, 2019.
R√©my Degenne, Wouter M Koolen, and Pierre M√©nard. Non-asymptotic pure exploration by solving games.
Advances in Neural Information Processing Systems , 32, 2019.
R√©my Degenne, Pierre M√©nard, Xuedong Shang, and Michal Valko. GamiÔ¨Åcation of pure exploration for
linear bandits. In International Conference on Machine Learning , pages 2432‚Äì2442. PMLR, 2020a.
40R√©my Degenne, Han Shao, and Wouter Koolen. Structure adaptive algorithms for stochastic bandits. In
International Conference on Machine Learning , pages 2443‚Äì2452. PMLR, 2020b.
Omar Darwiche Domingues, Pierre M√©nard, Emilie Kaufmann, and Michal Valko. Episodic reinforcement
learning in Ô¨Ånite mdps: Minimax lower bounds revisited. In Algorithmic Learning Theory , pages 578‚Äì598.
PMLR, 2021.
Kefan Dong and Tengyu Ma. Asymptotic instance-optimal algorithms for interactive decision making. arXiv
preprint arXiv:2206.02326 , 2022.
Simon S Du, Sham M Kakade, Jason D Lee, Shachar Lovett, Gaurav Mahajan, Wen Sun, and Ruosong Wang.
Bilinear classes: A structural framework for provable generalization in RL. International Conference on
Machine Learning , 2021.
Tanner Fiez, Lalit Jain, Kevin G Jamieson, and Lillian RatliÔ¨Ä. Sequential experimental design for transductive
linear bandits. Advances in neural information processing systems , 32, 2019.
Dylan J Foster and Alexander Rakhlin. Beyond UCB: Optimal and eÔ¨Écient contextual bandits with regression
oracles.International Conference on Machine Learning (ICML) , 2020.
Dylan J Foster, Zhiyuan Li, Thodoris Lykouris, Karthik Sridharan, and Eva Tardos. Learning in games:
Robustness of fast convergence. Advances in Neural Information Processing Systems , 29, 2016.
Dylan J Foster, Alexander Rakhlin, David Simchi-Levi, and Yunzong Xu. Instance-dependent complexity of
contextual bandits and reinforcement learning: A disagreement-based perspective. Conference on Learning
Theory (COLT) , 2020.
Dylan J Foster, Sham M Kakade, Jian Qian, and Alexander Rakhlin. The statistical complexity of interactive
decision making. arXiv preprint arXiv:2112.13487 , 2021.
Dylan J Foster, Noah Golowich, Jian Qian, Alexander Rakhlin, and Ayush Sekhari. A note on model-free
reinforcement learning with the decision-estimation coeÔ¨Écient. arXiv preprint arXiv:2211.14250 , 2022a.
Dylan J Foster, Alexander Rakhlin, Ayush Sekhari, and Karthik Sridharan. On the complexity of adversarial
decision making. arXiv preprint arXiv:2206.13063 , 2022b.
Dylan J. Foster, Noah Golowich, and Yanjun Han. Tight guarantees for interactive decision making with the
decision-estimation coeÔ¨Écient. arXiv preprint arXiv:2301.08215 , 2023.
Aur√©lien Garivier and Emilie Kaufmann. Optimal best arm identiÔ¨Åcation with Ô¨Åxed conÔ¨Ådence. In Conference
on Learning Theory , pages 998‚Äì1027, 2016.
Aur√©lien Garivier, Tor Lattimore, and Emilie Kaufmann. On explore-then-commit strategies. In Advances in
Neural Information Processing Systems , pages 784‚Äì792, 2016.
Aur√©lien Garivier, Pierre M√©nard, and Gilles Stoltz. Explore Ô¨Årst, exploit next: The true shape of regret in
bandit problems. Mathematics of Operations Research , 44(2):377‚Äì399, 2019.
Todd L Graves and Tze Leung Lai. Asymptotically eÔ¨Écient adaptive choice of control laws in controlled
Markov chains. SIAM journal on control and optimization , 35(3):715‚Äì743, 1997.
Botao Hao, Tor Lattimore, and Csaba Szepesvari. Adaptive exploration in linear contextual bandit. arXiv
preprint arXiv:1910.06996 , 2019.
Botao Hao, Tor Lattimore, and Csaba Szepesvari. Adaptive exploration in linear contextual bandit. In
International Conference on ArtiÔ¨Åcial Intelligence and Statistics , pages 3536‚Äì3545. PMLR, 2020.
Elad Hazan and Satyen Kale. Better algorithms for benign bandits. Journal of Machine Learning Research ,
12(4), 2011.
Kevin Jamieson, Matthew Malloy, Robert Nowak, and S√©bastien Bubeck. lil‚Äôucb: An optimal exploration
algorithm for multi-armed bandits. In Conference on Learning Theory , pages 423‚Äì439. PMLR, 2014.
41Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E Schapire. Contextual
decision processes with low Bellman rank are PAC-learnable. In International Conference on Machine
Learning , pages 1704‚Äì1713, 2017.
Chi Jin, Qinghua Liu, and Sobhan MiryooseÔ¨Å. Bellman eluder dimension: New rich classes of RL problems,
and sample-eÔ¨Écient algorithms. Neural Information Processing Systems , 2021.
Kwang-Sung Jun and Chicheng Zhang. Crush optimism with pessimism: Structured bandits beyond
asymptotic optimality. Advances in Neural Information Processing Systems , 33:6366‚Äì6376, 2020.
Sham Machandranath Kakade. On the sample complexity of reinforcement learning . University of London,
University College London (United Kingdom), 2003.
Julian Katz-Samuels, Lalit Jain, Kevin G Jamieson, et al. An empirical process approach to the union bound:
Practical algorithms for combinatorial and linear bandits. Advances in Neural Information Processing
Systems, 33:10371‚Äì10382, 2020.
Emilie Kaufmann, Olivier Capp√©, and Aur√©lien Garivier. On the complexity of best-arm identiÔ¨Åcation in
multi-armed bandit models. The Journal of Machine Learning Research , 17(1):1‚Äì42, 2016.
Johannes Kirschner, Tor Lattimore, Claire Vernade, and Csaba Szepesv√°ri. Asymptotically optimal
information-directed sampling. In Conference on Learning Theory , pages 2777‚Äì2821. PMLR, 2021.
Junpei Komiyama, Junya Honda, and Hiroshi Nakagawa. Regret lower bound and optimal algorithm in Ô¨Ånite
stochastic partial monitoring. Advances in Neural Information Processing Systems , 28, 2015.
Tze Leung Lai and Herbert Robbins. Asymptotically eÔ¨Écient adaptive allocation rules. Advances in Applied
Mathematics , 6(1):4‚Äì22, 1985.
Tor Lattimore. ReÔ¨Åning the conÔ¨Ådence level for optimistic bandit strategies. The Journal of Machine Learning
Research , 19(1):765‚Äì796, 2018.
Tor Lattimore and Csaba Szepesvari. The end of optimism? an asymptotic analysis of Ô¨Ånite-armed linear
bandits. In ArtiÔ¨Åcial Intelligence and Statistics , pages 728‚Äì737. PMLR, 2017.
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver,
and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971 ,
2015.
G√°bor Lugosi and Shahar Mendelson. Mean estimation and regression under heavy-tailed distributions: A
survey.Foundations of Computational Mathematics , 19(5):1145‚Äì1190, 2019.
Stefan Magureanu, Richard Combes, and Alexandre Proutiere. Lipschitz bandits: Regret lower bound and
optimal algorithms. In Conference on Learning Theory , pages 975‚Äì999. PMLR, 2014.
Teodor V Marinov, Mehryar Mohri, and Julian Zimmert. Stochastic online learning with feedback graphs:
Finite-time and asymptotic optimality. arXiv preprint arXiv:2206.10022 , 2022a.
Teodor Vanislavov Marinov, Mehryar Mohri, and Julian Zimmert. Open problem: Finite-time instance
dependent optimality for stochastic online learning with feedback graphs. In Conference on Learning
Theory, pages 5644‚Äì5649. PMLR, 2022b.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex
Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through
deep reinforcement learning. Nature, 518(7540):529, 2015.
Jungseul Ok, Alexandre Proutiere, and Damianos Tranos. Exploration in structured reinforcement learning.
Advances in Neural Information Processing Systems , 31, 2018.
Ian Osband and Benjamin Van Roy. On lower bounds for regret in reinforcement learning. arXiv preprint
arXiv:1608.02732 , 2016.
42Daniel Russo. Simple bayesian algorithms for best arm identiÔ¨Åcation. In Conference on Learning Theory ,
pages 1417‚Äì1418, 2016.
Daniel Russo and Benjamin Van Roy. Eluder dimension and the sample complexity of optimistic exploration.
InAdvances in Neural Information Processing Systems , pages 2256‚Äì2264, 2013.
Daniel Russo and Benjamin Van Roy. Learning to optimize via information-directed sampling. Operations
Research , 66(1):230‚Äì252, 2018.
Ohad Shamir. A variant of azuma‚Äôs inequality for martingales with subgaussian tails. arXiv preprint
arXiv:1110.2392 , 2011.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian
Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go
with deep neural networks and tree search. nature, 529(7587):484, 2016.
Max Simchowitz and Kevin G Jamieson. Non-asymptotic gap-dependent regret bounds for tabular mdps.
Advances in Neural Information Processing Systems , 32, 2019.
Max Simchowitz, Kevin Jamieson, and Benjamin Recht. The simulator: Understanding adaptive sampling in
the moderate-conÔ¨Ådence regime. In Conference on Learning Theory , pages 1794‚Äì1834. PMLR, 2017.
Wen Sun, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, and John Langford. Model-based RL in
contextual decision processes: PAC bounds and exponential improvements over model-free approaches. In
Conference on learning theory , pages 2898‚Äì2933. PMLR, 2019.
Andrea Tirinzoni, Matteo Pirotta, Marcello Restelli, and Alessandro Lazaric. An asymptotically optimal
primal-dual incremental algorithm for contextual linear bandits. Advances in Neural Information Processing
Systems, 33:1417‚Äì1427, 2020.
Andrea Tirinzoni, Matteo Pirotta, and Alessandro Lazaric. A fully problem-dependent regret lower bound
for Ô¨Ånite-horizon mdps. arXiv preprint arXiv:2106.13013 , 2021.
Andrea Tirinzoni, Aymen Al-Marjani, and Emilie Kaufmann. Near instance-optimal pac reinforcement
learning for deterministic mdps. arXiv preprint arXiv:2203.09251 , 2022.
Bart PG Van Parys and Negin Golrezaei. Optimal learning for structured bandits. arXiv preprint
arXiv:2007.07302 , 2020.
Andrew Wagenmaker and Kevin Jamieson. Instance-dependent near-optimal policy identiÔ¨Åcation in linear
mdps via online experiment design. arXiv preprint arXiv:2207.02575 , 2022.
Andrew Wagenmaker and Aldo Pacchiano. Leveraging oÔ¨Ñine data in online reinforcement learning. arXiv
preprint arXiv:2211.04974 , 2022.
Andrew J Wagenmaker, Max Simchowitz, and Kevin Jamieson. Task-optimal exploration in linear dynamical
systems. In International Conference on Machine Learning , pages 10641‚Äì10652. PMLR, 2021.
Andrew J Wagenmaker, Yifang Chen, Max Simchowitz, Simon Du, and Kevin Jamieson. Reward-free rl is no
harder than reward-aware rl in linear markov decision processes. In International Conference on Machine
Learning , pages 22430‚Äì22456. PMLR, 2022a.
Andrew J Wagenmaker, Max Simchowitz, and Kevin Jamieson. Beyond no regret: Instance-dependent pac
reinforcement learning. In Conference on Learning Theory , pages 358‚Äì418. PMLR, 2022b.
Ruosong Wang, Russ R Salakhutdinov, and Lin Yang. Reinforcement learning with general value function
approximation: Provably eÔ¨Écient approach via bounded eluder dimension. Advances in Neural Information
Processing Systems , 33, 2020.
Chen-Yu Wei and Haipeng Luo. More adaptive algorithms for adversarial bandits. In Conference On Learning
Theory, pages 1263‚Äì1291. PMLR, 2018.
43Yuhong Yang and Andrew R Barron. An asymptotic property of model selection criteria. IEEE Transactions
on Information Theory , 44(1):95‚Äì116, 1998.
Andrea Zanette and Emma Brunskill. Tighter problem-dependent regret bounds in reinforcement learning
without domain knowledge using value function bounds. In International Conference on Machine Learning ,
pages 7304‚Äì7312. PMLR, 2019.
Zihan Zhang, Xiangyang Ji, and Simon Du. Is reinforcement learning more diÔ¨Écult than bandits? a
near-optimal algorithm escaping the curse of horizon. In Conference on Learning Theory , pages 4528‚Äì4531.
PMLR, 2021.
Dongruo Zhou, Quanquan Gu, and Csaba Szepesvari. Nearly minimax optimal reinforcement learning for
linear mixture markov decision processes. In Conference on Learning Theory , pages 4532‚Äì4576. PMLR,
2021.
44A Additional Notation
Mathematical Notation DeÔ¨Ånition
DKL(k) KL divergence
DH(;) Hellinger distance
DTV(;) Total variation distance
D(k) General divergence
4X Set of probability distributions over X
DMSO Notation
M Model
M?Ground truth model
M Set of models
M0 Arbitrary subset of M
; Decision, set of all decisions 
r;R Rewardr, set of all rewards R
o;O Observation o, set of all observations O
EM;[];PM;[] Expectation and distribution of (r;o)M()
EM[];PM[] Expectation and distribution induced over histories on M
fM() Expected reward playing onM,fM() =EM;[r]
M Optimal decision of model M,M2arg max2fM()
M Set of optimal decisions of model M
M() Gap of decision on modelM,M() =fM(M) fM()
M
min Minimum gap on model M(see Eq. (2))
M+Set of all possible models, M+=fM: !4ROjfM()2[0;1]g
Reg(T) Regret after Trounds (see Eq. (1))
LKL Lipschitz constant of KL divergence (see Assumption 2.1)
VM Sub-gaussian parameter of log-likehood ratio (see Assumption 2.2)
Ncov(M;;) (;)covering number of M(see DeÔ¨Ånition 2.1)
dcov;Ccov Bounds on covering number (see Assumption 2.3)
nM
"Information content of optimal decision on Mwith
tolerance"(see DeÔ¨Ånition 2.2)
nM
" Maximum information content of optimal decision on M,nM
"= supM2MnM
"
Mx;yMx;y=fM2M : M
minx;nM
"yg
? ?= minf?
min;1=n?
"=36g
M?Restriction ofMinduced by M?(see Eq. (25))
Graves-Lai Notation
glc(M;M) Graves-Lai CoeÔ¨Écient for class M, modelM(see Eq. (3))
gM;g?Graves-Lai CoeÔ¨Écient for model M,M?;gM=glc(M;M),g?=glc(M;M?)
gMMinimum non-zero Graves-Lai CoeÔ¨Écient on M,gM:= minM2M:gM>0gM
Malt(M) Alternate set for model M,Malt(M) =fM02Mj M\M0=?g
M?
alt Alternate set for M?,M?
alt=Malt(M?)
 Allocation, 2R
+
 Normalized allocation, 24 
! Exploration distribution, !24 
(M;") Set of"-optimal normalized Graves-Lai allocations for model M(see Eq. (10))
(M;";nmax)Set of"-optimal normalized Graves-Lai allocations for model Mwith
normalization factor at most nmax(see Eq. (29))
Mgl
"() Models for which is an"-optimal Graves-Lai allocation (see Eq. (11))
Mgl
"(;nmax)Models for which is an"-optimal Graves-Lai allocation with
normalization factor at most nmax(see Eq. (38))
IM(;M) Information content of onMwith respect toM(see Eq. (37))
IM() IM() =IM(;M)
Tgl(M0;";nmax;)Minimum time to learn Graves-Lai allocation over M0(see Eq. (30))
45AEC Notation
aecM
"(M0;M) AEC with tolerance ", model setM0, reference model M(see Eq. (12))
aec"(M;M) aec"(M;M) =aecM
"(M;M)
aec"(M) aec"(M) = supM2co(M)aec"(M;M)
aecM
"(M0;) AEC with randomized estimator (see Eq. (21))
aec"(M;) aec"(M;) =aecM
"(M;)
aec"(M) aec"(M) = sup24Maec"(M;)
aecD
"(M;M) AEC deÔ¨Åned with respect to general divergence (see Eq. (39))
aecD
"(M;) AEC with randomized estimator, general divergence (see Eq. (40))
Uniform Exploration
Notation
C
exp(") Uniform exploration coeÔ¨Écient with respect to at scale"(see DeÔ¨Ånition 2.4)
p
exp(") Uniform exploration distribution with respect to at scale"
Cexp(M;") Uniform exploration coeÔ¨Écient for class Mat scale"
CD;
exp(") Uniform exploration coeÔ¨Écient, general divergence (see DeÔ¨Ånition B.1)
pD;
exp(") Uniform exploration distribution, general divergence
CD
exp(M;") Uniform exploration coeÔ¨Écient for class M, general divergence
Estimation Notation
AlgKL Estimation oracle
Est KL(s) Cumulative KL estimation error (see DeÔ¨Ånition 2.3)
Est D(s) Cumulative estimation error with respect to divergence D(see Eq. (41))
dEst D(s) Cumulative estimation error with arguments Ô¨Çipped (see Eq. (42))
Divergences. For probability distributions PandQover a measurable space (
;F)with a common
dominating measure, we deÔ¨Åne the total variation distance as
DTV(P;Q) = sup
A2FjP(A) Q(A)j=1
2Z
jdP dQj
and (squared) Hellinger distance as
D2
H(P;Q) =Zp
dP p
dQ2
:
Interactive decision making. We formalize probability spaces in the same fashion as Foster et al. (2021,
2022b). Decisions are associated with a measurable space (;P), rewards are associated with the space
(R;R), and observations are associated with the space (O;O). The history after round tis denoted by
Ht= (1;r1;o1);:::; (t;rt;ot). We deÔ¨Åne

t=tY
i=1(RO );andFt=tO
i=1(P
R
O)
so thatHtis associated with the space (
t;Ft).
When the algorithm is clear from context, we let PMdenote the law it induces on HTwhenM: !4RO
is the underlying model, and let EM[]denote the corresponding expectation. We will also overload notation
somewhat and let PM;the density of (r;o)M().
Notation for complexity measures and allocations. We letT()denote the number of times decision
is taken up to time T. For2R
+, we deÔ¨Åne
IM(;M) = inf
M02Malt(M)X
2()DKL(M()kM0()); (37)
46so that we can write glc(M;M?) =inf2R
+P
2()M?()jIM?(;M)1	
. We abbreviate IM() =
IM(;M)whenever the class Mis clear from context. We will occasionally overload notation and write
M() =P
2()M()for2R
+. We also letM?
alt:=Malt(M?)and
Mgl
"(;nmax) :=fM2M :2(M;";nmax)g: (38)
Recall the deÔ¨Ånition
(M;") =
24 j9n2R+s.t.E[M()](1 +")gM
n; inf
M02Malt(M)E[DKL(M()kM0())]1 "
n
:
For a given 2(M;"), we refer to the n2R+which realizes
E[M()](1 +")gM
nand inf
M02Malt(M)E[DKL(M()kM0())]1 "
n
as thenormalization factor of.
General divergences. While in the main text we have focused on results that hold for the KL divergence,
throughout the appendix we will consider more general divergences D 
k
. In particular, rather than
Ô¨Åxing the divergence in Eq. (19) to the KL divergence, we utilize divergence D. We also perform online
estimation with respect to Drather than with respect to the KL divergence. While Dcould be an arbitrary
non-negative function, we make the following assumptions on it.
First, we replace Assumption 2.1 with the following more general assumption.
Assumption A.1. For allM;M0;M002M, and2, there exists some LKLsuch that
jDKL(M()kM00()) DKL(M0()kM00())]jLKLq
D 
M()kM0()
:
Note that, by Jensen‚Äôs inequality, Assumption A.1 immediately implies that, for 24(M),
DKL(M()kM00()) EM[DKL 
M()kM00()
]LKLq
EM[D 
M()kM()
]:
We in addition make the following assumption, which we note is met for the KL divergence.
Assumption A.2. For allM;M02co(M), and, we have
D2
H(M();M0())D 
M()kM0()
:
Furthermore, D 
k
is convex in its second argument.
A direct consequence of this assumption is that, when rewards are observed and bounded in [0;1], we have
jfM() fM0()jq
D 
M()kM0()
:
Throughout the appendix, we assume that Assumption A.1 and Assumption A.2 hold for our divergence D.
We also generalize the deÔ¨Ånition of the Allocation-Estimation CoeÔ¨Écient to account for general divergences as
aecD
"(M;M) := inf
;!24sup
M2MnMgl
"()1
E![D 
M()kM()
](39)
and
aecD
"(M;) := inf
;!24sup
M2MnMgl
"()1
EM[E![D 
M()kM()
]]: (40)
47Other variants of the AEC are generalized similarly with a superscript D. Our guarantees will also depend on
a notion of estimation error for general divergences, given by
Est D(s) :=sX
i=1EcMi[Epi[D 
M?()kcM()
]]: (41)
It will also be convenient to work with the following notion of estimation error:
dEst D(s) :=sX
i=1EcMi[Epi[D cM()kM?()
]]: (42)
B Technical Tools
B.1 Online Learning
In this section, we state online estimation guarantees for variants of the Tempered Aggregation algorithm
of Chen et al. (2022). Throughout the section we abbreviate Et 1[] =E[jHt 1;pt;t]. We recall that
PM;(r;o)denotes the density over rewards and observations (r;o)underM()
Algorithm 3 Tempered Aggregation
1:input:Finite classM.
2:Initialize1 Unif(M).
3:fort= 1;2;3;:::do
4:Receive (t;rt;ot).
5:Update estimator:
t+1(M)/t(M)exp1
2logPM;t(rt;ot)
8M2M:
Proposition B.1 (Tempered Aggregation, Finite Class Setting) .AssumejMj1 andM?2M. Then
Algorithm 3 produces estimates (t)T
t=1which satisfy, with probability at least 1 ,
TX
t=1EMt[Ept[D2
H(M?();M())]]2 logjMj
:
Proof of Proposition B.1. We follow closely the proof of Theorem C.1 of Chen et al. (2022). DeÔ¨Åne the
random variable
At:= logEMt"
exp 
logPM;t(rt;ot)
PM?;t(rt;ot)!#
:
We have
Et 1[exp( At)] =Et 1"
EMt"
exp 
1
2logPM;t(rt;ot)
PM?;t(rt;ot)!##
=X
M2Mt(M)Et 1"
exp 
1
2logPM;t(rt;ot)
PM?;t(rt;ot)!#
=X
M2Mt(M)Et 1"
EoM?(t)"s
PM;t(rt;ot)
PM?;t(rt;ot)##
=X
M2Mt(M)
1 1
2Ept[D2
H(M?();M())]
48where the last equality holds by the deÔ¨Ånition of the Hellinger distance. This implies that
1 Et 1[exp( At)] =1
2EMt
Ept[D2
H(M?();M())]
:
By Lemma A.4 of Foster et al. (2021), we have that with probability at least 1 ,
TX
t=1At+ log1
TX
t=1 logEt 1[exp( At)]
TX
t=1 
1 Et 1[exp( At)]
=1
2TX
t=1EMt
Ept[D2
H(M?();M())]
:
We turn to upper boundingPT
t=1At. Following the proof of Theorem C.1 of Chen et al. (2022), we have
TX
t=1At= log X
M2M1(M) exp TX
t=11
2logPM;t(rt;ot)
PM?;t(rt;ot)!!
:
SinceM?2M, we can then bound
TX
t=1At log 
1(M?) exp TX
t=11
2logPM?;t(rt;ot)
PM?;t(rt;ot)!!
= logjMj:
Combining these expressions gives the result.
Proposition B.2 (Tempered Aggregation, InÔ¨Ånite Class Setting) .LetMcovdenote a (;)-cover ofM
with covering number Ncov(M;;). If we apply Algorithm 3 to Mcov, we have that whenever M?2M, with
probability at least 1  T,
TX
t=1EMt[Ept[D2
H(M?();M())]]2 logNcov(M;;)
+T:
Proof of Proposition B.2. DeÔ¨ÅningAtas in Proposition B.1, the Ô¨Årst part of the proof is identical to
that of Proposition B.1, and we conclude that, with probability at least 1 ,
TX
t=1At+ log1
1
2TX
t=1EMt[Ept[D2
H(M?();M())]]
and
TX
t=1At= log X
M2Mcov1(M) exp TX
t=11
2logPM;t(rt;ot)
PM?;t(rt;ot)!!
:
LetEdenotetheeventassociatedwith Mcov, asdeÔ¨ÅnedinDeÔ¨Ånition2.1, whichsatisÔ¨Åes supM02MsupPM0(Ej
). LetfM2M covdenote the element in the cover which satisÔ¨Åes
logPM?;(r;o)
PfM;(r;o)=logPM?;(r;o) logPfM;(r;o);
for all (r;o; )with supM02MPM0;(r;ojE)>0. We then have
TX
t=1logPM;t(rt;ot)
PM?;t(rt;ot)=TX
t=1 
logPM;t(rt;ot)
PfM;t(rt;ot)+ logPfM;t(rt;ot)
PM?;t(rt;ot)!
49so we can bound
TX
t=1AtlogjMcovj+1
2TX
t=1logPM?;t(rt;ot)
PfM;t(rt;ot)
which gives that with probability at least 1 ,
TX
t=1EMt[Ept[D2
H(M?();M())]]2 logjMcovj+ 2 log1
+TX
t=1logPM?;t(rt;ot)
PfM;t(rt;ot):
LetEtdenote the event that Eoccurs at step t. Denote the event E1:=\T
t=1Etand
E2:=(TX
t=1EMt[Ept[D2
H(M?();M())]]2 logjMcovj+ 2 log1
+T)
:
Then
PM?[E2]PM?[E2\E1] +PM?[Ec
1]:
By deÔ¨Ånition ofEtand a union bound we have PM?[Ec
1]T. Furthermore, on the event E1we can bound,
for eachtT,
logPM?;t(rt;ot)
PfM;t(rt;ot):
Thus, it follows that on E1,PT
t=1logPM?;t(rt;ot)
PfM;t(rt;ot)T, which implies that PM?[E2\E1]. The result
follows.
B.2 Properties of Graves-Lai Program
In this section, we establish some basic properties of the Graves-Lai coeÔ¨Écient gMglc(M;M). Through
out the section, we omit dependence on the class Mfor various quantities of interest whenever it is clear
from context. Throughout, we will use the fact that whenever M
min>0,Mis unique.
B.2.1 Basic Properties of Graves-Lai Program
Lemma B.1. For anyM2Mandn>0, we have
gM
ninf
24
M() :IM()1
n
:
Proof of Lemma B.1. Assume the contrary. Then there exists some esuch that
M(e)<gM=nandIM(e)1=n:
However, since both M()andIM()are linear in rescaling of , this implies that
M(ne)<gMandIM(ne)1:
By deÔ¨Ånition we have
gM= inf
2R
+M()s.t.IM()1:
This is a contradiction, so the desired conclusion follows.
50Lemma B.2. For anyM2M+with M
min>0, we can bound
gMC
exp(1
4(M
min)2)
for=IM.
Proof of Lemma B.2. By deÔ¨Ånition we have
gM= inf
2R
+M()s.t. inf
M02Malt(M)X
()DKL(M()kM0())1
inf
2R
+kk1s.t. inf
M02Malt(M)X
()DKL(M()kM0())1:
Let24Mdenote the distribution with (M) = 1. Let and let pexp:=p
exp(")denote the uniform exploration
distribution deÔ¨Åned with respect to , andC
exp(")the corresponding uniform exploration coeÔ¨Écient, for
some"to be chosen.
Consider some M02Malt(M). Since EM[Ep[DKL 
M()kM00()
]] =Ep[DKL(M()kM00())]for all
M00andDKL(M()kM())= 0for all, it follows from the deÔ¨Ånition of the uniform exploration coeÔ¨Écient
that
Epexp[DKL(M()kM0())]1=C
exp(") =)sup
p24Ep[DKL(M()kM0())]"
or, alternatively,
sup
p24Ep[DKL(M()kM0())]>"=)Epexp[DKL(M()kM0())]>1=C
exp("):
IfM02Malt(M), then it follows that M62M0. Take some M02M0. By deÔ¨Ånition we have fM(M)
fM(M0) + M
minandfM0(M0)fM0(M). Thus,
M
minfM(M) fM(M0) +fM0(M0) fM0(M)
jfM(M) fM0(M)j+jfM0(M0) fM(M0)j
q
D 
M(M)kM0(M)
+q
D 
M(M0)kM0(M0)
:
This implies that there exists some such thatD 
M()kM0()
(M
min=2)2, so we can lower bound
sup
p24Ep[DKL(M()kM0())]1
4(M
min)2:
Thus, setting "=1
4(M
min)2, we have that
Epexp[DKL(M()kM0())]>1=C
exp(1
4(M
min)2):
It follows that the allocation =C
exp(1
4(M
min)2)pexprealizes
inf
M02Malt(M)X
()DKL(M()kM0()) = inf
M02Malt(M)C
exp(1
4(M
min)2)EpMexp[DKL(M()kM0())]1;
which proves the result.
Lemma B.3. Assume gM>0;M
min>0;andnM
1=4<1. Then it must be the case that
gMM
min1
maxM02M;2DKL(M()kM0()):
51Proof of Lemma B.3. Recall that
gM= inf
2R
+M()s.t. inf
M02Malt(M)X
()DKL(M()kM0())1:
By the deÔ¨Ånition of nM
1=4, for any allocation 2R
+satisfying infM02Malt(M)P
()DKL(M()kM0())
3=4, the allocation edeÔ¨Åned ase() =()for6=M, ande(M) =nM
1=2satisÔ¨Åes
inf
M02Malt(M)X
e()DKL(M()kM0())1=2;
and furthermore M() = M(e). It follows that
gMinf
2R
+M()s.t. inf
M02Malt(M)X
()DKL(M()kM0())3=4
inf
2R
+M()s.t. inf
M02Malt(M)X
()DKL(M()kM0())1=2; (M)nM
1=4:
If for allM02Malt(M)we haveDKL(M(M)kM0(M))>0, this implies that we can distinguish Mfrom
M0by playing only M. Furthermore, by what we have just shown, this can be achieved by playing Mat
most 2nM
1=4times. It follows that, if DKL(M(M)kM0(M))>0for allM02Malt(M), then gM= 0. Thus,
ifgM>0, there must exist some M02Malt(M)such thatDKL(M(M)kM0(M)) = 0.
We then have
gMinf
2R
+M()s.t.X
6=M()DKL(M()kM0())1
= inf
2R
+;(M)=0M()s.t.X
6=M()DKL(M()kM0())1
 inf
2R
+;(M)=0M
minkk1s.t. max
DKL(M()kM0())kk11
= M
min1
maxDKL(M()kM0()):
The result follows.
B.2.2 Properties of the Information Content of Optimal Decisions
Lemma B.4. Let"2[0;1=2)andn>0be given. We can bound, for any function g(!;M )andM0M
with infM2M0M
min>0,
inf
!;24sup
M2M0nMgl
2"(;n)g(!;M )inf
!;24sup
M2M0nMgl
"()g(!;M )
as long as
nmax
M2M0max
nM
";4gM
M
min;2gM
M
min
;
where
:= min
M2M0:gM>0mingM
gM+ 2nM";M
min"
4
:
Proof of Lemma B.4. Note that each of these expressions only depend on throughMgl
2"(;n)and
Mgl
"(), respectively. To prove the result, it therefore suÔ¨Éces to show that, for every 24 , there exists
024 such thatM0\Mgl
"()M 0\Mgl
2"(0;n).
52Fix24 . Consider M2M 0\Mgl
"(). By deÔ¨Ånition we have that there exists some n>0such that
M()(1 +")gM=nandIM()(1 ")=n;
where here IM() =IM(;M). We consider two cases.
Case 1:=Ifor some2.First, suppose that 6=M. Then we have
M
minM()(1 +")gM=n=)n(1 +")gM=M
min:
It follows that as long as n(1 +")gM=M
min, thenM2Mgl
"(;n).
Now, suppose that =M. In this case, by the deÔ¨Ånition of nM
", we immediately have that it suÔ¨Éces to take
n=nM
". Thus, for =I, we haveMgl
2"() =Mgl
"(;n)as long as
nmaxfnM
";(1 +")gM=M
ming:
Case 2a:6=Ifor any2.Fix some2(0;1=2)to be chosen. Suppose that there exists some 0
such that(0)1 , and note that there can exist at most one such 0. DeÔ¨Åne0as
0(0) = 1 ; 0() =
1 (0)();86=0
and note that 024 . Our goal is to show that Mgl
"()Mgl
2"(0;n).
Suppose that M=0. Then
M(0) =
1 (0)M()
1 (0)(1 +")gM
n:
Denote n0:= (
1 (0)1
n) 1. SinceIM()(1 ")=n, we haveIM(n)1 ". Then, by the deÔ¨Ånition of
nM
", we have
inf
M02Malt(M)X
6=Mn()DKL(M()kM0()) + nM
"DKL(M(M)kM0(M))1 2":
However, note that
IM(n00) = inf
M02Malt(M)X
6=Mn0
1 (M)()DKL(M()kM0()) + n0(1 )DKL(M(M)kM0(M))
= inf
M02Malt(M)X
6=Mn()DKL(M()kM0()) +(1 )(1 (0))n
DKL(M(M)kM0(M))
(a)
 inf
M02Malt(M)X
6=Mn()DKL(M()kM0()) + nM
"DKL(M(M)kM0(M))
(b)
1 2"
where (a)follows as long as
(1 )(1 (0))n
nM
"; (43)
and(b)follows from what we have just shown. Rearranging, we have that Eq. (43) is equivalent to
(1 (0))n
(1 (0))n+nM":
53Note that by Lemma B.1 and the deÔ¨Ånition of , we have
(1 ")gM
ninf
e24
M(e) :IM(e)1 "
n
M();
which implies that
(1 ")gMM()n(1 (M))n:
As the functionx
x+nM"is increasing in x, a suÔ¨Écient choice of for thisMis then
minfgM
gM+ 2nM";3=8g
Thus, for such a , we have that M2 Mgl
"(0;n0), which implies that M2 Mgl
2"(0;n0). Note that
(1 (0))n(1 +")gM=M
minin this case, so we have that M2Mgl
2"(0;n)as long as
n2gM
M
min:
Consider now the case where M6=0. In this case, deÔ¨Åning 0as before, we can bound
M(0)+ (1 )M(0)+(0)M(0)+ M()+ (1 +")gM=n:
Furthermore,
IM(0) = inf
M02Malt(M)X
6=0
1 (0)()DKL(M()kM0()) + (1 )DKL(M(0)kM0(0))
 inf
M02Malt(M)X
6=0(1 )()DKL(M()kM0()) + (1 )(0)DKL(M(0)kM0(0))
= (1 )IM()
(1 )(1 ")=n:
Since06=M, we can lower bound M()(1 )M
min, so
(1 )M
minM()(1 +")gM=n=)n(1 +")gM
(1 )M
min4gM
M
min
We can therefore bound "gM=nas long as
M
min"
4:
Considerthat satisÔ¨Åes this inequality. Then M(0)(1+2")gM=n. We can also lower bound (1 )(1 ")
(1 2")as long as"
1 "". Thus, as long as
minf1;M
min
4g"and n4gM
M
min;
we have that M2Mgl
2"(0;n).
Case 2b:6=Ifor any2.Finally, it remains to handle the case then there does not exist 0such
that(0)1 . In this case, we can always lower bound
M
minM()(1 +")gM=n=)n(1 +")gM
M
min;
54so as long as n(1+")gM
M
min, we haveM2Mgl
2"(;n).
Since we are in the regime where 6=I, it must be the case that if M2M 0\Mgl
"(), then gM>0. Thus,
a suÔ¨Écient choice of is
= min
M2M0:gM>0mingM
gM+ 2nM";M
min"
4
:
Concluding the Proof. To show the result, we need that nis large enough for each M2M 0. The
argument above shows that it suÔ¨Éces to take
nmax
M2M0max
nM
";4gM
M
min;2gM
M
min
for
:= min
M2M0:gM>0mingM
gM+ 2nM";M
min"
4
:
This proves the result.
Lemma B.5. For everyM2Mwith M
min>0, there exists some 2(M;")with normalization factor n
satisfying
ngM=M
min+nM
";
i.e., we have M2Mgl
"(;n).
Proof of Lemma B.5. Consider some allocation 2R
+such that
M()gMandIM()1: (44)
Let0denote the allocation satisfying 0() =()for6=M, and0(M) = 0. Note that M()
M
mink0k1, which implies that
k0k1gM=M
min:
Let00denote the allocation satisfying 00() =()for6=Mand00(M) =nM
". Then by deÔ¨Ånition of
nM
", and since satisÔ¨Åes Eq. (44), we have IM(00)1 ". Furthermore, it is straightforward to see that
M(00)gM. This implies that 00=k00k12(M;")with normalization factor k00k1. However, we can
bound
k00k1=k0k1+nM
"gM=M
min+nM
":
This proves the result.
B.2.3 Bounding Allocation-Estimation CoeÔ¨Écient via Uniform Exploration CoeÔ¨Écient
In this section we prove a generalized version of Proposition 2.1. In particular, rather than specializing to
the KL divergence, we consider a general divergence D. We deÔ¨Åne the uniform exploration coeÔ¨Écient with
respect toDas follows.
DeÔ¨ÅnitionB.1 (UniformExplorationCoeÔ¨Écient, GeneralDivergences) .For a randomized estimator 24M
and divergence D 
k
, we deÔ¨Åne the uniform exploration coeÔ¨Écient with respect to at scale">0as the
value of the following program:
CD;
exp(") := min
C2R+;p24
C8M;M02M :maxM002fM;M0gEM[Ep[D 
M()kM00()
]]1=C
=)maxp024Ep0[D 
M()kM0()
]"
:
55We deÔ¨ÅnepD;
exp(")as the minimizing distribution for this program, and let
CD
exp(M;") := sup
24MCD;
exp(")
denote the uniform exploration constant for class M.
Lemma B.6 (Formal version of Proposition 2.1) .Let"2[0;1=2)andM0Mbe given, and assume that
infM2M0gM>0;infM2M0M
min>0,supM2M0nM
1=4<1, and Assumptions 2.2, A.1 and A.2 hold. Then
for any24M0, we can bound
min
;!24sup
M2M0nMgl
"()1
EM[E![D 
M()kM()
]]CD
exp(M0;)
for any>0satisfying
p
min
M2M0min(
min1
81LKL;M
min
34VM
"
2gM=M
min+nM
"=36;M
min
3)
:
Proof of Lemma B.6. Let >0be some tolerance to be chosen. Let fMdenote some M2M 0such
thatEM[Epexp[D 
M()kM()
]]1=Cexp, where we abbreviate Cexp:=CD
exp(M0;)andpexp:=pD;
exp()
is a distribution that achieves the value of CD
exp(M0;)for; if such anfMdoes not exist, we let fM=
arg minM2M0EM[Epexp[D 
M()kM()
]]. Let"0>0be some value to be chosen, and let e2(fM;"0)
denote the allocation in (fM;"0)with smallest normalizing factor n. Letendenote the value of this normalizing
factor, then:
fM(e)(1 +"0)gfM=enandIfM(e)(1 "0)=en:
We can bound:
min
24min
!24sup
M2Mgl
"()c\M01
EM[E![D 
M()kM()
]] sup
M2Mgl
"(e)c\M01
EM[Epexp[D 
M()kM()
]]
 sup
M2Mgl
"(e)c\M0IfEM[Epexp[D 
M()kM()
]]1=Cexpg
EM[Epexp[D 
M()kM()
]]+Cexp; (45)
where here we take0
0= 0. IfEM[Epexp[D 
M()kfM()
]]>1=Cexp, then by deÔ¨Ånition of fM, for all
M2M 0we have
EM[Epexp[D 
M()kM()
]]>1=Cexp;
so we can simply bound Eq. (45) Cexp. Going forward, we assume this is not the case, so that
EM[Epexp[D 
M()kfM()
]]1=Cexp. Our goal is to show that, for small enough ,e2(M;")for
every other M2M 0withEM[Epexp[D 
M()kfM()
]]1=Cexp, so thatM2Mgl
"(e). This will imply
that there does not exist M2Mgl
"(e)c\M 0withEM[Epexp[D 
M()kfM()
]]1=Cexp, which further
implies that Eq. (45) Cexp.
Fix anyM2M 0. We note that by the deÔ¨Ånition of pexp, ifEM[Epexp[D 
M()kM()
]]1=Cexp, then
sup
p24Ep[D fM()kM()
];
This implies in particular that, for each ,D fM()kM()
.
56Step 1: EM[Epexp[D 
M()kM()
]]1=CexpimpliesM=fM.As noted, if
EM[Epexp[D 
M()kM()
]]1=Cexp;
we haveD fM()kM()
for all. Assume that M6=fM(note that since infM2M0M
min>0by
assumption, all M2M 0have unique optimal). By deÔ¨Ånition we have ffM(fM)ffM(M) + fM
minand
fM(M)fM(fM). Thus,
fM
minffM(fM) ffM(M) +fM(M) fM(fM)
jffM(fM) fM(fM)j+jfM(M) ffM(M)j
q
D fM(fM)kM(fM)
+q
D fM(M)kM(M)
:
This implies that there exists some such thatD fM()kM()
(fM
min=2)2. Assuming
min
M2M0(fM
min=3)2; (46)
this is a contradiction. Thus, it must be the case that M=fM, as long as Eq. (46) is satisÔ¨Åed.
Step 2: EM[Epexp[D 
M()kM()
]]1=Cexpimpliese2(M;).Under Assumption A.2, we can
bound,82,
jffM() fM()jq
D fM()kM()
p
:
This implies that, for any 24 ,
jfM() M()jjffM(M) fM(M)j+X
jffM() fM()j4p
:
In addition, under Assumption A.1, we have
DKL(M()kM0())DKL fM()kM0()
 LKLq
D fM()kM()
DKL fM()kM0()
 2LKLp
:
This implies, for any 24 ,
IM() = inf
M02Malt(M)X
()DKL(M()kM0())
 inf
M02Malt(M)X
()DKL fM()kM0()
 2LKLp

=IfM() 2LKLp

where the Ô¨Ånal equality uses that, given what we have already shown, M=fM, so thatMalt(M) =Malt(fM).
Repeating the calculation in the other direction, we get that jIM() IfM()j2LKLp
.
We next relate gMtogfM. By deÔ¨Ånition we have
(1 +"0)gM=eninf
24M()s.t.IM()(1 "0)=en:
Applying our perturbation bounds we can lower bound this as
inf
24fM() 4p
s.t.IfM()(1 "0)=en 2LKLp

gfM
((1 "0)=en 2LKLp
) 1 4p

57where the last inequality follows from Lemma B.1. This implies that
gfM((1 "0)=en 2LKLp
) 1(1 +"0)gM
en+ 4((1 "0)=en 2LKLp
) 1p
: (47)
Assuming that
p
"0 2("0)2
2(1 + 2"0)LKLen;
some algebra shows that
Eq. (47)(1 + 2"0)2gM+ 4(1 + 2"0)enp
:
Now we can bound
M(e)fM(e) + 4p

(1 +"0)gfM=en+ 4p

(1 + 2"0)3gM=en+ 4(1 + 2"0)2p
+ 4p

and
IM(e)IfM(e) 2LKLp
(1 "0)=en 2LKLp
:
If"0andare small enough so that
(1 + 2"0)31 +"=2and 4(1 + 2"0)2p
+ 4p
"gM=2en
and
1 "01 "=2and 2LKLp
"=2en;
then M(e)(1 +")gM=enandIM(e)(1 ")=en, which implies that e2(M;")with scaling factor en.
Step 3: Condition on .Altogether, we have assumed that satisÔ¨Åes Eq. (46) and, that for some
M2M 0withEM[Epexp[D 
M()kM()
]]1=Cexp, we have
p
"0 2("0)2
2(1 + 2"0)LKLen;4(1 + 2"0)2p
+ 4p
"gM=2en;2LKLp
"=2en (48)
and
(1 + 2"0)31 +"=2;1 "01 "=2:
Some algebra shows that, as long as "1, it suÔ¨Éces to take "0="=36to satisfy the latter two conditions.
Furthermore, some calculation shows that a suÔ¨Écient condition for Eq. (48) to be met is that
p
min1
81LKL;gM
17
"
en:
By Lemma B.5 and our choice of en, we can bound
en2gfM=fM
min+nfM
"=36
so it suÔ¨Éces that we take
p
min1
81LKL;gM
17
"2gfM
fM
min+nfM
" 1
:
58AsfMwas chosen to an arbitrary model in M0withEM[Epexp[D 
M()kfM()
]]1=Cexp, we take it to
minimize gfMover this constraint. It suÔ¨Éces then that
p
min1
81LKL;gfM
17
"2gfM
fM
min+nfM
"=36 1
:
Finally, by Lemma C.13 (under Assumption 2.2) and Lemma B.3, we can lower bound gfMfM
min=2VM.
Combining this condition with Eq. (46) gives the result.
C Proofs from Section 2
Organization of Appendix C. In this section we prove the main results from Section 2. We consider a
slightly generalized version of the setting in Section 2, where we allow for divergences other than just the KL
divergence, as described below. This section is organized as follows.
‚Ä¢First, in Appendix C.1, we give the proof of our main result, Theorem 2.1. We break this proof into
two principle components: bounding the regret of AE2in the exploit phase (Section C.1.1), and explore
phase (Section C.1.2). The key results in this section are Lemma C.3, which formalizes the key algorithm
intuition given in Section 2, showing that exploring via the AEC yields low regret, and Lemma C.4,
which shows that, to enter the explore phase, the total ‚Äúinformation gain‚Äù must be bounded as O(logT),
which ultimately yields the optimal leading-order scaling. We combine these results with our estimation
guarantees in Appendix C.1.3, where we give the proof of Theorem 2.1.
‚Ä¢In Appendix C.2, we extend AE2and Theorem 2.1 to the case where we assume no lower bound on the
minimum gap over the model class, Ô¨Årst presenting our main algorithm in this setting, Algorithm 5,
and then giving a proof of Theorem 2.2. The structure of this section is similar to Appendix C.1‚Äîthe
primary diÔ¨Äerence being a slightly diÔ¨Äerent argument to handle the need to adapt to the minimum gap
of the ground truth instance.
‚Ä¢Finally, in Appendix C.3 we present our estimation routine with covering, and prove that it achieves low
estimation error, and in Appendix C.4 we provide the proofs of miscellaneous results used throughout
Appendix C.
C.1 Regret Bound for Uniformly Regular Classes (Theorem 2.1)
In this section we prove Theorem C.1, which generalizes Theorem 2.1. To do so, we analyze Algorithm 4,
wich generalizes Algorithm 1 to allow for general divergences.
Throughout, we deÔ¨Åne
gM:= min
M2M:gM>0gM:
C.1.1 Bounding Regret of Exploit Phase
We refer to the exploit phase as the subset of rounds tin which Line 6 is reached, and refer to the explore
phaseas the subset of rounds in which Line 8 is reached.
LemmaC.1. The total expected regret incurred by the exploit phase of Algorithm 4 is bounded by 2log logT+3.
Proof of Lemma C.1. LetEtdenote the event that we exploit at round t, and that cMs6=?. Then,
since we can incur suboptimality of at most 1 at each round, the total expected regret incurred by the exploit
phase is bounded by
TX
t=1EM?[IfEtg]:
59Algorithm 4 Allocation Estimation via Adaptive Exploration ( AE2, general divergences)
1:input:Optimality tolerance , model classM, estimation oracle AlgD.
2:Initializes 1," 
4+2,nmax nmax(M;");q 4nmax+gM
4nmax+2gM.
3:Compute1 AlgD(f?g)andcM1 EM1[M].
4:fort= 1;2;3;:::do
5:if9cMs2cMss.t.8M2Malt(cMs),Ps 1
i=1EcMih
logPcM;i(ri;oi)
PM;i(ri;oi)i
log(tlogt)then // Exploit
6: PlaycMs.
7:else // Explore
8: Setps qs+ (1 q)!sfor
s;!s arg min
;!24sup
M2MnMgl
"(;nmax)1
EcMs
E!
D cM()kM(): (49)
9: Drawsps, observers;os.
10: Compute estimate s+1 AlgD(f(i;ri;oi)gs
i=1)and letcMs=EMs[M].
11:s s+ 1.
LeteEtdenote the event
eEt:=(
8s1 :sX
i=1EcMi"
logPcM;i(ri;oi)
PM?;i(ri;oi)#
<log(tlogt))
:
By Lemma C.2, we have PM?[IfeEc
tg]1
tlogt, and we can bound
EM?[IfEtg]EM?[IfEt\eEtg] +EM?[IfeEc
tg]EM?[IfEt\eEtg] +1
tlogt:
Letstdenotetheexplorationroundatround t. Ifweexploitatround t, thisimpliesthatforall M2Malt(cMs),
we have
st 1X
i=1EcMi"
logPcM;i(ri;oi)
PM;i(ri;oi)#
log(tlogt): (50)
IfcMs6=?, thenM?2Malt(cMs), so Eq. (50) must hold for M M?. This contradicts eEt, however, so
EM?[IfEt\eEtg] = 0. Thus, EM?[IfEtg]1
tlogt, so
TX
t=1EM?[IfEtg]3 +TX
t=31
tlogt3 + 2ZT
e1
tlogtdt= 3 + 2 log log T:
Lemma C.2. Forf(ri;oi;i)gs
i=1generated as in Algorithm 4, we have that
PM?"
9s1 :sX
i=1EcMi"
logPcM;i(ri;oi)
PM?;i(ri;oi)#
"#
e ":
Proof of Lemma C.2. Denote
Xs:= exp sX
i=1EcMi"
logPcM;i(ri;oi)
PM?;i(ri;oi)#!
:
60We Ô¨Årst show that Xsis a supermartingale. Letting Fs 1denote the Ô¨Åltration up to s 1, we have
EM?[XsjFs 1] = exp s 1X
i=1EcMi"
logPcM;i(ri;oi)
PM?;i(ri;oi)#!
EM?
exp
EcMs
logPcM;s(rs;os)
PM?;s(rs;os)
jFs 1
=Xs 1EM?
exp
EcMs
logPcM;s(rs;os)
PM?;s(rs;os)
jFs 1
(a)
Xs 1EM?
EcMs
exp
logPcM;s(rs;os)
PM?;s(rs;os)
jFs 1
=Xs 1EM?"
EcMs[PcM;s(rs;os)]
PM?;s(rs;os)jFs 1#
=Xs 1
where (a)holds by Jensen‚Äôs inequality, and the Ô¨Ånal equality holds since EcMs[PcM;(;)]is a valid distribution
overRO. Thus,Xsis a supermartingale. Ville‚Äôs Maximal Inequality then immediately gives that
PM?[9s1 :Xse"]EM?[X1]
e":
To complete the proof, using the same calculation as above, we bound
EM?[X1] =EM?"
exp 
EcM1"
logPcM;1(r1;o1)
PM?;1(r1;o1)#!#
EM?"
EcM1"
exp 
logPcM;1(r1;o1)
PM?;1(r1;o1)!##
1:
C.1.2 Bounding Regret of Explore Phase
Lemma C.3 (Main Explore Phase Regret Bound) .LetsTdenote the total number of exploration rounds
(which is a random variable), and assume that 2[0;1=2). Then running Algorithm 4, if g?>0, we can
bound
E[sT]24n2
max+ 8nmaxgM
(gM)2aecD
"=2(M)E[dEst D(sT)] +12nmax
minE[Est KL(sT)]
+6nmax
EsTX
s=1inf
M2Malt(M?)Eps[DKL(M?()kM())]If?2cMsg
and the regret during exploration rounds is bounded as
EsTX
s=1?(s)
8nmax+ 2gM
gMaecD
"=2(M)E[dEst D(sT)] +2(1 +)g?
minE[Est KL(sT)]
+ (1 +)g?EsTX
s=1inf
M2Malt(M?)Eps[DKL(M?()kM())]If?2cMsg
:
Proof of Lemma C.3. The expected regret during exploration can be written as E[PsT
s=1?(s)] =
E[PsT
s=1Eps[?()]]. By deÔ¨Ånition, for exploration rounds, we have ps qs+ (1 q)!s. For eachssT,
we consider three cases to bound the instantaneous expected regret, Eps[?()] = ?(ps).
Case 1:M?2MnMgl
"(s;nmax).Denote such rounds as S1
exp. Write
?(ps) =h
?(ps) sEcMsh
Eps[D cM()kM?()
]ii
+sEcMsh
Eps[D cM()kM?()
]i
61for
s:=1 +
1 q1
EcMs[E!s[D cM()kM?()
]]: (51)
In this case we have that
sEcMs[Eps[D cM()kM?()
]] =1 +
1 q1
E!s[EcMs[D cM()kM?()
]]EcMs[Eps[D cM()kM?()
]]
1 +
EcMs[Eps[D cM()kM?()
]]EcMs[Eps[D cM()kM?()
]]
= 1 +:
Thus, since the suboptimality gap is always bounded by 1, we can bound
?(ps) sEcMsh
Eps[D cM()kM?()
]i
1 sEcMsh
Eps[D cM()kM?()
]i
 :
So,
?(ps) +sEcMsh
Eps[D cM()kM?()
]i
:
Case 2:M?2Mgl
"(s;nmax);?2cMs.Denote such rounds as S2
exp, and write
?(ps) = [?(ps) (1 +)g?Eps[DKL(M?()kM())]] + (1 +)g?Eps[DKL(M?()kM())]
for anyM2M?
alt. In this case, since M?2Mgl
"(s;nmax), we have that s2(M?;"). This then implies
that
?(s)(1 +")g?=n?and inf
M2M?
altEs[DKL(M?()kM())](1 ")=n?
for some n?nmax. SinceM2M?
alt, it follows that Es[DKL(M?()kM())](1 ")=n?. Thus,
?(ps) (1 +)g?Eps[DKL(M?()kM())]q[?(s) (1 +)g?Es[DKL(M?()kM())]] + 1 q
q[(1 +")g?=n? (1 +)(1 ")g?=n?] + 1 q
=q[2" (1 ")]g?
n?+ 1 q
(a)= q
2g?
n?+ 1 q
 q
2g?
nmax+ 1 q
(b)
 
4g?
nmax;
where (a)follows from our choice of "==2
2+and setting of n?, and (b)follows from our setting of
q=4nmax+gM
4nmax+2gM, since g?>0, and some algebra. Thus,
?(ps)(1 +)g?Eps[DKL(M?()kM())] 
4g?
nmax:
As this holds for every M2M?
alt, we therefore have
?(ps)inf
M2M?
alt(1 +)g?Eps[DKL(M?()kM())] 
4g?
nmax:
62Case 3:M?2Mgl
"(s;nmax);?62cMs.Denote such rounds as S3
exp, and write
?(ps) =
?(ps) 2(1 +)g?
minEcMs[Eps[DKL 
M?()kcM()
]]
+2(1 +)g?
minEcMs[Eps[DKL 
M?()kcM()
]]:
SinceM?2Mgl
"(s;nmax), we have that s2(M?;"). This then implies that for any M2M?
alt:
?(s)(1 +")g?=n?and inf
M2M?
altEs[DKL(M?()kM())](1 ")=n?
for some n?nmax. By Lemma C.9, since ?62cMs, we can lower bound EcMs[IfcM2M?
altg]1
2min.
Thus, we have
2(1 +)g?
minEcMs[Es[DKL 
M?()kcM()
]]2(1 +)g?
minEcMs[Es[DKL 
M?()kcM()
IfcM2M?
altg]]
(1 +)(1 ")g?
n?:
This implies that
?(ps) 2(1 +)g?
minEcMs[Eps[DKL 
M?()kcM()
]]q[(1 +")g?=n? (1 +)(1 ")g?=n?] + 1 q
 
4g?
nmax;
where the Ô¨Ånal inequality follows by the same argument as in Case 2. Thus,
?(ps)2(1 +)g?
minEcMs[Eps[DKL 
M?()kcM()
]] 
4g?
nmax:
Completing the Proof. In total we have
EsTX
s=1?(ps)
E
 jS1
expj g?
4nmaxjS2
exp[S3
expj+X
s2S1expsEcMs[Eps[D cM()kM?()
]]
+ (1 +)g?X
s2S2expinf
M2M?
altEps[DKL(M?()kM())]
+2(1 +)g?
minX
s2S3expEcMs[Eps[DKL 
M?()kcM()
]]
E
 g?
4nmaxsT+8nmax+ 2gM
gMaecD
"=2(M)dEst D(sT) +2(1 +)g?
minEst KL(sT)
+ (1 +)g?sTX
s=1inf
M2M?
altEps[DKL(M?()kM())]If?2cMsg
where the last inequality follows from Lemma C.10, which bounds
s8nmax+ 2gM
gMaecD
"=2(M);
and also using that for any s2S2
expwe have?2cMs. Upper bounding  g?
4nmaxsT0proves the second
claim in the lemma statement.
63For the Ô¨Årst claim, as regret is always nonnegative, it follows that
0E
 g?
4nmaxsT+8nmax+ 2gM
gMaecD
"=2(M)dEst D(sT) +2(1 +)g?
minEst KL(sT)
+ (1 +)g?sTX
s=1inf
M2M?
altEps[DKL(M?()kM())]If?2cMsg
which implies
E[sT]4nmax
g?E8nmax+ 2gM
gMaecD
"=2(M)dEst D(sT) +2(1 +)g?
minEst KL(sT)
+ (1 +)g?sTX
s=1inf
M2M?
altEps[DKL(M?()kM())]If?2cMsg
:
Lemma C.4. When running both Algorithm 4 and Algorithm 5, for all 2[0;1), we have
EsTX
s=1inf
M2Malt(M?)sEps[DKL(M?()kM())]If?2cMsg
E[s
T] logT+E
VMs1=2+
Tp
1344dcovlog(128CcovsT)
+ (VM+LKL)
4s1=2+=2
T
1 +sTX
s=1s1=2+=2EcMs[Eps[D cM()kM?()
]]
+E[s
T] log logT+ 7VM:
Proof of Lemma C.4. LetesTdenote the Ô¨Ånal exploration round for which ?2cMs. Then, upper
bounding the KL divergence by 2VMvia Lemma C.13, we have
sTX
s=1inf
M2M?
altsEps[DKL(M?()kM())]If?2cMsges
TesT 1X
s=1inf
M2M?
altEps[DKL(M?()kM())] + 2VMs
T:
Under Assumption A.1 and via Jensen‚Äôs inequality and AM-GM, we have, for any s>0andM2M,
Eps[DKL(M?()kM())]EcMs[Eps[DKL cM()kM()
]] +LKLq
EcMs[Eps[D cM()kM?()
]]
EcMs[Eps[DKL cM()kM()
]] +LKL1
s+sEcMs[Eps[D cM()kM?()
]]
;
We now wish to bound
E
es
TesT 1X
s=1inf
M2M?
altEcMs[Eps[DKL cM()kM()
]]
:
LetMj
covdenote an (j;j)-cover ofM?
altandEs
jthe corresponding event at step s2j, for somejand
jto be chosen. Let Ej:=\s2jEs
j. By deÔ¨Ånition PM?[Es
j]1 j, soPM?[Ej]1 2jj. DeÔ¨Åne an event
Aj:=(
8s2j;M2Mj
cov: (s+ 1)sX
i=1EcMi[Epi[DKL cM()kM()
]]
(s+ 1)sX
i=1EcMi"
logPcM;i(ri;oi)
PM;i(ri;oi)#
+VM(s+ 1)1
2+s
56 log2jNcov(M?
alt;j;j)
j
+VM 
4(s+ 1)1+
2
1 +sX
i=1i1+
2EcMi[Epi[D cM()kM?()
]!)
64sfor somejto be chosen. By invoking Lemma C.12 with i=i1=2+=2=(s+ 1)and a union bound, we
haveP[Aj]1 j(while Lemma C.12 does not contain the (s+ 1)term, the bound in the expression for
Ajsimply gives the bound from Lemma C.12 multiplied through with (s+ 1), which is non-random, so this
is admissible). We can decompose
E
es
TesT 1X
s=1inf
M2M?
altEcMs[Eps[DKL cM()kM()
]]
dlogTeX
j=1E
es
Tinf
M2M?
altesT 1X
s=1EcMs[Eps[DKL cM()kM()
]]IfesT2[2j 1;2j);Aj\Ejg
+dlogTeX
j=1E
es
Tinf
M2M?
altesT 1X
s=1EcMs[Eps[DKL cM()kM()
]]IfesT2[2j 1;2j);Ac
j[Ec
jg
:
We bound these terms separately. We can bound the second term as
dlogTeX
j=1E
es
Tinf
M2M?
altesT 1X
s=1EcMs[Eps[DKL cM()kM()
]]IfesT2[2j 1;2j);Ac
j[Ec
jg
dlogTeX
j=12VM22j(P[Ac
j] +P[Ec
j])
dlogTeX
j=12VM22j(j+ 2jj)
where the Ô¨Årst inequality follows by bounding Lemma C.13, and esT2j, and the second follows by our
bound on the probability of Aj. Lettingj=1
23j;j=1
24j, this term can be bounded by 4VM. We turn now
to the Ô¨Årst term. Fix j2[dlogTe]. By deÔ¨Ånition of the event Aj, and sinceesT2j, plugging in our choice
ofjwe can bound, for any M2Mj
cov,
E
es
Tinf
M2M?
altesT 1X
s=1EcMs[Eps[DKL cM()kM()
]]IfesT2[2j 1;2j);Aj\Ejg
E" 
es
Tinf
M2M?
altesT 1X
s=1EcMs
logPcM;s(rs;os)
PM;s(rs;os)
+VMes1=2+
Tq
56 log(23jNcov(M?
alt;j;j))
+VM 
4es1=2+=2
T
1 +esTX
s=1s1=2+=2EcMs[Eps[D cM()kM?()
]]!!
IfesT2[2j 1;2j);Ejg#
E" 
es
Tinf
M2M?
altesT 1X
s=1EcMs
logPcM;s(rs;os)
PM;s(rs;os)
+VMes1=2+
Tq
168 log(8esTNcov(M?
alt;j;es 4
T
16))
+VM 
4es1=2+=2
T
1 +esTX
s=1s1=2+=2EcMs[Eps[D cM()kM?()
]]!!
IfesT2[2j 1;2j);Ejg#
(52)
where the second inequality follows since, if esT2[2j 1;2j), then we can bound 2j2esT.
SinceesTis an exploration round by deÔ¨Ånition, we know that for all cMesT2cMesT, there exists some
M02Malt(cMesT)such that
esT 1X
s=1EcMs
logPcM;s(rs;os)
PM0;s(rs;os)
log(TlogT):
65By assumption we have ?2cMesT, so it follows that there exists some (random) M02M?
altsuch that the
above inequality holds. Let M002Mj
covdenote the model in Mj
covsuch that
logPM0;(r;o)
PM00;(r;o)=logPM0;(r;o) logPM00;(r;o)j;
for all (r;o; )for which supfM2MPfM;(r;ojEj)>0, which is guaranteed to exist by DeÔ¨Ånition 2.1. Note
that by deÔ¨Ånition of Mj
cov, we haveM002M?
alt. We then have
es
Tinf
M2M?
altesT 1X
s=1EcMs
logPcM;s(rs;os)
PM;s(rs;os)
IfesT2[2j 1;2j);Ejg
es
TesT 1X
s=1EcMs
logPcM;s(rs;os)
PM00;s(rs;os)
IfesT2[2j 1;2j);Ejg
=es
TesT 1X
s=1
EcMs
logPcM;s(rs;os)
PM0;s(rs;os)
+ logPM0;s(rs;os)
PM00;s(rs;os)
IfesT2[2j 1;2j);Ejg
es
Tlog(TlogT) +jes1+
T;
where the inequality holds since on Ej, we can ensure that logPM0;s(rs;os)
PM00;s(rs;os)j. Therefore, choosing
j= 2 3j, we have
Eq. (52)E" 
es
Tlog(TlogT) +jes1+
T+VMes1=2+
Tq
168esTlog(8 Ncov(M?
alt;j;es 4
T
16))
+VM 
4es1=2+=2
T
1 +esTX
s=1s1=2+=2EcMs[Eps[D cM()kM?()
]]!!
IfesT2[2j 1;2j)g#
E" 
es
Tlog(TlogT) + 2 j+VMes+1=2
Tq
168 log(8esTNcov(M?
alt;es 3
T
8;es 4
T
16))
+VM 
4es1=2+=2
T
1 +esTX
s=1s1=2+=2EcMs[Eps[D cM()kM?()
]]!!
IfesT2[2j 1;2j)g#
:
As this holds for each j, and the eventsfesT2[2j 1;2j)gare disjoint, we can sum over jto bound
dlogTeX
j=1E
es
Tinf
M2M?
altesT 1X
s=1EcMs[Eps[DKL cM()kM()
]]IfesT2[2j 1;2j);Aj\Ejg
E"
es
Tlog(TlogT) + 1 +VMes+1=2
Tq
168 log(8esTNcov(M?
alt;es 3
T
8;es 4
T
16))
+VM 
4es1=2+=2
T
1 +esTX
s=1s1=2+=2EcMs[Eps[D cM()kM?()
]]!#
:
Finally, under Assumption 2.3 and Lemma C.15 we can bound
log(8esTNcov(M?
alt;es 3
T
8;es 4
T
16))log(8esT) + log Ncov(M;es 3
T
8;es 4
T
16)
log(8esT) +dcovlog 
128Ccoves7
T
8dcovlog(128CcovesT):
The result follows.
66C.1.3 Completing the Proof
Theorem C.1 (Full version of Theorem 2.1) .For any1=2, if we setD 
k
=DKL(k)and instantiate
AlgDwith Algorithm 6, under Assumptions 1.1 to 1.3 and 2.1 to 2.4 and if g?>0, the expected regret of
Algorithm 4 is bounded by
EM?[Reg(T)](1 +)g?logT+Caecaec"=2(M)log3=2(logT) + lin
Clow;p
logT;log3=2(logT)
;
for
Clow:= lin
g?;max
M2MgM;V13=2
M;L2
KL;dcov;logCcov;1
2;1
3
min;nM
=6;q
aec"=2(M)
Caec:=cV2
Mdcovlog(Ccov)maxM2MgM( 1+VMnM
=6)
3
minlog(Clow)
and where lin()denotes a function linear and poly-logarithmic in its arguments and c >0is a universal
constant.
Proof of Theorem C.1. LettingTexploitdenote the exploitation rounds, we can bound the total expected
regret as
TX
t=1E[?(t)] =E2
4X
t2Texploit?(t)3
5+E"sTX
s=1?(s)#
2 log logT+ 3 + E"sTX
s=1?(s)#
;
where the inequality follows from Lemma C.1. It remains to bound the regret in the exploration rounds. By
Lemma C.3, we can bound this as
EsTX
s=1?(s)
8nmax+ 2gM
gMaecD
"=2(M)E[dEst D(sT)] +2(1 +)g?
minE[Est KL(sT)]
+ (1 +)g?EsTX
s=1inf
M2M?
altEps[DKL(M?()kM())]If?2cMsg
:
Applying Lemma C.4 with = 0, we can bound
EsTX
s=1inf
M2M?
altEps[DKL(M?()kM())]If?2cMsg
logT+E
VMp
1344dcovsTlog(128CcovsT)
+ (VM+LKL)
4psT+sTX
s=1psEcMs[Eps[D cM()kM?()
]]
+ log logT+ 7VM
logT+VMp
1344dcovE[sT]log(128CcovE[sT])
+ (VM+LKL)
4p
E[sT] +E"sTX
s=1psEcMs[Eps[D cM()kM?()
]]#
+ log logT+ 7VM
(53)
where the last inequality follows from Lemma C.16‚Äîapplied with == 1=2,a= 128Ccov‚Äîand the
concavity ofpx. Note that the condition of Lemma C.16 is met here since we assume Ccov1. It follows
from Lemma C.7 that
E"sTX
s=1psEcMs[Eps[D cM()kM?()
]]#
Eh
(2 + 6VM)(20dcovlog(64CcovsT) + 1)5p
2sTlog(2sT)i
+E[32(1 +VM) log(sT)] + 8
(2 + 6VM)(20dcovlog(64CcovE[sT]) + 1)5p
2E[sT] log(2E[sT])
67+ 32(1 +VM) logE[sT] + 8
=O
VMdcovlog(CcovE[sT])p
E[sT] log(E[sT]) +VMp
E[sT]
;
(54)
where the last inequality follows from Lemma C.16. Similarly, by Lemma C.8, we can bound both E[dEst D(sT)]
andE[Est KL(sT)]as
E[dEst D(sT)];E[Est KL(sT)]Eh
(2 + 5VM)(20dcovlog(64CcovsT) + 1)p
log(2sT)i
+E[32(1 +VM) log(sT)] + 8
(2 + 5VM)(20dcovlog(64CcovE[sT]) + 1)p
log(2E[sT])
+ 32(1 +VM) log(E[sT]) + 8
=O
VMdcovlog(CcovE[sT])p
log(E[sT]) +VMlog(E[sT])
;(55)
where the second inequality follows from Lemma C.16 and Jensen‚Äôs inequality, which lets us pass the
expectation through. Together this gives a regret bound of
EsTX
s=1?(s)
(1 +)g?logT+ (1 +)g?VMp
1344dcovE[sT]log(128CcovE[sT]) + (1 +)g?(log logT+ 7VM)
+ (1 +)g?(VM+LKL)Op
E[sT] +VMdcovlog(CcovE[sT])p
E[sT] log(E[sT]) +VMlogE[sT]
+8nmax+ 2gM
gMaecD
"=2(M)O
VMdcovlog(CcovE[sT])p
log(E[sT]) +VMlog(E[sT])
+2(1 +)g?
minO
VMdcovlog(CcovE[sT])p
log(E[sT]) +VMlog(E[sT])
By Lemma C.3 we can bound the total number of exploration rounds by
E[sT]24n2
max+ 8nmaxgM
(gM)2aecD
"=2(M)E[dEst D(sT)] +12nmax
minE[Est KL(sT)]
+6nmax
EsTX
s=1inf
M2M?
altEps[DKL(M?()kM())]If?2cMsg
24n2
max+ 8nmaxgM
(gM)2aecD
"=2(M)O
VMdcovlog(CcovE[sT])p
log(E[sT]) +VMlog(E[sT])
+12nmax
minO
VMdcovlog(CcovE[sT])p
log(E[sT]) +VMlog(E[sT])
+6nmax
(logT+VMp
1344dcovE[sT]log(128CcovE[sT]) + log logT+ 7VM)
+6nmax
(VM+LKL)
4p
E[sT] +O
VMdcovlog(CcovE[sT])p
E[sT] log(E[sT]) +VMp
E[sT]
where the second inequality follows from Equations (53) to (55). Using Lemma C.17 and bounding gMnmax,
we can solve this for E[sT]to get
E[sT]eO 
nmax
logT+VMdcovlogCcovn2
max
(gM)2aecD
"=2(M) +nmaxVMdcovlogCcov
min
+n2
max(VM+LKL)2(V2
Md2
covlog2Ccov+V2
M)
2!
:
68Finally, by Lemma C.13 and Lemma B.3 we can lower bound gMmin=2VM. Plugging this into the above
expression and using that
nmax=nmax(M;=6) :=32
2
min6
+ 2VMnM
=6
max
M2MgM;
gives the result, after simplifying.
C.2 Regret Bound without Uniform Regularity (Theorem 2.2)
In this section, we prove Theorem 2.2 (as well as a more general result, Theorem C.2), which gives regret
bounds for a variant of AE2, Algorithm 5, AE2
?, which does not require uniform regularity, and adapts to the
gap?
min:= M?
minfor the underlying model M?. Algorithm 5 is a slightly more general version of Algorithm 2,
incorporating general divergences D.
Throughout this section, we let s?denote the Ô¨Årst exploration round ssuch thatM?2M`in Algorithm 2.
Note thats?is a deterministic quantity (though, the Ô¨Årst round tin whichs=s?is not deterministic).
We remark brieÔ¨Çy that, to bound Eq. (56) by a restriction of the AEC, it is critical that our estimator
produced at each exploration round s,s, is only supported on M`. To accomplish this, we explicitly generate
a cover ofM`,M`
cov, and run an estimation procedure on this cover. As Algorithm 6, the estimation
procedure used to prove Theorem 2.1, directly covers all of M, to prove Theorem 2.2 we do not appeal
directly to this algorithm, yet we note that the covering and estimation procedure employed by AE2
?are
essentially identical to that in Algorithm 6, modulo the choice of which set is being covered.
Algorithm 5 Adaptive Exploration for Allocation Estimation for classes without uniform regularity ( AE2
?)
1:input:Optimality tolerance , divergence D, estimation oracle AlgD, growth parameters q,n,M.
2:s 1,` 1," 
4+2.
3:qs 1 s q,ns sn.
4:Compute1 AlgD(f?g)andcM1 EM1[M].
5:fort= 1;2;3;:::do
6:ifs2`then // Form active set and cover
7:` `+ 1.
8: ` arg min0s.t. aecD;M
"=2(M;1
)sM.
9:M` M`;1
`\
M2M :nM
"+1
M
min+4gM
M
min+2nM
"
gM+4
M
min"p
ns	
.
10:M`
cov (`;`)-cover ofM`for` 2 `;` 2 5`,D` ?.
11:if9cMs2cMss.t.8M2Malt(cMs),Ps 1
i=1EcMih
logPcM;i(ri;oi)
PM;i(ri;oi)i
log(tlogt)then // Exploit
12: PlaycMs.
13:else // Explore
14: Setps qss+ (1 qs)!sfor
s;!s arg min
;!24sup
M2M`nMgl
"(;ns)1
EcMs[E![D cM()kM()
]]: (56)
15: Drawsps, observers;os, setD` D`[f(s;rs;os)g.
16: Compute estimate s+1 AlgD(D`;M`
cov)andcMs+1=EMs+1[M].
17:s s+ 1.
69C.2.1 Bounding Regret of Explore Phase
Lemma C.5. We have
s?
aecD;M
"=2(M?)1
M+
n?
"+1
?
min+4g?
?
min+2n?
"
g?+4
?
min"2
n
:
Proof of Lemma C.5. We will have M?2M`as soon asM?2M0andM?2M00for
M0 
M2M :nM
"+1
M
min+4gM
M
min+2nM
"
gM+4
M
min"p
n2` 1
;M00 M`;1
`
where
`= arg min
0s.t. aecD;M
"=2(M;1
)sM:
Note that if this occurs for some `, then the Ô¨Årst exploration round ssuch thatM?2M`iss= 2` 1. From
the deÔ¨Ånition n2` 1= 2n(` 1), we haveM?2M0as soon as
p
2n(` 1)n?
"+1
?
min+4g?
?
min+2n?
"
g?+4
?
min"
() 2` 1
n?
"+1
?
min+4g?
?
min+2n?
"
g?+4
?
min"2=n
:
To haveM?2M00, we need `?
minandn?
"1
`, which will be the case once
aecD;M
"=2(M?)2(` 1)M()
aecD;M
"=2(M?)1
M2` 1:
The result then follows by combining these bounds.
Lemma C.6. LetsTdenote the total number of exploration rounds. For 1=2, running Algorithm 5, we
can almost surely bound bound
sT4
g?sTX
s=s?2sq+MEcMs[Eps[D cM()kM?()
] +sTX
s=s?sn2g?inf
M2M?
altEps[DKL(M?()kM())]If?2cMsg
+sTX
s=s?s3n=24g?EcMs[Eps[DKL 
M?()kcM()
]] + 4 g?(8
g?)1+n
q n
+s?:
In addition, the regret during exploration rounds is bounded as
EsTX
s=s??(s)
EsTX
s=s?2sq+MEcMs[Eps[D cM()kM?()
]
+sTX
s=s?(1 +)g?inf
M2M?
altEps[DKL(M?()kM())]If?2cMsg
+sTX
s=s?sn=24g?EcMs[Eps[DKL 
M?()kcM()
]]
+ 4g?(8
g?)1
q n:
Proof of Lemma C.6. We Ô¨Årst prove the bound on the regret during the exploration rounds, then use
this result to prove the bound on sT.
Recall that by deÔ¨Ånition, for exploration rounds, we have ps qss+ (1 qs)!s. We consider three cases to
bound the instantaneous expected regret, ?(ps), for eachss?.
70Case 1:M?2M`nMgl
"(s;ns).Denote such rounds as S1
exp. This case follows identically to Case 1 in
Lemma C.3 with
s=1 +g?
1 +qs1
EcMs[E!s[D cM()kM?()
]: (57)
We therefore omit the proof and conclude that
?(ps) g?+sEcMs[Eps[D cM()kM?()
]
sEcMs[Eps[D cM()kM?()
]:
As regret is always lower-bounded by 0, we have ?(ps)0, so for rounds s2S1
exp, we can also write
g?sEcMs[Eps[D cM()kM?()
]: (58)
Case 2:M?2Mgl
"(s;ns),?2cMs.Denote such rounds as S2
exp, and write
?(ps) = [?(ps) (1 +)g?Eps[DKL(M?()kM())]] + (1 +)g?Eps[DKL(M?()kM())]
for an arbitrary model M2M?
alt. In this case, since M?2Mgl
"(s;ns), we have that s2(M?;"). This
implies that
?(s)(1 +")g?=n?
sand inf
M2M?
altEs[DKL(M?()kM())](1 ")=n?
s
for some n?
sns.
SinceM2M?
alt, it follows that Es[DKL(M?()kM())](1 "s)=n?
s. Thus,
?(ps) (1 +)g?Eps[DKL(M?()kM())]qs[?(s) (1 +)g?Es[DKL(M?()kM())]] + 1 qs
qs[(1 +")g?=n?
s (1 +)(1 ")g?=n?
s] + 1 qs
=qs[2" (1 ")]g?
n?s+ 1 qs
(a)= (1 s q)
2g?
n?s+s q
(b)
(
s qs<(8n?
s
g?)1=q
 g?
4n?ss(8n?
s
g?)1=q
(c)
(
s q s<(8
g?)1
q n
 1
4g?s ns(8
g?)1
q n
(d)
2g?Ifs<(8
g?)1
q ng 1
4g?s n
where (a)follows from our choice of "==2
2+andqs= 1 s q,(b)follows from some algebra, (c)uses that
n?
snsandns=sn, and (d)follows since we will always have s q2g? 1
4g?s n. Thus:
?(ps)(1 +)g?Eps[DKL(M?()kM())] + 2 g?Ifs<(8
g?)1
q ng 1
4g?s n
(1 +)g?Eps[DKL(M?()kM())] + 2 g?Ifs<(8
g?)1
q ng:
As this holds for all M2M?
alt, we have
?(ps)(1 +)g?inf
M2M?
altEps[DKL(M?()kM())] + 2 g?Ifs<(8
g?)1
q ng:
71Since ?(ps)0, this also implies that for s2S2
exp:
1
4g?sn2g?inf
M2M?
altEps[DKL(M?()kM())] + 2 g?snIfs<(8
g?)1
q ng
sn2g?inf
M2M?
altEps[DKL(M?()kM())] + 2 g?(8
g?)n
q nIfs<(8
g?)1
q ng:(59)
Case 3:M?2Mgl
"(s;ns),?62cMs.Denote such rounds as S3
exp, and write
?(ps) =h
?(ps) 2(1 +)g?p
nsEcMs[Eps[DKL 
M?()kcM()
]]i
+ 2(1 +)g?p
nsEcMs[Eps[DKL 
M?()kcM()
]]:
SinceM?2Mgl
"(s), we have that s2(M?;"). This implies that for any M2M?
alt:
?(s)(1 +")g?=n?
sand inf
M2M?
altEs[DKL(M?()kM())](1 ")=n?
s
for some n?
sns. Assume that we are at epoch `. By construction we have that, for M2M`,1
M
min
p
n2`() M
min1p
n2`. Since nsis increasing in s, this implies that M
min1p
ns. Assis only supported
onM`, since?62cMs, Lemma C.9 implies that EMs[IfM2M?
altg]1
2p
ns. Thus, we have
2(1 +)g?p
nsEcMs[Es[DKL 
M?()kcM()
]]2(1 +)g?p
nsEcMs[Es[DKL 
M?()kcM()
IfM2M?
altg]]
2(1 +)g?p
ns1 "
2p
nsn?s
(1 +)(1 ")g?
n?s:
This implies that
?(ps) 2(1 +)g?p
nsEcMs[Eps[DKL 
M?()kcM()
]]qs[(1 +")g?=n?
s (1 +)(1 ")g?=n?
s] + 1 qs
2g?Ifs<(8
g?)1
q ng 1
4g?s n;
where the Ô¨Ånal inequality follows by the same argument as in Case 2. Thus,
?(ps)2(1 +)g?p
nsEcMs[Eps[DKL 
M?()kcM()
]] + 2 g?Ifs<(8
g?)1
q ng 1
4g?s n
=sn=22(1 +)g?EcMs[Eps[DKL 
M?()kcM()
]] + 2 g?Ifs<(8
g?)1
q ng 1
4g?s n
sn=22(1 +)g?EcMs[Eps[DKL 
M?()kcM()
]] + 2 g?Ifs<(8
g?)1
q ng:
Just as in Case 2, using that ?(ps)0, this also implies that for s2S3
exp:
1
4g?s3n=22(1 +)g?EcMs[Eps[DKL 
M?()kcM()
]] + 2 g?(8
g?)n
q nIfs<(8
g?)1
q ng:(60)
Completing the Proof. In total we have
sTX
s=s??(ps)X
s2S1expsEcMs[Eps[D cM()kM?()
] +X
s2S2exp(1 +)g?inf
M2M?
altEps[DKL(M?()kM())]
+X
s2S3expsn=22(1 +)g?EcMs[Eps[DKL 
M?()kcM()
]] + 4 g?(8
g?)1
q n:
72By Lemma C.11, for s2S1
exp, we can bound s(1 + g?)sq+M. This gives an upper bound on the
above of
sTX
s=s?(1 + g?)sq+MEcMs[Eps[D cM()kM?()
] +sTX
s=s?(1 +)g?inf
M2M?
altEps[DKL(M?()kM())]If?2cMsg
+sTX
s=s?sn=24g?EcMs[Eps[DKL 
M?()kcM()
]] + 4 g?(8
g?)1
q n;
which proves the regret bound.
We now bound the number of exploration rounds. Since for every s2S1
expEq. (58) holds, for every s2S2
exp
Eq. (59) holds, and for every s2S3
expEq. (60) holds, combining these inequalities gives
1
4g?jS1
exp[S2
exp[S3
expj
X
s2S1expsEcMs[Eps[D cM()kM?()
] +X
s2S2expsn2g?inf
M2M?
altEps[DKL(M?()kM())]
+X
s2S3
exps3n=24g?EcMs[Eps[DKL 
M?()kcM()
]] + 4 g?(8
g?)1+n
q n
sTX
s=s?(1 + g?)sq+MEcMs[Eps[D cM()kM?()
]
+sTX
s=s?sn2g?inf
M2M?
altEps[DKL(M?()kM())]If?2cMsg
+sTX
s=s?s3n=24g?EcMs[Eps[DKL 
M?()kcM()
]] + 4 g?(8
g?)1+n
q n:
Using thatjS1
exp[S2
exp[S3
expj=sT s?and rearranging gives the bound on sT.
C.2.2 Completing the Proof
Theorem C.2 (Full version of Theorem 2.2) .Consider Algorithm 5, and suppose we set 1=2,D 

k
=DKL(k),M= 1=2,= 1=8, andq= 1=4, and instantiate AlgDwith Algorithm 3. Then if
Assumptions 1.1 to 1.3 and 2.1 to 2.3 hold and g?>0, the expected regret of is bounded by
EM?[Reg(T)](1 +)g?logT+Caec
aecD;M
"=2(M?)3
log3=2logT+Clowlog6=7T
for" 
4+2,
Caec:=eO(VM+LKL)V3
Mdcovlog(Ccov)
?
min
;
and
Clow:=eO
poly
VM;LKL;n?
";dcov;logCcov;g?;1
?
min;1
;log logT
:
Proof of Theorem C.2. The bound on the regret incurred in the exploit phase follows identically to
Theorem C.1, since the exploit test is the same. We turn to bounding the regret in the explore phase. First,
since we can incur regret of at most 1 at every round, we bound
E"sTX
s=1M?(ps)#
E"sTX
s=s?M?(ps)#
+s?
73By Lemma C.6, we can bound
EsTX
s=s??(s)
EsTX
s=s?(1 + g?)sq+MEcMs[Eps[D cM()kM?()
]
+sTX
s=s?(1 +)g?inf
M2M?
altEps[DKL(M?()kM())]If?2cMsg
+sTX
s=s?sn=24g?EcMs[Eps[DKL 
M?()kcM()
]]
+ 4g?(8
g?)1
q n
EsTX
s=s?(1 + g?)sq+MEcMs[Eps[D cM()kM?()
]
+sTX
s=1(1 +)g?inf
M2M?
altEps[DKL(M?()kM())]If?2cMsg
+sTX
s=s?sn=24g?EcMs[Eps[DKL 
M?()kcM()
]]
+ 4g?(8
g?)1
q n:
Applying Lemma C.4 with = 0, we have
EsTX
s=1inf
M2M?
altEps[DKL(M?()kM())]If?2cMsg
logT+E
VMs1=2
Tp
1344dcovlog(128CcovsT)
+ (VM+LKL)
4s1=2
T+sTX
s=1s1=2EcMs[Eps[D cM()kM?()
]]
+ log logT+ 7VM:
Note that for ss?, our estimator is applied to a cover of a set containing M?. Furthermore, note that the
estimation procedure used in Algorithm 5 is, other than the diÔ¨Äerent choice of set to cover, identical to that
used in Algorithm 6. It follows that the analysis of Algorithm 6 can be applied to the estimation procedure
of Algorithm 5, with only the mild modiÔ¨Åcation accounting for the diÔ¨Äerence in the size of the cover (as we
are coveringM`instead ofM). However, as we can upper bound the size of the cover of M`by the size of
the cover ofMvia Lemma C.15, this change is inconsequential. Thus, by Lemma C.7, we can bound
E"sTX
s=s?2sq+MEcMs[Eps[D cM()kM?()
]#
O
Eh
VMdcovlog(CcovsT)sq+M
Tp
log(sT)i
O
VMdcovlog(CcovE[sT])E[sT]q+Mp
log(E[sT])
;
(61)
where the second inequality uses Jensen‚Äôs inequality and Lemma C.16 to pass the expectation through, which
holds as long as 1=100q+M3=4. Similarly,
E"sTX
s=s?sn=24g?EcMs[Eps[DKL 
M?()kcM()
]]#
O
g?VMdcovlog(CcovE[sT])E[sT]n=2p
log(E[sT])
;
(62)
where we have again used Jensen‚Äôs inequality and Lemma C.16 to pass the expectation through, which holds
as long 1=100n=23=4. Forss?, we do not have M?2M`, and therefore the estimation guarantees
are vacuous. In this regime, using that the KL divergence is always bounded by 2VM(Lemma C.13), we can
simply upper bound the estimation error by 2VM. Thus,
E"sTX
s=1s1=2EcMs[Eps[D cM()kM?()
]]#
E"sTX
s=s?s1=2EcMs[Eps[D cM()kM?()
]]#
+ 2VM(s?)3=2
74O
VMdcovlog(CcovE[sT])E[sT]1=2p
log(E[sT])
+ 2VM(s?)3=2;
which gives, applying Lemma C.16 again:
EsTX
s=1inf
M2M?
altEps[DKL(M?()kM())]If?2cMsg
logT+O
VME[sT]1=2p
dcovlog(CcovE[sT])
+ (VM+LKL)
E[sT]1=2+VMdcovlog(CcovE[sT])E[sT]1=2p
log(E[sT])
+ 2VM(s?)3=2+ log logT+ 7VM: (63)
Combining Eqs. (61) to (63), we have
EsTX
s=s??(s)
(1 +)g?logT+O
VMp
dcovE[sT] log(CcovE[sT])
+ (VM+LKL)VMdcovlog(CcovE[sT])E[sT]1=2p
log(E[sT])
+VMdcovlog(CcovE[sT])p
logE[sT]((1 + g?)E[sT]q+M+"g?E[sT]n=2)
+g?(1
g?)1
q n+ log logT+VM(s?)3=2
:
To control this, it remains to bound sT. By Lemma C.6 we have the following almost sure bound:
sT4
g?sTX
s=s?(1 + g?)sq+MEcMs[Eps[D cM()kM?()
]
| {z }
(a)
+sTX
s=s?sn2g?inf
M2M?
altEps[DKL(M?()kM())]If?2cMsg
| {z }
(b)
+sTX
s=1s3n=24g?EcMs[Eps[DKL 
M?()kcM()
]]
| {z }
(c)+4g?(4
g?)1+n
q n
+s?:
We bound the expectation of term (a)as in Eq. (61). To bound term (b), we apply Lemma C.4 with =n
to get
E[(b)]E[sn
T] logT+E
VMs1=2+n
Tp
1344dcovlog(128CcovsT)
+ (VM+LKL)
4s1=2+n=2
T
1 n+sTX
s=1s1=2+n=2EcMs[Eps[D cM()kM?()
]]
+E[sn
T] log logT+ 7VM:
Again applying Lemma C.7 and Lemma C.16, we have
E[(c)]O
g?VMdcovlog(CcovE[sT])E[sT]3n=2p
log(E[sT])
and
E"sTX
s=1s1=2+n=2EcMs[Eps[D cM()kM?()
]]#
O
VMdcovlog(CcovE[sT])E[sT]1=2+n=2p
log(E[sT])
+ 2VM(s?)3=2+n=2;
75as long as 1=100n1=4. We therefore have (using Lemma C.16 to pass the expectation through):
E[sT]1
g?O
E[sT]nlogT+VME[sT]1=2+np
dcovlog(CcovE[sT]) + (VM+LKL)E[sT]1=2+n=2
+ (VM+LKL)VMdcovlog(CcovE[sT])E[sT]1=2+n=2p
log(E[sT])
+VMdcovlog(CcovE[sT])(1 + g?)E[sT]q+Mp
log(E[sT])
+g?VMdcovlog(CcovE[sT])E[sT]3n=2p
log(E[sT]) +g?(1
g?)1+n
q n+VM(s?)3=2+n=2
:
We now set M= 1=2,n= 1=8, andq= 1=4, and note that all of the preceding parameter restrictions are
satisÔ¨Åed for these choices. Furthermore, this parameter choice implies that‚Äîusing Lemma C.17 to handle log
factors‚Äîwe have
E[sT]eO1
(g?)8=7log8=7T+ poly
VM;dcov;logCcov;LKL;g?;1
;1
g?
+VM(s?)3=2+n=2
g?
:
Plugging this into the regret bound given above, we have
EsTX
s=s??(s)
(1 +)g?logT+eO
poly
VM;dcov;logCcov;LKL;g?;1
;1
g?;log logT
log6=7T
+(1 + 1=g?)(VM+LKL)V2
Mdcovlog(Ccov) +VM
(s?)3=2log3=2logT
:
Finally, by Lemma C.5, we can bound s?as
s?
aecD;M
"=2(M?)1
M+
n?
"+1
?
min+4g?
?
min+2n?
"
g?+4
?
min"2
n
;
and, by Lemma B.3 and Lemma C.13, we can lower bound g??
min=2VM. Plugging this in gives the Ô¨Ånal
bound.
C.3 Estimation Guarantees
Algorithm 6 Estimation with Adaptive Covering
1:input:ClassM.
2:` 1,D` ?.
3:M`
cov (`;`)-cover ofMfor` 2 `;` 2 5`.
4:Initialize TemperedAggregation`as an instance of Algorithm 3 with M`
cov.
5:fors= 1;2;3;:::do
6:Receive (s;rs;os),D` D`[f(s;rs;os)g.
7:s TemperedAggregation`(D`),cMs EMs[M]
8:ifs2`then.
9:` `+ 1,D` ?.
10:M`
cov (`;`)-cover ofMfor` 2 `;` 2 5`.
11: Initialize TemperedAggregation`as an instance of Algorithm 3 with M`
cov.
Inthissection, weanalyzeAlgorithm6, whichisavariantoftheTemperedAggregationalgorithm(Algorithm3)
designed for inÔ¨Ånite classes. This algorithm is used within Algorithm 4 in order to prove Theorem C.1.
Algorithm 6 simply applies Algorithm 3 to a sequence of covers for the class Mon a doubling epoch schedule.
In particular, at every epoch `, Algorithm 6 restarts the Tempered Aggregation algorithm (Algorithm 3),
clearing the Tempered Aggregation instance from the previous epoch from memory. We denote the `th
instantiation of Tempered Aggregation as TemperedAggregation`.
76Lemma C.7. Letdenote some stopping time with respect to the Ô¨Åltration (Ft)T
t=1such thatTalmost
surely, and let 2(0;1). When running Algorithm 6 under Assumption 2.3, we have
E"X
s=1sEMs[Eps[D2
H(M?();M())]]#
E
(20dcovlog(64Ccov) + 1)22
2 1
+ 4:
In addition, if Assumption 2.2 also holds, then
E"X
s=1sEMs[Eps[DKL(M?()kM())]]#
E
(2 + 6VM)(20dcovlog(64Ccov) + 1)22
2 1p
log(2)
+E[32(1 +VM) log()] + 8 (64)
and
E"X
s=1sEMs[Eps[DKL(M()kM?())]]#
E
(2 + 6VM)(20dcovlog(64Ccov) + 1)22
2 1p
log(2)
+E[32(1 +VM) log()] + 8: (65)
Proof of Lemma C.7. LetSkdenote the set of svalues for which `=kand note thatSk=f2k 1+
1;2k 1+ 2;:::; 2kg. We can bound
E"X
s=1sEMs[Eps[D2
H(M?();M())]]#
dlog2TeX
k=1E2
4X
s2SksEMs[Eps[D2
H(M?();M())]]Ifsg3
5
dlog2TeX
k=12kE2
4X
s2SkEMs[Eps[D2
H(M?();M())]]Ifsg3
5 (66)
since we have that `dlog2Teby construction, and since s2kfors2Sk. LetAkdenote the event
Ak:=(
8s2Sk:sX
i=2k 1EMi[Epi[D2
H(M?();M())]]2 log2kNcov(M;k;k)
k+ 2kk)
:
By Proposition B.2 and a union bound, P[Ak]1 k 22kk. Since the Hellinger distance is always
bounded by 2 and jSkj2k, we can upper bound
Eq. (66)dlog2TeX
k=12k0
@X
s2SkE
EMs[Eps[D2
H(M?();M())]]Ifs;Akg
+ 22kE[IfAc
kg]1
A:
Choosingk= 2 3kand sincek= 2 5k, we have
dlog2TeX
k=12k2k+1E[IfAc
kg]dlog2TeX
k=122k+1(k+ 22kk) =dlog2TeX
k=122k+122 3k4:
Note that for s2Sk, ifs, this implies that 2k 1. On the event Ak, fork= 2 k, when>0we
can bound
dlog2TeX
k=12kX
s2SkE
EMs[Eps[D2
H(M?();M())]]Ifs;Akg
E2
4dlog2TeX
k=12k
2 log2kNcov(M;2 k;2 5k)
2 3k+ 1
If2k 1g3
5
E
2 logNcov(M; 1=2; 5=32)
 4=16+ 1
max
k(2
2 12kIf2k 1g)
E
2 logNcov(M; 1=2; 5=32)
 4=16+ 1
22
2 1(67)
77where the Ô¨Ånal two inequalities follow since 2k2. Under Assumption 2.3 we have
logNcov(M; 1=2; 5=32)
 4=1610dcovlog(64Ccov);
which gives the Ô¨Årst result.
Bound on KL Estimation Error. By Lemma C.14, for any x>0we have
DKL 
M()kfM()
(2 + 2VM+x)D2
H 
M();fM()
+ 32(1 +V2
M=x+V3
M=x2)exp( x2=8V2
M):
In particular choosing x=VMp
8 logs2, we have
DKL 
M()kfM()
(2 + 2VM+VMp
8 logs)D2
H 
M();fM()
+ 32(1 +VM)=s:
Repeating this for each step s, we can therefore bound
E"X
s=1sEMs[Eps[DKL(M?()kM())]]#
(2 + 6VM)E"X
s=1sp
logsEMs[Eps[D2
H(M?();M())]]#
+ 32(1 +VM)E[log]:
The result in (64) then follows from a calculation nearly identical to our above bound on Hellinger estimation
error. Applying Lemma C.14 in a similar fashion with the arguments Ô¨Çipped gives (65).
In the following, we extend Lemma C.7 to the case when = 0.
Lemma C.8. Letdenote some stopping time with respect to the Ô¨Åltration (Ft)T
t=1such thatTalmost
surely. When running Algorithm 6, under Assumption 2.3, we have
E"X
s=1EMs[Eps[D2
H(M?();M())]]#
E[(20dcovlog(64Ccov) + 1)(2 log+ 1)] + 4:
In addition, if Assumption 2.2 also holds,
E"X
s=1EMs[Eps[DKL(M?()kM())]]#
Eh
(2 + 5VM)(20dcovlog(64Ccov) + 1)(2 log + 1)p
log(2)i
+E[32(1 +VM) log()] + 8
and
E"X
s=1EMs[Eps[DKL(M()kM?())]]#
Eh
(2 + 5VM)(20dcovlog(64Ccov) + 1)(2 log + 1)p
log(2)i
+E[32(1 +VM) log()] + 8:
Proof of Lemma C.8. This follows identically to Lemma C.7 but replacing Eq. (67) with
dlog2TeX
k=12kX
s2SkE
EMs[Eps[D2
H(M?();M())]]Ifs;Akg
E2
4dlog2TeX
k=1
2 log2kNcov(M;2 k;2 5k)
2 3k+ 1
If2k 1g3
5
E
2 logNcov(M; 1=2; 5=32)
 4=16+ 1
(2 log+ 1)
:
The bound on the KL estimation error also follows from the same reasoning as in Lemma C.7.
78C.4 Supporting Lemmas
Lemma C.9. Consider running either Algorithm 4 or Algorithm 5. Assume that ?62cMsand that
minM2M :s(M)>0M
min. Then EMs[IfM2M?
altg]1
2.
ProofofLemmaC.9. RecallthatcMs=EMs[M],so2cMsimpliesthat 2arg max02EMs[fM(0)].
If?62cMs, then there exists some esuch that EMs[fM(e)]>EMs[fM(?)]. SincefM()2[0;1], this
implies that
0<EMs[fM(e) fM(?)]EMs[IfM2M?
altg] EMs[(fM(?) fM(e))IfM62M?
altg]:
ForM62M?
alt, we havefM(?) fM(e)M
min. Thus, the above implies
0<EMs[IfM2M?
altg] EMs[IfM62M?
altg]
() (1 EMs[IfM2M?
altg])<EMs[IfM2M?
altg]:
Rearranging gives EMs[IfM2M?
altg]
1+1
2.
Lemma C.10. When running Algorithm 4, on rounds sfor whichM?2MnMgl
"(s;nmax), we have
s(1 +)(4nmax+ 2gM)
gMaecD
"=2(M);
forsas deÔ¨Åned in Eq. (51).
Proof of Lemma C.10. Recall that !sandsare chosen to minimize
sup
M2MnMgl
"(s;nmax)1
EcMs[E![D cM()kM()
]:
SinceM?2MnMgl
"(s;nmax), we can therefore bound
1
EcMs[E!s[D cM()kM?()
]]inf
!24sup
M2MnMgl
"(s;nmax)1
EcMs[E![D cM()kM()
]]
= inf
;!24sup
M2MnMgl
"(;nmax)1
EcMs[E![D cM()kM()
]:
Recall that we set
nmax=1
min"+2VMnM
"
min
max
M2M32gM
min:
By Lemma C.13, under Assumption 2.2, we can bound DKL(M()kM0())2VMfor allM;M02Mand
2. It follows from Lemma B.3 that
2VM
min1
minM2M:gM>0gM;
so
nmax1
min"+nM
"
minM2M:gM>0gM
max
M2M32gM
min:
Given this, straightforward calculation shows that nmaxmeets the condition required by Lemma B.4, so
Lemma B.4 implies
inf
;!24sup
M2MnMgl
"(;nmax)1
EcMs[E![D cM()kM()
]inf
;!24sup
M2MnMgl
"=2()1
EcMs[E![D cM()kM()
]
79=aecD
"=2(M;s)
aecD
"=2(M):
By our choice of qwe have1
1 q=4nmax+2gM
gM. We can then bound
s=1 +
1 q1
EcMs[E!s[D cM()kM?()
](1 +)(4nmax+ 2gM)
gMaecD
"=2(M):
Lemma C.11. Consider running Algorithm 5. Then on rounds sfor whichM?2M`nMgl
"(s;ns), we have
s(1 + g?)sq+M
forsas deÔ¨Åned in Eq. (57), and q;Mparameters of Algorithm 5.
Proof of Lemma C.11. Assume we are at epoch `. Recall that !sandsminimize
sup
M2M`nMgl
"(s;ns)1
EcMs[E![D cM()kM()
]:
SinceM?2M`nMgl
"(s;ns), we have can therefore bound
1
EcMs[E!s[D cM()kM?()
]]inf
!24sup
M2M`nMgl
"(s;ns)1
EcMs[E![D cM()kM()
]]
= inf
;!24sup
M2M`nMgl
"(;ns)1
EcMs[E![D cM()kM()
]:
By construction, for every M2M`, we have
nM
"+1
M
min+4gM
M
min+2nM
"
gM+4
M
min"p
ns:
This implies that
:= min
M2M`mingM
gM+ 2nM";M
min"
4
1p
ns
and
max
M2M`max
nM
";4gM
M
min
p
ns;
which together imply that
max
M2M`max
nM
";4gM
M
min;2gM
M
min
ns
By Lemma B.4 and since M`is constructed such that infM2M`M
min>0, we can therefore bound
inf
;!24sup
M2M`nMgl
"(;ns)1
EcMs[E![D cM()kM()
]inf
;!24sup
M2M`nMgl
"=2()1
EcMs[E![D cM()kM()
]
aecD;M
"=2(M`;1
`;s)
80where the last inequality holds by the deÔ¨Ånition of M`and`. Note that by construction, sis only
supported onM`, so we can bound aecD;M
"=2(M`;1
`;s)aecD;M
"=2(M`;1
`). By construction, we also have
aecD;M
"=2(M`;1
`)2`MsM. Lastly, by our choice for qswe have1
1 qs=sq. We can then bound
s=1 +g?
1 qs1
EcMs[E!s[D cM()kM?()
](1 +)sq+M:
C.4.1 Likelihood Ratios
Lemma C.12. Consider running either Algorithm 4 or Algorithm 5. Under Assumption 2.2, with probability
at least 1 , we can bound, for any M2M,s, andi>0,
sX
i=1EcMi[Epi[DKL cM()kM()
]]sX
i=1EcMi"
logPcM;i(ri;oi)
PM;i(ri;oi)#
+VMp
56slog 1=
+VM sX
i=11=i+sX
i=1iEcMi[Epi[D cM()kM?()
]]!
:
Proof of Lemma C.12. By Theorem 2 of Shamir (2011), under Assumption 2.2 we have that with
probability at least 1 ,
sX
i=1E(r;o)M?
EcMi
logPcM;(r;o)
PM;(r;o)
jHi 1
sX
i=1EcMi"
logPcM;i(ri;oi)
PM;i(ri;oi)#
+VMp
56slog 1=:(68)
Note that
E"
EcMi"
logPcM;i(ri;oi)
PM;i(ri;oi)#
jHi 1#
=Epi
E(r;o)M?()
EcMi
logPcM;(r;o)
PM;(r;o)
=Epi
EcMi
E(r;o)M?()
logPcM;(r;o)
PM;(r;o)
By Lemma B.4 of Foster et al. (2022b), we can bound, for any cM2M,
E(r;o)M?()
logPcM;(r;o)
PM;(r;o)
 E(r;o)cM()
logPcM;(r;o)
PM;(r;o)
s
1
2
E(r;o)M?()
log2PcM;(r;o)
PM;(r;o)
+E(r;o)cM()
log2PcM;(r;o)
PM;(r;o)
D2
H 
M?();cM()
q
2V2
MD2
H 
M?();cM()
where the second inequality follows under the subgaussian assumption, Assumption 2.2. It follows that for
anycM2M,
Epi
E(r;o)M?()
logPcM;(r;o)
PM;(r;o)
Epi
E(r;o)cM()
logPcM;(r;o)
PM;(r;o)
 q
2V2
MEpi[D2
H 
M?();cM()
]
(a)=Epi[DKL cM()kM()
] q
2V2
MEpi[D2
H 
M?();cM()
]
(b)
Epi[DKL cM()kM()
] V2
M(Epi[D2
H 
M?();cM()
] + 1=)
81where (a)follows by the deÔ¨Ånition of KL divergence, and (b)from AM-GM for any  >0. Since this bound
holds uniformly for all cM2M, this implies that
E"
EcMi"
logPcM;i(ri;oi)
PM;i(ri;oi)#
jHi 1#
EcMi[Epi[DKL cM()kM()
]]
 V2
M(EcMi[Epi[D2
H 
M?();cM()
]] + 1=):
Combining this with Eq. (68), and using Assumption A.2 proves the result.
Lemma C.13. Under Assumption 2.2, we have, for any M;M02M,
DKL(M0()kM())2VM;82:
Proof of Lemma C.13. We have
DKL(M0()kM()) =E(r;o)M0()
logPM0;(r;o)
PM;(r;o)
E(r;o)M0()logPM0;(r;o)
PM;(r;o)
2VM
where the last inequality holds under Assumption 2.2.
Lemma C.14. Under Assumption 2.2, for any x>0andM;fM2M, we have
DKL 
M()kfM()
(2 + 2VM+x)D2
H
M();fM()
+ 32(1 +V2
M=x+V3
M=x2)exp( x2=8V2
M):
Proof of Lemma C.14. Fix. DeÔ¨Åne
E:=logPM;(r;o)
PfM;(r;o) DKL 
M()kfM()x
and forj2N,
Ej:=
ej 1x<logPM;(r;o)
PfM;(r;o) DKL 
M()kfM()ejx
:
Note thatE;(Ej)1
j=1form a partition of the probability space. Furthermore, since DKL 
M()kfM()
=
EoM()[logPM;(r;o)
PfM;(r;o)], under Assumption 2.2 we have that PM;(Ej)2exp( x2e2(j 1)=V2
M)andPM;(Ec)
2 exp( x2=V2
M). Now,
DKL 
M()kfM()
=Z
logPM;(r;o)
PfM;(r;o)dPM;(r;o)
=Z
ElogPM;(r;o)
PfM;(r;o)dPM;(r;o) +1X
j=1Z
EjlogPM;(r;o)
PfM;(r;o)dPM;(r;o):
Using that DKL 
M()kfM()
2VMby Lemma C.13, we can bound
1X
j=1Z
EjlogPM;(r;o)
PfM;(r;o)dPM;(r;o)1X
j=1(ejx+ 2VM)Z
EjdPM;(r;o)
=1X
j=1(ejx+ 2VM)PM;(Ej)
1X
j=1(ejx+ 2VM)2 exp( x2e2(j 1)=V2
M)
82Z1
0(ejx+ 2VM)2 exp( x2e2(j 1)=V2
M)dj
4(x+VM)Z1
0ejexp( ejx2e 2=V2
M)dj
= 4(x+VM)V2
Me2exp( x2e 2=V2
M)=x2
32(V2
M=x+V3
M=x2)exp( x2=8V2
M):
We turn now to the Ô¨Årst term. Note that we can write
Z
ElogPM;(r;o)
PfM;(r;o)dPM;(r;o) =Z
E;PfM;(r;o)>PM;(r;o)
logPM;(r;o)
PfM;(r;o)+PfM;(r;o)
PM;(r;o) 1
dPM;(r;o) PfM;(Ec)
+Z
E;PfM;(r;o)PM;(r;o)PM(oj)
PfM;(r;o)logPM;(r;o)
PfM;(r;o)+ 1 PM;(r;o)
PfM;(r;o)
dPfM;(r;o) +PM;(Ec)
Following the proof of Lemma 4 of Yang and Barron (1998) and using that logPM;(r;o)
PfM;(r;o)DKL 
M()kfM()
+
x2VM+xonE, we can bound this as
(2 + 2VM+x)Z
E(p
dPM;(r;o) q
dPfM;(r;o))2+PM;(Ec)
(2 + 2VM+x)Z
(p
dPM;(r;o) q
dPfM;(r;o))2+PM;(Ec)
(2 + 2VM+x)D2
H 
M();fM()
+ 2 exp( x2=V2
M):
C.4.2 Covering Numbers
Lemma C.15. For any subsetM0M, there exists some (;)-coverM0
covM0forM0such that
jM0
covjNcov(M;=2;).
Proof of Lemma C.15. LetMcovdenote a (=2;)-cover ofMwith eventE. Throughout the proof we
use the shorthand (r;o; )2Eto denote that there exists M2Msuch that PM;(r;ojE)>0. By deÔ¨Ånition,
it follows that for any M2M, there exists M02M covsuch that
sup
r;o; : (r;o; )2ElogPM;(r;o)
PM0;(r;o)=2: (69)
LetM0
cov=?and consider running the following procedure for every M02M cov:
1. Choose a single M2M0such that supr;o; :(r;o; )2ElogPM;(r;o)
PM0;(r;o)=2(if such an Mexists).
2. If there exists an M2M0in step 1, setM0
cov M0
cov[fMg. OtherwiseM0
covremains unchanged.
By constructionM0
covM0, andjM0
covjjM covj. We claim thatM0
covis a(;)-cover ofM0. To see why,
take someM2M0. LetM02M covdenote the point realizing Eq. (69) for M. LetM00denote the point
chosen in the above procedure for M0. Note that there must exist some M00chosen for this M0since Eq. (69)
holds forM, so in particular M2M0satisÔ¨Åes the condition of step 1 in the above procedure. Then,
sup
r;o; : (r;o; )2ElogPM;(r;o)
PM00(oj)= sup
r;o; : (r;o; )2ElogPM;(r;o)
PM0;(r;o)+ logPM0;(r;o)
PM00(oj)
 sup
r;o; : (r;o; )2ElogPM;(r;o)
PM0;(r;o)+ sup
r;o; : (r;o; )2ElogPM00(oj)
PM0;(r;o)
=2 +=2 =
83where the last inequality follows by our choice of M0and the deÔ¨Ånition of M00. Thus, it follows that M0
covis
a(;)-cover ofM0.
C.4.3 Further Lemmas
Lemma C.16. Fora > 0,[0;3=4], and > 0, the function xlog(ax)is concave in xforx
1
aexp
4

when1, and forxmax
1
aexpq
8( 1)

;1
aexp
4

when >1.
Proof of Lemma C.16. By some calculation, we have
d2
d2x
xlog(ax)
= ( 1 +)x 2+log 2(ax) + ( + 2)x 2+log 1(ax)
+ ( 1 +)x 2+log(ax):
If we restrict to x1=a, then to show that the function is concave it then suÔ¨Éces to show that
( 1 +)log 2(ax) + ( + 2) log 1(ax) + ( 1 +)0
which, since 3=4, is implied by
( 1 +)log 2(ax) +1
2log 1(ax)1
4
which is further implied by
( 1 +)log 2(ax)1
8and1
2log 1(ax)1
8:
The former condition is met for x>1=afor all2(0;1]. For >1, it is met as long as
8( 1)
log2(ax)()x1
aexp r
8( 1)
!
:
The latter condition is met for
x1
aexp4

:
Lemma C.17. For allB;C;n1, ifxClogn(Bx), thenxC(2n)nlogn(2nBC ).
Proof of Lemma C.17. This is a direct consequence of Lemma A.4 of Wagenmaker et al. (2022a).
D Proofs for Examples
In this section, we provide proofs for the examples given in Section 2. We begin in Appendix D.1 by
introducing a condition which implies nM?
"is bounded, and is easy to verify for many classes of interest. Next,
in Appendix D.2, we consider a variety of structured bandit settings, and in Appendix D.3 extend this to
contextual bandits with Ô¨Ånitely many arms. In Appendix D.4, we provide proofs for the informative arm
example of Section 1.3. Finally, in Appendix D.5, we consider tabular MDPs.
84D.1 Preliminaries: Regular Models
To bound the quantity n?
"=nM?
"for the examples we consider, it will be helpful to introduce the following
notion of a regular model .
DeÔ¨Ånition D.1 (Regular Model) .We say instance M2Mis aregular model if there exists some constant
LM
M>0such that, for any M02Malt(M)withDKL(M(M)kM0(M))>0, there exists M002Malt(M)
such thatDKL(M(M)kM00(M)) = 0and, for all 2,
jDKL(M()kM0()) DKL(M()kM00())jp
LM
MDKL(M(M)kM0(M))
+LM
MDKL(M(M)kM0(M)):(70)
Our deÔ¨Ånition of a regular model is a direct generalization of existing notions of class regularity found in
the literature (Degenne et al., 2020b). As we will see, for a variety of standard bandit classes (including
multi-armed bandits, linear bandits, and Lipschitz bandits), as well as tabular MDPs, one can show that M?
is a regular model with L?
M=LM?
Mbounded by a polynomial function of problem parameters. Intuitively,
M?will be a regular model if, for any instance M02Malt(M?)for which it is suÔ¨Écient to pull ?in order
to distinguish M?andM0(thereby ruling out M0while incurring no regret), then there exists some other
instanceM002Malt(M?)which is ‚Äúclose‚Äù to M0in a certain sense, and which cannot be distinguished from
M?by simply pulling ?. As the following result shows, the quantity n?
"can be bounded whenever M?is a
regular model.
Proposition D.1. IfMis a regular model with M
min>0, we can bound
nM
"2gM
M
min
1 +LM
M+2gM
"M
minLM
M
:
Given Proposition D.1, for many of the examples in this section, rather than bounding n?
"directly, we Ô¨Årst
show thatM?is a regular model with L?
Mwell-bounded, and then use Proposition D.1 to obtain a bound on
n?
".
Proof of Proposition D.1. To prove this result, it suÔ¨Éces to show that, for every normalized allocation
2(M;")with normalization factor n, there exists some allocation 2R
+such that 1) () =n()for
6=M, and(M)nfor some well-bounded n, and 2) M()(1 + 2")gMandIM()1 2".
Fix some2(M;")with normalization factor n>0. Note that if Mis a regular model, then IM(IM) = 0.
Since2(M;"), this implies that (M)<1. Let0denote the allocation 0(M) = 1 and0() =

1 (M)()for6=M, for someto be chosen.
We have
M(0) =
1 (M)M()
1 (M)(1 +")gM=n: (71)
TakeM02Malt(M)such thatDKL(M(M)kM0(M))>0and letM002Malt(M)denote the instance
guaranteed to exist under DeÔ¨Ånition D.1. We then have that, for any ,
DKL(M()kM0())DKL(M()kM00()) p
LM
MDKL(M(M)kM0(M)) LM
MDKL(M(M)kM0(M))
DKL(M()kM00()) (1 +)LM
MDKL(M(M)kM0(M)) 1

where the last inequality follows for any >0by AM-GM. Then
X
0()DKL(M()kM0())
=X
6=M
1 (M)()DKL(M()kM0()) + (1 )DKL(M(M)kM0(M))
85X
6=M()
1 (M)
DKL(M()kM00()) (1 +)LM
MDKL(M(M)kM0(M)) 1

+ (1 )DKL(M(M)kM0(M))
=X
6=M
1 (M)()DKL(M()kM00()) +
(1 ) (1 +)LM
M
DKL(M(M)kM0(M)) 
:
We haveM002Malt(M), so by deÔ¨Ånition
X
()DKL(M()kM00())(1 ")=n:
However,DKL(M(M)kM00(M)) = 0by assumption, so it follows that
X
6=M
1 (M)()DKL(M()kM00())
1 (M)1 "
n:
By assumption, M()(1 +")gM=n, and we can also lower bound M()(1 (M))M
min. Rearranging
these implies that
(1 (M))n(1 +")gM=M
min:
Set=(1+")gM
"M
min, then we have

1 (M)1 "
n 
=
1 (M)1 "
n "
(1 +")gM=M
min

1 (M)1 "
n "
(1 (M))n
=
1 (M)1 2"
n:
Furthermore, with this choice of , if
1
1 + (1 +)LM
M=1
1 + (1 + (1 + ")gM="M
min)LM
M;
we have

(1 ) (1 +)LM
M
DKL(M(M)kM0(M))0:
We therefore have that, for any M02Malt(M)withDKL(M(M)kM0(M))>0, that
X
0()DKL(M()kM0())
1 (M)1 2"
n:
Now consider M02Malt(M)withDKL(M(M)kM0(M)) = 0. In this case we have
X
0()DKL(M()kM0()) =
1 (M)X
()DKL(M()kM0())
1 (M)1 "
n
where the inequality follows since 2(M;")with normalization factor nby assumption. Together these
bounds then imply that:
IM(0)
1 (M)1 2"
n:
86Combining this with our bound on M(0)in (71) implies that 02(M; 2")with parameter n0=1 (M)
n.
To conclude, deÔ¨Åne the allocation :=n00. Then for6=M:
() =1 (M)
n0() =n()
and
(M) =1 (M)
n0(M)1 (M)
n
It follows then that M() =nM()(1 +")gM, andIM()n0IM(0)1 2". Therefore, satisÔ¨Åes the
desired condition. Since 2(M;"), we have
M()(1 +")gM=n=)(1 (M))n(1 +")gM=M
min;
and thus
(M)(1 +")gM
M
min2gM(1 + (1 + 2 gM="M
min)L?
M)
M
min:
which proves the result.
D.2 Structured Bandits with Gaussian Noise
In this section, we consider the problem of structured bandits with Gaussian noise, in which O=f?g, and
the mean reward functions belong to a given function class F. Concretely, we consider the model class
M=
M() =N(f();2) :f2F	
:
We set
D 
M()kM()
 DKL 
M()kM()
=1
22(fM() fM())2(72)
forDthe divergence used by AE2
?. In general in the following examples we take = 1for simplicity.
We begin by verifying that the basic regularity conditions required by our results are satisÔ¨Åed for generic
classesF, then provide bounds on the AEC for speciÔ¨Åc classes of interest.
Lemma D.1. For bandits with Gaussian noise:
1. ForD DKL, Assumptions 2.2, A.1 and A.2 hold with parameters
LKL=VM=2p
2
:
2.We can bound Ncov(M;;)by the covering number of Min the distance d(M;M0):=sup2jfM() 
fM0()jat tolerance2
2+p
22log(2=). Furthermore, it suÔ¨Éces to take E:=fjrj1 +p
22log(2=)g.
Proof of Lemma D.1. In this setting, we have that for any M;M02Mand any2,
DKL(M()kM0()) =1
22(fM() fM0())2:
ForM2M, we therefore have
DKL(M()kM0()) DKL 
M()kM0()=1
22(fM() fM0())2 (fM() fM0())2
872
2jfM() fM()j
=2p
2
q
D 
M()kM()
;
where the inequality follows from the Mean Value Theorem and the assumption that fM()2[0;1]for all
2. This veriÔ¨Åes that Assumption A.1 holds with LKL=2p
2
.
To show that Assumption 2.2 is met, we note that for all M;M;M02M,
logPM;(r;o)
PM;(r;o)=1
22(r fM())2 1
22(r fM())2
=1
22
fM()2+fM()2 2r(fM() fM())
=1
22
fM()2+fM()2 2fM0()(fM() fM()) + 2(fM0() r)(fM() fM())
=E(r;o)M0()
logPM;(r;o)
PM;(r;o)
+1
2(fM0() r)(fM() fM()):
It follows that
logPM;(r;o)
PM;(r;o) E(r;o)M0()
logPM;(r;o)
PM;(r;o)
is sub-gaussian with parameter ErM0()[(1
2(fM0() r)(fM() fM()))2]4
2, which veriÔ¨Åes Assumption
2.2 withV2
M=8
2.
Finally, we bound the covering number. Let E:=fjrj1 +p
22log(2=)g. Elementary manipulations
show that PM;(Ec)for anyM2Mand. Using the same calculation as above, we have
logPM;(r;o)
PM0;(r;o)=1
22(r fM())2 1
22(r fM0())21 +jrj
2jfM() fM0()j
where the inequality follows from the Mean Value Theorem. We therefore have that for any M;M02M,
sup
r;o; :jrj1+p
22log(2=)logPM;(r;o)
PM0;(r;o)2 +p
22log(2=)
2sup
jfM() fM0()j:
Itfollowsthatifwecanforma2
2+p
22log(2=)-coverofMinthedistance d(M;M0) =sup2jfM() fM0()j,
this will serve as an (;)cover ofM.
D.2.1 Discrete Structured Bandits
As a Ô¨Årst example of bandits with Gaussian noise, we present an additional class that satisÔ¨Åes the uniformly
regular assumption.
Example D.1 (Discrete Structured Bandits) .Fixmin>0, and consider a discrete reward space R [0;1]
satisfying minr;r02Rjr r0jmin. Consider any function class F (!R )deÔ¨Åned such that each f2F
has a unique optimal decision. Let our model class be deÔ¨Åned as
M=fM() =N(f();1)jf2Fg:
I t is straightforward to show that Assumption 2.1 and Assumption 2.2 are met with LKL;VM4, and
that Assumption 2.3 is satisÔ¨Åed with dcovscaling with the log-covering number of Fin the distance
d(f;f0) =sup2jf() f0()j, andCcov=O(1). Furthermore, Assumption 2.4 is satisÔ¨Åed by construction
ofRandF, and we can bound nM
"2
2
min.8We thus have the following corollary to Theorem 2.1.
8It is not diÔ¨Écult to see that, given the construction of R, once the optimal arm has been played2
2
mintimes, no additional
information can be extracted from playing it.
88Corollary D.1. In the discrete structured bandits setting considered above, if g?>0, the regret of AE2is
bounded as
EM?[Reg(T)](1 +")g?log(T) +aec"=12(M)poly
max
M2MgM;dcov;1
";1
min;log logT
log1=2(T)
As we show in Example 2.3, the AEC in this setting can be bounded in terms of the eluder dimension of
F. /
Proof for Example D.1. We provide calculations for the discrete structured bandits setting of Exam-
ple D.1. First, note that Assumptions 2.1 to 2.3 all hold by Lemma D.1. To bound nM, considerM2Mand
M02Malt(M). Note that either DKL(M(M)kM0(M))= 0, in which case there is no advantage to playing
M, or, due to the discretization of the means, DKL(M(M)kM0(M))1
22
min. Thus for any allocation
2R
+, as long as (M)2
2
min, we have(M)DKL(M(M)kM0(M))1. It follows that there is no
advantage to choosing (M)larger than2
2
min, so we can bound nM2
2
min.
D.2.2 Multi-Armed Bandits (Example 2.2)
In this section we prove the result in Example 2.2. First, note that for any M2M, we have gMA=M
min.
Assumptions 2.2, A.1 and A.2 are met due to Lemma D.1, and with constants LKL;VM4,dcov=O(A),
andCcov=O(1). By Lemma D.2‚Äîstated and proven below‚Äî M?is a regular model with L?
M=p
2as long
asfM?(?)<1. It remains to bound the AEC.
Proof of Proposition 2.2. It is immediate to see that Cexp(M;")O(A
")by choosing the exploration
distribution to be uniform over A. By Lemma B.6, we then
aecM
"(M?)c1A3
"26?
for a universal constant c1. By Proposition D.1, we have
n?
"=36c2g?
?
min
1 +g?
"?
min
c2A2
"(?
min)4;
for a universal constant c2, so we can lower bound ?c3"(?
min)4=A2, giving
aecM
"(M?)c4A15
"8(?
min)24:
Lemma D.2. In the multi-armed bandit setting of Example 2.2, any model M?2Mis a regular model with
L?
M=p
3as long asfM?(?)<1.
Proof of Lemma D.2. LetM02Malt(M?)and assume that DKL(M?(?)kM0(?))>0.
Case 1:fM0(M0) +fM?(?) fM0(?)1.LetM00denote the Gaussian bandit instance given by
fM00() =fM0() +fM?(?) fM0(?); by our assumption, M002M, andM002Malt(M?). Furthermore,
fM00(?) =fM?(?)which implies DKL(M?(?)kM00(?)) = 0as desired. Finally, for any , we have
fM0() fM00() =fM?(?) fM0(?):
This implies that for all , since all models in Mhave unit Gaussian rewards, using the expression for the
KL divergence given in Eq. (72):
jDKL(M?()kM0()) DKL(M?()kM00())jjfM?(?) fM0(?)j=p
2DKL(M?(?)kM0(?))
which implies the condition of DeÔ¨Ånition D.1 is met with L?
M=p
2.
89Case 2:fM0(M0) +fM?(?) fM0(?)>1.For this case the model M00constructed in Case 1 will not be
inM. Assume Ô¨Årst that fM?(?)fM0(M0)and in this case deÔ¨Åne M00to be the instance
fM00() =(
minffM?(?);fM0() +fM?(?) fM0(?)g6=M0
fM?(?) +  =M0
or some>0such thatfM?(?)+<1(note that such a exists since we have assumed fM?(?)<1). Note
that we now have M002M, andfM00()<fM00(M0)for all6=M0, soM002Malt(M?). Furthermore, we
havefM00(?) =fM?(?), soDKL(M?(?)kM00(?))= 0. For6=M0, ifminffM?(?);fM0() +fM?(?) 
fM0(?)g=fM?(?), this implies that
fM?(?)fM0() +fM?(?) fM0(?) =)fM?(?) fM0()fM?(?) fM0(?):
Since we have assumed fM?(?)fM0(M0), this implies that
jfM00() fM0()j=jfM?(?) fM0()jjfM?(?) fM0(?)j
So by the expression given for the KL divergence in Eq. (72), we have
jDKL(M?()kM0()) DKL(M?()kM00())jp
3DKL(M?(?)kM0(?)): (73)
For6=M0with minffM?(?);fM0() +fM?(?) fM0(?)g=fM0() +fM?(?) fM0(?), the bound
onjDKL(M?()kM0()) DKL(M?()kM00())jfollows identically to Case 1. For =M0, since we have
assumed that fM?(?)fM0(M0)we have
jfM00(M0) fM0(M0)j=fM?(?) fM0(M0) +fM?(?) fM0(?) +:
For small enough , this implies that Eq. (73) is satisÔ¨Åed for =M0as well.
Consider now the case where fM?(?)<fM0(M0). In this case deÔ¨Åne M00by
fM00() =(
minffM0(M0) ;fM0() +fM?(?) fM0(?)g6=M0
fM0(M0) =M0
for>0small enough that fM0(M0) >fM?(?). Note that M00andM002Malt(M?)by construction, and
thatfM00(?) =fM?(?)by our choice of , soDKL(M?(?)kM00(?))= 0. For6=M0, ifminffM0(M0) 
;fM0() +fM?(?) fM0(?)g=fM0(M0) , then we have
jfM00() fM0()j=jfM0(M0)  fM0()j
fM0(M0) fM0() +
jfM?(?) fM0(?)j+
where for the Ô¨Ånal inequality we have used that minffM0(M0) ;fM0()+fM?(?) fM0(?) =fM0(M0) .
It follows that Eq. (73) is satisÔ¨Åed for this for suÔ¨Éciently small . If we instead have minffM0(M0) 
;fM0() +fM?(?) fM0(?)g=fM0() +fM?(?) fM0(?), then the bound on jDKL(M?()kM0()) 
DKL(M?()kM00())jfollows identically to Case 1.
For=M0, we havejfM00(M0) fM0(M0)j= 0. This proves the result.
D.2.3 Structured Bandits with Bounded Eluder Dimension (Example 2.3)
In this section, we give generic bounds on the uniform exploration coeÔ¨Écient and Allocation-Estimation
CoeÔ¨Écient for structured bandit classes with bounded eluder dimension (cf. DeÔ¨Ånition 2.5). These result are
used by subsequent examples, including linear bandits.
90Lemma D.3. LetMfM() =N(f();1)jf2Fg. Then for all ">0, we have
Cexp(M;")16dE(F;p"=2)
":
Proof of Lemma D.3. Let24(M). Recall the expression for KL divergence between Gaussians of unit
variance:
EM[Ep[DKL 
M()kM()
]] =1
2EM[Ep[(fM() fM())2]]:
Abbreviate dE:=dE(F;p"=2)and letf1;:::;dEgdenote a maximal sequence of "-independent points. By
the deÔ¨Ånition of the eluder dimension, for any 2and anyM;M2M, we have:
vuutdEX
i=1(fM(i) fM(i))2p
"=2 =) jfM() fM()jp
"=2:
Now, setpto be the uniform distribution over f1;:::;dEg. Assume that M;M02Mare such that
max
M002fM;M0gEMEp[DKL 
M()kM00()
] =1
2max
M002fM;M0gEMEp[(fM00() fM())2]"=(16dE);
Markov‚Äôs inequality implies that for each M002fM;M0g, with probability at least 3=4over the draw of
M,
Ep[(fM00() fM())2]"=(2dE):
Taking a union bound, we conclude that with probability at least 1=2over the draw of M,
max
M002fM;M0gEp[(fM00() fM())2]"=(2dE): (74)
Going forward, let M2Mbe any model such that Eq. (74) holds; we have just proven that such a model
exists. It follows from the maximality of 1;:::;dEand the deÔ¨Ånition of pthat for alle2, andM002M,
Ep[(fM00() fM())2]"=(2dE) =)(fM00(e) fM(e))2"=2:
In particular, since this holds for both M2fM0;M00g, and since Eq. (74) holds, we have that for all ,
DKL(M0()kM00()) =1
2(fM0() fM00())2
(fM0() fM())2+ (fM() fM00())2
":
As this is the condition required by DeÔ¨Ånition 2.4, it suÔ¨Éces to take C
exp(") = 16dE(F;p"=2)=". Since this
bound holds uniformly for all choices of , the result follows.
Proof of Proposition 2.3. The bound
Cexp(M?;)16dE(F;p
=2)
:
follows from Lemma D.3, since dE(F0;)dE(F;)for allF0F.
By Lemma B.6 we can bound aecM
"(M?):
aecM
"(M?)Cexp(M?;)for= min
M2M?min(
min1
81LKL;M
min
34VM
"
2gM=M
min+nM
"=36;M
min
3)2
:
91Lemma Lemma B.2 implies that for all M2M?,
gMCexp(M?;1
4(M
min)2)64dE(F;1
2M
min)
(M
min)264dE(F;1
2?)
2?
where we have used that the eluder dimension increases as its scale "decreases; by Lemma D.1, it suÔ¨Éces to
takeLKL=VM= 2. A suÔ¨Écient value for is therefore
=c"28
?
dE(F;1
2?)2
The result follows.
D.2.4 Linear Bandits (Example 2.4)
Proof of Proposition 2.4. The result follows directly from Proposition 2.3, since it is known that linear
bandits have eluder dimension which scales as dE(F;") =O(dlog 1=")(Russo and Van Roy, 2013).
In what follows, we prove Proposition 2.5, providing suÔ¨Écient conditions under which it is possible to bound
the regularity constant L?
M(and hence n?
") for linear bandits.
We begin with an geometric assumption on ,X, and?, which we will show ensures that M?() =
N(hx;?i;1)is a regular model. To state our condition, we denote, for any vectors xandy, we deÔ¨Åne xy
andxyto be unique vectors satisfying x=xy+xyforxykyandxy?y.
Assumption D.1 (Regular Linear Bandits) .The sets ;Xand model parameter ?satisfy:
1.is a convex polytope.
2. For all2, we have that there exists some >0such thatf02Rd:k0 k2g.
3. Lettingx?2Xdenote the optimal action for ?, we have

2Rd:k ?k2max
x2X;x6=x??(x)=kxx?k2
:
The Ô¨Årst two points above are quite mild. The primary restriction of Assumption D.1 is Point 3, which
requires that ?is located suÔ¨Éciently far within the interior of . Using Assumption D.1 we can state the
full version of Proposition 2.5.
Proposition D.2 (Full Version of Proposition 2.5) .If;X, and?satisfy Assumption D.1, then n?
"is
bounded by a polynomial function of d;1=?
min;1=";g?, and a geometry-dependent term scaling with the
structure ofXand.
Remark D.1 (Comparison to Existing Work) .We remark that Assumption D.1 is similar to the conditions
required by existing works which achieve instance-optimality in linear bandits with polynomial lower-order
terms (Tirinzoni et al., 2020; Kirschner et al., 2021). Though neither of these works explicitly states such a
condition, closer inspection of their analysis reveals it is indeed required. In particular, the proof of Lemma
1 of Tirinzoni et al. (2020) relies on a result from Degenne et al. (2020b) which shows that a condition
analogous to DeÔ¨Ånition D.1 is met for linear bandits. However, the proof given in Degenne et al. (2020b)
appears to only hold when is unbounded, or a condition such as Assumption D.1 holds. As Tirinzoni
et al. (2020) assumes that is bounded, their results therefore only appear to hold if a condition similar to
Assumption D.1 also holds. Similarly, in the proof of Lemma 10 of Kirschner et al. (2021), it is assumed
that for every arm x6=x?, there exists some instance in the alternate set with optimal arm x. To satisfy
this condition, it appears that an assumption similar to Assumption D.1 is required.
92Thus, while not stated explicitly in the existing literature, it therefore seems that all existing results which obtain
reasonable lower-order terms require an assumption similar to Assumption D.1. Removing this assumption
(or showing it is necessary) is an interesting direction for future work.
Proof of Proposition D.2. Under Assumption D.1, this follows directly from Lemma D.4 and Proposi-
tion D.1.
Lemma D.4. Under Assumption D.1, the linear bandit model M?is regular for some L?
M<1whose value
depends on the geometry of andX.
Proof of Lemma D.4. Fix some?2and letx?denote its optimal arm. Let alt(?)denote
parameters with optimal arm x6=x?. Assume there exists some 2alt(?)such thath ?;x?i6= 0(if this
is not the case, M?immediately satisÔ¨Åes DeÔ¨Ånition D.1 and we are done). Let x=f2 :hx;ihx?;ig,
?=f2Rd:h ?;x?i= 0g, and ?
x= x\?. We Ô¨Årst show that, under Assumption D.1, ?
x6=?
for allx2X,x6=x?. We then use this to show that M?() =N(hx;?i;1)is a regular model.
Part 1: ?
x6=?.Fix somex2Xwithx6=x?. Consider =?+axx?for somea2Rto be chosen. By
construction we have h;x?i=h?;x?i, which implies that 2?for alla2R. We wish to choose alarge
enough thath;xih;x?i. Note that
h;xi=h?;xi+ahxx?;xx?+xx?i=h?;xi+akxx?k2
2
andh;x?i=h?;x?i. Thus, to satisfy h;xih;x?i, we need
akxx?k2
2h?;x? xi ()a?(x)=kxx?k2
2:
Leta= ?(x)=kxx?k2
2, then it follows that h;xih;x?i. Furthermore, we can bound
k ?k2akxx?k2= ?(x)=kxx?k2:
Under Assumption D.1, it follows that 2.
Part 2:M?is a Regular Model. Letx=f ?:2xg. Note that, since is a convex polytope,
andxsimply adds a linear inequality constraint, xis also convex. Let ?
x=f2x:h;x?i= 0g. From
Part1, wehave ?
x6=?. Lemma23ofKirschneretal.(2021)thengivesthatthereexistsageometry-dependent
constantC(;X)such that, for all 2x:
min
02?
xk 0k2C(;X)jh;x?ij:
This implies that for all 2x, we have:
min
02?xk 0k2C(;X)jh ?;x?ij:
Now consider some 2alt(?), and assume that h ?;x?i6= 0(by assumption such a exists). Assume
thathas optimal arm x, which implies that 2x. By what we have just shown, we know that there exists
some02withh0;xi>h0;x?iso that02alt(?),h0 ?;x?i= 0, and
k 0k2C(;X)jh ?;x?ij:
Note that, for any x02X, we have
jDKL(?(x0)k(x0)) DKL(?(x0)k0(x0))j=1
2jh? ;x0i2 h? 0;x0i2j
2 max
002jh00;x0ijjh 0;x0ij
2 max
002k00k2kx0k2
2k 0k2:
93Furthermore, note that
p
DKL(?(x?)k(x?)) =1p
2jh? ;x?ij;
so
jDKL(?(x0)k(x0)) DKL(?(x0)k0(x0))j
2p
2C(;X) max
002;x002Xk00k2kx00k2
2
p
DKL(?(x?)k(x?)):
As2?(?)was arbitrary, we have therefore shown that M?is a regular model with
L?
M=
2p
2C(;X)max
002;x002Xk00k2kx00k2
22
:
D.2.5 Generalized Linear Models (Example 2.5)
Proof Sketch for Example 2.5. The bound on the AEC follows as in Proposition 2.4, using that the
eluder dimension for generalized linear models is bounded as O(d(gmax
gmin)2loggmax
")(Russo and Van Roy,
2013). For the other regularity assumptions, note that by the Mean Value Theorem, we have
jDKL(M()kM0()) DKL(M()kM00())j=1
2j(g(h;xi) g(h0;xi))2 (g(h;xi) g(h00;xi))2j
2gmaxjh0 00;xij
and
p
DKL(M()kM0()) =1p
2jg(h;xi) g(h0;xi)jgminp
2jh 0;xij:
In light of these inequalities, bounds on all relevant regularity parameters for generalized linear bandits follow
from similar reasoning to the proofs for linear bandits. In particular, the conclusion of Lemma D.4 holds for
generalized linear bandits under Assumption D.1, with L?
Mas in Lemma D.4, but scaled by (gmax
gmin)2).
D.3 Contextual Bandits with Finitely Many Actions (Example 2.6)
In this setting we take D 
M()kM()
 DKL 
M()kM()
forDthe divergence employed by AE2
?. Note
that we have
DKL 
M()kM()
=1
2ExpX[Ea(x)[(fM(x;a) fM(x;a))2]]:
Lemma D.5. In the contextual bandits setting of Example 2.6:
1. Assumptions 2.2, A.1 and A.2 hold with parameters
LKL=VM= 2p
2
andD 
k
=DKL(k).
2.We can bound Ncov(M;;)by the covering number of Min the distance d(M;M0) =supx2X;a2AjfM(x;a) 
fM0(x;a)jat tolerance2
2+p
2 log(2=). Furthermore, it suÔ¨Éces to take E:=fjrj1 +p
2 log(2=)g.
94Proof of Lemma D.5. Using the expression for the KL divergence given above, for any M;M0;M2M
and2, we have
DKL(M()kM0()) DKL 
M()kM0()
=1
2ExpX[Ea(x)[(fM(x;a) fM0(x;a))2]] ExpX[Ea(x)[(fM(x;a) fM0(x;a))2]]
1
2ExpX
Ea(x)(fM(x;a) fM0(x;a))2 (fM(x;a) fM0(x;a))2
2ExpX
Ea(x)fM(x;a) fM(x;a)
2r
ExpXh
Ea(x)h
(fM(x;a) fM(x;a))2ii
= 2p
2q
DKL 
M()kM()
:
This veriÔ¨Åes Assumption A.1 holds with LKL= 2p
2. Assumption A.2 is immediate.
To show that Assumption 2.2 is met, we note that
logPM;(r;o)
PM;(r;o)= logPM;(rjo)PM;(o)
PM;(rjo)PM;(o)= logPM;(rjo)
PM;(rjo);
where the second equality holds because the context distribution is identical for all models. As the reward
likelihoods conditioned on the context are Gaussian, a calculation similar to Lemma D.1 shows that Assump-
tion 2.2 with VM= 2p
2. The covering number bound also follows from the same reasoning as Lemma D.1.
Lemma D.6. For the contextual bandit setting described above, we can bound
Cexp(M;")4A
":
Proof of Lemma D.6. Fix some24Mand letexpbe uniform overAfor each context. Then, for any
p024 , we have
Ep0[DKL(M()kM0())] =1
2Ep0[ExpX[Ea(x)[(fM(x;a) fM0(x;a))2]]]
EM[Ep0[ExpX[Ea(x)[(fM(x;a) fM(x;a))2+ (fM(x;a) fM0(x;a))2]]]]
X
a2AEM[ExpX[(fM(x;a) fM(x;a))2+ (fM(x;a) fM0(x;a))2]]
=AEM[ExpX[Eapexp(x)[(fM(x;a) fM(x;a))2+ (fM(x;a) fM0(x;a))2]]]
2AEM[DKL 
M(exp)kM(exp)
+DKL 
M(exp)kM0(exp)
]:
It follows that if
EM[DKL 
M(exp)kM(exp)
]"
4AandEM[DKL 
M(exp)kM0(exp)
]"
4A;
then we can bound Ep0[DKL(M()kM0())]". Thus, choosing pexp24()to place probability mass 1
onexp, a suÔ¨Écient bound on Cexp(M;")is4A=".
Proof of Proposition 2.7. The bound on Cexp(M?;")follows from Lemma D.6. Hence, by Lemma B.6
we can bound aecM
"(M?)as:
aecM
"(M?)4A
for= min
M2M?min(
min1
81LKL;M
min
34VM
"
2gM=M
min+nM
"=36;M
min
3)2
:
95By Lemma B.2, we have that for all M2M?,
gMCexp(M?;1
4(M
min)2)16A
2?:
By Lemma D.5, we can take LKL=VM= 2p
2. A suÔ¨Écient choice for is therefore
=c"28
?
A2:
The result follows.
D.4 Informative Arms (Example 2.1)
In this section, we provide calculations for the bandits with informative arms setting in Example 2.1. We
Ô¨Årst show that Assumptions 2.1 to 2.3 are satisÔ¨Åed.
‚Ä¢Lemma D.1 If 2[A], then the response is simply Gaussian, so by Lemma D.1, the condition of
Assumption 2.1 is met with LKL= 2. If=
i, then by the Mean Value Theorem we have
DKL(M()kM0()) DKL 
M()kM0()
=X
a2[A]PM;(a) logPM;(a)
PM0;(a) X
a2[A]PM;(a) logPM;(a)
PM0;(a)

1 + max
a2[A]maxlogPM;(a)
PM0;(a);logPM;(a)
PM0;(a)
X
a2[A]jPM;(a) PM;(a)j
=
1 + max
a2[A]maxlogPM;(a)
PM0;(a);logPM;(a)
PM0;(a)
DTV 
PM;;PM;

1 + max
a2[A]maxlogPM;(a)
PM0;(a);logPM;(a)
PM0;(a)
r
1
2DKL(PM;kPM;):
Using the bound on the log-likelihood ratio given above, this veriÔ¨Åes that Assumption 2.1 holds with
LKL= maxf2;1 + logA
1 g.
‚Ä¢If2[A], then the since the response is Gaussian, by Lemma D.1, the condition of Assumption
2.2 is met with VM= 2. If=
i, then for M2 M, either the observation is distributed as
1=A, soPM;(r;o) = 1=Afor allo2[A], oriis the informative arm for instance M, in which case
PM;(r;o) = (1 )=Aforo6=M, and PM;(r;o) =+ (1 )=Aforo=M(note that we can
disregardo=?since it occurs with probability 0 if an informative arm is pulled). The log-likehood
ratio is then at most
log+ (1 )=A
(1 )=AlogA
1 :
Thus, Assumption 2.2 is satisÔ¨Åed with VM= maxf2;logA
1 g.
‚Ä¢Using Lemma D.1, it is easy to see that Assumption 2.3 is met with dcov=O(A)andCcov=O(1).
Toboundtheparameter nM, considerM2MandM02Malt(M). Notethateither DKL(M(M)kM0(M))=
0,inwhichcasethereisnoadvantagetoplaying M,or, duetothediscretizationofthemeans, DKL(M(M)kM0(M))
1
22
min. Thus, for any allocation 2R
+, as long as (M)2
2
min, we have(M)DKL(M(M)kM0(M))1.
It follows that there is no advantage to choosing (M)larger than2
2
min, so we can bound nM2
2
min.
96D.4.1 Bounding the Allocation-Estimation CoeÔ¨Écient
We begin with some basic observations. First, since we restrict Mto only contain instances with a single
optimal decision, if fM(M) =b1
mincminfor someM2M, this implies that fM()<b1
mincminfor all
6=M. Fix some M2MsatisfyingfM(M) =b1
mincmin. It follows that, for every M02Malt(M), it
must be the case that fM0(M)<b1
mincmin. Therefore, since MandM0have diÔ¨Äerent reward means
atM, and since this holds for all M02Malt(M),Mcan be distinguished from every M02Malt(M)by
playingM. In this case, then, gM= 0, so any"-optimal Graves-Lai allocation must put all its mass on M,
implying (M;") =fIMg. Denote such instances MwithfM(M) =b1
mincminasM. Note that for M
withfM(M)<b1
mincmin, we have I
M2(M;").
We proceed to bound the value of the Fix M2co(M). For a Ô¨Årst case, assume that that fM(M)
b1
mincmin 1
2minand letk=arg maxi2[N]PM;
i(o=M)denote the index of the most informative arm
forM. Let=I
k, and note thatMgl
"()contains only instances in Mthat have informative arm k. Let
M0=fM2M :
M6=
kg[M. ThenMnMgl
"()M0. Let!=1
2Unif(f
igi2[N]) +1
2Unif([A]). Then,
aec"(M;M)sup
M2M01
E![DKL 
M()kM()
]
 sup
M2M;
M6=
k2NPN
i=1DKL 
M(
i)kM(
i)+ sup
M2M2AP
2[A]DKL 
M()kM()
If
M6=
k, this implies that oM(
i)is uniform on [A]for
i6=
M, andoM(
i)is distributed as
IM+ (1 )Unif([A])for
i=
M. Note that since k=arg maxi2[N]PM;
i(o=M)andM2co(M), we
can have at most PM;
i(o)1=A+=2for alloifi6=k, since if this were not the case, then imust bek. It
follows from Pinsker‚Äôs inequality that for Mwith
M6=
k:
DKL 
M(
M)kM(
M)
2DTV 
M(
M);M(
M)2
2jPM;
M(o=M) PM;
M(o=M)j2
= 2j 1=A =2j2
2j=4j2
where the last inequality uses our assumption that 4=A. ForM2M, sincefM(M)b1
mincmin 
1
2min, we have that
DKL 
M()kM()
1
82
min:
Thus, we can bound
aec"(M;M)64N
2+16A
2
min:
Now, consider the second case where MhasfM(M)>b1
mincmin 1
2min. Note that in this case we must
havejMj= 1. Set=IM. Then we have that Mgl
"()contains every instance except the single instance
withfM(M) =b1
mincmin. Let!=IM. Note that for any instance with fM(M)<b1
mincmin, i.e. every
instance inMnMgl
"(), we have
DKL 
M(M)kM(M)
1
82
min:
It follows that with such an M, we can bound
aec"(M;M)8
2
min:
97D.5 Tabular Reinforcement Learning (Section 2.7)
In this section, we prove all of the claims in Section 2.7 concerning tabular reinforcement learning.
Throughout this section we let Msh(a)denote the joint distribution of the next state and reward if we play
actionain statesat stephon MDPM2M. We also deÔ¨Åne
wM;
h(s;a) =PM;(sh=s;ah=a)
as the state-action visitation probabilities on MDP Munder policy (and deÔ¨Åne wM;
h(s)analogously). We
letrM
h(s;a) =ErRM
h(s;a)[r]denote the mean reward on MDP Mat(s;a;h ), and letrh(sh;ah)the denote
the realized (random reward) at step h. We letr:= (r1(s1;a1);:::;rH(sH;aH))denote the vector of all
random rewards in a given episode. = (s1;:::;sH)denotes a trajectory of states, and h=shthehth state
in the trajectory. We denote the Q-value function on Mfor policyby
QM;
h(s;a) =EM;"HX
h0=hrh0(sh0;ah0)jsh=s;ah=a#
and the value function by VM;
h(s) =Eah(s)[QM;
h(s;a)]. We denote the value of a policy by VM;
1:=
VM;
1(s1). For any function V:S!Rwe denote
PM
h[V](s;a) =Es0PM
h(js;a)[V(s0)]:
For all results in this section concerning general divergences, we take D 
k
 DKL(k).
Proof of Proposition 2.8. To bound the AEC, we Ô¨Årst move from KL divergence to Hellinger distance.
Since we always have DKL 
M()kM()
D2
H 
M();M()
, we upper bound
aecM
"(M?)sup
24Minf
;!24sup
M2M?nMgl
"()1
EM[E![D2
H 
M();M()
]]:
We then apply Lemma B.6 to bound this by CD
exp(M?;), withD 
k
 D2
H(;). The bound on CD
exp(M?;")
then follows directly from Lemma D.10, and gives
aecM
"(M?)SAH2log2H
2for= min
M2M?min(
min1
81LKL;M
min
34VM
"
2gM=M
min+nM
"=36;M
min
3)2
:
By Lemma D.8, we have that Assumptions 2.2 and A.1 hold with
LKL=VM= 4H+ max
M;M02Mmax
2max
2TlogPM0;()
PM;():
As we assume every transition has probability at least Pmin, we can lower bound PM;()PH
min, so it suÔ¨Éces
to take
LKL=VM=H(4 + log 1=Pmin):
By Lemma B.2, for M2M?we can bound
gMCexp(M?;1
4(M
min)2)cSAH2log2H
4?:
A suÔ¨Écient choice of is therefore
=c"212
?
S2A2H6log4Hlog21=Pmin:
98D.5.1 Tabular MDPs are Regular Classes
LemmaD.7. IfM?is such that rM?
h(s;a)2[0;1=H2)for all (s;a;h ), then in the setting of M M tab(Pmin),
M?is a regular model with constant
L?
M=96
?
min
Proof of Lemma D.7. Take some MDP M02Malt(M?)such thatDKL(M?(?)kM0(?))>0. LetM00
be such that
DKL(M?
sh(?(s;h))kM00
sh(?(s;h))) = 0;8s;h
and
DKL(M0
sh(a)kM00
sh(a)) = 0;8s;h;a6=?(s;h);
so thatM00is the MDP which is identical to M?on optimal actions, and identical to M0on suboptimal actions
(recall that optimal actions for M?are unique). By construction, we have that DKL(M?(?)kM00(?))= 0.
Furthermore, it is not diÔ¨Écult to see that M002M. In particular, to verify that PM00
h(s0js;a)Pmin
for each (s;a;h;s0), we note that since M?;M02M, for every (s;a;h;s0), we havePM?
h(s0js;a)Pmin
andPM0
h(s0js;a)Pmin. By construction, we have that PM00
h(js;a)is identical to either PM?
h(js;a)or
PM0
h(js;a)for each (s;a;h ), so it follows that PM00
h(s0js;a)Pmin. The remaining conditions for inclusion
inMare immediate. We consider two cases.
Case 1:M002Malt(M?).For2, by Lemma D.15 we have
DKL(M?()kM0()) =X
s;a;hwM?;
h(s;a)DKL(M?
sh(a)kM0
sh(a));
and
DKL(M?()kM00()) =X
s;a;hwM?;
h(s;a)DKL(M?
sh(a)kM00
sh(a))
=X
s;a;hwM?;
h(s;a)DKL(M?
sh(a)kM0
sh(a))Ifa6=?(s;h)g
so that
jDKL(M?()kM0()) DKL(M?()kM00())j=X
s;hwM?;
h(s;?(s;h))DKL(M?
sh(?(s;h))kM0
sh(?(s;h)))
sup
s;hwM?;
h(s;?(s;h))
wM?;?
h(s;?(s;h))DKL(M?(?)kM0(?))
sup
s;h1
wM?;?
h(s)DKL(M?(?)kM0(?))
1
?
minDKL(M?(?)kM0(?))
where the last inequality follows from Lemma D.13. Thus, in this case, M?is a regular model with
L?
M=1
?
min:
99Case 2:M0062Malt(M?).Let(es;ea;eh)be such that QM0;?
eh(es;ea)>QM0;?
eh(es;?(es;eh)), and note that such
a tuple is guaranteed to exist by Lemma D.11 since M02Malt(M?). LetfM00denote an MDP that is identical
toM00everywhere except for at (es;eh;ea), where we set rfM00
eh(es;ea)so that
QfM00;?
eh(es;ea) =QfM00;?
eh(es;?(es;eh)) + (75)
for some>0to be chosen. This will ensure eais the optimal action in (es;eh), sofM006=?. By construction we
have thatM?andfM00behave identically on ?, which implies that QfM00;?
eh(es;?(es;eh)) =QM?;?
eh(es;?(es;eh)).
Furthermore, by assumption we have rM?
h(s;a)<1=H2for all (s;a;h ), which implies QM?;?
eh(es;?(es;eh))<
1=H. AsQfM00;?
eh(es;ea) =rfM00
eh(es;ea) +PfM00
eh[VfM00;?
eh+1](es;ea)rfM00
eh(es;ea), it follows that for small enough , we can
ensure Eq. (75) is met with rfM00
eh(es;ea)<1=H, so thatfM002M.
If we can show that, for all ,jDKL(M?()kM0()) DKL 
M?()kfM00()
jis bounded by some function of
DKL(M?(?)kM0(?)), we are then done. We proceed to show this. First, note that, similar to Case 1:
jDKL(M?()kM0()) DKL 
M?()kfM00()
j
=X
s;hwM?;
h(s;?(s;h))DKL(M?
sh(?(s;h))kM0
sh(?(s;h)))
+wM?;
eh(es;ea)DKL 
M?
eseh(ea)kM0
eseh(ea)
 DKL 
M?
eseh(ea)kfM00
eseh(ea)
sup
s;h1
wM?;?
h(s)DKL(M?(?)kM0(?))
+1
2wM?;
eh(es;ea)(rM?
eh(es;ea) rM0
eh(es;ea))2 (rM?
eh(es;ea) rfM00
eh(es;ea))2(76)
where the inequality follows by what we showed in Case 1, and since M0andfM00have identical transitions at
(es;ea;eh), so the contribution to the KL divergence from the transitions cancels, leaving only the KL divergence
between unit-variance Gaussians. By the Mean Value Theorem and since rewards are in [0;1=H], we have
1
2(rM?
eh(es;ea) rM0
eh(es;ea))2 (rM?
eh(es;ea) rfM00
eh(es;ea))21
HjrM0
eh(es;ea) rfM00
eh(es;ea)j:
Thus, it suÔ¨Éces to bound jrM0
eh(es;ea) rfM00
eh(es;ea)j.
By assumption QM0;?
eh(es;ea)>QM0;?
eh(es;?(es;eh)). We can then ensure
QfM00;?
eh(es;ea) QfM00;?
eh(es;?(es;eh))QM0;?
eh(es;ea) QM0;?
eh(es;?(es;eh))
forsuÔ¨Éciently small. This is equivalent to, abbreviating eaM?:=?(es;eh):
rfM00
eh(es;ea) +PfM00
eh[VfM00;?
eh+1](es;ea) rfM00
eh(es;eaM?) PfM00
eh[VfM00;?
eh+1](es;eaM?)
rM0
eh(es;ea) +PM0
eh[VM0;?
eh+1](es;ea) rM0
eh(es;eaM?) PM0
eh[VM0;?
eh+1](es;eaM?):
By construction we have that VfM00;?
h (s) =VM?;?
h (s)for all (s;h),rfM00
eh(es;eaM?) =rM?
eh(es;eaM?), and
PfM00
eh[VfM00;?
eh+1](es;eaM?) =PM?
eh[VM?;?
eh+1](es;eaM?), sincefM00behaves identically to Mon actions taken by ?.
Furthermore, we have PfM00
eh[VfM00;?
eh+1](es;ea) =PM0
eh[VM?;?
eh+1](es;ea)sincefM00behaves identically to M0on actions
not taken by ?, other than the reward at (es;ea;eh). Using these simpliÔ¨Åcations and rearranging, we get
jrfM00
eh(es;ea) rM0
eh(es;ea)jjrM0
eh(es;eaM?) rM?
eh(es;eaM?)j+jPM?
eh[VM?;?
eh+1](es;eaM?) PM0
eh[VM0;?
eh+1](es;eaM?)j
+jPM0
eh[VM?;?
eh+1](es;ea) PM0
eh[VM0;?
eh+1](es;ea)j
100jrM0
eh(es;eaM?) rM?
eh(es;eaM?)j+jPM?
eh[VM?;?
eh+1](es;eaM?) PM0
eh[VM?;?
eh+1](es;eaM?)j
+jPM0
eh[VM?;?
eh+1](es;eaM?) PM0
eh[VM0;?
eh+1](es;eaM?)j+jPM0
eh[VM?;?
eh+1](es;ea) PM0
eh[VM0;?
eh+1](es;ea)j:
Since rewards are unit-variance Gaussian, we have
jrM0
eh(es;eaM?) rM?
eh(es;eaM?)jq
2DKL 
M?
eh;es(eaM?)kM0
eh;es(eaM?)
s
2
wM?;?
eh(es)DKL(M?(?)kM0(?)):
SinceVM?;?
eh+12[0;1], we have
jPM?
eh[VM?;?
eh+1](es;eaM?) PM0
eh[VM?;?
eh+1](es;eaM?)jX
s0jPM
eh(s0jes;eaM?) PM0
eh(s0jes;eaM?)j
2DTV
PM
eh(jes;eaM?);PM0
eh(jes;eaM?)
q
2DKL 
PM
eh(jes;eaM?)kPM0
eh(jes;eaM?)
s
2
wM?;?
eh(es)DKL 
PM
eh(jes;eaM?)kPM0
eh(jes;eaM?)
s
2
wM?;?
eh(es)DKL(M?(?)kM0(?)):
By Lemma D.12 we have
jPM0
eh[VM?;?
eh+1](es;eaM?) PM0
eh[VM0;?
eh+1](es;eaM?)jX
s0PM0
eh(s0jes;eaM?)jVM?;?
eh+1(s0) VM0;?
eh+1(s0)j
X
s0PM0
eh(s0jes;eaM?)s
8H
wM?;?
eh+1(s0)DKL(M?(?)kM0(?))
sup
ss
8H
wM?;?
eh+1(s)DKL(M?(?)kM0(?))
and similarly
jPM0
eh[VM?;?
eh+1](es;ea) PM0
eh[VM0;?
eh+1](es;ea)jsup
ss
8H
wM?;?
eh+1(s)DKL(M?(?)kM0(?)):
Altogether then:
jrfM00
eh(es;ea) rM0
eh(es;ea)j0
@s
8
wM?;?
eh(es)+ sup
ss
32H
wM?;?
eh+1(s)1
Ap
DKL(M?(?)kM0(?))
sup
s;hs
96H
wM?;?
h(s)p
DKL(M?(?)kM0(?)):
Combining this with Eq. (76), we have
jDKL(M?()kM0()) DKL 
M?()kfM00()
j
sup
s;h1
wM?;?
h(s)DKL(M?(?)kM0(?)) + sup
s;hs
96
HwM?;?
h(s)p
DKL(M?(?)kM0(?))
1011
?
minDKL(M?(?)kM0(?)) +s
96
H?
minp
DKL(M?(?)kM0(?))
where the second inequality uses Lemma D.13. Thus, in this case M?is a regular model with
L?
M=96
?
min:
D.5.2 Tabular MDPs Satisfy Basic Assumptions
Lemma D.8. Tabular MDPs with unit-variance Gaussian rewards satisfy Assumptions 2.2, A.1 and A.2
with
LKL=VM= 8H+ max
M;M02Mmax
2max
2TlogPM0;()
PM;();
andD 
k
=DKL(k), whereT:=SHandPM;()denotes the probability of observing state sequence
2TonMwhen playing policy .
Proof of Lemma D.8. We verify each assumption separately.
Verifying Assumption A.1. Fix someM;M0;M2M. Our goal is to bound
DKL(M()kM0()) DKL 
M()kM0():
LetfMdenote the MDP with transitions identical to Mbut rewards identical to M. Then
DKL(M()kM0()) DKL 
M()kM0()DKL(M()kM0()) DKL fM()kM0()
+DKL fM()kM0()
 DKL 
M()kM0()
We bound these terms separately. First, by Lemma D.15 we have
DKL(M()kM0()) =X
s;a;hwM;
h(s;a)DKL(Msh((s;h))kM0
sh((s;h)))
=X
s;a;hwM;
h(s;a)1
2(rM
h(s;a) rM0
h(s;a))2+DKL 
PM
h(js;(s;h))kPM0
h(js;(s;h))
and, given our deÔ¨Ånition of fM,
DKL fM()kM0()
=X
s;a;hwM;
h(s;a)1
2(rM
h(s;a) rM0
h(s;a))2+DKL 
PM
h(js;(s;h))kPM0
h(js;(s;h))
:
Thus,
DKL(M()kM0()) DKL fM()kM0()
=1
2X
s;a;hwM;
h(s;a)
(rM
h(s;a) rM0
h(s;a))2 (rM
h(s;a) rM0
h(s;a))2
(a)
X
s;a;hwM;
h(s;a)jrM
h(s;a) rM
h(s;a)j
102X
s;a;hwM;
h(s;a)q
2DKL 
Msh(a)kMsh(a)
s
2HX
s;a;hwM;
h(s;a)DKL 
Msh(a)kMsh(a)
=q
2HDKL 
M()kM()
;
where (a)holds by the Mean Value Theorem and the assumption that reward means are in [0;1], and the
Ô¨Ånal equality holds by Lemma D.15.
We turn now to bounding the second term. Let T=SHdenote the space of all possible state trajectories.
LetPM;(=)denote the probability of observing 2Twhen playing policy onM. We then have
DKL fM()kM0()
=Z
logPfM;(r;)
PM0;(r;)dPfM;(r;)
=Z Z
logPM;(rj)PM;()
PM0;(rj)PM0;()dPM;(rj)dPM;()
=Z
logPM;()
PM0;()dPM;() +ZZ
logPM;(rj)
PM0;(rj)dPM;(rj)
dPM;()
=X
2TPM;() logPM;()
PM0;()+X
2TPM;()DKL 
PM;(rj)kPM0;(rj)
:
It follows that
DKL fM()kM0()
 DKL 
M()kM0()X
2TPM;() logPM;()
PM0;() X
2TPM;() logPM;()
PM0;()
+X
2TjPM;() PM;()jDKL 
PM;(rj)kPM0;(rj)
:
Note that, since rewards at each state are independent,
DKL 
PM;(rj)kPM0;(rj)
=HX
h=1DKL 
PM;(rhj)kPM0;(rhj)
H;
since rewards have means are in [0;1]and are unit Gaussian. This implies
X
2TjPM;() PM;()jDKL 
PM;(rj)kPM0;(rj)
HX
2TjPM;() PM;()j
=HD TV 
M();M()
Hr
1
2DKL 
M()kM()
:
Note thatd
dxxlogx
y= 1 + logx
y, so by the Mean Value Theorem we have
PM;() logPM;()
PM0;() PM;() logPM;()
PM0;()

1 + maxlogPM;()
PM0;();logPM;()
PM0;()
jPM;() PM;()j

1 + max
M02Mmax
02TlogPM0;(0)
PM0;(0)
jPM;() PM;()j:
103It follows that
X
2TPM;() logPM;()
PM0;() X
2TPM;() logPM;()
PM0;()

1 + max
M02Mmax
02TlogPM0;(0)
PM0;(0)
X
2TjPM;() PM;()j
=
1 + max
M02Mmax
02TlogPM0;(0)
PM0;(0)
DTV 
M();M()

1 + max
M02Mmax
02TlogPM0;(0)
PM0;(0)
r
1
2DKL 
M()kM()
:
This veriÔ¨Åes Assumption A.1 with
LKL= 1 +p
2H+H+ max
M02M;M02Mmax
2max
2TlogPM0;()
PM0;():
Verifying Assumption A.2. ThatD2
H 
M();M0()
D 
M()kM0()
is immediate, since KL always
upper bounds Hellinger squared.
Verifying Assumption 2.2. We have
logPM;(r;o)
PM;(r;o)= logPM;()
PM;()+ logPM;(rj)
PM;(rj)= logPM;()
PM;()+HX
h=1logPM;(rhj)
PM;(rhj):
Using the same calculation as in Lemma D.1, we have that logPM;(rhj)
PM;(rhj)is 8-subgaussian, since rewards
are unit-variance Gaussian. As logPM;(rhj)
PM;(rhj)andlogPM;(rh0j)
PM;(rh0j)are independent for h6=h0, it follows that
logPM;(rj)
PM;(rj)is8H-subgaussian.
Furthermore, bounding
logPM;()
PM;()sup
M;M2Msup
2sup
2TlogPM;()
PM;()=:VT;
we have that logPM;()
PM;()isV2
T-subgaussian. Since the sum of subgaussian random variables is subgaussian, it
follows that logPM;(r;o)
PM;(r;o)=logPM;()
PM;()+logPM;(rj)
PM;(rj)is(V2
T+ 8H)-subgaussian, which veriÔ¨Åes Assumption
2.2.
Lemma D.9. LetPmin:=infM2Minfh;s0;s;aPM
h(s0js;a)and assumeMis such that Pmin>0. We can
construct a (;)-cover ofMwith respect toE:=fjrhj1 +p
2 log(2H=);8h2[H]g, with
Ncov(M;;)1
minfPmin
4H;2PmingS2AH(2H(2 +p
log(H=)))SAH
SAH:
Proof of Lemma D.9. Throughout this proof, we use M=f(PM
h)H
h=1;(rM
h)H
h=1g2Mto denote the MDP
inMwithRM
h(s;a) =N(rM
h(s;a);1); for brevity, weS,A, ands1to be Ô¨Åxed and the dependence on them.
Observe that for any models M;M02M, we have
logPM;(r;o)
PM0;(r;o)logPM;()
PM0;()+logPM;(rj)
PM0;(rj)
104=HX
h=1logPM
h(h+1jh;(h;h))
PM0
h(h+1jh;(h;h))+HX
h=1logPM;(rhj)
PM0;(rhj):
LetI"=f";2";:::;b1="c"g, so thatjI"j1=". LetP"denote an"cover of4Sin the`1-norm, so that for
anyP24S, there exists some P02P"such that sups2SjPs P0
sj". It suÔ¨Éces to choose P"=IS
"\4S,
so we can bound jP"j1="S. Let
Mcov=
M=f(PM
h)H
h=1;(rM
h)H
h=1g:PM
h(js;a)2P"1;rM
h(s;a)2I"2;8s;a;h	
for parameters "1;"2>0to be chosen. Then
Mcov= (jP"1jjI"2j)SAH1
"S2AH
11
"SAH
2:
We will show that Mcovis a(;)-cover ofMfor appropriately chosen Eand"1;"2>0.
LetE:=fjrhj1 +p
2 log(2H=);8h2[H]g. As we assume rewards are unit-variance Gaussian and have
means in [0;1], it is straightforward to see that P[Ec]. FixMand letM02M covdenote the instance
such that
jrM
h(s;a) rM0
h(s;a)j"2and sup
s02SjPM
h(s0js;a) PM0
h(s0js;a)j"1;8s;a;h:
Note that such an instance is guaranteed to exist by deÔ¨Ånition of Mcov.
By a similar argument as in Lemma D.1, we can bound, on E,
HX
h=1logPM;(rhj)
PM0;(rhj)HX
h=1(1 +jrhj)sup
s;ajrM
h(s;a) rM0
h(s;a)j
H(2 +p
2 log(2H=))sup
s;a;hjrM
h(s;a) rM0
h(s;a)j
H(2 +p
2 log(2H=))"2:
We also have
HX
h=1logPM
h(h+1jh;(h;h))
PM0
h(h+1jh;(h;h))Hsup
h;s0;s;alogPM
h(s0js;a)
PM0
h(s0js;a)
Hsup
jxj"1sup
h;s0;s;alogPM
h(s0js;a)
PM
h(s0js;a) x
Hsup
h;s0;s;a"1
PM
h(s0js;a) "1
wherethelastinequalityholdsaslongas infh;s0;s;aPM
h(s0js;a) "1>0. DenotingPmin:=infM2Minfh;s0;s;aPM
h(s0j
s;a), forMcovto be a (;)-cover, it therefore suÔ¨Éces that
H(2 +p
2 log(2H=))"2=2;2H"1
Pmin=2;andPmin"1=2
so it suÔ¨Éces to take
"1= minfPmin
4H;2Pmingand"2=
2H(2 +p
log(H=)):
The result now follows from our bound on jMcovj.
105D.5.3 Tabular MDPs have Bounded Uniform Exploration CoeÔ¨Écient
Lemma D.10. For the tabular MDP class Min (28), we can bound, for all ">0,
CD
exp(M;")320000SAH2log2H
"2:
forD 
k
 D2
H(;).
Proof of Lemma D.10. Let2(M)be given. DeÔ¨Åne
pexp= arg min
p24max
q24X
s;a;hEq"
EM"
(wM;
h(s;a))2
E0p[wM;0
h(s;a)]##
:
We Ô¨Årst show that, for any M2Mand any,
EM[D2
H 
M();M()
]q
SAH2EM[Epexp[D2
H 
M();M()
]]:
Consider any policy . We can bound
EM[D2
H 
M();M()
]
(a)
100 log(H)X
s;a;hEMh
wM;
h(s;a)D2
H 
Msh(a);Msh(a)i
(b)
100 log(H)vuutX
s;a;hEM"
wM;
h(s;a)2
E0pexp[wM;0
h(s;a)]#
sX
s;a;hE0pexph
EMh
wM;0
h(s;a)D4
H 
Msh(a);Msh(a)ii
(c)
200 log(H)vuutX
s;a;hEM"
wM;
h(s;a)2
E0pexp[wM;0
h(s;a)]#
sX
s;a;hE0pexph
EMh
wM;0
h(s;a)D2
H 
Msh(a);Msh(a)ii
where (a)follows from Lemma A.13 of Foster et al. (2021), (b)follows from Cauchy-Schwarz and Jensen‚Äôs
inequality, and (c)follows because the Hellinger distance is always bounded by 2. Now note that, by deÔ¨Ånition
ofpexp, we have
X
s;a;hEM"
wM;
h(s;a)2
E0pexp[wM;0
h(s;a)]#
min
p24max
q24X
s;a;hEq"
EM"
(wM;
h(s;a))2
E0p[wM;0
h(s;a)]##
and by the minimax theorem, we can bound
min
p24max
q24X
s;a;hEq"
EM"
(wM;
h(s;a))2
E0p[wM;0
h(s;a)]##
= max
q24min
p24X
s;a;hEq"
EM"
(wM;
h(s;a))2
E0p[wM;0
h(s;a)]##
max
q24X
s;a;hEq"
EM"
(wM;
h(s;a))2
E0q[wM;0
h(s;a)]##
max
q24X
s;a;hEq"
EM"
wM;
h(s;a)
E0q[wM;0
h(s;a)]##
=SAH:
By Lemma A.9 of Foster et al. (2021), since M(s;a;h ):=1
HwM;0
h(s;a)forms a valid distribution on
SA [H], we can upper bound
X
s;a;hwM;0
h(s;a)D2
H 
Msh(a);Msh(a)
HD2
H 
M(0);M(0)
:
106Altogether then, we have shown that for all 2,
EM[D2
H 
M();M()
]200 log(H)q
SAH2EM[Epexp[D2
H 
M();M()
]]
as desired. Since the Hellinger distance is a metric and satisÔ¨Åes the triangle inequality, this in particular
implies that, for any M;M0,
D2
H(M();M0())2EM[D2
H 
M();M()
] + 2EM[D2
H 
M();M0()
]
400 logHq
SAH2EM[Epexp[D2
H 
M();M()
]]
+ 400 logHq
SAH2EM[Epexp[D2
H 
M();M0()
]]:
Thus, if
EM[Epexp[D2
H 
M();M00()
]]"2
320000SAH2log2H
for bothM002 fM;M0g, thenD2
H(M();M0())". It follows that a suÔ¨Écient choice for CD
expis
320000SAH2log2H="2.
D.5.4 Supporting Lemmas
Lemma D.11. IfMhas a unique optimal policy MandM02Malt(M), then there exists some (es;ea;eh)
such that
QM0;M
eh(es;ea)>VM0;M
eh(es):
Proof of Lemma D.11. Assume that this is not the case, i.e. that for all (s;a;h ),
QM0;M
h(s;a)VM0;M
h(s) =QM0;M
h(s;M(s;h)):
Our goal is to show that in this case M0=M, which contradicts the fact that M02Malt(M). We proceed
by induction.
Base Case. Leth=Hand assume that for all (s;a),
QM0;M
H (s;a)QM0;M
H (s;M(s;h)):
This contradicts the assumption that Mis unique.
Inductive Case. Assume that M0(s;h0) =M(s;h0)for allsandh0> h. This then implies that
VM0;M
h+1(s) =VM0;M0
h+1(s)for alls. It follows that for all a
QM0;M
h(s;a) =rM0
h(s;a) +PM0
h[VM0;M
h+1](s;a) =rM0
h(s;a) +PM0
h[VM0;M0
h+1](s;a) =QM0;M0
h(s;a)
so in particular QM0;M
h(s;M(s;h)) =QM0;M0
h(s;M(s;h)). Since we have assumed that for all (s;a)
QM0;M
h(s;a)QM0;M
h(s;M(s;h))
we have
QM0;M0
h(s;M0(s;h))QM0;M0
h(s;M(s;h)):
However, since each M2Mhas a unique optimal action at each state, this is a contradiction unless
M0(s;h) =M(s;h), which proves the inductive hypothesis. The result follows.
107Lemma D.12. For MDPs M;M0with unit variance Gaussian rewards, we have
VM0;
h(s) VM;
h(s)s
8H
wM;
h(s)DKL(M()kM0()):
Proof of Lemma D.12. In the Gaussian reward setting, we have
rM0
h0(s0;a0) rM
h0(s0;a0)q
(rM0
h0(s0;a0) rM
h0(s0;a0))2q
2DKL 
Mh0;s0(a0)kM0
h0;s0(a0)
:
Furthermore, since VM0;
h0+1(s0)2[0;1], we have
PM0
h0[VM0;
h0+1](s0;a0) PM
h0[VM0;
h0+1](s0;a0)X
s00jPM0
h0(s00js0;a0) PM
h0(s00js0;a0)j
= 2DTV 
PM0
h0(js0;a0);PM
h0(js0;a0)
q
2DKL 
PM0
h0(js0;a0)kPM
h0(js0;a0)
q
2DKL 
Mh0;s0(a0)kM0
h0;s0(a0)
:
By Lemma D.14, Jensen‚Äôs inequality, and Lemma D.15, it follows that
VM0;
h(s) VM;
h(s)HX
h0=hX
s0;a0wM;
h0(s0;a0jsh=s)2q
2DKL 
Mh0;s0(a0)kM0
h0;s0(a0)
2vuut2HHX
h0=hX
s0;a0wM;
h0(s0;a0jsh=s)DKL 
Mh0;s0(a0)kM0
h0;s0(a0)
2vuut2H
wM;
h(s)HX
h0=hX
s0;a0wM;
h0(s0;a0)DKL 
Mh0;s0(a0)kM0
h0;s0(a0)
2s
2H
wM;
h(s)DKL(M()kM0())
where we have used that, for h<h0,
wM;
h0(s0;a0) =X
s00wM;
h0(s0;a0jsh=s00)wM
(s00;h)wM;
h0(s0;a0jsh=s)wM;
h(s):
Lemma D.13. For anyM2Mfor whichMis unique, we have
M
minmin
s;hwM;M
h(s):
Proof of Lemma D.13. Letedenote the policy that diÔ¨Äers from policy Monly at the state esand layereh
given by (es;eh) =arg mins;hwM;M
h(s). Note that this implies, in particular, that wM;M
eh(es) =wM;e
eh(es)sincee
andMtake identical actions up to step eh. By the Performance-DiÔ¨Äerence Lemma (Kakade, 2003), we have
VM;M
1 VM;e
1=HX
h=1X
s;awM;e
h(s;a)(VM;M
h(s) QM;M
h(s;a))
(a)=wM;e
eh(es;e(es;eh))(VM;M
eh(es) QM;M
eh(es;e(es;eh)))
108=wM;M
eh(es)(VM;M
eh(es) QM;M
eh(es;e(es;eh)))
wM;M
eh(es)
where (a)holds since VM;M
h(s) =QM;M
h(s;a)for all (s;a;h )withwM;e(s;a;h )>0other than at (es;eh). By
assumption, the optimal policy is unique, so VM;M
1 VM;e
1>0, and thus
M
min= min
2 :VM;M
1 VM;
1>0VM;M
1 VM;
1VM;M
1 VM;e
1wM;M
eh(es) = min
s;hwM;M
h(s):
Lemma D.14 (Lemma E.15 of Dann et al. (2017)) .For MDPs M;M0and policy, we have
VM0;
h(s) VM;
h(s) =HX
h0=hX
s0;a0wM;
h0(s0;a0jsh=s)h
rM0
h0(s0;a0) rM
h0(s0;a0) +PM0
h0[VM0;
h0+1](s0;a0)
 PM
h0[VM0;
h0+1](s0;a0)i
:
Lemma D.15. For MDPs M;M0and policy, we have
DKL(M()kM0()) =HX
h=1X
s;awM;
h(s;a)DKL(Mhs(a)kM0
hs(a)):
Proof of Lemma D.15. This is a standard calculation; see e.g. (Simchowitz and Jamieson, 2019; Tirinzoni
et al., 2021).
E Proofs and Additional Results from Section 3
E.1 Technical Lemmas
Throughout this section, when the class Mis clear from context, we deÔ¨Åne
(M;";n) :=f24 :9nns.t.M()(1 +")gM=n;IM(;M)(1 ")=ng:(77)
Lemma E.1 (Derandomization) .Letn>0be given. For any p24(4()), deÔ¨Åningp=Ep[]2(),
we have that for all M2M,
I
p=2(M;";n)	
 1Pp[ =2(M;"=2;n)];
where:="
2minn
1;gM
no
.
Proof of Lemma E.1. LetM2Mbe Ô¨Åxed and abbreviate IM() =IM(;M). Fixp24(4()). For
any2(M;"=2;n), let n>0denote the least n>0such that
M()(1 +"=2)gM=n;andIM(;M)(1 "=2)=n:
DeÔ¨Åne
n=
Ep1
nj2(M;"=2;n) 1
;
and note that by Jensen‚Äôs inequality,
nEp[nj2(M;"=2;n)]n:
109We Ô¨Årst observe that since M2[0;1],
M(p)Ep[M()j2(M;"=2;n)] +Pp[ =2(M;"=2;n)]
(1 +"=2)gMEp1
nj2(M;"=2;n)
+Pp[ =2(M;"=2;n)]
= (1 +"=2)gM
n+Pp[ =2(M;"=2;n)]:
Next, note that the map 7!IM()is concave and non-negative (it is an inÔ¨Åmum over non-negative linear
functions), so we have
IM(p)Ep[IM()]
Ep[IM()j2(M;"=2;n)]Pp[2(M;"=2;n)]
(1 "=2)Ep1
nj2(M;"=2;n)
Pp[2(M;"=2;n)]
= (1 "=2)1
nPp[2(M;"=2;n)]
= (1 "=2)1
n(1 Pp[ =2(M;"=2;n)]):
It follows that as long as
Pp[ =2(M;"=2;n)]:="
2min
1;gM
n
"
2min
1;gM
n
;
we have
M(p)(1 +")gM=n;andIM(p;M)(1 ")=n;
so thatp2(M;";n)(M;";n). We conclude that
I
p=2(M;";n)	
IfPp[ =2(M;"=2;n)]>g 1Pp[ =2(M;"=2;n)]:
Lemma E.2. LetM2Mbe given, and suppose Assumption 2.4 holds. Fix T2Nand consider an algorithm
Asuch that for all M02Malt(M)[fMg,
EM0;A[Reg(T)]RM0log(T):
for some bound RM2. DeÔ¨ÅneM2R
+via
M() =EM;AT()
log(T)
:
Then if
log(T)6
"log 
sup
M02Malt(M)[fMgRM0
M0
minlog(T)!
;
we must have
IM(M;M)(1 "):
Proof of Lemma E.2. Throughout this proof we will use that Mis uniquely deÔ¨Åned for all M2Mby
Assumption 2.4. Note that
EM;A[Reg(T)]RMlogT=)X
6=MEM;A[T()]RMlogT
M
min:
110Fix someM02Malt(M). ThenM6=M0(recall that under Assumption 2.4, each M2Mhas a unique
optimal), so
EM;A[T(M0)]RMlogT
M
min;EM0;A[T(M0)]T RM0logT
M0
min:
By Lemma H.1 of Simchowitz and Jamieson (2019), we have that for any HT-measurable variable Z2[0;1],
that
X
EM;A[T()]DKL(M();M0())d(EM;A[Z];EM0;A[Z])
ford(x;y) =xlogx
y+ (1 x) log1 x
1 y. Choosing Z=T(M0)=T, and using that
d(x;y)(1 x) log1
1 y log 2;
we have
X
EM;A[T()]DKL(M();M0())
1 RMlogT
M
minT
logT
T (T RM0logT
M0
min) log 2
=
1 RMlogT
M
minT
logT logRM0logT
M0
min
 log 2:
Now, if
log(T)2
"log 
supM02Malt(M)[fMgRM0logT
M0
min_2!
;
we have log(2)"log(T),
logT logRM0logT
M0
min
(1 ") log(T);
and1 RMlogT
M
minT(1 "), so we can lower bound
X
EM;A[T()]DKL(M();M0()) 
(1 ")2 "
log(T)(1 3") log(T):
As this is true for every M02Malt(M), the result follows.
Lemma 3.1. Let"2(0;2), and suppose that Assumption 2.4 holds. Fix T2Nand consider an algorithm A
such that for all M2M,
EM;A[Reg(T)](1 +")gMlog(T):
For eachM2M, deÔ¨ÅneM2R
+viaM() =EM;Ah
T()
log(T)i
, whereT()denotes the number of pulls of
decision, and deÔ¨Åne M=M=kMk1. Then if
log(T)6
"log
sup
M2M2gM
M
minlog(T)
;
we have that for all M2M,
M2(M;"): (36)
Proof of Lemma 3.1. Immediate consequence of Lemma E.2.
111E.2 Proof of Theorem 3.1
Theorem E.1 (Full Statement of Theorem 3.1) .Let">0andM0Mbe given. LetfnMgM2M0be a
collection of non-negative scalars indexed by M0, and set:="
2minn
1;infM2M0gM
nMo
. Unless
T >
8sup
M2M+aecM
2"(M0;M);
any algorithm must have, for some M2M 0:
PM;Ah
b =2(M;";nM)i

6:
Proof of Theorem E.1. Fix" >0, and let an algorithm Abe given. For any M2M+, deÔ¨ÅneqM=
PM;A(b=)24(4()), and let!M:=EM;Ah
1
TPT
t=1pti
24().
Fix>0andM2M+be Ô¨Åxed. DeÔ¨Åne
M= arg max
M2M0n
PqM[ =2(M;";nM)]jE!M
DKL 
M()kM()
2o
;
we assume that such an M2M 0does exist, as otherwise the claim we will prove is trivial. It is immediate
from this deÔ¨Ånition that we have
PM;Ah
b =2(M;";nM)i
=PqM[ =2(M;";nM)]
= sup
M2M0n
PqM[ =2(M;";nM)]jE!M
DKL 
M()kM()
2o
 inf
q24(4());!24()sup
M2M0
Pq[ =2(M;";nM)]jE!
DKL 
M()kM()
2	
=:opt;
with the convention that this value is zero if the set
M2M 0jE!
DKL 
M()kM()
2	
is empty.
In addition, we have
E!M
DKL 
M()kM()
2: (78)
Now, deÔ¨Åne :="
2minn
1;infM2M0gM
nMo
, and letq:=Eq[]. By Lemma E.1, we have
opt= inf
q24(4());!24()sup
M2M0
Pq[ =2(M;";nM)]jE!
DKL 
M()kM()
2	
 inf
q24(4());!24()sup
M2M0
I
q=2(M; 2";nM)	
jE!
DKL 
M()kM()
2	
 inf
24();!24()sup
M2M0
If =2(M; 2";nM)gjE!
DKL 
M()kM()
2	
 inf
24();!24()sup
M2M0
If =2(M; 2")gjE!
DKL 
M()kM()
2	
=In
2 
aecM
2"(M0;M) 1o
: (79)
We conclude that
PM;Ah
b =2(M;";nM)i
In
2 
aecM
2"(M0;M) 1o
: (80)
To proceed, using Lemma A.11 of Foster et al. (2021), we have
PM;Ah
b =2(M;";nM)i
1
3PM;Ah
b =2(M;";nM)i
 4
3DKL(PM;AkPM;A)
112
3In
2 
aecM
2"(M0;M) 1o
 4
3DKL(PM;AkPM;A):
Using (78) gives
DKL(PM;AkPM;A) =EM;A"TX
t=1EptDKL 
M()kM()#
=TE!M
DKL 
M()kM()
2T;
so we have
PM;Ah
b =2(M;";nM)i

3In
2 
aecM
2"(M0;M) 1o
 4
32T:
We set2=
8T, so that
PM;Ah
b =2(M;";nM)i

6In
2 
aecM
2"(M0;M) 1o
=
6I
T
8aecM
2"(M0;M)
:
By taking the supremum over all possible choices for M2M+, we conclude that unless
T >
8sup
M2M+aecM
2"(M0;M);
the algorithm must have PM;Ah
b =2(M;";nM)i

6.
E.3 Proof of Theorem 3.2
Recall that for M2M+we deÔ¨Åne
M= arg max
2fM()
as the set Mof all optimal decisions withfM() =max02fM(0). Unless otherwise stated, the
results in this subsection do not make use of Assumption 2.4. For M2M+andM0M, we deÔ¨Åne
Mopt
0(M) =
M2M 0jMM; D KL 
M()kM()
= 082M	
:
For a subset 0, let
N:0=
t2[T]jt=20	:
Note that for all M2Mopt
0(M), since MM, we have
N:MN:M:
Theorem 3.2 (Main lower bound‚Äîstrong variant) .Let">0,nmax>0, andM0Mbe given, and deÔ¨Åne
="
2minf1;infM2M0gM=nmaxg. Unless
sup
M2M0gM
M
minlog(T)
(2)sup
M2M+aecM
2"(Mopt
0(M);M);
there is no algorithm that simultaneously ensures that
1.EM;A[Reg(T)]2gMlog(T);8M2M 0.
2.PM;Ah
b =2(M;";nmax)i

12;8M2M 0.
113Proof of Theorem 3.2. For eachM2 M, ifEM;A[Reg(T)]2gMlog(T), then EM;A[N:M]
2gM
M
minlog(T). TheresultnowfollowsbyappealingtoTheoremE.2with nM=nmaxandR= 2supM2M0gM
M
minlog(T).
Theorem E.2. LetT2N," > 0,R1, andM0Mbe given. LetfnMgM2M0be a collection of
non-negative scalars indexed by M0. DeÔ¨Åne="
2minn
1;infM2M0gM
nMo
. Unless
R2
192sup
M2M+aecM
2"(Mopt
0(M);M);
there is no algorithm that simultaneously ensures that
1.EM;A[N:M]R;8M2M 0.
2.PM;Ah
b =2(M;";nM)i

12;8M2M 0.
Proof of Theorem E.2. Let">0be Ô¨Åxed. To prove the result, it suÔ¨Éces to lower bound the constrained
minimax value
M:= sup
M2M+inf
A(
sup
M2Mopt
0(M)PM;Ah
b =2(M;";nM)i
jEM0;A[N:M0]R8M02Mopt
0(M))
:(81)
We begin by appealing to the following technical lemma.
Lemma E.3. LetM2M+andT2Nbe given. Consider any algorithm Awith the property that for all
M2Mopt
0(M),
EM;A[N:M]R
for someR1. For any2(0;1), there exists a modiÔ¨Åed algorithm A0with the following properties:
‚Ä¢PM;A0h
N:M>dR
ei
= 0for all models M2M+.
‚Ä¢For allM2Mopt
0(M),
PM;Ah
b =2(M;";nM)i
PM;A0h
b =2(M;";nM)i
 :
By Lemma E.3, for any 2(0;1), we have
Msup
M2M+inf
A(
sup
M2Mopt
0(M)PM;Ah
b =2(M;";nM)i
jPM0;A
N:M>R

= 08M02M+)
 :
Now, consider an arbitrary choice for Mabove. We lower bound the minimax value using another technical
lemma.
Lemma E.4. LetT2Nand">0be given. LetfnMgM2Mbe a collection of non-negative scalars indexed
byM. Consider any algorithm Awith the property that
PM;A
N:M>R
= 0
for someR1. For anyM2M+, if we set:="
2minn
1;infM2Mopt
0(M)gM
nMo
, then unless
R>
8aecM
2"(Mopt
0(M);M);
the algorithm must have
sup
M2Mopt
0(M)PM;Ah
b =2(M;";nM)i

6:
114Boundingl
R
m
2R
, it follows from Lemma E.4 that unless
2R
>
8aecM
2"(Mopt
0(M);M);
where:="
2minn
1;infM2Mopt
0(M)gM
nMo
, we have
Minf
A(
sup
M2Mopt
0(M)PM;Ah
b =2(M;";nM)i
jPM0;A
N:M>R

= 08M02M+)
 
6 :
To conclude, we set =
12and maximize over M2M+.
Proof of Lemma E.3. Fix2(0;1)and letC:=dR
e. Fix A= (p;q)and consider the algorithm
A0= (p0;q0)deÔ¨Åned implicitly as follows. For t= 1;:::;T:
‚Ä¢Sampletpt(jHt 1).
‚Ä¢Ifj
itji=2M	
j=C, break and play an arbitrary decision 2Muntil round T.
Returnbq(jHT).
It is immediate from this construction that A0= (p0;q0)hasN:MCalmost surely under all possible
modelsM2M+. We now focus on bounding the performance. Let T0be the greatest value of tfor which
j
itji=2M	
jC. First, observe that for all M2Mopt
0(M), since the algorithms behave identically in
law whenever T0=T,
PM;A0h
b2(M;";nM)i
PM;A0h
b2(M;";nM)^T0=Ti
=PM;Ah
b2(M;";nM)^T0=Ti
=PM;Ah
b2(M;";nM)^N:MCi
:
By the union bound, we have
PM;Ah
b2(M;";nM)^N:MCi
PM;Ah
b2(M;";nM)i
 PM;A
N:M>C
:
Finally, we observe that by Markov‚Äôs inequality we have
PM;A
N:M>C
EM;A
N:M
CEM;A[N:M]
C;
where we have used that N:MN:M, since MM. Rearranging, we obtain
PM;Ah
b =2(M;";nM)i
PM;A0h
b =2(M;";nM)i
 :
Proof of Lemma E.4. Fix">0, and let an algorithm Abe given. For any M2M+, deÔ¨ÅneqM=PM;A(b=
)24(4()), and let!M:=EM;Ah
1
N:MP
t:t=2Mpti
24(), with the convention that the value inside
the expectation is zero whenever N:M= 0.9
Fix>0andM2M+be Ô¨Åxed. DeÔ¨Åne
M= arg max
M2Mopt
0(M)n
PqM[ =2(M;";nM)]jE!M
DKL 
M()kM()
2o
;
9IfN:M= 0almost surely under M, we can take R= 0, in which case the statement of the lemma is vacuous.
115we assume that such an M2Mopt
0(M)does exist, as otherwise the claim we will prove is trivial. It is
immediate from this deÔ¨Ånition that we have
PM;Ah
b =2(M;";nM)i
=PqM[ =2(M;";nM)]
= sup
M2Mopt
0(M)n
PqM[ =2(M;";nM)]jE!M
DKL 
M()kM()
2o
 inf
q24(4());!24()sup
M2Mopt
0(M)
Pq[ =2(M;";nM)]jE!
DKL 
M()kM()
2	
=:opt;
with the convention that this value is zero if the set
M2Mopt
0(M)jE!
DKL 
M()kM()
2	
is
empty. In addition, we have
E!M
DKL 
M()kM()
2: (82)
Now, deÔ¨Åne :="
2minn
1;infM2Mopt
0(M)gM
nMo
, and letq:=Eq[]. By Lemma E.1, we have
opt= inf
q24(4());!24()sup
M2Mopt
0(M)
Pq[ =2(M;";nM)]jE!
DKL 
M()kM()
2	
 inf
q24(4());!24()sup
M2Mopt
0(M)
I
q=2(M; 2";nM)	
jE!
DKL 
M()kM()
2	
 inf
24();!24()sup
M2Mopt
0(M)
If =2(M; 2";nM)gjE!
DKL 
M()kM()
2	
 inf
24();!24()sup
M2Mopt
0(M)
If =2(M; 2")gjE!
DKL 
M()kM()
2	
=In
2 
aecM
2"(Mopt
0(M);M) 1o
: (83)
Hence, we have
PM;Ah
b =2(M;";nM)i
In
2 
aecM
2"(Mopt
0(M);M) 1o
: (84)
To proceed, using Lemma A.11 of Foster et al. (2021), we have
PM;Ah
b =2(M;";nM)i
1
3PM;Ah
b =2(M;";nM)i
 4
3DKL(PM;AkPM;A)

3In
2 
aecM
2"(Mopt
0(M);M) 1o
 4
3DKL(PM;AkPM;A):
Now, recall that from the deÔ¨Ånition, we have that for all M2Mopt
0(M),
DKL(PM;AkPM;A) =EM;A2
4X
t:t=2MEptDKL 
M()kM()3
5
=EM;A2
4N:M
N:MX
t:t=2MEptDKL 
M()kM()3
5
REM;A2
41
N:MX
t:t=2MEptDKL 
M()kM()3
5
=RE!M
DKL 
M()kM()
2R;
116where the Ô¨Årst inequality uses that PM;A
N:M>R
= 0, and the second inequality uses (82).
With this, we have
PM;Ah
b =2(M;";nM)i

3In
2 
aecM
2"(Mopt
0(M);M) 1o
 4
32R:
We set2=
8R, so that
PM;Ah
b =2(M;";nM)i

6In
2 
aecM
2"(Mopt
0(M);M) 1o
=
6I
R
8aecM
2"(Mopt
0(M);M)
:
We conclude that unless
R>
8sup
M2M+aecM
2"(Mopt
0(M);M);
the algorithm must have PM;Ah
b =2(M;";nM)i

6.
E.4 Proofs for Lower Bound Examples
Proof of Example 3.2. Let2(0;1)andA2be given and set
M=n
M() =N(fM();1=2)jfM2[0;1]Ao
and
M0=fM2M : M
min=2g
DeÔ¨ÅneM2MviafM() = If=Ag. Fix"2(0;1=2)and deÔ¨Åne a subclass
M0=fMg[fMigi2[A 1]
via
fMi() = If=Ag+"If=ig:
Since"1=2, we haveM0Mopt(M)andM0M 0. In addition, we have
DKL 
M()kMi()
= (fM() fMi())2:
LetM00Mdenote the set of instances such that, for M02M00,DKL(Mi(A)kM0(A))= 0, andM02
Malt(Mi), for alli2[A 1]. Then,
IMi(;M) = inf
M02Malt(Mi)E[DKL(Mi()kM0())]
inf
M02M00E[DKL(Mi()kM0())]
= min
j2[A 1]
i(1 ")22Ifj=ig+j2Ifj6=ig	
:(85)
We also have that
gMi=g:=(A 2)
+1
(1 ");
where we have used again that "1=2.
117Fix any pair ;!24 and consider the value
sup
M2M0nMgl
"()(
1
E!
DKL 
M()kM())
:
Pick"=32, and letJ [A 1]be the set of models ifor which2(Mi;). For each such model, by
deÔ¨Ånition, there exists ni>0such that
Mi()(1 +)g
ni;andIMi(;M)(1 )1
ni:
DeÔ¨Åne n=maxi2Jniand n=mini2Jni. Using Eq. (85), it is immediate that for all j2[A 1],
j(1 ) 2
n, and that for all i2J,
i(1 )(1 ") 2 2
ni:
In particular, this implies that for all i2J,
(1 )(A 2) 1
n+ (1 )(1 ") 1 1
niMi()
(1 +)g
ni
= (1 +)(A 2) 1
ni+ (1 +)(1 ") 1 1
ni;
or by rearranging,
(1 )(A 2) 1
n(1 +)(A 2) 1
ni+ 2(1 ") 1 1
ni;
(1 +)(A 2) 1
ni+ 4 1
ni;
(1 + 2)(A 2) 1
ni
as long asA6. Since this holds uniformly for all i2J, rearranging once more gives
n(1 + 2)
1 n(1 + 2)2n(1 + 8)n;
where we have used that 1=2.
Now, observe that for all i2J, we have
Mi()(1 )(A jJj  1)1
n+ (1 )jJj1
(1 ")2n+ (1 )1
(1 ")ni;
(1 )(A jJj  1)1
n+(1 )
1 + 8jJj1
(1 ")2n+ (1 )1
(1 ")ni;
(1 )
n
(A jJj  1) +jJj1 8
(1 ")2
+ (1 )1
(1 ")ni;
(1 )
n
(A jJj  1) +jJj1
(1 ")
+ (1 )1
(1 ")ni;
where we have used that1
1+x1 x, and that"=8. Suppose thatjJjA
2. Then we have
(A jJj  1) +jJj1
(1 ")A
2
1 +1
1 "
 1(1 +"=2)A 1
118(1 +"=2)(A 1);
so that
Mi()(1 )(1 +"=2)
n(A 1) + (1 )1
(1 ")ni:
Noting that nniand"=8, we further have
Mi()(1 +"=4)A 1
1
ni+ (1 )1
(1 ")ni:
Observe that the right-hand side above is greater than (1 +)g
niif and only if
(1 +"=4)(A 1)>(1 +)(A 1) +2
1 ";
which is satisÔ¨Åed if "=32. In this case, we have
Mi()>(1 +)g
ni;
which contradicts the assumption that i2J. It follows that we must have jJj<A= 2.
Now, to conclude, select i= arg mini2[A 1]nJ!i, and consider the value
sup
M2M0nMgl
"()(
1
E!
DKL 
M()kM())
1
E!
DKL 
M()kMi()=1
!i"22;
where the Ô¨Årst inequality follows because  =2(Mi;)by deÔ¨Ånition, and the equality follows from the
construction of MandMi. SinceP
i2[A 1]nJ!i1andj[A 1]nJjA
2, we must have !i2
A, so that
1
!i"22A
2"22
as desired. To complete the proof, note that this holds uniformly for all choices for and!, and that
aecM
"(M0;M) = inf
;!24sup
M2M0nMgl
"()(
1
E!
DKL 
M()kM())
inf
;!24sup
M2M0nMgl
"()(
1
E!
DKL 
M()kM())
:
To obtain the parameter setting in the theorem statement, we rescale  2and" 32".
Proof of Example 3.3. We reduce the lower bound to that of multi-armed bandits via a standard tree
construction (Osband and Van Roy, 2016; Domingues et al., 2021); as the argument is standard, we only
sketch the approach. Assume without loss of generality that His a multiple of 2. SetH=log2(S=2).
Consider a sub-class M0MdeÔ¨Åned as follows. All models M2M0have identical, deterministic dynamics
given by a binary tree. Each layer hhas2h 1states, so that layer HhasS=4states, and the total number
of states is S 1. The agent begins from a root state s1deterministically. For hH 1, there are two
available actions, leftandright. Choosing leftleads to the left successor for the current layer, and rightleads
to the right successor for the current layer. There are no rewards for layer hH 1. For layerH, there are
Aavailable actions, and rewards are arbitrary, subject to the constraint that the mean lies in [0;1]and the
noise followsN(0;1=2).
It is clear that the class M0is equivalent to the class of multi-armed bandit instances with SA=4actions. As
a consequence, the lower bound follows from Example 3.2.
119Proof of Example 3.1. Let2(0;1=6),2(0;1), andA;N2be given. Consider the reference
modelM2M+deÔ¨Åned as follows:
‚Ä¢For each bandit arm k2[A], we havefM(k) =1
2+ Ifk=AgandrN(fM(k);1). There are no
observations, i.e. o=?almost surely.
‚Ä¢For each revealing arm 
k, we receive zero reward almost surely (so fM(
k) = 0) andoUnif([A]).
We deÔ¨Åne a subclass
M0=fMjgj2[N]Mopt
0(M)
as follows
‚Ä¢For each bandit arm k2[A], we havefMj(k) =1
2+ Ifk=AgandrN(fMj(k);1). There are no
observations, i.e. o=?almost surely.
‚Ä¢For each revealing arm 
k, we receive zero reward almost surely (so fMj(
k) = 0). We have
oUnif([A]); k 6=j;
Ii+ (1 )Unif([A]); k =j:
Note thatM0M 0. For allj2[N], a direct calculation gives
DKL 
M(
j)kMj(
j)
=A 1
Alog1
1 
+1
Alog1
1 +(A 1)
=:
and
DKL 
M()kMj()
=If=
jg: (86)
In addition, it is straightforward to see that 2whenever1=2. Next we calculate that for any
j2[N]andM2Mwith
M=
j, andM6=A,
DKL 
Mj(
j)kM(
j)
=log
1 +A
1 
=:;
which hasO()whenever1=Aandlog(1 +A). Lastly, we have that for all i2[A], all
M;M02Mhave
DKL(M(i)kM0(i)) =1
2(fM(i) fM0(i))2:
LetM00denote the set of instances such that for M02M00,M06=A, andfM0(A) =1
2+ , so that
DKL(Mj(A)kM0(A))= 0andM00Malt(Mj)for allj2[N]. Using the above calculations and the
deÔ¨Ånition of IMj(;M), we can then compute, for all j2[N],
IMj(;M)inf
M02M00E[DKL(Mj()kM0())] =2
2min
k2[A 1]k+
j(87)
and
gMj=g:= min
2A 1
;1
2+ 1

=1
2+ 1

whenever=2(A 1).
Fix any pair ;!24 and consider the value
sup
M2M0nMgl
1=2()(
1
E!
DKL 
M()kM())
:
120LetJ [N]be the set of models jfor which2(Mj; 1=2). For each such model, by deÔ¨Ånition, there exists
nj>0such that
Mj()(1 + 1=2)g
nj1
nj;andIMj(;M)1
2nj:
DeÔ¨Åne n= maxj2Jnjandn= minj2Jnj. Let us begin with some basic observations.
‚Ä¢Since Mj= Mfor allj, we have Mj()1
nj0for allj;j02J, and hence
Mj()1
n: (88)
‚Ä¢Anyj2Jmust have

j1
4nj1
4n: (89)
To see this, observe that if it were not the case, we would need mini2[A 1]i2
21
4njto satisfy the
constraint that IMj(;M)1
2nj(by (87)). But if this were to occur, we would have
A 1
2njMj()1
nj;
which would contradict the assumption that 2=(A 1).
Combing the inequalities (88) and (89), it follows that any j2Jmust have
jJj
8nj1
2X
k2J
kMj()1
nj;
which implies that jJj 8. Hence, as long as N16, we havej[N]nJjN=2.
To conclude, select k= arg minj2[N]nJ!
j, and consider the value
sup
M2M0nMgl
1=2()(
1
E!
DKL 
M()kM())
1
E!
DKL 
M()kMk()=1
!
k;
where the Ô¨Årst inequality follows because  =2(Mk; 1=2)by deÔ¨Ånition, and the equality follows from (86).
SinceP
j2[N]nJ!
j1andj[N]nJjN
2, we must have !
k2
N, so that
1
!
kN
2:
as desired. Since this holds uniformly for all choices for and!, the proof is completed.
E.5 Lower Bound on Regret for Algorithms with Well-Behaved Tails
In this section, we present an additional result, Theorem E.3, which shows that for algorithms for which the
tail behavior is ‚Äúwell-behaved‚Äù in a certain sense, the Allocation-Estimation CoeÔ¨Écient directly leads to lower
bounds on the least possible value of Tfor which any algorithm can achieve (approximate) instance-optimality.
Theorem E.3. Let the time horizon T2N,"2(0;1=2), andM0Mbe given. Suppose that there exists
an algorithm Awith the property that for all M2M 0,
1.EM;A[Reg(T)](1 +")gMlog(T).
2. For all2, ifEM;A[T()]6= 0, then EM;A[T()]1.
1213.p
EM;A[(Reg(T))2]2gMlog(T).
In addition, assume that 1) gM1for allM2M 0, 2) Assumption 2.4 holds, 3) Assumption 2.2 holds with
parameterVM1, and 4) that
log(T)12
"log
sup
M2M2gM
M
minlog(T)
:
Then if we deÔ¨Åne ="minf1;infM2M0gM
3gM=M
min+nM"g, it must be the case that
log3(T)2
Csup
M2M+aecM
4"(Mopt
0(M);M);
forCO
(supM2MgM
M
min)4V2
Mlog( 1)
"2
.
We prove Theorem E.3 by combining Theorem 3.2 with an another technical result, Proposition E.1 (stated
and proven in the sequel), which shows that for algorithms that satisfy the assumptions of Theorem E.3, it is
possible to use the empirical decision frequencies to compute an allocation that is approximately optimal
with high probability.
Proof of Theorem E.3. DeÔ¨Åne nM= 3gM
M
min+nM
", and set
:="min
1;inf
M2M0gM
nM
:
Assume that
log(T)12
"log
sup
M2M2gM
M
minlog(T)
:
LetAbethealgorithminthestatementoftheproposition, andlet A0bethemodiÔ¨Åedalgorithmcreatedthrough
PropositionE.1withparameter . Byassumption, wehavethatp
EM;A[N:M]R:= 2supM2MgM
M
minlog(T).
We deÔ¨Ånen=clog(24 1)
"2R3V2
M
log(T)for a suÔ¨Éciently large numerical constant candT0=Tn. Proposition E.1
implies that for time T0, the algorithm A0satisÔ¨Åes
q
EM;A0[N:M]R0:=Rn
and
PM;A0h
b2(M; 2";nM)i
1 
24:
On the other hand, since the precondition of Theorem E.2 is now satisÔ¨Åed with parameter R0, we have that
unless
R02
192sup
M2M+aecM
4"(Mopt
0(M);M); (90)
the algorithm must have
PM;A0h
b2(M; 2";nM)i
1 
12:
As
12>
24, this is a contradiction unless (90) holds.
Proposition E.1. Let the time horizon T2NandM0Mbe given. Let Abe an algorithm with the
property that for all M2M 0,
1.EM;A[Reg(T)](1 +")gMlog(T)for some"2(0;1).
1222. For all2, ifEM;A[T()]6= 0, then EM;A[T()]1.
3.q
EM;A
N2:M
Rfor someR2.
In addition, assume that 1) gM1for allM2M 0, 2) Assumption 2.4 holds, 3) Assumption 2.2 holds with
parameterVM1, and 4) that
log(T)12
"log
sup
M2M2gM
M
minlog(T)
:
Then for any 2(0;e 1), if we deÔ¨Åne n=clog( 1)
"2R3V2
M
log(T)for a suÔ¨Éciently large numerical constant c,
there exists an algorithm A0that, using T0:=Tnrounds, returns a normalized allocation b24 such that
PM;A0h
b2(M; 2";nM)i
1 ;
fornM3gM
M
min+nM
"and thatq
EM;A0
N2:M
Rnand
EM;A0[Reg(T0)](1 +")gMlog(T)n
for allM2M 0.
Proof of Proposition E.1. We Ô¨Årst state a technical lemma regarding robust mean estimation.
Lemma E.5. LetX2Rdbe a random variable with :=E[X]. Assume thatkk0s, wheresis a known
upper bound. For any e 1, there exists an estimator bnthat, given nindependent samples from X,
ensures that with probability at least 1 ,
kbn k124r
2sEkX k2
2log( 1)
n:
In addition,kbnk0swith probability 1.
Throughout, we will use that since Assumption 2.4 holds, Mis unique for all M2M. FixM2M 0. Let
b2R
+denote the vector of empirical decision frequencies when Ais run with horizon T, i.e.b() =T().
Let
M=EM;A[b]:
For parameters n2Nande 1we deÔ¨Åne A0as follows:
‚Ä¢RunAa total ofntimes independently (so that T0=Tn), and letb1;:::;bnbe the empirical decision
frequencies.
‚Ä¢Apply the algorithm from Lemma E.5 to b1;:::;bnwith parameters ands= 2R, and let 2R
+be
the resulting vector (note that we can take to have non-negative entries without loss of generality).
‚Ä¢Setb=arg max2, and sete() = ()If6=bgande(b) = nM
"logT(note that nM
"is a
class-dependent quantity, and so is known to the learner).
‚Ä¢Setb=e=kek1.
It is immediate that this construction satisÔ¨Åesq
EM;A0
N2:M
Rnand
EM;A0[Reg(T0)](1 +")gMlog(T)n;
so it remains to show that bis a near-optimal allocation with high probability when nis chosen appropriately.
We start by applying Lemma E.5. To do so, we carry out some prerequisite calculations. First, by assumption,
we haveM()1ifM6= 0. Using this, along with Assumption 2.4, we have
kMk01 +X
6=MM()1 +EM;A[N:M]1 +R2R:
123Second,
EM;Akb Mk2
2EM;Ah
(b(M) M(M))2i
+X
6=MEM;A
b()2
EM;Ah
(b(M) M(M))2i
+EM;A
N2
:M
:
Furthermore,
EM;Ah
(b(M) M(M))2i
=EM;A2
640
@X
6=Mb() X
6=MM()1
A23
75
EM;A2
640
@X
6=Mb()1
A23
75
=EM;A
N2
:M
:
so that
EM;Akb Mk2
22EM;A
N2
:M
2R2:
As a result, Lemma E.5 implies that with probability 1 ,
k Mk1r
C1log( 1)
n=:"stat (91)
whereC1=O(R3).
Next, we appeal to Lemma E.2, which implies that as long as
log(T)12
"log
sup
M2M2gM
M
minlog(T)
; (92)
we have
M(M)(1 +"=2)gMlog(T);andIM(M;M)(1 "=2) log(T):
Applying (91) and using that M2[0;1]andDKL(M()kM0())2VMby Lemma C.13, we have
M()(1 +"=2)gMlog(T) +"stat;andIM(;M)(1 "=2) log(T) "stat2VM:
Thus, as long as
"stat1
2"min
gMlog(T);(2VM) 1log(T)	
;
we have
M()(1 +")gMlog(T);andIM(;M)(1 ") log(T): (93)
Next, we claim that b=M. To see this, note that since EM;A[Reg(T)]2gMlog(T), we have
M(M)T 2gM
M
minlog(T)
T3
4T
as long asT >8gM
M
minlog(T), which is implied by the condition (92). Hence, as long as "statT
4, we have
(M)>T
2;
124which implies that b=M. By deÔ¨Ånition of nM
", and since satisÔ¨Åes Eq. (93) above, we then have that
setting (b) =nM
"log(T)does not aÔ¨Äect the regret, and only decreases the information gain by a factor of
"log(T). It follows that
M(e)(1 +")gMlog(T);andIM(e;M)(1 2") log(T):
We conclude that b2(M; 2";n)for
n=kek1=log(T):
To wrap up, we compute that
kek1X
6=MM() +"stat+nM
"log(T)2gM
M
minlog(T) +"stat+nM
"log(T)3gM
M
minlog(T) +nM
"log(T)
whenever"statgMlog(T).
Proof of Lemma E.5. From Proposition 1 of Lugosi and Mendelson (2019), we have that for any e 1,
there exists an estimator enthat, given nindependent samples of X, ensures that with probability at least
1 ,
ken k212r
EkX k2
2log( 1)
n:
Givenen,wedeÔ¨Ånebn=arg minu2Rd:kuk0sku enk2. Sincekk0s,wehaveken bnk2=minu:kuk0sken 
uk2ken k2. It follows that
kbn k2ken bnk2+ken k2ken k2:
Finally, we note that since bnandare boths-sparse,kbn k1p
2skbn k2.
125