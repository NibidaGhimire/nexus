On the Efficacy of Metrics to Describe Adversarial Attacks  
Tommaso Puccetti, T ommaso Zoppi , Andrea Ceccarelli  
Department of Mathematics and Informatics, University of Florence, Italy  
 
 
 
Abstract   
Adversarial defenses  are naturally evaluated on the ir ability 
to tolerate adversarial attacks. To test defenses, diverse ad-
versarial attacks are crafted, that are usually described in 
terms of their evading capability and the  L0, L1, L2, and  Lâˆ 
norms.  We question  if the evading capability and L -norms 
are the most effective  information to claim that defenses have 
been tested against a representative attack set . To this extent, 
we select image  quality metrics from the state of the art and 
search correlations between image p erturbation and detecta-
bility . We observe that computing L-norms  alone is rarely  the 
preferable solution . We observe a strong correlation between  
the identified metric s computed on an adversarial image and 
the output of a detector on such an image , to the extent that 
they can predict the response of a detector  with approxi-
mately 0.94  accuracy . Further, we observe that metrics can 
classify attacks based on similar perturbations and similar de-
tectability. This suggests a  possible  review of the approa ch to 
evaluate detectors, where additional metrics are included to 
assure that a representative attack dataset is selected . 
Introduction  
DNNs are vulnerable to adversarial attacks, in which the in-
put (e.g., images, texts, tabular data) is deliberately modi fied 
to mislead the target model  (Carlini  et al. 2019) , (Goodfel-
low et al. 2014) . These attacks are particularly effective in 
deceiving an image classifier to produce completely wrong 
predictions . 
Consequently, different defenses have been proposed  in 
the last years . Typically, defenses are evaluated based on  
their ability to  protect a targe t model against adversarial at-
tacks.  Attack s are crafted for the target model, starting from 
the input dataset, and  consist  of multiple adversarial images . 
Each adversarial image  is usually described using i) the at-
tack configuration parameters, ii) the success rate , and iii) 
values of the L 0, L1, L 2, and Lâˆ norms  (simply named L -
norms in what follows) . L-norms are the typical means  to 
describe the perturbation introduced in the input by the ap-
plication of an attack  (Carlini et al. 2019 ), (Xu et al. 2018), 
(Meng et al, 2017) .   
Information on the attack s is particularly important to 
evaluate the quality of a defense . The attack s used to evalu-
ate defenses  should be sufficiently representative of the pos-
sible attacks and the possible perturbation s introduced in the 
image. This should be evident from the description of the evaluation approach applied:  otherwise results on the ro-
bustness of the defenses could be not conclusive , specific to 
the considered attack sets , or difficult to interpret . 
We investigate whether computing the perturbation  
through different metrics  is a useful  proxy to use in the eval-
uation of adversarial defenses .  
 Intuitively, t he sole usage of L-norms  may be  misleading  
because  a defense may detect attack s of type A  and fail to 
detect attacks of type B, even if the y have similar L-norms 
value s. If only type A attack s are used , the defensive mech-
anism may be erroneously considered very robust  whereas 
it cannot reliably detect type B attacks . 
In this paper , we compute L-norms and other metrics 
from image quality t hrough an extensive experimental cam-
paign . The objective is to und erstand whether the effective-
ness of an attack  against a defensive solution  can be estab-
lished by quantifying the perturbation  introduced in an im-
age. As such, we  comput e and analyz e metrics to identify 
those  that identify  a strong correlation between the pertur-
bation and the prediction outcome of two state-of-the-art de-
fenses . 
Our empirical assessment is structured as follows. We 
generate  10 attack  sets from 10 different attacks . For each 
adversarial image , we compute  a total of 12 metrics: 4 L-
norm s and 8 image quality metrics  from the literature , 
namely  MSE, UQI, ERGAS, SAM, SCC, VIFP, RASE, and 
PSNR -B. We train and exercise two state -of-the-art detec-
tors to detect attacks on each attack set . We train a Random 
Forest  (RF)  classifier using the values of the 12 metrics  as 
features  and the output of the detector  as ground truth.  As 
such, t he RF classifier  provides a way to correlate the per-
turbation introduced in the image  to the ability of the attack 
to decei ve defenses . Our analysis highlights that  using im-
age quality metric s as features allows the classifier  to predict 
the output  of a detector with very high accuracy , even better 
than L -norms . Further, training the same RF classifier  with 
a leave -one-out approach shows  that image  quality metrics  
and L -norms  can identify  classes of attacks that introduce 
similar perturbations and lead to similar detection results. 
This is a stepping stone to wards  partition ing adversarial at-
tacks into classes, from which rep resentatives should be se-
lected when testing defenses.  Background  
Adversarial  Attacks on Image Classifiers  
We focus only on evasion  adversarial  attacks  that target im-
age classifiers . This is the typical application domain in re-
search on adversarial attac ks, and it is the focus of most of 
the works on the subject (Carlini et al. 2019), (Goodfellow  
et al. 2014), ( Madry  et al. 2017). An evasion attack aims at 
perturbating an image x, correctly classified, in such a way 
that the resulting image x' is wrongly classified, while being 
perceptually indistinguishable from the original . 
 An image classifier model can be seen as a function:  f(x, 
Î¸): RhÃ—wÃ—c â†’ {1 . . . k}  that maps an input image x  to a finite 
label set Y with k classes. The Î¸ indicates the parameters of 
the model f, and h, w, and c represent image height, width, 
and channel, respectively. Usually, the adversary generation 
method consists in searching for a perturbation Î´ âˆˆ RhÃ—wÃ—c 
that maximizes the loss function  L. Therefore, Î´ is estimated 
as Î´*: argmax (L p (Î¸, x+Î´, y) , where y is the label of x, and 
p can be 0, 1, 2 and âˆ. The adversarial counterpart of x is 
expressed as  x': = x + Î´*. 
Evasion attacks can be white -box or black -box. White -
box attacks imply that the attackers have full knowledge of 
the target model , while black-box attacks do not have th is 
information.   
Metrics to select adversarial attacks  
Protections against evasion attacks are naturally evaluated 
in terms of their ability to detect or tolerate attacks. The at-
tacks are typically crafted according to selected methodolo-
gies and tools . In addition, configuration parameters of the 
attacks are needed for reproducibility. The configuration pa-
rameters can change according to the attack method and the 
target classifier. For example, the FGM  (Goodfellow  et al. 
2014), and BIM  (Kurakin , 2017)  attacks require the specifi-
cation of the maximum number of iterations and the maxi-
mum amount of perturbation epsilon allowed.  Lastly, the 
perturbation introduced by the attack can be measured to 
have precise information about how far the adversarial im-
age is from the original one. The works in this field usually 
report on i) configuration parameters used to run the attack 
method, ii) the perturbation introduced by the attacker , and 
iii) the attack success rate (Carlini et al. 2019 ), (Xu et al. 
2018), (Meng et al, 2017) . This way, comparing  defensive 
solutions  is possible .  
At the state -of-the-art, the amount of perturbation intro-
duced by an attacker is quantified using the pixel Lp norms , 
for any p > 0 :  
||v|| p =  ( âˆ‘  |vi|p N
i=1 )1
p 
 
where v=||x â€“x'|| is the pertu rbation introduced in an image 
x to obtain his adversarial counterpart x', and  vi=||x iâ€“xi'|| is the difference pixel by pixel of the two images, with n the 
total number of pixels. It is common practice to report the 
value of one of the L âˆ, L0, L1, and L2 to describe the com-
position of the attack set. Each of these L-norms describes 
different aspects of the perturbation through a single value, 
thus, are i nformative in different ways , as reported  by (Car-
lini et al. 2019 ): 
â€¢ L0 distance measures the number of coordinates i such 
that xi â‰  x i'. The L 0 distance is equal to the number of 
pixels that have been altered in an image.  
â€¢ L1 distance, also known as Manhatt an Distance, is de-
fined as the sum of the absolute difference between pix-
els of two images.  
â€¢ L2 distance, also known as Euclidean distance, is defined 
as the squared root of the sum of the squared absolute 
difference between pixels of two images. The L 2 dis-
tance can remain small when there are many small 
changes to many pixels.  
â€¢ Lâˆ distance is defined as the largest absolute difference 
between pixels of two images .  It measures the maxi-
mum change to any of the coordinates: ||x â€“ x'||âˆ = max(| 
xi â€“ xi' |, â€¦,|x n â€“ xn'|).  For images, we can think there is 
a maximum budget, and each pixel is allowed to be 
changed by up to this limit, with no limit on the number 
of pixels that are modified.  
Image Quality Metrics  
We identify alternative metrics to quantify the perturbation 
with the aim of extrapolating as much information as possi-
ble from the  adversarial image. We explore  the image  qual-
ity domain because particularly fits our necessity.  Image 
quality metrics  have been crafted to quantitatively evaluate 
the perce ived quality of images modified by a variety of dis-
tortions ( e.g., processing, compression, etc.).  This is compa-
rable to introducing an adversarial perturbation into an im-
age. State -of-the-art image quality metrics can be classified 
according to the availa bility of an original (distortion -free) 
image to use as a comparison. In this paper, we consider  
only the case when the original image is available:   this is 
called the full reference approach . The rationale  is that  we 
want to use the original image as a reference to compute the 
adversarial perturbation. We selected the following metrics . 
Mean Squared Error (MSE)  (Yim et al. 2010)  is usually 
used to compute an estimator's quality . In our context, the 
vector xi and xi' can be seen as a linearized version of origi-
nal and adversarial images.  
MSE  = 1
ğ‘›âˆ‘ (ğ‘¥ğ‘–âˆ’ğ‘¥ğ‘–â€²ğ‘›
ğ‘–=1) 
Universal Quality Image Index (UQI)  (Wang et al. 2002)  
calculates  the amount of transformation of relevant data 
from the reference image into the perturbed image. The 
range of this metric is -1 to 1. The value 1 indicates that the 
reference and perturbed images are similar.  UQI = 4ğœğ‘¥ğ‘¥â€² ğ‘¥Ì…ğ‘¥Ì…â€²
(ğœğ‘¥2 + ğœğ‘¥â€²2)[(ğ‘¥Ì…)2 + (ğ‘¥Ì…â€²)2 ] 
Erreur Relative Globale Adimensionnelle SynthÃ¨se (ER-
GAS)  (Wald et al. 2000) calculates the average error of each 
band of the perturbed image with respect to the reference 
one. High values of ERGAS indicate the low quality of the 
perturbed image, w hile lower values indicate good quality.   
ERGAS = 100  â„
ğ‘¤[ 1
ğ‘› âˆ‘ ( ğ‘…ğ‘€ğ‘†ğ¸2
ğ‘šğ‘’ğ‘ğ‘›2 )ğ‘›
ğ‘˜=1  ] 1
ğ‘› 
where h and l are height and width of the image and the 
RMSE is the Root Mean Squared Error computed between 
original and altered image.  
Spectral Angle Mapper (SAM) (Yuhas  et al. 1992) com-
putes the spectral angle between the pixel  vector of the ref-
erence image, and of the perturbed image. It is worked out 
in either degrees or radians. It is performed on a pixel -by-
pixel basis. A value of SA M equal to zero denotes the ab-
sence of spectral distortion.  
SAM  = ğ‘ğ‘Ÿğ‘ğ‘ğ‘œğ‘  (âŒ©ğ‘¥, ğ‘¥â€²âŒª
||ğ‘¥||2âˆ™||ğ‘¥â€²||2) 
Spatial Correlation Coefficient (SCC)  (Zhou  et al. 1998 ) 
represents the correlation between two visual signals of im-
ages in a cortical visual space.  
SCC = âˆ‘ âˆ‘ (ğ‘¥min âˆ’ ğ‘¥Ì…)(ğ‘¥â€²min âˆ’ ğ‘¥Ì…â€²)ğ‘ğ‘›âˆ’1ğ‘€ğ‘šâˆ’1
âˆš(âˆ‘ âˆ‘ (ğ‘¥min âˆ’ ğ‘¥Ì…)2ğ‘ğ‘›âˆ’1ğ‘€ğ‘šâˆ’1 )(âˆ‘ âˆ‘ (ğ‘¥â€²min âˆ’ ğ‘¥â€²Ì…Ì…Ì…)2ğ‘ğ‘›âˆ’1ğ‘€ğ‘šâˆ’1 ) 
Relative Average Spectral Error (RASE)  (Gonzalez et al. 
2004) determine s the difference in spectral information be-
tween each band of the merged image and of th e original 
image. Given M the mean radiance of the N spectral bands 
Bi of the original image, and the RMSE the root mean square 
error computed in following the expression:  
RASE = 1
ğ‘€âˆš1
ğ‘ âˆ‘ ğ‘…ğ‘€ğ‘†ğ¸2(ğµğ‘–)ğ‘
ğ‘–=1  
Visual Information Fidelity (VIF)  (Sheik et al. 2011) is 
computed for a collection of wavelet coefficients from each 
sub-band  that could either represent an entire sub-band  of an 
image, or a spatially localized region of sub-band  coeffi-
cients. In the former case, the VIF quantifies the in formation 
fidelity for the entire image.  
VIF = âˆ‘ ğ¼(ğ¶âƒ—ğ‘,ğ‘—; ğ¹âƒ—ğ‘,ğ‘— | ğ‘ ğ‘,ğ‘—) ğ‘— âˆˆ ğ‘ ğ‘¢ğ‘ğ‘ğ‘ğ‘›ğ‘‘ğ‘ 
âˆ‘ ğ¼(ğ¶âƒ—ğ‘,ğ‘—; ğ¸âƒ—âƒ—ğ‘,ğ‘— | ğ‘ ğ‘,ğ‘—) ğ‘— âˆˆ ğ‘ ğ‘¢ğ‘ğ‘ğ‘ğ‘›ğ‘‘ğ‘  
Block Sensitive - Peak Signal -to-Noise Ratio (PSNR -B) 
(Yim et al. 2010) is a widely used metric . It is comput ed by 
the number of gray levels in the image divided by the corre-
sponding pixels in the original  and the perturbed  images. 
When the value is high, the perturbed and original images 
are similar.  
PSNR -B(x, xâ€²) = 10log 10ğ¿2
ğ‘€ğ‘†ğ¸ âˆ’ğµ(ğ‘¥,ğ‘¥â€²) Where L2 is the square of the numbers of pixels  in the 
image  
Methodology and Research Questions  
Selection of  Dataset and Classifier  
We select the CIFAR -10 (Krizhevsky  et al. 2009 ) dataset 
which is well -known at the state of the art , color based, and 
images are of l imited size. The size of the image s is 
32Ã—32Ã—3 and each image use s a 24 -bit representation of 
each pixel. Then we select  the ConvNet12 CNN  from (Ma 
et al.  2018) as target classifier  because  it is fast and of rea-
sonable accuracy ( 0.85). The model has 6 convolutional lay-
ers, 1 dense  layer, 3 pooling layers, and 3 dropout layers. 
The trainable parameters are 2,923,050.  
Generation of Adversarial Image Sets  
As suggested in  (Carlini et al. 2019) , we consider white box 
and black box attack s to build candidate attack sets . We se-
lect 7 white -box attacks and 3 black -box attacks  among 
those implemented by the Adversarial Robustness Toolbox 
ART  (Nicolae et al. 2019) . More specifically, we use the 
white -box attacks: CW2 -Carlini and Wagner L2 (Carlini et 
al. 2019) , FGM -Fast Gradient Method  (Goodfellow  et al.  
2014) , BIM -Basic Iterative Methods  (Kurakin et al. 2018) , 
DEEP -Deep Fool (Moosavi -Dezfooli  et al. 2016) , PGD - 
Projected Gradient Descent (Madry  et al. 2017) , NEW -
Newton Fool  (Jang et al. 2017) , ELA -Elastic Net Attack  
(Chen  et al. 2018) . We select  the black -box attacks: HOP - 
HopSkip Jump Attack  (Chen et al. 2020) , ZOO -Zeroth -Or-
der Optimization Attack (Chen et al. 2017) , BOUN D-
Boundary Attack (Brendel  et al. 2017). For each attack, we 
apply  many different parameters  configurations  to have at-
tack samples with different perturbation s: for each attack, 
images range from very little perturbation to evident pertur-
bation.  
We generate  adversarial images from the first 100 images 
in the CIFAR test set. We exclude the images that are origi-
nally misclassified by the ConvNet12 model because in this 
case, the effect of the adversarial perturbation is not rele-
vant. Further, we only include a ttacked images that effec-
tively deceive the image classifier because we are interested 
in the detection of adversarial images that affect the classi-
fication. Consequently, we obtain a set of adversarial images 
that have 100% of success rate  on the ConvNet1 2 model.  In 
total we obtain  222 attack configurations and  18062 adver-
sarial images.  
Research Questions  
We study, through an extensive experimental campaign, 
which  are the most suitable metrics to select the adversarial 
attack s when testing defens es. We search metrics that can relate the attacks to the defense. We formulate  the following 
questions:  
RQ1. Is a single L-norm  sufficiently informative to  use in 
the evaluation of adversarial defense s? L0, L1, L2, and Lâˆ 
norms are widely used:  we investigate if these metrics allow 
measuring the capability of an adversarial image in evading 
defensive detectors.  
RQ2. Is it possible to identify a correlation between the 
output of an adversarial image detector and the value of a 
single L -norm ? In other words, we investigate if any L-
norms  can predict the output of the detector just by looking 
at the image perturbation.  
RQ3. Alternatively to L-norms (see RQ1), is there an im-
age quality metric that allows to efficiently describe the 
strength of a n attack?  In other words, we suggest the possi-
bility of more informative alternatives  with respect to L -
norms .  
RQ4. Does combin ing L-norm s with image quality  met-
rics build a feature set that allows an accurate prediction of  
the output  of a detector with respect to a given image?  In 
other words, can we increase the  performance of the predic-
tor built in RQ2 using multiple metrics?   
RQ5 Is it meaningful to classify attacks based on the per-
turbation  introduced in the image ? We investigate if differ-
ent attacks generate similar perturbations that lead to similar 
outputs of the detector. If this is true, it would be possible to 
create groups of attacks, such that representatives from each 
group should be selected  to properly cover the attack s pace. 
To seek our answers, we need to compute metrics on ad-
versarial images. We apply L-norms and image quality met-
rics to the adversarial images described previously . We use 
the implementation of the quality metrics provided by the 
Sewar1 library , while L-norms  implementation s come from 
sklearn . After, each generated adversarial image is tested 
against two state -of-the-art detectors. We associate  each im-
age with the detection labels provided by the two detectors. 
Each label is 1 if the attacked im age has been detected by 
the detector, and 0 on the contrary. Summarizing, we 
achieve a dataset that associates each adversarial image to 
two detection labels and the computed metrics values.  
The two detectors are MagNet (Meng  et al. 2017)  and 
Squeez er (Xu et al. 2018) because i) they are very well -
known, and ii ) the source code is publicly available . MagNet 
(Meng et al. 2017)  calculates the distance between the input 
x and a reformed input x'. If the image is genuine, the dis-
tance between x and x' is expec ted below a target threshold. 
If it is greater than a such threshold value, the input x is con-
sidered an attack. The threshold is selected during training, 
to target a specific false positive rate on the validation set. 
Squeez er (Xu et al. 2018)  combines m ultiple feature squeez-
ers. It compares the prediction of the classifier on the inputs 
 
1 Andrew Khalel, Sewar library, last modified on 12/07/2022, 
github.com/andrewekhalel/sewar   with the predictions obtained using pre -processed inputs. 
The detector computes a score that is the maximum distance 
among these predictions. The final detection is done by se-
lecting a pre -defined threshold for a such score, to distin-
guish between genuine and adversarial images. The training 
of the detector consists in selecting a threshold on the score.  
Execution Environment and Repositor ies 
Experiments have been executed on a Dell Precision 5820 
Tower with an Intel I9 -9920X, GPU NVIDIA Quadro 
RTX6000 with 24GB VRAM, 128GB RAM, Ubuntu 18.04 
with kernel 5.4.0, and runtime CUDA 11.0.  
All data generated,  the configuration parameters,  and the 
code to reproduce our experiments is at 2. 
Experiments Execution and Results  
We elaborate  the answers to the 5 questions.  
Answer to RQ1.  We seek for adversarial images which 
show  similar values of the L-norms  but different response s 
from the detector.  We found that, in some examples , the 
value  a single norm is not enough to describe the effective-
ness of the attack.   
 
 L-norms  Detectors Labels  
At-
tacks L2 L1 Lâˆ L0 MagNet  Squeezer  
DEEP  10.717  477.524  0.682  3072  0 0 
BIM  10.728  478.13  0.673  3072  1 1 
CW2  33.87  1793.76  0.867  3072  1 1 
ELA  33.87  1795.76  0.864  3072  0 0 
Table 1: Examples of adversarial images from the attack sets . 
The attacks  in the first two row from Table 1 have an L 2 
norm close to 10.7 ; however, BIM attack is detected  by 
MagNet and Sqeezer  (detect or label s on the right of the table 
are 1 in the second row ), while DEEP is not detected ( detec-
tor labels are 0 in the first row ). The same behavior can be 
observed in the ELA  and CW2 highlighted in gray. They 
have the exact same L2 value, but an opposite decision from 
the detector s. Furthermore, in both attacks , the L 1, L0, and 
Lâˆ values are very close to each other , if not equal. This 
makes it difficult to determine the effectiveness of an attack 
using a single norm . The values of the L 1 and L âˆ norms are 
far from each other but, again, the detection label is the 
same.  
Answer to RQ2. We research a statistical correlation be-
tween the results obtained by the detectors and the values of 
the selected metrics.  
2  Tommaso Puccetti, Experiments Repository, 2022, github.com/Tom-
masoPuccetti/adversarial_perturbation   First, we compute the Pearson Correlation Coefficient  be-
tween each metric and the labels of the detectors , but with-
out relevant res ults. Then we use RF as a correlator : we train 
an RF classifier using part of the dataset created with the aim 
of predicting the response of the detectors. The idea is to 
predict the output  of a detector to a specific image knowing 
only information about the perturbation  described by the L -
norms . We use 66% of the dataset as the training set , leaving 
the rest for the test set.  
Norm Metrics  Random Forest Accuracy  
Magnet  Squeezer  
L0 0.695  0.733  
L1 0.836  0.832  
L2 0.833  0.823  
Lâˆ 0.761  0.785  
Table 2: Accuracy of the Random Forest  calssifier  trained using a 
single L-norm.  
The results in Table 2 show that the L-norm s have  some 
ability to predict the response of the detector s, obtaining 
0.836 accuracy in the best case . For example, training with 
L2, the Random Forest  can guess with an accuracy of 0.833 
if an image is classified as an attack or normal image by 
MagNet. Similar capabilities are shown for Squeezer with 
an accuracy of 0.823. We obtained the worst results using 
L0 with 0.695 and 0.733 of accuracy for MagNet and 
Squeezer respectively. The results are not surprising since 
L0 measures the number of pixels to which a perturbation 
has been applied but it does not say anything about the per-
turbation intensity. The L âˆ norm is more effective than the 
L0. Also, in th is case, the result is not surprising: the L âˆ norm 
measures the maximum perturbation applied to each pixel 
but does not explain how the perturbation is distributed 
among the pixel s. The L 2 can remain small when there are 
many small changes to many pixels, thus it is more informa-
tive about the overall perturbation.  The L 1 distance has a ca-
pability  comparable to the L2. However, for all the L -norms 
considered , the correlation ability is not surprisingly high.  
 
Image Quality 
Metrics  Random Forest Accuracy  
Magnet  Squeezer  
PSNR -B 0.852  0.882  
VIF 0.834  0.878  
RASE  0.827  0.877  
ERGAS  0.823  0.87 
Table 3: Accuracy of the Random Forrest classifier trained using 
image quality metrics.  
Answer to RQ3. We repeat the same approach of RQ1 and  
RQ2 but  with the image quality metrics.  We identify contra-
dictory examples  like in RQ1  suggest ing that, as for L -
norm s, solely computing quality metrics  is not enough in-
formative.  Individually, the correlation is simi lar to  the best L-norm . In fact, in Table 3 we report the  image quality met-
rics that make the RF reach the  top 4  accurac y scores . Table 
3 shows that  correlation with quality metrics is more evident 
than using the best L -norm s L 1 and L 2. In particular,  PSNR -
B show s a significant  increase in accuracy  with respect to an 
RF using L 1. 
Answer to RQ4. Since each norm gives us different infor-
mation about the p erturbation, we combine them to calculate 
more precise evaluations  of the effectiveness of the attack. 
We repeat the experiment of RQ2 but includin g various 
norms.  
First, w e try all the possible permutations  of the L-norm s 
as features  of the training set.  The RF is then exercised 
against the test set to evaluate its accuracy in guessing the 
response of the detector . Results in Table 4 show that the 
more metrics we include  in our training set, the more we can 
predict the output of the detectors on a given adversarial im-
age, reaching a correlation accuracy above 0.91. 
Table 4: Accuracy of the Random Forest classifier trained com-
bining multiple L-norms.  
Then, we  investigate  image quality metrics. First, w e ex-
ercise  a feature selection method  to understand  which fea-
tures  are the most informative . The importance of the met-
rics as feature s change according to the detector , but MSE 
and PSNR -B are the most  important for both detectors . We 
use the results of the feature selection method to select the 
metrics to combine. The combination we choose and the ir 
results in terms of RF accuracy are in  Table 5. 
The combination of the most important  quality metrics for 
both detectors  shows some improvement in accuracy (from 
0.912 to 0 .929 for Magnet, from 0 .915 to 0 .936 for 
Squeezer) with respect to the best results of Table 4 . Com-
bining all the quality metrics we get even better perfor-
mances. We achieve an accuracy of 0.936 for Magnet and 
0.949 for Squeezer. This suggests  that the image quality 
metrics can be more i nformative on the attacks and their per-
turbation  with respect to the L -norms . 
 
Detector s Image Quality Metrics  Random Forest Ac-
curacy  
Magnet  MSE, SCC, VIF, PSNR -B 0.929  
ALL_QUALITY  0.936  
Squeezer  MSE, ERGAS, SAM, PSNR -B 0.936  
ALL_QUALITY  0.949  
 
Table 5: Accuracy of the Random Forest using the best combina-
tion of four quality metrics and  including all of them 
(ALL_QUALITY).  Norm M etrics  Random Forest Accuracy  
Magnet  Squeezer  
L0, L1, Linf  0.889  0.893  
L1, L2, Linf  0.912  0.913  
L0, L2, Linf  0.889  0.889  
L0, L1, L2, Linf  0.912  0.915  Last, we investigate L-norms and image quality metrics 
together. The good performances of the MSE and  PSNR -B 
metrics are confirmed but, as in the previous case , the most 
relevant metrics for both detectors are different.  In Table 6, 
we show the accuracy achieved by t he RF classifier for the 
4 most  informative metrics with Magnet and Squeezer. Ac-
curacy is, respectively, 0.931, and 0.936. The score for Mag-
net is obtained using quality metrics MSE and PSNR -B 
along with L 2 and L 1. This combination allows obtaining a 
sligh t increase in accuracy than those obtained using the 4 
best quality metrics of Table 4 or the L -norms of Table 5. In 
the case of Squeezer, on the contrary, the most relevant fea-
tures are the same quality metrics of Table 5 (MSE, ERGAS, 
SAM, PSNR -B). In thi s case, therefore, there is no benefit 
from enriching the feature set with the L -norms. The differ-
ence between feature importance  in Magnet and Squeezer 
could be related to the different functional behavior of the 
two detectors.  
Detector s Image Quality Metrics  + L-Norms  Random Forest Ac-
curacy  
Magnet  MSE, PSNR -B, L 2, L1 0.931  
NORM + QUALITY  0.937  
Squeezer  MSE, ERGAS, SAM, PSNR -B 0.936  
NORMS + QUALITY  0.947  
 
 Table 6: Accuracy of the Random Forest using the best combina-
tion of metrics  and combining all the metrics (either L-norms or 
quality metrics)  
Last, we combine  L-norm s and all 8 image quality metrics 
achieving an accuracy of 0 .937 for Magnet and 0 .947 for 
Squeezer  (see ALL_QUALITY in Table 5) , which is com-
parable with the results obtained using only quality metrics. 
Thus, based on the correlation between the detection label 
and image pe rturbation, the image quality metrics can de-
scribe the strength of the attack even better than L -norms.   
Answer to RQ5. We investigate if the response of the de-
tector on unknown attacks  can be predicted using only the 
RF trained on known attacks , i.e., attacks included in the 
training set. If so, it would mean that different attacks intro-
duce alterations described in the same way by image quality 
metrics, which  are detected similarly .  
To achieve this goal, w e rely on a leave -one-out approach 
where we creat e different test sets consisting of a single type 
of attack  and, at the same time, exclude such attack  from the 
training  set. This process is repeated for each of the 10 attack 
types . Table 7 shows the accuracy of the RF against un-
known attacks . 
Results are heterogeneous. Table 7 reports  the attacks that 
resulted in a high accuracy in gray : in this case, the accuracy 
is comparable  to the values in the previous tables. The three 
black box attacks (BOUND, HOP, ZOO) plus three white 
box (ELA, PGD, BIM) have a high correlation.  For example, when BIM  is the unknown attack , the accu-
racy is 1 for both detectors : our RF can perfectly pred ict the 
behavior of the detectors on BIM attacks just by measuring 
the perturbation introduced on known attacks.  This lets us 
suppose that the alteration of the BIM images is similar to 
alterations in the training set .  
This suggests that we are able to id entify groups of attacks 
with similar perturbations and similar strength . In practice, 
this can result useful to evaluate the originality and unique-
ness of attacks  (not in terms of their mathematical formula-
tion, but in the practical modification s to the images ). At the 
same time, this way of reasoning could help reducing the 
attack type s to be tested against a defense : if attacks can be 
grouped based on  image perturbation  and detector outputs , 
only one representative from each group c ould be selected . 
 
Unknown Attack  Magnet  Squeezer  
BIM  1 1 
BOUND  0.874  0.95 
CW2  0.404  0.319  
DEEP  0.662  0.708  
ELA  0.932  0.959  
FGM  0.64 0.682  
HOP  0.942  0.924  
NEW  0.759  0.746  
PGD  0.948  0.756  
ZOO  0.78 0.79 
Table 7: Accuracy of Random Forest on unknown attacks.  
The accuracy is much lower when the unknown attack s 
are FGM, DEEP, and CW2.  The worst results  are obtained 
using  CW2 (accuracy is 0.40 on Magnet and 0.31 on 
Squeezer ). This suggests that the perturbations introduced 
by FGM, DEEP, and CW2 are different from any other at-
tack. Clearly, this consideration is valid under the assump-
tion that the 12 metrics are descriptive enough to capture the 
characteristics of the introduced perturbation.  To support 
our assumption, we remark that we used the most relevant 
image quality metrics from the state-of-the-art and the most 
used L -norms.  
Summarizing , in many cases, the information included in 
the image p rovides an indication of attack detectability. This 
suggests that an RF classifier , trained on multiple attacks as 
we did, could be used to verify the strength of an attack  
against a target defense . In addition, it could be used to clas-
sify attacks based o n the image perturbation (as measured 
by the 12 metrics) and the detectability , to assure that attacks 
from multiple classes are selected.  
Acknowledgment s  
This work has been partially supported by the  NextGen-
erationEU programme, Italian DM737 â€“ CUP 
B15F21005410003.  Takeovers  and Future Works  
We investigated the information contained in L -norms 
and image quality metrics and their p ossible use to support 
the evaluation of adversarial attack defenses.  
We observe that , under several circumstances , image 
quality metrics are more informative than  L-norms . Further, 
we observe a correlation between the detector performance 
and the value s of the identified metric s, to the extent that i) 
they can predict the response of a detector , and ii) they can 
classify attacks based on the perturbations introduced and 
their strength (ability to evade defenses ).  
In fact, we reach ed almost 95% of correlation accuracy  
between the values of the metrics and the detector outputs .  
As future works, we are extending our approach to craft a 
methodology that allows grouping attacks based on the 
measured perturbation by L -norms, quality metri cs, and de-
fense output, such that representative attacks selected from 
the different groups can justifiably cover the attack space at 
the state of the art. In fact, attacks with significantly differ-
ent values have been proven to be poorly correlated i.e., 
leading to very different detection outputs. To such an ex-
tent, t he preliminary results adopted in this work will be ex-
tended by considering additional detectors, classifiers, and 
datasets.  
Also, the RF classifier  could be a handy tool to estimate 
the str ength and novelty (in terms of introduced perturba-
tion) of an attack  toward  a target model . In fact, adversarial 
images could be fed to the trained RF classifier  without the 
need to install defenses (Magnet and Squeezer in our case) . 
The advantage is that the configuration and installation of a 
defense is a difficult task: it must be adapted to the target 
architecture, it requires the installation of many packages, 
and, often, a good deal of critical thinking to optimize the 
choice of parameters.  
Additional ly, in this work , we did not consider the success 
rate of an adversarial attack in evading a classifier. This way 
we will investigate if the identified metrics can also predict 
the success of an attack on the target classifier in addition to 
the detection capability of a defense.  
References  
Carlini, Nicholas, et al. "On evaluating adversarial robustness." 
arXiv:1902.06705 (2019).  
N. Carlini, and D. Wagner, "Towards evaluating the robustness of 
neural networks," IEEE symposium on security and privacy (SP), 
pp. 39 -57, IEEE, 2017.  
S.M. Moosavi -Dezfooli, A. Fawzi, and P.Frossard, "Depfool: a 
simple and accurate method to fool deep neural networks," Pro-
ceedings of the IEEE conference on computer vision and pattern 
recognition, pp. 2574 -2582, 2016.  
I. Goodfellow , J. Shlens, and C. Szegedy, "Explaining and harness-
ing adversarial examples," arXiv:1412.6572, 2014.  W. Xu, D. Evans, and Y. Qi, "Feature Squeezing: Detecting Ad-
versarial Examples in Deep Neural Networks,"  NDSS S, 2018.  
D. Meng, and H. Chen, "Magnet: a two -pronged defense against 
adversarial examples," ACM SIGSAC , 2017.  
A. Krizhevsky, and G. Hinton, "Learning multiple layers of fea-
tures from tiny images," Technical Report, 2009.  
M. I. Nicolae, et al., "Adversarial Robus tness Toolbox v1.0.0,"  
arXiv:1807.01069v4, 2019 . 
Yim, Changhoon, and Alan Conrad Bovik. "Quality assessment of 
deblocked images." IEEE Transactions on Image Processing 20.1 
(2010): 88 -98. 
T. Ranchin and L. Wald, "Fusion of high spatial and spectral reso-
lution images: The ARSIS concept and its implementation", Pho-
togramm. Eng. Remote Sens., vol. 66, pp. 49 -61, Jan. 2000.  
Wang, Zhou, and Alan C. Bovik. "A universal image quality in-
dex." IEEE signal processing letters 9.3 (2002): 81 -84. 
Wald, Lucien. "Quality of high-resolution  synthesised images: Is 
there a simple criterion? " SEE/URISCA, 2000.  
Yuhas , Roberta H., Alexander FH Goetz, and Joe W. Boardman. 
"Discrimination among semi -arid landscape endmembers using the 
spectral angle mapper (SAM) algorithm." Summaries of the 3rd 
Annual JPL Airborne Geoscience Workshop. Volume 1: AVIRIS 
Workshop. 1992.  
Chang, Hua -wen, and Ming -hui Wang. "Sparse correlation coeffi-
cient for objective image quality assessment." Signal processing: 
Image communication 26.10 (2011): 577 -588. 
Sheikh, Hamid R., and Alan C. Bovik. "Image information and vis-
ual quality." IEEE Transac tions on image processing 15.2 (2006) .  
GonzÃ¡lez -AudÃ­cana, MarÃ­a, et al. "Fusion of multispectral and pan-
chromatic images using improved IHS and PCA mergers based on 
wavelet decomposition." IEEE Transactions on Geoscience and 
Remote sensing 42.6 (2 004): 1291 -1299.  
Madry, Aleksander, et al. "Towards deep learning models resistant 
to adversarial attacks." arXiv:  1706.06083 (2017).  
Jang, Uyeong, Xi Wu, and Somesh Jha. "Objective metrics and 
gradient descent algorithms for adversarial examples in machi ne 
learning." Computer Security Applications Conference. 2017.  
Chen, Pin -Yu, et al. "Ead: elastic -net attacks to deep neural net-
works via adversarial examples." AAAI conference on artificial in-
telligence. Vol. 32. No. 1. 2018.  
Chen, Jianbo, Michael I. Jord an, and Martin J. Wainwright. "Hop-
skipjumpattack: A query -efficient decision -based attack." IEEE  
symposium on security and privacy (sp). IEEE, 2020.  
Chen, Pin -Yu, et al. "Zoo: Zeroth order optimization based black -
box attacks to deep neural networks withou t training substitute 
models." Proceedings of the 10th ACM workshop on artificial in-
telligence and security. 2017.  
Brendel, Wieland, Jonas Rauber, and Matthias Bethge. "Decision -
based adversarial attacks: Reliable attacks against black -box ma-
chine learning  models." arXiv:1712.04248 (2017).  
A. Kurakin, I. Goodfellow, and S. Bengio, "Adversarial examples 
in the physical world," In: Artificial intelligence safety and secu-
rity. Chapman and Hall/CRC, 2018.  
Zhou, Jie, Daniel L. Civco, and J. A. Silander. "A wave let trans-
form method to merge Landsat TM and SPOT panchromatic data." 
International journal of remote sensing 19.4 (1998) . 
Ma, Xingjun, et al. "Characterizing adversarial subspaces using lo-
cal intrinsic dimensionality." arXiv:1801.02613 (2018).  