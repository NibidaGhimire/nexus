JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 1
Fast-SNN: Fast Spiking Neural Network by
Converting Quantized ANN
Y angfan Hu, Qian Zheng, Member, IEEE , Xudong Jiang, Fellow, IEEE , and Gang Pan, Member, IEEE
Abstract —Spiking neural networks (SNNs) have shown advantages in computation and energy efficiency over traditional artificial
neural networks (ANNs) thanks to their event-driven representations. SNNs also replace weight multiplications in ANNs with additions,
which are more energy-efficient and less computationally intensive. However, it remains a challenge to train deep SNNs due to the
discrete spike function. A popular approach to circumvent this challenge is ANN-to-SNN conversion. However, due to the quantization
error and accumulating error, it often requires lots of time steps (high inference latency) to achieve high performance, which negates
SNN’s advantages. To this end, this paper proposes Fast-SNN that achieves high performance with low latency. We demonstrate the
equivalent mapping between temporal quantization in SNNs and spatial quantization in ANNs, based on which the minimization of the
quantization error is transferred to quantized ANN training. With the minimization of the quantization error, we show that the sequential
error is the primary cause of the accumulating error, which is addressed by introducing a signed IF neuron model and a layer-wise
fine-tuning mechanism. Our method achieves state-of-the-art performance and low latency on various computer vision tasks, including
image classification, object detection, and semantic segmentation. Codes are available at: https://github.com/yangfan-hu/Fast-SNN.
Index Terms —Deep Spiking Neural Networks, Neuromoprhic Computing, ANN-to-SNN Conversion, Object Detection, Semantic
Segmentation
✦
1 I NTRODUCTION
OVER the last decade, deep artificial neural networks
(ANNs) have made tremendous progress in various
applications, including computer vision, natural language
processing, speech recognition, etc. However, due to the in-
creasing complexity in models and datasets, state-of-the-art
ANNs require heavy memory and computational resources
[1]. This situation prohibits the deployment of deep ANNs
on resource-constrained environments (e.g., embedded sys-
tems or mobile devices). In contrast, the human brain can
efficiently perform complex perceptual and cognitive tasks
with a budget of approximately 20 watts [2]. Its remarkable
capacities may be attributed to spike-based temporal pro-
cessing that enables sparse and efficient information transfer
in networks of biological neurons [2]. Inspired by biological
neural networks, Maass [3] proposed a new class of neural
networks, the spiking neural networks (SNNs). SNNs ex-
change information via spikes (binary events that are either
0 or 1) instead of continuous activation values in ANNs. An
SNN unit (spiking neuron) only activates when it receives or
emits a spike and remains dormant otherwise. Such event-
driven, asynchronous characteristics of SNNs reduce energy
consumption over time. In addition, SNNs use accumulate
(AC) operations that are much less costly than the multiply-
and-accumulate (MAC) operations in state-of-the-art deep
ANNs. In the community of neuromorphic computing,
researchers are developing neuromorphic computing plat-
forms (e.g., TrueNorth [4], Loihi [5]) for SNN applications.
•Y. Hu, Q. Zheng, G. Pan are with College of Computer Science and
Technology, Zhejiang University, Hangzhou 310027, China.
E-mail:huyangfan@zju.edu.cn; qianzheng@zju.edu.cn; gpan@zju.edu.cn
•X. Jiang is with School of Electrical and Electronic Engineering, Nanyang
Technological University, Singapore 639798, Singapore.
E-mail:exdjiang@ntu.edu.sg
Manuscript receivedThese platforms, aiming at alleviating the von Neumann
bottleneck with co-located memory and computation units,
can perform SNN inference with low power consumption.
Moreover, SNNs are inherently compatible with emerging
event-based sensors (e.g., the dynamic vision sensor (DVS)
[6]).
However, the lack of efficient training algorithms ob-
structs deploying SNNs in real-time applications. Due to the
discontinuous functionality of spiking neurons, gradient-
descent backpropagation algorithms that have achieved
great success in ANNs are not directly applicable to SNNs.
Recently, researchers have made notable progress in training
SNNs directly with backpropagation algorithms. They over-
come the non-differentiability of the spike function by using
surrogate gradients [7], [8], [9], [10], [11]. Then they apply
backpropagation through time (BPTT) to optimize SNNs in
a way similar to the backpropagation in ANNs. However,
due to the sparsity of spike trains, directly training SNNs
with BPTT is inefficient in both computation and memory
with prevalent computing devices (e.g., GPUs) [7], [12].
Furthermore, the surrogate gradients would cause the van-
ishing or exploding gradient problem for deep networks,
making direct training methods less effective for tasks of
high complexity [12].
In contrast, rate-coded ANN-to-SNN conversion algo-
rithms [12], [13], [14], [15], [16], [17], [18], [19], [20], [21],
[22], [23] employ the same training procedures as ANNs,
which benefit from the efficient computation of ANN train-
ing algorithms. Besides, by approximating the ANN ac-
tivations with SNN firing rates, ANN-to-SNN conversion
algorithms have achieved promising performance of SNNs
on challenging tasks, including image classification [16]
and object detection [19]. Nevertheless, all existing methods
suffer from quantization error and accumulating error [16]arXiv:2305.19868v1  [cs.NE]  31 May 2023JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 2
(see Section 2.2 for further discussion), resulting in perfor-
mance degradation during conversion, especially when the
latency is short. Although we can significantly mitigate the
quantization error with an increasing number of discrete
states (higher inference latency), it will unfavorably reduce
the computation/energy efficiency of real-time applications.
The growing inference latency will proportionally increase
the number of operations and actual running time for a
deployed SNN.
To this end, this paper aims to build a Fast-SNN with
competitive performance (i.e., comparable with ANNs) and
low inference latency (i.e., 3, 7, 15) to conserve the SNN’s
advantages. Our basic idea is to reduce the quantization
error and the accumulating error. The main contributions
are:
•Quantization error minimization. We show the
equivalent mapping between temporal quantization
in SNNs and spatial quantization in ANNs. Based on
this mapping, we demonstrate that the quantization
error could be minimized by the supervised training
of quantized ANNs, which facilitates ANN-to-SNN
conversion by finding the optimal clipping range and
the novel distributions of weights and activations for
each layer. Besides, we derivate the upper bound of
inference latency that ensures the lossless conversion
from quantized ANNs to SNNs.
•Sequential error minimization. To further boost the
speed of SNNs and reduce the inference latency, we
minimize the accumulating error. We show that the
sequential error at each layer is the primary cause of
the accumulating error when converting a quantized
ANN to an SNN. Based on this observation, we pro-
pose a signed IF neuron to mitigate the impact from
wrongly fired spikes to address the sequential error
at each layer and propose a layer-wise fine-tuning
mechanism to alleviate the accumulating sequential
error across layers.
•Deep models for various computer vision tasks.
Our method provides a promising solution to con-
vert deep ANN models to SNN counterparts. It
achieves state-of-the-art performance and low la-
tency on various computer vision tasks, including
image classification (accuracy: 71.31%, time steps:
3, ImageNet), object detection (mAP: 73.43%, time
steps: 7, PASCAL VOC 2007), and semantic segmen-
tation (mIoU: 69.7%, time steps: 15, PASCAL VOC
2012).
2 R ELATED WORK
We present recent advances of two promising routes to build
SNNs: direct training with surrogate gradients and ANN-to-
SNN conversion.
2.1 Direct Training with Surrogate Gradients
Direct training methods view SNNs as special RNNs [24]
and employ BPTT to backward gradients through time.
To address the discontinuous, non-differentiable nature of
SNNs, researchers employ surrogate gradients [7], [8], [25],
[26] to approximate the derivative of the spiking function,which is a Dirac function. In [9], Wu et al. first proposed
a spatio-temporal backpropagation (STBP) framework to
simultaneously consider the spatial and timing-dependent
temporal domain during network training. Wu et al. further
proposed to enhance STBP with a neuron normalization
technique [27]. In [10], Gu et al. proposed spatio-temporal
credit assignment (STCA) for BPTT with a temporal based
loss function. To develop a batch normalization method
customized for BPTT, Kim et al. [28] proposed a batch
normalize through time (BNTT) technique. Similarly, Zheng
et al. [29] proposed a threshold-dependent batch normaliza-
tion (tdBN) for STBP . In [30], Zhang et al. proposed TSSL-
BP to break down error backpropagation across two types
of inter-neuron and intra-neuron dependencies, achieving
low-latency SNNs. In [31], Rathi et al. proposed to opti-
mize the leakage and threshold in the LIF neuron model.
In [32], Kim et al. proposed a Neural Architecture Search
(NAS) approach to find better SNN architectures. For other
perspectives on direct training methods, readers can refer to
[2], [33], [34], [35].
With these emerging techniques, direct training meth-
ods can build SNNs low latency and high accuracy [30],
[31], [36]. Various SNN applications with direct training
methods have also emerged [37], [38], [39], [40], [41], [42],
[43]. However, due to the sparsity of spike trains, directly
training SNNs with BPTT is inefficient in both computa-
tion and memory with prevalent computing devices (e.g.,
GPUs) [7], [12]. An SNN with Ttime steps propagates
Ttimes iteratively during forward and backward propa-
gation. Compared with a full-precision ANN, it consumes
more memory and requires about Ttimes more compu-
tation time. Furthermore, the surrogate gradients would
cause the vanishing or exploding gradient problem for deep
networks, making direct training methods less effective for
tasks of high complexity [12]. For these reasons, we focus
on ANN-to-SNN conversion methods for building SNNs. It
is worth noting that SNNs with T= 1 can be viewed as a
special case of binary neural networks (BNNs) [1], making
direct training and ANN-to-SNN conversion methods inter-
changeable. However, these SNNs suffer from a significant
performance drop compared with their corresponding full-
precision ANNs.
2.2 ANN-to-SNN Conversion
The first ANN-to-SNN conversion method by P ´erez-
Carrasco et al. [13] maps ANNs of sigmoid neurons into
SNNs of leaky integrate-and-fire (LIF) neurons by scaling
weights of pre-trained ANNs. The scaling factor is deter-
mined by neuron parameters (e.g., threshold, leak rate, per-
sistence). Although this method achieves promising results
on two DVS tasks (human silhouette orientation and poker
card symbol recognition), it does no apply to other tasks
as its hyperparameters (neuron parameters) are determined
manually. In [14], Cao et al. demonstrated that the rectified
linear unit (ReLU) function is functionally equivalent to
integrate-and-fire (IF) neuron, i.e., LIF neuron with no leaky
factor nor refractory period. With only one hyperparameter:
the firing threshold of spiking neurons, SNNs of IF neurons
can approximate ANNs with ReLU activation, no bias term,
and average pooling. With the great success of ReLU inJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 3
TABLE 1
A summary of recent ANN-to-SNN conversion methods with respect to the minimization of the quantization error and accumulating error.
MethodQuantization Error Accumulating Error Time Step
Clipping
Range
OptimizationDistribution of
Weights/Activations
OptimizationError Types MinimizationRequired for
SOTA
Performance
Sengupta et al. 2019 [18]
Statistical Analysis N/A Quantization & Sequential N/A ≥2048 Kim et al. 2020 [19]
Han et al. 2020 [20]
Yousefzadeh et al. 2019 [44] N/A N/A Quantization & Sequential N/A ≥800
Hu et al. 2018 [17]Statistical Analysis N/A Quantization & Sequential N/A ≥16Deng et al. 2021 [21]
Yan et al. 2021 [22]
Statistical Analysis Local Optimum Quantization & Sequential Fine-tuning ≥16 Li et al. 2021 [23]
Wu et al. 2021 [12]
Zou et al. 2020 [45] Fixed Value Local Optimum Sequential N/A ≥4
Ours Learning Based Global Optimum Sequential Fine-tuning 3
ANNs, most existing ANN-to-SNN conversion methods
follow the framework in [14].
With the framework in [14], the quantization error [16],
usually observed as the over-/under-activation of spiking
neurons compared with ANN activations [15], is recognized
as the primary factor that obstructs lossless ANN-to-SNN
conversion. In an SNN, the spiking functionality inherently
quantizes the inputs (clipping the inputs to a range rep-
resented by temporally discrete states) and introduces the
quantization error at each layer. To mitigate the quantization
error, researchers proposed various normalization methods.
Diehl et al. [15] proposed to scale the weights by the maxi-
mum possible activation. In [16], Rueckauer et al. improved
the weight normalization [15] by using the 99.9th percentile
of activation instead of the maximum. In [19], Kim et al.
proposed to apply channel-wise weight normalization to
eliminate extremely small activations. In [18], Sengupta et
al. proposed threshold balancing, an equivalent alternative
to weight normalization, to dynamically normalize SNNs at
run time. Following [18], Han et al. [20] proposed to scale
the threshold by the fan-in and fan-out of the IF neuron.
However, these methods only optimize the clipping range
based on statistical analysis, leaving the distribution of
weights/activations not optimized. Different from the above
methods, Yousefzadeh et al. [44] proposed to address the
over-activation problem with a signed neuron model that
adapts the firing rate based on the total membrane charges,
with a positive/negative firing threshold of +1/-1. How-
ever, it also leaves the distribution of weights/activations
not optimized. In addition, as analyzed in [16], the quanti-
zation error at each layer will contribute to the accumulating
error that severely distorts the approximation between ANN
activations and SNN firing rates at deep layers. Therefore,
these methods typically employ shallow architectures and
require a latency of hundreds or even thousands of time
steps to achieve high accuracy.
To address the accumulating error, researchers proposed
to train a fitting ANN and apply statistical post-processing.
In [17], Hu et al. employed ResNet, a robust deep architec-
ture, for conversion. They devised a systematic approach
to convert residual connections. They reported that the ob-
served accumulating error in a residual architecture is lower
than that in a plain architecture. Furthermore, they proposed
to counter the accumulating error by increasing the firingrate of neurons at deeper layers based on the statistically
estimated error. In [21], Deng et al. proposed to train ANNs
using a capped ReLU, i.e., ReLU1 and ReLU2. Then they
applied a scaling factor to normalize the firing thresholds
by the maximum activation of capped ReLU function.
Different statistical post-processing, an approach
that employs ANNs to adjust the distribution of
weights/activations emerges. In [45], Zou et al. proposed
to employ a quantization function with fixed steps during
ANN training to improve the mapping from ANNs to
SNNs. In [22], Yan et al. proposed a framework to adjust
the pre-trained ANNs with the knowledge of temporal
quantization in SNNs. They introduced a residual term
in ANNs to emulate the residual membrane potential in
SNNs and reduce the quantization error. In [23], Li et al.
introduced layer-wise calibration to optimize the weights
of SNNs. In [12], Wu et al. proposed a hybrid framework
called progressive tandem learning to fine-tune the full-
precision floating-point ANNs with the knowledge of
temporal quantization. This framework achieves promising
results with a latency of 16. However, it still significantly
suffers from quantization error when the latency is low,
e.g., 3 (see our results in Section).
In Table 1, we summarize how each SNN method ad-
dresses the quantization error and accumulating error. By
transferring the minimization of the quantization error to
quantized ANN training, our method is the first to employ
supervised training to optimize both the clipping range
and distribution of weights/activations. It facilitates ANN-
to-SNN conversion with a learnable clipping range and a
global optimum1for the distribution of weights/activations.
In contrast, methods such as [12], [22], [23] can only achieve
a local optimum as they first train a full-precision floating-
point (FP32) model and then apply fine-tuning to the FP32
model. As for [45], it can only achieve a local optimum
as it does not optimize the clipping range during training.
Thanks to transferring the quantization error to ANN train-
ing, our method simplifies the complexity of the accumulat-
ing error as it contains only the sequential error. With this
1. ANN quantization training naturally encourages or constraints
weights/activations to gather around the target quantized values dis-
tributed in an optimized range [46], [47], [48]. We consider their dis-
tributions globally/locally optimum if they are from an ANN trained
with/without such a constraint.JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 4
promising solution, we further explore various computer
vision tasks (including image classification, object detection,
and semantic segmentation) in contrast to previous SNN
methods that primarily focus on image classification.
3 F AST-SNN
To build Fast-SNNs, we first analyze the equivalent map-
ping between spatial quantization (mapping inputs to a set
of discrete finite values) in ANNs and temporal quantiza-
tion (mapping inputs to a set of temporal events/spikes)
in SNNs. This analysis reveals the activation equivalence
between ANNs and SNNs, based on which we transfer
the minimization of the quantization error to quantized
ANN training. With the minimized quantization error, the
accumulating sequential error, i.e., the combination of accu-
mulating error (an error caused by the mismatch in previous
layers and grows with network propagation) and sequential
error (an error caused by the sequence and firing mechanism
of spikes), becomes the primary factor that degrades SNN
performance. To mitigate the sequential error at each layer,
we introduce a signed IF neuron model. To alleviate the
accumulating error across layers, we introduce a layer-wise
fine-tuning mechanism to minimize the difference between
SNN firing rates and ANN activations. Fig. 1 illustrates the
overview of our framework.
3.1 Activation Equivalence between ANNs and SNNs
For a rate-coded spiking neuron iat layer l, its spike count
(number of output spikes) Nl
i∈ {0,1, . . . , T }, where T
is the length of spike trains. The spike count has T+ 1
discrete states (values). For a b-bit unsigned integer, it has
2bdiscrete states (values):0,1, . . . ,2b−1	
. Our basic idea
of the conversion from ANNs to SNNs is to map the integer
activation of quantizaed ANNs0,1, . . . ,2b−1	
to spike
count {0,1, . . . , T }, i.e., set Tto2b−1.
In the ANN domain, building ANNs with integer activa-
tions is naturally equivalent to compressing activations with
the uniform quantization function that outputs uniformly
distributed values. Such a function spatially discretizes a
full-precision activation xl
iof neuron iat layer lin an ANN
of ReLU activation into:
Ql
i=sl
2b−1clip(round ((2b−1)xl
i
sl),0,2b−1), (1)
where Ql
idenotes the spatially quantized value, bdenotes
the number of bits (precision), the number of states is
2b−1,round (·)denotes a rounding operator, sldenotes
the clipping threshold that determines the clipping range
of input xl
i,clip(x, min, max )is a clipping operator that
saturates xwithin range [min, max ].
In the SNN domain, we consider the SNN model defined
in [16] to facilitate conversion. This model employs direct
coding (direct currents at the first layer) and IF (no leakage)
neurons with reset-by-subtraction. At time step t, the total
membrane charge zl
i(t)for neuron iat layer lis formulated
as:
zl
i(t) =Ml−1X
j=1Wl
ijSl−1
j(t) +bl
i, (2)
Training Data
Layer -wise 
Fine-tuning
Module
Minimizing the 
Accumulating
Sequential ErrorQuantized ANN
Fast-SNNSingle ANN Layer
Single SNN Layer
Spatial 
QuantizationMinimizing the Quantization Error
Temporal 
Quantization
Signed IF 
Neuron Model  Activation
Equivalence 
between 
ANNs and SNNs
Wrongly Fired Spike Negative Spike SpikeFig. 1. Overview of our conversion framework. We first minimize the
quantization error during quantized ANN training. Then we provide an
equivalence mapping between quantized ANN values and SNN firing
rates. Finally, we address the accumulating error with a signed IF neuron
model and a layer-wise fine-tuning module. Details of the layer-wise fine-
tuning module can be found in Fig. 5.
where Ml−1is the set of neurons at layer l−1,Wl
ijis
the weight of synaptic connection between neuron iand
j,bl
iis the bias term indicates a constant injecting current,
Sl−1
j(t)indicates an input spike from neuron jat time t. The
membrane equation of IF neuron is then defined as follows:
Vl
i(t) =Vl
i(t−1) +zl
i(t)−θlΘ(Vl
i(t)−θl), (3)
where θldenotes the firing threshold, tdenotes the t-th time
step, Θis a step function defined as:
Θ(x) =(
1ifx≥0,
0otherwise.(4)
Given a spike train of length T, the total input membrane
charge over the whole time window exl
iis defined as:
exl
i=µl+TX
t=1zl
i(t), (5)
where µlis the initial membrane charge. The spiking func-
tionality of IF neurons inherently quantizes exl
iinto a quan-
tized value represented by the firing rate rl
i(t):
eQl
i=rl
i(t) =Nl
i
T=1
Tclip(floor (exl
i
θl),0, T), (6)
where eQl
idenotes the temporally quantized value, floor (·)
denotes a flooring operator. Since the 1st spiking layer (i.e.,
l= 1) receives direct currents as inputs, we have
ex1
i=Tx1
i. (7)
Comparing Eq. 6 with Eq. 1, we let µl=θl/2,T= 2b−1,
θl=sl. Since a flooring operator can be converted to a
rounding operator
floor (x+ 0.5) = round (x), (8)JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 5
IF
IF
IF
IF     
       
       
        
           
     
    1/3
    2/3
       
  3   2      3   2      3   2      3   2    
Fig. 2. An example of how quantized values in ANNs are mapped to
firing rates in SNNs. For a 2-bit ( b= 2) quantizer from Eq. 1 with
sl= 1, it has four possible quantized values (e.g., 0, 1/3, 2/3, 3/3). Each
quantized value is mapped to the spiking count (capped at T= 2b−1)
in SNNs, resulting in four corresponding firing rates (e.g., 0, 1/3, 2/3,
3/3).
we rewrite Eq. 6 to
eQl
i=1
Tclip(floor (Txl
i
θl),0, T) =Ql
i
sl. (9)
Then we scale the weights in the following layer to slWl+1,
making an output spike equivalent to a continuous output
of value sl. We can rewrite the effectively quantized value
from Eq. 9 to:
eQl
i=sl
Tclip(floor (Txl
i
θl),0, T) =Ql
i. (10)
In Eq. 10, our equivalent mapping between spatial quan-
tization in ANNs and temporal quantization in SNNs de-
rives the activation equivalence between ANNs and SNNs.
In Fig. 2, we present an example illustrating how quantized
values in ANNs are mapped to firing rates in SNNs.
Latency upper bound derivation. According to the
equivalence between quantized ANNs and SNNs shown in
Eq. 10, the latency upper bound could be computed by forc-
ing spiking neurons to start firing after receiving all possible
spikes. In such a case, the sequential error of converting a
quantized ANN to a rate-coded SNN is eliminated, and the
firing rates in converted SNNs are identical to activations in
quantized ANNs:
Latency =T×L= (2b−1)×L. (11)
According to the latency bound in Eq. 11, bandLare two
factors that obstruct the fast inference of SNNs. To fully real-
ize our Fast-SNN while maintaining network performance,
we explore the minimization of bin Section 3.2, and reduce
the impact from deep models (i.e., with a large L) in Section
3.3.
3.2 Quantization Error Minimization
In Eq. 11, the bit-precision of quantized ANNs determines
T, the length of spike trains, and the latency. Since the num-
ber of operations in SNNs grows proportionally to T, a high
bit-precision (e.g., b= 16 ,T= 65535 ) of quantized ANNs
will negate SNN’s advantages in computation/energy ef-
ficiency. The solution to reducing Twhile retaining highaccuracy is training low-precision networks (e.g., b= 2 ,
T= 3) with minimized quantization error for conversion.
Advances in ANN quantization [1] have shown that low-
precision models can reach full-precision baseline accuracy
with quantization-aware training [46], [47], [48]. With Eq.
10, we then equivalently convert the quantized ANNs to
SNNs, eliminating the quantization error during conver-
sion. Therefore, our framework fully inherits the advantages
of quantization-aware training, minimizing the quantiza-
tion error during training. On the one hand, we optimize
network parameters with the knowledge of quantization,
which minimizes the mismatch between the distribution of
weights/activations and that of discrete quantization states
[46], [47], [48]. On the other hand, we optimize the clipping
range for quantization by learning a clipping threshold
during supervised training.
Our scheme of a learnable clipping range for quantiza-
tion is quite different from existing SNN methods that clip
the inputs to a range determined by a pre-defined scaling
factor λ, e.g., finding the 99th or 99.9th percentile of all
ReLU activation values [12], [16], [17], [19]. Our method
determines the clipping range using supervised training
instead of statistical post-processing. Such difference brings
three advantages:
•Effectiveness. Compared with a clipping thresh-
old optimized during quantization-aware training in
ANNs, λdetermined by the 99th or 99.9th percentile
of all ReLU activation is not optimal.
•Efficiency. For each trained model, the normaliza-
tion approach additionally calculates both the 99th
and 99.9th percentile of ReLU activations for each
ANN layer, respectively. Then it chooses the per-
centile (99th or 99.9th) that yields higher inference
performance. The additional calculation would be
notoriously costly if λis calculated from the whole
training set.
•Stability. For a challenging dataset such as Ima-
geNet, it is impossible to calculate λfrom the whole
training set. A solution is calculating λfrom a batch
instead of the entire training set. However, the value
ofλwill vary from batch to batch, and the perfor-
mance of converted SNNs will fluctuate accordingly.
In Fig. 3, we present the distributions of all ReLU acti-
vations for all layers of a pre-trained 2-bit AlexNet with the
first batch of the original training set. As can be observed,
the clipping threshold between our method and others is
quite different. Our optimal clipping range brings a signifi-
cant performance advantage, as shown in Section 4.3.
3.3 Accumulating Error Minimization
According to Eq. 11, we can achieve lossless conversion
from quantized ANNs to SNNs by enforcing a waiting
period at each layer (spiking neurons can start firing after
receiving all possible spikes for each layer). However, this
scheme will prevent the practicability of our method on
deep neural networks (i.e., when Lis large). A latency of
T×Lgrows proportionally to the depth of the employed
network, resulting in longer running time and potentially
lower computation/energy efficiency in real-time applica-
tions (see our discussion in Section 4.6). If we reduce theJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 6
Fig. 3. Distributions of the activations of a pre-trained 2-bit AlexNet
using the first batch of the original training set. The batch size is set
to 128. Solid lines denote our clipping thresholds. Values of the clipping
threshold, max activation [15], 99th/99.9th percentile [16] of activations
for each layer are listed within the table. Note that the max activation,
99th/99.9th percentile of activations are determined statistically from the
same batch.
latency by removing the waiting period, it introduces the
sequential error that degrades the performance of converted
deep SNNs.
In Fig. 4, we demonstrate a case illustrating the cause
and impact of the sequential error. Previous works of rate-
coded SNNs (e.g., [16], [19], [20]) seldom consider the se-
quential error because it has little impact on performance
when Tis long enough (i.e., hundreds or thousands of time
steps). That is,
Nl
i
T≈Nl
i+ 1
T, (12)
where Nl
iis the number of output spikes. However, when
we reduce the latency Tto several time steps, the sequential
error will significantly distort the approximation between
ANN activations and SNN firing rates. Furthermore, the
sequential error at each layer accumulates as the network
propagates, causing significant deviation at deep layers.
Signed IF neuron. To address the sequential error at
each layer, we propose to cancel the wrongly fired spikes
by introducing a signed IF neuron model. As for possible
hardware implementation, neuromorphic hardware such as
Loihi [5] already supports signed spikes. In our signed IF
neuron model, a neuron can only fire a negative spike if it
reaches the negative firing threshold and has fired at least
one positive spike. To restore the wrongly subtracted mem-
brane potential, our model changes the reset mechanism for
negative spikes to reset by adding the positive threshold
(i.e.,θ). Then we rewrite the spiking function Θ(t)to
Θ(t) =

1 ifVl
i(t)≥θ,
−1ifVl
i(t)≤θ′andNl
i(t)≥1,
0 otherwise, no firing,(13)
where θ′is the negative threshold, Nl
i(t)is the number
of spikes neuron ihas fired at time step t. To boost the
sensitivity of negative spikes, we set the negative threshold
        2
   -2
            2
   -1
     
        2
   -1
       
        2
   -1IF
IF
SIF

(a)
(b)
(c)
(d)  3   2      3   2      3   2    
  3   2      3   2      3   2      3   2      3   2      3   2        
         
       
         
    
        1
       
        
    
         
        
       Fig. 4. An example of how the sequence of spikes causes the sequential
error. For simplicity, we set the firing threshold in SNNs to 1. SIF denotes
our signed IF neuron. For each SNN neuron, we use a table to display
the values of z,Θ,Vfor each time step t. (a) An ANN neuron receives
two inputs: 2 and -2. Its output activation is 0. (b) An IF neuron receives
three spike charges (-1, -1, 2) at t= 1,2,3. Its output firing rate is
equivalent to the ANN activation. (c) An IF neuron receives three spike
charges (2, -1, -1) at t= 1,2,3. However, it instantly fires a spike at
t= 1since the membrane potential is greater than the firing threshold
and outputs no events when t= 2,3, resulting in a firing rate that is
not equivalent to the ANN activation. (d) An SIF neuron receives three
spike charges (2, -1, -1) at t= 1,2,3. Although it fires a spike at t= 1,
our SIF model outputs no events when t= 2 as the incoming current
cancels the residual membrane potential and fires a negative spike at
t= 3, resulting in a firing rate that is equivalent to the ANN activation.
to a small negative value (empirically -1e-3). We then rewrite
the membrane dynamic of IF neuron in Eq.3 to:
Vl
i(t) =Vl
i(t−1) +zl
i(t)−θlΘ(Vl
i(t)−θl)
+θlΘ(θ′−Vl
i(t))Θ(Nl
i−1).(14)
With Eq.13 and Eq.14, the IF neuron will fire a negative spike
to cancel a wrongly fired spike and restore the membrane
potential. Compared with our signed IF neuron model, the
signed IF neuron model proposed by Kim et al. [19] does
not apply to our problem. It is designed to approximate
negative outputs of the leaky ReLU function in ANNs and
takes no consideration of the sequence of spikes in SNNs.
In [44], Yousefzadeh proposed a signed neuron model with
a fixed positive/negative firing threshold of +1/-1, making
it not sensitive to the sequential error.
Layer-wise fine-tuning scheme. Although the modified
neuron model narrows the sequential error at each layer, the
accumulating error still distorts the SNN firing rates at deep
layers and degrades network performance. According to our
analysis, the firing rate maps in an SNN with a latency of
T×Lare identical to feature maps in quantized ANNs,
indicating that the accumulating error in our framework
contains only the accumulating sequential error. With thisJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 7
ANN SNNProxy 
ANN
Layer 1
    Layer 2
Layer 3Layer 1 Layer 1
Layer 2 Layer 2
Layer 3 Layer 3     
    Forward PassLayer -wise
Euclidean Loss
Backward Pass Weight Sharing
Fig. 5. Layer-wise fine-tuning module. First, we obtain an SNN from a
quantized ANN with Llayers. Then we build a proxy ANN that shares its
parameters with the SNN. Starting from the 2nd layer (no sequential
error in the 1st layer), layer lof the proxy ANN receives the output
map of firing rates from layer l−1of the SNN as its input. Its output
is set to the map of firing rates from layer lof the SNN. We calculate the
Euclidean loss between the output of layer lin the proxy ANN and the
reference (activations of layer lin the quantized ANN). We then minimize
the Euclidean loss by optimizing the parameters (weights and biases) of
layer lin the proxy ANN. The updated parameters in the proxy ANN are
mapped back to the corresponding SNN layer. We repeat this process
until reaching the final classification layer. We bypass the last layer as
we directly use its membrane potential for classification. Please refer to
Algorithm 1 for more details.
Algorithm 1 Minimizing the Accumulating Error
Input : ANN A, training set X, latency T, network depth L
Output : SNN model S
1://Network Initialization
2:S.load state(A.state dict())
3:forlayer l =2toL−1do
4: //Initialize a proxy ANN from lth layer of A
5:Ap=deepcopy (A.l)
6: forxiinXdo
7: A(xi)//forward pre-trained ANN
8: fort=1toTdo
9: S(xi)//forward SNN
10: end for
11: ref =getactivations (A, l)//reference
12: input =getfiring rates (S, l−1)//get inputs
13: output =Ap(input )//forward proxy ANN
14: output =getfiring rate(S, l)//update outputs
15: loss =L2Loss(ref, output )
16: optimizer step on loss
17: S.l.load state(Ap.state dict())//update params
18: end for
19:end for
20:return S
insight, we propose to minimize the accumulating error by
minimizing the Euclidean distance between ANN activa-
tions (free from the sequential or accumulating error) and
SNN firing rates at each layer with a framework illustrated
in Fig. 5. To overcome the discontinuity in SNNs, we employ
a proxy ANN that shares its parameters with the SNN
to optimize network parameters (weights and biases). We
present the pseudo-codes of our proposed layer-wise fine-
tuning method in Algorithm 1. Compared with other fine-TABLE 2
Accuracy (%) of ANNs trained with different quantization precision on
CIFAR-10. We denote the bit-precision of weights and activations by
“W” and “A”, respectively. For the details of employed architectures,
please refer to Section 4.1.
NetworkPrecision
W/AANN
Acc.Precision
W/AANN
Acc.Precision
W/AANN
Acc.
AlexNet 32/32 92.87 3/3 92.54 2/2 91.52
VGG-11 32/32 93.60 3/3 93.71 2/2 93.06
Resnet-20 32/32 93.00 3/3 92.39 2/2 90.39
Resnet-44 32/32 94.17 3/3 93.41 2/2 91.57
Resnet-56 32/32 94.10 3/3 93.66 2/2 91.68
Resnet-18 32/32 95.85 32/3 95.62 32/2 95.51
TABLE 3
Accuracy (%) of ANNs trained with different quantization precision on
ImageNet. We denote the bit-precision of weights and activations by
“W” and “A”, respectively.
NetworkPrecision
W/AANN
Acc.Precision
W/AANN
Acc.Precision
W/AANN
Acc.
AlexNet 32/32 56.52 32/3 58.58 32/2 56.41
VGG-16 32/32 73.36 32/3 73.02 32/2 71.91
tuning methods [12], [22], [23], our framework simplifies the
optimization as it only needs to optimize the accumulating
sequential error. Compared with layer-wise progressive tan-
dem learning [12] that fine-tunes all subsequent ANN lay-
ers together, our end-to-end fine-tuning mechanism yields
less computation time and cost. Compared with layer-wise
weight calibration [23], our method incorporates the bias
term (constant injecting current) during fine-tuning to learn
a compensating membrane potential instead of calculating
statistically.
3.4 Implementation Details
We perform all our experiments with PyTorch [49]. To
facilitate network training, we employ batch normalization
layers [50] in our models to address the internal covariance
shift problem with
x−µ√
σ2+ϵγ+β, (15)
where µis mini-batch mean, σ2is mini-batch variance,
ϵis a constant, γandβare two learnable parameters.
For hardware implementation, batch normalization can be
incorporated into the firing threshold θas
¯θ=θ−β
γp
σ2+ϵ+µ. (16)
To facilitate the training of deep networks, we employ
shortcut connections from the residual learning framework
[51] to address the vanishing/exploding gradient problem.
For hardware implementation, it only requires doubling
pre-synaptic connections to receive inputs from the stacked
layers and shortcuts, as the integration of spikes naturally
performs addition operations.
To build quantized ANNs for conversion, we employ
a state-of-the-art quantization method [46] during training.
To enable our ANN-to-SNN conversion method, we ap-
ply uniform quantization to activations. When exploring
the building of low-precision SNNs, we quantize ANN
weights with additive powers-of-two quantization insteadJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 8
of uniform quantization for better performance. In Table 2
and 3, we demonstrate the classification accuracy of ANNs
trained with different quantization precision on CIFAR-
10 and ImageNet, respectively. On CIFAR-10, our ResNet-
56 (ANN) with both weights and activations quantized
to 3 bits achieves 93.66% accuracy, which is close to the
accuracy of full-precision ResNet-56 (ANN) implemented
by us (94.10%) or reported in [51] (93.03%). On ImageNet,
our VGG-16 (ANN) with activations quantized to 3 bits
achieves 73.02% accuracy, only 0.34% lower than the full-
precision VGG-16 (ANN) from TorchVision [52]. Our VGG-
16 (ANN) with activations quantized to 2 bits achieves
71.91% accuracy on ImageNet, only 1.45% lower than the
full-precision VGG-16 (ANN) from TorchVision [52]. Based
on the performance of these deep ANNs, we employ 3/2-bit
ANNs for conversion in our experiments.
Following previous works [14], [16], [21], [23], [27], we
employ the integrate-and-fire (IF) model as our spiking
neuron model to better approximate ANN activations with
SNN firing rates. For spike encoding, we employ the widely-
used direct coding [12], [16], [21], [23], [31]. The (positive)
firing threshold θis directly mapped from the scaling factor
soptimized in ANNs. As for the negative firing threshold
θ′, we empirically set it to -1e-3 for all experiments.
4 E XPERIMENTS ON IMAGE CLASSIFICATION
Image classification is a fundamental and heavily studied
task in computer vision. It determines which objects are in
an image or video. In the ANN domain, recent advances in
image classification focus on training deep networks [51],
[53], [54] and have achieved great success. In the SNN do-
main, image classification is also the most commonly used
task for evaluation. To facilitate comparison with other SNN
methods, we perform our primary experimental analysis in
this section.
4.1 Experimental Setup
Datasets. We perform image classification on two bench-
mark datasets: CIFAR-10 [55] and ImageNet [56]. CIFAR-10
comprises 50,000 training images and 10,000 testing images
in 10 classes. ImageNet comprises 1.2 million training im-
ages, 50,000 validation images, and 100,000 test images in
1,000 classes.
Data preprocessing. On CIFAR-10, we follow many
works (e.g., [51]) for data preprocessing, i.e., standardizing
the data to have zero mean and unit variance, randomly
taking a 32 ×32 crop from a padded 40 ×40 image (4 pixels
padded on each side) or its horizontal flip. On ImageNet, we
also follow existing works (e.g., [57]) for data preprocessing,
i.e., randomly cropping 10–100% of the original image size
with a random aspect ratio in (4/5, 5/4), resizing to 224 ×
224, applying random flip and normalization by ImageNet
color. For evaluation, we resize the input by its shorter edge
to 256 pixels and take a 224 ×224 center crop.
Network architecture. On CIFAR-10, we train 32/3/2-
bit AlexNet [53], VGG-11 [54], and ResNet-18/20/44/56 [51]
for evaluation. For ResNet-20/44/56, we employ the origi-
nal ResNet architectures defined in [51]. To facilitate com-
parison with the ResNet-19 in [36], we employ a ResNet-
18 similar to the ResNet-19 defined in [36]. To explore theTABLE 4
Initial Learning Rate and Weight Decay. We denote the bit-precision of
weights and activations by ‘W’ and ‘A’, respectively.
Dataset Precision (W/A) Learning Rate Weight Decay
CIFAR-1032/32 0.1 5e-4
4/4 4e-2 1e-4
3/3 4e-2 1e-4
2/2 4e-2 3e-5
ImageNet32/3 1e-2 1e-4
32/2 1e-2 1e-4
building of low-precision SNNs, we quantize both weights
and activations for AlexNet, VGG-11, and ResNet-20/44/56
during ANN training. For ResNet-18, we quantize activa-
tions to enable conversion and keep full-precision weights
for a fair comparison with other full-precision SNNs. On
ImageNet, we train 3/2-bit AlexNet [53] and VGG-16 [54]
for evaluation. For AlexNet and VGG-16, we quantize acti-
vations to enable conversion and keep full-precision weights
for a fair comparison with other full-precision SNNs. For
full-precision models, we report the performance of pre-
trained models from TorchVision [52].
Training details. We follow the training protocol defined
in [46]. We use stochastic gradient descent (SGD) with a
momentum of 0.9. Table 4 lists the weight decay and initial
learning rate for different bit-precision on CIFAR-10 and
ImageNet. On CIFAR-10, we divide the learning rate by 10
at the 150th, 225th epoch, and finish training at the 300th
epoch. On ImageNet, we decrease the number of epochs in
[46] to 60, and divide the learning rate by 10 at the 20th,
40th, 50th epoch.
Evaluation metrics. In addition to the performance ac-
curacy, we compare the computation/energy-efficiency of
converted SNNs to their ANN counterparts by counting the
number of operations during inference [16].
For an ANN, its number of operations is defined as:
Ops=LX
l=1fl
inMl(17)
where findenotes fan-in (number of incoming connections
to a neuron), Ldenotes number of layers, Mldenotes the
number of neurons at layer l.
For an SNN, its number of operations is defined as the
summation of all synaptic operations (number of membrane
charges over time):
Ops=TX
t=1LX
l=1MlX
j=1fl
out,jsl
j(t) (18)
where fl
out,j denotes fan-out of neuron jat layer l(number
of output projections to neurons in the subsequent layer),
Tdenotes latency (number of time steps), sl
j(t)denotes
number of spikes neuron jat layer lhas fired at time step t.
4.2 Overall Performance
In Table 5, we compare our method (Fast-SNN) with state-
of-the-art SNN methods (including normalization [18], [20],
[21], calibration [23], clamped and quantized training [22],JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 9
TABLE 5
Performance comparison of different SNN methods on CIFAR-10 and ImageNet. Notions for different methods: Norm. is normalization [18], [20],
[21], Cal. is calibration [23], CQT is clamped and quantized training [22], PTL is progressive tandem learning [12], STDP-tdBN [29], DIET -SNN
[31], Grad. Re-wt. is gradient re-weighting [36]. ∆Acc. = SNN Acc. - ANN Acc. On ImageNet, bracketed numbers denote top-5 accuracy. We
present the accuracy as a percent (%). We denote the bit-precision of weights and activations by ‘W’ and ‘A’, respectively. Best and second best
numbers are indicated by bold and underlined fonts, respectively.
Work Architecture MethodPrecision
(ANN W/A)ANN
Acc.Precision
(SNN W)SNN
Acc.∆Acc.Time
StepsCIFAR-10Sengupta et al. 2019 [18] VGG-16 Norm. 32/32 91.70 32 91.55 -0.15 2500
Han et al. 2020 [20] VGG-16 Norm. 32/32 93.63 32 93.63 0 2048
Deng et al. 2021 [21] VGG-16 Norm. 32/32 92.09 32 92.29 +0.20 16
Li et al. 2021 [23] VGG-16 Cal. 32/32 95.72 32 93.71 -2.01 32
Yan et al. 2021 [22] VGG-19 CQT 32/32 93.60 32 93.44 -0.06 1000
Yan et al. 2021 [22] VGG-19 CQT 32/32 93.60 9 93.43 -0.07 1000
Yan et al. 2021 [22] VGG-19 CQT 32/32 93.60 8 92.82 -0.78 1000
Wu et al. 2021 [12] VGG-11 PTL 32/32 90.59 32 91.24 +0.65 16
Wu et al. 2021 [12] AlexNet PTL 32/32 89.59 32 90.86 +1.27 16
Wu et al. 2021 [12] AlexNet PTL -/- - 8 90.11 - 16
Wu et al. 2021 [12] AlexNet PTL -/- - 4 89.48 - 16
Zheng et al. 2021 [29] ResNet-19 STBP-tdBN -/- - 32 93.16 - 6
Rathi et al. 2021 [31] VGG-16 DIET-SNN 32/32 93.72 32 92.70 -1.02 5
Deng et al. 2022 [36] VGG-16 Grad. Re-wt. 32/32 94.97 32 94.50 -0.47 6
Ours AlexNet Fast-SNN 3/3 92.54 3 92.53 -0.01 7
Ours AlexNet Fast-SNN 2/2 91.52 2 91.63 +0.11 3
Ours VGG-11 Fast-SNN 3/3 93.71 3 93.72 +0.01 7
Ours VGG-11 Fast-SNN 2/2 93.06 2 92.99 -0.07 3
Ours ResNet-18 Fast-SNN 32/3 95.62 32 95.57 -0.05 7
Ours ResNet-18 Fast-SNN 32/2 95.51 32 95.42 -0.09 3ImageNetSengupta et al. 2019 [18] VGG-16 Norm. 32/32 70.52 (89.39) 32 69.96 (89.01) -0.56 (-0.38) 2500
Han et al. 2020 [20] VGG-16 Norm. 32/32 73.49 (-) 32 73.09 (-) -0.40 (-) 4096
Deng et al. 2021 [21] VGG-16 Norm. 32/32 72.40 (-) 32 55.80 (-) -16.60 (-) 16
Li et al. 2021 [23] VGG-16 Cal. 32/32 75.36 (-) 32 63.64 (-) -11.72 (-) 32
Wu et al. 2021 [12] AlexNet PTL 32/32 58.53 (81.07) 32 55.19 (78.41) -3.34 (-2.66) 16
Wu et al. 2021 [12] VGG-16 PTL 32/32 71.65 (90.37) 32 65.08 (85.25) -6.57 (-5.12) 16
Zheng et al. 2021 [29] ResNet-34 STBP-tdBN -/- - 32 63.72 (-) - 6
Rathi et al. 2021 [31] VGG-16 DIET-SNN 32/32 70.08 (-) 32 69.00 (-) -1.08 5
Deng et al. 2022 [36] ResNet-34aGrad. Re-wt. 32/32 - 32 64.79 (-) - 6
Ours AlexNet Fast-SNN 32/3 58.58 (80.57) 32 58.52 (80.59) -0.06 (+0.02) 7
Ours AlexNet Fast-SNN 32/2 56.41 (79.11) 32 56.34 (79.00) -0.07 (-0.11) 3
Ours VGG-16 Fast-SNN 32/3 73.02 (91.28) 32 72.95 (91.08) -0.07 (-0.20) 7
Ours VGG-16 Fast-SNN 32/2 71.91 (90.58) 32 71.31 (90.21) -0.60 (-0.37) 3
aFor a fair comparison with other SNN methods, we report ResNet-34 in [36] by the version that restricts spiking neurons to fire at most one spike
at a single time step.
progressive tandem learning [12], STDP-tdBN [29], DIET-
SNN [31]) using AlexNet [53], VGG 11/16 [54], and ResNet-
18 [51]. All numbers for comparison are taken from corre-
sponding papers, including the performance of pre-trained
ANNs if provided. For a fair comparison with state-of-the-
art conversion methods, we refer to calibration [23] and
normalization [20] as our performance baseline on CIFAR-10
and ImageNet, respectively. We refer to progressive tandem
learning [12] as our latency baseline on both CIFAR-10 and
ImageNet.
CIFAR-10. Compared with the performance baseline
[23], our VGG-11 (latency is 7) achieves an accuracy
slightly higher than their VGG-16 while using about 5 ×
fewer time steps. Compared with the latency baseline
[12], our AlexNet/VGG-11 (latency is 3) outperforms their
AlexNet/VGG-11 by 0.77%/1.75% accuracy while using 5 ×
fewer time steps. It is worth noting that the weight precision
of our SNNs is 2-bit while their SNNs employ full-precision
weights. Compared with their AlexNet with 4-bit weight
precision, the accuracy of our AlexNet with 2-bit weight
precision is 2.15% higher than theirs. Compared with a
state-of-the-art direct training method [36], our ResNet-18
(latency is 3) outperforms their ResNet-19 by 0.92% accuracy
while using half time steps.ImageNet. Compared with the performance baseline
[20], our VGG-16 (latency is 7) achieves a top-1 accuracy
only slightly lower than their VGG-16 while using about
600×fewer time steps. Compared with the latency baseline
[12], our AlexNet/VGG-11 (latency is 3) outperforms their
AlexNet/VGG-11 by 1.15%(0.59%)/6.23%(4.96%) on top-1
(top-5) accuracy while using 5 ×fewer time steps. Notably,
our method achieves a small accuracy gap between the
SNNs and their counterpart ANNs. With a latency of 7,
the top-1 accuracy of our AlexNet/VGG-16 drops only
0.06%/0.07%. With a latency of 3, the top-1 accuracy of
our AlexNet/VGG-16 drops 0.07%/0.60%. In contrast, the
latency baseline [12] reported a significant drop in accuracy
(3.34%/6.57% for AlexNet/VGG-16 with a latency of 16)
compared with pre-trained ANNs. Compared with a state-
of-the-art direct training method [31], our VGG-16 (latency
is 3) outperforms their VGG-16 by 2.31% top-1 accuracy
while using about half time steps.
4.3 Evaluation for Minimizing the Quantization Error
To validate the efficacy of a learnable firing threshold,
we train a 2-bit AlexNet/VGG-11 on CIFAR-10. Then we
convert them to corresponding SNNs with different firing
thresholds: the clipping threshold, the max activation, andJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 10
TABLE 6
Accuracy (%) of SNNs converted from 2-bit networks using different
types of firing thresholds on CIFAR-10. We denote the maximum
activation [15] as Max, 99th/99.9th percentile [16] of activations as
99/99.9. We convert pre-trained ANNs directly to SNNs with no
improvements applied. We denote the bit-precision of weights and
activations by ‘W’ and ‘A’, respectively. Best numbers are indicated by
the bold font.
Precision (W/A) Network Threshold Acc. Steps
2/2 AlexNet(ANN) - 91.52 -
2/2 AlexNet(SNN) Max 32.71 3 ×7
2/2 AlexNet(SNN) 99.9 72.29 3 ×7
2/2 AlexNet(SNN) 99 89.16 3 ×7
2/2 AlexNet(SNN) Ours 91.52 3×7
2/2 AlexNet(SNN) Max 23.09 3
2/2 AlexNet(SNN) 99.9 54.56 3
2/2 AlexNet(SNN) 99 81.66 3
2/2 AlexNet(SNN) Ours 88.97 3
2/2 VGG-11(ANN) - 93.06 -
2/2 VGG-11(SNN) Max 21.25 3 ×11
2/2 VGG-11(SNN) 99.9 79.52 3 ×11
2/2 VGG-11(SNN) 99 91.56 3 ×11
2/2 VGG-11(SNN) Ours 93.06 3×11
2/2 VGG-11(SNN) Max 24.04 3
2/2 VGG-11(SNN) 99.9 45.56 3
2/2 VGG-11(SNN) 99 80.48 3
2/2 VGG-11(SNN) Ours 84.92 3
the 99th/99.9th percentile of activations. We determine the
max activation, 99th, and 99.9th percentile of activations
by the first batch of the original training set with a batch
size of 128. To compare the performance without sequential
error, we set the latency to 3 ×7/3×11 for AlexNet/VGG-
11 (spiking neurons start firing after receiving all possible
spikes). To demonstrate the impact of the sequential error,
we set the latency to 3.
As shown in Table 6, our learnable firing thresh-
old achieves higher accuracy than all other types of
firing thresholds under different latency configurations.
Compared with normalization methods (statistical post-
processing), our framework jointly optimizes the clipping
threshold (firing threshold) at each layer, resulting in a
clipping range that better fits the input data. It is worth
noting that our SNNs using a latency of (2b−1)×L(without
the sequential error) achieve the same accuracy as their pre-
trained ANNs. This result indicates no conversion error and
validates the efficacy of our latency bound in Eq. 11.
4.4 Evaluation for Minimizing the Accumulating Error
To validate the efficacy of our signed IF neuron model
and layer-wise fine-tuning mechanism, we stage by stage
apply each piece of our method to SNNs with a latency
of2b−1. We train 3/2-bit AlexNet/VGG-11 on CIFAR-
10 and AlexNet/VGG-16 on ImageNet with quantization-
aware training. Then we convert the trained ANNs to SNNs.
Here, we introduce a set of notions for SNNs. SNNαstands
for native SNNs directly converted from quantized ANNs.
SNNβstands for SNNαwith our signed IF neuron model.
SNNγstands for SNNβwith our layer-wise fine-tuning
mechanism.
As shown in Table 7, our signed IF neuron model
and layer-wise fine-tuning consistently improve the perfor-
mance of converted SNNs. On CIFAR-10, SNNβwith signed
IF neuron model consistently improves the performance ofTABLE 7
Accuracy (%) of SNNα(native SNN), SNNβ(SNN with our signed IF
neuron model), SNNγ(SNN with both our signed IF neuron model and
layer-wise fine-tuning) on CIFAR-10 and ImageNet. We denote the
bit-precision of weights and activations by ‘W’ and ‘A’, respectively.
Precision
W/ANetworkANN
Acc.SNNα
Acc.SNNβ
Acc.SNNγ
Acc.Time
StepsCIFAR-103/3 AlexNet 92.54 90.72 92.46 92.53 7
2/2 AlexNet 91.52 88.97 91.37 91.63 3
3/3 VGG-11 93.71 90.29 93.46 93.72 7
2/2 VGG-11 93.06 84.92 92.80 92.99 3ImageNet32/3 AlexNet 58.58 47.74 58.35 58.52 7
32/2 AlexNet 56.41 46.04 55.93 56.34 3
32/3 VGG-16 73.02 36.43 72.89 72.95 7
32/2 VGG-16 71.91 46.10 71.10 71.31 3
TABLE 8
Accuracy (%) of SNNα(native SNN), SNNβ(SNN with our signed IF
neuron model), SNNγ(SNN with both our signed IF neuron model and
layer-wise fine-tuning) on CIFAR-10. SNNs are converted from 3/2-bit
ResNets with a latency of 7/3. ∆Acc. = SNNγAcc. - SNNβAcc.
NetworkANN
Acc.SNNα
Acc.SNNβ
Acc.SNNγ
Acc.∆Acc.3-bitResNet-20 92.39 83.37 91.33 92.18 +0.85
ResNet-44 93.41 61.06 90.74 92.62 +1.88
ResNet-56 93.66 41.47 88.55 92.17 +3.622-bitResNet-20 90.39 81.08 88.81 90.28 +1.47
ResNet-44 91.57 59.84 85.89 89.59 +3.70
ResNet-56 91.68 60.95 81.25 89.25 +8.00
SNNαby at least 1.5%. For the 2-bit VGG-11, the improve-
ment achieves a wide margin of 7.88%. With layer-wise fine-
tuning applied, all our SNNs achieve almost lossless perfor-
mance compared with the corresponding quantized ANNs.
Recalling the full-precision ANNs results in Table 2 and
Table 3, it is also notable our method achieves an accuracy
comparable to full-precision ANN networks on CIFAR-10
and ImageNet with a latency of 7. Even with a latency of
3, the accuracy of SNNγ(AlexNet/VGG-11) on CIFAR-10
is only 1.24%/0.61% lower than full-precision ANNs. On
ImageNet, the top-1 accuracy of SNNγ(AlexNet/VGG-16)
with a latency of 3 is only 0.18%/2.05% lower than full-
precision ANNs.
In the above experiments, the improvement from our
layer-wise fine-tuning mechanism is less significant, for the
employed models (AlexNet, VGG-11, VGG-16) are relatively
shallow (less than 20 layers). According to Eq. 11, when Lis
small, the impact of accumulating sequential error is limited,
and SNN firing rates still approximate ANN activations. To
further validate the efficacy of our layer-wise fine-tuning
mechanism, we apply our layer-wise fine-tuning to deep
residual networks (ResNets) [51]. Kindly note that a deep
architecture like ResNet could achieve high performance
with fewer parameters and operations (e.g., on CIFAR-10,
32-bit AlexNet with an accuracy of 92.87% uses 43 ×more
parameters, 5 ×more operations than 32-bit ResNet-20 with
an accuracy of 93.00%). However, conventional SNNs may
not benefit from deep architectures as the sequential error
accumulates during network propagation.
On CIFAR-10, we train 3/2-bit ResNet-20/44/56 and
convert them to SNNs with a corresponding latency of 7/3.
We measure the efficacy of our fine-tuning method by the
difference between SNNγaccuracy and SNNβaccuracy. In
Table 8, our fine-tuning mechanism consistently improvesJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 11
the performance of all spiking ResNets. Notably, the ac-
curacy gain from fine-tuning is bigger when the network
is deeper or the precision is lower (lower latency). For
the 2-bit ResNet-56, our fine-tuning method improves the
accuracy of SNNβby a large margin of 8%. This result
coincides with our theoretical analysis that the accumulating
sequential error grows proportional to network depth L.
It also demonstrates the effectiveness of our method in re-
approximating ANN activations with fine-tuned SNN firing
rates.
4.5 Computational Efficiency
In a neural network, the number of operations required for
inference determines its computation/energy efficiency. In
a rate-coded SNN, the number of operations grows pro-
portionally to the latency. According to the analysis of the
quantization error [16], previous methods have to balance
the trade-off between classification accuracy and inference
latency. However, when the latency reaches a critical point,
it invalidates SNN’s advantages. For example, AlexNet
(SNN), with a latency of 32, yields almost the same number
of operations as AlexNet (ANN) on CIFAR-10. Therefore, we
should also validate the computation/energy efficiency of
SNNs concerning the latency required to achieve compara-
ble accuracy to ANNs. Following [16], we calculate the num-
ber of operations for our SNNs on CIFAR-10 with different
inference latency. For comparison, we choose progressive
tandem learning (PTL) [12] that achieves the shortest latency
among previous methods as our baseline. Then we repro-
duce AlexNet/VGG-11 from [12] on CIFAR-10 using their
public codes for both ANN training and SNN conversion
with different latency configurations.
In Fig. 6, we show the performance comparison ((a),
(b), (d), (e)) and ratio of operations ((c), (f)) on CIFAR-10
regarding different latency. As can be observed, our method
is much more computation/energy efficient compared with
PTL [12]. That is, the accuracy of our AlexNet/VGG-11
(latency is 3) is higher than that of PTL’s [12] AlexNet/VGG-
11 (latency is 15) by 1.75%/2.00%. With a latency of 15,
PTL [12] yields a ratio of operations of 47.4%/74.9%, while
ours is 11.1%/24.8% with a latency of 3. This result indi-
cates our AlexNet/VGG-11 is at least 4/3 ×more computa-
tion/energy efficient than PTL [12].
4.6 Discussion
On CIFAR-10, we train the quantized ANNs with both
weight and activation quantized. Therefore, the converted
SNNs naturally inherit the weight precision of ANNs and
are inherently compatible with neuromorphic hardware that
supports low-bit integer weight precision. As shown in
Table 5, 7, and 8, our SNNs with quantized weights could
achieve high performance with only a few time steps, mak-
ing them friendly to real-time applications on low-precision
neuromorphic hardware.
In addition, a latency of 3 also improves the com-
putation/energy efficiency of real-time SNN applications.
Regarding the number of operations, our SNNs (latency
is 3) are about 10 ×more efficient than their counterpart
ANNs, not to mention operations in SNNs are more efficient
than operations in ANNs. The deep ANNs are primarily
(a)
 (b)
 (c)
(d)
 (e)
 (f)
Fig. 6. Performance comparison with PTL [12] on AlexNet (top) and
VGG-11 (bottom). Dashed lines indicate the performance of full-
precision ANNs. (a) (d) Our performance. (b) (e) The performance of
PTL [22]. (c) (f) Ratio of the number of operations (SNN to ANN), smaller
is better.
composed of multiply-accumulate (MAC) operations that
lead to high execution time, power consumption, and area
overhead. In contrast, SNNs are composed of accumulate
(AC) operations that are much less costly than MAC op-
erations [58]. Recently, Arafa et al. [59] reported that the
energy consumption of an optimized 32-bit floating-point
add instruction is about 10 ×lower than that of a multiply
instruction under NVIDIA’s Turing GPU architecture.
Moreover, compared with the latency bound in Eq. 11,
our SNNs with a latency of Timprove the efficiency of
real-time applications in terms of running time ( L×faster).
In practice, it also benefits energy consumption. Because
even though the waiting period ideally does not introduce
additional energy consumption as the number of operations
remains unchanged, real-time applications still consume
energy in a dormant state due to the restrictions in current
neuromorphic devices.
5 E XPERIMENTS ON OBJECT DETECTION
Object detection is a fundamental and heavily studied task
in computer vision [61], [62]. It aims to recognize and locate
every object in an image, typically with a bounding box.
Thanks to the advances in deep learning, object detection
has received significant improvements over recent years,
especially with the application of Convolutional NeuralJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 12
TABLE 9
Performance comparison for object detection task on PASCAL VOC 2007 and MS COCO 2017. ‘Norm.’ denotes normalization. ∆mAP = SNN
mAP - ANN mAP . We present the mean AP as a percent (%). We denote the bit-precision of weights and activations by ‘W’ and ‘A’, respectively.
Best and second best numbers are indicated by bold and underlined fonts, respectively.
Work Architecture MethodPrecision
(ANN W/A)ANN
mAPPrecision
(SNN W)SNN
mAP∆mAPTime
StepsPASCAL VOCKim et al. 2020 [19] Tiny YOLO Norm. 32/32 53.01 32 51.83 -1.18 8000
Kim et al. 2020 [60] Tiny YOLO Norm. 32/32 53.01 32 51.44 -1.57 5000
Ours Tiny YOLO Fast-SNN 32/4 53.28 32 53.17 -0.11 15
Ours Tiny YOLO Fast-SNN 32/3 52.77 32 52.83 +0.06 7
Ours Tiny YOLO Fast-SNN 32/2 50.32 32 50.56 +0.24 3
Ours YOLOv2(ResNet-34) Fast-SNN 32/4 76.16 32 76.05 -0.11 15
Ours YOLOv2(ResNet-34) Fast-SNN 32/3 75.27 32 73.43 -1.84 7
Ours YOLOv2(ResNet-34) Fast-SNN 32/2 73.57 32 68.57 -5.00 3MS COCOKim et al. 2020 [19] Tiny YOLO Norm. 32/32 26.24 32 25.66 -0.58 8000
Kim et al. 2020 [60] Tiny YOLO Norm. 32/32 26.24 32 25.78 -0.46 5000
Ours Tiny YOLO Fast-SNN 32/4 27.74 32 27.59 -0.15 15
Ours Tiny YOLO Fast-SNN 32/3 26.84 32 26.49 -0.35 7
Ours Tiny YOLO Fast-SNN 32/2 24.34 32 22.88 -1.46 3
Ours YOLOv2(ResNet-34) Fast-SNN 32/4 46.96 32 46.40 -0.56 15
Ours YOLOv2(ResNet-34) Fast-SNN 32/3 46.32 32 41.89 -4.43 7
Ours YOLOv2(ResNet-34) Fast-SNN 32/2 43.33 32 33.84 -9.49 3
person: 0.67 person: 0.70 person: 0.70 person: 0.72
aeroplane : 0.92 aeroplane : 0.91 aeroplane : 0.88 aeroplane : 0.85
boot: 0.87 boot: 0.89 boot: 0.87 boot: 0.82
dog: 0.89 dog: 0.88 dog: 0.83 dog: 0.80
vase: <0.3 vase: 0.33 vase: <0.3 vase: <0.3
person: 0.94 person: 0.91 person : 0.89 person: 0.84
bear: 0.92 bear: 0.89 Bear: 0.89 Bear: 0.56
boat: 0.73 boat: 0.62 boat: 0.31 boat: 0.47
FP ANN SNN (   ) SNN (    ) SNN (   ) FP ANN SNN (   ) SNN (    ) SNN (   )PASCAL VOC 2007 MS COCO 2017
Fig. 7. Visual quality comparison of object detection results on testset of PASCAL VOC 2007 (left) and valset of MS COCO 2017 (right) with the
network architecture YOLOv2(ResNet-34). From left to right: results from full-precision (FP) ANN models, our SNN with T= 15,7,3, respectively.
Networks (CNNs). Currently, the main steam object de-
tection algorithms fall into two lines of research. One line
of research focuses on strong two-stage object detectors
[63], [64], [65], [66]. These Region-based CNN (R-CNN) [63]
algorithms first generate regions of interest (ROIs) with a
region proposal network (RPN), then perform classification
and bounding box regression. While being accurate, two-
stage methods suffer from a slow inference speed. Another
line of research focuses on one-stage object detectors [67],
[68]. These methods only use CNNs for feature extraction
and directly predict the categories and positions of objects.
With a balanced latency and accuracy, one-stage methods
are widely used in real-time applications. Although heav-
ily studied in the ANN domain, object detection requires
further exploration in the SNN domain. An existing SNN
object detector is the Spiking-YOLO [19] and its improved
version [60]. Spiking-YOLO employs an ANN-to-SNN con-version method with channel-wise normalization. However,
Spiking-YOLO is unfriendly to real-time applications for
requiring more than 5,000 time steps during inference. Here,
we explore our Fast-SNN for object detection using the
YOLO framework [69] for a fair comparison.
5.1 Experimental Setup
Datasets. We perform object detection task on two
datasets: PASCAL VOC [61] and MS COCO 2017 [62].
The PASCAL VOC dataset contains 20 object categories.
Specifically, the train/val/test data in VOC 2007 contains
24,640 annotated objects in 9,963 images. For VOC 2012, the
train/val data contains 27,450 annotated objects in 11,530
images. For MS COCO 2017 dataset, it contains 80 object
categories. It has 886,284 annotated objects spread in 118,287
training images and 5,000 validation images. Following a
common protocol [65], [68], [69] on PASCAL VOC, theJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 13
training data is the union of VOC 2007 trainval and VOC
2012 trainval datasets. As for testing, we use the VOC 2007
test dataset.
Data preprocessing. We use a similar data augmenta-
tion to YOLO [69] and SSD [68] with random crops, color
shifting, etc.
Network architecture. Following [19], we employ a
simple but efficient version of YOLO, the Tiny YOLO
[69] for evaluation. We modify Tiny YOLO for ANN-to-
SNN conversion by replacing all leakyReLU with ReLU.
We also remove max pooling layers by incorporating the
downsampling operations into convolution layers. To fur-
ther explore object detection with deep SNNs, we also
evaluate a YOLOv2 [69] with a backbone of ResNet-34. In
the remainder of this paper, we refer this architecture as
YOLOv2(ResNet-34). For PASCAL VOC 2007, we predict
5 boxes with 5 coordinates each and 20 classes per box,
resulting in 125 filters. For MS COCO 2017, we predict
5 boxes with 5 coordinates each and 80 classes per box,
resulting in 425 filters.
Training details. On PASCAL VOC and MS COCO, we
fine-tune the models initialized from pre-trained ImageNet
models to straightly adapt them to the object detection task.
For Tiny YOLO, we initialize the backbone network from a
model trained with the protocol in Section 4.1 on ImageNet.
For YOLOv2(ResNet-34), we directly initialize the backbone
from the pre-trained ResNet-34 in TorchVision [52]. Then
we fine-tune the initialized models for 250 epochs using the
standard SGD optimizer on PASCAL VOC 2007 and MS
COCO 2017. For the first two epochs we slowly raise the
learning rate from 0 to 1e-3. Then we continue training with
a learning rate of 1e-3 and divide the learning rate by 10 at
the the 150th, 200th epoch.
Evaluation metrics. The performance is measured in
terms of mean average precision (mAP). On PASCAL VOC,
we use the definitions from VOC 2007 to calculate average
precision (AP) and report the mAP over 20 object categories.
On MS COCO, we follow MS COCO protocol to calculate
AP and report the mAP over 80 object categories. For a fair
comparison with Spiking-YOLO [19], we report the mAPs
at IoU = 0.5 on both PASCAL VOC and MS COCO.
5.2 Overall Performance
We summarize and compare the performance in Table 9.
We include the Spiking-YOLO by Kim et al. [19] and its
improved version [60] for comparison. All numbers are
taken from corresponding papers. On PASCAL VOC 2007,
our Tiny YOLO (latency is 7) outperforms [60] (Tiny YOLO)
by 1.39% mean AP performance while using about 714×
fewer time steps. Furthermore, our Tiny-YOLO achieves
almost lossless conversion for all latency configurations (3,
7, 15). Thanks to the shallow architecture of Tiny YOLO,
our converted Tiny YOLO even outperforms its counter-
part ANN when the latency is 3/7. To further explore
the capacity of deep SNNs in object detection, we apply
our method to the more challenging YOLOv2(ResNet-34)
architecture. Compared with [60], our YOLOv2(ResNet-34)
(latency is 15/7/3) achieves 24.61%/21.99%/17.13% higher
mean AP performance while using 333/714/1,667×fewer
time steps. On MS COCO 2017, our Tiny YOLO (latencyis 7) outperforms [60] (Tiny YOLO) by 0.71% mean AP
performance while using about 714×fewer time steps.
Our YOLOv2(ResNet-34) (latency is 15/7/3) outperforms
[60] by 20.62%/16.11%/7.67% mean AP performance while
using 333/714/1,667×fewer time steps. We further provide
visual results of our YOLOv2(ResNet-34) in Fig. 7. As shown
in the figure, our SNNs are able to detect objects at a degree
close to the full-precision ANN. Our SNN with a latency of
15 can detect objects (e.g., vase) that the full-precision ANN
fails to detect.
6 E XPERIMENTS ON SEMANTIC SEGMENTATION
Semantic segmentation is a fundamental and heavily stud-
ied task in computer vision [61], [62]. It aims to predict
the object class of every pixel in an image or give it a
‘background’ status if not in listed classes. In recent years,
semantic segmentation with deep learning has achieved
great success. The fully convolutional network (FCN) [70]
that regards semantic segmentation as a dense per-pixel
classification problem has been the basis of semantic seg-
mentation with CNNs. To preserve image details, SegNet
[71] employs an encoder-decoder structure, U-Net [72] in-
troduces a skip connection between the downsampling
and up-sampling paths, and RefineNet [73] presents multi-
path refinement to exploit fine-grained low-level features.
To capture the contextual information at multiple scales,
Deeplab [74] introduces Atrous Spatial Pyramid Pooling
(ASPP), and PSPNet [75] performs spatial pyramid pooling
at different scales. Although heavily studied in the ANN
domain, semantic segmentation is scarcely explored in the
SNN domain. An existing work by Kim et al. [43] employs
SNNs directly trained with surrogate gradients for semantic
segmentation. However, this method suffers from a signif-
icant performance drop compared with ANNs. Here, we
explore our Fast-SNN for semantic segmentation using the
Deeplab framework [74] for a fair comparison.
6.1 Experimental Setup
Datasets. We perform semantic segmentation task on
two datasets: PASCAL VOC 2012 [61] and MS COCO 2017
[62]. PASCAL VOC 2012 is further augmented by the extra
annotations provided by [76], resulting in 10,582 training
images.
Data preprocessing. Following the original Deeplab pro-
tocol [74], [77], [78], we first standardize the data with the
mean and standard deviation from the ImageNet dataset.
Then we take a random 513×513crop from the image. We
further apply data augmentation by randomly scaling the
input images (from 0.5 to 2.0) and flipping them horizontally
at a chance of 50%.
Network architecture. We evaluate two Deeplab [74]
based architectures for semantic segmentation. The first
architecture is the VGG-9 defined in [43]. To facilitate ANN-
to-SNN conversion, we remove the average pooling layers
and incorporate downsampling operations into convolution
layers. With three downsampling layers of stride 2, the
output stride (defined as the ratio of input image spatial
resolution to final output resolution [78]) of this VGG-9
architecture is 8. The second architecture comprises a back-
bone of ResNet-34 and an ASPP module. In the remainder ofJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 14
TABLE 10
Performance comparison for semantic segmentation task on PASCAL VOC 2012 and MS COCO 2017. ‘DT’ denotes direct training. ∆mIoU = SNN
mIoU - ANN mIoU. We present the mean IoU as a percent (%). We denote the bit-precision of weights and activations by ‘W’ and ‘A’, respectively.
Best and second best numbers are indicated by bold and underlined fonts, respectively.
Work Architecture MethodPrecision
(ANN W/A)ANN
mIoUPrecision
(SNN W)SNN
mIoU∆mIoUTime
StepsPASCAL VOCKim et al. 2021 [43] VGG-9 DT 32/32 34.7 32 22.3 -12.4 20
Ours VGG-9 Fast-SNN 32/4 44.38 32 44.55 +0.17 15
Ours VGG-9 Fast-SNN 32/3 45.94 32 44.98 -0.96 7
Ours VGG-9 Fast-SNN 32/2 45.41 32 43.86 -1.55 3
Ours ResNet-34 + ASPP Fast-SNN 32/4 71.97 32 69.7 -2.27 15
Ours ResNet-34 + ASPP Fast-SNN 32/3 70.31 32 59.81 -10.50 7
Ours ResNet-34 + ASPP Fast-SNN 32/2 41.45 32 17.28 -24.17 3MS COCOOurs VGG-9 Fast-SNN 32/4 31.63 32 31.14 -0.49 15
Ours VGG-9 Fast-SNN 32/3 31.50 32 30.46 -1.04 7
Ours VGG-9 Fast-SNN 32/2 30.20 32 28.35 -1.85 3
Ours ResNet-34 + ASPP Fast-SNN 32/4 52.22 32 50.24 -1.98 15
Ours ResNet-34 + ASPP Fast-SNN 32/3 52.15 32 47.03 -5.12 7
Ours ResNet-34 + ASPP Fast-SNN 32/2 50.72 32 41.97 -8.75 3
FP ANN SNN (   ) SNN (    )SNN (   ) Original Image FP ANN SNN (   ) SNN (    ) SNN (   ) Original ImagePASCAL VOC 2012 MS COCO 2017
Fig. 8. Visual quality comparison of semantic segmentation results on valset of PASCAL VOC 2012 (left) and MS COCO 2017 (right) with the
network architecture ResNet-34 + ASPP . From left to right: the original image (input), results from full-precision (FP) ANN models, our SNN with
T= 15,7,3, respectively.
this paper, we refer to this architecture as ResNet-34 + ASPP .
The ASPP module contains five parallel convolution layers:
one1×1convolution and four 3×3atrous convolutions with
rates = (6,12,18,24) to capture multi-scale information,
following the configurations in [74]. The output stride of
ResNet-34 + ASPP is 16.
Training details. On PASCAL VOC and MS COCO,
we fine-tune the models initialized from pre-trained Im-
ageNet models to straightly adapt them to the semantic
segmentation task. For VGG-9, we initialize the first seven
convolution and batch normalization layers from the pre-
trained VGG-16 in TorchVision [52]. For ResNet-34 + ASPP ,
we initialize the backbone from the pre-trained ResNet-
34 in TorchVision [52]. Then we fine-tune the initialized
models for 50 epochs using the standard SGD optimizer on
both PASCAL VOC 2012 and MS COCO 2017. Following
[78], we initialize the learning rate to 0.007 and employ a
‘poly’ learning rate policy where the initial learning rate is
multiplied by
(1−iter
max iter)power(19)
with power = 0.9. We use a momentum of 0.9 and aweight decay of 1e-4. We progressively build ANNs with
activations quantized to 4/3/2 bits and apply ANN-to-SNN
conversion.
Evaluation metrics. The performance is measured in
terms of pixel intersection-over-union (IoU) averaged across
the 21 classes (20 foreground object classes and one back-
ground class) on PASCAL VOC 2012 and 81 classes on MS
COCO 2017 (80 foreground object classes and one back-
ground class).
6.2 Overall Performance
We summarize and compare the performance in Table 10.
On PASCAL VOC 2012, we include the Spiking-Deeplab
proposed by Kim et al. [43] comparison. All numbers are
taken from their paper. Compared with [43], our VGG-9
(latency is 3) outperforms their VGG-9 by 21.56% mean
IoU performance while using about 7×fewer time steps.
Compared with [43], our VGG-9 also achieves a limited
performance gap between ANNs and SNNs. With a latency
of 15, our converted VGG-9 even improves the ANN per-
formance by 0.17% at mean IoU. To further explore the
capacity of deep SNNs in semantic segmentation, we applyJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 15
our method to the more challenging ResNet-34 + ASPP . Our
ResNet-34 + ASPP achieves 69.7% mean IoU performance
with a latency of 15, outperforming [43] by 47.4% mean
IoU performance and 5 fewer time steps. For the first time,
our method demonstrate that SNNs can achieve compara-
ble performance to ANNs on the challenging MS COCO
dataset performing semantic segmentation task. Comparing
with corresponding ANNs, our VGG-9 has less than 2%
performance drop at mean IoU for all latency configurations
(3,7,15). Our VGG-9 (latency is 15) achieves 31.14% mean
IoU performance, only 0.46% lower than its ANN coun-
terpart. For a deeper architecture, our ResNet-34 + ASPP
(latency is 15) achieves 50.24% mean IoU performance. We
further provide visual results of our ResNet-34 + ASPP in
Fig. 8. As shown in the figure, our SNNs are able to segment
objects at a degree close to the full-precision ANN. For some
objects (e.g., TV), our SNNs yield better results than the full-
precision ANN.
7 C ONCLUSION
In this work, we propose a framework to build a Fast-SNN
with competitive performance (i.e., comparable with ANNs)
and low inference latency (i.e., 3, 7, 15). Our basic idea is
to minimize the quantization error and accumulating error.
We show the equivalent mapping between temporal quan-
tization in SNNs and spatial quantization in ANNs, based
on which we transfer the minimization of the quantization
error to quantized ANN training. This scheme facilitates
ANN-to-SNN conversion by finding the optimal clipping
range and the novel distributions of weights and activations
for each layer. This mapping also makes the accumulating
sequential error the only culprit of performance degradation
when converting a quantized ANN to an SNN. To mitigate
the impact of the sequential error at each layer, we propose a
signed IF neuron to cancel the wrongly fired spikes. To alle-
viate the accumulating sequential error, we propose a layer-
wise fine-tuning mechanism to minimize the difference be-
tween SNN firing rates and ANN activations. Our frame-
work derives the upper bound of inference latency that
guarantees no performance degradation when converting a
quantized ANN to an SNN. Our method achieves state-of-
the-art performance and low latency on various computer
vision tasks, including image classification, object detection,
and semantic segmentation. In addition, our SNNs that
inherit the quantized weights in ANNs are intrinsically
compatible with low-precision neuromorphic hardware.
REFERENCES
[1] I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, and Y. Bengio,
“Binarized neural networks,” Proc. Adv. Neural Inf. Process. Syst. ,
vol. 29, pp. 4114–4122, 2016.
[2] K. Roy, A. Jaiswal, and P . Panda, “Towards spike-based machine
intelligence with neuromorphic computing,” Nature , vol. 575, no.
7784, pp. 607–617, 2019.
[3] W. Maass, “Networks of spiking neurons: the third generation of
neural network models,” Neural networks , vol. 10, no. 9, pp. 1659–
1671, 1997.
[4] P . A. Merolla, J. V . Arthur, R. Alvarez-Icaza, A. S. Cassidy,
J. Sawada, F. Akopyan, B. L. Jackson, N. Imam, C. Guo, Y. Naka-
mura et al. , “A million spiking-neuron integrated circuit with a
scalable communication network and interface,” Science , vol. 345,
no. 6197, pp. 668–673, 2014.[5] M. Davies, N. Srinivasa, T.-H. Lin, G. Chinya, Y. Cao, S. H. Choday,
G. Dimou, P . Joshi, N. Imam, S. Jain et al. , “Loihi: A neuromorphic
manycore processor with on-chip learning,” IEEE Micro , vol. 38,
no. 1, pp. 82–99, 2018.
[6] P . Lichtsteiner, C. Posch, and T. Delbruck, “A 128 ×128 120 db 15 µ
s latency asynchronous temporal contrast vision sensor,” IEEE J.
Solid-State Circuits , vol. 43, no. 2, pp. 566–576, 2008.
[7] S. B. Shrestha and G. Orchard, “SLAYER: Spike layer error reas-
signment in time,” in Proc. Adv. Neural Inf. Process. Syst. , vol. 31,
2018, p. 1419–1428.
[8] F. Zenke and S. Ganguli, “SuperSpike: Supervised learning in
multilayer spiking neural networks,” Neural Comput. , vol. 30, no. 6,
pp. 1514–1541, 2018.
[9] Y. Wu, L. Deng, G. Li, J. Zhu, and L. Shi, “Spatio-temporal
backpropagation for training high-performance spiking neural
networks,” Front. Neurosci. , vol. 12, p. 331, 2018.
[10] P . Gu, R. Xiao, G. Pan, and H. Tang, “STCA: Spatio-temporal
credit assignment with delayed feedback in deep spiking neural
networks.” in Proc. Int. Jt. Conf. Artif. Intell. , 2019, pp. 1366–1372.
[11] W. Fang, Z. Yu, Y. Chen, T. Masquelier, T. Huang, and Y. Tian,
“Incorporating learnable membrane time constant to enhance
learning of spiking neural networks,” in Proc. IEEE Conf. Comput.
Vis. Pattern Recognit. , 2021, pp. 2661–2671.
[12] J. Wu, C. Xu, X. Han, D. Zhou, M. Zhang, H. Li, and K. C. Tan,
“Progressive tandem learning for pattern recognition with deep
spiking neural networks,” IEEE Trans. Pattern Anal. Mach. Intell. ,
vol. 44, no. 11, pp. 7824–7840, 2021.
[13] J. A. P ´erez-Carrasco, B. Zhao, C. Serrano, B. Acha, T. Serrano-
Gotarredona, S. Chen, and B. Linares-Barranco, “Mapping from
frame-driven to frame-free event-driven vision systems by low-
rate rate coding and coincidence processing–application to feed-
forward convnets,” IEEE Trans. Pattern Anal. Mach. Intell. , vol. 35,
no. 11, pp. 2706–2719, 2013.
[14] Y. Cao, Y. Chen, and D. Khosla, “Spiking deep convolutional
neural networks for energy-efficient object recognition,” Int. J.
Comput. Vis. , vol. 113, no. 1, pp. 54–66, 2015.
[15] P . U. Diehl, D. Neil, J. Binas, M. Cook, S.-C. Liu, and M. Pfeiffer,
“Fast-classifying, high-accuracy spiking deep networks through
weight and threshold balancing,” in Proc. Int. Jt. Conf. Neural Netw. ,
2015, pp. 1–8.
[16] B. Rueckauer, I.-A. Lungu, Y. Hu, M. Pfeiffer, and S.-C. Liu, “Con-
version of continuous-valued deep networks to efficient event-
driven networks for image classification,” Front. Neurosci. , vol. 11,
p. 682, 2017.
[17] Y. Hu, H. Tang, Y. Wang, and G. Pan, “Spiking deep residual
network,” arXiv preprint arXiv:1805.01352 , 2018.
[18] A. Sengupta, Y. Ye, R. Wang, C. Liu, and K. Roy, “Going deeper in
spiking neural networks: VGG and residual architectures,” Front.
Neurosci. , vol. 13, p. 95, 2019.
[19] S. Kim, S. Park, B. Na, and S. Yoon, “Spiking-YOLO: Spiking neu-
ral network for energy-efficient object detection,” in Proc. AAAI
Conf. Artif. Intell. , vol. 34, no. 07, 2020, pp. 11 270–11 277.
[20] B. Han, G. Srinivasan, and K. Roy, “RMP-SNN: Residual mem-
brane potential neuron for enabling deeper high-accuracy and
low-latency spiking neural network,” in Proc. IEEE Conf. Comput.
Vis. Pattern Recognit. , 2020, pp. 13 558–13 567.
[21] S. Deng and S. Gu, “Optimal conversion of conventional artificial
neural networks to spiking neural networks,” in Proc. Int. Conf.
Learn. Representations , 2021.
[22] Z. Yan, J. Zhou, and W.-F. Wong, “Near lossless transfer learning
for spiking neural networks,” in Proc. AAAI Conf. Artif. Intell. ,
vol. 35, no. 12, 2021, pp. 10 577–10 584.
[23] Y. Li, S. Deng, X. Dong, R. Gong, and S. Gu, “A free lunch
from ANN: Towards efficient, accurate spiking neural networks
calibration,” in Proc. Int. Conf. Mach. Learn. , 2021, pp. 6316–6325.
[24] E. O. Neftci, H. Mostafa, and F. Zenke, “Surrogate gradient learn-
ing in spiking neural networks: Bringing the power of gradient-
based optimization to spiking neural networks,” IEEE Signal Pro-
cess. Mag. , vol. 36, no. 6, pp. 51–63, 2019.
[25] J. H. Lee, T. Delbruck, and M. Pfeiffer, “Training deep spiking
neural networks using backpropagation,” Front. Neurosci. , vol. 10,
p. 508, 2016.
[26] C. Lee, S. S. Sarwar, P . Panda, G. Srinivasan, and K. Roy, “Enabling
spike-based backpropagation for training deep neural network
architectures,” Front. Neurosci. , vol. 14, p. 119, 2020.JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 16
[27] Y. Wu, L. Deng, G. Li, J. Zhu, Y. Xie, and L. Shi, “Direct training
for spiking neural networks: Faster, larger, better,” in Proc. AAAI
Conf. Artif. Intell. , vol. 33, no. 01, 2019, pp. 1311–1318.
[28] Y. Kim and P . Panda, “Revisiting batch normalization for training
low-latency deep spiking neural networks from scratch,” Front.
Neurosci. , vol. 15, p. 1638, 2021.
[29] H. Zheng, Y. Wu, L. Deng, Y. Hu, and G. Li, “Going deeper with
directly-trained larger spiking neural networks,” in Proc. AAAI
Conf. Artif. Intell. , vol. 35, no. 12, 2021, pp. 11 062–11 070.
[30] W. Zhang and P . Li, “Temporal spike sequence learning via back-
propagation for deep spiking neural networks,” Proc. Adv. Neural
Inf. Process. Syst. , vol. 33, pp. 12 022–12 033, 2020.
[31] N. Rathi and K. Roy, “DIET-SNN: A low-latency spiking neural
network with direct input encoding and leakage and threshold
optimization,” IEEE Trans. Neural Netw. Learn. Syst. , 2021.
[32] Y. Kim, Y. Li, H. Park, Y. Venkatesha, and P . Panda, “Neural
architecture search for spiking neural networks,” arXiv preprint
arXiv:2201.10355 , 2022.
[33] J. K. Eshraghian, M. Ward, E. Neftci, X. Wang, G. Lenz,
G. Dwivedi, M. Bennamoun, D. S. Jeong, and W. D. Lu, “Training
spiking neural networks using lessons from deep learning,” arXiv
preprint arXiv:2109.12894 , 2021.
[34] F. Zenke and E. O. Neftci, “Brain-inspired learning on neuromor-
phic substrates,” Proc. IEEE , vol. 109, no. 5, pp. 935–950, 2021.
[35] F. Zenke, S. M. Boht ´e, C. Clopath, I. M. Coms ¸a, J. G ¨oltz, W. Maass,
T. Masquelier, R. Naud, E. O. Neftci, M. A. Petrovici et al. ,
“Visualizing a joint future of neuroscience and neuromorphic
engineering,” Neuron , vol. 109, no. 4, pp. 571–575, 2021.
[36] S. Deng, Y. Li, S. Zhang, and S. Gu, “Temporal efficient training of
spiking neural network via gradient re-weighting,” arXiv preprint
arXiv:2202.11946 , 2022.
[37] Y. Venkatesha, Y. Kim, L. Tassiulas, and P . Panda, “Federated
learning with spiking neural networks,” IEEE Trans. Signal Process. ,
vol. 69, pp. 6183–6194, 2021.
[38] Y. Kim and P . Panda, “Optimizing deeper spiking neural networks
for dynamic vision sensing,” Neural Netw. , vol. 144, pp. 686–698,
2021.
[39] Y. Kim, Y. Li, H. Park, Y. Venkatesha, R. Yin, and P . Panda,
“Lottery ticket hypothesis for spiking neural networks,” arXiv
preprint arXiv:2207.01382 , 2022.
[40] A. Bhattacharjee, Y. Venkatesha, A. Moitra, and P . Panda,
“MIME: Adapting a single neural network for multi-task in-
ference with memory-efficient dynamic pruning,” arXiv preprint
arXiv:2204.05274 , 2022.
[41] Y. Kim, Y. Venkatesha, and P . Panda, “PrivateSNN: Fully
privacy-preserving spiking neural networks,” arXiv preprint
arXiv:2104.03414 , 2021.
[42] K. M. Stewart and E. O. Neftci, “Meta-learning spiking neural
networks with surrogate gradient descent,” Neuromorphic Comput.
Eng. , vol. 2, no. 4, p. 044002, 2022.
[43] Y. Kim, J. Chough, and P . Panda, “Beyond classification: Directly
training spiking neural networks for semantic segmentation,”
Neuromorphic Comput. Eng. , vol. 2, no. 4, p. 044015, 2022.
[44] A. Yousefzadeh, S. Hosseini, P . Holanda, S. Leroux, T. Werner,
T. Serrano-Gotarredona, B. L. Barranco, B. Dhoedt, and P . Simoens,
“Conversion of synchronous artificial neural network to asyn-
chronous spiking neural network using sigma-delta quantization,”
inProc. IEEE Int. Conf. Artif. Intell. Circuits Syst. , 2019, pp. 81–85.
[45] C. Zou, X. Cui, J. Ge, H. Ma, and X. Wang, “A novel conversion
method for spiking neural network using median quantization,”
inProc. IEEE Int. Symp. Circuits Syst. , 2020, pp. 1–5.
[46] Y. Li, X. Dong, and W. Wang, “Additive powers-of-two quantiza-
tion: An efficient non-uniform discretization for neural networks,”
inProc. Int. Conf. Learn. Representations , 2020.
[47] T. Han, D. Li, J. Liu, L. Tian, and Y. Shan, “Improving low-
precision network quantization via bin regularization,” in Proc.
IEEE Int. Conf. Comput. Vis. , 2021, pp. 5261–5270.
[48] J. H. Lee, J. Yun, S. J. Hwang, and E. Yang, “Cluster-promoting
quantization with bit-drop for minimizing network quantization
loss,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. , 2021, pp.
5370–5379.
[49] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,
T. Killeen, Z. Lin, N. Gimelshein, L. Antiga et al. , “PyTorch: An
imperative style, high-performance deep learning library,” in Proc.
Adv. Neural Inf. Process. Syst. , vol. 32, 2019, pp. 8026–8037.[50] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep
network training by reducing internal covariate shift,” in Proc. Int.
Conf. Mach. Learn. , 2015, pp. 448–456.
[51] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning
for image recognition,” in Proc. IEEE Conf. Comput. Vis. Pattern
Recognit. , 2016, pp. 770–778.
[52] T. maintainers and contributors, “TorchVision: PyTorch’s com-
puter vision library,” https://github.com/pytorch/vision, 2016.
[53] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classifica-
tion with deep convolutional neural networks,” Proc. Adv. Neural
Inf. Process. Syst. , vol. 25, pp. 1097–1105, 2012.
[54] K. Simonyan and A. Zisserman, “Very deep convolutional
networks for large-scale image recognition,” arXiv preprint
arXiv:1409.1556 , 2014.
[55] A. Krizhevsky and G. Hinton, “Learning multiple layers of fea-
tures from tiny images,” Master’s thesis, Department of Computer
Science, University of Toronto , 2009.
[56] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,
Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and
L. Fei-Fei, “ImageNet Large Scale Visual Recognition Challenge,”
Int. J. Comput. Vis. , vol. 115, no. 3, pp. 211–252, 2015.
[57] K. Desai and J. Johnson, “Virtex: Learning visual representations
from textual annotations,” in Proc. IEEE Conf. Comput. Vis. Pattern
Recognit. , 2021, pp. 11 162–11 173.
[58] M. Horowitz, “Energy table for 45nm process,” Stanford VLSI wiki ,
2014.
[59] Y. Arafa, A. ElWazir, A. ElKanishy, Y. Aly, A. Elsayed, A.-H.
Badawy, G. Chennupati, S. Eidenbenz, and N. Santhi, “Verified
instruction-level energy consumption measurement for NVIDIA
GPUs,” in Proc. ACM Int. Conf. Comput. Front. , 2020, pp. 60–70.
[60] S. Kim, S. Park, B. Na, J. Kim, and S. Yoon, “Towards fast and
accurate object detection in bio-inspired spiking neural networks
through bayesian optimization,” IEEE Access , vol. 9, pp. 2633–
2643, 2020.
[61] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zis-
serman, “The pascal visual object classes (VOC) challenge,” Int. J.
Comput. Vis. , vol. 88, no. 2, pp. 303–338, 2010.
[62] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P . Perona, D. Ramanan,
P . Doll ´ar, and C. L. Zitnick, “Microsoft COCO: Common objects in
context,” in Proc. Eur. Conf. Comput. Vis. , 2014, pp. 740–755.
[63] R. Girshick, J. Donahue, T. Darrell, and J. Malik, “Rich feature hier-
archies for accurate object detection and semantic segmentation,”
inProc. IEEE Conf. Comput. Vis. Pattern Recognit. , 2014, pp. 580–587.
[64] R. Girshick, “Fast R-CNN,” in Proc. IEEE Int. Conf. Comput. Vis. ,
2015, pp. 1440–1448.
[65] S. Ren, K. He, R. Girshick, and J. Sun, “Faster R-CNN: Towards
real-time object detection with region proposal networks,” Proc.
Adv. Neural Inf. Process. Syst. , vol. 28, pp. 91–99, 2015.
[66] K. He, G. Gkioxari, P . Doll ´ar, and R. Girshick, “Mask R-CNN,” in
Proc. IEEE Int. Conf. Comput. Vis. , 2017, pp. 2961–2969.
[67] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You only
look once: Unified, real-time object detection,” in Proc. IEEE Conf.
Comput. Vis. Pattern Recognit. , 2016, pp. 779–788.
[68] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, and
A. C. Berg, “SSD: Single shot multibox detector,” in Proc. Eur. Conf.
Comput. Vis. , 2016, pp. 21–37.
[69] J. Redmon and A. Farhadi, “Yolo9000: better, faster, stronger,” in
Proc. IEEE Conf. Comput. Vis. Pattern Recognit. , 2017, pp. 7263–7271.
[70] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional net-
works for semantic segmentation,” in Proc. IEEE Conf. Comput.
Vis. Pattern Recognit. , 2015, pp. 3431–3440.
[71] V . Badrinarayanan, A. Kendall, and R. Cipolla, “SegNet: A deep
convolutional encoder-decoder architecture for image segmenta-
tion,” IEEE Trans. Pattern Anal. Mach. Intell. , vol. 39, no. 12, pp.
2481–2495, 2017.
[72] O. Ronneberger, P . Fischer, and T. Brox, “U-Net: Convolutional
networks for biomedical image segmentation,” in Proc. Int. Conf.
Med. Image Comput. Comput.-Assisted Intervention , 2015, pp. 234–
241.
[73] G. Lin, A. Milan, C. Shen, and I. Reid, “RefineNet: Multi-path
refinement networks for high-resolution semantic segmentation,”
inProc. IEEE Conf. Comput. Vis. Pattern Recognit. , 2017, pp. 1925–
1934.
[74] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L.
Yuille, “DeepLab: Semantic image segmentation with deep convo-
lutional nets, atrous convolution, and fully connected CRFs,” IEEE
Trans. Pattern Anal. Mach. Intell. , vol. 40, no. 4, pp. 834–848, 2017.JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 17
[75] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, “Pyramid scene parsing
network,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. , 2017,
pp. 2881–2890.
[76] B. Hariharan, P . Arbel ´aez, L. Bourdev, S. Maji, and J. Malik,
“Semantic contours from inverse detectors,” in Proc. IEEE Int. Conf.
Comput. Vis. , 2011, pp. 991–998.
[77] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L.
Yuille, “Semantic image segmentation with deep convolutional
nets and fully connected CRFs,” arXiv preprint arXiv:1412.7062 ,
2014.
[78] L.-C. Chen, G. Papandreou, F. Schroff, and H. Adam, “Rethink-
ing atrous convolution for semantic image segmentation,” arXiv
preprint arXiv:1706.05587 , 2017.