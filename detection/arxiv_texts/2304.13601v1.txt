A Koopman Operator-Based Prediction Algorithm
and its Application to COVID-19 Pandemic‚Ä†
Igor Mezi ¬¥c1,4,*, Zlatko Drma Àác2, Nelida ÀáCrnjari ¬¥c-ÀáZic3, Senka Ma ¬¥ceÀási¬¥c3, Maria Fonoberova4,
Ryan Mohr4, Allan M. Avila1,4, Iva Manojlovi ¬¥c5, and Aleksandr Andrej Àácuk4
1University of California, Santa Barbara, CA, 93106, USA
2Faculty of Science, University of Zagreb, Croatia
3University of Rijeka, Croatia
4AIMdyn Inc., Santa Barbara, CA, 93101, USA
5Department of Applied Mathematics, Faculty of El. Engineering, Univ Zagreb, Croatia
*mezic@ucsb.edu
ABSTRACT
The problem of prediction of behavior of dynamical systems has undergone a paradigm shift in the second half of the 20th
century with the discovery of the possibility of chaotic dynamics in simple, physical, dynamical systems for which the laws of
evolution do not change in time. The essence of the paradigm is the long term exponential divergence of trajectories. However,
that paradigm does not account for another type of unpredictability: the ‚ÄúBlack Swan" event. It also does not account for the fact
that short-term prediction is often possible even in systems with exponential divergence. In our framework, the Black Swan type
dynamics occurs when an underlying dynamical system suddenly shifts between dynamics of different types. A learning and
prediction system should be capable of recognizing the shift in behavior, exempliÔ¨Åed by ‚ÄúconÔ¨Ådence loss". In this paradigm, the
predictive power is assessed dynamically and conÔ¨Ådence level is used to switch between long term prediction and local-in-time
prediction. Here we explore the problem of prediction in systems that exhibit such behavior. The mathematical underpinnings
of our theory and algorithms are based on an operator-theoretic approach in which the dynamics of the system are embedded
into an inÔ¨Ånite-dimensional space. The dynamical switching from global to local prediction algorithm enabled a successful
prediction of inÔ¨Çuenza cases. We show that the framework correctly identiÔ¨Åes the 2009-2010 Ô¨Çu pandemic as a Black Swan
event, that prevented machine learning-based algorithms from showing subsequent good performance. The world has recently
experienced a Black swan event that lead to the COVID-19 pandemic. We deployed our algorithm to assess its evolution. The
results show that, despite being capable of capturing the dynamics of the observed cases of the disease locally, in states and
counties, the prediction algorithm is robust to perturbations of the available data, induced for example by delays in reporting
or sudden increase in cases due to increase in testing capability. This is achieved in an entirely data-driven fashion, with no
underlying mathematical model of the disease. We discuss the prediction problem in other complex dynamics datasets, such
as signature indices of geomagnetic substorms. In addition, fundamental limits on predictability that our theory implies are
discussed.
1 Introduction
Ability for prediction of events is one of the key differentiators of homo sapiens. The key element of prediction is reliance on
collected data over some time interval for estimation of evolution over the next time period. Mathematicians have long worked
on formal aspects of prediction theory, and separate streaks such as the Wiener-Kolmogorov,1, Furstenberg2and Bayesian
prediction3have emerged. However, all of these are concerned with prediction of future events based on a, typically long,
sequence of prior observations. This is rooted in assumptions on statistical stationarity of the underlying stochastic process.
The point of view on prediction in this paper is quite different: we view the process over a short (local) time scale and extract
its coarse-grained ingredients. We proceed with prediction of the evolution based on these, learning the process and building
a global time-scale on which such prediction is valid. Then, we monitor for the change in such coarse-grained ingredients,
detect if a substantial change (a ‚ÄúBlack Swan" event15, see Supplementary Information section S1 for the mathematical
deÔ¨Ånition that we use.) is happening, and switch back to local learning and prediction. In this way, we accept the limitations on
predictability due to, possibly Ô¨Ånite time, nonstationarity, and incorporate them into the prediction strategy. In principle, such
strategy is valuable even in the case where over a long time interval the system is indeed a stationary stochastic process with
respect to some invariant measure. An example is the Lorenz system, the prototypical system exhibiting chaotic dynamics and
the ButterÔ¨Çy Effect51, 56, studied in Supplementary Information section S2.1. For typical learning algorithms Black Swan
‚Ä†Distribution Statement A: Approved for Public Release, Distribution Unlimited.arXiv:2304.13601v1  [math.DS]  26 Apr 2023events are devastating: the learning algorithm has to be restarted as otherwise it would learn the deviation as normal. Our
method includes a technique of retouching the Black Swan event data, wherein its replacement with a realization of the normal
process evolution obviates the need for a restart.
However, the case of Black Swan unpredictability and ButterÔ¨Çy Effect unpredictability are ontologically different. Namely,
the ButterÔ¨Çy Effect is the consequence of dynamics inherent to the system, while the Black Swan arises from an action external
to system dynamics.
We approach the prediction problem from the perspective of Koopman operator theory7, 9‚Äì12, 20in its recently developed
form that is applicable to nonstationary stochastic processes13, 21. The Koopman operator theory is predicated on existence of
the composition operator that dynamically evolves all the possible observables on the data, enabling the study of nonlinear
dynamics by examining its action on a linear space of observables. The key ingredients of this approach become eigenvalues
and eigenfunctions of the Koopman operator and the associated Koopman Mode Decomposition (KMD) of the observable
functions, which is then approximated numerically using Dynamic Mode Decomposition (DMD). The numerical approach used
in this work relies on lifting the available data to higher dimensional space using Hankel-Takens matrix and on the improved
implementation of DMD algorithm for discovering the approximations of the Koopman modes with small residuals. The
obtained Koopman mode approximations and the related eigenvalues, called Ritz pairs, are crucial for obtaining satisfactory
predictions using KMD. One of the main advantages of the method is that it completely data-driven, i.e., model-free.
The retouching method is presented in ¬ß2.2. In ¬ß2.3 we present the application of the prediction algorithm to infection
cases data of the current COVID-19 pandemic.
InSupplementary Information , we provide mathematical background of all algorithms, with worked examples that clarify
all technical details and offer more in depth discussions. Further, we test the predictive potential of the proposed method using
diverse case studies with the goal to stress the data driven (model free) nature of the method. For instance, an application of
the method to prediction of physiological process data, that additionally validates our methodology, could be of interest in
personalized medical treatment context. As further case studies, we use the Lorenz system and the ALindex of geomagnetic
substorms.
2 Results
2.1 Prediction with Koopman Mode Decomposition
Our approach starts with the Koopman operator family Ut, which acts on observables fby composition Utf(x) =f(x(t)).
Utis a linear operator that allows studying the nonlinear dynamics by examining its action on a linear space Fof observables.
In a data driven setting, usually the states zix(ti)of the dynamical system at discrete time moments tiare known. They are
governed by the discrete dynamical system zi+1=T(zi), for which the Koopman operator reads Uf=fT.
The key of the spectral analysis of the dynamical system is a representation of a vector valued observable f= (f1;:::; fd)T
as a linear combination of the eigenfunctions yjofU. Under certain assumptions, each observable fican be approximated
asfi(z)√•m
j=1yj(z)(vj)i, where y1;:::;ymare selected eigenfunctions and vj= ((vj)1:::(vj)d)Tare the vectors of the
coefÔ¨Åcients. Then, we can predict the values of the observable fat the future states T(z),T2(z);:::by numerically evaluating
(Ukf)(z)def=f(Tk(z))m
√•
j=1lk
jyj(z)vj;k=1;2;::: (1)
The decomposition of the observables is called the Koopman Mode Decomposition (KMD ); the scalars ljare the Koopman
eigenvalues, and the vj‚Äôs are the Koopman modes. Their numerical approximations can be computed based on the supplied data
pairs (f(zi);f(T(zi))),i=0;:::; M, using e.g. the Dynamic Mode Decomposition (DMD )28,12,13. In this work, the numerical
algorithm is fed with the sequence of successive data snapshots, called active window, to compute the approximation of KMD,
and then (1) is used for prediction. If we use wider windows and longer forecasting lead time, we speak of global prediction ;
for narrower windows with locally adapted widths and shorter forecasting lead time we have local prediction (nowcasting) .
2.2 Case study: InÔ¨Çuenza epidemics.
As Ô¨Årst example for showing our prediction methodology, we use the set of data associated with inÔ¨Çuenza epidemics. Clearly,
not driven by an underlying deterministic dynamical system, the inÔ¨Çuenza time series exhibits substantial regularity in that
it occurs typically during the winter months, thus enabling coarse-grained prediction of the type ‚Äúwe will see a very small
number of cases of inÔ¨Çuenza occurring in summer months". However, predicting the number of inÔ¨Çuenza cases accurately is
a notoriously hard problem15, exacerbated by the possibility that a vaccine designed in a particular year does not effectively
protect against infection. Moreover, the H1N1 pandemic that occurred in 2009 is an example of a Black Swan event.
The World Health Organization‚Äôs FluNet is a global web-based tool for inÔ¨Çuenza virological surveillance. FluNet makes
publicly available data on the number of specimens with the detected inÔ¨Çuenza viruses of type Aand type B. The data have
2/41been collected from different countries, starting with the year 1997, and are updated weekly by the National InÔ¨Çuenza Centers
(NICs ) of the Global InÔ¨Çuenza Surveillance and Response System ( GISRS ) and other national inÔ¨Çuenza reference laboratories,
collaborating actively with GISRS . We use the weekly reported data for different countries, which consist of the number of
received specimens in the laboratories, the distribution of the number of specimens with conÔ¨Årmed viruses of type A.
The Koopman Mode Decomposition was used in the context of analyzing the dynamics of the Ô¨Çu epidemic from different
- Google Flu - data in24. We remark that the authors of that paper have not attempted prediction, and have analyzed only
‚Äústationary" modes - e.g. the yearly cycles, thus making the paper‚Äôs goals quite different from the nonstationary prediction
pursued here.
We Ô¨Årst compare the global and the local prediction algorithms. The KMD is computed using active windows of size
w=312, and the 208104Hankel-Takens matrices. In Fig.1a, we show the performances of both algorithms, using the
learning data from the window April 2003 ‚Äì April 2009 (shadowed rectangle). In the global prediction algorithm the dynamics
is predicted for 104weeks ahead. The Ô¨Årst type of failure in the global prediction algorithm and forecasting appears after the
Black Swan event occurred in the years 2009 and 2010. This is recognized by the algorithm, so that it adapts by using the
smallest learning span and, with this strategy, it allows for reasonably accurate forecasting, at least for shorter lead times. This
data, in addition to those from Supplementary Information section S2.4 show the beneÔ¨Åts of monitoring the prediction error
and switching to local prediction. The initial Hankel-Takens matrix is 32, and the threshold for the local prediction relative
error in Supplementary Information Algorithm S4 is 0 :005.
(a)Local prediction
(b)Global prediction with retouching the Black Swan event data
Figure 1. InÔ¨Çuenza data (USA). (1a): The data are collected in the window April 2003 ‚Äì April 2009 (shadowed rectangle) and
then the dynamics is predicted for 104 weeks ahead. The local prediction algorithm recovers the prediction capability by
forgetting the old data and using narrower learning windows. The local prediction algorithm delivers prediction for one week
ahead. (1b): The active window (shadowed rectangle) is July 2004 ‚Äì July 2010, and the dynamics is predicted for 104 weeks
ahead. The global prediction fails due to the Black Swan data in the learning window. (Some predicted values were even
negative; those were replaced with zeros.) The global prediction algorithm recovers after the retouching the Black Swan event
data, which allows for using big learning window. Compare with positions of the corresponding colored rectangles in Figure 2.
3/41Retouching the Black Swan event data
Next, we introduce an approach that robustiÔ¨Åes the global algorithm in the presence of disturbances in the data, including
the missing data scenario. We use the data window July 2004‚ÄìJuly 2010, which contains a Black Swan event in the period
2009‚Äì2010. As shown in Figure 1b, the learned KMD failed to predict the future following the active training window. This is
expected because the perturbation caused by the Black Swan event resulted in the computed Ritz pairs that deviated from the
precedent ones (from a learning window before disturbance), and, moreover, with most of them having large residuals. This can
be seen as a second type of failure in the global prediction.
The proposed Black Swan event detecting device, built in the prediction algorithm (see Supplementary Information
Algorithm S3), checks for this anomalous behaviour of the Ritz values and pinpoints the problematic subinterval. Then, the
algorithm replaces the corresponding supplied data with the values obtained as predictions based on the time interval preceding
the Black Swan event. Figure 1b shows that such a retouching of the disturbance allows for a reasonable global prediction.
Note that in a realistic situation, global predictions of this kind will trigger response from authorities and therefore prevent
its own accuracy and induce loss of conÔ¨Ådence, whereas local prediction mechanisms need to be deployed again.
Figure 2. The real and imaginary parts of Ritz values with residuals bellow hr=0:075for sliding active windows. The color
intensity of eigenvalues indicates the amplitudes of the corresponding modes. Pink rectangles mark ends of training windows
with no acceptable Ritz values. Note how the unstable eigenvalues ( √Ç(l)>0) impact the prediction performance, and how the
retouching moves them towards neutral/stable ‚Äì this is shown in the yellow rectangle in panels (a) and (c). Also inÔ¨Çuenced by
the disturbance are the eigenvalues in the light blue rectangles in panels (a), (b); retouching moves the real parts of eigenvalues
towards neutral/stable and rearranges them in a lattice-like structure22, as shown in panels (c), (d). Compare with Figure 1b.
Monitoring and restoring the Ritz values
We now discuss the effect of the Black Swan event and its retouching to the computed eigenvalues and eigenvectors. We have
observed that, as soon as a disturbance starts entering the training windows, the Ritz values start exhibiting atypical behavior,
e.g. moving deeper into the right half plane (i.e. becoming more unstable), and having larger residuals because the training data
no longer represent the Krylov sequence of the underlying Koopman operator.
This is illustrated in the panels (a) and (b) in Figure 2, which show, for the sliding training windows, the real and the
imaginary parts of those eigenvalues for which the residuals of the associated eigenvectors are smaller than hr=0:075. Note
the absence of such eigenvalues in time intervals that contain the disturbance caused by the Black Swan event.
On the other hand, the retouching technique that repairs the distorted training data restores the intrinsic dynamics over
4/41the entire training window. The distribution of the relevant eigenvalues becomes more consistent, and the prediction error
decreases, see panels (c) and (d) in Figure 2, and in Supplementary Information Figure S16.
Discussion
Our proposed retouching procedure relies on detecting anomalous behavior of the Ritz values; a simple strategy of monitoring
the spectral radius of active windows (absolutely largest Ritz value extracted from the data in that window) is outlined in
Supplementary Information . Note that this can also be used as a litmus test for switching to the local prediction algorithm.
InSupplementary Information , we provide further examples, with the inÔ¨Çuenza data, that conÔ¨Årm the usefulness of the
retouching procedure. In general, this procedure can also be adapted to the situation when the algorithm receives a signal that
the incoming data is missing or corrupted.
2.3 COVID-19 prediction.
The second set of data we consider is that associated with the ongoing COVID-19 pandemic. Because the virus is new, the
whole event is, in a sense, a ‚ÄúBlack Swan". However, as we show below, the prediction approach advanced here is capable of
adjusting quickly to the new incoming, potentially sparse data and is robust to inaccurate reporting of cases.
At the beginning of the spread of COVID-19, we have witnessed at moments rather chaotic situation in gaining the
knowledge on the new virus and the disease. The development of COVID-19 diagnostic tests made tracking and modelling
feasible, but with many caveats: the data itself is clearly not ideal, as it depends on the reliability of the tests, testing
policies in different countries (triage, number of tests, reporting intervals, reduced testing during the weekends), contact
tracing strategies, using surveillance technology, credit card usage and phone contacts tracking, the number of asymptomatic
transmissions etc. Many different and unpredictable exogenous factors can distort it. So, for instance the authors of26comment
athttps://ourworldindata.org/coronavirus-testing that e.g. "The Netherlands, for instance, makes it clear
that not all labs were included in national estimates from the start. As new labs get included, their past cumulative total gets
added to the day they begin reporting, creating spikes in the time series." For a prediction algorithm, this creates a Black Swan
event that may severely impair prediction skills, see ¬ß2.2.
This poses challenging problems to the compartmental type models of (SIR, SEIR) which in order to be useful in practice
have to be coupled with data assimilation to keep adjusting the key parameters, see e.g.27. Our technique of retouching (¬ß2.2)
can in fact be used to assist data assimilation by detecting Black Swan disturbance and thus to avoid assimilating disturbance as
normal.
In the KMD based framework, the changes in the dynamics are automatically assimilated on-the-Ô¨Çy by recomputing the
KMD using new (larger or shifted) data snapshot windows. This is different from the compartmental type models of infectious
diseases, most notably in the fact that the procedure presented here does not assume any model and, moreover, that it is entirely
oblivious to the nature of the underlying process.
An example: European countries
As a Ô¨Årst numerical example, we use the reported cumulative daily cases in European countries. In Supplementary Information
section S1.5, we use this data for a detailed worked example that shows all technical details of the method. This is a good
test case for the method ‚Äì using the data from different countries in the same vector observable poses an additional difÔ¨Åculty
for a data driven revealing of the dynamics, because the countries independently and in an uncoordinated manner impose
different restrictions, thus changing the dynamics on local levels. For instance, at the time of writing these lines, a new and
seemingly more infectious strain of the virus circulating in some parts of London and in south of England prompted the UK
government to impose full lockdown measures in some parts of the United Kingdom. Many European countries reacted sharply
and immediately suspended the air trafÔ¨Åc with the UK.
In the Ô¨Årst numerical experiment, we use two datasets from the time period February 29 to November 19. and consider
separately two sets of countries: Germany, France and the UK in the Ô¨Årst, and Germany, France, UK, Denmark, Slovenia,
Czechia, Slovakia and Austria in the second. The results for a particular prediction interval are given in Figure 3 and Figure
4. For more examples and discussion how the prediction accuracy depends on the Government Response Stringency Index
(GRSI47, 48) see Supplementary Information section S1.5. In the above examples, the number of the computed modes was
equal to the dimension of the subspace of spanned by the training snapshots, so that the KMD of the snapshots themselves was
accurate up to the errors of the Ô¨Ånite precision arithmetic. In general, that will not be the case, and the computed modes will
span only a portion the training subspace, meaning that the KMD of the snapshots might have larger representation error. (Here
we refer the reader to Supplementary Information section S1.3, where all technical details are given.) This fact has a negative
impact to the extrapolation forward in time and the problem can be mitigated by giving more importance to reconstruction of
more recent weights. This is illustrated in Figures 5 and 6, where the observables are the raw data (reported cases) for Germany,
extended by a two additional sequence of Ô¨Åltered (smoothened) values.
5/41Figure 3. Prediction of COVID-19 cases (35 days ahead, starting July 11) for Germany, France and United Kingdom. Left
panel : The Hankel-Takens matrix His282172, the learning data consists of h1:40. The KMD uses 39modes. Middle panel :
The matrix His 363145, the learning data is h1:13. The KMD uses 12 modes. Right panel : The Koopman-Ritz values
corresponding to the Ô¨Årst (magenta circles) and the middle (blue plusses) panel. Note how the three rightmost values nearly
match.
The Ô¨Ågures illustrate an important point in prediction methodology, that we emphasized in the introduction: a longer dataset
and a better data reconstruction ability (i.e. interpolation) does not necessarily lead to better prediction. Namely, weighting
more recent data more heavily produces better prediction results. This was already observed in30for the case of trafÔ¨Åc dynamics,
and the method we present here can be used to optimize the prediction ability.
An example: USA and worldwide data
We have deployed the algorithm to assess the global and United States evolution of the COVID-19 pandemic. The evolution of
the virus is rapid, and "Black Swans" in the sense of new cases in regions not previously affected appear with high frequency.
Despite that, the Koopman Mode Decomposition based algorithm performed well.
In Figure 7a we show the worldwide forecast number of conÔ¨Årmed cases produced by the algorithm for November 13th,
2020. The forecasts were generated by utilizing the previous three days of data to forecast the next three days of data for regions
with higher than 100 cases reported. The bubbles in Ô¨Ågure 7a are color coded according to their relative percent error. As can
be observed, a majority of the forecasts fell below below 15% error. The highest relative error for November 13th, 2020 was
19.8% which resulted from an absolute error of 196 cases. The mean relative percent error, produced by averaging across all
locations, is 1.8% with a standard deviation of 3.36% for November 13th, 2020. Overall, the number of conÔ¨Årmed cases are
predicted accurately and since the forecasts were available between one to three days ahead of time, local authorities could very
well utilize our forecasts to focus testing and prevention measures in hot-spot areas that will experience the highest growth.
A video demonstrating the worldwide forecasts for March 25, 2020 - November 29, 2020 is provided in the Supplementary
Information online (Figure 7a is a snapshot from that video). Lastly, it is well known that the ability to test people for the
virus increased throughout the development of the pandemic and thus resulted in changes in the dynamics of reported cases.
Although it is impossible for a data-driven algorithm to account for changes due to external factors, such as increased testing
capabilities, it is important that the algorithm be able to adjust and relearn the new dynamics. For this reason, we encourage the
reader to reference the video and note that although periods of inaccuracy due to black swan events occur, the algorithm is
always able to stabilize and recover. In contrast, since this is at times a rapidly (exponentially) growing set of data, methods
like naive persistence forecast do poorly.
In Figures 7b, 7c we show the performance of the prediction for the cumulative data for the US in March-April 2020. It is
of interest to note that the global curve is obtained as a sum of local predictions shown in Ô¨Ågure 7a, rather than as a separate
algorithm on the global data. Again, the performance of the algorithm on this nonstationary data is good.
3 Discussion
In this work, we have presented a new paradigm for prediction in which the central tenet is understanding of the conÔ¨Ådence with
which the algorithm is capable of predicting the future realizations of a non-stationary stochastic process. Our methodology is
based on Koopman operator theory20. Operator-theoretic methods have been used for detection of change in complex dynamics
in the past, based on both Koopman1, 32and Perron-Frobenius operators33. Other methods include variational Ô¨Ånite element
techniques combined with information theoretic measure (Akaike‚Äôs information criterion) and maximum entropy principle50.
6/41Figure 4. Prediction errors and KMD spectrum of COVID-19 cases (28 days ahead, starting July 11) for Germany, France,
United Kingdom, Denmark, Slovenia, Czechia, Slovakia and Austria. Left panel : The Hankel-Takens matrix His 752172,
the learning data consists of h1:40. The KMD uses 39 modes. Middle panel : The matrix His 968145, the learning data is
h1:13. The KMD uses 12modes. Right panel : The Koopman-Ritz values corresponding to the Ô¨Årst two computations in Figure
3 (magenta circles and blue pluses, respectively) and the the Ô¨Årst two panels in this Figure (orange x-es and cyan squares,
respectively). Note how the corresponding Koopman-Ritz values nearly match for all cases considered.
Our approach to the problem of prediction of nonstationary processes has several key ingredients. First, the Koopman
operator on the space of the observables is used as a global linearization tool, whose eigenfunctions provide a coordinate system
suitable for representation of the observables. Second, in a numerical computation, we lift the available snapshots to a higher
dimensional Hankel-Takens structure, which in particular in the case of abundance of data, allows for better numerical (Ô¨Ånite
dimensional) Rayleigh-Ritz approximation of eigenvalues and eigenvectors of the associated Koopman operator, as well as the
KMD . Third, using our recent implementation of the DMD , we select the Koopman modes that have smallest residuals, and
thus highest conÔ¨Ådence, which is the key for the prediction capabilities of the KMD . In the absence of enough modes with
reasonably small residuals, i.e. low conÔ¨Ådence, we switch to local prediction, with narrower learning windows and shorter lead
time. By monitoring the prediction error, the algorithm may return back to global prediction.
Our methodology is entirely consistent with the typical training/test dataset validation techniques in machine learning.
Namely, the globally learned model on the training data is applied to test data for the next time interval. The novelty in our
approach is that we constantly check for how well the learned model generalizes, and if it does not generalize well, we restart
the learning. One can say that we implemented a feedback loop, within which the machine learning algorithm‚Äôs generalizability
from training to test dataset is constantly checked, and the system adapts to new conditions. Evidence for effectiveness of this
procedure is presented for the COVID-19 prediction example, where we show how the generalization error diminishes over
time.
4 Methods
Our starting assumption is that observed data is generated by a dynamical process realized on some underlying state space.
This is a broad enough assumption to cover data generated by both deterministic and stochastic dynamical systems11. The
(internal) state is often inaccessible; instead, an observable (output) is given as a function f(x(t))of the state vector x(t).
4.1 The Koopman operator and the KMD
The Koopman operator family Ut, acts on observables fby composition Utf(x) =f(x(t)). It is a global linearization tool:
Utis a linear operator that allows studying the nonlinear dynamics by examining its action on a linear space Fof observables.
In data analysis, for the discrete time steps ti, the discrete sequence zix(ti), generated as numerical software output, is then a
discrete dynamical system zi+1=T(zi), for which the Koopman operator reads Uf=fT.
The key of the spectral analysis of the dynamical system is a representation of a vector valued observable f= (f1;:::; fd)T
as a linear combination of the eigenfunctions yjofU. In a subspace spanned by eigenfunctions each observable fican be
written as fi(z)√•¬•
j=1yj(z)(vj)iand thus (see e.g.16, 20)
f(z) = f1(z)
...
fd(z)!
¬•
√•
j=1yj(z)vj;where vj=0
@(vj)1
...
(vj)d1
A; (2)
7/41Figure 5. Prediction experiment with data from Germany. Left panel : the computed residuals for the computed 102Koopman
Ritz pairs (extracted from a subspace spanned by 132 snapshots h1:132). Note that all residuals are small. The corresponding
Ritz values are shown in the Ô¨Årst panel in Figure 6. Middle panel :KMD reconstruction error for h1:132 and the error in the
predicted values h133:160 (encircled with). The reconstruction is based on the coefÔ¨Åcients
(aj)r
j=1=argminaj√•kkhk √•r
j=1lk
jajvjk2
2.Right panel : Prediction errors for the period October 11 ‚Äì November 7.
then, since Uyj=ljyj, we can envisage the values of the observable fat the future states T(z),T2(z);:::by
(Ukf)(z)def=f(Tk(z))¬•
√•
j=1lk
jyj(z)vj;k=1;2;::: (3)
The numerical approximation of KMD can be computed using for example DMD algorithms. Different versions of the algorithm
used in this work are described in details in Supporting Information-Methods .
4.2 Finite dimensional compression and Rayleigh-Ritz extraction
For practical computation, Uis restricted to a Ô¨Ånite dimensional space FDspanned by the dictionary of suitably chosen
functions D=ff1;:::; fdg, and we use a matrix representation Uof the compression YFDUjFD:FD!FD, where YFDis
aL2projection e.g. with respect to the empirical measure deÔ¨Åned as the sum of the Dirac measures concentrated at the zi‚Äôs.
SinceUis the adjoint of the DMD matrixAassociated with the snapshots zi, the approximate (numerical) Koopman modes and
the eigenvalues are the Ritz pairs (Ritz eigenvalues and eigenvectors) of A, computed using the Rayleigh-Ritz method. The
residuals of the Ritz pairs can be computed and used to check the accuracy12. See Supporting Information-Methods .
4.3 The Hankel-DMD (H-DMD)
The data snapshots (numerical values of the observables) can be rearranged in a Hankel-Takens matrix structure: for a
subsequence ( window ) ofwsuccessive snapshots fb;fb+1;:::; fw 1, split w=mH+nHand then deÔ¨Åne new snapshots as the
columns hiof the nHmHHankel-Takens matrix (see7, 22, 45, and Supporting Information )
H=0
BBB@fb fb+1 fb+mH
fb+1 fb+2 fb+mH+1
............
fb+nH 1fb+nH fb+nH+mH 11
CCCA= 
h1::: hmH+1
:
Then, for this data we compute the KMD and use (1) for prediction. Predictions of the observables fiare then extracted from
the predicted values of the observables hi.
The introduction of Hankel-Takens matrix alleviates issues that arise from using a basis on a potentially high dimensional
space: namely, taking products of basis elements on 1-dimensional subspaces - for example Fourier basis on an interval in R.
Such constructions lead to an exponential growth in the number of basis elements, and the so-called curse of dimensionality.
The Hankel-Takens matrix is based on the dynamical evolution of a one or more observables - functions on state space - that
span a Krylov subspace. The idea is that one might start even with a single observable, and due to its evolution span an invariant
subspace of the Koopman operator (note the connection of such methods with the Takens embedding theorem ideas1, 7, 22).
Since the number of basis elements is in this case equal to the number of dynamical evolution steps, in any dimension, Krylov
subspace-based methods do not suffer from the curse of dimensionality.
8/41Figure 6. Prediction experiment with DS3 with data from Germany. Left panel : the computed 102 Koopman Ritz values
(extracted from a subspace spanned by 132snapshots h1:132). The corresponding residuals are shown in the Ô¨Årst panel in Figure
5.Middle panel :KMD reconstruction error for h1:132 and the error in the predicted values h133:160 (encircled with). The
reconstruction is based on the coefÔ¨Åcients (aj)r
j=1=argminaj√•kw2
kkhk √•r
j=1lk
jajvjk2
2.Right panel : Prediction errors for
the period October 11 ‚Äì November 7. Compare with the third graph in Figure 5.
References
1.JL Doob, Stochastic processes . (New York Wiley) V ol. 101, (1953).
2.H Furstenberg, H Furstenberg, Stationary processes and prediction theory . (Princeton University Press), (1960).
3.A Pole, M West, J Harrison, Applied Bayesian forecasting and time series analysis . (Chapman and Hall/CRC), (2018).
4.NN Taleb, The Black Swan: The Impact of the Highly Improbable . (Random House Group), (2007).
5.EN Lorenz, Deterministic nonperiodic Ô¨Çow. J. atmospheric sciences 20, 130‚Äì141 (1963).
6.S Luzzatto, I Melbourne, F Paccaut, The Lorenz attractor is mixing. Commun. Math. Phys .260, 393‚Äì401 (2005).
7.BO Koopman, Hamiltonian systems and transformation in Hilbert space. Proc. national academy sciences united states
america 17, 315 (1931).
8.I Mezi ¬¥c, Spectral properties of dynamical systems, model reduction and decompositions. Nonlinear Dyn .41, 309‚Äì325
(2005).
9.JC Hua, F Noorian, D Moss, PH Leong, GH Gunaratne, High-dimensional time series prediction using kernel-based
Koopman mode regression. Nonlinear Dyn .90, 1785‚Äì1806 (2017).
10.D Giannakis, S Das, Extraction and prediction of coherent patterns in incompressible Ô¨Çows through space‚Äìtime Koopman
analysis. Phys. D: Nonlinear Phenom .402, 132211 (2020).
11.M Korda, I Mezi ¬¥c, Linear predictors for nonlinear dynamical systems: Koopman operator meets model predictive control.
Automatica 93, 149‚Äì160 (2018).
12.M Khodkar, AC Antoulas, P Hassanzadeh, Data-driven spatio-temporal prediction of high-dimensional geophysical
turbulence using Koopman operator approximation. arXiv preprint arXiv:1812.09438 (2018).
13.NÀáCrnjari ¬¥c-≈Ωic, S Ma ¬¥ce≈°i¬¥c, I Mezi ¬¥c, Koopman operator spectrum for random dynamical systems. J. Nonlinear Sci . pp.
1‚Äì50 (2017).
14.I Mezi ¬¥c, Spectrum of the Koopman operator, spectral expansions in functional spaces, and state-space geometry.
functional spaces, and state-space geometry. J. Nonlinear Sci . pp. 1‚Äì55 (2019).
15.D Lazer, R Kennedy, G King, A Vespignani, The parable of google Ô¨Çu: traps in big data analysis. Science 343, 1203‚Äì1205
(2014).
16.I Mezi ¬¥c, Analysis of Ô¨Çuid Ô¨Çows via spectral properties of the Koopman operator. Annu. Rev. Fluid Mech .45, 357‚Äì378
(2013).
17.P Schmid, Dynamic mode decomposition of numerical and experimental data. J. Fluid Mech .656, 5‚Äì28 (2010).
9/41(a)Worldwide predicted cases and prediction error for COVID-19 pandemic on November 13, 2020.
(b)Data and prediction for US number of COVID-19 cases.
 (c)Prediction error for US number of COVID-19 cases.
Figure 7. Prediction of conÔ¨Årmed COVID-19 cases utilizing the publicly available COVID-19 data repository provided by
Johns Hopkins. The true data ranges between March 22nd, 2020 and November 29th, 2020. We utilize the last three days of
data to forecast the following three days of data. (7a) Predicted conditions and prediction error worldwide on November 13.
The widths of the bubbles represent the number of cases in a region; only regions with more that 100 cases are used and the
bubbles are colored according to their relative percent error. (7b) Comparison of true and forecast data for cumulative
conÔ¨Årmed cases in the US for April to December 2020. The cumulative forecasts shown here were obtained by summing the
forecasts of the individual locations, indicating that the region speciÔ¨Åc forecasts were sufÔ¨Åciently accurate for tracking the
cumulative dynamics of the virus in the US. (7c) Percent error for the forecasts of the cumulative conÔ¨Årmed cases in the US. On
average the percent error is less than 5 percent and although spikes occur, which could be due to changes in testing availability,
the algorithm adjusts and the error stabilizes within a short amount of time. Furthermore, Johns Hopkins provided data for
around 1787 locations around the United States and we produced forecasts for each of those locations.
18.Z Drma Àác, I Mezi ¬¥c, R Mohr, Data driven modal decompositions: Analysis and enhancements. SIAM J. on Sci. Comput .40,
A2253‚ÄìA2285 (2018).
19.Z Drma Àác, I Mezi ¬¥c, R Mohr, Data driven Koopman spectral analysis in Vandermonde‚ÄìCauchy form via the DFT: Numerical
method and theoretical insights. SIAM J. on Sci. Comput .41, A3118‚ÄìA3151 (2019).
20.JH Tu, CW Rowley, DM Luchtenburg, SL Brunton, JN Kutz, On dynamic mode decomposition: theory and applications.
J. Comput. Dyn .1, 391‚Äì421 (2014).
21.H Arbabi, I Mezi ¬¥c, Ergodic theory, dynamic mode decomposition, and computation of spectral properties of the Koopman
operator. SIAM J. on Appl. Dyn. Syst .16, 2096‚Äì2126 (2017).
22.I Mezi ¬¥c, On numerical approximations of the Koopman operator. Mathematics 10(7), 1180 (2022).
23.I Mezi ¬¥c, A Banaszuk, Comparison of systems with complex behavior. Phys. D: Nonlinear Phenom .197, 101‚Äì133 (2004).
24.JL Proctor, PA Eckhoff, Discovering dynamic patterns from infectious disease data using dynamic mode decomposition.
Int. health 7, 139‚Äì145 (2015).
10/4125.I Mezi ¬¥c, Spectrum of the Koopman operator, spectral expansions in functional spaces, and state-space geometry. J.
Nonlinear Sci . (2019).
26.J Hasell, et al., A cross-country database of COVID-19 testing. Sci. Data 7(2020).
27.P Nadler, S Wang, R Arcucci, X Yang, Y Guo, An epidemiological modelling approach for COVID-19 via data
assimilation. Eur. J. Epidemiol .35, 749‚Äì761 (2020).
28.T Hale, S Webster, A Petherick, T Phillips, B Kira, Oxford COVID-19 government response tracker, (Blavatnik School of
Government), Technical report (2020).
29.A Petherick, et al., Variation in government responses to COVID-19, (Blavatnik School of Government), Technical Report
BSG-WP-2020/032 (2020).
30.A Avila, I Mezi ¬¥c, Data-driven analysis and forecasting of highway trafÔ¨Åc dynamics. Nat. communications 11, 1‚Äì16
(2020).
31.I Rahimi, F Chen, AH Gandomi, A review on covid-19 forecasting models. Neural Comput. Appl . pp. 1‚Äì11 (2021).
32.I Mezi ¬¥c, A Banaszuk, Comparison of systems with complex behavior: Spectral methods in Proceedings of the 39th IEEE
Conference on Decision and Control (Cat. No. 00CH37187) . (IEEE), V ol. 2, pp. 1224‚Äì1231 (2000).
33.JH Prinz, et al., Markov models of molecular kinetics: Generation and validation. The J. chemical physics 134, 174105
(2011).
34.P Metzner, L Putzig, I Horenko, Analysis of persistent nonstationary time series and applications. Commun. Appl. Math.
Comput. Sci. 7, 175‚Äì229 (2012).
Acknowledgements
This work was partially supported under DARPA contract HR001116C0116, DARPA contract HR00111890033, NIH/NIAAA
grant R01AA023667, and DARPA SBIR Contract No. W31P4Q- 21-C-0007. Any opinions, Ô¨Åndings and conclusions or
recommendations expressed in this material are those of the authors and do not necessarily reÔ¨Çect the views of the DARPA
SBIR Program OfÔ¨Åce. The support of scientiÔ¨Åc research of the University of Rijeka, project No. uniri-prirod-18-118-1257, and
the Croatian Science Foundation through grant IP-2019-04-6268.
Distribution Statement A: Approved for Public Release, Distribution Unlimited.
Author contributions statement
I.M. conceptualized the prediction algorithm, analyzed data, and wrote parts of the paper. Z.D. worked on the numerical
algorithms and writing of some parts of the paper. N.C.Z. and S.M. designed parts of the prediction algorithm, participated
in developing methodology and in the results, analysis, contributed to the preparation of the paper. M.F. participated in
methodology development, data preparation and analysis, contributed to the preparation of the paper. R.M. helped develop the
algorithm and prepared parts of the paper. A..M.A. helped write a part of the manuscript and produced the COVID forecasting
results. I.V . and A.A. were responsible for numerical experiments.
Data Availability Statement
The raw COVID-19 data is made publicly available by the Center for Systems Science and Engineering (CSSE) at Johns Hopkins
University at https://github.com/CSSEGISandData/COVID-19 . The raw InÔ¨Çuenza data is made publicly avail-
able by the World Health Organization at https://www.who.int/influenza/gisrs_laboratory/flunet/
en/. The raw geomagnetic storm data is made publicly available by the National Aeronautics and Space Administration at
https://omniweb.gsfc.nasa.gov/form/omni_min.html .
Competing interests
The authors declare no competing interests.
11/41Supplementary Information for
A Koopman Operator-Based Prediction Algorithm and its Application to
COVID-19 Pandemic
Igor Mezi ¬¥c, Zlatko Drma Àác, Nelida ÀáCrnjari ¬¥c-≈Ωic, Senka Ma ¬¥ce≈°i¬¥c, Maria Fonoberova, Ryan Mohr,
Allan M. Avila, Iva Manojlovi ¬¥c, Aleksandr Andrej Àácuk
S1 Methods
In this section we provide technical details of the numerical spectral analysis of dynamical systems, which is at the core
of the prediction algorithms presented in this work. The tools of trade are the Koopman operator, the KMD and the DMD
decompositions. We Ô¨Årst present a compact tutorial on the numerical aspects of the Koopman modal analysis of nonlinear
dynamical systems, and, then, we provide theoretical underpinnings for the methods presented in the paper.
We do not go into the details of convergence of the Koopman operator approximations utilized in the paper, but we mention
the associated work. For example, for on-attractor evolution, the properties of the Generalized Laplace Analysis (GLA)
method acting on L2functions were studied in1, 2. The off-attractor case was pursued in3in Hardy-type spaces. This study was
continued in4to construct dynamics-adapted Hilbert spaces. A study of convergence of DMD-type approximations utilized in
this paper is provided in5. There are two types of results presented in these papers 1) Convergence of the spectral objects over
an inÔ¨Ånite time interval and 2) rate of convergence to spectral objects. Since we are pursuing a Ô¨Ånite-time analysis, the results
of type 2) are more relevant. The gist of these results is that the convergence rate is 1=nfor regular dynamics (limit cycles,
limit tori), where nis the number of snapshots, and 1=p
(n)for irregular (chaotic) dynamics. This can be improved to even
exponential convergence under some conditions6. While the proofs indicated here are for GLA, and depend on convergence
time averages over trajectories, they can be extended for DMD methods since in general they can be related to time averages
over trajectories7.
Since the prediction methods in the paper are tightly connected to detection of ‚ÄúBlack Swan" events, and these are often
deÔ¨Åned in imprecise terms, we provide a mathematical deÔ¨Ånition that we utilize in this work for the orientation of the reader:
DeÔ¨Ånition S1.1 Letf:M!C,f2HandK:H!Han operator from the Hilbert space Hto itself. Consider the dynamics
given by f0=K f:Letmbe an ergodic invariant measure for K, i.e.
Z
Mf dm=lim
n!¬•1
nn 1
√•
j=0Kjf(x); (S4)
for almost all x2M. Assume the support Sofmis such that S6=M. A Black Swan event for an observable fisg=2f(S). The
magnitude of the Black Swan event for observation g is d (g;f(S))where d is a metric, and f (S)is the range of f on S.
The set Mdoes not necessarily need to be the state space. In the examples shown in the paper, the observables are spectral (e.g.
the spectral radius), M=Cand the operator Kis the operator acting on the spectral objects induced by the Koopman evolution.
For a non-degenerate stochastic process with an ergodic measure mit can be unlikely that the support of mis different from
M. In that case, the "BlackSwannes" of a value can be deÔ¨Åned as d(g;n), where dis e.g. the Wasserstein distance of a delta
distribution at fandn(E) =m(f 1(E))is the pushforward measure under f8. Note that this particular dcan be used in the
deterministic case described previously.
S1.1 Organization of the section
This section is organized as follows. First, in ¬ßS1.2 we review the continuous and the discrete autonomous dynamical systems,
and the deÔ¨Ånition of the Koopman operator Uon the space of observables. The review material is based on the papers on the
theory and applications of the Koopman operator9,10,11, and our recent work12,13,14. The spatio-temporal representation of the
evolution of the observables using the eigenvalues and eigenvectors of the Koopman operator (Koopman mode decomposition,
KMD ) is discussed in ¬ßS1.2.1. The next key ingredient, numerical computation of approximate eigenvalues and eigenvectors, is
reviewed in ¬ßS1.3. In particular, the details of the matrix representation of a compression of Uto the subspace of observables
are worked out in ¬ßS1.3.2; numerical realization of the Koopman mode decomposition is presented in detail in ¬ßS1.3.3 and
¬ßS1.3.4. In ¬ßS1.3.5 we provide the key elements of the Schmid‚Äôs DMD algorithm, and in ¬ßS1.3.6 its recent enhancement
that allows selection of Ritz pairs that can be used for a KMD suitable for prediction. In ¬ßS1.3.7 we review the least squares
methods for spatio-temporal representation of the snapshots using the selected Koopman modes. And Ô¨Ånally, after having
prepared all necessary ingredients, in ¬ßS1.4 we present the global prediction algorithm. The setup of the prediction framework
12/41is given in ¬ßS1.4.1 which introduces basic notation and ¬ßS1.4.2, where we lift the data snapshots in a Hankel structure that will
be used in the algorithms. In ¬ßS1.4.3, we discuss the limitations of the numerical realization of the KMD based prediction
scheme introduced in ¬ßS1.2.1. A worked example that illustrates technical details is provided in ¬ßS1.5, where we apply the
prediction scheme to the spread of the coronavirus disease in European countries. In ¬ßS1.7, we discuss the problem of Black
Swan events15in the training data, and we propose a novel technique, based on the method reviewed in ¬ßS1.3.6, for detecting
and retouching Black Swan type disturbances of the data. This makes the global prediction scheme resilient to sudden and
unpredictable disturbances that have become part of the learning data window. In ¬ßS1.7.2, we present a more Ô¨Çexible local
prediction scheme that dynamically resizes the learning data windows and the forecasting lead time in an event of sudden
changes and large prediction errors.
S1.2 Setting the scene: Koopman operator
Consider an autonomous system of differential equations
Àôx(t) =F(x(t)) F1(x(t))
...
FN(x(t))!
; (S5)
with state space Xand vector-valued nonlinear function F. HereXis a compact smooth N-dimensional manifold, endowed
with a Borel sigma algebra B, and for simplicity identiÔ¨Åed with a subset of RN, with F:X !RN. The associated Ô¨Çow map
jt:X !Xadvances an initial state x(t0)forward in time by a time unit t,
x(t0+t) =jt(x(t0)) = x(t0)+Zt0+t
t0F(x(t))dt: (S6)
Note that jt+s=jtjs, wheredenotes the composition of mappings. The (internal) state is often inaccessible; instead an
observable (output) is given as a function f:X !Cof the state, where the class (function space) F3fof observables is
appropriately chosen and endowed with a Banach or Hilbert space structure. For more detailed introduction we refer to16, in
particular Chapters II and V , and17. For instance, we can take F=Lp(X;m),1p¬•, with an appropriate measure mand
e.g. for p=2 with the corresponding Hilbert space structure.
The Koopman operator semigroup (Ujt)t0is deÔ¨Åned by
Ujtf=fjt;f2F: (S7)
Here we assume that jtpreserves sets of measure zero (if m(A) =0, then m((jt) 1)(A) =0) and that Ujtis deÔ¨Åned on the
equivalency classes (modulo m). It can be considered as a linearization tool for (S5): Ujtis a linear operator that allows
studying (S5) by examining its action on the inÔ¨Ånitely dimensional space Fof observables. If jtis measure-preserving (
(8A2B) (m((jt) 1(A)) =m(A)) thenUjtis an isometry. For an introduction to the theory of the Koopman operator on the
Banach lattice Lpsee [17, Chapter 7].
An analogous approach is applicable to a discrete dynamical system
zi+1=T(zi); (S8)
where T:X !Xis a measurable nonlinear map on a state space Xandi2Z. The Koopman operator UUTfor the
discrete system is deÔ¨Åned analogously by
Uf=fT;f2F: (S9)
Discrete dynamical systems naturally describe evolution of discrete events, e.g. stock market data, reported cases of inÔ¨Çuenza
illnesses, or lynx population in Europe, but they are also at the core of numerical analysis of the continuous systems. More
precisely, if we run a numerical simulation of (S5) in a time interval [t0;t], the numerical solution is obtained on a discrete
equidistant grid with Ô¨Åxed time lag Dt:
t0;t1=t0+Dt; :::; ti 1=ti 2+Dt;ti=ti 1+Dt; ::: (S10)
In this case, a black-box software toolbox acts as a discrete dynamical system zi=T(zi 1)that produces the discrete sequence
ofzix(ti); this is sampling with noise. For ti=t0+iDtwe have (using (S6), (S7) and the group property)
f(x(t0+iDt)) = ( fjiDt)(x(t0)) = (UjiDtf)(x(t0)) = (Ui
jDtf)(x(t0));Ui
jDt=UjDt:::UjDt: (S11)
13/41On the other hand, using (S9),
f(zi) =f(T(zi 1)) =:::=f(Ti(z0)) = (Uif)(z0); (S12)
where T2=TT,Ti=TTi 1. Hence, in a software simulation of (S5) with the initial condition z0=x(t0), we have an
approximation
(Uif)(z0)(Ui
jDtf)(z0);f2F;z02X;i=0;1;2;::: (S13)
with the Ô¨Ådelity that depends on the numerical scheme deployed in the software, and which is studied in the shadowing theory,
see e.g.18,19.
This can be obviously extended to vector valued observables: for g= (g1;:::; gd):X !Cdwe deÔ¨Åne
Udg=0
B@g1T
...
gdT1
CA=0
B@Ug1
...
Ugd1
CA: (S14)
The observables can be both purely physical quantities (e.g. temperature, pressure, energy) and mathematical constructs using
suitable classes of functions (e.g. (multivariate) Hermite polynomials, radial basis functions). In particular, if we set d=N,
gi(z) =eT
iz, where z2CN,ei= (dji)N
j=1,i=1;:::; N, then g(z) =zis called full state observable and (Udg)(zi) =zi+1.
S1.2.1 Spectral decomposition and representation of observables
Spectral decomposition of Uis the pillar of both theoretical and practical analysis of dynamical systems in the framework of
the Koopman linearizations (S7), (S9). We consider (S9) with F=L2(X;m), whereXis compact and big enough to contain
the states.
An eigenpair (lj;yj)of the eigenvalue lj2Cand nonzero function yj2F(eigenvector, eigenfunction) satisÔ¨Åes
Uyj=ljyj:
The key of the spectral analysis of the dynamical system is a representation of an observable as a linear combination of the
eigenfunctions of U. Since the Koopman operator can have continuous spectrum20, 21, this might not be always possible.
However, an observable that belongs to a subspace spanned by eigenfunctions can be written as
h(z) =0
B@h1(z)
...
h`(z)1
CA¬•
√•
j=1yj(z)vj;where hi(z)¬•
√•
j=1yj(z)(vj)i;vj=0
B@(vj)1
...
(vj)`1
CA: (S15)
Then we can envisage the values of the observable hat the future states T(z),T2(z);:::by
h(T(z)) =0
B@h1(T(z))
...
h`(T(z))1
CA=0
B@(Uh1)(z)
...
(Uh`)(z)1
CA= (Udh)(z)¬•
√•
j=1ljyj(z)vj;::: h(Tk(z))¬•
√•
j=1lk
jyj(z)vj;::: (S16)
For more details see e.g.22,23,24. If the dynamics is evolving on an attractor, then all eigenvalues are on the unit circle; however
we are also interested in an off-attractor analysis. The mapping jtis thus not assumed measure preserving, and thus Uis not
necessarily unitary. Detailed analysis of the spectrum in this general case and the function spaces associated with it can be
found in22.
S1.3 Numerical computation
For a practical application of the Koopman operator, we need numerical methods to compute its approximate eigenvalues and
eigenvectors, and the modes vjin (S15). We consider the discrete case (S8), (S9), (S14); for numerical computations with a
continuous system we invoke the discretization (S11), (S12), (S13). For more details on the Extended DMD and implementation
using the kernel trick see11.
14/41S1.3.1 The data
In a typical data driven setting, we will have a sequence of snapshots, where we use the notion of snapshot as a numerical value
of a scalar or vector valued observable at a speciÔ¨Åc instance in time. We do not assume explicit knowledge of the mappings
F(S5) or T(S8). For example, the snapshots may be obtained from high speed camera recording of a combustion process
in a turbine25, or. e.g. as the wind tunnel measurements. A less expensive and less restrictive is a numerical simulation of
(S5) represented by (S11), (S12), (S13), where we can feed an initial z0to a software tool (representing T, or its linearization
through a numerical scheme encoded in the software toolbox) to obtain the sequence
f(z0) = (U0
df)(z0);f(z1) = (Udf)(z0);f(z2) = (U2
df)(z0);:::; f(zM+1) = (UM+1
df)(z0); (S17)
where f= (f1;:::; fd)Tis a vector valued ( d>1) observable with the action of UddeÔ¨Åned by (S14). The time resolution Dt
can be set to obtain the desirable numerical accuracy. This can then be repeated for many initial values z0; if the new initial
value is deÔ¨Åned as ez0=p(z0), then the new simulation data can be incorporated by adding the new observables fipas new
components of f. In a CFD application, fmay be the full state observable, and the entries in the state vectors ziare then e.g. the
values of the pressure and of the components of the velocity at a discrete spatial grid in the physical domain.
Hence, independent of the underlying process, the numerical data values are of the form of a matrix Swith columns,
respectively, f(z0),f(zk+1) = (Udf)(zk),k=0;:::; M:
S= ( f(z0)f(z1)f(z2):::f(zM)f(zM+1)) =0
B@f1(z0)f1(z1)f1(z2):::f1(zM)f1(zM+1)
f2(z0)f2(z1)f2(z2):::f2(zM)f2(zM+1)
.........:::......
fd(z0)fd(z1)fd(z2):::fd(zM)fd(zM+1)1
CA2Cd(M+2);zk+1=T(zk);k=0;:::; M:(S18)
Although the snapshots are generated by the nonlinear system (S8), (S12), the recursion (Krylov sequence) (S17), driven by the
linear operator Udand numerically evaluated along a trajectory initialized at z0, motivates to seek out a linear operator (matrix)
A2Cddwhose action on the available snapshots is given by
Af(zk) = (Udf)(zk) = (Uf1)(zk)
...
(Ufd)(zk)!
=f(T(zk));k=0;:::; M: (S19)
Thus, if we set X=S(1 :d;1 :M+1),Y=S(1 :d;2 :M+2), then such an Awould satisfy Y=AX, and this could be
extended linearly to the span of the columns of XbyA(Xv) =Yv,v2CM+1. The action of Aoutside the range of Xis not
speciÔ¨Åed by the available data.
In general, XandYare not necessarily extracted from a single trajectory (S17, S18). The data may consist of several short
bursts with different initial conditions, arranged as a sequence of column vector pairs of snapshots (xk;yk), where xk=f(zk),
yk=f(T(zk))column-wise so that a kth column in Ycorresponds to the value of the observable in the kth column of Xthrough
the action of Ud, as in (S19); see26. Depending on the parameters dandM, the matrices X,Ycan be square, tall (more rows
than columns), or wide (more columns than rows). Then, analogously to (S19), we can search for a linear transformation A
such that Y=AX. Such an Amay not exist.
However, we can always deÔ¨Åne a particular matrix Awhich minimizeskY AXkF. Clearly, if XThas a nontrivial
null-space, Ais not unique; we can choose Bso that BX=0and thus (A+B)X=AX. One can impose an additional condition
of minimality ofkAkF, which yields the well known solution A=YX‚Ä†, expressed using the Moore-Penrose pseudoinverse
X‚Ä†ofX. This additional constraint, although useful to enforce uniqueness and boundness, has (to the best of our knowledge)
no other useful interpretation in this framework. If we are interested in approximating some eigenvalues and eigenvectors,
then with the restricted information on A(only its action on the range of Xis meaningfully deÔ¨Åned), we will use the Rayleigh
quotient X‚Ä†AX=X‚Ä†(A+B)X=X‚Ä†Y, so this non-uniqueness of Ais immaterial. If Xis of full row rank, then the optimal A
is unique. If Xis of full column rank, then A=YX‚Ä†satisÔ¨Åes Y=AXexactly. Throughout this paper we assume that Xis of
full (row or column) rank. However, even when full rank, Xmay be severely ill-conditioned so that its numerical rank is lower,
which poses notrivial numerical challenges; these issues are addressed in our recent work27.
S1.3.2 A dual representation and the compression of U
The sequence of vector valued observables (S18) naturally describes the (discrete) time dynamics, with the column index
representing the timestamp, and row indices representing the scalar observables (e.g. the pressure and the components of the
velocity at particular spacial coordinates) that build the vector observable. For instance, in a CFD application, the multi-indexed
(2D, 3D) spatial positions are mapped (vectorized) into a column vector in the usual way, and Sis generated by the software,
column by column.
15/41In a dual interpretation (reading) of the data, we can think of each row in (S18) as a set of values of an observable, sampled
over the spatial domain. In other words, we transpose the matrix Sin (S18), partition bS=STas
bS(1 :M+1;1 :d) =0
B@f1(z0)f2(z0)f3(z0):::fd(z0)
f1(z1)f2(z1)f3(z1):::fd(z1)
.........:::...
f1(zM)f2(zM)f3(zM):::fd(zM)1
CA=XT; (S20)
bS(2 :M+2;1 :d) =0
B@f1(T(z0))f2(T(z0))f3(T(z0)):::fd(T(z0))
f1(T(z1))f2(T(z1))f3(T(z1)):::fd(T(z1))
......... :::...
f1(T(zM))f2(T(zM))f3(T(zM)):::fd(T(zM))1
CA=YT; (S21)
and consider the action of Uon the space FDspanned by the dictionary of scalar functions D=ff1;:::; fdg. That is, we seek
a matrix representation Uof the compression YFDUjFD:FD !FD, where YFDis a suitable projection with the range
FD. This is the standard construction: we need a representation of Ufiof the form
(Ufi)(z) =fi(T(z)) =d
√•
j=1ujifj(z)+ri(z);i=1;:::; d;z2X: (S22)
With the data at hand, the projection is feasible only in the discrete (algebraic) sense: we can deÔ¨Åne the matrix U= (uji)2Cdd
column-wise by minimizing the residual ri(z)in (S22) over the states z=zk, using the values
(Ufi)(zk) =fi(T(zk));i=1;:::; d;k=0;:::; M: (S23)
To that end, write the least squares residual
1
M+1M
√•
k=0jri(zk)j2=1
M+1M
√•
k=0jd
√•
j=1ujifj(zk) fi(T(zk))j2; (S24)
which is the L2residual with respect to the empirical measure deÔ¨Åned as the sum of the Dirac measures concentrated at the zk‚Äôs,
dM+1= (1=(M+1))√•M
k=0dzk. Hence, the columns of the matrix representation are deÔ¨Åned as the solutions of the least squares
problems
Zd
√•
j=1ujifj fiT2
ddM+1=1
M+1" f1(z0)f2(z0):::fd(z0)
......:::...
f1(zM)f2(zM):::fd(zM)! u1i
...
udi!
  fi(T(z0))
...
fi(T(zM))!#2
2 ! min
u1i;:::;udi; (S25)
fori=1;:::; d. The solutions of the above algebraic least squares problems for all i=1;:::; dare compactly written as the
matrixU2Cddthat minimizeskXTU YTkF, i.e.
U= (XT)‚Ä†YT(YX‚Ä†)T=AT; (S26)
and the action of Ucan be represented, using (S22), as
U 
f1(z)::: fd(z)
= 
f1(z)::: fd(z)
U+ 
r1(z):::rd(z)
: (S27)
Similarly as with the computation of Ain ¬ßS1.3.1, Uis uniquely determined only if XTis of full column rank. Otherwise, we
must proceed carefully when using the spectral data of Uto infer approximate eigenvalues of U. In particular, if d>M+1,
XThas a nontrivial null-space, and if eUis another least squares solution, then XT(eU U) =0. On the other hand, along the
linear manifold U+Ker(XT) =fU+B:XTB=0g, the Rayleigh quotient (matrix representation of the compression of U
onto the range of X)X‚Ä†(U+B)X=X‚Ä†UX2C(M+1)(M+1)remains uniquely determined. (Note that in the case of complex
data we work with the adjoints XandY, instead of XT(S20) and YT(S21), to obtain U=A=AT, which is then the matrix
representation in the basis of complex conjugate functions fi.)
The quality of this Ô¨Ånite dimensional approximation of Udepends on the selected dictionary of the observables (capturing
a nearly invariant subspace that corresponds to the most relevant eigenvalues), as well as on the approximation level of the
underlying measure by the empirical one, in particular on the distribution of the zk‚Äôs. For related numerical issues see27and for
a theoretical study of convergence, see5.
16/41S1.3.3 Computation of eigenfunctions and the Koopman mode decomposition (KMD)
Next, we describe the framework for practical computation of the modal decomposition from ¬ßS1.2.1. It is the classical
Rayleigh-Ritz extraction, based on (S27) and the spectral decomposition of U.
Consider Ô¨Årst the case rank(X) =d; then dM+1, andUis uniquely deÔ¨Åned, column by column, from the solutions
of the least squares problems (S25), for i=1;:::; d. In this case all deigenvalues (with the corresponding eigenvectors) are
well determined by the data. For technical simplicity we assume that Uis diagonalizable, with the spectral decomposition
U=QLQ 1, with L=diag(li)d
i=1,Q= (q1;:::; qd),Uqi=liqi. We do not assume that the eigenvalues are simple, and in
the case of multiple eigenvalues we list them as successive diagonal entries of L. Then, for z2X,
U 
f1(z)::: fd(z)
Q= 
f1(z)::: fd(z)
QL+ 
r1(z):::rd(z)
Q; (S28)
and the approximate eigenfunctions of U, extracted from the span of f1;:::; fd, are
 
f1(z)::: fd(z)
= 
f1(z)::: fd(z)
S= 
√•d
i=1fi(z)Qi1:::√•d
i=1fi(z)Qid
;(Ufi)(z)=lifi(z)+d
√•
j=1rj(z)Qji:
Following ¬ßS1.2.1, we seek a decomposition of observables in terms of the fi‚Äôs, similar to (S15). In a numerical simulation,
these eigenfunctions are accessible, as well as the observables, only as the tabulated values for z2fz0;:::; zMg:
0
B@f1(z0) f2(z0) f3(z0):::fd(z0)
f1(z1) f2(z1) f3(z1):::fd(z1)
......... :::...
f1(zM+1)f2(zM+1)f3(zM+1):::fd(zM+1)1
CA=0
B@f1(z0) f2(z0) f3(z0)::: fd(z0)
f1(z1) f2(z1) f3(z1)::: fd(z1)
......... :::...
f1(zM+1)f2(zM+1)f3(zM+1):::fd(zM+1)1
CAQ=STQ: (S29)
Let now g(z)T= (g1(z);:::; gd(z))be a vector valued observable and let gi(z) =√•d
j=1gjifj(z) +ei(z), so that g(z)T=
(f1(z);:::; fd(z))G+E(z),G= (gji)2Cdd,E(z) = ( e1(z);:::; ed(z)). (If gi=fi, then G=IdandE=0. If gi2FD,
then E=0.) Hence
g(z)T= 
f1(z)::: fd(z)
QQ 1G+E(z) = 
f1(z):::fd(z)
Q 1G+E(z);z2X: (S30)
SetV=GTQ T= 
v1::: vd
, where viis the ith column. Then
0
B@g1(z)
...
gd(z)1
CA=GTQ T
|{z}
V0
B@f1(z)
...
fd(z)1
CA+E(z)T=d
√•
i=1vifi(z)+E(z)Td
√•
i=1vifi(z):
Since (Ufi)(z)lifi(z), we have
(Uk
dg)(z) =0
B@(Ukg1)(z)
...
(Ukgd)(z)1
CAd
√•
i=1vifi(z)lk
i: (S31)
In the sequel, we use G=Id; thus V=Q T. We can assume that kvik2=1, since vifi(z) = ( vi=kvik2)(kvik2fi(z)), where
kvik2fiis again an eigenfunction. To evaluate (S31) numerically at z0, use (S29).
If some eigenvalues are multiple, then with any block diagonal nonsingular matrix D=L
kDk, that commutes with L,
we have U= (QD)L(QD) 1. (The number of the blocks Dkequals the number of different eigenvalues, and the dimensions
corresponds to their multiplicities.) If we repeat the same construction with eQ=QD, then the new approximate eigenfunction
ofUare(ef1;:::;efd) = (f1;:::;fd)D, and the matrix of the modes is eV=VD 1. At the end, we obtain another representation
of the sum in (S31).
Using (S26), we conclude that AQ T=Q TL, i.e. the columns of Q Tare the (right) eigenvectors of A. Hence, for
computing the Koopman modes, we can proceed with computing the eigenvectors of A. The eigenvector matrix is necessarily
of the form Q TD 1with some D=L
kDk, as above.
Consider now the case d>M+1=rank(X). We have M+1<dRitz pairs of U, and in the decomposition (S28) the
matrix Qis tall rectangular, d(M+1), so we cannot immediately insert QQ 1as in (S30). To replace the spanning set
f1;:::; fdwith(f1;:::;fM+1) = ( f1;:::; fd)Q, we must use QQ‚Ä†6=Id. If the full column rank Qis extracted from the range of
X, then QQ‚Ä†X=X. We proceed with the assumption that the data snapshots are real ‚Äì the additional goal is to point out that in
that case all computation can be done (and in a software implementation it should) in real arithmetic, even if the eigenvalues
17/41and eigenvectors (the columns of Q) are complex. Since the matrix Uis then real as well, the pair L,Qcomputed by the
Rayleigh Ritz method will be closed under conjugation and can be indexed as follows: if li2R, then qi2Rd, and if √Å(li)>0,
thenli+1=li,qi+1=qi. Using the identity
 
qiqi
1 i
1 i
= 
2√Ç(qi)2√Å(qi)
we immediately conclude that Q=eQJ, whereeQis real and Jnonsingular. (Here √Ç()and√Å()denote the real and the
imaginary parts of complex scalars or vectors.) Hence, QQ‚Ä†=eQeQ‚Ä†is real symmetric and XTQQ‚Ä†=XT. On the other hand,
in a practical computation, we see the function values only at z2fz0;:::; zM+1g, and for those values we can use QQ‚Ä†instead
ofQQ 1in relation (S30). The rest is straightforward, yielding the modal matrix V=GTQ‚Ä†T, and QQ‚Ä†AQ‚Ä†T=Q‚Ä†TL. The
latter reveals that Q‚Ä†,Lcorrespond to Ritz pairs of A, extracted from the range of X.
In the next section, we derive the KMD directly from an application of the Rayleigh Ritz procedure to the matrix A.
S1.3.4 Krylov compression of Udand the KMD
Note that, for an f2F, (S17) naturally generates a Krylov sequence of functions f;Udf;:::;UM
df;UM+1
df, and that
Ud 
fUdfU2
df:::UM
df
|{z}
KM+1= 
fUdfU2
df:::UM
df
CM+1+EM+1; (S32)
UdKM+1=KM+1CM+1+EM+1;CM+1=0
@0 0 0 0 a0
1 0 0 0 a1
0 1 0 0 a2
0 0 1 0 a3
0 0 0 1 aM1
A; (S33)
where we have written KM+1= 
fUdfU2
df:::UM
df
,
UM+1
df=M
√•
i=0aiUi
df+rM+1; (S34)
andEM+1= 
0 r M+1
. In (S34), rM+1is the residual obtained after projecting UM+1
dfonto the subspace spanned by
[KM+1] =spanfUi
df;i=0;:::; Mg. Here we assume that M+1<d(possibly even M+1d), so that we expect nonzero
residual rM+1. Our earlier full rank assumption on Ximplies that its rank is M+1.
IfPM+1is the orthogonal projector onto [KM+1], then the compression PM+1Udj[KM+1]is represented by the matrix CM+1.
IfCM+1v=lv, where v= (v1;:::; vM+1)T6=0, then
Ud(M
√•
i=0vi+1Ui
df) =l(M
√•
i=0vi+1Ui
df)+vM+1rM+1: (S35)
This means that land the function h=√•M
i=0vi+1Ui
df=KM+1vsatisfyUdh=lh+vM+1rM+1, i.e.(l;h)is an approximate
eigenpair with the residual
kUdh lhk=jvM+1jkrM+1k (S36)
measured in the norm of the function space F.
Given the data snapshots (S18) as the only available numerical information, the coefÔ¨Åcients a= (a1;:::;aM)in (S34) can
be determined using the discretized (algebraic) least squares projection and the notation from ¬ßS1.3.1 as follows: The least
squares error to be minimized is
k(UM+1
df)(z0) M
√•
i=0ai(Ui
df)(z0)k2
2=kf(zM+1) M
√•
i=0aif(zi)k2
2=kyM+1 Xak2
2: (S37)
IfXis of full column rank, then a=X‚Ä†yM+1is the unique solution expressed using the Moore-Penrose pseudoinverse. Hence,
for a particular initial z0, the relation (S33) reads
(UdKM+1)(z0) =Y=KM+1(z0)CM+1+(yM+1 XX‚Ä†yM+1)eT
M+1eT
M+1= 
0::: 0;1
: (S38)
On the other hand, by (S19), (UdKM+1)(z0) =AKM+1(z0), and, as a concrete numerical realization of (S33) on the trajectory
starting at z0, we obtain the Krylov decomposition
Y=AX=XCM+1+(yM+1 XX‚Ä†yM+1)eT
M+1: (S39)
18/41where CM+1=X‚Ä†AX=X‚Ä†Yis the Rayleigh quotient. Note here that the full column rank assumption on Ximplies X‚Ä†X=I.
Also note that here we do not have Aexplicitly formed, nor we think of it as A=YX‚Ä†.
Hence, since the residual brM+1=yM+1 XX‚Ä†yM+1is unlikely to be zero, we can extract from Xonly approximate (Ritz)
eigenpairs of A. To that end, we Ô¨Årst compute the eigenvalues and eigenvectors of CM+1. Under the generic assumption that all
eigenvalues of CM+1are algebraically simple, *its spectral decomposition is CM+1=V 1
M+1LM+1VM+1, where
LM+1=0
B@l1
...
lM+11
CA;VM+1=0
BBB@1 l1::: lM
1
1 l2::: lM
2......:::...
1lM+1:::lM
M+11
CCCA;det(VM+1)√ï
j>k(lj lk)6=0: (S40)
In other words, the eigenvectors of CM+1are the columns of the inverse of the Vandermonde matrix VM+1. FromA(XV 1
M+1) =
(XV 1
M+1)LM+1+brM+1eTV 1
M+1(XV 1
M+1)LM+1, we see that the columns bviofbV=XV 1
M+1= (bv1;:::;bvM+1)are approximate
eigenvectors of A. With an eye towards (S31), we write X=bVVM+1, i.e., for k=0;:::; M,
(Uk
df)(z0) =Xek+1=M+1
√•
i=1bvi
kbvik2kbvik2lk
i=M+1
√•
i=1vikbvik2lk
i: (S41)
It is precisely this structure that yields the spatio-temporal representation in ¬ßS1.2.1. Indeed, if we set
F= 
fUdfU2
df:::UM
df
V 1
M+1=KM+1V 1
M+1;
thenljand the jth column F:jofFare a Ritz pair of Ud,UdF:jljF:j, see (S35), (S36). If F:j= (j1j;:::;jd j)T, then
Uji jljji j,i=1;:::; d. We have for k=0;:::; M
Uk
df(z0) =0
B@Ukf1(z0)
...
Ukfd(z0)1
CA=M+1
√•
i=1F:ilk
i=M+1
√•
i=10
B@j1i(z0)
...
jdi(z0)1
CAlk
i; (S42)
and we can extrapolate this to the future steps by increasing kwhich amounts to rising the powers of li. Also note that F
evaluated at z0equals precisely bV, so that (S41) is a concrete numerical realization of (S42). In an ideal situation, ljis
geometrically simple eigenvalue and ji jare nearly collinear for i=1;:::; d. However, this is not essential for the purposes
of snapshots representation and prediction because the action of Udis component-wise, and each ji jis an approximate
eigenfunction of U.
This algebraically elegant process has a drawback that becomes apparent when we consider its numerical software
implementation. Vandermonde matrices are notoriously ill-conditioned. Moreover, in case of an off‚Äìattractor analysis the
valuesjlijjmay vary in size over several orders of magnitude, which poses challenging problems for the Ô¨Ånite precision
computation. For that reason, an SVD based method of Schmid28, designated as DMD , has become the main computational
device for the KMD . However, we have recently shown in13that this companion matrix based approach can be implemented
more accurately using the DFT and specially tailored algorithms for the Vandermonde and the related Cauchy matrices.
S1.3.5 Schmid‚Äôs dynamic mode decomposition (DMD)
The Rayleigh-Ritz procedure outlined in ¬ßS1.3.4 is based on a Krylov sequence, which naturally Ô¨Åts the dynamics of a discrete
dynamical system driven by U(in the space of observables). However, it yields a numerically ill-conditioned problem, as
a consequence of that very representation. From a numerical point of view, the Rayleigh-Ritz procedure is best executed in
unitary/orthonormal bases, so that Xshould be replaced with an orthonormal matrix spanning the same subspace. Since Xcan
be nearly numerically rank deÔ¨Åcient (its columns are actually generated by the power method), Schmid28used the PCA29with
prescribed cutoff threshold to construct the best lower dimensional subspace (i.e. a POD basis) that captures the data, and then
used the Rayleigh-Ritz extraction from that subspace. For the readers‚Äô convenience, we brieÔ¨Çy review the DMD algorithm; we
assume the more general setting where the snapshots are generated with several initial conditions, so that the input data are
not necessarily of the form (S17). That is, the matrices XandYare such that, column-wise, xk=f(zk),yk=f(T(zk)) =Axk;
see26. The total number of snapshots (column dimension) is in this general case denoted by m; in the case of a single trajectory
(S17), m=M+1.
The theoretical underpinning is the classical matrix theorem on best low rank approximations.
*Since CM+1is an unreduced Hessenberg matrix, its eigenvalues must be of geometric multiplicity one. If CM+1has multiple eigevaules, then its
generalized eigenvector matrix is the inverse of the conÔ¨Çuent Vandermonde matrix generated by the distinct eigenvalues. The Jordan structure of each multiple
eigenvalue consists of a single Jordan block.
19/41Theorem S1.2 (Eckart-Young30, Mirsky31) Let the SVD ofX2Cdmbe
X=USV;S=diag(si)min(d;m)
i=1;s1 smin(d;m)0:
Forr2f1;:::; rank(X)g, deÔ¨Åne Ur=U(:;1 :r),Sr=S(1 :r;1 :r),Vr=V(:;1 :r), and Xr=UrSrV
r. Then, Xris closest
matrix of rank at most r to X, inkk 2and the Frobenius norm kk F, i.e.
min
rank(X)rkX Xk2=kX Xrk2=sr+1; min
rank(X)rkX XkF=kX XrkF=vuutmin(d;m)
√•
i=r+1s2
i: (S43)
Hence, we can replace Xwith its best low rank approximation by truncating its SVD X=USVUrSrV
r, where Ur=U(:
;1 :r)isdrorthonormal ( U
rUr=Ir),Vr=V(:;1 :r)ismr, also orthonormal ( V
rVr=Ir), and Sr=diag(si)r
i=1contains
the largest rsingular values of X. In brief, Uris the POD basis for the snapshots x1;:::; xm,
m
√•
i=1kxi UrU
rxik2
2=min
QQ=Irm
√•
i=1kxi QQxik2
2:
The index ris selected so that the approximation error (S43) is below a user prescribed threshold value, and it is a numerical
rank32ofX. Now, DMD uses the range of Urfor the Rayleigh-Ritz extraction. The Rayleigh quotient Ar=U
rAUris computed,
using
Y=AXAUrSrV
r;andAUr=YVrS 1
r; (S44)
as
Ar=U
rYVrS 1
r; (S45)
which is suitable for data driven setting because it does not use Aexplicitly. Clearly, (S44, S45) only require that Y=AX; it is
not necessary that Yis shifted Xas in ¬ßS1.3.4. Each eigenpair (l;w)ofArgenerates the corresponding Ritz pair (l;Urw)for
A. This is the essence of the Schmid‚Äôs method28, summarized in Algorithm S1 below.
Algorithm S1 [Vr;Lr] =DMD (X;Y)
Require:X= (x1;:::; xm);Y= (y1;:::; ym)2Cdmthat deÔ¨Åne a sequence of snapshots pairs (xk;ykAxk). (Tacit
assumption is that dis large and that md.)
1:[U;S;V] =svd(X); .The thin SVD :X=USV, U2Cdm,S=diag(si)m
i=1,V2Cmm.
2:Determine numerical rank r;
3:SetUr=U(:;1 :r);Vr=V(:;1 :r);Sr=S(1 :r;1 :r);
4:Ar= ((U
rY)Vr)S 1
r; .Schmid‚Äôs formula for the Rayleigh quotient U
rAUr.
5:[Wr;Lr] =eig(Ar); .Lr=diag(li)r
i=1;ArWr(:;i) =liWr(:;i);kWr(:;i)k2=1
6:Vr=UrWr. .Ritz vectors.
Ensure: V r= (v1:::vr),Lr.
Schmid‚Äôs DMD algorithm has been notably successful in CFD applications. For more on interesting applications and
modiÔ¨Åcations of the DMD , see e.g.11,33,34,35,36,37,38,39,40,41,42,43.
S1.3.6 ReÔ¨Åned Rayleigh-Ritz Data Driven Modal Decomposition (RRRDDMD)
Recently, in12, we revisited DMD and introduced several modiÔ¨Åcations. First, we show that the residuals kAvi livik2can be
computed in a data driven scenario as well. This allows for selecting good Ritz pairs, with small residuals, which proved to
be the key for selecting good modes for the prediction algorithm; see ¬ßS1.4.3. Further, we show that the Ritz vectors can be
improved by using the well known reÔ¨Ånement technique, which we have adapted to the data driven setting of the DMD .
S1.3.7 Spatio-temporal representation of the snapshots
In general, a DMD algorithm will compute rmRitz vectors (modes) with the corresponding eigenvalues. In particular, in the
Schmid‚Äôs DMD ,rmay be considerably smaller than m, as e.g. in the case of an off-attractor analysis of a dynamical systems,
after removing peripheral eigenvalues, see [12, ¬ß4.1]. In any case, the most important coherent structures of the process are
determined by a subset of the modes; so we may want to express the available data snapshots by r<mmodes. It is desirable
20/41Algorithm S2 [Vr;Lr;rezr;rr]=DDMD_RRR (X;Y;e){ReÔ¨Åned Rayleigh-Ritz Data Driven Modal Decomposition12}
Require:
‚Ä¢X= (x1;:::; xm);Y= (y1;:::; ym)2Cdmthat deÔ¨Åne a sequence of snapshots pairs (xk;ykAxk). (Tacit assump-
tion is that dis large and that md.)
‚Ä¢ Tolerance level efor numerical rank determination.
1:Dx=diag(kX(:;i)k2)m
i=1;X(1)=XD‚Ä†
x;Y(1)=YD‚Ä†
x;
2:[U;S;V] =svd(X(1)); .The thin SVD :X(1)=USV, U2Cdm,S=diag(si)m
i=1.
3:Determine numerical rank r, with the threshold e. See [12, ¬ß3.1.1] .
4:SetUr=U(:;1 :r);Vr=V(:;1 :r);Sr=S(1 :r;1 :r);
5:Br=Y(1)(VrS 1
r); .Schmid‚Äôs data driven formula for AUr.
6:[Qr;R] =qr( 
Ur;Br
); .The thin QRfactorization: 
Ur;Br
=QrR; Q rnot computed .
7:Ar=diag(Rii)r
i=1R(1 :r;r+1 : 2r); .Ar=U
rAUris the Rayleigh quotient .
8:Lr=eig(Ar) .Lr=diag(li)r
i=1;Ritz values, i.e. eigenvalues of A r.
9:fori=1;:::; rdo
10: [sli;wli] =svdmin(
R(1:r;r+1:2r) liR(1:r;1:r)
R(r+1:2r;r+1:2r)
);.Min. singular value and the corr. right sing. vector, see [12, ¬ß3.3].
11: Wr(:;i) =wli; rez r(i) =sli; .Optimal residual.
12: rr(i) =w
liArwli; .Rayleigh quotient, rr(i) = ( Urwli)A(Urwli).
13:end for
14:Vr=UrWr; .ReÔ¨Åned Ritz vectors .
Ensure: V r= (v1:::vr),Lr, rez r,rr.
that such modes can represent the snapshots reasonably well, and that they have small residuals which, as we shall see below, is
essential for the prediction of the evolution of the sequence (S17), with m=M+1. Assume that we have such a selection of
rnumerically linearly independent modes and, to ease the notation, assume that we have indexed the Ritz pairs so that the
selected ones are indexed with j=1;:::; r. With this setup, a modal decomposition of fk=f(zk)can be written as
fkr
√•
j=1lk
jajvj;k=0;:::; M+1: (S46)
Ifr=M+1=m, then the coefÔ¨Åcients a1;:::;amcan be computed as
(aj)m
j=1=V‚Ä†
mf0; (S47)
so that this reconstruction is exact for k=0;:::; M. In matrix notation, if we deÔ¨Åne Vr= 
v1::: vr
then we have
 
f0f1::: fM+1
 
v1v2::: vr0
BBB@a1
a2
...
ar1
CCCA0
BBB@1l1:::lM+1
1
1l2:::lM+1
2......:::...
1lr:::lM+1
r1
CCCAVrDaVr;M+2: (S48)
To compensate for the truncation error, the coefÔ¨Åcients ajcan be recomputed by solving the weighted least squares problem
(a1;:::;ar) =argminajM+1
√•
k=0w2
kkp
W(fk r
√•
j=1vjajlk
j)k2
2; (S49)
where wk0are the weights that can be used to emphasize importance of some time indicies or to introduce forgetting factors,
Wis positive deÔ¨Ånite matrix,‚Ä†andp
Wstands for the positive deÔ¨Ånite square root or the Cholesky factor of W. For numerical
methods for this optimization problem we refer to44,14. Here, for the reader‚Äôs convenience, we provide an explicit formula for
W=Id:
(a1;:::;ar)T= [(V
rVr)(Vr;M+2W2V
r;M+2)] 1[(Vr;M+2W(V
rXW))  !1]; (S50)
where W=diag(wk),  !1= (1;1;:::; 1)T, anddenotes the Hadamard matrix product; see [14, ¬ß3.2].
‚Ä†In fact, we allow also a diagonal semideÔ¨Ånite matrix Was a mean to exclude selected components of the fk‚Äôs from the minimization (S49).
21/41S1.4 Global Koopman prediction algorithm
Here we give the details of the new proposed algorithm, designated as Global Koopman Prediction (GKP ) algorithm. The
basic idea is to extract the intrinsic eigenvalues and modes of the dynamical system under consideration, and then to predict the
evolution of the system by using the principles outlined in ¬ßS1.2.1, ¬ßS1.3.3. In order to reveal the relevant eigenvalues and the
corresponding modes that capture the dynamics of the system on a larger time interval and not only locally, one has to use large
training sets. This strategy is at risk if the algorithm is oblivious to unusual and unexpected changes (perturbations) that can be
classiÔ¨Åed as Black Swan events. If such data are used in a learning window, the long term prediction is doomed to fail. We
use the numerically computed spectral information on Uto develop an additional device to equip the algorithm with a litmus
testfor detecting Black Swan events (a posteriori, of course), and, moreover, with a retouching scheme to restore the global
prediction capability (see ¬ß2.2 in the main text). Furthermore, an important feature is that the data snapshots are lifted in a
Hankel matrix structure, as described in ¬ßS1.4.2.
S1.4.1 Setting the scene - the prediction task
Consider a discrete dynamical system zk+1=T(zk)that is accessible through a sequence of snapshots
f0;f1;f2;::: (fk2Rd;k=0;1;2;:::; M;M+1;:::) (S51)
where d1is the dimension of the scalar or vector-valued system observable f:X!Rd, and fk=f(zk)is its value for the
(possibly unknown) state zk, with the time stamp tk,k=0;1;2;:::. The goal is to learn the dynamics from the available data
and then to predict the future values.
More precisely, suppose that the present time moment istp 1, and that, up to that moment, the data is readily available;
we seek a prediction of the data value at the next time moments tp;tp+1;:::; tp+t(p). We call that future time moments tpthe
prediction moments . The prediction will be based on a sliding window of size win the sequence (S51), i.e. we will use the
fk‚Äôs starting from the index b=p wthat deÔ¨Ånes the active window W(p;w)of consecutive data ( training set ) with indices
b=p w;b+1;:::; b+w 1=p 1. In terms of the system mapping T, these values can be represented as
fb= (fTb)(z0);fb+1= (fTb+1)(z0);:::; fb+w 1= (fTb+w 1)(z0): (S52)
S1.4.2 Lifting the data into a Hankel matrix structure and the H-DMD
The key for a successful application of the prediction framework from ¬ßS1.2.1 is that the Ô¨Ånite dimensional numerical
approximation from ¬ßS1.3.3 captures the spectral information accurately enough. To that end, we adopt the Hankel- DMD
approach of45,46. For an active window W(p;w), Ô¨Årst conveniently split w=mH+nH, and then lift the observables into the
higher dimensional space R`,`=dnH, and arrange them as columns of a `(mH+1)(block) Hankel matrix as follows:
H=0
BBB@fb fb+1 fb+mH 1 fb+mH
fb+1 fb+2 fb+mHfb+mH+1
...............
fb+nH 1fb+nH fb+nH+mH 2fb+nH+mH 11
CCCA= 
h1h2::: hmH+1
: (S53)
We can think of the hi‚Äôs as the values of the vector-valued observable h:X !R`composed with the powers of Tanalogously
to (S52), i.e. h= 
fTbfTb+1::: fTb+nH 1Tand
 
h1h2h3::: hmH+1
= 
h(zb)hT(zb)hT2(zb)::: hTmH(zb)
(S54)
= 
h(zb) (U`h)(zb) (U2
`h)(zb):::(UmH
`h)(zb)
; (S55)
which can be interpreted as a Krylov sequence for the Koopman operator U`=
`
1U,U`h=hT. The techniques from
¬ßS1.3 now apply in this new setting simply by setting hinstead of f, and`instead of d. The matrix Hplays the role of the
snapshot matrix‚Ä°S, and we have X=H(:;1 :mH),Y=H(:;2 :mH+1). We will attempt predicting the hk‚Äôs, and from the
obtained results extract the predictions of the original observable f. The starting points are the DMD ofH(H-DMD ), and
the corresponding KMD . Since the KMD changes with the sliding active data window W(p;w), we use the term active KMD
(AKMD ) when we refer to the computation used for prediction.
‚Ä°Note that this is different from a system identiÔ¨Åcation technique based on the SVD decomposition of H.
22/41S1.4.3 Prediction - the basic idea and its limitations
Suppose that the DMD algorithm, applied to (S53), has extracted r=mHRitz pairs, and that the Ritz vector span the range of
H. Then we can determine the coefÔ¨Åcients ajsuch that
hk=Ak 1h1=mH
√•
j=1lk 1
jajvj+dk;mH+1brmH+1;k=1;:::; mH+1; (S56)
where dk;mH+1is the Kronecker delta symbol, and brmH+1is the residual of the orthogonal projection of hmH+1onto the range
ofX. This means that the decomposition of the snapshots in terms of the modes is exact, except for the last one, which may not
belong to the range of X, and the residual brmH+1represents its decomposition error. For details we refer to [13, ¬ß2.4, ¬ß3.2].
If we want to extend the above relation beyond the index k=mH+1(i.e. to extrapolate into the future the evolution of the
sequence hk), we can apply the appropriate power of Aand use the approximation Avj=ljvj+rjljvj. This is a fairly
simple operation - it amounts to increasing the power of the lj‚Äôs. Of course, the residuals will accumulate with each such
iteration, e.g.
AhmH+1=mH
√•
j=1lmH+1
j ajvj+mH
√•
j=1lmH
jajrj+AbrmH+1; (S57)
A2hmH+1=mH
√•
j=1lmH+2
j ajvj+mH
√•
j=1lmH+1
j ajrj+mH
√•
j=1lmH
jajArj+A2brmH+1;A3hmH+1=::: (S58)
So, using the Ô¨Årst sums above (and ignoring the residual terms) to predict future of the hk‚Äôs has limited range, except in the
case of smallbrmH+1and small residuals rj, which are not too much ampliÔ¨Åed under the action of the powers of A. Hence, it is
desirable to have a KMD that uses only the selected modes corresponding to the Ritz pairs with small residuals, and that we can
have an accurate decomposition of the type (S48), using the selected modes. This selection is possible in data driven scenarios
using the methods from44and12, outlined in ¬ßS1.3.6. The desire to have a high Ô¨Ådelity representation of the data snapshots with
as few as possible modes vjis motivated by revealing latent coherent structures of the e.g. Ô¨Çow Ô¨Åeld; small residuals allow for
extrapolation of the dynamics forward in time.
If our goal is solely the prediction, the weight factors wiin (S49) can be tuned to favor most recent snapshots, and the
weighting matrix Wcan emphasize particular block rows in the hk‚Äôs; see ¬ßS1.3.7 and [14, ¬ß3]. In particular, with a suitable
choice of WandW, we can focus the reconstruction of the hk‚Äôs to the present snapshot fn+nH+mH 1=ftp 1.
Since the fk‚Äôs, starting from the past time stamp index band ending at the present index p 1=b+nH+mH 1, are in the
last block row of H(see (S53)), the corresponding formulas are obtained by taking the last dcomponents of the Ritz vectors
vj. To that end, deÔ¨Åne bvj=vj((nH 1)d+1 :nHd)as the trailing dcomponents of vj. Hence, from the AKMD of the lifted
observables, we read off approximate decomposition of the snapshots fkas
efk=r
√•
j=1bvjajlk 1
j: (S59)
Fork=b+nH;:::;b+nH+mH 1, (S59) is a reconstruction of the acquired data, while for k=p;p+1;:::; p+t(p), (S59)
is an extrapolation of the AKMD , and it gives us predictions for future data snapshots. We say that efp+t,t>0is the prediction
of the observable at the lead time t.
If the number of rows nHof Hankel matrix is smaller than number of columns mH(nH<mH)then the KMD gives some
sort of regression function for the data in the reconstruction window b+nH;:::; b+nH+mH 1. The reason why the Ô¨Årst
part of active window is not declared as reconstruction window is that when the KMD of the form (S59) is used, the Koopman
eigenfunction values are determined such that for t=0the sum of the right hand side is equal to the Ô¨Årst snapshot. When
applied to Hankel matrix this means that the data in the beginning of the active window b;:::; b+nH 1, which form the Ô¨Årst
column of Hankel matrix are reconstructed with high accuracy. On the other hand, if nHmHand if the Hankel matrix has full
column rank, the data in the whole active window are reconstructed with high accuracy.
S1.5 A worked example
We now illustrate the key elements of the procedure outlined in ¬ßS1.4.2, ¬ßS1.4.3 using a worked example. The problem under
study is the spread of the coronavirus disease (COVID-19). The data consists of reported cummulative daily cases in some
European countries. It should be stressed that the algorithm uses only the raw data ‚Äì no other information on the nature of the
data or on modelling parameters is assumed. Further, the data itself is clearly not ideal, as it depends on the reliability of the
tests, testing policies in different countries (triage, number of tests, reporting intervals, reduced testing during the weekends),
contact tracing strategies, the number of asymptomatic transmissions etc. Moreover, using the data from different countries in
23/41the same vector observable poses an additional difÔ¨Åculty for a data driven revealing of the dynamics, because the countries
independently and in an uncoordinated manner impose different restrictions, thus changing the dynamics.
For an analysis of a particular country, it is better to deÔ¨Åne the observables as the reported cases on local level, e.g. provinces,
counties, cities with similar conditions. Clearly, the dynamics of the spread of the infection depends on the population density
as well. This is best seen e.g. by comparing the heat map of the reported cases in the USA with the image of the USA from
space at night. In the numerical examples in this section, we purposely use data from different countries to make the prediction
task more challenging, which makes it an excellent stress test example.
Our goal with this example is twofold. First, we show the potentials and the limits of the proposed prediction algorithm.
Secondly, we discuss technical details of the computational scheme.
We use the following datasets:
DS1 The numbers of reported COVID-19 cases in Germany, France and the United Kingdom in the period February 29 to
November 18. The ordered triple of reported cases is an observable.
DS2 The dataset DS1 augmented by the numbers of reported cases in Denmark, Czechia, Slovenia, Austria and Slovakia.
DS3 The numbers of reported COVID-19 cases in a selected European country in the period February 29 to November 18,
augmented with two sequences of Ô¨Åltered data.
The test of the prediction algorithm runs on the lifted data ( 265observables from Rd:d=3forDS1 andDS3;d=8
forDS2) i.e. on the columns of the 94d172Hankel matrix H= 
h1h2::: h172
(see (S53)) with the block partition
94172, each block being d1. The matrix His used as a historical record, encoding the period February 29 ‚Äì November 18,
and we run the prediction algorithm starting from some past index and test its accuracy by comparison with the historical data.
We use simple increasing window starting at the index b=1and ending at p 1, where we choose different values of p. Then
we predict the next t+1 values from the moment pon.
In the Ô¨Årst experiment, we use DS1 and attempt prediction for 35days ahead. We take the Ô¨Årst 40columns of Has available
data and set X=H(1 : 282 ;1 : 39 ),Y=H(1 : 282 ;2 : 40 ). (This corresponds to the period February 29 ‚Äì July 10, and the
prediction for 35days ahead starts July 11.) The prediction relative error is shown on the left panel in Figure S8. The right
panel shows the Koopman Ritz values computed in the algorithm; note that the algorithm has revealed the eigenvalue 1, and
that all other Ritz values are inside the unit circle. The quite satisfactory prediction skill (recall, no information whatsoever on
the nature of the data is used) and well behaved Ritz values are related to the nature of the dynamics of the infection during the
summer.
In this example, it is instructive to check the Government Response Stringency Index (GRSI)47¬ßfor the entire time interval
involved in the computation. The three indexes behave differently: it can be noticed that France had sharper changes than
Germany and the United Kingdom (e.g. around June 20), and sometimes similar increase of stringency but a week earlier than
the other two countries (e.g. beginning to mid March). On the other hand, the GRSI for Germany and the United Kingdom
were not that much different throughout the observed period; see the left panel in Figure S9. It should be noted, however, that
the GRSI does not measure the quality of the implementation of the imposed restriction and that for a particular country it does
not necessarily indicate the trends in the dynamics of the disease spreading.
Now, in the same interval, we add new observables by including the data from Ô¨Åve more countries: Danemark, Czechia,
Slovenia, Austria and Slovakia. Hence, the matrix His752172. The prediction errors for a 28days prediction are given in
the left panel in Figure S10. Remarkably, the computed Koopman Ritz values nearly match the one computed in the Ô¨Årst test
with only three countries, see the right panel in Figure S10. Note that even with the differences shown in Figure S9, the main
trend of the implementation of the measures is similar. This might help explain the robustness of the spectrum indicated in
Figure S10, where such differences do not seem to lead to drastic change in the spectral behavior. We believe this indicates the
robustness of our methodology.
We proceed with the numerical experiment using the dataset DS1. We further expand the learning window and then consider
three consecutive steps with h1:105,h1:106,h1:107. The relative errors for 35days prediction are shown in the Ô¨Årst row of Figure
S11. In the context of policy changes that affected the dynamics of the infection spreading, and the fact that the algorithm
is purely data driven, the results can be considered satisfactory: in the Ô¨Årst graph, the error is below Ô¨Åve percent for 16days
and below ten percent for three weeks for all three countries (Ô¨Årst graph), below six percent for almost entire 35days period
(second graph), below Ô¨Åve percent for more than three weeks and below ten percent for 30 days (third graph).
In the next test, we use the data windows h1:132;:::; h1:140 for28days predictions for the time intervals October 11 -
November 7, October 12 - November, ..., October 19 - November 15. The results are shown in Figure S12.
Now, we go to the datased DS3. The focus is on some computational details related to the two main ingredients ‚Äì the
DMD and the KMD . The datased DS3 is constructed by a single and a double application of the Savitzky-Golay Ô¨Ålter to the
¬ßFor an interactive exploration of the GSRI see https://ourworldindata.org .
24/41Figure S8. Left panel : Relative prediction error for Germany, France and the United Kingdom for a 35 days prediction
starting after the data window h1:40. (In terms of the original data, prediction starts on July 11.) Right panel : The Koopman
Ritz values used in the KMD .
Figure S9. Government Response Stringency Index measures response indicators (OxCGRT indicators) such as school
closing, workplace closings, cancelling public events, restrictions on gathering size, closing public transport, stay at home
requirement, restriction on internal movement and international travel. The GRSI data for Germany, France, United Kingdom,
Denmark, Czechia, Slovenia, Austria and Slovakia are taken from47. For more details see48.
Germany data, so that d=3. (The Ô¨Ålter uses cubic polynomial and data window of width 5. On the left boundary, we add zero
values, and on the right boundary we leave the original data. The Ô¨Åltered data differ from the original at most Ô¨Åve to ten percent
relative error in the Ô¨Årst 30days and at most O(10 3)afterwards.) The purpose of the test is to create a situation that one could
encounter when deploying the Koopman/ DMD framework for data driven prediction or for a discovery and analysis of latent
coherent structures.
25/41Figure S10. Left panel : Relative prediction error for eight European countries for a 28days prediction starting after the data
window h1:40. (In terms of the original data, prediction starts on July 11.) Right panel : The Koopman Ritz values used in the
KMD , denoted as blue pluses (+). The red circles ( ) denote the Ritz values computed using only three countries as shown in
Figure S8.
Figure S13. Prediction experiment with DS3 with data from Germany. Left panel : the computed residuals for the computed
102 Koopman Ritz pairs (extracted from a subspace spanned by 132 snapshots h1:132). Note that all residuals are small. The
corresponding Ritz values are shown in the Ô¨Årst panel in Figure S14. Middle panel :KMD reconstruction error for h1:132 and the
error in the predicted values h133:160 (encircled with). The reconstruction is based on the coefÔ¨Åcients
(aj)r
j=1=argminaj√•kkhk √•r
j=1lk
jajvjk2
2.Right panel : Prediction errors for the period October 11 ‚Äì November 7.
Compare with the Ô¨Årst graph in Figure S12.
We recall that a DMD algorithm uses a rank revealing decomposition with some threshold value and that the number of
the computed Ritz pairs may be smaller than the column dimension of the matrix X; see ¬ßS1.3.5. Then the reconstruction
formula (S47) for the coefÔ¨Åcients is not valid, and one has to satisfy (S46) by solving the least squares problem √•kkhk 
√•r
j=1lk
jajvj;k2
2 !min aj, where the reconstruction error is not necessarily small, and it introduces noise into the extrapolation
process outlined in ¬ßS1.4.3. (Recall, if we have full set of modes, then the reconstruction is perfect and the only error is from the
Ô¨Ånite precision arithmetic.) The prediction skill based on this KMD is shown in Figure S13. Now we change the reconstruction
strategy and state the problem as the weighted least squares problem (S49) with the weights that favour the four most recent
snapshots with weights set to one, and with the weights of all other snapshots set to the machine round-off unit e2:210 16.
The effect of weighting is best seen by comparing the middle graphs in Figures S13 and S14. In the case of weighting, the
reconstruction of almost all leading snapshots is bad, but the ones more important for the prediction task have much smaller
error.
26/41Figure S11. First row : Prediction error for Germany, France and the United Kingdom for a 35days prediction starting from
the data windows h1:105 (prediction for September 14 ‚Äì October 18), h1:106 (prediction for September 15 ‚Äì October 19), h1:107
(prediction for September 16 ‚Äì October 20), respectively. Second row : The corresponding Koopman Ritz values used in the
KMD .
S1.6 Comments on SIR type models
The key coefÔ¨Åcient in SIR-type models, the so-called reproduction number R0can be estimated using Koopman operator
techniques. Namely, the classic SIR model reads
Àôs= bsi
Àôi=bsi ni
Àôr=ni
(S60)
Under condition s=1(inÔ¨Ånite reservoar of susceptibles), the exponential growth happens when b n>0, i.e.b=n>1. The
reproduction number is deÔ¨Åned by R0=b=n. Thus, R0is related to the coefÔ¨Åcient of exponential growth. Since we know that
eigenvalues of the linearized system are eigenvalues of the Koopman operator, the largest real Koopman eigenvalue is related to
R0. Another number commonly estimated for use in tracking of epidemics is the instantaneous reproduction number deÔ¨Åned
by49
Rinst
t=bsD;
where Dis the duration of the infectiousness. For small n, and constant s=1, we have
Rinst
tbD; (S61)
providing another connection between the SIR models, Koopman operator spectrum and Reproduction numbers.
S1.7 The framework for prediction and Black Swan event detection
InGKP algorithm, the spectral information is extracted from a sequence of active windows ‚Äì for each window, the snapshots
are arranged in a Hankel matrix whose columns deÔ¨Åne a new set of snapshots and approximate eigenvalues and eigenvectors are
computed using Algorithm S2. In the case when the dynamics of the system is not coupled with some other dynamical system,
we expect that, in the absence of unexpected disturbances, the AKMD will capture at least the basic trends of the dynamics. In
particular, the spectral radius of the active window (the maximal absolute value of the selected Ritz values) should not change
too much. Further, the DMD algorithm should compute Ritz pairs with reasonably small residuals. This is plausible, because
27/41Figure S12. Prediction error for Germany, France and the United Kingdom for a 28 days prediction, based on the windows
h1:132;:::; h1:140, respectively. The prediction intervals are, respectively, October 11 - November 7, October 12 - November, ...,
October 19 - November 15.
the sequence (S17) can be, at any moment, interpreted as an excerpt from a power method generated sequence, and the power
method in the limit reveals the absolutely dominant eigenvalues.
However, if the dynamical system data are hit by disturbance, this could be recognized e.g. by detecting the active windows
whose spectral radii change signiÔ¨Åcantly, or by the absence of Ritz pairs with small residuals (see Figures S15b, S15c, S15d ).
This enables us to pinpoint the discrete time moments/subintervals at which disturbances interfere with the original dynamics.
For the chosen reference interval I, ifmax jjljj62Ior if there are no Ritz pairs with reasonably small residuals, we Ô¨Çag the
observed active window as the window which possibly contains a Black Swan event. By sliding the active windows along
the computational domain, using the Ô¨Çagged windows, we determine the time sub-intervals containing disturbances whose
dynamics is not well captured by the corresponding AKMD models.
The reference interval Ican be determined (and dynamically adjusted) e.g. by Ô¨Årst computing the Ritz values for many
active windows, and then by trial and error, including a statistical reasoning and information theoretic techniques (see e.g.50)
learn to differentiate between the acceptable interval Ifor spectral radii and the values that are considered outliers. This is
best done on a case-by-case basis.
This scheme can be implemented with different sizes of the Hankel matrices (see ¬ßS1.4.2) and with different sizes of active
windows and then the Black Swan event intervals can be determined by taking into account all determined intervals.
S1.7.1 The retouching trick to process Black Swan events
If the Black Swan event data are included in the training set, the dynamics of the original system (decoupled from this
disturbance) cannot be revealed, which means that the prediction of the dynamics after the Black Swan event will be damaged,
if not impossible. However, instead of using the original data we can replace them with the data obtained by the prediction
28/41Figure S14. Prediction experiment with DS3 with data from Germany. Left panel : the computed 102 Koopman Ritz values
(extracted from a subspace spanned by 132snapshots h1:132). The corresponding residuals are shown in the Ô¨Årst panel in Figure
S13. Middle panel :KMD reconstruction error for h1:132 and the error in the predicted values h133:160 (encircled with). The
reconstruction is based on the coefÔ¨Åcients (aj)r
j=1=argminaj√•kw2
kkhk √•r
j=1lk
jajvjk2
2.Right panel : Prediction errors for
the period October 11 ‚Äì November 7. Compare with the Ô¨Årst graph in Figure S12, and with the third graph in Figure S13.
based on the information from the previous active windows, preceding Ô¨Çagged intervals. This is illustrated in (S62): the
perturbed value (Ukf)(z0)+ekis replaced with\(Ukf)(z0), which is a predicted value based on the previous undisturbed data.
The same can be done for the remaining data in the Ô¨Çagged window.
undisturbed :::(Uk 1f)(z0) (Ukf)(z0) (Uk+1f)(z0) :::
disturbance at k,k+1 :::(Uk 1f)(z0)(Ukf)(z0)+ek(Uk+1f)(z0)+Uek+ek+1:::
use prediction at k,k+1:::(Uk 1f)(z0)\(Ukf)(z0)\(Uk+1f)(z0) :::(S62)
The prediction after the Black Swan event then becomes more stable and in most cases quite successfully predicts data after the
Black Swan event; see ¬ß2.2 and Fig. 2 in the main text.
There are many variations of this scheme. For instance, it could happen that the Black Swan interval detected in the
algorithm is too long and possibly unrealistic. Therefore we limit the length of the interval on which the data are replaced in
order to prevent the algorithm from changing the dynamics too much. Then we apply the algorithm again and detect if the
replacements result with decreasing of maximum of the absolute value of the eigenvalues over the active windows. The whole
process can be repeated more times to remove eventual Black Swan events that are not taken into account in the previous steps.
Finally, the retouched data, cleaned from the Black Swan event disturbances, are used for the prediction.
S1.7.2 Local Koopman prediction
In some cases, the global prediction algorithm is not feasible. For instance, when we just start collecting the data, we have not
enough information for a GKP analysis. Or, in the situation when GKP recognizes the beginning of a Black Swan event, as
discussed at the beginning of ¬ßS1.7. Then, the available data cannot be used for prediction, because the dynamical system has
changed. The new model must be built from scratch, as if we just started getting new data. The best we can do is to create a
new local algorithm that needs less data, but also with a much shorter reach into the future.
In the Local Koopman Prediction ( LKP) algorithm we change the size of the active window depending on the success of the
previous prediction. The idea is to assimilate as much acquired data as possible, so we set Hankel matrix dimension variable
with prediction moment, i.e. nH=nH(p)andmH=mH(p). We also choose the minimal Hankel matrix dimension
(nH;mind)(mH;min+1); (S63)
and we start predictions with such minimal Hankel matrix i.e. for Ô¨Årst prediction p=p0we set
nH(p0) =nH;min;mH(p0) =mH;min: (S64)
When data fpat prediction time tpbecomes available, we can compute the error of the prediction efp, using suitable norm, as
ep=kefp fpk=kfpk: (S65)
At other prediction moments, if the prediction error (S65) is smaller than the referent error ere fwe assimilate the newly acquired
data into the active window by increasing the Hankel matrix size
nH(p) =nH(p 1)+1, or mH(p) =mH(p 1)+1: (S66)
29/41Algorithm S3 (Global Koopman Prediction ( GKP ) with Black Swan event detection and switching to local prediction)
Require:Data snapshots f0;f1;:::;fend;the size of the training data window w=nH+mH;the dimensions nH,mH
(nH>mH) of the Hankel matrices; the threshold h>0for the maximal acceptable value of the residual of Ritz vectors;
the sliding step Dp;the maximal number Nrepof iterative retouching of the perturbed data; the maximal time length
LBSfor one step replacement of the Black Swan event data with the predicted values; the reference interval Ifor spectral
radius used for the detection of Black Swan event moments; lead times tgandtlfor the global and local prediction.
Ensure: Predicted system observables efnH+mH;efnH+mH+1;:::;efw+ndmdDp+tg(orefw+ndmdDp+tl)
1:jrep=0;ndmd=bend w
Dpc;BSevent=False ,nBS=0
2:while (jrep<Nrep) and ( jrep=0 ornBS6=0)do
3: nBS=0
4: forp=w;w+Dp;:::;w+ndmdDpdo
5: For the active window W(p;w)apply KMD algorithm to obtain AKMD using nHmHHankel matrices.
6: If there are no Ritz values for which the associated residual is smaller than h, set max jjljj=¬•:
7: ifmax jjljj=2Ithen
8: ifBSevent=False then
9: SettBSbegin =tmax(0;p Dp);BSevent=True ;nBS=nBS+1 .New disturbance appears.
10: end if
11: Flag the time interval [tp Dp;tp]as a Black Swan event interval;
12: In the interval [tp Dp;tp]use Local Koopman Prediction with lead time tl(Algorithm S4)
13: .Remark: Local Koopman Prediction algorithm (Algorithm S4) can be applied on the whole domain and then
associated local prediction is used on the detected critical intervals.
14: Store the data from last active window not including Black Swan event for retouching the data in [tp Dp;tp]
using the prediction obtained by global AKMD .
15: else
16: ifBSevent=True then
17: SettBSend =tp Dp;BSevent=False .End of the Black Swan event.
18: Replace the original data in the Black Swan event interval [tBSbegin ;min(tBSbegin +LBS;tBSend]with the stored
retouched data.
19: else
20: Using the AKMD associated with W(p;w)and (S59), extrapolate to obtain the predictions efp+t,ttg.
21: end if
22: end if
23: end for
24: jrep=jrep+1
25:end while
Otherwise, i.e. if the prediction error (S65) is larger then the referent one, we reset the Hankel matrix dimension to the minimal
one: nH(p) =nH;min,mH(p) =mH;min.
In both cases we recompute the Hankel matrix and the AKMD for each new prediction.
S2 Supplementary material ‚Äì Discussion.
In this supplementary material we validate our approach by three case studies. In ¬ßS2.1, we use the Lorenz system to illustrate
the main idea of monitoring the Koopman Ritz values and the prediction skills of the proposed method. The model free aspect
of the proposed method is further illustrated in applications in two entirely different settings: physiological processes (resonant
breathing) in Supplementary section S2.2 and geomagnetic substorms (prediction of the ALindex) in Supplementary section
S2.3. Finally, in Supplementary section S2.4 we provide additional numerical results related to the inÔ¨Çuenza epidemics studied
in section 2.2 of the paper.
S2.1 Case study: Lorenz system
The critical underlying concept in chaotic dynamics is that of sensitivity to initial conditions and the associated positivity of
Lyapunov exponents that measure long term exponential deviation of nearby trajectories51. Namely, the long term exponential
divergence of nearby trajectories leads to unpredictability due to the Ô¨Ånite nature of (any) prediction algorithm. Even the
implementation of exact equations of a dynamical system on any computing device leads to Ô¨Ånite precision calculations and
30/41Algorithm S4 (Local Koopman Prediction ( LKP ) with resizing Hankel matrix)
Require:Data snapshots f0;f1;:::; fend;indices of time moments for the begin and the end of the local prediction k0;kf
(optionally)minimal Hankel matrix dimension nH;min,mH;min;error threshold ere f;lead time tl.
Ensure: Predicted system observables efnH;min+mH;min;efnH;min+mH;min+1;:::;efend+tl(orefk0, . . . ,efkf+tl)
1:ifk0andkfnot deÔ¨Åned then
2: k0=nH;min+mH;min;kf=end
3:end if
4:forp=k0;k0+1;:::;kflettp 1be the time of the last known data do
5: ifthe error ep(S65) is larger than referent error ere fthen
6: Resize the Hankel matrix to the minimal size (S64).
7: else
8: Increase the size of the Hankel matrix using (S66).
9: end if
10: Form the Hankel matrix (S53) and compute the AKMD .
11: Using the AKMD , extrapolate to obtain the prediction (S59).
12:end for
therefore ultimate exponential divergence of prediction from true trajectory. However, this neglects the Ô¨Åner aspects of chaotic
dynamics that are exhibited in the most paradigmatic of chaotic systems ‚Äì the Lorenz dynamical systems, modeled by Lorenz
equations
Àôx=0
@Àôx
Àôy
Àôz1
A=0
@s(y x)
x(r z) y
xy bz1
A (S67)
withs=10,b=8=3, and r=28for which the system exhibits chaotic behavior. For understanding of the prediction capability
for the Lorenz system, more important than the long term exponential divergence of trajectories is the short term divergence
typically induced by switching between the two wings of the butterÔ¨Çy (see Figure S15a).
At the core of our approach is the observation that, while inside one of the butterÔ¨Çy wings, the system behaves in a
predictable manner. The exponential divergence is ultimately due to switching between the two butterÔ¨Çy wings. The Ô¨Årst time
such a switch happens, the situation resembles a Black Swan event15(although there is an ontological difference highlighted
in the main text): the trajectory suddenly wonders off to a different part of the state space and starts exploring there, until
another switch happens taking it back to the known part of the state space. This Ô¨Åts our paradigm of splitting the state space
into domains over which prediction is possible and monitoring for the switch between such domains.
The current theory is thus an extension of the ideas in20, where deterministic components of stochastic dynamical systems
were extracted using Koopman operator methods, and52where it was shown that Lorenz system can be described well by a set
of linear evolution equations driven by stochastic term that induces switching. In both of these, the detection of the switching
moment and the precise interaction of local and global behavior on subdomains of state space was not accounted for; we
address it here.
We use (S67) to test the prediction potential of the KMD . We have generated data using numerical simulation ( ODE solver)
of (S67) with the time resolution dt=0:01s, thus obtaining a discrete dynamical system. For a present moment (index) p, an
active window of length wis selected as in ¬ßS1.4.2, with b=p w, and the selected data are lifted in the Hankel structure. The
KMD is computed for the corresponding vector valued observables hi, and used for their prediction as explained in ¬ß1 of the
paper. For computing the KMD for the global prediction algorithm, the active windows of size w=400and300100Hankel
matrices are used. By sliding the active windows along the computational domain we get prediction at different times. When
the actual data and the prediction errors become available, we either continue forecasting with the same KMD , or a switching
device invokes the local prediction scheme with 2111Hankel matrices if the error is above a preset threshold. The prediction
is then with shorter forecast lead time, and predicted data are based on a sequence of local KMD ‚Äôs. The local algorithm keeps
increasing the active windows and the lead time, whilst monitoring the error; see the Methods section.
In Figure S15, we show the KMD reconstruction and prediction results of the observable x1=xof the system (S67) for a
selection of Ô¨Åve active windows. While the reconstruction -indicated by green traces - works well (as expected, see e.g.44,13,14),
the prediction capability -indicated by magenta traces - is lost for the third and the fourth active windows. An inspection of the
quality of the approximate Ritz pairs computed by the DMD12shows that for the time interval containing those two windows
none of the computed pairs has the residual below 0:01, i.e. no useful spectral information, which is essential for the KMD ,
could be extracted from the available data snapshots, see Figures S15b, S15c. As a consequence, the prediction using numerical
31/41(a)Lorenz system (S67).
 (b)√Ç(l)
 (c)√Å(l)
(d)Global and local prediction for 2 :5sahead.
Figure S15. (S15a): the Lorenz system (S67). For each active window, the Ritz pairs with the residual below hr=0:01 are
selected (see [12, ¬ß3.2]); the real and the imaginary parts of the corresponding Ritz values are shown in (S15b, S15c). The
color intensity of the eigenvalues indicates the amplitudes of the corresponding modes. (S15d): KMD reconstruction and
prediction of the observable x1xfor the Lorenz system (S67). The data are collected in Ô¨Åve active windows (time intervals
[1;5],[6:5;10:5],[14:5;18:5],[21:5;25:5],[28:5;32:5], marked by shadowed rectangles) and then the dynamics is predicted for
the time moments ahead of the active window. Note - by comparing the positions of the intervals with poor prediction with the
eigenvalue-free pink rectangles in Figures S15b, S15c - that the failure of the global prediction occurs after the active windows
which do not contain Ritz pairs with sufÔ¨Åciently small residuals, as indicated by magenta curves. The local algorithm recovers
the prediction capability, using a sequence of shorter moving KMD ‚Äôs and prediction for 10 time steps ahead, as indicated by
orange curves.
realization of KMD cannot give satisfactory results.
On the other hand, in that part of the domain where the trajectory behaves chaotically, and the intensive change of the nature
of the eigenpairs precludes accurate numerical approximations, local prediction scheme quickly adapts to the new data, forgets
the previously acquired information, and delivers better results. See Figure S15d.
The reconstruction with a reduced number of modes (see Figure S15d) uses only the Ritz pairs with small residuals
(see [12, ¬ß3.2]). The number of modes used for prediction after the Ô¨Årst, the second and the Ô¨Åfth active window (gray rectangles)
were 10, 18 and 10, respectively. One can observe that the reconstruction and prediction capabilities - shown in dashed green -
are comparable with using the full KMD .
S2.1.1 Remark
Regarding the question of detecting the switching moments, Figure S15d provides an insight. If we look at the Ô¨Årst pink
zone with no ‚Äúgood" eigenvalues (Figures S15b, S15c), we see that it starts close to the switching moment. Also, this zone
is quite long because the switching moments in that zone are too close to each other and no learning data window can Ô¨Åt
in-between. Only when two switching moments are distant enough, we can Ô¨Ånd ‚Äúgood" eigenvalues, the learning data window
exits the pink zone, and the global prediction recovers. It is remarkable that recent works53, 54have found spectral objects -
pseudoeigenfunctions - that govern quite regular short term dynamics inside the wing of the Lorenz attractor. This dynamical
feature - discovered by careful analyses of the continuous Koopman operator spectrum for the Lorenz system - seems to enable
the prediction algorithm performance.
32/41S2.1.2 Discussion
Historically, the most discussed way in which a substantial change in dynamics can occur in dynamical systems is due to a
change of a value of a bifurcation parameter55. The prediction method that we propose is not necessarily related to a change of
parameter in the system. Namely, the original description of the black swan event does not relate to a change in parameter, just
to travel to another part of the space (here considering the dynamical system to be the ecological system). White swans were
known to exist in Europe, but explorers found black swans in Australia. The prediction that an explorer would make when
traveling to Australia might have been existence of a white swan. Upon observation, they concluded that black swans exist.
The bird had all other properties of the white swan, except for the color. There were no parameter changes, no bifurcation
that occurred. Similarly, the prediction of the dynamics while on one wing of the Lorenz butterÔ¨Çy attractor is based on the
eigenvalues of the Koopman operator detected while sampling that wing. Once the dynamics "travels" to the other wing, the
change in dynamics is recognized (although there are no parameter changes), but as the dynamics continues on the other wing,
the same eigenvalues are obtained. The difference is in the resulting local4eigenfunctions, (or pseudoeigenfunctions, as in53, 54),
that are related by the symmetry (x;y)!( x; y).
As is well known, chaotic dynamics is an asymptotic property of a dynamical system, and the associated unpredictability
is not due to local passage near saddles, but to long term repeat of such events, that ultimately leads to mixing dynamics56.
Switch in dynamics is here due to internal effects, and thus ontologically different from the Black Swan situation. The switch is
due - in the Lorenz case - precisely to the local saddle event, that transitions the dynamics from one wing of the butterÔ¨Çy to the
other. We presented a method by which such passage can actually be detected, and accounted for, inside a prediction algorithm.
We note there are methods of prediction of chaotic dynamical systems that can predict the evolution over several Lyapunov
times of chaotic systems57.
Note that our purpose is somewhat different than in57, We are more interested in detection of failure to predict accurately,
then establishing a method for long-term (climate) prediction. In separate work27we pursue the question of long-term prediction
of the Lorenz model and provide evidence of ability of Koopman based methods to predict over many Lyapunov time-scales.
S2.2 Case study: Resonant breathing
The mathematical model of the human cardiovascular system was developed by Ursino and Magosso in (58,59,60,61). This
model includes mathematical descriptions of a pulsating heart, as well as the mechanics of blood Ô¨Çow (62) and baroreÔ¨Çex
activation (58,59,60,61). It includes more than 90parameters and 21states (pressures, Ô¨Çows, volumes, resistances, and elastances).
Twenty-one delay differential equations reÔ¨Çect conservation of mass and balance of forces at arteries and veins, as well as
delayed physiological responses to vagal and sympathetic neural activity. This allows for simulation of high-resolution blood
pressure and heart period as a function of time. In63we modiÔ¨Åed the Ursino and Magosso model to use experimentally derived
respiration period as a model input. In addition, we set external noise from the Ursino and Magosso model to zero, because of
the noise in the respiratory input used in our model.
Data for model validation were provided by 12men and 12women who were healthy college students between 21and24
year of age. They were participants in an experiment one of the aims of which was to develop a computational physiology
approach to model how cardiovascular processes change when the baroreÔ¨Çex mechanism is challenged. This study was approved
by the Rutgers University Institutional Review Board for the protection of human subjects involved in research. One of the
tasks that the participants completed, was a 5-minutes resonance breathing task (6P) (64,65,66,67), during which they breathed
at a rate of approximately 6breaths/min following a visual pacer (Easy Air, Biofeedback Foundation of Europe, Montreal,
Canada). The speciÔ¨Åc details on the participants‚Äô selection/exclusion process and experimental procedure can be found in63.
In63, to Ô¨Ånd the optimal set of parameters for each subject, we selected as model output the cost function that takes into
account power spectral densities and time averages of several observables, such as heart period. Instead of doing brute-force
optimization on the cost function with over 90parameters in the model, we used the following procedure: 1) an initial sensitivity
analysis was performed to select the most important parameters to tune, and 2) optimization of these most important parameters
was performed to minimize the cost function. The details of the calibration procedure can be found in63.
We use the results of the chosen simulation to analyze the global prediction algorithm on it. The numerical solutions
were obtained using AIMdyn‚Äôs GOSUMD software. The used time step for numerical simulations was Dt0=0:003. The
parameters in the simulations, with the exception of the function modeling breathing, were chosen as obtained in63. The input
breathing function in this simulation was chosen so that in the Ô¨Årst part of simulation, the period of input breathing function
was constant and equal to 10seconds. The period of 10second simulates the rhythm of resonant breathing. In the second half
of the simulation, experimentally determined normal breathing function was used as an input.
As already mentioned, in the global prediction algorithm one should provide long enough set of data in order to extract the
dynamical system parameters related to the phenomena we want to capture with the algorithm. On the other hand, the time step
between the neighboring snapshots should be chosen so that the balance between the numerical complexity and of the length of
dynamical phenomena one want to reveal with used KMD algorithm is achieved.
33/41Since in the system there are no frequencies larger than 10, it is enough to take Dt=0:03between neighboring snapshots.
By using time-lagged embedding for each variable separately, we form the Hankel matrices and apply the GKP for the
reconstruction and prediction. In the computations we present here, we use the training sets that consists of 900snapshots
and Hankel matrices of dimension 600300sliding along the computational domain with the chosen step. The length of the
training sets was chosen so that at least two time periods of the global disturbance that we want to reveal are included in them.
The switching moment from resonant to normal breathing is nicely detected by eigenvalues provided by DMD algorithm in
Figure S16. The nature of eigenvalues changes signiÔ¨Åcantly in the active window beginning at t=150when switching of the
dynamics occurs. It is nicely visible from Figures S16 ‚Äì S17 that when the training set is in the zone of the resonant breathing,
theGKP results are perfect in that same zone, and then deteriorate as we move into the normal breathing zone. This is as
expected since after the moment of transition from the resonant to normal breathing the dynamical system is not governed by
the same set of parameters. When the training set is in the zone of the normal breathing, the GKP in that zone is much less
accurate then in the resonant breathing zone. It catches well the global behavior but it is poor in the details.
Figure S16. Physiology model. The real and the imaginary parts of eigenvalues for sliding active windows for which the
residuals are smaller than the threshold hr=0:025. The intensity of color of eigenvalues is associated with the amplitude of
modes.
Figure S17. Physiology model. Extrasplanchnic peripheral resistance and splanchnic venous unstressed volume. First
column: Reconstruction and prediction obtained using GKP for the chosen active windows. The full ( r=300) and reduced
(r<300) prediction obtained with GKP on active windows in the Ô¨Årst part of simulation in the breathing zones captures
accurately the dynamics, while even the full prediction obtained with the training set in the resonant breathing zone does not
capture well the dynamics. Second column: The prediction errors for the full and reduced prediction (with rmodes) for the
chosen active window in the Ô¨Årst part of simulation.
What we can conclude from the presented results is the following. When the training set is in the zone of the resonant breathing,
theGKP algorithm results are perfect in that same zone, and then deteriorate as we move into the normal breathing zone. This
34/41is as expected since after the moment of transition form the resonant to normal breathing the dynamical system is not governed
by the same set of parameters. When the training set is in the zone of the normal breathing, the GKP in that same normal
breathing zone is much less accurate then in the resonant breathing zone. It catches well the global behavior but it is poor in the
detail, most of the error value comes from the difference in the phase. This is also logical since normal breathing is much more
irregular than the resonant breathing and it turns out that it can not be learned with high accuracy.
S2.3 Case study: Geomagnetic substorms
Geomagnetic storms and substorms are violent disturbances of the Earth‚Äôs magnetosphere, caused by energy transfer of the
solar wind into the planets magnetosphere, with potentially severe impact on the human civilization6869.
Physics-based modeling (see e.g.70,71,72) of geomagnetic substorms and storm/substorm interaction is a challenging task
and the subject of intensive study. It must cover multiscale, nonlinear interactions of plasmas that are not in equilibrium, or are
in an unstable equilibrium, which makes such modeling difÔ¨Åcult to apply when prediction is needed, see e.g.73.
On the other hand, given an abundance of observation data, a data-driven approach is an attractive alternative; see e.g.74,75.
The intensity of a substorm is quantiÔ¨Åed by the Auroral Electrojet ( AE) index, the AL, which is a measure of the magnitude
of the geomagnetic Ô¨Åeld disturbances on the ground induced by ionospheric currents developed during substorm. Other
information such as e.g. solar wind data76, the Dstindex, and other substorm signature indices may be available77and used as
observables.
For the purpose of this case study of the proposed approach as a purely data driven black-box methodology, we
choose to use the ALindex as the only observable; the data are downloaded from the Kyoto Geomagnetism Data Ser-
vice (http://wdc.kugi.kyoto-u.ac.jp/ ).
The presented results are obtained by using global prediction algorithm with the active windows of size 30and the Hankel
matrices of dimension 2010. By sliding the active windows along the computational domain we get prediction at different
time moments. In Figure S18 we present the obtained reconstruction and prediction results for four active windows. Note that
the modal representation of the signal is good, and it could provide a valuable insights to the experts in magnetic storm physics.
In the framework of our theory, the poor global prediction results are to be expected ‚Äì almost all eigenvectors used in the
KMD have large residuals. This once more justiÔ¨Åes our approach, based on using the residuals of the Ritz pairs, computable
even in the data driven setting, using the method from12. However, large prediction errors trigger the switch to the local
prediction algorithm, which delivers more accurate predictions, at least for shorter lead time, as shown in Figure S18.
Figure S18. Geomagnetic substorms data: KMD reconstruction and prediction of the ALindex. For reconstruction and global
prediction, the data are collected in four active windows (time intervals [900;1050],[1200;1350],[1600;1750],[2000;2150],
indicated by shadowed rectangles), the algorithm uses 2010Hankel matrices and then the dynamics is predicted for 20time
steps ahead. The time resolution of the collected data is dt=5min. The local prediction algorithm uses 32matrices and the
error threshold for resizing the Hankel matrix to minimal size (switching to the local algorithm) is set to 10. The dynamics with
the local prediction algorithm is predicted two time steps ahead.
S2.4 Additional numerical results for prediction of inÔ¨Çuenza cases
Here we provide some additional results for the material in ¬ß2 of the paper. In Figures S19‚ÄìS22 we show prediction of the
dynamics of inÔ¨Çuenza for USA and UK for 2and52weeks ahead, obtained with the KMD decompositions in the global
prediction algorithm, using sliding active windows of size 312.
In Figure S23, we provide additional numerical illustration (most relevant eigenvalues before and after retouching, with the
corresponding prediction errors, and relation with the dominant frequencies from the DFT analysis) related to Figure 1b in the
35/41Figure S19. InÔ¨Çuenza data (USA). Global Koopman prediction on inÔ¨Çuenza with the size of active windows 312 and
different sizes of Hankel matrices. The prediction 2 weeks ahead by using KMD‚Äôs from the active windows sliding along
computational domain with sliding step Dp=1.
main paper.
References
1.Mezi ¬¥c, I. & Banaszuk, A. Comparison of systems with complex behavior. Phys. D: Nonlinear Phenom. 197, 101‚Äì133
(2004).
2.Levnaji ¬¥c, Z. & Mezi ¬¥c, I. Ergodic theory and visualization. i. mesochronic plots for visualization of ergodic partition and
invariant sets. Chaos: An Interdiscip. J. Nonlinear Sci. 20, 033114 (2010).
3.Mohr, R. & Mezi ¬¥c, I. Construction of eigenfunctions for scalar-type operators via laplace averages with connections to the
Koopman operator. arXiv preprint arXiv:1403.6559 (2014).
4.Mezi ¬¥c, I. Spectrum of the Koopman operator, spectral expansions in functional spaces, and state-space geometry. J.
Nonlinear Sci. 1‚Äì55 (2019).
5.Korda, M. & Mezi ¬¥c, I. On convergence of extended dynamic mode decomposition to the Koopman operator. J. on
Nonlinear Sci. 28, 687‚Äì10 (2018).
6.Das, S. & Yorke, J. A. Super convergence of ergodic averages for quasiperiodic orbits. Nonlinearity 31, 491 (2018).
7.Arbabi, H. & Mezi ¬¥c, I. Ergodic theory, dynamic mode decomposition, and computation of spectral properties of the
Koopman operator. SIAM J. on Appl. Dyn. Syst. 16, 2096‚Äì2126 (2017).
8.Mezi ¬¥c, I. & Runolfsson, T. Uncertainty propagation in dynamical systems. Automatica 44, 3003‚Äì3013 (2008).
9.Budi≈°i ¬¥c, M., Mohr, R. & Mezi ¬¥c, I. Applied Koopmanism. Chaos: An Interdiscip. J. Nonlinear Sci. 22(2012).
10.Susuki, Y ., Mezic, I., Raak, F. & Hikihara, T. Applied Koopman operator theory for power systems technology. Nonlinear
Theory Its Appl. IEICE 7, 430‚Äì459, DOI: 10.1587/nolta.7.430 (2016).
11.Williams, M., Kevrekidis, I. & Rowley, C. A data-driven approximation of the Koopman operator: extending dynamic
mode decomposition. J. Nonlinear Sci. 25, 1307‚Äì1346 (2015).
12.Drma Àác, Z., Mezi ¬¥c, I. & Mohr, R. Data driven modal decompositions: Analysis and enhancements. SIAM J. on Sci. Comput.
40, A2253‚ÄìA2285, DOI: 10.1137/17M1144155 (2018). https://doi.org/10.1137/17M1144155.
13.Drma Àác, Z., Mezi ¬¥c, I. & Mohr, R. Data driven Koopman spectral analysis in Vandermonde‚ÄìCauchy form via the DFT:
Numerical method and theoretical insights. SIAM J. on Sci. Comput. 41, A3118‚ÄìA3151, DOI: 10.1137/18M1227688
(2019). https://doi.org/10.1137/18M1227688.
36/41Figure S20. InÔ¨Çuenza data (USA). Global Koopman prediction on inÔ¨Çuenza with the size of active windows 312 and
different sizes of Hankel matrices. The prediction 52 weeks ahead by using KMD‚Äôs from the active windows sliding along
computational domain with sliding step Dp=1.
14.Drma Àác, Z., Mezi ¬¥c, I. & Mohr, R. On least squares problems with certain Vandermonde-Khatri-Rao structure with
applications to DMD. SIAM J. on Sci. Comput. 42, A3250‚ÄìA3284, DOI: 10.1137/19M1288474 (2020). https://doi.org/10.
1137/19M1288474.
15.Taleb, N. N. The Black Swan: The Impact of the Highly Improbable (Random House Group, 2007).
16.Singh, R. K. & Manhas, J. S. Composition operators on function spaces , vol. 179 of North-Holland Mathematics Studies
(North Holland, 1993).
17.Eisner, T., Farkas, B., Haase, M. & Nagel, R. Operator theoretic aspects of ergodic theory , vol. 272 of Graduate Texts in
Mathematics (Springer, 2015).
18.Pilyugin, S. Y . Theory of pseudo-orbit shadowing in dynamical systems. Differ. Equations 47, 1929‚Äì1938, DOI:
10.1134/S0012266111130040 (2011).
19.Pilyugin, S. Shadowing in Dynamical Systems , vol. 1706 of Lecture Notes in Mathematics (Springer, 1999).
20.Mezi ¬¥c, I. Spectral properties of dynamical systems, model reduction and decompositions. Nonlinear Dyn. 41, 309‚Äì325
(2005).
21.Mezi ¬¥c, I. Spectrum of the Koopman operator, spectral expansions in functional spaces, and state-space geometry. J.
Nonlinear Sci. 1‚Äì55 (2019).
22.Mezi ¬¥c, I. Spectrum of the Koopman operator, spectral expansions in functional spaces, and state-space geometry. J.
Nonlinear Sci. DOI: 10.1007/s00332-019-09598-5 (2019).
23.Giannakis, D. Data-driven spectral decomposition and forecasting of ergodic dynamical systems. Appl. Comput. Harmon.
Analysis 47, 338 ‚Äì 396, DOI: https://doi.org/10.1016/j.acha.2017.09.001 (2019).
24.Govindarajan, N., Mohr, R., Chandrasekaran, S. & Mezi ¬¥cc, I. On the approximation of Koopman spectra for measure
preserving transformations. SIAM J. on Appl. Dyn. Syst. 18, 1454‚Äì1497, DOI: 10.1137/18M1175094 (2019). https:
//doi.org/10.1137/18M1175094.
25.Ghosal, S., Ramanan, V ., Sarkar, S., Chakravarthy, S. & Sarkar, S. Detection and analysis of combustion instability from
hi-speed Ô¨Çame images using dynamic mode decomposition. In ASME. Dynamic Systems and Control Conference, Volume
1, DOI: 10.1115/DSCC2016-9907 (2016).
26.Tu, J. H., Rowley, C. W., Luchtenburg, D. M., Brunton, S. L. & Kutz, J. N. On dynamic mode decomposition: Theory and
applications. J. Comput. Dyn. 1, 391‚Äì421, DOI: 10.3934/jcd.2014.1.391 (2014).
37/41Figure S21. InÔ¨Çuenza data (UK). Global Koopman prediction on inÔ¨Çuenza with the size of active windows 312 and different
sizes of Hankel matrices. The prediction 2 weeks ahead by using KMD‚Äôs from the active windows sliding along computational
domain with sliding step Dp=1.
27.Drma Àác, Z., Mezi ¬¥c, I. & Mohr, R. IdentiÔ¨Åcation of nonlinear systems using the inÔ¨Ånitesimal generator of the Koopman
semigroup ‚Äì a numerical implementation of the Mauroy-Goncalves method. Mathematics 9, 2075, DOI: 10.3390/
math9172075 (2021).
28.Schmid, P. Dynamic mode decomposition of numerical and experimental data. J. Fluid Mech. 656, 5‚Äì28 (2010).
29.Pearson, K. On lines and planes of closest Ô¨Åt to systems of points in space. Philos. Mag. 2, 559‚Äì572 (1901).
30.Eckart, C. & Young, G. The approximation of one matrix by another of lower rank. Psychometrika 1, 211‚Äì218, DOI:
10.1007/BF02288367 (1936).
31.Mirsky, L. Symmetric gauge functions and unitarily invariant norms. The Q. J. Math. 11, 50, DOI: 10.1093/qmath/11.1.50
(1960).
32.Golub, G. H., Klema, V . C. & Stewart, G. W. Rank degeneracy and least squares problems. Tech. Rep. CS-TR-76-559,
STANFORD UNIV CA DEPT OF COMPUTER SCIENCE, Stanford, CA, USA (1976).
33.Taira, K. et al. Modal analysis of Ô¨Çuid Ô¨Çows: An overview. AIAA J. 55, 4013‚Äì4041 (2017).
34.Chen, K. K., Tu, J. H. & Rowley, C. W. Variants of dynamic mode decomposition: Boundary condition, Koopman, and
Fourier analyses. J. Nonlinear Sci. 22, 887‚Äì915 (2012).
35.Hemati, M. S., Rowley, C. W., Deem, E. A. & Cattafesta, L. N. De-biasing the dynamic mode decomposition for applied
Koopman spectral analysis. ArXiv e-prints (2015). 1502.03854.
36.Dawson, S. T. M., Hemati, M. S., Williams, M. O. & Rowley, C. W. Characterizing and correcting for the effect of sensor
noise in the dynamic mode decomposition. Exp. Fluids 57, 42, DOI: 10.1007/s00348-016-2127-7 (2016).
37.Hemati, M. S., Williams, M. O. & Rowley, C. W. Dynamic mode decomposition for large and streaming datasets. Phys.
Fluids 26, 111701 (2014).
38.Takeishi, N., Kawahara, Y . & Yairi, T. Subspace dynamic mode decomposition for stochastic Koopman analysis. Phys.
Rev. E 96, 033310, DOI: 10.1103/PhysRevE.96.033310 (2017).
39.Takeishi, N., Kawahara, Y ., Tabei, Y . & Yairi, T. Bayesian dynamic mode decomposition. In Proceedings of the Twenty-
Sixth International Joint Conference on ArtiÔ¨Åcial Intelligence, IJCAI-17 , 2814‚Äì2821, DOI: 10.24963/ijcai.2017/392
(2017).
40.Takeishi, N., Kawahara, Y . & Yairi, T. Sparse nonnegative dynamic mode decomposition. In 2017 IEEE International
Conference on Image Processing (ICIP) , 2682‚Äì2686, DOI: 10.1109/ICIP.2017.8296769 (2017).
38/41Figure S22. InÔ¨Çuenza data (UK). Global Koopman prediction on inÔ¨Çuenza with the size of active windows 312 and different
sizes of Hankel matrices. The prediction 52 weeks ahead by using KMD‚Äôs from the active windows sliding along
computational domain with sliding step Dp=1.
(a)Before retouching
 (b)After retouching
 (c)DFT analysis.
Figure S23. InÔ¨Çuenza data (USA). The most relevant eigenvalues and the prediction errors in the global algorithm (using
208104 Hankel matrices) for the active window as in Figure 1b in the main paper. Note how the unstable eigenvalues
(√Ç(l)>0) impact the prediction performance, and how the retouching moves them to the left. Compare with Figures 1b and 2
in the main paper. Note how the dominant frequencies from the DFT analysis correspond to the imaginary parts of the
eigenvalues computed after the retouching and selected by the residual criterion.
41.Takeishi, N., Kawahara, Y . & Yairi, T. Learning Koopman invariant subspaces for dynamic mode decomposition. In
Guyon, I. et al. (eds.) Advances in Neural Information Processing Systems 30 , 1130‚Äì1140 (Curran Associates, Inc., 2017).
42.Proctor, J., Brunton, S. & Kutz, J. Dynamic mode decomposition with control. SIAM J. on Appl. Dyn. Syst. 15, 142‚Äì161,
DOI: 10.1137/15M1013857 (2016). https://doi.org/10.1137/15M1013857.
43.Askham, T. & Kutz, J. Variable projection methods for an optimized dynamic mode decomposition. SIAM J. on Appl. Dyn.
Syst. 17, 380‚Äì416, DOI: 10.1137/M1124176 (2018). https://doi.org/10.1137/M1124176.
44.Jovanovi ¬¥c, M. R., Schmid, P. J. & Nichols, J. W. Sparsity-promoting dynamic mode decomposition. Phys. Fluids 26,
024103 (2014).
45.Tu, J. H., Rowley, C. W., Luchtenburg, D. M., Brunton, S. L. & Kutz, J. N. On dynamic mode decomposition: theory and
applications. J. Comput. Dyn. 1, 391‚Äì421 (2014).
46.Arbabi, H. & Mezi ¬¥c, I. Ergodic theory, Dynamic Mode Decomposition and Computation of Spectral Properties of the
Koopman operator. ArXiv e-prints (2016). 1611.06664.
47.Hale, T., Webster, S., Petherick, A., Phillips, T. & Kira, B. Oxford COVID-19 government response tracker. Tech. Rep.,
Blavatnik School of Government (2020).
39/4148.Petherick, A. et al. Variation in government responses to COVID-19. Tech. Rep. BSG-WP-2020/032, Blavatnik School of
Government (2020).
49.Gostic, K. M. et al. Practical considerations for measuring the effective reproductive number, rt.PLoS computational
biology 16, e1008409 (2020).
50.Metzner, P., Putzig, L. & Horenko, I. Analysis of persistent nonstationary time series and applications. Commun. Appl.
Math. Comput. Sci. 7, 175‚Äì229, DOI: 10.2140/camcos.2012.7.175 (2012).
51.Lorenz, E. N. Deterministic nonperiodic Ô¨Çow. J. atmospheric sciences 20, 130‚Äì141 (1963).
52.Brunton, S. L., Brunton, B. W., Proctor, J. L., Kaiser, E. & Kutz, J. N. Chaos as an intermittently forced linear system. Nat.
communications 8, 1‚Äì9 (2017).
53.Korda, M., Putinar, M. & Mezi ¬¥c, I. Data-driven spectral analysis of the Koopman operator. Appl. Comput. Harmon.
Analysis 48, 599‚Äì629 (2020).
54.Giannakis, D., Das, S. & Slawinska, J. Reproducing kernel hilbert space compactiÔ¨Åcation of unitary evolution groups.
arXiv preprint arXiv:1808.01515 (2018).
55.Iooss, G. & Joseph, D. D. Elementary stability and bifurcation theory (Springer Science & Business Media, 2012).
56.Luzzatto, S., Melbourne, I. & Paccaut, F. The Lorenz attractor is mixing. Commun. Math. Phys. 260, 393‚Äì401 (2005).
57.Pathak, J., Lu, Z., Hunt, B. R., Girvan, M. & Ott, E. Using machine learning to replicate chaotic attractors and calculate
lyapunov exponents from data. Chaos: An Interdiscip. J. Nonlinear Sci. 27, 121102 (2017).
58.Magosso, E. & Ursino, M. Cardiovascular response to dynamic aerobic exercise: A methematical model. Med. Biol. Eng.
Comput. 40, 660‚Äì674 (2002).
59.Ursino, M. Interaction between carotid baroregulation and the pulsating heart: a mathematical model. Am. J. Physiol. Circ.
Physiol. 275, H1733‚ÄìH1747 (1998).
60.Ursino, M. & Magosso, E. Acute cardiovascular response to isocapnic hypoxia. i. a mathematical model. Am. J. Physiol.
Circ. Physiol. 279, H149‚ÄìH165 (2000).
61.Ursino, M. & Magosso, E. Role of short-term cardiovascular regulation in heart period variability: a modeling study. Am.
J. Physiol. Circ. Physiol. 53, H1479 (2003).
62.Stefanovska, A. Physics of the human cardiovascular system. Contemp. Phys. 40, 31‚Äì55 (1999).
63.Fonoberova, M. et al. A computational physiology approach to personalized treatment models: the beneÔ¨Åcial effects of
slow breathing on the human cardiovascular system. Am. J. Physiol. Circ. Physiol. 307, H1073‚ÄìH1091 (2014).
64.Lehrer, P. M. et al. Heart rate variability biofeedback increases baroreÔ¨Çex gain and peak expiratory Ô¨Çow. Psychosom.
medicine 65, 796‚Äì805 (2003).
65.Lin, G. et al. Heart rate variability biofeedback decreases blood pressure in prehypertensive subjects by improving
autonomic function and baroreÔ¨Çex. The J. Altern. Complementary Medicine 18, 143‚Äì152 (2012).
66.Vaschillo, E., Lehrer, P., Rishe, N. & Konstantinov, M. Heart rate variability biofeedback as a method for assessing
baroreÔ¨Çex function: a preliminary study of resonance in the cardiovascular system. Appl. psychophysiology biofeedback
27, 1‚Äì27 (2002).
67.Vaschillo, E. G., Vaschillo, B., Buckman, J. F., Pandina, R. J. & Bates, M. E. Measurement of vascular tone and stroke
volume baroreÔ¨Çex gain. Psychophysiology 49, 193‚Äì197 (2012).
68.Space Weather Prediction Center, National Oceanic and Atmospheric Administration. https://www.swpc.noaa.gov/.
Accessed: 2020-01-04.
69.Hapgood, M. The great storm of may 1921: An exemplar of a dangerous space weather event. Space Weather. 17, 950‚Äì975
(2019).
70.Spencer, E., Vadepu, S. K., Srinivas, P., Patra, S. & Horton, W. The dynamics of geomagnetic substorms with the WINDMI
model. Earth, Planets Space 70(2018).
71.Sharma, A. S., Kamide, Y . & (Eds.), G. S. L. Disturbances in Geospace: The Storm-Substorm Relationship , vol. 142 of
Geophysical Monograph Series (American Geophysical Union, 2003).
72.Kamide, Y . et al. Current understanding of magnetic storms: Storm-substorm relationships. J. Geophys. Res. Space Phys.
103, 17705‚Äì17728, DOI: 10.1029/98JA01426 (1998).
40/4173.Morley, S. K., Freeman, M. P., & Tanskanen, E. I. A comparison of the probability distribution of observed substorm
magnitude with that predicted by a minimal substorm model. Ann. Geophys. 25, 2427‚Äì2437 (2007).
74.Giannakis, D., Gkioulidou, M. & Harlim, J. Probabilistic prediction of the AL index with the diffusion forecasting model.
arXiv e-prints arXiv:1612.07272 (2016). 1612.07272.
75.Maimaiti, M., Kunduri, B., Ruohoniemi, J. M., Baker, J. B. H. & House, L. L. A deep learning-based approach to forecast
the onset of magnetic substorms. Space Weather. 17(2019).
76.Newell, P. et al. Substorm probabilities are best predicted from solar wind speed. J. Atmospheric Solar-Terrestrial Phys.
146, 28 ‚Äì 37, DOI: https://doi.org/10.1016/j.jastp.2016.04.019 (2016).
77.Nos√©, M. et al. Wp index: A new substorm index derived from high-resolution geomagnetic Ô¨Åeld data at low latitude.
Space Weather. 10, DOI: 10.1029/2012SW000785 (2012).
41/41