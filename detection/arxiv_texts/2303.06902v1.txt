MOLECULAR PROPERTY PREDICTION BY
SEMANTIC -INVARIANT CONTRASTIVE LEARNING
Ziqiao Zhang
Fudan University
Shanghai
zqzhang18@fudan.edu.cnAilin Xie
Fudan University
Shanghai
alxie21@m.fudan.edu.cn
Jihong Guan
Tongji University
Shanghai
jhguan@tongji.edu.cnShuigeng Zhou
Fudan University
Shanghai
sgzhou@fudan.edu.cn
ABSTRACT
Contrastive learning have been widely used as pretext tasks for self-supervised pre-trained molecular
representation learning models in AI-aided drug design and discovery. However, exiting methods
that generate molecular views by noise-adding operations for contrastive learning may face the
semantic inconsistency problem, which leads to false positive pairs and consequently poor prediction
performance. To address this problem, in this paper we Ô¨Årst propose a semantic-invariant view
generation method by properly breaking molecular graphs into fragment pairs. Then, we develop
a Fragment-based Semantic-Invariant Contrastive Learning (FraSICL) model based on this view
generation method for molecular property prediction. The FraSICL model consists of two branches
to generate representations of views for contrastive learning, meanwhile a multi-view fusion and an
auxiliary similarity loss are introduced to make better use of the information contained in different
fragment-pair views. Extensive experiments on various benchmark datasets show that with the least
number of pre-training samples, FraSICL can achieve state-of-the-art performance, compared with
major existing counterpart models.
Keywords Molecular representation learning Molecular property prediction Contrastive learning
1 Introduction
Nowadays molecular property prediction (MPP) based on deep learning techniques has been a hot research topic of
the AI-aided Drug Discovery (AIDD) community [ 1,2,3,4,5,6,7]. As most of the molecular properties that drug
discovery studies concern require in vivo orin vitro wet-lab experiments to measure, labeled data for MPP tasks are
typically scarce, because it is expensive and time-consuming to acquire such data [ 8]. On the contrary, there are large
amounts of public available unlabeled data [ 9,10,11]. Therefore, how to use these large-scale unlabeled molecular
data to train deep neural networks to learn better molecular representations for MPP tasks, is of great interest to the
AIDD community.
Recently, as self-supervised pre-trained models (e.g. BERT [ 12], MoCo [ 13] and SimCLR [ 14]) have shown signiÔ¨Åcant
superiority in the Ô¨Åelds of Natural Language Processing (NLP) and Computer Vision (CV), self-supervised learning
(SSL) has become a mainstream method of utilizing large-scale unlabeled molecular data in MPP study. These
SSL methods typically use some inherent features within or between samples to construct pretext tasks, so that
unlabeled data can be leveraged to train deep models in a self-supervised learning manner [ 15]. Contrastive learning,
masked language model and predictive learning are the currently three categories of methods to design pretext tasks
in MPP studies [ 16,17,18,19,20,21,22,23]. Inspired by SimCLR, contrastive learning methods aim at learning
representations through contrasting positive data pairs against negative ones [ 16]. Original molecular structures arearXiv:2303.06902v1  [q-bio.BM]  13 Mar 2023FraSICL
augmented into multiple views, and views generated from the same molecule are typically used as positive data pairs,
while views of different molecules are taken as negative ones [16].
The way to generate molecular views is crucial to the design of contrastive learning pretext tasks for molecular
representation learning. As a kind of special objects, molecules can be represented by different methods, including
molecular Ô¨Ångerprints [ 24], SMILES [ 25], IUPAC [ 26], and molecular graph. These different molecular representation
methods therefore can naturally be leveraged to generate views for contrastive learning. For instance, the DMP [ 17] and
MEMO [ 18] models are designed in this way. Following the practice in CV , another widely used category of methods
tries to add noise into molecular structures to generate transformations of the original molecules. These noise-adding
operations include deleting atoms, replacing atoms, deleting bonds, deleting subgraph structures etc. MolCLR [ 16] and
GraphLoG [21] are such representative models.
Although the noise-adding methods for view generation have been widely used in CV studies [ 14,27], when applying
these methods into MPP tasks, a fact that has not been noticed by the researchers is that molecules are very sensitive
to noise. Arbitrarily modifying the topological structure of a molecule with noise, the generated new structure may
represent a totally different molecule. For instance, as shown in Fig. 1(a), adding noise into an dog image by randomly
masking some area will not change the semantic of the generated view, which is still a yellow dog. However, in Fig. 1(b),
for an acetophenone molecule, deleting a subgraph leads to a benzene molecule, indicating that acetophenone‚Äôs chemical
semantic is completely changed. And small modiÔ¨Åcation to molecular structure can lead to dramatic changes in the
properties of modiÔ¨Åed molecules, including both bio-activity and other physio-chemical properties. Concretely, from
the PubChem database we can Ô¨Ånd that the LogP value of acetophenone in Fig. 1(b) is 1.58, while that of benzene is
2.13. The difference is almost 35%. Therefore, it is unreasonable to treat these two views (molecules) as a positive pair
for contrastive learning.
OCH3
OCH3(a)(b)OriginalsampleAfternoise-adding
benzeneacetophenone
Figure 1: Illustration of the inÔ¨Çuence of noise-adding operation to the semantic of generated view. (a) After adding noise
to the image by randomly masking some area, the semantic of dog image does not change, i.e., the image still represents
a dog. (b) By adding noise into the molecular structure by masking some atoms and edges, the molecule acetophenone
becomes a completely different molecule benzene with different molecular properties. That is, the chemical semantic of
acetophenone is completely changed.
Aiming at solving this semantic inconsistency problem, this paper proposes a Fragment-based Semantic- Invariant
Contrastive Learning molecular representation model, named FraSICL. A semantic-invariant molecular view generation
method is developed, in which a molecular graph is properly broken into fragments by changing the message passing
topology while preserving the topological information. A multi-view fusion mechanism is introduced to FraSICL to
make better use of the information contained in views of different fragments and avoid the impact of randomness. In
addition, an auxiliary similarity loss is designed to train the backbone Graph Neural Network (GNN) to generate better
representation vectors.
Our contribution are summarized as follows:
‚Ä¢We raise the semantic inconsistency problem in molecular view construction for molecular constrastive
learning and develop an effective method to generate semantic-invariant graph views by changing message
passing topology while preserving the topological information.
‚Ä¢We propose a novel Fragment-based Semantic-Invariant Contrastive Learning molecular representation model
for effective molecular property prediction, which is also equipped with a multi-view fusion mechanism and
an auxiliary similarity loss to better leverage the information contained in unlabeled pre-training data.
2FraSICL
‚Ä¢Extensive experiments show that compared with SOTA pre-trained molecular property prediction models, the
proposed FraSICL can achieve better prediction accuracy on downstream target tasks with less amounts of
unlabeled pre-training data.
2 Method
Here, we Ô¨Årst formally deÔ¨Åne semantic-invariant molecular view in Sec. 2.1, then propose a semantic-invariant molecular
view generation method in Sec. 2.2 and a multi-view fusion scheme in Sec. 2.3. Finally, we present the structure of the
Fragment-based Semantic-Invariant Contrastive Learning (FraSICL) molecular representation model in Sec. 2.4 and its
loss functions in Sec. 2.5.
2.1 Semantic-invariant Molecular View
In Sec. 1, we give an example to illustrate how noise-adding operations may lead to semantic inconsistency and
consequently false positive pairs. Here, we will formally deÔ¨Åne semantic-invariant molecular view .
Given a molecule mand its molecular graph G=fV;E;X atom;Xbondg(hydrogen-depleted) where Vdenotes the set
of nodes that represent the atoms, Edenotes the set of edges between nodes, representing the bonds. Xatom andXbond
are feature matrix of atoms and bonds respectively. A transformation function F()is used to generate a molecular
graph view (or simply molecular view )G0ofG, i.e.,G0=F(G)andG0=fV0;E0;X0
atom;X0
bondg. In what follows, we
Ô¨Årst deÔ¨Åne two types of semantic inconsistent views.
DeÔ¨Ånition 1 (Semantic-conÔ¨Çict view) If there is another molecule m2whose molecular graph is G2, and
G2=G0=F(G), i.e., the view G0ofmis the same as the molecular graph G2ofm2, then we say G0is a semantic-conÔ¨Çict
view ofmwith regard to (w.r.t.) m2.
DeÔ¨Ånition 2 (Semantic-ambiguity view) If there exists another molecule m2whose molecular graph is G2, and
G0
2=F(G2)=G0=F(G), i.e., the view G0ofmis the same as a view G0
2ofm2. Then, we say G0is a semantic-ambiguity
view of molecule mw.r.t. molecule m2.
Both semantic-conÔ¨Çict views and semantic-ambiguity views will lead to false positive pairs for molecular representation
contrastive learning. For example, assume that a Graph Neural Network g()serves as an encoder to embed the
molecular graphs into latent graph embeddings hG=g(G). If we ignore the randomness in the encoder, it is obvious
that, for molecule m, if it has a semantic-conÔ¨Çict view w.r.t. molecule m2, i.e.,G0=F(G)=G2, then the representation
ofG0embedded by the graph neural network will be the same as that of G2. That is, hG0=g(G0)=hG2=g(G2). In this
case, as hGandhG0are considered as a positive pair in contrastive learning, hGandhG2are consequently used as a
positive pair. In another word, the contrastive loss will implicitly make the representations of molecule mandm2to be
close. However, as claimed before, the molecular properties of different molecules may be greatly different, so that they
cannot be used as a positive pair for contrastive learning. Therefore, semantic-conÔ¨Çict views will lead to false positive
pairs and degrade learning performance.
On the other hand, if a semantic-ambiguity view is generated as deÔ¨Åned in Def. 2, i.e., F(G2)=G0=F(G), indicating
that the contrastive loss will make hGandhG0,hG2andhG0to be close in the embedding space, thus hGandhG2to
be close too. That is, the representations of molecules mandm2are consequently close by contrastive learning. So
semantic-ambiguity views will also lead to false positive pairs.
To boost the performance of contrastive learning for MPP, we should avoid the generation of both semantic-conÔ¨Çict
views and semantic-ambiguity views. That is, we generate only semantic-invariant views, which are deÔ¨Åned as follows:
DeÔ¨Ånition 3 (Semantic-invariant view) Given a view G0of molecule mwith graphG, ifG0is neither a semantic-
conÔ¨Çict view nor a semantic-ambiguity view w.r.t. any other molecules, then we say G0is a semantic-invariant view of
m.
In next section, we will give a method to generate semantic-invariant views.
2.2 Semantic-invariant View Generation
According to Def. 3 in Sec. 2.1, semantic-invariant views should be neither semantic-conÔ¨Çict views nor semantic-
ambiguity views. Besides, from the perspective of prediction, they should also be discriminative. That is, they can be
encoded into different representations by neural network encoders. In this section, to achieve these goals, we propose a
semantic-invariant view generation method.
3FraSICL
Table 1: Properties of atoms and bonds in Xatom andXbond.
Indices of atomic features Description
0-15 Atomic symbol, a one-hot vector of [B,C,N,O,F,Si,P,S,Cl,As,Se,Br,Te,I,At,metal]
16-21 Number of bonds
22 Electrical charge
23 Number of radical electrons
24-29 Hybridization, a one-hot vector of [sp, sp2, sp3, sp3d, sp3d2, other]
30 Aromaticity
31-35 Number of connected hydrogens
36 Whether the atom is a chiral center
37-38 Chirality type, a one-hot vector of [R,S]
Indices of bond features Description
0-3 Bond type, a one-hot vector of [single, double, triple, aromatic]
4 Whether the bond is conjugated
5 Whether the bond is in a ring
6-9 Stereo, a one-hot vector of [StereoNone, StereoAny, StereoZ, StereoE]
In our previous study [ 6], to better capture the hierarchical structural information of molecules, a chemical-interpretable
molecule fragmentization method FraGAT is proposed. By considering acyclic single bonds as boundaries between
functional groups, the FraGAT model proposes to randomly breaking one of the acyclic single bonds to generate two
graph fragments corresponding to some chemical meaningful functional groups. The experimental results show that
learning representations by chemical meaningful molecular graph fragments can achieve good predictive performance
for MPP tasks. Inspired by these Ô¨Åndings, our semantic-invariant view generation method is designed as follows:
Given a molecule m, its molecular graph can be denoted as an annotated graph G=fV;E;X atom;Xbondg. The atom
feature matrix Xatom and the bond feature matrix Xbond are computed according to Tab. 1. Then, remove one of the
acyclic single bonds eijfromE, we obtainG0=fV;E0;Xatom;XbondgwhereE0=E feijg. We accept G0as a
view to be generated, i.e., a semantic-invariant view. As the graph G0consists of two disconnected molecular graph
fragments, it is also called fragment-pair view .
From the discrimination perspective, G0is a different graph from the original molecular graph G, so that it will make
GNN encoders to generate a different representation. Furthermore, as all the acyclic single bonds in a molecule are
unique, breaking different acyclic single bonds will lead to different fragment-pair views, whose representations after a
GNN encoder will also be different. That is, the generated views for a molecule are discriminative.
Then, isG0a real semantic-invariant view of molecule maccording to Def. 3? Let us check.
On the one hand, as the atom feature matrix Xatom is not modiÔ¨Åed, the numbers of bonds of the two vertex iandjof
the broken bond eijencoded in the atom feature vectors of the generated G0remain the same as that in the original
molecular graph G. However, the modiÔ¨Åed E0indicates that there is no edge between iandj, so that the numbers of
bonds encoded in the atom feature vectors are not consistent with that in the topological structure. The degrees of node
iandjin graphG0are lower than the numbers of bonds of iandjencoded inXatom . In another word, G0is not a valid
molecular graph of any molecule. Furthermore, from the graph perspective, because G0consists of two disconnected
subgraphs, and no any valid molecule corresponds to a disconnected graph. Therefore, G0cannot be a semantic-conÔ¨Çict
view w.r.t. any molecule according to Def. 1.
On the other hand, since only one single bond is removed in view G0, this discrepancy can only be discovered at nodes
iandj, and the numbers of bonds of iandjencoded inXatom must be only 1 larger than the degrees of iandj, so the
removed edge can only be between iandj, and the removed edge can only be a single bond. Thus, there is no other
molecular graph G2that can generate the same G0. Similarly, from graph perspective, the graph Gof molecule m
cannot be equal to the disconnected graph of any view generated from any other molecule. In summary, G0cannot be a
semantic-ambiguity view w.r.t. any molecule according to Def. 2.
Finally, from the perspective of graph rewiring [ 28,29,30], the topological information about the broken edge is
encoded in the atom feature matrix Xatom. So our method preserves the topological structural information of the
original molecular graph, but propagates message between nodes through a different topology. Thus, it realizes local
decoupling of the input graph topology and the message passing topology. Moreover, compared with randomly breaking
4FraSICL
any edges in the molecular graph, our method can generate chemical meaningful graph fragments to beneÔ¨Åt the
prediction of molecular properties, which has been demonstrated in the experiments of previous work [6].
In conclusion, our method is expected to generate better positive pairs, which will help to train neural networks to
generate better molecular representations by contrastive learning.
2.3 Multi-view Fusion
The number of acyclic single bonds of an organic molecule is often large, so there are various fragment-pair views
can be generated from one molecule by our proposed view generation method. As demonstrated in the experiments of
some existing work [ 6], different fragment pairs contain different information about functional groups that constitute a
molecule, which shows different predictive performance. So, to ensure that the information of the functional groups
that determine molecular properties and are contained in the fragment pairs can be obtained by the neural network,
in FraSICL, we no longer randomly generate fragment-pair views as other contrastive learning models do. Instead, a
multi-view fusion mechanism is introduced as follows: Given a molecule with Nbbreakable acyclic single bonds, all of
theNbfragment-pair views are generated and the representations of these fragment-pair views are calculated by a GNN
encoder. Then, a Transformer encoder is exploited to fuse these representations by the multi-head attention (MHA)
mechanism to produce a representation vector that contains information of all of the fragment pairs, named fragment
view. The details of fragment-pair view fusion are delayed to the next section. The fragment view and the molecule
view ( i.e., the original molecular graph) are used as two views of a molecule for contrastive learning.
2.4 Model Structure
The structure of the FraSICL model is shown in Fig. 2. Given a molecule mwith molecular graph Gmol=
fV;E;X atom;Xbondg, the model computes the representations of two views via two branches: the left branch is
themolecule view branch for generating molecular view, and the right one is the fragment view branch for generating
the fragment view.
In the molecule view branch, a GNN gmol()is used as an encoder to capture the representation of the molecular graph
hmol=gmol(Gmol). Attentive FP [ 5] is employed as the graph encoder in this work. Then, following the structure of
MolCLR [ 16],hmolis fed to a projection head lmol()and a regularization function norm ()to produce the projection
of the molecule view pmol=norm (lmol(hmol)). The structure of a projection head is shown in Fig. 3. And the
regularization function is norm (v) =v
kvk, which can make the length of the projection vector be 1.
And for the fragment view branch on the right, all of the Nbbreakable acyclic single bonds are enumerated and
broken by the method proposed in Sec. 2.2 to generate Nbfragment-pair views G1
frag=fV;E 1;Xatom;Xbondg,:::,
GNb
frag=fV;ENb;Xatom;Xbondg. Then, a GNN gfrag()is used as an encoder to compute the representation of each
fragment-pair view hi
frag=gfrag(Gi
frag). Attentive FP is also used here. Note that since there are two disconnected
components in each fragment-pair view Gi
frag,gfrag()will read out these two subgraphs separately and produce
two subgraph embeddings. The representation of a fragment-pair view is obtained by element-wisely adding its two
corresponding subgraph embeddings for permutation-invariant property.
Then, as described in Sec. 2.3, a multi-view fusion mechanism is introduced for leveraging all of the information
related to functional groups contained in the Nbfragment-pair views. SpeciÔ¨Åcally, a Transformer encoder T()is
employed, which uses the representations of fragment-pair views hi
frag as input tokens, and computes the interaction
relationships between the fragment-pair views by the multi-head attention (MHA) mechanism. The resulting attention
scores serve as weights to fuse the representations and obtain ^hi
frag. By summing up all of the representations of Nb
fragment-pair views , we can get the representation of fragment view hfv=PNb
i=1^hi
frag. Finally, the representation hfv
of the fragment view goes through a projection head lfv()and a normalization layer to get pfv=norm (lfv(hfv)).
Following the structure of MolCLR, the two projections pmolandpfvare used to calculate contrastive loss. And when
Ô¨Ånetuning on the downstream tasks, the model will output one of the representations hmolorhfvof a molecule to
serve as learned molecular representation. A downstream prediction head f()will use this representation as input, and
predict the molecular property by y=f(hmol)ory=f(hfv).
In addition, representations hi
frag ofNbfragment-pair views of a molecule goes through another projection head and
a normalization layer to produce projection pi
frag =norm (lfrag(hi
frag)). Inner product of these projections are
computed to generate a similarity matrix S=fsijjsij=<pi
frag;pj
frag>g;S2RNbNb, where<;>denotes the
inner product of two vectors. The usage of this similarity matrix Swill be introduced in the next section.
5FraSICL
‚Ä¶‚Ä¶+‚Ä¶‚Ä¶TransformerFragEncoderProjectionHead‚Ä¶‚Ä¶+‚Ä¶‚Ä¶‚Ä¶‚Ä¶+MolEncoder
ProjectionHead‚Ä¶‚Ä¶ProjectionHeadùíâ!"#ùíë!"#ùíâ$%ùíë$%ùíâ$&'()
ùíë$&'()ùíâ#$&'()ùëõùëúùëüùëö(
SimilarityMatrixùëÜDotsimilarity
ùëõùëúùëüùëö(ùëõùëúùëüùëö(
Figure 2: The structure of the FraSICL model.
ProjectionHeadLinearBatchNormReLULinearDropOut
Figure 3: The structure of a projection head. Following the design proposed in BYOL [ 27], a projection head consists
of a stack of linear layer, BN layer, activation layer, linear layer and dropout layer.
2.5 Loss Functions
The training of FraSICL in the pre-training phase is illustrated in Fig. 4. Here, given a batch of Nmolecules, the model
will calculate the projections pmolandpfvof each molecule. Then, contrastive learning is performed between all
samples in a batch. The view pair (i.e. molecule view and fragment view) of each sample is a positive pair, as shown by
the red line, and the view pairs of other samples in the batch are negative pairs, as shown by the blue line in the Ô¨Ågure.
The NT-Xent Loss is used for contrastive learning:
6FraSICL
FraContraùëù!"#$ùëù%&$ùëù!"#'ùëù%&'FraContra‚Ä¶‚Ä¶
‚Ä¶‚Ä¶‚Ä¶‚Ä¶NT-XentLossL2Loss
ùëÜ!L2Loss
ùëÜ"
Figure 4: The illustration of FraSICL training. FraSICL is trained by both NT-Xent contrastive loss and an auxiliary
similarity loss. In the contrastive loss, two projections of a molecule are treated as a positive pair, which is highlighted
by red lines in the Ô¨Ågure. Projections of other molecules in a batch are considered as negative pairs, which is shown by
blue lines. For each molecule, L2 loss is computed between the similarity Sand an all-one matrix as auxiliary similarity
loss.
Li
mol= loge sim (pi
mol;pi
fv)

PN
k=11fk6=ig
e sim (pi
mol;pk
mol)

+ e sim (pi
mol;pk
fv)
, (1)
Li
fv= loge sim (pi
mol;pi
fv)

PN
k=11fk6=ig
e sim (pi
fv;pk
mol)

+ e sim (pi
fv;pk
fv)
, (2)
where inner product similarity is adopted for sim(pi
mol;pi
fv), andis a temperature parameter. The sum of all
contrastive losses of a batch of molecules is denoted as Lclr=PN
i=1Li
mol+PN
i=1Li
fv.
In addition, although the representations of different fragment-pair views have been fused, from the perspective of
contrastive learning, the representations of different fragment-pair views of the same molecule should also be as close
as possible. And as demonstrated in previous study [ 6], representations of some fragment pairs of a molecule are
highly predictive on the downstream tasks, while some others are less effective. So, we hope that the representations of
fragment-pair views can use information from each other to train the GNN encoder to extract better representations.
To this end, an additional auxiliary loss Lsimis introduced to improve the similarity between representations of
fragment-pair views of a molecule, based on the similarity matrix S. Since the inner product of two normalized vectors
is equivalent to cosine similarity, and the maximum value of cosine similarity is 1, assuming a molecule khaveNk
b
7FraSICL
fragment-view pairs, the elements of the similarity matrix are sk
ij=<pk;i
frag;pk;j
frag>, our auxiliary similarity loss of
moleculekis:
Lk
sim=1
(Nk
b)2Nk
bX
i=1Nk
bX
j=1(sk
ij 1)2, (3)
i.e., as shown in Fig. 4, the sum of L2 loss between each element of the similarity matrix Sand that of an all-one matrix.
Denote the similarity loss of a batch of molecules as Lsim=PN
k=1Lk
sim, then the loss for pre-training the FraSICL
model is:
L=Lsim+Lclr, (4)
whereis a hyper-parameter to adjust the inÔ¨Çuence of the auxiliary similarity loss.
3 Experiments
3.1 Baseline experiments
Experimental setting. To construct the pre-training dataset, 200K molecules are randomly sampled from the pre-
training dataset of MolCLR, where 10 million molecules are gathered from the PubChem database [ 10]. The amount of
pre-training data is generally smaller than that of the other baseline models, as shown in Tab. 2. 5% of the pre-training
data are randomly selected as a validation set for model selection. 7 downstream tasks from MoleculeNet [ 31] are used
as downstream target tasks for the baseline experiments. Scaffold splitting is used on each downstream task, with an
8 : 1 : 1 ratio for the training/validation/test sets.
When transferring a pre-trained FraSICL model to the target tasks, different strategies can be applied, including using
which branch of the model for producing molecular representations, and whether to Ô¨Ånetune the pre-trained model
(PTM) on target tasks. In the baseline experiments, we adopt the more complex fragment view branch for molecular
representations, and Ô¨Ånetune the model together with prediction head on the target tasks.
Compared baseline models. Seven state-of-the-art self-supervised pre-training models for molecular representation
learning are used as baseline models for comparison, including MolCLR [ 16], DMP [ 17], MEMO [ 18], GROVER [ 19],
GraphLoG [ 21], PretrainGNNs [ 22] and KPGT [ 23]. The experimental results are shown in Tab. 3, where the data of
baseline models are cited from the original papers of these models. The best score on each dataset is bold, and the
second-best is underlined.
Table 2: The training details of the PTMs
PTM Source of the pre-training dataset Size
MolCLR PubChem 10M
DMP PubChem 10M
MEMO GEOM-Drug 300K
GROVER ZINC15 and ChEMBL 11M
GraphLoG ZINC15 2M
PretrainGNNs ZINC15 (self-supervised) 2M
ChEMBL(supervised) 456K
KPGT ChEMBL 2M
FraSICL PubChem 200K
Results and analysis. As shown in Tab. 3, FraSICL achieves the best predictive performance on 5 of the 7 downstream
MPP tasks, and the second on another one. As the number of pre-training samples used by FraSICL is only 200K,
which is the least among these compared baseline models, the experimental results show that FraSICL can make better
use of the information contained in the graph fragments of molecules to produce molecular representations with better
predictive performance. Compared with the MEMO model that uses the same amount of pre-training data, the predictive
performance of FraSICL on the 7 downstream tasks is signiÔ¨Åcantly improved, even exceeds 20% on the BBBP dataset.
And compared with the models such as GROVER and DMP-TF, FraSICL can achieve comparable or even higher
predictive performance with only about 1=50training samples. These results show the superiority of FraSICL to the
existing models on molecular property prediction tasks.
8FraSICL
Table 3: Results of performance comparison between FraSICL and major existing models on 7 downstream MPP tasks.
Model BACE BBBP ClinTox Tox21 ESOL FreeSolv Lipop
classiÔ¨Åcation classiÔ¨Åcation classiÔ¨Åcation classiÔ¨Åcation regression regression regression
MolCLR 0.890 0.003 0.7360.005 0.9320.017 0.7980.007 1.1100.010 2.2000.200 0.6500.080
DMP-TF 0.893 0.009 0.7810.005 0.9500.005 0.7880.005 0.7000.084 - -
MEMO 0.826 0.003 0.7160.010 0.8160.037 0.7670.004 0.9840.034 - 0.707 0.001
GROVER 0.894 0.028 0.9400.019 0.9440.021 0.8310.025 0.8310.120 1.5440.397 0.5600.035
PretrainGNNs 0.845 0.007 0.6870.013 0.7260.015 0.7810.006 - - -
GraphLoG 0.835 0.012 0.7250.008 0.7670.033 0.7570.005 - - -
KPGT 0.855 0.011 0.9080.010 0.9460.022 0.8480.013 0.8030.008 2.1210.837 0.6000.010
FraSICL 0.8960.010 0.9480.003 0.9570.011 0.8070.006 0.6260.008 1.0940.027 0.5810.013
3.2 Experiments with different transferring settings
Experimental setting. In the baseline experiments, we choose Ô¨Ånetuning the more complex and predictive fragment
view branch as the transferring setting. In this section, other transferring settings are tested, i.e., the combinations of
different branches and different Ô¨Åne-tuning strategies. Experiments are carried out on the BBBP, ClinTox, ESOL and
FreeSolv datasets. Four transferring settings are evaluated, which are denoted as FraSICL-ft-mol, FraSICL-ft-frag,
FraSICL-fr-mol, FraSICL-fr-frag, where ftrepresents Ô¨Ånetuning the PTM, frrepresents freezing the PTM, molindicates
using molecule views and frag indicates using fragment views.
Results and analysis. The experimental results are shown in Tab. 4. Since the two branches of FraSICL are asymmetric,
the structure of the fragment view branch is more complex and has stronger learning capability. Thus, as is revealed
by the experimental results, FraSICL-ft-frag achieves the best performance on 3 of the 4 target tasks. However, a
more complex model structure indicates that it is more likely to suffer overÔ¨Åtting on the downstream tasks. So, when
transferring to the FreeSolv dataset with only 642 samples, the performance of FraSICL-ft-frag is slightly inferior to
that of FraSICL-ft-mol. In addition, compared with freezing the pre-trained model, Ô¨Ånetuning model allows the PTM
to obtain information about speciÔ¨Åc molecular properties from the supervised loss, thereby the generated molecular
representations are more relevant to the target task. Thus a large improvement on the performance is achieved.
Table 4: Predictive performance of 4 FraSICL model variants with different transferring settings on 4 MPP tasks.
Model BBBP ClinTox ESOL FreeSolv
classiÔ¨Åcation classiÔ¨Åcation regression regression
FraSICL-fr-mol 0.852 0.003 0.6910.007 1.3210.002 2.5480.006
FraSICL-fr-frag 0.803 0.010 0.6280.020 1.5940.217 2.2930.009
FraSICL-ft-mol 0.917 0.006 0.9060.023 0.7580.029 1.0850.059
FraSICL-ft-frag 0.9480.003 0.9570.011 0.6260.008 1.0940.027
3.3 InÔ¨Çuence of the auxiliary similarity loss
Motivation. The auxiliary similarity loss, i.e., Equ. (3) introduced in Sec. 2.5, is designed for making the fragment-pair
views to learn from each other to better leverage the information encoded in different fragment-pair views. However, it
is intuitive that when the auxiliary loss takes an excessively dominant role in the total training loss, the model may tend
to generate exactly the same representation vectors for different fragment-pair views to decrease the similarity loss. On
this occasion, the representation vectors of fragment-pair views will not contain any information about the topological
structure, showing a model collapse phenomenon. Thus, the inÔ¨Çuence of the hyperparameter is crucial.
Experimental setting. In this section, experiments are conducted to test the inÔ¨Çuence of the auxiliary similarity loss by
setting different . In these experiments, is set to 0.1, 0.01, 0.005 and 0 respectively, where = 0indicates training
without the similarity loss, which can be regarded as an ablation study. Here, the transferring setting is the same as the
baseline experiments, i.e., Ô¨Ånetuning the fragment view branch.
Results and analysis. Results are shown in Tab. 5. As shown in Tab. 5, the auxiliary similarity loss has an obvious
impact on the predictive performance. When = 0, i.e., training without the similarity loss, the predictive performance
is not superior to that of the other three models trained with the auxiliary similarity loss, which demonstrates that the
auxiliary similarity loss can indeed promote the model to produce more predictive representations. And when = 0:1,
the model achieves even worse results than = 0on 3 of the 4 downstream tasks, which reveals that a large value of 
may make the similarity loss be harmful to the model and lead to performance degradation.
9FraSICL
Table 5: Experimental results on the inÔ¨Çuence of hyperparameter .
 BBBP ClinTox ESOL FreeSolv
classiÔ¨Åcation classiÔ¨Åcation regression regression
0.1 0.8900.023 0.9160.008 0.7340.028 1.1510.098
0.01 0.9480.003 0.9570.011 0.6260.008 1.0940.027
0.005 0.9270.008 0.9120.012 0.6670.024 1.1480.015
0 0.9060.009 0.8800.030 0.6620.051 1.1380.092
4 Conclusion
This paper focuses on the semantic inconsistency problem that may occur when using noise-adding operations to
generate new views for contrastive learning in self-supervised molecular property prediction studies. To solve this
problem, this paper Ô¨Årst deÔ¨Ånes semantic-invariant molecular view by introducing two types of semantic inconsistent
views that may lead to false positive pairs and consequently poor performance. Then, a semantic-invariant view
generation method is proposed. The views generated by this method will not cause semantic inconsistency, which
realizes the decoupling of the input graph topology and the message passing topology of GNNs. Thus, this method is
expected to promote the GNN encoders to extract better molecular representations.
Based on the semantic-invariant views, a Fragment-based Semantic-Invariant Contrastive Learning (FraSICL) molecular
representation model is developed. FraSICL is an asymmetric model with two branches, the molecule view branch and
the fragment view branch. A multi-view fusion mechanism is also introduced to make better use of the information
contained in the views of different fragment pairs. Furthermore, an auxiliary similarity loss is designed to train the
backbone GNN to produce better representations.
Baseline experiments are conducted on 7 target tasks, and experimental results show that FraSICL achieves state-of-
the-art predictive performance with the least number of pre-training data. Further experiments demonstrate that in
our model Ô¨Ånetuning is effective in boosting performance and the auxiliary similarity loss can improve the predictive
accuracy if a proper hyperparameter is selected. These Ô¨Åndings reveal that FraSICL can make better use of the
information of pre-training samples and generate representations with superior predictive performance.
References
[1]Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing
for quantum chemistry. In Proceedings of the 34th International Conference on Machine Learning (ICML) ,
volume 70, pages 1263‚Äì1272. PMLR, 2017.
[2]David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel, Al√°n Aspuru-Guzik,
and Ryan P Adams. Convolutional networks on graphs for learning molecular Ô¨Ångerprints. In Advances in Neural
Information Processing Systems , volume 28, pages 2224‚Äì2232, 2015.
[3]Sabrina Jaeger, Simone Fulle, and Samo Turk. Mol2vec: unsupervised machine learning approach with chemical
intuition. Journal of chemical information and modeling , 58(1):27‚Äì35, 2018.
[4]Ying Song, Shuangjia Zheng, Zhangming Niu, Zhang-Hua Fu, Yutong Lu, and Yuedong Yang. Communicative
representation learning on attributed molecular graphs. In Proceedings of the Twenty-Ninth International Joint
Conference on ArtiÔ¨Åcial Intelligence , pages 2831‚Äì2838, 2020.
[5]Zhaoping Xiong, Dingyan Wang, Xiaohong Liu, Feisheng Zhong, Xiaozhe Wan, Xutong Li, Zhaojun Li, Xiaomin
Luo, Kaixian Chen, Hualiang Jiang, et al. Pushing the boundaries of molecular representation for drug discovery
with the graph attention mechanism. Journal of medicinal chemistry , 63(16):8749‚Äì8760, 2019.
[6]Ziqiao Zhang, Jihong Guan, and Shuigeng Zhou. Fragat: a fragment-oriented multi-scale graph attention model
for molecular property prediction. Bioinformatics , 37(18):2981‚Äì2987, 2021.
[7]Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, and Tie-Yan Liu.
Do transformers really perform badly for graph representation? In Advances in Neural Information Processing
Systems , volume 34, pages 28877‚Äì28888, 2021.
[8]Han Altae-Tran, Bharath Ramsundar, Aneesh S Pappu, and Vijay Pande. Low data drug discovery with one-shot
learning. ACS central science , 3(4):283‚Äì293, 2017.
10FraSICL
[9]Anna Gaulton, Anne Hersey, Micha≈Ç Nowotka, A Patricia Bento, Jon Chambers, David Mendez, Prudence
Mutowo, Francis Atkinson, Louisa J Bellis, Elena Cibri√°n-Uhalte, et al. The chembl database in 2017. Nucleic
acids research , 45(D1):D945‚ÄìD954, 2017.
[10] Sunghwan Kim, Jie Chen, Tiejun Cheng, Asta Gindulyte, Jia He, Siqian He, Qingliang Li, Benjamin A Shoemaker,
Paul A Thiessen, Bo Yu, Leonid Zaslavsky, Jian Zhang, and Evan E Bolton. PubChem in 2021: new data content
and improved web interfaces. Nucleic Acids Research , 49(D1):D1388‚ÄìD1395, 2021.
[11] John J. Irwin, Khanh G. Tang, Jennifer Young, Chinzorig Dandarchuluun, Benjamin R. Wong, Munkhzul
Khurelbaatar, Yurii S. Moroz, John MayÔ¨Åeld, and Roger A. Sayle. Zinc20‚Äîa free ultralarge-scale chemical
database for ligand discovery. Journal of Chemical Information and Modeling , 60(12):6065‚Äì6073, 2020.
[12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional
transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter
of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT) , volume 1, pages
4171‚Äì4186. Association for Computational Linguistics, 2019.
[13] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual
representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR) , pages 9726‚Äì9735. CVF/IEEE, 2020.
[14] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive
learning of visual representations. In Proceedings of the 37th International Conference on Machine Learning
(ICML) , volume 119, pages 1597‚Äì1607. PMLR, 2020.
[15] Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Banerjee, and Fillia Makedon. A
survey on contrastive self-supervised learning. Technologies , 9(1):2, 2020.
[16] Yuyang Wang, Jianren Wang, Zhonglin Cao, and Amir Barati Farimani. Molecular contrastive learning of
representations via graph neural networks. Nature Machine Intelligence , 4(3):279‚Äì287, 2022.
[17] Jinhua Zhu, Yingce Xia, Tao Qin, Wengang Zhou, Houqiang Li, and Tie-Yan Liu. Dual-view molecule pre-training.
CoRR , abs/2106.10234, 2021.
[18] Yanqiao Zhu, Dingshuo Chen, Yuanqi Du, Yingze Wang, Qiang Liu, and Shu Wu. Featurizations matter: A
multiview contrastive learning approach to molecular pretraining. In ICML 2022 2nd AI for Science Workshop ,
2022.
[19] Yu Rong, Yatao Bian, Tingyang Xu, Weiyang Xie, Ying Wei, Wenbing Huang, and Junzhou Huang. Self-
supervised graph transformer on large-scale molecular data. In Advances in Neural Information Processing
Systems , volume 33, pages 12559‚Äì12571, 2020.
[20] Hyunseob Kim, Jeongcheol Lee, Sunil Ahn, and Jongsuk Ruth Lee. A merged molecular representation learning
for molecular properties prediction with a web-based service. ScientiÔ¨Åc Reports , 11(1):1‚Äì9, 2021.
[21] Minghao Xu, Hang Wang, Bingbing Ni, Hongyu Guo, and Jian Tang. Self-supervised graph-level representation
learning with local and global structure. In Proceedings of the 38th International Conference on Machine Learning
(ICML) , volume 139, pages 11548‚Äì11558. PMLR, 2021.
[22] Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande, and Jure Leskovec. Strategies
for pre-training graph neural networks. In 8th International Conference on Learning Representations, ICLR , 2020.
[23] Han Li, Dan Zhao, and Jianyang Zeng. Kpgt: Knowledge-guided pre-training of graph transformer for molecular
property prediction. arXiv preprint arXiv:2206.03364 , 2022.
[24] David Rogers and Mathew Hahn. Extended-connectivity Ô¨Ångerprints. Journal of chemical information and
modeling , 50(5):742‚Äì754, 2010.
[25] David Weininger. Smiles, a chemical language and information system. 1. introduction to methodology and
encoding rules. Journal of Chemical Information and Computer Sciences , 28(1):31‚Äì36, 1988.
[26] Henri A Favre and Warren H Powell. Nomenclature of Organic Chemistry . The Royal Society of Chemistry, 2014.
[27] Jean-Bastien Grill, Florian Strub, Florent Altch√©, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl
Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a
new approach to self-supervised learning. Advances in neural information processing systems , 33:21271‚Äì21284,
2020.
[28] Jake Topping, Francesco Di Giovanni, Benjamin Paul Chamberlain, Xiaowen Dong, and Michael M Bronstein.
Understanding over-squashing and bottlenecks on graphs via curvature. In 10th International Conference on
Learning Representations, ICLR , 2022.
11FraSICL
[29] Uri Alon and Eran Yahav. On the bottleneck of graph neural networks and its practical implications. In 9th
International Conference on Learning Representations, ICLR , 2021.
[30] Rickard Br√ºel-Gabrielsson, Mikhail Yurochkin, and Justin Solomon. Rewiring with positional encodings for
graph neural networks. arXiv preprint arXiv:2201.12674 , 2022.
[31] Zhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S Pappu, Karl
Leswing, and Vijay Pande. Moleculenet: a benchmark for molecular machine learning. Chemical science ,
9(2):513‚Äì530, 2018.
12