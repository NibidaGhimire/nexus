Adaptive Texture Filtering for Single-Domain Generalized Segmentation
Xinhui Li1, Mingjia Li1, Yaxing Wang2, Chuan-Xian Ren3, Xiaojie Guo1*
1Tianjin University, Tianjin, China
2Nankai University, Tianjin, China
3Sun Yat-sen University, Guangdong, China
flixinhui, mingjialig@tju.edu.cn, yaxing@nankai.edu.cn, rchuanx@mail.sysu.edu.cn, xj.max.guo@gmail.com
Abstract
Domain generalization in semantic segmentation aims to
alleviate the performance degradation on unseen domains
through learning domain-invariant features. Existing methods
diversify images in the source domain by adding complex or
even abnormal textures to reduce the sensitivity to domain-
speciﬁc features. However, these approaches depend heavily
on the richness of the texture bank, and training them can
be time-consuming. In contrast to importing textures arbi-
trarily or augmenting styles randomly, we focus on the sin-
gle source domain itself to achieve generalization. In this pa-
per, we present a novel adaptive texture ﬁltering mechanism
to suppress the inﬂuence of texture without using augmen-
tation, thus eliminating the interference of domain-speciﬁc
features. Further, we design a hierarchical guidance general-
ization network equipped with structure-guided enhancement
modules, which purpose is to learn the domain-invariant gen-
eralized knowledge. Extensive experiments together with ab-
lation studies on widely-used datasets are conducted to verify
the effectiveness of the proposed model, and reveal its supe-
riority over other state-of-the-art alternatives.
Introduction
The powerful data representation capability endows deep
neural networks with remarkable achievements in a series
of computer vision tasks, e.g., semantic segmentation, ob-
ject detection, and scene understanding. These successes
are usually based on the assumption that models should
be trained and tested on data having the same underlying
distribution. However, in realistic scenarios, it is impracti-
cal to collect sufﬁcient data that covers the entire space.
Even more, some deployed domains are completely inac-
cessible. For instance, collecting images under all possible
conditions and scenarios are impossible in the self-driving
system. Therefore, as a representative line, domain general-
ization (DG) (Carlucci et al. 2019; Qiao, Zhao, and Peng
2020; Peng et al. 2021) is extensively investigated to ad-
dress a fundamental ﬂaw: the network exhibits obvious per-
formance degradation in the face of out-of-distribution or
unseen-domain data. It is also known as the distribution shift
(Moreno-Torres et al. 2012) issue.
*Corresponding Author
Copyright © 2023, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.
Figure 1: Illustration of existing DG methods (Peng et al.
2021) using texture augmentation, and our proposed ap-
proach using texture suppression.
Since data in the target domain is inaccessible during the
training process , current DG methods advocate alleviating
the distribution shift by reducing the domain gap ( e.g., ap-
pearance, style, or texture). These approaches can be divided
into two main categories, i.e., multi-source DG and single-
source DG. Multi-source DG schemes (Li et al. 2018; Peng
et al. 2019; Matsuura and Harada 2020; Zhu and Li 2021)
attempt to align multiple available source domains through
minimizing the divergence among domains. However, the
alignment is typically based on the accessibility of multi-
ple domain distributions that are not always available. To
mitigate the pressure from data, another branch of DG ap-
proaches (Tobin et al. 2017; V olpi et al. 2018; Qiao, Zhao,
and Peng 2020; Peng et al. 2021; Wang et al. 2021b) using
single-source has been studied to achieve the goal. Some of
recent methods (Ulyanov, Vedaldi, and Lempitsky 2017; Pan
et al. 2018) investigate the normalization to standardize fea-
ture distributions, while the strategies (Prakash et al. 2019;
Kang et al. 2022; Tjio et al. 2022) focus on the style trans-
formation to diversify image styles. The other works (To-
bin et al. 2017; Ulyanov, Vedaldi, and Lempitsky 2017; Yue
et al. 2019; Peng et al. 2021) execute the domain random-
ization for texture synthesis. As shown in Fig. 1(top), somearXiv:2303.02943v1  [cs.CV]  6 Mar 2023works mainly focus on diversifying the appearance ( e.g., tex-
ture) (Peng et al. 2021). However, these data augmentation
methods are limited by the authenticity and randomness of
added textures, and can hardly cover all the appearances that
may arise in the target domain. To handle this problem, we
investigate the necessity of importing additional textures (or
appearances) through the complex transformation. Namely,
can we ﬁnd a straightforward way to learn domain-invariant
features by explicitly ﬁltering textures from images?
In this paper, we concentrate on the task of single-domain
generalization and introduce a method from a novel perspec-
tive. We work on texture suppression , as shown in Fig. 1
(bottom), which is more ﬂexible and simpler than the data
augmentation operation. We propose an adaptive ﬁltering
mechanism (AFM) to screen out the texture component from
images, aiming to explicitly mitigate the effect of texture.
More concretely, our AFM can break through the deﬁciency
of traditional ﬁltering which requires setting the ﬁltering
level manually. The proposed AFM can adaptively gener-
ate texture ﬁltering intensity parameters by predicting the
style of the input image, and then producing a content-
dependent image. Moreover, to further explore the domain-
invariant feature representation in the content-dependent im-
age, we design a hierarchical guidance generalization net-
work (HGGN). HGGN is customized with structure-guided
enhancement modules, which purpose is to extract domain-
invariant features and enhance spatial information learning
ability. The major contributions of this work are summarized
as follows:
• We propose a novel adaptive ﬁltering mechanism to gen-
erate the content-dependent image, aiming to eliminate
the interference of domain-speciﬁc features.
• We design a hierarchical guidance generalization net-
work, which can implicitly guide the network to learn
domain-invariant features.
• Extensive experiments are conducted on various datasets
to validate the effectiveness of our method and show the
superiority of our approach over other state-of-the-art.
Related Work
Unsupervised Domain Adaptation
Manually labeling semantic segmentation data is extremely
time-consuming and laborious. Thus unsupervised domain
adaptation (UDA) (Luo et al. 2022) has drawn growing at-
tention, which can liberate the requirement of annotation.
Basically, the manner of UDA is by narrowing the distri-
bution gap between the automatically labeled synthetic data
and unlabeled real data to achieve reasonable performance in
the real domain. For instance, several works utilize the max-
imum mean discrepancy (MMD) (Geng, Tao, and Xu 2011;
Long et al. 2015) to minimize a certain distance measure
between domains and then align the distributions of differ-
ent datasets. However, because of the insufﬁcient capacity of
MMD minimization, domain alignment is not always guar-
anteed. Other popular approaches leveraging the idea of ad-
versarial learning (Luo et al. 2019; Tsai et al. 2019; Zhang
and Wang 2020; Shermin et al. 2021) are applied by adopt-
ing domain discriminators. Meanwhile, style transfer andimage-to-image translation (Zhu et al. 2017; Huang and Be-
longie 2017; Lee et al. 2021) methods are also studied to the
UDA with the goal of converting the style of the source data
and possibly getting closer to the target data. Unfortunately,
the distribution discrepancy cannot be estimated from the
source domain alone when real data is not accessible during
training. In other words, hardly these techniques can cope
with domain generalization.
Domain Generalization
Unlike UDA, domain generalization (DG) methods cannot
access the target domain. The goal of DG is to train only
on the source domain and generalize on target domains.
The paradigm of DG methods is basically to learn gener-
alized feature representations. Existing DG methods can be
roughly divided into two categories: 1) Multiple-source DG
(Matsuura and Harada 2020; Wang et al. 2021a; Liu et al.
2021; Li et al. 2021), and 2) Single-source DG (V olpi et al.
2018; Qiao, Zhao, and Peng 2020; Peng et al. 2021; Wang
et al. 2021b; Xu et al. 2022).
Multiple-source DG methods attempt to explore com-
mon latent representations among the available source do-
mains to improve the generalization capability. Inspired by
the similar idea of feature alignment on UDA, many ap-
proaches mainly utilize explicit feature distribution con-
straints (Peng et al. 2019; Zhu and Li 2021) or feature nor-
malization (Nam and Kim 2018; Jia, Ruan, and Hospedales
2019) to learn domain-invariant feature representations on
multiple source domains. However, due to the uncertainty of
target domains, the feature alignment method has the risk of
over-ﬁtting source domains and leads to poor generalization
ability of the unseen domain. Moreover, various methods
such as adversarial feature learning (Li et al. 2018), meta-
learning (Li et al. 2019; Shu et al. 2021; Kim et al. 2022),
and metric learning (Motiian et al. 2017; Dou et al. 2019)
are also proposed to boost the generalization performance.
In this paper, we focus on single-source DG.
Single-source DG methods strive to solve the generaliza-
tion issue from the perspective of augmenting source data,
such as domain randomization (Yue et al. 2019) or style
transformation (Prakash et al. 2019; Peng et al. 2021) to di-
versify the distribution of single source domain. However,
it is inﬂexible to perform complex adversarial generation or
unstable stylized transformation on the source image. An-
other studies focus on exploiting latent feature representa-
tion through feature normalization, such as instance normal-
ization, batch normalization, and many other extended nor-
malization methods (Pan et al. 2018; Choi et al. 2021; Peng
et al. 2022). These works show that normalization is effec-
tive to solve the DG problem in preventing the over-ﬁtting
of training data. Although numerous studies have been pro-
posed to solve the generalization problem, most of the DG
methods mentioned above mainly focus on image classiﬁ-
cation (Kang et al. 2022), and a few researches (Yue et al.
2019; Choi et al. 2021) on semantic segmentation. However,
for the ﬁeld of autonomous driving, semantic segmentation
in DG is valuable and practical in the real world. Our work
belongs to the single-source DG and focuses on addressing
the practical application of DG in semantic segmentation.Figure 2: (a) Network architecture of the proposed method, including the adaptive ﬁltering mechanism (AFM) and hierarchical
guidance generalization network (HGGN). The AFM consists of the adaptive strength predictor (ASP) module and the texture
ﬁltering generator (TFG) module. (b) ASP predicts the ﬁltering intensity parameter aby calculating the mean and variance
2of the extracted features. TFG is responsible for generating the content-dependent image Icand texture-dependent image It.
(c) HGGN composed of structure-guided enhancement modules (SGE) guides the network to learn generalized representations.
Methodology
This work focuses on how to solve the single-domain gen-
eralization problem: a model is trained on the source do-
main, expecting to generalize well on many unseen real-
world domains. Due to the effect of domain-speciﬁc tex-
tures in generalization, as previously analyzed, our method
aims to alleviate the inﬂuence of domain-speciﬁc compo-
nent through texture ﬁltering. For this purpose, we propose
a novel adaptive ﬁltering mechanism to obtain the content-
dependent component. This is a straightforward and efﬁcient
way for the network to learn feature representation without
the interference of texture. In addition, we design a hierar-
chical guidance generalization network to implicitly guide
the network to learn domain-invariant features.
Overall Architecture
The overall architecture of our method is depicted in Fig. 2.
Given an input image I, we ﬁrst ﬁlter its texture by the
adaptive ﬁltering mechanism (AFM). Within it, the adaptive
strength predictor (ASP) estimates a ﬁltering intensity pa-
rameteradetermining the strength of the texture ﬁltering
generator (TFG). The TFG produces the texture-dependent
imageItand content-dependent image Ic. The hierarchi-
cal guidance generalization network (HGGN) stacked by
structure-guided enhancement (SGE) modules is linked after
the AFM to learn domain-invariant feature representations
under contour supervision.Adaptive Filtering Mechanism
The key to ensuring the generalization performance is the
ability to generate the content-dependent component that
allows the network to learn domain-invariant features di-
rectly. To this end, the adaptive ﬁltering mechanism is pro-
posed to separate the image into content-dependent compo-
nentIcand texture-dependent component It, which contains
the adaptive strength predictor (ASP) and texture ﬁltering
generator (TFG).
Adaptive Strength Predictor (ASP) ASP aims to generate
the ﬁlter strength value based on the style of the input im-
age. Speciﬁcally, we implement a feature extraction mod-
ule containing three convolution layers and then calculate
the meanand variance 2of the extracted features across
the spatial dimension of each channel, respectively. Due to
the effectiveness of the statistics in representing image in-
formation (Huang and Belongie 2017; Zhou et al. 2021), the
value of mean and variance are concatenated to represent the
style of the input image. Then, the value of ﬁlter strength a
can be obtained through a fully connected layer, as shown in
Fig. 2(b). For more detail, in the training period, we add the
awith an error term , whose value is obtained through the
sampling of the normal distribution with a mean of 0 and a
variance of 1, and truncated at 1.5. This operation can effec-
tively improve the robustness of the network during training
and is removed in the testing. Finally, after the processing of
ASP, the ﬁlter strength value arequired by TFG can be au-
tomatically generated based on the style of the input image.Figure 3: Network architecture of Structure-guided Enhancement Modules (SGE), which constitute the HGGN.
Texture Filtering Generator (TFG) TFG is designed to
ﬁlter the texture component from the image while keeping
the structure. Motivated by the effectiveness of image ﬁlter-
ing or smoothing (Guo, Li, and Ma 2020; Li et al. 2022) on
texture removal, we adopt smoothing operation as texture
ﬁlter. As shown in Fig. 2(b), we utilize a U-shape network
as the backbone of our smoothing generator. As noticed by
previous art (Chen et al. 2020), the smoothing strength can
be manipulated by inserting different convolution modules
in the skip-connection. According to the smoothing strength
mgiven we ﬁrst generate convolution kernels, and then
use the generated convolution kernel to perform structure-
preserving smoothing. Note that our TFG is pre-trained with
labels from the training split of the GTA5 dataset, and then
the parameters of TFG is ﬁxed when training the segmenta-
tion network.
We extract semantic boundaries from segmentation anno-
tations as training guidance to protect semantically mean-
ingful edges, which is signiﬁcant for segmentation. Given
input image Iand semantic boundary G, the loss function
of TFG can be expressed as follows:
Lsmooth =kI Sk2
2+mrS
rI+G+2
2;(1)
whereSis the smoothed output, kk2represents the `2norm,
rrepresents the gradient of an image, is a small constant
to avoid zeros in the denominator (set to 0.005 empirically)
andmis the smoothing strength. When training the TFG,
we randomly sample m2[0;4]from a uniform distribution
as the smoothing strength. The ﬁrst term in Lsmooth regular-
izes the outputs to be consistent with the input image, while
the second term tends to penalize the texture. That is to say,
the highermis, the more smooth the output will be. Finally,
through the cooperation of ASP and TFG, we regard the the
smoothed image Swithout texture as the content-dependent
imageIc. Meanwhile, the texture-dependent image Itis ex-
pressed asIt=I Ic. In Fig. 4, we show some samples of
content-dependent images and texture-dependent images.
Hierarchical Guidance Generalization Network
To learn the generalized representation of the content-
dependent image Iceffectively, we propose a hierarchi-
cal guidance generalization network (HGGN) including
structure-guided enhancement modules (SGE), as shown in
Fig. 3. Firstly, we utilize the Icgenerated from the AFM
as the input of the HGGN. Although the ﬁltering operation
Figure 4: Samples of content-dependent images Icand
texture-dependent images Itgenerated from the AFM in the
training period. Original images are from the GTA5 dataset.
can preserve the contour of objects, some internal structure
information of the object would be removed during this pro-
cessing. However, the internal structure information is also
helpful for the segmentation task (Ronneberger, Fischer, and
Brox 2015). To this end, we also extract the internal struc-
ture from the texture-dependent image It. In Fig. 4, it can be
clearly observed that Itcontains some internal structure in-
formation of the object. Therefore, we design SGE modules
to fuse the extracted feature from IcandIt, which can better
guide the network to learn the spatial feature representation.
More speciﬁcally, there are four SGE modules in the
HGGN, as shown in Fig. 3. To ensure that internal struc-
ture information in Itis better preserved, we adopt concate-
nate to fuse the features. Subsequently, we normalize the
fused features through instance normalization (IN) to extract
the style-normalized feature representations. Many previous
works like (Ulyanov, Vedaldi, and Lempitsky 2017; Jin et al.
2020) have proved the effectiveness of IN on style normal-
ization, but the spatial feature representation might be weak-
ened during IN. However, in the DG semantic segmenta-
tion task, the spatial features after IN need to be highlighted
due to the similar structural information among different do-
mains. Speciﬁcally, for this purpose, we design a contour
detector (CD). CD generates the predicted contour map y
through three convolution layers and then calculates the con-
tour lossLcontour with the contour ground truth, which is
obtained from the semantic label. Meanwhile, the contourmapyis also utilized to enhance the spatial feature repre-
sentation of the extracted feature from Itby element-wise
multiplication. A convolution layer is used to change the
channel dimension, namely the “Conv” in Fig. 3. The su-
pervised training of contour detection can explicitly assist
the model in learning the domain-invariant (shape and spa-
tial) information. We use the class-balanced cross-entropy
loss as the contour loss to supervise the predicted contour
map. For a predicted contour map y, the contour loss `contour
can be written as:
`contour = X
j2Y+log(yj= 1)
 (1 )X
j2Y log(yj= 0);(2)
where()is the sigmoid function.  =
jY j=(jY j+jY+j), wherejY jandjY+jdenote the
contour and non-contour in the ground truth Y. The total
contour lossLcontour is obtained by the sum of `contour from
four SGE modules.
In addition to the SGE, the classiﬁer is responsible for
generating semantic segmentation results. The segmentation
lossLseggiven by standard cross-entropy loss is deﬁned as:
Lseg= X
h;wCX
c=1y(h;w;c )
s logp(h;w;c )
s; (3)
wherep(h;w;c )
s denotes the predicted semantic segmentation
result.y(h;w;c )
s is the semantic ground truth label and Cis the
number of classes. Combining the above contour loss term
Lcontour yields our ﬁnal objective function:
Ltotal=Lseg+Lcontour; (4)
where we use hyper-parameter to balance the importance
of semantic segmentation loss and contour loss. Here, is
empirically set to 2.5.
Experiments
In this section, we ﬁrst describe the experimental setup in de-
tail and then reveal the advance of our design in comparison
with other state-of-the-art on different real-world datasets.
Finally, the effectiveness of our designed modules is ana-
lyzed in ablation studies.
Dataset Description
To verify the generalization ability of our method, we con-
duct experiments on extensive datasets following the com-
mon protocol adopted by prior works. There are two syn-
thetic datasets ( e.g., GTA5 and SYNTHIA) and three real-
world datasets ( e.g., Cityscapes, BDD-100K, and Mapillary)
in the experiments.
Source Domain Datasets For source domain datasets,
GTA5 (Richter et al. 2016) and SYNTHIA (Ros et al. 2016)
are used with automatically generated annotations during
training, respectively. GTA5 is a large synthetic dataset con-
taining 24966 urban scene images of size 1914 1052. It isrendered by Grand Theft Auto V game engine and automat-
ically annotated into different semantic categories by pixel.
SYNTHIA is a synthetic dataset with pixel-level semantic
annotation. We use the SYNTHIA-RAND-CITYSCAPES
subset in our experiments, which contains 9,400 synthetic
images with a high resolution of 1280 760.
Target Domain Datasets To evaluate the generalization
capability, three real-world datasets: Cityscapes (Cordts
et al. 2016), BDD-100k (Yu et al. 2020), and Mapillary
(Neuhold et al. 2017) are adopted during testing. We only
utilize the validation set of these datasets to test the perfor-
mance of our model for comparison with other approaches.
Cityscapes is a large-scale semantic segmentation dataset
collected from 50 different cities in street scenarios. It con-
tains a training set with 2,975 images and a validation
set with 500 images. BDD-100k is an urban driving scene
dataset collected from various locations in the US, which
has 7000 training images and 1000 validation images. Map-
illary is a diverse street view dataset with annotations of 66
classes, and we only use the overlapped classes with the
synthetic datasets. The training and validation sets contain
18,000 and 2,000 images, respectively.
Implementation Details
Following the current state-of-the-art, we train our model
on the synthetic datasets and then test on the unseen real-
world datasets. ResNet-50 and ResNet-101 (He et al. 2016)
are used as the backbone, respectively. The baseline model
is a segmentation network (Chen et al. 2018) which fol-
lows the same architecture as prior works (Choi et al. 2021).
We use pre-trained parameters on ImageNet to initialize the
model except for the classiﬁer. In the training phase, we use
Stochastic Gradient Descent (SGD) optimizer with a batch
size of 2, a momentum of 0.9, and a weight decay of 0.0005.
The initial learning rate is set to 2.5e-4 and follows the poly
learning rate policy with the power of 0.9. The training it-
eration is set to 200,000 for all involved models. We imple-
ment our method with PyTorch and use a single NVIDIA
RTX3090 with 24 GB memory. The PASCAL VOC Inter-
section over Union (IoU) (Everingham et al. 2015) is used
as the evaluation metric to measure the segmentation perfor-
mance, and mIoU is the mean value of IoUs across all cat-
egories. During the training, we use random cropping and
ﬂipping. Meanwhile, we also adopt augmentations by ran-
domly changing color, brightness, sharpness, and contrast.
Comparison with State-of-the-Art
We compare our method with state-of-the-art in domain gen-
eralization for semantic segmentation. To evaluate the ef-
fectiveness of our method, extensive experiments are con-
ducted to show the generalization capability in the unseen
domains. Table 1 summarizes the comparison results with
recent state-of-the-art on the semantic segmentation task
including IBN-Net (Pan et al. 2018), DRPC (Yue et al.
2019), ISW (Choi et al. 2021), GTR (Peng et al. 2021),
and SAN-SAW (Peng et al. 2022). Beneﬁting from the tex-
ture suppression in AFM and domain-invariant representa-Figure 5: Qualitative domain generalization results on semantic segmentation. The model is trained on GTA5 (Richter et al.
2016) and then generalized to Cityscapes (Cordts et al. 2016) and Mapillary (Neuhold et al. 2017) with ResNet-101.
Methods (GTA5) Mapillary BDD-100k Cityscapes Avg.
ResNet-101
Baseline 31.50 27.85 30.06 29.80
IBN-Net (ECCV’18) 37.94 38.10 37.23 37.76
DRPC (ICCV’19) 38.05 38.72 42.53 39.77
ISW (CVPR’21) 39.05 38.53 42.87 40.15
GTR (TIP’21) 39.10 39.60 43.70 40.80
SAN-SAW (CVPR’22) 40.77 41.18 45.33 42.43
Ours 45.59 42.31 44.83 44.24
ResNet-50
Baseline 29.28 25.31 28.97 27.85
IBN-Net (ECCV’18) 37.75 32.30 33.85 34.63
DRPC (ICCV’19) 34.12 32.14 37.42 34.56
ISW (CVPR’21) 40.33 35.20 36.58 37.37
GTR (TIP’21) 34.52 33.75 37.53 35.27
SAN-SAW (CVPR’22) 41.86 37.34 39.75 39.65
Ours 42.82 38.79 39.91 40.51
Table 1: The comparison in mIoU ( %) with other DG meth-
ods of the ResNet-101 and ResNet-50. The model is trained
on the GTA5 dataset and generalized to real-world datasets.
tion learning in HGGN, our method outperforms most meth-
ods on real-world datasets. Furthermore, we also adopt a
more lightweight backbone network ResNet-50 to verify the
generalization performance. It should be highlighted some
other methods encounter lower improvement when switch-
ing the backbone network from ResNet-50 to ResNet-101,
and even obtain degraded performance on the Mapillary
dataset. This indicates that the alternatives cannot overcome
the generalization problem on the heavier model, while ours
still maintains signiﬁcant improvements. In addition, Ta-Methods (SYN) Mapillary BDD-100k Cityscapes Avg.
ResNet-101
Baseline 21.84 25.01 23.85 23.57
IBN-Net (ECCV’18) 36.19 36.63 34.18 35.67
DRPC (ICCV’19) 34.12 34.34 37.58 35.35
ISW (CVPR’21) 35.86 33.98 37.21 35.68
GTR (TIP’21) 36.40 35.30 39.70 37.13
SAN-SAW (CVPR’22) 37.26 35.98 40.87 38.04
Ours 39.10 36.87 41.32 39.10
ResNet-50
Baseline 21.79 24.50 23.18 23.16
IBN-Net (ECCV’18) 32.16 30.57 32.04 31.60
DRPC (ICCV’19) 32.74 31.53 35.65 33.31
ISW (CVPR’21) 30.84 31.62 35.83 32.76
GTR (TIP’21) 32.89 32.02 36.84 33.92
SAN-SAW (CVPR’22) 34.52 35.24 38.92 36.23
Ours 37.14 36.07 39.48 37.56
Table 2: The comparison in mIoU ( %) with other DG meth-
ods of the ResNet-101 and ResNet-50 trained on the SYN-
THIA (SYN) and generalized to real-world datasets.
ble 2 shows the superior results trained on another syn-
thetic dataset (SYNTHIA) and then tested on the same real-
world datasets. Extensive experiments demonstrate that our
method can provide robust representations by using our pro-
posed adaptive ﬁltering mechanism and hierarchical guid-
ance generalization network.
Ablation Study
We investigate the proposed modules including AFM and
HGGN to ﬁnd out how they contribute to the generalizationSettings Mapillary BDD-100k Cityscapes Avg.
w/oAFM 39.72 35.49 38.24 37.82
Fixed level ( a=1) 42.34 41.02 42.96 42.11
Fixed level( a=2) 41.13 39.28 40.27 40.23
Table 3: Ablation study of adaptive ﬁltering mechanism
(AFM). The model is trained on the GTA5 with ResNet-101
and validated on three real-world datasets.
Settings Mapillary BDD-100k Cityscapes Avg.
w/oHGGN 39.13 37.94 38.91 38.66
w/1-level SGE 41.04 38.61 40.73 40.13
w/2-level SGE 42.72 40.10 42.33 41.72
w/3-level SGE 43.96 41.03 43.25 42.75
w/4-level SGE 45.59 42.31 44.83 44.24
Table 4: Ablation study of HGGN with SGE modules. The
model is trained on the GTA5 with ResNet-101 and vali-
dated on three real-world datasets.
ability of segmentation. Moreover, we study the sensitivity
of the hyper-parameter in the objective function.
Effect of Adaptive Filtering Mechanism To verify the
effectiveness of the proposed AFM, we conduct the ablation
experiment by removing AFM (“w/o AFM”), as shown in
the Table 3. In addition, we evaluate the importance of adap-
tive generation of ﬁlter strength in AFM by ﬁxing the ﬁlter
strengthaas 1 and 2, respectively. We can observe that
model with ﬁxed aperforms worse than the model with an
adaptive generation mechanism. This demonstrates that the
network can beneﬁt from the adaptive generation of texture
ﬁltering strength according to the style of the input image.
Effect of Hierarchical Guidance Generalization Network
HGGN is designed to extract the domain-invariant feature
representations. To better understand the extracted domain-
invariant feature, we visualize the features under the “w/
HGGN” and “w/o HGGN”. As shown in Fig. 6, it is ob-
vious that the features of “w/ HGGN” focus more on the
structure features. In Table 4, we verify the effectiveness of
the HGGN by conducting the setting of “w/o HGGN”. Be-
sides, we deploy different numbers of SGE in the network.
This means that in the absence of SGE, we adopt the corre-
sponding feature layer from the backbone network. Com-
pared with 4-level SGE, the performance of 1-level SGE
(without SGE-2/3/4), 2-level SGE (without SGE-3/4), and
3-level SGE (without SGE-4) on segmentation results are
decreased, which indicates that the spatial guidance design
after each feature layer can greatly promote generalization.
According to the quantitative evaluation in Table 4 and qual-
itative results in Fig. 5, the HGGN with SGE modules can
obtain better generalization performance.
Sensitivity to Hyper-parameter As described above,
there is a hyper-parameter to trade off the importance of
the segmentation loss and contour loss in the objective func-
tion. To evaluate the impact of , we set different values to
Figure 6: Visual comparison of “w/o HGGN” and “w/
HGGN”. The right column shows visualized features cor-
responding to the ﬁrst and second output features of SGE,
while the left column is without SGE.
Model (GTA5) Mapillary BDD-100k Cityscapes Avg.
=1.5 41.89 40.72 42.38 41.66
=2.0 43.77 41.66 43.61 43.01
=2.5 45.59 42.31 44.83 44.24
=3.0 43.94 41.75 43.70 43.13
Table 5: Comparison of mIoU ( %) with different values.
The model is trained on the GTA5 with ResNet-101 and val-
idated on three unseen datasets.
indicate the inﬂuence of hyper-parameter on segmentation
performance, as shown in Table 5. The experiments show
that the network can obtain better segmentation results when
is set to 2.5.
Conclusion
This paper studied how to eliminate domain-speciﬁc feature
inﬂuence and seek domain-invariant feature representation
in single-domain generalization. To alleviate the reliance on
domain-speciﬁc texture features, we proposed an adaptive
ﬁltering mechanism to achieve texture suppression and boost
the generalization ability. Moreover, a hierarchical guidance
generalization network has been designed to guide domain-
invariant feature learning, such as spatial and shape infor-
mation. Extensive experiments have been presented to re-
veal the effectiveness of our method. Our method shows its
superiority over other state-of-the-art on the semantic seg-
mentation task.Acknowledgments
This work was supported by the National Natural Science
Foundation of China under Grant no. 62072327.
References
Carlucci, F. M.; D’Innocente, A.; Bucci, S.; Caputo, B.; and
Tommasi, T. 2019. Domain Generalization by Solving Jig-
saw Puzzles. In CVPR , 2224–2233.
Chen, D.; Fan, Q.; Liao, J.; Avil ´es-Rivero, A. I.; Yuan, L.;
Yu, N.; and Hua, G. 2020. Controllable Image Processing
via Adaptive FilterBank Pyramid. IEEE TIP , 29: 8043–
8054.
Chen, L.-C.; Zhu, Y .; Papandreou, G.; Schroff, F.; and
Adam, H. 2018. Encoder-decoder with atrous separable con-
volution for semantic image segmentation. In ECCV , 801–
818.
Choi, S.; Jung, S.; Yun, H.; Kim, J. T.; Kim, S.; and Choo,
J. 2021. Robustnet: Improving domain generalization in
urban-scene segmentation via instance selective whitening.
InCVPR , 11580–11590.
Cordts, M.; Omran, M.; Ramos, S.; Rehfeld, T.; Enzweiler,
M.; Benenson, R.; Franke, U.; Roth, S.; and Schiele, B.
2016. The Cityscapes Dataset for Semantic Urban Scene
Understanding. In CVPR , 3213–3223.
Dou, Q.; Coelho de Castro, D.; Kamnitsas, K.; and Glocker,
B. 2019. Domain generalization via model-agnostic learning
of semantic features. In NeurIPS , 6447–6458.
Everingham, M.; Eslami, S.; Van Gool, L.; Williams, C. K.;
Winn, J.; and Zisserman, A. 2015. The pascal visual object
classes challenge: A retrospective. IJCV , 111(1): 98–136.
Geng, B.; Tao, D.; and Xu, C. 2011. DAML: Domain Adap-
tation Metric Learning. IEEE TIP , 20: 2980–2989.
Guo, X.; Li, Y .; and Ma, J. 2020. Mutually Guided Image
Filtering. IEEE TPAMI , 42: 694–707.
He, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep residual
learning for image recognition. In CVPR , 770–778.
Huang, X.; and Belongie, S. 2017. Arbitrary style transfer
in real-time with adaptive instance normalization. In ICCV ,
1501–1510.
Jia, J.; Ruan, Q.; and Hospedales, T. M. 2019. Frustratingly
Easy Person Re-Identiﬁcation: Generalizing Person Re-ID
in Practice. In BMVC , 117.
Jin, X.; Lan, C.; Zeng, W.; Chen, Z.; and Zhang, L. 2020.
Style normalization and restitution for generalizable person
re-identiﬁcation. In CVPR , 3143–3152.
Kang, J.; Lee, S.; Kim, N.; and Kwak, S. 2022. Style
Neophile: Constantly Seeking Novel Styles for Domain
Generalization. In CVPR , 7130–7140.
Kim, J. Y .; Lee, J.; Park, J.; Min, D.; and Sohn, K. 2022.
Pin the Memory: Learning to Generalize Semantic Segmen-
tation. In CVPR , 4350–4360.
Lee, S.; Hyun, J.; Seong, H.; and Kim, E. 2021. Unsu-
pervised Domain Adaptation for Semantic Segmentation by
Content Transfer. In AAAI , 8306–8315.Li, D.; Yang, J.; Kreis, K.; Torralba, A.; and Fidler, S.
2021. Semantic Segmentation with Generative Models:
Semi-Supervised Learning and Strong Out-of-Domain Gen-
eralization. In CVPR , 8296–8307.
Li, D.; Zhang, J.; Yang, Y .; Liu, C.; Song, Y .-Z.; and
Hospedales, T. M. 2019. Episodic training for domain gen-
eralization. In ICCV , 1446–1455.
Li, H.; Pan, S. J.; Wang, S.; and Kot, A. C. 2018. Domain
generalization with adversarial feature learning. In CVPR ,
5400–5409.
Li, M.; Fu, Y .; Li, X.; and Guo, X. 2022. Deep Flexible
Structure Preserving Image Smoothing. In ACM MM .
Liu, Q.; Chen, C.; Qin, J.; Dou, Q.; and Heng, P.-A. 2021.
FedDG: Federated Domain Generalization on Medical Im-
age Segmentation via Episodic Learning in Continuous Fre-
quency Space. In CVPR , 1013–1023.
Long, M.; Cao, Y .; Wang, J.; and Jordan, M. I. 2015. Learn-
ing Transferable Features with Deep Adaptation Networks.
InICML , 97–105.
Luo, X.; Zhang, J.; Yang, K.; Roitberg, A.; Peng, K.; and
Stiefelhagen, R. 2022. Towards Robust Semantic Segmen-
tation of Accident Scenes via Multi-Source Mixed Sampling
and Meta-Learning. In CVPR , 4429–4439.
Luo, Y .; Zheng, L.; Guan, T.; Yu, J.; and Yang, Y . 2019.
Taking a closer look at domain shift: Category-level adver-
saries for semantics consistent domain adaptation. In CVPR ,
2507–2516.
Matsuura, T.; and Harada, T. 2020. Domain Generaliza-
tion Using a Mixture of Multiple Latent Domains. In AAAI ,
11749–11756.
Moreno-Torres, J. G.; Raeder, T.; Ala ´ız-Rodr ´ıguez, R.;
Chawla, N.; and Herrera, F. 2012. A unifying view on
dataset shift in classiﬁcation. Pattern Recognit , 45: 521–
530.
Motiian, S.; Piccirilli, M.; Adjeroh, D. A.; and Doretto, G.
2017. Uniﬁed deep supervised domain adaptation and gen-
eralization. In ICCV , 5715–5725.
Nam, H.; and Kim, H.-E. 2018. Batch-Instance Normal-
ization for Adaptively Style-Invariant Neural Networks. In
NeurIPS , 2563–2572.
Neuhold, G.; Ollmann, T.; Bul `o, S. R.; and Kontschieder,
P. 2017. The Mapillary Vistas Dataset for Semantic Under-
standing of Street Scenes. In ICCV , 5000–5009.
Pan, X.; Luo, P.; Shi, J.; and Tang, X. 2018. Two at Once:
Enhancing Learning and Generalization Capacities via IBN-
Net. In ECCV , 484–500.
Peng, D.; Lei, Y .; Hayat, M.; Guo, Y .; and Li, W. 2022.
Semantic-aware domain generalized segmentation. In
CVPR , 2594–2605.
Peng, D.; Lei, Y .; Liu, L.; Zhang, P.; and Liu, J. 2021. Global
and Local Texture Randomization for Synthetic-to-Real Se-
mantic Segmentation. IEEE TIP , 30: 6594–6608.
Peng, X.; Bai, Q.; Xia, X.; Huang, Z.; Saenko, K.; and
Wang, B. 2019. Moment Matching for Multi-Source Do-
main Adaptation. In ICCV , 1406–1415.Prakash, A.; Boochoon, S.; Brophy, M.; Acuna, D.; Cam-
eracci, E.; State, G.; Shapira, O.; and Birchﬁeld, S. 2019.
Structured domain randomization: Bridging the reality gap
by context-aware synthetic data. In ICRA , 7249–7255.
Qiao, F.; Zhao, L.; and Peng, X. 2020. Learning to Learn
Single Domain Generalization. In CVPR , 12553–12562.
Richter, S. R.; Vineet, V .; Roth, S.; and Koltun, V . 2016.
Playing for Data: Ground Truth from Computer Games. In
ECCV , 102–118.
Ronneberger, O.; Fischer, P.; and Brox, T. 2015. U-net: Con-
volutional networks for biomedical image segmentation. In
MICCAI , 234–241.
Ros, G.; Sellart, L.; Materzynska, J.; V ´azquez, D.; and
L´opez, A. M. 2016. The SYNTHIA Dataset: A Large Col-
lection of Synthetic Images for Semantic Segmentation of
Urban Scenes. In CVPR , 3234–3243.
Shermin, T.; Lu, G.; Teng, S. W.; Murshed, M. M.; and So-
hel, F. 2021. Adversarial Network With Multiple Classiﬁers
for Open Set Domain Adaptation. IEEE TMM , 23: 2732–
2744.
Shu, Y .; Cao, Z.; Wang, C.; Wang, J.; and Long, M. 2021.
Open Domain Generalization with Domain-Augmented
Meta-Learning. In CVPR , 9619–9628.
Tjio, G.; Liu, P.; Zhou, J. T.; and Goh, R. S. M. 2022. Adver-
sarial Semantic Hallucination for Domain Generalized Se-
mantic Segmentation. In WACV , 3849–3858.
Tobin, J.; Fong, R.; Ray, A.; Schneider, J.; Zaremba, W.;
and Abbeel, P. 2017. Domain randomization for transfer-
ring deep neural networks from simulation to the real world.
InIROS , 23–30.
Tsai, Y .-H.; Sohn, K.; Schulter, S.; and Chandraker, M.
2019. Domain adaptation for structured output via discrim-
inative patch representations. In ICCV , 1456–1465.
Ulyanov, D.; Vedaldi, A.; and Lempitsky, V . S. 2017. Im-
proved Texture Networks: Maximizing Quality and Diver-
sity in Feed-Forward Stylization and Texture Synthesis. In
CVPR , 4105–4113.
V olpi, R.; Namkoong, H.; Sener, O.; Duchi, J. C.; Murino,
V .; and Savarese, S. 2018. Generalizing to unseen domains
via adversarial data augmentation. In NeurIPS , 5339–5349.
Wang, J.; Lan, C.; Liu, C.; Ouyang, Y .; and Qin, T. 2021a.
Generalizing to Unseen Domains: A Survey on Domain
Generalization. In IJCAI , 4627–4635.
Wang, Z.; Luo, Y .; Qiu, R.; Huang, Z.; and Baktashmotlagh,
M. 2021b. Learning to diversify for single domain general-
ization. In ICCV , 834–843.
Xu, Q.; Yao, L.; Jiang, Z.; Jiang, G.; Chu, W.; Han, W.;
Zhang, W.; Wang, C.; and Tai, Y . 2022. DIRL: Domain-
Invariant Representation Learning for Generalizable Seman-
tic Segmentation. In AAAI , 2884–2892.
Yu, F.; Chen, H.; Wang, X.; Xian, W.; Chen, Y .; Liu, F.;
Madhavan, V .; and Darrell, T. 2020. BDD100K: A Diverse
Driving Dataset for Heterogeneous Multitask Learning. In
CVPR , 2633–2642.Yue, X.; Zhang, Y .; Zhao, S.; Sangiovanni-Vincentelli, A.;
Keutzer, K.; and Gong, B. 2019. Domain randomization
and pyramid consistency: Simulation-to-real generalization
without accessing target domain data. In ICCV , 2100–2110.
Zhang, Y .; and Wang, Z. 2020. Joint adversarial learning
for domain adaptation in semantic segmentation. In AAAI ,
6877–6884.
Zhou, K.; Yang, Y .; Qiao, Y .; and Xiang, T. 2021. Domain
generalization with mixstyle. In ICLR .
Zhu, J.; Park, T.; Isola, P.; and Efros, A. A. 2017. Unpaired
Image-to-Image Translation Using Cycle-Consistent Adver-
sarial Networks. In ICCV , 2242–2251.
Zhu, R.; and Li, S. 2021. Self-supervised Universal Domain
Adaptation with Adaptive Memory Separation. In ICDM ,
1547–1552.