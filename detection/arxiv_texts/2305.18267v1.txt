Analysis of the (1+1) EA on LeadingOnes with Constraints
Tobias Friedrich
friedrich@hpi.de
Hasso Plattner Institute
University of Potsdam
Potsdam, GermanyTimo KÃ¶tzing
timo.koetzing@hpi.de
Hasso Plattner Institute
University of Potsdam
Potsdam, GermanyAneta Neumann
aneta.neumann@adelaide.edu.au
Optimisation and Logistics
The University of Adelaide
Adelaide, Australia
Frank Neumann
frank.neumann@adelaide.edu.au
Optimisation and Logistics
The University of Adelaide
Adelaide, AustraliaAishwarya Radhakrishnan
aishwarya.radhakrishnan@hpi.de
Hasso Plattner Institute
University of Potsdam
Potsdam, Germany
ABSTRACT
Understanding how evolutionary algorithms perform on constrained
problems has gained increasing attention in recent years. In this
paper, we study how evolutionary algorithms optimize constrained
versions of the classical LeadingOnes problem. We first provide a
run time analysis for the classical (1+1) EA on the LeadingOnes
problem with a deterministic cardinality constraint, giving Î˜(ğ‘›(ğ‘›âˆ’
ğµ)log(ğµ)+ğ‘›2)as the tight bound. Our results show that the be-
haviour of the algorithm is highly dependent on the constraint
bound of the uniform constraint. Afterwards, we consider the prob-
lem in the context of stochastic constraints and provide insights
using experimental studies on how the ( ğœ‡+1) EA is able to deal with
these constraints in a sampling-based setting.
KEYWORDS
Evolutionary algorithms, chance constraint optimization, run time
analysis, theory.
1 INTRODUCTION
Evolutionary algorithms [ 6] have been used to tackle a wide range
of combinatorial and complex engineering problems. Understand-
ing evolutionary algorithms from a theoretical perspective is crucial
to explain their success and give guidelines for their application.
The area of run time analysis has been a major contributor to the
theoretical understanding of evolutionary algorithms over the last
25 years [ 4,11,19]. Classical benchmark problems such as One-
Max and LeadingOnes have been analyzed in a very detailed way,
showing deep insights into the working behaviour of evolutionary
algorithms for these problems. In real-world settings, problems that
are optimized usually come with a set of constraints which often
limits the resources available. Studying classical benchmark prob-
lems even with an additional simple constraint such as a uniform
constraint, which limits the number of elements that can be chosen
in a given benchmark function, poses significant new technical chal-
lenges for providing run time bounds of even simple evolutionary
algorithms such as the (1+1) EA.
OneMax and the broader class of linear functions [ 5] have played
a key role in developing the area of run time analysis during the last
25 years, and run time bounds for linear functions with a uniform
constraint have been obtained [ 7,17]. It has been shown in [ 7] that
the (1+1) EA needs exponential time optimize OneMax under aspecific linear constraint which points to the additional difficulty
which such constraints impose on the search process. Tackling
constraints by taking them as additional objectives has been shown
to be quite successful for a wide range of problems. For example,
the behaviour of evolutionary multi-objective algorithms has been
analyzed for submodular optimization problems with various types
of constraints [ 20,21]. Furthermore, the performance of evolution-
ary algorithms for problems with dynamic constraints has been
investigated in [22, 23].
Another important area involving constraints is chance con-
strained optimization, which deals with stochastic components in
the constraints. Here, the presence of stochastic components in
the constraints makes it challenging to guarantee that the con-
straints are not violated at all. Chance-constrained optimization
problems [ 2,13] are an important class of the stochastic optimiza-
tion problems [ 1] that optimize a given problem under the condition
that a constraint is only violated with a small probability. Such prob-
lems occur in a wide range of areas, including finance, logistics
and engineering [ 9,12,14,29]. Recent studies of evolutionary al-
gorithms for chance-constrained problems focused on a classic
knapsack problem where the uncertainty lies in the probabilistic
constraints [ 26,27]. Here, the aim is to maximise the deterministic
profit subject to a constraint which involves stochastic weights and
where the knapsack capacity bound can only be violated with a
small probability of at most ğ›¼. A different stochastic version of the
knapsack problem has been studied in [ 16]. Here profits involve un-
certainties and weights are deterministic. In that work, Chebyshev
and Hoeffding-based fitness functions have been introduced and
evaluated. These fitness functions discount expected profit values
based on uncertainties of the given solutions.
Theoretical investigations for problems with chance constraints
have gained recent attention in the area of run time analysis. This
includes studies for montone submodular problems [ 15] and special
instances of makespan scheduling [24]. Furthermore, detailed run
time analyses have been carried out for specific classes of instances
for the chance constrained knapsack problem [18, 28].
1.1 Our contribution
In this paper, we investigate the behaviour of the (1+1) EA for the
classical LeadingOnes problem with additional constraints. We
first study the behaviour for the case of a uniform constraint whicharXiv:2305.18267v1  [cs.NE]  29 May 2023GECCO â€™23, July 15â€“19, 2023, Lisbon, Portugal Friedrich and KÃ¶tzing, et al.
limits the number of 1-bits that can be contained in any feasible
solution. Let ğµbe the upper bound on the number of 1-bits that
any feasible solution can have. Then the optimal solutions consists
of exactlyğµleading 1s and afterwards only 0s. The search for the
(1+1) EA is complicated by the fact that when the current solution
consists ofğ‘˜<ğµleading 1s, additional 1-bits not contributing to the
fitness score at positions ğ‘˜+2,...,ğ‘› might make solutions infeasible.
We provide a detailed analysis of such scenarios in dependence of
the given bound ğµ.
Specifically, we show a tight bound of Î˜(ğ‘›2+ğ‘›(ğ‘›âˆ’ğµ)log(ğµ))(see
Corollary 6). Note that [ 7] shows the weaker bound of ğ‘‚(ğ‘›2log(ğµ)),
which, crucially, does not give insight into the actual optimization
process at the constraint. Our analysis shows in some detail how
the search progresses. In the following discussion, for the current
search point of the algorithm, we call the part of the leading 1s the
head of the bit string, the first 0thecritical bit and the remaining
bits the tail. While the size of the head is less than ğµâˆ’(ğ‘›âˆ’ğµ),
optimization proceeds much like for unconstrained LeadingOnes;
this is because the bits in the tail of size about 2(ğ‘›âˆ’ğµ)are (almost)
uniformly distributed, contributing roughly a number of ğ‘›âˆ’ğµmany
1s additionally to the ğµâˆ’(ğ‘›âˆ’ğµ)many 1s in the head. This stays in
sum (mostly) below the cardinality bound ğµ, occasional violations
changing the uniform distribution of the tail to one where bits in
the tail are 1with probability a little less than 1/2(see Lemma 3).
Once the threshold of ğµâˆ’(ğ‘›âˆ’ğµ)many 1s in the head is passed,
the algorithm frequently runs into the constraint. For a phase of
equal LeadingOnes value, we consider the random walk of the
number of 1s of the bit string of the algorithm. This walk has
a bias towards the bound ğµ(its maximal value), where the bias
is light for LeadingOnes-values just a bit above ğµâˆ’(ğ‘›âˆ’ğµ)and
getting stronger as this value approaches ğµ. Since progress is easy
when not at the bound of ğµmany 1s in the bit string (by flipping
the critical bit and no other) and difficult otherwise (additionally
to flipping the critical bit, a 1in the tail needs to flip), the exact
proportion of time that the walk spends in states of less than ğµ
versus exactly ğµmany 1s is very important. In the final proofs, we
estimate these factors and have corresponding potential functions
reflecting gains (1) from changing into states of less than ğµmany
1s and (2) gaining a leading 1. Bounding these gains appropriately
lets us find asymptotically matching upper and lower bounds using
the additive drift theorem [10].
In passing we note that two different modifications of the setting
yield a better time of ğ‘‚(ğ‘›2). First, this time is sufficient to achieve a
LeadingOnes-values of ğµâˆ’ğ‘(ğ‘›âˆ’ğµ)for anyğ‘>0(see Corollary 7).
Second, considering the number of 1s as a secondary objective (to
be minimized) gives an optimization time of ğ‘‚(ğ‘›2)(see Theorem 8).
Afterwards, we turn to stochastic constraints and investigate an
experimental setting that is motivated by recent studies in the area
of chance constraints. We consider LeadingOnes with a stochastic
knapsack chance constraint, where the weights of a linear con-
straint are chosen from a given distribution. In the first setting, the
weight of each item is chosen independently according to a Normal
distribution ğ‘(ğœ‡,ğœ2). A random sample of weights is feasible if
the sum of the chosen sampled weights does not exceed a given
knapsack bound ğµ. In any iteration, all weights are resampled inde-
pendently for all evaluated individuals. Our goal is to understand
the maximal stable LeadingOnes value that the algorithm obtains.In the second setting which we study empirically, the weights are
deterministically set to 1and the bound is chosen uniformly at
random within an interval [ğµâˆ’ğœ–,ğµ+ğœ–], whereğœ–>0specifies
the uncertainty around the constraint bound. For both settings, we
examine the performance of the (1+1)EA and(10+1)-EA for
different values of ğµand show that a larger parent population has
a highly positive effect for these stochastic settings.
The paper is structured as follows. In Section 2, we introduce the
problems and algorithms that we study in this paper. We present our
run time analysis for the LeadingOnes problem with a deterministic
uniform constraint in Section 3. In section 4, we discuss a way to
obtain Î˜(ğ‘›2)bound on the run time for the same problem and
report on our empirically investigations for the stochastic settings
in Section 5. Finally, we finish with some concluding remarks. Note
that some proofs are ommitted due to space constraints.
2 PRELIMINARIES
In this section we define the objective function, constraints and the
algorithms used in our analysis. With |ğ‘¥|1we denote the number
of1s in a bit string ğ‘¥âˆˆ{0,1}ğ‘›.
2.1 Cardinality Constraint
Letğ‘“:{0,1}ğ‘›â†’R,ğµâ‰¤ğ‘›and forğ‘¥âˆˆ{0,1}ğ‘›, letğ‘¥ğ‘–denote theğ‘–
-th bit ofğ‘¥. In this paper, optimizing ğ‘“with cardinality constraint
ğµmeans finding, maxğ‘¥âˆˆ{0,1}ğ‘›ğ‘“(ğ‘¥)s.tÃğ‘›
ğ‘–=1ğ‘¥ğ‘–â‰¤ğµ.
2.2 Stochastic Constraint
Letğ‘“:{0,1}ğ‘›â†’R,ğµâ‰¤ğ‘›and forğ‘¥âˆˆ{0,1}ğ‘›, letğ‘¥ğ‘–denote theğ‘–-th
bit ofğ‘¥. In this paper we empirically analyse the following normal
stochastic constraint with uncertainty in the weights optimization
problem,
max
ğ‘¥âˆˆ{0,1}ğ‘›ğ‘“(ğ‘¥)s.tğ‘›âˆ‘ï¸
ğ‘–=1ğ‘¤ğ‘–Â·ğ‘¥ğ‘–â‰¤ğµ, whereğ‘¤ğ‘–âˆ¼ğ‘(ğœ‡,ğœ2).
Letğ‘“:{0,1}ğ‘›â†’R,ğµâ‰¤ğ‘›and forğ‘¥âˆˆ{0,1}ğ‘›, letğ‘¥ğ‘–denote
theğ‘–-th bit ofğ‘¥. In this paper we also empirically analyse the
following uniform stochastic constraint with uncertainty in the
bound optimization problem,
max
ğ‘¥âˆˆ{0,1}ğ‘›ğ‘“(ğ‘¥)s.t|ğ‘¥|1â‰¤ğ‘¦, whereğ‘¦âˆ¼ğ‘ˆ(ğµâˆ’ğœ–,ğµ+ğœ–).
2.3 Objective Function
We consider the LeadingOnes function as our objective with car-
dinality and stochastic constraints for our analysis.
LeadingOnes :{0,1}ğ‘›â†’R, is a function which maps a bit
string of length ğ‘›to number of 1s before the first 0in the bit string.
For everyğ‘¥âˆˆ{0,1}ğ‘›,LeadingOnes(ğ‘¥)=Ãğ‘›
ğ‘–=1Ãğ‘–
ğ‘—=1ğ‘¥ğ‘—.
2.4 (ğœ‡+1) EA
The (ğœ‡+1) EA on a real valued fitness function ğ‘“with constraint ğµ
is given in Algorithm 1. The ( ğœ‡+1) EA at each iteration maintains
a population of size ğœ‡. The initial population ğ‘ƒ0hasğœ‡random bit
strings chosen uniformly. At each iteration ğ‘¡>0, a bit string is cho-
sen uniformly at random from ğ‘ƒğ‘¡followed by a mutation operation
which flips each bit of the chosen bit string with probability1
ğ‘›. The
mutated bit string is added to ğ‘ƒğ‘¡and the bit string with the leastAnalysis of the (1+1) EA on LeadingOnes with Constraints GECCO â€™23, July 15â€“19, 2023, Lisbon, Portugal
fitness among the ğœ‡+1individuals is removed. Since we can also
sample a bit string which violates the constraint, we consider the
following function for optimization.
ğ‘”(ğ‘¥)=(
ğ‘“(ğ‘¥), if|ğ‘¥|1<ğµ;
ğµâˆ’|ğ‘¥|,otherwise.
Algorithm 1: (ğœ‡+1) EA on fitness function ğ‘“with con-
straint B
1ğ‘ƒ0â†ğœ‡individuals from{0,1}ğ‘›chosen u.a.r.;
2ğ‘¡=0;
3while stopping criterion not met do
4ğ‘¥â†uniform random bit string from ğ‘ƒğ‘¡;
5ğ‘¦â†flip each bit of ğ‘¥independently with probab. 1/ğ‘›;
6ğ‘ƒğ‘¡=ğ‘ƒğ‘¡âˆª{ğ‘¦};
7ğ‘ƒğ‘¡+1=ğ‘ƒğ‘¡\{an individual ğ‘¥âˆˆğ‘ƒğ‘¡with leastğ‘”(ğ‘¥)value};
8ğ‘¡=ğ‘¡+1;
3 UNMODIFIED SETTING
In this section we give a tight analysis of the (1+1) EA on the
objective LeadingOnes with cardinality constraint ğµ.
We start with a technical lemma which we need for our proof of
the upper bound.
Lemma 1. Forğ‘¡â‰¥0, letğ‘¥ğ‘¡denote the parent bit string at ğ‘¡-th iter-
ation while (1+1) EA is optimizing LeadingOnes with the cardinality
constraint B. And for ğ‘¡>0, letğ´ğ‘¡denote the event that |ğ‘¥ğ‘¡+1|1=ğµ
andğ¿ğ‘‚(ğ‘¥ğ‘¡+1)=ğ¿ğ‘‚(ğ‘¥ğ‘¡). Thenğ‘ƒğ‘Ÿ(ğ´ğ‘¡|ğ‘¥ğ‘¡|1<ğµ)â‰¤ğ‘›âˆ’ğµ
ğ‘›.
Proof. First note that, if|ğ‘¥ğ‘¡|1=ğ‘˜<ğµandğ¶ğ‘¡denote the event
thatğ‘¥ğ‘¡+1is formed by flipping ğµâˆ’ğ‘˜number of 0bits to 1out of
ğ‘›âˆ’ğ‘˜âˆ’1(except the left most 0) number of 0bits, then
ğ‘ƒğ‘Ÿ(ğ´ğ‘¡|ğ‘¥ğ‘¡|1<ğµ)â‰¤ğ‘ƒğ‘Ÿ(ğ¶ğ‘¡|ğ‘¥ğ‘¡|1<ğµ).
The eventğ´ğ‘¡is a sub-event of ğ¶ğ‘¡, since in the event ğ¶ğ‘¡we do not
have any restriction on the bits other than ğµâˆ’ğ‘˜number of 0bits
out ofğ‘›âˆ’ğ‘˜âˆ’1number of them and we have to flip at least ğµâˆ’ğ‘˜
number of 0bits to 1to get the desired ğ‘¥ğ‘¡+1in the event ğ´ğ‘¡. Hence,
ğ‘ƒğ‘Ÿ(ğ¶ğ‘¡)=ğ‘›âˆ’ğ‘˜âˆ’1
ğµâˆ’ğ‘˜ 1
ğ‘›ğµâˆ’ğ‘˜
=(ğ‘›âˆ’ğ‘˜âˆ’1)Â·(ğ‘›âˆ’ğ‘˜âˆ’2)Â·Â·Â·(ğ‘›âˆ’ğµ)
1Â·2Â·Â·Â·(ğµâˆ’ğ‘˜)Â·1
ğ‘›ğµâˆ’ğ‘˜
â‰¤ğ‘›âˆ’ğµ
ğ‘›.
The last inequality holds because, for every ğ‘Ÿ>0,ğ‘›âˆ’ğ‘˜âˆ’ğ‘Ÿ
ğ‘›â‰¤1.â–¡
In the Theorem 2 below we give an upper bound on the ex-
pected run time of the (1+1) EA on LeadingOnes with cardinality
constraintğµ. Later we show that this bound is tight by proving a
matching lower bound.
Theorem 2. Letğ‘›,ğµâˆˆNandğµ<ğ‘›. Then the expected optimiza-
tion time of the (1+1) EA on LeadingOnes with cardinality constraint
ğµisğ‘‚ ğ‘›2+ğ‘›(ğ‘›âˆ’ğµ)logğµ.Proof. From [ 8, Lemma 3], we know that the (1+1) EA is ex-
pected to find a feasible solution within ğ‘‚(ğ‘›log(ğ‘›/ğµ))iterations.
Now we calculate how long it takes in expected value to find the
optimum after a feasible solution is sampled.
To do this, we construct a potential function that yields an drift
value greater than 1at each time ğ‘¡until the optimum is found. For
ğ‘–âˆˆ{0,Â·Â·Â·,ğµ}, letğ‘”ğµ(ğ‘–)be the potential of a bit string ğ‘¥âˆˆ{0,1}ğ‘›
with exactly ğµnumber of 1s andğ¿ğ‘‚(ğ‘¥)=LeadingOnes(ğ‘¥)=ğ‘–.
Forğ‘–âˆˆ{0,Â·Â·Â·,ğµâˆ’1}, letğ‘”<ğµ(ğ‘–)be the potential of a bit string
ğ‘¥âˆˆ{0,1}ğ‘›with less than ğµnumber of 1s andğ¿ğ‘‚(ğ‘¥)=ğ‘–.
Letğ‘”ğµ(0)=0andğ‘”<ğµ(0)=ğ‘’ğ‘›
ğµ.And for every ğ‘–âˆˆ{1,Â·Â·Â·,ğµ},
let
ğ‘”ğµ(ğ‘–)=ğ‘’ğ‘›
1+ğ‘’Â·(ğ‘›âˆ’ğµ)
ğµâˆ’ğ‘–+1
+ğ‘”<ğµ(ğ‘–âˆ’1),
and for every ğ‘–âˆˆ{1,Â·Â·Â·,ğµâˆ’1}, let
ğ‘”<ğµ(ğ‘–)=ğ‘’ğ‘›
ğµâˆ’ğ‘–+ğ‘”ğµ(ğ‘–).
Forğ‘¡>0, letğ‘‹ğ‘¡be the parent bit string of (1+1) EA at iteration
ğ‘¡. and letğ‘‡be the iteration number at which (1+1) EA finds the
optimum for the first time. Let
ğ‘“(ğ‘‹ğ‘¡)=(
ğ‘”ğµ(ğ¿ğ‘‚(ğ‘‹ğ‘¡)) if|ğ‘‹ğ‘¡|1=ğµ,
ğ‘”<ğµ(ğ¿ğ‘‚(ğ‘‹ğ‘¡)if|ğ‘‹ğ‘¡|1<ğµ.(1)
We consider two different cases, |ğ‘‹ğ‘¡|1=ğµand|ğ‘‹ğ‘¡|1<ğµand
show in both the cases the drift is at least 1. Suppose we are in an
iterationğ‘¡<ğ‘‡withğ¿ğ‘‚(ğ‘‹ğ‘¡)=ğ‘–and|ğ‘‹ğ‘¡|1=ğµ. Then the probability
that the number of 1s in the search point can decrease by 1in the
next iteration is at leastğµâˆ’ğ‘–
ğ‘’ğ‘›. This is because we can get a desired
search point by flipping only one of the 1bits ofğµâˆ’ğ‘–, excluding
the leading 1s, and not flipping any other bit. Therefore,
ğ¸[ğ‘“(ğ‘‹ğ‘¡+1)âˆ’ğ‘“(ğ‘‹ğ‘¡)ğ¿ğ‘‚(ğ‘‹ğ‘¡)=ğ‘–,|ğ‘‹ğ‘¡|1=ğµ]
â‰¥(ğ‘”<ğµ(ğ‘–)âˆ’ğ‘”ğµ(ğ‘–))Â·ğ‘ƒğ‘Ÿ(|ğ‘‹ğ‘¡+1|1<ğµ)
â‰¥ğ‘’ğ‘›
ğµâˆ’ğ‘–+ğ‘”ğµ(ğ‘–)âˆ’ğ‘”ğµ(ğ‘–)
Â·ğµâˆ’ğ‘–
ğ‘’ğ‘›
=1.
Suppose we are in an iteration ğ‘¡<ğ‘‡withğ¿ğ‘‚(ğ‘‹ğ‘¡)=ğ‘–and|ğ‘‹ğ‘¡|1<ğµ.
Then in the next iteration the value of LeadingOnes can increase
when the leftmost 0is flipped to 1as this does not violate the
constraint. This happens with probability at least1
ğ‘’ğ‘›. Since|ğ‘‹ğ‘¡|1<
ğµ, we can also stay in the same level (same number of leading 1s)
and the number of 1s can increase to ğµwith probability at most
ğ‘›âˆ’ğµ
ğ‘›(see Lemma 1). This implies that the potential can decrease by
ğ‘’ğ‘›
ğµâˆ’ğ‘–with probability at mostğ‘›âˆ’ğµ
ğ‘›.
ğ¸[ğ‘“(ğ‘‹ğ‘¡+1)âˆ’ğ‘“(ğ‘‹ğ‘¡)ğ¿ğ‘‚(ğ‘‹ğ‘¡)=ğ‘–,|ğ‘‹ğ‘¡|1<ğµ]
â‰¥(ğ‘”(ğ‘–+1,ğµ)âˆ’ğ‘”<ğµ(ğ‘–))Â·1
ğ‘’ğ‘›âˆ’ğ‘’ğ‘›
ğµâˆ’ğ‘–Â·ğ‘›âˆ’ğµ
ğ‘›
â‰¥
ğ‘’ğ‘›
1+ğ‘’Â·(ğ‘›âˆ’ğµ)
ğµâˆ’ğ‘–
+ğ‘”<ğµ(ğ‘–)âˆ’ğ‘”<ğµ(ğ‘–)
Â·1
ğ‘’ğ‘›
âˆ’ğ‘’Â·(ğ‘›âˆ’ğµ)
ğµâˆ’ğ‘–
=1.GECCO â€™23, July 15â€“19, 2023, Lisbon, Portugal Friedrich and KÃ¶tzing, et al.
This results in an expected additive drift value greater than 1in all
the cases, so according to the additive drift theorem [ 10, Theorem 5],
ğ¸[ğ‘‡]â‰¤ğ‘“(ğ‘‹ğ‘‡)=ğ‘”ğµ(ğµ)
=ğµâˆ’1âˆ‘ï¸
ğ‘–=0(ğ‘”<ğµ(ğ‘–)âˆ’ğ‘”ğµ(ğ‘–))+ğµâˆ‘ï¸
ğ‘–=1(ğ‘”ğµ(ğ‘–)âˆ’ğ‘”<ğµ(ğ‘–âˆ’1))
=ğµâˆ’1âˆ‘ï¸
ğ‘–=0ğ‘’ğ‘›
ğµâˆ’ğ‘–+ğµâˆ‘ï¸
ğ‘–=1ğ‘’ğ‘›
1+ğ‘’Â·(ğ‘›âˆ’ğµ)
ğµâˆ’ğ‘–+1
=ğ‘’ğ‘›ğµâˆ‘ï¸
ğ‘–=11
ğ‘–
+ğ‘’ğ‘›ğµ+ğ‘’2Â·ğ‘›(ğ‘›âˆ’ğµ)ğµâˆ‘ï¸
ğ‘–=11
ğ‘–
â‰¤ğ‘’ğ‘›(logğµ+1)+ğ‘’ğ‘›ğµ+ğ‘’2Â·ğ‘›(ğ‘›âˆ’ğµ)(logğµ+1)
=ğ‘‚(ğ‘›2+ğ‘›(ğ‘›âˆ’ğµ)logğµ).
â–¡
We now turn to the lower bound. When (1+1) EA optimizes
LeadingOnes in unconstrained setting the probability that a bit
which is after the left-most 0is1is exactly1
2. But this is not true in
the constrained setting. The following lemma gives an upper bound
on this probability during the cardinality constraint optimization.
Lemma 3. For anyğ‘¡â‰¥0, letğ‘¥ğ‘¡denote the search point at iteration
ğ‘¡when (1+1) EA is optimizing LeadingOnes with the cardinality
constraintğµ. Then for any ğ‘¡â‰¥0andğ‘–>ğ¿ğ‘‚(ğ‘¥ğ‘¡),ğ‘ƒğ‘Ÿ(ğ‘¥ğ‘¡
ğ‘–=1)â‰¤1/2.
Proof. We will prove this by induction. The base case is true
because we have an uniform random bit string at ğ‘¡=0. Lets assume
that the statement is true for ğ‘¡, i.e. for any ğ‘–>ğ¿ğ‘‚(ğ‘¥ğ‘¡),ğ‘ƒğ‘Ÿ(ğ‘¥ğ‘¡
ğ‘–=
1)â‰¤1/2. Letğ´be the event that the offspring is accepted. Then,
forğ‘–>ğ¿ğ‘‚(ğ‘¥ğ‘¡+1),
ğ‘ƒğ‘Ÿ(ğ‘¥ğ‘¡+1
ğ‘–=1)=ğ‘ƒğ‘Ÿ((ğ‘¥ğ‘¡
ğ‘–=0)âˆ©(ğ‘–ğ‘¡â„bit is flipped)âˆ©ğ´)
+ğ‘ƒğ‘Ÿ((ğ‘¥ğ‘¡
ğ‘–=1)âˆ©(ğ‘–ğ‘¡â„bit is flipped)âˆ©ğ´ğ‘)
+ğ‘ƒğ‘Ÿ((ğ‘¥ğ‘¡
ğ‘–=1)âˆ©(ğ‘–ğ‘¡â„bit is not flipped).
Letğ‘ƒğ‘Ÿ(ğ‘¥ğ‘¡
ğ‘–=1)=ğ‘,ğ‘ƒğ‘Ÿ(ğ´(ğ‘–ğ‘¡â„bit is flippedâˆ©ğ‘¥ğ‘¡
ğ‘–=0))=ğ‘
andğ‘ƒğ‘Ÿ(ğ´(ğ‘–ğ‘¡â„bit is flippedâˆ©ğ‘¥ğ‘¡
ğ‘–=1))=ğ‘. Then note that ğ‘â‰¤
ğ‘(because we have at least as many events as in probability ğ‘
contributing to the probability ğ‘) and by induction hypothesis,
ğ‘ƒğ‘Ÿ(ğ‘¥ğ‘¡+1
ğ‘–=1)=(1âˆ’ğ‘)Â·1/ğ‘›Â·ğ‘+ğ‘Â·1/ğ‘›Â·(1âˆ’ğ‘)+ğ‘Â·(1âˆ’1/ğ‘›)
=ğ‘/ğ‘›âˆ’(ğ‘Â·ğ‘)/ğ‘›+ğ‘/ğ‘›âˆ’(ğ‘Â·ğ‘)/ğ‘›+ğ‘âˆ’ğ‘/ğ‘›
=ğ‘/ğ‘›âˆ’ğ‘Â·(1âˆ’ğ‘/ğ‘›âˆ’ğ‘/ğ‘›)
â‰¤ğ‘/ğ‘›+1/2Â·(1âˆ’ğ‘/ğ‘›âˆ’ğ‘/ğ‘›)
â‰¤ğ‘/ğ‘›+1/2Â·(1âˆ’ğ‘/ğ‘›âˆ’ğ‘/ğ‘›)=1/2.
â–¡
We use the previous lemma to prove the Î©(ğ‘›2)lower bound on
the expected time in the next theorem.
Theorem 4. Letğ‘›,ğµâˆˆN. Then the expected optimization time
of the (1+1) EA on the LeadingOnes with cardinality constraint ğµis
Î© ğ‘›2.Proof. We use the fitness level method with visit probabilities
technique defined in [ 3, Theorem 8] to prove this lower bound.
Similar to [ 3, Theorem 11], we also partition the search space {0,1}ğ‘›
based on the LeadingOnes values. For every ğ‘–â‰¤ğµ, letğ´ğ‘–contain
all the bit strings with the LeadingOnes valueğ‘–. If our search point
is inğ´ğ‘–, then we say that the search point is in the state ğ‘–. For every
ğ‘–âˆˆ{1,Â·Â·Â·,ğµâˆ’1}, we have to find the visit probabilities ğ‘£ğ‘–and an
upper bound for ğ‘ğ‘–, the probability to leave the state ğ‘–.
The best case scenario for the search point to leave the state ğ‘–
is when the number of 1s in the search point is less than ğµ. In this
case, we have to flip the (ğ‘–+1)ğ‘¡â„bit to 1and should not flip any of
the firstğ‘–bits to 0. This happens with the probability1
ğ‘›Â·
1âˆ’1
ğ‘›ğ‘–
.
Therefore, for every ğ‘–âˆˆ{1,Â·Â·Â·,ğµâˆ’1},ğ‘ğ‘–â‰¤1
ğ‘›Â·
1âˆ’1
ğ‘›ğ‘–
.
Next, we claim that, for each ğ‘–âˆˆ{1,Â·Â·Â·,ğµâˆ’1},ğ‘£ğ‘–â€“ the proba-
bility to visit the state ğ‘–is at least1
2. We use [ 3, Lemma 10] to show
this. Suppose the initial search point is in a state greater than or
equal toğ‘–, then the probability for it to be in state ğ‘–is equal to the
probability that the (ğ‘–+1)ğ‘¡â„bit is 0. Since the initial bit string is
chosen uniformly at random the probability that the (ğ‘–+1)ğ‘¡â„bit is 0
is1
2. This shows the first required bound on the probability for the
lemma in [ 3, Lemma 10]. Suppose the search point is transitioning
into a level greater than or equal to ğ‘–, then the probability that it
transition into state ğ‘–is equal to the probability that (ğ‘–+1)ğ‘¡â„bit is
0. From Lemma 3, we know that this probability is at least 1/2. This
gives the second bound required for the [ 3, Lemma 10], therefore
ğ‘£ğ‘–is at least1
2.
By using fitness level method with visit probabilities theorem
[3, Theorem 8], if ğ‘‡is the time taken by the (1+1) EA to find an
individual with ğµnumber of LeadingOnes for the first time then,
we have,ğ¸[ğ‘‡]â‰¥Ãğµâˆ’1
ğ‘–=0ğ‘£ğ‘–
ğ‘ğ‘–=ğ‘›
2Â·ğµâˆ’1âˆ‘ï¸
ğ‘–=0
1âˆ’1
ğ‘›âˆ’ğ‘–
â‰¥ğ‘›2
2=Î©(ğ‘›2).â–¡
We aim to show the Î©(ğ‘›2+ğ‘›(ğ‘›âˆ’ğµ)logğµ)lower bound and
Theorem 4 gives the Î©(ğ‘›2)lower bound. Therefore, next we con-
sider the case where ğµis such thatğ‘›(ğ‘›âˆ’ğµ)logğµâ‰ ğ‘‚(ğ‘›2)to prove
the desired lower bound.
Theorem 5. Letğ‘›,ğµâˆˆNand suppose ğ‘›(ğ‘›âˆ’ğµ)logğµ=ğœ”(ğ‘›2).
Then the expected optimization time of the (1+1) EA on the objective
LeadingOnes with cardinality constraint ğµisÎ©(ğ‘›(ğ‘›âˆ’ğµ)logğµ).
Proof. We consider the potential function ğ‘”such that, for all
ğ‘¥âˆˆ{0,1}ğ‘›,
ğ‘”(ğ‘¥)=ğ‘›Â·|ğ‘¥|1
ğµâˆ’ğ¿ğ‘‚(ğ‘¥)+1+ğµâˆ’1âˆ‘ï¸
ğ‘–=ğ¿ğ‘‚(ğ‘¥)ğ‘›(ğ‘›âˆ’ğµ)
32ğ‘’2(ğµâˆ’ğ‘–).
The first term appreciates progress by reducing the number of 1s.
This is scaled to later derive constant drift in expectation from
such a reduction whenever |ğ‘¥|1=ğµ, the case where progress by
increasing the number of leading 1s is not easy. The second term
appreciates progress by increasing the number of leading 1s, scaled
to derive constant drift in case of |ğ‘¥|1<ğµ.
The idea of the proof is as follows. We show that the potential
decreases by at most 10in expectation. Then the lower bound of
additive drift theorem will give the desired lower bound on the
expected run time (see [10, Theorem 5]).Analysis of the (1+1) EA on LeadingOnes with Constraints GECCO â€™23, July 15â€“19, 2023, Lisbon, Portugal
We start by calculating the expected potential at ğ‘¡=0. Since the
initial bit string is chosen uniformly at random the probability that
the first bit is 0is1
2. Thereforeğ‘ƒğ‘Ÿ(ğ¿ğ‘‚(ğ‘¥0)=0)=1
2, which implies
ğ¸[ğ‘”(ğ‘¥0)]â‰¥1
2Â·ğ¸ï£®ï£¯ï£¯ï£¯ï£¯ï£°ğµâˆ’1âˆ‘ï¸
ğ‘–=ğ¿ğ‘‚(ğ‘¥0)ğ‘›(ğ‘›âˆ’ğµ)
32ğ‘’2(ğµâˆ’ğ‘–)ğ¿ğ‘‚(ğ‘¥0)=0ï£¹ï£ºï£ºï£ºï£ºï£»
=ğ‘›(ğ‘›âˆ’ğµ)
64ğ‘’2ğµâˆ’1âˆ‘ï¸
ğ‘–=01
ğµâˆ’ğ‘–
=ğ‘›(ğ‘›âˆ’ğµ)
64ğ‘’2Â·ğµâˆ‘ï¸
ğ‘–=11
ğ‘–â‰¥ğ‘›(ğ‘›âˆ’ğµ)ln(ğµ)
64ğ‘’2.
Therefore, there exits a constant ğ‘>0such thatğ¸[ğ‘”(ğ‘¥0)]â‰¥ğ‘ğ‘›(ğ‘›âˆ’
ğµ)logğµ.The optimum has a potential value of ğ‘›ğµ; thus, we can
find a lower bound on the optimization time by considering the
time to find a potential value of at most ğ‘›ğµ. Letğ‘‡=min{ğ‘¡â‰¥0|
ğ‘”(ğ‘¥ğ‘¡)â‰¤ğ‘›ğµ}. Note thatğ‘‡may not be the time at which we find
the optimum for the first time. From ğ‘›(ğ‘›âˆ’ğµ)logğµ=ğœ”(ğ‘›2)we
get, forğ‘›large enough, that ğ¸[ğ‘”(ğ‘¥0)]>ğ‘›ğµ, which implies that the
expected optimization time is at least ğ¸[ğ‘‡].
In order to show the lower bound on the drift, we consider two
different cases,|ğ‘¥ğ‘¡|1=ğµand|ğ‘¥ğ‘¡|1<ğµand show in both the cases
drift is at most 10. First, we examine the case where the algorithm
has currently ğµnumber of 1s. For anyğ‘¡, letğ´ğ‘¡be the event that
|ğ‘¥ğ‘¡|1=ğµand let Î”ğ‘¡=ğ‘”(ğ‘¥ğ‘¡)âˆ’ğ‘”(ğ‘¥ğ‘¡+1)and
Î”ğ‘ 
ğ‘¡=ğµâˆ’1âˆ‘ï¸
ğ‘–=ğ¿ğ‘‚(ğ‘¥ğ‘¡)ğ‘›(ğ‘›âˆ’ğµ)
32ğ‘’2(ğµâˆ’ğ‘–)âˆ’ğµâˆ’1âˆ‘ï¸
ğ‘–=ğ¿ğ‘‚(ğ‘¥ğ‘¡+1)ğ‘›(ğ‘›âˆ’ğµ)
32ğ‘’2(ğµâˆ’ğ‘–)
=ğ¿ğ‘‚(ğ‘¥ğ‘¡+1)âˆ’1âˆ‘ï¸
ğ‘–=ğ¿ğ‘‚(ğ‘¥ğ‘¡)ğ‘›(ğ‘›âˆ’ğµ)
32ğ‘’2(ğµâˆ’ğ‘–).
Then,ğ¸[Î”ğ‘¡ğ´ğ‘¡]=ğ‘›Â·ğ¸|ğ‘¥ğ‘¡|1
ğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡)+1âˆ’|ğ‘¥ğ‘¡+1|1
ğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡+1)+1ğ´ğ‘¡
+ğ¸[Î”ğ‘ 
ğ‘¡ğ´ğ‘¡]
â‰¤ğ‘›Â·ğ¸|ğ‘¥ğ‘¡|1âˆ’|ğ‘¥ğ‘¡+1|1
ğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡+1)+1ğ´ğ‘¡
+ğ¸[Î”ğ‘ 
ğ‘¡ğ´ğ‘¡].
Now we calculate the bounds for all the required expectations in
the above equation.
First we calculate a bound for ğ‘›Â·ğ¸h|ğ‘¥ğ‘¡|1âˆ’|ğ‘¥ğ‘¡+1|1
ğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡+1)+1ğ´ğ‘¡i
by using
the definition of the expectation. Let ğ¼={0,Â·Â·Â·,ğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡)}and
ğ½={âˆ’1,0,Â·Â·Â·,ğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡+1)âˆ’1}. Then the possible values the
random variable|ğ‘¥ğ‘¡|1âˆ’|ğ‘¥ğ‘¡+1|1can have are the values in ğ¼. And the
possible values ğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡+1)+1can have are{ğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡)âˆ’ğ‘—|ğ‘—âˆˆğ½}.
Forğ‘–âˆˆ{1,Â·Â·Â·,ğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡)}, the probability ğ‘ƒğ‘Ÿ((|ğ‘¥ğ‘¡|1âˆ’|ğ‘¥ğ‘¡+1|1=ğ‘–)âˆ©
(ğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡+1)+1=ğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡)+1))â‰¤ ğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡)
ğ‘–
1
ğ‘›ğ‘–
â‰¤ğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡)
ğ‘–!ğ‘›
and forğ‘–âˆˆ{1,Â·Â·Â·,ğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡)}andğ‘—âˆˆ{0,Â·Â·Â·,ğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡+1)âˆ’1}},
the probability ğ‘ƒğ‘Ÿ((|ğ‘¥ğ‘¡|1âˆ’|ğ‘¥ğ‘¡+1|1=ğ‘–)âˆ©(ğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡+1)+1=ğµâˆ’
ğ¿ğ‘‚(ğ‘¥ğ‘¡)âˆ’ğ‘—))â‰¤ ğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡)
ğ‘–
1
ğ‘›ğ‘–+11
2ğ‘—â‰¤ğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡)
ğ‘–!ğ‘›21
2ğ‘—(see Lemma
3). Forğ‘–âˆˆğ¼andğ‘—âˆˆğ½, letğ‘ğ‘–ğ‘—
ğ‘¡=ğ‘ƒğ‘Ÿ((|ğ‘¥ğ‘¡|1âˆ’|ğ‘¥ğ‘¡+1|1=ğ‘–)âˆ©(ğµâˆ’
ğ¿ğ‘‚(ğ‘¥ğ‘¡+1)+1=ğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡)âˆ’ğ‘—))andğ¾=ğ½\{ğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡)+1}and
ğ¾ğ‘={ğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡)+1}. Then,ğ‘›Â·ğ¸h|ğ‘¥ğ‘¡|1âˆ’|ğ‘¥ğ‘¡+1|1
ğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡+1)+1ğ´ğ‘¡i=ğ‘›Â·âˆ‘ï¸
ğ‘–âˆˆğ¼âˆ‘ï¸
ğ‘—âˆˆğ½ğ‘–Â·ğ‘ğ‘–ğ‘—
ğ‘¡
ğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡)âˆ’ğ‘—
=ğ‘›Â·âˆ‘ï¸
ğ‘–âˆˆğ¼âˆ‘ï¸
ğ‘—âˆˆğ¾ğ‘ğ‘–Â·ğ‘ğ‘–ğ‘—
ğ‘¡
ğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡)âˆ’ğ‘—+ğ‘›Â·âˆ‘ï¸
ğ‘–âˆˆğ¼âˆ‘ï¸
ğ‘—âˆˆğ¾ğ‘–Â·ğ‘ğ‘–ğ‘—
ğ‘¡
ğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡)âˆ’ğ‘—
=âˆ‘ï¸
ğ‘–âˆˆğ¼ğ‘–(ğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡))
ğ‘–!(ğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡)+1)+âˆ‘ï¸
ğ‘–âˆˆğ¼ğ‘–(ğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡))
ğ‘–!ğ‘›âˆ‘ï¸
ğ‘—âˆˆğ¾1
2ğ‘—
ğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡)âˆ’ğ‘—
â‰¤âˆ‘ï¸
ğ‘–âˆˆğ¼\{0}1
(ğ‘–âˆ’1)!+âˆ‘ï¸
ğ‘–âˆˆğ¼1
(ğ‘–âˆ’1)!âˆ‘ï¸
ğ‘—âˆˆğ¾1
2ğ‘—
â‰¤ğ‘’+2ğ‘’=3ğ‘’â‰¤9. (2)
We used the infinite sum valuesÃâˆ
ğ‘–=11
(ğ‘–âˆ’1)!=ğ‘’,Ãâˆ
ğ‘–=01
2ğ‘–=2, to
bound our required finite sums in the above calculation.
Now, we calculate ğ¸[Î”ğ‘ 
ğ‘¡ğ´ğ‘¡], to get an upper bound for ğ¸[Î”ğ‘¡
ğ´ğ‘¡]. When|ğ‘¥ğ‘¡|1=ğµ, the probability to gain in the LeadingOnes -
values is at mostğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡)
ğ‘›Â·1
ğ‘›. Therefore we have
ğ¸[Î”ğ‘ 
ğ‘¡ğ´ğ‘¡]=ğ‘›(ğ‘›âˆ’ğµ)
32ğ‘’2Â·ğ¸ï£®ï£¯ï£¯ï£¯ï£¯ï£°ğ¿ğ‘‚(ğ‘¥ğ‘¡+1)âˆ’1âˆ‘ï¸
ğ‘–=ğ¿ğ‘‚(ğ‘¥ğ‘¡)1
ğµâˆ’ğ‘–ğ´ğ‘¡ï£¹ï£ºï£ºï£ºï£ºï£»
â‰¤ğ‘›(ğ‘›âˆ’ğµ)
32ğ‘’2Â·ğ¸ğ¿ğ‘‚(ğ‘¥ğ‘¡+1)âˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡)
ğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡+1)+1ğ´ğ‘¡
.
(3)
We calculate an upper bound for ğ¸hğ¿ğ‘‚(ğ‘¥ğ‘¡+1)âˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡)
ğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡+1)+1ğ´ğ‘¡i
. The
probability that ğ¿ğ‘‚(ğ‘¥ğ‘¡+1)âˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡)=ğ‘–given that we gain at least
a leading one is the probability that next ğ‘–âˆ’1bits after left-most 0
bit) is 1followed by a 0bit. This implies that the probability that
ğ¿ğ‘‚(ğ‘¥ğ‘¡+1)âˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡)=ğ‘–given that we gain at least a leading one is
at most1
2ğ‘–âˆ’1. Therefore, we have ğ¸hğ¿ğ‘‚(ğ‘¥ğ‘¡+1)âˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡)
ğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡+1)+1ğ´ğ‘¡i
â‰¤ğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡)
ğ‘›Â·1
ğ‘›Â·ğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡)âˆ’1âˆ‘ï¸
ğ‘–=1ğ‘–Â·21âˆ’ğ‘–
ğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡)âˆ’ğ‘–. (4)
Equations 3 and 4 imply that, ğ¸[Î”ğ‘ 
ğ‘¡ğ´ğ‘¡]
â‰¤ğ‘›(ğ‘›âˆ’ğµ)
32ğ‘’2Â·ğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡)
ğ‘›Â·1
ğ‘›Â·ğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡)âˆ’1âˆ‘ï¸
ğ‘–=1ğ‘–Â·21âˆ’ğ‘–
ğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡)âˆ’ğ‘–
â‰¤1
16ğ‘’2Â·ğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡)âˆ’1âˆ‘ï¸
ğ‘–=1(ğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡))Â·ğ‘–
(ğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡)âˆ’ğ‘–)Â·2ğ‘–
=1
16ğ‘’2Â·ğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡)âˆ’1âˆ‘ï¸
ğ‘–=1(ğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡)âˆ’ğ‘–+ğ‘–)Â·ğ‘–
(ğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡)âˆ’ğ‘–)Â·2ğ‘–
â‰¤1
16ğ‘’2Â©Â­
Â«ğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡)âˆ’1âˆ‘ï¸
ğ‘–=1ğ‘–
2ğ‘–+ğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡)âˆ’1âˆ‘ï¸
ğ‘–=1ğ‘–2
2ğ‘–ÂªÂ®
Â¬
â‰¤1
16ğ‘’2(2+6)=1
2ğ‘’2â‰¤1. (5)
We used the infinite sum valuesÃâˆ
ğ‘–=1ğ‘–
2ğ‘–=2,Ãâˆ
ğ‘–=0ğ‘–2
2ğ‘–=6, to
bound our required finite sums in the above calculation.GECCO â€™23, July 15â€“19, 2023, Lisbon, Portugal Friedrich and KÃ¶tzing, et al.
From Equations 2 and 5, we have ğ¸[Î”ğ‘¡ğ´ğ‘¡]â‰¤10which con-
cludes the first case (when |ğ‘¥ğ‘¡|1=ğµ). Next we calculate the bound
for the drift conditioned on the event ğ´ğ‘
ğ‘¡(when|ğ‘¥ğ‘¡|1<ğµ).
ğ¸[Î”ğ‘¡ğ´ğ‘
ğ‘¡]=ğ‘›Â·ğ¸|ğ‘¥ğ‘¡|1
ğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡)+1âˆ’|ğ‘¥ğ‘¡+1|1
ğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡+1)+1ğ´ğ‘
ğ‘¡
+ğ¸[Î”ğ‘ 
ğ‘¡ğ´ğ‘
ğ‘¡]
â‰¤ğ‘›Â·ğ¸|ğ‘¥ğ‘¡|1âˆ’|ğ‘¥ğ‘¡+1|1
ğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡+1)+1ğ´ğ‘
ğ‘¡
+ğ¸[Î”ğ‘ 
ğ‘¡ğ´ğ‘
ğ‘¡].
Similar to the previous case, for this case also we start by finding a
bound forğ‘›Â·ğ¸h|ğ‘¥ğ‘¡|1âˆ’|ğ‘¥ğ‘¡+1|1
ğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡+1)+1ğ´ğ‘
ğ‘¡i
. LetÎ”1
ğ‘¡=|ğ‘¥ğ‘¡|1âˆ’|ğ‘¥ğ‘¡+1|1. Then
ğ‘›Â·ğ¸hÎ”1
ğ‘¡
ğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡+1)+1ğ´ğ‘
ğ‘¡i
=ğ‘›Â·ğ¸"
Î”1
ğ‘¡
ğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡+1)+1ğ´ğ‘
ğ‘¡,Î”1
ğ‘¡>0#
Â·ğ‘ƒğ‘Ÿ(Î”1
ğ‘¡>0)
+ğ‘›Â·ğ¸"
Î”1
ğ‘¡
ğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡+1)+1ğ´ğ‘
ğ‘¡,Î”1
ğ‘¡<0#
Â·ğ‘ƒğ‘Ÿ(Î”1
ğ‘¡<0).
Now we find upper bounds for both the quantities in the above equa-
tion. By doing calculations similar to the calculations which lead to
the Equation (2), we get ğ‘›Â·ğ¸hÎ”1
ğ‘¡
ğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡+1)+1ğ´ğ‘
ğ‘¡,Î”1
ğ‘¡>0i
â‰¤9. Since
there are at least ğ‘›âˆ’ğµnumber of 0bits, the probability to gain a 1
bit is at leastğ‘›âˆ’ğµ
ğ‘’ğ‘›. And the probability that ğ¿ğ‘‚(ğ‘¥ğ‘¡)=ğ¿ğ‘‚(ğ‘¥ğ‘¡+1)is at
least1
2ğ‘’, forğ‘›large enough. Therefore, ğ‘›Â·ğ¸hÎ”1
ğ‘¡
ğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡+1)+1Î”1
ğ‘¡<0i
Â·
ğ‘ƒğ‘Ÿ(Î”1
ğ‘¡<0)â‰¤âˆ’(ğ‘›âˆ’ğµ)
2ğ‘’2(ğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡)+1). By combining these two bounds
we have
ğ‘›Â·ğ¸"
Î”1
ğ‘¡
ğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡+1)+1ğ´ğ‘
ğ‘¡#
â‰¤9âˆ’(ğ‘›âˆ’ğµ)
2ğ‘’2(ğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡)+1).(6)
Next we calculate ğ¸[Î”ğ‘ 
ğ‘¡ğ´ğ‘
ğ‘¡], to get an upper bound for ğ¸[Î”ğ‘¡
ğ´ğ‘
ğ‘¡]. When|ğ‘¥ğ‘¡|1<ğµ, the probability to gain in LeadingOnes -value
is at most1
ğ‘›. Therefore,
ğ¸[Î”ğ‘ 
ğ‘¡ğ´ğ‘
ğ‘¡]â‰¤ğ‘›(ğ‘›âˆ’ğµ)
32ğ‘’2Â·ğ¸ï£®ï£¯ï£¯ï£¯ï£¯ï£°ğ¿ğ‘‚(ğ‘¥ğ‘¡+1)âˆ’1âˆ‘ï¸
ğ‘–=ğ¿ğ‘‚(ğ‘¥ğ‘¡)1
ğµâˆ’ğ‘–ğ´ğ‘
ğ‘¡ï£¹ï£ºï£ºï£ºï£ºï£»
â‰¤ğ‘›(ğ‘›âˆ’ğµ)
32ğ‘’2Â·ğ¸ğ¿ğ‘‚(ğ‘¥ğ‘¡+1)âˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡)
ğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡+1)+1ğ´ğ‘
ğ‘¡
â‰¤ğ‘›(ğ‘›âˆ’ğµ)
32ğ‘’2Â·1
ğ‘›Â·ğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡)âˆ’1âˆ‘ï¸
ğ‘–=1ğ‘–
(ğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡)âˆ’ğ‘–)Â·2ğ‘–âˆ’1
=ğ‘›âˆ’ğµ
32ğ‘’2(ğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡))Â·ğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡)âˆ’1âˆ‘ï¸
ğ‘–=1(ğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡))Â·ğ‘–
(ğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡)âˆ’ğ‘–)Â·2ğ‘–âˆ’1
=ğ‘›âˆ’ğµ
16ğ‘’2(ğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡))Â·ğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡)âˆ’1âˆ‘ï¸
ğ‘–=1(ğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡)âˆ’ğ‘–+ğ‘–)Â·ğ‘–
(ğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡)âˆ’ğ‘–)Â·2ğ‘–
â‰¤ğ‘›âˆ’ğµ
16ğ‘’2(ğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡))Â©Â­
Â«ğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡)âˆ’1âˆ‘ï¸
ğ‘–=1ğ‘–
2ğ‘–+ğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡)âˆ’1âˆ‘ï¸
ğ‘–=1ğ‘–2
2ğ‘–ÂªÂ®
Â¬â‰¤ğ‘›âˆ’ğµ
16ğ‘’2(ğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡))(2+6)=ğ‘›âˆ’ğµ
2ğ‘’2(ğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡)). (7)
Sinceğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡)â‰¥1, we haveğ‘›âˆ’ğµ
2ğ‘’2(ğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡))â‰¤ğ‘›âˆ’ğµ
ğ‘’2(ğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡)+1).
From Equations 6 and 7,we have
ğ¸[Î”ğ‘¡ğ´ğ‘¡]â‰¤9âˆ’(ğ‘›âˆ’ğµ)
2ğ‘’2(ğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡)+1)+ğ‘›âˆ’ğµ
2ğ‘’2(ğµâˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡))â‰¤10.
Which concludes the second case (when |ğ‘¥ğ‘¡|1<ğµ). Now we have
ğ¸[Î”ğ‘¡|ğ‘”(ğ‘¥ğ‘¡)]â‰¤ 10. Therefore, by the lower bounding additive drift
theorem [10, Theorem 5],
ğ¸[ğ‘‡]â‰¥ğ¸[ğ‘”(ğ‘¥0)]âˆ’ğ‘›ğµ
10=Î©(ğ‘›(ğ‘›âˆ’ğµ)logğµ).
â–¡
Corollary 6. Letğ‘›,ğµâˆˆN. Then the expected optimization time
of the (1+1) EA on the LeadingOnes with cardinality constraint ğµis
Î˜ ğ‘›2+ğ‘›(ğ‘›âˆ’ğµ)logğµ.
Proof. From Theorem 4 and Theorem 5 we have the required
lower bound and we have the upper bound from Theorem 2. There-
fore the expected optimization time is Î˜ ğ‘›2+ğ‘›(ğ‘›âˆ’ğµ)logğµ.â–¡
4 BETTER RUN TIMES
In this section we discuss two ways to obtain the (optimal) run time
ofğ‘‚(ğ‘›2). First, we state a corollary to the proof of Theorem 2, that
we can almost reach the bound within ğ‘‚(ğ‘›2)iterations.
Corollary 7. Letğ‘›,ğµâˆˆNandğ‘>0. Then the (1+1) EA on
LeadingOnes with the cardinality constraint ğµfinds a search point
withğµâˆ’ğ‘(ğ‘›âˆ’ğµ)leading 1s withinğ‘‚(ğ‘›2)in expectation.
With the next theorem we show that incorporating the number
of0s of a bit string as a secondary objective gives an expected run
time of the (1+1) EA of Î˜(ğ‘›2)to optimize cardinality constrained
LeadingOnes .
Theorem 8. Letğµâ‰¤ğ‘›âˆ’1and for any ğ‘¥âˆˆ{0,1}ğ‘›, let
ğ‘“(ğ‘¥)=(
(ğ¿ğ‘‚(ğ‘¥),|ğ‘¥|0) |ğ‘¥|1â‰¤ğµ,
âˆ’|ğ‘¥|1 otherwise.
Then (1+1) EA takes Î˜(ğ‘›2)in expectation to optimize ğ‘“in the lexico-
graphic order with the cardinality constraint ğµ.
Proof. For anyğ‘¥âˆˆ{0,1}ğ‘›, letğ‘”(ğ‘¥)=3ğ‘’ğ¿ğ‘‚(ğ‘¥)+|ğ‘¥|0,where|ğ‘¥|0
represents the number of 0s inğ‘¥. Intuitively, we value both progress
in decreasing the number of (unused) 1s, as well as an increase in
leading 1s, but we value an increase in leading 1s higher (since this
is the ultimate goal, and typically comes at the cost of increasing the
number of 1by a constant). Now we will show that ğ‘”(ğ‘¦)=3ğ‘’ğµ+ğ‘›âˆ’ğµ
if and only if ğ‘¦is the optimum of ğ‘“. Suppose for some ğ‘¦âˆˆ{0,1}ğ‘›,
ğ‘”(ğ‘¦)=3ğ‘’ğµ+ğ‘›âˆ’ğµ. Then 3ğ‘’ğ¿ğ‘‚(ğ‘¦)+|ğ‘¦|0=3ğ‘’ğµ+ğ‘›âˆ’ğµ, which
implies that 3ğ‘’ğ¿ğ‘‚(ğ‘¦)=3ğ‘’ğµ+ğ‘›âˆ’ğµâˆ’|ğ‘¦|0. Sinceğ¿ğ‘‚(ğ‘¦)â‰¤ğµand
|ğ‘¦|0â‰¤ğ‘›âˆ’ğ¿ğ‘‚(ğ‘¦),3ğ‘’ğ¿ğ‘‚(ğ‘¦)=3ğ‘’ğµ+ğ‘›âˆ’ğµâˆ’|ğ‘¦|0implies that ğ¿ğ‘‚(ğ‘¦)=ğµ.
Therefore,ğ‘¦is optimal.
Letğ‘‡=min{ğ‘¡â‰¥0ğ‘”(ğ‘¥ğ‘¡)â‰¥3ğ‘’ğµ+ğ‘›âˆ’ğµ}. We will examine the
drift at two different scenarios, |ğ‘¥ğ‘¡|1<ğµand|ğ‘¥ğ‘¡|1=ğµand show
that in both the cases the drift is at least 1/ğ‘›. LetÎ”ğ‘¡=ğ‘”(ğ‘¥ğ‘¡+1)âˆ’ğ‘”(ğ‘¥ğ‘¡)
andğ´ğ‘¡be the event that the left-most 0inğ‘¥ğ‘¡is flipped. ThenAnalysis of the (1+1) EA on LeadingOnes with Constraints GECCO â€™23, July 15â€“19, 2023, Lisbon, Portugal
ğ¸[Î”ğ‘¡ğ´ğ‘
ğ‘¡]â‰¥0, because, if the number of LeadingOnes does not
increase then|ğ‘¥ğ‘¡+1|0âˆ’|ğ‘¥ğ‘¡|0â‰¥0which in turn implies Î”ğ‘¡â‰¥0.
Therefore, for any 0â‰¤ğ‘¡<ğ‘‡,
ğ¸[Î”ğ‘¡|ğ‘¥ğ‘¡|1<ğµ]=ğ¸[Î”ğ‘¡ğ´ğ‘¡,|ğ‘¥ğ‘¡|1<ğµ]Â·ğ‘ƒğ‘Ÿ[ğ´ğ‘¡]
+ğ¸[Î”ğ‘¡ğ´ğ‘
ğ‘¡,|ğ‘¥ğ‘¡|1<ğµ]Â·ğ‘ƒğ‘Ÿ[ğ´ğ‘
ğ‘¡]
â‰¥1
ğ‘›Â·ğ¸[ğ‘”(ğ‘¥ğ‘¡+1)âˆ’ğ‘”(ğ‘¥ğ‘¡)ğ´ğ‘¡,|ğ‘¥ğ‘¡|1<ğµ]+0
=1
ğ‘›Â·3ğ‘’ğ¸[ğ¿ğ‘‚(ğ‘¥ğ‘¡+1)âˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡)ğ´ğ‘¡,|ğ‘¥ğ‘¡|1<ğµ]
+1
ğ‘›Â·ğ¸[|ğ‘¥ğ‘¡+1|0âˆ’|ğ‘¥ğ‘¡|0ğ´ğ‘¡,|ğ‘¥ğ‘¡|1<ğµ].
Note thatğ¸[ğ¿ğ‘‚(ğ‘¥ğ‘¡+1)âˆ’ğ¿ğ‘‚(ğ‘¥ğ‘¡)ğ´ğ‘¡,|ğ‘¥ğ‘¡|1<ğµ]is greater than
or equal to the probability of not flipping any other bits, since
it increases the number of LeadingOnes by at least one. And
ğ¸[|ğ‘¥ğ‘¡|0âˆ’|ğ‘¥ğ‘¡+1|0ğ´ğ‘¡,|ğ‘¥ğ‘¡|1<ğµ])is upper bounded by the sum
1+|ğ‘¥ğ‘¡|0âˆ’1Ã
ğ‘–=1ğ‘ƒğ‘Ÿ(flipping the ğ‘–ğ‘¡â„0bit). This is because we lose one
0bit by flipping the left-most 0bit and we flip each other 0-bit
independently with probability1
ğ‘›. And|ğ‘¥ğ‘¡|0âˆ’1
ğ‘›â‰¤1, therefore,
ğ¸[Î”ğ‘¡|ğ‘¥ğ‘¡|1<ğµ]â‰¥1
ğ‘› 
3ğ‘’
1âˆ’1
ğ‘›ğ‘›âˆ’1
âˆ’
1+|ğ‘¥ğ‘¡|0âˆ’1
ğ‘›!
â‰¥1
ğ‘›.
This concludes the first case. Now, lets consider the case |ğ‘¥ğ‘¡|1=ğµ.
Letğ·be the event that the mutation operator flips exactly one
1bit which lies after the left-most 0bit and flips no other bits.
Since|ğ‘¥ğ‘¡|1=ğµandğ¿ğ‘‚(ğ‘¥ğ‘¡)<ğµ, there is at least one such 1bit,
which implies ğ¸[|ğ‘¥ğ‘¡+1|0âˆ’|ğ‘¥ğ‘¡|0|ğ‘¥ğ‘¡|1=ğµ,ğ·]â‰¥1. Also note that
ğ‘ƒğ‘Ÿ(ğ·)â‰¥1
ğ‘’ğ‘›. If a search point is accepted, then the number of 1
bits is at most ğµand the LeadingOnes value cannot decrease; thus,
ğ¿ğ‘‚(ğ‘¥ğ‘¡+1)â‰¥ğ¿ğ‘‚(ğ‘¥ğ‘¡)and|ğ‘¥ğ‘¡+1|0â‰¥ğ‘›âˆ’ğµ. Overall we have ğ‘”(ğ‘¥ğ‘¡+1)=
3ğ‘’ğ¿ğ‘‚(ğ‘¥ğ‘¡+1)+|ğ‘¥ğ‘¡+1|0â‰¥3ğ‘’ğ¿ğ‘‚(ğ‘¥ğ‘¡)+ğ‘›âˆ’ğµ=ğ‘”(ğ‘¥ğ‘¡). Therefore, ğ¸[Î”ğ‘¡
|ğ‘¥ğ‘¡|1=ğµ,ğ·ğ‘]â‰¥0and
ğ¸[Î”ğ‘¡|ğ‘¥ğ‘¡|1=ğµ]=ğ¸[|ğ‘¥ğ‘¡+1|0âˆ’|ğ‘¥ğ‘¡|0|ğ‘¥ğ‘¡|1=ğµ,ğ·]Â·ğ‘ƒğ‘Ÿ(ğ·)
+ğ¸[Î”ğ‘¡|ğ‘¥ğ‘¡|1=ğµ,ğ·ğ‘]Â·ğ‘ƒğ‘Ÿ(ğ·ğ‘)
â‰¥1
ğ‘’ğ‘›.
The expected number of 0s in the initially selected uniform
random bit string isğ‘›
2and the expected number of LeadingOnes
is at least zero, therefore ğ¸[ğ‘”(ğ‘¥0)]â‰¥ğ‘›
2. We have an drift of at least
1
ğ‘’ğ‘›in both the cases, therefore we get the required upper bound by
the additive drift theorem [10, Theorem 5],
ğ¸[ğ‘‡]â‰¤ğ‘’ğ‘›Â·(3ğ‘’ğµ+ğ‘›âˆ’ğµâˆ’ğ¸[ğ‘”(ğ‘¥0)])â‰¤ 3ğ‘’2ğ‘›ğµ+ğ‘’ğ‘›2
2âˆ’ğ‘’ğ‘›ğµ=ğ‘‚(ğ‘›2).
This proves the upper bound. And the lower bound follows from
Theorem 4. â–¡
5 EMPIRICAL ANALYSIS
We want to extend our theoretical work on deterministic constraint
the case of stochastic constraint models (as defined in Section 2.2).
For the first model we use parameters ğœ‡=1andğœ=0.1and for
the second model we use ğœ–=âˆš
3. Note that in the second modelğ‘ˆ(ğµâˆ’âˆš
3,ğµ+âˆš
3)has variance 1. For both the models we considered
two different ğµvalues 75 and 95 (also ğµ= 85 in the Appendix). As
we will see, the (1+1) EA struggles in these settings; in order to
show that already a small parent population can remedy this, we
also consider the(10+1)EA in our experiments.
We use the following lemma for discussing certain probabilities
in this section.
Lemma 9. Letğ‘˜âˆˆ{1,Â·Â·Â·,ğµâˆ’1},ğ‘¥âˆˆ{0,1}ğ‘›,ğµâˆˆ[ğ‘›],ğ‘Šğ‘¥=Ãğ‘›
ğ‘–=1ğ‘¥ğ‘–Â·ğ‘Œğ‘–whereğ‘Œğ‘–âˆ¼ğ‘(1,ğœ2)andğ‘¥ğ‘–be theğ‘–âˆ’th bit ofğ‘¥and
|ğ‘¥|1â‰¤ğµâˆ’ğ‘˜. Thenğ‘ƒğ‘Ÿ(ğ‘Šğ‘¥>ğµ) â‰¤1âˆšğœ‹ğ‘’âˆ’ğ‘˜2
2ğ‘›2ğœ2andğ‘ƒğ‘Ÿ(ğ‘Šğ‘¥>ğµ|
|ğ‘¥|1=ğµ)=1
2.
In Figure 1 we have a single sample run of (1+1) EA on the first
model. We observe that if the (1+1) EA finds a bit string with ğµnum-
ber of 1s it violates the constraint with probability1
2(see Lemma 9)
and accepts a bit string with a lower number of LeadingOnes .
This process keeps repeating whenever the (1+1) EA encounters an
individual with a number of 1s closer toğµ.
Figure 1: (1+1) EA sample run with ğ‘›=100,ğµ=85
andğ‘(1,0.1)chance constraint for 10000 iteration.
Figure 2: (10+1) EA and (1+1) EA on LeadingOnes
withğ‘›=100,ğµ=75andğ‘(1,0.1)chance constraint
for40000 iterations.
Figures 2 and 3 are about the first model in which we have
theLeadingOnes -values of the best individual (bit string with the
maximum fitness value) in each iteration of the (10+1) EA, the
LeadingOnes values of the second-worst individuals (bit string
with the second-smallest fitness value) in each iteration of the (10+1)
EA and the LeadingOnes values at each iteration of the (1+1) EA.
Each curve is the median of thirty independent runs and the shadedGECCO â€™23, July 15â€“19, 2023, Lisbon, Portugal Friedrich and KÃ¶tzing, et al.
area is the area between the 25âˆ’th and the 75âˆ’th quantile values.
For all three ğµ-values, after initial iterations, all the individuals
except the worst individual in the (10+1) EA population have ğµâˆ’2
number of leading 1s. This is because, for this model, the probability
that an individual with ğµâˆ’2number of 1s violates the constraint
is at mostğ‘’âˆ’2âˆšğœ‹(from Lemma 9).
Figure 3: (10+1) EA and (1+1) EA on LeadingOnes
withğ‘›=100,ğµ=95andğ‘(1,0.1)chance constraint
for40000 iterations.
Figures 4 and 5 are about the second model and the curves repre-
sent the same things as in the previous figures but with respect to
the second model. In these figures we can see that the best and the
second worst individuals of the (10+1) EA are not the same because
of the changing constraint values.
Figure 4: (10+1) EA and (1+1) EA on LeadingOnes
withğ‘›=100,ğµ=75andğ‘ˆ(ğµâˆ’âˆš
3,ğµ+âˆš
3)stochastic
constraint for 40000 iterations.
Figure 5: (10+1) EA and (1+1) EA on LeadingOnes
withğ‘›=100,ğµ=95andğ‘ˆ(ğµâˆ’âˆš
3,ğµ+âˆš
3)stochastic
constraint for 40000 iterations.6 CONCLUSIONS
Understanding how evolutionary algorithms deal with constrained
problems is an important topic of research. We investigated the
classical LeadingOnes problem with additional constraints. For the
case of a deterministic uniform constraint we have carried out
a rigorous run time analysis of the (1+1) EA which gives results
on the expected optimization time in dependence of the chosen
constraint bound. Afterwards, we examined stochastic constraints
and the use of larger populations for dealing with uncertainties.
Our results show a clear benefit of using the (10+1)EA instead of
the(1+1)EA. We regard the run time analysis of population-based
algorithms for our examined settings of stochastic constraints as
an important topic for future work.
7 ACKNOWLEDGEMENTS
Frank Neumann has been supported by the Australian Research
Council (ARC) through grant FT200100536. Tobias Friedrich and
Timo KÃ¶tzing were supported by the German Research Foundation
(DFG) through grant FR 2988/17-1.
REFERENCES
[1]Hans-Georg Beyer and Bernhard Sendhoff. 2007. Robust optimizationâ€“a com-
prehensive survey. Computer methods in applied mechanics and engineering 196,
33-34 (2007), 3190â€“3218.
[2]Abraham Charnes and William W Cooper. 1959. Chance-constrained program-
ming. Management science 6, 1 (1959), 73â€“79.
[3]Benjamin Doerr and Timo KÃ¶tzing. 2021. Lower Bounds from Fitness Levels
Made Easy. In Proceedings of the Genetic and Evolutionary Computation Confer-
ence (GECCO 2021) . Association for Computing Machinery, New York, NY, USA,
1142â€“1150. https://doi.org/10.1145/3449639.3459352
[4]Benjamin Doerr and Frank Neumann (Eds.). 2020. Theory of Evolutionary
Computation - Recent Developments in Discrete Optimization . Springer. https:
//doi.org/10.1007/978-3-030-29414-4
[5]Stefan Droste, Thomas Jansen, and Ingo Wegener. 2002. On the analysis of the
(1+1) evolutionary algorithm. Theor. Comput. Sci. 276, 1-2 (2002), 51â€“81.
[6]A. E. Eiben and James E. Smith. 2015. Introduction to Evolutionary Computing,
Second Edition . Springer.
[7]Tobias Friedrich, Timo KÃ¶tzing, J. A. Gregor Lagodzinski, Frank Neumann, and
Martin Schirneck. 2020. Analysis of the (1+1) EA on subclasses of linear functions
under uniform and linear constraints. Theor. Comput. Sci. 832 (2020), 3â€“19.
[8]Tobias Friedrich, Timo KÃ¶tzing, J. A. Gregor Lagodzinski, Frank Neumann, and
Martin Schirneck. 2017. Analysis of the (1+1) EA on Subclasses of Linear Func-
tions under Uniform and Linear Constraints. In Foundations of Genetic Algorithms
(FOGA) . ACM Press, 45â€“54.
[9]Grani A Hanasusanto, Vladimir Roitch, Daniel Kuhn, and Wolfram Wiesemann.
2015. A distributionally robust perspective on uncertainty quantification and
chance constrained programming. Mathematical Programming 151 (2015), 35â€“62.
[10] Jun He and Xin Yao. 2004. A study of drift analysis for estimating computation
time of evolutionary algorithms. Natural Computing 3 (2004), 21â€“35.
[11] Thomas Jansen. 2013. Analyzing Evolutionary Algorithms - The Computer Science
Perspective . Springer. https://doi.org/10.1007/978-3-642-17339-4
[12] Pu Li, Harvey Arellano-Garcia, and GÃ¼nter Wozny. 2008. Chance constrained
programming approach to process optimization under uncertainty. Computers &
chemical engineering 32, 1-2 (2008), 25â€“45.
[13] Bruce L Miller and Harvey M Wagner. 1965. Chance constrained programming
with joint constraints. Operations Research 13, 6 (1965), 930â€“945.
[14] Rahul Nair and Elise Miller-Hooks. 2011. Fleet management for vehicle sharing
operations. Transportation Science 45, 4 (2011), 524â€“540.
[15] Aneta Neumann and Frank Neumann. 2020. Optimising Monotone Chance-
Constrained Submodular Functions Using Evolutionary Multi-objective Algo-
rithms. In PPSN (1) (Lecture Notes in Computer Science, Vol. 12269) . Springer,
404â€“417.
[16] Aneta Neumann, Yue Xie, and Frank Neumann. 2022. Evolutionary Algorithms
for Limiting the Effect of Uncertainty for the Knapsack Problem with Stochastic
Profits. In Parallel Problem Solving from Nature - PPSN XVII - 17th International
Conference, PPSN 2022, Proceedings, Part I (Lecture Notes in Computer Science,
Vol. 13398) . Springer, 294â€“307. https://doi.org/10.1007/978-3-031-14714-2_21
[17] Frank Neumann, Mojgan Pourhassan, and Carsten Witt. 2021. Improved Runtime
Results for Simple Randomised Search Heuristics on Linear Functions with aAnalysis of the (1+1) EA on LeadingOnes with Constraints GECCO â€™23, July 15â€“19, 2023, Lisbon, Portugal
Uniform Constraint. Algorithmica 83, 10 (2021), 3209â€“3237.
[18] Frank Neumann and Andrew M. Sutton. 2019. Runtime analysis of the (1 + 1)
evolutionary algorithm for the chance-constrained knapsack problem. In Proceed-
ings of the 15th ACM/SIGEVO Conference on Foundations of Genetic Algorithms,
FOGA 2019 . ACM, 147â€“153. https://doi.org/10.1145/3299904.3340315
[19] Frank Neumann and Carsten Witt. 2010. Bioinspired Computation in Combinato-
rial Optimization . Springer. https://doi.org/10.1007/978-3-642-16544-3
[20] Chao Qian, Jing-Cheng Shi, Yang Yu, and Ke Tang. 2017. On Subset Selection
with General Cost Constraints. In Proceedings of the Twenty-Sixth International
Joint Conference on Artificial Intelligence, IJCAI 2017 . ijcai.org, 2613â€“2619. https:
//doi.org/10.24963/ijcai.2017/364
[21] Chao Qian, Yang Yu, and Zhi-Hua Zhou. 2015. Subset Selection by Pareto Op-
timization. In Advances in Neural Information Processing Systems 28: Annual
Conference on Neural Information Processing Systems 2015 . 1774â€“1782.
[22] Vahid Roostapour, Aneta Neumann, and Frank Neumann. 2022. Single- and multi-
objective evolutionary algorithms for the knapsack problem with dynamically
changing constraints. Theor. Comput. Sci. 924 (2022), 129â€“147. https://doi.org/10.
1016/j.tcs.2022.05.008
[23] Vahid Roostapour, Aneta Neumann, Frank Neumann, and Tobias Friedrich. 2022.
Pareto optimization for subset selection with dynamic cost constraints. Artif.
Intell. 302 (2022), 103597. https://doi.org/10.1016/j.artint.2021.103597
[24] Feng Shi, Xiankun Yan, and Frank Neumann. 2022. Runtime Analysis of Simple
Evolutionary Algorithms for the Chance-Constrained Makespan Scheduling
Problem. In PPSN (2) (Lecture Notes in Computer Science, Vol. 13399) . Springer,
526â€“541.
[25] Eric W. Weisstein. [n. d.]. Complementary Error Function. https://mathworld.
wolfram.com/Erfc.html. Accessed: 2023-02-01.
[26] Yue Xie, Oscar Harper, Hirad Assimi, Aneta Neumann, and Frank Neumann.
2019. Evolutionary algorithms for the chance-constrained knapsack problem.
InProceedings of the Genetic and Evolutionary Computation Conference, (GECCO
2019) . ACM, 338â€“346. https://doi.org/10.1145/3321707.3321869
[27] Yue Xie, Aneta Neumann, and Frank Neumann. 2020. Specific single- and multi-
objective evolutionary algorithms for the chance-constrained knapsack problem.
InProceedings of the Genetic and Evolutionary Computation Conference, (GECCO
2020) . ACM, 271â€“279. https://doi.org/10.1145/3377930.3390162
[28] Yue Xie, Aneta Neumann, Frank Neumann, and Andrew M. Sutton. 2021. Runtime
analysis of RLS and the (1+1) EA for the chance-constrained knapsack problem
with correlated uniform weights. In Proceedings of the Genetic and Evolutionary
Computation Conference, (GECCO 2021) . ACM, 1187â€“1194.
[29] Hui Zhang and Pu Li. 2011. Chance constrained programming for optimal power
flow under uncertainty. IEEE Transactions on Power Systems 26, 4 (2011), 2417â€“
2424.
A APPENDIX
Corollary 10. Letğ‘›,ğµâˆˆNandğ‘>0. Then the (1+1) EA on
LeadingOnes with the cardinality constraint ğµfinds a search point
withğµâˆ’ğ‘(ğ‘›âˆ’ğµ)leading 1s withinğ‘‚(ğ‘›2)in expectation.
Proof. Letğ‘¥ğ‘¡be the search point at the ğ‘¡ğ‘¡â„iteration of the (1+1)
EA optimizing LeadingOnes with the cardinality constraint ğµand
ğ‘‡1=min{ğ‘¡â‰¥0ğ¿ğ‘‚(ğ‘¥ğ‘¡)â‰¥ğµâˆ’ğ‘(ğ‘›âˆ’ğµ)}. And letğ‘˜=ğµâˆ’ğ‘(ğ‘›âˆ’ğµ).
Then, from the proof of Theorem 2 we know that
ğ¸[ğ‘‡1]â‰¤ğ‘”<ğµ(ğ‘˜)â‰¤ğ‘’ğ‘›ğµ
2+(ğ‘’ğ‘›+ğ‘’2ğ‘›(ğ‘›âˆ’ğµ))
1+lnğµ
ğ‘(ğ‘›âˆ’ğµ)
.
Since lnğ‘¦â‰¤ğ‘¦for anyğ‘¦>0, we have(ğ‘›âˆ’ğµ)
ln
ğµ
ğ‘(ğ‘›âˆ’ğµ)
â‰¤ğµ
ğ‘=
ğ‘‚(ğ‘›). Therefore, ğ¸[ğ‘‡1]=ğ‘‚(ğ‘›2). â–¡
Corollary 11. Letğ‘›,ğµâˆˆNandğ‘>0. Then the (1+1) EA on
LeadingOnes with the cardinality constraint ğµfinds a search point
withğµâˆ’ğ‘(ğ‘›âˆ’ğµ)number of LeadingOnes withinğ‘‚(ğ‘›2)in expecta-
tion.
Proof. Letğ‘¥ğ‘¡be the search point at the ğ‘¡ğ‘¡â„iteration of the (1+1)
EA optimizing LeadingOnes with the cardinality constraint ğµand
ğ‘‡1=min{ğ‘¡â‰¥0ğ¿ğ‘‚(ğ‘¥ğ‘¡)=ğµâˆ’ğ‘(ğ‘›âˆ’ğµ)}. And letğ‘˜=ğµâˆ’ğ‘(ğ‘›âˆ’ğµ).Then from Theorem 2 we know that,
ğ¸[ğ‘‡1]â‰¤ğ‘”<ğµ(ğ‘˜)
=ğ‘˜âˆ‘ï¸
ğ‘–=0(ğ‘”<ğµ(ğ‘–)âˆ’ğ‘”ğµ(ğ‘–))+ğ‘˜âˆ‘ï¸
ğ‘–=1(ğ‘”ğµ(ğ‘–)âˆ’ğ‘”<ğµ(ğ‘–âˆ’1))
=ğµâˆ’ğ‘(ğ‘›âˆ’ğµ)âˆ‘ï¸
ğ‘–=0ğ‘’ğ‘›
ğµâˆ’ğ‘–+ğµâˆ’ğ‘(ğ‘›âˆ’ğµ)âˆ‘ï¸
ğ‘–=1ğ‘’ğ‘›
1+ğ‘’Â·(ğ‘›âˆ’ğµ)
ğµâˆ’ğ‘–+1
â‰¤ğ‘’ğ‘›ğµâˆ‘ï¸
ğ‘–=ğ‘(ğ‘›âˆ’ğµ)1
ğ‘–
+ğ‘’ğ‘›ğµ
2+ğ‘’2Â·ğ‘›(ğ‘›âˆ’ğµ)ğµâˆ‘ï¸
ğ‘–=ğ‘(ğ‘›âˆ’ğµ)1
ğ‘–
â‰¤ğ‘’ğ‘›ğµ
2+(ğ‘’ğ‘›+ğ‘’2ğ‘›(ğ‘›âˆ’ğµ))
1+lnğµ
ğ‘(ğ‘›âˆ’ğµ)
.
Since lnğ‘¦â‰¤ğ‘¦for anyğ‘¦>0, we have(ğ‘›âˆ’ğµ)
ln
ğµ
ğ‘(ğ‘›âˆ’ğµ)
â‰¤ğµ
ğ‘â‰¤ğ‘›.
Therefore,ğ¸[ğ‘‡1]=ğ‘‚(ğ‘›2). â–¡
Lemma 12. Letğ‘˜âˆˆ{1,Â·Â·Â·,ğµâˆ’1},ğ‘¥âˆˆ{0,1}ğ‘›,ğµâˆˆ[ğ‘›],ğ‘Šğ‘¥=Ãğ‘›
ğ‘–=1ğ‘¥ğ‘–Â·ğ‘Œğ‘–whereğ‘Œğ‘–âˆ¼ğ‘(1,ğœ2)andğ‘¥ğ‘–be theğ‘–âˆ’th bit ofğ‘¥and
|ğ‘¥|1â‰¤ğµâˆ’ğ‘˜. Thenğ‘ƒğ‘Ÿ(ğ‘Šğ‘¥>ğµ)â‰¤1âˆšğœ‹ğ‘’âˆ’ğ‘˜2
2ğ‘›2ğœ2.
Proof. First note that ğ‘Šğ‘¥is nothing but sum of |ğ‘¥|1normal ran-
dom variables with mean 1and variance ğœ2, i.e.ğ‘Šğ‘¥âˆ¼ğ‘(|ğ‘¥|1,|ğ‘¥|1Â·
ğœ2)andğ‘ƒğ‘Ÿ(ğ‘Šğ‘¥>ğµ||ğ‘¥|1=ğµ)=1
2.
ğ‘ƒğ‘Ÿ(ğ‘Šğ‘¥>ğµ)=1âˆ’ğ‘ƒğ‘Ÿ(ğ‘Šğ‘¥â‰¤ğµ)
=1âˆ’1
2
1+erfğµâˆ’|ğ‘¥|1
|ğ‘¥|1Â·ğœâˆš
2
=1
2
1âˆ’erfğµâˆ’|ğ‘¥|1
|ğ‘¥|1Â·ğœâˆš
2
=1
2erfcğµâˆ’|ğ‘¥|1
|ğ‘¥|1Â·ğœâˆš
2
.
Since complementary error function erfcis a decreasing function
and|ğ‘¥|1â‰¤ğµâˆ’ğ‘˜, we have
ğ‘ƒğ‘Ÿ(ğ‘Šğ‘¥>ğµ)=1
2erfcğµâˆ’|ğ‘¥|1
|ğ‘¥|1Â·ğœâˆš
2
(8)
â‰¤1
2erfcğ‘˜
|ğ‘¥|1Â·ğœâˆš
2
â‰¤1
2erfcğ‘˜
ğ‘›ğœâˆš
2
Sinceğ‘Ÿğ‘˜=ğ‘˜
ğ‘›ğœâˆš
2>0, we can use the upper bound for the erfcfrom
[25],
ğ‘ƒğ‘Ÿ(ğ‘Šğ‘¥>ğµ)â‰¤1
2erfcğ‘˜
ğ‘›ğœâˆš
2
â‰¤1âˆšğœ‹ğ‘’âˆ’ğ‘Ÿ2
ğ‘˜
ğ‘Ÿğ‘˜+âˆšï¸ƒ
ğ‘Ÿ2
ğ‘˜+4
ğœ‹GECCO â€™23, July 15â€“19, 2023, Lisbon, Portugal Friedrich and KÃ¶tzing, et al.
â‰¤1âˆšğœ‹ğ‘’âˆ’ğ‘˜2
2ğ‘›2ğœ2.
From Equation 8, we have ğ‘ƒğ‘Ÿ(ğ‘Šğ‘¥>ğµ||ğ‘¥|1=ğµ)=1
2erfcğµâˆ’|ğ‘¥|1
|ğ‘¥|1Â·ğœâˆš
2
which is1
2. â–¡
The following two figures are the experimental results for the
constraint value ğµ=85.
Figure 6: (10+1) EA and (1+1) EA on LeadingOnes
withğ‘›=100,ğµ=85andğ‘(1,0.1)chance constraint
for40000 iterations.
Figure 7: (10+1) EA and (1+1) EA on LeadingOnes
withğ‘›=100,ğµ=85andğ‘ˆ(ğµâˆ’âˆš
3,ğµ+âˆš
3)stochastic
constraint for 40000 iterations.