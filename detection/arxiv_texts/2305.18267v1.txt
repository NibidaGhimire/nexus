Analysis of the (1+1) EA on LeadingOnes with Constraints
Tobias Friedrich
friedrich@hpi.de
Hasso Plattner Institute
University of Potsdam
Potsdam, GermanyTimo K√∂tzing
timo.koetzing@hpi.de
Hasso Plattner Institute
University of Potsdam
Potsdam, GermanyAneta Neumann
aneta.neumann@adelaide.edu.au
Optimisation and Logistics
The University of Adelaide
Adelaide, Australia
Frank Neumann
frank.neumann@adelaide.edu.au
Optimisation and Logistics
The University of Adelaide
Adelaide, AustraliaAishwarya Radhakrishnan
aishwarya.radhakrishnan@hpi.de
Hasso Plattner Institute
University of Potsdam
Potsdam, Germany
ABSTRACT
Understanding how evolutionary algorithms perform on constrained
problems has gained increasing attention in recent years. In this
paper, we study how evolutionary algorithms optimize constrained
versions of the classical LeadingOnes problem. We first provide a
run time analysis for the classical (1+1) EA on the LeadingOnes
problem with a deterministic cardinality constraint, giving Œò(ùëõ(ùëõ‚àí
ùêµ)log(ùêµ)+ùëõ2)as the tight bound. Our results show that the be-
haviour of the algorithm is highly dependent on the constraint
bound of the uniform constraint. Afterwards, we consider the prob-
lem in the context of stochastic constraints and provide insights
using experimental studies on how the ( ùúá+1) EA is able to deal with
these constraints in a sampling-based setting.
KEYWORDS
Evolutionary algorithms, chance constraint optimization, run time
analysis, theory.
1 INTRODUCTION
Evolutionary algorithms [ 6] have been used to tackle a wide range
of combinatorial and complex engineering problems. Understand-
ing evolutionary algorithms from a theoretical perspective is crucial
to explain their success and give guidelines for their application.
The area of run time analysis has been a major contributor to the
theoretical understanding of evolutionary algorithms over the last
25 years [ 4,11,19]. Classical benchmark problems such as One-
Max and LeadingOnes have been analyzed in a very detailed way,
showing deep insights into the working behaviour of evolutionary
algorithms for these problems. In real-world settings, problems that
are optimized usually come with a set of constraints which often
limits the resources available. Studying classical benchmark prob-
lems even with an additional simple constraint such as a uniform
constraint, which limits the number of elements that can be chosen
in a given benchmark function, poses significant new technical chal-
lenges for providing run time bounds of even simple evolutionary
algorithms such as the (1+1) EA.
OneMax and the broader class of linear functions [ 5] have played
a key role in developing the area of run time analysis during the last
25 years, and run time bounds for linear functions with a uniform
constraint have been obtained [ 7,17]. It has been shown in [ 7] that
the (1+1) EA needs exponential time optimize OneMax under aspecific linear constraint which points to the additional difficulty
which such constraints impose on the search process. Tackling
constraints by taking them as additional objectives has been shown
to be quite successful for a wide range of problems. For example,
the behaviour of evolutionary multi-objective algorithms has been
analyzed for submodular optimization problems with various types
of constraints [ 20,21]. Furthermore, the performance of evolution-
ary algorithms for problems with dynamic constraints has been
investigated in [22, 23].
Another important area involving constraints is chance con-
strained optimization, which deals with stochastic components in
the constraints. Here, the presence of stochastic components in
the constraints makes it challenging to guarantee that the con-
straints are not violated at all. Chance-constrained optimization
problems [ 2,13] are an important class of the stochastic optimiza-
tion problems [ 1] that optimize a given problem under the condition
that a constraint is only violated with a small probability. Such prob-
lems occur in a wide range of areas, including finance, logistics
and engineering [ 9,12,14,29]. Recent studies of evolutionary al-
gorithms for chance-constrained problems focused on a classic
knapsack problem where the uncertainty lies in the probabilistic
constraints [ 26,27]. Here, the aim is to maximise the deterministic
profit subject to a constraint which involves stochastic weights and
where the knapsack capacity bound can only be violated with a
small probability of at most ùõº. A different stochastic version of the
knapsack problem has been studied in [ 16]. Here profits involve un-
certainties and weights are deterministic. In that work, Chebyshev
and Hoeffding-based fitness functions have been introduced and
evaluated. These fitness functions discount expected profit values
based on uncertainties of the given solutions.
Theoretical investigations for problems with chance constraints
have gained recent attention in the area of run time analysis. This
includes studies for montone submodular problems [ 15] and special
instances of makespan scheduling [24]. Furthermore, detailed run
time analyses have been carried out for specific classes of instances
for the chance constrained knapsack problem [18, 28].
1.1 Our contribution
In this paper, we investigate the behaviour of the (1+1) EA for the
classical LeadingOnes problem with additional constraints. We
first study the behaviour for the case of a uniform constraint whicharXiv:2305.18267v1  [cs.NE]  29 May 2023GECCO ‚Äô23, July 15‚Äì19, 2023, Lisbon, Portugal Friedrich and K√∂tzing, et al.
limits the number of 1-bits that can be contained in any feasible
solution. Let ùêµbe the upper bound on the number of 1-bits that
any feasible solution can have. Then the optimal solutions consists
of exactlyùêµleading 1s and afterwards only 0s. The search for the
(1+1) EA is complicated by the fact that when the current solution
consists ofùëò<ùêµleading 1s, additional 1-bits not contributing to the
fitness score at positions ùëò+2,...,ùëõ might make solutions infeasible.
We provide a detailed analysis of such scenarios in dependence of
the given bound ùêµ.
Specifically, we show a tight bound of Œò(ùëõ2+ùëõ(ùëõ‚àíùêµ)log(ùêµ))(see
Corollary 6). Note that [ 7] shows the weaker bound of ùëÇ(ùëõ2log(ùêµ)),
which, crucially, does not give insight into the actual optimization
process at the constraint. Our analysis shows in some detail how
the search progresses. In the following discussion, for the current
search point of the algorithm, we call the part of the leading 1s the
head of the bit string, the first 0thecritical bit and the remaining
bits the tail. While the size of the head is less than ùêµ‚àí(ùëõ‚àíùêµ),
optimization proceeds much like for unconstrained LeadingOnes;
this is because the bits in the tail of size about 2(ùëõ‚àíùêµ)are (almost)
uniformly distributed, contributing roughly a number of ùëõ‚àíùêµmany
1s additionally to the ùêµ‚àí(ùëõ‚àíùêµ)many 1s in the head. This stays in
sum (mostly) below the cardinality bound ùêµ, occasional violations
changing the uniform distribution of the tail to one where bits in
the tail are 1with probability a little less than 1/2(see Lemma 3).
Once the threshold of ùêµ‚àí(ùëõ‚àíùêµ)many 1s in the head is passed,
the algorithm frequently runs into the constraint. For a phase of
equal LeadingOnes value, we consider the random walk of the
number of 1s of the bit string of the algorithm. This walk has
a bias towards the bound ùêµ(its maximal value), where the bias
is light for LeadingOnes-values just a bit above ùêµ‚àí(ùëõ‚àíùêµ)and
getting stronger as this value approaches ùêµ. Since progress is easy
when not at the bound of ùêµmany 1s in the bit string (by flipping
the critical bit and no other) and difficult otherwise (additionally
to flipping the critical bit, a 1in the tail needs to flip), the exact
proportion of time that the walk spends in states of less than ùêµ
versus exactly ùêµmany 1s is very important. In the final proofs, we
estimate these factors and have corresponding potential functions
reflecting gains (1) from changing into states of less than ùêµmany
1s and (2) gaining a leading 1. Bounding these gains appropriately
lets us find asymptotically matching upper and lower bounds using
the additive drift theorem [10].
In passing we note that two different modifications of the setting
yield a better time of ùëÇ(ùëõ2). First, this time is sufficient to achieve a
LeadingOnes-values of ùêµ‚àíùëê(ùëõ‚àíùêµ)for anyùëê>0(see Corollary 7).
Second, considering the number of 1s as a secondary objective (to
be minimized) gives an optimization time of ùëÇ(ùëõ2)(see Theorem 8).
Afterwards, we turn to stochastic constraints and investigate an
experimental setting that is motivated by recent studies in the area
of chance constraints. We consider LeadingOnes with a stochastic
knapsack chance constraint, where the weights of a linear con-
straint are chosen from a given distribution. In the first setting, the
weight of each item is chosen independently according to a Normal
distribution ùëÅ(ùúá,ùúé2). A random sample of weights is feasible if
the sum of the chosen sampled weights does not exceed a given
knapsack bound ùêµ. In any iteration, all weights are resampled inde-
pendently for all evaluated individuals. Our goal is to understand
the maximal stable LeadingOnes value that the algorithm obtains.In the second setting which we study empirically, the weights are
deterministically set to 1and the bound is chosen uniformly at
random within an interval [ùêµ‚àíùúñ,ùêµ+ùúñ], whereùúñ>0specifies
the uncertainty around the constraint bound. For both settings, we
examine the performance of the (1+1)EA and(10+1)-EA for
different values of ùêµand show that a larger parent population has
a highly positive effect for these stochastic settings.
The paper is structured as follows. In Section 2, we introduce the
problems and algorithms that we study in this paper. We present our
run time analysis for the LeadingOnes problem with a deterministic
uniform constraint in Section 3. In section 4, we discuss a way to
obtain Œò(ùëõ2)bound on the run time for the same problem and
report on our empirically investigations for the stochastic settings
in Section 5. Finally, we finish with some concluding remarks. Note
that some proofs are ommitted due to space constraints.
2 PRELIMINARIES
In this section we define the objective function, constraints and the
algorithms used in our analysis. With |ùë•|1we denote the number
of1s in a bit string ùë•‚àà{0,1}ùëõ.
2.1 Cardinality Constraint
Letùëì:{0,1}ùëõ‚ÜíR,ùêµ‚â§ùëõand forùë•‚àà{0,1}ùëõ, letùë•ùëñdenote theùëñ
-th bit ofùë•. In this paper, optimizing ùëìwith cardinality constraint
ùêµmeans finding, maxùë•‚àà{0,1}ùëõùëì(ùë•)s.t√çùëõ
ùëñ=1ùë•ùëñ‚â§ùêµ.
2.2 Stochastic Constraint
Letùëì:{0,1}ùëõ‚ÜíR,ùêµ‚â§ùëõand forùë•‚àà{0,1}ùëõ, letùë•ùëñdenote theùëñ-th
bit ofùë•. In this paper we empirically analyse the following normal
stochastic constraint with uncertainty in the weights optimization
problem,
max
ùë•‚àà{0,1}ùëõùëì(ùë•)s.tùëõ‚àëÔ∏Å
ùëñ=1ùë§ùëñ¬∑ùë•ùëñ‚â§ùêµ, whereùë§ùëñ‚àºùëÅ(ùúá,ùúé2).
Letùëì:{0,1}ùëõ‚ÜíR,ùêµ‚â§ùëõand forùë•‚àà{0,1}ùëõ, letùë•ùëñdenote
theùëñ-th bit ofùë•. In this paper we also empirically analyse the
following uniform stochastic constraint with uncertainty in the
bound optimization problem,
max
ùë•‚àà{0,1}ùëõùëì(ùë•)s.t|ùë•|1‚â§ùë¶, whereùë¶‚àºùëà(ùêµ‚àíùúñ,ùêµ+ùúñ).
2.3 Objective Function
We consider the LeadingOnes function as our objective with car-
dinality and stochastic constraints for our analysis.
LeadingOnes :{0,1}ùëõ‚ÜíR, is a function which maps a bit
string of length ùëõto number of 1s before the first 0in the bit string.
For everyùë•‚àà{0,1}ùëõ,LeadingOnes(ùë•)=√çùëõ
ùëñ=1√éùëñ
ùëó=1ùë•ùëó.
2.4 (ùúá+1) EA
The (ùúá+1) EA on a real valued fitness function ùëìwith constraint ùêµ
is given in Algorithm 1. The ( ùúá+1) EA at each iteration maintains
a population of size ùúá. The initial population ùëÉ0hasùúárandom bit
strings chosen uniformly. At each iteration ùë°>0, a bit string is cho-
sen uniformly at random from ùëÉùë°followed by a mutation operation
which flips each bit of the chosen bit string with probability1
ùëõ. The
mutated bit string is added to ùëÉùë°and the bit string with the leastAnalysis of the (1+1) EA on LeadingOnes with Constraints GECCO ‚Äô23, July 15‚Äì19, 2023, Lisbon, Portugal
fitness among the ùúá+1individuals is removed. Since we can also
sample a bit string which violates the constraint, we consider the
following function for optimization.
ùëî(ùë•)=(
ùëì(ùë•), if|ùë•|1<ùêµ;
ùêµ‚àí|ùë•|,otherwise.
Algorithm 1: (ùúá+1) EA on fitness function ùëìwith con-
straint B
1ùëÉ0‚Üêùúáindividuals from{0,1}ùëõchosen u.a.r.;
2ùë°=0;
3while stopping criterion not met do
4ùë•‚Üêuniform random bit string from ùëÉùë°;
5ùë¶‚Üêflip each bit of ùë•independently with probab. 1/ùëõ;
6ùëÉùë°=ùëÉùë°‚à™{ùë¶};
7ùëÉùë°+1=ùëÉùë°\{an individual ùë•‚ààùëÉùë°with leastùëî(ùë•)value};
8ùë°=ùë°+1;
3 UNMODIFIED SETTING
In this section we give a tight analysis of the (1+1) EA on the
objective LeadingOnes with cardinality constraint ùêµ.
We start with a technical lemma which we need for our proof of
the upper bound.
Lemma 1. Forùë°‚â•0, letùë•ùë°denote the parent bit string at ùë°-th iter-
ation while (1+1) EA is optimizing LeadingOnes with the cardinality
constraint B. And for ùë°>0, letùê¥ùë°denote the event that |ùë•ùë°+1|1=ùêµ
andùêøùëÇ(ùë•ùë°+1)=ùêøùëÇ(ùë•ùë°). ThenùëÉùëü(ùê¥ùë°|ùë•ùë°|1<ùêµ)‚â§ùëõ‚àíùêµ
ùëõ.
Proof. First note that, if|ùë•ùë°|1=ùëò<ùêµandùê∂ùë°denote the event
thatùë•ùë°+1is formed by flipping ùêµ‚àíùëònumber of 0bits to 1out of
ùëõ‚àíùëò‚àí1(except the left most 0) number of 0bits, then
ùëÉùëü(ùê¥ùë°|ùë•ùë°|1<ùêµ)‚â§ùëÉùëü(ùê∂ùë°|ùë•ùë°|1<ùêµ).
The eventùê¥ùë°is a sub-event of ùê∂ùë°, since in the event ùê∂ùë°we do not
have any restriction on the bits other than ùêµ‚àíùëònumber of 0bits
out ofùëõ‚àíùëò‚àí1number of them and we have to flip at least ùêµ‚àíùëò
number of 0bits to 1to get the desired ùë•ùë°+1in the event ùê¥ùë°. Hence,
ùëÉùëü(ùê∂ùë°)=ùëõ‚àíùëò‚àí1
ùêµ‚àíùëò 1
ùëõùêµ‚àíùëò
=(ùëõ‚àíùëò‚àí1)¬∑(ùëõ‚àíùëò‚àí2)¬∑¬∑¬∑(ùëõ‚àíùêµ)
1¬∑2¬∑¬∑¬∑(ùêµ‚àíùëò)¬∑1
ùëõùêµ‚àíùëò
‚â§ùëõ‚àíùêµ
ùëõ.
The last inequality holds because, for every ùëü>0,ùëõ‚àíùëò‚àíùëü
ùëõ‚â§1.‚ñ°
In the Theorem 2 below we give an upper bound on the ex-
pected run time of the (1+1) EA on LeadingOnes with cardinality
constraintùêµ. Later we show that this bound is tight by proving a
matching lower bound.
Theorem 2. Letùëõ,ùêµ‚ààNandùêµ<ùëõ. Then the expected optimiza-
tion time of the (1+1) EA on LeadingOnes with cardinality constraint
ùêµisùëÇ ùëõ2+ùëõ(ùëõ‚àíùêµ)logùêµ.Proof. From [ 8, Lemma 3], we know that the (1+1) EA is ex-
pected to find a feasible solution within ùëÇ(ùëõlog(ùëõ/ùêµ))iterations.
Now we calculate how long it takes in expected value to find the
optimum after a feasible solution is sampled.
To do this, we construct a potential function that yields an drift
value greater than 1at each time ùë°until the optimum is found. For
ùëñ‚àà{0,¬∑¬∑¬∑,ùêµ}, letùëîùêµ(ùëñ)be the potential of a bit string ùë•‚àà{0,1}ùëõ
with exactly ùêµnumber of 1s andùêøùëÇ(ùë•)=LeadingOnes(ùë•)=ùëñ.
Forùëñ‚àà{0,¬∑¬∑¬∑,ùêµ‚àí1}, letùëî<ùêµ(ùëñ)be the potential of a bit string
ùë•‚àà{0,1}ùëõwith less than ùêµnumber of 1s andùêøùëÇ(ùë•)=ùëñ.
Letùëîùêµ(0)=0andùëî<ùêµ(0)=ùëíùëõ
ùêµ.And for every ùëñ‚àà{1,¬∑¬∑¬∑,ùêµ},
let
ùëîùêµ(ùëñ)=ùëíùëõ
1+ùëí¬∑(ùëõ‚àíùêµ)
ùêµ‚àíùëñ+1
+ùëî<ùêµ(ùëñ‚àí1),
and for every ùëñ‚àà{1,¬∑¬∑¬∑,ùêµ‚àí1}, let
ùëî<ùêµ(ùëñ)=ùëíùëõ
ùêµ‚àíùëñ+ùëîùêµ(ùëñ).
Forùë°>0, letùëãùë°be the parent bit string of (1+1) EA at iteration
ùë°. and letùëábe the iteration number at which (1+1) EA finds the
optimum for the first time. Let
ùëì(ùëãùë°)=(
ùëîùêµ(ùêøùëÇ(ùëãùë°)) if|ùëãùë°|1=ùêµ,
ùëî<ùêµ(ùêøùëÇ(ùëãùë°)if|ùëãùë°|1<ùêµ.(1)
We consider two different cases, |ùëãùë°|1=ùêµand|ùëãùë°|1<ùêµand
show in both the cases the drift is at least 1. Suppose we are in an
iterationùë°<ùëáwithùêøùëÇ(ùëãùë°)=ùëñand|ùëãùë°|1=ùêµ. Then the probability
that the number of 1s in the search point can decrease by 1in the
next iteration is at leastùêµ‚àíùëñ
ùëíùëõ. This is because we can get a desired
search point by flipping only one of the 1bits ofùêµ‚àíùëñ, excluding
the leading 1s, and not flipping any other bit. Therefore,
ùê∏[ùëì(ùëãùë°+1)‚àíùëì(ùëãùë°)ùêøùëÇ(ùëãùë°)=ùëñ,|ùëãùë°|1=ùêµ]
‚â•(ùëî<ùêµ(ùëñ)‚àíùëîùêµ(ùëñ))¬∑ùëÉùëü(|ùëãùë°+1|1<ùêµ)
‚â•ùëíùëõ
ùêµ‚àíùëñ+ùëîùêµ(ùëñ)‚àíùëîùêµ(ùëñ)
¬∑ùêµ‚àíùëñ
ùëíùëõ
=1.
Suppose we are in an iteration ùë°<ùëáwithùêøùëÇ(ùëãùë°)=ùëñand|ùëãùë°|1<ùêµ.
Then in the next iteration the value of LeadingOnes can increase
when the leftmost 0is flipped to 1as this does not violate the
constraint. This happens with probability at least1
ùëíùëõ. Since|ùëãùë°|1<
ùêµ, we can also stay in the same level (same number of leading 1s)
and the number of 1s can increase to ùêµwith probability at most
ùëõ‚àíùêµ
ùëõ(see Lemma 1). This implies that the potential can decrease by
ùëíùëõ
ùêµ‚àíùëñwith probability at mostùëõ‚àíùêµ
ùëõ.
ùê∏[ùëì(ùëãùë°+1)‚àíùëì(ùëãùë°)ùêøùëÇ(ùëãùë°)=ùëñ,|ùëãùë°|1<ùêµ]
‚â•(ùëî(ùëñ+1,ùêµ)‚àíùëî<ùêµ(ùëñ))¬∑1
ùëíùëõ‚àíùëíùëõ
ùêµ‚àíùëñ¬∑ùëõ‚àíùêµ
ùëõ
‚â•
ùëíùëõ
1+ùëí¬∑(ùëõ‚àíùêµ)
ùêµ‚àíùëñ
+ùëî<ùêµ(ùëñ)‚àíùëî<ùêµ(ùëñ)
¬∑1
ùëíùëõ
‚àíùëí¬∑(ùëõ‚àíùêµ)
ùêµ‚àíùëñ
=1.GECCO ‚Äô23, July 15‚Äì19, 2023, Lisbon, Portugal Friedrich and K√∂tzing, et al.
This results in an expected additive drift value greater than 1in all
the cases, so according to the additive drift theorem [ 10, Theorem 5],
ùê∏[ùëá]‚â§ùëì(ùëãùëá)=ùëîùêµ(ùêµ)
=ùêµ‚àí1‚àëÔ∏Å
ùëñ=0(ùëî<ùêµ(ùëñ)‚àíùëîùêµ(ùëñ))+ùêµ‚àëÔ∏Å
ùëñ=1(ùëîùêµ(ùëñ)‚àíùëî<ùêµ(ùëñ‚àí1))
=ùêµ‚àí1‚àëÔ∏Å
ùëñ=0ùëíùëõ
ùêµ‚àíùëñ+ùêµ‚àëÔ∏Å
ùëñ=1ùëíùëõ
1+ùëí¬∑(ùëõ‚àíùêµ)
ùêµ‚àíùëñ+1
=ùëíùëõùêµ‚àëÔ∏Å
ùëñ=11
ùëñ
+ùëíùëõùêµ+ùëí2¬∑ùëõ(ùëõ‚àíùêµ)ùêµ‚àëÔ∏Å
ùëñ=11
ùëñ
‚â§ùëíùëõ(logùêµ+1)+ùëíùëõùêµ+ùëí2¬∑ùëõ(ùëõ‚àíùêµ)(logùêµ+1)
=ùëÇ(ùëõ2+ùëõ(ùëõ‚àíùêµ)logùêµ).
‚ñ°
We now turn to the lower bound. When (1+1) EA optimizes
LeadingOnes in unconstrained setting the probability that a bit
which is after the left-most 0is1is exactly1
2. But this is not true in
the constrained setting. The following lemma gives an upper bound
on this probability during the cardinality constraint optimization.
Lemma 3. For anyùë°‚â•0, letùë•ùë°denote the search point at iteration
ùë°when (1+1) EA is optimizing LeadingOnes with the cardinality
constraintùêµ. Then for any ùë°‚â•0andùëñ>ùêøùëÇ(ùë•ùë°),ùëÉùëü(ùë•ùë°
ùëñ=1)‚â§1/2.
Proof. We will prove this by induction. The base case is true
because we have an uniform random bit string at ùë°=0. Lets assume
that the statement is true for ùë°, i.e. for any ùëñ>ùêøùëÇ(ùë•ùë°),ùëÉùëü(ùë•ùë°
ùëñ=
1)‚â§1/2. Letùê¥be the event that the offspring is accepted. Then,
forùëñ>ùêøùëÇ(ùë•ùë°+1),
ùëÉùëü(ùë•ùë°+1
ùëñ=1)=ùëÉùëü((ùë•ùë°
ùëñ=0)‚à©(ùëñùë°‚Ñébit is flipped)‚à©ùê¥)
+ùëÉùëü((ùë•ùë°
ùëñ=1)‚à©(ùëñùë°‚Ñébit is flipped)‚à©ùê¥ùëê)
+ùëÉùëü((ùë•ùë°
ùëñ=1)‚à©(ùëñùë°‚Ñébit is not flipped).
LetùëÉùëü(ùë•ùë°
ùëñ=1)=ùëù,ùëÉùëü(ùê¥(ùëñùë°‚Ñébit is flipped‚à©ùë•ùë°
ùëñ=0))=ùëé
andùëÉùëü(ùê¥(ùëñùë°‚Ñébit is flipped‚à©ùë•ùë°
ùëñ=1))=ùëè. Then note that ùëé‚â§
ùëè(because we have at least as many events as in probability ùëé
contributing to the probability ùëè) and by induction hypothesis,
ùëÉùëü(ùë•ùë°+1
ùëñ=1)=(1‚àíùëù)¬∑1/ùëõ¬∑ùëé+ùëù¬∑1/ùëõ¬∑(1‚àíùëè)+ùëù¬∑(1‚àí1/ùëõ)
=ùëé/ùëõ‚àí(ùëù¬∑ùëé)/ùëõ+ùëù/ùëõ‚àí(ùëù¬∑ùëè)/ùëõ+ùëù‚àíùëù/ùëõ
=ùëé/ùëõ‚àíùëù¬∑(1‚àíùëé/ùëõ‚àíùëè/ùëõ)
‚â§ùëé/ùëõ+1/2¬∑(1‚àíùëé/ùëõ‚àíùëè/ùëõ)
‚â§ùëé/ùëõ+1/2¬∑(1‚àíùëé/ùëõ‚àíùëé/ùëõ)=1/2.
‚ñ°
We use the previous lemma to prove the Œ©(ùëõ2)lower bound on
the expected time in the next theorem.
Theorem 4. Letùëõ,ùêµ‚ààN. Then the expected optimization time
of the (1+1) EA on the LeadingOnes with cardinality constraint ùêµis
Œ© ùëõ2.Proof. We use the fitness level method with visit probabilities
technique defined in [ 3, Theorem 8] to prove this lower bound.
Similar to [ 3, Theorem 11], we also partition the search space {0,1}ùëõ
based on the LeadingOnes values. For every ùëñ‚â§ùêµ, letùê¥ùëñcontain
all the bit strings with the LeadingOnes valueùëñ. If our search point
is inùê¥ùëñ, then we say that the search point is in the state ùëñ. For every
ùëñ‚àà{1,¬∑¬∑¬∑,ùêµ‚àí1}, we have to find the visit probabilities ùë£ùëñand an
upper bound for ùëùùëñ, the probability to leave the state ùëñ.
The best case scenario for the search point to leave the state ùëñ
is when the number of 1s in the search point is less than ùêµ. In this
case, we have to flip the (ùëñ+1)ùë°‚Ñébit to 1and should not flip any of
the firstùëñbits to 0. This happens with the probability1
ùëõ¬∑
1‚àí1
ùëõùëñ
.
Therefore, for every ùëñ‚àà{1,¬∑¬∑¬∑,ùêµ‚àí1},ùëùùëñ‚â§1
ùëõ¬∑
1‚àí1
ùëõùëñ
.
Next, we claim that, for each ùëñ‚àà{1,¬∑¬∑¬∑,ùêµ‚àí1},ùë£ùëñ‚Äì the proba-
bility to visit the state ùëñis at least1
2. We use [ 3, Lemma 10] to show
this. Suppose the initial search point is in a state greater than or
equal toùëñ, then the probability for it to be in state ùëñis equal to the
probability that the (ùëñ+1)ùë°‚Ñébit is 0. Since the initial bit string is
chosen uniformly at random the probability that the (ùëñ+1)ùë°‚Ñébit is 0
is1
2. This shows the first required bound on the probability for the
lemma in [ 3, Lemma 10]. Suppose the search point is transitioning
into a level greater than or equal to ùëñ, then the probability that it
transition into state ùëñis equal to the probability that (ùëñ+1)ùë°‚Ñébit is
0. From Lemma 3, we know that this probability is at least 1/2. This
gives the second bound required for the [ 3, Lemma 10], therefore
ùë£ùëñis at least1
2.
By using fitness level method with visit probabilities theorem
[3, Theorem 8], if ùëáis the time taken by the (1+1) EA to find an
individual with ùêµnumber of LeadingOnes for the first time then,
we have,ùê∏[ùëá]‚â•√çùêµ‚àí1
ùëñ=0ùë£ùëñ
ùëùùëñ=ùëõ
2¬∑ùêµ‚àí1‚àëÔ∏Å
ùëñ=0
1‚àí1
ùëõ‚àíùëñ
‚â•ùëõ2
2=Œ©(ùëõ2).‚ñ°
We aim to show the Œ©(ùëõ2+ùëõ(ùëõ‚àíùêµ)logùêµ)lower bound and
Theorem 4 gives the Œ©(ùëõ2)lower bound. Therefore, next we con-
sider the case where ùêµis such thatùëõ(ùëõ‚àíùêµ)logùêµ‚â†ùëÇ(ùëõ2)to prove
the desired lower bound.
Theorem 5. Letùëõ,ùêµ‚ààNand suppose ùëõ(ùëõ‚àíùêµ)logùêµ=ùúî(ùëõ2).
Then the expected optimization time of the (1+1) EA on the objective
LeadingOnes with cardinality constraint ùêµisŒ©(ùëõ(ùëõ‚àíùêµ)logùêµ).
Proof. We consider the potential function ùëîsuch that, for all
ùë•‚àà{0,1}ùëõ,
ùëî(ùë•)=ùëõ¬∑|ùë•|1
ùêµ‚àíùêøùëÇ(ùë•)+1+ùêµ‚àí1‚àëÔ∏Å
ùëñ=ùêøùëÇ(ùë•)ùëõ(ùëõ‚àíùêµ)
32ùëí2(ùêµ‚àíùëñ).
The first term appreciates progress by reducing the number of 1s.
This is scaled to later derive constant drift in expectation from
such a reduction whenever |ùë•|1=ùêµ, the case where progress by
increasing the number of leading 1s is not easy. The second term
appreciates progress by increasing the number of leading 1s, scaled
to derive constant drift in case of |ùë•|1<ùêµ.
The idea of the proof is as follows. We show that the potential
decreases by at most 10in expectation. Then the lower bound of
additive drift theorem will give the desired lower bound on the
expected run time (see [10, Theorem 5]).Analysis of the (1+1) EA on LeadingOnes with Constraints GECCO ‚Äô23, July 15‚Äì19, 2023, Lisbon, Portugal
We start by calculating the expected potential at ùë°=0. Since the
initial bit string is chosen uniformly at random the probability that
the first bit is 0is1
2. ThereforeùëÉùëü(ùêøùëÇ(ùë•0)=0)=1
2, which implies
ùê∏[ùëî(ùë•0)]‚â•1
2¬∑ùê∏Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞ùêµ‚àí1‚àëÔ∏Å
ùëñ=ùêøùëÇ(ùë•0)ùëõ(ùëõ‚àíùêµ)
32ùëí2(ùêµ‚àíùëñ)ùêøùëÇ(ùë•0)=0Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£ª
=ùëõ(ùëõ‚àíùêµ)
64ùëí2ùêµ‚àí1‚àëÔ∏Å
ùëñ=01
ùêµ‚àíùëñ
=ùëõ(ùëõ‚àíùêµ)
64ùëí2¬∑ùêµ‚àëÔ∏Å
ùëñ=11
ùëñ‚â•ùëõ(ùëõ‚àíùêµ)ln(ùêµ)
64ùëí2.
Therefore, there exits a constant ùëê>0such thatùê∏[ùëî(ùë•0)]‚â•ùëêùëõ(ùëõ‚àí
ùêµ)logùêµ.The optimum has a potential value of ùëõùêµ; thus, we can
find a lower bound on the optimization time by considering the
time to find a potential value of at most ùëõùêµ. Letùëá=min{ùë°‚â•0|
ùëî(ùë•ùë°)‚â§ùëõùêµ}. Note thatùëámay not be the time at which we find
the optimum for the first time. From ùëõ(ùëõ‚àíùêµ)logùêµ=ùúî(ùëõ2)we
get, forùëõlarge enough, that ùê∏[ùëî(ùë•0)]>ùëõùêµ, which implies that the
expected optimization time is at least ùê∏[ùëá].
In order to show the lower bound on the drift, we consider two
different cases,|ùë•ùë°|1=ùêµand|ùë•ùë°|1<ùêµand show in both the cases
drift is at most 10. First, we examine the case where the algorithm
has currently ùêµnumber of 1s. For anyùë°, letùê¥ùë°be the event that
|ùë•ùë°|1=ùêµand let Œîùë°=ùëî(ùë•ùë°)‚àíùëî(ùë•ùë°+1)and
Œîùë†
ùë°=ùêµ‚àí1‚àëÔ∏Å
ùëñ=ùêøùëÇ(ùë•ùë°)ùëõ(ùëõ‚àíùêµ)
32ùëí2(ùêµ‚àíùëñ)‚àíùêµ‚àí1‚àëÔ∏Å
ùëñ=ùêøùëÇ(ùë•ùë°+1)ùëõ(ùëõ‚àíùêµ)
32ùëí2(ùêµ‚àíùëñ)
=ùêøùëÇ(ùë•ùë°+1)‚àí1‚àëÔ∏Å
ùëñ=ùêøùëÇ(ùë•ùë°)ùëõ(ùëõ‚àíùêµ)
32ùëí2(ùêµ‚àíùëñ).
Then,ùê∏[Œîùë°ùê¥ùë°]=ùëõ¬∑ùê∏|ùë•ùë°|1
ùêµ‚àíùêøùëÇ(ùë•ùë°)+1‚àí|ùë•ùë°+1|1
ùêµ‚àíùêøùëÇ(ùë•ùë°+1)+1ùê¥ùë°
+ùê∏[Œîùë†
ùë°ùê¥ùë°]
‚â§ùëõ¬∑ùê∏|ùë•ùë°|1‚àí|ùë•ùë°+1|1
ùêµ‚àíùêøùëÇ(ùë•ùë°+1)+1ùê¥ùë°
+ùê∏[Œîùë†
ùë°ùê¥ùë°].
Now we calculate the bounds for all the required expectations in
the above equation.
First we calculate a bound for ùëõ¬∑ùê∏h|ùë•ùë°|1‚àí|ùë•ùë°+1|1
ùêµ‚àíùêøùëÇ(ùë•ùë°+1)+1ùê¥ùë°i
by using
the definition of the expectation. Let ùêº={0,¬∑¬∑¬∑,ùêµ‚àíùêøùëÇ(ùë•ùë°)}and
ùêΩ={‚àí1,0,¬∑¬∑¬∑,ùêµ‚àíùêøùëÇ(ùë•ùë°+1)‚àí1}. Then the possible values the
random variable|ùë•ùë°|1‚àí|ùë•ùë°+1|1can have are the values in ùêº. And the
possible values ùêµ‚àíùêøùëÇ(ùë•ùë°+1)+1can have are{ùêµ‚àíùêøùëÇ(ùë•ùë°)‚àíùëó|ùëó‚ààùêΩ}.
Forùëñ‚àà{1,¬∑¬∑¬∑,ùêµ‚àíùêøùëÇ(ùë•ùë°)}, the probability ùëÉùëü((|ùë•ùë°|1‚àí|ùë•ùë°+1|1=ùëñ)‚à©
(ùêµ‚àíùêøùëÇ(ùë•ùë°+1)+1=ùêµ‚àíùêøùëÇ(ùë•ùë°)+1))‚â§ ùêµ‚àíùêøùëÇ(ùë•ùë°)
ùëñ
1
ùëõùëñ
‚â§ùêµ‚àíùêøùëÇ(ùë•ùë°)
ùëñ!ùëõ
and forùëñ‚àà{1,¬∑¬∑¬∑,ùêµ‚àíùêøùëÇ(ùë•ùë°)}andùëó‚àà{0,¬∑¬∑¬∑,ùêµ‚àíùêøùëÇ(ùë•ùë°+1)‚àí1}},
the probability ùëÉùëü((|ùë•ùë°|1‚àí|ùë•ùë°+1|1=ùëñ)‚à©(ùêµ‚àíùêøùëÇ(ùë•ùë°+1)+1=ùêµ‚àí
ùêøùëÇ(ùë•ùë°)‚àíùëó))‚â§ ùêµ‚àíùêøùëÇ(ùë•ùë°)
ùëñ
1
ùëõùëñ+11
2ùëó‚â§ùêµ‚àíùêøùëÇ(ùë•ùë°)
ùëñ!ùëõ21
2ùëó(see Lemma
3). Forùëñ‚ààùêºandùëó‚ààùêΩ, letùëùùëñùëó
ùë°=ùëÉùëü((|ùë•ùë°|1‚àí|ùë•ùë°+1|1=ùëñ)‚à©(ùêµ‚àí
ùêøùëÇ(ùë•ùë°+1)+1=ùêµ‚àíùêøùëÇ(ùë•ùë°)‚àíùëó))andùêæ=ùêΩ\{ùêµ‚àíùêøùëÇ(ùë•ùë°)+1}and
ùêæùëê={ùêµ‚àíùêøùëÇ(ùë•ùë°)+1}. Then,ùëõ¬∑ùê∏h|ùë•ùë°|1‚àí|ùë•ùë°+1|1
ùêµ‚àíùêøùëÇ(ùë•ùë°+1)+1ùê¥ùë°i=ùëõ¬∑‚àëÔ∏Å
ùëñ‚ààùêº‚àëÔ∏Å
ùëó‚ààùêΩùëñ¬∑ùëùùëñùëó
ùë°
ùêµ‚àíùêøùëÇ(ùë•ùë°)‚àíùëó
=ùëõ¬∑‚àëÔ∏Å
ùëñ‚ààùêº‚àëÔ∏Å
ùëó‚ààùêæùëêùëñ¬∑ùëùùëñùëó
ùë°
ùêµ‚àíùêøùëÇ(ùë•ùë°)‚àíùëó+ùëõ¬∑‚àëÔ∏Å
ùëñ‚ààùêº‚àëÔ∏Å
ùëó‚ààùêæùëñ¬∑ùëùùëñùëó
ùë°
ùêµ‚àíùêøùëÇ(ùë•ùë°)‚àíùëó
=‚àëÔ∏Å
ùëñ‚ààùêºùëñ(ùêµ‚àíùêøùëÇ(ùë•ùë°))
ùëñ!(ùêµ‚àíùêøùëÇ(ùë•ùë°)+1)+‚àëÔ∏Å
ùëñ‚ààùêºùëñ(ùêµ‚àíùêøùëÇ(ùë•ùë°))
ùëñ!ùëõ‚àëÔ∏Å
ùëó‚ààùêæ1
2ùëó
ùêµ‚àíùêøùëÇ(ùë•ùë°)‚àíùëó
‚â§‚àëÔ∏Å
ùëñ‚ààùêº\{0}1
(ùëñ‚àí1)!+‚àëÔ∏Å
ùëñ‚ààùêº1
(ùëñ‚àí1)!‚àëÔ∏Å
ùëó‚ààùêæ1
2ùëó
‚â§ùëí+2ùëí=3ùëí‚â§9. (2)
We used the infinite sum values√ç‚àû
ùëñ=11
(ùëñ‚àí1)!=ùëí,√ç‚àû
ùëñ=01
2ùëñ=2, to
bound our required finite sums in the above calculation.
Now, we calculate ùê∏[Œîùë†
ùë°ùê¥ùë°], to get an upper bound for ùê∏[Œîùë°
ùê¥ùë°]. When|ùë•ùë°|1=ùêµ, the probability to gain in the LeadingOnes -
values is at mostùêµ‚àíùêøùëÇ(ùë•ùë°)
ùëõ¬∑1
ùëõ. Therefore we have
ùê∏[Œîùë†
ùë°ùê¥ùë°]=ùëõ(ùëõ‚àíùêµ)
32ùëí2¬∑ùê∏Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞ùêøùëÇ(ùë•ùë°+1)‚àí1‚àëÔ∏Å
ùëñ=ùêøùëÇ(ùë•ùë°)1
ùêµ‚àíùëñùê¥ùë°Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£ª
‚â§ùëõ(ùëõ‚àíùêµ)
32ùëí2¬∑ùê∏ùêøùëÇ(ùë•ùë°+1)‚àíùêøùëÇ(ùë•ùë°)
ùêµ‚àíùêøùëÇ(ùë•ùë°+1)+1ùê¥ùë°
.
(3)
We calculate an upper bound for ùê∏hùêøùëÇ(ùë•ùë°+1)‚àíùêøùëÇ(ùë•ùë°)
ùêµ‚àíùêøùëÇ(ùë•ùë°+1)+1ùê¥ùë°i
. The
probability that ùêøùëÇ(ùë•ùë°+1)‚àíùêøùëÇ(ùë•ùë°)=ùëñgiven that we gain at least
a leading one is the probability that next ùëñ‚àí1bits after left-most 0
bit) is 1followed by a 0bit. This implies that the probability that
ùêøùëÇ(ùë•ùë°+1)‚àíùêøùëÇ(ùë•ùë°)=ùëñgiven that we gain at least a leading one is
at most1
2ùëñ‚àí1. Therefore, we have ùê∏hùêøùëÇ(ùë•ùë°+1)‚àíùêøùëÇ(ùë•ùë°)
ùêµ‚àíùêøùëÇ(ùë•ùë°+1)+1ùê¥ùë°i
‚â§ùêµ‚àíùêøùëÇ(ùë•ùë°)
ùëõ¬∑1
ùëõ¬∑ùêµ‚àíùêøùëÇ(ùë•ùë°)‚àí1‚àëÔ∏Å
ùëñ=1ùëñ¬∑21‚àíùëñ
ùêµ‚àíùêøùëÇ(ùë•ùë°)‚àíùëñ. (4)
Equations 3 and 4 imply that, ùê∏[Œîùë†
ùë°ùê¥ùë°]
‚â§ùëõ(ùëõ‚àíùêµ)
32ùëí2¬∑ùêµ‚àíùêøùëÇ(ùë•ùë°)
ùëõ¬∑1
ùëõ¬∑ùêµ‚àíùêøùëÇ(ùë•ùë°)‚àí1‚àëÔ∏Å
ùëñ=1ùëñ¬∑21‚àíùëñ
ùêµ‚àíùêøùëÇ(ùë•ùë°)‚àíùëñ
‚â§1
16ùëí2¬∑ùêµ‚àíùêøùëÇ(ùë•ùë°)‚àí1‚àëÔ∏Å
ùëñ=1(ùêµ‚àíùêøùëÇ(ùë•ùë°))¬∑ùëñ
(ùêµ‚àíùêøùëÇ(ùë•ùë°)‚àíùëñ)¬∑2ùëñ
=1
16ùëí2¬∑ùêµ‚àíùêøùëÇ(ùë•ùë°)‚àí1‚àëÔ∏Å
ùëñ=1(ùêµ‚àíùêøùëÇ(ùë•ùë°)‚àíùëñ+ùëñ)¬∑ùëñ
(ùêµ‚àíùêøùëÇ(ùë•ùë°)‚àíùëñ)¬∑2ùëñ
‚â§1
16ùëí2¬©¬≠
¬´ùêµ‚àíùêøùëÇ(ùë•ùë°)‚àí1‚àëÔ∏Å
ùëñ=1ùëñ
2ùëñ+ùêµ‚àíùêøùëÇ(ùë•ùë°)‚àí1‚àëÔ∏Å
ùëñ=1ùëñ2
2ùëñ¬™¬Æ
¬¨
‚â§1
16ùëí2(2+6)=1
2ùëí2‚â§1. (5)
We used the infinite sum values√ç‚àû
ùëñ=1ùëñ
2ùëñ=2,√ç‚àû
ùëñ=0ùëñ2
2ùëñ=6, to
bound our required finite sums in the above calculation.GECCO ‚Äô23, July 15‚Äì19, 2023, Lisbon, Portugal Friedrich and K√∂tzing, et al.
From Equations 2 and 5, we have ùê∏[Œîùë°ùê¥ùë°]‚â§10which con-
cludes the first case (when |ùë•ùë°|1=ùêµ). Next we calculate the bound
for the drift conditioned on the event ùê¥ùëê
ùë°(when|ùë•ùë°|1<ùêµ).
ùê∏[Œîùë°ùê¥ùëê
ùë°]=ùëõ¬∑ùê∏|ùë•ùë°|1
ùêµ‚àíùêøùëÇ(ùë•ùë°)+1‚àí|ùë•ùë°+1|1
ùêµ‚àíùêøùëÇ(ùë•ùë°+1)+1ùê¥ùëê
ùë°
+ùê∏[Œîùë†
ùë°ùê¥ùëê
ùë°]
‚â§ùëõ¬∑ùê∏|ùë•ùë°|1‚àí|ùë•ùë°+1|1
ùêµ‚àíùêøùëÇ(ùë•ùë°+1)+1ùê¥ùëê
ùë°
+ùê∏[Œîùë†
ùë°ùê¥ùëê
ùë°].
Similar to the previous case, for this case also we start by finding a
bound forùëõ¬∑ùê∏h|ùë•ùë°|1‚àí|ùë•ùë°+1|1
ùêµ‚àíùêøùëÇ(ùë•ùë°+1)+1ùê¥ùëê
ùë°i
. LetŒî1
ùë°=|ùë•ùë°|1‚àí|ùë•ùë°+1|1. Then
ùëõ¬∑ùê∏hŒî1
ùë°
ùêµ‚àíùêøùëÇ(ùë•ùë°+1)+1ùê¥ùëê
ùë°i
=ùëõ¬∑ùê∏"
Œî1
ùë°
ùêµ‚àíùêøùëÇ(ùë•ùë°+1)+1ùê¥ùëê
ùë°,Œî1
ùë°>0#
¬∑ùëÉùëü(Œî1
ùë°>0)
+ùëõ¬∑ùê∏"
Œî1
ùë°
ùêµ‚àíùêøùëÇ(ùë•ùë°+1)+1ùê¥ùëê
ùë°,Œî1
ùë°<0#
¬∑ùëÉùëü(Œî1
ùë°<0).
Now we find upper bounds for both the quantities in the above equa-
tion. By doing calculations similar to the calculations which lead to
the Equation (2), we get ùëõ¬∑ùê∏hŒî1
ùë°
ùêµ‚àíùêøùëÇ(ùë•ùë°+1)+1ùê¥ùëê
ùë°,Œî1
ùë°>0i
‚â§9. Since
there are at least ùëõ‚àíùêµnumber of 0bits, the probability to gain a 1
bit is at leastùëõ‚àíùêµ
ùëíùëõ. And the probability that ùêøùëÇ(ùë•ùë°)=ùêøùëÇ(ùë•ùë°+1)is at
least1
2ùëí, forùëõlarge enough. Therefore, ùëõ¬∑ùê∏hŒî1
ùë°
ùêµ‚àíùêøùëÇ(ùë•ùë°+1)+1Œî1
ùë°<0i
¬∑
ùëÉùëü(Œî1
ùë°<0)‚â§‚àí(ùëõ‚àíùêµ)
2ùëí2(ùêµ‚àíùêøùëÇ(ùë•ùë°)+1). By combining these two bounds
we have
ùëõ¬∑ùê∏"
Œî1
ùë°
ùêµ‚àíùêøùëÇ(ùë•ùë°+1)+1ùê¥ùëê
ùë°#
‚â§9‚àí(ùëõ‚àíùêµ)
2ùëí2(ùêµ‚àíùêøùëÇ(ùë•ùë°)+1).(6)
Next we calculate ùê∏[Œîùë†
ùë°ùê¥ùëê
ùë°], to get an upper bound for ùê∏[Œîùë°
ùê¥ùëê
ùë°]. When|ùë•ùë°|1<ùêµ, the probability to gain in LeadingOnes -value
is at most1
ùëõ. Therefore,
ùê∏[Œîùë†
ùë°ùê¥ùëê
ùë°]‚â§ùëõ(ùëõ‚àíùêµ)
32ùëí2¬∑ùê∏Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞ùêøùëÇ(ùë•ùë°+1)‚àí1‚àëÔ∏Å
ùëñ=ùêøùëÇ(ùë•ùë°)1
ùêµ‚àíùëñùê¥ùëê
ùë°Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£ª
‚â§ùëõ(ùëõ‚àíùêµ)
32ùëí2¬∑ùê∏ùêøùëÇ(ùë•ùë°+1)‚àíùêøùëÇ(ùë•ùë°)
ùêµ‚àíùêøùëÇ(ùë•ùë°+1)+1ùê¥ùëê
ùë°
‚â§ùëõ(ùëõ‚àíùêµ)
32ùëí2¬∑1
ùëõ¬∑ùêµ‚àíùêøùëÇ(ùë•ùë°)‚àí1‚àëÔ∏Å
ùëñ=1ùëñ
(ùêµ‚àíùêøùëÇ(ùë•ùë°)‚àíùëñ)¬∑2ùëñ‚àí1
=ùëõ‚àíùêµ
32ùëí2(ùêµ‚àíùêøùëÇ(ùë•ùë°))¬∑ùêµ‚àíùêøùëÇ(ùë•ùë°)‚àí1‚àëÔ∏Å
ùëñ=1(ùêµ‚àíùêøùëÇ(ùë•ùë°))¬∑ùëñ
(ùêµ‚àíùêøùëÇ(ùë•ùë°)‚àíùëñ)¬∑2ùëñ‚àí1
=ùëõ‚àíùêµ
16ùëí2(ùêµ‚àíùêøùëÇ(ùë•ùë°))¬∑ùêµ‚àíùêøùëÇ(ùë•ùë°)‚àí1‚àëÔ∏Å
ùëñ=1(ùêµ‚àíùêøùëÇ(ùë•ùë°)‚àíùëñ+ùëñ)¬∑ùëñ
(ùêµ‚àíùêøùëÇ(ùë•ùë°)‚àíùëñ)¬∑2ùëñ
‚â§ùëõ‚àíùêµ
16ùëí2(ùêµ‚àíùêøùëÇ(ùë•ùë°))¬©¬≠
¬´ùêµ‚àíùêøùëÇ(ùë•ùë°)‚àí1‚àëÔ∏Å
ùëñ=1ùëñ
2ùëñ+ùêµ‚àíùêøùëÇ(ùë•ùë°)‚àí1‚àëÔ∏Å
ùëñ=1ùëñ2
2ùëñ¬™¬Æ
¬¨‚â§ùëõ‚àíùêµ
16ùëí2(ùêµ‚àíùêøùëÇ(ùë•ùë°))(2+6)=ùëõ‚àíùêµ
2ùëí2(ùêµ‚àíùêøùëÇ(ùë•ùë°)). (7)
Sinceùêµ‚àíùêøùëÇ(ùë•ùë°)‚â•1, we haveùëõ‚àíùêµ
2ùëí2(ùêµ‚àíùêøùëÇ(ùë•ùë°))‚â§ùëõ‚àíùêµ
ùëí2(ùêµ‚àíùêøùëÇ(ùë•ùë°)+1).
From Equations 6 and 7,we have
ùê∏[Œîùë°ùê¥ùë°]‚â§9‚àí(ùëõ‚àíùêµ)
2ùëí2(ùêµ‚àíùêøùëÇ(ùë•ùë°)+1)+ùëõ‚àíùêµ
2ùëí2(ùêµ‚àíùêøùëÇ(ùë•ùë°))‚â§10.
Which concludes the second case (when |ùë•ùë°|1<ùêµ). Now we have
ùê∏[Œîùë°|ùëî(ùë•ùë°)]‚â§ 10. Therefore, by the lower bounding additive drift
theorem [10, Theorem 5],
ùê∏[ùëá]‚â•ùê∏[ùëî(ùë•0)]‚àíùëõùêµ
10=Œ©(ùëõ(ùëõ‚àíùêµ)logùêµ).
‚ñ°
Corollary 6. Letùëõ,ùêµ‚ààN. Then the expected optimization time
of the (1+1) EA on the LeadingOnes with cardinality constraint ùêµis
Œò ùëõ2+ùëõ(ùëõ‚àíùêµ)logùêµ.
Proof. From Theorem 4 and Theorem 5 we have the required
lower bound and we have the upper bound from Theorem 2. There-
fore the expected optimization time is Œò ùëõ2+ùëõ(ùëõ‚àíùêµ)logùêµ.‚ñ°
4 BETTER RUN TIMES
In this section we discuss two ways to obtain the (optimal) run time
ofùëÇ(ùëõ2). First, we state a corollary to the proof of Theorem 2, that
we can almost reach the bound within ùëÇ(ùëõ2)iterations.
Corollary 7. Letùëõ,ùêµ‚ààNandùëê>0. Then the (1+1) EA on
LeadingOnes with the cardinality constraint ùêµfinds a search point
withùêµ‚àíùëê(ùëõ‚àíùêµ)leading 1s withinùëÇ(ùëõ2)in expectation.
With the next theorem we show that incorporating the number
of0s of a bit string as a secondary objective gives an expected run
time of the (1+1) EA of Œò(ùëõ2)to optimize cardinality constrained
LeadingOnes .
Theorem 8. Letùêµ‚â§ùëõ‚àí1and for any ùë•‚àà{0,1}ùëõ, let
ùëì(ùë•)=(
(ùêøùëÇ(ùë•),|ùë•|0) |ùë•|1‚â§ùêµ,
‚àí|ùë•|1 otherwise.
Then (1+1) EA takes Œò(ùëõ2)in expectation to optimize ùëìin the lexico-
graphic order with the cardinality constraint ùêµ.
Proof. For anyùë•‚àà{0,1}ùëõ, letùëî(ùë•)=3ùëíùêøùëÇ(ùë•)+|ùë•|0,where|ùë•|0
represents the number of 0s inùë•. Intuitively, we value both progress
in decreasing the number of (unused) 1s, as well as an increase in
leading 1s, but we value an increase in leading 1s higher (since this
is the ultimate goal, and typically comes at the cost of increasing the
number of 1by a constant). Now we will show that ùëî(ùë¶)=3ùëíùêµ+ùëõ‚àíùêµ
if and only if ùë¶is the optimum of ùëì. Suppose for some ùë¶‚àà{0,1}ùëõ,
ùëî(ùë¶)=3ùëíùêµ+ùëõ‚àíùêµ. Then 3ùëíùêøùëÇ(ùë¶)+|ùë¶|0=3ùëíùêµ+ùëõ‚àíùêµ, which
implies that 3ùëíùêøùëÇ(ùë¶)=3ùëíùêµ+ùëõ‚àíùêµ‚àí|ùë¶|0. SinceùêøùëÇ(ùë¶)‚â§ùêµand
|ùë¶|0‚â§ùëõ‚àíùêøùëÇ(ùë¶),3ùëíùêøùëÇ(ùë¶)=3ùëíùêµ+ùëõ‚àíùêµ‚àí|ùë¶|0implies that ùêøùëÇ(ùë¶)=ùêµ.
Therefore,ùë¶is optimal.
Letùëá=min{ùë°‚â•0ùëî(ùë•ùë°)‚â•3ùëíùêµ+ùëõ‚àíùêµ}. We will examine the
drift at two different scenarios, |ùë•ùë°|1<ùêµand|ùë•ùë°|1=ùêµand show
that in both the cases the drift is at least 1/ùëõ. LetŒîùë°=ùëî(ùë•ùë°+1)‚àíùëî(ùë•ùë°)
andùê¥ùë°be the event that the left-most 0inùë•ùë°is flipped. ThenAnalysis of the (1+1) EA on LeadingOnes with Constraints GECCO ‚Äô23, July 15‚Äì19, 2023, Lisbon, Portugal
ùê∏[Œîùë°ùê¥ùëê
ùë°]‚â•0, because, if the number of LeadingOnes does not
increase then|ùë•ùë°+1|0‚àí|ùë•ùë°|0‚â•0which in turn implies Œîùë°‚â•0.
Therefore, for any 0‚â§ùë°<ùëá,
ùê∏[Œîùë°|ùë•ùë°|1<ùêµ]=ùê∏[Œîùë°ùê¥ùë°,|ùë•ùë°|1<ùêµ]¬∑ùëÉùëü[ùê¥ùë°]
+ùê∏[Œîùë°ùê¥ùëê
ùë°,|ùë•ùë°|1<ùêµ]¬∑ùëÉùëü[ùê¥ùëê
ùë°]
‚â•1
ùëõ¬∑ùê∏[ùëî(ùë•ùë°+1)‚àíùëî(ùë•ùë°)ùê¥ùë°,|ùë•ùë°|1<ùêµ]+0
=1
ùëõ¬∑3ùëíùê∏[ùêøùëÇ(ùë•ùë°+1)‚àíùêøùëÇ(ùë•ùë°)ùê¥ùë°,|ùë•ùë°|1<ùêµ]
+1
ùëõ¬∑ùê∏[|ùë•ùë°+1|0‚àí|ùë•ùë°|0ùê¥ùë°,|ùë•ùë°|1<ùêµ].
Note thatùê∏[ùêøùëÇ(ùë•ùë°+1)‚àíùêøùëÇ(ùë•ùë°)ùê¥ùë°,|ùë•ùë°|1<ùêµ]is greater than
or equal to the probability of not flipping any other bits, since
it increases the number of LeadingOnes by at least one. And
ùê∏[|ùë•ùë°|0‚àí|ùë•ùë°+1|0ùê¥ùë°,|ùë•ùë°|1<ùêµ])is upper bounded by the sum
1+|ùë•ùë°|0‚àí1√ç
ùëñ=1ùëÉùëü(flipping the ùëñùë°‚Ñé0bit). This is because we lose one
0bit by flipping the left-most 0bit and we flip each other 0-bit
independently with probability1
ùëõ. And|ùë•ùë°|0‚àí1
ùëõ‚â§1, therefore,
ùê∏[Œîùë°|ùë•ùë°|1<ùêµ]‚â•1
ùëõ 
3ùëí
1‚àí1
ùëõùëõ‚àí1
‚àí
1+|ùë•ùë°|0‚àí1
ùëõ!
‚â•1
ùëõ.
This concludes the first case. Now, lets consider the case |ùë•ùë°|1=ùêµ.
Letùê∑be the event that the mutation operator flips exactly one
1bit which lies after the left-most 0bit and flips no other bits.
Since|ùë•ùë°|1=ùêµandùêøùëÇ(ùë•ùë°)<ùêµ, there is at least one such 1bit,
which implies ùê∏[|ùë•ùë°+1|0‚àí|ùë•ùë°|0|ùë•ùë°|1=ùêµ,ùê∑]‚â•1. Also note that
ùëÉùëü(ùê∑)‚â•1
ùëíùëõ. If a search point is accepted, then the number of 1
bits is at most ùêµand the LeadingOnes value cannot decrease; thus,
ùêøùëÇ(ùë•ùë°+1)‚â•ùêøùëÇ(ùë•ùë°)and|ùë•ùë°+1|0‚â•ùëõ‚àíùêµ. Overall we have ùëî(ùë•ùë°+1)=
3ùëíùêøùëÇ(ùë•ùë°+1)+|ùë•ùë°+1|0‚â•3ùëíùêøùëÇ(ùë•ùë°)+ùëõ‚àíùêµ=ùëî(ùë•ùë°). Therefore, ùê∏[Œîùë°
|ùë•ùë°|1=ùêµ,ùê∑ùëê]‚â•0and
ùê∏[Œîùë°|ùë•ùë°|1=ùêµ]=ùê∏[|ùë•ùë°+1|0‚àí|ùë•ùë°|0|ùë•ùë°|1=ùêµ,ùê∑]¬∑ùëÉùëü(ùê∑)
+ùê∏[Œîùë°|ùë•ùë°|1=ùêµ,ùê∑ùëê]¬∑ùëÉùëü(ùê∑ùëê)
‚â•1
ùëíùëõ.
The expected number of 0s in the initially selected uniform
random bit string isùëõ
2and the expected number of LeadingOnes
is at least zero, therefore ùê∏[ùëî(ùë•0)]‚â•ùëõ
2. We have an drift of at least
1
ùëíùëõin both the cases, therefore we get the required upper bound by
the additive drift theorem [10, Theorem 5],
ùê∏[ùëá]‚â§ùëíùëõ¬∑(3ùëíùêµ+ùëõ‚àíùêµ‚àíùê∏[ùëî(ùë•0)])‚â§ 3ùëí2ùëõùêµ+ùëíùëõ2
2‚àíùëíùëõùêµ=ùëÇ(ùëõ2).
This proves the upper bound. And the lower bound follows from
Theorem 4. ‚ñ°
5 EMPIRICAL ANALYSIS
We want to extend our theoretical work on deterministic constraint
the case of stochastic constraint models (as defined in Section 2.2).
For the first model we use parameters ùúá=1andùúé=0.1and for
the second model we use ùúñ=‚àö
3. Note that in the second modelùëà(ùêµ‚àí‚àö
3,ùêµ+‚àö
3)has variance 1. For both the models we considered
two different ùêµvalues 75 and 95 (also ùêµ= 85 in the Appendix). As
we will see, the (1+1) EA struggles in these settings; in order to
show that already a small parent population can remedy this, we
also consider the(10+1)EA in our experiments.
We use the following lemma for discussing certain probabilities
in this section.
Lemma 9. Letùëò‚àà{1,¬∑¬∑¬∑,ùêµ‚àí1},ùë•‚àà{0,1}ùëõ,ùêµ‚àà[ùëõ],ùëäùë•=√çùëõ
ùëñ=1ùë•ùëñ¬∑ùëåùëñwhereùëåùëñ‚àºùëÅ(1,ùúé2)andùë•ùëñbe theùëñ‚àíth bit ofùë•and
|ùë•|1‚â§ùêµ‚àíùëò. ThenùëÉùëü(ùëäùë•>ùêµ) ‚â§1‚àöùúãùëí‚àíùëò2
2ùëõ2ùúé2andùëÉùëü(ùëäùë•>ùêµ|
|ùë•|1=ùêµ)=1
2.
In Figure 1 we have a single sample run of (1+1) EA on the first
model. We observe that if the (1+1) EA finds a bit string with ùêµnum-
ber of 1s it violates the constraint with probability1
2(see Lemma 9)
and accepts a bit string with a lower number of LeadingOnes .
This process keeps repeating whenever the (1+1) EA encounters an
individual with a number of 1s closer toùêµ.
Figure 1: (1+1) EA sample run with ùëõ=100,ùêµ=85
andùëÅ(1,0.1)chance constraint for 10000 iteration.
Figure 2: (10+1) EA and (1+1) EA on LeadingOnes
withùëõ=100,ùêµ=75andùëÅ(1,0.1)chance constraint
for40000 iterations.
Figures 2 and 3 are about the first model in which we have
theLeadingOnes -values of the best individual (bit string with the
maximum fitness value) in each iteration of the (10+1) EA, the
LeadingOnes values of the second-worst individuals (bit string
with the second-smallest fitness value) in each iteration of the (10+1)
EA and the LeadingOnes values at each iteration of the (1+1) EA.
Each curve is the median of thirty independent runs and the shadedGECCO ‚Äô23, July 15‚Äì19, 2023, Lisbon, Portugal Friedrich and K√∂tzing, et al.
area is the area between the 25‚àíth and the 75‚àíth quantile values.
For all three ùêµ-values, after initial iterations, all the individuals
except the worst individual in the (10+1) EA population have ùêµ‚àí2
number of leading 1s. This is because, for this model, the probability
that an individual with ùêµ‚àí2number of 1s violates the constraint
is at mostùëí‚àí2‚àöùúã(from Lemma 9).
Figure 3: (10+1) EA and (1+1) EA on LeadingOnes
withùëõ=100,ùêµ=95andùëÅ(1,0.1)chance constraint
for40000 iterations.
Figures 4 and 5 are about the second model and the curves repre-
sent the same things as in the previous figures but with respect to
the second model. In these figures we can see that the best and the
second worst individuals of the (10+1) EA are not the same because
of the changing constraint values.
Figure 4: (10+1) EA and (1+1) EA on LeadingOnes
withùëõ=100,ùêµ=75andùëà(ùêµ‚àí‚àö
3,ùêµ+‚àö
3)stochastic
constraint for 40000 iterations.
Figure 5: (10+1) EA and (1+1) EA on LeadingOnes
withùëõ=100,ùêµ=95andùëà(ùêµ‚àí‚àö
3,ùêµ+‚àö
3)stochastic
constraint for 40000 iterations.6 CONCLUSIONS
Understanding how evolutionary algorithms deal with constrained
problems is an important topic of research. We investigated the
classical LeadingOnes problem with additional constraints. For the
case of a deterministic uniform constraint we have carried out
a rigorous run time analysis of the (1+1) EA which gives results
on the expected optimization time in dependence of the chosen
constraint bound. Afterwards, we examined stochastic constraints
and the use of larger populations for dealing with uncertainties.
Our results show a clear benefit of using the (10+1)EA instead of
the(1+1)EA. We regard the run time analysis of population-based
algorithms for our examined settings of stochastic constraints as
an important topic for future work.
7 ACKNOWLEDGEMENTS
Frank Neumann has been supported by the Australian Research
Council (ARC) through grant FT200100536. Tobias Friedrich and
Timo K√∂tzing were supported by the German Research Foundation
(DFG) through grant FR 2988/17-1.
REFERENCES
[1]Hans-Georg Beyer and Bernhard Sendhoff. 2007. Robust optimization‚Äìa com-
prehensive survey. Computer methods in applied mechanics and engineering 196,
33-34 (2007), 3190‚Äì3218.
[2]Abraham Charnes and William W Cooper. 1959. Chance-constrained program-
ming. Management science 6, 1 (1959), 73‚Äì79.
[3]Benjamin Doerr and Timo K√∂tzing. 2021. Lower Bounds from Fitness Levels
Made Easy. In Proceedings of the Genetic and Evolutionary Computation Confer-
ence (GECCO 2021) . Association for Computing Machinery, New York, NY, USA,
1142‚Äì1150. https://doi.org/10.1145/3449639.3459352
[4]Benjamin Doerr and Frank Neumann (Eds.). 2020. Theory of Evolutionary
Computation - Recent Developments in Discrete Optimization . Springer. https:
//doi.org/10.1007/978-3-030-29414-4
[5]Stefan Droste, Thomas Jansen, and Ingo Wegener. 2002. On the analysis of the
(1+1) evolutionary algorithm. Theor. Comput. Sci. 276, 1-2 (2002), 51‚Äì81.
[6]A. E. Eiben and James E. Smith. 2015. Introduction to Evolutionary Computing,
Second Edition . Springer.
[7]Tobias Friedrich, Timo K√∂tzing, J. A. Gregor Lagodzinski, Frank Neumann, and
Martin Schirneck. 2020. Analysis of the (1+1) EA on subclasses of linear functions
under uniform and linear constraints. Theor. Comput. Sci. 832 (2020), 3‚Äì19.
[8]Tobias Friedrich, Timo K√∂tzing, J. A. Gregor Lagodzinski, Frank Neumann, and
Martin Schirneck. 2017. Analysis of the (1+1) EA on Subclasses of Linear Func-
tions under Uniform and Linear Constraints. In Foundations of Genetic Algorithms
(FOGA) . ACM Press, 45‚Äì54.
[9]Grani A Hanasusanto, Vladimir Roitch, Daniel Kuhn, and Wolfram Wiesemann.
2015. A distributionally robust perspective on uncertainty quantification and
chance constrained programming. Mathematical Programming 151 (2015), 35‚Äì62.
[10] Jun He and Xin Yao. 2004. A study of drift analysis for estimating computation
time of evolutionary algorithms. Natural Computing 3 (2004), 21‚Äì35.
[11] Thomas Jansen. 2013. Analyzing Evolutionary Algorithms - The Computer Science
Perspective . Springer. https://doi.org/10.1007/978-3-642-17339-4
[12] Pu Li, Harvey Arellano-Garcia, and G√ºnter Wozny. 2008. Chance constrained
programming approach to process optimization under uncertainty. Computers &
chemical engineering 32, 1-2 (2008), 25‚Äì45.
[13] Bruce L Miller and Harvey M Wagner. 1965. Chance constrained programming
with joint constraints. Operations Research 13, 6 (1965), 930‚Äì945.
[14] Rahul Nair and Elise Miller-Hooks. 2011. Fleet management for vehicle sharing
operations. Transportation Science 45, 4 (2011), 524‚Äì540.
[15] Aneta Neumann and Frank Neumann. 2020. Optimising Monotone Chance-
Constrained Submodular Functions Using Evolutionary Multi-objective Algo-
rithms. In PPSN (1) (Lecture Notes in Computer Science, Vol. 12269) . Springer,
404‚Äì417.
[16] Aneta Neumann, Yue Xie, and Frank Neumann. 2022. Evolutionary Algorithms
for Limiting the Effect of Uncertainty for the Knapsack Problem with Stochastic
Profits. In Parallel Problem Solving from Nature - PPSN XVII - 17th International
Conference, PPSN 2022, Proceedings, Part I (Lecture Notes in Computer Science,
Vol. 13398) . Springer, 294‚Äì307. https://doi.org/10.1007/978-3-031-14714-2_21
[17] Frank Neumann, Mojgan Pourhassan, and Carsten Witt. 2021. Improved Runtime
Results for Simple Randomised Search Heuristics on Linear Functions with aAnalysis of the (1+1) EA on LeadingOnes with Constraints GECCO ‚Äô23, July 15‚Äì19, 2023, Lisbon, Portugal
Uniform Constraint. Algorithmica 83, 10 (2021), 3209‚Äì3237.
[18] Frank Neumann and Andrew M. Sutton. 2019. Runtime analysis of the (1 + 1)
evolutionary algorithm for the chance-constrained knapsack problem. In Proceed-
ings of the 15th ACM/SIGEVO Conference on Foundations of Genetic Algorithms,
FOGA 2019 . ACM, 147‚Äì153. https://doi.org/10.1145/3299904.3340315
[19] Frank Neumann and Carsten Witt. 2010. Bioinspired Computation in Combinato-
rial Optimization . Springer. https://doi.org/10.1007/978-3-642-16544-3
[20] Chao Qian, Jing-Cheng Shi, Yang Yu, and Ke Tang. 2017. On Subset Selection
with General Cost Constraints. In Proceedings of the Twenty-Sixth International
Joint Conference on Artificial Intelligence, IJCAI 2017 . ijcai.org, 2613‚Äì2619. https:
//doi.org/10.24963/ijcai.2017/364
[21] Chao Qian, Yang Yu, and Zhi-Hua Zhou. 2015. Subset Selection by Pareto Op-
timization. In Advances in Neural Information Processing Systems 28: Annual
Conference on Neural Information Processing Systems 2015 . 1774‚Äì1782.
[22] Vahid Roostapour, Aneta Neumann, and Frank Neumann. 2022. Single- and multi-
objective evolutionary algorithms for the knapsack problem with dynamically
changing constraints. Theor. Comput. Sci. 924 (2022), 129‚Äì147. https://doi.org/10.
1016/j.tcs.2022.05.008
[23] Vahid Roostapour, Aneta Neumann, Frank Neumann, and Tobias Friedrich. 2022.
Pareto optimization for subset selection with dynamic cost constraints. Artif.
Intell. 302 (2022), 103597. https://doi.org/10.1016/j.artint.2021.103597
[24] Feng Shi, Xiankun Yan, and Frank Neumann. 2022. Runtime Analysis of Simple
Evolutionary Algorithms for the Chance-Constrained Makespan Scheduling
Problem. In PPSN (2) (Lecture Notes in Computer Science, Vol. 13399) . Springer,
526‚Äì541.
[25] Eric W. Weisstein. [n. d.]. Complementary Error Function. https://mathworld.
wolfram.com/Erfc.html. Accessed: 2023-02-01.
[26] Yue Xie, Oscar Harper, Hirad Assimi, Aneta Neumann, and Frank Neumann.
2019. Evolutionary algorithms for the chance-constrained knapsack problem.
InProceedings of the Genetic and Evolutionary Computation Conference, (GECCO
2019) . ACM, 338‚Äì346. https://doi.org/10.1145/3321707.3321869
[27] Yue Xie, Aneta Neumann, and Frank Neumann. 2020. Specific single- and multi-
objective evolutionary algorithms for the chance-constrained knapsack problem.
InProceedings of the Genetic and Evolutionary Computation Conference, (GECCO
2020) . ACM, 271‚Äì279. https://doi.org/10.1145/3377930.3390162
[28] Yue Xie, Aneta Neumann, Frank Neumann, and Andrew M. Sutton. 2021. Runtime
analysis of RLS and the (1+1) EA for the chance-constrained knapsack problem
with correlated uniform weights. In Proceedings of the Genetic and Evolutionary
Computation Conference, (GECCO 2021) . ACM, 1187‚Äì1194.
[29] Hui Zhang and Pu Li. 2011. Chance constrained programming for optimal power
flow under uncertainty. IEEE Transactions on Power Systems 26, 4 (2011), 2417‚Äì
2424.
A APPENDIX
Corollary 10. Letùëõ,ùêµ‚ààNandùëê>0. Then the (1+1) EA on
LeadingOnes with the cardinality constraint ùêµfinds a search point
withùêµ‚àíùëê(ùëõ‚àíùêµ)leading 1s withinùëÇ(ùëõ2)in expectation.
Proof. Letùë•ùë°be the search point at the ùë°ùë°‚Ñéiteration of the (1+1)
EA optimizing LeadingOnes with the cardinality constraint ùêµand
ùëá1=min{ùë°‚â•0ùêøùëÇ(ùë•ùë°)‚â•ùêµ‚àíùëê(ùëõ‚àíùêµ)}. And letùëò=ùêµ‚àíùëê(ùëõ‚àíùêµ).
Then, from the proof of Theorem 2 we know that
ùê∏[ùëá1]‚â§ùëî<ùêµ(ùëò)‚â§ùëíùëõùêµ
2+(ùëíùëõ+ùëí2ùëõ(ùëõ‚àíùêµ))
1+lnùêµ
ùëê(ùëõ‚àíùêµ)
.
Since lnùë¶‚â§ùë¶for anyùë¶>0, we have(ùëõ‚àíùêµ)
ln
ùêµ
ùëê(ùëõ‚àíùêµ)
‚â§ùêµ
ùëê=
ùëÇ(ùëõ). Therefore, ùê∏[ùëá1]=ùëÇ(ùëõ2). ‚ñ°
Corollary 11. Letùëõ,ùêµ‚ààNandùëê>0. Then the (1+1) EA on
LeadingOnes with the cardinality constraint ùêµfinds a search point
withùêµ‚àíùëê(ùëõ‚àíùêµ)number of LeadingOnes withinùëÇ(ùëõ2)in expecta-
tion.
Proof. Letùë•ùë°be the search point at the ùë°ùë°‚Ñéiteration of the (1+1)
EA optimizing LeadingOnes with the cardinality constraint ùêµand
ùëá1=min{ùë°‚â•0ùêøùëÇ(ùë•ùë°)=ùêµ‚àíùëê(ùëõ‚àíùêµ)}. And letùëò=ùêµ‚àíùëê(ùëõ‚àíùêµ).Then from Theorem 2 we know that,
ùê∏[ùëá1]‚â§ùëî<ùêµ(ùëò)
=ùëò‚àëÔ∏Å
ùëñ=0(ùëî<ùêµ(ùëñ)‚àíùëîùêµ(ùëñ))+ùëò‚àëÔ∏Å
ùëñ=1(ùëîùêµ(ùëñ)‚àíùëî<ùêµ(ùëñ‚àí1))
=ùêµ‚àíùëê(ùëõ‚àíùêµ)‚àëÔ∏Å
ùëñ=0ùëíùëõ
ùêµ‚àíùëñ+ùêµ‚àíùëê(ùëõ‚àíùêµ)‚àëÔ∏Å
ùëñ=1ùëíùëõ
1+ùëí¬∑(ùëõ‚àíùêµ)
ùêµ‚àíùëñ+1
‚â§ùëíùëõùêµ‚àëÔ∏Å
ùëñ=ùëê(ùëõ‚àíùêµ)1
ùëñ
+ùëíùëõùêµ
2+ùëí2¬∑ùëõ(ùëõ‚àíùêµ)ùêµ‚àëÔ∏Å
ùëñ=ùëê(ùëõ‚àíùêµ)1
ùëñ
‚â§ùëíùëõùêµ
2+(ùëíùëõ+ùëí2ùëõ(ùëõ‚àíùêµ))
1+lnùêµ
ùëê(ùëõ‚àíùêµ)
.
Since lnùë¶‚â§ùë¶for anyùë¶>0, we have(ùëõ‚àíùêµ)
ln
ùêµ
ùëê(ùëõ‚àíùêµ)
‚â§ùêµ
ùëê‚â§ùëõ.
Therefore,ùê∏[ùëá1]=ùëÇ(ùëõ2). ‚ñ°
Lemma 12. Letùëò‚àà{1,¬∑¬∑¬∑,ùêµ‚àí1},ùë•‚àà{0,1}ùëõ,ùêµ‚àà[ùëõ],ùëäùë•=√çùëõ
ùëñ=1ùë•ùëñ¬∑ùëåùëñwhereùëåùëñ‚àºùëÅ(1,ùúé2)andùë•ùëñbe theùëñ‚àíth bit ofùë•and
|ùë•|1‚â§ùêµ‚àíùëò. ThenùëÉùëü(ùëäùë•>ùêµ)‚â§1‚àöùúãùëí‚àíùëò2
2ùëõ2ùúé2.
Proof. First note that ùëäùë•is nothing but sum of |ùë•|1normal ran-
dom variables with mean 1and variance ùúé2, i.e.ùëäùë•‚àºùëÅ(|ùë•|1,|ùë•|1¬∑
ùúé2)andùëÉùëü(ùëäùë•>ùêµ||ùë•|1=ùêµ)=1
2.
ùëÉùëü(ùëäùë•>ùêµ)=1‚àíùëÉùëü(ùëäùë•‚â§ùêµ)
=1‚àí1
2
1+erfùêµ‚àí|ùë•|1
|ùë•|1¬∑ùúé‚àö
2
=1
2
1‚àíerfùêµ‚àí|ùë•|1
|ùë•|1¬∑ùúé‚àö
2
=1
2erfcùêµ‚àí|ùë•|1
|ùë•|1¬∑ùúé‚àö
2
.
Since complementary error function erfcis a decreasing function
and|ùë•|1‚â§ùêµ‚àíùëò, we have
ùëÉùëü(ùëäùë•>ùêµ)=1
2erfcùêµ‚àí|ùë•|1
|ùë•|1¬∑ùúé‚àö
2
(8)
‚â§1
2erfcùëò
|ùë•|1¬∑ùúé‚àö
2
‚â§1
2erfcùëò
ùëõùúé‚àö
2
Sinceùëüùëò=ùëò
ùëõùúé‚àö
2>0, we can use the upper bound for the erfcfrom
[25],
ùëÉùëü(ùëäùë•>ùêµ)‚â§1
2erfcùëò
ùëõùúé‚àö
2
‚â§1‚àöùúãùëí‚àíùëü2
ùëò
ùëüùëò+‚àöÔ∏É
ùëü2
ùëò+4
ùúãGECCO ‚Äô23, July 15‚Äì19, 2023, Lisbon, Portugal Friedrich and K√∂tzing, et al.
‚â§1‚àöùúãùëí‚àíùëò2
2ùëõ2ùúé2.
From Equation 8, we have ùëÉùëü(ùëäùë•>ùêµ||ùë•|1=ùêµ)=1
2erfcùêµ‚àí|ùë•|1
|ùë•|1¬∑ùúé‚àö
2
which is1
2. ‚ñ°
The following two figures are the experimental results for the
constraint value ùêµ=85.
Figure 6: (10+1) EA and (1+1) EA on LeadingOnes
withùëõ=100,ùêµ=85andùëÅ(1,0.1)chance constraint
for40000 iterations.
Figure 7: (10+1) EA and (1+1) EA on LeadingOnes
withùëõ=100,ùêµ=85andùëà(ùêµ‚àí‚àö
3,ùêµ+‚àö
3)stochastic
constraint for 40000 iterations.