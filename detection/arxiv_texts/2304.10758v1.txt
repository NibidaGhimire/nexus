Enhancing Wind Power Forecast Precision via
Multi-head Attention Transformer: An Investigation
on Single-step and Multi-step Forecasting
Md Rasel Sarkar, Sreenatha G. Anavatti, Tanmoy Damy, Mahardhika Pratamaz, and Berlian Al Kindhix
School of Engineering & Information Technology, The University of New South Wales, Canberra, ACT 2610, Australia
ySaab-NTU Joint Lab, Nanyang Technological University , 637460, Singapore
zSTEM, University of South Australia, Adelaide 5095, Australia
xDepartment of Electrical Automation Engineering, Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia
Abstract —The main objective of this study is to propose an
enhanced wind power forecasting (EWPF) transformer model
for handling power grid operations and boosting power market
competition. It helps reliable large-scale integration of wind
power relies in large part on accurate wind power forecasting
(WPF). The proposed model is evaluated for single-step and
multi-step WPF, and compared with gated recurrent unit (GRU)
and long short-term memory (LSTM) models on a wind power
dataset. The results of the study indicate that the proposed
EWPF transformer model outperforms conventional recurrent
neural network (RNN) models in terms of time-series forecasting
accuracy. In particular, the results reveal a minimum perfor-
mance improvement of 5% and a maximum of 20% compared
to LSTM and GRU. These results indicate that the EWPF
transformer model provides a promising alternative for wind
power forecasting and has the potential to signiﬁcantly improve
the precision of WPF. The ﬁndings of this study have implications
for energy producers and researchers in the ﬁeld of WPF.
Index Terms —Wind Power Forecasting, LSTM, GRU, Trans-
former
I. I NTRODUCTION
Renewable energy has come into prominence as a possible
alternative energy source due to the declining supply of fossil
fuels and the continued increase in power consumption [1].
Wind power is an efﬁcient, renewable, and environmentally
friendly option for solving the issue of fossil fuel degradation
because of its cheap expenses for operation, environmental
friendliness, and capacity to produce energy on a commercial
scale. Conversely, the reliability of power system operation
is reduced by the variability of wind speed, indirectness,
and the low density of energy [2] [3]. In order to ensure
the safe and cost-effective functioning of power grids with
large-scale wind power, enhancing the accuracy of wind
power forecasts has become a critical issue for the existing
power system [4]. There are three ways to predict wind
power so far: the physical model, the statistical model, and
the machine learning model. Physical modeling approaches
attempt to provide a realistic mathematical representation of
WPF using data on geography and climate. Realistic real-
time prediction tasks may not beneﬁt from this approach as
the unstable nature involved could make it impractical [5]–
[7]. However, statistical techniques use error minimization todetermine the ideal correlation between projected wind power
and past information [8]. Conventional statistical time series
forecasting (TSF) methods, namely state space models (SSMs)
[9], autoregressive (AR) [10], and moving average (MA)
[11], and autoregressive integrated moving average (ARIMA)
[12] models are ﬁtted to each time series individually [13],
[14]. Statistical models are especially well-suited for situations
when the series data structure is well understood since they
explicitly integrate structural assumptions in the model [6].
The most common challenges of statistical models are; weaker
performance in making long-term predictions, not being suit-
able for time series that depend on the seasons, and doubts
about the reliability [15]. These models make interpretable
but require some work to identify the structure. However,
deep neural networks (DNNs) [16], [17] are one example of
a kind of machine learning model that has been offered as an
alternate way to address the aforementioned challenges, with
RNNs [18], [19] being used to represent time series in an
autoregressive form. When dealing with sequential data, DNNs
approaches such as recurrent models are often regarded as
among the most cutting-edge methods currently available. In
order to determine the probability of future occurrences, RNN-
based models like LSTM [20] and GRU [21] are used. These
models allow for the extraction of lengthy dependencies from
past sequences. Elsewhere, conventional RNNs are challenging
to train because of concerns with vanishing and exploding
gradients, which cause the network to be unstable and unable
to acquire the knowledge links among input items from
enormous amounts of sequential information. The vanishing
gradient issue was resolved using techniques like LSTM and
GRU despite the advent of recurrent variants. Besides, there
are some major challenges that still exist. Since the model
is processing the data sequentially, it is unable to be trained
in parallel and unable to capture long-range dependencies. In
addition, it shows poorer performance for non-stationary time
series datasets and training needs an input of a ﬁxed size [22]
[23].
Transformers have obtained superior performance in a vari-
ety of natural language processing (NLP), image processing,
and other tasks, which has piqued the interest of the commu-arXiv:2304.10758v1  [eess.SY]  21 Apr 2023nity of time series researchers. In recent years, transformer
architecture has gained signiﬁcant recognition in recent years
as an advanced method that can surpass conventional RNN-
based networks through its ability to acquire global informa-
tion using an attention mechanism. [24]. Moreover, before
using these models, they have to pick a cut-off length. This
makes the choice of the cut-off length random as well as
subjective. In numerous real-world applications, the essential
information in an extended sequence of data is concealed
locally and globally. However, unlike time series data, the
quadratic model does not exhibit the same growth patterns.
Due to the quadratic difﬁculty of sequence length, RNNs are
computationally expensive, unable to capture long-dependence
time series data, and unable to process input at once. While
transformers have many applications their ability to describe
time series is particularly promising because of their capacity
to capture long-range relationships and interactions [25].
Overall the literature has identiﬁed that a more complex and
adaptive model is required to obtain efﬁcient correlations of
long-range time-series data sequences. Therefore, it is aimed
to evaluate the utility of the transformer model in WPF and
to perform detailed comparisons with GRU and LSTM for
single-step and multi-step. The proposed transformer model
has overcome the recurrent issues of recurrent models. In
addition, the model can be captured the dependencies of long-
term wind power for forecasting. Finally, The proposed model
has been carried out with LSTM and GRU to verify the
improved performance of the proposed method.
The structure of this paper is as follows: Section 2 presents
the problem formulation. The design of the EWPF transformer
model is described in detail in Section 3. In addition, the ex-
perimental details are shown in Section 4. Section 5 elaborates
on the results and discussion. Lastly, Section 5 concludes this
paper.
II. PROBLEM FORMULATION
The objective of time-series forecasting is to predict future
values based on past observations. Given a time-series X with
length T, represented as X= [x1;x2;:::;xT], wherextis
a part of the time series at time instant t-th, the problem
is to use a model !to make step-ahead predictions for f!,
parameterized by !, that maps the input space to the target
space,f!:X!Y. Each input, xt, and target, yt, belongs to
RD+1, where Dis the dimension of feature where the feature
is a univariate at the t-th time index. The model f!(:)is
represented by wind power forecasting using a transformer.
The target attribute, yt, is set equal to xt+whereis the
number of time steps. In this paper, both single-step and multi-
step time-series forecasting are taken into consideration, with
predictions made well ahead of time where is large.
III. D ESIGN OF EWPF T RANSFORMER
The transformer architecture is a novel approach to sequence
data processing that was ﬁrst proposed by Vaswani et al.
in 2017 [26]. Since then, it has been widely used for NLP
tasks [27], [28], and one of its main advantages is its abilityto process entire sequences of data [29], [30]. Although the
transformer was initially designed for NLP, recent studies
have explored its application to time series data. Li et al.
[30] compared the transformer with the ARIMA method and
RNN-based models, ﬁnding that the transformer had supe-
rior performance. A multi-horizon univariate time series was
forecasted using the transformer architecture by Lim, Arik,
et al. [31]. In the task of predicting inﬂuenza prevalence,
Wu, Green et al. [29] employed a transformer architecture
and demonstrated its superiority over ARIMA, LSTM, and
GRU sequence-to-sequence models with attention. Although
the transformer model was modiﬁed to suit the time series
data, its decoder component was not included. In terms of
accuracy, this transformer model achieved a score of 0.738
(R2), while an LSTM model scored 0.4868 ( R2), withR2
representing the coefﬁcient of determination. Song, Rajam et
al. [32] utilized the transformer to forecast continuous values
for four distinct clinical datasets that consisted of multivariate
time series data. Contrariwise, the decoder part of the model
was eliminated, and an encoder block was employed to process
the multivariate time series data. Subsequently, a regression
block was integrated into the model to generate the continuous
value predictions. The study found that an LSTM model
performed similarly to the transformer model in terms of
accuracy for one of the datasets. Nevertheless, considering the
transformer architecture’s superior performance in handling
multivariate time series data over other neural networks, the
model utilized in this study was a modiﬁed version of the
transformer architecture introduced by Zerveas et al. [33] and
N. Wu et al. [29]. To the best of my knowledge, there has
been the minimal exploration of EWPF transformers utilizing
multi-head attention-based models for single and multi-step
forecasting, despite the considerable interest in this area. Fig.
1 illustrates the details of the model.
A. Input Embedding
To begin with, the time series data undergoes embedding.
Since time is a crucial factor in time series problems, unlike
other types of sequential data, the model must take it into
account. The transformer model does not have any recurrence
or convolution, and as a result, it must rely on the sequence
order. Thus, to enable the model to utilize the sequence
order, information about the relative or absolute location of
the tokens in the sequence needs to be incorporated. This is
accomplished by employing positional encoding for the input
embedding at the encoder and decoder stack bottoms. The
positional encodings and embeddings have the same dimension
dmodel , and the formula for the input embedding is as follows
[34].
Emd=PE (pos; 2t)+PE (pos; 2t+1) (1)
where,
PE (pos; 2t)=sin(pos=100002i=dmodel) (2)
PE (pos; 2t+1)=cos(pos=100002i=dmodel) (3)Fig. 1. Architecture of transformer of enhanced wind power forecasting [26].
whereiis the individual dimension of the embedding and
posdenotes the position of the index of the input sequence.
Speciﬁcally, each positional encoding sinusoid functions cor-
respond to even positions, while cosine functions correspond
to odd positions.
B. Encoder
The transformer encoder comprises several blocks denoted
asEb
, withbrepresenting the block number. Each block
is divided into two sub-layers: the ﬁrst one is a multi-head
self-attention mechanism, while the second is a simple fully-
connected position-wise feed-forward neural network (FFNN).
To compute the output of each sub-layer, residual connec-
tions with layer normalization are utilized [35], [36]. To
be speciﬁc, the output is obtained through the expression
LayerNorm (x+Sublayer (x)), whereSublayer (x)repre-
sents the function of the sub-layer. To support these residual
connections, all sub-layers and embedding layers in the model
have a uniform dimension dmodel = 512 , which is also
employed in the transformer model discussed in [26]. The
dmodel corresponds to the number of dimensions of the model
used for input to the multi-head attention mechanism. The term
”attention function” refers to a mapping between a query and
a set of key-value pairs, where both the query and the key-
value pairs are vectors. The output is computed as a weightedsum of the values, where the weight assigned to each value
is determined by the query’s compatibility function with the
corresponding key.
1) Scaled Dot-Product Attention: Special attention in the
multi-head attention is namely ‘Scaled Dot-Product Attention’
shown in Fig. 1. The input consists of dimension dkqueries
and keys, as well as dimension dvvalues. The attention can be
calculated by the query’s dot product with all keys, dividing
each byp
(dk), then using a softmax function to get the
weights on the values. In execution, it computes the attention
function on a set of queries at the same time, which are
grouped into a matrix Q. The matrices K and V also contain a
dense packing of the keys and values. The attention constructs
the output matrix as follows:
Attention (Q;K;V ) =softmax (QKTM
pdk)V (4)
In addition, both additive attention and dot-product (mul-
tiplicative) attention [37] are widely utilized. However, Dot-
product attention with a scaling factor of 1=p
(dk)is used in
the proposed system. Speciﬁcally, a single hidden layer FFNN
is used for the determination of the compatibility function
with the help of additive attention. Dot-product attention has
the same theoretical complexity as the alternative technique,
but it can be carried out with optimized matrix multiplication,which makes it more time- and space-efﬁcient. Although both
strategies perform fairly well for small amounts of dk, when
applied to bigger values of dk, the additive attention strategy
outperforms the dot-product attention strategy without any
scaling [38].
2) Multi-head Attention: Instead of using a single attention
function with keys, queries, and values of dimension dmodel ,
it was discovered that projecting the queries, keys, and values
times with learned linear projections to dimensions dq,dk, and
dv, respectively, was more beneﬁcial. This resulted in head
(h) parallel versions of queries, keys, and values that could be
processed by the attention mechanism. The output values had
a dimension of dvand were combined and projected again to
obtain the ﬁnal values. Multi-head attention allows the model
to attend to input from different representation subspaces
at various stages and averaging prevents over-attentiveness
compared to a single-attention head.
MultiHead (Q;K;V ) =Concat (head 1:::::::;head h)(5)
whereheadi=Attention (QWQ
i;KWK
i;VWV
i)
the projections are parameter matrices and symbolized
asWQ
i2I RdmodeldkWk
t2I RdmodeldkWV
t2
I RdmodeldvW02I Rhdmodeldkwhere,WQ,WKandWV
are quires, keys and values weights respectively. There are
hhead = 8 parallel attention has been used in the multi-head
attention layer. In addition, each head has dk=dv=dmodel=h
= 64 dimension. The overall computing cost is comparable to
that of single-head attention with full dimensions because of
the lower dimension of each head.
C. Feed-Forward Neural Network (FFNN)
Besides the attention sub-layers, the encoder and decoder
layers also contain a FFNN that is connected fully and operates
on each position in the sequence uniformly and independently.
This network comprises two linear transformations separated
by a ReLU activation function.
FFN (x) =max(0;xW 1+b1)W2+b2 (6)
Although the linear transformations are identical across all
positions, the parameters differ from layer to layer. This is
analogous to performing two convolutions with kernel size 1.
The dimensions of the input and output are both dmodel =
512, while the inner-layer of the FFNN has a dimensionality
ofdff= 2048. In the transformer model, the input and
output embeddings are converted to the input and output vector
dimensions of dmodel = 512. The decoder output is then passed
through the next probability model. The transformer model
uses the same weight matrix for the two embedding layers
and the pre-softmax linear transformation, as proposed by
Press and Wolf [39]. The weights in the embedding layers
are multiplied byp
(dmodel ).D. Decoder
The transformer model is composed of multiple decoder
blocksDb
, each indexed by b. In addition to the two sub-
layers present in each encoder layer, a third sub-layer is added
in each decoder block, which performs multi-head attention
over the encoder stack’s output. Like the encoder, each sub-
layer in the decoder is surrounded by residual connections
and followed by layer normalization. Furthermore, the self-
attention sub-layer in the decoder stack is modiﬁed to prevent
positions from attending to subsequent positions. Along with
the output embedding being moved by one position, makes
sure that forecasts for position iare only based on known
outputs at places that come before i. [26].
The ﬁnal loss function is deﬁned as follows,
LMSE =TX
t=11
2(yt D((E(xt)))2(7)
Algorithm 1 contains a detailed and thorough description of
the EWPF transformer, providing all the necessary information
and steps required to understand and implement the algorithm
effectively.
Algorithm 1 EWPFT
1:Input: Time series data Xtrain=
f(x1;y1);(x2;y2);:::;(xT;yT+)g, Similarly,
Xtest=f(xt
1;yt
1);(xt
2;yt
2);:::;(xt
T;yt
T+)g,
number of steps ahead m, model parameters,
ff(:) =E;D;Emdg, batch=B, epoch =nepoch ,
Adam hyper parameters ( lr;1;2), time-step= 
2:/* Initialisation of three model component parameters */
3:Emd,EandDN(0;0:022)
4:forpinf1;2;:::;nepochgdo
5: /* Embedding the input with batch, B2Xtrain*/
6:fxigB
i=1 Emd(fxigB
i=1)
7: /* Encoding the embedding output */
8:fzigB
i=1 E(fxigB
i=1)
9: /* Decoding the encode output */
10: ^yi DfzigB
i=1
11: /* Calculate the MSE loss in (7) */
12: /* all the parameters are updated simultaneously */
13:Emdupdate through MSE loss
!(r1
BLMSE);;lr;1;2)
14:Eupdate through MSE loss
!(r1
BLMSE);;lr;1;2)
15:Dupdate through MSE loss
!(r1
BLMSE);;lr;1;2)
16: ifnepoch % 1 == 0 then
17: /* Estimate of MSE error values */
18:f(:)forXtestdatasets.
19: end if
20:end forIV. E XPERIMENTAL DETAILS
In this paper, there are three deep learning models namely,
multi-head attention-based transformer, LSTM, and GRU have
been investigated. Our model is constructed based on PyTorch
with Python 3.7. The hardware speciﬁcation of the computer
is a single NVIDIA Corporation / Mesa Intel® UHD Graphics
(TGL GT1) for training. Prior to being converted to sequences,
the raw input data is ﬁrst normalized using a Min Max
Normalization ( 1:0;+1:0)technique. Normalisation is a
crucial data preprocessing phase in deep learning, as it can
enhance the model’s efﬁciency and convergence during train-
ing. Normalization entails adjusting the input data so that
its characteristics have comparable ranges and distributions,
which may contribute to the model’s learning. As a result of
these ﬁndings, we have explored how DNN models do when
confronted with varying sequence lengths. There are three
distinct representations of the input data sequence, such as
10, 20, and 50. The historical data was associated with the
model’s input sequences.
Since there is no recurrent network layer built into the
transformer encoder, the transformer injects positional in-
formation into the embedding using positional encoding. In
the position encoding, for each odd and even time step, the
sine and cosine functions produced two different vectors. The
positional information would be injected by including each of
them in the appropriate embedding vector. The self-attention
mechanism in the transformer model allows the model to learn
relationships between different time steps in the time series.
Self-attention computes a weighted sum of all the time steps in
the time series, where the weights are learned during training.
In addition, the model can assign more weight to relevant time
steps and less weight to irrelevant time steps, allowing it to
attend to the most important parts of the time series. Moreover,
multi-head attention computes multiple attention mechanisms
in parallel. This allows the model to capture different types of
dependencies and relationships within the time series. Since
the transformer model does not use recurrent connections, it
needs a way to encode the position of each time step in the
time series. This is achieved using positional encoding, which
adds a ﬁxed-length vector to the input of each self-attention
layer to represent the position of each time step. After the
self-attention layers, the output of the transformer model is
passed through a series of fully connected layers (FFNN).
These layers allow the model to learn complex non-linear
relationships between the input time series and the output
predictions. The output layer of the transformer model for
time series forecasting uses a mask to prevent the model
from predicting future time steps that have not occurred yet.
This is important because, in time series forecasting, the
model should only have access to information that is available
at the time of prediction. The training is carried out using
the rolling forecasting option (stride size=1). The encoder
and decoder input lengths are 10, 20, and 50, respectively,
which correspond to the prediction lengths of the 1stand
5thsteps. We update the dropout rate for the progressiveTABLE I
CRUCIAL VARIABLES OF THE TRANSFORMER
Category Hyper-parameter Parameter Value
Structure of Modeldmodel 512
Number hidden 512
Number head 8
Number layer 8
Kernel size 1
Hyper-parametersdropout 0.1
Optimizer Adam
Learning rate (lr) 0.0001
Batch Size 64
training scheme every 5 iterations, with a dropout limit of 0.1.
Moreover, during training, the model is optimized to minimize
the difference between the predicted values and the true values
of the target time series. This is done using a loss function such
as mean squared error (MSE) or mean absolute error (MAE).
The transformer for time series forecasting is a robust model
that can capture long-term dependencies and relationships
within a time series, making it well-suited for accurate and
efﬁcient forecasting. Table 1 contains more information about
the model parameters. To evaluate the performance of the
models, two metrics are used namely MSE and MAE, and
R2.
A transformer is built with a stack encoder and decoder
with self-attention layers that were connected point-by-point.
It mapped an input sequence. The experimental settings for
the transformer’s encoder and decoder block are as fol-
lows:dmodel =Numberhidden = 512,Numberlayer = 8, and
dropout = 0.1,Inputfeature andOutputfeature = 1. In
addition, loss and optimization were calculated using the MSE
loss function and adaptive moment estimation (Adam) [40]
with alrof 0.0001 [41], [42] for transformer, LSTM, and
GRU. Hence, the aim of the paper is to identify the best
model from the transformer, LSTM, and GRU for time series
forecasting. Therefore, the same parameters have been set for
model evaluation. The LSTM and GRU model parameters are
Numberhidden =512,lr= 0.0001,1= 0,2= 0:5[43],
Numberlayer = 8.
V. R ESULTS AND DISCUSSION
Wind farms depend on wind power predictions. Accurate
forecasting is needed to keep the electrical grid stable as it
is interrupted and varied. Grid operators need wind power
forecasts to balance energy supply and demand in real time.
Moreover, single-step and multi-step forecasts both look into
the future and estimate values, but they differ in key ways.
The single-step method trains a model to make predictions
just one step into the future. Whereas, there are no long-term
trends or seasonal patterns, and the values of the time series in
the future rely simply on their values in the past, this method
may be helpful. In contrast, the model is educated to anticipate
a certain number of steps rather than a single eventual state.
Meanwhile, there are long-term trends or seasonal patterns
in the data, and the future values of the time series rely
on both the past values and the future values, multi-stepFig. 2. Single-step wind power forecasting for 20 sequences.
 Fig. 3. Multi-step wind power forecasting for 20 sequences.
prediction is helpful. However, two comparative experiments
were carried out in this paper to validate the robustness of
the proposed model for single and multi-step WPF. Three
models—transformer, LSTM, and GRU are used to process
a series of data in order to undertake single-step (1-hour) and
multi-step (5-hour) wind power forecasts in order to evaluate
model effectiveness. In the experiment, the proposed model
utilized an open historical dataset from Germany, which in-
cluded actual measurements of wind power (Watt) generation.
Each record in the dataset included one parameter sampled
every hour, for a total of 61,500 samples. In addition, it
is divided into training (70%) and testing (30%) . The data
has been divided into 10, 20, and 50 input sequences. These
sequences are applied in the LSTM, GRU, and transformer
models for both single-step and multi-step forecasting. In a
single step, the next step of the input sequence is forecasted
and in a multi-step, the 5thstep of the input sequence is
forecasted. After several trial errors using different epochs,
the epochs with 50 setups were used for the training of the
model. After 50 epochs, there was no longer any variation
in the MSE, thus that number was used. In the result section,
only 20 input sequence results are shown for single and multi-
step. Fig. 2 shows the single-step time series forecasting for
20 sequences of input data. Fig. 3 shows the multi-step time
series forecasting for 20 sequences of input data. In both cases,
experimental results show that the transformer model provides
better performance when compared to the most popular deep
learning models, LSTM and GRU, respectively. From Fig. 4
and Fig. 5, it can be said that the transformer model shows
a lower error when comparing the LSTM and GRU. This
will also be advantageous when forecasting multivariate time
series. In addition, in our investigation, three performance
indicators are used.
Table 2 presents the performance indicators of three differ-
ent neural network models: LSTM, GRU, and Transformer,
for time series prediction tasks. The models were evaluated
using three different sequence lengths: 10, 20, and 50, for
both single-step and multi-step prediction tasks. The evaluationTABLE II
PERFORMANCE INDICATORS OF TRANSFORMER , LSTM, AND GRU.
Step Sequence Model MSE MAE R2
Single-step10LSTM 0.0029 0.396 0.9729
GRU 0.0038 0.552 0.9723
Transformer 0.0010 0.095 0.9923
20LSTM 0.0054 0.442 0.9672
GRU 0.0038 0.610 0.9695
Transformer 0.0008 0.0790 0.9940
50LSTM 0.0023 0.3932 0.9701
GRU 0.0040 0.443 0.9706
Transformer 0.0007 0.0820 0.9958
Multi-step10LSTM 0.0345 0.856 0.7454
GRU 0.0598 1.340 0.5622
Transformer 0.0212 0.480 0.8331
20LSTM 0.345 0.962 0.7496
GRU 0.2876 0.911 0.7812
Transformer 0.1901 0.456 0.8471
50LSTM 0.294 0.845 0.7833
GRU 0.0263 0.876 0.8067
Transformer 0.0103 0.650 0.8431
is divided into two types of prediction tasks: single-step
and multi-step prediction. In single-step prediction, the task
is to predict the next value in the time series, given the
current value. In multi-step prediction, the task is to predict
a sequence of future values, given the current value. For
each model, sequence length, and prediction task, Table 2
reports three performance indicators: MSE, MAE, and R2.
The MSE measures the average squared difference between
the predicted and actual values, and it gives an indication of
the accuracy of the model. The MAE measures the average
absolute difference between the predicted and actual values,
and it indicates how far off the model’s predictions are from
the actual values. The R2measures how well the model ﬁts
the data, with higher values indicating a better ﬁt. Looking at
the results, the transformer model consistently outperformed
the LSTM and GRU models across all sequence lengths and
prediction tasks. Speciﬁcally, for single-step prediction tasks,Fig. 4. MSE loss of single-step WPF for 20 sequences.
 Fig. 5. MSE loss of multi-step WPF for 20 sequences.
the transformer model achieved the lowest MSE and MAE
values and the highest R2coefﬁcients. For the sequence length
of 10, the transformer model has an MSE of 0.001, an MAE
of 0.095, and an R2coefﬁcient of 0.992, which are better than
the corresponding values for the LSTM and GRU models.
For multi-step prediction tasks, the performance difference
between the models is relatively smaller, but the transformer
still achieves the lowest MSE and highest R2coefﬁcient across
all sequence lengths. For instance, for the sequence length of
20, the transformer model has an MSE of 0.1901 and an R2
coefﬁcient of 0.8471, which were better than the corresponding
values for the LSTM and GRU models. the results suggest that
the transformer model is a promising option for time series
prediction tasks, as it defeated the LSTM and GRU models
in terms of accuracy and ﬁt, for single-step and multi-step
prediction tasks.
VI. C ONCLUSION
Forecasting time series is a difﬁcult endeavor because time
series contain both linear and non-linear trends. In this paper,
the transformer model has been used to make predictions
about time series. The transformer has been shown to be
able to capture long-term dependencies in wind power data,
but LSTM and GRU may struggle. The attention-based trans-
former is used to make wind power forecasting faster and more
accurate. To determine the impact of the long sequence input
data, the proposed model applies several input sequences, such
as 10, 20, and 50 processes, to a multi-head attention-based
transformer, LSTM, and GRU. As measurement indices, the
MSE, MAE, and R2were employed. The results showed that
the proposed multi-head attention-based transformer model
performed more effectively than LSTM and GRU by making
the MSE, MAE, and R2values better for input sequences of
10, 20, and 50 for both single-step and multi-step forecasting.
Future efforts will focus on the creation of more efﬁcient and
precise hybrid transformer prediction methods for multivariate
wind power forecasting. Furthermore, the hybrid transformerwill be applied to the irregular time series forecasting model
to handle the model’s irregularity.REFERENCES
[1] P. Scarabaggio, S. Grammatico, R. Carli, and M. Dotoli, “Distributed
demand side management with stochastic wind power forecasting,” IEEE
Transactions on Control Systems Technology , vol. 30, no. 1, pp. 97–112,
2021.
[2] Y . Dong, H. Zhang, C. Wang, and X. Zhou, “Wind power forecasting
based on stacking ensemble model, decomposition and intelligent opti-
mization algorithm,” Neurocomputing , vol. 462, pp. 169–184, 2021.
[3] R. Sarkar, S. Julai, S. Hossain, W. T. Chong, and M. Rahman, “A com-
parative study of activation functions of nar and narx neural network for
long-term wind speed forecasting in malaysia,” Mathematical Problems
in Engineering , vol. 2019, 2019.
[4] Z. Ma and G. Mei, “A hybrid attention-based deep learning approach
for wind power prediction,” Applied Energy , vol. 323, p. 119608, 2022.
[5] J. Zhao, Z.-H. Guo, Z.-Y . Su, Z.-Y . Zhao, X. Xiao, and F. Liu, “An
improved multi-step forecasting model based on wrf ensembles and
creative fuzzy systems for wind speed,” Applied Energy , vol. 162, pp.
808–826, 2016.
[6] T. Dam and A. K. Deb, “Interval type-2 modiﬁed fuzzy c-regression
model clustering algorithm in ts fuzzy model identiﬁcation,” in 2016
IEEE International Conference on Fuzzy Systems (FUZZ-IEEE) . IEEE,
2016, pp. 1671–1676.
[7] ——, “Ts fuzzy model identiﬁcation by a novel objective function based
fuzzy clustering algorithm,” in 2014 IEEE Symposium on Computational
Intelligence in Ensemble Learning (CIEL) . IEEE, 2014, pp. 1–7.
[8] H.-z. Wang, G.-q. Li, G.-b. Wang, J.-c. Peng, H. Jiang, and Y .-t. Liu,
“Deep learning based ensemble approach for probabilistic wind power
forecasting,” Applied energy , vol. 188, pp. 56–70, 2017.
[9] J. Durbin and S. J. Koopman, Time series analysis by state space
methods . OUP Oxford, 2012, vol. 38.
[10] C. Meek, D. M. Chickering, and D. Heckerman, “Autoregressive tree
models for time-series analysis,” in Proceedings of the 2002 SIAM
International Conference on Data Mining . SIAM, 2002, pp. 229–244.
[11] B. R. Marshall, N. H. Nguyen, and N. Visaltanachoti, “Time series
momentum and moving average trading rules,” Quantitative Finance ,
vol. 17, no. 3, pp. 405–421, 2017.
[12] P. J. Brockwell and R. A. Davis, Introduction to time series and
forecasting . Springer, 2002.
[13] T. Dam and A. Deb, “Block sparse representations in modiﬁed fuzzy c-
regression model clustering algorithm for ts fuzzy model identiﬁcation,”
in2015 IEEE Symposium Series on Computational Intelligence . IEEE,
2015, pp. 1687–1694.
[14] T. Dam and A. K. Deb, “A clustering algorithm based ts fuzzy model
for tracking dynamical system data,” Journal of the Franklin Institute ,
vol. 354, no. 13, pp. 5617–5645, 2017.
[15] P. Gomes and R. Castro, “Wind speed and wind power forecasting using
statistical models: autoregressive moving average (arma) and artiﬁcial
neural networks (ann),” International Journal of Sustainable Energy
Development , vol. 1, no. 1/2, 2012.
[16] D. Salinas, V . Flunkert, J. Gasthaus, and T. Januschowski, “Deepar:
Probabilistic forecasting with autoregressive recurrent networks,” Inter-
national Journal of Forecasting , vol. 36, no. 3, pp. 1181–1191, 2020.
[17] V . Le Guen and N. Thome, “Shape and time distortion loss for training
deep time series forecasting models,” Advances in neural information
processing systems , vol. 32, 2019.
[18] G. Lai, W.-C. Chang, Y . Yang, and H. Liu, “Modeling long-and
short-term temporal patterns with deep neural networks,” in The 41st
International ACM SIGIR Conference on Research & Development in
Information Retrieval , 2018, pp. 95–104.
[19] C. Smith and Y . Jin, “Evolutionary multi-objective generation of recur-
rent neural network ensembles for time series prediction,” Neurocom-
puting , vol. 143, pp. 302–311, 2014.
[20] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural
computation , vol. 9, no. 8, pp. 1735–1780, 1997.
[21] K. Cho, B. Van Merri ¨enboer, D. Bahdanau, and Y . Bengio, “On the
properties of neural machine translation: Encoder-decoder approaches,”
arXiv preprint arXiv:1409.1259 , 2014.
[22] U. Khandelwal, H. He, P. Qi, and D. Jurafsky, “Sharp nearby, fuzzy
far away: How neural language models use context,” arXiv preprint
arXiv:1805.04623 , 2018.
[23] J. J. Webster and C. Kit, “Tokenization as the initial phase in nlp,”
inCOLING 1992 volume 4: The 14th international conference on
computational linguistics , 1992.[24] Y . Lin, I. Koprinska, and M. Rana, “Springnet: Transformer and spring
dtw for time series forecasting,” in International Conference on Neural
Information Processing . Springer, 2020, pp. 616–628.
[25] Q. Wen, T. Zhou, C. Zhang, W. Chen, Z. Ma, J. Yan, and L. Sun, “Trans-
formers in time series: A survey,” arXiv preprint arXiv:2202.07125 ,
2022.
[26] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” Advances in
neural information processing systems , vol. 30, 2017.
[27] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,
Y . Zhou, W. Li, and P. J. Liu, “Exploring the limits of trans-
fer learning with a uniﬁed text-to-text transformer,” arXiv preprint
arXiv:1910.10683 , 2019.
[28] A. Mullick, S. Pal, P. Chanda, A. Panigrahy, A. Bharadwaj, S. Singh,
and T. Dam, “D-fj: Deep neural network based factuality judgment,”
Technology , vol. 50, p. 173, 2019.
[29] N. Wu, B. Green, X. Ben, and S. O’Banion, “Deep transformer mod-
els for time series forecasting: The inﬂuenza prevalence case,” arXiv
preprint arXiv:2001.08317 , 2020.
[30] S. Li, X. Jin, Y . Xuan, X. Zhou, W. Chen, Y .-X. Wang, and X. Yan, “En-
hancing the locality and breaking the memory bottleneck of transformer
on time series forecasting,” Advances in Neural Information Processing
Systems , vol. 32, 2019.
[31] B. Lim, S. ¨O. Arık, N. Loeff, and T. Pﬁster, “Temporal fusion transform-
ers for interpretable multi-horizon time series forecasting,” International
Journal of Forecasting , vol. 37, no. 4, pp. 1748–1764, 2021.
[32] H. Song, D. Rajan, J. J. Thiagarajan, and A. Spanias, “Attend and
diagnose: Clinical time series analysis using attention models,” in Thirty-
second AAAI conference on artiﬁcial intelligence , 2018.
[33] G. Zerveas, S. Jayaraman, D. Patel, A. Bhamidipaty, and C. Eickhoff, “A
transformer-based framework for multivariate time series representation
learning,” in Proceedings of the 27th ACM SIGKDD Conference on
Knowledge Discovery & Data Mining , 2021, pp. 2114–2124.
[34] J. Gehring, M. Auli, D. Grangier, D. Yarats, and Y . N. Dauphin, “Con-
volutional sequence to sequence learning,” in International conference
on machine learning . PMLR, 2017, pp. 1243–1252.
[35] J. L. Ba, J. R. Kiros, and G. E. Hinton, “Layer normalization,” arXiv
preprint arXiv:1607.06450 , 2016.
[36] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in Proceedings of the IEEE conference on computer vision
and pattern recognition , 2016, pp. 770–778.
[37] D. Bahdanau, K. Cho, and Y . Bengio, “Neural machine translation by
jointly learning to align and translate,” arXiv preprint arXiv:1409.0473 ,
2014.
[38] D. Britz, A. Goldie, M.-T. Luong, and Q. Le, “Massive explo-
ration of neural machine translation architectures,” arXiv preprint
arXiv:1703.03906 , 2017.
[39] O. Press and L. Wolf, “Using the output embedding to improve language
models,” arXiv preprint arXiv:1608.05859 , 2016.
[40] H. Boonlia, T. Dam, M. M. Ferdaus, S. G. Anavatti, and A. Mullick, “Im-
proving self-supervised learning for out-of-distribution task via auxiliary
classiﬁer,” in 2022 IEEE International Conference on Image Processing
(ICIP) . IEEE, 2022, pp. 3036–3040.
[41] T. Dam, S. G. Anavatti, and H. A. Abbass, “Mixture of spectral
generative adversarial networks for imbalanced hyperspectral image
classiﬁcation,” IEEE Geoscience and Remote Sensing Letters , 2020.
[42] T. Dam, M. M. Ferdaus, M. Pratama, S. G. Anavatti, S. Jayavelu,
and H. Abbass, “Latent preserving generative adversarial network for
imbalance classiﬁcation,” in 2022 IEEE International Conference on
Image Processing (ICIP) . IEEE, 2022, pp. 3712–3716.
[43] T. Dam, M. M. Ferdaus, S. G. Anavatti, S. Jayavelu, and H. A.
Abbass, “Does adversarial oversampling help us?” in Proceedings of
the 30th ACM International Conference on Information & Knowledge
Management , 2021, pp. 2970–2973.