IMPROVED GENERALIZATION WITH DEEP NEURAL OPERATORS
FOR ENGINEERING SYSTEMS : PATH TOWARDS DIGITAL TWIN
A P REPRINT
Kazuma Kobayashi
Nuclear, Plasma & Radiological Engineering
University of Illinois at Urbana-Champaign
Urnaba, IL 61801, USAJames Daniell
Nuclear Engineering and Radiation Science
Missouri University of Science and Technology
Rolla, MO 65409, USA
Syed Bahauddin Alam
Nuclear, Plasma & Radiological Engineering
University of Illinois at Urbana-Champaign
Urnaba, IL 61801, USA
April 30, 2024
ABSTRACT
Neural Operator Networks (ONets) represent a novel advancement in machine learning algorithms,
offering a robust and generalizable alternative for approximating partial differential equations (PDEs)
solutions. Unlike traditional Neural Networks (NN), which directly approximate functions, ONets
specialize in approximating mathematical operators, enhancing their efficacy in addressing complex
PDEs.In this work, we evaluate the capabilities of Deep Operator Networks (DeepONets), an ONets
implementation using a branchâ€“trunk architecture. Three test cases are studied: a system of ODEs,
a general diffusion system, and the convectionâ€“diffusion Burgersâ€™ equation. It is demonstrated
that DeepONets can accurately learn the solution operators, achieving prediction accuracy (R2)
scores above 0.96 for the ODE and diffusion problems over the observed domain while achieving
zero-shot (without retraining) capability. More importantly, when evaluated on unseen scenarios
(zero-shot feature), the trained models exhibit excellent generalization ability. This underscores
ONetsâ€™ vital niche for surrogate modeling and digital twin development across physical systems.
While convectionâ€“diffusion poses a greater challenge, the results confirm the promise of ONets and
motivate further enhancements to the DeepONet algorithm. This work represents an important step
towards unlocking the potential of digital twins through robust and generalizable surrogates.
1 Introduction
Machine Learning (ML) models have become a popular and important tool for solving engineering problems in complex
systems. Due to this growth, new ML techniques are being developed to solve problems that other traditional or
ML models need to be equipped with. One method receiving significant development is Operator Learning. While
traditional ML models such as Neural Networks (NNs) approximate a solution by mapping one function to another,
operator learning strategies opt to approximate an operator which can more generally map a relationship with no regard
to input or output dimensionality Lu et al. [2021a], Kovachki et al. [2021], Wang et al. [2022], Li et al. [2022a], Lu et al.
[2022a], Garg et al. [2022a]. Utilizing this response space conveys benefits in engineering problems that occur over a
specific space, specifically physical spaces that can be effectively digitally approximated or complex spaces in which
the parameters interact significantly. These models, when developed, are commonly referred to as Operator Networks
(ONets). Due to their displayed strengths, ONets are primed to find an impactful and vital niche in computational
modeling for engineering solutions.
One of the primary benefits of operator learning is its capability to map functions over a possibly infinite-dimensional
space. NNs map inputs directly to outputs, resulting in an undefined domain unless normalized by activation functionsarXiv:2301.06701v3  [cs.LG]  29 Apr 2024Deep neural operators for engineering systems: Path towards digital twin A P REPRINT
Kovachki et al. [2021]. This can result in unreliable predictions when providing inputs outside of bounds used for
training Zhu et al. [2022]. Conversely, by defining the space over which the approximated operator performs, a holistic
view of the modeled system or phenomena can be developed Lu et al. [2022a], Lin et al. [2021a]. Functioning over a
defined domain also simplifies physical engineering problems. 2 or 3-dimensional spaces can be simply handled by the
model and used to represent a physical cross-section or volume Lu et al. [2022a], Yin et al. [2022], Lin et al. [2021b].
Furthermore, this allows the operator to learn effects from sensor readings both locally and globally Lu et al. [2022a],
Wen et al. [2022], Li et al. [2022b].
However, the unique architecture of operator learning models requires inputs and training to be handled differently from
traditional NNs. Domain and feature information must be input and defined separately Lu et al. [2021a], de Hoop et al.
[2022]. Consequently, operator learning exacerbates the data requirement problem already present in NNs Garg et al.
[2022a], de Hoop et al. [2022], Pickering and Sapsis [2022]. Furthermore, data preprocessing becomes more robust due
to the input and output formats. Larger amounts of data result in large computational requirements in training cycles.
RAM bottlenecks are common due to the spatial information required in both the input and output since the domain is
provided with the observed function. These issues prevent operator learning models from being used universally in
problems where standard NNs or ML models may be more efficient.
Due to the unique benefits of operator learning compared to other ML models, these techniques are expected to
be used for modeling physical spaces such as cross-sections or volumes. The strengths of operator learning, such
as its continuous nature and predefined space, contribute positively to the analysis of physical systems, especially
high-importance systems which may be difficult to maintain instrumentation in Li et al. [2022a], Lu et al. [2022a], Yin
et al. [2022]. Diffusion problems are primed for operator learning since the domain of the problem can be defined
by the developer, and a prediction can be obtained at any point inside that domain Lu et al. [2022a], Li et al. [2020],
Tripura and Chakraborty [2022]. This type of problem is also beneficial since visualization in this space is simple and
intuitive. Additionally, diffusion problems can be posed in multiple dimensions, and data can be generated or collected
more easily than in other data-driven problems Li et al. [2022a], Lu et al. [2022a]. Materials analysis will likely be an
area of ONet learning using 2-dimensional cross sections. Materials problems such as stress distribution, temperature
distribution, cracking, or material phase can be represented in this space, allowing a robust collection of input features
through the branch network Lu et al. [2022a].
This paper aims to understand the feasibility and capability of DeepONets, a type of operator learning method, in
solving generalized engineering problems such as a system of ODEs, diffusion-reaction, and convection-diffusion.
Therefore, three test cases are shown using DeepONets. These test cases can be used to demonstrate the capability of
DeepONets by operator approximation. Each test case utilizes fully-connected NNs for branch and trunk networks to
retain mathematical significance by the generalized Universal Approximation Theorem for operators.
2 Literature on Operator Learning
2.1 Neural Networks
Based on the architectural structure of the brain, NNs function by accepting inputs and feeding them through a series of
layers containing artificial neurons to make a prediction. NNs aim to increase their prediction accuracy by updating
neurons with a pre-defined learning rule, typically by using the prediction error of a single or group of samples Yu et al.
[2002], Schmidhuber [2015]. This results in an NNâ€™s ability to continually approach accurate solutions if given sufficient
information and trained effectively. Consequently, NNs are valuable tools for function approximation for problems
where a function is not known or analytically solvable but where data is available Anastassiou [2011], Cardaliaguet and
Euvrard [1992], Selmic and Lewis [2002].
NNs, as one of the most commonly used ML methods for engineering problem solving, are sufficient for simple systems
or basic function approximation. NNs function based on the Universal Approximation Theorem, effectively stating
that they can capture both linear and nonlinear functions by optimizing the neuron weights to reduce prediction error
Cardaliaguet and Euvrard [1992]. This is especially true when modeling a single physical phenomenon or mathematical
function.
However, NNs struggle to make accurate predictions when modeling complex problems, such as those with many
component phenomena or performing outside the data used to train the model. This can become problematic for
simple NNs when training data is not available in the same regions as desired predictions or where the individual
phenomena cannot be distinguished from one another Ruthotto and Haber [2020]. While multiple NNs can be used to
capture system behavior in different regions or for different phenomena, information about the interaction between
these locations can be lost, and the development time for these models increases.
2Deep neural operators for engineering systems: Path towards digital twin A P REPRINT
By examining these drawbacks, it can be seen that an ML model that can holistically capture a system with the same
self-optimization capabilities of NNs is desirable for complex engineering systems. More specifically, data-driven
models that can predict system behavior more generally than the functional level would be useful for engineering
problem-solving and analysis.
2.2 Modern Operator Learning Methods
Operator learning methods have begun to be explored for scientific applications with three primary types of models:
Fourier Neural Operators (FNO) Kovachki et al. [2021], Li et al. [2022a], Guan et al. [2021], Wavelet Neural Operators
(WNO) Tripura and Chakraborty [2022], Thakur et al. [2022], and DeepONets Lu et al. [2021a], Wang et al. [2022],
Goswami et al. [2022]. Each model satisfies the Universal Approximation Theorem for Operators, although they are
slightly different from a design perspectiveLu et al. [2021a, 2022b]. This allows the models to achieve the goal of
producing an approximation that is continuous in an infinite dimensional output spaceGorban and Wunsch [2002], Chen
and Chen [1995], de Hoop et al. [2022], Pickering and Sapsis [2022].
FNOs and WNOs utilize a different approach from DeepONets, which utilize concepts native to deep learning NNs
to produce an approximation Kovachki et al. [2021], Li et al. [2020], Lu et al. [2022b]. FNOs and WNOs use
transformations to produce an approximation in the infinite-dimensional response space Kovachki et al. [2021], Li et al.
[2020], Lu et al. [2022b]. FNOs and WNOs both use a NN based front-end for filtering information and generalizing
inputs, which are then transformed into the response space using a Fourier transform for FNOs or a generalized wavelet
transform for WNOs Li et al. [2022a], Kovachki et al. [2021]. The NN portions of the architecture for these models are
trained using backpropagation methods in order to approximate the desired operator more accurately. These models are
useful for multiphase flow and materials deformation problems, as well as general PDE solutions in which information
about the phenomenon is known Li et al. [2022a], Wen et al. [2022], You et al. [2022], Li et al. [2022b], Tripura and
Chakraborty [2022]. Furthermore, due to the ability to temporally synchronize information from data, WNOs are useful
for uncertainty quantification over time Pickering and Sapsis [2022], Thakur et al. [2022], Tripura and Chakraborty
[2022].
DeepONets directly utilize methods native to deep learning problem solving instead of the transformations in FNOs and
WNOs and inherit directly from NN design Lu et al. [2021a], Marcati and Schwab [2021]. A dot product combines
information from the branch and trunk networks in the model architecture to approximate the operator Wang et al.
[2022], Yin et al. [2022]. This methodology allows for mapping finite inputs to the infinite response space Lu et al.
[2021a], Wang et al. [2022], Lu et al. [2022b]. Additionally, DeepONets have seen active development for some
scientific applications through Python libraries such as DeepXDE Lu et al. [2021b]. This has allowed scientists and
engineers to develop ML-based techniques for general PDE approximation Marcati and Schwab [2021], Lin et al.
[2021a], Pickering et al. [2022].
Furthermore, the DeepONet architecture has been modified for additional applications due to its relative simplicity.
Similar to the Physics Informed Neural Network (PINN), Physics Informed Neural Operators (PINOs) Goswami et al.
[2022], Li et al. [2021] have been developed to improve physics synchronization for applications in which some physics
knowledge is known Goswami et al. [2022]. Since DeepONet approximations can be examined continuously, the
data-physics fusion approach can be used to develop highly reliable models of systems Lu et al. [2022a]. Furthermore,
for applications with minimal data, multi-fidelity approaches to operator learning using DeepONets have been developed
Lu et al. [2022a]. These methods utilize low-fidelity data from equations or empirical models alongside sensor data to
form an approximation based on both datasets Thakur et al. [2022], Lu et al. [2022a].
2.3 Research Gaps
The primary research gap with modern operator learning methods is due to the types of systems and associated
applications. Due to their recent development, operator learning methods focus on analytically solvable problems for
which data can be generated. This is because operator learning methods can be applied without experimental data
and easily checked against ground truth values with these problems. However, data-driven prediction for unknown
or unsolved systems has not been explored using operator learning principles. While this is the predicted use case of
approximated operators from these models, the large data requirement for producing accurate and robust results poses a
challenge in most industries. Additionally, available sensors and instrumentation in engineering systems result in a
data-sparse environment where sensor locations remain constant. Therefore, robust operator approximation methods
must be developed for data-driven methods in data-sparse environments that can safely make predictions, ideally
without being forced to make assumptions. This is where the DeepONet, a natively data-driven method, can be utilized.
3Deep neural operators for engineering systems: Path towards digital twin A P REPRINT
3 Operator Learning & Digital Twin Applications
As modern engineering systems are developed, so do approaches to explaining these systems. Advances in instrumenta-
tion and controls have led to systems that can collect significant amounts of information compared to their previous
counterparts. This data and information have been used for validation and experimental correlations. However, the
rapid emergence of data-driven explanation methods, including ML models, has expanded the applications and value
of the data. Since information is becoming an increasingly vital part of engineering systems, methods to aggregate
and adequately explain this information are necessary Bonney et al. [2022]. This is the role of digital twins (DTs) in
modern engineering systems.
By combining previous knowledge, data-driven methods, and real-time information, a DT may be developed to
encapsulate knowledge about a system and present it in a way humans can understand. Additionally, with physics
knowledge and data-driven methods, advanced DTs can utilize information to make predictions about the system or
produce information about system components that sensors cannot observe. However, suppose information from a DT
is required during operation. In that case, it must be developed such that computation time is sufficiently short enough
to produce these results when the information is needed Kabir [2010a,b], et al. [2010a,b]. This can be challenging
due to the accuracy tradeoff for multi-physics simulations, which typically require increased computation time for
high-accuracy results. Modern ML methods can help solve this issue by providing computationally cheap estimates for
system parameters, which can be used alongside physics equations to provide sufficiently accurate information for the
DT Kobayashi et al. [2022a,b].
Ultimately, DTs in complex engineering systems are plausible because of the increased utility of information. More
specifically, the Internet of Things (IoT) approach to modern systems increases the viability and capability of DTs in
engineering systems. A framework for producing DT-type depictions of complex systems can begin to form by linking
sensors and aggregating this information. IoT approaches network information, sensors, and system components in a
way that can be used externally. DTs can be used alongside the IoT to synchronize information, which is used alongside
the approaches above to predict, extrapolate, and generalize data to produce a more complete picture of the system
Bonney et al. [2022].
By their design criteria, DeepONets lend themselves to DT development in complex engineering systems. Since
DeepONets can map infinite-dimensional function spaces to infinite-dimensional output spaces, the development of
supporting DT modules can be generalized for a system, and additional sensors or components can be added without
overhauling model development. Furthermore, DeepONets can robustly predict system behavior with a well-defined
system space due to their ability to generalize phenomena versus direct equation mapping from traditional NNs or other
finite-dimensional ML models Lin et al. [2021a]. DeepONets could also be used as supplementary controller modules
inside of DTs, which aggregate information from single-physics models and determine system behavior and interaction
without using computationally expensive multi-physics modeling directly Kobayashi et al. [2022a,b]. In this sense,
developing DTs for complex systems is directly correlated to developing effective and computationally cheap neural
operators for advanced systems.
There are five essential components in our proposed DT framework Kobayashi et al. [2023]: (1) prediction module,
(2) Real-time update module for â€œOn the Fly" temporal synchronization, (3) data processing module, (4) visualization
module, and (5) decision-making module. Also, there are two important components to ensure trustworthiness and
reliability, which are (6) ML Risk analysis and (7) ML Reliability, and (8) Trustworthy AI. Figure 1 shows the proposed
Intelligent DT Framework Kobayashi et al. [2023], and initially, this methodology has been proposed by Garg et al.
[2022b]. According to the figure, the ML algorithm collects data from a physical asset and a high-fidelity simulation
that integrates both high- and low-fidelity data streams. The ML algorithm analyzes the multi-fidelity data to make
predictions of interest, with the predicted output stored on a server. The prediction and system update modules require
sophisticated ML algorithms. A key component is the system update module based on temporal synchronization, which
utilizes a Bayesian filter combined with an ML algorithm to enable real-time, on-the-fly temporal synchronization
Kobayashi et al. [2023]. Figure 1 shows the ML components (Red Boxes) exploited in different segments of the DT
framework for prognostic. Overall, the system leverages Bayesian statistical modeling fused with ML techniques for
prediction and continuously updating the model state based on streaming multi-fidelity data from the physical system
and simulations.
In addition, Figure 1 shows an explainable and trustworthy AI component. To meet NISTâ€™s Tabassi [2023] definition
requirement (released on 03/17/2022) on â€œTrustworthy AI & ML Risk Management Framework,â€ Zhang [June 29,
2022,J] specific tests need to be performed such as (1) Robustness Test under covariate perturbation and worst sampler
resilience to measure model sensitivity to variations in uncontrollable factors. (2) Prediction Reliability Test by split
conformal prediction, segmented bandwidth, and distribution shift (Reliable vs. unreliable) to ensure the model
consistently generates the same results within the bounds of acceptable statistical error, and (3) Resilience Test for
4Deep neural operators for engineering systems: Path towards digital twin A P REPRINT
prediction performance degradation by worst-case subsampling and under out-of-distribution scenarios. Furthermore,
assessing the reliability Zhang [June 29, 2022,J] of machine learning predictions is important for understanding where a
model makes less reliable forecasts. Wider prediction intervals indicate less reliable predictions. Quantifying prediction
reliability can be done with split conformal prediction, assuming exchangeability. For binary classifiers, reliability
diagrams show probability calibration relative to empirically observed success rates. Validation trustworthiness is
confirmed by incrementally adding additional noise to all measured parameters to determine if the signal-to-noise (SNR)
ratios were to degrade by an additional X% that will assess the comfort margin. Furthermore, to ensure trustworthiness,
it requires managing both aleatory and epistemic uncertainties while explicitly accounting for the multiscale uncertainty
in specific problem cases.
The proposed digital twin also aims to implement explainable AI (XAI) for decision-making and observation of
anomalies by adversarial robustness and semantic saliency, currently under development by the authors. These methods,
if successful, will assess the theoretical basis and practical limits of the explainability of the neural operator algorithm
used. In our future work, we will employ a proof-based method to probe all hidden layers of the neural operator model,
identifying the most important layers and neural networks involved in predictions to ensure prediction explainability.
Although the components of DT vary depending on its purpose, the standard functions that form its core are prediction
and system parameter updating using real-time data sent from sensors installed in the physical system. Because of its
property of mapping between input and output functions, ONets can be a viable option in implementing these essential
functions. In this section, we will briefly explain prediction, system parameter update, and utilization of ONets to them.
One of the purposes of DT is to make a real-time prediction of the systemâ€™s behaviors using sensor data from a physical
system. Although DT is sometimes confused with simulation, this difference distinguishes the two. In a simulation,
the user provides input data to the simulation solver (e.g., FEM, FDM, FVM), such as the operating conditions of the
system and material properties, and obtains predictions as output. However, the computational cost of simulation is a
critical issue when targeting complex systems. Simulating the fluid velocity profile in a complex system using a FEM
solver can take hours. Therefore, even if the solver solves the system state using data from the sensors, there is a lag
until the calculation results are obtained; DT aims to minimize this lag as much as possible and predict the system state
immediately. To solve this issue, surrogate modeling methods have been utilized as a new approach in recent years
Kobayashi et al. [2022a], Daniell et al. [2022], Rahman et al. [2022].
In this approach, a solver predicts the system state under various input variable conditions (initial conditions, boundary
conditions, material properties, etc.) in advance and builds an ML model using these as training data. Figure 2 shows the
relationship between conventional simulation and surrogate modeling. The constructed ML model returns immediate
predictions for the input variables. Therefore, it can satisfy DTâ€™s requirements. Although there are several supervised
ML methods (NN, PINN, MFNN), ONets could be a new option when building this surrogate model.
DT requires temporal synchronization of system parameters between the digital and physical systems to predict future
system states. For all practical purposes, the systemâ€™s lifecycle is much slower (in months or years) than the system
dynamicsâ€™ time scale. Therefore, a temporal synchronization of system variables in a slow time scale is necessary for
prediction over the operational life of a system. The importance of the system parameter update is explained in the
following. Letâ€™s assume the system is a mass-spring system and described as a function of dynamicsâ€™ time-scale ( ğ‘¡) and
slow time-scale ( ğ‘¡ğ‘ ):
ğ‘€(ğ‘¡ğ‘ )ğœ•2ğ‘‹(ğ‘¡,ğ‘¡ğ‘ )
ğœ•ğ‘¡2+ğ¾(ğ‘¡ğ‘ )ğ‘‹(ğ‘¡,ğ‘¡ğ‘ )=0 (1)
whereğ‘€(ğ‘¡ğ‘ )is the mass and is assumed to be a constant over the lifecycle, ğ¾(ğ‘¡ğ‘ )is the spring constant, and ğ‘‹(ğ‘¡,ğ‘¡ğ‘ )is
the system state. If the value of the spring constant is given, the system state can be obtained by solving ODE. Figure
3 shows the sample solutions as solid blue lines. However, assuming this system will operate for approximately 20
years, system variables may change gradually on a slow time scale. Various environmental factors, such as temperature,
pressure, and even irradiation, can cause the degradation of system parameters. The orange and green solid lines in
Figure 3 represent the system states when the decreasing of the spring constant is taken into account. As Figure 3 shows,
system parameters over long time scales affect the system state and require their updating to compensate.
The use of Bayesian filters is an option to implement system parameter updates. The Unscented Kalman Filter (UKF)
is expected among the filters due to its superior performance for higher-order nonlinear systems. While the UKF
is generally not suited for long operation lifetimes, a new algorithm that employs an analytical resampling process
combined with ML is developed Garg et al. [2022b]. Although the details of filter design in this study will not be
examined, it is essential to note that the ML method can be used to extend the conventional UKF for DT. While Garg
et al. [2022b] employed the Gaussian process (GP) as a supervised ML, they also suggested replacing it with other ML
5Deep neural operators for engineering systems: Path towards digital twin A P REPRINT
Figure 1: Intelligent Digital Twin Framework with Explainable AI and Interpretable ML module. The diagram shows
the ML components (Red Boxes) exploited in different segments of the digital twin framework Kobayashi and Alam
[2023].
6Deep neural operators for engineering systems: Path towards digital twin A P REPRINT
Figure 2: Concept of surrogate modeling method. The surrogate model can return predictions immediately when the
input variables are given. The demand for conventional simulations has not disappeared to prepare the training data for
ML modeling.
Figure 3: Acceleration and position of the mass-spring system described by equation 1. ğ‘‡ğ‘ =0represents the ideal
solutions.ğ‘‡ğ‘ =4000 andğ‘‡ğ‘ =8000 represent system operation time in days.
7Deep neural operators for engineering systems: Path towards digital twin A P REPRINT
algorithms. The basic concept of system parameter updates is presented in Figure 4, and we expect to utilize ONets
instead of GP.
Figure 4: Concept of update module in DT - synchronizing the physical system and DT Kobayashi et al. [2023].
4 Governing Principles of Operator Learning
Much like NNs, DeepONets were developed using the concept of universal approximation, although in this case,
the Universal Approximation Theorem for Operators is examined Lu et al. [2021a]. NNs have been traditionally
utilized to map inputs into a designated function space. In contrast, DeepONets are designed to transform information
from functional forms into operators applicable within an arbitrary domain. Within the DeepONet architecture, input
functions undergo discretization via sampling at specific locations, represented as {ğ‘¥1,ğ‘¥2,...,ğ‘¥ğ‘š}, whereğ‘šdenotes
the total number of discretized points. This approach enables DeepONet to adeptly manage two types of network inputs:
[ğ‘¢(ğ‘¥1),ğ‘¢(ğ‘¥2),...,ğ‘¢(ğ‘¥ğ‘š)]ğ‘‡andğ‘ƒ, which respectively correspond to the sampling positions and system output ğ‘ . The
efficacy of this method is grounded in the Universal Approximation Theorem for Operators, formalized as Eq. 2.
ğº(ğ‘¢)(ğ‘ƒ)âˆ’ğ‘™âˆ‘ï¸
ğ‘˜=1ğ‘›âˆ‘ï¸
ğ‘–=1ğ‘ğ‘˜
ğ‘–ğœÂ©Â­
Â«ğ‘šâˆ‘ï¸
ğ‘—=1ğœ‰ğ‘˜
ğ‘–ğ‘—ğ‘¢(ğ‘¥ğ‘—)+ğœƒğ‘˜
ğ‘–ÂªÂ®
Â¬|                              {z                              }
branchğœ(ğ‘¤ğ‘˜Â·ğ‘ƒ+ğœğ‘˜)|            {z            }
trunk<ğœ– (2)
This equation shows that a prediction can be generated by utilizing neurons for information filtering, which is then
compared to the ground truth value, much like NNs. Unlike NN training, an operator ğº(ğ‘¢)(ğ‘ƒ)is tested against instead
of a sensor value. In this case, ğº(ğ‘¢)(ğ‘ƒ)represents the operator ğºperforming on sensor value(s) ğ‘¢over domain ğ‘ƒ.
The summation used in (2)is similar to the standard Universal Approximation Theorem, which implies that a function
object like a NN could be used in this location. Additionally, since ğ‘¢in this equation represents some function, the
feature space can be infinitely large.
ğº(ğ‘¢)(ğ‘ƒ)â‰ˆğ‘™âˆ‘ï¸
ğ‘˜=1ğ‘ğ‘˜(ğ‘¢(ğ‘¥1),ğ‘¢(ğ‘¥2),...,ğ‘¢(ğ‘¥ğ‘š))|                              {z                              }
branchğ‘¡ğ‘˜(ğ‘ƒ)|{z}
trunk(3)
ğº(ğ‘¢)(ğ‘ƒ)âˆ’âŸ¨g(ğ‘¢(ğ‘¥1,ğ‘¢2,Â·Â·Â·,ğ‘¢(ğ‘¥ğ‘š))|                         {z                         }
branch,f(ğ‘ƒ)|{z}
trunkâŸ©<ğœ– (4)
The Universal Approximation Theorem for Operators can be manipulated such that the sensor and domain information
can be separated to draw implications about model architectures required for predictions. Represented as 3, the sensors
8Deep neural operators for engineering systems: Path towards digital twin A P REPRINT
Figure 5: DeepONet Branch-Trunk Architecture following the proposed approach from Lu et al. [2021a]. The training
dataset is composed of (1) input function ğ‘¢(ğ‘¥ğ‘š), (2) sampling positions ğ‘ƒfor the system output, and (3) system output
ğ‘ at the position ğ‘ƒ.
and domain can be input separately to a model Lu et al. [2021a], Chen and Chen [1995]. For DeepONets, a branch-trunk
architecture is used. The branch is applied to encode sensor information while the trunk is applied to encode domain
information Lu et al. [2021a], Chen and Chen [1995]. By utilizing this architecture, the operator ğºcan be approximated.
Furthermore, by utilizing 4, predictions from the operator approximation can be compared to ground truth observations
to develop a way to train the model. A single NN is used for the domain information for the trunk f, while one or
multiple NNs are used to encode sensor information from the function for the branch g. This â€˜encodingâ€™ is handled
internally by the DeepONet by filtering the input information through the section of the model aimed at using that data.
Two different branch architecture types can be used for DeepONet development; â€˜stackedâ€™ DeepONets utilize multiple
NNs in the branch while â€˜unstackedâ€™ DeepONets utilize only a single NN for the branch Lu et al. [2021a, 2022b]. The
branch and trunk networks are then utilized to approximate the operator for the system, which is trained using the loss
associated with the prediction. This architecture can be seen in figure 5. In this case, ğ‘¢represents the observed function,
andğ‘ƒrepresents the domain over which it occurs. ğº(ğ‘¢)(ğ‘ƒ)represents the prediction generated via the operation of the
approximated operator ( ğº) on the received data, ğ‘¢andğ‘ƒ. This is iterated over the training process with exposure to
multiple samples (combinations of ğ‘¢andğ‘ƒ) to adjust neuron values in the model based on results from the loss function
to approach a more accurate operator approximation.
DeepONet input structure was developed to allow any number of observations from ğ‘¢to be utilized. Each sensor on ğ‘¢
collects information from the system at those points, which can be used for operator learning to improve approximations.
Additionally, input and output dimensions become trivialized by using the concepts from the Universal Approximation
Theorem for Operators. The relationship between an infinite dimensional input space and an infinite dimensional output
space can be characterized by approximating a generalized operator. This is contrary to traditional ML models, which
can be viewed as a parametric map of some finite input function to the output function.
9Deep neural operators for engineering systems: Path towards digital twin A P REPRINT
5 Test Problems & Results
In this section, the versatility and effectiveness of DeepONets are demonstrated by applying them to various simple
systems. Specifically, three scenarios were investigated: (1) a system of ODEs, (2) a 1-dimensional diffusion-reaction
process, and (3) a 1-dimensional convection-diffusion-reaction phenomenon. These case studies showcased the
robustness and applicability of DeepONets in various dynamic systems, highlighting their potential as a powerful
modeling tool.
For each problem, the solutions obtained from solving the equations with the solver were utilized as training data to
construct a surrogate model using DeepONet. The performance of the surrogate model was assessed by calculating
several evaluation metrics using the test data. These metrics, including R2(Coefficient of Determination), MSE (Mean
Squared Error), RMSE (Root Mean Squared Error), MAE (Mean Absolute Error), and RMSE/MAE1provided valuable
insights into the accuracy and reliability of the DeepONets surrogate model.
Furthermore, a comparative analysis was conducted between DeepONets and conventional machine learning modeling
methods, namely FCN (fully connected neural network) and CNN (convolutional neural network), for test cases with
the highest and lowest R2values. This Comparison allowed for an evaluation of the performance of DeepONets in
relation to established modeling approaches.
This comprehensive evaluation showcases the effectiveness of DeepONets as a versatile and accurate modeling approach
capable of outperforming traditional machine learning methods in capturing the complexities of dynamic systems. The
implementation of DeepONets was done using the scientific machine learning library DeepXDE Lu et al. [2021b].
5.1 Setup of a System of ODEs
In the foundational problem addressed by this study, we scrutinize a system of ODEs encapsulated by equation 5. The
focal point of this analysis is to discern the operator that governs the relationship between the input function ğ‘¢(ğ‘¥)and
the output state function ğ‘ (ğ‘¥)over the domain ğ‘¥âˆˆ(0,1], while honoring the initial condition ğ‘ (0)=0.
(
ğ‘‘ğ‘ (ğ‘¥)
ğ‘‘ğ‘¥=ğ‘¢(ğ‘¥), ğ‘¥âˆˆ(0,1]
ğ‘ (0)=0(5)
The state function ğ‘ (ğ‘¥), arising as a solution to the ODE, is a scalar field with its dimensionality explicitly defined as
one. Mathematically, this is characterized by the mapping ğ‘ :Râ†’R, signifying that for each real-valued point ğ‘¥within
our domain, there is a corresponding real-valued scalar state ğ‘ (ğ‘¥).
To curate the training data for the branch network, we opted for a 1-dimensional Gaussian random field (GRF) as
the stochastic input function ğ‘¢(ğ‘¥). A compendium of 150 samples of ğ‘¢(ğ‘¥)was generated, each uniformly probed at
predefined sensor nodes {ğ‘¥1,ğ‘¥2,Â·Â·Â·,ğ‘¥100}, as demonstrated in the left panel of Figure 6. This systematic sampling
approach was devised to secure a consistent and orderly dataset, which is essential for the efficacious training of our
neural network.
The training data for the trunk network was generated by solving the ODE using the Runge-Kutta (RK) method with
1,000 steps for each corresponding input function, as depicted in the right panel of Figure 6. From the obtained solutions,
100ğ‘ (ğ‘¥)values were randomly selected for each input function. This selection process aims to capture diverse solution
patterns and facilitate comprehensive learning of the operator mapping from input to output functions.
It is worth emphasizing that the 100 random solution sampling locations for each input function were fixed and
maintained throughout the dataset. This approach, commonly called "aligned data" Lu et al. [2021b], ensures consistent
and comparable training and evaluation of the models.
Similarly, a test dataset was prepared consisting of 1,000 random input functions. This independent test dataset evaluates
the modelâ€™s generalization ability and thoroughly assesses its performance on unseen data.
The architecture of DeepONet is defined as follows: both the branch and trunk networks are fully connected neural
networks. The branch network has a size of [100, 40, 40], while the trunk network has a size of [1, 40, 40]. The
activation functions utilized in both networks are Rectified Linear Units (ReLU). Weight initialization is performed
using the Glorot initialization method, which ensures effective initialization of the network parameters. It is important
1When only random noise remains as the error in a model, causing it to conform to a normal distribution, the ratio of RMSE to
MAE is approximately 1.253.
10Deep neural operators for engineering systems: Path towards digital twin A P REPRINT
Figure 6: Test Case from 5.1: Left pane represents 150 input function ğ‘¢(ğ‘¥)sampled from 1-dimensional GRF. The
numerical solution obtained using the RK method is represented in the right pane.
to note that the same activation functions and initialization methods are employed in the subsequent problems discussed
in Sections 5.2 and 5.3.
The Adam optimization algorithm is chosen as the optimization method during the training process. The mean ğ¿2
relative error is used as the evaluation metric to measure the performance of the model. The model is trained for 10,000
iterations. The learning rate for training is set to 0.001, which controls the step size during gradient descent and affects
the convergence speed and accuracy of the training process. These choices of optimization algorithm, evaluation metric,
and learning rate are consistent across the problems discussed in this paper, ensuring a fair and comparable evaluation
of the models.
When applying traditional NN modeling approaches like FCNs or CNNs to ODEs and partial differential equations
(PDEs), separate models need to be built for each specific condition, such as initial and boundary conditions. Con-
sequently, for a problem involving 150 conditions (input function patterns), it would be necessary to construct 150
individual models.
In contrast, ONets, as mentioned earlier, are designed to establish a mapping between input and output functions. As a
result, a single DeepONet model can effectively handle multiple conditions within the trained input function domain.
This advantage significantly reduces the complexity and computational burden associated with constructing separate
models for each condition, making DeepONets a more efficient and versatile modeling approach.
5.1.1 Results of Ordinary Differential Equation
The performance evaluation of the DeepONet model on the test data is summarized in Table 1, providing key summary
metrics such as the mean, standard deviation, minimum, and maximum values. The model consistently achieved
convergence in terms of R2, with an average value of approximately 0.997, indicating a high level of agreement between
the predicted and actual values. This demonstrates the effectiveness of the DeepONet model in capturing the underlying
patterns in the data.
A detailed analysis of the modelâ€™s performance is illustrated in Figure 7, where the Distribution of evaluation metrics is
presented. Notably, the R2scores exhibit strong performance across the test dataset, with the lowest observed value of
0.532. While this suggests that there are certain instances with relatively larger deviations between the predicted and
ground truth values, the overall performance of the model remains highly reliable.
Further examination of the metrics reveals that MSE, RMSE, and MAE values are consistently low, ranging from 10âˆ’4
to10âˆ’2. These results, depicted in Figures 7 (b), (c), and (d), indicate the modelâ€™s ability to capture the underlying
dynamics of the problem accurately. The relatively small magnitudes of these errors highlight the effectiveness of the
DeepONet model in accurately predicting the target variable.
11Deep neural operators for engineering systems: Path towards digital twin A P REPRINT
The mean ratio of RMSE to MAE, calculated as 1.164 and listed in Table 1, suggests that there may be a uniform
distribution error present in each sample. This information can guide further analysis and improvement of the model to
address potential sources of error.
To evaluate the modelsâ€™ performance more comprehensively, a comparison was conducted to determine the highest
and lowest prediction accuracies relative to FCN and CNN. These results are summarized in Table 2, offering valuable
insights into the differential performance of DeepONet compared to traditional methods (FCN and CNN). Furthermore,
Figure 8 presents a specific test case, showcasing the Comparison between simulations and ML predictions. The left
figures display the simulation results, while the right figures illustrate the predictions made by DeepONet, FCN, and
CNN. Notably, for Test ID 122, which exhibits the highest R2score, Figures 8 (a) and (b) clearly demonstrate that
the predictions of DeepONet, FCN, and CNN are in approximate agreement and accurately reproduce the simulation
results. However, in the case of Test ID 72, DeepONet exhibits a lower R2score compared to FCN, and CNN fails
to reproduce the simulation result altogether, as shown in Figures 8 (c) and (d). These comparative results provide
valuable insights into the differential performance of DeepONet and traditional methods for specific test cases.
Table 1: Overall performance metrics of the DeepONet model for the system of ODEs
Statistics R2MSE RMSE MAE RMSE/MAE
Mean 9.974Ã—10âˆ’17.420Ã—10âˆ’57.163Ã—10âˆ’36.271Ã—10âˆ’31.164
Std 1.649Ã—10âˆ’21.009Ã—10âˆ’54.788Ã—10âˆ’34.307Ã—10âˆ’38.022Ã—10âˆ’2
Min 5.316Ã—10âˆ’12.861Ã—10âˆ’75.350Ã—10âˆ’43.810Ã—10âˆ’41.040
Max 1.000 6 .372Ã—10âˆ’42.524Ã—10âˆ’22.279Ã—10âˆ’21.722
Table 2: Comparison of the DeepONet model with FCN and CNN for the cases of R2takes highest or lowest
Test ID Models R2MSE RMSE MAE RMSE/MAE
122 (Highest)DeepONet 1.000 2 .000Ã—10âˆ’61.291Ã—10âˆ’31.002Ã—10âˆ’31.289
FCN 9.996Ã—10âˆ’14.848Ã—10âˆ’56.962Ã—10âˆ’35.300Ã—10âˆ’31.314
CNN 9.969Ã—10âˆ’14.107Ã—10âˆ’42.027Ã—10âˆ’21.194Ã—10âˆ’21.395
72 (Lowest)DeepONet 5.316Ã—10âˆ’14.200Ã—10âˆ’56.462Ã—10âˆ’34.531Ã—10âˆ’31.426
FCN 9.484Ã—10âˆ’12.003Ã—10âˆ’61.415Ã—10âˆ’39.501Ã—10âˆ’41.490
CNNâˆ’7.758 3 .398Ã—10âˆ’41.843Ã—10âˆ’21.769Ã—10âˆ’21.042
5.2 Diffusion System
The second problem is a diffusion system, commonly encountered in various engineering fields, including heat transfer,
chemical reactions, and neutron diffusion. The focus is on a time-dependent 1-dimensional diffusion equation given by:
ğœ•ğ‘ 
ğœ•ğ‘¡=ğ·ğœ•2ğ‘ 
ğœ•ğ‘¥2+ğ‘˜ğ‘ 2+ğ‘¢(ğ‘¥) (6)
In this equation, with ğ·=0.01representing the diffusion coefficient and ğ‘˜=0.05denoting the reaction rate, the
domain is defined as ğ‘¥âˆˆ(0,1)andğ‘¡âˆˆ(0,1]. The input function ğ‘¢(ğ‘¥)is considered as the source term. To generate
diverse input functions, we utilize a 1-dimensional Gaussian Random Field (GRF) to create 10,000 patterns of ğ‘¢(ğ‘¥)at
100 fixed sensor positions.
Numerical solutions ğ‘ (ğ‘¥,ğ‘¡)are obtained for each input function by applying the finite difference method (FDM) on a
100Ã—100grid. The process involves randomly sampling 100 points from the grid (ğ‘¥,ğ‘¦), and this sampling procedure
is repeated 100 times for each input function. Consequently, a set of numerical solutions is generated. It is worth noting
that the random grid points used for sampling differ for each input function ğ‘¢(ğ‘¥). This type of dataset is commonly
referred to as an "unaligned dataset" Lu et al. [2021b], and the left panel of Figure 9 provides a visual representation of
this concept. The total size of the training data is determined by the product of the number of input functions ( 104) and
the number of samplings ( 102), resulting in a training data size of 106.
12Deep neural operators for engineering systems: Path towards digital twin A P REPRINT
Figure 7: Distribution of each metric over the test data for the ODE system (Section 5.1).
DeepONet employs fully connected neural networks for branch and trunk networks, with a branch network size of [100,
40, 40]. In this problem, the output function ğ‘ takes a two-dimensional input (ğ‘¥,ğ‘¡), leading to a layer size adjustment [2,
40, 40]. During the training process, the Adam optimization algorithm is utilized for optimization. The mean squared
error (MSE) is employed as the evaluation metric to assess the modelâ€™s performance. The model undergoes training for
10,000 iterations, with a learning rate set to 0.001.
5.2.1 Results of Dissusion System
Table 3 summarizes the performance metrics for the DeepONet model on the test data. The model shows remarkable
convergence in R2, with an average score of approximately 0.999, indicating strong agreement between predicted and
actual values. Figure 10 shows the Distribution of the performance metrics, and even the lowest R2scores depicted in
Figure 10 (a) are above 0.9, demonstrating the robustness of the modelâ€™s performance for this problem setup. Moreover,
the values of MSE, RMSE, and MAE are significantly low, ranging from 10âˆ’4to10âˆ’2, as observed in Figures 10 (b),
(c), and (d), indicating high accuracy and precision of the DeepONet model in capturing the diffusion behavior. This is
consistent with the performance achieved in the ODE test case described in Section 5.1.
The mean ratio of RMSE to MAE, calculated as 1.300 and listed in Table 3 and shown in Figure 10 (e), suggests that
some data may be significantly different from the predictions. However, the high R2score ensures the overall validity
of the model.
A comprehensive evaluation of the modelsâ€™ performance was conducted by comparing the highest and lowest prediction
accuracies with respect to FCN and CNN. The results are summarized in Table 4, providing valuable insights into the
differential performance of DeepONet compared to traditional methods. Notably, for Test ID 47303, which achieves the
13Deep neural operators for engineering systems: Path towards digital twin A P REPRINT
Figure 8: Comparison between simulations and machine learning (ML) predictions. The test cases with IDs 122 and
72 correspond to the highest and lowest R2scores achieved by DeepONet, respectively. The left figures display the
simulation results, while the right figures illustrate the predictions made by DeepONet (blue lines), FCN (green lines),
and CNN (orange lines).
Figure 9: Concept of random selected points ğ‘ƒ(ğ‘¥,ğ‘¡)(dots) on a 10Ã—10grid for the input variable space of ğ‘¥âˆˆ(0,10)
andğ‘¡âˆˆ(0,10). The numerical solutions ğ‘¢(ğ‘¥,ğ‘¡)are sampled at the points. The left pane represents a dataset called
"aligned dataset," and the right pane is "unaligned dataset" Lu et al. [2021a].
14Deep neural operators for engineering systems: Path towards digital twin A P REPRINT
highest R2score, the predictions of DeepONet closely align with the results obtained from FCN and CNN. Conversely,
in the case of Test ID 36475, where FCN and CNN fail to reproduce the test data, DeepONet can accurately predict the
desired outcomes accurately. This is evident from the agreement between the DeepONet predictions and the simulation
results in Figures 11 (c) and (d) for Test ID 36475.
Table 3: Overall performance metrics of the DeepONet model for the diffusion system
Statistics R2MSE RMSE MAE RMSE/MAE
Mean 9.990Ã—10âˆ’18.600Ã—10âˆ’58.889Ã—10âˆ’36.838Ã—10âˆ’31.300
Std 1.087Ã—10âˆ’35.200Ã—10âˆ’52.612Ã—10âˆ’31.974Ã—10âˆ’37.326Ã—10âˆ’2
Min 9.361Ã—10âˆ’15.000Ã—10âˆ’62.272Ã—10âˆ’31.678Ã—10âˆ’31.116
Max 9.999Ã—10âˆ’18.600Ã—10âˆ’42.932Ã—10âˆ’21.938Ã—10âˆ’21.993
Table 4: Comparison of the DeepONet model with FCN and CNN for the cases of R2takes highest or lowest
Test ID Models R2MSE RMSE MAE RMSE/MAE
47303 ( R2Highest)DeepONet 9.999Ã—10âˆ’11.345Ã—10âˆ’53.668Ã—10âˆ’32.909Ã—10âˆ’31.261
FCN 9.934Ã—10âˆ’11.209Ã—10âˆ’33.477Ã—10âˆ’21.853Ã—10âˆ’21.876
CNN 9.968Ã—10âˆ’17.178Ã—10âˆ’42.679Ã—10âˆ’21.562Ã—10âˆ’21.716
36475 ( R2Lowest)DeepONet 9.361Ã—10âˆ’11.376Ã—10âˆ’53.710Ã—10âˆ’32.740Ã—10âˆ’31.354
FCN 9.844Ã—10âˆ’16.517Ã—10âˆ’42.553Ã—10âˆ’21.301Ã—10âˆ’11.950
CNN 9.941Ã—10âˆ’12.468Ã—10âˆ’41.571Ã—10âˆ’21.016Ã—10âˆ’21.546
5.3 Convection-Diffusion Reaction System
This test case considers a conventional diffusion scenario encompassing fluid mechanics, gas dynamics, and nonlinear
acoustics. It finds application in the analysis of general industrial products such as small engines, pumps, as well
as large turbines used in power plants and aircraft. As an illustrative example, a one-dimensional Burgersâ€™ equation
equation, which corresponds to the neglect of the pressure term in the Navier-Stokes equation, is selected. The viscous
Burgersâ€™ equation, describing the problem for a given field ğ‘ (ğ‘¥,ğ‘¡), is given by:
ğœ•ğ‘ 
ğœ•ğ‘¡+ğ‘ ğœ•ğ‘ 
ğœ•ğ‘¥=ğœˆğœ•2ğ‘ 
ğœ•ğ‘¥2, ğ‘¥âˆˆ(0,10),ğ‘¡âˆˆ(0,10) (7)
Here, the kinematic viscosity is denoted as ğœˆ=0.01, whileğ‘¥represents the spatial coordinate, and ğ‘¡represents
the temporal coordinate. This problem aims to learn the operator ğº, which maps the initial condition ğ‘ (ğ‘¥,0)to the
output function ğ‘ (ğ‘¥,ğ‘¡). To solve Equation 7, the initial condition is specified as ğ‘ (ğ‘¥,0)=ğ‘¢(ğ‘¥), whereğ‘¢(ğ‘¥)denotes a
one-dimensional Gaussian Random Field (GRF).
For the generation of the training dataset, the solution to Equation 7 is obtained using the Fast Fourier transform (FFT)
pseudo-spectral method on a 100Ã—100grid. The random locations are fixed for each ğ‘¢(ğ‘¥). This type of dataset is
classified as an "aligned dataset" Lu et al. [2021b], as depicted in the left pane of Figure 9. The total size of the training
dataset amounts to 150, while the test dataset comprises 1,000 instances.
The architecture of ONets is defined by the following: both branch and trunk networks are set to fully connected neural
networks. The branch net size is [100, 40, 40]. Like the diffusion system problem in Section 5.2, this problem has
two-dimensional input (ğ‘¥,ğ‘¡). Therefore, the layer size of trunk networks is modified as [2, 40, 40]. During the training
process, the Adam optimization algorithm is utilized for optimization. The mean L2 relative error is employed as the
evaluation metric to assess the modelâ€™s performance. The model undergoes training for a total of 50,000 iterations, with
a learning rate set to 0.001.
15Deep neural operators for engineering systems: Path towards digital twin A P REPRINT
Figure 10: Distribution of each metric over the test data for the diffusion system (Section 5.2).
5.3.1 Results of Convection-Diffusion Reaction System
The summary metrics of the test data for the DeepONet model are presented in Table 5. The mean R2score obtained in
this study is approximately 0.437, indicating a relatively low model accuracy. The distributions of metrics are visually
represented in Figure 12. In particular, Figure 12 (a) reveals that a significant number of test cases result in negative R2
scores, indicating that the model fails to make accurate predictions. Additionally, the values of MSE, RMSE, and MAE
shown in Figures 12 (b), (c), and (d), respectively, are relatively large at the order of 10âˆ’1, indicating inadequate overall
accuracy of the model.
To provide further insights into the performance of DeepONet compared to traditional methods, a comparison of the
highest and lowest prediction accuracies with respect to FCN and CNN was conducted. The results are summarized in
Table 6. For Test ID 749, which achieved the highest R2score of 0.966, the predictions of DeepONet closely align with
the results obtained from FCN and CNN. However, in the case of Test ID 649, which achieved the lowest R2score of
âˆ’3.63, DeepONet fails to reproduce the test data even when FCN and CNN succeed. These differences in performance
are visually demonstrated in Figure 13, where a comparison between simulations and the model predictions for these
test cases is shown. In Figure 13 (a) and (b), it can be observed that DeepONetâ€™s prediction for Test ID 749 is not in
perfect agreement with the simulation, but the overall trend appears to be reproduced. On the other hand, Figure 13 (c)
and (d) clearly indicate that for Test ID 649, DeepONet returns predictions that are quite different from the simulation
result.
16Deep neural operators for engineering systems: Path towards digital twin A P REPRINT
Figure 11: Comparison between simulations and DeepONet predictions of the diffusion system. The test cases with IDs
47303 and 36475 correspond to the highest and lowest R2scores achieved by DeepONet, respectively. The right figures
show prediction results obtained by the operator ğº:ğ‘¢(ğ‘¥)â†¦â†’ğ‘ (ğ‘¥,ğ‘¡).
Table 5: Overall performance metrics of the DeepONet model for the Burgersâ€™ equation equation
Statistics R2MSE RMSE MAE RMSE/MAE
Mean 4.365Ã—10âˆ’11.532Ã—10âˆ’29.736Ã—10âˆ’27.079Ã—10âˆ’21.391
Std 6.461Ã—10âˆ’13.769Ã—10âˆ’27.649Ã—10âˆ’25.947Ã—10âˆ’21.445Ã—10âˆ’1
Minâˆ’3.615 1 .510Ã—10âˆ’41.229Ã—10âˆ’28.8880Ã—10âˆ’31.157
Max 9.661Ã—10âˆ’16.141Ã—10âˆ’17.837Ã—10âˆ’16.686Ã—10âˆ’11.901
Table 6: Comparison of the DeepONet model with FCN and CNN for the cases of R2takes highest or lowest
Test ID Models R2MSE RMSE MAE RMSE/MAE
749 ( R2Highest)DeepONet 9.661Ã—10âˆ’19.000Ã—10âˆ’43.000Ã—10âˆ’22.342Ã—10âˆ’21.281
FCN 9.989Ã—10âˆ’12.837Ã—10âˆ’55.326Ã—10âˆ’33.673Ã—10âˆ’31.450
CNN 9.989Ã—10âˆ’12.852Ã—10âˆ’55.341Ã—10âˆ’33.864Ã—10âˆ’31.382
649 ( R2Lowest)DeepONetâˆ’3.615 2 .003Ã—10âˆ’21.415Ã—10âˆ’11.112Ã—10âˆ’11.272
FCN 9.268Ã—10âˆ’11.583Ã—10âˆ’33.978Ã—10âˆ’22.359Ã—10âˆ’21.686
CNN 9.830Ã—10âˆ’13.669Ã—10âˆ’41.916Ã—10âˆ’21.316Ã—10âˆ’21.456
17Deep neural operators for engineering systems: Path towards digital twin A P REPRINT
Figure 12: Distribution of each metric over the test data for the Burgersâ€™ equation equation (Section 5.3).
6 Discussion
Based on the findings in Sections 5.2 and 5.3, itâ€™s evident that the accuracy of the model for the convection-diffusion-
reaction (Burgersâ€™ equation) system is relatively lower. While the lack of training iterations or a smaller neural network
size might be suspected initially, experiments in A and B indicate that modifying these parameters does not significantly
impact the modelâ€™s performance on the test data. This leads to the conclusion that the primary issue likely resides in the
training data.
To mitigate this issue, augmenting the training data or enhancing the sampling of points (ğ‘¥,ğ‘¦)for the output functions
could be beneficial. However, practical constraints in applying these solutions to complex physical systems must be
considered, especially in domains like nuclear systems modeling. The physical space limitations and severe operational
conditions can impose restrictions on the number and installation of sensors, making increasing the number of sampling
points for improved training data acquisition not always feasible. Therefore, exploring alternative approaches to enhance
the modelâ€™s performance without solely relying on increased sampling or unaligned datasets is imperative.
Considering potential deviations of the modelâ€™s predictions from the targets, when examining a fluid water system
in a pipe with a water temperature of ğ‘‡=293ğ¾, the kinematic viscosity of water, ğœˆ=0.01ğ‘š2/ğ‘ , is equivalent to
the problem setup. Erosion caused by fluid represents a significant concern under these assumptions. The solution
of the Burgersâ€™ equation, denoted as ğ‘ (ğ‘¥,ğ‘¡), provides the water speed at specific spatial and temporal coordinates,
necessitating the acquisition of the fluid velocity distribution. The erosion risk is evaluated by determining whether the
fluid speed exceeds the erosion velocity threshold. Ensuring accurate predictions of fluid velocity distribution is crucial
due to the potential impacts of erosion in fluid systems. Inaccuracies can compromise the assessment of erosion risk and
18Deep neural operators for engineering systems: Path towards digital twin A P REPRINT
Figure 13: Comparison between simulations and DeepONet predictions of the Burgersâ€™ equation equation. The test
cases with IDs 749 and 649 correspond to the highest and lowest R2scores achieved by DeepONet, respectively. The
right figures show prediction results obtained by the operator ğº:ğ‘¢(ğ‘¥)â†¦â†’ğ‘ (ğ‘¥,ğ‘¡).
lead to the implementation of inadequate safety measures. The erosion velocity, ğ‘‰ğ‘’, can be calculated using the formula
ğ‘‰ğ‘’=ğ¶âˆšğœŒ, whereğ¶is an empirical constant of 240, and ğœŒis the fluid density, yielding an erosion velocity of 7.6ğ‘š/ğ‘ .
Figure 13 (c) illustrates that the fluid velocity remains below the threshold throughout the simulation. However,
the most egregious prediction error by DeepONet is depicted in Figure 13 (d). The squared residuals in Figure 14
accentuate the discrepancy between the simulation and DeepONet predictions. The most significant difference occurs
at(ğ‘¥,ğ‘¡)=(2.53,8.28), with the actual fluid velocity at 1.45ğ‘š/ğ‘ , and the prediction ratio is 1.22. While this doesnâ€™t
exceed the threshold, the predicted erosion risk is elevated by a factor of 1.22.
Given the paramount importance of accuracy in erosion risk assessment, ensuring the modelâ€™s reliability and capturing
fluid behavior is crucial, especially for systems where accidents can have a profound societal impact, such as power
plants and aircraft. In such cases, rigorous validation and verification of the model are indispensable.
7 Conclusions
Deep Operator Networks (DeepONets) offer a promising approach for learning solution operators to partial differential
equations (PDEs) from data. This study evaluates DeepONets on three test cases: a system of ODEs, a general diffusion
system, and the convection-diffusion equation. Accurate predictions are achieved for the ODEs and diffusion cases, with
R2scores above 0.96 over the observed domain. However, the convection-diffusion case requires further refinements to
the DeepONet algorithm. Nonetheless, the results showcase DeepONetsâ€™ feasibility as a prediction module for digital
twin applications. Going forward, verification, validation, and uncertainty quantification remain critical to ensure robust
and reliable surrogate models. This study motivates the integration of neural operators with Bayesian and non-Bayesian
19Deep neural operators for engineering systems: Path towards digital twin A P REPRINT
Figure 14: The squared residuals were computed between the simulation and the DeepONet prediction for Test ID 649.
The largest value gets at (ğ‘¥,ğ‘¡)=(2.53,8.28), where the true fluid velocity is 1.45 m/s.
filters for real-time digital twin updates. Key future work will focus on accelerating DeepONet predictions for real-time
usage and rigorous uncertainty characterization. This study represents an important advance toward generalizable
surrogates for spatiotemporal systems. The results confirm the promise of neural operators for digital twins while
highlighting essential areas for continued research and maturation.
Acknowledgement
The computational part of this work was supported in part by the National Science Foundation (NSF) under Grant No.
OAC-1919789.
20Deep neural operators for engineering systems: Path towards digital twin A P REPRINT
A Effect of Number of Training Iterations
The sensitivity of the number of training iterations to the DeepONet model for the convection-diffusion-reaction system
described in Section 5.3 was investigated. Fully connected neural networks were used for the branch and trunk networks,
with sizes [100, 40, 40] and [2, 40, 40], respectively. The modelâ€™s performance was evaluated using mean L2 relative
error, and the Adam optimization algorithm was employed. The model underwent training for 100,000 iterations with a
learning rate of 0.001.
Correlations between the number of iterations and the training loss, test loss, and test metric were illustrated in Figure
15. It was observed that the training loss continued to decrease with iterations, while the values of test loss and test
metrics reached a plateau after 50,000 iterations.
Figure 15: Correlations between the number of iterations and the training loss, test loss, and test metric. The shaded
region represents corresponds to over 50,000 iterations.
21Deep neural operators for engineering systems: Path towards digital twin A P REPRINT
B Effect of Trunk Networkâ€™s Architecture
The effect of different trunk network architectures on the convection-diffusion reaction system (Section 5.3) was
investigated. The branch network size was fixed at [100, 40, 40], while the hidden layer of the trunk network varied
from [2, 20, 40] to [2, 80, 40] with increments of 20 neurons. Model metrics were computed using mean L2 relative
error and training employed the Adam optimization algorithm. The models underwent 50,000 iterations with a learning
rate of 0.001. However, the results in Table 7 indicate that modifying the number of neurons in the trunk networkâ€™s
hidden layer did not significantly improve the model metrics.
Table 7: Impact of the branch network size. The number of neurons in the hidden layer is varied.
Neurons R2MSE RMSE MAE RMSE/MAE
20 4.018Ã—10âˆ’12.154Ã—10âˆ’21.022Ã—10âˆ’17.335Ã—10âˆ’21.418
40 4.365Ã—10âˆ’11.532Ã—10âˆ’29.736Ã—10âˆ’27.079Ã—10âˆ’21.391
60 4.000Ã—10âˆ’12.195Ã—10âˆ’21.119Ã—10âˆ’18.294Ã—10âˆ’21.371
80 3.889Ã—10âˆ’11.810Ã—10âˆ’21.019Ã—10âˆ’17.456Ã—10âˆ’21.379
22Deep neural operators for engineering systems: Path towards digital twin A P REPRINT
C Comparisons between DeepONet and FCN/CNN
A comparative analysis was conducted between the DeepONet and conventional neural network architectures, Fully-
Connected Networks (FCNs) and Convolutional Neural Networks (CNNs). All neural network models were developed
using the PyTorch 2.0 deep learning library to ensure standardized implementation and leverage optimized routines.
This provided a unified framework to impartially assess the performance of DeepONet against established function
approximators on the test problems.
C.1 Data Preparation
The following is an explanation of how the data for training and testing FCNs and CNNs was prepared using the
example of the diffusion system.
For validation of DeepONet model, 100,000 unseen input functions were generated as the test data. The input variable
was defined as the point ğ‘ƒğ‘–=(ğ‘¥ğ‘–,ğ‘¡ğ‘–)representing the spatial coordinate ğ‘¥and timeğ‘¡in the diffusion system domain.
The DeepONet modelâ€™s R2performance was computed for each test function using the test data. The input functions
yielding the maximal and minimal R2were identified (Test IDs 47303 and 36475). Given an input function ğ‘¢(e.g.,
corresponding to Test ID), the DeepONet operator ğº(ğ‘¢)maps the input variable ğ‘ƒ=(ğ‘¥,ğ‘¡)to the output ğ‘ (ğ‘ƒ). This
allows formulating the problem conventionally as approximating ğ‘ from the input vector ğ‘ƒusing NNs.
In this study, the finite difference method obtained reference solutions ğ‘ (ğ‘¥,ğ‘¡)by solving the diffusion equation on a
100Ã—100grid. As shown in Figure 16, 100 points ğ‘ƒğ‘–and corresponding ğ‘ (ğ‘ƒğ‘–)were sampled to create the test data. To
train the FCNs and CNNs models for a given input function ğ‘¢, theğ‘ƒğ‘–âˆ’ğ‘ (ğ‘ƒğ‘–)pairs were resampled excluding points
used in the DeepONetâ€™s test set.
For each ODE, Diffusion, and Convection Diffusion demonstration, the training data for FCN and CNN were generated
by resampling 50, 100, and 100 on the input variables for the DeepONet test.
Figure 16: Sampling of training and test datasets for FCN and CNN. The red dots represent the sampled data for Testing
DeepONet, FCN, and CNN. The black dots are the training data points for FCN and CNN.
C.2 Network Architectures and Training Parameters
The network architectures and training parameters for the FCN and CNN models used in the comparative studies are
described.
23Deep neural operators for engineering systems: Path towards digital twin A P REPRINT
C.2.1 FCN
The FCN architectures comprised an input layer of ğ‘›nodes, two hidden layers with 30 rectified linear unit (ReLU)
activated nodes, and an output layer of 1 node for regression. The input layerâ€™s size varies depending on the test
cases; the ODE problem has ğ‘›=1, and Diffusion and Burgersâ€™ equation has ğ‘›=2. Mean squared error (MSE) loss
was optimized using the Adam algorithm with a learning rate 0.001 during training. Mini-batch gradient descent
was implemented for 2000 epochs with a batch size of 10 samples. Parameters of the training process are listed in Table 8.
FCN(
( a c t i v a t i o n ) : ReLU ( )
( l i n e a r s ) : ModuleList (
( 0 ) : L i n e a r ( i n _ f e a t u r e s =n_inp , o u t _ f e a t u r e s =30 , b i a s =True )
( 1 âˆ’ 2 ) : 2 x L i n e a r ( i n _ f e a t u r e s =30 , o u t _ f e a t u r e s =30 , b i a s =True )
( 3 ) : L i n e a r ( i n _ f e a t u r e s =30 , o u t _ f e a t u r e s =1 , b i a s =True )
)
)
C.3 CNN
The CNN architectures comprised an input layer of 1 nodes, one 1-dimensional convolution layer which generates 30
output nodes, two linear layer with 30 nodes, and an output layer of 1 node for regression. Mean squared error (MSE)
loss was optimized using the Adam algorithm with a learning rate of 0.001 during training. Mini-batch gradient descent
was implemented for 2000 epochs with a batch size of 20 samples. Parameters of training process are listed in Table 8.
CNN(
( a c t i v a t i o n ) : Tanh ( )
( cnn1 ) : Conv1d ( 1 , 32 , k e r n e l _ s i z e = ( 1 , ) , s t r i d e = ( 1 , ) )
( f l a t ) : F l a t t e n ( s t a r t _ d i m =1 , end_dim = âˆ’1)
( d r o p o u t ) : Dropout ( p = 0 . 5 , i n p l a c e = F a l s e )
( l i n 1 ) : L i n e a r ( i n _ f e a t u r e s =32 , o u t _ f e a t u r e s =2048 , b i a s =True )
( l i n 2 ) : L i n e a r ( i n _ f e a t u r e s =2048 , o u t _ f e a t u r e s =1 , b i a s =True )
)
Table 8: Parameters of training for FCN and CNN
Test CaseFCN/CNN
Loss function Optimizer Leaning rate Epochs Batch size
(a) ODE MSE Adam 0.001 2000 10/20
(b) Diffusion MSE Adam 0.001 2000 10/20
(c) Burgersâ€™ Equation MSE Adam 0.001 2000 10/20
24Deep neural operators for engineering systems: Path towards digital twin A P REPRINT
Declaration of Generative AI and AI-assisted technologies in the writing process
During the preparation of this work the author(s) used ChatGPT September 25 Version in order to language editing
and refinement. After using this tool/service, the author(s) reviewed and edited the content as needed and take(s) full
responsibility for the content of the publication. [Elsevier Publishing Ethics]
References
Lu Lu, Pengzhan Jin, Guofei Pang, Zhongqiang Zhang, and George Em Karniadakis. Learning nonlinear operators
via deeponet based on the universal approximation theorem of operators. Nature Machine Intelligence 2021 3:3 ,
3:218â€“229, 3 2021a. ISSN 2522-5839. doi:10.1038/s42256-021-00302-5. URL https://www.nature.com/
articles/s42256-021-00302-5 .
Nikola Kovachki, Zongyi Li, Zongyili@caltech Edu, Caltech Burigede Liu, Kamyar Azizzadenesheli, Kaushik
Bhattacharya, Andrew Stuart, and Anima Anandkumar. Neural operator: Learning maps between function spaces. 8
2021. doi:10.48550/arxiv.2108.08481. URL https://arxiv.org/abs/2108.08481v4 .
Sifan Wang, Hanwen Wang, and Paris Perdikaris. Improved architectures and training algorithms for deep operator
networks. Journal of Scientific Computing , 92:1â€“42, 8 2022. ISSN 15737691. doi:10.1007/S10915-022-01881-
0/FIGURES/27. URL https://link.springer.com/article/10.1007/s10915-022-01881-0 .
Zongyi Li, Daniel Zhengyu Huang, Burigede Liu, and Anima Anandkumar. Fourier neural operator with learned
deformations for pdes on general geometries. 7 2022a. doi:10.48550/arxiv.2207.05209. URL https://arxiv.
org/abs/2207.05209v1 .
Lu Lu, RaphaÃ«l Pestourie, Steven G. Johnson, and Giuseppe Romano. Multifidelity deep neural operators for efficient
learning of partial differential equations with application to fast inverse design of nanoscale heat transport. Physical
Review Research , 4, 4 2022a. ISSN 26431564. doi:10.48550/arxiv.2204.06684. URL https://arxiv.org/abs/
2204.06684v1 .
Shailesh Garg, Harshit Gupta, and Souvik Chakraborty. Assessment of deeponet for reliability analysis of stochastic
nonlinear dynamical systems. 1 2022a. doi:10.48550/arxiv.2201.13145. URL https://arxiv.org/abs/2201.
13145v1 .
Min Zhu, Handi Zhang, Anran Jiao, George Em Karniadakis, and Lu Lu. Reliable extrapolation of deep neural
operators informed by physics or sparse observations. 12 2022. doi:10.48550/arxiv.2212.06347. URL https:
//arxiv.org/abs/2212.06347v1 .
Chensen Lin, Martin Maxey, Zhen Li, and George Em Karniadakis. A seamless multiscale operator neural network for
inferring bubble dynamics. Journal of Fluid Mechanics , 929:A18, 2021a. doi:10.1017/jfm.2021.866.
Minglang Yin, Enrui Zhang, Yue Yu, and George Em Karniadakis. Interfacing finite elements with deep neural operators
for fast multiscale modeling of mechanics problems. Computer Methods in Applied Mechanics and Engineering ,
402:115027, 12 2022. ISSN 0045-7825. doi:10.1016/J.CMA.2022.115027.
Chensen Lin, Zhen Li, Lu Lu, Shengze Cai, Martin Maxey, and George Em Karniadakis. Operator learning for
predicting multiscale bubble growth dynamics. The Journal of Chemical Physics , 154:104118, 3 2021b. ISSN
0021-9606. doi:10.1063/5.0041203. URL https://aip.scitation.org/doi/abs/10.1063/5.0041203 .
Gege Wen, Zongyi Li, Kamyar Azizzadenesheli, Anima Anandkumar, and Sally M. Benson. U-fnoâ€”an enhanced
fourier neural operator-based deep-learning model for multiphase flow. Advances in Water Resources , 163:104180, 5
2022. ISSN 0309-1708. doi:10.1016/J.ADVWATRES.2022.104180.
Zhijie Li, Wenhui Peng, Zelong Yuan, and Jianchun Wang. Fourier neural operator approach to large eddy simulation of
three-dimensional turbulence. Theoretical and Applied Mechanics Letters , 12:100389, 11 2022b. ISSN 2095-0349.
doi:10.1016/J.TAML.2022.100389.
Maarten V . de Hoop, Daniel Zhengyu Huang, Elizabeth Qian null, and Andrew M. Stuart. The cost-accuracy trade-off
in operator learning with neural networks. Journal of Machine Learning , 1:299â€“341, 3 2022. ISSN 2790-203X.
doi:10.48550/arxiv.2203.13181. URL https://arxiv.org/abs/2203.13181v3 .
Ethan Pickering and Themistoklis P. Sapsis. Structure and distribution metric for quantifying the quality of un-
certainty: Assessing gaussian processes, deep neural nets, and deep neural operators for regression. 3 2022.
doi:10.48550/arxiv.2203.04515. URL https://arxiv.org/abs/2203.04515v1 .
Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart,
and Anima Anandkumar. Fourier neural operator for parametric partial differential equations. 10 2020.
doi:10.48550/arxiv.2010.08895. URL https://arxiv.org/abs/2010.08895v3 .
25Deep neural operators for engineering systems: Path towards digital twin A P REPRINT
Tapas Tripura and Souvik Chakraborty. Wavelet neural operator: a neural operator for parametric partial differential
equations. 5 2022. ISSN 00457825. doi:10.48550/arxiv.2205.02191. URL https://arxiv.org/abs/2205.
02191v1 .
Xinghuo Yu, M. Onder Efe, and Okyay Kaynak. A general backpropagation algorithm for feedforward neural networks
learning. IEEE Transactions on Neural Networks , 13:251â€“254, 1 2002. ISSN 10459227. doi:10.1109/72.977323.
JÃ¼rgen Schmidhuber. Deep learning in neural networks: An overview. Neural Networks , 61:85â€“117, 1 2015. ISSN
0893-6080. doi:10.1016/J.NEUNET.2014.09.003.
George A. Anastassiou. Multivariate hyperbolic tangent neural network approximation. Computers & Mathematics
with Applications , 61:809â€“821, 2 2011. ISSN 0898-1221. doi:10.1016/J.CAMWA.2010.12.029.
Pierre Cardaliaguet and Guillaume Euvrard. Approximation of a function and its derivative with a neural network.
Neural Networks , 5:207â€“220, 1 1992. ISSN 0893-6080. doi:10.1016/S0893-6080(05)80020-6.
Rastko R. Selmic and Frank L. Lewis. Neural-network approximation of piecewise continuous functions: Appli-
cation to friction compensation. IEEE Transactions on Neural Networks , 13:745â€“751, 5 2002. ISSN 10459227.
doi:10.1109/TNN.2002.1000141.
Lars Ruthotto and Eldad Haber. Deep neural networks motivated by partial differential equations. Journal of
Mathematical Imaging and Vision , 62:352â€“364, 2020.
Steven Guan, Ko-Tsung Hsu, and Parag V Chitnis S Guan. Fourier neural operator networks: A fast and general
solver for the photoacoustic wave equation. page 22102, 8 2021. doi:10.48550/arxiv.2108.09374. URL https:
//arxiv.org/abs/2108.09374v1 .
Akshay Thakur, Tapas Tripura, and Souvik Chakraborty. Multi-fidelity wavelet neural operator with application
to uncertainty quantification. 8 2022. doi:10.48550/arxiv.2208.05606. URL https://arxiv.org/abs/2208.
05606v1 .
Somdatta Goswami, Aniruddha Bora, Yue Yu, and George Em Karniadakis. Physics-informed deep neural operator
networks. 7 2022. doi:10.48550/arxiv.2207.05748. URL https://arxiv.org/abs/2207.05748v2 .
Lu Lu, Xuhui Meng, Shengze Cai, Zhiping Mao, Somdatta Goswami, Zhongqiang Zhang, and George Em Kar-
niadakis. A comprehensive and fair comparison of two neural operators (with practical extensions) based on
fair data. Computer Methods in Applied Mechanics and Engineering , 393:114778, 4 2022b. ISSN 0045-7825.
doi:10.1016/J.CMA.2022.114778.
A.N. Gorban and D.C. Wunsch. The general approximation theorem. pages 1271â€“1274, 11 2002.
doi:10.1109/IJCNN.1998.685957.
Tianping Chen and Hong Chen. Universal approximation to nonlinear operators by neural networks with arbitrary
activation functions and its application to dynamical systems. IEEE Transactions on Neural Networks , 6:911â€“917,
1995. ISSN 19410093. doi:10.1109/72.392253.
Huaiqian You, Quinn Zhang, Colton J. Ross, Chung-Hao Lee, and Yue Yu. Learning deep implicit fourier neural
operators (ifnos) with applications to heterogeneous material modeling. Computer Methods in Applied Mechanics
and Engineering , 398, 3 2022. doi:10.1016/j.cma.2022.115296. URL http://arxiv.org/abs/2203.08205http:
//dx.doi.org/10.1016/j.cma.2022.115296 .
Carlo Marcati and Christoph Schwab. Exponential convergence of deep operator networks for elliptic partial differential
equations. 12 2021. doi:10.48550/arxiv.2112.08125. URL https://arxiv.org/abs/2112.08125v2 .
Lu Lu, Xuhui Meng, Zhiping Mao, and George Em Karniadakis. DeepXDE: A deep learning library for solving
differential equations. SIAM Review , 63(1):208â€“228, 2021b. doi:10.1137/19M1274067.
Ethan Pickering, Stephen Guth, George Em Karniadakis, and Themistoklis P. Sapsis. Discovering and forecasting
extreme events via active learning in neural operators. Nature Computational Science 2022 2:12 , 2:823â€“833,
12 2022. ISSN 2662-8457. doi:10.1038/s43588-022-00376-0. URL https://www.nature.com/articles/
s43588-022-00376-0 .
Zongyi Li, Hongkai Zheng, Nikola Kovachki, David Jin, Haoxuan Chen, Burigede Liu, Kamyar Azizzadenesheli,
and Anima Anandkumar. Physics-informed neural operator for learning partial differential equations. 11 2021.
doi:10.48550/arxiv.2111.03794. URL https://arxiv.org/abs/2111.03794v2 .
Matthew S Bonney, Marco De Angelis, Mattia Dal Borgo, Luis Andrade, Sandor Beregi, Nidhal Jamia, and David J
Wagg. Development of a digital twin operational platform using python flask. Data-Centric Engineering , 3, 2022.
Hussain Mohammed Dipu Kabir. Non-linear down-sampling and signal reconstruction, without folding. In 2010 Fourth
UKSim European Symposium on Computer Modeling and Simulation , pages 142â€“146. IEEE, 2010a.
26Deep neural operators for engineering systems: Path towards digital twin A P REPRINT
Hussain Mohammed Dipu Kabir. A theory of loss-less compression of high quality speech signals with comparison. In
2010 Fourth UKSim European Symposium on Computer Modeling and Simulation , pages 136â€“141. IEEE, 2010b.
Hussain Mohammed Dipu Kabir et al. Watermarking with fast and highly secured encryption for real-time speech
signals. In 2010 IEEE International Conference on Information Theory and Information Security , pages 446â€“451.
IEEE, 2010a.
Hussain Mohammed Dipu Kabir et al. A loss-less compression technique for high quality speech signals and its
implementation with mpeg-4 als for better compression. In 2010 IEEE International Conference on Information
Theory and Information Security , pages 781â€“785. IEEE, 2010b.
Kazuma Kobayashi, James Daniell, Shoaib Usman, Dinesh Kumar, and Syed Alam. Surrogate modeling-driven
physics-informed multi-fidelity kriging: Path forward to digital twin enabling simulation for accident tolerant fuel.
Springer Nature Handbook of Smart Energy Systems, arXiv preprint arXiv:2210.07164 , 2022a.
Kazuma Kobayashi, Shoaib Usman, Carlos Castano, Ayodeji Alajo, Dinesh Kumar, and Syed Alam. Data-driven
multi-scale modeling and robust optimization of composite structure with uncertainty quantification. Springer Nature
Handbook of Smart Energy Systems, arXiv preprint arXiv:2210.09055 , 2022b.
Kazuma Kobayashi, Bader Almutairi, Md Nazmus Sakib, Souvik Chakraborty, and Syed Bahauddin Alam. Explainable,
interpretable & trustworthy ai for intelligent digital twin: Case study on remaining useful life. 2023.
Shailesh Garg, Souvik Chakraborty, and Budhaditya Hazra. Physics-integrated hybrid framework for model form error
identification in nonlinear dynamical systems. Mechanical Systems and Signal Processing , 173:109039, 2022b.
Elham Tabassi. Artificial intelligence risk management framework (ai rmf 1.0). 2023.
Aijun Zhang. Machine learning model validation. part 1: Machine learning interpretability. In QU-ML Model Validation
Workshop, Session 1 , pages 1â€“35. Springer, June 29, 2022.
Aijun Zhang. Machine learning model validation. part 2: Model diagnostics and validation. In QU-ML Model Validation
Workshop, Session 2 , pages 1â€“35. Springer, July 6, 2022.
Kazuma Kobayashi and Syed Bahauddin Alam. Explainable, interpretable, and trustworthy ai for an intelligent digital
twin: A case study on remaining useful life. Engineering Applications of Artificial Intelligence , 2023.
James Daniell, Kazuma Kobayashi, Dinesh Kumar, Souvik Chakraborty, Ayodeji Alajo, Ethan Taber, Joseph Graham,
and Syed Alam. Physics-informed multi-stage deep learning framework development for digital twin-centred
state-based reactor power prediction. arXiv preprint arXiv:2211.13157 , 2022.
M Rahman, Abid Khan, Sayeed Anowar, Md Al-Imran, Richa Verma, Dinesh Kumar, Kazuma Kobayashi, and Syed
Alam. Leveraging industry 4.0â€“deep learning, surrogate model and transfer learning with uncertainty quantification
incorporated into digital twin for nuclear system. Springer Nature Handbook of Smart Energy Systems, arXiv preprint
arXiv:2210.00074 , 2022.
27