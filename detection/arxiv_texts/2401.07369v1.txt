Proceedings of Machine Learning Research vol vvv:1–32, 2024
CoVO-MPC: Theoretical Analysis of Sampling-based MPC
and Optimal Covariance Design
Zeji Yi * ZEJIY @ANDREW .CMU .EDU
Chaoyi Pan∗CHAOYIP @ANDREW .CMU .EDU
Guanqi He GUANQIHE @ANDREW .CMU .EDU
Guannan Qu GQU@ANDREW .CMU .EDU
Guanya Shi GUANYAS @ANDREW .CMU .EDU
Carnegie Mellon University
Abstract
Sampling-based Model Predictive Control (MPC) has been a practical and effective approach in
many domains, notably model-based reinforcement learning, thanks to its flexibility and paralleliz-
ability. Despite its appealing empirical performance, the theoretical understanding, particularly in
terms of convergence analysis and hyperparameter tuning, remains absent. In this paper, we char-
acterize the convergence property of a widely used sampling-based MPC method, Model Predictive
Path Integral Control (MPPI). We show that MPPI enjoys at least linear convergence rates when
the optimization is quadratic, which covers time-varying LQR systems. We then extend to more
general nonlinear systems. Our theoretical analysis directly leads to a novel sampling-based MPC
algorithm, CoVariance-Optimal MPC ( CoVO-MPC ) that optimally schedules the sampling covari-
ance to optimize the convergence rate. Empirically, CoVO-MPC significantly outperforms standard
MPPI by 43-54% in both simulations and real-world quadrotor agile control tasks. Videos and
Appendices are available at https://lecar-lab.github.io/CoVO-MPC/ .
Keywords: Sampling-based Model Predictive Control, Convergence, Optimal Control, Robotics
1. Introduction
Model Predictive Control (MPC) has achieved remarkable success and become a cornerstone in
various applications such as process control, robotics, transportation, and power systems (Mayne,
2016). Sampling-based MPC, in particular, has gained significant attention in recent years due to
its ability and flexibility to handle complex dynamics and cost functions and its massive paralleliz-
ability on GPUs. The effectiveness of sampling-based MPC has been demonstrated in various ap-
plications, including path planning (Helvik and Wittner, 2001; Durrant-Whyte et al., 2012; Nguyen
et al., 2021; Argenson and Dulac-Arnold, 2021), and control (Chua et al., 2018; Williams et al.,
2017). Particularly, thanks to its accessibility and flexibility to deal with learned dynamics and cost
or reward functions, sampling-based MPC has been widely used as a subroutine in model-based re-
inforcement learning (MBRL) (Mannor et al., 2003; Menache et al., 2005; Ebert et al., 2018; Zhang
et al., 2019; Kaiser et al., 2020; Bhardwaj et al., 2020), where the learned dynamics (often in latent
space) are highly nonlinear with nonconvex cost functions.
However, there is little theoretical understanding of sampling-based MPC, especially regarding
its convergence and contraction properties (e.g., whether and how fast it converges to the optimal
control sequence when sampled around some suboptimal control sequence). Moreover, despite its
*Equal contributions.
© 2024 Z. Yi, C. Pan, G. He, G. Qu & G. Shi.arXiv:2401.07369v1  [cs.LG]  14 Jan 2024YIPANHEQUSHI
empirical success, there is no theoretical guideline for key hyperparameter tunings, especially the
temperate λand the sampling covariance Σ. For instance, one of the most popular and practical
sampling-based MPC algorithms, Model Predictive Path Integral Control (MPPI, Williams et al.
(2016, 2017)) uses isotropic Gaussians (i.e., there is no correlation between different time steps or
different control dimensions) and heuristically tunes temperature. Further, most sampling-based
MPC algorithms are unaware of the underlying dynamics and optimization landscape. This paper
makes the first step in theoretically understanding the convergence and contraction properties of
sampling-based MPC. Based on such analysis, we provide a novel, practical, and effective MPC
algorithm by optimally scheduling the covariance matrix Σ. Our contribution is three-fold:
• For the first time, we present the convergence analysis of MPPI. When the total cost is quadratic
w.r.t. the control sequence, we prove that MPPI contracts toward the optimal control sequence
and precisely characterize the contraction rate as a function of Σ, λ, and system parameters. We
then extend beyond the quadratic setting in three cases: (1) strongly convex total cost, (2) linear
systems with nonlinear residuals, and (3) general systems.
• An immediate application of our theoretical results is a novel sampling-based MPC algorithm
that optimizes the convergence rate, namely, CoVariance-Optimal MPC ( CoVO-MPC ). To do so,
CoVO-MPC computes the optimal covariance Σby leveraging the underlying dynamics and cost
functions, either in real-time or via offline approximations.
• We thoroughly evaluate the proposed CoVO-MPC algorithm in different robotic systems, from
Cartpole and quadrotor simulations to real-world scenarios. In particular, compared to the stan-
dard MPPI algorithm, the performance is enhanced by 43% to54% across different tasks.
Collectively, this paper advances the theoretical understanding of sampling-based MPC and
offers a practical and efficient algorithm with significant empirical advantages. This work opens
avenues for further exploration and refinement of sampling-based MPC strategies, with implications
for a broad spectrum of applications.
2. Preliminaries and Related Work
Notations. In our work, Hrepresents the horizon of the MPC. At each time step h, the state is
denoted as xh∈Rn, and the control input is uh∈Rm. The control input sequence over the horizon
isU=u1:H∈RmH, which is obtained by flattening the sequence [U]1=u1, . . . , [U]H=uH.
We denote random variables using the curly letter U, andUirepresents the i-th sample in a set of N
total samples. The notation − →
p, as used in our work, indicates convergence in probability.
2.1. Optimal Control and MPC
Consider a deterministic optimal control framework:
min
u1:HJ(u1:H) =HX
h=1ch(xh, uh) +cf(xH+1)s.t.xh+1=fh(xh, uh),1≤h≤H (1)
Here fh:Rn×Rm→Rncharacterizes the dynamics, ch:Rn×Rm→R≥0captures the cost
function, and cf:Rn→R≥0represents the terminal cost function. This formulation constitutes
a constrained optimization problem focused on J(u1:H). For a T-step control problem ( T≫H),
at every step, MPC solves (1) in a receding-horizon manner. In detail, upon observing the current
state, MPC sets the x1in (1) to be the current state and solves (1), which amounts to solving the
optimal control only considering the costs for the next Hsteps. Afterward, MPC only executes the
first control in the solved optimal control sequence, after which the next state can be obtained from
2COVARIANCE -OPTIMAL MPC
the system, and the procedure repeats. In other words, the H-step problem (1) is a subroutine for
MPC. Throughout the theoretical parts (Secs. 3 and 4), we focus on this H-step subroutine.
The optimization in (1) is potentially highly nonconvex due to nonlinear fhand nonconvex ch
andcf. Non-sampling-based nonlinear MPC (NMPC) typically calls specific nonlinear program-
ming (NLP) solvers to solve (1), which depends on certain formulations like sequential quadratic
programming (SQP, Sideris and Bobrow (2005)), differentiable dynamic programming (DDP, Tassa
et al. (2014)), and iterative linear quadratic regulator (iLQR, Carius et al. (2018)). While NMPC has
optimality guarantees in simple settings such as linear systems with quadratic costs, its effectiveness
is limited to specific systems and solvers (Song et al., 2023; Forbes et al., 2015). In the next section,
we introduce sampling-based MPC, a recently popular paradigm that can mitigate these limitations.
2.2. Sampling-based MPC and Applications in Model-based Reinforcement Learning
Recently, sampling-based MPC gained popularity as an alternative to NMPC by employing the
zeroth-order optimization strategy (Drews et al., 2018; Mannor et al., 2003; Helvik and Wittner,
2001). Instead of deploying specific NLP solvers to optimize (1), sampling-based MPC samples
a set of control sequences from a particular distribution, evaluates the costs of all sequences by
rolling them out, optimizes and updates the sampling distribution as a function of samples and their
evaluated costs. Unlike NMPC, sampling-based MPC has no particular assumptions on dynamics or
cost functions, resulting in a versatile approach in different systems. Moreover, the sampling nature
enables massive parallelization on modern GPUs. For example, MPPI is deployed onboard for
real-time robotic control (Williams et al., 2016; Pravitra et al., 2020; Sacks et al., 2023), sampling
thousands of trajectories at more than 50Hz with a single GPU.
Sampling-based MPC can be categorized by what sampling-based optimizer it deploys. Co-
variance Matrix Adaptation Evolution Strategy (CMA-ES, Hansen et al. (2003)) adapts sampling
mean and covariance using all samples to align with the desired distribution. Cross-Entropy Method
(CEM, Botev et al. (2013)) deploys a similar procedure but only considers the low-cost samples.
Wagener et al. (2019) unifies different sampling-based MPC approaches by connecting them to
online learning with different utility functions and Bregman divergences. Among these variants,
MPPI (Williams et al., 2017, 2016) is one of the most popular and practical, and it reformulates
the control problem into optimal distribution matching from an information-theoretic perspective.
MPPI’s performance is sensitive to hyperparameters and system dynamics, leading to several vari-
ants that improve its efficiency and robustness, e.g., using Tsallis Divergence (Wang et al., 2021),
imposing extra constraints when sampling (Gandhi et al., 2021; Balci et al., 2022), or using learned
optimizers (Sacks et al., 2023). Compared to these works, CoVO-MPC optimally schedules the
sampling covariance followed by our rigorous theoretical analysis.
MPPI has been particularly popular in the Model-based Reinforcement Learning (MBRL) con-
text, an RL paradigm that first learns a dynamics model and optimizes a policy using the learned
model. Specifically, largely because of its parallelizability and its flexibility to handle nonlinear
and potentially latent learned models, MPPI has been a popular control and planning subroutine in
MBRL (Menache et al., 2005; Szita and L ¨orincz, 2006; Janner et al., 2021; Lowrey et al., 2019;
Hafner et al., 2019). Moreover, MPPI and CEM have been widely deployed for uncertainty-aware
control and planning with probabilistic models (e.g., PETS (Chua et al., 2018), PlaNet (Hafner
et al., 2019)), and integrated with learned value and policy to improve the MBRL performance
(e.g., TD-MPC (Hansen et al., 2022), MBPO (Argenson and Dulac-Arnold, 2021)).
3YIPANHEQUSHI
Despite its broad applications, there is little theoretical understanding of sampling-based MPC
algorithms. In this paper, we aim to provide theoretical characterizations of MPPI’s optimal-
ity and convergence, based on which we design an improved algorithm CoVO-MPC . In addition,
CoVO-MPC can potentially be an efficient backbone for a broad class of MBRL algorithms.
3. Problem Formulation
While MPPI is an iterative approach with a receding horizon, to streamline the analysis, we consider
a single step in MPPI, that is solving a trajectory optimization problem (1) using sampling-based
method1. Specifically, MPPI rewrites the cost function in (1), as only a function of the control input
(i.e., substitute the state xhwith control inputs U). To minimize J(U), MPPI samples the control
sequence Uand outputs a weighted sum of these samples. Specifically, the control sequence is
sampled from a Gaussian distribution: U ∼ N (Uin,Σ), where Uin∈RmHis the mean and Σ∈
RmH×mHis the covariance matrix. Given the samples, we calculate the output control sequence
Uout∈RmHusing a softmax-style weighted sum based on each sample’s cost J(Ui):
Uout=PN
i=1Uiexp
−J(Ui)
λ
PN
i=1exp
−J(Ui)
λ, (2)
where λis the temperature parameter commonly used in statistical physics (Busetti, 2003) and
machine learning. As λ→0, the focus intensifies on the top control sequence, while a larger λ
distributes the influence over a set of relatively favorable trajectories. In the next section, we will
characterize the single-step convergence properties of MPPI in different systems.
4. Main Theoretical Results
Our primary goal is to investigate the optimality and convergence rate of MPPI. We are driven by
the following questions: Under what conditions does the algorithm exhibit outputs close to the
optimal solution? And what is the corresponding convergence rate? Moreover, if the convergence
is influenced by factors such as Uin, λ,Σ, how can we strategically design the sample covariance to
expedite the convergence process?
To answer the above questions, we first analyze the convergence of the single-step MPPI (2) in a
quadratic cost’s environment. In Sec. 4.1, we show that the expected result of equation (2) contracts
to the optimal solution in Theorem 1. Then, in Sec. 4.2, based on the contraction of the expectation
output, we further give an optimal design principle for the covariance to better utilize the known
dynamics. Moreover, in Sec. 4.3, we address general nonlinear environments and prove that MPPI
still keeps the contraction property, though with bounded associated errors.
4.1. Convergence Analysis for Quadratic J(U)
We start by considering the total cost J(U)of the following form,
J(U) =U⊤DU+U⊤d, (3)
which is quadratic in U. One simple but general example that satisfies the above is when the
dynamics is LTI or LTV , and the cost is quadratic, which we will elaborate on in Example 1. We
will also discuss the generalization to non-quadratic J(U)and nonlinear dynamics in Sec. 4.3.
1. We will discuss the full implementation of MPPI with a receding horizon in Sec. 5.
4COVARIANCE -OPTIMAL MPC
In this setting, our first result proves the convergence of MPPI, i.e., (2). Specifically, we show
that the expected output control sequence contracts, and critically, we provide a precise contraction
rate in terms of the system parameter Dand the algorithm parameters Σ, λ.
Theorem 1 Given Uinand the sampling distribution N(Uin,Σ), under the assumption that the
total cost J(U)is in quadratic form in (3)andDis positive semi-definite, the weighted sum of the
samples Uout, as in (2), converges in probability to a contraction towards the optimal solution U∗
when the number of sample N→ ∞ in the following way:
||Uout−U∗||
||Uin−U∗||− →
p(2
λΣD+I)−1<1 (4)
Similarly, the expected cost contracts to the optima J∗in the following way:
J(Uout)−J∗− →
pJout−J∗≤(J(Uin)−J∗)(I+2
λD1
2ΣD1
2)−2, (5)
where Joutis a constant determined by function J and Uin,Σ, D.
Theorem 1 means, when the number of sample Ngoes to infinity, MPPI enjoys a linear contrac-
tion2towards the optimal control sequence U∗and the optimal cost J∗. Further, from the contraction
rate in both (4) and (5), we can observe that smaller λleads to faster contraction, which we will
discuss more on in Sec. 4.3. However, the faster contraction comes at the cost of higher sample
complexity due to a higher variance on Uout, as shown by Busetti (2003).
In addition to λ, the contraction rate is also ruled by the matrix D1
2ΣD1
2, indicating that choos-
ingΣis vital for the contraction rate, and further, the optimal choice of Σshould depend on the
Hessian matrix D. Note that the standard MPPI chooses Σ =σI, i.e., sampling isotropically over
a landscape defined by D, which could yield slow convergence. Based on this observation, we will
develop an optimal design scheme of Σin Sec. 4.2.
Example 1 (Time-variant LQR) Here is an example of the linear quadratic regulator (LQR) set-
ting with time-variant dynamics and costs. The dynamics is xh+1=Ahxh+Bh[U]h+whand
the cost is given by J(U) =PH
h=1 
x⊤
hQhxh+ [U]⊤
hRh[U]h
, where U= ([U]1, . . . , [U]H)and
Qh⪰0, Rh≻0. Plugging in the states xhas a function of U, the cost can be reformulated as
J(U) =U⊤DU+U⊤d+J0where dandJ0are constants derived from system parameters. And
the Hessian is D=M⊤QM+R, with
M=
0 ···
B1 0 ···
˜A2,2B1 B2 0···
......
˜AH−1,2B1˜AH−1,3B2···BH−1
,Q=
Q10···
0Q2···
......
0 0 ···QH
,R=
R10···
0R2···
......
0 0 ···RH
,
and˜Ai,j=AiAi−1···Ajfori≥j.
2. MPPI can be considered a regularized Newton’s method, utilizing the Hessian Matrix D, which typically results in
linear convergence (contraction). Moreover, it exhibits superlinear convergence in the proximity of the optimum.
Further discussion on this topic will be provided in Appendix A.1.
5YIPANHEQUSHI
Theorem 1 primarily addresses scenarios with quadratic J(U), suitable for time-varying LQR
as seen in Example 1. Even though time-varying LQR can be solved using well-known analyti-
cal methods, our theoretical exploration still holds significant value, and its underlying principles
extend beyond linear systems. Firstly, implementation-wise, nonlinear systems can be linearized
around a nominal trajectory, resulting in a Linear Time-Variant (LTV) system to which Theorem 1
directly applies, like in iterative Linear Quadratic Regulator (iLQR, Li and Todorov (2004)) and
Differential Dynamic Programming (DDP, Mayne (1966)). Secondly, from a theory perspective, we
further discuss the linearization error and more generally, non-quadratic J(U)in Sec. 4.3. Lastly,
the analysis within this quadratic framework elucidates the optimal covariance design principle in
Sec. 4.2, which we use to design the CoVO-MPC algorithm in Sec. 5 that can be implemented be-
yond linear systems. Empirically (Sec. 6), CoVO-MPC demonstrates superior performance across
various nonlinear problems, affirming the value of Theorem 1 beyond quadratic formulations.
4.2. Optimal Covariance Design
Theorem 1 proves that MPPI guarantees a contraction to the optima with contraction rate depend-
ing on the choice of Σ(see (5)). We now investigate the optimal Σto achieve a faster contraction
rate. Based on (5), it is evident that scaling Σwith a scalar larger than 1brings better contraction
in expectation. However, doing so is equivalent to decreasing λ, which will lead to higher sample
complexity according to Sec. 4.1. In light of this, we formulate the optimal covariance Σdesign
problem as a constrained optimization problem: How can we design Σto achieve an optimal con-
traction rate, subject to the constraint that the determinant of the covariance is upper-bounded, that
isdet Σ≤α. The following theorem optimally solves this constrained optimization problem.
Theorem 2 Under the constraint that positive semi-definite matrix Σsatisfies det Σ ≤α, the
contraction rate(I+2
λD1
2ΣD1
2)−2in(5)is minimized when Σhas the following form: The
Singular Value Decomposition (SVD) of Σ = V⊤ΛVshares the same eigenvector matrix Vwith
the SVD of the Hessian matrix D=V⊤OV, and
[O]2
i[Λin]2
i[Λ]i
(1 +2
λ[Λ]i[O]i)3= Constant ,∀i= 1,···, mH, (6)
where Λ,O ∈RmH×mH,Λ = diag([Λ]1,[Λ]2, . . . , [Λ]mH]),O=diag([O]1,[O]2, . . . , [O]mH).
Further, Λin∈RmHis the coordinates of Uin−U∗under the basis formed by the eigenvectors V.
In other words, Λin=V⊤(Uin−U∗) = [[Λ in]1,[Λin]2, . . . , [Λin]mH]⊤. Lastly, the Constant in the
right-hand-side of (6)is selected subject to the constraint det Σ = α.
While Theorem 2 offers insights into the optimal design of Σ, solving (6) relies on prior knowl-
edge of U∗. In Corollary 3 below, we present an approximation to the solution of (6) in Theorem 2.
This approximation is not only efficient to solve but also eliminates the need for knowing U∗.
These advantages come under the assumptions of an isotropic gap between U∗andUinand a small
λregime. The isotropy gap assumption is necessary as we do not have additional information about
U∗3. A small λregime is also practical since Theorem 1 implies a smaller λyields faster contrac-
tion given a sufficiently large sample number N. Given modern GPU’s capabilities in large-scale
parallel sampling, we can generate sufficient samples for small values of λ(in practice, MPPI often
3. We show that the isotropy assumption is minimizing the max cost from a family of gap Uin−U∗in Remark 8 of
Appendix A.2.
6COVARIANCE -OPTIMAL MPC
usesλ < 0.01), which justifies the adoption of the small λregime in Corollary 3. Notably, this
approximation serves as the foundational design choice for CoVO-MPC introduced in Sec. 5, and
empirical experiments consistently validate its feasibility.
Corollary 3 Under the assumption Uin−U∗∼ N(0, I), when λ→0, asymptotically, the optimal
solution of (6)is:
lim
λ→0,N→∞Σ = ( αdetD1/2)1
mHD−1/2(7)
Note that in both Theorem 2 and Corollary 3, the choice of Σdepends on system parameters.
The underlying concept of this design principle is to move away from isotropic sampling of the con-
trol sequence. Instead, the sample distribution is tailored to align with the structure of the system
dynamics and cost functions. In essence, if we view the covariance matrix as an ellipsoid in the
control action space, Theorem 2 and Corollary 3 implies that the optimal covariance matrix should
be compressed in the direction where the total cost J(U)is smoother and stretched along the direc-
tion aligned with the J’s gradient. Guided by this optimal covariance matrix design principle, we
introduce a practical algorithm in Sec. 5 that incorporates this optimality into the MPPI framework.
This framework computes the Hessian matrix and designs the covariance matrix, thus leveraging
the system structure for improved performance.
4.3. Generalization Beyond the Quadratic J(U)
Sec. 4.1 gives the convergence properties of MPPI when J(U)is quadratic, but in practice, MPPI
works well beyond quadratic J(U). Therefore, in this section, we extend the convergence analysis
in Sec. 4.1 to general non-quadratic total costs. Specifically, we consider three different settings,
from strongly convex (not necessarily quadratic) J(U), linear systems with nonlinear residuals, to
general systems.
In many cases, the total cost function Jdemonstrates strong convexity without necessarily being
quadratic. A typical example is the time-varying LQR (Example 1) with the quadratic term x⊤Qx
in the cost being replaced with a strongly convex function. In light of this, we now study the
convergence of MPPI under a β-strongly convex total cost function J(U).
Theorem 4 (Strongly convex J. Informal version of Theorem 4 in Appendix A.3.1) Given a β-
strongly convex function J(U)which has Lipschitz continuous derivatives, that∂J
∂UisLd-Lipschitz,
Uoutconverges to a neighborhood of the optimal point U∗in probability as the number of samples N
tends to infinity, i.e., Uout− →
pUc. Here Ucsatisfies ∥Uc−U∗∥ ≤λ
β2∥Σ−1∥
(1+λ
β∥Σ∥)∥Uin−U∗∥+∥Uerror∥,
where ∥Uerror∥ ∼O(√
λ
β).
Theorem 4 demonstrates MPPI maintains a contraction rate of O(λ
β)with a small λ, approaching
0when λ≪β. The remaining residual error Uerror, also of O(λ
β), suggests MPPI’s convergence to
a neighborhood around the optimal point with size O(λ
β). The contraction rate in strongly convex
J(U)differs from the quadratic case (Sec. 4.1) due to the difference between the quadratic density
function in Gaussian. The non-quadratic J(U)has a slightly looser bound in terms of constant.
Beyond strongly convex J(U), we next consider nonlinear dynamics. Specifically, we consider
the same cost function as in Example 1 with the following dynamics: xh+1=Axh+B[U]h+
w+g([U]h), where wis a constant, and grepresents nonlinear residual dynamics. We then recast
the total cost as J(U) =U⊤DU+d⊤U+Jres(U)by retaining all higher-order terms in Jres(U).
7YIPANHEQUSHI
Subsequently, we establish an exponential family using D,d, and Jres(U)as sufficient statistics.
Given the characteristics of the residual dynamics, we can bound the variations in these three statis-
tics relative to the quadratic total cost. Utilizing the properties of the exponential family, we then
show that, with bounded residual dynamics g, contraction is still guaranteed to some extent.
Theorem 5 (Linear systems with nonlinear residuals. Informal version of Theorem 5 in Appendix A.3.2 ).
Given the residual dynamics g,Uoutcontracts in the same way as in (4)with contraction error
∥Uerror∥ ∼L1
λ∥Q∥(O(A3H)C3+O(A2H)C2)withL1as a Lipschitz constant from the
exponential family, and C2, C3as constants that coming from the bounded residual dynamics g.
Additionally, it is also worthnoting that the Lipschitz constant L1is task-specific since different
tasks have different sufficient statistics Jreswithin the exponential family. Given Theorem 5 for
residual dynamics, a direct and straightforward corollary arises for general systems, with direct
access to the cost function, specifically Jres.
Corollary 6 (General systems. Informal version of Theorem 6 in Appendix A.3.3.) Uoutcontracts
the same as (4)with a contraction error, ∥Uerror∥ ∼O(L1
λ(∥D−D′∥)), where D′is the Hessian
under the residual dynamic g, and Dis the Hessian without the residual dynamic.
5. The CoVariance-Optimal MPC (CoVO-MPC) Algorithm
Algorithm 1: CoVO-MPC
Input: H;N;T;c1:T;C(·);shift (·);
1fort= 1 : Tdo
2 Σt←C(Dt=∇2Jt(Uin|t))
3 Sample Ui|t∼ N(Uin|t,Σt)for1≤i≤N
4 Compute weight κi|t←exp(−Jt(Ui|t)
λ)
PN
i=1exp(−Jt(Ui|t)
λ))
5 Uout|t←PN
i=1κi|tUi|t
6 Execute ut←[Uout|t]1, receive xt+1
7 Uin|t+1←shift (Uout|t)
8endCorollary 3 in Sec. 4.2 provides the optimal
covariance matrix Σas a function of the Hes-
sian matrix D. We define the mapping in (7)
asΣ =C(D). Deploying C(·)at every time
step in the sampling-based MPC framework,
we propose CoVO-MPC (Alg. 1), a general
sampling-based MPC algorithm.
As shown in Line 2 of Alg. 1, at each time
step,CoVO-MPC will first generate an opti-
mal sampling covariance matrix Σtbased on
the cost’s Hessian matrix around the sampling
mean∇2Jt(Uin|t). Next, we will follow the
MPPI framework to calculate control sequences from the sampling distribution (Lines 3 to 5) and
execute the first control command in the sequence (Line 6). After that, we will shift the sampling
mean forward to the next time step using the shift operator (Line 7) and repeat the process.
The optimal covariance design (Line 2) is critical for CoVO-MPC , which requires computing the
Hessian matrix Dt=∇2Jt(Uin|t))in real-time. The total cost Jt(Uin|t) =PH
h=1ct+h(xt+h|t,[Uin|t]h)
is gathered by rolling out the sampling mean sequence Uin|twith the dynamics initialized at xt,
where xt+h|tis the hthrollout state and ctis the running cost at time step t. To ensure Dtto be pos-
itive definite, we add a small positive value to the diagonal elements of Dtsuch that Dt⪰ϵI≻0,
which is equivalent to adding a small quadratic control penalty term to the cost function.
Offline covariance matrix approximation. Computing DtandΣtin real time could be expen-
sive. Therefore, we propose an offline approximation variant of CoVO-MPC . In this variant, we
cache the covariance matrices across all toffline by rolling out the dynamics using a nominal con-
troller (e.g., PID). More specifically, offline, we calculate the whole sequence of covariance matrix
8COVARIANCE -OPTIMAL MPC
Σoff
1:Tin simulation using the nominal controller. Then, online, Σoff
tserves as an approximation of
the optimal covariance matrix. The details can be found in Appendix B.1. Empirically, we observe
that this offline approximation performs marginally worse than CoVO-MPC with less computation.
6. Experiments
We evaluate CoVO-MPC in three nonlinear robotic systems. To demonstrate the effectiveness of
CoVO-MPC , we first compare it with the standard MPPI , in both simulation and the real world.
To further understand the difference between CoVO-MPC andMPPI , we quantify their computa-
tional costs and visualize their cost distributions. Our results show that CoVO-MPC significantly
outperforms MPPI with a more concentrated cost distribution.
6.1. Tasks and Implementations
16 32 64 128 256 512 1024
Sampling number (N)5.07.510.012.515.017.520.0Position tracking error (cm)method
MPPI
CoVO-MPC (offline approx)
CoVO-MPC
Figure 1: Quadrotor tracking errors as
the sample number Nincreases.We choose three tasks (illustrated as Fig. 3 in Ap-
pendix B.2): (a) CartPole environment with
force applied to the car as control input (Lange,
2023). (b) Quadrotor follows zig-zag infeasible
trajectories. The action space is the desired thrust
and body rate ( dim(u) = 4 ). (c) Quadrotor
(real) : same tasks as (b) but on a real-world
quadrotor platform based on the Bitcraze Crazyflie
2.1 (Huang et al., 2023; Shi et al., 2021).
For both the simulation and the real quadrotor,
all results were evaluated across 3different trajec-
tories, each repeatedly executed 10times. The im-
plementation details of all tasks can be found in Ap-
pendix B.2. All tasks use the same hyperparameter (Table 3 in Appendix B.1). It is worth noting
that Alg. 1 ensures the covariance matrix’s determinant of CoVO-MPC is identical to that of the
MPPI baseline, to keep their sampling volumes the same.
6.2. Performance and Computational Cost
Table 1 illustrates the evaluated cost for each task. CoVO-MPC outperforms MPPI across all tasks
with varying performance gains from 43% to54%. In simulations, the performance of CoVO-MPC
(offline approx.) stays close to CoVO-MPC by approximating the Hessian matrix offline using
simple nominal PID controllers, which implies the effectiveness of CoVO-MPC comes from the
optimized non-trivial pattern of Σt(Fig. 4 in Appendix C) rather than its precise values. When
transferred to the real-world quadrotor control, the offline approximation’s performance degrades
due to sim-2-real gaps but still outperforms MPPI by a significant margin ( 22%). The real-world
tracking results are visualized in Fig. 2, where CoVO-MPC can effectively track the desired triangle
trajectory while MPPI fails.
Besides optimality, another important aspect of sampling-based MPC is its sampling efficiency.
To understand CoVO-MPC ’s sampling efficiency, we evaluate all methods in quadrotor simulation
with various sampling numbers N. The results in Fig. 1 show that CoVO-MPC and its approximated
variant with fewer samples outperforms the standard MPPI algorithm with much more samples.
9YIPANHEQUSHI
Tasks CartPole Quadrotor Quadrotor (real)
CoVO-MPC 0.70±0.21 3 .71±0.37 8 .36±4.86
CoVO-MPC (offline approx.) 0.71±0.23 3 .86±0.34 12 .09±6.33
MPPI 1.52±0.46 6 .48±0.64 15 .49±5.82
Table 1: The cost associated with CoVO-MPC compared with that of MPPI while tracking an infea-
sible zig-zag trajectory. For Quadrotor andQuadrotor (real) , the value indicates
tracking error in centimeters.
Figure 2: (a-b) Real-world quadrotor trajectory tracking results. CoVO-MPC can track the challeng-
ing infeasible triangle trajectory closer than MPPI . (c) The cost distribution of sampled
trajectories at a certain time step in Quadrotor simulation. The cost of CoVO-MPC is
more concentrated and has a lower mean than MPPI .
To further understand the performance difference between CoVO-MPC andMPPI , we plot the
cost distributions of their sampled trajectories at a particular time step in Fig. 2(c). The cost of
CoVO-MPC is more concentrated and has a lower mean than MPPI , which directly implies the
effectiveness of the optimal covariance procedure of CoVO-MPC . In other words, CoVO-MPC can
automatically adapt to different optimization landscapes while MPPI cannot.
Algorithm Online Time (ms) Offline Time (ms)
CoVO-MPC 9.22±0.39 0 .26±0.47
Offline approx. 2.03±0.53 2179 .59±4.57
MPPI 2.17±0.67 2 .16±0.29
Table 2: Computational time comparison.As expected, the superior
performance of CoVO-MPC
comes with extra computa-
tional costs for the Hessian
matrix Dtand the optimal co-
variance Σt. Therefore, we
quantify extra computational burdens in Table 2. While CoVO-MPC indeed requires more com-
putation, the offline approximation of CoVO-MPC has the same computational cost as MPPI but
with better performance.
7. Limitations and Future Work
While CoVO-MPC introduces new theoretical perspectives and shows compelling empirical per-
formance, it currently relies on the differentiability of the system. Our future efforts will focus
on adapting our theory and algorithm to more general scenarios. We also aim to investigate the
finite-sample analysis of sampling-based MPC with a focus on the variance of the algorithm out-
10COVARIANCE -OPTIMAL MPC
put. Generalizing our results to receding-horizon settings (Lin et al., 2021; Yu et al., 2020) is also
interesting. Moreover, CoVO-MPC holds substantial promise for MBRL, so we plan to integrate
CoVO-MPC with the MBRL framework, leveraging learned dynamics, value functions, or policies.
We also recognize there exist efficient online approximations of CoVO-MPC , e.g., reusing previous
Hessians or covariances, which could lead to more efficient implementations.
References
Arthur Argenson and Gabriel Dulac-Arnold. Model-Based Offline Planning, March 2021.
Isin M. Balci, Efstathios Bakolas, Bogdan Vlahov, and Evangelos Theodorou. Constrained Covari-
ance Steering Based Tube-MPPI, April 2022.
Mohak Bhardwaj, Ankur Handa, Dieter Fox, and Byron Boots. Information Theoretic Model Pre-
dictive Q-Learning, May 2020.
Zdravko I. Botev, Dirk P. Kroese, Reuven Y . Rubinstein, and Pierre L’Ecuyer. Chapter 3 - The
Cross-Entropy Method for Optimization. In C. R. Rao and Venu Govindaraju, editors, Handbook
ofStatistics, volume 31 of Handbook ofStatistics, pages 35–59. Elsevier, January 2013. doi:
10.1016/B978-0-444-53859-8.00003-5.
Franco Busetti. Simulated annealing overview. World Wide Web URL www. geocities.
com/francorbusetti/saweb. pdf, 4, 2003.
Jan Carius, Ren ´e Ranftl, Vladlen Koltun, and Marco Hutter. Trajectory Optimization With Implicit
Hard Contacts. IEEE Robotics andAutomation Letters, 3(4):3316–3323, October 2018. ISSN
2377-3766. doi: 10.1109/LRA.2018.2852785.
Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep Reinforcement
Learning in a Handful of Trials using Probabilistic Dynamics Models, November 2018.
Paul Drews, Grady Williams, Brian Goldfain, Evangelos A. Theodorou, and James M. Rehg.
Vision-Based High Speed Driving with a Deep Dynamic Observer, December 2018.
Hugh Durrant-Whyte, Nicholas Roy, and Pieter Abbeel. Cross-Entropy Randomized Motion Plan-
ning. In Robotics: Science andSystems VII, pages 153–160. MIT Press, 2012.
Frederik Ebert, Chelsea Finn, Sudeep Dasari, Annie Xie, Alex Lee, and Sergey Levine. Visual Fore-
sight: Model-Based Deep Reinforcement Learning for Vision-Based Robotic Control, December
2018.
Michael G. Forbes, Rohit S. Patwardhan, Hamza Hamadah, and R. Bhushan Gopaluni. Model
Predictive Control in Industry: Challenges and Opportunities. IFAC-PapersOnLine, 48(8):531–
538, January 2015. ISSN 2405-8963. doi: 10.1016/j.ifacol.2015.09.022.
Manan S. Gandhi, Bogdan Vlahov, Jason Gibson, Grady Williams, and Evangelos A. Theodorou.
Robust Model Predictive Path Integral Control: Analysis and Performance Guarantees. IEEE
Robotics andAutomation Letters, 6(2):1423–1430, April 2021. ISSN 2377-3766, 2377-3774.
doi: 10.1109/LRA.2021.3057563.
11YIPANHEQUSHI
Wojciech Giernacki, Mateusz Skwierczy ´nski, Wojciech Witwicki, Paweł Wro ´nski, and Piotr Kozier-
ski. Crazyflie 2.0 quadrotor as a platform for research and education in robotics and control en-
gineering. In 2017 22nd International Conference onMethods andModels inAutomation and
Robotics (MMAR), pages 37–42, August 2017. doi: 10.1109/MMAR.2017.8046794.
Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James
Davidson. Learning Latent Dynamics for Planning from Pixels, June 2019.
Nicklas Hansen, Xiaolong Wang, and Hao Su. Temporal Difference Learning for Model Predictive
Control, July 2022.
Nikolaus Hansen, Sibylle D. M ¨uller, and Petros Koumoutsakos. Reducing the Time Com-
plexity of the Derandomized Evolution Strategy with Covariance Matrix Adaptation (CMA-
ES). Evolutionary Computation, 11(1):1–18, March 2003. ISSN 1063-6560. doi: 10.1162/
106365603321828970.
Bjarne E. Helvik and Otto Wittner. Using the Cross-Entropy Method to Guide/Govern Mobile
Agent’s Path Finding in Networks. In Samuel Pierre and Roch Glitho, editors, Mobile Agents for
Telecommunication Applications, Lecture Notes in Computer Science, pages 255–268, Berlin,
Heidelberg, 2001. Springer. ISBN 978-3-540-44651-4. doi: 10.1007/3-540-44651-6 24.
Kevin Huang, Rwik Rana, Alexander Spitzer, Guanya Shi, and Byron Boots. Datt: Deep adaptive
trajectory tracking for quadrotor control. In 7thAnnual Conference onRobot Learning, 2023.
Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to Trust Your Model: Model-
Based Policy Optimization, November 2021.
Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy H. Campbell, Konrad
Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, Afroz Mohiuddin,
Ryan Sepassi, George Tucker, and Henryk Michalewski. Model-Based Reinforcement Learning
for Atari, February 2020.
Robert Tjarko Lange. Reinforcement Learning Environments in JAX, November 2023.
Weiwei Li and Emanuel Todorov. Iterative linear quadratic regulator design for nonlinear biolog-
ical movement systems. In Proceedings oftheFirst International Conference onInformatics in
Control, Automation andRobotics, pages 222–229, Set ´ubal, Portugal, 2004. SciTePress - Science
and and Technology Publications. ISBN 978-972-8865-12-2. doi: 10.5220/0001143902220229.
Yiheng Lin, Yang Hu, Guanya Shi, Haoyuan Sun, Guannan Qu, and Adam Wierman. Perturbation-
based regret analysis of predictive control in linear time varying systems. Advances inNeural
Information Processing Systems, 34:5174–5185, 2021.
Kendall Lowrey, Aravind Rajeswaran, Sham Kakade, Emanuel Todorov, and Igor Mordatch. Plan
Online, Learn Offline: Efficient Learning and Exploration via Model-Based Control, January
2019.
Shie Mannor, Reuven Rubinstein, and Yohai Gat. The Cross Entropy Method for Fast Policy Search.
Proceedings, Twentieth International Conference onMachine Learning, 2003.
12COVARIANCE -OPTIMAL MPC
David Mayne. A second-order gradient method for determining optimal trajectories of non-linear
discrete-time systems. International Journal ofControl, 3(1):85–95, 1966.
David Mayne. Robust and stochastic model predictive control: Are we going in the right di-
rection? Annual Reviews inControl, 41:184–192, January 2016. ISSN 1367-5788. doi:
10.1016/j.arcontrol.2016.04.006.
Ishai Menache, Shie Mannor, and Nahum Shimkin. Basis Function Adaptation in Temporal Differ-
ence Reinforcement Learning. Annals ofOperations Research, 134(1):215–238, February 2005.
ISSN 0254-5330, 1572-9338. doi: 10.1007/s10479-005-5732-z.
Konstantin Mishchenko. Regularized Newton Method with Global $O(1/k ˆ2)$ Convergence, March
2023.
Tung Nguyen, Rui Shu, Tuan Pham, Hung Bui, and Stefano Ermon. Temporal Predictive Coding
For Model-Based Planning In Latent Space, June 2021.
Michael O’Connell, Guanya Shi, Xichen Shi, Kamyar Azizzadenesheli, Anima Anandkumar,
Yisong Yue, and Soon-Jo Chung. Neural-fly enables rapid learning for agile flight in strong
winds. Science Robotics, 7(66):eabm6597, 2022.
Jintasit Pravitra, Kasey A. Ackerman, Chengyu Cao, Naira Hovakimyan, and Evangelos A.
Theodorou. L1-Adaptive MPPI Architecture for Robust and Agile Control of Multirotors.
In2020 IEEE/RSJ International Conference onIntelligent Robots andSystems (IROS), pages
7661–7666, October 2020. doi: 10.1109/IROS45743.2020.9341154.
James A. Preiss, Wolfgang Honig, Gaurav S. Sukhatme, and Nora Ayanian. Crazyswarm: A large
nano-quadcopter swarm. In 2017 IEEE International Conference onRobotics andAutomation
(ICRA), pages 3299–3304, May 2017. doi: 10.1109/ICRA.2017.7989376.
Jacob Sacks, Rwik Rana, Kevin Huang, Alex Spitzer, Guanya Shi, and Byron Boots. Deep Model
Predictive Optimization, October 2023.
Guanya Shi, Xichen Shi, Michael O’Connell, Rose Yu, Kamyar Azizzadenesheli, Animashree
Anandkumar, Yisong Yue, and Soon-Jo Chung. Neural lander: Stable drone landing control
using learned dynamics. In 2019 international conference onrobotics andautomation (icra),
pages 9784–9790. IEEE, 2019.
Guanya Shi, Wolfgang H ¨onig, Xichen Shi, Yisong Yue, and Soon-Jo Chung. Neural-swarm2:
Planning and control of heterogeneous multirotor swarms using learned interactions. IEEE
Transactions onRobotics, 38(2):1063–1079, 2021.
A. Sideris and J.E. Bobrow. An efficient sequential linear quadratic algorithm for solving nonlinear
optimal control problems. In Proceedings ofthe2005, American Control Conference, 2005.,
pages 2275–2280 vol. 4, June 2005. doi: 10.1109/ACC.2005.1470308.
Yunlong Song, Angel Romero, Matthias M ¨uller, Vladlen Koltun, and Davide Scaramuzza. Reaching
the limit in autonomous racing: Optimal control versus reinforcement learning. Science Robotics,
8(82):eadg1462, September 2023. doi: 10.1126/scirobotics.adg1462.
13YIPANHEQUSHI
Istv´an Szita and Andr ´as L¨orincz. Learning tetris using the noisy cross-entropy method, 2006.
Yuval Tassa, Nicolas Mansard, and Emo Todorov. Control-limited differential dynamic program-
ming. In 2014 IEEE International Conference onRobotics andAutomation (ICRA), pages 1168–
1175, May 2014. doi: 10.1109/ICRA.2014.6907001.
Nolan Wagener, Ching-an Cheng, Jacob Sacks, and Byron Boots. An Online Learning Approach to
Model Predictive Control. In Robotics: Science andSystems XV. Robotics: Science and Systems
Foundation, June 2019. ISBN 978-0-9923747-5-4. doi: 10.15607/RSS.2019.XV .033.
Ziyi Wang, Oswin So, Jason Gibson, Bogdan Vlahov, Manan S. Gandhi, Guan-Horng Liu, and
Evangelos A. Theodorou. Variational Inference MPC using Tsallis Divergence, April 2021.
Grady Williams, Paul Drews, Brian Goldfain, James M. Rehg, and Evangelos A.
Theodorou. Aggressive driving with model predictive path integral control. In 2016
IEEE International Conference onRobotics andAutomation (ICRA), pages 1433–1440, May
2016. doi: 10.1109/ICRA.2016.7487277.
Grady Williams, Nolan Wagener, Brian Goldfain, Paul Drews, James M. Rehg, Byron Boots, and
Evangelos A. Theodorou. Information theoretic MPC for model-based reinforcement learning.
In2017 IEEE International Conference onRobotics andAutomation (ICRA), pages 1714–1721,
May 2017. doi: 10.1109/ICRA.2017.7989202.
Chenkai Yu, Guanya Shi, Soon-Jo Chung, Yisong Yue, and Adam Wierman. The power of pre-
dictions in online control. Advances inNeural Information Processing Systems, 33:1994–2004,
2020.
Marvin Zhang, Sharad Vikram, Laura Smith, Pieter Abbeel, Matthew J. Johnson, and Sergey
Levine. SOLAR: Deep Structured Representations for Model-Based Reinforcement Learning,
June 2019.
Appendix A. Proof Details
A.1. Proof Details for Quadratic J(U)
We first introduce Slutsky’s Theorem below.
Lemma 7 (Slutsky’ theorem) LetXn, Ynbe sequences of scalar/vector/matrix random variables.
IfXnconverges in probability to a random element XandYnconverges in probability to a constant
c, then Xn+Ynp→X+c;XnYn→Xc;Xn/Ynp→X/c, provided that cis invertible, wherep→
denotes convergence in probability.
With Slutsky’s Theorem, we now give the detailed proof of Theorem 1.
Proof Firstly, we focus on the case Uin=U∗and show the optimal point U∗serves as a fixed point
of (2). Then, we move on to the general case.
The case Uin=U∗.The quadratic nature of the function J(U)allows us to express it as
U⊤DU+U⊤d+Jc=J(U∗) + (U−U∗)⊤D(U−U∗),
14COVARIANCE -OPTIMAL MPC
withJcbeing a constant and J∗defined as J(U∗). To show the convergence of (2), we separately
analyze the distribution and convergence properties of the numerator and denominator of (2).
Our first step is to demonstrate that the denominator converges to a constant in probability. To
this end, we calculate the expectation of exp(−1
λJ(U)).
E(exp(−1
λJ(U)))
=Zexp(−J∗
λ)
p
(2π)kdet1
2|Σ|exp(−1
λ(U−U∗)⊤D(U−U∗)−1
2(U−U∗)⊤Σ−1(U−U∗))dU
=exp(−J∗
λ)
det1
2|I+2
λDΣ|.
Utilizing the weak law of large numbers, we observe that1
NP
iexp(−J(Ui)
λ)converges in probabil-
ity toE(exp(−J(U)
λ)) =exp(−J∗
λ)
det1
2|I+2
λDΣ|. This convergence is due to the Ui’s being independent and
identically distributed (i.i.d.) samples from the random variable U. Notably, convergence in proba-
bility implies convergence in distribution; hence,1
NP
iexp(−J(Ui)
λ)also converges in distribution
toexp(−J∗
λ)
det1
2|I+2
λDΣ|.
Next, we focus on analyzing the convergence properties of the numerator by first computing the
expectation of Uexp(−J(U)
λ):
E(Uexp(−1
λJ(U)))
=ZUexp(−J∗
λ)
p
(2π)kdet1
2|Σ|exp(−1
λ(U−U∗)⊤D(U−U∗)−1
2(U−U∗)⊤Σ−1(U−U∗))dU
=ZUexp(−J∗
λ)
p
(2π)kdet1
2Σexp(−1
2(U−U∗)⊤(Σ−1+2
λD)(U−U∗))dU
=U∗exp(−J∗
λ)
det1
2(I+2
λDΣ)
Combining the results from the denominator and numerator, using Slutsky’s Theorem, we are
able to show that Uout− →
pU∗.
We next show the general contraction property of MPPI where Uinmay not necessarily be U∗.
General case. Recall that now U ∼ N (Uin,Σ). And the cost function can be reorganized into
J(U) = (U−U∗)⊤D(U−U∗) +J∗
= (U−Uin)⊤D(U−Uin) + (Uin−U∗)⊤D(Uin−U∗)
+ (Uin−U∗)⊤D(U−Uin) + (U−Uin)⊤D(Uin−U∗) +J∗
15YIPANHEQUSHI
We then again calculate the expectation of the denominator:
E(exp(−1
λJ(U)))
=Zexp(−J(U)
λ)
p
(2π)kdet1
2Σexp(−1
2(U−Uin)⊤Σ−1(U−Uin))dU
=Zexp(−J∗+(Uin−U∗)⊤D(Uin−U∗)
λ)
p
(2π)kdet1
2Σ·exp(−1
λ(U−Uin)⊤D(U−Uin)−1
2(U−Uin)⊤Σ−1(U−Uin))·
exp(−1
λ(Uin−U∗)⊤D(U−Uin) + (U−Uin)⊤D(Uin−U∗))dU
=CZ
exp(−1
2(U+ (2
λΣ−1+D)−1D(Uin−U∗))⊤(Σ−1+2
λD)(U+ (λ
2Σ−1+D)−1D(Uin−U∗)))dU
Notice that the last equality coming from a change of variable that we substitute UwithU+Uin,
andCis defined as
C=exp(−J∗+(Uin−U∗)⊤D(Uin−U∗)
λ+2
λ2(Uin−U∗)⊤D(Σ−1+2
λD)−1D(Uin−U∗))
det1
2Σp
(2π)k.
So the expectation of the denominator is E(exp(−1
λJ(U))) = C√
(2π)kdet1
2Σ
det1
2I+2
λDΣ.
Similarly, for the numerator, we have
E(Uexp(−1
λJ(U)))
=CZ
(U+Uin) exp(−(U+ (2
λΣ−1+D)−1D(Uin−U∗))⊤(Σ−1+2
λD)(U+ (λ
2Σ−1+D)−1D(Uin−U∗)))
2dU
=UinE(exp(−1
λJ(U))) +2
λ(Σ−1+2
λD)−1D(U∗−Uin)E(exp(−1
λJ(U)))
With the weak law of large number1
NP
iexp(−1
λJ(Ui))− →
pE(exp(−1
λJ(U)))and1
NP
iUiexp(−1
λJ(Ui))− →
p
E(Uexp(−1
λJ(U))). We then apply Slutsky’s theorem here and can get
Uout=1
NPUexp(−1
λJ(U))
1
NPexp(−1
λJ(U))− →
pUin+2
λ(Σ−1+2
λD)−1D(U∗−Uin).
Therefore, we have
(Uout−U∗)− →
p(I−2
λ(Σ−1+2
λD)−1D)(Uin−U∗) = (2
λΣD+I)−1(Uin−U∗). (8)
Because the RHS of (8) is a constant, we are allowed to take a norm on both sides to get ∥Uout−U∗∥ − →
p(Uin−U∗)(2
λΣD+I)−1. Then, with Cauchy-Schwartz inequality, we can have
(2
λΣD+I)−1(Uin−U∗)
2≤ ∥Uin−U∗∥2(2
λΣD+I)−1
2.
16COVARIANCE -OPTIMAL MPC
This leads to
∥Uout−U∗∥2
∥Uin−U∗∥2− →
pContractionRate ≤(2
λΣD+I)−1
2≤1
By Continuous Mapping Theorem, J(U)is a continuous function of U, so
J(Uout)− →
p(Uin−U∗)⊤(2
λΣD+I)−⊤D(2
λΣD+I)−1(Uin−U∗) +J∗
= (Uin−U∗)⊤D1
2D−1
2(2
λΣD+I)−⊤D1
2D1
2(2
λΣD+I)−1D−1
2D1
2(Uin−U∗) +J∗
= (D1
2(Uin−U∗))⊤(2
λD1
2ΣD1
2+I)−⊤(2
λD1
2ΣD1
2+I)−1(D1
2(Uin−U∗)) +J∗
=(2
λD1
2ΣD1
2+I)−1(D1
2(Uin−U∗))2
2+J∗
≤(2
λD1
2ΣD1
2+I)−12
2(D1
2(Uin−U∗))2
2+J∗
=(2
λD1
2ΣD1
2+I)−12
2(J(Uin)−J∗) +J∗(9)
where the inequality comes from Cauchy-Schwartz inequality and the last equation comes from
J(Uin)−J∗= (Uin−U∗)⊤D(Uin−U∗) =(D1
2(Uin−U∗))2
2. As a result, the above leads to
J(Uout)−J∗− →
pJout−J∗≤(J(Uin)−J∗)(I+2
λD1
2ΣD1
2)−2.
The contraction of the cost function have a single-step contraction rate of(I+2
λD1
2ΣD1
2)−2.
This means that if we apply (2) to solve (1) iteratively with infinite samples, then it takes
O(log1
ϵ)steps to reach J(Uout)−J∗≤ϵ. We also notice that the contraction shares the same
form with regularized newtown’s method. Regularized Newtown’s method has the following form
Unewton
in =Uin− 
∇2J(Uin) +CI−1∇f(Uin) =Uin−(D+CI)−1D(Uin−U∗)
Organizing the RHS, we can find Unewton
in = (D
C+I)−1(Uin−U∗). When taking Σ =λ
2CI
(Identity is a common choice for MPPI), the expectation of the output control as in (5) follows
exactly the regularized Newtown’s method, which runs Newtown’s method on regularized cost
function J(U) +λ(U−Uin)⊤(U−Uin). Such regularized Newton’s method, in general, has
contraction, which is consistent with the Theorem 1 above. Further, as Theorem 2 of Mishchenko
(2023) shows, with properly (sometimes adaptively) chosen λandΣ(for instance, Σ = I, λ∝p
∥D(Uin−U∗)∥), we only need O(log log(1
ϵ))iterations for MPPI to converge.4
4. The convergence rate only holds locally as stated in the literature. However, this neighborhood is actually defined by
the inverse of Hessian’s Lipschitz. For quadratic function, the Hessian is a constant. Therefore, it holds globally.
17YIPANHEQUSHI
A.2. Proof Details for Optimal Covariance Design
Initially, let’s assume that ΣandDare commutative. The justification for this assumption will be
elaborated upon in the proof of Corollary 3. Under this assumption, ΣandDshare the same set of
eigenvectors, denoted by the matrix V. Correspondingly, we define ΛandO, both in RmH×mH,
where Λ = diag([Λ]1,[Λ]2, . . . , [Λ]mH)andO=diag([O]1,[O]2, . . . , [O]mH).
Moreover, let Λin∈RmHrepresent the vector formed by the coordinates Uin−U∗in the basis of
V. In mathematical terms, this is expressed as Λin=V(Uin−U∗) = [[Λ in]1,[Λin]2, . . . , [Λin]mH]⊤.
With the above setup, here we give the detailed proof of Theorem 2.
Proof The objective function is
(Uin−U∗)⊤(2
λΣD+I)−⊤D(2
λΣD+I)−1(Uin−U∗)
And substitute ΣwithV⊤ΛVandDwithV⊤OV. The optimization objective is equivalent to
(Uin−U∗)⊤(2
λV⊤ΛV V⊤OV+I)−⊤V⊤OV(2
λV⊤ΛV V⊤OV+I)−1(Uin−U∗),
which simplifies to (Uin−U∗)⊤V⊤(2
λΛO+I)−1O(2
λΛO+I)−1V(Uin−U∗). Therefore, the
objective of the optimization problem is:
[V(Uin−U∗)]⊤(2
λΛO+I)−1O(2
λΛO+I)−1[V(Uin−U∗)]
Because Λ,Oare now diagonal matrix, we can rewrite it into:
mHX
i=1[Λin]2
i[O]i
(2
λ[O]i[Λ]i+ 1)2
The constraint optimization problem can be rewrite into:
min
[Λ]mHX
i=1[Λin]2
i[O]i
(2
λ[O]i[Λ]i+ 1)2
s.t.Y
[Λ]i≤α,[Λ]i>0
Notice that the objective is monotone with respect to any [Λ]i. We now employ the Lagrangian
multiplier method here.
The Lagrangian Lfor the optimization problem combines the objective function with the con-
straint using a Lagrangian multiplier µ. The problem is:
min
[Λ]mHX
i=1[Λin]2
i[O]i
(2
λ[O]i[Λ]i+ 1)2
s.t.Y
[Λ]i≤α,[Λ]i>0
The corresponding Lagrangian is:
L([Λ], µ) =mHX
i=1[Λin]2
i[O]i
(2
λ[O]i[Λ]i+ 1)2+µ mHY
i=1[Λ]i−α!
18COVARIANCE -OPTIMAL MPC
To find the minimum, we set the derivative of Lwith respect to [Λ]iandµto zero.
For each i= 1,···, mH , the derivative with respect to [Λ]iis:
∂L
∂[Λ]i=−2[Λin]2
i[O]2
i
(2
λ[O]i[Λ]i+ 1)3+µ∂
∂[Λ]i
mHY
j=1[Λ]j
= 0
The derivative of the product termQmH
j=1[Λ]jwith respect to [Λ]iis the product of all [Λ]jterms
except for [Λ]i.
[O]2
i[Λin]2
i[Λ]i
(1 +2
λ[Λ]i[O]i)3=µmHY
j=1[Λ]j,∀i= 1,···, mH. (10)
It is clear thatQmH
j=1[Λ]jis a constant for any i. According to the Lagrange multiplier theorem
any maximum or minimum corresponding to a µthat solves (10). Along with the fact that the
problem is monotonically decreasing with respect to any [Λ]i, and the biggest allowedQmH
j=1[Λ]jis
α. Therefore, the solution is:
[O]2
i[Λin]2
i[Λ]i
(1 +2
λ[Λ]i[O]i)3=Const, ∀i= 1,···, mH.
s.t.mHY
j=1[Λ]j=α
Remark 8 Without further knowledge of the gap between U∗andUin, the most reasonable way
is to assume that Uin−U∗∼ N (0, I). We define CJ= (2
λΣD+I)−⊤D(2
λΣD+I)−1Then
E(Uin−U∗)⊤CJ(Uin−U∗) = Tr( CJ). Moreover, if we fix the norm of ∥(Uin−U∗)∥= 1. The
mini-max problem of (Uin−U∗)⊤CJ(Uin−U∗)with respect to CJ
min
CJmax
U∗(Uin−U∗)⊤CJ(Uin−U∗)
s.t.∥(Uin−U∗)∥= 1
has a solution of CJ∝I.
Now we give the proof of Corollary 3.
Proof From the remark, our goal is to design (2
λΣD+I)−⊤D(2
λΣD+I)−1∝Iwhen λ→0.
And(2
λΣD+I)−⊤D(2
λΣD+I)−1= (2
λΣD1/2+D−1/2)−⊤(2
λΣD1/2+D−1/2)−1. Therefore
(2
λΣD1/2+D−1/2)⊤(2
λΣD1/2+D−1/2)∝I
. When λ→0.(2
λΣD1/2+D−1/2)⊤(2
λΣD1/2+D−1/2)≈4
λ2D1
2Σ2D1
2∝I. So, we can say that
Σ∝D−1
2. And Σ = ( αdetD1/2)1
mHD−1/2
19YIPANHEQUSHI
A.3. Proof Details for Non-Quadratic J(U)
A.3.1. S TRONGLY CONVEX
In numerous practical scenarios, the total cost function Jexhibits strong convexity without being
quadratic. As an example, this is often encountered in RL reward designs, where instead of the
standard quadratic form x⊤Qx, a nonlinear convex function Q(x)is used. Motivated by such cases,
we here consider the convergence property of a β-strongly convex total cost function J(U)that
satisfies the inequality J(U′)≥J(U) +∇J(U)⊤(U′−U) +β
2∥U′−U∥2for all U′andU.
Theorem 4 Given a β-strongly convex function J(U)which has Lipschitz continuous derivatives,
that is∂J
∂UisLd-Lipschitz, Uoutconverges to a neighborhood of the optimal point U∗in probability
as the number of samples Ntends to infinity, i.e., Uout− →
pUc. Here Ucsatisfies ∥Uc−U∗∥ ≤
λ
β2∥Σ−1∥
(1+λ
β∥Σ∥)∥Uin−U∗∥+∥Uerror∥, where ∥Uerror∥ ∼O(√
λ
β).
In Theorem 4, notice that Uoutconsists of two parts: a contraction from UintoU∗of the form
O(λ
β∥Uin−U∗∥)and an error term (coming from the nature of the softmax style zeroth order
method) O(√
λ
β). This implies that with a small λ, MPPI maintains a linear contraction rate of
O(λ
β), which approaches zero as λ≪β. The remaining residual error Uerror, ofO(√
λ
β), suggests
the eventual convergence of MPPI to a neighborhood around the optimal point of size O(√
λ
β).
Notably, this contraction rate differs from that in Sec. 4.1 because non quadratic J(U)leads to
slightly larger constant in some inequalities in the proof. We now provide a proof for Theorem 4.
Proof According to Importance Sampling, a random variable U’s expectation under distribution
P(U)can be estimated by samples from distribution Q(U)with the following equation:
ˆEPU=1
NNX
i=0UiP(Ui)
Q(Ui)(11)
Let
P(U)∝ N(U|Uin,Σ) exp( −J(U)
λ) = exp( −J′(U)
λ)
where J′(U)is defined as J′(U) =U⊤(D+λ
2Σ−1)U+U⊤(d−λΣ−1Uin). We also define Q(U)∼
N(Uin,Σ)and substitute into (11), we can find (2) is estimating distribution P(U)∝exp(−J′(U)
λ)
with samples from distribution N(Uin,Σ). Therefore, from weak law of large number, we have
Uout− →
pEPU. This means that to prove Theorem 4, we only need to bound the distance between
the distribution’s mean EPUandU∗, which can be bounded by a combination of two parts. The first
part is the gap between EPU, and the optimal point of J′(U); the second part is the gap between
the optimal point of J′(U)andU∗. We bound these two parts separately below.
Part 1: Gap between EPUand the optimal point of J′(U).The approach in this section begins
by transforming the function J′(U)into˜J(U) =J′(U−U∗′), where U∗′represents the optimizer
ofJ′. We denote ˜p(U)as the distribution determined by ˜J(U)that˜p(U)∝exp(−˜J(U)) =
exp(−J′(U−U∗′)). Therefore, EP(U) =E˜P(U) +U∗′, and we have
20COVARIANCE -OPTIMAL MPC
EPU=R
Uexp(−J′(U)/λ)dUR
exp(−J′(U)/λ)dU=R
Uexp(−˜J(U)/λ)dUR
exp(−˜J(U)/λ)dU| {z }
E˜P(U)+U∗′(12)
This leads to a bound on EP(U)thatE˜P(U). Here we will give the bound of E˜P(U)by separately
determining the upper bound for the numerator and the lower limit for the denominator, setting the
stage for the subsequent analysis.
This redefinition places the optimal point of ˜J(U)at˜J(0) = 0 . Moreover, ˜J(U)also has
Lipschitz continuous deriavtive, satisfying the condition∂2˜J(U)
∂U2≤LH,∀U. Given that ˜J(0) = 0
and∂˜J
∂U= 0|U=0, we can bound ˜J(U)by˜J(U)≤1
2LH∥U∥2. Consequently, we can express this
bound as˜J(U)
λ≤LH
2∥U∥2
λ.
We have thatR
exp(−˜J(U)/λ)dU≥R
exp(−1
2LH∥U∥2/λ)dU=cdR∞
0e−LH
2λx2dx, where
cdis a dimension specific constant (e.g. c1= 2c2= 2π,c3= 4π). Further, the integral is in the
family of Gamma function becauseR∞
0e−tmdt=1
mR∞
0s1/m−1e−sds=1
mΓ(1/m) = Γ(1+1 /m).
By changing the variable, we can getR∞
0e−tm/adt=R∞
0e−vma1/mdv=a1/mR∞
0e−tm/adt. By
taking m= 2anda=2λ
LH, and we have
Z
exp(−˜J(U)/λ)dU≥cdcΓ(2λ
LH)1
2=cdcΓ(2
LH)1
2λ1
2, (13)
where cΓ= Γ(3
2)≈0.886, is a constant. With the fact that Jisβ-strongly convex, we now that
J′and˜Jare at least β-strongly convex. Meanwhile, the optimal point for ˜JisU= 0. Therefore,
˜J(U)≥U⊤β
2IU.We can then boundR
Uexp(−˜J(U)/λ)dUwith
Z
Uexp(−˜J(U)/λ)dU≤Z
∥U∥exp(−β
2λU⊤U)dU=cdλ
β(14)
Putting (13) and (14) together, we can bound the gap between the mean EUof distribution P(U)∝
exp(−J′(U)
λ)with the optimal point U∗′ofJ′as follows:
EpU −U∗′≤λ
β√LH√
2λcΓ∼O(√
λ
β). (15)
Part 2: gap between U∗and optimal point of J′(U). Recall that
J′(U) =J(U)−λU⊤
inΣ−1U+1
2λU⊤Σ−1U.
From the definition of U∗′, we have J′(U∗′)≤J′(U∗), which leads to
J(U∗′)−λU⊤
inΣ−1U∗′+1
2λU∗′⊤Σ−1U∗′≤J(U∗)−λU⊤
inΣ−1U∗+1
2λU∗⊤Σ−1U∗.
Because J(U)isβ-Strongly Convex and ∇J(U∗) = 0 , we have,
J(U∗′)≥J(U∗) + (U∗′−U∗)⊤β
2I(U∗′−U∗).
21YIPANHEQUSHI
As a result, we get
J(U∗)+(U∗′−U∗)⊤β
2I(U∗′−U∗)−λU⊤
inΣ−1U∗′+1
2λU∗′⊤Σ−1U∗′≤J(U∗)−λU⊤
inΣ−1U∗+1
2λU∗⊤Σ−1U∗.
Denoting (U∗′−U∗)with△U, we can get
△U⊤β
2I△U−λU⊤
inΣ−1△U+1
2λ△U⊤Σ−1△U+λU∗⊤Σ−1△U≤0.
Simplifying it, we can get
△U⊤(β
2I+1
2λΣ−1)△U≤λ(Uin−U∗)⊤Σ−1△U,
which leads to
∥△U∥2(β
2+λ
21
∥Σ∥)≤λΣ−1∥Uin−U∗∥∥△U∥.
We can then get
∥△U∥ ≤2λΣ−1∥Uin−U∗∥
(β+λ1
∥Σ∥)
Putting the two parts together concludes the proof of the theorem.
A.3.2. L INEAR SYSTEMS WITH NONLINEAR RESIDUALS
Before we formally state our results, we briefly introduce a tool we use in this section, the expo-
nential family. A family {Pη}of distributions forms an s-dimensional exponential family if the
distributions Pηhave densities of the form:
P(U;η) = exp"sX
i=1ηiTi(U)−Z(η)#
h(U),
where ηi∈R(i= 1, . . . , s ) are the parameters and Z(η)∈Ris a function that maps the parameters
to a real number, and the Ti(x)’s are known as the sufficient statistics. The term Z(η)is known as the
log-normalization constant or the log-partition function, which is meant to make the distribution’s
integral equal to 1, i.e.
Z(η) = log"Z
exp"sX
i=1ηiTi(U)#
h(U)dU#
.
Taking the derivatives of Zwith respect to ηwe obtain that,
∂Z(η)
∂ηi=R
XTi(U) exp [Ps
i=1ηiTi(U)]h(U)dUR
Xexp [Ps
i=1ηiTi(U)]h(U)dU
=E[Ti(U)].
Exponential family is a big class of distributions which has great expressive ability. Next, we
first give a demonstration of exponential family by showing that quadratic total cost leads to a
distribution in the exponential family. Then, we use exponential family to generalize beyond the
quadratic case in Theorem 5, which is the main result of this section.
22COVARIANCE -OPTIMAL MPC
Example 2 (Exponential Family for Quadratic Case) Consider the quadratic case where J(U) =
U⊤DU+d⊤U= (U−U∗)⊤D(U−U∗)+J∗. The normal distribution and the exponential inverse
distribution of the total cost P(U)∝ N(U|Uin,Σ),P(U)∝exp(−J(U)
λ)can both be expressed
in the form of an exponential family distribution. To see this, consider an exponential family:
P(U;η1, η2) =h(U) exp( η⊤
1U+U⊤η2U−Z(η1, η2)),
where η1∈RmH, η2∈RmH×mHare the natural parameters of the family. Z(η1, η2)is the log-
partition of the exponential family, ensuring the integral of the distribution equals 1. The sufficient
statistics are set as T1(U) =UandT2(U) =UU⊤. Here T2(U)∈RmH×mHand it’s straight for-
ward that U⊤η2U=PmH
i=1PmH
j=1[η2]ij[UU⊤]ij, where [ ]ijdenote the i’th row and j’th column’s
element of the matrix. Note that
P(U)∝ N(U|Uin,Σ)∝exp(−1
2(U−Uin)⊤Σ−1(U−Uin))∝exp(U⊤
inΣ−1U+U⊤(−1
2Σ−1)U).
We can then tell that N(Uin,Σ)follows a distribution from this family with η1= Σ−1Uin, η2=
−1
2Σ−1.
Likely, the distribution P(U)∝ N (U|Uin,Σ) exp( −J(U)
λ)is also part of this exponential
family, because:
P(U)∝ N(U|Uin,Σ) exp( −J(U)
λ)∝exp(U⊤
inΣ−1U+U⊤(−1
2Σ−1)U) exp(−1
λ(U⊤DU+U⊤d))
∝exp(( U⊤
inΣ−1−1
λd⊤)U+U⊤(−1
2Σ−1−1
λD)U)
∝exp(( U⊤
inΣ−1−2
λU∗⊤D)U+U⊤(−1
2Σ−1−1
λD)U)
where the last ∝comes from the variant of J(U)thatJ(U) =U⊤DU+d⊤U= (U−U∗)⊤D(U−
U∗), and the second equivalent comes from a constant level shift of Jdoes not affect the result.
Therefore, P(U)∝ N(U|Uin,Σ) exp( −J(U)
λ)also follows a distribution in the exponential family
with parameters η1= Σ−1Uin−1
λd, η2=−1
2Σ−1−1
λDorη1= Σ−1Uin−2
λDU∗, η2=−1
2Σ−1−
1
λD.
Recall that, according to (12), we can tell that Uout− →
pEPU, where P(U)∝ N(U|Uin,Σ) exp( −J(U)
λ).
Leveraging the characteristics of the exponential family, we have EPU=EP(T1(U)) =∂Z(η)
∂η1.
With the help of the exponential family, we demonstrate that even with bounded residual non-
linear dynamics (in which case J(U)is no longer quadratic), contraction is still guaranteed to some
extent. Specifically, we consider the same cost function as in (1) with the following dynamics:
xt+1=Axt+B[U]t+g(xt,[U]t), where grepresents a small residual nonlinear dynamics.
Theorem 5 Suppose the log-partition function’s derivative∂Z(η)
∂η1isL1-lipschitz continuous, the
optimal control under the residual dynamic U∗′and under the original dynamic U∗are both
bounded by a constant that ∥U∗∥ ≤ cu∗andU∗′≤cu∗, and the residual dynamics gsatis-
fies some mild assumption that ∥g(x, u)∥ ≤c0,∂g
∂x≤cx1≤min(A
H,(∥A∥ −1)∥A∥),∂g
∂U≤
23YIPANHEQUSHI
cu1,∂2g
∂U∂x≤cxu,∂2g
∂x2≤cx2. We further assume that the optimal trajectory x∗
iis also bounded
that∥x∗
i∥ ≤cx∗. Then,
Uout−U∗− →
p(I−2
λ(Σ−1+2
λD)−1D⊤)(Uin−U∗) +Uerror, (16)
where ∥Uerror∥ ∼L1
λ∥Q∥(O(A3H)C3+O(A2H)C2), with C2= (1 + 2 cu∗)4∥B∥∥A∥
(cu1+Hcx1∥AB∥+cu1
∥A∥−cx1H)+2( cu1+Hcx1∥AB∥+cu1
∥A∥−cx1H)2+2cx∗∥A∥(∥AB∥+cu1
∥A∥−cx1H)2(cx2
(∥A∥−1)∥A∥−cx1),and
C3= (1 + 2 cu∗)c0(∥AB∥+cu1
∥A∥−cx1H)2(cx2
(∥A∥−1)∥A∥−cx1)
Proof In the proof, we focus on 1-dimensional system that A, B, Q ∈Rare scalars where xt,[U]t∈
Rare also scalars. This is for the purpose of simplifying notation can can easily generalize to the
multi-dimensional case. Here we denote Jwr(U)as the total cost with the residual dynamics, and
xresis the state with the residual dynamics. In other words,
Jwr(U) =HX
h=1xres⊤
hQxres
h+ [U]⊤
hR[U]h
with dynamics xres
h+1=Axres
h+B[U]h+g(xres
h,[U]h),1≤h≤H.
The counterpart to this is the nominal trajectory total cost, which we now denote as Jn(U), and xn
respectively.
Jn(U) =HX
h=1xnom⊤
hQxnom
h+ [U]⊤
hR[U]h
with dynamics xnom
h+1=Axnom
h+B[U]h,1≤h≤H
For the simplicity of further derivation, we define δxh=xres
h−xnom
h. Therefore,
Jwr(U) =HX
h=1(xnom
h+δxh)⊤Q(xnom
h+δxh) + [U]⊤
hR[U]h.
We also have Jn(U) = ( U−U∗)⊤D(U−U∗). Further, we denote the optimal point of Jwras
U∗′. Then, we can split JwrintoJwr(U) = ( U−U∗′)⊤D′(U−U∗′) +Jres(U), where D′=
∂2Jwr
∂U2|U=U∗′,Jres(U) =Jwr(U)−(U−U∗′)⊤D′(U−U∗′)is a higher order residual cost with
∂Jres
∂U|U=U∗′= 0,∂2Jres
∂U2|U=U∗′= 0.
The remaining of the proof is divided into two parts. In the first part, we show that Uoutcon-
verges to the expectation of an exponential family distribution. We further show that by decom-
posing the total cost, the difference in the expectation can be bounded by the difference of Hessian
brought by the residual dynamics. For the second part, we show that for small enough residuals
with bounded derivatives, the difference of the Hessian matrix can be bounded.
Part 1: Bound the Expectation by Lipschitz of Log-Partition. In this proof, we first augment the
exponential family with an additional sufficient statistics T3(x) =Jres(x). The exponential family
can now be denoted as:
P(U;η1, η2, η3) =h(x) exp( η⊤
1U+U⊤η2U+η3Jres(U)−Z(η1, η2, η3)).
24COVARIANCE -OPTIMAL MPC
We first consider a distribution Pfrom the exponential family that η1= Σ−1Uin−2
λDU∗′, η2=
−1
2Σ−1−1
λD, η 3= 0. From the derivation in Theorem 1, the expectation of EP(U)on this
distribution is
EP(U) = (I−2
λ(Σ−1+2
λD)−1D⊤)(Uin−U∗′).
However, Uoutunder the residual dynamics converge to the expectation of the distribution
Pwr(U)∝exp(−1
λJwr(U))N(U|Uin,Σ),
which is a distribution in the exponential family with slightly different natural parameters η′
1=
Σ−1Uin−2
λD′U∗′, η′
2=−1
2Σ−1−1
λD′, η′
3=−1
λ.
With the Lipschitz constant of∂Z(η)
∂η1, we can bound the difference of EP(U)andEPwr(U)as
follows
∥EP(U)−EPwr(U)∥ ≤L1(η1−η′
1+η2−η′
2+η3−η′
3)
=L1(2
λU∗′⊤(D−D′)+1
λD−D′)
≤L1(1
λ(2U∗′⊤+ 1)D−D′) (17)
≤L1(1
λ(2cu∗+ 1)D−D′). (18)
From above, one can see the remaining task is to bound ∥D−D′∥, which we do now.
Part 2: Bounding the difference of Hessian brought about by the residual dynamics. The
trajectory after adding the residual can be calculated as
xres
t=Atxres
0+t−1X
i=0At−iBUi+t−1X
i=0At−i−1g(xres
i,[U]i).
Meanwhile, the nominal trajectory can be calculated as
xnom
t=Atxnom
0+tX
i=0At−iBUi.
The first order derivative of the trajectory with respect to Ucan be denoted as
∂xres
t
∂[U]k=At−kB+At−k−1∂g
∂[U]k+t−1X
i=k+1At−i−1(∂g
∂xres
i∂xres
i
∂[U]k). (19)
We next prove by induction that∂xres
i
∂[U]j≤α1Ai−jholds for some α1to be determined later.
Assuming this holds for a pair of i, j, and substituting it into (19), we can find that
∂xres
t
∂[U]k=At−kB+At−k−1∂g
∂[U]k+t−1X
i=k+1At−i−1(∂g
∂xres
i∂xres
i
∂[U]k)
25YIPANHEQUSHI
≤At−kB+At−k−1∂g
∂[U]k+t−1X
i=k+1At−i−1(∂g
∂xres
i∂xres
i
∂[U]k)
≤At−kB+cu1At−k−1+cx1α1HAt−k−1.
The Last inequality comes from the induction assumption and the truth that t−k−1≤H.
For small residual dynamics g, as aforementioned, we assume cx1≤1
H∥A∥. Therefore, when
α1≥∥AB∥+cu1
∥A∥−cx1H, we have∂xres
i
∂[U]j≤α1Ai−jholds for every 0≤i, j≤H. Similarly
∂δxt
∂[U]k=At−k−1cu1+t−1X
i=k+1At−i−1(cx1α1Ai−k)≤(cu1+ (t−k−1)cx1α1)At−k−1.
Then∂δxt
∂[U]kis bounded by∂δxt
∂[U]k≤αδ1At−k−1, where αδ1=cu1+Hcx1α1. Likely, the
second order derivative can be calculated:
∂2xres
t
∂[U]k[U]l=At−k−1∂2g
∂[U]k∂xk∂xres
k
∂[U]l+t−1X
i=l+1At−i−1(∂2g
∂x2
i∂xres
i
∂[U]k∂xres
i
∂[U]l+∂g
∂xi∂2xres
i
∂[U]k∂[U]l).
Without loss of generality, here we assume l≤k. Because the residual dynamic is bounded∂2g
∂[U]k∂xk≤cxu,∂2g
∂x2
i≤cx2and this leads to,
∂2xres
t
∂[U]k[U]l≤α1cxuAt−l−1+t−1X
i=l+1cx2α2
1At+i−k−l−1+cx1∂2xres
i
∂[U]k∂[U]lAt−i−1
=cxuα1At−l−1+cx2α2
1A2t−k−l−1
(∥A∥ −1)+t−1X
i=l+1cx1At−i−1∂2xres
i
∂[U]k∂[U]l
(20)
Again, we bound∂2xres
t
∂[U]k[U]lby induction method. Assuming∂2xres
t
∂[U]k[U]l≤α2A2t−k−l, by the
assumption and (20), we show that
∂2xres
t
∂[U]k[U]l≤α1cxuAt−l−1+cx2α2
1A2t−k−l−1
(∥A∥ −1)+cx1α2A2t−k−l−1
(∥A∥ −1).
With the assumption on cx1≤(∥A∥ −1)∥A∥from property of residual dynamic, we further get∂2xres
t
∂[U]k[U]l≤α2A2t−k−l−1holds by induction when
α2≥cx2α2
1
(∥A∥ −1)∥A∥ −cx1= (∥AB∥+cu1
∥A∥ −cx1H)2 cx2
(∥A∥ −1)∥A∥ −cx1.
Notice that∂2xnom
t
∂[U]k[U]l= 0, we can conclude that∂2δxt
∂[U]k[U]l=∂2xres
t
∂[U]k[U]l≤α2A2t−k−l−1. With
the above bound on∂2xt
∂[U]k[U]l, and∂xt
∂[U]k, we now formulate the total cost J’s derivative∂2J
∂[U]k[U]l.
The first-order derivative can be calculated as
26COVARIANCE -OPTIMAL MPC
∂J
∂[U]k= 2R[U]k+HX
i=k+12x⊤
iQ∂xi
∂[U]k.
And Assuming l≥k, the second order derivative of the total cost Jcan be formulated as:
∂2J
∂[U]k[U]l=HX
i=l+12∂xi
∂[U]k⊤
Q∂xi
∂[U]l+ 2x⊤
iQ∂2xi
∂[U]k∂[U]l.
Here we define the cost under the residual dynamics as Jwsand the cost under the nominal trajectory
asJnAnd the difference between∂2Jws
∂[U]2
kand∂2Jn
∂[U]2
kcan be calculated:
∂2Jws
∂[U]k∂[U]l−∂2Jn
∂[U]k∂[U]l=HX
i=k+12∂xi
∂[U]k⊤
Q∂δxi
∂[U]l+ 2∂xi
∂[U]l⊤
Q∂δxi
∂[U]k+ 2∂δxi
∂[U]k⊤
Q∂δxi
∂[U]l
+ 2x⊤
iQ∂2(δxi)
∂[U]k∂[U]l+ 2δx⊤
iQ(∂2(δxi)
∂[U]k∂[U]l).
With a slight abuse of notation, we want to stress that xdenote the trajectory under the residual
dynamic g,δx=x−xnomdenote its difference to the nominal trajectory xnom, Substituting∂x
∂[U],
∂δx
∂[U],∂2δx
∂[U]2with their bound, we show that
∂2Jws
∂[U]k∂[U]l−∂2Jn
∂[U]k∂[U]l
≤HX
i=k+12∂xi
∂[U]k⊤
Q∂δxi
∂[U]l+2∂xi
∂[U]l⊤
Q∂δxi
∂[U]k+2∂δxi
∂[U]k⊤
Q∂δxi
∂[U]l
+2x⊤
iQ∂2(δxi)
∂[U]k∂[U]l+2δx⊤
iQ∂2(δxi)
∂[U]k∂[U]l
≤HX
i=k+1(αδ1Ai−kB∥Q∥Ai−l−1+ 2αδ1Ai−lB∥Q∥Ai−k−1+ 2α2
δ1Ai−k−1Ak−l−1∥Q∥
+ 2cx∗α2∥Q∥A2i−k−l−1+i−1X
j=0Ai−j−1g(xres
i,[U]i)∥Q∥α2A2i−k−l−1
≤HX
i=k+1(A2i−k−l−2(4αδ1∥Q∥∥B∥∥A∥+ 2∥Q∥α2
δ1+ 2cx∗∥Q∥α2∥A∥) +c0∥Q∥α2i−1X
j=0Ai−j)
≤A2H−k−l−Ak−l
A2−1(4∥B∥∥Q∥∥A∥αδ1+ 2∥Q∥α2
δ1+ 2α2cx∗∥Q∥∥A∥)
+c0α2∥Q∥A3H−k−l+2−A2k−l+2
A3−1.
27YIPANHEQUSHI
Where the trajectory is bounded by ∥xt∥ ≤cx∗and residual is bounded by ∥g(x, u)∥ ≤c0. There-
fore, with the beginning assumption that the system is 1-d, we can get that
HX
k=1HX
l=1∂2Jws
∂[U]k∂[U]l−∂2Jn
∂[U]k∂[U]l≤O(A2H)(4∥B∥∥Q∥∥A∥αδ1+ 2∥Q∥α2
δ1+ 2∥Q∥∥A∥cx∗α2)
+O(A3H)c0∥Q∥α2.
Substituting αδ1, α2into the right hand side. We can get : RHS = O(A2H)(4∥B∥∥Q∥∥A∥(cu1+
Hcx1α1)+2∥Q∥(cu1+Hcx1α1)2+2cx∗∥Q∥∥A∥(∥AB∥+cu1
∥A∥−cx1H)2 cx2
(∥A∥−1)∥A∥−cx1+O(A3H)c0∥Q∥
(∥AB∥+cu1
∥A∥−cx1H)2 cx2
(∥A∥−1)∥A∥−cx1.
Remark 9 Consider the scenario of a constant residual. In this case, the Lipschitz constants for
the derivatives of the residual dynamics gare all equal to zero. This implies that Uerror is also zero,
further indicating that a constant residual does not introduce any error in convergence.
A.3.3. T HEGENERAL CASE
A simple but straightforward following corollary would be when we have direct access to the cost
function’s structure, i.e. knowing both of DandD′or the difference Jwr−Jn. Also, Notice that the
Lipschitz constant L1is task-specific because different tasks will lead to sufficient statistics Jwr−Jn
different in the exponential family.
Corrollary 6 If the log-partition function’s derivative∂Z(η)
∂η1isL1-lipschitz continuous. We denote
the cost function under the residual dynamics as Jwr(U), the cost function of the nominal dynamics
asJn=U⊤DU+d⊤U, and the optimal control under the residual as U∗′. Then
Uout−U∗− →
p(I−2
λ(Σ−1+2
λD)−1D)(Uin−U∗′) +Uerror. (21)
with∥Uerror∥ ∼O(L1
λD−∂2Jwr
∂U2|U=U∗′).
Proof From (18) in proof of Theorem 5, obviouslly, Uerror can be bounded by the difference of
Hessian, i.e.D−∂2Jwr
∂U2|U=U∗′.
Appendix B. Implementation Details
B.1. Algorithm Implementation
The annotated pseudocode for the CoVO-MPC algorithm is shown in Alg. 2. The full version of
offline approximation CoVO-MPC is shown in Alg. 3. The major difference between those two
algorithms is that the offline approximation uses the covariance matrix from the buffer instead of
calculating it online. Here we describe the PID controller used in our offline approximation:
For the Quadrotor environment, we generate the linearization point using a differential-
flatness-based nonlinear controller:
28COVARIANCE -OPTIMAL MPC
afb=−KP
p−pd
−KD
v−vd
−KIZ
p−pd
+ad−g
zfb=afb
∥afb∥,z=Re3, f Σ=a⊤
fbz
ωdes=−KRzfb×z+ψfbz, ψ fb=−Kyaw(ψ⊖ψref)
For the Cartpole environment, we use the linearization point from the simple feedback con-
troller with gain K=
0.5 0.5 5.0 5.0
.
Algorithm 2: CoVO-MPC: CoVariance-Optimal MPC
Input: H←Horizon; N←Sample number ;T←Dynamic system time limit ;
x0←Initial state ;c1:T←Cost function ;C(·)←Optimal covariance function ;
shift (·)←Shift operator to 1 step forward ;
1fort= 1 : Tdo
2 Σt←C(Dt=∇2Jt(Uin|t)) // Calculate the sampling covariance matrix
(our contribution here.)
3 Ui|t∼ N(Uin|t,Σt) // Sample Ncontrols
4 κi|t←exp(−βJt(Ui|t))PN
i=1exp(−βJt(Ui|t))// Calculate the accumulated cost and Get the
weight of each control
5 Uout|t←PN
i=1κi|tUi|t // Calculate the future control sequence
6 Executing [Uout|t]1to get xt+1 // Get the next control
7 Uin|t+1←shift (Uout|t) // Update the mean with shift operator
8end
29YIPANHEQUSHI
Algorithm 3: CoVO-MPC (offline approximation)
Input: H←Horizon; N←Sample number ;T←Dynamic system time limit ;
x0←Initial state ;c1:T←Cost function ;C(·)←Optimal covariance function ;
shift (·)←Shift operator to 1 step forward ;
// Cache the covariance matrix for all future time step t= 1 : Twith
suboptimal controller πK
1fort= 1 : Tdo
// Rollout future step with nominal controller πK
2 fort′= 1 : Hdo
3 U′
in←πK(x′
t) // Get the next control
4 Executing U′
into get x′
t+1
5 Record control [Uoff|t]t′←Uin
6 end
7 Σoff|t←C(Dt=∇2Jt(Uoff|t)) // Cache the covariance matrix
8end
9fort= 1 : Tdo
10 Σt←Σoff|t // Get covarance matrix from buffer
11 Ui|t∼ N(Uin|t,Σt) // Sample Ncontrols
12 κi|t←exp(−βJt(Ui|t))PN
i=1exp(−βJt(Ui|t))// Calculate the accumulated cost and Get the
weight of each control
13 Uout|t←PN
i=1κi|tUi|t // Calculate the future control sequence
14 Executing [Uout|t]1to get xt+1 // Get the next control
15 Uin|t+1←shift (Uout|t) // Update the mean with shift operator
16end
For all the experiments, we keep the hyperparameter the same across all the experiments. We
also keep the determinant of the covariance matrix the same between CoVO-MPC andMPPI to
make sure the sampling volume is the same. The related hyperparameters are listed in Table 3.
Parameter Value
Horizon H 32
Sampling Number N 8192
Temperature λ 0.01
Sampling Covariance Determinant α0.532
Table 3: MPC hyperparameters
B.2. Environment Details
We use the following dynamic model for the Quadrotor environment:
x=
p
v
R
ω
,u=T
m
τ
,˙x=f(x,u) (22)
30COVARIANCE -OPTIMAL MPC
f(x,u) =
v
e3g+Re3T
m+dp
Rω
J−1(τ+dτ−ω×Jω)
(23)
where p,v∈R3are the position and velocity, R∈SO(3)is the rotation matrix, ω∈R3is the
angular velocity, u∈R4is the control input, dp∈R3is the disturbance on the position (Shi et al.,
2019; O’Connell et al., 2022), dτ∈R3is the disturbance on the torque, e3is the unit vector in the
zdirection, gis the gravitational acceleration, mis the mass, and J∈R3×3is the inertia matrix.
The disturbance dpis sampled from a zero-mean Gaussian distribution with a standard deviation
denoted by σd.
For hardware validation, we implement our algorithm on the Bitcraze Crazyflie 2.1 platform (Gier-
nacki et al., 2017). Concurrently, we leverage Crazyswarm aiding communication (Preiss et al.,
2017). The state estimator acquires position data from an external OptiTrack motion capture sys-
tem, while orientation data is relayed back from the drone via radio. Regarding control mechanisms,
an on-board, lower-level PI body-rate controller τ=−Kω
P(ω−ωdes)−Kω
IR
(ω−ωdes)oper-
ates at a frequency of 500Hz. This works together with an off-board higher-level controller, sending
out desired thrust fdand body rate ωdat a frequency of 50Hz. All communications are established
using a 2.4GHz Crazyradio 2.0.
Here is an example of real-world experiment setup, where the Crazyflie is trying to tracking a
triangular trajectory.
Figure 3: The experiment setup.
Appendix C. Sampling distribution visualization
Fig. 4 shows the sampling covariance difference between two algorithms. The right most one is the
element-wise difference between CoVO-MPC andMPPI . We can see that CoVO-MPC generated a
covariance matrix with richer patterns inlcuding:
1.Patterns inside a sigle control input :CoVO-MPC has a different scale for each control input,
while MPPI has the same scale for all control inputs.
2.Patterns between control inputs :CoVO-MPC has a strong correlation between control in-
puts at different time steps, which also enables more effective sampling leveraging the dy-
namic system property.
3.Time-varying patterns :CoVO-MPC has a time-varying diagonal terms, which enable richer
sampling schedule alongside the time-varying system dynamics.
31YIPANHEQUSHI
0
3
6
9
12
15
18
21
24
27
30
33
36
39
42
45
48
51
54
57036912151821242730333639424548515457Covariance matrix of CoVO-MPC
0
3
6
9
12
15
18
21
24
27
30
33
36
39
42
45
48
51
54
57036912151821242730333639424548515457Covariance matrix of MPPI
0
3
6
9
12
15
18
21
24
27
30
33
36
39
42
45
48
51
54
57036912151821242730333639424548515457Difference of covariance matrix between CoVO-MPC and MPPI
0.2
0.1
0.00.10.2
0.2
0.1
0.00.10.2
0.06
0.04
0.02
0.000.020.040.06
Figure 4: Covariance matrix visualization at certain timestep. The most right one is the element-
wise difference between CoVO-MPC andMPPI .
32