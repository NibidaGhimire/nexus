Unraveling Instance Associations: A Closer Look for Audio-Visual Segmentation
Yuanhong Chen1*Yuyuan Liu1*Hu Wang1Fengbei Liu1
Chong Wang1Helen Frazer2Gustavo Carneiro3
1Australian Institute for Machine Learning, University of Adelaide
2St Vincent’s Hospital Melbourne
3Centre for Vision, Speech and Signal Processing, University of Surrey
Abstract
Audio-visual segmentation (AVS) is a challenging task
that involves accurately segmenting sounding objects based
on audio-visual cues. The effectiveness of audio-visual
learning critically depends on achieving accurate cross-
modal alignment between sound and visual objects. Suc-
cessful audio-visual learning requires two essential compo-
nents: 1) a challenging dataset with high-quality pixel-level
multi-class annotated images associated with audio files,
and 2) a model that can establish strong links between au-
dio information and its corresponding visual object. How-
ever, these requirements are only partially addressed by cur-
rent methods, with training sets containing biased audio-
visual data, and models that generalise poorly beyond this
biased training set. In this work, we propose a new cost-
effective strategy to build challenging and relatively unbi-
ased high-quality audio-visual segmentation benchmarks.
We also propose a new informative sample mining method
for audio-visual supervised contrastive learning to leverage
discriminative contrastive samples to enforce cross-modal
understanding. We show empirical results that demonstrate
the effectiveness of our benchmark. Furthermore, experi-
ments conducted on existing AVS datasets and on our new
benchmark show that our method achieves state-of-the-art
(SOTA) segmentation accuracy1.
1. Introduction
The human nervous system exhibits multi-modal percep-
tion [51], combining input signals from different modali-
ties to improve the detection and classification of multiple
stimuli [51]. Such functionality has been emulated by re-
cent papers [1–3, 6, 23, 38, 39] that aim to associate visual
objects with their corresponding audio sequences, in a task
known as audio-visual correspondence (A VC) [2, 3].
*First two authors contributed equally to this work.
1This work was supported by Australian Research Council through
grant FT190100525.
Bird
Ours TP A VI Image
Male Helic. NoiseFigure 1. Current A VS datasets [63] tend to assume specific ob-
jects as consistent sound sources. Such a bias influences A VS
methods, like TPA VI [63] (2nd row), to favour segmenting the pre-
sumed sound source, even when replacing the original audio with
different sound types such as a person speaking (2nd column), bird
chirping (3rd column), or background noise (4th row). Our paper
proposes a new cost-effective strategy to build a relatively unbi-
ased audio-visual segmentation benchmark and a supervised con-
trastive learning method that mines informative samples to better
constrain the learning of audio-visual embeddings (last row).
A particularly interesting A VC task is the audio-visual
segmentation (A VS) [63, 64] that aims to segment all pix-
els of the sounding visual objects using a fully supervised
model. A major challenge in A VS is achieving cross-modal
alignment between sound and visual objects [50]. Current
datasets poorly establish and evaluate this alignment, lead-
ing to undesired system behaviour and less effective eval-
uation. For instance, the dataset in [63] shows a “com-
monsense” bias because it assumes that certain objects are
always the sound source in some scenarios . Fig. 1 shows
an example of a scene from [63] with a bias toward the
segmentation of the helicopter, even though other sound
sources (e.g., person’s speech or bird’s singing) are plau-
1arXiv:2304.02970v7  [cs.CV]  14 Aug 2024sible. Such biases can reduce cross-modal alignment un-
derstanding. Another challenge in A VS datasets [63] is the
localisation of the sound-producing object when multiple
instances of the same object class are present . While visual
cues (e.g., motion or visual semantics) can help, some ac-
tions with silent audio can introduce false positives [40].
Spatial audio can also be helpful [47, 58], as shown in
Fig. 3, where we are more likely to segment the dog on
the right if the spatial audio suggests that the sounding ob-
ject is located on the right-hand side of the image. Re-
grettably, addressing the A VS dataset challenges mentioned
above by collecting new datasets can be exorbitantly ex-
pensive. Therefore, we consider alternative data collection
procedures to mitigate the problems described above and to
effectively enable the training and evaluation of A VS meth-
ods that can better generalise to diverse A VS conditions.
Many A VL [6, 39] and A VS [35] methods rely on audio-
visual contrastive learning (A V-CL). A V-CL bears some
similarities with metric learning, particularly with respect to
the selection of informative samples (also called hard sam-
pling) for improving training efficiency and model perfor-
mance [44, 48, 60]. In A VC tasks, such a selection of infor-
mative training samples is not well-studied, but it is critical
because the more representative visual data can overpower
the weaker audio modality [45], resulting in false detections
that are relatively independent of the audio-visual input, as
shown in Fig. 1 for TPA VI (columns 2 to 4). Also, current
A V-CL methods [35, 38] consist of unsupervised learning
approaches that treat each audio-visual data pair as an in-
dependent contrastive class. However, such instance-based
CL is unable to effectively mine informative samples that
can mitigate the false detections mentioned above.
In this paper, we introduce a new cost-effective A VS
data collection procedure for training and evaluating A VS
methods that aim to mitigate the aforementioned problems
of A VS datasets [63], and a new A VS method developed
to address the shortcomings of A V-CL approaches. Our
new A VS dataset collection and annotation, called Visual
Post-production (VPO), consists of matching images from
COCO [28] and audio files from VGGSound [5] based on
the semantic classes of the visual objects of the images.
The proposed VPO dataset has three settings: 1) the sin-
gle sound-source (VPO-SS), which contains multiple vi-
sual objects, but just a single sounding object; 2) multi-
ple sound-source (VPO-MS), which contains multiple vi-
sual objects with multiple sounding objects from different
classes; and 3) multiple sound-source multi-instance (VPO-
MSMI), which has multiple sets of visual objects from the
same or different classes with multiple sounding objects. In
these three settings, stereo sound is used to disambiguate
visual objects. We also propose a new A VS method, named
contrastive audio-visual pairing (CA VP), with a supervised
contrastive learning approach that leverages audio-visualpairings to mine informative contrastive samples. To sum-
marise, our main contributions are
• A new cost-effective strategy to build A VS datasets,
named Visual Post-production (VPO), which aims to re-
duce the biases observed in current datasets [63] by pair-
ing images [28] and audio files [5] based on the visual
classes of the image objects. Three new VPO benchmarks
are built using this strategy: the single sound source
(VPO-SS: single sounding object per image), multiple
sound sources (VPO-MS: multiple sounding objects per
image from separate classes), and multiple sound sources
multi-instance (VPO-MSMI: multiple sets of sounding
objects from the same class).
• A new supervised audio-visual contrastive learning
method that mines informative contrastive pairs from ar-
bitrary audio-visual pairings to better constrain the learn-
ing of audio-visual embeddings.
• A thorough evaluation of SOTA A VS methods on A VS-
Bench and VPO datasets. The methods are also assessed
on A VS salient and semantically labelled objects with the
resized image and traditional full-resolution setups.
We first show the effectiveness of our VPO strategy by
modifying the A VSBench dataset [63] with the matching
of A VSBench images with new VGGSound [5] audio files
from the same classes. We then train TPA VI [63] on
the original and modified A VSBench datasets and show
that both datasets lead to the equivalent performance of
TPA VI on the testing set of A VSBench. We also con-
ducted experiments to test the segmentation accuracy of
our proposed A VS method on A VSBench-Objects [63],
A VSBench-Semantics [64], VPO-SS, VPO-MS and VPO-
MSMI, and results display a consistent improvement of our
method compared to the SOTA.
2. Related Works
Audio-visual Localisation (A VL) is a binary classification
task for detecting sounding visual objects in videos, using
image sequences and audio signals. It employs unsuper-
vised A VL training with ImageNet pre-trained backbone
models [13]. Prior research focused on creating joint audio-
visual representations, with feature concatenation [2] or at-
tention modules [6, 38, 39]. However, neglecting the contri-
bution from audio can be a concern when audio and visual
representations are not properly constrained [50]. This is-
sue is addressed with contrastive learning [8, 9, 20], which
emphasizes discriminative audio-visual feature learning for
each instance [6, 23, 38, 39, 49, 53]. However, Senocak et
al. [50] argue that A VL with instance discrimination may
hinder genuine cross-modal semantic understanding [41].
Hence, they propose leveraging a richer positive set, ob-
tained through strong augmentation or nearest neighbours,
to facilitate cross-modal alignment. Despite these advance-
ments, the lack of pixel-level annotation with semantic la-
2bels still hinders the accurate detection of visual objects.
Audio-visual Segmentation (A VS) addresses the limita-
tions observed in A VL by providing pixel-level binary an-
notations. Zhou et al. [63] introduced the A VSBench-
Object and A VSBench-Semantic benchmarks [64], which
encompass single-source and multi-source A VS tasks for
salient/multi-class object segmentation. The manual anno-
tation of A VS datasets is costly and hence limits dataset di-
versity. Our VPO aims to mitigate this issue with off-the-
shelf semantic segmentation datasets [28] combined with
audio data from YouTube [5] to build datasets that facilitate
model training and cross-modal alignment evaluations.
A recently published paper, developed in parallel with
ours, proposes the A VS-Synthetic [32] benchmark aimed
to reduce annotation costs by matching audio and visual
categories to create a synthetic dataset. While our VPO
and A VS-Synthetic share a similar concept of dataset cre-
ation, the data selection and partition process in A VS-
Synthetic [32] has several problems, such as: 1) most an-
notated images consist of trivial cases containing a single
sounding object, 2) it cannot assess models under differ-
ent scenarios like single-source and multi-source settings,
as proposed in A VSBench [63], 3) the use of binary labels
limits semantic understanding, and 4) it shows ambiguous
cases where multiple instances (MI) of the same class are
associated with single audio. Our dataset collection ad-
dresses these four points, and in particular, it provides a
cost-effective method to create spatial audio based on an
object’s relative position within the scene.
Like A VL, A VS methods [16, 24, 26, 29–32] use au-
dio for segmentation queries. For example, some methods
adopt MaskFormer [10] to perform image segmentation us-
ing audio queries and cross-attention layers. These meth-
ods benefit from the attention mechanism’s ability to cap-
ture long-range dependencies and segment images, enhanc-
ing spatial-temporal reasoning [26] and task-related fea-
tures [16, 26, 29, 30, 36, 63]. Also, certain methods [35, 36]
have investigated the utilization of conditional generative
models [22, 52], alongside contrastive learning, to create
discriminative latent spaces. However, the reliance on bi-
nary annotations and image resizing limits their application
and segmentation accuracy.
Contrastive learning has shown promise in A VL meth-
ods [6, 23, 38, 39]. These methods bring together aug-
mented representations from the same instance as posi-
tives while separating representation pairs from different
instances as negatives within a batch. The issue with
current A VL contrastive learning is its reliance on self-
supervision [8] to connect audio and visual representations
of the same class. In our work, we propose a new super-
vised contrastive learning [25, 27, 56] that mines informa-
tive contrastive pairs from arbitrary audio-visual pairings to
constrain the learning of audio-visual embeddings.3. Visual Post-production (VPO) Benchmark
Our VPO benchmark includes three evaluation scenarios: 1)
the single-source (VPO-SS) shown in Fig. 2 (2nd frame), 2)
the multi-source (VPO-MS) displayed in Fig. 2 (3rd frame),
and 3) the multi-source multi-instance (VPO-MSMI) dis-
played in Fig. 2 (4th frame). VPO is built by combining
images and semantic segmentation masks from COCO [28]
with audio files for the 21 COCO classes, including hu-
mans, animals, vehicles, sports, and electronics, sourced
from VGGSound [5, 63]. The audio files were obtained
from YouTube videos under the Creative Commons license,
each trimmed to 10 seconds, as verified by [5]. We then ran-
domly matched the COCO semantic segmentation masks
with related audio files based on instance labels to create the
VPO dataset. Below, we describe the three VPO settings:
VPO-SS, VPO-MS, and VPO-MSMI. For dataset statistics,
examples and a detailed description of the collection pro-
cess please refer to Supplementary Material .
TheVPO-SS comprises 12,202 samples (11,312 train-
ing and 890 testing samples), where a sample consists of
an image, a pixel-level semantic segmentation mask of a
single-sounding object, and an audio file of the sounding
object class. During image collection, our process priori-
tized the inclusion of sounding visual classes from multiple
categories within the same image, even when only one class
matched the audio file. This strategy aims to reduce the
“commonsense” bias in A VS datasets and avoid the domi-
nance of a single visual object in the segmentation process,
minimizing incidental correlations. This is done by ensur-
ing images containing visual objects from various COCO
classes are given higher priority than single-class images.
By employing this collection protocol, we aim to produce a
benchmark that has a diverse set of sounding visual objects
because we can match images with many different types of
audio files that represent visual objects, resulting in numer-
ous combinations, as demonstrated in Fig. 2 (2nd frame).
TheVPO-MS comprises 9,817 images, with 8,380 im-
ages for training and 1,437 for testing. Each image can
include up to five sounding objects from the 21 COCO
classes, where each visual object is accompanied by its
pixel-level semantic segmentation mask and corresponding
audio file. In total, the dataset contains 13,496 semantic
segmentation masks. Following the same VPO-SS strategy,
we prioritise the collection of images that have sounding
objects from multiple classes but exclude images that con-
tain multiple instances of the same class. Additionally, to
prevent methods that detect all visual objects from an im-
age from performing well in our benchmark, we randomly
remove the sound of some visual objects from images con-
taining more than two objects. We merge audio files from
multiple sounding visual objects into a single file using ad-
dition operations performed on the waveform data [23, 59].
Fig. 2 (3rd frame) shows VPO-MS examples.
3Visual Post-production (SS)
 Visual Post- production  (MSMI)
50
Visual Post- production (MS)
 AVSBench (SS)Figure 2. VPO Benchmarks. Using four classes, including “ female ”, “cat”, “dog”, and “ car”, the A VSBench (SS) (1st frame) provides
pixel-level multi-class annotations to the images containing a single sounding object. The proposed VPO benchmarks (2nd frame to 4th
frame) pair a subset of the segmented objects in an image with relevant audio files to produce pixel-level multi-class annotations.
Based on VPO-MS, we additionally searched 3,038 im-
ages, each containing multiple instances that share a com-
mon semantic class. The resulting VPO-MSMI contains
12,855 images, with 11,080 images for training and 1,775
for testing. The VPO-MSMI is a challenging A VS task,
represented by the accurate localisation of sound sources
amid multiple sources of sound with the assistance of spa-
tial cues. The main cue used to allow the segmentation of
multiple instances in VPO-MSMI is the use of spatial au-
dio to localize the sound source in an image. We simulate
spatial audio with stereo sound by leveraging the object’s
spatial location information to modulate the volume of the
left or right audio channel. Assuming we have an RGB im-
age with resolution H×Wand a particular object centred
at(ch, cw), then the relative position of that object w.r.t the
width of the image is denoted by αi=cw
W. For instance, we
show an example of processing the audio files from a human
and dog in Fig. 3, utilizing position coefficients αM, αD
for volume control and deriving cwfrom ground-truth mask
center of mass. Based on this modelling method, we can
work with an arbitrary number of sound sources.
4. Method
The multi-class audio-visual dataset is denoted as D=
{(ai,xi,yi,ti)}|D|
i=1, where xi∈ X ⊂ RH×W×3is an RGB
image with resolution H×W,a∈ A ⊂ RT×Fdenotes
the Mel Spectrogram audio representation with time Tand
FMel filter banks, yi∈ Y ⊂ { 0,1}H×W×Cdenotes the
pixel-level ground truth for the Cclasses (the background
class is included in these Cclasses), and ti∈ Y ⊂ { 0,1}|C|
+
+
Figure 3. Synthesising stereo sound for the VPO-MSMI setting.
is a multi-label ground truth audio annotation.
4.1. Preliminaries about Cross-Attention
Our goal is to learn the parameters θ∈Θfor the
model fθ:X × A → [0,1]H×W×C, which com-
prises the image and audio backbones that extract features
withua=fγ(a)anduv=fϕ(x), respectively, where
γ, ϕ∈θ, and ua,uv∈ U , with Udenoting a unified
feature space. Our approach is similar to other early fu-
sion methods [16, 42, 63] that combine audio and video
features with a multi-head attention block [55] which es-
timates the co-occurrence of audio and visual data with
fMHA (Q,K,V) = softmax
QKT
√
D
V,where√
Dde-
notes the scaling factor [55], and Q,K,Vrepresent the
query, key and value inputs. Previous A VS methods [16, 63]
usually use audio as the cross-attention (CA) query Qto
produce ˆuv=uv⊕uv⊙fMHA (ua,uv,uv), where ⊕is the
element-wise addition operator, and ⊙is the element-wise
4BG
BG
BG
BG
BG
BG
BG
BG
Anchor
Positive
Negative
 BG
Positive Feature
(       ,       )
  (                 ,                 )
 (                 ,                 )
 (       ,       )
 (                 ,                 )
 (       ,       )
BG
(       ,       )
  (                 ,                 )
 (                 ,                 )
 (       ,       )
BG
 (                 ,                 )
 (       ,       )
Negative Feature
,
 ,(       ,       )
,
 ,
PullRepel
AnchorFigure 4. Illustration of our CA VP method for the “Dog” anchor. Starting with the audio-visual foreground anchor set Efg
dog, we create the
positive and negative audio-visual features denoted by PfgandNfg=Nfg
hard∪ Nfg
easyrespectively defined in Eq. (3). The CA VP loss in
Eq. (4) pulls the anchor and positive audio-visual features closer while repelling the anchor and negative audio-visual features.
multiplication operator. However, we empirically observe
that the use of audio as query leads to a situation where
the visual feature dominates the CA process, suppressing
the audio representation. We address this issue by using
the visual feature as the query and removing the ⊙opera-
tion to produce ˆuv=fCA(uv,ua), where fCA(uv,ua) =
uv⊕fMHA (uv,ua,ua). Another important point about
fMHA (.)is that we replace softmax( .)bysigmoid( .)to
enable the highlighting of multiple regions of varying sizes
in the attention map that is related to the audio. Please refer
to the Supplementary Material for the visual results.
4.2. Contrastive Audio-Visual Pairing (CA VP)
The ultimate goal of our contrastive learning is to discrim-
inate the positive fusion of feature representations from
the same semantic class and the negative fusion of feature
representations from different semantic classes. Previous
works [25, 34] suggest that the discrimination between sam-
ples is critical for contrastive learning, so simply drawing
random negative samples from original audio-visual pairs
could limit the representation learning effectiveness. A
practical way to create an informative contrastive dataset
is to mine all possible combinations of audio-visual pairs in
the latent space. However, it is not possible to create some
of the negative pairs because we do not have access to all
instance segmentation masks for each image (e.g., we can-
not create fCA(ucat
v,udog
a)in Fig. 4 because we do not have
pixels labelled as “cat”). Motivated by our VPO procedure
in Sec. 3, we observe that this barrier can be overcome by
randomly paring audio and visual features within the batch,
as shown in Fig. 4, offering us the potential to mine most of
the conceivable audio-visual combinations.
CA VP starts by dividing the original audio-visual dataset
Dinto a visual dataset Dv={(xi,yi)}|D|
i=1and audio
dataset Da={(ak,tk)}k=perm({1,...,|D|}), where jis a per-
mutation of the index set of D. We define the randomlypaired audio-visual feature dataset as:
Z=n
(z,t,y(ω))|z=f(ω)
CA(fϕ(x), fγ(a)),(a,t)∈ Da,(x,y)∈ Dvo
,
(1)
where ω∈Ωis the lattice of size H×W, andf(ω)
CA(.)is the
cross-attention output at lattice position ω. To simplify the
notation, we represent v(ω) = (z,t,y(ω))and define the
audio-visual anchor sets as:
Efg=
v(ω)|v(ω)∈ Z,(t=y(ω))∧(t̸=bg)∧(y(ω)̸=bg)	
,
Eunknow=
v(ω)|v(ω)∈ Z,(tj̸=yj(ω))∧(yj(ω) =bg)	
,
Ebg=Z \(Efg∪ Eunknow),
(2)
where ∧and∨are respectively the “AND” and “OR” logic
operators, and “bg” is the background class label. The fore-
ground anchor set Efgcontains samples from Zin Eq. (1)
that have the same audio and visual labels, and both are dif-
ferent from the background class; the Eunknowrepresents the
anchors with uncertain semantic meaning due to the lack
of instance segmentation masks for all the available targets,
and the background anchor set Ebgare all the samples in Z
that are not in Efg∪ Eunknow.
The mining of informative contrastive samples ex-
plores all combinations of audio-visual features after cross-
attention fusion to form positive sets that will enable the
enhancement of the similarity between semantically related
samples while diminishing the similarity of samples with
different semantic concepts. For foreground anchors in
Efg, its contrastive positive set Pfgand negative set Nfg=
Nfg
hardSNfg
easyare defined by (see Fig. 4):
Pfg(v(ω)) =
zj|vj(φ)∈ Z,((tj=t)∧(yj(φ) =y(ω)))	
,
Nfg
hard(v(ω)) =
zj|vj(φ)∈ Z,((tj=t)∧(yj(φ)̸=y(ω)))
∨((tj̸=t)∧(yj(φ) =y(ω)))	
,
Nfg
easy(v(ω)) =
zj|vj(φ)∈ Z,((tj̸=t)∧(yj(φ)̸=y(ω)))	
,
(3)
5Table 1. Quantitative ( J,F) audio-visual segmentation results (in %) on A VSBench dataset [63, 64] (resized to 224 ×224) with
ResNet50 [19] backbone. Best results in bold , second best underlined . Improvements against the second best are in the brackets.
ImageNet Pretrained Backbone
D-ResNet50 [19] MethodA VSBench-Object (SS) A VSBench-Object (MS) A VSBench-Semantics
J&F ↑ J ↑ F ↑ J&F ↑ J ↑ F ↑ J&F ↑ J ↑ F ↑
TransformerCATR [26] 80.70 74.80 86.60 59.05 52.80 65.30 - - -
AuTR [31] 80.10 75.00 85.20 55.30 49.40 61.20 - - -
A VSegFormer [16] 80.67 76.54 84.80 56.17 49.53 62.80 27.12 24.93 29.30
A VSC [29] 81.13 77.02 85.24 55.55 49.58 61.51 - - -
BA VS [30] 81.63 77.96 85.29 56.30 50.23 62.37 27.16 24.68 29.63
Per-pixel
ClassificationTPA VI [63] 78.80 72.79 84.80 52.84 47.88 57.80 22.69 20.18 25.20
A VSBG [18] 79.77 74.13 85.40 50.88 44.95 56.80 - - -
ECMV AE [36] 81.42 76.33 86.50 54.70 48.69 60.70 - - -
DiffusionA VS [35] 81.35 75.80 86.90 55.94 49.77 62.10 - - -
Ours 83.84 78.78 88.89 61.48 55.82 67.14 32.83 30.37 35.29
COCO Pretrained Backbone
Transformer AQFormer 81.70 77.00 86.40 61.30 55.70 66.90 - - -
Per-pixel Cls. Ours 83.75 78.72 88.77 62.34 56.42 68.25 - - -
where φ∈Ω. For background cases in Ebg, the con-
trastive positive set is Pbg=Ebg, while the negative set
isNbg=Efg. Let us represent the set of anchors by
E=EfgSEbg, the set of positives with Pthat is equal to
Pfgif the anchor is from Efg, or equal to Pbgif the anchor
is from Ebg(and similarly for Nw.r.t.NfgorNbg). Adopt-
ing the supervised InfoNCE [25] as the objective function
to pull the anchor v(ω)∈ E and respective positive audio-
visual features closer while repelling anchors and their neg-
ative audio-visual features, we define the following loss:
ℓCP(v(ω)) =1
|P(v(ω))|X
zp∈P(v(ω))
−logexp (z·zp/τ)
exp (z·zp/τ) +P
zn∈N(v(ω))exp (z·zn/τ),
(4)
where zis the anchor feature from v(ω), and τis the tem-
perature hyper-parameter.
The overall training loss function is defined as:
ℓ(D, θ) =1
|D||Ω||D|X
i=1X
φ∈Ω
ℓCE(yi(φ),ˆ yi(φ))
+1
|E|X
v(ω)∈E
ℓCP(v(ω))
,
(5)
where ℓCE(.)is the cross-entropy loss, ˆ y=fθ(x,a)
is the model (parameterised by θ) prediction, with ˆy∈
[0,1]H×W×C,Ωis the image lattice, and ℓCP(.)is our con-
trastive loss, calculated based on the anchor sets Efrom
Eq.(2), as specified in Eq. (4).
5. Experiments
5.1. Implementation Details
Evaluation protocols: We first adopt the widely used eval-
uation protocols for the A VSBench-Object [63] (includingsingle-source (SS) and multi-source (MS) with binary la-
bels) and A VSBench-Semantics dataset (with multi-class
labels) [64] by resizing all images to 224 ×224 for fair
comparison. Note that the use of binary labels and image
resizing limits the application scope as well as the model
performance. Hence, we follow traditional segmentation
benchmarks [12, 15, 28, 43, 62] and use original image
resolution for training and testing. Also following previ-
ous A VS methods [63, 64], we calculate mean intersection
over union (mIoU) [15] to quantify the average segmenta-
tion quality and use Fβscore with β2= 0.3[37, 63] to
measure precision and recall performance and false detec-
tion rate (FDR) to highlight the false positive classification
in a pixel-wise manner. For training and inference details,
please refer to Supplementary Material .
5.2. Results
Performance on resized A VSBench. We divide the per-
formance comparison on A VSBench into the A VSBench-
Objects (SS & MS) [63] and A VSBench-Semantics [64].
We compare the performance of SOTA methods with our
CA VP in Tab. 1 using Jaccrd index ( J) and F-Score ( F)
based on [63], which shows that our model surpasses
the second-best methods w.r.t. mIoU by of 2.21% on
A VSBench-Object (SS) [63], 2.43% on A VSBench-Object
(MS) [63] and 5.68% on A VSBench-Semantics [64] using
ResNet50 [19] backbone. We separate the AQFormer [24]
in the bottom of Tab. 1 as their backbone model utilizes
pre-trained weights based on COCO [28] instead of Ima-
geNet [13]. Additionally, we report model performance un-
der conventional semantic segmentation evaluation metrics,
same as Pascal VOC [15] and CityScape [12] as this method
do not skip the no sounding frames during evaluation.
Performance on original resolution A VSBench. We intro-
6Table 2. Quantitative (mIoU, Fβ)audio-visual segmentation results (in %) on A VSBench dataset [63, 64] (resized to 224 ×224) with
ResNet50 [19] backbone. * indicate the our initial results.
D-ResNet50 [19] MethodA VSBench-Object (SS) A VSBench-Object (MS) A VSBench-Semantics
mIoU↑ Fβ↑ mIoU↑ Fβ↑ mIoU↑ Fβ↑
Per-Pixel
ClassificationCA VP* 85.77 92.86 62.39 73.62 44.70 57.76
CA VP 89.43 94.50 72.79 83.05 44.70 57.76
Table 3. Quantitative (mIoU, Fβ, FDR) audio-visual segmentation results (in %) on A VSBench-Semantic dataset [64] (original resolution)
with ResNet50 [19] backbone. Best results in bold , second best underlined . Improvements against the second best are in the last row.
D-ResNet50 [19] MethodA VSBench-Semantics (SS) A VSBench-Semantics (MS) A VSBench-Semantics
mIoU↑Fβ↑ FDR↓ mIoU↑Fβ↑ FDR↓ mIoU↑Fβ↑ FDR↓
Transformer A VSegFormer [16] 68.95 82.64 15.45 33.23 45.63 43.16 41.48 56.21 38.77
Per-pixel ClassificationTPA VI [63] 64.30 81.06 14.81 36.29 50.36 40.61 43.39 59.24 34.66
Ours 73.08 85.57 12.64 46.40 60.25 32.11 50.75 64.57 32.25
Improvement Ours +4.13 +2.93 -2.17 +10.11 +9.89 -8.50 +7.36 +5.33 -2.41
Table 4. Quantitative (mIoU, Fβ, FDR) audio-visual segmentation results (in %) on VPO dataset (original resolution) with ResNet50 [19]
backbone. Best results in bold , second best underlined . Improvements against the second best are in the last row.
D-ResNet50 [19] MethodVPO (SS) VPO (MS) VPO (MSMI)
mIoU↑Fβ↑ FDR↓mIoU↑Fβ↑ FDR↓mIoU↑Fβ↑ FDR↓
Transformer A VSegFormer [16] 57.55 73.03 19.76 58.33 74.28 22.13 54.22 70.39 25.51
Per-pixel ClassificationTPA VI [63] 52.75 69.54 22.83 54.30 71.95 22.45 51.73 68.85 26.75
Ours 62.31 78.46 13.56 64.31 78.92 18.67 60.36 75.60 22.12
Improvements Ours +4.76 +5.43 -6.20 +5.98 +4.64 -3.46 +6.14 +5.21 -3.39
Table 5. Ablation study of the CA VP components.
MethodA VSBench-Semantics VPO (MSMI)
mIoU↑ Fβ↑ mIoU↑ Fβ↑
TPA VI [63] 42.74 58.11 52.83 70.39
fCA(·) 47.18 61.74 58.50 73.19
SupCon 48.72 61.85 59.35 74.20
CA VP 50.75 62.31 64.31 64.13
duce a new benchmark based on the A VSBench-Semantics
dataset [64] in Tab. 3 to address the absence of the orig-
inal resolution images in previous benchmarks shown in
Tab. 1. In this new benchmark, we use A VSegFormer [16]2
and TPA VI [63]3as the baseline methods4. To save train-
ing and evaluation time, we train on the entire training set
of A VSBench-Semantics and test the model on A VSBench-
Semantics (SS) and A VSBench-Semantics (MS) subsets, as
well as the entire testing set for A VSBench-Semantics to
show partitioned model performance. The results on the
entire A VSBench-Semantics in the last columns of Tables 1
and 3 show a significant mIoU improvement of +16.55%
for A VSegFromer [16], +23.21% for TPA VI, and + 6.05%
for our method when compared to the results obtained
with low image resolution under ResNet-50 backbone [19].
These results suggest the importance of using the original
resolution in the A VSBench-Semantics dataset [64]. Also
in Tab. 3, results show that our method consistently out-
2https://github.com/vvvb-github/AVSegFormer
3https://github.com/OpenNLPLab/AVSBench
4We cannot include other methods from Tab. 1 as they were not publicly
available online at the time of submission.performs the baselines by a minimum of 4.13% and2.93%
improvements in mIoU and Fβ, respectively. We show a vi-
sualisation of a 6-second video clip in Fig. 5 that displays a
qualitative comparison between TPA VI, A VSegFormer and
our CA VP. Notice how our method approximates the ground
truth segmentation of the sounding objects more consis-
tently than the other methods. Please refer to the Supple-
mentary Material for more qualitative results.
Effectiveness of Visual Post-production (VPO): To test
the effectiveness of our VPO approach to build a bench-
mark dataset, we simulate VPO on the initial single source
A VSBench-Object [63]. This involves the substitution of
the original audio waveform data with samples from the
same category taken from VGGSound [5] (emulating the
way we build the VPO datasets). We denote this dataset as
A VS-M-SS. We directly use the original TPA VI code [63]
and run the model training and testing three times for both
the modified and the original datasets. On A VSBench-
Object (SS), TPA VI achieves mIoU= 72.01±0.7%and
Fβ= 83 .36±1.2%(mean ±standard deviation), and
on A VS-M-SS, TPA VI achieves mIoU= 72.07±0.7%and
Fβ= 83 .70±1.0%. The p-values (two-sided t-test) of
mIoU and Fβare0.66and0.73, respectively, which means
that we fail to reject the null hypnosis that the performance
on the VPO dataset is significantly different from the orig-
inal one. This experiment shows the validity of building
A VS datasets using pairs of audio-visual data obtained by
matching images and audio based on the semantic classes
7GT
AVSegFormerTPAVI
CAVPFigure 5. Qualitative audio-visual segmentation results on A VSBench-Semantics [64] by TPA VI [63], A VSegFormer [16], and our CA VP,
which can be compared with the ground truth (GT) of the first row.
(mIoU)
50.7548.3748.2947.75
SigmoidMin-maxChannel-AttnSoftMax
Figure 6. mIoU results for Activation functions used by fCA(.)on
A VSBench-Semantic [64] with D-ResNet50 [19] backbone.
Table 6. Training with different proportions of positive and nega-
tive samples in (4). We use D-ResNet50 [19] and DeepLabV3+ [7]
as the backbone and the segmentation architecture, respectively.
Proportion ( %) A VSBench-Semantics [64]
Pos Sample Neg Sample mIoU↑Fβ↑ FDR↓
10% 90% 48.88 63.03 33.13
50% 50% 50.75 64.57 32.25
90% 10% 49.19 61.82 34.50
of the visual objects of the images.
Performance on VPO: We show model performance on
our VPO benchmarks in Tab. 4 with ResNet50 [19] image
backbone. Our method outperforms the baseline methods
on all experimental settings by a minimum of 4.76% and
3.46% for mIoU and false detection rate, respectively.
5.3. Ablation Study
We first perform an analysis of CA VP components on
A VSBench-Semantics [64] and VPO(MSMI) in Tab. 5.
Starting from a baseline consisting of TPA VI-like cross-
attention (CA) fusion layer (1st row), we replace this TPA VI
CA layer by our CA layer, represented by fCA(.)and ob-
serve an average mIoU improvement of around +4.5% on
both datasets. Subsequently, by integrating the model with
the supervised contrastive learning loss defined in [25], we
achieve an additional mIoU improvement of around +1%,
as shown in the 3rd row. The final row displays CA VP
method with all components, including the selection of in-
formative samples, which reaches a further average mIoU
improvement of +3.4%.
Cross-attention. From Sec. 4.1, recall that fCA(.)can havedifferent activation functions, and we opted for the sigmoid
activation to enable the highlighting of multiple regions of
varying sizes in the attention map. We now study the use
of different activation functions, namely: 1) softmax [55],
2) channel-wise attention adopted in A VSegFormer [16], 3)
min-max normalisation to produce attention map, and the
4) sigmoid function. Results in Fig. 6 show that the setting
with the “sigmoid” improves the second-best method, “min-
max”, by +2.38% mIoU.
Sampling Analysis. Naturally, the amount of negative sam-
ples is overwhelmingly larger than the number of positive
samples in the anchor sets of Eq. (2), so we need to balance
these two sets to enable more effective training. Please refer
to the Supplementary Material for details on the balancing
process. We conduct an ablation study of the model’s per-
formance under three different settings containing different
proportions of positive and negative samples, as shown in
Tab. 6. The results reveal that adopting a balanced positive
and negative sampling strategy benefits the model’s perfor-
mance, with improvements in mIoU of 1.87% and1.56%
compared to scenarios where either 90% of the samples are
negative or 90% are positive.
Mono and Stereo Sound. To emphasize the significance
of stereo sound in the A VS task, we conducted an ablation
study on the VPO dataset, toggling the stereo sound on and
off, with results shown in Fig. 7. Notice that for all VPO
subsets, the stereo sound can improve the performance on
all evaluation measures5. This improvement is particularly
noticeable in the MSMI split as it significantly enhances the
model’s performance across all three measures. For the nu-
merical results, please refer to the Supplementary Material .
5Note that PPV = 1-FDR in the graph.
8mIoU
F-Beta PPV65707580VPO-SS
mIoU
F-Beta PPV65707580VPO-MS
mIoU
F-Beta PPV6065707580VPO-MSMIFigure 7. Mono (blue) and Stereo (red) audio VPO performance.
6. Discussion and Conclusion
In this work, we have presented new cost-effective VPO
benchmarks and the innovative CA VP method for Audio-
Visual Segmentation (A VS). Our proposed VPO bench-
marks are both scalable, cost-effective and challenging,
while our data collection and annotation protocols provide
a substantial reduction of the “common-sense” bias found
A VS tasks, where certain objects are always the source of
sound in some scenarios. We also introduce a new super-
vised audio-visual contrastive learning that utilises arbitrary
audio-visual pairs to mine informative contrastive pairs that
can better constrain the learning of audio-visual features.
Overall, our dataset and method can provide a valuable re-
source for future research in A VS tasks.
Limitations and future work. We recognize that our VPO
dataset lacks temporal information and may exhibit a class
imbalance issue similar to that observed in A VSBench-
Semantics [64]. This imbalance results from our strict filter-
ing of trivial cases with training images containing a single
dominating-sounding object. Furthermore, our current ap-
proach does not support simulating spatial audio based on
objects’ visual depth or modelling arrival time differences
in the VPO. We intend to tackle these issues in our future
work to enhance A VS models’ robustness and applicability.
A. Further Details for VPO Dataset
A.1. Dataset Statistics
We show the distribution of visual classes in VPO-SS,
VPO-MS and VPO-MSMI in Figure 8. Similar to the
A VSBench-Semantics [64], we also observe a data imbal-
ance issue within our VPO dataset. We follow [54] to re-
port an imbalance ratio (Nmax
Nmin) of 12.48% (female & zebra),
12.43% (female & zebra) and 12.62% (female & cow) on
the three VPO subsets, and 59.57% (man & axe & missile-
rocket) on A VSBench-Semantics [64]. These class imbal-
ance issues can affect the model performance during testing,
which will be discussed in Sec. C.2. For the demonstration
of training examples, please refer to the “ video demo.mp4 ”
file within the supplementary materials.
A.2. Creation Procedure
We show a graphical illustration of our Visual Post-
production (VPO) benchmark in Fig. 9. We divide the entire
dataset generation process into three major steps:Table 7. Detailed lookup table correlating audio tags with visual
labels in the VPO creation process.
Visual-label Audio-label
bird mynah bird singing
keyboard typing on computer keyboard
bus driving buses
catcat purring / cat purring
cat meowing / cat caterwaulin
dogdog growling / dog bow-wow
dog whimpering / dog howling
dog barking / dog baying
horse horse neighing / horse clip-clop
carcar passing by / car engine idling
car engine starting
race car, auto racing
car engine knocking
sports ball shot football
airplane airplane / airplane flyby
sheep sheep bleating
cow cow lowing
motorcycle driving motorcycle
mouse mouse clicking
cell phone cell phone buzzing
elephant elephant trumpeting
zebra zebra braying
tennis racket playing tennis
skateboard skateboarding
malemale speech, man speaking
male singing
female female speech, woman speaking
babybaby babbling / baby crying
baby laughter
•Data collection : We gather datasets from off-the-shelf
segmentation datasets (e.g., COCO [28]) and audio
datasets (e.g., VGGSound [5]), focusing on the overlap-
ping classes listed in Tab. 7. We randomly match audio
and video files to form new samples based on their se-
mantic labels.
•Data processing : We prioritise the collection of images
with multiple objects and incorporate spatial location in-
formation based on each selected instance mask.
•Subset creation : We organize subsets according to
their keywords (e.g., single-source, multi-sources, multi-
instances) and further partition each subset into training
and testing sets.
A.3. Strengths and Weaknesses of VPO
As we discussed in the main paper, our VPO dataset enjoys
the following strengths :
•Cost-effectiveness . The manually labelled A VS dataset
needs annotators to watch and listen to an entire video so
they can provide labels. The VPO production process can
9(a) VPO-SS
 (b) VPO-MS
 (c) VPO-MSMI
Figure 8. Visual class distribution in our proposed VPO-SS, VPO-MS and VPO-MSMI benchmark datasets.
Extract Class
intersectionsSegmentation
DatasetAudio
Dataset
VPO
RAWVPO
(SS)
VPO
(MS)
Data Collection
Stereo info
GenerationCompute Positional
information for each
instance maskNo
Multiple
source
YesMultiple
InstancePrioritize the
collection of
images containing
multiple objects. No
VPO
(MSMI)randomly remove the
sound of some visual
objectsYesNo
N > 2
N: Number of
sound sourcesYes
Subset Creation
 Data Processing
Collect dataframe with selected
classes based on the lookup table
Avoid easy cases and add spatial
location information based on the
instance segmentation mask Merge
Figure 9. Illustration of VPO collection and production process.
significantly reduce such annotation costs.
•Data scalability . We can easily increase the amount of
data by leveraging existing visual (ADE20K [61], Pascal
VOC [14], etc.) and audio (AudioSet [17], ESC-50 [46],
etc.) datasets.
•More diverse scenarios. In our proposed VPO datasets,
every object within the scene will have a chance to
be the sounding object, which is crucial to mitigate
the “commonsense” bias that is observed in A VSBench-
Semantics [64] and addressing the assessment of spurious
correlation [4, 11, 33].
•Isolation of motion information .The disentanglement of
motion and sound in our VPO benchmark prevents the
model from solely relying on motion information to make
predictions, encouraging the learning and evaluation of
cross-modal alignment [50].
•Introduction of stereo audio .The use of stereo audio en-
courages the study of spatial prompts.We also identified the following weaknesses of VPO:
•Data imbalance issue .Such imbalance can affect the seg-
mentation accuracy, particularly for the tail classes.
•Lack of temporal image data .Since we match still im-
ages with audio, we cannot use image motion informa-
tion.
•Comprehensive simulations of spatial audio .Our VPO
does not include the modelling of arrival time differences
and microphone distance.
B. Experiments
B.1. Implementation Details
Training & Inference: During training, we apply data
augmentation for image inputs with colour jitter, horizon-
tal flipping and random scaling between 0.5 to 2.0. We
randomly crop images to 512×512 pixels. For the au-
dio data, we extract the log Mel-Spectrogram with 64 mel
10Ground
T ruth
Attention
MapFigure 10. Qualitative results for cross-attention heatmap on A VSBench-Semantics [64].
(a) Ground Truth
 (b) Softmax
 (c) Min-max
 (d) Sigmoid
Figure 11. Visualisation of attention map process by Softmax and Sigmoid activation.
(a) Without CA VP
 (b) With CA VP
Figure 12. T-SNE visualisation of features after cross-attention fusion layer, trained with or without our CA VP.
filter banks over 1s [63] or 3s [38] of the waveform at
16 kHz on A VSBench and VPO. We set temperature τas
0.1. We use the SGD optimizer with a momentum of 0.9,
weight decay of 0.0005, and a polynomial learning-rate de-
cay(1−iter
total iter)powerwith power = 0.9. The initial learning
rate is set to 0.001, the mini-batch size is 16 and training
lasts for 80 epochs. During inference, we use the origi-
nal resolution resizing and cropping with a mini-batch size
of 1. We adopt two image backbones (ResNet [19], PVT-
V2-B5 [57]) and DeepLabV3+ [7] for the segmentation
network. For the audio backbones, we use VGGish [21]
(following [63]) and ResNet-18 [19] (following [6, 38])
for A VSBench and VPO, respectively. The overwhelming
amount of negative samples is mitigated by maintaining a
memory bank [20] to store raw waveform data for each
class. During training, we proportionally transfer negative
pairs from the negative set to the positive set by pairing pos-
itive audio with respect to the image label.C. Attention Map Visualisation
To demonstrate the effectiveness of our cross-attention
module, we visualize the audio-visual attention
heatmap. We employ models pre-trained on A VSBench-
Semantics [64], utilizing a full-resolution set-up and
equipped with a ResNet50 [19] backbone. As shown
in Fig. 10, the heatmaps illustrate that our module can
effectively retrieve the foreground object by leveraging
the interaction between audio and visual embeddings.
Additionally, we show a visual comparison amount of the
application of softmax( .),Minmax( .)andsigmoid( .)
activation functions. As depicted in Fig. 11, using Softmax
under spatial dimension may lead to a diminished attention
map (Fig. 11b), while incorporating Minmax activation
over the dot product of the audio-visual feature could yield
a noisy and limited discernment of relevant audio-visual
correspondences. (Fig. 11c). Our method in Fig. 11d,
utilizing sigmoid( .), demonstrates a better efficacy com-
pared to these two methods in terms of cross-modal feature
activation.
11Table 8. Quantitative (mIoU, Fβ, FDR) audio-visual segmentation results (in %) on VPO dataset with ResNet50 [19] backbone and mono
audio. Best results are in bold , and second best areunderlined . Improvements against the second best are in the last row.
D-ResNet50 [19] MethodVPO (SS) VPO (MS) VPO (MSMI)
mIoU↑Fβ↑ FDR↓mIoU↑Fβ↑ FDR↓mIoU↑Fβ↑ FDR↓
Transformer A VSegFormer [16] 52.96 67.89 25.50 56.46 71.89 24.66 50.96 64.96 32.72
Per-pixel ClassificationTPA VI [63] 51.84 68.77 23.64 44.08 58.14 30.82 50.37 66.80 29.82
Ours 61.48 75.53 18.79 61.85 74.60 20.24 57.22 72.26 24.04
Improvements Ours +8.52 +6.76 -4.85 +5.39 +2.71 -4.42 +6.26 +5.46 -5.78
C.1. T-SNE Visualisation
To present the qualitative results of our CA VP method
in the latent space, we extract the features computed be-
fore the classification layers, and generate T-SNE plots in
Fig. 12. We employ models pre-trained on A VSBench-
Semantics [64], utilizing a full-resolution set-up and
equipped with a ResNet50 [19] backbone. We consider
both scenarios: with the proposed CA VP method (Fig. 12b)
and without it (Fig. 12a). The results demonstrate that our
method can enhance intra-class compactness while preserv-
ing intra-class separability.
C.2. Additional Results
We present supplementary results for the paper, showcas-
ing the performance of A VSegformer [16], TPA VI [63], and
our model on VPO with mono audio with ResNet50 [19]
backbone, as depicted in Tab. 8. Our method outper-
forms the baseline methods on all experimental settings
by a minimum of 5.39%, 2.71% and 4.42% for mIoU, Fβ
score and false detection rate, respectively. We also pro-
videclass-wise results on A VSBench-Semantics [64] in Ta-
bles 9,10,11. We observe that the tail classes, such as clip-
pers, axe, missile-rocket and utv, show significantly worse
results than the remaining classes, illustrating the impor-
tance of addressing the imbalance issue in the A VS task.
 Mono CAVP
 Stereo CAVP Image
 GT
Figure 13. Visual comparison between CA VP models trained with
mono audio and stereo audio.
To showcase the model’s performance with both mono
and stereo audio inputs, we opted for two CA VP models
utilizing the ResNet50 [19] backbone. Trained on VPO-
MSMI, these models were chosen to assess prediction out-comes in multi-instance scenarios, illustrated in Fig. 13.
Our observations reveal that, when aided by stereo audio,
the model effectively diminishes its focus on the incorrect
spatial direction. However, challenges persist in handling
images with multiple instances, exemplified in the last col-
umn (depicting two dogs) of Fig. 13. This underscores a sig-
nificant challenge in the audio-visual segmentation (A VS)
task.
Image
 TPAVI (VPO)
Bird
 Male Helic. Noise
TPAVI
Figure 14. Visual comparison between TPA VI models trained on
A VSBench [63] (2nd row) and VPO (3rd row), using an identical
set of synthetic test samples. The columns display the original
audio (1st column) and alternative sound types, including a person
speaking (2nd column), bird chirping (3rd column), or background
noise (4th column).
To further demonstrate the effectiveness of our VPO
dataset, we used the TPA VI [63] trained on VPO and ap-
plied it to test the synthetic examples, shown in Fig. 14.
Despite hallucinations persisting in the 3rd column (class
“Bird”), there is a noticeable improvement, with the correct
segment of “Helicopter”, “Male”, and “Noise”, compared
to the results in the 2nd row.
Finally, we show a qualitative comparison visualization
among TPA VI [63], A VSegFormer [16], and our CA VP on
VPO in Fig. 15 and on A VSBench-Semantics in Fig. 16.
We demonstrate that our method consistently provides a
more effective approximation of the true segmentation of
objects in the scene compared to alternative methods. For
the demonstration of full video examples on A VSBench-
12Semantics, please refer to the “ video demo.mp4 ” file
within the attached supplementary materials.
References
[1] Triantafyllos Afouras, Andrew Owens, Joon Son Chung, and
Andrew Zisserman. Self-supervised learning of audio-visual
objects from video. In Computer Vision–ECCV 2020: 16th
European Conference, Glasgow, UK, August 23–28, 2020,
Proceedings, Part XVIII 16 , pages 208–224. Springer, 2020.
1
[2] Relja Arandjelovic and Andrew Zisserman. Look, listen and
learn. In Proceedings of the IEEE international conference
on computer vision , pages 609–617, 2017. 1, 2
[3] Relja Arandjelovic and Andrew Zisserman. Objects that
sound. In Proceedings of the European conference on com-
puter vision (ECCV) , pages 435–451, 2018. 1
[4] Martin Arjovsky, L ´eon Bottou, Ishaan Gulrajani, and David
Lopez-Paz. Invariant risk minimization. arXiv preprint
arXiv:1907.02893 , 2019. 10
[5] Honglie Chen, Weidi Xie, Andrea Vedaldi, and Andrew
Zisserman. Vggsound: A large-scale audio-visual dataset.
InICASSP 2020-2020 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP) , pages
721–725. IEEE, 2020. 2, 3, 7, 9
[6] Honglie Chen, Weidi Xie, Triantafyllos Afouras, Arsha Na-
grani, Andrea Vedaldi, and Andrew Zisserman. Localizing
visual sounds the hard way. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 16867–16876, 2021. 1, 2, 3, 11
[7] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian
Schroff, and Hartwig Adam. Encoder-decoder with atrous
separable convolution for semantic image segmentation. In
Proceedings of the European conference on computer vision
(ECCV) , pages 801–818, 2018. 8, 11
[8] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-
offrey Hinton. A simple framework for contrastive learning
of visual representations. In International conference on ma-
chine learning , pages 1597–1607. PMLR, 2020. 2, 3
[9] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He.
Improved baselines with momentum contrastive learning.
arXiv preprint arXiv:2003.04297 , 2020. 2
[10] Bowen Cheng, Alex Schwing, and Alexander Kirillov. Per-
pixel classification is not all you need for semantic segmen-
tation. Advances in Neural Information Processing Systems ,
34:17864–17875, 2021. 3
[11] V olkan Cirik, Louis-Philippe Morency, and Taylor Berg-
Kirkpatrick. Visual referring expression recognition: What
do systems actually learn? arXiv preprint arXiv:1805.11818 ,
2018. 10
[12] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo
Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe
Franke, Stefan Roth, and Bernt Schiele. The cityscapes
dataset for semantic urban scene understanding. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition , pages 3213–3223, 2016. 6
[13] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical imagedatabase. In 2009 IEEE conference on computer vision and
pattern recognition , pages 248–255. Ieee, 2009. 2, 6
[14] Mark Everingham, Luc Van Gool, Christopher KI Williams,
John Winn, and Andrew Zisserman. The pascal visual object
classes (voc) challenge. International journal of computer
vision , 88:303–338, 2010. 10
[15] Mark Everingham, SM Ali Eslami, Luc Van Gool, Christo-
pher KI Williams, John Winn, and Andrew Zisserman. The
pascal visual object classes challenge: A retrospective. In-
ternational journal of computer vision , 111:98–136, 2015.
6
[16] Shengyi Gao, Zhe Chen, Guo Chen, Wenhai Wang, and Tong
Lu. Avsegformer: Audio-visual segmentation with trans-
former. arXiv preprint arXiv:2307.01146 , 2023. 3, 4, 6,
7, 8, 12, 14, 15
[17] Jort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren
Jansen, Wade Lawrence, R Channing Moore, Manoj Plakal,
and Marvin Ritter. Audio set: An ontology and human-
labeled dataset for audio events. In 2017 IEEE interna-
tional conference on acoustics, speech and signal processing
(ICASSP) , pages 776–780. IEEE, 2017. 10
[18] Dawei Hao, Yuxin Mao, Bowen He, Xiaodong Han, Yuchao
Dai, and Yiran Zhong. Improving audio-visual seg-
mentation with bidirectional generation. arXiv preprint
arXiv:2308.08288 , 2023. 6
[19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition , pages 770–778, 2016. 6, 7, 8, 11, 12, 16
[20] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross
Girshick. Momentum contrast for unsupervised visual rep-
resentation learning. In Proceedings of the IEEE/CVF con-
ference on computer vision and pattern recognition , pages
9729–9738, 2020. 2, 11
[21] Shawn Hershey, Sourish Chaudhuri, Daniel PW Ellis, Jort F
Gemmeke, Aren Jansen, R Channing Moore, Manoj Plakal,
Devin Platt, Rif A Saurous, Bryan Seybold, et al. Cnn archi-
tectures for large-scale audio classification. In 2017 ieee in-
ternational conference on acoustics, speech and signal pro-
cessing (icassp) , pages 131–135. IEEE, 2017. 11
[22] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-
fusion probabilistic models. Advances in neural information
processing systems , 33:6840–6851, 2020. 3
[23] Xixi Hu, Ziyang Chen, and Andrew Owens. Mix and local-
ize: Localizing sound sources in mixtures. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 10483–10492, 2022. 1, 2, 3
[24] Shaofei Huang, Han Li, Yuqing Wang, Hongji Zhu, Jiao Dai,
Jizhong Han, Wenge Rong, and Si Liu. Discovering sound-
ing objects by audio queries for audio visual segmentation.
arXiv preprint arXiv:2309.09501 , 2023. 3, 6
[25] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna,
Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and
Dilip Krishnan. Supervised contrastive learning. Advances
in neural information processing systems , 33:18661–18673,
2020. 3, 5, 6, 8
13 TPAVI
 AVSegFormer
 CAVP GTImage
 TPAVI
 AVSegFormer
 CAVP GTImage
 TPAVI
 AVSegFormer
 CAVP GTImage
 TPAVI
 AVSegFormer
 CAVP GTImage
 TPAVI
 AVSegFormer
 CAVP GTImage
 TPAVI
 AVSegFormer
 CAVP GTImageFigure 15. Qualitative audio-visual segmentation results on VPO by TPA VI [63], A VSegFormer [16], and our CA VP. The prediction results
can be compared with the ground truth (GT) of the first row of each sample.
14 GT
 TPAVI
 AVSegFormer
 CAVP
 GT
 TPAVI
 AVSegFormer
 CAVP
 TPAVI
 AVSegFormer
 CAVP GT
 TPAVI
 AVSegFormer
 CAVP GT
 TPAVI
 AVSegFormer
 CAVP GT
 TPAVI
 AVSegFormer
 CAVP GT
 TPAVI
 AVSegFormer
 CAVP GT
 TPAVI
 AVSegFormer
 CAVP GT
 GT
 TPAVI
 AVSegFormer
 CAVP GT
 TPAVI
 AVSegFormer
 CAVP
 GT
 TPAVI
 AVSegFormer
 CAVPFigure 16. Qualitative audio-visual segmentation results on A VSBench-Semantics [64] by TPA VI [63], A VSegFormer [16], and our CA VP.
The prediction results can be compared with the ground truth (GT) of the first row of each video.
15Table 9. Class-level (mIoU) audio-visual segmentation results (in %) on A VSBench-Semantic dataset [64] (original resolution) with
ResNet50 [19] backbone.
Class background accordion airplane axe baby bassoon bell bird boat boy bus car cat cello clarinet
A VSegformer 0.9093 0.8610 0.9003 0.0000 0.4178 0.1683 0.0618 0.2251 0.7751 0.0130 0.4240 0.2800 0.3952 0.4553 0.0033
TPA VI 0.9111 0.7484 0.9261 0.0000 0.4147 0.1578 0.0795 0.3814 0.7149 0.0251 0.7019 0.4214 0.5209 0.4537 0.1624
CA VP 0.9168 0.9229 0.9291 0.0000 0.4212 0.2580 0.3376 0.2481 0.8096 0.1194 0.3486 0.3786 0.4968 0.5490 0.0180
Class clipper clock dog donkey drum duck elephant emergency-car erhu flute frying-food girl goose guitar
A VSegformer 0.0000 0.5482 0.2847 0.3876 0.0288 0.5581 0.8649 0.5377 0.2396 0.2287 0.5187 0.1163 0.2171 0.7520
TPA VI 0.0000 0.4179 0.2329 0.1913 0.3180 0.4620 0.7492 0.5342 0.3697 0.3134 0.4161 0.1737 0.0054 0.7203
CA VP 0.0000 0.7088 0.3259 0.4141 0.2987 0.5154 0.7597 0.6329 0.5898 0.3358 0.4761 0.1691 0.6841 0.8172
Class gun guzheng hair-dryer handpan harmonica harp helicopter hen horse keyboard leopard lion man marimba
A VSegformer 0.2456 0.4480 0.4926 0.8611 0.0000 0.6583 0.6952 0.1423 0.1709 0.7823 0.6721 0.7946 0.4050 0.8194
TPA VI 0.2355 0.4999 0.6422 0.7801 0.0000 0.5573 0.7185 0.2600 0.3104 0.7592 0.6018 0.7331 0.4229 0.8223
CA VP 0.3059 0.5587 0.6075 0.8885 0.0000 0.6473 0.7762 0.7078 0.2506 0.7800 0.6797 0.8328 0.4300 0.8392
Class missile-rocket motorcycle mower parrot piano pig pipa saw saxophone sheep sitar sorna squirrel tabla
A VSegformer 0.0000 0.0501 0.6662 0.1493 0.6031 0.2891 0.5213 0.2953 0.3404 0.0837 0.4364 0.2090 0.4190 0.6088
TPA VI 0.1086 0.0106 0.6237 0.1239 0.5406 0.2505 0.6602 0.4154 0.4560 0.0507 0.5225 0.6431 0.4567 0.6963
CA VP 0.0000 0.1063 0.6487 0.1158 0.6178 0.6247 0.6518 0.5625 0.5141 0.1069 0.6552 0.4723 0.6357 0.6383
Class tank tiger tractor train trombone truck trumpet tuba ukulele utv vacuum-cleaner violin wolf woman
A VSegformer 0.3381 0.5467 0.4995 0.8996 0.4133 0.2434 0.3514 0.7967 0.5930 0.0000 0.1720 0.4008 0.7839 0.3837
TPA VI 0.4718 0.5849 0.4586 0.8529 0.4425 0.1831 0.2631 0.7744 0.5388 0.0000 0.1558 0.5302 0.6924 0.4285
CA VP 0.6514 0.6226 0.4922 0.9427 0.6415 0.1948 0.3839 0.8249 0.7038 0.0000 0.5680 0.6005 0.8510 0.4157
Table 10. Class-level ( Fβ) audio-visual segmentation results (in %) on A VSBench-Semantic dataset [64] (original resolution) with
ResNet50 [19] backbone.
Class background accordion airplane axe baby bassoon bell bird boat boy bus car cat cello clarinet
A VSegformer 0.9484 0.9395 0.9488 0.0000 0.5795 0.4054 0.1657 0.2971 0.8914 0.0351 0.5099 0.3539 0.5359 0.6597 0.0129
TPA VI 0.9445 0.8090 0.9581 0.0000 0.6223 0.4018 0.2440 0.5241 0.8458 0.0786 0.8474 0.5883 0.7307 0.6560 0.3185
CA VP 0.9528 0.9579 0.9505 0.0000 0.6393 0.5019 0.5094 0.3887 0.8846 0.2930 0.4237 0.4606 0.6165 0.7143 0.0535
Class clipper clock dog donkey drum duck elephant emergency-car erhu flute frying-food girl goose guitar
A VSegformer 0.0000 0.7194 0.4178 0.6265 0.0956 0.7510 0.9100 0.6769 0.5244 0.3931 0.7594 0.2153 0.4183 0.8626
TPA VI 0.0000 0.6666 0.4233 0.4442 0.6056 0.6664 0.8798 0.7391 0.6092 0.5772 0.7110 0.3057 0.0105 0.8266
CA VP 0.0000 0.8454 0.5669 0.6258 0.5230 0.7024 0.8076 0.7269 0.7465 0.6553 0.7321 0.2821 0.8127 0.9053
Class gun guzheng hair-dryer handpan harmonica harp helicopter hen horse keyboard leopard lion man marimba
A VSegformer 0.4728 0.7464 0.5995 0.9431 0.0000 0.7688 0.8334 0.3946 0.2563 0.8703 0.7653 0.8616 0.5782 0.9098
TPA VI 0.5106 0.7231 0.7612 0.9119 0.0000 0.7621 0.8379 0.5141 0.4684 0.8806 0.7856 0.8201 0.6245 0.9119
CA VP 0.5433 0.7352 0.6854 0.9427 0.0000 0.7616 0.8725 0.8698 0.3430 0.8783 0.7884 0.8958 0.6217 0.9223
Class missile-rocket motorcycle mower parrot piano pig pipa saw saxophone sheep sitar sorna squirrel tabla
A VSegformer 0.0000 0.1409 0.8527 0.3047 0.7714 0.3990 0.7633 0.4944 0.5863 0.2197 0.7148 0.4113 0.6475 0.7012
TPA VI 0.2028 0.0316 0.8035 0.2810 0.7535 0.4973 0.8320 0.6863 0.7137 0.1161 0.7707 0.8317 0.6656 0.7887
CA VP 0.0000 0.2981 0.8605 0.2403 0.8134 0.7346 0.8311 0.8037 0.7085 0.2074 0.8023 0.5639 0.7770 0.7387
Class tank tiger tractor train trombone truck trumpet tuba ukulele utv vacuum-cleaner violin wolf woman
A VSegformer 0.6493 0.6707 0.6241 0.9607 0.6450 0.5623 0.6298 0.9178 0.7482 0.0000 0.2465 0.5906 0.8749 0.5682
TPA VI 0.7202 0.7762 0.5412 0.9239 0.6158 0.4848 0.4430 0.8686 0.7479 0.0000 0.2122 0.7115 0.8528 0.6369
CA VP 0.8315 0.7188 0.5650 0.9710 0.7491 0.4830 0.6486 0.8840 0.8353 0.0000 0.6977 0.7634 0.9193 0.6123
Table 11. Class-level (FDR) audio-visual segmentation results (in %) on A VSBench-Semantic dataset [64] (original resolution) with
ResNet50 [19] backbone.
Class background accordion airplane axe baby bassoon bell bird boat boy bus car cat cello clarinet
A VSegformer 0.0550 0.0479 0.0502 0.0000 0.4288 0.3772 0.7397 0.7448 0.0926 0.9487 0.5460 0.6959 0.4878 0.3081 0.9300
TPA VI 0.0631 0.2274 0.0449 0.0000 0.3432 0.3230 0.4422 0.4977 0.1435 0.8376 0.1323 0.4156 0.2250 0.3140 0.6382
CA VP 0.0505 0.0437 0.0601 0.0000 0.3147 0.3791 0.4865 0.6186 0.1240 0.5693 0.6331 0.5954 0.4191 0.2809 0.9047
Class clipper clock dog donkey drum duck elephant emergency-car erhu flute frying-food girl goose guitar
A VSegformer 1.0000 0.2708 0.6018 0.3006 0.7584 0.2165 0.1045 0.3412 0.2448 0.5871 0.1603 0.7783 0.5090 0.1339
TPA VI 1.0000 0.2492 0.5280 0.3382 0.2250 0.3010 0.0993 0.2199 0.3155 0.2966 0.1332 0.6854 0.9897 0.1824
CA VP 1.0000 0.1405 0.3474 0.3350 0.4075 0.2775 0.2348 0.3099 0.2496 0.1147 0.1721 0.7238 0.1870 0.0896
Class gun guzheng hair-dryer handpan harmonica harp helicopter hen horse keyboard leopard lion man marimba
A VSegformer 0.4300 0.0935 0.4443 0.0412 0.0000 0.2514 0.1549 0.2107 0.7680 0.1360 0.2649 0.1580 0.4202 0.0822
TPA VI 0.2801 0.2203 0.2559 0.0553 1.0000 0.1932 0.1607 0.3488 0.5360 0.1038 0.1826 0.2010 0.3472 0.0798
CA VP 0.3705 0.2483 0.3653 0.0559 0.0000 0.2581 0.1288 0.0918 0.6947 0.1201 0.2287 0.1150 0.3599 0.0692
Class missile-rocket motorcycle mower parrot piano pig pipa saw saxophone sheep sitar sorna squirrel tabla
A VSegformer 1.0000 0.7616 0.0958 0.6423 0.2116 0.6355 0.1542 0.4672 0.3244 0.6554 0.1578 0.5089 0.2943 0.3404
TPA VI 0.7909 0.9448 0.1636 0.6326 0.1957 0.3731 0.1337 0.1972 0.1895 0.8594 0.1386 0.1214 0.2973 0.2370
CA VP 1.0000 0.4343 0.0645 0.7223 0.1387 0.2925 0.1293 0.1074 0.2643 0.7786 0.1885 0.4891 0.2233 0.2929
Class tank tiger tractor train trombone truck trumpet tuba ukulele utv vacuum-cleaner violin wolf woman
A VSegformer 0.1411 0.3576 0.4080 0.0274 0.2927 0.1022 0.2312 0.0539 0.2487 1.0000 0.7833 0.3927 0.1284 0.4196
TPA VI 0.1946 0.1879 0.5165 0.0732 0.3823 0.0581 0.5315 0.1349 0.2057 1.0000 0.8205 0.2718 0.1152 0.3276
CA VP 0.1282 0.3183 0.4969 0.0285 0.2766 0.1775 0.2415 0.1326 0.1567 1.0000 0.3237 0.2250 0.0810 0.3646
[26] Kexin Li, Zongxin Yang, Lei Chen, Yi Yang, and Jun
Xun. Catr: Combinatorial-dependence audio-queried trans-
former for audio-visual video segmentation. arXiv preprint
arXiv:2309.09709 , 2023. 3, 6
[27] Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe Yang,Rogerio S Feris, Piotr Indyk, and Dina Katabi. Targeted su-
pervised contrastive learning for long-tailed recognition. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 6918–6928, 2022. 3
[28] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
16Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and C Lawrence
Zitnick. Microsoft coco: Common objects in context. In
Computer Vision–ECCV 2014: 13th European Conference,
Zurich, Switzerland, September 6-12, 2014, Proceedings,
Part V 13 , pages 740–755. Springer, 2014. 2, 3, 6, 9
[29] Chen Liu, Peike Li, Xingqun Qi, Hu Zhang, Lincheng Li,
Dadong Wang, and Xin Yu. Audio-visual segmentation by
exploring cross-modal mutual semantics, 2023. 3, 6
[30] Chen Liu, Peike Li, Hu Zhang, Lincheng Li, Zi Huang,
Dadong Wang, and Xin Yu. Bavs: Bootstrapping audio-
visual segmentation by integrating foundation knowledge.
arXiv preprint arXiv:2308.10175 , 2023. 3, 6
[31] Jinxiang Liu, Chen Ju, Chaofan Ma, Yanfeng Wang, Yu
Wang, and Ya Zhang. Audio-aware query-enhanced trans-
former for audio-visual segmentation. arXiv preprint
arXiv:2307.13236 , 2023. 6
[32] Jinxiang Liu, Yu Wang, Chen Ju, Ya Zhang, and Weidi Xie.
Annotation-free audio-visual segmentation. arXiv preprint
arXiv:2305.11019 , 2023. 3
[33] Runtao Liu, Chenxi Liu, Yutong Bai, and Alan L Yuille.
Clevr-ref+: Diagnosing visual reasoning with referring ex-
pressions. In Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition , pages 4185–4194,
2019. 10
[34] Yunze Liu, Qingnan Fan, Shanghang Zhang, Hao Dong,
Thomas Funkhouser, and Li Yi. Contrastive multimodal fu-
sion with tupleinfonce. In Proceedings of the IEEE/CVF In-
ternational Conference on Computer Vision , pages 754–763,
2021. 5
[35] Yuxin Mao, Jing Zhang, Mochu Xiang, Yunqiu Lv, Yi-
ran Zhong, and Yuchao Dai. Contrastive conditional la-
tent diffusion for audio-visual segmentation. arXiv preprint
arXiv:2307.16579 , 2023. 2, 3, 6
[36] Yuxin Mao, Jing Zhang, Mochu Xiang, Yiran Zhong, and
Yuchao Dai. Multimodal variational auto-encoder based
audio-visual segmentation. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , pages 954–
965, 2023. 3, 6
[37] David R Martin, Charless C Fowlkes, and Jitendra Ma-
lik. Learning to detect natural image boundaries using lo-
cal brightness, color, and texture cues. IEEE transactions on
pattern analysis and machine intelligence , 26(5):530–549,
2004. 6
[38] Shentong Mo and Pedro Morgado. A closer look at weakly-
supervised audio-visual source localization. arXiv preprint
arXiv:2209.09634 , 2022. 1, 2, 3, 11
[39] Shentong Mo and Pedro Morgado. Localizing visual sounds
the easy way. In Computer Vision–ECCV 2022: 17th Eu-
ropean Conference, Tel Aviv, Israel, October 23–27, 2022,
Proceedings, Part XXXVII , pages 218–234. Springer, 2022.
1, 2, 3
[40] Pedro Morgado, Ishan Misra, and Nuno Vasconcelos. Ro-
bust audio-visual instance discrimination. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 12934–12945, 2021. 2
[41] Pedro Morgado, Nuno Vasconcelos, and Ishan Misra. Audio-
visual instance discrimination with cross-modal agreement.InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 12475–12486, 2021.
2
[42] Arsha Nagrani, Shan Yang, Anurag Arnab, Aren Jansen,
Cordelia Schmid, and Chen Sun. Attention bottlenecks for
multimodal fusion. Advances in Neural Information Pro-
cessing Systems , 34:14200–14213, 2021. 4
[43] Gerhard Neuhold, Tobias Ollmann, Samuel Rota Bulo, and
Peter Kontschieder. The mapillary vistas dataset for semantic
understanding of street scenes. In Proceedings of the IEEE
international conference on computer vision , pages 4990–
4999, 2017. 6
[44] Hyun Oh Song, Yu Xiang, Stefanie Jegelka, and Silvio
Savarese. Deep metric learning via lifted structured fea-
ture embedding. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pages 4004–4012,
2016. 2
[45] Xiaokang Peng, Yake Wei, Andong Deng, Dong Wang, and
Di Hu. Balanced multimodal learning via on-the-fly gradient
modulation. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 8238–
8247, 2022. 2
[46] Karol J Piczak. Esc: Dataset for environmental sound classi-
fication. In Proceedings of the 23rd ACM international con-
ference on Multimedia , pages 1015–1018, 2015. 10
[47] Kranthi Kumar Rachavarapu, Vignesh Sundaresha, AN Ra-
jagopalan, et al. Localize to binauralize: Audio spatializa-
tion from visual sound source localization. In Proceedings
of the IEEE/CVF International Conference on Computer Vi-
sion, pages 1930–1939, 2021. 2
[48] Florian Schroff, Dmitry Kalenichenko, and James Philbin.
Facenet: A unified embedding for face recognition and clus-
tering. In Proceedings of the IEEE conference on computer
vision and pattern recognition , pages 815–823, 2015. 2
[49] Arda Senocak, Tae-Hyun Oh, Junsik Kim, Ming-Hsuan
Yang, and In So Kweon. Learning to localize sound source
in visual scenes. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition , pages 4358–
4366, 2018. 2
[50] Arda Senocak, Hyeonggon Ryu, Junsik Kim, Tae-Hyun Oh,
Hanspeter Pfister, and Joon Son Chung. Sound source local-
ization is all about cross-modal alignment. In Proceedings
of the IEEE/CVF International Conference on Computer Vi-
sion, pages 7777–7787, 2023. 1, 2, 10
[51] Dana M Small and John Prescott. Odor/taste integration and
the perception of flavor. Experimental brain research , 166:
345–357, 2005. 1
[52] Kihyuk Sohn, Honglak Lee, and Xinchen Yan. Learning
structured output representation using deep conditional gen-
erative models. Advances in neural information processing
systems , 28, 2015. 3
[53] Weixuan Sun, Jiayi Zhang, Jianyuan Wang, Zheyuan Liu,
Yiran Zhong, Tianpeng Feng, Yandong Guo, Yanhao Zhang,
and Nick Barnes. Learning audio-visual source localization
via false negative aware contrastive learning. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 6420–6429, 2023. 2
17[54] Kaihua Tang, Jianqiang Huang, and Hanwang Zhang. Long-
tailed classification by keeping the good and removing the
bad momentum causal effect. Advances in Neural Informa-
tion Processing Systems , 33:1513–1524, 2020. 9
[55] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. Advances in neural
information processing systems , 30, 2017. 4, 8
[56] Wenguan Wang, Tianfei Zhou, Fisher Yu, Jifeng Dai, En-
der Konukoglu, and Luc Van Gool. Exploring cross-image
pixel contrast for semantic segmentation. In Proceedings
of the IEEE/CVF International Conference on Computer Vi-
sion, pages 7303–7313, 2021. 3
[57] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao
Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pvt
v2: Improved baselines with pyramid vision transformer.
Computational Visual Media , 8(3):415–424, 2022. 11
[58] Xinyi Wu, Zhenyao Wu, Lili Ju, and Song Wang. Binaural
audio-visual localization. In Proceedings of the AAAI Con-
ference on Artificial Intelligence , pages 2961–2968, 2021. 2
[59] Yao-Yuan Yang, Moto Hira, Zhaoheng Ni, Artyom Asta-
furov, Caroline Chen, Christian Puhrsch, David Pollack,
Dmitriy Genzel, Donny Greenberg, Edward Z Yang, et al.
Torchaudio: Building blocks for audio and speech process-
ing. In ICASSP 2022-2022 IEEE International Conference
on Acoustics, Speech and Signal Processing (ICASSP) , pages
6982–6986. IEEE, 2022. 3
[60] Wenzhao Zheng, Zhaodong Chen, Jiwen Lu, and Jie Zhou.
Hardness-aware deep metric learning. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 72–81, 2019. 2
[61] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela
Barriuso, and Antonio Torralba. Scene parsing through
ade20k dataset. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pages 633–641,
2017. 10
[62] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fi-
dler, Adela Barriuso, and Antonio Torralba. Semantic under-
standing of scenes through the ade20k dataset. International
Journal of Computer Vision , 127:302–321, 2019. 6
[63] Jinxing Zhou, Jianyuan Wang, Jiayi Zhang, Weixuan Sun,
Jing Zhang, Stan Birchfield, Dan Guo, Lingpeng Kong,
Meng Wang, and Yiran Zhong. Audio–visual segmentation.
InComputer Vision–ECCV 2022: 17th European Confer-
ence, Tel Aviv, Israel, October 23–27, 2022, Proceedings,
Part XXXVII , pages 386–403. Springer, 2022. 1, 2, 3, 4, 6, 7,
8, 11, 12, 14, 15
[64] Jinxing Zhou, Xuyang Shen, Jianyuan Wang, Jiayi Zhang,
Weixuan Sun, Jing Zhang, Stan Birchfield, Dan Guo, Ling-
peng Kong, Meng Wang, et al. Audio-visual segmentation
with semantics. arXiv preprint arXiv:2301.13190 , 2023. 1,
2, 3, 6, 7, 8, 9, 10, 11, 12, 15, 16
18