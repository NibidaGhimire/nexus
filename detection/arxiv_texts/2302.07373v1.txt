LINEARIZED WASSERSTEIN DIMENSIONALITY REDUCTION WITH
APPROXIMATION GUARANTEES
ALEXANDER CLONINGER1,2, KEATON HAMM3, VARUN KHURANA1, AND CAROLINE MOOSMÜLLER4
ABSTRACT . We introduce LOT Wassmap, a computationally feasible algorithm to un-
cover low-dimensional structures in the Wasserstein space. The algorithm is motivated
by the observation that many datasets are naturally interpreted as probability measures
rather than points in Rn, and that ﬁnding low-dimensional descriptions of such datasets
requires manifold learning algorithms in the Wasserstein space. Most available algo-
rithms are based on computing the pairwise Wasserstein distance matrix, which can
be computationally challenging for large datasets in high dimensions. Our algorithm
leverages approximation schemes such as Sinkhorn distances and linearized optimal
transport to speed-up computations, and in particular, avoids computing a pairwise
distance matrix. We provide guarantees on the embedding quality under such approxi-
mations, including when explicit descriptions of the probability measures are not avail-
able and one must deal with ﬁnite samples instead. Experiments demonstrate that LOT
Wassmap attains correct embeddings and that the quality improves with increased sam-
ple size. We also show how LOT Wassmap signiﬁcantly reduces the computational cost
when compared to algorithms that depend on pairwise distance computations.
CONTENTS
1. Introduction 2
2. Notation and Background 5
3. LOT Wassmap algorithm and Main Theorem 11
4. Bounds for compactly supported target measures 14
5. Bounds for non-compactly supported target measures 18
6. Conditions on Hand¹(Compact case) 21
7. Conditions on Hand¹(Non-compact case) 21
8. Experiments 22
Acknowledgements 26
References 26
Appendix A. Helper Theorems and Lemmas 29
Appendix B. Plug-in estimator approximation results 32
Appendix C. Non-Compactly Supported Measures Proofs and Results 33
Appendix D. Proofs and Results for Conditions on Hand¹ 36
1DEPARTMENT OF MATHEMATICS , U NIVERSITY OF CALIFORNIA , SANDIEGO , CA
2HALICIO ˘GLU DATA SCIENCE INSTITUTE , U NIVERSITY OF CALIFORNIA , SANDIEGO , CA
3DEPARTMENT OF MATHEMATICS , U NIVERSITY OF TEXAS AT ARLINGTON , ARLINGTON , TX
4DEPARTMENT OF MATHEMATICS , U NIVERSITY OF NORTH CAROLINA AT CHAPEL HILL, NC
E-mail addresses :acloninger@ucsd.edu, keaton.hamm@uta.edu, vkhurana@ucsd.edu,
cmoosm@unc.edu .
2020 Mathematics Subject Classiﬁcation. 49Q22, 60D05, 68T10.
Key words and phrases. Optimal Transport, Dimensionality Reduction, Wasserstein Space, Multidimen-
sional Scaling, Isomap.
1arXiv:2302.07373v1  [cs.LG]  14 Feb 20232 LINEARIZED WASSERSTEIN DIMENSIONALITY REDUCTION WITH APPROXIMATION GUARANTEES
1. I NTRODUCTION
A classical problem in analyzing large volume, high-dimensional datasets is to de-
velop efﬁcient algorithms that classify points based on a similarity measure, or based
on a subset of preclassiﬁed training data points. Even when data points lie in high-
dimensional Euclidean space, they can often be approximated by low-dimensional struc-
tures, such as subspaces or submanifolds. This observation has led to signiﬁcant ad-
vances in the ﬁeld, mostly through the development of manifold learning algorithms ,
which produce a low-dimensional representation of a given dataset; see for example
[8, 15, 26, 38]. In many of these frameworks, the data points are assumed to be sam-
pled from a low-dimensional Riemannian manifold embedded in Euclidean space, and
approximately preserve intrinsic properties such as geodesic distances.
In many applications however, data points are more naturally interpreted as distri-
butions { ¹i}N
iÆ1overRn, or ﬁnite samples XiÆ{x(i)
j}Ni
jÆ1with x(i)
j»¹i. Examples include
imaging data [36], text documents (the bag-of-word model uses word count within a
text as features, creating a histogram for each document [45]), and gene expression data,
which can be interpreted as a distribution over a gene network [14, 28]. In this setting, a
Euclidean embedding space with Euclidean distances locally approximating the intrin-
sic distance of the data may not be geometrically meaningful, and datasets are better
modeled as probability measures in the Wasserstein space [39].
We assume that our data points { ¹i}N
iÆ1belong to the quadratic Wasserstein space
W2(Rn) of probability measures with ﬁnite second moment, equipped with the Wasser-
stein distance
W2(¹,º) :Æ inf
¼2¡(¹,º)µZ
R2nkx¡yk2d¼(x,y)¶1
2
, (1)
whereP(R2n) is the set of all probability measures over R2nand¡(¹,º) :Æ{°2P(R2n) :
°(A£Rn)Æ¹(A),°(Rn£A)Æº(A) for all A½Rn} is the set of all joint probability mea-
sures with marginals ¹andº. Under regularity assumptions on ¹, the optimal coupling
¼has the form ¼Æ(id,T)]¹, where T2L2(Rn,¹) is the “optimal transport map” [10, 39].
The Wasserstein space and optimal transport have gained popularity in the machine
learning community, as they are based on a solid theoretical foundation [39] (for exam-
ple, (1) is a metric), while providing a versatile framework for applications (for example,
as a cost function for generative models [6], in semi-supervised learning [37], and in
pattern detection for neuronal data [31]).
In this paper, we are interested in uncovering low-dimensional submanifolds in the
Wasserstein space in a computationally feasible manner as well as analyzing the qual-
ity of the embedding. To this end, we follow the idea of [21, 40], which introduces the
Wassmap algorithm (see Section 2.6 for more details), a version of the Multidimensional
Scaling algorithm (MDS) [27] (see Algorithm 1), or more generally, the Isomap algorithm
[38].
A central part of manifold learning algorithms like MDS or Isomap relies on the com-
putation of the pairwise Euclidean distances. Wassmap uses the pairwise Wasserstein
distance matrix instead, which leads to O(N2) Wasserstein distance computations, each
of which is of the order O(n3log( n)) if one uses interior point methods to solve the lin-
ear program (1). If both Nand nare large, computing all pairwise distances becomes
infeasible. To deal with this issue, approximations of the Wasserstein distance can be
considered. In this paper, we are interested in entropic regularized distances (SinkhornLINEARIZED WASSERSTEIN DIMENSIONALITY REDUCTION WITH APPROXIMATION GUARANTEES 3
distances) [2, 17], which deal with the computational issue involving n, and in linearized
optimal transport (LOT) [20, 40], to reduce the computational cost in N.
Our results are twofold:
(1) Approximation guarantees :
²We provide bounds on the embedding quality of the Multidimensional Scal-
ing algorithm (MDS) [27] (see Algorithm 1) applied to a dataset in the Wasser-
stein space, where the pairwise Wasserstein distances are only available up
to an error ¿.
²We study the size of ¿in common approximation schemes such as entropic
regularization and linearized approximations, and when explicit descrip-
tions of the data points ¹i,iÆ1,..., Nare not available, and one must deal
with ﬁnite samples instead.
(2) Efﬁcient algorithm (LOT Wassmap) : We provide an algorithm, “LOT Wassmap” ,
inspired by the Wassmap algorithm of [21]. It essentially uses linearized Wasser-
stein distance approximations through LOT in the Multidimensional Scaling al-
gorithm, leveraging our approximation guarantees from (1). However, we do
not compute the LOT-Wasserstein distance matrix and feed it into MDS, but in-
stead compute the truncated SVD of centered transport maps. This is the same
in theory, but computationally more efﬁcient.
1.1. Previous work. The idea of replacing pairwise Euclidean distances with pairwise
Wasserstein distances in common manifold learning algorithms has been explored in
many settings; for example in [44] to study shape spaces of proteins, in [28, 14] to ana-
lyze gene expression data, and in [40] for cancer detection.
Theoretical results on the reconstruction of certain submanifolds in W2(Rn) through
the MDS algorithm using pairwise Wasserstein distances are presented in [21]. The as-
sociated algorithm, Wassmap, is the basis for our LOT Wassmap algorithm.
Related to the idea of uncovering submanifolds in the Wasserstein space is “Wasser-
stein dictionary learning” as discussed in [33, 42]. The authors propose to represent
complex data in the Wasserstein space as Wasserstein barycenters of a dictionary.
1.2. Approximation guarantees. Using approximations of the Wasserstein distance in
manifold learning algorithms such as MDS may change the embedding quality, and our
main result provides theoretical bounds on the error:
Theorem 1.1 (Informal version of Theorem 3.3) .Assume that data points {¹i}N
iÆ1are
¿1¡close to a d-dimensional submanifold Win the Wasserstein space, which is isometric
to a subset ­of Euclidean space Rd. Furthermore assume that we only have access to ap-
proximations ¸i jof the pairwise distances W 2(¹i,¹j), and that the approximation error
is¿2.
Then, under some technical assumptions, the Multidimensional Scaling algorithm us-
ing distances ¸i jas input recovers data points {zi}N
iÆ1½Rd, which are C N,W(¿1Å¿2)-close
to­up to rigid transformations.
Some remarks on this result:
²The ﬁrst source of error, ¿1, depends on how close the data points are to the
submanifold Wisometric to a subspace of Rd, which is completely determined
by the dataset.4 LINEARIZED WASSERSTEIN DIMENSIONALITY REDUCTION WITH APPROXIMATION GUARANTEES
²The second source of error, ¿2, depends on the approximation scheme used,
and can be made arbitrarily small with sufﬁcient computational time or good
choice of parameters.
A signiﬁcant part of this paper is dedicated to providing bounds for ¿2, when common
approximation schemes for W2(¹i,¹j) are used, and when { ¹i}N
iÆ1are only available
through samples, i.e. when ¹i¼b¹iÆ1
miPmi
jÆ1±Y(i)
jwith Y(i)
j»¹ii.i.d. In particular, we
introduce empirical linearized Wasserstein-2 distance ,cWLOT
2,¾, which uses two approxi-
mation schemes:
(a) Entropic regularized formulation : A very successful approximation framework for
efﬁcient Wasserstein distance computation is the entropic regularized formulation
of (1), which depends on a parameter ¯, and leads to Sinkhorn distances [17]:
min
¼2¡(¹,º)Z
R2n1
2kx¡yk2d¼(x,y)Å¯DKL(¼k¹­º), (2)
where DKLis the Kullback–Leibler divergence of measures [23]. This formulation
leads to a unique solution (in contrast to (1)), and to a signiﬁcant computational
speed-up in n, achieving O(n2log( n)) through matrix scaling algorithms (Sinkhorn’s
algorithm) [2, 17].
(b) Linearized Wasserstein distances : Linearized optimal transport (LOT) [20, 40] ap-
proximates Wasserstein distances by linear L2¡distances in the tangent space at a
chosen reference measure ¾:
WLOT
2,¾(¹,º) :ÆµZ
RnkT¹
¾(x)¡Tº
¾(x)k2d¾(x)¶1/2
, (3)
where T¹
¾denotes the optimal transport map from ¾to¹(either computed through
(1) or (2), and using barycentric projections to make a transport plan into a trans-
port map). Instead of computing all pairwise optimal transport maps, in this frame-
work, one computes T¹i
¾from ¾to¹i, and approximates pairwise maps between ¹i
and¹jas a composition of T¹i
¾and T¹j
¾, reducing the computation in NtoO(N).
This framework has been successfully applied signal and image classiﬁcation tasks
[34, 41], such as visualizing phenotypic differences between types of cells [7]. There
furthermore exist error bounds for WLOT
2,¾[9, 19, 20, 25, 29, 32].
With these approximation schemes at hand, we deﬁne the empirical linearized Wasserstein-
2 distance :
cWLOT
2,¾(b¹,bº) :ÆÃ
1
mmX
jÆ1kTb¹
¾(Xj)¡Tbº
¾(Xj)k2!1/2
, (4)
where Xj»¾i.i.d. and the transport maps are either computed by (1) or (2) (and with
barycentric projections, if necessary).
We provide values for ¿2as in Theorem 1.1, by bounding jW2(¹,º)2¡cWLOT
2,¾(b¹,bº)2j,
using either a linear program or Sinkhorn iterations to compute the transport plans.
These bounds are derived by combining the following results:
²Estimation of optimal transport maps with plug-in estimators, i.e. bounds on
kTbº
¹¡Tº
¹k¹, which are provided by [18] for the linear program case, and by [35]
in the regularized case. Both [18] and [35] assume compactly supported ¹and
º, while we are able to relax the compact support assumption on the target mea-
sure, as long as it can be approximated by compactly supported measures.LINEARIZED WASSERSTEIN DIMENSIONALITY REDUCTION WITH APPROXIMATION GUARANTEES 5
²Approximation results for WLOT
2,¾, which are provided in [25, 32], and are based
on the idea that ¹iare generated by almost compatible functions Happlied to
a ﬁxed generator ¹. We also strengthen some of the approximation results in
[25, 32].
1.3. Efﬁcient algorithm: LOT Wassmap. The Wassmap algorithm of [21] requires com-
puting the pairwise Wasserstein distance matrix W2(¹i,¹j),i,jÆ1,..., N, which leads
toO(N2) expensive computations. We introduce LOT Wassmap (see Algorithm 2), which
uses LOT distances (3) to linearly approximate W2(¹i,¹j) (since the input of our algo-
rithm are empirical samples b¹i, we actually use the empirical linearized Wasserstein-2
distance (4)). This results in only O(N) optimal transport computations.
However, in practice, we avoid computing the pairwise LOT distance matrix. Instead,
we compute the truncated SVD of the centered transport maps, which is computation-
ally more efﬁcient. We show that in theory this produces a result equivalent to Theo-
rem 1.1:
Corollary 1.2 (Informal version of Corollary 3.4) .Assume that data points {¹i}N
iÆ1are
¿1¡close to a d-dimensional submanifold Win the Wasserstein space, which is isomet-
ric to a subset ­of Euclidean space Rd. Choose a reference measure ¾and compute all
transport maps T¹i
¾(either with a linear program (1)or with Sinkhorn approximations
(2), and with barycentric projections, if necessary). Let ¿2be the error between the empir-
ical linearized Wasserstein-2 distance cWLOT
2,¾(b¹i,b¹j)of(4)and the actual Wasserstein-2
distance W 2(¹i,¹j).
Then, under some technical assumptions, the truncated SVD of the centered transport
maps T¹i
¾(column-stacked) produces data points {zi}N
iÆ1½Rd, which are C N,W(¿1Å¿2)-
close to ­up to rigid transformations.
We note that Corollary 1.2 is a corollary of Theorem 1.1 and that the technical as-
sumptions and constants are the same in both results.
In Section 8, we provide experiments demonstrating that LOT Wassmap does attain
correct embeddings given ﬁnite samples without explicitly computing the pairwise LOT
distance matrix. In particular, we show that the embedding quality improves with in-
creased sample size and that LOT Wassmap signiﬁcantly reduces the computational
cost when compared to Wassmap.
1.4. Organization of the paper. This paper is organized as follows: We start by intro-
ducing important notation and background in Section 2. This includes discussion of
the MDS and Wassmap algorithms, (linearized) optimal transport, and plug-in estima-
tors. Section 3 introduces the LOT Wassmap algorithm and provides the main results.
Sections 4 and 5 provide approximation guarantees for cWLOT
2,¾(b¹,bº) for compactly and
non-compactly supported target measures, respectively. The approximation guaran-
tees come with many technical assumptions, and Sections 6 and 7 are dedicated to dis-
cussing settings in which these assumptions hold. The paper concludes with experi-
ments in Section 8, which show the effectiveness of LOT Wassmap. Proofs are provided
in Appendices A to D.
2. N OTATION AND BACKGROUND
This paper has a signiﬁcant amount of background and notation which is summa-
rized categorically here. See Table 1 for an overview of notation used in the paper.6 LINEARIZED WASSERSTEIN DIMENSIONALITY REDUCTION WITH APPROXIMATION GUARANTEES
Notation Deﬁnition Reference
¢ Square Euclidean distance matrix Algorithm 1
¤ Perturbed distance matrix Corollary 3.2
X†Moore–Penrose pseudoinverse of matrix X Section 2.1
¹ Template measure Section 2.4
b¹ Empirical measure approximating ¹ (7)
¾ Reference measure for LOT Section 2.4
k¢k Sp Schatten p-norm Section 2.1
k¢k Spectral norm of a matrix or Euclidean norm of a
vectorSection 2.1
k¢k F Frobenius norm of a matrix Section 2.1
k¢k max (Entrywise) maximum norm of a matrix Section 2.1
k¢k¹ Norm on L2(Rn,¹) Section 2.3
n Dimension of Euclidean space that probability mea-
sures are deﬁned onSection 2.3
P(Rn) Probability measures on RnSection 2.3
Pac(Rn) Absolutely continuous probability measures on RnSection 2.3
W2(Rn) Wasserstein-2 space over RnSection 2.3
W2(¹,º) Wasserstein-2 distance between ¹andº (5)
WLOT
2,¾(¹,º)Linearized Wasserstein-2 distance between ¹andº,
with¾as reference(6)
cWLOT
2,¾(¹,º)Empirical linearized Wasserstein-2 distance (12)
T¹
¾ Optimal transport (Monge) map from ¾to¹ Section 2.3
T]¹ Pushforward of ¹with respect to T Section 2.3
Tb¹
¾ Barycentric projection of an optimal transport plan
(Kantorovich potential)(10)
d Embedding dimension of MDS Section 2.2
k Sample size that generates b¹ (7)
m Sample size that generates b¾ Algorithm 2
N Number of data points Algorithm 2
" Distance from compatibility Deﬁnition 2.2
¯ Regularizer for Sinkhorn OT Section 4.2
TABLE 1. Overview of notation used in the paper.
2.1. Linear Algebra Preliminaries. Given A2Rm£n, its Singular Value Decomposition
(SVD) is given by AÆU§V>, where U2Rm£mand V2Rn£nare orthogonal matri-
ces and §2Rm£nhas non-zero entries along its main diagonal (singular values). The
singular values are the square roots of the eigenvalues of A>Aand are taken in de-
scending order ¾1¸¾2¸ ¢¢¢ ¸ ¾min{ m,n}¸0. The truncated SVD of order dofAis
AdÆUd§dV>
dwhere Udand Vdconsist of the ﬁrst dcolumns of Uand V, respectively,
and§dÆdiag(¾1,...,¾d)2Rd£d. The Moore–Penrose pseudoinverse of A2Rm£nis the
n£mmatrix denoted by A†and deﬁned by A†ÆV§†U>where §†is the n£mmatrix
with entries1
¾1,...,1
¾min{ m,n}along its main diagonal.LINEARIZED WASSERSTEIN DIMENSIONALITY REDUCTION WITH APPROXIMATION GUARANTEES 7
The Schatten p-norms (1 ·p· 1 ) are a general class of unitarily invariant, submul-
tiplicative norms on Rm£nand are deﬁned to be the `pnorms of the vector of singu-
lar values: kAkSp:Æ k(¾1,...,¾min{ m,n})k`p. The Frobenius norm, which is the Schatten
2-norm is denoted by k¢k F, and the spectral norm, which is the Schatten 1-norm is
denoted simply by k¢k. We also use k¢k to denote the Euclidean norm of a vector.
2.2. Multidimensional scaling. Let 1be the all-ones vector in RN, and J:ÆI¡1
N11>.
Then Multidimensional Scaling (MDS) is summarized in Algorithm 1. For more details
see [27].
Algorithm 1: Multidimensional Scaling (MDS) [27]
Input : Points { yi}N
iÆ1½RD; embedding dimension d¿D.
Output: Low-dimensional embedding points { zi}N
iÆ1½Rd
Compute pairwise distance matrix ¢i jÆ kyi¡yjk2
BÆ ¡1
2J¢J
(Truncated SVD): BdÆVd§dVT
d
ziÆ(Vd§d)(i,:), for iÆ1,..., N
Return {zi}N
iÆ1
MDS produces an isometric embedding RD!Rdif and only if the matrix Bis sym-
metric positive semi-deﬁnite with rank d, a result that goes back to Young and House-
holder [43]. In this case, the embedding points { zi}N
iÆ1½Rdsatisfy kzi¡zjk Æ k yi¡yjk
and are unique up to rigid transformation.
2.3. Optimal Transport Preliminaries. LetP(Rn) be the space of all probability mea-
sures on Rn, withPac(Rn) being the subset of all probability measures which are abso-
lutely continuous with respect to the Lebesgue measure. Given ¹2Pac(Rn), we denote
its probability density function by f¹. The quadratic Wasserstein space W2(Rn) is the
subset of P(Rn) of measures with ﬁnite second momentR
Rnkxk2d¹(x)Ç 1 equipped
with the quadratic Wasserstein metric given by
W2(¹,º) :Æ inf
¼2¡(¹,º)µZ
R2nkx¡yk2d¼(x,y)¶1
2
, (5)
where ¡(¹,º) :Æ{°2P(R2n) :°(A£Rn)Æ¹(A),°(Rn£A)Æº(A) for all A½Rn} is the set
of couplings, i.e., measures on the product space whose marginals are ¹andº.
In [10], Brenier showed that if ¹is absolutely continuous with respect to the Lebesgue
measure, the optimal coupling of (5) takes the special form ¼Æ(id,Tº
¹)]¹, where ]is the
pushforward operator ( S]¹(A)Æ¹(S¡1(A)) for Ameasurable) and Tº
¹2L2(Rn,¹) solves
min
T:T]¹ÆºZ
RnkT(x)¡xk2d¹(x).
For simplicity, we denote the norm on L2(Rn,¹) bykfk2
¹:ÆR
Rnkf(x)k2d¹(x). Note that
ifTº
¹exists, then
W2(¹,º)Æ kTº
¹¡idk¹.
Furthermore, [10] shows that when ¹is absolutely continuous with respect to the Lebesgue
measure, the map Tº
¹is uniquely deﬁned as the gradient of a convex function Á, i.e.
Tº
¹Æ rÁ(up to an additive constant).8 LINEARIZED WASSERSTEIN DIMENSIONALITY REDUCTION WITH APPROXIMATION GUARANTEES
2.4. Linearized optimal transport. Linearized optimal transport (LOT) [20, 29, 34, 41]
deﬁnes an embedding of P(Rn) into the linear space L2(Rn,¾), with ¾being a ﬁxed
reference measure. Under the assumption that the optimal transport map exists, the
embedding is deﬁned by ¹7!T¹
¾. This embedding can be used as a feature space, for
example, to classify subsets of P(Rn), to linearly approximate the Wasserstein distance,
or for fast Wasserstein barycenter computations [1, 25, 29, 32, 34].
In particular, the LOT embedding deﬁnes a linearized Wasserstein-2 distance:
WLOT
2,¾(¹,º) :Æ kT¹
¾¡Tº
¾k¾. (6)
In certain settings, this linearized distance approximates the Wasserstein-2 distance.
The strongest results can be obtained when the so-called compatibility condition is sat-
isﬁed:
Deﬁnition 2.1 (Compatibility condition [1, 32, 34]) .Let¾,¹2W2(Rn)\Pac(Rn). We say
that the LOT embedding is compatible with the ¹-pushforward of a function g 2L2(Rn,¹)
if
Tg]¹
¾Æg±T¹
¾.
The compatibility condition describes an interaction between the optimal transport
map and the pushforward operator, namely it requires invertability of the exponential
map [20].
When the compatibility condition holds for two functions g1,g2, then LOT is an isom-
etry, i.e. WLOT
2,¾(g1]¹,g2]¹)ÆW2(g1]¹,g2]¹) as shown in Lemma A.3 and [32, 34]. In par-
ticular, this is the case when gis either a shift or scaling, or a certain type of shearing
[25, 32, 34].
We can furthermore consider a generalization to “almost compatible” functions, also
termed "-compatible:
Deﬁnition 2.2 ("-compatibility) .Let¾,¹2W2(Rn)\Pac(Rn). We say that His"- com-
patible with respect to ¾and¹, if for every h 2H, there exists a compatible transforma-
tion g such that kg¡hk¹Ç", where g ±T¹
¾ÆTg]¹
¾.
We remark that compatibility is stable. Similar to compatibility implying isometry,
there exist results that imply "-compatible transformations imply “almost"-isometry
between WLOT
2,¾and W2. Some of these results are accounted for in [32, Proposition 4.1];
however, we also extend these almost-compatibility results in Theorem A.4. These re-
sults make use of the Hölder regularity bounds for WLOT
2,¾of [20, 29]. We note that the
“isometry under compatibility” result mentioned above is a direct consequence of the
preceding proposition, namely by setting "Æ0.
In this paper, we consider measures ¹i,iÆ1,..., Nof the form ¹iÆhi]¹, where ¹is
a ﬁxed template measure , and h2HwithHa space of functions in L2(Rn,¹). This is
similar to assumptions in [1, 25, 32, 34], where Hconsists of shifts and scalings, com-
patible maps, or has other properties, such as convexity and compactness. We will write
¹i»H]¹to indicated that ¹iis of such a form for all iÆ1,..., N, andHwill be speciﬁed
in the respective context. Note that [1] calls this data generation process an “algebraic
generative model” .
2.5. Optimal transport with plug-in estimators. Explicit descriptions of the measures
¹are often unavailable in applications, and one must instead deal with ﬁnite samplesLINEARIZED WASSERSTEIN DIMENSIONALITY REDUCTION WITH APPROXIMATION GUARANTEES 9
of the measure. In this paper, we consider empirical distributions
b¹Æ1
kkX
iÆ1±Yi(7)
with Yi»¹i.i.d. In what follows, we will consider approximations of both the target and
reference distributions via empirical distributions.
The Kantorovich problem (5) has a (possibly non-unique) solution for transporting
an absolutely continuous measure ¾to an empirical measure of the form (7). Following
[18], we deﬁne the set of Kantorovich plans
¡min:Æargmin
¼2¡(¾,b¹)Z
R2nkx¡yk2d¼(x,y), (8)
which may contain more than one transport plan. In practice, these optimal transport
plans are exactly computed via linear programming to solve (8). We call optimal trans-
port plans solved with linear programming °LP. It is much faster, however, to approxi-
mate the optimal transport plan by using an entropic regularized plan [17]. In particular,
we get a unique solution by solving
°¯:Æargmin
¼2¡(¾,b¹)Z1
2kx¡yk2d¼(x,y)Å¯DKL(¼k¾­b¹), (9)
where DKLis the Kullback–Leibler divergence of measures [23], ¾­b¹is the measure on
the product space Rn£Rnwhose marginals are ¾andb¹, and ¯denotes the regular-
izer. We solve (9) with Sinkhorn’s algorithm, which yields entropic potentials f¯and g¯
corresponding to ¾andb¹, respectively.
Regardless of whether we solve the optimal transport plan using (8) or (9), we can
make a transport plan °2¡into a map by deﬁning the barycentric projection
Tb¹
¾(x;°) :ÆR
yyd°(x,y)
R
yd°(x,y), for x2supp( ¾). (10)
This leads to a natural way to consider linearized Wasserstein-2 distances of the form
(6) with absolutely continuous reference ¾, and for empirical distributions:
WLOT
2,¾(b¹,bº;°) :Æ kTb¹
¾(¢;°b¹)¡Tbº
¾(¢;°bº)k¾, (11)
where °2{°LP,°¯} denotes the method used to calculate the transport plans °b¹and
°bº, which are transport plans from ¾tob¹andbº, respectively. We suppress this nota-
tion and will simply use Tb¹
¾(¢;°LP) or Tb¹
¾(¢;°¯) to denote the barycentric projection map
computed via linear programming and Sinkhorn, respectively, so that °LPand°¯are
understood to be in ¡(¾,b¹).
To account for mﬁnite samples of the reference distribution, we deﬁne the empirical
linearized Wasserstein-2 distance by
cWLOT
2,¾(b¹,bº;°) :ÆÃ
1
mmX
jÆ1kTb¹
¾(Xj;°b¹)¡Tbº
¾(Xj;°bº)k2!1/2
, (12)
where Xj»¾i.i.d.10 LINEARIZED WASSERSTEIN DIMENSIONALITY REDUCTION WITH APPROXIMATION GUARANTEES
Remark 2.3. When we use °¯for a transport plan between b¾andb¹, note that our
barycentric projection map is given by
Tb¹
b¾(x;°¯) :Æ1
kPk
iÆ1yiexp³³
g¯,k(yi)¡1
2kx¡yik2´
/¯´
1
kPk
iÆ1exp³³
g¯,k(yi)¡1
2kx¡yik2´
/¯´, (13)
where g¯,kdenotes the entropic potential corresponding to b¹,yi2supp(b¹), and kis the
sample size for both b¾andb¹.
Remark 2.4. Since our approximations will require us to use msamples from the refer-
ence distributions, the barycentric projection map Tb¹
¾(x) will only work for x2supp(b¾);
however, for general computation, we can just interpolate to calculate Tb¹
¾(x) for x2
supp( ¾) \ supp( b¾).
In what follows, we are interested in bounds for
jW2(¹,º)2¡cWLOT
2,¾(b¹,bº;°)2j
for°2{°LP,°¯}. In particular, we want similar results to Theorem A.4 (Wasserstein-
2 compared to LOT) and results in [18] (Wasserstein-2 compared to Wasserstein-2 on
empirical distributions). This requires comparisons between all of W2(¹,º),WLOT
2,¾(¹,º),
WLOT
2,¾(b¹,bº;°), andcWLOT
2,¾(b¹,bº;°), which are discussed in Section 4 and Section 5.
2.6. Wassmap. Various generalizations of MDS have been explored [16] including stress
minimization, which is useful in graph drawing [24, 30], Isomap [38] which replaces
pairwise distance by a graph estimation of manifold geodesics, and is useful for em-
bedding data from d–dimensional nonlinear manifolds in RD. Wang et al. [40] utilized
MDS with ¢i jÆW2(¹i,¹j)2for data considered as probability measures in Wasserstein
space with applications to cell imaging and cancer detection. Subsequently, Hamm et
al. [21] proved that several types of submanifolds of W2can be isometrically embedded
via MDS with Wasserstein distances (as in [40]) and empirically studied Wassmap: a
variant of Isomap that approximates nonlinear submanifolds of W2. In particular, [21]
shows that for some submanifolds of W2(Rm) of the form H]¹whereHÆ{hµ:µ2£½
Rd} which are isometric Euclidean space, the parameter set £½Rdcan be recovered
up to rigid transformation via MDS with Wasserstein distances (e.g., translations and
anisotropic dilations).
2.7. Other notations. For scalars aand bwe use a_bto denote the maximum and a^b
to denote the minimum value of the pair. Throughout the paper, constants will typically
be denoted by Cand may change from line to line, and subscripts will be used to denote
dependence on a given set of parameters. We use a³bto mean that ca·b·C a for
some absolute constance 0 Çc,CÇ 1 .
For a random variable Xn, we say that XnÆOp(an) if for every "È0 there exists MÈ0
and NÈ0 such that
Pµ¯¯¯¯Xn
an¯¯¯¯ÈM¶
Ç"8n¸N.
We denote by O(d) the orthogonal group over Rd, and the related Procrustes distance
(in the Frobenius norm) between matrices X,Y2Rd£Nis min
Q2O(d)kX¡QYkF.LINEARIZED WASSERSTEIN DIMENSIONALITY REDUCTION WITH APPROXIMATION GUARANTEES 11
3. LOT W ASSMAP ALGORITHM AND MAIN THEOREM
Here we present our main algorithm which is an LOT approximation to the Wassmap
embedding of [21], and our main theorem which describes the quality of the embedding
using some existing perturbation bounds for MDS.
3.1. The LOT Wassmap Embedding Algorithm. The algorithm presented here (Algo-
rithm 2) takes discretized samples of a set of measures { ¹i}N
iÆ1½W2(Rn) and a dis-
cretized sample of a reference measure ¾2W2(Rn), computes transport maps from
the empirical reference measure b¾to each empirical target measure c¹iusing optimal
transport solvers and barycentric projections. Finally, the truncated right singular vec-
tors and singular values of the centered transport map matrix are used to produce the
low-dimensional embedding of the measures. Two things are important to note here:
ﬁrst, the output of the algorithm is the same as the output of multi-dimensional scaling
using pairwise squared LOT distances (or Sinkhorn distances in the approximate case),
but we use the same trick as the reduction of PCA to the SVD to avoid actually comput-
ing the distance matrix; second, in contrast to the Wassmap embedding of [21] which
requires O(N2) Wasserstein distance computations, Algorithm 2 requires computation
of only O(N) optimal tranport maps. Given the high cost of computing a single optimal
transport map for densely sampled measures, this represents a signiﬁcant savings.
Note that the factor of1pmappearing in the computation of the ﬁnal embedding is
due to (12) where the1
mappears in the deﬁnition of the empirical LOT distance. Lemma
Lemma A.1 shows that T>Twhere Tis as in Algorithm 2 is actually the MDS matrix
¡1
2J¤Jwhere ¤consists of the empirical LOT distances between the data, hence we
absorb the1
minto the norm in (12) to get the matrix Tin Algorithm 2.
Algorithm 2: LOT WassMap Embedding
Input : Reference point cloud { wi}m
iÆ1»¾2W2(Rn)
Sample point clouds { xk
j}nk
jÆ1»¹k2W2(Rn) (kÆ1,..., N)
OT solver (with regularizer if Sinkhorn)
Embedding dimension d
Output: Low-dimensional embedding points { zi}N
iÆ1µRd
forkÆ1,..., Ndo
Calculate cost matrix Ci jÆ kwi¡xk
jk2
Compute OT plan °k2Rm£nkbetween { wi}m
iÆ1and { xk
j}nk
jÆ1using Cand OT
solver
Calculate barycentric projection eTk(wi)Æ³Pnk
jÆ1xk
j(°k)i j´
/³Pnk
jÆ1(°k)i j´
bTÆ£eTj(wi)¤m,n
iÆ1,jÆ1
for kÆ1,..., Ndo
T:kÆ1pm(bT:k¡1
NPN
kÆ1bT:k)
Compute the truncated SVD of TasTdÆUd§dV>
d
Return ziÆVd§d(i,:)
3.2. MDS Perturbation Bounds. As stated above, the output of Algorithm 2 is equiv-
alent to the output of MDS on the transport map matrix Ttherein. Consequently, the12 LINEARIZED WASSERSTEIN DIMENSIONALITY REDUCTION WITH APPROXIMATION GUARANTEES
analysis of the algorithm will require some results regarding MDS. On the road to stating
our main result, we summarize some nice MDS perturbation results of [5].
Theorem 3.1 ([5, Theorem 1]) .Let Y ,Z2Rd£Nwith d ÇN such that rank( Y)Æd, and
let"2:Æ kZ>Z¡Y>YkSpfor some p 2[1,1]. Then,
min
Q2O(d)kZ¡QYkSp·8
<
:kY†k"2Å³
(1¡kY†k2"2)¡1
2kY†k"2´
^d1
2p",kY†k"Ç1,
kY†k"2Åd1
2p", o.w.
Consequently, if kY†k"·1p
2, then
min
Q2O(d)kZ¡QYkSp·(1Åp
2)kY†k".
Corollary 3.2. Let y 1,..., yN2Rdbe centered, span Rd, and have pairwise dissimilarities
¢i jÆ kyi¡yjk2. Let {¤i j}N
i,jÆ1be arbitrary real numbers and p 2[1,1]. IfkY†kk¤¡
¢k1
2
Sp·1p
2, then MDS (Algorithm 1) with input dissimilarities {¤i j}N
i,jÆ1and embedding
dimension d returns a point set z 1,..., zN2Rdsatisfying
min
Q2O(d)kZ¡QYkSp·(1Åp
2)kY†kk¤¡¢kSp.
Proof of Corollary 3.2. The proof follows along similar lines to that of [5, Corollary 2]
with some modiﬁcations. First, note that the centering matrix Jin MDS satisﬁes kJk Æ1
as it is an orthogonal projection. Then, by using the fact that kABkSp· kAkkBkSp, we
can estimate
1
2kJ(¤¡¢)JkSp·1
2kJk2k¤¡¢kSp·1
2k¤¡¢kSpÇ¾2
d(Y), (14)
where the ﬁnal inequality follows by assumption.
Since Yis a centered point set, we have Y>YÆJY>Y JÆ ¡1
2J¢J(Lemma A.1). Thus
by Weyl’s inequality, the fact that k¢k · k¢k Spfor all p, and (14),
¾dµ
¡1
2J¤H¶
¸¾dµ
¡1
2J¢J¶
¡1
2kJ(¤¡¢)JkSp
¸¾dµ
¡1
2J¢J¶
¡1
2kJ(¤¡¢)JkSp
Æ¾2
d(Y)¡1
2kJ(¤¡¢)JkSp
È0.
Consequently, ¡1
2J¤Jhas rank d, so if Zcontains the columns of the MDS embedding
corresponding to ¤, then Z>Zis the best rank- dapproximation of ¡1
2J¤J(by construc-
tion). It follows from Mirsky’s inequality that
°°°°Z>ZÅ1
2J¤J°°°°
Sp·°°°°1
2J(¤¡¢)J°°°°
Sp. (15)
Combining (14) and (15), we have
"2:Æ kZ>Z¡Y>YkSp·°°°°Z>ZÅ1
2J¤J°°°°
SpÅ°°°°1
2J(¤¡¢)J°°°°
Sp· kJ(¤¡¢)JkSp
· k¤¡¢kSp.LINEARIZED WASSERSTEIN DIMENSIONALITY REDUCTION WITH APPROXIMATION GUARANTEES 13
Thus, kY†k"· kY†kk¤¡¢k1
2
Sp·1p
2, so we may apply the ﬁnal bound of Theorem 3.1 to
yield the conclusion. 
3.3. Main Theorem. The following theorem shows the quality of an MDS embedding of
a discrete subset of W2(Rn) when approximations of the pairwise W2(Rn) distances are
used (via, for example, LOT approximations, Sinkhorn regularization, or other approx-
imation techniques). The embedding quality is understood in two parts: ﬁrst, how far
away the set is from a subset of W2(Rn) that is isometric to Rd, and second, how good
an approximation to the Wasserstein distances one utilizes in MDS. The second source
of error can always be made arbitrarily small given sufﬁcient computation time or judi-
cious choice of parameters (as in Sinkhorn, for example). However, the ﬁrst source of
error arises from the geometry of the set of points, and may or may not be small.
Note that using Corollary 3.2 outright would require computing a proxy distance ma-
trix and applying MDS; however, to make Algorithm 2 computationally efﬁcient, we in-
stead compute the truncated SVD of the centered transport maps rather than on the
distance matrix between the transport maps. These are the same in theory, but allow for
signiﬁcantly less computation in practice. Below, we state our main theorem, which is
stated in terms of the output of MDS on an estimation of Wasserstein distances between
measures; but we stress that we are able to easily transfer the bounds to the output of
Algorithm 2, which does not require any distance matrix computation.
Theorem 3.3. Let{¹i}N
iÆ1½W2(Rn). Suppose W½W2(Rn)is a subset of Wasserstein space
that is isometric to a subset of Euclidean space ­½Rd, and {ºi}N
iÆ1½Wand {yi}½­are
such that jyi¡yjj ÆW2(ºi,ºj). Let¢i j:ÆW2(ºi,ºj)2,¡i j:ÆW2(¹i,¹j)2, and¤i j:Æ¸2
i j
for some ¸i j2R. Let {zi}N
iÆ1be the output of MDS (Algorithm 1) with input ¤.
IfjW2(¹i,¹j)2¡W2(ºi,ºj)2j ·¿1andjW2(¹i,¹j)2¡¸2
i jj ·¿2for some ¿1and¿2, and
kY†kp
N(¿1Å¿2)1
2·1p
2, (16)
then {zi}N
iÆ1½Rdsatisﬁes
min
Q2O(d)kZ¡QYkF·(1Åp
2)kY†kN(¿1Å¿2).
Proof. Note that
k¤¡¢kF· k¡¡¢kFÅk¤¡¡kF·N(¿1Å¿2).
Consequently, (16) allows us to apply Corollary 3.2 to yield the conclusion. 
Specializing Theorem 3.3 to the case of Algorithm 2 yields the following corollary,
which shows that the truncated SVD of the centered LOT transport matrix Tis equiva-
lent to the output ziof MDS in Theorem 3.3.
Corollary 3.4. Invoke the notations and assumptions of Theorem 3.3. Choose a refer-
ence measure ¾2W2(Rn)and compute all transport maps T¹i
¾. Let T be the transport
map matrix created by centering and column-stacking the transport maps T¹i
¾as in Al-
gorithm 2. Let U d§dV>
dbe the truncated SVD of T , and let z iÆVd§d(i,:)for1·i·N
(i.e., z iis the output of Algorithm 2). If (16) holds, then
min
Q2O(d)kZ¡QYkF·(1Åp
2)kY†kN(¿1Å¿2).14 LINEARIZED WASSERSTEIN DIMENSIONALITY REDUCTION WITH APPROXIMATION GUARANTEES
Proof. Since Tis centered, Lemma A.1 implies that T>TÆJT>T JÆ ¡1
2J¤J. Conse-
quently, if ¡1
2J¤JÆV§2V>ÆT>T, then Thas truncated SVD TdÆUd§dV>
d, and
therefore ziÆVd§d(i,:) arises from the truncated SVD of Tand is also the output of
MDS with input ¤. The conclusion follows by direct application of Theorem 3.3.

In the rest of the paper, we will discuss how various LOT approximations to Wasser-
stein distances affect the value of the bound ¿2appearing in Theorem 3.3 and Corol-
lary 3.4. In particular, we get different values of ¿2when we have compactly supported
target measures (as in Theorem 4.2 for linear programming estimators and Theorem 4.7
for Sinkhorn estimators) and non-compactly supported target measures (as in Theo-
rem 5.4 for linear programming estimators and Theorem 5.5 for Sinkhorn estimators).
4. B OUNDS FOR COMPACTLY SUPPORTED TARGET MEASURES
To capture the bound ¿2of Theorem 3.3, we turn our attention to approximating the
pairwise square-distance matrix£
W2
2(¹i,¹j)¤N
i,jÆ1appearing in the theorem statement
with the ﬁnite sample, discretized LOT distance matrix that comes from differences be-
tween transport maps to a ﬁxed reference, a ﬁnite sampling of ¹i, and a discretization of
the reference distribution ¾. In particular, the main approximation argument consists
of the following triangle inequality:
¯¯W2(¹1,¹2)2¡cWLOT
2,¾(c¹1,c¹2;°)2¯¯·¯¯W2(¹1,¹2)2¡WLOT
2,¾(¹1,¹2)2¯¯
| {z }
LOT error
Å¯¯WLOT
2,¾(¹1,¹2)2¡WLOT
2,¾(c¹1,c¹2;°)2¯¯
| {z }
ﬁnite sample and optimization error
Å¯¯WLOT
2,¾(c¹1,c¹2;°)2¡cWLOT
2,¾(c¹1,c¹2;°)2¯¯
| {z }
discretized ¾sampling error.
There are four sources of error between these two distance matrices:
(1) approximating the Wasserstein distance with LOT distance,
(2) approximating LOT embeddings between ¹iand¹jwith the barycenteric ap-
proximations computed using ﬁnite samples c¹iandc¹j,
(3) approximating the integral with respect to the reference measure ¾by the dis-
cretized sampling b¾, and
(4) optimization error in approximating the optimal transport map.
The error from (1) and (3) are handled in Appendix B whilst the error from (2) gives
us the main theorems of this section. Error from (4) is also implicitly considered by
handling error from (2) since the optimization error for using a linear programming
optimizer versus a Sinkhorn optimizer is seen in the error bounds of Theorem 4.2 and
Theorem 4.7. We deal with each error separately and chain the bounds together at the
end.
Before dealing with any of the details of the proofs, we need the following assump-
tions on ¾,¹, andH:
Assumption 4.1. Consider the following conditions on ¾,¹, andH
(i)¾2Pac(­)for a compact convex set ­µB(0,R)½Rnwith probability density f ¾
bounded above and below by positive constants.
(ii)¹has ﬁnite p-th moment with bound M pwith p Èn and p ¸4.LINEARIZED WASSERSTEIN DIMENSIONALITY REDUCTION WITH APPROXIMATION GUARANTEES 15
(iii) There exist a ,AÈ0such that every h 2Hsatisﬁes a kxk · k h(x)k ·Akxk.
(iv)His compact and "-compatible with respect to ¾,¹2W2(Rn). Moreover, suph,h02Hkh¡
h0k¹·M.
(v)¹i»H]¹i.i.d.
These assumptions ensure that "-compatible transformations are also “ "-isometric"
as shown in Theorem A.4.
4.1. Using the Linear Program to compute transport maps. In this subsection, we as-
sume that the classical linear program is used to compute the optimal transport maps
fromc¹ito the reference (and its discretization).
Theorem 4.2. Let±È0. Along with Assumption 4.1 and that ¹2Pac(­)for the ­in
Assumption 4.1, assume that
(i) T¹i
¾is L-Lipschitz, which may occur if T¹
¾is L-Lipschitz. Note that if ¾and¹are
both compactly supported, then T¹
¾itself is L-Lipschitz.
(ii) We estimate ¹iwith an empirical measure c¹iusing k samples and discretize ¾with
m samples. Let our estimator be given by (10) with°solved using linear program-
ming.
Then with probability at least 1¡±,
¯¯W2(¹1,¹2)2¡cWLOT
2,¾(c¹1,c¹2;°LP)2¯¯·(MÅ2R)³
C"p
6pÅ16nÅ2Op(r(k)
nlog(1Åk)tn,®)
ÅRs
2log(2/ ±)
m1
A. (17)
where C is the constant from Theorem A.4 depending on n ,p,­,Mp, the constants a and
A come from Assumption 4.1 (iv), and
r(k)
nÆ8
><
>:2k¡1/2nÆ2,3
2k¡1/2log(1Åk)nÆ4
2k¡2/dn¸5,tn,®Æ8
><
>:(4®)¡1(4Å((2®Å2n®¡n)_0)) nÇ4
(®¡1_7/2)¡1 nÆ4
2(1Ån¡1) nÈ4,
so that r(k)
nand t n,®are on the order of k¡1/nand 2(1Ån¡1), respectively. In this case, ¿2
of Corollary 3.4 is bounded above by the right-hand side of (17) .
Proof. Note that the transport plan that we are using for the following proof is °LP.
Henceforth, we will suppress °LPfrom the terms cWLOT
2,¾(c¹1,c¹2;°LP) and Tc¹j
¾(¢;°LP) for
simplicity.
Since jx2¡y2j Æ jxÅyjjx¡yj, we need to bound both
(a)¯¯¯W2(¹1,¹2)ÅcWLOT
2,¾(c¹1,c¹2)¯¯¯,
(b)¯¯¯W2(¹1,¹2)¡cWLOT
2,¾(c¹1,c¹2)¯¯¯.
We start with (a): Since both ¹1and¹2are pushforwards of a ﬁxed template distribution
¹, we know that ¹iÆhi]¹, where by [3, Eq. 2.1] and our assumptions, it follows that
W2(¹1,¹2)ÆW2(h1]¹,h2]¹)· kh1¡h2k¹·M.16 LINEARIZED WASSERSTEIN DIMENSIONALITY REDUCTION WITH APPROXIMATION GUARANTEES
Moreover, since His compact, ¹is compactly supported, and ¹i»H]¹, we know that
¹iis compactly supported with supp( ¹i)µB(0,R) for all i. This implies that
cWLOT
2,¾(c¹1,c¹2)Æ0
B@1
mmX
jÆ1jTc¹1
¾(Xj)¡Tc¹2
¾(Xj)j2
| {z }
·4R21
CA1/2
·2R.
Putting these estimates together, we have
¯¯W2(¹1,¹2)ÅcWLOT
2,¾(c¹1,c¹2)¯¯·MÅ2R.
We continue with (b): From the triangle inequality we get
¯¯W2(¹1,¹2)¡cWLOT
2,¾(c¹1,c¹2)¯¯·¯¯W2(¹1,¹2)¡WLOT
2,¾(¹1,¹2)¯¯Å¯¯WLOT
2,¾(¹1,¹2)¡WLOT
2,¾(c¹1,c¹2)¯¯
Å¯¯WLOT
2,¾(c¹1,c¹2)¡cWLOT
2,¾(c¹1,c¹2)¯¯
We now bound these three parts individually:
a) By Assumption 4.1, we can use "-compatibility of Hin Theorem A.4 to get that
¯¯W2(¹1,¹2)¡WLOT
2,¾(¹1,¹2)¯¯·C"p
6pÅ16n,
where Cis from Theorem A.4.
b) For the second term, we again assume that any transport maps involving discrete
measures are obtained from the linear program. In particular, we see that
WLOT
2,¾(¹1,¹2)Æ kT¹1
¾¡T¹2
¾k¾
· kT¹1
¾¡Tb¹1
¾k¾ÅkTb¹1
¾¡Tb¹2
¾k¾ÅkTb¹2
¾¡T¹2
¾k¾
Æ kT¹1
¾¡Tb¹1
¾k¾ÅkTb¹2
¾¡T¹2
¾k¾ÅWLOT
2,¾(c¹1,c¹2).
Note that Assumption 4.1(i) implies that there exists some tÈ0 and ®È0 such that
E¾[tkxk®]Ç 1 . Together with T¹
¾being Lipschitz, this allows us to use Theorem B.1
to conclude that
jWLOT
2,¾(¹1,¹2)¡WLOT
2,¾(c¹1,c¹2)j · k T¹1
¾¡Tb¹1
¾k¾ÅkTb¹2
¾¡T¹2
¾k¾
·2Op(r(k)
nlog(1Åk)tn).
c) From Theorem B.3 we know that with probability at least 1 ¡±,
¯¯WLOT
2,¾(c¹1,c¹2)¡cWLOT
2,¾(c¹1,c¹2)¯¯·Rs
2log(2/ ±)
m.
Putting these bounds together yields the result. 
4.2. Using entropic regularization (Sinkhorn) to compute transport maps. Although
[18] gives estimation rates in terms of a transport map constructed from solving the lin-
ear program associated to the optimal transport problem, solving the regularized opti-
mal transport problem (9) and using the barycentric projection map (13) is much faster.
For this section, we will assume that the target and reference measures are discretized
with the same number of samples k.
Remark 4.3. Since we can choose ¾as well as the sample size for b¾, we can allow kÆm
in this case. We believe, however, that choosing a larger sample size for ¾than¹i(i.e.
mÈk) will result in better approximation.
For the following results, we make use of the following quantity:LINEARIZED WASSERSTEIN DIMENSIONALITY REDUCTION WITH APPROXIMATION GUARANTEES 17
Deﬁnition 4.4. Consider the Wasserstein geodesic between ¾Æ¹0and¹Æ¹1with¹t
being the measure on the geodesic for t 2(0,1) . Let f (t,x)be the density corresponding
to¹t. Then the integrated Fisher information along the Wasserstein geodesic between ¾
and¹is given by
I0(¾,¹)ÆZ1
0Z
Rn°°°rxlogf(t,x)°°°2
2f(t,x)d xd t .
Moreover, recall that the convex conjugate of a function Á2Rnis given by
Á¤(x¤)Æsup
x2Rnx¤>x¡Á(x),
see, e.g., [4, p. 45]. Now by using Theorem 3 from [35], we will show that under suitable
conditions the entropic map Tc¹i
b¾(¢;°¯) is close to T¹i
¾.
Theorem 4.5 ([35, Theorem 3]) .Assume that
(A1) ¾,¹i2Pac(­)for a compact set ­½Rnwith densities satisfying f ¾,f¹i·B and
f¹i¸bÈ0for all x 2­.
(A2) Á2C2(­)andÁ¤2C®Å1(­)for®È1, where Á¤denotes the convex conjugate of
Á.
(A3) T¹i
¾Æ rÁwith mI ¹ r2Á(x)¹LI for m ,LÈ0for all x 2­.
Then the entropic map Tc¹i
b¾(¢;°¯)fromb¾toc¹iwith regularization parameter ¯³k¡1
n0Åe®Å1
satisﬁes
E°°Tc¹i
b¾(¢;°¯)¡T¹i
¾°°2
¾·¡
1ÅI0(¾,¹i)¢
k¡e®Å1
2n0Åe®Å1logk,
where n0Æ2dn/2e,e®Æ®^3, k is the sample size for both b¾andc¹i, and I 0(¾,¹i)is the
integrated Fisher information along the Wasserstein geodesic between ¾and¹i.
Given the sample size kfor both b¾andc¹i, if we let
ZkÆ°°°Tc¹i
b¾(¢;°¯)¡T¹i
¾°°°
¾,
then by Jensen’s inequality (for concave functions) and Theorem 4.5 we have that
E[Zk]·E£
Z2
k¤1/2·r
¡
1ÅI0(¾,¹i)¢
k¡e®Å1
2n0Åe®Å1logk
Æq
log( k)(1ÅI0(¾,¹i))k¡e®Å1
2(2n0Åe®Å1).
Now using Markov’s inequality, we easily have the following corollary.
Corollary 4.6. Assume that ¾and¹isatisfy (A1)–(A3) of Theorem 4.5 and let ±È0. Then
with probability at least 1¡±, we have that
°°Tc¹i
b¾(¢;°¯)¡T¹i
¾°°
¾·1
±q
log( k)¡
1ÅI0(¾,¹i)¢
k¡e®Å1
2(2n0Åe®Å1).
Now we can approximate T¹i
¾with the entropic map that is derived from using Sinkhorn’s
algorithm. Although the barycentric projection map and entropic map approximations
have similar rates of convergence, the entropic map is computationally faster at the cost
of more stringent assumptions in the theorem. The main difference in assumptions be-
low is the addition of (A1)–(A3) from Theorem 4.5 and the asymptotic bound on the
regularization parameter ¯used in the entropic regularization.
Theorem 4.7. Let±È0. Along with Assumption 4.1 and ¹2Pac(­)for the ­in Assump-
tion 4.1, assume that18 LINEARIZED WASSERSTEIN DIMENSIONALITY REDUCTION WITH APPROXIMATION GUARANTEES
(i)¾and¹isatisfy assumptions (A1)–(A3) from Theorem 4.5 for all i . Note that (A1),
regularity of Áin (A2), and the upper bound of (A3) are satisﬁed under the condi-
tions of Caffarelli’s regularity theorem.
(ii) Given empirical distributions b¾andc¹iboth with k sample size, assume that we
have associated entropic potentials (f¯,k,g¯,k), where ¯³k¡1
n0Åe®Å1and n0ande®are
deﬁned in Theorem 3 from [35] . Assume our estimator is Tc¹i
b¾(¢;°¯)given by (13) .
Then with probability at least 1¡±,
¯¯W2(¹i,¹j)2¡cWLOT
2,¾(c¹i,c¹j;°¯)2¯¯·(MÅ2R)³
C"p
6pÅ16nÅ
2
±q
log( k)(1ÅI0(¾,¹i))k¡e®Å1
2(2n0Åe®Å1)ÅRs
2log(2/ ±)
k1
A.
where C is from Theorem A.4 and I 0(¾,¹i)is deﬁned in Theorem 4.5. In this case, ¿2in
Corollary 3.4 is bounded above by the right-hand side of the inequality above.
Proof. Note that the transport plan that we are using for the following proof is °¯. Hence-
forth, we will suppress °¯from the notation cWLOT
2,¾(c¹1,c¹2;°¯) for simplicity.
Using the same reasoning as in Theorem 4.2, we ﬁnd that
³
W2(¹i,¹j)ÅcWLOT
2,¾(c¹i,c¹j)´
·MÅ2R.
Similar to the proof of Theorem 4.2, we bound
¯¯W2(¹i,¹j)¡cWLOT
2,¾(c¹i,c¹j)¯¯·¯¯¯W2(¹i,¹j)¡°°T¹i
¾¡T¹j
¾°°
¾¯¯¯
Å°°T¹i
¾¡Tc¹i
b¾(¢;°¯)°°
¾Å°°T¹j
¾¡Tc¹j
b¾(¢;°¯)°°
¾
Å¯¯¯°°Tc¹i
b¾(¢;°¯)¡Tc¹j
b¾(¢;°¯)°°
¾¡cWLOT
2,¾(c¹i,c¹j)¯¯¯.
The ﬁrst and last term are bounded the same way as in the proof of Theorem 4.2 above.
Since assumption (i) of Assumption 4.1, implies assumption (A1) of Theorem 4.5, we get
that with probability at least 1 ¡±
°°T¹`
¾¡Tc¹`
b¾(¢;°¯)°°
¾·1
±q
log( k)(1ÅI0(¾,¹`))k¡e®Å1
2(2n0Åe®Å1)
for`Æiand`Æj. Putting the bounds together, we get the result. 
Using Theorem 4.2 and Theorem 4.7, we see that as long as ¹iare"-compatible push-
forwards of ¹and the number of samples used in the empirical distribution is large
enough, then our LOT distance is a computationally efﬁcient and a tractable approxi-
mation for the Wasserstein distance and the distortion of the LOT Wassmap embedding
of {¹i} is small with high probability.
5. B OUNDS FOR NON -COMPACTLY SUPPORTED TARGET MEASURES
In the last section, we saw that for compactly supported ¹i»H]¹(as well as a few
other conditions), either the barycentric estimator Tc¹i
¾(¢;°LP) or the entropic estimator
Tc¹i
¾(¢;°¯) will allow for fast yet accurate approximation of the pairwise Wasserstein
distances W2(¹i,¹j), which in turn allows for fast, accurate LOT approximation to the
Wassmap embedding [21] via Algorithm 2. In this section, we show that we can adapt
Theorem 4.2 and Theorem 4.7 to non-compactly supported measures as long as we canLINEARIZED WASSERSTEIN DIMENSIONALITY REDUCTION WITH APPROXIMATION GUARANTEES 19
approximate the non-compactly supported measure with a compactly supported and
absolutely continuous measure. To this end, we use the main theorem of [19].
Theorem 5.1 ([19]) .Let­be a compact convex set and let ¾be a probability density on ­,
bounded from above and below by positive constants. Let p Èn and p ¸4. Assume that
¹,º2W2(Rn)have bounded p-th moment, and max( Mp(¹),Mp(º))·MpÇ 1 . Then
kT¹
¾¡Tº
¾k¾·Cn,p,­,MpW1(¹,º)p
6pÅ16n.
To achieve our purposes, we will assume that ¹is a non-compactly supported mea-
sure that has a suitable tail decay rate, and then show that there exists a compactly sup-
ported absolutely continuous e¹that approximates ¹well (i.e., W1(¹,e¹)Ç´.). We achieve
this in the following lemma.
Lemma 5.2. Fix´È0, and let ¾satisfy the assumptions of Theorem 5.1. Moreover, let
¹2W2(Rn)with density f ¹have a bounded p-th moment for some p Èn and p ¸4.
Finally, assume that there exists some R È0such that for every x 62B(0,R), we have
f¹(x)Ç³´
Cn,p,­,Mp´6pÅ16n
p 1
3CkxknÅ2,
where C denotes the constant from integrating over concentric n-spheres. Then there ex-
ists a compactly supported absolutely continuous measure e¹such that
kT¹
¾¡Te¹
¾k¾Ç´.
The next lemma will be useful in establishing conditions on Hand¹so that our
truncated measure has a density that is bounded away from 0.
Lemma 5.3. Let¾satisfy the assumptions of Theorem 5.1 and let ¹2W2(Rn)with density
f¹·CÇ 1 have a bounded p-th moment for some p Èn and p ¸4. Moreover, assume
that there exists some R È0and´È0such that for x 2B(0,R), we have f ¹(x)¸cÈ0; and
for every x ÝB(0,R), we have
f¹(x)·³´
Cn,p,­,Mp´6pÅ16n
p 1
C0kxknÅ2,
where C n,p,­,Mpcomes from from Theorem 5.1, C0is a constant from integrating over
concentric n-spheres as well as another constant from our approximation method. Then
there exists a compactly supported, absolutely continuous measure e¹with density 0Çc·
b·fe¹·BÇ 1 such that
kT¹
¾¡Te¹
¾k¾Ç´.
The proofs of both Lemma 5.2 and Lemma 5.3 are located in Appendix C. With these
two lemmas above, we obtain the following theorems. Note that Theorem 5.4 replaces
the assumption that ¹is compactly supported with one of polynomial (in the ambient
dimension) tail decay; while the second assumption below is the same as Theorem 4.2,
the ﬁnal assumption differs from that of Theorem 4.2 by requiring the discretizations of
¾and¹ito have the same sample size to apply the lemmas above.
Theorem 5.4. Let±È0. Along with Assumption 4.1, assume that
(i) Every ¹ihas bounded p-th moment for some p Èn and p ¸4. Moreover, assume
that for all i , there exists some R È0such that for every x 62B(0,R), we have
f¹iÇ³´
Cn,p,­,Mp´6pÅ16n
p 1
3CkxknÅ2.20 LINEARIZED WASSERSTEIN DIMENSIONALITY REDUCTION WITH APPROXIMATION GUARANTEES
Deﬁnee¹ito be the truncated measure found in Lemma 5.2 or Lemma 5.3 such that
W1(¹i,e¹i)Ç".
(ii) Te¹i
¾is L-Lipschitz (this happens, e.g., if ¾ande¹iare both compactly supported).
(iii) Given empirical distributions b¾andc¹iwith supp(c¹i)µB(0,R)and sample sizes m
and k, respectively, let our estimator be the barycentric estimator (10) , with °LP.
Then with probability at least 1¡±,
¯¯W2(¹i,¹j)2¡cWLOT
2,¾(c¹i,c¹j;°LP)2¯¯·(MÅ2R)³
C"p
6pÅ16nÅ2´Å2Op(r(k)
nlog(1Åk)tn,®)
ÅRs
2log(2/ ±)
m1
A,
where r(k)
nand t n,®are deﬁned in Theorem 4.2 and C is a constant coming from Theo-
rem A.4. In this case, ¿2of Corollary 3.4 is bounded above by the right-hand side of the
inequality above.
Similarly for the entropic map case we have the following. Note that the primary
difference in assumption between Theorem 5.5 and Theorem 5.4 is the addition of (A1)–
(A3) from Theorem 4.5 and the asymptotic assumption on the regularization parameter
for the entropic map. The assumptions (i) and (ii) below are essentially the same as
those of Theorem 4.7, but with b¹ireplaced with e¹iarising from Theorem 5.4, whereas
the additional assumptions below are that ¹ihave decaying tails as opposed to being
compactly supported.
Theorem 5.5. Let±È0. Along with Assumption 4.1 and (i) of Theorem 5.4, assume that
(i)¾ande¹isatisfy assumptions (A1)–(A3) in 4.5 for all i , where e¹iis the truncated
measure from Theorem 5.4.
(ii) Given empirical distributions b¾andc¹iwith supp(c¹i)µB(0,R)and sample size k
for both, assume that we have associated entropic potentials (f¯,k,g¯,k), where ¯³
k¡1
n0Åe®Å1and n0ande®are deﬁned in Theorem 4.5. Moreover, assume our estimator
is given by (13) .
Then with probability at least 1¡±,
¯¯W2(¹i,¹j)2¡cWLOT
2,¾(c¹i,c¹j)2¯¯·(MÅ2R)³
C"p
6pÅ16nÅ2´Å
2
±q
log( k)(1ÅI0(¾,¹i))k¡e®Å1
2(2n0Åe®Å1)ÅRs
2log(2/ ±)
k1
A,
where I 0(¾,¹i)is deﬁned in Theorem 4.5 and C is a constant from Theorem A.4. In this
case,¿2of Corollary 3.4 is bounded above by the right-hand side of the inequality above.
The following is a proof for both theorems above.
Proof of Theorems 5.4 and 5.5. In the following, we let Tc¹i
¾denote the optimal transport
map estimator that we are considering (either the barycentric estimator with °LPor the
entropic estimator with °¯) since the same proof works for both cases. The only dif-
ference in the compactly supported case and these theorems is that our approximation
now becomes
¯¯W2(¹i,¹j)¡cWLOT
2,¾(c¹i,c¹j)¯¯·¯¯¯W2(¹i,¹j)¡kT¹i
¾¡T¹j
¾k¾¯¯¯LINEARIZED WASSERSTEIN DIMENSIONALITY REDUCTION WITH APPROXIMATION GUARANTEES 21
Å¯¯¯kT¹i
¾¡T¹j
¾k¾¡kTf¹i
¾¡Tf¹j
¾k¾¯¯¯
Å¯¯¯kTf¹i
¾¡Tf¹j
¾k¾¡kTc¹i
¾¡Tc¹j
¾k¾¯¯¯
Å¯¯¯kTc¹i
¾¡Tc¹j
¾k¾¡cWLOT
2,¾(c¹i,c¹j)¯¯¯,
wheree¹iis deﬁned as in the theorem statement and c¹idenotes the empirical measure
of¹i. Since we assume that supp( c¹i)µB(0,R), we know that c¹ican equivalently be
thought of as being sampled from e¹irather than ¹i. This means that the same bounds
as before hold for most of the terms, while additionally,
¯¯¯kT¹i
¾¡T¹j
¾k¾¡kTf¹i
¾¡Tf¹j
¾k¾¯¯¯· kT¹i
¾¡Te¹i
¾k¾|{z}
·´ÅkT¹j
¾¡Te¹i
¾k¾|{z}
·´·2´.
The rest of the terms are bounded the same exact way as before, and the result follows.

In this section, we have shown that results for the case when the ¹iare compactly
supported can be extended to non-compactly supported ¹ias long as their densities
decay fast enough and the reference distribution ¾has a compact and convex support.
6. C ONDITIONS ON H AND¹(COMPACT CASE )
In this section, we derive conditions on Hand¹so that the assumptions of the the-
orems above are satisﬁed for ¹i»H]¹. In particular, we can break down our require-
ments on Hand¹by noting the necessary conditions on ¹ifor the barycentric map
estimator and entropic map estimator separately. For simplicity, we will assume that H
is exactly compatible with respect to ¾and¹.
Theorem 6.1 (Barycentric Map Case (Compact)) .Along with Assumption 4.1 (with "Æ0
so that every h 2His exactly compatible with ¾and¹), assume that ¹i»H]¹i.i.d. and
that
(i)¹is compactly supported,
(ii)¾is chosen such that T¹
¾is Lipschitz,
Then ¹isatisﬁes the conditions of Theorem 4.2, i.e., each ¹iis compactly supported and
T¹i
¾is Lipschitz.
For the entropic case, the assumptions on ¹and¾are the same, but we require an
additional assumption regarding the Jacobian of elements of H.
Theorem 6.2 (Entropic Map Case (Compact)) .Under the assumptions of Theorem 6.1,
as well as
(iv)¾and¹satisfy (A1)-(A3),
¹isatisﬁes the conditions of Theorem 4.7.
The proofs of both Theorems 6.1 and 6.2 are given in Appendix D.1.
7. C ONDITIONS ON H AND¹(NON-COMPACT CASE )
For the non-compactly supported cases, we need to add assumptions that His closed
under inversion as well as lower and upper boundedness of the density f¹. This gives us
the following theorems.22 LINEARIZED WASSERSTEIN DIMENSIONALITY REDUCTION WITH APPROXIMATION GUARANTEES
Theorem 7.1 (Barycentric Map Case (Non-Compact)) .Along with Assumption 4.1 (with
"Æ0so that every h 2His exactly compatible with ¾and¹), assume that ¹i»H]¹i.i.d.
Assume further that
(i) for every h 2H, there exists an inverse h¡12H.
(ii) The density of ¹is supported on all of Rnwith f ¹(x)·CÇ 1 for all x, and f ¹(x)¸
cÈ0for all x 2B(0,RL). Moreover, f ¹has a decay rate as in Lemma 5.3 for x 62
B(0,R).
Then ¹isatisﬁes the conditions of Theorem 5.4.
Theorem 7.2 (Entropic Map Case (Non-Compact)) .Assume that ¹i»H]¹i.i.d. and
that¹,H, and ¾satisfy the conditions of Theorem 7.1. Then ¹isatisﬁes the conditions of
Theorem 5.5.
The proofs of both Theorems 7.1 and 7.2 are found in Appendix D.2.
8. E XPERIMENTS
We demonstrate that Algorithm 2 does in fact attain correct embeddings given ﬁnite
sampling and without explicitly computing the pairwise Wasserstein distances. We test
both variants of our algorithm above using the linear program or entropic regularization
to compute the transport maps from the data to the reference measure, and illustrate
the quality of embeddings as well as the relative embedding error
min
QkY¡QXkF
kYkF
as a function of the sample size mof the data and reference measures.
In all experiments, we generate Ndata measures, ¹i, which are Gaussians of vari-
ous means and covariance, and a ﬁxed reference measure ¾drawn from the standard
normal distribution N(0,I). We randomly sample mpoints from each measure to form
the empirical measure, and random noise from a Wishart distribution is added to the
covariance matrices of the data measures ¹i. Additionally, in each experiment we com-
pute the optimal rotation of the embeddings to properly align them with the true em-
bedding and thus give an accurate error estimate for each trial.
For each experiment, we provide a ﬁgure for qualitative assessment of the embed-
ding as well as a quantitative ﬁgure in which we compute the relative error as above for
the embeddings as a function of m, the sample size used to generate the empirical data
and reference measures. For the latter ﬁgures, we run 10 trials of the embedding and av-
erage the relative error; error bands showing one standard deviation are shown on each
ﬁgure. A jupyter notebook containing all of the experiments that generate the ﬁgures
below can be found at https://github.com/varunkhuran/LOTWassMap .
8.1. Experiment 1: circle translation manifold. First, we consider a 1-dimensional
manifold of translations as follows. We uniformly choose NÆ10 points on the circle of
radius 8, which we denote xi, and each data measure ¹iis a Gaussian with mean xiand
covariance matrix·1¡.5
¡.5 1¸
. Thus, our data set is a set of Gaussians translated around
the circle. The Wishart noise added to the covariance matrix prior to sampling the ¹iis
of the form GG>where Ghas i.i.d. N(0,0.5) entries. We choose the standard normal
distribution N(0,I) as our reference measure ¾. We randomly sample mÆ1000 points
from each data measure and the reference measure independently. Figure 1 shows the
original sampled data and the reference measure (in blue), the true embedding pointsLINEARIZED WASSERSTEIN DIMENSIONALITY REDUCTION WITH APPROXIMATION GUARANTEES 23
xi, and the embeddings of Algorithm 2 when using the linear program and Sinkhorn
with regularization parameter ¸Æ1.
One can easily see that the embeddings are qualitatively good as expected given the
theory above and the results of [21] in similar experiments. Figure 2 shows the relative
error vs. sampling size mof the measures, and one can see the good performance for
modest sample sizes.
FIGURE 1. 1-D Manifold of translations: (Left) reference measure ¾»
N(0,I) in blue and data measures ¹iwhich are Gaussians with the
same covariance matrix and means xiuniformly sampled from the cir-
cle of radius 8. (Left Middle) Means xiof¹iwhich are the true embed-
ding points. (Right Middle) Embedding attained with Algorithm 2 us-
ing the linear program. (Right) Embedding attained with Algorithm 2
using the Sinkhorn distance with ¸Æ1.
FIGURE 2. Embedding error vs. m(number of sample points from
data and reference distributions for the 1-D translation manifold. Op-
timal transport maps are computed via the Linear Program (Left) and
Sinkhorn with ¸Æ1(Right) .
8.2. Experiment 2: rotation manifold. Next, we consider a 1-dimensional rotation man-
ifold in which we generate NÆ10 data measures of Gaussians whose means lie at uni-
form samples of the circle of radius 8, which we denote (8cos µi,8sin µi), and whose
covariance matrices are rotations of·2 0
0 .5¸
by the angles µi. As in experiment 1, the
noise level added is 0.5 and we sample mÆ1000 points from each measure. Figure 3
shows the data measures, true embedding, and embeddings from Algorithm 2 using
both the linear program and Sinkhorn (with ¸Æ1) to compute the optimal transport
maps. Figure 4 shows the relative error vs. sample size.24 LINEARIZED WASSERSTEIN DIMENSIONALITY REDUCTION WITH APPROXIMATION GUARANTEES
FIGURE 3. 1-D Manifold of rotations: (Left) reference measure ¾»
N(0,I) in blue and data measures ¹iwhich are Gaussians with means
lying on the circle of radiu 8 and covariance matrices that are rotations
of each other. (Left Middle) Means xiof¹iwhich are the true embed-
ding points. (Right Middle) Embedding attained with Algorithm 2 us-
ing the linear program. (Right) Embedding attained with Algorithm 2
using the Sinkhorn distance with ¸Æ1.
FIGURE 4. Embedding error vs. m(number of sample points from
data and reference distributions for the 1-D rotation manifold. Op-
timal transport maps are computed via the Linear Program (Left) and
Sinkhorn with ¸Æ1(Right) .
8.3. Experiment 3: grid translation manifold. Here, we consider a 2-dimensional trans-
lation manifold in which we generate NÆ25 data measures of Gaussians whose means
lie on a 5 £5 uniform grid on the cube [ ¡10,10]2and which have constant covariance
matrix·1¡.5
¡.5 1¸
. We sample mÆ1000 points from each measure and the noise level
is again 0.5. In the Sinkhorn embedding, we use regularization ¸Æ10. Figures 5 and 6
show the data, embeddings, and relative error vs. sample size.
8.4. Experiment 4: Dilation manifold. Here, we consider a 2-dimensional anisotropic
dilation manifold in which we generate NÆ9 data measures of Gaussians with mean 0
and anisotropically scaled covariance matrices of the form diag( ®2
i,¯2
i) for (®i,¯i) taken
from a uniform 3 £3 grid on [1,4]2. We sample mÆ1000 points from the reference mea-
sure and nÆ2500 points from the data measures and the noise level added to the co-
variance matrices is 0.5 as before. In the Sinkhorn embedding, we use regularization
¸Æ100. Figure 7 show the data measures, true embedding parameters, and embed-
dings from Algorithm 2. Note that the true embedding parameters are centered to allow
them to be comparable to the output of Algorithm 2 which are naturally centered.
Figure 8 shows the relative error vs. m, and for this experiment we choose nÆm
so that the sampling order of the data and reference measure are the same. For thisLINEARIZED WASSERSTEIN DIMENSIONALITY REDUCTION WITH APPROXIMATION GUARANTEES 25
FIGURE 5. 2-D Manifold of translations: (Left) data measures ¹i
which are Gaussians with the same covariance matrix and means xi
taken from a 5 £5 uniform grid on [ ¡10,10]2.(Left Middle) Means xi
of¹iwhich are the true embedding points. (Right Middle) Embed-
ding attained with Algorithm 2 using the linear program. (Right) Em-
bedding attained with Algorithm 2 using the Sinkhorn distance with
¸Æ10.
FIGURE 6. Embedding error vs. m(number of sample points from
data and reference distributions for the 2-D translation manifold. Op-
timal transport maps are computed via the Linear Program (Left) and
Sinkhorn with ¸Æ10(Right) .
case, we see that the relative error of the embedding decays much more slowly than the
previous experiments. One possible reason for this is that there is signiﬁcant overlap in
the distributions for the dilated measures, and to overcome this issue one may have to
sample many more points in forming the empirical distribution so that the tails of the
data measures are sampled more frequently.
8.5. Experiment 5: Time Comparison. Here, we repeat Experiment 3 in which data
measures are centered on a uniform grid and are translations of a ﬁxed Gaussian mea-
sure. We plot the time it takes to compute the embedding via Algorithm 2 using the Lin-
ear Program or Sinkhorn with ¸Æ1 and the Wassmap algorithm of [21] which requires
computing the entire square Wasserstein distance matrix [ W2(¹i,¹j)]N
i,jÆ1and the SVD
of its centered version as in Algorithm 1. For this experiment, we always choose nÆm
so that the reference measure and data measure sampling rates are the same. One can
easily see that a substantial gain in timing is achieved by LOT Wassmap, while previ-
ous experiments show that the quality of the embedding does not degrade signiﬁcantly
when LOT is used.
Finally, we plot the timing for the same experiment for the Linear Program and Sinkhorn
with¸Æ1 and ¸Æ10 for larger sample sizes to illustrate the character of these choices26 LINEARIZED WASSERSTEIN DIMENSIONALITY REDUCTION WITH APPROXIMATION GUARANTEES
FIGURE 7. 2-D Manifold of Anisotropic Dilations: (Left) data mea-
sures ¹iwhich are Gaussians with mean 0 and anisotropically dilated
covariance matrices where dilations are taken from a 3 £3 uniform
grid on [1,4]2.(Left Middle) Dilation factors ( xi,yi) of¹iwhich are the
true embedding points. (Right Middle) Embedding attained with Al-
gorithm 2 using the linear program. (Right) Embedding attained with
Algorithm 2 using the Sinkhorn distance with ¸Æ100.
FIGURE 8. Embedding error vs. m(number of sample points from
data and reference distributions for the 2-D translation manifold. Op-
timal transport maps are computed via the Linear Program (Left) and
Sinkhorn with ¸Æ10(Right) .
(Figure 10). As expected, larger regularization parameter yields faster computation time,
though the difference is relatively small even for modestly large sample size.
ACKNOWLEDGEMENTS
K.H. acknowledges support from the UTA Research Enhancement Program and the
Fields Institute for Research in Mathematical Sciences. C.M. is supported by NSF award
DMS-2306064. A.C. is partially supported by NSF award DMS-2012266 and a gift from
Intel research. K.H. and A.C. thank the Fields Institute and participants of the Focus
Program on Data Science, Approximation Theory, and Harmonic Analysis for their hos-
pitality, which facilitated the initial discussions of this research.
REFERENCES
[1] Akram Aldroubi, Shiying Li, and Gustavo K Rohde. Partitioning signal classes using transport transforms
for data analysis and machine learning. Sampling Theory, Signal Processing, and Data Analysis , 19(1):1–
25, 2021.
[2] Jason Altschuler, Jonathan Weed, and Philippe Rigollet. Near-linear time approximation algorithms for
optimal transport via Sinkhorn iteration. Advances in Neural Information Processing Systems , 2017-
December:1965–1975, 2017.LINEARIZED WASSERSTEIN DIMENSIONALITY REDUCTION WITH APPROXIMATION GUARANTEES 27
FIGURE 9. Timing vs. sample size mof the reference distribution and
data measures. The data set consists of NÆ25 measures translated on
a 5£5 uniform grid on [ ¡10,10]2as in Experiment 3. Shown are the
computation times to compute the Wassmap embedding and the em-
beddings of Algorithm 2 using the Linear Program (LP) and Sinkhorn
with regularization parameter ¸Æ1.
FIGURE 10. Timing vs. sample size mof the reference distribution and
data measures. The data set consists of NÆ25 measures translated on
a 5£5 uniform grid on [ ¡10,10]2as in Experiment 3. Shown are the
computation times to compute the embeddings of Algorithm 2 using
the Linear Program (LP) and Sinkhorn with regularization parameters
¸Æ1 and ¸Æ10.
[3] Luigi Ambrosio and Nicola Gigli. A user’s guide to optimal transport. In Modelling and optimisation of
ﬂows on networks , pages 1–155. Springer, 2013.
[4] Luigi Ambrosio, Nicola Gigli, and Giuseppe Savaré. Gradient Flows: in Metric Spaces and in the Space of
Probability Measures . Springer Science & Business Media, 2008.
[5] Ery Arias-Castro, Adel Javanmard, and Bruno Pelletier. Perturbation bounds for procrustes, classical scal-
ing, and trilateration, with applications to manifold learning. Journal of machine learning research , 21,
2020.
[6] Martin Arjovsky, Soumith Chintala, and Léon Bottou. Wasserstein generative adversarial networks. In
Doina Precup and Yee Whye Teh, editors, Proceedings of Machine Learning Research , volume 70, pages
214–223. PMLR, 2017.28 LINEARIZED WASSERSTEIN DIMENSIONALITY REDUCTION WITH APPROXIMATION GUARANTEES
[7] Saurav Basu, Soheil Kolouri, and Gustavo K. Rohde. Detecting and visualizing cell phenotype differences
from microscopy images using transport-based morphometry. Proceedings of the National Academy of
Sciences , 111(9):3448–3453, 2014.
[8] Mikhail Belkin and Partha Niyogi. Laplacian eigenmaps for dimensionality reduction and data represen-
tation. Neural Computation , 15(6):1373–1396, 2003.
[9] R.J. Berman. Convergence rates for discretized Monge–Ampère equations and quantitative stability of
optimal transport. Found Comput Math , 21:1099–1140, 2021.
[10] Yann Brenier. Polar factorization and monotone rearrangement of vector-valued functions. Communi-
cations on pure and applied mathematics , 44(4):375–417, 1991.
[11] Luis A. Caffarelli. Boundary regularity of maps with convex potentials. Communications on Pure and
Applied Mathematics , 45(9):1141–1151, 1992.
[12] Luis A. Caffarelli. The regularity of mappings with a convex potential. Journal of the American Mathemat-
ical Society , 5(1):99–104, 1992.
[13] Luis A. Caffarelli. Boundary regularity of maps with convex potentials–II. Annals of Mathematics ,
144(3):453–496, 1996.
[14] Yongxin Chen, Filemon Dela Cruz, Romeil Sandhu, Andrew L. Kung, Prabhjot Mundi, Joseph O. Deasy,
and Allen Tannenbaum. Pediatric sarcoma data forms a unique cluster measured via the earth mover’s
distance. Scientiﬁc Reports , 7(1):7035, 2017.
[15] Ronald R Coifman and Stéphane Lafon. Diffusion maps. Applied and Computational Harmonic Analysis ,
21(1):5–30, 2006.
[16] Michael AA Cox and Trevor F Cox. Multidimensional scaling. In Handbook of data visualization , pages
315–347. Springer, 2008.
[17] Marco Cuturi. Sinkhorn distances: lightspeed computation of optimal transport. In NIPS , volume 2,
page 4, 2013.
[18] Nabarun Deb, Promit Ghosal, and Bodhisattva Sen. Rates of estimation of optimal transport maps us-
ing plug-in estimators via barycentric projections. Advances in Neural Information Processing Systems ,
34:29736–29753, 2021.
[19] Alex Delalande and Quentin Mérigot. Quantitative stability of optimal transport maps under variations
of the target measure. arXiv preprint arXiv:2103.05934 , 2021.
[20] Nicola Gigli. On Hölder continuity-in-time of the optimal transport map towards measures along a curve.
Proceedings of the Edinburgh Mathematical Society , 54(2):401–409, 2011.
[21] Keaton Hamm, Nick Henscheid, and Shujie Kang. Wassmap: Wasserstein isometric mapping for image
manifold learning. arXiv preprint arXiv:2204.06645 , 2022.
[22] Jean-Baptiste Hiriart-Urruty and Claude Lemaréchal. Convex analysis and minimization algorithms II:
Advanced theory and bundle methods . Springer Verlag, 1996.
[23] James M Joyce. Kullback-leibler divergence. In International encyclopedia of statistical science , pages 720–
722. Springer, 2011.
[24] Marc Khoury, Yifan Hu, Shankar Krishnan, and Carlos Scheidegger. Drawing large graphs by low-rank
stress majorization. In Computer Graphics Forum , volume 31, pages 975–984. Wiley Online Library, 2012.
[25] Varun Khurana, Harish Kannan, Alexander Cloninger, and Caroline Moosmüller. Supervised learning of
sheared distributions using linearized optimal transport. Sampling Theory, Signal Processing, and Data
Analysis , 21(1), 2023.
[26] Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-SNE. Journal of Machine Learning
Research , 9(11):2579–2605, 2008.
[27] Kantilal Varichand Mardia. Multivariate analysis. Technical report, 1979.
[28] James Mathews, Maryam Pouryahya, Caroline Moosmüller, Ioannis G. Kevrekidis, Joseph O. Deasy, and
Allen Tannenbaum. Molecular phenotyping using networks, diffusion, and topology: soft-tissue sar-
coma. Scientiﬁc Reports , 9, 2019. Article number: 13982.
[29] Quentin Mérigot, Alex Delalande, and Frédéric Chazal. Quantitative stability of optimal transport maps
and linearization of the 2-Wasserstein space. In Silvia Chiappa and Roberto Calandra, editors, Proceed-
ings of the Twenty Third International Conference on Artiﬁcial Intelligence and Statistics , volume 108 of
Proceedings of Machine Learning Research , pages 3186–3196. PMLR, 26–28 Aug 2020.
[30] Jacob Miller, Vahan Huroyan, and Stephen Kobourov. Spherical graph drawing by multi-dimensional
scaling. arXiv preprint arXiv:2209.00191 , 2022.
[31] Gal Mishne, Ronen Talmon, Ron Meir, Jackie Schiller, Maria Lavzin, Uri Dubin, and Ronald R Coifman.
Hierarchical coupled-geometry analysis for neuronal structure and activity pattern discovery. IEEE Jour-
nal of Selected Topics in Signal Processing , 10(7):1238–1253, 2016.LINEARIZED WASSERSTEIN DIMENSIONALITY REDUCTION WITH APPROXIMATION GUARANTEES 29
[32] Caroline Moosmüller and Alexander Cloninger. Linear optimal transport embedding: Provable Wasser-
stein classiﬁcation for certain rigid transformations and perturbations. Information and Inference: A
Journal of the IMA , 12(1):363–389, 2023.
[33] Marshall Mueller, Shuchin Aeron, James M Murphy, and Abiy Tasissa. Geometric sparse coding in
Wasserstein space. arXiv preprint arXiv:2210.12135 , 2022.
[34] Se Rim Park, Soheil Kolouri, Shinjini Kundu, and Gustavo K. Rohde. The cumulative distribution trans-
form and linear pattern classiﬁcation. Applied and Computational Harmonic Analysis , 45(3):616 – 641,
2018.
[35] Aram-Alexandre Pooladian and Jonathan Niles-Weed. Entropic estimation of optimal transport maps.
arXiv:2109.12004 , 2021.
[36] Yossi Rubner, Carlo Tomasi, and Leonidas J Guibas. The earth mover’s distance as a metric for image
retrieval. International journal of computer vision , 40(2):99–121, 2000.
[37] Justin Solomon, Raif Rustamov, Leonidas Guibas, and Adrian Butscher. Wasserstein propagation for
semi-supervised learning. In International Conference on Machine Learning , pages 306–314, 2014.
[38] Joshua B Tenenbaum, Vin De Silva, and John C Langford. A global geometric framework for nonlinear
dimensionality reduction. Science , 290(5500):2319–2323, 2000.
[39] Cédric Villani. Optimal Transport: Old and New , volume 338. Springer Science & Business Media, 2008.
[40] Wei Wang, John A Ozolek, Dejan Slepˇ cev, Ann B Lee, Cheng Chen, and Gustavo K Rohde. An optimal
transportation approach for nuclear structure-based pathology. IEEE transactions on medical imaging ,
30(3):621–631, 2010.
[41] Wei Wang, Dejan Slepˇ cev, Saurav Basu, John A. Ozolek, and Gustavo K. Rohde. A linear optimal trans-
portation framework for quantifying and visualizing variations in sets of images. Int J Comput Vis ,
101:254–269, 2013.
[42] M. E. Werenski, R. Jiang, A. Tasissa, S. Aeron, and J. M. Murphy. Measure estimation in the barycentric
coding model. In Proceedings of the 39 th International Conference on Machine Learning , pages 23781–
23803. PMLR, 2022.
[43] Gale Young and Aiston S Householder. Discussion of a set of points in terms of their mutual distances.
Psychometrika , 3(1):19–22, 1938.
[44] Nathan Zelesko, Amit Moscovich, Joe Kileel, and Amit Singer. Earthmover-based manifold learning for
analyzing molecular conformation spaces. In 2020 IEEE 17th International Symposium on Biomedical
Imaging (ISBI) , pages 1715–1719, 2020.
[45] Yin Zhang, Rong Jin, and Zhi-Hua Zhou. Understanding bag-of-words model: a statistical framework.
International Journal of Machine Learning and Cybernetics , 1(1-4):43–52, 2010.
APPENDIX A. H ELPER THEOREMS AND LEMMAS
We use the following lemma to extend Corollary 3.2 to get our main theorem (Theo-
rem 3.3). The proof follows standard arguments, e.g., as in [27]; the proof is included for
completeness.
Lemma A.1 ([27, Theorem 14.2.1], for example) .Consider a matrix V whose columns are
centered vectors v 1,..., vnsuch thatPn
jÆ1vjÆ0. Let J ÆI¡1
n11>be the centering matrix
from MDS (Algorithm 1), G ÆV>V be the Gram matrix for V , and D be the squared
distance matrix D i jÆ kvi¡vjk2. Then G Æ ¡1
2JD J.
Proof. Note ﬁrst that
(JD J )i jÆDi jÅ1
n2nX
k,`Æ1Dk`¡1
nnX
kÆ1(Di kÅDk j).
Moreover, because Di jÆv>
iviÅv>
jvj¡2v>
ivj, we get that
(JD J )i jÆv>
iviÅv>
jvj¡2v>
ivjÅ1
n2³
2nnX
kÆ1v>
kvkÅ21>V>V1´
¡1
n³
nv>
iviÅnv>
jvjÅ2nX
kÆ1v>
kvk¡21>V>vj¡2v>
iV1´
.30 LINEARIZED WASSERSTEIN DIMENSIONALITY REDUCTION WITH APPROXIMATION GUARANTEES
Note here that V1Æ0 sincePn
jÆ1vjÆ0. After cancelling terms, we get
(JD J )i jÆ ¡2v>
ivjÆ ¡2Gi j.
So our result is immediate. 
The next results are used to recount the "-compatibility as well as its effects on LOT.
First, we show that every "-compatible map has a compatible map (with "Æ0) nearby
whose LOT distance from the "-compatible map is small.
Lemma A.2. Assume that
(i)¾is supported on a compact convex set ­½Rnwith probability density f ¾bounded
above and below by positive constants.
(ii)¹has ﬁnite p-th moment with bound M pwith p Èd and p ¸4.
(iii) There exist a ,AÈ0such that every h 2Hsatisﬁes a kxk · k h(x)k ·Akxk.
LetHbe"-compatible with respect to ¾and¹. Then for every h 2Hthere exists a
compatible g such that
°°°Tg]¹
¾¡Th]¹
¾°°°
¾·Cn,p,­,a¡1ApMp¢"p
6pÅ16n
kh±T¹
¾¡Th]¹
¾k¾Ç"ÅCn,p,­,a¡1ApMp¢"p
6pÅ16n.
Proof. Let h2H, then there exists an exactly compatible transformation gsuch that
g±T¹
¾ÆTg]¹
¾withkh¡gk¹Ç"by deﬁnition of "-compatibility. Then notice that
°°°h±T¹
¾¡Th]¹
¾°°°
¾Æ°°°h±T¹
¾¡g±T¹
¾ÅTg]¹
¾¡Th]¹
¾°°°
¾
· kh¡gk¹Å°°°Tg]¹
¾¡Th]¹
¾°°°
¾.
By assumption, we know that kh¡gk¹Ç". Since h2Hand are Lipschitz, we know that
Z
­kxkpfh]¹(x)d xÆZ
­kh(x)kp
|{z}
·ApkxkpjJh¡1(x)j|{z}
a¡1f¹(x)d x·a¡1ApMp.
Similarly, we have the same bound for gsince g2H. Now using Theorem 5.1 and
equation 2.1 of [3], we get that
°°°Tg]¹
¾¡Th]¹
¾°°°
¾·Cn,p,­,a¡1ApMpW1(g]¹,h]¹)p
6pÅ16n
·Cn,p,­,a¡1ApMpW2(g]¹,h]¹)p
6pÅ16n
·Cn,p,­,a¡1ApMpkh¡gkp
6pÅ16n
¹
·Cn,p,­,a¡1ApMp¢"p
6pÅ16n.
This implies that
kh±T¹
¾¡Th]¹
¾k¾Ç"ÅCn,p,­,a¡1ApMp¢"p
6pÅ16n.

Now we can show that the LOT embedding between exactly compatible transforma-
tions is isometric with the Wasserstein manifold.LINEARIZED WASSERSTEIN DIMENSIONALITY REDUCTION WITH APPROXIMATION GUARANTEES 31
Lemma A.3. Let g 1and g 2be exactly compatible transformations, i.e. g 1±T¹
¾ÆT(g1)]¹
¾
and g 2±T¹
¾ÆT(g2)]¹
¾ , then
°°°T(g1)]¹
¾¡T(g2)]¹
¾°°°
¾ÆW2³
(g1)]¹,(g2)]¹´
.
Proof. First notice that since everything is absolutely continuous, we can use a change
of variables formula to get
°°°T(g1)]¹
¾¡T(g2)]¹
¾°°°
¾Æ°°°I¡T(g2)]¹
¾±T¾
(g1)]¹°°°
(g1)]¹.
Because T(g2)]¹
(g1)]¹is the minimizer of the optimal transport problem and the triangle in-
equality, we get
W2³
(g1)]¹,(g2)]¹´
Æ°°°I¡T(g2)]¹
(g1)]¹°°°
(g1)]¹·°°°I¡T(g2)]¹
¾±T¾
(g1)]¹°°°
(g1)]¹
·°°°I¡T(g2)]¹
(g1)]¹°°°
(g1)]¹Å°°°T(g2)]¹
(g1)]¹¡T(g2)]¹
¾±T¾
(g1)]¹°°°
(g1)]¹.
Note that Theorem 24 of [25] implies that given an exactly compatible transformation
g,Jg(T¹
¾(x)) must share the same eigenspaces as JT¹
¾(x). By Corollary 4 of [25], we know
that exactly compatible transformations are optimal transport maps themselves. This
means that Tg]¹
¹Ægfor exactly compatible transport maps. Moreover, for an exactly
compatible h02H, this means that T(g0)]¹
g]¹Æg0±g¡1because g0±g¡1is a gradient of a
convex function (since the Jacobian of gand g0share the same eigenspaces) that pushes
g]¹to (g0)]¹. In the context of g1and g2, this gives us that
T(g2)]¹
(g1)]¹Æg1±g¡1
2Æg1±T¹
¾±T¾
¹±g¡1
2ÆT(g2)]¹
¾±T¾
(g1)]¹.
In particular, we get that
°°°T(g1)]¹
¾¡T(g2)]¹
¾°°°
¾ÆW2³
(g1)]¹,(g2)]¹´
.

Finally, we show that "-compatible transformations have LOT embeddings that are
“"p
6pÅ16n-isometric" in the sense of the following theorem.
Theorem A.4. Assume that
(i)¾is supported on a compact convex set ­½Rnwith probability density f ¾bounded
above and below by positive constants.
(ii)¹has ﬁnite p-th moment with bound M pwith p Èn and p ¸4.
(iii) There exists constants a ,AÈ0such that Every h 2Hsatisﬁes a kxk · k h(x)k ·Akxk.
LetHbe"-compatible with respect to absolutely continuous measures ¾and¹and that
h]¹is absolutely continuous. Then for h 1,h22H,
¯¯¯¯W2³
(h1)]¹,(h2)]¹´
¡°°°T(h1)]¹
¾¡T(h2)]¹
¾°°°
¾¯¯¯¯Ç2³
"ÅCn,p,­,a¡1ApMp¢"p
6pÅ16n´
ÇC"p
6pÅ16n
Proof. By deﬁnition, we know that there exist g1and g2such that kg1¡h1k¹Ç"and
kg2¡h2k¹Ç". First, note that
°°°T(h1)]¹
¾¡T(h2)]¹
¾°°°
¾·°°°T(h1)]¹
¾¡T(g1)]¹
¾°°°
¾Å°°°T(g1)]¹
¾¡T(g2)]¹
¾°°°
¾Å°°°T(g2)]¹
¾¡T(h2)]¹
¾°°°
¾.32 LINEARIZED WASSERSTEIN DIMENSIONALITY REDUCTION WITH APPROXIMATION GUARANTEES
By Lemma A.3, we know that
°°°T(g1)]¹
¾¡T(g2)]¹
¾°°°
¾ÆW2³
(g1)]¹,(g2)]¹´
.
However, by equation 2.1 of [3] and the triangle inequality, we have
W2³
(g1)]¹,(g2)]¹´
·W2³
(g1)]¹,(h1)]¹´
|{z }
·kg1¡h1k¹Ç"ÅW2³
(h1)]¹,(h2)]¹´
ÅW2³
(h2)]¹,(g2)]¹´
|{z }
·kh2¡g2k¹Ç"
·W2³
(h1)]¹,(h2)]¹´
Å2".
Moreover, by Lemma A.2, for iÆ1,2, we know that
°°°T(gi)]¹
¾¡T(hi)]¹
¾°°°
¾·Cn,p,­,a¡1ApMp¢"p
6pÅ16n.
This implies that
W2³
(h1)]¹,(h2)]¹´
·°°°T(h1)]¹
¾¡T(h2)]¹
¾°°°
¾
·W2³
(h1)]¹,(h2)]¹´
Å2³
"ÅCn,p,­,a¡1ApMp"p
6pÅ16n´
,
and the proof is complete. 
APPENDIX B. P LUG -IN ESTIMATOR APPROXIMATION RESULTS
In this section, we provide some auxiliary results that are used along the way to prove
the theorems of Section 4.
B.1. Using the Linear Program to compute transport maps. Recall that for a random
variable Xm, we say that XmÆOp(am) if for every "È0 there exists MÈ0 and NÈ0
such that
P³
jXm/amj ÈM´
Ç"8m¸N.
The following theorem from [18] is used in the proofs of our main results, including
Theorem 4.2.
Theorem B.1 ([18, Theorem 2.2]) .Suppose that T¹
¾is L-Lipschitz, and ¹is compactly
supported and E¾[exp( tkxk®)]Ç 1 for some t È0,®È0. Assume we draw k i.i.d. samples
from¹and consider the estimator b¹. Then
sup
°2¡minZ
kTb¹
¾(x;°LP)¡T¹
¾(x)k2d¾(x)·Op(r(k)
nlog(1Åk)tn,®),
where
r(k)
nÆ8
><
>:2k¡1/2nÆ2,3
2k¡1/2log(1Åk)nÆ4
2k¡2/dn¸5,tn,®Æ8
><
>:(4®)¡1(4Å((2®Å2n®¡n)_0)) nÇ4
(®¡1_7/2)¡1 nÆ4
2(1Ån¡1) nÈ4,
so that r(k)
nand t n,®are on the order of k¡1/nand 2(1Ån¡1), respectively.
Remark B.2. We note that Theorem B.1 is the “semi-discrete” version described in [18].
The paper also provides equivalent bounds in the instance that ¾is similarly estimated.
However, the bounds only guarantee that the transport maps agree when integrated
againstb¾, whereas we need the bound for ¾itself.LINEARIZED WASSERSTEIN DIMENSIONALITY REDUCTION WITH APPROXIMATION GUARANTEES 33
B.2. Approximating with Finite Samples from the Reference Distribution. Some of
the norms from Theorem 4.2 and Theorem 4.7 are assumed to be integrated against
the true ¾. However, we need to consider the discretized ¾for each norm, and estab-
lish that we can estimate these norms with high probability. For these bounds, we use
McDiarmid’s inequality on the function
f(X1,...,Xm)Æ1
mmX
jÆ1¯¯Tc¹1
¾(Xj;°c¹1)¡Tc¹2
¾(Xj;°c¹2)¯¯2ÆcWLOT
2,¾(c¹1,c¹2;°)2,
where Xj»¾,°c¹jis a transport plan between ¾andc¹jfor jÆ1,2, and °2{°LP,°¯}
denotes the optimization method used to get °c¹j. If¹iare supported in a ball of radius
R, then McDiarmid’s inequality implies
PÃ¯¯¯¯¯1
mmX
jÆ1jTc¹1
¾(Xj;°c¹1)¡Tc¹2
¾(Xj;°c¹2)j2¡kTc¹1
¾(¢;°c¹1)¡Tc¹2
¾(¢;°c¹2)k2
2¾¯¯¯¯¯Èt!
·2e¡mt2
32R4.
Note that since fÆcWLOT
2,¾(c¹1,c¹2;°)2, we get
P¡¯¯cWLOT
2,¾(c¹1,c¹2;°)2¡WLOT
2,¾(c¹1,c¹2;°)2¯¯Èt¢
·2e¡mt2
32R4. (18)
Theorem B.3. Consider ¹i,¾2W2(Rn)with¾absolutely continuous with respect to the
Lebesgue measure. Assume supp (¹i)½B(0,R)for iÆ1,2. Let±È0. Then with probability
at least 1¡±,
¯¯WLOT
2,¾(c¹1,c¹2;°)¡cWLOT
2,¾(c¹1,c¹2;°)¯¯·Rs
2log(2/ ±)
m,
where m is the number of samples used to estimate ¾.
Proof. Deﬁne
aÆWLOT
2,¾(c¹1,c¹2;°),bÆcWLOT
2,¾(c¹1,c¹2;°).
Then both a·2Rand b·2R. Now, since a2¡b2Æ(aÅb)(a¡b), we get that
ja¡bj ¸1
4Rja2¡b2j.
This, together with (18), implies that
P¡¯¯cWLOT
2,¾(c¹1,c¹2;°)¡WLOT
2,¾(c¹1,c¹2;°)¯¯Èt¢
·2e¡mt2
2R2.
Solving ±Æ2e¡mt2
2R2fortyields the conclusion.

APPENDIX C. N ON-COMPACTLY SUPPORTED MEASURES PROOFS AND RESULTS
Here, we give the proofs of the lemmas preceding Theorems 5.4 and 5.5.
Proof of Lemma 5.2. We will construct the measure e¹by constructing a transport map
that sends ¹to a compactly supported absolutely continuous measure. The compact
set that e¹will be supported on is going to be B(0,R). In particular, for some 0 Ç½¿1,
consider the map
SR,½(x)Æ(
x x 2B(0,R)
Rx
kxkÅmin{kxk¡R,½}x
1Åkxkx62B(0,R).34 LINEARIZED WASSERSTEIN DIMENSIONALITY REDUCTION WITH APPROXIMATION GUARANTEES
Then let e¹Æ(SR,½)]¹, and note that
W1(¹,e¹)Æmin
S:S]¹Æe¹Z
RnkS(x)¡xkd¹(x)·Z
RnkSR,½(x)¡xkd¹(x)
ÆZ
B(0,R)kx¡xk|{z}
Æ0d¹(x)ÅZ
Rn\B(0,R)°°°³
1¡R
kxk¡min{kxk¡R,½}
1Åkxk´
x°°°d¹(x)
·Z
Rn\B(0,R)kxkÅ R|{z}
·kxkÅkxkmin{kxk¡R,½}
1Åkxk| {z }
·½·1·kxkd¹(x)·Z
Rn\B(0,R)3kxkd¹(x).
However, recall that d¹(x)Æf¹(x)d x; thus,
Z
Rn\B(0,R)3kxkd¹(x)ÆZ
Rn\B(0,R)3kxkf¹(x)d x
·Z
Rn\B(0,R)³´
Cn,p,­,Mp´6pÅ16n
p 1
CkxknÅ1d x
·³´
Cn,p,­,Mp´6pÅ16n
pZ
r¸Rrn¡1
rnÅ1dr
|{z}
·1
Æ³´
Cn,p,­,Mp´6pÅ16n
p,
where Cis a constant from integrating over concentric n-spheres. Invoking Theorem 5.1,
this means that
kT¹
¾¡Te¹
¾k¾·Cn,p,­,MpW1(¹,e¹)p
6pÅ16n·Cn,p,­,Mp´
Cn,p,­,MpÆ´.
To see that e¹is compactly supported, notice that for x2Rn\B(0,R), we have
kSR,½(x)k Æ°°°Rx
kxkÅmin{kxk¡R,½}x
1Åkxk°°°·RÅ½kxk
1Åkxk|{z}
·1·RÅ½.
The case for when x2B(0,R) is trivial since SR,½is the identity map on B(0,R). More-
over, to see that e¹is absolutely continuous with respect to the Lebesgue measure, we
will take a generic set Aand break it up into components and analyze each component.
We ﬁrst notice that SR,½is continuous. Indeed, for xsuch that kxk ÆR, we see that
Rx
kxk|{z}
xÅmin{kxk¡R,½}|{z}
Ækxk¡RÆ0x
1ÅkxkÆx.
Now, let A2Rnsuch that ¸(A)Æ0 for the Lebesgue measure ¸, then
AÆ(A\B(0,R))©(A\B(0,R))©(A\@B(0,R))
Æ) (SR,½)]¹(A)Æ(SR,½)]¹(A\B(0,R))Å(SR,½)]¹(A\B(0,R))Å(SR,½)]¹(A\@B(0,R))
Æ¹(SR,½¡1(A\B(0,R)))Å¹(SR,½¡1(A\B(0,R)))Å¹(SR,½¡1(A\@B(0,R)))
Æ¹(A\B(0,R))Å¹(A\@B(0,R))|{z}
·¹(@B(0,R))Æ0Å¹(SR,½¡1(A\B(0,R))),LINEARIZED WASSERSTEIN DIMENSIONALITY REDUCTION WITH APPROXIMATION GUARANTEES 35
where we use the additivity of measures over disjoint sets, the form of SR,½onB(0,R),
and the absolutely continuity of ¹so that ¹(@B(0,R))·¸(@B(0,R))Æ0. Moreover, note
that¹(A\B(0,R))·¹(A)·¸(A)Æ0. The only term left is A\B(0,R). Since SR,½is
smooth on Rn\B(0,R), there exists a density gfor ( SR,½)]¹with respect to ¹for sets in
Rn\B(0,R). This means ( SR,½)]¹¿¹onRn\B(0,R). Since ¹¿¸, we have
¸(A)Æ0Æ)¹(A)Æ0Æ)¹(A\B(0,R))Æ0Æ) (SR,½)]¹(A\B(0,R))Æ0.
This shows that ( SR,½)]¹is absolutely continuous with respect to ¸, so the proof is com-
plete. 
Proof of Lemma 5.3. Rather than constructing a transport map, we will construct a den-
sity fe¹and will argue that the transport map from ¹toe¹(the measure with density fe¹)
behaves nicely. To do this, consider the following density
fe¹,a,R(x)Æ8
>><
>>:f¹(x) x2B(0,R)
f¹³
Rx
kxk´
Å®³
kxk
R¡1´
x2B(0,a) \B(0,R)
0 otherwise,
for some ®È0. Notice that ais not speciﬁed at the moment, but it depends on Rand®.
Since we want e¹to be a probability measure, we note that
e¹(Rd)ÆZ
B(0,R)f¹(x)d x
|{z}
¹(B(0,R))ÅZa
Rrd¡1C(r)³
f¹³
Rx
kxk´
Å®³kxk
R¡1´´
dr
| {z }
I(a),
where C(r) is the integral over the sphere at radius r. Notice that I(a) has an integrand
that is increasing as a function of rso that I(a) itself is increasing as a function of a
(i.e. lim a!1 I(a)Æ 1 ). Moreover, because I(R)Æ0, we know from the intermediate
value theorem that there exists some a¤such that I(a¤)Æ¹(Rd\B(0,R)). Note that
from this construction, e¹is compactly supported, absolutely continuous with respect
to the Lebesgue measure, and 0 Çc·b·fe¹·BÇ 1 for some constants band B.
Now, we would like to bound W1(¹,e¹). Let us consider Ssuch that S]¹Æe¹and S(x)Æ
xifx2B(0,R). Such an Sexists because we can consider the pushforward that is the
identity on B(0,R) and pushes the rest of the mass of ¹from Rd\B(0,R) to B(0,a) \
B(0,R). Note that S(x)2B(0,a) for x2B(0,a) \B(0,R); thus, there exists eCsuch that
kS(x)k ·eCkxk(ifaÇ2R, theneC·2). For the following calculation, we assume that
f¹(x)·³´
Cn,p,­,Mp´6pÅ16n
p 1
C0kxknÅ2:Æ³´
Cn,p,­,Mp´6pÅ16n
p 1
(eCÅ1)CspherekxknÅ2,
where Csphere denotes a constant from integrating over concentric n-spheres and Cn,p,­,Mp
denotes the constant from Theorem 5.1. Now note that
W1(¹,e¹)·Z
RdkS(x)¡xkd¹(x)ÆZ
B(0,R)kx¡xk|{z}
Æ0d¹(x)ÅZ
Rd\B(0,R)kS(x)¡xkd¹(x)
·Z
Rd\B(0,R)kS(x)kÅk xkd¹(x)·Z
Rd\B(0,R)(eCÅ1)kxkf¹(x)d x
·Z
Rd\B(0,R)(eCÅ1)³´
Cn,p,­,Mp´6pÅ16n
p 1
(eCÅ1)CspherekxknÅ1d x36 LINEARIZED WASSERSTEIN DIMENSIONALITY REDUCTION WITH APPROXIMATION GUARANTEES
·³´
Cn,p,­,Mp´6pÅ16n
pZ
r¸Rrn¡1
rnÅ1dr
|{z}
·1·³´
Cn,p,­,Mp´6pÅ16n
p.
Invoking Theorem 5.1, this means that
kT¹
¾¡Te¹
¾k¾·Cn,p,­,MpW1(¹,e¹)p
6pÅ16n·Cn,p,­,Mp´
Cn,p,­,MpÆ´.
Thus, we have the desired result. 
APPENDIX D. P ROOFS AND RESULTS FOR CONDITIONS ON H AND¹
This section provides the proofs of the results in Sections 6 and 7.
D.1. Compact Case Proofs and Results. Here we prove the results of Section 6 which
provide conditions on ¾,¹, andHwhich guarantee that ¹i»H]¹satisfy the conditions
of the theorems from Section 4.
Proof of Theorem 6.1. For the barycentric map estimator, we need to show that the ¹i’s
are compactly supported within a ball of radius Rand T¹i
¾is Lipschitz.
²Compact Support: To ensure that a given ¹iis compactly supported, it sufﬁces
for¹to have compact support and Hto consist of continuous maps. Indeed,
under these assumptions, ¹iis compactly supported since the image of a com-
pact set under a continuous map is compact. Since we are considering only a
ﬁnite number of measures { ¹i}N
iÆ1, each with compact support, there exists a
sufﬁciently large radius Rsuch that supp( ¹i)µB(0,R) for all i.
²Lipschitz OT Map: To make sure that each T¹i
¾is Lipschitz, we will need that hi
is Lipschitz. In particular, we note that ¹iÆ(hi)]¹for some hi2H. Thus, by
compatibility, we know that T¹i
¾Æhi±T¹
¾, which implies that if hiis Lipschitz
and T¹
¾is Lipschitz, then T¹i
¾is Lipschitz.

Proof of Theorem 6.2. For the entropic map estimator, the ¹i’s need to again be compactly-
supported, T¹i
¾needs to be Lipschitz, and ¾and¹itogether satisfy assumptions ( A1)¡
(A3). It will turn out, that we will only need to assume that there exist constants a,AÈ0
such that
aI¹Jh(x)¹AI.
That ¹iis compactly supported and each T¹i
¾are Lipschitz follow from the same
analysis as in the proof of Theorem 6.1.
²Ensuring that ¹isatisfy (A1):Recall that the change of variables formula for the
density of a pushforward measure e¹Æh]¹is given by
fe¹(x)Æf¹(h¡1(x))jJh¡1(x)j,
where jJh¡1(x)jdenotes the determinant of the Jacobian of h¡1. From [25, Corol-
lary 4], we know that his an optimal transport map if it is compatible. This
implies that Jh(x) is positive semideﬁnite; however, if his positive deﬁnite and
Lipschitz (i.e.
aI¹Jh(x)¹AI
for some em,MÈ0), we know that
A¡1I¹Jh¡1(x)¹a¡1I.LINEARIZED WASSERSTEIN DIMENSIONALITY REDUCTION WITH APPROXIMATION GUARANTEES 37
This implies that jJh¡1j È0 for all x. In particular, since the determinant of a
matrix is the product of its eigenvalues, we have that
A¡d· jJh¡1(x)j ÆnY
jÆ1¸j(Jh¡1(x))·a¡n.
Finally, since ¹itself adheres to (A1), this implies that
b
An·f¹(x)jJh¡1(x)j ·B
an.
So ( A1) holds for e¹if there are constants a,AÈ0 such that
aI¹Jh(x)¹AI.
²Ensuring that ¹isatisfy (A2):From [22, Corollary 4.2.10], we can ensure that
(A2) is satisﬁed if ( A3) is satisﬁed, which is proved below.
²Ensuring that ¹isatisfy (A3):First, notice that by compatibility of h, we have
that Th]¹
¾Æh±T¹
¾; thus, a direct corollary of [25, Theorem 24] gives that
(ma)I¹J
Th]¹
¾(x)¹(AL)I
for all x, where mand Lcome from assuming ¾and¹satisfy (A3) whilst aand
Acome from Assumption 4.1. So ( A3) holds for ¾ande¹.

The result above essentially states that the entropic estimator works if every h2His
(exactly) compatible and is uniformly positive deﬁnite.
D.2. Non-Compact Case Proofs and Results. Here we prove the results of Section 7
which provide conditions on ¾,¹, andHwhich guarantee that ¹i»H]¹satisfy the
conditions of the theorems from Section 5.
Proof of Theorem 7.1. Assume that e¹is the truncated measure approximating h]¹for
h2H. Given the assumptions of Lemma 5.3, the truncated measure e¹is compactly
supported, upper and lower bounded, and absolutely continuous. If we can ensure that
the truncated measure e¹also has uniformly convex support, we will fulﬁll the condi-
tions of Caffarelli’s regularity theorem, which guarantees that the optimal transport map
is Lipschitz continuous.
²Decay rate condition: Assuming that ¹has the necessary decay rate f¹(x)·
CÇ 1 and 0 Çc·f¹(x) on a large enough ball where the decay rate is active,
we need that h]¹Æ¹also has the same decay rate up to a constant. For what
follows, we must assume that h2Hhas an inverse h¡1. If we assume further
thatHsatisﬁes Assumption 4.1 (iv) (i.e.
akxk · k h(x)k ·Akxk
for some a,AÈ0), then we know that
A¡1kxk · k h¡1(x)k ·a¡1kxk,
or equivalently,
A¡1
kh¡1(x)k·1
kxk·a¡1
kh¡1(x)k.
The bi-Lipshitz assumption further implies that
A¡1I¹Jh¡1(x)¹a¡1I.38 LINEARIZED WASSERSTEIN DIMENSIONALITY REDUCTION WITH APPROXIMATION GUARANTEES
Thus, for kxk ¸LR(so that kh¡1(x)k ¸R) and the bounds above, we ﬁnd that
f¹(x)Æf¹(h¡1(x))jJh¡1(x)j|{z}
·a¡n
·³´
Cn,p,­,Mp´6pÅ16n
p 1
C0kh¡1(x)knÅ2a¡n
·³´
Cn,p,­,Mp´6pÅ16n
p 1
C0kxknÅ2a¡nAnÅ2.
The constants aand Acan be absorbed into the other decay rate constants;
thus, Assumption 4.1 (iv) gives us the decay rate we want. Noting that the form
of the density f¹also implies that ca¡n·f¹(x) on some large enough ball. In
particular, we get that the truncated measure e¹has a density 0 Çb·fe¹(x)·BÇ
1from Lemma 5.3.
²Uniformly convex support: If¹is supported on all of Rn, we would want h2H
such that ¹Æh]¹is also supported on all of Rn. Recall that the resulting density
of¹is given by
f¹(x)Æf¹(h¡1(x))jJh¡1(x)j|{z}
·a¡n
Note that ¹is supported on all of Rnifkh¡1(x)k ! 1 askxk ! 1 . Indeed, if
we assume Assumption 4.1 (iv), then A¡1kxk · k h¡1(x)k, which implies that ¹
is supported on all of Rn. This would imply that the truncated measure e¹will
be supported on a ball of some radius. This implies that the support of e¹is
uniformly convex and compact.
From the decay rate condition and the uniformly convex support condition, we get
that the truncated measure e¹will satisfy the assumptions of Caffarelli’s regularity theo-
rem. This implies that Te¹
¾will be a C2and Lipschitz function (since Te¹
¾pushes forward
a compact support to a compact support). The other assumptions of the theorem are
trivially satisﬁed. 
Proof of Theorem 7.2. From the proof of Theorem 7.1 above, we easily see that if As-
sumption 4.1 is fulﬁlled and ¹fulﬁlls the conditions of Lemma 5.3 and is supported on
all of Rn, then Te¹
¾will be Lipschitz. We need, however, that e¹also satisﬁes ( A1)-( A3)
from 4.5. We get ( A1) for free since the density fe¹is lower bounded from the proof of
Lemma 5.3. We also get ( A2) since Te¹
¾is differentiable from Caffarelli’s regularity theo-
rem [11, 12, 13] and if (A3) is satisﬁed, which comes from [22, Corollary 4.2.10].
Now we only need to ensure that ( A3) holds. Indeed, since Caffarelli’s regularity the-
orem holds, we know that the potential Ásuch that Te¹
¾Æ rÁis strictly convex, which
implies that r2Á(x) is positive deﬁnite. Moreover, the minimum eigenvalue of r2Á(x)
is a continuous function of x. Since x2supp( ¾), which is compact, we know that
0Ç¸min(¾)Æmin x2supp( ¾)¸min(r2Á(x)), which implies that JTe¹
¾(x)º¸min(¾)I. This
guarantees that ( A3) is satisﬁed for ¾ande¹. 