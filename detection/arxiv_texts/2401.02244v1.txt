Policy-regularized Offline Multi-objective
Reinforcement Learning
Qian Lin
Sun Yat-sen University
Guangzhou, China
linq67@mail2.sysu.edu.cnChao Yu
Sun Yat-sen University
Guangzhou, China
yuchao3@mail.sysu.edu.cn
Zongkai Liu
Sun Yat-sen University
Guangzhou, China
liuzk@mail2.sysu.edu.cnZifan Wu
Sun Yat-sen University
Guangzhou, China
wuzf5@mail2.sysu.edu.cn
ABSTRACT
In this paper, we aim to utilize only offline trajectory data to train a
policy for multi-objective RL. We extend the offline policy-regularized
method, a widely-adopted approach for single-objective offline RL
problems, into the multi-objective setting in order to achieve the
above goal. However, such methods face a new challenge in offline
MORL settings, namely the preference-inconsistent demonstration
problem. We propose two solutions to this problem: 1) filtering
out preference-inconsistent demonstrations via approximating be-
havior preferences, and 2) adopting regularization techniques with
high policy expressiveness. Moreover, we integrate the preference-
conditioned scalarized update method into policy-regularized of-
fline RL, in order to simultaneously learn a set of policies using
a single policy network, thus reducing the computational cost in-
duced by the training of a large number of individual policies for
various preferences. Finally, we introduce Regularization Weight
Adaptation to dynamically determine appropriate regularization
weights for arbitrary target preferences during deployment. Em-
pirical results on various multi-objective datasets demonstrate the
capability of our approach in solving offline MORL problems.1
KEYWORDS
Offline RL; Multi-objective RL; Policy regularization
ACM Reference Format:
Qian Lin, Chao Yu, Zongkai Liu, and Zifan Wu. 2024. Policy-regularized
Offline Multi-objective Reinforcement Learning. In Proc. of the 23rd Interna-
tional Conference on Autonomous Agents and Multiagent Systems (AAMAS
2024), Auckland, New Zealand, May 6 â€“ 10, 2024 , IFAAMAS, 15 pages.
1 INTRODUCTION
In recent years, Reinforcement Learning (RL) [ 21] has achieved
tremendous successes in solving various challenging problems
including robotics [ 12,13], games [ 25,26] and recommendation
systems [ 32]. In the standard RL setting, the primary goal is to
optimize a policy that maximizes a cumulative scalar reward. How-
ever, there often exist multiple, potentially conflicting objectives
1The code for this paper can be found at https://github.com/qianlin04/PRMORL
Proc. of the 23rd International Conference on Autonomous Agents and Multiagent Systems
(AAMAS 2024), N. Alechina, V. Dignum, M. Dastani, J.S. Sichman (eds.), May 6 â€“ 10, 2024,
Auckland, New Zealand .Â©2024 International Foundation for Autonomous Agents
and Multiagent Systems (www.ifaamas.org). This work is licenced under the Creative
Commons Attribution 4.0 International (CC-BY 4.0) licence.in many real-world applications, which motivates the research of
Multi-Objective Reinforcement Learning (MORL) [ 17]. In contrast
to the characteristic of single-best solution in single-objective set-
tings, the MORL paradigm accommodates multiple optimal policies
catering to various preferences that indicate the trade-off among
multiple objectives. Many MORL approaches have been proposed
in the past few years [ 2,17,18]. However, most existing approaches
focus on the online setting, and thus are inappropriate for many
safety-critical real-world scenarios due to the potential risk from
substantial online exploration. Offline MORL, which aims to lever-
age only offline datasets for the training of multi-objective policies,
holds promise for solving this challenge but has received limited
attention and research.
Current offline MORL algorithms either assume that the target
preferences during deployment are static and pre-known [ 28], or
require access to behavior policies [ 23] or their preference informa-
tion [ 33], which can hinder the applicability of these algorithms in
many offline MORL scenarios. The primary objective of this paper
is to leverage trajectory data obtained from preference-unknown
behavior policies to train a policy capable of achieving satisfac-
tory performance for arbitrary target preferences during deploy-
ment. To achieve this goal, we extend the offline policy-regularized
methods [ 10,14,27,29], a kind of widely studied and effective
offline RL approach, to the multi-objective linear-preference set-
ting to address the offline MORL problem. First, we introduce the
preference-inconsistent demonstration problem, a challenge encoun-
tered when directly applying offline policy-regularized methods to
multi-objective datasets collected by behavior policies with diverse
preferences. Specifically, when a policy makes decisions based on a
preference ğâ€²deviating from the target preference ğ, trajectories
generated by this policy can serve as arbitrarily inferior demonstra-
tions under preference ğ, due to conflicts in final decision-making
objectives between ğandğâ€². Consequently, these trajectories can
misguide policy learning through the behavior cloning term used
by offline policy-regularized methods. To address this problem, one
intuitive approach is to directly filter out preference-inconsistent
demonstrations from training datasets, which can be achieve by
approximating the preferences of behavior policies. In addition,
given that preference-inconsistent demonstrations can increase
the difficulty of implicitly modeling behavior policies in offline
policy-regularized methods, we mitigate this issue by adopting
regularization techniques with high policy expressiveness.arXiv:2401.02244v1  [cs.LG]  4 Jan 2024Another challenge is that training an individual policy for each
possible target preference can lead to considerable computational
cost. To solve this problem, we integrate the preference-conditioned
scalarized update method [ 2] into policy-regularized offline RL by
employing the vector value estimate, scalarized value update and
preference-conditioned actors and critics. This approach enables
the simultaneous learning of a set of policies with a single pol-
icy network. Moreover, since the optimal regularization weight in
offline MORL setting can vary across different preferences, Regu-
larization Weight Adaptation is proposed to dynamically determine
the appropriate regularization weights for each target preference
during deployment. Specifically, we treat behavior cloning as an
additional decision-making objective and the regularization weight
as the corresponding preference weight, thus incorporating them
into the original multi-objective setting during training. The ap-
propriate regularization weights then can be dynamically adjusted
using a limited number of collected trajectories during deployment.
We conduct several experiments on D4MORL [ 33], an offline
MORL dataset designed for continuous control tasks. The results
demonstrate that, compared to the state-of-the-art offline MORL
method PEDA [ 33], our approach achieves competitive or even
better performance without using the preference data of behavior
policies. Additionally, since many current offline datasets are col-
lected by behavior policies with a single and fixed preference, it is
meaningful to explore the feasibility of utilizing these datasets for
MORL. Therefore, we introduce the MOSB dataset, a multi-objective
variant of D4RL [ 7] collected by single-preference behavior poli-
cies, which is different from D4MORL where behavior policies are
explicitly trained for MORL. Our approach consistently exhibits out-
standing performance on this dataset, highlighting its superiority
in terms of generalizability across various offline MORL scenarios.
2 RELATED WORK
MORL Existing work within the domain of MORL [ 17,24] can
be broadly categorized into two classes, namely single-policy and
multi-policy methods. Single-policy methods aim to transform a
multi-objective problem into a single-objective problem under one
specific preference among multiple objectives, and then solve it to
obtain a single optimal policy using standard RL approaches [ 1,3].
Conversely, multi-policy methods learn a set of policies to approxi-
mate the Pareto front of optimal policies. To achieve this goal, one
can explicitly maintain a finite set of policies by either applying
a single-policy algorithm individually for each candidate prefer-
ence [ 18,20], or employing evolutionary algorithms to generate
a population of diverse policies [ 11,30]. However, these methods
suffer from low scalability to high-dimensional preference space
due to the limited number of learned policies. To tackle the above
challenge, a feasible approach is to simultaneously learn a set of
policies represented by a single network [ 2,4,31]. Conditioned
Network (CN) method [ 2] utilizes a single preference-conditioned
network to represent value functions over the entire preference
space and applies the scalarized update method [ 18] to update the
vector value function. Based on this method, several works [ 4,31]
are proposed to enhance performance while improving sample
efficiency during online training.Policy-regularized Offline RL The main challenge of offline
RL is the distribution shift between the state-action pairs in the
dataset and those induced by the learned policy [ 16]. Policy regular-
ization [ 10,14,27,29] is proposed as a simple yet efficient approach
to address this issue. This method typically incorporates a behav-
ior cloning mechanism into the policy learning to regularize the
learned policy stay close towards the behavior policy. BCQ [ 10]
employs a conditional variational auto-encoder to model behav-
ior policies and learns a perturbation model to generate bounded
adjustments to the actions of the behavior policy. BEAR [ 14] reg-
ularizes the learned policy with the maximum mean discrepancy,
which is estimated using multiple samples from learned policy
and pre-modeled behavior policy. BRAC [ 29] proposes a general
regularization framework and comprehensively evaluates the ex-
isting techniques in recent methods. TD3+BC [ 8] adds maximum
likelihood estimation (MLE) as a minimal change to the TD3 algo-
rithm, which exhibits surprising effectiveness in addressing offline
RL problems. Diffusion Q-learning [ 27] is a state-of-the-art offline
policy regularization method that leverages the diffusion model as
the learned policy while utilizing the training loss of the diffusion
policy as a policy regularizer.
Offline MORL There exist a limited number of works that focus
on the offline MORL problem. PEDI [ 28] utilize the dual gradient as-
cent and pessimism principle to find an optimal policy that achieves
the highest utility under a fixed preference while keeping the vec-
tor value constrained within a target set. MO-SPIBB [ 23] adopts
the Seldonian framework [ 22] to achieve safe policy improvement
under some predefined preferences using offline datasets. These
methods require prior knowledge of target preferences and cannot
be adapted to new preferences during deployment. To learn a sin-
gle policy that works for all preferences and solve the continuous
control problem in MORL settings, PEDA [ 33] introduces D4MORL,
a large-scale benchmark for offline MORL, and extends supervised
offline RL methods [ 5,6] to MORL by directly incorporating the
preference information into the input of decision models.
Compared to the above works, we provide an intuitive yet un-
explored solution to offline MORL problems by integrating two
well-established methods from the online MORL and offline RL
domains, i.e., offline policy-regularized methods and the preference-
conditioned scalarized update technique, respectively. The major
advantage of our proposed method is that it can respond to arbi-
trary target preferences without access to behavior policies or their
preference information, which is typically unavailable in real-world
scenarios. The empirical results show that our approach demon-
strates superior or competitive performance compared with PEDA
on both datasets collected by multi-preference behavior polices and
by single-preference behavior polices.
3 BACKGROUND
Multi-objective RL The problem of multi-objective RL can be
formulated as a multi-objective Markov decision process (MOMDP),
which is represented by a tuple (S,A,P,ğ’“,ğ›¾,Î©,ğ‘“Î©)with state
spaceS, action spaceA, transition distribution P(ğ‘ â€²|ğ‘ ,ğ‘), vec-
tor reward function ğ’“(ğ‘ ,ğ‘), discount factor ğ›¾âˆˆ [0,1], space of
preferences Î©, and preference functions ğ‘“ğ(ğ’“). The vector reward
function is ğ’“(ğ‘ ,ğ‘)=[ğ‘Ÿ(ğ‘ ,ğ‘)1,ğ‘Ÿ(ğ‘ ,ğ‘)2,...,ğ‘Ÿ(ğ‘ ,ğ‘)ğ‘›]ğ‘‡whereğ‘›is thenumber of objectives and ğ‘Ÿ(ğ‘ ,ğ‘)ğ‘–is the reward of the ğ‘–thobjective.
The preference function ğ‘“ğ(ğ’“)maps a reward vector ğ’“(ğ‘ ,ğ‘)to a
scalar utility by using the given preference ğâˆˆÎ©. The expected
vector return ğ‘®ğœ‹of a policyğœ‹is denoted as the discount sum of
reward vectors, i.e., ğ‘®ğœ‹=Eğœ‹[Ã
ğ‘¡ğ›¾ğ‘¡ğ’“(ğ‘ ğ‘¡,ğ‘ğ‘¡)].
In multi-objective problems, there is typically no optimal pol-
icy that maximizes all objectives simultaneously due to the inner
conflict among different goals. Instead, a set of non-dominated poli-
cies are needed in multi-objective problems. A policy ğœ‹is Pareto
optimal if it cannot be dominated by any other policy ğœ‹â€², i.e., no
other policy ğœ‹â€²can achieve a better return in one objective with-
out incurring a worse return than ğœ‹in other objectives. In this
work, we consider the common framework of an MOMDP with
linear preferences, i.e., ğ‘“ğ(ğ’“(ğ‘ ,ğ‘))=ğğ‘‡ğ’“(ğ‘ ,ğ‘)where ğis a vector
indicating the linear preference. Therefore, given a preference ğ,
we should train a policy ğœ‹that maximizes the expected scalarized
return ğğ‘‡ğ‘®ğœ‹=Eğœ‹[ğğ‘‡Ã
ğ‘¡ğ›¾ğ‘¡ğ’“(ğ‘ ğ‘¡,ğ‘ğ‘¡)]. If all possible preferences
inÎ©are considered, a set of corresponding Pareto policies can be
obtained to form an approximation of the Pareto front.
In the context of offline MORL, we assume no access to on-
line interactions with the environment and offline dataset D=
{(ğ‘ ,ğ‘,ğ‘ â€²,ğ’“)}is the only data available for training. The dataset D
is generated by the behavior policies ğœ‹ğ›½(Â·|ğ)based on various
preferences ğof multiple objectives. Therefore, we define the be-
havior preference ğğœas the preference of the policy that generates
trajectoryğœ. Given that the preferences of behavior policies are
either unknown or difficult to be explicitly represented in many
real-world applications, in this paper, we address the offline MORL
problems under the assumption that the real behavior preferences
are inaccessible during the policy training process, and the goal is
set to learn a policy from Dthat performs well for arbitrary target
preference during deployment.
Preference-conditioned Scalarized Update To extend the
single-objetive methods to multi-objective settings, scalarized deep
Q-learning [ 18] proposes to learn a vector-valued Q-value ğ‘¸=
[ğ‘„1,ğ‘„2,...,ğ‘„ğ‘›], whereğ‘„ğ‘–is the Q value w.r.t. ğ‘–thobjective. For a
specific target preference ğ,ğ‘¸is updated by minimizing the loss:
ğ¿ğ‘¸=E(ğ‘ ,ğ‘,ğ’“,ğ‘ â€²)âˆ¼ğ·
|ğ’šâˆ’ğ‘¸(ğ‘ ,ğ‘)|2
, (1)
where ğ’š=ğ’“+ğ›¾ğ‘¸(ğ‘ â€²,arg maxğ‘â€²[ğğ‘‡ğ‘¸(ğ‘ â€²,ğ‘â€²)]). To respond to dy-
namically determined target preferences, Abels et al . [2] propose a
vector-valued Q-network with outputs conditioned on the given
preference, i.e., ğ‘¸(ğ‘ ,ğ‘,ğ). This method can be extended to actor-
critic framework by introducing the preference-conditioned vector
critic ğ‘¸(ğ‘ ,ğ‘,ğ)and actorğœ‹(ğ‘|ğ‘ ,ğ). In this framework, for each
possible target preference ğ, the critic ğ‘¸(ğ‘ ,ğ‘,ğ)is updated by
minimizing the loss:
ğ¿critic=E(ğ‘ ,ğ‘,ğ’“,ğ‘ â€²)âˆ¼ğ·
|ğ’šâˆ’ğ‘¸(ğ‘ ,ğ‘,ğ)|2
, (2)
where ğ’š=ğ’“+ğ›¾Eğ‘â€²âˆ¼ğœ‹(Â·|ğ‘ ,ğ)ğ‘¸(ğ‘ â€²,ğ‘â€²,ğ). As for the actor ğœ‹(ğ‘|ğ‘ ,ğ),
the loss can be written as:
ğ¿actor=âˆ’Eğ‘ âˆ¼ğ·h
Eğ‘âˆ¼ğœ‹(Â·|ğ‘ ,ğ)h
ğğ‘‡ğ‘¸(ğ‘ ,ğ‘,ğ)ii
. (3)
Offline Policy-regularized Methods In offline RL, the errors in
value estimation from out-of-distribution actions often lead to the
final poor performance. To address this issue, policy regularizationis applied to encourage the learned policy ğœ‹to stay close to the
behavior policy ğœ‹ğ›½, thus preventing out-of-distribution actions and
avoiding performance degradation. In this paper, we focus on a com-
mon form of policy regularization, where a behavior cloning term
is explicitly incorporated into the original actor loss to maximize
the action-state value within the actor-critic framework:
ğ¿actor=âˆ’E(ğ‘ ,ğ‘)âˆ¼ğ·
Eğ‘â€²âˆ¼ğœ‹(Â·|ğ‘ )
ğ‘„(ğ‘ ,ğ‘â€²)
âˆ’ğœ…ğ¿bc(ğ‘ ,ğ‘)
,(4)
whereğ‘„(ğ‘ ,ğ‘)is state-action value learned by critic, ğ¿bcis a be-
havior cloning term and ğœ…is a regularization weight to control
the strength of the behavior cloning. The widely used behavior
cloning terms in previous offline RL studies include: 1) MSE regu-
larization [8] extends the TD3 [ 9] algorithm by adding a behavior
cloning term in the form of mean square error (MSE) between the
actions of the learned policy and the behavior policy in the dataset,
i.e.,ğ¿bc(ğ‘ ,ğ‘)=Eğ‘â€²âˆ¼ğœ‹(Â·|ğ‘ )
âˆ¥ğ‘âˆ’ğ‘â€²âˆ¥2
; 2)CVAE regularization uti-
lizes a policy class based on the conditional variational auto-encoder
(CVAE) and employs the loss of evidence lower bound as the be-
havior cloning term, i.e. ğ¿bc(ğ‘ ,ğ‘)=Eğ‘§âˆ¼ğ‘(Â·|ğ‘ ,ğ‘)[logğ‘(ğ‘|ğ‘ ,ğ‘§)]âˆ’
ğ·KL(ğ‘(ğ‘§|ğ‘ ,ğ‘)||N( 0,ğ‘°)), whereğ‘is an encoder and ğ‘is a decoder
that is used for decision-making; and 3): Diffusion regulariza-
tion [27] adopts the diffusion model as the learned policy and
the diffusion reconstruction loss as the behavior cloning term, i.e.,
Eğ‘–âˆ¼U,ğœ–âˆ¼N( 0,ğ‘°)
âˆ¥ğœ–âˆ’ğœ–ğœƒ(âˆšğ›¼ğ‘–ğ‘+âˆš1âˆ’ğ›¼ğ‘–ğœ–,ğ‘ ,ğ‘–)âˆ¥2
, whereğ‘–is the dif-
fusion timestep, ğ›¼ğ‘–are pre-defined parameters and ğœ–ğœƒ(Â·)is a diffu-
sion model serves as the learned policy.
4 POLICY-REGULARIZED OFFLINE MORL
In this section, we first introduce the preference-inconsistent demon-
stration (PrefID) problem when applying offline policy-regularized
methods to multi-objective datasets collected by behavior policies
with diverse preferences, and present two approaches to mitigating
this problem: 1) filtering out preference-inconsistent demonstra-
tions via approximating behavior preferences and 2) adopting regu-
larization techniques with high policy expressiveness. Then, a nat-
ural integration of policy-regularized offline RL and the preference-
conditioned scalarized update method is proposed to simultane-
ously learn a set of policies using a single policy network. Finally,
we introduce a regularization weight adaptation method to dynam-
ically determine the appropriate regularization weights for each
target preference during deployment.
4.1 The PrefID Problem
When employing the offline policy-regularized method to train an
individual policy for a specific target preference ğ, we minimize
the following loss:
ğ¿actor=âˆ’E(ğ‘ ,ğ‘)âˆ¼ğ·
Eğ‘â€²âˆ¼ğœ‹(Â·|ğ‘ )
ğ‘„(ğ‘ ,ğ‘â€²)
âˆ’ğœ…ğ¿bc(ğ‘ ,ğ‘)
,(5)
ğ¿critic=E(ğ‘ ,ğ‘,ğ’“,ğ‘ â€²)âˆ¼ğ·
(ğ‘¦âˆ’ğ‘„(ğ‘ ,ğ‘â€²))2
, (6)
whereğ‘¦=ğğ‘‡ğ’“+ğ›¾Eğ‘â€²âˆ¼ğœ‹(Â·|ğ‘ )ğ‘„(ğ‘ â€²,ğ‘â€²)andğ‘„(ğ‘ ,ğ‘â€²)is the value func-
tion defined as the expectation of total scalar reward ğğ‘‡ğ’“. Despite
its effectiveness in solving traditional single-objective offline prob-
lems, this approach encounters a new challenge, i.e., the PrefID
problem, when applied to multi-objective datasets collected by be-
havior policies with diverse preferences. Specifically, if a preference
ğâ€²deviates from the target preference ğ, the policy based on ğâ€²0.5 1.0 1.5 2.0 2.5
1e30.51.01.52.02.5Ant (amateur)
Objective 21e3
Dataset
Ratio: 0.2
Ratio: 0.4
Ratio: 0.6
Ratio: 0.8
Ratio: 1.0
2.50 2.75 3.00 3.25 3.50 3.75 4.00
Objective 1
                    (a). Ratio of Training Samples1e33.54.04.55.0Hopper (expert)
Objective 21e3
0.0 0.5 1.0 1.5 2.0 2.5
1e30.00.51.01.52.02.5objective 21e3
Dataset
:  0.1
:  0.25
:  0.5
:  0.75
:  1.0
2.5 3.0 3.5 4.0
Objective 1
                    (b). Regularization weight (Ratio=1.0)1e3345objective 21e3
0.5 1.0 1.5 2.0 2.5
1e30.51.01.52.02.5objective 21e3
Dataset
MSE
CVAE
Diffusion
2.50 2.75 3.00 3.25 3.50 3.75 4.00
Objective 1
                    (c). Regularization term1e33.54.04.55.0objective 21e3
Figure 1: The performance when applying policy-regularized methods on the Ant amateur and Hopper expert datasets within
D4MORL. Two solid circles with the same color represent the expected vector return of policies trained under preferences ğ1
andğ2respectively (the farther these two circles extend to the high area of axes, the better performance they exhibit). Each
cross signifies an aborted training due to the divergence of value estimates. The black dots represent trajectories of entire
offline dataset, which reflect performance of behavior policies under various preferences.
can exhibit a conflict decision-making objective with the policy
based on ğin decision-making. Therefore, trajectories generated
from behavior preference ğâ€²can become arbitrarily inferior demon-
strations (i.e., preference-inconsistent demonstrations) during the
policy training process under the target preference ğ. These demon-
strations can significantly misguide policy learning through the
behavior cloning term used by offline policy-regularized methods
and thus lead to performance degradation.
To study the above problem, we first introduce the sub-dataset
Dğ, which is a subset of Dthat satisfies:
(1)|Dğ|/|D| =a certain ratio ;
(2)1âˆ’ğÂ·ğğœğ‘–
||ğ||Â·||ğğœğ‘–||â‰¤1âˆ’ğÂ·ğğœğ‘—
||ğ||Â·||ğğœğ‘—||,âˆ€ğœğ‘–âˆˆDğ,ğœğ‘—âˆˆD\D ğ.
In other words,Dğcontains a specific proportion of offline trajecto-
ries with behavior preferences having the smallest cosine distance
to preferences ğwithin the entire offline dataset D. Given that the
divergence between two preferences can be effectively captured
by their cosine distance under the linear preference setting, Dğ
with a smaller ratio can be considered to contain fewer preference-
inconsistent demonstrations.
To further illustrate the PrefID problem and explore potential
solutions, we conduct experiments on D4MORL by utilizing offline
policy-regularized methods to train two policies individually, each
maximizing the scalar utility under one of the two target preferences
ğ1=[1,0]andğ2=[0,1]. Figure 1 (a) shows the performance
of the policy trained under ğğ‘–using TD3+BC [ 8] on sub-datasets
Dğğ‘–with various ratios |Dğğ‘–|/|D| forğ‘–=1,2. We observe that if
the entire dataset is used for policy training (i.e., |Dğğ‘–|/|D| =1.0),
the policies learned under preferences ğ1andğ2both exhibit infe-
rior performance compared to behavior policies, while obtaining
highly similar vector returns. Moreover, as preference-inconsistent
samples are progressively excluded from the training dataset (i.e.,
|Dğğ‘–|/|D| decreases), the policy performance gradually improves,
which demonstrates that the preference-inconsistent samples in
the training dataset can hinder policy learning.4.2 Mitigating the PrefID Problem
Based on the above results, one approach to addressing the PrefID
problem is to filter out performance-inconsistent demonstrations
from the training dataset based on the cosine distance between
the behavior preference and the target preference. However, the
real behavior preferences used for calculating cosine distance are
inaccessible under our assumption. Inspired by the observation that
the policy preferences and their associated expected vector return
often exhibit strong correlation [ 4], we propose to approximate the
behavior preference of trajectories by using the L1 normalization of
their vector returns. Specifically, we assign each trajectory ğœwith
an approximate behavior preference:
Ë†ğğœ=ğ‘¹(ğœ)/âˆ¥ğ‘¹(ğœ)|âˆ¥1, (7)
where ğ‘¹(ğœ)=Ã
ğ‘¡ğ’“ğ‘¡is the total vector return of ğœ. Then we define
the sub-datasetD(ğ)used in the policy training process under the
target preference ğas:
D(ğ)={ğœ|ğœâˆˆD,1âˆ’ğÂ·Ë†ğğœ
||ğ||Â·|| Ë†ğğœ||â‰¤2ğœƒ}, (8)
which indicates that the cosine distance between the approximate
behavior preferences in D(ğ)and the target preference ğdoes not
exceed the preset threshold 2ğœƒ. Whenğœƒis set to 1, i.e.,D(ğ)=D,
the entire offline dataset is employed for policy training under ğ,
while only the trajectories with an approximate behavior preference
Ë†ğğœ=ğare utilized when ğœƒis set to 0. Although this approach is
intuitive and simple, it can effectively mitigate the emergence of
preference-inconsistent demonstrations in D(ğ). This is because
trajectories generated by policies with inconsistent preferences
typically exhibit noticeable differences in their vector returns, and
thus such trajectories are scarcely included simultaneously in the
sameD(ğ)due to the similarity of Ë†ğğœinD(ğ).
In addition to directly excluding performance-inconsistent demon-
strations from training datasets, a parallel approach is to mitigate
the adverse effect of preference-inconsistent demonstrations from
behavior cloning by utilizing a smaller regularization weight. How-
ever, as depicted in Figure 1 (b), employing MSE regularization
with a reduced regularization weight fails to improve policy perfor-
mance, and even leads to the divergence of value estimates due toAlgorithm 1 Policy-regularized Offline MORL
1:Input: Offline datasetD, hyperparameter ğœƒandğœ”min
bc, batch
sizeğµ
2:Initialized policy ğœ‹and critic Ë†ğ‘¸
3:foreach interaction do
4: Sample mini-batch B={(ğ‘ ğ‘–,ğ‘ğ‘–,ğ’“ğ‘–,ğ‘ â€²
ğ‘–)}ğµ
ğ‘–=1âˆ¼D
5: Sample target preferences ğğ‘–fromğ‘ƒ(ğ|ğœğ‘–)whereğœğ‘–is the
trajectory containing (ğ‘ ğ‘–,ğ‘ğ‘–,ğ’“ğ‘–,ğ‘ â€²
ğ‘–)
6: Sampleğœ”bc,ğ‘–uniformly from[ğœ”min
bc,1]
7: SetË†ğğ‘–= (1âˆ’ğœ”bc,ğ‘–)Â·ğ,ğœ”bc,ğ‘–
8: Update the policy by batch gradient descent on B
ğ¿actor=âˆ’1
ğµğµâˆ‘ï¸
ğ‘–=1Eğ‘â€²âˆ¼ğœ‹(Â·|ğ‘ ğ‘–,Ë†ğğ‘–)Ë†ğğ‘–Ë†ğ‘¸(ğ‘ ğ‘–,ğ‘â€²,Ë†ğğ‘–)
9: Update the critc by batch gradient descent on B
ğ¿critic=1
ğµğµâˆ‘ï¸
ğ‘–=1(ğ’“ğ‘–+ğ›¾Eğ‘â€²âˆ¼ğœ‹(Â·|ğ‘ ,Ë†ğğ‘–)ğ‘¸(ğ‘ â€²
ğ‘–,ğ‘â€²,Ë†ğğ‘–)âˆ’ğ‘¸(ğ‘ ğ‘–,ğ‘ğ‘–,Ë†ğğ‘–))2
10:end for
distribution shift issues. Nevertheless, adopting regularization tech-
niques with high policy expressiveness is an effective approach to
mitigating the PrefID problem. Given that preference-inconsistent
demonstrations generated from various preferences can exhibit
entirely diverse behaviors, the behavior policy that is implicitly
modeled by behavior cloning used in offline policy-regularized
methods could demonstrate a high level of complexity, which in
term leads to inaccurate modeling of the behavior policy and subse-
quent policy degradation. This issue can be alleviated by employing
regularization techniques with high policy expressiveness (e.g.,
CVAE and diffusion regularization) for policy learning, which have
been proven effective in modeling the complex behavior policy [ 27].
Figure 1 (c) shows that using diffusion and CVAE regularization
can indeed improve performance and the diffusion regularization
method performs best among all these regularization techniques.
4.3 Responding to Arbitrary Target Preferences
To respond to arbitrary target preference during deployment, we
can utilize offline policy-regularized methods to train an individ-
ual policy for each possible preference. However, this approach
faces a significant challenge in terms of computational costs and
storage requirements, as it requires the training of a large number
of policies to cover all possible preferences. To address this chal-
lenge, we incorporate the preference-conditioned scalarized update
method into policy-regularized offline RL, which enables simulta-
neous learning of a set of policies with a single policy network. The
goal can be formulated as minimizing the loss:
ğ¿actor=âˆ’Eğâˆ¼ğ‘ƒ(Î©)
E(ğ‘ ,ğ‘)âˆ¼ğ·(ğ)

Eğ‘â€²âˆ¼ğœ‹(Â·|ğ‘ ,ğ)h
ğğ‘‡ğ‘¸(ğ‘ ,ğ‘â€²,ğ)i
âˆ’ğœ…ğ¿bc(ğ‘ ,ğ‘)
,
ğ¿critic=Eğâˆ¼ğ‘ƒ(Î©)
E(ğ‘ ,ğ‘,ğ’“,ğ‘ â€²)âˆ¼ğ·(ğ)
(ğ’šâˆ’ğ‘¸(ğ‘ ,ğ‘,ğ))2
,
(9)
where ğ’š=ğ’“+ğ›¾Eğ‘â€²âˆ¼ğœ‹(Â·|ğ‘ ,ğ)ğ‘¸(ğ‘ â€²,ğ‘â€²,ğ), andğ‘ƒ(Î©)is a prior dis-
tribution of ğ. One challenge in the implementation of Eq. (9) iscomputational complexity from reconstructing D(ğ)for each new
ğsampled from ğ‘ƒ(Î©)during training . Therefore, in our implemen-
tation, we adopt an sampling scheme that alters the sampling order
ofğœandğ, i.e., replace Eğâˆ¼ğ‘ƒ(ğ)Eğœâˆ¼ğ·(ğ)withEğœâˆ¼ğ·Eğâˆ¼ğ‘ƒ(ğ|ğœ)
whereğ‘ƒ(ğ|ğœ)=ğ‘ˆ({ğâˆˆÎ©|1âˆ’ğÂ·Ë†ğğœ
||ğ||Â·||Ë†ğğœ||â‰¤2ğœƒ}).
In Eq. (9),ğœ…is the regularization weight that determines the
trade-off between behavior cloning and policy improvement. An
oversizedğœ…can degenerate policy learning into mere behavior
cloning, consequently impeding the policy improvement, while
an undersized ğœ…can lead to the performance degradation due to
severe distribution shift issues. In offline single-objective settings,
an appropriate regularization weight can be determined through
meticulous fine-tuning. However, the optimal regularization weight
in offline MORL settings can vary across different preferences, due
to the differing data coverage in the optimal trajectory space for
various preferences. Therefore, the regularization weight should
be dynamically adjusted to accommodate diverse preferences.
Algorithm 2 Regularization Weight Adaption
1:Input: Target preference ğ, number of update interaction ğ‘,
number of trajectories collected within each iteration ğ¾
2:Initialized a truncated Gaussian distribution N(ğœ‡,ğœ)
3:forğ‘–=1,2,..,ğ‘ do
4: Sampleğœ”bcfromN(ğœ‡,ğœ)and let Ë†ğ=((1âˆ’ğœ”bc)Â·ğ,ğœ”bc)
5: Collectğ¾trajectories via ğœ‹(Â·|Ë†ğ)to form a mini-batch B
6: Updateğœ‡andğœby minimizing:
âˆ’Eğœ”bcâˆ¼N(ğœ‡,ğœ)[Eğœâˆ¼B[ğğ‘¹(ğœ)]].
7:end for
8:Returnğœ‡as final regularization weight ğœ”bc
To this end, we propose Regularization Weight Adaptation to
achieve adaptive adjustments of the regularization weight for di-
verse preferences. First, we define a behavior cloning objective with
a scalar value:
ğ‘‰ğ
bc(ğ‘ )=âˆ’E(ğ‘ â€²,ğ‘â€²)âˆ¼ğ·(ğ)|ğ‘ â€²=ğ‘ 
ğœ‚ğ¿bc(ğ‘ â€²,ğ‘â€²)
, (10)
whereğœ‚is a scale hyperparameter. Then, we introduce its corre-
sponding preference ğœ”bcthat serves as an adaptable regularization
weight. Next, we incorporate this behavior cloning objective and
its corresponding preference into the original MORL framework
by extending the value vector and preference vector to include the
behavior cloning objective:
Ë†ğâ‰œ((1âˆ’ğœ”bc)Â·ğ,ğœ”bc), (11)
Ë†ğ‘¸(ğ‘ ,ğ‘,Ë†ğ)â‰œ
ğ‘¸(ğ‘ ,ğ‘,Ë†ğ),ğ‘‰Ë†ğ
bc(ğ‘ )
. (12)
Due to the incorporation of the behavior cloning objective, both of
the actor and the critic should be conditioned on an argumented
preference Ë†ğ, i.e.,ğœ‹(Â·|ğ‘ ,Ë†ğ),ğ‘¸(ğ‘ ,ğ‘,Ë†ğ). Afterwards, the loss of actor
and critic can be reformulated as follows:
ğ¿actor=âˆ’EË†ğâˆ¼ğ‘ƒ(Ë†Î©)h
Eğ‘ âˆ¼D(ğ)h
Eğ‘âˆ¼ğœ‹(Â·|ğ‘ ,Ë†ğ)h
Ë†ğğ‘‡Ë†ğ‘¸(ğ‘ ,ğ‘,Ë†ğ)iii
,
ğ¿critic=EË†ğâˆ¼ğ‘ƒ(Ë†Î©)
E(ğ‘ ,ğ‘,ğ’“,ğ‘ â€²)âˆ¼ğ·(ğ)
(ğ’šâˆ’ğ‘¸(ğ‘ ,ğ‘,Ë†ğ))2
.
(13)
where ğ’š=ğ’“+ğ›¾Eğ‘â€²âˆ¼ğœ‹(Â·|ğ‘ ,Ë†ğ)ğ‘¸(ğ‘ â€²,ğ‘â€²,Ë†ğ). The argumented prefer-
ence Ë†ğis obtained by combining ğsampled from ğ‘ƒ(ğ|ğœ)andğœ”bcsampled from[ğœ”min
bc,1]whereğœ”min
bc>0is a hyperparameter to
circumvent instances of a zero value for ğœ”bc. The main discrepancy
between Eq. (9) and Eq. (13) lies in the regularization weight, which
is a preset hyperparameter ğœ…in Eq. (9), but assumed unknown
preference Ë†ğin Eq. (13).
The remaining challenge lies in determining the regularization
weight (i.e., behavior cloning preference) ğœ”bcfor a given target
preference ğto maximize the expected scalarized return ğğ‘®ğœ‹.
Drawing inspiration from the policy adaptation method in [ 31], we
employ a gradient approach to efficiently infer the regularization
weightğœ”bc. To respond to a given preference ğduring deployment,
we sampleğœ”bcfrom a truncated Gaussian distribution N(ğœ‡,ğœ)with
learnable parameter ğœ‡andğœ, and then combine ğœ”bcandğas inputs
to the policy ğœ‹(Â·|Ë†ğ)for decision-making. During policy deployment,
we can iteratively optimize the parameter ğœ‡andğœthrough gradient
decent using a limited number of trajectory samples collected by
ğœ‹(Â·|Ë†ğ)as follows:
arg max
ğœ‡,ğœEğœ”bcâˆ¼N(ğœ‡,ğœ)h
Eğœâˆ¼ğœ‹(Â·|Ë†ğ)h
ğğ‘‡ğ‘¹(ğœ)ii
. (14)
Despite the involvement of online interactions during the inference
ofğœ”bc, the associated risks are constrained by two key factors:
1) the policy ğœ‹(Â·|Ë†ğ)is well-trained to encompass a broad range
ofğœ”bc; and 2) only a limited number of samples (e.g. 30in our
experiments) are sufficient to optimize the two scalar parameters
ğœ‡andğœto achieve satisfactory performance. The pseudo codes of
policy training and regularization weight adaptation are presented
in Algorithm 1 and 2, respectively.
5 EXPERIMENTS
In this section, we evaluate our approach on D4MORL [ 33], an
offline MORL dataset collected by behavior policies with diverse
preference, and a multi-objective variant of D4RL [ 7] generated by
single-objective behavior policies (i.e., the MOSB dataset). Due to
space limits, we present ablation experiments on the technique of
filling out the preference-inconsistent samples and Regularization
Weight Adaptation in Appendix E.1 and E.2, respectively.
Evaluation Protocols Given the fact that the true Pareto
front is often unavailable in many problems, Hypervolume ( Hv)
and Sparsity ( Sp) are commonly used metrics for evaluating the
relative quality of the approximated Pareto front of different algo-
rithms, without requiring knowledge of the true Pareto front [ 1,4,
19,30,33]. Hv measures the volume enclosed by the returns of poli-
cies in the approximated Pareto set ğ‘ƒ, indicating the performance
increase across all objectives, while Sp measures the average return
distance of two adjacent solutions, offering insights into the density
of polices in the approximated Pareto set. A larger Hv and a smaller
Sp indicate a better performance, with Hv typically serving as the
primary comparison metric and Sp the secondary metric. This is
because optimizing policies primarily based on Sp metric can lead
to very narrow Pareto front and a low Hv. More details about met-
rics of Hv and Sp can be found in Appendix C. In all experiments,
we evaluate the methods using 101equidistant preference point
pairs (i.e.,[0.00,1.00],[0.01,0.99],...,[1.00,0.00]) and average the
vector returns of 5trajectories sampled from randomly initialized
environments for each preference point. Hv and Sp metrics are
calculated based on these averaged vector return. We perform 3runs with various seeds for each algorithm and report the average.
Except for Hv and Sp metrics, Appendix E.3 provides additional
performance comparisons using the expected utility (EU) metric.
Comparative Algorithms We present the results of our pro-
posed policy-regularized offline MORL method with MSE regular-
ization, CVAE regularization and Diffusion regularization. Our pri-
mary comparative baseline is the state-of-the-art method PEDA [33],
which extends the DT [ 5] and RvS [ 6] algorithms to MORL settings,
referred as MODT (P) andMORvS (P) , respectively, by incor-
porating preference information and vector-valued returns into
corresponding model architectures. In addition, we also provide
the results of the following two methods: 1) preference-conditioned
behavior cloning, denoted as BC (P) , where the policy receives
not only the state but also its corresponding behavior preference
as input and generates predicted actions; and 2) MO-CQL , where
the Conservative Q-Learning (CQL) [ 15] is incorporated into the
naive scalarized-updated actor-critic to cope with the distribution
shift problem. Note that algorithms MODT (P), MORvS (P) and
BC (P) require access to the preference information of behavior
policies, whereas our approach and MO-CQL have no such an as-
sumption. More implementation details of algorithms can be found
in Appendix A and B.
Regularization Weight Adaptation Phase Before evaluation,
we conduct regularization weight adaptation to infer the appro-
priate regularization weight for each evaluated preference in our
approach. In this phase, we employ gradient descent to update the
parameters ğœ‡andğœof regularization weight distribution N(ğœ‡,ğœ).
Specifically, we conduct a total of 3 gradient descent iterations, with
each iteration using 10 trajectories collected by ğœ‹(Â·|Ë†ğ)from the
environment. Finally, we utilize the updated ğœ‡as the finalğœ”bcfor
decision-making of policy ğœ‹(Â·|Ë†ğ)during evaluation.
5.1 Experiments on D4MORL
Environments and Datasets D4MORL [ 33] is a dataset de-
signed for offline MORL, which contains millions of trajectories col-
lected by a set of multi-objective behavior policies in various multi-
objective MuJoCo environments. D4MORL contains two types of
datasets with different behavior policies: 1) Amature Dataset , a
combination of expert demonstrations and suboptimal data gener-
ated via randomly perturbed expert policies; and 2) Expert Dataset ,
consisting entirely of trajectories collected by expert policies. In ad-
dition, the datasets we utilized for comparison consist of trajectories
with behavior preferences uniformly sampled from a wide range of
preference space. These datasets also serve as the primary dataset
for experimentation in PEDA [ 33]. More details about D4MORL are
provided in Appendix D.
Results and Analysis Tables 1 provides the performance of dif-
ferent methods on the Amateur dataset and Expert dataset. First,
we observe that three regularization techniques yield similar per-
formance, but Diffusion regularization demonstrates an overall
advantage compared to the others, which is consistent with the
discussions about policy regularization techniques in Section 4.2.
On amateur dataset, our approach consistently performs the best
for the primary metric Hv, especially in the Ant, Simmer and Hop-
per environments, where our approach achieves significant im-
provement compared to the state-of-the-art baseline PEDA. ThisTable 1: Results on D4MORL Amateur and Expert datasets. B indicates the performance of behavior policies from PEDA.
Env metric B BC (P) MO-CQL MODT (P)â€ MORvS (P)â€ Diffusion CVAE MSEAmateurAntHV ( 106)5.61 3.39Â±.81 6.28Â±.00 5.63Â±.03 5.81Â±.03 6.15Â±.01 5.48Â±.01 5.49Â±.02
SP (104) - 2.44Â±2.27 1.37Â±.02 1.93Â±.27 0.76Â±.08 0.95Â±.03 1.35Â±.38 1.81Â±.19
SwimmerHV ( 104)2.11 2.79Â±.03 2.94Â±.03 0.61Â±.01 2.79Â±.05 3.11Â±.03 3.14Â±.01 3.02Â±.03
SP (101) - 1.44Â±.09 11.75Â±.78 1.80Â±.08 1.60Â±.40 3.13Â±.42 3.86Â±.62 3.20Â±.62
HalfCheetahHV ( 106)5.68 5.42Â±.05 5.59Â±.01 5.63Â±.05 5.74Â±.00 5.72Â±.02 5.74Â±.00 5.75Â±.01
SP (104) - 1.21Â±.58 4.11Â±.16 0.46Â±.05 0.34Â±.06 0.53Â±.21 0.51Â±.08 0.57Â±.09
HopperHV ( 107)1.97 1.60Â±.03 1.84Â±.00 1.56Â±.19 1.68Â±.07 1.88Â±.03 1.75Â±.05 1.83Â±.07
SP (105) - 0.21Â±.03 4.57Â±3.88 2.13Â±2.15 0.47Â±.16 0.09Â±.03 0.17Â±.03 0.42Â±.15
Walker2dHV ( 106)4.99 3.67Â±.09 4.15Â±.03 3.17Â±.12 4.93Â±.05 4.97Â±.02 4.94Â±.02 4.99Â±.07
SP (104) - 18.67Â±16.2633.14Â±15.28 46.88Â±11.87 0.84Â±.12 1.06Â±.17 0.99Â±.15 0.91Â±.27ExpertAntHV ( 106)6.32 4.42Â±.57 6.12Â±.01 5.89Â±.03 6.08Â±.04 6.16Â±.02 5.72Â±.05 5.75Â±.03
SP (104) - 4.08Â±2.78 1.55Â±.21 2.33Â±.65 1.15Â±.12 0.71Â±.08 2.04Â±1.19 1.51Â±.13
SwimmerHV ( 104)3.25 3.18Â±.01 2.26Â±.04 3.22Â±.01 3.22Â±.01 3.19Â±.01 3.14Â±.03 3.15Â±.02
SP (101) - 4.00Â±.24 0.02Â±.01 2.96Â±.17 4.38Â±.26 5.44Â±1.25 11.26Â±4.2410.60Â±4.41
HalfCheetahHV ( 106)5.79 5.60Â±.03 5.65Â±.02 5.65Â±.00 5.75Â±.00 5.76Â±.00 5.75Â±.00 5.76Â±.00
SP (104) - 0.49Â±.18 1.05Â±.12 0.77Â±.06 0.68Â±.10 0.34Â±.04 0.64Â±.09 0.58Â±.01
HopperHV ( 107)2.09 0.88Â±.16 1.71Â±.06 1.79Â±.13 1.97Â±.01 1.67Â±.02 1.76Â±.13 1.98Â±.03
SP (105) - 23.62Â±11.30 3.47Â±.75 1.21Â±.40 2.05Â±1.75 6.20Â±3.85 2.28Â±1.37 0.83Â±.16
Walker2dHV ( 106)5.21 2.12Â±.55 2.55Â±.03 5.00Â±.08 5.05Â±.07 4.98Â±.06 4.93Â±.11 4.91Â±.04
SP (104) - 34.77Â±23.21 0.00Â±.00 2.12Â±.60 1.86Â±.51 4.14Â±1.39 3.67Â±1.71 3.71Â±1.38
â€ Due to slight differences in evaluation protocols, the presented results of PEDA exhibit minor discrepancies compared to the original paper.
0.0 0.5 1.0 1.5 2.0 2.5
objective 11e3012objective 21e3       Ant-amateur_uniform       
0.5 1.0 1.5 2.0
objective 11e20.60.81.01.21.41e2       Swimmer-amateur_uniform       
0.0 0.5 1.0 1.5 2.0 2.5
objective 11e3121e3       HalfCheetah-amateur_uniform       
0 1 2 3 4
objective 11e30241e3       Hopper-amateur_uniform       
0.5 1.0 1.5 2.0
objective 11e30.51.01.52.02.51e3       Walker2d-amateur_uniform       
Offline data Dominated Solutions Undominated Solutions
Figure 2: Approximate Pareto fronts learned by our approach with Diffusion regularization on the Amateur datasets.
is because, in contrast to the supervised offline models utilized
in PEDA, which strongly rely on high-quality demonstrations for
decision-making, our approach utilizes explicit policy improvement
through the guidance of value estimates rather than solely follow-
ing the expert demonstrations, making it more robust to suboptimal
demonstrations. In the Expert dataset, where the data are entirely
collected by expert behavior policies, our approach still exhibits
competitive performance compared to PEDA. Finally, we observe
that our approach performs slightly worse than PEDA in the sec-
ondary metric, Sp. A reasonable explanation is that, given the fixed
number of sampling preferences, the improvement in Hv, signifying
the expansion of the Pareto front, can inevitably lead to increased
distances between solutions, consequently leading to an increase in
Sp. The approximate Pareto fronts are partially shown in Figure 2
and fully displayed in Appendix E.5, which demonstrate the capa-
bility of our approach in learning a broad and dense approximate
Pareto front.5.2 Experiments on MOSB
Environments and Datasets In many real-world scenarios, the
offline data are collected by single-objective policies or policies with
a fixed preference over multiple objectives. To explore the feasibility
of utilizing these datasets for MORL, we introduce a new offline
multi-objective dataset, namely MOSB, which is built upon the
D4RL [ 7] Mujoco dataset. The main differences between the MOSB
dataset and the D4MORL dataset include: 1) behavior preferences,
where the behavior preferences in the MOSB dataset are identical
and inaccessible, whereas those in D4MORL are sampled from a
wide range of preference spaces; 2) behavior policy types, where
MOSB is collected from diverse sources, including the partially-
trained policy (medium), the replay buffer (medium-replay), and
a combination of the partially-trained policy and expert policy
(medium-expert); and 3) data quantity, where MOSB contains fewer
transition data compared to D4MORL ( 0.2âˆ¼2M vs. 25M). More
details about the MOSB dataset are provided in Appendix D.Table 2: Results on MOSB dataset. D indicate the Hv and Sp of the vector return front of offline data.
Env Metric D BC(P)â€ MORvS(p)â€ MO-CQL Diffusion CVAE MSEmedium-replayHalfCheetahHv (107)1.21 1.56Â±.07 1.71Â±.02 1.59Â±.07 3.25Â±.04 3.08Â±.10 2.93Â±.07
Sp (105)2.74 4.52Â±.46 1.18Â±.36 9.23Â±1.69 3.32Â±.75 3.55Â±1.693.75Â±1.17
HopperHv (107)1.22 0.64Â±.15 0.53Â±.03 1.37Â±.07 1.46Â±.01 1.06Â±.08 1.27Â±.06
Sp (105)14.87 7.36Â±4.89 13.01Â±4.98 0.59Â±.55 0.48Â±.36 2.76Â±1.890.93Â±.38
Walker2dHv (107)1.02 0.42Â±.27 0.96Â±.03 0.37Â±.06 1.46Â±.06 0.95Â±.04 1.17Â±.10
Sp (104)9.71 32.37Â±3.84234.56Â±74.90190.24Â±1.04 2.24Â±1.26 0.08Â±.00 1.04Â±.63mediumHalfCheetahHv (107)1.32 1.17Â±.07 1.17Â±.08 1.19Â±.08 3.56Â±.11 3.18Â±.37 3.39Â±.03
Sp (105)2.29 0.29Â±.22 0.08Â±.05 58.81Â±1.80 2.08Â±.77 2.19Â±.79 1.00Â±.11
HopperHv (107)1.19 0.43Â±.04 0.52Â±.04 1.28Â±.02 1.37Â±.02 1.42Â±.03 1.25Â±.03
Sp (103)0.00 6.87Â±2.72 14.33Â±2.27 29.22Â±1.96 0.20Â±.05 2.97Â±1.330.78Â±1.10
Walker2dHv (107)0.95 0.64Â±.41 0.97Â±.02 0.80Â±.00 1.27Â±.03 1.66Â±.05 1.17Â±.02
Sp (104)18.82 3.87Â±2.29 13.65Â±2.68 1.06Â±.95 0.48Â±.07 0.49Â±.03 0.29Â±.07medium-expertHalfCheetahHv (107)2.70 1.09Â±.13 2.83Â±.22 2.88Â±.16 3.39Â±.15 2.00Â±.66 3.83Â±.29
Sp (106)1.73 0.26Â±.22 1.73Â±1.31 0.05Â±.02 2.02Â±.88 1.08Â±.98 0.75Â±.26
HopperHv (107)1.48 0.93Â±.45 1.54Â±.01 1.54Â±.01 1.56Â±.01 1.52Â±.01 1.48Â±.01
Sp (102)15.97 9.70Â±1.37 3.87Â±1.89 8.04Â±1.50 2.05Â±.67 1.88Â±1.150.72Â±.44
Walker2dHv (107)1.15 1.02Â±.06 1.29Â±.00 1.08Â±.02 1.46Â±.01 0.93Â±.81 1.34Â±.00
Sp (105)9.76 0.44Â±.16 3.74Â±1.53 1.27Â±1.26 0.01Â±.00 0.01Â±.01 0.00Â±.00
â€ Since these algorithms require real behavior preferences, which are unavailable in MOSB, we use the approximate preferences in Eq. (7) as
substitutes for policy training.
2 4 6 8
objective 11e3024objective 21e3    HalfCheetah-medium-replay    
2 4 6
objective 11e31
01231e3    HalfCheetah-medium    
0.25 0.50 0.75 1.00 1.25
objective 11e4123451e3    HalfCheetah-medium-expert    
0 1 2 3 4
objective 11e3012341e3    Walker2d-medium-replay    
0 1 2 3 4
objective 11e301231e3    Walker2d-medium    
0 1 2 3 4 5
objective 11e301231e3    Walker2d-medium-expert    
Offline data Dominated Solutions Undominated Solutions
Figure 3: Vector returns of trajectories in MOSB dataset and approximate Pareto fronts learned by Diffusion regularization.
Results and Analysis The results of different algorithms on the
MOSB dataset are presented in Table 2. Our approaches equipped
with different regularization techniques consistently outperform
other methods. Compared to the vector return of the offline data (i.e.
D in Table 2), our approach shows significant improvements in both
Hv and Sp metrics. These improvements indicate that our approach
has learned a broadly expanded approximate Pareto front with a
dense policy set, which is consistent with the results in Figure 3.
In contrast, BC (P) and PEDA rely on accurate behavior preference
data, leading to their poorer performance on MOSB.
We present vector returns of trajectories in MOSB and the ap-
proximate Pareto front learned from our approach on HalfCheetah
and Walker2d in Figure 3, and a comprehensive display of other
environments is provided in Appendix E.5. Due to the fixed pref-
erences of behavior policies, the vector returns distribution of tra-
jectories in the offline dataset is extremely unbalanced. There are
a greater number of trajectories with high returns on objective 1,
while fewer trajectories exhibit high returns on objective 2. How-
ever, our approach can still achieve a broad approximate Pareto
front with a dense policy set, which demonstrates the feasibility
of learning multi-objective policies from the dataset collected bysingle-preference behavior policies. Moreover, our obtained approx-
imate Pareto front significantly expands outward compared to the
front composed of vector returns in the dataset, which indicates that
our approach can achieve substantial policy improvement rather
than simply imitate behavior policies.
6 CONCLUSION
In this paper, we make a first attempt to employ offline policy-
regularized methods to tackle the offline MORL problems. We pro-
vide two intuitive yet effective solutions to mitigating the PrefID
problem encountered in such problems. In addition, we introduce
the preference-conditioned scalarized update method to policy-
regularized offline RL, in order to simultaneously learn a set of
policies using a single policy network. Moreover, a novel method
Regularization Weight Adaptation is proposed to dynamically deter-
mine the appropriate regularization weights for each target prefer-
ence during deployment. Empirically, the experiments conducted
on various multi-objective datasets demonstrate that, compared
with the state-of-the-art offline MORL algorithms, our approach
can achieve competitive or superior performance without requiring
preference information of behavior polices.REFERENCES
[1]Abbas Abdolmaleki, Sandy Huang, Leonard Hasenclever, Michael Neunert, Fran-
cis Song, Martina Zambelli, Murilo Martins, Nicolas Heess, Raia Hadsell, and
Martin Riedmiller. 2020. A distributional view on multi-objective policy opti-
mization. In International conference on machine learning . PMLR, 11â€“22.
[2]Axel Abels, Diederik Roijers, Tom Lenaerts, Ann NowÃ©, and Denis Steckelmacher.
2019. Dynamic weights in multi-objective deep reinforcement learning. In Inter-
national conference on machine learning . PMLR, 11â€“20.
[3]Mridul Agarwal, Vaneet Aggarwal, and Tian Lan. 2022. Multi-objective re-
inforcement learning with non-linear scalarization. In Proceedings of the 21st
International Conference on Autonomous Agents and Multiagent Systems . 9â€“17.
[4]Toygun Basaklar, Suat Gumussoy, and Umit Y Ogras. 2022. Pd-morl: Preference-
driven multi-objective reinforcement learning algorithm. arXiv preprint
arXiv:2208.07914 (2022).
[5]Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin,
Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. 2021. Decision transformer:
Reinforcement learning via sequence modeling. Advances in neural information
processing systems 34 (2021), 15084â€“15097.
[6]Scott Emmons, Benjamin Eysenbach, Ilya Kostrikov, and Sergey Levine. 2021.
Rvs: What is essential for offline rl via supervised learning? arXiv preprint
arXiv:2112.10751 (2021).
[7]Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. 2020.
D4rl: Datasets for deep data-driven reinforcement learning. arXiv preprint
arXiv:2004.07219 (2020).
[8]Scott Fujimoto and Shixiang Shane Gu. 2021. A minimalist approach to offline
reinforcement learning. Advances in neural information processing systems 34
(2021), 20132â€“20145.
[9]Scott Fujimoto, Herke Hoof, and David Meger. 2018. Addressing function ap-
proximation error in actor-critic methods. In International conference on machine
learning . PMLR, 1587â€“1596.
[10] Scott Fujimoto, David Meger, and Doina Precup. 2019. Off-policy deep rein-
forcement learning without exploration. In International conference on machine
learning . PMLR, 2052â€“2062.
[11] Hisashi Handa. 2009. Solving multi-objective reinforcement learning problems by
EDA-RL-acquisition of various strategies. In 2009 ninth international conference
on intelligent systems design and applications . IEEE, 426â€“431.
[12] Josiah P Hanna, Siddharth Desai, Haresh Karnan, Garrett Warnell, and Peter
Stone. 2021. Grounded action transformation for sim-to-real reinforcement
learning. Machine Learning 110, 9 (2021), 2469â€“2499.
[13] Elia Kaufmann, Leonard Bauersfeld, Antonio Loquercio, Matthias MÃ¼ller, Vladlen
Koltun, and Davide Scaramuzza. 2023. Champion-level drone racing using deep
reinforcement learning. Nature 620, 7976 (2023), 982â€“987.
[14] Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. 2019.
Stabilizing off-policy q-learning via bootstrapping error reduction. Advances in
Neural Information Processing Systems 32 (2019).
[15] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. 2020. Conserva-
tive q-learning for offline reinforcement learning. Advances in Neural Information
Processing Systems 33 (2020), 1179â€“1191.
[16] Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. 2020. Offline rein-
forcement learning: Tutorial, review, and perspectives on open problems. arXiv
preprint arXiv:2005.01643 (2020).
[17] Chunming Liu, Xin Xu, and Dewen Hu. 2014. Multiobjective reinforcement
learning: A comprehensive overview. IEEE Transactions on Systems, Man, andCybernetics: Systems 45, 3 (2014), 385â€“398.
[18] Hossam Mossalam, Yannis M Assael, Diederik M Roijers, and Shimon White-
son. 2016. Multi-objective deep reinforcement learning. arXiv preprint
arXiv:1610.02707 (2016).
[19] Mathieu Reymond, Eugenio Bargiacchi, and Ann NowÃ©. 2022. Pareto Conditioned
Networks. In Proceedings of the 21st International Conference on Autonomous
Agents and Multiagent Systems . 1110â€“1118.
[20] Diederik M Roijers, Shimon Whiteson, and Frans A Oliehoek. 2014. Linear
support for multi-objective coordination graphs. In AAMASâ€™14: PROCEEDINGS
OF THE 2014 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS &
MULTIAGENT SYSTEMS , Vol. 2. IFAAMAS/ACM, 1297â€“1304.
[21] Richard S Sutton and Andrew G Barto. 2018. Reinforcement Learning: An Intro-
duction . MIT press.
[22] Philip S Thomas, Bruno Castro da Silva, Andrew G Barto, Stephen Giguere, Yuriy
Brun, and Emma Brunskill. 2019. Preventing undesirable behavior of intelligent
machines. Science 366, 6468 (2019), 999â€“1004.
[23] Philip S Thomas, Joelle Pineau, Romain Laroche, et al .2021. Multi-objective
spibb: Seldonian offline policy improvement with safety constraints in finite
mdps. Advances in Neural Information Processing Systems 34 (2021), 2004â€“2017.
[24] Peter Vamplew, Richard Dazeley, Adam Berry, Rustam Issabekov, and Evan
Dekker. 2011. Empirical evaluation methods for multiobjective reinforcement
learning algorithms. Machine learning 84 (2011), 51â€“80.
[25] Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, MichaÃ«l Mathieu, An-
drew Dudzik, Junyoung Chung, David H Choi, Richard Powell, Timo Ewalds,
Petko Georgiev, et al. 2019. Grandmaster level in StarCraft II using multi-agent
reinforcement learning. Nature 575, 7782 (2019), 350â€“354.
[26] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu,
Linxi Fan, and Anima Anandkumar. 2023. Voyager: An open-ended embodied
agent with large language models. arXiv preprint arXiv:2305.16291 (2023).
[27] Zhendong Wang, Jonathan J Hunt, and Mingyuan Zhou. 2022. Diffusion policies
as an expressive policy class for offline reinforcement learning. arXiv preprint
arXiv:2208.06193 (2022).
[28] Runzhe Wu, Yufeng Zhang, Zhuoran Yang, and Zhaoran Wang. 2021. Offline
constrained multi-objective reinforcement learning via pessimistic dual value
iteration. Advances in Neural Information Processing Systems 34 (2021), 25439â€“
25451.
[29] Yifan Wu, George Tucker, and Ofir Nachum. 2019. Behavior regularized offline
reinforcement learning. arXiv preprint arXiv:1911.11361 (2019).
[30] Jie Xu, Yunsheng Tian, Pingchuan Ma, Daniela Rus, Shinjiro Sueda, and Wojciech
Matusik. 2020. Prediction-guided multi-objective reinforcement learning for
continuous robot control. In International conference on machine learning . PMLR,
10607â€“10616.
[31] Runzhe Yang, Xingyuan Sun, and Karthik Narasimhan. 2019. A generalized algo-
rithm for multi-objective reinforcement learning and policy adaptation. Advances
in neural information processing systems 32 (2019).
[32] Guanjie Zheng, Fuzheng Zhang, Zihan Zheng, Yang Xiang, Nicholas Jing Yuan,
Xing Xie, and Zhenhui Li. 2018. DRN: A deep reinforcement learning framework
for news recommendation. In Proceedings of the 2018 World Wide Web Conference .
167â€“176.
[33] Baiting Zhu, Meihua Dang, and Aditya Grover. 2023. Scaling pareto-efficient
decision making via offline multi-objective rl. arXiv preprint arXiv:2305.00567
(2023).A IMPLEMENTATION DETAILS OF
POLICY-REGULARIZED OFFLINE MORL
The pseudo codes for policy training and adaption are presented
in Algorithm 1 and Algorithm 2. We reverse the sampling order
for target preferences and offline transition samples, which means
that we first collect transition samples and then generate the corre-
sponding target preferences. When ğœƒ=0, the target preferences
are the approximate behavior preferences corresponding to the
samples, and when ğœƒ=1, the target preferences are randomly sam-
pled from the preference space. The behavior cloning preferences
ğœ”bcare sampled uniformly from [ğœ”min
bc,1]to form the augmented
preference, where ğœ”min
bc>0is a hyperparameter to circumvent
instances of a zero value for ğœ”bc.
Here we list several critical hyperparameters in our experiments
on both D4MORL and MOSB. These hyperparameters include the
total training iterations (set at 1ğ‘’6for both datasets), the number
of critics (2 for both datasets), the minimum behavior cloning
weight (ğœ”min
bcset to 0.2 for both datasets), the parameter ğœƒthat
determines whether to exclude preference-inconsistent demonstra-
tions (set to 1 for Diffusion regularization on the Amateur dataset
and 0 for all other cases in D4MORL, while itâ€™s set to 1 in MOSB),
the behavior cloning scale ğœ‚(in D4MORL, it is set to 100 for
Diffusion regularization, 200 for CVAE regularization, and 50 for
MSE regularization; in MOSB, it is set to 0.01 for HalfCheetah, 0.5
for Walker2d, and 0.5 for Hopper), the update iteration for ğœ‡
andğœduring adaptation (3 iterations for both datasets), and the
number of trajectories collected within each iteration during
adaptation (set at 10 for both datasets).
BIMPLEMENTATION DETAILS OF BASELINES
For the baselines MODT (P) and MORvS (P), we use the original hy-
perparameter setting and implementations in the official codebases,
i.e., https://github.com/baitingzbt/PEDA. For the baseline MO-CQL,
we adopt the Gaussian policy and optimize it by minimizing the
losses:
ğ¿actor=âˆ’Eğâˆ¼ğ‘ƒ(Î©)
E(ğ‘ ,ğ‘)âˆ¼ğ·(ğ)
Eğ‘â€²âˆ¼ğœ‹(Â·|ğ‘ ,ğ)
ğğ‘¸(ğ‘ ,ğ‘â€²,ğ) 
,
ğ¿critic=Eğâˆ¼ğ‘ƒ(Î©)
E(ğ‘ ,ğ‘,ğ’“,ğ‘ â€²)âˆ¼ğ·(ğ)
(ğ’šâˆ’ğ‘¸(ğ‘ ,ğ‘,ğ))2+
ğ›¼
ğEğ‘â€²âˆ¼ğœ‹(Â·|ğ‘ ,ğ)
ğ‘¸(ğ‘ ,ğ‘â€²,ğ)
âˆ’ğğ‘¸(ğ‘ ,ğ‘,ğ)
,
(15)
whereğ›¼is a conservative weight, which is tuned to the value in
the set{1.0,2.0,5.0,10.0,20.0}for both D4MORL and MOSB.
C EVALUATION METRICS
Hypervolume (Hv) and sparsity (Sp) are commonly used metrics
for evaluating the relative quality of the approximated Pareto front
among different algorithms, when the true Pareto front is unavail-
able. Below, we provide the formal definitions of hypervolume and
sparsity , along with respective illustration shown in Figure 4.
Definition C.1 (Hypervolume). HypervolumeH(ğ‘ƒ)measures the
volume enclosed by the returns of policies in the approximated
Pareto setğ‘ƒ:
H(ğ‘ƒ)=âˆ«
ğ‘…ğ‘›1ğ»(ğ‘ƒ,ğ‘Ÿ0)(ğ’›)ğ‘‘ğ’›, (16)whereğ»(ğ‘ƒ,ğ’“0)={ğ’›âˆˆğ‘…ğ‘›|âˆƒ1â‰¤ğ‘–â‰¤|ğ‘ƒ|:ğ’“0âª¯ğ’›âª¯ğ‘ƒ(ğ‘–)},ğ‘Ÿ0is
the reference point, ğ‘ƒ(ğ‘–)is the return of ğ‘–thpolicy in P,âª¯is the
dominance relation operator and 1ğ»(ğ‘ƒ,ğ‘Ÿ0)equals 1ifğ’›âˆˆğ»(ğ‘ƒ)and
0otherwise. Higher hypervolumes are better.
Definition C.2 (Sparsity). SparsityS(ğ‘ƒ)measures the density of
policies in the approximated Pareto set ğ‘ƒ:
S(ğ‘ƒ)=1
|ğ‘ƒ|âˆ’1ğ‘›âˆ‘ï¸
ğ‘–=1|ğ‘ƒ|âˆ’1âˆ‘ï¸
ğ‘—=1(Ëœğ‘ƒğ‘–(ğ‘—)âˆ’Ëœğ‘ƒğ‘–(ğ‘—+1))2, (17)
where Ëœğ‘ƒğ‘–is a list of policiesâ€™ return sorted by the component w.r.t.
ğ‘–thobjective, Ëœğ‘ƒğ‘–(ğ‘—)is theğ‘—thorder of return in sorted list Ëœğ‘ƒğ‘–. Lower
sparsity is better.
ğ‘®ğŸ
ğ‘®ğŸHypervolum e
Sparsity
Pareto Front ğ‘‘0
ğ‘‘1Reference Point
Figure 4: Illustration of Hypervolume and sparsity metrics.
D DETAILS OF ENVIRONMENTS AND
DATASETS
The D4MORL dataset is collected from multi-objective Mujoco
environments provided by PGMORL [ 30]. These environments
involve multiple objectives such as running fast, minimizing energy
consumption, and achieving high jumping heights. One can refer
to the PEDA paper [ 33] for more information about the D4MORL
dataset.
Our proposed MOSB dataset is built upon the D4RL Mujoco
dataset [7], which is collected from single-objective behavior poli-
cies. we follow the multi-objective setting of D4MORL and set the
multiple objectives as running fast and minimizing energy con-
sumption. We retain the original state and action of transition data
in the D4RL Mujoco dataset but recalculate the rewards correspond-
ing to two new objectives for each transition in the dataset thereby
constructing vector rewards for subsequent offline MORL. The dis-
tribution of returns on different objectives in the MOSB dataset are
depicted by yellow points in Figure 7.Table 3: D4MORL dataset: the performance of policies trained w or w/o exclusion of preference-inconsistent demonstrations
(i.e.ğœƒ=0orğœƒ=1).
Env metric Diffusion ( ğœƒ=0) Diffusion ( ğœƒ=1) CVAE (ğœƒ=0) CVAE ( ğœƒ=1) MSE (ğœƒ=0) MSE ( ğœƒ=1)AmateurAntHv (106) 6.13Â±.04 6.15Â±.01 5.48Â±.01 4.66Â±.20 5.49Â±.02 3.78Â±.72
Sp (104) 0.90Â±.10 0.95Â±.03 1.35Â±.38 0.31Â±.10 1.81Â±.19 0.20Â±.04
SwimmerHv (104) 3.10Â±.01 3.11Â±.03 3.14Â±.01 2.19Â±.06 3.02Â±.03 0.75Â±.15
Sp (101) 12.19Â±5.35 3.13Â±.42 3.86Â±.62 1.45Â±.47 3.20Â±.62 0.00Â±.00
HalfCheetahHv (106) 5.69Â±.01 5.72Â±.02 5.74Â±.00 4.54Â±.08 5.75Â±.01 3.54Â±.89
Sp (104) 1.41Â±.40 0.53Â±.21 0.51Â±.08 0.01Â±.00 0.57Â±.09 0.00Â±.00
HopperHv (107) 1.84Â±.02 1.88Â±.03 1.75Â±.05 1.46Â±.03 1.83Â±.07 1.55Â±.02
Sp (105) 1.43Â±1.02 0.09Â±.03 0.17Â±.03 0.01Â±.01 0.42Â±.15 0.02Â±.00
Walker2dHv (106) 4.95Â±.01 4.73Â±.01 4.94Â±.02 3.69Â±.21 4.99Â±.07 2.63Â±.13
Sp (104) 1.36Â±.07 3.33Â±.79 0.99Â±.15 1.36Â±.42 0.91Â±.27 0.04Â±.03ExpertAntHv (106) 6.15Â±.02 5.80Â±.09 5.70Â±.04 5.47Â±.07 5.75Â±.04 4.16Â±.55
Sp (104) 0.76Â±.04 2.01Â±.58 2.30Â±1.39 0.59Â±.08 1.50Â±.15 0.51Â±.01
SwimmerHv (104) 3.19Â±.01 2.83Â±.08 3.13Â±.03 1.02Â±.31 3.16Â±.02 2.63Â±.13
Sp (101) 4.58Â±.35 30.08Â±1.89 11.78Â±5.12 9.99Â±9.77 7.79Â±2.33 1.28Â±.21
HalfCheetahHv (106) 5.75Â±.00 5.73Â±.00 5.76Â±.00 4.81Â±.11 5.76Â±.00 2.85Â±.81
Sp (104) 0.37Â±.00 0.45Â±.09 0.58Â±.05 0.04Â±.04 0.59Â±.01 0.02Â±.00
HopperHv (107) 1.46Â±.21 0.96Â±.25 1.84Â±.05 1.63Â±.02 1.98Â±.04 1.63Â±.01
Sp (105) 2.07Â±2.07 4.53Â±.34 3.05Â±1.01 0.19Â±.11 0.72Â±.07 0.05Â±.02
Walker2dHv (106) 5.02Â±.02 4.50Â±.01 4.93Â±.14 3.97Â±.19 4.92Â±.04 3.99Â±.03
Sp (104) 3.18Â±.34 12.21Â±.57 4.01Â±2.01 2.52Â±1.41 2.74Â±.15 0.40Â±.28
Table 4: D4MORL dataset: the performance of MO-CQL w or
w/o exclusion of preference-inconsistent demonstrations (i.e.
ğœƒ=0orğœƒ=1).
Env metric MO-CQL (ğœƒ=0) MO-CQL ( ğœƒ=1)AmateurAntHv (106) 6.25Â±.04 6.28Â±.00
Sp (104) 0.88Â±.11 1.37Â±.02
SwimmerHv (104) 3.16Â±.01 2.94Â±.03
Sp (101) 10.28Â±3.17 11.75Â±.78
HalfCheetahHv (106) 5.74Â±.01 5.59Â±.01
Sp (104) 0.54Â±.12 4.11Â±.16
HopperHv (107) 1.80Â±.01 1.84Â±.00
Sp (105) 1.28Â±.06 4.57Â±3.88
Walker2dHv (106) 4.89Â±.01 4.15Â±.03
Sp (104) 4.62Â±.30 33.14Â±15.28ExpertAntHv (106) 6.19Â±.02 6.12Â±.01
Sp (104) 0.96Â±.11 1.55Â±.21
SwimmerHv (104) 3.21Â±.01 2.26Â±.04
Sp (101) 4.45Â±1.37 0.02Â±.01
HalfCheetahHv (106) 5.70Â±.01 5.65Â±.02
Sp (104) 0.72Â±.17 1.05Â±.12
HopperHv (107) 1.45Â±.00 1.71Â±.06
Sp (105) 2.29Â±.00 3.47Â±.75
Walker2dHv (106) 4.54Â±.05 2.55Â±.03
Sp (104) 13.51Â±1.73 0.00Â±.00E ADDITIONAL RESULTS
E.1 Ablation on Exclusion of
Preference-inconsistent Demonstrations
Table 3 provide a comprehensive overview of policy performance,
with and without the exclusion of preference-inconsistent demon-
strations, respectively. We observed that the excluding preference-
inconsistent demonstrations significantly improves the policy per-
formance, especially for MSE regularization and CVAE regulariza-
tion, which are more vulneratble to the misguidance of preference-
inconsistent demonstrations. In contrast, the Diffusion regulariza-
tion method appears to be more resilient to preference-inconsistent
demonstrations. On the amateur dataset, the Diffusion regulariza-
tion method with exclusion of preference-inconsistent demonstra-
tions even slightly outperforms the approach without exclusion.
Besides, to further explore the effect of the exclusion technique,
we combine MO-CQL with the exclusion of preference-inconsistent
demonstrations, and present the comparative performance in Ta-
ble 4. The results demonstrate a performance improvement when
incorporating the PrefID exclusion into MO-CQL, which verifies
the effectiveness of the exclusion technique as well as the potential
benefit of combining it with other offline MORL approaches.
E.2 Ablation on Regularization Weight
Adaptation
To further verify the effectiveness of Regularization Weight Adapta-
tion in improving final performance, we compare the performance
of learned policies under different regularization weight settings: 1)
Fixed : using a fixed regularization weight; 2) Adapting : evaluat-
ing average performance of 30samples collected for regularization
weight adaptation; 3) Adapted : utilizing the regularization weight
obtained via regularization weight adaptation; and 4) Oracle : em-
ploying the best-performing regularization weight from 20equidis-
tant point within the range of regularization weight. We use theperformance under the Oracle setting to approximate the upper
performance bound achievable through tuning the regularization
weight. Note that the results of our approach presented in the full
paper are generated under the Adapted setting. The results are
presented in Table 5.
We can observe that policies using the adapted regularization
weight exhibit better performance in both Hv and Sp compared to
a fixed regularization weight. Additionally, the stable performance
under the Adapting setting indicates that there is no significant
performance degradation during regularization weight adaptation,
thereby avoiding additional risks from online interactions. Finally,
the outstanding performance under the Oracle setting demon-
strates the potential of our approach for achieving better perfor-
mance with an increase in the deployment time.
Table 5: Results on D4MORL Amateur dataset under different
regularization weight settings.
Env metric Fixed Adapting Adapted Oracle
AntHV ( 106)6.11 6.12Â±.01 6.15Â±.01 6.23Â±.02
SP (104)1.22 0.59Â±.05 0.95Â±.03 0.74Â±.14
SwimmerHV ( 104)3.09 3.10Â±.02 3.11Â±.03 3.13Â±.02
SP (101)4.32 2.25Â±.37 3.13Â±.42 2.64Â±.19
HalfCheetahHV ( 106)5.72 5.72Â±.02 5.72Â±.02 5.73Â±.02
SP (104)0.51 0.29Â±.06 0.53Â±.21 0.57Â±.23
HopperHV ( 107)1.82 1.85Â±.04 1.88Â±.03 1.91Â±.03
SP (105)0.09 0.03Â±.02 0.09Â±.03 0.08Â±.03
Walker2dHV ( 106)4.88 4.79Â±.03 4.97Â±.02 5.02Â±.01
SP (104)2.55 4.39Â±.85 1.06Â±.17 0.60Â±.07
E.3 Performance on Expected Utility Metric
The expected utility (EU) metric is calculated by:
EU=Eğâˆ¼ğ‘ƒdeploy(ğ)"
Eğœ‹(Â·|ğ)"
ğğ‘‡âˆ‘ï¸
ğ‘¡ğ’“(ğ‘ ğ‘¡,ğ‘ğ‘¡)##
, (18)
whereğ‘ƒdeploy(ğ)is the preference distribution during deployment.
We assume that ğ‘ƒdeploy(ğ)is a uniform distribution on the whole
feasible preference space, and then approximate the expectation
Eğâˆ¼ğ‘ƒdeploy(ğ)[Â·]by averaging over 101 equidistant preference vec-
tor. The EU performance on the D4MORL are presented on Table 6.Table 6 demonstrates competitive or superior performance of our
algorithm in the EU metric, aligning with the outcomes assessed by
Hv and Sp metrics. We note a significant advantage of our algorithm
in Hopper and Walker2d environments. This is because, in these en-
vironments, most solutions derived by PEDA for out-of-distribution
target preferences significantly stray from the Pareto front, fall into
the inferior zone and thus exhibit low utilities. These low-utility so-
lutions are more prominently reflected in the EU metric compared
to the Hv and Sp metrics, since they are excluded from compu-
tation of Hv and Sp metrics as dominated solutions. In contrast
to PEDA, most solutions of our algorithm for out-of-distribution
target preferences remain consistently close to the Pareto front and
thus exhibit comparatively high utility. This outcome demonstrates
superior performance stability of our algorithm in dealing with
out-of-distribution preferences.
E.4 Relationship between adapted
regularization weight and offline data
distribution
In Figure 5, we present the data density under different behavior
preferences using the orange line. A higher point on the orange
line indicates a larger number of trajectories with the respective
behavior preference. Additionally, we display a heatmap within the
same subplot where brighter areas signify higher utility obtained
by using the corresponding regularization weight under the corre-
sponding preference. We observe a strong correlation between the
optimal regularization weight and the distribution of data under
various behavior preferences. In some environments (e.g., such as
HalfCheetah-amateur-uniform, Swimmer-amateur-uniform, and
HalfCheetah-medium-expert), a higher data density is associated
with a larger optimal regularization weight, while in other envi-
ronments (e.g., HalfCheetah-medium-replay and Walker2d-mateur-
uniform), the opposite trend is observed.
E.5 Approximate Pareto Front on all
Environments
The approximate Pareto fronts generated from our methods on
D4MORL and MOSB datasets are depicted in Figure 6 and Figure 7,
respectively. The results demonstrate that our method consistently
achieves a broadly expanded approximate Pareto front with a dense
set of learned policies, using datasets collected from both multi-
preference and single-preference behavior policies.Table 6: Expected Utility (EU) performance on D4MORL Amateur and Expert datasets.
Env metric BC (P) MO-CQL MODT (P) MORvS (P) Diffusion CVAE MSEAmateurAnt EU ( 103)1.46Â±.32 2.30Â±.00 2.14Â±.01 2.15Â±.01 2.28Â±.00 2.03Â±.042.05Â±.02
Swimmer EU ( 103)0.14Â±.00 0.16Â±.00 0.09Â±.00 0.14Â±.00 0.17Â±.00 0.16Â±.000.15Â±.00
HalfCheetah EU ( 103)2.00Â±.16 2.20Â±.01 2.21Â±.01 2.10Â±.05 2.26Â±.00 2.20Â±.002.20Â±.00
Hopper EU ( 103)1.10Â±.29 3.21Â±.09 1.41Â±.34 1.40Â±.11 4.06Â±.00 3.21Â±.283.34Â±.24
Walker2d EU ( 103)1.02Â±.08 1.29Â±.37 0.94Â±.03 1.71Â±.07 2.01Â±.01 1.97Â±.031.99Â±.01ExpertAnt EU ( 103)1.69Â±.27 2.26Â±.01 2.18Â±.02 2.19Â±.01 2.27Â±.00 2.04Â±.092.08Â±.04
Swimmer EU ( 103)0.16Â±.00 0.15Â±.00 0.16Â±.00 0.16Â±.00 0.15Â±.01 0.15Â±.010.15Â±.00
HalfCheetah EU ( 103)2.16Â±.01 2.26Â±.01 2.20Â±.01 2.18Â±.00 2.25Â±.00 2.19Â±.002.20Â±.01
Hopper EU ( 103)0.65Â±.25 2.42Â±.40 1.54Â±.04 1.64Â±.22 2.58Â±.20 1.77Â±.162.68Â±.39
Walker2d EU ( 103)0.73Â±.12 0.86Â±.04 1.51Â±.08 1.61Â±.01 1.86Â±.06 1.82Â±.131.83Â±.10
0.0 0.2 0.4 0.6 0.8 1.00.20.40.60.81.0Regularization weight    HalfCheetah-amateur-uniform    
0.00.51.01.52.0
0.0 0.2 0.4 0.6 0.8 1.00.20.40.60.81.0    Walker2d-amateur-uniform    
0.00.51.01.5
0.0 0.2 0.4 0.6 0.8 1.00.20.40.60.81.0    Swimmer-amateur-uniform    
0.00.20.40.60.81.01.2
0.0 0.2 0.4 0.6 0.8 1.00.20.40.60.81.0    HalfCheetah-medium-replay    
012345
0.0 0.2 0.4 0.6 0.8 1.00.20.40.60.81.0    HalfCheetah-medium    
010203040
0.0 0.2 0.4 0.6 0.8 1.00.20.40.60.81.0    HalfCheetah-medium-expert    
0510152025
Data density
0.0 0.2 0.4 0.6 0.8
Normalized utilityPreference (the component on objective 1)
Figure 5: Relationship between adapted regularization weight and offline data distribution. The orange line represents the
data density under different behavior preference in offline dataset. The heatmap indicates the utility under various target
preferences and different regularization weights.0.5 1.0 1.5 2.0 2.5
1e312Ant
expert
objective 21e3Diffusion
0.5 1.0 1.5 2.0 2.5
1e30121e3CVAE
0.5 1.0 1.5 2.0 2.5
1e30121e3MSE
0.0 0.5 1.0 1.5 2.0 2.5
1e20.51.01.5Swimmer
expert
objective 21e2
0 1 2
1e20.51.01.51e2
0 1 2
1e20.51.01.51e2
0.5 1.0 1.5 2.0 2.5
1e30.51.01.52.02.5HalfCheetah
expert
objective 21e3
0.5 1.0 1.5 2.0 2.5
1e30.51.01.52.02.51e3
0.5 1.0 1.5 2.0 2.5
1e30.51.01.52.02.51e3
1 2 3 4
1e3024Hopper
expert
objective 21e3
0 1 2 3 4
1e30241e3
0 1 2 3 4
1e30241e3
0.5 1.0 1.5 2.0 2.5
1e30.51.01.52.02.5Walker2d
expert
objective 21e3
0.5 1.0 1.5 2.0 2.5
1e30.51.01.52.02.51e3
0.0 0.5 1.0 1.5 2.0 2.5
1e31.01.52.02.51e3
0.0 0.5 1.0 1.5 2.0 2.5
1e3012Ant
amateur
objective 21e3
0.0 0.5 1.0 1.5 2.0 2.5
1e30121e3
0.0 0.5 1.0 1.5 2.0 2.5
1e30121e3
0.5 1.0 1.5 2.0 2.5
1e20.500.751.001.251.50Swimmer
amateur
objective 21e2
0.0 0.5 1.0 1.5 2.0 2.5
1e20.51.01.51e2
0.0 0.5 1.0 1.5 2.0 2.5
1e20.51.01.51e2
0.0 0.5 1.0 1.5 2.0 2.5
1e312HalfCheetah
amateur
objective 21e3
0.0 0.5 1.0 1.5 2.0 2.5
1e3121e3
0.0 0.5 1.0 1.5 2.0 2.5
1e3121e3
0 1 2 3 4
1e3024Hopper
amateur
objective 21e3
0 1 2 3 4
1e30241e3
0 1 2 3 4
1e30241e3
0.5 1.0 1.5 2.0
objective 11e30.51.01.52.02.5Walker2d
amateur
objective 21e3
0.5 1.0 1.5 2.0
objective 11e30.51.01.52.02.51e3
0.0 0.5 1.0 1.5 2.0
objective 11e30.51.01.52.02.51e3
Undominated Solutions Offline data Dominated SolutionsFigure 6: The approximate Pareto Front obtained by our methods on all D4MORL environments.2 4 6 8
1e3024HalfCheetah
medium-replay
objective 21e3Diffusion
2 4 6 8
1e30241e3 CVAE
2 4 6 8
1e30241e3 MSE
2 4 6
1e31
0123HalfCheetah
medium
objective 21e3
2 4 6 8
1e312341e3
0.2 0.4 0.6 0.8 1.0
1e4123451e3
0.25 0.50 0.75 1.00 1.25
1e412345HalfCheetah
medium-expert
objective 21e3
0.25 0.50 0.75 1.00 1.25
1e412341e3
0.25 0.50 0.75 1.00 1.25
1e412341e3
0 1 2 3 4
1e301234Walker2d
medium-replay
objective 21e3
0 1 2 3 4
1e301231e3
0 1 2 3 4
1e3012341e3
0 1 2 3 4
1e30123Walker2d
medium
objective 21e3
0 1 2 3 4
1e3012341e3
0 1 2 3 4
1e301231e3
0 1 2 3 4 5
1e30123Walker2d
medium-expert
objective 21e3
0 1 2 3 4 5
1e301231e3
0 1 2 3 4 5
1e301231e3
0 1 2 3
1e3024Hopper
medium-replay
objective 21e3
0 1 2 3
1e3012341e3
0 1 2 3
1e30241e3
1 2 3
1e31234Hopper
medium
objective 21e3
1 2 3
1e312341e3
1 2 3
1e312341e3
1 2 3
objective 11e31234Hopper
medium-expert
objective 21e3
1 2 3
objective 11e312341e3
1 2 3
objective 11e312341e3
Offline data Dominated Solutions Undominated SolutionsFigure 7: The approximate Pareto Front obtained by our methods on all MOSB environments.