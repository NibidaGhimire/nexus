Knowledge Graph Guided Semantic Evaluation of
Language Models For User Trust
Kaushik Roy
Artiﬁcial Intelligence Institute
University of South Carolina
Columbia, SC, USA
kaushikr@email.sc.eduTarun Garg
Birla Institute of Technology
Pilani
f20160450h@alumni.bits-pilani.ac.in
Vedant Palit
Indian Institute of Technology
Kharagpur
vedantpalit@kgpian.iitkgp.ac.inYuxin Zi
Artiﬁcial Intelligence Institute
University of South Carolina
Columbia, SC, USA
yzi@email.sc.edu
Vignesh Narayanan
Artiﬁcial Intelligence Institute
University of South Carolina
Columbia, SC, USA
vignar@sc.eduAmit Sheth
Artiﬁcial Intelligence Institute
University of South Carolina
Columbia, SC, USA
amit@sc.edu
Abstract —A fundamental question in natural language pro-
cessing is - what kind of language structure and semantics is
the language model capturing? Graph formats such as knowl-
edge graphs are easy to evaluate as they explicitly express
language semantics and structure. This study evaluates the
semantics encoded in the self-attention transformers by lever-
aging explicit knowledge graph structures. We propose novel
metrics to measure the reconstruction error when providing
graph path sequences from a knowledge graph and trying to
reproduce/reconstruct the same from the outputs of the self-
attention transformer models. The opacity of language models
has an immense bearing on societal issues of trust and explainable
decision outcomes. Our ﬁndings suggest that language models are
models of stochastic control processes for plausible language pat-
tern generation. However, they do not ascribe object and concept-
level meaning and semantics to the learned stochastic patterns
such as those described in knowledge graphs. Furthermore, to
enable robust evaluation of concept understanding by language
models, we construct and make public an augmented language
understanding benchmark built on the General Language Un-
derstanding Evaluation (GLUE) benchmark. This has signiﬁcant
application-level user trust implications as stochastic patterns
without a strong sense of meaning cannot be trusted in high-
stakes applications.
Index Terms —Knowledge Graph, Graph Neural Networks,
Transformers
I. I NTRODUCTION
Recent studies have studied self-attention models such as
transformers for their ability to encode underlying graph struc-
tures by drawing parallels with graph neural networks (GNNs)[1] Intuitively, there is a correspondence between the self-
attention map in the transformer and the normalized adjacency
matrix in GNNs. Also, there is a correspondence between
GNN node representations and the output value vectors from
a transformer. The multiple routings of the transformer output
through layers of the transformer are similar to multiple graph
convolution aggregations in a GNN. Thus, both transformers
may be an effective way to learn graph contexts between
language tokens. In this study, we aim to test this perceived
equivalence rigorously.
Do transformers encode semantic graphs between input
sequence tokens? We perform simple experiments that feed
various graph path sequence inputs to transformers (we test
with multiple knowledge graphs (KGs) and language models
(LMs)) and try reconstructing the input graph from transformer
outputs. In our experiments, we ﬁnd that in doing so, a high re-
construction error is observed for certain types of graph paths,
paths that require strongly typed real-world concept level
knowledge (e.g., V olvo is typically a type of high-performance
car which is, in turn, a type of car). Several previous works
have performed similar knowledge graph-based reconstruction
experiments. However, they have measured link prediction per-
formance alone and not path predictions [2]. Link prediction
is a weak evaluation of knowledge graph semantics as the
richness of concepts in a knowledge graph comes from graph
paths connecting concepts comprising multiple relationships.
Furthermore, they have not qualitatively analyzed the results of
successful and failed outcomes. In this study, we quantitativelyarXiv:2305.04989v1  [cs.CL]  8 May 2023measure the ability of transformers to predict relationships
and concepts in knowledge graph paths. We also qualitatively
inspect the paths on which the model makes errors to evaluate
their conceptual understanding capabilities.
II. M ETHDOLOGY
First, we extract masked graph paths from the knowledge
graphs for processing by the language model. Figure 1 il-
lustrates the masked graph path extraction process from the
knowledge graph. Next, we predict the masked tokens and
Apple 
is_a
FruitGrape has
Antioxidants 
Watermelon 
1. Apple is_a Fruit 
2. Apple has_Anxtioxidants 
3. Grape  is_a Fruit
4. Grape  has_Antioxidants 
5. Watermelon is_a Fruit 1. Antioxidants has-1 Apple is_a Fruit 
2. Antioxidants has-1 Grape  is_a Fruit 
3. Watermelon is_a Fruit 1. Antioxidants has-1 MASK  is_a Fruit 
2. MASK  has-1 Grape  is_a Fruit 
3. Watermelon is_a MASK Step 1 
Step 2 
Step 3 Step 4 
Fig. 1. Steps 1, 2, and 3 show the process of converting the knowledge graph
links to paths. Step 4 shows the masked inputs to the language model that will
predict the masked tokens. The links are connected two make longer paths
through the use of inverse relationships, e.g., has-1.
calculate the percentage of times the language models assign
the correct token top ﬁve prediction ranks (measured using
softmax over logits). Figure 2 illustrates this process. The ﬁnal
softmax logits obtained can be ranked in order of probability
values. For our evaluation metric, we calculate the percentage
of times the correct answer is within the top ﬁve probabilities.
We call this metric %Top@5 .
III. E XPERIMENTS
We extract approximately 300K knowledge graph links
from the knowledge graphs DBPedia, ConceptNet, Wiktionary,
WordNet, and OpenCyc Ontology [3]. The relationships we
ﬁnd are Antonym, DistinctFrom, EtymologicallyRelatedTo, Lo-
catedNear, RelatedTo, SimilarTo, Synonym, AtLocation, Capa-
bleOf, Causes, CausesDesire, CreatedBy, DeﬁnedAs, Derived-
From, Desires, Entails, ExternalURL, FormOf, HasA, Has-
Context, HasFirstSubevent, HasLastSubevent, HasPrerequi-
site, HasProperty, InstanceOf, IsA, MadeOf, MannerOf, Moti-
vatedByGoal, ObstructedBy, PartOf, ReceivesAction, SenseOf,
SymbolOf, and UsedFor . The data can be found at this link.
For the language models, we use bert-base-uncased, bert-large,
GPT-Neo small, medium, and large with 0.1B, 0.3B, 1B, 2.7B,
and 6B parameters, respectively (B stands for billion) [4].
A. Quantitative Results
Figure 3 shows the quantitative results. We explain the
results in the ﬁgure caption due to space limitations.
eAntioxidants 
Antioxidants has-1 MASK  is_a Fruit ehas-1eMASK eis_aeFruit…Tokenized Input 
Representation 
Input Text 
…Self-Attention Softmax over 
logits 
eAntioxidants ehas-1eMASK eis_aeFruit…{Antioxidants : 0.1, has-1 : 0.1, Apple : 0.4, Grape : 0.4, …..} 
ehas-1
eMASK 
eis_a
eFruitFig. 2. The ﬁgure shows how the masked graph path inputs are processed
through the self-attention transformer models to obtain softmax logit outputs.
Fig. 3. The X-axis denotes the number of parameters in billions, and the
Y-axis measures the %Top@5 . The performance measured using %Top@5
increases steadily with the number of model parameters. However, after a
certain amount of parameters is reached ( 1 billion), the performance starts
to ﬂat-line. The variance across different runs remains signiﬁcant (   5),
although it also shows a decreasing trend with increased model parameters.
B. Qualitative Results
We manually inspect the knowledge graph paths at which
the language models fail, which we will call false paths . Inter-
estingly, the false paths almost exclusively involve knowledge
of strongly typed objects and their properties as seen in the
real world. Some examples include “volvo IsA car CapableOf
slow down”, “retrograde motion HasContext astronomy IsA
physics”, “handicapped SimilarTo unﬁt RelatedTo unhealthy”,
and “ultimate frisbee IsA ﬁeld game IsA outdoor game”.
The remaining examples are at this link. This ﬁnding is
particularly interesting as it supports third-party observationsabout language models’ fundamental lack of a conceptual
world model when asked about physics-related questions (e.g.,
block-stacking) [5].
IV. T HEGLUE B ENCHMARK
The General Language Understanding Evaluation (GLUE)
benchmark revolutionized the evaluation of LMs with several
challenging language understanding phenomena, speciﬁcally,
Textual Entailment (TE), Textual Similarity (TS), and Natural
Language Inference (NLI) [6]. The success of the GLUE
benchmark directly led to rapid advances in language under-
standing evaluation benchmarks such as SuperGLUE, KILT,
and BIG-BENCH, to name a few [7]–[9]. We identify deﬁning
characteristics of these benchmarks and explain why such
characteristics are meaningless without further qualiﬁcation
by aligning with conceptual semantics such as those found
in knowledge graphs. We ﬁrst describe TE, TS, and NLI phe-
nomena as they are currently established and deﬁned. Then, we
report shortcomings resulting from their incomplete deﬁnitions
due to non-alignment with knowledge graph semantics.
a) Natural Language Inference (NLI): The GLUE tasks
- MNLI (Multi-genre Natural Language Inference), QNLI
(Question Answering Natural Language Inference), and WNLI
(Winograd Natural Language Inference) test NLI capabili-
ties from varying angles. MNLI tests whether the model
can appropriately judge if a sentence logically follows from
another, i.e., logical entailment. QNLI tests similar logical
entailment between question and statement pairs - does it make
logical sense to ask a follow-up question? WNLI tests logical
entailment in the presence of pronouns and the nouns they
reference.
b) Textual Entailment (TE): The GLUE task RTE (Rec-
ognizing Textual Entailment) tests for logical entailment sim-
ilar to the NLI task MNLI. However, RTE emphasizes the
meaning - given two text fragments, whether the meaning of
one can be entailed (or can be inferred) from the other.
c) Textual Similarity (TS): The GLUE task QQP (Quora
Question Pairs) tests for the ability to assess the semantic
equivalence, measured as the similarity between a pair of
questions that appear on the social media forum Quora.
Together, the TE, TS, and NLI phenomena constitute
the fundamental “characteristics” of language understanding.
Many other characteristics deﬁne human-like language under-
standing, such as abstraction, analogy, and implicit mentions.
We posit that if the TE, TS, and NLI characteristics are
satisfactorily incorporated into an LM’s basic skill set, those
other characteristics can arise through compositions of this
basic skill set.
V. C ONSTRUCTING THE MODIFIED GLUE B ENCHMARK
A. Knowledge Graphs Augmented GLUE
We now augment the GLUE benchmark using the Knowl-
edge Graphs (KGs) DBPedia, ConceptNet, Wiktionary, Word-
Net, and the OpenCyc Ontology which consist of semantic
associations as networks of connected objects and their rela-
tionships [3], [10], [11]. These KGs are human-curated andcarry ontological commitments (a good majority of humans
that curated the knowledge agree on a common semantic
interpretation of the KGs). For the ﬁrst step of building the
new benchmark, we augment the task datasets described in
Section IV, using order one and order two (one-hop and two-
hop paths) associations from the aforementioned knowledge
sources. Tokenization of the data points is essential when
querying knowledge sources for paths. Therefore, we deﬁne
tokenization tailored to speciﬁc sources from which we extract
knowledge paths based on manual inspection and determina-
tion of optimal practices and heuristics. For example, one of
the tokenization techniques we use is using a sliding window
on the input data and extracting tokens of span length 1-5
as we found entity mentions of span length 5 (e.g., Standard
and Poor Index 500). We limit the augmentation to order two
paths as we ﬁnd higher-order paths introduce noisy concepts
and paths to the dataset and can, therefore, hinder the semantic
correctness of the dataset.
The relationships we ﬁnd are the same as described in
Section III. We make our augmented dataset available at this
link.
Concept Understanding Evaluation Metrics: Our aug-
mented dataset supports the evaluation metric for knowledge
graph path alignment, i.e., %Top@5 , introduced in Section
II as well as standard performance scores of accuracy, F1
score, precision, and recall, both being necessary to conclu-
sively determine a language model’s conceptual understanding
capabilities.
VI. C ONCLUSION
This paper opens the black-box language models’ ability
to model knowledge graph semantics by proposing masked
prediction tasks on graph paths. We do this to understand a
language model’s conceptual understanding and its bearings
on application-level user trust issues. We introduce metrics
for the evaluation of the results and also manually inspect the
outcomes.
Our ﬁndings suggest that language models are models of
stochastic control processes for plausible language pattern
generation. However, they do not ascribe object and concept-
level meaning and semantics to the learned stochastic patterns
such as those described in knowledge graphs. This has signif-
icant application-level user trust implications for applications
requiring concept-level understanding (e.g., healthcare) and
physical simulations (e.g., war-time strategies). Our ﬁndings
suggest that using language models alone, which are stochastic
control models, to drive high-stake application-level decisions
would be highly unsafe and irresponsible. Finally, the paper
constructs and makes public a knowledge-augmented GLUE
benchmark that can foster the development of trustworthy
language models through concept grounding in human-curated
knowledge sources such as knowledge graphs. We also touch
upon how novel evaluation metrics combined with such aug-
mented datasets can be used to further increase the reliability
and robustness of language model evaluations with regard to
their ability to accurately encode conceptual understanding.ACKNOWLEDGEMENTS
This work was supported in part by the National Sci-
ence Foundation under Grant 2133842, “EAGER: Advancing
Neuro-symbolic AI with Deep Knowledge-infused Learning”
and was carried out under the advisement of Prof. Amit Sheth
[12], [13].
REFERENCES
[1] E. Choi, Z. Xu, Y . Li, M. Dusenberry, G. Flores, E. Xue, and A. Dai,
“Learning the graphical structure of electronic health records with graph
convolutional transformer,” in Proceedings of the AAAI conference on
artiﬁcial intelligence , vol. 34, pp. 606–613, 2020.
[2] H. Ma and D. Z. Wang, “A survey on few-shot knowledge graph com-
pletion with structural and commonsense knowledge,” arXiv preprint
arXiv:2301.01172 , 2023.
[3] R. Speer, J. Chin, and C. Havasi, “Conceptnet 5.5: An open multilingual
graph of general knowledge,” in Proceedings of the AAAI conference on
artiﬁcial intelligence , vol. 31, 2017.
[4] R. Gozalo-Brizuela and E. C. Garrido-Merchan, “Chatgpt is not all you
need. a state of the art review of large generative ai models,” arXiv
preprint arXiv:2301.04655 , 2023.
[5] E. M. Bender, T. Gebru, A. McMillan-Major, and S. Shmitchell, “On
the dangers of stochastic parrots: Can language models be too big?,” in
Proceedings of the 2021 ACM conference on fairness, accountability,
and transparency , pp. 610–623, 2021.
[6] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman,
“Glue: A multi-task benchmark and analysis platform for natural lan-
guage understanding,” arXiv preprint arXiv:1804.07461 , 2018.
[7] A. Wang, Y . Pruksachatkun, N. Nangia, A. Singh, J. Michael, F. Hill,
O. Levy, and S. Bowman, “Superglue: A stickier benchmark for general-
purpose language understanding systems,” Advances in neural informa-
tion processing systems , vol. 32, 2019.
[8] F. Petroni, A. Piktus, A. Fan, P. Lewis, M. Yazdani, N. De Cao,
J. Thorne, Y . Jernite, V . Karpukhin, J. Maillard, et al. , “Kilt: a
benchmark for knowledge intensive language tasks,” arXiv preprint
arXiv:2009.02252 , 2020.
[9] A. Srivastava, A. Rastogi, A. Rao, A. A. M. Shoeb, A. Abid, A. Fisch,
A. R. Brown, A. Santoro, A. Gupta, A. Garriga-Alonso, et al. , “Beyond
the imitation game: Quantifying and extrapolating the capabilities of
language models,” arXiv preprint arXiv:2206.04615 , 2022.
[10] S. Auer, C. Bizer, G. Kobilarov, J. Lehmann, R. Cyganiak, and Z. Ives,
“Dbpedia: A nucleus for a web of open data,” in The semantic web ,
pp. 722–735, Springer, 2007.
[11] C. Matuszek, M. Witbrock, J. Cabral, and J. DeOliveira, “An introduc-
tion to the syntax and content of cyc,” UMBC Computer Science and
Electrical Engineering Department Collection , 2006.
[12] A. Sheth, K. Roy, and M. Gaur, “Neurosymbolic ai-why, what, and how,”
arXiv preprint arXiv:2305.00813 , 2023.
[13] A. Sheth, M. Gaur, K. Roy, and K. Faldu, “Knowledge-intensive
language understanding for explainable ai,” IEEE Internet Computing ,
vol. 25, no. 5, pp. 19–24, 2021.