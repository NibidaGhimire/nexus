Maria Curie-Skłodowska University in Lublin
Faculty of Mathematics, Physics and Computer Science
Maryia Shpak
Structure Learning and Parameter Estimation
for Graphical Models via Penalized Maximum
Likelihood Methods
PhD dissertation
Supervisor
dr hab. Mariusz Bieniek, prof. UMCS
Institute of Mathematics
University of Maria Curie-Sklodowska
April 2022arXiv:2301.13269v1  [stat.ML]  30 Jan 2023Abstract
Probabilisticgraphicalmodels(PGMs)provideacompactandﬂexibleframeworktomodel
very complex real-life phenomena. They combine the probability theory which deals
with uncertainty and logical structure represented by a graph which allows to cope with
the computational complexity and also interprete and communicate the obtained know-
ledge. In the thesis we consider two diﬀerent types of PGMs: Bayesian networks (BNs)
which are static, and continuous time Bayesian networks which, as the name suggests,
have temporal component. We are interested in recovering their true structure, which
is the ﬁrst step in learning any PGM. This is a challenging task, which is interesting
in itself from the causal point of view, for the purposes of interpretation of the model
and the decision making process. All approaches for structure learning in the thesis
are united by the same idea of maximum likelihood estimation with LASSO penalty.
Theproblemofstructurelearningisreducedtotheproblemofﬁndingnon-zerocoeﬃcients
in the LASSO estimator for a generalized linear model. In case of CTBNs we consider
the problem both for complete and incomplete data. We support the theoretical results
with experiments.
Keywords and phrases: Probabilistic graphical models, PGM, Bayesian
networks, BN, continuous time Bayesian networks, CTBN, maximum like-
lihood, LASSO penalty, structure learning, Markov Jump Process, MJP,
Markov chain, Markov chain Monte Carlo, MCMC, Stochastic Proximal Gra-
dient Descent, drift condition, incomplete data, Expectation-Maximization,
EM.
iiAcknowledgements
ThroughouttheprocessofwritingthisthesisIhavereceivedalotofsupportandassistance
and I wish to express my gratitude.
First, I would like to thank my supervisor, Professor Mariusz Bieniek, who was a great
support during this challenging process. His curiosity, open-mindedness and extensive
knowledge gave me a chance to research things that are outside of his main ﬁeld of
expertise, and his strive for quality and perfection never let me settle for mediocre results.
Next, I want to thank my second advisor Professor Błażej Miasojedow from University
ofWarsaw, whointroducedustotheﬁeldofprobabilisticgraphicalmodelsandsomeother
areas of statistics, stochastic processes and numerical approximation. His great expertise
and enormous patience allowed me to gain massive knowledge and understanding of these
ﬁelds, when sometimes I did not believe I could.
I also would like to thank dr. Wojciech Rejchel from Nicolaus Copernicus University in
Toruń , whose expertise in model selection was key in the analysis of theoretical properties
of our novel methods for structure learning. I wish to thank mgr. Grzegorz Preisbich and
mgr. Tomasz Cąkała for making many numerical results for our methods possible.
I want to thank my university, Maria Skłodowska-Curie University, for an academic
leave giving me the opportunity to ﬁnish the dissertation and some additional funding.
The part of the research was also supported by the Polish National Science Center grant:
NCN contract with the number UMO-2018/31/B/ST1/00253. Also I would like to show
my appreciation to other people from my university helping me in various ways, among
them are Professor Maria Nowak, Professor Jarosław Bylina, Professor Tadeusz Kuczu-
mow, Professor Jurij Kozicki and many others.
Finally, I would like to thank my parents, Pavel and Natallia, who were always there
for me to guide me and help me through years of research, without their support this
thesis would not be possible. I also wish to extend my special thanks to my dear friends
for their emotional support and helping me to stay disciplined, especially I thank Elvira
Tretiakova and Olga Kostina.
iiiContents
Abstract ii
Acknowledgements iii
1 Introduction 1
1.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
1.2 Probabilistic Graphical Models . . . . . . . . . . . . . . . . . . . . . . . . 3
1.3 Overview of the thesis and its contributions . . . . . . . . . . . . . . . . . 4
2 Preliminaries 6
2.1 Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
2.2 Bayesian networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
2.3 Continuous Time Markov Processes . . . . . . . . . . . . . . . . . . . . . . 12
2.4 Conditional Markov Processes . . . . . . . . . . . . . . . . . . . . . . . . . 14
2.5 Continuous time Bayesian networks . . . . . . . . . . . . . . . . . . . . . . 15
2.6 The LASSO penalty . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
3 Statistical inference for networks with known structure 21
3.1 Learning probabilities in BNs . . . . . . . . . . . . . . . . . . . . . . . . . 21
3.2 Inference in Bayesian networks . . . . . . . . . . . . . . . . . . . . . . . . . 25
3.3 Learning probabilities in BNs for incomplete data . . . . . . . . . . . . . . 47
3.4 Learning parameters for CTBNs . . . . . . . . . . . . . . . . . . . . . . . . 49
3.5 Inference for CTBNs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
4 Structure learning for Bayesian networks 59
4.1 Problem of learning structure of Bayesian Networks . . . . . . . . . . . . . 59
4.2 Partition MCMC method . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
4.3 The novel approach to structure learning . . . . . . . . . . . . . . . . . . . 62
4.4 Discrete case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71
4.5 Numerical results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73
5 Structure learning for CTBNs for complete data 77
5.1 Notation and preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . 77
5.2 Main results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81
iv5.3 Proofs of the main results . . . . . . . . . . . . . . . . . . . . . . . . . . . 83
5.4 Numerical examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93
5.5 Extension of the results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96
6 Structure learning for CTBNs for incomplete data 98
6.1 Introduction and notation . . . . . . . . . . . . . . . . . . . . . . . . . . . 98
6.2 Sampling the Markov chain with Rao and Teh’s algorithm . . . . . . . . . 100
6.3 Structure learning via penalized maximum likelihood function . . . . . . . 102
6.4 Numerical results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114
6.5 FFBS Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117
7 Conclusions and discussion 119
vChapter 1
Introduction
1.1 Motivation
It is a common knowledge that we live in the world where data plays crucial role in many
areas and applications of great importance for our society and the importance of data is
still growing. The amount of data in the world is now estimated in dozens of zettabytes,
and by 2025 the amount of data generated dailyis expected to reach hundreds of exa-
bytes. There is a demand for models and algorithms that can deal with these amounts
of data eﬀectively ﬁnding useful patterns and providing better insights into the data. On
top of it, most environments require reasoning under uncertainty. Probabilistic graphical
models (PGMs) provide such a framework that allows to deal with these and many other
challenges in various situations. The models combine the probability theory which deals
with uncertainty in a mathematically consistent way, and logical structure which is repre-
sented by a graph encoding certain independence relationships among variables allowing
to cope with the computational complexity.
PGMs encode joint distributions over a set of random variables (often of a signiﬁcant
amount) combining the graph theory and probabilities, which allows to represent many
complex real-world phenomena compactly and overcome the complexity of the model
which is exponential in the number of variables. There are also some other advantages
that these models have. Namely, because of their clear structure, PGMs enable us to
visualize, interprete and also communicate the gained knowledge to others as well as
make decisions. Some models, for example Bayesian networks, have directed graphs in
their core and oﬀer ways to establish causality in various cases. Moreover, graphical
models allow us not only to ﬁt the observed data but also elegantly incorporate prior
knowledge, e.g. from experts in the domain, into the model. Besides, certain models take
into account a temporal component and consider systems’ dynamics in time.
Graphical models are successfully applied to a large number of domains such as image
processing and object recognition, medical diagnosis, manufacturing, ﬁnance, statistical
physics, speech recognition, natural language processing and many others. Let us brieﬂy
present here a few examples of various applications.
Bayesian networks, one of the PGMs considered in this thesis, are extensively used in
1the development of medical decision support systems helping doctors to diagnose patients
more accurately. In the work by Wasyluk et al. (2001) the authors built and described a
probabilistic causal model for diagnosis of liver disorders. In the domain of hepatology,
inexperienced clinicians have been found to make a correct diagnosis in jaundiced patients
in less than 45% of the cases. Moreover, the number of cases of liver disorders is on
the rise and, especially at early stages of a disease, the correct diagnosis is diﬃcult yet
critical, because in many cases damage to the liver caused by an untreated disorder
may be irreversible. As we already mentioned and as it is stressed out in the work
above, a huge advantage that these models have is that they allow to combine existing
frequency data with expert judgement within the framework as well as update themselves
when the new data are obtained, for example patients data within a hospital or a clinic.
What is also important in the medical diagnosis is that PGMs, Bayesian networks in
particular, eﬃciently model simultaneous presence of multiple disorders, which happens
quite often, but in many classiﬁcation approaches the disorders are considered to be
mutually exclusive. The overall model accuracy, as the authors Wasyluk et al. (2001)
claim, seems to be better than that of beginning diagnosticians and reaches almost 80%,
which can be used for the diagnosis itself as well as the way to help new doctors to
learn the strategy and optimization of the diagnosis process. A few other examples of
the PGMs application in medical ﬁeld are management of childhood malaria in Malawi
(Bathla Taneja et al. (2021)), estimating risk of coronary artery disease (Gupta et al.
(2019)), etc.
The next popular area of graphical models application is computational biology, for
example Gene Regulatory Network (GRN) inference. GRN consists of genes or parts of
genes, regulatory proteins and interactions between them and plays a key role in medi-
ating cellular functions and signalling pathways in cells. Accurate inference of GRN for
a speciﬁc disease returns disease-associated regulatory proteins and genes, serving as po-
tential targets for drug treatment. Chen and Xuan (2020) argued that Bayesian inference
is particularly suitable for GRNs as it is very ﬂexible for large-scale data integration,
because the main challenge of GRNs is that there exist hundreds of proteins and tens
of thousands of genes with one protein possibly regulating hundreds of genes and their
regulatory relationship may vary across diﬀerent cell types, tissues, or diseases. More-
over, the estimation is more robust and easier to compare on multiple datasets. Chen
and Xuan (2020) demonstrated this by applying their model to breast cancer data and
identiﬁed genes relevant to breast cancer recurrence. As another example in this area,
Sachs et al. (2005) used Bayesian network computational methods for derivation of causal
inﬂuences in cellular signalling networks. These methods automatically elucidated most
of the traditionally reported signalling relationships and predicted novel interpathway
network causalities, which were veriﬁed experimentally. Reconstruction of such networks
might be applied to understanding native-state tissue signalling biology, complex drug
actions, and dysfunctional signalling in diseased cells.
Theuseofprobabilitymodelsisextensivealsoincomputervisionapplications. Intheir
work Frey and Jojic (2005) advocate for the use of PGMs in the computer vision problems
2requiring decomposing the data into interacting components, for example, methods for
automatic scene analysis. They apply diﬀerent techniques in a vision model of multiple,
occluding objects and compare their performances. Occlusion is a very important eﬀect
and one of the biggest challenges in computer vision that needs to be taken into account,
and PGMs are considered to be a good tool to handle that eﬀect. PGMs are also used
for tracking diﬀerent moving objects in video sequences, for example long-term tracking
of groups of pedestrians on the street (Jorge et al. (2007)), where the main diﬃculties
concern total occlusions of the objects to be tracked, as well as group merging and split-
ting. Another example is on-line object tracking (Jorge et al. (2004)) useful in real time
applications such as video surveillance, where authors overcame the problem of needing
to analyze the whole sequence before labelling trajectories to be able to use the tracker
on-line and also the problem of unboundedly growing complexity of the network.
1.2 Probabilistic Graphical Models
In the previous subsection we described the advantages of PGMs and why one might be
interested in studying them. In this work we focus on two types of PGMs: Bayesian
Networks (BN) and Continuous Time Bayesian Networks (CTBN). The ﬁrst term has
rather long history and tracks back to 1980s (Pearl (1985)) whereas the second term is
relatively modern (Nodelman et al. (2002)). The underlying structure for both models
is a directed graph, which can be treated either as a representation of a certain set of
independencies or as a skeleton for factorizing a distribution. In some cases the directions
of arrows in the graph can suggest causality under certain conditions and allow not only
the inference from the data but also intervene into the model and manipulate desired
parameters in the future. BNs are static models, i.e. they do not consider a temporal
component, while in CTBNs as the name suggests we study models in the context of
continuous time. The framework of CTBNs is based on homogeneous Markov processes,
but utilizes ideas from Bayesian networks to provide a graphical representation language
for these systems.
A broad and comprehensive tutorial on existing research for learning Bayesian net-
works and some adjacent models can be found in Daly et al. (2011). The subject of
causality is extensively explored in Spirtes et al. (2000) and Pearl (2000), some references
are also given in Daly et al. (2011). Several examples of the use of BNs were presented
above.
In contrast to regular Bayesian networks, CTBNs have not been studied that well yet.
The most extensive work concerning CTBNs is PhD thesis of Nodelman (2007). Some
related works include for example learning CTBNs in non-stationary domains (Villa and
Stella (2018)), in relational domains (Yang et al. (2016)) and continuous time Bayesian
network classiﬁers (Stella and Amer (2012)). As an example, CTBNs have been suc-
cessfully used to model the presence of people at their computers together with their
availability (Nodelman and Horvitz (2004)), for dynamical systems reliability modeling
3and analysis (Boudali and Dugan (2006)), for network intrusion detection (Xu and Shel-
ton (2008)), to model social networks (Fan and Shelton (2012)), to model cardiogenic
heart failure (Gatti et al. (2012)), and for gene network inference (Stella et al. (2014) or
Stella et al. (2016)).
1.3 Overview of the thesis and its contributions
There are several problems within both the BN and CTBN frameworks. Both of them
have graph structures which need to be discovered and this is considered to be one of the
main challenges in the ﬁeld. This thesis is dedicated exclusively to solving this problem
in both frameworks. Another problem is to learn the parameters of the model: in the
case of BNs it is a set of conditional probability distributions and in the case of CTBNs
it is a set of conditional intensity matrices (for details see Chapter 2). The last problem
is the statistical inference based on the obtained network (details are in Chapter 3).
The thesis is constructed as follows. In Chapter 2 we provide all the necessary pre-
liminaries for better understanding the frameworks of Bayesian networks and continuous
time Bayesian networks. Next, in Chapter 3 we overview known results on learning net-
works’ parameters as well as inference to fully cover the concept of interest. Chapter 4 is
dedicated to the structure learning problem for BNs, where we provide novel algorithms
for both discrete and continuous data. Chapters 5 and Chapter 6 cover the problems
of structure learning for CTBNs in cases of complete and incomplete data, respectively.
Finally, Chapter 7 concludes the thesis with the summary and the discussion of obtained
results.
Algorithms in both Chapters 4 and 5 lean on feature selection in generalized linear
modelswiththeuseofLASSO(LeastAbsoluteShrinkageandSelectionOperator)penalty
function. It relies on the idea of penalizing the parameters of the model, i.e. adding
or subtracting the sum of absolute values of the parameters of the model with some
hyperparameter, in order to better ﬁt the model and perform a variable selection by
forcing some parameters to be equal to 0. The term ﬁrst appeared in Tibshirani (1996).
More on the topic of LASSO can be found for example in Hastie et al. (2015). In Section
2.6 we provide a short description of the concept.
The main contributions of the thesis are collected in Chapters 4, 5 and 6 and they are
as follows:
•we provide the novel algorithm for learning the structure of BNs based on penalized
maximum likelihood function both for discrete and continuous data;
•we present and prove the consistency results for the algorithm in case of continuous
data;
•we compare the eﬀectiveness of our method with other most popular methods for
structure learning applied to benchmark networks of continuous data of diﬀerent
sizes;
4•we provide the novel algorithm for learning the structure of CTBNs based on pena-
lized maximum likelihood function for complete data and present two theoretical
consistency results with proofs;
•we provide the novel algorithm for learning the structure of CTBNs based on pe-
nalized maximum likelihood function for incomplete data where the log-likelihood
function is replaced by its Markov Chain Monte Carlo (MCMC) approximation due
to inability to express it explicitly;
•we present and prove the convergence of the proposed MCMC scheme and the
consistency of the learning algorithm;
•for the mentioned above MCMC approximation we designed the algorithm to pro-
duce necessary samples;
•in both cases of complete and incomplete data we provide results of the simulations
to show the eﬀectiveness of proposed algorithms.
Part of the content (Chapter 5) in its early stages has been published on arXiv:
Shpak, M., Miasojedow, B., and Rejchel, W., Structure learning for CTBNs via pe-
nalized maximum likelihood methods , arXiv e-prints, 2020, https://doi.org/10.48550/
arXiv.2006.07648 .
5Chapter 2
Preliminaries
In this chapter we provide theoretical background on Bayesian networks (BNs), Markov
processes,conditionalMarkovprocessesandcontinuoustimeBayesiannetworks(CTBNs).
We start with the notation common for BNs and CTBNs which we will use through
the whole thesis. Then we provide a few basic deﬁnitions needed to deﬁne and under-
stand the concepts of BNs and CTBNs with their interpretation and examples. Most of
the contents of this chapter comes from the Nodelman et al. (2002), Nodelman (2007),
Koller and Friedman (2009).
2.1 Notation
First, by upper case letters, for example, Xi,B,Y, we denote random variables. In
the case of CTBNs upper case letters represent the whole collection of random variables
indexed by continuous time, hence in this case Xi(t),Y(t)are random variables for par-
ticular time points t.
Values of variables are denoted by lower case letters, sometimes indexed by numbers
or otherwise representing diﬀerent values of the same random variable - e.g. xi,s,s0.
The set of possible values for a variable Xis denoted by Val(X)and byjXjwe will
denote the number of its elements.
Sets of variables are denoted by bold-face upper case letters - e.g. X- and correspon-
ding sets of values are denoted by bold-face lower case letters - e.g. xorx. The set of
possible values and its size is denoted by Val(X)andjXj.
A pairG= (V;E)denotes a directed graph, where Vis the set of nodes and Eis
the set of edges. The notation u!wmeans that there exists an edge from the node uto
the nodew. We will also call them arrows. The set Vnfwgis denoted by w. Moreover,
we deﬁne the set of the parents of the node win the graphGby
paG(w) =fu2V :u!wg:
When there is no confusion, for convenience we sometimes write pa(w)instead of paG(w).
Other useful and relevant locally notation we provide in the corresponding sections.
62.2 Bayesian networks
In this section we provide an overview of Bayesian networks (BNs). We start with the in-
tuition behind BNs followed by the representation of BNs together with its formal deﬁni-
tion and notation. The problems of inference and learning for BNs are considered more
thoroughly in Section 3.2 and Chapter 4 respectively.
The goal is to represent a joint distribution pover some set of random variables
X=fX1;:::;Xng:Even in the simplest case where these variables are binary-valued,
the joint distribution requires the speciﬁcation of 2n 1numbers - the probabilities of
the2ndiﬀerent assignments of the values fx1;:::;xng. The explicit representation of
the joint distribution is hard to handle from every perspective except for small values
ofn. Computationally, it is very expensive to manipulate and generally too large to store
in computer memory. Cognitively, it is impossible to acquire so many numbers from a
human expert; moreover, most of the numbers would be very small and would correspond
to events that people cannot reasonably consider. Statistically, if we want to learn the
distribution from data, we would need ridiculously large amounts of data to estimate so
many parameters robustly (Koller and Friedman (2009)).
Bayesian networks help us specify a high-dimensional joint distribution compactly by
exploiting its independence properties. The key notion behind the BN representation is
conditional independence , which on the one hand allows to reduce amount of estimated
parameters signiﬁcantly and on the other hand, allows to avoid very strong and naive
independence assumptions.
Deﬁnition 2.1. Two random variables XandYareindependent (denoted by X?Y)
if and only if the equality
P(X2A;Y2B) =P(X2A)P(Y2B)
holds for all Borel sets A;BR.
For short, we will write it in the form P(X;Y ) =P(X)P(Y):There is also another
way to think of independence. If the random variables XandYare independent, then
P(X2jY) =P(X2). Intuitively, this says that having evidence about Ydoes not
change the distribution of our beliefs on the occurrence of X.
If we wish to model a more complex domain represented by some set of variables,
it is unlikely that any of the variables will be independent of each other. Conditional
independence is a weaker notion of independence, but it is more common in real-life
situations.
Deﬁnition 2.2. Two random variables XandYareconditionally independent given
a set of random variables C(symbolically X?YjC) if and only if
P(X2A;Y2BjC) =P(X2AjC)P(Y2BjC) (2.1)
holds for all Borel sets A;BR.
7Obviously (2.1) implies
P(X2AjC;Y) =P(X2AjC);
which can be written shortly as
P(XjC;Y) =P(XjC):
So intuitively, the inﬂuence that XandYhave on each other is mediated through
the variables in the set C. It means that, when we have some evidence about variables
fromC, having any additional information about Ydoes not change our beliefs about X.
Let us demonstrate this deﬁnition on a simpliﬁed example. Let Xbe a random variable
representing the case if a person has lung cancer and Yrepresenting the case if the same
person has yellow teeth. These variables are not independent as having yellow teeth is
one of the secondary symptoms of lung cancer. However, when we know that the person
is a smoker knowing that they have yellow teeth does not give us any additional insight on
lung cancer, and vice versa, as we consider smoking to be the reason of both symptoms.
It is easier to believe that in a given domain most variables will not directly aﬀect most
other variables. Instead, for each variable only a limited set of other variables inﬂuence it.
ThisistheintuitionwhichleadstothenotionofaBayesiannetwork Boverasetofrandom
variables Bwhich is a compact representation of a speciﬁc joint probability distribution.
The formal deﬁnition is as follows.
Deﬁnition 2.3. A Bayesian network Bover a set of random variables Bis formed by
•a directed acyclic graph (DAG) Gwhose nodes correspond to the random variables
Bi2B,i= 1;:::;n:
•the set of conditional probability distributions (CPDs) for each Bi, specifying the
conditional distribution P(BijpaG(Bi))ofBias a function of its parent set in G.
The CPDs form a set of local probability models that can be combined to describe
the full joint distribution over the variables Bvia the chain rule:
P(B1;B2;:::;Bn) =nY
i=1P(BijpaG(Bi)): (2.2)
The graphGof a Bayesian network encodes a set of conditional independence assump-
tions. In particular, a variable B2Bis independent of its non-descendants given the set
of its parents paG(B). See for example Figure 2.1 of an Extended Student network taken
from Koller and Friedman (2009). As it can be seen, each variable is connected only to
a small amount of other variables in the network. In this example according to (2.2) the
joint distribution takes the following form:
P(C;D;I;G;S;L;J;H ) =
=P(C)P(DjC)P(I)P(GjD;I)P(SjI)P(LjG)P(JjL;S)P(HjG;J):
8Figure 2.1: The Extended Student network
This example will be considered in more detail further in the thesis.
Now we discuss basic structures for BNs including some examples and give the inter-
pretation of the structures. BNs represent probability distributions that can be formed
via products of smaller, local conditional probability distributions (one for each variable).
If the joint distribution is expressed in this form, it means that the independence assump-
tions for certain variables are introduced into our model. To understand what types of
independencies are described by directed graphs for simplicity let us start from looking at
BNBwith three nodes: X; Y;andZ. In this case,Bessentially has only three possible
structures, each of which leads to diﬀerent independence assumptions.
•Common parent, also called common cause. IfGis of the form X Y!Z, andY
is observed, then X?ZjY. However, if Yis unobserved, then X6?Z. Intuitively
this stems from the fact that Ycontains all the information that determines the
outcomes of XandZ; once it is observed, there is nothing else that aﬀects these
variables’ outcomes. The case with smoking and lung cancer described above is such
an example of common cause. See the illustration (c) in Figure 2.2.
•Cascade, or indirect connection. IfGis of the form X!Y!Z, andYis
observed, then, again X?ZjY. However, if Yis unobserved, then X6?Z. Here,
the intuition is again that Yholds all the information that determines the outcome
ofZ; thus, it does not matter what value Xtakes. In Figure 2.2 in (a) and (b)
there are shown cases of indirect causal and indirect evidential eﬀects, respectively.
•V-structure or common eﬀect , also known as explaining away . IfGis of the form
X!Y Z, then knowing YcouplesXandZ. In other words, X?ZifYis
unobserved, but X6?ZjYifYis observed. See the case (d) in Figure 2.2.
The last case requires additional explanation. Suppose that Yis a Boolean variable
9Figure 2.2: The four possible two-edge trails from XtoYviaZ: (a) An indirect causal
eﬀect; (b) An indirect evidential eﬀect; (c) A common cause; (d) A common eﬀect.
that indicates whether our lawn is wet one morning; XandZare two explanations for it
being wet: either it rained (indicated by X), or the sprinkler turned on (indicated by Z).
If we know that the grass is wet ( Yis true) and the sprinkler did not go on ( Zis false),
then the probability that Xis true must be one, because that is the only other possible
explanation. Hence, XandZare not independent given Y.
To generalize this for a case of more variables and demonstrate the power but also
the limitations of Bayesian networks we will need the notions of d-separation and I-maps.
LetQ;W;andObe three sets of nodes in a Bayesian network Brepresented byG,
where the variables Oare observed. Let us use the notation I(p)to denote the set of all
independencies of the form (Q?WjO)that hold in a joint distribution p. To extend
structures mentioned above to more general networks we can apply them recursively over
any larger graph, which leads to the notion of d-separation.
Recall that we say that there exists an undirected path in Gbetween the nodes u
andwif there exists the sequence v1;:::;vn2Vsuch thatvi!vi+1orvi vi+1for each
i= 0;1;:::;n, wherev0=uandvn+1=w. Moreover, an undirected path in Gbetween
Q2QandW2Wis called activegiven observed variables Oif for every consecutive
triple of variables X;Y;Zon the path, one of the following holds:
•common cause: X Y!ZandY =2O(Yis unobserved);
•causal trail: X!Y!ZandY =2O(Yis unobserved);
•evidential trail: X Y ZandY =2O(Yis unobserved);
•common eﬀect: X!Y ZandYor any of its descendants are observed.
10Figure 2.3: An example for d-separation: X1andX6ared-separated given X2,X3(left),
X2;X3arenotd-separated given X1;X6(right).
Finally, we say that QandWared-separated given Oif there are no active paths bet-
ween any node A2QandB2Wgiven O. See examples for d-separation in Figure 2.3.
In the second example there is no d-separation because there is an active path which
passes through the V-structure created when X6is observed. The notion of d-separation
lets us describe a large fraction of the dependencies that hold in our model. It can be
shown that if QandWared-separated given O, then Q?WjO.
We will write I(G) =f(Q?WjO) :Q;Wared-separated given Ogto denote the
set of independencies corresponding to all d-separations inG. Ifpfactorizes overG, then
I(G)I(p)andpcan be constructed easily. In this case, we say that Gis anI-map
forp. In other words, all the independencies encoded in Gare sound: variables that are
d-separated inGare conditionally independent with respect to p. However, the converse
is not true: a distribution may factorize over G, yet have independencies that are not
captured inG.
So an interesting question here is whether for the probability distribution pwe can
always ﬁnd a perfectmapI(G)for whichI(G) =I(p)or not. The answer is no (see
an example from Koller and Friedman (2009)). Another related question is whether
perfect maps are unique when they exist. This is not the case either, for example, DAGs
X!YandX Yencode the same independencies, yet form diﬀerent graphs. In a ge-
neral case we say that two Bayesian networks B1,B2areI-equivalent if their DAGs encode
the same dependencies I(G1) =I(G2):For a case of three variables we can notice that
graphs (a), (b) and (c) in Figure 2.2 encode the same dependencies, so as long as we
do not turn graphs into V-structures ((d) is the only structure which encodes the de-
pendencyX6?YjZ) we can change directions in them and get I-equivalent graphs.
This brings us to a fact that if G1;G2have the same skeleton (meaning that if we drop
the directionality of the arrows, we obtain the same undirected graph) and the same
V-structures, then I(G1) =I(G2). For the full proof of this statement, other previously
made statements and more information about BNs see Koller and Friedman (2009).
112.3 Continuous Time Markov Processes
In this section we collect auxiliary results on Markov processes with continuous time. We
can think of a continuous time random process Xas a collection of random variables
indexed by time t2[0;1). It is sometimes more convenient to view Xacross all values
oftasasinglevariable, whosevaluesarefunctionsoftime, alsocalledpathsortrajectories.
Deﬁnition2.4. The Markov condition is the assumption that the future of a process is in-
dependent of its past given its present. More explicitly, the process Xsatisﬁes the Markov
property iﬀ P(X(t+t)jX(s);0st) =P(X(t+t)jX(t))for allt;t>0(Chung
and Walsh (2005)).
In this thesis we focus on Markov processes with ﬁnite state space which are basically
deﬁned by initial distribution and a matrix of transition intensities. The framework of
CTBNs is based on the notion of homogeneous Markov processes in which the transition
intensities do not depend on time.
Deﬁnition 2.5. LetXbe a stochastic process with continuous time. Let the state space
ofXbeVal(X) =fx1;x2;:::;xNg. ThenXis a homogeneous Markov process if and
only if its behavior can be speciﬁed in terms of an initial distribution PX
0overVal(X)and
a Markovian transition model usually presented as an intensity matrix
QX=2
66664 q1q12::: q 1N
q21 q2::: q 2N
............
qN1qN2::: qN3
77775; (2.3)
whereqi=P
j6=iqijand all the entries qiandqijare positive.
Intuitively, the intensity qigives the “instantaneous probability” of leaving state xiand
the intensity qijgives the “instantaneous probability” of the jump from xitoxj. More
formally, for i6=j
lim
t!0P(X(t+ t) =xjjX(t) =xi) =qijt+O(t2); (2.4)
and for alli= 1;:::;N
lim
t!0P(X(t+ t) =xijX(t) =xi) = 1 qit+O(t2): (2.5)
Therefore, the matrix QXdescribes the instantaneous behavior of the process Xand also
makes the process satisfy the Markov assumption since it is deﬁned solely in terms of its
current state.
The instantaneous speciﬁcation of the transition model of Xinduces a probability
distribution over the set of its possible trajectories. To see how the distribution is induced,
we must ﬁrst recall the notion of a matrix function.
12Deﬁnition 2.6. The matrix exponential for a matrix Qis deﬁned as
expQ=1X
k=0Qk
k!:
Now the set of Equations (2.4) and (2.5) can be written collectively in the form
lim
t!0P(X(t+ t)jX(t)) = lim
t!0exp(QXt) = lim
t!0 
I+QXt+O(t2)
:(2.6)
So given the matrix QXwe can describe the transient behavior of X(t)as follows. If
X(0) =xithen the process stays in state xifor an amount of time exponentially dis-
tributed with parameter qi. Hence, the probability density function fand the corres-
ponding distribution function Ffor the time when X(t)remains equal to xiare given by
f(t) =qiexp( qit); t0;
F(t) = 1 exp( qit); t0:
The expected time of changing the state is 1=qi. Upon transitioning, Xjumps to the
statexjwith probability qij=qiforj6=i.
Example 2.7.Assume that we want to model the behavior of the barometric pressure B(t)
discretized into three states ( b1=falling,b2=steady, and b3=rising). Then for instance
we could write the intensity matrix as
QB=2
64 0:21 0:2 0:01
0:05 0:1 0:05
0:01 0:2 0:213
75:
Ifweviewunitsoftimeashours, thismeansthatifthepressureisfalling, weexpectthatit
will stop falling in a little less than 5 hours (1/0.21 hours). It will then transition to being
steadywithprobability 0:2=0:210:95andtofallingwithprobability 0:01=0:210:0476.
When the transition model is deﬁned solely in terms of an intensity matrix (as above),
we refer to it as using a pure intensity parameterization. The parameters for an Nstate
process arefqi;qij2QX;1i;jN; i6=jg:
This is not the only way to parameterize a Markov process. Note that the distribution
over transitions of Xfactors into two pieces: an exponential distribution over whenthe
next transition will occur and a multinomial distribution over wherethe process jumps.
This is called a mixed intensity parameterization.
Deﬁnition 2.8. The mixed intensity parameterization for a homogeneous Markov pro-
cessXwithNstates is given by two sets of parameters
qX=fqi;1iNg
and
X=fij;1i;jN; i6=jg;
whereqXis a set of intensities parameterizing the exponential distributions over when
the next transition occurs and Xis a set of probabilities parameterizing the distribution
over wherethe process jumps.
13To relate these two parametrizations we note the following theorem from Nodelman
(2007).
Theorem 2.9. LetXandYbe two Markov processes with the same state space and
the same initial distribution. If Xis deﬁned by the intensity matrix QXgiven by (2.3),
andYis the process deﬁned by the mixed intensity parameterization qY=fq0
1;:::;q0
Ng
andY=f0
ij;i6=jg, thenXandYare stochastically equivalent, meaning they have the
same state space and transition probabilities, if and only if q0
i=qifor alli= 1;:::;Nand
0
ij=qij
qi
for all 1i;jN; i6=j.
2.4 Conditional Markov Processes
In order to compose Markov processes in a larger network, we need to introduce the
notion of a conditional Markov process. This is an inhomogeneous Markov process where
the intensities vary with time, but not as a direct function of time. Rather, the intensities
depend on the current values of a set of other variables, which also evolve as Markov
processes.
LetYbe a process with a state space Val(Y) =fy1;y2;:::;ymg. Assume that Yevolves
as a Markov process Y(t)whose dynamics are conditioned on a set Vof variables, each
of which can also evolve over time. Then we have a conditional intensity matrix (CIM)
which can be written as
QYjV=2
66664 q1(V)q12(V)::: q 1m(V)
q21(V) q2(V)::: q 2m(V)
............
qm1(V)qm2(V)::: qm(V)3
77775:
Equivalently,wecanviewCIMasasetofintensitymatrices QYjvoneforeachinstantiation
of valuesvto the variables V, see Example 2.10. Since the framework of CTBNs which
we consider in the thesis has a graph at its core, we will refer to the set of variables Vas
the set of parents of Yand denote it by paG(Y). Note that if the parent set paG(Y)is
empty, then CIM is simply a standard intensity matrix. Just as a regular intensity matrix,
CIM induces the distribution of the dynamics of Ygiven the behavior of paG(Y) =V.
IfVtakes the value von the interval [t;t+")for some">0, then as in Equation (2.6)
lim
t!0P(Yt+tjYt;v) = lim
t!0exp(QYjvt) = lim
t!0 
I+QYjvt+O(t2)
:
If we specify an initial distribution of Y, then we have deﬁned a Markov process whose
behavior depends on the instantiation vof values of paG(Y).
Example 2.10.Consider a variable E(t)which models whether or not a person is eating
(e1=not eating, e2=eating) conditioned on a variable H(t)which models whether or
14not the person is hungry ( h1=not hungry, h2=hungry). Then we can specify exemplary
CIM forE(t)as
QEjh1="
 0:01 0:01
10 10#
QEjh2="
 2 2
0:01 0:01#
.
For instance, given this model, we expect that a person who is hungry and not eating
is going to start eating in half an hour. Also, we expect a person who is not hungry and
is eating to stop eating in 6 minutes (1/10 hour).
2.5 Continuous time Bayesian networks
In this section we deﬁne the notion of CTBN, which in essence is a probabilistic graphical
model with the nodes as variables, the state evolving continuously over time, and where
the evolution of each variable depends on the state of its parents in the graph.
Before the formal deﬁnition we recall an example from Nodelman et al. (2002). Con-
siderthesituationinmedicalresearchwheresomedrughasbeenadministeredtoapatient
and we wish to know how much time it takes for the drug to have an eﬀect. The answer
to this question will likely depend on various factors, such as how recently the patient ate.
We want to model the temporal process for the eﬀect of the drug and how its dynamics
depends on other factors. In contrast to previously developed methods of approaching
such a problem (e.g. event history analysis, Markov process models) the notion of CTBN
introduced by Nodelman et al. (2002) allows the speciﬁcation of models with a large struc-
tured state space where some variables do not directly depend on others. For example,
the distribution of how fast the drug takes eﬀect might be mediated through how fast
it reaches the bloodstream, which in turn may be aﬀected by how recently the person
ate. Figure 2.4 shows an exemplary graph structure for CTBN modelling the drug eﬀect.
There are nodes for the uptake of the drug and for the resulting concentration of the drug
in the bloodstream. The concentration is also aﬀected by how full patient’s stomach is.
The drug is supposed to alleviate joint pain, which may be aggravated by falling pressure.
The drug may also cause drowsiness. The model contains a cycle, indicating that whether
the person is hungry depends on how full their stomach is, which depends on whether or
not they are eating.
LetG= (V;E)denote a directed graph with possible cycles , whereVis the set of
nodes andEis the set of edges. Further in the context of probabilistic graphical models
we use the terms “nodes” and “random variables” interchangeably. For every w2 V
we consider a corresponding space Xwof possible states at wand we assume that each
spaceXwis ﬁnite. We consider a continuous time stochastic process on a product space
X=Q
w2VXw, so a state s2Xis a conﬁguration s= (sw)w2V, wheresw2Xw. IfWV;
then we write sW= (sw)w2Wfor the conﬁguration srestricted to the nodes in W. We
also use the notation XW=Q
w2WXw, so we can write sW2 XW. In what follows
15Figure 2.4: (a)
we use the bold symbol sto denote conﬁgurations belonging to Xonly. All restricted
conﬁgurations will be denoted with the standard font s.
Now suppose we have a family of functions Qw:XpaG(w)(XwXw)![0;1).
For a ﬁxed c2X paG(w)we consider Qw(c;;)as a conditional intensity matrix (CIM)
at the node w(only oﬀ-diagonal elements of this matrix have to be speciﬁed, the dia-
gonal ones are irrelevant). The state of CTBN at time tis a random element X(t)
of the spaceXof all the conﬁgurations. Let Xw(t)denote itsw-th coordinate. The
processf(Xw(t))w2V:t0gis assumed to be Markov and its evolution can be described
informally as follows: transitions, or jumps, at the node wdepend on the current conﬁ-
guration of its parents. If the state of any parent changes, then the node wswitches to
other transition probabilities. If sw6=s0
w;wheresw;s0
w2Xw, then
P(Xw(t+ dt) =s0
wjX w(t) =s w;Xw(t) =sw) =Qw(spaG(w);sw;s0
w) dt:
Deﬁnition 2.11. A continuous time Bayesian network Nover a set of random variables
X=fX1;:::;Xngis formed by two components. The ﬁrst one is an initial distribution P0
X
speciﬁed as a Bayesian network BoverX. The second component is a continuous transi-
tion model, speciﬁed as
•a directed (possibly cyclic) graph Gwhose nodes correspond to the random vari-
ablesXi;
•a conditional intensity matrix QXijpaG(Xi), specifying the continuous dynamic of each
variableXigiven its parents’ conﬁguration.
16Essentially, CTBN is a Markov jump process (MJP) on the state space Xwith tran-
sition intensities given by
Q(s;s0) =8
<
:Qw(spaG(w);sw;s0
w);ifs w=s0
 wandsw6=s0
wfor somew,
0; ifs w6=s0
 wfor allw,(2.7)
fors6=s0:Obviously, Q(s;s)is deﬁned “by subtraction” to ensure thatP
s0Q(s;s0) = 0.
For convenience, we will often write Q(s) = Q(s;s)so thatQ(s)0. In particular,
Qw(c;sw) = P
s6=s0Qw(c;s;s0).
It is important to note that we make a fundamental assumption in the construction
of the CTBN model: two variables cannot transition at the same time (a zero in the
deﬁnition of Q(s;s)). This can be viewed as a formalization of the view that variables
must represent distinct aspects of the world. We should not, therefore, model a domain in
whichwehavetwovariablesthatfunctionallyanddeterministicallychangesimultaneously.
For example, in the drug eﬀect network, we should not add a variable describing the type
of food, if any, a person is eating. We could, however, change the value space of the
“Eating” variable from a binary “yes/no” to a more descriptive set of possibilities.
Further we omit the symbol Gin the indices and write pa(w)instead of paG(w):For
CTBN the density of a sample trajectory X=X([0;T])on a bounded time interval [0;T]
decomposes as follows:
p(X) =(X(0))Y
w2Vp(XwjjXpa(w)); (2.8)
whereis the initial distribution on Xandp(XwjjXpa(w))is the density of piecewise
homogeneous Markov jump process with the intensity matrix equal to Qw(c;;)in every
time sub-interval such that Xpa(w)=c. Below we explicitly write an expression for
the density p(XwjjXpa(w))in terms of moments of jumps and the skeleton of the process
(Xw;Xpa(w)), as in (2.8), where by skeleton we understand the sequence of states of
the process corresponding to the sequence of moments of time.
LetTw= (tw
0:::;tw
i;:::)andTpa(w)= (tpa(w)
0;:::;tpa(w)
j;:::)denotemomentsofjumps
at the node w2Vand at parent nodes, respectively. By convention, put tw
0=tpa(w)
0 = 0
andtw
jTwj+1=tpa(w)
jTpa(w)j+1=tmax. Analogously, SwandSpa(w)denote the corresponding
skeletons. Thus we divide the time interval [0;tmax]into disjoint segments [tpa(w)
j;tpa(w)
j+1),
j= 0;1;:::jTpa(w)jsuch thatXpa(w)is constant and Xwis homogeneous in each segment.
Next we deﬁne sets Ij=fi >0 :tpa(w)
j< tw
i< tpa(w)
j+1gwith notation jbegandjendfor
17the ﬁrst and the last element of Ij. Then we obtain the following formula.
p(XwjjXpa(w)) =p(Tw;SwjjSpa(w);Tpa(w)) =
=jTpa(w)jY
j=0(
I(Ij6=;)"Y
i2IjQw(spa(w)
j;sw
i 1;sw
i)
Y
i2Ijnfjbeggexp
 (tw
i tw
i 1)Qw(spa(w)
j;sw
i 1)

exp
 (tw
jbeg tpa(w)
j)Qw(spa(w)
j;sw
jbeg 1) (tpa(w)
j+1 tw
jend)Qw(spa(w)
j;sw
jend)#
+
+I(Ij=;) exp
 (tpa(w)
j tpa(w)
j+1)Qw(spa(w)
j;sw
jbeg 1))
:
Below in Figure 2.5 there is an example of a trajectory of the node wwith two possible
states and of its parent with also two possible states 0 and 1. In this case the sets of
indices are I0=f2;3;4g,I1=f;gandI2=f7g.
Figure 2.5: An exemplary trajectory of a node wand its parents pa(w).
In consequence, using the fundamental property of the exponential function we may
writep(XwjjXpa(w))in the form
p(XwjjXpa(w)) =Y
c2Xpa(w)Y
s2XwY
s02Xw
s06=sQw(c;s;s0)nT
w(c;s;s0)exp
 Qw(c;s;s0)tT
w(c;s)
;
(2.9)
where
•nT
w(c;s;s0)denotes the number of jumps from s2Xwtos02Xwat the node won
the time interval [0;T], which occur when the parent conﬁguration is c2Xpa(w),
18•tT
w(c;s)is the length of time that the node wis in the state s2Xwon the time
interval [0;T];when the conﬁguration of parents is c2Xpa(w).
To simplify the notation we omit the upper index TinnT
w(c;s;s0)andtT
w(c;s)further in
the thesis, except for the part where we consider martingales.
2.6 The LASSO penalty
In this section we shortly describe the notions of the LASSO penalty and LASSO estima-
tors which constitute the base of the novel algorithms for structure learning in the thesis.
LASSO is the acronym for Least Absolute Shrinkage and Squares Operator. The term
was invented by Tibshirani (1996) though the general concept was introduced even earlier.
Most of the contents of this section come from Hastie et al. (2015).
The underlying idea of the LASSO estimators is the assumption of sparsity. A sparse
statistical model is one in which only a relatively small number of parameters (or predic-
tors) play an important role. Consider a linear regression model with Nobservations yiof
a target variable and xi= (xi1;:::;xip)>ofpassociated predictor variables which are also
called features. The goal is to predict the target from the predictors for future data and
also to discover which predictors are relevant. In the linear regression model we assume
that
yi=0+pX
j=1jxij+i;
where= (0;1;:::;p)is the vector of unknown parameters and iis an error term.
The standard way to ﬁnd is to minimize the least-squares function
NX
i=1
yi 0 pX
j=1jxij2
:
Typicallyalloftheestimatesappeartobenon-zero, whichcomplicatestheinterpretability
ofthemodelespeciallywithahighnumberofpossiblepredictors. Moreover, sincethedata
have noise the model will try to ﬁt the training observations too much and the parameters
will most probably take extreme values. In case when p>Nthe estimates are not even
unique, so most of solutions will overﬁt the data.
Thesolutionisto regularize theestimationprocess, i.e.addsomeconstraintsonthepa-
rameters. The LASSO estimator uses `1-penalty, which means that we minimize the least-
square function with an additional bound on `1-norm of, namelykk1=Pp
j=1jjjt.
The valuetis the user-speciﬁed parameter usually called hyperparameter. The motiva-
tion to use`1-penalty instead of any other `q-penalty comes from the fact that if tis small
enoughweobtainasparsesolutionwithonlyasmallamountofnon-zeroparameters. This
does not happen for `q-norm ifq>1, and ifq<1the solutions are sparse but the problem
is not convex. Convexity simpliﬁes the computations as well as the theoretical analysis of
the properties of the estimator. This allows for scalable algorithms capable of handling
19problems with even millions of parameters. Before the optimization process we typically
standardize the predictors so that each column is centred, i.e. the mean for each column
is 0, and has unit variance, i.e. the mean of squares is equal to 1. We also centre the target
column, so in the result we can omit the intercept term 0in the estimation process.
The LASSO penalty is used not only in linear regression but in a wide variety of
models, for example generalized linear models where the target and the linear model are
connected through some link function. Hence, in a more general case we can formulate
the optimization problem as
^= argmin
2Rp[L(;D) +kk1];
whereL(;D)is the arbitrary loss function for the data Dand the parameter vector .
The tuning hyperparameter corresponds to the constraining value t, there is one-to-
one correspondence. This is so-called Lagrangian form for the LASSO problem described
above.
In the setting of structure learning for Bayesian networks, both static and continuous,
we formulate the problem as an optimization problem for a linear or generalized linear
model, where the parameter vectors encode the dependencies between variables in the
network. We use the LASSO penalty in all the formulated problems, hence the prob-
lem of ﬁnding arrows in the graph reduces to recovering certain non-zero parameters in
the LASSO estimator. As the loss functions we use the negative log-likelihood function
and the residual sum of squares.
20Chapter 3
Statistical inference for networks with
known structure
There are three main classes of problems concerning Bayesian networks (both static and
continuous time). The ﬁrst one is to discover the structure of the network. Namely, we
need to specify the underlying graph of the network, which nodes are the variables of
interest, and its edges encode the dependencies between the variables. This problem will
be covered in subsequent chapters.
The second problem is to learn the parameters of the network. Namely, knowing
the structure of the network we need to specify the behaviour of the network in any
speciﬁed node given the states of its parents. In the context of static BN this behaviour
is encoded by conditional probability distributions (CPD, see (2.2)). The corresponding
parameters in case of CTBNs are conditional intensity matrices (CIM, see (2.7)).
The third type of problems is to make statistical inference using the network with
known structure and parameters. For instance, we may want to predict the state of some
nodeofinterestor, knowingstatesofsomenodes, ﬁndwhichcombinationoftheremaining
nodes explains them the best. Finally, we may be interested in prediction of the future
dynamics (in time) of some nodes of the network.
In this chapter we discuss well known results concerning the problems of learning
the parameters of the network and then the inference based on the fully discovered net-
work. The contents of this chapter are mainly based on Koller and Friedman (2009),
Nodelman (2007) and Heckerman (2021) with more detailed references throughout it.
3.1 Learning probabilities in BNs
First we discuss the discrete case. We assume that the Bayesian network with the
known underlying graph Gincludesnnodes each corresponding to a variable Xi2Xfor
i= 1;:::;n:Also, each variable Xiis discrete, having ripossible values x1
i;x2
i;:::;xri
i:
We denote an observed value of Xiinl-th observation as Xi[l]. If each node is observed
mtimes, then we obtain the sample dataset D=fD1;D2;:::;Dmgwith the sample
Dl= (X1[l];X2[l];:::;Xn[l])indicating the observed values of all the nodes in the l-th
21sampling. We refer to each Dlasa case. If all cases are complete, i.e. no missing values oc-
curred in the dataset D, it is considered as complete data ; otherwise, it is called incomplete
data. Missing values in data can occur for many diﬀerent reasons, for instance, people
ﬁlling out a survey may prefer not to answer some questions or certain measurements
might not be available for some patients in a medical setting.
There are mainly two categories of methods for parameter estimation in BN: one is
for dealing with the complete data, and the other is for incomplete data. We will provide
concise descriptions of two algorithms for the ﬁrst category such as maximum likelihood
estimation and Bayesian method; and we will brieﬂy discuss algorithms for the second
category.
Assume that as in (2.2) we can write the joint distribution of the variables in Xas
follows
P(X1;X2;:::;Xnj) =nY
i=1P(XijpaG(Xi);i)
for some vector of parameters = (1;:::;n), whereiis the vector of parameters for
the local distribution P(XijpaG(Xi);i). For shortness, further in this chapter we will
write pa(Xi)instead of paG(Xi). In the case of discrete and completely observed data
categorical distribution is commonly used. We note that in literature concerning learning
Bayesian networks this type of distribution is often referred to as multinomial distribution
or in some cases as unrestricted multinomial distribution (for example Heckerman (2021))
to diﬀerentiate this distribution from multinomial distributions that are low-dimensional
functions of pa(Xi).
Hence we assume that each local distribution function is a collection of categorical
distributions, one distribution for each conﬁguration of its parents, namely
P(Xi=xk
ijpaj
i;i) =ijk>0;for1kri;1jqi; (3.1)
whereqi=Q
Xj2pa(Xi)rjandpa1
i;pa2
i;:::;paqi
idenote all possible conﬁgurations of pa(Xi),
andi= ((ijk)ri
k=2)qi
j=1are the parameters. Note that the parameter ij1is given by
the diﬀerence 1 Pri
k=2ijk. For convenience, let us denote the vector of parameters
ij= (ij2;ij3;:::;ijri)for all 1inand1jqiso thati= (ij)qi
j=1.
As it is well known, the maximum likelihood estimation (MLE) is a method of estima-
ting the parameters of a probability distribution by maximizing the likelihood function,
so that under the assumed statistical model the observed data is the most probable. Ba-
sically, ifCkis the result of a random test for an event Cwith several possible outcomes
C1;C2;:::;Cnit will appear in the maximum likelihood for this event. Hence, the esti-
mated value of ^Cwill be set as parameter if it maximizes the value of the likelihood
function P(Cj).
22For the general Bayesian network with nnodes we denote the likelihood function as
L(:D) =P(Dj) =mY
l=1P(Dlj) =mY
l=1P(X1[l];X2[l];:::;Xn[l]j) =
=mY
l=1nY
i=1P(Xi[l]jpai[l];i) =nY
i=1mY
l=1P(Xi[l]jpai[l];i) =Y
iLi(i:D);(3.2)
whereby pai[l] =pa(Xi)[l]wedenotethe l-thobservationoftheparentsvectorofthevari-
ableXi. This representation shows that the likelihood decomposes as a product of inde-
pendent factors, one for each CPD in the network. This important property is called the
global decomposition of the likelihood function. Moreover, this decomposition is an imme-
diate consequence of the network structure and does not depend on any particular choice
of the parameterization for CPDs (see Koller and Friedman (2009)).
If the conditional distribution of Xigiven its parents paG(Xi)is the categorical dist-
ribution, then the local likelihood function can be further decomposed as follows
Li(i:D) =mY
l=1P(Xi[l]jpai[l];i) =mY
l=1qiY
j=1riY
k=1P(Xi[l] =xk
ijpai[l] =paj
i;i)
=qiY
j=1riY
k=1N(xk
i;paj
i)
ilk;(3.3)
whereN(xk
i;paj
i)is the number of cases in Dfor whichXi=xk
iandpa(Xi) =paj
i.
Considering that the dataset is complete for each possible value paj
iof the parents
pa(Xi)of the node Xi, the probability P(Xijpaj
i)is the independent categorical dist-
ribution not related to any other conﬁgurations pal
iofpa(Xi)forj6=l. Therefore, as
the result of the MLE method we obtain the estimated parameter ^as follows
^ijk=N(xk
i;paj
i)
N(paj
i);
whereN(paj
i)denotes the number of cases when the conﬁguration paj
iappears in the full
set of observations for the vector of variables pa(Xi).
Note that in general the MLE approach attempts to ﬁnd the parameter vector that
is “the best” given the data C. On the other hand, the Bayesian approach does not
attempt to ﬁnd such a point estimate. Instead, the underlying principle is that we should
keep track of our beliefs about values of , and use these beliefs for reaching conclusions.
In other words, we should quantify the subjective probability we have initially assigned
to diﬀerent values of taking into account new evidence. Note that in representing such
subjectiveprobabilitieswenowtreat asarandomvariable. Thus, theBayesianapproach
is based on the Bayes rule
p(jC) =p(Cj)p()
p(C): (3.4)
Hence, the basic idea of the Bayesian method for parameter learning is the following.
We treatas a random variable with a prior distribution p(), and it is very common
23tosetpastheuniformdistribution, especiallyinthecasewhenwehavenopriorknowledge
about. Given a distribution with unknown parameters and a complete set of observed
dataC, new beliefs about , namelyp(jC), can be estimated according to the previous
knowledge. The aim is to calculate p(jC)which is called the posterior probability of
the parameter . For computational eﬃciency we want to use a conjugate prior, i.e. when
the posterior distribution after conditioning on the data is in the same parametric family
as the prior one.
Here we assume that each vector ijhas the prior Dirichlet distribution, so that
p(ij) =Dir(ijjij1;:::;ijri) = (ij)Qri
k=1 (ijk)riY
k=1ijk 1
ijk; (3.5)
whereij=Pri
k=1ijk,ijk>0,k= 1;:::;ri,ij1;:::;ijriarehyperparametersand  ()
is Gamma function. This is the standard conjugate prior to both categorical and multi-
nomial distributions. Hence, the probability of observed samples is
p(D) =Z
p(ij)p(Djij)dij=
=Z (ij)Qri
k=1 (ijk)riY
k=1ijk 1
ijkriY
k=1Nijk
ijkdij=
= (ij)
 (ij+Nij)riY
k=1 (ijk+Nijk)
 (ijk);(3.6)
where for shortness Nijk=N(xk
i;paj
i)andNij=N(paj
i) =Pri
k=1Nijk. The integral is
(ri 1)-dimensional over the set fijk0;2kri;Pri
k=2ijk1g.
As we have already mentioned, in Bayesian method, if we do not have prior distri-
bution we assume it to be uniform, which is consistent with the principle of maximum
entropyininformationtheory, itmaximizestheentropyofrandomvariableswithbounded
support. Thus, if there is no information used for determination of prior distribution, we
set hyperparameters 1==r= 1:
Combining (3.4), (3.5) and (3.6) under the assumptions of parameter independence
and complete data ﬁnally we obtain the posterior distribution as follows
p(ijjD) =Dir(ijjij1+Nij1;:::;ijri+Nijri): (3.7)
Therefore, we have an estimate for each parameter ijkfrom dataDas follows
^ijk=ijk+Nijk
ij+Nij;1kri:
Continuous Variable Networks. When we were discussing the MLE method for disc-
rete BNs, we mentioned the global decomposition rule which applies to any type of CPD.
That is, if the data are complete, the learning problem reduces to a set of local lear-
ning problems, one for each variable. The main diﬀerence is in applying the maximum
likelihood estimation process to CPD of a diﬀerent type: how we deﬁne the suﬃcient
24statistics, and how we compute the maximum likelihood estimate from them. In this
paragraph, we brieﬂy discuss how MLE principles can be applied in the setting of linear
Gaussian Bayesian networks.
Consider a variable Xwith parents U=fU1;:::;Ukgwith linear Gaussian CPD:
p(Xju) =N(0+1u1++kuk;2):
Our task is to learn the parameters ^XjU= (0;1;:::;k;2). To ﬁnd the MLE values of
these parameters, we need to diﬀerentiate the likelihood function and to solve the equa-
tions that deﬁne a stationary point. As usual, it is easier to work with the log-likelihood
function. Using the deﬁnition of the Gaussian distribution, we have that
`(XjU:D) = logLX(XjU:D) =
=X
l
 1
2log(22) 1
22(0+1u1[l] ++kuk[l] x[l])2
:
We consider the gradients of the log-likelihood with respect to all of the parameters
0;:::;kand2and as a result we get a number of equations, which describe the solution
to a system of linear equations. From the Theorem 7.3 in Koller and Friedman (2009), it
follows that ifBis a linear Gaussian Bayesian network, then it deﬁnes a joint distribution
that is jointly Gaussian, and the MLE estimate has to match the constraints implied by it.
Brieﬂy speaking, to estimate p(XjU)we estimate the means of XandUand
the covariance matrix of fXg[Ufrom the data. The vector of means and the covariance
matrix deﬁne the joint Gaussian distribution over fXg[U. Then, for example using
the formulas provided by Theorem 7.3 in Koller and Friedman (2009), we ﬁnd the unique
linear Gaussian that matches the joint Gaussian with these parameters.
The suﬃcient statistics we need to collect to estimate linear Gaussians are the uni-
variate terms of the formP
mx[m]andP
mui[m], and the interaction terms of the formP
mx[m]ui[m]andP
muj[m]ui[m]. From these we can estimate the mean and the co-
variance matrix of the joint distribution.
3.2 Inference in Bayesian networks
In this section we assume that the network structure is known, meaning we know all
the existing edges and their directions as well as all the CPDs. The problem of inference
for BNs is a challenging task on its own and there is a lot of research done on the subject.
We will not go into much of a detail on the inference since our focus is on learning their
structure. However, the question of inference is worth mentioning here in order to get
a wholesome picture of such a powerful tool as BNs.
First, we discuss what the notion of inference means in the case of BNs. Typically it
refers to:
•marginal inference, i.e. ﬁnding the probability of a variable being in a certain state,
giventhatother variables are set to certain values; or
25•maximum a posteriori (MAP) inference, i.e. ﬁnding the values of a given set of
variables that best explain (in the sense of the highest MAP probability) why a set
of other variables have certain values,
Let us demonstrate both categories of questions using an example. We will use the BN
structure of a well-known ASIA network (see Figure 3.1) ﬁrst introduced in Lauritzen
and Spiegelhalter (1988). It illustrates the causal structure of a patient having a certain
lung disease based on several factors, one being whether or not the patient has recently
been to Asia. In this case, an exemplary question on marginal inference might be what
is the probability of a patient who is a smoker and has dyspnoea having a certain lung
disease, e.g. lung cancer. For the MAP inference, we might want to know what is the
most likely set of conditions (with “smoking” and “dyspnoea” excluded) that could have
caused the symptoms mentioned above.
Now we provide short descriptions of the most popular exact and approximate infe-
rence algorithms for BNs. Among them are variable elimination and belief propagation for
the marginal inference, methods for the MAP inference and the sampling-based inference.
For the purposes of transparency of the presentation the inference methods for BNs will
be demonstrated for the discrete and ﬁnite case.
Figure 3.1: The ASIA Bayesian network structure
3.2.1 Variable Elimination
This inference algorithm is deﬁned in terms of so-called factors and is developed to answer
questions of marginal inference. Factors generalize the notion of CPDs. A factoris
a function of value assignments of a set of random variables Vwith positive real values.
The set of variables Vis called the scope of the factor. There are two operations on
factors that are repeatedly performed in a variable elimination algorithm (VE) and hence
are of great importance.
26•The factor product. IfV1;V2;andV3are disjoint sets of variables and we have
factors1and2with scopes V1[V2andV2[V3respectively, then we deﬁne
the factor product 12as a new factor  with the scope V1[V2[V3by
 (V1;V2;V3) =1(V1;V2)2(V2;V3):
Thisproductisthenewfactorovertheunionofthevariablesdeﬁnedforeachinstan-
tiation by multiplying the value of 1on the particular instantiation by the value
of2on the corresponding instantiation. More precisely,
 (v1;v2;v3) =1(v1;v2)2(v2;v3)
for each instantiation, where v12Val(V1);v22Val(V2)andv32Val(V3).
•The factor marginalization. This operation “locally” eliminates a set of variables
from a factor. If we have a factor (V1;V2)over two sets of variables V1;V2,
marginalizing V2produces a new factor
(V1) =X
V2(V1;V2);
wherethesumisoveralljointassignmentsforthesetofvariables V2. Moreprecisely,
(v1) =X
v22Val(V2)(v1;v2);v12Val(V1)
for each instantiation v12Val(V1).
Thus, in the context of factors we can write our distribution over all variables as a product
of factors, where each factor presents a CPD as in (2.2):
P(X1;X2;:::;Xn) =nY
i=1i(Ai); (3.8)
where Ai= (Xi;paG(Xi))represents a set of variables including the i-th variable and its
parents in the network.
Now we can describe the full VE algorithm. Assume we want to ﬁnd marginal dist-
ribution of a ﬁxed variable from X1;:::;Xn. First we need to choose in which order O
to eliminate remaining variables. The choice of an optimal elimination ordering Ois
anNP-hard problem and it may dramatically aﬀect the running time of the variable
elimination algorithm. Some intuitions and techniques on how to choose an adequate
ordering are given for example in Koller and Friedman (2009). For each variable Xi
(ordered according to the ordering O) we perform the following steps:
•multiply all factors containing Xi(on the ﬁrst round all the icontainingXi);
•marginalizeout Xiaccordingtothedeﬁnitionofthefactormarginalizationtoobtain
a new factor (which does not necessarily correspond to a probability distribution,
even though each is CPD);
27•replace the factors used in the ﬁrst step with .
Essentially, we loop over the variables as ordered by Oand eliminate them in this order.
Performing those steps we use simple properties of product and summation on factors,
namely, both operations are commutative and products are associative. The most im-
portant rule is that we can exchange summation and product, meaning that if a set of
variables Xis not in the scope of the factor 1, then
X
X12=1X
X2: (3.9)
So far we saw that the VE algorithm can answer queries of the form P(V), where V
is some subset of variables. However, in addition to this type of questions it can answer
marginal queries of the form
P(YjE=e) =P(Y;E=e)
P(E=e);
where P(X;Y;E)is a probability distribution over sets of query variables Y, observed
evidence variables E, and unobserved variables X. We can compute this probability by
performing variable elimination once on P(Y;E=e)and then once again on P(E=e)
taking into account only instantiations consistent with E=e.
An exemplary run of the VE algorithm is presented in Table 3.1. It corresponds
to Extended Student example ﬁrst mentioned in Section 2.2.
StepVariables Factors used Variables New
eliminated involved factor
1C C(C);D(D;C )C;D 1(D)
2D G(G;I;D );1(D)G;I;D 2(G;I)
3II(I);S(S;I);2(G;I)G;S;I 3(G;S)
4H H(H;G;J ) H;G;J 4(G;J)
5G3(G;S);4(G;J);L(L;G)G;J;L;S 5(J;L;S )
6S5(J;L;S );J(J;L;S )J;L;S 6(J;L)
7L 6(J;L) J;L 7(J)
Table 3.1: A run of variable elimination for the query P(J).
3.2.2 Message Passing Algorithms
Markovrandomﬁelds. Intheframeworkofprobabilisticgraphicalmodelsthereexists
another technique for compact representation and visualization of a probability distribu-
tionwhichisformulatedinthelanguageofundirectedgraphs. Thisclassofmodels(known
as Markov Random Fields or MRFs) can succinctly represent independence assumptions
that directed models cannot represent and the opposite is also true. There are advan-
tages and drawbacks to both of those methods but that is not the focus of this thesis.
28We will introduce and discuss MRFs only to the extent we need to properly describe and
explain notions and methods concerning BNs. Note that the methods provided below for
marginal and MAP inference are applicable both to MRFs and BNs.
Deﬁnition 3.1. A Markov Random Field (MRF) is a probability distribution over vari-
ablesX1;:::;Xndeﬁned by an undirected graph Gin which nodes correspond to vari-
ablesXi. The probability has the form
P(X1;X2;:::;Xn) =1
ZY
c2Cc(Xc);
whereCdenotes the set of cliques (i.e. fully connected subgraphs) of Gand each factor c
is a non-negative function over the variables in a clique. The partition function
Z=X
(x1;:::;xn)Y
c2Cc(Xc)
is a normalizing constant that ensures that the distribution sums to one, where the sum-
mation is taken over all possible instantiations of all the variables.
Thus, given a graph G, our probability distribution may contain factors whose scope
is any clique inGand the clique can be a single node, an edge, a triangle, etc. Note that
we do not need to specify a factor for each clique.
It is not hard to see that Bayesian networks are a special case of MRFs with a norma-
lizing constant equal to 1 where the clique factors correspond to CPDs. One can notice
that if we take a directed graph G, add side edges to all parents of a given node and remove
their directionality, then the CPDs (seen as factors over each variable and its ancestors)
factorize over the resulting undirected graph. The resulting process is called moralization
(see Figure 3.2). A Bayesian network can always be converted into an undirected network
with normalizing constant 1.
Figure 3.2: Moralization of a Bayesian network.
Message passing. As we mentioned above, the VE algorithm can answer marginal
queries of the form P(YjE=e). However, if we want to ask the model for another
query, e.g. P(Y2jE2=e2), we need to restart the algorithm from scratch. Fortunately, in
the process of computing marginals, VE algorithm produces many intermediate factors 
as a side-product of the main computation, which turn out to be the same as the ones
that we need to answer other marginal queries.
29Many complicated inference problems can be solved by message-passing algorithms, in
which simple messages are passed locally among simple elements of the system. An illust-
rative example was shown in the book MacKay (2003) for a problem of counting soldiers.
Consider a line of soldiers walking in the mist. The commander, which is in the line,
wishes to count the soldiers. The straightforward calculation is impossible because of
the mist. However, it can be done in a simple way which does not require any complex
operations. The algorithm requires the soldiers’ ability to add two integer numbers and
add 1 to it. The algorithm consists of the following steps (for example see Figure 3.3):
•the front soldier in the line says the number ‘one’ to the soldier behind him,
•the rearmost soldier in the line says the number ‘one’ to the soldier in front of him,
•the soldier, which is told a number from the soldier ahead or the soldier behind,
adds 1 to it and passes the new number to the next soldier in the line on the other
side.
Figure 3.3: A line of soldiers counting themselves using message-passing rule-set.
Hence, the commander can ﬁnd the global number of soldiers by simply adding to-
gether the numbers: heard from the soldier in front of him, from the soldier behind him
and 1. This method makes use of a property of the total number of soldiers: the number
can be written as the sum of the number of soldiers in front of a point and the number
behind that point, two quantities which can be computed separately, because the two
groups are separated by the commander. When this requirement is satisﬁed this message-
passing algorithm can be modiﬁed for a general graph with no cycles (as an example see
Figure 3.4a). When the graph has no cycles (see Figure 3.4a) for each soldier we can
uniquely separate the group into two groups, ‘those in front’, and ‘those behind’ and
perform the algorithm above. However, it is not always possible for a graph with cycles,
for instance for a soldier in a cycle (such as ‘Jim’) in Figure 3.4b such a separation is not
unique.
Using the same principle we will now describe the message passing for tree-structured
networks (called belief propagation , BP for short) and then the modiﬁcation of the method
for general networks (called clique tree algorithm ).
Beliefpropagation. Letusﬁrstlookattree-structuredgraphs. Considerwhathappens
if we run the VE algorithm on a tree in order to compute a marginal distribution P(Xi).
30(a) No cycles.
(b) Contains a cycle.
Figure 3.4: A swarm of soldiers.
We can easily ﬁnd the optimal ordering for this problem by rooting the tree at the node
associated with Xiand iterating through the nodes in post-order (from leaves to the root),
just like for a swarm of soldiers with no cycles. At each step, we will eliminate one of
the variables, say Xj; this will involve computing the factor k(xk) =P
xj(xk;xj)j(xj),
whereXkis the parent of Xjin the tree. At a later step, the variable Xkwill be eliminated
in the same manner, i.e. k(xk)will be passed up the tree to the parent XlofXkin order to
be multiplied by the factor (xl;xk)before being marginalized out. As a result we obtain
the new factor l(xl). The factor j(xj)can be thought of as a message that Xjsends
toXkthat summarizes all of the information from the subtree rooted at the node Xj.
We can visualize this transfer of information using arrows on the tree, see Figure 3.5.
At the end of the VE algorithm, the node Xireceives messages from all of its children
and the ﬁnal marginal P(Xi)is obtained by marginalizing those messages out.
Figure 3.5: Message passing order when using VE to compute P(X3)on a small tree.
With the same indices as above, suppose that after computing P(Xi)we want to
compute P(Xk)as well. We would again run VE for the new tree rooted at the node Xk,
waiting until it receives all messages from its children. Note that the new tree consists
31of two parts. The ﬁrst one is the subtree rooted at Xkwith all its descendants from
the original tree (i.e. rooted at Xi). The other part is the subtree rooted at Xl(which
was the parent of Xkin the original tree, but now is the child of Xk). Therefore, this
part contains the node Xi. The key insight is that the messages received by XkfromXj
now will be the same as those received when Xiwas the root. Thus, if we store the inter-
mediary messages of the VE algorithm, we can quickly compute other marginals as well.
Notice for example, that the messages sent to Xkfrom the subtree containing Xiwill need
to be recomputed. So, how do we compute all the messages we need? Again, referring to
the soldier counting problem, a node is ready to transmit a message to its parent after it
has received all the messages from all of its children. All the messages will be sent out
after precisely 2jEjsteps, wherejEjis the number of edges in the graph, since each edge
can receive messages only twice.
To deﬁne belief propagation (BP) algorithm formally let us see what kind of messages
can be sent. For the purposes of marginal inference we will use sum-product message
passing. This algorithm is deﬁned as follows: while there is a node Xkready to transmit
toXlit sends the message
mk!l(xl) =X
xk(xk)(xk;xl)Y
j2Nb(k)nflgmj!k(xk);
whereNb(k)nflgmeans all the neighbours of the k-th node, excluding l-th node. Note
that this message is precisely the factor thatXkwould transmit to Xlduring a round
of variable elimination with the goal of computing P(Xi), and also note that the product
on the RHS of this equation naturally equals to 1for leaves in the tree.
After having computed all messages, we may answer marginal queries over any vari-
ableXjin constant time using the equation:
P(Xj)/ (Xj)Y
l2Nb(j)ml!j(xj);
where (Xj)is a product of all factors whose scope contains Xj. In case of BNs we
have the equality instead of proportionality.
Clique Tree Algorithm. First let us deﬁne what is meant by a clique tree. Clique tree
isanundirectedtreesuchthatitsnodesareclusters Ciofvariables, meaning Ciisasubset
of a set of all variables fX1;:::;Xng. Each edge between clusters CiandCjis associated
with asepset(separation set) Si;j=Ci\Cj. See a simple example demonstrating a
clique tree for a chain network in Figure 3.6.
So far we assumed that the graph is a tree. What if that is not the case? Then the
clique tree algorithm (also called the junction tree algorithm in the literature) can be
used; it partitions the graph into clusters of variables so that interactions among clusters
will have a tree structure, i.e. a cluster will be only directly inﬂuenced by its neighbours
in the tree, we denote it T. Then we can perform message passing on this tree. This leads
to tractable global solutions if the local (cluster-level) problems can be solved exactly.
In addition, clique trees must satisfy two following properties:
32Figure 3.6: An example of a chain network consisting of three variables A,BandC;
corresponding MRF and a clique tree with C1=fA;Bg,C2=fB;CgandS1;2=fBg.
1.family preservation , i.e. for each factor there is a cluster such that factor’s scope
is a subset of the cluster;
2.running intersection property (RIP) , i.e. for each pair of clusters Ci,Cjand a vari-
ableX2Ci\Cjall clusters and sepsets on the unique path between CiandCj
contain the variable X.
Note that we may always ﬁnd a trivial clique tree with one node containing all the vari-
ablesin theoriginalgraph, butobviouslysuchtreesareuseless. Optimal trees aretheones
that make the clusters as small and modular as possible; unfortunately, as in case of VE,
the problem of ﬁnding the optimal tree is also NP-hard. A special case when we can ﬁnd
it is when we originally have a tree, in this case we can put each connected pair of nodes
into a separate cluster, it is easy to check that both conditions are met. One of the prac-
tical ways to ﬁnd a good clique tree is to use a simulation of VE, i.e. the elimination order
ﬁxed for VE will induce the graph from which we will take maximal cliques and set them
as our clusters and form a tree. RIP will be satisﬁed automatically. Note that we do not
need to run VE, just to simulate it for a chosen ordering and get the induced graph. More
formally:
Deﬁnition 3.2. Letbe a set of factors (CPDs in the case of Bayesian Networks)
overX=fX1;:::;Xng, andbe an elimination ordering for some subset X X.
The induced graph denoted by I;is an undirected graph over X, whereXiandXj
are connected by an edge if they both appear in some intermediate factor  generated by
the VE algorithm using as an elimination ordering.
In Figure 3.7 there is an example of an induced graph for the Student example using
the elimination ordering of Table 3.1, cliques in that graph and a corresponding clique
tree. One can see that RIP is satisﬁed, for a proof that trees corresponding to induced
graphs by VE will satisfy RIP see Koller and Friedman (2009).
Now let us deﬁne the full clique tree algorithm. First, we deﬁne the potential  i(Ci)
of each cluster Cias the product of all the factors inGthat have been assigned to Ci.
33By the family preservation property, this is well-deﬁned, and we may assume that our
distribution is of the form
P(X1;:::;Xn) =1
ZY
i i(Ci):
Then, at each step of the algorithm, we choose a pair of adjacent clusters Ci,Cjin
a tree graphTand compute a message whose scope is the sepset Si;jbetween the two
clusters
mi!j(Si;j) =X
CinSi;j i(Ci)Y
l2Nb(i)nfjgml!i(Sl;i): (3.10)
In the context of clusters, Nb(i)denotes the set of indices of neighboring clusters of Ci.
We choose CiandCjonly if Cihas received messages from all of its neighbors except
Cj. Just as in belief propagation, this procedure will terminate in exactly 2jETjsteps
because this process is equivalent to making an upwardpass and a downward pass. In the
upward pass, we ﬁrst pick a root and send all messages towards it starting from leaves.
When this process is complete, the root has all the messages. Therefore, it can now send
the appropriate message to all of its children. This algorithm continues until the leaves
of the tree are reached, at which point no more messages need to be sent. This second
phase is called the downward pass. After it terminates, we will deﬁne the belief of each
cluster based on all the messages that it receives
i(Ci) = i(Ci)Y
l2Nb(i)ml!i(Sl;i): (3.11)
These updates are often referred to as Shafer-Shenoy updates and the full procedure
is also referred as sum-product belief propagation . Then each belief is the marginal of
the clique
i(Ci) =X
XnCiP(X1;:::;Xn):
Now if we need to compute the marginal probability of a particular variable Xwe
can select any clique whose scope contains X, and eliminate the redundant variables
in the clique. A key point is that the result of this process does not depend on the clique
we selected. That is, if Xappears in two cliques, they must agree on its marginal. Two
adjacent cliques CiandCjare said to be calibrated if
X
CinSi;ji(Ci) =X
CjnSi;jj(Cj):
A clique treeTis calibrated if all pairs of adjacent cliques are calibrated. For a calibrated
clique tree, we use the term clique beliefs for i(Ci)and sepset beliefs for i;j(Si;j)deﬁned
as either side of the above equality.
As the end result of sum-product belief propagation procedure we get a calibrated
tree, which is more than simply a data structure that stores the results of probabilistic
inference for all of the cliques in the tree, i.e. their beliefs (3.11). It can also be viewed
34as an alternative representation of the joint measure over all variables. For sepset beliefs
we have that
i;j(Si;j) =mi!j(Si;j)mj!i(Si;j):
Using this fact at convergence of the clique tree calibration algorithm, we get the unnor-
malized joint measure ~Pas
~P(X1;:::;Xn) =Y
i i(Ci) =Q
ii(Ci)Q
(i;j)i;j(Si;j); (3.12)
where the product in the numerator is over all cliques and the product in the denominator
is over all sepsets in the tree. As a result we get a diﬀerent set of parameters that
captures unnormalized measure that deﬁned our distribution (in case of BNs it is simply
the distribution) and there is no information lost in the process. Thus, we can view
the clique tree as an alternative representation of the joint measure, one that directly
reveals the clique marginals.
The second approach, mathematically equivalent but using a diﬀerent intuition, is
message passing with division. In sum-product belief propagation messages were passed
between two cliques only after one had received messages from all of its neighbors except
the other one as in (3.10) and the resulting belief was (3.11). Nonetheless, a diﬀerent
approach to compute the same expression is to multiply in all of the messages, and then
divide the resulting factor by the message from the other clique to avoid double-counting.
To make this notion precise, we must deﬁne a factor-division operation.
LetXandYbe disjoint sets of variables and let 1and2be two factors with scopes
X[YandYrespectively. Then we deﬁne the division1
2as a factor-division  with
the scope X[Yas follows
 (X;Y) =1(X;Y)
2(Y);
where we deﬁne0
0= 0. We now see that we can compute the expression of equation
(3.10) by computing the beliefs as in equation (3.11) and then dividing by the remaining
message
mi!j(Si;j) =P
CinSi;ji(Ci)
mj!i(Si;j):
The belief of the j-th clique is updated by multiplying its previous belief by mi!jand
dividing it by the previous message passed along this edge (regardless of the direction)
stored in sepset belief i;jto avoid double counting. This algorithm is called belief update
message passing and is also known as the Lauritzen-Spiegelhalter algorithm .
3.2.3 MAP inference
The maximum a posteriori (MAP) problem has a broad range of applications, in computer
vision, computational biology, speech recognition, and more. By using MAP inference we
lose the ability to measure our conﬁdence (or uncertainty) in our conclusions. Never-
theless, there are good reasons for using a single MAP assignment rather than using
35Figure 3.7: (a) Induced graph for VE in the Student example, using the elimination order
of Table 3.1 (b) Cliques in the induced graph: fC;Dg,fD;I;Gg,fG;I;Sg,fG;J;S;Lg
andfG;H;Jg. (c) Clique tree for the induced graph.
the marginal probabilities of the diﬀerent variables. The ﬁrst reason is the preference
for obtaining a single coherent joint assignment, whereas a set of individual marginals
may not make sense as a whole. The second is that there are inference methods that are
applicable to the MAP problem and not to the task of computing probabilities, so that
the former may be tractable even when the latter is not. The problem of ﬁnding the MAP
assignment in the general case is NP hard (Cooper (1990)).
There are two types of Maximum a Posteriori (MAP) inference: a MAP query and
a marginal MAP query. Assume ﬁrst that the set of all variables X=Y[Econsists of
two disjoint sets, where Eis the evidence meaning that we know values of those variables.
Then a MAP query aims to ﬁnd the most likely assignment to all of the non-evidence
variables Y
MAP (YjE=e) = argmax
yP(Y=yjE=e):
Now assume that the set of all variables X=Y[W[Econsists of three disjoint sets,
where Eis still the evidence. In this case a marginal MAP query aims to ﬁnd the most
likely assignment to the subset Y, marginalizing over the rest of the variables W
MAP (YjE=e) = argmax
yP(Y=yjE=e) =
= argmax
yX
wP(Y=y;W=wjE=e):
36Both tasks can be solved within the same variable elimination (VE) and message passing
frameworks as marginal inference, where instead of summation we use maximization. The
second type of query is much more complicated both in theory and in practice since it
involves both maximization and summation. In particular, exact inference methods such
as VE can be intractable, even in simple networks. Hence, ﬁrst we will brieﬂy discuss
them and then introduce some more eﬃcient methods.
Recall that while discussing VE we introduced two operations on factors, which were
the foundation in performing the algorithm. Now we need to introduce one additional
operationcalled the factor maximization . LetXbeasetofvariables, and Y62Xavariable
not belonging to the set X. Let(X;Y)be a factor over those variables. We deﬁne
the factor maximization of Yinto be a factor  overXsuch that:
 (X) = max
Y(X;Y):
More precisely,
 (x) = max
y2Val(Y)(x;y)
for each instantiation x2Val(X). Similarly to the property (3.9) we have that if a set
of variables Xis not in the scope of the factor 1, then
max
X(12) =1max
X2 (3.13)
and
max
X(1+2) =1+ max
X2: (3.14)
This leads us to a max-product variable elimination algorithm for a general MAP query,
which is constructed in the same way as a sum-product variable elimination algorithm in
Subsection 3.2.1, but we replace the marginalizing step (summation) with maximization
over corresponding variables.
This way we ﬁnd the maximum value for the joint probability, though the original
and more interesting problem is to ﬁnd the most probable assignment corresponding to
that maximum probability. This process is called a traceback procedure , which is quite
straightforward (details can be found in Koller and Friedman (2009)). In the process of
eliminating variables we ﬁnd their maximizing value given the values of the variables that
have not yet been eliminated. When we pick the value of the ﬁnal variable, we can then
go back and pick the values of the remaining variables accordingly.
Recall that the joint distribution Pin Bayesian networks is represented by a product
of factors, where each factor coincides with a CPD (we introduced this representation
in (3.8)). Then we can write the marginal MAP query as
argmax
yX
WP(y;W) = argmax
yX
WY
ii;
where we skipped the evidence set for the transparency of notation since it does not eﬀect
the main point of discussion. First we compute
max
yX
WY
ii:
37This form immediately suggests an algorithm combining the ideas of sum-product and
max-product variable elimination. Speciﬁcally, the summations and maximizations out-
side the product can be viewed as operations on factors. Thus, to compute the value of
this expression, we simply have to eliminate the variables in Wby summing them out,
and the variables in Yby maximizing them out. When eliminating a variable X, whether
by summation or by maximization, we simply multiply all the factors whose scope in-
volvesX, and then eliminate Xto produce the resulting factor. The ability to perform
this step is justiﬁed by the interchangeability of factor summation and maximization with
factor product (properties (3.9) and (3.13)). The traceback procedure to ﬁnd the most
probable assignment can also be found in Koller and Friedman (2009).
At ﬁrst glance it seems that algorithms for both queries have the same complexity
but that is not the case. It can be shown that even on very simple networks, elimination
algorithmscanrequireexponentialtimetosolveamarginalMAPquery(seeExample13.7
in Koller and Friedman (2009)). The diﬃculty comes from the fact that we are not free
to choose an arbitrary elimination ordering. When summing out variables, we can utilize
the fact that the operations of summing out diﬀerent variables commute. Thus, when
performing summing-out operations for sum-product variable elimination, we could sum
out the variables in any order. Similarly, we could use the same ﬂexibility in the case of
max-product elimination. Unfortunately, the max and sum operations do not commute.
Thus, in order to maintain the correct semantics of marginal MAP queries, as speciﬁed
in the equation, we must perform all the variable summations before we can perform any
of the variable maximizations.
We can also use the message passing framework, or more general case of clique tree
algorithm, toMAPinference. InSubsection3.2.2weusedcliquetreestocomputethesum-
marginals over each of the cliques in the tree. Here, we compute a set of max-marginals
over each of those cliques. By the max-marginal of a function fdeﬁned on the set X
relative to a set of variables YXwe denote such a factor that for each y2Y
MaxMarginal f(y) = max
hxiY=yf(x)
determines the value of the unnormalized probability of the most likely joint assign-
mentx2Xconsistent with y. We compute the whole set for two reasons. First, the set of
max-marginalscanbeausefulindicatorforhowconﬁdentweareinparticularcomponents
oftheMAPassignment. Second, inmanycases, anexactsolutiontotheMAPproblemvia
avariableeliminationprocedureisintractable. Inthiscase, tocomputeapproximatemax-
marginals we can use message passing procedure in cluster graphs, similar to the clique
tree procedure. These pseudo-max-marginals can be used for selecting an assignment;
while this assignment is not generally the MAP assignment, we can nevertheless provide
some guarantees in certain cases. As before, our task consists of two parts: computing
the max-marginals and decoding them to extract a MAP assignment.
As for the ﬁrst part, in the same way as we modiﬁed sum-product VE to sum-product
message-passingwemodifymax-productVE tomax-productbeliefpropagationalgorithm
in clique trees. The resulting algorithm executes precisely the same initialization and
38overall message scheduling as in the sum-product belief propagation algorithm. The only
diﬀerence is that we use max-product rather than sum-product message passing. As
a result of running the algorithm we will get a set of max-marginals for every clique of
our clique tree.
Each belief is the max-marginal of the clique i(Ci) =MaxMarginal p(Ci)and all
pairs of adjacent cliques are max-calibrated
i;j(Si;j) = max
CinSi;ji(Ci) = max
CjnSi;jj(Cj):
Similarly to sum-product message passing we get reparameterization of the distribution
in the form (3.12) with corresponding beliefs of the max-product belief propagation algo-
rithm.
Now we need to decode those max-marginals to get a MAP assignment. In the case
of variable elimination, we had the max-marginal only for a single last to be eliminated
variableandcouldidentifytheassignmentforthatparticularvariable. Tocomputetheas-
signments to the rest of the variables, we had to perform a traceback procedure. Now
the situation appears diﬀerent. One obvious solution is to use the max-marginal for each
variable to compute its own optimal assignment, and thereby compose a full joint assign-
ment to all variables. However, this simplistic approach works only in case when there is
a unique MAP assignment, equivalently, each max-marginal has a unique maximal value.
For generic probability measures this is not a very rigid constraint, thus, we can ﬁnd the
unique MAP assignment by locally optimizing the assignment to each variable separately.
Otherwise, in most cases to break ties we can introduce a slight random perturbation
into all of the factors, making all of the elements in the joint distribution have slightly dif-
ferent probabilities. However, there might be cases when we need to preserve the structure
in relationships between some variables, for example some variables can share parameters
or there might be some deterministic structure that should be preserved. Under these
circumstances we ﬁnd a locally optimal assignment using for example traceback proce-
dure. Afterwards we can verify if this assignment is a MAP assignment (for procedure
and veriﬁcation see Koller and Friedman (2009)).
MAP as Linear Optimization Problem. In MAP inference we search for assign-
ments which maximize a certain measure, in our case either the joint probability over all
non-evidence variables or the probability over some set of variables. Therefore, it is na-
tural to consider it directly as an optimization problem. There exists extensive literature
on optimization algorithms and we can apply some of those ideas and algorithms to our
speciﬁc case.
The main idea here is to reduce our MAP problem to an Integer Linear Program-
ming (ILP) problem, i.e. an optimization problem over a set of integer valued variables,
where both the objective and the constraints are linear. First, to deﬁne ILP problem we
need to turn the product representation of the joint probability as in (3.8) into a sum,
replacing the probability with its logarithm. It is possible because all the factors (CPDs)
39are positive. Hence, we want to compute
argmax
nY
i=1i(Ai) = argmax
nX
i=1log(i(Ai));
whereis a general assignment for the whole vector of variables in the network, and
Ai= (Xi;paG(Xi))represents a set of variables including the i-th variable and its parents
in the network. Note that the whole discussion in this paragraph is actually identical for
MRFs with positive factors, the only diﬀerence is the number of factors, but since they are
not the focus of this thesis, we formulate everything in the Bayesian networks framework.
For variable indices r2f1;:::;ngwe deﬁne the number of corresponding possible
vector instantiations nr=jVal(Ar)j:For any joint assignment , if this assignment con-
strained to the variables from Artakes the value of aj
r; j=f1;:::;nrg;i.e.Ar=aj
r,
then the factor log(r)makes a contribution to the objective of a quantity denoted as
r
j= log(r(aj
r)):
We introduce optimization variables q(xj
r), whererenumerates the diﬀerent factors,
andjenumerates the diﬀerent possible assignments to the variables from Ar. These
variables take binary values, so that q(xj
r) = 1if and only if Ar=aj
rand 0 otherwise.
It is important to distinguish the optimization variables from the random variables in
our original graphical model; here we have an optimization variable q(xj
r)for each joint
assignmentaj
rto the model variables Ar.
Letqdenote a vector of the optimization variables fq(xj
r);1rn;1jnrg
anddenote a vector of the coeﬃcients j
rsorted in the same order. Both of these are
vectors of dimension N=Pn
r=1nr. With this interpretation, the MAP objective can be
rewritten as:
max
qnX
r=1nrX
j=1j
rq(xj
r) (3.15)
or, in shorthand, max
q>q.
Now that we have an objective to maximize we need to add some consistency con-
straints that would guarantee that an assignment q2f0;1gNwe get as a solution of
optimization problem is legal, meaning it corresponds to some assignment in X:Namely,
ﬁrst we require that we restrict attention to integer solutions, then we construct two
constraints to make sure that these integer solutions are consistent. The ﬁrst constraint
enforces the mutual exclusivity within a factor and the second one implies that factors
in our network agree on the variables in the intersection of their scopes. In this way we
reformulate the MAP task as an integer linear program, where we optimize the linear
objective of equation (3.15) subject to discussed constraints. We note that the problem
of solving integer linear programs is itself NP-hard, so that we do not avoid the basic
hardness of the MAP problem.
One of the methods often used to tackle ILP problems is the method of linear program
relaxation. In this approach we turn a discrete, combinatorial optimization problem into
a continuous problem. This problem is a linear program (LP), which can be solved in
40polynomial time, and for which a range of very eﬃcient algorithms exists. One can
then use the solutions to this LP to obtain approximate solutions to the MAP problem.
To perform this relaxation, we substitute the condition that the solutions are integer with
a relaxed constraint that they are non-negative.
This linear program is a relaxation of our original integer program, since every assign-
ment toqthat satisﬁes the constraints of the integer problem also satisﬁes the constraints
of the linear program, but not the other way around. Thus, the optimal value of the ob-
jective of the relaxed version will be no less than the value of the (same) objective in
the exact version, and it can be greater when the optimal value is achieved at an assign-
ment toqthat does not correspond to a legal assignment . An important special case
are tree-structured graphs, in which the relaxation is guaranteed to always return integer
solutions, which are in turn optimal (for proof and more detailed discussion see Koller
and Friedman (2009)). Otherwise we get approximate solutions, which in order we need
to transform into integer (and legal) assignments.
One approach is a greedy assignment process, which assigns values to the variables Xi
one at a time. Another approach is to round the LP solution to its nearest integer value.
This approach works surprisingly well in practice and has theoretical guarantees for some
classes of ILPs (Koller and Friedman (2009)).
An alternative method for the MAP problem which also comes from the optimization
theory is called dual decomposition . Dual decomposition uses the principle that our prob-
lem can be decomposed into sub-problems, together with linear constraints (the same as
in ILP) that enforce some notion of agreement between solutions to the diﬀerent prob-
lems. The sub-problems are chosen such that they can be solved eﬃciently using exact
combinatorial algorithms. The agreement constraints are incorporated using Lagrange
multipliers, it is called Lagrangian relaxation, and an iterative algorithm - for example,
a subgradient algorithm - is used to minimize the resulting dual. The initial work on
dual decomposition in probabilistic graphical models was focused on the MAP problem
for MRFs (see Komodakis et al. (2007)).
By formulating our problem as a linear program or its dual, we obtain a very ﬂexible
framework for solving it; in particular, we also can easily incorporate additional con-
straints into the LP, which reduce the space of possible assignments of q, eliminating
some solutions that do not correspond to actual distributions over X. The problems are
convex and in principle they can be solved directly using standard techniques, but the size
of the problems is very large, which makes this approach unfeasible in practice. However,
the LP has special structure: when viewed as a matrix, the equality constraints in this
LP all have a particular block structure that corresponds to the structure of adjacent
clusters. Moreover, when the network is not densely connected, the constraint matrix is
also sparse, thus, standard LP solvers may not be fully suited for exploiting this special
structure. The theory of convex optimization provides a wide spectrum of tools, and
some are already being adapted to take advantage of the structure of the MAP problem
(see for example, Wainwright et al. (2005), Sontag and Jaakkola (2007)). The empirical
evidence suggests that the more specialized solution methods for the MAP problems are
41often more eﬀective.
Other methods. Another method for solving a MAP problem is local search algo-
rithms. It is a heuristic-type solution, which starts with an arbitrary assignment and
performs “moves” on the joint assignment that locally increase the probability. This tech-
nique does not oﬀer theoretical justiﬁcation; however, we can often use prior knowledge
to come up with highly eﬀective moves. Therefore, in practice, local search may perform
extremely well.
There are also searching methods that are more systematic. They search the space
so as to ensure that assignments that are not considered are not optimal, and thereby
guarantee an optimal solution. Such methods generally search over the space of partial
assignments, starting with the empty assignment and successively assigning variables one
at a time. One such method is known as branch-and-bound.
These methods have much greater applicability in the context of marginal MAP prob-
lem, where most other methods are not currently applicable. In the next subsection
we discuss sample-based algorithms which can be applied both to marginal and MAP
inference.
3.2.4 Sampling-based methods for inference
In practice, the probabilistic models that we use can often be quite complex, and simple
algorithms like VE may be too slow for them. In addition, many interesting classes of
models may not have exact polynomial-time solutions at all, and for this reason, much re-
search eﬀort in machine learning is spent on developing algorithms that yield approximate
solutions to the inference problem. In this subsection we consider some sampling methods
that can be used to perform both marginal and MAP inference queries; additionally, they
can compute various interesting quantities, such as the expectation E[f(X)]of a function
of the random vector distributed according to a given probabilistic model.
In general, sampling is rather a hard problem. The aim is to generate a random sample
of the observations of X. However, our computers can only generate samples from very
simple distributions, such as the uniform distribution over [0;1]. All sampling techniques
involve calling some kind of simple subroutine multiple times in a properly constructed
way. For example, in case of multinomial distribution with parameters 1;:::;kinstead
of directly sampling a multinomial variable we can sample a single uniform variable pre-
viously subdividing a unit interval into kregions with region ihaving size i. Then we
sample uniformly from [0;1]and return the value of the region in which our sample falls.
Forward sampling. Now let us return to the case of Bayesian networks (BN). We can
apply the same sampling technique to BNs with multinomial variables. We start from
thenodeswhichdonothaveparents, thesevariablessimplyhavemultinomialdistribution,
and we go down the network to the next generation as arrows point out until we reach
the leaves. Therefore, for a particular node we need to wait until all of its parents are
42sampled. When we know all the values of parents the variable naturally has multinomial
distribution. In the Student example to sample student’s grade, we would ﬁrst sample
an exam diﬃculty d0and an intelligence level i0. Then, once we have samples d0andi0, we
generate a student grade g0fromP(gjd0;i0). There is one problem though, as we cannot
perform it in case of having evidence for any variables besides roots.
Monte Carlo and rejection sampling. Algorithms that construct solutions based on
a large number of samples from a given distribution are referred to as Monte Carlo (MC)
methods. Sampling from an arbitrary distribution plets us compute integrals of the form
EXp[f(X)] =X
xf(x)p(x);
where the summation extends over all possible values of Xandpcan be thought of as
the density of Xwith respect to counting measure. Below we follow the same interpreta-
tion also with regards to joint and conditional distributions.
Iff(X)does not have special structure that matches the BN structure of p, this
integral will be impossible to compute analytically; instead, we will approximate it using
a large number of samples from p. Using Monte Carlo technique we approximate a target
expectation with
EXp[f(X)]IT=1
TTX
t=1f(xt);
wherex1;:::;xTare samples drawn according to p. It is easy to show that ITis an un-
biased estimator for EXp[f(X)]and its variance can be made arbitrarily small with
a suﬃciently large number of samples.
Now let us consider rejection sampling as a special case of Monte Carlo integration.
For example, suppose we have a Bayesian network over the set of variables X=Z[E.
We may use rejection sampling to compute marginal probabilities P(E=e). We can
rewrite the probability as
P(E=e) =X
zP(Z=z;E=e) =X
xP(X=x)I(E=e) =EXp[I(E=e)]
and then take the Monte Carlo approximation. In other words, we draw many samples
frompandreportthefractionofsamplesthatareconsistentwiththevalueofthemarginal.
Importance sampling. Unfortunately, rejection sampling can be very wasteful. If
P(E=e)equals, say, 1%, then we will discard 99% of all samples. A better way of
computing such integrals uses importance sampling. The main idea is to sample from
an auxiliary distribution q(hopefully with q(x)roughly proportional to f(x)p(x)),
and then reweigh the samples in a principled way, so that their sum still approximates
the desired integral.
43More formally, suppose we are interested in computing EXp[f(X)]. Adopting anal-
ogous convention regarding notation for probability distribution we may rewrite this in-
tegral as
EXp[f(X)] =X
xf(x)p(x) =X
xf(x)p(x)
q(x)q(x) =
=EXq[f(X)w(X)]1
TTX
t=1f(xt)w(xt);
wherew(x) =p(x)
q(x)and the samples xtare drawn from q. In other words, instead
of sampling from pwe may take samples from qand reweigh them with w(x); the ex-
pected value of this Monte Carlo approximation will be the original integral. By choosing
q(x) =jf(x)jp(x)R
jf(x)jp(x)dxwe can set the variance of the new estimator to zero. Note that
the denominator is the quantity we are trying to estimate in the ﬁrst place and sampling
from suchqisNP-hard in general.
In the context of our previous example for computing P(E=e), we may take qto be
the uniform distribution and apply importance sampling as follows:
P(E=e) =Ezp[p(ejz)] =Ezq
p(ejz)p(z)
q(z)
=
=Ezqp(e;z)
q(z)
=Ezq[we(z)]1
TTX
t=1we(xt);
wherewe(z) =p(e;z)
q(z). Unlike rejection sampling, this will use all the samples; if p(zje)
is not too far from uniform, this will converge to the true probability after only a very
small number of samples.
Markov chain Monte Carlo. Now let us turn to performing marginal and MAP
inference using sampling. We will solve these problems using a very powerful technique
called Markov chain Monte Carlo (MCMC).
A key concept in MCMC is that of a Markov chain, which is a sequence of random
elements having Markov property (see 2.3). A Markov chain X= (X0;X1;X2;:::)with
each random vector Xitaking values from the same state space Val(X)is speciﬁed by
the initial distribution P(X0=x),x2Val(X), and the set of transition probabilities
P(Xk+1=x0jXk=x)
forx;x02Val(X), which do not depend on k(in this case the Markov chain is called
homogeneous). Therefore, the transition probabilities at any time in the entire process
depend only on the given state and not on the history of the process. In what follows,
we consider ﬁnite state space only so we may assume Val(X) =f1;:::;dg, unless stated
otherwise.
44If the initial state X0is drawn from a vector of probabilities p0, we may represent
the probability ptof ending up in each state after tsteps as
pt=Ttp0;
whereTdenotes the transition probability matrix with Tij=P(Xk+1=ijXk=j),
i;j2f1;:::;dg, andTtdenotes matrix exponentiation. If the limit lim
t!1pt=exists, it
is called a stationary distribution of the Markov chain. A suﬃcient condition for to be
a stationary distribution is called detailed balance:
(j)Tij=(i)Tji
for alli;j2Val(X).
The high-level idea of MCMC is to construct a Markov chain whose states are joint
assignments to the variables in the model and whose stationary distribution is equal to
the model probability p. Then, running the chain for a number of times, we obtain
the sample from the distribution p. In order to construct such a chain, we ﬁrst recall
the conditions under which stationary distributions exist. This turns out to be true under
two suﬃcient conditions: irreducibility , meaning that it is possible to get from any state x
to any other state x0with positive probability in a ﬁnite number of steps, and aperiodicity ,
meaning that it is possible to return to any state at any time. In the context of continuous
variables, the Markov chain must be ergodic, which is a slightly stronger condition than
the above. For the sake of generality, we will require our Markov chains to be ergodic.
At a high level, MCMC algorithms will have the following structure. They take as
an argument a transition operator Tspecifying a Markov chain whose stationary distri-
bution isp, and an initial assignment X0=x0of the chain. An MCMC algorithm then
performs the following steps:
1. Run the Markov chain from x0forBburn-in steps.
2. Run the Markov chain for Nsampling steps and collect all the states that it visits.
The aim of the burn-in phase is to wait until the state distribution is reasonably close to p.
Therefore, weomittheﬁrst Bstatesvisitedbythechainandthenwecollectasamplefrom
the chain of the size N. A common approach to set the number Bis to use a variety of
heuristics to try to evaluate the extent to which a sample trajectory has “mixed”, i.e. when
it is reasonably close to p(see Koller and Friedman (2009)). Also Geyer (2011) advocates
that burn-in is unnecessary and uses other ways of ﬁnding good starting points. Gelman
and Shirley (2012) propose to discard the ﬁrst half of generated sequences. We may then
use these samples for Monte Carlo integration (or in importance sampling). We may
also use them to produce Monte Carlo estimates of marginal probabilities. Finally, we
may take the sample with the highest probability and use it as an estimate of the mode
(i.e. perform MAP inference).
Before wediscuss two mostimportant special cases, note that sampling-based methods
have theoretical asymptotic justiﬁcation. Therefore, their application for ﬁnite samples of
45reasonable size may lead to drastically inaccurate results, especially in sophisticated and
complex models. Successful implementation heavily depends on how well we understand
structure of the model as well as on intensive experimentation. It can also be achieved
by combining sampling with other inference methods.
Metropolis-Hastings Algorithm. The Metropolis-Hastings (MH) algorithm (Hast-
ings (1970)) is one of the ﬁrst ways to construct Markov chains within MCMC. The MH
method constructs a transition operator Tfrom two components:
1. A transition kernel qspeciﬁed by the user. In practice, the distribution q(x0jx)
can take almost any form and very often it is a Gaussian distribution centered at x.
2. An acceptance probability for moves proposed by q, speciﬁed by the algorithm as
A(x0jx) = min
1;p(x)q(x0jx)
p(x0)q(xjx0)
:
At each step, if the Markov chain is in the state x, then we choose a new point x0
according to the distribution q. Then, we either accept this proposed change with the
probability =A(x0jx), or with the probability 1 we remain at our current state.
Notice that the acceptance probability encourages the chain to move towards more likely
points in the distribution (imagine for example that qis uniform); when qsuggests that
we move into a low-probability region, we follow that move only a certain fraction of time.
Given anyqthe MH algorithm ensures that pis a stationary distribution of the resulting
Markov Chain. More precisely, pwill satisfy the detailed balance condition with respect
to the Markov chain generated by MH algorithm. This is a straight consequence of
the deﬁnition of A(x0jx).
AstheresultwewishtobuildtheMarkovchainwithasmallcorrelationbetweensubse-
quent values, which allows to explore the support of the target distribution rather quickly.
This correlation consists of two components. The higher the variance of q, the lower the
correlationbetweenthecurrentstateandthenewlychosenone, andthelowerthevariance
ofq, the lower the correlation when we stay at the same state hitting the low-probability
region. To choose a good kernel qwe need to ﬁnd good balance between the two. For
multivariatedistributionsthecovariancematrixfortheproposaldistributionshouldreﬂect
the covariance structure of the target.
Gibbs sampling. A widely-used special case of the Metropolis-Hastings methods is
Gibbs sampling. It was ﬁrst described in Geman and Geman (1984). Suppose we have
a ﬁnite sequence of random variables X1;:::;Xn. We denote the i-th sample as x(i)=
(X(i)
1;:::;X(i)
n). Starting with an arbitrary conﬁguration x(0)we perform the procedure
below.
Repeat until convergence for t= 1;2;3;::::
1. Setx x(t 1)
462. For each variable Xi
•SampleX0
iP(XijX i)
•Updatex (X(t)
1;:::;X(t)
i 1;X0
i;X(t 1)
i+1;:::;X(t 1)
n)
3. Setx(t) x
ByX iwe denote all the variables in our set except Xi. At each epoch of the step 2
only one site undergoes a possible change, so that successive samples for each iteration
can diﬀer in at most one coordinate. Note that at this step we use updated values of
the variables for which we have already sampled new values. The sampling step is quite
easy to perform because we only condition on variables from Xi-th Markov blanket, which
consists of its parents, children and other parents of its children.
In Geman and Geman (1984) it was stated that the distribution of x(t)converges
toast!1regardless ofx(0). The only assumption is that we continue to visit each
site which is obviously a necessary condition for convergence. As in case of any MCMC
algorithm if we choose an arbitrary starting conﬁguration there is a burn-in phase, for
the list of intuitions on how to decide how many samples we want to discard see Casella
and George (1992). To avoid the high correlation between successive samples in Gibbs
samplerwecanalsotakeevery r-thsampleinsteadofallofthem,whichisratheraquestion
of heuristics and experimenting.
3.3 Learning probabilities in BNs for incomplete data
Here we again consider categorical distributions. Suppose we observe a single incomp-
lete case in our data, which we denote as d2D. Under the assumption of parameter
independence, we can compute the posterior distribution of ijfor our network as follows:
p(ijjd) = (1 p(paj
ijd))fp(ij)g+riX
k=1p(xk
i;paj
ijd)fp(ijjxk
i;paj
i)g:
Each term in curly brackets in this equation is a Dirichlet distribution. Thus, unless
bothXiand all the variables in pa(Xi)are observed in case d, the posterior distribution
ofijwill be a linear combination of Dirichlet distributions, that is a Dirichlet mixture
with mixing coeﬃcients (1 p(paj
ijd))andp(xk
i;paj
ijd),1kri. See Spiegelhalter
and Lauritzen (1990) for the details of derivation.
When we observe a second incomplete case, some or all of the Dirichlet components
in the previous equation will again split into Dirichlet mixtures. More precisely, the pos-
terior distribution for ijwill become a mixture of Dirichlet mixtures. As we continue to
observe incomplete cases, where each case has missing values for the same set of variables,
the posterior distribution for ijwill contain a number of components that is exponential
in the number of cases. In general, for any interesting set of local likelihoods and priors,
the exact computation of the posterior distribution for will be intractable. Thus, we
require an approximation for incomplete data.
47One of the possible ways to approximate is Monte-Carlo methods discussed previously,
for example the Gibbs sampler, which must be irreducible and each variable must be
chosen inﬁnitely often. More speciﬁcally for our case, to approximate p(jD)given
an incomplete data set we start with some initial states of the unobserved variables in
each case (chosen randomly or otherwise) and as a result, we have a complete random
sampleDc. Then we choose some variable Xi[l](variableXiin casel) that is not observed
in the original random sample D, and reassign its state according to the probability
distribution
p(x0
iljDcnfxilg) =p(x0
il;Dcnfxilg)P
x00
ilp(x00
il;Dcnfxilg);
whereDcnxildenotes the data set Dcwith observation xilremoved, and the sum in the de-
nominator runs over all states of the variable Xi. Both the numerator and denominator
can be computed eﬃciently as in (3.6). In the third step we repeat this reassignment for
all unobserved variables in D, producing a new complete random sample D0
c. The fourth
step is to compute the posterior density p(ijjD0
c)as in (3.7) and, under the assumption
of parameter independence, the joint posterior p(jD0
c)will be a product of all densities
p(ijjD0
c). Finally, we iterate through last three steps, and use the average of p(jD0
c)
as our approximation.
Monte-Carlomethodsyield accurateresults butthey areoften intractable, forexample
when the sample size is large. Another approximation that is more eﬃcient than Monte-
Carlo methods and often accurate for relatively large samples is the Gaussian approxi-
mation. The idea is that for large amounts of data we can approximate the distribution
p(jD)/p(Dj)p()as a multivariate-Gaussian distribution, namely
p(jD)p(Dj~)p(~) exp
 1
2( ~)H( ~)>
;
where ~is the conﬁguration of that maximizes g() = ln(p(Dj)p())andHis a
negative Hessian of g(). The vector ~is also called the maximum a posteriori (MAP)
conﬁguration of . There are various methods to compute the second derivatives proposed
in literature (Meng and Rubin (1991), Raftery (1995), Thiesson (1995)).
One more way to learn probabilities from incomplete data is the Expectation-Ma-
ximization (EM) algorithm. It is an iterative algorithm consisting of two alternating
steps - Expectation and Maximization. When the data is incomplete we cannot calculate
the likelihood function as in (3.2) and (3.3). Now instead of maximizing likelihood or log-
likelihood function we will be maximizing the expected log-likelihood of the complete data
set with respect to the joint distribution for Xconditioned on the assigned conﬁguration
of the parameter vector 0and the known data D. The calculation of the expected log-
likelihood (Expectation step) amounts to computing expected suﬃcient statistics . For
incomplete data the expected log-likelihood takes the following form
E[`()jD;0] =nX
i=1qiX
l=1riX
k=1^Nilklog(ilk);
48where
^Nilk=E[I(Xi=xk
i;pa(Xi) =pal
i)jD;0] =mX
j=1P(Xi=xk
i;pa(Xi) =pal
ijdj;0):
Heredjis possibly incomplete j-th case inD. WhenXiand all the variables in pa(Xi)
are observed, the term for this case requires a trivial computation: it is either zero or
one. Otherwise, we can use any Bayesian network inference algorithm discussed above to
evaluate the term.
Having performed the Expectation step we want to ﬁnd the new parameter vector,
which is obtained by maximization of the expected log-likelihood (Maximization step).
In our case we have new parameters on the r-th iteration
r
ilk=^NilkPri
k=1^Nilk:
We start algorithm with an arbitrary (for example, random) parameter conﬁguration 0
and iteratively perform two steps described above until the convergence. Dempster et al.
(1977) showed that, under certain regularity conditions, iterations of the expectation and
maximization steps will converge to a local maximum.
3.4 Learning parameters for CTBNs
The new method we propose in next chapters for learning CTBNs is capable of performing
bothtasksofparameterlearningandstructurelearningsimultaneously,althoughnaturally
these tasks can be performed separately. In this section we review selected methods
focused only on parameter learning.
3.4.1 Data
In this thesis we discuss both complete and incomplete data. In essence, CTBN models
the joint trajectories of its variables, hence having complete, or fully observed, data means
thatforeachpointintimeofeachtrajectory, weknowthefullinstantiationtoallvariables.
ByD=f[1];:::; [m]gwe denote a data set of trajectories. In case of complete
data each[i]is a complete set of state transitions and the times at which they occurred.
Another way to specify each trajectory is to assign a sequence of states xi2Val(X), each
with an associated duration.
In contrast to the deﬁnition of complete data, an incomplete data set can be repre-
sented by a set of one or more partial trajectories. A partially observed trajectory 2D
can be speciﬁed as a sequence of subsystems SiofX, each with an associated duration.
AsubsystemSdescribes the behaviour of the process over a subset of the full state space,
i.e.Val(S)Val(X). It is simply a nonempty subset of states of X, in which we know
the system stayed for the duration of the observation. Some transitions are partially ob-
served, i.e. we know only that they take us from one subsystem to another. Transitions
49from one state to another within the subsystem are fully unobserved, hence, we do not
know how many transitions there are inside of a particular subsystem nor when they do
occur.
3.4.2 Learning parameters for complete data
Recall,thatCTBN Nconsistsoftwoparts. Theﬁrstisaninitialdistribution PX
0,speciﬁed
as a Bayesian network over X. The second is a continuous transition model, speciﬁed as
a directed (and possibly cyclic) graph and a set of conditional intensity matrices (CIM),
one for each variable Xiin the network. For the purposes of this section we abbreviate
paG(Xi)aspa(Xi)and we denote CIMs as QXijpa(Xi). Recall that each QXijpa(Xi)consists
of intensity matrices QXijpai, where paiis a single conﬁguration of pa(Xi). Strictly
speaking, paiis one of the possible parent conﬁgurations pa1
i;:::;paqi
isimilar to (3.1).
Intermsofpureintensityparameterizationwedenoteelementsofthesematricesas qxx0jpai
andqxjpai. Note, that by Theorem 2.9 we can divide the set of parameters in terms of
mixed intensity into two sets. Then for each variable Xiand each instantiation paiof its
set of parents pa(Xi)the parameters of QXijpa(Xi)will beqXi=fqxjpai:x2Val(Xi)g
andXi=fxx0jpai:x;x02Val(Xi); x6=x0g. More precisely, for each Xiand every
x2Val(Xi)we have
xx0jpai=qxx0jpaiP
x0qxx0jpai; x02Val(Xi); x6=x0:
The learning problem for the initial distribution is a Bayesian network learning task,
which was discussed previously in this chapter. Therefore, it remains to learn the vector
of parameters (q;).
Likelihood estimation. Let us start from a fully observed case and a single homo-
geneous Markov process X(t):As all the transitions are observed, the likelihood of D
can be decomposed as a product of the likelihoods for individual transitions d. Let
d=hxd;td;x0
di2Dbe the transition where Xtransitions to state x0
dafter spending
the amount of time tdin statexd. Using the mixed intensity parameterization, we can
write the likelihood for the single transition das
LX(q;:d) =LX(q:d)LX(:d) =qxdexp( qxdtd)xdx0
d:
Then multiplying the likelihoods for each transition din our dataDwe can summarize
it in terms of suﬃcient statistics T[x]which describes the amount of time spent in each
statex2Val(X)andM[x;x0]which encodes the number of transitions from xtox0,
wherex6=x0as follows:
LX(q;:D) = Y
d2DLX(q:d)! Y
d2DLX(:d)!
= Y
xqM[x]
xexp( qxT[x])! Y
xY
x06=xM[x;x0]
xx0!
;(3.16)
50whereM[x] =P
x0M[x;x0].
Now in case of CTBNs, each variable Xof the networkNis conditioned on its par-
ent set Pa=paG(X), and each transition of Xmust be considered in the context of
the instantiation paofPa. With complete data, we know the value of Paduring the en-
tire trajectory, so at each point in time we know precisely which homogeneous intensity
matrix QXjpagoverned the dynamics of X.
Thus, the likelihood decomposes into the product of likelihoods, each corresponding
to the variable in the network, as
LN(q;:D) =Y
Xi2XLXi(qXijUi;XijUi:D) =Y
Xi2XLXi(qXijUi:D)LXi(XijUi:D):
ThetermLX(XjPa:D)istheprobabilityofthesequenceofstatetransitions,disregarding
thetimesbetweentransitions. Thesestatechangesdependonlyonthevalueoftheparents
at the moment of the transition. For each variable X2XletM[x;x0jpa]denote
the number of transitions from X=xtoX=x0while Pa=pa. Then, with this set of
suﬃcient statistics M[x;x0jpa], we have
LX(XjPa:D) =Y
paY
xY
x06=xM[x;x0jpa]
xx0jpa:
The computation of LX(qXjPa:D)is more subtle since the duration in the state can be
terminated not only due to a transition of X, but also due to a transition of one of its
parents. The total amount of time where X=xandPa=pacan be decomposed into
two diﬀerent kinds of durations T[xjpa] =Tr[xjpa] +Tc[xjpa], whereTr[xjpa]is
the total length of the time intervals that terminate with Xremaining equal to x, and
Tc[xjpa]is the total length of the time intervals that terminate with a change in the value
ofX. However, it is easy to show that we do not need to maintain the distinction between
the two of them and we can use the set of T[xjpa]as suﬃcient statistics.
Finally, wecanwritethelog-likelihoodasasumoflocalvariablelikelihoodsoftheform
`X(q;:D) =`X(q:D) +`X(:D) =
="X
paX
xM[xjpa] logqxjpa qxjpaT[xjpa]#
+"X
paX
xX
x06=xM[x;x0jpa] logxx0jpa#
:
(3.17)
Nowwecanwritethemaximum-likelihood(MLE)parametersasfunctionsofthesuﬃcient
statistics as follows (for the proof see Nodelman (2007)):
^qxjpa=M[xjpa]
T[xjpa]; ^xx0jpa=M[x;x0jpa]
M[xjpa]:
The Bayesian approach. The other way to estimate parameters in case of fully ob-
served data is the Bayesian approach. To perform Bayesian parameter estimation, simi-
larly to the case of Bayesian networks, for computational eﬃciency we use a conjugate
51prior (one where the posterior after conditioning on the data is in the same parametric
family as the prior) over the parameters of our CTBN.
For a single Markov process we have two types of parameters, a vector of parameters 
for categorical distribution and qfor exponential distribution. An appropriate conjugate
prior for the exponential parameter qis the Gamma distribution P(q) =Gamma (;),
and as we mentioned in Section 3.1, the standard conjugate prior to categorical distri-
bution is a Dirichlet distribution P() =Dir(xx1;:::;xxk). The posterior distribu-
tionsP(jD)andP(qjD)given data are Dirichlet and Gamma distributions, respec-
tively.
In order to apply this idea to an entire CTBN we need to make two standard assump-
tions for parameter priors in Bayesian networks, global parameter independence :
P(q;) =Y
X2XP(qXjpaG(X);XjpaG(X))
andlocal parameter independence for each variable Xin the network:
P(qXjPa;XjPa) = Y
xY
paP(qxjpa)! Y
xY
paP(xjpa)!
:
If our parameter prior satisﬁes these assumptions, so does our posterior, as it belongs
to the same parametric family. Thus, we can maintain our parameter distribution in
the closed form, and update it using the obvious suﬃcient statistics M[x;x0jpa]forxjpa
andM[xjpa];T[xjpa]forqxjpa.
Given a parameter distribution, we can use it to predict the next event, averaging out
the event probability over the possible values of the parameters. As usual, this prediction
is equivalent to using “expected” parameter values, which have the same form as the MLE
parameters, but account for the “imaginary counts” of the hyperparameters:
^qxjpa=xjpa+M[xjpa]
xjpa+T[xjpa]; ^xx0jpa=xx0jpa+M[x;x0jpa]
xjpa+M[xjpa]:
Note that, in principle, this choice of parameters is only valid for predicting a single
transition, afterwhichweshouldupdateourparameterdistributionaccordingly. However,
as is often done in other settings, we can approximate the exact Bayesian computation by
“freezing” the parameters to these expected values, and use them for predicting an entire
trajectory.
3.4.3 Learning parameters for incomplete data
Recall, that in case of Bayesian networks one of the methods to deal with missing data
was Expectation-Maximization (EM) algorithm. Here we provide a concise description
of the algorithm based on EM for CTBNs presented in detail in Nodelman et al. (2012).
We start again with reviewing the EM scheme for a single Markov process X, which is
the basis of the algorithm for CTBNs. Let D=f[1];:::; [m]gdenote the set of all
partially observed trajectories of X.
52For each partial trajectory [i]2Dwe can consider the space H[i]of possible comple-
tions of this trajectory. For every transition of [i]each completion h[i]2H[i]speciﬁes
which underlying transition of Xoccurred. Also it speciﬁes all the entirely unobserved
transitions of X. Combining [i]andh[i]gives us a complete trajectory +[i]overX.
Note that, in a partially observed trajectory, the number of possible unobserved transi-
tions is unknown. Moreover, there are uncountably many times at which each transition
can take place. Nevertheless, we can deﬁne the set D+=f+[1];:::;+[m]gof comple-
tions of all of the partial trajectories in D. For examples of completions see Nodelman
et al. (2012).
As we mentioned in the previous subsection, the suﬃcient statistics of the set of
complete trajectories D+for a Markov process are T[x], the total amount of time that X
staysinx, andM[x;x0], thenumberoftimesinwhich Xtransitionsfrom xtox0. Applying
logarithm to ( 3:16) we can write the log-likelihood `X(q;:D+)forXas an expression
of these suﬃcient statistics.
Letrbe a probability density over each completion in H[i]which, in turn, yields
a density over possible completions of the data D+. We can write the expectations of
the suﬃcient statistics with respect to the probability density over possible completions
of the data as T[x],M[x;x0]andM[x]. These expected suﬃcient statistics allow us to
write the expected log-likelihood for Xas
Er[`X(q;:D+)] =Er[`X(q:D+)] +Er[`X(:D+)] =
=X
x 
M[x] ln(qx) qxT[x] +X
x06=xM[x;x0] ln(xx0)!
:
Now we can use the EM algorithm to ﬁnd maximum-likelihood parameters q;ofX.
The EM algorithm begins with an arbitrary initial parameter assignment, q0;0. It then
repeats the two steps, Expectation and Maximization, updating the parameter set, until
convergence. After the k-th iteration we start with parameters qk;k. The Expectation
step goes as following: using the current set of parameters, we deﬁne for each [i]2D,
the probability density rk(h[i]) =p(h[i]j[i];qk;k). We then compute expected suﬃ-
cientstatistics T[x],M[x;x0]andM[x]accordingtothisposteriordensityovercompletions
of the data given the data and the model. Using the expected suﬃcient statistics we just
have computed as if they came from a complete data set, we set qk+1andk+1to be
the new maximum likelihood parameters for our model as follows
qk+1
x=M[x]
T[x]; k+1
xx0=M[x;x0]
M[x]: (3.18)
The diﬃcult part in this algorithm is the Expectation Step. The space over which we are
integrating is highly complex, and it is not clear how to compute the expected suﬃcient
statistics in a tractable way.
In Nodelman et al. (2012) and Nodelman (2007) authors provided in detail the algo-
rithmonhowtocomputeexpectedsuﬃcientstatisticsforan n-statehomogeneousMarkov
processXtwith intensity matrix QXwith respect to the posterior probability density over
53completions of the data given the observations and the current model. The statistics are
computed for each partially observed trajectory 2Dseparately and then the results
are combined.
A partially observed trajectory is given as a sequence of Nsubsystems so that
the state is restricted to subsystem Siduring the interval [ti;ti+1)for0iN 1.
To conduct all the necessary computations, for each time t, the forward and backward
probabilityvectors tandtaredeﬁned,whichincludeevidenceofanytransitionattime t,
and also vectors  
tand+
t, neither of which include evidence of a transition at time t.
The total expected time E[T[j]]is obtained by summing the integrals over all intervals
of constant evidence [v;w)with the subsystem Sto which the state is restricted on that
interval. Each integrand is an expression containing v,wandQS. The computations for
each integral are performed via the Runge-Kutta method of fourth order with an adaptive
step size.
Regarding the expected number of transitions E[M[x;x0]]from the state xtox0dis-
crete time approximations of M[x;x0]are considered which in the limit as the size of
the discretization goes to zero yields an exact equation. As a result we get the sum of
expressions where each summand is associated with a time interval. The overall expres-
sion for the expected number of transitions consists of two parts: the sum of products
corresponding to intervals with partially observed transitions and containing  
tand+
t
for diﬀerent time points tand the sum of integrals of practically identical form to those
obtained for total expected time.
In order to compute tandta forward-backward style algorithm (Rabiner and Juang
(1986))overtheentiretrajectoryisusedtoincorporateevidenceandgetdistributionsover
the state of the system at every time ti. If needed it is possible to exclude incorporation of
the evidence of the transition from either forward or backward vector and also obtain  
t
and+
t. We can then write the distribution over the state of the system at time tgiven
all the evidence.
Continuous time Bayesian networks are a factored representation for homogeneous
Markovprocesses, hence, extendingtheEMalgorithmtotheminvolvesmakingitsensitive
to a factored state space. As mentioned previously, the log-likelihood decomposes as
the sum of local log-likelihoods for each variable. With the suﬃcient statistics T[xjpa],
M[x;x0jpa]andM[xjpa]of the set of complete trajectories D+for each variable Xin
CTBNNthe likelihood for each variable Xfurther decomposes as in (3.17). By linearity
of expectation, the expected log-likelihood function also decomposes in the same way.
So we can write the expected log-likelihood Er[`(q;:D+)]as a sum of terms, one for
each variable X, in a similar form as (3.17), but using the expected suﬃcient statistics
T[xjpa],M[x;x0jpa]andM[xjpa].
The EM algorithm for CTBNs is essentially the same as for homogeneous Markov
processes. We need only specify how evidence in the network induces evidence on the in-
duced Markov process, and how expected suﬃcient statistics in the Markov process give
us the necessary suﬃcient statistics for CTBN.
TheMaximizationstepispracticallythesameasin(3.18), wejustuseproperexpected
54suﬃcient statistics for the CTBN case:
qk+1
xjpa=M[xjpa]
T[xjpa]; k+1
xx0jpa=M[x;x0jpa]
M[xjpa]:
The Expectation step is again more diﬃcult and could be done by ﬂattening the CTBN
into a single homogeneous Markov process with a size of the state space exponential
in the number of variables. Then we follow the method described above. However, as
the number of variables in the CTBN grows the process becomes intractable, so we are
forced to use approximate inference.
We want this approximate algorithm to be able to compute approximate versions of
the forward and backward messages tandsand extract the relevant suﬃcient statistics
from these messages eﬃciently. In the next subsection we review a cluster graph infe-
rence algorithm which can be used to perform this type of approximate inference. Using
obtained cluster beliefs (see below) we can compute ti+1andtiand use them in the
forward-backward message passing procedure. The cluster distributions are represented
as local intensity matrices from which we can compute the expected suﬃcient statistics
over families Xi;paG(Xi)as described above.
3.5 Inference for CTBNs
To gain the perspective on the whole concept of continuous time Bayesian networks and
their power, similarly to Bayesian networks, we discuss the questions of inference although
it is not the key subject of this thesis. We start with a discussion of the types of queries
we might wish to answer and the diﬃculties of the exact inference.
Inference for CTBNs can take a number of forms. The common types of queries are:
•querying the marginal distribution of a variable at a particular time or also the time
at which a variable ﬁrst takes a particular value,
•querying the expected number of transitions for a variable during a ﬁxed time in-
terval,
•querying the expected amount of time a variable stayed in a particular state during
an interval.
Previously we showed that we can view CTBN as a compact representation of a joint
intensity matrix for a homogeneous Markov process. Thus, at least in principle, we can
use CTBN to answer any query that we can answer using an explicit representation of
a Markov process: we can form the joint intensity matrix and then answer queries just as
we would do for any homogeneous Markov process.
The obvious ﬂaw is that this approach for answering these queries requires us to
generate the full joint intensity matrix for the system as a whole. The size of the matrix
is exponential in the number of variables, making this approach generally intractable. The
55graphical structure of the CTBN immediately suggests that we perform the inference in
a decomposed way, as in Bayesian networks. Unfortunately, the problems are signiﬁcantly
more complex in this setting.
In Nodelman et al. (2002) the authors describe an approximate inference algorithm
based on ideas from clique tree inference, but without any formal justiﬁcation for the algo-
rithm. More importantly, the algorithm covers only point evidence, meaning observations
of the value of a variable at a point in time, but in many applications we observe a variable
for an interval or even for its entire trajectory. Therefore, we shortly describe an approx-
imate inference algorithm called Expectation Propagation (EP) presented in Nodelman
et al. (2005) that allows both point and interval evidence. The algorithm uses message
passing in a cluster graph (with clique tree algorithms as a special case), where the clus-
ters do not contain distributions over the cluster variables at individual time points, but
over trajectories of the variables through a duration.
As we discussed in this chapter, in cluster graph algorithms we construct a graph
whose nodes correspond to clusters of variables and then pass messages between these
clusters to produce an alternative parameterization, in which the marginal distribution of
the variables in each cluster can be read directly from the cluster. In discrete graphical
models,whentheclustergraphisacliquetree,twopassesofthemessagepassingalgorithm
produce the exact marginals. In generalized belief propagation (Yedidia et al. (2001)),
messagepassingisappliedtoagraphwhichisnotacliquetree, inwhichcasethealgorithm
may not converge, and produces only approximate solutions. There are several forms of
message passing algorithm as we have discussed in Subsection 3.2.2. The algorithm of
Nodelman et al. (2005) is based on multiply-marginalize-divide scheme of Lauritzen and
Spiegelhalter (1988), which we now brieﬂy review.
A cluster graph is deﬁned in terms of a set of clusters Ci, whose scope is some subset
of the variables X. Clusters are connected to each other by edges, along which messages
are passed. The edges are annotated with a set of variables called a sepset Si;j, which
is the set of variables in Ci\Cj. The messages passed over an edge between CiandCj
are factors over the scope Si;j. Each clusterCimaintains a potential i, which is a factor
reﬂecting its current beliefs over the variables in its scope. Each edge similarly maintains
a message i;jwhich encodes the last message sent over the edge. The potentials are
initialized with a product of some subset of factors parameterizing the model (CIMs in
our setting). Messages are initialized to be uninformative. Clusters then send messages to
each other, and use incoming messages to update their beliefs over the variables in their
scope. The message mi!jfromCitoCjis the marginal distribution Si;jaccording to i.
The neighbouring cluster Cjassimilates this message by multiplying it into i, but avoids
double-counting by ﬁrst dividing by the stored message i;j. Thus, the message update
takes the form j  jmi!j
i;j.
In the algorithm the cluster beliefs represent not the factors over values of random
variables themselves, but rather cluster potentials and messages both encode measures
over entire trajectories of the variables in their scope. The number of parameters grows
exponentially with the size of the network, and thus we cannot pass messages exactly
56without giving up the computational eﬃciency of the algorithm. To address this issue
Nodelman et al. (2005) used the expectation propagation (EP) approach of Minka (2001),
which performs approximate message passing in cluster graphs. In order to get an approx-
imate message each message mi!jis projected into a compactly representable space so as
to minimize the KL-divergence between the message and its approximation. To encode
the cluster potentials CIMs are used. In order to apply the EP algorithm to clusters of
this form some basic operations over CIMs need to be deﬁned. They include CIM product
and division, approximate CIM marginalization, as well as incorporating the evidence into
CIM.
Themessagepropagationalgorithmisﬁrstconsideredforonesegmentofthetrajectory
with constant continuous evidence. Exactly the same as for Bayesian networks, this
process starts with constructing the cluster tree for the graph G. Note that cycles do not
introduce new issues. We can simply moralize the graph connecting all parents of a node
with undirected edges and then make all the remaining edges undirected. If there is
a cycle, it simply turns into a loop in the resulting undirected graph. Next we select a set
of clustersCi. These clusters can be selected so as to produce a clique tree for the graph,
using any standard method for constructing such trees. We can also construct a loopy
cluster graph and use generalized belief propagation. We did not discuss this topic in
the thesis (for more details see Koller and Friedman (2009)). The message passing scheme
described in this section is the same in both cases.
The algorithm iteratively selects an edge connecting the clusters CiandCjin the clus-
ter graph and passes the message from the former to the latter. In clique tree propagation
the order in which we chose edges was basically ﬁxed, meaning that we started from leaves
to roots performing an upward pass and then going in the opposite direction. In genera-
lized belief propagation, we might use a variety of message passing schemes. Convergence
occurs when messages cease to aﬀect the potentials which means that neighboring clus-
tersCiandCjagree on the approximate marginals over the variables from Si;j.
Now we can generalize the algorithm for a single segment to trajectories containing
multiple segments of continuous evidence. Nodelman et al. (2005) applied this algorithm
separately to every segment, passing information from one segment to the next one in
the form of distributions. More precisely, consider a trajectory deﬁning a sequence of
time points t1;:::;tn, with constant continuous evidence on every interval [ti;ti+1)and
possible point evidence or observed transition at each ti. Then a sequence of cluster
graphs over each segment is constructed. Starting from the initial segment EP inference
is run on each cluster graph using the algorithm for a single segment described above,
and the distribution at the end time point of the interval is computed. The resulting
distribution is then conditioned on any point evidence or the observed transition, and
next used as the initial distribution for the next interval.
However, there is one subtle diﬃculty relating to the propagation of messages from
one interval to another. If a variable Xappears in two clusters CiandCjin a cluster
graph, the distribution over its values in these two clusters is not generally the same, even
if the EP computation converges. The reason is that even calibrated clusters only agree
57on the projected marginals over their sepset, not the true marginals. To address this issue
and to obtain a coherent distribution which can be transmitted to the next cluster graph
the individual cluster marginals and sepsets for the state variables at the end time point
of the previous interval are recalibrated to form a coherent distribution (the conditioning
on point evidence can be done at the same time if needed). Then we can extract the new
distribution as a set of calibrated cluster and sepset factors, and introduce each factor
into the appropriate cluster or sepset in the cluster graph for the next time interval.
Theabovealgorithmperformsthepropagationofbeliefsforwardintime. Itisalsopos-
sibletodoasimilarpropagationbackwardsandpassmessagesinreverse, wherethecluster
graph for one time interval passes a message to the cluster graph for the previous one.
Also to achieve more accurate beliefs we can repeat the forward-backward propagation
until the entire network is calibrated, essentially treating the entire network as a single
cluster graph. Note that since one cluster graph is used for each segment of ﬁxed con-
tinuous evidence, then each cluster will approximate the trajectory of all the variables it
contains as a homogeneous Markov process for the duration of the entire segment. There-
fore, the choice of segments and the resulting subsets of variables, over which we compute
the distribution, determine the quality of the approximation.
58Chapter 4
Structure learning for Bayesian
networks
Recall the Deﬁnition 2.3 of Bayesian Networks (BN), the notion of which combines
the structure given by a Directed Acyclic Graph (DAG) and the probability distribu-
tion encoded by Conditional Probability Distributions (CPD). By far, in Chapter 3 we
discussed the problem of ﬁnding CPDs and making the inference given the structure. In
this chapter we will discuss the problem of learning the structure of Bayesian networks.
In Section 4.1 we brieﬂy review known approaches to the problem. In Section 4.2 we
recall partition MCMC algorithm for learning the structure of the network, whose part
concerning the division of the graph into layers will be the ﬁrst step of our new method.
In Sections 4.3 and 4.4 we present a novel approach to structure learning with the use of
the above algorithm and LASSO approach for continuous and discrete data, respectively.
Section 4.5 is dedicated to numerical results.
4.1 Problem of learning structure of Bayesian Networks
Structure learning is known to be a hard problem, especially due to the superexponential
growth of the DAG space when the number of nodes is increasing. Generally speaking
the literature on the structure learning can be divided into three classes: constraint-
based methods, score-and-search algorithms and the dynamic programming approach (as
discussed for example in Koller and Friedman (2009)), even though this division is not
that strict. The contents of this section come mostly from Kuipers and Moﬀa (2017) and
Daly et al. (2011).
Constraint-based methods use conditional independence tests to obtain information
about the underlying causal structure. They start from the full undirected graph and
then make decisions about removing the edge in the network based on tests of conditional
independence. The widely used algorithm of this nature, PC algorithm (Spirtes et al.
(2000)), and constraint-based methods in general are sensitive to the order in which they
are run. However Colombo and Maathuis (2014) proposed some modiﬁcations for PC
algorithm to remove either partially or altogether this dependence. These methods scale
59well with the dimension but are sensitive to local errors of the independence tests which
are used.
One of the most widely studied ways of learning a Bayesian network structure has
been the use of so-called ’score-and-search’ techniques. These algorithms comprise of:
•a search space consisting of the various allowable states, each of which represents
a Bayesian network structure;
•a mechanism to encode each of the states;
•a mechanism to move from state to state in the search space;
•a scoring function assigning some score to a state in the search space which describes
the goodness of ﬁt with the sample data.
Also some hybrid methods combining ideas from both techniques were proposed, for
example the max-min-hill-climbing of Tsamardinos et al. (2006).
Within the family of search and score methods we can distinguish a separate class
of MCMC methods for the graph space exploration. Their main and huge advantage is
that they can provide a collection of samples from the posterior distribution of the graph
given the data. This means that rather than making the inference based on a single
graphical model, we can account for model uncertainty by averaging over all the models
in the obtained class. In particular, we can estimate the expectation of any given network
feature, such as the posterior probability of an individual edge, by averaging the posterior
distributions under each of the models, weighted by their posterior model probabilities
(Madigan et al. (1995), Kuipers and Moﬀa (2017)). This is especially important in high
dimensional domains with sparse data where the single best model cannot be clearly
identiﬁed, so the inference relying on the best scoring model is not justiﬁed.
The ﬁrst MCMC algorithm over graph structures is due to Madigan et al. (1995),
later reﬁned by Giudici and Castelo (2003). To improve on the mixing and convergence,
Friedman and Koller (2001) instead suggested to build a Markov chain on the space of
node orders, at the price of introducing a bias in the sampling. For smaller systems with
smaller space and time complexity one of the eﬃcient approaches is the dynamic prog-
ramming (Koivisto and Sood (2004)), which can be further used to extend the proposals
of standard structure MCMC approach in a hybrid method (Eaton and Murphy (2007)).
Within the MCMC approach, to avoid the bias while keeping reasonable convergence
rate, Grzegorczyk and Husmeier (2008) more recently proposed a new edge reversal move
method combining ideas both of standard structure and order based MCMC. Recently
Kuipers and Moﬀa (2017) presented another MCMC algorithm designed on the combi-
natorial structure of DAGs, with the advantage of improving convergence with respect
to structure MCMC, while still providing an unbiased sample since it acts directly on
the space of DAGs. Moreover, it can also be combined with the algorithm of Grzegorczyk
and Husmeier (2008) to improve the convergence rate even further.
604.2 Partition MCMC method
In this section we describe the Partition MCMC algorithm of Kuipers and Moﬀa (2017),
which will be the base of our novel method for learning the structure of BNs. This algo-
rithm considers combinatorial representation of DAGs to build an eﬃcient MCMC scheme
directly on the space of DAGs. Its convergence is better than that of the structure MCMC
and does not introduce bias as the order based MCMC. As we mentioned, the authors
also proposed a way to combine their method with the new edge reversal move approach
of Grzegorczyk and Husmeier (2008) and improve upon their MCMC sampler.
First we need to introduce the notion of layersandpartitions for DAG. Given DAG
G= (V;E)wedeﬁnelayers `iofthenodes(calledinterchangeablyvariables)inthenetwork
as follows:
•`0=fv2V:paG(v) =;gis the layer of the nodes which do not have any parents;
•having deﬁned the layer `ifori= 0;1;:::;k 1we deﬁne the next layer as
`k=fv2V:9w2`k 1such that w2paG(v)andpaG(v)Lk 1g;
whereLk 1=S
ik 1`i.
Note that variables from the same layer do not have arrows between them, and that each
variable (except for the layer `0) has at least one arrow directed towards it from any
variable from the adjacent previous layer. For instance, the graph in Figure 4.1 has three
layers:`0=f1;3;5g,`1=f4gand`2=f2g.
Suppose that for some arbitrary graph we have q+1layers. Each layer `ihas a certain
amountkiof nodes, which in sum gives the total number of nodes d, i.e.qP
i=0ki=d.
In addition, with each layer representation there is associated a permutation of nodes,
where we list nodes in the layer order. More precisely, ﬁrst we write nodes from the ﬁrst
layer, then from the second one, etc. For the graph in Figure 4.1 we have the partition
= [3;1;1]and the permutation = [1;3;5;4;2]. Together a pair (;)is called a
labelled partition .
KuipersandMoﬀa(2017)proposedaneﬃcientMCMCalgorithmforexploringthespa-
ce of partitions to ﬁnd the most probable layer representation given the observed data.
Although the full algorithm is suited for structure learning, we want to improve on this
algorithm and replace the second part of it with the LASSO estimator. The authors
deﬁne an MCMC algorithm on the space of node partitions avoiding in this way over-
representation of certain DAGs. Compared to other MCMC methods mentioned above
partitionMCMCisfasterthanstructureMCMCofMadiganetal.(1995). Itisslowerthan
order MCMC of Friedman and Koller (2001) but does not introduce any bias. The basic
move consists of splitting one element of the partition (i.e. layer) into two parts or joining
twoadjacentelements(theauthorsalsoproposeanadditionalmoveconsistingofswapping
two nodes in adjacent layers). All the partitions reachable from a given partition in
61Figure 4.1: An example of partition representation of the DAG.
one basic move are called the neighbourhood of the partition. So the MCMC scheme
consists of sampling a partition from the neighbourhood of the previous partition with
a small probability to stay still deﬁned by the user. The obtained partition is scored and
the score coincides with the posterior probability of the labelled partition. After sampling
the partition we sample a single DAG weighted according to its posterior. Then we can
average the acquired DAGs in the MCMC chain and choose the model. However, we
propose to change the step where we sample DAG from the posterior distribution and
average DAGs from the MCMC chain. It is well suited for inference and estimation of
network parameters but we believe that we can improve the Bayesian averaging approach
in the case of structure learning. We propose to use partition MCMC for ﬁnding the best
scoring partition and next to use it for recovering arrows with the LASSO estimator where
each parameter corresponds to a certain arrow in the network.
4.3 The novel approach to structure learning
We want to combine advantages of partition MCMC and LASSO for linear models. First
we ﬁnd the best layer representation using partition MCMC algorithm. Next we obtain
the ﬁnal DAG solving dLASSO problems, where dis the number of variables (nodes).
Having found the most probable layer representation for a DAG we consider two models:
one for continuous data and one for discrete data.
4.3.1 Gaussian Bayesian Networks
For the continuous case we consider Gaussian Bayesian Networks (GBN) introduced in
Section 3.1. We denote as Xm
ithem-th random variable in the i-th layer, where m2
f1;:::;kig. We assume that each m
ihas the normal distribution N(0;m
i). We also
assume that each m
iis independent of all Xm
i. Now given the partition [k0;k1;:::;kq]
we can write the problem of ﬁnding the DAG structure as a set of the following dlinear
62model problems:
X1
0=1
0;0+1
0
...
Xk0
0=k0
0;0+k0
0
X1
1=1
1;0+1;1
1;0X1
0++1;k0
1;0Xk0
0+1
1
...
Xk1
1=k1
1;0+k1;1
1;0X1
0++k1;k0
1;0Xk0
0+k1
1
X1
2=1
2;0+1;1
2;0X1
0++1;k0
2;0Xk0
0+1;1
2;1X1
1++1;k1
2;1Xk1
1+1
2
...
Xkq
q=kq
q;0+X
j<q;
1mjkjkq;mj
q;jXmj
j+kq
q:(4.1)
Then the problem of ﬁnding DAG’s structure is equivalent to the problem of ﬁnding
non-zero parameters ml;mi
l;i. This corresponds to starting from the full possible graph
and removing non-existing edges by shrinking the parameters to 0. It is possible due to
the fact that we have a partition, where we know which nodes can be parents for which
nodes. This would not be possible otherwise because the graph has to be acyclic and
we would have to introduce other constraints to the optimization problem. To solve this
problem we will apply LASSO regression to each linear model, which tends to shrink
the coeﬃcients to 0 by penalizing those coeﬃcients with `1-norm.
Letmbe the number of observations. Let (Xik)be the matrix of observations,
wherei2f1;:::;mgandk2f1;:::;dg:Then byXjwe denote the matrix formed
by the columns corresponding to the j-th layer. Similarly, by X0:(j 1)we denote the ma-
trix which consists of the columns of the matrix of observations corresponding to the ﬁrst
jlayers. By Xj[i]we denote the column of the matrix Xjcorresponding to the i-th
variable from the j-th layer and by Xiwe denote the row of the matrix (Xik)correspond-
ing to the i-th observation. Moreover, let i
j= [i;1
j;1;:::;i;k0
j;1;i;1
j;2;:::;i;kj 1
j;j 1], where
j=f1;:::;qgandi=f1;:::;kjg. To ﬁnd the required vectors i
jwe solve the following
doptimization problems
^i
j= argmin
2Rk0++kj 1[RSSj;i() +j;ikk1]; (4.2)
whereRSSj;i() = 1=2kXj[i] >X0:(j 1)k2
2is a residual sum of squares for the i-th
variable in the j-th layer andkk1=j 1P
l=0klP
ml=1jml
ljis thel1-norm of. Note, that 
depends both on jandi. The tuning parameters j;i>0balance the minimization of
the cost function and the penalty function. The form of the penalty is crucial, because
its singularity at the origin implies that some coordinates of the minimizer ^i
jare exactly
equal to 0 if j;iis suﬃciently large. Thus, starting from the graph with all possible
arrows for the given layer representation (i.e. there are arrows from variables on each
63layer towards all the variables in next layers) we remove irrelevant edges. The functions
RSSj;i()and the penalty are convex, so (4.2) is a convex minimization problem. This is
an important fact from both practical and theoretical points of view.
4.3.2 Theoretical results for GBNs
BySj;iwe denote the support of the true vectors of parameters i
j;i.e. the sets of non-
zero coordinates of each i
j, and byS=fS1;1;:::;S 1;k1;S2;1;:::;Sq;kqg:Moreover,i
j;min
is the smallest in the absolute value element of i
jrestricted to Sj;i. The setSc
j;idenotes
the complement of Sj;i, that is the set of zero coordinates of i
j:For any vector awe
denote itsl1-norm bykak1= maxkjakj:For a vector aand a subset of indices IbyaI
we denote the vector arestricted to its coordinates from the set I, i.e. (aI)i=aifor
i2Iand(aI)i= 0otherwise. Moreover, jIjdenotes the number of elements of I. For
a vectora= (a1;:::;an)byCov(a)we denote the matrix (cij), wherecii=Var(ai)and
cij=Cov(ai;aj).
Before we state the main results of this chapter we introduce the cone invertibility
factor (CIF), which plays an important role in the theoretical analysis of the properties of
LASSO estimators. In literature there are three related notions which are commonly used
in said analysis and help to provide some constraints on the optimized function so that
the estimator is “good” in certain sense. These notions are the cone invertibility factor,
compatibility factor and restricted eigenvalue (see Huang et al. (2013) and references
therein). For any  >1we deﬁne the cones C(;Sj;i) =n
:kSc
j;ik1kSj;ik1o
. Then
CIF is deﬁned as
Fj;i() = inf
06=2C(;Sj;i)kjk1
kk1; (4.3)
where jis the covariance matrix for a random vector (X1;:::;Xj 1)of variables from
the ﬁrstjlayers. More precisely,
j=1
m 
X0:(j 1)>X0:(j 1):
Our goal will be to show that the estimators ^i
jare close to the true vectors i
jin a certain
sense. However, if the curvature of the function in (4.2) around i
jis relatively small,
then the closeness between its values at ^i
jandi
jdoes not necessarily imply the closeness
between the arguments ^i
jandi
j. Hence, we require some additional conditions, for
instance, strong convexity of RSSj;iati
j, i.e. that the smallest eigenvalue of its Hessian
is positive. In the high-dimensional case it is too strong of an assumption, therefore
one usually considers restricted strong convexity or restricted smallest eigenvalues, where
“restricted” means that we take inﬁmum over C(;Sj;i)instead of the whole space. CIF
(4.3) is an example of such reasoning. We also introduce a non-random version Fj;i()of
CIF for each j2f1;:::;qgas follows. First we deﬁne
Hj=Cov(X1[0 : (j 1)]);
64whereX1[0 : (j 1)]denotes the restriction of X1to variables from the ﬁrst jlayers.
We assume that each Hjis positive deﬁnite and elements on the diagonal are equal to 1,
i.e.Hj
ii= 1, wherei2f1;2;:::;k 1++kj 1g. Then
Fj;i() = inf
06=2C(;Sj;i)kHjk1
kk1: (4.4)
Since in a Gaussian Bayesian Network the joint probability of all variables is assumed to
be Gaussian, then each marginal is Gaussian as well. Hence, for simplicity we can bound
the variance for each variable by the same constant 2. Also we denote
mj;i=jSj24(1 +)2log(jLj 1j2qkj=")
F2
j;i()(4.5)
for eachj2f1;:::;qgandi2f1;:::;kjg.
Theorem 4.1. Fix arbitrary "2(0;1)and >1. Assume that Fj;i()deﬁned in (4.4)is
positive for each j2f1;:::;qgandi2f1;:::;kjg. In addition suppose that
mK1max
j;imj;i (4.6)
and for each iandjwe have
j;iK2+ 1
 1i
js
log(jLj 1jqkj=")
mj;i
for some universal constants K1andK2. Then
P
k^ k14
+ 1max
j;ij;i
Fj;i
1 ":
The second main result is about thresholded version of LASSO estimator. It will
be proved after the proof of Theorem 4.1. Consider the Thresholded LASSO estima-
tor with the sets of nonzero coordinates ^Sj;i. The set ^Sj;icontains only those coeﬃ-
cients of the LASSO estimator (4.2), which are larger in the absolute value than some
pre-speciﬁed threshold j;ifor eachj2 f1;:::;qgandi2 f1;:::;kjg. We denote
f^S1;1;:::; ^S1;k1;^S2;1;:::; ^Sq;kqgas^S:
Corollary 4.2. Suppose that assumptions of Theorem 4.1 are satisﬁed. If for each j,i
and arbitrary  >1we havei
j;min=2>j;i4j;i
(+ 1)Fj;i;then Thresholded LASSO with
= [1;1;:::;q;kq]is consistent, i.e.
P
^S=S
1 ":
Before the proof of Theorem 4.1 we state and prove an auxiliary result Proposition 4.3,
which is interesting on its own. It describes a slightly more general case and it will be used
multiple times for diﬀerent numbers and sets of predictors and targets in order to prove
65Theorem 4.1. Moreover, to avoid any confusion with indices and notation introduced
before for convenience we use more general notation in subsequent proofs.
Hence, let (Y1;Z1);:::; (Ym;Zm)bei.i.d.randomvectorssuchthat Yi2RpandZi2R.
The coordinates of Yiwill be denoted by Yijfor eachj=f1;:::;pgand byYwe denote
the full (mp) matrix of predictors Y= (Y1;:::;Ym)>. Moreover, let H=Cov(Y1)is
a positive deﬁnite matrix with diagonal elements Hjj= 1forj= 1;:::;p. We assume
that
Zi=>Yi+"i; i= 1;:::;m; (4.7)
where"1;:::;"mare i.i.d. random variables with E"i= 0, which are subgaussian with
the parameter 2, and are independent of ppredictorsYi;:::;Yp. Subgaussianity means
that for each ianda2R
Eexp(a"i)exp(a22=2):
We also assume that predictors are subgaussian with the parameter 2;i.e.Eexp(aY1j)
exp(a22=2)for eachj= 1;:::;p:
The goal is to ﬁnd the set of indices of the relevant predictors
S=fj2f1;:::;pg:j6= 0g: (4.8)
The setScdenotes the complement of S, that is the set of zero coordinates of :Now
consider the LASSO estimator
^= argmin
2Rp[RSS () +kk1]; (4.9)
where
RSS () =1
2mmX
i=1 
Zi >Yi2:
For any >1we deﬁne the cone C(;S) =f:kSck1kSk1g. Then CIF is deﬁned as
F() = inf
06=2C(;S)kY>Y=mk1
kk1;
and its non-random version is given by
F() = inf
06=2C(;S)kHk1
kk1:
Proposition 4.3. Fix arbitrary a2(0;1)and >1. Suppose that F()is positive and
mK1jSj24(1 +)2log(p2=a)
F2()(4.10)
and
K2+ 1
 1r
log(p=a)
m; (4.11)
whereK1,K2are some universal constants. Then
P
k^ k14
(+ 1)F()
>1 2a: (4.12)
66The proof of Proposition 4.3 relies on Lemma 4.4 and 4.6 below.
Lemma 4.4. In the context of previously deﬁned random variables Yijand"i, where
i=f1;:::;mg, for arbitrary j= 1;:::;pandu>0we have
P 
1
nnX
i=1Yij"i>2 
2r
2u
n+u
n!!
exp( u):
The proof of Lemma 4.4 uses the following Corollary 8.2 of van de Geer (2016).
Lemma 4.5. Suppose that Z1;:::;Znare i.i.d. random variables and there exists L>0
such thatC2=Eexp (jZ1j=L)is ﬁnite. Then for arbitrary u>0
P 
1
nnX
i=1(Zi EZi)>2L 
Cr
2u
n+u
n!!
exp( u):
Proof of Lemma 4.4. Fixj= 1;:::;pandu>0. We consider an average of i.i.d. centered
random variables Zj=Yij"iwithEZj= 0, so we can use Lemma 4.5. We need to ﬁnd
L;C > 0such that Eexp (jY1j"1j=L)C2:Note that
Eexp (jY1j"1j=L)Eexp (Y1j"1=L) +Eexp ( Y1j"1=L): (4.13)
For the ﬁrst term on the right-hand side of (4.13) we have
Eexp (Y1j"1=L) =E[E(exp(Y1j"1=L)jY1j)]:
Using independence of Y1jand"1;and subgaussianity of "1for eachy2Rwe obtain
E[exp (Y1j"1=L)jY1j=y] =Eexp (y"1=L)exp 
y22=(2L2)
:
Therefore we have
E[E(exp(Y1j"1=L)jY1j)]Eexp 
Y2
1j2=(2L2)
;
which, using subgaussianity of Y1jand Lemma 7.4 of Baraniuk et al. (2011), we can bound
from above by
1p
(1 22=L2);
provided that L > : The second expectation on the right-hand side of (4.13) can be
bounded analogously, hence, we obtain
Eexp (jY1j"1j=L)2p
(1 22=L2);
provided that L>:We can take L= 2and obtain C2
4p
3;which ﬁnishes the proof.
Lemma4.6. Suppose that assumptions of Proposition 4.3 are satisﬁed. Then for arbitrary
"2(0;1)and >1with probability at least 1 "we have F()F()=2.
67Proof.Fix"2(0;1)and >1. We start with considering the l1-norm of the matrix
1
mY>Y EY>
1Y1
1= max
j;k=1;:::;p1
mmX
i=1YijYik EY1jY1k:
Fixj;k2f1;:::;pg. Notice that for any two numbers aandbwe have the inequality
aba2
2+b2
2:Hence, we can write
jY1jY1kjY2
1j
2+Y2
1k
2:
Therefore, ﬁrst using the previous inequality and Cauchy-Schwarz inequality afterwards
for any positive constant Lwe obtain
Eexp (jY1jY1kj=L)Eexp 
Y2
1j=(2L)
exp 
Y2
1k=(2L)
q
Eexp 
Y2
1j=L
Eexp (Y2
1k=L):(4.14)
The variable Y1jis subgaussian, so using Lemma 7.4 of Baraniuk et al. (2011) we can
bound the ﬁrst expectation under the square root in (4.14) from above by
1 22
L 1=2
,
provided that 22< L. The second expectation under the square root in (4.14) can be
bounded by the same value when we use the subgaussianity of Y1k. Therefore,
Eexp (jY1jY1kj=L)
1 22
L 1=2
;
provided that 22<L:Applying Lemma 4.5 with L= 32andC= 2andu= log(p2=")
we obtain
P 1
mmX
i=1YijYik EY1jY1k>K2r
log(p2=")
m!
"
p2;
whereKis an universal constant. Therefore,
P 1
mY>Y EY>
1Y1
1>K2r
log(p2=")
m!
=
=P 
max
j;k=1;:::;p1
mmX
i=1YijYik EY1jY1k>K2r
log(p2=")
m!

X
j;kP 1
mmX
i=1YijYik EY1jY1k>K2r
log(p2=")
m!
":(4.15)
Proceeding similarly to the proof of Lemma 4.1 of Huang et al. (2013) we ﬁrst obtain
1
mY>Y
1 kHk11
mY>Y H
11
mY>Y H
1kk1=
=1
mY>Y H
1(kSk1+kSck1)(1 +)jSjkk11
mY>Y EY>
1Y1
1:
68This implies that
1
mY>Y
1kHk1 (1 +)jSjkk11
mY>Y EY>
1Y1
1:
Thenbydividingbothsidesby kk1,takinginﬁmumwithrespectto overtheconeC(;S)
and using (4.15) we derive that
F()F() K(1 +)jSj2r
log(p2=")
m
with probability higher than 1 ". Finally, using (4.10) we have
F()F() KpK1F() =
1 KpK1
F():
We ﬁnish the proof by taking suﬃciently large K1.
Proof of Proposition 4.3. The central part of the proof is to show that
P
k^ k12
(+ 1) F()
>1 a: (4.16)
Let us denote 
 =fkrRSS ()k1 1
+1g:Now we want to bound from below the pro-
bability of 
. For eachj= 1;:::;pwe can calculate j-th partial derivative of RSS ()at
true
rjRSS () =@RSS
@j() =1
mmX
i=1Yiji
and we bound it from above with high probability using Lemma 4.4. Therefore, taking
(4.11) into account we have
P(
) = P
max
jjrjRSS ()j 1
+ 1
=P p\
j=1
jrjRSS ()j 1
+ 1!
=
= 1 P p[
j=1
jrjRSS ()j> 1
+ 1!
1 pX
j=1P
jrjRSS ()j> 1
+ 1

1 pX
j=1P 
jrjRSS ()j>K 2r
log(p=a)
m!
:
Now applying Lemma 4.4 with u= log(p=2a)and appropriately chosen K2we bound
from below this probability by 1 a.
In further argumentation we consider only the event 
:Besides, we denote ~=^ 
where ^is a minimizer of a convex function given in (4.9), which is equivalent to
8
>><
>>:@RSS
@j(^) = sgn(^j);if^j6= 0;
@RSS
@j(^); if^j= 0;(4.17)
69wherej= 1;:::;p. Next we show that ~2C(;S). Our argumentation is analogous to
Ye and Zhang (2010). From conditions in (4.17) and the fact that k~k1=k~Sk1+k~Sck1
we obtain
0~>Y>Y~=m =~>h
rRSS (^) rRSS ()i
=
=X
j2S~jrjRSS (^) +X
j2Sc^jrjRSS (^) ~>rRSS ()
X
j2Sj~jj X
j2Scj^jj+k~k1krRSS ()k1=
= [+krRSS ()k1]k~Sk1+ [krRSS ()k1 ]k~Sck1:
Since we exclusively consider the event 
, we obtain the following inequality
k~Sck1+krRSS ()k1
 krRSS ()k1k~Sk1k~Sk1:
Hence, we have just proved that ~belongs to the cone C(;S). Therefore from the deﬁ-
nition of F()we have
k^ k1kY>Y(^ )=mk1
F()krRSS (^)k1+krRSS ()k1
F():
Using the second condition in (4.17) and the deﬁnition of the event 
we then obtain
(4.16). Finally, having shown (4.16), we apply Lemma 4.6 and obtain (4.12) which ﬁnishes
the proof.
Proof of Theorem 4.1. In order to show that our estimator is close to the true parameter
vectorwe ﬁrst use union bounds. So here we have
P
k^ k14
(+ 1)max
j;ij;i
Fj;i
P \
j;i
k^i
j i
jk14j;i
(+ 1)Fj;i!
=
= 1 P [
j;i
k^i
j i
jk1>4j;i
(+ 1)Fj;i!

1 qX
j=1kjX
i=1P
k^i
j i
jk1>4j;i
(+ 1)Fj;i
:
Then, using Proposition 4.3 separately for each variable Xi
jin each layer `jforj= 1::::;q
with=j;i, with the number of predictors equal to p=jLj 1janda="
qkjwe obtain
that the expression above can be bounded from below by 1 ":The bound on the number
of observations mis chosen according to (4.5) and (4.6).
To prove Corollary 4.2 we apply the same methodology. Namely, we prove an auxi-
liary lemma concerning the model described by (4.7), so the set Sis deﬁned by (4.8).
Additionally, by minwe denote the smallest in the absolute value non-zero coordinate
of the true parameter vector . By ^Swe denote the set of non-zero coordinates of
the Thresholded LASSO estimator with the level , i.e. the coordinates of the vector ^,
which are greater than .
70Lemma 4.7. Fixa2(0;1)and >1. Then under the assumptions of Proposition 4.3
and4
(+ 1)F()min=2 (4.18)
we have
P
^S=S
1 a:
Proof.Take anyj =2S. Then from Proposition 4.3 and (4.18) with the probability greater
than 1 awe have
j^jj=j^j jjk ^ k1:
Therefore, the j-th coordinate of Thresholded LASSO ^TH
j= 0. Next, we take j2Sand
obtain, also from Proposition 4.3 and (4.18), that with probability greater than 1 a
j^jjjjj j^j jjmin k^ k1:
Hence, ^TH
j6= 0.
Proof of Corollary 4.2. FromLemma4.7foreach j2f1;:::;qgandi=f1;:::;kjgunder
the assumptions of Theorem 4.1 we have that for arbitrary aj;i2(0;1)
P
^Sj;i6=Sj;i
<aj;i:
Now we obtain
P
^S6=S
=P [
j;if^Sj;i6=Sj;ig!
qX
j=1kjX
i=1P
^Sj;i6=Sj;i
:
By takingaj;i="
qkjwe obtain the bound P(^S6=S)<"and ﬁnish the proof.
4.4 Discrete case
As we discussed in Section 3.1 in the discrete case as the distribution of the model we
take a collection of categorical distributions for each variable. First we assume a binary
case so that each Xi2f0;1gand we consider the logistic regression model. Let us denote
thesigmoid function as (x) =1
1 +e x. In this setting we can write probabilities for
each variable in each layer similar to (4.1) as follows
P(X1
1= 1) =(1
1;0+1;1
1;0X1
0++1;k0
1;0Xk0
0)
...
P(Xk1
1= 1) =(k1
1;0+k1;1
1;0X1
0++k1;k0
1;0Xk0
0)
P(X1
2= 1) =(1
2;0+1;1
2;0X1
0++1;k0
2;0Xk0
0+1;1
2;1X1
1++1;k1
2;1Xk1
1)
...
P(Xkq
q= 1) =(kq
q;0+X
kq;mj
q;jXmj
j):(4.19)
71Using the same notation as for the continuous case we need to solve the following dopti-
mization problems
^i
j= argmin
2Rk0++kj 1[`j;i() +j;ikk1]; j = 1;:::;q; i = 1;:::;kj;
where`j;iis the negative log-likelihood for the i-th variable in the j-th layer and has
the following form
`j;i() = mX
l=1X(p+i)llogh

i
j>X0:(j 1)i
+ (1 X(p+i)l) logh
1 
i
j>X0:(j 1)i
:
Here we denote by p=p(j) =k0++kj 1the number of variables in the pre-
viousj 1layers.
Wecanalsogeneralizetheabovecasetothecasewhereeachvariablehasadiscreteand
ﬁnite state space, namely each Xi
j2f1;:::;Ni
jg:Now instead of the sigmoid function we
use the so-called softmax function. For any vector a= (a1;:::;an)we deﬁne the softmax
function(a)as the vector (a) = ((a)[1];:::; (a)[n]), where(a)[i] =exp(ai)Pn
j=1exp(aj).
We denote as Xj= (X1
0;X2
0;:::;Xk0
0;X1
1;:::;Xkj 1
j 1)>forj= 1;:::;q. Also we denote
the vectors of parameters corresponding to the l-th class of the i-th variable in the j-th
layer as
i
j[l] = (i;1
j;0[l];:::;i;k0
j;0[l];i;1
j;1[l];:::;i;kj 1
j;j 1[l])
forj= 0;:::;q 1,i= 1;:::;kjandl= 1;:::;Ni
j. Then the model analogous to
the logistic model in (4.19) takes the form
P(X1
1= 1) =(1
1;0[1] +1
1[1]X1;:::;1
1;0[N1
1] +1
1[N1
1]X1)[1]
...
P(X1
1=N1
1) =(1
1;0[1] +1
1[1]X1;:::;1
1;0[N1
1] +1
1[N1
1]X1)[N1
1]
...
P(Xk1
1= 1) =(k1
1;0[1] +k1
1[1]X1;:::;k1
1;0[Nk1
1] +k1
1[Nk1
1]X1)[1]
...
P(Xi
j=l) =(i
j;0[1] +i
j[1]Xj;:::;i
j;0[Ni
j] +i
j[Ni
j]Xj)[l]
...
P(Xkq
q=Nkq
q) =(kq
q;0[1] +kq
q[1]Xq;:::;kq
q;0[Nkq
q] +kq
q[Nkq
q]Xq)[Nkq
q]:
This is called multinomial logistic regression . It is not diﬃcult to notice that logistic
regression is a particular case of multinomial logistic regression with two possible classes.
Foreachvariable Xi
jwedenotethefullvectorofparameters i
j= (i
j[1];:::;i
j[Ni
j]). Then
we need to solve doptimization problems analogous to the case of logistic regression
^i
j= argmin
2R(k0++kj 1)Ni
j[`j;i() +j;ikk1]; j = 1;:::;q; i = 1;:::;kj;
72where`j;iis also the negative log-likelihood for the i-th variable in the j-th layer and in
this case has the following form
`j;i() = mX
l=1Ni
jX
k=1I(X(p+i)l=k)2
4i
j[l]X0:(j 1) log0
@Ni
jX
l=1i
j[l]X0:(j 1)1
A3
5;
where we again denoted by p=p(j) =k0++kj 1the number of variables in the pre-
viousj 1layers.
4.5 Numerical results
In this section we describe the details of algorithm implementation as well as the results
of experimental studies comparing our algorithm to others.
4.5.1 Details of implementation
We provide in details practical implementation of the proposed algorithm. The solu-
tion of (4.2) depends on the choice of j;i. Finding the „optimal” parameters j;iand
the thresholds j;iin practice is diﬃcult. We solve it using the information criteria (Xue
et al., 2012; Pokarowski and Mielniczuk, 2015; Miasojedow and Rejchel, 2018).
First, recall the function which is being minimized in (4.2)
RSSj;i() +j;ikk1=1
2Xj[i] >X0:(j 1)2
2+j 1X
l=0klX
ml=1jml
lj;
withXj[i]being the vector of the length mof observations for the i-th variable in the j-th
layer. We perform the optimization separately for each variable and the vector is from
Rk0+:::+kj 1forj= 1;:::;qandi= 1;:::;kj. In our implementation we use the following
scheme. We start with computing a sequence of minimizers on the grid, i.e. for any j
andiwe create a ﬁnite sequence fkgN
k=1uniformly spaced on the log scale, starting from
thelargestk, whichcorresponds tothe empty model. Next, foreachvalue kwecompute
the estimator ^i
j[k]of the vector i
j
^i
j[k] = argmin
2Rk0+:::;kj 1fRSSj;i() +kkk1g: (4.20)
To solve (4.20) numerically for a given kwe use the FISTA algorithm with backtracking
from Beck and Teboulle (2009). The ﬁnal LASSO estimator ^i
j:=^i
j[k]is chosen using
the Bayesian Information Criterion (BIC), which is a popular method of choosing j;iin
the literature (Xue et al., 2012; Miasojedow and Rejchel, 2018), i.e.
k= argmin
1kNn
mlog(RSS (^i
j[k])) + log(m)k^i
j[k]k0o
:
Herek^i
j[k]k0denotes the number of non-zero elements of ^i
j[k]andmis the number of
observations of the network. In our simulations we use N= 100.
73Finally, the threshold is obtained using the Generalized Information Criterion (GIC).
A similar way of choosing a threshold was used previously in Pokarowski and Mielniczuk
(2015); Miasojedow and Rejchel (2018). For a prespeciﬁed sequence of thresholds Dwe
calculate

j;i= argmin
2Dn
mlog(RSS (^i
j;)) + log(k0++kj 1)k^i
j;k0o
;
where ^i
j;is the LASSO estimator ^i
jafter thresholding with the level :
4.5.2 Experiments
In this subsection we compare our algorithm to other algorithms developed for this prob-
lem applying them to benchmark networks. We use the bnlearn package in R(Scutari
(2010)), in which many algorithms for learning Bayesian networks including structure
learning are implemented. Algorithms of diﬀerent types discussed in the beginning of
this chapter such as constraint-based algorithms, score-and-search algorithms and hybrid
algorithms can be found there. The choice of speciﬁc algorithms was made empirically,
i.e. we selected the best performing ones on the chosen networks. We took the networks
with continuous data of a medium, large and very large amount of nodes and arcs. We
refer to medium, large and very large sizes as 20-50 nodes, 50-100 nodes or 100-1000
nodes, respectively, adopting this classiﬁcation from the authors of bnlearn package.
We chose a medium-size network ECOLI 70with 46 nodes and 70 arcs (Schäfer and
Strimmer (2005)), a large network MAGIC -IRRI* with 64 nodes and 102 arcs and
a very large network ARTH 150with 107 nodes and 150 arcs (Opgen-Rhein and Strim-
mer (2007)). The algorithms chosen for comparison are hill-climbing ( hc) algorithm,
tabu search ( tabu), max-min hill-climbing ( mmhc) and Hybrid HPC ( h2pc) algorithm.
Hill-climbing (Scutari et al. (2018)) is a greedy search algorithm that explores the space
of the directed acyclic graphs (DAGs) by an addition, removal or reversal of a single
edge and uses random restarts to avoid local optima. Tabu search (Russell and Norvig
(2010)) is a modiﬁed hill-climbing method, which is able to escape local optima by se-
lecting a network that minimally decreases the score function. Both methods above use
search-and-score approach. Max-min hill-climbing algorithm (Tsamardinos et al. (2006))
is a hybrid method combining a constraint-based algorithm called max-min parents and
children and hill-climbing. H2PC (Hybrid HPC, Gasse et al. (2014)) algorithm is a hybrid
algorithm combining an ensemble of weak PC learners (Spirtes and Glymour (1991)) and
hill-climbing. For more diﬀerent comparisons of methods in bnlearn package see Scutari
et al. (2018).
*Themodel MAGIC -IRRIwasdevelopedasanexampleofmultipletraitmodellingin
plant genetics for the invited talk “Bayesian Networks, MAGIC Populations and Multiple
Trait Prediction” delivered by Marco Scutari, the author of bnlearn package, at the 5th
International Conference on Quantitative Genetics (ICQG 2016).
74For each network we used two sizes of the data set with m= 300andm= 1000
observations. In the tables with results we denoted them with 2-3 ﬁrst letters of the name
of the network followed by the number of observations so it does not create any confusion.
For each algorithm we ran the experiment for 100 times, each time with a new set of mob-
servations, and averaged the results in terms of three performance measures:
•power,i.e. the proportion of correctly discovered edges.
•false discovery rate (FDR), i.e. the fraction of incorrectly selected edges among
all selected edges.
•structural Hamming distance (SHD), i.e. the smallest number of operations
(such as adding or removing the edge and changing the direction of the arrow)
required to match the true DAG and a learned one.
In Tables 4.1-4.3 we provide the results of experiments for mentioned above data sets and
methods including ours. In terms of power our algorithm performs right in the middle
of score-and-search and hybrid methods for ECOLI 70data set, similarly to the hybrid
methods in case of small MAGIC -IRRIdata sets. We note that for data sets of 1000
observations it performs worse than other methods, however, with the number of obser-
vations growing the algorithm’s power grows as well. With the number of observations of
10000 it grows up to 0.5 performing as good as other score methods without any increase
in FDR. The same situation we observe with ARTH 150data set.
In terms of FDR our algorithm performs the best consistently giving very low numbers
for false discoveries. This is especially important when the cost of a false discovery is high
and makes obtained discoveries more certain. With the numbers of observations of 10000
we constantly get numbers in the range 0.2-0.4% for all data sets. When it comes to
structural Hamming distance (SHD) our algorithm performs the best or close to the best
numbers as well. With the growing number of observations it decreases due to increasing
power and consistently low FDR. For the number of observations of 10000 it outperforms
other algorithms or has close SHDs to hybrid methods and reaches around 28, 62 and 86
forECOLI 70,MAGIC -IRRIandARTH 150data sets, respectively.
We also checked our method on a discrete binary network called ASIA, introduced
in Chapter 3. It is a small network of 8 nodes and 8 edges. Our algorithm recognizes 6
arrows and makes 2 false discoveries, discovering 8 arrows in total. However, after a closer
look we noticed that it could not recognize 2 arrows due to incorrect assignment of layers.
Finally, one false discovery was an arrow of an opposite direction to the true one, and
the other one was an arrow from the start to the end of a causal trail (we obtained
an additional arrow X!Wfor the trail X!Y!Z!W), hence still recovering
dependencies in both cases.
75Method EC300EC1000MAG 300MAG 1000AR300AR1000
hc 0.57 0.65 0.28 0.45 0.58 0.67
tabu 0.6 0.7 0.31 0.5 0.59 0.68
mmhc 0.39 0.45 0.25 0.46 0.48 0.58
h2pc 0.4 0.49 0.23 0.45 0.5 0.61
MCMC + LASSO 0.49 0.55 0.24 0.38 0.38 0.46
Table 4.1: Average power for ECOLI 70,MAGIC -IRRIandARTH 150networks for
300 and 1000 observations.
Method EC300EC1000MAG 300MAG 1000AR300AR1000
hc 0.049 0.044 0.04 0.037 0.028 0.19
tabu 0.047 0.036 0.04 0.036 0.028 0.018
mmhc 0.021 0.024 0.022 0.022 0.01 0.008
h2pc 0.02 0.023 0.14 0.019 0.006 0.006
MCMC + LASSO 0.004 0.004 0.004 0.006 0.004 0.003
Table 4.2: Average FDR for ECOLI 70,MAGIC -IRRIandARTH 150networks for 300
and 1000 observations.
Method EC300EC1000MAG 300MAG 1000AR300AR1000
hc 65.6 51.8 129.5 96.5 214.2 151.1
tabu 64.8 46.9 127.1 93.1 215 150.9
mmhc 48.1 39.2 101.7 72.2 127.3 104.1
h2pc 45.9 37.9 91.6 67.21 103.7 90.3
MCMC + LASSO 39.2 35.1 85.9 79.3103 98.7
Table 4.3: Average SHD for ECOLI 70,MAGIC -IRRIandARTH 150networks for 300
and 1000 observations.
76Chapter 5
Structure learning for CTBNs for
complete data
In this chapter we consider continuous time Bayesian networks (CTBNs) introduced and
deﬁned in Section 2.5. First we consider the fully observed case where we observe the be-
haviour of the network at each moment of time.
5.1 Notation and preliminaries
In this section we describe the proposed method, using the notation introduced in Sec-
tion 2.5. First, we consider the full graph G= (V;E), namely we assume that paG(w) =
pa(w) = wfor eachw2V. Then we remove unnecessary edges using the penalized
likelihood technique. We start by introducing the new parametrization of the model. For
simplicity, in the main part of this chapter we consider the binary graph, i.e. Xw=f0;1g
for eachw2V:The extension of our results to more general case is described in Sec-
tion 5.5.
Letdbe the number of nodes in the graph. Consider a ﬁxed order (w1;w2;:::;wd)of
nodes of the graph. Using this order we deﬁne a (2d)d-dimensional matrix
= 
w1
0;1;w1
1;0;w2
0;1;w2
1;0;:::;wd
0;1;wd
1;0>; (5.1)
whose rows are vectors w
s;s02Rdfor allw2Vands;s02f0;1gsuch thats6=s0:
Obviously, the matrix can be easily transformed to 2d2-dimensional vector in a standard
way. In this chapter we assume that for all w2V,c2X w,s;s02f0;1g,s6=s0
the conditional intensity matrices satisfy
log(Qw(c;s;s0)) =w
s;s0>Zw(c); (5.2)
whereZw:X w! f 0;1gdis a binary deterministic function described below. With
the slight abuse of notation, by Zwe will denote the set of all functions Zw1;:::;Zwd.
In (5.2) the conditional intensity matrix Qw(;s;s0)is modeled in the analogous way
to the regression function in generalized linear models (GLM) and the functions Zw()
77play the role of explanatory variables (covariates). In our setting the link function is
logarithmic. The analogous approach can be found in Andersen and Gill (1982); Huang
et al. (2013), where the Cox model is considered. The relation between the intensity and
covariates in those papers is similar to (5.2). Since the considered CTBNs do not contain
explanatory variables, we introduce them artiﬁcially as any possible representations of
parents’ states. Thus, for every w2Vthese explanatory variables are dummy variables
encoding all possible conﬁgurations in pa(w) = w:To make it more transparent we
consider the following example.
Example 5.1.We consider CTBN with three nodes A;BandC:For the node Awe deﬁne
the function ZAas
ZA(b;c) = [1;I(b= 1);I(c= 1)]>
for eachb;c2f0;1g, where I()is the indicator function. Therefore, for each conﬁgu-
ration of parents’ states (i.e. values in the nodes BandC) the value of the function
ZA(;)is a three-dimensional binary vector, whose coordinates correspond to the inter-
cept, the value in the parent Band the value in the parent C;respectively. Analogously,
we deﬁne representations for remaining nodes: ZB(a;c) = [1;I(a= 1);I(c= 1)]>and
ZC(a;b) = [1;I(a= 1);I(b= 1)]>for eacha;b;c2f0;1g:In this example the parameter
vector(5.1)isdeﬁnedas = 
A
0;1;A
1;0;B
0;1;B
1;0;C
0;1;C
1;0>. Withslightabuseofnotation,
the vectorA
0;1is given asA
0;1=
A
0;1(1);A
0;1(B);A
0;1(C)>and we interpret (5.2) as fol-
lows:A
0;1(B) = 0meansthattheintensityofthechangefromthestate 0to1atthenode A
does not depend on the state at the node B:Similarly,A
0;1(C)describes the dependence
between the above intensity and the state at the node C;andA
0;1(1)corresponds to
theintercept. Forthenode Bthecoordinatesofthevector B
0;1=
B
0;1(1);B
0;1(A);B
0;1(C)
describe the relation between the intensity of the jump from the state 0to1at the node B
to the intercept, states at nodes AandC;respectively.
Now what if Z=fZA;ZB;ZCgwas deﬁned diﬀerently? The new function ZAcan be
deﬁned in 3 more diﬀerent ways, for example ZA(b;c) = [1;I(b= 0);I(c= 1)]>. The same
applies to the functions ZBandZC. Having deﬁned Z=fZA;ZC;ZCgwe obtain the new
vector of the parameters . Then for instance we have A
0;1=h
A
0;1(1);A
0;1(B);A
0;1(C)i
and so on. Note that both sets ZandZfully describe the state conﬁguration of the net-
work and both A
0;1andA
0;1correspond to the same dependencies as above. In par-
ticular, it is easy to check that for instance A
0;1(B) =A
1;0(B) = 0if and only if
A
0;1(B) =A
1;0(B) = 0.
Analogously as in Example 5.1, for w2V,u6=w, ands;s02f0;1g,s6=s0we
deﬁne the coordinate of the function Zwcorresponding to the node uas an indicator of
its state equal to either 0 or 1. Moreover, we denote the coordinate of w
s;s0corresponding
to the node ubyw
s;s0(u). We interpret w
s;s0(u)as the parameter describing dependence of
the intensity of the jump from the state stos0at the node won the state at the node u.
Our goal is to ﬁnd edges in a directed graph (V;E):We deﬁne the relation between
78parameters and edges in (V;E)in the following way
w
0;1(u)6= 0 orw
1;0(u)6= 0,the edgeu!wexists; (5.3)
which makes parameters compatible with the considered CTBNs. Roughly speaking,
the nodeuis a parent of wmeans that the intensity of switching a state at wdepends
on the state at u. Therefore, the problem of ﬁnding edges in the graph is reformulated as
the problem of the estimation of the parameter :
Remark 5.2.As we mentioned previously in the example, the set Zfully describes the pa-
rents state conﬁguration and the relation above does not depend on the choice of Z. More
precisely, assume we have two diﬀerent properly deﬁned ZandZand the corresponding
vectors of parameters
=
w1>
0;1;w1>
1;0;w2>
0;1;w2>
1;0;:::;wd>
0;1;wd>
1;0>
;
=
w1>
0;1;w1>
1;0;w2>
0;1;w2>
1;0;:::;wd>
0;1;wd>
1;0>
:
Then the following is true
w
0;1(u) = 0^w
1;0(u) = 0,w
0;1(u) = 0^w
1;0(u) = 0:
This means that no matter how we deﬁne our explanatory functions Zw, we will get
the same arrows in the underlying CTBN.
Remark 5.3.For simplicity, in the rest of the thesis, we omit the ﬁrst coordinate w
s;s0(1)in
the vectorw
s;s0for allw;s6=s0;because it corresponds to the intercept and is not involved
in the recognition of the edges in the graph. The ﬁrst coordinates of representations Zw(c)
are discarded as well.
Remark 5.4.The Markov equivalence/identiﬁability/non-uniqueness problem is challen-
ging for directed graphical models. However, this problem does not appear here for
CTBNs. It is a consequence of our Assumption (5.2), which states that we restrict to
models having a conditional intensity in the GLM form. Moreover, under this assump-
tionis uniquely determined. Moreover, this uniquely deﬁned determines uniquely
the structure of a graph by (5.3). In fact, our main result (Theorem 5.5 below) shows
consistency of the estimator of , which is a much stronger property than identiﬁability.
Finally, in Assumption (5.2) we require that a conditional intensity of a variable is a linear
function of the states of its parents. This condition can be easily extended to a polynomial
dependence, so it can cover quite general dependence structure.
Our method is based on estimating the parameter using the penalized likelihood
method. In the rest of the thesis the term is reserved for the true value of the parameter.
Other quantities are denoted by :First, we consider a function
`() =1
TX
w2VX
c2X wX
s6=s0
 nw(c;s;s0)w
s;s0>Zw(c) +tw(c;s) exp 
w
s;s0>Zw(c)
;(5.4)
79where the third sum in (5.4) is over all s;s02Xwsuch thats6=s0:Recall that nw(c;s;s0)
andtw(c;s)were introduced in Section 2.5 to denote the number of jumps from a state s
tos0and the total time in the state sfor the node w, respectively, while the parents
conﬁguration equals to c. Notice that the function (5.4) is the negative log-likelihood .
Indeed, we just apply the negative logarithm to the density (2.8) combined with (2.9) and
(5.2), where pa(w) = wfor eachw2V:Then we divide it by Tand omit the term
corresponding to the initial distribution ;becausedoes not depend on :We deﬁne
an estimator of as
^= argmin
2R2d(d 1)f`() +kk1g; (5.5)
wherekk1=P
w2VP
s6=s0P
u2 wjw
s;s0(u)jis thel1-norm of:The tuning parameter  > 0
characterizes a balance between minimizing the negative log-likelihood and the penalty
function. As we have mentioned, the form of the penalty is crucial, because its singularity
at the origin implies that some coordinates of the minimizer ^are exactly equal to 0,
ifis suﬃciently large. Thus, starting from the full graph we remove irrelevant edges and
estimate parameters for existing ones simultaneously. The function `()and the penalty
are convex functions, so (5.5) is a convex minimization problem, which is an important
fact from both practical and theoretical point of views.
Atﬁrstglance,computing(5.5)seemstobecomputationallycomplex,becausethenum-
ber of summands in (5.4) is d2d:However, the number of nonzero terms of the form
nw(c;s;s0)andtw(c;s)is bounded by the total number of jumps, which grows linearly
with timeT. Hence, most of summands in (5.4) are also zeroes and the minimizer (5.5)
can be calculated eﬃciently.
Before we state and prove main results of this chapter we introduce some additional
notation. First, for each w2Vwe denote its parents indicated by the true parameter as
Sw=
u2 w:w
0;1(u)6= 0orw
1;0(u)6= 0	
:
BySwe denote the support of ;i.e. the set of nonzero coordinates of :Moreover,minis
the smallest in the absolute value element of restricted to S. The set Scdenotes
the complement of S, that is the set of zero coordinates of :Besides, for each w2Vwe
deﬁne Sw=VnfSw[wgand denote  = max
s;s02X;s6=s0Q(s;s0):
Recall that for a vector awe denote its l1-norm bykak1= maxkjakj:For a subset I
the vector aIdenotes a vector such that (aI)i=aifori2Iand(aI)i= 0otherwise.
Moreover,jIjdenotes the number of elements of I:
Letbe the stationary distribution of the Markov jump process (MJP), which is
deﬁned by an intensity matrix Q:The initial distribution of this process is denoted by 
andwedeﬁnekk2
2=P
s2X2(s)=(s):Moreover,1denotesthesmallestpositiveeigenvalue
of the matrix 1=2(Q+Q), whereQis an adjoint matrix of Q:
805.2 Main results
In this subsection, we state two key results on the structure learning for CTBNs for
complete data. In the ﬁrst one (Theorem 5.5) we show that the estimation error of the mi-
nimizer ^given by (5.5) can be controlled with probability close to 1. In the second main
result (Corollary 5.6) we state that the thresholded version of (5.5) is able to recognize
the structure of the graph with high probability.
First, we introduce the cone invertibility factor (CIF), which plays an important role
in the theoretical analysis of the properties of LASSO estimators. Our goal is to show that
theestimator ^isclosetothetruevector . ToaccomplishthisgoalweshowinLemma5.9
that the gradient of the likelihood (5.4) evaluated at is close to 0. However, this is not
suﬃcient since the likelihood function cannot be too “ﬂat”. Namely, its curvature around
the local optimum needs to be relatively high, because we want to avoid the situation
when the loss diﬀerence can be small whereas the error is large. In the high-dimensional
scenariothisisoftenprovidedbyimposingtherestrictedstrongconvexitycondition(RSC)
on (5.4), as in Negahban et al. (2009). CIF deﬁned below in (5.6) plays a similar role
to RSC, but gives sharper consistency results (Ye and Zhang, 2010). Therefore, it is
used here. CIF is deﬁned analogously to Ye and Zhang (2010); Huang and Zhang (2012);
Huang et al. (2013) and is closely related to the compatibility factor (van de Geer, 2008)
or the restricted eigenvalue condition (Bickel et al., 2009). Recall that in the previous
chapter we also used a version of CIF for Bayesian networks. Thus, for any  > 1we
deﬁne the coneC(;S) =f:kSck1kSk1g, where the set Sdenotes the support of 
as mentioned above. Then CIF is deﬁned as
F() = inf
06=2C(;S)>r2`()
kSk1kk1: (5.6)
Notice that only the value of the Hessian r2`()at the true parameter is taken into
consideration in (5.6). The main diﬃculty with CIF in our case is that it is a minimum of
the sum of random terms, which number grows exponentially in d. To be able to control
this quantity, we bound CIF from below by its deterministic counterpart with much fewer
summands. Namely, in Lemma 5.11 we prove that F()is bounded from below by the
product of 0given in Theorem 5.5 and
F() = inf
06=2C(;S)X
w2VX
s06=sX
cSw2XSwexp 
w>
s;s0Zw(cSw;0)
w>
s;s0Zw(cSw;0)2
kSk1kk1(5.7)
with probability close to 1. Here we divided each parent conﬁguration c= (cSw;c Sw)
into two parts: the ﬁrst one corresponds to the true parents nodes and the second part
correspondstotheremainingnodes. Belowwewillalsouseasimilarnotationforanystate
of the network s2Xdeﬁning it as a triple s= (cSw;c Sw;s)of the state of true parents
of the node w, the conﬁguration for the nodes from S wand the state in the node w.
Note, that we restricted the summation in (5.7) only to cSw2XSwby takingc Sw= 0.
This allows us to derive the lower bound on F()without considering exponentially many
81random summands. Our argumentation will be also valid in the case, when we choose
some nonzero values in c Sw, unless this values depend on wandcSw:Next, we state two
main results of this chapter.
Theorem 5.5. Fix arbitrary "2(0;1)and >1. Suppose that
T >36
max
w2VjSwj+ 1
log 2 + log ( dkk2=")
1min
w2V;s2Xw
cSw2XSw2(cSw;0;s): (5.8)
We also assume that T2and we choose such that
2+ 1
 1log(K=")r

T20F()
e(+ 1)jSj; (5.9)
whereK= 2(2 +e2)d(d 1)and
0= min
w2V;s2Xw
cSw2XSw(cSw;0;s)=2:
Then with probability at least 1 2"we have
k^ k12e
(+ 1)0F(): (5.10)
NowconsidertheThresholdedLASSOestimatorwiththesetofnonzerocoordinates ^S:
The set ^Scontains only those coeﬃcients of the LASSO estimator (5.5), which are larger
in the absolute value than a pre-speciﬁed threshold :
Corollary 5.6. Suppose that assumptions of Theorem 5.5 are satisﬁed and let Rdenote
the right-hand side of the inequality (5.10). IfR <  min=2;then for2[R; min=2)we
haveP
^S=S
1 2":
These two results will be proven in the next section and here we give some comments
on their meaning and signiﬁcance. The above two results describe the properties of
the proposed estimator (5.5) in recognizing the structure of the graph. Theorem 5.5 gives
conditions under which the estimation error of (5.5) can be controlled. Namely, let us
for a moment ignore constants, and parameters of MJP such as ;; 1;0, etc. in
the assumptions. By condition (5.9), if
Tlog2(d=")jSj2
F2(); (5.11)
then the estimation error is small. This forms some restrictions on the number of vertices
in the graph, sparsity of the graph (i.e. the number of edges has to be small enough) and
the expression (5.7), which is discussed in Lemma 5.7 (below). The condition (5.11) is
similar to standard results for LASSO estimators in Ye and Zhang (2010); Bühlmann and
van de Geer (2011); Huang and Zhang (2012); Huang et al. (2013). The only diﬀerence is
82that the right-hand side of (5.11) usually depends linearly on log(d="), but here we have
log2(d="):The square in the logarithm could be omitted, if we imposed some additional
restrictions on observation time Tin the crucial auxiliary result (Lemma 5.9), where we
use the Bernstein-type inequality for the Poisson random variable. Obviously, it would
reduce the applicability of the main result. In our opinion, the gain (having log(d=")
instead of log2(d=")) is “smaller” than the price (additional assumptions), so we do not
focus on it.
The next assumption in Theorem 5.5 that T2is quite natural since observation
time has to increase when the maximal intensity of transitions decreases. Moreover,
the conditions (5.8) and (5.9) depend also on parameters of MJP. More precisely, they
depend on the stationary distribution and the spectral gap 1;which in general decrease
exponentially with d:However, in some speciﬁc cases, it can be proved that they decrease
polynomially.
Corollary 5.6 states that the LASSO estimator after thresholding is able to recognize
the structure of a graph with probability close to 1, if the nonzero coeﬃcients of are
not too close to zero and the threshold is appropriately chosen. However, Corollary 5.6
does not give a way of choosing the threshold , because both endpoints of the inter-
val[R; min=2]are unknown. It is not a surprising fact and has been already observed,
for instance, in linear models (Ye and Zhang, 2010, Theorem 8). In the experimental
subsection of this chapter we propose a method of choosing a threshold that relies on
information criteria. A similar procedure can be found in Pokarowski and Mielniczuk
(2015); Miasojedow and Rejchel (2018).
Now we state a lower bound for (5.7) which has an intuitive interpretation.
Lemma 5.7. DeﬁneA=P
w2VP
s06=sP
u:w
s;s0(u)6=0exp 
 w
s;s0(u)
:Then for every  > 1we
haveF()(A) 1:
Noticethattheterm Aislarger,andinturn F()issmaller,whennegativecoeﬃcients
of„dominate” intheabsolutevaluethepositiveones. Notethat, themorethesenegative
coeﬃcients dominate the more our process „gets stuck”, i.e. tends to stay in the same state
because intensities in this case tend to be close to zero (recall (5.2)). Such behaviour in
thecontextofMJPsisnatural, becausemultiplyingtheintensitymatrix Qbyaconstant 
is equivalent to considering time T=instead ofT. WhileF()appears in the lower
bound (5.11) on T, such dependence on is expected.
5.3 Proofs of the main results
This subsection contains the proofs of all the statements made in the previous subsec-
tion. The proofs of the theorem and the corollary are based on a number of auxiliary
results. Some of these results are well-known facts for LASSO estimators and some of
them are new (Lemmas 5.9 and 5.11). The main novelty and diﬃculty of the considered
model is the continuous time nature of the observed phenomena which we investigate.
83In Lemma 5.9 we derive the new concentration inequality for MJPs based on the mar-
tingale theory. In Lemma 5.11 we give new upper bounds on the occupation time for
MJPs.
In the proofs of subsequent results we use the ﬁrst and second derivatives of `given
by (5.4), which can be also expressed in the following form
`() =1
TX
w2VX
s6=s0`w
s;s0(w
s;s0); (5.12)
where
`w
s;s0(w
s;s0) =X
c2X w
 nw(c;s;s0)w
s;s0>Zw(c) +tw(c;s) exp(w
s;s0>Zw(c))
:
Therefore, we can calculate partial derivatives
r`w
s;s0(w
s;s0) =X
c2X w[ nw(c;s;s0) +tw(c;s) exp(w
s;s0>Zw(c))]Zw(c):(5.13)
By Remark 5.3 the matrix of all parameters has 2drows and (d 1)columns. It can
be also considered as a 2d(d 1)-dimensional vector
=
w1>
0;1;w1>
1;0;w2>
0;1;w2>
1;0;:::;wd>
0;1;wd>
1;0>
;
where (w1;w2;:::;wd)is a ﬁxed order of the nodes of the graph. Using this order we
obtain the following representation of the gradient of `.
r`() =1
T
r`w
s;s0(w
s;s0)
w2V;s6=s0: (5.14)
Analogously we calculate second derivatives
r2`w
s;s0(w
s;s0) =X
c2X wtw(c;s) exp(w
s;s0>Zw(c))Zw(c)Zw(c)>:
The second derivative of `()consists of matrices1
Tr2`w
s;s0(w
s;s0)along its diagonal and
zeroes elsewhere. Moreover, for any vector 2R2d(d 1)and the true parameter vector 
we have
>r2`()=1
TX
w2VX
c2X wX
s06=stw(c;s) 
w
s;s0>Zw(c)2exp(w
s;s0>Zw(c)):(5.15)
Next we provide an auxiliary proposition needed to prove an important concentration
inequality for MJPs in Lemma 5.9.
Proposition 5.8. LetX()be a Markov jump process with a bounded intensity matrix Q.
Let
n
s;s0=X
c2X w:Zw(c)[k]=1n
w(c;s;s0)
84be a number of jumps from stos0on the interval [0;]andt
sbe an occupation time at
stateson the interval [0;]. Then
M() =n
s;s0 t
sQ(s;s0)
is a martingale with respect to the natural ﬁltration F. The notation M()means that
the distribution at time 0is.
Proof.For anyu<we have
E(M()jFu) =M(u) +E(M() M(u)jFu) =
=M(u) +E(M() M(u)jX(u))
=M(u) +E(MX(u)( u)jX(u));
where the last equality is the consequence of Proposition 20.3 from Bass (2011). Now it
is enough to show that for all  >0and all initial measures we have EM() = 0, since
it implies that E(E(M()jFu)) = 0foru<.
For anyn2Ndeﬁne the sequence ki=ki(n) =i
nfor alli= 0;:::;n. Since
the trajectory of the process is càdlàg, we have
EM() =Elim
n!1nX
i=1h
I(X(ki 1) =s;X(ki) =s0) 
nQ(s;s0)I(X(ki 1) =s)i
:
We observe that for all n2N
nX
i=1h
I(X(ki 1) =s;X(ki) =s0) 
nQ(s;s0)I(X(ki 1) =s)iN() +;(5.16)
whereN()is the total number of jumps. Since N()is a Poisson process with a bounded
intensity, the right-hand side of (5.16) is integrable and by the dominated convergence
theorem and the deﬁnition of Qwe get
EM() = lim
n!1EnX
i=1h
I(X(ki 1) =s;X(ki) =s0) 
nQ(s;s0)I(X(ki 1) =s)i
= lim
n!1EnX
i=1h
E(I(X(ki 1) =s;X(ki) =s0)jX(ki 1)) 
nQ(s;s0)I(X(ki 1) =s)i
:
Next fors6=s0we have
P(X(ki 1) =s;X(ki) =s0jX(ki 1) =s) =
=P(X(ki) =s0jX(ki 1) =s) =Q(s;s0)
n+o(1=n)
and for6=s
P(X(ki 1) =s;X(ki) =s0jX(ki 1) =) = 0:
Hence,
E[I(X(ki 1) =s;X(ki) =s0)jX(ki 1)] =
nQ(s;s0) +o(1=n)
I(X(ki 1) =s):
85Therefore, we further obtain
EM() = lim
n!1EnX
i=1h
nQ(s;s0) +o(1=n)
I(X(ki 1) =s) 
nQ(s;s0)I(X(ki 1) =s)i
= lim
n!1EnX
i=1o(1=n)I(X(ki 1) =s) = 0;
whereo(1=n)does not depend on i.
Lemma 5.9. Let"2(0;1)and >1be arbitrary. Assume that T2and
2+ 1
 1log(K=")r

T;
whereK= 2(2 +e2)d(d 1). Then we have
P
kr`()k1 1
+ 1
1 ":
Proof.Note that by (5.2), (5.13) and (5.14) we have the following inequality
kr`()k11
Tmax
w2V;s6=s0;1kd 1X
c2X w:Zw(c)[k]=1[nw(c;s;s0) tw(c;s)Qw(c;s;s0)];
whereZw(c)[k]is thek-th coordinate of Zw(c)for eachw2V;c2X w:The core step of
the proof is to show that for ﬁxed w2V; s6=s0;1kd 1and= 2 log (K=")
P0
@X
c2X w:Zw(c)[k]=1[nw(c;s;s0) tw(c;s)Qw(c;s;s0)]>p
T1
A(2 +e2) exp
 
2
:
(5.17)
Having (5.17) we ﬁnish the proof of Lemma 5.9 using union bounds. More precisely,
P
kr`()k1> 1
+ 1

P0
@1
Tmax
w2V;s6=s0;1kd 1X
c2X w:Zw(c)[k]=1[nw(c;s;s0) tw(c;s)Qw(c;s;s0)]>r

T1
A
2d(d 1)P0
@X
c2X w:Zw(c)[k]=1[nw(c;s;s0) tw(c;s)Qw(c;s;s0)]>p
T1
A
2d(d 1)(2 +e2) exp( log(K=")) =":
Therefore, we focus on proving (5.17). The proof of this inequality is based on the mar-
tingale arguments, so we make the dependence on the time explicit in (5.17), that is
nw(c;s;s0)andtw(c;s)becomenT
w(c;s;s0)andtT
w(c;s);respectively.
For2[0;T]we deﬁne a process
M() =X
c2X w:Zw(c)[k]=1[n
w(c;s;s0) t
w(c;s)Qw(c;s;s0)]: (5.18)
86We use the upper index inn
w(c;s;s0)andt
w(c;s)to indicate that these quantities
correspond to the time interval [0;]:Using Proposition 5.8 to each summand in (5.18)
we obtain that the process fM() :2[0;T]gis a martingale. Let us deﬁne its jumps by
M() =M() M( ) =X
c2X w:Zw(c)[k]=1I[X( ) = (s;c);X() = (s0;c)];
whereM( )is the left limit at . By Theorem II.37 of Protter (2005) and Theorem
I.4.61 of Jacod and Shiryaev (2003) for any x> 1the process
Ex() = exp ( xM())Y
u(1 +xM(u)) exp( xM(u))
= exp
xM() (x log(1 +x))n
s;s0	
is a local martingale, where n
s;s0=P
c2X w:Zw(c)[k]=1n
w(c;s;s0)is computed for a trajec-
tory at the time interval [0;]. Therefore, by Markov inequality together with the triangle
inequality we get for any x2(0;1]
P(jM(T)j>L)P(jxM(T) (x log(1 +x))nT
s;s0j>xL= 2) +
+P((x log(1 +x))nT
s;s0>xL= 2)
2 exp xL
2
+P((x log(1 +x))nT
s;s0>xL= 2):
We observe that nT
s;s0is bounded from above by the total number of jumps up to time T,
which in turn is bounded by a Poisson random variable N(T)with the intensity T.
Hence, again by Markov inequality we have
P((x log(1 +x))nT
s;s0>xL= 2)exp xL
2+Tex
1 +x 1
:
Applying an inequality ex1=(1 x)forx<1and setting x= 1=p
Twe get
P((x log(1 +x))nT
s;s0>xL= 2)exp L
2p
T+T
T 1
:
We useT2and we plug in L=p
Tto conclude the proof.
The next lemma is a direct application of Theorem 3.4 of Lezaud (1998) and it will
be used in the second crucial auxiliary Lemma 5.11.
Lemma 5.10. For anyw2V,s2Xw,cSw2XSwwe have
P1
Ttw(cSw;0;s)(cSw;0;s)=2
kk2exp
 2(cSw;0;s)1T
16 + 20(cSw;0;s)
:
Proof.Fixw2V,s2Xw,cSw2XSw:By the deﬁnition we have
tw(cSw;0;s) =ZT
0I[X(t) = (cSw;0;s)]dt:
Let us deﬁne f(X(t)) =(cSw;0;s) I(X(t) = (cSw;0;s)). Taking=(cSw;0;s)=2in
Theorem 3.4 of Lezaud (1998), we conclude the proof.
87Lemma 5.11. Let"2(0;1), >1be arbitrary. Suppose that F()deﬁned in (5.7)is
positive and
T >36
max
w2VjSwj+ 1
log 2 + log ( dkk2=")
1min
w2V;s2Xw
cSw2XSw2(cSw;0;s); (5.19)
then
P F()0F()
1 ";
where0= min
w2V;s2Xw
cSw2XSw(cSw;0;s)=2:
Proof.By the deﬁnition of F(), the equation (5.7) and the formula for Hessian of `(see
(5.15)) we have
F()
F()1
Tmin
w2V;s;cSw2XSwtw(cSw;0;s): (5.20)
We complete the proof by bounding the right-hand side of (5.20) from below. First, we
can calculate that
P
min
w2V;s2Xw;cSw2XSw1
Ttw(cSw;0;s)0

P
8w2V;s2Xw;cSw2XSw1
Ttw(cSw;0;s)(cSw;0;s)=2

1 2d max
w2V;s2Xw;cSw2XSw2jSwjP1
Ttw(cSw;0;s)<(cSw;0;s)=2
:(5.21)
Using Lemma 5.10 we bound (5.21) from below by
1 2d max
w2V;s2Xw;cSw2XSw2jSwjkk2exp
 2(cSw;0;s)1T
16 + 20(cSw;0;s)
:
Applying (5.19) we conclude the proof.
Next we state and prove three lemmas, where Lemmas 5.12 and 5.14 will be used in
the proof of the main result Theorem 5.5 and Lemma 5.13 is needed to prove Lemma 5.14.
Lemma 5.12. Let~=^ ,z=kr`()k1:Then
( z)k~Sck1~>h
r`(^) r`()i
+ ( z)k~Sck1(+z)k~Sk1:(5.22)
Besides, for arbitrary  >1on the event

1=
kr`()k1 1
+ 1
the random vector ~belongs to the cone C(;S):
The proof of Lemma 5.12 is similar to the proof of Lemma 3.1 of Huang et al. (2013)
and is based on convexity of `()and properties of the LASSO penalty. For convenience
of the reader we will provide it here using our notation.
88Proof.Since`()is a convex function, then
~>h
r`(^) r`()i
=~>h
r`(+~) r`()i
0;
which instantly proves the left-hand side inequality in (5.22). As we have already men-
tioned, the minimized target function, given in (5.5), is also convex because of the convex-
ityofthenegativelog-likelihood `()and`1-penaltyfunctions. Hence ^willbeaminimizer
in (5.5) if and only if the following conditions are met
8
>>><
>>>:@`(^)
@j= sgn(j);if^j6= 0;
@`(^)
@j; if^j= 0;(5.23)
wherej2f1;:::; 2d(d 1)g. First, we can write
~>h
r`(+~) r`()i
=X
j2Sc~j@`(+~)
@j+X
j2S~j@`(+~)
@j+~>( r`()):
Since ~j=^jforj2Sc, then applying the conditions (5.23) we can bound the last
expression from above by
X
j2Sc^j( sgn(^j)) +X
j2Sj~jj+k~k1z=X
j2Sc j~jj+k~Sk1+zk~Sk1+zk~Sck1:
This in turn equals to (z )k~Sck1+ (+z)k~Sk1meaning that the right-hand side
inequality holds as well. Note that we used the fact that@`(^)
@j= sgn(j)only on
the setSc\fj:^j6= 0g, because ~j=^j j= 0;whenj2Scand ^j= 0:Finally,
by (5.22) and the deﬁnition of 
1we obtain
k~Sck1+z
 zk~Sk1k~Sk1
proving the last claim of the lemma.
Lemma 5.13. For anyb2R2d(d 1)we deﬁnecb= max
w2V;s6=s0;c2X wexp bw
s;s0>Zw(c)
.
Then we have
c 1
bb>r2`()bb>[r`(+b) r`()]cbb>r2`()b (5.24)
and
c 1
br2`()r2`(+b)cbr2`(); (5.25)
where for two symmetric matrices A;Bthe expression ABmeans that B Ais a non-
negative deﬁnite matrix.
89Proof.First we prove the inequality (5.24). By (5.14) we have
b>[r`(+b) r`()] =
=1
TX
w2VX
c2X wX
s06=stw(c;s) 
bw
s;s0>Zw(c)
exp(w
s;s0>Zw(c))
exp(bw
s;s0>Zw(c)) 1
:(5.26)
Moreover, as in (5.15), we have
b>r2`()b=1
TX
w2VX
c2X wX
s06=stw(c;s) 
bw
s;s0>Zw(c)2exp(w
s;s0>Zw(c)):(5.27)
Let us consider an arbitrary summand in (5.26) and the corresponding one in (5.27). We
can focus only on cases where tw(c;s)>0andbw
s;s0>Zw(c)6= 0:From the mean value
theorem we obtain for all non-zero x2R
e jxjex 1
xejxj: (5.28)
So using (5.28) we can write
exp( jbw
s;s0>Zw(c)j)exp(bw
s;s0>Zw(c)) 1
bw
s;s0>Zw(c)exp(jbw
s;s0>Zw(c)j):
Finally, we multiply each side by the expression tw(c;s)(bw
s;s0>Zw(c))2exp(w
s;s0>Zw(c))to
conclude the proof of (5.24). Similarly we can prove (5.25). Finally, for any vector
x2R2d(d 1)we have
x>r2`()x=1
TX
w2VX
c2X wX
s06=stw(c;s) 
xw
s;s0>Zw(c)2exp(w
s;s0>Zw(c))
and
x>r2`(+b)x=1
TX
w2VX
c2X wX
s06=stw(c;s) 
xw
s;s0>Zw(c)2exp(w
s;s0>Zw(c)) exp(bw
s;s0>Zw(c)):
Comparing each summand separately and taking into account the deﬁnition of cband
the inequality (5.28) we ﬁnish the proof.
Lemma 5.14. Let >1be arbitrary and assume that F()>0. Moreover, let us denote
=(+ 1)jSj
2F()and the event 
2=f <e 1g:Then 
1\
2A;where
A=
k^ k12e
(+ 1) F()
and<1is the smaller solution of the equation e =:
Proof.The proof is similar to Theorem 3.1 of Huang et al. (2013) or Lemma 6 of Miaso-
jedow and Rejchel (2018). Suppose we are on the event 
1\
2:Denote again ~=^ ,
so by the previous lemma we have =~
k~k12C(;S):Let us consider the function
g(t) =>r`(+t) >r`(); t0:
90This function is non-decreasing, because the negative log-likelihood function is convex.
Hence, for every t2(0;k~k1)we haveg(t)g(k~k1):By Lemma 5.12 on the event 
1
we obtain
>[r`(+t) r`()] +2
+ 1kSck12
+ 1kSk1: (5.29)
Using Lemma 5.13 with b=tand (5.24) we obtain
t>[r`(+t) r`()]t2exp( t)>r2`(); (5.30)
in this case cb=ctexp(t):Now using the deﬁnition of CIF F();the fact that 
belongs to the cone C(;S)and applying the bounds (5.29), (5.30) we get
texp( t)F()kSk2
1
jSjtexp( t)>r2`()>[r`(+t) r`()]
2
+ 1kSk1 2
+ 1kSck1=
= 2kSk1 2
+ 1(+ 1)kSk2
1=2:
This means that for any tsatisfying (5.29) we have
texp( t)(+ 1)jSj
2F()=: (5.31)
Since, as we mentioned, the function g(t)is non-decreasing, the set of all non-negative t
satisfying (5.29) is a closed interval [0;~t]for some ~t >0. Hence, (5.31) implies ~t,
whereis the smallest solution of the equation e=. Now from (5.29) and (5.30) we
obtain
k~k1e ~te ~t~texp( ~t)>r2`()
F()kTk1kk1>[r`(+~t) r`()]
F()kTk1kk1
2
(+ 1) F()kk1=2k~k1
(+ 1) F()k~k1;
which ﬁnishes the proof.
Proof of Theorem 5.5. Fixarbitrary ">0and >1:ThenF()ispositivebyLemma5.7.
Thus from Lemma 5.11 we know that P F()0F()
1 ":Using it with the right-
hand side of (5.9) we obtain that P(
2)1 ":Moreover, from Lemma 5.9 we have
thatP(
1)1 ":Therefore, Lemmas 5.12 and 5.14 (with = 1for simplicity) imply
the inequality
P
k^ k12e
(+ 1) F()
1 2":
Finally, we bound F()from below by 0F():
Proof of Corollary 5.6. The proof is a simple consequence of the uniform bound (5.10)
obtained in Theorem 5.5. Indeed, for arbitrary w2V,s6=s0andj-th coordinate of
the vectorw
s;s0such thatw
s;s0(j) = 0we obtain
j^w
s;s0(j)j=j^w
s;s0(j) w
s;s0(j)jk ^ k1:
91Analogously, for each w2V,s6=s0andj-th coordinate such that w
s;s0(j)6= 0we have
j^w
s;s0(j)jjw
s;s0(j)j j^w
s;s0(j) w
s;s0(j)jmin k^ k1>2 R;
which concludes the proof.
Proof of Lemma 5.7. Fix >1:For eachwandcSwwe haveZw(cSw;0) = (cSw;0);so
F() = inf
06=2C(;S)X
w2VX
s06=sX
cSw2XSwexp 
(w
s;s0)>
SwcSw
(w
s;s0)>
SwcSw2
kSk1kk1;
where 
w
s;s0
Swand 
w
s;s0
Sware restrictions of w
s;s0andw
s;s0to coordinates from Sw;
respectively. Therefore, we need to bound from below the expression
P
w2VP
s06=sP
cSw2XSwexp 
(w
s;s0)>
SwcSw
(w
s;s0)>
SwcSw2
kSk1kk1(5.32)
for each2C(;S)and6= 0. First, we restrict the third sum in the numerator of (5.32)
to the summands corresponding only to vectors eu2XSwhaving 1 on the coordinate
corresponding to the node u2Swand 0 elsewhere. Then we reduce the numerator
of (5.32) to the following form
X
w2VX
s06=sX
u2Swexp 
w
s;s0(u)
w
s;s0(u)2: (5.33)
Recall that Sw=
u2 w:w
0;1(u)6= 0orw
1;0(u)6= 0	
:Therefore, if w
s;s0(u)6= 0;then
u2Sw;so the sum (5.33) can be bounded from below by
X
w2VX
s06=sX
u:w
s;s0(u)6=0exp 
w
s;s0(u)
w
s;s0(u)2; (5.34)
because (5.33) has more summands and the summands are nonnegative. Using reverse
Hölder’s inequality we replace (5.34) by
A 1
2
4X
w2VX
s06=sX
u:w
s;s0(u)6=0jw
s;s0(u)j3
52
; (5.35)
whereAis
A=X
w2VX
s06=sX
u:w
s;s0(u)6=0exp 
 w
s;s0(u)
:
Next, recall that Sis the set of nonzero coordinates of ;so (5.35) is justkSk2
1=A:
Summarizing, the sum (5.32) is bounded from below by
kSk1
Akk1(5.36)
for each2C(;S)and6= 0:The vector belongs to the cone C(;S);which implies
thatkSck1kSck1kSk1and
kk1= max(kSk1;kSck1)max(kSk1;kSk1);
which gives uskk1kSk1:Applying it in (5.36), we ﬁnish the proof.
925.4 Numerical examples
In this section we describe the details of algorithm implementation as well as the results
of experimental studies.
5.4.1 Details of implementation
We provide in details practical implementation of the proposed algorithm. The solution
of (5.5)dependsonthechoiceof . Findingthe„optimal” parameter andthethreshold 
is diﬃcult in practice. Here we solve it using the information criteria (Xue et al., 2012;
Pokarowski and Mielniczuk, 2015; Miasojedow and Rejchel, 2018).
First, using (5.12) we write the minimized function in (5.5) as the sum
`() kk1=X
w2VX
s6=s0 
1
T`w
s;s0(w
s;s0) X
u2 wjw
s;s0j!
;
wheres;s02f0;1g. Therefore, for ﬁxed w2Vands;s02f0;1gwiths6=s0, the cor-
responding summand is a function which depends on the vector restricted only to its
coordinate vector w
s;s0(see notation (5.1)). So, for each triple wands6=s0we can
solve the problem separately. In our implementation we use the following scheme. We
start with computing a sequence of minimizers on the grid, i.e. for any triple w2V,
s6=s0we create a ﬁnite sequence figN
i=1uniformly spaced on the log scale, starting from
the largesti, which corresponds to the empty model. Next, for each value iwe compute
the estimator ^w
s;s0[i]of the vector w
s;s0
^w
s;s0[i] = argmin
w
s;s0
`w
s;s0(w
s;s0) +ikw
s;s0k1	
; (5.37)
where as in (5.12)
`w
s;s0(w
s;s0) =1
TX
c2X w
 nw(c;s;s0)w
s;s0>Zw(c) +tw(c;s) exp 
w
s;s0>Zw(c)
:
The notation ^w
s;s0[i]should not be confused with w
s;s0(u)introduced before. Namely,
^w
s;s0[i]is thei-th approximation of w
s;s0, whilew
s;s0(u)is the coordinate of w
s;s0corres-
ponding to the node u. To solve (5.37) numerically for a given iwe use the FISTA
algorithm with backtracking from Beck and Teboulle (2009). The ﬁnal LASSO estimator
^w
s;s0:=^w
s;s0[i]is chosen using the Bayesian Information Criterion (BIC), which is a po-
pular method of choosing the value of in the literature (Xue et al., 2012; Miasojedow
and Rejchel, 2018), i.e.
i= argmin
1i100n
n`w
s;s0(^w
s;s0[i]) + log(n)k^w
s;s0[i]k0o
:
Herek^w
s;s0[i]k0denotes the number of non-zero elements of ^w
s;s0[i]andnis the number of
observed jumps of the process. In our simulations we use N= 100.
93Finally, the threshold is obtained using the Generalized Information Criterion (GIC).
ThesimilarwayofchoosingathresholdwasusedpreviouslyinPokarowskiandMielniczuk
(2015); Miasojedow and Rejchel (2018). For a prespeciﬁed sequence of thresholds Dwe
calculate
= argmin
2Dn
n`w
s;s0(^w;
s;s0) + log(2d(d 1))k^w;
s;s0k0o
;
where ^w;
s;s0is the LASSO estimator ^w
s;s0after thresholding with the level :
5.4.2 Simulated data
We consider three models deﬁned as follows. For shortness we denote these models later
on asM1,M2andM3, respectively.
Model1.All vertices have the “chain structure”, i.e. for any node, except for the ﬁrst
one, its set of parents contains only a previous node. Namely, we put V=f1;:::;dg
andpa(k) =fk 1g, ifk >1andpa(1) =;. We construct CIM in the following way.
For the ﬁrst node the intensities of leaving both states are equal to 5. For the rest of
the nodesk= 2;:::;d, we choose randomly a2f0;1gand we deﬁne Qk(c;s;s0) = 9;if
s6=jc ajand1otherwise. In other words, we choose randomly whether the node prefers
to be at the same state as its parent ( a= 0) or not (a= 1). Say, the node kprefers to
be at the same state as the node k 1. Then if these two states coincide, the intensity
of leaving the current state is 1, otherwise it is 9. The intensity is deﬁned analogously,
when the node kdoes not prefer to be at the same state as the node k 1.
Model2.The ﬁrst 5vertices are correlated, while the remaining vertices are independent.
We sample 10arrows between ﬁrst 5nodes by choosing randomly 2parents for each node.
In order to deﬁne the intensity matrix, consider the node w2Vwith pa(w)6=;and
a conﬁguration c2X pa(w)of the parents states. We denote jcj= 1if all the parents of w
are in the state 1, and jcj= 0otherwise. Next we deﬁne intensities as follows
Qw(c;s;s0) =8
>>>>>>>><
>>>>>>>>:5ifpa(w) =;,
9ifpa(w)6=;,sis preferred state and jcj= 1;
1ifpa(w)6=;,sis preferred state and jcj= 0;
9ifpa(w)6=;,sis not preferred state and jcj= 0;
1ifpa(w)6=;,sis not preferred state and jcj= 1:
As in the previous model, the preferred state is chosen randomly from f0;1g. In words,
for every node wwith pa(w)6=;we choose randomly one state, say 0. In this case, if all
parents are 1the process prefers to be in 1and if some of the parents are 0the process
prefers to be in 0.
Model3.All vertices have a „binary tree” structure with arrows from leaves to the root.
So, leaves have no parents, while the inner nodes have two parents, with the exception
that one node has only a single parent, if dis even. If the node has no parents or its
94parents have diﬀerent states, then the intensity of leaving a state is 5. Otherwise, if a node
has only one parent or both parents are in the same state, then the intensity of leaving
a state are computed as in Model 1.
The model M1has a simple structure, which involves all vertices and satisﬁes our
assumption (5.2). The model M2corresponds to a dense structure on a small subset of
vertices and does not satisfy assumption (5.2). Another potential diﬃculty is related to
possible feedback loops, which are usually hard to recognize. Therefore, we also consider
modelM2+, which looks like M2, but contains the interaction terms and fulﬁlls (5.2).
The model M3has slightly more complex structure than M1, but it also satisﬁes our
assumption (5.2).
We consider two cases: d= 20andd= 50for all four models. So, the considered
number of possible parameters of the model (the size of ) is2d2= 800or5000, respec-
tively. For model with interactions, number of possible parameters is d2(d+ 1) = 8400
or127500:We useT= 10and50for all models and we replicate simulations 100times
for each scenario. In Table 5.1 we present averaged results of the simulations in terms of
three quality measures
•power, which is a proportion of correctly selected edges;
•false discovery rate (FDR) , which is a fraction of incorrectly selected edges
among all selected edges;
•model dimension (MD) , which is a number of selected edges.
We observe that in the models M1andM3the results of experiments conﬁrm that
the proposed method works in a satisfactory way. For T= 10the algorithm has high
power and its FDR is not large. The ﬁnal model selected by our procedure is slightly
too small (it does not discover a few existing edges). When we increase observation time
(T= 50), then our estimator behaves almost perfectly.
The model M2is much more diﬃcult and this fact has a direct impact on simulation
results. Namely, for T= 10the power of the algorithm is relatively low with FDR
also being rather small. The procedure performs slightly better when we take T= 50:
However, for both observation times the estimator cannot ﬁnd the true edges in the graph.
One of the reasons of such behaviour is that in M2the dependence structure in CIM
is not additive in parents. This fact combined with possible feedback loops leads to
recovering existing edges, but having the opposite to the true ones directions. Looking
deeper into the results for a few examples chosen from our experiments we conﬁrm this
claim, i.e. the edges between nodes are correctly selected, but their directions are wrong.
Therefore, we can conclude that in the complex model M2our estimator seems at least
to be able to recognize interactions between nodes, which is important in many practical
problems on its own. The results for M2+conﬁrm that the performance of our method
increases, when we consider more complex parametrization with interaction terms.
95Table 5.1: Results for simulated data. In the model M1andM3the true dimension is 19
ford= 20and49ford= 50. In the model M2the true model dimension is 10.
Modeld T Power FDR MD
M120 10 0.86 0.03 16.8
50 1 0.02 19.3
50 10 0.61 0.01 30.3
50 1 0.01 49.3
M220 10 0.16 0.2 2
50 0.78 0.04 8.1
50 10 0.10 0.15 1.28
50 0.62 0.02 6.4
M2+20 10 0.35 0.1 3.9
50 0.9 0.02 9.2
50 10 0.17 0.08 2
50 0.68 0.01 6.9
M320 10 0.17 0.1 3.7
50 0.97 0.01 18.7
50 10 0.6 0.09 3.2
50 0.88 0.003 43
5.5 Extension of the results
In this chapter we proposed the method for structure learning of CTBNs. We conﬁrmed
the good performance of our method both theoretically and experimentally. To simplify
the notation and help the reader to follow our reasoning we restricted ourselves to graphs
with only two possible states for each node. However, our results can be generalized
in a straightforward way to any ﬁnite graphs by extending to other possible jumps
and possible values of parents. In terms of the explanatory variable, it is equivalent to
the standard encoding of qualitative variables in linear or generalized linear models. To
demonstrate the generalization more clearly we present an example similar to Example 5.1
presented in the very beginning of this chapter.
Example 5.15.We consider CTBN with three nodes A;BandC:Let their state spaces
beXA=f0;1;2g,XB=f0;1;2;3gandXC=f0;1;2g, respectively. Then for the node A
we deﬁne the function ZAas
ZA(b;c) = [1;I(b= 1);I(b= 2);I(b= 3);I(c= 1);I(c= 2)]>
for eachb2f0;1;2;3gandc2f0;1;2g. Analogously, we can deﬁne representations for
the remaining nodes:
ZB(a;c) = [1;I(a= 1);I(a= 2);I(c= 1);I(c= 2)]>
96for eacha2f0;1;2gandc2f0;1;2gand
ZC(a;b) = [1;I(a= 1);I(a= 2);I(b= 1);I(b= 2);I(b= 3)]>
for eacha2f0;1;2gandb2f0;1;2;3g.
Therefore, for each node wand for each conﬁguration of parents’ states (e.g. for the
nodeAand values in the nodes BandC) the value of the function Zw(;)is still a binary
vector with the dimension equal to the sum of the numbers of states in all parents nodes
with subtracted number of nodes and added 2. In this example the parameter vector (5.1)
is deﬁned as
= 
A
0;1;A
1;0;A
0;2;A
2;0;A
1;2;A
2;1;B
0;1;B
1;0;B
0;2;:::;B
3;1;B
2;3;B
3;2;C
0;1;:::;C
2;1>:
With a slight abuse of notation, the vector A
0;1is given as
A
0;1=
A
0;1(1);A
0;1(B= 1);A
0;1(B= 2);A
0;1(B= 3);A
0;1(C= 1);A
0;1(C= 2)>;
and we interpret it as follows: if all A
0;1(B= 1),A
0;1(B= 2)andA
0;1(B= 3)are equal
to 0, then the intensity of the change from the state 0to1at the node Adoes not
depend on the state at the node B:Similarly, the coordinates A
0;1(C= 1)andA
0;1(C= 2)
describe the dependence between the above intensity and the state at the node C;and
A
0;1(1)corresponds to the intercept. For the node Bthe coordinates of the vector B
1;3=
B
1;3(1);B
1;3(A= 1);B
1;3(A= 2);B
1;3(C= 1);B
1;3(C= 2)
describe the relation between
the intensity of the jump from the state 1to3at the node Bto the intercept, states at
the nodesAandC;respectively.
Our results can be also easily generalized to the case, where we consider not only
additiveeﬀectin(5.2), butalsointeractionsbetweenparents. Letusagainuseanexample.
Example 5.16.As previously we consider CTBN with three nodes A; BandCwith
corresponding state spaces XA=f0;1g,XB=f0;1;2gandXC=f0;1g. In a linear
model we have the following Zwfunctions:
ZA(b;c) = [1;I(b= 1);I(b= 2);I(c= 1)]>;
ZB(a;c) = [1;I(a= 1);I(c= 1)]>;
ZC(a;b) = [1;I(a= 1);I(b= 1)]>
for eacha;c2 f0;1gandb2 f0;1;2g. Then after we add pairwise interactions to
the linear model the functions above take the form
ZA(b;c) = [1;I(b= 1);I(b= 2);I(c= 1);I(b= 1;c= 1);I(b= 2;c= 1)]>;
ZB(a;c) = [1;I(a= 1);I(c= 1);I(a= 1;c= 1)]>;
ZC(a;b) = [1;I(a= 1);I(b= 1);I(a= 1;b= 1)]>:
For models with more nodes we can also take into account more complex interactions.
97Chapter 6
Structure learning for CTBNs for
incomplete data
In the previous chapter we considered the case when we observe CTBN at each moment of
time. Under this assumption we introduced a novel method of structure learning. In this
chapter we show that our method can be adapted to partially observed and noisy data.
In the case of partial observations we need to introduce the observation and the likelihood
of the observed data given a hidden trajectory of a process. We can again parametrize
CIM by (5.2). However, in this case the problem (5.5) becomes more challenging and
leads to the following two problems. First, the theoretical analysis becomes more chal-
lenging because the loss function is not convex. Second, the likelihood function can not be
calculated explicitly, hence, it is diﬃcult to obtain from the computational perspective.
InoursolutionweformulatetheEMalgorithmforthiscase, wheretheexpectationstep
is standard and concerns the calculation of the expected log-likelihood. The maximization
step is performed in the same way as for the complete data. Since the density belongs to
the exponential family, the E-step requires to compute the expected values of suﬃcient
statistics, which is done with the MCMC algorithm developed in Rao and Teh (2013). In
addition, the results from Majewski et al. (2018) combined with Miasojedow and Niemiro
(2017) are used in the analysis of the Monte Carlo scheme.
6.1 Introduction and notation
Lett= (t0;t1;:::;tn)with 0 =t0< t 1< ::: < t nandS= (S0;S1;:::;Sn)describe
the full trajectory Xof the process on the interval [0;T](tdenotes times of jumps, Sis
a skeleton, where Si2Xis a state at the moment ti). Let Xdenote the set of all possible
trajectories of the process. Let us consider the case when instead of observing the full
trajectoryXwe have access only to the partial and noisy data Ywith the conditional
densityp(YjX). More precisely, we assume that Yis represented by the observation
ofXat timestobs
1;:::;tobs
kwith the likelihoods gj(Sjt)forj= 1;:::;k;where
jt= maxfi:titobs
jg:
98We assume that 0<C <g j<~Cfor1jk. In this case the full density is given by
p(X;Y ) =p(YjX)p(X);
wherep(X)is given by (2.8). Observe that the dependence of p(X)onis mediated
through our assumption (5.2), which can be inserted into (2.9). For the clarity of pre-
sentation we assume that p(YjX)is known, however, the adaptation of our method to
the case where p(YjX)depends also on some unknown parameters is straightforward.
The negative of the log-likelihood function in this case is given by
`() = logZ
Xp(X;Y )dX
;
where symbol dXmeans the summationﬁrstover allpossible numbers of jumps of the tra-
jectoryX, thenoverallpossiblejumps, andtheintegrationwithrespecttotimesofjumps.
More precisely,
Z
f(X)dX=1X
n=0X
S12XX
Sn2Xt2Z
0t3Z
t1TZ
tn 1f(n;t1;:::;tn;S1;:::;Sn)dt1:::dtn:
Again we can deﬁne the estimator of the parameter vector , as previously,
^= argmin
2R2d(d 1)f`() +kk1g: (6.1)
Since we are not able to compute `()analytically, we need to propose an eﬃcient algo-
rithm for ﬁnding ^. One of the eﬃcient algorithms of solving complex optimization prob-
lems of the form (6.1) is the projected Proximal Gradient Descent (p-PGD) algorithm
(see for example Beck and Teboulle (2009) and Majewski et al. (2018)). For a closed
compact convex set KbyQ
K(a)we denote the projection of aontoK. Then p-PGD is
deﬁned iteratively by
k+1=Y
K 
proxk;kk1(k kr`(k))
; (6.2)
wherefkgk0is a sequence of step-sizes, and \ prox " denotes the proximal operator
deﬁned for any convex function gby
prox;g(x) = argmin
y
g(y) +1
2ky xk2
:
In the case of `1penalty, i.e. g=kk 1, the proximal operator is just a soft-thresholding
operator. Element-wise soft-thresholding operator S:Rn!Rnis deﬁned as
S(xi) = [jxij ]+sgn(xi);
where []+denotes the positive part.
In our case we are not able to evaluate the gradient r`explicitly and we will use
stochastic version of the projected proximal gradient algorithm (p-SPGD), where r`
99is replaced by its Monte Carlo approximation. Under the regularity conditions given
for example in Assumption AD:1in Douc et al. (2014) we derive that the gradient of
the negative log-likelihood is given by
r`() = rlogZ
p(X;Y )dX
= rR
p(X;Y )dXR
p(X;Y )dX=
= R
p(X;Y )rlog(p(X;Y ))dXR
p(X;Y )dX=
= Zrlog(p(X;Y ))p(X;Y )R
p(X;Y )dXdX=
= Z
rlog(p(X;Y ))p(XjY)dX=
= E(rlog(p(X;Y ))jY):(6.3)
This equation is sometimes referred as Fisher’s identity . Now based on (6.3) we can
approximater`by
(;X1;:::;Xm) = 1
mmX
i=1rlog(p(Xi;Y)); (6.4)
whereX1;:::;Xmis a set of subsequent states of the Markov chain with the stationary
distribution =p(XjY)/p(X;Y ), where in particular each of X1;:::;Xmis
a trajectory of the process X. To generate this Markov chain we will use the procedure
described below.
6.2 Sampling the Markov chain with Rao and Teh’s
algorithm
Consider the set Mof all possible intensity matrices of Markov jump processes with
the state spaceXequipped with some matrix distance. Therefore, for any Q2Mand
s2Xeach element Q(s;s)on the diagonal of Qis nonpositive, and otherwise it is non-
negative. LetLM be a compact set and choose  > sup
Q2Lmax
s2XQ(s), where we used
the notation Q(s) = Q(s;s)introduced in Section 2.5. To generate the Markov chain
parametrized by Q2Lwe will use Rao and Teh’s algorithm, which uses the idea of
uniformization and the notion of virtual jumps (Rao and Teh (2013)). A virtual jump in
a trajectory described by a pair of time points tand a corresponding skeleton Smeans
that for two subsequent time points tiandti+1we haveSi=Si+1. Simply put, it means
that the process can jump from a certain state back to the same state. Here we provide
a comprehensive description of a single iteration of the algorithm. Given an arbitrary
trajectory (t;S)ofX, such that Y(tobs
i) =X(tobs
i)for1ik;we generate another
trajectory (t;S)with this property using the following procedure:
1. We generate times of virtual jumps vfrom piecewise homogeneous Poisson process
with the intensity  Q(X(t)), which means that for every interval [ti;ti+1)we
100sample a number kiof virtual jumps from the Poisson distribution with the para-
meter equal to ( Q(Si))(ti+1 ti); then the times of virtual jumps are uniformly
distributed on [ti 1;ti).
2. We add virtual jumps to the trajectory in a correct order and we obtain new times
of jumpst0=t[vand a new corresponding skeleton S0= (S0
0;:::;S0
n0), wheren0is
thenumberofelementsof t0. ThereforeS0= (S0;:::;S0;S1;:::;S1;:::;Sn;:::;Sn)
withk0instances ofS0,k1instances ofS1, etc. In other words, this skeleton con-
tains the same jumps at the same time points as Sand for every interval [ti;ti+1)
the states inS0are equal toSi 1.
3. We sample a new skeleton Sof the size n0using the standard Forward-Filtering
Backward-Samplingalgorithm(FFBS),whichforcompletenessisprovidedattheend
of this chapter in Section 6.5. Thus, the resulting distribution of the skeleton will be
(S0)Qn0
i=1P(S0
i 1;S0
i)Qk
j=1gj(S0
jt0)
P
S(S0)Qn0
i=1P(Si 1;Si)Qk
j=1gj(Sjt0);
whereis the initial distribution (which does not depend on Q), and
P(s;s0) =8
><
>:Q(s;s0)
;ifs6=s0;
1 Q(s)
;ifs=s0;(6.5)
where s;s02X. The summation in the denominator is taken with respect to all
possible skeletons of size n0containing virtual jumps.
4. From the trajectory (t0;S)we discard newly acquired virtual jumps (i.e. we re-
move Sisuch that Si=Si 1) and we obtain a new set tof times of jumps and
the resulting trajectory (t;S);which describes the desired Markov chain.
The procedure above describes one step of the algorithm and Rao and Teh (2013) showed
that both trajectories (t;S)and(t;S)describe the same MJP. Simply put, the Poisson
ratedominates the leaving rates of all states of the MJP and the new skeleton will
contain more events than there are jumps in the MJP path. The corresponding trajectory
is regarded as a redundant representation of a pure-jump process that always jumps to
a new state. Note, that our new stochastic matrix Pdeﬁned in (6.5) allows self-transitions
(we refer to them as virtual jumps), and as increases their number grows as well. These
self-transitions will be discarded in the ﬁnal step of the algorithm, which compensates for
an increased number of events.
The step of the algorithm is the composition of two Markov kernels. First we add
101virtual jumps according to the kernel MJ
Qdeﬁned by
MJ
Q((t;S);(~t;~S)) =I(t=t[v)
n 1Y
i=0(
[( Q(si))(ti+1 ti)]kie ( Q(si))(ti+1 ti)I(ti<vi;1<<vi;ki<ti+1)ji+1Y
l=jiI(~Sl=Si))
:
(6.6)
Next we draw a skeleton according to the kernel MS
Qgiven by
MS
Q((~t;~S);(t;S)) =I(t=~t)(S0)Qn
i=1P(Si 1;Si)Qk
j=1gj(Sjt)
P
~Sn
(~S0)Qn
i=1P(~Si 1;~Si)Qk
j=1gj(~Sj~t)o;(6.7)
wherendenotes the length of the vector ~t. The dependence of MS
QonQis hidden in
the deﬁnition of P.
Notethataddingvirtualjumptimes vdeﬁnesthenewskeletonuniquelyandwedenote
itbySv. Therefore, sincesamplinganewskeletondoesnotchangetimesofjumps, thefull
kernelMQ=MS
QMJ
Qis given by
MQ((t;S);(t;S)) =MJ
Q((t;S);(t;Sv))MS
Q((t;Sv);(t;S)): (6.8)
The kernel MQacts on a function f(t;S)as follows
MQf(t;S) =E
f(t;S)j(t;S)
=
=1X
k0=01X
kn 1=0Zn 1Y
i=0Ifti<vi;1<<vi;ki<ti+1gX
Sf(t;S)MQ((t;S);(t;S))dv:
where the inside sum is taken with respect to all possible skeletons of the same length
ast. Further, when it does not cause any confusion, for convenience we often denote any
single trajectory (t;S)asX, and asXi the trajectory obtained after i-th iteration of
the algorithm with the starting trajectory X0. Then for any two adjacent trajectories Xi
andXi+1the function MQf(Xi)stands for E[f(Xi+1)jXi]. Moreover, for any trajec-
toryXletV(X) =V(t;S) =n+ 1(recall that ndenotes the number of jumps on the
trajectoryXdescribed by tandS).
6.3 Structure learning via penalized maximum likeli-
hood function
Recall the assumption (5.2) introduced in the previous chapter
log(Qw(c;s;s0)) =w
s;s0>Zw(c):
For a given parameter vector 2R2d(d 1)this deﬁnes the intensity matrix Qand this
mapping can be regarded as an isometry. So, if belongs to a compact set K2R2d(d 1),
102thenQbelongs to some compact set L2M and the construction above is still valid.
In this case, we will frequently write Minstead ofMQ. Now we introduce the main
theoretical result regarding the convergence of our algorithm.
Theorem 6.1. LetK 2R2d(d 1)be some compact convex set. Denote as NK()the
normal cone to the set Kat the point 
NK() =fa2R2d(d 1):ha;z ifor allz2Kg:
Moreover, denote
S=f2K: 02r`() +@kk1 NKg;
where@kk1denotes the subgradient. Suppose that (`+kk1)(S)has non-empty interior.
Assume also EV2(X0)<1. Let the sequence fk;k2Ngsatisfyk>0,lim
k!1k= 0,
and1X
k=1k=1;1X
k=12
k<1;1X
k=1jk k+1j<1:
Letfkgbe a sequence generated by the projected stochastic proximal gradient descent as
in (6.2). Then
dist(k;S\K )k!1!0a:s:
Remark 6.2.
(1) Obviously, lim
k!1k= 0follows from the convergenceP1
k=12
k<1.
(2) The symbol (`+kk 1)(S)should be understood as the image of the set Sunder
the function h() =`() +kk1.
The theorem is a consequence of Theorem 5.4 of Majewski et al. (2018). It states that
the sequence of parameter vectors kgenerated by the projected SPGD algorithm con-
verges almost surely to a stationary point of the function being minimized, where instead
of the gradient of the negative log-likelihood we used its Markov chain approximation.
Before proving the theorem we need a few auxiliary results and some additional no-
tation. For any function fof the trajectory Xand any signed-measure , we deﬁne its
V-variation by
kkV= sup
jfjVj(f)j; (6.9)
where
(f) =Z
Xfd;
and the integration is over all possible trajectories of the process. Also we denote
jfjV= sup
X2Xjf(X)j
V(X); (6.10)
where supremum is taken with respect to all possible trajectories.
First we prove three auxiliary lemmas concerning the kernels MQdeﬁned by (6.8).
103Lemma 6.3. Fix a compact set L2M. Then there exist constants 1;22(0;1)and
b1;b2<1such that for any trajectory Xwe have
sup
Q2LMQV(X)1V(X) +b1
and
sup
Q2LMQV2(X)2V2(X) +b2:
Proof.The proof is a simple extension of proofs of Lemma 5 and Proposition 6 in Miaso-
jedow and Niemiro (2017). First we note that in the ﬁrst step of Rao and Teh’s algorithm
we do not add any new jumps. This implies that in order to obtain the desired bounds
onMV(X)we simply need to bound the expectation EV(X0)of the jumps for the tra-
jectoryX0obtained on the Step 3. Analogously, instead of bounding MV2(X)we bound
EV2(X0). Indeed we have
MV2(X) =E[V2(X0)jX] =E[E(V2(X0)jX0j=n0+ 1)jX]
andE(jX0j=n0+1jX)n+1+TwithV(X) =n+1, wherejX0jdenotes the number
of states in the trajectory X0. For the trajectory X0= (t;S0)we have
V(X0) = 1 +n0 1X
i=0I(S0
i6=S0
i+1):
Therefore, we get
EV2(X0) = 1 + 2 E"n0 1X
i=0I(S0
i6=S0
i+1)#
+E"X
i6=jI(S0
i6=S0
i+1)I(S0
j6=S0
j+1)#
:(6.11)
ApplyingLemma2ofMiasojedowandNiemiro(2017)togetherwiththedeﬁnition(6.5),
the deﬁnition of and assumptions on likelihoods gj(Sjt), for eachi= 0;:::;n0 1we
obtain
P 
S0
i=sjS0
i+1=s
i>0:
ThisisalowerboundforthebackwardtransitionprobabilityusedbytheFFBSalgorithm.
An analogous inequality for the forward transition probability is also true. Hence,
E 
I(S0
i6=S0
i+1)
=E
E(I(S0
i6=S0
i+1)jS0
i+1)
=
=P 
S0
i6=sjS0
i+1=s
1 i;(6.12)
which means that we can bound the second term on the RHS of (6.11) from above
by2Pn0 1
i=0(1 i). Also, for each i6=jwe have
I(S0
i6=S0
i+1)I(S0
j6=S0
j+1)I(S0
i6=S0
i+1): (6.13)
104Thus, using (6.12) and (6.13), the third term on the RHS of (6.11) can be bounded by
E2
64n0 1X
j=0n0 1X
i=0
i6=jI(S0
i6=S0
i+1)3
75=n0 1X
j=0n0 1X
i=0;
i6=jE
E(I(S0
i6=S0
i+1)jS0
i+1)
=
=n0 1X
j=0n0 1X
i=0;
i6=jP(S0
i6=sjS0
i+1=s)
n0 1X
j=0n0 1X
i=0;
i6=j(1 i):
Combining both bounds we obtain
EV2(X0)1 + 2n0 2n0 1X
i=0i+n0(n0 1) n0 1X
j=0n0 1X
i=0;
i6=ji
(1 )(n0+ 1)2+ 1(1 )(n+ 1) +b;
where= min
ifig2 (0;1)andbis some ﬁnite constant. This ﬁnishes the proof of
the second part of the lemma.
The ﬁrst inequality can be shown either analogously to the second one or by applying
Jensen’s inequality. Indeed,
(MQV(X))2MQV2(X)2V2(X) +b2;
which in turn implies that
MQV(X)p
2V2(X) +b2p2V(X) +p
b2;
which concludes the proof.
Lemma 6.4. IfE[V(X0)]<1, then sup
n1MQV(Xn)<1:If in addition E[V2(X0)]<1,
then sup
n1MQV2(Xn)<1:
Proof.Recall that Xnis the trajectory obtained after n-th iteration of Rao and Teh’s
algorithm starting from the trajectory X0. As previously we can consider EV(Xn+1)
instead ofMQV(Xn). Hence, by the previous lemma we have
E[V(Xn+1)] =E[E[V(Xn+1)jXn]] =E[MQV(Xn)]EV(Xn) +b;
where2(0;1)andb <1. Then, by iterating this majorization procedure recursively
we get
E[V(Xn+1)]n+1EV(X0) +bn+1X
i=1iEV(X0) +b
1 :
Since the RHS of this inequality does not depend on n, then E[V(Xn+1)]is bounded by
a ﬁnite constant. This concludes the proof for the ﬁrst bound. The second inequality can
be shown analogously using the bound for supQ2LMQV2(X)in Lemma 6.3.
105Lemma 6.5. For any compact set L  M there exists C2(0;1)such that for
anyQ;~Q2Land all trajectories Xwe have
kMQ(X;) M~Q(X;)kVCV(X)kQ ~Qk:
Proof.For anyQ;~Q2Lthe expression of interest (see the deﬁnitions (6.6)-(6.9)) can be
bounded by the sum of two terms as follows
sup
jfjVMQf(t;S) M~Qf(t;S)= sup
jfjVMS
QMJ
Qf(t;S) MS
~QMJ
~Qf(t;S)
sup
jfjVMS
QMJ
Qf(t;S) MS
QMJ
~Qf(t;S)+ sup
jfjVMS
QMJ
~Qf(t;S) MS
~QMJ
~Qf(t;S)
:=I1+I2:
We can bound I1by
1X
k0=01X
kn 1=0Zn 1Y
i=0Ifti<vi;1<<vi;ki<ti+1gX
Sjf(t;S)j
MJ
Q((t;S);(t;Sv))MS
Q((t;Sv);(t;S)) MJ
~Q((t;S);(t;Sv))MS
Q((t;Sv);(t;S))dv:
Recall that kidenotes the number of virtual jumps on the interval [ti;ti+1). SincejfjV
and for any possible Swe haveV(t;S)1+n+Pn 1
i=0ki, andP
SMS
Q 
(t;Sv);(t;S)
= 1
(see (6.7)), then we can further bound I1by
1X
k0=01X
kn 1=0Zn 1Y
i=0Ifti<vi;1<<vi;ki<ti+1g
 
1 +n+n 1X
i=0ki!MJ
Q((t;S);(t;Sv)) MJ
~Q((t;S);(t;Sv))dv;(6.14)
Next, letUdenote the set of indices usuch that only u-th rows of Qand ~Qdiﬀer. Let
Q1=Q,QjUj=~Qand foru2Uwe deﬁne the matrix Qu+1asQuwith theu-th row
replaced by the corresponding row of ~Q. In particular, for u2Uwe haveQu(~s)6=Qu+1(~s)
for a certain state ~sand for all states s6=~swe haveQu(s) =Qu+1(s). Then the expression
under the integral can be bounded by
MJ
Q((t;S);(t;Sv)) MJ
~Q((t;S);(t;Sv))
X
uMJ
Qu((t;S);(t;Sv)) MJ
Qu+1((t;S);(t;Sv)):
Each term in the last sum can be expressed in the form
106MJ
Qu((t;S);(t;Sv)) MJ
Qu+1((t;S);(t;Sv))=
=n 1Y
i=0[( Qu(Si)) (ti+1 ti)]kie ( Qu(Si))(ti+1 ti) 
 n 1Y
i=0[( Qu+1(Si)) (ti+1 ti)]kie ( Qu+1(Si))(ti+1 ti)=
=n 1Y
i=0(ti+1 ti)kin 1Y
i=0
Si6=~s( Qu(Si))kie ( Qu(Si))(ti+1 ti)
( Qu(~s))P
Si=~ski
e ( Qu(~s))P
Si=~s(ti+1 ti)
 ( Qu+1(~s))P
Si=~ski
e ( Qu+1(~s))P
Si=~s(ti+1 ti):
Now letx= Qu(~s),y= Qu+1(~s),a=P
Si=~skiandb=P
Si=~s(ti+1 ti). Let
alsor= max(x;y). Hence, we need to bound the expression jxae bx yae byjfor some
x;y;a;b> 0. From Lagrange’s mean value theorem we have
xae bx yae bysup
z2(x;y)d
dz(zae bz)jx yj= sup
z2(x;y)aza 1e bz bzae bzjx yj:
Next we can bound the above supremum by
sup
z2(x;y)aza 1e bz bzae bzsup
z2(x;y)max(aza 1e bz;bzae bz): (6.15)
Let us assume ﬁrst that the ﬁrst expression of (6.15) is the maximum, then we obtain
aza 1e bza~a 1e b~; (6.16)
where ~= min(r;a 1
b). In the case where the second expression is the maximum we
obtain analogously bzae bzb~ae b~, where ~= min(r;a
b). Using the ﬁrst assumption
with the corresponding inequality (6.16), the fact that an 1P
i=0kiand the fact that the set
of indicesUis ﬁnite, we can bound I1by
X
u1X
k0=01X
kn 1=0Zn 1Y
i=0Ifti<vi;1<<vi;ki<ti+1gn 1Y
i=0
Si6=~s( Qu(Si))kie ( Qu(Si))(ti+1 ti)
n 1Y
i=0
Si=~s~kie ~(ti+1 ti)n 1X
i=0ki 
1 +n+n 1X
i=0ki!
jQu+1(~s) Qu(~s)jdv
X
ujQu+1(~s) Qu(~s)jE"n 1X
i=0ki 
1 +n+n 1X
i=0ki!#

jUjkQ ~Qk(T+nT+T+ (T)2) =
=jUjkQ ~Qk(T(n+ 2) + (T)2) =C1(n)kQ ~Qk;
107whereC1(n)is a certain linear function of n:
Using the same technique and the fact that bTwe can bound I1by
X
ujQu+1(~s) Qu(~s)jE"
T 
1 +n+n 1X
i=0ki!#
TjUjkQ ~QkE 
1 +n+n 1X
i=0ki!
=
=TjUjkQ ~Qk(1 +n+T) =C2(n)kQ ~Qk;
whereC2(n)is a certain linear function of n:
Now we can bound I2in a similar way as we did for I1by an expression similar
to (6.14), namely by
1X
k0=01X
kn 1=0Zn 1Y
i=0Ifti<vi;1<<vi;ki<ti+1g 
1 +n+n 1X
i=0ki!

MJ
~Q((t;S);(t;Sv))X
SMS
Q((t;Sv);(t;S)) MS
~Q((t;Sv);(t;S))dv:
Before we continue, for any possible skeleton Slet us denote a few auxiliary functions
LQ(S) =(S0)nY
i=1P(Si 1;Si)kY
j=1gj(Skj);
RQ=X
SLQ(S); HQ(S) =LQ(S)
RQ:
Therefore, we need to obtain the bound for
X
SHQ(S) H~Q(S)=X
SLQ(S)
RQ L~Q(S)
R~Q
X
SLQ(S) L~Q(S)
RQ+X
SL~Q(S)1
RQ 1
R~Q:(6.17)
The initial distribution and likelihoods gjfor alljare the same for diﬀerent intensity
matricesQ. Using this fact and “triangle” inequality for two products of positive numbers
nY
j=1xj nY
j=1yjnX
j=1jxj yjjj 1Y
i=1xinY
i=j+1yi;
withxi=P(Si 1;Si)andyi=~P(Si 1;Si), where ~Pis deﬁned by ~Qin the same way
asPdeﬁned byQ(see (6.5)), we obtain the inequality
X
SLQ(S) L~Q(S)X
S~Ck
kQ ~QknX
j=1j 1Y
i=1P(Si 1;Si)nY
i=j+1~P(Si 1;Si)
~Ck
kQ ~QknX
j=1X
SjX
S<jX
S>jj 1Y
i=1P(Si 1;Si)nY
i=j+1~P(Si 1;Si)~Ck
kQ ~QknjSj:
108Here we used the assumption that all likelihoods are bounded Cgj~Cfor1jk,
and we locally denoted the number of all possible states of the process as jSj. Note, that
the sum over all possible skeletons Swas divided into three sums: the ﬁrst one is over
all possible states of Sj, the second - over all possible states of S1;:::;Sj 1and the last
one is over all possible states Sj+1;:::;Sn. Applying again bounds on the likelihoods we
easily obtain that for any Q
1
~Ck1
RQ1
Ck;
which leads to the fact that the ﬁrst expression in (6.17) is bounded from above by
C3(n)kQ ~Qk, whereC3(n)is some linear function of n. The second expression in (6.17)
is bounded by
~Ck1
RQ 1
R~Q~CkP
SLQ(S) L~Q(S)
RQR~Q:
Applying two previously obtained inequalities we derive the bound C4(n)kQ ~Qk, where
C4(n)is some linear function of n. Now combining all obtained bounds for I1andI2we
conclude the proof.
Lemma 6.6. For the measurable function V:X![1;+1)let us denote by
DV(;0) = sup
XkM(X;) M0(X;)kV
V(X)
theV-variation of the kernels MandM0and letF:X!R+be the function such that
sup2KjFjV<1. Moreover, deﬁne
^F=X
n0Mn
(F (F)):
Then
jM^F M0^F0jVCfDV(;0) +jF F0jVg:
Proof.The proof follows the same arguments as the proof of the Lemma 4.2 in Fort et al.
(2011) in the supplement materials to the paper. In addition, some references to the ﬁrst
papers using similar argumentation can be found there.
First, we use the following decomposition of Mk
f Mk
0ffor anyk1
Mk
f Mk
0f=k 1X
j=0Mj
(M M0)
Mk j 1
0f 0(f)
:
By the Proposition 7 in Miasojedow and Niemiro (2017) the sets fX:jV(X)j< hgare
the small sets for any h2R. Therefore, combining it with Lemma (6.3) we have by
Theorem 9 in Roberts and Rosenthal (2004) that there exist constants Cand2(0;1)
such that
kMk
(X;) kVCk
V(X):
109Thispropertyiscalled geometric ergodicity ofthekernel Mwithinvariantdistribution .
Hence, for any k1and any trajectory X?
k 0kV
k Mk
(X?;)kV+kMk
(X?;) Mk
0(X?;)kV+kMk
0(X?;) 0kV
 
Ck
+C0k
0
V(X?)
+ sup
jfjV1k 1X
j=0Mj
(M M0)
Mk j 1
0f 0(f)
(X?): (6.18)
We can bound each summand from the sum on the RHS by
sup
jfjV1Mj
(M M0)(Mk j 1
0f 0(f))(X?):
Now let us denote H= [Mk j 1
0f 0(f)]. Then the expression within the absolute
value operator is bounded by
sup
jfjV1sup
jgjjHjj(M M0)g(X?)jsup
jfjV1jHjVsup
jgjVj(M M0)g(X?)j=
= sup
jfjV1sup
XjMk j 1
0f(X) 0(f)(X)j
V(X)kM(X?;) M0(X?;)kV
C0k j 1
0DV(;0)V(X?):
Thus the last term in the (6.18) is bounded by
C0DV(;0)k 1X
j=0k j 1
0Mj
V(X?)
C0DV(;0)k 1X
j=0k j 1
0
(V) +Cj
V(X?)	

C0
1 0DV(;0) ((V) +CV(X?)):
Taking the limit as k!+1in the ﬁrst term in (6.18) we obtain
k 0kVC0
1 0DV(;0) ((V) +CV(X?)): (6.19)
Now from (7) in Fort et al. (2011) we have
M^F M0^F0=X
n1n 1X
j=0 
Mj
 
(M M0) 
Mn j 1
0F 0(F)
 X
n1fMn
0(F0 F) 0(F0 F)g X
n1fMn
0F 0(F)g:(6.20)
110Let us consider the ﬁrst term. Similarly to the previous step by Gwe denote the operator
G= [Mn j 1
0F 0(F)]. Then we can bound
 
Mj
 
(M M0) 
Mn j 1
0F 0(F)
(X)
sup
jgjjGj 
Mj
 
(M M0)g(X)
jGjVsup
jgjV 
Mj
 
(M M0)g(X)
jGjV sup
jhjkM M0kV 
Mj
 
h(X)
jGjVDV(;0) sup
jhjV 
Mj
 
h(X)
C0n j 1
0jFjVDV(;0)Cj
V(X):
For the second and third terms in (6.20) we obtain the bounds
Mn
0(F0 F)(X) 0(F0 F)C0n
0V(X)jF0 FjV
and
jfMn
0F 0(F)g(X)j=j( 0)fMn
0F 0(F)g(X)j
k 0kVjMn
0F(X) 0(F)jV
k 0kVC0n
0jFjV:(6.21)
Therefore, combining the inequalities (6.19) – (6.21) we get
jM^F(X) M0^F0(X)jC0C
(1 0)(1 )jFjVDV(;0)V(X)+
+C0
1 0V(X)jF0 FjV+
+C0
(1 0)jFjVDV(;0) ((V) +CV(X)):
Thus, since sup2KjFjV<1, there exists a positive constant L;0for which we have
jM^F(X) M0^F0(X)jL;0V(X)(DV(;0) +jF0 FjV):
This concludes the proof.
The proof of the main Theorem 6.1 is based on Theorem 6.7 which is obtained by com-
bining Theorem 5.4 and Proposition 5.5 of Majewski et al. (2018) with a slight adjustment
of the notation to our context. For any compact convex set Kby
NK(x) =fa2Rd:ha;z xifor allz2Kg
we denote the normal cone to Kat the point x. We consider an open set B2Rdand
functionsf;g:B!R. We assume that fis a continuously diﬀerentiable function and
also for all 2Kits gradient satisﬁes
rf() =Z
X(;X)(dX)
111for some probability measure and an integrable function (;X).
Byfk;k2Ngwe denote the sequence generated by the projected SPGD:
k2Y
K 
proxk;g(k 1 k(k 1;k))
; (6.22)
wherekis a random variable with k 1distribution. Moreover, by fk;n2Ngwe denote
the gradient perturbation sequence deﬁned by
k= (k 1;k) r`(k 1):
Moreover, for any measurable function W:X![1;+1)recall the deﬁnitions of
kkWandjfjWgiven in (6.9) and (6.10). Then we deﬁne W-variation of the kernels M
andM0by
DW(;0) = sup
XkM(X;) M0(X;)kW
W(X):
Theorem 6.7. Denote
S=f2K: 02rf() +@g() NK()g;
where@gis a subgradient of g:B!R(see e.g. Rockafellar (1970)). Suppose that the set
(f+g)(S)has empty interior and sup
k2Nkkk1. We also make the following assumptions.
(1) The function gis convex, Lipschitz and bounded from below.
(2) The sequence of step sizes fkgsatisﬁesk>0andlimk!1k= 0and
1X
k=1k=1;1X
k=1jk k 1j<1;1X
k=12
k<1:
(3) There exist constants 2[0;1)andb <1and a measurable function W:X!
[1;+1)such that
sup
2Kj(;)jW1=2<1;sup
2KMWW+b:
In addition, for any l2(0;1]there exists C <1and2(0;1)such that for
anyX2X
sup
2KkMn
(X;) kWlCnWl(X):
(4) The kernels Mand the stationary distributions are locally Lipschitz with respect
to, i.e. for any compact set Kand any;02Kthere exists C <1such that
sup
2Kk(;) (0;)kW1=2+DW1=2(;0)Ck 0k:
(5)E[W(1)]<1.
Then the sequence fxk;k2Nggenerated by iterations (6.22)converges toS.
112Proof of Theorem 6.1. For better transparency of the proof we will use m= 1in (6.4),
the generalization of the reasoning to the case of m > 1is straightforward. In our case
the role of the function fplays the negative log-likelihood `()and the function gis the
`1-penalty. Both functions satisfy the assumptions of Theorem 6.7.
Then, by the formula (5.14) for the gradient of the negative log-likelihood function
and the Fisher identity in (6.3) the function (k;X)in our case takes the form
(k;X) =X
w2VX
c2X wX
s6=s0
 nw(c;s;s0) +tw(c;s) exp(w
k;s;s0>Zw(c))
Zw(c);(6.23)
where we take kfrom thek-th iteration of the p-SPGD algorithm as the parameter
vector. Other components such as nw(c;s;s0),tw(c;s)andZ(c)correspond to a single
trajectoryXof the Markov jump process. Integrating this function over all possible
trajectories with respect to =p(YjX)gives us the desired gradient r`()of
the negative log-likelihood.
In place of the function Win the assumptions of Theorem 6.7 we take the function V2.
Note that the original Theorem 5.4 of Majewski et al. (2018) on the convergence of
the algorithm does not use the function W, instead it has an additional assumption on
the gradient perturbation sequence
k= (k 1;Xk) r`(k 1); k2N:
That assumption states that the sequence fk;k2Ngcan be decomposed as k=e
k+r
k,
wherefe
k;k2Ngandfr
k;k2Ngare two sequences satisfying limk!1kr
kk= 0and the
seriesP1
k=1ke
kconverges. However, Proposition 5.5 in Majewski et al. (2018) implies
that by introducing Assumptions (3)–(5) we obtain the required decomposition of k.
Therefore let us check the rest of the assumptions of Theorem 6.7.
Assumption(2)onstep-sizesisautomaticallysatisﬁed. FirstwereviewAssumption(3)
withW=V2, which consists of three conditions. The ﬁrst condition that sup
k2Kj(k;)jV
is bounded is easy to check because for any trajectory Xthe sum of the terms nw(c;s;s0)
is bounded by the total number of jumps V(X), the sum of the terms tw(c;s)is bounded
by the total observation time Tand vectors kcome from the compact set K, which
means that exponent is bounded by some constant. The second condition follows directly
from Lemma 6.3. The last condition representing geometric ergodicity was shown in
Lemma 6.6.
In our setting Assumption (4) takes the form
DV(;0) +j^(;) ^(0;)jVCk 0k
for some constant C. We obtain it by combining Lemma 6.5 and the trivial fact that
j(;) (0;)jVCk 0kfor some constant C.
InthecourseoftheproofofthementionedabovedecompositionMajewskietal.(2018)
used the following property of the function W, which needs to be checked as well. For any
trajectorykunder the assumption EW(0)<1there holds sup
k1E[W(k)]<1. In our
113case we can obtain the same property. Assuming EV2(X0)<1by Lemma 6.4 we have
that sup
k1E[V2(Xk)]<1:This concludes the proof of the theorem.
6.4 Numerical results
In this section we describe the details of implementation of the proposed algorithm as
well as the results of experimental studies.
6.4.1 Details of implementation
We provide in details implementation of the proposed algorithm in practice. Recall
that the optimization problem (6.1) is solved by the iterative algorithm called projected
stochastic proximal gradient descent given in (6.2). Instead of the gradient of the ne-
gative log-likelihood r`()we use its MCMC approximation (;X1;:::;Xm), where
X1;:::;Xmis a set of trajectories generated by Rao and Teh’s scheme given in Sec-
tion 6.2. The solution of (6.1) depends on the choice of . As we mentioned in previous
chapters, ﬁnding the „optimal” parameter and the threshold is diﬃcult in practice. In
this case we also solve it using the same information criteria as in Chapter 5, where again
instead of the gradient of the negative log-likelihood we use its MCMC approximation.
The function (;X1;:::;Xm)is an average of the functions (;Xi)introduced
in (6.23) (recall that we use the symbol only for the true parameter vector and 
otherwise). Now, in the analogous way as we divided the optimization problem (5.5) in
Subsection 5.4.1 we can divide the current one. Namely, for ﬁxed w2Vands;s02f0;1g
withs6=s0, the corresponding summand in (;X1;:::;Xm)is a function which depends
on the vector restricted only to its coordinate vector w
s;s0(see notation (5.1)). So, for
each triple wands6=s0we can solve the problem separately. Let us denote these
summands of (;X1;:::;Xm)asw
s;s0(w
s;s0).
Hence, in the current implementation we can use the scheme from Subsection 5.4.1.
Namely, we start with computing a sequence of minimizers on the grid, i.e. for any triple
w2V,s6=s0wecreateaﬁnitesequence figN
i=1uniformlyspacedonthelogscale, starting
from the largest i, which corresponds to the empty model. Next, for each value iwe
compute the estimator ^w
s;s0[i]of the vector w
s;s0
^w
s;s0[i] = argmin
w
s;s0
w
s;s0(w
s;s0) +ikw
s;s0k1	
: (6.24)
The notation ^w
s;s0[i]means thei-th approximation of w
s;s0. To solve (6.24) numerically
for a given iwe use the SPGD algorithm without the projection onto the compact set.
In practice, the algorithm still converges well so we did not use the projection. The ﬁnal
LASSOestimator ^w
s;s0:=^w
s;s0[i]ischosenusingtheBayesianInformationCriterion(BIC)
applied to the MCMC approximation of the gradient of the negative log-likelihood, i.e.
i= argmin
1iNn
nw
s;s0(w
s;s0)(^w
s;s0[i]) + log(n)k^w
s;s0[i]k0o
:
114Herek^w
s;s0[i]k0denotes the number of non-zero elements of ^w
s;s0[i]andnis the number of
jumps in the trajectory generated by Rao and Teh’s algorithm. In our simulations we use
N= 100.
Finally, the threshold is obtained using the Generalized Information Criterion (GIC)
as in Subsection 5.4.1, also applied to the MCMC approximation of the gradient of the ne-
gative log-likelihood. For a prespeciﬁed sequence of thresholds Dwe calculate
= argmin
2Dn
nw
s;s0(^w;
s;s0) + log(2d(d 1))k^w;
s;s0k0o
;
where ^w;
s;s0is the LASSO estimator ^w
s;s0after thresholding with the level :
6.4.2 Simulated data
We consider the chain model analogous to the model M1in Subsection 5.4.2. All vertices
have the “chain structure”, i.e. for any node, except for the ﬁrst one, its set of parents
contains only a previous node. Namely, we put V=f1;:::;dgandpa(k) =fk 1g, if
k>1andpa(1) =;. We construct CIM in the same way as in Subsection 5.4.2. Namely,
for the ﬁrst node the intensities of leaving both states are equal to 5. For the rest of
the nodesk= 2;:::;d, we choose randomly a2f0;1gand we deﬁne Qk(c;s;s0) = 9;if
s6=jc ajand1otherwise. In other words, we choose randomly whether the node prefers
to be at the same state as its parent ( a= 0) or not (a= 1).
We consider two cases with the number of nodes equal to d= 5andd= 10. So,
the considered number of possible parameters of the model (the size of ) is2d2= 50
or200, respectively. We use T= 10for 5 nodes and T= 20for 10 nodes. We replicate
simulations 100times for each scenario. As the partial observation we take 100, 200 and
400 equally spaced points for 5 nodes and 200, 400 and 800 for 10 nodes. In Figure 6.1
we present averaged results of the simulations in terms of three quality measures
•power, which is a proportion of correctly selected edges;
•false discovery rate (FDR) , which is a fraction of incorrectly selected edges
among all selected edges;
•true model (TM) , which is an indicator whether the algorithm selected the true
model without any errors.
In Figure 6.2 we provide the results of simulations for the same models in case of com-
plete trajectories. We observe that the results of experiments conﬁrm that the proposed
method works in a satisfactory way. We observe that with increasing number of observa-
tion points results are close to the ones in case of complete data. The larger the number
of points the higher the power of the algorithm and tends to 1. The FDR is quite low in
all cases. For the half simulations in case of 10 nodes and the time T= 20the algorithm
discovers the true model when we choose a big enough number of observation points.
115FDR TM Power
10 20 30 400.10.20.30.4
0.00.10.20.30.40.5
0.60.70.80.91.0
Number of observation on interval [0,1]Number of nodes
5 (Time =10)
10 (Time =20)Figure 6.1: Results of simulations for partially observed data.
FDR TM Power
20 40 60 800.0500.0750.1000.125
0.00.20.40.6
0.9250.9500.9751.000
Observation timeNumber of nodes
5
10
20
Figure 6.2: Results of simulations for fully observed data.
1166.5 FFBS Algorithm
For completeness of the proposed scheme we provide the description of the forward-
ﬁltering backward-sampling algorithm for discrete-time Markov chains taken from Rao
and Teh (2013) with a slightly changed notation. Earlier references for the FFBS algo-
rithm can be found there as well.
Let(S0;:::;Sn)be a discrete-time Markov chain with a discrete state space X=
f1;:::;Ng. LetPbe a transition matrix P(s;s0) =p(Sj+1=s0jSj=s). Letbe
an initial distribution over states at time point 0and letY= (Y0;:::;Yn)be a sequence
of noisy observations with likelihoods gj(s) =p(YjjSjt=s). Given a set of observations
Y= (Y0;:::;Yn), FFBS returns an independent posterior sample of the state vector.
Deﬁneaj(s) =p(Y0;:::;Yj 1;Sj=s). From the Markov property, we have the fol-
lowing recursion:
aj+1(s0) =X
saj(s)gj(s)P(s;s0):
We calculate this for all possible states s02Xperforming a forward pass. At the end of
the forward pass we obtain the distribution
bn(s) =gn(s)an(s) =p(Y;Sn=s)/p(Sn=sjY)
and sampleSnfrom it. Next, note that
p(Sj=sjSj+1=s0;Y)/p(Sj=s;Sj+1=s0;Y) =
=aj(s)gj(s)P(s;s0)p(Yj+1;:::;YnjSj+1=s0)/
/aj(s)gj(s)P(s;s0);
where the second equality follows from the Markov property. This is also an easy distribu-
tion to sample from, and the backward pass of FFBS successively samples new elements
of Markov chain from Sn 1toS0. The pseudocode for the algorithm is given below.
117Algorithm 1: The forward-ﬁltering backward-sampling algorithm
Input:An initial distribution over states , a transition matrix P, a sequence of
noisy observations Y= (Y0;:::;Yn)with likelihoods
gj(s) =p(YjjSjt=s).
Output: A realization of the Markov chain (S0;:::;Sn)
Initializea0(s) =(s). ;
forj= 0ton 1
aj+1(s0) =P
saj(s)gj(s)P(s;s0)fors02X:;
SampleSnbn(), wherebn=gn(s)an(s).
forj=n 1to0
Deﬁnebj(s) =aj(s)gj(s)P(s;Sj+1);;
SampleSjbj().;
return (S0;:::;Sn)
118Chapter 7
Conclusions and discussion
In this thesis we explored two types of probabilistic graphical models (PGM): Bayesian
networks (BN) and continuous time Bayesian networks (CTBN). First, we explained
the concept of PGMs and the motivation to study them with a few examples of suc-
cessful applications. Then, we discussed more thoroughly PGMs of interest describing
the problems within both frameworks and provided necessary preliminaries. In terms of
contributions we were focused on structure learning, which is one of the most challenging
tasks in the process of exploring PGMs and is interesting in itself. We also discussed other
types of problems and reviewed some previously known results concerning these problems
to provide some context.
The problem of structure learning for BNs is diﬃcult due to the superexponential
growth of the space of directed acyclic graphs (DAG) with the number of variables and
also because the underlying graph needs to be acyclic. We solve this problem by dividing
it into two tasks. First, we use a known method called partition MCMC to slice the set of
variables into layers where any variable in any layer can have parents only from the pre-
vious layers and has at least one parent from the previous adjacent layer. Second, we ﬁnd
the arrows using the knowledge about the layers. In the case of continuous data we use
the assumption that our network is a Gaussian Bayesian network and hence each variable
is a linear combination of its parents. Thus, we solve the problem of ﬁnding arrows by
ﬁnding the non-zero coeﬃcients in the linear combination of all the variables from previ-
ous layers using Thresholded LASSO estimator. In the case of discrete and binary data
we use the assumption that probability of each variable being equal to 1 is a sigmoid func-
tion of a linear combination of its parents. Hence, again we solve the problem of ﬁnding
arrows by ﬁnding the non-zero coeﬃcients in the linear combination of all the variables
from previous layers using Thresholded LASSO estimator for logistic regression. Finally,
for the discrete data where each variable has a ﬁnite state space we use a softmax func-
tion instead of the sigmoid function. We demonstrated theoretical consistency of LASSO
and Thresholded LASSO estimators for the continuous model and showed their eﬀec-
tiveness on the benchmark Bayesian networks of diﬀerent sizes and structure comparing
the proposed method to several existing methods for structure learning.
The problem of structure learning for CTBNs in the case of complete data is also
119reduced to solving the optimizational problem for the penalized with `1-penalty maxi-
mum likelihood function. We assumed that a conditional intensity of a variable is a linear
function of the states of its parents, which can be easily extended to a polynomial depen-
dence. Starting from the full graph we remove irrelevant edges and estimate parameters
for existing ones simultaneously in case of LASSO estimator. In case of thresholded ver-
sion of this LASSO estimator we only learn the structure. We proved the consistency of
the proposed estimators and demonstrated coherence of theoretical results with numerical
results from simulated data.
The last problem considered in the thesis was structure learning for CTBNs in the case
of incomplete data. The optimizational problem takes the same form as for complete
data but we cannot write the likelihood function explicitly anymore. Instead of the nega-
tive log-likelihood function we used its Markov chain Monte Carlo approximation, where
Markov chain was generated using Rao and Teh’s algorithm. The optimizational problem
itself was solved by projected stochastic proximal gradient descent algorithm. We proved
the convergence of this algorithm to the set of stationary points of the minimized func-
tion. We used the same assumption on conditional intensities as in the case of complete
data. In practice to discover the arrows we used the thresholded version of the obtained
estimator. We showed on a small simulated example that the quality of the proposed
method is similar to the case of complete data and increases with the number of observed
points per interval.
As for the future research we want to obtain similar theoretical results for Bayesian
networksinthecaseofdiscretedataaswehaveobtainedforthecontinuousdata. Infuture
we intend to perform more experiments and comparisons with existing approaches for all
proposed methods. For some methods there are no open implementations or there are
implementations in diﬀerent programming languages, which makes it diﬃcult to perform
the comparison. The main goal was to show theoretical value of the proposed methods
and show that the results of experiments are consistent with theory, which in our opinion
was achieved.
120Bibliography
Andersen, P. K. and Gill, R. D. (1982). Cox’s regression model for counting processes: A
large sample study. Ann. Statist. , 10:1100–1120.
Baraniuk, R., Duarte, M., and Hegde, C. (2011). Introduction to compressive sensing.
Connexions e-textbook .
Bass, R. F. (2011). Stochastic Processes . Cambridge Series in Statistical and Probabilistic
Mathematics. Cambridge University Press.
BathlaTaneja,S., Douglas,G., Cooper, G.,Michaels, M.,Druzdzel, M.,andVisweswaran,
S. (2021). Bayesian network models with decision tree analysis for management of
childhood malaria in Malawi. BMC Medical Informatics and Decision Making , 21.
Beck, A. and Teboulle, M. (2009). A fast iterative shrinkage-thresholding algorithm for
linear inverse problems. SIAM Journal on Imaging Sciences , 2:183–202.
Bickel, P. J., Ritov, Y., and Tsybakov, A. B. (2009). Simultaneous analysis of Lasso and
Dantzig selector. The Annals of Statistics , 37:1705–1732.
Boudali, H. and Dugan, J. B. (2006). A continuous-time Bayesian network reliability
modeling, and analysis framework. IEEE transactions on reliability , 55(1):86–97.
Bühlmann, P. and van de Geer, S. (2011). Statistics for high-dimensional data: methods,
theory and applications . Springer Series in Statistics, New York: Springer.
Casella, G. and George, E. I. (1992). Explaining the Gibbs sampler. The American
Statistician , 46(3):167–174.
Chen, X. and Xuan, J. (2020). Bayesian inference of gene regulatory network. In Tang,
N., editor, Bayesian Inference on Complicated Data , chapter 5. IntechOpen, Rijeka.
Chung, K.andWalsh, J.(2005). Markov processes, Brownian motion, and time symmetry.
2nd ed. Springer New York, NY.
Colombo, D. and Maathuis, M. H. (2014). Order-independent constraint-based causal
structure learning. J. Mach. Learn. Res. , 15(1):3741–3782.
Cooper, G. F. (1990). The computational complexity of probabilistic inference using
Bayesian belief networks. Artiﬁcial Intelligence , 42(2):393–405.
121Daly, R., Shen, Q., and Aitken, S. (2011). Learning Bayesian networks: approaches and
issues.The Knowledge Engineering Review , 26(2):99–157.
Dempster, A. P., Laird, N. M., and Rubin, D. B. (1977). Maximum likelihood from
incomplete data via the EM algorithm. Journal of the Royal Statistical Society: Series
B (Methodological) , 39(1):1–22.
Douc, R., Moulines, E., and Stoﬀer, D. (2014). Nonlinear time series. Theory, methods
and applications with R examples . CRC Press.
Eaton, D. and Murphy, K. (2007). Bayesian structure learning using dynamic program-
ming and MCMC. UAI.
Fan, Y. and Shelton, C. R. (2012). Learning continuous-time social network dynamics.
arXiv:1205.2648 .
Fort, G., Moulines, E., and Priouret, P. (2011). Convergence of adaptive and interacting
Markov chain Monte Carlo algorithms. Ann. Statist. , 39:3262–3289.
Frey, B. and Jojic, N. (2005). A comparison of algorithms for inference and learning in
probabilistic graphical models. IEEE Transactions on Pattern Analysis and Machine
Intelligence , 27(9):1392–1416.
Friedman, N. and Koller, D. (2001). Being Bayesian about network structure: A bayesian
approach to structure discovery in bayesian networks. Mach Learn , 50.
Gasse, M., Aussem, A., and Elghazel, H. (2014). A hybrid algorithm for Bayesian net-
work structure learning with application to multi-label learning. Expert Syst. Appl. ,
41(15):6755–6772.
Gatti, E., Luciani, D., and Stella, F. (2012). A continuous time Bayesian network model
for cardiogenic heart failure. Flexible Services and Manufacturing Journal , 24(4):496–
515.
Gelman, A. and Shirley, K. (2012). Inference from simulations and monitoring conver-
gence.Handbook of Markov Chain Monte Carlo .
Geman, S. and Geman, D. (1984). Stochastic relaxation, Gibbs distributions, and the
Bayesian restoration of images. IEEE Transactions on Pattern Analysis and Machine
Intelligence , PAMI-6(6):721–741.
Geyer, C. (2011). Introduction to Markov Chain Monte Carlo , pages 3–48. CRC Press.
Giudici, P. and Castelo, R. (2003). Improving Markov Chain Monte Carlo model search
for data mining. Machine Learning , 50:127–158.
122Grzegorczyk, M. and Husmeier, D. (2008). Improving the structure MCMC sampler for
Bayesian networks by introducing a new edge reversal move. Machine Learning , 71(2-
3):265–305.
Gupta, A., Slater, J., Boyne, D., Mitsakakis, N., Beliveau, A., Druzdzel, M., Brenner, D.,
Hussain, S., and Arora, P. (2019). Probabilistic graphical modeling for estimating risk
of coronary artery disease: Applications of a ﬂexible machine-learning method. Medical
Decision Making , 39:1032–1044.
Hastie, T., Tibshirani, R., and Wainwright, M. (2015). Statistical Learning with Sparsity:
The Lasso and Generalizations . Chapman & Hall/CRC.
Hastings, W. K. (1970). Monte Carlo sampling methods using Markov chains and their
applications. Biometrika , 57(1):97–109.
Heckerman, D. (2021). A tutorial on learning with Bayesian networks. arXiv 2002.00269 .
Huang, J., Sun, T., Ying, Z., Yu, Y., and Zhang, C.-H. (2013). Oracle inequalities for the
lasso in the Cox model. Annals of statistics , 41(3):1142–1165.
Huang, J. and Zhang, C.-H. (2012). Estimation and selection via absolute penalized
convex minimization and its multistage adaptive applications. Journal of Machine
Learning Research , 13:1839–1864.
Jacod, J. and Shiryaev, A. N. (2003). Limit Theorems for Stochastic Processes . Springer
Berlin Heidelberg.
Jorge, P., Abrantes, A., Lemos, J., and Marques, J. (2007). Long term tracking of
pedestrians with groups and occlusions. Bayesian Network Technologies: Applications
and Graphical Models , pages 151–175.
Jorge, P., Abrantes, A., and Marques, J. (2004). On-line object tracking with Bayesian
Networks. https://www.researchgate.net/publication/251372022 .
Koivisto, M. and Sood, K. (2004). Exact Bayesian structure discovery in Bayesian net-
works.J. Mach. Learn. Res. , 5:549–573.
Koller, D. and Friedman, N. (2009). Probabilistic Graphical Models: Principles and Tech-
niques. Adaptive computation and machine learning. MIT Press.
Komodakis, N., Paragios, N., and Tziritas, G. (2007). MRF optimization via dual decom-
position: Message-passing revisited. In 2007 IEEE 11th International Conference on
Computer Vision , pages 1–8.
Kuipers, J. and Moﬀa, G. (2017). Partition MCMC for inference on acyclic digraphs.
Journal of the American Statistical Association , 112(517):282–299.
123Lauritzen, S. L. and Spiegelhalter, D. J. (1988). Local computations with probabilities
on graphical structures and their application to expert systems. Journal of the Royal
Statistical Society. Series B (Methodological) , 50(2):157–224.
Lezaud, P. (1998). Chernoﬀ-type bound for ﬁnite Markov chains. The Annals of Applied
Probability , 8(3):849–867.
MacKay, D. J. C. (2003). Information Theory, Inference, and Learning Algorithms . Copy-
right Cambridge University Press.
Madigan, D., York, J., and Allard, D. (1995). Bayesian graphical models for discrete data.
International Statistical Review / Revue Internationale de Statistique , 63(2):215–232.
Majewski, S., Miasojedow, B., and Moulines, E. (2018). Analysis of nonsmooth stochastic
approximation: the diﬀerential inclusion approach. arXiv: 1805.01916v1 .
Meng, X.-L. and Rubin, D. B. (1991). Using EM to obtain asymptotic variance-covariance
matrices: The SEM algorithm. Journal of the American Statistical Association ,
86(416):899–909.
Miasojedow, B.andNiemiro, W.(2017). GeometricergodicityofRaoandTeh’salgorithm
for Markov jump processes and CTBNs. Electronic Journal of Statistics , 11(2):4629–
4648.
Miasojedow, B. and Rejchel, W. (2018). Sparse estimation in Ising model via penalized
Monte Carlo methods. Journal of Machine Learning Research , 19(75):1–26.
Minka, T. P. (2001). Expectation propagation for approximate Bayesian inference. In
Proceedings of the 17th Conference in Uncertainty in Artiﬁcial Intelligence , UAI ’01,
pages 362–369, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.
Negahban, S., Yu, B., Wainwright, M. J., and Ravikumar, P. K. (2009). A uniﬁed frame-
work for high-dimensional analysis of M-estimators with decomposable regularizers. In
Advances in Neural Information Processing Systems , pages 1348–1356.
Nodelman, U. (2007). Continuous Time Bayesian Networks . PhD thesis, Department of
Computer Science, Stanford University.
Nodelman, U. and Horvitz, E. (2004). Continuous time Bayesian networks for in-
ferring users’ presence and activities with extensions for modeling and evaluation.
https://www.researchgate.net/publication/228686433 .
Nodelman, U., Koller, D., andShelton, C. (2005). Expectation propagationfor continuous
time Bayesian networks. In Proceedings of the Twenty-ﬁrst Conference on Uncertainty
in AI (UAI) , pages 431–440, Edinburgh, Scottland, UK.
124Nodelman, U., Shelton, C., and Koller, D. (2002). Continuous time Bayesian networks.
InProceedings of the Eighteenth Conference on Uncertainty in Artiﬁcial Intelligence
(UAI), pages 378–387.
Nodelman, U., Shelton, C. R., and Koller, D. (2012). Expectation Maximization and com-
plex duration distributions for continuous time Bayesian networks. arXiv 1207.1402 .
Opgen-Rhein, R. and Strimmer, K. (2007). From correlation to causation networks: a
simple approximate learning algorithm and its application to high-dimensional plant
gene expression data. BMC Systems Biology , 1(37).
Pearl, J. (1985). Bayesian networks: A model of self-activated memory for evidential
reasoning. In Proc. of Cognitive Science Society (CSS-7) .
Pearl, J. (2000). Causality: Models, Reasoning and Inference . Cambridge University
Press.
Pokarowski, P. and Mielniczuk, J. (2015). Combined l1and greedy l0penalized least
squares for linear model selection. J. Mach. Learn. Res. , 16:961–992.
Protter, P. E. (2005). Stochastic Integration and Diﬀerential Equations . Springer Berlin
Heidelberg.
Rabiner, L. and Juang, B. (1986). An introduction to hidden Markov models. IEEE
ASSP Magazine , 3(1):4–16.
Raftery, A. E. (1995). Bayesian model selection in social research. Sociological Methodol-
ogy, 25:111–163.
Rao, V. and Teh, Y. W. (2013). Fast MCMC sampling for Markov jump processes and
extensions. Journal of Machine Learning Research , 14:3207–3232.
Roberts, G. O. and Rosenthal, J. S. (2004). General state space Markov chains and
MCMC algorithms. Probability Surveys , 1:20–71.
Rockafellar, R. T. (1970). Convex analysis . Princeton Mathematical Series. Princeton
University Press, Princeton, N. J.
Russell, S. and Norvig, P. (2010). Artiﬁcial Intelligence: A Modern Approach . Prentice
Hall, 3 edition.
Sachs, K., Perez, O., Pe’er, D., Lauﬀenburger, D. A., and Nolan, G. P. (2005). Causal
protein-signaling networks derived from multiparameter single-cell data. Science,
308(5721):523–529.
Schäfer, J. and Strimmer, K. (2005). A shrinkage approach to large-scale covariance
matrix estimation and implications for functional genomics. Statistical Applications in
Genetics and Molecular Biology , 4(1).
125Scutari, M. (2010). Learning Bayesian networks with the bnlearn R package. Journal of
Statistical Software , 35(3):1–22.
Scutari, M., Graaﬂand, C. E., and Gutiérrez, J. M. (2018). Who learns better
Bayesian network structures: Accuracy and speed of structure learning algorithms.
arXiv:1205.2648 .
Sontag, D. and Jaakkola, T. (2007). New outer bounds on the marginal polytope. In
Platt, J., Koller, D., Singer, Y., andRoweis, S., editors, Advances in Neural Information
Processing Systems , volume 20. Curran Associates, Inc.
Spiegelhalter, D. J. and Lauritzen, S. L. (1990). Sequential updating of conditional prob-
abilities on directed graphical structures. Networks , 20(5):579–605.
Spirtes, P.andGlymour, C.(1991). Analgorithmforfastrecoveryofsparsecausalgraphs.
Social Science Computer Review - SOC SCI COMPUT REV , 9:62–72.
Spirtes, P., Glymour, C., and Scheines, R. (2000). Causation, Prediction, and Search,
2nd Edition . Springer New York, NY.
Stella, F., Acerbi, E., Vigano, E., Poidinger, M., Mortellaro, A., and Zelante, T. (2016).
Continuous time Bayesian networks identify Prdm1 as a negative regulator of TH17
cell diﬀerentiation in humans. Scientiﬁc Reports , 6.
Stella, F., Acerbi, E., Zelante, T., and Narang, V. (2014). Gene network inference using
continuous time Bayesian networks: A comparative study and application to Th17 cell
diﬀerentiation. BMC Bioinformatics , 15.
Stella, F. and Amer, Y. (2012). Continuous time Bayesian network classiﬁers. Journal of
Biomedical Informatics , 45(6):1108–1119.
Thiesson, B. (1995). Accelerated quantiﬁcation of Bayesian networks with incomplete
data. In Proceedings of the First International Conference on Knowledge Discovery
and Data Mining , KDD’95, pages 306–311. AAAI Press.
Tibshirani, R. (1996). Regression shrinkage and selection via the LASSO. Journal of the
Royal Statistical Society: Series B (Methodological) , 58(1):267–288.
Tsamardinos, I., Brown, L. E., and Aliferis, C. F. (2006). The max-min hill-climbing
Bayesian network structure learning algorithm. Mach. Learn. , 65(1):31–78.
van de Geer, S. (2008). High-dimensional generalized linear models and the LASSO. The
Annals of Statistics , 36:614–645.
van de Geer, S. (2016). Estimation and Testing Under Sparsity: Cole d’t de Probabilits
de Saint-Flour XLV - 2015 . Springer Publishing Company, Incorporated, 1st edition.
126Villa, S. and Stella, F. (2018). Learning continuous time Bayesian networks in non-
stationary domains (extended abstract). In Proceedings of the 27th International Joint
Conference on Artiﬁcial Intelligence , IJCAI’18, pages 5656–5660. AAAI Press.
Wainwright,M.J.,Jaakkola,T.,andWillsky,A.S.(2005). MAPestimationviaagreement
on trees: message-passing and linear programming. IEEE Transactions on Information
Theory, 51:3697–3717.
Wasyluk, H., Onisko, A., and Druzdzel, M. (2001). Support of diagnosis of liver disorders
based on a causal Bayesian network model. Medical science monitor : international
medical journal of experimental and clinical research , 7 Suppl 1:327–32.
Xu, J. and Shelton, C. R. (2008). Continuous time Bayesian networks for host level
network intrusion detection. In Joint European Conference on Machine Learning and
Knowledge Discovery in Databases , pages 613–627. Springer.
Xue, L., Zou, H., and Cai, T. (2012). Nonconcave penalized composite conditional likeli-
hood estimation of sparse Ising models. The Annals of Statistics , 40:1403–1429.
Yang, S., Khot, T., Kersting, K., and Natarajan, S. (2016). Learning continuous time
Bayesian networks in relational domains: A non-parametric approach. In AAAI.
Ye, F. and Zhang, C.-H. (2010). Rate Minimaxity of the Lasso and Dantzig Selector for
thelqloss inlrBalls.Journal of Machine Learning Research , 11:3519–3540.
Yedidia, J., Freeman, W., and Weiss, Y. (2001). Generalized belief propagation. Advances
in Neural Information Processing Systems 13 , 13.
127