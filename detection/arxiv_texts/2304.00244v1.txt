arXiv:2304.00244v1  [math.OC]  1 Apr 2023An active-set based recursive approach for solving
convex isotonic regression with generalized order
restrictions
Xuyu Chen∗, Xudong Li†, Yangfeng Su‡
April 4, 2023
Abstract
This paper studies the convex isotonic regression with generalized o rder restrictions induced
by a directed tree. The proposed model covers various intriguing o ptimization problems with
shape or order restrictions, including the generalized nearly isoton ic optimization and the total
variationonatree. Inspiredbythe successofthepool-adjacent -violatoralgorithmanditsactive-
set interpretation, we propose an active-set based recursive ap proach for solving the underlying
model. Unlike the brute-force approach that traversesan expon ential number of possible active-
set combinations, our algorithm has a polynomial time computational complexity under mild
assumptions.
Keywords: Active set methods; convex isotonic regression; generaliz ed order restrictions
AMS subject classiﬁcations: 90C25, 90C30
1 Introduction
Given a directed tree G= (V,E), we consider the following convex isotonic regression pro blem with
generalized order restrictions:
min
x∈ℜ|V|/summationdisplay
i∈Vfi(xi)+/summationdisplay
(i,j)∈Eλi,j(xi−xj)++/summationdisplay
(i,j)∈Eµi,j(xj−xi)+, (1)
where for each i∈V, fi:ℜ → ℜis a convex loss function, λi,jandµi,jfor (i,j)∈E,are possibly
inﬁnite nonnegative scalars, i.e., 0 ≤λi,j,µi,j≤+∞, and (x)+= max(0 ,x) is the nonnegative
part ofxfor anyx∈ ℜ. In (1), when λi,j= +∞(respectively, µi,j= +∞), the corresponding
termλi,j(xi−xj)+(respectively, µi,j(xj−xi)+) should be understood as the indicator function
δ(xi,xj|xi−xj≤0) (respectively, δ(xi,xj|xi−xj≥0)), or equivalently the constraint xi−xj≤0
(respectively, xi−xj≥0). See Figure 1 for some simple examples of directed trees.
As one can observe, the involvement of the directed tree Gmakes problem (1) a rather general
model containing many interesting variants as special case s. Here, for simplicity, we only mention
∗School of Mathematical Sciences, Fudan University, Shangh ai, 200433, China, chenxy18@fudan.edu.cn
†School of Data Science, Fudan University, Shanghai, 200433 , China, lixudong@fudan.edu.cn
‡School of Mathematical Sciences, Fudan University, Shangh ai, 200433, China, yfsu@fudan.edu.cn
112···n
(a) chain1
2 3
4 56
(b) arborescence1
2 3
45
(c) general directed tree
Figure 1: Examples of directed trees. A directed tree is a dir ected graph whose underlying graph
is a tree, and the directed trees are also referred to as direc ted acyclic graphs.
two of them. Theﬁrstone is the generalized nearly isotonic optimization (GNIO) problem proposed
in [26]:
min
x∈ℜnn/summationdisplay
i=1fi(xi)+n−1/summationdisplay
i=1λi(xi−xi+1)++n−1/summationdisplay
i=1µi(xi+1−xi)+, (2)
which is clearly a special case of (1) with Gchosen as a chain, as illustrated in Figure 1a. As is
mentioned in [26], model (2) recovers, as special cases, man y classic problems in shape restricted
statistical regression, including isotonic regression [6 , 7], unimodal regression [12, 21], and nearly
isotonic regression [22]. The second one is the total variation on a tree considered in [15]:
min
x∈ℜ|V|/summationdisplay
i∈Vfi(xi)+/summationdisplay
(i,j)∈Ewi,j|xi−xj|, (3)
whereG= (V,E) is a directed tree and each fiis assumed to be piecewise linear or piecewise
quadratic. Other special cases of model (1) have also been ex amined in the literature, for example,
[8, 25] studied the isotonic regression problems with parti al order restrictions induced by an ar-
borescence. These special cases, as well as their applicati ons in statistic inference [20], operations
research [1], signal processing [17, 9], medical prognosis [19], and traﬃc and climate data analysis
[16, 24], reveal the importance and necessity of studying mo del (1).
To the best of our knowledge, there is currently no eﬃcient al gorithm available for directly
solving the general model (1). However, certain special cas es of the model can be solved by existing
algorithms. For example, the GNIO problem (2) can be eﬃcient ly solved by employing a dynamic
programming approach designed in [26]. Moreover, assuming boundednessof the decision variables,
the KKT based fast algorithm proposed in [13] can also solve t he GNIO problem. However, both
algorithms rely heavily on the underlying chain structure, and therefore cannot be applied to solve
the general model (1) that involves a directed tree. If Gis a chain and each fiis quadratic,
the total variation problem (3) reduces to the well-known ℓ2total variation denoising problem,
which has been extensively studied in signal processing [10 , 14]. The direct algorithm [10] and the
taut-string algorithms [2] are considered to be the state-o f-the-art for solving the ℓ2total variation
denoising problem. Meanwhile, if Gis assumed to be a directed tree and each fiis assumed to
be continuous piecewise linear or piecewise quadratic with a ﬁnite number of breakpoints in (3),
the message passing algorithm studied in [15] can be applied . However, these algorithms can not
handle problem (1) with general convex loss functions fiinvolved.
There is also another line of work dedicated to solving speci al cases of problem (1). In the
1950s, Ayer in [1] proposed the famous Pool-Adjacent-Violator algorithm (PAVA) for solving the
2following isotonic regression problem:
min
x∈ℜn1
2n/summationdisplay
i=1(xi−yi)2,
s.t.x1≤x2≤...≤xn,(4)
which is clearly a special case of problem (1). The PAVA has be en widely regarded as the state-of-
the-art technique for solving the isotonic regression prob lem since its inception. Later in [4], Best
and Chakravarti discovered that the PAVA is, in fact, a dual f easible active set method for solving
(4). In [5], the PAVA was generalized to handle (4) but with th e least squares objectives replaced
by general separable convex loss functions. In [25], Yu and X ing further generalized the PAVA to
solve convex separable minimization with order constraint s induced by an arborescence. However,
the generalized regularizers present in the objective of mo del (1) were not studied in [25]. As far
as we know, it remains unclear whether the ideas behind the PA VA can be adopted to solve the
more general model (1).
Encouraged by the successes of the PAVA and its variants in so lving special cases of the
generalized convex isotonic regression problem (1), we pro pose a novel active-set based algorithm in
thispaper. Ourapproachdiﬀersfromthebrute-forcemethodt hatexploresapotentiallyexponential
numberof diﬀerentactive sets. Instead, arecursiveapproac h isproposedtoaccelerate thesearch for
the desired active sets. We show that problem (1) can be tackl ed via recursively solving a sequence
of smaller subproblems. For these subproblems, special rec ursive structures of the corresponding
Karush-Kuhn-Tucker (KKT) conditions are carefully examin ed, which further allows us to design
a novel active-set based recursive approach (ASRA). In part icular, this approach enables us to
derive semi-closed formulas of the optimal solutions to the aforementioned recursive subproblems.
Under mild assumptions, we further show that the ASRA enjoys a polynomial time computational
complexity for solving problem (1).
The subsequent sections of this paper are organized as follo ws. Section 2 covers the necessary
preliminaries associated with problem (1), including fund amental concepts in graph theory and the
corresponding KKT conditions. In addition, we describe a na ive active-set method to solve (1).
Our recursive approach, the ASRA, is described in detail in S ection 3. Finally, we conclude the
paper in Section 4. The Appendix includes an example of how to apply the ASRA to solve a simple
instance of (1).
2 Preliminaries
We start with some relevant preliminaries in graph theory. A directed tree G= (V,E) is a directed
graph whose underlying graph is a tree, and an arborescence ( also known as rooted directed tree)
[11, 23] is a directed tree with exactly one node of zero in-de gree. The node is also referred to as
therootof the arborescence. Let G= (V,E) andB= (VB,EB) be two directed trees. If VB⊆V
andEB⊆E, then we say that Bis asubtreeofG, denoted by B⊂G. Two subtrees are disjoint
if their node sets are disjoint. Given P={Bk}K
k=1as a collection of disjoint subtrees of a certain
directed tree G= (V,E), ifV=∪K
k=1VBk, thenPis said to be a partition ofG.
For a given directed tree G= (V,E), we can choose any node l∈Vas theancestor ofG. Then,
for anyi,j∈V, we say that jis a child of i, denoted by j ⊳i, if the undirected path connecting l
andiis strictly contained in the one connecting landj. For example, if we pick the node 2 as the
3ancestor in the directed tree presented in Figure 1c, then we have 3⊳1⊳2. Now, let D∈ ℜ|V|×|E|
be the node-arc incidence matrix associated with G. We know from [3] that rank( D) =|E|and
the matrix ˜Dl∈ ℜ|E|×|E|obtained by deleting the l-th row from Dis invertible. Given a vector
b∈ ℜ|E|, we obtain in the following lemma a closed-form formula for t he solution to the linear
system˜Dlz=b.
Lemma 1. For any given b∈ ℜ|E|, the unique solution z∗= (zi,j)(i,j)∈E∈ ℜ|E|to the linear system
˜Dlz=btakes the following form:
z∗
i,j=

/summationdisplay
k∈Cibk,ifi⊳j,
−/summationdisplay
k∈Cjbk,ifj ⊳i,∀(i,j)∈E,
where for any node i,Ciconsists of iand all its children, i.e., Ci:={j∈V|j ⊳i}∪{i}.
Proof.This result is a simple consequence of the special structure of the node-arc incidence matrix
and can be veriﬁed directly.
Next, we state the blanket assumption on the loss functions fi,i∈V, and derive the KKT con-
ditions associated with problem (1). To express our main ide as clearly, we put strong assumptions
onfi, such as strong convexity and diﬀerentiability. However, as can be observed, these strong
assumptions could be removed if more subtle analysis is empl oyed.
Assumption 1. Eachfi:ℜ → ℜ,i∈Vin(1)is diﬀerentiable and strongly convex.
From the strong convexity of each fi, we know that the objective function in problem (1) is also
strongly convex and therefore level-set bounded. Moreover , by [18, Theorems 27.1 and 27.2],
problem (1) has a unique solution. We also note that Assumpti on 1 holds in some statistical and
machine learning problems [4, 10, 22]. Under Assumption 1, w e know from [18] that each f∗
iis also
a strongly convex diﬀerentiable function. Moreover, both f′
iand (f∗
i)′are strictly increasing on ℜ,
and for any given x,y∈ ℜ,y=f′
i(x) if and only if x= (f∗
i)′(y).
Now, we are ready to write down the KKT conditions associated with problem (1). For
0≤λ,µ≤+∞, let


h−
λ(x) :=δ(x|x≥0),ifλ= +∞,
h−
λ(x) :=/braceleftBigg
−λx, x < 0,
0, x≥0,if 0≤λ <+∞,and

h+
µ(x) :=δ(x|x≤0),ifµ= +∞,
h+
µ(x) :=/braceleftBigg
0, x≤0,
µx, x > 0,if 0≤µ <+∞.
For (i,j)∈E, we deﬁne hi,j:ℜ →[0,+∞] by
hi,j(x) :=h−
λi,j(x)+h+
µi,j(x),∀x∈ ℜ.
Clearly, for each ( i,j)∈E,hi,jis convex and its subdiﬀerential at x∈ ℜtakes the following form:
∂hi,j(x) =

{−λi,j},ifx <0,
[−λi,j,µi,j],ifx= 0,
{µi,j},ifx >0.(5)
4Here,∂hi,j(x) ={+∞}or∂hi,j(x) ={−∞}should be understood as ∂hi,j(x) =∅. We also adopt
the conventions in (5) that [ −∞,+∞] = (−∞,+∞), [−∞,α] = (−∞,α], and [α,+∞] = [α,+∞)
for some α∈ ℜ.
DeﬁneH(z) :=/summationtext
(i,j)∈Ehi,j(zi,j) forz∈ ℜ|E|, andF(x) :=/summationtext
i∈Vfi(xi) forx∈ ℜ|V|. Let
M=−DT∈ ℜ|E|×|V|, where Dis the node-arc incidence matrix associated with G. That is,
fore= (i,j)∈E,M(e,i) =−1 andM(e,j) = 1 and all other entries of Mare zero. Let
HM(x) :=H(Mx) forx∈ ℜ|V|. Then, it can be easily veriﬁed that problem (1) can be equiva lently
rewritten as
min
x∈ℜ|V|F(x)+HM(x).
Then, we have the following lemma on the KKT conditions assoc iated with problem (1).
Lemma 2. Problem (1)has a unique minimizer x∗∈ ℜ|V|. Moreover, x∗solves problem (1)if
and only if there exists a unique multiplier z∗∈ ℜ|E|, such that (x∗,z∗)satisﬁes the following KKT
system:/summationdisplay
k:(i,k)∈Ez∗
i,k−/summationdisplay
k:(k,i)∈Ez∗
k,i=f′
i(x∗
i),∀i∈V,
z∗
i,j∈

{−λi,j},ifx∗
i> x∗
j,
[−λi,j,µi,j],ifx∗
i=x∗
j,
{µi,j},ifx∗
i< x∗
j,∀(i,j)∈E.(6)
Proof.The existence and the uniqueness of the optimal solution to p roblem (1) follows from the
the strong convexity of F. SinceFis diﬀerentiable, we know from [18, Theorem 23.8] that
0∈F′(x∗)+∂HM(x∗).
From [18, Theorem 23.9], it can be seen that ∂HM(x∗) =MT∂H(Mx∗). Thus, there exists
z∗∈∂H(Mx∗), such that
F′(x∗)+MTz∗=F′(x∗)−Dz∗= 0. (7)
Since the e-th entry of Mx∗is given by x∗
j−x∗
i, we have from (5) that
z∗
i,j∈

{−λi,j},ifx∗
j−x∗
i<0,
[−λi,j,µi,j],ifx∗
j−x∗
i= 0,
{µi,j},ifx∗
j−x∗
i>0,∀(i,j)∈E.
Thus, we obtain the KKT conditions (6). The uniqueness of z∗follows from (7) and the fact that
rank(D) =|E|. We thus complete the proof.
Next, we investigate anaive active set methodfor solving pr oblem (1). For each edge ( i,j)∈E,
we can associate it with a sign # ∈ {<,=,>}to obtain a triple ( i,j,#) representing the relation
xi#xj. For the consistency, when dealing with edges ( i,j) withλi,j= +∞(orµi,j= +∞), the
corresponding sign # can only be chosen from {<,=}(or{>,=}). We denote by Athe collection
of all these triples and term it as an active set associated with problem (1). Then, the active set A
induces the following A-reduced problem from (1):
min
x∈ℜ|V|/summationtext
i∈Vfi(xi)+/summationtext
(i,j)∈A>λi,j(xi−xj)+/summationtext
(i,j)∈A<µi,j(xj−xi),
s.t. xi=xj,∀(i,j)∈ A=,(8)
5whereA#:={(i,j)|(i,j,#)∈ A}. IfA==∅, then (8) reduces to an unconstrained optimiza-
tion problem, which can be eﬃciently solved since its object ive function is separable, smooth and
strongly convex. For i,j∈V, we say they are A-connected if and only if there exists an undirected
path inA=, which is obtained by treating all edges in A=as undirected edges, that connects iand
j. LetPAbe the collection of all A-connected components of G. Then, it is not diﬃcult to observe
thatPAis naturally a partition of G. We thus term PAas thepartition induced by A. Without loss
of generality, assume PA={Bk}K
k=1with each Bkbeing a subtree of G, we see that the A-reduced
problem (8) can be decoupled into Kindependent subproblems as follows:
min
x∈ℜ|VBk|

/summationdisplay
i∈VBkˆfi(xi)|xi=xj,∀(i,j)∈EBk

,1≤k≤K, (9)
where for each i∈VBk,
ˆfi(xi) :=fi(xi)+(/summationdisplay
j:(i,j)∈A>λi,j−/summationdisplay
j:(i,j)∈A<µi,j)xi+(/summationdisplay
l:(l,i)∈A<µl,i−/summationdisplay
l:(l,i)∈A>λl,i)xi.
Clearly, the simple constraints in problem (9) can be elimin ated. The resulting unconstrained
optimization problem has a univariate smooth and strongly c onvex objective function and thus can
be eﬃciently solved. In this way, we obtain the optimal solut ion to the A-reduced problem (8).
Unfortunately, there can be up to 3|E|diﬀerent choices for the active set A. Thus, the naive
methodofexploringallthepossiblechoices ofdiﬀerentacti ve setsneedstosolveexponentialnumber
ofA-reduced problems. In order to reduce this prohibitive comp utational costs, we introduce a
novel active-set based recursive algorithm in the next sect ion.
3 An recursive algorithm for solving problem (1)
In this section, we present our recursive algorithm for solv ing problem (1). We ﬁrst claim that,
without loss of generality, the directed tree Gin (1) can be assumed to be an arborescence with
the node 1 to be its root. Moreover, we can decompose Ginto a sequence of subtrees {Gm=
(Vm,Em)}n
m=1, whereG1⊂G2⊂ ··· ⊂ Gn=GandVm={1,2,...,m}for 1≤m≤n, and the
set of edges Em+1\Emcontains exactly one edge ( im,m+1), where im∈Vm. Further details are
deferred to the Appendix.
For each 1 ≤m≤n, problem (1), when restricted to the the subtree Gm, takes the following
form:
min
x∈ℜ|Vm|/summationdisplay
i∈Vmfi(xi)+/summationdisplay
(i,j)∈Emλi,j(xi−xj)++/summationdisplay
(i,j)∈Emµi,j(xj−xi)+. (10)
From Lemma 2, it is not diﬃcult to see that the unique primal-d ual optimal pair to problem (10),
denote by ( x(m),z(m))∈ ℜ|Vm|×ℜ|Em|, satisﬁes the following KKT system:
/summationdisplay
k:(i,k)∈Emzi,k−/summationdisplay
k:(k,i)∈Emzk,i=f′
i(xi),∀i∈Vm,
zi,j∈

{−λi,j},ifxi> xj,
[−λi,j,µi,j],ifxi=xj,
{µi,j},ifxi< xj,∀(i,j)∈Em.(11)
6The unique optimal pair ( x(m),z(m)) is also referred to as the Gm-optimal pair for convenience.
By carefully exploiting the special structures in the KKT co nditions (11), we propose to solve
problem (1) in a recursive fashion. Speciﬁcally, we will rec ursively generate the Gm+1-optimal pair
(xm+1,z(m+1)) from the Gm-optimal pair ( x(m),z(m)) form= 1,...,n−1.
We summarize the detailed steps of the above recursive appro ach in Algorithm 1. In the
algorithm, the generate subroutine is designed to generate the Gm+1-optimal pair from the Gm-
optimal pair. In the next subsection, we will show that this p rocedure is accomplished via a novel
active-set searching scheme. Hence, it is natural for us to c all Algorithm 1 an active-set based
recursive approach (ASRA).
Algorithm 1 ASRA: An active-set based recursive approach for solving pr oblem (1)
1:Initialize: x(1)
1= (f∗
1)′(0)∈ ℜ, andz(1)=∅
2:form= 1,...,n−1do
3:(x(m+1),z(m+1)) =generate(x(m),z(m),Gm+1)
4:end for
5:Return: (x(n),z(n))∈ ℜn×ℜn−1
3.1 The generate subroutine
To eﬃciently obtain the Gm+1-optimal pair from the given Gm-optimal pair, we shall investigated
the KKT conditions associated with the subproblem induced b y the subtree Gm+1. Specially, it
takes the following form:
/summationdisplay
k:(i,k)∈Emzi,k−/summationdisplay
k:(k,i)∈Emzk,i=f′
i(xi),∀i∈Vm\{im}, (12)
zi,j∈

{−λi,j},ifxi> xj,
[−λi,j,µi,j],ifxi=xj,
{µi,j},ifxi< xj,∀(i,j)∈Em, (13)
/summationdisplay
k:(im,k)∈Emzim,k−/summationdisplay
k:(k,im)∈Emzk,im+zim,m+1=f′
im(xim), (14)
−zim,m+1=f′
m+1(xm+1), (15)
zim,m+1∈

{−λim,m+1},ifxim> xm+1,
[−λim,m+1,µi,j],ifxim=xm+1,
{µim,m+1},ifxim< xm+1.(16)
As one can observe, instead of writing the KKT conditions as a whole set of equations, we have
singled out those, namely (14), (15) and (16), associated wi th the dual variable zim,m+1, which
corresponds to the newly added edge {(im,m+ 1)}=Em+1\Em. Based on the above KKT
conditions, we have the following proposition regarding th e sign of zim,m+1.
Proposition 1. It holds that z(m+1)
im,m+1f′
m+1(x(m)
im)≤0, where(x(m),z(m))and(x(m+1),z(m+1))are
theGm-optimal pair and the Gm+1-optimal pair, respectively.
7Proof.Note that when f′
m+1(x(m)
im) = 0, the desired result naturally holds. For the remaining p arts,
we only prove the case where f′
m+1(x(m)
im)>0, since the proof for the case with f′
m+1(x(m)
im)<0 can
be easily modiﬁed from the arguments here.
Suppose that f′
m+1(x(m)
im)>0, then we shall prove that z(m+1)
im,m+1≤0. Assume on the contrary
thatz(m+1)
im,m+1>0. Then, from (15), we have x(m+1)
m+1= (f∗
m+1)′(−z(m+1)
im,m+1)<(f∗
m+1)′(0). Moreover,
(16) implies that x(m+1)
m+1≥x(m+1)
im. Thus, we have from the strict monotonicity of ( f∗
m+1)′the
following inequality:
x(m)
im>(f∗
m+1)′(0)> x(m+1)
m+1≥x(m+1)
im. (17)
Now, from (12), (13), and (14), we see that /tildewidex∈ ℜ|Vm|with/tildewidexi=x(m+1)
ifori∈Vmis the
optimal solution to the following optimization problem:
min
x∈ℜ|Vm|F1(x) :=/summationdisplay
i∈Vmfi(xi)+/summationdisplay
(i,j)∈Em{λi,j(xi−xj)++µi,j(xj−xi)+}−z(m+1)
im,m+1xim.
Meanwhile, since ( x(m),z(m)) is theGm-optimal pair, x(m)is the optimal solution to the following
optimization problem:
min
x∈ℜ|Vm|F0(x) :=/summationdisplay
i∈Vmfi(xi)+/summationdisplay
(i,j)∈Em{λi,j(xi−xj)++µi,j(xj−xi)+}.
Then, it holds that
0≥F1(/tildewidex)−F1(x(m)) =F0(/tildewidex)−F0(x(m))+z(m+1)
im,m+1(x(m)
im−/tildewidexim).
SinceF0(/tildewidex)−F0(x(m))≥0,z(m+1)
im,m+1>0, and/tildewidexim=x(m+1)
im, we have x(m)
im−x(m+1)
im≤0, which
contradicts to (17). Thus, we have z(m+1)
im,m+1≤0 andz(m+1)
im,m+1f′
m+1(x(m)
im)≤0, and complete the
proof.
FromProposition1, wecandeterminethesignof z(m+1)
im,m+1bythevalueof f′
m+1(x(m)
im). Moreover,
iff′
m+1(x(m)
im) = 0, we can easily construct the Gm+1-optimal pair as follows:
x(m+1)
i=/braceleftBigg
x(m)
i,∀i∈Vm,
x(m)
im, i=m+1,andz(m+1)
i,j=/braceleftBigg
z(m)
i,j,∀(i,j)∈Em,
0,(i,j) = (im,m+1).
Hence, we focus on the case with f′
m+1(x(m)
im)/\e}atio\slash= 0 in the subsequent discussions. For this purpose,
we consider the following parametric optimization problem with the parameter t∈ ℜ:
min
x∈ℜ|Vm+1|/summationdisplay
i∈Vm+1fi(xi)+/summationdisplay
(i,j)∈Em{λi,j(xi−xj)++µi,j(xj−xi)+}−t(xim−xm+1),(18)
8whose KKT conditions are presented below:
/summationdisplay
k:(i,k)∈Emzi,k−/summationdisplay
k:(k,i)∈Emzk,i+1{i=im}t=f′
i(xi),∀i∈Vm,
zi,j∈

{−λi,j},ifxi> xj,
[−λi,j,µi,j],ifxi=xj,
{µi,j},ifxi< xj,∀(i,j)∈Em,
−t=f′
m+1(xm+1).(19)
Since each fiis strongly convex, problem (18) has a unique optimal soluti on, denoted by x∗(t), for
eacht∈ ℜ. Moreover, using the Fenchel-Rockafellar duality theorem [18] and the diﬀerentiability
of eachfi, we know that there exists a unique dual optimal solution to p roblem (18), denoted by
z∗(t), which together with x∗(t) satisﬁes the KKT conditions (19). If for certain t∗∈ ℜ, it holds
that
t∗∈

{−λim,m+1}, ifx∗
im(t∗)> x∗
m+1(t∗),
[−λim,m+1,µim,m+1],ifx∗
im(t∗) =x∗
m+1(t∗),
{µim,m+1}, ifx∗
im(t∗)< x∗
m+1(t∗).(20)
Then, by comparing the equations (19) and (20) and the KKT con ditions in equations (12) to (16),
we can obtain the Gm+1-optimal pair based on ( x∗(t∗),z∗(t∗)). Indeed, the Gm+1-optimal pair
(x(m+1),z(m+1)) can be constructed via
x(m+1)=x∗(t∗),andz(m+1)
i,j=z∗
i,j(t∗) for (i,j)∈Em,andz(m+1)
im,m+1=t∗.
This observation also indicates that one can determine the s ign oft∗using Proposition 1.
To ﬁnd the desired t∗, we start from the initial guess t0= 0. We note that when t0= 0,
the corresponding primal-dual optimal pair ( x∗(t0),z∗(t0)) is readily known with x∗
i(t0) =x(m)
ifor
i∈Vmandx∗
m+1(t0) = (f∗
m+1)′(−t0), andz∗(t0) =z(m). Then, we can easily check if t0= 0
satisﬁes (20) by comparing x∗
m+1(t0) andx∗
im(t0). Ifx∗
m+1(t0)/\e}atio\slash=x∗
im(t0), we can use Proposition
1 to determine if tshould be decreased or increased. Assume without loss of the generality that
f′
m+1(x∗
im(t0)) =f′
m+1(x(m)
im)>0. From theabove discussionsandProposition 1, weseethat t∗<0.
Then, we rely on an active-set strategy to iteratively updat e our guess of t∗.
Starting from the initial guess t0= 0, we denote the active set corresponding to Emin (18) by
A0={(i,j,#)|(i,j)∈Em, x∗
i(t0)#x∗
j(t0)},where # ∈ {<,=,>}. (21)
Then, we add the equality constraints induced by edges in A0
=to problem (18) and obtain the
A0-reduced problem of problem (18). The key observation is tha t the primal-dual optimal solution
pair to the A0-reduced problem can be written in a semi-closed form as func tions of the parameter
t, denoted by ( x0(t),z0(t)). Then, we construct a dual candidate ˜ z0(t) to problem (18) as follows:
˜z0
i,j(t) =/braceleftBigg
z0
i,j(t),if (i,j)∈ A0
=,
z∗
i,j(t0),otherwise ,∀(i,j)∈Em.
9We will show that if ( x0(t),˜z0(t)) satisﬁes the complementarity conditions in (19), i.e.,
˜z0
i,j(t)∈

{−λi,j},ifx0
i(t)> x0
j(t),
[−λi,j,µi,j],ifx0
i(t) =x0
j(t),
{µi,j},ifx0
i(t)< x0
j(t),∀(i,j)∈Em,
then (x0(t),˜z0(t)) is the primal-dual optimal solution pair to problem (18).
Based on this observation, a new guess of t∗is constructed by searching for the smallest
possible t1such that −λim,m+1≤t∗≤t1≤t0= 0 and ( x0(t1),˜z0(t1)) still satisﬁes the above
complementarity conditions. Then, we have ( x∗(t1),z∗(t1)) = (x0(t1),˜z0(t1)) and we can check if
t1satisﬁes the system (20). If not, then a new active set A1is constructed and the above process
continues until t∗is found. In a nutshell, our approach is summarized in the fol lowing ﬂowchart:
(t0,x∗(t0),z∗(t0),A0)⇒ ··· ⇒ (tq,x∗(tq),z∗(tq),Aq)⇒ ··· ⇒ (t∗,x∗(t∗),z∗(t∗),A∗).
In what follows, we shall discuss the detailed steps of our pr ocedure and we will prove that the
search process of t∗terminates in at most 2 m−1 steps.
Attqwitht∗< tq≤t0, we assume that ( x∗(tq),z∗(tq)), and the corresponding active set Aq
are available. Then, we construct the following Aq-reduced parametric optimization problem with
parameter t∈ ℜ:
min
x∈ℜ|Vm+1|/summationdisplay
i∈Vm+1fi(xi)+/summationdisplay
(i,j)∈Aq
>λi,j(xi−xj)+/summationdisplay
(i,j)∈Aq
<µi,j(xj−xi)−t(xim−xm+1),
s.t. xi=xj,∀(i,j)∈ Aq
=,(22)
whoseuniqueprimal-dual optimal pair is denotedby ( xq(t),zq(t)). IfAq
==∅, then weset zq(t) =∅.
Here, we require the following compatibility conditions be tweenAqand (x∗(tq),z∗(tq)), which also
servers as an induction hypothesis.
Assumption 2. The active set Aqand the primal-dual pair (x∗(tq),z∗(tq))are compatible. That
is,x∗(tq)is the optimal solution to the problem (22)att=tq, i.e.,xq(tq) =x∗(tq)and the
corresponding dual optimal solution zq(tq)can be constructed via zq
i,j(tq) =z∗
i,j(tq)for(i,j)∈ Aq
=.
Moreover, it holds that x∗
im(tq)−x∗
m+1(tq)>0.
We shall emphasize that according to the construction of A0, it is not diﬃcult to observe that the
active set A0and the primal-dual pair ( x∗(t0),z∗(t0)) are compatible, and x∗
im(t0)−x∗
m+1(t0)>0.
Next, we focus on obtaining ( tq+1,x∗(tq+1),z∗(tq+1),Aq+1) from (tq,x∗(tq),z∗(tq),Aq).
We start by investigating the optimal primal-dual solution pair corresponding to problem (22).
Particularly, instead of solving problem (22) for each t/\e}atio\slash=tq, we derive in the following proposition
the semi-closed formulas for ( xq(t),zq(t)) under Assumption 2. We also show that the optimal
primal-dual solution pair of problem (18) can be obtained fr om (xq(t),zq(t)) provided that some
complementarity conditions hold.
Proposition 2. LetPAqbe the partition of Gminduced by AqandBq∈PAqbe the subtree such
thatim∈Bq. Then, under Assumption 2, for any t∈ ℜ, the primal optimal solution xq(t)takes
10the following form: 

xq
i(t) =x∗
i(tq),∀i∈Vm\VBq,
xq
i(t) =/parenleftbig
(/summationdisplay
i∈VBqfi)∗/parenrightbig′/parenleftbigg
t+βq/parenrightbigg
,∀i∈VBq,
xq
m+1(t) = (f∗
m+1)′(−t),(23)
where
βq=/summationdisplay
(i,k)∈Em
i∈VBq,k/∈VBqz∗
i,k(tq)−/summationdisplay
(k,i)∈Em
k/∈VBq,i∈VBqz∗
k,i(tq).
Pickimas the ancestor of Bq. Then, for any t∈ ℜ,zq(t)is given by


zq
i,j(t) =z∗
i,j(tq),∀(i,j)∈ Aq
=\EBq,
zq
i,j(t) =

/summationdisplay
l∈Cif′
l(xq
l(t))−αq
i,j,ifi⊳j,
/summationdisplay
l∈Cj−f′
l(xq
l(t))+αq
i,j,ifi⊳j,∀(i,j)∈EBq,(24)
whereCi:={j∈VBq|j ⊳i}∪{i}for anyi∈VBq, and
αq
i,j=

/summationdisplay
(l,k)∈Em
l∈Ci,k/∈VBqz∗
l,k(tq)−/summationdisplay
(k,l)∈Em
l∈Ci,k/∈VBqz∗
k,l(tq),ifi⊳j,
/summationdisplay
(l,k)∈Em
l∈Cj,k/∈VBqz∗
l,k(tq)−/summationdisplay
(k,l)∈Em
l∈Cj,k/∈VBqz∗
k,l(tq),ifj ⊳i,∀(i,j)∈EBq.
LetΩq={(i,j)∈Em\EBq|exactly one of iandjis inVBq}. If
zq
i,j(t)∈[−λi,j,µi,j],∀(i,j)∈EBq, (25)
z∗
i,j(tq)∈

{−λi,j},ifxq
i(t)> xq
j(t),
[−λi,j,µi,j],ifxq
i(t) =xq
j(t),
{µi,j},ifxq
i(t)< xq
j(t),∀(i,j)∈Ωq, (26)
then(xq(t),˜zq(t))solves the KKT system (19), where
˜zq
i,j(t) =/braceleftBiggzq
i,j(t),if(i,j)∈ Aq
=,
z∗
i,j(tq),otherwise ,∀(i,j)∈Em. (27)
Proof.Without loss of generality, we can assume that PAq={Bk}K
k=1∪BqwhereBk, 1≤k≤K,
andBqare subtrees of Gm. Then, problem (22) can be decomposed into K+ 2 independent
subproblems on each subtree BkandBqand the singleton {m+1}. Note that the parameter tonly
11appears in the subproblems corresponding to the subtree Bqand the singleton {m+ 1}. Hence,
from Assumption 2, it is not diﬃcult to deduce that for any t∈ ℜ,
xq
i(t) =x∗
i(tq), i∈Vm\VBq,andzq
i,j(t) =z∗
i,j(tq),(i,j)∈ Aq
=\EBq.
The subproblem associated with {m+1}is easily solved via xq
m+1(t) = (f∗
m+1)′(−t). Therefore, we
only need to focus on the subproblem associated with the subt reeBq:
min
x∈ℜ|VBq|

/summationdisplay
i∈VBqˆfi(xi)−txim|xi=xj,∀(i,j)∈EBq

, (28)
where
ˆfi(xi) :=fi(xi)+/summationdisplay
k/negationslash∈VBq
(k,i)∈Emz∗
k,i(tq)xi−/summationdisplay
k/negationslash∈VBq
(i,k)∈Emz∗
i,k(tq)xi,∀i∈VBq.
LetLbe the Lagrangian function associated with problem (28)
L(x;z) =/summationdisplay
i∈VBqˆfi(xi)−txim−/summationdisplay
(i,j)∈EBqzi,j(xi−xj),∀(x,z)∈ ℜ|VBq|×ℜ|EBq|.
Then, the optimal primal-dual solution pair to problem (28) satisﬁes the following KKT system:


xi=xj,∀(i,j)∈EBq,
f′
i(xi)+/summationdisplay
k/negationslash∈VBq
(k,i)∈Emz∗
k,i(tq)+/summationdisplay
k∈VBq
(k,i)∈EBqzk,i−/summationdisplay
k/negationslash∈VBq
(i,k)∈Emz∗
i,k(tq)−/summationdisplay
k∈VBq
(i,k)∈EBqzi,k−1{i=im}t= 0,∀i∈VBq.
(29)
Summing over all i∈VBq, we deduce from the above system that
/summationdisplay
i∈VBqf′
i(xq
i(t)) =−/summationdisplay
(k,i)∈Em
k/negationslash∈VBq,i∈VBqz∗
k,i(tq)+/summationdisplay
(i,k)∈Em
i∈VBq,k/negationslash∈VBqz∗
i,k(tq)+t,
i.e.,
xq
i(t) = ((/summationdisplay
i∈VBqfi)∗)′(t+/summationdisplay
(i,k)∈Em
i∈VBq,k/∈VBqz∗
i,k(tq)−/summationdisplay
(k,i)∈Em
k/∈VBq,i∈VBqz∗
k,i(tq)),∀i∈VBq.
Next, we obtain from the above KKT system (29) the following l inear system corresponding
tozi,jfor (i,j)∈EBq:
/summationdisplay
k:(i,k)∈EBqzi,k−/summationdisplay
k:(k,i)∈EBqzk,i=f′
i(xq
i(t))+/summationdisplay
k/∈VBq
(k,i)∈Emz∗
k,i(tq)−/summationdisplay
k/∈VBq
(i,k)∈Emz∗
i,k(tq),∀i∈Vm\{im}.
Sinceimis the ancestor of the subtree B, we obtain from Lemma 1 the updated formula for zq
i,j(t),
(i,j)∈EBq. Thus, we proved (24).
Finally, it is not diﬃcult to see that if the assumed conditio ns (25) and (26) are satisﬁed, then
xq(t) and ˜zq(t) satisfy the complementarity conditions in the KKT system ( 19). The rest equations
in (19) hold automatically by noting (27) and the KKT system ( 29).
12Usingthesemi-closedformulasinProposition2, wecompute thefollowinglowerbound∆ tq≤0:
∆tq:= min{∆t|(25) and (26) hold for all t∈[tq+∆t,tq]}.
Thecomputations aredividedintotwoparts. Firstly, wefoc usonthevalueof zq
i,j(t)for(i,j)∈EBq.
For any ( i,j)∈EBq, we note that zq
i,j(tq)∈[−λi,j,µi,j] andzq
i,j(t) is increasing if i ⊳ jand is
decreasing if j ⊳iwith respect to tfrom (24). We deﬁne the threshold ∆( EBq) as follows:
∆(EBq) :=

max
(i,j)∈EBq∆ti,j,ifEBq/\e}atio\slash=∅,
−∞,otherwise .(30)
Here, each ∆ ti,j≤0 solves
zq
i,j(tq+∆ti,j) =−λi,j,ifi⊳j,andzq
i,j(tq+∆ti,j) =µi,j,ifj ⊳i. (31)
Next, the relations in (26) corresponding to the edges in Ωqare examined. For this purpose, we
divide Ωqinto two parts, namely,
Ωq
+={(i,j)∈Ωq|i∈VBq, j∈Vm\VBq}and Ωq
−={(i,j)∈Ωq|i∈Vm\VBq, j∈VBq},(32)
and handle them separately. From (23), we know that xq
i(t) takes the same value for all i∈VBq
and is increasing with respect to t. Hence, we can simply denote xBq(t) =xq
i(t) for any i∈VBq.
Then, we compute the threshold ∆(Ωq) := max {∆(Ωq
+),∆(Ωq
−)}, where
∆(Ωq
+) :=

∆tsatisfying xBq(tq+∆t) = max
(i,j)∈Ωq
+∩Aq
>x∗
j(tq),if Ωq
+∩Aq
>/\e}atio\slash=∅,
−∞,otherwise,(33)
and
∆(Ωq
−) :=

∆tsatisfying xBq(tq+∆t) = max
(i,j)∈Ωq
−∩Aq
<x∗
i(tq),if Ωq
−∩Aq
</\e}atio\slash=∅,
−∞,otherwise.(34)
It can be easily veriﬁed that
∆tq= max{∆(EBq),∆(Ωq)}. (35)
Thus, using Proposition 2, we can obtain the semi-closed for m for the optimal solution x∗(t), as
well as its corresponding dual optimal solution z∗(t), to problem (18) for any t∈[tq+∆tq,tq].
Now, we are ready to discuss the search of tq+1. Note that according to Assumption 2, we have
xq
im(tq)−xq
m+1(tq) =x∗
im(tq)−x∗
m+1(tq)>0.
Usingthe closed-form formulas in Proposition 2, we know tha txq
im(t)−xq
m+1(t) is strictly increasing
with respect to t, and we can obtain a unique ∆ /tildewidetq<0 via solving the following univariate nonlinear
equation:
xq
im(tq+∆/tildewidetq)−xq
m+1(tq+∆/tildewidetq) = 0,
13which is nothing but the optimality condition associated wi th the following univariate strongly
convex optimization problem:
tq+∆/tildewidetq= argmin
t

(/summationdisplay
i∈VBqfi)∗(t+βq)+(f∗
m+1)(−t)

.
The existence of ∆ /tildewidetqis thus guaranteed. Then, we set
tq+1= max{tq+∆tq,tq+∆/tildewidetq,−λim,m+1}. (36)
As one can observe, it always holds that tq+1∈[tq+∆tq,tq] and
x∗
im(tq+1)−x∗
m+1(tq+1) =xq
im(tq+1)−xq
m+1(tq+1)
≥xq
im(tq+∆/tildewidetq)−xq
m+1(tq+∆/tildewidetq) = 0.(37)
Then, we reveal the relation between tq+1andt∗in the following lemma.
Lemma 3. It holds that −λim,m+1≤t∗≤tq+1≤tq≤0. Moreover, tq+1=t∗if and only if
x∗
im(tq+1)−x∗
m+1(tq+1) = 0ortq+1=−λim,m+1.
Proof.Ift∗> tq+1, we have from (36) that t∗> tq+1≥ −λim,m+1. It then follows from (20) that
xq
im(t∗)−xq
m+1(t∗) =x∗
im(t∗)−x∗
m+1(t∗) = 0.
However, we know from (37) and the strict monotonicity of xq
im(t)−xq
m+1(t) that
xq
im(t∗)−xq
m+1(t∗)> xq
im(tq+1)−xq
m+1(tq+1)≥0.
We arrive at a contradiction. Thus, t∗≤tq+1.
Next, if x∗
im(tq+1)−x∗
m+1(tq+1) = 0 or tq+1=−λim,m+1, one can easily verify that tq+1,
x∗
im(tq+1) andx∗
m+1(tq+1) satisfy (20), i.e., t∗=tq+1. Conversely, if t∗=tq+1, we have tq+1≥
−λim,m+1. Iftq+1>−λim,m+1, it follows directly from (20) that x∗
im(t∗)−x∗
m+1(t∗) = 0. We thus
complete the proof of the lemma.
Remark 1. It is only necessary to compute ∆/tildewidetqat most once during the entire search process for
t∗. Indeed, let
∆∗:=/braceleftBigg
xq
im(tq+∆tq)−xq
m+1(tq+∆tq),if∆tq>−∞,
−∞,otherwise.
If∆∗≥0, then by the strict monotonicity of xq
im(t)−xq
m+1(t), we must have ∆/tildewidetq≤∆tq. In this
case, we can directly set
tq+1= max{tq+∆tq,−λim,m+1},
without computing ∆/tildewidetq. Only when ∆∗<0, we shall compute ∆/tildewidetqand set
tq+1= max{tq+∆/tildewidetq,−λim,m+1}.
Then, from Lemma 3, it holds that tq+1=t∗. Therefore, ∆/tildewidetqonly needs to be computed at most
once.
14Iftq+1/\e}atio\slash=t∗, we know from (36), (37), and Lemma 3 that t∗< tq+1and
tq+1=tq+∆tq,andx∗
im(tq+1)−x∗
m+1(tq+1)>0. (38)
Then, we give the details of the construction of Aq+1. LetM(EBq) =M(E+
Bq)∪M(E−
Bq) with
/braceleftBigg
M(E+
Bq) ={(i,j)∈EBq|∆ti,j= ∆tq,andi⊳j},
M(E−
Bq) ={(i,j)∈EBq|∆ti,j= ∆tq,andj ⊳i},(39)
andM(Ωq) =M(Ωq
+)∪M(Ωq
−) with
/braceleftBigg
M(Ωq
+) ={(i,j)∈Ωq
+∩A(tq)>|xq
i(tq+∆tq) =x∗
j(tq)},
M(Ωq
−) ={(i,j)∈Ωq
−∩A(tq)<|xq
j(tq+∆tq) =x∗
i(tq)}.(40)
The active set Aq+1is constructed via


Aq+1
==/parenleftbig
Aq
=∪M(Ωq)/parenrightbig
\M(EBq),
Aq+1
>=/parenleftbig
Aq
>∪M(E+
Bq)/parenrightbig
\M(Ωq
+),
Aq+1
<=/parenleftbig
Aq
<∪M(E−
Bq)/parenrightbig
\M(Ωq
−).(41)
Similar to (27), we can construct /tildewidezq(tq+1) fromzq(tq+1) as follows:
˜zq
i,j(tq+1) =/braceleftBiggzq
i,j(tq+1),if (i,j)∈ Aq
=,
z∗
i,j(tq),otherwise ,∀(i,j)∈Em.
Then, we obtain the optimal primal-dual solution pair ( x∗(tq+1),z∗(tq+1)) = (xq(tq+1),/tildewidezq(tq+1)) to
problem (18) with t=tq+1.
Next, it can be easily veriﬁed from the construction of Aq+1in (41), and the computation
steps oftq+1in (36) that the new active set Aq+1and the primal-dual pair ( x∗(tq+1),z∗(tq+1)) are
compatible. This, together with (38), allows us to perform i nduction on q∈Nand obtain that for
allq∈N, as long as tq/\e}atio\slash=t∗, it always holds that Aqand (x∗(tq),z∗(tq)) are compatible and
x∗
im(tq)−x∗
m+1(tq)>0.
Therefore, we can iteratively repeat the above searching pr ocess, i.e., from ( tq,x∗(tq),z∗(tq),Aq) to
(tq+1,x∗(tq+1),z∗(tq+1),Aq+1), untilt∗is obtained. The details of the search process are summa-
rized in Algorithm 2. We name it the update−subroutine, since in this case t∗<0. The procedure
corresponding to the case with t∗>0, which we termed as the update+subroutine, can be eas-
ily adapted from the update−subroutine. Details of the update+subroutine can be found in the
Appendix.
Before presenting the details of the generate subroutine, we make some key observations about
the active set Aq+1in the following lemma.
Lemma 4. For any given q∈N, the following propositions hold:
(a) Iftq+1/\e}atio\slash=t∗, thenAq+1
=/\e}atio\slash=Aq
=;
15Algorithm 2 (tq+1,x∗(tq+1),z∗(tq+1),Aq+1,t∗) =update−(tq,x∗(tq),z∗(tq),Aq,λ)
1:Input: (tq,x∗(tq),z∗(tq),Aq),λ≥0;
2:Compute ∆( EBq),∆(Ωq
+),∆(Ωq
−) via deﬁnitions (30), (33) and (34)
3:∆(Ωq) = max{∆(Ωq
−),∆(Ωq
+)}
4:∆tq= max{∆(EBq),∆(Ωq)}
5:∆∗=xq
im(tq+∆tq)−xq
m+1(tq+∆tq)
6:if∆∗≥0then
7:tq+1= max{tq+∆tq,−λ}
8:else
9:∆/tildewidetq=−tq+argmin
t/braceleftBig
(/summationtext
i∈VBqfi)∗(t+βq)+(f∗
m+1)(−t)/bracerightBig
10:tq+1= max{tq+∆/tildewidetq,−λ}
11:end if
12:(x∗(tq+1),z∗(tq+1)) = (xq(tq+1),/tildewidezq(tq+1))
13:iftq+1=−λorx∗
im(tq+1) =x∗
m+1(tq+1)then
14:t∗=tq+1
15:LetAq+1={(i,j,#)|(i,j)∈Em, x∗
i(tq+1)#x∗
j(tq+1)}
16:else
17:t∗=∅
18:UpdateAq+1fromAqvia (41)
19:end if
20:Output: (tq+1,x∗(tq+1),z∗(tq+1),Aq+1,t∗)
(b) If(i,j)∈ M(EBq), then for any /hatwideq∈Nwith/hatwideq > qandt/hatwideq/\e}atio\slash=t∗,(i,j)/∈ A/hatwideq
=.
Proof.We prove (a) ﬁrst. If tq+1/\e}atio\slash=t∗, from (38), we have tq+1=tq+ ∆tq> t∗. Hence, at
least one of the two sets, M(EBq) andM(Ωq), is nonempty. The desired result thus follows since
Aq+1
==/parenleftbig
Aq+1∪M(Ωq)/parenrightbig
\M(EBq) andM(EBq)∩M(Ωq) =∅.
Next, we prove (b). We ﬁrst consider the case where i⊳j. If (i,j)∈ M(EBq) andi⊳j, we see
from (31), (39) and (41) that
zq
i,j(tq+∆tq) =−λi,j,and (i,j)∈ M(E+
Bq)⊆ Aq+1
>.
Since (i,j)∈ Aq+1
>, then at least one of iandjis not in Bq+1, i.e., (i,j)/∈EBq+1. Sincei⊳j, we
have the following two possible cases:
(i)j∈Bq+1,i /∈Bq+1. In this case we have ( i,j)∈Ωq+1
−. Since (i,j)∈ Aq+1
>, it holds from (40)
that (i,j)/∈ M(Ωq+1
+). Thus, (41) implies that ( i,j)∈ Aq+2
>.
(ii)j /∈Bq+1,i /∈Bq+1. From (32), we know that ( i,j)/∈Ωq+1. Hence, (40) and (41) imply that
(i,j)∈ Aq+2
>.
Therefore, in both cases, we have ( i,j)/∈Ωq+2
+and (i,j)∈ Aq+2
>. By induction, we can prove that
(i,j)/∈Ω/hatwideq
+and (i,j)∈ A/hatwideq
>for all/hatwideq > q.
Similarly, for the case with j ⊳i, we can obtain that ( i,j)/∈Ω/hatwideq
−and (i,j)∈ A/hatwideq
<for all/hatwideq > q.
We thus complete the proof.
16With the two subroutines update−andupdate+at hand, we are ready to present the details of
thegenerate subroutine in Algorithm 3. As one can easily observe, the com plexity of the generate
subroutine depends critically on the number of executions o f the while-loops (i.e., lines 9-12 and
lines 15-18 in Algorithm 3).
Algorithm 3 Thegenerate subroutine: ( x(m+1),z(m+1)) =generate (x(m),z(m),Gm+1)
1:Input:x(m)∈ ℜm,z(m)∈ ℜm−1,Gm+1= (Vm+1,Em+1)
2:Letx∗
i(0) =x(m)
ifori∈Vmandx∗
m+1(0) = (f∗
m+1)′(0)
3:Letz∗
i,j(0) =z(m)
i,jfor (i,j)∈Emandt∗=∅
4:
5:iff′
m+1(x∗
im(0)) = 0 then
6:t∗= 0
7:else iff′
m+1(x∗
im(0))>0then
8:Lett0= 0,q= 0 and A0be the active set constructed from x∗(0) as in (21)
9:whilet∗=∅do
10: (tq+1,x∗(tq+1),z∗(tq+1),Aq+1,t∗)=update−(tq,x∗(tq),z∗(tq),Aq,λim,m+1)
11: q=q+1
12:end while
13:else
14:Lett0= 0,q= 0 and A0be the active set constructed from x∗(0) as in (21)
15:whilet∗=∅do
16: (tq+1,x∗(tq+1),z∗(tq+1),Aq+1,t∗)=update+(tq,x∗(tq),z∗(tq),Aq,µim,m+1)
17: q=q+1
18:end while
19:end if
20:Letx(m+1)=x∗(t∗),z(m+1)
i,j=z∗
i,j(t∗) for (i,j)∈Em, andz(m+1)
im,m+1=t∗
21:Return: (x(m+1),z(m+1))∈ ℜm+1×ℜm
Lemma 5. The while-loops executed in the generate subroutine will ﬁnd t∗in at most 2m−1
iterations.
Proof.Without loss of generality, we only consider the case f′
m+1(x∗
im(0))>0, i.e.,t∗<0. Assume
that after 2 m−2 times executions of the while-loops, t∗has not been found. That is, the algorithm
generates {(ti,x∗(ti),z∗(ti),Ai)}2m−2
i=1andti> t∗for alli= 0,...,2m−2. From Lemma 4(a), we
know that
Aq
=/\e}atio\slash=Aq+1
=,∀q= 0,...,2m−3. (42)
Next, we note from Lemma 4(b) that if some edge ( i,j)∈Emis removed from Aq
=for some q,
then (i,j)/\e}atio\slash∈ A/hatwideq
=for all 2m−2≥/hatwideq≥q≥0. Therefore, for each edge ( i,j)∈Em, it can be added to
and removed from Aq
=for at most once. This, together with (42) and the fact that |Em|=m−1,
implies that at t2m−2, every edge in Emhas been added to and removed from some Aq
=. Thus,
A2m−2
==∅, and the sets A2m−2
>andA2m−2
<remain unchanged in the next iterations, i.e., EBq=∅,
Ω2m−2
+∩A2m−2
>=∅and Ω2m−2
−∩A2m−2
<=∅. Therefore, we have ∆ t2m−2=−∞from its deﬁnition
in (35). By (36) and Lemma 3, we have t2m−1=t∗and complete the proof.
17Lemma 5 guarantees that t∗can be found by the generate subroutine eﬃciently. Along with t∗,
theGm+1-optimal pair ( x(m+1),z(m+1)), i.e., the outputof the generate subroutine, is also obtained.
We thus naturally obtain the correctness of our Algorithm 1.
Theorem 1. The output x(n)of Algorithm 1 is the optimal solution to problem (1).
At the end of this section, we provide a brief analysis of the w orst-case complexity of our Algo-
rithm 1. Here, we assume that for a given strongly convex diﬀer entiable function fandx∈ ℜ, the
computational complexity of ﬁnding tsuch that f′(t) =xisO(1). Then, the computational com-
plexity of update−(andupdate+) isO(m). By Lemma 5, we see that the computational complexity
of thegenerate subroutine is O(m2). Therefore, the computational complexity of Algorithm 1 i s
O(n3).
4 Conclusion
In this paper, we focus on the convex isotonic regression pro blem (1) with tree-induced generalized
order restrictions. Inspired by the successes of the PAVA, a n eﬃcient active-set based recursive
approach, ASRA, is carefully designed to solve (1). Under mi ld assumptions, we show that ASRA
has a polynomial time computational complexity.
5 Appendix
5.1 The arborescence assumption on G
For the given G= (V,E) in the formulation of problem (1), let /hatwideG= (V,/hatwideE) be an arborescence that
shares the same underlying graph with G. Therefore, for any ( i,j)∈/hatwideE, we have either ( i,j)∈E
or (j,i)∈E. Then, for any ( i,j)∈/hatwideE, let
/hatwideλi,j=/braceleftBigg
λi,j,if (i,j)∈E,
µj,i,if (j,i)∈E,and/hatwideµi,j=/braceleftBigg
µi,j,if (i,j)∈E,
λj,i,if (j,i)∈E.
It can be easily veriﬁed that problem (1) is equivalent to the following optimization problem:
min
x∈ℜV/summationdisplay
i∈Vfi(xi)+/summationdisplay
(i,j)∈/hatwideE/hatwideλi,j(xi−xj)++/summationdisplay
(i,j)∈/hatwideE/hatwideµi,j(xj−xi)+.
Hence, we can assume that the directed tree Gin (1) is an arborescence.
1
2 3
45
(a) a directed tree G⇒1
2 3
45
(b) underlying graph of Gand/hatwideG⇐1
2 3
45
(c) an arborescence /hatwideG
Figure 2: A directed tree Gand an arborescence /hatwideGthat share the same underlying graph.
18Next, we discuss the decomposition of G. For an arborescence G= (V,E), letn=|V|.
Without loss of generality, we assume that the node 1 is the ro ot ofG, and the nodes in Gare
arranged such that for any edge ( i,j)∈E,i < jalways holds. Then, we deﬁne Gn=G, and let
Gm−1= (Vm−1,Em−1) be the subgraph of Gm= (Vm,Em) obtained by deleting the node mand
the related edges from Gm, wheren≥m≥2. Since for any ( i,j)∈E, it holds that i < j, we know
that the node mmust be a leaf node of Gm, hence, according to [23], Gm−1is still a directed tree
andGm−1⊂Gmform= 2,...,n. It’s easy to verify that Vm={1,2,...,m}for 1≤m≤n, and
{(im,m+1)}=Em+1\Emwithim∈Vmfor 1≤m≤n−1.
5.2 The update+subroutine
We brieﬂydescribethe update+subroutinehere, whichcorrespondstothecase with t∗>0. Assume
that we have obtained a guess tqoft∗satisfying 0 = t0≤tq< t∗≤µim,m+1, Meanwhile, the
corresponding primal-dual optimal solution pair ( x∗(tq),z∗(tq)) and the active set Aqare available,
such that Aqand (x∗(tq),z∗(tq)) are compatible and x∗
im(tq)−x∗
m+1(tq)<0. Then, the semi-closed
formulas (23) and (24) for the Aq-reduced problem in Proposition 2 still hold.
Here, we need to search
∆tq:= max{∆t|(25) and (26) hold for all t∈[tq,tq+∆t]}.
First, let
∆(EBq) =

min
(i,j)∈EBq∆ti,j,ifEBq/\e}atio\slash=∅,
+∞,otherwise ,(43)
where each ∆ ti,j≥0 solves:
zq
i,j(tq+∆ti,j) =µi,j,ifi⊳j,andzq
i,j(tq+∆ti,j) =−λi,j,ifj ⊳i.
Next, let ∆(Ωq) = min{∆(Ωq
+),∆(Ωq
−)}, where
∆(Ωq
+) :=

∆tsatisfying xBq(tq+∆t) = min
(i,j)∈Ωq
+∩Aq
<x∗
j(tq),if Ωq
+∩Aq
</\e}atio\slash=∅,
+∞,otherwise,(44)
and
∆(Ωq
−) :=

∆tsatisfying xBq(tq+∆t) = min
(i,j)∈Ωq
−∩Aq
>x∗
i(tq),if Ωq
−∩Aq
>/\e}atio\slash=∅,
+∞,otherwise.(45)
Then, ∆tq= min{∆(EBq),∆(Ωq)}. Compute ∆ /tildewidetq≥0 via solving xq
im(tq+∆/tildewidetq)−xq
m+1(tq+∆/tildewidetq) =
0, and set
tq+1= min{tq+∆tq,tq+∆/tildewidetq,µim,m+1}.
Iftq+1< t∗, we will update the active set Aq+1in the following fashion. Let M(EBq) =
M(E+
Bq)∪M(E−
Bq) with
/braceleftBigg
M(E+
Bq) ={(i,j)∈EBq|∆ti,j= ∆tq,andi⊳j},
M(E−
Bq) ={(i,j)∈EBq|∆ti,j= ∆tq,andj ⊳i},
19andM(Ωq) =M(Ωq
+)∪M(Ωq
−) with
/braceleftBigg
M(Ωq
+) ={(i,j)∈Ωq
+∩A(tq)<|xq
i(tq+∆tq) =x∗
j(tq)},
M(Ωq
−) ={(i,j)∈Ωq
−∩A(tq)>|xq
j(tq+∆tq) =x∗
i(tq)}.
Then,Aq+1is obtained via


Aq+1
==/parenleftbig
Aq
=∪M(Ωq)/parenrightbig
\M(EBq),
Aq+1
>=/parenleftbig
Aq
>∪M(E−
Bq)/parenrightbig
\M(Ωq
−),
Aq+1
<=/parenleftbig
Aq
<∪M(E+
Bq)/parenrightbig
\M(Ωq
+).(46)
We summarize the update+subroutine in Algorithm 4.
Algorithm 4 (tq+1,x∗(tq+1),z∗(tq+1),Aq+1,t∗) =update+(tq,x∗(tq),z∗(tq),Aq,µ)
1:Input: (tq,x∗(tq),z∗(tq),Aq),µ≥0;
2:Compute ∆( EBq),∆(Ωq
+),∆(Ωq
−) via deﬁnitions (43), (44) and (45)
3:∆(Ωq) = min{∆(Ωq
−),∆(Ωq
+)}
4:∆tq= min{∆(EBq),∆(Ωq)}
5:∆∗=xq
im(tq+∆tq)−xq
m+1(tq+∆tq)
6:if∆∗≤0then
7:tq+1= min{tq+∆tq,µ}
8:else
9:∆/tildewidetq=−tq+argmin
t/braceleftBig
(/summationtext
i∈VBqfi)∗(t+βq)+(f∗
m+1)(−t)/bracerightBig
10:tq+1= min{tq+∆/tildewidetq,µ}
11:end if
12:(x∗(tq+1),z∗(tq+1)) = (xq(tq+1),/tildewidezq(tq+1))
13:iftq+1=µorx∗
im(tq+1) =x∗
m+1(tq+1)then
14:t∗=tq+1
15:LetAq+1={(i,j,#)|(i,j)∈Em, x∗
i(tq+1)#x∗
j(tq+1)}
16:else
17:t∗=∅
18:UpdateAq+1fromAqvia (46)
19:end if
20:Output:(tq+1,x∗(tq+1),z∗(tq+1),Aq+1,t∗)
5.3 An illustration of the ASRA
We provide an example of applying the ASRA for solving proble m (1). Let G= (V,E) be the
directed tree shown in Figure 3a, where V={1,2,3,4,5}andE={(1,2),(1,3),(3,4),(3,5)}. Let
fi(xi) =1
2(xi−yi)2fori= 1,...,4,wherey= (4,2,2,8)∈ ℜ4,andf5(x5) =x2
5+1
4x4
5,
and we set the regularization parameters as follows:
(λ1,2,µ1,2) = (+∞,0),(λ1,3,µ1,3) = (0,+∞),(λ3,4,µ3,4) = (0,4),and (λ3,5,µ3,5) = (3,3).
201
2 3
45
(a) directed tree G= (V,E)4
(b)G1-optimal pair3
3−1
(c)G2-optimal pair
3
3 2−1 0
(d)G3-optimal pair4
4 4
4−2 2
4
(e)G4-optimal pair3
3 3
4 1−1 0
4 −3
(f)G5-optimal pair
Figure 3: An example of applying the ASRA for solving problem (1) with given G= (V,E). The
ﬁrst subﬁgure represents the directed tree G= (V,E), and the remaining ﬁve subﬁgures are the
illustrations of the Gm-optimal pairs for m= 1,2,3,4,5, where the values of xifori∈Vare
presented within the circles while the values of zi,jfor (i,j)∈Eare presented above the edges.
The detailed steps of the ASRA are given below:
(i) First, we initialize with x(1)
1= 4.
(ii) Since( f∗
2)′(x(1)
1)>0, itholdsthat t∗≤0. Westartfrom t0= 0andterminateat t∗=t1=−1.
Then, the G2-optimal pair ( x(2),z(2)) isx(2)= (3,3) andz(2)
1,2=−1.
(iii) Since ( f∗
3)′(x(2)
1)>0, we have t∗≤0. Here, we have t∗=t0=−λ1,3= 0. The corresponding
G3-optimal pair ( x(3),z(3)) isx(3)= (3,3,2), andz(3)
1,2=−1,z(3)
1,3= 0.
(iv) Since ( f∗
4)′(x(3)
3)<0, it holds that t∗≥0. Starting at t0= 0, we ﬁrst arrive at t1= 1, and
modify the corresponding active set, i.e., replace (2 ,3,>) with (2 ,3,=), then continue the
searching of t∗. We terminate at t∗=t2= 4. Therefore, the G4-optimal pair ( x(4),z(4)) is
x(4)= (4,4,4,4), andz(4)
1,2=−2,z(4)
1,3= 2,z(4)
3,4= 4.
(v) Since ( f∗
5)′(x(4)
3)>0, we have t∗≤0. Starting from t0= 0, we ﬁrst arrive t1= 0 and replace
(3,4,=) with (3 ,4,<) in the corresponding active set. Then, we terminate the sea rching at
t∗=t2=−3, and the G5-optimal pair ( x(5),z(5)) isx(5)= (3,3,3,4,1), andz(5)
1,2=−1,z(5)
1,3=
0,z(5)
3,4= 4,z(5)
3,5=−3.
Thus, the optimal solution to problem (1) is x∗= (3,3,3,4,1). An illustration of the above
procedure is presented in Figure 3.
21References
[1]M. Ayer, H. D. Brunk, G. M. Ewing, W. T. Reid, and E. Silverman ,An empiri-
cal distribution function for sampling with incomplete inf ormation , Annals of Mathematical
Statistics, 26 (1955), pp. 641–647.
[2]A. Barbero and S. Sra ,Modular proximal optimization for multidimensional total -
variation regularization , Journal of Machine Learning Research 19 (2018), pp. 1–82.
[3]D. Bertsimas and J. N. Tsitsiklis ,Introduction to Linear Optimization , AthenaScientiﬁc,
MA, 1997.
[4]M. J. Best and N. Chakravarti ,Active set algorithms for isotonic regression: a unifying
framework , Mathematical Programming 47 (1990), pp. 425–439.
[5]M. J. Best, N. Chakravarti, and V. A. Ubhaya ,Minimizing separable convex functions
subject to simple chain constraints , SIAM Journal on Optimization, 10 (2000), pp. 658–672.
[6]H. D. Brunk ,Maximum likelihood estimates of monotone parameters , Annals of Mathemat-
ical Statistics, 26 (1955), pp. 607–616.
[7]N. Chakravarti ,Isotonic median regression: a linear programming approach , Mathematics
of Operation Research, 14 (1989), pp. 303–308.
[8]N. Chakravarti ,Isotonic median regression for orders representable by root ed trees, Naval
Research Logistics, 39 (1992), pp. 599–611.
[9]X. Chang, Y.-L. Yu, Y. Yang, and E. P. Xing ,Semantic pooling for complex event anal-
ysis in untrimmed videos , IEEE Transactions on Pattern Analysis and Machine Intelli gence,
39 (2016), pp. 1617–1732.
[10]L. Condat ,A direct algorithm for 1D total variation denoising , IEEE Signal Processing
Letters, 20 (2013), pp. 1054–1057.
[11]N. Deo,Graph theory with applications to engineering and computer science, Prentice Hall,
NJ, 1974.
[12]M. Frisen ,Unimodal regression , The Statistician, 35 (1986), pp. 479–485.
[13]C. Lu and D. S. Hochbaum ,A uniﬁed approach for a 1D generalized total variation prob-
lem, Mathematical Programming, 194 (2022), pp. 415–442.
[14]H. H¨oefling ,A path algorithm for the fused lasso signal approximator , Journal of Compu-
tational and Graphical Statistics, 19 (2010), pp. 984–1006 .
[15]V. Kolmogorov, T. Pock, and M. Rolinek ,Total varaition on a tree , SIAM Journal of
Imaging Sciences, 9 (2016), pp. 605–636.
[16]I. Matyasovszky ,Estimating red noise spectra of climatological time series , QuarterlyJour-
nal of the Hungarian Meteorological Service, 117 (2013), pp . 187–200.
22[17]A. Restrepo and A. C. Bovik ,Locally monotonic regression , IEEETransactions on Signal
Processing, 41 (1993), pp. 2796–2810.
[18]R. T. Rockafellar ,Convex Analysis , Princeton University Press, Princeton, NJ, 1970.
[19]Y. U. Ryu, R. Chandrasekaran, and V. Jacob ,Prognosis using an isotonic prediction
technique , Management Science, 50 (2004), pp. 777–785.
[20]M. J. Silvapulle and P. K. Sen ,Constrained Statistical Inference: Inequality, Order and
Shape Restrictions , John Wiley & Sons, 2005.
[21]Q. F. Stout ,Unimodal regression via preﬁx isotonic regression , Computational Statistics &
Data Analysis, 53 (2008), pp. 289–297.
[22]R. Tibshirani, H. H ¨oefling, and R. Tibshirani ,Nearly-isotonic regression , Technomet-
rics, 53 (2011), pp. 54–61.
[23]D. B. West ,Intorduction to Graph Theory , 2nd edition, Upper Saddle River: Prentice hall,
2001.
[24]C. Wu, J. Thai, S. Yadlowsky, A. Pozdnoukhov, and A. Bayen ,Cellpath: Fusion
of cellular and traﬃc sensor data for route ﬂow estimation vi a convex optimization , Trans-
portation Research Part C: Emerging Technologies, 59 (2015 ), pp. 111–128.
[25]Y.-L. Yu and E. P. Xing ,Exact algorithms for isotonic regression and related , Journal of
Physics: Conference Series 699, 2016.
[26]Z. Yu, X. Chen, and X. D. Li ,A dynamic programming approach for generalized nearly
isotonic regression , Mathematical Programming Computation, 15 (2023), pp. 195 –225.
23