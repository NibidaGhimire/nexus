Vehicle: Bridging the Embedding Gap in the
Verification of Neuro-Symbolic Programs⋆
Matthew L. Daggitt12, Wen Kokke3, Robert Atkey3, Natalia Slusarz1, Luca
Arnaboldi4, and Ekaterina Komendantskaya1
1Heriot-Watt University, Edinburgh, UK
2University of Western Australia, Perth, Australia
3University of Strathclyde, Glasgow, UK
4University of Birmingham, Birmingham, UK
Abstract. Neuro-symbolic programs – programs containing both ma-
chine learning components and traditional symbolic code – are becoming
increasingly widespread. However, we believe that there is still a lack
of a general methodology for verifying these programs whose correct-
ness depends on the behaviour of the machine learning components. In
this paper, we identify the “embedding gap” – the lack of techniques for
linkingsemantically-meaningful“problem-space” propertiestoequivalent
“embedding-space” properties – as one of the key issues, and describe Ve-
hicle, a tool designed to facilitate the end-to-end verification of neural-
symbolic programs in a modular fashion. Vehicle provides a convenient
language for specifying “problem-space” properties of neural networks
and declaring their relationship to the “embedding-space", and a pow-
erful compiler that automates interpretation of these properties in the
language of a chosen machine-learning training environment, neural net-
work verifier, and interactive theorem prover. We demonstrate Vehicle’s
utility by using it to formally verify the safety of a simple autonomous
car equipped with a neural network controller.
Keywords: Verification ·Neural networks ·Neural Network Solvers ·
Interactive Theorem Provers.
1 Introduction
In the last decade, neural-symbolic programs which have the ability to influ-
ence people’s lives have become increasingly common. Examples include cyber-
physical systems such as self-driving cars [3] and drones [34], and decision mak-
ing systems such as insurance assessments [39]. As these programs have real
world consequences, there is an increasing need to have formal guarantees about
them [33]. However, thanks to their machine learning components, they are far
harder to reason about compared to purely symbolic programs. We start by
⋆This work was supported by the EPSRC grant EP/T026952/1, AISEC: AI Secure
and Explainable by ConstructionarXiv:2401.06379v1  [cs.AI]  12 Jan 20242 M.L. Daggitt et al.
modelling an abstract neuro-symbolic program and discuss what is required to
obtain formal guarantees about its behaviour.
Consider a purely symbolic program s(·), whose completion requires comput-
ing a complex, unknown function H:P → Rthat maps objects in the problem
input space Pto those in the problem output space R. Given an embedding
function e:P →Rmand an unembedding function u:Rn→ R, we can approx-
imate Hby training a neural network f:Rm→Rnsuch that u◦f◦e≈ H. We
refer to u◦f◦eas thesolution, and refer to RmandRnas theembedding input
spaceandembedding output space respectively. Unlike objects in the problem
space, the vectors in the embedding space are often not directly interpretable.
The completed neuro-symbolic program is then modelled as s(u◦f◦e).
Our end goal is to prove that s(u◦f◦e)satisfies a property Ψ, which we
will refer to as the program property . The natural way to proceed is to establish
asolution property Φand anetwork property Ξsuch that the proof of Ψis
decomposable into the following three lemmas:
Ξ(f) (1)
∀g:Ξ(g)⇒Φ(u◦g◦e) (2)
∀h:Φ(h)⇒Ψ(s(h)) (3)
i.e. Lemma 1 proves that the network fobeys the network property Ξ, then
Lemma2provesthatthisimplies u◦f◦e,theneuralnetworkliftedtotheproblem
space, obeys the solution property Φ, and finally Lemma 3 proves that this
implies s(u◦f◦e), the neuro-symbolic program, obeys the program property Ψ.
We will now discuss some of the issues with implementing this proof strategy.
Thanks to many decades of research, the community is well placed to prove
results of form of Lemma 3 using a variety of powerful interactive theorem
provers (ITPs) [5,31]. However, experience has shown that properties of the
form of Lemma 1 are much more difficult to prove using ITPs. The first is-
sue is that the modular reasoning that ITPs excel at is not applicable to the
non-interpretable, and non-compositional nature of neural network semantics.
Furthermore the sheer size of modern neural networks, often millions or billions
of weights [24,40], make even representing the network, let alone proving any-
thing about it, impractical in an ITP. For example, the largest neural network
verified to date purely in an ITP is a few hundreds of neurons (or a few thousand
of weights) [4,8,15,16].
In contrast, the automated theorem prover (ATP) community has been sig-
nificantly more successful at proving properties of the form of Lemma 1. Starting
with Reluplex [26], the community has rapidly developed highly specialised SMT
and abstract interpretation-based solvers which are capable of verifying proper-
ties of neural networks with up to millions of neurons [27,29,38,41].
Problem 1: crossing the embedding gap - This leaves Lemma 2. ATPs are ill
suited to proving properties of this form, as they are not designed to reason
about the arbitrary computation present in the embedding functions uande.
Unfortunately, ITPs are equally ill-suited as it would require writing down theVehicle: Bridging the Embedding Gap 3
property Φin the ITP directly (remember Φis often uninterpretable and scales
with the size of the embedding space i.e. potentially hundreds of thousands of
inputs). We call this lack of practical methodology for creating proofs of the
form of Lemma 2 and using them to link the results of the ATPs with larger
proofs in ITPs, the embedding gap .
Problem 2: integration of disparate tools - Unlike conventional software which
is usually at least morally (if not actually) correct at verification time, neural
networks often struggle to learn property Ξfrom data alone [37]. Consequently,
we also need property Ξto influence the training of the network f, e.g. using
techniquessuchasdifferentiablelogic[17]andlineartemporallogic-basedreward
functions [22]. Therefore to construct and verify a neuro-symbolic program, we
need i) ATPs, ii) ITPs and iii) training tools to work together. Unfortunately,
usually each of these have their own specialised input formats and semantics
and so often the only viable approach is write out the specification three times
for the three different tools. This is deeply suboptimal as it requires an informal
judgement that each of the three representations encode the same property.
Problem 3: interpretability of specifications - Finally, we argue that specifica-
tions should be interpretable by not just the verification engineer, but also by
experts in the problem domain who may or may not know about the verification
tools being used. This is currently not the case for many common input formats
for training and verification tools, which necessarily require specifications to be
expressed in the embedding space.
1.1 Our Contributions
Embedding
gapProblem space Embedding space
Specification
ofΦ,e,uSpecification
languageTraining
withΞTraining platform
Tensorflow etc.
Verification
ofΞATPs
Marabou etc.Integration
ofΦwithΨITPs
Agda etc.
Fig.1: The architecture of Vehicle for neuro-symbolic program verification.
Dashed lines indicate information flow and solid lines automatic compilation.
Figure1showswhatwebelieveanidealisedsystemthatsolvestheseproblems
should look like. The user expresses the specification Φand the embedding and4 M.L. Daggitt et al.
unembedding functions eanduin terms of the semantically-meaningful prob-
lem space using a suitable domain-specific language (DSL). This specification
is automatically compiled down to Ξin the embedding space in a form usable
for training and verification. After verification, the original specification Φcan
be exported to an ITP so that its proof can be integrated into the proof of the
program property Ψ.
The rest of this paper describes Vehicle, a tool that implements this proposal.
In Section 2 we describe a proof of temporal correctness of a simple autonomous
car model operating in a non-deterministic, imperfect information environment,
so that we may use it as a running example throughout the paper. In Section 3,
we present the Vehicle specification language, a high-level, pure, functional lan-
guage designed for writing problem-space specifications for neural networks. The
language is optimised for expressivity and readability, with support for binding
neural networks and large datasets, first-class quantifiers, higher-order functions
and operations over tensors. In Section 4 we provide an overview of the Vehi-
cle compiler, describing how specifications can be compiled down to the three
different backends and in Section 5 we discuss its correctness.
2 Motivating Example: Wind Controller
We will use a modified version of the verification problem presented by Boyer,
Green and Moore [7] and illustrated in Figure 2 as a running example. An
autonomous vehicle is travelling along a straight road of width 6 parallel to the
x-axis, with a varying cross-wind that blows perpendicular to the x-axis. The
vehicle has an imperfect sensor that provides noisy measurements of its position
on the y-axis, and can change its velocity with respect to the y-axis in response.
The car’s controller takes in both the current sensor reading and the previous
sensor reading and its goal is to keep the car on the road. For simplicity, we
assume that both the wind-speed and the car’s velocity in the y-direction can
grow arbitrarily large.
0
-1
-23
1
Road edgesy-velocity
windspeedy-positiondirection
of travel
-32
Fig.2: A simple model of an autonomous vehicle compensating for a cross-wind.
As in [7] we discretise the model s(·), and formalise it in Agda, an interactive
theorem prover, as follows. The state of the system consists of the current windVehicle: Bridging the Embedding Gap 5
speed, the position and velocity of the car and the most recent sensor reading.
An oracle provides updates in the form of observations consisting of the shift
in wind speed and the error on the sensor reading. The missing component is a
controller that takes in the current and the previous sensor reading and produces
a resulting change in velocity:
record State : Set where
constructor state
field
windSpeed : Q
position : Q
velocity : Q
sensor : Qrecord Observation : Set where
constructor observe
field
windShift : Q
sensorError : Q
controller : Q→Q→Q
Using this, we can define the evolution of the system as follows:
nextState : Observation →State→State
nextState o s= state newWindSpeed newPosition newVelocity newSensor
where
newWindSpeed = windSpeed s+ windShift o
newPosition = position s+ velocity s+ newWindSpeed
newSensor = newPosition + sensorError o
newVelocity = velocity s+ controller newSensor (sensor s)
finalState : List Observation →State
finalState xs= foldr nextState initialState xs
about which we would like to prove the following program property Ψ:
Theorem 1. If the wind-speed never shifts by more than 1 per unit time and
the sensor is never off by more than 0.25 then the car will never leave the road.
Given a suitable encoding of ValidObservation andOnRoad, this theorem can be
stated in Agda as follows:
finalState-onRoad : ∀xs→All ValidObservation xs→OnRoad (finalState xs)
The full proof of this statement is in Appendix A of the supplementary material
but, crucially, it requires the implementation of controller to satisfy the following
solution property Φ:
controller-lemma : ∀x y→|x| ≤3.25Q→|y| ≤3.25Q→
|controller x y+ 2Q*x-y|<1.25Q
The challenge is then to implement the Agda function controller with a neural
network satisfying this specification. In our case, the controller is implemented
with a 3-layer densely connected neural network f. The embedding function e
normalisesthesemanticallymeaningfulproblemspaceinputsmeasuredinmetres6 M.L. Daggitt et al.
in the range [-4, 4] to the range [0,1] in the embedding space. The unembedding
function uis the identity function.
Obviously this is a toy scenario that is hardly a challenging problem to solve
with a neural network and was primarily chosen as a small enough example to
fit in this paper. Nonetheless we hope that the current lack of ability to use
ITPs and ATPs together to prove the correctness of even such a trivial scenario,
illustrates the Vehicle’s utility.
3 The Vehicle Specification Language
TheVehicleConditionLanguage( VCL)isalanguageforwritinghigh-levelprob-
lem space specifications for neural networks. At its core is a dependently-typed
λ-calculus with built-in operations for logic, arithmetic and vectors operations.
The abstraction capabilities of the λ-calculus allow users to write modular and
reusable specifications and the dependent types allow the tracking of tensors
dimensions in the types to avoid trivial specification errors such as mismatched
dimensionsandout-of-boundsindexing.Thesyntaxofthecorelanguageisshown
in Figure 3.
⟨prog⟩::= [] | d :: p
⟨decl⟩ ∋d::=
|type⟨id⟩=τ
|def⟨id⟩:τ=e
|@network ⟨id⟩:τ
|@dataset ⟨id⟩:τ
|@parameter ⟨id⟩:τ
|@property ⟨id⟩:τ=e⟨kind⟩ ∋k::=Type|Nat|Num|Bool
⟨type⟩ ∋τ::=Π(x:k).τ|x|τ→τ|n∈N|Vecτ τ|
Index τ|Bool|Real
⟨expr⟩ ∋e::=Λ(x:k).e|e{τ}|λ(x:τ).e|e e|x|
r∈R|e+e|e*e|true|false|note|eande|
eore|e==e|e!=e|e<=e|e<e|ifethen eelse e|
forall (x:τ).e|exists (x:τ).e|i∈N|[e,..., e]
|e!e|foreach (x: n).e|foldr e e e
Fig.3: Syntax for the core calculus of the Vehicle language
A Vehicle program is a list of declarations. The keyword typedeclares a new
typesynonym,and defdeclaresanewvalue/function.Theotherfourdeclaration
types are unique to VCLand describe how a specification links to the outside
world. Firstly, @network declarations introduce an abstract neural network into
scope and @dataset and@parameter declarations can be used to introduce ex-
ternal values into scope. For example, in epsilon-ball robustness [9], the set of
points around which you want the network to be robust can be included using a
@dataset , and the value of ϵmay be varied using a @parameter . Crucially, only
the types, and not the values, of these three declarations need to be provided
when writing the specification. As discussed in Section 4, their value/implemen-
tation is provided by the user at compile time. Finally, @property declarations
are used to explicitly mark a declaration as a property we would like the neural
network to satisfy. A full explanation of the syntax for expressions can be foundVehicle: Bridging the Embedding Gap 7
1type Input = Tensor Rat [2]
2currentPosition = 0
3previousPosition = 1
4
5type Output = Tensor Rat [1]
6velocity = 0
7
8@network
9controller : Input -> Output
10
11 normalise : Input -> Input
12 normalise x = forall i . (x ! i + 4.0) / 8.0
13
14 safeInput : Input -> Bool
15 safeInput x = -3.25 <= x ! currentPosition <= 3.25 and
16 -3.25 <= x ! previousPosition <= 3.25
17
18 safeOutput : Output -> Bool
19 safeOutput x = let y = controller (normalise x) ! velocity in
20 -1.25 < y + 2 * x ! currentPosition - x ! previousPosition < 1.25
21
22 @property
23 safe : Bool
24 safe = forall x . safeInput x => safeOutput x
Fig.4: The specification of the safety property expressed in Vehicle for car’s
neural network controller.8 M.L. Daggitt et al.
online in the user manual [12]. Figure 4 shows how the specification of Φfrom
Section 2 can be expressed in Vehicle. The embedding function eis encoded
as the “normalise” function on Lines 11-12. Crucially, the safety conditions on
Lines 14-20 are expressed in terms of the problem space, (i.e. the values are in
metres and hence meaningful to the user) rather than the embedding space.
4 The Vehicle Compiler
The Vehicle compiler converts VCLspecifications into forms understandable by
the three backends: loss functions for training, queries for verification tools and
an interface for ITP code. Implemented in Haskell, the compiler is mostly eas-
ily simply by running ‘ pip install vehiclepy ’, which provides both Python
and command line bindings. Note: this command will not install Tensorflow,
Marabou or Agda which have to be installed separately.
4.1 Backend 1: Tensorflow Loss Functions for Training
There have been many different methods proposed for incorporating specifi-
cations into the training [14,21]. Vehicle currently supports a technique called
differentiable logic (DL) [17,28,36], which translates a logical specification into
a loss function that penalises the network from deviating from the desired prop-
erty. The resulting function is differentiable nearly everywhere with respect to
the network weights, and so can be combined with standard gradient descent
algorithms to train the network. This technique was chosen due to a) its gen-
erality - depending on the differentiable logic used, any well-typed Vehicle spec
can be converted to a loss function, and b) its flexibility - the logical training
can occur either during standard data-driven or reinforcement-based training or
can be added as an additional training step afterwards.
Concretely, given a @property p , the Vehicle compiles pinto a pure function
that takes the external resources (i.e. networks, datasets and parameters) as
inputs and return a numeric output representing "how false" the property pis.
The exact translation method is determined by which differential logic is used.
There have been many logics proposed, and Vehicle currently implements the
DL2 [17], Godel, Product, Lukasiewicz and Yager logics [28].
Compilation can be broken down into three main steps. Firstly, the property
is fully normalised. Next, for each quantified variable in the property, the portion
of the quantifier predicate that describes the domain of the variable (i.e. the set
of values the quantified variable is allowed to assume) is extracted and attached
to the quantifier node in the AST to allow for efficient sampling. Finally, all
remaining boolean operations are translated to numeric operations, according
to the differentiable logic being used. When the specification shown in Figure 4
is compiled with the DL2 logic, the following Python function is generated:
def lossFn(controller):
return sample(10, [[-4,-4],[4,4]], sum (lambda x: max(0.0,Vehicle: Bridging the Embedding Gap 9
sum([max(0.0, -3.25 - xi) + max(0.0, xi - 3.25) for xi in x]) *
(max(0.0, -1.25 - (controller([xi + 4.0) / 8.0 for xi in x])[0]
+ 2.0 * x[0] - x[1]))) +
max(0.0, - 1.25 + controller([xi + 4.0) / 8.0 for xi in x])[0]
+ 2.0 * x[0] - x[1]))))))
Note that the inside of this function is notmeant to be seen by the user.
Instead this function is returned by the call to load_loss_function in the
Vehicle python API. A representative sample of user code using it is shown
below:
1import vehicle_lang as vcl
2
3model = ... // create neural network
4
5loss_fn = vcl.load_loss_function(
6 specification_path="controller-spec.vcl",
7 property_name="safe",
8 target=vcl.DifferentiableLogic.DL2,
9)
10
11 for epoch in range(num_epochs):
12 with tf.GradientTape() as tape:
13 loss = loss_fn(controller=model)
14 grads = tape.gradient(loss, model.trainable_weights)
15 optimizer.apply_gradients(zip(grads, model.trainable_weights))
Fig.5: Minimal Python code for training a model to obey the specification in
Figure 4.
Note that although logical loss training is a very general technique and has
shown to be effective in practice [18], it does not provide formal guarantees that
the network will satisfy the specification after training.
4.2 Backend 2: Marabou Queries for Verification
The second backend can test whether a Vehicle specification representing Φ, is
satisfied by converting into a set of queries for the Marabou verifier [27] rep-
resenting Ξ. The specification is normalised via a normalisation by evaluation
algorithm [6] and the problem-space quantified variables are solved by Gaus-
sian [2] and Fourier-Motzkin [13] elimination in terms of the network input and
output variables in the embedding space. The end result of compilation is a tree
structure, with each node in the tree being either an andoror, and the leaves
being verifier queries. The queries generated for the specification from Section 2
are shown in Figure 6.10 M.L. Daggitt et al.
1x0 >= 0.09375
2x0 <= 0.90625
3x1 >= 0.09375
4x1 <= 0.90625
516.0 x0 -8.0 x1 +y0 <= 2.75
(a) Query 11x0 >= 0.09375
2x0 <= 0.90625
3x1 >= 0.09375
4x1 <= 0.90625
5-16.0 x0 8.0 x1 -y0 <= -5.25
(b) Query 2
Fig.6: Marabou queries generated from the specification in Figure 4. Note that
the numeric values have been transformed to the problem space.
The tree and the queries are written to disk in the form of a “Vehicle cache".
Using this cache, Vehicle can then traverse the tree, calling Marabou on queries
as required to check the status of the property. The verifier results are added to
the cache, and any counter-examples found for Ξin the input-output spaces are
then lifted to counter-examples for Φin the problem-result spaces.
It is important to note that Vehicle does not increase the size of the class of
specifications verifiable by Marabou, but instead increases the interpretability
of specifications and the ease of generating queries. If the embedding functions
are not linear functions, or contain alternating quantifiers, compilation will error
with a detailed explanation of why the specification cannot be compiled [11].
4.3 Backend 3: Agda Code for Interfacing with System Proofs
In the final backend the specification, representing the solution property Φ, is
convertedtoequivalentAgdacodesothatitcaninterfacewiththelargerproof Ψ
about the system. Unlike, the previous two backends, the translation is almost
trivial and the resulting Agda code is nearly identical to the original VCLspec-
ification. Therefore for space reasons, the code generated for the specification in
Figure 4 may be found in Appendix A.
As discussed in the introduction for performance reasons we can’t represent
the network in Agda itself. Therefore, the challenge facing this backend is to
maintain the interactivity of Agda whilst also maintaining the integrity of the
proof. In particular, the Agda proof should still fail if the ONNX network file
on disk is changed (e.g. because the network was retrained after being verified).
At the same time, we can’t simply re-verify the proof each time as interacting
with Agda requires regularly rechecking. The key to this is the Vehicle cache
discussed in Section 4.2. As well as the query tree, Vehicle also stores the hash
and the location of all networks and datasets used during compilation of the
verifier queries. Therefore when Agda queries the validity of the proof, Vehicle
can locate and rehash these external resources to check the integrity of the
verification result without having to recall Marabou.Vehicle: Bridging the Embedding Gap 11
5 Soundness of Vehicle
With such a complicated system, an obvious question is how do we guarantee
its soundness? To address this, we have come up with a formal semantics for
the Vehicle core language, and for the target languages of both the training and
the verifier backends. Using this we have proved not just the soundness of the
compilation to loss functions and verifier queries, but also that the loss function
andtheverifierqueriesgeneratedarelogicallyequivalent,i.e.thatwe’reverifying
the same property we trained for. These proofs have been formalised in Agda,
and can be found at [1].
Having said that, in contrast to neural networks which are usually imple-
mented using floating point semantics, the Vehicle syntax, and hence the proofs
described above, currently use rational number semantics. This is a well-known
source of unsoundness, and an issue that neural network verifiers themselves
have been shown to suffer from [25]. While addressing this unsoundness issue
is important future work, we believe that it does not invalidate the proposed
approach of Vehicle to neuro-symbolic program verification.
6 Conclusions and Related Work
In this paper we have identified the embedding gap as an existing problem in the
verificationofneural-symbolicprogramsanddescribedVehicle,thefirsttoolthat
aims to bridge that gap. We have shown how Vehicle facilitates proofs about the
correctness of neuro-symbolic programs by linking specifications to training and
ITPs, as well as just verifiers. This sets it apart from similar frameworks with
neural network specification DSLs such as DNNV [35] and CAISAR [20], which
focus primarily on improving interoperability between different solvers. We have
also demonstrated its utility, by verifying the correctness of a neuro-symbolic
car controller. We believe this to be the first ever modular proof of the complete
verification of a neuro-symbolic program that utilises both ATPs and ITPs.
Of course, Vehicle’s methodology can only be applied in domains where a
suitable specification is available, and therefore is not currently easily applicable
todomainssuchasNLPandvisionwhichdefyclearlydefinedcorrectnesscriteria.
Vehicle can therefore be seen as complementary to work such as [23] and [32]
which obtain formal statistical guarantees about neural networks used as sensors
in cyber-physical systems. Planned future work includes using Vehicle to verify
larger cyber-physical systems, which will necessitate supporting a further ITPs
more suitable to cyber-physical system verification, e.g. KeYmaera X [19] and
Isabelle/HOL [30]. A user manual and an in-depth tutorial for Vehicle can be
found online [10,12].
References
1. Atkey, R., Daggitt, M.L., Kokke, W.: Vehicle formalisation. https://github.com/
vehicle-lang/vehicle-formalisation (2024)12 M.L. Daggitt et al.
2. Atkinson, K.: An introduction to numerical analysis. John wiley & sons (1991)
3. Badue, C., Guidolini, R., Carneiro, R.V., Azevedo, P., Cardoso, V.B., Forechi, A.,
Jesus, L., Berriel, R., Paixao, T.M., Mutz, F., et al.: Self-driving cars: A survey.
Expert Systems with Applications 165, 113816 (2021)
4. Bagnall, A., Stewart, G.: Certifying the True Error: Machine Learning in Coq with
Verified Generalization Guarantees. In: Proceedings of the Thirty-Third AAAI
Conference on Artificial Intelligence. AAAI’19, AAAI Press (2019). https://doi.
org/10.1609/aaai.v33i01.33012662
5. Barras, B., Boutin, S., Cornes, C., Courant, J., Filliatre, J.C., Gimenez, E., Herbe-
lin, H., Huet, G., Munoz, C., Murthy, C., et al.: The Coq proof assistant reference
manual: Version 6.1. Ph.D. thesis, Inria (1997)
6. Berger, U., Schwichtenberg, H.: An inverse of the evaluation functional for typed
lambda-calculus (1991)
7. Boyer, R.S., Green, M.W., Moore, J.S.: The use of a formal simulator to verify a
simple real time control program. In: Beauty Is Our Business, pp. 54–66. Springer
(1990)
8. Brucker, A.D., Stell, A.: Verifying feedforward neural networks for classification
in Isabelle/HOL. In: Chechik, M., Katoen, J., Leucker, M. (eds.) Formal Methods
- 25th International Symposium, FM 2023, Lübeck, Germany, March 6-10, 2023,
Proceedings. Lecture Notes in Computer Science, vol. 14000, pp. 427–444. Springer
(2023). https://doi.org/10.1007/978-3-031-27481-7_24
9. Casadio, M., Komendantskaya, E., Daggitt, M.L., Kokke, W., Katz, G., Amir,
G., Refaeli, I.: Neural network robustness as a verification property: A princi-
pled case study. In: Shoham, S., Vizel, Y. (eds.) Computer Aided Verification
- 34th International Conference, CAV 2022, Haifa, Israel, August 7-10, 2022,
Proceedings, Part I. Lecture Notes in Computer Science, vol. 13371, pp. 219–
231. Springer (2022). https://doi.org/10.1007/978-3-031-13185-1_11 ,https:
//doi.org/10.1007/978-3-031-13185-1_11
10. Daggitt, M., Kokke, W., Komendantskaya, E., Atkey, R., Arnaboldi, L., Slusarz,
N., Casadio, M., Coke, B., Lee, J.: The vehicle tutorial: Neural network verification
with vehicle. In: Narodytska, N., Amir, G., Katz, G., Isac, O. (eds.) Proceedings
of the 6th Workshop on Formal Methods for ML-Enabled Autonomous Systems.
Kalpa Publications in Computing, vol. 16, pp. 1–5. EasyChair (2023). https://
doi.org/10.29007/5s2x ,https://easychair.org/publications/paper/Rkrv
11. Daggitt, M.L., Atkey, R., Kokke, W., Komendantskaya, E., Arnaboldi, L.: Com-
piling higher-order specifications to SMT solvers: How to deal with rejection
constructively. In: Krebbers, R., Traytel, D., Pientka, B., Zdancewic, S. (eds.)
Proceedings of the 12th ACM SIGPLAN International Conference on Certi-
fied Programs and Proofs, CPP 2023, Boston, MA, USA, January 16-17, 2023.
pp. 102–120. ACM (2023). https://doi.org/10.1145/3573105.3575674 ,https:
//doi.org/10.1145/3573105.3575674
12. Daggitt,M.L.,Kokke,W.,Bob,A.,Ślusarz,N.,Casadio,M.:Vehicle(2023), https:
//github.com/vehicle-lang/vehicle , accessed on 12.01.2024
13. Dantzig, G.B., Eaves, B.C., et al.: Fourier-Motzkin elimination and its dual. J.
Comb. Theory, Ser. A 14(3), 288–297 (1973)
14. Dash, T., Chitlangia, S., Ahuja, A., Srinivasan, A.: A review of some techniques
for inclusion of domain-knowledge into deep neural networks. Scientific Reports
12(1), 1040 (2022)
15. De Maria, E., Bahrami, A., l’Yvonnet, T., Felty, A., Gaffé, D., Ressouche, A.,
Grammont, F.: On the use of formal methods to model and verify neuronal
archetypes. Frontiers of Computer Science 16(3), 1–22 (2022)Vehicle: Bridging the Embedding Gap 13
16. Desmartin,R.,Passmore,G.O.,Komendantskaya,E.,Daggit,M.:CheckINN:Wide
Range Neural Network Verification in Imandra. In: PPDP 2022: 24th International
Symposium on Principles and Practice of Declarative Programming, Tbilisi, Geor-
gia, September 20 - 22, 2022. pp. 3:1–3:14. ACM (2022). https://doi.org/10.
1145/3551357.3551372 ,https://doi.org/10.1145/3551357.3551372
17. Fischer, M., Balunovic, M., Drachsler-Cohen, D., Gehr, T., Zhang, C., Vechev,
M.T.: DL2: training and querying neural networks with logic. In: Chaudhuri, K.,
Salakhutdinov, R. (eds.) Proceedings of the 36th International Conference on Ma-
chine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA. Pro-
ceedings of Machine Learning Research, vol. 97, pp. 1931–1941. PMLR (2019),
http://proceedings.mlr.press/v97/fischer19a.html
18. Flinkow, T., Pearlmutter, B.A., Monahan, R.: Comparing differentiable logics for
learning systems: A research preview. In: Farrell, M., Luckcuck, M., Gleirscher, M.,
Schwammberger, M. (eds.) Proceedings Fifth International Workshop on Formal
Methods for Autonomous Systems, FMAS@iFM 2023, Leiden, The Netherlands,
15th and 16th of November 2023. EPTCS, vol. 395, pp. 17–29 (2023). https:
//doi.org/10.4204/EPTCS.395.3 ,https://doi.org/10.4204/EPTCS.395.3
19. Fulton, N., Mitsch, S., Quesel, J.D., Völp, M., Platzer, A.: KeYmaeraX]: An ax-
iomatic tactical theorem prover for hybrid systems
20. Girard-Satabin, J., Alberti, M., Bobot, F., Chihani, Z., Lemesle, A.: Caisar:
A platform for characterizing artificial intelligence safety and robustness. In:
AISafety. CEUR-Workshop Proceedings, Vienne, Austria (Jul 2022), https://
hal.archives-ouvertes.fr/hal-03687211
21. Giunchiglia, E., Stoian, M.C., Lukasiewicz, T.: Deep Learning with Logical Con-
straints. In: Raedt, L.D. (ed.) Proceedings of the International Joint Conference
on Artificial Intelligence 2022. pp. 5478–5485. ijcai.org (2022). https://doi.org/
10.24963/ijcai.2022/767
22. Hasanbeig, H., Kroening, D., Abate, A.: Certified reinforcement learning with logic
guidance. Artificial Intelligence 322, 103949 (2023)
23. Hsieh, C., Li, Y., Sun, D., Joshi, K., Misailovic, S., Mitra, S.: Verifying controllers
with vision-based perception using safe approximate abstractions. IEEE Trans-
actions on Computer-Aided Design of Integrated Circuits and Systems 41(11),
4205–4216 (2022)
24. Hughes, A.: ChatGPT: Everything you need to know about OpenAI’s GPT-4 tool.
BBC Science Focus (2023)
25. Jia,K.,Rinard,M.:Exploitingverifiedneuralnetworksviafloatingpointnumerical
error. In: International Static Analysis Symposium. pp. 191–205. Springer (2021)
26. Katz, G., Barrett, C., Dill, D.L., Julian, K., Kochenderfer, M.J.: Reluplex: An
efficient smt solver for verifying deep neural networks. In: International Conference
on Computer Aided Verification. pp. 97–117. Springer (2017)
27. Katz, G., Huang, D.A., Ibeling, D., Julian, K., Lazarus, C., Lim, R., Shah, P.,
Thakoor, S., Wu, H., Zeljić, A., et al.: The Marabou framework for verification
and analysis of deep neural networks. In: International Conference on Computer
Aided Verification. pp. 443–452. Springer (2019)
28. van Krieken, E., Acar, E., van Harmelen, F.: Analyzing differentiable fuzzy logic
operators. Artificial Intelligence 302, 103602 (2022). https://doi.org/10.1016/
j.artint.2021.103602
29. Müller, M.N., Makarchuk, G., Singh, G., Püschel, M., Vechev, M.: Prima: general
and precise neural network certification via scalable convex hull approximations.
Proceedings of the ACM on Programming Languages 6(POPL), 1–33 (2022)14 M.L. Daggitt et al.
30. Nipkow, T., Wenzel, M., Paulson, L.C.: Isabelle/HOL: a proof assistant for higher-
order logic. Springer (2002)
31. Norell, U.: Dependently typed programming in Agda. In: International school on
advanced functional programming. pp. 230–266. Springer (2008)
32. Păsăreanu, C.S., Mangal, R., Gopinath, D., Getir Yaman, S., Imrie, C., Cali-
nescu, R., Yu, H.: Closed-loop analysis of vision-based autonomous systems: A
case study. In: International Conference on Computer Aided Verification. pp. 289–
303. Springer (2023)
33. Seshia, S.A., Sadigh, D., Sastry, S.S.: Toward verified artificial intelligence. Com-
munications of the ACM 65(7), 46–55 (2022)
34. Shi, G., Shi, X., O’Connell, M., Yu, R., Azizzadenesheli, K., Anandkumar, A.,
Yue, Y., Chung, S.J.: Neural lander: Stable drone landing control using learned
dynamics. In: 2019 international conference on robotics and automation (icra). pp.
9784–9790. IEEE (2019)
35. Shriver, D., Elbaum, S., Dwyer, M.B.: DNNV: A Framework for Deep Neural Net-
work Verification. In: Silva, A., Leino, K.R.M. (eds.) Computer Aided Verification.
pp. 137–150. Springer International Publishing, Cham (2021)
36. Slusarz, N., Komendantskaya, E., Daggitt, M.L., Stewart, R.J., Stark, K.: Logic
of differentiable logics: Towards a uniform semantics of DL. In: LPAR-24: The
International Conference on Logic for Programming, Artificial Intelligence and
Reasoning (2023)
37. Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I.,
Fergus, R.: Intriguing properties of neural networks. In: International Conference
on Learning Representations (2013), http://arxiv.org/abs/1312.6199
38. Wang, S., Zhang, H., Xu, K., Lin, X., Jana, S., Hsieh, C.J., Kolter, J.Z.: Beta-
CROWN: Efficient bound propagation with per-neuron split constraints for com-
plete and incomplete neural network verification. Advances in Neural Information
Processing Systems 34(2021)
39. Wüthrich,M.V.:Biasregularizationinneuralnetworkmodelsforgeneralinsurance
pricing. European Actuarial Journal 10(1), 179–202 (2020)
40. Yuan, L., Chen, D., Chen, Y.L., Codella, N., Dai, X., Gao, J., Hu, H., Huang, X.,
Li, B., Li, C., Liu, C., Liu, M., Liu, Z., Lu, Y., Shi, Y., Wang, L., Wang, J., Xiao,
B., Xiao, Z., Yang, J., Zeng, M., Zhou, L., Zhang, P.: Florence: A new foundation
model for computer vision (2021)
41. Zhang, H., Wang, S., Xu, K., Li, L., Li, B., Jana, S., Hsieh, C.J., Kolter, J.Z.:
General cutting planes for bound-propagation-based neural network verification.
Advances in Neural Information Processing Systems (2022)