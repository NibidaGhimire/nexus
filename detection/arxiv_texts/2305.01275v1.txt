Segment Anything is A Good Pseudo-label Generator
for Weakly Supervised Semantic Segmentation
Peng-Tao Jiang
State Key Lab of CAD&CG, Zhejiang UniversityYuqi Yang*
TMCC, CS, Nankai University
Abstract
Weakly supervised semantic segmentation with weak labels is a long-lived ill-
posed problem. Mainstream methods mainly focus on improving the quality of
pseudo labels. In this report, we attempt to explore the potential of ’prompt to
masks’ from the powerful class-agnostic large segmentation model, i.e., segment-
anything. Speciﬁcally, different weak labels are used as prompts to the segment-
anything model, generating precise class masks. The class masks are utilized to
generate pseudo labels to train the segmentation networks. We have conducted
extensive experiments on PASCAL VOC 2012 dataset. Experiments demonstrate
that segment-anything can serve as a good pseudo-label generator. The code will
be made publicly available.
1 Introduction
Semantic segmentation [ 26;44;6] is a classic computer vision task that aims to classify each pixel in
the image. Training segmentation models usually needs large-scale ﬁnely-annotated segmentation
datasets, such as PASCAL VOC [ 8], MS COCO [ 23], ADE20K [ 46]. However, constructing such
large-scale datasets consumes much time and cost, even using polygon annotations. Thus, in recent
years, researchers have attempted to focus on weakly supervised semantic segmentation that aims to
utilize cheaper annotations than pixel-level annotations to train segmentation models. The cheaper
annotations include image labels [ 11], points [ 3], scribbles [ 22], and bounding boxes [ 19]. Previous
mainstream works [ 16;12;20] follow an idea that utilizes the cheaper annotations as initial spatial
priors to generate pseudo labels [39] or learn afﬁnity propagation [22].
Recently, large models [ 4;5;9;29;27] have dominated computer vision and natural language pro-
cessing, which beneﬁt from large-scale data and billions of model parameters. A large segmentation
model, called segment-anything [17], is proposed for the segmentation ﬁeld. The segment-anything
model (SAM) can receive different kinds of spatial prompts and output several object masks, where
the spatial prompts include points, bounding boxes, and texts. We observe that object masks usually
have precise boundaries, which can facilitate the weakly supervised semantic segmentation task.
In this report, we propose to utilize SAM to generate pseudo labels and utilize them to train the
segmentation networks. Speciﬁcally, we attempt to explore different weak annotations as prompts for
SAM and generate object masks with precise boundaries. We present a detailed analysis about the
impact of different prompts on the quality of pseudo labels. Finally, we present the ﬁnal segmentation
results of different prompts. Using scribbles as prompts, we can generate precise pseudo labels with
an 89.7% mIoU score on PASCAL VOC 2012 train set, approximating ground-truth labels. The ﬁnal
segmentation model achieves a 76.6 % mIoU score on the test set.
* denotes equal contribution.
Preprint. Under review.arXiv:2305.01275v1  [cs.CV]  2 May 2023SAM
pointsboxesclass
aeroplaneperson
ImageObject masksPseudo labelFigure 1: Pipeline of our method. Text prompt is not available in SAM.
2 Method
In this section, we introduce how we utilize different weak labels to generate prompts for SAM. The
overall pipeline of our method is shown in Fig. 1. SAM can receive points, boxes, and texts as input
prompts and output the corresponding object masks located by these prompts. Note SAM does not
make the text prompt available now.
2.1 Image-level Labels
Image-level labels only contain information about which category exists in an image. It does not
provide any object localization information. Previous weakly supervised semantic methods [ 2;28;36]
usually utilize class activation maps (CAMs) [ 45;30;15] to generate pseudo labels. They mainly
focus on improving the localization ability of CAMs. Early works [ 38;10;13;32] aim to locate more
integral object regions as CAMs usually locate small object regions. They generate accurate pseudo
labels with the aid of saliency maps [ 25] that usually have precise object boundaries. Another line of
works [2; 1] exploits pixel afﬁnities to locate integral object regions with precise object boundaries.
In this report, we aim to explore the potential of large segmentation models, i.e., segment-anything, to
generate pseudo labels. We propose two methods to utilize image-level labels with SAM. (i)One is to
sample points on object regions located by CAMs and then utilize the sampled points as prompts. (ii)
Another is to generate object masks for all spatial locations ﬁrst and utilize BLIP-2 [ 21] to classify
each mask. In the following, we introduce these two methods in detail.
First, we study how to sample points from CAMs to generate pseudo labels. To generate point
prompts, we utilize two settings to sample points from CAMs. The ﬁrst is to utilize all conﬁdence
pixels in CAMs as a prompt. Another is to sample conﬁdence pixels from CAMs as a prompt, where
the sampled pixels exhibit higher values than their neighboring pixels within a given range. As shown
in Tab. 1, we can see that sampling conﬁdent pixels achieves a higher mIoU score. Besides, SAM
provides a mechanism that iteratively receives new point prompts for mask reﬁnement. It can be
seen that the iterative reﬁnement cannot improve the quality of pseudo labels. We analyze that this is
because point prompts located by CAMs have much noise, which will harm the reﬁnement. Finally,
when multiple classes exist in the image, they can serve as negative point prompts for other classes,
Table 1: Comparisons of mIoU scores under different settings. mIoU train denotes the mIoU score
of the pseudo segmentation labels on the training set.
Annotations All conﬁdent pixels Sample conﬁdent pixels Iterative input Negative points mIoU train
Image-level labels47.1
X 50.9
X 61.5
X X 59.4
X X 61.9
PointsX 69.2
X X 71.7
X X X 71.5
ScribblesX 74.6
X 81.0
X X 84.3
X X X 89.7
Bounding boxes X 91.5
2Image Pseudo label GT Image Pseudo label GT
Figure 2: Failure pseudo labels generated by SAM with image-level labels.
bringing 0.4% mIoU score improvement. In this report, we only exploit the basic CAMs to locate
point prompts. Better CAMs [ 35;37;40;43] can be further explored. In Fig. 2, we present several
failure cases from point prompts located by CAMs. In the top row, SAM generates the wrong object
masks due to the coarse locations by CAMs. In the second row, SAM locates a part of the dining
table because multiple objects are placed on the tables.
Furthermore, we utilize SAM with BLIP-2 [ 21] to generate pseudo labels. Speciﬁcally, we ﬁrst
generate masks for all strided points. Then the classiﬁcation of each mask is restricted to the target
classes with the background. The mIoU score of pseudo labels achieves 3.3% higher than AdvCAM
[18] with the reﬁnement of IRNet [ 2]. Such surprising performance indicates the effectiveness of
using SAM to generate pseudo labels.
2.2 Points
Point labels [ 3] locate one pixel in each object for all target classes, which can directly serve as a
prompt to SAM. For each point, SAM will generate corresponding object masks. We utilize these
masks to compose the ﬁnal pseudo labels. As point labels can be regarded as a particular case
of scribbles, the settings are nearly the same with scribble labels. As shown in Tab. 1, iteratively
inputting each pixel of a class achieves the highest mIoU score.
Table 2: Quantitative comparisons of the pseudo labels of different methods.
Annotations Methods Publication Train (%)
Image-level labelsCAM [45; 41] CVPR’16 47.1
AdvCAM [18] CVPR’21 55.6
AdvCAM+IRNet [1] CVPR’21 69.9
CLIP-ES [24] arXiv’22 70.8
CAM + SAM – 61.9
CLIP-ES + SAM – 72.4
SAM + BLIP-2 – 73.2
Points Point + SAM – 69.1
Scribbles Scribbles + SAM 89.7
Bounding boxes Bounding boxes + SAM – 91.5
2.3 Scribbles
Scribble labels [ 34] are a set of pixels in each object for all target classes. Scribbles provide more
object localization information than image-level labels and points. Lin et al. [22] utilized the graphical
model to propagate the scribble information to unknown pixels. Tang et al. [33] designed a normalized
cut loss to learn segmentation networks based on scribble labels.
We utilize scribble labels as the prompt to SAM. As scribble labels in each object contain multiple
pixels, there are several settings to input scribbles to SAM. We have conducted experiments for these
settings. As shown in Tab. 2, we ﬁnd that sampling 20% scribble pixels outperforms inputting all
scribble pixels in an object by 6.4%. Besides, iteratively inputting scribble pixels of a class can further
improve the performance by 3.3%. We analyze that iterative input is more effective for scribbles and
3points than image-level labels due to accurate point locations. Finally, when inputting the scribble
pixels of one class, the scribble pixels of other classes can be regarded as negative points. We can
see that adding negative points can further improve the quality of pseudo labels. Using the best
pseudo labels from scribble prompts, DeepLab-v2 [ 6] can reach 75.9% and 76.6% mIoU scores on
the validation and test sets, as shown in Tab. 3.
2.4 Bounding Boxes
Bounding box labels [ 7] provide a tight box for each object of a class. Given bounding box labels, we
send each box of a class to SAM and generate its corresponding object masks. As shown in Tab. 2,
it achieves 91.5% mIoU score on the train set, which are the best pseudo labels among all weak
annotations. Note we do not add the negative points for bounding box prompts as the bounding boxes
cannot provide accurate point locations.
Table 3: Quantitative comparisons of the pseudo labels of different methods.
Annotations Methods Publication Val (%) Test (%)
Image-level labelsAdvCAM [18] CVPR’21 68.1 68.0
EPS [20] CVPR’22 70.9 70.8
Image-level labels + SAM – 71.1 72.2
PointsWhatsPoint [3] ECCV’16 46.1 -
Points + SAM – 69.0 68.7
ScribblesScribbleSup [22] CVPR’16 63.1 -
NCLoss [33] CVPR’18 72.8 -
PSI [42] ICCV’21 74.9 -
Scribbles + SAM – 75.9 76.6
Bounding boxesWSSL [28] ICCV’15 60.6 62.2
BoxSup [7] ICCV’15 62.0 64.6
SDI [16] CVPR’17 69.4 -
Song et al. [31] CVPR’19 70.2 -
BBAM [19] CVPR’21 73.7 73.7
Bounding boxes + SAM – 76.3 75.8
3 Experiment Setting
All the experiments are conducted on PASCAL VOC 2012 dataset, which contain 10582/1449/1456
images in the train/val/test set. We utilize the third mask of SAM’s three output masks to generate
pseudo labels. Following [ 14], DeepLab-v2 [ 6] based on ResNet-101 is selected as the default
segmentation network, whose parameters are initialized using the COCO pre-trained model. We keep
the same training settings with [14].
4 Conclusion
In this report, we have conducted experiments to explore the potential of SAM for generating accurate
object masks. Experiments on PASCAL VOC 2012 dataset demonstrate that SAM can serve as a
good pseudo-label generator. In the future, we plan to conduct experiments on more complex datasets,
such as MS COCO. Besides, we plan to explore the potential of SAM for instance segmentation task.
References
[1]Jiwoon Ahn, Sunghyun Cho, and Suha Kwak. Weakly supervised learning of instance segmentation with
inter-pixel relations. In IEEE Conf. Comput. Vis. Pattern Recog. , pages 2209–2218, 2019.
[2]Jiwoon Ahn and Suha Kwak. Learning pixel-level semantic afﬁnity with image-level supervision for
weakly supervised semantic segmentation. In IEEE Conf. Comput. Vis. Pattern Recog. , pages 4981–4990,
2018.
4[3]Amy Bearman, Olga Russakovsky, Vittorio Ferrari, and Li Fei-Fei. What’s the point: Semantic segmenta-
tion with point supervision. In Eur. Conf. Comput. Vis. , pages 549–565, 2016.
[4]Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S
Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of
foundation models. arXiv preprint arXiv:2108.07258 , 2021.
[5]Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.
InAdv. Neural Inform. Process. Syst. , volume 33, pages 1877–1901, 2020.
[6]Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Deeplab:
Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs.
IEEE Trans. Pattern Anal. Mach. Intell. , 40(4):834–848, 2017.
[7]Jifeng Dai, Kaiming He, and Jian Sun. Boxsup: Exploiting bounding boxes to supervise convolutional
networks for semantic segmentation. In Int. Conf. Comput. Vis. , pages 1635–1643, 2015.
[8]Mark Everingham, SM Ali Eslami, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew
Zisserman. The pascal visual object classes challenge: A retrospective. Int. J. Comput. Vis. , 111(1):98–136,
2015.
[9]Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang,
and Yue Cao. Eva: Exploring the limits of masked visual representation learning at scale. arXiv preprint
arXiv:2211.07636 , 2022.
[10] Qibin Hou, PengTao Jiang, Yunchao Wei, and Ming-Ming Cheng. Self-erasing network for integral object
attention. In Advances in Neural Information Processing Systems , volume 31, pages 549–559, 2018.
[11] Zilong Huang, Xinggang Wang, Jiasi Wang, Wenyu Liu, and Jingdong Wang. Weakly-supervised semantic
segmentation network with deep seeded region growing. In IEEE Conf. Comput. Vis. Pattern Recog. , pages
7014–7023, 2018.
[12] Peng-Tao Jiang, Ling-Hao Han, Qibin Hou, Ming-Ming Cheng, and Yunchao Wei. Online attention
accumulation for weakly supervised semantic segmentation. IEEE Trans. Pattern Anal. Mach. Intell. , 2021.
[13] Peng-Tao Jiang, Qibin Hou, Yang Cao, Ming-Ming Cheng, Yunchao Wei, and Hong-Kai Xiong. Integral
object mining via online attention accumulation. In Int. Conf. Comput. Vis. , pages 2070–2079, 2019.
[14] Peng-Tao Jiang, Yuqi Yang, Qibin Hou, and Yunchao Wei. L2g: A simple local-to-global knowledge
transfer framework for weakly supervised semantic segmentation. In IEEE Conf. Comput. Vis. Pattern
Recog. , pages 16886–16896, 2022.
[15] Peng-Tao Jiang, Chang-Bin Zhang, Qibin Hou, Ming-Ming Cheng, and Yunchao Wei. Layercam: Exploring
hierarchical class activation maps for localization. IEEE Trans. Image Process. , 30:5875–5888, 2021.
[16] Anna Khoreva, Rodrigo Benenson, Jan Hosang, Matthias Hein, and Bernt Schiele. Simple does it: Weakly
supervised instance and semantic segmentation. In IEEE Conf. Comput. Vis. Pattern Recog. , pages 876–885,
2017.
[17] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete
Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint
arXiv:2304.02643 , 2023.
[18] Jungbeom Lee, Eunji Kim, and Sungroh Yoon. Anti-adversarially manipulated attributions for weakly and
semi-supervised semantic segmentation. In IEEE Conf. Comput. Vis. Pattern Recog. , pages 4071–4080,
2021.
[19] Jungbeom Lee, Jihun Yi, Chaehun Shin, and Sungroh Yoon. Bbam: Bounding box attribution map for
weakly supervised semantic and instance segmentation. In IEEE Conf. Comput. Vis. Pattern Recog. , pages
2643–2652, 2021.
[20] Seungho Lee, Minhyun Lee, Jongwuk Lee, and Hyunjung Shim. Railroad is not a train: Saliency as
pseudo-pixel supervision for weakly supervised semantic segmentation. In IEEE Conf. Comput. Vis.
Pattern Recog. , pages 5495–5505, 2021.
[21] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training
with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597 , 2023.
5[22] Di Lin, Jifeng Dai, Jiaya Jia, Kaiming He, and Jian Sun. Scribblesup: Scribble-supervised convolutional
networks for semantic segmentation. In IEEE Conf. Comput. Vis. Pattern Recog. , pages 3159–3167, 2016.
[23] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár,
and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Eur. Conf. Comput. Vis. , 2014.
[24] Yuqi Lin, Minghao Chen, Wenxiao Wang, Boxi Wu, Ke Li, Binbin Lin, Haifeng Liu, and Xiaofei He. Clip
is also an efﬁcient segmenter: A text-driven approach for weakly supervised semantic segmentation. arXiv
preprint arXiv:2212.09506 , 2022.
[25] Jiang-Jiang Liu, Qibin Hou, Ming-Ming Cheng, Jiashi Feng, and Jianmin Jiang. A simple pooling-based
design for real-time salient object detection. In IEEE Conf. Comput. Vis. Pattern Recog. , pages 3917–3926,
2019.
[26] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmenta-
tion. In IEEE Conf. Comput. Vis. Pattern Recog. , pages 3431–3440, 2015.
[27] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy V o, Marc Szafraniec, Vasil Khalidov, Pierre
Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual
features without supervision. arXiv preprint arXiv:2304.07193 , 2023.
[28] George Papandreou, Liang-Chieh Chen, Kevin P Murphy, and Alan L Yuille. Weakly-and semi-supervised
learning of a deep convolutional network for semantic image segmentation. In Int. Conf. Comput. Vis. ,
pages 1742–1750, 2015.
[29] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish
Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from
natural language supervision. In Int. Conf. Mach. Learn. , pages 8748–8763. PMLR, 2021.
[30] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and
Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In Int.
Conf. Comput. Vis. , pages 618–626, 2017.
[31] Chunfeng Song, Yan Huang, Wanli Ouyang, and Liang Wang. Box-driven class-wise region masking and
ﬁlling rate guided loss for weakly supervised semantic segmentation. In IEEE Conf. Comput. Vis. Pattern
Recog. , pages 3136–3145, 2019.
[32] Guolei Sun, Wenguan Wang, Jifeng Dai, and Luc Van Gool. Mining cross-image semantics for weakly
supervised semantic segmentation. In Eur. Conf. Comput. Vis. , pages 347–365, 2020.
[33] Meng Tang, Abdelaziz Djelouah, Federico Perazzi, Yuri Boykov, and Christopher Schroers. Normalized
cut loss for weakly-supervised cnn segmentation. In IEEE Conf. Comput. Vis. Pattern Recog. , pages
1818–1827, 2018.
[34] Bin Wang, Guojun Qi, Sheng Tang, Tianzhu Zhang, Yunchao Wei, Linghui Li, and Yongdong Zhang.
Boundary perception guidance: A scribble-supervised semantic segmentation approach. In Int. Joint Conf.
Artif. Intell. , 2019.
[35] Haofan Wang, Zifan Wang, Mengnan Du, Fan Yang, Zijian Zhang, Sirui Ding, Piotr Mardziel, and Xia Hu.
Score-cam: Score-weighted visual explanations for convolutional neural networks. In IEEE Conf. Comput.
Vis. Pattern Recog. Worksh. , pages 24–25, 2020.
[36] Xiang Wang, Shaodi You, Xi Li, and Huimin Ma. Weakly-supervised semantic segmentation by iteratively
mining common object features. In IEEE Conf. Comput. Vis. Pattern Recog. , pages 1354–1362, 2018.
[37] Yude Wang, Jie Zhang, Meina Kan, Shiguang Shan, and Xilin Chen. Self-supervised equivariant attention
mechanism for weakly supervised semantic segmentation. In IEEE Conf. Comput. Vis. Pattern Recog. ,
pages 12275–12284, 2020.
[38] Yunchao Wei, Jiashi Feng, Xiaodan Liang, Ming-Ming Cheng, Yao Zhao, and Shuicheng Yan. Object
region mining with adversarial erasing: A simple classiﬁcation to semantic segmentation approach. In
IEEE Conf. Comput. Vis. Pattern Recog. , pages 1568–1576, 2017.
[39] Yunchao Wei, Huaxin Xiao, Honghui Shi, Zequn Jie, Jiashi Feng, and Thomas S Huang. Revisiting dilated
convolution: A simple approach for weakly-and semi-supervised semantic segmentation. In IEEE Conf.
Comput. Vis. Pattern Recog. , pages 7268–7277, 2018.
[40] Tong Wu, Junshi Huang, Guangyu Gao, Xiaoming Wei, Xiaolin Wei, Xuan Luo, and Chi Harold Liu.
Embedded discriminative attention mechanism for weakly supervised semantic segmentation. In IEEE
Conf. Comput. Vis. Pattern Recog. , pages 16765–16774, 2021.
6[41] Zifeng Wu, Chunhua Shen, and Anton Van Den Hengel. Wider or deeper: Revisiting the resnet model for
visual recognition. Pattern Recogn. , 90:119–133, 2019.
[42] Jingshan Xu, Chuanwei Zhou, Zhen Cui, Chunyan Xu, Yuge Huang, Pengcheng Shen, Shaoxin Li, and
Jian Yang. Scribble-supervised semantic segmentation inference. In Int. Conf. Comput. Vis. , pages
15354–15363, 2021.
[43] Yazhou Yao, Tao Chen, Guo-Sen Xie, Chuanyi Zhang, Fumin Shen, Qi Wu, Zhenmin Tang, and Jian
Zhang. Non-salient region object mining for weakly supervised semantic segmentation. In IEEE Conf.
Comput. Vis. Pattern Recog. , pages 2623–2632, 2021.
[44] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing
network. In IEEE Conf. Comput. Vis. Pattern Recog. , pages 2881–2890, 2017.
[45] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. Learning deep features
for discriminative localization. In IEEE Conf. Comput. Vis. Pattern Recog. , pages 2921–2929, 2016.
[46] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing
through ade20k dataset. In IEEE Conf. Comput. Vis. Pattern Recog. , pages 633–641, 2017.
7