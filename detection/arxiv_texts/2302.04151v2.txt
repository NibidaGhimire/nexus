Policy Evaluation in Decentralized
POMDPs with Belief Sharing
MERT KAYAALP1, FATIMA GHADIEH2, ALI H. SAYED1
1Adaptive Systems Laboratory, ¬¥Ecole Polytechnique F ¬¥ed¬¥erale de Lausanne (EPFL), CH-1015, Lausanne, Switzerland
2American University of Beirut, Beirut 1107 2020, Lebanon
CORRESPONDING AUTHOR: M. Kayaalp. E-mails: mert.kayaalp@epÔ¨Ç.ch
The work of F . Ghadieh was performed while she was a student intern at the Adaptive Systems Laboratory, EPFL.
ABSTRACT Most works on multi-agent reinforcement learning focus on scenarios where the state of the
environment is fully observable. In this work, we consider a cooperative policy evaluation task in which
agents are not assumed to observe the environment state directly. Instead, agents can only have access to
noisy observations and to belief vectors. It is well-known that Ô¨Ånding global posterior distributions under
multi-agent settings is generally NP-hard. As a remedy, we propose a fully decentralized belief forming
strategy that relies on individual updates and on localized interactions over a communication network. In
addition to the exchange of the beliefs, agents exploit the communication network by exchanging value
function parameter estimates as well. We analytically show that the proposed strategy allows information
to diffuse over the network, which in turn allows the agents‚Äô parameters to have a bounded difference
with a centralized baseline. A multi-sensor target tracking application is considered in the simulations.
INDEX TERMS multi-agent reinforcement learning, distributed state estimation, partially observable
Markov decision process, belief state, value function learning
I. INTRODUCTION
Multi-agent reinforcement learning (MARL) [1], [2] is a
useful paradigm for determining optimal policies in sequen-
tial decision making tasks involving a group of agents.
MARL has been applied successfully in several contexts,
including sensor networks [3], [4], team robotics [5], and
video games [6], [7]. MARL owes this success in part to
recent developments in better function approximators such
as deep neural networks [8].
Many works on MARL focus on the case where agents
can directly observe the global state of the environment.
However, in many scenarios, agents can only receive partial
information about the state. The decentralized partially ob-
servable Markov decision process (Dec-POMDP) framework
[9] is applicable to these types of situations. However, a
large body of MARL work assumes that Dec-POMDPs
observe data that are deterministic and known functions
of the underlying state, which is not the case in general.
Consider, for example, robots that receive noisy observations
from their sensors. The underlying observation model is
stochastic in this case.
Under stochastic observation models, one common strat-
egy is to keep track of the posterior distribution (belief) overthe set of states, which is known to be a sufÔ¨Åcient statistic of
the history of the system [10], [11]. For single agents, this
posterior distribution can be obtained at each iteration with
the optimal Bayesian Ô¨Åltering recursion [12]. Unfortunately,
for multi-agent systems, forming this global posterior belief
requires aggregation of all data from across all agents in
general. The agents can form it in a distributed manner only
when they have access to the private information from other
agents in the network. And even when agents have access to
this level of global knowledge, the computational complexity
of forming the global posterior distribution is known to be
NP-hard [13] in addition to its large memory requirements.
Moreover, obtaining beliefs necessitates signiÔ¨Åcant knowl-
edge about the underlying model of the environment, which
is generally not available in practice.
Therefore, instead of forming beliefs, most MARL al-
gorithms [14]‚Äì[16] resort to a model-free and end-to-end
approach where agents try to simultaneously learn a pol-
icy and an embedding of the history that can replace the
beliefs (e.g., recurrent neural networks (RNNs)). Neverthe-
less, recent empirical works suggest that this model-free
approach can be sub-optimal when the underlying signals
of the environment are too weak to train a model such as
This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/
1arXiv:2302.04151v2  [cs.LG]  16 May 2023KAYAALP ET AL .: POLICY EVALUATION IN DECENTRALIZED POMDPs WITH BELIEF SHARING
RNN [17], [18]. Moreover, RNNs (or alternative machine
learning models) are usually treated as black boxes. In other
words, these algorithms lack model interpretability, which
is critical for trustworthy systems (see [19]). Furthermore,
even though end-to-end approaches have shown remarkable
performance empirically, they are still based on heuristics
and lack theoretical guarantees on their performance. Com-
pared to modular approaches, they are inefÔ¨Åcient in terms of
adaptability and generalization to similar tasks.
As an alternative, there is a recent interest towards improv-
ing belief-based MARL approaches [20]‚Äì[22]. These works
have focused on emulating conventional beliefs with genera-
tive models, or with models learned from action/observation
trajectories (in a supervised fashion). In this paper, we also
examine belief-based strategies for MARL. In particular, we
are interested in the multi-agent policy evaluation problem.
Our work complements [20]‚Äì[22] in the sense that we
assume that agents are already capable of forming local
beliefs with sufÔ¨Åcient knowledge (i.e., with learned local
likelihood and transition models) or with generative models.
Our focus is on the challenge of approximating the global
Bayesian posterior in a distributed manner.
Contributions .
We consider a setting where agents only get partial
observations from the underlying state of nature, as
opposed to prior work on MARL over networks [23]‚Äì
[31] that assume agents have full state information.
Moreover, as opposed to the literature on decentralized
stochastic control [32]‚Äì[35], in our setting, agents need
to learn their value functions from data. More specif-
ically, in our Dec-POMDP framework, agents only
know their local observations, actions, and rewards but
they are allowed to communicate with their immediate
neighbors over a graph. In the proposed strategy (Al-
gorithm 2), agents exchange both their belief and value
function estimates.
We show in Theorem 1 that by exchanging beliefs,
agents keep a bounded disagreement with the global
posterior distribution, which requires fusing all ob-
servations and actions. Also, exchanging value func-
tion parameters enables agents to cluster around the
network centroid for sufÔ¨Åciently small learning rates
(Theorem 2). Furthermore, we prove that the network
centroid attains a bounded difference with a strategy
that requires centralized training (Theorem 3).
By means of simulations, we illustrate that agents
attain a small mean-square distance from the network
centroid. Moreover, the squared Bellman error (SBE)
averaged over the network is shown to be comparable
to the SBE of the centralized strategy.
Paper Organization . In Sec. II, we present additional re-
lated work. In Sec. III, for ease of exposition and introducing
notation, we describe the problem in single-agent setting. In
Sec. IV, we propose algorithms for multi-agent policy eval-uation. Sec. V includes the theoretical results, and Sec. VI
includes numerical simulations.
II. OTHER RELATED WORK
Our proposed strategy is based on temporal-difference (TD)
learning [36], [37], and makes use of function approxima-
tion. TD-learning for POMDPs are considered in [38], [39],
and function approximations are incorporated in [40], [41],
albeit in single-agent setting. The main contribution of the
present work is to the networked multi-agent setting.
A plethora of work studies decentralized policy evaluation
over networks [23]‚Äì[31]. Distributed versions of the TD-
learning with linear function approximations are considered
in [29]‚Äì[31]. However, these works assume that either the
global state, or a deterministic function of it, is available
to all agents. They overlook the stochastic nature of ob-
servations that takes place in many real-world applications.
Also in deterministic setting, the works [42], [43] examine
distributed linear quadratic control task when agents can
observe local states only. In particular, [43] proposes a
cooperative strategy for tracking the global state that exploits
networked communication between agents. However, in this
strategy, global state estimation at each iteration is indepen-
dent of the previous estimations. It ignores the correlation
between consecutive states. Furthermore, communication be-
tween the agents is utilized only for global state estimation,
and not utilized for local Q-function estimate sharing. In
contrast, in the present work, (i)observations are stochastic,
(ii)agents take advantage of the transition model of the state,
and(iii)they exchange value function parameters with their
neighbors as well.
Our work is also related to the Ô¨Åeld of decentralized
stochastic control [32] and dynamic team theory [33]. This
Ô¨Åeld studies problems in which different decision-makers
have access to different sets of information while working
towards a common team goal. Typically, these problems
are deÔ¨Åned by an information structure that speciÔ¨Åes which
agents have access to which pieces of information (e.g.,
observations or actions) [44], [45]. Some approaches to
solving these problems rely on the common information that
arises from partial history sharing to allother agents [35],
[46], [47]. In our networked setting, agents exchange value
function parameters or beliefs at each iteration, without ex-
plicitly exchanging raw data, with their immediate neighbors
only. Nonetheless, repeated application of this procedure
causes information to mix and diffuse throughout the whole
network. Moreover, most existing works in the decentralized
stochastic control literature assume full model knowledge of
the system, whereas we consider the case of learning from
data since the reward model is not known a priori. Also, shar-
ing value function parameters and beliefs instead of raw data
makes our algorithm advantageous in terms of privacy and
scalability. A similar approach is considered in [34], where
the author proposes a belief-sharing pattern for decentralized
control, rather than explicit information sharing as in prior
2work. However, they use a belief propagation algorithm over
acyclic graphs, while we use a diffusion-based belief-sharing
algorithm over cyclic networks. In addition, [34] considers
the planning problem only whereas in this work we consider
the policy evaluation problem, which requires learning from
data.
For constructing local beliefs that approximate the global
Bayesian posterior, we extend the diffusion HMM strategy
(DHS) [48], [49]. This algorithm requires only one round of
communication per state change, as opposed to other strate-
gies [50], [51] that require multiple rounds of communication
until network consensus at each iteration. Also, in contrast to
other distributed Bayesian Ô¨Åltering algorithms [52], it does
not combine likelihoods of data from different time instants.
Instead, likelihoods are combined with time-adjusted beliefs.
These properties make DHS communication efÔ¨Åcient and
successful in tracking highly dynamic state transitions. Note
that [48], [49] deal with state estimation task only, and there
are no rewards or actions in their setting. Therefore, we make
proper modiÔ¨Åcations to the algorithm in the sequel.
In addition to these, the analysis in the current work is
related to literature on the distributed optimization over
networks [53]‚Äì[56]. In particular, we adopt the two-step
approach from [57]‚Äì[59]. In the Ô¨Årst step, these works
establish that agents cluster around the network centroid,
and then, they show that this centroid converges to a
neighborhood of the optimal solution, under constant
learning rates. However, their focus is on optimization
and supervised learning rather than reinforcement learning,
which creates non-trivial distinctions in the analysis.
Notation. Random variables are denoted in bold. For K
vectorsw1;w2;:::;wK2RMof dimension M1each,
and for arbitrary matrices fA;Bg, the notation colfwkgK
k=1
and diagfA;Bgstand for
colfwkgK
k=1=2
6664w1
w2
...
wK3
7775;diagfA;Bg=
A0
0B
:(1)
The`p-norm for a vector wis represented bykwkp, while
the`p-induced norm for a matrix Ais represented bykAkp.
To simplify the notation, we use kwkandkAkto denote the
`2-norm, without explicitly stating the subscript. The all-
ones vector of dimension Kis denoted by 1K. The symbol

represents the Kronecker product. The Kullback-Leibler
divergence [60] between two distributions 1;2is denoted
byDKL(1jj2). We use the notation ‚Äúproportional to‚Äù, i.e.,
/, whenever the LHS of the expression is the normalized
version of the RHS. For example, for s2S and function f:
(s)/f(s)()(s) =f(s)P
s02Sf(s0): (2)III. PRELIMINARIES
In this work, we are interested in multi-agent policy evalua-
tion under partially observable stochastic environments. For
clarity of the exposition and to motivate the notation, we
brieÔ¨Çy review the procedure of single-agent policy evaluation
under both fully and partially observable states.
A. Fully-Observable Case
For modeling a learning agent under fully observable and
dynamic environments, the traditional setting is a Ô¨Ånite
Markov Decision Process (MDP). An MDP is deÔ¨Åned by
the quintuple (S;A;T;r;), whereSis a set of states
with cardinalityjSj=S,Ais a set of actions, Tis a
transition model where T(s0ja;s)denotes the probability
of transitioning from s2 S tos02 S when the agent
executes action a2A ,r(s;a;s0)denotes the reward the
agent receives when it executes action aand the environment
transitions from state stos0, and2[0;1)is a discount
factor that determines the importance given to immediate
rewards (!0)or the total reward (!1).
The goal of policy evaluation is to learn the value function
V(s)of a target policy (ajs), where the value function is
the expected return if the agent starts from state sand follows
policy, namely,
V(s) =Eh1X
i=0ir(si;ai;si+1)js0=si
; (3)
wheresiis the state at time iandaiis the action chosen
by the agent according to the policy, ai(ajsi). In many
applications, the state space is too large (or inÔ¨Ånite), which
makes it impractical to keep track of the value function for all
states. Therefore, function approximations are used to reduce
the dimension of the problem. For instance, linear approx-
imations, which are the focus of the theoretical analysis of
this work, correspond to using a parameter w2RMto
approximate V(s)(s)Tw, where:S!RMis a
pre-deÔ¨Åned feature mapping for representing state s.
A standard stochastic approximation algorithm to learn
wfrom data is TD-learning [19], [36] such as the TD(0)
strategy [61] and variations thereof. If we denote the value
function estimate at w2RMbybV(s;w),(s)Tw, then,
under this strategy, the agent Ô¨Årst computes the TD-error i
at timeiby using the observed transition tuple (si;ri;si+1):
i=ri+bV(si+1;wi) bV(si;wi); (4)
whereri,r(si;ai;si+1)is the instantaneous reward at
timei. Subsequently, the agent uses this error to update the
current parameter estimate wito
wi+1=wi+irwbV(si;wi); (5)
where>0is the learning rate, and
rwbV(si;wi) =(si) (6)
for the linear function approximation case. This algorithm
can be viewed as a ‚Äústochastic gradient algorithm‚Äù where
the effective stochastic gradient is gi, i(si). In this
3KAYAALP ET AL .: POLICY EVALUATION IN DECENTRALIZED POMDPs WITH BELIEF SHARING
work, we consider an `2-regularized version of the algorithm,
which changes the update step (5) to
wi+1= (1 2)wi+irwbV(si;wi); (7)
where >0is a constant hyper-parameter. As opposed to
supervised learning, regularization is rather under-explored
in reinforcement learning, with notable exceptions in [62],
[63]. However, recent work [64], [65] suggests that regular-
ization can increase generalization and sample-efÔ¨Åciency in
function approximation with over-parameterized models.
B. Partially-Observable Case
In many applications, the agent does not directly observe
the statesi. For instance, a robot may receive noisy and
partially informative observations from its sensors about the
environment. The observation ithat the agent receives at
timeiis generally assumed to be distributed according to
some likelihood function linking it to the unobservable state,
say,iL(jsi), which is conditioned on si. In these
scenarios, the agent will need to estimate the latent state
Ô¨Årst from the observations. To do so, the agent will need
to learn a probability vector i2M (S)over the set of
statesS, which is called the belief vector [10], [19]. Here,
M(S)denotes the S-dimensional probability simplex, and
the entryi(s)2[0;1]of the belief vector quantiÔ¨Åes the
conÔ¨Ådence the agent has about state sbeing the true state
at timei. The value of i(s)corresponds to the posterior
probability of sconditioned on the action-observation history
(a.k.a. trajectory):
Fi,fi;ai 1;i 1;:::g; (8)
which means:
i(s),P(si=sjFi): (9)
This posterior satisÔ¨Åes the following temporal recursion [10],
[12], [19]:
i(s)/L(ijs)i(s); (10)
wherei(s)is the time-adjusted prior deÔ¨Åned by
i(s),P(si=sjFa
i 1)=X
s02ST(sjs0;ai 1)i 1(s0):(11)
Here,Fa
i 1is the collection of past observations and actions,
i.e.,
Fa
i 1,fai 1;i 1;ai 2;:::g; (12)
where it is important to notice that Fi=fig[Fa
i 1.
If beliefs are used as substitutes for hidden states, then
partially-observable MDPs (POMDPs) can be treated as
continuous MDPs, since beliefs are continuous even if the
number of states is Ô¨Ånite. In this way, the policy evaluation
problem would correspond to evaluating V()where the
value function is now deÔ¨Åned as the expected return when
the agent starts from the belief state and follows the policy
(aj), namely [10], [19]:
V() =Eh1X
i=0irij0=i
: (13)Observe that, in contrast to the fully-observable case, the
agent now chooses action aiaccording to the policy
ai(aji), which is conditioned on the belief vector.
Algorithm (4)‚Äì(7) can be adjusted for POMDPs by using
the belief vectors (i;i+1)instead of the states (si;si+1).
Thus, we let
i=ri+bV(i+1;wi) bV(i;wi); (14)
and
wi+1= (1 2)wi+irwbV(i;wi); (15)
where the approximations bV(;w)are computed by us-
ing the feature vectors (), now dependent on , to
evaluatebV(;w),()Tw. Note that from now on
:M(S)!RMis a different feature mapping that
representsinstead ofs, and agents‚Äô goal is to learn w
that satisÔ¨Åes V()()Tw.
Observe from (10)‚Äì(11) that in order for the agent to
update the belief vectors (i;i+1), it needs to know the
transition model Tand the likelihood functions L(ijs)for
each state. However, the agent does not need to know the
underlying reward model r. It can use instantaneous reward
samplesrito run the algorithm. In this sense, the algorithm
is a mixture of model-based and model-free reinforcement
learning. Motivation for this approach is at least two-fold.
First, in some applications, learning the transition and ob-
servation models from data is inherently easier than learning
the reward function. This is because the reward function can
depend on some latent characteristics of the environment or
some human expert, which may be challenging to estimate.
One example where this scenario can arise is autonomous
cars [66]. In this case, the observations from environmental
sensors and cameras are processed with a learned likelihood
model such as a convolutional neural network. The transition
dynamics of the car depends on various parameters such as
speed, acceleration, position, and incline, and can be mod-
eled based on physics laws and mapping of the surroundings.
However, learning a reward function for this application is
notoriously difÔ¨Åcult, as it is challenging to cover all possible
situations [67]. Second, the agent can still run (14)‚Äì(15) even
if beliefs are not formed through (10)‚Äì(11), but estimated by
some other approach, as in [20]‚Äì[22].
IV. MULTI-AGENT POLICY EVALUATION
We now consider a set KofKcooperative agents that aim to
evaluate the average value function under a joint policy =
fkgK
k=1that consists of individual policies k. The frame-
work we consider is a decentralized POMDP (Dec-POMDP)
[9], which is deÔ¨Åned by the sextuple (S;Ak;Ok;T;rk;).
Here, the set of states Sand the transition model Tare
common to all agents, where the notation T(sjs0;a)now
speciÔ¨Åes the probability that the environment transitions
froms0toswhen the agents execute the joint action
a=fakgK
k=1. The individual action akof each agent k
takes values from the set Ak, andrk(s;a;s0)is the local
rewardkgets when the agents execute the collection of
4actionsaand the environment transitions from stos0. Note
that this setting covers general teamwork scenarios where
the local reward of an individual agent can be dependent on
all actions, and not only on its own actions. SpeciÔ¨Åcally,
it covers the scenarios that all agents observe the same
reward, i.e.,rk(s;a;s0) =r(s;a;s0);8k2K . Remember
that agents receive instantaneous rewards as they progress
through the POMDP, and they are not required to know the
joint action afrom all agents. Moreover, Okis a set of
private observations. At each time instant i, agentkreceives
observationk;i2 Okemitted by state si, and assumed
to be distributed according to the local marginal likelihood
Lk(kjsi).
Similar to the single-agent case, Dec-POMDPs can be
treated as multi-agent belief MDPs by replacing the hidden
states with joint centralized beliefs deÔ¨Åned by [9, Chap. 2]
i(s),P(si=sjFi)/L(ijs)i(s): (16)
Here,Fidenotes the history of all observations and past
actions from across all agents until time i, where in the
deÔ¨Ånition (8), i,fk;igK
k=1is now the aggregate of
the observations from across the network, and ai 1is a
tuple aggregating actions from all agents at time i 1.
Moreover, under spatial independence, the joint likelihood
L(ijs)appearing in (16) is given by
L(ijs) =KY
k=1Lk(k;ijs): (17)
In a manner similar to the single-agent case, the belief i(s)
is the time-adjusted prior conditioned on Fa
i 1(12):
i(s),P(si=sjFa
i 1)=X
s02ST(sjs0;ai 1)i 1(s0):(18)
The goal of policy evaluation is to learn the team value
function, which is the expected average reward of all agents
starting from some belief state , i.e.,
V() =Eh1X
i=0i1
KKX
k=1rk;i0=i
; (19)
whererk;idenotes the instantaneous local reward agent k
gets at time i.
There is one major inconvenience with this approach. In
order to compute the joint belief (16), it is necessary to
fuse all observations and actions from across the agents
in a central location. This is possible in settings where
there exists a fusion center. However, many applications rely
solely on localized processing. In the following, we discuss
and compare two strategies for multi-agent reinforcement
learning under partial observations: (i)a centralized strategy,
(ii)and a fully decentralized strategy.
A. Centralized Strategy
In the fully centralized strategy, the state estimation and pol-
icy evaluation phases are centralized and, hence, the setting
is equivalent to a single-agent POMDP, already discussed in
Sec. III-B, using the joint likelihood L(ijs)and the averagerewardri,K 1PK
k=1rk;i. The fusion center computes
the joint belief (16), and agents take actions based on this
joint belief, i.e., ak;ik(akji). The fusion center then
computes the centralized TD-error:
i=ri+bV(i+1;wi) bV(i;wi); (20)
and updates the estimate to
wi+1= (1 2)wi+irwbV(i;wi): (21)
This construction is listed under Algorithm 1.
Algorithm 1 Centralized policy evaluation under POMDPs
1:set initial prior 0(s)>0,8s2S
2:initializew0
3:whilei0do
4: each agentkobservesk;i
5: collect all observations i,fk;igK
k=1andevaluate
i(s)/L(ijs)i(s) (22)
6: foreach agentk2K do
7: Take actionak;ik(akji)
8: Get rewardrk;i=rk(si;ai;si+1)
9: end for
10: then, evolve
i+1(s) =X
s02ST(sjs0;ai)i(s0) (23)
11: average the rewardsri=1
KPK
k=1rk;i
12: update the model:
13:
i=ri+bV(i+1;wi) bV(i;wi) (24)
wi+1= (1 2)wi+irwbV(i;wi) (25)
14:end while
B. Decentralized Strategy
The centralized strategy is disadvantageous in the sense
that(i)failure of the fusion center results in failure of the
system; (ii)there can be communication bottlenecks at the
fusion center; (iii)and agents can be spatially distributed to
begin with. Therefore, in this section, we propose a fully
decentralized strategy for policy evaluation where agents
communicate with their immediate neighbors only.
1) Decentralized Network Model
We refer to Fig. 1 and assume that the graph is strongly
connected [53], which means that paths exist connecting
any pair of agents (`;k)in both directions, and in addition,
there exists at least one agent in the graph that does not
discard its own information (i.e., ckk>0for at least one
agentk). Under this assumption, the combination matrix
C= [c`k], where entry c`k0scales the information agent
kreceives from agent `, becomes primitive. If two agents
5KAYAALP ET AL .: POLICY EVALUATION IN DECENTRALIZED POMDPs WITH BELIEF SHARING
1<latexit sha1_base64="r4j53QRufGSBu66EFFOs0fGzpjA=">AAAB53icbVBNS8NAEJ34WetX1aOXxSJ4KokI6q3oxWMLxhbaUDbbSbt2swm7G6GE/gIvHlS8+pe8+W/ctjlo64OBx3szzMwLU8G1cd1vZ2V1bX1js7RV3t7Z3duvHBw+6CRTDH2WiES1Q6pRcIm+4UZgO1VI41BgKxzdTv3WEyrNE3lvxikGMR1IHnFGjZWaXq9SdWvuDGSZeAWpQoFGr/LV7Scsi1EaJqjWHc9NTZBTZTgTOCl3M40pZSM6wI6lksaog3x26IScWqVPokTZkobM1N8TOY21Hseh7YypGepFbyr+53UyE10FOZdpZlCy+aIoE8QkZPo16XOFzIixJZQpbm8lbEgVZcZmU7YheIsvLxP/vHZd85oX1fpNkUYJjuEEzsCDS6jDHTTABwYIz/AKb86j8+K8Ox/z1hWnmDmCP3A+fwDpO4yJ</latexit><latexit sha1_base64="r4j53QRufGSBu66EFFOs0fGzpjA=">AAAB53icbVBNS8NAEJ34WetX1aOXxSJ4KokI6q3oxWMLxhbaUDbbSbt2swm7G6GE/gIvHlS8+pe8+W/ctjlo64OBx3szzMwLU8G1cd1vZ2V1bX1js7RV3t7Z3duvHBw+6CRTDH2WiES1Q6pRcIm+4UZgO1VI41BgKxzdTv3WEyrNE3lvxikGMR1IHnFGjZWaXq9SdWvuDGSZeAWpQoFGr/LV7Scsi1EaJqjWHc9NTZBTZTgTOCl3M40pZSM6wI6lksaog3x26IScWqVPokTZkobM1N8TOY21Hseh7YypGepFbyr+53UyE10FOZdpZlCy+aIoE8QkZPo16XOFzIixJZQpbm8lbEgVZcZmU7YheIsvLxP/vHZd85oX1fpNkUYJjuEEzsCDS6jDHTTABwYIz/AKb86j8+K8Ox/z1hWnmDmCP3A+fwDpO4yJ</latexit><latexit sha1_base64="r4j53QRufGSBu66EFFOs0fGzpjA=">AAAB53icbVBNS8NAEJ34WetX1aOXxSJ4KokI6q3oxWMLxhbaUDbbSbt2swm7G6GE/gIvHlS8+pe8+W/ctjlo64OBx3szzMwLU8G1cd1vZ2V1bX1js7RV3t7Z3duvHBw+6CRTDH2WiES1Q6pRcIm+4UZgO1VI41BgKxzdTv3WEyrNE3lvxikGMR1IHnFGjZWaXq9SdWvuDGSZeAWpQoFGr/LV7Scsi1EaJqjWHc9NTZBTZTgTOCl3M40pZSM6wI6lksaog3x26IScWqVPokTZkobM1N8TOY21Hseh7YypGepFbyr+53UyE10FOZdpZlCy+aIoE8QkZPo16XOFzIixJZQpbm8lbEgVZcZmU7YheIsvLxP/vHZd85oX1fpNkUYJjuEEzsCDS6jDHTTABwYIz/AKb86j8+K8Ox/z1hWnmDmCP3A+fwDpO4yJ</latexit><latexit sha1_base64="r4j53QRufGSBu66EFFOs0fGzpjA=">AAAB53icbVBNS8NAEJ34WetX1aOXxSJ4KokI6q3oxWMLxhbaUDbbSbt2swm7G6GE/gIvHlS8+pe8+W/ctjlo64OBx3szzMwLU8G1cd1vZ2V1bX1js7RV3t7Z3duvHBw+6CRTDH2WiES1Q6pRcIm+4UZgO1VI41BgKxzdTv3WEyrNE3lvxikGMR1IHnFGjZWaXq9SdWvuDGSZeAWpQoFGr/LV7Scsi1EaJqjWHc9NTZBTZTgTOCl3M40pZSM6wI6lksaog3x26IScWqVPokTZkobM1N8TOY21Hseh7YypGepFbyr+53UyE10FOZdpZlCy+aIoE8QkZPo16XOFzIixJZQpbm8lbEgVZcZmU7YheIsvLxP/vHZd85oX1fpNkUYJjuEEzsCDS6jDHTTABwYIz/AKb86j8+K8Ox/z1hWnmDmCP3A+fwDpO4yJ</latexit>2<latexit sha1_base64="T+EERirJh67ollwP0uc+cjp/T5k=">AAAB53icbVBNS8NAEJ3Ur1q/qh69LBbBU0mKoN6KXjy2YGyhDWWznbRrN5uwuxFK6S/w4kHFq3/Jm//GbZuDtj4YeLw3w8y8MBVcG9f9dgpr6xubW8Xt0s7u3v5B+fDoQSeZYuizRCSqHVKNgkv0DTcC26lCGocCW+Hodua3nlBpnsh7M04xiOlA8ogzaqzUrPXKFbfqzkFWiZeTCuRo9Mpf3X7CshilYYJq3fHc1AQTqgxnAqelbqYxpWxEB9ixVNIYdTCZHzolZ1bpkyhRtqQhc/X3xITGWo/j0HbG1Az1sjcT//M6mYmuggmXaWZQssWiKBPEJGT2NelzhcyIsSWUKW5vJWxIFWXGZlOyIXjLL68Sv1a9rnrNi0r9Jk+jCCdwCufgwSXU4Q4a4AMDhGd4hTfn0Xlx3p2PRWvByWeO4Q+czx/qvoyK</latexit><latexit sha1_base64="T+EERirJh67ollwP0uc+cjp/T5k=">AAAB53icbVBNS8NAEJ3Ur1q/qh69LBbBU0mKoN6KXjy2YGyhDWWznbRrN5uwuxFK6S/w4kHFq3/Jm//GbZuDtj4YeLw3w8y8MBVcG9f9dgpr6xubW8Xt0s7u3v5B+fDoQSeZYuizRCSqHVKNgkv0DTcC26lCGocCW+Hodua3nlBpnsh7M04xiOlA8ogzaqzUrPXKFbfqzkFWiZeTCuRo9Mpf3X7CshilYYJq3fHc1AQTqgxnAqelbqYxpWxEB9ixVNIYdTCZHzolZ1bpkyhRtqQhc/X3xITGWo/j0HbG1Az1sjcT//M6mYmuggmXaWZQssWiKBPEJGT2NelzhcyIsSWUKW5vJWxIFWXGZlOyIXjLL68Sv1a9rnrNi0r9Jk+jCCdwCufgwSXU4Q4a4AMDhGd4hTfn0Xlx3p2PRWvByWeO4Q+czx/qvoyK</latexit><latexit sha1_base64="T+EERirJh67ollwP0uc+cjp/T5k=">AAAB53icbVBNS8NAEJ3Ur1q/qh69LBbBU0mKoN6KXjy2YGyhDWWznbRrN5uwuxFK6S/w4kHFq3/Jm//GbZuDtj4YeLw3w8y8MBVcG9f9dgpr6xubW8Xt0s7u3v5B+fDoQSeZYuizRCSqHVKNgkv0DTcC26lCGocCW+Hodua3nlBpnsh7M04xiOlA8ogzaqzUrPXKFbfqzkFWiZeTCuRo9Mpf3X7CshilYYJq3fHc1AQTqgxnAqelbqYxpWxEB9ixVNIYdTCZHzolZ1bpkyhRtqQhc/X3xITGWo/j0HbG1Az1sjcT//M6mYmuggmXaWZQssWiKBPEJGT2NelzhcyIsSWUKW5vJWxIFWXGZlOyIXjLL68Sv1a9rnrNi0r9Jk+jCCdwCufgwSXU4Q4a4AMDhGd4hTfn0Xlx3p2PRWvByWeO4Q+czx/qvoyK</latexit><latexit sha1_base64="T+EERirJh67ollwP0uc+cjp/T5k=">AAAB53icbVBNS8NAEJ3Ur1q/qh69LBbBU0mKoN6KXjy2YGyhDWWznbRrN5uwuxFK6S/w4kHFq3/Jm//GbZuDtj4YeLw3w8y8MBVcG9f9dgpr6xubW8Xt0s7u3v5B+fDoQSeZYuizRCSqHVKNgkv0DTcC26lCGocCW+Hodua3nlBpnsh7M04xiOlA8ogzaqzUrPXKFbfqzkFWiZeTCuRo9Mpf3X7CshilYYJq3fHc1AQTqgxnAqelbqYxpWxEB9ixVNIYdTCZHzolZ1bpkyhRtqQhc/X3xITGWo/j0HbG1Az1sjcT//M6mYmuggmXaWZQssWiKBPEJGT2NelzhcyIsSWUKW5vJWxIFWXGZlOyIXjLL68Sv1a9rnrNi0r9Jk+jCCdwCufgwSXU4Q4a4AMDhGd4hTfn0Xlx3p2PRWvByWeO4Q+czx/qvoyK</latexit>4<latexit sha1_base64="WBHF4kMSevXodtV6Szf35fn3/Ls=">AAAB53icbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoN6KXjy2YGyhDWWznbRrN5uwuxFK6S/w4kHFq3/Jm//GbZuDtj4YeLw3w8y8MBVcG9f9dgpr6xubW8Xt0s7u3v5B+fDoQSeZYuizRCSqHVKNgkv0DTcC26lCGocCW+Hodua3nlBpnsh7M04xiOlA8ogzaqzUrPXKFbfqzkFWiZeTCuRo9Mpf3X7CshilYYJq3fHc1AQTqgxnAqelbqYxpWxEB9ixVNIYdTCZHzolZ1bpkyhRtqQhc/X3xITGWo/j0HbG1Az1sjcT//M6mYmuggmXaWZQssWiKBPEJGT2NelzhcyIsSWUKW5vJWxIFWXGZlOyIXjLL68S/6J6XfWatUr9Jk+jCCdwCufgwSXU4Q4a4AMDhGd4hTfn0Xlx3p2PRWvByWeO4Q+czx/txIyM</latexit><latexit sha1_base64="WBHF4kMSevXodtV6Szf35fn3/Ls=">AAAB53icbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoN6KXjy2YGyhDWWznbRrN5uwuxFK6S/w4kHFq3/Jm//GbZuDtj4YeLw3w8y8MBVcG9f9dgpr6xubW8Xt0s7u3v5B+fDoQSeZYuizRCSqHVKNgkv0DTcC26lCGocCW+Hodua3nlBpnsh7M04xiOlA8ogzaqzUrPXKFbfqzkFWiZeTCuRo9Mpf3X7CshilYYJq3fHc1AQTqgxnAqelbqYxpWxEB9ixVNIYdTCZHzolZ1bpkyhRtqQhc/X3xITGWo/j0HbG1Az1sjcT//M6mYmuggmXaWZQssWiKBPEJGT2NelzhcyIsSWUKW5vJWxIFWXGZlOyIXjLL68S/6J6XfWatUr9Jk+jCCdwCufgwSXU4Q4a4AMDhGd4hTfn0Xlx3p2PRWvByWeO4Q+czx/txIyM</latexit><latexit sha1_base64="WBHF4kMSevXodtV6Szf35fn3/Ls=">AAAB53icbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoN6KXjy2YGyhDWWznbRrN5uwuxFK6S/w4kHFq3/Jm//GbZuDtj4YeLw3w8y8MBVcG9f9dgpr6xubW8Xt0s7u3v5B+fDoQSeZYuizRCSqHVKNgkv0DTcC26lCGocCW+Hodua3nlBpnsh7M04xiOlA8ogzaqzUrPXKFbfqzkFWiZeTCuRo9Mpf3X7CshilYYJq3fHc1AQTqgxnAqelbqYxpWxEB9ixVNIYdTCZHzolZ1bpkyhRtqQhc/X3xITGWo/j0HbG1Az1sjcT//M6mYmuggmXaWZQssWiKBPEJGT2NelzhcyIsSWUKW5vJWxIFWXGZlOyIXjLL68S/6J6XfWatUr9Jk+jCCdwCufgwSXU4Q4a4AMDhGd4hTfn0Xlx3p2PRWvByWeO4Q+czx/txIyM</latexit><latexit sha1_base64="WBHF4kMSevXodtV6Szf35fn3/Ls=">AAAB53icbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoN6KXjy2YGyhDWWznbRrN5uwuxFK6S/w4kHFq3/Jm//GbZuDtj4YeLw3w8y8MBVcG9f9dgpr6xubW8Xt0s7u3v5B+fDoQSeZYuizRCSqHVKNgkv0DTcC26lCGocCW+Hodua3nlBpnsh7M04xiOlA8ogzaqzUrPXKFbfqzkFWiZeTCuRo9Mpf3X7CshilYYJq3fHc1AQTqgxnAqelbqYxpWxEB9ixVNIYdTCZHzolZ1bpkyhRtqQhc/X3xITGWo/j0HbG1Az1sjcT//M6mYmuggmXaWZQssWiKBPEJGT2NelzhcyIsSWUKW5vJWxIFWXGZlOyIXjLL68S/6J6XfWatUr9Jk+jCCdwCufgwSXU4Q4a4AMDhGd4hTfn0Xlx3p2PRWvByWeO4Q+czx/txIyM</latexit>5<latexit sha1_base64="lR5CeQtHUH/ndumVZziO+fLyPXQ=">AAAB53icbVBNS8NAEJ3Ur1q/qh69LBbBU0lEUW9FLx5bMLbQhrLZTtu1m03Y3Qgl9Bd48aDi1b/kzX/jts1BWx8MPN6bYWZemAiujet+O4WV1bX1jeJmaWt7Z3evvH/woONUMfRZLGLVCqlGwSX6hhuBrUQhjUKBzXB0O/WbT6g0j+W9GScYRHQgeZ8zaqzUuOiWK27VnYEsEy8nFchR75a/Or2YpRFKwwTVuu25iQkyqgxnAielTqoxoWxEB9i2VNIIdZDNDp2QE6v0SD9WtqQhM/X3REYjrcdRaDsjaoZ60ZuK/3nt1PSvgozLJDUo2XxRPxXExGT6NelxhcyIsSWUKW5vJWxIFWXGZlOyIXiLLy8T/6x6XfUa55XaTZ5GEY7gGE7Bg0uowR3UwQcGCM/wCm/Oo/PivDsf89aCk88cwh84nz/vR4yN</latexit><latexit sha1_base64="lR5CeQtHUH/ndumVZziO+fLyPXQ=">AAAB53icbVBNS8NAEJ3Ur1q/qh69LBbBU0lEUW9FLx5bMLbQhrLZTtu1m03Y3Qgl9Bd48aDi1b/kzX/jts1BWx8MPN6bYWZemAiujet+O4WV1bX1jeJmaWt7Z3evvH/woONUMfRZLGLVCqlGwSX6hhuBrUQhjUKBzXB0O/WbT6g0j+W9GScYRHQgeZ8zaqzUuOiWK27VnYEsEy8nFchR75a/Or2YpRFKwwTVuu25iQkyqgxnAielTqoxoWxEB9i2VNIIdZDNDp2QE6v0SD9WtqQhM/X3REYjrcdRaDsjaoZ60ZuK/3nt1PSvgozLJDUo2XxRPxXExGT6NelxhcyIsSWUKW5vJWxIFWXGZlOyIXiLLy8T/6x6XfUa55XaTZ5GEY7gGE7Bg0uowR3UwQcGCM/wCm/Oo/PivDsf89aCk88cwh84nz/vR4yN</latexit><latexit sha1_base64="lR5CeQtHUH/ndumVZziO+fLyPXQ=">AAAB53icbVBNS8NAEJ3Ur1q/qh69LBbBU0lEUW9FLx5bMLbQhrLZTtu1m03Y3Qgl9Bd48aDi1b/kzX/jts1BWx8MPN6bYWZemAiujet+O4WV1bX1jeJmaWt7Z3evvH/woONUMfRZLGLVCqlGwSX6hhuBrUQhjUKBzXB0O/WbT6g0j+W9GScYRHQgeZ8zaqzUuOiWK27VnYEsEy8nFchR75a/Or2YpRFKwwTVuu25iQkyqgxnAielTqoxoWxEB9i2VNIIdZDNDp2QE6v0SD9WtqQhM/X3REYjrcdRaDsjaoZ60ZuK/3nt1PSvgozLJDUo2XxRPxXExGT6NelxhcyIsSWUKW5vJWxIFWXGZlOyIXiLLy8T/6x6XfUa55XaTZ5GEY7gGE7Bg0uowR3UwQcGCM/wCm/Oo/PivDsf89aCk88cwh84nz/vR4yN</latexit><latexit sha1_base64="lR5CeQtHUH/ndumVZziO+fLyPXQ=">AAAB53icbVBNS8NAEJ3Ur1q/qh69LBbBU0lEUW9FLx5bMLbQhrLZTtu1m03Y3Qgl9Bd48aDi1b/kzX/jts1BWx8MPN6bYWZemAiujet+O4WV1bX1jeJmaWt7Z3evvH/woONUMfRZLGLVCqlGwSX6hhuBrUQhjUKBzXB0O/WbT6g0j+W9GScYRHQgeZ8zaqzUuOiWK27VnYEsEy8nFchR75a/Or2YpRFKwwTVuu25iQkyqgxnAielTqoxoWxEB9i2VNIIdZDNDp2QE6v0SD9WtqQhM/X3REYjrcdRaDsjaoZ60ZuK/3nt1PSvgozLJDUo2XxRPxXExGT6NelxhcyIsSWUKW5vJWxIFWXGZlOyIXiLLy8T/6x6XfUa55XaTZ5GEY7gGE7Bg0uowR3UwQcGCM/wCm/Oo/PivDsf89aCk88cwh84nz/vR4yN</latexit>6<latexit sha1_base64="+0NiW+lgv6QdEXkzfKy4+T0AmBU=">AAAB53icbVBNS8NAEJ3Ur1q/qh69LBbBU0lE/LgVvXhswdhCG8pmO23XbjZhdyOU0F/gxYOKV/+SN/+N2zYHbX0w8Hhvhpl5YSK4Nq777RRWVtfWN4qbpa3tnd298v7Bg45TxdBnsYhVK6QaBZfoG24EthKFNAoFNsPR7dRvPqHSPJb3ZpxgENGB5H3OqLFS46JbrrhVdwayTLycVCBHvVv+6vRilkYoDRNU67bnJibIqDKcCZyUOqnGhLIRHWDbUkkj1EE2O3RCTqzSI/1Y2ZKGzNTfExmNtB5Hoe2MqBnqRW8q/ue1U9O/CjIuk9SgZPNF/VQQE5Pp16THFTIjxpZQpri9lbAhVZQZm03JhuAtvrxM/LPqddVrnFdqN3kaRTiCYzgFDy6hBndQBx8YIDzDK7w5j86L8+58zFsLTj5zCH/gfP4A8MqMjg==</latexit><latexit sha1_base64="+0NiW+lgv6QdEXkzfKy4+T0AmBU=">AAAB53icbVBNS8NAEJ3Ur1q/qh69LBbBU0lE/LgVvXhswdhCG8pmO23XbjZhdyOU0F/gxYOKV/+SN/+N2zYHbX0w8Hhvhpl5YSK4Nq777RRWVtfWN4qbpa3tnd298v7Bg45TxdBnsYhVK6QaBZfoG24EthKFNAoFNsPR7dRvPqHSPJb3ZpxgENGB5H3OqLFS46JbrrhVdwayTLycVCBHvVv+6vRilkYoDRNU67bnJibIqDKcCZyUOqnGhLIRHWDbUkkj1EE2O3RCTqzSI/1Y2ZKGzNTfExmNtB5Hoe2MqBnqRW8q/ue1U9O/CjIuk9SgZPNF/VQQE5Pp16THFTIjxpZQpri9lbAhVZQZm03JhuAtvrxM/LPqddVrnFdqN3kaRTiCYzgFDy6hBndQBx8YIDzDK7w5j86L8+58zFsLTj5zCH/gfP4A8MqMjg==</latexit><latexit sha1_base64="+0NiW+lgv6QdEXkzfKy4+T0AmBU=">AAAB53icbVBNS8NAEJ3Ur1q/qh69LBbBU0lE/LgVvXhswdhCG8pmO23XbjZhdyOU0F/gxYOKV/+SN/+N2zYHbX0w8Hhvhpl5YSK4Nq777RRWVtfWN4qbpa3tnd298v7Bg45TxdBnsYhVK6QaBZfoG24EthKFNAoFNsPR7dRvPqHSPJb3ZpxgENGB5H3OqLFS46JbrrhVdwayTLycVCBHvVv+6vRilkYoDRNU67bnJibIqDKcCZyUOqnGhLIRHWDbUkkj1EE2O3RCTqzSI/1Y2ZKGzNTfExmNtB5Hoe2MqBnqRW8q/ue1U9O/CjIuk9SgZPNF/VQQE5Pp16THFTIjxpZQpri9lbAhVZQZm03JhuAtvrxM/LPqddVrnFdqN3kaRTiCYzgFDy6hBndQBx8YIDzDK7w5j86L8+58zFsLTj5zCH/gfP4A8MqMjg==</latexit><latexit sha1_base64="+0NiW+lgv6QdEXkzfKy4+T0AmBU=">AAAB53icbVBNS8NAEJ3Ur1q/qh69LBbBU0lE/LgVvXhswdhCG8pmO23XbjZhdyOU0F/gxYOKV/+SN/+N2zYHbX0w8Hhvhpl5YSK4Nq777RRWVtfWN4qbpa3tnd298v7Bg45TxdBnsYhVK6QaBZfoG24EthKFNAoFNsPR7dRvPqHSPJb3ZpxgENGB5H3OqLFS46JbrrhVdwayTLycVCBHvVv+6vRilkYoDRNU67bnJibIqDKcCZyUOqnGhLIRHWDbUkkj1EE2O3RCTqzSI/1Y2ZKGzNTfExmNtB5Hoe2MqBnqRW8q/ue1U9O/CjIuk9SgZPNF/VQQE5Pp16THFTIjxpZQpri9lbAhVZQZm03JhuAtvrxM/LPqddVrnFdqN3kaRTiCYzgFDy6hBndQBx8YIDzDK7w5j86L8+58zFsLTj5zCH/gfP4A8MqMjg==</latexit>7<latexit sha1_base64="6D76TCE2Ea/SN28umgmnxW6vjRI=">AAAB53icbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqN6KXjy2YGyhDWWznbRrN5uwuxFK6S/w4kHFq3/Jm//GbZuDtj4YeLw3w8y8MBVcG9f9dgpr6xubW8Xt0s7u3v5B+fDoQSeZYuizRCSqHVKNgkv0DTcC26lCGocCW+Hodua3nlBpnsh7M04xiOlA8ogzaqzUrPXKFbfqzkFWiZeTCuRo9Mpf3X7CshilYYJq3fHc1AQTqgxnAqelbqYxpWxEB9ixVNIYdTCZHzolZ1bpkyhRtqQhc/X3xITGWo/j0HbG1Az1sjcT//M6mYmuggmXaWZQssWiKBPEJGT2NelzhcyIsSWUKW5vJWxIFWXGZlOyIXjLL68S/6J6XfWal5X6TZ5GEU7gFM7BgxrU4Q4a4AMDhGd4hTfn0Xlx3p2PRWvByWeO4Q+czx/yTYyP</latexit><latexit sha1_base64="6D76TCE2Ea/SN28umgmnxW6vjRI=">AAAB53icbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqN6KXjy2YGyhDWWznbRrN5uwuxFK6S/w4kHFq3/Jm//GbZuDtj4YeLw3w8y8MBVcG9f9dgpr6xubW8Xt0s7u3v5B+fDoQSeZYuizRCSqHVKNgkv0DTcC26lCGocCW+Hodua3nlBpnsh7M04xiOlA8ogzaqzUrPXKFbfqzkFWiZeTCuRo9Mpf3X7CshilYYJq3fHc1AQTqgxnAqelbqYxpWxEB9ixVNIYdTCZHzolZ1bpkyhRtqQhc/X3xITGWo/j0HbG1Az1sjcT//M6mYmuggmXaWZQssWiKBPEJGT2NelzhcyIsSWUKW5vJWxIFWXGZlOyIXjLL68S/6J6XfWal5X6TZ5GEU7gFM7BgxrU4Q4a4AMDhGd4hTfn0Xlx3p2PRWvByWeO4Q+czx/yTYyP</latexit><latexit sha1_base64="6D76TCE2Ea/SN28umgmnxW6vjRI=">AAAB53icbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqN6KXjy2YGyhDWWznbRrN5uwuxFK6S/w4kHFq3/Jm//GbZuDtj4YeLw3w8y8MBVcG9f9dgpr6xubW8Xt0s7u3v5B+fDoQSeZYuizRCSqHVKNgkv0DTcC26lCGocCW+Hodua3nlBpnsh7M04xiOlA8ogzaqzUrPXKFbfqzkFWiZeTCuRo9Mpf3X7CshilYYJq3fHc1AQTqgxnAqelbqYxpWxEB9ixVNIYdTCZHzolZ1bpkyhRtqQhc/X3xITGWo/j0HbG1Az1sjcT//M6mYmuggmXaWZQssWiKBPEJGT2NelzhcyIsSWUKW5vJWxIFWXGZlOyIXjLL68S/6J6XfWal5X6TZ5GEU7gFM7BgxrU4Q4a4AMDhGd4hTfn0Xlx3p2PRWvByWeO4Q+czx/yTYyP</latexit><latexit sha1_base64="6D76TCE2Ea/SN28umgmnxW6vjRI=">AAAB53icbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqN6KXjy2YGyhDWWznbRrN5uwuxFK6S/w4kHFq3/Jm//GbZuDtj4YeLw3w8y8MBVcG9f9dgpr6xubW8Xt0s7u3v5B+fDoQSeZYuizRCSqHVKNgkv0DTcC26lCGocCW+Hodua3nlBpnsh7M04xiOlA8ogzaqzUrPXKFbfqzkFWiZeTCuRo9Mpf3X7CshilYYJq3fHc1AQTqgxnAqelbqYxpWxEB9ixVNIYdTCZHzolZ1bpkyhRtqQhc/X3xITGWo/j0HbG1Az1sjcT//M6mYmuggmXaWZQssWiKBPEJGT2NelzhcyIsSWUKW5vJWxIFWXGZlOyIXjLL68S/6J6XfWal5X6TZ5GEU7gFM7BgxrU4Q4a4AMDhGd4hTfn0Xlx3p2PRWvByWeO4Q+czx/yTYyP</latexit>8<latexit sha1_base64="iCbDsWM2/1S99bF/4yvjnF1i8Cc=">AAAB53icbVBNS8NAEJ3Ur1q/qh69LBbBU0lEsN6KXjy2YGyhDWWznbRrN5uwuxFK6S/w4kHFq3/Jm//GbZuDtj4YeLw3w8y8MBVcG9f9dgpr6xubW8Xt0s7u3v5B+fDoQSeZYuizRCSqHVKNgkv0DTcC26lCGocCW+Hodua3nlBpnsh7M04xiOlA8ogzaqzUrPXKFbfqzkFWiZeTCuRo9Mpf3X7CshilYYJq3fHc1AQTqgxnAqelbqYxpWxEB9ixVNIYdTCZHzolZ1bpkyhRtqQhc/X3xITGWo/j0HbG1Az1sjcT//M6mYlqwYTLNDMo2WJRlAliEjL7mvS5QmbE2BLKFLe3EjakijJjsynZELzll1eJf1G9rnrNy0r9Jk+jCCdwCufgwRXU4Q4a4AMDhGd4hTfn0Xlx3p2PRWvByWeO4Q+czx/z0IyQ</latexit><latexit sha1_base64="iCbDsWM2/1S99bF/4yvjnF1i8Cc=">AAAB53icbVBNS8NAEJ3Ur1q/qh69LBbBU0lEsN6KXjy2YGyhDWWznbRrN5uwuxFK6S/w4kHFq3/Jm//GbZuDtj4YeLw3w8y8MBVcG9f9dgpr6xubW8Xt0s7u3v5B+fDoQSeZYuizRCSqHVKNgkv0DTcC26lCGocCW+Hodua3nlBpnsh7M04xiOlA8ogzaqzUrPXKFbfqzkFWiZeTCuRo9Mpf3X7CshilYYJq3fHc1AQTqgxnAqelbqYxpWxEB9ixVNIYdTCZHzolZ1bpkyhRtqQhc/X3xITGWo/j0HbG1Az1sjcT//M6mYlqwYTLNDMo2WJRlAliEjL7mvS5QmbE2BLKFLe3EjakijJjsynZELzll1eJf1G9rnrNy0r9Jk+jCCdwCufgwRXU4Q4a4AMDhGd4hTfn0Xlx3p2PRWvByWeO4Q+czx/z0IyQ</latexit><latexit sha1_base64="iCbDsWM2/1S99bF/4yvjnF1i8Cc=">AAAB53icbVBNS8NAEJ3Ur1q/qh69LBbBU0lEsN6KXjy2YGyhDWWznbRrN5uwuxFK6S/w4kHFq3/Jm//GbZuDtj4YeLw3w8y8MBVcG9f9dgpr6xubW8Xt0s7u3v5B+fDoQSeZYuizRCSqHVKNgkv0DTcC26lCGocCW+Hodua3nlBpnsh7M04xiOlA8ogzaqzUrPXKFbfqzkFWiZeTCuRo9Mpf3X7CshilYYJq3fHc1AQTqgxnAqelbqYxpWxEB9ixVNIYdTCZHzolZ1bpkyhRtqQhc/X3xITGWo/j0HbG1Az1sjcT//M6mYlqwYTLNDMo2WJRlAliEjL7mvS5QmbE2BLKFLe3EjakijJjsynZELzll1eJf1G9rnrNy0r9Jk+jCCdwCufgwRXU4Q4a4AMDhGd4hTfn0Xlx3p2PRWvByWeO4Q+czx/z0IyQ</latexit><latexit sha1_base64="iCbDsWM2/1S99bF/4yvjnF1i8Cc=">AAAB53icbVBNS8NAEJ3Ur1q/qh69LBbBU0lEsN6KXjy2YGyhDWWznbRrN5uwuxFK6S/w4kHFq3/Jm//GbZuDtj4YeLw3w8y8MBVcG9f9dgpr6xubW8Xt0s7u3v5B+fDoQSeZYuizRCSqHVKNgkv0DTcC26lCGocCW+Hodua3nlBpnsh7M04xiOlA8ogzaqzUrPXKFbfqzkFWiZeTCuRo9Mpf3X7CshilYYJq3fHc1AQTqgxnAqelbqYxpWxEB9ixVNIYdTCZHzolZ1bpkyhRtqQhc/X3xITGWo/j0HbG1Az1sjcT//M6mYlqwYTLNDMo2WJRlAliEjL7mvS5QmbE2BLKFLe3EjakijJjsynZELzll1eJf1G9rnrNy0r9Jk+jCCdwCufgwRXU4Q4a4AMDhGd4hTfn0Xlx3p2PRWvByWeO4Q+czx/z0IyQ</latexit>k<latexit sha1_base64="4k4wc5epfIngZWGE5jZJOI+hDKI=">AAAB53icbVBNS8NAEJ34WetX1aOXxSJ4KokI6q3oxWMLxhbaUDbbSbt2swm7G6GE/gIvHlS8+pe8+W/ctjlo64OBx3szzMwLU8G1cd1vZ2V1bX1js7RV3t7Z3duvHBw+6CRTDH2WiES1Q6pRcIm+4UZgO1VI41BgKxzdTv3WEyrNE3lvxikGMR1IHnFGjZWao16l6tbcGcgy8QpShQKNXuWr209YFqM0TFCtO56bmiCnynAmcFLuZhpTykZ0gB1LJY1RB/ns0Ak5tUqfRImyJQ2Zqb8nchprPY5D2xlTM9SL3lT8z+tkJroKci7TzKBk80VRJohJyPRr0ucKmRFjSyhT3N5K2JAqyozNpmxD8BZfXib+ee265jUvqvWbIo0SHMMJnIEHl1CHO2iADwwQnuEV3pxH58V5dz7mrStOMXMEf+B8/gBA+IzD</latexit><latexit sha1_base64="4k4wc5epfIngZWGE5jZJOI+hDKI=">AAAB53icbVBNS8NAEJ34WetX1aOXxSJ4KokI6q3oxWMLxhbaUDbbSbt2swm7G6GE/gIvHlS8+pe8+W/ctjlo64OBx3szzMwLU8G1cd1vZ2V1bX1js7RV3t7Z3duvHBw+6CRTDH2WiES1Q6pRcIm+4UZgO1VI41BgKxzdTv3WEyrNE3lvxikGMR1IHnFGjZWao16l6tbcGcgy8QpShQKNXuWr209YFqM0TFCtO56bmiCnynAmcFLuZhpTykZ0gB1LJY1RB/ns0Ak5tUqfRImyJQ2Zqb8nchprPY5D2xlTM9SL3lT8z+tkJroKci7TzKBk80VRJohJyPRr0ucKmRFjSyhT3N5K2JAqyozNpmxD8BZfXib+ee265jUvqvWbIo0SHMMJnIEHl1CHO2iADwwQnuEV3pxH58V5dz7mrStOMXMEf+B8/gBA+IzD</latexit><latexit sha1_base64="4k4wc5epfIngZWGE5jZJOI+hDKI=">AAAB53icbVBNS8NAEJ34WetX1aOXxSJ4KokI6q3oxWMLxhbaUDbbSbt2swm7G6GE/gIvHlS8+pe8+W/ctjlo64OBx3szzMwLU8G1cd1vZ2V1bX1js7RV3t7Z3duvHBw+6CRTDH2WiES1Q6pRcIm+4UZgO1VI41BgKxzdTv3WEyrNE3lvxikGMR1IHnFGjZWao16l6tbcGcgy8QpShQKNXuWr209YFqM0TFCtO56bmiCnynAmcFLuZhpTykZ0gB1LJY1RB/ns0Ak5tUqfRImyJQ2Zqb8nchprPY5D2xlTM9SL3lT8z+tkJroKci7TzKBk80VRJohJyPRr0ucKmRFjSyhT3N5K2JAqyozNpmxD8BZfXib+ee265jUvqvWbIo0SHMMJnIEHl1CHO2iADwwQnuEV3pxH58V5dz7mrStOMXMEf+B8/gBA+IzD</latexit><latexit sha1_base64="4k4wc5epfIngZWGE5jZJOI+hDKI=">AAAB53icbVBNS8NAEJ34WetX1aOXxSJ4KokI6q3oxWMLxhbaUDbbSbt2swm7G6GE/gIvHlS8+pe8+W/ctjlo64OBx3szzMwLU8G1cd1vZ2V1bX1js7RV3t7Z3duvHBw+6CRTDH2WiES1Q6pRcIm+4UZgO1VI41BgKxzdTv3WEyrNE3lvxikGMR1IHnFGjZWao16l6tbcGcgy8QpShQKNXuWr209YFqM0TFCtO56bmiCnynAmcFLuZhpTykZ0gB1LJY1RB/ns0Ak5tUqfRImyJQ2Zqb8nchprPY5D2xlTM9SL3lT8z+tkJroKci7TzKBk80VRJohJyPRr0ucKmRFjSyhT3N5K2JAqyozNpmxD8BZfXib+ee265jUvqvWbIo0SHMMJnIEHl1CHO2iADwwQnuEV3pxH58V5dz7mrStOMXMEf+B8/gBA+IzD</latexit>`<latexit sha1_base64="5rEVNfGss0b42QOQ47E/+As79q4=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEUG9FLx4rGFtoQ9lsJ+3SzSbsboQS+he8eFDx6i/y5r9x0+ag1QcDj/dmmJkXpoJr47pfTmVldW19o7pZ29re2d2r7x886CRTDH2WiER1Q6pRcIm+4UZgN1VI41BgJ5zcFH7nEZXmibw30xSDmI4kjzijppD6KMSg3nCb7hzkL/FK0oAS7UH9sz9MWBajNExQrXuem5ogp8pwJnBW62caU8omdIQ9SyWNUQf5/NYZObHKkESJsiUNmas/J3Iaaz2NQ9sZUzPWy14h/uf1MhNdBjmXaWZQssWiKBPEJKR4nAy5QmbE1BLKFLe3EjamijJj46nZELzll/8S/6x51fTuzhut6zKNKhzBMZyCBxfQgltogw8MxvAEL/DqxM6z8+a8L1orTjlzCL/gfHwDepmODw==</latexit><latexit sha1_base64="5rEVNfGss0b42QOQ47E/+As79q4=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEUG9FLx4rGFtoQ9lsJ+3SzSbsboQS+he8eFDx6i/y5r9x0+ag1QcDj/dmmJkXpoJr47pfTmVldW19o7pZ29re2d2r7x886CRTDH2WiER1Q6pRcIm+4UZgN1VI41BgJ5zcFH7nEZXmibw30xSDmI4kjzijppD6KMSg3nCb7hzkL/FK0oAS7UH9sz9MWBajNExQrXuem5ogp8pwJnBW62caU8omdIQ9SyWNUQf5/NYZObHKkESJsiUNmas/J3Iaaz2NQ9sZUzPWy14h/uf1MhNdBjmXaWZQssWiKBPEJKR4nAy5QmbE1BLKFLe3EjamijJj46nZELzll/8S/6x51fTuzhut6zKNKhzBMZyCBxfQgltogw8MxvAEL/DqxM6z8+a8L1orTjlzCL/gfHwDepmODw==</latexit><latexit sha1_base64="5rEVNfGss0b42QOQ47E/+As79q4=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEUG9FLx4rGFtoQ9lsJ+3SzSbsboQS+he8eFDx6i/y5r9x0+ag1QcDj/dmmJkXpoJr47pfTmVldW19o7pZ29re2d2r7x886CRTDH2WiER1Q6pRcIm+4UZgN1VI41BgJ5zcFH7nEZXmibw30xSDmI4kjzijppD6KMSg3nCb7hzkL/FK0oAS7UH9sz9MWBajNExQrXuem5ogp8pwJnBW62caU8omdIQ9SyWNUQf5/NYZObHKkESJsiUNmas/J3Iaaz2NQ9sZUzPWy14h/uf1MhNdBjmXaWZQssWiKBPEJKR4nAy5QmbE1BLKFLe3EjamijJj46nZELzll/8S/6x51fTuzhut6zKNKhzBMZyCBxfQgltogw8MxvAEL/DqxM6z8+a8L1orTjlzCL/gfHwDepmODw==</latexit><latexit sha1_base64="5rEVNfGss0b42QOQ47E/+As79q4=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEUG9FLx4rGFtoQ9lsJ+3SzSbsboQS+he8eFDx6i/y5r9x0+ag1QcDj/dmmJkXpoJr47pfTmVldW19o7pZ29re2d2r7x886CRTDH2WiER1Q6pRcIm+4UZgN1VI41BgJ5zcFH7nEZXmibw30xSDmI4kjzijppD6KMSg3nCb7hzkL/FK0oAS7UH9sz9MWBajNExQrXuem5ogp8pwJnBW62caU8omdIQ9SyWNUQf5/NYZObHKkESJsiUNmas/J3Iaaz2NQ9sZUzPWy14h/uf1MhNdBjmXaWZQssWiKBPEJKR4nAy5QmbE1BLKFLe3EjamijJj46nZELzll/8S/6x51fTuzhut6zKNKhzBMZyCBxfQgltogw8MxvAEL/DqxM6z8+a8L1orTjlzCL/gfHwDepmODw==</latexit>KNk
<latexit sha1_base64="ckniautMrdGhkSA8vrcVzAq1ZIc=">AAACEnicbVDLSsNAFL3xWeur6tJNsAiuSlIFXUnBjSupYB/QhjKZ3rZDJ5M4MxFKyF8IbvU33Ilbf8C/8BOctFnY1gMDh3POvXM5fsSZ0o7zba2srq1vbBa2its7u3v7pYPDpgpjSbFBQx7Ktk8UciawoZnm2I4kksDn2PLHN5nfekKpWCge9CRCLyBDwQaMEm0krxsQPaKEJ3dpb9wrlZ2KM4W9TNyclCFHvVf66fZDGgcoNOVEqY7rRNpLiNSMckyL3VhhROiYDLFjqCABKi+ZHp3ap0bp24NQmie0PVX/TiQkUGoS+CaZHakWvUz8z+vEenDlJUxEsUZBZx8NYm7r0M4asPtMItV8YgihkplbbToiklBteprblJfjJRgrk4x0aipyFwtZJs1qxT2vVO8vyrXrvKwCHMMJnIELl1CDW6hDAyg8wgu8wpv1bL1bH9bnLLpi5TNHMAfr6xc9Fp9k</latexit>3<latexit sha1_base64="PDcf3tlZipfwGTSkILZYXMZoK2E=">AAAB53icbVBNS8NAEJ3Ur1q/qh69LBbBU0lUUG9FLx5bMLbQhrLZTtu1m03Y3Qgl9Bd48aDi1b/kzX/jts1BWx8MPN6bYWZemAiujet+O4WV1bX1jeJmaWt7Z3evvH/woONUMfRZLGLVCqlGwSX6hhuBrUQhjUKBzXB0O/WbT6g0j+W9GScYRHQgeZ8zaqzUOO+WK27VnYEsEy8nFchR75a/Or2YpRFKwwTVuu25iQkyqgxnAielTqoxoWxEB9i2VNIIdZDNDp2QE6v0SD9WtqQhM/X3REYjrcdRaDsjaoZ60ZuK/3nt1PSvgozLJDUo2XxRPxXExGT6NelxhcyIsSWUKW5vJWxIFWXGZlOyIXiLLy8T/6x6XfUaF5XaTZ5GEY7gGE7Bg0uowR3UwQcGCM/wCm/Oo/PivDsf89aCk88cwh84nz/sQYyL</latexit><latexit sha1_base64="PDcf3tlZipfwGTSkILZYXMZoK2E=">AAAB53icbVBNS8NAEJ3Ur1q/qh69LBbBU0lUUG9FLx5bMLbQhrLZTtu1m03Y3Qgl9Bd48aDi1b/kzX/jts1BWx8MPN6bYWZemAiujet+O4WV1bX1jeJmaWt7Z3evvH/woONUMfRZLGLVCqlGwSX6hhuBrUQhjUKBzXB0O/WbT6g0j+W9GScYRHQgeZ8zaqzUOO+WK27VnYEsEy8nFchR75a/Or2YpRFKwwTVuu25iQkyqgxnAielTqoxoWxEB9i2VNIIdZDNDp2QE6v0SD9WtqQhM/X3REYjrcdRaDsjaoZ60ZuK/3nt1PSvgozLJDUo2XxRPxXExGT6NelxhcyIsSWUKW5vJWxIFWXGZlOyIXiLLy8T/6x6XfUaF5XaTZ5GEY7gGE7Bg0uowR3UwQcGCM/wCm/Oo/PivDsf89aCk88cwh84nz/sQYyL</latexit><latexit sha1_base64="PDcf3tlZipfwGTSkILZYXMZoK2E=">AAAB53icbVBNS8NAEJ3Ur1q/qh69LBbBU0lUUG9FLx5bMLbQhrLZTtu1m03Y3Qgl9Bd48aDi1b/kzX/jts1BWx8MPN6bYWZemAiujet+O4WV1bX1jeJmaWt7Z3evvH/woONUMfRZLGLVCqlGwSX6hhuBrUQhjUKBzXB0O/WbT6g0j+W9GScYRHQgeZ8zaqzUOO+WK27VnYEsEy8nFchR75a/Or2YpRFKwwTVuu25iQkyqgxnAielTqoxoWxEB9i2VNIIdZDNDp2QE6v0SD9WtqQhM/X3REYjrcdRaDsjaoZ60ZuK/3nt1PSvgozLJDUo2XxRPxXExGT6NelxhcyIsSWUKW5vJWxIFWXGZlOyIXiLLy8T/6x6XfUaF5XaTZ5GEY7gGE7Bg0uowR3UwQcGCM/wCm/Oo/PivDsf89aCk88cwh84nz/sQYyL</latexit><latexit sha1_base64="PDcf3tlZipfwGTSkILZYXMZoK2E=">AAAB53icbVBNS8NAEJ3Ur1q/qh69LBbBU0lUUG9FLx5bMLbQhrLZTtu1m03Y3Qgl9Bd48aDi1b/kzX/jts1BWx8MPN6bYWZemAiujet+O4WV1bX1jeJmaWt7Z3evvH/woONUMfRZLGLVCqlGwSX6hhuBrUQhjUKBzXB0O/WbT6g0j+W9GScYRHQgeZ8zaqzUOO+WK27VnYEsEy8nFchR75a/Or2YpRFKwwTVuu25iQkyqgxnAielTqoxoWxEB9i2VNIIdZDNDp2QE6v0SD9WtqQhM/X3REYjrcdRaDsjaoZ60ZuK/3nt1PSvgozLJDUo2XxRPxXExGT6NelxhcyIsSWUKW5vJWxIFWXGZlOyIXiLLy8T/6x6XfUaF5XaTZ5GEY7gGE7Bg0uowR3UwQcGCM/wCm/Oo/PivDsf89aCk88cwh84nz/sQYyL</latexit>
FIGURE 1: An illustration of a network model.
are not connected by an edge then c`k= 0. We assume C
is symmetric and doubly-stochastic, meaning that
KX
`=1c`k= 1; c`k=ck`; (26)
or in matrix notation:
C1K=1K; C =CT: (27)
2) Local Belief Formation
In the fully decentralized strategy, the agents cannot form
the joint belief (16) since they do not have access to the
observations and actions of all other agents. They, however,
can construct local beliefs. To do so, we will extend the
diffusion HMM strategy (DHS) from [48] and [49], which is
originally designed for hidden Markov models, to the current
POMDP setting.
In DHS, the global belief vectors fi;igare replaced by
local belief vectors fk;i;k;ig, and the latter are updated by
using local observations and by relying solely on interactions
with the immediate neighbors. The original DHS algorithm is
designed for actionless partially observable Markov chains,
and each agent can use the same global transition model.
However, in POMDPs, transition of the global state depends
on the joint action, and the agents cannot perform a central-
ized time-adjustment step as in (23) since they do not know
the actions of all agents in the network.
Therefore, one strategy is to use a transition model that is
obtained by marginalizing over actions that are unknown to
agentk. More speciÔ¨Åcally, let aNk2ANkdenote a tuple of
actions taken by the set of neighbors of agent k(which we
are denoting byNk). These actions can be assumed to be
known by agent kif, for instance, agents share their actions
with their neighbors. Let ac
Nk2Ac
Nkdenote the remaining
actions by all other agents in the network, so that a=aNk[
ac
Nk. Then, each agent can use the following local transition
model approximation:
T
k(sjs0;aNk)/X
ac
Nk2Ac
NkT(sjs0;aNk;ac
Nk)(aNk;ac
Nkjs0)(28)in lieu of T(sjs0;a), to time-adjust its local belief:
k;i(s) =X
s02ST
k(sjs0;aNk;i 1)k;i 1(s0); (29)
Here,aNk;i 1is the tuple of actions taken by the neighbors
of agentkat time instant i 1. Moreover, in (28), the notation
(aNk;ac
Nkjs0)represents the joint action probability:
(aNk;ac
Nkjs0) =KY
`=1`(a`js0); (30)
where the notation (ajs)is now a shorthand for (aj)
when
= [0:::1:::0]T; (31)
i.e., when the belief attains value 1 for state sand is
0 otherwise. Note that this construction leads to a richer
scenario compared to [48], [49], with transition models that
are different across the agents.
The rest of the algorithm is the same as the DHS strategy.
Following (29), and based on the personal observation k;i,
each agentkforms an intermediate belief using a -scaled
Bayesian update of the form:
 k;i(s)/(Lk(k;ijs))k;i(s); (32)
where > 0. Finally, agents in the neighborhood of
kshare their intermediate beliefs, which allows agent k
to update its belief using the weighted geometric average
expression:
k;i(s)/Y
`2Nk 
 `;i(s)c`k: (33)
This procedure of repeated updating and exchanging of
beliefs allows information to diffuse over the network.
3) Diffusion Policy Evaluation
In the fully decentralized strategy, the local belief formation
strategy is used during both training and execution phases.
Namely, the target value function in (19) represents the av-
erage return agents get when they execute the policy with
their local beliefs formed via the DHS strategy. Moreover,
since the policy evaluation is also decentralized, during the
training phase, they again need to use DHS to approximate
the global belief state on top of the function approximation.
More speciÔ¨Åcally, using its local belief vectors, each agent
kcomputes a local TD error:
k;i=rk;i+bV(k;i+1;wk;i) bV(k;i;wk;i);(34)
whererk;i=rk(si;ai;si+1)is also a function of the
local beliefs since each agent know executes the action
ak;ik(akjk;i). Subsequently, each agent kforms an
intermediate parameter estimate denoted by
zk;i+1= (1 2)wk;i+k;irwbV(k;i;wk;i):(35)
After receiving the intermediate estimates from its neighbors,
agentkupdateswk;ito
wk;i+1=X
`2Nkc`kz`;i+1: (36)
6The local adaptation step (35) followed by the combination
step (36) are reminiscent of diffusion strategies for dis-
tributed learning [19], [53]. Observe that there are actually
two combination steps involved in diffusion policy evalua-
tion: the belief combination (33) with geometric averaging
(GA), and the parameter combination (36) with arithmetic
averaging (AA). These choices of fusion rules are supported
by recent results in the literature [68], [69] that promote
the use of GA for probability density functions and AA for
point estimates. The listing of the proposed diffusion policy
evaluation strategy for POMDPs appears in Algorithm 2.
Algorithm 2 Diffusion policy evaluation under POMDPs
1:set initial priors k;0(s)>0,8s2S and8k2K
2:choose >0
3:initializewk;0for8k2K
4:whilei0do
5: each agentkobservesk;i
6: foreach agentk2K ands2Sadapt andcombine
 k;i(s)/(Lk(k;ijs))k;i(s) (37)
k;i(s)/Y
`2Nk 
 `;i(s)c`k(38)
7: end for
8: foreach agentk2K do
9: Take actionak;ik(akjk;i)
10: Get rewardrk;i=rk(si;ai;si+1)
11: end for
12: foreach agentk2K evolve
13: Compute T
k(sjs0;aNk;i)using (28), and
k;i+1(s) =X
s02ST
k(sjs0;aNk;i)k;i(s0) (39)
14: end for
15: foreach agentk2K update locally
k;i=rk;i+bV(k;i+1;wk;i) bV(k;i;wk;i)(40)
zk;i+1= (1 2)wk;i+k;irwbV(k;i;wk;i)(41)
16: end for
17: foreach agentk2K combine
wk;i+1=X
`2Nkc`kz`;i+1 (42)
18: end for
19:end while
Algorithm 2 has the following listed advantages:
Decentralized information structure: The algorithm
is designed to be fully decentralized, with each agent
only having access to its own private data, such as
observations and rewards, without the need to share
this information with other agents. Importantly, agents
do not require knowledge of the joint distributionof observations or the network topology. They only
know their own marginal likelihood function, and their
actions are only known by (or transmitted to) their
immediate neighbors. If agents happen to know their
own marginal transition models, they do not need to
know the policies of other agents or the global transition
model. However, if the application requires them to
approximate it themselves, they require knowledge of
the other policies and the global transition model.
Privacy : The algorithm is also advantageous in terms
of privacy since (i)communicating beliefs allows
information diffusion without explicitly sharing raw
observational data, and (ii)exchanging value param-
eters allows agents learn the cumulative reward across
network without explicitly sharing local rewards.
Complexity :(i)The memory requirement is constant
over time, with each agent only needing to store its
value function parameter estimate ( M-dimensional) and
local belief ( S-dimensional), as well as the necessary
model functions. (ii)The communication requirement
is also manageable, with each agent communicating
only with its immediate neighbors through belief and
parameter sharing. The communication load is not
affected by the network size, making our algorithm
scalable and avoiding communication bottlenecks. (iii)
The computational complexity depends on whether the
application at hand allows agents to have access to
the local transition model. If this is the case, then the
computational complexity is equivalent to the single-
agent Bayesian Ô¨Åltering case, which is O(S2). The
combination steps add only linear additional complexity
O(S)with Ô¨Åxed neighborhood size. However, if agents
need to approximate the transition model themselves,
the computational complexity increases with the net-
work size, and becomes O(KS2). This is due to the
need to average over non-neighbors‚Äô actions in (28),
whose size grows with the network size in general.
Compared to alternative approaches such as relaying
raw data, incremental approaches [70], or Bayesian
belief forming [71], our algorithm is much lighter in
terms of complexity. Relaying raw data, for example,
would result in an exponential increase of memory and
communication overload at each hop, making it highly
impractical. The incremental approach of relaying over
a cyclic path (which is NP-hard to Ô¨Ånd [72]) that visits
each agent once would reduce the overload. However,
it is not robust against failures and not scalable, making
it impractical for a decentralized setting. The Bayesian
belief forming strategy requires knowledge of the net-
work topology and other agents‚Äô functions, and known
to be NP-hard, even in the much simpler case of Ô¨Åxed
state and no action setting [13].
7KAYAALP ET AL .: POLICY EVALUATION IN DECENTRALIZED POMDPs WITH BELIEF SHARING
V. MAIN RESULTS
In this section, we analyze the performance of the decen-
tralized strategy in Algorithm 2. In particular, we Ô¨Årst show
in Sec. B that the value function parameters fwk;igof the
agents cluster around the network centroid. Then, in Sec. C,
we show that this network centroid has a bounded difference
from the parameter of a baseline strategy (which will be
presented in Algorithm 3). Our analysis relies on bounding
the disagreement between the joint centralized belief iand
the local estimate k;i, which is presented next.
A. Belief Disagreement
In a manner similar to [49], we introduce the following risk
functions in order to assess the disagreement between the
local beliefs formed via (37)‚Äì(39) with the joint centralized
beliefs formed via (22)‚Äì(23):
Jk;i,EFiDKL(ijjk;i); (43)
and
eJk;i,EFa
i 1DKL(ijjk;i): (44)
The risks in (43) and (44) measure the disagreement after
and before the joint observation i, respectively. Remember
that [49] considers a naive state estimation setting rather
than a POMDP. SpeciÔ¨Åcally, in their setting, the transition
model does not depend on actions, and it is assumed that
every agent knows the global transition model accurately.
In comparison, in the current work, each agent uses a local
approximation for the global transition model based on (28).
Therefore, we need to make some non-trivial adjustments to
the belief disagreement analysis. We begin with adjusting
the assumptions from [49] to our model.
1) Modeling Conditions
Likelihood functions : Each observation has bounded
information about the true state. More formally,
DKL(Lk(js)jjLk(js0))<1 (45)
which ensures that likelihoods for each state pair ( s,s0)
share the same support, and in addition to this,
logLk(js)B (46)
over its support for each state s2S and agentk2K.
Transition model : The Markov chain induced by any
joint action a2A isirreducible andaperiodic . Since
the number of states is Ô¨Ånite, this assumption implies
that the transition model T(sjs0;a)is ergodic [73,
Chap. 2]. Like [49], we focus on the important class
ofgeometrically ergodic models, which additionally
satisfy the relation (Ta)(T)for some constant
(T)<1. Here,(Ta)is the Dobrushin coefÔ¨Åcient [12,
Chap. 2] deÔ¨Åned by:
(Ta),sup
s0;s002S1
2X
s2STa
ss0 Ta
ss00; (47)
whereTa
ss0,T(sjs0;a)is a generic entry of the
SStransition matrix Ta. Due to space limitations, werefer the reader to [12, Chap. 2] for a comprehensive
discussion on the Dobrushin coefÔ¨Åcient (Ta). In short,
(Ta)quantiÔ¨Åes how fast the transition model forgets
its initial conditions. Namely, as (Ta)!0, past con-
ditions are forgotten faster. Instances of geometrically
ergodic transition models include transition matrices
with all positive elements, or that satisfy the minoriza-
tion condition in [12, Theorem 2.7.4]. In addition to
this condition from [48], [49], we have an additional
assumption on the transition model to regulate the
disagreement stemming from the local transition model
estimates:
Assumption 1 (Transition model disagreement ).For
each agent k, consider the n-hop neighbors set Nkn
and its complement Nc
kn. In other words, Nknis the
set of agents that have at most n-hop distance to the
agentk. We deÔ¨Åne the transition model approximation
that usesn-hop neighbors‚Äô actions as follows:
T
k(sjs0;aNkn)
/X
ac
Nkn2Ac
NknT(sjs0;aNkn;ac
Nkn)(aNkn;ac
Nknjs0):
(48)
Then, we assume that
DKL 
T
k
ss0;aNknT
k
ss0;aNkn+1!
<1;
(49)
which ensures that transition model approximations
induced from n-hop and (n+1)-hop neighbors‚Äô actions
share the same support. Moreover, we assume that over
the shared support,
logT
k
sjs0;aNkn
T
k
sjs0;aNkn+1: (50)
forn1.

This assumption basically makes sure that the increase
in the error of the transition model approximation of
agents due to lack of information about actions is
bounded at each geodesic distance increase to that
agent.
2) Difference with Centralized Strategy
The following result provides upper bounds on the disagree-
ment measures in (43)‚Äì(44).
Theorem 1 (Bounds on belief disagreement). For each
agentk, the belief disagreement risks (43) and (44) get
bounded with a linear rate of (T). Namely, as i!1 ,
Jk;i2p
KB
1 (T)+(K dmin)
1 (T)(51)
and
eJk;i2(T)p
KB
1 (T)+(K dmin)
1 (T)(52)
8wheredminis the minimum degree over the graph, i.e.,
minimum number of neighbors any agent over the network
possesses, and ,maxfj1 K
j;2gwhere2<1is the
mixing rate (second largest modulus eigenvalue) of C.
Proof. See Appendix A. 
In Theorem 1, the Ô¨Årst terms in both bounds are equivalent
to the bounds obtained in [49]. However, the terms propor-
tional to (K dmin)are new, and they arise from the fact
that agents do not observe the joint actions and hence only
have a local estimate of the transition model. Nevertheless,
the bounds get smaller with increasing network connectivity,
i.e., as2!0anddmin!K, which shows the beneÔ¨Åt of
cooperation. In particular, if =Kand the network is fully
connected (2= 0;dmin=K), then the bounds are equal to
0. In other words, local beliefs match the centralized belief
in this situation. It is important to note that the linear term
(K dmin)represents a worst-case bound that holds true
for any strongly connected network topology. For instance,
in a scenario where each agent has N > 1neighbors, it
is straightforward to modify the proof and show that these
linear terms will instead be logarithmic, i.e., proportional to
logK=logN.
We use Theorem 1 in the performance analysis of the
diffusion policy evaluation. To that regard, we Ô¨Årst present
the following consequence of Theorem 1, which provides a
bound in terms of disagreement norms.
Corollary 1 (Bounds on disagreement norms). Theorem 1
implies that, as i!1 ,
Ei k;iBTV (53)
and
Ei k;ieBTV; (54)
where we introduce the constants
BTV,2 
1 expn
 2p
KB + (K dmin)
1 (T)o!1=2
(55)
and
eBTV,2 
1 expn
 2(T)p
KB + (K dmin)
1 (T)o!1=2
(56)
Proof. See Appendix B. 
B. Network Disagreement
In this section, we study the variation of agent parameters
from the network centroid. To that end, let us incorporate the
linear approximation bV(;w) =()Twinto the TD-error
expression (40) to obtain the following relation:
k;i=rk;i+(k;i+1)Twk;i (k;i)Twk;i: (57)
SincerwbV(;w) =()for the linear case, it follows that
zk;i+1=
(1 2)I Hk;i
wk;i+dk;i; (58)where
Hk;i,(k;i)(k;i)T (k;i)(k;i+1)T;(59)
and
dk;i,rk;i(k;i): (60)
To proceed, we introduce the following regularity assump-
tion on the feature vector.
Assumption 2 (Feature vector). The feature mapping ()
is bounded and Lipschitz continuous in the domain of the
S-dimensional probability simplex. Namely, for any vectors
1;22M (S),
k(1) (2)kLk1 2k;k(1)kB:(61)

Lemma 1 (Belief feature difference). For each agent
k2K, the belief feature matrix Hk;iin(59) has bounded
expected difference in relation to the centralized belief fea-
ture matrixH?
i, deÔ¨Åned below, i.e.,
EkHk;i H?
ik2BLBTV(1 +); (62)
where
H?
i,(i)(i)T (i)(i+1)T: (63)
Proof. See Appendix C. 
We also assume that all rewards are non-negative and uni-
formly bounded, i.e., 0rk;iRmaxfor each agent k2K,
and all time instants i. Now, we proceed to study the network
disagreement. To that end, we deÔ¨Åne the network centroid
as
wc;i,1
KKX
k=1wk;i; (64)
which is an average of the parameters of all agents. The
following result shows that the agents cluster around this
network centroid after sufÔ¨Åcient iterations.
Theorem 2 (Network agreement). The average distance
to the network centroid is bounded for  > BL=p
2
after sufÔ¨Åcient number of iterations. In particular, if 
0:75BL, then
1
KKX
k=1Ekwk;i wc;ik2
(1 2)+O(2) (65)
where>0is a constant deÔ¨Åned by
,RmaxB2BTV(1 +)
0:08+ 1
: (66)
Proof. See Appendix D. 
Theorem 2 states that the parameter estimates by the
agents cluster around the network centroid within mean `2-
distance on the order of O(2)in the limit as i!1 .
This result conÔ¨Årms that agents can get arbitrarily close to
each other by setting the learning rate sufÔ¨Åciently small.
Besides, dense networks have in general small 2, which
results in a small disagreement within the network.
9KAYAALP ET AL .: POLICY EVALUATION IN DECENTRALIZED POMDPs WITH BELIEF SHARING
C. Performance of Diffusion Policy Evaluation
We can therefore use the network centroid as a proxy for
all agents to show that the disagreement between the fully
decentralized strategy of Alg. 2 and a baseline strategy that
requires a central processor during training is bounded. We
start by describing this baseline strategy and explain why
it is a more suitable baseline compared to using the fully
centralized strategy Alg. 1.
In some applications, even though agents are supposed
to work in a decentralized fashion once implemented in the
Ô¨Åeld, they can nevertheless rely on central processing during
the training phase in order to learn the best policy. In the
literature, this paradigm is referred to as centralized training
for decentralized execution [16], [74]. For our problem, the
crucial point is that during training the centralized processor
can form beliefs based on all observations, but it should keep
in mind that agents will execute their actions based on local
beliefs once implemented. Therefore, in the baseline strategy,
actions and rewards are based on local beliefs as in (37)‚Äì
(39), whereas parameter updates are based on the centralized
posterior as in (22)‚Äì(23). Algorithm 3 lists this baseline
procedure. Notice that the algorithm consists of both local
belief construction (see (67), (68), and (70)) and centralized
belief construction (see (69) and (71)). The former is used
for action execution ak;ik(akjk;i), while the latter is
used for value function parameter updates in (72)‚Äì(73).
In the fully centralized strategy of Alg. 1, the actions
by the agents and the subsequent rewards are based on the
centralized belief. Therefore, the target value function that
Alg. 1 aims to learn corresponds to the average cumulative
reward obtained under centralized execution. In comparison,
the target value functions that Algs. 2 and 3 try to learn
are the same and they correspond to the average cumulative
reward under decentralized execution. While trying to learn
the same parameter w, the baseline strategy can utilize
centralized processing, but the diffusion strategy is fully
decentralized. Nonetheless, the following result illustrates
that the expected disagreement between the baseline strategy
and the fully decentralized strategy remains bounded.
Theorem 3 (Disagreement with the baseline solution).
The expected distance between the baseline strategy and the
network centroid is bounded after sufÔ¨Åcient iterations for
>BL=p
2. In particular, if 0:75BL, then
Ekw?
i wc;ikBTVRmax0
0:08BL(74)
afterii0=o(1=(BL))iterations, where 0>0is
a constant deÔ¨Åned by
0,2B(1 +)
0:08+L: (75)
Proof. See Appendix E. 
Theorem 3 implies that the disagreement between the
network centroid, around which agents cluster, and the
baseline strategy is on the order of BTV. This means that ifAlgorithm 3 Centralized evaluation for decentralized exe-
cution
1:set initial priors k;0(s)>0;0(s)>0, for8s2S and
8k2K
2:choose >0
3:initializew?
0
4:whilei0do
5: each agentkobservesk;i
6: foreach agentk2K ands2Sadapt andcombine
 k;i(s)/(Lk(k;ijs))k;i(s) (67)
k;i(s)/Y
`2Nk 
 `;i(s)a`k(68)
7: end for
8: to form centralized belief with joint observation i,
fk;igK
k=1,adapt
i(s)/L(ijs)i(s) (69)
9: foreach agentk2K do
10: Take actionak;ik(akjk;i)
11: Get rewardrk;i=rk(si;ai;si+1)
12: end for
13: average the rewardsr?
i=1
KPK
k=1rk;i
14: foreach agentk2K evolve
15: Compute T
k(sjs0;aNk;i)using (28), and
k;i+1(s) =X
s02ST
k(sjs0;aNk;i)k;i(s0) (70)
16: end for
17: evolve the centralized belief
i+1(s) =X
s02ST(sjs0;ai)i(s0) (71)
18: update value function parameter
?
i=r?
i+bV(i+1;w?
i) bV(i;w?
i) (72)
w?
i+1= (1 2)w?
i+?
irwbV(i;w?
i) (73)
19:end while
the local beliefs are similar to the centralized belief, agents
get closer to the baseline parameter. In this regard, from
the deÔ¨Ånition (55) of BTV, it can be observed that BTVgets
smaller with increasing network connectivity (i.e., decreasing
2), as!K. In fact, it is equal to zero for fully-connected
networks with the choice of =Kandc`k= 1=K.
Therefore, by changing andc`k, the fully decentralized
strategy can match the value function estimates of a cen-
tralized training strategy that can gather all observations and
actions in a fusion center. In the next section, by means of
numerical simulations, we further compare the value function
estimate accuracies of all Algorithms 1, 2 and 3 by using
squared Bellman error (SBE).
10(a) Initial positions at the beginning of an
iteration.
(b) Agents receive noisy observations and in-
corporate them into their beliefs.
(c) Agents exchange beliefs with their imme-
diate neighbors.
(d) Agents take actions based on the beliefs.
The target relocates based on the actions.
(e) Agents update and exchange value function
parameters.
(f) The legend. Image credit for agent, target,
and action: freepik.com
FIGURE 2: Experimental scenario. For visual purposes, the procedure is shown for only one agent. In fact, all agents
execute the same procedure simultaneously.
VI. SIMULATION RESULTS
For numerical simulations, we consider a multi-agent target
localization application. The implementation is available
online1. We use a set of K= 8 agents and a moving target
in a1010two-dimensional grid world environment. The
locations of the agents are Ô¨Åxed and their coordinates are
randomly assigned at the beginning of the simulation. The
goal of the agents is to cooperatively evaluate a given policy
for hitting the target. Agents cannot observe the location
(i.e., state) of the target accurately, but instead receive noisy
observations based on how far they are from the real location
of the target. The target is moving according to some pre-
deÔ¨Åned transition model that takes the actions (i.e., hits) of
agents into account. SpeciÔ¨Åcally, the target is trying to evade
the hits of agents.
A possible scenario for this setting is a network of
sensors and an intruder (e.g., a spy drone) ‚Äî see Fig. 2.
The sensors try to localize the intruder based on noisy
measurements and belief exchanges. Moreover, in order
to disrupt the communication between the intruder and its
owner, each sensor sends a narrow sector jamming beam
towards its target location estimate. However, the intruder
is capable of detecting energy abnormalities and determines
its next location by favoring distant locations from the
1github.com/asl-epÔ¨Ç/DecPOMDP Policy Evaluation w-Belief Sharingjamming signals. We now describe the setting in more detail.
Combination matrix: The entries of the combination matrix
are set such that they are inversely proportional to the `1-
distance between the agents. That is to say, the further the
agents are from each other, the smaller the value of the
weight that is assigned to the edge connecting them. Weights
smaller than some threshold are set to 0, which implies
that agents that are too far from each other do not need to
communicate. The resulting communication topology graph
is illustrated in Fig. 3a.
Transition model: The target is moving between cells in a
grid (i.e., states) randomly. The probability of a cell being the
next location of the target depends on the current location of
the target and the location of the agents‚Äô hits. Namely, each
state in the grid is assigned a score based on its `1-distance to
the current location of the target and to the average location
of the agents‚Äô hits ‚Äî see Table 1. For example, observe from
Table 1 that the cells that are in the proximity of the target‚Äôs
current location and also far away from the agents‚Äô strikes
are given the highest score. These scores are normalized to
yield a probabilistic transition kernel.
Likelihood function: Agents cannot observe where the
target is. They can only receive noisy observations. Each
agent gets a more accurate observation of the target‚Äôs
11KAYAALP ET AL .: POLICY EVALUATION IN DECENTRALIZED POMDPs WITH BELIEF SHARING
(a) Communication topology.
 (b) Agreement error over time.
 (c) SBE over time (running window of size 20) for
CC (Alg. 1), Diffusion (Alg. 2), CD (Alg. 3).
FIGURE 3
initial position
`1-
distance4>4
average location
of agents‚Äô hits<4 10 5
4 100 50
TABLE 1: The table of scores used in the transition model.
Each candidate state for next state (location) of the target
gets a score based on the initial position of the target and
the average action of agents.
position if the target is in close proximity to the agent.
Otherwise, the larger the distance between the agent and
the target, the higher the noise level. Depending on how
close the target is to the agent, and in order to construct
the likelihood function, we Ô¨Årst assign scores to each cell
in the grid that reÔ¨Çect how probable it is to Ô¨Ånd the target
in that cell ‚Äî see Table 2. Following that, the scores are
normalized in order to yield a distribution function. For
instance, if the target lies at an `1-distance that is less than
3 grid squares from the location of the agent, the actual
position of the target gets a likelihood score of 400, cells
within an`1distance of 2 grid squares from the agent get
a likelihood score of 200, and cells within an `1distance
of 4 grid squares from the agents get a likelihood score of 30.
Reward function: The reward function in the environment
is such that an agent receives a reward of 1 if the agent is
able to hit the position of the target. The agent also receives
a reward of 0.2 if the `1-distance between the predicted
location and the actual location of the target is less than 3real location of target
`1-
distance=
0<
3<
5<
7<
9
9
location
of
agent<3 400 200 30 1 1 1
6 200 180 100 1 1 1
>6 25 25 25 25 4 1
TABLE 2: The table of scores used in the likelihood function
model. Each state, when observed, gets a score that deter-
mines the likelihood of the presence of the target within the
state, based on the position of the target and the average
action of agents.
grid units. Otherwise, it gets 0 reward. Agents do not know
the reward model, and use the instantaneous rewards instead.
Policy: We Ô¨Åx the policy that the agents evaluate as the
maximum a-posteriori policy. Namely, agents detect (hit) a
location if it corresponds to the maximum entry in their
belief vector.
We use the belief vectors as the features directly, i.e., 
is an identity transformation. We set = 0:1,= 0:0001 ,
and=K= 8, and average over 3 different realizations
for all cases. In Fig. 3b, the average mean-square distance
to the network centroid, i.e.,
Agreement error,1
KKX
k=1Ekwk;i wc;ik2; (76)
is plotted over time for the fully decentralized strategy.
ConÔ¨Årming Theorem 2, it can be seen that agreement error
rapidly decreases and converges to a small value.
In Fig. 3c, we plot the evolution of the average squared
Bellman error (SBE) in the log domain, where the SBE
12expression is given by:
SBE,1
KKX
k=12
k;i; (77)
and similarly for the centralized cases. It measures the net-
work average of instantaneous TD-errors. It can be seen that
all approaches converge, and in particular, diffusion strategy
(Alg. 2) yields a comparable performance with CD (Alg. 3).
This observation is in line with Theorem 3, which states
that the disagreement between the fully decentralized strat-
egy and the baseline centralized training for decentralized
execution strategy is bounded. Notice also that CC (Alg. 1)
results in a higher SBE compared to the diffusion and CD,
despite being a fully centralized strategy. This is because, CC
evaluates a different policy, namely, the centralized execution
policy. Therefore, as argued in Sec. V-C, the SBE of CC is
not a suitable baseline for the diffusion strategy.
VII. CONCLUDING REMARKS
In this paper, we proposed a policy evaluation algorithm
for Dec-POMDPs over networks. We carried out a rigorous
analysis that established: (i)the beliefs formed with local
information and interactions have a bounded disagreement
with the global posterior distribution, (ii)agents‚Äô value
function parameters cluster around the network centroid, and
(iii)the decentralized training can match the performance
of the centralized training with appropriate parameters and
increasing network connectivity.
There are two limitations of the current work that can
be addressed in future work. First, we assume that agents
know the local likelihood and transition models accurately.
One possible question is if agents have approximation errors
for the models, how would these affect the analytical results.
Second, an implication of Theorem 3 is that there is necessity
for regularization (>0). We leave the question of whether
one can get bounds that does not require this, possibly with
more assumptions on the model, to future work.
APPENDIX
A. Proof of Theorem 1
We can rewrite the risk function as
Jk;i=EFiDKL(ijjk;i)
=EFihX
s2Si(s) logi(s)
k;i(s)i
(a)=EFihX
s2SP(si=sjFi) logi(s)
k;i(s)i
(b)=EFih
EsijFi
logi(si)
k;i(si)i
=EFi;sih
logi(si)
k;i(si)i
; (78)
where (a)follows from deÔ¨Ånition (9), (b)follows from
the deÔ¨Ånition of conditional expectation with respect to sigivenFi. Merging the diffusion adaptation step (37) and the
combination step (38) together yields the following form:
k;i(s)/Y
`2Nk(L`(`;ijs))c`k(`;i(s))c`k; (79)
which, combined with the update equation (22) for the
centralized solution, results in:
logi(s)
k;i(s)=X
`2Nkc`k
logL(ijs)
(L`(`;ijs))+ logi(s)
`;i(s)
+ logX
s02SY
`2Nk(L`(`;ijs0))c`kY
`2Nk(`;i(s0))c`k
 logmi(i): (80)
Here, we have introduced the marginal distribution of new
observation given the past observations and actions:
mi(i),P(i=ijFa
i 1) =X
s2SP(i=i;si=sjFa
i 1)
=X
s2SL(ijs)P(si=sjFa
i 1)
=X
s2SL(ijs)i(s): (81)
First, observe that the expectation of the log-likelihood
ratio terms in (80) satisÔ¨Åes:
X
`2Nkc`kEi;sih
logL(ijsi)
(L`(`;ijsi))i
(a)=Ei;sihKX
`=1logL`(`;ijsi)i
 X
`2Nkc`kE`;i;sih
logL`(`;ijsi)i
=Ei;sihKX
`=1(1 c`k) logL`(`;ijsi)i
(82)
where in (a)we used the spatial independency of the
observations. Second, the expectation of the time-adjusted
terms in (80) can be rewritten as:
X
`2Nkc`kEFi;sih
logi(si)
`;i(si)i
(a)=X
`2Nkc`kEFi;sih
logi(si)
e`;i(si)+ loge`;i(si)
`;i(si)i
=X
`2Nkc`kEFa
i 1;sih
EijFa
i 1;si
logi(si)
e`;i(si)+ loge`;i(si)
`;i(si)i
(b)=X
`2Nkc`kEFa
i 1;sih
logi(si)
e`;i(si)+ loge`;i(si)
`;i(si)i
(83)
where in (a)we deÔ¨Åne the agent-speciÔ¨Åc distribution:
e`;i(s),X
s02ST(sjs0;ai 1)`;i 1(s0); (84)
and(b)follows from the fact that the arguments are deter-
ministic given the current state and the history of actions
13KAYAALP ET AL .: POLICY EVALUATION IN DECENTRALIZED POMDPs WITH BELIEF SHARING
and observations. The Ô¨Årst term of (83) can be written as a
KL-divergence because of the following:
X
`2Nkc`kEFa
i 1;sih
logi(si)
e`;i(si)i
=X
`2Nkc`kEFa
i 1h
EsijFa
i 1
logi(si)
e`;i(si)i
=X
`2Nkc`kEFa
i 1hX
s2SP(si=sjFa
i 1) logi(s)
e`;i(s)i
(11)=X
`2Nkc`kEFa
i 1hX
s2Si(s) logi(s)
e`;i(s)i
=X
`2Nkc`kEFa
i 1h
DKL(ijje`;i)i
: (85)
This expected KL-divergence can be bounded by using the
strong-data processing inequality [75]:
X
`2Nkc`kEFa
i 1h
DKL(ijje`;i)i
X
`2Nkc`k(T)EFi 1h
DKL(i 1jj`;i 1)i
|{z}
J`;i 1:(86)
The second term of (83) arises due to transition model
disagreement with the centralized belief. To bound it, we Ô¨Årst
introduce the LogSumExp function fwith vector arguments
2RS:
f(),logX
s2Sexpf(s)g: (87)
Its gradient is given by
rf(),coln@f()
@(s)o
s2S=colnexpf(s)gP
s0expf(s0)go
s2S:
(88)
Observe that if we deÔ¨Åne the vectors
e`;i,coln
log 
T(sijs;ai 1)`;i 1(s)o
s2S(89)
and
`;i,coln
log 
T
`(sijs;aN`;i 1)`;i 1(s)o
s2S;(90)
then, we can rewrite the second expression of (83) as follows:
X
`2Nkc`kEFa
i 1;sih
loge`;i(si)
`;i(si)i
=X
`2Nkc`kEFa
i 1;sih
f(e`;i) f(`;i)i
: (91)
Applying mean value theorem to this difference yields
EFa
i 1;sih
f(e`;i) f(`;i)i
=EFa
i 1;si
(rf(`;i))T(e`;i `;i)
(88)=EFa
i 1;si
colnexpf`;i(s)gP
s0expf`;i(s0)goT
s2S(e`;i `;i)
(89);(90)=EFa
i 1;si
colnexpf`;i(s)gP
s0expf`;i(s0)goT
s2Scoln
logT(sijs;ai 1)
T
`(sijs;aN`;i 1)o
s2S
(92)
for some`;ibetweene`;iand`;i. The term in (92) is
bounded as follows:EFa
i 1;si
colnexpf`;i(s)gP
s0expf`;i(s0)goT
s2S
coln
logT(sijs;ai 1)
T
`(sijs;aN`;i 1)o
s2S
(a)
EFa
i 1;sicolnexpf`;i(s)gP
s0expf`;i(s0)goT
s2S
coln
logT(sijs;ai 1)
T
`(sijs;aN`;i 1)o
s2S
(b)
EFa
i 1;si"colnexpf`;i(s)gP
s0expf`;i(s0)go
s2S
1
coln
logT(sijs;ai 1)
T
`(sijs;aN`;i 1)o
s2S
1#
(c)=Esi;ai 1coln
logT(sijs;ai 1)
T
`(sijs;aN`;i 1)o
s2S
1(93)
where (a)follows from the Jensen‚Äôs inequality, (b)follows
from the H ¬®older‚Äôs inequality, and (c)follows from the fact
thatcolnexpf`;i(s)gP
s02Sexpf`;i(s0)go
1= 1: (94)
Furthermore, due to Assumption 1 and to the fact that the
number of maximum hops outside Nkis(K jNkj), we
havelogT(sijs;ai 1)
T
k(sijs;aNk;i 1)(K jNkj)
(K dmin): (95)
If we combine (86), (91), and (95), the expectation of the
time-adjusted terms in (80) can be bounded as:
X
`2Nkc`kEFa
i 1;sih
logi(si)
`;i(si)i
(K dmin)+X
`2Nkc`k(T)J`;i 1 (96)
Next, we bound the expectation of the remaining normal-
ization terms in (80), which follows similar steps to what
was done in [49]:
EFi"
logX
s02SY
`2Nk(L`(`;ijs0))c`kY
`2Nk(`;i(s0))c`k#
 EFih
logmi(i)i
(a)
EFi"
logX
s02SY
`2Nk(L`(`;ijs0))c`kX
`2Nkc`k`;i(s0)#
14 EFih
logmi(i)i
=EFi"
logX
s02SY
`2Nk(L`(`;ijs0))c`kX
`2Nkc`k`;i(s0)#
 EFi"
logX
s02SKY
`=1L`(`;ijs0)X
`2Nkc`k`;i(s0)#
+EFi"
logX
s02SKY
`=1L`(`;ijs0)X
`2Nkc`k`;i(s0)#
 EFih
logmi(i)i
(b)
EFi"
logX
s02SKY
`=1(L`(`;ijs0))c`kX
`2Nkc`k`;i(s0)#
 EFi"
logX
s02SKY
`=1L`(`;ijs0)X
`2Nkc`k`;i(s0)#
(97)
where (a)follows from the arithmetic-geometric mean in-
equality, (b)follows from:
 EFi"
logmi(i)
P
s02SQK
`=1L`(`;ijs0)P
`2Nkc`k`;i(s0)#
= EFa
i 1EijFa
i 1"
logmi(i)
my
i(i)#
= EFa
i 1DKL(mi(i)jjmy
i(i))
0 (98)
where we use the deÔ¨Ånition:
my
i(i),X
s02SKY
`=1L`(`;ijs0)X
`2Nkc`k`;i(s0)
;(99)
which is a density (or mass function if observations are
discrete) since:
Z
imy
i(i)di=Z
iX
s02SKY
`=1L`(`;ijs0)X
`2Nkc`k`;i(s0)
di
=X
s02ShZ
iKY
`=1L`(`;ijs0)di
|{z}
1KX
`=1c`k`;i(s0)i
=X
s02ShKX
`=1c`k`;i(s0)i
=KX
`=1c`khX
s02S`;i(s0)i
= 1: (100)
Notice that the expression in (97) can be rewritten as
EFi"
logX
s02SKY
`=1(L`(`;ijs0))c`kX
`2Nkc`k`;i(s0)#
 EFi"
logX
s02SKY
`=1L`(`;ijs0)X
`2Nkc`k`;i(s0)#=EFih
f(#k;i)i
 EFih
f(e#k;i)i
; (101)
if we use the LogSumExp function ffrom (87) and use the
deÔ¨Ånitions:
#k;i,col(
logKY
`=1(L`(`;ijs0))c`kX
`2Nkc`k`;i(s0))
s02S(102)
and
e#k;i,col(
logKY
`=1L`(`;ijs0)X
`2Nkc`k`;i(s0))
s02S:
(103)
Following the steps in (92) and (93), this difference can be
bounded as:
EFih
f(#k;i)i
 EFih
f(e#k;i)i
EicolnKX
`=1(c`k 1) logL`(`;ijs0)o
s02S
1:(104)
Moreover, by assumptions on the graph topology (27) and on
the likelihood functions (46), this expression can be further
bounded as [49]:colnKX
`=1(c`k 1) logL`(`;ijs0)o
s02S
1p
KB
(105)
Subsequently, if we insert the bounds (82), (96), and (105)
to (80), we arrive at the bound on the risk function:
Jk;iEi;sihKX
`=1(1 c`k) logL`(`;ijsi)i
+(T)X
`2Nkc`kJ`;i 1+p
KB + (K dmin)
(105)
(T)X
`2Nkc`kJ`;i 1+ 2p
KB + (K dmin):
(106)
Expanding this recursion over time yields:
Jk;i(2p
KB + (K dmin))i 1X
j=0((T))j
+ ((T))iKX
`=1[Ci]`kJ`;0
=1 ((T))i
1 (T)(2p
KB + (K dmin))
+ ((T))iKX
`=1[Ci]`kJ`;0; (107)
which implies that if (T)<1, the risk function is bounded
asi!1 :
lim sup
i!1Jk;i2p
KB + (K dmin)
1 (T): (108)
15KAYAALP ET AL .: POLICY EVALUATION IN DECENTRALIZED POMDPs WITH BELIEF SHARING
By (96), this also implies that
lim sup
i!1eJk;i(K dmin)+(T) lim sup
i!1Jk;i
(K dmin)
1 (T)+(T)2p
KB
1 (T):(109)
B. Proof of Corollary 1
In view of the Bretagnolle-Huber inequality [76], it holds
thatX
s2Si(s) k;i(s)2 
1 expf DKL(ijjk;i)g1
2:
(110)
If we take the expectation of both sides, we get:
EhX
s2Si(s) k;i(s)i
2E 
1 expf DKL(ijjk;i)g1
2
(a)
2 
1 Eexpf DKL(ijjk;i)g1
2
(b)
2 
1 expf Jk;ig1
2; (111)
where (a)and(b)follow from Jensen‚Äôs inequality. Together
with Theorem 1, this implies that
Ei k;i
1BTV; (112)
where we use the deÔ¨Ånition (55). Furthermore, on account
of the fact that `2norm is no greater than `1norm in RS,
it is also true that
Ei k;iBTV: (113)
With similar arguments, it can be shown that
Ei k;ieBTV; (114)
where we use the deÔ¨Ånition (56).
C. Proof of Lemma 1
Inserting the deÔ¨Ånitions (59) and (63), the expected differ-
ence can be expanded as
EkHk;i H?
ik=E(k;i)(k;i)T (k;i)(k;i+1)T
 (i)(i)T+(i)(i+1)T
E(k;i)(k;i)T (i)(i)T
+E(k;i)(k;i+1)T (i)(i+1)T;
(115)
where the last step follows from the triangle inequality. Here,
the Ô¨Årst term can be bounded as(k;i)(k;i)T (i)(i)T
(k;i)((k;i)T (i)T)
+((k;i) (i))(i)T
(k;i)(k;i) (i)
+(k;i) (i)(i)
(a)
BLkk;i ik+BLkk;i ik;(116)where (a)follows from Assumption 2. Taking expectations
and using (53) and (116), it follows that
E(k;i)(k;i)T (i)(i)T2BLBTV:(117)
Similarly, the second term in (115) can be bounded as(k;i)(k;i+1)T (i)(i+1)T
(k;i)((k;i+1)T (i+1)T)
+((k;i) (i))(i+1)T
(k;i)(k;i+1) (i+1)
+(k;i) (i)(i+1)
(a)
BLkk;i+1 i+1k+BLkk;i ik(118)
where (a)follows from Assumption 2. Using (53) and (54)
we get:
E(k;i)(k;i+1)T (i)(i+1)T
BL(BTV+eBTV): (119)
Combining (117) and (119) in addition to the fact that
eBTVBTV(since(T)<1) yields:
EkHk;i H?
ik2BLBTV(1 +): (120)
D. Proof of Theorem 2
For compactness of notation, it is useful to introduce the
following quantities, which collect variables from across all
agents:
Wi,colfw1;i;:::;wK;ig (121)
C,C
IM (122)
Hi,diagfHk;igK
k=1(123)
H?
i,IK
H?
i (124)
di,colfdk;igK
k=1(125)
Then, the equations (40)‚Äì(42) can be written as:
Wi+1=CT
(I(1 2) Hi)Wi+di
: (126)
Moreover, we can deÔ¨Åne the following K-times extended
centroid vector:
Wc;i,1K
wc;i=1
K1K1T
K
I
Wi: (127)
If we decompose Hiinto its centralized component H?
i
and the respective disagreement matrix i,Hi H?
i,
we obtain:
Wi+1 Wc;i+1
=
CT 1
K1K1T
K
I
(I(1 2) Hi)Wi+di
=
CT 1
K1K1T
K
I

(I(1 2) H?
i i)Wi+di
=
CT 1
K1K1T
K
I
16
(I(1 2) H?
i) (Wi Wc;i) iWi+di
;
(128)
where the last step follows from the fact that
CT 
I(1 2) H?
i
Wc;i
=1
K1K1T
K
I 
I(1 2) H?
i
Wc;i:(129)
Furthermore, taking the norms of both sides in (128) leads
toWi+1 Wc;i+1
CT 1
K1K1T
K
I
(I(1 2) H?
i) (Wi Wc;i) iWi+di
CT 1
K1K1T
K
IkI(1 2) H?
ikkWi Wc;ik
+CT 1
K1K1T
K
I
kikkWik+kdik
:(130)
Since the combination matrix Cis a primitive stochastic
matrix, it follows from the Perron-Frobenius theorem [53],
[77] that its maximum eigenvalue is 1, and all other eigenval-
ues are strictly smaller than 1 in absolute value. Moreover,
Cis assumed to be symmetric, therefore its eigenvalue
decomposition has the following form:
C=UU>
=
u1u2uK2
66641 0 0
02 0
............
0 0K3
77752
6664u>
1
u>
2
...
u>
K3
7775
whereUis the orthogonal matrix of eigenvectors fukg, and
is the diagonal matrix of eigenvalues. Additionally, the
powers ofCconverge (because it is primitive) to the scaled
all-ones matrix (because it is doubly-stochastic):
lim
i!1Ci=
u1u2uK2
66641 0 0
0 0 0
............
0 0 03
77752
6664u>
1
u>
2
...
u>
K3
7775
=1
K2
66641 1 1
1 1 1
............
1 1 13
7775=1
K1K1>
K
Therefore, the difference of these matrices becomes:
C 1
K1K1>
K=U2
66640 0 0
02 0
............
0 0K3
7775U>; (131)
which implies:
C 1
K1K1>
K=2 (132)where2is the second largest modulus eigenvalue of C.
Moreover, the Kronecker product with the identity matrix
does not change the spectral norm, hence:
CT 1
K1K1T
K
I=2<1: (133)
Moreover, we know from Lemma 1 that
Ekik2BLBTV(1 +): (134)
Additionally, in Appendix F, we establish (135)‚Äì(138) which
hold for any realization (with probability one). From (161),
note that: I(1 2) H?
i<1 (135)
whenever>LB=p
2. SpeciÔ¨Åcally, if 0:75LB,
thenI(1 2) H?
i(1 0:08LB): (136)
In addition, we show in Lemma 2 that
kWikp
KR max
0:08L(137)
and in expression (162) that
kdikp
KR maxB: (138)
Inserting these results into (130) yields the following norm
recursion:
EWi+1 Wc;i+1
2(1 0:08BL)EWi Wc;i+2p
K:
(139)
Let us deÔ¨Åne the constant e2,2(1 0:08BL).
Iterating (139) over time, we arrive at
EkWi+1 Wc;i+1k
ei+1
2kW0 Wc;0k+2p
Ki+1X
j=1ei+1 j
2
ei+1
2kW0 Wc;0k+2p
K1
1 e2
(a)
2p
K1
1 e2+O(2) (140)
where (a)holds whenever:
ei
2kW0 Wc;0kc2
()iloge22 log+ logc logkW0 Wc;0k
()i2 log
loge2+O(1) =O(log) =o(1=);(141)
wherecis an arbitrary constant.
17KAYAALP ET AL .: POLICY EVALUATION IN DECENTRALIZED POMDPs WITH BELIEF SHARING
E. Proof of Theorem 3
We begin by rewriting the baseline strategy recursion (72)‚Äì
(73) in the form:
w?
i+1= ((1 2)I H?
i)w?
i+d?
i; (142)
whereH?
iis deÔ¨Åned in (63), and
d?
i,1
KKX
k=1rk;i
(i): (143)
We introduce the K-times extended versions of the vectors:
D?
i,1K
d?
i;W?
i=1K
w?
i: (144)
Then, the baseline recursion (142) transforms into
W?
i+1= ((1 2)I H?
i)W?
i+D?
i: (145)
It follows from the extended network centroid deÔ¨Ånition
(127) and (145) that
W?
i+1 Wc;i+1
= (I(1 2) H?
i) (W?
i Wc;i)
 1
K1K1T
K
I
iWi+1
K1K1T
K
I
(D?
i di)
(146)
where we used the facts that1
K1K1T
K
I
D?
i=D?
i; (147)
and1
K1K1T
K
I
H?
iWi=H?
iWc;i: (148)
Next, if we deÔ¨Åne the following average agent disagreement
relative to the baseline term
edi,1
KKX
k=1(d?
i dk;i); (149)
it holds that
eDi,1K
edi=1
K1K1T
K
I
(D?
i di): (150)
Subsequently, taking the norm of both sides in (146) and
applying the triangle inequality, we getW?
i+1 Wc;i+1
I(1 2) H?
iW?
i Wc;i
+1
K1K1T
K
IiWi+eDi:(151)
First, observe that1
K1K1T
K
I= 1: (152)
Moreover, from Assumption 2 and Corollary 1, it holds that
Eedi=E1
KKX
k=1rk;i((i) (k;i))
RmaxLBTV; (153)and accordingly,
EeDip
KR maxLBTV: (154)
By using the same bounds (135)‚Äì(138) from Appendix D
for the other terms (which are established in Lemma 1,
Lemma 2, (161), and (162)), we arrive at the recursion:
EW?
i+1 Wc;i+1
(1 0:08BL)EW?
i Wc;i+p
K?;
(155)
where
?,RmaxBTV2B(1 +)
0:08+L
: (156)
Iterating over time, we get:
EW?
i+1 Wc;i+1
(1 0:08BL)i+1kW?
0 Wc;0k
+p
K?i+1X
j=1(1 0:08BL)i+1 j
(1 0:08BL)i+1kW?
0 Wc;0k+p
K?
0:08BL
(a)
p
K?
0:08BL+o(1) (157)
where (a)holds whenever
(1 0:08BL)i+1W?
0 Wc;0=o(1)
()ilog(1 0:08BL) =o(1)
()io(1)
log(1 0:08BL)o1
BL
:
(158)
F. Auxiliary Results
In the following lemma, we prove that the value function
parameters are bounded in norm.
Lemma 2 (Bounded parameters). For each agent k2K,
the iteratewk;iis bounded in norm if >BL=p
2, with
probability one. In particular, if 0:75BL, then
kWikp
KR max
0:08L(159)
afterii0=o(1=(BL))iterations.
Proof. Taking the norms of both sides of (126) yields:
Wi+1=CT 
((1 2)I Hi)Wi+di
CT((1 2)I Hi)Wi+di
(a)
((1 2)I Hi)Wi+di
(1 2)I HiWi+di(160)
where (a)follows from the fact that the singular values of
doubly-stochastic matrices are equal to one. Note that
(1 2)I Hk;i
18=(1 2)I (k;i)(k;i)T+(k;i)(k;i+1)T
=(1 2)I (1 )(k;i)(k;i)T
 (k;i) 
(k;i)T (k;i+1)T
(1 2)I (1 )(k;i)(k;i)T
+k(k;i)k(k;i)T (k;i+1)T
(a)
(1 2) +k(k;i)k(k;i)T (k;i+1)T
(b)
(1 2) +BLk;i k;i+1
(c)
(1 2) +BLp
2 (161)
where (a)follows from the equality of spectral norm and
maximum eigenvalue for symmetric matrices, (b)follows
from Assumption 2, and (c)follows from the fact that the
mean-square distance cannot exceed 2 over the probability
simplex. The upper bound in (161) is smaller than 1 when-
ever>BL=p
2. Moreover,
dk;i=rk;i(k;i)RmaxB: (162)
As a result, if 0:75BL, we get:
Wi+1(161)
(1 0:08BL)Wi+di
(162)
(1 0:08BL)Wi+p
KR maxB:
(163)
Iterating this recursion starting from i= 0 results in
Wi+1p
KR maxBi+1X
j=1(1 0:08BL)i+1 j
+ (1 0:08BL)i+1W0
p
KR max
0:08L+ (1 0:08BL)i+1W0
=p
KR max
0:08L+o(1); (164)
where the last step holds whenever
(1 0:08BL)i+1W0=o(1)
()ilog(1 0:08BL) =o(1)
()io(1)
log(1 0:08BL)o1
BL
:
(165)

ACKNOWLEDGMENTS
The authors would like to thank Visa Koivunen for useful
discussions on Dec-POMDPs and their applications in sens-
ing.
REFERENCES
[1] L. Busoniu, R. Babuska, and B. De Schutter, ‚ÄúA comprehensive survey
of multiagent reinforcement learning,‚Äù IEEE Transactions on Systems,
Man, and Cybernetics, Part C (Applications and Reviews) , vol. 38,
no. 2, pp. 156‚Äì172, 2008.[2] K. Zhang, Z. Yang, and T. Bas ¬∏ar, ‚ÄúMulti-agent reinforcement learn-
ing: A selective overview of theories and algorithms,‚Äù Handbook of
Reinforcement Learning and Control , pp. 321‚Äì384, 2021.
[3] J. W. Huang, Q. Zhu, V . Krishnamurthy, and T. Basar, ‚ÄúDistributed
correlated Q-learning for dynamic transmission control of sensor
networks,‚Äù in Proc. IEEE International Conference on Acoustics,
Speech and Signal Processing , 2010, pp. 1982‚Äì1985.
[4] J. Lunden, S. R. Kulkarni, V . Koivunen, and H. V . Poor, ‚ÄúMultiagent
reinforcement learning based spectrum sensing policies for cognitive
radio networks,‚Äù IEEE Journal of Selected Topics in Signal Processing ,
vol. 7, no. 5, pp. 858‚Äì868, 2013.
[5] S. Bhattacharya, S. Kailas, S. Badyal, S. Gil, and D. Bertsekas,
‚ÄúMultiagent rollout and policy iteration for POMDP with application to
multi-robot repair problems,‚Äù in Proc. Conference on Robot Learning .
PMLR, 2021, pp. 1814‚Äì1828.
[6] O. Vinyals, I. Babuschkin, W. M. Czarnecki, M. Mathieu, A. Dudzik,
J. Chung, D. H. Choi, R. Powell, T. Ewalds, P. Georgiev, et al. ,
‚ÄúGrandmaster level in StarCraft II using multi-agent reinforcement
learning,‚Äù Nature , vol. 575, no. 7782, pp. 350‚Äì354, 2019.
[7] M. Samvelyan, T. Rashid, C. Schroeder de Witt, G. Farquhar,
N. Nardelli, T. G. J. Rudner, C.-M. Hung, P. H. S. Torr, J. Foerster,
and S. Whiteson, ‚ÄúThe starcraft multi-agent challenge,‚Äù in Proc. Inter-
national Conference on Autonomous Agents and MultiAgent Systems ,
ser. AAMAS ‚Äô19, 2019, p. 2186‚Äì2188.
[8] Y . LeCun, Y . Bengio, and G. Hinton, ‚ÄúDeep learning,‚Äù Nature , vol.
521, no. 7553, pp. 436‚Äì444, 2015.
[9] F. A. Oliehoek and C. Amato, A Concise Introduction to Decentralized
POMDPs . Springer, 2016.
[10] E. J. Sondik, ‚ÄúThe optimal control of partially observable Markov
processes over the inÔ¨Ånite horizon: Discounted costs,‚Äù Operations
Research , vol. 26, no. 2, pp. 282‚Äì304, 1978.
[11] L. P. Kaelbling, M. L. Littman, and A. R. Cassandra, ‚ÄúPlanning and
acting in partially observable stochastic domains,‚Äù ArtiÔ¨Åcial Intelli-
gence , vol. 101, no. 1, pp. 99‚Äì134, 1998.
[12] V . Krishnamurthy, Partially Observed Markov Decision Processes:
From Filtering to Controlled Sensing . Cambridge University Press,
2016.
[13] J. Hazla, A. Jadbabaie, E. Mossel, and M. A. Rahimian, ‚ÄúBayesian
decision making in groups is hard,‚Äù Operations Research , vol. 69,
no. 2, pp. 632‚Äì654, 2021.
[14] S. OmidshaÔ¨Åei, J. Pazis, C. Amato, J. P. How, and J. Vian, ‚ÄúDeep
decentralized multi-task multi-agent reinforcement learning under par-
tial observability,‚Äù in Proc. International Conference on Machine
Learning . PMLR, 2017, pp. 2681‚Äì2690.
[15] J. K. Gupta, M. Egorov, and M. Kochenderfer, ‚ÄúCooperative multi-
agent control using deep reinforcement learning,‚Äù in Proc. AAMAS ,
2017, pp. 66‚Äì83.
[16] J. Foerster, G. Farquhar, T. Afouras, N. Nardelli, and S. Whiteson,
‚ÄúCounterfactual multi-agent policy gradients,‚Äù in Proc. AAAI Confer-
ence on ArtiÔ¨Åcial Intelligence , 2018, pp. 2974‚Äì2982.
[17] P. Moreno, J. Humplik, G. Papamakarios, B. A. Pires, L. Buesing,
N. Heess, and T. Weber, ‚ÄúNeural belief states for partially observed
domains,‚Äù in NeurIPS Workshop on Reinforcement Learning under
Partial Observability , 2018, pp. 1‚Äì5.
[18] K. Gregor, G. Papamakarios, F. Besse, L. Buesing, and T. Weber,
‚ÄúTemporal difference variational auto-encoder,‚Äù in Proc. International
Conference on Learning Representations , 2019, pp. 1‚Äì17.
[19] A. H. Sayed, Inference and Learning from Data . Cambridge
University Press, 2022, 3 vols.
[20] P. Moreno, E. Hughes, K. R. McKee, B. A. Pires, and T. Weber,
‚ÄúNeural recursive belief states in multi-agent reinforcement learning,‚Äù
arXiv:2102.02274 , 2021.
[21] D. Muglich, L. M. Zintgraf, C. A. S. De Witt, S. Whiteson, and J. Fo-
erster, ‚ÄúGeneralized beliefs for cooperative AI,‚Äù in Proc. International
Conference on Machine Learning , vol. 162, Jul 2022, pp. 16 062‚Äì
16 082.
[22] W. Mao, K. Zhang, E. Miehling, and T. Basar, ‚ÄúInformation state em-
bedding in partially observable cooperative multi-agent reinforcement
learning,‚Äù in Proc. IEEE CDC , 2020, pp. 6124‚Äì6131.
[23] S. Kar, J. M. F. Moura, and H. V . Poor, ‚Äú QD-learning: A collaborative
distributed strategy for multi-agent reinforcement learning through
consensus+innovations ,‚ÄùIEEE Transactions on Signal Processing ,
vol. 61, no. 7, pp. 1848‚Äì1862, 2013.
19KAYAALP ET AL .: POLICY EVALUATION IN DECENTRALIZED POMDPs WITH BELIEF SHARING
[24] K. Zhang, Z. Yang, H. Liu, T. Zhang, and T. Basar, ‚ÄúFully decentral-
ized multi-agent reinforcement learning with networked agents,‚Äù in
Proc. International Conference on Machine Learning . PMLR, 2018,
pp. 5872‚Äì5881.
[25] L. Cassano, K. Yuan, and A. H. Sayed, ‚ÄúMultiagent fully decentralized
value function learning with linear convergence rates,‚Äù IEEE Transac-
tions on Automatic Control , vol. 66, no. 4, pp. 1497‚Äì1512, 2021.
[26] S. V . Macua, I. Davies, A. Tukiainen, and E. M. De Cote, ‚ÄúFully
distributed actor-critic architecture for multitask deep reinforcement
learning,‚Äù The Knowledge Engineering Review , vol. 36, pp. 1‚Äì30,
2021.
[27] X. Sha, J. Zhang, K. You, K. Zhang, and T. Bas ¬∏ar, ‚ÄúFully asynchronous
policy evaluation in distributed reinforcement learning over networks,‚Äù
Automatica , vol. 136, p. 110092, 2022.
[28] Y . Lin, G. Qu, L. Huang, and A. Wierman, ‚ÄúMulti-agent reinforcement
learning in stochastic networked systems,‚Äù in Advances in Neural
Information Processing Systems , vol. 34, 2021, pp. 7825‚Äì7837.
[29] G. Wang, S. Lu, G. Giannakis, G. Tesauro, and J. Sun, ‚ÄúDecentralized
TD tracking with linear function approximation and its Ô¨Ånite-time
analysis,‚Äù in Advances in Neural Information Processing Systems ,
2020, pp. 13 762‚Äì13 772.
[30] J. Sun, G. Wang, G. B. Giannakis, Q. Yang, and Z. Yang, ‚ÄúFinite-
time analysis of decentralized temporal-difference learning with linear
function approximation,‚Äù in Proc. International Conference on ArtiÔ¨Å-
cial Intelligence and Statistics , 2020, pp. 4485‚Äì4495.
[31] Q. Lin and Q. Ling, ‚ÄúDecentralized TD(0) with gradient tracking,‚Äù
IEEE Signal Processing Letters , vol. 28, pp. 723‚Äì727, 2021.
[32] A. Mahajan and M. Mannan, ‚ÄúDecentralized stochastic control,‚Äù
Annals of Operations Research , vol. 241, no. 1-2, pp. 109‚Äì126, 2016.
[33] A. A. Malikopoulos, ‚ÄúOn team decision problems with nonclassical
information structures,‚Äù IEEE Transactions on Automatic Control ,
2022.
[34] S. Yuksel, ‚ÄúStochastic nestedness and the belief sharing information
pattern,‚Äù IEEE Transactions on Automatic Control , vol. 54, no. 12, pp.
2773‚Äì2786, 2009.
[35] A. Nayyar, A. Mahajan, and D. Teneketzis, ‚ÄúDecentralized stochastic
control with partial history sharing: A common information approach,‚Äù
IEEE Transactions on Automatic Control , vol. 58, no. 7, pp. 1644‚Äì
1658, 2013.
[36] R. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduc-
tion. MIT Press, 2018.
[37] J. Tsitsiklis and B. Van Roy, ‚ÄúAnalysis of temporal-difference learning
with function approximation,‚Äù Advances in Neural Information Pro-
cessing Systems , vol. 9, 1996.
[38] S. P. Singh, T. Jaakkola, and M. I. Jordan, ‚ÄúLearning without state-
estimation in partially observable Markovian decision processes,‚Äù in
Proc. Machine Learning , 1994, pp. 284‚Äì292.
[39] A. Rodriguez, R. Parr, and D. Koller, ‚ÄúReinforcement learning using
approximate belief states,‚Äù in Advances in Neural Information Pro-
cessing Systems , vol. 12. MIT Press, 1999.
[40] H. Kimura, K. Miyazaki, and S. Kobayashi, ‚ÄúReinforcement learning
in POMDPs with function approximation,‚Äù in Proc. ICML , vol. 97,
1997, pp. 152‚Äì160.
[41] Q. Cai, Z. Yang, and Z. Wang, ‚ÄúReinforcement learning from partial
observation: Linear function approximation with provable sample
efÔ¨Åciency,‚Äù in Proc. International Conference on Machine Learning ,
2022, pp. 2485‚Äì2522.
[42] Y . Li, Y . Tang, R. Zhang, and N. Li, ‚ÄúDistributed reinforcement learn-
ing for decentralized linear quadratic control: A derivative-free policy
optimization approach,‚Äù IEEE Transactions on Automatic Control , pp.
6429‚Äì6444, 2021.
[43] H. Wang, S. Lin, H. Jafarkhani, and J. Zhang, ‚ÄúDistributed Q-
learning with state tracking for multi-agent networked control,‚Äù in
Proc. International Conference on Autonomous Agents and Multi-
Agent Systems , 2021, pp. 1692‚Äì1694.
[44] A. Mahajan, N. C. Martins, M. C. Rotkowitz, and S. Y ¬®uksel, ‚ÄúIn-
formation structures in optimal decentralized control,‚Äù in Proc. IEEE
CDC . IEEE, 2012, pp. 1291‚Äì1306.
[45] N. Saldi and S. Y ¬®uksel, ‚ÄúGeometry of information structures, strategic
measures and associated stochastic control topologies,‚Äù Probability
Surveys , vol. 19, pp. 450‚Äì532, 2022.
[46] J. Arabneydi and A. Mahajan, ‚ÄúReinforcement learning in decentral-
ized stochastic control systems with partial history sharing,‚Äù in Proc.
American Control Conference (ACC) , 2015, pp. 5449‚Äì5456.[47] A. Nayyar and D. Teneketzis, ‚ÄúCommon knowledge and sequential
team problems,‚Äù IEEE Transactions on Automatic Control , vol. 64,
no. 12, pp. 5108‚Äì5115, 2019.
[48] M. Kayaalp, V . Bordignon, S. Vlaski, and A. H. Sayed, ‚ÄúHidden
Markov modeling over graphs,‚Äù in Proc. IEEE Data Science and
Learning Workshop (DSLW) , 2022, pp. 1‚Äì6.
[49] M. Kayaalp, V . Bordignon, S. Vlaski, V . Matta, and A. H. Sayed,
‚ÄúDistributed Bayesian learning of dynamic states,‚Äù arXiv:2212.02565 ,
2022.
[50] O. Hlinka, O. Slu Àáciak, F. Hlawatsch, P. M. Djuri ¬¥c, and M. Rupp,
‚ÄúLikelihood consensus and its application to distributed particle Ô¨Ål-
tering,‚Äù IEEE Transactions on Signal Processing , vol. 60, no. 8, pp.
4334‚Äì4349, 2012.
[51] G. Battistelli and L. Chisci, ‚ÄúKullback‚ÄìLeibler average, consensus on
probability densities, and distributed state estimation with guaranteed
stability,‚Äù Automatica , vol. 50, no. 3, pp. 707‚Äì718, 2014.
[52] S. Bandyopadhyay and S. Chung, ‚ÄúDistributed Bayesian Ô¨Åltering using
logarithmic opinion pool for dynamic sensor networks,‚Äù Automatica ,
vol. 97, pp. 7‚Äì17, 2018.
[53] A. H. Sayed, ‚ÄúAdaptation, learning, and optimization over networks,‚Äù
Foundations and Trends in Machine Learning , vol. 7, no. 4-5, pp.
311‚Äì801, July 2014.
[54] A. Nedic and A. Ozdaglar, ‚ÄúDistributed subgradient methods for multi-
agent optimization,‚Äù IEEE Transactions on Automatic Control , vol. 54,
no. 1, pp. 48‚Äì61, 2009.
[55] A. G. Dimakis, S. Kar, J. M. Moura, M. G. Rabbat, and A. Scaglione,
‚ÄúGossip algorithms for distributed signal processing,‚Äù Proc. IEEE ,
vol. 98, no. 11, pp. 1847‚Äì1864, 2010.
[56] P. Di Lorenzo and G. Scutari, ‚ÄúNext: In-network nonconvex optimiza-
tion,‚Äù IEEE Transactions on Signal and Information Processing over
Networks , vol. 2, no. 2, pp. 120‚Äì136, 2016.
[57] J. Chen and A. H. Sayed, ‚ÄúOn the learning behavior of adaptive net-
works‚Äîpart I: Transient analysis,‚Äù IEEE Transactions on Information
Theory , vol. 61, no. 6, pp. 3487‚Äì3517, 2015.
[58] ‚Äî‚Äî, ‚ÄúOn the learning behavior of adaptive networks‚Äîpart II: Perfor-
mance analysis,‚Äù IEEE Transactions on Information Theory , vol. 61,
no. 6, pp. 3518‚Äì3548, 2015.
[59] M. Kayaalp, S. Vlaski, and A. H. Sayed, ‚ÄúDif-MAML: Decentralized
multi-agent meta-learning,‚Äù IEEE Open Journal of Signal Processing ,
vol. 3, pp. 71‚Äì93, 2022.
[60] I. Csisz ¬¥ar and J. K ¬®orner, Information Theory: Coding Theorems for
Discrete Memoryless Systems , 2nd ed. Cambridge University Press,
2011.
[61] R. S. Sutton, ‚ÄúLearning to predict by the methods of temporal
differences,‚Äù Machine Learning , vol. 3, no. 1, pp. 9‚Äì44, 1988.
[62] J. Z. Kolter and A. Y . Ng, ‚ÄúRegularization and feature selection
in least-squares temporal difference learning,‚Äù in Proc. International
Conference on Machine Learning , 2009, pp. 521‚Äì528.
[63] M. W. Hoffman, A. Lazaric, M. Ghavamzadeh, and R. Munos,
‚ÄúRegularized least squares temporal difference learning with nested `2
and `1penalization,‚Äù in Proc. European Workshop on Reinforcement
Learning . Springer, 2011, pp. 102‚Äì114.
[64] J. Farebrother, M. C. Machado, and M. Bowling, ‚ÄúGeneralization and
regularization in DQN,‚Äù arXiv:1810.00123 , 2018.
[65] K. Cobbe, O. Klimov, C. Hesse, T. Kim, and J. Schulman, ‚ÄúQuanti-
fying generalization in reinforcement learning,‚Äù in Proc. International
Conference on Machine Learning , vol. 97, 09‚Äì15 Jun 2019, pp. 1282‚Äì
1289.
[66] B. R. Kiran, I. Sobh, V . Talpaert, P. Mannion, A. A. A. Sallab, S. Yo-
gamani, and P. P ¬¥erez, ‚ÄúDeep reinforcement learning for autonomous
driving: A survey,‚Äù IEEE Transactions on Intelligent Transportation
Systems , vol. 23, no. 6, pp. 4909‚Äì4926, 2022.
[67] E. Bƒ±yƒ±k, D. P. Losey, M. Palan, N. C. LandolÔ¨Å, G. Shevchuk, and
D. Sadigh, ‚ÄúLearning reward functions from diverse sources of human
feedback: Optimally integrating demonstrations and preferences,‚Äù The
International Journal of Robotics Research , vol. 41, no. 1, pp. 45‚Äì67,
2022.
[68] T. Li, H. Fan, J. Garc ¬¥ƒ±a, and J. M. Corchado, ‚ÄúSecond-order statistics
analysis and comparison between arithmetic and geometric average fu-
sion: Application to multi-sensor target tracking,‚Äù Information Fusion ,
vol. 51, pp. 233‚Äì243, 2019.
[69] M. Kayaalp, Y . Inan, E. Telatar, and A. H. Sayed, ‚ÄúOn the arith-
metic and geometric fusion of beliefs for distributed inference,‚Äù
arXiv:2204.13741 , April 2022.
20[70] D. Bertsekas and J. Tsitsiklis, Parallel and Distributed Computation:
Numerical Methods . Athena ScientiÔ¨Åc, 2015.
[71] D. Acemoglu, M. A. Dahleh, I. Lobel, and A. Ozdaglar, ‚ÄúBayesian
learning in social networks,‚Äù The Review of Economic Studies , vol. 78,
no. 4, pp. 1201‚Äì1236, 2011.
[72] M. R. Garey and D. S. Johnson, Computers and Intractability . San
Francisco: Freeman, 1979, vol. 174.
[73] S. I. Resnick, Adventures in Stochastic Processes . Birkh ¬®auser, 2002.
[74] E. Jorge, M. K Àöageb ¬®ack, F. D. Johansson, and E. Gustavsson, ‚ÄúLearning
to play guess who? and inventing a grounded language as a conse-
quence,‚Äù arXiv:1611.03218 , 2016.
[75] Y . Polyanskiy and Y . Wu, ‚ÄúStrong data-processing inequalities for
channels and Bayesian networks,‚Äù in Convexity and Concentration .
New York, NY: Springer New York, 2017, pp. 211‚Äì249.
[76] J. Bretagnolle and C. Huber, ‚ÄúEstimation des densit ¬¥es : Risque mini-
max,‚Äù in S¬¥eminaire de Probabilit ¬¥es XII , C. Dellacherie, P. A. Meyer,
and M. Weil, Eds. Berlin, Heidelberg: Springer Berlin Heidelberg,
1978, pp. 342‚Äì363.
[77] S. Pillai, T. Suel, and S. Cha, ‚ÄúThe Perron-Frobenius theorem: some
of its applications,‚Äù IEEE Signal Processing Magazine , vol. 22, no. 2,
pp. 62‚Äì75, 2005.
21