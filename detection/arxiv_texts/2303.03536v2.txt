Certifying the absence of spurious local minima at
infinity
CÃ©dric Joszâˆ—Xiaopeng Liâ€ 
Abstract
When searching for global optima of nonconvex unconstrained optimization
problems, it is desirable that every local minimum be a global minimum. This
property of having no spurious local minima is true in various problems of
interest nowadays, including principal component analysis, matrix sensing, and
linear neural networks. However, since these problems are non-coercive, they
may yet have spurious local minima at infinity. The classical tools used to
analyze the optimization landscape, namely the gradient and the Hessian, are
incapable of detecting spurious local minima at infinity. In this paper, we
identify conditions that certify the absence of spurious local minima at infinity,
one of which is having bounded subgradient trajectories. We check that they
hold in several applications of interest.
Keywords: global optimization, Morse-Sard theorem, subgradient trajectories.
1 Introduction
The idea that the absence of spurious local minima alone does not guarantee the
success of first-order methods was first expressed in the context of binary classification
in the mid-nineties. It was shown that gradient trajectories are bounded if the
objective function satisfies several technical conditions tailored to the problem at
hand [4, Theorems 3.6-3.8]. This property was referred to as having no attractors
at infinity. More recently, it was proved that adding an exponential neuron to
a wide class of neural networks eliminates all spurious local minima [ 33], but it
was soon realized that this procedure simply sends them to infinity [ 42]. These
results suggest that besides spurious local minima, a certain notion of spurious
âˆ—cj2638@columbia.edu , IEOR, Columbia University, New York. Research supported by NSF
EPCN grant 2023032 and ONR grant N00014-21-1-2282.
â€ xl3040@columbia.edu , IEOR, Columbia University, New York.
1arXiv:2303.03536v2  [math.OC]  13 Jul 2023local minima at infinity also affects the convergence of first-order methods to global
optima. However, the current optimization literature lacks a precise definition of
local minima at infinity, and, accordingly, there is little theoretical understanding of
them. Worse still, classical tools for landscape analysis, such as the gradient and
the Hessian, cannot detect spurious local minima at infinity even in simple scenarios
(see Example 1), let alone handle nonsmooth functions without a gradient.
Example 1. Consider an instance of matrix completion problem, i.e., minimize
f(x1, x2, y1, y2) := ( x1y1âˆ’1)2+ (x2y1âˆ’1)2+ (x2y2âˆ’1)2.
By solving âˆ‡f(x1, x2, y1, y2) = 0, the set of critical points of fcan be decomposed
into four connected components:
C1={(x1, x2, y1, y2) = (t, t,1/t,1/t)|tâˆˆR\ {0}},
C2={(x1, x2, y1, y2) = (t,0,1/t,âˆ’1/t)|tâˆˆR\ {0}},
C3={(x1, x2, y1, y2) = (t,âˆ’t,0,âˆ’1/t)|tâˆˆR\ {0}},
C4={(x1, x2, y1, y2) = (0 ,0,0,0)}.
The critical values are f(C1) ={0},f(C2) = f(C3) ={2}, and f(C4) ={3}.
Furthermore, C1is the set of global minima, and by computing the Hessian âˆ‡2f,
we find that it has positive and negative eigenvalues at all points in C2,C3and
C4. Therefore, fhas no spurious local minima and all saddle points are strict [ 30,
Definition 2]. One would expect first-order methods like gradient descent to converge
to a global minimum for almost all initial points [ 30, Theorem 11]. However, the
numerical experiments in Figure 1 show otherwise. This is because the function is
not coercive.
Two newly proposed concepts related to spurious local minima at infinity are
setwise local minima [ 27] and spurious valleys [ 44]. Setwise local minima [ 27, Defini-
tion 2.5] generalize the notion of local minima from points to compact sets. The first
author and co-authors recently established that the uniform limit (on all compact
subsets) of a sequence of continuous functions which are devoid of spurious setwise
local minima is itself devoid of spurious strict setwise local minima [ 27, Proposition
2.7]. However, due to the boundedness assumption, setwise local minima cannot
be directly used to study spurious local minima at infinity. Spurious valleys [ 44,
Definition 1] do have the potential to handle spurious local minima at infinity but
they fail to detect them when there are flat regions, such as in the ReLU network
with one-hidden layer (x1, x2)7â†’(x2max{x1,0}âˆ’1)2(see Figure 2). Spurious valleys
also rely on the notion of path-connectedness, which is actually not necessary for
defining spurious local minimum at infinity. In this paper, we extend the concept
of setwise local minima by relaxing the boundedness assumption. This enables us
to define spurious local minima at infinity as unbounded setwise local minima over
2Figure 1: Gradient method initialized uniformly at random in [âˆ’1,1]4with constant
step size 0.01sometimes gets stuck at a spurious local minimum at infinity (3 among
10 trails in the experiment).
which the infimum of the objective function is greater than the global infimum. It
also allows us to handle classical spurious local minima and flat regions in a unified
way.
An existing strategy to analyze the landscape of non-coercive functions is to
construct a strictly decreasing path to a global minimum from any initial point. Such
a path was shown to exist in half-rectified neural network [ 22]. This strategy is used
to prove the existence of spurious local minima in neural networks with almost all
nonlinear activations [ 15]. It also explains the phase transition from the existence
of sub-optimal basins in narrow networks to their disappearance in wide networks
[31]. Finally, it is used to prove the absence of spurious valleys for over-parametrized
one-hidden layer neural network [ 44]. However, such a strategy needs to be tailored
to each application since one needs to select a particular path for each specific loss
function. In this paper, we instead develop a theory allowing one to use a common
decreasing path - subgradient trajectory - to analyze the landscape in various different
contexts. We can then rule out spurious setwise local minima (and thus those at
infinity) for a general class of functions. Our main result is as follows.
Theorem 1. Suppose a locally Lipschitz function is bounded below, admits a chain
rule, has finitely many critical values, and has bounded subgradient trajectories. Then
it has no spurious local minima if and only if it has no spurious setwise local minima.
The above statement is meant to help readers get a first taste of our main result
in this paper. Precise definitions and a detailed mathematical background will be
3Figure 2: Function devoid of spurious valleys containing a spurious local minimum
at infinity.
given in the main body1, along with a discussion on the role of its assumptions. Let
us mention already that two of its assumptions, namely those regarding the chain
rule and critical values, automatically hold for functions definable in an o-minimal
expansion of the real field [ 43] (by [6, Proposition 2 (iv)] and the definable Morse-Sard
theorem [ 5, Corollary 9 (ii)]). This includes semi-algebraic, globally subanalytic,
and log-exp functions, and importantly, many applications of interest nowadays [ 6,
Section 4.1]. The locally Lipschitz and lower bounded assumptions usually come
for free in applications, so that in practice the sole assumption that one needs to
check for is that subgradient trajectories are bounded. Theorem 1 thus serves as
a handy device to conclude that there are no spurious setwise local minima for a
family of functions that are widely used in machine learning, especially in deep
neural networks and matrix sensing. We summarize the problems that we are going
to consider in the following corollary.
Corollary 1. The following problems have no spurious local minima at infinity:
1. deep linear neural network
inf
W1,...,W Lâˆ¥WLÂ·Â·Â·W1Xâˆ’Yâˆ¥2
F;
2. one dimensional deep neural network with sigmoid activation function Ïƒ
inf
w1,...,w L(wLÏƒ(wLâˆ’1Â·Â·Â·Ïƒ(w1x))âˆ’y)2;
1The terminology in the theorem will be given in Definition 4 (locally Lipschitz), Definition 8
(chain rule), Definition 7 (bounded subgradient trajectories), and Definition 2 (setwise local
minimum).
43. matrix recovery with restricted isometry property (RIP)
inf
X,Y1
2mmX
i=1(âŸ¨Ai, XYTâŸ©Fâˆ’bi)2;
4. nonsmooth matrix factorization where rank( M) = 1andMijÌ¸= 0
inf
x,ymX
i=1nX
j=1|xiyjâˆ’Mij|.
Again, the statement above aims at giving readers some feeling on what type of
functions we are considering. More rigorous descriptions of the applications will be
given in the main body.
The paper is organized as follows. Section 2 contains background material
on setwise local minima, the Clarke subdifferential, and subgradient trajectories.
Section 3 contains the proof of our main result, namely Theorem 1. Finally, Section 4
contains applications of our main result as delineated in Corollary 1.
2 Background
This section contains prerequisites for the proof of Theorem 1 in the next section.
Throughout this paper, unless otherwise specified, we always equip the Euclidean
spaceRnwith an inner product âŸ¨Â·,Â·âŸ©and its induced norm âˆ¥ Â· âˆ¥:=p
âŸ¨Â·,Â·âŸ©.
2.1 Setwise local minimum
In this subsection, we present the formal definitions and some useful properties of
setwise local minimum and local minimum at infinity mentioned in Section 1. We
first review the classical definition of local and global minima. Throughout this
paper, B(x, Ïµ)denotes the open ball centered at xâˆˆRnwith radius Ïµ >0.
Definition 1. A point xâˆˆRnis alocal minimum (respectively, global minimum ) of
a function f:Rnâ†’Riff(x)â©½f(y)for all yâˆˆB(x, Ïµ)for some Ïµ >0(respectively,
yâˆˆRn). A local minimum is spurious if it is not a global minimum.
From Definition 1, one can see the definition of a local minimum only considers
the landscape of a function at any finite point. To discuss the function landscape
at infinity, we generalize the notion of setwise local minimum first proposed in [ 27,
Definition 2.5].
Definition 2 (Setwise local minimum) .A nonempty closed subset SâŠ‚Rnis a
setwise local minimum of a continuous function f:Rnâ†’Rif there exists an open
setUâŠ‚Rnsuch that SâŠ‚Uandf(x)â©½f(y)for all xâˆˆS,yâˆˆU\S.
5It is easy to see that a local minimum is a setwise local minimum by taking
Sto be a singleton. We also define a strict setwise local minimum by replacing
f(x)â©½f(y)with f(x)< f(y)in Definition 2.
Definition 3 (Setwise global minimum) .A subset SofRnis asetwise global
minimum of a function f:Rnâ†’RifSis a setwise local minimum of fand
infSf= inf Rnf.
Note that infSfis a shorthand for infxâˆˆSf(x), and similar for supSfandmax Sf.
Setwise local minima include setwise global minima as a special case, and we say a
setwise local minimum is spurious if it is not a setwise global minimum. Note that
Definition 2 is not exactly the same as [ 27, Definition 2.5] because we do not require a
setwise local minimum to be a compact set. In other words, a setwise local minimum
can be either bounded or unbounded, and we say a (spurious) setwise local minimum
is a (spurious) local minimum at infinity if it is unbounded. For example, consider
the loss function of a one-hidden layer neural network with sigmoid activation Ïƒand
two data points (1,1)and(âˆ’1,âˆ’3)in Figure 3. One can see that Sis a setwise local
minimum (in particular, a local minimum at infinity) and Uis the corresponding
open set in Definition 2. Finally, observe that Rnis a strict setwise local minimum
at infinity of any function.
Figure 3: Local minimum at infinity of f(w1, w2) =1
2[(w2Ïƒ(w1)âˆ’1)2+ (w2Ïƒ(âˆ’w1) +
3)2].
Now we introduce one of the most useful properties of setwise local minima in
Lemma 1. This property is intuitive and will be used in different scenarios throughout
this paper. Let Â¯S,Sâ—¦, and âˆ‚S:=Â¯S\Sâ—¦respectively denote the closure, interior, and
boundary of a subset SofRn.
Lemma 1. IfSis a setwise local minimum of a continuous function f:Rnâ†’R,
then f(x) = supSffor all xâˆˆâˆ‚S.
6Proof.See Appendix A.
It is worth relating our notion of setwise local minimum to the concept of valley
proposed in [ 44, Definition 1]. A valleyof a function f:Rnâ†’Ris defined as a
path-connected component2of a sublevel set of f. These two definitions are distinct
in general. The interval [âˆ’1,1]is a setwise local minimum of f:Râ†’Rdefined
byf(x) := 0for all xâˆˆRyet it is not a valley. Conversely, X:={(x1, x2)âˆˆ
(R\ {0})Ã—R|x2=sin(1/x1)}is a valley of f:R2â†’Rwhere fis defined as
the distance between xandX, yet it is not a setwise local minimum since it is not
closed. (The sublevel set of fcorresponding to the value zero is composed of two
path-connected components, namely Xand{0} Ã—[âˆ’1,1], whose union is X.) Under
some mild conditions, their relation can be summarized in proposition 1.
Proposition 1. For a continuous function from RntoR,
(a)a path-connected component of a strict setwise local minimum (respectively,
setwise local minimum) is a valley (respectively, subset of a valley);
(b)a connected component3of a sublevel set which has finitely many connected
components is a strict setwise local minimum.
Proof.See Appendix B.
Remark 1. The assumption on finiteness of connected components is necessary, or
else counterexample may occur when the function is oscillatory. For example,
f(x) :=(
0 if xâ©½0,
x2sin1
xifx >0.
The function fis continuous on R, but the sublevel set {xâˆˆR|f(x)â©½0}has
infinitely many connected components. Take a connected component C1= (âˆ’âˆ,0]
(also path-connected, thus a valley), and it is not a setwise local minimum because
for any open set Ucontaining C1, there exists some x0âˆˆNsuch that f(x0)<0.
Finally, we discuss the case of coercive functions. Recall that a function f:Rnâ†’
Ris coercive if f(x)â†’ âˆasâˆ¥xâˆ¥ â†’ âˆ.
Proposition 2. If a continuous function from RntoRis coercive, then it has no
spurious local minima at infinity.
2A subset SofRnis path-connected if for all x, yâˆˆS, there exists a continuous function
Î³: [0,1]â†’Ssuch that Î³(0) = xandÎ³(1) = y. A maximal path-connected connected set is called
a path-connected component. Path-connected components can be viewed as equivalence classes
over a set.
3A subset SofRnis disconnected if there exist nonempty disjoint open (in S) sets AandB
such that S=AâˆªB. It is connected if it is not disconnected. A maximal connected set is called a
connected component.
7Proof.See Appendix C.
In many statistical learning problems, the loss functions without regularizer are
usually not coercive, so spurious local minima at infinity may exist. Therefore, it
is important to develop some device to check whether spurious local minima exist
or not so that optimization algorithms can be designed to avoid getting trapped in
them.
2.2 Clarke subdifferential
In this subsection, we will review some concepts and results on generalized derivative
in the sense of Clarke [ 11, p. 27], since Theorem 1 also considers nonsmooth functions.
Definition 4. A function f:Rnâ†’Rmislocally Lipschitz if for all aâˆˆRn, there
exist positive constants randLsuch that
âˆ€x, yâˆˆB(a, r),âˆ¥f(x)âˆ’f(y)âˆ¥â©½Lâˆ¥xâˆ’yâˆ¥.
Notice that for a locally Lipschitz function, by [ 20, Theorem 3.2], the derivative
exists almost everywhere. Without any assumption on convexity, in order to ensure
the existence of a subdifferential, we adopt the notion of Clarke subdifferential.
Definition 5. [11, p. 27] Let f:Rnâ†’Rbe a locally Lipschitz function. The
Clarke subdifferential is the set-valued mapping âˆ‚ffromRnto the subsets of Rn
defined for all xâˆˆRnby
âˆ‚f(x) :={sâˆˆRn|fâ—¦(x, d)â©¾âŸ¨s, dâŸ©,âˆ€dâˆˆRn}
where
fâ—¦(x, d) := lim sup
yâ†’x
tâ†˜0f(y+td)âˆ’f(y)
t.
It is well known that for any locally Lipschitz function fand any xâˆˆRn, the
Clarke subdifferential âˆ‚f(x)is a nonempty, convex, and compact set [ 11, Proposition
2.1.2(a)]. Similar to differentiable functions, a point xâˆˆRnis called (Clarke) critical
pointif0âˆˆâˆ‚f(x). A real number yis called a (Clarke) critical value offif there
exists a (Clarke) critical point xâˆˆRnoffsuch that f(x) =y.
2.3 Subgradient trajectory
In this subsection, we will introduce some basic concepts and fundamental properties
related to subgradient trajectories.
8Definition 6. [2, Definition 1 p. 12] Given two real numbers a < b, a function
x: [a, b]â†’Rnisabsolutely continuous if for all Ïµ >0, there exists Î´ >0such that,
for any finite collection of disjoint subintervals [a1, b1], . . . , [am, bm]of[a, b]such thatPm
i=1biâˆ’aiâ©½Î´, we havePm
i=1âˆ¥x(bi)âˆ’x(ai)âˆ¥â©½Ïµ.
By virtue of [ 36, Theorem 20.8], x: [a, b]â†’Rnis absolutely continuous if and
only if it is differentiable almost everywhere on (a, b), its derivative xâ€²is Lebesgue
integrable, and x(t)âˆ’x(a) =Rt
axâ€²(Ï„)dÏ„for all tâˆˆ[a, b]. Given a non-compact
interval IofR,x:Iâ†’Rnis absolutely continuous if it is absolutely continuous on
all compact subintervals of I.
An absolutely continuous function x: [0,âˆ)â†’Rnis called a subgradient
trajectory off:Rnâ†’Rstarting at x0âˆˆRnif it satisfies the following differential
inclusion with initial condition:
xâ€²(t)âˆˆ âˆ’âˆ‚f(x(t)),for almost every tâ©¾0, x(0) = x0, (1)
where â€œalmost everyâ€ means all elements except for those in a set of zero measure.
However, a subgradient trajectory may not always exist for arbitrary f, even if f
is a smooth function. Let f(x) =âˆ’1
3x3andx0= 1, then it is easy to see x(t) =1
1âˆ’t
is the unique solution for tâˆˆ[0,1)and it cannot be extended to an absolutely
continuous function on [0,âˆ)due to the singularity at t= 1. In this case, one would
seek a family of functions including many loss functions arising in applications that
guarantee the existence of a subgradient trajectory. We say a function f:Rnâ†’R
isbounded below ifinfRnf=c >âˆ’âˆ. It was shown in [ 34, Theorem 3.2] that a
primal lower nice function bounded below by a linear function suffices. However,
in general it is not easy to check whether those nonconvex functions in statistical
learning problems are primal lower nice. For easily checkable conditions, the following
result generalized from [ 41, Proposition 2.3] for differentiable functions tells us that
a locally Lipschitz function bounded below also suffices.
Proposition 3. Iff:Rnâ†’Ris locally Lipschitz and bounded below, then there
exists a subgradient trajectory of fstarting at arbitrary x0âˆˆRn.
Proof.See Appendix D.
We remark here that with Proposition 3, one can recover Ekelandâ€™s variational
principle [ 19, Corollary 2.3] [ 25, Corollary] for locally Lipschitz lower bounded
functions with a chain rule (see [ 24, Theorem 3.1] for an extension to lower semi-
continuous lower bounded functions). Indeed, Proposition 3 implies that for all Ïµ >0,
there exists (x, s)âˆˆgraph âˆ‚fsuch that f(x)â©½inff+Ïµandâˆ¥sâˆ¥â©½Ïµ.4Note that
Proposition 3 only guarantees the existence of a solution to (1) for all tâ©¾0, but the
solution x(t)could go to infinity as tâ†’ âˆ. This motivates the following definition.
4This follows from the formula f(x(t))âˆ’inffâ©¾Râˆ
td(0, âˆ‚f(x(Ï„)))2dÏ„where d(x, X) :=
infyâˆˆXâˆ¥xâˆ’yâˆ¥(see [14, Lemma 5.2] and [16, Proposition 4.10]).
9Definition 7. A locally Lipschitz lower bounded function f:Rnâ†’Rhasbounded
subgradient trajectories if for any x0âˆˆRn, there exists a subgradient trajectory xof
fstarting at x0and a constant r >0, such that âˆ¥x(t)âˆ¥â©½rfor all tâ©¾0.
Finally, notice that when fis continuously differentiable, by [ 11, Proposition
2.2.4], (1) reduces to the classical Cauchy problem of differential equation
xâ€²(t) =âˆ’âˆ‡f(x(t)),for all tâ©¾0,x(0) = x0.
and subgradient trajectory reduces to gradient trajectory by imposing xto be
continuously differentiable. Recall the descent property of gradient trajectories [ 1,
Proposition 17.1.1], i.e., fâ—¦xis a decreasing function for any gradient trajectory x
off. We want this nice property to hold even in the general case when fis only
locally Lipschitz. We adopt the notion of chain rule in [ 14, Definition 5.1]. Note
that functions admitting a chain rule are also referred to as path differentiable [ 6,
Definition 3].
Definition 8. Letf:Rnâ†’Rbe locally Lipschitz. We say fadmits a chain rule if
for any absolutely continuous function x: [0,âˆ)â†’Rn, we have
(fâ—¦x)â€²(t) =âŸ¨v, xâ€²(t)âŸ©,âˆ€vâˆˆâˆ‚f(x(t)),
for almost every tâˆˆ[0,âˆ).
Thus, for any locally Lipschitz function that admits a chain rule, by [ 14, Lemma
5.2], the function value is always decreasing in time along the subgradient trajectory.
A detailed discussion on what class of functions admits a chain rule can be found in
[6]. Note that general Lipschitz functions are far from admitting a chain rule since
they generically have a maximal Clarke subdifferential [12, 7, 13].
3 Proof of Theorem 1
This section contains the proof of the main result, i.e., Theorem 1. After the proof,
we will explain the necessity of the assumptions in Theorem 1 by raising some
counterexamples. For emphasis, we summarize all assumptions in Theorem 1 below.
Assumption 1. Letf:Rnâ†’Rbe a function such that it
(a) is bounded below, namely, infRnf >âˆ’âˆ;
(b) is locally Lipschitz continuous on Rn; see Definition 4;
(c) admits a chain rule; see Definition 8;
(d) has finitely many critical values; see Section 2.2;
10(e) has bounded subgradient trajectories; see Definition 7.
Proof of Theorem 1. Letf:Rnâ†’Rbe a function satisfying assumption 1 If fhas
no spurious setwise local minima, then fhas no spurious local minima. We next
prove the converse. Let SâŠ‚Rnbe a setwise local minimum of f. We seek to show
thatSis a setwise global minimum of f. IfSâ—¦=âˆ…, then by Lemma 1 f(x) =supSf
for all xâˆˆâˆ‚S=Â¯S\Sâ—¦=Ssince Sis closed by Definition 2. Thus fis constant on
S. By definition of setwise local minima (Definition 2), there exists an open subset
UofRncontaining Ssuch that the constant value of fonSis less than or equal to
f(y)for all yâˆˆU\S. Thus every point in Sis a local minimum of f. Since every
local minimum of fis a global minimum, infSf=infRnfandSis a setwise global
minimum according to Definition 3. The rest of the proof deals with the case when
Sâ—¦Ì¸=âˆ…. Let Cbe the set of all critical points of finSand consider the following
optimization problem:
inf
xâˆˆCf(x). (2)
We claim that the set of (global) solutions to (2) is nonempty, and that any solution
is a local minimum of fbelonging to the setwise local minimum S. We first show
that the feasible set of (2) is nonempty.
Since Sâ—¦Ì¸=âˆ…, let x0âˆˆSâ—¦. Ifx0âˆˆC, then the feasible set Cis nonempty.
We thus assume that x0/âˆˆC. Since fis locally Lipschitz and bounded below, by
Proposition 3 there exists a subgradient trajectory x: [0,âˆ)â†’Rnstarting at x0.
We next show that x([0,âˆ))âŠ‚S. We reason by contradiction and assume that
Scâˆ©x([0,âˆ))Ì¸=âˆ…, where Scis the complement of SinRn. Then Sâ—¦andScare
disjoint open subsets of Rnsuch that Sâ—¦âˆ©x([0,âˆ))Ì¸=âˆ…(the intersection contains
x0),Scâˆ©x([0,âˆ))Ì¸=âˆ…, and x([0,âˆ)) = ( x([0,âˆ))âˆ©Sâ—¦)âˆª(x([0,âˆ))âˆ©Sc)âŠ‚Rn\âˆ‚S
(since5f(x(t))< f(x(0)) = f(x0)â©½f(x)for all t >0andxâˆˆâˆ‚S, where the last
inequality follows from Lemma 1). Thus the connected set x([0,âˆ))is the union of
two relatively open disjoint nonempty sets, which is a contradiction.
Since fhas bounded subgradient trajectories and x(Â·)is an arbitrary subgradient
trajectory starting at x0, by Definition 7 and without loss of generality there exists
r >0such that âˆ¥x(t)âˆ¥â©½rfor all tâ©¾0. We next show that there exists a critical
point of finB(0, r)âˆ©S. Suppose that there exist two constants T, Ïµ > 0for which
âˆ¥xâ€²(t)âˆ¥â©¾Ïµfor all tâ©¾Tsuch that xâ€²(t)âˆˆ âˆ’âˆ‚f(x(t)). By [14, Lemma 5.2], we
have (fâ—¦x)â€²(t) =âˆ’âˆ¥xâ€²(t)âˆ¥2â©½âˆ’Ïµ2for almost every tâ©¾T. By integrating, we
getf(x(t))âˆ’f(x(T))â©½âˆ’Ïµ2tand thus f(x(t))converges to âˆ’âˆastâ†’ âˆ. This
is impossible since x(t)âˆˆB(0, r)andfis continuous. Hence there exists a time
sequence tkâ†’ âˆsuch that âˆ¥xâ€²(tk)âˆ¥ â†’ 0askâ†’ âˆandxâ€²(tk)âˆˆ âˆ’âˆ‚f(x(tk))for
allkâˆˆN:={0,1,2, . . .}. By the Bolzanoâ€“Weierstrass theorem, there exists a
5If there exists t >0such that f(x(t)) =f(x(0)), then f(x(t))âˆ’f(x(0)) =Rt
0âˆ¥xâ€²(s)âˆ¥2ds= 0
andxâ€²(s) = 0for almost every sâˆˆ(0, t). Since xâ€²(s)âˆˆ âˆ’âˆ‚f(x(s))for almost every s >0, by [11,
2.1.5 Proposition (b) p. 29] we have 0âˆˆâˆ‚f(x(0)).
11subsequence x(tkj)ofx(tk)such that x(tkj)â†’ËœxâˆˆRnasjâ†’ âˆ. Since xâ€²(tkj)âˆˆ
âˆ’âˆ‚f(x(tkj)), by [11, 2.1.5 Proposition (b) p. 29] we have 0âˆˆ âˆ’âˆ‚f(Ëœx). Finally, since
x([0,âˆ))âŠ‚SandSis closed, we have ËœxâˆˆC. We obtain that CÌ¸=âˆ…as desired.
Since fhas finitely many critical values and CÌ¸=âˆ…, the set of solutions to (2)
is nonempty. Let xâˆ—âˆˆCbe a solution, that is to say f(xâˆ—) =min Cf. Recall
that Cis a subset of the setwise local minimum S. Ifxâˆ—is a local minimum of f,
then it is a global minimum of finSsince every local minimum of fis a global
minimum. Thus infSf=infRnfandSis a setwise global minimum. For the
remainder of the proof, we consider the case where xâˆ—is not a local minimum and
show that this leads to a contradiction. We first show that there exists s0âˆˆSâ—¦such
that f(s0)< f(xâˆ—). This is clearly true if xâˆ—âˆˆSâ—¦since one can then find a ball
centered at xâˆ—inside Sâ—¦. Ifxâˆ—âˆˆS\Sâ—¦=âˆ‚S, then we reason by contradiction and
assume that f(x)â©¾f(xâˆ—)for all xâˆˆSâ—¦. By Lemma 1, we have f(x) =f(xâˆ—) =
supSfâ©¾f(y)â©¾f(xâˆ—)for all (x, y)âˆˆ(S\Sâ—¦)Ã—Sâ—¦. Hence f(xâˆ—) =f(x)for all
xâˆˆS. Since Sis a setwise local minimum, there exists an open set Usuch that
f(x)â©¾f(xâˆ—)holds for all xâˆˆU\S. Thus f(x)â©¾f(xâˆ—)for all xâˆˆUandxâˆ—
is a local minimum. This yields a contradiction. Hence let s0âˆˆSâ—¦be such that
f(s0)< f(xâˆ—). The nonempty closed set Sâ€²:=Sâˆ©[fâ©½(f(s0) +f(xâˆ—))/2]is a
setwise local minimum of fwhere [fâ©½Î±] :={xâˆˆRn|f(x)â©½Î±}. Indeed, for
allxâˆˆSâ€²andyâˆˆU\Sâ€²= (U\S)âˆª(U\[fâ©½(f(s0) +f(xâˆ—))/2]), we have6
f(x)â©½(f(s0) +f(xâˆ—))/2â©½f(y). Since s0âˆˆSâ—¦andf(s0)<(f(s0) +f(xâˆ—))/2, we
have s0âˆˆSâ—¦âˆ©[fâ©½(f(s0) +f(xâˆ—))/2]â—¦= (Sâˆ©[fâ©½(f(s0) +f(xâˆ—))/2]))â—¦= (Sâ€²)â—¦.
Hence the setwise local minimum Sâ€²has nonempty interior. Also, Sâ€²âŠ‚Sand
supSâ€²f < f (xâˆ—) =min Cfwhere we remind the reader that Cis the set of critical
points in S. Thus Sâ€²is devoid of critical points. However, by the previous paragraph,
setwise local minima of fwith nonempty interior must contain a critical point. This
yields a contradiction.
Remark 2 (Finitely many critical values) .This assumption is not intuitive and we
explain why it is necessary by the following example. Define f:Râ†’Ras
f(x) :=ï£±
ï£´ï£´ï£´ï£²
ï£´ï£´ï£´ï£³(x+ 4)2âˆ’8 ifxâ©½âˆ’2;
âˆ’x2ifxâˆˆ[âˆ’2,0];
âˆ’2âˆ’k(xâˆ’2k)2k+1âˆ’3(1âˆ’2âˆ’k)ifxâˆˆ[2k,2k+ 1], kâˆˆN;
2âˆ’k(xâˆ’2k)2k+1âˆ’3(1âˆ’2âˆ’k)ifxâˆˆ[2kâˆ’1,2k], kâˆˆN+.
where N+is the set of all positive integers. To be more intuitive, we give the plot of
fon[âˆ’7,7]in Figure 4.
By standard calculus, one can see fis continuously differentiable, f(x)â†’ âˆ’3
asxâ†’ âˆ, and f(x)â©¾âˆ’8overR. Furthermore, {âˆ’4} âˆª { 2k}kâˆˆNare all critical
6Indeed, for any sets A, B, and Cit holds that A\(Bâˆ©C) = (A\B)âˆª(A\C).
12Figure 4: An example of function with infinitely many critical values
points of f, with critical values {âˆ’8} âˆª {âˆ’ 3(1âˆ’2âˆ’k)}kâˆˆNrespectively. Finally, the
subgradient trajectory of fstarting at x0<0will converge to the critical point
x=âˆ’4; the one starting at x0= 0will stay at the critical point x= 0; and the
one starting at x0>0such that 2k < x 0â©½2k+ 2will converge to x= 2k+ 2, for
allkâˆˆN. This shows fhas bounded subgradient trajectories. Thus, fsatisfies all
conditions in assumption 1 except the finiteness of critical values. It is also easy
to see fhas no spurious local minima because all of its critical points are either
global minimum ( x=âˆ’4), or local maximum ( x= 0), or saddle points. However,
for any a >0, the set [a,âˆ)is a spurious local minima at infinity. This shows that
Theorem 1 may not hold for functions with infinitely many critical values.
Remark 3 (Bounded subgradient trajectories) .This is the main assumption of
Theorem 1. Without it, one could easily think of a smooth function without any
spurious local minimum, yet has spurious local minimum at infinity. This is the case
of the function in Figure 3 in which the yellow curve corresponds to an unbounded
gradient trajectory. In order to prove the necessity of the boundedness assumption,
it suffices to consider the univariate function fdefined in [ 27, Figure 4(a)] defined by
f(x) :=x2(1 +x2)
1 +x4, fâ€²(x) =âˆ’2x(x4âˆ’2x2âˆ’1)
(x4+ 1)2.
By solving fâ€²(x) = 0, we know that fhas three critical points, among which x= 0
is the global minimum and x=Â±(âˆš
2âˆ’1)âˆ’1/2are two global maxima. Thus, fis
bounded below, continuously differentiable (hence locally Lipschitz and admits a
chain rule), has finitely many critical values, and has no spurious local minima. Since
fis strictly decreasing for all xâ©¾(âˆš
2âˆ’1)âˆ’1/2â‰ˆ1.55andf(x)â†’1asxâ†’ âˆ,
one can easily see [2,âˆ)is a spurious local minimum at infinity. This shows that
13Theorem 1 does not hold and the reason is that fdoes not have bounded subgradient
trajectories. To see this explicitly, consider the Cauchy problem
Ë™x=2x(x4âˆ’2x2âˆ’1)
(x4+ 1)2, x(0) = 2 .
By using separation of variables, the unique solution x(t)is given by
c+ 2t=1
4x4+x2+ (2 +âˆš
2) log( x2âˆ’âˆš
2âˆ’1)
+ (2âˆ’âˆš
2) log( x2+âˆš
2âˆ’1)âˆ’logx=:g(x),
where cis a constant determined by x(0) = 2. It is easy to see that xis strictly
increasing so x(t)â©¾2for all tâˆˆ[0,âˆ). Note that gis continuous on [2,âˆ), so if xis
bounded, then gâ—¦xis bounded. This contradicts the fact that g(x(t)) = 2 t+câ†’ âˆ
astâ†’ âˆ, and thus fhas an unbounded subgradient trajectory.
4 Applications
In this section, we use Theorem 1 to analyze the landscape of some widely used loss
functions in unconstrained optimization. To be more specific, we will consider deep
linear neural network, one dimensional deep sigmoid neural network, matrix sensing,
and nonsmooth matrix factorization in the following four subsections respectively.
4.1 Deep linear neural network
As a prototypical example in deep learning, the landscape of deep linear neural
network has been widely studied; see for example [ 28,29,44]. Consider minimizing
the loss function of linear neural network without bias term
f(W1, . . . , W L) :=1
2âˆ¥WLÂ·Â·Â·W1Xâˆ’Yâˆ¥2
F, (3)
where XâˆˆRd0Ã—dx,YâˆˆRdLÃ—dx, and WiâˆˆRdiÃ—diâˆ’1fori= 1, . . . , L. Here âˆ¥ Â· âˆ¥ F
denotes the Frobenius norm. It was recently established that fhas no spurious
valleys [44, Theorem 11], however this fact alone does not imply the absence of
spurious local minima at infinity (recall Figure 2). Together with the fact that fhas
no spurious local minima [ 46, Corollary 1] and that fis semi-algebraic, it can be
deduced that fhas no spurious setwise local minima (and thus no spurious local
minima at infinity).
The proof of the absence of spurious valleys [ 44, Theorem 11] is tailored to
the problem at hand. Using linear algebra, it argues that from any initial point
one can construct a piecewise linear path to a global minimum along which the
14objective function is non-increasing. The proof spans multiple pages and requires
several technical lemmas. The proof that we propose is shorter and follows a
general principle, namely Theorem 1, that applies to various problems as the next
subsections will show. The first four assumptions of Theorem 1 are easy to verify: f
is nonnegative, hence bounded below; fis continuously differentiable, hence locally
Lipschitz and admits a chain rule; fis semi-algebraic, by [ 38, Corollary 1.1], it
has finitely many critical values. Thus, it suffices to show fhas bounded gradient
trajectories.
Proposition 4. Linear neural network with loss function (3)has bounded gradient
trajectories.
An existing proof of Proposition 4 under additional assumptions on network
structure, initialization, input data, or target data can be found, for instance, in
[3,18,9]. To the best of our knowledge, the closest result to Proposition 4 is [ 3,
Theorem 3.2], which shows that gradient trajectories are bounded if XXTis of full
rank. In the proof of Proposition 4, we show that this rank assumption on Xcan be
removed and hence Proposition 4 applies to any linear neural network.
Proof of Proposition 4. Since fis locally Lipschitz and lower bounded, by
Proposition 3 there exists a gradient trajectory for any initial point. By [ 3, Lemma
2.1], the gradient trajectories of fsatisfy the initial value problem
Ë™Wi=âˆ’(WLÂ·Â·Â·Wi+1)T(WLÂ·Â·Â·W1Xâˆ’Y)(Wiâˆ’1Â·Â·Â·W1X)T, (4a)
Wi(0) = W0
i, W0
iâˆˆRdiÃ—diâˆ’1is a given constant matrix , (4b)
for all i= 1, . . . , L. Note that if i=L, (4a) reduces to
Ë™WL=âˆ’(WLÂ·Â·Â·W1Xâˆ’Y)(WLâˆ’1Â·Â·Â·W1X)T,
and if i= 1, (4a) reduces to
Ë™W1=âˆ’(WLÂ·Â·Â·W2)T(WLÂ·Â·Â·W1Xâˆ’Y)XT.
Note that [ 3, Theorem 3.2] proved the boundedness of gradient trajectories of
fwhen XXTis invertible. Thus, we only need to show we can always reduce the
boundedness of gradient trajectories of ffor general Xto the boundedness of gradient
trajectories of another function gin the same form as fbut with invertible XXT. Let
X=UÎ£VTbe a singular value decomposition, where UâˆˆRd0Ã—d0andVâˆˆRdxÃ—dx
are orthogonal matrices, and Î£âˆˆRd0Ã—dxis a rectangular matrix satisfying
Î£ =Î› 0
0 0
,Î› = diag( Î»1, . . . , Î» r)â‰»0,
15where râ©½min{d0, dx}. Eliminating Xin (4a), it reduces to
Ë™Wi=âˆ’(WLÂ·Â·Â·Wi+1)T(WLÂ·Â·Â·W1UÎ£VTâˆ’Y)(Wiâˆ’1Â·Â·Â·W1UÎ£VT)T
=âˆ’(WLÂ·Â·Â·Wi+1)T(WLÂ·Â·Â·W1UÎ£âˆ’Y V)(Wiâˆ’1Â·Â·Â·W1UÎ£)T.
Define Z:=Y VâˆˆRdLÃ—dx, and (4) reduces to
Ë™Wi=âˆ’(WLÂ·Â·Â·Wi+1)T(WLÂ·Â·Â·W1UÎ£âˆ’Z)(Wiâˆ’1Â·Â·Â·W1UÎ£)T,(5a)
Wi(0) = W0
i,âˆ€i= 1, . . . L. (5b)
Denote W1:=W1UâˆˆRd1Ã—d0andW0
1:=W0
1UâˆˆRd1Ã—d0. To keep the notation
consistent, also let Wi:=WiandW0
i:=W0
ifori= 2, . . . , L. Thus, (5) reduces to
Ë™Wi=âˆ’(WLÂ·Â·Â·Wi+1)T(WLÂ·Â·Â·W1Î£âˆ’Z)(Wiâˆ’1Â·Â·Â·W1Î£)T,(6a)
Wi(0) = W0
i,âˆ€i= 1, . . . L. (6b)
Partition the matrices W1,W0
1, and Zinto two column blocks:
W1=
W11W12
,W0
1=h
W0
11W0
12i
, Z =
Z1Z2
,
where W11,W0
11, and Z1consist of the first rcolumns of W1,W0
1andZrespectively.
Thus, when i= 1, (6) can be reduced into
Ë™W11=âˆ’(WLÂ·Â·Â·W2)T(WLÂ·Â·Â·W2W11Î›âˆ’Z1)Î›T,Ë™W12= 0,
W11(0) = W0
11,W12(0) = W0
12.
When i= 2, . . . , L, (6) can be reduced into
Ë™Wi=âˆ’(WLÂ·Â·Â·Wi+1)T(WLÂ·Â·Â·W2W11Î›âˆ’Z1)(Wiâˆ’1Â·Â·Â·W2W11Î›)T,
Wi(0) = W0
i.
It indicates that W12(t) =W0
12for all tâ©¾0. Denote fW1:=W11andfW0
1:=W0
11.
To keep the notation consistent, also let fWi:=WiandfW0
i:=W0
ifori= 2, . . . , L.
Therefore, (6) reduces to
Ë™fWi=âˆ’(fWLÂ·Â·Â·fWi+1)T(fWLÂ·Â·Â·fW1Î›âˆ’Z1)(fWiâˆ’1Â·Â·Â·fW1Î›)T, (7a)
fWi(0) =fW0
i,âˆ€i= 1, . . . L. (7b)
Define the new function gas
g(fW1, . . . ,fWL) :=1
2âˆ¥fWLÂ·Â·Â·fW1Î›âˆ’Z1âˆ¥2
F.
16Notice that the gradient trajectories of gsatisfy (7). To prove fhas bounded gradient
trajectories, it is equivalent to prove ghas bounded gradient trajectories, because
âˆ¥W1âˆ¥F=âˆ¥W1Uâˆ¥F=âˆ¥W1âˆ¥Fandâˆ¥W1(t)âˆ¥2
F=âˆ¥fW1(t)âˆ¥2
F+âˆ¥W0
12âˆ¥2
Ffor all tâ©¾0.
Since Î›Î›Tis invertible, by [ 3, Theorem 3.2], ghas bounded gradient trajectories,
and so does f.
With Proposition 4, we verified that fsatisfies assumption 1. Thus, (3) has no
spurious setwise local minima if and only if it has no spurious local minima. Since
a local minimum at infinity is an unbounded setwise local minimum, and fhas no
spurious local minima, we conclude that fhas no spurious local minima at infinity.
This proves the first result in Corollary 1.
4.2 One dimensional deep sigmoid neural network
Though famous for its benign theoretical properties, linear neural network is rarely
used in practice because of its low representation power. We want to take a step
further in the case of nonlinear deep neural network. In this subsection, we focus on
neural network with sigmoid activation function in one dimensional case. Landscape
analysis of one or two-hidden layer sigmoid neural network can be found, for instance,
in [44,15,31]. However, none of the results above can be easily generalized to
arbitrary many layers.
Consider minimizing the following loss function of sigmoid neural network
f(w1, . . . , w L) :=1
2(wLÏƒ(wLâˆ’1Â·Â·Â·Ïƒ(w1x))âˆ’y)2, (8)
where Ïƒ(z) := (1 + eâˆ’z)âˆ’1is the sigmoid function and wi, x, yâˆˆRfor all i= 1, . . . , L.
We want to apply Theorem 1 to conclude that (8) has no spurious setwise local
minimum, and hence no local minima at infinity. Again, the first three assumptions
in assumption 1 are easy to verify: fis nonnegative, hence bounded below; fis
continuously differentiable, hence locally Lipschitz, and admits a chain rule. Note
that fis not semi-algebraic, but it is definable in the real exponential field [ 45] [6,
Section 6.2], so by Morseâ€“Sard theorem for definable functions [ 5, Corollary 9(ii)], it
has finitely many critical values.
Again, it remains to show (8) has bounded gradient trajectories. However, the
techniques in the proof of Proposition 4 cannot be adapted to this case because the
auto-balancing property in [ 17, Theorem 2.1] does not hold. Surprisingly, it is still
true that (8) has bounded gradient trajectories.
Proposition 5. One dimensional sigmoid neural network with loss function (8)has
bounded gradient trajectories.
Proof.Since fis locally Lipschitz and lower bounded, by Proposition 3 there exists a
gradient trajectory for any initial point. For simplicity, define pifori= 0, . . . , L âˆ’2
17recursively by p0:=x,p1:=Ïƒ(w1x)andpi+1:=Ïƒ(wi+1pi). The gradient trajectories
offsatisfy
Ë™wL=âˆ’(wLÏƒ(wLâˆ’1pLâˆ’2)âˆ’y)Ïƒ(wLâˆ’1pLâˆ’2), (9a)
Ë™wi=piâˆ’1
1 +ewipiâˆ’1Ë™wi+1wi+1, i=Lâˆ’1, . . . , 1. (9b)
We will prove each wiis bounded inductively from the last layer to the first layer.
The relation between the last two layers wLandwLâˆ’1, and the relation between the
first two layers can be regarded as the base cases.
We claim that there exists a time Tsuch that Ë™wiandwidoes not change sign
for all tâ©¾Tand for all i. To verify this, first notice that the claim is true for the
last layer, i.e., Ë™wLandwLwill not change sign for all tâ©¾T. Suppose Ë™wLchanges
sign, by continuity and mean value theorem, there exists tâˆ—>0such that Ë™wL(tâˆ—) = 0.
However, Ë™wL(tâˆ—) = 0implies Ë™wi(tâˆ—) = 0for all i, meaning that a critical point is
achieved and the gradient trajectory is stopped for all tâ©¾tâˆ—. In this case, all wiâ€™s
are trivially bounded. Thus, we assume the trajectory will never stop at a finite time.
In this case, either Ë™wL(t)>0orË™wL(t)<0for all tâ©¾0. Since wLis monotonic, it
either keeps the sign unchanged or changes the sign only once. Thus, there exists
TL>0such that wLdoes not change sign on [TL,âˆ). Notice that for all iâ©¾2,
piâˆ’1(t)âˆˆ(0,1)for all tâ©¾0. Since Ë™wLwLdoes not change sign on [TL,âˆ), (9b)
implies that Ë™wLâˆ’1does not change sign on [TL,âˆ)either. Therefore, we conclude
thatwLâˆ’1is monotonic. Similarly, there exists TLâˆ’1> T Lsuch that Ë™wLâˆ’1andwLâˆ’1
does not change sign on [TLâˆ’1,âˆ). Recursively using the above argument, we can
show the claim is true for all iâ©¾2on[T2,âˆ). For i= 1, although p0=xmay
not be in (0,1), since xis a constant, the fact that Ë™w2andw2do not change sign
still implies that Ë™w1does not change sign and hence there exists T1> T 2such that
w1does not change sign on [T1,âˆ). Therefore, the claim holds for i= 1, . . . , Lby
choosing T=T1.
By the claim proved in the last paragraph, for i= 1, . . . , L, either Ë™wiwiis
nonnegative or Ë™wiwiis negative on [T,âˆ). Now we are going to prove each wiis
bounded. The first step is to prove the last two layers wLandwLâˆ’1are bounded.
Consider the case where Ë™wLwLis nonnegative on [T,âˆ). (9b) implies that Ë™wLâˆ’1â©¾0
andwLâˆ’1isincreasingover [T,âˆ), sothereexistsaconstant cLâˆ’1suchthat wLâˆ’1(t)â©¾
cLâˆ’1forall tâ©¾0. Since pLâˆ’2âˆˆ(0,1), wehave Ïƒ(wLâˆ’1pLâˆ’2)â©¾Ïƒ(âˆ’|cLâˆ’1|)>0. Again,
by [14, Lemma 5.2],d
dtf(w1, . . . , w L)â©½0andf(w1, . . . , w L)â©½Cfor some constant
Con[0,âˆ). Thus, it is easy to see |wL|Ïƒ(wLâˆ’1pLâˆ’2)â©½C1for some constant C1on
[0,âˆ). Since Ïƒ(wLâˆ’1pLâˆ’2)âˆˆ[Ïƒ(âˆ’|cLâˆ’1|),1), we conclude |wL|is bounded. Suppose
wLâˆ’1is unbounded. Since it is increasing and does not change sign, wLâˆ’1(t)>0for
alltâ©¾TandwLâˆ’1(t)â†’ âˆastâ†’ âˆ. By (9b),
Ë™wLâˆ’1=pLâˆ’2
1 +ewLâˆ’1pLâˆ’2Ë™wLwLâ©½Ë™wLwL, (10)
18because pLâˆ’2âˆˆ(0,1)and1 +ewLpLâˆ’2>1. By (10), wLâˆ’1âˆ’1
2w2
Lis a decreasing
function on [T,âˆ). Hence, wLâˆ’1âˆ’1
2w2
Lâ©½C2for some constant C2. Notice that wL
is bounded but wLâˆ’1(t)â†’ âˆastâ†’ âˆ, so a contradiction occurs. Therefore, wLâˆ’1
is bounded.
Now we consider the case where Ë™wLwLis negative on [T,âˆ). In this case, (9b)
implies Ë™wLâˆ’1â©½0, sowLâˆ’1is decreasing on [T,âˆ)and there exists a constant dLâˆ’1
such that wLâˆ’1â©½dLâˆ’1. Since pLâˆ’2/(1 +ewLâˆ’1pLâˆ’2)âˆˆ(0,1)and Ë™wLwLâ©½0on[T,âˆ),
we have Ë™wLâˆ’1â©¾Ë™wLwL. This shows wLâˆ’1âˆ’1
2w2
Lis increasing on [T,âˆ), and hence
wLâˆ’1â©¾ËœdLâˆ’1for some constant ËœdLâˆ’1. Therefore, wLâˆ’1âˆˆ[ËœdLâˆ’1, dLâˆ’1]is bounded.
By exactly the same argument as in the case when Ë™wLwLis nonnegative, we know
Ïƒ(wLâˆ’1pLâˆ’2)âˆˆ[Ïƒ(âˆ’|ËœdLâˆ’1|),1)andwLis bounded by using the boundedness of
objective function f.
Up to now, we have proved boundedness for the last two layers wLandwLâˆ’1. For
i= 2, . . . , L âˆ’2, by discussing two cases Ë™wi+1wi+1â©¾0and Ë™wi+1wi+1â©½0, together
with the boundedness of wi+1, we can prove that wiis bounded by exactly the same
argument as we did in the last two paragraphs. The induction starts with proving
wLâˆ’2is bounded and ends with proving w2is bounded. Once we prove w2is bounded,
consider the relation between w1andw2,
(1 +ew1x) Ë™w1=xË™w2w2.
Ifx= 0, then Ë™w1= 0implies w1is a constant over [0,âˆ), so it must be bounded.
Suppose xÌ¸= 0, by taking integration with respect to tand multiplying xon both
sides, we have
w1x+ew1x=x2
2w2
2+C3.
Letz=w1x, then z+ezâ†’ Â±âˆaszâ†’ Â±âˆ. Thus, the boundedness of w2implies
the boundedness of z=w1x. Since xÌ¸= 0is a constant, w1is bounded. Therefore,
we proved that wiis bounded for all i= 1, . . . , L.
WithProposition5, wecanconcludethat fhasnospurioussetwiselocalminimum
if and only if it has no spurious local minima. However, from the gradient of f, we
can easily see that any critical point of it will be a global minimum, so fhas neither
spurious local minimum nor spurious setwise local minimum. This verifies the second
result in Corollary 1.
Unfortunately, unlike linear neural networks, the result in Proposition 5 is not
true in general even in one-hidden layer case, if more than one data point is given;
see Example 2. However, it is still an open question whether the gradient trajectories
will be bounded in the over-parameterized case (in which case there exists at least
one achievable global minimum).
19Example 2. Consider the following function
f(w1, w2) :=1
2[(w2Ïƒ(w1)âˆ’1)2+ (w2Ïƒ(âˆ’w1) + 1)2]. (11)
The above function represents a one-hidden layer sigmoid neural network with two
data (x1, y1) = (1 ,1)and(x2, y2) = (âˆ’1,âˆ’1). By directly computing the gradient,
one can easily see that (11) has only one critical point (0,0)which is a strict saddle
with f(0,0) = 1. The global minimum is asymptotically attained as w1â†’ Â±âˆand
w2â†’1âˆ’2(1 + e2w1)âˆ’1, and its corresponding objective value approaches to 1/2. In
this case, the gradient trajectory of (11) starting at any point x0such that f(x0)<1
must be unbounded.
4.3 Matrix sensing
Matrix sensing is a widely used model in computer vision and statistics; see for
instance [ 10,39]. Given râ©¾1, the goal is to recover an unknown target matrix
MâˆˆRn1Ã—n2of rank less than or equal to rfrom a set of linear measurements
bi=âŸ¨Ai, MâŸ©F, where AiâˆˆRn1Ã—n2fori= 1, . . . , mare sensing matrices and âŸ¨Â·,Â·âŸ©Fis
the Frobenius inner product. In order to do so, we minimize the mean square loss
f(X, Y) :=1
2mmX
i=1(âŸ¨Ai, XYTâŸ©Fâˆ’bi)2. (12)
where XâˆˆRn1Ã—randYâˆˆRn2Ã—r. The landscape of (12) has been studied widely,
for example, in [ 47,37,32]. Most of these work are based on the restrictive isometry
property (RIP) of sensing matrices. A set of sensing matrices Aifori= 1, . . . , mare
said to have (r, Î´r)-RIP [39] if there exists Î´râˆˆ(0,1)such that
(1âˆ’Î´r)âˆ¥fMâˆ¥2
Fâ©½1
mmX
i=1âŸ¨Ai,fMâŸ©2
Fâ©½(1 +Î´r)âˆ¥fMâˆ¥2
F
holds for any matrix fMwith rank(fM)â©½r. To the best of our knowledge, the
minimal assumptions to guarantee no spurious local minima for (12) is for the
sensing matrices to satisfy (4r, Î´4r)-RIP with Î´4râ©½1/5, as proposed in [ 32, Theorem
III.1].
However, Theorem 1 is applicable to matrix sensing under a weaker condition
than RIP. The first four assumptions in assumption 1 hold because of exactly the
same reasons as in the linear neural network case. Thus, it suffices to show (12)
has bounded gradient trajectories. A sufficient condition is to require the sensing
matrices to be lower bounded , i.e., there exists a constant c >0such that for any
matrix fMâˆˆRn1Ã—n2with rank(fM)â©½r,
1
mmX
i=1âŸ¨Ai,fMâŸ©2
Fâ©¾câˆ¥fMâˆ¥2
F.
20It is easy to see any level of RIP will imply the existence of such a constant c.
Proposition 6. Matrix sensing with loss function (12)and lower bounded sensing
matrices has bounded gradient trajectories.
Proof.Since fis locally Lipschitz and lower bounded, by Proposition 3 there exists
a gradient trajectory for any initial point. The gradient trajectories of fsatisfy the
initial value problem
Ë™X=âˆ’1
mmX
i=1(âŸ¨Ai, XYTâŸ©Fâˆ’bi)AiY,
Ë™Y=âˆ’1
mmX
i=1(âŸ¨Ai, XYTâŸ©Fâˆ’bi)AT
iX,
X(0) = X0, Y (0) = Y0.
Notice that Ë™XTX=YTË™YandXTË™X=Ë™YTY, so
d
dt(XTXâˆ’YTY) =Ë™XTX+XTË™Xâˆ’Ë™YTYâˆ’YTË™Y= 0.
This implies that XTXâˆ’YTY=Cwhere CâˆˆRrÃ—ris a constant. Since the function
value is decreasing along gradient trajectories [ 14, Lemma 5.2], there exists a constant
c1such that f(X(t), Y(t))â©½c1for all tâ©¾0. Combined with the assumption that
sensing matrices are lower bounded, there exist constants candc2such that
câˆ¥XYTâˆ¥2
Fâ©½1
mmX
i=1âŸ¨Ai, XYTâŸ©2
Fâ©½1
mmX
i=1[2(âŸ¨Ai, XYTâŸ©Fâˆ’bi)2+ 2b2
i]
= 2f(X, Y) +2
mmX
i=1b2
iâ©½2c1+2
mmX
i=1b2
i=:c2.
We have âˆ¥XYTâˆ¥2
Fâ©½c3:=c2/c. Notice that
âˆ¥XTXâˆ¥2
F+âˆ¥YTYâˆ¥2
F=âˆ¥XTXâˆ’YTYâˆ¥2
F+ 2âˆ¥XYTâˆ¥2
Fâ©½âˆ¥Câˆ¥2
F+ 2c3.
Define the constant c4:= 2c3+âˆ¥Câˆ¥2
F. By the Cauchy-Schwarz inequality,
âˆ¥Xâˆ¥4
F+âˆ¥Yâˆ¥4
Fâ©½rank( X)âˆ¥XTXâˆ¥2
F+ rank( Y)âˆ¥YTYâˆ¥2
Fâ©½(n1+n2+r)c4.
Thus, XandYare bounded.
Therefore, Theorem 1 says that matrix sensing has no spurious setwise local
minima if and only if it has no spurious local minima, given that the sensing matrices
are lower bounded. Equipped with (4r, Î´4r)-RIP where Î´4râ©½1/5, we conclude
that matrix sensing has no spurious local minima at infinity, as shown in the third
statement in Corollary 1.
214.4 Nonsmooth matrix factorization
In this subsection, we consider the application of Theorem 1 in a nonsmooth setting,
namely, the nonsmooth matrix factorization problem. We consider minimizing the
loss function
f(X, Y) :=âˆ¥XYTâˆ’Mâˆ¥1, (13)
where XâˆˆRmÃ—r,YâˆˆRnÃ—rare decision variables and MâˆˆRmÃ—nis the given
data matrix. Here âˆ¥Aâˆ¥1:=Pm
i=1Pn
j=1|Aij|for any AâˆˆRmÃ—n. In robust principal
component analysis (PCA) problem with sparse noise, (13) is usually used as a
surrogate function for the original â„“0-norm formulation; see [ 23,8]. There are few
landscape results of (13) in the general rank case. However, if rank(M) = 1 = r, (13)
is shown to have no spurious local minima if every entry MijofMis nonzero [26].
It is hard to analyze (13) because it is nonsmooth, nonconvex, and noncoercive.
Despite all those â€œnonâ€ properties, we show that Theorem 1 is still applicable to
(13) without any rank assumption on M. As a corollary, when rank(M) = 1and
every entry of Mis nonzero, (13) has no spurious setwise local minimum, hence no
spurious local minima at infinity. Again, the first four assumptions in assumption 1
are easy to check: fis bounded below because it is nonnegative; fis locally Lipschitz
because of [ 11, Theorem 2.3.10]; since fis semi-algebraic, by [ 16, Corollary 5.4] and
[38, Corollary 1.1], it admits a chain rule and has finitely many critical values.
To verify (13) has bounded subgradient trajectories, we discover that the auto-
balancing property in [ 17, Theorem 2.2] also holds for nonsmooth matrix factorization.
The result can be summarized in the following proposition.
Proposition 7. Nonsmooth matrix factorization with loss function (13)has bounded
subgradient trajectories.
Proof.Since fis locally Lipschitz and lower bounded, by Proposition 3 there exists
a subgradient trajectory for any initial point. Let (X0, Y0)âˆˆRmÃ—rÃ—RnÃ—r. Consider
an absolutely continuous function Z: [0,âˆ)â†’RmÃ—rÃ—RnÃ—rsuch that
Zâ€²(t)âˆˆ âˆ’âˆ‚f(Z(t)),for almost every tâ©¾0,andZ(0) = ( X0, Y0).
By [11, Theorem 2.3.10],
âˆ‚f(X, Y) =Î›Y
Î›TXÎ›âˆˆsign(XYTâˆ’M)
where signis an element-wise operation mapping each entry of a matrix to a real
number in [âˆ’1,1]such that
sign(x) :=ï£±
ï£²
ï£³âˆ’1ifx <0,
âˆ’1,1
ifx= 0,
1ifx >0.
22Hence, with Z=: (X, Y), for almost every tâ©¾0we have
Xâ€²(t) =âˆ’Î›(t)Y(t), Yâ€²(t) =âˆ’Î›(t)TX(t), (14a)
Î›(t)âˆˆsign(X(t)Y(t)Tâˆ’M). (14b)
Consider Ï•: [0,âˆ)â†’Rdefined by Ï•(t) := X(t)TX(t)âˆ’Y(t)TY(t). By taking
derivative, we have
Ï•â€²(t) =Xâ€²(t)TX(t) +X(t)TXâ€²(t)âˆ’Yâ€²(t)TY(t)âˆ’Y(t)TYâ€²(t).(15)
Combining (14a) and (15), we have
Ï•â€²(t) =âˆ’Y(t)TÎ›(t)TX(t)âˆ’X(t)TÎ›(t)Y(t)
+X(t)TÎ›(t)Y(t) +Y(t)TÎ›(t)TX(t) = 0 .
Hence the continuous function Ï•is constant on [0,âˆ). Also, we have
âˆ¥XTXâˆ’YTYâˆ¥2
F=âˆ¥XTXâˆ¥2
F+âˆ¥YTYâˆ¥2
Fâˆ’2âŸ¨XTX, YTYâŸ©F
=âˆ¥XTXâˆ¥2
F+âˆ¥YTYâˆ¥2
Fâˆ’2âˆ¥XYTâˆ¥2
F
â©¾âˆ¥XTXâˆ¥2
2+âˆ¥YTYâˆ¥2
2âˆ’2âˆ¥XYTâˆ¥2
F
=âˆ¥Xâˆ¥4
2+âˆ¥Yâˆ¥4
2âˆ’2âˆ¥XYTâˆ¥2
F
â©¾âˆ¥Xâˆ¥4
2+âˆ¥Yâˆ¥4
2âˆ’2mnâˆ¥XYTâˆ¥2
1
â©¾âˆ¥Xâˆ¥4
2+âˆ¥Yâˆ¥4
2âˆ’2mn(âˆ¥XYTâˆ’Mâˆ¥1+âˆ¥Mâˆ¥1)2.
Hereâˆ¥ Â· âˆ¥ 2denotes the spectral norm. Therefore, for all tâ©¾0, we have
âˆ¥X(t)âˆ¥4
2+âˆ¥Y(t)âˆ¥4
2â©½âˆ¥X(t)TX(t)âˆ’Y(t)TY(t)âˆ¥2
F
+ 2mn(âˆ¥X(t)Y(t)Tâˆ’Mâˆ¥1+âˆ¥Mâˆ¥1)2
â©½âˆ¥XT
0X0âˆ’YT
0Y0âˆ¥2
F
+ 2mn(âˆ¥X0YT
0âˆ’Mâˆ¥1+âˆ¥Mâˆ¥1)2.
Combined with Proposition 7, Theorem 1 shows that (13) has no spurious setwise
local minimum if and only if it has no spurious local minima. Under the condition
in [26, Theorem 1], i.e., rank(M) = 1 = rand all the entries of Mare non-zero, (13)
reduces to
f(x, y) :=mX
i=1nX
j=1|xiyjâˆ’Mij|, (16)
where xâˆˆRmandyâˆˆRn. In this case, (16) has no spurious local minima, thus it
has no spurious local minima at infinity, and we obtain the last result in Corollary 1.
23Acknowledgments
We thank the reviewers and the associate editor for their valuable feedback.
A Proof of Lemma 1
LetSâŠ‚Rnbe a setwise local minimum of a continuous function f:Rnâ†’R. Let
UâŠƒSbe an open set such that f(x)â©½f(y)for all xâˆˆSandyâˆˆU\S. Note that
Sis closed, so its boundary is defined by âˆ‚S:=S\Sâ—¦. Let zâˆˆâˆ‚Sand consider
any real number Ïµ >0. Since f(z) + (âˆ’Ïµ, Ïµ)is a neighborhood of f(z), by continuity
off, there exists a neighborhood N(z)ofzsuch that f(N(z))âŠ‚f(z) + (âˆ’Ïµ, Ïµ).
Since Uis a neighborhood of z,Nâ€²(z) :=Uâˆ©N(z)is also a neighborhood of zwith
f(Nâ€²(z))âŠ‚f(z) + (âˆ’Ïµ, Ïµ). The set Nâ€²(z)âˆ©Sis nonempty because zâˆˆSand the set
Nâ€²(z)\Sis nonempty because zâˆˆâˆ‚S. For any xâˆˆNâ€²(z)âˆ©SandyâˆˆNâ€²(z)\S, it
follows that
inf
U\Sfâˆ’Ïµâ©½f(y)âˆ’Ïµ < f (z)< f(x) +Ïµâ©½sup
Sf+Ïµâ©½inf
U\Sf+Ïµ.
The last inequality follows from the definition of setwise local minima. As Ïµ >0was
arbitrary, we deduce that
inf
U\Sf=f(z) = sup
Sf.
Thus, fis a constant on the boundary of Sandfattains its maximum over Son
the boundary of S.
B Proof of Proposition 1
(a)LetSbe a setwise local minimum. By lemma 1, we know that c:=supSf=
f(z)for all zâˆˆâˆ‚S. Take a path-connected component CofS. Then CâŠ‚
[fâ©½c] :={xâˆˆRn|f(x)â©½c}. Since Cis path-connected, there exists a
path-connected component Vof[fâ©½c]such that CâŠ‚V. By definition, V
is a valley. This shows that a path-connected component of a setwise local
minimum is a subset of a valley.
If in addition, Sis a strict setwise local minimum, then we distinguish two
cases. If S=Rn, then the path-connected component CofSis equal to Rn
and is therefore a valley. Otherwise, it suffices to show that VâŠ‚S. Indeed, V
is then a path-connected subset of Scontaining the path-connected component
CofS, so that by maximality, V=C. Therefore Cis valley.
Consider an open set UâŠƒSsuch that f(x)> f(y)for all xâˆˆU\Sand
yâˆˆS. In order to show that VâŠ‚S, it suffices to show that Vâˆ©(U\S) =âˆ…and
24Vâˆ©Uc=âˆ…because if so, then V= (Vâˆ©S)âˆª(Vâˆ©(U\S))âˆª(Vâˆ©Uc) =Vâˆ©S.
Since f(w)â©½cfor all wâˆˆVandf(w)> cfor all wâˆˆU\S(the supremum
function value f(y) =ccan be attained by some yâˆˆâˆ‚SâŠ‚S), we know that
Vâˆ©(U\S) =âˆ…. Thus, V= (Vâˆ©S)âˆª(Vâˆ©Uc). Note that Vâˆ©Sis nonempty
and closed because CâŠ‚Vâˆ©SandVandSare both closed. Since Vâˆ©Ucis
also closed and Vis connected, Vâˆ©Ucmust be empty.
(b)Letf:Rnâ†’Rbe a continuous function and aâˆˆRbe a nonempty sublevel
set of f. By continuity of f,[fâ©½a]is closed in Rn. Suppose [fâ©½a]has
finitely many connected components C1, . . . , C k. Denote Bas the closure of
any set BâŠ‚Rn. Since Ciâ€™s are connected, by [ 35, Theorem 23.4], Ciâ€™s are also
connected. Since CiâŠ‚[fâ©½a],CiâŠ‚[fâ©½a]= [fâ©½a]. By [35, Theorem 25.1],
Cihas no intersection with any other CjforjÌ¸=i. Together with the fact
that [fâ©½a] =Sk
i=1Ci, we have CiâŠ‚Ci, and hence Ci=Ci. Thus, each Ciis
closed in Rn.
For any fixed i, denote Câˆ’i:= [fâ©½a]\Ci, then Câˆ’i=Sk
j=1,jÌ¸=iCjis a
closed set disjoint with Ci. By [35, Theorem 32.2], there exist disjoint open
setsD, EâŠ‚Rnsuch that CiâŠ‚DandCâˆ’iâŠ‚E. Take U=Din Definition 2,
then f(x)â©½afor all xâˆˆCibecause CiâŠ‚[fâ©½a]. Furthermore, f(y)> afor
allyâˆˆU\Cibecause (U\Ci)âˆ©[fâ©½a] =âˆ…. This verifies that Ciis a strict
setwise local minimum of f.
C Proof of Proposition 2
LetSbe a spurious setwise local minimum at infinity. Since infSf >infRnf, it must
be that SÌ¸=Rn. By Definition 2, there exists yâˆˆScsuch that SâŠ‚ {xâˆˆRn|f(x)â©½
f(y)}. Since fis coercive, its sublevel sets are bounded and hence Sis bounded. S
is thus not a spurious local minimum at infinity.
D Proof of Proposition 3
For a fixed real number Ï„ >0, define a sequence xÏ„
krecurrently by letting xÏ„
0:=x0
and
xÏ„
k+1âˆˆarg min
xâˆˆRn
f(x) +âˆ¥xâˆ’xÏ„
kâˆ¥2
2Ï„
,âˆ€kâˆˆN.
A solution exists because fis bounded below and the objective function is coercive.
Any solution satisfies
vÏ„
k+1:=xÏ„
k+1âˆ’xÏ„
k
Ï„âˆˆ âˆ’âˆ‚f(xÏ„
k+1),âˆ€kâˆˆN.
25Define two functions xÏ„,ËœxÏ„:R+â†’Rnwhere R+:= [0,âˆ)by
xÏ„(t) :=xÏ„
k+1,ËœxÏ„(t) :=xÏ„
k+ (tâˆ’kÏ„)vÏ„
k+1,âˆ€tâˆˆ(kÏ„,(k+ 1)Ï„]
for all kâˆˆN, with initial condition xÏ„(0) = ËœxÏ„(0) = x0. Note that ËœxÏ„is absolutely
continuous because it is piecewise affine. On the contrary, xÏ„is not continuous. Also,
define vÏ„:R+â†’Rnby
vÏ„(t) :=vÏ„
k+1,âˆ€tâˆˆ(kÏ„,(k+ 1)Ï„],âˆ€kâˆˆN,
and choose vÏ„(0)âˆˆ âˆ’âˆ‚f(x0). Since (ËœxÏ„)â€²=vÏ„on(kÏ„,(k+ 1)Ï„)for all kâˆˆN, and
vÏ„(t)âˆˆ âˆ’âˆ‚f(xÏ„(t))for all tâ©¾0, we conclude that (ËœxÏ„)â€²(t)âˆˆ âˆ’âˆ‚f(xÏ„(t))for almost
every tâˆˆR+. By optimality of xÏ„
k+1, we have
f(xÏ„
k+1) +âˆ¥xÏ„
k+1âˆ’xÏ„
kâˆ¥2
2Ï„â©½f(xÏ„
k),âˆ€kâˆˆN.
For any lâˆˆN, we have
lX
k=0âˆ¥xÏ„
k+1âˆ’xÏ„
kâˆ¥2
2Ï„â©½f(xÏ„
0)âˆ’f(xÏ„
l+1)â©½f(x0)âˆ’inf
Rnf=:C <âˆ
since fis bounded below. Observe that
lX
k=0âˆ¥xÏ„
k+1âˆ’xÏ„
kâˆ¥2
2Ï„=lX
k=0Ï„
2âˆ¥vÏ„
k+1âˆ¥2=1
2lX
k=0Z(k+1)Ï„
kÏ„âˆ¥(ËœxÏ„)â€²(t)âˆ¥2dt.
FixTâ©¾0from now on. From the above, we have
ZT
0âˆ¥(ËœxÏ„)â€²(t)âˆ¥2dtâ©½âŒŠT/Ï„âŒ‹X
k=0Z(k+1)Ï„
kÏ„âˆ¥(ËœxÏ„)â€²(t)âˆ¥2dtâ©½2C. (17)
Since ËœxÏ„is absolutely continuous, for any s, tâˆˆ[0, T]we have
âˆ¥ËœxÏ„(t)âˆ’ËœxÏ„(s)âˆ¥=Zt
s(ËœxÏ„)â€²(u)du(18a)
â©½ZT
0âˆ¥(ËœxÏ„)â€²(t)âˆ¥2dt1/2
|tâˆ’s|1/2â©½âˆš
2C|tâˆ’s|1/2(18b)
where we use the Cauchy-Schwarz inequality. Now one can see (ËœxÏ„)Ï„>0is a family
of uniformly bounded and equicontinuous functions on the compact interval [0, T].
Therefore, by ArzelÃ -Ascoli theorem [ 40, Theorem 7.25], there exists a sequence of
positive reals (Ï„k)kâˆˆNsuch that Ï„kâ†’0andËœxÏ„kâ†’xâˆ—uniformly on [0, T]askâ†’ âˆ.
26For all kâˆˆNandtâˆˆ(kÏ„,(k+1)Ï„], we have ËœxÏ„((k+1)Ï„) =xÏ„
k+Ï„vÏ„
k+1=xÏ„
k+1=xÏ„(t).
Thusâˆ¥ËœxÏ„(t)âˆ’xÏ„(t)âˆ¥=âˆ¥ËœxÏ„(t)âˆ’ËœxÏ„((k+ 1)Ï„)âˆ¥â©½âˆš
2CÏ„1/2for all tâˆˆ[0, T]where the
inequality is due to (18) (take s:= (k+ 1)Ï„). Combined with the fact that ËœxÏ„kâ†’xâˆ—
uniformly on [0, T], one can see that xÏ„kâ†’xâˆ—uniformly on [0, T]. Since (17) implies
that ((ËœxÏ„k)â€²)kâˆˆNis a bounded sequence in L2([0, T],Rn), there exists a subsequence
(Ï„kj)jâˆˆNsuch that (ËœxÏ„kj)â€²â†’vâˆ—weakly in L1([0, T],Rn)asjâ†’ âˆby [21, Corollary
14 p. 413]. Since ËœxÏ„kjis absolutely continuous, for all tâˆˆ[0, T], we have
ËœxÏ„kj(t)âˆ’ËœxÏ„kj(0) =Zt
0(ËœxÏ„kj)â€²(u)du.
Take jâ†’ âˆon both sides, we have
xâˆ—(t)âˆ’xâˆ—(0) =Zt
0vâˆ—(u)du,
where the convergence of the integral relies on the fact that the constant functions
equal to the canonical basis of Rnlie in Lâˆ([0, T],Rn). Thus, xâˆ—is absolutely
continuous and (xâˆ—)â€²(t) =vâˆ—(t)for almost every tâˆˆ[0, T]. Recall that for all kâˆˆN,
it holds for almost every tâˆˆ[0, T]that
(ËœxÏ„k)â€²(t) =vÏ„k(t)âˆˆ âˆ’âˆ‚f(xÏ„k(t)).
Since fis locally Lipschitz, the set-valued function âˆ’âˆ‚fis upper semicontinuous [ 11,
2.1.5 Proposition (d) p. 29] with nonempty compact values [ 11, 2.1.2 Proposition
(a) p. 27], hence proper upper hemicontinuous [ 2, Proposition 1 p. 60]. In addition,
xÏ„kâ†’xâˆ—uniformly on [0, T]and(ËœxÏ„k)â€²â†’(xâˆ—)â€²weakly in L1([0, T],Rn). Therefore,
(xâˆ—)â€²(t)âˆˆ âˆ’âˆ‚f(xâˆ—(t))for almost all tâˆˆ[0, T]by [2, Theorem 1 p. 60]7. The initial
condition also holds since ËœxÏ„(0) = x0for all Ï„ >0.
We have proved that for any initial point x0, there exists xâˆ—: [0, T]â†’Rnsuch
that (xâˆ—)â€²(t) =âˆ’âˆ‚f(xâˆ—(t))holds for almost every tâˆˆ[0, T]with any T >0. Since
Tis independent of x0, by setting T= 1, there exists a sequence of absolutely
continuous functions (xk)kâˆˆNsuch that
xâ€²
k(t)âˆˆ âˆ’âˆ‚f(xk(t)),for a.e. tâˆˆ[0,1], x k(0) = xkâˆ’1(1),
for all kâˆˆNwhere xâˆ’1(0) = x0. Therefore, the desired function x: [0,âˆ)â†’Rn
can be defined in a piecewise fashion by
x(t) :=xk(tâˆ’k), tâˆˆ[k, k+ 1),âˆ€kâˆˆN.
By construction, xis absolutely continuous on any compact interval [a, b]âŠ‚[0,âˆ).
7In the theorem we take F:=âˆ’âˆ‚f,X=Y:=Rn, and I:= [0, T].
27References
[1]H. Attouch, G. Buttazzo, and G. Michaille. Variational analysis in Sobolev and
BV spaces: applications to PDEs and optimization . SIAM, Philadelphia, 2014.
[2]J.-P. Aubin and A. Cellina. Differential inclusions: set-valued maps and viability
theory, volume 264. Springer-Verlag, Berlin, 1984.
[3]B. Bah, H. Rauhut, U. Terstiege, and M. Westdickenberg. Learning deep
linear neural networks: Riemannian gradient flows and convergence to global
minimizers. Information and Inference: A Journal of the IMA , 11(1):307â€“353,
2022.
[4]K. L. Blackmore, R. C. Williamson, and I. M. Mareels. Local minima and
attractors at infinity for gradient descent learning algorithms. Journal of
Mathematical Systems Estimation and Control , 6:231â€“234, 1996.
[5]J. Bolte, A. Daniilidis, A. Lewis, and M. Shiota. Clarke subgradients of stratifi-
able functions. SIAM Journal on Optimization , 18(2):556â€“572, 2007.
[6]J. Bolte and E. Pauwels. Conservative set valued fields, automatic differentiation,
stochastic gradient methods and deep learning. Mathematical Programming ,
pages 1â€“33, 2020.
[7]J.BorweinandX.Wang. Lipschitzfunctionswithmaximalclarkesubdifferentials
are generic. Proceedings of the American Mathematical Society , 128(11):3221â€“
3229, 2000.
[8]V. Charisopoulos, Y. Chen, D. Davis, M. DÃ­az, L. Ding, and D. Drusvyatskiy.
Low-rank matrix recovery with composite optimization: good conditioning and
rapid convergence. Foundations of Computational Mathematics , pages 1â€“89,
2021.
[9]K. Chen, D. Lin, and Z. Zhang. On non-local convergence analysis of deep linear
networks. In International Conference on Machine Learning , pages 3417â€“3443.
PMLR, 2022.
[10]Y. Chi, Y. M. Lu, and Y. Chen. Nonconvex optimization meets low-rank
matrix factorization: An overview. IEEE Transactions on Signal Processing ,
67(20):5239â€“5269, 2019.
[11]F. H. Clarke. Optimization and Nonsmooth Analysis . SIAM Classics in Applied
Mathematics, Philadelphia, 1990.
[12]A. Daniilidis and D. Drusvyatskiy. Pathological subgradient dynamics. SIAM
Journal on Optimization , 30(2):1327â€“1338, 2020.
28[13]A. Daniilidis and G. Flores. Linear structure of functions with maximal clarke
subdifferential. SIAM Journal on Optimization , 29(1):511â€“521, 2019.
[14]D. Davis, D. Drusvyatskiy, S. Kakade, and J. D. Lee. Stochastic subgradient
method converges on tame functions. Foundations of computational mathematics ,
20(1):119â€“154, 2020.
[15]T. Ding, D. Li, and R. Sun. Suboptimal local minima exist for wide neural
networks with smooth activations. Mathematics of Operations Research , 2022.
[16]D. Drusvyatskiy, A. D. Ioffe, and A. S. Lewis. Curves of descent. SIAM Journal
on Control and Optimization , 53(1):114â€“138, 2015.
[17]S. S. Du, W. Hu, and J. D. Lee. Algorithmic regularization in learning deep
homogeneous models: Layers are automatically balanced. Advances in Neural
Information Processing Systems , 31, 2018.
[18]A. Eftekhari. Training linear neural networks: Non-local convergence and
complexity results. In International Conference on Machine Learning , pages
2836â€“2847. PMLR, 2020.
[19]I. Ekeland. On the variational principle. Journal of Mathematical Analysis and
Applications , 47(2):324â€“353, 1974.
[20]L. C. Evans and R. F. Garzepy. Measure theory and fine properties of functions .
Routledge, Oxfordshire, 2018.
[21]P. M. Fitzpatrick and H. L. Royden. Real Analysis . Pearson, Upper Saddle
River, NJ, 4 edition, Jan. 2010.
[22]C. D. Freeman and J. Bruna. Topology and geometry of half-rectified network
optimization. In International Conference on Learning Representations , 2017.
[23]N.GillisandS.A.Vavasis. Onthecomplexityofrobustpcaand â„“1-normlow-rank
matrix approximation. Mathematics of Operations Research , 43(4):1072â€“1084,
2018.
[24]T. X. D. Ha. The ekeland variational principle for set-valued maps involving
coderivatives. Journal of mathematical analysis and applications , 286(2):509â€“523,
2003.
[25]J.-B. Hiriart-Urruty. A short proof of the variational principle for approximate
solutions of a minimization problem. The American Mathematical Monthly ,
90(3):206â€“207, 1983.
29[26]C. Josz and L. Lai. Nonsmooth rank-one matrix factorization landscape. Opti-
mization Letters , pages 1â€“21, 2021.
[27]C. Josz, Y. Ouyang, R. Y. Zhang, J. Lavaei, and S. Sojoudi. A theory on
the absence of spurious solutions for nonconvex and nonsmooth optimization.
NeurIPS , Dec. 2018.
[28]K. Kawaguchi. Deep learning without poor local minima. In Advances in Neural
Information Processing Systems , volume 29. PMLR, 2016.
[29]T. Laurent and J. Brecht. Deep linear networks with arbitrary loss: All local
minima are global. In International conference on machine learning , pages
2902â€“2907. PMLR, 2018.
[30]J. D. Lee, M. Simchowitz, M. I. Jordan, and B. Recht. Gradient Descent Only
Converges to Minimizers. COLT, 2016.
[31]D. Li, T. Ding, and R. Sun. On the benefit of width for neural networks:
Disappearance of basins. SIAM Journal on Optimization , 32(3):1728â€“1758,
2022.
[32]S. Li, Q. Li, Z. Zhu, G. Tang, and M. B. Wakin. The global geometry of
centralized and distributed low-rank matrix recovery without regularization.
IEEE Signal Processing Letters , 27:1400â€“1404, 2020.
[33]S. Liang, R. Sun, J. D. Lee, and R. Srikant. Adding one neuron can eliminate
all bad local minima. Advances in Neural Information Processing Systems , 31,
2018.
[34]S. Marcellin and L. Thibault. Evolution problems associated with primal lower
nice functions. Journal of convex Analysis , 13(2):385, 2006.
[35] J. R. Munkres. Topology. Prenctice Hall, US , 2000.
[36]O. A. Nielsen. An introduction to integration and measure theory , volume 17.
Wiley-Interscience, New York, 1997.
[37]D. Park, A. Kyrillidis, C. Carmanis, and S. Sanghavi. Non-square matrix sensing
without spurious local minima via the burer-monteiro approach. In Artificial
Intelligence and Statistics , pages 65â€“74. PMLR, 2017.
[38]T. S. Pham and H. H. Vui. Genericity in polynomial optimization , volume 3.
World Scientific, London, 2016.
[39]B. Recht, M. Fazel, and P. A. Parrilo. Guaranteed minimum-rank solutions of
linear matrix equations via nuclear norm minimization. SIAM review , 52(3):471â€“
501, 2010.
30[40]W. Rudin et al. Principles of mathematical analysis , volume 3. McGraw-hill
New York, 1964.
[41]F. Santambrogio. {Euclidean, metric, and Wasserstein }gradient flows: an
overview. Bulletin of Mathematical Sciences , 7(1):87â€“154, 2017.
[42]J. Sohl-Dickstein and K. Kawaguchi. Eliminating all bad local minima from loss
landscapes without even adding an extra unit. arXiv preprint arXiv:1901.03909 ,
2019.
[43]L. Van den Dries. Tame topology and o-minimal structures , volume 248. Cam-
bridge university press, 1998.
[44]L. Venturi, A. S. Bandeira, and J. Bruna. Spurious valleys in one-hidden-layer
neural network optimization landscapes. Journal of Machine Learning Research ,
20:133, 2019.
[45]A. J. Wilkie. Model completeness results for expansions of the ordered field
of real numbers by restricted pfaffian functions and the exponential function.
Journal of the American Mathematical Society , 9(4):1051â€“1094, 1996.
[46]L. Zhang. Depth creates no more spurious local minima. arXiv preprint
arXiv:1901.09827 , 2019.
[47]Z. Zhu, Q. Li, G. Tang, and M. B. Wakin. Global optimality in low-rank matrix
optimization. IEEE Transactions on Signal Processing , 66(13):3614â€“3628, 2018.
31