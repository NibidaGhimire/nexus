Certifying the absence of spurious local minima at
infinity
Cédric Josz∗Xiaopeng Li†
Abstract
When searching for global optima of nonconvex unconstrained optimization
problems, it is desirable that every local minimum be a global minimum. This
property of having no spurious local minima is true in various problems of
interest nowadays, including principal component analysis, matrix sensing, and
linear neural networks. However, since these problems are non-coercive, they
may yet have spurious local minima at infinity. The classical tools used to
analyze the optimization landscape, namely the gradient and the Hessian, are
incapable of detecting spurious local minima at infinity. In this paper, we
identify conditions that certify the absence of spurious local minima at infinity,
one of which is having bounded subgradient trajectories. We check that they
hold in several applications of interest.
Keywords: global optimization, Morse-Sard theorem, subgradient trajectories.
1 Introduction
The idea that the absence of spurious local minima alone does not guarantee the
success of first-order methods was first expressed in the context of binary classification
in the mid-nineties. It was shown that gradient trajectories are bounded if the
objective function satisfies several technical conditions tailored to the problem at
hand [4, Theorems 3.6-3.8]. This property was referred to as having no attractors
at infinity. More recently, it was proved that adding an exponential neuron to
a wide class of neural networks eliminates all spurious local minima [ 33], but it
was soon realized that this procedure simply sends them to infinity [ 42]. These
results suggest that besides spurious local minima, a certain notion of spurious
∗cj2638@columbia.edu , IEOR, Columbia University, New York. Research supported by NSF
EPCN grant 2023032 and ONR grant N00014-21-1-2282.
†xl3040@columbia.edu , IEOR, Columbia University, New York.
1arXiv:2303.03536v2  [math.OC]  13 Jul 2023local minima at infinity also affects the convergence of first-order methods to global
optima. However, the current optimization literature lacks a precise definition of
local minima at infinity, and, accordingly, there is little theoretical understanding of
them. Worse still, classical tools for landscape analysis, such as the gradient and
the Hessian, cannot detect spurious local minima at infinity even in simple scenarios
(see Example 1), let alone handle nonsmooth functions without a gradient.
Example 1. Consider an instance of matrix completion problem, i.e., minimize
f(x1, x2, y1, y2) := ( x1y1−1)2+ (x2y1−1)2+ (x2y2−1)2.
By solving ∇f(x1, x2, y1, y2) = 0, the set of critical points of fcan be decomposed
into four connected components:
C1={(x1, x2, y1, y2) = (t, t,1/t,1/t)|t∈R\ {0}},
C2={(x1, x2, y1, y2) = (t,0,1/t,−1/t)|t∈R\ {0}},
C3={(x1, x2, y1, y2) = (t,−t,0,−1/t)|t∈R\ {0}},
C4={(x1, x2, y1, y2) = (0 ,0,0,0)}.
The critical values are f(C1) ={0},f(C2) = f(C3) ={2}, and f(C4) ={3}.
Furthermore, C1is the set of global minima, and by computing the Hessian ∇2f,
we find that it has positive and negative eigenvalues at all points in C2,C3and
C4. Therefore, fhas no spurious local minima and all saddle points are strict [ 30,
Definition 2]. One would expect first-order methods like gradient descent to converge
to a global minimum for almost all initial points [ 30, Theorem 11]. However, the
numerical experiments in Figure 1 show otherwise. This is because the function is
not coercive.
Two newly proposed concepts related to spurious local minima at infinity are
setwise local minima [ 27] and spurious valleys [ 44]. Setwise local minima [ 27, Defini-
tion 2.5] generalize the notion of local minima from points to compact sets. The first
author and co-authors recently established that the uniform limit (on all compact
subsets) of a sequence of continuous functions which are devoid of spurious setwise
local minima is itself devoid of spurious strict setwise local minima [ 27, Proposition
2.7]. However, due to the boundedness assumption, setwise local minima cannot
be directly used to study spurious local minima at infinity. Spurious valleys [ 44,
Definition 1] do have the potential to handle spurious local minima at infinity but
they fail to detect them when there are flat regions, such as in the ReLU network
with one-hidden layer (x1, x2)7→(x2max{x1,0}−1)2(see Figure 2). Spurious valleys
also rely on the notion of path-connectedness, which is actually not necessary for
defining spurious local minimum at infinity. In this paper, we extend the concept
of setwise local minima by relaxing the boundedness assumption. This enables us
to define spurious local minima at infinity as unbounded setwise local minima over
2Figure 1: Gradient method initialized uniformly at random in [−1,1]4with constant
step size 0.01sometimes gets stuck at a spurious local minimum at infinity (3 among
10 trails in the experiment).
which the infimum of the objective function is greater than the global infimum. It
also allows us to handle classical spurious local minima and flat regions in a unified
way.
An existing strategy to analyze the landscape of non-coercive functions is to
construct a strictly decreasing path to a global minimum from any initial point. Such
a path was shown to exist in half-rectified neural network [ 22]. This strategy is used
to prove the existence of spurious local minima in neural networks with almost all
nonlinear activations [ 15]. It also explains the phase transition from the existence
of sub-optimal basins in narrow networks to their disappearance in wide networks
[31]. Finally, it is used to prove the absence of spurious valleys for over-parametrized
one-hidden layer neural network [ 44]. However, such a strategy needs to be tailored
to each application since one needs to select a particular path for each specific loss
function. In this paper, we instead develop a theory allowing one to use a common
decreasing path - subgradient trajectory - to analyze the landscape in various different
contexts. We can then rule out spurious setwise local minima (and thus those at
infinity) for a general class of functions. Our main result is as follows.
Theorem 1. Suppose a locally Lipschitz function is bounded below, admits a chain
rule, has finitely many critical values, and has bounded subgradient trajectories. Then
it has no spurious local minima if and only if it has no spurious setwise local minima.
The above statement is meant to help readers get a first taste of our main result
in this paper. Precise definitions and a detailed mathematical background will be
3Figure 2: Function devoid of spurious valleys containing a spurious local minimum
at infinity.
given in the main body1, along with a discussion on the role of its assumptions. Let
us mention already that two of its assumptions, namely those regarding the chain
rule and critical values, automatically hold for functions definable in an o-minimal
expansion of the real field [ 43] (by [6, Proposition 2 (iv)] and the definable Morse-Sard
theorem [ 5, Corollary 9 (ii)]). This includes semi-algebraic, globally subanalytic,
and log-exp functions, and importantly, many applications of interest nowadays [ 6,
Section 4.1]. The locally Lipschitz and lower bounded assumptions usually come
for free in applications, so that in practice the sole assumption that one needs to
check for is that subgradient trajectories are bounded. Theorem 1 thus serves as
a handy device to conclude that there are no spurious setwise local minima for a
family of functions that are widely used in machine learning, especially in deep
neural networks and matrix sensing. We summarize the problems that we are going
to consider in the following corollary.
Corollary 1. The following problems have no spurious local minima at infinity:
1. deep linear neural network
inf
W1,...,W L∥WL···W1X−Y∥2
F;
2. one dimensional deep neural network with sigmoid activation function σ
inf
w1,...,w L(wLσ(wL−1···σ(w1x))−y)2;
1The terminology in the theorem will be given in Definition 4 (locally Lipschitz), Definition 8
(chain rule), Definition 7 (bounded subgradient trajectories), and Definition 2 (setwise local
minimum).
43. matrix recovery with restricted isometry property (RIP)
inf
X,Y1
2mmX
i=1(⟨Ai, XYT⟩F−bi)2;
4. nonsmooth matrix factorization where rank( M) = 1andMij̸= 0
inf
x,ymX
i=1nX
j=1|xiyj−Mij|.
Again, the statement above aims at giving readers some feeling on what type of
functions we are considering. More rigorous descriptions of the applications will be
given in the main body.
The paper is organized as follows. Section 2 contains background material
on setwise local minima, the Clarke subdifferential, and subgradient trajectories.
Section 3 contains the proof of our main result, namely Theorem 1. Finally, Section 4
contains applications of our main result as delineated in Corollary 1.
2 Background
This section contains prerequisites for the proof of Theorem 1 in the next section.
Throughout this paper, unless otherwise specified, we always equip the Euclidean
spaceRnwith an inner product ⟨·,·⟩and its induced norm ∥ · ∥:=p
⟨·,·⟩.
2.1 Setwise local minimum
In this subsection, we present the formal definitions and some useful properties of
setwise local minimum and local minimum at infinity mentioned in Section 1. We
first review the classical definition of local and global minima. Throughout this
paper, B(x, ϵ)denotes the open ball centered at x∈Rnwith radius ϵ >0.
Definition 1. A point x∈Rnis alocal minimum (respectively, global minimum ) of
a function f:Rn→Riff(x)⩽f(y)for all y∈B(x, ϵ)for some ϵ >0(respectively,
y∈Rn). A local minimum is spurious if it is not a global minimum.
From Definition 1, one can see the definition of a local minimum only considers
the landscape of a function at any finite point. To discuss the function landscape
at infinity, we generalize the notion of setwise local minimum first proposed in [ 27,
Definition 2.5].
Definition 2 (Setwise local minimum) .A nonempty closed subset S⊂Rnis a
setwise local minimum of a continuous function f:Rn→Rif there exists an open
setU⊂Rnsuch that S⊂Uandf(x)⩽f(y)for all x∈S,y∈U\S.
5It is easy to see that a local minimum is a setwise local minimum by taking
Sto be a singleton. We also define a strict setwise local minimum by replacing
f(x)⩽f(y)with f(x)< f(y)in Definition 2.
Definition 3 (Setwise global minimum) .A subset SofRnis asetwise global
minimum of a function f:Rn→RifSis a setwise local minimum of fand
infSf= inf Rnf.
Note that infSfis a shorthand for infx∈Sf(x), and similar for supSfandmax Sf.
Setwise local minima include setwise global minima as a special case, and we say a
setwise local minimum is spurious if it is not a setwise global minimum. Note that
Definition 2 is not exactly the same as [ 27, Definition 2.5] because we do not require a
setwise local minimum to be a compact set. In other words, a setwise local minimum
can be either bounded or unbounded, and we say a (spurious) setwise local minimum
is a (spurious) local minimum at infinity if it is unbounded. For example, consider
the loss function of a one-hidden layer neural network with sigmoid activation σand
two data points (1,1)and(−1,−3)in Figure 3. One can see that Sis a setwise local
minimum (in particular, a local minimum at infinity) and Uis the corresponding
open set in Definition 2. Finally, observe that Rnis a strict setwise local minimum
at infinity of any function.
Figure 3: Local minimum at infinity of f(w1, w2) =1
2[(w2σ(w1)−1)2+ (w2σ(−w1) +
3)2].
Now we introduce one of the most useful properties of setwise local minima in
Lemma 1. This property is intuitive and will be used in different scenarios throughout
this paper. Let ¯S,S◦, and ∂S:=¯S\S◦respectively denote the closure, interior, and
boundary of a subset SofRn.
Lemma 1. IfSis a setwise local minimum of a continuous function f:Rn→R,
then f(x) = supSffor all x∈∂S.
6Proof.See Appendix A.
It is worth relating our notion of setwise local minimum to the concept of valley
proposed in [ 44, Definition 1]. A valleyof a function f:Rn→Ris defined as a
path-connected component2of a sublevel set of f. These two definitions are distinct
in general. The interval [−1,1]is a setwise local minimum of f:R→Rdefined
byf(x) := 0for all x∈Ryet it is not a valley. Conversely, X:={(x1, x2)∈
(R\ {0})×R|x2=sin(1/x1)}is a valley of f:R2→Rwhere fis defined as
the distance between xandX, yet it is not a setwise local minimum since it is not
closed. (The sublevel set of fcorresponding to the value zero is composed of two
path-connected components, namely Xand{0} ×[−1,1], whose union is X.) Under
some mild conditions, their relation can be summarized in proposition 1.
Proposition 1. For a continuous function from RntoR,
(a)a path-connected component of a strict setwise local minimum (respectively,
setwise local minimum) is a valley (respectively, subset of a valley);
(b)a connected component3of a sublevel set which has finitely many connected
components is a strict setwise local minimum.
Proof.See Appendix B.
Remark 1. The assumption on finiteness of connected components is necessary, or
else counterexample may occur when the function is oscillatory. For example,
f(x) :=(
0 if x⩽0,
x2sin1
xifx >0.
The function fis continuous on R, but the sublevel set {x∈R|f(x)⩽0}has
infinitely many connected components. Take a connected component C1= (−∞,0]
(also path-connected, thus a valley), and it is not a setwise local minimum because
for any open set Ucontaining C1, there exists some x0∈Nsuch that f(x0)<0.
Finally, we discuss the case of coercive functions. Recall that a function f:Rn→
Ris coercive if f(x)→ ∞as∥x∥ → ∞.
Proposition 2. If a continuous function from RntoRis coercive, then it has no
spurious local minima at infinity.
2A subset SofRnis path-connected if for all x, y∈S, there exists a continuous function
γ: [0,1]→Ssuch that γ(0) = xandγ(1) = y. A maximal path-connected connected set is called
a path-connected component. Path-connected components can be viewed as equivalence classes
over a set.
3A subset SofRnis disconnected if there exist nonempty disjoint open (in S) sets AandB
such that S=A∪B. It is connected if it is not disconnected. A maximal connected set is called a
connected component.
7Proof.See Appendix C.
In many statistical learning problems, the loss functions without regularizer are
usually not coercive, so spurious local minima at infinity may exist. Therefore, it
is important to develop some device to check whether spurious local minima exist
or not so that optimization algorithms can be designed to avoid getting trapped in
them.
2.2 Clarke subdifferential
In this subsection, we will review some concepts and results on generalized derivative
in the sense of Clarke [ 11, p. 27], since Theorem 1 also considers nonsmooth functions.
Definition 4. A function f:Rn→Rmislocally Lipschitz if for all a∈Rn, there
exist positive constants randLsuch that
∀x, y∈B(a, r),∥f(x)−f(y)∥⩽L∥x−y∥.
Notice that for a locally Lipschitz function, by [ 20, Theorem 3.2], the derivative
exists almost everywhere. Without any assumption on convexity, in order to ensure
the existence of a subdifferential, we adopt the notion of Clarke subdifferential.
Definition 5. [11, p. 27] Let f:Rn→Rbe a locally Lipschitz function. The
Clarke subdifferential is the set-valued mapping ∂ffromRnto the subsets of Rn
defined for all x∈Rnby
∂f(x) :={s∈Rn|f◦(x, d)⩾⟨s, d⟩,∀d∈Rn}
where
f◦(x, d) := lim sup
y→x
t↘0f(y+td)−f(y)
t.
It is well known that for any locally Lipschitz function fand any x∈Rn, the
Clarke subdifferential ∂f(x)is a nonempty, convex, and compact set [ 11, Proposition
2.1.2(a)]. Similar to differentiable functions, a point x∈Rnis called (Clarke) critical
pointif0∈∂f(x). A real number yis called a (Clarke) critical value offif there
exists a (Clarke) critical point x∈Rnoffsuch that f(x) =y.
2.3 Subgradient trajectory
In this subsection, we will introduce some basic concepts and fundamental properties
related to subgradient trajectories.
8Definition 6. [2, Definition 1 p. 12] Given two real numbers a < b, a function
x: [a, b]→Rnisabsolutely continuous if for all ϵ >0, there exists δ >0such that,
for any finite collection of disjoint subintervals [a1, b1], . . . , [am, bm]of[a, b]such thatPm
i=1bi−ai⩽δ, we havePm
i=1∥x(bi)−x(ai)∥⩽ϵ.
By virtue of [ 36, Theorem 20.8], x: [a, b]→Rnis absolutely continuous if and
only if it is differentiable almost everywhere on (a, b), its derivative x′is Lebesgue
integrable, and x(t)−x(a) =Rt
ax′(τ)dτfor all t∈[a, b]. Given a non-compact
interval IofR,x:I→Rnis absolutely continuous if it is absolutely continuous on
all compact subintervals of I.
An absolutely continuous function x: [0,∞)→Rnis called a subgradient
trajectory off:Rn→Rstarting at x0∈Rnif it satisfies the following differential
inclusion with initial condition:
x′(t)∈ −∂f(x(t)),for almost every t⩾0, x(0) = x0, (1)
where “almost every” means all elements except for those in a set of zero measure.
However, a subgradient trajectory may not always exist for arbitrary f, even if f
is a smooth function. Let f(x) =−1
3x3andx0= 1, then it is easy to see x(t) =1
1−t
is the unique solution for t∈[0,1)and it cannot be extended to an absolutely
continuous function on [0,∞)due to the singularity at t= 1. In this case, one would
seek a family of functions including many loss functions arising in applications that
guarantee the existence of a subgradient trajectory. We say a function f:Rn→R
isbounded below ifinfRnf=c >−∞. It was shown in [ 34, Theorem 3.2] that a
primal lower nice function bounded below by a linear function suffices. However,
in general it is not easy to check whether those nonconvex functions in statistical
learning problems are primal lower nice. For easily checkable conditions, the following
result generalized from [ 41, Proposition 2.3] for differentiable functions tells us that
a locally Lipschitz function bounded below also suffices.
Proposition 3. Iff:Rn→Ris locally Lipschitz and bounded below, then there
exists a subgradient trajectory of fstarting at arbitrary x0∈Rn.
Proof.See Appendix D.
We remark here that with Proposition 3, one can recover Ekeland’s variational
principle [ 19, Corollary 2.3] [ 25, Corollary] for locally Lipschitz lower bounded
functions with a chain rule (see [ 24, Theorem 3.1] for an extension to lower semi-
continuous lower bounded functions). Indeed, Proposition 3 implies that for all ϵ >0,
there exists (x, s)∈graph ∂fsuch that f(x)⩽inff+ϵand∥s∥⩽ϵ.4Note that
Proposition 3 only guarantees the existence of a solution to (1) for all t⩾0, but the
solution x(t)could go to infinity as t→ ∞. This motivates the following definition.
4This follows from the formula f(x(t))−inff⩾R∞
td(0, ∂f(x(τ)))2dτwhere d(x, X) :=
infy∈X∥x−y∥(see [14, Lemma 5.2] and [16, Proposition 4.10]).
9Definition 7. A locally Lipschitz lower bounded function f:Rn→Rhasbounded
subgradient trajectories if for any x0∈Rn, there exists a subgradient trajectory xof
fstarting at x0and a constant r >0, such that ∥x(t)∥⩽rfor all t⩾0.
Finally, notice that when fis continuously differentiable, by [ 11, Proposition
2.2.4], (1) reduces to the classical Cauchy problem of differential equation
x′(t) =−∇f(x(t)),for all t⩾0,x(0) = x0.
and subgradient trajectory reduces to gradient trajectory by imposing xto be
continuously differentiable. Recall the descent property of gradient trajectories [ 1,
Proposition 17.1.1], i.e., f◦xis a decreasing function for any gradient trajectory x
off. We want this nice property to hold even in the general case when fis only
locally Lipschitz. We adopt the notion of chain rule in [ 14, Definition 5.1]. Note
that functions admitting a chain rule are also referred to as path differentiable [ 6,
Definition 3].
Definition 8. Letf:Rn→Rbe locally Lipschitz. We say fadmits a chain rule if
for any absolutely continuous function x: [0,∞)→Rn, we have
(f◦x)′(t) =⟨v, x′(t)⟩,∀v∈∂f(x(t)),
for almost every t∈[0,∞).
Thus, for any locally Lipschitz function that admits a chain rule, by [ 14, Lemma
5.2], the function value is always decreasing in time along the subgradient trajectory.
A detailed discussion on what class of functions admits a chain rule can be found in
[6]. Note that general Lipschitz functions are far from admitting a chain rule since
they generically have a maximal Clarke subdifferential [12, 7, 13].
3 Proof of Theorem 1
This section contains the proof of the main result, i.e., Theorem 1. After the proof,
we will explain the necessity of the assumptions in Theorem 1 by raising some
counterexamples. For emphasis, we summarize all assumptions in Theorem 1 below.
Assumption 1. Letf:Rn→Rbe a function such that it
(a) is bounded below, namely, infRnf >−∞;
(b) is locally Lipschitz continuous on Rn; see Definition 4;
(c) admits a chain rule; see Definition 8;
(d) has finitely many critical values; see Section 2.2;
10(e) has bounded subgradient trajectories; see Definition 7.
Proof of Theorem 1. Letf:Rn→Rbe a function satisfying assumption 1 If fhas
no spurious setwise local minima, then fhas no spurious local minima. We next
prove the converse. Let S⊂Rnbe a setwise local minimum of f. We seek to show
thatSis a setwise global minimum of f. IfS◦=∅, then by Lemma 1 f(x) =supSf
for all x∈∂S=¯S\S◦=Ssince Sis closed by Definition 2. Thus fis constant on
S. By definition of setwise local minima (Definition 2), there exists an open subset
UofRncontaining Ssuch that the constant value of fonSis less than or equal to
f(y)for all y∈U\S. Thus every point in Sis a local minimum of f. Since every
local minimum of fis a global minimum, infSf=infRnfandSis a setwise global
minimum according to Definition 3. The rest of the proof deals with the case when
S◦̸=∅. Let Cbe the set of all critical points of finSand consider the following
optimization problem:
inf
x∈Cf(x). (2)
We claim that the set of (global) solutions to (2) is nonempty, and that any solution
is a local minimum of fbelonging to the setwise local minimum S. We first show
that the feasible set of (2) is nonempty.
Since S◦̸=∅, let x0∈S◦. Ifx0∈C, then the feasible set Cis nonempty.
We thus assume that x0/∈C. Since fis locally Lipschitz and bounded below, by
Proposition 3 there exists a subgradient trajectory x: [0,∞)→Rnstarting at x0.
We next show that x([0,∞))⊂S. We reason by contradiction and assume that
Sc∩x([0,∞))̸=∅, where Scis the complement of SinRn. Then S◦andScare
disjoint open subsets of Rnsuch that S◦∩x([0,∞))̸=∅(the intersection contains
x0),Sc∩x([0,∞))̸=∅, and x([0,∞)) = ( x([0,∞))∩S◦)∪(x([0,∞))∩Sc)⊂Rn\∂S
(since5f(x(t))< f(x(0)) = f(x0)⩽f(x)for all t >0andx∈∂S, where the last
inequality follows from Lemma 1). Thus the connected set x([0,∞))is the union of
two relatively open disjoint nonempty sets, which is a contradiction.
Since fhas bounded subgradient trajectories and x(·)is an arbitrary subgradient
trajectory starting at x0, by Definition 7 and without loss of generality there exists
r >0such that ∥x(t)∥⩽rfor all t⩾0. We next show that there exists a critical
point of finB(0, r)∩S. Suppose that there exist two constants T, ϵ > 0for which
∥x′(t)∥⩾ϵfor all t⩾Tsuch that x′(t)∈ −∂f(x(t)). By [14, Lemma 5.2], we
have (f◦x)′(t) =−∥x′(t)∥2⩽−ϵ2for almost every t⩾T. By integrating, we
getf(x(t))−f(x(T))⩽−ϵ2tand thus f(x(t))converges to −∞ast→ ∞. This
is impossible since x(t)∈B(0, r)andfis continuous. Hence there exists a time
sequence tk→ ∞such that ∥x′(tk)∥ → 0ask→ ∞andx′(tk)∈ −∂f(x(tk))for
allk∈N:={0,1,2, . . .}. By the Bolzano–Weierstrass theorem, there exists a
5If there exists t >0such that f(x(t)) =f(x(0)), then f(x(t))−f(x(0)) =Rt
0∥x′(s)∥2ds= 0
andx′(s) = 0for almost every s∈(0, t). Since x′(s)∈ −∂f(x(s))for almost every s >0, by [11,
2.1.5 Proposition (b) p. 29] we have 0∈∂f(x(0)).
11subsequence x(tkj)ofx(tk)such that x(tkj)→˜x∈Rnasj→ ∞. Since x′(tkj)∈
−∂f(x(tkj)), by [11, 2.1.5 Proposition (b) p. 29] we have 0∈ −∂f(˜x). Finally, since
x([0,∞))⊂SandSis closed, we have ˜x∈C. We obtain that C̸=∅as desired.
Since fhas finitely many critical values and C̸=∅, the set of solutions to (2)
is nonempty. Let x∗∈Cbe a solution, that is to say f(x∗) =min Cf. Recall
that Cis a subset of the setwise local minimum S. Ifx∗is a local minimum of f,
then it is a global minimum of finSsince every local minimum of fis a global
minimum. Thus infSf=infRnfandSis a setwise global minimum. For the
remainder of the proof, we consider the case where x∗is not a local minimum and
show that this leads to a contradiction. We first show that there exists s0∈S◦such
that f(s0)< f(x∗). This is clearly true if x∗∈S◦since one can then find a ball
centered at x∗inside S◦. Ifx∗∈S\S◦=∂S, then we reason by contradiction and
assume that f(x)⩾f(x∗)for all x∈S◦. By Lemma 1, we have f(x) =f(x∗) =
supSf⩾f(y)⩾f(x∗)for all (x, y)∈(S\S◦)×S◦. Hence f(x∗) =f(x)for all
x∈S. Since Sis a setwise local minimum, there exists an open set Usuch that
f(x)⩾f(x∗)holds for all x∈U\S. Thus f(x)⩾f(x∗)for all x∈Uandx∗
is a local minimum. This yields a contradiction. Hence let s0∈S◦be such that
f(s0)< f(x∗). The nonempty closed set S′:=S∩[f⩽(f(s0) +f(x∗))/2]is a
setwise local minimum of fwhere [f⩽α] :={x∈Rn|f(x)⩽α}. Indeed, for
allx∈S′andy∈U\S′= (U\S)∪(U\[f⩽(f(s0) +f(x∗))/2]), we have6
f(x)⩽(f(s0) +f(x∗))/2⩽f(y). Since s0∈S◦andf(s0)<(f(s0) +f(x∗))/2, we
have s0∈S◦∩[f⩽(f(s0) +f(x∗))/2]◦= (S∩[f⩽(f(s0) +f(x∗))/2]))◦= (S′)◦.
Hence the setwise local minimum S′has nonempty interior. Also, S′⊂Sand
supS′f < f (x∗) =min Cfwhere we remind the reader that Cis the set of critical
points in S. Thus S′is devoid of critical points. However, by the previous paragraph,
setwise local minima of fwith nonempty interior must contain a critical point. This
yields a contradiction.
Remark 2 (Finitely many critical values) .This assumption is not intuitive and we
explain why it is necessary by the following example. Define f:R→Ras
f(x) :=

(x+ 4)2−8 ifx⩽−2;
−x2ifx∈[−2,0];
−2−k(x−2k)2k+1−3(1−2−k)ifx∈[2k,2k+ 1], k∈N;
2−k(x−2k)2k+1−3(1−2−k)ifx∈[2k−1,2k], k∈N+.
where N+is the set of all positive integers. To be more intuitive, we give the plot of
fon[−7,7]in Figure 4.
By standard calculus, one can see fis continuously differentiable, f(x)→ −3
asx→ ∞, and f(x)⩾−8overR. Furthermore, {−4} ∪ { 2k}k∈Nare all critical
6Indeed, for any sets A, B, and Cit holds that A\(B∩C) = (A\B)∪(A\C).
12Figure 4: An example of function with infinitely many critical values
points of f, with critical values {−8} ∪ {− 3(1−2−k)}k∈Nrespectively. Finally, the
subgradient trajectory of fstarting at x0<0will converge to the critical point
x=−4; the one starting at x0= 0will stay at the critical point x= 0; and the
one starting at x0>0such that 2k < x 0⩽2k+ 2will converge to x= 2k+ 2, for
allk∈N. This shows fhas bounded subgradient trajectories. Thus, fsatisfies all
conditions in assumption 1 except the finiteness of critical values. It is also easy
to see fhas no spurious local minima because all of its critical points are either
global minimum ( x=−4), or local maximum ( x= 0), or saddle points. However,
for any a >0, the set [a,∞)is a spurious local minima at infinity. This shows that
Theorem 1 may not hold for functions with infinitely many critical values.
Remark 3 (Bounded subgradient trajectories) .This is the main assumption of
Theorem 1. Without it, one could easily think of a smooth function without any
spurious local minimum, yet has spurious local minimum at infinity. This is the case
of the function in Figure 3 in which the yellow curve corresponds to an unbounded
gradient trajectory. In order to prove the necessity of the boundedness assumption,
it suffices to consider the univariate function fdefined in [ 27, Figure 4(a)] defined by
f(x) :=x2(1 +x2)
1 +x4, f′(x) =−2x(x4−2x2−1)
(x4+ 1)2.
By solving f′(x) = 0, we know that fhas three critical points, among which x= 0
is the global minimum and x=±(√
2−1)−1/2are two global maxima. Thus, fis
bounded below, continuously differentiable (hence locally Lipschitz and admits a
chain rule), has finitely many critical values, and has no spurious local minima. Since
fis strictly decreasing for all x⩾(√
2−1)−1/2≈1.55andf(x)→1asx→ ∞,
one can easily see [2,∞)is a spurious local minimum at infinity. This shows that
13Theorem 1 does not hold and the reason is that fdoes not have bounded subgradient
trajectories. To see this explicitly, consider the Cauchy problem
˙x=2x(x4−2x2−1)
(x4+ 1)2, x(0) = 2 .
By using separation of variables, the unique solution x(t)is given by
c+ 2t=1
4x4+x2+ (2 +√
2) log( x2−√
2−1)
+ (2−√
2) log( x2+√
2−1)−logx=:g(x),
where cis a constant determined by x(0) = 2. It is easy to see that xis strictly
increasing so x(t)⩾2for all t∈[0,∞). Note that gis continuous on [2,∞), so if xis
bounded, then g◦xis bounded. This contradicts the fact that g(x(t)) = 2 t+c→ ∞
ast→ ∞, and thus fhas an unbounded subgradient trajectory.
4 Applications
In this section, we use Theorem 1 to analyze the landscape of some widely used loss
functions in unconstrained optimization. To be more specific, we will consider deep
linear neural network, one dimensional deep sigmoid neural network, matrix sensing,
and nonsmooth matrix factorization in the following four subsections respectively.
4.1 Deep linear neural network
As a prototypical example in deep learning, the landscape of deep linear neural
network has been widely studied; see for example [ 28,29,44]. Consider minimizing
the loss function of linear neural network without bias term
f(W1, . . . , W L) :=1
2∥WL···W1X−Y∥2
F, (3)
where X∈Rd0×dx,Y∈RdL×dx, and Wi∈Rdi×di−1fori= 1, . . . , L. Here ∥ · ∥ F
denotes the Frobenius norm. It was recently established that fhas no spurious
valleys [44, Theorem 11], however this fact alone does not imply the absence of
spurious local minima at infinity (recall Figure 2). Together with the fact that fhas
no spurious local minima [ 46, Corollary 1] and that fis semi-algebraic, it can be
deduced that fhas no spurious setwise local minima (and thus no spurious local
minima at infinity).
The proof of the absence of spurious valleys [ 44, Theorem 11] is tailored to
the problem at hand. Using linear algebra, it argues that from any initial point
one can construct a piecewise linear path to a global minimum along which the
14objective function is non-increasing. The proof spans multiple pages and requires
several technical lemmas. The proof that we propose is shorter and follows a
general principle, namely Theorem 1, that applies to various problems as the next
subsections will show. The first four assumptions of Theorem 1 are easy to verify: f
is nonnegative, hence bounded below; fis continuously differentiable, hence locally
Lipschitz and admits a chain rule; fis semi-algebraic, by [ 38, Corollary 1.1], it
has finitely many critical values. Thus, it suffices to show fhas bounded gradient
trajectories.
Proposition 4. Linear neural network with loss function (3)has bounded gradient
trajectories.
An existing proof of Proposition 4 under additional assumptions on network
structure, initialization, input data, or target data can be found, for instance, in
[3,18,9]. To the best of our knowledge, the closest result to Proposition 4 is [ 3,
Theorem 3.2], which shows that gradient trajectories are bounded if XXTis of full
rank. In the proof of Proposition 4, we show that this rank assumption on Xcan be
removed and hence Proposition 4 applies to any linear neural network.
Proof of Proposition 4. Since fis locally Lipschitz and lower bounded, by
Proposition 3 there exists a gradient trajectory for any initial point. By [ 3, Lemma
2.1], the gradient trajectories of fsatisfy the initial value problem
˙Wi=−(WL···Wi+1)T(WL···W1X−Y)(Wi−1···W1X)T, (4a)
Wi(0) = W0
i, W0
i∈Rdi×di−1is a given constant matrix , (4b)
for all i= 1, . . . , L. Note that if i=L, (4a) reduces to
˙WL=−(WL···W1X−Y)(WL−1···W1X)T,
and if i= 1, (4a) reduces to
˙W1=−(WL···W2)T(WL···W1X−Y)XT.
Note that [ 3, Theorem 3.2] proved the boundedness of gradient trajectories of
fwhen XXTis invertible. Thus, we only need to show we can always reduce the
boundedness of gradient trajectories of ffor general Xto the boundedness of gradient
trajectories of another function gin the same form as fbut with invertible XXT. Let
X=UΣVTbe a singular value decomposition, where U∈Rd0×d0andV∈Rdx×dx
are orthogonal matrices, and Σ∈Rd0×dxis a rectangular matrix satisfying
Σ =Λ 0
0 0
,Λ = diag( λ1, . . . , λ r)≻0,
15where r⩽min{d0, dx}. Eliminating Xin (4a), it reduces to
˙Wi=−(WL···Wi+1)T(WL···W1UΣVT−Y)(Wi−1···W1UΣVT)T
=−(WL···Wi+1)T(WL···W1UΣ−Y V)(Wi−1···W1UΣ)T.
Define Z:=Y V∈RdL×dx, and (4) reduces to
˙Wi=−(WL···Wi+1)T(WL···W1UΣ−Z)(Wi−1···W1UΣ)T,(5a)
Wi(0) = W0
i,∀i= 1, . . . L. (5b)
Denote W1:=W1U∈Rd1×d0andW0
1:=W0
1U∈Rd1×d0. To keep the notation
consistent, also let Wi:=WiandW0
i:=W0
ifori= 2, . . . , L. Thus, (5) reduces to
˙Wi=−(WL···Wi+1)T(WL···W1Σ−Z)(Wi−1···W1Σ)T,(6a)
Wi(0) = W0
i,∀i= 1, . . . L. (6b)
Partition the matrices W1,W0
1, and Zinto two column blocks:
W1=
W11W12
,W0
1=h
W0
11W0
12i
, Z =
Z1Z2
,
where W11,W0
11, and Z1consist of the first rcolumns of W1,W0
1andZrespectively.
Thus, when i= 1, (6) can be reduced into
˙W11=−(WL···W2)T(WL···W2W11Λ−Z1)ΛT,˙W12= 0,
W11(0) = W0
11,W12(0) = W0
12.
When i= 2, . . . , L, (6) can be reduced into
˙Wi=−(WL···Wi+1)T(WL···W2W11Λ−Z1)(Wi−1···W2W11Λ)T,
Wi(0) = W0
i.
It indicates that W12(t) =W0
12for all t⩾0. Denote fW1:=W11andfW0
1:=W0
11.
To keep the notation consistent, also let fWi:=WiandfW0
i:=W0
ifori= 2, . . . , L.
Therefore, (6) reduces to
˙fWi=−(fWL···fWi+1)T(fWL···fW1Λ−Z1)(fWi−1···fW1Λ)T, (7a)
fWi(0) =fW0
i,∀i= 1, . . . L. (7b)
Define the new function gas
g(fW1, . . . ,fWL) :=1
2∥fWL···fW1Λ−Z1∥2
F.
16Notice that the gradient trajectories of gsatisfy (7). To prove fhas bounded gradient
trajectories, it is equivalent to prove ghas bounded gradient trajectories, because
∥W1∥F=∥W1U∥F=∥W1∥Fand∥W1(t)∥2
F=∥fW1(t)∥2
F+∥W0
12∥2
Ffor all t⩾0.
Since ΛΛTis invertible, by [ 3, Theorem 3.2], ghas bounded gradient trajectories,
and so does f.
With Proposition 4, we verified that fsatisfies assumption 1. Thus, (3) has no
spurious setwise local minima if and only if it has no spurious local minima. Since
a local minimum at infinity is an unbounded setwise local minimum, and fhas no
spurious local minima, we conclude that fhas no spurious local minima at infinity.
This proves the first result in Corollary 1.
4.2 One dimensional deep sigmoid neural network
Though famous for its benign theoretical properties, linear neural network is rarely
used in practice because of its low representation power. We want to take a step
further in the case of nonlinear deep neural network. In this subsection, we focus on
neural network with sigmoid activation function in one dimensional case. Landscape
analysis of one or two-hidden layer sigmoid neural network can be found, for instance,
in [44,15,31]. However, none of the results above can be easily generalized to
arbitrary many layers.
Consider minimizing the following loss function of sigmoid neural network
f(w1, . . . , w L) :=1
2(wLσ(wL−1···σ(w1x))−y)2, (8)
where σ(z) := (1 + e−z)−1is the sigmoid function and wi, x, y∈Rfor all i= 1, . . . , L.
We want to apply Theorem 1 to conclude that (8) has no spurious setwise local
minimum, and hence no local minima at infinity. Again, the first three assumptions
in assumption 1 are easy to verify: fis nonnegative, hence bounded below; fis
continuously differentiable, hence locally Lipschitz, and admits a chain rule. Note
that fis not semi-algebraic, but it is definable in the real exponential field [ 45] [6,
Section 6.2], so by Morse–Sard theorem for definable functions [ 5, Corollary 9(ii)], it
has finitely many critical values.
Again, it remains to show (8) has bounded gradient trajectories. However, the
techniques in the proof of Proposition 4 cannot be adapted to this case because the
auto-balancing property in [ 17, Theorem 2.1] does not hold. Surprisingly, it is still
true that (8) has bounded gradient trajectories.
Proposition 5. One dimensional sigmoid neural network with loss function (8)has
bounded gradient trajectories.
Proof.Since fis locally Lipschitz and lower bounded, by Proposition 3 there exists a
gradient trajectory for any initial point. For simplicity, define pifori= 0, . . . , L −2
17recursively by p0:=x,p1:=σ(w1x)andpi+1:=σ(wi+1pi). The gradient trajectories
offsatisfy
˙wL=−(wLσ(wL−1pL−2)−y)σ(wL−1pL−2), (9a)
˙wi=pi−1
1 +ewipi−1˙wi+1wi+1, i=L−1, . . . , 1. (9b)
We will prove each wiis bounded inductively from the last layer to the first layer.
The relation between the last two layers wLandwL−1, and the relation between the
first two layers can be regarded as the base cases.
We claim that there exists a time Tsuch that ˙wiandwidoes not change sign
for all t⩾Tand for all i. To verify this, first notice that the claim is true for the
last layer, i.e., ˙wLandwLwill not change sign for all t⩾T. Suppose ˙wLchanges
sign, by continuity and mean value theorem, there exists t∗>0such that ˙wL(t∗) = 0.
However, ˙wL(t∗) = 0implies ˙wi(t∗) = 0for all i, meaning that a critical point is
achieved and the gradient trajectory is stopped for all t⩾t∗. In this case, all wi’s
are trivially bounded. Thus, we assume the trajectory will never stop at a finite time.
In this case, either ˙wL(t)>0or˙wL(t)<0for all t⩾0. Since wLis monotonic, it
either keeps the sign unchanged or changes the sign only once. Thus, there exists
TL>0such that wLdoes not change sign on [TL,∞). Notice that for all i⩾2,
pi−1(t)∈(0,1)for all t⩾0. Since ˙wLwLdoes not change sign on [TL,∞), (9b)
implies that ˙wL−1does not change sign on [TL,∞)either. Therefore, we conclude
thatwL−1is monotonic. Similarly, there exists TL−1> T Lsuch that ˙wL−1andwL−1
does not change sign on [TL−1,∞). Recursively using the above argument, we can
show the claim is true for all i⩾2on[T2,∞). For i= 1, although p0=xmay
not be in (0,1), since xis a constant, the fact that ˙w2andw2do not change sign
still implies that ˙w1does not change sign and hence there exists T1> T 2such that
w1does not change sign on [T1,∞). Therefore, the claim holds for i= 1, . . . , Lby
choosing T=T1.
By the claim proved in the last paragraph, for i= 1, . . . , L, either ˙wiwiis
nonnegative or ˙wiwiis negative on [T,∞). Now we are going to prove each wiis
bounded. The first step is to prove the last two layers wLandwL−1are bounded.
Consider the case where ˙wLwLis nonnegative on [T,∞). (9b) implies that ˙wL−1⩾0
andwL−1isincreasingover [T,∞), sothereexistsaconstant cL−1suchthat wL−1(t)⩾
cL−1forall t⩾0. Since pL−2∈(0,1), wehave σ(wL−1pL−2)⩾σ(−|cL−1|)>0. Again,
by [14, Lemma 5.2],d
dtf(w1, . . . , w L)⩽0andf(w1, . . . , w L)⩽Cfor some constant
Con[0,∞). Thus, it is easy to see |wL|σ(wL−1pL−2)⩽C1for some constant C1on
[0,∞). Since σ(wL−1pL−2)∈[σ(−|cL−1|),1), we conclude |wL|is bounded. Suppose
wL−1is unbounded. Since it is increasing and does not change sign, wL−1(t)>0for
allt⩾TandwL−1(t)→ ∞ast→ ∞. By (9b),
˙wL−1=pL−2
1 +ewL−1pL−2˙wLwL⩽˙wLwL, (10)
18because pL−2∈(0,1)and1 +ewLpL−2>1. By (10), wL−1−1
2w2
Lis a decreasing
function on [T,∞). Hence, wL−1−1
2w2
L⩽C2for some constant C2. Notice that wL
is bounded but wL−1(t)→ ∞ast→ ∞, so a contradiction occurs. Therefore, wL−1
is bounded.
Now we consider the case where ˙wLwLis negative on [T,∞). In this case, (9b)
implies ˙wL−1⩽0, sowL−1is decreasing on [T,∞)and there exists a constant dL−1
such that wL−1⩽dL−1. Since pL−2/(1 +ewL−1pL−2)∈(0,1)and ˙wLwL⩽0on[T,∞),
we have ˙wL−1⩾˙wLwL. This shows wL−1−1
2w2
Lis increasing on [T,∞), and hence
wL−1⩾˜dL−1for some constant ˜dL−1. Therefore, wL−1∈[˜dL−1, dL−1]is bounded.
By exactly the same argument as in the case when ˙wLwLis nonnegative, we know
σ(wL−1pL−2)∈[σ(−|˜dL−1|),1)andwLis bounded by using the boundedness of
objective function f.
Up to now, we have proved boundedness for the last two layers wLandwL−1. For
i= 2, . . . , L −2, by discussing two cases ˙wi+1wi+1⩾0and ˙wi+1wi+1⩽0, together
with the boundedness of wi+1, we can prove that wiis bounded by exactly the same
argument as we did in the last two paragraphs. The induction starts with proving
wL−2is bounded and ends with proving w2is bounded. Once we prove w2is bounded,
consider the relation between w1andw2,
(1 +ew1x) ˙w1=x˙w2w2.
Ifx= 0, then ˙w1= 0implies w1is a constant over [0,∞), so it must be bounded.
Suppose x̸= 0, by taking integration with respect to tand multiplying xon both
sides, we have
w1x+ew1x=x2
2w2
2+C3.
Letz=w1x, then z+ez→ ±∞asz→ ±∞. Thus, the boundedness of w2implies
the boundedness of z=w1x. Since x̸= 0is a constant, w1is bounded. Therefore,
we proved that wiis bounded for all i= 1, . . . , L.
WithProposition5, wecanconcludethat fhasnospurioussetwiselocalminimum
if and only if it has no spurious local minima. However, from the gradient of f, we
can easily see that any critical point of it will be a global minimum, so fhas neither
spurious local minimum nor spurious setwise local minimum. This verifies the second
result in Corollary 1.
Unfortunately, unlike linear neural networks, the result in Proposition 5 is not
true in general even in one-hidden layer case, if more than one data point is given;
see Example 2. However, it is still an open question whether the gradient trajectories
will be bounded in the over-parameterized case (in which case there exists at least
one achievable global minimum).
19Example 2. Consider the following function
f(w1, w2) :=1
2[(w2σ(w1)−1)2+ (w2σ(−w1) + 1)2]. (11)
The above function represents a one-hidden layer sigmoid neural network with two
data (x1, y1) = (1 ,1)and(x2, y2) = (−1,−1). By directly computing the gradient,
one can easily see that (11) has only one critical point (0,0)which is a strict saddle
with f(0,0) = 1. The global minimum is asymptotically attained as w1→ ±∞and
w2→1−2(1 + e2w1)−1, and its corresponding objective value approaches to 1/2. In
this case, the gradient trajectory of (11) starting at any point x0such that f(x0)<1
must be unbounded.
4.3 Matrix sensing
Matrix sensing is a widely used model in computer vision and statistics; see for
instance [ 10,39]. Given r⩾1, the goal is to recover an unknown target matrix
M∈Rn1×n2of rank less than or equal to rfrom a set of linear measurements
bi=⟨Ai, M⟩F, where Ai∈Rn1×n2fori= 1, . . . , mare sensing matrices and ⟨·,·⟩Fis
the Frobenius inner product. In order to do so, we minimize the mean square loss
f(X, Y) :=1
2mmX
i=1(⟨Ai, XYT⟩F−bi)2. (12)
where X∈Rn1×randY∈Rn2×r. The landscape of (12) has been studied widely,
for example, in [ 47,37,32]. Most of these work are based on the restrictive isometry
property (RIP) of sensing matrices. A set of sensing matrices Aifori= 1, . . . , mare
said to have (r, δr)-RIP [39] if there exists δr∈(0,1)such that
(1−δr)∥fM∥2
F⩽1
mmX
i=1⟨Ai,fM⟩2
F⩽(1 +δr)∥fM∥2
F
holds for any matrix fMwith rank(fM)⩽r. To the best of our knowledge, the
minimal assumptions to guarantee no spurious local minima for (12) is for the
sensing matrices to satisfy (4r, δ4r)-RIP with δ4r⩽1/5, as proposed in [ 32, Theorem
III.1].
However, Theorem 1 is applicable to matrix sensing under a weaker condition
than RIP. The first four assumptions in assumption 1 hold because of exactly the
same reasons as in the linear neural network case. Thus, it suffices to show (12)
has bounded gradient trajectories. A sufficient condition is to require the sensing
matrices to be lower bounded , i.e., there exists a constant c >0such that for any
matrix fM∈Rn1×n2with rank(fM)⩽r,
1
mmX
i=1⟨Ai,fM⟩2
F⩾c∥fM∥2
F.
20It is easy to see any level of RIP will imply the existence of such a constant c.
Proposition 6. Matrix sensing with loss function (12)and lower bounded sensing
matrices has bounded gradient trajectories.
Proof.Since fis locally Lipschitz and lower bounded, by Proposition 3 there exists
a gradient trajectory for any initial point. The gradient trajectories of fsatisfy the
initial value problem
˙X=−1
mmX
i=1(⟨Ai, XYT⟩F−bi)AiY,
˙Y=−1
mmX
i=1(⟨Ai, XYT⟩F−bi)AT
iX,
X(0) = X0, Y (0) = Y0.
Notice that ˙XTX=YT˙YandXT˙X=˙YTY, so
d
dt(XTX−YTY) =˙XTX+XT˙X−˙YTY−YT˙Y= 0.
This implies that XTX−YTY=Cwhere C∈Rr×ris a constant. Since the function
value is decreasing along gradient trajectories [ 14, Lemma 5.2], there exists a constant
c1such that f(X(t), Y(t))⩽c1for all t⩾0. Combined with the assumption that
sensing matrices are lower bounded, there exist constants candc2such that
c∥XYT∥2
F⩽1
mmX
i=1⟨Ai, XYT⟩2
F⩽1
mmX
i=1[2(⟨Ai, XYT⟩F−bi)2+ 2b2
i]
= 2f(X, Y) +2
mmX
i=1b2
i⩽2c1+2
mmX
i=1b2
i=:c2.
We have ∥XYT∥2
F⩽c3:=c2/c. Notice that
∥XTX∥2
F+∥YTY∥2
F=∥XTX−YTY∥2
F+ 2∥XYT∥2
F⩽∥C∥2
F+ 2c3.
Define the constant c4:= 2c3+∥C∥2
F. By the Cauchy-Schwarz inequality,
∥X∥4
F+∥Y∥4
F⩽rank( X)∥XTX∥2
F+ rank( Y)∥YTY∥2
F⩽(n1+n2+r)c4.
Thus, XandYare bounded.
Therefore, Theorem 1 says that matrix sensing has no spurious setwise local
minima if and only if it has no spurious local minima, given that the sensing matrices
are lower bounded. Equipped with (4r, δ4r)-RIP where δ4r⩽1/5, we conclude
that matrix sensing has no spurious local minima at infinity, as shown in the third
statement in Corollary 1.
214.4 Nonsmooth matrix factorization
In this subsection, we consider the application of Theorem 1 in a nonsmooth setting,
namely, the nonsmooth matrix factorization problem. We consider minimizing the
loss function
f(X, Y) :=∥XYT−M∥1, (13)
where X∈Rm×r,Y∈Rn×rare decision variables and M∈Rm×nis the given
data matrix. Here ∥A∥1:=Pm
i=1Pn
j=1|Aij|for any A∈Rm×n. In robust principal
component analysis (PCA) problem with sparse noise, (13) is usually used as a
surrogate function for the original ℓ0-norm formulation; see [ 23,8]. There are few
landscape results of (13) in the general rank case. However, if rank(M) = 1 = r, (13)
is shown to have no spurious local minima if every entry MijofMis nonzero [26].
It is hard to analyze (13) because it is nonsmooth, nonconvex, and noncoercive.
Despite all those “non” properties, we show that Theorem 1 is still applicable to
(13) without any rank assumption on M. As a corollary, when rank(M) = 1and
every entry of Mis nonzero, (13) has no spurious setwise local minimum, hence no
spurious local minima at infinity. Again, the first four assumptions in assumption 1
are easy to check: fis bounded below because it is nonnegative; fis locally Lipschitz
because of [ 11, Theorem 2.3.10]; since fis semi-algebraic, by [ 16, Corollary 5.4] and
[38, Corollary 1.1], it admits a chain rule and has finitely many critical values.
To verify (13) has bounded subgradient trajectories, we discover that the auto-
balancing property in [ 17, Theorem 2.2] also holds for nonsmooth matrix factorization.
The result can be summarized in the following proposition.
Proposition 7. Nonsmooth matrix factorization with loss function (13)has bounded
subgradient trajectories.
Proof.Since fis locally Lipschitz and lower bounded, by Proposition 3 there exists
a subgradient trajectory for any initial point. Let (X0, Y0)∈Rm×r×Rn×r. Consider
an absolutely continuous function Z: [0,∞)→Rm×r×Rn×rsuch that
Z′(t)∈ −∂f(Z(t)),for almost every t⩾0,andZ(0) = ( X0, Y0).
By [11, Theorem 2.3.10],
∂f(X, Y) =ΛY
ΛTXΛ∈sign(XYT−M)
where signis an element-wise operation mapping each entry of a matrix to a real
number in [−1,1]such that
sign(x) :=

−1ifx <0,
−1,1
ifx= 0,
1ifx >0.
22Hence, with Z=: (X, Y), for almost every t⩾0we have
X′(t) =−Λ(t)Y(t), Y′(t) =−Λ(t)TX(t), (14a)
Λ(t)∈sign(X(t)Y(t)T−M). (14b)
Consider ϕ: [0,∞)→Rdefined by ϕ(t) := X(t)TX(t)−Y(t)TY(t). By taking
derivative, we have
ϕ′(t) =X′(t)TX(t) +X(t)TX′(t)−Y′(t)TY(t)−Y(t)TY′(t).(15)
Combining (14a) and (15), we have
ϕ′(t) =−Y(t)TΛ(t)TX(t)−X(t)TΛ(t)Y(t)
+X(t)TΛ(t)Y(t) +Y(t)TΛ(t)TX(t) = 0 .
Hence the continuous function ϕis constant on [0,∞). Also, we have
∥XTX−YTY∥2
F=∥XTX∥2
F+∥YTY∥2
F−2⟨XTX, YTY⟩F
=∥XTX∥2
F+∥YTY∥2
F−2∥XYT∥2
F
⩾∥XTX∥2
2+∥YTY∥2
2−2∥XYT∥2
F
=∥X∥4
2+∥Y∥4
2−2∥XYT∥2
F
⩾∥X∥4
2+∥Y∥4
2−2mn∥XYT∥2
1
⩾∥X∥4
2+∥Y∥4
2−2mn(∥XYT−M∥1+∥M∥1)2.
Here∥ · ∥ 2denotes the spectral norm. Therefore, for all t⩾0, we have
∥X(t)∥4
2+∥Y(t)∥4
2⩽∥X(t)TX(t)−Y(t)TY(t)∥2
F
+ 2mn(∥X(t)Y(t)T−M∥1+∥M∥1)2
⩽∥XT
0X0−YT
0Y0∥2
F
+ 2mn(∥X0YT
0−M∥1+∥M∥1)2.
Combined with Proposition 7, Theorem 1 shows that (13) has no spurious setwise
local minimum if and only if it has no spurious local minima. Under the condition
in [26, Theorem 1], i.e., rank(M) = 1 = rand all the entries of Mare non-zero, (13)
reduces to
f(x, y) :=mX
i=1nX
j=1|xiyj−Mij|, (16)
where x∈Rmandy∈Rn. In this case, (16) has no spurious local minima, thus it
has no spurious local minima at infinity, and we obtain the last result in Corollary 1.
23Acknowledgments
We thank the reviewers and the associate editor for their valuable feedback.
A Proof of Lemma 1
LetS⊂Rnbe a setwise local minimum of a continuous function f:Rn→R. Let
U⊃Sbe an open set such that f(x)⩽f(y)for all x∈Sandy∈U\S. Note that
Sis closed, so its boundary is defined by ∂S:=S\S◦. Let z∈∂Sand consider
any real number ϵ >0. Since f(z) + (−ϵ, ϵ)is a neighborhood of f(z), by continuity
off, there exists a neighborhood N(z)ofzsuch that f(N(z))⊂f(z) + (−ϵ, ϵ).
Since Uis a neighborhood of z,N′(z) :=U∩N(z)is also a neighborhood of zwith
f(N′(z))⊂f(z) + (−ϵ, ϵ). The set N′(z)∩Sis nonempty because z∈Sand the set
N′(z)\Sis nonempty because z∈∂S. For any x∈N′(z)∩Sandy∈N′(z)\S, it
follows that
inf
U\Sf−ϵ⩽f(y)−ϵ < f (z)< f(x) +ϵ⩽sup
Sf+ϵ⩽inf
U\Sf+ϵ.
The last inequality follows from the definition of setwise local minima. As ϵ >0was
arbitrary, we deduce that
inf
U\Sf=f(z) = sup
Sf.
Thus, fis a constant on the boundary of Sandfattains its maximum over Son
the boundary of S.
B Proof of Proposition 1
(a)LetSbe a setwise local minimum. By lemma 1, we know that c:=supSf=
f(z)for all z∈∂S. Take a path-connected component CofS. Then C⊂
[f⩽c] :={x∈Rn|f(x)⩽c}. Since Cis path-connected, there exists a
path-connected component Vof[f⩽c]such that C⊂V. By definition, V
is a valley. This shows that a path-connected component of a setwise local
minimum is a subset of a valley.
If in addition, Sis a strict setwise local minimum, then we distinguish two
cases. If S=Rn, then the path-connected component CofSis equal to Rn
and is therefore a valley. Otherwise, it suffices to show that V⊂S. Indeed, V
is then a path-connected subset of Scontaining the path-connected component
CofS, so that by maximality, V=C. Therefore Cis valley.
Consider an open set U⊃Ssuch that f(x)> f(y)for all x∈U\Sand
y∈S. In order to show that V⊂S, it suffices to show that V∩(U\S) =∅and
24V∩Uc=∅because if so, then V= (V∩S)∪(V∩(U\S))∪(V∩Uc) =V∩S.
Since f(w)⩽cfor all w∈Vandf(w)> cfor all w∈U\S(the supremum
function value f(y) =ccan be attained by some y∈∂S⊂S), we know that
V∩(U\S) =∅. Thus, V= (V∩S)∪(V∩Uc). Note that V∩Sis nonempty
and closed because C⊂V∩SandVandSare both closed. Since V∩Ucis
also closed and Vis connected, V∩Ucmust be empty.
(b)Letf:Rn→Rbe a continuous function and a∈Rbe a nonempty sublevel
set of f. By continuity of f,[f⩽a]is closed in Rn. Suppose [f⩽a]has
finitely many connected components C1, . . . , C k. Denote Bas the closure of
any set B⊂Rn. Since Ci’s are connected, by [ 35, Theorem 23.4], Ci’s are also
connected. Since Ci⊂[f⩽a],Ci⊂[f⩽a]= [f⩽a]. By [35, Theorem 25.1],
Cihas no intersection with any other Cjforj̸=i. Together with the fact
that [f⩽a] =Sk
i=1Ci, we have Ci⊂Ci, and hence Ci=Ci. Thus, each Ciis
closed in Rn.
For any fixed i, denote C−i:= [f⩽a]\Ci, then C−i=Sk
j=1,j̸=iCjis a
closed set disjoint with Ci. By [35, Theorem 32.2], there exist disjoint open
setsD, E⊂Rnsuch that Ci⊂DandC−i⊂E. Take U=Din Definition 2,
then f(x)⩽afor all x∈Cibecause Ci⊂[f⩽a]. Furthermore, f(y)> afor
ally∈U\Cibecause (U\Ci)∩[f⩽a] =∅. This verifies that Ciis a strict
setwise local minimum of f.
C Proof of Proposition 2
LetSbe a spurious setwise local minimum at infinity. Since infSf >infRnf, it must
be that S̸=Rn. By Definition 2, there exists y∈Scsuch that S⊂ {x∈Rn|f(x)⩽
f(y)}. Since fis coercive, its sublevel sets are bounded and hence Sis bounded. S
is thus not a spurious local minimum at infinity.
D Proof of Proposition 3
For a fixed real number τ >0, define a sequence xτ
krecurrently by letting xτ
0:=x0
and
xτ
k+1∈arg min
x∈Rn
f(x) +∥x−xτ
k∥2
2τ
,∀k∈N.
A solution exists because fis bounded below and the objective function is coercive.
Any solution satisfies
vτ
k+1:=xτ
k+1−xτ
k
τ∈ −∂f(xτ
k+1),∀k∈N.
25Define two functions xτ,˜xτ:R+→Rnwhere R+:= [0,∞)by
xτ(t) :=xτ
k+1,˜xτ(t) :=xτ
k+ (t−kτ)vτ
k+1,∀t∈(kτ,(k+ 1)τ]
for all k∈N, with initial condition xτ(0) = ˜xτ(0) = x0. Note that ˜xτis absolutely
continuous because it is piecewise affine. On the contrary, xτis not continuous. Also,
define vτ:R+→Rnby
vτ(t) :=vτ
k+1,∀t∈(kτ,(k+ 1)τ],∀k∈N,
and choose vτ(0)∈ −∂f(x0). Since (˜xτ)′=vτon(kτ,(k+ 1)τ)for all k∈N, and
vτ(t)∈ −∂f(xτ(t))for all t⩾0, we conclude that (˜xτ)′(t)∈ −∂f(xτ(t))for almost
every t∈R+. By optimality of xτ
k+1, we have
f(xτ
k+1) +∥xτ
k+1−xτ
k∥2
2τ⩽f(xτ
k),∀k∈N.
For any l∈N, we have
lX
k=0∥xτ
k+1−xτ
k∥2
2τ⩽f(xτ
0)−f(xτ
l+1)⩽f(x0)−inf
Rnf=:C <∞
since fis bounded below. Observe that
lX
k=0∥xτ
k+1−xτ
k∥2
2τ=lX
k=0τ
2∥vτ
k+1∥2=1
2lX
k=0Z(k+1)τ
kτ∥(˜xτ)′(t)∥2dt.
FixT⩾0from now on. From the above, we have
ZT
0∥(˜xτ)′(t)∥2dt⩽⌊T/τ⌋X
k=0Z(k+1)τ
kτ∥(˜xτ)′(t)∥2dt⩽2C. (17)
Since ˜xτis absolutely continuous, for any s, t∈[0, T]we have
∥˜xτ(t)−˜xτ(s)∥=Zt
s(˜xτ)′(u)du(18a)
⩽ZT
0∥(˜xτ)′(t)∥2dt1/2
|t−s|1/2⩽√
2C|t−s|1/2(18b)
where we use the Cauchy-Schwarz inequality. Now one can see (˜xτ)τ>0is a family
of uniformly bounded and equicontinuous functions on the compact interval [0, T].
Therefore, by Arzelà-Ascoli theorem [ 40, Theorem 7.25], there exists a sequence of
positive reals (τk)k∈Nsuch that τk→0and˜xτk→x∗uniformly on [0, T]ask→ ∞.
26For all k∈Nandt∈(kτ,(k+1)τ], we have ˜xτ((k+1)τ) =xτ
k+τvτ
k+1=xτ
k+1=xτ(t).
Thus∥˜xτ(t)−xτ(t)∥=∥˜xτ(t)−˜xτ((k+ 1)τ)∥⩽√
2Cτ1/2for all t∈[0, T]where the
inequality is due to (18) (take s:= (k+ 1)τ). Combined with the fact that ˜xτk→x∗
uniformly on [0, T], one can see that xτk→x∗uniformly on [0, T]. Since (17) implies
that ((˜xτk)′)k∈Nis a bounded sequence in L2([0, T],Rn), there exists a subsequence
(τkj)j∈Nsuch that (˜xτkj)′→v∗weakly in L1([0, T],Rn)asj→ ∞by [21, Corollary
14 p. 413]. Since ˜xτkjis absolutely continuous, for all t∈[0, T], we have
˜xτkj(t)−˜xτkj(0) =Zt
0(˜xτkj)′(u)du.
Take j→ ∞on both sides, we have
x∗(t)−x∗(0) =Zt
0v∗(u)du,
where the convergence of the integral relies on the fact that the constant functions
equal to the canonical basis of Rnlie in L∞([0, T],Rn). Thus, x∗is absolutely
continuous and (x∗)′(t) =v∗(t)for almost every t∈[0, T]. Recall that for all k∈N,
it holds for almost every t∈[0, T]that
(˜xτk)′(t) =vτk(t)∈ −∂f(xτk(t)).
Since fis locally Lipschitz, the set-valued function −∂fis upper semicontinuous [ 11,
2.1.5 Proposition (d) p. 29] with nonempty compact values [ 11, 2.1.2 Proposition
(a) p. 27], hence proper upper hemicontinuous [ 2, Proposition 1 p. 60]. In addition,
xτk→x∗uniformly on [0, T]and(˜xτk)′→(x∗)′weakly in L1([0, T],Rn). Therefore,
(x∗)′(t)∈ −∂f(x∗(t))for almost all t∈[0, T]by [2, Theorem 1 p. 60]7. The initial
condition also holds since ˜xτ(0) = x0for all τ >0.
We have proved that for any initial point x0, there exists x∗: [0, T]→Rnsuch
that (x∗)′(t) =−∂f(x∗(t))holds for almost every t∈[0, T]with any T >0. Since
Tis independent of x0, by setting T= 1, there exists a sequence of absolutely
continuous functions (xk)k∈Nsuch that
x′
k(t)∈ −∂f(xk(t)),for a.e. t∈[0,1], x k(0) = xk−1(1),
for all k∈Nwhere x−1(0) = x0. Therefore, the desired function x: [0,∞)→Rn
can be defined in a piecewise fashion by
x(t) :=xk(t−k), t∈[k, k+ 1),∀k∈N.
By construction, xis absolutely continuous on any compact interval [a, b]⊂[0,∞).
7In the theorem we take F:=−∂f,X=Y:=Rn, and I:= [0, T].
27References
[1]H. Attouch, G. Buttazzo, and G. Michaille. Variational analysis in Sobolev and
BV spaces: applications to PDEs and optimization . SIAM, Philadelphia, 2014.
[2]J.-P. Aubin and A. Cellina. Differential inclusions: set-valued maps and viability
theory, volume 264. Springer-Verlag, Berlin, 1984.
[3]B. Bah, H. Rauhut, U. Terstiege, and M. Westdickenberg. Learning deep
linear neural networks: Riemannian gradient flows and convergence to global
minimizers. Information and Inference: A Journal of the IMA , 11(1):307–353,
2022.
[4]K. L. Blackmore, R. C. Williamson, and I. M. Mareels. Local minima and
attractors at infinity for gradient descent learning algorithms. Journal of
Mathematical Systems Estimation and Control , 6:231–234, 1996.
[5]J. Bolte, A. Daniilidis, A. Lewis, and M. Shiota. Clarke subgradients of stratifi-
able functions. SIAM Journal on Optimization , 18(2):556–572, 2007.
[6]J. Bolte and E. Pauwels. Conservative set valued fields, automatic differentiation,
stochastic gradient methods and deep learning. Mathematical Programming ,
pages 1–33, 2020.
[7]J.BorweinandX.Wang. Lipschitzfunctionswithmaximalclarkesubdifferentials
are generic. Proceedings of the American Mathematical Society , 128(11):3221–
3229, 2000.
[8]V. Charisopoulos, Y. Chen, D. Davis, M. Díaz, L. Ding, and D. Drusvyatskiy.
Low-rank matrix recovery with composite optimization: good conditioning and
rapid convergence. Foundations of Computational Mathematics , pages 1–89,
2021.
[9]K. Chen, D. Lin, and Z. Zhang. On non-local convergence analysis of deep linear
networks. In International Conference on Machine Learning , pages 3417–3443.
PMLR, 2022.
[10]Y. Chi, Y. M. Lu, and Y. Chen. Nonconvex optimization meets low-rank
matrix factorization: An overview. IEEE Transactions on Signal Processing ,
67(20):5239–5269, 2019.
[11]F. H. Clarke. Optimization and Nonsmooth Analysis . SIAM Classics in Applied
Mathematics, Philadelphia, 1990.
[12]A. Daniilidis and D. Drusvyatskiy. Pathological subgradient dynamics. SIAM
Journal on Optimization , 30(2):1327–1338, 2020.
28[13]A. Daniilidis and G. Flores. Linear structure of functions with maximal clarke
subdifferential. SIAM Journal on Optimization , 29(1):511–521, 2019.
[14]D. Davis, D. Drusvyatskiy, S. Kakade, and J. D. Lee. Stochastic subgradient
method converges on tame functions. Foundations of computational mathematics ,
20(1):119–154, 2020.
[15]T. Ding, D. Li, and R. Sun. Suboptimal local minima exist for wide neural
networks with smooth activations. Mathematics of Operations Research , 2022.
[16]D. Drusvyatskiy, A. D. Ioffe, and A. S. Lewis. Curves of descent. SIAM Journal
on Control and Optimization , 53(1):114–138, 2015.
[17]S. S. Du, W. Hu, and J. D. Lee. Algorithmic regularization in learning deep
homogeneous models: Layers are automatically balanced. Advances in Neural
Information Processing Systems , 31, 2018.
[18]A. Eftekhari. Training linear neural networks: Non-local convergence and
complexity results. In International Conference on Machine Learning , pages
2836–2847. PMLR, 2020.
[19]I. Ekeland. On the variational principle. Journal of Mathematical Analysis and
Applications , 47(2):324–353, 1974.
[20]L. C. Evans and R. F. Garzepy. Measure theory and fine properties of functions .
Routledge, Oxfordshire, 2018.
[21]P. M. Fitzpatrick and H. L. Royden. Real Analysis . Pearson, Upper Saddle
River, NJ, 4 edition, Jan. 2010.
[22]C. D. Freeman and J. Bruna. Topology and geometry of half-rectified network
optimization. In International Conference on Learning Representations , 2017.
[23]N.GillisandS.A.Vavasis. Onthecomplexityofrobustpcaand ℓ1-normlow-rank
matrix approximation. Mathematics of Operations Research , 43(4):1072–1084,
2018.
[24]T. X. D. Ha. The ekeland variational principle for set-valued maps involving
coderivatives. Journal of mathematical analysis and applications , 286(2):509–523,
2003.
[25]J.-B. Hiriart-Urruty. A short proof of the variational principle for approximate
solutions of a minimization problem. The American Mathematical Monthly ,
90(3):206–207, 1983.
29[26]C. Josz and L. Lai. Nonsmooth rank-one matrix factorization landscape. Opti-
mization Letters , pages 1–21, 2021.
[27]C. Josz, Y. Ouyang, R. Y. Zhang, J. Lavaei, and S. Sojoudi. A theory on
the absence of spurious solutions for nonconvex and nonsmooth optimization.
NeurIPS , Dec. 2018.
[28]K. Kawaguchi. Deep learning without poor local minima. In Advances in Neural
Information Processing Systems , volume 29. PMLR, 2016.
[29]T. Laurent and J. Brecht. Deep linear networks with arbitrary loss: All local
minima are global. In International conference on machine learning , pages
2902–2907. PMLR, 2018.
[30]J. D. Lee, M. Simchowitz, M. I. Jordan, and B. Recht. Gradient Descent Only
Converges to Minimizers. COLT, 2016.
[31]D. Li, T. Ding, and R. Sun. On the benefit of width for neural networks:
Disappearance of basins. SIAM Journal on Optimization , 32(3):1728–1758,
2022.
[32]S. Li, Q. Li, Z. Zhu, G. Tang, and M. B. Wakin. The global geometry of
centralized and distributed low-rank matrix recovery without regularization.
IEEE Signal Processing Letters , 27:1400–1404, 2020.
[33]S. Liang, R. Sun, J. D. Lee, and R. Srikant. Adding one neuron can eliminate
all bad local minima. Advances in Neural Information Processing Systems , 31,
2018.
[34]S. Marcellin and L. Thibault. Evolution problems associated with primal lower
nice functions. Journal of convex Analysis , 13(2):385, 2006.
[35] J. R. Munkres. Topology. Prenctice Hall, US , 2000.
[36]O. A. Nielsen. An introduction to integration and measure theory , volume 17.
Wiley-Interscience, New York, 1997.
[37]D. Park, A. Kyrillidis, C. Carmanis, and S. Sanghavi. Non-square matrix sensing
without spurious local minima via the burer-monteiro approach. In Artificial
Intelligence and Statistics , pages 65–74. PMLR, 2017.
[38]T. S. Pham and H. H. Vui. Genericity in polynomial optimization , volume 3.
World Scientific, London, 2016.
[39]B. Recht, M. Fazel, and P. A. Parrilo. Guaranteed minimum-rank solutions of
linear matrix equations via nuclear norm minimization. SIAM review , 52(3):471–
501, 2010.
30[40]W. Rudin et al. Principles of mathematical analysis , volume 3. McGraw-hill
New York, 1964.
[41]F. Santambrogio. {Euclidean, metric, and Wasserstein }gradient flows: an
overview. Bulletin of Mathematical Sciences , 7(1):87–154, 2017.
[42]J. Sohl-Dickstein and K. Kawaguchi. Eliminating all bad local minima from loss
landscapes without even adding an extra unit. arXiv preprint arXiv:1901.03909 ,
2019.
[43]L. Van den Dries. Tame topology and o-minimal structures , volume 248. Cam-
bridge university press, 1998.
[44]L. Venturi, A. S. Bandeira, and J. Bruna. Spurious valleys in one-hidden-layer
neural network optimization landscapes. Journal of Machine Learning Research ,
20:133, 2019.
[45]A. J. Wilkie. Model completeness results for expansions of the ordered field
of real numbers by restricted pfaffian functions and the exponential function.
Journal of the American Mathematical Society , 9(4):1051–1094, 1996.
[46]L. Zhang. Depth creates no more spurious local minima. arXiv preprint
arXiv:1901.09827 , 2019.
[47]Z. Zhu, Q. Li, G. Tang, and M. B. Wakin. Global optimality in low-rank matrix
optimization. IEEE Transactions on Signal Processing , 66(13):3614–3628, 2018.
31