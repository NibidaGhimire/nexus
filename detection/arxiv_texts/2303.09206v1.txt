arXiv:2303.09206v1  [math.ST]  16 Mar 2023Error analysis of regularized trigonometric linear regres sion
with unbounded sampling: a statistical learning viewpoint
Anna Scampicchio, Elena Arcari, Melanie N. Zeilinger
Abstract — The effectiveness of non-parametric, kernel-based
methods for function estimation comes at the price of high
computational complexity, which hinders their applicabil ity in
adaptive, model-based control. Motivated by approximatio n
techniques based on sparse spectrum Gaussian processes, we
focus on models given by regularized trigonometric linear
regression. This paper provides an analysis of the performa nce
of such an estimation set-up within the statistical learnin g
framework. In particular, we derive a novel bound for the
sample error in ﬁnite-dimensional spaces, accounting for n oise
with potentially unbounded support. Next, we study the ap-
proximation error and discuss the bias-variance trade-off as a
function of the regularization parameter by combining the t wo
bounds.
I. INTRODUCTION
Non-parametric approaches for regularized function esti-
mation are a key tool in machine learning, and have been
successfully applied to, e.g., system identiﬁcation [1] an d
learning-based control [2], [3]. Nevertheless, their appl i-
cability in real-time scenarios is hindered by their high
computational complexity, which scales cubically with the
number of data. The strategies proposed to enable fast
adaptation of kernel-based methods can be grouped into
two main categories: input location selection, and low-ran k
approximations of the kernel [4]. In this second class of
approaches, a vast success was achieved by sparse spectrum
Gaussian processes [5], [6], where operations on the (shift -
invariant) kernel yield a parametric approximation by mean s
of linear combinations of Fourier features.
In this paper, we draw inspiration from the latter method and
perform regularized regression within a ﬁnite-dimensiona l
hypothesis space Hdeﬁned by a span of Epredeﬁned
trigonometric functions. Such a set-up relaxes the assumpt ion
of having shift-invariant kernels, and results more robust
against potential basis function mis-speciﬁcation compar ed
to non-regularized, projection-based approaches [7]; for a
review of parametric methods based on Fourier features,
we refer to [8, Chapter 1.7]. Our goal is to assess the
performance of the proposed estimator as a key step towards
deriving reliable guarantees for data-driven, model-base d
control schemes that leverage such a model (see, e.g., [9],
[10], [11]).
We frame this analysis in the statistical learning set-up [1 2],
[13]. The function to be estimated (i.e., the regression func-
tionfρ) is deﬁned as the minimizer of the expected risk over
a (partially) unknown probability distribution, jointly d eﬁned
The Authors are afﬁliated with the Institute
of Dynamic Systems and Control, ETH Zürich.
{ascampicc,earcari,mzeilinger }@ethz.chover the input-output spaces, and from which i.i.d. samples
are drawn. Consequently, this formulation can also handle
fully nonlinear measurement models. Furthermore, fρis
generally assumed to belong to the space of square-integrab le
functions L2, and the hypothesis space is typically taken as
an inﬁnite-dimensional Reproducing Kernel Hilbert Space
(RKHS), which is related to L2by interpolation spaces argu-
ments ([14, Theorem 2], [15]). Differently from classic non -
parametric set-ups, the regression function is not assumed
to belong to the hypothesis space. Thus, two objects can
be therein deﬁned: the actual data-based estimate fzand its
data-free limit fH. The goal of error analysis consists in
quantifying the approximation error, or bias,∝bardblfρ−fH∝bardblL2ρX,
and the sample error, or variance ,∝bardblfH−fz∝bardblL2ρX. As regards
the latter, results abound in the statistical learning lite rature.
Most of them deal with probability measures on the outputs
that have bounded support, and thus obtain bounds leveragin g
concentration inequalities such as Hoeffding’s or Bennett ’s
[16], [13, Chapter 3.1]. Works in this direction are, e.g., [ 17],
[13], [18], [19], [20]. Contributions considering unbound ed
sampling include [21], [22], [23]. The bounds therein deriv ed
leverage the so-called moment hypothesis, which relaxes th e
boundedness assumption, and hold also for (sub-)Gaussian
noises. Such results rely on the computation of covering
numbers quantifying the capacity of the hypothesis space
[24], and showcase optimal rates of convergence; neverthe-
less, they tend to be of limited practical relevance in the no n-
asymptotic case due to the large values of the multiplicativ e
coefﬁcients, which are often furthermore difﬁcult to compu te.
In this work, we perform error analysis for ﬁnite-dimension al
hypothesis spaces given by trigonometric functions. Our
ﬁrst contribution is a sample error bound, which is less
conservative than the ones available in the literature even
if it accounts also for noises with unbounded supports. Our
second contribution consists in studying the bias-varianc e
trade-off of the regularized trigonometric regression set -up.
To this end, we obtain two bounds on the approximation error,
combine them with the sample complexity result and analyze
the conditions ensuring the existence of a unique value of
the regularization parameter γreturning the optimal trade-
off. The differences of the two approaches for estimating th e
regularization parameter are investigated in a Monte Carlo
study, which shows that one of the two criteria returns a
value ofγthat captures the oracle behavior (i.e., minimizing
the overall error), thus leading to fast estimation schemes
that do not need preliminary hyper-parameter selection.II. PROBLEM SET-UP
Let the metric space of inputs Xbe a compact subset of
R: without loss of generality, we take X= [−X/2,X/2]
for someX∈R+(the scalar case is presented just for ease
of visualization: the multi-dimensional is a straightforw ard
extension). The output space is assumed to be Y=R.
There is a probability measure ρdeﬁned over Z=X×
Ythat decomposes into ρX(x)andρ(y|x)according to
Fubini’s Theorem. In the considered set-up, the probabilit y
measure deﬁned on Xis the standard uniform: denoting
withµthe Lebesgue measure, we have that ρX(A) =
µ(A∩X)/µ(X) =µ(A∩X)/Xfor any set Ain the
σ−algebra of interest. In this way, ρXis a Borel non-
degenerate,σ−ﬁnite measure. As regards ρ(y|·), we assume
it is unknown and deﬁned over R.
HavingNindependent samples drawn from ρcollected in
the data-setD={(xt, yt)}N
t=1, the goal is to estimate the
regression function
fρ(x) =/integraldisplay
Yydρ(y|x). (1)
We make use of the following Assumption.
Assumption 1: The regression function fρbelongs to the
space of square-integrable functions on X, denoted by
L2
ρX, and is such that ∝bardblfρ∝bardblL2
ρX=/radicalBig/integraltext
Xf2(x)dρX(x) =/radicalBig/integraltext
Xf2(x)dµ(x)/X≤Bf. Moreover, we also have that
σ2
ρ=/integraltext
Xσ2
ρ(x)dρX(x) =/integraltext
Z(y−fρ(x))2dρ≤B2
σ./square
In other words, we assume to have access to bounds on the
energy of the unknown function to be estimated, and on the
variance of the additive noises.
The space L2
ρXis a separable Hilbert space whose complete
orthonormal basis by means of trigonometric functions [25]
is given by
/braceleftBigg
√
2sin/parenleftBig2πq
Xx/parenrightBig
,√
2cos/parenleftBig2πq
Xx/parenrightBig/bracerightBigg
q∈N(2)
={¯ϕs
q(x),¯ϕc
q(x)}q∈Nwithx∈X. (3)
Accordingly, any function f∈L2
ρXcan be expressed as
f(·) =/summationtext
q∈Nαs
q¯ϕs
q(·) +αc
q¯ϕc
q(·), which will be also com-
pactly written as f(·) =/summationtext
q∈Nαq¯ϕq(·), with/summationtext
q∈Nα2
q<∞.
Within this representation, we denote the target function a s
fρ(·) =/summationtext
q∈N¯αs
q¯ϕs
q(·)+ ¯αc
q¯ϕc
q(·) =/summationtext
q∈N¯αq¯ϕq(·).
Function estimation in L2
ρXcannot be performed, because
pointwise evaluation is not well deﬁned. Therefore, we
perform such a task within a hypothesis space having the
structure of a RKHS. Speciﬁcally, we consider the RKHS
obtained from a subset of functions in (3) with cardinality E,
whereEis chosen according to our computational capacity.
Denote by Qthe set of selected frequencies, i.e., Q=
{qj}E/2
j=1⊂N, and consider the following functions extracted
from (3) using Qdeﬁned, for j= 1,...,E/2, as
ϕi(x) =/braceleftBigg
¯ϕs
qj(x), i=j
¯ϕc
qj(x), i=j+E
2.(4)Then, the RKHS of interest is the one induced by the
following kernel:
K(xa,xb) =φ⊤(xa)Σαφ(xb), (5)
whereΣα= diag(λ1,...,λE)is a positive deﬁnite
matrix, and the vector φ(·)∈REis such that
φ⊤(x) = [ϕ1(x)... ϕE(x)].Clearly, (5) is a Mercer
kernel ([26, (6), p.346]; it satisﬁes Mercer’s condition/integraltext
X/integraltext
XK(x,x′)2dρX(x)dρX(x′) =/summationtextE
i=1λ2
i, and it is
non-stationary if and only if λi∝ne}ationslash=λi+E/2for alli=
1,...,E/2. Furthermore, using the argument in [27, Chapter
4.3]), it holds that
CK= sup
xa,xb∈X/radicalbig
K(xa,xb)
≤/radicaltp/radicalvertex/radicalvertex/radicalbtE/2/summationdisplay
i=1max{λi,,λi+E/2}<+∞. (6)
Being a Mercer kernel, we have from Moore-Aronszajn
Theorem [26] that Kis in one-to-one correspondence with
the Hilbert space of functions (H,∝an}bracketle{t·,·∝an}bracketri}htH), which is
H={f∈L2
ρX:f(·) =φ⊤(·)α, α∈RE} (7)
with inner product given, for f(♮)(·) =φ⊤(·)α(♮)and♮=
a, b:
∝an}bracketle{tf(a), f(b)∝an}bracketri}htH=∝an}bracketle{tΣ−1/2
αα(a),Σ−1/2
αα(b)∝an}bracketri}ht2. (8)
Within the hypothesis space, we can compute the esti-
mate from the data-set Das follows. Consider the sam-
pling operatorSX:H→RNsuch thatSX(f) =
[f(x1)... f(xN)]⊤, together with its adjoint S⊤
X:RN→
HyieldingS⊤
Xc=/summationtextN
t=1ctK(xt,·). Thus, considering
Y= [y1,...,yN]⊤and regularization parameter γ >0, we
have
fz= arg min
f∈H1
NN/summationdisplay
t=1(yt−f(xt))2+γ∝bardblf∝bardbl2
H (9)
=/parenleftBig1
NS⊤
XSX+γI/parenrightBig−11
NS⊤
XY. (10)
The aim of error analysis is to quantify the discrepancy
betweenfzandfρ. To this end, we additionally consider
the data-free limit of (9) as
fH= arg min
f∈H/integraldisplay
X(f(x)−fρ(x))2dρX(x)+γ∝bardblf∝bardbl2
H
(11)
= (LK+γI)−1LKfρ, (12)
whereLK(f)(¯x) =/integraltext
XK(¯x,x)f(x)dρX(x)is an integral
operator which, thanks to the properties of K, is (a) is self-
adjoint and strictly positive, (b) continuous and compact,
(c) satisties the Spectral Theorem [13, Theorem 4.3] with
eigenpairs{(ϕi(·),λi)}E
i=1. Thanks to these properties, given
an arbitrary L2
ρXfunctionf(x) =/summationtext
q∈Nαq¯ϕq(x), using
linearity and orthonormality of the basis, we have
LK(f)(¯x) =E/summationdisplay
i=1λiαπ
iϕi(¯x), (13)where we deﬁne the i−th component of the vector απfor
i= 1,...,E , along the lines of (4), as follows:
Forj= 1,...,E
2, απ
i=/braceleftBigg
αs
qj, i=j
αc
qj, i=j+E/2.
(14)
Moreover, thanks to property (a), we can also deﬁne the r-th
power of the integral operator1as [12]:
Lr
K(f)(¯x) =E/summationdisplay
i=1λr
iαπ
iϕi(¯x). (15)
In the following, we study the sample error ∝bardblfz−fH∝bardblL2ρXintroduced by the ﬁniteness of the data-set D, and the
approximation error ∝bardblfH−fρ∝bardblL2ρXdetermined by the
choice of the hypothesis space. The two bound the overall
error as∝bardblfz−fρ∝bardblL2ρX≤∝bardblfz−fH∝bardblL2ρX+∝bardblfH−fρ∝bardblL2ρX,
which is to be minimized as a function of the regularization
parameterγ.
III. SAMPLE ERROR
In this Section we provide the novel result concerning the
error between fzandfHintroduced in (10) and (12). Its
proof can be found in Appendix B.
Theorem 1: Let Assumption 1 hold. Consider CKas
introduced in (6), and deﬁne ˘λ= min i=1,...,Eλi. Then, with
conﬁdence at least 1−δ, it holds that
∝bardblfz−fH∝bardblL2ρX≤C3
K
γ/radicalBigg
B2
f+B2σ
˘λNδ. (16)
/square
Remark 1: We did not study bounds for EZ[ρN(∝bardblfz−
fH∝bardblL2ρX], because they typically return conservative values.
A result for unbounded sampling is given, e.g., in [28, Propo -
sition 20]. Note also that our probabilistic guarantees fal l in
the category of “honest" bounds rather than “exact" bounds,
following the deﬁnitions given in [29]: this means that,
for a user-chosen conﬁdence level δ, the result holds with
conﬁdence "at least 1−δ" and not with "exact probability
1−δ".
IV. APPROXIMATION ERROR
We now study the error due the choice of the hypothesis
space H, i.e., the L2
ρX-distance between the solution fH
introduced in (12) and the regression function fρdeﬁned
in (1). We ﬁrst provide an expression for fH: letting the
regression function be expressed through the basis functio ns
1Note that the case r=−1/2plays a crucial role in connecting the
norms in L2
ρXandHfor functions in the hypothesis space. Indeed,
considering f(·) =/summationtextE
i=1αiϕi(·), one has by deﬁnition of Hthat
/bardblf/bardbl2
H=/bardblΣ−1/2
αα/bardbl2
2=/summationtextE
i=1α2
i/λi. On the other hand, we have
thatL−1/2
K(f)(·) =/summationtextE
i=1αi/√λiϕi(·), and its L2
ρX-norm is equal, by
Parseval’s Theorem, to/summationtextE
i=1α2
i/λi. Therefore, we obtain that /bardblf/bardbl2
H=
/bardblL−1/2
Kf/bardbl2
L2ρX.ofL2
ρXasfρ(·) =/summationtext
q∈N¯αq¯ϕq(·), and recalling the deﬁ-
nition of the RKHS basis functions ϕi(·)in (4) and of the
coefﬁcients απ
iin (14), we have
fH(·) =E/summationdisplay
i=1λi
λi+γ¯απ
iϕi(·). (17)
Thanks to this result, we derive two bounds on the approx-
imation error depending on different norms of the vector
¯απdeﬁned in (14). The discussion of their performance
is deferred to Section VI-C. We present the result in the
following Proposition, which is proven in Appendix C.
Proposition 1: In the trigonometric linear regression
framework presented in Section II, the approximation error
∝bardblfH−fρ∝bardblL2ρXadmits the following upper bounds:
(a)γ
˘λ+γ∝bardbl¯απ∝bardbl2+/radicalBigg/summationdisplay
q∈N\Q¯α2q (18)
(b)∝bardbl¯απ∝bardbl∞γE/summationdisplay
i=11
λi+/radicalBigg/summationdisplay
q∈N\Q¯α2q. (19)
/square
V. BIAS-VARIANCE TRADE-OFF
In this section we combine the bounds on the sample and
approximation errors derived in Theorem 1 and Proposition
1, respectively, and study the estimated overall error ∝bardblfz−
fρ∝bardblL2ρXas a function of the regularization parameter γ. We
perform our analysis after the RKHS Hhas been completely
speciﬁed, i.e., after having ﬁxed Qand{λi}E
i=1.
The main result is presented in the following Proposition
proven in Appendix D.
Proposition 2:
(a) Consider the approximation error bound as in (18).
Then, if the number of data Nand the conﬁdence
parameterδare such that
√
Nδ>C3
K
˘λ3/2/radicalBigg
B2
f+B2σ/summationtextE
i=1(¯απ
i)2, (20)
there exists a unique γ= ˆγ(a)minimizing the estimated
error∝bardblfz−fρ∝bardblL2
ρX.
(b) Take now the approximation error bound as in (19).
Then, there always exist a unique γ= ˆγ(b)minimizing
the estimated error ∝bardblfz−fρ∝bardblL2
ρX. /square
The closed-form expressions for ˆγ(a)andˆγ(b)are provided
in the proof.
VI. DISCUSSION
We ﬁrst study the performance of the sample error bound
provided in Section III by comparing it with other bounds
given in [14] and [28]. Next, we discuss the result of
Proposition 2, especially showcasing the capability of γ(b)to
capture the behaviour of the oracle γminimizing the overall
error.A. Comparison with sample error bound in [14, Theorem 5]
In the numerical set-up we assume that a uniformly
distributed random noise with a Signal-to-Noise Ratio (SNR )
of 150 affects the measurements of the regression function
fρ(x) =/summationtext
q∈N¯ϕq(x)¯αqwithx∈[−1250,1250] . Such a
function is assumed to be characterized by 20 sine/cosine
couples{¯ϕq}, whereqis randomly drawn without repeti-
tions from the set {1,...,30}. The hypothesis space His
characterized by a subset of E/2 = 10 sine/cosine couples
randomly selected among those that deﬁne the regression
function.
We perform a Monte Carlo study of 500 runs, where at each
step we draw a new set of basis functions deﬁning the regres-
sion function and the hypothesis space. Coefﬁcients ¯αqof the
regression function are drawn from a Gaussian distribution
N(0,λ), whereλis sampled from a uniform distribution
on(0,5), and also enters the deﬁnition of the hypothesis
space as in (8) as λi=λfor alli= 1,...,E . At each
run, the number of data-points Nis randomly sampled
from the set{100,101,...,1000}. We consider a conﬁdence
level ofδ= 0.1. Then, we evaluate the sample error
bounds corresponding to the minimum value of γsatisfying
the bound in [14, Theorem 5], and evaluate their relative
difference with respect to the true sample error attained
with such a γ. The results are displayed in Figure 1. Both
bounds decay as 1/√
N, but (16) evidences a more favorable
behaviour in terms of the conﬁdence level, at least for value s
ofδsmaller than the solution of 1/√
δ= log(4/δ)in(0,1],
that is≈0.0539 . Conservatism in the bound in [14, Theorem
5] is mostly due to the linear dependence on the output values
bound,M. The explicit condition on Mensuring bound (16)
to be more conservative is the following:
M≤C2
K
12/radicalBigg
B2
f+B2σ
˘λγ1√
δlog(4/δ). (21)
Such a value tends to be very small: e.g., in the Monte Carlo
test, the bound (21) returned a mean value of 3.50±2.02,
while the true value Memerging from the (quite favorable)
SNR attained a mean value of 39.02±14.66.
B. Comparison with sample error bound in [28, Proposition
20]
We consider the same numerical set-up as the previous
section, and we translate the bound of [28, Proposition 20]
into a statement of the same type as Theorem 1 by using
Markov’s inequality. To further adapt to the context given i n
Section II, we set p= 2 andN(γ) =/summationtextE
i=1λi/(λi+γ).
The bound of [28, Proposition 20] shows a slower behaviour
in the number of data Nwith respect to (16); moreover,
it depends on the approximation error, which is generally
not known. We performed the Monte Carlo study by set-
ting∝bardblfH−fρ∝bardblL2ρXto its true value, and the results are
very conservative, as displayed in Figure 1.
C. On the choice of γin view of the bias-variance trade-off
We now perform a Monte Carlo study to discuss the results
given in Proposition 2. Consider X= [−5×105,5×105]Theorem 1 [Smale, et al. 2007] [Lin, et al. 2017]51015Logarithm of sample error relative difference
Fig. 1: Behaviour of the sample error bounds in the Monte Carl o
trials in Sections VI-A and VI-B. The adopted score is the dif fer-
ence between bound and true sample error, normalized by the t rue
sample error. For Theorem 1, such an error attains a mean valu e
of21.44±4.093, while for the bound in [14] it is 440.03±99.33,
and3.40×106±3.32×106for the one in [28]. We display the
values in logarithmic scale to facilitate visualization.
as input domain. The regression function is characterized
by 30 sine/cosine pairs {¯ϕ}30
q=1, where each qis randomly
selected without repetitions from the set {1,...,100}, and
each component of the vector of coefﬁcients ¯αis drawn from
a Gaussian random variable with zero mean and variance λ=
1. The latter hyper-parameter also enters the deﬁnition of th e
RKHS H. The set of frequencies Qis selected as a random
subset with cardinality 10 from the set of those characteris ing
the regression function. Fixing an SNR equal to 50, we draw
50 random regression functions and select the basis functio ns
for the hypothesis space. The number of input/output pairs
for each run is N= 2500 , and we consider a conﬁdence
parameterδ= 0.5. For each run, we compute γ(a)andγ(b)
as in Proposition 2, compute the sample- and approximation
error bounds as in Theorem 1 and Proposition 1, and compare
their values to the true errors yielded by γ(a)andγ(b). We
observe that the bounds computed with γ(a)are closer to the
true values. We display the values in Table I.
γ(a) True value Bound
/bardblfz−fH/bardblL2ρX0.036±0.0070.419±0.033
/bardblfH−fρ/bardblL2ρX(a)9.313±0.07710.05±0.992
γ(b) True value Bound
/bardblfz−fH/bardblL2ρX0.387±0.07614.04±1.942
/bardblfH−fρ/bardblL2ρX(b)7.351±0.57520.41±2.193
TABLE I: Overall values (mean ±standard deviation) of sample
and approximation error bounds compared to the true errors.
The test above described was performed ﬁxing the regu-
larization parameter, and focused on the single errors. If w e
instead consider the overall error ∝bardblfz−fρ∝bardblL2
ρX, and compare
values ofγ(a)andγ(b)with the oracle value γ∗(obtained
via grid search) minimizing it, we observe that γ(b)is the
one that performs best. The poor performance of γ(a)is due
to the fact that the condition in (20) needs a large number
of data to be satisﬁed, and this leads to an overestimation
of the regularization parameter. In this speciﬁc test, γ∗was
located at the minimum value of the grid, i.e. γ∗= 0.1; the
mean values for γ(a)andγ(b)were7.7703±0.2115 and
0.2308±0.0230 , respectively.VII. CONCLUSIONS
In this paper, we analysed the estimation errors occurring
in regularized trigonometric regression within the statis tical
learning set-up. To the best of the Authors’ knowledge, such
a study was missing in the literature, that mostly focused
on non-parametric methods or non-regularized trigonometr ic
regression. We derived a novel bound on the sample error
that does not require the support of the output distribution
to be ﬁnite; numerical tests showed it to be less conservativ e
than classical bounds, at least in the non-asymptotic regim e.
Next, we computed two bounds for the approximation error,
and combined them with the sample error bound to retrieve
a selection criterion for the regularization parameter γ, op-
timizing the trade-off between estimated bias and variance .
In particular, we showed that one of the two criteria yields
a value of the regularization parameter that is close to the
oracle, and thus can in principle be used to speed up hyper-
parameter selection. We stress that such an analysis can be
extended to any other orthogonal basis of L2
ρX. Moreover,
we foresee that the generality of such a set-up can have an
impact on an abstract treatment of bias learning, which is a
planned extension of the present work. Forthcoming researc h
will also focus on asymptotic behavior in terms of number
of dataNand of the basis functions E.
REFERENCES
[1] G. Pillonetto, F. Dinuzzo, T. Chen, G. De Nicolao, and L. L jung, “Ker-
nel methods in system identiﬁcation, machine learning and f unction
estimation: A survey,” Automatica , vol. 50, no. 3, pp. 657–682, 2014.
[2] L. Hewing, K. P. Wabersich, M. Menner, and M. N. Zeilinger ,
“Learning-based model predictive control: Toward safe lea rning in con-
trol,” Annual Review of Control, Robotics, and Autonomous Systems ,
vol. 3, no. 1, pp. 269–296, 2020.
[3] T. Koller, F. Berkenkamp, M. Turchetta, and A. Krause, “L earning-
based model predictive control for safe exploration,” in 2018 IEEE
Conference on Decision and Control (CDC) , 2018, pp. 6059–6066.
[4] H. Liu, Y .-S. Ong, X. Shen, and J. Cai, “When gaussian proc ess meets
big data: A review of scalable gps,” IEEE Transactions on Neural
Networks and Learning Systems , vol. 31, no. 11, pp. 4405–4423, 2020.
[5] M. Lázaro-Gredilla, J. Quiñonero-Candela, C. E. Rasmus sen, and A. R.
Figueiras-Vidal, “Sparse spectrum Gaussian process regre ssion,” The
Journal of Machine Learning Research , vol. 11, pp. 1865–1881, 2010.
[6] A. Rudi and L. Rosasco, “Generalization properties of le arning with
random features,” in Advances in Neural Information Processing
Systems , vol. 30. Curran Associates, Inc., 2017.
[7] A. E. Hoerl and R. W. Kennard, “Ridge regression: Biased e stimation
for nonorthogonal problems,” Technometrics , vol. 12, no. 1, pp. 55–67,
1970.
[8] A. B. Tsybakov, “Introduction to nonparametric estimat ion,” in
Springer series in statistics , 2009.
[9] Y . Pan, X. Yan, E. A. Theodorou, and B. Boots, “Prediction under
uncertainty in sparse spectrum Gaussian processes with app lications
to ﬁltering and control,” in Proceedings of the 34th International
Conference on Machine Learning , ser. PMLR, vol. 70, August 2017,
pp. 2760–2768.
[10] E. Arcari, A. Scampicchio, A. Carron, and M. N. Zeilinge r, “Bayesian
multi-task learning using ﬁnite-dimensional models: A com parative
study,” in 2021 60th IEEE Conference on Decision and Control (CDC) ,
2021, pp. 2218–2225.
[11] E. Arcari, M. V . Minniti, A. Scampicchio, A. Carron, F. F arshidian,
M. Hutter, and M. N. Zeilinger, “Bayesian multi-task learni ng
mpc for robotic mobile manipulation,” 2022. [Online]. Avai lable:
https://arxiv.org/abs/2211.10270
[12] F. Cucker and S. Smale, “On the mathematical foundation s of learn-
ing,” Bulletin of the American Mathematical Society , vol. 39, pp. 1–49,
2002.[13] F. Cucker and D. X. Zhou, Learning Theory: An Approximation Theory
Viewpoint , ser. Cambridge Monographs on Applied and Computational
Mathematics. Cambridge University Press, 2007.
[14] S. Smale and D.-X. Zhou, “Learning theory estimates via integral oper-
ators and their approximations,” Constructive Approximation , vol. 26,
pp. 153–172, 2007.
[15] A. Lunardi, Interpolation Theory , ser. Publications of the Scuola
Normale di Pisa. Edizioni della Normale Pisa, 2009.
[16] S. Boucheron, G. Lugosi, and O. Bousquet, Concentration Inequalities .
Berlin, Heidelberg: Springer Berlin Heidelberg, 2004, pp. 208–240.
[17] Q. Wu, Y . Ying, and D.-X. Zhou, “Learning rates of least- square
regularized regression,” Foundations of Computational Mathematics ,
pp. 171–192, 2006.
[18] F. Cucker and S. Smale, “Best choices for regularizatio n parameters
in learning theory: On the bias—variance problem,” Foundations of
Computational Mathematics , vol. 2, pp. 413–428, March 2008.
[19] S. Mendelson and J. Neeman, “Regularization in kernel l earning,” The
Annals of Statistics , vol. 38, no. 1, pp. 526 – 565, 2010.
[20] C. Wang and D.-X. Zhou, “Optimal learning rates for leas t squares reg-
ularized regression with unbounded sampling,” Journal of Complexity ,
vol. 27, no. 1, pp. 55–67, 2011.
[21] A. Caponnetto and E. de Vito, “Optimal rates for the regu larized least-
squares algorithm,” Foundations of Computational Mathematics , vol. 7,
pp. 331–368, 2007.
[22] C. Wang and D.-X. Zhou, “Optimal learning rates for leas t squares reg-
ularized regression with unbounded sampling,” Journal of Complexity ,
vol. 27, no. 1, pp. 55–67, 2011.
[23] Z.-C. Guo and D.-X. Zhou, “Concentration estimates for learning
with unbounded sampling,” Advances in Computational Mathematics ,
vol. 38, pp. 207–223, 2013.
[24] D.-X. Zhou, “Capacity of reproducing kernel spaces in l earning
theory,” IEEE Transactions on Information Theory , vol. 49, pp. 1743
– 1752, August 2003.
[25] N. Akhiezer and I. Glazman, Theory of Linear Operators in Hilbert
Space , ser. Dover Books on Mathematics. Dover Publications, 2013 .
[26] N. Aronszajn, “Theory of reproducing kernels,” Transactions of the
American Mathematical Society , vol. 68, no. 3, pp. 337–404, 1950.
[27] I. Steinwart and A. Christmann, Support Vector Machines , 1st ed.
Springer Publishing Company, Incorporated, 2008.
[28] S.-B. Lin, X. Guo, and D.-X. Zhou, “Distributed learnin g with
regularized least squares,” Journal of Machine Learning Research ,
vol. 18, no. 92, pp. 1–31, 2017.
[29] P. L. Davies, A. Kovac, and M. Meise, “Nonparametric reg ression,
conﬁdence regions and regularization,” Annals of Statistics , vol. 37,
pp. 2597–2625, 2007.
[30] P. Niyogi and F. Girosi, “Generalization bounds for fun ction ap-
proximation from scattered noisy data,” Advances in Computational
Mathematics , vol. 10, no. 1, 1999.
[31] S. Berberian, Introduction to Hilbert Space . Oxford University Press,
1961.
[32] G. Wahba and Y . Wang, Representer Theorem . American Cancer
Society, 2019, pp. 1–11.
[33] S. Smale and D.-X. Zhou, “Shannon sampling II: Connecti ons to learn-
ing theory,” Applied and Computational Harmonic Analysis , vol. 19,
no. 3, pp. 285–302, 2005.
[34] ——, “Shannon sampling and function reconstruction fro m point
values,” Bulletin of The American Mathematical Society , vol. 41, pp.
279–306, July 2004.
[35] T. Zhang, “Learning Bounds for Kernel Regression Using Effective
Data Dimensionality,” Neural Computation , vol. 17, no. 9, pp. 2077–
2098, September 2005.
[36] B. Mu, T. Chen, and L. Ljung, “On asymptotic properties o f hyperpa-
rameter estimators for kernel-based regularization metho ds,”Automat-
ica, vol. 94, pp. 381–395, 2018.
[37] W. Gilks, S. Richardson, and D. Spiegelhalter, Markov Chain Monte
Carlo in Practice , ser. Chapman & Hall/CRC Interdisciplinary Statis-
tics. Taylor & Francis, 1995.
[38] G. Pillonetto and A. Chiuso, “Tuning complexity in regu larized kernel-
based regression and linear system identiﬁcation: The robu stness of
the marginal likelihood estimator,” Automatica , vol. 58, pp. 106–117,
2015.
[39] J. Maritz, Empirical Bayes Methods with Applications . CRC Press,
2018.APPENDIX
We provide in Appendix A an introduction to error analysis in the statistical learning framework. Appendices B, C and
D report the proofs for the theoretical results stated in Sec tions III, IV and V, respectively. For ease of referencing, t he
statements of the Theorems used as benchmarks in Section VI a re given in Appendix E. Appendix F presents an additional
comparison with the sample error bound with unbounded noise support reported in [22]. In Appendix G, we show additional
plots related to the experiment of Section VI-C. A discussio n on the beneﬁts of regularization is given in Appendix H.
A. Statistical learning framework
Let the metric space of inputs Xbe compact, and let Y=Rbe the space of outputs. There is a probability measure
ρdeﬁned over Z=X×Ythat decomposes as ρ(y|x)andρX(x)according to Fubini’s Theorem: given an integrable
functionψ:Z→R, it holds/integraldisplay
Zψ(z)dρ(z) =/integraldisplay
X/parenleftBig/integraldisplay
Yψ(x,y)dρ(y|x)/parenrightBig
dρX(x).
Assume to collect Nindependent samples drawn from ρin the data-setD={(xt, yt)}N
t=1. The aim of statistical learning
is that of estimating the regression function ofρdeﬁned as
fρ(x) =/integraldisplay
Yydρ(y|x).
Such a function is the minimizer of the expected risk I[f] =Eρ[(y−f(x))2] =/integraltext
Z(y−fρ(x) +fρ(x)−f(x))2dρ=
I[fρ]+/integraltext
Z(fρ(x)−f(x))2dρ[30] and can be viewed as the ﬁrst statistical moment of ρ(y|x). Its variance is deﬁned as
σ2
ρ(x) =/integraldisplay
Y(y−fρ(x))2dρ(y|x),
whose integral over the Xdomain is
σ2
ρ=/integraldisplay
Xσ2
ρ(x)dρX(x) =/integraldisplay
Z(y−fρ(x))2dρ=I[fρ].
Note thatσ2
ρrepresents the unavoidable cost in the minimization of I(f)and it measures how well conditioned ρis: in
other words, it is analogous to the notion of condition numbe r in numerical linear algebra [12].
The regression function is assumed to belong to a certain target space T, but is not computable in practice because ρis
not known. Therefore, the estimate fzis searched within a hypothesis space Hthat is amenable to perform computations.
To this aim, His chosen as a Reproducing Kernel Hilbert Space (RKHS) with i nner product∝an}bracketle{t·,·∝an}bracketri}htH, and the regression
function is estimated by solving the following Tikhonov reg ularization problem:
fz= arg min
f∈H1
NN/summationdisplay
t=1(yt−f(xt))2+γ∝bardblf∝bardbl2
H.
The key feature in RKHSs is that function evaluation at any po int in the domain Xis well deﬁned by a functional that is
linear and continuous. From Moore-Aronszajn Theorem [26], it results that the RKHS is in one-to-one correspondence wit h
a positive semi-deﬁnite kernel operator (see, e.g., [13, De ﬁnition 2.8])
K:X×X→R
such that the reproducing property holds, i.e.,f(x) =∝an}bracketle{tK(x,·),f∝an}bracketri}htH.These facts, together with Riesz-Frechet theorem (see,
e.g., [31][Chapter V , Theorem 1] ) lead to the so-called Repr esenter Theorem [32], stating that the solution fzis a linear
combination of{K(xt,·)}N
t=1, i.e., of the kernel sections centered at the given input loc ations in X. The same result can
be expressed via the sampling operator SX[33], [14]:
Lemma 1: LetSX:H→RNbe an operator such that SX(f) = [f(x1)... f(xN)]⊤, and consider its adjoint
S⊤
X:RN→HyieldingS⊤
Xc=/summationtextN
t=1ctK(xt,·). Introducing Y= [y1... yN]⊤, we have
fz=/parenleftBig1
NS⊤
XSX+γI/parenrightBig−11
NS⊤
XY.
Proof. Let us begin with the derivation of the expression for S⊤
X. By deﬁnition of adjoint operator, one must have
that∝an}bracketle{tSXf,c∝an}bracketri}ht2=∝an}bracketle{tf,S⊤
Xc∝an}bracketri}htH. Now,∝an}bracketle{tSXf,c∝an}bracketri}ht2=/summationtextN
t=1ctf(xt), which by the reproducing property is equal to/summationtextN
t=1ct∝an}bracketle{tf,K(xt,·)∝an}bracketri}htH=∝an}bracketle{tf,/summationtextN
t=1ctK(xt,·)∝an}bracketri}htH. By inspection of the deﬁnition of adjoint operator, it foll ows that
S⊤
Xc=/summationtextN
t=1ctK(xt,·).Let us now retrieve the expression for fz. Write the objective as
1
N/parenleftBig
∝an}bracketle{tSXf,SXf∝an}bracketri}ht2+∝bardblY∝bardbl2
2−2∝an}bracketle{ty,SXf∝an}bracketri}ht2/parenrightBig
+γ∝an}bracketle{tf,f∝an}bracketri}htH
=/angbracketleftBig/parenleftBig1
NS⊤
XSX+γI/parenrightBig
f,f/angbracketrightBig
H−2
N∝an}bracketle{tS⊤
Xy,f∝an}bracketri}htH+1
N∝bardblY∝bardbl2
2.
Solution follows by taking the functional derivative of the last expression. /squaresolid
The primary question of interest in error analysis is about q uantifying how well fzapproximates fρ. To this aim, we
introduce the data-free limit of fzas
fH= arg min
f∈H/integraldisplay
X(f(x)−fρ(x))2dρX(x)+γ∝bardblf∝bardbl2
H.
Its solution is given by means of the integral operator
LK(f)(¯x) =/integraldisplay
XK(¯x,x)f(x)dρX(x),
and its expression is ([12], II.2, Theorem 3; and III.6, Prop osition 7)
fH= (LK+γI)−1LKfρ.
Having deﬁned fρ,fzandfH, consider the metric ∝bardblfz−fρ∝bardbl♮, where♮indicates the type of norm of interest. The overall
error thus decomposes as
∝bardblfz−fρ∝bardbl♮≤∝bardblfz−fH∝bardbl♮+∝bardblfH−fρ∝bardbl♮.
The ﬁrst addendum is named sample error , and indicates the error within the RKHS Hdue to the fact that we are operating
with a ﬁnite amount of data. The second is called approximation error and arises from the choice of the hypothesis space.
When considering the error in the space of square-integrabl e functions, the two terms are also called variance andbias,
respectively. The choice on the size of Hhas an opposite effect on them: the larger the hypothesis spa ce is, the smaller
the distance from fHtofρcan be; on the other hand, the more complex the model is, the mo re data are required to ﬁt it.
Remark 2: The statistical learning viewpoint was originally juxtapo sed to the so-called sampling theory approach for
function estimation [34], [30]. The ﬁrst can be viewed as a mo re ﬂexible framework to perform error analysis, and compris es
the latter as a special case. Indeed, one could envisage fρas the “true" function to be estimated, assuming data are gen erated
asyt=fρ(xt)+etand having noises with zero mean and variance σ2
ρ(xt). For further comments, please refer to [33, Section
7].
B. Proof of Theorem 1
Deﬁningξt:Z→Hsuch thatξt(·) = (yt−fH(xt))K(xt,·), it holds that EZ[ξt](·) =LK(fρ−fH)(·) =γfH(·).
From this, and recalling the deﬁnition of sampling operator , it follows that fz(x)−fH(x)is equal to [14]
/parenleftBig1
NS⊤
XSX+γI/parenrightBig−1/bracketleftBigg
1
NN/summationdisplay
t=1ξt(x)−EZ[ξ](x)/bracketrightBigg
.
We can now study the L2
ρX−norm of the expression above. Since Xis compact and the measure ρXon it deﬁned is a
probability measure, ∝bardblf∝bardblL2ρX≤∝bardblf∝bardbl∞for any function f∈L2
ρX: therefore,∝bardblfz−fH∝bardblL2ρXis upper bounded by
/vextenddouble/vextenddouble/vextenddouble/parenleftBig1
NS⊤
XSX+γI/parenrightBig−1/vextenddouble/vextenddouble/vextenddouble
∞/vextenddouble/vextenddouble/vextenddouble1
Nn/summationdisplay
t=1ξt−EZ[ξ]/vextenddouble/vextenddouble/vextenddouble
∞.
Since the operator norm can be bounded byCK
γ√
˘λ(the proof is reported at the end of this subsection), we can n ow study an
upper bound for ρN(∝bardblfz−fH∝bardblL2ρX>ǫ)which, for an arbitrary ǫ>0, is
ρN/parenleftBigg/vextenddouble/vextenddouble/vextenddouble1
Nn/summationdisplay
t=1ξt−EZ[ξ]/vextenddouble/vextenddouble/vextenddouble
∞>ǫγ/radicalbig
˘λ
CK/parenrightBigg
. (22)
At an arbitrary input location x∈Xand a given ¯ǫ∈(0,1), Chebychev’s inequality yields
ρN/parenleftBig/vextendsingle/vextendsingle/vextendsingle1
NN/summationdisplay
t=1ξt(x)−EZ[ξ](x)/vextendsingle/vextendsingle/vextendsingle>¯ǫ/parenrightBig
≤var(ξ)(x)
N¯ǫ2,noting that{ξt}N
t=1are independent and identically distributed. Using this re sult, we can further bound (22) as
ρN(∝bardblfz−fH∝bardblL2ρX>ǫ)≤C2
K
γ2˘λ∝bardblvar(ξ)∝bardbl∞
Nǫ2. (23)
The variance term can be bounded as
sup
¯x∈Xvar(ξ)(¯x)≤sup
¯x∈X/integraldisplay
ZK(¯x,x)2(y−fH(x))2dρ≤C4
K/integraldisplay
Z(y−fH(x))2dρ≤B2
f+B2
σ,
where the last inequality follows from the fact that/integraltext
Z(f(x)−y)2dρ−/integraltext
Z(fρ(x)−y)2dρ=∝bardblf−fρ∝bardbl2
L2ρXfor anyf:X→Y
[14], and that∝bardblfH−fρ∝bardbl2
L2ρX+γ∝bardblfH∝bardbl2
H=J(fH)≤J(0) =∝bardblfρ∝bardbl2
L2ρX≤B2
f. Coming back to (23), we have that
ρN/parenleftBig
∝bardblfz−fH∝bardblL2
ρX>ǫ/parenrightBig
≤C6
K
γ2˘λ(B2
f+B2
σ)
Nǫ2=δ. (24)
The proof is concluded by retrieving the expression for ǫfromδin the equality (24). /squaresolid
Proof for operator norm bound: By deﬁnition, we look for a constant C∞is such that, for any u∈H,
∝bardbl(S⊤
XSX/N+γI)−1u∝bardbl∞≤C∞∝bardblu∝bardbl∞. By the reproducing property,/vextenddouble/vextenddouble/vextenddouble/parenleftBig
1
NS⊤
XSX+γI/parenrightBig−1
u/vextenddouble/vextenddouble/vextenddouble
∞= sup¯x∈X/vextendsingle/vextendsingle/vextendsingle/angbracketleftBig/parenleftBig
1
NS⊤
XSX+
γI/parenrightBig−1
u(·),K(¯x,·)/angbracketrightBig
H/vextendsingle/vextendsingle/vextendsingle, which is further upper bounded by CK/vextenddouble/vextenddouble/vextenddouble/parenleftBig
1
NS⊤
XSX+γI/parenrightBig−1/vextenddouble/vextenddouble/vextenddouble
H∝bardblu∝bardblHby Cauchy-Schwartz
inequality and (6). Now, by the bound on the operator norm in Hprovided in [14, Equation 3.5], we have/vextenddouble/vextenddouble/vextenddouble/parenleftBig
1
NS⊤
XSX+
γI/parenrightBig−1
u/vextenddouble/vextenddouble/vextenddouble
∞≤CK
γ∝bardblL−1/2
K∝bardblL2ρX∝bardblu∝bardblL2ρX≤CK
γ∝bardblL−1/2
K∝bardblL2ρX∝bardblu∝bardbl∞. The proof is concluded by deriving the operator norm
for∝bardblL−1/2
K∝bardblL2ρX, which is∝bardblL−1/2
K∝bardblL2ρX≤1//radicalbig
˘λbecause, for an arbitrary f∈L2
ρX,∝bardblL−1/2
Kf∝bardblL2ρX=/radicalBig/summationtextE
i=1α2
i
λi≤/radicalBig
1
˘λ/summationtextE
i=1α2
i≤1√
˘λ∝bardblf∝bardblL2ρX.
C. Proof of Proposition 1
Expressing the regression function as fρ=/summationtext
q∈N¯αq¯ϕqandfHas in (17), we apply the triangle inequality and Parseval’s
Theorem on∝bardblfH−fρ∝bardblL2ρXand obtain
∝bardblfH−fρ∝bardblL2ρX=/vextenddouble/vextenddouble/vextenddoubleE/summationdisplay
i=1λi
λi+γ¯απ
iϕi−/summationdisplay
q∈N¯αq¯ϕq/vextenddouble/vextenddouble/vextenddouble
L2ρX≤/radicaltp/radicalvertex/radicalvertex/radicalbtE/summationdisplay
i=1/parenleftBigg
γ
λi+γ/parenrightBigg2
(¯απ
i)2+/radicalBigg/summationdisplay
q∈N\Q¯α2q.
Let us now focus on the ﬁrst term on the right-hand side. The ﬁr st bound (18) is obtained by considering (λi+γ)−1≤
(˘λ+γ)−1. As for the second, we take ¯απ
i≤∝bardbl¯απ∝bardbl∞, bound the square root of the sum as the sum of the square roots , and
take(λi+γ)−1≤(λi)−1.
D. Proof of Proposition 2
(a) Consider the sample and approximation errors as obtaine d in (16) and (18), respectively. Introducing the following
notation:
A=C3
K/radicalBigg
B2
f+B2σ
Nδ˘λ, b=˘λ, B=/radicaltp/radicalvertex/radicalvertex/radicalbtE/summationdisplay
i=1(¯απ
i)2, C=/radicalBigg/summationdisplay
q∈N\Q¯α2q, (25)
we have that the overall error can be bounded as follows:
∝bardblfz−fρ∝bardblL2ρX≤A
γ+Bγ
b+γ+C=F(γ). (26)
The function F(γ)is always positive for γ >0. We aim at ﬁnding the condition for which there exists a uniqu e, ﬁnite value
ofγminimizing F(γ). To this end, let us study the ﬁrst derivative:
dF
dγ= 0−→γ2(Bb−A)−2Abγ−Ab2= 0. (27)
By applying Descartes’ rule, we obtain that the condition en suring a unique root on the positive real axis is Bb−A>0,
which is (20). Such a condition implies the existence of a uni que ﬂexus on γ >0: this follows from the fact that
lim
γ→+∞F(γ) =B+C,but the claim can be also veriﬁed by applying Descartes’ rule ond2F(γ)
d2γ.
Finally, the optimal γis obtained by solving (27) and has the following expression :
ˆγ(a)=b(A+√
ABb)
Bb−A.
(b) We proceed along the lines of the preceding argument, but considering the approximation error bound as in (19).
Considering the following coefﬁcients:
Aas in (25), D=E/summationdisplay
i=1∝bardbl¯απ∝bardbl∞
λi, (28)
the claim follows by proving that the function F(γ) =A
γ+Dγhas a unique minimum for γ >0. This is shown by studying
the ﬁrst and second derivatives, and using the fact that both AandDare positive.
The resulting optimal γalways exists and takes the following value:
ˆγ(b)=/radicalbigg
A
D.
E. Statement of benchmark Theorems in Section VI
For ease of referencing, we report the statements of [14, The orem 5] and [28, Proposition 20] used in Sections VI-A and
VI-B, respectively.
Theorem 2 ([14], Theorem 5): Letρsatisfy|y|≤Malmost surely. Then for any 0<δ<1
∝bardblfz−fH∝bardblL2ρX≤12CKMlog(4/δ)√Nγ(29)
provided that
γ≥8C2
Klog(4/δ)√
N. (30)
/square
Proposition 3 ([28], Proposition 20): AssumeE[y2]<∞and thatσ2
ρ∈Lp
Xfor some 1≤p≤∞ . Moreover, deﬁne
the effective dimension N(γ) =Tr((LK+γI)−1LK)[35]. Then,
EZ[∝bardblfz−fH∝bardblL2ρX]≤(2+56C4
K+57C2
K)/parenleftBig
1+1
(Nγ)2+N(γ)
Nγ/parenrightBig
×/braceleftBigg
C1
p
K/radicalBig
∝bardblσρ∝bardblp/parenleftBigN(γ)
N/parenrightBig1
2(1−1
p)/parenleftBig1
Nγ/parenrightBig1
2p++CK∝bardblfH−fρ∝bardblL2ρX√Nγ/bracerightBigg
.
/square
F . Comparison with sample error bound in [22]
We now perform another Monte Carlo study in another set-up wh ere noises have unbounded support. We compare
bound (16) with the following:
Theorem 3 ([22], Theorem 1): Assume there exist constants ˜M >0andC >0such that the so-called moment hypothesis
holds: /integraldisplay
Y|y|ℓdρ(y|x)≤Cℓ!˜Mℓ∀ℓ∈N, x∈X. (31)
Furthermore, assume that there exists some 0<β≤1and a constant Cβ>0such that
∝bardblfH−fρ∝bardbl2
L2ρX+γ∝bardblfH∝bardbl2
H≤Cβγβ. (32)
Then, if the kernel Kis inﬁnitely differentiable on X×X, then for any 0<ε<1and0<δ<1, with conﬁdence 1−δ
we have, by taking γ=Nε−1,
∝bardblfz−fH∝bardbl2
L2ρX≤˜CεNε−1log(4/δ)4
ε+2. (33)
/square
Before presenting the details of the numerical experiment, we derive the expressions for C,˜M,Cβand˜Cε. Theorem 3
is a corollary of the following result:Theorem 4 ([22], Theorem 2): Assume the moment hypothesis (31) with constants Cand˜Mholds; moreover, let
condition (32) with 0< β≤1and constant Cβbe valid. Deﬁne N(B1,η)the minimum number of disks with radius
ηthat cover the balls B1={f∈H:∝bardblf∝bardblH≤1}, and assume that Hhas polynomial complexity exponent s>0, i.e.,
logN(B1,η)≤C0/parenleftBig1
η/parenrightBigs
. (34)
If0<ε<β
1+s, then by taking γ=Nε
β−1
s+1, for any 0<δ<1, with conﬁdence 1−δwe have
∝bardblfz−fρ∝bardbl2
L2ρX≤˜CεNε−β
s+1/parenleftBig
log/parenleftBig4
δ/parenrightBig/parenrightBigβ(1+β)
(s+1)ε+2
. (35)
The constant ˜Cεcan be computed as follows:
˜Cε=C5
ε2C4β
ε(s+1)
2/parenleftBig
1+log/parenleftBig
1+2
ε(s+1)/parenrightBig/parenrightBigβ(1+β)
ε(s+1)+2
,
where we have 

C5= 38Cβ+2(C1+322(C+1)2)C2
4(2/(s+1))2+480(CK+1)2Cβ
C4=˜M(2CK(C+(1+2√
2C)+1))+C3
C3=/radicalbig38Cβ+(CK+1)/radicalbig480Cβ+˜M
C2=/radicalbig
2[C1+322(C+1)2]
C1= 6CK+6C+8(1+√
2C)/˜M+520(CK+C+2(C+1))2(C0+1).(36)
/square
The choice in [22] to obtain the statement of Theorem 3 from th e result above presented is to assume that fρbelongs to
the hypothesis space H, and setting β= 1 ands=ε
1−εwith0<ε<1/2(and then rescaling 2εtoε). Our task is now
to obtain explicit values for the constants C,˜M,CβandC0. The bound for CKis given in (6).
Constants for moment hypothesis (31) (Cand˜M):Following Example 1 in [22], we obtain that the moment hypoth esis
(31) is satisﬁed for C= 4 and˜M= max{√B0,B∞}, whereB0=B2
σ(see Assumption 1) and B∞≥∝bardblfρ∝bardbl∞. Note that
B∞<∞becausefρbelongs to H.
Constant entering bound (32) (Cβ):We evaluate the cost (11) at fH= (I+γL−1
K)−1fρ. We then obtain
∝bardblf−fρ∝bardbl2
L2ρX+γ∝bardblL−1/2
Kf∝bardbl2
L2ρX=∝bardbl[(I+γL−1
K)−1−I]fρ∝bardbl2
L2ρX+γ∝bardblL−1/2
K(I+γL−1
K)−1fρ∝bardbl2
L2ρX
=E/summationdisplay
i=1/bracketleftBig/parenleftBigλi
λi+γ−1/parenrightBig2
+γ/parenleftBig√λi
λi+γ/parenrightBig2/bracketrightBig
(¯απ
i)2
=γE/summationdisplay
i=11
λi+γ(¯απ
i)2=γE/summationdisplay
i=1/parenleftBigλβ
i
λi+γ/parenrightBig
λ−β
i(¯απ
i)2
for anyβin(0,1]. We can bound such an expression as follows (see Theorem 3, Ch apter II.2 in [12] for the whole derivation):
γE/summationdisplay
i=1/parenleftBigλβ
i
λi+γ/parenrightBig
λ−β
i(απ
i)2≤γ/parenleftBig
sup
ττβ
τ+γ/parenrightBig
∝bardblL−β/2
Kfρ∝bardbl2
L2ρX≤γβ∝bardblL−β/2
Kfρ∝bardbl2
L2ρX.
Therefore, the constant Cβis obtained by bounding ∝bardblL−β/2
Kfρ∝bardbl2
L2ρX. In the case β= 1, which is of our interest, a solution
could beCβ=1≤B2
f/˘λ.
Constant for polynomial complexity (34) (C0):Since Kin (5) is inﬁnitely differentiable, condition (34) is known to
hold for any s >0; however, since we are considering s=ε
1−εwith0< ε <1/2, we are restricting our attention to
0<s<1.
Since His ﬁnite-dimensional with dimension E, Theorem 5.3 [13] gives that N(B1,η)≤(1+2/η)E. Taking logarithms
on both sides, we obtain
logN(B1,η)≤Elog/parenleftBig
1+2
η/parenrightBig
≤2EG(s)/parenleftBig1
η/parenrightBigs
.
We now need to ﬁnd G(s). Its expression is summarized in the following result.
Lemma 2: Considerx>0. In order to have log(1+x)<G(s)xsfor0<s<1, it has to hold that
G(s)>/parenleftBig1−s
s/parenrightBig1−s
.Proof. Consider the function g(x) =G(s)xs−log(1+x). We want it to be always positive for x>0. Sinceg(0) = 0 , this
amounts to imposing thatdg(x)
dxis always positive. So, studying the ﬁrst derivative, we hav e
dg(x)
dx=G(s)s(x+1)−x1−s
x1−s(1+x)>0−→x
(1+x)1
1−s</parenleftBig
G(s)s/parenrightBig1
1−s.
The claim follows by maximising the term on the left-hand sid e. /squaresolid
We are now ready to perform the numerical test. We randomize b oth on the regression function and on the number
of data-points. The ﬁrst is drawn in the same way as in Section VI-A, while data-set cardinalities Nare drawn from
the set{300,315,...,6990}. As in Section VI-A, the SNR is set to 150, but noises are now di stributed as Gaussian. The
hyperparameters{λi}20
i=1ruling both the sampling of ¯αqand the hypothesis space Hare all set to 10. The numerical
values of the sample error bounds are computed with γset as in the statement of Theorem 3. For each Monte Carlo run, we
evaluate the bounds with εin the grid{0.05,0.1,...,0.95}.To compare the numerical values of the bounds, the score we
consider is the difference between bound and true sample err or, divided by the true sample error. The results are display ed
in Figure 2. From (33) it is clear that the convergence rate in the number of data Nis better compared with that presented
in (16); however, the numerical values returned by the choic e ofγare extremely conservative.
0 0.2 0.4 0.6 0.8 1050100150200
εLogarithm of mean sample error relative difference
Theorem 1
Wang,Zhou 2011
Theorem 1 [Wang,Zhou 2011]5101520
ε= 0.95Logarithm of sample error relative difference
Fig. 2: Statistics of the difference between bound and true s ample error, normalized by true sample error, in logarithmi c scale. Top panel:
behaviour of the mean value over the Monte Carlo iterations a s a function of ε. Bottom panel: boxplots over the Monte Carlo runs for
the caseε= 0.95. The values (in logarithmic scale) returned by Theorem 1 are 6.238±4.694, while the ones obtained using Theorem
3 are21.961±21.421.
G. Additional results for the tests in Section VI-C
We recall that the numerical values chosen for the Monte Carl o test were the following:
•X= [−5×104,5×104];
•δ= 0.5;
•Regression function fρ=/summationtext
q¯αq¯ϕqcharacterized by 30 sine/cosine pairs {¯ϕ}30
q=1, where each qis randomly selected
without repetitions from the set {1,...,100};
•all components of ¯α∈R30are independent samples from a Gaussian distribution with z ero mean and variance λ= 1;
•in the hypothesis space, λi=λfor alli= 1,...,E ;
•Qis selected as a random subset with cardinality 10 from the se t of frequencies characterising the regression function;
•SNR = 50;
•at each iteration, we draw a random regression function and s elect the basis functions of H;
•N= 2500.
We further display the results reported in Table I in Figure 3 . In each boxplot, we consider the difference between the
bound and the true error, and we normalize it by the latter.
γ(a) γ(b)2040
SNR=50γ(a) γ(b)012
SNR=50
Fig. 3: Boxplots of the normalized, relative differences be tween bounds and true values. Left panel: results for the sam ple error; right
panel: results for the approximation errors. These complem ent the results presented in Table I, Section VI-C.H. The beneﬁts of regularization: the case of additive noise model
We now discuss the performance of the estimation scheme prop osed in (9) with respect to the one that would have been
obtained without regularization. We carry out the analysis assuming an additive noise model, and regarding the regress ion
function as the "true" function to be estimated from data. In this setting, the measurements model for each t= 1,...,N is
yt=φ⊤(xt)¯απ+r(xt)+et, where the ﬁrst term is given by the projection of the regress ion function on the subspace spanned
by the basis functions {ϕi(·)}i}E
i=1deﬁned in (4) and entering φ⊤(·); the second collects the contribution of frequencies
that have not been included in the subspace and is assumed to b e bounded; the last one is the additive noise, which is
assumed to be i.i.d. with zero mean and known variance σ2. The overall estimation problem to be solved reads as follow s:
ˆα= arg min
α∈RE∝bardblY−Φα∝bardbl2+Nσ2α⊤P−1α, (37)
whereΦ∈RN×Estacks all{φ⊤(xt)}N
t=1,Yis deﬁned as the one entering (10), and P−1is a regularization matrix. The
solution (37) reads as
ˆα= (Φ⊤Φ+Nσ2P−1)−1Φ⊤Y. (38)
Our goal is to discuss the performance of such an estimator wh en considering P−1=γΣ−1
α
σ2, which directly relates to (9),
as a function of γ. To do so, we consider M(γ) =Ee[∝bardblˆα−¯απ∝bardbl2], whereEe[·]denotes expectation with respect to the
noise distribution, as a performance score. Carrying out th e computations with the particular choice of the regularize r above
introduced, and deﬁning r= [r(x1),...,r(xN)]⊤and˜Σα= Σα/N, it turns out that
M(γ) =Tr/parenleftBigg
(γ˜Σ−1
α+Φ⊤Φ)−1/bracketleftBig
σ2Φ⊤Φ+γ2˜Σ−1
α¯απ(¯απ)⊤˜Σ−1
α+Φ⊤rr⊤Φ+2γ˜Σ−1
α¯απr⊤Φ/bracketrightBig
(γ˜Σ−1
α+Φ⊤Φ)−1/parenrightBigg
.(39)
Note thatM(0) = Tr((Φ⊤Φ)−1[Φ⊤rr⊤Φ+σ2Φ⊤Φ](Φ⊤Φ)−1). Denoting with Rthe expression in square brackets in (39),
we obtain
dM(γ)
dγ= 2Tr/parenleftBigg
(γ˜Σ−1
α+Φ⊤Φ)−1/bracketleftBig
γ˜Σ−1
α¯απ(¯απ)⊤˜Σ−1
α+˜Σ−1
α¯απr⊤Φ−˜Σ−1
α(γ¯Σ−1
α+Φ⊤Φ)−1R/bracketrightBig
(γ˜Σ−1
α+Φ⊤Φ)−1/parenrightBigg
.
Studying its limit as γ→0+, one has
2Tr/parenleftBigg
(Φ⊤Φ)−1˜Σ−1
α/bracketleftBig
¯απr⊤Φ−(Φ⊤Φ)−1(Φ⊤rr⊤Φ+σ2Φ⊤Φ)/bracketrightBig
(Φ⊤Φ)−1/parenrightBigg
. (40)
The only case that is easy to study occurs when there is no resi dual term (i.e., the regression function belongs to the
hypothesis space, so that r(x) = 0 for allx). In that scenario, the expression above is clearly negativ e, and this proves the
fact thatM(γ)<M(0)at least in some small neighbourhood of the origin: see also P roposition 2 in [36].
To numerically test the impact of regularization, we perfor m a Monte Carlo study of 500 trials. We consider an input
domain X= [−25,25], and a regression function characterized by 50 basis functi ons randomly selected among the ﬁrst
80 (ordered with increasing qin (3)), linearly combined by a vector drawn from a Gaussian d istribution with zero mean,
i.i.d. components and variance λ= 10 . The latter hyper-parameter also enters the deﬁnition of H; the set of frequencies
Qdeﬁning it is a sample of random dimension E/2in{5,6,...,50}, selected among the ones deﬁning fρ. Thus, in this
testing situation the approximation error ruled by the resi dualr(·)is different from 0. We consider an SNR of 100 yielded
by a Gaussian, zero-mean, i.i.d. noise. On each run, the numb er of dataNis randomly selected in the interval {5,...,E/2}.
For the regularization parameter, we both use γ(b)as in Proposition 2(b) and a value ˆγestimated via marginal likelihood
optimization performed with a Gibbs sampling scheme levera ging the Bayesian interpretation of the problem in (37) ([37 ,
Chapter 1]; see also [38] for a thorough discussion on the rob ustness of the marginal likelihood hyper-parameter estima tion.
More details are reported below). We then study the values of the overall error ¯M(γ) =∝bardblfρ−fz∝bardblL2ρX, wherefz(·) =φ⊤ˆα
withˆαas in (38), attained by γ(b),γ= 0 andˆγ, compared to the oracle value corresponding to γ=γ∗computed by
grid search. The results are summarized in Figure 4. We note t hat the two regularized estimators yield comparable result s,
meaning that γ(b)is a good estimator of ˆγ(and is faster to be computed); and that both always outperfo rm the case with
no regularization involved. Speciﬁcally, we obtain that th e relative discrepancy between, e.g., the case with γ= 0 andˆγ
has a median value of 24.99%, with minimum and maximum values equal to 0.64% and66.97%, respectively.
We conclude by providing the details for the selection rule f or hyper-parameter γused as baseline in the Monte Carlo
test. Let us consider the estimation problem in (37), i.e.,
ˆα= arg min
α∈RE∝bardblY−Φα∝bardbl2+Nσ2α⊤P−1α= (Φ⊤Φ+Nσ2P−1)−1Φ⊤Y, (41)ˆγ γ= 0 γ(b)012
Values ofγ
Fig. 4: Relative discrepancy 100%(¯M(ˆγ)−¯M(γ∗))/¯M(γ∗)for the regularized and non-regularized cases.
whereΦ∈RN×Eis the matrix that stacks all {φ⊤(xt)}N
t=1(i.e., the row vectors containing the basis functions {ϕi(·)}E
i=1
as deﬁned in (4)), P−1is a regularization matrix, and Yis the output measurements vector [y1,...,yN]⊤.
The objective in (41) admits a stochastic interpretation. C onsider a measurements model Y= Φα+E, whereEis an
N-dimensional Gaussian vector with zero mean and known covar ianceσ2IN, andα∈REis unknown. Taking the Bayesian
viewpoint,αis modelled as a random vector; speciﬁcally, assume it is Gau ssian, with zero mean and covariance P/N .
In this set-up, the objective in (41) is (apart from constant s not depending on α) the negative logarithm of the posterior
probabilityα|Y: that is, ˆαis computed as the Maximum a Posteriori (MAP) estimate, whic h corresponds to the minimum
variance linear estimate in the Gaussian case we are conside ring. In fact, the solution to (41) is indeed the expression o f the
posterior mean.
Within this framework, assume that (some of the) hyper-para meters entering P−1, orσ2, are unknown, and collect them in a
vectorη. A possible strategy consists in estimating them from data, leveraging the so-called empirical Bayes approach [39]:
in particular, the estimate for ηis computed by maximising the evidence Y|η(or, more conveniently, minimizing its negative
logarithm), which is equivalent to the joint distribution Y,α|ηwhere the dependence from αis integrated out. In the Gaussian
case, this reads as
ˆη= argmin
ηY⊤ΣY(η)−1Y+logdetΣ Y(η), (42)
whereΣY= ΦPΦ⊤/N+σ2IN. This problem is non-convex, and deterministic optimizati on routines might return unreliable
results due to their sensitivity to initial conditions. A wa y to overcome this issue consists in resorting to Markov Chai n
Monte Carlo (MCMC). Speciﬁcally, the idea is to run a (single -component) Metropolis-Hastings algorithm to construct
a Markov chain whose invariant distribution is (proportion al to) the marginal likelihood of interest: this is a mechani sm
to draw samples from such a distribution, and solve (42) in sa mple-based form. For an introduction to MCMC we refer to [37] .
Let us now relate (41) to the original function estimation pr oblem stated in (9) to provide an expression for P−1, and
detail the MCMC-based procedure for marginal likelihood op timization. The function is estimated as
fz= arg min
f∈H1
NN/summationdisplay
t=1(yt−f(xt))2+γ∝bardblf∝bardbl2
H.
According to the deﬁnitions (7) and (5) specifying the RKHS H, one can write fz(·) =φ⊤(·)ˆα. In this way, estimating
fz(·)translates into solving (41) for the particular choice P−1=γΣ−1
α
σ2, whereΣαis given by the choice of the hypothesis
space (see, e.g., (5)), and γis the positive scalar to be tuned.
Let us now detail the MCMC procedure for the scenario above sp eciﬁed. First, denoting with p(·)the probability density
function of interest, we note that
p(α,γ|Y) =p(α|γ,Y)p(γ|Y)∝p(α|γ,Y)p(Y|γ),
which tells us that drawing samples from α,γ|Yis a suitable way to explore the marginal likelihood. Theref ore, we set
up a Gibbs sampler (a particular case of single-component Me tropolis-Hastings algorithm) whose invariant distributi on has
densityp(α,γ|Y) =π(α,γ). In this particular scenario, samples from the full conditi onals are easy to be computed: in fact,
denoting with ˜Σα= Σα/Nand with Γ(a,b)a Gamma distribution with mean a/b, using conjugate distributions properties
we obtain
π(α|γ)←−α|γ,Y∼N/parenleftBig
(˜Σ−1
α+Φ⊤Φ/γ)−1Φ⊤Y/γ,(˜Σ−1
α+Φ⊤Φ/γ)−1/parenrightBig
(43)
π(γ|α)←−γ|α,Y∼Γ/parenleftBigN
2,∝bardblY−Φα∝bardbl2
2/parenrightBig
. (44)
Finally, the last Ng< NGsamples obtained from the Gibbs sampler can be used to comput eˆγmaximising the marginal
likelihood.