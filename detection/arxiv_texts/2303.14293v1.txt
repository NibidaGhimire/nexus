arXiv:2303.14293v1  [cs.LG]  24 Mar 20231
Efﬁcient Lipschitzian Global Optimization of
H¨older Continuous Multivariate Functions
Kaan Gokcesu, Hakan Gokcesu
Abstract —This study presents an effective global optimization
technique designed for multivariate functions that are H ¨older
continuous. Unlike traditional methods that construct low er
bounding proxy functions, this algorithm employs a predete r-
mined query creation rule that makes it computationally sup erior.
The algorithm’s performance is assessed using the average o r
cumulative regret, which also implies a bound for the simple
regret and reﬂects the overall effectiveness of the approac h. The
results show that with appropriate parameters the algorith m
attains an average regret bound of O(T−α
n)for optimizing a
H¨older continuous target function with H ¨older exponent αin
ann-dimensional space within a given time horizon T. We
demonstrate that this bound is minimax optimal.
I. I NTRODUCTION
The challenge of ﬁnding the global optimum of a function
while minimizing the number of evaluations is a well-known
problem in optimization theory. Pinter [1] introduced the
concept of global optimization, which has become a key
technique in many applications, including hyper-paramete r
tuning in complex learning systems [2], [3]. However, in suc h
systems, the objective function may not exhibit properties
that facilitate optimization, such as convexity. As a resul t,
several terms, including derivative-free optimization [4 ], black-
box optimization [5], and global optimization [1], have bee n
used to refer to the sequential optimization of unknown and
potentially non-convex functions.
The problem of global optimization has garnered signiﬁcant
attention in a variety of research ﬁelds, including non-con vex
optimization [6]–[8], Bayesian optimization [9], convex o pti-
mization [10]–[12], bandit optimization [13], and stochas tic
optimization [14], [15]. Moreover, global optimization ca n
be readily applied to various real-world applications, suc h as
distribution estimation [16]–[18], multi-armed bandits [ 19]–
[21], control theory [22], signal processing [23], game the ory
[24], prediction [25], [26], decision theory [27], and anom aly
detection [28]–[30]. The importance of global optimizatio n
has grown signiﬁcantly in recent years due to its widespread
applicability across different research domains and its po tential
to address complex problems that are difﬁcult to solve with
traditional optimization methods.
Global optimization techniques aim to optimize the ob-
jective function f(·)by accessing only its evaluations at
speciﬁc query points. Several heuristics can be employed to
achieve this goal, but we focus on regularity-based approac hes
[31], [32], with a speciﬁc emphasis on Lipschitz (and its
generalization H¨ older) regularity. The Lipschitz regula rity ap-
proach was ﬁrst introduced by Piyavskii to optimize univari ate
Lipschitz continuous functions. The key idea is to construc t
lower bounding proxy functions for the objective and querytheir minimum [33]. Shubert developed a similar method
[34], which is commonly referred to as the Piyavskii-Shuber t
algorithm.
The Piyavskii-Shubert algorithm for Lipschitz continuous
functions has been extensively studied and improved upon
over the years for various applications [7], [8], [35]–[41] . Over
time, the algorithm has been extended to multivariate funct ions
using Taylor expansions [42], and its performance has been
accelerated in [43]. Other approaches have also been propos ed,
such as using smooth auxiliaries [44] and a variant for
differentiable univariate functions presented in [45]. Br ent’s
method, proposed in [46], is suitable for functions deﬁned o n
a compact interval with a bounded second derivative. Recent
work has also focused on univariate global optimization for
functions with generalized Lipschitz regularities [47].
Algorithms for global optimization are evaluated based
on their convergence to the optimal value of the objective
function, which is measured using simple regret [48]. This
metric quantiﬁes the difference between the value of the bes t
query made so far and the value of the optimal solution.
Forn-dimensional Lipschitz continuous objective functions,
the Piyavskii-Shubert algorithm has a regret bound of ˜rT=
O(T−1
n)[37]. For univariate functions, [7] proposed a variant
of the Piyavskii-Shubert algorithm that achieves a simple
regret of ǫwithinO(/integraltext1
0(f(x)−f(x∗) +ǫ)−1dx)queries,
which improves upon the previous result of [48]. Similarly,
[45] improves upon the previous results of [7], [48] for
univariate functions. LIPO, a variant of the Piyavskii-Shu bert
algorithm, achieves better simple regret bounds under stro nger
assumptions [49]. Finally, [50] investigates the simple re gret
of the Piyavskii-Shubert algorithm under noisy evaluation s.
The Piyavskii-Shubert algorithms have been analyzed for
their effectiveness in cumulative regret bounds for univar iate
functions, which is a more stringent measure than simple reg ret
[47]. While these methods work well for Lipschitz continuou s
functions, the expense of optimizing lower bounding functi ons
to determine queries can be substantial. To address this iss ue,
a recent study [51] uses pre-determined sampling sets and
achieves similar cumulative regret bounds to the Piyavskii -
Shubert algorithms for univariate cases. However, these me th-
ods are only applicable to objective functions with a single
argument. Another work extends this approach to multivaria te
Lipschitz continuous functions [52]. We extend that approa ch
for use in general H¨ older continuous functions.
In Section II, we provide some preliminaries including the
problem deﬁnition. In Section III, we provide the algorithm
and its implementation. In Section IV, we provide its cumu-
lative regret analysis and show that minimax optimal regret is
achieved with suitable parameter selections.2
II. P RELIMINARIES
We begin by providing the formal deﬁnition of the problem
of global optimization in the multivariate case. The object ive
is to minimize a function f(·)with multiple variables, where
f(·) : Ω→ ℜ,andΩis a compact subset of ℜn. In this
study, we focus on the case where Ωcorresponds to the unit
n-dimensional cube, denoted as Ω≡[0,1]n. Despite f(·)
not being convex, it is not an arbitrary function and typical ly
exhibits some regularity, such as Holder continuity.
Deﬁnition 1. Letf(·)satisfy the following regularity:
|f(x)−f(y)| ≤C/bardblx−y/bardblα,
for anyx,y∈Ω, where1> α >0is the H ¨older exponent,
C >0is the regularity constant /bardbl·/bardblis the Euclidean norm.
We adopt an iterative approach to minimize the multivariate
function f(·)by iteratively selecting queries based on their
evaluations, which are obtained from the previously evalua ted
queries. The function responsible for selecting the next qu ery
point is denoted by Γ(·)and is usually derived from Piyavskii-
Shubert variants. The aim is to identify the optimal points
with the minimum possible number of evaluations. This is
equivalent to treating the objective function as a loss func tion,
where each evaluation f(xt)represents a loss incurred when
selecting the query point xt∈Ω. To analyze our approach’s
performance, we use the concept of regret, which compares
our evaluations with the optimal evaluation. Speciﬁcally, we
deﬁnex∗as the global minimizer of f(·)and use it to compare
our selections’ evaluations. As f(·)may have arbitrarily high
evaluations, we use regret to compare the evaluations of our
selections with the optimal evaluation.
There are two types of regret analysis for a time horizon
Tin global optimization problems. The ﬁrst is simple regret,
which only takes into account the selected points as incurri ng
a loss and not the queried points. The selected point ˜xTis
chosen from the queried points {xt}T
t=1and its evaluation
is compared to the minimum possible evaluation of f(·),
resulting in the simple regret, ˜rT=f(˜xT)−minx∈Ωf(x).
The second type of regret analysis is average regret, which
considers all queried points as contributing to the loss. Th e
average regret is calculated as the difference between the
average evaluation of the queried points and the minimum
evaluation of f(·),
rT=1
TT/summationdisplay
t=1f(xt)−min
x∈Ωf(x)
.
Although evaluating an algorithm’s convergence to the
optimal evaluation or simple regret can be useful in some pro b-
lem scenarios, it is may not suitable for global optimizatio n
problems due to their inherent difﬁculty. This is illustrat ed by
the fact that while the basic grid search method’s simple reg ret
is minimax optimal, its average regret remains constant [52 ].
Moreover, a bound on the average regret implies a bound on
the simple regret. Hence, we will focus on using average regr et
for evaluating algorithm performance. Our aim is to develop
an algorithm that attains a minimax optimal average regret o f
O(T−α
n).III. T HEALGORITHM
In this section, we propose an algorithm for minimizing a
non-convex objective function f(·)subject to the constraint de-
ﬁned in Deﬁnition 1. Although our optimization is performed
on ann-dimensional unit cube [0,1]n, it can be extended
to work with arbitrary cubes of varying sizes and positions
through appropriate scaling and translation of inputs. Unl ike
traditional global optimization algorithms such as Piyavs kii-
Shubert variants, which use lower bounding proxy functions ,
our algorithm employs a ﬁxed query construction rule that
improves efﬁciency. The algorithm is outlined as follows:
1) Set the parameter C0as input.
2) Wrap the domain Ω = [0,1]ninto a hyper-rectangle
Θ = [0,θn−1]×[0,θn−2]×...×[0,θ0], whereθ= 21
n.
Extendf(·)analytically by evaluating any ˜x∈Θafter
projecting it onto Ω, i.e.,f(˜x) =f(argminx∈Ω/bardbl˜x−x/bardbl),
which truncates its elements by 1.
3) Set the middle point xa={θ−1,θ−2,...,θ−n}and its
edge vector va={θ−1,θ−2,...,θ−n}. Sample xaand
set its evaluation as fa=f(argminx∈Ω/bardblx−xa/bardbl).
4) Determine two candidate points xb,xcwith their edge
vectorsvb,vc, and their scores sb,sc, usingxa,va, and
faas inputs. Add the candidates to the list of potential
queries.
5) Sample the candidate with the lowest score s′, remove
it from the list, and set it as the query x′. Evaluate f′as
f(argminx∈Ω/bardblx−x′/bardbl), and set its edge vector as v′.
6) If the number of queries is less than T, go to Step 4
with the inputs xa=x′,va=v′,fa=f′. Otherwise,
stop and return all the queries and their evaluations.
Remark 1. The algorithm has the following characteristics:
•The algorithm maintains the parameters of the potential
query points until they are evaluated.
•It generates two new potential queries after each query.
•The number of potential queries increases linearly with
the number of queries made.
•Several stopping criteria can be considered based on the
speciﬁc problem and available computational resources.
For instance, the algorithm can terminate after a ﬁxed
number of queries or when a certain level of accuracy is
achieved.
•Each potential query can be represented as a binary
string, which is compact, efﬁcient, and allows for easy
storage and communication of potential queries between
different parts of the algorithm.
The selection of potential query points in this algorithm
differs from that of popular Piyavskii-Shubert variants. R ather
than constructing proxy lower bounding functions that pass
through the sampled points and minimizing them, the can-
didate query points are determined by dividing the hyper-
rectangle deﬁned by the sampled point x′and its edge vector
v′along its largest dimension into two equally sized hyper-
rectangles. The center points of these newly created hyper-
rectangles serve as the potential new queries. The exact
expression for these points is provided below.3
Deﬁnition 2. For an objective function f(·), given the queried
pointxaand its edge vector va; the potential queries xb,xc
and their edge vectors vb,vcare given by
xb=xa+z, v b=va−z,
xc=xa−z, v c=va−z,
wherezis all- zero except at I= argmini∈{1,2,...,n}va(i),
whereva(i)denotes the ithelement of va. Hence,
z(I) = 0.5va(I), z (i) = 0,i/ne}ationslash=I.
Remark 2. The scores sbandscare created as the following
sb=fa−C0/bardblva/bardbl, s c=fa−C0/bardblva/bardbl.
Remark 3. We point out that because of the working structure
of the algorithm, if all the scores were offset by some ǫ, the
sampled points would not change.
Lemma 1. For the input C0and the regularity in Deﬁnition 1
with its respective Candα, we have
ǫ0≤Cα
α−1
0C−1
α−1,
whereǫ0is the minimum ǫsuch that for all possible x,y∈Ω,
ǫ≥C/bardblx−y/bardblα−C0/bardblx−y/bardbl.
Proof. In terms of the parameters C0,C,α; an equivalent
representation is the following:
ǫ0= argmin
ǫ∈ℜǫ:ǫ≥C△α−C0△,0≤ △ ≤√n(1)
To solve for ǫ, we need to investigate the derivative of C△α
with respect to △, which is Cα△α−1. LetC0=Cα△α−1
0,
for some △0∈ ℜ, where△0=/parenleftbigC0
Cα/parenrightbig1
α−1.Then, we have
ǫ0+C0△0=C△α
0.Hence,
ǫ0=C/parenleftbiggC0
Cα/parenrightbiggα
α−1
−C0/parenleftbiggC0
Cα/parenrightbigg1
α−1
(2)
=Cα
α−1
0C−1
α−1/parenleftBig
αα
1−α−α1
1−α/parenrightBig
(3)
≤Cα
α−1
0C−1
α−1, (4)
which concludes the proof.
Lemma 2. For an objective function f(·)that satisﬁes
Deﬁnition 1, when the point xais queried with its evaluation
fa; with the scores sbandscfor the candidate points xb,xc
with their respective edge vectors vb,vc; we have
sb−ǫ0≤fa−C/bardblva/bardblα,
sc−ǫ0≤fa−C/bardblva/bardblα,
i.e., the translated scores sb−ǫ0andsc−ǫ0lower bound the
evaluation of the respective regions of the queries xbandxc.
Proof. A pointxbwith its edge vector vbcovers the hyper-
rectangle region Hbwhose center is xbwhere the individual
distance to the boundary planes are given by its edge vector vb.
Because of the regularity in Deﬁnition 1, we have for x∈ Hb;
f(x)≤fa−C/bardblx−xa/bardblα≤fa−C/bardblva/bardblα.Similar arguments
follow for xc,vcandHc; which concludes the proof.
In the next section we provide the performance analyses of
our algorithm.IV. R EGRET ANALYSIS
We start the regret analysis by bounding the regret of a
single sampled point.
Lemma 3. For a given objective function f(·)that satisﬁes
Deﬁnition 1, let us sample the point xbwith the respective
scoresbthat was created after the query xawith its evaluation
fa. When the evaluation of the point xbisfb, we have the
following result.
fb−min
x∈Ωf(x)≤C0θ/bardblvb/bardbl+C/bardblvb/bardblα+ǫ0, (5)
wherevbis the associated edge vector of the query xb.
Proof. We know that the score of query xb, i.e.,sb, completely
lower bounds the evaluations in the hyper-rectangle Hbthat
is associated with the center point xband the edge vector vb,
i.e.,sb−ǫ0≤minx∈Hbf(x).By the design of the algorithm,
the query list consists of a set of potential queries Xwhose
associated regions Hare disjoint and their union covers up the
whole search domain. Hence, we have sb−ǫ0≤minx∈Ωf(x).
Combining with Lemma 2, we have
fa−C0/bardblva/bardbl−ǫ0≤min
x∈Ωf(x). (6)
Moreover, because of the regularity from Deﬁnition 1, we hav e
fb≤fa+C/bardblvb/bardblα, (7)
where the query xbis created after the sampling of xawith
its evaluation fa. Thus, combining the two results, we get
fb≤min
x∈Ωf(x)+C0θ/bardblvb/bardbl+C/bardblvb/bardblα+ǫ0, (8)
sinceθ/bardblvb/bardbl=/bardblva/bardblby design, which concludes the proof.
This lemma bounds the individual regret of a sampled
point in the algorithm with its boundary values and their
corresponding functional values, and is a worst case bound
Lemma 4. The algorithm has the following cumulative regret
T/summationdisplay
t=1ft−T/summationdisplay
t=1f(x∗)≤C0θT/summationdisplay
t=1/bardblvt/bardbl+CT/summationdisplay
t=1/bardblvt/bardblα+Tǫ0.
for the queries points {xt}T
t=1, where {ft}T
t=1are their
evaluations and {vt}T
t=1are their edge vectors, respectively.
Proof. We run the algorithm for Tsampling times. Let this
samples be {xt}T
t=1with their respective edge vectors {vt}T
t=1
and evaluations {ft}T
t=1. Because of Lipschitz continuity of
Deﬁnition 1, for the initial sampling, we have
f1−f(x∗)≤C/bardblv1/bardblα. (9)
For the other samplings, i.e., t≥2, we have from Lemma 3
ft−f(x∗)≤C0θ/bardblvb/bardbl+C/bardblvb/bardblα+ǫ0. (10)
Thus, the cumulative regret is bounded by
T/summationdisplay
t=1ft−T/summationdisplay
t=1f(x∗)≤C0θT/summationdisplay
t=1/bardblvt/bardbl+CT/summationdisplay
t=1/bardblvt/bardblα+Tǫ0,
(11)
which concludes the proof.4
The cumulative bound is dependent on the sum of powered
edge vector norms. To this end, we have the following result.
Lemma 5. We have
T/summationdisplay
t=1/bardblvt/bardblβ≤θn−β
θn−β−1VβTn−β
n,
whereV=/bardblv1/bardblfor anyβ.
Proof. From the construction of the algorithm, with each new
sampling, the largest element in the edge vector of the sampl ed
point is halved. Since, at the beginning of the algorithm, th e
initial edge vector starts as v1={θ−1,θ−2,...,θ−n},where
θ= 21
n; the norm of the edge vector multiplicatively decreases
by21
n. Thus, in the worst-case scenario, we will have
T/summationdisplay
t=1/bardblvt/bardblβ≤K−1/summationdisplay
i=02iVβ
2iβ
n+MVβ
2Kβ
n, (12)
where2K−1+M=T,2K≥M≥1andV=/bardblv1/bardbl. Using
M≤2K, we have
T/summationdisplay
t=1/bardblvt/bardblβ≤K/summationdisplay
i=02iVβ
2iβ
n, (13)
≤VβK/summationdisplay
i=02i(n−β
n)(14)
≤Vβ2(K+1)n−β
n
2n−β
n−1(15)
≤θn−β
θn−β−1VβTn−β
n, (16)
which concludes the proof.
By using the result of Lemma 5 for β∈ {α,1}, we have
the following result.
Theorem 1. WhenC0=λ0CVα−1T1−α
nfor some λ0. We
have the following cumulative regret bound
T/summationdisplay
t=1ft−T/summationdisplay
t=1f(x∗)≤O/parenleftBig
(1+λ0+λα
α−1
0)CVαTn−α
n/parenrightBig
Proof. From Lemma 4 and Lemma 5, we have
T/summationdisplay
t=1ft−T/summationdisplay
t=1f(x∗)≤C0θT/summationdisplay
t=1/bardblvt/bardbl+CT/summationdisplay
t=1/bardblvt/bardblα+Tǫ0,
(17)
≤C0θn
θn−1−1VTn−1
n
+Cθn−α
θn−α−1VαTn−α
n
+Cα
α−1
0C−1
α−1T (18)
≤O/parenleftBig
(1+λ0+λα
α−1
0)CVαTn−α
n/parenrightBig
,
(19)
which concludes the proof.
Next, we show that when λ0is constant, we have a minimax
optimal cumulative regret.Theorem 2. Whenλ0is constant, the result in Theorem 1 is
minimax optimal.
Proof. Suppose, we have an objective function f(·)whose
value is globally optimal at some point x∗, i.e.,f(x∗) =
minx∈Ωf(x). Suppose, the function is such that, for some
ǫ >0, theǫ-neighborhood of the global optimum is decreases
with the condition in Deﬁnition 1 and constant with 0every-
where else, i.e.,
f(x) =/braceleftBigg
C(/bardblx−x∗/bardblα−Cǫα),/bardblx−x∗/bardbl ≤ǫ
0, /bardblx−x∗/bardbl> ǫ.(20)
Let an algorithm sample the points {xt}T
t=1. Its cumulative
regret will be given by
RT=T/summationdisplay
t=1(f(xt)−f(x∗)) (21)
Suppose, f(·)is such that all the evaluations are valued 0, i.e.,
f(xt) = 0,∀t. Hence, RT=−Tf(x∗) =CTǫα. For this, let
ǫ∗= min
τ/bardblxτ−x∗/bardbl, (22)
which will maximize ǫ, hence, the regret. Thus, the minimax
regret is
R∗
T≥CTmin
{xt}T
t=1max
x∗∈Ωmin
τ/bardblxτ−x∗/bardblα(23)
≥Ω(Cnα
2Tn−α
n), (24)
by ﬁlling the unit cube with small balls for constant C, which
concludes the proof.
Remark 4. For constant C,α,V ; we can also write the regret
in a more compact form as
RT≤O/parenleftBig
(λ0+λ−α
1−α
0)Tn−α
n/parenrightBig
, (25)
sinceλ0+λ−α
1−α
0 is convex and minimum at λ0=/parenleftBig
α
1−α/parenrightBig1−α
,
which lower bounds the expression with 1.
Corollary 1. WhenC0=T1−α′
nfor some parameter α′, we
haveRT≤O(Tg(α′))for some function g(·). This function is
a piecewise linear function that is maximum at its boundarie s
g(0) = 1 andg(1) = 1 ; and minimum at g(α) = 1−α/n.
Proof. LetC0=T1−α′
n. Then,
λ0=Aα′Tα−α′
n, (26)
for some constant Aα′. Hence,
λ−α
1−α
0=Bα′Tα′−α
1−αα
n, (27)
for some constant Bα′. The regret will be given by
RT≤O/parenleftBig
T1−α′
n+T1−1−α′
1−αα
n/parenrightBig
(28)
Thus,RT≤O(Tg(α′)), where
g(α′) =/braceleftBigg
1−α′
n,0< α′≤α,
1−1−α′
1−αα′
n, α < α′<1,(29)
which concludes the proof.5
REFERENCES
[1] J. D. Pint´ er, “Global optimization in action,” Scientiﬁc American , vol.
264, pp. 54–63, 1991.
[2] N. Cesa-Bianchi and G. Lugosi, Prediction, learning, and games .
Cambridge university press, 2006.
[3] H. V . Poor, An Introduction to Signal Detection and Estimation . NJ:
Springer, 1994.
[4] L. M. Rios and N. V . Sahinidis, “Derivative-free optimiz ation: a review
of algorithms and comparison of software implementations, ”Journal of
Global Optimization , vol. 56, no. 3, pp. 1247–1293, 2013.
[5] D. R. Jones, M. Schonlau, and W. J. Welch, “Efﬁcient globa l optimiza-
tion of expensive black-box functions,” Journal of Global optimization ,
vol. 13, no. 4, pp. 455–492, 1998.
[6] P. Jain and P. Kar, “Non-convex optimization for machine learning,”
Foundations and Trends® in Machine Learning , vol. 10, no. 3-4, pp.
142–336, 2017.
[7] P. Hansen, B. Jaumard, and S.-H. Lu, “On the number of iter ations of
piyavskii’s global optimization algorithm,” Mathematics of Operations
Research , vol. 16, no. 2, pp. 334–350, 1991.
[8] P. Basso, “Iterative methods for the localization of the global maximum,”
SIAM Journal on Numerical Analysis , vol. 19, no. 4, pp. 781–792, 1982.
[9] E. Brochu, V . M. Cora, and N. De Freitas, “A tutorial on bay esian
optimization of expensive cost functions, with applicatio n to active
user modeling and hierarchical reinforcement learning,” arXiv preprint
arXiv:1012.2599 , 2010.
[10] S. Boyd, S. P. Boyd, and L. Vandenberghe, Convex optimization .
Cambridge university press, 2004.
[11] Y . Nesterov, Introductory lectures on convex optimization: A basic
course . Springer Science & Business Media, 2003, vol. 87.
[12] S. Bubeck, “Convex optimization: Algorithms and compl exity,” Foun-
dations and Trends® in Machine Learning , vol. 8, no. 3-4, pp. 231–357,
2015.
[13] R. Munos, “From bandits to monte-carlo tree search: The optimistic prin-
ciple applied to optimization and planning,” Foundations and Trends®
in Machine Learning , vol. 7, no. 1, pp. 1–129, 2014.
[14] S. Shalev-Shwartz et al. , “Online learning and online convex optimiza-
tion,” Foundations and Trends® in Machine Learning , vol. 4, no. 2, pp.
107–194, 2012.
[15] J. C. Spall, Introduction to stochastic search and optimization: estim a-
tion, simulation, and control . John Wiley & Sons, 2005, vol. 65.
[16] K. Gokcesu and S. S. Kozat, “Online density estimation o f nonstationary
sources using exponential family of distributions,” IEEE Trans. Neural
Networks Learn. Syst. , vol. 29, no. 9, pp. 4473–4478, 2018.
[17] F. M. J. Willems, “Coding for a binary independent piece wise-
identically-distributed source.” IEEE Transactions on Information The-
ory, vol. 42, no. 6, pp. 2210–2217, 1996.
[18] G. I. Shamir and N. Merhav, “Low-complexity sequential lossless coding
for piecewise-stationary memoryless sources,” IEEE Transactions on
Information Theory , vol. 45, no. 5, pp. 1498–1519, Jul 1999.
[19] M. M. Neyshabouri, K. Gokcesu, H. Gokcesu, H. Ozkan, and S. S.
Kozat, “Asymptotically optimal contextual bandit algorit hm using hier-
archical structures,” IEEE transactions on neural networks and learning
systems , vol. 30, no. 3, pp. 923–937, 2018.
[20] S. Bubeck and N. Cesa-Bianchi, “Regret analysis of stoc hastic and
nonstochastic multi-armed bandit problems,” Foundations and Trends
in Machine Learning , vol. 5, no. 1, pp. 1–122, 2012.
[21] K. Gokcesu and S. S. Kozat, “An online minimax optimal al gorithm for
adversarial multiarmed bandit problem,” IEEE Transactions on Neural
Networks and Learning Systems , vol. 29, no. 11, pp. 5565–5580, 2018.
[22] H. R. Berenji and P. Khedkar, “Learning and tuning fuzzy logic
controllers through reinforcements,” IEEE Transactions on Neural Net-
works , vol. 3, no. 5, pp. 724–740, Sep 1992.
[23] H. Ozkan, M. A. Donmez, S. Tunc, and S. S. Kozat, “A determ inistic
analysis of an online convex mixture of experts algorithm,” IEEE
Transactions on Neural Networks and Learning Systems , vol. 26, no. 7,
pp. 1575–1580, July 2015.
[24] R. Song, F. L. Lewis, and Q. Wei, “Off-policy integral re inforcement
learning method to solve nonlinear continuous-time multip layer nonzero-
sum games,” IEEE Transactions on Neural Networks and Learning
Systems , vol. PP, no. 99, pp. 1–10, 2016.
[25] N. D. Vanli, K. Gokcesu, M. O. Sayin, H. Yildiz, and S. S. K ozat,
“Sequential prediction over hierarchical structures,” IEEE Transactions
on Signal Processing , vol. 64, no. 23, pp. 6284–6298, Dec 2016.
[26] A. C. Singer and M. Feder, “Universal linear prediction by model order
weighting,” IEEE Transactions on Signal Processing , vol. 47, no. 10,
pp. 2685–2699, Oct 1999.[27] J. Moody and M. Saffell, “Learning to trade via direct re inforcement,”
IEEE Transactions on Neural Networks , vol. 12, no. 4, pp. 875–889, Jul
2001.
[28] K. Gokcesu, M. M. Neyshabouri, H. Gokcesu, and S. S. Koza t, “Se-
quential outlier detection based on incremental decision t rees,” IEEE
Trans. Signal Process. , vol. 67, no. 4, pp. 993–1005, 2019.
[29] I. Delibalta, K. Gokcesu, M. Simsek, L. Baruh, and S. S. K ozat, “Online
anomaly detection with nested trees,” IEEE Signal Process. Lett. , vol. 23,
no. 12, pp. 1867–1871, 2016.
[30] K. Gokcesu and S. S. Kozat, “Online anomaly detection wi th minimax
optimal density estimation in nonstationary environments ,”IEEE Trans-
actions on Signal Processing , vol. 66, no. 5, pp. 1213–1227, 2017.
[31] P. L. Bartlett, V . Gabillon, and M. Valko, “A simple para meter-free and
adaptive approach to optimization under a minimal local smo othness
assumption,” in Algorithmic Learning Theory . PMLR, 2019, pp. 184–
206.
[32] J.-B. Grill, M. Valko, and R. Munos, “Black-box optimiz ation of noisy
functions with unknown smoothness,” Advances in Neural Information
Processing Systems , vol. 28, pp. 667–675, 2015.
[33] S. Piyavskii, “An algorithm for ﬁnding the absolute ext remum of a
function,” USSR Computational Mathematics and Mathematical Physics ,
vol. 12, no. 4, pp. 57–67, 1972.
[34] B. O. Shubert, “A sequential method seeking the global m aximum of
a function,” SIAM Journal on Numerical Analysis , vol. 9, no. 3, pp.
379–388, 1972.
[35] F. Schoen, “On a sequential search strategy in global op timization
problems,” Calcolo , vol. 19, no. 3, pp. 321–334, 1982.
[36] D. Q. Mayne and E. Polak, “Outer approximation algorith m for nondif-
ferentiable optimization problems,” Journal of Optimization Theory and
Applications , vol. 42, no. 1, pp. 19–30, 1984.
[37] R. H. Mladineo, “An algorithm for ﬁnding the global maxi mum of a
multimodal, multivariate function,” Mathematical Programming , vol. 34,
no. 2, pp. 188–200, 1986.
[38] Z. Shen and Y . Zhu, “An interval version of shubert’s ite rative method
for the localization of the global maximum,” Computing , vol. 38, no. 3,
pp. 275–280, 1987.
[39] R. Horst and H. Tuy, “On the convergence of global method s in multiex-
tremal optimization,” Journal of Optimization Theory and Applications ,
vol. 54, no. 2, pp. 253–271, 1987.
[40] P. Hansen and B. Jaumard, “Lipschitz optimization,” in Handbook of
global optimization . Springer, 1995, pp. 407–493.
[41] R. Horst and H. Tuy, Global optimization: Deterministic approaches .
Springer Science & Business Media, 2013.
[42] L. Breiman and A. Cutler, “A deterministic algorithm fo r global op-
timization,” Mathematical Programming , vol. 58, no. 1, pp. 179–199,
1993.
[43] W. Baritompa and A. Cutler, “Accelerations for global o ptimization
covering methods using second derivatives,” Journal of Global Opti-
mization , vol. 4, no. 3, pp. 329–341, 1994.
[44] Y . D. Sergeyev, “Global one-dimensional optimization using smooth
auxiliary functions,” Mathematical Programming , vol. 81, no. 1, pp.
127–146, 1998.
[45] R. Ellaia, M. Z. Es-Sadek, and H. Kasbioui, “Modiﬁed piy avskii’s global
one-dimensional optimization of a differentiable functio n,”Applied
Mathematics , vol. 3, pp. 1306–1320, 2012.
[46] R. P. Brent, Algorithms for minimization without derivatives . Courier
Corporation, 2013.
[47] K. Gokcesu and H. Gokcesu, “Regret analysis of global op timiza-
tion in univariate functions with lipschitz derivatives,” arXiv preprint
arXiv:2108.10859 , 2021.
[48] Y . M. Danilin, “Estimation of the efﬁciency of an absolu te-minimum-
ﬁnding algorithm,” USSR Computational Mathematics and Mathemati-
cal Physics , vol. 11, no. 4, pp. 261–267, 1971.
[49] C. Malherbe and N. Vayatis, “Global optimization of lip schitz func-
tions,” in International Conference on Machine Learning . PMLR, 2017,
pp. 2314–2323.
[50] C. Bouttier, T. Cesari, and S. Gerchinovitz, “Regret an alysis of the
piyavskii-shubert algorithm for global lipschitz optimiz ation,” arXiv
preprint arXiv:2002.02390 , 2020.
[51] K. Gokcesu and H. Gokcesu, “Low regret binary sampling m ethod
for efﬁcient global optimization of univariate functions, ”arXiv preprint
arXiv:2201.07164 , 2022.
[52] ——, “Efﬁcient minimax optimal global optimization of l ipschitz con-
tinuous multivariate functions,” arXiv preprint arXiv:2206.02383 , 2022.