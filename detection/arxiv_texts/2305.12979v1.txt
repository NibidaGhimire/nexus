When Computing Power Network Meets Distributed
Machine Learning: An EfÔ¨Åcient Federated Split
Learning Framework
Xinjing Yuany, Lingjun Puy, Lei Jiaox, Xiaofei Wangz, Meijuan Yangy, and Jingdong Xuy
yInstitute of Systems and Networks, College of Computer Science, Nankai University, China
xDepartment of Computer Science, University of Oregon, USA
zSchool of Computer Science and Technology, Tianjin University, China
Corresponding author: Lingjun Pu (pulingjun@nankai.edu.cn)
Abstract ‚ÄîIn this paper, we advocate CPN-FedSL , a novel
and Ô¨Çexible Federated Split Learning (FedSL) framework over
Computing Power Network (CPN). We build a dedicated model
to capture the basic settings and learning characteristics (e.g.,
training Ô¨Çow, latency and convergence). Based on this model,
we introduce Resource Usage Effectiveness (RUE), a novel per-
formance metric integrating training utility with system cost,
and formulate a multivariate scheduling problem that maxi-
mizes RUE by comprehensively taking client admission, model
partition, server selection, routing and bandwidth allocation
into account (i.e., mixed-integer fractional programming). We
design ReÔ¨Ånery , an efÔ¨Åcient approach that Ô¨Årst linearizes the
fractional objective and non-convex constraints, and then solves
the transformed problem via a greedy based rounding algorithm
in multiple iterations. Extensive evaluations corroborate that
CPN-FedSL is superior to the standard and state-of-the-art
learning frameworks (e.g., FedAvg and SplitFed), and besides
ReÔ¨Ånery is lightweight and signiÔ¨Åcantly outperforms its variants
andde facto heuristic methods under a variety of settings.
I. I NTRODUCTION
The signiÔ¨Åcant trend of network and computing convergence
in the upcoming Network 2030 [1], [2] inspires a new concept
of Computing Power Network (CPN), also known as Com-
pute First Networking (CFN) [3], [4]. CPN aims to connect
diverse computing power in the cloud, on the edge, and
across devices to implement on-demand resource scheduling.
Recently, ITU-T has launched the Ô¨Årst CPN standard [5] to
facilitate its proactive development, and many organizations
have incorporated CPN into their future research, such as
‚ÄúEastern Data and Western Computing‚Äù in China and ‚ÄúSky
Computing‚Äù advocated by UC Berkeley [6]. It can be foreseen
that CPN will be a key scenario to boost many emerging
services (e.g., extended reality and big data analytics).
As distributed machine learning plays a key role in future
large-scale data collection and analytics, we consider it should
be one of the killer services in CPN. Recently, integrating
federated learning (FL) with split learning (SL), federated
split learning (FedSL) is introduced as a promising distributed
machine learning paradigm that offers many advantages, such
as data privacy, parallel training and lightweight on-device
Data 
ClientsCloud Site
Edge SiteMetro SiteCPN -FedSL Controller
Model 
PartitionServer 
SelectionClient 
AdmissionRoutingBandwidth 
AllocationModel 
PartitionServer 
SelectionClient 
AdmissionRoutingBandwidth 
Allocation
Control Plane
Data PlaneStatus Collector
ClientTraining Manager
Network Site RegistrationParameter 
Server
Client Computing Capacity Data Amount  Site Computing CapacityFig. 1: The overview of CPN-FedSL (The computing sites include
cloud sites, metro sites, edge sites and even mobile devices).
computation requirement [7]‚Äì[11]. However, directly deploy-
ing existing FedSL frameworks in CPN is inefÔ¨Åcient, since
they cannot fully utilize the computing power at different
computing sites and overlook the heterogeneity of data clients
(e.g., different computing capacity and different data amount).
SpeciÔ¨Åcally, they generally adopt only one training server and
split the training model in the same manner across all the
clients (i.e., only one partition point), which could prolong
the global aggregation and consequently damage the model
convergence. Therefore, it is of signiÔ¨Åcance to design a Ô¨Çexible
and efÔ¨Åcient FedSL framework in CPN.
In this paper, we advocate CPN-FedSL as shown in Fig. 1,
a novel and Ô¨Çexible federated split learning framework over
computing power network, where the registered users (i.e.,
cooperative data clients) only need to customize their training
task (e.g., the training model, the number of training rounds
and the expected training accuracy), then a service controller
will take over the resource scheduling for federated split 979-8-3503-9973-8/23/$31.00 ¬©2023 IEEEarXiv:2305.12979v1  [cs.NI]  22 May 2023learning per training round, and besides a dedicated parameter
server will conduct the global model aggregation. Compared
with existing FedSL frameworks, CPN-FedSL will assign
an exclusive virtualized training server (e.g., in the form of
containers) to each client, in terms of distributed computing
sites, and it will further independently split the training model
for each pair of client and server.
Despite potential beneÔ¨Åts, designing efÔ¨Åcient CPN-FedSL
is challenging, since the service controller requires to tackle
the following multivariate scheduling:
Client Admission . FedSL intrinsically inherits the character-
istics of both FL and SL. As such, choosing more clients per
global round in FedSL could speed up the model convergence
from the perspective of FL (e.g., FedAvg [12] and FedSGD
[13]), while leading to more exchanged data and computing
workloads at the server side from the perspective of SL [14],
[15]. Naturally, there is a tradeoff between training utility and
system cost, and accordingly the client admission should be
considered in order to jointly optimize them.
Model Partition, Server Selection and Routing . When the
client admission is given, a joint model partition, server selec-
tion and routing should be addressed. This is because model
partition concurrently determines the amount of computing
workloads at the server side and the amount of exchanged data
between client and server, which affects the decision-making
of server selection and routing. Moreover, all of them have
effect on both training utility and system cost. Further com-
plicating this joint decision are the global model aggregation
and the bandwidth competition for network links. The former
one requires each chosen client and its assigned server should
jointly accomplish the training per global round within a given
deadline, in order to avoid the cask effect that delays the global
aggregation, and the latter one results in the correlation among
the routings for different pairs of client and server.
Bandwidth Allocation . In the context of CPN, we can con-
trol the end-to-end bandwidth for each service [5]. Intuitively,
allocating a large amount of bandwidth can facilitate the
service performance, while it could lead to a big bandwidth
waste. Therefore, on-demand bandwidth allocation is required.
The above intertwined client admission, model partition,
server selection, routing and bandwidth allocation (i.e., the
multivariate scheduling) is non-trivial and as far as we know
has not been explored. BrieÔ¨Çy, the previous FL-based studies
have extensively investigated the client admission, while they
do not touch upon the model partition (see the surveys [16],
[17] as references). Although the previous SL-based studies
[15], [18]‚Äì[20] pay attention to the model partition, they
conduct the training process sequentially across the clients
(i.e., sequential training), different from the parallel training
inCPN-FedSL . Moreover, they consider a single partition
point, also different from ours. There are a variety of resource
optimization studies on distributed (edge) clouds including
service placement, workload distribution and request routing,
but none of them fully captures the above multivariate schedul-
ing (see the surveys [21], [22] as references). Despite the
resource optimization studies on task ofÔ¨Çoading, especiallythe recent neural network partition and ofÔ¨Çoading for model
inference (e.g., [23]‚Äì[26]) are similar to ours, most of them
optimize a single objective (e.g., the inference latency), while
our objective is to jointly optimize training utility and system
cost. In addition, they consider single user scenario or neglect
the bandwidth competition among multiple users.
In order to efÔ¨Åciently address the above multivariate
scheduling for CPN-FedSL , we build a dedicated model that
captures the basic settings (e.g., the multivariate scheduling)
and learning characteristics (e.g., training Ô¨Çow, latency and
convergence) of FedSL over CPN. Then, we model training
utility and system cost. On this basis, we introduce Resource
Usage Effectiveness (RUE), a novel performance metric that
integrates training utility with system cost (i.e., RUE ,
Utility=Cost). In the end, we formulate a RUE maximization
problem for the multivariate scheduling (Sec. II).
We design ReÔ¨Ånery , an efÔ¨Åcient approach to address the
formulated mixed-integer fractional programming problem.
BrieÔ¨Çy, ReÔ¨Ånery resorts to the Dinkelbach‚Äôs transform [27]
to linearize the fractional objective, and further decouples the
product of control variables in the non-convex constraints with
their interior relationship. Then, it iteratively solves the trans-
formed NP-Hard problem (i.e., a new variant of unsplittable
multi-commodity Ô¨Çow problem) via a greedy based rounding
algorithm, until the objective converges (Sec. III).
We consider two kinds of training tasks (i.e., MobileNet
and DenseNet) and synthesize different scales of CPN-FedSL
in terms of two realistic network topologies (i.e., NSFNET
and USNET). Extensive evaluations demonstrate that (1)
CPN-FedSL is superior to the standard and state-of-the-art
learning frameworks (e.g., FedAvg [12] and SplitFed [7]);
(2) Compared with different variants, ReÔ¨Ånery veriÔ¨Åes that
each part of the multivariate scheduling has noticeable effect
on RUE; (3) Despite there is no other approach that jointly
optimizes training utility and system cost (e.g., RUE) in
terms of the multivariate scheduling considered in this paper,
ReÔ¨Ånery signiÔ¨Åcantly outperforms several de facto heuristic
methods under a variety of settings; (4) The proposed greedy
based rounding algorithm is reasonably good, i.e., it can
empirically reach 65% ‚Äì 80% of the optimum (Sec. IV).
II. F RAMEWORK MODEL
We Ô¨Årst provide the basic settings and learning characteris-
tics of CPN-FedSL . Then, we present the model of training
utility and system cost, and on this basis we introduce Re-
source Usage Effectiveness, an integrated performance metric
forCPN-FedSL . Finally, we give the problem formulation.
A. Basic Settings and Learning Characteristics
Network Scenario . We consider that CPN-FedSL can be
viewed as a dedicated network slicing in computing power
network for the federated split learning service. As shown
in Fig. 1, the data plane of CPN-FedSL can be modeled
by a directed graph G=fV;Eg, whereV,fVC;VR;VSg
refers to the union set of data clients (e.g., users, hospitals and
companies), CPN routers (e.g., supporting segment routing)Client A Client B
Federated LearningClient A Client B
Split Learning1
2
345Server
Client A Client BServer
Federated Split Learning12 123 3
Client A Client BServer A Server B
CPN -FedSL12
123 3
3 3
Intermediate Results Weights GradientsParameter Server
 Parameter Server
 Parameter ServerFig. 2: Distributed machine learning paradigms (FL: The model training is in the client, and the parameter server calculates the average
weights among clients and overrides the local weights [12]; SL: The server partition sequentially trains with each of the clients and the
client weights are shared with the next training client [14]; FedSL: The server trains clients‚Äô intermediate results (i.e., forward-prorogation
of the cut-layer‚Äôs activation and the labels of data samples) in parallel and the clients‚Äô weights are averaged by a parameter server [7];
CPN-FedSL : Multiple server and client pairs train simultaneously in an SL manner, and their weights are averaged in an FL manner).
and computing sites (e.g., edge, metro and cloud sites), and E
refers to the set of physical network links. The control plane
ofCPN-FedSL can comprehensively monitor the computing
and network resources, and efÔ¨Åciently schedule each training
request to optimal computing site along optimal path.
In this paper, we simply assume CPN-FedSL will provide
an exclusive sub-slicing for each training task1. In this context,
we consider each training task is conducted by a set N
VCof federated clients with the assistance of a set MV S
of computing sites. These N=jNj clients and M=jMj
computing sites are connected by a set RV Rof CPN routers,
and we consider there are multiple available paths denoted by
Lijfor each pair of client iand computing site j. Intuitively,
each path consists of a series of physical network links.
Training Setting . As shown in Fig. 2, similar to many
FL and FedSL frameworks, we consider the control plane of
CPN-FedSL which is physically located in a given computing
site will deploy a parameter server in the same computing site
for each training task. This parameter server takes charge of
global model aggregation and aims to derive the optimal train-
ing model w(i.e, model parameter vector) that minimizes
the loss function F(w) =1
NPN
i=1piFi(w)withinTglobal
rounds. Note that Fi(w)refers to the empirical loss function
of clientideÔ¨Åned asFi(w)=1
jDijP
d2Dif(d;w), whereDi
is the local dataset of client i, andf(d;w)is the loss computed
by the current model wand the data sample d. Besides,pi
refers to the weight of client iwithPN
i=1pi=1.
Different from previous SL and FedSL frameworks which
adopt only one training server and split the training model w
into the same client-side module wCand server-side module
wSacross all the clients (i.e., only one partition point), we
consider each client iis assigned an exclusive virtualized train-
ing server (e.g., container) in a computing site j, and the train-
ing model for each pair of client and server is independently
1Similar to many FL, SL and FedSL studies, we will independently treat
each training task (i.e, different training tasks do not interfere with each other).partitioned into two modules as wt=
wC
it(k);wS
it(k)
;8i2N
per global round t2f1;2;:::;Tg, wherek2f1;2;:::;K wg
indicates the partition point (i.e., the cut-layer), and Kwis the
total number of layers of training model w. Note thatk=Kw
refers to client local training, and we do not allow k= 0(i.e.,
data samples forwarding and training only at the server side)
due to data privacy issue.
Basic Notations . We introduce a series of notations to
facilitate our following descriptions. SpeciÔ¨Åcally, we denote
bycitthe computing capacity of client iin roundt, which
remains the same within each global round while could change
across different rounds. We consider each computing site j
can provide a number of virtualized training servers (e.g.,
containers), and we denote by wjthe computing capacity of
each training server and by 
jthe number of available training
servers. In other words, the training servers are homogeneous
in each computing site with the help of advanced virtualization
techniques such as Kubernetes and Nvidia vGPU. As to a
training model, given the partition point k, we denote by qC
k
the computing density (e.g., FLOPS) of the client-side module,
byqS
kthe computing density of the server-side module and by
skthe amount of exchanged data between client and server
per training batch. As to a network link e2E, we denote by
Bethe bandwidth capacity.
Remark . Given a training task, the CPN-FedSL controller
as shown in Fig. 1 can derive qC
k,qS
kandskin an ofÔ¨Çine man-
ner, obtainpiduring client registration, collect citperiodically
from client uploading (i.e., status monitoring), and set values
forwj,
jandBeas the sub-slicing for that task.
Multivariate Scheduling . As to client admission , we in-
troduce a binary control variable zit2f0;1gto indicate if
clientiis chosen in round t. As to model partition ,server
selection androuting , we introduce a binary control variable
xkl
ijt2f0;1gto indicate if client iis assigned a training server
in computing site jconnected by the path2l2Lijwhen the
2We consider the general single-path routing (i.e., unsplittable Ô¨Çow), and
we can exploit the segment routing technique to implement it in practice [28].partition point is kin roundt. As to bandwidth allocation, we
introduce a control variable yk
ijt2R+to indicate the allocated
end-to-end bandwidth between client iand computing site j
when the partition point is kin roundt. Intuitively, we have
the following constraints:P
jP
kP
lxkl
ijt=zit; (1)
P
iP
kP
lxkl
ijt
j; (2)
P
iP
jP
kP
lyk
ijtxkl
ijt 1l(e)
ijBe: (3)
The constraint (1) indicates that an exclusive training server in
a computing site should be assigned to each chosen client i, the
constraint (2) indicates that each computing site jcan serve at
most 
jclients due to the number of available training servers,
and the constraint (3) refers to the link bandwidth capacity
constraint where 1l(e)
ij is a binary indicator that indicates
whether network link eis on pathl2Lijbetween client i
and computing site j.
Training Flow . In terms of the preceding discussions, we
next present the training Ô¨Çow for each task, with the initial
model w0, the number of epochs Eand the batch size H.
Step 1 (Multivariate Scheduling) : In the beginning of each
roundt, the CPN-FedSL controller Ô¨Årst collects the statusof
data clients (i.e., cit) and strategically chooses a set Atof
clients, assigns an exclusive training server in a computing
site to each client i2A t, decides the model partition point
k, the end-to-end routing path and its allocated end-to-end
bandwidth for each pair of client and server.
Step 2 (Model Download) : The parameter server exploits
multicast to send the previous aggregated model wtto the
chosen clients inAtand their corresponding training servers.
Then, each pair of client and server can obtain the correspond-
ing client-side module wC
it(k)and server-side module wS
it(k)
in terms of wtand the speciÔ¨Åed partition point k.
Step 3 (Split Model Training) : According to the batch size,
each client i2A thasjDij=Hbatches. For each batch per
epoch, client iexecutes forward-propagation (FP) with the
current batch, sends the FP of the last layer‚Äôs activation of
client-side module wC
it(k)associated with the data labels in
the current batch to the assigned server. The server proceeds
FP, computes the loss and generates the gradient to update
the server-side module (i.e., wS
it(k)!wS
it+1(k)). Then, the
server sends the back-propagation of the activation‚Äôs (i.e., the
cut-layer‚Äôs) gradient back, with which client ican update the
client-side module. Note that the above procedure iteratively
operatesEjDij=H(i.e., epochbatch) times per round t.
Step 4 (Model Aggregation) : Each client i2A tuploads
a synthetic model w0
it+1=
wC
it+1(k);0
, and its assigned
server uploads the counterpart model w00
it+1=
0;wS
it+1(k)
to the parameter server (i.e., wit+1=w0
it+1+w00
it+1) for model
aggregation (e.g., FedAvg). Then, t=t+ 1and go to Step 1.
Training Latency Analysis . Given the multivariate schedul-
ing (i.e.,xkl
ijt;yk
ijt;zitare decided), we denote by 0
itthe time
when the controller sends scheduling decision and previous
aggregated model to the chosen client i(i.e., latency in Step
1 and Step 2), which can be expressed by
0
it= (+jwtj)zit=bit;whereandjwtjrespectively refer to the size of scheduling
decision and aggregated model, and bitrefers to the end-to-end
bandwidth between client iand the parameter server. Note that
bitis not a control variable but can be estimated by historical
data samples (i.e., bandwidth prediction).
To proceed, we denote by 1
itthe training latency of each
pair of client and server in Step 3, which can be expressed by
1
it=EjDij
HP
jP
kP
lhqC
k
cit+qS
k
wj+sk
yk
ijti
xkl
ijt;
where the Ô¨Årst term is the total computing time at the
client-side, the second term is the total computing time at the
server-side and the third term is the total data (i.e., activation
and gradient) transmission time between client and server.
At last, we denote by 2
it= (jw0
it+1j+0)zit=bitthe time
when client iuploads its synthetic model and its predicted
computing capacity whose sizes are respectively jw0
it+1jand
0to the controller. Note that we do not need to incorporate
the communication time (i.e., model download and upload)
between the controller and the assigned servers. For example,
the server model download in Step 2 must have Ô¨Ånished before
client sends the intermediate results to server in Step 3.
To avoid the cask effect that several slow client-server pairs
delay the model aggregation per global round, we have the
following training latency constraint for each client-server pair
(refers to the time length of global round):
0
it+1
it+2
it: (4)
Training Convergence Analysis . Besides the common as-
sumptions such as the global and local function FandFi
satisfy L-smooth ,bounded local variance and bounded gra-
dient [7]‚Äì[15], we also assume that: the chosen setAtat
least containsKclients randomly selected per global round
according to the sampling probabilities p1;p2;:::;p N. In this
context, if the global and local function FandFiare strongly
convex, we can derive the following convergence bound:
E
F(wT)
 F(w)=O 1
T+1
KT
: (5)
Alternatively, if they are non-convex, we can derive the fol-
lowing convergence bound:
1
TPT
t=1E
jjrF(wt)jj2
2
=O 1
T+1p
KT
: (6)
To facilitate our following discussions, we only highlight
the relevant components regarding the multivariate scheduling
in the above convergence bound. Note that we can prove the
bound for strongly convex function in (5) with a similar proce-
dure in [12] which however does not consider the theoretical
analysis for the non-convex case. We complement it with (6),
and the detailed proof are provided in our online technical
report [29] for clarity.
B. Training Utility and System Cost
Training Utility . In terms of the above convergence analysis
(i.e., the assumption on the chosen set Atand the derived
convergence bound), we can lower the bound by proportionally
increasing the value of K. To this end, we introduce the follow-
ing fairness-aware client admission metric to approximately
indicate the training utility per global round:
Ut=P
ipizit+P
iQi(t)zit=P
i 
pi+Qi(t)
zit;whereQi(t+ 1) =Qi(t) zit+piwithQ(0) = 0 . Note that
a large value of Utrefers to a high training utility, which can
be interpreted as follows: (1) the termP
ipizitemphasizes
the clients with a higher weight p(i.e., the larger size of
local dataset), which contributes to increasing the number of
training data samples; (2) the termP
iQi(t)zitenables the
average number of times each client is chosen consists with
the given sampling probability throughout the training (i.e.,
fairness), which is inspired from Lyapunov virtual queues [30]
and queuing theory (i.e., the average service ratePT
t=1zit=T
is no less than the average arrival rate piif the virtual queue
Qi(t)is stable). Intuitively, if client ihas not been chosen in
the recent global rounds, then its weight Qi(t)will accumulate
continuously, and consequently it is more likely to be chosen in
the next few rounds. Besides, we allow the weight Qi(t);8ito
take negative values so as to avoid the case that several clients
are frequently chosen. The parameter is used to balance
those two terms.
System Cost . In the context of Computing Power Network,
the service controller should efÔ¨Åciently exploit the computing
resources and the network resources. To this end, we introduce
the following system cost model per round in CPN-FedSL :
Ct=nP
jjtP
iP
kP
lxkl
ijt
+P
eetP
iP
jP
kP
lyk
ijtxkl
ijt 1l(e)
ij
+P
iP
jP
kP
l
it+0
jt
xkl
ijto
;
whereP
iP
kP
lxkl
ijtindicates the number of occupied train-
ing servers in computing site j,P
iP
jP
kP
lyk
ijtxkl
ijt 1l(e)
ij
indicates the amount of occupied bandwidth on network link
e,jtrefers to the unit server cost in computing site j,
andetrefers to the unit bandwidth cost on network link e
per round. Besides the above cost regarding the split model
training in Step 3, we also introduce it+0
jtto indicate the
communication cost for status collection, model download and
upload in Step 1, 2 and 4. Note that refers to the time length
of global round as mentioned before.
Resource Usage Effectiveness . We attempt to jointly op-
timize training utility and system cost in CPN-FedSL . To
achieve this goal, we introduce Resource Usage Effectiveness
(RUE), a novel performance metric to integrate them, which
indicates how efÔ¨Åciently CPN uses resources for FedSL. We
formally present it as
RUE =1
TPT
t=1Ut=Ct;
the average training utility achieved by unit system cost per
round. In this context, our objective is to maximize RUE.
C. Problem Formulation
In terms of the preceding discussions, we next formulate a
multivariate scheduling problem that maximizes RUE by com-
prehensively taking client admission, model partition, server
selection, routing and bandwidth allocation into consideration:
max RUE
s:t: (1), (2), (3), (4),
varzit2f0;1g; xkl
ijt2f0;1g; yk
ijt2R+:To facilitate the challenge analysis and algorithm design,
we make the following simpliÔ¨Åcation. First, we omit the
round index t, since the multivariate scheduling is independent
across different global rounds (Note that Qiis not a control
variable but an input parameter updated per round). Second,
we substitute ziwithP
jP
kP
lxkl
ijin terms of constraint
(1). In this context, the formulated problem becomes
maxP0=P
iP
jP
kP
l 
pi+Qi
xkl
ijP
iP
jP
kP
l 
0
ijxkl
ij+P
e0e 1l(e)
ijyk
ijxkl
ij
s:t:C1:P
jP
kP
lxkl
ij1;
C2:P
iP
kP
lxkl
ij
j;
C3:P
iP
jP
kP
lyk
ijxkl
ij 1l(e)
ijBe;
C4:P
jP
kP
lh
k
ij+s0
k=yk
iji
xkl
ij;
varxkl
ij2f0;1g; yk
ij2R+;
where0
ij,(j+i+0
j),0
e,e,k
ij,(+
0+2jwj)=bit+EjDijqC
k=(Hcit)+EjDijqS
k=(Hwj)ands0
k,
EjDijsk=H.
III. M ULTIVARIATE SCHEDULING ALGORITHM
Algorithmic Challenges . To tackle the problem P0, we
should address two main challenges: First, P0belongs to
the mixed-integer fractional programming, which is generally
intractable. Second, P0incorporates non-convex constraints C3
andC4, which further complicates the algorithm design. To this
end, our basic idea is to Ô¨Årst linearize the fractional objective
and non-convex constraints, and then develop an efÔ¨Åcient
algorithm to solve the linearized mixed-integer problem.
v  Transformed 
Problem Transformed 
Problem Linearized 
Problem Linearized 
Problem EndEnd
Endu Original 
Problem Original 
Problem 
w   
Relaxation &
LP-Solver
Rounding &
Validation w   
Relaxation &
LP-Solver
Rounding &
Validation Updated œÅ  Updated x*,y*  Converge
 Greedy based 
Rounding Algorithm
Fig. 3: Overview of ReÔ¨Ånery .
Algorithm Design . We design ReÔ¨Ånery as illustrated in
Fig. 3, an efÔ¨Åcient approach to solve the problem P0.
Step¬∂: We resort to the Dinkelbach‚Äôs transform [27]
to linearize the fractional objective. SpeciÔ¨Åcally, if we
treat the objective of the original problem as P0(x;y) =
 (x;y)=	(x;y), then we can reformulate it as P0(x;y;) =
 (x;y) 	(x;y), whereis a parametric variable (initially
it is 0). In this context, we can iteratively solve the problem P0
with the parametric objective P0(x;y;), the same original
constraints and the updated =  (x;y)=	(x;y)until
the value of the parametric objective P0(x;y;)converges
(i.e., its difference between two continuous iterations is within
a given tolerance). At that time, the derived optimal solution
(i.e.,fx;yg) in terms of the parametric objective is also the
optimal solution of the original problem. With this principle,
our purpose turns to seeking the optimal solution of the
problemP0with the parametric objective P0(x;y;)(i.e.,
the fractional objective has been linearized).Step¬∑: We linearize the non-convex constraints C3andC4,
since they both involve the product of control variables. In
term of constraint C4, if we assume the training of a pair of
clientiand serverjgiven the partition point kis accomplished
exactly at the given deadline, then the control variable yk
ijwill
be bounded by a constant value 'k
ijdeÔ¨Åned as
'k
ij=s0
k=( k
ij): (7)
Remark . For each chosen client i, given the assigned server
jand the partition point k, the corresponding end-to-end
bandwidth should be allocated at least 'k
ij(i.e.,yk
ij'k
ij).
Besides, let the value of yk
ijdown to'k
ijhas no impact on
the optimal solution in terms of constraint C3andC4, since
the occupied bandwidth could reduce on some network links
given a lower end-to-end bandwidth requirement (i.e., yk
ij).
In this context, we can integrate constraint C3withC4as
C0
3:P
iP
jP
kP
l'k
ijxkl
ij 1l(e)
ijBe;
and meanwhile modify the corresponding component in the
objective function (i.e., yk
ij='k
ij). To sum up, the non-convex
constraints have been linearized, and consequently the origi-
nal problem becomes a linearized problem with a simpliÔ¨Åed
parametric objective P0(x;).
Moreover, in terms of constraint C0
3, we can have the
following theorem to derive the optimal partition point k
given a pair of client iand serverj.
Theorem 1 .Given a pair of client iand server j, the
optimal partition point ksatisÔ¨Åesk= arg min k'k
ijfor each
kwhose'k
ijderived by (7) is positive.
We can easily prove it by contradiction. BrieÔ¨Çy, suppose
the optimal partition point is k0whose'k0
ijis larger than 'k
ij.
Then, we can make 'k0
ijdown to'k
ij, which will not violate
constraintC0
3while increasing the value of objective function.
From Theorem 1, we can also derive the optimal end-to-end
bandwidth allocation given a pair of client iand serverj.
Corollary 1 .Given a pair of client iand server j, the
optimal end-to-end bandwidth allocation yk
ijwill be'k
ij.
According to Theorem 1, we introduce '
ij= minf'k
ijj'k
ij>
0gand we can further simplify the optimization problem as
maxP1=P
iP
jP
l
pi+Qi (0
ij+P
e0
e 1l(e)
ij'
ij)
l
ij
s:t:P
jP
ll
ij1;
P
iP
ll
ij
j;
P
iP
jP
l'
ijl
ij 1l(e)
ijBe;
varl
ij2f0;1g:
In the following, our purpose turns to seeking the optimal
solution of the problem P1, with which we can derive the
corresponding optimal fxkl
ijgin terms of Theorem 1 (i.e., l
ij+
k!xkl
ij) and the optimalfyk
ijgin terms of Corollary 1.
Remark . The problemP1is a new variant of the Un-
splittable Multi-commodity Flow Problem (UMFP) with the
undecided Ô¨Çow destinations (i.e., undecided client-server pairs)
and additional hard capacity constraints (i.e., 
j). We can
easily proveP1is NP-Hard by degrading it to the standard
UMFP. Note that if we loosen the capacity constraint of
computing sites (i.e., removing the second constraint), thenAlgorithm 1: Greedy based Rounding Algorithm
Input:Aacc,Arej,Acur,^
Output: ^
InitiallyAacc;Arej;^ =;;Acur=N;
WhileAcur6=;do
Solve relaxed problem to derive the fractional solution ;
Sortin the descending order of value !l
ijl
ij, where!l
ij
refers topi+Qi (0
ij+P
e0
e 1l(e)
ij'
ij)in the
objective;
Denoted by the ordered list of ;
For each l
ij2in order do
Round l
ijup to 1 and the rest fl0
ij0ginvolvingi
to 0;
Invoke SMT forAacc[figunder three constraints;
Ifthere exists a feasible solution then
Aacc=Aacc[fig,Acur=Acurnfig,
^ = ^[l
ij,break ;
Else
Recover the value of l
ijand its associatedfl0
ij0g;
 = nfl
ijg;
For eachi2A cur, where l
ij=2;8j;8ldo
Arej=Arej[fig,Acur=Acurnfig;
we can resort to the probability theory and Chernoff-type
bound to design a tree-based approximation algorithm [31],
[32]. Alteratively, if we loosen the capacity constraint of
network links (i.e., removing the third constraint), then we
can optimally solve the problem due to the totally unimodular
of its constraints. To the best of our knowledge, there is no
existing algorithms that can approximately solve the problem
P1in polynomial time.
Step¬∏: We will develop a greedy based heuristic algorithm
to solve the problem P1via relaxation, rounding and validation
as presented in Alg. 1.
We respectively introduce Aaccto indicate the set of chosen
clients,Arejto indicate the set of rejected clients and Acurto
indicate the set of undecided clients. Initially, AaccandArej
are empty, andAcurincludes all the client i2N. We relax
the integer control variables involving the client i2Acurand
further exploit some standard algorithms such as simplex and
interior-point method to solve the relaxed problem. Then, we
sort the fractional solution in the descending order of value
!l
ijl
ij, where!l
ijrefers topi+Qi (0
ij+P
e0
e 1l(e)
ij'
ij)
in the objective. We denote by the ordered list of . For
each l
ij2in order, we directly round it up to 1 and the
other variables involving client idown to 0. To proceed, we
resort to SatisÔ¨Åability Modulo Theories (SMT) [33] to judge
if there is a feasible solution under the three constraints in
problemP1. If there exists a feasible solution, then we set
Aacc=Aacc[fig,Acur=Acurnfigand record the rounded
control variable (i.e., ^ = ^[l
ij). Otherwise, the rounding
ofl
ijand the associated variables involving client iwill be
undone. Besides, l
ijwill be removed from and go back
to consider the next ordered element in . Note that if all the
control variables involving client ihave been removed from
, which means client iis ‚Äúincompatible‚Äù with those chosen
clients, we therefore put it in the rejected set.We repeat the preceding procedures until the undecided
client set is empty. At that time, the derived solution for the
problemP1is saved by ^, with which we can eventually
derive the solution of the problem P0with the parametric
objectiveP0(x;y;)in terms of Theorem 1 and Corollary
1. If the difference of the parametric objective P0(x;y;)
between two continuous iterations is within a given tolerance
(i.e., convergence), then we derive the Ô¨Ånal solution for the
original problemP0. Otherwise, we will proceed the problem
solving with the updated .
Practical Discussions . We next make some discussions
regarding the algorithm performance and overhead.
Performance : We believe ReÔ¨Ånery can Ô¨Ånd a reasonably
good solution for the original mixed-integer fractional pro-
gramming problem P0, as the utilized two linearizing strate-
gies (i.e., Dinkelbach‚Äôs transform and Theorem 1) and the
proposed greedy based rounding algorithm for the new variant
of the Unsplittable Multi-commodity Flow Problem are the
best that can be achieved to date. Although our approach
cannot guarantee the theoretical performance, its performance
is experimentally justiÔ¨Åed by extensive evaluations in the
next section. Besides, as the CPN-FedSL controller does not
require to make realtime multivariate scheduling, we consider
the complexity of ReÔ¨Ånery , mainly attributed to the operations
to derivefl
ijg(i.e., Alg. 1), is acceptable. BrieÔ¨Çy, as the size
ofAcuris reduced by at least one in each ‚Äúwhile‚Äù iteration,
the outer loop terminates in at most Niterations. In addition,
the inner loop iterates over all available variables, which is at
mostO(NMjLj). Moreover, the relaxed linearized problem
and the SMT check can be solved in polynomial time. To
sum up, we can derive fl
ijgin polynomial time.
Overhead : As mentioned in the Remark on page 3, in order
to make multivariate scheduling for each training task, the
CPN-FedSL controller will derive qC
k,qS
kandskin an ofÔ¨Çine
manner, obtain piduring client registration, and collect cit
periodically from client uploading. We consider the latter two
operations are common behavior with acceptable overhead in
the context of CPN [5]. However, as the neural networks are
getting deeper and deeper, a training model with several tens
and even more layers is not surprising, which will lead to a
high overhead to derive qC
k,qS
kandskper partition point k.
012Flopsx1011
Client Server
12345678910
Partition Point012Data Size (Byte)x108
FP BP
(a) DenseNet.
0246Flopsx109
4 8 12 16 20 24 28
Partition Point0246Data Size (Byte)x107 (b) MobileNet.
Fig. 4: The computing density and the amount of exchanged data of
different training models (the number of trained data samples is 100).
Fortunately, we observe the number of ‚Äúeffective‚Äù partition
points is limited by investigating a variety of training models.Consider two kinds of training tasks: DenseNet [34], a large
model that includes 200 layers and 10 neural network mod-
ules3; MobileNet [35], a small model that includes 28 layers.
Fig. 4 presents the corresponding computing density and the
amount of exchanged data (i.e., FP activation and BP gradient).
From the perspective of computing density, it is fairly regular
that a later partition point corresponds to a higher computing
density (i.e., more training layers) at the client side. However,
there is no clear trend from the perspective of data exchange.
In this context, we can check each partition point from the
beginning to the end and Ô¨Ålter out the ‚Äúeffective‚Äù partition
points. SpeciÔ¨Åcally, we consider an effective partition point
should satisfy the amount of data exchange at that point is
much smaller than that at each pervious point, since a later
partition point requires more computing capacity at the client
side. With this principle, we can obtain the effective partition
points of DenseNet are 1, 3, 5, 9 and those of MobileNet are
1, 4, 8, 12, 24, which signiÔ¨Åcantly contracts the partition point
set and thus reduces the overhead to derive fqC
k,qS
k,skg.
IV. P ERFORMANCE EVALUATION
In this section, we conduct extensive experiments to evaluate
CPN-FedSL by answering the following questions.
What is the performance of CPN-FedSL compared with
standard and SOTA learning frameworks? ( Exp#1 )
Whether each part of multivariate control in ReÔ¨Ånery has
effect on the performance? ( Exp#2 )
What is the performance of ReÔ¨Ånery compared with de
facto heuristic methods? ( Exp#3 )
How is the performance of the greedy based rounding
algorithm in ReÔ¨Ånery ? (Exp#4 )
A. Experiment Settings
Training Task . We consider two training tasks: DenseNet
and MobileNet with the widely used dataset ImageNet [36].
The computing density and the amount of exchanged data
of these two tasks have provided in Fig. 4. The number of
epoch is set to 1. To compromise with the following computing
capacity setting at both client and server side as well as the
computing density of each training model, the batch size is
set to 8 for DenseNet and 4 for MobileNet. In addition, the
time length of global round (i.e., the given deadline ) is set
to 150s for DenseNet and 5s for MobileNet.
(a) NSFNET.
 (b) USNET.
Fig. 5: Network topology.
3Note that it is unnecessary to partition a neural network module (i.e.,
including several intertwined layers), and thus we consider 10 partition points
for DenseNet (i.e., partition between modules).Network Scenario . We synthesize different scales of net-
work scenarios for CPN-FedSL , in terms of two realistic
network topologies as shown in Fig. 5, where NSFNET stands
for a small topology including 14 nodes and 21 links, and
USNET stands for a more dense topology including 24 nodes
and 43 links. We consider four network settings with different
client distributions:
NS1: randomly select 6 nodes as computing sites and the
rest 8 nodes each connects with 6 clients in NSFNET;
NS2: randomly select 6 nodes as computing sites and 16
nodes each connecting with 1 client in USNET;
NS3: randomly select 6 nodes as computing sites and 16
nodes each connecting with 3 clients in USNET;
NS4: randomly select 6 nodes as computing sites and 3
nodes each connecting with 16 clients in USNET.
TABLE I: The settings of computing sites.
Site 1 Site 2 Site 3 Site 4 Site 5 Site 6
Capacity 4400 4400 4400 6500 6500 6500
Utilization 5% 10% 15% 5% 10% 15%
Cost 800 800 800 1500 1500 1500
The settings of 6 computing sites are given in Tab. I, and
the actual computing capacity is ‚ÄúCapacity Utilization‚Äù. The
number of training servers per computing site is set to 8 for
NS1, NS3 and NS4 while 3 for NS2. The unit bandwidth
cost of network link is randomly from 1 to 10 for DenseNet
and from 0.1 to 1 for MobileNet. The bandwidth capacity of
network link is randomly from 3000 to 5000.
Data Client . We consider three kinds of client computing
capacity: 400, 800 and 1200. Each client is assigned a capacity
with a random utilization from 2% to 20% per global round.
Each client has a random number of data samples from 4000
to 20000. The client weight pis calculated in terms of the size
of dataset (i.e., pi=jDij=PN
i=1jDij). Besides, we introduce
an auxiliary parameter p0= 10000 to balance the value of
training utility and system cost (i.e., pi=pip0).
B. Experiment Evaluation
(Exp#1) Different Learning Frameworks . We consider the
following distributed machine learning frameworks.
FedAvg [12]: each client will conduct the local training
if it can accomplish it within the given deadline, which
is the lower bound of learning performance;
SplitFed (Unlimited) [7]: select the computing site with
the largest computing capacity and choose the partition
point to beneÔ¨Åt the most of clients without the constraints
on link bandwidth and the number of training servers,
which is the upper bound of learning performance;
SplitFed (Limited) : similar to SplitFed (Unlimited) while
taking two capacity constraints into account;
CPN-FedSL (NQ) : similar to CPN-FedSL without tak-
ing the fairness-aware client admission into account.
We create Python programs to conduct the performance
comparison under each network scenario (i.e., NS1 ‚Äì NS4)
via simulation. For ease of evaluation, we consider eachexperiment runs 30 training rounds(i.e., T= 30 ), and exploit
Average Training Amount , the average number of trained data
samples per round as an approximate metric to indicate the
convergence time (due to the simulation based evaluation),
since training more data samples per round is beneÔ¨Åcial for
model convergence to some extent [12]. In addition, we con-
sider another performance metric Normalized Accuracy , the
derived accuracy of each distributed learning framework with
its trained data samples compared with the centralized learning
with all the data samples.
Note that, in order to derive the normalized accuracy in
practice, we exploit three desktops to respectively act as client,
training server and parameter server. As each pair of client and
server trains the model in parallel, we exploit the real pair to
mimic the behavior of each virtual pair in the experiment. That
is, the real pair sequentially trains the model in terms of the
decided partition point for each virtual pair per round, while
uploading the derived model parameters of each virtual pair
to the parameter server. The parameter server conducts the
FedAvg operation [12] when receiving the model parameters
from all the virtual pairs of client and server.
The evaluation results are presented in Tab. II. Overall,
as for each training task, the performance of each SL en-
abled framework signiÔ¨Åcantly outperforms that of FedAvg
in each network scenario. These results indicate the great
potential by integrating FL with SL (i.e., more computing
resources). Our performance (in bold) can achieve the clos-
est performance (i.e., normalized accuracy) compared with
SplitFed (Unlimited), and can increase the normalized ac-
curacy by 11.4% (8.2%) compared with SplitFed (Limited)
for MobileNet (DenseNet). These results reveal the beneÔ¨Åts
of adopting various servers and Ô¨Çexible partition for model
training. Despite the average training amount of CPN-FedSL
is slightly lower than that of CPN-FedSL (NQ), the accuracy
on the contrary is slightly higher. This result emphasizes the
effectiveness of fairness-aware client admission. To sum up,
we can conclude that CPN-FedSL is superior to standard and
state-of-the-art learning frameworks under a variety of settings.
(Exp#2) Different Variants of ReÔ¨Ånery . We consider the
following three variants (i.e., ablation experiments).
Replaced Client Admission (RCA) : randomly choose
each client in terms of the client weight p;
Replaced Model Partition (RMP) : partition the model
at the same point (i.e., only a single partition point);
Replaced Path Selection (RPS) : select the shortest path
after the decision-making of server selection.
We exploit the same simulation setting with Exp#1 except
the evaluation metric is RUE, the integrated metric introduced
in this paper. The evaluation results are presented in Fig. 6.
Compared with RCA, ReÔ¨Ånery improves the average RUE
by 1.84in NS1, by 1.19in NS2, by 1.89in NS3
and by 1.62in NS4. These results indicate that although
RCA can also achieve the fairness-aware client admission,
it could not fully utilize the available computing resources
(i.e., some clients are not chosen due to the randomness). In
other words, our client admission that jointly captures fairnessTABLE II: Performance comparison among different learning frameworks (DenseNet and MobileNet).
Learning
FrameworkDenseNet MobileNet
Training Amount (x 104) Normalized Accuracy (%) Training Amount (x 104) Normalized Accuracy (%)
NS1 NS2 NS3 NS4 NS1 NS2 NS3 NS4 NS1 NS2 NS3 NS4 NS1 NS2 NS3 NS4
FedAvg 13.7 5.3 13.1 12.9 81.6 72.3 80.6 81.1 14.1 5.5 14.9 14.1 60.5 56.7 61.2 60.7
SplitFed (Unlimited) 58.9 19.3 59.2 59.2 95.6 84.3 96.1 96.1 60.1 20.2 60.8 60.8 89.7 69.4 89.9 89.9
SplitFed (Limited) 24.6 8.7 25.2 25.1 87.7 80.1 87.9 87.9 24.5 8.7 25.7 25.2 75.6 58 77.1 76.9
CPN-FedSL (NQ) 44.6 15.1 48.5 51.0 94.3 82.3 94.9 95.2 44.6 11.8 48.9 46.8 84.5 59.2 85.1 83.6
CPN-FedSL 42.9 13.5 48.5 46.1 95.3 82.8 96.1 95.8 42.2 13.3 42.2 42.1 87.0 61.5 86.9 86.5
NS1 NS2 NS3 NS4
Topology0123RUEOurs RCA RMP RPS
(a)DenseNet.
NS1 NS2 NS3 NS4
Topology0123RUEOurs RCA RMP RPS (b)MobileNet.
Fig. 6: Performance comparison among different variants.
NS1 NS2 NS3 NS4
Topology0123RUEOurs MTU MCC MNC(a)DenseNet.
NS1 NS2 NS3 NS4
Topology0123RUEOurs MTU MCC MNC (b)MobileNet.
Fig. 7: Performance comparison among different heuristic methods.
and effectiveness is more preferable. Compared with RMP,
ReÔ¨Ånery improves the average RUE by 3.04 in NS1, by
1.73in NS2, by 2.68in NS3 and by 2.26 in NS4.
These results emphasize that Ô¨Çexible model partition is more
signiÔ¨Åcant compared with the randomized client admission.
Indeed, with the single partition point, some pairs of client
and server may lead to more data exchange (e.g., not the
optimal partition point), and therefore increase the system cost.
Compared with RPS, ReÔ¨Ånery improves the average RUE
by 1.14in NS1, by 1.07in NS2, by 1.50in NS3 and
by 1.74in NS4. We can observe that RPS is good at the
sparse scenario (i.e., NS2) while weak in the dense scenario
(i.e., NS4). As ReÔ¨Ånery jointly takes server selection and
routing into account, its RUE is always greater than 1. In
addition, it can achieve better performance when the clients
are well distributed in the network (i.e., NS3). To sum up, we
can conclude that each part of the multivariate scheduling in
ReÔ¨Ånery has a signiÔ¨Åcant effect on the RUE performance.
(Exp#3) Different Heuristic Methods . We consider the
following three de facto heuristic methods.
Maximize Training Utility (MTU) : sort the clients in
ascending order of client computing capacity, and then
select the computing site with the largest computing
capacity for each client in order (If one site is fully Ô¨Ålled,
then select the second largest one, and so on);
Minimize Computing Cost (MCC) : shufÔ¨Çe the clients,
and then select the computing site with the lowest unit
server cost for each client in order (If one site is fully
Ô¨Ålled, then select the second lowest one, and so on);
Minimize Network Cost (MNC) : select the computing
site for each client in terms of their routing hops.
We exploit the same simulation setting with Exp#2. The
evaluation results are presented in Fig. 7. Compared withMTU, ReÔ¨Ånery improves the average RUE by 3.67 in
NS1, by 3.12in NS2, by 3.17in NS3 and by 3.42 in
NS4. These results indicate that solely optimizing the training
utility could suffer from a high system cost. Therefore, we
should jointly optimize them. Compared with MCC, ReÔ¨Ånery
improves the average RUE by 1.51 in NS1, by 16.57in
NS2, by 1.85in NS3 and by 1.74 in NS4. Remarkably,
MCC is weak in the sparse scenario, since the random shufÔ¨Çe
and cost-driven server selection could not always Ô¨Ånd suitable
pairs of client and server, and accordingly lead to a relatively
low training utility (i.e., a low RUE). Compared with MNC,
ReÔ¨Ånery improves the average RUE by 1.07 in NS1, by
1.01in NS2, by 1.41in NS3 and by 1.52 in NS4. We can
observe that MNC is pretty good at the small-scale and sparse
scenario (i.e., NS1 and NS2), and can achieve a slightly better
performance than ours in some cases. This is because when the
scenario is sparse, the distance-driven server selection could
not interference with each other. Indeed, when the network
scenario becomes denser (i.e., NS2 !NS1!NS3!NS4), the
performance of MNC cannot catch up with ours. To sum up,
we can conclude that ReÔ¨Ånery signiÔ¨Åcantly outperforms de
facto heuristic methods in most cases.
(Exp#4) Different Algorithms for P1. We consider the
following optimal solution and two alternative algorithms.
Optimal Solution (OPT) : directly solve the integer pro-
grammingP1via the GLPK solver;
Weighted Randomized Rounding (WRR) : solve the
relaxed problem and conduct the randomized rounding
in terms of!l
ijl
ij;
Randomized Rounding (RR) : solve the relaxed problem
and conduct the randomized rounding in terms of l
ij.
We exploit the same simulation setting with Exp#2 (That is,
we substitute our proposed algorithm with them in ReÔ¨Ånery toNS1 NS2 NS3 NS4
Topology01234RUEOurs OPT WRR RR(a) DenseNet.
NS1 NS2 NS3 NS4
Topology01234RUEOurs OPT WRR RR (b) MobileNet.
Fig. 8: Performance comparison among different algorithms for P1.
evaluate the performance). The evaluation results are presented
in Fig. 8. Overall, we can observe that the performance of
WRR is better than that of RR, since the former algorithm fully
takes the objective into account. Compared with WRR, our
greedy based rounding algorithm improves the average RUE
by 1.32in NS1, by 1.30in NS2, by 1.43in NS3 and by
1.44in NS4. This is because WRR can be roughly viewed as
a one-time operation in the outer loop of ours without the SMT
check. Compared with OPT, our performance will account for
68% in NS1, 80% in NS2, 75% in NS3 and 66% in NS4
(i.e., 72.8% on average). To sum up, we can conclude that the
proposed greedy based rounding algorithm is reasonably good
compared with alternative algorithms.
V. C ONCLUSION
In this paper, we advocate CPN-FedSL , a novel federated
split learning framework over computing power network. We
build a dedicated model to capture its basic settings and
learning characteristics. Based on this model, we introduce
a novel performance metric integrating training utility with
system cost and formulate a multivariate scheduling problem.
We design ReÔ¨Ånery , an efÔ¨Åcient approach that Ô¨Årst linearizes
the fractional objective and non-convex constraints, and then
solves the transformed problem via a greedy based rounding
algorithm in multiple iterations. Extensive evaluations corrob-
orate the superior performance of CPN-FedSL andReÔ¨Ånery .
VI. A CKNOWLEDGMENT
We thank the anonymous reviewers for their constructive
feedback. This work was supported in part by the National
Natural Science Foundation of China (No. 62172241), the
Natural Science Foundation of Tianjin (No. 20JCZDJC00610)
and the U.S. National Science Foundation (CNS-2047719 and
CNS-2225949).
REFERENCES
[1] ‚ÄúITU-T Technical Report: Representative use cases and key network
requirements for Network 2030.‚Äù Available in: https://www.itu.int/dms
pub/itu-t/opb/fg/T-FG-NET2030-2020-SUB.G1-PDF-E.pdf.
[2] ‚ÄúHuawei Industry Report: Communications Network 2030.‚Äù
Available in: https://www-Ô¨Åle.huawei.com/-/media/corp2020/pdf/
giv/industry-reports/communications network 2030 en.pdf.
[3] M. Kr ¬¥ol, S. Mastorakis, et al. , ‚ÄúCompute Ô¨Årst networking: Distributed
computing meets icn,‚Äù in ACM ICN , 2019.
[4] J. Crowcroft, P. Eardley, et al. , ‚ÄúFindings and Recommendations for
Compute First Networking,‚Äù in Dagstuhl Reports , 2021.[5] ‚ÄúITU-T Technical Report: Computing power network ‚Äî‚Äì Frame-
work and architecture.‚Äù Available in: https://www.itu.int/rec/T-REC-Y .
2501-202109-I.
[6] I. Stoica and S. Shenker, ‚ÄúFrom cloud computing to sky computing,‚Äù in
ACM HotOS , 2021.
[7] C. Thapa, M. A. P. Chamikara, et al. , ‚ÄúSplitfed: When federated learning
meets split learning,‚Äù in AAAI , 2022.
[8] D.-J. Han, H. I. Bhatti, et al. , ‚ÄúAccelerating federated learning with split
learning on locally generated losses,‚Äù in ICML , 2021.
[9] S. Oh, J. Park, et al. , ‚ÄúLocFedMix-SL: Localize, Federate, and Mix for
Improved Scalability, Convergence, and Latency in Split Learning,‚Äù in
ACM WWW , 2022.
[10] J. Hong, H. Wang, et al. , ‚ÄúEfÔ¨Åcient split-mix federated learning for
on-demand and in-situ customization,‚Äù in ICLR , 2022.
[11] Z. Zhang, A. Pinto, et al. , ‚ÄúPrivacy and efÔ¨Åciency of communications
in federated split learning,‚Äù arXiv preprint arXiv:2301.01824 , 2023.
[12] X. Li, K. Huang, et al. , ‚ÄúOn the Convergence of FedAvg on Non-IID
Data,‚Äù in ICLR , 2020.
[13] H. Yuan and T. Ma, ‚ÄúFederated accelerated stochastic gradient descent,‚Äù
inNeurIPS , 2020.
[14] P. Vepakomma, O. Gupta, et al. , ‚ÄúSplit learning for health: Distributed
deep learning without sharing raw patient data,‚Äù arXiv preprint
arXiv:1812.00564 , 2018.
[15] S. Tuli, G. Casale, and N. R. Jennings, ‚ÄúSplitPlace: AI Augmented
Splitting and Placement of Large-Scale Neural Networks in Mobile Edge
Environments,‚Äù IEEE TMC , 2022.
[16] W. Y . B. Lim, N. C. Luong, et al. , ‚ÄúFederated learning in mobile edge
networks: A comprehensive survey,‚Äù IEEE Commun. Surv. Tut. , 2020.
[17] L. U. Khan, W. Saad, et al. , ‚ÄúFederated learning for internet of things:
Recent advances, taxonomy, and open challenges,‚Äù IEEE Commun. Surv.
Tut., 2021.
[18] W. Wu et al. , ‚ÄúSplit Learning over Wireless Networks: Parallel Design
and Resource Management,‚Äù arXiv preprint arXiv:2204.08119 , 2022.
[19] M. Krouka, A. Elgabli, et al. , ‚ÄúCommunication-efÔ¨Åcient split learning
based on analog communication and over the air aggregation,‚Äù in IEEE
GLOBECOM , 2021.
[20] Y . Gao, M. Kim, et al. , ‚ÄúEvaluation and optimization of distributed
machine learning techniques for internet of things,‚Äù IEEE TC , 2021.
[21] Y . Mao, C. You, et al. , ‚ÄúA survey on mobile edge computing: The
communication perspective,‚Äù IEEE Commun. Surv. Tut. , 2017.
[22] B. Sonkoly, J. Czentye, et al. , ‚ÄúSurvey on placement methods in the
edge and beyond,‚Äù IEEE Commun. Surv. Tut. , 2021.
[23] G. Gao, L. Wu, et al. , ‚ÄúOcdst: OfÔ¨Çoading chained dnns for streaming
tasks,‚Äù in IEEE GLOBECOM , 2021.
[24] M. Gao, W. Cui, et al. , ‚ÄúDeep neural network task partitioning and
ofÔ¨Çoading for mobile edge computing,‚Äù in IEEE GLOBECOM , 2019.
[25] W. He et al. , ‚ÄúJoint dnn partition deployment and resource allocation
for delay-sensitive deep learning inference in iot,‚Äù IEEE IOTJ , 2020.
[26] L. Zhang, L. Chen, and J. Xu, ‚ÄúAutodidactic neurosurgeon: Collaborative
deep inference for mobile edge intelligence via online learning,‚Äù in ACM
WWW , 2021.
[27] W. Dinkelbach, ‚ÄúOn nonlinear fractional programming,‚Äù INFORMS
Management science , 1967.
[28] L. He, S. Wang, et al. , ‚ÄúEnabling application-aware trafÔ¨Åc engineering
in ipv6 networks,‚Äù IEEE Network , 2022.
[29] ‚ÄúOnline Technical Report.‚Äù https://www.dropbox.com/s/
mwikm0ox0y1xaac/technical%20report.pdf?dl=0.
[30] M. J. Neely, ‚ÄúStochastic network optimization with application
to communication and queueing systems,‚Äù Synthesis Lectures on
Communication Networks , 2010.
[31] A. Chakrabarti, C. Chekuri, et al. , ‚ÄúApproximation algorithms for the
unsplittable Ô¨Çow problem,‚Äù Springer Algorithmica , 2007.
[32] P. Raghavan, ‚ÄúProbabilistic construction of deterministic algorithms:
approximating packing integer programs,‚Äù Elsevier Journal of Computer
and System Sciences , 1988.
[33] C. Barrett and C. Tinelli, ‚ÄúSatisÔ¨Åability modulo theories,‚Äù in Springer
Handbook of model checking , 2018.
[34] G. Huang, Z. Liu, et al. , ‚ÄúDensely connected convolutional networks,‚Äù
inIEEE CVPR , 2017.
[35] A. G. Howard, M. Zhu, et al. , ‚ÄúMobilenets: EfÔ¨Åcient convolutional
neural networks for mobile vision applications,‚Äù arXiv preprint
arXiv:1704.04861 , 2017.
[36] J. Deng, W. Dong, et al. , ‚ÄúImagenet: A large-scale hierarchical image
database,‚Äù in IEEE CVPR , 2009.