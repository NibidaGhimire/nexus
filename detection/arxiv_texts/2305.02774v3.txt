IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, XXXX 2023 1
Spatial and Modal Optimal Transport for Fast
Cross-Modal MRI Reconstruction
Qi Wang, Zhijie Wen, Jun Shi, Qian Wang, Dinggang Shen, Fellow, IEEE , and Shihui Ying, Member, IEEE
Abstract — Multi-modal magnetic resonance imaging
(MRI) plays a crucial role in comprehensive disease di-
agnosis in clinical medicine. However, acquiring certain
modalities, such as T2-weighted images (T2WIs), is time-
consuming and prone to be with motion artifacts. It nega-
tively impacts subsequent multi-modal image analysis. To
address this issue, we propose an end-to-end deep learn-
ing framework that utilizes T1-weighted images (T1WIs) as
auxiliary modalities to expedite T2WIs’ acquisitions. While
image pre-processing is capable of mitigating misalign-
ment, improper parameter selection leads to adverse pre-
processing effects, requiring iterative experimentation and
adjustment. To overcome this shortage, we employ Optimal
Transport (OT) to synthesize T2WIs by aligning T1WIs and
performing cross-modal synthesis, effectively mitigating
spatial misalignment effects. Furthermore, we adopt an
alternating iteration framework between the reconstruction
task and the cross-modal synthesis task to optimize the
final results. Then, we prove that the reconstructed T2WIs
and the synthetic T2WIs become closer on the T2 image
manifold with iterations increasing, and further illustrate
that the improved reconstruction result enhances the syn-
thesis process, whereas the enhanced synthesis result
improves the reconstruction process. Finally, experimental
results from FastMRI and internal datasets confirm the
effectiveness of our method, demonstrating significant im-
provements in image reconstruction quality even at low
sampling rates.
Index Terms — MRI reconstruction, cross-modal recon-
struction, spatial alignment, optimal transport.
I. INTRODUCTION
MAGNETIC resonance imaging (MRI) is widely used
for disease diagnosis due to its clear and high-contrast
images, absence of ionizing radiation, and ability to reveal
internal pathological changes for treatment guidance [1]. It
has become a crucial technology in clinical and research
settings. Multi-modal MRI has gained popularity in various
This work was supported by the National Key R & D Program of
China under Grant 2021YFA1003004 and the National Natural Science
Foundation of China under Grant 11971296. (Corresponding author:
Shihui Ying. (e-mail: shying@shu.edu.cn))
Qi Wang, Z. Wen, and S. Ying are with the Department of Mathemat-
ics, School of Science, Shanghai University, Shanghai 200444, China.
J. Shi is with the Key laboratory of Specialty Fiber Optics and Optical
Access Networks, Joint International Research Laboratory of Specialty
Fiber Optics and Advanced Communication, Shanghai Institute for Ad-
vanced Communication and Data Science, School of Communication
and Information Engineering, Shanghai University, Shanghai 200444,
China
Qian Wang and D. Shen are with the School of Biomedical Engi-
neering, ShanghaiTech University, Shanghai 201210, China, and also
with Shanghai United Imaging Intelligence Co., Ltd., Shanghai 200030,
China.applications, providing comprehensive information for AI-
based disease diagnosis and overcoming limitations of single-
modality analysis [2]. For example, T1-weighted images
(T1WIs) capture morphological and structural details, while
T2-weighted images reveal edema and inflammation in the
same region of the body. However, some modalities, such as
T2-weighted images (T2WIs), require longer scan times due
to extended repetition (TR) and echo times (TE). Prolonged
image acquisition increases the risk of motion artifacts, espe-
cially in chest and abdominal MRI, which can significantly
impact subsequent multi-modal image analysis. To address
these challenges, researchers have focused on reconstructing
MRI from under-sampled data by reducing the sampling in the
k-space [3]–[9]. For instance, Zhou et al. introduced a dual-
domain self-supervised method aimed at accelerating non-
cartesian MRI reconstruction, which effectively preserves k-
space self-similarity [9]. However, these approaches solely rely
on single-modal information, limiting its effectiveness.
Consequently, the field has seen a growing interest in multi-
modal MRI reconstruction [10]–[14]. Liu et al. designed a
mechanism for shareable feature aggregation and selection
among multi-modal MRI images, significantly enhancing the
quality of the reconstructed images [14]. Additionally, Li et
al.introduced a multi-scale transformer network with edge-
aware pre-training as a solution for cross-modal magnetic
resonance image synthesis [11]. Their findings demonstrated
that under-sampled target magnetic resonance images can be
better reconstructed with the assistance of auxiliary modalities,
which provide complementary information about the same
region of interest. However, a common issue arises from the
spatial differences prevalent between images from different
modalities, even within the same subject. Fig. 1 illustrates
this spatial discrepancy between auxiliary images and target
images, where the length of the yellow double-arrowed line in
the target image is noticeably shorter than in the auxiliary
image. This mismatch leads to negative information trans-
fer. To address this challenge, various image pre-processing
techniques have been proposed [15]–[18]. However, improper
parameter selection during image pre-processing undermines
the alleviation of misalignment, leading to undesired pre-
processing outcomes. Therefore, iterative experimentation and
adjustment are essential to address these issues effectively.
From this viewpoint, Xuan et al. proposed a novel deep
learning method for image registration [19]. Their approach
involved the integration of a cross-modal registration module
into a spatial alignment network during the reconstruction
process. By leveraging the power of deep learning, theirarXiv:2305.02774v3  [eess.IV]  21 May 20242 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, XXXX 2023
method demonstrated better results in achieving accurate and
reliable image registration, as depicted in Fig. 1.
Fig. 1. In the zoomed-in views, we observe a slight spatial misalign-
ment between the auxiliary (T1-weighted) image and the target (T2-
weighted) image. The paper [19] addresses this issue by using a spatial
alignment method to generate the auxiliary alignment. The warped-map
illustrates the disparity between the original and warped auxiliary (T1-
weighted) images. The color transition from blue to red indicates an
increasing difference between the two images.
Although their approach achieves good reconstruction per-
formance, using cross-modality-synthesis-based registration to
indirectly measure similarity between two modality images
is a big challenge due to substantial differences between
modalities. Moreover, aligning auxiliary and under-sampled
target images is problematic, especially at low sampling rates.
For instance, at a 6.25% undersampling rate, target images
often lack crucial structural details. The absence of these
essential features can substantially degrade the accuracy of
the registration, undermining their method’s utility in practical
scenarios. Addressing these limitations is crucial for reliable
and accurate registration.
Fortunately, the optimal transport (OT) framework provides
a metric known as the Wasserstein distance, which measures
the dissimilarity between distributions. The Wasserstein dis-
tance is often preferred over other divergence measures, such
as the Kullback-Leibler divergence, in various applications
like generative modeling [20]. This preference arises from
the topological and statistical properties exhibited by the
Wasserstein distance. By utilizing the Wasserstein distance, re-
searchers overcome the limitations associated with alternative
divergence measures and achieve more desirable outcomes in
tasks such as distribution alignment and modeling. Moreover,
OT offers a powerful framework for interpreting the network
by minimizing the Wasserstein distance between distribu-
tions directly [21]. In their paper, the theoretical perspective
of OT in Generative Adversarial Networks (GANs) brings
transparency to a previously opaque process, transforming
probability distribution through convex optimization based
on OT theory. Based on this perspective, we utilize OT to
enhance the spatial alignment process, facilitating coherent and
meaningful information transfer across different modalities.
In this paper, we propose a comprehensive approach that
integrates a reconstruction task and a synthetic task, with the
objective of minimizing the discrepancy between their outputs
on the T2 image manifold. To enhance the interpretability of
the synthetic task, we introduce an OT-based network. Notably,our methodology involves decomposing the synthetic process
into two key components: spatial alignment and cross-modal
synthesis. In this way, we can better analyze these two different
and relatively independent mappings.
By incorporating the reconstruction task and synthetic task,
our approach offers a more interconnected framework that
leverages the benefits of each task. The reconstruction task
helps to improve the fidelity and accuracy of the generated
images, while the synthetic task focuses on bridging the
gap between different modalities and ensuring consistency.
The utilization of an OT-based network further enhances the
interpretability of the synthetic task, enabling a better under-
standing of the alignment process and the synthesis process.
By decomposing the synthetic process into spatial alignment
and cross-modal synthesis, we address the challenges asso-
ciated with modality differences and ensure a more reliable
spatial alignment outcome. This approach not only improves
the interpretability of the synthetic task but also enhances the
overall quality and accuracy of the registration results.
In summary, the proposed framework makes the following
main contributions:
•We introduce a novel synthetic process that combines
spatial alignment and cross-modal synthesis within an OT
framework. This approach leverages the spatial alignment
mapping on the T1 image manifold, directly enhancing
the alignment accuracy, while also considering the cross-
modal synthesis mapping to bridge the gap between
modalities.
•We introduce an alternating iteration framework that
integrates the reconstruction task and the synthetic task.
Unlike traditional pre-processing approaches that treat
these tasks independently, our framework demonstrates
that these tasks are complementary and mutually reinforc-
ing. This iterative process helps minimize the discrepancy
between their outputs on the T2 image manifold, leading
to improved overall performance.
•The quantitative and qualitative experimental results
prove that our model achieves better reconstruction re-
sults than state-of-the-art approaches on both an open
dataset FastMRI and an in-house dataset.
II. R ELATED WORK
In this Section, Section II-A describes the development of
MRI reconstruction and the reason for better performance of
cross-modal reconstruction over single-modality reconstruc-
tion, and then current state of research on cross-modal image
synthesis is discussed in Section II-B. Finally, Section II-C
displays the superiority of the OT-based Learning methods.
A. Deep Learning for Accelerating MRI
Accelerating MRI primarily focuses on recovering under-
sampled k-space signals. To address this challenge, simple
methods like zero-filling and linear filtering have been used,
but they often introduce artifacts and compromise image qual-
ity. Based on the objective of accelerating MRI and addressing
the challenges of under-sampled MRI reconstruction, a signif-
icant advancement has been achieved through the applicationWANG et al. : SPATIAL AND MODAL OPTIMAL TRANSPORT FOR FAST CROSS-MODAL MRI RECONSTRUCTION 3
of Compressed Sensing (CS) [22]. CS enables the production
of high-quality images sampled at much lower rates than
the Nyquist sampling rate, given certain conditions on the
sampling matrix. However, CS methods often rely on prior
knowledge or assumptions about the sparsity of the signal,
which limits their applicability. In recent years, deep learning-
based methods have surpassed conventional techniques, es-
tablishing a new state-of-the-art in MRI reconstruction. No-
tably, Wang et al. designed an offline prior algorithm that
utilizes Convolutional Neural Networks (CNNs) to capture the
relationship between zero-filled magnetic resonance images
and ground-truth images [23]. Additionally, subsequent studies
have explored the advantages of data consistency layers [24],
recurrent neural networks [25], dictionary learning [26], and
federated learning [27], further pushing the boundaries of MRI
reconstruction methods. For instance, Feng et al. proposed a
Federated Learning algorithm aiming at preserving useful de-
tails for local MRI reconstruction from under-sampled k-space
data [27]. While some methods overlook the spatial frequency
properties and fail to utilize complementary information from
adjacent slices, Du et al. introduced adaptive CNNs for k-
space data interpolation address the limitations [28].
In contrast to the aforementioned methods that indepen-
dently reconstruct target images from a single MRI modality,
this paper explores the use of an image from another modality
to restore the target modality. Xiang et al. initially employed
a Dense-Unet architecture that takes data from different MRI
modalities as input to accelerate the reconstruction of a target
image [6]. Moreover, Liu et al. showcased that incorporat-
ing multi-modal MRI input into the reconstruction network
enhances the anatomical accuracy of the reconstructed images
[12]. Based on the observation that k-space and image restora-
tion are complementary, Zhou et al. proposed DuDoRNet with
T1 priors, which aims to recover both source modal images
and target modal images [3]. Recognizing the limitation of
CNNs in capturing global knowledge, Feng et al. introduced
transformers into cross-modal reconstruction, incorporating
a novel cross-attention mechanism for efficient fusion of
different modalities [4]. Additionally, Zhou et al. combined
hybrid operations that include both convolutional layers and
Swin Transformer blocks to explore the relative information
between multi-modal MRI images [13]. This approach attained
performance comparable to supervised reconstruction meth-
ods. However, most of these methods overlook the common
issue of slight misalignment between paired images from
different modalities. Inspired by Xuan et al. [19], this work
decomposes the cross-modal image synthesis process into two
parts: spatial alignment and cross-modal synthesis, with the
goal of enhancing the reconstruction quality.
B. Cross-modal Image Synthesis
Cross-modal image synthesis has emerged as a prominent
area of research in computer vision, encompassing tasks
such as super-resolution [29], style transfer [30], and image
reconstruction [19]. Traditionally, cross-modal image synthesis
has been approached as a regression problem, where the
input is an image in the source modality and the outputis its corresponding image in the target modality. For in-
stance, Jog et al. employed a regression forest to establish
nonlinear mappings between tissue modalities. However, the
main challenge lies in learning an effective mapping from
a source modality image to its target modality counterpart
[31]. In recent years, deep learning techniques utilizing CNNs
have shown remarkable progress in expanding the mapping
capabilities. Notably, Gulrajani et al. proposed a PixelCNN-
based model with an auto-regressive decoder to capture global
features in generated images [32]. Dong et al. introduced a
cross-domain adaptive filters module, known as MMSR, to
learn the joint probability distribution of low-resolution images
and high-resolution images [29].
OT [33] provides a mathematical framework for measuring
the similarity or dissimilarity between probability distribu-
tions, which can be applied to find the most efficient mapping
between two distributions. In the context of cross-modal image
synthesis, the goal is to generate target modality images that
closely resemble the source modality while minimizing the
OT cost. OT can be utilized to establish a mapping between
the source modality and the target modality, aligning their
respective distributions or manifolds. By leveraging this align-
ment, cross-modal image synthesis can generate more accurate
and coherent synthetic images, overcoming the challenges of
spatial misalignment and capturing the underlying information
shared between modalities. For example, Tian et al. leveraged
OT to establish the mapping from text cues to generated
images, effectively addressing the mode collapse issue and
ensuring image diversity compared to simple metrics [34].
Taking cues from the favorable outcomes resulting, we utilize
OT to facilitate the synthesis process, ensuring consistency
and meaningful transformation of information across different
modalities.
C. OT -based Learning Methods
OT is a unique optimization problem that defines the
Wasserstein distance between two probability distributions
within a Lagrangian framework [33], [35]. This distance can
be estimated solely based on the empirical distribution of
data, while maintaining a favorable convergence property even
when the supports of the two distributions have a trivial
intersection [36]. Leveraging these advantageous character-
istics, OT-based deep learning methods have found appli-
cations in diverse domains, including image reconstruction
[37], generative models [38], and domain adaptation [39]. OT
empowers the exploration of optimal correspondences between
disparate data modalities, facilitating the seamless transfer of
significant and coherent information. For instance, OT is used
as a mathematical framework to derive the architecture of the
OT-cycleGAN for accelerated MRI reconstruction in this pa-
per[40]. It helps guide the mapping between the under-sampled
k-space data and the fully-sampled image domain, facilitating
the generation of high-resolution magnetic resonance images
from accelerated data. Moreover, deep learning based methods
have demonstrated remarkable performance in various tasks
compared to traditional approaches, but their interpretabil-
ity is limited. OT provides a topological interpretation of4 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, XXXX 2023
CNNs and offers insights into the learned data distribution.
In the context of generative models like GANs, the main
objectives are manifold learning and probability distribution
transformation [41]. From the OT perspective, the generator
in GAN is responsible for computing a transport map, while
the discriminator measures the Wasserstein distance between
the generated distribution and the real distribution. An et al.
[41] proposed an AE-OT-GAN model that overcame mode
collapse and incorporated explicit correspondence between
latent codes and real images by applying a semi-discrete OT.
In this paper, we combine OT and deep learning methods
for improved interpretability and enhanced performance in
generative models.
III. M ETHODS
In Section III-A, two OT processes regarding the spatial
alignment module and the cross-modal synthesis module are
represented. Secondly, Section III-B demonstrates that the
proposed framework ensures the reconstructed T2WIs are
close to the synthetic T2WIs on the T2 image manifold.
Finally, we display the training scheme in Section III-C.
A. T1WI-aided Cross-modal Synthesis Process
T1WIs and T2WIs provide complementary information
about the same interest region of human beings. Inspired by
the same underlying information and latent feature correlation
among them, the target image reconstruction benefits from
fully-sampled source images. In this paper, the fully-sampled
T1WIs xT1are used to synthesize the fully-sampled T2WIs
xG
T2.
The proposed T1WI-based cross-modal synthesis process is
expressed as:
xG
T2=Gφ(xT1), (1)
where Gφis the cross-modal synthesis network with the
parameters φ.
However, as shown in Fig. 1, the spatial misalignment
between paired images from different modalities is inevitable,
which brings the negative transfer of the auxiliary information
and reduces the reconstruction quality. To alleviate the negative
transfer, before the cross-modal synthesis process, we design
a spatial alignment module to align the auxiliary T1WIs with
the under-sampled target T2WIs.
Finally, the T1WI-based cross-modal synthesis mapping Gφ
is decomposed into two OT processes, the spatial alignment
mapping OTSofT1modality and the cross-modal synthesis
mapping OTMfrom the aligned T1WI xA
T1to the synthetic
T2WI xG
T2. Thus, the T1WI-based cross-modal synthesis map-
pingGφ, which is formulated as:
Gφ(xT1) =OTM◦OTS(xT1). (2)
This method employs an iterative strategy for the opti-
mization of Gφ. Initially, a global parameter update for the
synthesis mapping φis conducted. Subsequently, we undertake
Iiterations of refinement, focusing on the sub-problems OTS
andOTM. Each utilizing their respective loss functions. The
purpose of delineating Iiterations is to precisely tune the
individual components within their specific parameter spaces.During the initial phase of each iteration, we focus on
optimizing OTM, whose network parameters are denoted by
m. Next, we fix the parameters mand optimize OTS, with
its network parameters denoted by s. This staged refinement
process aims to incrementally enhance the model’s efficacy in
cross-modal synthesis.
An OT problem can be described as that of weighing the
differences between two probability distributions, which is
primarily proposed by Monge in 1781 [42]. The transport plan
aims to minimize the cost of warping an input probability
distribution onto another. In 1942, Kantorovich assumed that
a source would be transported onto several targets, which
promotes further development [43].
1) An OT -view of Spatial Alignment Module: This work in-
troduces an optimal transport (OT) perspective on spatial
alignment mapping. Suppose p(xT1)is the probability mea-
sure of a T1-weighted image (T1WI). In accordance with
the Manifold Distribution Hypothesis [44], the distribution
of a specific class of natural data is concentrated on a low-
dimensional manifold within a high-dimensional data space.
Consequently, the probability measure is concentrated on a
manifold, specifically the T1 manifold, embedded in the space
X. The probability distribution transformation map OTSis
designed to transport the probability measure p(xT1)to the
probability measure p(xA
T1)concentrated on the T1 manifold
embedded in the space X. In our optimization strategy, a total
ofIiterations are executed. At each iteration i, we define the
OT problem as follows:
LGφ= min
OT(i)
SZ
Xc
xT1, OT(i)
M◦OT(i)
S(xT1)
dp(xT1)
s. t.OT(i)
M◦OT(i)
S#p(xT1) =p(xT2).(3)
where OT(i)
Mrepresents the parameters of the cross-modal
synthesis mapping that have been individually optimized for
iiterations and are now fixed. OT(i)
S#p(xT1) = p(xA(i)
T1)
ensures that the distribution of the spatially aligned T1-
weighted images xA(i)
T1. If a minimum OT∗
Sexists, it is named
as the OT map. According to convex optimization theory, Eq.
(3) can be relaxed to the Kantorovich problem. Its dual form
is as follows [33]:
min
OT(i)
Smax
ψγZ
ψγ(OT(i)
M◦OT(i)
S(xT1))dp(xT1)
+Z
ψc
γ(xT2)dp(xT2)(4)
In the spatial alignment task, the discriminator Dγ, denoted
byψγ, enhances the estimation of the Wasserstein distance.
This distance is between the synthetic T2-weighted image xG
T2
and the ground truth image xT2. The aim is to indirectly
measure how well the images align . A smaller distance means
a better alignment. The discriminator ψγalso serves as the
Kantorovich potential. ψγsatisfies the 1-Lipschitz condition,
which is typically ensured via spectral normalization [45]. For
L1transportation cost, its c-transform exhibits the relationship
ψc
γ=−ψγ. Through an iterative optimization algorithmWANG et al. : SPATIAL AND MODAL OPTIMAL TRANSPORT FOR FAST CROSS-MODAL MRI RECONSTRUCTION 5
Fig. 2. The exhibition of the designed framework. The reconstruction network Rθuses the under-sampled T2WI xT2to obtain the reconstructed
T2WIp(xR
T2)concentrated on T2manifold. The OT process about the spatial alignment module OTSmaps the distribution of the full-sampled
T1WIp(xT1)to the distribution of the aligned T1WI p(xA
T1)inT1manifold, which is closer to the distribution of the reconstructed T2WI p(xR
T2).
The OT process about the cross-modal synthesis module OTMtransports the distribution of the aligned T1WI p(xA
T1)to the distribution of the
synthetic T2WI p(xG
T2)inT2manifold, which is similar to the distribution of the ground truth T2WI p(xT2).
between maximization and minimization, we can numerically
obtain both the map OT(i)
Sand the maximizer ψ∗
γ[21].
2) An OT -view of the Cross-modal Synthesis Module: This
work also presents an OT view of cross-modal synthesis
module. p(xA(i)
T1)concentrated on the T1manifold is the
probability measure in the space X. The goal of the probability
distribution transformation map OT(i+1)
M is to convert the
probability measure of the aligned T1WI p(xA(i)
T1)into the
probability measure of the ground truth T2WI p(xT2), which
is focused on the T2manifold embedded in the space Y.
Suppose a map OT(i+1)
M :X → Y transports the probability
measure p(xA(i)
T1)into the probability measure p(xT2). The OT
problem is formulated as:
LOTM= min
OT(i+1)
MZ
Xc
xA(i)
T1, OT(i+1)
M (xA(i)
T1)
dp(xA(i)
T1)
s. t.OT(i+1)
M #p(xA(i)
T1) =p(xT2).(5)
Similarly, the Kantorovich dual form is as follows:
min
OT(i+1)
Mmax
ψωZ
ψω(OT(i+1)
M (xA(i)
T1))dp(xA(i)
T1)
+Z
ψc
ω(xT2)dp(xT2)(6)In the task of cross-modal synthesis, the discriminator Dω
refines the approximation of the Wasserstein distance ψω.
This distance is measured between the synthetic T2WI xG
T2
and the ground truth T2WI xT2. The synthetic image xG
T2
is produced by the cross-modal synthesis module, which is
achieved by applying map OT(i+1)
M to the aligned T1WI xA(i)
T1.
Subsequently, the same optimization method is employed to
numerically obtain the map OT(i+1)
M and the maximizer ψ∗
ω.
B. The Complementarity between Two Tasks
When the high-quality reconstructed T2WI xR
T2is achieved,
the aligned T1WI xA
T1is more close to the ground truth xT2.
At the same time, the synthetic T2WI xG
T2has shorter distance
to the ground truth xT2inT2manifold. In the proposed
framework, we also aim to making the distance between the
distribution of the synthetic T2WI xG
T2and the reconstructed
T2WI xR
T2short by minimizing their distance to the ground
truth T2WI xT2. Therefore, the reconstruction task and the
cross-modal task benefit each other, which is presented in the
following theorem.
Theorem 1: LetxR
T2,xG
T2be the reconstructed T2WI and
the synthetic T2WI, and xT2be the full sampled T2WI, then6 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, XXXX 2023
we have the following inequality held:
∥xR
T2−xG
T2∥L1≤ ∥xR
T2−xT2∥L1+C∥xG
T2−xT2∥W1,(7)
where ∥ · ∥ L1and∥ · ∥ W1areL1and1-Wasserstein norms,
respectively, and the C > 0is a constant.
Proof: Let Ω⊂R3be the region of images, and it is compact.
It is a natural assumption that xT2, xR
T2, xG
T2, xT1, xA
T1∈
BV(Ω), where BV(Ω) is the set of all bounded variation
functions on Ω. Then, it is easily validated that (BV(Ω), L1),
is a metric space with L1-metric. Therefore, from the triangle
inequality of the distance, we have
∥xR
T2−xG
T2∥L1≤ ∥xR
T2−xT2∥L1+∥xG
T2−xT2∥L1.
On the other hand, because Ωis compact, from Remark
8.2 [33], there is ∥xG
T2−xT2∥W1≤diam (Ω)∥xG
T2−xT2∥L1,
where diam( Ω) is the diameter of the set Ω. Therefore, we
have the inequality (7) held, where the constant C=1
diam (Ω).
Remark: It is remarkable that the inequality (7) gives an
explanation of complementarity between two tasks. That is,
the error between the reconstructed T2WIs and the generated
T2WIs is controlled by the errors of two tasks. In other
words, with the iteration increasing, xR
T2andxG
T2become
more and more close. Due to the alternative updating, the
better reconstructed result improves the generating process,
while the better generated result enhances the reconstructing
process. It actually realizes two tasks complement each other.
C. The Training Scheme of the Proposed Method
As shown in Fig. 4, the reconstruction task under the deep
learning framework in this paper is formulated as:
LRec= min
θ∥xT2− R θ(xT2)∥1, (8)
where Rθis a T2WI reconstruction network which restores
the unclear target images. θare the optimized parameters of
the reconstruction network.
Similar to the work [19], in the spatial alignment process
OTS, if we blindly pursue the maximum similarity between
the two distributions p(xA
T1)andp(xR
T2), it will lead to image
distortion. Thus, a regularization term is introduced to avoid
extremely deforming the image, that is:
LReg=X
a∈S(b)H(a, b)∥ϕ(a)−ϕ(b)∥, (9)
where S(b)represents neighboring pixels of a pixel b.
H(a, b) = e−∥xA
T1[a]−xA
T1[b]∥is a bilateral filter which is
introduced to prevent over-smoothness. ϕis the estimated
displacement field. Finally, in this paper the overall function
of loss is formulated as
min
θ,φ,γ,ωαLRec+βLGφ+δLOTM+ηLReg, (10)
where α, β, δ, η are hyperparameters that need to be adjusted.
The detailed training procedure of the cross-modal recon-
struction is outlined in Algorithm 1.Algorithm 1 The Proposed Cross-modal Reconstruction
Method
Input: Paired fully-sampled T1WIs xT1and fully-sampled
T2WIs xT2.
Output: Optimized neural networks, Rθ∗,Gφ∗,Dγ∗,Dω∗.
1:repeat
2:xR
T2← R θ(¯xT2);
3:xA
T1←OTS 
xT1, xR
T2
;
4:xG
T2←OTM 
xA
T1
;
5: Use Eqs. (4), (6), (8) and (9) to calculate LGφ,LOTM,
LRecandLReg, respectively.
6:θ←θ−ϵ∂θ 
αLRec+βLGφ+δLOTM+ηLReg
7:φ←φ−ϵ∂φ 
αLRec+βLGφ+δLOTM+ηLReg
8: fori= 1 toIdo
9: m←m−ϵ∂mLOTM
10: ω←ω−ϵ∂ω
Dω(xT2)− D ω(xG(i−1)
T2)
11: xG(2i−1
2)
T2 ←OT(i)
M
xA(i−1)
T1
12: s←s−ϵ∂sLGφ
13: γ←γ−ϵ∂γ
Dγ(xT2)− D γ(xG(2i−1
2)
T2 )
14: xA(i)
T1←OT(i)
S(xT1)
15: xG(i)
T2←OT(i)
M
xA(i)
T1
16: end for
17:until Convergence
IV. E XPERIMENTS
In this Section, we first introduce the datasets in Section
IV-A. Implementation details and baselines are presented in
Section IV-B. In Section IV-C, we summarize and analyze the
experimental results from the view of quantitative and qual-
itative evaluation. Finally, Section IV-D provides an ablation
study to investigate the effectiveness of the proposed cross-
modal strategies.
A. Experimental Datasets
The proposed framework aims to reconstruct T2-weighted
axial brain MRIs using fully-sampled T1-weighted MRIs,
taking advantage of the complementary information provided
by these modalities. In this section, we evaluate the perfor-
mance of the proposed method in two MRI datasets: FastMRI,
which consists of partial brain MRI scans, and an in-house
dataset comprising whole brain MRI scans. Both datasets
are acquired from Siemens scanners and are available in
Digital Imaging and Communications in Medicine (DICOM)
and Neuroimaging Informatics Technology Initiative (NIfTI)
formats, respectively. To handle the 3D MRI volumes, we
decompose them into 2D slices and treat them as inputs to
the framework. In the FastMRI dataset, we adopt the same
selection criteria as the reference [19], which includes 340
pairs of T1 and T2 weighted axial brain MRIs. Specifically, we
use 170 pairs of volumes (2720 pairs of slices) for training, 68
pairs of volumes (1088 pairs of slices) for validation, and 102
pairs of volumes (1632 pairs of slices) for testing. The in-plane
size of T1WIs and T2WIs is 320 ×320, with a resolution of
0.68×0.68 and a slice spacing of 5mm.WANG et al. : SPATIAL AND MODAL OPTIMAL TRANSPORT FOR FAST CROSS-MODAL MRI RECONSTRUCTION 7
The in-house dataset consists of 36 subjects’ whole
brain MRI scans, including 3D paired T1WIs (TE= 1.4e+
02ms; Time= 161149 .853ms) and T2WIs (TE= 1.2e+ 02ms;
Time= 163011 .360ms). Out of these, 30 pairs of volumes
(2097 pairs of slices) are used for training, 3 pairs of volumes
(225 pairs of slices) for validation, and 3 pairs of volumes (225
pairs of slices) for testing. The in-plane spacing is 0.68mm ×
0.68mm, and the slice thickness is 3mm. Each slice has a
spatial size of 320 ×320.
B. Baselines and Implementation Details
The undersampled magnetic resonance images are obtained
by applying a sampling ratio of 25%,12.5%, or6.25% to mask
thek-space. We consider three sampling patterns: random,
equispaced, and radial. For random and equispaced sampling,
32% of low-spatial-frequency data captures the general shape
of organs. The rest focuses on finer details with high-spatial-
frequency data. The radial sampling method is different. It
captures data along radial lines in the k-space. This method
is effective for capturing structural details at various sampling
densities.
The reconstruction network, denoted as Rθ, is constructed
using cascades of E2E-VarNet architectures [46]. Each E2E-
VarNet architecture comprises four down-sampling CNN lay-
ers in the encoder and four up-sampling CNN layers in the
decoder. The spatial alignment module consists of a deforma-
tion generator and a re-sampling layer, with the deformation
field generator utilizing four down-sampling CNN layers in
the encoder and four up-sampling CNN layers in the de-
coder. The cross-modal synthesis process, denoted as OTM,
employs the same architecture as the reconstruction network.
The discriminator networks DγandDωtake complex-valued
inputs represented by two channels and output a real-valued
scalar indicating the similarity between the magnetic reso-
nance images. The Adam optimizer with a learning rate of
1×10−4is used for all networks. The reconstruction loss
LRec, spatial alignment loss LGφ, cross-modal image synthesis
lossLOTM, and smoothness loss LRegare weighted by 3000 ,
1,1, and 1000 , respectively. The experiments are trained until
convergence on a NVIDIA GeForce RTX 2080 Ti.
For the comparison, we evaluate the proposed OT-
based model against several competing methods: SAN[19],
MTrans[4], MT-NET[47], and MC-CDic[48]. SAN addresses
spatial discrepancies in cross-modal reconstruction. MTrans
uses an improved multi-head attention mechanism and a cross
attention module for rich multi-modal information capture.
MT-NET employs an Edge-preserving masked autoencoder for
contextual and structural information from a reference modal-
ity in cross-modal image synthesis. MC-CDic combines con-
volutional dictionary learning with deep unfolding techniques
for multi-contrast MRI super-resolution and reconstruction.
It shows effectiveness in diverse MRI contrasts and image
resolution enhancement. In our approach, we aim to combine
spatial alignment with cross-modal synthesis within the OT
framework. This effort seeks to enhance alignment accuracy
and potentially bridge modal differences.
To assess the reconstruction performance accurately and
comprehensively, we utilize Peak Signal-to-Noise Ratio(PSNR) [49], Structural Similarity Index Metric (SSIM) [50],
and Normalized Mean Square Error (NMSE) [51] as metrics
to measure the discrepancy between the reconstructed T2WI
and the real one.
C. Experiments on Two MRI Datasets
Fig. 3. The provided violin plot visually depicts the results of the
Fast dataset obtained through random sampling at a ratio of 6.25%
using random masks. The plot showcases three distinct violin-shaped
distributions, each representing the outcomes for PSNR, SSIM, and
NMSE obtained through different methods.
1) Quantitative Evaluation: We evaluate the performance
of the proposed cross-modal reconstruction algorithms using
metrics such as PSNR, SSIM, and NMSE, which quantify the
similarity between the restored image and the fully-sampled
ground truth image. The results of the reconstruction on
two magnetic resonance image datasets under various under-
sampling patterns are presented in Tables I and II. In these
tables, the best result is highlighted in bold.
Our analysis reveals that the OT-based method consistently
demonstrates superior performance at sample ratios of 12.5%
and25%. Even at a significantly low sample ratio of 6.25%,
the OT-based method outperforms other techniques, exhibiting
substantially higher PSNR and SSIM values, as well as lower
NMSE values.
While the SSIM of the OT-based method may be slightly
lower than that of SAN [19] and MC-CDic [48], it is important
to note that the OT-based method exhibits a smaller standard
deviation. Consequently, the improvement in PSNR is more
effective. For instance, on the FastMRI dataset with a sample
ratio of 12.5%using a random pattern, the OT-based method
enhances the PSNR result from 38.06 dB to 39.38 dB. From
Fig. 3, we can see that even if the sampling rate is very low,
we can still obtain good reconstruction results.
2) Qualitative Evaluation: The qualitative reconstruction re-
sults on the FastMRI dataset and the in-house dataset are
depicted in Fig. 4 and Fig. 5, providing insights into the per-
formance of different methods. The first, fourth, and seventh
rows showcase the reconstruction results of various methods at
a sample ratio of 6.25% using an equispaced pattern, a random
pattern, and a radial pattern, respectively. The second, fifth,
and eighth rows display the magnified detail images of these
reconstructions. The third, sixth, and ninth rows present the
corresponding error maps, which are obtained by subtracting
the reconstructed T2-weighted images (T2WIs) from the real
images. In the error map, regions appearing yellow indicate
higher error magnitudes, while more pronounced structures
indicate worse restoration quality.
Analyzing the results, we observe that the MTrans method
exhibits noticeable aliasing artifacts and significant loss of8 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, XXXX 2023
TABLE I
AVERAGE (WITH STANDARD DEVIATION )RECONSTRUCTION RESULTS ,
IN TERMS OF PSNR, SSIM AND NMSE ONFASTMRI WITH DIFFERENT
MASKS . THE BEST RESULT IS IN BOLD .
Ratio Masks Methods PSNR SSIM NMSE
25%
RandomZero-filling 26.97 ±1.16 0.7282 ±0.0360 0.0492 ±0.0286
MTrans [4] 35.34 ±1.39 0.9448 ±0.0071 0.0097 ±0.0013
MC-CDic [48] 39.92 ±2.85 0.9702 ±0.0156 0.0047 ±0.0084
SAN [19] 43.91 ±1.94 0.9888±0.0053 0.0019±0.0007
OT-Based 44.23±1.32 0.9882±0.0031 0.0012±0.0005EquispacedZero-filling 26.79 ±1.17 0.7321 ±0.0377 0.0515 ±0.0302
MTrans [4] 34.28 ±1.35 0.9593 ±0.0080 0.0121 ±0.0303
MC-CDic [48] 40.78 ±2.97 0.9771 ±0.0130 0.0051 ±0.0091
SAN [19] 40.81 ±1.92 0.9821±0.0069 0.0040±0.0009
OT-Based 41.46±0.44 0.9809±0.0007 0.0024±0.0005RadialZero-filling 33.37 ±1.44 0.6952 ±0.1404 0.0218 ±0.0140
MTrans [4] 36.82 ±1.28 0.9623 ±0.0210 0.0081 ±0.0043
MC-CDic [48] 43.01 ±1.00 0.9812 ±0.0107 0.0026 ±0.0077
SAN [19] 45.55 ±1.42 0.9902 ±0.0047 0.0020±0.0161
OT-Based 45.66±2.44 0.9905 ±0.0045 0.0024±0.0005
12.5%
RandomZero-filling 23.60 ±1.82 0.5462 ±0.0490 0.0991 ±0.0474
MTrans [4] 33.18 ±1.34 0.9318 ±0.0201 0.0154 ±0.0037
MC-CDic [48] 35.79 ±2.92 0.9498 ±0.0267 0.0117 ±0.0122
SAN [19] 38.06 ±2.04 0.9729 ±0.0103 0.0056 ±0.0007
OT-Based 39.38±1.00 0.9744 ±0.0037 0.0031 ±0.0003EquispacedZero-filling 24.76 ±1.81 0.5428 ±0.0488 0.0956 ±0.0458
MTrans [4] 32.10 ±1.25 0.9240 ±0.0172 0.0178 ±0.0122
MC-CDic [48] 36.64 ±2.19 0.9561 ±0.0242 0.0099 ±0.0142
SAN [19] 39.38 ±2.06 0.9775±0.0088 0.0036±0.0010
OT-Based 39.94±1.12 0.9765±0.0026 0.0035±0.0006RadialZero-filling 29.44 ±1.88 0.5675 ±0.0649 0.0533 ±0.0286
MTrans [4] 36.20 ±1.90 0.9427 ±0.0209 0.0092 ±0.0402
MC-CDic [48] 39.75 ±2.14 0.9694 ±0.0180 0.0051 ±0.0107
SAN [19] 41.85 ±1.37 0.9824 ±0.0083 0.0036 ±0.0130
OT-Based 42.52±2.37 0.9838 ±0.0078 0.0032 ±0.0117
6.25%
RandomZero-filling 24.43 ±1.87 0.4850 ±0.0552 0.1500 ±0.0137
MTrans [4] 30.78 ±1.78 0.9193 ±0.0106 0.0206 ±0.0078
MC-CDic [48] 34.38 ±2.07 0.9434 ±0.0287 0.0162 ±0.0068
SAN [19] 33.91 ±1.28 0.9488 ±0.0073 0.0121 ±0.0011
OT-Based 36.30±1.27 0.9512 ±0.0061 0.0089 ±0.0004EquispacedZero-filling 23.96 ±1.56 0.4589 ±0.1491 0.1639 ±0.0529
MTrans [4] 29.63 ±1.35 0.8922 ±0.0103 0.0273 ±0.0069
MC-CDic [48] 34.78 ±2.69 0.9466 ±0.0274 0.0151 ±0.0172
SAN [19] 34.76 ±1.74 0.9474 ±0.0062 0.0175 ±0.0006
OT-Based 35.45±1.42 0.9573 ±0.0097 0.0143 ±0.0087RadialZero-filling 26.79 ±1.68 0.4773 ±0.1632 0.0937 ±0.0438
MTrans [4] 32.13 ±1.56 0.9103 ±0.0520 0.0103 ±0.0398
MC-CDic [48] 37.41 ±2.93 0.9596 ±0.0212 0.0089 ±0.0198
SAN [19] 38.60 ±2.36 0.9703 ±0.0136 0.0069 ±0.0188
OT-Based 39.92±2.31 0.9749 ±0.0123 0.0053 ±0.0145TABLE II
AVERAGE (WITH STANDARD DEVIATION )RECONSTRUCTION RESULTS ,
IN TERMS OF PSNR, SSIM AND NMSE ON IN -HOUSE WITH DIFFERENT
MASKS . THE BEST RESULT IS IN BOLD .
Ratio Masks Methods PSNR SSIM NMSE
25%
RandomZero-filling 25.33 ±1.28 0.7738 ±0.0564 0.2386 ±0.0973
MTrans [4] 36.93 ±1.67 0.9730 ±0.0201 0.0395 ±0.0073
MC-CDic [48] 37.10 ±3.04 0.9863 ±0.0082 0.0361 ±0.0225
SAN [19] 38.12 ±1.81 0.9940 ±0.0072 0.0061 ±0.0007
OT-Based 38.48±1.76 0.9946 ±0.0023 0.0057 ±0.0008EquispacedZero-filling 25.27 ±1.33 0.7799 ±0.0545 0.2433 ±0.1022
MTrans [4] 32.50 ±2.10 0.9763 ±0.0135 0.0609 ±0.0097
MC-CDic [48] 35.02 ±2.26 0.9882 ±0.0068 0.0332 ±0.0235
SAN [19] 34.99 ±1.79 0.9898 ±0.0041 0.0416 ±0.0073
OT-Based 35.87±1.51 0.9922 ±0.0032 0.0306 ±0.0015RadialZero-filling 25.94 ±1.68 0.7143 ±0.0732 0.2178 ±0.1069
MTrans [4] 35.71 ±1.28 0.9320 ±0.0123 0.0389 ±0.0249
MC-CDic [48] 37.15 ±2.63 0.9899 ±0.0062 0.0203 ±0.0165
SAN [19] 41.67 ±1.29 0.9961 ±0.0013 0.0062 ±0.0043
OT-Based 43.81±1.52 0.9971 ±0.0011 0.0039 ±0.0031
12.5%
RandomZero-filling 24.05 ±1.27 0.6552 ±0.0525 0.2993 ±0.0936
MTrans [4] 29.09 ±1.07 0.9629 ±0.0056 0.0705 ±0.0036
MC-CDic [48] 30.61 ±2.14 0.9750 ±0.0108 0.0732 ±0.0387
SAN [19] 30.70 ±1.34 0.9748 ±0.0029 0.0431 ±0.0093
OT-Based 31.39±1.09 0.9825 ±0.0037 0.0348 ±0.0037EquispacedZero-filling 23.91 ±3.14 0.6515 ±0.0510 0.3070 ±0.0933
MTrans [4] 28.39 ±1.73 0.9566 ±0.0100 0.0806 ±0.0079
MC-CDic [48] 31.76 ±2.37 0.9796 ±0.0098 0.0578 ±0.0297
SAN [19] 31.63 ±1.24 0.9812 ±0.0107 0.0378 ±0.0009
OT-Based 32.56±1.34 0.9849 ±0.0071 0.0240 ±0.0008RadialZero-filling 24.54 ±1.85 0.5835 ±0.0231 0.2797 ±0.1142
MTrans [4] 31.23 ±2.15 0.9001 ±0.0204 0.0410 ±0.0160
MC-CDic [48] 36.02 ±2.19 0.9882 ±0.0066 0.0240 ±0.0164
SAN [19] 37.79 ±1.37 0.9930 ±0.0029 0.0147 ±0.0092
OT-Based 38.68±2.80 0.9939 ±0.0024 0.0127 ±0.0089
6.25%
RandomZero-filling 20.70 ±1.26 0.5367 ±0.0497 0.3912 ±0.0757
MTrans [4] 23.23 ±2.15 0.9201 ±0.0089 0.2303 ±0.0127
MC-CDic [48] 25.45 ±2.16 0.9441 ±0.0140 0.2159 ±0.0771
SAN [19] 26.47 ±1.12 0.9557 ±0.0064 0.1473 ±0.0037
OT-Based 27.56±1.05 0.9653 ±0.0033 0.1269 ±0.0021EquispacedZero-filling 20.59 ±2.23 0.5375 ±0.0022 0.4000 ±0.0741
MTrans [4] 26.03 ±1.38 0.9303 ±0.0128 0.2596 ±0.0137
MC-CDic [48] 26.45 ±2.38 0.9645 ±0.0143 0.1743 ±0.0501
SAN [19] 27.09 ±1.36 0.9617 ±0.0083 0.1694 ±0.0089
OT-Based 27.39±1.27 0.9652 ±0.0049 0.1433 ±0.0014RadialZero-filling 23.45 ±2.95 0.4699 ±0.0785 0.3394 ±0.1028
MTrans [4] 28.31 ±1.45 0.9087 ±0.0078 0.0834 ±0.0311
MC-CDic [48] 33.26 ±2.49 0.9825 ±0.0087 0.0414 ±0.0241
SAN [19] 32.09 ±1.30 0.9821 ±0.0082 0.0541 ±0.0329
OT-Based 34.16±2.34 0.9875 ±0.0057 0.0339 ±0.0212WANG et al. : SPATIAL AND MODAL OPTIMAL TRANSPORT FOR FAST CROSS-MODAL MRI RECONSTRUCTION 9
Fig. 4. Comparison of the reconstruction results on the FastMRI dataset
is shown. The first row shows outcomes from 16× equispaced sampling.
The fourth row reveals results from 16× random sampling. Lastly, the
seventh row depicts outcomes using 16× radial sampling.
anatomical structures. On the other hand, the SAN method,
as a representative of cross-modal reconstruction, provides
reconstructions with relatively clearer details. However, the
OT-based method surpasses both methods by achieving further
improvements. Specifically, at a sample ratio of 6.25% using
a random pattern, the OT-based method demonstrates the
lowest reconstruction error and preserves anatomical details
significantly.
D. Ablation Study
In this Section, we analyze the complementary nature of
tasks, highlighting the importance of integrating multiple
modalities for enhanced performance. Then, we demonstrate
the essentiality of spatial and modal decomposition in the
cross-modal synthesis process. Furthermore, We then delve
into the significance of alternating iteration modules for image
spatial alignment, emphasizing the necessity of addressing
spatial misalignment to improve the synthesis process. Lastly,
we provide a comprehensive analysis of the computational
complexity of the proposed method.
1) Complementary of Two Tasks: To assess the impact of the
Cross-Modal Synthesis (CMS) process, a series of experiments
was conducted on both FastMRI data and in-house data using
Fig. 5. Comparison of the reconstruction results on the in-house
dataset is shown. The first row shows outcomes from 16× equispaced
sampling. The fourth row reveals results from 16× random sampling.
Lastly, the seventh row depicts outcomes using 16× radial sampling.
different sampling patterns with random masks. The primary
focus of these experiments was to evaluate the performance
of the reconstruction process. Specifically, we compared the
reconstructed images only from the under-sampled data with
the fully-sampled ground truth images in a setting referred to
as ‘Without CMS’. To measure the accuracy and quality of the
reconstructed images, similar quantitative metrics were em-
ployed. The obtained results, presented in Table III, illustrate
that the reconstruction process effectively recovers missing
details and enhances the overall fidelity of the reconstructed
images, thanks to the integration of the cross-modal synthesis
process.
To demonstrate the impact of the information provided
by low-quality images, this study compares the proposed
method in this paper with MT-NET. The MT-NET method
only utilizes information from the auxiliary modality for
reconstruction. The paired dataset used for evaluation is 100% .
The results on the FastMRI data and in-house data datasets
are presented in Table IV. From the experimental results,
it can be observed that even when reconstructing with low-
quality images obtained at a sampling rate of 6.25%, our
method still outperforms MT-NET. This indicates the need for
simultaneous acquisition of low-quality to reconstruct high-10 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, XXXX 2023
TABLE III
ABLATION STUDY ON THE IMPACT OF CROSS -MODAL SYNTHESIS PROCESS . THE EXPERIMENTS ARE CARRIED OUT ON TWO DATASETS USING
RANDOM MASKS ,AND THE BEST RESULT IS INDICATED IN BOLD .
MethodsSampling Ratio of 6.25% on FastMRI Dataset Sampling Ratio of 6.25% on In-House Dataset
PSNR SSIM NMSE PSNR SSIM NMSE
Without CMS 33.48 ±1.63 0.9400 ±0.0200 0.0189 ±0.0009 25.78 ±1.23 0.9431 ±0.0003 0.1789 ±0.0103
With CMS 36.30±1.27 0.9512 ±0.0061 0.0089 ±0.0004 27.56 ±1.05 0.9653 ±0.0033 0.1269 ±0.0021
quality target images. In the future, we will further improve
the reconstruction results at even lower sampling rates.
2) Spatial and Modal Decomposition in CMS Process: To
verify the necessity of eliminating spatial differences among
target images and auxiliary images, we remove the effect of
the Image Spatial Alignment (ISA) module. As a result, the
original under-sampled target images and the fully-sampled
auxiliary images directly are entered into the cross-modality
synthesis network for high-quality target images. We define
this setting as ‘Without ISA’ and define the proposed method
as ‘With ISA’. In the ‘Without ISA’ setting, we directly
input the misalignment images into the cross-modal synthesis
network. From Table V, the value of PSNR is improved from
33.99 dB to 35.45 dB, the value of SSIM from 0.9341 to
0.9573 and the value of NMSE is reduced from 0.0217 to
0.0143 on the FastMRI dataset. Notably, it is necessary to pay
attention to the misalignment. Furthermore, the reconstruction
performance will be advanced by integrating the OT-Based
method. Further, we employed the UMAP technique for 3D
projection of the data to assess the impact of the ISA module.
In Fig. 6, the UMAP visualization of 169 3D brain images
from the FastMRI dataset is presented, revealing the distribu-
tion characteristics among Original T1WIs, Aligned T1WIs,
Synthetic T2WIs, and Ground Truth T2WIs. The findings
demonstrated that Aligned T1WIs are closer to the Ground
Truth T2WIs, validating the effectiveness of the ISA module.
Similarly, the close grouping of Synthetic T2WIs with Ground
Truth T2WIs showcased the high fidelity of the synthesis
process.
3) Alternating Iteration Strategy for Image Spatial Alignment:
In this section, we show our superior alignment accuracy and
improved preservation of structural details. As depicted in Fig.
7, upon the length of the yellow bidirectional, we can observe
that the proposed method achieves precise spatial alignment
during the cross-modal synthesis process. The integration of
alignment with the reconstruction process leads to enhanced
alignment accuracy, improved preservation of structural de-
tails, and overall superior performance. This demonstrates the
effectiveness of our approach in achieving accurate alignment
and producing visually coherent synthesized images.
4) Computational Complexity: Computational complexity is
a crucial measure for evaluating algorithm performance. Ta-
ble VI presents three key complexity metrics, including the
number of parameters, the amount of multiply-accumulates
(MACs), and memory requirements. MACs are measured
using the ptflops tool, while memory footprint is computed on
a Nvidia GeForce RTX GPU. In this study, both the proposed
method and SAN consider the issue of spatial misalignment,
Fig. 6. UMAP 3D Projection Analysis of MRI Image Distributions. The
UMAP visualization shows the distribution patterns within a collection
of 169 3D brain images from the FastMRI dataset, highlighting the
distinctions between Original T1WIs, Aligned T1WIs, Synthetic T2WIs,
and Ground Truth T2WIs.
Fig. 7. A subtle spatial misalignment between the auxiliary (T1-
weighted) image and the target (T2-weighted) image is noticeable in
this figure. To address this concern, the approach SAN proposed in [19]
utilizes a spatial alignment technique to produce an aligned-auxiliary
image. ‘SAN Alignment’ refers to the T1-weighted image warped using
the SAN method proposed in [19], whereas the ‘OT -Based Alignment’ is
generated using our own method.
allowing for a fair comparison of their computational com-
plexities.
Comparing the computational complexities of the proposed
method with SAN, the OT-Based method exhibits superior
results with fewer parameters, lower MACs, and reduced
memory requirements. For instance, the proposed method only
requires 27.86 M parameters, whereas SAN necessitates 47.23
M parameters. The proposed method significantly reduces theWANG et al. : SPATIAL AND MODAL OPTIMAL TRANSPORT FOR FAST CROSS-MODAL MRI RECONSTRUCTION 11
TABLE IV
ABLATION STUDY ON THE INFLUENCE OF INFORMATION PROVIDED BY LOW -QUALITY TARGET IMAGES . THE FIRST ROW PRESENTS THE RESULTS OF
THEMT-NET METHOD ON BOTH THE FASTMRI DATASET AND OUR IN -HOUSE DATASET . THIS METHOD DOES NOT UTILIZE THE INFORMATION FROM
LOW-QUALITY T2WI S. THE SECOND AND THIRD ROWS DISPLAY THE RESULTS OF THE PROPOSED METHOD AT A 6.25% SAMPLING RATE .
Methods MasksFastMRI Dataset In-House Dataset
PSNR SSIM NMSE PSNR SSIM NMSE
MT-NET [47] ——– 30.31 ±2.13 0.9400 ±0.0200 0.0189 ±0.0009 26.31 ±1.47 0.9511 ±0.0120 0.1721 ±0.0211
OT-BasedRandom 36.30 ±1.27 0.9512 ±0.0061 0.0089 ±0.0004 27.56 ±1.05 0.9653 ±0.0033 0.1269 ±0.0021
Equispaced 35.45 ±1.42 0.9573 ±0.0097 0.0143 ±0.0087 27.39 ±1.27 0.9652 ±0.0049 0.1433 ±0.0014
TABLE V
ABLATION STUDY ON THE ESSENTIALITY OF SPATIAL AND MODAL DECOMPOSITION IN CROSS -MODAL SYNTHESIS PROCESS . THE EXPERIMENTS
ARE CARRIED OUT ON TWO DATASETS USING EQUISPACED MASKS ,AND THE BEST RESULT IS INDICATED IN BOLD .
MethodsSampling Ratio of 6.25% on FastMRI Dataset Sampling Ratio of 6.25% on In-House Dataset
PSNR SSIM NMSE PSNR SSIM NMSE
Without ISA 33.99 ±1.74 0.9341 ±0.0107 0.0217 ±0.0071 26.89 ±1.39 0.9423 ±0.0109 0.1799 ±0.0082
With ISA 35.45±1.42 0.9573 ±0.0097 0.0143 ±0.0087 27.39 ±1.27 0.9652 ±0.0049 0.1433 ±0.0014
TABLE VI
COMPUTATIONAL COMPLEXITY COMPARISON ON THE FASTMRI
DATDASET .
Methods Parameters (M) MACs (G) Memory(MiB)
SAN [19] 47.23 174.71 352.21
OT-Based 27.86 104.01 224.97
parameter count compared to SAN. Notably, our method also
achieves a notable reduction in memory requirements, reduc-
ing it from 352.21 MiB to 224.97 MiB. These results clearly
demonstrate the effectiveness of the proposed algorithm.
V. CONCLUSION
In this paper, our focus is on accelerating reconstruction of
T2WIs. To achieve it, we employ a reconstruction network
that takes the under-sampled T2WI as input and produces
a high-quality reconstructed T2WI. In order to leverage the
complementary information across different modalities, we
introduce a cross-modal synthesis process to generate syn-
thetic T2WI. We prove that the cross-modal synthesis task
and the reconstruction task mutually support each other by
minimizing the difference between the reconstructed T2WI
and the synthetic one in the T2manifold. Moreover, to
eliminate the negative transfer of subtle misalignment between
paired images from the T1-weighted modality and the T2-
weighted modality, we decompose the cross-modal synthesis
process into two OT processes. The one OT process focuses
on spatial alignment mapping of the T1-weighted modality,
while the other OT process synthesizes the mapping from
the T1-weighted modality to the T2-weighted modality. This
decomposition enhances the interpretability of the cross-modal
synthesis process. Finally, experimental evaluations on both
the FastMRI dataset and an in-house whole brain MRI dataset
confirm the effectiveness of the proposed method.REFERENCES
[1] K. Krupa and M. Bekiesi ´nska-Figatowska, “Artifacts in
magnetic resonance imaging,” Polish Journal of Radi-
ology , vol. 80, pp. 93–106, 2015.
[2] V . Calhoun and J. Sui, “Multimodal fusion of brain
imaging data: A key to finding the missing link(s) in
complex mental illness,” Biological Psychiatry: Cog-
nitive Neuroscience and Neuroimaging , vol. 1, no. 3,
pp. 230–244, 2016.
[3] B. Zhou and S. Zhou, “DuDoRNet: Learning a dual-
domain recurrent network for fast MRI reconstruction
with deep T1 prior,” in Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition ,
2020, pp. 4273–4282.
[4] C. Feng, Y . Yan, G. Chen, et al. , “Multi-modal trans-
former for accelerated MR imaging,” IEEE Transac-
tions on Medical Imaging , 2022.
[5] M. Lustig, D. Donoho, and J. Pauly, “Sparse MRI:
The application of compressed sensing for rapid MR
imaging,” Magnetic Resonance in Medicine , vol. 58,
no. 6, pp. 1182–1195, 2007.
[6] L. Xiang, Y . Chen, W. Chang, et al. , “Ultra-fast T2-
weighted MR reconstruction using complementary T1-
weighted information,” in Medical Image Computing
and Computer-Assisted Intervention , 2018, pp. 215–
223.
[7] C. Feng, H. Fu, S. Yuan, and Y . Xu, “Multi-contrast
MRI super-resolution via a multi-stage integration net-
work,” in Proceedings of the International Conference
on Medical Image Computing and Computer-Assisted
Intervention , 2021, pp. 140–149.
[8] G. Yiasemis, J. Sonke, C. S ´anchez, and J. Teuwen,
“Recurrent variational network: A deep learning inverse
problem solver applied to the task of accelerated MRI
reconstruction,” in Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition ,
2022, pp. 732–741.12 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, XXXX 2023
[9] B. Zhou, J. Schlemper, N. Dey, et al. , “Dual-domain
self-supervised learning for accelerated non-cartesian
MRI reconstruction,” Medical Image Analysis , vol. 81,
p. 102 538, 2022.
[10] L. Xiang, Y . Chen, W. Chang, et al. , “Deep-learning-
based multi-modal fusion for fast MR reconstruction,”
IEEE Transactions on Biomedical Engineering , vol. 66,
no. 7, pp. 2105–2114, 2018.
[11] Y . Li, T. Zhou, K. He, Y . Zhou, and D. Shen, “Multi-
scale transformer network with edge-aware pre-training
for cross-modality MR image synthesis,” IEEE Trans-
actions on Medical Imaging , 2023.
[12] X. Liu, J. Wang, J. Jin, et al. , “Deep unregistered
multi-contrast MRI reconstruction,” Magnetic Reso-
nance Imaging , vol. 81, pp. 33–41, 2021.
[13] B. Zhou, N. Dey, J. Schlemper, et al. , “DSFormer: A
dual-domain self-supervised transformer for accelerated
multi-contrast MRI reconstruction,” in Proceedings of
the IEEE/CVF winter conference on applications of
computer vision , 2023, pp. 4955–4964.
[14] X. Liu, J. Wang, S. Lin, S. Crozier, and F. Liu, “Opti-
mizing multicontrast MRI reconstruction with shareable
feature aggregation and selection,” Nuclear Magnetic
Resonance in Biomedicine , vol. 34, no. 8, e4540, 2021.
[15] B. B. Avants, N. J. Tustison, G. Song, P. A. Cook,
A. Klein, and J. C. Gee, “A reproducible evaluation of
ants similarity metric performance in brain image reg-
istration,” Neuroimage , vol. 54, no. 3, pp. 2033–2044,
2011.
[16] W. D. Penny, K. J. Friston, J. T. Ashburner, S. J. Kiebel,
and T. E. Nichols, Statistical parametric mapping: the
analysis of functional brain images . Elsevier, 2011.
[17] M. Jenkinson, P. Bannister, M. Brady, and S. Smith,
“Improved optimization for the robust and accurate lin-
ear registration and motion correction of brain images,”
Neuroimage , vol. 17, no. 2, pp. 825–841, 2002.
[18] M. Dadar, V . S. Fonov, D. L. Collins, A. D. N. Initia-
tive, et al. , “A comparison of publicly available linear
mri stereotaxic registration techniques,” Neuroimage ,
vol. 174, pp. 191–200, 2018.
[19] K. Xuan, L. Xiang, X. Huang, et al. , “Multi-modal MRI
reconstruction assisted with spatial alignment network,”
IEEE Transactions on Medical Imaging , vol. 41, no. 9,
pp. 2499–2509, 2022.
[20] M. Arjovsky, S. Chintala, and L. Bottou, “Wasserstein
generative adversarial networks,” in International con-
ference on machine learning , PMLR, 2017, pp. 214–
223.
[21] N. Lei, D. An, Y . Guo, et al. , “A geometric under-
standing of deep learning,” Engineering , vol. 6, no. 3,
pp. 361–374, 2020.
[22] Z. Lai, X. Qu, H. Lu, et al. , “Sparse MRI reconstruction
using multi-contrast image guided graph representa-
tion,” Magnetic Resonance Imaging , vol. 43, pp. 95–
104, 2017.
[23] S. Wang, Z. Su, L. Ying, et al. , “Accelerating magnetic
resonance imaging via deep learning,” in Proceedingsof the IEEE International Symposium on Biomedical
Imaging , 2016, pp. 514–517.
[24] N. Dhengre and S. Sinha, “Multiscale U-net-based ac-
celerated magnetic resonance imaging reconstruction,”
Signal, Image and Video Processing , vol. 16, no. 4,
pp. 881–888, 2022.
[25] P. Guo, J. Valanarasu, P. Wang, J. Zhou, S. Jiang,
and V . Patel, “Over-and-under complete convolutional
RNN for MRI reconstruction,” in Proceedings of the
International Conference on Medical Image Computing
and Computer-Assisted Intervention , 2021, pp. 13–23.
[26] M. Sandilya and S. Nirmala, “Compressed sensing MRI
reconstruction using convolutional dictionary learning
and laplacian prior,” in IOT with Smart Systems , 2022,
pp. 661–669.
[27] C. Feng, Y . Yan, S. Wang, Y . Xu, L. Shao, and H.
Fu, “Specificity-preserving federated learning for MR
image reconstruction,” IEEE Transactions on Medical
Imaging , 2022.
[28] T. Du, H. Zhang, Y . Li, et al. , “Adaptive convolutional
neural networks for accelerating magnetic resonance
imaging via k-space data interpolation,” Medical Image
Analysis , vol. 72, p. 102 098, 2021.
[29] X. Dong, N. Yokoya, L. Wang, and T. Uezato, “Learn-
ing mutual modulation for self-supervised cross-modal
super-resolution,” in European Conference on Computer
Vision , 2022, pp. 1–18.
[30] W. Yin, H. Yin, K. Baraka, D. Kragic, and M.
Bj¨orkman, “Dance style transfer with cross-modal trans-
former,” in IEEE Winter Conference on Applications of
Computer Vision , 2023, pp. 5058–5067.
[31] A. Jog, A. Carass, S. Roy, D. Pham, and J. Prince, “Ran-
dom forest regression for magnetic resonance image
synthesis,” Medical Image Analysis , vol. 35, pp. 475–
488, 2017.
[32] I. Gulrajani, K. Kumar, F. Ahmed, et al. , “PixelV AE:
A latent variable model for natural images,” in Pro-
ceedings of the International Conference on Learning
Representations , 2017.
[33] G. Peyr ´e and M. Cuturi, “Computational optimal trans-
port,” Center for Research in Economics and Statistics
Working Papers , no. 2017-86, 2017.
[34] Y . Tian, M. Cuturi, and D. Ha, “Simultaneous multiple-
prompt guided generation using differentiable optimal
transport,” International Conference on Computational
Creativity , 2022.
[35] C. Villani, Optimal transport: old and new . Springer
Science & Business Media, 2008, vol. 338.
[36] J. Zhang, T. Liu, and D. Tao, “An optimal transport
analysis on generalization in deep learning,” IEEE
Transactions on Neural Networks and Learning Sys-
tems, pp. 1–12, 2021.
[37] W. Wang, F. Wen, Z. Yan, and P. Liu, “Optimal transport
for unsupervised denoising learning,” IEEE Transac-
tions on Pattern Analysis and Machine Intelligence ,
2022.
[38] B. Kamsu-Foguem, S. Msouobu Gueuwou, and C.
Kounta, “Generative adversarial networks based on opti-WANG et al. : SPATIAL AND MODAL OPTIMAL TRANSPORT FOR FAST CROSS-MODAL MRI RECONSTRUCTION 13
mal transport: A survey,” Artificial Intelligence Review ,
pp. 1–51, 2022.
[39] R. Turrisi, R. Flamary, A. Rakotomamonjy, and M.
Pontil, “Multi-source domain adaptation via weighted
joint distributions optimal transport,” in Uncertainty in
Artificial Intelligence , 2022, pp. 1970–1980.
[40] G. Oh, B. Sim, H. Chung, L. Sunwoo, and J. Ye,
“Unpaired deep learning for accelerated MRI using
optimal transport driven cyclegan,” IEEE Transactions
on Computational Imaging , vol. 6, pp. 1285–1296,
2020.
[41] D. An, Y . Guo, M. Zhang, X. Qi, N. Lei, and X. Gu,
“AE-OT-GAN: Training GANs from data specific latent
distribution,” in European Conference on Computer
Vision , 2020, pp. 548–564.
[42] G. Monge, “M ´emoire sur la th ´eorie des d ´eblais et des
remblais,” Histoire de l’Academie Royale d ´es Sciences
de Paris , pp. 666–704, 1781.
[43] L. Kantorovitch, “On the translocation of masses,”
Journal of Mathematical Sciences , vol. 5, no. 1, pp. 1–4,
1958.
[44] J. Tenenbaum, V . Silva, and J. Langford, “A global
geometric framework for nonlinear dimensionality re-
duction,” science , vol. 290, no. 5500, pp. 2319–2323,
2000.
[45] T. Miyato, T. Kataoka, M. Koyama, and Y . Yoshida,
“Spectral normalization for generative adversarial net-
works,” arXiv preprint arXiv:1802.05957 , 2018.
[46] A. Sriram, J. Zbontar, T. Murrell, et al. , “End-to-End
variational networks for accelerated MRI reconstruc-
tion,” in Proceedings of the International Conference
on Medical Image Computing and Computer-Assisted
Intervention , 2020, pp. 64–73.
[47] Y . Li, T. Zhou, K. He, Y . Zhou, and D. Shen, “Multi-
scale transformer network with edge-aware pre-training
for cross-modality mr image synthesis,” IEEE Transac-
tions on Medical Imaging , 2023.
[48] P. Lei, F. Fang, G. Zhang, and M. Xu, “Deep unfolding
convolutional dictionary model for multi-contrast mri
super-resolution and reconstruction,” in Proceedings of
the Thirty-Second International Joint Conference on
Artificial Intelligence , E. Elkind, Ed., Main Track, In-
ternational Joint Conferences on Artificial Intelligence
Organization, Aug. 2023, pp. 1008–1016.
[49] A. Hore and D. Ziou, “Image quality metrics: PSNR vs.
SSIM,” in Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , IEEE, 2010,
pp. 2366–2369.
[50] Z. Wang, A. Bovik, H. Sheikh, and E. Simoncelli,
“Image quality assessment: From error visibility to
structural similarity,” IEEE Transactions on Image Pro-
cessing , vol. 13, no. 4, pp. 600–612, 2004.
[51] A. Poli and M. Cirillo, “On the use of the normal-
ized mean square error in evaluating dispersion model
performance,” Atmospheric Environ. Part A. General
Topics , vol. 27, no. 15, pp. 2427–2434, 1993.