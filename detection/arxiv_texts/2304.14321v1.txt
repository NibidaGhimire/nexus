Person Re-ID through Unsupervised Hypergraph
Rank Selection and Fusion
Lucas Pascotti Valem, Daniel Carlos Guimar Àúaes Pedronette
Department of Statistics, Applied Mathematics and Computing (DEMAC),
SÀúao Paulo State University (UNESP), Rio Claro, Brazil
Abstract
Person Re-ID has been gaining a lot of attention and nowadays is of funda-
mental importance in many camera surveillance applications. The task consists of
identifying individuals across multiple cameras that have no overlapping views.
Most of the approaches require labeled data, which is not always available, given
the huge amount of demanded data and the difÔ¨Åculty of manually assigning a
class for each individual. Recently, studies have shown that re-ranking methods
are capable of achieving signiÔ¨Åcant gains, especially in the absence of labeled
data. Besides that, the fusion of feature extractors and multiple-source training
is another promising research direction not extensively exploited. We aim to Ô¨Åll
this gap through a manifold rank aggregation approach capable of exploiting the
complementarity of different person Re-ID rankers. In this work, we perform
a completely unsupervised selection and fusion of diverse ranked lists obtained
from multiple and diverse feature extractors. Among the contributions, this work
proposes a query performance prediction measure that models the relationship
among images considering a hypergraph structure and does not require the use of
any labeled data. Expressive gains were obtained in four datasets commonly used
for person Re-ID. We achieved results competitive to the state-of-the-art in most
of the scenarios.
Keywords: Person Re-ID, Unsupervised, Hypergraph, Rank, Selection, Fusion
Preprint submitted to Image and Vision ComputingarXiv:2304.14321v1  [cs.CV]  27 Apr 20231. Introduction
Person Re-ID is of critical importance in the majority of modern security and
surveillance applications [1‚Äì4]. The task consists in, given a query image of one
person, to identify the same individual across different cameras which have no
overlapping views. There are many difÔ¨Åculties for Re-ID retrieval [5], among
them: (i)different viewpoints, (ii)possible low-image resolutions, (iii)illumina-
tion changes, (iv)occlusions, (v)difÔ¨Åculty of manually labeling data for training,
(vi)large amount of data to be processed. The challenge of improving the effec-
tiveness of these systems, especially in open-world scenarios, has attracted a lot
of research efforts from the scientiÔ¨Åc community [4‚Äì6].
Initially, person Re-ID retrieval systems were mainly based on the use of hand-
crafted feature representations [7‚Äì15]. Besides that, other strategies commonly
used to generic image retrieval, like bag of visual words [13, 16], have also been
employed. Aiming at further improving the quality of the results, rather than
using traditional distance measures, researchers have proposed metric learning
approaches for Re-ID [11, 17‚Äì20], most of them based on supervised models.
In [6], an extensive evaluation of multiple combinations of feature extractors and
metric learning approaches is discussed.
Due to the signiÔ¨Åcant impact of deep learning on common image retrieval and
machine learning [5], the Convolutional Neural Networks (CNN) also have gained
a lot of attention and have been widely employed to solve Re-ID tasks in the recent
years [5, 21‚Äì29]. Among the works, existing networks have been trained for Re-
ID [30, 31], and some new architectures have been proposed solely focusing on
Re-ID [21‚Äì25].
Despite the success of deep learning, one of the main difÔ¨Åculties resides in
the lack of large publicly available Re-ID datasets for training [5], mainly be-
2cause manually labeling images is a very difÔ¨Åculty task, especially in open-world
scenarios [6], where the datasets may increase dynamically. In order to mitigate
this problem, many authors have proposed multi-source training, which consists
in joining multiple available datasets for training [26, 32, 33], usually increasing
the network capacity for generalization.
However, despite several advances in multi-source approaches, it is inevitable
the need for intensive manual annotation for obtaining training data. In practice,
the demand for extensive training data restricts the generalization and scalability
of supervised approaches, especially on person Re-ID tasks, which are not only
resource-intensive to acquire identity annotation but also impractical for large-
scale data [34, 35].
Usually, there are two main steps associated to Re-ID labeling process [5, 36,
37]:(i)intra-camera annotation that requires to compare a person with all the other
unlabeled persons in a single camera with multiple views; and (ii)inter-camera
that requires to match a person across different cameras with multiple views. Let P
be the number of persons and Sthe number of camera views. The intra-camera an-
notation complexity is O(SP2)and inter-camera annotation complexity ranges
from O(SP2)andO(S2P2). The worst case of inter-camera occurs because
not all persons appear in every camera view in the majority of cases, which makes
the association requiring to repeat for all Vcamera views [36, 37]. Commonly,
inter-camera association signiÔ¨Åcantly increases standard annotation costs. There
are approaches that mitigate these issues; among them there is the Intra-Camera
Supervised (ICS) [36, 37], Multi-Task Multi-Label (MATE) [36, 37], and Cross-
camera Feature Prediction [38], which were recently proposed.
Due to the challenge of obtaining large amounts of strongly labeled data, semi-
supervised methods have been employed, which is a typical strategy for supervi-
sion minimization. Based on information learned from a small set of labeled data,
3the idea is to generate labels from unlabeled training data. Some research has been
made in this direction [39‚Äì42]. However, these methods often suffer from perfor-
mance degradation and often require a large proportion of expensive cross-view
pairwise labeling [36].
There are also weakly supervised strategies that replace accurate labels with
inaccurate annotations. In [43], the authors proposed the idea of obtaining multi-
ple bounding boxes of the same person from untrimmed videos. This is done by
training a deep learning model capable of extracting multiple bounding boxes of
the same person in a video. Recently, [44] proposed to replace image-level anno-
tations with bag-level annotations. Weakly supervised Re-ID is very challenging,
since it is rather difÔ¨Åcult to model the considerable variances across camera views
(e.g. occlusion and illumination) without using strong label data [36, 43].
As an alternative solution, unsupervised approaches [34, 35, 45‚Äì51] have been
attracting a lot of attention from the research community, especially because,
once labeled data are not required, the methods become more suitable for real-
world scenarios. In a promising research direction, to address the lack of labels
issue, there are works proposed to post-process person Re-ID results by analyzing
similarity relationships encoded in the datasets. Several authors have proposed
unsupervised re-ranking approaches for Re-ID [52‚Äì60]. In [52], the original dis-
tances among images are improved by calculating Jaccard correlation scores of
each ranked list. Various approaches exploit the reciprocal neighborhood infor-
mation and other co-occurrence indexes aiming at improving the ranking results.
Another strategy commonly employed on generic unsupervised image retrieval
and few exploited on Re-ID tasks consists in fusion approaches [61]. In general,
the both broad categories of fusion have been successfully used in generic image
retrieval: (i)early fusion, which combines the feature vectors; and (ii)late fu-
sion, which usually combines ranked lists. SigniÔ¨Åcant results have been achieved
4based on fusion of different ranked lists and features [62, 63], with the purpose of
obtaining more effective results by exploiting the complementarity of each input.
On person Re-ID tasks, some authors have proposed late fusion strategies
based on rank aggregation [64, 65]. In [65], the aggregation is performed by
attributing weights for each query of each ranker, but no pre-selection of features
is performed, and all the features are used as input for the fusion step. There
are also early fusion approaches for Re-ID, as [66], that propose a supervised
multi-hypergraph fusion model for early fusion of feature extractors. It learns a
hypergraph for each feature through a star expansion strategy and they are fused
according to weights that the method has learned from training. In [67], a hyper-
graph structure is used with a deep learning model to improve the performance of
the acquired features for Re-ID.
Although fusing different features can represent a signiÔ¨Åcant advantage due to
extra information available, how to choose what features to fuse can be a challeng-
ing task. Even for supervised approaches, selecting high-effective combinations
of visual features remains a complex task, since it is necessary to consider various
aspects, such as diversity and complementarity of results. Therefore, selecting
features in an unsupervised way, without any labeled data is even more challeng-
ing, since no information about the effectiveness of individual visual features is
available.
This paper addresses the challenging task of unsupervised selection and fu-
sion of different features for more effective person re-identiÔ¨Åcation. We propose
a novel Hypergraph Rank Selection and Fusion (HRSF) framework, which com-
bines an unsupervised rank-based formulation for feature selection [62] with a
robust hypergraph model [68] for query performance prediction and rank aggre-
gation based on manifold learning. Among our main contributions, we can high-
light:
5‚Ä¢ The proposed HRSF framework uses a rank-based late fusion model, suit-
able for selection and fusion of a broad diversity of features. The selection
is performed by exploiting an unsupervised measure for query performance
prediction;
‚Ä¢ A hypergraph rank-based formulation is used to encode the high-order rela-
tionship among images. This strategy is exploited for both selection and fu-
sion tasks. Hypergraph models were little exploited in Re-ID literature [66,
67];
‚Ä¢ The proposed technique is able to learn representations through the hyper-
graph structure that encodes multiple features from different rankers. The
manifold learning based on a hypergraph model [68] allows effective fusion
and Ô¨Ånal ranking. In addition, our approach innovates by fusing different
feature extractors trained on different datasets;
‚Ä¢ Different from most of fusion approaches for Re-ID which often consider
ad hoc selections or combine all the features of the input set [64, 65, 67],
our approach is capable of dealing with various features in a completely un-
supervised scenario, selecting combinations in a very large search space. To
the best of our knowledge, this is the Ô¨Årst work which performs an explicit
selection and subsequently fusion of features on person Re-ID tasks in a
completely unsupervised way.
In fact, the proposed Hypergraph Rank Selection and Fusion (HRSF) approach
is based on both [62] and [68]. However, while there are some aspects in common,
also there are crucial differences. Among them, we can mention:
‚Ä¢The problem of selecting and fusing rankers in unsupervised scenar-
ios is very challenging. Hence, the use of an effective algorithm for
6selection is fundamental . While USRF [62] employs traditional query
performance prediction approaches (Authority and Reciprocal scores), we
propose a new measure named Hypergraph Query Performance Prediction
(HQPP);
‚Ä¢ Originally, USRF [62] uses the CPRR [69] as a method for fusion tasks.
Differently, the proposed HRSF uses the LHRR [68] method, which in com-
bination with the HQPP, makes both the selection and fusion based on
hypergraph structures. The LHRR [68] method is also more robust than
the CPRR [69], achieving superior retrieval results in most datasets;
‚Ä¢ Both [62] and [68] were originally proposed and evaluated only on general
purpose image retrieval scenarios. The proposed HRSF is employed and
validated for person Re-ID tasks;
‚Ä¢ In unsupervised scenarios, the optimal neighborhood size ( kparameters)
can be challenging to deÔ¨Åne. The experiments revealed that HRSF is more
robust than HQPP to different neighborhood sizes , leading to the most
effective results in the majority of scenarios.
A wide experimental evaluation was conducted on 4 different datasets with
sizes ranging from 14,097 to 39,902 images. Up to 28 different rankers where
considered in each case, resulting in millions of possible combinations. Experi-
ments indicated that our approach was capable of selecting and fusing the rankers,
achieving high-effective results superior to all rankers in isolation and competitive
to state-of-the-art, when more than 20 recent Re-ID approaches are considered.
The remainder of this paper is organized as follows. Section 2 discusses a for-
mal deÔ¨Ånition of problem setting. Section 3 presents the proposed rank selection
and fusion for Re-ID. Section 4 discusses the conducted experimental evaluation
and, Ô¨Ånally, Section 5 presents our conclusions.
72. Problem Formulation
This section presents the formal deÔ¨Ånition of the rank model used in this work.
LetC=fx1;x2; :::; xNgbe an image collection, where Ndenotes the collection
size. Let us consider a retrieval task where, given a query image, returns a list of
images from the collection C.
Formally, given a query image xq, a ranker denoted by Rjcomputes a ranked
listtq=(x1,x2,:::,xk)in response to the query. The ranked list tqcan be deÔ¨Åned
as a permutation of the k-neighborhood set N(q;k), which contains the kmost
similar images to image xqin the collection C. The permutation tqis a bijection
from the set N(q;k)onto the set [k] =f1;2;:::; kg. Thetq(i)notation denotes the
position of image xiin the ranked list tq.
The ranker Rjcan be deÔ¨Åned based on diverse approaches, including feature
extraction or learning methods. In this paper, a feature-based approach is consid-
ered, deÔ¨Åning Ras a tuple (e;r), where e:C!Rdis a function that extracts a
feature vector vxfrom an image x2C; and d:RdRd!Ris a distance function
that computes the distance between two images according to their corresponding
feature vectors. Formally, the distance between two images xi;xjis deÔ¨Åned by
r(e(xi),e(xj)). The notation r(i;j)is used for readability purposes.
A ranked list can be computed by sorting images in a crescent order of dis-
tance. In terms of ranking positions we can say that, if image xiis ranked before
image xjin the ranked list of image xq, that is, tq(i)<tq(j), then r(q;i)r(q;j).
Taking every image in the collection as a query image xq, a set of ranked lists T
=ft1;t2; :::;tNgcan be obtained.
Different features and distance functions give rises to different rankers which,
in turn, produce distinct ranked lists. Let R=fR1,R2;:::,Rmgbe a set of rankers
andRj2R, we denote by Tjthe set of ranked lists produced by Rj. A ranked list
computed by the ranker Rjin response to a query xqis denoted by tj;q.
8Our proposed method (HRSF) intends to select the most effective rankers from
Rbased on their respective set of ranked lists, without the need of any labeled
data. The selection function ( fs) is formally deÔ¨Åned by Equation (1).
X
n=fs(T1;T2;T3;:::;Tm); (1)
whereX
nis the set of selected rankers, whose cardinality is n, such thatjX
nj=n.
With the objective of facilitating the understanding of the reader, Table 1
presents the main symbols used in this paper.
3. Unsupervised Hypergraph Rank Selection and Fusion
This paper proposes a framework named Hypergraph Rank Selection and Fu-
sion (HRSF) for unsupervised person Re-ID tasks. Our model is inspired by a
recent approach [62] proposed for rank selection and fusion on general image re-
trieval tasks. The method is based on effectiveness estimations and correlation
among features computed by a rank-based analysis. In [62], reciprocal references
are exploited for effectiveness estimation and feature selection, while rank corre-
lation measures are used for analyzing complementary and diversity aspects. The
selected features are fused through a rank-based similarity learning method [69].
The proposed approach differs from previous work [62] on four main aspects:
(i) the unsupervised measure used to estimate the quality of individual features,
which is based on hypergraph structures; ( ii) a more robust method to fuse the
selected features; ( iii) the proposed approach is evaluated and validated for person
Re-ID and; ( iv) it is more robust to different neighborhood sizes, leading to the
most effective results in the majority of scenarios. We innovate by employing a
robust hypergraph model for both tasks: query performance prediction and rank
fusion. A recent manifold learning approach based on a rank-based hypergraph
formulation [68] is exploited.
9Table 1: Table of symbols.
Type Symbol Description
RetrievalC Image collection.
ModelN Image collection size.
xi Image of index i.
N(q;k)Neighborhood set for a query image xqof size k.
tq Ranked list for the query image xq.
tq(j) Position of the image xjin the ranked list of the image xq.
L Size of the ranked lists.
T Set of ranked lists for all the images in the dataset.
Selectionfs Function for ranker selection.
ModelRi Ranker of index i.
ti;q Ranked list of the image xqcomputed by the ranker xi.
Ti Set of ranked lists produced by the ranker Ri.
R Set of rankers.
m Size of the set R.
Xn Candidate combination composed by nrankers.
X
n Selected combination composed by nrankers.
XSelected combination among all sizes.
n Size of a combination.
wp Selection measure for pairs of rankers.
b Weight or relevance of the correlation.
k Neighborhood size.
g Effectiveness estimation measure (HQPP).
l Correlation measure (RBO).
a Constant used in RBO correlation measure.
HypergraphV Set of vertexes.
Modelvi Vertex of index i.
E Set of hyperedges.
ei Hyperedge of index i.
h(ei;vj)Reliance of vertex vjto belong to a hyperedge ei.
r(ei;vj)Density of ranking references to vjin ranking of xiand its neighbors.
HG Hypergraph model.
H Incidence matrix.
hr(i;x)Function that assigns a weight to image xaccording to its position in ti.
hf Fused afÔ¨Ånity measure used for rank aggregation.
10The hyperedges weights, used for estimating the conÔ¨Ådence of hyperedge as-
sociations, are exploited in our approach to predict the effectiveness of the dif-
ferent person Re-ID rankers in an unsupervised fashion. Additionally, we use a
manifold learning algorithm for fusion tasks, the Log-based Hypergraph of Rank-
ing References (LHRR) [68]. This keeps our approach completely unsupervised,
and more robust, mostly based on hypergraph structures.
Figure 1 presents an overview of the proposed approach, where the main steps
are illustrated and numerated. Given a set of different rankers provided by diverse
features extractors and distance measures, (1) a hypergraph estimation measure is
employed in order to predict the performance of each ranker without using data
labels. In (2), a correlation measure is applied for each pair of rankers. The com-
puted measures are used in the equation presented in the step (3), which computes
the equation for each combination. The rankers selected in stage (3) are fused in
stage (4), which uses LHRR [68] for rank aggregation.
3.1. Unsupervised Ranker Selection
Given a set of available rankers for person Re-ID and no labeled data, we aim
to select a combination which produces the most effective results. The selection
measure proposed on [62] relies on the idea that rankers can be analyzed in pairs
using an effectiveness estimator and a correlation measure. It consists in attribut-
ing a weight to each pair ( wp), in such way that, the ones composed by the most
effective rankers and the highest/lowest correlated ones should receive a higher
score. This is presented in Equation (2).
wp(fR1;R2g) =g(R1)g(R2)
(1+l(R1;R2))b; (2)
where gandlare used to measure the effectiveness and correlation of rankers,
respectively. The gcorresponds to our proposed Hypergraph Query Performance
111
2
3
4
Hypergraph Rank
Selection and Fusion (HRSF)Ranker R
1Ranker R
2Ranker R
m
HQPP Effec. Estimation
Correlation Measure
 Correlation Measure
Selection Measure
(R
1,R
2)
(R
7,R
8)
(R
3,R
m)Pairs SelectionCombination of 
Rankers
with size n
LHRR 
FusionHQPP Effec. Estimation HQPP Effec. Estimation
v
1v
2v
3
v
4e
1
e
2e
3 e
4
Figure 1: Overview of the HRSF proposed approach.
Prediction (HQPP), which is described in Section 3.2. While the effectiveness
can be individually estimated for each ranker, the correlation measure is applied
to pairs. The exponent bcan be employed to decide if the selection favors the
most correlated or diverse rankers. We adopt b= 1, since in [62] it was used for
scenarios with a higher number of features.
Therefore, in our approach, a ranker pair is selected based on the score ob-
tained by wp. All ranker pairs are ranked according to wp(in descending order).
Only the pairs with the highest scores are selected. The user can choose the num-
ber of top combinations to be selected.
After the selection of a pair of rankers, which is denoted by X
2, combinations
of other sizes are selected by performing intersection and union operations, in a
12procedure detailed described in [62].
For computing the correlation between rankers, the Rank-Biased Overlap (RBO) [70]
measure is used. This measure considers the overlap between top- klists at increas-
ing depths. The weight of the overlap is calculated based on probabilities deÔ¨Åned
at each depth. It can be formally deÔ¨Åned as follows:
l(ti;tj;k;a) = ( 1 a)k
√•
d=1ad 1jN(i;k)\N(j;k)j
d; (3)
where N(i;k)denotes the natural neighborhood of the top- kimages for xianda
is a constant ( a=0:9 was used for all the experiments).
In this work, rather than the effectiveness estimations and the fusion method
employed in [62], we used a query performance prediction measure (HQPP) and a
recent manifold ranking aggregation method (LHRR) that model the ranked lists
through hypergraph structures [68]. Both are discussed in the next sub-sections.
3.2. Hypergraph Query Performance Prediction
Query Performance Prediction (QPP) can be broadly deÔ¨Åned as the task of
estimating the effectiveness of a search/retrieval operation performed in response
to a query, where no labeled data is available [71]. Initially proposed for textual
retrieval systems [72, 73], the task assumed a diversiÔ¨Åed taxonomy in the literature
and has been establishing as a promising approach in image retrieval systems [74].
In this work, we propose to use a Hypergraph Query Performance Prediction
(HQPP) score for predicting the effectiveness of rankings produced by Person
Re-ID features. The HQPP score uses a hypergraph formulation recently pro-
posed [68] for manifold ranking on multimedia retrieval. Hypergraphs are a ro-
bust generalization of graphs, providing a powerful tool for capturing high-order
relationships in several domains [75‚Äì77]. In opposition to traditional graph-based
approaches, which represent only pairwise relationships, hypergraphs allow con-
13necting any number of nodes in order to represent similarity among sets of ob-
jects [78].
This work and the HQPP score use a hypergraph model mainly based on the
following main hypotheses and ideas:
‚Ä¢ Similar objects present similar ranked lists and, therefore, similar hyper-
edges. Once the hyperedges are represented by an incidence matrix, the
product of the hyperedges can be exploited to compute a more effective
similarity measure between nodes;
‚Ä¢ Similar objects are expected to reference each other in the same hyperedge.
Therefore, hyperedges that concentrate a high number of ranking references
on a few nodes are expected to be more effective. The Hypergraph Query
Performance Prediction (HQPP) is formally deÔ¨Åned based on this conjec-
ture.
Following the deÔ¨Ånition of [68], a hypergraph can be deÔ¨Åned as a tuple HG=
(V;E;hp), where Vrepresents a set of vertices and Edenotes the hyperedge set.
The set of hyperedges Ecan be deÔ¨Åned as a family of subsets of Vsuch that
S
e2E=V. To each hyperedge ei, a positive score hp(ei)denotes the conÔ¨Ådence
of relationships among a set of vertices established by the hyperedge ei.
While graphs are commonly represented by adjacency matrices, hypergraphs
are often represented by incidence matrices. The incidence of a hyperedge eion a
vertice vjis represented by an incidence matrix H, deÔ¨Åned as follows:
h(ei;vj) =8
<
:r(ei;vj);ifvj2ei;
0; otherwise,(4)
where h(ei;vj)denotes the reliance of the vertex vjto belong to a hyperedge ei
andr(ei;vj)is a function with a codomain in the R+that indicates the degree to
14which the vertex vjbelongs to a hyperedge ei. A hyperedge eiis deÔ¨Åned for each
image xi2Cbased on the k-neighborhood set of xiand its respective neighbors.
In this context, the function r(ei;vj)is deÔ¨Åned based on the density of ranking
references to vjin the ranking of xiand its neighbors. Formally, the function is
deÔ¨Åned as:
r(ei;vj) = √•
y2N(i;k)^j2N(y;k)hr(i;y)hr(y;j);(5)
where hr(i;x)is a function that assigns a weight of relevance to image xaccording
to its position in the ranked list ti. The weight assigned to xaccording to its
position in the ranked list tiis deÔ¨Åned as follows:
hr(i;x) =1 logkti(x): (6)
The size of the hyperedges varies according to the number of co-occurrence
of images. A high diversity of elements may indicate a high degree of uncertainty
and this information will be exploited for deÔ¨Åning the weights of hyperedges. The
weight of a hyperedge hp(ei)denotes the conÔ¨Ådence of relationships established
among vertices by the hyperedge.
In order to compute the weight hp(ei), we use the Hypergraph Neighborhood
SetNh, which contains the kvertices with the greatest h(ei;)scores in the hyper-
edge ei. As such, the hyperedge weight hp(ei)is deÔ¨Åned as:
hp(ei) =√•
j2Nh(i;k)h(i;j): (7)
A high-effective hyperedge is expected to present an elevated value of hp, in-
dicating a consistent co-occurrence of the same elements with high conÔ¨Ådence
of membership. Therefore, the hyperedge weight hp(ei)is deÔ¨Åned as the Hyper-
15graph Query Performance Prediction (HQPP) for ranked list of image xi, which is
denoted by g:
g(ti) =hp(ei): (8)
For a given ranker Ri, thegcan be computed for all the ranked lists to obtain
the value of g(Ri)in Equation (2). We highlight that, HQPP was used to deÔ¨Åne gin
this work, but our approach is Ô¨Çexible and capable of supporting other measures.
3.3. Hypergraph Manifold Rank Aggregation
Once the person Re-ID features are selected, we fuse the respective produced
rankings through a recently proposed manifold learning algorithm [68]. The Log-
based Hypergraph of Ranking References (LHRR) [68], brieÔ¨Çy described in this
section, captures the dataset manifold structure through a hypergraph-based simi-
larity measure, which can be used to rank aggregation tasks.
LHRR [68] exploits the hypergraph formulation discussed in the last section
to represent high-order similarity relationships encoded in the dataset manifold.
Subsequently, pairwise similarity scores are computed, allowing more effective
ranking results. The pairwise similarity is computed based on the conjecture that
similar elements present similar hyperedge representations. The similarity be-
tween hyperedges is computed based on the product of the incidence matrix H
and its transpose to encode reciprocal relationship. A pairwise similarity matrix S
is computed as:
S= (HHT)(HTH) (9)
In addition to the product of hyperedges, a Cartesian product operation is con-
ducted to extract useful pairwise relationships direct from the set of elements de-
Ô¨Åned by the hyperedges. Given two hyperedges eq;ei2E, the Cartesian product
between them can be deÔ¨Åned as:
16eqei=f(vx;vy):vx2eq^vy2eig: (10)
Leteq2denote the Cartesian product between the elements of the same hyper-
edge eq, for each pair of vertices (vi;vj)2eq2a pairwise similarity relationship
cpis computed to deÔ¨Åne the membership degrees of viandvj. The function is
formally deÔ¨Åned as:
cp(eq;vi;vj) =hp(eq)h(eq;vi)h(eq;vj): (11)
A similarity measure based on Cartesian product is deÔ¨Åned through a matrix
C, with each position computed as follows:
c(i;j) = √•
eq2E^(vi;vj)2eq2cp(vi;vj): (12)
The pairwise similarity deÔ¨Åned based on hyperedges and Cartesian product
operations provides complementary information. Hence, an afÔ¨Ånity matrix Wis
computed by combining both matrices as:
W=CS: (13)
Based on the afÔ¨Ånity measure deÔ¨Åned by W, a ranking procedure can be per-
formed for each feature giving rise to a new set of ranked lists. Next, a multiplica-
tive rank-based formulation is used to combine the features, exploiting an adaptive
weight, which is assigned to each query/feature according to the weight of the re-
spective hyperedge. Let hfdenote the fused afÔ¨Ånity measure; each element is
computed as follows considering the top- Lpositions of tq:
hf(q;i) =m
√ï
f=1(1+hp(f;eq))
(1+logLtq;f(i)); (14)
17where hp(f;eq)is the weight of hyperedge eqaccording to the feature fandtq;f(i)
denote the position of xiin the ranked list of xqaccording to the feature f. The
combined afÔ¨Ånity measure hf(;)gives rise to a unique set of ranked lists which
is re-processed by the LHRR [68] algorithm as a single feature.
4. Experimental Evaluation
This section presents the experimental results conducted for evaluating our
proposed approach. The experimental evaluation was conducted on 4 datasets of-
ten used for person Re-ID with sizes ranging from 14,097 to 39,902. For each
dataset, up to 28 rankers were considered of different modalities (e.g. traditional
descriptors, bag of visual words, deep learning). The large number of rankers is
used with the objective of evaluating the capacity of our selection approach. It is
desirable that, if the selection is accurate, only the most effective are selected to
be fused. Also, there is a very large number of possible combinations, when all
the possible sizes are considered. With 28 rankers, there are 268,435,456 possible
combinations. Since it is impractical to execute all of them, selection is of funda-
mental importance in this context. The experimental evaluation also considers a
comparison with fusion baselines and with state-of-the-art Re-ID approaches.
4.1. Experimental Protocol
Our experimental protocol considers 4 datasets, which are detailed in Table 2.
For CUHK03 [15], we considered the detected version of the dataset, which uses
the bounding boxes extracted by a DPM detector, and adopted the experimental
protocol proposed by [52]. Both Market1501 [13] and DukeMTMC [79] use the
experimental protocol proposed by the original authors that published the datasets.
In the case of the Airport [6], a different protocol was adopted, where all the
images (training, probe, and gallery) are considered as queries (probes) and part of
the gallery at the same time. For all of them, the MAP (Mean Average Precision)
18was reported. The R1 corresponds to the Ô¨Årst value of the CMC (Cumulative
Matching Characteristics) curve, which indicates the number of ranked lists that
have an image that corresponds to the same individual in the Ô¨Årst position after
the query image. All the evaluations are single-shot (single-query), this means
that each query corresponds to only one image.
Table 2: Datasets considered in the experimental evaluation.
Dataset People BBox Cam Label Evaluation
CUHK03 [15, 52] 1,467 14,097 2 DPM R1/ MAP [52]
Market1501 [13] 1,501 32,217 6 DPM R1/MAP [13]
DukeMTMC [79] 1,812 36,411 8 Manual R1/MAP [79]
Airport [6] 9,651 39,902 6 ACF MAP (all queries)
Table 3 presents all the descriptors used in the experimental evaluation (i.e., as
the input of our method). The number of rankers per dataset ranges from 21 to 28,
and emcompasses different modalities (e.g. statistical, bag of visual words, deep
learning). For most of the extractions, the Euclidean distance was considered,
no metric learning was applied aiming at keeping the protocol fully unsupervised
(not using the labels from the dataset being evaluated). Except for OSNET-AIN,
where cosine distance was used, as done in [22]. For the non-deep methods (GBI-
COV , LOMO, GOG, WHOS, ELF, HLBP, SDC, and BOVW), we applied PCA to
reduce the feature vectors to 100 dimensions before the distances were extracted,
as also done by other authors [6]. In total, we considered 7 different Convolutional
Neural Networks (CNN): MobileNetV2, RESNET50, HACNN, MLFN, OSNET,
OSNET-IBN, and OSNET-AIN. For each of these models, three different trainings
were performed: on Market dataset, which is indicated by (M); on DukeMTMC
dataset, which is indicated by (D); and on MSMT17 dataset, which is indicated by
(MT). Different trainings and models were used with the objective of evaluating
our selection and fusion. Most of them were pre-trained on the ImageNet [80]
19dataset, except HACNN which was trained on Re-ID datasets from scratch. The
MSMT17 [81] was used to train some of the networks, since it is a large dataset
(126,441 images of 4,101 people in 15 cameras) that can facilitate the capacity of
generalization. The majority of the networks were trained considering only the
training subset, but RESNET50 and OSNET variants used all of the images (train,
probe, and gallery) when trained on MSMT17. The CNN features were extracted
with the Torchreid [82] trained weights1.
To keep the comparisons fair, since our evaluation is unsupervised, the
models that were trained on the target dataset were not used. For example,
train on Market and test on Market is removed. These cases are reported with
‚Äú‚Äî‚Äù.
4.2. Experimental Analysis
The neighborhood size, denoted by k, is used in multiple steps of our method:
for calculating the effectiveness measure, for the correlation measure, and in the
fusion stage. Figure 2 presents an experiment that was conducted to evaluate the
impact of kon Market1501 dataset, where both R1 and MAP are shown for dif-
ferent values of k. Notice that the method is robust to different parameter settings.
We used k=20 for CUHK03, Market1501, DukeMTMC and k=10 for Airport
in all of the remaining experiments. For the Airport dataset, a smaller kseems to
be more adequate, since it has less images per individual (around 4) compared to
the other collections.
In this work, HQPP is proposed as a measure to estimate the quality of each
ranker in the selection stage. Aiming at assessing the use of this measure in Re-ID
scenarios, Figure 3 shows an experiment where each dot corresponds to a different
ranker and the MAP (measure that uses labeled data) is compared to the HQPP
1https://kaiyangzhou.github.io/deep-person-reid/MODEL_ZOO.html
20Table 3: Values of MAP (%) and R-01 (%) for the descriptors and datasets considered on the
experiments. The dataset used to train the CNN is referenced between parentheses (M = Market,
D = DukeMTMC, MT = MSMT17).
Datasets
CUHK03 Market1501 DukeMTMC Airport
Features R1 MAP R1 MAP R1 MAP MAP
GBICOV [9] 0.63 0.82 10.21 3.27 ‚Äî ‚Äî ‚Äî
LOMO [11] 0.79 0.89 19.15 6.46 6.60 2.82 35.35
GOG [10] 0.49 0.77 21.56 7.55 10.82 4.40 34.11
WHOS [12] 0.39 0.56 20.01 6.23 7.50 2.65 34.75
ELF [8] 0.34 0.52 12.02 3.85 2.42 0.83 31.17
HLBP [14] 0.32 0.43 7.07 2.18 0.76 0.54 32.68
SDC [16] 0.18 0.34 11.02 3.78 2.96 1.18 31.57
BOVW-350 [13] 1.69 1.80 33.11 13.34 14.41 6.71 32.73
BOVW-500 [13] 1.56 1.81 32.33 12.94 14.14 6.68 33.09
MobileNetV2 (M) [31] 4.39 4.34 ‚Äî ‚Äî 24.01 12.34 35.62
MobileNetV2 (D) [31] 4.30 4.30 37.80 15.63 ‚Äî ‚Äî 37.23
MobileNetV2 (MT) [31] 8.87 8.51 37.86 16.56 42.59 23.79 38.84
RESNET50 (M) [30] 3.84 3.90 ‚Äî ‚Äî 25.67 13.62 38.01
RESNET50 (D) [30] 5.84 5.85 42.64 18.39 ‚Äî ‚Äî 40.25
RESNET50 (MT) [30] 13.68 13.08 46.59 22.82 52.29 32.00 41.95
HACNN (M) [24] 5.51 5.69 ‚Äî ‚Äî 23.79 13.13 36.40
HACNN (D) [24] 3.11 3.28 43.74 18.87 ‚Äî ‚Äî 38.85
HACNN (MT) [24] 9.71 9.68 49.23 23.30 42.19 25.57 42.94
MLFN (M) [23] 4.91 5.19 ‚Äî ‚Äî 30.39 16.96 38.67
MLFN (D) [23] 4.72 4.74 45.55 20.26 ‚Äî ‚Äî 40.15
MLFN (MT) [23] 10.58 10.19 46.59 21.98 48.70 28.98 41.17
OSNET (MT) [21] 20.83 19.84 65.94 37.36 65.98 45.20 45.47
OSNET-IBN (M) [22] 10.48 10.22 ‚Äî ‚Äî 48.52 26.59 40.96
OSNET-IBN (D) [22] 8.01 7.85 57.48 26.01 ‚Äî ‚Äî 40.65
OSNET-IBN (MT) [22] 21.70 20.78 66.45 37.13 67.41 45.52 45.37
OSNET-AIN (M) [22] 12.14 11.67 ‚Äî ‚Äî 52.42 30.35 42.05
OSNET-AIN (D) [22] 9.54 9.24 61.10 30.64 ‚Äî ‚Äî 42.79
OSNET-AIN (MT) [22] 28.49 27.00 69.95 43.30 71.14 52.69 52.26
21performance prediction score. As can be seen in the graph, there is a high corre-
lation between the measures. The Pearson correlation among the dots is 0.9678,
which indicates the high effectiveness of the selection strategy.
0 10 20 30 40 50020406080
k ValueEffectiveness (%)Parameter k evaluation on Market1501 dataset
MAP
R1
Figure 2: Evaluation of the impact of parameter kon MAP and R1 for Market1501 dataset.
23 24 25 26 27 28 29 300102030405060
HQPP ScoreMAP (%)Evaluation of the HQPP on DukeMTMC dataset
Pearson Correlation = 0.9678
Figure 3: Evaluation of the HQPP measure compared to the MAP on DukeMTMC dataset.
The HRSF ranks the best combinations for each size. The user can choose
the number of top combinations to be selected. We conducted an experiment on
22CUHK03, Market, and DukeMTMC (Figures 4, 5, and 6) where the average MAP
and R1 of the selected ranker pairs is presented as the number of selected pairs
changes. Notice that the highest MAP and R1 values are in the Ô¨Årst position (top-
1), which evinces that the combination with the highest wp(ranked in the Ô¨Årst
position) is also the one with highest effectiveness.
1 2 3 4 5 6 7 8 9 100.330.340.350.360.370.380.390.4Average EffectivenessAverage Effec. of Pairs on CUHK03 dataset
MAP
R1
Number of Pairs (ranked by w )p
Figure 4: Average MAP of top pairs on CUHK03 dataset.
1 2 3 4 5 6 7 8 9 100.550.60.650.70.75Average EffectivenessAverage Effec. of Pairs on Market dataset
MAP
R1
Number of Pairs (ranked by w )p
Figure 5: Average MAP of top pairs on Market dataset.
23The best combination available in the top-5 for each size is reported in Ta-
ble 4. We report sizes from 1 to 6 ( X
2, ...,X
6) and the best combination among
them (which can be denoted just as X) is highlighted in bold. The best isolated
ranker in each case is also listed for comparison purposes and to facilitate the
visualization of the relative MAP gain. Notice that OSNET, OSNET-AIN, and
OSNET-IBN are the most commonly selected rankers, which evinces the effec-
tiveness of our selection, once these rankers are among the most effective ones.
Additionally, in all cases the proposed selection and fusion achieves better re-
sults the the best ranker in isolation . The complementarity among the methods
can be exploited by our approach achieving gains up to +47% (MAP) after the
selection and fusion is performed. The results also indicate that gains can be ob-
tained when networks trained on different datasets are combined, even when the
same architecture is used.
1 2 3 4 5 6 7 8 9 100.60.650.70.750.8
Number of Pairs (ranked by w )Average EffectivenessAverage Effec. of Pairs on Duke dataset
MAP
R1
p
Figure 6: Average MAP of top pairs on Duke dataset.
24Table 4: The best selected combination of each size (among top-5) is reported on each dataset.
DatasetComb.
Selected and Fused RankersR1 MAP MAP
Size (%) (%) R. Gain
CUHK03Best R OSNET-AIN (MT) 28.49 27.00 ‚Äî
X
2 OSNET-AIN (MT) + OSNET-IBN (MT) 39.04 39.69 +47.00%
X
3 OSNET-AIN (MT) + OSNET-IBN (MT) + OSNET (MT) 39.13 39.58 +46.59%
X
4OSNET-AIN (MT) + OSNET-IBN (MT) + OSNET (MT)
+ OSNET-AIN (M)38.02 38.80 +43.70%
X
5OSNET-AIN (MT) + OSNET-IBN (MT) + OSNET (MT)
+ OSNET-AIN (M) + OSNET-IBN (M)36.15 37.11 +37.44%
X
6HACNN (MT) + OSNET-AIN (D) + OSNET-AIN (M)
+ OSNET-AIN (MT) + OSNET-IBN (MT) + OSNET (MT)35.46 36.17 +33.96%
Market1501Best R OSNET-AIN (MT) 69.95 43.30 ‚Äî
X
2 OSNET-AIN (MT) + OSNET (MT) 74.32 60.89 +40.62%
X
3 OSNET-AIN (MT) + OSNET (MT) + OSNET-AIN (D) 75.56 62.64 +44.67%
X
4OSNET-AIN (MT) + OSNET (MT) + OSNET-AIN (D)
+ OSNET-IBN (MT)75.71 62.94 +45.36%
X
5OSNET-AIN (MT) + OSNET (MT) + OSNET-AIN (D)
+ OSNET-IBN (MT) + HACNN (D)74.00 60.69 +40.16%
X
6HACNN (MT) + OSNET-AIN (D) + OSNET-AIN (MT)
+ OSNET-IBN (D) + OSNET-IBN (MT) + OSNET (MT)73.57 59.85 +38.22%
DukeMTMCBest R OSNET-AIN (MT) 71.14 52.69 ‚Äî
X
2 OSNET-AIN (MT) + OSNET-IBN (MT) 76.80 68.51 +30.02%
X
3 OSNET-AIN (MT) + OSNET-IBN (MT) + OSNET (MT) 77.24 68.88 +30.73%
X
4OSNET-AIN (MT) + OSNET-IBN (MT) + OSNET (MT)
+ RESNET (MT)76.89 68.56 +30.12%
X
5OSNET-AIN (MT) + OSNET-IBN (MT) + OSNET (MT)
+ RESNET (MT) + OSNET-AIN (M)76.39 67.72 +28.53%
X
6OSNET-AIN (MT) + OSNET-IBN (MT) + OSNET (MT)
+ RESNET (MT) + OSNET-AIN (M) + MLFN (MT)75.90 66.96 +27.08%
AirportBest R OSNET-AIN (MT) ‚Äî 52.26 ‚Äî
X
2 OSNET-AIN (MT) + OSNET (MT) ‚Äî 52.43 +0.33%
X
3 OSNET-AIN (MT) + OSNET (MT) + OSNET-IBN (MT) ‚Äî 53.38 +2.14%
X
4OSNET-AIN (MT) + OSNET (MT) + OSNET-IBN (MT)
+ OSNET-AIN (M)‚Äî 53.91 +3.16%
X
5OSNET-AIN (MT) + OSNET (MT) + OSNET-IBN (MT)
+ OSNET-AIN (M) + HACNN (MT)‚Äî 54.09 +3.50%
X
6OSNET-AIN (MT) + OSNET (MT) + OSNET-IBN (MT)
+ OSNET-AIN (M) + HACNN (MT) + MLFN (MT)‚Äî 54.02 +3.37%
254.3. Comparison with Fusion Baselines
In order to evaluate the proposed method compared to other approaches that
both select and fuse the input features, Table 5 presents the proposed approach,
HRSF, compared to both early and late fusion baselines on the four datasets. In
all the cases, the same set of features, which were presented in the experimental
protocol, were used. For the early fusion methods, the default parameters were
used and all the features were processed with PCA to reduce the feature vectors
to 100 components. From all the features, the top-1000 were selected to compose
the new feature vector and the Euclidean distance was computed. As can be seen,
the results of our approach are superior in most cases (CUHK03, Market1501,
Airport) and comparable in others (DukeMTMC).
Table 5: Proposed approach compared to early and late fusion baselines.
Dataset Category Method R1 (%) MAP (%)
CUHK03Early FusionLaplace [83] 9.56 10.19
SPEC [84] 9.29 9.97
Late FusionUSRF [62] 38.24 39.03
HRSF (ours) 39.04 39.69
Market1501Early FusionLaplace [83] 82.07 61.26
SPEC [84] 77.14 54.90
Late FusionUSRF [62] 75.97 62.69
HRSF (ours) 75.71 62.94
DukeMTMCEarly FusionLaplace [83] 59.29 43.56
SPEC [84] 59.29 43.56
Late FusionUSRF [62] 77.82 68.98
HRSF (ours) 77.24 68.88
AirportEarly FusionLaplace [83] ‚Äî 45.33
SPEC [84] ‚Äî 45.32
Late FusionUSRF [62] ‚Äî 39.75
HRSF (ours) ‚Äî 54.09
26An experiment was conducted with the objective of clarifying the robustness
of HRSF to different values of kwhen compared to USRF. Figures 7 and 8 present
the MAP of the best combination ( X) among top-5 for different values of kon
Market and Duke datasets, respectively. The MAP for k=20 is the same of Ta-
ble 5. Notice that, while both methods seem comparable for k=20,our method
provided a signiÔ¨Åcant higher MAP for other values of k.This is fundamental
for unsupervised scenarios, where the optimal kcan be challenging to deÔ¨Åne.
15 20 25 300.540.560.580.60.620.640.66
Parameter kMAPSelected Combination on Market dataset
USRF
Our Method
Figure 7: Selected Combination (among top-5) on Market considering MAP.
15 20 25 300.640.660.680.70.72
Parameter kMAPSelected Combination on Duke dataset
USRF
Our Method
Figure 8: Selected Combination (among top-5) on Duke considering MAP.
274.4. State-of-the-Art
This section presents comparisons with recent baselines. The taxonomy often
varies in the literature; some of the subcategories of unsupervised Re-ID methods
are: ( i)Cross-domain Methods: they are trained on one (single-source) or more
(multi-source) labeled source domains. The evaluation is conducted on the target
dataset. It can be understood as a type of transfer learning; ( ii)Domain Adaptive
Methods: they use cross-domain training and perform a second step (adaptation)
using unlabeled data from the target domain.
However, since they are all unsupervised and often there is overlap among the
categories, we insert all of them into a single group named Unsupervised Domain
Adaptation Methods [85]. Table 6 presents our results compared to around 20
state-of-the-art Unsupervised Domain Adaptation Methods [85] .
In order to perform a fair comparison, it contains only methods that did not
use the labels from the target dataset for training (train on CUHK03 and test
on CUHK03 is not used, for example). Therefore, supervised and semi-supervised
methods are not included. The abbreviations in parentheses indicate the datasets
used for training2. For example, the use of (D, M) indicates that the reported result
corresponds to a training done either on Duke or on Market dataset. The results
reported on Market were trained on Duke and the results reported on Duke were
trained on Market. None of the presented methods were trained using labels
from the target dataset. The abbreviations were omitted for baselines that used
more than 5 datasets as source for training (CAMEL [33] and baseline by [26]),
but they can be consulted in [26], which used similar baselines and protocol.
We provided the best results for each method (considering the original papers)
with the objective of keeping the evaluation as far as possible. Since the code and
2C02 = CUHK02, C03 = CUHK03, M = Market1501, D = DukeMTMC, MT = MSMT17
28implementation are not available for the majority of methods, it is not possible to
provide results considering the same sources in all cases.
Both MAP and R1 are reported in all the cases and our best results are pre-
sented in bold. The baselines do not perform any form of selection. We highlight
that our method receives all the rankers as input and performs a wide selection
among rankers with high and low effectiveness, which is a very challenging sce-
nario. In contrast, none of the baselines are required to perform any selection
and the features are chosen manually. The selection stage is an important aspect
and contribution of HRSF that it is hard to replicate in the baselines. Notice
that our approach achieved competitive or superior results for all the evaluated
datasets.
With the objective to facilitate the visualization of the best results in the state-
of-the-art, Table 7 presents the rank of the methods according to MAP and R1.
The gray cells with bold values correspond to methods that have achieved a higher
rank than HRSF in each case. Our method achieved the best MAP on DukeMTMC
and had the second position in the other two datasets. For R1, HRSF is positioned
among the top-4 in all cases. The mean of the rank on each dataset is presented in
the most right columns. Notice that our method achieved one of the highest rank
means among all of the methods, being only slightly behind ISSDA [86]. The
comparisons show that, besides our results being among the best for all datasets,
in some cases, other non-fusion based methods provided higher values than our
approach. There are some possible explanations for this:
‚Ä¢ Each dataset has different aspects (e.g., image resolution, picture angles,
environment, number of images per person, dataset size). For this reason,
different methods may perform better or worse on distinct datasets ;
‚Ä¢ The Baseline by [26] performs a multi-source training. The results reported
by [26] are based on the transfer learning of a training performed on 7 Re-
29ID datasets . It considers, by far, the largest labeled source of all the base-
lines , which leads to high results, especially on Market where it is ranked
as the 2nd/3rd best R1/MAP (shown in Table 7);
‚Ä¢ An idea that is exploited by some baselines is the generation of pseudo-
labels. The most promising example is ISSDA [86], which has the best
results on Market and is well ranked on DukeMTMC. Different from the
others, ISSDA employs a self-supervised iterative pseudo-label generation
and training. However, besides the effectiveness, the authors [86] claim
that the training stage is very time-consuming since it requires, among
other aspects, the execution of a clustering algorithm. Furthermore, ISSDA
has an average ranking of 1.5 against 1.67 of our method (Table 7, MAP
measure); However, ISSDA does not report results on CUHK03. Our
average ranking without considering CUHK03 is also 1.5 ;
‚Ä¢ The MAR [47] performs soft label generation . The applied strategy is ca-
pable of achieving promising results for improving the DukeMTMC dataset
with R1 measure. However, apparently, the quality of the soft labels varies
according to the dataset.
Since the proposed method is Ô¨Çexible, among future works, we intend to ex-
pand our approach by exploiting the use of pseudo-labels. This can be done by
modifying HRSF or by providing methods like ISSDA [86] and MAR [47] as
input, for example. In this work, only the rankers in Table 3 were used.
30Table 6: State-of-the-art comparison considering MAP (%) and R-01 (%).
Datasets
Market1501 DukeMTMC CUHK03
R1 MAP R1 MAP R1 MAP
Unsupervised Domain Adaptation Methods
ARN [45] 70.3 39.4 60.2 33.4 ‚Äî ‚Äî
EANet [27] 66.4 40.6 45.0 26.4 51.4 31.7
ECN [46] 75.1 43.0 63.3 40.4 ‚Äî ‚Äî
MAR [47] 67.7 40.0 87.1 48.0 ‚Äî ‚Äî
TAUDL [48] 63.7 41.2 61.7 43.5 44.7 31.2
UTAL [49] 69.2 46.2 62.3 44.6 56.3 42.3
HHL (D,M) [87] 62.2 31.4 46.9 27.2 ‚Äî ‚Äî
HHL (C03) [87] 56.8 29.8 42.7 23.4 ‚Äî ‚Äî
ATNet (D,M) [28] 55.7 25.6 45.1 24.9 ‚Äî ‚Äî
CSGLP (D,M) [88] 63.7 33.9 56.1 36.0 ‚Äî ‚Äî
ISSDA (D,M) [86] 81.3 63.1 72.8 54.1 ‚Äî ‚Äî
EANet (C03) [27] 59.4 33.3 39.3 22.0 ‚Äî ‚Äî
EANet (D,M) [27] 61.7 32.9 51.4 31.7 ‚Äî ‚Äî
SPGAN (D,M) [89] 43.1 17.0 33.1 16.7 ‚Äî ‚Äî
DAAM (D,M) [90] 42.3 17.5 29.3 14.5 ‚Äî ‚Äî
AF3 (D,M) [91] 67.2 36.3 56.8 37.4 ‚Äî ‚Äî
AF3 (MT) [91] 68.0 37.7 66.3 46.2 ‚Äî ‚Äî
PAUL (MT) [92] 68.5 40.1 72.0 53.2 ‚Äî ‚Äî
EMTL (C02+D+M) [32] 52.8 25.1 39.7 22.3 ‚Äî ‚Äî
CAMEL [33] 54.5 26.3 ‚Äî ‚Äî 31.9 ‚Äî
Baseline by [26] 80.5 56.8 67.4 46.9 29.4 27.4
Unsupervised Selection and Fusion (ours)
HRSF ( X
2) 74.32 60.89 76.80 68.51 39.04 39.69
HRSF ( X
3) 75.56 62.64 77.24 68.88 39.13 39.58
HRSF ( X
4) 75.71 62.94 76.89 68.56 38.02 38.80
HRSF ( X
5) 74.00 60.69 76.39 67.72 36.15 37.11
HRSF ( X
6) 73.57 59.85 75.90 66.96 35.46 36.17
HRSF ( X, best result) 75.71 62.94 77.24 68.88 39.04 39.69
31Table 7: State-of-the-art methods ranked by their results.
Market1501 DukeMTMC CUHK03 Mean
R1 MAP R1 MAP R1 MAP R1 MAP
Unsupervised Domain Adaptation Methods
ARN [45] 5 10 10 12 ‚Äî ‚Äî 7.5 11
EANet [27] 11 7 16 15 2 3 9.67 8.34
ECN [46] 4 5 7 9 ‚Äî ‚Äî 5.5 7
MAR [47] 9 9 1 4 ‚Äî ‚Äî 5 6.5
TAUDL [48] 12 6 9 8 3 4 8 6
UTAL [49] 6 4 8 7 1 1 5 4
HHL (D,M) [87] 14 16 14 14 ‚Äî ‚Äî 14 15
HHL (C03) [87] 17 17 17 17 ‚Äî ‚Äî 17 17
ATNet (D,M) [28] 18 19 15 16 ‚Äî ‚Äî 16.5 17.5
CSGLP (D,M) [88] 13 13 12 11 ‚Äî ‚Äî 12.5 12
ISSDA (D,M) [86] 1 1 3 2 ‚Äî ‚Äî 2 1.5
EANet (C03) [27] 16 14 19 19 ‚Äî ‚Äî 11.67 16.5
EANet (D,M) [27] 15 15 13 13 ‚Äî ‚Äî 14 14
SPGAN (D,M) [89] 21 22 20 20 ‚Äî ‚Äî 20.5 21
DAAM (D,M) [90] 22 21 21 21 ‚Äî ‚Äî 21.5 21
AF3 (D,M) [91] 10 12 11 10 ‚Äî ‚Äî 10.5 11
AF3 (MT) [91] 8 11 6 6 ‚Äî ‚Äî 7 8.5
PAUL (MT) [92] 7 8 4 3 ‚Äî ‚Äî 5.5 5.5
EMTL (C02+D+M) [32] 20 20 18 18 ‚Äî ‚Äî 19 19
CAMEL [33] 19 18 ‚Äî ‚Äî 5 ‚Äî 12 18
Baseline by [26] 2 3 5 5 6 5 4.34 4.34
Unsupervised Selection and Fusion (ours)
HRSF ( X, best result) 3 2 2 1 4 2 3 1.67
4.5. Visual Results
Some qualitative results were also elaborated to evince the quality of our ob-
tained results. Two different queries ( x1,x2) for the same person (ID) were chosen
32from the DukeMTMC dataset. Figure 9 presents a graph for each ranker that com-
poses the best combination ( X) on DukeMTMC and the HRSF result. Each dot
represents a gallery image, which is positioned in the graph according to its dis-
tance to the query images ( x1,x2). The idea is that, since the query images are of
the same person, the distance between them should be small and the images of the
same ID should be closer to the bottom left corner. Images obtained from different
camera views are presented in different symbols. It shows that the HRSF method
was capable of reducing the distance of all the images belonging to the same class
when compared to isolated rankers (OSNET, OSNET-IBN, OSNET-AIN).
0.2 0.4 0.6 0.8 1 1.2 1.40.40.60.811.21.4
1 (cam2)Distance of x2 (cam5)OSNET-AIN (MT) result on DukeMTMC dataset
Not the same ID
Same ID in cam1
Same ID in cam2
Same ID in cam5
Same ID in cam6
10 20 30 40 50 60102030405060
1 (cam2)2 (cam5)OSNET-IBN (MT) result on DukeMTMC dataset
Not the same ID
Same ID in cam1
Same ID in cam2
Same ID in cam5
Same ID in cam6
10 20 30 40 50 602030405060
1 (cam2)2 (cam5)OSNET (MT) result on DukeMTMC dataset
Not the same ID
Same ID in cam1
Same ID in cam2
Same ID in cam5
Same ID in cam6
10-810-710-610-510-410-310-210-110010-710-610-510-410-310-210-1100
Distance of x1 (cam2)2 (cam5)HRSF result on DukeMTMC dataset
Not the same ID
Same ID in cam1
Same ID in cam2
Same ID in cam5
Same ID in cam6
img1
img2
Distance of x Distance of xDistance of xDistance of x Distance of x
Distance of x
Figure 9: Distance distribution for two query images on DukeMTMC dataset.
Figures 10 and 11 present examples of visual queries on the DukeMTMC and
CUHK03 datasets, respectively. The results are shown for the best combination
obtained by HRSF ( X) and the rankers that compose it. The query image is
presented with green borders, and the wrong results with red borders. Notice that,
33in these cases, beyond selecting the best results, our approach was also capable of
removing most of the incorrectly retrieved images.
OSNET-AIN (MT)
OSNET-IBN (MT)
OSNET (MT)
HRSF Fusion ( X)
Figure 10: Examples to illustrate the impact of HRSF selection and fusion on DukeMTMC dataset.
34OSNET-AIN (MT)
OSNET-IBN (MT)
HRSF Fusion ( X)
Figure 11: Examples to illustrate the impact of HRSF selection and fusion on CUHK03 dataset.
5. Conclusion
In this work, we have presented an approach to select and aggregate results
from different Re-ID methods without using the target dataset training labels. The
relationship among the dataset elements is modeled using hypergraph structures
both for query performance prediction and fusion. In most of the cases, the best
rankers were properly selected and fused. Our approach achieved signiÔ¨Åcant gains
in the majority of the scenarios and results competitive or superior to the state-of-
the-art, including the most recent methods.
35As future work, we intend to investigate the use of the proposed approach
in multi-query scenarios and further expand the experimental protocol, including
other datasets and feature extractors. Another idea is to expand our approach
by exploiting the use of pseudo-labels. Our method, as currently presented, is
ready to be executed in scenarios that do not require real time. In addition, we
intend to address these real time scenarios in future work, where optimizations
and parallelization techniques can be employed.
ACKNOWLEDGEMENTS
The authors are grateful to the S Àúao Paulo Research Foundation - FAPESP
(grant #2018/15597-6 and #2017/25908-6), Brazilian National Council for Scien-
tiÔ¨Åc and Technological Development - CNPq (grant #308194/2017-9), Microsoft
Research, and Petrobras (grant #2017/00285-6).
References
[1] O. Camps, M. Gou, T. Hebble, S. Karanam, O. Lehmann, Y . Li, R. J. Radke, Z. Wu,
F. Xiong, From the lab to the real world: Re-identiÔ¨Åcation in an airport camera
network, IEEE Transactions on Circuits and Systems for Video Technology 27 (3)
(2017) 540‚Äì553.
[2] M. Cristani, V . Murino, Chapter 10 - person re-identiÔ¨Åcation, in: R. Chellappa,
S. Theodoridis (Eds.), Academic Press Library in Signal Processing, V olume
6, Academic Press, 2018, pp. 365 ‚Äì 394. doi:https://doi.org/10.1016/
B978-0-12-811889-4.00010-5 .
[3] Y . Li, Z. Wu, S. Karanam, R. Radke, Real-world re-identiÔ¨Åcation in an airport cam-
era network, 2014, pp. 1‚Äì6. doi:10.1145/2659021.2659039 .
36[4] L. Zheng, Y . Yang, A. G. Hauptmann, Person re-identiÔ¨Åcation: Past, present and
future, CoRR abs/1610.02984. arXiv:1610.02984 .
URL http://arxiv.org/abs/1610.02984
[5] M. Ye, J. Shen, G. Lin, T. Xiang, L. Shao, S. C. H. Hoi, Deep learning for person
re-identiÔ¨Åcation: A survey and outlook (2020). arXiv:2001.04193 .
[6] S. karanam, M. Gou, Z. Wu, A. Rates-Borras, O. Camps, R. J. Radke, A system-
atic evaluation and benchmark for person re-identiÔ¨Åcation: Features, metrics, and
datasets, IEEE Transactions on Pattern Analysis and Machine Intelligence 41 (3)
(2019) 523‚Äì536.
[7] Y . Yang, J. Yang, J. Yan, S. Liao, D. Yi, S. Z. Li, Salient color names for person
re-identiÔ¨Åcation, in: D. Fleet, T. Pajdla, B. Schiele, T. Tuytelaars (Eds.), Computer
Vision ‚Äì ECCV 2014, Springer International Publishing, Cham, 2014, pp. 536‚Äì551.
[8] D. Gray, H. Tao, Viewpoint invariant pedestrian recognition with an ensemble of
localized features, in: D. Forsyth, P. Torr, A. Zisserman (Eds.), Computer Vision ‚Äì
ECCV 2008, Springer Berlin Heidelberg, Berlin, Heidelberg, 2008, pp. 262‚Äì275.
[9] B. Ma, Y . Su, F. Jurie, Covariance descriptor based on bio-inspired features for
person re-identiÔ¨Åcation and face veriÔ¨Åcation, Image and Vision Computing 32 (6)
(2014) 379 ‚Äì 390.
[10] T. Matsukawa, T. Okabe, E. Suzuki, Y . Sato, Hierarchical gaussian descriptor for
person re-identiÔ¨Åcation, in: 2016 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2016, pp. 1363‚Äì1372.
[11] S. Liao, Y . Hu, Xiangyu Zhu, S. Z. Li, Person re-identiÔ¨Åcation by local maximal
occurrence representation and metric learning, in: 2015 IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), 2015, pp. 2197‚Äì2206.
37[12] G. Lisanti, I. Masi, A. D. Bagdanov, A. D. Bimbo, Person re-identiÔ¨Åcation by iter-
ative re-weighted sparse ranking, IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence 37 (8) (2015) 1629‚Äì1642.
[13] L. Zheng, L. Shen, L. Tian, S. Wang, J. Wang, Q. Tian, Scalable person re-
identiÔ¨Åcation: A benchmark, in: 2015 IEEE International Conference on Computer
Vision (ICCV), 2015, pp. 1116‚Äì1124.
[14] F. Xiong, M. Gou, O. Camps, M. Sznaier, Person re-identiÔ¨Åcation using kernel-
based metric learning methods, in: D. Fleet, T. Pajdla, B. Schiele, T. Tuytelaars
(Eds.), Computer Vision ‚Äì ECCV 2014, Springer International Publishing, Cham,
2014, pp. 1‚Äì16.
[15] W. Li, R. Zhao, T. Xiao, X. Wang, Deepreid: Deep Ô¨Ålter pairing neural network for
person re-identiÔ¨Åcation, in: 2014 IEEE Conference on Computer Vision and Pattern
Recognition, 2014, pp. 152‚Äì159.
[16] R. Zhao, W. Ouyang, X. Wang, Unsupervised salience learning for person re-
identiÔ¨Åcation, in: 2013 IEEE Conference on Computer Vision and Pattern Recogni-
tion, 2013, pp. 3586‚Äì3593.
[17] M. K ¬®ostinger, M. Hirzer, P. Wohlhart, P. M. Roth, H. Bischof, Large scale met-
ric learning from equivalence constraints, in: 2012 IEEE Conference on Computer
Vision and Pattern Recognition, 2012, pp. 2288‚Äì2295.
[18] J. Garc ¬¥ƒ±a, N. Martinel, A. Gardel, I. Bravo, G. L. Foresti, C. Micheloni, Modeling
feature distances by orientation driven classiÔ¨Åers for person re-identiÔ¨Åcation, Journal
of Visual Communication and Image Representation 38 (2016) 115 ‚Äì 129.
[19] N. Martinel, C. Micheloni, G. L. Foresti, Kernelized saliency-based person re-
identiÔ¨Åcation through multiple metric learning, IEEE Transactions on Image Pro-
cessing 24 (12) (2015) 5645‚Äì5658.
38[20] L. Zhang, T. Xiang, S. Gong, Learning a discriminative null space for person re-
identiÔ¨Åcation, in: Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, 2016.
[21] K. Zhou, Y . Yang, A. Cavallaro, T. Xiang, Omni-scale feature learning for person re-
identiÔ¨Åcation, in: The IEEE International Conference on Computer Vision (ICCV),
2019.
[22] K. Zhou, Y . Yang, A. Cavallaro, T. Xiang, Learning generalisable omni-scale repre-
sentations for person re-identiÔ¨Åcation, arXiv preprint arXiv:1910.06827.
[23] X. Chang, T. M. Hospedales, T. Xiang, Multi-level factorisation net for person re-
identiÔ¨Åcation, in: The IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), 2018.
[24] W. Li, X. Zhu, S. Gong, Harmonious attention network for person re-identiÔ¨Åcation,
in: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
2018.
[25] Y . Sun, L. Zheng, Y . Yang, Q. Tian, S. Wang, Beyond part models: Person re-
trieval with reÔ¨Åned part pooling (and a strong convolutional baseline), in: V . Fer-
rari, M. Hebert, C. Sminchisescu, Y . Weiss (Eds.), Computer Vision ‚Äì ECCV 2018,
Springer International Publishing, Cham, 2018, pp. 501‚Äì518.
[26] D. Kumar, P. Siva, P. Marchwica, A. Wong, Fairest of them all: Establishing a strong
baseline for cross-domain person reid, CoRR abs/1907.12016.
URL http://arxiv.org/abs/1907.12016
[27] H. Huang, W. Yang, X. Chen, X. Zhao, K. Huang, J. Lin, G. Huang, D. Du, Eanet:
Enhancing alignment for cross-domain person re-identiÔ¨Åcation, arXiv preprint
arXiv:1812.11369.
39[28] J. Liu, Z.-J. Zha, D. Chen, R. Hong, M. Wang, Adaptive transfer network for cross-
domain person re-identiÔ¨Åcation, in: The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2019.
[29] C. Tay, S. Roy, K. Yap, Aanet: Attribute attention network for person re-
identiÔ¨Åcations, in: 2019 IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR), 2019, pp. 7127‚Äì7136.
[30] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recognition, in:
2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016,
pp. 770‚Äì778.
[31] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, L. Chen, Mobilenetv2: Inverted
residuals and linear bottlenecks, in: 2018 IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition, 2018, pp. 4510‚Äì4520.
[32] Y . Xian, H. Hu, Enhanced multi-dataset transfer learning method for unsupervised
person re-identiÔ¨Åcation using co-training strategy, IET Computer Vision 12 (8)
(2018) 1219‚Äì1227.
[33] H.-X. Yu, A. Wu, W.-S. Zheng, Cross-view asymmetric metric learning for unsuper-
vised person re-identiÔ¨Åcation, in: The IEEE International Conference on Computer
Vision (ICCV), 2017.
[34] Y . Lin, Y . Wu, C. Yan, M. Xu, Y . Yang, Unsupervised person re-identiÔ¨Åcation via
cross-camera similarity exploration, IEEE Transactions on Image Processing doi:
https://doi.org/10.1109/TIP.2020.2982826 .
[35] Y . Lin, L. Xie, Y . Wu, C. Yan, Q. Tian, Unsupervised person re-identiÔ¨Åcation via
softened similarity learning, in: IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2020, 2020, pp. 3387‚Äì3396.
40[36] X. Zhu, X. Zhu, M. Li, P. Morerio, V . Murino, S. Gong, Intra-camera supervised
person re-identiÔ¨Åcation, International Journal of Computer Vision 129 (5) (2021)
1580‚Äì1595. doi:10.1007/s11263-021-01440-4 .
[37] X. Zhu, X. Zhu, M. Li, V . Murino, S. Gong, Intra-camera supervised person re-
identiÔ¨Åcation: A new benchmark, in: 2019 IEEE/CVF International Conference
on Computer Vision Workshop (ICCVW), 2019, pp. 1079‚Äì1087. doi:10.1109/
ICCVW.2019.00138 .
[38] W. Ge, C. Pan, A. Wu, H. Zheng, W.-S. Zheng, Cross-Camera Feature Prediction
for Intra-Camera Supervised Person Re-IdentiÔ¨Åcation across Distant Scenes, Asso-
ciation for Computing Machinery, New York, NY , USA, 2021, p. 3644‚Äì3653.
URL https://doi.org/10.1145/3474085.3475382
[39] D. Figueira, L. Bazzani, H. Q. Minh, M. Cristani, A. Bernardino, V . Murino, Semi-
supervised multi-feature learning for person re-identiÔ¨Åcation, in: 2013 10th IEEE
International Conference on Advanced Video and Signal Based Surveillance, 2013,
pp. 111‚Äì116. doi:10.1109/AVSS.2013.6636625 .
[40] X. Liu, M. Song, D. Tao, X. Zhou, C. Chen, J. Bu, Semi-supervised coupled dictio-
nary learning for person re-identiÔ¨Åcation, in: 2014 IEEE Conference on Computer
Vision and Pattern Recognition, 2014, pp. 3550‚Äì3557. doi:10.1109/CVPR.2014.
454.
[41] H. Wang, X. Zhu, T. Xiang, S. Gong, Towards unsupervised open-set person re-
identiÔ¨Åcation, in: 2016 IEEE International Conference on Image Processing (ICIP),
2016, pp. 769‚Äì773. doi:10.1109/ICIP.2016.7532461 .
[42] X. Xin, J. Wang, R. Xie, S. Zhou, W. Huang, N. Zheng, Semi-supervised person re-
identiÔ¨Åcation using multi-view clustering, Pattern Recognition 88 (2019) 285‚Äì297.
doi:https://doi.org/10.1016/j.patcog.2018.11.025 .
URL https://www.sciencedirect.com/science/article/pii/
S0031320318304126
41[43] J. Meng, S. Wu, W.-S. Zheng, Weakly supervised person re-identiÔ¨Åcation, in: 2019
IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019,
pp. 760‚Äì769. doi:10.1109/CVPR.2019.00085 .
[44] G. Wang, G. Wang, X. Zhang, J. Lai, Z. Yu, L. Lin, Weakly supervised person re-
id: Differentiable graphical learning and a new benchmark (2020). arXiv:1904.
03845 .
[45] Y .-J. Li, F.-E. Yang, Y .-C. Liu, Y .-Y . Yeh, X. Du, Y .-C. Frank Wang, Adaptation
and re-identiÔ¨Åcation network: An unsupervised deep transfer learning approach to
person re-identiÔ¨Åcation, in: The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR) Workshops, 2018.
[46] Z. Zhong, L. Zheng, Z. Luo, S. Li, Y . Yang, Invariance matters: Exemplar memory
for domain adaptive person re-identiÔ¨Åcation, in: Proceedings of IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), 2019.
[47] H.-X. Yu, W.-S. Zheng, A. Wu, X. Guo, S. Gong, J.-H. Lai, Unsupervised person
re-identiÔ¨Åcation by soft multilabel learning, in: The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), 2019.
[48] M. Li, X. Zhu, S. Gong, Unsupervised person re-identiÔ¨Åcation by deep learning
tracklet association, in: Proceedings of the European Conference on Computer Vi-
sion (ECCV), 2018, pp. 737‚Äì753.
[49] M. Li, X. Zhu, S. Gong, Unsupervised tracklet person re-identiÔ¨Åcation, IEEE Trans-
actions on Pattern Analysis and Machine Intelligence.
[50] D. Li, D. Li, Z. Zhang, L. Wang, T. Tan, Unsupervised cross-domain person re-
identiÔ¨Åcation: A new framework, 2019, pp. 1222‚Äì1226. doi:10.1109/ICIP.
2019.8804418 .
42[51] H. Yu, A. Wu, W. Zheng, Unsupervised person re-identiÔ¨Åcation by deep asymmetric
metric embedding, IEEE Transactions on Pattern Analysis and Machine Intelligence
42 (4) (2020) 956‚Äì973.
[52] Z. Zhong, L. Zheng, D. Cao, S. Li, Re-ranking person re-identiÔ¨Åcation with k-
reciprocal encoding, in: CVPR, 2017.
[53] W. Li, Y . Wu, M. Mukunoki, M. Minoh, Common-near-neighbor analysis for person
re-identiÔ¨Åcation, in: 2012 19th IEEE International Conference on Image Processing,
2012, pp. 1621‚Äì1624.
[54] V . Nguyen, T. D. Ngo, K. M. T. T. Nguyen, D. A. Duong, K. Nguyen, D. Le, Re-
ranking for person re-identiÔ¨Åcation, in: 2013 International Conference on Soft Com-
puting and Pattern Recognition (SoCPaR), 2013, pp. 304‚Äì308.
[55] Q. Leng, R. Hu, C. Liang, Y . Wang, J. Chen, Person re-identiÔ¨Åcation with con-
tent and context re-ranking, Multimedia Tools and Applications 74. doi:10.1007/
s11042-014-1949-7 .
[56] A. J. Ma, P. Li, Query based adaptive re-ranking for person re-identiÔ¨Åcation, in:
D. Cremers, I. Reid, H. Saito, M.-H. Yang (Eds.), Computer Vision ‚Äì ACCV 2014,
Springer International Publishing, Cham, 2015, pp. 397‚Äì412.
[57] J. Garc ¬¥ƒ±a, N. Martinel, C. Micheloni, A. Gardel, Person re-identiÔ¨Åcation ranking op-
timisation by discriminant context information analysis, in: 2015 IEEE International
Conference on Computer Vision (ICCV), 2015, pp. 1305‚Äì1313.
[58] M. Ye, J. Chen, Q. Leng, C. Liang, Z. Wang, K. Sun, Coupled-view based rank-
ing optimization for person re-identiÔ¨Åcation, in: X. He, S. Luo, D. Tao, C. Xu,
J. Yang, M. A. Hasan (Eds.), MultiMedia Modeling, Springer International Publish-
ing, Cham, 2015, pp. 105‚Äì117.
43[59] J. Garc ¬¥ƒ±a, N. Martinel, A. Gardel, I. Bravo, G. L. Foresti, C. Micheloni, Discrim-
inant context information analysis for post-ranking person re-identiÔ¨Åcation, IEEE
Transactions on Image Processing 26 (4) (2017) 1650‚Äì1665.
[60] R. Guo, C. Li, Y . Li, J. Lin, Density-adaptive kernel based re-ranking for person
re-identiÔ¨Åcation, in: 2018 24th International Conference on Pattern Recognition
(ICPR), 2018, pp. 982‚Äì987.
[61] L. Piras, G. Giacinto, Information fusion in content based image retrieval: A com-
prehensive overview, V ol. 37, 2017, pp. 50 ‚Äì 60.
[62] L. P. Valem, D. C. G. Pedronette, Unsupervised selective rank fusion for image
retrieval tasks, Neurocomputing 377 (2020) 182 ‚Äì 199.
[63] J. Wang, Z. Zhu, Image retrieval system based on multi-feature fusion and relevance
feedback, in: 2010 International Conference on Machine Learning and Cybernetics,
V ol. 4, 2010, pp. 2053‚Äì2058.
[64] M. Ye, C. Liang, Y . Yu, Z. Wang, Q. Leng, C. Xiao, J. Chen, R. Hu, Person rei-
dentiÔ¨Åcation via ranking aggregation of similarity pulling and dissimilarity pushing,
IEEE Transactions on Multimedia 18 (12) (2016) 2553‚Äì2566.
[65] L. Zheng, S. Wang, L. Tian, Fei He, Z. Liu, Q. Tian, Query-adaptive late fusion for
image search and person re-identiÔ¨Åcation, in: 2015 IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), 2015, pp. 1741‚Äì1750.
[66] L. An, X. Chen, S. Yang, X. Li, Person re-identiÔ¨Åcation by multi-hypergraph fusion,
IEEE Transactions on Neural Networks and Learning Systems 28 (11) (2017) 2763‚Äì
2774.
[67] Q. Zeng, H. Yu, Group afÔ¨Ånity guided deep hypergraph model for person re-
identiÔ¨Åcation, Electronics Letters 55 (4) (2019) 186‚Äì188.
44[68] D. C. G. Pedronette, L. P. Valem, J. Almeida, R. da S. Torres, Multimedia retrieval
through unsupervised hypergraph-based manifold ranking, IEEE Transactions on
Image Processing 28 (12) (2019) 5824‚Äì5838.
[69] L. P. Valem, D. C. G. Pedronette, J. Almeida, Unsupervised similarity learning
through cartesian product of ranking references, Pattern Recognition Letters 114
(2018) 41 ‚Äì 52.
[70] W. Webber, A. Moffat, J. Zobel, A similarity measure for indeÔ¨Ånite rankings, ACM
Transactions on Information Systems 28 (4) (2010) 20:1‚Äì20:38.
[71] A. Shtok, O. Kurland, D. Carmel, Query performance prediction using reference
lists, ACM Trans. Inf. Syst. 34 (4).
[72] Y . Zhou, W. B. Croft, Ranking robustness: A novel framework to predict query
performance, in: ACM Int. Conference on Information and Knowledge Management
(CIKM‚Äô06), 2006, pp. 567‚Äì574.
[73] Y . Zhou, W. B. Croft, Query performance prediction in web search environments,
in: ACM SIGIR Conference on Research and Development in Information Retrieval
(SIGIR‚Äô07), 2007, pp. 543‚Äì550.
[74] A. A. de Oliveira, E. Oakley, R. da Silva Torres, A. Rocha, Relevance prediction
in similarity-search systems using extreme value theory, J. Vis. Commun. Image
Represent. 60 (2019) 236‚Äì249.
[75] Y . Huang, Q. Liu, S. Zhang, D. N. Metaxas, Image retrieval via probabilistic hyper-
graph ranking, in: IEEE Conference on Conference on Computer Vision and Pattern
Recognition (CVPR‚Äô10), 2010, pp. 3376‚Äì3383.
[76] B. Sch ¬®olkopf, J. Platt, T. Hofmann, Learning with hypergraphs: Clustering, classi-
Ô¨Åcation, and embedding, in: Advances in Neural Information Processing Systems
(NIPS‚Äô07), 2007, pp. 1601‚Äì1608.
45[77] L. Sun, S. Ji, J. Ye, Hypergraph spectral learning for multi-label classiÔ¨Åcation, in:
ACM SIGKDD International Conference on Knowledge Discovery and Data Min-
ing, KDD ‚Äô08, 2008, pp. 668‚Äì676.
[78] A. Bretto, Hypergraph Theory: An Introduction, Springer Publish. Company, 2013.
[79] Z. Zheng, L. Zheng, Y . Yang, Unlabeled samples generated by gan improve the
person re-identiÔ¨Åcation baseline in vitro, in: Proceedings of the IEEE International
Conference on Computer Vision, 2017, p. 3754‚Äì3762.
[80] A. Krizhevsky, I. Sutskever, G. E. Hinton, Imagenet classiÔ¨Åcation with deep convo-
lutional neural networks, in: F. Pereira, C. J. C. Burges, L. Bottou, K. Q. Weinberger
(Eds.), Advances in Neural Information Processing Systems 25, Curran Associates,
Inc., 2012, pp. 1097‚Äì1105.
[81] L. Wei, S. Zhang, W. Gao, Q. Tian, Person transfer gan to bridge domain gap for
person re-identiÔ¨Åcation, in: 2018 IEEE/CVF Conference on Computer Vision and
Pattern Recognition, 2018, pp. 79‚Äì88.
[82] K. Zhou, T. Xiang, Torchreid: A library for deep learning person re-identiÔ¨Åcation in
pytorch, arXiv preprint arXiv:1910.10093.
[83] X. He, D. Cai, P. Niyogi, Laplacian score for feature selection, in: Proceedings of the
18th International Conference on Neural Information Processing Systems, NIPS‚Äô05,
MIT Press, Cambridge, MA, USA, 2005, pp. 507‚Äì514.
[84] Z. Zhao, H. Liu, Spectral feature selection for supervised and unsupervised learning,
in: Proceedings of the 24th International Conference on Machine Learning, ICML
‚Äô07, ACM, New York, NY , USA, 2007, pp. 1151‚Äì1157.
[85] Z. Bai, Z. Wang, J. Wang, D. Hu, E. Ding, Unsupervised multi-source domain
adaptation for person re-identiÔ¨Åcation, in: 2021 IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition (CVPR), 2021, pp. 12909‚Äì12918. doi:
10.1109/CVPR46437.2021.01272 .
46[86] H. Tang, Y . Zhao, H. Lu, Unsupervised person re-identiÔ¨Åcation with iterative self-
supervised domain adaptation, in: The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR) Workshops, 2019.
[87] Z. Zhong, L. Zheng, S. Li, Y . Yang, Generalizing a person retrieval model hetero-
and homogeneously, in: The European Conference on Computer Vision (ECCV),
2018.
[88] C. Ren, B. Liang, Z. Lei, Domain adaptive person re-identiÔ¨Åcation via camera style
generation and label propagation, CoRR abs/1905.05382.
URL http://arxiv.org/abs/1905.05382
[89] W. Deng, L. Zheng, Q. Ye, G. Kang, Y . Yang, J. Jiao, Image-image domain
adaptation with preserved self-similarity and domain-dissimilarity for person re-
identiÔ¨Åcation, in: CVPR, 2018.
[90] Y . Huang, P. Peng, Y . Jin, J. Xing, C. Lang, S. Feng, Domain adaptive
attention model for unsupervised cross-domain person re-identiÔ¨Åcation, CoRR
abs/1905.10529.
URL http://arxiv.org/abs/1905.10529
[91] H. Liu, J. Cheng, S. Wang, W. Wang, Attention: A big surprise for cross-domain
person re-identiÔ¨Åcation, CoRR abs/1905.12830.
URL http://arxiv.org/abs/1905.12830
[92] Q. Yang, H.-X. Yu, A. Wu, W.-S. Zheng, Patch-based discriminative feature learning
for unsupervised person re-identiÔ¨Åcation, in: The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), 2019.
47