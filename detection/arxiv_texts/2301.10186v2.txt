ViHOS: Hate Speech Spans Detection for Vietnamese
Phu Gia Hoang, Canh Duc Luu, Khanh Quoc Tran,
Kiet Van Nguyen ,Ngan Luu-Thuy Nguyen
University of Information Technology, Ho Chi Minh City, Vietnam
Vietnam National University, Ho Chi Minh City, Vietnam
{19520215, 19521272}@gm.uit.edu.vn
{khanhtq, kietnv, ngannlt}@uit.edu.vn
Abstract
The rise in hateful and offensive language di-
rected at other users is one of the adverse side
effects of the increased use of social network-
ing platforms. This could make it difficult
for human moderators to review tagged com-
ments filtered by classification systems. To
help address this issue, we present the Vi-
HOS ( Vietnamese Hate and Offensive Spans)
dataset, the first human-annotated corpus con-
taining 26k spans on 11k comments. We also
provide definitions of hateful and offensive
spans in Vietnamese comments as well as de-
tailed annotation guidelines. Besides, we con-
duct experiments with various state-of-the-art
models. Specifically, XLM-R Large achieved
the best F1-scores in Single span detection and
All spans detection, while PhoBERT Large ob-
tained the highest in Multiple spans detection.
Finally, our error analysis demonstrates the
difficulties in detecting specific types of spans
in our data for future research. Our dataset is
released on GitHub1.
Disclaimer : This paper contains real com-
ments that could be considered profane, offen-
sive, or abusive.
1 Introduction
Social networking sites have been widely used all
over the world. Here, users can easily share their
thoughts, connect with others, or earn money by
selling items, creating content, and so on. Since
these sites are universally accepted, many extreme
users misuse comment functions to abuse other
individuals or parties with hate and offensive lan-
guage. Consequently, it has been proved that these
types of speech could harm other users’ health (Mo-
han et al., 2017; Anjum et al., 2018). Sometimes
these behaviors can be considered cyberbullying,
cyber threats, or online harassment.
However, current studies are mainly about clas-
sifying comments as a whole with binary labels
1https://github.com/phusroyal/ViHOS
Figure 1: An example of the aid of spans detection
for traditional foul language tagging system can pro-
vide additional insightful knowledge about tagged com-
ments for human moderators.
(Zampieri et al., 2019; Nguyen et al., 2021b) or
multiple labeling schemes of abusive behaviors
(Davidson et al., 2017; Founta et al., 2018; Mathur
et al., 2018). These efforts are made to aid hu-
man moderators, who need to review a massive
number of online tagged comments that violate
their community standards. However, a system
that can highlight the spans that make a comment
hateful or offensive can be more advantageous
to human moderators who frequently deal with
long and tedious comments and prefer explana-
tions over a system-generated unexplained tag per
comment. Furthermore, in some cases, using high-
lighted spans and moderators’ context knowledge,
they can take some actions to stop cyberbullying
or online harassment. Nevertheless, there is only a
study on toxic spans, SemEval-2021 Task 5: Toxic
Spans Detection (Pavlopoulos et al., 2021). On
the other hand, in a study of Mathew et al. (2020),
hate and offensive spans worked as a rationale to
support models in classifying the whole comments.
In Vietnamese, the resources about hate and of-
fensive language are limited, namely ViHSD (Luu
et al., 2021), HSD-VLSP (Vu et al., 2020), and
UIT-ViCTSD (Nguyen et al., 2021b). Indeed, therearXiv:2301.10186v2  [cs.CL]  26 Jan 2023is no study about hate and offensive spans in Viet-
namese. This motivated us to (i) develop a new
task of extracting hate and offensive spans from
Vietnamese social media texts that conceivably im-
pact research and downstream applications and (ii)
provide the Natural Language Processing (NLP)
research community with a new dataset for rec-
ognizing hate and offensive spans in Vietnamese
social media texts.
Our two main contributions are summarized:
1. We created the first human-annotated dataset
for Vietnamese Hate and Offensive Spans (Vi-
HOS) comprising 26,467 human-annotated
spans on 11,056 comments. Our dataset is
annotated with a clear definition of hate and
offensive spans, along with detailed and spe-
cialized guidelines for a less-studied language
like Vietnamese. Compared to the toxic spans
dataset at SemEval-2021 Task 5 (Pavlopou-
los et al., 2021), which is built to detect toxic
spans from toxic comments, or the HateX-
plain dataset (Mathew et al., 2020), which has
spans working as a rationale for classifying the
whole sentence, ours includes not only a large
number of texts with annotated hate and of-
fensive spans but also clean texts without any
spans. This effort is made to serve a new task
of detecting hate and offensive spans from
Vietnamese online social media comments.
2.To evaluate the efficacy of our dataset, strong
baselines are empirically investigated on Vi-
HOS, including BiLSTM-CRF (Lample et al.,
2016), XLM-R (Conneau et al., 2019), and
PhoBERT (Nguyen and Nguyen, 2020). We
conducted various experiment schemas, in-
cluding comparing the full dataset having ad-
ditional clean comments with the dataset that
does not have; Single span detection, Multiple
spans detection, and All spans detection. We
obtain that: (i) Additional clean comments
help the baselines have better performance
than the dataset without them for 10 2% (ii)
After fine-tuning the deep learning model and
pre-trained language models, results show that
the pre-trained language models outperform
the deep learning models.
2 Related work
To the best of our knowledge, much of the research
in the field of hate speech detection has been con-
ducted in English due to the abundance of corporaand the robust pre-trained models. Many bench-
mark datasets for hate and offensive speech in other
languages have also been published in recent years,
including Arabic (Mubarak et al., 2020), Dutch
(Tulkens et al., 2016), and French (Chiril et al.,
2019). Novel models are introduced to improve
the efficiency of hate and offensive speech detec-
tion. Initial approaches were based on typical ma-
chine learning and deep neural networks with word
embeddings. Transformer-based models such as
BERT (Devlin et al., 2018), BERTology (Rogers
et al., 2020), and BERT-based transfer learning
(Ruder et al., 2019) have recently been used to de-
tect hate and offensiveness that achieved competi-
tive results in major SemEval shared tasks such as
SemEval-2020 Task 12 (Zampieri et al., 2020), and
SemEval-2021 Task 5 (Pavlopoulos et al., 2021).
However, research in Vietnamese is still limited
in terms of the dataset and experimental meth-
ods. Only a few outstanding research exist, such as
ViHSD (Luu et al., 2021), HSD-VLSP (Vu et al.,
2020), and UIT-ViCTSD (Nguyen et al., 2021b).
For the topic of detecting foul spans, there are
only a few case studies in English that are closely
related, namely the SemEval-2021 Task 5: Toxic
Spans Detection dataset (Pavlopoulos et al., 2021)
and the HateXplain dataset (Mathew et al., 2020).
The toxic spans, defined in the SemEval-2021 Task
5 dataset, are the sequences of words that make
a text toxic. There are a total of 10,629 posts in
this dataset, which stems from the Civil Comments
dataset (Borkan et al., 2019). Another dataset with
hate and offensive spans at the word level is Hat-
eXplain. The HateXplain contains 20,148 Gab and
Twitter posts. Each post is manually classified into
one of three labels: hateful, offensive, and normal.
In this study, we focus on Vietnamese to close
the gap and develop the first Vietnamese hate
and offensive spans detecting benchmark .
3 Dataset Creation
3.1 Dataset Source
ViHOS consists of 11,056 comments derived from
the ViHSD dataset (Luu et al., 2021). The Viet-
namese Hate Speech Detection dataset (ViHSD)
is one of the few large and credible social me-
dia text datasets in a low-resource language like
Vietnamese. ViHSD contains 27,624, 3,514, and
2,262 of CLEAN, HATE, and OFFENSIVE com-
ments, respectively. Comments in ViHSD are
public and collected from social media platforms.Figure 2: Detailed annotation guidelines for annotating comments for annotators. In which, Ha/Off? stands for a
requirement for annotators to check whether the associated components are hate (Ha) or offensive (Off) or not.
Thus, metaphors, idioms, proverbs, and other tricky
characteristics of online comments abound.
All of the HATE, OFFENSIVE comments from
ViHSD after removing duplicates (5,528 comments
left) are used to annotate the hate and offensive
spans. Otherwise, 5,528 CLEAN comments, which
also come from ViHSD and do not violate any hate
or offensive definition defined in Section 3.2, are
manually annotated for our dataset. We append
the 5,528 CLEAN comments because: (*) We aim
to detect the hate and offensive spans directly in
online comments; (**) With an equal number of
span and non-span (clean) comments helps models
not be biased towards any type.
3.2 Annotation Guidelines
Our goal is to create a dataset that contains com-
prehensive hate and offensive thoughts, meanings,
or opinions within the comments rather than just a
lexicon of hate and offensive terms. We define the
hate or offensive spans as follows to help annotators
understand our goals:
•Harassing, cursing, insulting, disrespecting
others.
•Sexual or verbal abuse towards one or a group
of individuals based on their sensitive char-
acteristics such as region, religion, politics,
body, gender, etc.
•Insinuations, metaphors, metonymy used for
hate, offensive or controversial purposes on
sensitive issues such as region, gender, reli-
gion, politics, human rights, etc.•Disuniting any factions or parties based on
their politics, religion, ideologies, genders,
etc.
•Causing verbal disrespect by using inappro-
priate pronouns.
•If replaced or removed, the sentence will no
longer be hateful or offensive.
However, the hate or offensiveness in Viet-
namese comments might cover one or even many
components of a sentence. For instance: "th/uni1EB1ng
ad th/uni1EDF ra cái tư duy như tr/uni1EBB l/uni1EDBp m/uni1EA7m" ( Eng: the
admin speaks as his mind just like a kindergarten
boy.) This comment consists of three nouns/nouns
phrases: "th/uni1EB1ng ad" the admin (it is offensive when
calling a guy as a "th/uni1EB1ng") as subject; "cái tư
duy" mind(the appearance of the word "cái" causes
this noun phase become offensive) and "tr/uni1EBB l/uni1EDBp
m/uni1EA7m" kindergarten boy as objects; one verb: "th/uni1EDF" (it
usually means breathe , but in this context, we could
consider it as speak but in a hate manner). As de-
fined above, we must annotate a part of the subject,
"th/uni1EB1ng," and the whole phrase of the verb with its
objects, "th/uni1EDF ra cái tư duy như tr/uni1EBB l/uni1EDBp m/uni1EA7m" ( Eng:
speaks as his mind just like a kindergarten boy ) in
order to capture the whole hate or offensive ideas.
Therefore, we provided detailed guidelines (Fig-
ure 2) to assist annotators in determining when to
annotate one or multiple components in a hateful
and offensive sentence. As we observed, most of
the comments in our dataset are colloquial. These
comments are written freely and lack many gram-
matical rules. As a result, we frequently witness
comments that lack subject(s), verb(s), conjunc-Phase 1Phase 2 Phase 31st2nd3rd4th
Kappa score 0.4161 0.4568 0.4936 0.6402 0.7239 0.7215
F1-score 0.7085 0.7186 0.7534 0.8219 0.8219 0.8585
Table 1: Inner-annotator agreement scores in three phases of annotation. In which, 1st, 2nd, 3rd, 4thare corre-
sponding with four rounds of training annotators in Phase 1.
tions, punctuation marks, and so on. To deal with
this, the comments were split into clauses/sentences
(units). The smallest subdivided unit must have at
least a cluster of "Ch/uni1EE7 ng/uni1EEF"-"V/uni1ECB ng/uni1EEF" ("Subject"-
"Verb, objects, complements"), which is considered
as a simple sentence or clause, or a cluster of "Ch/uni1EE7
ng/uni1EEF"-"V/uni1ECB ng/uni1EEF" nesting in another "Ch/uni1EE7 ng/uni1EEF"-"V/uni1ECB
ng/uni1EEF" as a complex sentence or clause. From this,
annotators can recognize components in clauses or
sentences before annotating them.
Furthermore, in Appendix A and Appendix B,
we also provided notices for annotators in Table 9
and ways to help them deal with nine online foul
linguistic phenomena in Table 8 while annotating
the data.
3.3 Dataset Construction Process
For dataset construction, we conducted three
phases in which Phases 2 and 3 were inspired by
Truong et al. (2021) and used metrics as bellow to
calculate Inter-Annotator Agreement (IAA) among
annotators. LightTag (Perry, 2021) is the tool we
used for annotating data.
Assessment of Inter-Annotator Agreement
Cohen’s Kappa is widely used to measure inter-
annotator agreement (IAA) in most tasks and is
accepted as the standard measure (McHugh, 2012).
However, numerous studies indicated that Kappa is
not the most proper measure for the NLP sequence
tagging task like NER (Hripcsak and Rothschild,
2005; Grouin et al., 2011). The reason is that the
definite number of negative cases required to cal-
culate the Kappa does not exist for named entity
spans. Spans in our task are the sequences of char-
acters rather than sequences of tokens since hate
and offensive spans could be icon(s), word(s), or
distinct character set(s) (see Table 8 for more de-
tails). Therefore, the pre-existing fixed number of
characters to consider in the process of annotating
is not existent.
A solution to deal with this is to calculate a
character-level-based Kappa. Still, it has two asso-
ciated problems: (1) annotators need to look at se-quences of one or more characters instead of char-
acters alone, causing the Kappa not to reflect the
annotation task well; and (2) the "O"-labeled char-
acters (the negative cases) outnumber the hate and
offensive ones (the positive cases), provoking the
Kappa to be computed on highly imbalanced data.
For these reasons, the F1-score calculated without
the negative cases is usually the measure for cal-
culating IAA for the NLP tagging tasks like NER
(Deleger et al., 2012). In this paper, IAA based on
both F1-score (macro average) and character-level-
based Kappa are calculated, while the former is the
primary measure.
Phase 1: Pilot Annotation
Six undergraduate students were hired for our
annotation tasks. The primary purpose of this pilot
annotation phase was to familiarize our annotators
with this task before entering the Main Annotation
phase. We then developed an initial version of an-
notation guidelines with examples and distributed
them to annotators. All annotators were required to
carefully study the guidelines and give feedback be-
fore annotating the same 100 random samples from
the 5,528 HATE, OFFENSIVE comments from
ViHSD. This process was conducted four times
with the F1-score and the Kappa for measuring
IAA, which was calculated by averaging the re-
sults of pairwise comparisons across all annotators,
shown in Table 1. All annotators were qualified
as there was no F1-score of pairwise comparisons
below 0.8.
Phase 2: Ground Truth Annotation
We randomly sampled a Ground Truth set of 600
comments from the 5,528 HATE, OFFENSIVE
comments for this phase. Two guideline develop-
ers annotated the Ground Truth set separately us-
ing the well-developed guidelines from the former
phase, resulting in an F1-score of 0.86 and Kappa
(Cohen’s Kappa) of 0.72. Afterward, we hosted
a discussion to deal with annotation conflicts and
update the annotation guidelines.
Phase 3: Main Annotation
We split the remaining HATE and OFFENSIVETrain Dev Test
Number of clean comments 4,552 569 575
Number of Ha/Off comments 4,422 553 553
Average clean comments length 8.69 8.50 9.04
Average Ha/Off comments length 16.81 17.68 16.13
Clean comments vocabulary size 4,234 1,423 1,400
Ha/Off comments vocabulary size 5,162 2,089 2,013
Number of multi-span comments (%) 2,322 (26.26) 308 (27.85) 296 (26.76)
Number of single-span comments (%) 1,970 (22.27) 229 (20.70) 235 (21.25)
Number of non-span comments (%) 4,552 (51.47) 569 (51.45) 575 (51.99)
Average number of spans 2.10 2.09 2.00
Table 2: ViHOS statistics. Vocabularies size and comments length are calculated at the syllable level.
comments (Luu et al., 2021) (4,928 comments left)
into six non-overlapping and equal subsets. We
also divided the 600-sample Ground Truth set from
Phase 2 into six equal 100-sample smaller sets to
insert into each subset. Each well-trained anno-
tator from Phase 1 received a subset to annotate.
Their annotation performance was assessed by cal-
culating the F1 score and the Kappa score of the
100-sample Ground Truth sets in their subset. If
any score is below 0.81 in terms of the F1 score,
its corresponding annotator has to annotate again
until it meets the requirement. This process was
completed with an F1-score of 0.86 in the mean.
Furthermore, our annotators manually annotated
CLEAN comments from ViHSD to spot any hate
and offensive spans before being added to our
dataset. This process collected 5,528 additional
clean comments that met our requirements of hav-
ing no hate and offensive spans.
3.4 Dataset Statistics
Before conducting dataset analysis and experi-
ments, ViHOS has a total of 11,056 comments af-
ter the annotation process and is divided into three
subsets: train, development, and test, with an 8:1:1
ratio. In detail, ViHOS has 5,360 comments with
hate and offensive spans and 5,696 clean comments
without in which 5,528 comments were addition-
ally added and 168 comments have no hate and
offensive spans after Phase 3 in the annotation pro-
cess. Table 2 contains more information on the
ViHOS statistics. It is apparent that the vocabu-
lary of ViHOS is medium-sized, which is due to
the small number of words in comments and com-
ments in our dataset. In addition, more statistics
about spans in ViHOS are shown in Table 3.Train Dev TestSpans Quantity0 span (%) 4,552 (51.47) 569 (51.45) 575 (51.99)
1 span (%) 1,970 (22.27) 229 (20.71) 235 (21.25)
2 - 3 spans (%) 1,527 (17.27) 207 (18.72) 202 (18.26)
4 - 6 spans (%) 601 (6.80) 75 (6.78) 68 (6.15)
7 - 10 spans (%) 164 (1.85) 18 (1.63) 21 (1.90)
>10 spans (%) 30 (0.34) 8 (0.72) 5 (0.45)Spans Length1 syllable (%) 5,253 (52.03) 699 (52.48) 647 (52.77)
2 - 3 syllables (%) 3,554 (35.20) 466 (34.98) 474 (38.66)
4 - 6 syllables (%) 916 (9.07) 122 (9.16) 112 (9.14)
7 - 10 syllables (%) 259 (2.57) 31 (2.33) 19 (1.55)
>10 syllables (%) 114 (1.13) 14 (1.05) 14 (1.14)
Table 3: Spans quantity and length statistics.
4 Experiments and Results
4.1 Baseline Models
We treat the task of detecting hate and offensive
spans as a task of sequence tagging. As a result, we
make use of IOB format (Ramshaw and Marcus,
1995) to tag characters for model training, and test-
ing. We conduct experiments on a set of solid base-
line models, including BiLSTM-CRF and two pre-
trained language models, XLM-R and PhoBERT,
to assess the difficulty of our dataset.
BiLSTM-CRF : We use BiLSTM-CRF (Lample
et al., 2016), a model that achieves high perfor-
mance in the span detection tasks (Pavlopoulos
et al., 2021; Nguyen et al., 2021a). We imple-
mented this model with three main layers: (1) The
word embedding layer using pre-trained PhoW2V
(Nguyen et al., 2020), (2) The BiLSTM layer, and
(3) the Conditional Random Field (CRF).
XLM-R: XLM-RoBERTa (Conneau et al., 2019)
is a multilingual language model and a variant of
RoBERTa, pre-trained on 2.5T of data across 100
languages containing 137GB of Vietnamese texts.
On several cross-lingual benchmarks, XLM-R out-
performs mBERT.BiLSTM-CRF
+ Pho2W syllableBiLSTM-CRF
+ Pho2W wordXLM-R Base XLM-R Large PhoBERT Base PhoBERT Large
Full Data 0.7453 0.7036 0.7467 0.7770 0.7569 0.7716
W/o additional clean comments 0.6241 0.6244 0.6479 0.6756 0.6738 0.6867
Table 4: Experimental results on Full Data versus Without additional clean comments.
ModelSingle span Multiple spans All spans
P R F1 P R F1 P R F1SyllableBiLSTM-CRF + Pho2W syllable 0.4222 0.5009 0.4329 0.5134 0.5712 0.5068 0.7452 0.7769 0.7453
XLM-R Base 0.7604 0.7653 0.7203 0.7927 0.7574 0.7327 0.7766 0.7574 0.7467
XLM-R Large 0.7577 0.7679 0.7214 0.7829 0.7569 0.7357 0.8071 0.7887 0.7770WordBiLSTM-CRF + Pho2W word 0.3196 0.4468 0.3594 0.3533 0.5001 0.4013 0.6823 0.7489 0.7036
PhoBERT Base 0.7392 0.7485 0.7016 0.7761 0.7329 0.7092 0.7870 0.7680 0.7569
PhoBERT Large 0.7435 0.7567 0.7067 0.7878 0.7557 0.7321 0.8028 0.7835 0.7716
Table 5: Experimental results on Single span, Multiple spans, and All spans subsets.
PhoBERT: PhoBERT (Nguyen and Nguyen,
2020) is a monolingual language model which is
pre-trained on a 20GB Vietnamese dataset and has
the same architecture and approach as RoBERTa.
PhoBERT is proven as a state-of-the-art method in
multiple Vietnamese-specific NLP tasks such as
Part-Of-Speech Tagging, Dependency Parsing, and
NER (Truong et al., 2021; Nguyen and Nguyen,
2020).
4.2 Experimental Settings
We empirically fine-tuned all pre-trained language
models using simpletransformers2. For the tok-
enizer, each comment was tokenized using Vn-
CoreNLP (Vu et al., 2018) in word-level and
syllable-level for fine-tuning the PhoBERT and the
XLM-R, respectively. In addition, we used Adam
optimizer with a learning rate of 2e-5, a batch size
of 8, and trained with 10 epochs.
We utilized a pre-trained word embedding -
PhoW2V both syllable-level and word-level set-
tings (Nguyen et al., 2020) with 100 dims to im-
plement the BiLSTM-CRF model. The optimal
hyper-parameters of BiLSTM-CRF are described
in Table 6. All baseline models were trained on a
system having 26GB RAM and an NVIDIA Tesla
P100 GPU.
4.3 Evaluation Metrics
The macro-average F1-score (F1) is used to eval-
uate our models. For each pair of gold-predicted
spans, we compute F1 and then calculate the arith-
metic mean of F1 for each of these cases. It should
be noted that the final F1-score, Accuracy, and Pre-
2https://simpletransformers.ai/ (ver.0.63.3)Hyper-parameters Values
Optimizer Adam
Learning rate 0.001
Mini-batch size 64
LSTM hidden state size 60
Embedding size 100
Dropout [0.1, 0.1]
Epochs 10
Table 6: Hyper-parameters of the BiLSTM-CRF.
cision reported are an average of more than ten runs
with various random seeds.
4.4 Experiments and Results
Table 4 reports the baseline results before and af-
ter adding the 5,528 additional clean comments.
We discover that after the addition, the perfor-
mances improve 0.1002 0.0210. Specifically,
PhoBERT Large considerably outperforms other
models in the dataset without additional clean data,
achieving 0.6867 in F1-score. In addition, the best
model trained on Full data is XLM-R Large , which
has an F1-score of 0.7770. We find that XLM-
RLarge increased by 0.1014 and PhoBERT Large
increased by 0.0849. These results demonstrate that
the appearance of the additional clean comments
successfully reduces model bias and improves per-
formance.
Table 5 reports our results in three subsets corre-
sponding to Single, Multiple, and All spans. Both
Single and Multiple spans subsets are made by the
process of splitting the All spans, which also known
as the test set, based on the number of spans in each
comment. Their results are described as follows:
Single span : We experimented with bothFigure 3: Error analysis conducted on prediction on dev set made by PhoBERT Large and XLM-R Large . We divide
error cases into 12 categories including wrong spans detection, wrong spans boundary, allusion, annotation error,
abbreviation, spelling mistake, needing domain knowledge, lack of context, idiom or proverb, missing diacritical
marks, metaphor or metonymy, and others (rare characters, mixing other languages, all words stick together, etc.).
These error cases are defined in Appendix C.
syllable-level and word-level language models.
We discover that the pre-trained language mod-
els outperform the BiLSTM-CRF model by
0.35210.0099 in F1. This significant gap proves
the fact that word embedding and features extrac-
tion of the pre-trained language models on the
social media texts are superior to the BiLSTM-
CRF. The XLM-R Large model achieves the best
performance with a 0.7214 in F1-score. On the
other hand, PhoBERT Large achieves a 0.7067 in
F1-score. These results show no significant differ-
ence in performances among the multilingual and
monolingual pre-trained models in the Single span.
Multiple spans : We experimented with the
syllable-level and word-level and found that the
pre-trained language models beat the BiLSTM-
CRF model by 0.3212 0.0132. In addition, the
performance of XLM-R Large is slightly better than
the PhoBERT Large by 0.0036 in F1-score. The re-
sults on the Multiple spans are always better than
the Single span, which might be explained by the
fact that data in Multiple spans comprise more hate
and offensive spans that can assist the models in
learning more features of the data.
All spans : The results of the experiments on the
All data are higher than the Single span and the
Multiple spans. Specifically, in terms of F1-score,
results of the XLM-R Large model are higher by
0.0556 and 0.0413 than the highest in the Single
span and the Multiple spans, respectively while
the figures for the PhoBERT Large are 0.0649 and
0.0395, respectively.
4.5 Results Analysis
We choose two best models: PhoBERT Large and
XLM-R Large to conduct error analysis. As shownin Figure 3, we report the statistics of the ratio of
various types of error cases3of 100 random sam-
ples in the dev set. We notice that wrong spans pre-
diction4,wrong spans boundary5,allusion6,anno-
tation error7, and lack of context8are major types
of prediction failure of the PhoBERT Large and the
XLM-R Large .
We show some cases from the ViHOS develop-
ment set in Table 7. In the first case of "Ns v/uni1EADy
l/uni1EA1i xúc ph/uni1EA1m cái đ/uni1EA7u b**i *neutral face emoji*"
(Eng:If you said so, you insult the d**k head *neu-
tral face emoji* ), we notice that the PhoBERT Large
could only predict "đ/uni1EA7u" headas a hate and offensive
span, whereas XLM-R Large predicts "b**i" bi.
Both fail to predict the full boundary of "đ/uni1EA7u
b**i" dk head . The reason is that asterisks ex-
ist in the text ("b**i"). As defined in Subsection
4.2, the PhoBERT Large , which was fine-tuned on
the word-level data, cannot identify these tokens,
but the XLM-R Large , which was fine-tuned on the
syllable-level data, can somewhat predict more ac-
curate hate and offensive span even if it has aster-
isks.
Furthermore, in the second sampled comment,
both best models failed to predict "đám c/uni1EDD vàng"
(Eng:those yellow flag ones ) as a hate and offen-
sive span, owing to the fact that this phase is an
offensive metaphor for a rival party to Vietnamese
3Definition of errors are explained in the Appendix C.
4The model predicts clean spans as hateful and offensive.
5The model predicts inadequate boundary or fails to predict
correctly.
6The comment refers to another person or subject indirectly
and disrespectfully.
7The annotated span is wrong in terms of our guidelines.
There is no later annotation modification in ViHOS.
8The comment has multiple meanings in different contexts,
which mislead the prediction.Ground truth spansModels
PhoBERT Large XLM-R Large
Ns v/uni1EADy l/uni1EA1i xúc ph/uni1EA1m cái đ/uni1EA7ub**i *neutral face emoji*
(Eng:If you said so, you insult the d**k head *neu-
tral face emoji* )["xúc", "đ/uni1EA7u"] ["b**i"]
@username nhân dân VN t/uni1EE9c là c/uni1EDD đ/uni1ECF. Còn đámc/uni1EDD
vàng là lũsúcv/uni1EADtlưuvong. Hi/uni1EC3u hông?
(Eng:@username Vietnamese people are red flag.
Theyellowflagones are animals andexiled. Under-
stand? )["đ/uni1ECF.", "đám", "lũ",
"súc"]["đám", "lũ", "súc",
"v/uni1EADt", "vong."]
Cư/uni1EDBp đêmlàgi/uni1EB7c, cư/uni1EDBp ngày làquan. 24/7 lúc nào
cũng ph/uni1EA3i nơm n/uni1EDBp. Than ôicáiđ/uni1EA5tnư/uni1EDBc h/uni1EA1nh phúc
(Eng:Night thieves areenemies, daythieves arebu-
reaucrats. 24/7, we are in a perpetual state of fear
and anxiety. What ahappy country)["Cư/uni1EDBp", "cư/uni1EDBp",
"n/uni1EDBp."]["Cư/uni1EDBp", "gi/uni1EB7c,",
"cư/uni1EDBp"]
Đúng r/uni1ED3i đéo th/uni1EC3 tin đư/uni1EE3c. Đáng l/uni1EBD ph/uni1EA3i cào b/uni1EB1ng ra
m/uni1EDBi đúng. Th/uni1EADt không th/uni1EC3 tin n/uni1ED5i. Ph/uni1EA3i cào b/uni1EB1ng ra
m/uni1EDBi đư/uni1EE3c
(Eng:Yes,fucking unbelievable. They should dig
until it comes out. Unbelievable. Should dig till it
comes out )["đéo", "cào", "cào"] ["đéo"]
Table 7: Case studies in the dev set from ViHOS that are complicated for the PhoBERT Large and XLM-R Large .
The highlighted spans in first column are Ground truth spans associated with their comments. Eng refers to the
English meaning of associated comments.
people. In the third instance, the phrase "cư/uni1EDBp đêm
là gi/uni1EB7c, cư/uni1EDBp ngày là quan," ( Eng:night thieves are
enemies, day thieves are bureaucrats ) which is an
idiom that originated from folk poetry, also mis-
leads the prediction. In the final example, the verb
"cào" dighas no object and must be comprehended
in context. These intriguing and challenging lin-
guistic phenomena encourage more research into
more robust models and methods in this field.
5 Conclusion and Future Work
We presented ViHOS, a new Vietnamese dataset
for evaluating hate and offensive spans detec-
tion models. ViHOS includes 26,467 human-
annotated spans on 11,056 comments. In addi-
tion, state-of-the-art models are conducted as the
first baseline models, including BiLSTM-CRF and
pre-trained language models such as XLM-R Base,
XLM-R Large , PhoBERT Base, and PhoBERT Large .
As a result, the XLM-R Large model achieves the
best performance, with an F1-score of 0.7770. Fur-
thermore, we discover that the performance when
detecting multiple spans is better than the perfor-
mance in detecting single spans in Vietnamese hateand offensive spans detection. Our dataset is avail-
able publicly at the GitHub link9.
Despite the study’s many promising contribu-
tions, the proposed research work still has sev-
eral potential concerns, especially since the per-
formance is still modest, and incorrect predictions
could harm users’ reputations if they rely heavily on
our method. We intend to expand the dataset size
and diversity of hate and offensive context for Viet-
namese in the future to address this shortcoming.
Furthermore, pre-and post-processing techniques
will be used to standardize social networking texts
(Clark and Araki, 2011) and deal with complex
cases (as discussed in Subsection 4.5) to improve
model performance(Suman and Jain, 2021; Kotyu-
shev et al., 2021; Chhablani et al., 2021), particu-
larly for Vietnamese pre-trained language models.
9https://github.com/phusroyal/ViHOSLimitations, Social Impacts, and Ethical
Considerations
Limitations and Social Impacts
There are numerous incomprehensible comments
in our dataset due to the lack of context. Con-
sequently, our annotators had to place themselves
in imaginary contexts in order to annotate those
comments (see Table 9 for more details about our
solution). This shortcoming combined with the
limitations of the neural networks in terms of un-
derstanding various linguistic phenomena (see Ta-
ble 8 for more details about nine different linguistic
phenomena) caused their performances of this task
still insufficient to become practical.
We also acknowledge the risk associated with
publicizing a dataset of hate and offensive spans
(e.g. utilizing ours as a source for building abusive
chatbots). However, we firmly believe that our
proposed benchmark creates more value than risks.
Ethical Considerations
The undergraduate students in the annotation pro-
cess are Vietnamese native speakers; have at least
12 years of studying Vietnamese with average
scores on the Vietnam National Exam on Liter-
ature of 6.5; have at least three years of using social
network platforms. They were explicitly warned
that their tasks will display hateful and offensive
content and if they became overwhelmed, they were
also urged to stop labeling. These undergraduate
students were paid $0.1 per comment, which takes
an average of 6.44 seconds to complete (excluding
the time used by workers who took exceptionally
lengthy comments).
All the comments in ViHOS originated in the
study of Luu et al., 2021, which preserved users’
anonymity by removing all of them when creat-
ing the ViHSD. As a result, the comments in our
dataset do NOT reflect our thoughts or viewpoints.
ViHOS is available to the public under a usage
agreement for research and related purposes only.
Acknowledgments
This work has been funded by The VNUHCM-
University of Information Technology’s Scientific
Research Support Fund.
References
Amna Anjum, Xu Ming, Ahmed Faisal Siddiqi, and
Samma Faiz Rasool. 2018. An empirical study an-alyzing job productivity in toxic workplace environ-
ments. International journal of environmental re-
search and public health , 15(5):1035.
Daniel Borkan, Lucas Dixon, Jeffrey Sorensen, Nithum
Thain, and Lucy Vasserman. 2019. Nuanced met-
rics for measuring unintended bias with real data for
text classification. In Companion proceedings of the
2019 world wide web conference , pages 491–500.
Gunjan Chhablani, Abheesht Sharma, Harshit Pandey,
Yash Bhartia, and Shan Suthaharan. 2021. Nlrg at
semeval-2021 task 5: Toxic spans detection leverag-
ing bert-based token classification and span predic-
tion techniques. arXiv preprint arXiv:2102.12254 .
Patricia Chiril, Farah Benamara, Véronique Moriceau,
Marlène Coulomb-Gully, and Abhishek Kumar.
2019. Multilingual and multitarget hate speech de-
tection in tweets. In Conférence sur le Traitement
Automatique des Langues Naturelles (TALN-PFIA
2019) , pages 351–360. ATALA.
Eleanor Clark and Kenji Araki. 2011. Text normal-
ization in social media: progress, problems and ap-
plications for a pre-processing system of casual en-
glish. Procedia-Social and Behavioral Sciences ,
27:2–11.
Alexis Conneau, Kartikay Khandelwal, Naman Goyal,
Vishrav Chaudhary, Guillaume Wenzek, Francisco
Guzmán, Edouard Grave, Myle Ott, Luke Zettle-
moyer, and Veselin Stoyanov. 2019. Unsupervised
cross-lingual representation learning at scale. arXiv
preprint arXiv:1911.02116 .
Thomas Davidson, Dana Warmsley, Michael Macy,
and Ingmar Weber. 2017. Automated hate speech
detection and the problem of offensive language. In
Proceedings of the International AAAI Conference
on Web and Social Media , volume 11.
Louise Deleger, Qi Li, Todd Lingren, Megan Kaiser,
Katalin Molnar, et al. 2012. Building gold stan-
dard corpora for medical natural language process-
ing tasks. In AMIA Annual Symposium Proceedings ,
volume 2012, page 144. American Medical Infor-
matics Association.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing.arXiv preprint arXiv:1810.04805 .
Antigoni Maria Founta, Constantinos Djouvas, De-
spoina Chatzakou, Ilias Leontiadis, Jeremy Black-
burn, Gianluca Stringhini, Athena Vakali, Michael
Sirivianos, and Nicolas Kourtellis. 2018. Large
scale crowdsourcing and characterization of twitter
abusive behavior. In Twelfth International AAAI
Conference on Web and Social Media .
Cyril Grouin, Sophie Rosset, Pierre Zweigenbaum,
Kar¨en Fort, Olivier Galibert, and Ludovic Quintard.
2011. Proposal for an extension of traditional named
entities: From guidelines to evaluation, an overview.InProceedings of the 5th linguistic annotation work-
shop, pages 92–100.
Hoang Phe. 1983. Vietnamese Dictionary . Hong Duc,
Vietnam.
George Hripcsak and Adam S Rothschild. 2005.
Agreement, the f-measure, and reliability in infor-
mation retrieval. Journal of the American medical
informatics association , 12(3):296–298.
Mikhail Kotyushev, Anna Glazkova, and Dmitry Mo-
rozov. 2021. Mipt-nsu-utmn at semeval-2021 task
5: Ensembling learning with pre-trained language
models for toxic spans detection. arXiv preprint
arXiv:2104.04739 .
Guillaume Lample, Miguel Ballesteros, Sandeep Sub-
ramanian, Kazuya Kawakami, and Chris Dyer. 2016.
Neural architectures for named entity recognition.
CoRR , abs/1603.01360.
Son T Luu, Kiet Van Nguyen, and Ngan Luu-Thuy
Nguyen. 2021. A large-scale dataset for hate speech
detection on vietnamese social media texts. arXiv
preprint arXiv:2103.11528 .
Binny Mathew, Punyajoy Saha, Seid Muhie Yi-
mam, Chris Biemann, Pawan Goyal, and Animesh
Mukherjee. 2020. Hatexplain: A benchmark dataset
for explainable hate speech detection. arXiv preprint
arXiv:2012.10289 .
Puneet Mathur, Rajiv Shah, Ramit Sawhney, and De-
banjan Mahata. 2018. Detecting offensive tweets in
Hindi-English code-switched language. In Proceed-
ings of the Sixth International Workshop on Natural
Language Processing for Social Media , pages 18–
26, Melbourne, Australia. Association for Computa-
tional Linguistics.
Mary L McHugh. 2012. Interrater reliability: the
kappa statistic. Biochemia medica , 22(3):276–282.
Shruthi Mohan, Apala Guha, Michael Harris, Fred
Popowich, Ashley Schuster, and Chris Priebe. 2017.
The impact of toxic language on the health of reddit
communities. In Canadian Conference on Artificial
Intelligence , pages 51–56. Springer.
Hamdy Mubarak, Ammar Rashed, Kareem Darwish,
Younes Samih, and Ahmed Abdelali. 2020. Arabic
offensive language on twitter: Analysis and experi-
ments. arXiv preprint arXiv:2004.02192 .
Anh Tuan Nguyen, Mai Hoang Dao, and Dat Quoc
Nguyen. 2020. A Pilot Study of Text-to-SQL Se-
mantic Parsing for Vietnamese. In Findings of the
Association for Computational Linguistics: EMNLP
2020, pages 4079–4085.
Dat Quoc Nguyen and Anh Tuan Nguyen. 2020.
Phobert: Pre-trained language models for viet-
namese. arXiv preprint arXiv:2003.00744 .Kim Thi-Thanh Nguyen, Sieu Khai Huynh, Luong Luc
Phan, Phuc Huynh Pham, Duc-Vu Nguyen, and Kiet
Van Nguyen. 2021a. Span detection for aspect-
based sentiment analysis in vietnamese. arXiv
preprint arXiv:2110.07833 .
Luan Thanh Nguyen, Kiet Van Nguyen, and Ngan Luu-
Thuy Nguyen. 2021b. Constructive and toxic speech
detection for open-domain social media comments
in vietnamese. arXiv preprint arXiv:2103.10069 .
Nguyen Hoai Nguyen. 2010. Phonological character-
istics of Nghe-Tinh dialect with the study of Viet-
namese language history. Vinh University, Viet-
nam.
John Pavlopoulos, Jeffrey Sorensen, Léo Laugier, and
Ion Androutsopoulos. 2021. Semeval-2021 task
5: Toxic spans detection. In Proceedings of the
15th International Workshop on Semantic Evalua-
tion (SemEval-2021) , pages 59–69.
Tal Perry. 2021. LightTag: Text annotation platform.
InProceedings of the 2021 Conference on Empiri-
cal Methods in Natural Language Processing: Sys-
tem Demonstrations , pages 20–27, Online and Punta
Cana, Dominican Republic. Association for Compu-
tational Linguistics.
Lance Ramshaw and Mitch Marcus. 1995. Text chunk-
ing using transformation-based learning. In Third
Workshop on Very Large Corpora .
Anna Rogers, Olga Kovaleva, and Anna Rumshisky.
2020. A primer in bertology: What we know about
how bert works. Transactions of the Association for
Computational Linguistics , 8:842–866.
Sebastian Ruder, Matthew E Peters, Swabha
Swayamdipta, and Thomas Wolf. 2019. Trans-
fer learning in natural language processing. In
Proceedings of the 2019 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Tutorials , pages 15–18.
Thakur Ashutosh Suman and Abhinav Jain. 2021.
Astartwice at semeval-2021 task 5: Toxic span
detection using roberta-crf, domain specific pre-
training and self-training. In Proceedings of the
15th International Workshop on Semantic Evalua-
tion (SemEval-2021) , pages 875–880.
Thinh Hung Truong, Mai Hoang Dao, and Dat Quoc
Nguyen. 2021. COVID-19 named entity recognition
for Vietnamese. In Proceedings of the 2021 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies , pages 2146–2153, Online. As-
sociation for Computational Linguistics.
Stéphan Tulkens, Lisa Hilte, Elise Lodewyckx,
Ben Verhoeven, and Walter Daelemans. 2016.
A dictionary-based approach to racism detec-
tion in dutch social media. arXiv preprint
arXiv:1608.08738 .Thanh Vu, Dat Quoc Nguyen, Dai Quoc Nguyen, Mark
Dras, and Mark Johnson. 2018. Vncorenlp: A viet-
namese natural language processing toolkit. arXiv
preprint arXiv:1801.01331 .
Xuan-Son Vu, Thanh Vu, Mai-Vu Tran, Thanh Le-
Cong, and Huyen Nguyen. 2020. Hsd shared task
in vlsp campaign 2019: Hate speech detection for
social good. arXiv preprint arXiv:2007.06493 .
Marcos Zampieri, Shervin Malmasi, Preslav Nakov,
Sara Rosenthal, Noura Farra, and Ritesh Kumar.
2019. Predicting the type and target of offen-
sive posts in social media. In Proceedings of the
2019 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, Volume 1 (Long and
Short Papers) , pages 1415–1420, Minneapolis, Min-
nesota. Association for Computational Linguistics.
Marcos Zampieri, Preslav Nakov, Sara Rosenthal, Pepa
Atanasova, Georgi Karadzhov, Hamdy Mubarak,
Leon Derczynski, Zeses Pitenis, and C ¸ a ˘grı C ¸ ¨oltekin.
2020. Semeval-2020 task 12: Multilingual offensive
language identification in social media (offenseval
2020). arXiv preprint arXiv:2006.07235 .A Abusive Language Characteristics
Table 8: Characteristics of abusive language in ViHOS with examples, explanations, and solutions for annotators.
Abusive language
characteristicsExamples Explanations and Solutions
Non-diacritical
marks comments(1) Dit me cai quy trinh,vao cap cuu
deo co tien,deo bao hiem thi nam do
di.
(Eng: Fuck the procedure, without
money or insurance, you could just
lay there and no one cares about your
emergency
(2) Dung la con linh dien dien vua
thoi chang qua nt goi de choc my
dien thoi "con ng" nhu linh dien thi
ai them
(Eng: That must be the Crazy Linh,
so crazy! I just call to tease the Crazy
My. No one gonna love Crazy/The
person like Crazy Linh!Explanation: some of these non-
diacritical marks comments might
trick annotators a little bit.
(1): this is a non-diacritical marks
comment but still able to under-
stand.
(2): there are some problems caus-
ing annotators to re-read multiple
times as no punctuation, diacritic,
and the text "con ng" could be
considered as "crazy girl" or "the
type (of human)" and both of these
meanings is inappropriate.
Solution: Non-diacritical marks
comments are annotated as others.
Annotators have to re-read until
they fully understand the texts
if needed. Those examples are
annotated as follows:
(1): ["Dit me", "deo", "deo"]
(Eng: ["Fuck", "fuck", "fuck"])
(2): ["con", "dien", "dien", "con
ng", "dien", "ai them"]
(Eng: ["con", "crazy ", "crazy ",
"crazy/the person like ", "crazy ",
"No one gonna love "] in which the
word "con" is an inappropriate way
to call a woman.)Table 8 continued from the previous page.
Metaphors,
metonymies(1) cái mi/uni1EC7ng r/uni1ED9ng quá đ/uni1EBB con ra còn
l/uni1ECDt
(Eng: The mouth is too big that a
baby could even be born through it )
(2) Dm! qua đ/uni1EE3t d/uni1ECBch này thì th/uni1EB1ng
s/uni1ED1ng ích k/uni1EF7 này ch/uni1EAFc s/uni1EDBm gia nh/uni1EADp
juventuts
(Eng: Fuck it! After the pandemic,
this selfish boy will soon join Juven-
tuts)Explanation: in our dataset,
many comments use metaphors or
metonymy to convey their hate or
offensiveness in another way.
(1): this is a metaphor of a mouth
as a vagina.
(2): this is a metonymy of Ju-
ventus’ jersey as prison shirts (a
common metonymy in Vietnamese).
Solution: Annotators are re-
quired to annotate the whole ideas
of metaphors, metonymy.
(1): ["cái mi/uni1EC7ng r/uni1ED9ng quá đ/uni1EBB con ra
còn l/uni1ECDt"]
(2): ["Dm", "th/uni1EB1ng s/uni1ED1ng ích k/uni1EF7 này
ch/uni1EAFc s/uni1EDBm gia nh/uni1EADp juventuts"]
(Eng: ["Fuck", "this selfish boy will
soon join Juventuts "])
Puns(1) Ad đăng bài này cũng là B/uni1ED3n Kỳ
L/uni1EAFc nè nè nè :)
(Eng: The admin, who posts this, is
also a "B/uni1ED3n Kỳ L/uni1EAFc")Explanation: Some comments
use phrases that only read them
backwards, they make sense. As in
the example, "B/uni1ED3n Kỳ L/uni1EAFc", if this
phrase is read backwards, it is "B/uni1EAFc
Kỳ L/uni1ED3n" (pussy north).
Solution: Annotators are re-
quired to annotate these puns too.
(1): ["B/uni1ED3n Kỳ L/uni1EAFc"]
Using non-words
characters to form
hieroglyphs(1) có tin t l/uni1EA5y *knife symbol* xiên
ch/uni1EBFt c/uni1EE5 m ko :)))
(Eng: Do you belive that I could
get a *knife symbol* to fucking kill
you?)
(2) Đâm vào ()
(Eng: stab in the ()Explanation:
(1): the *knife symbol* is used
instead of the word knife.
(2): the existence of "()" can be
considered as pussy in this context.
Solution: Annotating the non-
words only if they can convey a full
meaning of hate or offensiveness,
and the whole phase if they can not.
(1): ["t", "*knife symbol*", "xiên
ch/uni1EBFt c/uni1EE5 m""]
(Eng: ["t", "*knife symbol* ",
"fucking kill you "]) in which "t",
"m" are inappropriate pronouns.)
(2): ["Đâm vào ()"]Table 8 continued from the previous page.
Spelling mistakes(1) Tôi đeo hi/uni1EC3u b/uni1EA1n noi cai gì luôn a
:)))
(Eng: I have no fucking idea about
what you saying )
(2) Mà dx cái ch/uni1EEFi ngta nghe hài vcl
bù l/uni1EA1i c/uni1EE7ng đ/uni1EDF
(Eng: Those fucking curses is so
funny that can even refill that )
(3)Khi b/uni1EA1n xai trính tã nhưng cá ghén
vít đún trính tã :))
(Eng: When you make spelling mis-
takes but trying to fix it :)) )Explanation:
(1): the words "đeo," "noi," "cai,"
"a" are spelling mistakes. The
words can be understood as "đéo,"
"nói," "á". As that, these words are
the same in accidentally missing
acute marks.
(2): the words: "ch/uni1EEFi," "c/uni1EE7ng,"
"đ/uni1EDF" are spelling mistakes is a
phenomenon of mistaking tilde
mark for hook above mark, and
this often happens in some parts of
Vietnam (Nguyen Hoai Nguyen,
2010). There are also familiar
phenomena of mistaking marks
such as tilde mark for underdot
mark, acute mark for hook above
mark, and so on (Nguyen Hoai
Nguyen, 2010).
(3): the phases: "xai trính tã,"
"cá ghén," "vít đúng trính tã" are
spelling mistakes but on purpose.
This comment utilizes spelling
mistakes to attack opponents who
also have spelling mistakes. Fur-
thermore, these spelling mistakes
also abuse opponents based on
regional distinctions in accent,
which cause some phenomenon of
mistaking diacritical marks as in
Example (2).
Solution: The same as deal-
ing with non-diacritical marks
comments, annotators work as
usual.
(1): ["deo"]
(Eng: ["fuck"])
(2): ["ch/uni1EEFi", "vcl"]
(Eng: ["curse ", "fuck"])
(3): ["b/uni1EA1n xai trính tã nhưng cá
ghén vít đún trính tã :))"]Table 8 continued from the previous page.
Allusive language(1) ra gì thì toi r/uni1ED3i, nó khác gì
cách đ/uni1EA3ng CS ch/uni1ECDn ngư/uni1EDDi, h/uni1ED3ng hơn
chuyên. Suy nghĩ kĩ đi. Gi/uni1EA3i đ/uni1ED9c
c/uni1ED9ng s/uni1EA3n đã khó, gi/uni1EA3i đ/uni1ED9c tư tư/uni1EDFng
cánh t/uni1EA3 còn khó hơn.
(Eng: If it had something, it was
done! It is just like the way CS
(stands for Communism) chooses
people, beauty over the profession.
Detoxifying Communism is hard;
detoxifying the left-wing political ide-
ologies is even harder. )
(2) Rút kinh nghi/uni1EC7m l/uni1EA1i đư/uni1EE3c xài
nghìn t/uni1EF7
(Eng: Just say learned and then can
use trillion VND )Solution: Annotators are required
to annotate the whole profound
abuse.
(1): ["toi", "h/uni1ED3ng hơn chuyên",
"Gi/uni1EA3i đ/uni1ED9c c/uni1ED9ng s/uni1EA3n đã khó, gi/uni1EA3i đ/uni1ED9c
tư tư/uni1EDFng cánh t/uni1EA3 còn khó hơn"]
(Eng: ["die", "beauty over the pro-
fession ", "Detoxifying Communism
is hard; detoxify the left-wing politi-
cal ideologies even harder "])
(2): ["Rút kinh nghi/uni1EC7m l/uni1EA1i đư/uni1EE3c xài
nghìn t/uni1EF7"]
Homonym(1) coin card
("coin card" is a homonym of "con
c/uni1EB7c", which means dick)Solution: annotate the whole abu-
sive homonym.
(1): ["coin card"]
Mixing languages(1) Vl fake
Eng: Fuck! Fake)
(2) phe Xcompat t/uni1ED5ng Y=))
(Eng: X’s side combats Y’s side )
(3) T/uni1ED1i Th/uni1EA7y stream đá phò đi th/uni1EA7y ơi
(Eng: You should stream fucking
some whores tonight, please! )Solution: treat foreign words as
others and annotate if they meet
the definition of hate and offensive
spans.
(1): ["Vl", "fake"]
(Eng: ["fuck", "fake"])
(2): ["compat"]
(Eng: ["combat "])
(3): ["đá phò"]
(Eng: ["fuck some whores "])Table 8 continued from the previous page.
Trick hate speech de-
tecting systems on
purpose(1) Đ/uni1EA5y Ông già x/uni1EA1o l*n c/uni1EE7a các b/uni1EA1n
đư/uni1EE3c t/uni1EAFm b/uni1ED3n Thái Lan đ/uni1EA5y=)))
(Eng: See, your fucking old liar is
Thai bathing again =))) )
(2) th/uni1EB1ng c h.ó ngu
(Eng: Fucking stupid guy )
(3) vualonemlabaonhieu cm
(Eng: How long in cm to fit your
pussy? )Explanation : some hate or offen-
sive comments use punctuation to
censor their inappropriate words
(as in (1), asterisk is used in the
word l*n (pussy)) or to disunite
characters in words (as in (2), dot
and space are used to disunite the
words chó (dog) into c h.ó).
These efforts actually can trick
many hate speech detecting sys-
tems, or to put all words together
(as in (3), vualonemlabaonhieu cm
should be "v/uni1EEBa l/uni1ED3n em là bao nhiêu
cm").
Solution: we annotate all charac-
ters which can be form into hate or
offensive phase.
(1): ["x/uni1EA1o l*n"]
(Eng: ["fucking lie "])
(2): ["th/uni1EB1ng c h.ó", "ngu"]
(3): ["lon"]
(Eng: ["pussy "])
B Notices for annotators
Table 9: Additional notices for annotators.
Notices Example Explanation
Try to figure out and
consider as in the
original context of
the comments.This could help annotators under-
stand complex and non-context hate,
offensive comments.
Do not let emotion
affect the annotating
process.Annotators exposed in a long time
to toxic comments are reported to get
used to the frequently appearing hate,
offensiveness.
Check the provided
Vietnamese Dictio-
nary if there is any
uncertainty in being
sure a word is hate or
offensive.We use the most reputable Viet-
namese dictionary (Hoang Phe,
1983) to provide to annotators.
Should span the ob-
ject is compared to
in an inappropriate
comparison."Ăn cơm nhìn như chó"
(Eng: You eat like a dog )
Spans: ["nhìn như chó"]We consider this is an inappropriate
comparison in which annotators must
span "chó" dog.Table 9 continued from the previous page.
However, we should
span the whole com-
parison if spanning
only the object is
compared to might
not convey the com-
plete hate or offen-
sive idea"ý th/uni1EE9c như tr/uni1EBB l/uni1EDBp m/uni1EA7m"
Spans: ["ý th/uni1EE9c như tr/uni1EBB l/uni1EDBp m/uni1EA7m"]
(Eng: Your awareness is just like a
kindergarten kid )If we only span "tr/uni1EBB l/uni1EDBp m/uni1EA7m"
(kindergarten kid), it will not convey
the complete offensive idea. As that,
annotators are encouraged to span the
whole text instead.
Do not span conjunc-
tions; exceptional
Vietnamese cases;
standard ways to call
LGBTQ+.(1) Vì th/uni1EBF, nên, nhưng, mà, etc.
(Eng: so, so, but, but, etc. )
(2) Gay, les, etc
(3) Bóng, bê đê, ái nam ái n/uni1EEF, etc.(1) Some conjunctions in Viet-
namese.
(2) Appropriate ways to specify
LGBTQ+.
(3) Inappropriate ways to specify
LGBTQ+ that need to be spanned in
comments.
Blatant hate and
offensive words
prioritize being
spanned over the
others, especially in
sentences without
diacritics and could
not be understanded.May Cha H O an Tro. Cap. Ranh c.
Di ngoi le. Nhieu chuyen. Do Cai
Thu , do tam than
Spans: ["Cap", "ngoi le", "Nhieu
chuyen", "do tam than"]
(Eng: ["steal", "gossip ", "talkative ",
"psycho "])Similar to this non-diacritical and in-
comprehensible comment, words as
highlighted ones are more straight-
forward to be spanned.
Span the whole
phase violating
human rights."V/uni1EC1 nư/uni1EDBc anh nên vào tù /uni1EDF trư/uni1EDBc thay
vì đi cách ly."
(Eng: When you return to Vietnam,
you should go to jail first instead of
going to isolation )
Spans: ["V/uni1EC1 nư/uni1EDBc anh nên vào tù /uni1EDF
trư/uni1EDBc thay vì đi cách ly"]Comments violate human rights, usu-
ally complex to specify hate, offen-
sive words to span. As in this exam-
ple, a Vietnamese citizen comes back
from a foreign country has a right to
have isolated healthcare firstly.
Span the whole ob-
scene acronyms.(1) clgv
(Eng: wtf is that? )
(2) clmn
(Eng: your mom’s pussy )
(3) cmn
(Eng: your mother ).
Follow the rule with
words that do not
have diacritics and
conjoin to span out
the hate, offensive
spans.cailongithe
(Eng: wtf is that? )Some comments have strings being
constructed by many words missing
diacritical marks, but still able to un-
derstand. The annotators should only
span the hate and offensive characters
set out of the string as in the example.
Span hate, offensive-
ness separately."C/uni1EA3 đám dlv là nhóm ngu d/uni1ED1t, tr/uni1EBB trâu
potay vcl luôn."
(Eng: The whole dlv crew are stupid,
bull-headed kids (so I have) no fuck-
ing thing to say. )
Spans: ["đám", "ngu d/uni1ED1t", "tr/uni1EBB trâu"]
(Eng: ["crew", "stupid ", "bull-
headed kids "])This comment must be spanned as in
the example, but not as "đám," "ngu
d/uni1ED1t, tr/uni1EBB trâu," "vcl," "NGU."C Definition of Error Cases for Error
Analysis
We introduce 12 error definitions as follows:
1.Wrong spans prediction : The model pre-
dicts clean spans as hateful and offensive.
2.Wrong spans boundary : The model predicts
inadequate boundary or fails to predict cor-
rectly.
3.Allusion : The comment refers to another per-
son or subject in an indirect and disrespectful
way.
4.Annotation error : Annotators have improp-
erly annotated the span. The reason might be
that they somehow do not follow the provided
guidelines. However, there is no modification
in the final dataset.
5.Abbreviation : The comment contain short
forms of words.
6.Spelling mistake : The comment is spelling
mistake.
7.Needing domain knowledge : Dialect and
professional expertise are required to detect
span in comments.
8.Lack of context : In different contexts, the
comment could be understand in multiple
meaning.
9.Idiom or proverb : The comment contains
idiom or proverb.
10.Missing diacritical marks : Words in the
comment do not have diacritical marks.
11.Metaphor or metonymy : The comment con-
tains metaphor or metonymy.
12.Others : The comment contains rare charac-
ters, other languages, words in it are stick to-
gether, etc.