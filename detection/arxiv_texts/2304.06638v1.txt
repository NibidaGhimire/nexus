How Useful are Educational Questions Generated
by Large Language Models?
Sabina Elkins1;2, Ekaterina Kochmar2;3,
Iulian Serban2, and Jackie C.K. Cheung1;4
1McGill University & MILA (Quebec Artiﬁcial Intelligence Institute)
2Korbit Technologies Inc.
3MBZUAI
4Canada CIFAR AI Chair
Abstract. Controllable text generation (CTG) by large language mod-
els has a huge potential to transform education for teachers and students
alike. Speciﬁcally, high quality and diverse question generation can dra-
matically reduce the load on teachers and improve the quality of their
educational content. Recent work in this domain has made progress with
generation, but fails to show that real teachers judge the generated ques-
tionsassuﬃcientlyusefulfortheclassroomsetting;orifinsteadtheques-
tions have errors and/or pedagogically unhelpful content. We conduct a
human evaluation with teachers to assess the quality and usefulness of
outputs from combining CTG and question taxonomies (Bloom’s and a
diﬃculty taxonomy). The results demonstrate that the questions gener-
ated are high quality and suﬃciently useful, showing their promise for
widespread use in the classroom setting.
Keywords: ControllableTextGeneration ·PersonalizedLearning ·Prompt-
ing ·Question Generation
1 Introduction
The rapidly growing popularity of large language models (LLMs) has taken the
AI community and general public by storm. This attention can lead people to
believe LLMs are the right solution for every problem. In reality, the question of
the usefulness of LLMs and how to adapt them to real-life tasks is an open one.
Recent advancements in LLMs have raised questions about their impact on
education, including promising use cases [1,8,9,10]. A robust question generation
(QG) system has the potential to empower teachers by decreasing their cognitive
load while creating teaching material. It could allow them to easily generate per-
sonalized content to ﬁll the needs of diﬀerent students by adapting questions to
Bloom’s taxonomy levels (i.e., learning goals) or diﬃculty levels. Already, inter-
ested teachers report huge eﬃciency increases using LLMs to generate questions
[1,8]. These improvements hinge on the assumption that the candidates are high
quality and are actually judged to be useful by teachers generally. To the best
of our knowledge, there has yet to be a study assessing how a larger group of
teachers perceive a set of candidates from LLMs.5We investigate if LLMs can
5[10] show that subject matter experts can’t distinguish between machine and human
written questions, but state that a future direction is to assess CTG with teachers.arXiv:2304.06638v1  [cs.CL]  13 Apr 20232 S. Elkins et al.
generate diﬀerent types of questions from a given context that teachers think are
appropriate for use in the classroom. Our experiment shows this is the case, with
high quality and usefulness ratings across two domains and 9 question types.
2 Background Research
Auto-regressive LLMs are deep learning models trained on huge corpora of data.
Their training goal is to predict the next word in a sequence, given all of the
previous words [11]. An example of an auto-regressive LLM is the GPT family of
models, such as GPT-3. Recently, GPT-3 has been ﬁne-tuned with reinforcement
learning to create a powerful LLM called InstructGPT, which outperforms its
predecessors in the GPT family [6]. Using human-annotated data, the creators
of InstructGPT use supervised learning to train a reward model, which acts as
a reward signal to learn to choose preferred outputs from GPT-3.
An emerging paradigm for text generation is to prompt (or ‘ask’) LLMs for
a desired output [5]. This works by feeding an input prompt or ‘query’ (with
a series of examples for a one- or few-shot setting) to a LLM. This paradigm
has inspired a new research direction called prompt engineering . One of the
most common approaches to prompt engineering involves prepending a string to
the context given to a LLM for generation [4]. For controllable text generation
(CTG), such a preﬁx must contain a control element, such as a keyword that
will guide the generation [5].
Questions are one of the most basic methods used by teachers to educate. As
this learning method is so broad, it uses many organizational taxonomies which
takediﬀerentapproachestodividequestionsintogroups.Onepopularexampleis
Bloom’s taxonomy [3], which divides educational material into categories based
onstudent’slearninggoals.Anotherexampleisadiﬃculty-leveltaxonomy,which
usually divides questions into 3 categories of easy, medium, and hard [7]. By
combining CTG and these question taxonomies, we open doors for question
generationbypromptingLLMstomeetspeciﬁcationsoftheeducationaldomain.
3 Methodology
3.1 Controllable Generation Parameters
Parameter settings used in this paper were guided by preliminary experimen-
tation. Firstly, ‘long’ context passages (6-9 sentences) empirically appeared to
improve generation. Secondly, the few-shot setting outperformed the zero-shot
setting, with ﬁve-shot (i.e., with 5 context/related question type pairs included
in the prompt) performing best. Few-shot generation is where prompts con-
sist of an instruction (e.g., "Generate easy questions."), examples (e.g., set of
n context/easy question pairs), and the desired task (e.g., context to generate
from). Thirdly, there was not a large enough sample size to deﬁnitively say which
question taxonomies are superior to use as control elements for CTG. Two rep-
resentative taxonomies were chosen for the experiments in Section 3.2: Bloom’s
taxonomy [3] (which includes remembering ,understanding ,applying,analyzing ,How Useful are Educational Questions Generated by LLMs? 3
evaluating , and creating question types) and a diﬃculty-level taxonomy (which
includes beginner,intermediate , and advanced question types) [7]. These tax-
onomies approach the organization of questions in diﬀerent ways, by the learn-
ing goal and by complexity respectively. This creates an interesting comparison
among the taxonomic categories to help explore the limits of the CTG approach.
3.2 Teacher Assessment Experiment
Question Generation The human assessment experiment was conducted with
candidates generated in the machine learning (ML) and biology (BIO) domains.
There are 68 ‘long’ context passages (6-9 sentences) pulled from Wikipedia (31
are ML, 37 are BIO). Using hand-crafted examples for 5-shot prompting, In-
structGPT was prompted to generate 612 candidate questions.6Each passage
has 9 candidates, one with each taxonomic category as the control element.
Annotators There are two cohorts of annotators, BIO and ML. The 11 BIO
annotators have biology teaching experience at least at a high school level, and
were recruited on the freelance platform Up Work. The 8 ML annotators have
CS, ML, AI, math or statistics teaching experience at a university level, and were
recruited through word of mouth at McGill and Mila. All of the annotators are
proﬁcient English speakers and are from diverse demographics. Their teaching
experience ranges from 1-on-1 tutoring to hosting lectures at a university. The
experiments are identical for both cohorts. As such, the experiment is explained
in a domain-agnostic manner. The results will be presented separately, as the
goal of this work is not to show identical trends between the two domains, but
that CTG is appropriate for education in general.
Metrics Each annotator was trained to assess the generated candidates on two
of four quality metrics, as well as a usefulness metric. This division was done to
reduce the cognitive load on an individual annotator. The quality metrics are:
relevance (binary variable representing if the question is related to the context),
adherence (binary variable representing if the question is an instance of the de-
sired question taxonomy level); and grammar (binary variable representing if the
question is grammatically correct), answerability (binary variable representing
if there is a text span from the context that is an answer/leads to one). The
relevance ,grammar ,7answerability , and adherence metrics are binary as they
are objective measures, often seen in QG literature to assess typical failures of
LLMs such as hallucinations or malformed outputs [5]. The subjective metric
assessed, the usefulness metric, is rated on a scale because it is more nuanced.
This is deﬁned by a teacher’s answer to the question: “Assume you wanted to
teach about context X. Do you think candidate Y would be useful in a lesson,
homework, quiz, etc.?” This ordinal metric has the following four categories: not
useful,useful with major edits (taking more than a minute), useful with minor
edits(taking less than a minute), and useful with no edits . If a teacher rates a
6The passages, few-shot examples, prompt format, taxonomic level deﬁnitions, anno-
tator demographics and raw results are available: https://tinyurl.com/y2hy8m4p .
7Despite not being a teacher’s opinion, this is evaluated because we want to know the
model’s success here without relying on automatic assessment.4 S. Elkins et al.
question as not useful oruseful with major edits we also ask them to select from
a list of reasons why (or write their own).
Reducing Bias We ﬁrst conducted a pilot study to ensure the metrics and anno-
tator training were unambiguous. We randomized the order of candidates pre-
sented and asked annotators to rate one metric at a time to avoid conﬂation. We
included unmarked questions in order to ascertain if the annotators were paying
attention. These questions were obviously wrong (e.g., a random question from
a diﬀerent context, a candidate with injected grammatical errors). Any annota-
tors who did not agree on a minimum of 80%of these ‘distractor’ questions were
excluded. The annotators’ performance on these is discussed in Section 4.2.
4 Results and Analysis
4.1 Generation Overlap
Weobservedoverlapswithinthegeneratedcandidatesforthisexperiment.Specif-
ically, despite having diﬀerent control elements, sometimes the LLM generates
the same question for a given context passage twice. As a result, out of 612can-
didates,there are 540uniqueones ( 88:24%areunique). We believethis overlapis
lowenoughsothegeneratedcandidatesarestillsuﬃcientlydiverseforateacher’s
needs. It is important to keep in mind that this overlap is not reﬂected in the
following results, as teachers were asked to rank every candidate independently.
Future work by the authors will remove this independence assumption.
4.2 Annotator Agreement
All of the participants annotated candidates from 6 context passages. In order
to assess their agreement on the task, they annotated a 7thpassage that was the
same for all annotators in a given domain cohort. The results for each metric
are reported in Table 1. In both domains, relevance ,grammar , and answerability
have between 85%and100%observed agreement. The adherence metric has
lower agreement, between 60%and80%. Since this metric is more complex
than the others and captures the annotators’ interpretations of the question
taxonomies,weconsiderthismoderateagreementtobeacceptableandexpected.
Unlike the binary metrics, all candidates were rated on usefulness by two
annotators. As before, only one context passage, the agreement on which is
presented in Table 1, was seen by all annotators. Section 4.4 discusses the ag-
gregation of the usefulness scores on the rest of the data. In both cohorts, the
observed agreement on usefulness is around 63%. This metric is deﬁned accord-
ing to a teacher’s opinion, and as such is subjective. Thus, the lower agreement
between annotators is to be expected. Using Cohen’s to measure the agree-
ment yields a = 0:537for the ML cohort and a = 0:611for the BIO cohort,
which implies moderate and substantial agreement respectively [2]. Additionally,
the agreement of the annotators on the included ‘distractor’ candidates for this
metric (see Section 3.2) is = 1(i.e., perfect agreement), which shows that
the annotators agree on the fundamental task but might ﬁnd diﬀerent questions
useful for their particular approach to teaching.How Useful are Educational Questions Generated by LLMs? 5
Table 1: The quality metrics’ mean ( ), standard deviation ( ), and observed
agreement (i.e., % of the time the annotators chose the same label).
Metric (ML)Agreement % (ML) (BIO)Agreement % (BIO)
Relevance 0.9670.180 100 0.9720.165 100
Grammar 0.9570.204 92.6 0.9700.170 100
Adherence 0.6740.470 62.2 0.6910.463 79.9
Answerability 0.9140.282 94.4 0.9300.256 86.7
Usefulness 3.5090.670 62.7 3.5930.682 62.8
4.3 Quality Metrics
Three quality metrics, relevance ,grammar , and answerability , are consistently
high for all generated candidates (see in Table 1). The fourth quality metric,
adherence , varies across the taxonomic categories as seen in Figure 1a. This
variation is similar within the two domains. As might be expected, the more
objectivecategoriesareeasierfortheLLMtogenerate.Forinstance,lookingonly
at the ‘remembering’ category has an adherence of83:3%for the ML cohort and
91:7%for the BIO cohort. This category is intended to ask for a student to recall
a fact or deﬁnition. This might be simple for the LLM to replicate by identifying
a relevant text span, and reﬂects the traditional QG task. By contrast, asking
a LLM to generate a ‘creating’ question is a more open-ended problem, where
a text span from the context may not be the answer. Accordingly, the model
struggles on this less constrained task, and has an adherence of only 40:0%for
the ML cohort and 36:1%for the BIO cohort.
4.4 Usefulness Metric
Theusefulness metric’s ordinal categories (see Section 3.2) are mapped from 1
(not useful ) to4(useful with no edits ). The average usefulness for all candidates
is 3.509 for the ML cohort and 3.593 for the BIO cohort. Note that an individual
candidate’s usefulness is already the average score between two annotator’s rat-
ings, and the whole average usefulness is the average across all candidates. This
is a highly promising result showing that on average teachers ﬁnd that these
generated candidates will be useful in a classroom setting.
(a)Adherence by taxonomic category.
 (b) Avg. usefulness by taxonomic category.
Fig.1: Visualizations of the usefulness andadherence metrics.6 S. Elkins et al.
There is no signiﬁcant diﬀerence between the usefulness scores of any of the
question taxonomy categories, though some variation is present (see Figure 1b).
On average, each of the question taxonomies are rated between useful with mi-
nor edits anduseful with no edits (i.e., [3;4]). Considering the adherence that
diﬀers across categories, it is also important to note that a question which does
not adhere to its question taxonomy can still be useful in a diﬀerent way than
intended. 56:8%of the time the reason cited for ‘not useful’ candidates is related
to their grammar or phrasing. This can possibly be ﬁxed by a ﬁlter that removes
malformed questions, but it will lower the available diversity of questions.
Conclusion This work takes steps to demonstrate the realistic usefulness of
applying CTG to generate educational questions. The results show that CTG
is a highly promising method that teachers ﬁnd useful in a classroom setting.
We do not include baselines because the goal is not to show these questions are
better than others, only to show they are of high enough quality. Limitations
include the single LLM considered, the independence assumption seen in Section
4.1,andthelackofcomparisonbetweenhumanandmachine-authoredquestions.
The authors plan to explore these avenues in future work. Applying generated
candidates to form real-world lessons and evaluate their impact will demonstrate
their ultimate value. CTG could pave the way for a new approach to education
and transform the experiences of millions of teachers and students.
Acknowledgements We’d like to thank Mitacs for their grant for this project,
and CIFAR for their continued support. We are grateful to both the annotators
for their time and the anonymous reviewers for their valuable feedback.
References
1. Baidoo-Anu, D., & Owusu Ansah, L. (2023). Education in the Era of Generative Artiﬁcial In-
telligence (AI): Understanding the Potential Beneﬁts of ChatGPT in Promoting Teaching and
Learning. Available at SSRN 4337484.
2. Landis, J. R., & Koch, G. G. (1977). The measurement of observer agreement for categorical
data. Biometrics, 159-174.
3. Krathwohl, D. R. (2002). A revision of Bloom’s taxonomy: An overview. Theory into practice,
41(4), 212-218.
4. Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., & Neubig, G. (2023). Pre-train, prompt, and
predict: A systematic survey of prompting methods in natural language processing. ACM Com-
puting Surveys, 55(9), 1-35.
5. Mulla, N., & Gharpure, P. (2023). Automatic question generation: a review of methodologies,
datasets, evaluation metrics, and applications. Progress in Artiﬁcial Intelligence, 1-32.
6. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., ... & Lowe, R.
(2022). Training language models to follow instructions with human feedback. arXiv preprint
arXiv:2203.02155.
7. Pérez, E. V., Santos, L. M. R., Pérez, M. J. V., de Castro Fernández, J. P., & Martín, R. G.
(2012). Automatic classiﬁcation of question diﬃculty level: Teachers’ estimation vs. students’
perception. In 2012 Frontiers in Education Conference Proceedings (pp. 1-5). IEEE.
8. Terwiesch, C. (2023). Would Chat GPT Get a Wharton MBA? A Prediction Based on Its Per-
formance in the Operations Management Course. Mack Institute for Innovation Management at
the Wharton School, University of Pennsylvania.
9. Wang, X., Fan, S., Houghton, J., & Wang, L. (2022). Towards Process-Oriented, Modular, and
Versatile Question Generation that Meets Educational Needs. arXiv preprint arXiv:2205.00355.
10. Wang, Z., Valdez, J., Basu Mallick, D., & Baraniuk, R. G. (2022). Towards Human-Like Educa-
tional Question Generation with Large Language Models. In Artiﬁcial Intelligence in Education:
23rdInternationalConference,AIED2022,Durham,UK,2022,Proceedings,PartI(pp.153-166).
Cham: Springer International Publishing.
11. Zhang, H., Song, H., Li, S., Zhou, M., & Song, D. (2022). A survey of controllable text generation
using transformer-based pre-trained language models. arXiv preprint arXiv:2201.05337.