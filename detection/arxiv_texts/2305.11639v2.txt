arXiv:2305.11639v2  [cs.DS]  23 May 2023Distributed MIS with Low Energy and Time Complexities
Mohsen Ghaﬀari
MIT
ghaﬀari@mitJulian Portmann
ETH Zurich
pjulian@inf.ethz.ch
Abstract
We present randomized distributed algorithms for the maximal indep endent set problem
(MIS) that, while keeping the time complexity nearly matching the bes t known, reduce the
energy complexity substantially. These algorithms work in the stand ard CONGEST model of
distributed message passing with O(logn) bit messages. The time complexity measures the
number of rounds in the algorithm. The energy complexity measures the number of rounds each
node is awake; during other rounds, the node sleeps and cannot pe rform any computation or
communications.
Our ﬁrst algorithm has an energy complexity of O(log logn) and a time complexity of
O(log2n). Our second algorithm is faster but slightly less energy-eﬃcient: it achieves an en-
ergy complexity of O(log2logn) and a time complexity of O(logn·log logn·log∗n). Thus, this
algorithm nearly matches the O(logn) time complexity of the state-of-the-art MIS algorithms
while signiﬁcantly reducing their energy complexity from O(logn) toO(log2logn).
1 Introduction and Related Work
We present randomized distributed algorithms for the maxim al independent set (MIS) problem
that perform well in both time and energy complexity measure s. The MIS problem has been one
of the central problems in the study of distributed graph alg orithms. Much of the research on this
problem has focused on the time complexity measure, i.e., th e maximum time each node spends
running the distributed algorithm. The state-of-the-art t ime complexity is O(logn) [Lub86 ,ABI86 ];
this has remained the best-known bound for general graphs fo r nearly four decades.
Another important measure, which is vital in applications s uch as sensor and wireless networks,
is the amount of energy spent by each node. Roughly speaking, the energy consumed by a node can
be approximated as being proportional to the time that the no de is active in the distributed algo-
rithm and performs computations or communications. In cont rast to the time complexity measure,
which has been widely studied, we know considerably less abo ut the energy complexity measure.
Standard algorithms have their energy complexity equal to t he time complexity because usually it
is unclear how to let each node be inactive during a substanti al part of the algorithm while retaining
the algorithm’s correctness and time complexity. In this pa per, we show MIS algorithms whose time
complexity nearly matches the best-known bound while achie ving a substantially reduced energy
complexity. We next state our formal results, after reviewi ng the model and the precise complexity
measures, and the state of the art.
1.1 Setup
Distributed Model. We work with the standard synchronous model of distributed m essage
passing, often referred to as the CONGEST model [ Pel00 ]. The network is abstracted as an n-node
1undirected graph G= (V,E), where each node represents one computer. These nodes oper ate in
synchronous rounds. Per round, each node performs some comp utations on the data that it holds,
and then it can send one message to each of its neighbors in G. The standard assumption is that
each message has size B=O(logn), which is suﬃcient to describe constant many nodes or edges
and values polynomially bounded in n. The variant where messages are allowed to be unbounded
is often called the LOCAL model [ Lin92 ]. The messages are delivered by the end of the round.
Then, the algorithm proceeds to the next round. In formulati ng distributed graph problems, the
assumption is that the nodes do not know the global topology o fG. Initially, each node knows
only its own neighbors, perhaps along with some bounds on glo bal parameters, e.g., a polynomial
bound on n. At the end of the algorithm, each node should know its part of the output, e.g., in the
problem of computing an MIS, each node should know whether it is in the computed set or not.
Time and Energy Complexities. The primary measure of interest has been the time complexity ,
sometimes also called round complexity. This is the maximum number of rounds each node runs
the algorithm, i.e., the time from the start of the algorithm until the node terminates. The energy
complexity is a sharper version that measures only the rounds in which a n ode is actually active and
does some computations or communications. Let us say per rou nd each node is either active/awake
orsleeping . During a round in which the node sleeps, it cannot perform an y computation, and it
does not send or receive any messages. Further, it also canno t be woken up by any other node, it
only wakes up after a scheduled amount of time. The energy complexity of the algorithm is deﬁned
as the maximum number of rounds each node is active/awake. Cl early, the energy complexity is
upper bounded by the time complexity. We believe it is intere sting—from both the perspective of
applications and the view of technical algorithm-design ch allenges—to devise algorithms that have
a much better energy complexity, ideally without a signiﬁca nt sacriﬁce in the time complexity.
The energy complexity measure—considering awake or sleepi ng rounds and measuring the num-
ber of awake rounds—has been studied for over two decades. Se e, e.g., [ JKZ02a ,JKZ02b ,KKP13 ,
CKP+17,CDH+18,CDHP20 ,CGP20 ,BM21a ,AMP22 ,GP22 ,DMP23 ]. Much of this was in the con-
text of the radio network model1. More recent work investigates this measure in the context o f
the more basic synchronous message-passing models of distr ibuted graph algorithms, namely the
CONGEST andLOCAL models [ CGP20 ,BM21a ,AMP22 ,GP22 ,DMP23 ].
We remark that sometimes the word awake-complexity is used instead of energy-complexity.
Moreover, sometimes this variant of the CONGEST model that considers sleeping nodes is called
the sleeping model [CGP20 ]. In this article, we decided to refrain from the latter term inology as
we think sleeping is a separate aspect and it can be considere d for any of the distributed models,
as it has been done for the radio network model, the CONGEST model, the LOCAL model, etc.
1.2 Prior Work
Let us start with time complexity alone. Luby’s O(logn) round randomized MIS algorithm [ Lub86 ,
ABI86 ] remains state of the art in time complexity. Much research h as been done on time com-
plexity, and there are faster algorithms for special graph f amilies; see e.g., [ BEPS16 ,Gha16 ,GS19 ].
The best known lower bound is Ω(/radicalbig
logn/log logn) [KMW16 ]. For the restriction to deterministic
algorithms, a best lower bound of Ω(log n/log logn) is known [ BBH+21], and there are determinis-
tic algorithms within a polynomial of this, i.e., with round complexity poly(log n) [RG20 ,GGR21 ,
FGG+23]. All of these have a high energy complexity, and in particul ar, Luby’s O(logn)-round
1In the radio network model, per round, each node is transmitt ing or listening, and a listening node receives a
message only if there is exactly one transmitting neighbor. When adding the energy complexity consideration, we
again have awake or sleeping rounds, and a node can be transmi tting or listening only in awake rounds.
2algorithm also has energy complexity O(logn). Ideally, we would like the energy complexity to be
much lower.
We next discuss prior results that studied MIS from both ener gy and time views. Some previ-
ously known algorithms can provide bounds on the average energy spent per node, i.e., the average
over all nodes of the summation of their awake rounds. This is instead of the energy complexity
deﬁned above that measures the maximum energy spent by each n ode, i.e., the maximum over all
nodes of the number of rounds in which this node is awake. Chat terjee, Gmyr, and Panduran-
gan [ CGP20 ] presented an MIS algorithm that achieves an O(1) average energy and O(log3.41n)
time complexity. Ghaﬀari and Portmann [ GP22 ] improved this to an algorithm that achieves an
O(1) average energy with a time complexity of O(logn), which matches the state-of-the-art time
complexity of MIS. However, these algorithms still have Ω(l ogn) energy complexity; that is, some
nodes have to be awake and active during at least Θ(log n) rounds.
In a recent work, Dufoulon, Moses Jr., and Pandurangan [ DMP22 ] presented algorithms that
achieve a sublogarithmic energy complexity. Their ﬁrst alg orithm reaches an energy complexity
ofO(log logn), but at the expense of having a time complexity of poly( n). Note that this time
complexity is exponentially higher than the state-of-the- artO(logn) complexity of MIS. Their
second algorithm achieves an energy complexity of O(log logn·log∗n) and a time complexity of
˜O(log3n). However, it should be noted that this algorithm needs mess ages of size at least Ω(log3n),
as it performs some non-trivial topology gathering. If one l imits the algorithm to the more standard
O(logn) message size of the CONGEST model, then the algorithm’s energy-complexity blows up to
˜Ω(log2n), because some nodes need to send O(log3n) bits of information. This energy complexity
would make the result uninteresting because Luby’s classic algorithm [ Lub86 ] has better time and
energy complexities. Concurrent to our work, Dufoulon, Mos es Jr., and Pandurangan [ DMP23 ]
have also improved the message size of their algorithms to O(logn), while maintaining the same
round and energy complexities. This appears in the conferen ce version [ DMP23 ], as well as in an
updated version of their preprint [ DMP22 ]. We also comment that Hourani et al. [ HPR22 ] gave
some related results for MIS in the very special case of rando m graphs embedded in low-dimensional
geometric spaces, but their results are subsumed by those of [DMP23 ].
1.3 Our Results
We are interested in algorithms that are (almost) as fast as t he state-of-the-art but use substan-
tially less energy. That is, algorithms that achieve good en ergy complexity without (considerably)
sacriﬁcing the more standard measure of time complexity. We remark that this is technically a far
more challenging problem than if we give the algorithm much m ore time. The algorithms we desire
have to achieve time complexity almost equal to the best know n, while each node sleeps almost all
the time, except for an exponentially tiny fraction of the ro unds.
Our ﬁrst algorithm obtains the best known O(log logn) bound of the energy complexity while
running in O(log2n) rounds.
Theorem 1.1. There is a randomized distributed MIS algorithm with time comp lexityO(log2n)
and energy complexity O(log logn). That is, the algorithm runs for O(log2n)rounds, each node is
awake in only O(log logn)rounds, and the algorithm computes an independent set of ver tices such
that, with high probability, this set is maximal.
Our second algorithm has time complexity almost matching th e long-standing O(logn)
bound [ Lub86 ], while achieving an energy complexity of O(log2logn).
Theorem 1.2. There is a randomized distributed MIS algorithm with time comp lexityO(logn·
log logn·log∗n)and energy complexity O(log2logn). That is, the algorithm runs for O(logn·
3log logn·log∗n)rounds, each node is awake in only O(log2logn)rounds, and the algorithm computes
an independent set of vertices such that, with high probabil ity, this set is maximal.
As an additional point, we also show that—with the addition o f one suitable algorithmic
module—these algorithms can ensure that the average energy spent over all nodes remains O(1),
while keeping the time and energy complexities unchanged. H ence, these augmented algorithms
simultaneously match the O(1) average-energy bounds of the algorithms of [ GP22 ,CGP20 ]; recall
that the latter do not provide sublogarithmic (worst-case) energy complexity. This extension to
constant node-averaged energy appears in Section 4 .
2 Algorithm 1
In this section, we present our ﬁrst algorithm, which has tim e complexity O(log2n) and energy
complexity O(log logn). The algorithm is split into three phases: The ﬁrst phase wi ll reduce the
maximum degree to poly(log n), the second phase will break the graph into small component s, also
known as shattering , and the third phase will be a clean-up that deals with the rem aining small
components. Finally, we will put everything together and pr oveTheorem 1.1 .
2.1 Phase I
In this ﬁrst phase, our goal is to compute an independent set o f nodes such that, after removing
these nodes and their neighbors from the graph, the remainin g graph has maximum degree at most
O(log2n). This is formalized in the following lemma.
Lemma 2.1. There is an algorithm that, given a graph of maximum degree ∆, computes an inde-
pendent set Sof nodes with the following properties: removing all nodes o fStogether with their
neighbors from the graph results in a graph with maximum degr eeO(log2n). Furthermore, the algo-
rithm runs in O(log ∆·logn)rounds of the CONGEST model, and each node is awake for at most
O(log logn)rounds. The algorithm succeeds with probability at least 1−n−10.
We will ﬁrst describe the algorithm as if all nodes were alway s active. Afterward, we discuss
how our modiﬁcations allow the algorithm to be executed with low energy complexity.
Regularized Luby. To achieve this goal, we will use a slowed-down variant of Lub y’s algorithm
[Lub86 ], sometimes also called regularized Luby. In its basic form , the algorithm works as follows:
In the ﬁrst iteration, each node is marked with probability1
10∆per round. Then, each round
consists of two sub-rounds, which each correspond to one rou nd of communication in the network:
In the ﬁrst sub-round, each marked node vinforms all its neighbors that it is marked, and learns
if any of its neighbors are marked. In the second sub-round, i fvis marked and has no marked
neighbors, vjoins the MIS, removes itself from the graph, and informs all its neighbors that they
will be removed from the graph. This is repeated for c·lognrounds. We can argue that afterward,
no node of degree more than∆
2remains with high probability. More generally, in iteratio ni, nodes
are marked with probability2i
10∆forc·lognrounds. We can show that at the end of iteration i,
the maximum degree drops to∆
2iwith high probability. Finally, after O(log ∆) iterations, all nodes
that are isolated are added to the MIS.
Modiﬁcations. We will modify the algorithm as follows: The ﬁrst modiﬁcatio n is that we let
nodes be marked at most once. That is, after the ﬁrst time a nod e is marked, if it did not join
the MIS, this node is considered spoiled and will remain spoiled until the end of this ﬁrst phase
of the entire algorithm. The second modiﬁcation is that we wi ll not perform log ∆ iterations but
4only log ∆ −2 log lognmany. Both these modiﬁcations also aﬀect which guarantees we expect from
the algorithm: In the end, we will show that the graph of all no des that were not removed due to
themselves or their neighbor joining the MIS has degree at mo stO(log2n).
Analysis. First, recall that we call a node that was marked but did not su bsequently join the
MISspoiled for the remaining part of the algorithm. We call a node active if neither itself nor any
of its neighbors were added to the MIS (yet), and inactive otherwise. Note that nodes that are
spoiled can also become inactive and active nodes can be spoi led. We will show that the following
invariants are maintained w.h.p.: At the end of iteration i, we have that
•A(i): for any active node, at most ( i+ 1)·Clognneighbors of it are active and spoiled, and
•B(i): each active node has at most∆
2(i+1)neighbors that are active and not spoiled.
We will prove these two invariants separately and both by ind uction. At the beginning of the algo-
rithm, these invariants are trivially satisﬁed. For simpli city, we consider these to be the statements
A(−1) andB(−1). Thus, we focus our attention on the case of i≥0.
Lemma 2.2. For any i≥0, we have that B(i−1)impliesB(i)with probability 1−n−11.
Proof. Let us focus on one round in iteration i, and one active node vwhich has more than∆
2(i+1)
active and non-spoiled neighbors. We will show that either ( a) this number of neighbors drops
below the threshold, or (b) node vbecomes inactive with high probability.
First, note that the number of active and non-spoiled neighb ors can only decrease, by nodes
becoming spoiled or inactive due to being removed by their ne ighbors. By B(i−1) we know that
v, and any other active node, has at most∆
2iactive and non-spoiled neighbors throughout iteration
i. Thus, for a given neighbor u, the probability that uis the only marked neighbor in both its own
andv’s neighborhood is at least
2i
∆·/parenleftbigg
1−2i
∆/parenrightbigg2∆
2i
≥2i
∆·4−2/10≥2i
20∆.
Since these events are independent for all neighbors of v, the probability that at least one of them
occurs is at least 1 /40. This is also a lower bound on the probability that vis removed: the reason
is that if any of its neighbors is the only marked node in its ne ighborhood, it will join the MIS and
removevfrom the graph. Thus, vis removed with a probability of at least 1 /40.
Assuming that the number of active and non-spoiled neighbor s never drops below∆
2i, this is true
for each round. Since rounds within one iteration are indepe ndent, the probability that vis still
active and has more than∆
2(i+1)neighbors after clognrounds is at most 1 −n−12forcsuﬃciently
large. By a union bound over all nodes, we have that B(i) thus holds with probability 1 −n−11.
Lemma 2.3. For anyi≥0, we have that A(i−1)andB(i−1)implyA(i)with probability 1−n−11.
Proof. We will focus on one node vand union bound over all nodes in the last step. Since we know
thatA(i−1) is true, we just need to show that in iteration i, at most Clognactive and non-spoiled
neighbors of vbecome spoiled. In fact, we will show that at most as many neig hbors are marked,
which are the only nodes that can become spoiled. From B(i−1), we get that vhas at most∆
2i
active and non-spoiled neighbors at the beginning of iterat ioni. For each of these neighbors u, let
Xube the event that uis ever marked in iteration i. We have that Pr[ Xu= 1]≤clogn2i
∆. Let
X=/summationtext
u∈N(v)Xube the number of neighbors marked in iteration i. We have that E[X]≤clogn.
5Since all Xuare independent, we can use Chernoﬀ’s Bound to obtain:
Pr[X≥Clogn]≤Pr/bracketleftbigg
X−E[X]≥(C−c) logn
E[X]E[X]/bracketrightbigg
≤exp
−(C−c)2log2n
E[X]2E[X]
2 +(C−c)logn
E[X]
≤exp/parenleftbigg
−(C−c)2log2n
(C+c) logn/parenrightbigg
≤n−12
Where we assumed that C≥100cin the last inequality. By a union bound over all nodes, A(i)
holds with probability 1 −n−11.
Having proven both those induction steps, we can put them tog ether to obtain the following:
Lemma 2.4. AfterT= log ∆ −log logniterations, each active node has at most O(logn)ac-
tive neighbors that are not spoiled, and each active node has at mostO(log2n)active and spoiled
neighbors, with probability 1−n−10.
Proof. Let us call an iteration isuccessful if both Lemma 2.2 andLemma 2.3 hold. This occurs
with probability at least 1 −2·n−11. By a union bound, the probability that all Titerations are
successful is at least 1 −n−10. Thus, with the same probability, we have that A(T) andB(T) hold.
FromB(T) we get that every node has at most∆
2log∆−loglogn+1=O(logn) active and non-spoiled
neighbors that remain, proving the ﬁrst part of the statemen t. The second part follows directly
fromA(log ∆), by observing that T≤logn.
Thus, our algorithm achieves the desired guarantees for the maximum degree in the remaining
graph. What remains is to show that it can be implemented with low worst-case energy complexity.
Low Energy Complexity. We now describe how the algorithm can be implemented with jus t
O(log logn) worst-case energy complexity. Observe, that since each no de is marked at most once,
there is also at most one round where a node can join the indepe ndent set. Further, since all
marking probabilities are ﬁxed at the beginning of the algor ithm, each node vcan ﬁnd its round rv
in which it is sampled before the algorithm even starts, or ob serve that it is never sampled. So we
have a setting where each node has (at most) one round rvassigned to it. Ideally, we would want
vto only be awake in round rv, however, it needs some additional information: vneeds to know if
any of its neighbors joined the independent set in a previous round, meaning that vis inactive in
rv.
We can do so by ﬁrst adding a (third) sub-round to each round, w hich will be used to exchange
this information. Thus, we want for each node a set of rounds Svin which it should be awake, such
that for any neighbor uwithru≤rv, there is a round k∈Sv∩Suwithru≤k≤rv, in which both
vanduare active, and vcan thus learn if ujoined the independent set in this new sub-round. In
the ﬁrst two sub-rounds of round k, only nodes wwithrw=kare awake; all other nodes are only
awake for the last sub-round. Further, we want each set Svto be as small as possible, since the
maximum size of them will correspond to the worst-case energ y complexity of our algorithm. And
ﬁnally, instead of assigning a set Svto each node v, we can ﬁnd a set Skfor each round k, and
nodevwill be assigned the set Skwithrv=k.
The problem is now formalized in the following lemma. Simila r results have been proven in
[BM21a ,AMP22 ], and such schedules have also been referred to as a “virtual binary tree”. For this
work to be self-contained, we reprove the statement diﬀerent ly, with a concise recursion.
Lemma 2.5. LetTbe the total number of rounds, numbered 1,...,T. For each round k, we can
construct a set of rounds Skwith|Sk|=O(logT), such that for any two rounds i≤jthere is a
roundlwithi≤l≤jandl∈Si∩Sj.
6Proof. We will use a divide-and-conquer approach to construct the s etsSk. To generalize the
description, say our rounds are numbered L,...,H . Initially, we have L= 1 and H=T. Let
M=L+⌊(H−L)/2⌋. In the ﬁrst step, our goal is to allow communication between all rounds
i≤Mandj≥M+ 1. We can do so by having everyone be awake in round M, i.e., adding M
to each set Sk. This takes care of all such pairs iandj. Thus, all pairs of rounds i≤jthat do
not satisfy the lemma statement yet must have either i≤j≤MorM+ 1≤i≤j. So we can
do the same procedure on each of those two remaining parts. Fo r the ﬁrst part we set L1=Land
H1=M, and for the second part we set L2=M+ 1 and H2=H. This process terminates after
⌈logn⌉levels of recursion, and in each level, we add one element to t he setSkfor each round k.
Thus, we have |Sk| ≤ ⌈logn⌉.
Wrap-Up. Now we ﬁnally have all the necessary parts to prove the main le mma of this subsection.
Proof of Lemma 2.1 .First, we will review how the algorithm is executed in a distr ibuted fashion.
Initially, each node vperforms the sampling and ﬁnds a round rvin which it was sampled, or it
does not sample itself in any round. In the latter case, the no de will stay asleep for the entire
algorithm.
All nodes vthat have found a round rvwill use Lemma 2.5 withT=clogn·(log ∆−log logn)
to compute a set Srvfor all rounds they will be awake. Let i∈Srvbe a round in which vis
awake. If i < rv, thenvwill only be awake in the third sub-round and listen if any of i ts neighbors
already joined the MIS in a previous round. For the case i=rv, it will be awake in all three
sub-rounds. In the ﬁrst two, it will execute the algorithm to gether with all other neighbors that
were also sampled in this round, and then in the third round, i t will inform its neighbors if vwas
added to the independent set. Finally, if i > rv, thenvwill only be awake in the third sub-round
again and inform its neighbors if it joined the independent s et in round rv.
It remains to argue that this algorithm achieves all the clai med guarantees. From Lemma 2.4 ,
we know that each node has at most O(logn)+O(log2n) =O(log2n) active neighbors that remain.
We have T=clogn·(log ∆−log logn) =O(log2n) total rounds, and from Lemma 2.5 we know that
the worst-case energy complexity is O(logT) =O(log logn). Finally, the algorithm works in the
CONGEST model since all messages that are exchanged are only about wh ether or not nodes were
marked or included in the independent set, which can be sent u sing just single-bit messages.
2.2 Phase II
From the previous phase, we know that we now have the followin g setting: A set of nodes that is
inactive, meaning either the node or one of its neighbors is i n the independent set, and a set of
remaining nodes that are still active, which induce a graph w ith maximum degree ∆ 2=O(log2n).
The inactive nodes will be ignored from now on (since they alr eady satisfy the condition for an
MIS). We focus on the graph induced by the active nodes.
The goal of this phase is to remove an additional set of nodes t hat form an independent set,
whose removal, together with their neighbors, leaves only c onnected components of size poly(log n).
Further, each of these components only has diameter O(logn). This has already been obtained in
previous work by Ghaﬀari [ Gha16 ,Gha19 ]. We restate the results we will use. Note that these
results assume that nodes are awake in all rounds. This will n ot negatively impact our running
time since for graphs of maximum degree ∆ 2=O(log2n), the time complexity is only O(log logn),
and we can indeed keep all nodes awake during this part of the a lgorithm.
Lemma 2.6 (Ghaﬀari [ Gha16 ,Gha19 ]).There is an algorithm that, given a graph of maximum
degree ∆ = poly(log n), computes an independent set Sof nodes, and a clustering of all nodes that
7are not in or adjacent to Sinto clusters of diameter O(log logn). Each connected component of
the graph that remains after the removal of Sand all its neighbors contains at most poly(logn)
nodes and O(logn/log logn)clusters. Furthermore, the algorithm runs in O(log ∆)rounds of the
CONGEST model and succeeds with probability at least 1−n−10.
2.3 Phase III
Lastly, we have to deal with the remaining small components. So we describe what we do for one
component, but all components can be handled simultaneousl y. Recall from the previous section
that each node is part of a cluster of diameter at most O(log logn) and each connected component
contains at most O(logn/log logn) clusters. Our goal will be to merge all these clusters into
one cluster per component. If our message size would be unlim ited, this would already solve our
problem since we could just exchange the topology of the enti re component, and have each node
run a sequential algorithm to obtain the result. Since we wan t our algorithms to work in the
CONGEST model, we will have to use an additional step, also used in [ Gha19 ], to ﬁnally compute
a maximal independent step. That is, we run several randomiz ed algorithms, each with an overall
small success probability in parallel, and use the fact that clusters have low diameter to ﬁnd a
successful execution, which exists with high probability. In particular, we will show the following:
Lemma 2.7. There is an algorithm that, given a graph of at most poly(logn)nodes clustered
intoO(logn/log logn)clusters, each with diameter O(log logn), ﬁnds an independent set, which
is maximal with probability 1−n−10. The algorithm runs in O(logn·log2logn)rounds of the
CONGEST model, and each node is awake for at most O(log logn)rounds.
The main ingredient is an algorithm merging all clusters int o one cluster per component, which
is also associated with a spanning tree of diameter O(logn). This will allow us to perform broadcast
and convergecast on this spanning tree with O(1) awake time per node in O(logn) rounds. The
problem of merging clusters in an energy-eﬃcient manner has been studied before, e.g., for minimum
spanning tree algorithms. Prior work [ BM21a ,AMP22 ,DMP23 ] obtained various trade-oﬀs between
energy and time complexity for both deterministic and rando mized algorithms. All these algorithms
follow the outline of Bor˚ uvka’s algorithm [ Bor26 ] for the minimum spanning tree problem, which
was ﬁrst used in the distributed setting by Gallager, Humble t, and Spira [ GHS83 ].
Compared to these previous works, we are interested in a diﬀer ent trade-oﬀ between time and
energy complexity, and simply changing their parameters is not suﬃcient for us. Further, we
remark that while there are randomized algorithms achievin g our desired (and better) trade-oﬀs,
these would not work in our setting. Since the graphs only hav e size poly(log n), the success
probability of standard randomized algorithms is only 1 −1/poly(logn). Thus, our algorithm will
need to be deterministic. We will ﬁrst state our desired guar antees, then give a brief overview
of how to communicate within clusters in an energy-eﬃcient m anner, and ﬁnally, present the full
algorithm for connecting all clusters in a component.
Lemma 2.8. There is an algorithm that, given a graph of at most poly(logn)nodes clustered into
O(logn/log logn)clusters, each with diameter O(log logn)ﬁnds a spanning tree with diameter
O(logn)of the graph, in O(logn·log2logn)rounds of the CONGEST model, of which each node is
awake for at most O(log logn)rounds, and succeeds with probability at least 1−n−10.
Energy Eﬃcient Broadcast and Convergeast. Before giving the algorithm proving
Lemma 2.8 , we outline how, given a cluster associated with a rooted spa nning tree of diameter D
and each node knowing its distance to the root and D, we can perform broadcast and convergecast
8in this cluster with O(1) energy complexity and O(D) time complexity. We will describe the
broadcast procedure; convergecast works similarly. For a d etailed and formal description of these
procedures, we refer to [ AMP22 ] or [ BM21a ], where the assumed structure of our clusters is called
“Labeled Distance Tree (LDT)” and “Distributed Layered Tre e (DLT)” respectively.
Note that in the standard broadcast in a tree, each node only p erforms an operation in two
rounds. In the ﬁrst round, it receives the message from its pa rent, and in the second round, it sends
it to its children. Since we assume that a node vknows its distance to the root dv, these rounds
are precisely dvanddv+ 1. Thus, it suﬃces for vto be awake in only these two rounds, which it
can calculate beforehand, and be sleeping in the remaining r ounds. This means that each node is
awake in O(1) rounds, and as for the standard broadcast, the procedure requires O(logn) rounds.
With these procedures as subroutines, we can now give the alg orithm proving Lemma 2.8 .
Proof of Lemma 2.8 .As a ﬁrst step, we will ensure that each cluster has the requir ed structure.
For this, all nodes are awake in each round. Each cluster will ﬁnd a root by electing a leader (i.e.,
the node of minimum identiﬁer), after which the root will sta rt a broadcast, building a spanning
tree. Each node will learn its distance from the root. For the diameter of a cluster, we will always
useO(logn) as an upper bound. Since we have O(logn/log logn) clusters of diameter O(log logn),
and we only ever merge clusters, this is indeed an upper bound of the diameter of any cluster.
Then, we perform O(log logn) iterations, in each of which we will reduce the number of rem ain-
ing clusters by a constant factor. Some steps will be easier t o describe in the following view: Let
Hbe the graph obtained by contracting each cluster into one no de, and connecting two clusters in
Hif they contain adjacent nodes. We will call this Hthecluster graph . With this, we are ready
to describe the steps in each iteration.
Finding Outgoing Edges. As in Bor˚ uvka’s algorithm, each cluster will ﬁrst select on e incident
edge as outgoing. This will be the edge to the neighboring clu ster whose cluster identiﬁer (which
is just the identiﬁer of its root) is minimal. In the case ther e are multiple edges to the same
cluster, we choose one in an arbitrary but consistent way (e. g., by choosing the edge with the
minimum identiﬁer, where the edge ID is the concatenation of the ID of its incident nodes). To
obtain an energy-eﬃcient algorithm, this can be implemente d as follows: In the ﬁrst round, all
nodes are awake and exchange the identiﬁer of their cluster w ith all their neighbors. Then, each
node chooses the cluster with the minimum identiﬁer, and we s tart a convergecast in each cluster,
always forwarding the neighboring cluster with the minimum identiﬁer. Once this has reached the
root, it will broadcast to all nodes whose edge was chosen. Th en, we remove all edges that were
not selected as outgoing from H. If two neighboring clusters chose the same edge as their out going
edge, we will set this edge aside for now, and ignore the edges in both directions in H. Let the set
of edges be M. By ignoring these parallel edges, and noting that outgoing edges always point to a
cluster with the lower identiﬁer, the remaining graph Hcannot contain any cycles and must be a
forest of oriented trees. This observation concludes this s tep and we move on to the next one.
High- and Low-Indegree Clusters. We call a cluster high indegree if 10 or more other clusters
chose it as their outgoing neighbor. Otherwise, we say it has low degree. In the same way as before,
a cluster can learn its indegree by each node being awake in th e ﬁrst round of this step, and if it is
incident to the edge selected by its cluster, it will inform i ts neighbor about this selection. Then,
we can perform a convergecast in each cluster, summing up the number of incoming edges. Finally,
each cluster performs a broadcast such that all nodes in the c luster learn the indegree. If a cluster
has high indegree, it will remove its outgoing edge from Handaccept all remaining incoming edges.
We call this set of accepted and non-removed edges EH. This will be one of the sets of edges we
use for merging clusters. This takes care of all edges incide nt to high indegree clusters, so from
9now on we will focus on edges between low-degree clusters. Le tHLbe the subgraph of Hinduced
by low indegree clusters. We are interested in only one prope rty ofHL, namely that if we drop the
orientations, HLhas degree at most 10 (at most 9 incoming and 1 outgoing edge).
Maximal Matching on HL.To compute a maximal matching on HL, we will ﬁrst compute a
coloring of HL. Before we describe the algorithm, we argue that we can simul ate the execution of any
CONGEST model algorithm on HLin the base graph using only one broadcast, one convergecast ,
and one additional round: First, each root broadcasts the me ssages it intends to send its neighbor
to all nodes in its cluster. Then, each node that has an adjace nt edge in HLsends the received
message to the corresponding neighboring cluster. Finally , by a convergecast in each cluster, the
root learns all messages. Since HLhas maximum degree 10, there are at most 10 messages sent and
received in each cluster, and this still works in the CONGEST model with O(logn)-bit messages.
For coloring HLwe can now use Linial’s algorithm [ Lin92 , Theorem 5.1]. Given a graph of
maximum degree ∆ and a kcoloring, this algorithm returns a O(∆2logk) coloring in one CONGEST
round. Performing two rounds of this will give us a coloring w ith at most O(log logn) colors, where
we used the cluster identiﬁer as the initial coloring. Then, we go through the color classes one
by one and use the given orientation of HL. For each color class i, all clusters Cof coloricheck
whether or not they were matched to a cluster with lower color . If not, Cmatches itself to one
of its unmatched incoming neighbors, if such a neighbor exis ts. Let this computed set of edges be
ML.
We argue that MLis a maximal matching in HL. First, it is a matching because no cluster is
adjacent to more than one edge. In each color class, no two clu sters can be matched to the same
cluster since each cluster that is not of the current color ca n only be matched to its out-neighbor,
of which there is at most one. And since it is a proper coloring , there are no two neighboring
clusters of the same color. Further, the matching is maximal since if a cluster Cis unmatched all
its outgoing neighbors must be matched, as otherwise Cwould have been matched to one of them.
The same is true for the incoming neighbor since if it was not m atched, it would have been matched
toC.
Merging Clusters. Now we can discuss how and which clusters we merge. We will mer ge along
all edges from the sets M,EH, andMLone by one, and ﬁnally, along a set of edges Rwhich we
describe now: Each low-degree cluster C, that has no incident edge in M∪EH∪ML, chooses an
arbitrary outgoing neighbor that has an incident edge in M∪EH∪MLand adds this edge to R.
Such a neighbor must exist: Ccan only have low-degree neighbors by deﬁnition, and if one o f them
was not matched in ML, it would contradict ML’s maximality. This set Rinduces a subgraph of
H, where each connected component is a star since we connect tw o disjoint sets of clusters.
Merging along all those sets, we get that each cluster is merg ed with at least one other cluster,
meaning that the number of clusters decreases by a factor of t wo in each iteration. However, we still
need to describe how we perform the merges: We show how we perf orm star-shaped merges, where
we have a number of leaf clusters that are merging oriented to wards a center cluster, which is the
case forEHandR. Since both MandMLare matchings, this is just a special case of star-shaped
merges. We can merge clusters by just adding the edges connec ting them to the spanning tree.
Then for all edges ( u,v), such that uis in the center cluster and vis in a leaf cluster, we set uas the
parent of vand set the distance of vto the root dvto bedu+ 1. Then, we perform a convergecast
in the leaf cluster, allowing us to set the distance to vof all nodes that are between vand the root
of the leaf cluster. For computing the distance of all the rem aining nodes to v, we perform one
more broadcast from the original root of the leaf cluster, al lowingvto become the new root.
Time and Energy Complexity. First, we analyze the time complexity. Since we half the numb er
of clusters in each iteration, we have O(log logn) iterations. In each step, except for computing the
10matching on HL, we use a constant number of rounds for communication betwee n clusters, plus a
constant number of broadcasts and convergecasts. For compu ting the matching on HL, we perform
one broadcast and one convergecast, plus one additional rou nd per color class. Since the cluster
diameter is at most O(logn), the overall time complexity is O(logn·log2logn).
For the energy complexity, using the same argument as for the time complexity gives an upper
bound of O(log2logn). However, despite going through the O(log logn) color classes, we still only
needO(1) awake rounds. Each cluster only has a constant number of n eighboring clusters, and
only needs to be awake in the rounds where these clusters coul d be matched to them, to learn
whether or not it has been matched in future rounds. Thus, ite rating through all the O(log logn)
color classes still only requires O(1) awake rounds per node, and thus also O(1) awake rounds per
iteration suﬃce. Overall, we have O(log logn) rounds where a node needs to be awake.
Computing the MIS From Lemma 2.8 we know that we can obtain a spanning tree of depth
O(logn) in each cluster, and we know that we can perform broadcast an d convergecast in each
cluster with O(1) energy complexity. If we had messages of unlimited size, we could just perform
one convergecast for the root to learn the topology of the ent ire cluster, compute an MIS locally,
and then distribute this information to all nodes. For the CONGEST model, this does not work,
but we can use the same idea as in [ Gha19 ] to get around this issue, which we describe now.
Proof of Lemma 2.7 .We ﬁrst use the algorithm from Lemma 2.8 clusters to get a spanning tree
of diameter O(logn) of the graph. To compute an MIS we will use Ghaﬀari’s algorith m [Gha16 ,
Theorem 3.1] with all nodes being awake for the entire algori thm. This algorithm can be executed
using 1-bit messages and gives the following guarantee, res tated for graphs with maximum degree
poly(logn). After O(log logn) rounds of the algorithm, for a given node v, the probability that
neithervnor any of its neighbors have joined the MIS is at most 1 /poly(log n). Since each
component has poly(log n) nodes, by a union bound the probability that any of them rema ins
(i.e. has not joined the MIS or one of its neighbors joined the MIS) is at most 1 /poly(log n) still.
With one execution of the algorithm using only 1-bit message s, we can run O(logn) independent
executions of the algorithm in parallel, using O(logn) bit messages. Since the success of each
of those executions is independent, the probability that no ne of them computes an MIS is less
thann−10. Each node vcan now check if an execution was successful locally. It chec ks if any
neighbor uofvjoined the MIS. Then, vis successful if either vjoined the MIS and there is no such
neighbor u, orvdid not join the MIS and there is a neighbor uthat did. Again, this can be done
for each execution of the algorithm with 1-bit messages, thu s for all executions in parallel in the
CONGEST model. To check if one execution was successful for all nodes a convergecast with 1-bit
messages suﬃces, where the aggregate function is just the lo gical AND. This means we can check
all executions in parallel and the root of the spanning tree c an pick the ﬁrst successful one, and
inform all nodes to use their output in this execution. Since at least one execution was successful
with probability 1 −n−10, the algorithm succeeds with the same probability.
2.4 Putting Everything Together
Having described all three phases, we can now prove our ﬁrst m ain result.
Proof of Theorem 1.1 .First, we execute the algorithm described in Lemma 2.1 , and remove the
independent set together with its neighbors from the graph. Since the degree of the remaining
graph has now dropped to O(log2n), we can use the algorithm from Lemma 2.6 now. This again
computes an independent set, which we remove from the graph t ogether with their neighbors. The
remaining nodes are clustered into O(log logn)-diameter clusters, and each connected component
11of the remaining graph contains at most poly(log n) nodes, and O(logn/log logn). By treating
each remaining connected component as an independent graph , we can execute the algorithm from
Lemma 2.7 on each of them. The resulting set is maximal in the original g raph, if the ﬁrst two
phases succeed, and the third phase computes an MIS in each cl uster. By a union bound over
all clusters, the latter occurs with probability at least 1 −n−9. Combining this with the success
probabilities of the ﬁrst two phases, the probability that t he algorithm computes an MIS is 1 −n−8.
The algorithm runs for O(log2n) +O(log logn) +O(logn·log2logn) =O(log2n) rounds, and
each node is awake for 3 ·O(log logn) =O(log logn) rounds.
3 Algorithm 2
In this section, we will describe an algorithm that has energ y complexity O(log2logn) and a time
complexity of O(logn·log logn·log∗n). This will prove Theorem 1.2 . Compared to the previous the
main change will be in phase I, which is an entirely diﬀerent al gorithm. Phase II will be identical,
and for Phase III we have the same algorithm, however, we will set the parameters diﬀerently to
obtain a better time complexity, at the cost of a slightly wor se energy complexity.
3.1 Phase I
As in the ﬁrst algorithm, the goal of this phase is to ﬁnd an ind ependent set, such that removing
it and its neighbors from the graph, reduces the maximum degr ee to poly(log n). The algorithm is
made up of diﬀerent iterations, in each of which we will go from a graph with maximum degree ∆
to a graph with maximum degree ∆0.7, as long as ∆ = Ω(log20n). Every such iteration will require
O(logn) rounds and O(log logn) energy complexity.
The general idea is similar to a diﬀerent version of Luby’s alg orithm: In each round, a node v
which still has deg( v)active neighbors is marked with probability1
2deg(v). For any edge that has
both endpoints marked, we remove the marking of the endpoint with the lower degree, breaking
ties arbitrarily. Now each node vthat is still marked has no marked neighbor, so we add vto
the MIS and remove vand all its neighbors from the graph. It can be shown that this algorithm
computes a maximal independent set in O(logn) rounds with high probability.
For our goal of achieving low energy complexity, we have an ad ditional obstacle compared to the
previous algorithm: The marking probabilities depend on th e current state of the graph, which in
turn depends on the execution of the algorithm and the random choices in the previous steps. Some
of the issues of the previous section remain, namely that nod es can be marked Θ(log n) times2.
As before, in order to deal with these issues we also need to we aken the guarantees of the
algorithm. While Luby’s algorithm computes an MIS, our algo rithm will only be able to ﬁnd
an independent set, such that the remaining graph has maximu m degree at most ∆0.7, as long
as ∆ = Ω(log20n). This allows us to almost ignore all nodes of degree less tha n ∆0.6. For the
remaining high-degree nodes v, we can get an accurate estimate /tildewidestdeg(v) of the degree deg( v) by
sampling their neighbors with probability1
∆0.5. Further, we also cap the marking probability at
roughly1
∆0.6. This means we can pre-mark nodes with the same probability, and then later adjust
it, while ensuring that only a small number of nodes are pre-m arked. By being able to do this at
the beginning of the algorithm, the nodes can also set their a wake schedule at this time. To sum
up, we now have two ways in which nodes can be sampled, and in bo th of them, the sampling
probability is at most1
∆0.5. This allows us to use the same ideas as in the ﬁrst algorithm, where we
2Indeed, e.g. on constant degree graphs this will occur with h igh probability for at least one node.
12only consider a node sampled once and then discard it from the algorithm, regardless of if it was
removed from the graph. This will allow us to get the desired b ounds on the energy complexity.
To sum up, we will show that there is an algorithm with the foll owing guarantees:
Lemma 3.1. There is an algorithm that, given a graph of maximum degree ∆≥Ω(log20n)com-
putes an independent set Sof nodes, such that, with probability 1−n−6, the removal of all nodes
fromStogether with their neighbors results in a graph of degree at most ∆0.7. The algorithm runs
inO(logn)rounds and each node is awake for at most O(log logn)rounds.
By repeatedly executing this algorithm for O(log log ∆) iterations, each time with a smaller
maximum degree ∆, until ∆ ≤O(log20n), we can conclude:
Corollary 3.2. There is an algorithm that, given a graph of maximum degree ∆computes an
independent set Sof nodes, such that, with probability 1−n−5, the removal of all nodes from S
together with their neighbors results in a graph of degree at mostO(log20n). The algorithm runs
inO(logn·log log ∆) rounds and each node is awake for at most O(log logn·log log ∆) rounds.
Algorithm. We describe one iteration of the algorithm. Given a graph of d egree ∆ = Ω(log20n),
it returns an independent set such that, w.h.p., the maximum degree in the remaining graph is
∆0.7. This is then applied recursively until the degree drops bel ow Ω(log20n) and we move on to
the second phase. For simplicity, we ﬁrst describe the algor ithm as if all nodes are active in all
rounds. We discuss how it can be implemented with low energy c omplexity later in this section.
At the beginning of each iteration, all nodes are active . As a ﬁrst step, they check if any of
their neighbors joined the MIS in the previous iteration. If they have such a neighbor, they are
removed from the graph and become inactive . Then, they perform two types of sampling: (A) for
every round, a node vﬂips a coin which comes up head with probability1
∆0.5. Once a node vﬂips
heads in round i, it istagged in round iand will not participate in the rest of the sampling of type
(A). These nodes will be used to estimate the degree for setti ng the sampling probability. For the
sampling of type (B) each node vperforms the same process again, but the probability of ﬂipp ing
heads is1
2∆0.6. In the ﬁrst round ithatvﬂips heads, it is pre-marked in round iand again, will
not participate in the sampling of the following rounds. We u se this sampling to compute MIS. We
call a node vspoiled in round jif there is a round i < j , such that vwas sampled of either type
(A) or (B) in round i.
The algorithm performs O(logn) of the following rounds, in each of which we remove some node s,
and then proceed on the remaining subgraph. We split each rou ndiinto three sub-rounds, each
corresponding to one round of communication in the base grap h. In the ﬁrst sub-round, all nodes
that were tagged (i.e. sampled of type (A)), inform all their neighbors of this tagging. Then, each
nodevthat was pre-marked keeps track of the number of neighbors th at were sampled. Let Avbe
this number of tagged neighbors. The degree can now be estima ted as/tildewidestdeg(v) = ∆0.5·Av. Since the
node was pre-marked with probability1
2∆0.6, to get a sampling with probability min/braceleftBig
1
2∆0.6,1
5/tildewidestdeg(v)/bracerightBig
,
we re-sample each node with probability min/braceleftBig
1,2∆0.6
5/tildewidestdeg(v)/bracerightBig
and call a node marked if it was re-
sampled.
In the second sub-round, each marked node vinforms its neighbors that it was marked, and also
sends them its estimated degree /tildewidestdeg(v). If a marked node vlearns that one of its neighbors uwas
marked, it removes its marking if /tildewidestdeg(v)≤/tildewidestdeg(u), and keeps its marking if no such neighbor exists.
Finally, each node that kept its marking joins the independe nt set and informs all its neighbors
about this decision in the ﬁnal sub-round. The nodes that joi ned the independent set, as well as
13their neighbors, are now considered inactive . Inactive nodes and nodes that are spoiled in iterations
j > i , are removed from the graph, and we proceed to the next round.
Once an iteration has ﬁnished, we will show that the active (a nd possibly spoiled) nodes that
have more than 4∆0.6active and non-spoiled neighbors form an independent set, w ith high proba-
bility. Thus, all active nodes will communicate with all the ir neighbors to get their exact number
of active and non-spoiled neighbors. If this number is more t han 4∆0.6, they will inform their
neighbors of this fact. Then, all nodes that have degree more than 4∆0.6, and no neighbor with
degree at least 4∆0.6will join the independent set.
Analysis. Our analysis is close to a standard analysis by Alon, Babai, a nd Itai [ ABI86 ], as also
outlined in [ MR95 ]. However, due to our changes in the algorithm, each step in t he analysis needs
to be adapted. Since we do not work with exact degrees, we ﬁrst need to prove a standard result
that for nodes of large degree, their degree estimate is clos e to their true degree. Then, we will show
that there are certain nodes, which we call good, which are removed with constant probability in
each step. And ﬁnally, we can show at least half of the edges be tween high-degree nodes are good
and are thus also removed with constant probability. After O(logn) such iteration, this means that
no edges between high-degree nodes remain, and the nodes tha t we add in the ﬁnal step indeed
form an independent set.
In order to try and avoid the ambiguity between the degree at t he beginning and during the
execution of the algorithm, we start by introducing the foll owing notation.
Deﬁnition 3.3. The remaining degree degi(v)of a node vin round iis the number of active and
non-spoiled neighbors, i.e. the number of neighbors that st ill participate in the algorithm.
For the analysis, let us ﬁrst prove that each node has an accur ate estimation of its degree.
Lemma 3.4. Let∆≥Ω(log20n). For any node vthat has remaining degree degi(v)≥∆0.6in a
given round i, we have that the estimate /tildewidestdeg(v)∈[1/2 degi(v),2 degi(v)]with probability 1−n−10.
Further, for any node uwith remaining degree degi(u)<∆0.6we have that /tildewidestdeg(u)≤2∆0.6with
probability 1−n−10as well.
Proof. Remember that we estimated the degree of vas/tildewidestdeg(v) = ∆0.5·Av, whereAvis the number
of tagged neighbors of vin round i. We can write Av=/summationtext
u∈Ni(v)Xu, whereXuis the indicator
variable for ubeing tagged in round i, and we have Pr[ Xu= 1] = ∆−0.5sinceuis not spoiled.
It holds that E[Av] = ∆−0.5·degi(v), so the probability that /tildewidestdeg(v)∈[1/2 degi(v),2 degi(v)] is
equal to the probability that Av∈[1/2E[Av],2E[Av]]. Since all Xuare independent, we can use
Chernoﬀ’s Bound to bound the inverse probability:
Pr[|Av−E[Av]| ≥1/2E[Av]]≤2 exp(−E[Av]/12)
From degi(v)≥∆0.6we have that E[Av]≥∆0.1≥log2nsince ∆ ≥Ω(log20n). Thus, the
probability is at most n−10and we have proven the ﬁrst part of the statement.
For the second part, we can follow the same line of argument, b ut we are interested in the
probability of Au≤2∆0.1which is the same as the probability that /tildewidestdeg(u)≤2∆0.6. We can again
boundE[Au] from degi(u)≤∆0.6byE[Au]≤∆0.1. We can use Chernoﬀ’s Bound again to bound
14the inverse probability:
Pr/bracketleftbigg
Au−E[Au]≥∆0.1
E[Au]·E[Au]/bracketrightbigg
≤exp
−∆0.2
E[Au]2E[Au]
2 +∆0.1
E[Au]

= exp/parenleftbigg
−∆0.2
2E[Au] + ∆0.1/parenrightbigg
≤exp(∆0.1/3)
Again, we have that ∆ ≥Ω(log20n), so this probability is at most n−10proving the second part.
This now allows us to deﬁne what kind of nodes we call good, which have their name because
they become inactive with constant probability in each iter ation. We note here that spoiled nodes
can also be good, even if the naming could suggest otherwise. This is necessary because once a
node is marked it can become spoiled with constant probabili ty. So in the likely event that there
is a high-degree node that becomes spoiled, it still needs to be removed from the graph, or at least
its degree among active nodes needs to drop. Thus, we still ne ed to consider it in our analysis.
Deﬁnition 3.5. We call a node goodin round i, if it is not inactive, has degree degi(v)≥∆0.6
and more than a third of its neighbors uhave lower degree, i.e. have degi(u)<degi(v). Otherwise,
we callvbad. An edge eis good if both its endpoints are good and bad otherwise.
Unfortunately, we cannot make any statements about the numb er of good nodes compared to
the number of bad nodes. However, when looking at the edges, w e can say that at least half the
edges between high-degree nodes are good.
Lemma 3.6. At least half of all edges where both endpoints have degree at least ∆0.6are good.
Proof. First, for the analysis, orient all edges from the endpoint w ith the lower degree to the
endpoint with the higher degree, breaking ties arbitrarily . Let us look at the subgraph induced by
all nodes of degree at least ∆0.6. Note that all outgoing edges of nodes of degree at least ∆0.6are
preserved in this subgraph, as they point to nodes of even hig her degree.
Let us place 1 dollar on each bad edge incident to a node of degr ee at least ∆0.6, even if their
other endpoint might have a lower degree. For each bad node, w e have that at least 2/3 of its
incident edges are outgoing and thus are in the relevant subg raph. So we can move the dollar on
each bad edge, to its endpoint, which can then distribute it s uch that each of its outgoing edges
receives 1/2 dollar. Thus, each edge in our subgraph receive s 1/2 dollar, meaning at most half the
edges can be bad. In other words, this means that at least half the edges are good, proving our
statement.
As the main technical part of this proof, we now show that good nodes are indeed removed with
constant probability, as long as all degree estimates are ac curate.
Lemma 3.7. Assume that the statement of Lemma 3.4 holds (with probability 1) and that ∆≥
Ω(log20n). Then, for every good node vwith degree degi(v)≥∆0.6, the probability that a neighbor
ofvis marked and not unmarked is at least (1−e−1/30)/2.
Proof. First, we will show that the probability that no neighbor wit h a lower degree is marked
is at most e−1/30. Sincevis good, it has at least 1 /3 degi(v) neighbors with a lower degree. By
15Lemma 3.4 , each of those neighbors uestimates their degree to be at most /tildewidestdeg(u)≤2 degi(v).
Combining this with degi(v)≥∆0.6, we get that the marking probability of uis
min/braceleftBigg
1
2∆0.6,1
5/tildewidestdeg(u)/bracerightBigg
≥min/braceleftbigg1
2∆0.6,1
10 degi(v)/bracerightbigg
=1
10 degi(v).
Thus, the probability that a lower degree neighbor is not mar ked is at most 1 −1
10degi(v). Since all
neighbors are marked independently, the probability that n one of them is marked is at most
/parenleftbigg
1−1
10 degi(v)/parenrightbigg1/3degi(v)
≤exp/parenleftbigg
−1/3 degi(v)
10 degi(v)/parenrightbigg
=e−1/30.
Ideally, we could now just argue that a node uwhich is marked is removed with constant prob-
ability, which is true, however combining these two stateme nts results in issues with dependencies.
Instead, we have to be a bit more careful about how the random c hoices are made. Let us look at
the neighbors of vone by one, and expose their random choices for the marking un til we reach one
node that is marked. By the previous paragraph, this process succeeds with probability 1 −e−30
before it runs out of neighbors. Now that we have such a marked neighbor u, we need to argue
that it remains with probability at least 1/2. We have not yet revealed the random choices of any
neighbors of uthat could be marked. If ushares any neighbors with v, they were all not marked.
There are at most degi(u) neighbors wofuremaining, which could still be marked and have
/tildewidestdeg(w)≥/tildewidestdeg(u). Let us ﬁrst consider the case that degi(u)≥∆0.6. In that case, by Lemma 3.4 ,
we have /tildewidestdeg(u)≥1/2 deg(u). Thus, the probability that a given neighbor is marked is at most
min/braceleftBigg
1
2∆0.6,1
5/tildewidestdeg(w)/bracerightBigg
≤min/braceleftBigg
1
2∆0.6,1
5/tildewidestdeg(u)/bracerightBigg
≤min/braceleftbigg1
2∆0.6,1
2 degi(u)/bracerightbigg
=1
2 degi(u).
By a union bound over all neighbors of uwith a higher estimated degree, the probability that at
least one of them is marked is at most degi(u)·1
2degi(u)= 1/2. This means that the probability
that none of them is marked is at least 1 /2.
For the second case that degi(u)≤∆0.6we have that each neighbor wofuis sampled with
probability at most1
2∆0.6. Thus, the probability that any of them is sampled is at most ∆0.6·1
2∆0.6,
which is at most 1/2. Finally, this means that in either case n o higher degree neighbor of uis
marked with probability at least 1 /2 and thus uis not unmarked.
This directly means that every good edge is also removed with constant probability by at least
one of its endpoints. By linearity of expectation, this mean s that the expected number of edges
drops by a constant factor. However, this still assumes that the degree estimates are accurate,
which might not be true. But we have shown that this occurs wit h such a small probability that we
can essentially ignore this case since its contribution to t he expectation is negligible. This intuition
is formally proven in the following lemma.
Lemma 3.8. For∆≥Ω(log20n), letmi−1be the number of edges whose endpoints both have degree
at least ∆0.6at the beginning of round i. Then, we have that at the end of round i, the expected
number of such edges is E[mi|mi−1]≤(1−(1−e−1/30)/8)·mi−1.
Proof. LetEbe the event that the statement of Lemma 3.4 holds for all nodes. Since the statement
ofLemma 3.4 holds with probability at least 1 −n−10for each node v, by a union bound over all
16nodes, we have Pr[ E]≥1−n−9and Pr[E]≤n−9. We can rewrite the expectation we are interested
in as:
E[mi|mi−1] =E[mi|mi−1andE]·Pr[E] +E[mi|mi−1andE]·Pr[E]
For the ﬁrst term, we know from Lemma 3.6 that at least mi−1/2 of all edges whose endpoints both
have degree at least ∆0.6are good. For those edges, we know that both their endpoints a re good
and have degree at least ∆0.6. Thus, by Lemma 3.7 each endpoint is removed with probability at
least (1−e−1/30)/2, and the same is true for the removal of each good edge. This m eans that for
the expected value we have
E[mi|mi−1andE]≤mi−1−mi−1/2·(1−e−1/30)/2 = (1−(1−e−1/30)/4)·mi−1.
For Pr[E], we will just need that it is at most 1.
For the second term we trivially have that E[mi|mi−1]≤mi−1, andmi−1≤n2. Thus, we have
E[mi|mi−1andE]·Pr[E]≪(1−e−1/30)/8·mi−1.
Putting everything together, we have
E[mi|mi−1]≤(1−(1−e−1/30)/4)·mi−1+ (1−e−1/30)/8·mi−1= (1−(1−e−1/30)/8)·mi−1.
In words, Lemma 3.8 just says that in each round, the expected number of edges bet ween nodes
of degree at least ∆0.6decreases by a constant factor. Thus, for a large enough cons tantC, after
Clognrounds, the expected number of such edges that remain is at mo stn−10. By Markov’s
inequality, the probability that this is at least one edge is at mostn−10. Thus we can conclude the
following:
Corollary 3.9. For∆≥Ω(log20n), afterO(logn)rounds, no edges between nodes of degree ∆0.6
remain with probability at least 1−n−10.
Since we remove all nodes of degree at least ∆0.6that remain after O(logn) rounds, this might
seem like we have already proven Lemma 3.1 . However, we have discarded nodes that were spoiled,
without them or any of their neighbors necessarily being in t he computed independent set. Thus,
we still need to show that each node does not have too many neig hbors that were spoiled.
Lemma 3.10. For∆≥Ω(log20n), each node vhas at most 4∆0.6neighbors that are spoiled during
the execution of the algorithm with probability at least 1−n−10.
Proof. We will show that each node vhas at most 4∆0.6neighbors that were ever sampled (in the
form of being tagged or pre-marked). Since nodes can only be s poiled after they were sampled, this
proves the lemma.
Let us ﬁrst look at the tagging of nodes. For each node u, letXube the indicator variable that
uwas ever tagged in one of the O(logn) rounds. Then, the number of tagged neighbors of vis
X=/summationtext
u∈N(v)Xu. In each round, we have a probability of ∆−0.5to tagu. Thus, the probability
Pr[Xu= 1] that uis tagged in any round is at most O(logn)·∆−0.5≤∆−0.4since ∆≥Ω(log20n).
This means that E[X]≤∆0.6. Since all Xuare independent, we can use Chernoﬀ’s Bound to obtain
Pr[X≥2∆0.6]≤Pr/bracketleftbigg
X−E[X]≥∆0.6
E[X]E[X]/bracketrightbigg
≤exp
−∆1.2
E[X]2E[X]
2 +∆0.6
E[X]

≤exp/parenleftbigg
−∆1.2
2E[X] + ∆0.6/parenrightbigg
≤exp/parenleftbig
−∆0.6/3/parenrightbig
≪n−11,
17where we again used ∆ ≥Ω(log20n) in the last inequality. To sum up, this means that at most
2∆0.6neighbors of vare ever tagged during the algorithm with probability at lea st 1−n−10.
For the marking, we can use the same kind of argument. Let Yube the indicator variable
thatuis pre-marked in any round. Since the pre-marking probabili ty is capped, we have that
Pr[Yu= 1]≤1
2∆0.6. We can again deﬁne Y=/summationtext
u∈N(v)Yuas the number of neighbors of vthat
are ever pre-marked. Since Pr[ Yu= 1]≤Pr[Xu= 1], we also have that Pr[ Y≥2∆0.6]≤Pr[X≥
2∆0.6]≤n−11.
Finally, the probability that either the number of tagged no des or the number of pre-marked
nodes exceeds 2∆0.6is at most 2 n−11≤n−10. Thus, the probability that the number of spoiled
neighbors is less than 4∆0.6is at most 1 −n−10.
With this additional fact, we can ﬁnally prove Lemma 3.1 :
Proof of Lemma 3.1 .First, note that the algorithm always computes an independe nt set. Because
we add an independent set of nodes in each iteration and then r emove all their neighbors, the union
of all those sets is still an independent set.
Next, let us consider the maximum degree in the remaining gra ph. In the last step, all nodes of
degree at least 4∆0.6are removed if they form an independent set, which by Corollary 3.9 happens
with probability at least 1 −n−10. From Lemma 3.10 we have that the number of spoiled neighbors
(even inactive ones) is at most 4∆0.6as well, with probability at least 1 −n−10. Thus, with
probability 1 −2n−10≥1−n−6the total number of remaining neighbors is at most 8∆0.6≪∆0.7.
It remains to prove the time and energy complexities. For the time complexity, the algorithm
runs for O(logn) rounds, each of which corresponds to three rounds of commun ication. This
communication is possible in the CONGEST model since both the tagging and decision to join the
independent set can be exchanged with 1-bit messages. For th e second sub-round, it is suﬃcient for
a nodevto send the number of tagged neighbors Avto each neighbor u. All other computations
can then also be performed by u, and since Avis an integer and at most n, this can be sent in a
O(logn)-bit message. Thus, the algorithm runs in O(logn) rounds of the CONGEST model.
For the energy complexity, we proceed in the same way as in pha se I of the ﬁrst algorithm. In
the beginning, each node vperforms the sampling for being tagged or pre-marked. Let rvbe the
ﬁrst round in which vis sampled of either type. Note that vcan also be sampled of both kinds
in the same round. Since this is the ﬁrst round that vis sampled, it cannot be spoiled yet, but
it still needs to know whether or not it is still active. We can again do this by using Lemma 2.5
withT=O(logn) to get a set Srvof rounds in which a node vwill be awake. Then, we add a
fourth sub-round to each round, in which nodes will inform th eir neighbors whether or not they
joined the independent set in a previous round. This allows u s to execute the same algorithm, but
with each node being awake in only O(log logn) rounds, which proves our bound on the energy
complexity.
3.2 Phase III
While the second phase is identical to the ﬁrst algorithm, th ere is a slight change in the third phase.
We are now interested in a diﬀerent trade-oﬀ between round and energy complexity. Since the ﬁrst
phase has energy complexity O(log2logn), we want to now minimize the round complexity of the
third phase, while still keeping the energy complexity belo wO(log2logn).
The best such trade-oﬀ we are able to achieve is the following : A round complexity of O(logn·
log logn·log∗n) and an energy complexity of O(log logn·log∗n). This result has already been
proven in (the full version of) previous work [ BM21a ,BM21b , Theorem 5.2] so we refer to their
paper for a full proof. Since the adaptation to our algorithm is minor, we also give an outline here.
18In the step, where we compute a maximal matching on HL, we will compute a coloring with
O(1) colors instead of O(log logn) colors, which will save an O(log logn) factor in the running time.
This can be achieved by running Linial’s algorithm for O(log∗n) rounds instead of just 2. However,
it comes at the cost that in each round of Linial’s algorithm w e need to communicate between
neighboring clusters. One such round of communication requ ires each node to be awake for O(1)
rounds, which are O(log∗n) rounds for the coloring step, giving the ﬁnal complexities .
3.3 Putting Everything Together
With the described changes to the ﬁrst and third phases of the algorithm, we can give the proof of
our second main result.
Proof of Theorem 1.2 .First, we execute the algorithm from Lemma 3.1 , and remove the indepen-
dent set together with its neighbors from the graph. Since th e degree of the remaining graph has
now dropped to O(log20n), we can use the algorithm from Lemma 2.6 now. This again computes
an independent set, which we remove from the graph together w ith their neighbors. The remaining
nodes are clustered into O(log logn)-diameter clusters, and each connected component of the re -
maining graph contains at most poly log nnodes, and O(logn/log logn). We treat each remaining
connected component as an independent graph, and compute an MIS on each of them in parallel.
By replacing the algorithm from Lemma 2.8 with the algorithm from [ BM21a , Theorem 5.2] in the
proof of Lemma 2.7 , we get the same guarantees as in Lemma 2.7 , but inO(logn·log logn·log∗n)
rounds, of which a node is awake for O(log logn·log∗n) of them. This algorithm is then executed
on all connected components in parallel. The resulting set i s maximal in the original graph, if the
ﬁrst two phases succeed, and the third phase computes an MIS i n each cluster. By a union bound
over all clusters, the latter occurs with probability at lea st 1−n−9. Combining this with the success
probabilities of the ﬁrst two phases, the probability that t he algorithm computes an MIS is 1 −n−8.
The algorithm runs for O(logn·log logn) +O(log logn) +O(logn·log logn·log∗n) =O(logn·
log logn·log∗n) rounds, and each node is awake for O(log2logn) rounds.
4 Constant Average Energy Complexity
In this section, we show that the previous algorithms can be e xtended to have O(1) average energy
complexity, with high probability. There will be two parts t o achieve this: First, we show that
Phase I of both algorithms already has constant average ener gy complexity. Second, we provide a
new algorithm that works in between phases I and II. Its goal i s to reduce the number of nodes
toO(n/log2logn), which will mean that the algorithms from phases II and III o nly incur a total
energy complexity of O(n) on these remaining nodes, which is O(1) averaged over all nodes.
For this, we will change the parameters of the algorithm from Section 2.1 , with the goal of
further reducing the degree in the remaining graph to poly lo g logn. However, this will come at
the cost of some nodes failing and having to be discarded from the algorithm. We will show that
the probability that a single node fails is at most 1 /poly logn, which (with some extra steps due
to dependencies between nodes) will allow us to conclude tha t at most O(n/logn) nodes fail. We
will ignore these failed nodes, and deal with them later. All nodes that do not fail will succeed and
leave us with a graph of degree poly log log n.
For the successful nodes, that now have degree at most poly lo g logn, we use an algorithm
from [ GP22 ], with the parameters adapted to our setting, to reduce the n umber of remaining nodes
ton/log2logn. Together with the failed nodes, this will mean that O(nlog2logn) nodes remain,
19in a subgraph of degree at most poly(log n), so we can use the algorithms from phases II and III
while still having an average energy complexity.
4.1 Phase I
For the ﬁrst algorithm, we have that maximum sampling probab ility per round is∆
2TforT=
log ∆−2 log logn, which is1
log2n. The probability that a node is ever sampled is thus at most
T/summationdisplay
i=0c·logn·∆
2i≤c·logn·2
log2n≤2c/logn.
Since nodes are sampled independently, by Chernoﬀ’s Bound, the number of nodes sampled is
O(n/logn) with high probability. And since each sampled node is awake forO(log logn) rounds,
the average awake time, or energy complexity, is O(1).
For the second algorithm, we have that the recursion stops on ce we reach a graph with maximum
degreeO(log20n). Between tagging and pre-marking, the probability that a n ode is sampled of
either kind in a given round is at most ∆−0.5≤1/log10n. Thus, the probability that a node is
ever sampled is at most Ω(1 /log9n). Again, by Chernoﬀ’s Bound, the number of nodes sampled
isO(n/logn) with high probability. And again, since each sampled node i s awake for at most
O(log logn) rounds, the average energy complexity is O(1).
4.2 Phase I-II
As with Phase II previously, this will be the same for both alg orithms. However, unlike before, this
section will be the main technical and novel part to obtain co nstant average energy complexity.
We will give an algorithm that removes all but O(n/log2logn) nodes from the graph. Executing
any algorithm with worst-case energy complexity O(log2logn) on these remaining nodes, then only
addsO(1) to the total node-averaged energy complexity. Thus, the following will directly allow us
to get algorithms with node-averaged energy complexity O(1).
Lemma 4.1. There is an algorithm that, given a graph of maximum degree ∆2= poly log n
computes an independent set S, such that removing all nodes in or adjacent to Sleaves at most
O(n/log2logn)nodes. The algorithm runs in O(log2logn)clock rounds, has O(log logn)worst-
case, and O(1)node-averaged energy complexity.
There are two main steps, the ﬁrst has the goal of reducing the degree to poly log log n, which
we will be able to achieve for all but a small number of nodes. W ith the maximum degree this low,
we can use an algorithm from [ GP22 ] to remove all but O(n/log2logn) nodes. Together with the
nodes that failed in the ﬁrst step, there will still be O(n/log2logn) nodes that remain in total.
For the ﬁrst step, we formally show an algorithm achieving th e following.
Lemma 4.2. There is an algorithm that, given a graph of maximum degree ∆2= poly log ncom-
putes an independent set Sand a partition of all nodes that are not in or adjacent to Sin the
following two sets:
A: The nodes in Aform an induced subgraph of degree at most O(log100logn), and
F: Each node vis part of the set of failed nodes Fwith probability 1/log10n.
The algorithm runs in O(log2logn)rounds, each node is awake for at most O(log log log n)rounds,
and the average energy complexity is O(1)with high probability.
20Proof. The algorithm is almost the same as in the proof of Lemma 2.1 , with three key diﬀerences:
First, we only execute O(log logn) rounds in each iteration, and, second, we stop once we reach
iteration T= log ∆ 2−100 log log log n. The third, and biggest, diﬀerence is that at the end of each
iteration, all nodes are awake for three rounds. In the ﬁrst r ound, they learn if they are inactive,
due to one of their neighbors joining the independent set. In the second round, they inform their
neighbors of their current status, i.e. whether or not they a re still active or spoiled. Then, each
node can ﬁgure out if it is failed . We now call a node vfailed at the end of iteration i, if it has
(A) more than ( i+ 1)·Clog lognactive and spoiled neighbors, or (B) more than∆
2(i+1)active and
non-spoiled neighbors. Finally, nodes that failed add them selves to Fand inform their neighbors
of this in the third round For all future rounds, we consider n odes that have failed as inactive.
Note that the degree guarantee of the nodes in Atrivially holds. Since i≤log ∆2−
100 log log log n, all nodes that are not failed have degree at most O(log100logn) So it remains
to be shown that nodes fail with probability at most 1 /log10n. We focus on one node vand
one iteration i, and show that if vis not failed at the beginning of the iteration, it is failed w ith
probability at most 1 /log11nat the end of i. There are two ways in which a node could fail, (A)
and (B); we will ﬁrst consider (B). In this case, vhas more than∆
2(i+1)active and non-spoiled
neighbors remain in each round of iteration i. In each such round, as in Lemma 2.2 ,vis removed
with probability at least 1 /40. Over O(log logn) rounds, the probability that this never occurs is
at most 1 /log12nsince diﬀerent rounds are independent.
The other reason a node can fail is (A) more than ( i+1)·Clog lognof its neighbors are spoiled.
Sincevwas not spoiled at the end of the previous iteration, this can only occur if more than
Clog lognneighbors are spoiled in this iteration. In the same way as in Lemma 2.3 , we can also
bound this probability using Chernoﬀ’s Bound to be at most 1 /log12n.
Thus, in one iteration, a node fails with probability at most 1/2 log12n≤1/log11n. Over all
O(log logn) iterations, this probability is at most 1 /log10nas desired.
For the time complexity, we have O(log ∆) = O(log logn) iterations of O(log logn) rounds,
which is at most O(log2logn) rounds in total. For the worst-case energy complexity, we n ow have
a simpler argument: Each node is only sampled in one iteratio n, thus it can just be awake for all
thoseO(log logn) rounds, and sleep for the other iterations. In addition to t hat, nodes are also
awake at the end of each iteration for 3 rounds. Since there ar eO(log logn) iterations, there are a
total ofO(log logn) rounds that a node that was ever sampled is awake.
So for the average energy complexity, we still need to argue t hat the number of nodes that are
sampled is at most O(n/log logn). As in the previous subsection, the probability that node i s ever
sampled is at most O(log logn)·2
log100logn≤1/log logn. Since nodes are sampled independently,
by Chernoﬀ’s Bound, the number of sampled nodes is at most O(n/log logn) with high probability,
concluding our proof.
We now know that each node joined Fwith probability at most 1 /log10n, however, these events
are not independent, so we cannot directly use the standard v ersion of Chernoﬀ’s Bound. What will
still make it possible to show that there are at most O(n/logn) nodes in Fwith high probability is
that nodes at distance Ω(log2logn) are independently added to F. We can think of the algorithm
as each node ﬁrst producing a random string that it will then u se for all its random choices during
the algorithm. Then, the output of the algorithm at a node vafterTrounds can also be described
as a mapping of the T-hop neighborhood, and all random strings within this neigh borhood, to the
output of v. Thus, all nodes at a distance more than 2 Twill have their outputs be independent of
each other.
With this observation, we can now formally state and prove th e result.
21Lemma 4.3. The number of failed nodes after the algorithm of Lemma 4.2 , i.e. the size of the set
F, is at most O(n/logn)with high probability.
Proof. LetXvbe the indicator variable for the event that vfailed. We know that E[Xv]≤1/log12n.
Since the input graph has degree poly log n, and the algorithm runs for O(log2logn) rounds, each
Xvis independent of all but at most logO(log2logn)nrandom variables Xufor other nodes u. This
now allows us to use the following extension of Chernoﬀ’s Bou nd.
Theorem 4.4 (Chernoﬀ Bound’s Extension to Bounded-Dependency Cases [ Pem01 ]).LetX1, ...,
XNbe binary random variables, and let a1,...,a nbe coeﬃcients in [0,1]. Suppose that each Xiis
mutually independent of all other variables except for at mo st∆′of them. Let ¯X=/summationtextN
i=1ai·Xi,
andµ=E[¯X].Then, for any δ≥0, we have
Pr/parenleftbig
|¯X−E[¯X]| ≥δE[¯X]/parenrightbig
≤8(∆′+ 1)·exp(−E[¯X]δ
3(∆′+ 1)).
Applied to our setting, we have that Xi=Xvall nodes v, andai= 1. Since we have
E[¯X]≤n/log10n, the probability that more than O(n/logn) nodes are in Fis at most
Pr[/vextendsingle/vextendsingleX−E[¯X]/vextendsingle/vextendsingle≥C·n/logn]. Thus, for the extended Chernoﬀ’s Bound we have dependency
degree ∆′= logO(log2logn)n, andδsuch that δE[¯X] =C·n/log). With these parameters, the
probability bound given by Theorem 4.4 isn−cfor any constant c, and a suﬃciently large constant
C. Thus, with high probability there are at most O(n/logn) failed nodes.
So what remains is to show that in a graph of degree O(log100logn) we can compute an indepen-
dent set, such that removing it together with its neighbors o nly leaves O(n/log2logn) remaining
nodes. For this, we will use the algorithm from [ GP22 , Section 3.2]. While they do not state the
guarantees of their algorithm in its full generality, they s tate their proof in a way such that they
directly also prove the following:
Lemma 4.5 (Lemma 3.2 in [ GP22 ]).There is an algorithm that, given a graph of maximum
degreed=O(log2n), and a parameter k=O(log logn)for the number of iterations, ﬁnds an
independent set S, such that removing Sand its neighbors from the graph leaves at most O(n/2k)
with high probability. The algorithm runs in O(k·log3d)rounds of the CONGEST model, and has
node-averaged energy complexity O(1), with high probability.
Wrap-Up. With these parts, we can now ﬁnally give the proof of the main r esult of this subsection.
Proof of Lemma 4.1 .First, we will execute the algorithm from Lemma 4.2 . This will result in each
node either being removed from the graph or being in the sets AorF. We will ignore the nodes
inF(i.e. they will not be awake for the remainder of the algorith m), and only consider nodes
fromAfor now. Since the nodes from Ainduce a subgraph of maximum degree O(log100logn), we
can use Lemma 4.5 withd=O(log100logn) andk= 2 log log log n. This gives an algorithm that
runs inO(log4log logn) rounds, has node-averaged energy complexity O(1), and leaves at most
O(n/log2logn) nodes remaining, with high probability. Thus, together wi th the nodes from F, at
mostO(n/log2logn) nodes remain, with high probability.
For the total round complexity, we spend O(log2logn) +O(log4log logn) =O(log2logn) clock
rounds. The worst-case energy complexity is O(log logn) +O(log4log logn) =O(log logn), where
for the second algorithm, we just use the number of clock roun ds as an upper bound on the number
of rounds a node is awake. Finally, on average, each node is aw ake forO(1) rounds in each of the
two algorithms, with high probability. Thus the overall nod e-averaged energy complexity is O(1)
with high probability as well.
224.3 Putting Everything Together
For the claimed algorithms with constant node-averaged ene rgy complexity, we ﬁrst execute the
algorithms from Section 2.1 orSection 3.1 respectively. As argued in Section 4.1 , the node averaged
energy complexity of these algorithms is constant. Then, we use the algorithm from Lemma 4.1
in between phases I and II, which again only requires nodes to be awake for a constant number
of rounds on average. Since the number of nodes is now only O(n/log2logn), any algorithm with
worst-case energy complexity O(log2logn) will only require O(1) average energy complexity. Thus,
we can proceed with Phases II and III of the previous algorith ms, as described in Section 2.2 and
Sections 2.3 and3.2. Finally, since the algorithm from Lemma 4.1 has worst-case energy complexity
O(log logn) and requires at most O(log2logn) many clock rounds, the previous bounds on both
the number of awake and the total number of rounds still hold.
5 Conclusion and Open Problems
In this work, we have given round eﬃcient algorithms with low worst-case energy complexity for the
maximal independent set problem. Allowing for a worst-case energy complexity of O(log2logn),
the round complexity nearly matches the O(logn) round complexity of Luby’s algorithm [ Lub86 ,
ABI86 ]. For the best-known energy complexity of O(log logn), we also made progress on improving
the round complexity to O(log2n). And ﬁnally, we showed that all this can also be achieved wit h
constant node-averaged energy complexity.
There are two natural ways, in which this could be improved. T he ﬁrst is improving the
round complexity while maintaining the best-known worst-c ase energy complexity, i.e., getting an
algorithm with round complexity O(logn) and worst-case energy complexity O(log logn). The
second way in which the results could be improved is by loweri ng the worst-case energy complexity
too(log logn). We note that O(log logn) is possibly a barrier for algorithms that require a sort of
“update” between all pairs of rounds, i.e., for algorithms t hat need to solve the problem described
inLemma 2.5 . As any MIS algorithm needs /tildewideΩ√lognrounds [ KMW16 ], using this as a subroutine
would always require an energy complexity of Ω(log log n). Formalizing this, or proving any ω(1)
lower bound on the worst-case energy complexity, would also improve our understanding of energy-
eﬃcient distributed algorithms.
References
[ABI86] Noga Alon, L´ aszl´ o Babai, and Alon Itai. A fast and s imple randomized parallel algorithm
for the maximal independent set problem. Journal of algorithms , 7(4):567–583, 1986.
[AMP22] John Augustine, William K. Moses, Jr., and Gopal Pan durangan. Brief Announcement:
Distributed MST Computation in the Sleeping Model: Awake-O ptimal Algorithms and
Lower Bounds. In Proceedings of the 2022 ACM Symposium on Principles of Distri buted
Computing (PODC) , pages 51–53, 2022.
[BBH+21] Alkida Balliu, Sebastian Brandt, Juho Hirvonen, Dennis Olivetti, Mika¨ el Rabie, and
Jukka Suomela. Lower bounds for maximal matchings and maxim al independent sets.
Journal of the ACM (JACM) , 68(5):1–30, 2021.
[BEPS16] Leonid Barenboim, Michael Elkin, Seth Pettie, and Johannes Schneider. The locality
of distributed symmetry breaking. Journal of the ACM (JACM) , 63(3):1–45, 2016.
23[BM21a] Leonid Barenboim and Tzalik Maimon. Deterministic Logarithmic Completeness in the
Distributed Sleeping Model. In 35th International Symposium on Distributed Comput-
ing, DISC 2021 , 2021.
[BM21b] Leonid Barenboim and Tzalik Maimon. Deterministic Logarithmic Completeness in the
Distributed Sleeping Model. arXiv preprint arXiv:2108.01963 , 2021.
[Bor26] Otakar Bor˚ uvka. O jist´ em probl´ emu minim´ aln´ ım . 1926.
[CDH+18] Yi-Jun Chang, Varsha Dani, Thomas P Hayes, Qizheng He, We nzheng Li, and Seth
Pettie. The energy complexity of broadcast. In Proceedings of the 2018 ACM Symposium
on Principles of Distributed Computing , pages 95–104, 2018.
[CDHP20] Yi-Jun Chang, Varsha Dani, Thomas P Hayes, and Seth Pettie. The energy complex-
ity of bfs in radio networks. In Proceedings of the 39th Symposium on Principles of
Distributed Computing , pages 273–282, 2020.
[CGP20] Soumyottam Chatterjee, Robert Gmyr, and Gopal Pand urangan. Sleeping is Eﬃcient:
MIS in O(1)-Rounds Node-Averaged Awake Complexity. In Proceedings of the 39th
Symposium on Principles of Distributed Computing , pages 99–108, 2020.
[CKP+17] Yi-Jun Chang, Tsvi Kopelowitz, Seth Pettie, Ruosong Wan g, and Wei Zhan. Exponen-
tial separations in the energy complexity of leader electio n. InProceedings of the 49th
Annual ACM SIGACT Symposium on Theory of Computing , pages 771–783, 2017.
[DMP22] Fabien Dufoulon, William K. Moses, Jr., and Gopal Pa ndurangan. Sleeping is supereﬃ-
cient: Mis in exponentially better awake complexity. arXiv preprint arXiv:2204.08359 ,
2022.
[DMP23] Fabien Dufoulon, William K. Moses, Jr., and Gopal Pa ndurangan. Distributed MIS in
O(log logn) Awake Complexity. In Proceedings of the 42nd Symposium on Principles
of Distributed Computing , page to appear, 2023.
[FGG+23] Salwa Faour, Mohsen Ghaﬀari, Christoph Grunau, Fabian Ku hn, and V´ aclav Rozhoˇ n.
Local Distributed Rounding: Generalized to MIS, Matching, Set Cover, and Beyond. In
Symposium on Discrete Algorithms (SODA) , pages to appear, arXiv:2209.11651, 2023.
[GGR21] Mohsen Ghaﬀari, Christoph Grunau, and V´ aclav Rozho ˇ n. Improved Deterministic Net-
work Decomposition. In Proceedings of the 2021 ACM-SIAM Symposium on Discrete
Algorithms (SODA) , pages 2904–2923, 2021.
[Gha16] Mohsen Ghaﬀari. An improved distributed algorithm f or maximal independent set. In
Proc. 27th ACM-SIAM Symp. on Discrete Algorithms (SODA) , pages 270–277, 2016.
[Gha19] Mohsen Ghaﬀari. Distributed Maximal Independent Se t using Small Messages. In
Proceedings of the Thirtieth Annual ACM-SIAM Symposium on Disc rete Algorithms ,
pages 805–820, 2019.
[GHS83] Robert G. Gallager, Pierre A. Humblet, and Philip M. Spira. A distributed algorithm
for minimum-weight spanning trees. ACM Transactions on Programming Languages
and systems (TOPLAS) , 5(1):66–77, 1983.
24[GP22] Mohsen Ghaﬀari and Julian Portmann. Average awake com plexity of mis and match-
ing. In Proceedings of the 34th ACM Symposium on Parallelism in Algor ithms and
Architectures , pages 45–55, 2022.
[GS19] Mohsen Ghaﬀari and Ali Sayyadi. Distributed arborici ty-dependent graph coloring via
all-to-all communication. In 46th International Colloquium on Automata, Languages,
and Programming (ICALP 2019) . Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik,
2019.
[HPR22] Khalid Hourani, Gopal Pandurangan, and Peter Robin son. Awake-eﬃcient distributed
algorithms for maximal independent set. In 2022 IEEE 42nd International Conference
on Distributed Computing Systems (ICDCS) , pages 1338–1339. IEEE, 2022.
[JKZ02a] Tomasz Jurdzi´ nski, Miros/suppress law Kuty/suppress lowski, and J an Zatopia´ nski. Eﬃcient algorithms for
leader election in radio networks. In Proceedings of the twenty-ﬁrst annual symposium
on Principles of distributed computing , pages 51–57, 2002.
[JKZ02b] Tomasz Jurdzi´ nski, Miros/suppress law Kuty/suppress lowski, and J an Zatopia´ nski. Energy-eﬃcient size
approximation of radio networks with no collision detectio n. InInternational Computing
and Combinatorics Conference , pages 279–289. Springer, 2002.
[KKP13] Marcin Kardas, Marek Klonowski, and Dominik Pajak. Energy-eﬃcient leader election
protocols for single-hop radio networks. In 2013 42nd International Conference on
Parallel Processing , pages 399–408. IEEE, 2013.
[KMW16] Fabian Kuhn, Thomas Moscibroda, and Roger Wattenho fer. Local computation: Lower
and upper bounds. Journal of the ACM , 63(2), 2016.
[Lin92] Nati Linial. Locality in distributed graph algorit hms.SIAM Journal on Computing ,
21(1):193–201, 1992.
[Lub86] Michael Luby. A simple parallel algorithm for the ma ximal independent set problem.
SIAM Journal on Computing , 15:1036–1053, 1986.
[MR95] Rajeev Motwani and Prabhakar Raghavan. Randomized algorithms . Cambridge univer-
sity press, 1995.
[Pel00] David Peleg. Distributed Computing: A Locality-Sensitive Approach . SIAM, 2000.
[Pem01] Sriram V Pemmaraju. Equitable coloring extends che rnoﬀ-hoeﬀding bounds. In Approx-
imation, Randomization, and Combinatorial Optimization: A lgorithms and Techniques ,
pages 285–296. Springer, 2001.
[RG20] V´ aclav Rozhoˇ n and Mohsen Ghaﬀari. Polylogarithmic -Time Deterministic Network
Decomposition and Distributed Derandomization. In Proceedings of the 52nd Annual
ACM SIGACT Symposium on Theory of Computing , 2020.
25