JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 1
RotationDrag: Point-based Image Editing with
Rotated Diffusion Features
Minxing Luo, Wentao Cheng, and Jian Yang
Abstract —A precise and user-friendly manipulation of im-
age content while preserving image fidelity has always been
crucial to the field of image editing. Thanks to the power of
generative models, recent point-based image editing methods
allow users to interactively change the image content with high
generalizability by clicking several control points. But the above
mentioned editing process is usually based on the assumption
that features stay constant in the motion supervision step from
initial to target points. In this work, we conduct a comprehensive
investigation in the feature space of diffusion models, and find
that features change acutely under in-plane rotation. Based on
this, we propose a novel approach named RotationDrag, which
significantly improves point-based image editing performance
when users intend to in-plane rotate the image content. Our
method tracks handle points more precisely by utilizing the
feature map of the rotated images, thus ensuring precise op-
timization and high image fidelity. Furthermore, we build a in-
plane rotation focused benchmark called RotateBench, the first
benchmark to evaluate the performance of point-based image
editing method under in-plane rotation scenario on both real im-
ages and generated images. A thorough user study demonstrates
the superior capability in accomplishing in-plane rotation that
users intend to achieve, comparing the DragDiffusion baseline
and other existing diffusion-based methods. See the project
page https://github.com/Tony-Lowe/RotationDrag for code and
experiment results.
Index Terms —Point-based Image Editing,Stable Diffusion
I. I NTRODUCTION
IMAGE editing using generative models has gained great
attention and achieved splendid results [1]–[4]. It is crucial
for personalized image manipulation, e.g. changing pose,
and interactive image creation. Recently, Pan et al. [5] has
proposed DragGAN, a point-based Image editing method.
DragGAN manipulates the generated image by optimizing
latent code of StyleGAN [6]. To be more specific, DragGAN
allows users to provide pairs of handle and target points on the
image and optimizes the latent code around the handle points
towards target points one pixel-step at a time. Afterward, new
handle points are localized by searching the features of initial
ones in the updated feature map.
Although DragGAN has successfully achieved pixel-level
image manipulation, it inherits the limitation in generality
of generative adversarial networks (GANs) [7]. Numerous
attempts have been made to overcome the limitation of Drag-
GAN. Recent diffusion models [8]–[11] have shown superior
Corresponding authors: Wentao Cheng and Jian Yang.
Minxing Luo, Wentao Cheng and Jian Yang are with VCIP, CS,
the College of Computer Science, Nankai University, Tianjin, 300350,
China (e-mail: lmx@mail.nankai.edu.cn; wentaocheng@nankai.edu.cn;
csjyang@nankai.edu.cn)
(a) Original Image
 (b) User Edit
 (c) DragDiff
(d) SDE-Drag
 (e) FreeDrag
 (f) Ours
Fig. 1. Rotation Drag significantly improves the point-based editing
performance under rotation scenario . Given an input image, user provide
pairs of handle points(red), target points(blue) and mask determining the
editing region.
capacity over GANs in the area of image generation. Shi et
al.[12] propose DragDiffusion, which replaces GANs with
diffusion models for point-based image editing. DragDiffu-
sion first fine-tunes the LoRA [13] to retain image fidelity
during editing. Then the latent codes of handle points are
optimized towards the target points, using the feature map
generated by UNet [14] to supervise the motion changes. Nie
et al. [15] propose SDE-Drag that employs diffusion models
based on stochastic differential equation (SDE) instead of
commonly used probability flow ordinary differential equation
(ODE). In terms of editing, SDE-Drag raises a copy-and-
paste method, which eliminates optimization process and
copies the latent code of the handle points directly towards
the target points. Intuitively, both DragDiffusion and SDE-
Drag outperform DragGAN thanks to the superior ability
of diffusion models. Nevertheless, all the above mentioned
methods make an assumption that the feature of the handle
points remains unchanged throughout the editing process.
When preforming point tracking, they locate the new handle
point solely dependent on the feature of the initial handle
points. This leads to losing track of the handle points or
tracking the wrong regions resemble to the feature of the initial
handle points during optimization, resulting in inaccurate or
undesired editing results. To tackle this problem, Ling et al.
[16] propose FreeDrag based on GANs. FreeDrag replaces
the point tracking with fuzzy localization. During fuzzy lo-arXiv:2401.06442v1  [cs.CV]  12 Jan 2024JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 2
calization, template feature is put forward to preserve the
weighted feature of each step. Although FreeDrag outperforms
DragGAN, the template feature itself lacks interpretability and
it cannot guarantee successful localization when the feature
patch changes significantly during the dragging procedure.
Take dragging a content over the brick wall towards the
sky for example. When intersection is reached, the template
feature will gradually lose the feature of the dragging content
after numerous weighted sum, leading to optimizing incorrect
handle points.
A natural question arises: under what condition does feature
space change acutely? To answer the question and improve
the point tracking accuracy of the point-based method using
diffusion models, we first investigate the feature space of the
UNet. Tang et al. [17] find that the diffusion models possess
rich semantic and geometry information in the feature map
of UNet decoder layers. Following their work, we take a
step further to examine how diffusion models perform under
affine transformations like translation, rotation, scaling, etc and
find that the diffusion model performs poorly in the in-plane
rotation scenario. The feature map changes drastically when
the input image is rotated. Meanwhile we observe that in real
world editing, a large portion of the dragging falls into the
rotation category. For instance, users might want to wave the
tail of animals or make the leaning tower up-straight.
This observation motivates us to use feature map of rotated
images to guide the point tracking. To overcome the above
mentioned issues, we propose RotationDrag, the first point-
based image editing method utilizing feature map of the
rotated image to guide the point tracking. In the process of
point tracking, we first calculate the rotation angle between
the initial and current handle points. Then, we rotate the input
image according to the rotation angle to obtain the feature
map and use it to locate the new handle points. By utilizing
rotated image’s feature map, we ensure a more reliable and
precise point-based Image editing under rotation scenario,
therefore obtaining better optimization results (see Fig. 1). For
evaluation, we present a in-plane rotation focused dataset for
point-based editing called Rotatiobench. Both real world pho-
tographs and generated images are included to ensure diversity.
We attach drag configuration for each image, which includes
mask, prompt and coordinates of handle and target points.
Moreover, a comprehensive user study on the RotationBench
demonstrates the supremacy over the DragDiffusion baseline
and other existing diffusion-based methods under in-plane
rotation scenario.
To summarize, our key contributions are as follows:
•We propose RotationDrag, a simple yet effective point-
based image editing that utilizes feature map of rotated
image to improve the point tracking accuracy.
•We introduce RotationBench, the first benchmark dataset
focusing on evaluating point-based editing methods per-
formance under in-plane rotation scenario.
•Experiments demonstrate the advantages of RotationDrag
in point-based editing under in-plane rotation scenario.
DDIM 
Inversion Optimization 
Rotation 
DDIM 
Inversion Input Image 
Latent code aLatent code a
Optimization 
DDIM Denoiser Latent code 
Latent code aUNet Feature UNet Feature 
Point Tracking 
Final Image 
Rotated Image 
Fig. 2. Overview of RotationDrag. Given an input image, we first obtain
the latent code of the input image through DDIM [18] Inversion. Then we
optimize the latent code step-by-step. During optimization, latent code of
rotated image is used for UNet feature extraction, ensuring a more reliable
point tracking. When optimization is finished, the latent code will go through
DDIM Denoiser to restore the edited image.
II. M ETHOD
A. Preliminaries
1) Diffusion Models: Denoising diffusion probabilistic
models (DDPM) [8], [9] are in the category of latent generative
models. The idea behind DDPMs is that the data distribution x
can be considered as a Markov chain of noisy transformations,
where noise zis removed iteratively. The objective is to learn
a denoising function that can reverse the noise process and
recover the original data from a random noise input. DDPMs
define a diffusion process as follows:
xt=√αtx0+√
1−αtzt, (1)
where zt∼ N (0, I)is the random sampled noise, αtis the
noise level at step tandTis the total number of timesteps.
The denoising function is then a conditional distribution
qθ(xt−1|xt)which is parameterized by the neural network θ.
Once trained, θcan reverse the diffusion process to restore
the image data from noisy data. θis usually implemented as
UNet [14] in image generation.
2) Point-Based Image Editing: Point-based image editing
methods utilizes feature space of generative models to perform
point tracking and motion supervision. Specifically, DragDif-
fusion employ feature extracted from the 3rdupsampling block
of UNet in diffusion models to supervise the motion changes
during editing.
B. Investigation on UNet Feature map
Tang et al. [17] evaluate the UNet feature map of SD,
namely DIFT, for homography estimation using HPatches
benchmark [19]. Specifically, the feature map extracted from
the3rdupsampling block of the UNet has the strongest
geometric feature representation, which is deployed to track
handle points in DragDiffusion.
We further investigate the performance of SD on affine
transformations with specifically curated datasets. Using a
subset of HPatches, we crop the reference image to ensure
the whole transformation is performed within the image
boundaries, warp the image using exactly one type of affineJOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 3
TABLE I
HOMOGRAPHY ESTIMATION ACCURACY [%] AT3PIXELS . THEFEATURE
SPACE OF DIFFUSION ’SUN ET CHANGES DRASTICALLY UNDER IN -PLANE
ROTAION SCENARIO .
Method Scaling Rotation Perspective Translation
SIFT [20] 99.6 100.0 99.6 99.298
Superpoint [21] 97.2 26.0 98.9 98.2
DIFT [17] 95.4 37.5 98.6 97.9
transformation and categorize them accordingly. The warped
images are classified into four distinct categories: Scaling,
Rotation, Perspective, and Translation.
Following Superpoint [21], we use the corner correctness
metric for evaluation. We report the comparison of homogra-
phy accuracy in Table I. As vividly shown, DIFT exhibits
subpar performance in the in-plane rotation scenario. Our
conjecture underlying this observation is that the training
objective of SD primarily focuses on generative tasks. The
training dataset lacks in-plane rotated images. Consequently,
SD encounters challenges in accurately interpreting in-plane
rotated images due to this deficiency in its training data.
C. RotationDrag
Considering subpar performance of SD in the rotational
transformations and the majority of the dragging can be cate-
gorized into in-plane rotation in real world editing, we propose
a in-plane rotation focused approach named RotationDrag to
improve the performance of diffusion models in point-based
image editing, as shown in Fig. 2.
We first process the the user input, from which we can ob-
tain the original image I, source points {si= (xs,i, ys,i)|i=
1,2, ..., n}, target points {ti= (xt,i, yt,i)|i= 1,2, ..., n}and
the binary mask M. We consider the point as the rotational
axis if source point and handle point overlap, denoted as
{ci= (xc,i, yc,i)|i= 1,2, ..., n}. If overlapped points do not
exist, we position the rotational axis at the furthest extremity
of the line perpendicular to the dragging direction within the
mask. Then, we invert the image into the latent code ztafter
tsteps of DDIM inversion [18] and optimize it towards the
target point. Each optimization step can be divided into 2
parts: motion supervision and point tracking. We will illustrate
them in detail in the following part of this section. When
optimization is done, the processed latent code z∗
tis fed into
the DDIM denoiser to obtain the final results in image.
1) Motion Supervision: Take the handle points and opti-
mized latent code at the k-th step optimization as {hk
i=
(xk
h,i, yk
h,i)|i= 1,2, ..., n}andzk
t. The square patch around hk
i
is defined as Ω(hk
i, r1) ={(x, y)||x−xk
h,i| ≤r1,|y−yk
h,i| ≤
r1}. The goal of the optimization is to optimize the patch
around handle points towards the target point, while keeping
the content outside the mask intact. We supervise the optimiza-
tion using the latent feature map Fzk
twith strong geometric
representation extracted from UNet. Same as DragDiffusion
[12], the motion supervision can be formulated as follows:
Lmotion =nX
i=0X
qi∈Ω(hk
i,r1)∥F(zk
t, qi)−F(zk
t, qi+di)∥1
+λ∥(zk
t−1−zt−1)(1−M)∥1,(2)where F(zk
t, qi)denotes the feature values of latent zk
tat pixel
qi,di=ti−hk
i
∥ti−hk
i∥2is the normalized directional vector pointing
from hk
itowards ti.
2) Point Tracking: To ensure the dragging is performed
on the desired direction, the location of the handle points
needs to be updated together with the latent as well. After
each optimization on latent, we need to relocate the handle
points. Dragdiffusion [12] used the UNet feature maps of
the updated latent code and the original latent code to track
new handle points. As explained in Sec. II-B, UNet features
change immensely under in-plane rotational change, resulting
in finding the incorrect handle points. To tackle this problem,
we need to recognize UNet feature representation of the latent
around the handle points under the current rotation angle.
The simplest way is to rotate the input image accordingly
to generate feature map for more precise point tracking. We
conduct another DDIM inversion to obtain the latent of the
rotated image and used it to extract feature vectors of the
rotated source points. The angle of the rotation for handle
point hk
iis computed as follows:
θk
i= arctan(yh,i−yc,i
xh,i−xc,i)−arctan(ys,i−yc,i
xs,i−xc,i).(3)
The final result of image rotation is independent of the rotation
axis, and even if the rotated source point does not coincide
with the handle point, it is permissible to perform rotation
solely based on the angle due to minor effects translation had
on UNet feature. The rotated image and the corresponding
source point is denoted as(r)I,(r)si. The t-th inversion step
latent of the rotated image is denoted as(r)zt. We use the UNet
feature maps F((r)zt)andF(zk+1
t)to track the new handle
points. To update the handle points, a nearest neighbour search
of the handle points is performed as follows:
hk+1
i= arg min
q∈Ω(hk
i,r2)∥F(zk+1
t, q)−F((r)zt,(r)si)∥.(4)
III. E XPERIMENTS
A. Implementation Details and Dataset
Following DragDiffusion [12], We use Stable Diffusion 1.5
[10] as our diffusion model and finetune LoRA [13] before the
optimization. During optimization, we adopt Adam optimizer
[22] with a learning rate of 0.01 to optimize the latent code.
The hyper parameters are set to be r1= 1, r2= 3, λ= 0.1. In
our implementation, the optimization stops when the distance
between the handle points and the corresponding target points
is smaller than 2 pixels. The maximum optimization step is
160.
Since point-based image editing is a relatively new method
and few people focused on in-plane rotation, the existing
benchmark cannot illustrate the performance of the editing
method under in-plane rotation properly. Therefore we present
RotationBench, a dataset built upon both real world and
generated images focusing on in-plane rotation. We provide
drag configuration file for every image, including binary mask,
prompt and points coordinates for dragging.JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 4
(a) Input Image
 (b) Dragdiff
 (c) FreeDrag
 (d) SDE-Drag
 (e) Ours
Fig. 3. Visual comparison between DragDiffusion, FreeDrag(our diffusion implementation version), SDE-Drag and RotationDrag. The left column displays
the input images, while columns in the right displays editing results of DragDiffusion, Diffuison version FreeDrag of our implementation, FreeDrag, SDE-Drag
and RotationDrag respectively.
Fig. 4. Results of user study. RotationDrag outperforms all competitors by a
large margin.
B. Experimental Results
1) Qualitative Results: Fig. 3 shows the qualitative com-
parsion between DragDiffusion, FreeDrag(our diffusion im-
plementation version), SDE-Drag and our method. As shown
in the figure, our RotationDrag accurately moves the handle
points towards target points and achieves highest quality on
both real-world ( 1strow of Fig. 3), art ( 2ndrow of Fig. 3)
and generated images ( 3rdrow of Fig. 3). Specifically, when
rotational axis is not given in drag instructions, our method
can not only produce content closer to the destination but also
maintain reasonable image content as shown in the 2ndrow
of Fig. 3. These results demonstrate the superior ability of
RotationDrag to maintain the image fidelity while performing
intense dragging or rotating.
2) User Study: There were 20 participants and 25 questionsfor each comparison between DragDiffusion, FreeDrag, SDE-
Drag and our method by default. In each question, participants
were presented with the original image, the user editing
including handle points, target points and masks on the original
image and 4 edited images produced by distinct models. The
participants were asked to select the best editing result in the 4
edited images. As shown in Fig. 4, RotationDrag outperforms
all competitors by a large margin.
3) Discussion: Even though our method has achieved re-
markable results under in-plane rotation scenario, it is still
slower than original DragDiffusion. The reason behind this
phenomenon is that inversion is performed in each point
tracking procedure. Weak performance in rotation of the Stable
Diffusion network is the main cause for this approach. We did
try to rotate the latent code to produce a rotated image, only
to produce distorted images. We will explore the possibility of
teaching stable diffusion for a better knowledge of rotation.
IV. C ONCLUSION
In this work, we investigate the UNet features map’s
performance on affine transformation and report the subpar
performance under in-plane rotation scenario. Based on this
insight, we propose RotationDrag to boost the performance
of Stable Diffusion under in-plane rotation scenario in inter-
active point-based image editing. By utilizing feature map of
rotated images, we track better point movement and achieve
high quality dragging results. Experiments are conducted to
demonstrate the superiority of RotationDrag when dealing in-
plane rotation.JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 5
REFERENCES
[1] D. Roich, R. Mokady, A. H. Bermano, and D. Cohen-Or, “Pivotal tuning
for latent-based editing of real images,” ACM Transactions on graphics
(TOG) , vol. 42, no. 1, pp. 1–13, 2022.
[2] B. Kawar, S. Zada, O. Lang, O. Tov, H. Chang, T. Dekel, I. Mosseri, and
M. Irani, “Imagic: Text-based real image editing with diffusion models,”
inProceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , 2023, pp. 6007–6017.
[3] T. Brooks, A. Holynski, and A. A. Efros, “Instructpix2pix: Learning
to follow image editing instructions,” in Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition , 2023, pp.
18 392–18 402.
[4] B. Yang, S. Gu, B. Zhang, T. Zhang, X. Chen, X. Sun, D. Chen, and
F. Wen, “Paint by example: Exemplar-based image editing with diffusion
models,” in Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , 2023, pp. 18 381–18 391.
[5] X. Pan, A. Tewari, T. Leimk ¨uhler, L. Liu, A. Meka, and C. Theobalt,
“Drag your gan: Interactive point-based manipulation on the generative
image manifold,” in ACM SIGGRAPH 2023 Conference Proceedings ,
2023, pp. 1–11.
[6] T. Karras, S. Laine, and T. Aila, “A style-based generator architecture
for generative adversarial networks,” in Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition , 2019, pp. 4401–
4410.
[7] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,
S. Ozair, A. Courville, and Y . Bengio, “Generative adversarial networks,”
Communications of the ACM , vol. 63, no. 11, pp. 139–144, 2020.
[8] J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli,
“Deep unsupervised learning using nonequilibrium thermodynamics,”
inInternational conference on machine learning . PMLR, 2015, pp.
2256–2265.
[9] J. Ho, A. Jain, and P. Abbeel, “Denoising diffusion probabilistic models,”
Advances in neural information processing systems , vol. 33, pp. 6840–
6851, 2020.
[10] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, “High-
resolution image synthesis with latent diffusion models,” in Proceedings
of the IEEE/CVF conference on computer vision and pattern recognition ,
2022, pp. 10 684–10 695.
[11] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton,
K. Ghasemipour, R. Gontijo Lopes, B. Karagol Ayan, T. Salimans
et al. , “Photorealistic text-to-image diffusion models with deep language
understanding,” Advances in Neural Information Processing Systems ,
vol. 35, pp. 36 479–36 494, 2022.
[12] Y . Shi, C. Xue, J. Pan, W. Zhang, V . Y . Tan, and S. Bai, “Dragdiffusion:
Harnessing diffusion models for interactive point-based image editing,”
arXiv preprint arXiv:2306.14435 , 2023.
[13] E. J. Hu, Y . Shen, P. Wallis, Z. Allen-Zhu, Y . Li, S. Wang, L. Wang,
and W. Chen, “Lora: Low-rank adaptation of large language models,”
arXiv preprint arXiv:2106.09685 , 2021.
[14] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks
for biomedical image segmentation,” in Medical Image Computing
and Computer-Assisted Intervention–MICCAI 2015: 18th International
Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III
18. Springer, 2015, pp. 234–241.
[15] S. Nie, H. A. Guo, C. Lu, Y . Zhou, C. Zheng, and C. Li, “The blessing
of randomness: Sde beats ode in general diffusion-based image editing,”
arXiv preprint arXiv:2311.01410 , 2023.
[16] P. Ling, L. Chen, P. Zhang, H. Chen, and Y . Jin, “Freedrag: Point
tracking is not you need for interactive point-based image editing,” arXiv
preprint arXiv:2307.04684 , 2023.
[17] L. Tang, M. Jia, Q. Wang, C. P. Phoo, and B. Hariharan, “Emergent
correspondence from image diffusion,” in Thirty-seventh Conference
on Neural Information Processing Systems , 2023. [Online]. Available:
https://openreview.net/forum?id=ypOiXjdfnU
[18] J. Song, C. Meng, and S. Ermon, “Denoising diffusion implicit models,”
inInternational Conference on Learning Representations , 2020.
[19] V . Balntas, K. Lenc, A. Vedaldi, and K. Mikolajczyk, “Hpatches: A
benchmark and evaluation of handcrafted and learned local descriptors,”
inProceedings of the IEEE conference on computer vision and pattern
recognition , 2017, pp. 5173–5182.
[20] D. G. Lowe, “Distinctive image features from scale-invariant keypoints,”
International journal of computer vision , vol. 60, pp. 91–110, 2004.
[21] D. DeTone, T. Malisiewicz, and A. Rabinovich, “Superpoint: Self-
supervised interest point detection and description,” in Proceedings
of the IEEE conference on computer vision and pattern recognition
workshops , 2018, pp. 224–236.[22] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”
arXiv preprint arXiv:1412.6980 , 2014.