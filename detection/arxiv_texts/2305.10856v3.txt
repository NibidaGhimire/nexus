C. WANG et al. : SPATIAL-FREQUENCY DISCRIMINABILITY FOR REVEALING ADVERSARIAL PERTURBATIONS 1
Spatial-Frequency Discriminability for Revealing
Adversarial Perturbations
Chao Wang, Shuren Qi, Zhiqiu Huang, Yushu Zhang, Senior Member, IEEE ,
Rushi Lan, Xiaochun Cao, Senior Member, IEEE , and Feng-Lei Fan Senior Member, IEEE
Abstract —The vulnerability of deep neural networks to adver-
sarial perturbations has been widely perceived in the computer
vision community. From a security perspective, it poses a critical
risk for modern vision systems, e.g., the popular Deep Learning
as a Service (DLaaS) frameworks. For protecting deep models
while not modifying them, current algorithms typically detect
adversarial patterns through discriminative decomposition for
natural and adversarial data. However, these decompositions are
either biased towards frequency resolution or spatial resolution,
thus failing to capture adversarial patterns comprehensively.
Also, when the detector relies on few fixed features, it is practical
for an adversary to fool the model while evading the detector ( i.e.,
defense-aware attack). Motivated by such facts, we propose a dis-
criminative detector relying on a spatial-frequency Krawtchouk
decomposition. It expands the above works from two aspects:
1) the introduced Krawtchouk basis provides better spatial-
frequency discriminability, capturing the differences between
natural and adversarial data comprehensively in both spatial
and frequency distributions, w.r.t. the common trigonometric
or wavelet basis; 2) the extensive features formed by the
Krawtchouk decomposition allows for adaptive feature selection
and secrecy mechanism, significantly increasing the difficulty
of the defense-aware attack, w.r.t. the detector with few fixed
features. Theoretical and numerical analyses demonstrate the
uniqueness and usefulness of our detector, exhibiting competitive
scores on several deep models and image sets against a variety
of adversarial attacks.
Index Terms —Adversarial example, detection, orthogonal de-
composition, spatial-frequency.
I. I NTRODUCTION
MODERN deep neural networks (DNN) are highly dis-
criminative representation techniques, exhibiting im-
pressive performance in extensive scenarios ranging from per-
ceptual information understanding to hard scientific problem
This work was supported in part by the Joint Funds of the National Natural
Science Foundation of China under Grant U2241216, and in part by the
Youth Talents Support Project of Guangzhou Association for Science and
Technology. (Corresponding authors: S. Qi and Z. Huang .)
C. Wang, S. Qi, Z. Huang, and Y . Zhang are with the College of Computer
Science and Technology, Nanjing University of Aeronautics and Astronautics,
Nanjing 211106, China; S. Qi is also with the Department of Mathematics,
The Chinese University of Hong Kong, Hong Kong (e-mail: c.wang, shurenqi,
zqhuang, yushu@nuaa.edu.cn).
R. Lan is with the Guangxi Key Laboratory of Image and Graphic
Intelligent Processing, Guilin University of Electronic Technology, Guilin
541004, China (e-mail: rslan@guet.edu.cn).
X. Cao is with the School of Cyber Science and Technology, Shenzhen
Campus of Sun Yat-sen University, Shenzhen 518107, China (e-mail: caoxi-
aochun@mail.sysu.edu.cn).
F. Fan is with the Department of Mathematics, The Chinese University of
Hong Kong, Hong Kong (e-mail: flfan@math.cuhk.edu.hk).
Copyright © 2024 IEEE. Personal use of this material is permitted.
However, permission to use this material for any other purposes must be
obtained from the IEEE by sending an email to pubs-permissions@ieee.org.deciphering [1]. Besides their effectiveness, the emerging Deep
Learning as a Service (DLaaS) paradigm allows developers
to apply or deploy proven DNN models in a simple and
efficient manner [2]. The above two factors have led to
the widespread emergence of deep learning-based artificial
intelligence systems in everyday life, even expanding to many
security and trust sensitive scenarios [3].
On the other hand, the robustness of DNN models has raised
general concerns, especially in the computer vision community
[4]. It has been shown that adversarial perturbations on the
input example can cause significant fluctuations on such deep
representations, even though such perturbations are almost
imperceptible for humans [5].
Since the first work by Szegedy et al. [6], various attack
methods have been proposed for well crafting such adversarial
perturbations. In general, The goals of attackers cover high
fooling rate [7], low perceptual loss [8], efficient generation
[9], high transferability w.r.t. models [10], high universality
w.r.t. examples [11], less need for model knowledge [12],
easy physical implementation [13], etc. Their applications have
expanded from classical classification tasks to semantic seg-
mentation [14], object tracking [15], low-level processing [16],
security [17], forensic [18], privacy [19] and so on. Hence,
these recent advances allow an adversary to perform effective
evasion at a low cost, which interferes the deep representation
and thus fundamentally destabilizes the system. In addition,
the popularity of DLaaS-based development further increases
this security threat. Specifically, a successful attack on few
off-the-shelf deep models in DLaaS platform will affect a
wide range of hosts and their systems, especially where safety-
critical scenarios may be involved.
Integrating the above facts, it is generally agreed that
adversarial perturbations have become a real threat that the
security community has to face.
A. State of the Arts
For protecting deep neural networks and their application
systems, a plethora of defense strategies towards adversarial
attacks are designed over diverse research hypotheses and
implementation paths.
The straightforward strategy is to include adversarial exam-
ples into the training, called adversarial training , which pro-
motes the model learning and adapting to adversarial patterns
[20]. Such data-level defenses are intuitively and empirically
effective, but the retraining increases the implementation cost,
and the resulting robustness cannot be adaptively/interpretably
generalized to unseen adversarial patterns [21].arXiv:2305.10856v3  [cs.CV]  7 Aug 2024C. WANG et al. : SPATIAL-FREQUENCY DISCRIMINABILITY FOR REVEALING ADVERSARIAL PERTURBATIONS 2
Clean example setAdversarial perturbed 
example setTraining Phase
n
m
Px
Py
Krawtchouk transform with key
Frequency -band integration
and feature enhancement
SVM predictionCleanPerturbed
Adversarial Example Detector 
An example 
under analysis Inference Phase
Adversarial example detector
with key and leaned parameters
Adversarial example 
detected, service denied
Adversarial example
not detected , service offered 
(data randomization optional) 
 Securing DLaaS from
adversarial example
Fig. 1. Illustration for the training phase of the proposed adversarial example detector. The detector is trained on a set of adversarial/clean image examples
along with corresponding labels. The detector consists of three main steps: 1) the image is projected into a space defined by Krawtchouk polynomials, where
the frequency parameters (n, m)and spatial parameters (Px, Py)are determined by key; 2) the obtained coefficients are integrated and enhanced to form a
compact-but-expressive feature vector by certain beneficial priors; 3) such features are fed into an SVM for the prediction, which is the only learning part in
the detector.
Clean example setAdversarial perturbed 
example setTraining Phase
n
m
Px
Py
Krawtchouk transform with key
Frequency -band integration
and feature enhancement
SVM predictionCleanPerturbed
Adversarial Example Detector 
An example 
under analysis Inference Phase
Adversarial example detector
with key and leaned parameters
Adversarial example 
detected, service denied
Adversarial example
not detected , service offered 
(data randomization optional) 
 Securing DLaaS from
adversarial example
Fig. 2. Illustration for the inference phase of the proposed adversarial example detector. When the detector is trained ( i.e., with the key and learned SVM
parameters), it can be deployed in various real-world scenarios, where a DLaaS scenario is chosen as an example. For an image under analysis, our detector
predicts whether it contains adversarial perturbations. With such prediction, the DLaaS is able to deny the service when the adversarial perturbation is revealed.
Another popular strategy seeks architecture-level designs as
defenses, e.g.,regularization structure [22]–[24] and certified
robustness [25]–[27], embedding adversarial robustness priors
into networks. Ideally, such built-in designs have the potential
to achieve adaptive/interpretable robustness against adversarial
examples. Yet, in practice, tricky trade-offs between clean and
robust accuracy are almost inevitable [28], and the additional
cost for resetting DLaaS is also significant.
In contrast to the above direct attempts on robustness
enhancement, the detection-only approach [29] serve as a pre-
processing for deep models to filter out potential adversarial
examples, without modifying the model itself. Such an adver-
sarial perturbation detector can reduce both clean-accuracy
loss and additional implementation costs w.r.t. adversarial
training and architecture-level design, being particularly suit-
able for DLaaS scenarios. Technically, such forensic detectors
strongly relies on proper representation of the adversarial
patterns while not being disturbed by the image contents, i.e.,
the discriminative decomposition of natural-artificial data [30],
[31].
Here, the priori knowledge of adversarial perturbation is po-
tentially useful to achieve such discriminative decomposition.
As a classical prior, certain visual loss regularization forces the
learning perturbation to appear mainly in high frequency bands
where the human visual system is less sensitive. From thisassumption, researchers have tried to extract high-frequency
clues with few fixed features and non-orthogonal decompo-
sition, such as denoising filters [32] and Spatial Rich Model
(SRM) [33]. However, such detectors have been found to be
no longer accurate and secure due to the emergence of low-
frequency adversarial perturbations [34]. Therefore, a series of
orthogonal decompositions are introduced, such as Principal
Component Analysis (PCA) [35], Discrete Cosine Transform
(DCT) [36], Discrete Sine Transform (DST) [37], and Dis-
crete Wavelet Transform (DWT) [37], capturing distribution
differences in a wider sense than just high frequencies. (See
Appendix A for a comprehensive review of related works.)
B. Motivations
This line of adversarial perturbation detectors still falls short
in accuracy and security, especially for real-world scenarios.
•For the accuracy , they are able to provide good detection
performance when the scale of either the image patterns
or the perturbation patterns is limited ( e.g., MNIST with
one attack), while the accuracy degrades significantly
when both are large [38]. This phenomenon implies that
existing representation methods fail to capture adversarial
patterns comprehensively.
•As for the security , the successful defense-aware attack
that evades the detector and fools the model, is realisticC. WANG et al. : SPATIAL-FREQUENCY DISCRIMINABILITY FOR REVEALING ADVERSARIAL PERTURBATIONS 3
when the detector relies on few fixed features. Techni-
cally, new perturbation can be learned by penalizing the
specific features that the detector focuses on.
C. Contributions
Motivated by above facts, we attempt to present a more
accurate and secure adversarial example detector, technically
enabled by a spatial-frequency discriminative decomposition,
as shown in Figs. 1 and 2. To the best of our knowledge, this
is a very early work on improving both accuracy and security
of detector from the fundamental decomposition stage.
•Regarding the accuracy , we attribute the failure at larger
scales to the contradiction of spatial and frequency dis-
criminability in the decomposition, surprisingly revisiting
a classical problem of signal processing [39]. Recently,
Agarwal et al. [37], [38] preliminarily explored this
fundamental problem, where a decision-level fusion of
the DST (biased towards frequency) and DWT (biased
towards spatial) was designed to mitigate such contradic-
tion. In this paper, we introduce Krawtchouk polynomials
as basis functions for the discriminative decomposition,
providing a mid-scale representation different from the
global trigonometric basis in DST and the local wavelet
basis in DWT. Note that such a representation with rich
spatial-frequency information can provide more clues
of adversarial patterns for the prediction, being a more
flexible detector than the decision-level fusion [37], [38].
•Regarding the security , we attribute the defense-aware
attack to the accessibility of such few fixed features for
the adversary, as a foundational threat in some existing
methods. In this paper, we propose a confusion strategy
for the adversary based on random feature selection [40].
More specifically, a pseudorandom number generator
determines the decomposition parameters, and the host
controls such generator by setting the secret keys ( i.e.,
the seed values). After that, the defense-aware attack
becomes more difficult as the confusion of the boundary
between to-be-attacked and to-be-evaded features.
II. G ENERAL FORMULATION
In this section, we formulate the basic aspects involved in
this paper, i.e., model, attack, and defense.
A. Model Formulation
We focus on deep convolutional neural network models
for image classification tasks. Such classification models can
be formulated as a mapping M:X → Y , where X ⊂
[0,1]W×H×Cis the image space with the image size W×H×
Cand normalized pixel intensity [0,1];Y={1,2, ..., N}is
the label space with the category size N. For a clean data point
(x,y)∈ X × Y , the classification model Mis considered to
be correct if and only if M(x) =y.B. Attack Formulation
Attack objective. We focus on evasion attacks against the
above image classification models. For an image xwith the
true label y, the goal of the attacker is to find an adversarial
perturbation δsuch that
M(x+δ)̸=y, (1)
i.e., fooling the prediction, typically under a norm-based
constraint for the imperceptibility of perturbation:
||δ||< ε. (2)
The resulting perturbed input for the model, i.e., the ad-
versarial example, is denoted as x′=x+δ∈ X . Note that
the above attack objective is a formulation for the defense-
unaware scenario, in line with the threat assumption of most
related works. As for the defense-aware attack, the goal of
the attacker include the fooling of the adversarial perturbation
detector: D(x′) =D(x), in addition to the misclassifica-
tion and imperceptibility above. Here, we denote the above
defense-unaware and defense-aware attacks as A(x) = δ.
More detailed formulation on attacker and defender will be
provided later.
Knowledge and capability. For the defense-unaware sce-
nario, the attacker has perfect knowledge of the image clas-
sification model M(i.e., full access to its mechanism and
parameters), but has zero knowledge of the detector D(or
not aware of its presence). For the defense-aware scenario,
the attacker likewise has perfect knowledge of the model M
and has limited knowledge of the detector D— the attacker
is aware that the given model Mis being secured with a
detector. For both scenarios, the attacker has the capability to
arbitrarily modify pixels within a given image x.
C. Defense Formulation
Defense objective. The goal of our defense is to design an
adversarial perturbation detector Dsuch that D(x) = 0 (i.e.,
predicted as clean example) and D(x′) = 1 (i.e., predicted
as adversarial example) for any clean image x∈ X and
corresponding adversarial example x′by any pertinent A. In
other words, since xandx′differ only in δ, the above binary
classification task is practically equivalent to a hypothesis
testing for the presence of adversarial patterns ( w.r.t. δ) under
the strong interference from image content ( w.r.t.x).
III. T OWARDS ACCURATE AND SECURE DETECTOR :
FORMULATION
In this section, we provide new formal analyses of the
concerned accuracy and security issues, as the theoretical basis
for the proposed detector.
A. Discriminability Analysis
The design of an effective detector Drelies heavily on a
discriminative decomposition Fw.r.t. xandδinx′. Such
decomposition can be formulated as a mapping F:X → C ,
where Xis the image space and Cis the space of decompo-
sition coefficients.C. WANG et al. : SPATIAL-FREQUENCY DISCRIMINABILITY FOR REVEALING ADVERSARIAL PERTURBATIONS 4
To achieve a high discriminability, two constraints are
typically imposed on the explicit forms of F.
Formulation 1. (Distributive Property of Addition). F
should be distributed over addition to fulfill: F(x′) =F(x+
δ) =F(x)+F(δ),i.e., the decomposition of the adversarial
example is equivalent to the sum of the decomposition of the
clean example and the perturbation.
The distributive property of addition facilitates the separa-
tion of natural-artificial data, and the operations like filtering
[32], convolution [33], and inner product [36], [37] from
successful detectors satisfy this property.
Formulation 2. (Statistical Regularity). F(x)is expected
to exhibit a consistent statistical pattern in Cfor any clean
image x∈ X, andF(δ)is also expected to exhibit another
consistent statistical pattern in Cfor any δby pertinent A;
meanwhile, such two statistical patterns should be signifi-
cantly different.
Here, we provide only a general description for the reg-
ularity. However, more specific assumptions about statistical
regularity are needed in order to design Fexplicitly.
Next, we introduce two conjectures on the frequency and
spatial distribution patterns of natural images and adversarial
perturbations, as the main assumptions of our work. Note
that such intuitive conjectures have general recognition in the
research community [32], [33], [37], despite the lack of serious
proofs.
Conjecture 1. (Frequency Pattern). Natural images and
adversarial perturbations have different frequency distribu-
tions. For natural images, their local smoothness and non-
local self-similarity lead to the dominance of low-frequency
components, with the power law of frequency-domain energy
[41]. For adversarial perturbations, the distribution of their
frequency-domain energies very likely does not obey the
same power law, since there is no term in the typical
generation to directly control the frequency distribution of
the perturbations [42], [43].
Conjecture 2. (Spatial Pattern). Natural images and ad-
versarial perturbations have different spatial distributions.
For natural images, each of their frequency components
is distributed in spatial plane with nonrandom structures
[44]. For adversarial perturbations, the distribution of their
frequency components very likely does not obey the same
structure, since there is no term in the typical generation to
directly control the spatial distribution of the perturbations
[42], [43].
Remark. With Conjectures 1 and 2, we note that both
frequency and spatial properties are of interest in the design of
F. More specifically, discriminative features should be formed
in such frequency and spatial ranges where the distribution
patterns of xandδdiffer, e.g., a naturally smooth region but
with artificial high-frequency perturbations. Therefore, Fis
expected to analyze above frequency and spatial differences
with sufficient resolution.However, it is also well known that there is a trade-off
between frequency and spatial resolutions of the orthogonal
transform. In the related works, global transforms such as
DST [37] bias towards frequency resolution, at the cost of
spatial resolution. As the opposite, local transforms such
as DWT [37] bias towards spatial resolution, at the cost
of frequency resolution. Therefore, neither can provide rich
spatial-frequency information.
Motivated by above facts, we introduce a mid-scale repre-
sentation based on Krawtchouk polynomials, which provides a
good trade-off between spatial and frequency resolutions than
global/local transforms.
B. Security Analysis
Suppose the adversarial perturbations generated by (1) and
(2) have a strong response (with main energy) on a subset
of coefficients: CA⊂ C , and the detector-interested (with
higher weights) subset of coefficients is denoted as CD⊂ C.
Therefore, the effectiveness of the detector Dis in fact built on
the intersection CA∩ CD, where we denote the corresponding
coefficient subset for a perturbation δasFCA∩CD(δ).
Formulation 3. (Defense-aware Attack: General Objec-
tive). With the above assumptions and notations, the objective
of the defense-aware attack can be modeled on the correla-
tionρas
ρ(FCA∩CD(δold),FCA∩CD(δnew))< η, (3)
for an image x, with also objectives (1) and (2), where δoldis
the generated perturbation in the defense-unaware scenario,
i.e., by only (1) and (2), and δnewis the perturbation being
generated.
Formulation 4. (Defense-aware Attack: A Special Case). In
practice, objective (3) can be converted into another easily
implemented objective — directly shifting the main energy of
the perturbation δnewout of CA∩ CD:
||FCA∩CD(δnew)||< λ, (4)
such an objective allows defense-aware perturbations to form
on the relative complement C\(CA∩CD), hence destroying the
consistent pattern of F(δold)andF(δnew)on the adversarial
and detector-interested CA∩ CD.
Remark. Note that the above new objectives in the defense-
aware scenario, i.e., (3) or (4), rely in fact on the sufficient
knowledge of features CA∩ CD. For the related works with
few fixed features [32], [33], the adversary is able to easily
identify the critical CA∩CDand hence successfully evade the
detector Das well as fool the model Mby the objectives (1)
and (2) with (3) or (4).
Motivated by above facts, we introduce a secrecy strategy
onFby random feature selection with keys, which confuses
the boundary between to-be-attacked features C\(CA∩CD)and
to-be-evaded features CA∩ CD.C. WANG et al. : SPATIAL-FREQUENCY DISCRIMINABILITY FOR REVEALING ADVERSARIAL PERTURBATIONS 5
 
  0 25 50 75 99-0.3-0.2-0.100.10.20.3
(a)P= 0.25
 
  0 25 50 75 99-0.3-0.2-0.100.10.20.3 (b)P= 0.5
 0 25 50 75 99-0.3-0.2-0.100.10.20.3 (c)P= 0.75
Fig. 3. Illustration for the weighted Krawtchouk polynomials ¯Kl(z;P, L)withl={2,4,8},P={0.25,0.5,0.75}, andL= 100 . Note that the number and
location of zeros of ¯Kcan be adjusted explicitly by landP, respectively, meaning the time-frequency discriminability of the represented image information.
IV. T OWARDS ACCURATE AND SECURE DETECTOR :
METHODOLOGY
In this section, we specify the proposed detector against ad-
versarial perturbation. We will first give an overview drawing
a high-level intuition for readers, and then the main techniques
within the methodology are presented separately.
A. Overview
In general, the proposed detector consists of three main
steps, going through training and inference phases.
As shown in Fig. 1, the training of our detector aims to fit a
mapping from a set of adversarial/clean image examples to the
corresponding labels. For an efficient mapping, our detector
is equipped with three steps that perform decomposition,
featuring, and classification, respectively.
•Regarding the decomposition , the image is projected into
a space defined by Krawtchouk polynomials, in which
the clean image xand adversarial perturbation δare
better separable from x′(see also the discriminability
analysis in Section III-A). As for security, the frequency
parameters (n, m)and spatial parameters (Px, Py)are
determined by the host key, which is considered as
an opaque mechanism for decomposition (see also the
security analysis in Section III-B). The description on
decomposition will be presented in Section IV-B.
•Regarding the featuring , a compact-but-expressive feature
vector is formed on the obtained decomposition coef-
ficients. Here, the statistical regularity in the frequency
and spatial domains are introduced, where coefficients are
integrated and enhanced as features by means of above
beneficial priors. The description on featuring will be
presented in Section IV-C.
•Regarding the classification , the above features are fed
into a Support Vector Machine (SVM) for an automatic
two-class separation of feature space. Note that the de-
composition and featuring are non-learning, and the SVM
is the only learning part of the detector. The description
on classification will be presented in Section IV-D.
As shown in Fig. 2, the trained detector is an accurate
and secure defense tool against adversarial attacks, technically
enabled by a spatial-frequency discriminative decomposition
with secret keys. It can be deployed in various real-world
scenarios, where a DLaaS scenario is chosen as an example.For an image under analysis, our detector (with the same key
and SVM parameters in training) predicts whether it contains
adversarial perturbations. By such prediction, the DLaaS is
able to deny the service when the adversarial perturbation is
revealed.
B. Spatial-frequency Discriminative Decomposition with Se-
cret Keys
Our defense framework starts with a spatial-frequency dis-
criminative decomposition of input examples. In this subsec-
tion, we discuss the explicit definition of such a decomposi-
tion, as well as the security enhancement strategy with secret
keys.
Definition 1. (Orthogonal Decomposition). The orthogonal
decomposition of an image function f∈ X , denoted as F,
is defined as the inner product of the image function fand
the basis function V[45]:
F(f) =ZZ
DV∗
nm(x, y)f(x, y)dxdy, (5)
where the frequency parameters (n, m)∈Z2, the domain of
basis function D⊂ {(x, y)∈R2}, the asterisk ∗denotes
the complex conjugate. Here, the basis function Vsatisfy
orthogonality over the domain Das
ZZ
DVnm(x, y)V∗
n′m′(x, y)dxdy =δnn′δmm′, (6)
where δis the Kronecker delta function: δab= [a=b].
With Definition 1, one can note that the orthogonal de-
composition methods used in existing detectors all have the
form (5) and (6), and their difference lies in the definition
of the basis function V. For example, the DST and DWT
in the detector of Agarwal et al. [37] are with the global
trigonometric basis and local wavelet basis, respectively.
In this paper, we define a mid-scale basis by Krawtchouk
polynomials for a better trade-off between spatial and fre-
quency resolutions. In [46], Yap et al. introduced a new set of
decompositions from the weighted Krawtchouk polynomials.
Here, the orthogonality of the weighted Krawtchouk polyno-
mials ensures information preservation and decorrelation; also
no numerical approximation is involved, since the weighted
Krawtchouk polynomials are discrete like digital images. Such
properties are what make it a good choice for this paper.C. WANG et al. : SPATIAL-FREQUENCY DISCRIMINABILITY FOR REVEALING ADVERSARIAL PERTURBATIONS 6
Definition 2. (Krawtchouk Basis). The Krawtchouk basis
function V[46] is defined as
Vnm(x, y) =¯Kn(x;Px, W)¯Km(y;Py, H), (7)
where domain of basis function D={(x, y)∈[0,1, ..., W ]×
[0,1, ..., H ]}with image size W×H, frequency parameters
(n, m)∈[0,1, ..., W ]×[0,1, ..., H ], spatial parameters
(Px, Py)∈(0,1)2, and weighted Krawtchouk polynomials
¯Kare defined as
¯Kl(z;P, L) =vuuuutL
z
Pz(1−P)L−z
(−1)l(1−P
P)ll!Γ(−L)
Γ(l−L)
·2F1(−l,−z;−L;1
P),(8)
where hypergeometric function 2F1is defined as
2F1(a, b;c;d) =∞X
k=0(a)k(b)k
(c)kdk
k!, (9)
with Pochhammer symbol: (a)k= Γ( a+k)/Γ(a). For
more efficient computation of (8), i.e., avoiding the infinite
summation in (9), we introduce the recursive formula for
weighted Krawtchouk polynomials ¯K:
¯Kl+1=q
(1−P)(l+1)
P(L−l)(LP−2lP+l−z)¯Kl
P(l−L)
−q
(1−P)2(l+1)l
P2(L−l)(L−l+1)l(1−P)¯Kl−1
P(l−L),(10)
with initial items ¯K1and¯K0:
¯K1(z;P, L) = (1 −z
PL)¯K0, (11)
¯K0(z;P, L) =sL
z
Pz(1−P)L−z. (12)
By substituting the basis of Definition 2 into the decom-
position of Definition 1, we have formulated the Krawtchouk
decomposition, which is fundamental in our detector.
1) Discriminability Analysis: Next, we will discuss the key
property of Krawtchouk decomposition, i.e., time-frequency
discriminability, and its role in the detection of adversarial
perturbation.
Property 1. (Time-frequency Discriminability). The fre-
quency and spatial properties of the represented image
information by Krawtchouk decomposition can be controlled
with the frequency parameters (n, m)and spatial parameters
(Px, Py), respectively [46].
Remark. In the study of image representation, it has been
found that the frequency and spatial properties of orthogonal
decomposition rely on the number and location of zeros of
the basis functions, respectively [47]. Specific to this paper,
the core of time-frequency discriminability in Krawtchouk
decomposition is that the number and location of zeros can
be adjusted explicitly by (n, m)and(Px, Py), respectively:•The number of zeros of the 1D ¯Kl(z;P, L)is propor-
tional to l. As for the 2D Vnm(x, y) = ¯Kn(x)¯Km(y),
the similar conclusion holds w.r.t. thenandmat the
x-direction and y-direction, respectively..
•The location of zeros of the 1D ¯Kl(z;P, L)is biased
towards 0 when P < 0.5, uniform when P= 0.5, and
biased towards 1 when P > 0.5, where the more devi-
ation of Pfrom 0.5 is, the more biased the distribution
of zeros is. As for the 2D Vnm(x, y) =¯Kn(x)¯Km(y),
the similar conclusion holds w.r.t. thePxandPyat the
x-direction and y-direction, respectively.
In Fig. 3, we illustrate 1D plots of weighted Krawtchouk
polynomials ¯Kl(z;P, L)for a high-level intuition of such
time-frequency discriminability. Here, the plots under different
parameter settings: l={2,4,8}andP={0.25,0.5,0.75}
with L= 100 . As can be expected, changing lwill change
the number of zeros of ¯K, which in turn corresponds to a
change in the frequency properties. As for P, the change of
its value will change the distribution of zeros, which in turn
corresponds to a change in the spatial properties. The 2D plots
ofVnm(x, y)w.r.t. (n, m)and(Px, Py)are given in Fig. 1,
where frequency and spatial property changes at the xandy
directions can be observed.
Main Result 1. (Increased Discriminability for Adversarial
Perturbation). The Krawtchouk decomposition is discrim-
inative for adversarial perturbation due to the following
factors: 1) The Krawtchouk decomposition defined by the
inner product (Definitions 1 and 2) satisfies the distributive
property of addition (Formulation 1); 2) The Krawtchouk de-
composition with time-frequency discriminability (Property
1) is able to explore the statistical regularity (Formulation
2), when the frequency pattern (Conjecture 1) and spatial
pattern (Conjecture 2) hold in natural images and adver-
sarial perturbations.
Remark. Although decompositions in competing detectors,
e.g., denoising filters [32], SRM [33], DCT [36], DST [37],
and DWT [37], all satisfy the distributive property of addition
(Formulation 1), achieving the statistical regularity (Formu-
lation 2) is still an open problem. Global decompositions,
e.g., DST, are biased towards frequency resolution, thus better
exploiting frequency patterns, but fail in mining spatial pat-
terns. In contrast, local decompositions, e.g., denoising filters,
SRM, and DWT, can fully exploit the spatial patterns, but
only provide limited frequency resolution and thus fail in
mining frequency patterns. As a mid-scale representation, the
Krawtchouk decomposition provides a good trade-off between
spatial and frequency resolutions than above global/local meth-
ods. It is thus expected to reveal a more comprehensive pattern
of adversarial perturbation over the both spatial and frequency
dimensions.
2) Security Analysis: With the Property 1 and the Main
Result 1, we have analyzed the theoretical effectiveness of
the Krawtchouk decomposition in detecting adversarial per-
turbations. Next, we will discuss the secrecy strategy of
decomposition to provide certain security guarantees against
defense-aware attacks.C. WANG et al. : SPATIAL-FREQUENCY DISCRIMINABILITY FOR REVEALING ADVERSARIAL PERTURBATIONS 7
In the implementation of previous detectors, the parameters
of the orthogonal decomposition were often determined explic-
itlyby the host. In our implementation, they are determined by
a pseudorandom number generator, i.e., random feature selec-
tion [40]. The host keeps the seed value of the generator secret,
where such a seed is considered as the key for decomposition.
In fact, the secret key determines the set of coefficients ( w.r.t.
spatial and frequency features) that the detector can explore
(see also Property 1).
Main Result 2. (Increased Security for Defense-aware
Attack). The random feature selection increase the security
for the defense-aware attack due to the following factor. After
the randomization of Krawtchouk decomposition, the adver-
sary is more difficult to identify the adversarial and detector-
interested CA∩ CD(Formulations 3 and 4). Furthermore,
such secrecy strategy confuses the boundary between to-be-
attacked features C\(CA∩ CD)and to-be-evaded features
CA∩ CD, resulting in a dilemma for the adversary in
attacking the model Mand evading the detector D.
Remark . Some decompositions in detectors, e.g., denoising
filters [32] and SRM [33], are based on few fixed filters/bases.
Such design is easy for the adversary to form an effective
evasion of the detector. Other detectors use a more compre-
hensive bases as the decomposition, e.g., DCT [36], DST [37],
and DWT [37], which enhances both the discriminability and
security. However, the risk of evading the detector remains,
where the adversary can still determine the critical CA∩CDdue
to the transparency in the design of detector and the definition
of decomposition. In our work, the random feature selection
provides a secrecy mechanism, placing fundamental dilemma
for the defense-aware attack.
We would like to state that the Main Result 2 is not perfectly
secure — it should be considered as a means of increasing the
attack difficulty (also security) than the few fixed features of
many existing detectors: 1) Note that our security is a special
case of the general modeling by Chen et al. [40] for the ran-
dom feature selection in forensic detectors. Here, such random
feature selection permits to greatly enhance security against
full-feature attack ( i.e., the worst case in one-shot attacks),
according to the detailed proofs and numerical results of [40].
2) For multi-shot attacks, e.g.model reverse engineering, they
can indeed theoretically successfully attack any detector with
random feature selection. On the other hand, their black-box
mechanism forces them to rely on many queries about the
inputs and outputs of the detector. In practice, this short-term
over-querying behavior is easily identified and blocked, and
has become a current consensus in server security (like DDoS
mitigation). In addition, changing the host key periodically is a
practical trick that can further increase difficulty (also security)
w.r.t. multi-shot attacks.
C. Frequency-band Integration and Feature Enhancement
We first recall Section IV-B and provide some proper
notations.
After the Krawtchouk decomposition, the set of spatial-
frequency discriminability (Main Result 1) coefficients isdenoted as C={cn,m,P x,Py}, where the random sampling
provides certain security guarantees w.r.t. defense-aware at-
tacks (Main Result 2). There are some guidelines on the
sampling strategy of the parameters: 1) a very comprehensive
set of candidate parameters is set first, motivated by both the
accuracy and security of the detector; 2) some parameters in
this set are then blocked randomly with a certain probability,
motivated by the security of the detector, where trade-off be-
tween accuracy and security can be controlled by the blocking
probability — higher blocking probability leads to potentially
higher security but lower accuracy, and vice versa (see also
proofs by Chen et al. [40]); 3) the final set of parameters is
formed by further automatic selection or learning, where more
discriminative parameters (features) under a given training set
can be automatically filtered by statistical correlation or more
complex feature selection learning.
Frequency-band Integration. With a given sampling strat-
egy of the parameters, the coefficient set Cis very high-
dimensional and exhibits certain information redundancy,
where direct knowledge engineering is usually inefficient.
Here, the statistical regularity (Formulation 2) are explored
for forming a compact-but-expressive feature vector Von the
coefficient set C. More specifically, the coefficients of similar
frequency properties in Care integrated as a component of
feature vector V, inspired by the frequency pattern (Conjecture
1) and spatial pattern (Conjecture 2). First, the space of
frequency coefficients of the Krawtchouk decomposition, i.e.,
(n, m)∈[0,1, ..., W ]×[0,1, ..., H ], is divided equally into
#Bbands under ℓ2norm
Bi={(n, m) :i−1
#B||(W, H )||2≤ ||(n, m)||2
<i
#B||(W, H )||2},(13)
where i= 1,2, ...,#BandC=∪#B
i=1Bi.
Then, the coefficients cn,m,P x,Pyin the frequency band Bi
are considered with similar frequency properties and they are
integrated as a feature component
Vi(Px, Py) =X
(n,m)∈Bicn,m,P x,Py, (14)
where the feature vector V={Vi(Px, Py)}.
Feature Enhancement. In fact, the feature Vobtained by
frequency-band integration can still be enhanced, i.e., improv-
ing compactness and expressiveness. Here, we present two
simple enhancement strategies, note that they are optional and
not mandatory in the implementation.
•Weighting: Starting from pairs of clean and adversar-
ial examples in the training set, we evaluate in which
frequency bands the features exhibit stronger variability.
Simple functions ( e.g.Gaussian functions) reflecting such
variability can be set to weight the obtained feature vec-
tor, where the more discriminative bands are highlighted.
•Ranking: Starting from the features and labels of the
training examples, we calculate the correlation of each
feature dimension/component with the labels. Then, the
feature vector is re-ranked according to the relevance,C. WANG et al. : SPATIAL-FREQUENCY DISCRIMINABILITY FOR REVEALING ADVERSARIAL PERTURBATIONS 8
 
       
Original BIM(𝜖𝜖 = 0.03)  BIM(𝜖𝜖 = 0.05)  FGSM(𝜖𝜖 = 0.03)  FGSM(𝜖𝜖 = 0.05)  PGD(𝜖𝜖 = 0.03)  PGD(𝜖𝜖 = 0.05)  
         
         
         
         
Original APGD  DeepFoo l PGD -L2 FFGSM  FAB  Square  TPGD  EOTPGD  
 
 
(b) 
 
       
       
       
Original  Uni._VGG -16 Uni._Goog LeNet  Uni._CaffeNet  F3_VGG -16 F3_GoogLeNet  F3_CaffeNet  
 (c) up:imagenet, mid:MEDS; down:multipie  
   
 
       
Original BIM(𝜖𝜖 = 0.03)  BIM(𝜖𝜖 = 0.05)  FGSM(𝜖𝜖 = 0.03)  FGSM(𝜖𝜖 = 0.05)  PGD(𝜖𝜖 = 0.03)  PGD(𝜖𝜖 = 0.05)  
         
         
         
         
Original APGD  DeepFoo l PGD -L2 FFGSM  FAB  Square  TPGD  EOTPGD  
 
 
(b) 
 
       
       
       
Original  Uni._VGG -16 Uni._Goog LeNet  Uni._CaffeNet  F3_VGG -16 F3_GoogLeNet  F3_CaffeNet  
 (c) up:imagenet, mid:MEDS; down:multipie  
   
Fig. 4. Illustration for some datasets and adversarial attacks involved in experiments.
where the dimension/component with low relevance can
be dropped directly.
D. SVM Prediction
After the featuring of Section IV-C, we formed compact-
but-expressive feature Vfrom the Krawtchouk coefficient set
C. Considering the discriminative power of such features, a
simple SVM classification model is sufficient to support a two-
class separation of feature space. Since the SVM prediction is
not our main technical contribution, only a brief description
is provided.
Here, a set of paired clean-adversarial image examples is
employed as the training set. The features formed on training
set and corresponding labels are fed into a SVM. The training
process attempt to achieve a proper mapping from features to
labels by minimizing the loss. The learned SVM parameters
are then saved as an important part of the detector, and
such parameters will be shared to the inference phase of the
practical deployment. Note that the SVM is the only learning
part of the detector.
V. E XPERIMENTS
In this section, we will evaluate the capability of the
proposed method to detect adversarial examples by exten-
sive quantitative analysis. The code is available online at
https://github.com/ChaoWang1016/ASD .
We first provide the basic setup w.r.t. models, attacks, and
defenses in the following experiments. Then, the proposed
detector is evaluated with benchmarking, crossing, and chal-
lenging protocols, thus determining its position w.r.t. current
state-of-the-art detectors and its effectiveness for realistic
scenarios. Finally, we discuss limitations regarding the defense
scope of the proposed detector.
A. Experiment Setup
In general, the experiments of this paper involve the setup
of three aspects: 1) the foundational deep models along
with datasets, which are going to be attacked/defended; 2)
the adversarial perturbation generators for attacking; 3) the
adversarial perturbation detectors for defensing. In Fig. 4, we
provide the visualization for some adversarial images.
 
 
 
 
Figure 5: The benchmarking  of adversarial perturbation  detect ion accuracy  for different detector s 
on the MNIST  dataset. 
  
Fig. 5. The benchmarking of adversarial perturbation detection accuracy for
different detectors on the MNIST dataset.
 
 
 
Figure 6: The benchmarking  of adversarial perturbation  detect ion accuracy  for different detector s 
on the CIFAR -10 dataset. 
  
Fig. 6. The benchmarking of adversarial perturbation detection accuracy for
different detectors on the CIFAR-10 dataset.
The deep models involved in experiments are LeNet [48],
VGG-16 [49], GoogLeNet [50], CaffeNet [51], ResNet-50
[52], Vit-B/16 [53], and Swin-B [54]. The datasets involved
in experiments are MNIST [55], CIFAR-10 [56], MEDS
[57], Multi-PIE [58], PubFig [59], and ImageNet [60]. The
adversarial attacks involved in experiments are FGSM [9],
BIM [7], PGD [61], APGD [62], DeepFool [63], FFGSM [64],
FAB [65], Square [66], TPGD [67], EOTPGD [68], Universal
[11], F3 [69], and Jitter [70].
The adversarial perturbation detectors ( i.e., comparison
methods) involved in experiments are
•arXiv’17 [71] by looking at Bayesian uncertainty esti-
mates;
•ICLR’18 [72] by detecting out-of-distribution images;
•TDSC’18 [32] by denoising filters;
•IJCV’19 [73] by characterizing abnormal filter response
behavior;C. WANG et al. : SPATIAL-FREQUENCY DISCRIMINABILITY FOR REVEALING ADVERSARIAL PERTURBATIONS 9
 
 
 
Figure 7: The benchmarking  of adversarial perturbation  detect ion accuracy  for different detector s 
on the MEDS  dataset. 
  
Fig. 7. The benchmarking of adversarial perturbation detection accuracy for
different detectors on the MEDS dataset.
 
 
 
Figure 8: The benchmarking  of adversarial perturbation  detect ion accuracy  for different detector s 
on the Multi -PIE dataset. 
  
Fig. 8. The benchmarking of adversarial perturbation detection accuracy for
different detectors on the Multi-PIE dataset.
•CVPR’19 [33] by steganalysis and spatial rich model;
•NDSS’19 [74] by neural network invariant checking;
•TDSC’20 [37] by global and local orthogonal decompo-
sition.
B. Benchmarking Experiments
In this part, we provide benchmarking evaluations of the
proposed detector w.r.t. current state-of-the-art detectors. Note
that the comparison results for above methods are mainly
cited from [37]. As detailed in [37], we used the exact same
protocol for the next benchmarking experiments to ensure a
fair comparison.
1) MNIST: Fig. 5 shows the accuracy comparison of 7
detectors w.r.t. 7 attacks on the MNIST dataset. Here, 9000
clean images are selected from the MNIST, and then 9000
corresponding perturbed images are formed by each attack.
For each competing detector, it is trained and tested based on
the above images, with a training-testing split of 50%-50%
on both original and perturbed images. It can be observed
that both TDSC’20 and proposed detectors achieve significant
gains in detection accuracy w.r.t. other advanced detectors over
different attacks. A possible explanation is that the orthogonal
decomposition provides more comprehensive representations
of perturbations for the classifier. Note that we complement
more comprehensive results on MNIST with different attacks
in Appendix B.
2) CIFAR-10: Fig. 6 shows the accuracy comparison of
6 detectors w.r.t. 7 attacks on the CIFAR-10 dataset. Here,
the experiment covers 10000 clean images and 10000 corre-
sponding perturbed images for each attack, also with 50%-50%
training-testing split. Compared to the results on MNIST, one
 
 
 
Fig.9 Adversarial detection performance of the proposed and existing algorithms on the ‘ImageNet ’ 
database  
Fig. 9. The benchmarking of adversarial perturbation detection accuracy for
different detectors on the ImageNet dataset.
TABLE I
RECALL , PRECISION , F1, AND ACCURACY SCORES (%) OF
BENCHMARKING EXPERIMENTS ON THE IMAGE NET DATASET WITH
MORE COMPREHENSIVE MODELS AND ATTACKS .
Model Attack Recall Precision F1 Accuracy
VGG-16FGSM 93.59 88.89 91.18 90.95
BIM 87.42 78.41 82.67 81.67
PGD 82.70 80.27 81.47 81.19
GoogLeNetFGSM 95.65 90.95 93.24 93.07
BIM 90.73 80.27 85.18 84.21
PGD 90.97 79.92 85.08 84.05
ResNetFGSM 99.11 96.66 97.87 97.84
BIM 93.11 87.53 90.23 89.92
PGD 97.02 93.95 95.46 95.38
ViT-B/16FGSM 96.41 95.07 95.74 95.71
BIM 86.33 79.80 82.94 82.24
PGD 92.86 89.75 91.28 91.13
Swin-BFGSM 99.23 98.84 99.03 99.03
BIM 96.09 94.34 95.21 95.16
PGD 98.07 96.62 97.34 97.32
can note a significant performance degradation in competing
methods, even >10% degradation in ICLR’18 and TDSC’18
w.r.t. PGD attacks. This is mainly due to the richer patterns of
image content in CIFAR-10, which acts as a strong interfer-
ence for representing perturbation patterns. Among them, the
proposed detector exhibits the least performance degradation
over different attacks, even when compared to TDSC’20. Such
a phenomenon further confirms the effectiveness of our spatial-
frequency discriminative decomposition.
3) MEDS and Multi-PIE: In Fig. 7 and Fig. 8, we provide
accuracy comparison of 7 detectors w.r.t. 6 attacks on face
datasets MEDS and Multi-PIE, respectively. Here, universal
perturbation is imposed on small-scale face images, with 50%-
50% training-testing split, where both perturbation and content
patterns are relatively homogeneous. Even under this protocol,
none of the competing methods except TDSC’20 achieved >
90% detection accuracy. This implies that non-complete image
representations are not sufficient for supporting an efficient
detector, even in small-scale detection scenarios. Under such
simple experimental protocol, both the proposed detector and
TDSC’20 exhibit ∼100% accuracy, in line with general
expectations of this paper.C. WANG et al. : SPATIAL-FREQUENCY DISCRIMINABILITY FOR REVEALING ADVERSARIAL PERTURBATIONS 10
4) ImageNet: As for Fig. 9, the accuracy comparison of
7 detectors w.r.t. 2 attacks on ImageNet dataset is illustrated.
Here, universal perturbation is imposed on large-scale natural
images, also of 50%-50% training-testing split. Clearly, the
diversity of the image content increases significantly w.r.t. the
previous experimental protocol, increasing also the difficulty
of the detection. An interesting phenomenon is that the gap
between TDSC’20 and other competing methods becomes
smaller. This implies that the simple decision-level fusion of
global and local orthogonal transforms in TDSC’20 cannot
handle more complex detection tasks well. It should be con-
sidered as an inflexible remedy for the contradiction of spatial
and frequency discriminability. In general, our method still
achieves ∼5% gain w.r.t. TDSC’20 and ∼10% gain w.r.t.
other competing methods. In Table I, more comprehensive
benchmarking scores on the ImageNet dataset are listed.
Here, 3 representative attacks are imposed on large-scale
natural images, with 5 models covering recent transformer
architectures, also of 50%-50% training-testing split. In the
total of 15 attack-model pairs, our detector exhibits >90%
scores of F1 and accuracy for 9 pairs. Note that all competing
detectors in Fig. 9 fail to achieve >90% scores even with
the universal perturbation. For the worst case, the F1 and
accuracy scores of our detector are still ∼80%, reaching the
level of scores for competing detectors in Fig. 9 (but with
significantly simpler protocol). It is an important illustration
on the strength of our spatial-frequency discriminative detector
w.r.t. variable perturbations in the presence of complex content
interference. Note that we complement more comprehensive
results on ImageNet with different intensities in Appendix B.
The above consistent performance gains in Figs. 5 ∼9 and
Table I validate the advanced nature of the proposed detector,
revealing the potential of our spatial-frequency discriminative
decomposition in perturbation detection tasks. In addition to
the above prediction scores, we present the prediction times
in Appendix B, which also exhibit real-time potential.
C. Crossing Experiments
Through above benchmark experiments, we have positioned
our detector w.r.t. some state-of-the-art detectors. Such exten-
sive results indicate a consistent performance advantage of the
proposed method under the typical experiment protocol. In this
part, we will further analyze whether the above performance
advantage arises from a certain over-fitting. More specifically,
we will test a trained detector by crossing to other similar
experiment protocols, thereby quantifying its transferability to
reasonable changes.
1) Crossing Dataset: Table II lists the various performance
scores of the proposed detector on crossing dataset protocol
with universal perturbation. Here, the protocol involves 4
datasets: ImageNet, MEDS, Multi-PIE, and PubFig. In general,
after sufficient learning, a promising detector is expected
to be generalizable to natural differences of the training
and testing phases. As can be observed, for all cases with
sufficient learning, our detector exhibits ∼100% scores of
recall, precision, F1, and accuracy. Here, the detector trained
on the very diverse ImageNet can be smoothly transferredTABLE II
RECALL , PRECISION , F1, AND ACCURACY SCORES (%) OFCROSSING
DATASET EXPERIMENTS .
Training Testing Recall Precision F1 Accuracy
ImageNetMEDS 98.60 99.86 99.22 99.23
Multi-PIE 99.80 99.87 99.84 99.84
PubFig 99.67 99.80 99.74 99.74
MEDSImageNet 95.04 72.94 82.54 79.90
Multi-PIE 100.00 100.00 100.00 100.00
PubFig 99.91 99.86 99.88 99.88
Multi-PIEImageNet 99.88 54.84 70.80 58.82
MEDS 100.00 91.52 95.57 95.37
PubFig 99.99 99.27 99.63 99.63
PubFigImageNet 98.92 68.88 81.21 77.19
MEDS 99.86 99.86 99.86 99.86
Multi-PIE 100.00 100.00 100.00 100.00
TABLE III
RECALL , PRECISION , F1, AND ACCURACY SCORES (%) OFCROSSING
MODEL EXPERIMENTS .
Training Testing Dataset Recall Precision F1 Accuracy
VGG-16 GoogleNetMEDS 100.00 97.00 98.48 98.46
Multi-PIE 100.00 99.74 99.87 99.87
PubFig 99.47 99.83 99.65 99.65
VGG-16 CaffeNetMEDS 90.45 96.99 93.61 93.82
Multi-PIE 100.00 99.74 99.87 99.87
PubFig 98.78 99.83 99.30 99.30
to the lower diversity face datasets MEDS, Multi-PIE, and
PubFig. Crossing experiments between the 3 face datasets
(with similar diversity) also exhibited consistently good scores.
Such numerical evidence suggests that our spatial-frequency
discriminative decomposition provides complete and intrinsic
features of adversarial perturbations, and therefore generalizes
well to unseen-but-similar datasets. As can be expected, when
the diversity of the training dataset is much smaller than that of
the testing one, our detector shows a significant performance
degradation. Therefore, such overfitting caused by biased
training should be strongly avoided in practical defense, by
training with sufficient diversity data.
Remark. In addition, we would like to provide some supple-
mentary information as comparison baselines. Based on scores
reported in the literature, competing algorithms arXiv’17 and
TDSC’18 achieve ∼80% accuracy scores on the crossing
dataset protocol from MEDS to Multi-PIE, and ∼70% ac-
curacy scores for the reverse protocol. Thus they exhibit a
>20% performance gap w.r.t. the proposed detector. It is
further verified that the proposed decomposition serves as a
more generic representation than such simple filters.
2) Crossing Model: In Table III, we provide the various
performance scores of the proposed detector on crossing model
protocol with universal perturbation. Here, our detector is
trained on VGG-16 and tested on GoogleNet or CaffeNet,
where the training and testing are also considered on three face
datasets. Note that although the adversarial perturbations on
different models differ significantly at the numerical level, they
still exhibit specific statistical consistency; the mining of this
consistency largely reflects the generalizability of the detector.
Obviously, the proposed detector remains stable, regardless ofC. WANG et al. : SPATIAL-FREQUENCY DISCRIMINABILITY FOR REVEALING ADVERSARIAL PERTURBATIONS 11
TABLE IV
RECALL , PRECISION , F1, AND ACCURACY SCORES (%) OFCROSSING
ATTACK EXPERIMENTS .
Training Testing Recall Precision F1 Accuracy
BIMFGSM 100.00 100.00 100.00 100.00
PGD 100.00 99.98 99.99 99.99
FAB 84.82 99.53 91.59 92.21
Square 78.82 99.50 87.96 89.21
Jitter 99.68 99.98 99.83 99.83
FGSMBIM 99.98 99.86 99.92 99.92
PGD 100.00 99.86 99.93 99.93
FAB 76.90 99.82 86.87 88.38
Square 77.48 99.82 87.24 88.67
Jitter 98.46 100.00 99.22 99.23
PGDBIM 100.00 99.86 99.93 99.93
FGSM 100.00 99.86 99.93 99.93
FAB 80.28 99.83 88.99 90.07
Square 78.70 99.82 88.01 89.28
Jitter 99.78 99.86 99.82 99.82
FABBIM 100.00 99.11 99.55 99.55
FGSM 100.00 99.11 99.55 99.55
PGD 100.00 99.11 99.55 99.55
Square 79.06 98.87 87.86 89.08
Jitter 100.00 98.93 99.46 99.46
SquareBIM 100.00 98.68 99.33 99.33
FGSM 100.00 98.78 99.38 99.38
PGD 100.00 98.48 99.24 99.23
FAB 89.14 98.52 93.60 93.90
Jitter 100.00 99.44 99.72 99.72
JitterBIM 100.00 99.92 99.96 99.96
FGSM 100.00 99.92 99.96 99.96
PGD 100.00 99.92 99.96 99.96
FAB 86.20 99.91 92.55 93.06
Square 78.24 99.72 87.68 89.01
the different datasets and the different performance metrics.
For most cases, our detector exhibits ∼100% scores, and
even the worst case (VGG-16 to CaffeNet on MEDS) is
still 93.82%. These consistent phenomena suggest that the
proposed detector and its foundational decomposition have
well generalizability to unseen-but-similar models.
Remark. In addition, we would like to provide some supple-
mentary information as comparison baselines. Based on scores
reported in the literature, competing algorithm TDSC’20
achieves ∼93% accuracy score on the crossing model protocol
from VGG-16 to GoogleNet on MEDS, and ∼96% accuracy
score for the similar protocol on Multi-PIE. Therefore, our
detector still exhibits gains w.r.t. this orthogonal transform
based detector.
3) Crossing Attack: As for Table IV , we list the various
performance scores of the proposed detector on crossing attack
protocol on MNIST dataset. Here, the protocol involves 6
attacks, i.e., BIM, FGSM, PGD, FAB, Square, and Jitter, with
quite significant differences in their designs. The experiment
will consider any crossing pair of these 6 attacks in training
and testing, for a total of 30 pairs. As can be expected,
the adversarial perturbations derived from these attacks are
different numerically, but meanwhile have similar statistical
properties. In the table, the extensive performance scores
indicate that the proposed detector is capable of representing
the statistical consistency over such attacks. For 20 crossingpairs, our detector exhibits ∼100% scores of F1 and accuracy.
For the worst case, the F1 and accuracy scores of our detector
are still ∼87%. In general, the performance of the proposed
detector is quite promising, even w.r.t. the generalizability of
the unseen and somewhat different adversarial perturbations.
Note that the detector trained on more complex attacks ( i.e.,
FAB, Square, and Jitter) usually has better transferability
for simple attacks ( i.e., BIM, FGSM, and PGD). The above
common phenomena illustrate the importance of introducing
sufficiently diverse data during training. Therefore in the
next challenging experiments we will consider the sufficient
training with a mixture of multiple attacks to achieve real-
world defense.
Remark. In addition, we would like to provide some supple-
mentary information as comparison baselines. Based on scores
reported in the literature, competing algorithms arXiv’17,
ICLR’18, TDSC’18, CVPR’19, and TDSC’20 achieve ∼64%,
∼76%,∼65%,∼68%, and ∼95% average accuracy scores
over FGSM-like crossing pairs, respectively. In Table IV , our
algorithm exhibits an average accuracy score of ∼96%. Note
that even though our protocol involves more diverse attacks,
the proposed detector still provides better scores than above
competing methods.
The above consistent performance gains in Tables II ∼IV
validate the advanced nature of the proposed detector, reveal-
ing the generalizability of our spatial-frequency discriminative
decomposition for unseen-but-similar scenarios.
D. Challenging Experiments
The above benchmarking and crossing experiments have ex-
tensively demonstrated the advantages of the proposed detector
over state-of-the-art detectors under ideal protocols. In this
part, we turn to more challenging protocols. Such protocols
can verify the discriminability, generalization, completeness,
and efficiency of the proposed detectors in a comprehensive
and realistic manner.
1) From Ideal to Practice: We begin with a discussion
on the practical deployment of the adversarial perturbation
detector and the corresponding training strategy. In real-
world scenarios, it is expected that the detector enables an
accurate and generic defense for the widest possible range of
adversarial attacks. In general, two training and deployment
approaches exist to achieve this goal:
•Integration during inference. Multiple detectors are in-
volved here, each of which is trained on adversarial exam-
ples from only one attack. When deployed, the example
under analysis is filled into these detectors separately, and
the corresponding results are then integrated as the final
prediction. Therefore, this approach can be construed as
a late integration strategy.
•Integration during training. Only one detector is involved
here, which is trained directly on adversarial examples
from the widest possible range of attacks. After training,
the detector is deployed directly to predict arbitrary
examples. Therefore, this approach can be construed as
an early fusion strategy.
It is clear that the integration during training is more
compact. Compared to the integration during inference, itC. WANG et al. : SPATIAL-FREQUENCY DISCRIMINABILITY FOR REVEALING ADVERSARIAL PERTURBATIONS 12
1000
399636633330299726642331
4329
46621998
50001 1Training
5000FGSM
BIM
PGD
APGD
DeepFoolTesting
2000
3000
4000×55001
10000FGSM
BIM
PGD
APGD
DeepFool
×155001
10000Training Testing
1665333
666
999
1332Training Testing
×105001
100001
50001000
2000
3000
4000
5500500
2500
3500
4500FGSM
BIM
PGD
APGD
DeepFool
FGSM -L1
FGSM -L2
BIM-L1
BIM-L2
PGD -L2FGSM -L1
FGSM -L2
BIM-L1
BIM-L2
PGD -L2
FFGSM
FAB
Square
TPGD
EOTPGD
(a)N= 5
1000
399636633330299726642331
4329
46621998
50001 1Training
5000FGSM
BIM
PGD
APGD
DeepFoolTesting
2000
3000
4000×55001
10000FGSM
BIM
PGD
APGD
DeepFool
×155001
10000Training Testing
1665333
666
999
1332Training Testing
×105001
100001
50001000
2000
3000
4000
5500500
2500
3500
4500FGSM
BIM
PGD
APGD
DeepFool
FGSM -L1
FGSM -L2
BIM-L1
BIM-L2
PGD -L2FGSM -L1
FGSM -L2
BIM-L1
BIM-L2
PGD -L2
FFGSM
FAB
Square
TPGD
EOTPGD (b)N= 10
1000
399636633330299726642331
4329
46621998
50001 1Training
5000FGSM
BIM
PGD
APGD
DeepFoolTesting
2000
3000
4000×55001
10000FGSM
BIM
PGD
APGD
DeepFool
×155001
10000Training Testing
1665333
666
999
1332Training Testing
×105001
100001
50001000
2000
3000
4000
5500500
2500
3500
4500FGSM
BIM
PGD
APGD
DeepFool
FGSM -L1
FGSM -L2
BIM-L1
BIM-L2
PGD -L2FGSM -L1
FGSM -L2
BIM-L1
BIM-L2
PGD -L2
FFGSM
FAB
Square
TPGD
EOTPGD (c)N= 15
Fig. 10. Illustration for the adversarial image division in challenging experiments. Note that the more attacks involved, the fewer training examples for each
attack, and therefore this protocol is a comprehensive challenge for the discriminability, generalization, completeness, and efficiency of the detector.
 
  
 
  
(b)N= 5
 
  
 (c)N= 10
 
 (d)N= 15
Fig. 11. Recall, precision, F1, and accuracy scores (%) of challenging experiments. Note that the proposed detector performs consistently even with more
attacks and fewer training examples, verifying its discriminability, generalization, completeness, and efficiency.
will consume significantly fewer computational resources in
deployment, while not involving tricky integration policy
design for the results from multiple detectors. However, most
of the benchmarking and crossing experiments in the literature
are trained only with single kind of attack, and thus only
verifying the effectiveness for the integration during inference.
Therefore, we not only consider benchmarking and crossing
experiments for the integration during inference (Sections V-B
and V-C), but will also focus on more challenging experiments
for the integration during training (Section V-D).
2) A Comprehensive and Realistic Protocol: For a compre-
hensive investigation of the proposed detector in the scenario
of integration during training, we design the following experi-
mental protocol. We selected 10000 original images in MNIST
for generating the experimental images, in which the first/latter
5000 images are used to derive the training/testing examples
respectively. Assuming that Nattacks are considered, where
N= 5,10,15in our experiments. For the training, the first
5000 images are equally divided into Nparts for generating
the adversarial images corresponding to per attack, and then
they are used as training examples together with the first
5000 original images. Our detector will be trained directly
on such 10000 images. For the testing, the examples under
each attack will consist of the latter 5000 original images and
5000 corresponding adversarial images, resulting in a total of
N×10000 testing examples.In Fig. 10, we present an illustration for the above adver-
sarial image division w.r.t. N= 5,10,15. Note that when
Nincreases, the number of adversarial images from each
attack in the training set, i.e.,5000/N, will decrease, while
the counterpart in the testing set remains 5000. Obviously, this
protocol is a comprehensive challenge for the discriminability,
generalization, completeness, and efficiency of the detector.
In Fig. 11, we show the various performance scores of the
proposed detector on challenging experiment protocol. Here,
corresponding to Fig. 10, the protocol involves increasing
number of attacks and decreasing number of training examples
from (a) to (c). In general, a promising detector is expected
to be stable to realistic variations in the problem complexity
or the training scale. As can be observed here, our detector
exhibits consistent recall, precision, F1, and accuracy scores
on all three scenarios (a) ∼(c). For the most challenging
scenario (c), where number of attacks is 15 and number of
training adversarial images per attack is only 333, the scores
w.r.t. most attacks (except for APGD, DeepFool, and Squara)
are even ∼100%. The worst case is DeepFool with ∼87%
scores of F1 and accuracy. Such scores are still generally
satisfactory, considering the training/testing adversarial images
ratio is 300/5000. The above phenomenon suggests that our
detector and its foundational decomposition have:
•Discriminability for a wide range of attacks;
•Generalization for unseen-but-similar examples;C. WANG et al. : SPATIAL-FREQUENCY DISCRIMINABILITY FOR REVEALING ADVERSARIAL PERTURBATIONS 13
Original
 Gaussian
 S&P
 Compression
 Adversarial
Fig. 12. Illustration of harmless perturbations (noise and compression) versus
adversarial perturbations.
TABLE V
RECALL , PRECISION , F1, AND ACCURACY SCORES (%) IN
DISTINGUISHING BETWEEN HARMLESS AND ADVERSARIAL
PERTURBATIONS .
Dataset Recall Precision F1 Accuracy
CleanMNIST 100.00 99.98 99.99 99.99
CIFAR-10 99.24 94.17 96.64 96.55
ImageNet 93.59 88.89 91.18 90.95
GaussianMNIST 97.96 99.94 98.94 98.95
CIFAR-10 93.96 92.88 93.42 93.38
ImageNet 94.07 84.77 89.18 88.59
S&PMNIST 99.86 100.00 99.93 99.93
CIFAR-10 98.86 86.08 92.03 91.44
ImageNet 92.10 84.53 88.15 87.62
CompressionMNIST 100.00 93.28 96.53 96.40
CIFAR-10 99.46 98.53 98.99 98.99
ImageNet 96.77 91.12 93.86 93.67
•Completeness for potential perturbation patterns;
•Efficiency on training scale.
Starting from such properties, the proposed detector can be
regarded as a more comprehensive and effective defense for
the realistic scenarios with integration during training.
E. Limitations and Discussions
In this part, we provide discussions on the limitations of the
defense scope of the proposed detector.
Recalling Conjectures 1 and 2, our detector is better suited
fornoise-like perturbation patterns where the spatial-frequency
distribution is far from natural images. Therefore, there may
be two limitations on the defense scope: 1) false positives
for harmless noise-like perturbations ( e.g., noise and com-
pression), and 2) false negatives for adversarial perturbations
beyond noise-like patterns.
Regarding harmless noise-like perturbations, false positives
can be greatly reduced by introducing harmless perturbed
examples into the training phase. As shown in Fig. 12,
the 3 types of harmless perturbed examples, i.e., Gaussian
noise, salt-and-pepper (S&P) noise, and compression, share
the same training labels as the original clean examples. In
table V , we list the various performance scores of the proposeddetector with such examples on the 3 datasets. In general,
the scores after introducing harmless perturbations do not
differ much from the baseline case with only original clean
examples. Clearly, our detector is capable of distinguishing
between harmless and adversarial perturbations, even with
similar noise-like patterns. Therefore, if the defense scenario
treats a certain type of perturbation as harmless, it is necessary
to introduce its examples in the training of our detector.
Regarding adversarial perturbations beyond noise-like pat-
terns, our detectors do not have good applicability. As demon-
strated in the previous experiments, noise-like modeling suc-
cessfully covers a wide range of adversarial perturbation types,
some of which are highly influential mainstream attacks. With
the emergence of more diverse attacks, the imperceptibility of
perturbations has been greatly expanded conceptually, leading
to so-called semantic perturbations ( e.g., recoloring as ad-
versarial perturbation [75]). Clearly, all methods of checking
artifacts at the digital level will fail, covering almost all of
the competing detectors listed in this paper. In this regard,
future work is open and involves the understanding of semantic
artifacts.
VI. C ONCLUSION
In general, our main goal is to provide a more com-
prehensive design of adversarial perturbation detector. As
a technical foundation of the detector, we have proposed
the spatial-frequency discriminative decomposition with secret
keys, motivated by the accuracy and security issues of existing
detectors. Here, the accuracy and security ingredients in this
paper can be summarized as follows.
•Regarding the accuracy , we attribute the accuracy bot-
tleneck of existing detectors to the fundamental contra-
diction of spatial and frequency discriminability in the
decomposition. Specifically, the non-orthogonal decom-
position ( e.g., SRM) is not sufficient to completely rep-
resent a wide range of potential perturbations. The global
(e.g., DST) or local ( e.g., DWT) orthogonal decompo-
sition cannot mine both frequency and spatial patterns
(Conjectures 1 and 2), thereby failing to fully reveal
the statistical regularity (Formulation 2). In this paper,
we have introduced the Krawtchouk basis (Definition
2) for more discriminative decomposition, providing a
mid-scale representation with rich spatial-frequency in-
formation (Property 1). Therefore, the resulting detector
exhibits better discriminability in the decomposition of
natural images and adversarial perturbations (Main Result
1).
•Regarding the security , we attribute the defense-aware
attack to the transparency of detector-interested features
for the attacker. With such knowledge, the attacker can
regenerate adversarial perturbations without exhibiting
obvious artifacts on these features (Formulations 3 and
4). In this paper, we have proposed the random feature se-
lection for secrecy, where a key controlled pseudorandom
number generator determines the spatial and frequency
parameters of the decomposition. Therefore, the resulting
detector is more secure against the defense-aware attack:C. WANG et al. : SPATIAL-FREQUENCY DISCRIMINABILITY FOR REVEALING ADVERSARIAL PERTURBATIONS 14
it is more difficult to divide between to-be-attacked and
to-be-evaded features (Main Result 2).
We have provided statistical comparisons with state-of-the-
art detectors, by the benchmarking (Section V-B), crossing
(Section V-C), and challenging (Section V-D) experiments for
both ideal and realistic scenarios ( w.r.t. integration during in-
ference and training). In general, such extensive experimental
results confirm the effectiveness of our detector, exhibiting
quite satisfactory discriminability, generalization, complete-
ness, and efficiency w.r.t. existing works.
Our future work will focus on more formal statistical
analysis for the decomposition of natural images and adver-
sarial perturbations, potentially involving coefficient statistical
modeling and hypothesis testing.
REFERENCES
[1] Y . Bengio, A. Courville, and P. Vincent, “Representation learning: A
review and new perspectives,” IEEE Trans. Pattern Anal. Mach. Intell. ,
vol. 35, no. 8, pp. 1798–1828, 2013.
[2] L. Zhao, Q. Wang, C. Wang, Q. Li, C. Shen, and B. Feng, “Veriml:
Enabling integrity assurances and fair payments for machine learning
as a service,” IEEE Trans. Parallel Distrib. Syst. , vol. 32, no. 10, pp.
2524–2540, 2021.
[3] J. M. Wing, “Trustworthy AI,” Commun. ACM , vol. 64, no. 10, pp.
64–71, 2021.
[4] A. Fawzi, S.-M. Moosavi-Dezfooli, and P. Frossard, “The robustness of
deep networks: A geometrical perspective,” IEEE Signal Process Mag. ,
vol. 34, no. 6, pp. 50–62, 2017.
[5] I. Goodfellow, P. McDaniel, and N. Papernot, “Making machine learning
robust against adversarial inputs,” Commun. ACM , vol. 61, no. 7, pp.
56–66, 2018.
[6] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow,
and R. Fergus, “Intriguing properties of neural networks,” in Proc. Int.
Conf. Learn. Represent. , 2014.
[7] A. Kurakin, I. Goodfellow, and S. Bengio, “Adversarial examples in the
physical world,” in Artificial intelligence safety and security . Chapman
and Hall/CRC, 2018, pp. 99–112.
[8] J. Su, D. V . Vargas, and K. Sakurai, “One pixel attack for fooling deep
neural networks,” IEEE Trans. Evol. Comput. , vol. 23, no. 5, pp. 828–
841, 2019.
[9] I. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harnessing
adversarial examples,” in Proc. Int. Conf. Learn. Represent. , 2015.
[10] N. Carlini and D. Wagner, “Towards evaluating the robustness of neural
networks,” in Proc. IEEE Symp. Secur. Priv. , 2017, pp. 39–57.
[11] S.-M. Moosavi-Dezfooli, A. Fawzi, O. Fawzi, and P. Frossard, “Univer-
sal adversarial perturbations,” in Proc. IEEE Conf. Comput. Vis. Pattern
Recognit. , 2017, pp. 1765–1773.
[12] P.-Y . Chen, H. Zhang, Y . Sharma, J. Yi, and C.-J. Hsieh, “Zoo: Zeroth
order optimization based black-box attacks to deep neural networks
without training substitute models,” in Proc. ACM Workshop Artif. Intell.
Secur. , 2017, pp. 15–26.
[13] K. Eykholt, I. Evtimov, E. Fernandes, B. Li, A. Rahmati, C. Xiao,
A. Prakash, T. Kohno, and D. Song, “Robust physical-world attacks
on deep learning visual classification,” in Proc. IEEE Conf. Comput.
Vis. Pattern Recognit. , 2018, pp. 1625–1634.
[14] P. Li, Y . Zhang, L. Yuan, J. Zhao, X. Xu, and X. Zhang, “Adversarial
attacks on video object segmentation with hard region discovery,” IEEE
Trans. Circuits Syst. Video Technol. , 2023.
[15] Z. Zhou, Y . Sun, Q. Sun, C. Li, and Z. Ren, “Only once attack: Fooling
the tracker with adversarial template,” IEEE Trans. Circuits Syst. Video
Technol. , 2023.
[16] T. Chen and Z. Ma, “Towards robust neural image compression: Adver-
sarial attack and model finetuning,” IEEE Transactions on Circuits and
Systems for Video Technology , 2023.
[17] X. Qin, B. Li, S. Tan, W. Tang, and J. Huang, “Gradually enhanced ad-
versarial perturbations on color pixel vectors for image steganography,”
IEEE Trans. Circuits Syst. Video Technol. , vol. 32, no. 8, pp. 5110–5123,
2022.
[18] B. Wang, M. Zhao, W. Wang, X. Dai, Y . Li, and Y . Guo, “Adversarial
analysis for source camera identification,” IEEE Trans. Circuits Syst.
Video Technol. , vol. 31, no. 11, pp. 4174–4186, 2020.[19] J. Zhang, J. Wang, H. Wang, and X. Luo, “Self-recoverable adversarial
examples: a new effective protection mechanism in social networks,”
IEEE Trans. Circuits Syst. Video Technol. , vol. 33, no. 2, pp. 562–574,
2022.
[20] M. Andriushchenko and N. Flammarion, “Understanding and improving
fast adversarial training,” Proc. Conf. Neural Inf. Proces. Syst. , vol. 33,
pp. 16 048–16 059, 2020.
[21] H. Liu, Y . Wang, W. Fan, X. Liu, Y . Li, S. Jain, Y . Liu, A. Jain, and
J. Tang, “Trustworthy AI: A computational perspective,” ACM Trans.
Intell. Syst. Technolog. , vol. 14, no. 1, pp. 1–59, 2022.
[22] C. Xiang, A. N. Bhagoji, V . Sehwag, and P. Mittal, “PatchGuard: A
provably robust defense against adversarial patches via small receptive
fields and masking.” in Proc. USENIX Secur. Symp. , 2021, pp. 2237–
2254.
[23] A. Nayebi and S. Ganguli, “Biologically inspired protection of deep
networks from adversarial attacks,” arXiv preprint arXiv:1703.09202 ,
2017.
[24] A. Ross and F. Doshi-Velez, “Improving the adversarial robustness
and interpretability of deep neural networks by regularizing their input
gradients,” in Proc. AAAI Conf. Artif. Intell. , vol. 32, no. 1, 2018.
[25] M. Lecuyer, V . Atlidakis, R. Geambasu, D. Hsu, and S. Jana, “Certified
robustness to adversarial examples with differential privacy,” in Proc.
IEEE Symp. Secur. Priv. , 2019, pp. 656–672.
[26] J. Cohen, E. Rosenfeld, and Z. Kolter, “Certified adversarial robustness
via randomized smoothing,” in Proc. Int. Conf. Mach. Learn. , 2019, pp.
1310–1320.
[27] A. Cullen, P. Montague, S. Liu, S. Erfani, and B. Rubinstein, “Double
bubble, toil and trouble: enhancing certified robustness through transi-
tivity,” Proc. Adv. Neural Inf. Proces. Syst. , vol. 35, pp. 19 099–19 112,
2022.
[28] H. Zhang, Y . Yu, J. Jiao, E. Xing, L. El Ghaoui, and M. Jordan,
“Theoretically principled trade-off between robustness and accuracy,”
inProc. Int. Conf. Mach. Learn. , 2019, pp. 7472–7482.
[29] X. Zhang, X. Zheng, and W. Mao, “Adversarial perturbation defense on
deep neural networks,” ACM Comput. Surv. , vol. 54, no. 8, pp. 1–36,
2021.
[30] S. Qi, Y . Zhang, C. Wang, J. Zhou, and X. Cao, “A principled design
of image representation: Towards forensic tasks,” IEEE Trans. Pattern
Anal. Mach. Intell. , 2022.
[31] G.-L. Chen and C.-C. Hsu, “Jointly defending DeepFake manipulation
and adversarial attack using decoy mechanism,” IEEE Trans. Pattern
Anal. Mach. Intell. , 2023.
[32] B. Liang, H. Li, M. Su, X. Li, W. Shi, and X. Wang, “Detecting
adversarial image examples in deep neural networks with adaptive noise
reduction,” IEEE Trans. Dependable Secure Comput. , vol. 18, no. 1, pp.
72–85, 2018.
[33] J. Liu, W. Zhang, Y . Zhang, D. Hou, Y . Liu, H. Zha, and N. Yu, “Detec-
tion based defense against adversarial examples from the steganalysis
point of view,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. ,
2019, pp. 4825–4834.
[34] C. Guo, J. S. Frank, and K. Q. Weinberger, “Low frequency adversarial
perturbation,” in Proc. Uncertainty Artif. Intell. , 2020, pp. 1127–1137.
[35] A. N. Bhagoji, D. Cullina, C. Sitawarin, and P. Mittal, “Enhancing
robustness of machine learning systems via data transformations,” in
Proc. Annu. Conf. Inf. Sci. Syst. , 2018, pp. 1–5.
[36] N. Akhtar, J. Liu, and A. Mian, “Defense against universal adversarial
perturbations,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. ,
2018, pp. 3389–3398.
[37] A. Agarwal, R. Singh, M. Vatsa, and N. Ratha, “Image transformation-
based defense against adversarial perturbation on deep learning models,”
IEEE Trans. Dependable Secure Comput. , vol. 18, no. 5, pp. 2106–2121,
2020.
[38] A. Agarwal, G. Goswami, M. Vatsa, R. Singh, and N. K. Ratha, “Damad:
Database, attack, and model agnostic adversarial perturbation detector,”
IEEE Trans. Neural Networks Learn. Syst. , vol. 33, no. 8, pp. 3277–
3289, 2021.
[39] S. G. Mallat, “A theory for multiresolution signal decomposition: the
wavelet representation,” IEEE Trans. Pattern Anal. Mach. Intell. , vol. 11,
no. 7, pp. 674–693, 1989.
[40] Z. Chen, B. Tondi, X. Li, R. Ni, Y . Zhao, and M. Barni, “Secure
detection of image manipulation by means of random feature selection,”
IEEE Trans. Inf. Forensics Secur. , vol. 14, no. 9, pp. 2454–2469, 2019.
[41] Z. Zha, X. Yuan, J. Zhou, C. Zhu, and B. Wen, “Image restoration
via simultaneous nonlocal self-similarity priors,” IEEE Trans. Image
Process. , vol. 29, pp. 8561–8576, 2020.C. WANG et al. : SPATIAL-FREQUENCY DISCRIMINABILITY FOR REVEALING ADVERSARIAL PERTURBATIONS 15
[42] F. Wu, W. Yang, L. Xiao, and J. Zhu, “Adaptive wiener filter and natural
noise to eliminate adversarial perturbation,” Electronics , vol. 9, no. 10,
p. 1634, 2020.
[43] V . Veerabadran, J. Goldman, S. Shankar, B. Cheung, N. Papernot,
A. Kurakin, I. Goodfellow, J. Shlens, J. Sohl-Dickstein, M. C. Mozer
et al. , “Subtle adversarial image manipulations influence both human
and machine perception,” Nat. Commun. , vol. 14, no. 1, p. 4933, 2023.
[44] E. P. Simoncelli and B. A. Olshausen, “Natural image statistics and
neural representation,” Annu. Rev. Neurosci. , vol. 24, no. 1, pp. 1193–
1216, 2001.
[45] S. Qi, Y . Zhang, C. Wang, J. Zhou, and X. Cao, “A survey of
orthogonal moments for image representation: theory, implementation,
and evaluation,” ACM Comput. Surv. , vol. 55, no. 1, pp. 1–35, 2021.
[46] P.-T. Yap, R. Paramesran, and S.-H. Ong, “Image analysis by
Krawtchouk moments,” IEEE Trans. Image Process. , vol. 12, no. 11,
pp. 1367–1377, 2003.
[47] H. Yang, S. Qi, J. Tian, P. Niu, and X. Wang, “Robust and discriminative
image representation: Fractional-order Jacobi-Fourier moments,” Pattern
Recognit. , vol. 115, p. 107898, 2021.
[48] Y . LeCun, L. Bottou, Y . Bengio, and P. Haffner, “Gradient-based learning
applied to document recognition,” Proc. IEEE , vol. 86, no. 11, pp. 2278–
2324, 1998.
[49] K. Simonyan and A. Zisserman, “Very deep convolutional networks for
large-scale image recognition,” in Proc. Int. Conf. Learn. Represent. ,
2015.
[50] C. Szegedy, W. Liu, Y . Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
V . Vanhoucke, and A. Rabinovich, “Going deeper with convolutions,”
inProc. IEEE Conf. Comput. Vis. Pattern Recognit. , 2015, pp. 1–9.
[51] Y . Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,
S. Guadarrama, and T. Darrell, “Caffe: Convolutional architecture for
fast feature embedding,” in Proc. ACM Int. Conf. Multimed. , 2014, pp.
675–678.
[52] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” Proc. IEEE Conf. Comput. Vis. Pattern Recognit. , pp. 770–
778, 2016.
[53] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,
T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al. ,
“An image is worth 16x16 words: transformers for image recognition at
scale,” Proc. Int. Conf. Learn. Represent. , 2020.
[54] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang, S. Lin, and
B. Guo, “Swin transformer: hierarchical vision transformer using shifted
windows,” Proc. IEEE Int. Conf. Comput. Vis. , pp. 10 012–10 022, 2021.
[55] Y . LeCun, C. Corinna, and J. B. Christopher, “MNIST,” http://yann.
lecun.com/exdb/mnist/.
[56] A. Krizhevsky, V . Nair, and G. Hinton, “CIFAR-10,” https://www.cs.
toronto.edu/ ∼kriz/cifar.html.
[57] A. Founds, N. Orlans, G. Whiddon, and C. Wat-
son, “MEDS,” https://www.nist.gov/itl/iad/image-group/
special-database-32-multiple-encounter-dataset-meds.
[58] R. Gross, I. Matthews, J. Cohn, T. Kanade, and S. Baker, “Multi-PIE,”
https://www.cs.cmu.edu/afs/cs/project/PIE/MultiPie/Multi-Pie/Home.
html.
[59] N. Kumar, A. C. Berg, P. N. Belhumeur, and S. K. Nayar, “PubFig,”
https://www.cs.columbia.edu/CA VE/databases/pubfig/.
[60] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “ImageNet,”
https://www.image-net.org/index.php.
[61] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu, “Towards
deep learning models resistant to adversarial attacks,” in Proc. Int. Conf.
Learn. Represent. , 2018.
[62] F. Croce and M. Hein, “Reliable evaluation of adversarial robustness
with an ensemble of diverse parameter-free attacks,” in Proc. Int. Conf.
Mach. Learn. , 2020, pp. 2206–2216.
[63] S.-M. Moosavi-Dezfooli, A. Fawzi, and P. Frossard, “Deepfool: a simple
and accurate method to fool deep neural networks,” in Proc. IEEE Conf.
Comput. Vis. Pattern Recognit. , 2016, pp. 2574–2582.
[64] E. Wong, L. Rice, and J. Z. Kolter, “Fast is better than free: Revisiting
adversarial training,” in Proc. Int. Conf. Learn. Represent. , 2020.
[65] F. Croce and M. Hein, “Minimally distorted adversarial examples with
a fast adaptive boundary attack,” in Proc. Int. Conf. Mach. Learn. , 2020,
pp. 2196–2205.
[66] M. Andriushchenko, F. Croce, N. Flammarion, and M. Hein, “Square
attack: a query-efficient black-box adversarial attack via random search,”
inProc. Eur. Conf. Comput. Vis. , 2020, pp. 484–501.
[67] H. Zhang, Y . Yu, J. Jiao, E. Xing, L. El Ghaoui, and M. Jordan,
“Theoretically principled trade-off between robustness and accuracy,”
inProc. Int. Conf. Mach. Learn. , 2019, pp. 7472–7482.[68] R. S. Zimmermann, “Comment on ”Adv-BNN: Improved adversarial
defense through robust Bayesian Neural Network”,” arXiv preprint
arXiv:1907.00895 , 2019.
[69] K. R. Mopuri, U. Garg, and R. V . Babu, “Fast Feature Fool: A data
independent approach to universal adversarial perturbations,” in Proc.
Br. Mach. Vis. Conf. , 2017.
[70] L. Schwinn, R. Raab, A. Nguyen, D. Zanca, and B. Eskofier, “Exploring
misclassifications of robust neural networks to enhance adversarial
attacks,” Appl. Intell. , vol. 53, no. 17, pp. 19 843–19 859, 2023.
[71] R. Feinman, R. R. Curtin, S. Shintre, and A. B. Gardner, “Detecting
adversarial samples from artifacts,” arXiv preprint arXiv:1703.00410 ,
2017.
[72] S. Liang, Y . Li, and R. Srikant, “Principled detection of out-of-
distribution examples in neural networks,” in Proc. Int. Conf. Learn.
Represent. , 2018.
[73] G. Goswami, A. Agarwal, N. Ratha, R. Singh, and M. Vatsa, “Detecting
and mitigating adversarial perturbations for robust face recognition,” Int.
J. Comput. Vision , vol. 127, pp. 719–742, 2019.
[74] S. Ma, Y . Liu, G. Tao, W.-C. Lee, and X. Zhang, “Nic: Detecting
adversarial samples with neural network invariant checking,” in Proc.
Net. Distribut. Sys. Secur. Symp. , 2019.
[75] S. Yuan, Q. Zhang, L. Gao, Y . Cheng, and J. Song, “Natural color fool:
Towards boosting black-box unrestricted attacks,” in Proc. Adv. Neural
Inf. Proces. Syst. , vol. 35, 2022, pp. 7546–7560.
Chao Wang received the B.S. and M.S. degrees
from Liaoning Normal University, Dalian, China,
in 2017 and 2020, respectively. She is currently
pursuing the Ph.D. degree in computer science at
Nanjing University of Aeronautics and Astronautics,
Nanjing, China. She has authored or coauthored
academic papers in top-tier venues including IEEE
Transactions on Information Forensics and Security
and IEEE Transactions on Pattern Analysis and
Machine Intelligence . Her research interests include
trustworthy artificial intelligence, adversarial learn-
ing, and media forensics.
Shuren Qi received the Ph.D. degree in computer
science from Nanjing University of Aeronautics and
Astronautics, Nanjing, China, in 2024. He is cur-
rently a Postdoctoral Fellow with the Department
of Mathematics, The Chinese University of Hong
Kong, Hong Kong. He has published academic pa-
pers in top-tier venues including ACM Computing
Surveys and IEEE Transactions on Pattern Analy-
sis and Machine Intelligence . His research involves
the general topics of invariance, robustness, and
explainability in computer vision, with a focus on
invariant representations, for closing today’s trustworthiness gap in artificial
intelligence, e.g., forensic and security of visual data.
Zhiqiu Huang received the Ph.D. degree in com-
puter science from Nanjing University of Aeronau-
tics and Astronautics, Nanjing, China, in 1999. He
is a professor with the College of Computer Science
and Technology, Nanjing University of Aeronautics
and Astronautics, China. He has authored or coau-
thored more than 80 journal and conference papers.
His research interests include software engineering,
formal methods, and knowledge engineering.C. WANG et al. : SPATIAL-FREQUENCY DISCRIMINABILITY FOR REVEALING ADVERSARIAL PERTURBATIONS 16
Yushu Zhang (Senior Member, IEEE) received the
Ph.D. degree in computer science from Chongqing
University, Chongqing, China, in 2014. He held
various research positions with the City University
of Hong Kong, Southwest University, University
of Macau, and Deakin University. He is a Pro-
fessor with the College of Computer Science and
Technology, Nanjing University of Aeronautics and
Astronautics, Nanjing, China. His research interests
include multimedia processing and security, artificial
intelligence, and blockchain. Dr. Zhang is an Asso-
ciate Editor of Signal Processing andInformation Sciences .
Rushi Lan received the B.S. degree in information
and computational science and the M.S. degree
in applied mathematics from Nanjing University
of Information Science and Technology, Nanjing,
China, in 2008 and 2011, respectively, and the
Ph.D. degree in soft engineering from University
of Macau, Macau, China, in 2016. He is currently
a Professor with the Guangxi Key Laboratory of
Image and Graphic Intelligent Processing, Guilin
University of Electronic Technology, Guilin, China.
His research interests include image processing and
machine learning.
Xiaochun Cao (Senior Member, IEEE) received the
B.E. and M.E. degrees in computer science from
Beihang University, Beijing, China, in 1999 and
2002, respectively, and the Ph.D. degree in computer
science from the University of Central Florida, Or-
lando, FL, USA, in 2006. After graduation, he spent
about three years at ObjectVideo Inc., as a Research
Scientist. From 2008 to 2012, he was a Professor
at Tianjin University, Tianjin, China. Before joining
Sun Yat-sen University, Shenzhen, China, he was a
Professor at the Institute of Information Engineering,
Chinese Academy of Sciences, Beijing, China. He is a Professor and the
Dean with the School of Cyber Science and Technology, Shenzhen Campus
of Sun Yat-sen University. He has authored or coauthored more than 200
journal and conference papers. Dr. Cao’s dissertation was nominated for
the University Level Outstanding Dissertation Award. He was a recipient
of the Piero Zamperoni Best Student Paper Award at the International
Conference on Pattern Recognition , in 2004 and 2010. He was on the Editorial
Boards of IEEE Transactions on Circuits and Systems for Video Technology
and IEEE Transactions on Multimedia . He is on the Editorial Boards of
IEEE Transactions on Pattern Analysis and Machine Intelligence andIEEE
Transactions on Image Processing .
Feng-Lei Fan (Senior Member, IEEE) received the
B.S. degree in instrumentation engineering from
the Harbin Institute of Technology (HIT), Harbin,
China, in 2017, and the Ph.D. degree from Rens-
selaer Polytechnic Institute (RPI), Troy, NY , USA,
in 2021. He is currently a Research Assistant Pro-
fessor with the Department of Mathematics, The
Chinese University of Hong Kong, Hong Kong. He
has authored 26 papers in flagship AI and medical
imaging journals. His primary research interests lie
in deep learning theory and methodology, neuro-
science, and medical image processing. Dr. Fan served as a PC Member
in many conferences such as the International Joint Conference on Artificial
Intelligence and the Association for the Advancement of Artificial Intelligence .
He was a recipient of the 2021 International Neural Network Society Doctoral
Dissertation Award.