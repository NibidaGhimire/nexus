MMFORMER: MULTIMODAL TRANSFORMER USING MULTISCALE SELF-ATTENTION
FOR REMOTE SENSING IMAGE CLASSIFICATION
Bo Zhang1, Zuheng Ming2, Wei Feng3, Yaqian Liu1, Liang He1, Kaixing Zhao1
1School of Software, Northwestern Polytechnical University
2Laboratoire L2TI, Institut Galil Â´ee, Universit Â´e Sorbonne Paris Nord
3School of Electronic Engineering, Xidian University
ABSTRACT
To beneï¬t the complementary information between hetero-
geneous data, we introduce a new Multimodal Transformer
(MMFormer) for Remote Sensing (RS) image classiï¬cation
using Hyperspectral Image (HSI) accompanied by another
source of data such as Light Detection and Ranging (Li-
DAR). Compared with traditional Vision Transformer (ViT)
lacking inductive biases of convolutions, we ï¬rst introduce
convolutional layers to our MMFormer to tokenize patches
from multimodal data of HSI and LiDAR. Then we propose
a Multi-scale Multi-head Self-Attention (MSMHSA) module
to address the problem of compatibility which often limits
to fuse HSI with high spectral resolution and LiDAR with
relatively low spatial resolution. The proposed MSMHSA
module can incorporate HSI to LiDAR data in a coarse-to-ï¬ne
manner enabling us to learn a ï¬ne-grained representation. Ex-
tensive experiments on widely used benchmarks (e.g., Trento
and MUUFL) demonstrate the effectiveness and superiority
of our proposed MMFormer for RS image classiï¬cation.
Index Terms â€”Multimodal Transformer, Multi-Scale
Multi-head self-attention, RS image classiï¬cation
1. INTRODUCTION
Remote sensing (RS) has been playing an vital role in vari-
ous Earth Observing (EO) tasks because of its rapid imaging
feature and wide application prospect. Generally, RS can be
used (but not limited) in different tasks, such as landcover
classiï¬cation [1â€“3], mineral and forest resources exploration
[4], object/target detection [5, 6], environmental monitoring
[7], urban planning [8], biodiversity conservation, as well as
disaster response and management. With the ever-growing
availability of RS data, research on RS has begun to shift
to data-driven methods, and various Machine Learning (ML)
and Deep Learning (DL) models have been applied in RS
systems. However, in the past few years, most studies fo-
cused only on single EO sensors, such as HSI sensors, rather
Corresponding author. 2021050018@nwpu.edu.cn(Liang He)
Corresponding author. kaixing.zhao@nwpu.edu.cn(Kaixing Zhao)than combining different types of sensor data. Although HSI
acquired from different sensors can provide more rich spec-
tral information, it cannot differentiate the landcover objects
such as roads and roofs using the same materials [9]. In con-
trast, LiDAR provides elevation information that allows dis-
tinguishing objects with identical spectral signatures but dif-
ferent elevations, such as roads and roof built-in cement [10].
Multimodal data integration for RS classiï¬cation can a solu-
tion to the dilemma.
In recent years, DL methods have been widely used in
multimodal data fusion for RS image classiï¬cation [11, 12].
More recently, Vision Transformers (ViT) [13] using self-
attention considering the local-global context of an image
or image sequence has been becoming a new mainstream
for RS image classiï¬cation. SpectralFormer [14] learned the
spectral representation of neighboring bands using a cross-
layer encoder module of ViT. However, it neglected spatial
information and only uses spectral information. Swalpa et al.
proposed MFT [10] which incorporates HSI and other source
data to construct a multimodal ViT for RS image classiï¬ca-
tion. Nevertheless, MFT did not consider the compatibility
problem when fusing the data with a large resolution gap
such as HSI and LiDAR data which may affect the land-
cover classiï¬cation in complex scenes [15]. To leverage the
complementary information between the different modalities,
we propose a multimodal transformer using self-attention
between two modalities HSI and LiDAR data for RS image
classiï¬cation. Instead of using a vanilla ViT as in MFT or
SpectralForm, we propose a Multi-scale Multi-head Self-
Attention (MSMHSA) using different heads of transformer
aiming to better fuse the feature representations with different
resolutions of heterogeneous data from different modalities.
Besides, we also introduce convolutions to our MMFormer
using Convolutional Tokenization to tokenize HSI and Li-
DAR rather than using linear projection to encode Q, K, V
feature maps for self-attention.
The main contributions of this paper can be summa-
rized as follows: 1) We design a Multimodal Transformer
(MMFormer) for RS image classiï¬cation using HSI and
LiDAR data. 2) A Multi-scale Multi-Head Self-AttentionarXiv:2303.13101v1  [cs.CV]  23 Mar 2023(MSMHSA) implemented on a single transformer allows
better fusing of multimodal data of different resolutions. 3)
The introduction of convolutions to MMFormer to integrate
desirable convolution proprieties. 4) The state-of-art per-
formance demonstrates that the proposed MMFormer can
serve as an effective new backbone for multimodal RS image
classiï¬cation.
2. RELATED WORKS
In the past few years, various traditional methods have been
investigated to extract more effective information from multi-
source RS data, such as random forest (RF), morphological
proï¬les (MPs) [16], attribute proï¬les (APs) [17], extinction
proï¬les (EPs) [18] and subspace learning [19]. Ham et al.
investigated hierarchical classiï¬ers based on RF to improve
the generalization in analysis of quantity limited hyperspec-
tral data[20]. Recently, deep learning techniques have been
widely used in the task of multimodal data fusion for RS data
classiï¬cation and have shown excellent feature extraction
abilities. In [11], Makantasis et al. exploited a CNN2D based
network to encode pixelsâ€™ spectral and spatial information.
Vision Transformers methods used self-attention to learn the
local-global context of an HSI data. SpectralFormer [14]
used a cross-layer encoder module of ViT to learn the spec-
tral features between HSI bands. And Swalpa et al. proposed
MFT [10] which leverage the complementary information
between the different modalities to construct a multimodal
ViT for RS image classiï¬cation. Inspired by the former
methods, we propose a Multi-scale Multi-head Self-Attention
transformer to improve the overall fusion and classiï¬cation
accuracy.
3. METHODOLOGY
3.1. Overall Architecture
An overview of the proposed MMFormer is depicted in Fig-
ure 1. To balance the performance and the parameters of
model, we set the proposed transformer depth to 2.
Convolutional Tokenization . Unlike the vanilla Trans-
former, we use convolutional embedding instead of linear
projection to tokenize Q=K=V2RCAHAWAfeature
maps. Firstly, the 11*11 data cubes are padding to 16*16.
And sequential layers Conv3D [12] and HetConv2D [21] are
used to extract the feature map of HSI cube and reduce the
HSI spectral channels down to 64. A Conv2D layer is used
to extract LiDAR cubeâ€™s feature map and expend the band
to 64. After that, the feature maps, concatenated in the last
dimension, act as the input of convolutional embedding to get
theQ,KandV.
Then the obtained Q,KandVare fed into the MSMHSA
module to learn the local-global dependencies in the fu-sion data. In the output layer of MSMHSA module, we
also replace the linear layer and layer normalization with
a convolution layer(with kernel size = 3, padding = 1) and
a LeakyReLU activation function. The mentioned modules
could be deï¬ned as follow:
Q;K;V=Conv 2D(Xin;k= (1;1)) (1)
(
Xout=Conv 2D(Xin;k= (3;3);p= (1;1))
X=LeakyReLU (Xout;0:2)(2)
The details of MSMHSA module are described in Section 3.2.
Finally, we add Feed-Forward Network (FFN) with Norm lay-
ers at the end of transformer. In this work, linear projection
layers in FFN are also replaced by convolution layers. As in
ViT [22], an MLP Head is connected to the transformer to
generate the classiï¬cation embeddings.
3.2. Multi-scale Multi-Head Self-Attention (MSMHSA)
The goal of MSMHSA, as shown in Figure 1(b), is to in-
troduce a pyramid structure into the self-attention module to
generate multi-scale feature maps which can employ pixel-
level ï¬ne-grained feature fusion on the complimentary fea-
tures. The proposed MSMHSA is applied on different heads
of each layer of transformer. All the heads share the similar
protocol to calculate the self-attention.
In particular, the feature maps Q=K=Vare equally di-
vided to each head along the dimension before inputting them
to MSMHSA.
Given a transformer with three heads fed by input fea-
ture maps Q=K=Vof sizeCAHAWA, the feature
maps for each head are Qi=Ki=Vi2RCA
3HAWA. For
the ï¬rst head Head 1, we take a full-size patch q1=k1=v12
RCA
3HAWAto calculate a global self-attention feature
map for the ï¬rst head Head 1. We can obtain the self-
attention feature map h12RCA
3HAWAofHead 1. Then
forHead 2, we divide Q2=K2=V2into22patches, each
patch q2=k2=v2of size 4CA
3HA
2WA
2and then ob-
tainh22R4CA
3HA
2WA
2ofHead 2. We continue to
divide Q3=K3=V3into42patches to calculate the self-
attention feature map h32R16CA
3HA
4WA
4ofHead 3.
Finally, we concatenate the obtained self-attention feature
mapsfh1;h2;h3gto generate the ï¬nal multi-scale attention
feature map H2RCAHAWAin layerLi(We need to
reshape h2;h3to be consistent with h1):
H=Concat (h1;Reshape (h2);Reshape (h3)):(3)
The self-attention feature map hiis given by:
hi= N
mN
nSoftmax (qi;mkT
i;npdhead i;n)vi;n; (4)
whereiis corresponding to the ith headHead i,qi;mis
themth patch partitioned from feature map Qifor theHSI
LiDAR11Ã—11Ã—B
11Ã—11Ã—CUpsampling
Conv2DConv3DHetConv2D
16Ã—16Ã—64
16Ã—16Ã—64ConcatenationNormConvolutional 
Projection
Q
K
V...Concatenation
Head1 Head2 Head3Multi -Scale
MHSANormFeed ForwardMLP Categoriesğ¸ğ‘¥
QKVQKVQ
K
V
....SoftmaxQKV
.... ....
....
....Head 1ConcatenationMulti -Scale MHSA
Head 2
Head 3
Q
K
V
(a) (b)ZFig. 1 : (a) Overall architecture of the proposed : Multimodal Transformer Using Multiscale Self-Attention for RS image
classiï¬cation (MMFormer) introducing convolutions to transformers. (b) The proposed Multi-scale Multi-Head Self-Attention
(MSMHSA) module implemented on a single transformer.
ith headHead i,ki;n=vi;nare thenth patches partitioned
from feature maps Ki=Vifor theith headHead i. Then,
qi;m=ki;n=vi;n2RCA
3HA
lWA
l;l2[1;2;4]. andNis the
total number of patches of ith head, i.e., N=Tl2;l2
[1;2;4].dhead ijis the dimension of the qi;m. For each head
Head i, we stack the partitioned patches qi=ki=vifrom all
frames to calculate the self-attention feature map hi. Thus,
the self-attention of each head always considers simultane-
ously the local attention focusing on local spatial information,
i.e., using qi;m=ki;nfrom the neighborhood to calculate self-
attention, and global attention focusing on the global context
information calculated by qi;m=ki;nfrom far regions within
the image. Instead of adding an extra token as ViT for image
classiï¬cation, we learn a representation of a data sequence,
i.e., the output of MLP Head Z, based on all tokens with
cross-entropy loss to conduct ï¬nal landcover classiï¬cation in
this work.
4. EXPERIMENTS AND ANALYSIS
4.1. Experimental Setup
(1) Data Description To test the effectiveness of our pro-
posed MMFormer, we conduct experiments on two widely
used hyperspectral and LiDAR fusion datasets.
Trento dataset : this dataset contains similarly one HSI and
one LiDAR data, collected from a rural area south of the city
of Trento, Italy. HSI and LiDAR data are collected with 63
bands and 1 band speciï¬cally. Both types of data contain 166
Ã— 600 pixels with a spatial resolution of 1 m, containing a to-
tal of 6 different classes.
MUUFL dataset : this dataset was collected over the Uni-
versity of Southern Mississippi Gulf Park, both the HSI andTable 1 : OA, AA and Kappa values on the Trento data (in %)
by considering HSI and LiDAR data. The Best is shown in
bold .
ClassRF CNN2D ViTSpectral-MFT MMFormerNo. Former
1 83.73 Â± 00.06 96.98 Â± 00.21 90.87 Â± 00.77 96.76 Â± 01.71 98.23 Â± 00.38 99.71 Â± 0.25
2 96.30 Â± 00.06 97.56 Â± 00.14 99.32 Â± 00.77 97.25 Â± 00.66 99.34 Â± 00.02 98.06 Â± 0.80
3 70.94 Â± 01.55 55.35 Â± 00.00 92.69 Â± 01.53 58.47 Â± 11.54 89.84 Â± 09.00 94.47 Â± 1.77
4 99.73 Â± 00.07 99.66 Â± 00.03 100.0 Â± 00.00 99.24 Â± 00.21 99.82 Â± 00.26 99.96 Â± 0.02
5 95.35 Â± 00.25 99.56 Â± 00.07 97.77 Â± 00.86 93.52 Â± 01.75 99.93 Â± 00.05 99.90 Â± 0.07
6 72.63 Â± 00.90 76.91 Â± 00.15 86.72 Â± 02.02 73.39 Â± 06.78 88.72 Â± 00.94 95.34 Â± 1.32
OA 92.57 Â± 00.07 96.14 Â± 00.03 96.47 Â± 00.49 93.51 Â± 01.27 98.32 Â± 00.25 99.18 Â± 0.02
AA 86.45 Â± 00.32 87.67 Â± 00.04 94.56 Â± 00.57 86.44 Â± 02.96 95.98 Â± 01.64 97.91 Â± 0.25
 90.11 Â± 00.09 94.83 Â± 00.04 95.28 Â± 00.65 91.36 Â± 01.67 97.75 Â± 00.00 98.90 Â± 0.02
LiDAR data contain 325 Ã— 220 pixels. HSI initially contained
72 bands, however initial and ï¬nal four bands are removed
due to noise issues, the remaining 64 bands were used for the
experiment, and the LiDAR data contained 2 bands. There
are 11 different classes.
(2) Experimental Setup All the tests have been performed on
a CentOS Linux server(release 7.9.2009) and a single GPU,
Nvidia 3090 with 24576MB of VRAM.
A batch size of 64 and 500 has been used for training and
testing the performance of the considered models. The mod-
els have been trained with a Adam optimizer (learning rate =
5e-4, weight decay = 5e-3) and a step scheduler (step size = 50
and gamma = 0.9). Each experiment has been conducted us-
ing 500 epochs repeating 3 times and the average and standard
deviations are reported. The source code was implemented
using PyTorch 1.12.1 and Python 3.8.7.
(3) Evaluation metrics We evaluate the classiï¬cation perfor-
mance of our model quantitatively using three widely-used
metrics, i.e., overall accuracy (OA), average accuracy (AA)
and statistical Kappa ( ) coefï¬cients.(b) RF
 (c) CNN2D
 (a) GT
(d) ViT
 (e) SpectralFormer
 (f) MFT
(g) MMFormerbackground
buildings
woods
roadsapplesground
vineyardFig. 2 : (a) Ground truth and classiï¬cation maps inferred by: (b) RF,(c) CNN2D, (d) ViT, (e) SpectralFormer [14], (f) MFT, (g)
MMFormer on HSI and LiDAR data for the Trento dataset
Table 2 : OA, AA and Kappa values on the MUUFL data (in
%) by considering HSI and LiDAR data. The Best is shown
inbold .
ClassRF CNN2D ViTSpectral-MFT MMFormerNo. Former
1 95.42 Â± 00.09 95.79 Â± 00.11 97.85 Â± 00.29 97.30 Â± 00.83 97.90 Â± 00.39 98.88 Â± 0.15
2 74.03 Â± 00.11 72.76 Â± 00.58 76.06 Â± 02.40 69.35 Â± 05.16 92.11 Â± 01.58 88.84 Â± 1.66
3 75.81 Â± 00.38 78.92 Â± 00.52 87.58 Â± 03.46 78.48 Â± 03.41 91.80 Â± 00.82 90.00 Â± 0.80
4 68.59 Â± 00.77 83.59 Â± 00.99 92.05 Â± 02.31 82.63 Â± 03.68 91.59 Â± 02.25 95.19 Â± 0.24
5 88.17 Â± 00.18 78.29 Â± 01.12 94.73 Â± 00.60 87.91 Â± 02.97 95.60 Â± 01.21 95.28 Â± 0.48
6 77.28 Â± 00.93 50.34 Â± 02.13 82.02 Â± 01.13 58.77 Â± 02.76 88.19 Â± 03.49 88.48 Â± 0.97
7 64.83 Â± 00.97 79.70 Â± 00.26 87.11 Â± 01.54 85.87 Â± 00.62 90.27 Â± 02.13 92.94 Â± 1.14
8 93.29 Â± 00.27 71.95 Â± 01.10 97.60 Â± 00.16 95.60 Â± 01.26 97.26 Â± 00.53 97.84 Â± 0.53
9 19.15 Â± 01.37 43.92 Â± 01.24 57.83 Â± 04.45 53.52 Â± 04.32 61.35 Â± 03.80 65.02 Â± 1.79
10 04.41 Â± 00.72 12.45 Â± 00.27 31.99 Â± 08.86 08.43 Â± 02.22 17.43 Â± 04.63 36.97 Â± 3.39
11 71.88 Â± 00.84 26.82 Â± 02.60 58.72 Â± 03.85 35.29 Â± 06.00 72.79 Â± 09.25 80.85 Â± 5.58
OA 85.32 Â± 00.09 83.40 Â± 00.04 92.15 Â± 00.19 88.25 Â± 00.56 94.34 Â± 00.07 94.73 Â± 0.20
AA 66.62 Â± 00.16 63.14 Â± 00.21 78.50 Â± 01.28 68.47 Â± 01.44 81.48 Â± 00.70 84.57 Â± 0.35
 80.39 Â± 00.12 77.94 Â± 00.06 89.56 Â± 00.27 84.40 Â± 00.77 92.51 Â± 00.10 93.02 Â± 0.26
4.2. Quantitative analysis
Table 1 and Table 2 report the quantitative OA, AA, kappa
and each class accuracy on two widely used datasets Trento
and MUUFL to compare the proposed MMFormer with other
state-of-art methods, i.e., RF [20], CNN2D [11], ViT [22],
SpectralFormer [14] and MFT [10]. Our model MMFormer
obtains the best performance on all three indices on both
two benchmarks, such as 99.18% and 94.73% OA, 97.91%
and 84.57% AA and 98.90% and 93.02% Kappa on Trento
and MUUFL datasets. Our model has also achieved the best
performance for almost each class accuracy, and even it has
gained 20% improvement on landcover Yellow-Curb (class
10 ) compared to the latest MFT on MUUFL data.
4.3. Visualization
Figure 2 illustrates a qualitative evaluation by visualizing
the classiï¬cation maps obtained by different methods Trento
dataset using HSI and LiDAR data. The MSMHSA module
helps to learn the features in a ï¬ne-to-coarse manner, which
obtains a classiï¬cation map with less noise and ï¬ner details.
4.4. Ablation study
All ablation studies are conducted on the Trento dataset. And
the best results in the tables are shown in bold.Table 3 : Multimodal v.s.
single-modal MMFormer.
Class LiDAR only HSI only Multimodal
1 97.93 Â± 0.56 99.21 Â± 0.27 99.71 Â± 0.25
2 74.47 Â± 4.60 91.20 Â± 2.48 98.06 Â± 0.80
3 58.11 Â± 5.98 93.58 Â± 2.18 94.47 Â± 1.77
4 93.79 Â± 0.43 99.83 Â± 0.20 99.96 Â± 0.02
5 96.67 Â± 0.56 99.96 Â± 0.03 99.90 Â± 0.07
6 67.03 Â± 3.18 77.34 Â± 4.60 95.34 Â± 1.32
OA 90.29 Â± 0.45 96.56 Â± 0.60 99.18 Â± 0.02
AA 81.33 Â± 1.47 93.52 Â± 1.35 97.91 Â± 0.25
 86.99 Â± 0.61 95.39 Â± 0.81 98.90 Â± 0.02Table 4 : Effectiveness of
multiscale MMFormer.
16*16 8*8 4*4 2*2 OA AAp99.04 97.29 98.71p98.46 97.03 97.94p98.89 97.50 98.51p98.97 96.93 98.62p p99.06 96.72 98.74p p99.18 97.61 98.90p p99.17 97.58 98.89p p98.78 96.96 98.36p p98.70 96.11 98.26p p99.06 98.09 98.74p p p99.13 97.21 98.83p p p99.05 97.42 98.73p pp99.18 97.91 98.91p p p98.71 97.28 98.27p p p p99.14 97.22 98.85
Effectiveness of multimodal fusion. In order to evaluate
the effectiveness of multimodal fusion, we trained the pro-
posed model respectively on single modality and multimodal-
ities. Only one branch has been used to input the data when
training the model in single modality as shown in Figure 1.
In Table 3, we can see that the performance of multimodal
model is superior to single-modal either HSI or LiDAR for
all three indices, i.e., OA, AA, and almost for each class
accuracy, which demonstrate the effectiveness of our multi-
modal transformer MMFormer for RS image classiï¬cation.
Effectiveness of multi-scale MHSA. In Table 4, we can
see that the model using MSMHSA performs always better
than the one using single-scale MHSA, e.g., the model using
1616, 44, 22 gains 1%improvement in terms OA,AA, .
5. CONCLUSION
We design a Multimodal Transformer (MMFormer) which
enables us to explore the complementary information be-
tween spectral information in HSI and spatial information
in LiDAR for RS image classiï¬cation. The proposed Multi-
scale Multi-Head Self-Attention (MSMHSA) in Transformer
aims to better fuse the multimodal data with very different
resolutions. We also introduce convolutions to our model
to integrate desirable proprieties of CNNs which can gain a
good computation-accuracy balance. The effectiveness and
state-of-art performance demonstrate that the proposed MM-
Former can serve as a new valuable backbone for RS image
classiï¬cation.References
[1] Muhammad Ahmad, Sidrah Shabbir, et al., â€œHyperspec-
tral image classiï¬cationâ€”traditional to deep models: A
survey for future prospects,â€ IEEE Journal of Selected
Topics in Applied Earth Observations and Remote Sens-
ing, vol. 15, pp. 968â€“999, 2022.
[2] Etienne Bartholome and Allan S Belward, â€œGlc2000:
a new approach to global land cover mapping from
earth observation data,â€ International Journal of Remote
Sensing , vol. 26, no. 9, pp. 1959â€“1977, 2005.
[3] Swalpa Kumar Roy, Purbayan Kar, et al., â€œRevisit-
ing deep hyperspectral feature extraction networks via
gradient centralized convolution,â€ IEEE Transactions
on Geoscience and Remote Sensing , vol. 60, pp. 1â€“19,
2021.
[4] Benjamin Koetz, Felix Morsdorf, et al., â€œMulti-source
land cover classiï¬cation for forest ï¬re management
based on imaging spectrometry and lidar data,â€ Forest
Ecology and Management , vol. 256, no. 3, pp. 263â€“271,
2008.
[5] Xin Wu, Danfeng Hong, et al., â€œOrsim detector:
A novel object detection framework in optical remote
sensing imagery using spatial-frequency channel fea-
tures,â€ IEEE Transactions on Geoscience and Remote
Sensing , vol. 57, no. 7, pp. 5146â€“5158, 2019.
[6] Xin Wu, Danfeng Hong, et al., â€œFourier-based rotation-
invariant feature boosting: An efï¬cient framework for
geospatial object detection,â€ IEEE Geoscience and Re-
mote Sensing Letters , vol. 17, no. 2, pp. 302â€“306, 2019.
[7] Susan L Ustin, Manual of remote sensing, remote sens-
ing for natural resource management and environmental
monitoring , vol. 4, John Wiley & Sons, 2004.
[8] Chen Chen, Jining Yan, et al., â€œClassiï¬cation of urban
functional areas from remote sensing images and time-
series user behavior data,â€ IEEE Journal of Selected
Topics in Applied Earth Observations and Remote Sens-
ing, vol. 14, pp. 1207â€“1221, 2020.
[9] Pedram Ghamisi, Jon Atli Benediktsson, and Stuart R.
Phinn, â€œLand-cover classiï¬cation using both hyperspec-
tral and lidar data,â€ International Journal of Image and
Data Fusion , 2015.
[10] Swalpa Kumar Roy, Ankur Deria, et al., â€œMultimodal
fusion transformer for remote sensing image classiï¬ca-
tion,â€ arXiv preprint arXiv:2203.16952 , 2022.
[11] Konstantinos Makantasis, Konstantinos Karantzalos,
Anastasios Doulamis, and Nikolaos Doulamis, â€œDeep
supervised learning for hyperspectral data classiï¬cationthrough convolutional neural networks,â€ International
Geoscience and Remote Sensing Symposium , 2015.
[12] Amina Ben Hamida, Alexandre Benoit, Patrick Lam-
bert, and Chokri Ben Amar, â€œ3-d deep learning ap-
proach for remote sensing image classiï¬cation,â€ IEEE
Transactions on Geoscience and Remote Sensing , 2018.
[13] Ashish Vaswani, Noam Shazeer, et al., â€œAttention is all
you need,â€ Advances in neural information processing
systems , vol. 30, 2017.
[14] Danfeng Hong, Zhu Han, Jing Yao, Lianru Gao, Bing
Zhang, Antonio Plaza, and Jocelyn Chanussot, â€œSpec-
tralformer: Rethinking hyperspectral image classiï¬ca-
tion with transformers,â€ arXiv: Computer Vision and
Pattern Recognition , 2021.
[15] Lianru Gao, Danfeng Hong, Jing Yao, Bing Zhang,
Paolo Gamba, and Jocelyn Chanussot, â€œSpectral super-
resolution of multispectral imagery with joint sparse and
low-rank learning,â€ IEEE Transactions on Geoscience
and Remote Sensing , 2021.
[16] J Â´on Atli Benediktsson, J Â´on Palmason, et al., â€œClassiï¬-
cation of hyperspectral data from urban areas based on
extended morphological proï¬les,â€ IEEE Transactions
on Geoscience and Remote Sensing , vol. 43, no. 3, pp.
480â€“491, 2005.
[17] Mauro Dalla Mura, J Â´on Atli Benediktsson, et al., â€œMor-
phological attribute proï¬les for the analysis of very high
resolution images,â€ IEEE Transactions on Geoscience
and Remote Sensing , vol. 48, no. 10, pp. 3747â€“3762,
2010.
[18] Pedram Ghamisi, Roberto Souza, et al., â€œExtinction pro-
ï¬les for the classiï¬cation of remote sensing data,â€ IEEE
Transactions on Geoscience and Remote Sensing , vol.
54, no. 10, pp. 5631â€“5645, 2016.
[19] Fernando De La Torre and Michael J Black, â€œA frame-
work for robust subspace learning,â€ International Jour-
nal of Computer Vision , vol. 54, pp. 117â€“142, 2003.
[20] J. Ham, Yangchi Chen, et al., â€œInvestigation of the ran-
dom forest framework for classiï¬cation of hyperspectral
data,â€ IEEE Transactions on Geoscience and Remote
Sensing , vol. 43, no. 3, pp. 492â€“501, 2005.
[21] Pravendra Singh, Vinay Kumar Verma, et al., â€œHet-
conv: Heterogeneous kernel-based convolutions for
deep cnns,â€ in Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition ,
2019, pp. 4835â€“4844.
[22] Alexey Dosovitskiy, Lucas Beyer, et al., â€œAn image is
worth 16x16 words: Transformers for image recognition
at scale,â€ arXiv preprint arXiv:2010.11929 , 2020.