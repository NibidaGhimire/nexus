Collaborative Residual Metric Learning
Tianjun Wei
tjwei2-c@my.cityu.edu.hk
City University of Hong Kong
Kowloon, Hong KongJianghong Ma∗
majianghong@hit.edu.cn
Harbin Institute of Technology
Shenzhen, ChinaTommy W. S. Chow
eetchow@cityu.edu.hk
City University of Hong Kong
Kowloon, Hong Kong
ABSTRACT
In collaborative filtering, distance metric learning has been applied
to matrix factorization techniques with promising results. How-
ever, matrix factorization lacks the ability of capturing collaborative
information, which has been remarked by recent works and im-
proved by interpreting user interactions as signals. This paper aims
to find out how metric learning connect to these signal-based mod-
els. By adopting a generalized distance metric, we discovered that
in signal-based models, it is easier to estimate the residual of dis-
tances, which refers to the difference between the distances from
a user to a target item and another item, rather than estimating
the distances themselves. Further analysis also uncovers a link be-
tween the normalization strength of interaction signals and the
novelty of recommendation, which has been overlooked by existing
studies. Based on the above findings, we propose a novel model to
learn a generalized distance user-item distance metric to capture
user preference in interaction signals by modeling the residuals
of distance. The proposed CoRML model is then further improved
in training efficiency by a newly introduced approximated rank-
ing weight. Extensive experiments conducted on 4 public datasets
demonstrate the superior performance of CoRML compared to the
state-of-the-art baselines in collaborative filtering, along with high
efficiency and the ability of providing novelty-promoted recommen-
dations, shedding new light on the study of metric learning-based
recommender systems.
CCS CONCEPTS
•Information systems →Recommender systems ;Collabora-
tive filtering .
KEYWORDS
collaborative filtering, metric learning, recommender system
ACM Reference Format:
Tianjun Wei, Jianghong Ma, and Tommy W. S. Chow. 2023. Collaborative
Residual Metric Learning. In Proceedings of the 46th International ACM SIGIR
Conference on Research and Development in Information Retrieval (SIGIR
’23), July 23–27, 2023, Taipei, Taiwan. ACM, New York, NY, USA, 10 pages.
https://doi.org/10.1145/3539618.3591649
∗Corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
SIGIR ’23, July 23–27, 2023, Taipei, Taiwan
©2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-9408-6/23/07. . . $15.00
https://doi.org/10.1145/3539618.35916491 INTRODUCTION
A growing interest in Collaborative Filtering (CF) [ 1] has been seen
in both academia and industry. The main challenge of CF is to
handle the interactions between users and recommended targets,
often named as items [ 6]. Since this interaction can be naturally
represented by an interaction matrix, factorization-based models
have become one of the common paradigms in CF. The most basic
factorization-based model is the traditional Matrix Factorization
(MF) [ 12,19], where user-item preferences are computed by lightly
designed dot product of user and item embeddings. Against its
simplicity in design, MF is considered to lack the ability to capture
higher-order user-item relationships [ 26], which is questioned and
improved by emerging Graph Convolutional Network (GCN) mod-
els recently [ 9,26,27]. In contrast, signal-based models treat the
interaction matrix as signals for each user [ 20], and learn relation-
ships between items. A straightforward signal-based approach is
the linear autoencoder [ 17,23], which models item-item relations
as a square matrix of linear mappings, achieving competitive per-
formance against factorization-based models with high training
efficiency. A recent study [ 20] adopts graph signal processing to
handle user features and proposes a signal-based graph filtering
framework, also yielding competitive performance.
In recent years, growing attention has been paid to recommender
systems based on metric learning [ 29,32]. By learning a distance
metric, metric learning drives the distance between samples to
comply with their similarity or dissimilarity. In this way, metric
learning has a natural fit with CF, which aims to explore the re-
lationship between users and their interacted and uninteracted
items. The emergence of many CF models based on metric learning
[2,11,14,18,30] is consistent with this intuition, where the most
representative one is the Collaborative Metric Learning (CML) [ 11].
In [11], the authors point out that the dot product in the traditional
factorization-based models violates a crucial rule in a valid distance
metric, the triangle inequality , and therefore fails in capturing
fine-grained user-item relationship information. To overcome the
deficient, CML is proposed as a new framework for estimating
user-item preferences via Euclidean distances between embedding
vectors rather than dot product. CML establishes a connection be-
tween factorization-based model and metric learning in Euclidean
space, and its superior performance compared to MF models in-
spires a promising direction. However, metric learning on Euclidean
space lacks the ability to accommodate signal-based models, where
the user features are expressed using a fixed interaction signals.
To our advantage, research on metric learning is not limited to
the Euclidean distance, but has been extended to the generalized
Mahalanobis distance [ 8]. The paradigm of metric learning in a gen-
eralized scenario is similar to the signal-based CF models, thereby
tempting us to explore their connections. To this end, we are eager
to investigate the following research questions:arXiv:2304.07971v1  [cs.IR]  17 Apr 2023SIGIR ’23, July 23–27, 2023, Taipei, Taiwan Wei and Chow, et al.
•With the definition of generalized Mahalanobis distance, can
a signal-based model learn a valid distance metric? If so,
what conditions need to be satisfied?
•If a signal-based model can learn a distance metric, how the
characteristics of that metric will affect CF task in terms of
performance and other metrics, like novelty?
To answer the raised questions, we carry out an analysis on
existing signal-based models. First, on the basis of the ranking-
based feature in CF tasks, we conduct investigation on the relative
relationships of the distance between users and different items,
rather than the values of the absolute distance. Such differences,
also called as the residual1of distance, is shown to be associated
with the item-item relationship in the signal-based models. Specifi-
cally, when the symmetry and zero diagonal conditions of item-item
relationship matrix are satisfied, the residual of generalized Maha-
lanobis distances between different user-item pairs are explicitly
related to the residuals of the preference scores produced by a
signal-based model. With this observation, we are able to learn a
signal-based model to take advantage of metric learning and capture
fine-grained user-item relationships. Besides, we further explore
the role of the normalization strength of the interaction signals in
signal-based models and demonstrate its importance in mitigating
popularity bias of recommender systems and promoting the novelty
of the recommendations, which is overlooked by existing studies.
This motivated us to propose a new signal-based model to learn a
generalized distance metric, aiming to derive novelty-promoting
recommendations.
Based on the above analysis, we finally propose a novel model for
CF task, named Collaborative Residual Metric Learning (CoRML).
Specifically, by adopting widely used triplet margin loss in metric
learning, we propose a simplified residual margin loss to maximize
the residual of preference score between interacted and uninter-
acted items. Through this loss, CoRML can learn a generalized
Mahalanobis distance metric under any normalization strength,
which is an extension of existing signal-based models. By tuning
the normalized strength of the interaction signal, CoRML is able
to generate highly accurate recommendations while ensuring the
novelty of the recommendations. Then, by converting the original
pairwise learning objective to point-wise, and approximating the
dynamically updated ranking of items though a novel proposed
ranking weight, CoRML is further improved in training efficiency
compared with existing metric learning models in CF. Extensive
experiments on 4 public datasets shows that CoRML is able to pro-
duce novelty-promoting recommendations with ensuring superior
performance when comparing with state-of-the-art baselines. The
PyTorch implementation code of the proposed CoRML model is
publicly available at https://github.com/Joinn99/CoRML.
To summarize, the contributions of this paper are listed below:
•We reveal the connection between existing signal-based mod-
els and metric learning, and identify critical factors in such
models for promoting the novelty of recommendations.
•To address the limitations of existing models, we propose a
novel CoRML model that efficiently models the residuals of
the distance to capture user preferences.
1In this paper, "residual" refers to the difference in distances or scores between the
user’s pair with the target item and the pair of other items.•Extensive experiments demonstrate the superiority of CoRML
over state-of-the-art CF models in terms of recommendation
performance, training speed, and novelty.
2 PRELIMINARIES
2.1 Problem Formulation
In this paper, we focus on the Collaborative Filtering (CF) task with
user-item implicit feedbacks. Suppose the user set Uand item set
I. For each user 𝑢∈U, the non-empty set I𝑢⊆I denotes the
items that user 𝑢has interacted with. Then, given the interacted
item setI𝑢, the goal of CF is to generate an 𝐾-item candidate set
I′
𝑢(𝐾)⊂I\I𝑢as the recommended items for user 𝑢.
2.2 Metric Learning
Given a collection of data points {x𝑖}𝑁
𝑖=1⊂R𝑑with size𝑁, metric
learning aims to learn a distance metric to decrease the distance
between similar points and increase the distance between dissimilar
points [ 32]. The similarity of data points is typically determined by
the priori information, such as the class labels in a classification
problem. In metric learning, a widely adopted distance metric is
generalized Mahalanobis distance [ 8]. The generalized Mahalanobis
distance between data points x𝑖andx𝑗is defined as
𝑑(x𝑖,x𝑗)=√︃
(x𝑖−x𝑗)𝑇W(x𝑖−x𝑗), (1)
where Wis a symmetric positive semi-definite (PSD) weight matrix
to ensure the learned distance metric is valid and do not violate the
triangle inequality:
𝑑(x𝑖,x𝑗)≤𝑑(x𝑖,x𝑘)+𝑑(x𝑘,x𝑗). (2)
When Wis the identity matrix I, the distance measured is the
Euclidean distance between x𝑖andx𝑗. To derive W, metric learning
generally formulates an optimization problem by measuring the
similarity and dissimilarity of sample points. One of the classic
solutions is to minimize the distances between similar data points
and maximize the distances between dissimilar data points [7]:
minimize
W∑︁
(x𝑖,x𝑗)∈S𝑑(x𝑖,x𝑗)−∑︁
(x𝑖,x𝑗)∈D𝑑(x𝑖,x𝑗)
𝑠.𝑡.W⪰0,𝑡𝑟(W)=𝛼,(3)
whereSandDdenote the similar and dissimilar pairs of data
points, respectively. The trace of Wis restricted to be a positive
constant𝛼to prevent the trivial result W=0. To date, numerous
studies have extended the field of metric learning, while the core
ideas mentioned above are still retained [8].
2.3 Collaborative Metric Learning
With the growing interest in research on recommender systems,
various studies have focused on the role of metric learning in CF
tasks. To capture the user preference towards different target items,
CF models draw their attention to deal with the historical user-item
interactions. Given user set Uand item setI, the historical user-
item interactions with implicit feedbacks can be represented as an
interaction matrix R∈{0,1}|U|×|I|, which is defined as
𝑅𝑢𝑖=(
1,if interaction(𝑢,𝑖)is observed,
0,otherwise.(4)Collaborative Residual Metric Learning SIGIR ’23, July 23–27, 2023, Taipei, Taiwan
In CF, the classical matrix factorization (MF) [ 12] techniques have
been widely adopted to deal with R. MF factorizes Rto generate
𝑑-dimension embedding vectors E𝑈∈R|U|×𝑑for users and E𝐼∈
R|I|×𝑑for items by solving the following optimization problem:
minimize
EU,EI1
2∥R−E𝑈E𝑇
𝐼∥2
𝐹+𝜃
2(∥E𝑈∥2
𝐹+∥E𝐼∥2
𝐹), (5)
where𝜃is a hyperparameter for regularization. Learning embed-
ding vectors for users and items is followed by a great deal of
research and became a mainstream setting in CF tasks [9, 16].
Next, we will introduce how metric learning is used to refine CF
models. Collaborative Metric Learning (CML) [ 11] is first proposed
to formulate user preferences towards items from the perspective
of distance metrics. CML considers the Euclidean distance between
the user embedding vector e𝑢and the item embedding vector e𝑖as
the score of the user’s preference to the item, which is defined as:
𝑑(e𝑢,e𝑖)=∥e𝑢−e𝑖∥2=√︃
e𝑇𝑢e𝑢+e𝑇
𝑖e𝑖−2e𝑇𝑢e𝑖. (6)
In Eq. (6), the term e𝑇𝑢e𝑖is the score function in MF models. There-
fore, CML essentially adds the embedding norm to the preference
score to avoid violating triangle inequality, which is proved to be
effective in retaining fine-grained preference information [ 11]. In
CML, the interaction matrix Ris used to group similar and dissimi-
lar pairs of users and items. All user and item pairs (𝑢,𝑖)that have
𝑅𝑢𝑖=1are considered similar pairs, whose distances are optimized
to be smaller than other pairs through triplet margin loss:
minimize
EU,EI∑︁
(𝑢,𝑖)∈S∑︁
(𝑢,𝑗)∈D(𝑑2(e𝑢,e𝑖)−𝑑2(e𝑢,e𝑗)+𝜁)+,(7)
where𝜁is the hyperparameter denotes the margin of distance, S
andDdenote the set of pairs of user and interacted item, the set of
pairs of user and uninteracted item, respectively. In general, triplet
margin loss aims to pull all interacted items closer to user, and push
the uninteracted items farther away to a safety margin [11].
Although CML and subsequent studies [ 2,11,14] are conducted
based on the distance metric in the Euclidean space, they can still
be translated into generalized Mahalanobis distance by the transfor-
mation of the feature space. Suppose the feature space of users and
items is an identity matrix I∈R(|U|+|I|)×(|U|+|I|), the distance
between user 𝑢and item𝑖can be equivalently represented as
𝑑(i𝑢,i𝑖)=√︃
(i𝑢−i𝑖)𝑇EE𝑇(i𝑢−i𝑖), (8)
where E∈R(|U|+|I|)× 𝑑is the concatenated embedding vectors
of users and items, and the weight matrix W=EE𝑇is a rank-𝑑
symmetric PSD matrix.
2.4 Signal-based Models
In Section 2.3, we show that the traditional CML models based on
MF can be interpreted as a special case of generalized Mahalanobis
distance metric learning. Like Eq. (8), the features of both users and
items are represented as an identity matrix, while the parameters
are the low-rank approximation of the weight matrix W. Here, it is
apparent that CML does not take full advantage of the interaction
data to build the feature space. The interaction matrix Ris only used
to classify similar and dissimilar data points, leaving the feature
space to be orthogonal identity matrix.The same weakness also exists in traditional MF models, which
is questioned and improved by considering the interaction matrix
as signals of users [ 5]. Here, we consider the row-wise signals in R,
representing the interacted items for each user. Suppose the feature
space of users and items are P∈R|I|×|U|and Q∈R|I|×|I|
defined as follows:
P=D−𝑡
𝐼R𝑇,Q=D𝑡
𝐼, (9)
where D𝐼=𝑑𝑖𝑎𝑔(1𝑇R)is the degree matrix of items, 𝑡is a factor
for the normalization strength of signal. Suppose 𝑢-th column of P
isp𝑢and𝑖-th column of Qisq𝑖, signal-based models learn a weight
matrix C∈R|I|×|I|and generate preference score by
𝑦𝑢𝑖=p𝑇
𝑢Cq𝑖. (10)
One of the most well-known signal-based recommendation ap-
proaches is the linear autoencoder [17,23,24]. In general, linear
autoencoder learns the weight matrix by solving a constrained
optimization problem [23]:
minimize
C1
2∥R−RC∥2
𝐹, 𝑠.𝑡.𝑑𝑖𝑎𝑔(C)=0, (11)
where diagonal zero constraint is used to prevent trivial solution
C=I. On the other hand, a recent study formulates the CF problem
as a low-pass graph filtering process [ 5] and propose a graph
filtering model . It first performs singular value decomposition
(SVD) on the normalized interaction matrix D−1
2
𝑈RD−1
2
𝐼, which is
also known as graph Laplacian matrix. Here D𝑈=𝑑𝑖𝑎𝑔(R1)is the
degree matrix of users. Then, the right singular vectors V∈R|I|×𝑘
with top-𝑘largest singular values are applied to approximate C,
and generate recommendations by
RD−1
2
𝐼VV𝑇D1
2
𝐼. (12)
Taken together, both linear autoencoders and graph filtering model
can be considered as a special case of signal-based models, with
different formulations of Cand different settings in normalization
strength𝑡, respectively.
3 IDENTIFYING THE RELATIONSHIP
BETWEEN SIGNAL-BASED MODELS AND
METRIC LEARNING
In reviewing the discussion of metric learning, signal-based models
share many characteristics with Mahalanobis distance metric. This
makes us curious if the user and item relationships learned in signal-
based model can be expressed as generalized Mahalanobis distances.
If so, what conditions do the weight matrix need to satisfy? These
questions will be discussed in the following pages.
3.1 Can Signal-based Models Learn a Distance
Metric?
Suppose the feature spaces of users and items are PandQrespec-
tively, then the generalized Mahalanobis distance of user 𝑢and item
𝑖can be represented as
𝑑(p𝑢,q𝑖)=√︃
(p𝑢−q𝑖)𝑇W(p𝑢−q𝑖)
=√︃
p𝑇𝑢Wp𝑢+q𝑇
𝑖Wq𝑖−2p𝑇𝑢Wq𝑖.(13)SIGIR ’23, July 23–27, 2023, Taipei, Taiwan Wei and Chow, et al.
In Eq. (13), each terms are shared in different pairs of distances. For
example, the term p𝑇𝑢Wp𝑢exists in all distances that involve user
𝑢. In metric learning, what we are primarily concerned with is the
relative relationship of distances between similar and dissimilar
nodes. This is consistent with the objective of the CF task, which
achieves recommendations by sorting the preference scores for
users and different items. Therefore, we next focus on the residual
of distance between different pairs of user and item. Suppose that
user𝑢and two items 𝑖and𝑗, the residual of squared distances
𝑑2(p𝑢,q𝑖)and𝑑2(p𝑢,q𝑗)is derived as
Δ𝑑2=𝑑2(p𝑢,q𝑖)−𝑑2(p𝑢,q𝑗)
=q𝑇
𝑖Wq𝑖−q𝑇
𝑗Wq𝑗−2p𝑇
𝑢W(q𝑖−q𝑗)
=𝑊𝑖𝑖(𝑑2𝑡
𝑖−2𝑅𝑢𝑖)−𝑊𝑗𝑗(𝑑2𝑡
𝑗−2𝑅𝑢𝑗)−2p𝑇
𝑢H(q𝑖−q𝑗),(14)
where𝑑𝑖is the degree of item node 𝑖, also refers to 𝑖-th element of
D𝐼,His a diagonal-zero matrix contains the non-diagonal values of
W, also known as the Hollow matrix . To learn a valid generalized
Mahalanobis distance metric, Whas to be symmetric PSD. The next
proposition shows that the above condition is easy to satisfy with
only ensuring the symmetry of H.
Theorem 3.1. For any𝑛×𝑛symmetric hollow matrix Hand
positive vector x∈R𝑛+, there always exists a positive value 𝜔such
thatH+𝜔𝑑𝑖𝑎𝑔(x)⪰0.
Proof. Let𝜔=𝑚𝑎𝑥 1≤𝑖≤𝑛ℎ𝑖
𝑥𝑖, whereℎ𝑖is the sum of absolute
value of the non-diagonal entries in the 𝑖-th row of H. With Lemma
3.2, we can show that every eigenvalue of W=H+𝜔𝑑𝑖𝑎𝑔(x)lies
within at least one of the range [𝜔𝑥𝑖−ℎ𝑖,𝜔𝑥𝑖+ℎ𝑖]. As𝜔𝑥𝑖≥ℎ𝑖
for all 1≤𝑖≤𝑛, we have
𝜆𝑘(W)≥0,1≤𝑘≤𝑛, (15)
where𝜆𝑘(W)is the𝑘-th eigenvalue of W. Hence, W⪰0is proved.
□
Lemma 3.2 (Gershgorin circle theorem for symmetric real
matrix). LetAbe a symmetric square real matrix, 𝑟𝑖is the sum of
the absolute values of the non-diagonal entries in the 𝑖-th row of A:
𝑟𝑖=∑︁
𝑗≠𝑖|𝐴𝑖𝑗|. (16)
Then every eigenvalue of Alies within at least one of the range
[𝐴𝑖𝑖−𝑟𝑖,𝐴𝑖𝑖+𝑟𝑖].
Then, let𝑑𝑖𝑎𝑔(x)=D−2𝑡
𝐼, the residual of distance in Eq. (14)is
derived as
1
2Δ𝑑2= 
−˜𝑦𝑢𝑖𝑗, 𝑖 ∉I𝑢,𝑗∉I𝑢
−˜𝑦𝑢𝑖𝑗−𝜔𝑑−2𝑡
𝑖, 𝑖∈I𝑢,𝑗∉I𝑢
−˜𝑦𝑢𝑖𝑗−𝜔(𝑑−2𝑡
𝑖−𝑑−2𝑡
𝑗), 𝑖∈I𝑢,𝑗∈I𝑢,(17)
where ˜𝑦𝑢𝑖𝑗=𝑦𝑢𝑖−𝑦𝑢𝑗=p𝑇𝑢H(q𝑖−q𝑗). Here we name ˜𝑦𝑢𝑖𝑗aspref-
erence residual , as𝑦𝑢𝑖can be produced by signal-based models
like Eq. (10)as the preference score. Then, from Eq. (17), there are
several findings:
(1)When a user has not interacted with both item 𝑖and𝑗,Δ𝑑2
can be obtained by ˜𝑦𝑢𝑖𝑗without error.
(2)When a user has only interacted with the item 𝑖, there is
always a positive margin between −2˜𝑦𝑢𝑖𝑗andΔ𝑑2.
Direction of
Optimization
Update 
 
 
Contour  
of Graph Filtering Model  
( )Linear Autoencoder  
( )Figure 1: Illustration of signal-based models in the view of
metric learning.
(3)When a user has interacted with item 𝑖and𝑗, the difference
between Δ𝑑2and−2˜𝑦𝑢𝑖𝑗is varied based on 𝑑𝑖and𝑑𝑗, which
is a constant 0only when𝑡=0.
And conclusions can be drawn from these findings:
•Conclusion 1 : The ranking of the generalized Mahalanobis
distances between user and all uninteracted items can be ac-
curately derived by the preference residual. This is essential
for CF tasks, which typically generates recommendations by
sorting the preference scores of uninteracted items.
•Conclusion 2 : When the user’s interacted items are consid-
ered, the preference residual is in general biased compared
to the residual of generalized Mahalanobis distance.
3.2 Revisiting Existing Signal-based Models
Next, we revisit the signal-based models above with considering
the learning of generalized Mahalanobis distance metrics. From
Eq.(11)and Eq. (12), by ensuring the symmetry and zero diagonal
of the weight matrix, linear autoencoders and graph filter models
can be treated as the case of 𝑡=0and𝑡=0.5in our proposed
framework, respectively. Figure 1 visualizes the relations of the
preference scores and distances under different cases.
1) Linear autoencoder drives the preference score of interacted
item𝑦𝑢𝑖+to 1 and the preference score of uninteracted item 𝑦𝑢𝑖−to
0 to learn the weight matrix. According to Eq. (17), when𝑡=0, thisCollaborative Residual Metric Learning SIGIR ’23, July 23–27, 2023, Taipei, Taiwan
learning goal is equivalent to updating the contour of the distance
𝑑(p𝑢,q𝑖)so that all interacted or uninteracted items are on the same
contour. The target contour of the distance for interacted items is
smaller than the contour for uninteracted items by√
2+2𝜔.
2) Graph filtering model shows the case of 𝑡=0.5, where the
above equivalence relationship of distance residual and preference
residual does not hold for the interacted items. Since the graph
filtering model is training-free, this feature does not affect the
producing of recommendations, as the equivalence of 𝑑(p𝑢,q𝑖)and
𝑦𝑢𝑖still holds for uninteracted items. However, it poses a problem
for the trainable model to apply this normalization strategy, as it
will inevitably consider the interacted items.
Thus far, we have revealed the connections between signal-based
models and distance metric learning. The different strategies of
linear autoencoders and graph filtering models in the normalization
strength motivate us to seek to build a trainable signal-based model
and generalize it to all cases of 𝑡. But before that, one question
still needs to be answered: What impact does this normalization
strength actually have on the results of the recommendation?
3.3 Effect of Normalization Strength
To identify the effect of normalization strength, we consider the
case with involving items with different popularity. When the user
𝑢is specified, 𝑦𝑢𝑖in Eq. (10) can be written as
𝑦𝑢𝑖=p𝑇
𝑢(𝑑𝑡
𝑖·c𝑖), (18)
where c𝑖is the𝑖-th column of C. Now, we consider a general opti-
mization problem that introduces non-negative restrictions and 𝑙2
regularization, which are commonly adopted in the matrix factor-
ization [ 22] and the learning of linear autoencoders [ 17,25]. The
optimization problem of Cis formulated as
minimize
CL(C)+∥ C∥2
𝐹, 𝑠.𝑡. C≥0, (19)
whereL(C)is the original loss function for C. The inclusion of 𝑙2
regularization results in smaller entries in C, and the non-negative
constraint makes all preference score 𝑦𝑢𝑖positive. Then, we can eas-
ily establish the following connection between preference score and
item popularity. When 𝑡grows, more popular items with greater
𝑑𝑖are more affected, resulting in larger value of 𝑦𝑢𝑖. At this point,
in order to obtain the same preference score 𝑦𝑢𝑖, unpopular item
requires larger values in c𝑖, which is being penalized by 𝑙2reg-
ularization. Thus, greater 𝑡will facilitate signal-based models to
generate recommendations of highly popular items. Conversely,
a negative𝑡will lead to higher preference scores for less popular
items, promoting the novelty of the recommendation. This mo-
tivates us to extend the linear autoencoders and graph filtering
models to all cases of normalization strength, thus improving the
recommended performance while taking novelty into account.
4 COLLABORATIVE RESIDUAL METRIC
LEARNING
In this section, we introduce our proposed CoRML model in detail.
4.1 Triplet Residual Margin Loss
The triplet margin loss formulated in Eq. (7)has been widely used
in metric learning models in CF task [ 2,11,18]. The objective oftriplet margin loss is to keep the distance between dissimilar nodes
at least greater than the distance between similar nodes up to the
margin𝜁. A variation in the margin setting can be seen between
different models, which can be a fixed hyperparameter [ 11] or
the trainable parameters for each user and item [ 14]. As triplet
margin loss focuses on the residual of distance, it is consistent
with the design of the preference residual. However, according
to finding (2) in Section 3.1, the always present bias results in an
inaccurate reflection of the distance by the preference residual.
Meanwhile, the nature of this bias provides us with an idea of using
the bias to substitute the margin in the triplet margin loss. Since
the bias is always positive and is only proportional to the degree
of the interacted items, it can act as an adaptive margin in the
loss function. Based on the above discussions, we derive the triplet
residual margin loss L𝑇𝑅𝑀 as follows:
L𝑇𝑅𝑀=∑︁
𝑢∈U∑︁
𝑖+∈I𝑢,𝑖−∉I𝑢(−˜𝑦𝑢𝑖+𝑖−)+
=∑︁
𝑢∈U∑︁
𝑖+∈I𝑢,𝑖−∉I𝑢(𝑦𝑢𝑖−−𝑦𝑢𝑖+)+,(20)
where(·)+preserve all positive values and set all negative values to
zero. By minimizing the preference residual when the recommenda-
tion score of the uninteracted item larger than the interacted item,
this loss function will learn a generalized Mahalanobis distance
metric to pull the interacted item closer, and push the uninteracted
item away and beyond a positive margin. The L𝑇𝑅𝑀 in Eq. (20)is
formulated based on triplets of users and two items. For simplicity,
we combine recommendation score terms in different triplets and
normalize the weights for each user-item pair. The loss function of
L𝑇𝑅𝑀 can be rewritten as
L𝑇𝑅𝑀=∑︁
𝑢∈U(∑︁
𝑖+∈I𝑢𝛼𝑢𝑖+𝑦𝑢𝑖++∑︁
𝑖−∉I𝑢𝛽𝑢𝑖−𝑦𝑢𝑖−). (21)
Here,𝛼𝑢𝑖+and𝛽𝑢𝑖−are the weights defined as
𝛼𝑢𝑖+=∑︁
𝑖−∉I𝑢−𝛿(𝑦𝑢𝑖−>𝑦𝑢𝑖+)
|I|−|I𝑢|, 𝛽𝑢𝑖−=∑︁
𝑖+∈I𝑢𝛿(𝑦𝑢𝑖−>𝑦𝑢𝑖+)
|I𝑢|,(22)
where𝛿(·)is the indicator function equals to 1 when the condition
is satisfied and 0 otherwise.
4.2 Approximated Ranking Weights
In Eq. (21), the weights 𝛼and𝛽are dependent on the ordering rela-
tionship of𝑦𝑢𝑖with the same 𝑢and different 𝑖. Since𝑦𝑢𝑖is changed
during the optimization, 𝛼and𝛽need to be updated by sorting 𝑦𝑢𝑖
of all items at each iteration, incurring highly expensive computa-
tional costs. Here, instead of seeking exact numerical values of 𝛼
and𝛽, we turn our attention to the relationships between different
(𝑢,𝑖)pairs. From Eq. (22), given a specific 𝑢,𝛼𝑢𝑖is always negative
and its absolute value decreases with the growth of 𝑦𝑢𝑖. In contrast,
𝛽𝑢𝑖is always positive and increases when 𝑦𝑢𝑖is growing. This pro-
vides us with an idea to approximate 𝛼and𝛽by the numerical
value of𝑦𝑢𝑖. Here, we propose the approximated ranking weights
˜𝛼and ˜𝛽as
˜𝛼𝑢𝑖+=𝜙𝑦𝑢𝑖+−1,˜𝛽𝑢𝑖−=𝜙𝑦𝑢𝑖−. (23)
Since the original ranking weights 𝛼and𝛽are normalized to[−1,0]
and[0,1]respectively, we introduce a factor 𝜙to obtain the similarSIGIR ’23, July 23–27, 2023, Taipei, Taiwan Wei and Chow, et al.
effect by scaling the preference score 𝑦𝑢𝑖. The definition of 𝜙is
categorized into the following two components:
•Global scaling: Scale all preference scores 𝑦𝑢𝑖with a fixed
global factor.
•User-degree scaling: The user’s degree 𝑑𝑢indicates the num-
ber of non-zero elements in the user’s signal, which may
result in the preference scores 𝑦𝑢𝑖of different users being
in different ranges. For this reason, we use a scaling factor
based on the user’s degree to adjust the range of 𝑦𝑢𝑖.
Then the scaling factor 𝜙for user𝑢is then formulated as
𝜙𝑢=𝜖(𝑑𝑢
𝑚𝑎𝑥𝑢∈U(𝑑𝑢))−𝑡𝑢, (24)
where𝜖is the global scaling hyperparameter, and 𝑡𝑢is a normal-
ization factor for user-degree scaling. With a suitable adjustment
of𝜙𝑢, these approximated weights can then satisfy the conditions
discussed above and preserve the relative relationships. By replac-
ing𝛼and𝛽in loss Eq. (21)with ˜𝛼and ˜𝛽respectively, we obtain the
loss function of Collaborative Residual Metric Learning (CoRML)
L𝐶𝑜𝑅𝑀𝐿 as
L𝐶𝑜𝑅𝑀𝐿 =∑︁
𝑢∈U∑︁
𝑖∈I𝑦𝑢𝑖(𝜙𝑢𝑦𝑢𝑖−𝑅𝑢𝑖)=𝑡𝑟(Y𝑇(ΦY−R)),(25)
where Φis a diagonal matrix containing 𝜙𝑢for each user, and
Y∈R|U|×|I|is the preference score matrix. Then, inspired by the
linear autoencoder and graph filtering models, we design a hybrid
preference score for CoRML as
Y=R(𝜆D−𝑡
𝐼Hd𝑡
𝐼+(1−𝜆)D−1
2
𝐼GD1
2
𝐼), (26)
where G=(VV𝑇−𝑑𝑖𝑎𝑔(VV𝑇))+is obtained by applying positive
and diagonal zero constraints to the weight matrix in the graph
filtering model. Finally, the optimization problem in CoRML is
formulated as
minimize
H𝑡𝑟(Y𝑇(ΦY−R)),
𝑠.𝑡. 𝑑𝑖𝑎𝑔(H)=0,H≥0,H=H𝑇.(27)
4.3 Optimization
The steps of solving problem in Eq. (27)contain the Sylvester equa-
tion, which is not easy to find an closed-form solution. Therefore,
we transform the original problem by multiplying both YandR
in Eq. (27)by a term D−𝑡
𝐼. The derived problem is shown below,
which can be efficiently solved with Alternating Directions Method
of Multipliers (ADMM) [4, 25]:
minimize
H,Z𝑡𝑟(D−𝑡
𝐼Y𝑇(ΦY−R)D−𝑡
𝐼)+𝜃
2∥D1
2
𝐼H∥2
𝐹
𝑠.𝑡. 𝑑𝑖𝑎𝑔(H)=0,Z≥0,Z=Z𝑇,H=Z,(28)
where𝜃is introduced to control the strength of 𝑙2regularization.
The regularization term of each row in His weighted by D𝐼based
on their occurrence in L𝐶𝑜𝑅𝑀𝐿 . Then, Hcan be updated by adopt-
ing augmented Lagrangian method, and Zcan be updated by the
analytic solution of the continuous Lyapunov equation [ 3]. The
derived matrix Hwill be used to generate the preference score for
each user-item pair through Eq. (26).Table 1: Statistics of datasets.
Dataset #User #Item #Interaction Density (%)
Pinterest 55,187 9,916 1,463,581 0.2675
Gowalla 29,858 40,981 1,027,370 0.0840
Yelp2018 31,668 38,048 1,561,406 0.1296
ML-20M 136,674 13,680 9,977,451 0.5336
5 EXPERIMENT
5.1 Experimental Setup
5.1.1 Datasets and metrics. We conduct the experiment on four
public available datasets: Pinterest ,Gowalla ,Yelp2018 andML-20M .
ForML-20M dataset, users with at least 5 interactions are retained
for consistency with previous studies [ 15,21]. The statistics of
datasets are summarized in Table 1. In each dataset, the interac-
tions are split into train set, valid set and test set with the ratio
of 0.6/0.2/0.2. The model performance are evaluated based on two
widely used metrics in CF task: Normalized Discounted Cumulative
Gain at𝐾(NDCG@𝐾) and Mean Reciprocal Rank at 𝐾(MRR@𝐾),
where𝐾is set to 5, 10 and 20, respectively.
5.1.2 Baselines. Several types of baselines are involved in the per-
formance comparison with CoRML:
•Metric learning : Classical CML [ 11] and the latest DPCML
[2] designed to promote the diversity of recommendations.
In addition, we replace the embeddings in CML with the
embeddings produced by graph convolution in LightGCN
[9] to incorporate graph neighboring information. The model
is named L-CML.
•Autoencoder : Linear autoencoder SLIM [ 17], EASE [ 23],
and non-linear autoencoder RecVAE [21].
•Graph filtering model : GFCF [20].
•GCN model : UltraGCN [ 16] and the state-of-the-art SimGCL
[34] based on contrastive learning.
5.1.3 Hyperparameter Tuning. To make a fair comparison, we
make consistent settings on some key hyperparameters for all com-
parison models. For all baselines iteratively train the embedding
vectors of users and items, optimizer Adam is used with learning
rate set to 1e-3, the embedding size is fixed to 64, and the train-
ing batch size is set to 4096. For autoencoders and CoRML, the
learned weight matrix can be dense or sparse, where the sparsity
cannot be explicitly set. To maintain consistency, we perform a
sparse approximation [ 25] of the derived matrices C(equivalent
toHin CoRML) by setting the entries to 0 where |C|≤𝛾. The
threshold𝛾will be adjusted so that the storage size of the sparse
matrix C𝑠𝑝𝑎𝑟𝑠𝑒 is less than other types of models with embedding
size 64. All sparse matrices are stored in compressed sparse row
(CSR) format, which contains approximately twice the parameter
numbers as the number of non-zero values (NNZ) in C𝑠𝑝𝑎𝑟𝑠𝑒 . For
other hyperparameters, a five-fold cross-validation is performed
on each model to fine-tune the hyperparameters. For CoRML, 𝜆is
tuned between 0 and 1 with the step size of 0.05, 𝜖and𝜃are tuned
in [0.01,0.1,1], 𝑡𝑢is chosen in [0, 0.5, 1], and 𝑡is tuned between -0.2
to 0.2 with the step 0.05.Collaborative Residual Metric Learning SIGIR ’23, July 23–27, 2023, Taipei, Taiwan
Table 2: Performance comparison on 4 datasets.
Dataset Metric CML L-CML DPCML SLIM EASE RecVAE GFCF UltraGCN SimGCL CoRML
PinterestNDCG@5 0.0509 0.0594 0.0563 0.0488 0.0558 0.0516 0.0620 0.0572 0.0616 *0.0655
NDCG@10 0.0665 0.0766 0.0724 0.0630 0.0704 0.0668 0.0785 0.0729 0.0783 *0.0824
NDCG@20 0.0897 0.1021 0.0965 0.0841 0.0921 0.0895 0.1031 0.0962 0.1031 *0.1076
MRR@5 0.1018 0.1186 0.1133 0.0957 0.1125 0.1024 0.1239 0.1146 0.1237 *0.1306
MRR@10 0.1164 0.1343 0.1283 0.1084 0.1262 0.1164 0.1390 0.1292 0.1390 *0.1458
MRR@20 0.1261 0.1444 0.1381 0.1171 0.1353 0.1258 0.1488 0.1387 0.1488 *0.1556
GowallaNDCG@5 0.0853 0.0985 0.0999 0.1100 0.1211 0.0890 0.1174 0.1108 0.1229 *0.1317
NDCG@10 0.0953 0.1093 0.1087 0.1156 0.1268 0.0978 0.1257 0.1181 0.1295 *0.1383
NDCG@20 0.1125 0.1281 0.1261 0.1302 0.1412 0.1140 0.1440 0.1348 0.1460 *0.1554
MRR@5 0.1533 0.1743 0.1811 0.1912 0.2186 0.1613 0.2121 0.2001 0.2235 *0.2334
MRR@10 0.1682 0.1899 0.1957 0.2043 0.2323 0.1752 0.2269 0.2144 0.2377 *0.2479
MRR@20 0.1768 0.1984 0.2040 0.2118 0.2393 0.1832 0.2352 0.2225 0.2454 *0.2558
Yelp2018NDCG@5 0.0483 0.0574 0.0556 0.0535 0.0611 0.0525 0.0587 0.0585 0.0646 *0.0690
NDCG@10 0.0521 0.0617 0.0592 0.0554 0.0628 0.0558 0.0617 0.0621 0.0676 *0.0716
NDCG@20 0.0629 0.0742 0.0709 0.0644 0.0722 0.0663 0.0731 0.0737 0.0795 *0.0832
MRR@5 0.1007 0.1188 0.1156 0.1117 0.1277 0.1106 0.1236 0.1234 0.1349 *0.1435
MRR@10 0.1149 0.1345 0.1304 0.1245 0.1413 0.1247 0.1380 0.1385 0.1499 *0.1586
MRR@20 0.1241 0.1443 0.1399 0.1327 0.1496 0.1336 0.1472 0.1478 0.1594 *0.1679
ML-20MNDCG@5 0.2319 0.2731 0.2620 0.2785 0.3025 0.3045 0.2718 0.2365 0.2675 *0.3189
NDCG@10 0.2326 0.2689 0.2588 0.2710 0.2934 0.3033 0.2671 0.2280 0.2644 *0.3103
NDCG@20 0.2486 0.2832 0.2725 0.2813 0.3036 0.3204 0.2799 0.2369 0.2794 *0.3212
MRR@5 0.3761 0.4341 0.4190 0.4478 0.4829 0.4777 0.4356 0.3919 0.4310 *0.4967
MRR@10 0.3932 0.4494 0.4347 0.4621 0.4963 0.4923 0.4506 0.4063 0.4466 *0.5098
MRR@20 0.4002 0.4554 0.4409 0.4677 0.5014 0.4976 0.4566 0.4124 0.4527 *0.5149
In each metric, the best result is bolded and the runner-up is underlined. * indicates the statistical significance of 𝑝<0.01.
5.2 Performance Comparison
We conduct all experiments with the same Intel(R) Core(TM) i9-
10900X CPU @ 3.70GHz machine with a Nvidia RTX A6000 GPU.
Table 2 reports the performance comparison on 4 public datasets.
The highlights of Table 2 are summarized as follows:
1) Among metric learning baselines, L-CML shows competitive or
superior performance compared to the original CML and the latest
MF-based metric learning model DPCML. It provides evidence that
GCN is effective in capturing higher-order relationships between
users and items, as well as bringing performance improvement of
metric learning models.
2) Despite the fact that the performance of different baseline
methods varies on datasets, trends can be observed based on the
types and characteristics of the datasets. On Gowalla andYelp2018
datasets, GCN models demonstrate better performance among all
the baseline models. One possible reason is the balance between the
number of user and item. Since GCN models learn embeddings with
a fixed length for each user and item, it may experience performance
degradation when the number of user and item is unbalanced.
3) Signal-based models, including graph filtering model GFCF
and autoencoders, show superior performance on denser Pinterest
andML-20M datasets. Different normalization strengths, i.e., the
choice of𝑡in signal-based models, may explain the difference of
their performance on such two datasets.
4) Overall, our proposed CoRML shows superior performance
on all datasets. This can be attributed to the adoption of the idea of
metric learning and the extension of the signal-based models. The
former achieves similarity propagation by learning a valid distance
metric, and the latter can capture various characteristics of signals
under different normalization strength.5.3 Benefits of CoRML
5.3.1 Mitigating item popularity bias. In recent works in CF, the
popularity bias has been brought to light in recommendation sce-
narios [ 35,38]. In CoRML, the normalization strength 𝑡has been
previously discussed to be associated with the popularity of items in
Section 3.3. In order to ascertain how 𝑡affects the performance and
novelty of recommendations, we conduct experiments on CoRML
and two chasing baselines. The performance of the recommenda-
tion is still measured by MRR and NDCG, while the novelty is
measured by a metric introduced by [37] as:
𝑁𝑜𝑣@𝐾=1
|U|𝐾∑︁
𝑢∈U∑︁
𝑖∈I′𝑢(𝐾)−1
𝑙𝑜𝑔2|U|𝑙𝑜𝑔2𝑑𝑖
|U|, (29)
whereI′𝑢(𝐾)is the top-𝐾items recommended for user 𝑢. Figure 2
shows the results on the Gowalla andML-20M datasets.
•Clearly, the novelty of the recommendations of CoRML is de-
creasing as𝑡increases from -0.2 to 0.2, indicating more pop-
ular items are recommended to users. These results provide
further support for the discussion in Section 3.3, indicating
the ability of CoRML to control the novelty of recommenda-
tion and reduce popularity bias.
•In contrast to the monotonic variation of novelty with 𝑡,
there is a peak in the performance of the recommendations
when𝑡varies between -0.2 and 0.2. For MRR and NDCG
metrics, CoRML showed differences in two tested datasets,
with optimal values obtained at 𝑡=0.05 and𝑡=0.1 respec-
tively. It shows that the performance and novelty of the
recommendations are not just trade-offs.
•When compared to baselines with item popularity taken into
account, CoRML can ensure superiority both in performanceSIGIR ’23, July 23–27, 2023, Taipei, Taiwan Wei and Chow, et al.
/uni00000013/uni00000011/uni00000015/uni00000016/uni00000013/uni00000011/uni00000015/uni00000017/uni00000013/uni00000011/uni00000015/uni00000018/uni00000030/uni00000035/uni00000035/uni00000023/uni00000014/uni00000013
/uni00000013/uni00000011/uni00000014/uni00000016/uni00000013/uni00000011/uni00000014/uni00000017/uni00000031/uni00000027/uni00000026/uni0000002a/uni00000023/uni00000014/uni00000013
/uni00000013/uni00000011/uni00000015
 /uni00000013/uni00000011/uni00000014
 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000014 /uni00000013/uni00000011/uni00000015
t/uni00000013/uni00000011/uni00000018/uni00000017/uni00000013/uni00000011/uni00000018/uni0000001c/uni00000013/uni00000011/uni00000019/uni00000017/uni00000031/uni00000052/uni00000059/uni00000023/uni00000014/uni00000013
/uni0000002a/uni00000052/uni0000005a/uni00000044/uni0000004f/uni0000004f/uni00000044
/uni00000026/uni00000052/uni00000035/uni00000030/uni0000002f /uni00000036/uni0000004c/uni00000050/uni0000002a/uni00000026/uni0000002f /uni0000002a/uni00000029/uni00000026/uni00000029
/uni00000013/uni00000011/uni00000017/uni0000001b/uni00000013/uni00000011/uni00000018/uni00000014/uni00000030/uni00000035/uni00000035/uni00000023/uni00000014/uni00000013
/uni00000013/uni00000011/uni00000015/uni0000001b/uni00000013/uni00000011/uni00000016/uni00000014/uni00000031/uni00000027/uni00000026/uni0000002a/uni00000023/uni00000014/uni00000013
/uni00000013/uni00000011/uni00000015
 /uni00000013/uni00000011/uni00000014
 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000014 /uni00000013/uni00000011/uni00000015
t/uni00000013/uni00000011/uni00000015/uni00000013/uni00000013/uni00000011/uni00000015/uni00000017/uni00000013/uni00000011/uni00000015/uni0000001b/uni00000031/uni00000052/uni00000059/uni00000023/uni00000014/uni00000013
/uni00000030/uni0000002f/uni00000010/uni00000015/uni00000013/uni00000030
/uni00000026/uni00000052/uni00000035/uni00000030/uni0000002f /uni00000028/uni00000024/uni00000036/uni00000028 /uni00000035/uni00000048/uni00000046/uni00000039/uni00000024/uni00000028
Figure 2: Effects of the normalization strength 𝑡on CoRML.
100/uni00000003/uni00000056
101/uni00000003/uni00000056
102/uni00000003/uni00000056103/uni00000003/uni000000565×103/uni00000003/uni00000056
/uni0000002a/uni00000029/uni00000026/uni00000029/uni00000003/uni00000028/uni00000024/uni00000036/uni00000028/uni00000003/uni00000026/uni00000052/uni00000035/uni00000030/uni0000002f/uni00000003/uni00000036/uni0000002f/uni0000002c/uni00000030/uni00000003/uni00000027/uni00000033/uni00000026/uni00000030/uni0000002f/uni00000003/uni00000035/uni00000048/uni00000046/uni00000039/uni00000024/uni00000028/uni00000003/uni00000036/uni0000004c/uni00000050/uni0000002a/uni00000026/uni0000002f/uni00000003
Figure 3: Comparison of training time on ML-20M dataset.
and novelty. This phenomenon suggests that it is possible
to ensure recommendation performance while considering
novelty in recommendation scenarios of different natures by
the fine-tuning of 𝑡.
5.3.2 Efficient training. Figure 3 shows the training times of rep-
resentative baselines on the large-scale dataset ML-20M . It can
be observed that the latest GCN model SimGCL, the non-linear
autoencoder RecVAE, and the MF-based metric learning model
DPCML take a substantial amount of time in training. The reason
for this is that they require multiple iterations of optimization in
mini-batch. For comparison, signal-based models such as EASE and
GFCF achieve up to tens of times higher efficiency. At the same
time, our proposed CoRML is based on an extension of signal-based
models and eventually formulates an optimization problem with a
similar form. Consequently, it also achieves high efficiency in the
training process, with training times in the same order of magnitude
as the most efficient baselines. Compared to existing CML models,
CoRML retains the effective part of a distance metric through resid-
ual learning, which retains the advantages of metric learning and
brings significant efficiency gains.
0.2 0.4 0.6 0.8
0.130.140.15MRR@10
0.070.08
NDCG@10
Pinterest
MRR@10
NDCG@10
0.2 0.4 0.6 0.8
0.190.230.27MRR@10
0.090.120.15
NDCG@10
Gowalla
MRR@10
NDCG@10
0.2 0.4 0.6 0.8
0.140.150.16MRR@10
0.060.07
NDCG@10
Yelp2018
MRR@10
NDCG@10
0.2 0.4 0.6 0.8
0.450.50MRR@10
0.250.30
NDCG@10
ML-20M
MRR@10
NDCG@10Figure 4: Effect of weighting factor 𝜆on CoRML.
5.4 Detailed Study of CoRML
5.4.1 How do weights of preference score affect the performance?
As shown in Eq. (26), the preference score in CoRML is formulated
based on HandG, which are balanced by hyperparameter 𝜆. To
investigate the effect of weighting residuals on the model perfor-
mance, we conduct experiments on all test datasets. Figure 4 shows
the variation of performance when 𝜆is tuned between 0.05-0.95. It
can be observed that when 𝜆<0.5, performance is consistently poor
across all datasets. Increasing 𝜆results in optimum performance
when 0.6≤𝜆≤0.8, followed varying degrees of drop. ML-20M ,
the dataset with the highest density, exhibits the slightest drop.
This possibly provides an evidence for the inference that the graph
filtering residual focused on the low-rank approximation of weight
matrix is more important on sparse dataset, where users and items
have fewer interactions.
5.4.2 Effect of approximated ranking weights. In CoRML, approxi-
mated ranking weights ˜𝛼and ˜𝛽formulated as Eq. (23)are designed
to facilitate training without ranking all items. The weights ˜𝛼and
˜𝛽are scaled by factor 𝜙𝑢to act on the optimization of positive and
negative user-item pairs, respectively. The factor 𝜙𝑢is controlled
by𝜖as global scaling hyperparameter and 𝑡𝑢as user-degree scaling
factor, whose effects are tested and shown in Figure 5. We can make
the following observations: 1) The role of 𝑡𝑢is to enlarge preference
scores in varying user degrees. Therefore, increasing 𝑡𝑢in general
makes the value of 𝜖smaller when the performance is optimal. 2)
Varying𝑡𝑢has a small effect on the optimal performance of CoRML.
Setting𝑡𝑢=0.5can result in excellent performance on all datasets.
3) Global scaling factor 𝜖shows more significant impact on the
model performance. On the logarithmic scale, extremely large or
extremely small 𝜖may deteriorate the model performance. This
can be justified through the definition of the approximated ranking
weights. As shown in Eq. (23), a very small 𝜖makes all positive user-
item pairs have nearly the same weight -1, while a very large 𝜖can
reverse the sign of ˜𝛼, deviating the learning objective. In general,
keeping𝜖in the range of[0.1,1]can lead to good recommendation
performance.Collaborative Residual Metric Learning SIGIR ’23, July 23–27, 2023, Taipei, Taiwan
103
102
101
1001010.130.140.15MRR@10
Pinterest
103
102
101
100101
0.070.08NDCG@10
103
102
101
1001010.220.230.24
Gowalla
103
102
101
100101
0.120.130.14
103
102
101
1001010.150.16
Yelp2018
103
102
101
100101
0.060.07
103
102
101
1001010.40.5
ML-20M
103
102
101
100101
0.200.250.30
tu=0.0tu=0.5tu=1.0
Figure 5: Effect of the approximated ranking weights 𝜙on CoRML.
6 RELATED WORKS
6.1 Collaborative Filtering
Collaborative Filtering (CF) [ 1] has become a popular research
topic in the Internet era. Since CF can be considered as a task to
complete entries in the user-item interaction matrix, Matrix Fac-
torization (MF) [ 12], as a strategy for matrix completion, naturally
becomes the foundation of the mainstream approach in CF. MF
assumes that the user-item interaction matrix is low-rank and can
be recovered by learning the embedding vectors of users and items.
Most MF models generate predicted entries by the dot product of
user and item embedding vectors, while they can be optimized by
minimizing the error of individual entries [ 12] or maximizing the
difference between positive and negative samples [ 19]. They are
both widely adopted in the subsequent proposed methods which
introduce refined structures like neural networks [ 10,33]. These
approaches enable a light design by focusing on the modeling of sin-
gle user-item entry, while neglecting the synergy between different
interactions. In recent years, this type of global relationships has
gradually received more attention and has been incorporated in CF
models in the form of interaction graph [ 36]. With the emerging of
Graph Convolutional Networks (GCN), GCN models [ 9,26] quickly
become the state-of-the-art in MF-based models and are contin-
uously improved to achieve advances in efficiency and accuracy
[16, 34].
Unlike MF-based models, another class of method implements CF
by treating the user’s historical interactions as features, and model-
ing the relationships between items [ 6,28]. A classical approach
is the linear autoencoder [ 17,23,25], which models an item-item
relationship matrix to encode user features. This idea is then ex-
tended by subsequent studies and applied to nonlinear denoising
autoencoders [ 31] and variational autoencoders [ 15,21]. A recent
work [ 20] considers CF in terms of graph signal processing and
proposes a framework for signal-based models, which can incor-
porate the linear autoencoders and the ideal case of MF and GCN
models. They also propose a simple but effective graph filtering
model GFCF to model item relationships.
6.2 Metric Learning
Metric learning [ 7,32] learns a distance metric to fit the distance
and similarity between training data: separating dissimilar sam-
ples and pushing similar samples closer. Over the decades, metriclearning has gained attention and adoption in many fields, such
as Computer Vision [ 29] and Nature Language Processing [ 13]. In
CF, the recent progress related to metric learning has been mainly
influenced by work [ 11]. In [ 11], based on the idea of MF, the au-
thors propose a metric learning framework CML to estimate the
user-item relationship by the distance of embedding vectors in Eu-
clidean space. CML is then adopted and improved in subsequent
studies by incorporating translation vectors [ 18], adopting adaptive
margin [14], and promoting diversity [2].
As discussed, existing studies of metric learning on CF have
been conducted over the distance metric on Euclidean space. In a
recent survey of metric learning [ 8], the authors formulate a typical
metric learning problem as the learning of generalized Mahalanobis
distance, and show that the Euclidean distance is a special case of
generalized Mahalanobis distance. On the other hand, CML and its
subsequent studies are based on MF and do not involve signal-based
models. This provides us with motivation and becomes the major
difference between our work and existing works.
7 CONCLUSION
In this paper, we delve into the signal-based model, unveil its con-
nection to the distance metric, and finally propose a novel CoRML
model. In particular, we identify the preference scores in signal-
based models are strongly tied to the residuals of distance between
user and different items. We also found that the normalization
strengths of user interaction signals have an explicit effect on the
novelty of recommendation, which is neglected by existing works.
By leveraging connections between preference scores and distance
residuals, CoRML is able to capture fine-grained user preferences
with full advantages of metric learning. Moreover, it yields high
training efficiency through introducing a novel approximated rank-
ing weight. A comprehensive comparison with existing CF models
shows advantages of CoRML in terms of performance, efficiency
and novelty, validating the role of metric learning in signal-based
CF models.
ACKNOWLEDGMENTS
This work was partially supported by the Research Grants Council
of the Hong Kong Special Administrative Region, China (Project
No. CityU 11216620), and the National Natural Science Foundation
of China (Project No. 62202122).SIGIR ’23, July 23–27, 2023, Taipei, Taiwan Wei and Chow, et al.
REFERENCES
[1]G. Adomavicius and A. Tuzhilin. 2005. Toward the next generation of rec-
ommender systems: a survey of the state-of-the-art and possible extensions.
IEEE Transactions on Knowledge and Data Engineering 17, 6 (2005), 734–749.
https://doi.org/10.1109/TKDE.2005.99
[2]Shilong Bao, Qianqian Xu, Zhiyong Yang, Yuan He, Xiaochun Cao, and Qingming
Huang. 2022. The Minority Matters: A Diversity-Promoting Collaborative Metric
Learning Algorithm. In Advances in Neural Information Processing Systems (NIPS
’22). https://openreview.net/forum?id=xubxAVbOsw
[3]R. H. Bartels and G. W. Stewart. 1972. Solution of the Matrix Equation AX + XB =
C [F4]. Commun. ACM 15, 9 (sep 1972), 820–826. https://doi.org/10.1145/361573.
361582
[4]Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein. 2011.
Distributed Optimization and Statistical Learning via the Alternating Direction
Method of Multipliers. Foundations and Trends ®in Machine Learning 3, 1 (2011),
1–122. https://doi.org/10.1561/2200000016
[5]Chao Chen, Dongsheng Li, Junchi Yan, Hanchi Huang, and Xiaokang Yang. 2021.
Scalable and Explainable 1-Bit Matrix Completion via Graph Signal Learning. In
Proceedings of the AAAI Conference on Artificial Intelligence (AAAI ’21, Vol. 35) .
7011–7019. https://doi.org/10.1609/aaai.v35i8.16863
[6]Mukund Deshpande and George Karypis. 2004. Item-Based Top-N Recommen-
dation Algorithms. ACM Trans. Inf. Syst. 22, 1 (Jan. 2004), 143–177. https:
//doi.org/10.1145/963770.963776
[7]Ali Ghodsi, Dana Wilkinson, and Finnegan Southey. 2007. Improving Embeddings
by Flexible Exploitation of Side Information. In Proceedings of the 20th Interna-
tional Joint Conference on Artifical Intelligence (Hyderabad, India) (IJCAI’07) .
810–816.
[8]Benyamin Ghojogh, Ali Ghodsi, Fakhri Karray, and Mark Crowley. 2022. Spectral,
Probabilistic, and Deep Metric Learning: Tutorial and Survey. arXiv e-prints
(2022). https://doi.org/10.48550/arXiv.2201.09267
[9]Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, YongDong Zhang, and Meng
Wang. 2020. LightGCN: Simplifying and Powering Graph Convolution Network
for Recommendation. In Proceedings of the 43rd International ACM SIGIR Confer-
ence on Research and Development in Information Retrieval (SIGIR ’20) . 639–648.
https://doi.org/10.1145/3397271.3401063
[10] Xiangnan He, Xiaoyu Du, Xiang Wang, Feng Tian, Jinhui Tang, and Tat-Seng
Chua. 2018. Outer Product-based Neural Collaborative Filtering. In Proceedings
of the 27th International Joint Conference on Artificial Intelligence, IJCAI-18 (IJCAI
’18). 2227–2233. https://doi.org/10.24963/ijcai.2018/308
[11] Cheng-Kang Hsieh, Longqi Yang, Yin Cui, Tsung-Yi Lin, Serge Belongie, and
Deborah Estrin. 2017. Collaborative Metric Learning. In Proceedings of the 26th In-
ternational Conference on World Wide Web (Perth, Australia) (WWW ’17) . 193–201.
https://doi.org/10.1145/3038912.3052639
[12] Yehuda Koren, Robert Bell, and Chris Volinsky. 2009. Matrix Factorization
Techniques for Recommender Systems. Computer 42, 8 (2009), 30–37. https:
//doi.org/10.1109/MC.2009.263
[13] G. Lebanon. 2006. Metric learning for text documents. IEEE Transactions on
Pattern Analysis and Machine Intelligence 28, 4 (2006), 497–508. https://doi.org/
10.1109/TPAMI.2006.77
[14] Mingming Li, Shuai Zhang, Fuqing Zhu, Wanhui Qian, Liangjun Zang, Jizhong
Han, and Songlin Hu. 2020. Symmetric Metric Learning with Adaptive Margin for
Recommendation. In Proceedings of the AAAI Conference on Artificial Intelligence
(AAAI ’20, Vol. 34) . 4634–4641. https://doi.org/10.1609/aaai.v34i04.5894
[15] Dawen Liang, Rahul G. Krishnan, Matthew D. Hoffman, and Tony Jebara. 2018.
Variational Autoencoders for Collaborative Filtering. In Proceedings of the 2018
World Wide Web Conference (Lyon, France) (WWW ’18) . 689–698. https://doi.
org/10.1145/3178876.3186150
[16] Kelong Mao, Jieming Zhu, Xi Xiao, Biao Lu, Zhaowei Wang, and Xiuqiang He.
2021. UltraGCN: Ultra Simplification of Graph Convolutional Networks for
Recommendation. In Proceedings of the 30th ACM International Conference on
Information & Knowledge Management (Virtual Event, Queensland, Australia)
(CIKM ’21) . 1253–1262. https://doi.org/10.1145/3459637.3482291
[17] Xia Ning and George Karypis. 2011. SLIM: Sparse Linear Methods for Top-N
Recommender Systems. In 2011 IEEE 11th International Conference on Data Mining
(ICDM ’11) . 497–506. https://doi.org/10.1109/ICDM.2011.134
[18] Chanyoung Park, Donghyun Kim, Xing Xie, and Hwanjo Yu. 2018. Collaborative
Translational Metric Learning. In 2018 IEEE International Conference on Data
Mining (ICDM) (ICDM ’18) . 367–376. https://doi.org/10.1109/ICDM.2018.00052
[19] Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme.
2009. BPR: Bayesian Personalized Ranking from Implicit Feedback. In Proceedings
of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (Montreal,
Quebec, Canada) (UAI ’09) . AUAI Press, Arlington, Virginia, USA, 452–461. https:
//doi.org/10.5555/1795114.1795167
[20] Yifei Shen, Yongji Wu, Yao Zhang, Caihua Shan, Jun Zhang, B. Khaled Letaief,
and Dongsheng Li. 2021. How Powerful is Graph Convolution for Recommen-
dation?. In Proceedings of the 30th ACM International Conference on Information
& Knowledge Management (Virtual Event, Queensland, Australia) (CIKM ’21) .1619–1629. https://doi.org/10.1145/3459637.3482264
[21] Ilya Shenbin, Anton Alekseev, Elena Tutubalina, Valentin Malykh, and Sergey I.
Nikolenko. 2020. RecVAE: A New Variational Autoencoder for Top-N Recom-
mendations with Implicit Feedback. In Proceedings of the 13th International Con-
ference on Web Search and Data Mining (Houston, TX, USA) (WSDM ’20) . 528–536.
https://doi.org/10.1145/3336191.3371831
[22] Vikas Sindhwani, Serhat S. Bucak, Jianying Hu, and Aleksandra Mojsilovic. 2010.
One-Class Matrix Completion with Low-Density Factorizations. In 2010 IEEE
International Conference on Data Mining (ICDM ’10) . 1055–1060. https://doi.org/
10.1109/ICDM.2010.164
[23] Harald Steck. 2019. Embarrassingly Shallow Autoencoders for Sparse Data. In The
World Wide Web Conference (San Francisco, CA, USA) (WWW ’19) . Association
for Computing Machinery, New York, NY, USA, 3251–3257. https://doi.org/10.
1145/3308558.3313710
[24] Harald Steck. 2020. Autoencoders That Don’t Overfit towards the Identity. In
Proceedings of the 34th International Conference on Neural Information Processing
Systems (Vancouver, BC, Canada) (NIPS’20) . Article 1644, 11 pages. https://doi.
org/10.5555/3495724.3497368
[25] Harald Steck, Maria Dimakopoulou, Nickolai Riabov, and Tony Jebara. 2020.
ADMM SLIM: Sparse Recommendations for Many Users. In Proceedings of the
13th International Conference on Web Search and Data Mining (Houston, TX, USA)
(WSDM ’20) . 555–563. https://doi.org/10.1145/3336191.3371774
[26] Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua. 2019.
Neural Graph Collaborative Filtering. In Proceedings of the 42nd International
ACM SIGIR Conference on Research and Development in Information Retrieval
(Paris, France) (SIGIR’19) . 165–174. https://doi.org/10.1145/3331184.3331267
[27] Tianjun Wei, Tommy W.S. Chow, Jianghong Ma, and Mingbo Zhao. 2023.
ExpGCN: Review-aware Graph Convolution Network for explainable recom-
mendation. Neural Networks 157 (2023), 202–215. https://doi.org/10.1016/j.
neunet.2022.10.014
[28] Tianjun Wei, Jianghong Ma, and Tommy W. S. Chow. 2023. Fine-tuning Partition-
aware Item Similarities for Efficient and Scalable Recommendation. In Proceedings
of the ACM Web Conference 2023 (WWW ’23) . Association for Computing Ma-
chinery, 10 pages. https://doi.org/10.1145/3543507.3583240
[29] Kilian Q. Weinberger and Lawrence K. Saul. 2009. Distance Metric Learning
for Large Margin Nearest Neighbor Classification. Journal of Machine Learning
Research 10, 9 (2009), 207–244.
[30] Hao Wu, Qimin Zhou, Rencan Nie, and Jinde Cao. 2020. Effective metric learn-
ing with co-occurrence embedding for collaborative recommendations. Neural
Networks 124 (2020), 308–318. https://doi.org/10.1016/j.neunet.2020.01.021
[31] Yao Wu, Christopher DuBois, Alice X. Zheng, and Martin Ester. 2016. Collabora-
tive Denoising Auto-Encoders for Top-N Recommender Systems. In Proceedings
of the Ninth ACM International Conference on Web Search and Data Mining (San
Francisco, California, USA) (WSDM ’16) . Association for Computing Machinery,
New York, NY, USA, 153–162. https://doi.org/10.1145/2835776.2835837
[32] Eric Xing, Michael Jordan, Stuart J Russell, and Andrew Ng. 2002. Distance Metric
Learning with Application to Clustering with Side-Information. In Advances in
Neural Information Processing Systems (NIPS ’02, Vol. 15) , S. Becker, S. Thrun, and
K. Obermayer (Eds.). MIT Press, 521–528.
[33] Hong-Jian Xue, Xinyu Dai, Jianbing Zhang, Shujian Huang, and Jiajun Chen. 2017.
Deep Matrix Factorization Models for Recommender Systems. In Proceedings of
the 26th International Joint Conference on Artificial Intelligence, IJCAI-17 (IJCAI
’17). 3203–3209. https://doi.org/10.24963/ijcai.2017/447
[34] Junliang Yu, Hongzhi Yin, Xin Xia, Tong Chen, Lizhen Cui, and Quoc Viet Hung
Nguyen. 2022. Are Graph Augmentations Necessary? Simple Graph Contrastive
Learning for Recommendation. In Proceedings of the 45th International ACM
SIGIR Conference on Research and Development in Information Retrieval (SIGIR
’22). https://doi.org/10.1145/3477495.3531937
[35] Minghao Zhao, Le Wu, Yile Liang, Lei Chen, Jian Zhang, Qilin Deng, Kai Wang,
Xudong Shen, Tangjie Lv, and Runze Wu. 2022. Investigating Accuracy-Novelty
Performance for Graph-Based Collaborative Filtering. In Proceedings of the 45th
International ACM SIGIR Conference on Research and Development in Information
Retrieval (Madrid, Spain) (SIGIR ’22) . 50–59. https://doi.org/10.1145/3477495.
3532005
[36] Lei Zheng, Chun-Ta Lu, Fei Jiang, Jiawei Zhang, and Philip S. Yu. 2018. Spectral
Collaborative Filtering. In Proceedings of the 12th ACM Conference on Recom-
mender Systems (Vancouver, British Columbia, Canada) (RecSys ’18) . 311–319.
https://doi.org/10.1145/3240323.3240343
[37] Tao Zhou, Zoltán Kuscsik, Jian-Guo Liu, Matúš Medo, Joseph Rushton Wake-
ling, and Yi-Cheng Zhang. 2010. Solving the apparent diversity-accuracy
dilemma of recommender systems. Proceedings of the National Academy of
Sciences 107, 10 (2010), 4511–4515. https://doi.org/10.1073/pnas.1000488107
arXiv:https://www.pnas.org/doi/pdf/10.1073/pnas.1000488107
[38] Ziwei Zhu, Yun He, Xing Zhao, Yin Zhang, Jianling Wang, and James Caverlee.
2021. Popularity-Opportunity Bias in Collaborative Filtering. In Proceedings of
the 14th ACM International Conference on Web Search and Data Mining (Virtual
Event, Israel) (WSDM ’21) . 85–93. https://doi.org/10.1145/3437963.3441820