THIS WORK HAS BEEN SUBMITTED TO THE IEEE FOR POSSIBLE PUBLICATION 1
Conformal Loss-Controlling Prediction
Di Wang, Ping Wang, Zhong Ji, Senior Member, IEEE , Xiaojun Yang, Hongyue Li
Abstract —Conformal prediction is a learning framework con-
trolling prediction coverage of prediction sets, which can be
built on any learning algorithm for point prediction. This work
proposes a learning framework named conformal loss-controlling
prediction, which extends conformal prediction to the situation
where the value of a loss function needs to be controlled.
Different from existing works about risk-controlling prediction
sets and conformal risk control with the purpose of controlling
the expected values of loss functions, the proposed approach
in this paper focuses on the loss for any test object, which is
an extension of conformal prediction from miscoverage loss to
some general loss. The controlling guarantee is proved under the
assumption of exchangeability of data in finite-sample cases and
the framework is tested empirically for classification with a class-
varying loss and statistical postprocessing of numerical weather
forecasting applications, which are introduced as point-wise
classification and point-wise regression problems. All theoretical
analysis and experimental results confirm the effectiveness of our
loss-controlling approach.
Index Terms —Conformal prediction, Loss-controlling predic-
tion, Finite-sample guarantee, Weather forecasting.
I. I NTRODUCTION
Prediction sets convey uncertainty or confidence information
for users, which is more preferred than prediction points,
especially for sensitive applications such as medicine, finance
and weather forecasting [1] [2] [3]. One example is construct-
ing prediction intervals with confidence 1−δfor regression
problems, where the statistical guarantee is expected such
that the true labels are covered in probability 1−δ[4].
Nowadays, many researches have been proposed to build
set predictors. Bayesian methods [5] and Gaussian process
[6] are straightforward ways of producing prediction sets
based on posterior distributions. However, their prediction
sets can be misleading if the prior assumptions are not
correct, which is often the case since the prior is usually
unknown in applications [7] [8]. Other statistical methods
such as bootstrap-based methods [9] and quantile regression
[10] are also able to output prediction sets for test labels,
but their coverage guarantees can only be obtained in the
asymptotic setting, and the prediction sets may fail to cover the
This work has been submitted to the IEEE for possible publication.
Copyright may be transferred without notice, after which this version may
no longer be accessible.
This work was supported by the National Natural Science Foundation of
China under Grant 62106169. (Corresponding author: Hongyue Li)
Di Wang and Zhong Ji are with the School of Electrical and Information
Engineering, Tianjin University, Tianjin 300072, China, and also with the
Tianjin Key Laboratory of Brain-inspired Intelligence Technology, School of
Electrical and Information Engineering, Tianjin University, Tianjin 300072,
China. (email: wangdi2015@tju.edu.cn; jizhong@tju.edu.cn;).
Ping Wang and Hongyue Li are with the School of Electrical and In-
formation Engineering, Tianjin University, Tianjin 300072, China. (email:
wangps@tju.edu.cn; lihongyue@tju.edu.cn).
Xiaojun Yang is with the Tianjin Meteorological Observatory, Tianjin
300074, China. (email: boluo0127@yeah.net)labels frequently in finite-sample cases. Different from these
works, conformal prediction (CP), a promising non-parametric
learning framework aiming to provide reliable prediction sets,
can provide the finite-sample coverage guarantee only under
the assumption of exchangeability of data samples [11]. This
property of validity has been proved both theoretically and
empirically in many works and applied to many areas [12]
[13]. Besides, many researches extend CP to more general
cases, such as conformal prediction for multi-label learning
[14] [15], functional data [16] [17], few-shot learning [18],
distribution shift [19] [20] and time series [21] [22].
However, the researches about set predictors mentioned
above mainly make promise about the coverage of prediction
sets, i.e., they only control the miscoverage loss of set pre-
dictors, which can not be applied to other broad applications
concerning controlling general losses. For example, consider
classifying MRI images into several diagnostic categories [23],
where different categories cause different consequence. In this
setting, the loss of the true label ybeing not included in the
prediction set should be dependent on y, which is the problem
of classification with a class-varying loss. Another example is
tumor segmentation [24]. Instead of making prediction sets to
overly cover the pixels of tumor, one may care more about
controlling other losses such as false negative rate. Other
practical settings include controlling the l1projective distance
for protein structure prediction, controlling a hierarchical
distance for hierarchical classification and controlling F1-
score for open-domain question answering [23] [24]. In these
applications, the prediction sets with the coverage guarantee
are not useful, as they are not constructed with controlling
these general losses in mind.
To tackle this issue, two works for extending the finite-
sample coverage guarantee of CP have been proposed recently.
One is the work of conformal prediction sets with limited false
positives (CPS-LFP) [25]. It employs DeepSets [26] to esti-
mate the expected value or the cumulative distribution function
of the number of false positives, and then uses calibration data
to control the number of false positives of prediction sets.
Conformal risk control (CRC) [24] extends CP to prediction
tasks of controlling the expected value of a general loss
based on finding the optimal parameter for nested prediction
sets. The spirit is to employ calibration data to obtain the
information of the upper bound of the expected value of the
loss function at hand and control the expected value for the test
object, whose main idea was originally proposed from their
pioneer work named risk-controlling prediction sets (RCPS)
[23]. CRC and RCPS aim to control the expected value instead
of the value of a general loss for set predictors. By contrast,
CPS-LFP can control the value of the loss related to false
positives, but it is not general enough.
In some applications, controlling the value of a general lossarXiv:2301.02424v2  [cs.LG]  23 Jan 2024THIS WORK HAS BEEN SUBMITTED TO THE IEEE FOR POSSIBLE PUBLICATION 2
can be more preferred than controlling the expected value,
since one may only care about the loss value for a specific
test object, just like the coverage guarantee made by CP and
the(k, δ)-FP validity acheived by CPS-LFP. Therefore, this
paper extends CP to the situation where the value of a general
loss needs to be controlled, which has been not considered in
the literature to our best knowledge. Our approach is similar to
CRC with the main difference being that we focus on finding
the optimal parameter for nested prediction sets to control the
loss. Therefore, we also concentrate on inductive conformal
prediction [27] or split conformal prediction [28] process like
CRC.
Recall that inductive conformal prediction makes the cov-
erage guarantee as follows,
P 
Yn+1∈C(n)
1−δ(Xn+1)!
≥1−δ,
where δis the significance level preset by users, C(n)
1−δ
is the set predictor made by CP based on ncalibration
data{(Xi, Yi)}n
i=1,(Xn+1, Yn+1)is the test feature-response
pair, and the randomness is from both {(Xi, Yi)}n
i=1and
(Xn+1, Yn+1). By comparison, conformal loss-controlling
prediction (CLCP), the learning framework proposed in this
paper, provides the controlling guarantee as follows,
P 
L
Yn+1, Cλ∗(Xn+1)
≤α!
≥1−δ,
where Lis a loss function satisfying some monotonic con-
ditions as in [24], αis the preset level of loss, Cλis a set
predictor usually constructed by an underlying algorithm and
a parameter λ. The optimal λ∗is obtained based on α,δand
calibration data. The controlling guarantee needs two levels
αandδto be chosen by users, which is similar with that
in [23], i.e., CLCP guarantees that the prediction loss is not
greater than αwith high probability 1−δwhen δis small such
as0.1. IfLis defined based on false positives for multi-label
classification, the controlling guarantee above can be seen as
the (α, δ)-FP validity defined in Definition 4.2 in [25].
We prove the controlling guarantee for distribution-free and
finite-sample settings with the assumption of exchangeability
of data samples. The main idea is that we find the λ∗to
make the 1−δquantile of the loss values on calibration
data not greater than α, which is inspired by CRC focusing
on making the mean of the loss values not greater than α.
Since the property of the set predictors and loss functions
used in CLCP is the same as that used in CRC, CLCP can
also be applied to many applications concerning controlling
general losses. These applications include not only the areas
about classification and image segmentation, but also the field
of graph signal processing [29] [30], for example, protein
structure prediction.
The proposed CLCP is a novel learning framework com-
pared to existing researches. Different from those aiming to
control the value of the miscoverage loss, CLCP is a more
general approach for the purpose of controlling the value
of a general loss. Besides, CLCP can be widely used for
many situations whereas CPS-LFP is specifically designed forcontrolling the loss related to false negatives. Also, CLCP
differs from CRC and RCPS as their purpose is to control the
expected value instead. Therefore, in the experimental section,
we concentrate on designing the experiments to verify the
theoretical conclusion for different applications, as the idea
of controlling general losses for set predictors is original. To
be specific, we test our proposed CLCP in classification with
a class-varying loss introduced in [23], and postprocessing of
numerical weather forecasts, which we consider as point-wise
classification and point-wise regression problems. The exper-
imental results empirically confirm the theoretical guarantee
we prove in this paper.
In summary, the main contributions of this paper are:
•A learning framework named conformal loss-controlling
prediction (CLCP) is proposed for controlling the pre-
diction loss for the test object. The approach is simple
to implement and can be built on any machine learning
algorithm for point prediction.
•The controlling guarantee is proved mathematically for
finite-sample cases with the exchangeability assumption,
without any further assumption for data distribution.
•The controlling guarantee is empirically verified by clas-
sification with a class-varying loss and weather forecast-
ing problems, which confirms the effectiveness of CLCP.
The rest of this paper is organized as follows. Section II
reviews inductive conformal prediction and conformal risk
control. Section III introduces conformal loss-controlling pre-
diction and its theoretical guarantee. Section IV conducts
experiments to test the proposed method and the conclusions
are drawn in Section V .
II. I NDUCTIVE CONFORMAL PREDICTION AND
CONFORMAL RISKCONTROL
This section reviews inductive conformal prediction and
conformal risk control. Throughout this paper, {(Xi, Yi)}n+1
i=1
denotes n+ 1 data drawn exchangeably from PXY on
X × Y , where {(Xi, Yi)}n
i=1is the calibration dataset and
(Xn+1, Yn+1)is the test object-response pair. We use lower-
case letter (xi, yi)to represent the realization of (Xi, Yi).
The set-valued function and loss function considered in
this paper are the same as those in [24] and [23], which we
formally introduce as follows. Let Cλ:X → Y′be a set-
valued function with a parameter λ∈ R, where Y′represents
some space of sets and Ris the set of real numbers. Taking
single-label classification for example, Y′can be the power
set ofY. For binary image segmentation, Y′can be equal to
Yas the space of all possible results of image segmentation,
where the sets here stand for all of the pixels of positive class
for the image.
We also introduce the nesting property for prediction sets
and losses as in [23] as follows. For each realization of input
object x, we assume that Cλ(x)satisfies the following nesting
property:
λ1< λ 2=⇒Cλ1(x)⊆Cλ2(x). (1)THIS WORK HAS BEEN SUBMITTED TO THE IEEE FOR POSSIBLE PUBLICATION 3
Furthermore, with S1andS2being two subsets of Y, we
assume that L:Y ×Y′→ R is a loss function respecting the
following nesting property for each realization of response y:
S1⊆S2⊆ Y′=⇒L(y, S 2)≤L(y, S 1)≤B, (2)
where Bis the upper bound of the loss function.
A. Inductive Conformal Prediction
Inductive conformal prediction (ICP) is a computationally
efficient version of the original conformal prediction approach.
It starts with any measurable function named nonconformity
measure A:X ×Y → R and obtains nnonconformity scores
as
Ai=A(Xi, Yi),
fori= 1,···, n. Then, with the exchangeable assumption and
a preset δ∈(0,1), one can conclude that
P 
A(Xn+1, Yn+1)≤Q(n)
1−δ!
≥1−δ,
where Q(n)
1−δis the 1−δquantile of {Ai}n
i=1∪ {∞} [19].
Therefore, the prediction set made by ICP is
C(n)
1−δ(Xn+1) ={y:A(Xn+1, y)≤Q(n)
1−δ},
which satisfies
P 
Yn+1∈C(n)
1−δ(Xn+1)!
≥1−δ.
The nonconformity measure Ais often defined based on
a point prediction model ˆflearned from some other training
samples, each of which is also drawn from PXY.
Here is an example of constructing prediction sets with CP.
For a classification problem with Kclasses, one can first train
a classifier ˆf:X → [0,1]Kwith the ith output being the
estimation of the probability of the ith class, and calculate the
nonconformity scores as
A(x, y) = 1−ˆfk(x),
where ˆfkis the kth output of ˆf(x), ifystands for the kth
class. Therefore, the corresponding prediction set for an input
object xis
C(n)
1−δ(x) ={k:ˆfk(x)≥1−Q(n)
1−δ},
which indicates that k∈C(n)
1−δ(x)if the estimated probability
ofkth class is not less than 1−Q(n)
1−δ.
B. Conformal Risk Control
Different from conformal prediction, CRC starts with a set-
valued function with the nesting property, whose approach is
inspired by nested conformal prediction [31] and was first
proposed in the researches about risk-controlling prediction
sets.
Assume one has a way of constructing a set-valued function
Cλwith the nesting property of formula (1). Given a lossfunction Lwith the nesting property of formula (2), the
purpose of CRC is to find λ∗such that
Eh
L(Yn+1, Cλ∗(Xn+1))i
≤α, (3)
i.e., the expected loss or the risk is not greater than α.
To do so, CRC first calculates Li(λ)as
Li(λ) =L(Yi, Cλ(Xi)), (4)
with the fact that Li(λ)is a monotone decreasing function of
λbased on the nesting properties. Then , CRC searches for λ∗
using the following equation,
λ∗= inf(
λ:n
n+ 1ˆRn(λ) +B
n+ 1≤α)
,
where ˆRn(λ) = (L1(λ) +···+Ln(λ))/nis an estimation of
the risk on calibration data and Bis introduced to make the
estimation not overconfident.
These two steps of CRC are too simple that one may
surprise about its theoretical conclusion that with the assump-
tion of exchangeability of data samples, the prediction set
Cλ∗(Xn+1)obtained by CRC satisfies formula (3), which has
been also proved empirically in [24]. CRC extends CP from
controlling the expected value of miscoverage loss to some
general loss, which can be applied to the cases where Yis
beyond real numbers or vectors, such as images, fields and
even graphs.
After tackling the theoretical issue, the problem for CRC
is how to construct Cλ. Here, we also give an example of a
classification problem with Kclasses. In fact, with the same
notations of the example in Section II-A, CRC can construct
the prediction set as
Cλ(x) ={k:ˆfk(x)≥1−λ}.
Therefore, as long as Lsatisfies formula (2), such as Lis the
indicator of miscoverage, CRC guarantees to control the risk
as formula (3).
III. C ONFORMAL LOSS-CONTROLLING PREDICTION AND
ITSTHEORETICAL ANALYSIS
This section introduces the approach of CLCP and its
theoretical analysis. CLCP also has two steps like CRC, and
the main difference between them is that CLCP focuses on
whether the estimation of the 1−δquantile of the losses is not
greater than αwhile CRC concentrates on whether the mean
of the losses not greater than α. The controlling of the 1−δ
quantile of the losses makes CLCP able to control the value of
a general loss by employing the probability inequation derived
from the exchangeability assumption, which is also employed
by ICP if the loss is seen as the nonconformity score.
Suppose one has a way of constructing a set-valued function
Cλwith the nesting property of formula (1), which can be the
same as that used in CRC. Here, we assume that the parameter
λis selected from a discrete set Λ, such as from 0to1with a
step size 0.01, which avoids us from the assumption of right
continuous for the loss function in theoretical analysis, and is
also reasonable since we actually search for λ∗with some stepTHIS WORK HAS BEEN SUBMITTED TO THE IEEE FOR POSSIBLE PUBLICATION 4
size in practice [24] [23]. Besides, the latest paper about risk-
controlling prediction also makes this discrete assumption for
general cases [32]. After determining CλandΛ, CLCP first
calculates Li(λ)on calibration data as formula (4). Then , for
any preset α∈ R andδ∈(0,1), CLCP searches for λ∗such
that
λ∗= min(
λ∈Λ :Q(n)
1−δ(λ)≤α)
, (5)
withQ(n)
1−δ(λ)being the 1−δquantile of {Li(λ)}n
i=1∪ {B}.
The approach of CLCP is summarised in Algorithm 1, which
is easy to implement.
Algorithm 1 Conformal Loss-Controlling Prediction
Input:
Calibration dataset {(xi, yi)}n
i=1, test input object xn+1,
the set predictor Cλsatisfying formula (1), the loss
function Lsatisfying formula (2), preset α∈ R and
δ∈(0,1).
Output:
Predictive set for yn+1.
1:Based on formula (4), calculate {Li(λ)}n
i=1.
2:Search for λ∗satisfying formula (5).
3:return Cλ∗(xn+1)
Next, we introduce the definition of (α, δ)-loss-controlling
set predictors and then prove our theoretical conclusion about
CLCP.
Definition 1. Given a loss function L:Y × Y′→ R and a
random sample (X, Y)∈ X ×Y , a random set-valued function
Cwhose realization is in the space of functions X → Y′is a
(α, δ)-loss-controlling set predictor if it satisfies that
P 
L
Y, C(X)
≤α!
≥1−δ,
where the randomness is both from Cand(X, Y).
After all these preparations, we can prove in Theorem 1
thatCλ∗constructed by CLCP is a (α, δ)-loss-controlling set
predictor.
Theorem 1. Suppose {(Xi, Yi)}n+1
i=1aren+ 1 data drawn
exchangeably from PXYonX × Y ,Cλ:X → Y′is a set-
valued function satisfying formula (1) with the parameter λ
taking values from a discrete set Λ⊂ R ,L:Y × Y′→ R
is a loss function satisfying formula (2) and Li(λ)is defined
as formula (4). For any preset α∈ R, ifLalso satisfies the
following condition,
min
λmax
iLi(λ)≤α, (6)
then for any δ∈(1
n+1,1), we have
P 
L
Yn+1, Cλ∗(Xn+1)
≤α!
≥1−δ, (7)
where λ∗is defined as formula (5).Proof. LetQ(n+1)
1−δ(λ)be the 1−δquantile of {Li(λ)}n+1
i=1,
and define ˜λas
˜λ= min(
λ∈Λ :Q(n+1)
1−δ(λ)≤α)
.
Similarly, let Q(n)
1−δ(λ)be the 1−δquantile of {Li(λ)}n
i=1∪
{B}, and we have
λ∗= min(
λ∈Λ :Q(n)
1−δ(λ)≤α)
.
Asδ∈(1
n+1,1)and formula (6) holds, ˜λandλ∗are well
defined. Since Bis the upper bound of Ln+1(λ), by definition,
we have
˜λ≤λ∗,
which leads to
Ln+1(λ∗)≤Ln+1(˜λ), (8)
asCλandLsatisfy the nesting properties of formula (1) and
(2).
Since ˜λis dependent on the whole dataset {(Xi, Yi)}n+1
i=1,
{Li(˜λ)}n+1
i=1are exchangeable variables, which leads to
P 
Ln+1(˜λ)≤Q(n+1)
1−δ(˜λ)!
≥1−δ, (9)
asQ(n+1)
1−δ(˜λ)is just the corresponding 1−δquantile (See the
proof of Lemma 1 in [19]).
Combining the definition of ˜λ, formula (8) and (9), we have
P 
Ln+1(λ∗)≤α!
≥1−δ,
which completes the proof.
At the end of this section, we show that CP can be seen
as a special case of CLCP from the following viewpoint.
Suppose Cλis constructed by a nonconformity score A, which
is defined as
Cλ(x) ={y:A(x, y)≤λ},
andLis the miscoverage loss such that
Li(λ) =L(yi, Cλ(xi)) = I{yi/∈Cλ(xi)},
where Iis the indicator function. In this case, Q(n)
1−δ(λ)can
only be 0or1as the loss can only be these two numbers.
Besides, only α∈[0,1)is meaningful, which means that
one wants to control the miscoverage. For CLCP, let Λbe
an arithmetic sequence whose common difference, minimum
and maximum are ∆,λmin andλmax respectively and set
α= 0. By definition, λ∗can be written as
λ∗= min(
λ∈Λ :1
n+ 1nX
i=1I{ai≤λ} ≥1−δ)
= min(
λ∈Λ :1
nnX
i=1I{ai≤λ} ≥⌈(1−δ)(n+ 1)⌉
n)
,THIS WORK HAS BEEN SUBMITTED TO THE IEEE FOR POSSIBLE PUBLICATION 5
where ai=A(xi, yi)is the nonconformity score of the ith
calibration data for CP. In comparison, referring to [24], the
optimal ˆλfor CP is
ˆλ= inf(
λ∈ R:1
nnX
i=1I{ai≤λ} ≥⌈(1−δ)(n+ 1)⌉
n)
.
Therefore, if λmin< ai< λmax for each i, we have
|λ∗−ˆλ| ≤∆,
which implies that the prediction sets of CP and CLCP are
nearly the same if ∆is small enough. In summary, if Cλ
andLhave special forms and Λincludes the upper and lower
bounds of nonconformity scores with ∆being small enough
to be ignored, CP can be seen as a special case of CLCP.
IV. E XPERIMENTS
This section conducts the experiments to empirically test the
approach of CLCP. First, we build CLCP for the classification
problem with a class-varying loss introduced in [23]. Then,
we focus on two types of weather forecasting applications,
which can be seen as point-wise classification and point-
wise regression problems respectively. All experiments were
coded in Python [33]. The statistical learning methods used in
Section IV-A were implemented using Scikit-learn [34] and
the deep learning methods used in Section IV-B and Section
IV-C were implemented with Pytorch [35].
A. CLCP for classification with a class-varying loss
We collected 20binary or multiclass classification datasets
from UCI repositories [36] whose information is summarized
in Table I. The problem is to make the prediction sets of labels
controlling the following loss
L(y, C) =LyI{y /∈C},
where Lyis the loss for ybeing not in the prediction set
C. The loss for each label is generated uniformly on (0,1)
like [23]. Support vector machine (SVM) [37], neural network
(NN) [38] and random forests (RF) [39] were employed as the
underlying algorithms separately to construct prediction sets
based on CLCP. The prediction set Cλis constructed as
Cλ(x) ={k:ˆfk(x)≥1−λ},
where ˆfkis the estimated probability of the observation being
kth class by the corresponding underlying algorithm. For each
dataset, we used 20% of the data for testing and 80% and20%
of the remaining data for training and calibration respectively.
Based on the training data, we selected the meta-parameters
with three-fold cross-validation and used the optimal meta-
parameters to train the classifiers. The regularization parameter
of SVM was selected from {0.001,0.01,0.1,1,10,100}, and
the learning rate and the epochs of NN were selected from
{0.001,0.0001}and{200,500,1000}. The number of trees
of RF were selected from {100,300,500}and the partition
criterion was either gini or entropy. After training, we used the
trained classifiers and the calibration data to search for λ∗with
Algorithm 1 and construct the final set predictors. All of theTABLE I
DATASETS FROM UCI R EPOSITORIES
Dataset Examples Dimensionality Classes
bc-wisc-diag 569 30 2
car 1728 6 4
chess-kr-kp 3196 36 2
contrac 1473 9 3
credit-a 690 15 2
credit-g 1000 20 2
ctg-10classes 2126 21 10
ctg-3classes 2126 21 3
haberman 306 3 2
optical 5620 62 10
phishing-web 11055 30 2
st-image 2310 18 7
st-landsat 6435 36 6
tic-tac-toe 958 9 2
wall-following 5456 24 4
waveform 5000 21 3
waveform-noise 5000 40 3
wilt 4839 5 2
wine-quality-red 1599 11 6
wine-quality-white 4898 11 7
features were normalized to [0,1]by min–max normalization
and for each dataset, the experiments were conducted 10times
and the average results were recorded.
The bar plots in Fig. 1 and Fig. 2 show the experimental
results for 20public datasets with δ∈ {0.05,0.1,0.15,0.2}
andα∈ {0.1,0.2}. The results in Fig. 1 concern about the
frequency of the prediction losses being greater than αon test
set, which is the estimated probability of
P 
L
Yn+1, Cλ∗(Xn+1)
> α!
,
and should be near or lower than δempirically due to formula
(7). The bar plots of Fig. 1 demonstrate that the frequency of
the prediction losses being greater than αis near or below δ,
which verifies the conclusion of Theorem 1.
The bar plots of Fig. 2 show the average sizes of prediction
sets for different δ, describing the informational efficiency of
the prediction sets. Changing δcan effectively change the
average size of prediction sets and changing αmay slightly
change average size (such as the results for wine-quality-red).
Although many prediction sets are meaningful with average
sizes being near 1, the prediction sets for the dataset contrac
may be not useful, since no matter how to change δandα,
the average sizes of the prediction sets are all near or above
2, whereas the number of classes of contrac is 3. Thus, how
to construct efficient prediction sets in the learning framework
of CLCP is worth exploring for further researches.
Combining Fig. 1 and Fig. 2, we observe that different
classifiers can perform differently for different datasets, which
indicates that the underlying algorithm affects the performance
and the model selection approach is necessary for CLCP.
B. CLCP for high-impact weather forecasting
The remaining experiments apply CLCP to weather fore-
casting problems. Here we concentrate on postprocessing of
the forecasts made by numerical weather prediction (NWP)
models [40] [41]. NWP models use equations of atmospheric
dynamics and estimations of current weather conditions to doTHIS WORK HAS BEEN SUBMITTED TO THE IEEE FOR POSSIBLE PUBLICATION 6
Fig. 1. Bar plots of the frequencies of the prediction losses being greater than αvs.δ= 0.05,0.1,0.15,0.2on test data for classification with a class-varying
loss. The first row corresponds to α= 0.1and the second row corresponds to α= 0.2. Different columns represent different classifiers. All bars are near or
below the preset δ, which confirms the controlling guarantee of CLCP empirically.
Fig. 2. Bar plots of the average sizes of prediction sets vs. δ= 0.05,0.1,0.15,0.2on test data for classification with a class-varying loss. The first row
corresponds to α= 0.1and the second row corresponds to α= 0.2. Different columns represent different classifiers. The plots demonstrate the information
in prediction sets. In general, large δleads to small average size and different classifiers have different informational efficiency.
weather forecasting, which is the mainstream weather fore-
casting technique nowadays especially for forecasting beyond
12hours. Many errors affect the performance of NWP models,
such as the estimation errors of initial conditions and the
approximation errors of NWP models, leading to the research
topic about postprocessing the forecasts of NWP models. Most
postprocessing methods are built on some learning process,
which takes the forecasts of NWP models as inputs and the
observations of weather elements or events as outputs.
In this paper, we use CLCP to postprocess the ensemble
forecasts with the control forecast and 50perturbed forecastsissued by the NWP model from European Centre for Medium-
Range Weather Forecasts (ECMWF) [42], which are obtained
from the THORPEX Interactive Grand Global Ensemble
(TIGGE) dataset [43]. We focus on 2-m maximum temperature
and minimum temperature between the forecast lead times
of12nd hour and 36th hour with the forecasts initialized at
0000 UTC. The forecast fields are grided with the resolution
of0.5◦×0.5◦and the corresponding label fields with the
same resolution are extracted from the ERA5 reanalysis data
[44]. The area ranges from 109◦E to122◦E in longitude and
from 29◦N to 42◦N in latitude, covering the main parts ofTHIS WORK HAS BEEN SUBMITTED TO THE IEEE FOR POSSIBLE PUBLICATION 7
Fig. 3. Bar plots of the frequencies of the prediction losses being greater than αvs.δ= 0.05,0.1,0.15,0.2on test data for high-impact weather forecasting.
The first row corresponds to HighTemp and the second row corresponds to LowTemp. Different columns represent different α. All bars are near or below
the preset δ, which confirms the controlling guarantee of CLCP empirically.
Fig. 4. Boxen plots of the prediction losses vs. δ= 0.05,0.1,0.15,0.2on test data for high-impact weather forecasting. The first row corresponds to
HighTemp and the second row corresponds to LowTemp. Different columns represent different α. The loss distributions are controlled by αandδproperly
to obtain the empirical validity in Fig. 3.
North China, East China and Central China, whose grid size
is27×27. The ECMWF forecast data and ERA5 reanalysis
data are collected from 2007 to2020 (14years).
We first consider high-impact weather forecasting, which
is to forecast whether a high-impact weather exists for each
grid and can be seen as a point-wise classification problem or
image segmentation problem for computer vision. The high-
impact weather we consider is whether the 2-m maximum
temperature is above 35◦Cor the 2-m minimum temperature
is below −15◦Cfor each grid. These two cases are treated
as high temperature weather or low temperature weather in
China, which make meteorological observatories issue high
temperature warning or low temperature warning respectively.
The prediction sets and the loss function used for high-
impact weather forecasting are the same as those for image
segmentation in [24]. Taking the ensemble forecast fields of
the NWP model as input x, the corresponding label yis a set
of grids having high-impact weather, which can be seen asa segmentation problem for high-impact weather. Therefore,
we first train a segmentation neural network f(x), where
f(p,q)(x)is the estimated probability of the grid (p, q)having
high-impact weather. Then the set-valued function Cλcan be
constructed as
Cλ(x) ={(p, q) :f(p,q)(x)≥1−λ}, (10)
and the loss function is
L(y, C) = 1−|y∩C|
|y|, (11)
which measures the ratio of the prediction sets failing to do the
warning. We use CLCP with the prediction set and the loss
function above to do high temperature and low temperature
forecasting respectively.
1) Dataset for high temperature forecasting: The reanalysis
fields of 2-m maximum temperature were collected from
ERA5 and the label fields were calculated based on whether
the2-m maximum temperature is above 35◦C. To make theTHIS WORK HAS BEEN SUBMITTED TO THE IEEE FOR POSSIBLE PUBLICATION 8
Fig. 5. Boxen plots for the distributions of normalized sizes of prediction sets vs. δ= 0.05,0.1,0.15,0.2on test data for high-impact weather forecasting.
The first row corresponds to HighTemp and the second row corresponds to LowTemp. Different columns represent different α. U-Net performs better than
nDNN, which indicates the importance of careful design of the underlying algorithm.
loss function take finite values, we only collected the data
whose label fields have at least one high temperature grid
to do this empirical study, which resulted in 1200 samples
in total, i.e., 1200 ensemble forecasts from the NWP model
of ECMWF and corresponding label fields calculated from
ERA5. We name this dataset as HighTemp.
2) Dataset for low temperature forecasting: The dataset
for testing CLCP for low temperature weather forecasting
was constructed in a similar way. The reanalysis fields of
2-m minimum temperature were collected from ERA5 and
the label fields were calculated based on whether the 2-m
minimum temperature is below −15◦C. We only collected
the data whose label fields have at least one low temperature
grid to do this empirical study, which resulted in 1233 samples
in total. We name this dataset as LowTemp.
For each dataset, the same process was used to conduct the
experiment as Section IV-A , i.e., all forecasts from the NWP
model were normalized to [0,1]by min–max normalization,
and we used 20% of the data for testing and 80% and20%
of the remaining data for training and calibration respectively.
We employed two fully convolutional neural networks [45]
for binary image segmentation as our underlying algorithms.
One was U-Net [46] with the same structure as that in [47],
whose numbers of hidden feature maps were all set to 32. The
other was the naive deep neural network (nDNN) with the
same encoder-decoder structure as the U-Net without skip-
connections, i.e., the U-Net removing skip-connections. We
use these two neural networks to show that the design of the
underlying algorithm is necessary for better performance, as
U-Net fuses multi-scale features and nDNN does not. The data
for training U-Net and nDNN were further partitioned into the
validation part ( 10%) for model selection and proper training
part ( 90%) for updating the parameters. Adam optimization
[48] was used for training. The learning rate was set to 0.0001
and the number of epochs was set to 50. After training 50
epochs, the model with lowest binary cross entropy on vali-
dation data was used for formula (10) to construct prediction
sets, where λis searched from 1to0with step size 0.01. Theexperiments of using CLCP for the loss function as formula
(11) were conducted 10times and the prediction results on
test set are shown in Fig. 3, Fig. 4 and Fig. 5.
Fig. 3 also shows the bar plots of the frequencies of the
prediction losses being greater than αforδ= 0.05,0.1,0.15
and0.2. Four columns stand for the cases where α=
0.05,0.1,0.15and0.2respectively. It can be seen that for
the two datasets HighTemp and LowTemp, all bars are near
or below the preset δ, which verifies formula (7) empirically.
Fig. 4 further shows the distributions of the losses for different
δand different αusing boxen plots, which contain more
information than box plots by drawing narrow boxes for tails.
It can be seen that larger αandδlead to larger losses, which
is reasonable since large αandδrelax the constraint on
prediction losses. We measure the informational efficiency of
the prediction set Cλ∗(x)using its normalized size defined
as|Cλ∗(x)|/PQ , where PandQare the numbers of the
vertical and the horizontal grids respectively. The distributions
of normalized sizes in Fig. 5 show that U-Net is more
informationally efficient than nDNN, which indicates that
design of the underlying algorithm is important for CLCP.
Different αandδlead to different normalized sizes, implying
the trade-off among the preset loss level α, confidence level
1−δand informational efficiency of the prediction sets. By
choosing αandδproperly, the prediction sets of CLCP can
have reasonable sizes. Also, we can see that forecasting low
temperature is somehow easier than high temperature with
the fact that for the same αandδ, the normalized sizes of
forecasting low temperature are distributed lower than the
ones of forecasting high temperature, indicating the need of
design of the underlying algorithms to improve performance
for forecasting high temperature.
C. CLCP for maximum temperature and minimum tempera-
ture forecasting
This section focuses on using CLCP to forecast the 2-m
maximum temperature or minimum temperature value for eachTHIS WORK HAS BEEN SUBMITTED TO THE IEEE FOR POSSIBLE PUBLICATION 9
Fig. 6. Bar plots of the frequencies of the prediction losses being greater than αvs.δ= 0.05,0.1,0.15,0.2on test data for maximum temperature and
minimum temperature forecasting. The first row corresponds to MaxTemp and the second row corresponds to MinTemp. Different columns represent different
α. All bars are near or below the preset δ, which confirms the controlling guarantee of CLCP empirically.
Fig. 7. Boxen plots of the prediction losses vs. δ= 0.05,0.1,0.15,0.2on test data for maximum temperature and minimum temperature forecasting. The
first row corresponds to MaxTemp and the second row corresponds to MinTemp. Different columns represent different α. The loss distributions are controlled
byαandδproperly to obtain the empirical validity in Fig. 6.
grid, which is a point-wise regression problem or image-to-
image regression problem. To construct the prediction sets,
we follow the procedure proposed in [49] and train the neural
network with 3output channels jointly predicting the point-
wise 0.05,0.5and0.95quantiles of the fields using quantile
regression [10] [49], which are denoted by f0.05(x),f0.5(x)
andf0.95(x). Then the prediction set Cλ(x)is equal to
n
y:y(p,q)∈[f0.5
(p,q)(x)−λ∆−
(p,q)(x), f0.5
(p,q)(x)+λ∆+
(p,q)(x)]o
,
where
∆−(x) = max {f0.5(x)−f0.05(x),10−6},
∆+(x) = max {f0.95(x)−f0.5(x),10−6},
andmax is a point-wise operator making ∆−and∆+at least
10−6. This prediction set is a prediction band for the output
field, whose prediction interval at grid (p, q)is
[f0.5
(p,q)(x)−λ∆−
(p,q)(x), f0.5
(p,q)(x) +λ∆+
(p,q)(x)]with the point-wise width being an increasing function of λ.
This construction was proposed in [49] for image-to-image
regression and we use the same loss function in [49] measuring
miscoverage rate of a prediction band Cfor a field y, which
can be formalized as
L(y, C) =1
PQn
(p, q) :y(p,q)/∈C(p,q)o,
where C(p,q)is the prediction interval at grid (p, q)for
prediction band C.
All of the data collected from 2007 to2020 were used, lead-
ing to 4945 samples for each forecasting application and the
datasets are named as MaxTemp and MinTemp respectively.
The experimental design is the same as that in Section IV-
B, except that we also normalized the label for each grid to
[0,1]by min–max normalization, used quantile loss for model
selection and we searched for λ∗with two steps. First we
found two values λ1andλ2from{100,10,1,0.1,0.01, ...}
such that Q(n)
1−δ(λ1)≤αandQ(n)
1−δ(λ2)> α . Then weTHIS WORK HAS BEEN SUBMITTED TO THE IEEE FOR POSSIBLE PUBLICATION 10
Fig. 8. Boxen plots for the distributions of average interval length vs. δ= 0.05,0.1,0.15,0.2on test data for maximum temperature and minimum temperature
forecasting. The first row corresponds to MaxTemp and the second row corresponds to MinTemp. Different columns represent different α. U-Net performs
better than nDNN, which indicates the importance of careful design of the underlying algorithm.
searched for λ∗from 100values starting with λ1and ending
withλ2using a common step size. The experimental results
are recorded in Fig. 6, Fig 7 and Fig 8.
Although the set predictors and the loss function used in
this section are different from those in Section IV-B, the
experimental results and conclusions are similar. From Fig.
6, we can see that the frequencies of the prediction losses
being greater than αare controlled by δ, which also verifies
formula (7) empirically. Larger αandδlead to larger losses,
which is shown in Fig. 7. Here we use the following average
interval length
1
PQPX
p=1QX
q=1λ∗(∆+
(p,q)(x)−∆−
(p,q)(x))
to measure the informational efficiency of the prediction set
Cλ∗(x)and Fig. 8 also depicts the trade-off among the preset
loss level α, confidence level 1−δand informational efficiency
of the prediction sets and indicates that better design of
underlying algorithms leads to better performance.
V. C ONCLUSION
This paper extends conformal prediction to the situation
where the value of a loss function needs to be controlled,
which is inspired by risk-controlling prediction sets and con-
formal risk control approaches. The loss-controlling guarantee
is proved in theory with the assumption of exchangeability
and is empirically verified for different kinds of applications
including classification with a class-varying loss and weather
forecasting. Different from conformal prediction, conformal
loss-controlling prediction approach proposed in this paper
has two preset parameters αandδ, which guarantees that the
prediction loss is not greater than αwith confidence 1−δ. Both
parameters impose restrictions on prediction sets and should
be set based on specific applications. Despite loss-controlling
guarantee, informational efficiency of the prediction sets built
by conformal loss-controlling prediction is highly related to
underlying algorithms, which has been shown in empiricalstudies. Since this is a rather new topic, the underlying
algorithms and the way of constructing set predictors are
inherited from conformal risk control. This leaves the im-
portant question on how to build informationally efficient set
predictors in an optimal way, which is one of our further
researches in the future.
REFERENCES
[1] V . Balasubramanian, S.-S. Ho, and V . V ovk, Conformal prediction
for reliable machine learning: theory, adaptations and applications .
Newnes, 2014.
[2] C. Li, G. Tang, X. Xue, A. Saeed, and X. Hu, “Short-term wind speed
interval prediction based on ensemble gru model,” IEEE Transactions
on Sustainable Energy , vol. 11, no. 3, pp. 1370–1380, 2019.
[3] P. Wang, P. Wang, D. Wang, and B. Xue, “A conformal regressor
with random forests for tropical cyclone intensity estimation,” IEEE
Transactions on Geoscience and Remote Sensing , vol. 60, pp. 1–14,
2021.
[4] G. Morales and J. W. Sheppard, “Dual accuracy-quality-driven neural
network for prediction interval generation,” IEEE Transactions on Neu-
ral Networks and Learning Systems , 2023.
[5] J. Lu, J. Ding, C. Liu, and T. Chai, “Hierarchical-bayesian-based sparse
stochastic configuration networks for construction of prediction inter-
vals,” IEEE Transactions on Neural Networks and Learning Systems ,
vol. 33, no. 8, pp. 3560–3571, 2021.
[6] V . G ´omez-Verdejo, E. Parrado-Hern ´andez, and M. Mart ´ınez-Ram ´on,
“Adaptive sparse gaussian process,” IEEE Transactions on Neural Net-
works and Learning Systems , 2023.
[7] T. Melluish, C. Saunders, I. Nouretdinov, and V . V ovk, “Comparing the
bayes and typicalness frameworks,” in European Conference on Machine
Learning . Springer, 2001, pp. 360–371.
[8] H. Papadopoulos, “Guaranteed coverage prediction intervals with gaus-
sian process regression,” arXiv preprint arXiv:2310.15641 , 2023.
[9] Z. Zhang, Y . Dong, and W.-C. Hong, “Long short-term memory-based
twin support vector regression for probabilistic load forecasting,” IEEE
Transactions on Neural Networks and Learning Systems , 2023.
[10] R. Koenker and G. Bassett Jr, “Regression quantiles,” Econometrica:
Journal of the Econometric Society , pp. 33–50, 1978.
[11] V . V ovk, A. Gammerman, and G. Shafer, Algorithmic learning in a
random world . Springer Science & Business Media, 2005.
[12] A. N. Angelopoulos and S. Bates, “A gentle introduction to confor-
mal prediction and distribution-free uncertainty quantification,” arXiv
preprint arXiv:2107.07511 , 2021.
[13] M. Fontana, G. Zeni, and S. Vantini, “Conformal prediction: a unified
review of theory and new challenges,” Bernoulli , vol. 29, no. 1, pp.
1–23, 2023.THIS WORK HAS BEEN SUBMITTED TO THE IEEE FOR POSSIBLE PUBLICATION 11
[14] H. Wang, X. Liu, I. Nouretdinov, and Z. Luo, “A comparison of three
implementations of multi-label conformal prediction,” in International
Symposium on Statistical Learning and Data Sciences . Springer, 2015,
pp. 241–250.
[15] S. Messoudi, S. Destercke, and S. Rousseau, “Copula-based conformal
prediction for multi-target regression,” Pattern Recognition , vol. 120, p.
108101, 2021.
[16] J. Lei, A. Rinaldo, and L. Wasserman, “A conformal prediction approach
to explore functional data,” Annals of Mathematics and Artificial Intel-
ligence , vol. 74, no. 1, pp. 29–43, 2015.
[17] J. Diquigiovanni, M. Fontana, and S. Vantini, “Conformal prediction
bands for multivariate functional data,” Journal of Multivariate Analysis ,
vol. 189, p. 104879, 2022.
[18] A. Fisch, T. Schuster, T. Jaakkola, and R. Barzilay, “Few-shot conformal
prediction with auxiliary tasks,” in International Conference on Machine
Learning . PMLR, 2021, pp. 3329–3339.
[19] R. J. Tibshirani, R. Foygel Barber, E. Candes, and A. Ramdas, “Confor-
mal prediction under covariate shift,” Advances in Neural Information
Processing Systems , vol. 32, 2019.
[20] R. F. Barber, E. J. Candes, A. Ramdas, and R. J. Tibshirani, “Conformal
prediction beyond exchangeability,” arXiv preprint arXiv:2202.13415 ,
2022.
[21] V . Jensen, F. M. Bianchi, and S. N. Anfinsen, “Ensemble conformalized
quantile regression for probabilistic time series forecasting,” IEEE
Transactions on Neural Networks and Learning Systems , 2022.
[22] M. Zaffran, O. F ´eron, Y . Goude, J. Josse, and A. Dieuleveut, “Adaptive
conformal predictions for time series,” in International Conference on
Machine Learning . PMLR, 2022, pp. 25 834–25 866.
[23] S. Bates, A. Angelopoulos, L. Lei, J. Malik, and M. Jordan,
“Distribution-free, risk-controlling prediction sets,” Journal of the ACM
(JACM) , vol. 68, no. 6, pp. 1–34, 2021.
[24] A. N. Angelopoulos, S. Bates, A. Fisch, L. Lei, and T. Schuster,
“Conformal risk control,” arXiv preprint arXiv:2208.02814 , 2022.
[25] A. Fisch, T. Schuster, T. Jaakkola, and R. Barzilay, “Conformal predic-
tion sets with limited false positives,” arXiv preprint arXiv:2202.07650 ,
2022.
[26] M. Zaheer, S. Kottur, S. Ravanbakhsh, B. Poczos, R. R. Salakhutdinov,
and A. J. Smola, “Deep sets,” Advances in neural information processing
systems , vol. 30, 2017.
[27] H. Papadopoulos, “Inductive conformal prediction: Theory and applica-
tion to neural networks,” in Tools in artificial intelligence . IntechOpen,
2008.
[28] J. Lei, M. G’Sell, A. Rinaldo, R. J. Tibshirani, and L. Wasserman,
“Distribution-free predictive inference for regression,” Journal of the
American Statistical Association , vol. 113, no. 523, pp. 1094–1111,
2018.
[29] A. Ortega, P. Frossard, J. Kova ˇcevi´c, J. M. Moura, and P. Vandergheynst,
“Graph signal processing: Overview, challenges, and applications,” Pro-
ceedings of the IEEE , vol. 106, no. 5, pp. 808–828, 2018.
[30] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and S. Y . Philip, “A
comprehensive survey on graph neural networks,” IEEE transactions on
neural networks and learning systems , vol. 32, no. 1, pp. 4–24, 2020.
[31] C. Gupta, A. K. Kuchibhotla, and A. Ramdas, “Nested conformal pre-
diction and quantile out-of-bag ensemble methods,” Pattern Recognition ,
vol. 127, p. 108496, 2022.
[32] A. N. Angelopoulos, S. Bates, E. J. Cand `es, M. I. Jordan, and L. Lei,
“Learn then test: Calibrating predictive algorithms to achieve risk
control,” arXiv preprint arXiv:2110.01052 , 2021.
[33] G. Van Rossum and F. L. Drake, Python 3 reference manual . Scotts
Valley, CA: CreateSpace, 2009.
[34] F. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel, B. Thirion,
O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V . Dubourg, J. Vander-
plas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duch-
esnay, “Scikit-learn: Machine learning in Python,” Journal of Machine
Learning Research , vol. 12, pp. 2825–2830, 2011.
[35] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,
T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf,
E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner,
L. Fang, J. Bai, and S. Chintala, “Pytorch: An imperative style,
high-performance deep learning library,” in Advances in Neural
Information Processing Systems 32 . Curran Associates, Inc., 2019,
pp. 8024–8035. [Online]. Available: http://papers.neurips.cc/paper/
9015-pytorch-an-imperative-style-high-performance-deep-learning-library.
pdf
[36] A. Asuncion and D. Newman, “Uci machine learning repository,” 2007.[37] J. Platt, “Probabilistic outputs for support vector machines and com-
parisons to regularized likelihood methods,” Advances in Large Margin
Classifiers , vol. 10, no. 3, pp. 61–74, 1999.
[38] J. Schmidhuber, “Deep learning in neural networks: An overview,”
Neural Networks , vol. 61, pp. 85–117, 2015.
[39] L. Breiman, “Random forests,” Machine Learning , vol. 45, no. 1, pp.
5–32, 2001.
[40] S. Vannitsem, D. S. Wilks, and J. Messner, Statistical postprocessing of
ensemble forecasts . Elsevier, 2018.
[41] S. Vannitsem, J. B. Bremnes, J. Demaeyer, G. R. Evans, J. Flowerdew,
S. Hemri, S. Lerch, N. Roberts, S. Theis, A. Atencia et al. , “Statistical
postprocessing for weather forecasts: Review, challenges, and avenues
in a big data world,” Bulletin of the American Meteorological Society ,
vol. 102, no. 3, pp. E681–E699, 2021.
[42] T. Palmer, “The ecmwf ensemble prediction system: Looking back (more
than) 25 years and projecting forward 25 years,” Quarterly Journal of
the Royal Meteorological Society , vol. 145, pp. 12–24, 2019.
[43] National Centers for Environmental Prediction, National Weather
Service, NOAA, U.S. Department of Commerce, Japan Meteorological
Agency, Japan, Met Office, Ministry of Defence, United Kingdom,
China Meteorological Administration, China, Meteorological Service
of Canada, Environment Canada, Korea Meteorological Administration,
Republic of Korea, Meteo-France, France, European Centre for
Medium-Range Weather Forecasts, and Bureau of Meteorology,
Australia, “Thorpex interactive grand global ensemble (tigge) model
tropical cyclone track data,” Boulder CO, 2008. [Online]. Available:
https://doi.org/10.5065/D6GH9GSZ
[44] H. Hersbach, B. Bell, P. Berrisford, S. Hirahara, A. Hor ´anyi, J. Mu ˜noz-
Sabater, J. Nicolas, C. Peubey, R. Radu, D. Schepers et al. , “The
era5 global reanalysis,” Quarterly Journal of the Royal Meteorological
Society , vol. 146, no. 730, pp. 1999–2049, 2020.
[45] Z. Li, F. Liu, W. Yang, S. Peng, and J. Zhou, “A survey of convo-
lutional neural networks: Analysis, applications, and prospects,” IEEE
Transactions on Neural Networks and Learning Systems , 2021.
[46] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional net-
works for biomedical image segmentation,” in International Conference
on Medical Image Computing and Computer-Assisted Intervention .
Springer, 2015, pp. 234–241.
[47] P. Gr ¨onquist, C. Yao, T. Ben-Nun, N. Dryden, P. Dueben, S. Li,
and T. Hoefler, “Deep learning for post-processing ensemble weather
forecasts,” Philosophical Transactions of the Royal Society A , vol. 379,
no. 2194, p. 20200092, 2021.
[48] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”
arXiv preprint arXiv:1412.6980 , 2014.
[49] A. N. Angelopoulos, A. P. Kohli, S. Bates, M. Jordan, J. Malik,
T. Alshaabi, S. Upadhyayula, and Y . Romano, “Image-to-image regres-
sion with distribution-free uncertainty quantification and applications in
imaging,” in International Conference on Machine Learning . PMLR,
2022, pp. 717–730.