BACKPROPAGATION THROUGH BACK SUBSTITUTION WITH A
BACKSLASH
ALAN EDELMAN∗, EKIN AKY ¨UREK†,AND YUYANG WANG‡
Abstract. We present a linear algebra formulation of backpropagation which allows the calcu-
lation of gradients by using a generically written “backslash” or Gaussian elimination on triangular
systems of equations. Generally, the matrix elements are operators. This paper has three contri-
butions: (i) it is of intellectual value to replace traditional treatments of automatic differentiation
with a (left acting) operator theoretic, graph-based approach; (ii) operators can be readily placed in
matrices in software in programming languages such as Julia as an implementation option; (iii) we
introduce a novel notation, “transpose dot” operator “ {}T•” that allows for the reversal of operators.
We further demonstrate the elegance of the operators approach in a suitable programming lan-
guage consisting of generic linear algebra operators such as Julia [3], and that it is possible to realize
this abstraction in code. Our implementation shows how generic linear algebra can allow operators
as elements of matrices. In contrast to “operator overloading,” where backslash would normally have
to be rewritten to take advantage of operators, with “generic programming” there is no such need.
1. Preface: Summary and the Challenge. This paper provides the math-
ematics to show how an operator theoretic, graph-based approach can realize back-
propagation by applying back substitution to a matrix whose elements are operators.
As a showcase result, one can back-propagate to compute the gradient on feed-
forward neural networks (or Multi-layer Perceptron (MLP)) [5] with
∇J=MT((I−˜L)T\g), (1.1)
where M(source to non-source nodes) and ˜L(within non-source nodes) are blocks
of the adjacency matrix of the computational graph (see Section 3.2.2 for precise
definitions), gis the vector of gradients of the loss function, and Iis the identity
matrix. For readers unfamiliar with the backslash notation, an equivalent expression
of (1.1) is ∇J=MT(I−˜L)−Tg.
We then set up a challenge to ourselves. Could we correctly implement (1.1) by
simply typing the command (after basic setup but without overloading of backslash )
?
We demonstrate that indeed the backpropagation can be achieved, almost by
magic, in a programming language armed with generic programming allowing for
operators as elements of matrices. The software in Section 6.1 is by itself interesting
not for the usual reasons of what it does, but in this case how it does it: how a
powerful language with generic programming and multiple dispatch can allow this
abstract mathematical formulation to be realized.
2. Introduction: Linear Algebra, Graphs, Automatic Differentiation
(AD), Operators, and Julia. Automatic differentiation (AD) is fundamental to
gradient-based optimization of neural networks and is used throughout scientific com-
puting. There are two popular approaches to AD: namely, forward and backward
(reverse) modes [6, 7, 2, 13], the latter of which is also known as backpropagation in
the Machine Learning (ML) literature.1A common high-level description of AD is
∗Department of Mathematics and CSAIL, MIT, Cambridge, MA (edelman@mit.edu).
†Department of EECS and CSAIL, MIT, Cambridge, MA (akyurek@mit.edu)
‡AWS AI Labs, Santa Clara, CA (yuyawang@amazon.com). Work done prior to joining Amazon.
1Despite similar terminology, the term “forward propagation” (or forward pass) in machine learn-
ing (ML) has no connection to forward mode automatic differentiation (AD). Instead, it refers to the
1arXiv:2303.15449v2  [math.NA]  31 Aug 2023that it is really “only” the chain-rule. The centuries old technology of taking deriva-
tives is taking on a modern twist in the form of Differentiable Programming [8, 11].
Who would have thought that one of the most routine college course subjects would
now be the subject of much renewed interest both in applied mathematics and com-
puter science?
This paper introduces the notion that AD is best understood with a matrix-based
approach. The chain-rule explanation, in retrospect, feels to us as a distraction or at
least extra baggage. We suspect that while the chain-rule is well known, it is under-
stood mechanically rather than deeply by most of us. We argue that a linear algebra
based framework for AD, while mathematically equivalent to other approaches, pro-
vides a simplicity of understanding, and equally importantly a viable approach worthy
of further study.
Regarding software, while most high-level languages allow for matrices whose
elements are scalars, the ability to work with matrices whose elements might be oper-
ators without major changes to the elementwise software is an intriguing abstraction.
We discuss a Julia implementation that makes this step particularly mathematically
natural.
It is our view that a linear algebraic approach sheds light on how backpropagation
works in its essence. We theoretically connect backpropagation to the back substi-
tution method for triangular systems of equations. Similarly, forward substitution
corresponds to the forward mode calculation of automatic differentiation. As is well
documented in the preface to the book Graph Algorithms in the Language of Linear
Algebra [10], there have been many known benefits to formulate mathematically a
graph algorithm in linear algebraic terms.
The ability to implement these abstractions while retaining performance is demon-
strated using Julia, a language that facilitates abstractions, multiple dispatch, the type
system, and which offers generic operators.
3. A Matrix Method for Weighted Paths.
3.1. “Forward and Back” through Graphs and Linear Algebra. In the
spirit of reaching the mathematical core, let us strip away the derivatives, gradients,
Jacobians, the computational graphs, and the “chain rule” that clutter the story of
how is it possible to compute the same thing forwards and backwards. We set ourselves
the goal of explaining the essence of forward mode vs backward mode in AD with a
single figure. Figure 3.1 is the result. Note that “forward mode” differentiation is not
to be confused with the forward computation of the desired quantity.
3.1.1. Path Weights on Directed Graphs. Consider a directed acyclic graph
(DAG) with edge weights as in Figure 3.1 where nodes 1 and 2 are sources (starting
nodes), and node 5 is a sink (end node). The problem is to compute the path
weights , which we define as the products of the weights from every start node to
every sink node.
Evidently, the path weights that we seek in Figure 3.1 may be obtained by cal-
process where a neural network calculates its output by sequentially passing input data through each
layer, applying weighted sums and activation functions, until it reaches the output layer. In later
sections, Algorithm 5.1 and 5.2 illustrate such a procedure. Whereas, “backpropagation” (backward
pass, or reverse mode AD) is so named because information flows backwards through the network
during this process.
2culating
path weights =
1 0
0 1
0 0
0 0
0 0
T
|{z}
sources(I−LT)−1
0
0
0
0
1

|{z}
sink, (3.1)
where LT, the adjacency matrix (edge weight matrix), is displayed in the lower left
of Figure 3.1. One explanation of why (3.1) works for calculating the path weights is
that ( LT)k
ijsums the path weights of length kfrom node ito node jand ( I−LT)−1=
I+LT+. . .+ (LT)n−1then counts path weights of all lengths from itoj.
<latexit sha1_base64="iGO5AZbg4UWVIsQpaZhYrFoqiKM=">AAADXHicfVJBa9swGJXtbkvTdUs32GUXsdDRgWuskM7ZYVC6wzbYoRtNW4jdIMtKKirLRpLHgvGf3K2X/ZVNdg1Lk5IPhB7v6el9+lCcc6a0799atrP16PGTznZ35+nus+e9vRfnKiskoWOS8UxexlhRzgQda6Y5vcwlxWnM6UV886nWL35SqVgmzvQip1GK54LNGMHaUNM9Sx58hYfw29XZu6vyEFXwIwxjOmeizFOsJftVdRF8Cz2zyrCJm/z4fBKVwZGLgoGLPowqiKuNKlmTERq4w5GL3vu1nFQwDLt1Atp0T7wxJd6cEi+leG2QcaxaBsPARUe+GxhL7ViyPNjf8vl7Ac0GG0vNhlQk/wc67fV9z28KrgPUgj5o63Ta+x0mGSlSKjThWKkJ8nMdlVhqRjitumGhaI7JDZ7TiYECp1RFZdNnBfcNk8BZJs0SGjbssqPEqVKL1Ix33zR4rVa1mnxImxR6NopKJvJCU0HugmYFhzqD9U+DCZOUaL4wABPJTK+QXGOJiTb/sR4CWn3yOjgfeMj30Pdh//ikHUcHvAZvwAFAIADH4As4BWNArFvrr92xt+0/zpaz4+zeHbWt1vMS3Cvn1T8X9PK3</latexit>abcdaacacd123415bbcbcd2acdcd5Forward Path WeightsBackward Path WeightsDirected Acyclic Graph (DAG)1234512345Edge Weights MatrixPath Weights Matrixdbcd
<latexit sha1_base64="pRa2Zk4kEIlF/DtllGDRmPjxwWk=">AAACeHicbZE7T8MwEMed8CrlVWDsYlGeS5RUSLAgVbAwMIBEW6SmVI5zLVYdJ9gOoor6GfhubHwQFiacNiAonHTy37+7053PQcKZ0q77Ztlz8wuLS6Xl8srq2vpGZXOrpeJUUmjSmMfyLiAKOBPQ1ExzuEskkCjg0A6GF3m8/QRSsVjc6lEC3YgMBOszSrRBvcrL1f0tPsP+Y0pC7AcwYCJLIqIlex47eB/nTorTwb7/xQI8C6dXOuW/Ye7hLPrK80GE3y3LvUrNddyJ4b/CK0QNFXbdq7z6YUzTCISmnCjV8dxEdzMiNaMcxmU/VZAQOiQD6BgpSASqm00WN8Z7hoS4H0vjQuMJ/VmRkUipURSYTDPgg5qN5fC/WCfV/dNuxkSSahB02qifcqxjnP8CDpkEqvnICEIlM7Ni+kAkodr8Vb4Eb/bJf0Wr7niu490c1xrnxTpKqIp20CHy0AlqoEt0jZqIonerau1ae9aHje0D+2iaaltFzTb6ZXb9E7PerKI=</latexit>
Fig. 3.1: Legend : Purple: target weights, Blue: forward computation, Orange:
backward computation. The dots in matrices denote zeros.
(Upper Left:) Multiply the weights along the paths from source node 1 to sink node
5 and also source node 2 to sink node 5 to obtain acd and bcd. (Right Blue:) The ob-
vious forward method. (Right Orange:) A backward method that requires one fewer
multiplication.
(Below:) A matrix method: if LT
ij= the weight on edge ij, then ( I−LT)−1simul-
taneously exhibits the forward (i.e. a→ac→acdandb→bc→bcd) and backward
methods (i.e. d→cd→bcd→acd/bcd ).
If one follows step by step the linear algebra methods of forward substitution
for lower triangular matrices or back substitution for upper triangular matrices, one
obtains path weights algorithms as summarized in Figure 3.2. We remind the reader
3that forward and back substitution are the standard methods to solve lower and upper
triangular systems respectively.
Two Equivalent Ways to Compute the Path Weights in Figure 3.1:
Forward Substitution:
0
0
0
0
1
T
(I−L)−1
1 0
0 1
0 0
0 0
0 0


Back Substitution:
1 0
0 1
0 0
0 0
0 0
T
(I−LT)−1
0
0
0
0
1


Fig. 3.2: The forward and backward methods compared: Both are seen equivalently
as a choice of parenthesizing (3.1) or as forward substitution vs. back substitution.
Generally speaking, when the number of sources is larger than the number of sinks,
one might expect the backward method to have less complexity.
3.1.2. Generalizing “Forward and Back” to a Catalan number of pos-
sibilities. Continuing with the same Lmatrix from Section 3.1, we can begin to un-
derstand all of the possibilities including the forward method, the backward method,
the mixed-modes methods, and even more possibilities:

1 0
0 1
0 0
0 0
0 0
T
(I−LT)−1
0
0
0
0
1
=

1 0
0 1
0 0
0 0
0 0
T
1. a . .
.1. . .
. . 1. .
. . . 1.
. . . . 1

1. . . .
.1b . .
. . 1. .
. . . 1.
. . . . 1

1. . . .
.1. . .
. . 1c .
. . . 1.
. . . . 1

1. . . .
.1. . .
. . 1. .
. . . 1d
. . . . 1

0
0
0
0
1
.
It is well known [14], that there are a Catalan number, C5= 42, ways to paren-
thesize the above expression. One of the 42 choices evaluates left to right; this is
forward substitution which computes the graph weights forward. Another evaluating
from right to left is backward substitution. There are three other “mixed-modes” [12]
which combine forward and backward methods. The remaining 37 methods require
matrix-matrix multiplication as a first step. We encourage the reader to work out
some of these on the graph. Partial products correspond to working through sub-
graphs. Perhaps readers might find cases where working from the middle outward
4can be useful. For example it would be possible to go from the middle outward using
the Example of Figure 3.1: we would go from ctocdthen compute acdandbcd.
Fig. 3.3: Elimination of the edge from node 3 to node 4 on the graphs and with
matrices. The matrix versions involve a rank one update to a row and column deleted
matrix in the case of the Edge Weights Matrix and only a deletion of a row and
column in the Path Weights Matrix.
3.1.3. Edge Elimination. It is possible to eliminate an edge (and preserve the
path weights) by moving the weight of the edge to the weights of the incoming edges.
5We illustrate this in Figure 3.3 by eliminating the edge from node 3 to node 4, moving
the weight cto the incoming edges by multiplication. The corresponding linear algebra
operation on LTis the deletion of column 3 and row 3 and the rank 1 update based on
this column and row with the (3,3) element deleted. The corresponding linear algebra
operation on ( I−LT)−1is merely the deletion of column 3 and row 3. This example
is representative of the general case.
3.1.4. Edge addition at the Sink Node. We will be interested in the case
where the edge weight graph is modified by adding one edge to the sink node. Con-
tinuing our example from Figure 3.3, we will add an edge “ e” by starting with:
path weights =
1 0
0 1
0 0
0 0
0 0
T
|{z}
sources(I−LT)−1
|{z}
path weights matrix
0
0
0
0
1

|{z}
sink(3.2)
and then updating by augmenting the graph with one end node to become
updated path weights =
1 0
0 1
0 0
0 0
0 0
0 0
T
|{z}
sources
(I−LT)−1.
. 1
1. . . . .
.1. . . .
. . 1. . .
. . . 1. .
. . . . 1e
. . . . . 1

| {z }
updated path weights matrix
0
0
0
0
0
1

|{z}
sink.(3.3)
The update from the path weights matrix in (3.2) to the updated path weights
matrix in (3.3) can be verified in many ways. One simple way is to look at the explicit
elements of the path weights matrix before and after and then notice that the new
matrix has a column with one more element eaugmented with a 1.
It is an easy exercise in linear algebra to show that (3.3) is the same as (3.4)
which folds the added edge emultiplicatively into the sink vector.
updated path weights =
1 0
0 1
0 0
0 0
0 0
T
|{z}
sources(I−LT)−1
.
.
.
.
e

|{z}
sink. (3.4)
3.2. Examples of DAGs and Weighted Paths.
3.2.1. The “Complete DAG” and Weighted Paths. Consider as an exam-
ple in Figure 3.4, the complete DAG on four nodes with graph weights evaluated
through a forward and backward method. There is one source and one sink. We find
that this complete DAG example reveals most clearly the equivalence between path
weights and the inverse matrix.
6We see that the forward path weights folds in the edges labelled “ a,” then “ b,”
then “ c.” This works through the matrix LTby columns. The backward mode folds
in the edges with subscript “3,” then “2,” then “1.” This works through the matrix
LTby rows from bottom to top.
Fig. 3.4: The complete DAG on four nodes illustrates a symmetric situation where for-
ward and backward have the same complexity but arrive at the same answer through
different operations.
3.2.2. The “multi-layer perceptron DAG” and Weighted Paths. Fig-
ure 3.5 is the DAG for the derivatives in a multi-layer perceptron (MLP). It may be
thought of as a spine with feeds for parameters (nodes 1,2,3, and 4 in the figure).
If sources are labeled 1 , . . . , s (in Figure 3.5, s= 4), then the top left sbys
matrix in LTis the zero matrix as there are no connections. We can then write
LT=0MT
0˜LT
, (3.5)
where
MT=
w . . .
. x . .
. . y .
. . . z
,˜LT=
. a . .
. . b .
. . . c
. . . .
,
where the matrix MTcorresponds to connections between the sources and internal
nodes, and ˜LTcorresponds to internal connections. In this example MTis diagonal
corresponding to a bipartite matching between nodes 1 ,2,3,4 and 5 ,6,7,8. The ˜LT
7Fig. 3.5: This diagram contains most of what is needed to understand forward and
backward propagation of derivatives through a MLP. The details of what the weights
look like will come later. If we take n= 4 for the pictured network, the sources
are labeled 1 : nand the sink is labeled 2 n. Forward mode requires n(n−1)/2
multiplications while backward mode requires 2 n−3 multiplications.
matrix represents internal connections, in this case it is the “spine” linearly connecting
nodes 5 ,6,7,8.
Now we have
(I−LT) =I−MT
0I−˜LT
,and ( I−LT)−1=
I MT(I−˜LT)−1
0 ( I−˜LT)−1
.
If the last node is the one unique sink, then we obtain the useful formula
Path weights = MT(I−˜LT)−1
0
...
0
1
. (3.6)
We can now take a close look at Figure 3.5 and fully grasp the path weight struc-
ture. The spine consisting of a, b, c and 1 (understood) requires the computation of the
cumulative suffix product 1 , c, bc, abc . What follows is an element-wise multiplication
8byz, y, x, w , from which we can calculate the last column of MT(I−˜LT)−1
MT(I−˜LT)−1
0
0
0
1
=
wabc
xbc
yc
z
. (3.7)
3.3. Computational Graphs, Derivative Graphs, and their superposi-
tion. Many treatments of automatic differentiation introduce computational graphs
at the start of the discussion. Our treatment shows that this is not necessary. How-
ever, in the end the key application of edge weights will be as derivatives of computed
quantities. To this end, we define
Definition 3.1.A computational graph is a node labelled DAG, where leaf nodes
consist of variable names, and non-leaf nodes contain variable names and formulas
that depend on incoming variables.
We remark that there are variations on where the variable names and formulas
live on a computational graph, but we believe the definition here is the cleanest when
wishing to incorporate derivative information.
Fig. 3.6: An example of Computational (node) graph, derivative (edge) graph, and
their “superposition.”
3.3.1. The chain rule, derivatives, and Jacobians. Here we say explicitly
how the edge weights and path weights relate to derivatives in a computation.
Consider the computation from Figure 3.6, the next three algorithms show the
computation, the derivatives of each line of code, and the overall derivatives. We see
that the one step derivatives are edge weights and the overall derivatives are path
weights.
If the final output is a scalar, we immediately have that the gradient with respect
to the source xandy(in Figure 3.6) is exactly the path weight defined in (3.6),
gradient = the last column of MT(I−˜LT)−1, (3.8)
which corresponds to the output in Algorithm 3.8 with
MT=y· ·
x· ·
,˜L=
·2p·
· · − e−q
· · ·
.
9Player Description
Edge weight from
node iandjThese are the derivatives of one step of a computa-
tion. These can be scalars but in general these are
Jacobian matrices (or operators).
Path weight from
node itojThese are the derivatives that reach back into a chain
of computations. The chain rule states that if you
multiply (“chain together”) the derivatives at each
step you get the dependence of one variable on an
earlier variable.
SourceThe sources in the graph are typically parameters in
real computations, as many modern applications are
interested in the derivatives with respect to the input
parameters.
SinkThe sink is usually what is known as a loss function
in modern applications.
Table 1: A dictionary translating graph elements to AD (cf. Figure 3.6).
Equation (3.8) fully describes backpropogation. For completeness, the term ”forward
propagation” (forward pass) describes the process of executing the computational
graph in a forward direction (left to right), storing the intermediate values that are
subsequently utilized in backpropagation (backward pass).
Algorithm 3.1 Simple Algorithm Ex-
ample from Figure 3.6
1:p←multiply( x, y)
2:q←square( p)
3:r←expneg(q)
4:output rAlgorithm 3.2 Edge weights (deriva-
tives of one line of code)
1:d{multiply( x, y)}/dx=y(=∂p
∂x)
2:d{multiply( x, y)}/dy=x(=∂p
∂y)
3:d{square( p)}/dp= 2p (=∂q
∂p)
4:d{expneg(q)}/dr=−e−q(=∂r
∂q)
Algorithm 3.3 Path weights (Chained derivatives)
1:dr/dx =y×2p×(−e−q) (Chain lines 1,3, and 4 of Algorithm 3.2)
2:dr/dy =x×2p×(−e−q) (Chain lines 2,3, and 4 of Algorithm 3.2 )
4. Linear Operators as elements of Matrices. We will illustrate in Section
6.1 the value of software that allows linear operators as elements of matrices. Here
we set the mathematical stage, starting with a question.
Consider a matrix transformation of Xsuch as TA,B:X7→BXAT,how should
we represent the Jacobian ∂TA,B/∂X?
Before we answer, we remind the reader how the Kronecker product works. One view
of the Kronecker product A⊗Bof two matrices is that it multiplies every element
inAtimes every element of Bplacing the elements in such a way that we have the
10identity
(A⊗B)vec(X) = vec( BXAT),
where vecdenotes the flattening of a matrix Xinto a vector by stacking its columns.
We may abuse notation when there is no confusion and write
(A⊗B)(X) =BXAT,
for the linear operator TA,Bthat sends XtoBXAT. Identifying the matrix A⊗B
with the operator is more than a handy convenience, it makes computations practical
in a software language that allows for this. Table 2 defines some operators of interest.
Symbol Definition Dense Representation
Kronecker Product
ofA, BA⊗B X7→BXATA⊗B m 1n1×mn
Left Multiplication
byBBL X7→BX I⊗B m 1n×mn
Right Multiplica-
tion by AAR X7→XA AT⊗I mn 1×mn
Hadamard Product
with MMH X7→M.∗X diag(vec( M)) mn×mn
Matrix inner prod-
uct with GGT• X7→Tr(GTX)vec(G)T1×mn
Table 2: Matrix Operators and the size of their dense representations assuming X:
m×n, A :n1×n, B :m1×m, M :m×n,andG:m×n. We overload A⊗B
to be both the operator and the matrix.
Consider the inner product (matrix dot product) ⟨X, Y⟩= Tr( XTY). The iden-
tity⟨X, AY ⟩=⟨ATX, Y⟩implies ( AL)T= (AT)L,in words, the operator adjoint with
respect to the operator AL(left multiplication by A) is left multiplication by AT. The
operator transposes are ( AL)T= (AT)L, (BR)T= (BT)R, and ( MH)T=MH(sym-
metric).
We wish to propose a carefully thought out notation for another useful operator,
GT•(“Gtranspose dot”), the matrix inner (or dot) product with G.
Definition 4.1.LetGT•(“Gtranspose dot”) denote the matrix inner (or dot)
product with G. This operator takes a matrix Xof the same size as Gand returns
the scalar, GT•X:=Tr(GTX)= vec( G)Tvec(X) =P
i,jGijXij.
Many communities choose a notation where small Roman letters denote a column
vector, so that x7→gTxdenotes a linear function of x. Those who are used to this
notation no longer “see” the transpose so much as turning a column into a row, but
rather they see the linear function gTas an object that acts on (“eats”) vectors and
returns scalars. In the same way we propose that one might denote a linear function
of a matrix X7→Tr(GTX) with the operator notation X7→GT•X, an operator that
“eats” matrices and returns scalars.
Lemma 4.1.If the superscript “ ()T” is overloaded to denote real operator adjoint
or matrix transpose as appropriate, Lis a linear operator and Gis a matrix, then we
11have the operator identity: (LTG)T•=GT•L.Notice that if we pretend all letters are
just matrices and you ignore the dot, the notation has the appearance of the familiar
transpose rule.
Proof. We have that for all X,
(LTG)T•X=⟨LTG, X⟩=⟨G,LX⟩=GT•LX,
showing that as operators ( LTG)T•=GT•L.
As an example, we have
(AT
LG)T•=X7→Tr((ATG)TX),and GT•AL=X7→Tr(GTAX),
which shows that ( AT
LG)T•=GT•AL. We encourage the reader to follow the matri-
cesA, G, ATand the operators AT
L, AL,(AT
LG)T•, GT•.(See Section 5.4 for why this
notation can be valuable.)
5. Operator Methodology. We proceed from matrices of scalars to matrices
of vectors to matrices of operators in Sections 5.1, 5.2, and 5.3. ultimately taking
advantage of Julia’s capabilities. We encourage the reader to compare the matrices
in each of these sections. Section 5.4 illustrates the power of the GT•notation, while
Section 5.5 shows the relationship to the adjoint method that is well known in the
field of scientific computing.
Algorithm 5.1 Scalar MLP without Bias (forward propagation)
1:Input data x0, initial weights wi, i= 1,···, N
2:Select activation functions hi(·) such as sigmoid, tanh, ReLU, etc.
3:fori= 1 to Ndo
4: xi←hi(wixi−1)
5: (δi←h′
i(wixi−1))
6:end for
7:Output xN
5.1. Matrices of scalars. The simple case of scalar neural networks (shown
in Algorithm 5.1) without bias shows the power of the graph approach. However,
the full power is revealed in the coming sections. Here we remind the reader of the
algorithm, draw the graphs, and instantly write down the linear algebra that provides
the gradients through backpropogation. (The graphs and matrices are illustrated for
N= 4 for ease of presentation.)
12Gradient
w.r.t. pa-
rameters p
(leaf nodes)∇pL= MT× (I−L)−1× g
=
m1
m2
m3
m4
T
×
I
−l2I
−l3I
−l4I
−T
×
.
.
.
g4

(i) Scalar
p={wi} mi=δixi−1 li=δiwi g4=L′(x4)
(ii) Vector
p =
{[wi, bi]}mi= [δixi−1δi] '' '' '' ''
(iii) Matri-
ces
p =
{[Wi, Bi]}mi= [∆ iH◦Xi−1R∆iH] li= ∆ iH◦WiL g4=∇X4L
↑−Operators −↑
Table 3: Algebraic Structure for an MLP when the parameters (i.e. the set of leaf
nodes collectively referred to as p) are (i) only scalar weights (ii) a weight/bias vector,
and (iii) a vector of weight/bias matrices. We emphasize the common algebraic struc-
ture and the benefit of software that can represent matrices of vectors and matrices
of operators.
Fig. 5.1: Top left: computational graph of a scalar MLP. This computation, which
has nothing to do with derivatives, is often referred to as forward propagation because
of its direction. Evaluation must generally necessarily go from left to right. Top right:
derivative edge weights. Since derivatives are linear, multiple directions are possible
to evaluate the products. Bottom: the superimposed graph showing both the forward
computation and the derivative edge weights.13Starting with
˜LT=
. δ2w1 . .
. . δ 3w2 .
. . . δ 4w3
. . . .
, M=
δ1x0 . . .
. δ 2x1 . .
. . δ 3x2 .
. . . δ 4x3
,
it is an immediate consequence of our graph theory methodology which concluded
with (3.4) and (3.7) that the backpropagated gradient is computed by evaluating
efficiently
∇wL=
δ1x0
δ2x1
δ3x2
δ4x3

1
−δ2w1 1
−δ3w2 1
−δ4w31
−T
.
.
.
L′(x4)

5.2. Matrices of vectors. As a baby step towards the matrices of operators
approach, we show how one can (optionally) group weights and biases that appear
in a neuron. Algorithm 5.1 is modified so that wixi−1is replaced with wixi−1+bi.
In the interest of space, we will simply write the answer of ∇[w,b]Land discuss its
format,

[δ1x0δ1]
[δ2x1δ2]
[δ3x2δ3]
[δ4x3δ4]
T
1
−δ2w1 1
−δ3w2 1
−δ4w31
−T
.
.
.
L′(x4)
.
We see we have an ordinary matrix back substitution followed by multiplication
by a diagonal matrix of row vectors of length 2 so that the result is a vector of column
vectors of length 2 which nicely packages the gradients with respect to the weight and
bias in each neuron. We remark that the transpose applies recursively in the diagonal
matrix. The transpose is overkill in this case but is critical in the next section.
5.3. Matrices of operators. Letting Idenote the identity operator and empty
space the zero operator, we have the following
∇[W,B]L=
[∆1H◦X0R∆1H]
[∆2H◦X1R∆2H]
[∆3H◦X2R∆3H]
[∆4H◦X3R∆4H]
T
×
I
−∆2H◦W2L I
−∆3H◦W3L I
−∆4H◦W4LI
−T
.
.
.
∇X4L

for the matrix neural network in Algorithm 5.2. The entries of our matrix of operators
may be read immediately from the differential of line 4 of Algorithm 5.2:
dXi=d[hi(WiXi−1+Bi]
= (∆ iH◦Xi−1R)dWi+ ∆iHdBi+ (∆ iH◦WiL)dXi−1,
14where ∆ iis the gradient matrix, and the definitions of the operators ∆ iH,WiL, and
Xi−1Rare given in Table 2.
Algorithm 5.2 Matrix MLP (forward propagation)
1:Input data X0(n0×k), and initial weight matrices and corresponding bias terms
Wi(ni×ni−1), Bi(ni×k)
2:Select activation functions hi(·) such as sigmoid, tanh, ReLU, etc.
3:fori:= 1 to Ndo
4: Xi←hi(Wi∗Xi−1+Bi).
5: (∆i←h′
i(Wi∗Xi−1+Bi))
6:end for
7:output XN
Fig. 5.2: Computational and derivative graphs of a matrix MLP with their superim-
posed version. Compared to Figure 5.1 of the scalar MLP, everything remains the
same except for two changes: the elements in the computational graph are now ma-
trices, and the edges in the derivative graph have been replaced by operators.
5.4. The Power of Notation. We read directly off the edge weight graph in
Figure 5.2 that for a matrix neural network we have
Forward Mode Operators (right to left)
∂L
∂Wi=GT•(∆N)H(WN)L. . .(Wi+2)L(∆i+1)H(Wi+1)L(∆i)H(Xi−1)R
∂L
∂Bi=GT•(∆N)H(WN)L. . .(Wi+2)L(∆i+1)H(Wi+1)L(∆i)H(5.1)
15or going the other way we have,
backward Mode Operators (right to left)
∂L
∂WiT•
=
(XT
i−1)R(∆i)H(WT
i+1)L(∆i+1)H(WT
i+2)L. . .(WT
N)L(∆N)HG	T•
∂L
∂BiT•
=
(∆i)H(WT
i+1)L(∆i+1)H(WT
i+2)L. . .(WT
N)L(∆N)HG	T•.(5.2)
Understanding these operators. The forward operators in Equation (5.1) may
be thought of as sensitivity operators or as a means of computing the full gradient.
As a sensitivity operator, one can state that the directional derivative of Lin the
direction ∆ Wiis∂L
∂W i(∆Wi). Alternatively, each operator can be written out as a
(large) matrix, and ultimately a gradient can be computed. The backward operator
is intended to be evaluated from right to left inside the braces. Doing so computes
the gradient directly.
We hope the reader appreciates the power of the “ T•” notation, whereby one
feels we are taking transposes of matrices and reversing order, but in fact we are
transposing the operators. Either way the operators can be read right off the graphs.
5.5. Relationship to the Adjoint Method of scientific computing. We
will show how to derive (3.8) and (3.7) using the adjoint method so-named because
of its focus on the transpose (the adjoint) of the Jacobian. We encourage interested
readers to see [9] and [4] to learn more about adjoint methods in numerical computa-
tion.
We find it satisfying that the graph theoretic interpretation of backward mode
AD and the adjoint method of scientific computing yield the same answer from two
very different viewpoints.
Consider a general computation with known constant input x0∈Rand parame-
tersp= [p1, . . . , p k]:
Algorithm 5.3 General Computation
1:Input constant x0
2:Input parameters p1, . . . , p k
3:x1←Φ1(;p1, . . . , p k;x0)
4:x2←Φ2(x1;p1, . . . , p k;x0)
5:......
6:xN←ΦN(x1, . . . , x N−1;p1, . . . , p k;x0)
7:Output xN
Algorithm 5.3 is an explicit computation. The function ϕicomputes the value of
thevariable xi. The notationdΦi
dxjordΦi
dpjgives the partial derivatives of one step of the
algorithm. By contrast, the notationdxi
dpjgives the partial derivatives across multiple
steps of the algorithm. Algorithm 5.3 is the general case of Algorithm 3.1, thedΦi
dxj
anddΦi
dpjare general cases of what is seen in Algorithm 3.2, and thedxi
dpjgeneralize
what is seen in Algorithm 3.3.
We note that the adjoint method literature tends to consider a yet more general
implicit approach. Placing Section 3 of [9] in an explicit setting, we define a function
16fsuch that f(x, p) = 0. To this end, let
f(x, p) =x−Φ(x, p) :=
x1
x2
...
xN
−
Φ1(;p;x0)
Φ2(x1;p;x0)
...
ΦN(x1, . . . , x N−1;p;x0)
. (5.3)
Clearly, given p, the computed x= (x1, . . . , x N) from Algorithm 5.3 is a solution
tof(x, p) = 0. Our goal is to reproduce (3.8), which is the derivative of xNw.r.t. to
the parameter p.
Let us first consider the derivation for xp, which is the derivative of x, implicitly
defined by f(x, p) = 0, w.r.t. to p. To connect the viewpoints a table of notation for
various Jacobians is helpful:
Adjoint Method Nabla Notation Matrix Size
fx ∇xf I −˜L N ×N
fp ∇pf −MTN×k
xp ∇px (I−˜L)−1MTN×k
The matrices themselves are explicitly:
˜L=∂Φi
∂xj
i,j, i > j, j = 1, . . . , N −1,
and
MT=∂Φi
∂pj
i,j,∇px=∂xi
∂pj
i,j, i∈1, . . . , N, j ∈1, . . . , k.
The matrix ˜Lthat contains the partials ∂Φj/∂xjis strictly lower triangular ex-
actly because Algorithm 5.3 is an explicit computation, whereas an implicit function
would generally have a dense Jacobian. Since f(x, p) =x−Φ(x, p), the Jacobian
∇xf=I−˜L. Differentiating 0 = f(x, p) with respect to pwe get 0 = fxxp+fpor
xp=−f−1
xfpwhich is ( I−˜L)−1MTin matrix notation explaining the bottom row of
the above table.
Ifg(x) is any scalar function of x, then the key adjoint equation is
∇pg=gxxp=−gxf−1
xfp:=−λTfp,
where λsatisfies the so-called adjoint equation fT
xλ=gT
x. Since gxis an 1 by kvector,
by computing the adjoint λfirst, we reduce the computation of a matrix-matrix mul-
tiplication and a matrix-vector multiplication to two matrix-vector multiplications.
If we take g(x) =xNthen gx= [0, . . . , 0,1]. The gradient is then
∇pg(x) = [0 , . . . , 0,1](I−˜L)−1MT,
achieving our goal of reproducing (3.7).
17So much is happening here that it is worth repeating with other notation. We
can use the Jacobian of fwith respect to xandpto differentiate (5.3):
0 =
dx1
dx2
...
dxN
−
0 0 . . . 0
∂Φ2
∂x10 . . . 0
............
∂ΦN
∂x1. . .∂ΦN
∂xN−10

dx1
dx2
...
dxN
−
∂Φ1
∂p1. . .∂Φ1
∂pk.........
∂ΦN
∂p1. . .∂ΦN
∂pk

dp1
dp2
...
dpk
,
which can be solved to obtain

dx1
dx2
...
dxN
=
I−
0 0 . . . 0
∂Φ2
∂x10 . . . 0
............
∂ΦN
∂x1. . .∂ΦN
∂xN−10

−1
∂Φ1
∂p1. . .∂Φ1
∂pk.........
∂ΦN
∂p1. . .∂ΦN
∂pk

dp1
dp2
...
dpk
.
Some readers unfamiliar with the notation of differentials might prefer what
amounts to a notational change, but avoids the notation of differentials:

∂x1
∂p1. . .∂x1
∂pk.........
∂xN
∂p1. . .∂xN
∂pk
=
I−
0 0 . . . 0
∂Φ2
∂x10 . . . 0
............
∂ΦN
∂x1. . .∂ΦN
∂xN−10

−1
∂Φ1
∂p1. . .∂Φ1
∂pk.........
∂ΦN
∂p1. . .∂ΦN
∂pk
.
6. Julia, the power of language.
6.1. The challenge. This section provides a complete realization of the chal-
lenge described in the preface (Section 1). The question we asked is whether we could
bring to life the linear algebra mathematics expressed in
∇J=MT((I−L)T\g)
by typing the command
and computing the backpropagated gradient of a matrix neural network almost by
magic?
We remark that it is common to see code in papers. Code can serve the purpose
of specifying details, facilitating reproducibility, and verifiability. Code can also allow
users to adapt methods to their own situations. In addition to all of the above, we
have a further purpose. We believe the code example we provide shows the power,
elegance, and utility of the Julia programming language in ways that may be difficult
or impossible to imagine in other languages.
At the risk of showing the end of the code before the start, 63 lines of setup
culminate in exactly what we wanted: code which looks just like the math of ma-
trices with operators that correctly calculates the gradient fulfilling our title goal of
backpropagating through back substitution with a backslash:
18The first 28 lines elegantly set up the mathematics very much like a mathematician
defining operators and algebraic axioms:
Lines 10-14 above define matrix operators and their adjoints. Lines 16-28 define
various math operations, such as the negative operator on line 21, or the composition
of operators on line 25.
19For completeness we list lines 29 through 63 which constitute the setup of a basic
forward pass through a matrix neural net. We remark that lines 30 and 38 allow an
index origin of 0. The readers are encouraged to try the code at https://github.com/
alanedelman/BackBackBack.
6.2. Modern Computer Science meets Linear Algebra. The venerable po-
sition of numerical linear algebra libraries can not be undersold. Years of rigorous mathe-
matical and algorithmic research have culminated in the modern LAPACK library [1] which
represents a crowning achievement of value to a huge number of users who call LAPACK
perhaps from, for example, Julia, NumPy, or MATLAB. In most cases the users are unaware
of the scientific bedrock of which they are beneficiaries.
Continuing this grand tradition, we wish to highlight some of the computer science in-
novations that allow for the code in Section 6.1 to look so deceptively simple.
Generic Programming or how can the backslash just work? We invite the reader to consider
how the innocent backslash on line 75 of the code in Section 6.1 could possibly perform a
backpropogation of derivative. We believe this would be impossible in, for example, NumPy
or MATLAB as these packages currently exist. From a computer science point of view, Julia’s
multiple dispatch mechanism and generic programming features allow the generic backslash
to work with matrices and vectors whose elements are operators and compositions of opera-
tors. We remind the reader that the operators are not language constructs, but are created
in software on the first 28 lines of code. The backslash, however, is not LAPACK’s backslash,
as the LAPACK library is constrained to floating point real and complex numbers. Julia’s
backslash currently runs LAPACK when dispatched by matrices of floats, but, as is the case
here, the generic algorithm is called. We are fascinated by the fact that the author of the
20generic algorithm would not have imagined how it might be used. We are aware of back-
slash being run on quaternion matrices, block matrices, matrices over finite fields, and now
matrices with operators. Such is the mathematical power of abstraction and what becomes
possible if software is allowed to be generic. In the context of backpropagation, replacing
the “for loops” with the backslash helps us see backpropogation from a higher viewpoint.
The significance of transpose all the way down. Not without controversy, Julia implements
transpose recursively. We believe this is the preferred behavior. This means a block matrix of
block matrices of matrices (etc.) will transpose in the expected manner. Similarly matrices
of complex number or quaternions will perform conjugate transposes as expected. In this
work the Mas seen in Line 66 of the code in Section 6.1 is diagonal, but is not symmetric.
In line 75 we are transposing a diagonal matrix of 1 ×2 matrices of composed operators M’
while in that same line we are also transposing a bidiagonal matrix of operators. Because
the operator adjoint is defined on lines 10-14 of the code and the adjoint for a composed
operator is defined on line 25, Julia’s generic implementation, again, just works. We are not
aware of any other linear algebra system whereby the transpose would just work this readily.
The page https://discourse.julialang.org/t/why-is-transpose-recursive/2550 doc-
uments some of the controversy. We are extremely grateful that the recursive definition won
the day.
A quick word about performance. There is nothing in the backslash formulation that would
impede performance.
Possible extensions to the example code in Section 6.1. We deliberately only used as an
example the matrix neural network. We also have implemented a fully connected neural
network where the matrix I−Lis a Julia triangular type, whereas the reference example
was bidiagonal. We also implemented a square case where the Wparameter was constant
from one iteration to the next. We also conceived of the case of being restricted to a mani-
fold. We thus stress that we did not build a fully functional package at this time, and thus
emphasize that this could be future research, but we have not yet seen any roadblock to this
methodology.
Concluding Moral. Exciting new innovations in numerical algorithms are emerging from soft-
ware developments. Critical elements for creativity include: generic programming (generic
operators), abstract representations, fast performance without waste, multiple dispatch, and
an aggressive type system.
Abstraction matters. Software matters. Language matters.
7. Acknowledgments. We would like to thank the anonymous reviewers for their
valuable comments. We wish to thank David Sanders and Jeremy Kepner for helpful con-
versations. This material is based upon work supported by the National Science Foundation
under grant no. OAC-1835443, grant no. SII-2029670, grant no. ECCS-2029670, grant no.
OAC-2103804, and grant no. PHY-2021825. We also gratefully acknowledge the U.S. Agency
for International Development through Penn State for grant no. S002283-USAID. The in-
formation, data, or work presented herein was funded in part by the Advanced Research
Projects Agency-Energy (ARPA-E), U.S. Department of Energy, under Award Number DE-
AR0001211 and DE-AR0001222. This material is based upon work supported by the Defense
Advanced Research Projects Agency (DARPA) under Agreement No HR00112290091. We
also gratefully acknowledge the U.S. Agency for International Development through Penn
State for grant no. S002283-USAID. The views and opinions of authors expressed herein do
not necessarily state or reflect those of the United States Government or any agency thereof.
This material was supported by The Research Council of Norway and Equinor ASA through
Research Council project ”308817 - Digital wells for optimal production and drainage”. Re-
search was sponsored by the United States Air Force Research Laboratory and the United
States Air Force Artificial Intelligence Accelerator and was accomplished under Cooperative
21Agreement Number FA8750-19-2-1000. The views and conclusions contained in this doc-
ument are those of the authors and should not be interpreted as representing the official
policies, either expressed or implied, of the United States Air Force or the U.S. Government.
The U.S. Government is authorized to reproduce and distribute reprints for Government
purposes notwithstanding any copyright notation herein.
REFERENCES
[1]E. Anderson, Z. Bai, C. Bischof, L. S. Blackford, J. Demmel, J. Dongarra, J. Du Croz,
A. Greenbaum, S. Hammarling, A. McKenney, et al. ,LAPACK users’ guide , SIAM,
1999.
[2]A. G. Baydin, B. A. Pearlmutter, A. A. Radul, and J. M. Siskind ,Automatic differen-
tiation in machine learning: a survey , Journal of Machine Learning Research, 18 (2018),
pp. 1–43, http://jmlr.org/papers/v18/17-468.html.
[3]J. Bezanson, A. Edelman, S. Karpinski, and V. B. Shah ,Julia: A fresh approach to nu-
merical computing , SIAM review, 59 (2017), pp. 65–98.
[4]A. M. Bradley ,PDE-constrained optimization and the adjoint method , 2010, https://cs.
stanford.edu/ ∼ambrad/adjoint tutorial.pdf.
[5]I. Goodfellow, Y. Bengio, and A. Courville ,Deep Learning , MIT Press, 2016. http:
//www.deeplearningbook.org.
[6]A. Griewank ,A mathematical view of automatic differentiation , Acta Numerica, 12 (2003),
pp. 321–398.
[7]A. Griewank and A. Walther ,Evaluating derivatives: principles and techniques of algorith-
mic differentiation , SIAM, 2008.
[8]M. Innes, A. Edelman, K. Fischer, C. Rackauckus, E. Saba, V. Shah, and W. Teb-
butt ,∂P: A differentiable programming system to bridge machine learning and scientific
computing , 2019, http://arxiv.org/abs/1907.07587.
[9]S. G. Johnson ,Notes on adjoint methods for 18.335 , 2006, https://math.mit.edu/ ∼stevenj/
18.336/adjoint.pdf.
[10]J. Kepner and J. Gilbert ,Graph algorithms in the language of linear algebra , SIAM, 2011.
[11]T.-M. Li, M. Gharbi, A. Adams, F. Durand, and J. Ragan-Kelley ,Differentiable program-
ming for image processing and deep learning in halide , ACM Transactions on Graphics
(TOG), 37 (2018), pp. 1–13.
[12]J. Revels, T. Besard, V. Churavy, B. D. Sutter, and J. P. Vielma ,Dynamic automatic
differentiation of GPU broadcast kernels , 2018, https://arxiv.org/abs/arXiv:1810.08297.
[13]J. Revels, M. Lubin, and T. Papamarkou ,Forward-mode automatic differentiation in Julia ,
arXiv preprint arXiv:1607.07892, (2016).
[14]R. P. Stanley ,Catalan numbers , Cambridge University Press, 2015.
22