IEEE TRANSACTIONS ON SOFTWARE ENGINEERING 1
LTM: Scalable and Black-box Similarity-based
Test Suite Minimization based on Language
Models
Rongqi Pan, Taher A. Ghaleb, and Lionel C. Briand, Fellow, IEEE
Abstract —Test suites tend to grow when software evolves, making it often infeasible to execute all test cases with the allocated testing
budgets, especially for large software systems. Test suite minimization (TSM) is employed to improve the efficiency of software testing
by removing redundant test cases, thus reducing testing time and resources while maintaining the fault detection capability of the
test suite. Most existing TSM approaches rely on code coverage (white-box) or model-based features, which are not always available
to test engineers. Recent TSM approaches that rely only on test code (black-box) have been proposed, such as ATM and FAST -R.
The former yields higher fault detection rates ( FDR ) while the latter is faster. To address scalability while retaining a high FDR, we
propose LTM ( Language model-based Test suite Minimization), a novel, scalable, and black-box similarity-based TSM approach based
on large language models (LLMs), which is the first application of LLMs in the context of TSM. To support similarity measurement
using test method embeddings, we investigate five different pre-trained language models: CodeBERT, GraphCodeBERT, UniXcoder,
StarEncoder, and CodeLlama, on which we compute two similarity measures: Cosine Similarity and Euclidean Distance. Our goal is
to find similarity measures that are not only computationally more efficient but can also better guide a Genetic Algorithm (GA), which
is used to search for optimal minimized test suites, thus reducing the overall search time. Experimental results show that the best
configuration of LTM (UniXcoder/Cosine) outperforms ATM in three aspects: (a) achieving a slightly greater saving rate of testing time
(41.72% versus 41.02%, on average); (b) attaining a significantly higher fault detection rate ( 0.84versus 0.81, on average); and, most
importantly, (c) minimizing test suites nearly five times faster on average, with higher gains for larger test suites and systems, thus
achieving much higher scalability.
Index Terms —Test suite minimization, Test suite reduction, Pre-trained language models, Genetic algorithm, Black-box testing
✦
1 I NTRODUCTION
Software testing is an essential activity that ensures high-
quality software systems by detecting faults in software
releases [1]. When test suites are large, especially for large
code bases, the scalability of running all test cases in a
test suite quickly becomes a critical issue, as time and
resources are limited for testing in industrial contexts [2].
The problem is particularly acute as test suites tend to
increase in size over time with software evolution, making
it impossible to run all test cases for every code change
with allocated testing budgets [1]. To address this issue,
test suite minimization (TSM) is widely employed to make
the testing process more efficient while maintaining its fault
detection capability [1]. In contrast to test case selection [3],
•R. Pan is with the School of EECS, University of Ottawa, Ottawa,
Canada. E-mail: rpan099@uottawa.ca
•T. A. Ghaleb is with the Computer Science Department, Trent University,
Peterborough, Canada (Part of this work was done while the author was
with the University of Ottawa, Canada). E-mail: taherghaleb@trentu.ca
•L. C. Briand holds shared appointments with the Lero SFI Centre for
Software Research, University of Limerick, Ireland and the school of EECS,
University of Ottawa, Ottawa, Canada. E-mail: lbriand@uottawa.ca
This work was supported by a research grant from Huawei Technologies
Canada Co., Ltd, as well as by the Mitacs Accelerate Program, the Science
Foundation Ireland grant 13/RC/2094-2, and the Canada Research Chair and
Discovery Grant programs of the Natural Sciences and Engineering Research
Council of Canada (NSERC). The experiments conducted in this work were
enabled in part by support provided by the Digital Research Alliance of Canada
(https://alliancecan.ca).which selects a subset of test cases that are relevant to code
changes, and test case prioritization [3], which ranks test
cases based on certain characteristics, such as fault detection
capability, the objective of test suite minimization is to
eliminate redundant or similar test cases that are unlikely
to detect different faults [1, 3].
Though there exist various TSM approaches, most of
them rely on either code coverage information (white-
box) [3], which requires access to software production code,
or model-based features, such as transition coverage for
UML state machine-based testing [2]. This information is not
always accessible by test engineers and can be challenging
to collect [4, 5]. Further, collecting code coverage informa-
tion is expensive [6, 7] and can lead to scalability issues for
large software systems. Cruciani et al. [6] and Pan et al. [8]
addressed this issue by proposing TSM approaches that rely
only on the source code of test cases (black-box). FAST-R,
which is based on clustering algorithms, has been shown
to be much more efficient than white-box approaches, but
achieved relatively low fault detection rates. ATM, which
is based on test case similarity and evolutionary search,
achieved higher fault detection rates within practically ac-
ceptable, yet longer time. Since test suite minimization is
performed on major software releases [8, 9], as opposed to
test case selection and prioritization which are performed
for every code change, ATM offers a better trade-off com-
pared to FAST-R in many practical situations. However,
ATM still presents scalability issues for very large soft-arXiv:2304.01397v5  [cs.SE]  30 Sep 2024IEEE TRANSACTIONS ON SOFTWARE ENGINEERING 2
ware systems, since its total minimization time (the sum
of preparation time and search time) increases rapidly with
test suite size, represented by the number of test cases per
project version. Our analysis of the total minimization time
of ATM reveals that similarity measures play a major role
in limiting its scalability, due to the fact that (1) computing
tree-based similarity is expensive, taking up to 41.20% of the
total minimization time, and (2) similarity measures impact
the search convergence and speed. This motivated us to
investigate similarity measures that are both more efficient
to calculate and more informative to guide the search.
Language models pre-trained on programming lan-
guages convert test code into vector-based embeddings.
This enables vector-based similarity measurement, which
facilitates implementation optimizations and results in
much higher computational efficiency than tree-based sim-
ilarity measurement that relies on traversing AST trees.
Moreover, these language models are pre-trained on large
source code corpora with various code understanding and
generation tasks, thus generating embeddings that capture
informative syntactic and contextual details from test code.
This suggests that such embeddings with vector-based simi-
larity might be potentially more informative than tree-based
similarity in guiding evolutionary search. Therefore, in this
paper, we propose LTM ( Language model-based Test suite
Minimization), a scalable, black-box similarity-based TSM
approach that is based on pre-trained Large Language Mod-
els (LLMs) and vector-based similarity measures, making it
the first application of LLMs in the context of TSM, to the
best of our knowledge. LTM uses the source code of test
cases (Java test methods), without requiring any preprocess-
ing, as input to five alternative pre-trained language models,
namely CodeBERT [10], GraphCodeBERT [11], UniXcoder [12],
StarEncoder [13], and CodeLlama [14], in order to extract
test method embeddings. Considering that test suite diver-
sity was reported to have a positive correlation with fault
detection [2, 15, 16], LTM employs two similarity measures:
Cosine similarity and Euclidean distance , to calculate the sim-
ilarity between the extracted embeddings. Using the calcu-
lated similarity values, LTM employs a Genetic Algorithm
(GA) to minimize test suites, which searches for the most
optimal subset of a test suite, for a given testing budget. We
evaluated LTM with the same dataset used to evaluate ATM
(DEFECTS 4J), followed a similar experimental design, and
used the same evaluation metrics of effectiveness and effi-
ciency: Fault Detection Rate ( FDR ) and Minimization Time
(MT), respectively. In addition, considering the potential
variation in test case execution times, assessing minimized
test suites based on their number of test cases only might
not always be accurate [1]. Therefore, we extended our eval-
uation of minimized test suites using an additional metric:
Time Saving Rate ( TSR). Moreover, we optimized GA by
utilizing a more efficient data structure to accelerate fitness
calculation and enhance memory usage, which led to a 190-
fold reduction in minimization time, without requiring any
additional computation resources. Then, we identified the
best configuration of LTM by considering both effectiveness
and efficiency, among all its alternatives, and compared it
to the two best ATM configurations. Finally, we expanded
our experiments by running LTM on a much larger project
that ATM could not handle, to further assess the scalabilityof our approach.
Specifically, we address the following research questions.
•RQ1: How does LTM perform for test suite minimization
under different configurations? LTM achieves high FDR
results (an overall average FDR of0.79across con-
figurations) for a 50% minimization budget (i.e., the
percentage of test cases retained in the minimized test
suite). The best configuration of LTM is UniXcoder
using Cosine similarity when considering both effec-
tiveness ( 0.84FDR on average) and efficiency ( 0.82min
on average), which also achieves a greater time saving
rate (an average TSR of41.72% ). For the large project,
Closure , UniXcoder using Cosine Similarity takes only
17.80min in terms of MTand achieves an FDR of0.79,
while saving 52.55% of testing time.
•RQ2: How does LTM compare to ATM? The best config-
uration of LTM outperforms ATM by achieving sig-
nificantly better FDR (0.84versus 0.81, on average),
and more importantly, running much faster than ATM
(0.82min versus 4.06min, on average), in terms of
both preparation time (up to two orders of magnitude
faster) and search time (up to one order of magnitude
faster). The latter is particularly important on typically
large industrial systems and test suites where such
differences practically matter.
To summarize, the contributions of this work are as
follows:
•We propose a novel black-box TSM approach (LTM)
that relies, for the first time, on LLMs and two distance
functions based on the generated embeddings. We
investigate and conduct a comprehensive comparison
among five recent language models with various model
architectures, parameter sizes, inputs, and tasks used
for pre-training.
•We optimize the search process of LTM by utilizing
a more efficient data structure for fitness calculation,
which in turn reduced the search time by 190folds.
•We conduct a thorough comparative analysis of the effi-
ciency and effectiveness, as well as the achieved saving
in testing time, of black-box TSM approaches based
on a large-scale test suite minimization experiments
involving 17Java projects with 835 versions, thus
yielding valuable insights into the relative performance
of alternatives. Overall, the experiments took around
three months in calendar time corresponding to 6 years
of computation on a cluster with 83,216available CPU
cores.
•We analyze the minimization time of LTM and show
that it is much more scalable than the state-of-the-art
(SOTA) approaches by running five times faster on
average—with even higher gains for larger systems
and test suites typically encountered in practice—while
achieving significantly higher fault detection rates, as a
result of using LLMs for embeddings.
The rest of this paper is organized as follows. Section 2
reviews related work and motivates our research. Section 3
describes our test suite minimization approach. Section 4
validates our approach, reports the experimental design and
results, and discusses the implications in practice. Section 5IEEE TRANSACTIONS ON SOFTWARE ENGINEERING 3
discusses the validity threats to our results. Section 6 draws
conclusions and suggests future work.
2 R ELATED WORK
Test suite minimization (TSM) aims at improving the effi-
ciency of software testing by removing redundant test cases,
thus reducing testing time and resources while maintain-
ing the effectiveness of the test suite (i.e., fault detection
capability) [1]. There are various approaches that have been
proposed to address TSM, including (a) greedy heuristics-
based approaches [9, 17], which select test cases iteratively
based on code coverage information, (b) clustering-based
approaches [18, 19], which group test cases based on the
similarity of their coverage information, and (c) search-
based approaches [2, 20], which employ evolutionary search
to find an optimal subset of the test suite using model-
based features. Most of these approaches utilize code cover-
age information (white-box) [3] (i.e., requires access to the
system production code) or model-based features, which
are not always available to test engineers [5, 8]. Moreover,
collecting code coverage information can result in up to 30%
time overhead [6, 7], making it not scalable for large test
suites and systems.
Viggiato et al. [21] investigate the similarity of test cases
that are written in natural language. Though their objective
is to identify redundancy among test cases, as opposed to
TSM within a budget, their similarity measures can priori
be further used for this purpose. They convert test cases
into vector-based representations using five text embed-
ding techniques (i.e., Word2Vec [22], BERT [23], Sentence-
BERT [24], Universal Sentence Encoder [25], and TF-IDF
(Term Frequency–Inverse Document Frequency) [26]). They
then identify similar test cases utilizing clustering algo-
rithms to group similar test steps based on two different
similarity metrics (Word Mover’s Distance (WMD) [27] and
Cosine Similarity). The results show that their approach
achieves an F-Score of 83.47% for identifying similar test
cases. In summary, different from our work, their focus is on
identifying redundant test cases written in natural language,
as opposed to code, and they do not aim to optimize a test
suite within a budget.
Philip et al. [28] proposed a black-box approach, called
FastLane, which relies on test and commit logs containing
historical information, to predict test case outcomes (i.e.,
pass orfail) using logistic regression to skip their execution.
Using such information, FastLane reduces testing time and
resources by running only a subset of test cases instead of
the whole test suite. Results showed that FastLane reached
99.99% in terms of test outcome accuracy and saved up
to18.04% of testing time. However, it requires historical
information about multiple runs of test cases, thus making
FastLane inapplicable for new or recent test cases. Moreover,
FastLane cannot be easily adapted to given minimization
budgets (i.e., a predefined percentage of test cases to be
executed). Arrieta et al. [5] relied on test case inputs and
outputs for simulation-based testing and employed Non-
Dominated Sorting Genetic Algorithm II (NSGA-II) [29] to
find an optimal subset of test cases as a minimized test
suite. Chang et al. [30] detected redundancy in test caseswritten in natural language using word embedding tech-
niques and Cosine similarity. However, such information is
not accessible in many contexts, thus rendering such black-
box approaches not applicable.
Cruciani et al. [6] proposed a black-box approach, called
FAST-R, that relies solely on the source code of test cases.
FAST-R converts test code into vectors using a term fre-
quency model [31] and then employs a random projection
technique [32] to reduce the dimensionality of vectors.
Based on these vectors, clustering-based algorithms were
performed with the centroids of clusters selected as a min-
imized test suite. Results showed that, compared to white-
box approaches, FAST-R was much more efficient in terms
of total minimization time while achieving comparable ef-
fectiveness in terms of fault detection capability. However,
FAST-R achieved relatively lower fault detection capability
for Java projects, with median fault detection rates ranging
from 0.18to0.22for minimization budgets ranging from
1%to30% . With such low effectiveness, FAST-R is therefore
not a viable option in many practical contexts.
To achieve a better trade-off between the effectiveness
and efficiency of test suite minimization, Pan et al. [8] pro-
posed a black-box test suite minimization approach, called
ATM, which relies on test code similarity and evolution-
ary search. ATM preprocessed test code by removing the
information that is irrelevant to the testing rationale and
then converted it into Abstract Syntax Trees (ASTs). Then,
four different tree-based similarity measures (i.e., top-down,
bottom-up, combined, and tree edit distance) were used
to calculate similarity values between these ASTs. Finally,
evolutionary search (i.e., GA and NSGA-II) was employed
using the similarity values as fitness to find, for a given
test budget, an optimal subset of the test suite that contains
diverse test cases. Results showed that the best configura-
tion of ATM (i.e., GA with combined similarity) achieved a
better trade-off in terms of effectiveness and efficiency of test
suite minimization than FAST-R by achieving a significantly
higher average fault detection rate ( +19% ) while running
within practically acceptable time ( 1.2hours on average),
thus making it a better option in many contexts compared
to FAST-R.
However, ATM suffers from limitations regarding its
scalability for very large projects in the dataset, such as
Time , which consists of 30klines of code in its most recent
version with large test suites (nearly 4ktest cases per ver-
sion), as minimization took more than 10hours per project
version as compared to FAST-R ( 2.17seconds) and random
minimization ( 0.006seconds). The similarity measures used
by ATM had a direct impact on its scalability. First, the
similarity calculation time took up to 41.20% (using tree edit
distance) of the overall minimization time of ATM, which is
due to the fact that converting test code into ASTs and cal-
culating tree-based similarity based on ASTs are both time-
and resource-consuming. Second, the search time, which
was influenced by the employed similarity measures and
the number of test cases per version, increased rapidly with
the test suite size. Interestingly, though tree edit distance
took much longer to calculate than other ATM similarity
measures, it enabled GA search to converge faster towards
the termination criterion (a fitness improvement of less than
0.0025 across generations). Although GA with combinedIEEE TRANSACTIONS ON SOFTWARE ENGINEERING 4
similarity was identified as the best ATM configuration
based on the average minimization time across projects
(1.2hours ), it is important to note that it was 2.5hours
slower than GA with tree edit distance for the largest test
suite with nearly 4ktest cases. Therefore, when dealing
with large test suites, adopting a more informative similarity
measure that facilitates faster search convergence can result
in a notable reduction in minimization time, regardless of
the computational cost associated with the similarity calcu-
lation.
In contrast to the above TSM approaches, LTM relies
on test code without requiring any preprocessing, and em-
ploys large pre-trained language models (i.e., CodeBERT,
GraphCodeBERT, UniXcoder, StarEncoder, and CodeLlama)
and commonly used vector-based similarity measures (i.e.,
Cosine similarity and Euclidean distance) [33] to generate
more informative similarity values between test case pairs.
The goal is to both calculate similarity more efficiently and
better guide the search algorithm (GA) to converge faster to
a better fault detection capability.
3 LTM: L ANGUAGE MODEL -BASED TEST SUITE
MINIMIZATION
This section describes our approach for test suite mini-
mization, called LTM, relying on language models to com-
pute test code similarity and enable the use of evolution-
ary search for minimization. Our goal is to find a black-
box solution that achieves a better trade-off, in terms of
scalability and effectiveness, than the latest state-of-the-art
(SOTA) approach, namely ATM [8]. Despite the higher fault
detection rates achieved by ATM, it has limitations in terms
of scalability, which are due to the transformation of test
cases into ASTs and the time-consuming calculation of tree-
based similarity, making it difficult to apply for very large
software systems. Similar to ATM, LTM is also a similarity-
based approach operating under the assumption that there
exists a positive correlation between test suite diversity and
its fault detection capability [2]. We aim to find a similarity
measure that is not only more computationally efficient but
also provides better guidance for the search to converge
faster to a higher fault detection capability and thus result in
less search time. To achieve this, we employed pre-trained
language models to generate code embeddings and used
them as the basis to measure the vector-based similarity
between test case pairs. The computation of vector-based
similarity is much more efficient than tree-based similarity
requiring tree traversal. Moreover, language models have
the capability to capture patterns based on the syntax and
semantics of test case code without requiring preprocessing
or feature extraction, making them a more suitable alterna-
tive in our context.
Figure 1 outlines the main steps of LTM to minimize test
suites. Regarding the first two steps, we describe how we
tokenized the source code of test cases (Section 3.1.1) and
then extracted embeddings using language models (Sec-
tion 3.1.2). Then, we describe the algorithms (Cosine Sim-
ilarity and Euclidean Distance) we employed for measuring
the similarity between these embeddings (Section 3.2). Fi-
nally, we describe the search algorithm (Genetic Algorithm)we employed to minimize test suites using the similarity
measures as fitness.
3.1 Language Models for Test Method Representation
ATM used ASTs (represented in XML format) to preserve
the syntactic structure of test case code and calculated tree-
based similarity between the ASTs of test case pairs, which
we observed to be time- and resource-consuming due to
the need for tree traversal, thus leading to scalability issues.
In addition, we found that among all tree-based similarity
measures, ATM using tree edit distance achieved the highest
FDR and required the least search time across all project
versions, since it made the search algorithm converge faster
towards the termination criterion, that is fitness improve-
ment is less than 0.0025 across generations. This suggests
that, regardless of the computational cost associated with
its calculation, tree edit distance is a more informative
similarity measure as it can be more effective in guiding the
search to converge faster towards a higher fault detection
rate. Therefore, we aim to develop a solution that overcomes
the scalability issues of ATM by finding similarity measures
that (1) do not require test code preprocessing, (2) are com-
putationally efficient, and (3) are more informative, offering
better guidance to the search algorithm and accelerating its
convergence, thus reducing search time.
In this paper, we employ five pre-trained language mod-
els, namely CodeBERT [10], GraphCodeBERT [11], UniX-
coder [12], StarEncoder [13], and CodeLlama [14]. We select
these models for the following reasons:
•CodeBERT, GraphCodeBERT, and UniXcoder are open-
source publicly available language models that are
specifically designed for programming languages and
pre-trained on a large source code corpus. These lan-
guage models surpassed the performance of other
SOTA models in code clone detection tasks [10–12,
34], relying on the similarity between pairs of code
fragments [12], thus making them viable options for
similarity-based test suite minimization.
•Similar to CodeBERT and its follow-up models, StarEn-
coder is a language model designed for understanding
and analyzing code. Though it was not assessed for
code clone detection tasks, StarEncoder is more recent
and was pre-trained on a dataset encompassing more
diverse ( 86) programming languages, enabling it to
recognize test case patterns written in different coding
styles.
•Unlike the above models that are pre-trained for code
understanding tasks, CodeLlama is specialized for code
generation and infilling tasks (i.e., filling a missing
part of a code snippet). It has billions of parameters,
which is significantly higher than the parameter size
(125Million ) of the above models, and was pre-trained
on a larger dataset, making it potentially able to distin-
guish test code more precisely.
In our context, these language models take the source
code of test cases as input, without requiring any prepro-
cessing or transformation into other formats, and generate
embeddings that capture both semantic and contextual in-
formation from test case code. The output of these language
models is represented as numeric vectors, which offer theIEEE TRANSACTIONS ON SOFTWARE ENGINEERING 5
Test SuiteTokenize 
Test MethodsTest M ethod  
TokensExtract EmbeddingsTest Method 
EmbeddingsCodeBERT
GraphCodeBERT
UniX coder
StarEncoder
CodeLlama -7b
Calculate Similarity 
between Test Case PairsEvolutionary Search
(Genetic Algorithm)Similarity 
Scores
Cosine Similarity
Euclidean DistanceMinimized 
Test Suite
Legend
Input/Output Process Data Flow
Figure 1. Main steps of LTM to perform test suite minimization
opportunity to employ a variety of vector-based algorithms
to measure similarities between test cases. The process starts
by passing the source code of a test case (a Java test method
in our context) as input to the language models, which is
then converted to a list of tokens, each of which is assigned
an individual numerical vector representation. The final
output is a vector representation (embedding) generated
for the test method, which aggregates the information from
all individual vector representations, which is called a test
method embedding in our context.
3.1.1 Tokenizing Test Methods
Language models deal with a test code fragment (method)
as a sequence of tokens and split it using byte-pair-encoding
(BPE) [35], which is a subword segmentation algorithm
that splits words into a sequence of sub-words. This en-
ables language models to better handle out-of-vocabulary
words, such as method and variable names. For example,
the tokenizer splits the method name testCloning into ˙Gtest
(where ˙Gdenotes a space), Cland oning , since the testCloning
word does not exist in the vocabulary and is thus separated
into these three sub-words. The generated tokens are then
processed as follows.
•CodeBERT, GraphCodeBERT, and StarEncoder add two
special tokens, namely [CLS ]and[SEP ], to the be-
ginning and end of each sequence of tokens, respec-
tively. The CLS token is a special token that represents
the whole input sequence, whereas the [SEP ]token
is a separator token that denotes the end of the se-
quence [23]. In summary, each test method is repre-
sented as [CLS ], c1, c2, ..., c m,[SEP ], where cidenotes
theithcode token and mis the total number of code
tokens.
•For UniXcoder, besides the [CLS ]and[SEP ]tokens,
an additional special token ( [Enc],[Dec], or [E2D])
is added to the beginning of the input indicating
the pre-training mode of the model as encoder-only,
decoder-only, or encoder-decoder mode, respectively.
These modes differ in terms of model architectures
and training tasks during pre-training, thus supporting
various downstream tasks. We used the encoder-only
mode for producing contextualized code embeddings,
as decoder-related modes are used for code genera-
tion [12]. Therefore, the final input for UniXcoder is rep-
resented as [CLS ],[Enc],[SEP ], c1, c2, ..., c m,[SEP ].•CodeLlama uses <s> and</s> tokens denoting the
beginning and end of each token sequence, respec-
tively: ( <s>, c 1, c2, ..., c m,</s> ).
During pre-training, each token is then mapped to a 768-
dimensional vector representation that contains the seman-
tic and contextual information of this token for all language
models above, except for CodeLlama where the vector size
is4,096. We set the token length to 512 for all language
models during the tokenization process.
3.1.2 Generating Test Method Embeddings
A test method embedding is a numerical vector representa-
tion of a test method that captures semantic and contextual
information from the source code. It is based on how and
on what data a model was pre-trained, as follows.
•CodeBERT pre-trained code representations are based
on a large corpus called CodeSearchNet [36], which
contains both natural language (e.g., code comments)
and source code across six programming languages. Its
follow-up models (GraphCodeBERT and UniXcoder)
leveraged data flow information during pre-training,
which captures relationships between variables in the
input code fragments and ASTs of the source code,
respectively, to enhance the code representation.
•StarEncoder was pre-trained on a large dataset encom-
passing 86 programming languages collected from The
Stack [37], which is specifically crawled from GitHub
repositories for pre-training code LLMs.
•CodeLlama was pre-trained on a massive dataset
(500Billion tokens) containing both source code and
natural language. CodeLlama has multiple variations
and model sizes, depending on the type of training
data and parameter size. In this paper, considering the
importance of scalability, we used the smallest version
of CodeLlama (i.e., CodeLlama-7b), which has seven
billion parameters.
In terms of model architecture:
•CodeBERT and StarEncoder employ a multi-layer bidi-
rectional self-attentive Transformer [38] as model ar-
chitecture to help the model capture contextual and
positional information for each token from the entire
input sequence.
•GraphCodeBERT extends such architecture using a
graph-guided masked attention function to help the
model encode graph-based data flow information.IEEE TRANSACTIONS ON SOFTWARE ENGINEERING 6
•UniXcoder, on the other hand, is based on a multi-layer
Transformer with a prefix denoting the pre-training
mode of the model. The model architecture for the
encoder-only mode, which is employed by LTM, allows
the model to learn, for each token, the contextual infor-
mation from the entire input sequence.
•CodeLlama uses an optimized auto-regressive trans-
former, enabling the model to efficiently predict miss-
ing parts within code sequences. Despite being a
decoder-only model, CodeLlama exhibits the capabil-
ity to understand the test code context and generate
missing parts accurately.
During pre-training:
•CodeBERT employs Masked Language Modeling
(MLM) [23], which allows the model to predict the
masked tokens from the input sequence and whether a
token was randomly replaced in a given sequence [39].
These two tasks help the model generate code embed-
dings that contain more accurate contextual informa-
tion, accounting for the position of the tokens in the
source code input.
•GraphCodeBERT also employs MLM with two addi-
tional tasks: Edge Prediction [11] and Node Align-
ment [11]. Edge Prediction allows the model to pre-
dict which edges, which denote dependencies between
variables, have been masked for a given variable in the
data flow graph. Node Alignment allows the model to
predict which code token is related to a given variable
in the data flow graph. These two tasks further enhance
the code representation using data flow information.
•UniXcoder also uses the MLM task for the encoder-
only mode, and further learns the test method embed-
ding, which is first generated by taking the average
(i.e., mean pooling) of all token embeddings and then
further trained by utilizing multi-modal contrastive
learning [40] and cross-modal generation [12], which
both leverage AST and code comments to further
enhance test method embeddings. Multi-modal con-
trastive learning generates different embeddings for the
same input but with different dropout masks, which al-
lows the model to predict the original embedding using
the other generated embeddings. This task helps the
model to better distinguish between different method
embeddings and thus can better deal with downstream
tasks, such as test method similarity analysis [40].
Cross-modal generation generates code comments for
the input test method. This helps the model to fuse the
semantic information from the code comments, which
describe the function of the code, to the test method
embeddings.
•StarEncoder also employs MLM in addition to Next
Sentence Prediction (NSP) [23], which helps the model
learn the relationship between test code segments by
predicting whether segments follow other segments.
•Unlike the above models, CodeLlama employs the code
infilling task, which allows the model to generate the
missing part of a code snippet while comprehending
the entire context. This enables the model to under-
stand the logical structure of the test code and the
relationships between test code segments.For LTM using CodeBERT, GraphCodeBERT, and
StarEncoder, we use the embedding that corresponds to
the[CLS ]token as a test method embedding, since it
aggregates the information from all tokens of that method.
For LTM using UniXcoder, we use the pre-trained test
method embedding corresponding to each test method. For
LTM using CodeLlama, we rely on the mean pooling of the
last hidden states (i.e., the average of all token embeddings
extracted from the last hidden layer of the model), which
is also an effective strategy that aggregates the information
from all code tokens [41].
3.2 Similarity Measurement of Test Method Embed-
dings
LTM employs two similarity measures for calculating
the similarity between test method embeddings: Cosine
Similarity and Euclidean Distance. They measure the
similarity between test method embeddings from different
aspects: Cosine similarity measures the angle between
two vectors, whereas Euclidean distance calculates the
straight-line distance between them.
Cosine similarity. This is a measure of similarity between
two vectors based on the cosine of the angle between
them [42], which is the dot product of the vectors divided
by the product of their lengths, and is calculated as follows:
Cosine Similarity =T1·T2
∥T1∥∥T2∥(1)
where T1andT2denote the embeddings of test case T1
andT2, respectively.
The value of Cosine similarity ranges from −1to1. The
higher the value of Cosine similarity, the more similar the
two test cases are. In order to bound the value of Cosine
similarity between 0and1, we normalized it as follows:
Norm. Cosine Sim. = 1−arccosT1·T2
∥T1∥∥T2∥
π(2)
Euclidean distance. This is a measure of similarity between
two vectors based on the square root of the sum of squared
differences between corresponding elements of the two vec-
tors [42], which is calculated as follows:
Euclidean Distance = (mX
i(t1i−t2i)2)1/2(3)
where t1iandt2iis the ithelement of embedding T1and
T2, respectively. mis the total number of elements in each
embedding ( 4,096 for CodeLlama and 768 for the other
models).
The value of Euclidean Distance ranges from 0to∞. The
higher the value of Euclidean distance, the less similar the
two test cases. In order to bound the Euclidean distance
between 0and 1, and obtain a similarity measure, we
normalized the distance as follows [43]:
Norm. Euclidean Dist. =1
1 + (Pm
i(t1i−t2i)2)1/2(4)IEEE TRANSACTIONS ON SOFTWARE ENGINEERING 7
We used the ‘ pdist ’ function1from the Scipy Python
library, which performs pairwise calculations for large
datasets. The ‘ pdist ’ employs highly optimized C code to
improve the efficiency of similarity calculation on vector-
based data, and stores the output in a condensed matrix,
which further reduces the required memory resources.
3.3 Search-based Test Suite Minimization
Given that test suite minimization is an NP-hard prob-
lem [2], it can be addressed efficiently using meta-heuristic
search algorithms [8] to find feasible, near-optimal solutions,
a minimized test suite containing diverse test cases for a
given budget in our context. Like ATM, we relied on a
Genetic Algorithm (GA) since it has shown, in the context
of TSM, to achieve a better trade-off between effectiveness
and efficiency than its multi-objective alternative, namely
Non-Dominated Sorting Genetic Algorithm II (NSGA-II) [8].
The optimization problem addressed by the GA is defined
as a fixed-size subset selection problem [44]. Each solution
(chromosome) is a subset and is represented as a binary
vector where 1denotes the selection of the test case and
0otherwise. The vector length equals the total number
of test cases in the test suite before minimization. Given
specified minimization budgets (25%, 50%, and 75%), the
percentage of selected test cases in each solution is set to
equal to the budget. For the fitness function, we used the
summation of the maximum squared similarity values (i.e.,
Normalized Cosine Similarity and Normalized Euclidean
Distance), shown below:
Fitness =P
i,ti∈Mn(Max i,j,ti,tj∈Mn,i̸=jSim(ti, tj))2
n(5)
where Mnis a minimized test suite of size n, and Sim(ti, tj)
is the normalized similarity score of test cases pair tiandtj.
To select hyperparameter values, we followed the same
published guidelines2as ATM for GA search, by using
a population size of 100, a mutation rate of 0.01, and
a crossover rate of 0.90. The GA process maximizes the
diversity of the subset by iteratively evolving the population
using crossover and mutation operators while evaluating
each subset using the predefined fitness function. For each
generation, the size of each solution (subset) remains fixed
and equal to the minimization budget. This process is re-
peated until the fitness value improvement across genera-
tions is less than 0.0025 .
In terms of implementation, LTM optimizes GA search
by changing the data structure of the input (i.e., similarity
values) to (1) accelerate the fitness calculations and (2) en-
hance memory usage. The data structure employed by ATM
for storing similarity values is a data frame3, which has
a complex indexing and labeling mechanism, thus making
fitness calculation computationally expensive. Therefore, we
use matrices4instead, since they facilitate faster numer-
ical calculations due to the fact that (1) it is a simpler
1https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.
distance.pdist.html
2https://www.obitko.com/tutorials/genetic-algorithms/
recommendations.php
3https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html
4https://numpy.org/doc/stable/reference/generated/numpy.array.html
Sim TC1 TC2 TC3
TC1 0 0.8 0.3
TC2 0 0 0.6
TC3 0 0 0Figure 2. An example of how similarity values of pairs of test cases are
represented using a matrix. Values on and below the diagonal are set to
0 as they are either useless or duplicates.
data structure without complex indexing and labeling and
(2) the matrix operations are implemented using highly
optimized C code that ensures efficient computation and
memory management. As shown in Figure 2, we stored the
similarity values for all pairs of test cases in a matrix format
without any duplicate and useless values (the similarity
values on and below the diagonal of the matrix were set
to 0). To save memory and further accelerate calculations,
we employed a sparse matrix format5, which exclusively
preserves the value and position (indicating for which pair
of test cases a given similarity value represents) of non-
zero similarity values. Note that we did not parallelize the
fitness calculation as it significantly increases the memory
required by the matrix, which must contain similarity values
for all considered subsets, making it less scalable. Although
fitness calculations can be parallelized in other ways, such
as multiprocessing, which utilizes multiple CPUs to par-
allelize fitness calculation, or distributed computing [44],
which distributes the fitness calculation to various devices
or nodes, they require additional computational resources,
which contradicts our goal of improving scalability.
4 V ALIDATION
This section reports on the experiments we conducted to
assess the effectiveness and efficiency of LTM.
4.1 Research Questions
RQ1: How does LTM perform for test suite minimization
under different configurations? Test suite minimization aims
to reduce the number of test cases in a test suite by re-
moving redundant test cases while maintaining its fault
detection power, thus achieving higher testing efficiency
and effectiveness. In particular, the performance of LTM can
be influenced by (a) the language models used for extracting
test method embeddings and (b) the distance functions used
for measuring the similarity between test method embed-
dings. In this RQ, we assess the performance, in terms of
effectiveness and efficiency, of LTM under different con-
figurations, each with a different combination of language
model and distance function. We evaluated LTM using three
minimization budgets (i.e., 25% ,50% , and 75% ).
•RQ1.1: How effectively can LTM minimize test
suites? The language models used for LTM (i.e., Code-
BERT, GraphCodeBERT, UniXcoder, StarEncoder, and
5https://docs.scipy.org/doc/scipy/reference/sparse.htmlIEEE TRANSACTIONS ON SOFTWARE ENGINEERING 8
CodeLlama-7b) vary in terms of architectures and train-
ing data, thus generating embeddings that capture dif-
ferent information about test case code. The distance
functions we used to measure the similarity or distance
between test method embeddings differently, which
can lead to different comparison results. Therefore, the
combination of language models and distance func-
tions can affect the ability of LTM to remove redundant
test cases, which can in turn affect its fault detection
capability. In this RQ, we assess the effectiveness of
LTM in terms of fault detection capability under dif-
ferent configurations. In addition, we calculated the
execution time of minimized test suites to quantify
the achieved time savings resulting from test suite
minimization.
•RQ1.2: How efficiently can LTM minimize test suites?
Test suite minimization should run within practical
time. The use of different combinations of language
models and distance functions can impact the prepa-
ration time, and the resulting similarity values can
guide the GA search to varying degrees of efficiency,
thus resulting in different search time. In this RQ, we
assess the efficiency of LTM in terms of preparation
time (taken for test code tokenization, embeddings
extraction, and similarity calculation) and search time
(taken for GA search).
RQ2: How does LTM compare to ATM? ATM was devel-
oped as a black-box test suite minimization approach that
achieved a better trade-off, in terms of effectiveness and
efficiency, than the SOTA approach (FAST-R [6]). However,
the minimization time of ATM increased rapidly with test
suite size, making it less scalable for large projects with large
test suites. For example, ATM required more than 10hours,
in terms of total minimization time, to minimize a large
project in the dataset used for its evaluation, thus indicating
scalability issues. In this RQ, we assess the performance
of LTM compared to the two best configurations of ATM,
specifically ATM using GA with combined and tree edit
distance similarity measures, in terms of both effectiveness
and efficiency.
•RQ2.1: How effectively can LTM minimize test suites
compared to ATM? We aim to find an informative
similarity measure that can better capture similarities
between test cases, thus achieving higher FDR . In this
RQ, we compare the best configuration of LTM to
the two best configurations of ATM in terms of fault
detection capability.
•RQ2.2: How efficiently can LTM minimize test suites
compared to ATM? To achieve better scalability com-
pared to ATM, we aim to reduce (a) preparation time,
by investigating a similarity measure that is more effi-
cient to calculate, and (b) search time, by investigating
a more informative similarity measure that can better
guide the search, thus resulting in faster convergence.
In this RQ, we compare the efficiency of LTM to the
two best configurations of ATM in terms of both prepa-
ration time and search time. Further, to better assess
scalability for both LTM and ATM, we fit a regression
model to investigate how minimization time (the sumof preparation time and search time) increases with the
size of a test suite.
4.2 Experimental Design and Dataset
We conducted experiments to evaluate the performance of
alternative LTM configurations, each with a different combi-
nation of language model and similarity measure, resulting
in ten configurations in total (five different language models
and two similarity measures), using the same experimental
design and dataset as ATM. We conducted our experiments
on a cluster of 1,340nodes with 83,216available CPU cores,
each with a 2xAMD Rome 7532 with 2.40 GHz CPU, 256M
cache L3, 249GB RAM, running CentOS 7. We considered
each Java project version as an independent subject and
conducted our experiments on all Java project versions ( 835
in total) in the dataset to increase the number of instances for
experimental evaluation and, therefore, enhance the sound-
ness of our conclusions. Each TSM approach was performed
on every project version and ran 10times to account for
randomness and draw statistically valid conclusions, using
three minimization budgets ( 25% ,50% , and 75% ), which is
the percentage of test cases that retained in the minimized
test suite). There is a total of 25,050 runs ( 835 Java project
versions ×10runs per version ×3minimization budgets)
for each configuration. Overall, the experiments, including
similarity calculations, GA search, as well as executing all
test suites 10times, took more than three months in calendar
time corresponding to 6 years of computation. We iden-
tified the best configuration of LTM considering both the
effectiveness and efficiency and compared it to the two best
configurations of ATM. Our data, scripts, and raw results
can be found in our replication package [45]
4.2.1 Baseline approach (ATM)
We compared LTM to ATM [8], a similarity-based, search-
based test suite minimization approach that relies solely on
the source code of test cases. ATM preprocesses test code
by removing information that is irrelevant to testing logic
from the test code, then converts test code into ASTs. ATM
calculates four different tree-based similarity measurements
between test case ASTs: top-down, bottom-up, combined
and tree edit distance similarity. Note that ATM does not
use the full code (i.e., the source code of test cases without
preprocessing) as it would make the already-long prepara-
tion phase of ATM much longer and impractical in realistic
settings. This is due to the fact that (1) the XML files used
by ATM to store Abstract Syntax Trees (ASTs) of test cases
would require substantial storage space and (2) it would
take longer time to traverse the ASTs of test cases when
calculating similarity, thus yielding much longer prepara-
tion time for transforming the test case code into ASTs and
calculating similarity values. As a result, ATM’s preparation
time can take up to 41.2% of the total minimization time,
and using the full code in ATM would make matters much
worse.
After the preprocessing step, search algorithms (i.e., GA
and NSGA-II) were then employed based on similarity
values to find an optimal subset of test cases by removing
redundant test cases. Using GA with combined similarity,
ATM achieved a better trade-off in terms of effectivenessIEEE TRANSACTIONS ON SOFTWARE ENGINEERING 9
and efficiency compared to a SOTA test suite minimization
approach, FAST-R [6]. However, it suffered from scalability
issues as, for example, it took more than 10hours to mini-
mize 4ktest cases for the largest test suite. This motivated
us to compare LTM to ATM, in terms of fault detection
capability and, more importantly, minimization time. We
aim to assess whether LTM can be more scalable than ATM
while achieving comparable or even higher fault detection
capability. We compared LTM to the two best configurations
of ATM, using GA with combined similarity (ATM/Com-
bined) and tree edit distance (ATM/TreeEditDistance). The
former was the best configuration considering the trade-
off between effectiveness and efficiency, whereas the latter
achieved higher fault detection capability but was more
time-consuming than the former in terms of total mini-
mization time. We further evaluated the achieved saving
in testing time for every minimized test suite for both LTM
and ATM. We relied on the publicly available replication
package of ATM6provided by its authors.
4.2.2 Dataset
To facilitate comparisons, we ran our experiments using
the same dataset as ATM to evaluate the effectiveness
and, more importantly, the scalability of LTM. The dataset
consists of 16Java projects collected from D EFECTS 4J, a
well-known dataset that offers realand reproducible faults
extracted from real-world, open-source Java projects to sup-
port software testing research. There is no publicly avail-
able industrial system or open source system that contains
information to automatically trace test failures to system
faults, a key requirement for our experiments. To further
validate the performance of LTM, we included an additional
large project from Defects4J, called Closure , which was not
part of the dataset used to evaluate ATM due to scalability
issues. Each project has numerous faulty versions, each of
which contains a single real fault that is present in the
production code and leads to the failures of one or more
test cases. It is worth noting that there is currently no
publicly available dataset that offers multiple realfaults per
version, as automatically establishing a clear link between
faults and test case failures would be challenging. Table1
presents the characteristics of all 17projects. Overall, the
project sizes in terms of KLoc range from 2to179 KLoc,
with the corresponding tests sizes ranging from 4to253
KLoc. The number of faulty versions ranges from 4to174
across projects, with the average number of test cases per
version ranging from 152 to7,308 across projects. Project
sizes and tests sizes (i.e., lines of code) were extracted by
analyzing the most recent version of each project using the
CLOC7tool. In summary, we evaluated LTM using a total of
835versions from 17projects. However, given the scalability
limitations of ATM [8], we compared our results to ATM
using only the 661 versions from the original 16projects,
excluding the Closure project.
4.2.3 Evaluation metrics
We used the same evaluation metrics as ATM, specifically
fault detection rate and total minimization time, in addition
6https://doi.org/10.5281/zenodo.7455766
7https://github.com/AlDanial/clocTABLE 1
Summary of the 17Java projects
ProjectProject Size # of versions Average # of test cases Tests Size
(KLoC) (faults) (methods) per version (KLoC)
Chart 92 26 1,817 41
Cli 2 39 256 4
Codec 9 18 413 15
Collections 30 4 1,040 38
Compress 45 47 404 29
Csv 2 16 193 7
Gson 9 18 984 20
JacksonCore 31 26 356 45
JacksonDatabind 74 112 1,814 72
JacksonXml 6 6 152 10
Jsoup 14 93 494 13
JxPath 20 22 250 6
Lang 30 64 1,796 61
Math 71 106 2,078 73
Mockito 21 38 1,182 36
Time 30 26 3,918 56
Closure 179 174 7,308 253
to the achieved saving in testing time of minimized test
suites.
Fault Detection Rate (FDR). We use FDR to assess the
effectiveness of LTM compared to ATM. Given that each
project version contains a single fault, the corresponding
fault detection rate per version can either be 1(fault de-
tected) or 0(fault undetected). Therefore, we calculate the
fault detection rate for each project by considering all its
versions, as follows:
FDR =Pm
i=1fi
m(6)
where mdenotes the total number of versions of the project.
For a project version i,fiis equal to 1if at least one failing
test case is included in the minimized test suite, indicating
the detection of the fault of that version, or 0otherwise.
For example, in the Chart project, which has a total of
26faulty versions, if the minimized test suites detected the
faults in 21 versions but did not in 5versions, FDR would
be calculated as 21/26, resulting in an FDR of0.81. Note
that we ran our experiments for each version a total of 10
times to mitigate randomness. Hence, the final FDR for each
project is obtained by taking the average FDR results across
the10runs.
Total Minimization Time (MT). We assess the efficiency
of LTM compared to ATM by computing (1) the prepa-
ration time , which includes the time required for loading
the language models, tokenizing test case code, extracting
test method embeddings, and calculating similarity values
between all pairs of test cases, and (2) the search time , which
is the time required for running the GA.
Time Saving Rate (TSR). We use TSR to quantify the
achieved testing time savings resulting from test suite mini-
mization. It is important to note that the reduction in test
execution time may not be directly proportional to the
reduction in the test suite size, as the time required to
execute individual test methods may vary [1]. Using the
DEFECTS 4J infrastructure, we ran the test suite for each
project version ( 835versions in total) 10times and collected
the average execution time for each test method in a clean,
stable environment, a cluster with many available CPU
cores. This ensures that each run runs independently and
is not impacted by external interferences and fluctuations in
memory or CPU performance. Then, we calculate the TSR
for each minimized test suite as follows:IEEE TRANSACTIONS ON SOFTWARE ENGINEERING 10
TSR = (1−test execution time after TSM
test execution time before TSM)∗100 (7)
Fisher’s exact test. We use Fisher’s exact test [46], a non-
parametric statistical hypothesis test, to assess whether the
difference in proportions of detected faults between the
alternative LTM and ATM configurations is significant.
4.3 Results
In this section, given space constraints, we only report
results for a minimization budget of 50% (the percentage of
test cases retained in the minimized test suite) as trends and
conclusions are similar for other budgets (25% and 75%),
which are available in our replication package [45].
4.3.1 Using full test code versus preprocessed test code in
LTM
One important question is whether the source code of test
cases requires preprocessing, such as removing test case
names, logging statements, comments, and test assertions.
Though such preprocessing was performed by ATM [8],
due to necessity (Section 4.2.1), this information may still
convey relevant information about test case similarity, thus
retaining them can potentially improve fault detection capa-
bility. This information can also help language models bet-
ter learn about semantic and contextual information about
test cases and thus generate more informative test method
embeddings. For example, code comment ‘Tests the equals
method’ describes the functionality of the test code, thus
providing potentially important semantic information [12].
Therefore, we conducted a preliminary study to compare the
FDR results of LTM using full code (i.e., the source code of
test cases without preprocessing) and preprocessed code on
a sample of 10smaller projects from the D EFECTS 4J dataset.
We observed that LTM achieved higher FDR results using
full code than using prepossessed code ( +0.02on average)
across all configurations, thus making the former a better
choice for our experiments. We summarize the results and
provide further details in our replication package [45].
4.3.2 Improvement using optimized GA.
As discussed in Section 3.3, we optimized GA to accelerate
the search process. Our results revealed that, compared to
MT before optimization, the search time of ATM/Com-
bined and ATM/TreeEditDistance is 224.66and 203.02
times faster per project version, respectively. For LTM, the
search time is 190.48times faster per project version, across
all configurations, which is a significant improvement in
scalability. Note that the following comparisons between
LTM and ATM are based on applying this optimized GA
to both approaches.
4.3.3 RQ1 results
Tables 2, 3, and 4 provide the results of LTM in terms of FDR ,
MT(in min), and TSR (in percentage), respectively, when us-
ing CodeBERT, GraphCodeBERT, UniXcoder, StarEncoder,
and CodeLlama-7b with Cosine and Euclidean similarity
measures, for a 50% minimization budget.RQ1.1 results. Table 2 shows that all LTM configura-
tions achieved a high FDR , ranging from 0.65to0.95
across projects, with both the mean and median rang-
ing from 0.75to0.84for all configurations, for a 50%
minimization budget. The highest overall average FDR
was achieved using UniXcoder/Cosine (mean = 0.84, me-
dian = 0.82), ranging from 0.75to0.95across projects.
However, LTM using CodeBERT/Euclidean also yielded a
high FDR (mean = 0.82, median = 0.84). CodeBERT/Eu-
clidean and UniXcoder/Cosine showed standard deviations
of0.06and0.07, respectively. In addition, Fisher’s exact test
results reveal that LTM using UniXcoder/Cosine achieved
significantly better FDR results than all other configurations,
with α= 0.01.
Figure 3 depicts the average FDR for all projects across
generations, for all configurations of LTM. Compared to
other configurations, LTM using UniXcoder/Cosine con-
verges faster to a higher FDR (0.84across projects), suggest-
ing that it offers better guidance for the GA search, making
it more effective at removing redundant test cases and thus
leading to a better fault detection rate.
We observed that, for each language model, there seems
to be a significant difference in FDR between using Co-
sine Similarity and Euclidean Distance, which is further
confirmed by Fisher’s exact test with α= 0.01. This sug-
gests that FDR results are influenced by the combination of
language models and distance functions. The embeddings
generated by UniXcoder and GraphCodeBERT are enriched
with detailed information about the nodes and edges within
test code ASTs. This could be the reason that Cosine Simi-
larity, as a directional similarity, is more informative in dis-
tinguishing such test method embeddings than Euclidean
Distance, a magnitude similarity. For CodeLlama-7b, the
embedding dimensionality is 4,096, compared to 768 for
other language models, which can explain its better perfor-
mance with Cosine Similarity over Euclidean Distance, since
the latter is less effective in high-dimensional spaces [47].
In certain projects ( Cli,Codec ,Csv, and Time ), UniX-
coder/Cosine yielded a 5%to10% lower FDR than other
configurations. However, Fisher’s Exact Test revealed that
those differences are not significant. Nevertheless, we pro-
vide some tentative explanations regarding these projects:
(1) A larger training data set covering a greater variety of
programming languages (StarEncoder) and a substantially
greater number of parameters (CodeLlama) might be bene-
ficial for understanding test code from these projects, and
(2) using ASTs as input for pre-training might introduce
unnecessary details into test method embeddings, if test
cases in these projects are similar in terms of code structure
but rather differ regarding low-level details (e.g., complex
string manipulation or domain-specific character strings),
thus making UniXcoder less effective in understanding their
test code.
In addition, we observed that UniXcoder/Cosine
achieved a higher average FDR than CodeLlama-7b/Cosine
(mean = 0.80, median = 0.81) and CodeLlama-7b/Euclidean
(mean = 0.76, median = 0.76). This suggests that language
models with fewer parameters ( 125Million for UniXcoder)
can outperform much larger language models ( 7Billion for
CodeLlama-7b) for our minimization problem. This can be
attributed to the fact that CodeLlama-7b, like most large lan-IEEE TRANSACTIONS ON SOFTWARE ENGINEERING 11
guage models, is a decoder-only model that was pre-trained
for code generation tasks, while UniXcoder employs code
understanding tasks that are more effective in analyzing and
comprehending the contextual semantic information from
the source code [48].
Recall that the GA terminates when the fitness value
improves by less than 0.0025 , thus resulting in different
numbers of generations for each configuration across project
versions. The overall average number of generations of all
configurations across versions is 47.73, with UniXcoder/Co-
sine having a higher average ( 63.10generations). However,
if a fixed number of generations had been used, say 40
generations, UniXcoder/Cosine would still yield the highest
FDR (0.81) among all configurations. This clearly makes
UniXcoder/Cosine the best LTM alternative, in terms of
FDR .
/uni00000013 /uni00000015/uni00000013 /uni00000017/uni00000013 /uni00000019/uni00000013 /uni0000001b/uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000015/uni00000013
/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056/uni00000013/uni00000011/uni00000019/uni00000013/uni00000013/uni00000011/uni00000019/uni00000018/uni00000013/uni00000011/uni0000001a/uni00000013/uni00000013/uni00000011/uni0000001a/uni00000018/uni00000013/uni00000011/uni0000001b/uni00000013/uni00000013/uni00000011/uni0000001b/uni00000018/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000029/uni00000044/uni00000058/uni0000004f/uni00000057/uni00000003/uni00000027/uni00000048/uni00000057/uni00000048/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000035/uni00000044/uni00000057/uni00000048/uni00000024/uni00000037/uni00000030/uni00000012/uni00000026/uni00000052/uni00000050/uni00000045/uni0000004c/uni00000051/uni00000048/uni00000047
/uni00000024/uni00000037/uni00000030/uni00000012/uni00000037/uni00000055/uni00000048/uni00000048/uni00000003/uni00000028/uni00000047/uni0000004c/uni00000057/uni00000003/uni00000027/uni0000004c/uni00000056/uni00000057/uni00000044/uni00000051/uni00000046/uni00000048
/uni0000002f/uni00000037/uni00000030/uni0000001d/uni00000026/uni00000052/uni00000047/uni00000048/uni00000025/uni00000028/uni00000035/uni00000037/uni00000012/uni00000026/uni00000052/uni00000056/uni0000004c/uni00000051/uni00000048
/uni0000002f/uni00000037/uni00000030/uni0000001d/uni00000026/uni00000052/uni00000047/uni00000048/uni00000025/uni00000028/uni00000035/uni00000037/uni00000012/uni00000028/uni00000058/uni00000046/uni0000004f/uni0000004c/uni00000047/uni00000048/uni00000044/uni00000051
/uni0000002f/uni00000037/uni00000030/uni0000001d/uni0000002a/uni00000055/uni00000044/uni00000053/uni0000004b/uni00000026/uni00000052/uni00000047/uni00000048/uni00000025/uni00000028/uni00000035/uni00000037/uni00000012/uni00000026/uni00000052/uni00000056/uni0000004c/uni00000051/uni00000048
/uni0000002f/uni00000037/uni00000030/uni0000001d/uni0000002a/uni00000055/uni00000044/uni00000053/uni0000004b/uni00000026/uni00000052/uni00000047/uni00000048/uni00000025/uni00000028/uni00000035/uni00000037/uni00000012/uni00000028/uni00000058/uni00000046/uni0000004f/uni0000004c/uni00000047/uni00000048/uni00000044/uni00000051
/uni0000002f/uni00000037/uni00000030/uni0000001d/uni00000038/uni00000051/uni0000004c/uni0000003b/uni00000046/uni00000052/uni00000047/uni00000048/uni00000055/uni00000012/uni00000026/uni00000052/uni00000056/uni0000004c/uni00000051/uni00000048
/uni0000002f/uni00000037/uni00000030/uni0000001d/uni00000038/uni00000051/uni0000004c/uni0000003b/uni00000046/uni00000052/uni00000047/uni00000048/uni00000055/uni00000012/uni00000028/uni00000058/uni00000046/uni0000004f/uni0000004c/uni00000047/uni00000048/uni00000044/uni00000051
/uni0000002f/uni00000037/uni00000030/uni0000001d/uni00000026/uni00000052/uni00000047/uni00000048/uni0000004f/uni0000004f/uni00000044/uni00000050/uni00000044/uni00000010/uni0000001a/uni00000045/uni00000012/uni00000026/uni00000052/uni00000056/uni0000004c/uni00000051/uni00000048
/uni0000002f/uni00000037/uni00000030/uni0000001d/uni00000026/uni00000052/uni00000047/uni00000048/uni0000004f/uni0000004f/uni00000044/uni00000050/uni00000044/uni00000010/uni0000001a/uni00000045/uni00000012/uni00000028/uni00000058/uni00000046/uni0000004f/uni0000004c/uni00000047/uni00000048/uni00000044/uni00000051
/uni0000002f/uni00000037/uni00000030/uni0000001d/uni00000036/uni00000057/uni00000044/uni00000055/uni00000028/uni00000051/uni00000046/uni00000052/uni00000047/uni00000048/uni00000055/uni00000012/uni00000026/uni00000052/uni00000056/uni0000004c/uni00000051/uni00000048
/uni0000002f/uni00000037/uni00000030/uni0000001d/uni00000036/uni00000057/uni00000044/uni00000055/uni00000028/uni00000051/uni00000046/uni00000052/uni00000047/uni00000048/uni00000055/uni00000012/uni00000028/uni00000058/uni00000046/uni0000004f/uni0000004c/uni00000047/uni00000048/uni00000044/uni00000051
Figure 3. FDR across projects for each generation of LTM and ATM
Achieved Saving in Testing Time. Table 4 shows that, for
the50% minimization budget, LTM achieved an average
TSR ranging from 41.15% to43.37% , across configura-
tions. The average TSR achieved by UniXcoder/Cosine was
41.72% . This indicates that, reducing the number of test
cases by 50% results in a 41.72% reduction in testing time.
RQ1.2 results. Table 3 shows that the average MT of all
LTM configurations ranges from 0.61to14.74min per
project version. The MT of LTM using UniXcoder/Co-
sine, which achieved the highest average FDR (0.84), also
appears to be efficient with mean = 0 .82min and
median = 0.35min .
Preparation Time. The average preparation time ranges
from 0.26to17.71min per project version, across config-
urations. We observed that the preparation time of UniX-
coder/Cosine is 65.43times shorter than CodeLlama-7b.
This is because (1) larger language models require much
longer model loading time ( 12.07min for CodeLlama-7b,
compared to UniXcoder for 0.04min, on average) and sig-
nificantly larger memory ( 13.10GB for CodeLlama-7b com-
pared to 504MB for UniXcoder) and (2) the embeddings
generation time of larger language models is significantly
longer ( 5.15min for CodeLlama-7b compared to 0.16min
for UniXcoder, on average) as they have more complex
architectures and a larger number of parameters, making
the generation of embeddings more time-consuming.Search Time. The average search time of LTM ranges
from 0.46to1.90min per project version, across configu-
rations. LTM using CodeBERT with Cosine similarity was
the fastest in terms of search time, ranging from 0.04to
3.86min, across project versions. Not only LTM using UniX-
coder/Cosine took a shorter search time ( 0.78min) than the
average search time across configurations ( 1.03min), it also
achieved the highest FDR among all configurations, thus
making it the best option for LTM.
Results for the Closure project. We further evaluated
LTM on the Closure project, which has 174 versions with
an average of 7,308 test cases per version. Though this
project is part of the D EFECTS 4J dataset, it was previously
excluded from the ATM evaluation due to its scalability lim-
itations [8] and the large size of our experiments. However,
evaluating the effectiveness and efficiency of LTM on this
project, considering its larger scale and test suite, can be
informative. Table 5 presents the results regarding FDR ,MT
(in min), and TSR (in percentage), respectively. We can see
that UniXcoder/Cosine achieves once again the highest FDR
of0.79and an MT of only 17.80min , while saving 52.55%
testing time, which indicates that LTM can be effective and
scalable for larger projects and test suites.
RQ1 summary. LTM achieved high FDR results ( 0.79, on
average, across configurations) for a 50% minimization
budget. UniXcoder/Cosine is the best LTM configura-
tion when considering both effectiveness ( 0.84FDR on
average) and efficiency ( 0.82min on average), with an
average TSR of41.72% .
4.3.4 RQ2 results
RQ2.1 results. For a 50% minimization budget, Tables 2,
3, and 4 report FDR ,MT, and TSR, respectively, for the
best configuration of LTM (i.e., UniXcoder/Cosine) and the
two best configurations of the baseline approach: ATM with
combined similarity and tree edit distance similarity.
We observe that LTM significantly outperforms ATM/-
Combined, the best ATM alternative, and ATM/TreeEdit-
Distance in terms of FDR , with α= 0.01. Compared to
ATM, LTM achieves higher average FDR (0.84) with lower
standard deviation ( 0.06compared to 0.10for ATM/Com-
bined and 0.08for ATM/TreeEditDistance).
Figure 3 shows that UniXcoder/Cosine converges faster
to a higher FDR (0.84across projects), compared to ATM/-
Combined ( 0.80across projects) and ATM/TreeEditDistance
(0.81across projects). This may be explained by the fact
that (1) UniXcoder can handle full code (i.e., the source
code of test cases without preprocessing), whereas ATM
uses preprocessed code; and (2) UniXcoder was pre-trained
on various code understanding tasks, enabling the model
to learn contextual information for each token and the
meaning of each code element (e.g., method, variable names,
and string characters) in the input test code. In contrast,
ATM relies on tree-based similarity based on ASTs, which
lack information regarding the syntactic context and certain
details of the test code.
For some projects, ATM achieved higher FDR than LTM.
However, a Fisher’s Exact Test suggests that, for theseIEEE TRANSACTIONS ON SOFTWARE ENGINEERING 12
TABLE 2
Results and descriptive statistics of FDR of LTM and ATM across projects for the 50% minimization budget. The highest FDR results are
highlighted in bold.
ProjectApproach CodeBERT GraphCodeBERT UniXcoder StarEncoder CodeLlama-7b UniXcoder-p ATM ATM
Cosine Euclidean Cosine Euclidean Cosine Euclidean Cosine Euclidean Cosine Euclidean Cosine Combined Tree Edit Distance
Chart 0.82 0.84 0.79 0.75 0.82 0.69 0.71 0.79 0.72 0.73 0.77 0.88 0.89
Cli 0.81 0.81 0.83 0.86 0.82 0.81 0.83 0.86 0.87 0.86 0.80 0.87 0.87
Codec 0.89 0.90 0.83 0.88 0.80 0.81 0.82 0.83 0.87 0.78 0.78 0.77 0.80
Collections 0.70 0.82 0.95 0.75 0.95 0.70 0.70 0.75 0.78 0.70 1.00 0.98 0.92
Compress 0.85 0.86 0.87 0.87 0.86 0.81 0.78 0.84 0.85 0.79 0.89 0.88 0.89
Csv 0.86 0.87 0.88 0.91 0.85 0.79 0.84 0.86 0.93 0.88 0.88 0.92 0.91
Gson 0.70 0.71 0.72 0.75 0.81 0.75 0.76 0.76 0.81 0.77 0.76 0.76 0.77
JacksonCore 0.70 0.73 0.73 0.73 0.75 0.67 0.74 0.74 0.75 0.75 0.78 0.69 0.72
JacksonDatabind 0.66 0.69 0.68 0.67 0.76 0.65 0.65 0.69 0.67 0.65 0.74 0.69 0.70
JacksonXml 0.77 0.85 0.83 0.73 0.83 0.82 0.67 0.65 0.83 0.85 0.87 0.60 0.73
Jsoup 0.75 0.74 0.77 0.76 0.80 0.76 0.73 0.75 0.77 0.75 0.76 0.70 0.69
JxPath 0.88 0.90 0.92 0.86 0.94 0.83 0.72 0.77 0.78 0.75 0.94 0.79 0.79
Lang 0.81 0.85 0.86 0.75 0.90 0.71 0.77 0.85 0.83 0.71 0.84 0.78 0.83
Math 0.74 0.78 0.75 0.74 0.78 0.65 0.71 0.78 0.75 0.67 0.77 0.81 0.82
Mockito 0.80 0.83 0.84 0.81 0.91 0.79 0.77 0.83 0.80 0.77 0.78 0.77 0.79
Time 0.83 0.88 0.85 0.82 0.82 0.77 0.82 0.79 0.81 0.76 0.86 0.89 0.88
Statistics
Min 0.66 0.69 0.68 0.67 0.75 0.65 0.65 0.65 0.67 0.65 0.74 0.60 0.69
25% Quantile 0.73 0.77 0.77 0.75 0.80 0.70 0.71 0.75 0.77 0.73 0.77 0.75 0.76
Mean 0.79 0.82 0.82 0.79 0.84 0.75 0.75 0.78 0.80 0.76 0.83 0.80 0.81
Median 0.81 0.84 0.83 0.76 0.82 0.77 0.75 0.79 0.81 0.76 0.79 0.79 0.81
75% Quantile 0.84 0.86 0.86 0.86 0.87 0.81 0.79 0.83 0.84 0.78 0.87 0.88 0.88
Max 0.89 0.90 0.95 0.91 0.95 0.83 0.84 0.86 0.93 0.88 1.00 0.98 0.92
TABLE 3
Results and descriptive statistics of MT(in min) of LTM and ATM across projects for the 50% minimization budget. The shortest MTresults are
highlighted in bold.
ProjectApproach CodeBERT GraphCodeBERT UniXcoder StarEncoder CodeLlama-7b UniXcoder-p ATM ATM
Cosine Euclidean Cosine Euclidean Cosine Euclidean Cosine Euclidean Cosine Euclidean Cosine Combined Tree Edit Distance
Chart 0.92 1.51 0.97 1.20 1.33 0.99 2.24 2.68 21.02 20.92 2.18 7.86 94.22
Cli 0.11 0.16 0.14 0.15 0.17 0.14 0.16 0.22 4.39 4.38 0.22 0.23 1.20
Codec 0.15 0.21 0.18 0.17 0.22 0.18 0.25 0.33 5.75 5.65 0.31 0.42 3.02
Collections 0.37 0.51 0.43 0.50 0.58 0.38 0.79 1.11 8.96 8.80 0.87 2.16 14.89
Compress 0.16 0.20 0.19 0.18 0.22 0.17 0.25 0.34 9.26 9.18 0.32 0.55 6.55
Csv 0.10 0.13 0.12 0.11 0.15 0.11 0.14 0.16 4.19 4.15 0.19 0.16 0.35
Gson 0.31 0.39 0.31 0.33 0.47 0.32 0.68 0.79 8.31 8.26 0.77 1.48 6.09
JacksonCore 0.14 0.16 0.16 0.15 0.19 0.15 0.22 0.25 11.10 11.06 0.27 0.35 2.13
JacksonDatabind 0.92 1.40 0.91 1.35 1.43 1.33 2.15 2.46 34.67 35.41 2.50 4.90 22.69
JacksonXml 0.11 0.11 0.11 0.10 0.13 0.11 0.12 0.13 6.86 6.84 0.18 0.13 0.34
Jsoup 0.17 0.17 0.17 0.17 0.22 0.18 0.29 0.29 20.92 20.91 0.33 0.55 1.63
JxPath 0.11 0.15 0.13 0.13 0.17 0.13 0.17 0.21 4.98 4.95 0.23 0.22 0.73
Lang 0.91 1.47 0.91 1.08 1.38 0.97 2.35 2.82 15.09 14.98 2.53 5.44 33.47
Math 1.33 2.33 1.31 2.06 2.12 1.98 3.41 4.22 21.12 23.09 3.69 12.05 215.43
Mockito 0.40 0.50 0.42 0.40 0.59 0.41 0.97 1.12 17.40 17.93 1.15 2.32 9.34
Time 3.49 5.35 3.43 3.91 3.73 3.44 9.06 10.45 41.75 39.60 7.61 26.13 163.18
Statistics
Min 0.10 0.11 0.11 0.10 0.13 0.11 0.12 0.13 4.19 4.15 0.18 0.13 0.34
25% Quantile 0.13 0.16 0.16 0.15 0.19 0.15 0.21 0.24 6.58 6.54 0.26 0.32 1.52
Mean 0.61 0.92 0.68 0.75 0.82 0.69 1.45 1.72 14.74 14.72 1.46 4.06 35.95
Median 0.24 0.30 0.25 0.26 0.35 0.25 0.49 0.57 10.18 10.12 0.55 1.02 6.32
75% Quantile 0.91 1.42 0.91 1.11 1.34 0.98 2.17 2.52 20.95 20.91 2.26 5.04 25.39
Max 3.49 5.35 3.43 3.91 3.73 3.44 9.06 10.45 41.75 39.60 7.61 26.13 215.43
TABLE 4
Results and descriptive statistics of TSR (in percentage) of LTM and ATM across projects for the 50% minimization budget. The highest TSR
results are highlighted in bold.
ProjectApproach CodeBERT GraphCodeBERT UniXcoder StarEncoder CodeLlama-7b UniXcoder-p ATM ATM
Cosine Euclidean Cosine Euclidean Cosine Euclidean Cosine Euclidean Cosine Euclidean Cosine Combined Tree Edit Distance
Chart 55.45% 57.35% 58.78% 59.81% 58.03% 56.11% 58.47% 63.08% 56.64% 59.14% 55.27% 59.77% 57.53%
Cli 29.90% 27.90% 30.54% 29.75% 28.32% 28.17% 29.52% 27.52% 28.25% 28.46% 29.95% 28.07% 27.34%
Codec 44.11% 40.68% 47.59% 49.93% 55.58% 50.62% 51.05% 48.38% 45.27% 48.83% 45.84% 34.62% 45.94%
Collections 57.40% 55.05% 56.21% 56.10% 55.41% 59.05% 59.41% 57.36% 55.01% 56.78% 56.37% 53.57% 57.65%
Compress 37.74% 37.98% 36.56% 38.10% 39.10% 40.70% 37.87% 35.26% 36.45% 40.11% 36.33% 33.85% 32.59%
Csv 18.66% 18.78% 19.90% 20.07% 18.13% 10.11% 16.40% 16.15% 12.82% 9.37% 6.92% 20.45% 19.89%
Gson 41.81% 40.60% 42.16% 42.01% 41.40% 44.63% 41.85% 40.10% 41.69% 43.63% 39.51% 40.35% 38.17%
JacksonCore 47.45% 48.44% 44.37% 46.18% 47.10% 48.00% 46.02% 46.42% 46.95% 46.98% 45.22% 43.59% 46.71%
JacksonDatabind 42.54% 41.28% 41.68% 41.63% 38.32% 42.37% 41.71% 39.56% 41.29% 41.75% 39.98% 50.61% 47.92%
JacksonXml 42.81% 42.05% 42.63% 44.12% 45.80% 46.32% 45.70% 49.52% 44.03% 43.60% 44.48% 39.17% 39.34%
Jsoup 42.96% 42.03% 41.33% 43.80% 33.90% 41.65% 38.60% 35.61% 35.12% 37.55% 32.55% 40.39% 38.20%
JxPath 43.37% 42.82% 41.00% 41.90% 40.83% 42.70% 42.46% 41.84% 44.12% 44.16% 41.04% 44.40% 44.46%
Lang 38.36% 36.49% 40.14% 41.24% 34.05% 44.76% 41.04% 35.37% 42.83% 44.97% 32.19% 37.71% 38.06%
Math 50.24% 49.43% 50.26% 49.47% 49.67% 49.67% 50.00% 49.72% 50.90% 51.34% 45.44% 47.93% 49.10%
Mockito 27.65% 29.87% 34.33% 38.38% 33.97% 39.93% 44.82% 41.80% 32.13% 39.77% 24.12% 24.31% 26.13%
Time 48.12% 47.57% 48.33% 48.49% 47.89% 49.06% 47.35% 46.94% 46.84% 48.38% 46.17% 45.93% 47.35%
Statistics
Min 18.66% 18.78% 19.90% 20.07% 18.13% 10.11% 16.40% 16.15% 12.82% 9.37% 6.92% 20.45% 19.89%
25% Quantile 38.21% 37.61% 39.25% 40.53% 34.03% 41.41% 40.43% 35.55% 36.12% 40.03% 32.46% 34.43% 36.69%
Mean 41.79% 41.15% 42.24% 43.19% 41.72% 43.37% 43.27% 42.16% 41.27% 42.80% 38.84% 40.30% 41.02%
Median 42.89% 41.66% 41.92% 42.91% 41.12% 44.70% 43.64% 41.82% 43.43% 43.90% 40.51% 40.37% 41.90%
75% Quantile 47.62% 47.79% 47.78% 48.74% 48.34% 49.21% 48.01% 48.67% 46.87% 48.49% 45.54% 46.43% 47.49%
Max 57.40% 57.35% 58.78% 59.81% 58.03% 59.05% 59.41% 63.08% 56.64% 59.14% 56.37% 59.77% 57.65%IEEE TRANSACTIONS ON SOFTWARE ENGINEERING 13
TABLE 5
Results of FDR (in percentage), MT(in min) and TSR (in percentage) of LTM for the Closure project and a 50% minimization budget. The highest
FDR andTSR as well as the shortest MTare highlighted in bold.
MetricApproach CodeBERT GraphCodeBERT UniXcoder StarEncoder CodeLlama-7b
Cosine Euclidean Cosine Euclidean Cosine Euclidean Cosine Euclidean Cosine Euclidean
FDR 0.77 0.77 0.78 0.78 0.79 0.74 0.73 0.76 0.77 0.76
MT 15.02 30.53 19.73 25.98 17.80 27.61 16.45 34.17 54.87 66.84
TSR 53.18% 51.98% 53.75% 52.53% 52.55% 54.27% 54.40% 52.51% 53.81% 53.70%
projects, there is no significant difference between the FDR
results of ATM and LTM. Nevertheless, a possible explana-
tion for such performance difference could be that the per-
centage of test code exceeding the token limits of language
models on these projects (5.72%, 3.21%, 3.25%, and 7.25% for
Chart ,Compress ,Collections , and Math , respectively) is
higher than the average of 2.98% across projects, and that the
truncated parts are important for distinguishing test cases.
Another possible explanation could be due to variability
across test cases in some projects (e.g., Cli andTime ),
regarding their low-level code elements such as complex
string manipulation and domain-specific character strings,
rather than structural information. This makes tree-based
similarity better at capturing these differences, by compar-
ing AST node labels, than using the Cosine Similarity of
embeddings.
Note that the way ATM preprocessed test cases, by re-
moving all the comments and assertions, may also influence
its performance. For example, for the Codec project, 26.18%
of the test methods contain comments, which is higher than
the average percentage across projects (14.85%). Such com-
ments may convey rich information about test code, such as
test case descriptions and expected results. Therefore, due to
such loss of information in code comments, ATM may not
efficiently achieve higher FDR for this project.
The average number of generations required by
LTM was 63.10across project versions (maximum = 100),
which is lower than ATM/Combined (mean = 85.60and
maximum = 130) and ATM/TreeEditDistance (mean = 74.88
and maximum = 115). This indicates that LTM converges
faster across generations than ATM, in terms of fitness value,
which is the sum of the maximum squared similarity values
for all test case pairs per version. Related to this, if due to
time constraints a fixed number of generations is used as
the termination criterion for search, say 40generations, it
is clear based on Figure 3 that UniXcoder/Cosine would
yield the highest FDR . This indicates that LTM offers better
guidance for the GA search, making it more effective at
removing redundant test cases while maintaining a higher
fault detection rate, thus explaining the results in Table 2
and why LTM is a better option than ATM for test suite
minimization in practice.
Achieved Saving in Testing Time. The average time
saving rate of the best configuration of LTM ranges from
18.13% to58.03% across projects, with a mean of 41.72%
across projects. We observe that this time saving rate is
slightly higher than, but not significantly different from, that
of ATM/Combined ( 40.30% ) and ATM/TreeEditDistance
(41.02% ), as shown by the Wilcoxon signed-rank test.
Comparing LTM and ATM using preprocessed code. As
discussed in Section 4.2.1, ATM cannot handle full code
due to scalability issues. However, the preprocessing stepmight remove useful information from the test code and
thus impact the effectiveness of ATM. Therefore, in order
to explain the LTM FDR results when compared to ATM,
we want to determine how the use of full code and the
LTM test method embeddings individually affect FDR . For
this, we conducted an additional experiment to assess LTM
with UniXcoder/Cosine, the best LTM configuration, using
preprocessed code, on all 16projects. Tables 2 and 4 report
theFDR and TSR results, respectively, for this configuration
using preprocessed code (UniXcoder-p) and the two best
configurations of ATM (using GA with combined and tree
edit distance similarity measures), for a 50% minimization
budget. We observe that, using the same input as ATM
(preprocessed test code), LTM with UniXcoder/Cosine still
yields a higher average FDR (0.83) than ATM ( 0.81). Still, it
is lower than what we achieved using LTM with full code
(0.84). The average TSR result for LTM with preprocessed
code is however slightly lower than that of ATM ( 38.84%
versus 41.02% ), though the difference is not statistically
significant, as resulted by the Wilcoxon signed-rank test.
These results therefore suggest that the small improvements
inFDR observed with LTM are due to both LTM’s test
method embeddings and its capacity to handle full code
in a scalable way. TSR remains similar for LTM and ATM,
with or without preprocessing. We report the results for
LTM with UniXcoder/Cosine using preprocessed code on
all16projects in our replication package [45].
RQ2.2 results.
Figure 4 depicts MT, preparation time, and search time—
the first one including the last two—as a function of the
number of test cases per version, for the 661 versions of
the16projects. This is done for the best configuration of
LTM (i.e., UniXcoder/Cosine) and compared to the best
configurations of ATM (ATM/Combined and ATM/TreeEd-
itDistance) for the 50% minimization budget.
We observe that LTM ran much faster than ATM in terms
ofMT(79.80% and97.72% faster than ATM/Combined and
ATM/TreeEditDistance on average, respectively) with both
much less preparation time ( 92.70% and99.35% less than
ATM/Combined and ATM/TreeEditDistance on average,
respectively) and search time ( 35.16% and7.81% less than
ATM/Combined and ATM/TreeEditDistance on average,
respectively). In addition, we observe that the observations
in Figure 4 follow a quadratic relationship rather than a
linear one. Therefore, a quadratic regression model is fitted
for the preparation time, search time, and MT as a function
of the number of test cases per version, respectively, to
quantify and compare relationships across approaches.
The equation of the quadratic regression model is as
follows.
Time =a∗n2+b∗n+c (8)IEEE TRANSACTIONS ON SOFTWARE ENGINEERING 14
1000 2000 3000 4000
Number of T est Cases0125250375500T otal Minimization Time
LTM
ATM/Combined
ATM/Tree Edit
Distance
1000 2000 3000 4000
Number of T est Cases0125250375500Preparation Time
LTM
ATM/Combined
ATM/Tree Edit
Distance
1000 2000 3000 4000
Number of T est Cases02468Search Time
LTM
ATM/Combined
ATM/Tree Edit
Distance
Figure 4. Scatter plots of the number of test cases and MT, preparation time, and search time (in min), for LTM (UniXcoder/Cosine) and ATM, across
all the 661project versions for the 50% minimization budget
where ndenotes the number of test cases per project ver-
sion, aandbare regression coefficients, and cdenotes the
intercept of the quadratic regression model.
The results of quadratic regression confirmed that the
MT and the number of test cases, for both LTM and ATM,
follow a quadratic relationship as the coefficients on the
quadratic term were statistically significant with α= 0.01.
This is, as expected, due to the fact that (1) similarity
values are measured between test case pairs, which in-
creases quadratically with the number of test cases, and
(2) the search space for GA increases rapidly with the
number of test cases. The R2values for the regression model
of LTM and ATM/Combined were 98.89% and 89.51% ,
respectively. This indicates that a high percentage of the
variation in MT can be explained by the number of test
cases. However, the R2value for ATM/TreeEditDistance
was only 50.36% , indicating that a much smaller proportion
of the variation in MT can be explained by the number of
test cases in this case. This is because the preparation time
of ATM/TreeEditDistance, which is part of its MT, was also
largely influenced by AST sizes. More discussions will be
provided in the following section.
Moreover, a comparison of regression coefficients in-
dicates that the MT of LTM increases up to two or-
ders of magnitude slower than ATM with the squared
number of test cases per project version: 8.255e−06for
LTM versus 9.314e−05and4.446e−04for ATM/Combined
and ATM/TreeEditDistance, respectively. For large industry
projects with large test suites, LTM is thus much more
scalable in terms of MT, and therefore a better option than
ATM in practice.
Preparation Time. We found that LTM is much more
efficient, in terms of preparation time across project ver-
sions (mean = 0.23min and median = 0.14min), com-
pared to ATM/Combined (mean = 3.15min and me-
dian = 0.74min) and ATM/TreeEditDistance (mean = 35.31
min and median = 6.10min). Moreover, the prepara-
tion time of LTM takes only 35.75% of the MT per ver-sion, which is much less than ATM/Combined ( 71.22% )
and ATM/TreeEditDistance ( 93.74% ). Quadratic regres-
sion shows that the coefficient on the quadratic term for
LTM ( 1.241e−06) is one order of magnitude smaller than
the coefficients for ATM/Combined ( 6.634e−05) and two
orders of magnitude smaller than ATM/TreeEditDistance
(4.3680e−04), with all coefficients statistically significant
with α= 0.01. This indicates that, on larger industry
projects with much larger test suites, the absolute calcu-
lation time difference between LTM and ATM is expected
to be extremely large, thus making LTM a better option
regarding scalability in preparation time.
TheR2values for the regression model of LTM and
ATM/Combined are 97.70% and87.86% , respectively. This
indicates that more than 87% of the variation in preparation
time can be explained by the number of test cases. However,
theR2value for ATM/TreeEditDistance is 49.96% , which is
mainly due to its preparation time as it is highly influenced
by test code length. For instance, as shown in Figure 4,
there are project versions with nearly 4ktest cases from the
Time project, which have less preparation time than project
versions with around 3ktest case from the Math project.
This is due to the fact that the average test code for the Math
project is longer than that for the Time project, resulting in
its AST tree size being twice as long. Therefore, although the
test suite size of the Math project is smaller, its preparation
time is significantly longer than that of the Time project. In
contrast, the preparation time for LTM is not affected by the
length of the test code, which further supports our claims
regarding its scalability.
Search Time. We observe that the time LTM takes for
searching the optimal test suite (mean = 0.59min and
median = 0.21min across projects) is also shorter than that
for ATM/Combined (mean = 0.91min and median = 0.29
min) and ATM/TreeEditDistance (mean = 0.64min and
median = 0.23).
Moreover, quadratic regression reveals that, while
quadratically increasing with the number of test cases perIEEE TRANSACTIONS ON SOFTWARE ENGINEERING 15
version, the search time of LTM increases more slowly with
the squared number of test cases per version (a coefficient on
the quadratic term equal to 7.013e−06) than ATM/TreeEd-
itDistance ( 7.844e−06) and one order of magnitude more
slowly than ATM/Combined ( 2.680e−05), with more than
79% (R2) of the variation in MT being explained by the
number of test cases.
Note that the search time not only depends on the
number of test cases per version but is also impacted by
the number of generations required by the GA to converge.
For instance, as shown in Figure 4, though the number
of test cases per version for the Math project ( 2,078) is
less than that for the Time project ( 3,918), GA needed
more generations to converge for Math , thus resulting in
longer search time. Test cases of the Math project are more
diverse than that of the Time project, with a higher standard
deviation of similarity values ( 0.13compared to 0.09), thus
leading to more generations and longer convergence time
for the GA search.
In conclusion, in addition to being more efficient than
ATM in terms of preparation time, the similarity measure
employed by LTM provides better guidance and thus helps
the search algorithm converge faster than ATM, thus result-
ing in less search time.
RQ2 summary. LTM outperforms ATM by achieving
significantly higher FDR (0.84versus 0.81, on average).
However, the main benefit of LTM is that it is running
much faster than ATM ( 0.82min versus 4.06min, on
average), in terms of both preparation time (up to two
orders of magnitude faster) and search time (up to one
order of magnitude faster). Given the acute scalability
issues met in industrial contexts regarding minimization
for large test suites and systems, this result is of high
practical importance.
4.4 Discussion
4.4.1 Scalable test suite minimization with much less
preparation time than the SOTA.
Our results show that LTM runs up to two orders of mag-
nitude faster than the SOTA approach (i.e., ATM) while
achieving a statistically significantly higher fault detection
rate. We should note that, according to the average MT
across projects, LTM runs five times faster than ATM/-
Combined and 44times faster than ATM/TreeEditDistance.
These differences tend to increase significantly as the test
suite size grows. For example, for the Math project with
2078 test cases, LTM runs 6and 102 times faster than
ATM/Combined and ATM/TreeEditDistance, respectively.
This suggests that LTM, while achieving higher effective-
ness, can save enormous time and resources for minimizing
test suites of the much larger software systems and test
suites encountered in practice. The preparation phase plays
a crucial role in the scalability of LTM. Since we employed
pre-trained language models to extract vector-based test
method embeddings and used a highly optimized func-
tion to compute similarity between them, we managed to
achieve much shorter preparation times compared to ATM.
In addition, since we used the test code as input withoutpreprocessing it and converting it into other formats, such
as ASTs for ATM, we saved substantial memory space as
well.
Unlike coverage-based techniques, the size of a project
(i.e., source lines of code) has no impact on the minimization
time of black-box TSM approaches, including LTM and
ATM, as minimization time mostly depends on the number
of test cases per project version. In addition, we optimize
the search process by changing the data structure for input
similarity values and accelerating the fitness calculation,
which reduced the search time by 190.48times. The Closure
project—the largest by far, comprising 179 KLoC and 174
versions, with 7,308test cases per version—took only 17.80
min in terms of MT and achieved a FDR of0.79, for a
50% minimization budget. Note that for ATM, Closure was
far too large to even run our experiments, thus further
illustrating the scalability gains with LTM.
Though LTM uses the same search termination criterion
as ATM, which is defined as a fitness improvement of
less than 0.0025 across generations, we observed that FDR
reaches a plateau after a certain number of generations
(e.g., about an average of 65generations across project
versions using LTM with UniXcoder/Cosine). However, the
GA search continues as long as there is sufficient improve-
ment in fitness (e.g., LTM with UniXcoder/Cosine reached a
maximum of 100generations). This suggests that the search
termination criterion unnecessarily prolongs the search and
thus needs further investigation. For example, using a fixed
number of generations, determined based on experience
with past versions, could save time and resources while
achieving high FDR values.
4.4.2 Effective test suite minimization reduces testing cost.
Further, LTM achieves an average saving of 41.72% in test
execution time with a 50% test suite minimization budget.
However, though this is due to the relatively modest size
of the systems we experiment with, one might point out
that the average execution time of test suites in our dataset
is smaller than LTM minimization time. Nevertheless, LTM
still offers significant advantages as testing is performed
frequently, especially in CI contexts, while minimization is
occasional, unlike test case selection and prioritization. It
can thus achieve substantial savings in time and resources
over time, particularly in CI environments, where numerous
builds are tested for every code change, thus warranting
the elimination of redundant or unnecessary test cases to
significantly reduce the overall test execution time and
required testing resources (e.g., memory, CPU).
4.4.3 More informative similarity measures that better
guide the search.
Our results show that the similarity measures LTM em-
ployed better guide the GA to converge faster to a higher
FDR , when compared to ATM, confirming the usefulness
of using pre-trained language models for extracting test
method embeddings.
Results also suggest that the FDR results are influenced
by multiple factors including the language models, distance
functions, and test code characteristics. Language models
differ in the types of input used for pre-training, the model
architectures employed, and the kinds of tasks they wereIEEE TRANSACTIONS ON SOFTWARE ENGINEERING 16
pre-trained for, thus generating embeddings with various
characteristics. While Cosine Similarity captures directional
similarity by measuring the angle between embeddings [49],
Euclidean Distance captures magnitude similarity by mea-
suring the straight-line distance between embeddings in
a multidimensional space [43]. Therefore, using different
combinations of language models and distance functions is
expected to yield varying FDR results.
In addition, the test code characteristic (e.g., domain-
specific character strings or complex string manipulations)
of certain projects may also impact FDR performance, de-
pending on the input (test code or ASTs) and dataset (size
and diversity of programming languages ) used for pre-
training the language models. In conclusion, the perfor-
mance of LTM in terms of FDR may be affected by several
confounding factors. Future research should explore the
effects of these factors through deeper analyses of test code
structures and controlled experiments.
4.4.4 Language models with fewer parameters may offer
a better trade-off than those with many more parameters in
TSM contexts.
The results show that UniXcoder ( 125Million parame-
ter size) outperforms CodeLlama-7b ( 7Billion parameter
size) in terms of both effectiveness and efficiency. This
suggests that larger language models, though comprising
substantially more parameters and being pre-trained on
larger datasets, may not necessarily perform better for cer-
tain downstream tasks such as TSM. CodeLlama-7b, as a
decoder-only model, was pre-trained for code generation
tasks, such as code infilling. While generating (infilling) the
next token, the structure of the decoder makes each token
only learn information from preceding tokens, thereby po-
tentially making its embedding less informative due to the
absence of information from subsequent tokens. In contrast,
code understanding tasks (e.g., MLM) allow the model to
learn the information before and after each token, thus
making it more effective in learning the contextual semantic
information from the source code. Furthermore, UniXcoder
enhanced the code representation by utilizing AST as input,
which further enables the model to understand the source
code structure and grammar, thus resulting in better FDR .
Note that, to the best of our knowledge, there is no
publicly available open-source language model that outper-
forms UniXcoder on the code clone detection task, which
is similar in some ways to our minimization task. More
importantly, given that scalability is crucial for LTM, larger
language models may require much more memory, compu-
tational resources, and time when generating embeddings
due to their substantially higher number of parameters and
complex model architectures. Therefore, for a downstream
task, such as TSM, that requires assessing test code similar-
ity, relatively smaller language models can be a better option
than larger ones.
4.4.5 Using full code versus preprocessed code.
We recommend that LTM be used with the source code
of test cases without preprocessing, since it yielded better
results than when using preprocessed code, as reported
in Section 4.3.1 and also in Section 4.3.4. However, someaspects of preprocessing, such as normalizing variable iden-
tifiers, may be beneficial, since they do not have an impact
on the data flow in the source code. Therefore, using such
techniques may potentially help language models generate
embeddings that are more generalizable and suitable for
code similarity calculation across projects, thus improving
the fault detection power of LTM.
5 T HREATS TO VALIDITY
Internal Validity Internal threats to validity are concerned
with the ability to draw conclusions from our experimental
results. To make fair comparisons and draw valid conclu-
sions, we conducted our experiments using the same exper-
imental design and dataset used to evaluate ATM [8]. We
evaluated the alternative LTM configurations and compared
the best configuration to the two best configurations of
ATM. For minimizing test suites, we used the same fitness
function and parameter settings as ATM, but we further
optimized GA by utilizing a more efficient data structure
for storing the input similarity values and accelerating the
fitness calculation. We compared the performance using
optimized GA for both LTM and ATM.
ATM used preprocessed code as input because of its very
long preparation time. In contrast, LTM used full source
code, raising the question of whether differences in FDR
between LTM and ATM are due to differences in test repre-
sentation (test method embeddings or ASTs) or code input
(full code or preprocessed code). To answer that question,
we conducted an additional experiment to assess the best
configuration of LTM (i.e., UniXcoder/Cosine) using pre-
processed code, which showed that LTM with preprocessed
code still fares better than ATM in terms of FDR .
Another threat is related to the token length limit of
language models, which might truncate the source code of
some test cases [50], thus resulting in information loss that
might in turn negatively impact the performance of LTM.
However, we observed that, for each version, the percentage
of test methods with the token length exceeding the limit is
only 2.98% across projects, which can be considered negli-
gible. Nevertheless, some projects have a relatively higher
percentage of test methods exceeding that limit (e.g., 7.25%
forMath ), which might explain the slightly lower FDR of
LTM compared to ATM on these projects. Future research
should further investigate the use of additional language
models with longer token length limits, though they might
greatly impact efficiency.
External Validity External threats are concerned with the
ability to generalize our results. We assessed both the per-
formance of ATM and LTM on a large dataset consisting of
16Java projects with a total of 661 versions collected from
DEFECTS 4J for which we could trace failures to faults. We
further validated the performance of LTM using an addi-
tional larger project from Defects4J, called Closure , consist-
ing of 174 versions, for which we could not evaluate ATM.
Though we did not evaluate LTM on projects written in
other programming languages, LTM can be easily adapted
to other programming languages as the language models we
used were pre-trained on multiple programming languages
and have been demonstrated to yield good performanceIEEE TRANSACTIONS ON SOFTWARE ENGINEERING 17
in various downstream tasks in other programming lan-
guages [10–12, 34]. Future research should extend our study
to investigate LTM on projects with other programming
languages to further generalize our conclusions. In addition,
test method execution time may vary from run to run. To
measure the achieved time-saving in test execution time
more precisely, we used the D EFECTS 4J infrastructure to run
test suites 10times and collect the average execution time
for each test method in a clean, stable environment, which
ensures we run each test suite without interference and fluc-
tuations in memory or CPU usage (e.g., exclusive access to a
node).8However, we found many test cases ( 24.14% across
projects) with 0execution time as they took less than 0.0001
seconds to run.9Future research should consider evaluating
TSM approaches on much larger industrial systems.
6 C ONCLUSION
In this paper, we proposed LTM, a scalable and black-box
similarity-based test suite minimization (TSM) approach
based on pre-trained language models. We investigated five
alternative language models—CodeBERT, GraphCodeBERT,
UniXcoder, StarEncoder, and CodeLlama—to extract test
method embeddings and two similarity measures (i.e., Co-
sine Similarity and Euclidean Distance) for similarity calcu-
lation based on these embeddings. We employed an opti-
mized version of the Genetic Algorithm (GA) by utilizing
an efficient data structure for handling similarity values to
improve fitness calculation and find optimal subsets of test
suites. We assessed the performance of alternative LTM con-
figurations on a large dataset consisting of 17Java projects
with a total of 835 versions collected from D EFECTS 4J. We
used the Fault Detection Rate ( FDR ), Minimization Time
(MT), and Time Saving Rate ( TSR) as evaluation metrics to
assess the effectiveness and efficiency of LTM. We identified
the best configuration of LTM and compared it to ATM, a
recent black-box TSM approach that achieved better trade-
off in terms of effectiveness and efficiency than the alter-
native SOTA TSM approach (FAST-R). Results indicate that
the best LTM configuration (i.e., UniXcoder/Cosine) outper-
formed the two best configurations of ATM by achieving
significantly higher FDR results ( 0.84versus 0.81, on aver-
age) and more importantly, running much faster ( 0.82min
versus 4.06min, on average) than ATM, in terms of both
preparation time (up to two orders of magnitude faster) and
search time (up to one order of magnitude faster), which
is particularly important on much larger industrial systems
and test suites, while achieving a saving of 41.72% in test
execution time for a 50% test minimization budget.
Future work. We plan to assess the performance of LTM in
practice using large industrial software systems and projects
8We further collected the minimum and maximum execution times
for each test method across the 10runs. The results show that the
difference in TSR, based on minimum and maximum execution times,
ranges from 0.05% to 1.38% across all configurations, which indicates
that the variation in test method execution times across runs is small
and does not impact our conclusions regarding TSR.
9To ensure this did not impact our results, we investigated the effect
of these test cases on the Time Saving Rate by assuming their exe-
cution times to be 0.0001 —their maximum possible value—and re-
calculating the Time Saving Rate. We found that the average Time
Saving Rate of all configurations slightly increased by 0.08% to 0.09%
only, which is negligible and thus does not invalidate our conclusions.in other programming languages to further generalize our
conclusions. The main challenge is the ability to automati-
cally trace test failures to system faults.
7 D ATAAVAILABILITY
The replication package of our experiments, including the
data, code, results for other minimization budgets, and
detailed FDR ,MT, and TSR results of LTM and baseline
approaches, is available on Zenodo [45].
REFERENCES
[1] Saif Ur Rehman Khan, Sai Peck Lee, Nadeem Javaid, and Wadood
Abdul. A systematic review on test suite reduction: Approaches,
experiment’s quality evaluation, and guidelines. IEEE Access ,
6:11816–11841, 2018.
[2] Hadi Hemmati, Andrea Arcuri, and Lionel Briand. Achieving
scalable model-based testing through test case diversity. ACM
Transactions on Software Engineering and Methodology (TOSEM) ,
22(1):1–42, 2013.
[3] Shin Yoo and Mark Harman. Regression testing minimization,
selection and prioritization: A survey. Software testing, verification
and reliability , 22(2):67–120, 2012.
[4] Rongqi Pan, Mojtaba Bagherzadeh, Taher A Ghaleb, and Lionel
Briand. Test case selection and prioritization using machine learn-
ing: a systematic literature review. Empirical Software Engineering ,
27(2):29, 2022.
[5] Aitor Arrieta, Shuai Wang, Urtzi Markiegi, Ainhoa Arruabarrena,
Leire Etxeberria, and Goiuria Sagardui. Pareto efficient multi-
objective black-box test case selection for simulation-based testing.
Information and Software Technology , 114:137–154, 2019.
[6] Emilio Cruciani, Breno Miranda, Roberto Verdecchia, and Antonia
Bertolino. Scalable approaches for test suite reduction. In 2019
IEEE/ACM 41st International Conference on Software Engineering
(ICSE) , pages 419–429. IEEE, 2019.
[7] Kim Herzig. Testing and continuous integration at scale: Limits,
costs, and expectations. In Proceedings of the 11th International
Workshop on Search-Based Software Testing , pages 38–38, 2018.
[8] Rongqi Pan, Taher A. Ghaleb, and Lionel Briand. Atm: Black-box
test case minimization based on test code similarity and evolu-
tionary search. In Proceedings of the 45th IEEE/ACM International
Conference on Software Engineering , pages 1–12, 2023.
[9] Raphael Noemmer and Roman Haas. An evaluation of test suite
minimization techniques. In International Conference on Software
Quality , pages 51–66. Springer, 2020.
[10] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng
Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang,
and Ming Zhou. CodeBERT: A pre-trained model for program-
ming and natural languages. arXiv preprint arXiv:2002.08155 , 2020.
[11] Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie
Liu, Long Zhou, Nan Duan, Jian Yin, Daxin Jiang, and M. Zhou.
Graphcodebert: Pre-training code representations with data flow.
arXiv preprint arXiv:2009.08366 , 2020.
[12] Daya Guo, Shuai Lu, Nan Duan, Yanlin Wang, Ming Zhou, and
Jian Yin. Unixcoder: Unified cross-modal pre-training for code
representation. arXiv preprint arXiv:2203.03850 , 2022.
[13] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff,
Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki,
Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue
Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel
Lamy-Poirier, Jo ˜ao Monteiro, Oleh Shliazhko, Nicolas Gontier,
Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar
Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov,
Zhiruo Wang, Rudra Murthy, Jason Stillerman, Siva Sankalp
Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan
Zhang, Nour Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam
Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor
Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding,
Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank
Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Bren-
dan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried,
Dzmitry Bahdanau, Yacine Jernite, Carlos Mu ˜noz Ferrandis, Sean
Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and
Harm de Vries. Starcoder: may the source be with you! arXiv
preprint arXiv:2305.06161 , 2023.IEEE TRANSACTIONS ON SOFTWARE ENGINEERING 18
[14] Baptiste Rozi `ere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai
Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre,
Tal Remez, J ´er´emy Rapin, Artyom Kozhevnikov, Ivan Evtimov,
Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron
Grattafiori, Wenhan Xiong, Alexandre D ´efossez, Jade Copet, Faisal
Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas
Scialom, and Gabriel Synnaeve. Code Llama: Open Foundation
Models for Code. arXiv preprint arXiv:2308.12950 , 2023.
[15] Hadi Hemmati, Andrea Arcuri, and Lionel Briand. Reducing the
cost of model-based testing through test case diversity. In Testing
Software and Systems: 22nd IFIP WG 6.1 International Conference,
ICTSS 2010, Natal, Brazil, November 8-10, 2010. Proceedings 22 , pages
63–78. Springer, 2010.
[16] Zohreh Aghababaeyan, Manel Abdellatif, Mahboubeh Dadkhah,
and Lionel Briand. Deepgd: A multi-objective black-box test
selection approach for deep neural networks. ACM Transactions
on Software Engineering and Methodology , 2023.
[17] Breno Miranda and Antonia Bertolino. Scope-aided test prioriti-
zation, selection and minimization for software reuse. Journal of
Systems and Software , 131:528–549, 2017.
[18] Carmen Coviello, Simone Romano, Giuseppe Scanniello, Alessan-
dro Marchetto, Giuliano Antoniol, and Anna Corazza. Clustering
support for inadequate test suite reduction. In 2018 IEEE 25th
International Conference on Software Analysis, Evolution and Reengi-
neering (SANER) , pages 95–105. IEEE, 2018.
[19] Yue Liu, Kang Wang, Wang Wei, Bofeng Zhang, and Hailin Zhong.
User-session-based test cases optimization method based on ag-
glutinate hierarchy clustering. In 2011 International Conference on
Internet of Things and 4th International Conference on Cyber, Physical
and Social Computing , pages 413–418. IEEE, 2011.
[20] Man Zhang, Shaukat Ali, and Tao Yue. Uncertainty-wise test case
generation and minimization for cyber-physical systems. Journal
of Systems and Software , 153:1–21, 2019.
[21] Markos Viggiato, Dale Paas, Chris Buzon, and Cor-Paul Bezemer.
Identifying similar test cases that are specified in natural language.
IEEE Transactions on Software Engineering , 2022.
[22] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff
Dean. Distributed representations of words and phrases and their
compositionality. Advances in neural information processing systems ,
26, 2013.
[23] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova. Bert: Pre-training of deep bidirectional transformers
for language understanding. arXiv preprint arXiv:1810.04805 , 2018.
[24] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence
embeddings using siamese bert-networks. arXiv preprint
arXiv:1908.10084 , 2019.
[25] Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limti-
aco, Rhomni St John, Noah Constant, Mario Guajardo-Cespedes,
Steve Yuan, Chris Tar, et al. Universal sentence encoder. arXiv
preprint arXiv:1803.11175 , 2018.
[26] Thorsten Joachims et al. A probabilistic analysis of the rocchio
algorithm with tfidf for text categorization. In ICML , volume 97,
pages 143–151. Citeseer, 1997.
[27] Matt Kusner, Yu Sun, Nicholas Kolkin, and Kilian Weinberger.
From word embeddings to document distances. In International
conference on machine learning , pages 957–966. PMLR, 2015.
[28] Adithya Abraham Philip, Ranjita Bhagwan, Rahul Kumar, Chan-
dra Sekhar Maddila, and Nachiappan Nagppan. Fastlane: Test
minimization for rapidly deployed large-scale online services. In
2019 IEEE/ACM 41st International Conference on Software Engineer-
ing (ICSE) , pages 408–418. IEEE, 2019.
[29] Kalyanmoy Deb, Amrit Pratap, Sameer Agarwal, and TAMT Me-
yarivan. A fast and elitist multiobjective genetic algorithm: NSGA-
II.IEEE transactions on evolutionary computation , 6(2):182–197, 2002.
[30] Zhiyuan Chang, Mingyang Li, Junjie Wang, Qing Wang, and
Shoubin Li. Putting them under microscope: a fine-grained ap-
proach for detecting redundant test cases in natural language.
InProceedings of the 30th ACM Joint European Software Engineering
Conference and Symposium on the Foundations of Software Engineering ,
pages 1161–1172, 2022.
[31] Peter D Turney and Patrick Pantel. From frequency to meaning:
Vector space models of semantics. Journal of artificial intelligence
research , 37:141–188, 2010.
[32] William B Johnson. Extensions of lipschitz mappings into a hilbert
space. Contemp. Math. , 26:189–206, 1984.
[33] MK Vijaymeena and K Kavitha. A survey on similarity measuresin text mining. Machine Learning and Applications: An International
Journal , 3(2):19–28, 2016.
[34] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svy-
atkovskiy, Ambrosio Blanco, Colin Clement, Dawn Drain, Daxin
Jiang, Duyu Tang, Ge Li, Lidong Zhou, Linjun Shou, Long Zhou,
Michele Tufano, Ming Gong, Ming Zhou, Nan Duan, Neel Sun-
daresan, Shao Kun Deng, Shengyu Fu, and Shujie Liu. Codexglue:
A machine learning benchmark dataset for code understanding
and generation. arXiv preprint arXiv:2102.04664 , 2021.
[35] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural
machine translation of rare words with subword units. arXiv
preprint arXiv:1508.07909 , 2015.
[36] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis,
and Marc Brockschmidt. Codesearchnet challenge: Evaluating the
state of semantic code search. arXiv preprint arXiv:1909.09436 , 2019.
[37] Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao
Mou, Carlos Mu ˜noz Ferrandis, Yacine Jernite, Margaret Mitchell,
Sean Hughes, Thomas Wolf, Dzmitry Bahdanau, Leandro von
Werra, and Harm de Vries. The stack: 3 tb of permissively licensed
source code. arXiv preprint arXiv:2211.15533 , 2022.
[38] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin.
Attention is all you need. Advances in neural information processing
systems , 30, 2017.
[39] Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D
Manning. Electra: Pre-training text encoders as discriminators
rather than generators. arXiv preprint arXiv:2003.10555 , 2020.
[40] Tianyu Gao, Xingcheng Yao, and Danqi Chen. Simcse: Sim-
ple contrastive learning of sentence embeddings. arXiv preprint
arXiv:2104.08821 , 2021.
[41] Xiaofei Ma, Zhiguo Wang, Patrick Ng, Ramesh Nallapati, and Bing
Xiang. Universal text representation from bert: An empirical study.
arXiv preprint arXiv:1910.07973 , 2019.
[42] Wael H Gomaa and Aly A Fahmy. A survey of text similarity ap-
proaches. international journal of Computer Applications , 68(13):13–
18, 2013.
[43] Michel Marie Deza and Elena Deza. Encyclopedia of distances. In
Encyclopedia of distances , pages 1–583. Springer, 2009.
[44] J. Blank and K. Deb. Pymoo: Multi-objective optimization in
Python. IEEE Access , 8:89497–89509, 2020.
[45] Rongqi Pan, Taher A. Ghaleb, and Lionel C. Briand. LTM: Scalable
and Black-box Similarity-based Test Suite Minimization based on
Language Models (replication package). https://doi.org/10.5281/
zenodo.13685828, 2023.
[46] Michel Raymond and Franc ¸ois Rousset. An exact test for popula-
tion differentiation. Evolution , pages 1280–1283, 1995.
[47] Shuyin Xia, Zhongyang Xiong, Yueguo Luo, Guanghua Zhang,
et al. Effectiveness of the euclidean distance in high dimensional
spaces. Optik , 126(24):5614–5619, 2015.
[48] Angela Fan, Beliz Gokkaya, Mark Harman, Mitya Lyubarskiy,
Shubho Sengupta, Shin Yoo, and Jie M Zhang. Large language
models for software engineering: Survey and open problems.
arXiv preprint arXiv:2310.03533 , 2023.
[49] ANRL Sirisha and Ashok Kumar Pradhan. Cosine similarity
based directional comparison scheme for subcycle transmission
line protection. IEEE Transactions on Power Delivery , 35(5):2159–
2167, 2019.
[50] Sakina Fatima, Taher A Ghaleb, and Lionel Briand. Flakify: A
black-box, language model-based predictor for flaky tests. IEEE
Transactions on Software Engineering , 2022.
Rongqi Pan is currently working toward the PhD
degree with the School of EECS, University of
Ottawa and a member of Nanda Lab, Canada.
She obtained her Master’s degree in Statistics at
the University of Illinois at Urbana-Champaign,
USA. Her research interests include automated
software testing, natural language processing,
and applied machine learning.IEEE TRANSACTIONS ON SOFTWARE ENGINEERING 19
Taher A. Ghaleb is an Assistant Professor in the
Computer Science Department at Trent Univer-
sity, Canada. He earned his Ph.D. in Comput-
ing from Queen’s University, Canada, where he
was awarded the Ontario Trillium Scholarship, a
highly prestigious award for doctoral students.
Following his Ph.D., he worked as a senior re-
search scientist at the University of Toronto and
as a postdoctoral research fellow at the Univer-
sity of Ottawa. His research interests include continuous integration and
delivery (CI/CD), data-driven software analytics, artificial intelligence
for software engineering, and mining software repositories. For more
information, visit taher-ghaleb.github.io.
Lionel C. Briand is professor of software engi-
neering and has shared appointments between
(1) The University of Ottawa, Canada, and (2)
The Lero SFI Centre—the national Irish centre
for software research—hosted by the University
of Limerick, Ireland. In collaboration with col-
leagues, for over 30 years, he has run many
collaborative research projects with companies
in the automotive, satellite, aerospace, energy,
financial, and legal domains. Lionel has held various engineering, aca-
demic, and leading positions in seven countries. He currently holds a
Canada Research Chair (Tier 1) on ”Intelligent Software Dependability
and Compliance” and is the director of Lero, the national Irish centre for
software research. Lionel was elevated to the grades of IEEE Fellow and
ACM Fellow for his work on software testing and verification. Further,
he was granted the IEEE Computer Society Harlan Mills award, the
ACM SIGSOFT outstanding research award, and the IEEE Reliability
Society engineer-of-the-year award. He also received an ERC Advanced
grant in 2016 on modelling and testing cyber-physical systems, the most
prestigious individual research award in the European Union and was
elected a fellow of the Academy of Science, Royal Society of Canada in
2023. More details can be found at: http://www.lbriand.info.