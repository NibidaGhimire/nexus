arXiv:2303.05924v4  [math.AP]  20 Oct 2024Journal of Machine Learning ISSN : 2790-2048(e), 2790-203X(p)
Variational formulations of ODE-Net as a mean-ﬁeld op-
timal control problem and existence results
Noboru Isobe *1and Mizuho Okumura† 2
1Graduate School of Mathematical Sciences, The University o f Tokyo, Tokyo, Japan.
2Graduate School of Science, Tohoku University, Sendai, Jap an.
Abstract. This paper presents a mathematical analysis of ODE-Net, a co ntinuum model of deep neural net-
works (DNNs). In recent years, Machine Learning researcher s have introduced ideas of replacing the deep
structure of DNNs with ODEs as a continuum limit. These studi es regard the “learning” of ODE-Net as the
minimization of a “loss” constrained by a parametric ODE. Al though the existence of a minimizer for this
minimization problem needs to be assumed, only a few studies have investigated the existence analytically
in detail. In the present paper, the existence of a minimizer is discussed based on a formulation of ODE-Net
as a measure-theoretic mean-ﬁeld optimal control problem. The existence result is proved when a neural net-
work describing a vector ﬁeld of ODE-Net is linear with respe ct to learnable parameters. The proof employs
the measure-theoretic formulation combined with the direc t method of Calculus of Variations. Secondly, an
idealized minimization problem is proposed to remove the ab ove linearity assumption. Such a problem is in-
spired by a kinetic regularization associated with the Bena mou–Brenier formula and universal approximation
theorems for neural networks.
Keywords:
Deep Learning,
ResNet,
ODE-Net,
Benamou–Brenier formula,
Mean-Field Game.Article Info.:
Volume: 1
Number: 2
Pages: 1- xx
Date: June/2022
doi.org/10.4208/jml.xxxArticle History:
Received: 10/12/2023
Accepted: 11/09/2024
Communicated by:
Jiequn Han
1 Introduction
Deep Neural Networks (DNNs), or Deep Learning, now constitu te a core of artiﬁcial
intelligence technology, but their theoretical inner mech anisms have yet to be explored.
In particular, there have been few theoretical contributio ns regarding “learning” DNNs,
despite practical demands for them, where “learning” is, br oadly speaking, to minimize
the so-called “loss” by optimizing a parameter θof DNNs.
Our research aims to establish a well-posed mathematical fo rmulation of the learning.
To achieve this aim, some researchers have brought language s of dynamical systems and
differential equations into DNNs, for example, in [ 22,27,54]. In short, one can regard a
continuum limit of DNNs in their depth as an ODE. Many researc hers have attempted
to dissect DNNs through some ODEs, designated as ODE-Net throughout the paper. For
more information on these attempts, see the survey in Sectio n2. Based on this survey,
well-posednesses, such as the existence of a minimizer of lo ss, have not yet been fully
explored in the context of these studies.
*Corresponding author. mail: nobo0409@g.ecc.u-tokyo.ac.jp , Supported by JSPS KAKENHI 22J20130.
†okumura.mizuho.p3@gmail.com .
https://www.global-sci.com/jml Global Science PressJ. Mach. Learn., 1(2):1-xx 2
Accordingly, our goal in this paper is to prove the existence of a minimizer for learning
ODE-Net, formulated as a regularized minimization problem constrained by a continuity
equation.
1.1 Target problems and main results
First of all, we are going to study the existence of a minimize r of the following kinetic-
regularized minimization problem:
Problem 1.1 (Kinetic regularized learning problem constrained by ODE- Net) .Letλ≥0
and ǫ>0 be constants, let Ybe a subset of Rdand let v:Rd×Rm→Rdandℓ:Rd×Y →
R+be continuous. Let µ0∈ P c(Rd×Y)be a given training data. Set
J(µ,θ):=/integraldisplay
Rd×YℓdµT+/integraldisplayT
0/integraldisplay
Rd×Y/parenleftbiggλ
2|v(x,θt)|2+ǫ
2|θt|2/parenrightbigg
dµt(x,y)dt (1.1)
forµ∈C([0,T];(P2(Rd×Y),W2))and θ∈L2(0,T;Rm). Note that v(•,θ)∈L2(dµ)is
a vector ﬁeld on Rdforµ∈ P c(Rd)and θ∈Rm. The learning problem constrained by
ODE-Net is posed as the following constrained minimization problem:
inf/braceleftBig
J(µ,θ)µ∈C/parenleftBig
[0,T];(P2(Rd×Y),W2)/parenrightBig
,θ∈L2(0,T;Rm)/bracerightBig
,
subject to
/braceleftBigg
∂tµt+div x(µt(x,y)v(x,θt)) = 0,(x,y)∈Rd×Y,t∈(0,T),
µt|t=0=µ0,(1.2)
wherePc(Rd×Y)denotes the set of regular and Borel probability measures co mpactly
supported on Rd× Y,(P2(Rd× Y),W2)denotes the ( L2-)Wasserstein space deﬁned in
Subsection 3.2,C([0,T];(P(Rd×Y),W2))denotes the set of curves which is continuous
with respect to the Wasserstein topology (see also Deﬁnitio n3.2), and
µ∈C([0,T];(P2(Rd×Y),W2))
is supposed to solve the equation ( 1.2) in the distributional sense of Deﬁnition 3.7.
Remark 1.2.In Problem 1.1, the ODE-Net corresponds to the continuity equation ( 1.2) with
a parameter θt, and the learning to the minimization of a functional Jwith respect to a
parameter θtand a solution µtto ODE ( 1.2).
The ﬁrst term in ( 1.1) measures the so-called loss. The second term in ( 1.1) is called a “ki-
netic regularization” in [ 25] because it represents the kinetic energy when v(•,θ)(θ∈Rm)
is regarded as a velocity ﬁeld on Rd. By letting this kinetic energy be as small as possible,
we could control the velocity ﬁeld so that the support of the s olution µtto (1.2) does not
change wildly. The third term is often called an L2-regularization, which is familiar with
the well-known Ridge regression.
In order to prove existence of a minimizer for Problem 1.1, we shall impose the follow-
ing assumptions on Y,ℓand v:J. Mach. Learn., 1(2):1-xx 3
Assumption 1.3. The label set Y ⊂ Rdis compact, and the loss function ℓ:Rd×Y → R+
is a continuous function of 2-growth, see also Deﬁnition 3.2.
In addition, following the previous works on ODE-Net [ 7,48,49,52,61], we impose
the assumption below that the neural network v(x,θ)is linear with respect to θ, but not
necessarily linear with respect to x.
Assumption 1.4. The neural network vin (1.2) is linear with respect to θ, i.e., the parameter
θis ad×pmatrix and vsatisﬁes
(1.3) v(x,θ) =θf(x),
where f:Rd→Rpis a Lipschitz continuous function.
Assumption 1.4is not a serious restriction. In fact, [ 58, Theorem 1] shows that for a
neural network vthat is nonlinear with respect to θ, there exists another neural network
that is linear with respect to θand can approximate the solution µof ODE-Net ( 1.2). Thus,
Assumption 1.4is not so restrictive in discussing the existence of the mini mizer for Prob-
lem 1.1. Rather, Assumption 1.4can address neural networks unbounded with respect to
parameters θ, which commonly appear in modern DNNs. In contrast, previou s theoretical
works often assume a bounded neural network, details of whic h will be given in Section 2
below.
Under these assumptions, we obtain one of our main results in the present paper.
Theorem 1.5 (Existence of a minimizer) .Under Assumptions 1.3and1.4, there exists a mini-
mizer(µ,θ)∈C([0,T];(P2(Rd×Y),W2))×L2(0,T;Rm)for Problem 1.1.
It should be noted that by virtue of this theorem, one can assu me that the deep learning
model as in Problem 1.1with Assumptions 1.3and 1.4is well-deﬁned so that we can
pursue the mathematical analysis of the learning of ODE-Net s. We also remark here that
the uniqueness of such minimizers cannot be generally expec ted since the problem is over-
determined with a large degree of freedom in θ. We will also mention the uniqueness in
Remark 4.7below.
We note that Assumption 1.4does not hold for all neural networks. For example, two-
layer ReLU networks v(x,θ) = A(Bx)+,θ= (A,B),A,B∈Rd×d, are not linear with
respect to θ. This network is quite commonly used, not only in ODE-Net but also in the
so-called ResNet, as illustrated in [ 28, Figure 2].
In order to provide existence results for these cases as well , we shall consider an ideal or
relaxed version of Problem 1.1. To this end, we shall employ the universal approximation
theorem by Cybenko [ 18] or the Kolmogorov–Arnol’d representation theorem shown b y
[5,33,55]; they insist that neural networks vcan approximate or represent arbitrary vector
ﬁelds. Those theorems inspire that the ODE-Net is no longer p arametrized by θ, i.e., the
ODE-Net is just driven by a family of vector ﬁelds (vt)t∈[0,T]. From this perspective, our
ideal setting for the learning reads:
Problem 1.6 (Ideal learning problem) .Letλ>0 be a strictly positive constant, let ℓ:Rd×
Y → R+be continuous, and let µ0∈ P c(Rd×Y)be a given input data. Set
/hatwideJ(µ,v):=/integraldisplay
Rd×YℓdµT+/integraldisplayT
0/integraldisplay
Rd×Yλ
2|v(x,t)|2dµt(x,y)dt (1.4)J. Mach. Learn., 1(2):1-xx 4
forµ∈C([0,T];(P2(Rd×Y),W2))and v∈L2(dµtdt), where v∈L2(dµtdt)means that
the squared integral of v(x,t)in the measure d µt(x)over Rdis integrable in time tover
[0,T]. Then an ideal learning problem constrained by ODE-Net is posed as the foll owing
constrained minimization problem:
inf/braceleftBig
/hatwideJ(µ,v)µ∈C([0,T];(P2(Rd×Y),W2)),v∈L2(dµtdt)/bracerightBig
,
subject to
/braceleftbigg∂tµt+div x(vtµt)=0 (in the sense of Deﬁnition 3.7),
µt|t=0=µ0.(1.5)
In contrast to Problem 1.1, where the parameter θis a variable to the functional J, the
vector ﬁeld vitself is a variable to the functional /hatwideJin Problem 1.6. For this idealized
problem containing a broader class of vector ﬁelds, we also e stablish the existence of a
minimizer as in the following theorem:
Theorem 1.7 (Existence of a minimizer) .Under Assumption 1.3, there exists a minimizer µ∈
C([0,T];(P2(Rd×Y),W2))and v∈L2(dµtdt)for Problem 1.6.
We have been discussing the well-posedness of “learning” of ODE-Net by formulating
it via a mean-ﬁeld optimal control problem, in the sense that we have to control trajectories
in the space of probability measures µt. Through the above discussion, it is suggested that
such a learning framework successfully gives a mathematica l way to analyze the learning
processes of DNNs. Our main results obtained in this analysi s are interesting from the
viewpoint of the Calculus of Variations in that minimizers e xist for nonlinear optimal
control problems such as Problems 1.1and 1.6. Moreover, the proofs of our theorems will
ensure that every minimizing sequence contains a convergen t subsequence in a suitable
topology, leading to the well-posedness of sequential mini mization algorithms such as
Gradient Descent (GD).
1.2 Contributions of the paper
The present paper contributes to establishing the existenc e of a minimizer under situ-
ations where the regularization parameter λis not necessarily large in Theorem 1.5. This
situation can be addressed because, in contrast to the paper [10], we use an argument
that does not rely on a strong convexity of Jto prove the existence of a minimizer. In
addition, this theorem can apply to unbounded and non-diffe rentiable neural networks
v(x,θ), which are important targets in applications.
As a comparison, a key to our convergence results is to obtain the existence of a mini-
mizer of both µand θunder reasonable assumptions. The authors in [ 10] required strong
convexity of J, or a sufﬁciently large parameter λ, in order to obtain strong compactness.
In addition, Thorpe and Gennip [ 57] and Esteve et al. [ 24] obtained existence results un-
der an H1-regularization of θt, and Herty et al. [ 29] under boundedness for the Lipschitz
constant of θ:[0,T]→Rm; broadly speaking, both of them are assuming that the “diffe r-
entials” of the parameters θtin time are controlled. While these studies are novel in thatJ. Mach. Learn., 1(2):1-xx 5
they do not impose assumptions on regularization parameter s such as λ, the assumptions
of the continuity or differentiability on parameters θ:[0,T]→Rmshould be relaxed or re-
moved because the functions that ODE-Net can approximate ar e limited and the expected
value ofℓcannot be sufﬁciently small. Furthermore, in the ﬁeld of ens emble optimal con-
trol, an existence result in the L2-setting using ODE similar to ( 2.4) is proved by Scagliotti
in [53, Theorem 3.2]. In [ 45], Pogodaev proved the existence of optimal control of the
continuity equation with parameters θrelaxed to Young measures on a bounded domain.
As a corollary, the existence of optimal parameters follows if the neural network satisﬁes
certain convexity conditions, but the continuity of an opti mal curve µ∗with respect to tis
not clear.
Theorem 1.5also provides one theoretical justiﬁcation for the experim ental algorithm
in [25]. The authors developed an algorithm to approximate the min imizer of the Benamou–
Brenier type problem, which is guaranteed to exist. However , the guarantee does not hold
for the algorithm because the vector ﬁeld vin the continuity equation is constrained by
the neural network vθ. This study supplements the existence of a minimizer, even i n this
case.
In Section 5, we combine the neural network property of universal approx imation with
the training of ODE-Net in Problem 1.6. This new combination makes it possible to ob-
tain existence results (Theorem 1.7) without the linearity assumption (Assumption 1.4). It
is also interesting that Problem 1.6has a similar formulation to (variational) Mean-Field
Game (MFG) [ 9,35,50]. This similarity between Deep Learning and MFG has recentl y
been pointed out by E et al. [ 23] and Ruthotto et al. [ 47]. Our results are expected to sug-
gest a strong connection between MFG and ODE-Net. In fact, fo r the proof of Theorem 1.7,
we will give an auxiliary theorem (Lemma 5.3) that is proved via the so-called Lagrange
perspective for easy handling of the vector ﬁelds v(see also [ 50, Subsection 2.2.2]).
1.3 Organization of the paper
This paper is organized as follows. In Section 2, we will give a brief review of previous
studies on ODE-Net. In the ﬁrst half, we summarize the histor y of the development of
ODE-Net, and in the second half, we review mathematical form ulations of the learning of
ODE-Net. In Section 3, we will provide preliminary facts on the convergence of pro babil-
ity measures and distributional solutions of the continuit y equation, which will be used to
set up and prove our main results. In Section 4, we will prove Theorem 1.5. By virtue of
the regularization term in ( 1.1) and the Benamou–Brenier formula in Lemma 3.9, we will
easily get the appropriate compactness of minimizing seque nces. Hence, we can apply
the direct method of the Calculus of Variations to reach the e xistence results. In Section 5,
we will exhibit how Problem 1.6is formulated through an idealization in a detailed man-
ner, and then we prove our main result (Theorem 1.7). One cannot prove the theorem by
simply applying the arguments used in Section 4. Instead, we show the theorem by the
use of a supplementary problem (see Problem 5.2and Lemma 5.3) based on the Lagrange
perspective. Section 6presents a summary of the paper and discusses some tasks to be
undertaken in future studies. In Appendices Aand B, we show and review the existence
results for problems given by Bonnet et al. [ 10] and Thorpe and Gennip [ 57]. These prob-
lems adopt different regularization terms from Problem 1.1. By comparing the proofs ofJ. Mach. Learn., 1(2):1-xx 6
Theorems 1.5,A.2and B.5, one can observe differences in how compactness is obtained to
minimizing sequences.
2 Background and related works
This section provides an overview of previous research on le arning of ODE-Net. Sub-
section 2.1reviews how ODE-Net has been proposed. Subsection 2.2describes how the
learning has been formulated and discussed.
2.1 Background to the development of ODE-Net
Before describing the history of the development of ODE-Net , we shall review a type
of DNN called ResNet that led to the improvement of DNN’s perf ormance. ResNet was
devised to facilitate optimization of DNN in [ 28]. The simplest L-layer ResNet consists of
the difference equation
(2.1)x0=g(x,θ),
xt+1=xt+v(xt,θt),t=0, . . . , L−1,
y=h(xL,θL),
where x∈Rdis an input data, y∈ Y denotes a ﬁnal output, and g(•,θ):Rd→Rd0
and h(•,θL):RdL→ Y ⊂ RdYare some linear maps with parameters θ∈Rd0×dand
θL∈RdY×dLrespectively. In addition, v(•,θt):Rdt→Rdt+1is multiple compositions
of some afﬁne maps with θt, and nonlinear functions, called activation functions, su ch
as Rectiﬁed Linear Unit (ReLU) [ 42]. Out of various models of (Deep) Neural Networks,
we shall refer to the above mapping v(•,θt)associated with ResNet as a neural network
simply in this paper. Experimentally, ResNet is known to per form better than other DNNs.
In particular, deep ResNet, i.e., ( 2.1) with L≫1 outperforms other Machine Learning
methods.
When ResNet is very deep, it is natural to observe ResNet ( 2.1) as the explicit Euler
discretization of an ODE with unit step size. With the pionee ring works in [ 22,27,54],
a trend started to analyze DNNs and develop algorithms by rep lacing “discrete” DNNs
with “continuum” ODEs. For example, Haber et al. [ 27] employed the linear stability anal-
ysis in the theory of dynamical systems to stabilize ResNet, and Lorin et al. [ 37] utilized
the parallel computing for differential equations to speed up the training of ResNet. These
“continuum” ODEs corresponding to DNNs are often called Neu ral ODE in [ 16], or ODE-
Net in [ 46,62]. Speciﬁcally, the following parameterized dynamical sys tem is often called
ODE-Net :
(2.2)x0=g(x,θ),
˙xt=v(xt,θt),t∈(0,T),
y=h(xT,θT),
where x∈Rd,y∈ Y,g:Rd×Rd×d→Rd,h:Rd×RdY×d→ Y and vare deﬁned as in
(2.1). Note that for simplicity, it is assumed that xt∈Rdfor any t∈[0,T], and accordingly,J. Mach. Learn., 1(2):1-xx 7
the neural network v(•,θt)becomes a vector ﬁeld on Rd. Also, the ﬁnite-dimensional
parameters θ0,θ1, . . . , and θL−1in (2.1) are replaced with a (measurable) function on [0,T].
While θ:[0,T]→Rmis sometimes supposed to be continuous for theoretical reas ons, the
function θon[0,T]can be discontinuous during the learning process as seen in [ 39, Figure
2] and [ 6, Figure 1]. Thus, we impose the Lebesgue integrability cond ition on θin our
setting. The terminal time T>0 is an arbitrary given constant.
Although there is not so much mathematical research on ODE-N et, the basic properties
of general DNNs have also been studied for ODE-Net. For examp le, ODE-Net has univer-
sal approximation properties proved by [ 56] and that the objective functional Jhas no
local minima shown in [ 19,20,38]. It is also known that speciﬁc additional assumptions
(e.g., continuity of θ:[0,T]→Rm) are necessary to regard ResNet as the discretization of
ODE-Net (see, e.g., [ 31,49,57]) and to guarantee the convergence of learning algorithms
in [31].
2.2 Formulations of the learning of ODE-Net and existence re sults
Practically, people want ODE-Net to output a desired yfor an input x. For this purpose,
ODE-Net needs to learn , i.e., we optimize the parameter θin ODE-Net ( 2.2). Thus, it is
necessary to establish a theory of the learning of ODE-Net. E et al. were the ﬁrst to attempt
a general formulation of the learning of ODE-Net ( 2.2) in [ 22,23]. They modeled the
learning as a mean-ﬁeld optimal control problem as follows:
Problem 2.1 (Learning problem constrained by ODE-Net [ 23, Equation 3]) .LetY=Rl, let
Θbe a subset of Rmand let v:Rd×Rm→Rd,ℓ:Rd×Y → R+and L:Rd×Rm→R+
be continuous. For a given input data µ0∈ P c(Rd×Y), the learning problem constrained
by ODE-Net is posed as the following constrained minimizati on problem:
minimize
θ∈L∞(0,T;Θ)E/bracketleftBigg
ℓ(xT,y)+/integraldisplayT
0L(xt,θt)dt/bracketrightBigg
, (2.3)
subject to/braceleftbigg˙xt=v(xt,θt),t∈(0,T),
(x0,y)∼µ0.(2.4)
The meanings of symbols appearing in Problem 2.1are as follows. The given probabil-
ity measure µ0is called training data; a probability distribution of inpu t-output pairs of
a random variable (x,y)in (2.2) used for the learning. The vector ﬁeld v(•,θ),θ∈Rm,
onRdrepresents the neural network explained in ( 2.2). After expanded by the linear-
ity of the expected values, the ﬁrst term of ( 2.3) represents the expected value of a loss
function ℓ(x,y), which is the target we want to make as small as possible durin g the
learning process. One often uses the squared loss ℓ(x,y) =|x−y|2/2 for regression
problems or the cross-entropy for classiﬁcation problems ( see, e.g., Pytorch’s document
for the speciﬁc form). However, when using a neural network w ith many parameters,
minimizing only the loss E[ℓ(xT,y)]can lead to the so-called overﬁtting; see basic statis-
tics and machine learning textbooks, e.g., [ 41, Subsection 1.4.7]. To avoid this overﬁtting,
we also minimize the second expected value, which is called a regularization term. ForJ. Mach. Learn., 1(2):1-xx 8
example, some researchers use the L2-regularization L(x,θ) = λ|θ|2/2,L1-regularization
L(x,θ) =λ|θ|, and entropy regularization used in [ 26,31]. In addition, the kinetic regular-
ization L(x,θ) = λ|v(x,θ)|2/2 that Finlay et al. proposed with the help of the Benamou–
Brenier formula in [ 25] can make the trajectories of ODEs’ solutions well-behaved . An-
other way to deal with the overﬁtting is to restrict Θ⊂Rmto compact sets. In Optimal
Control Theory, by virtue of the compactness of Θ, one can easily show the existence of
optimal parameters (see, e.g., [ 13, Theorem 5.1.1]). It should be noted that these various
regularizations require an assumption upon a function spac e to which the parameters θ
belong. As is seen in the above Problem 2.1, E et al. set the function space to L∞-space in
[23].
Remark 2.2 (On neglecting input and output transformations in ( 2.2)).ODE-Net intro-
duced in ( 2.2) contains input and output transformations gand h, leading to a learning
problem corresponding to a minimization with respect to θ∈Rd×d,θ•∈L2(0,T;Rm)
and θL∈RdY×d. However, current theoretical studies of ODE-Net often use formulations
that ignore g(x,θ)and h(x,θL), and consider minimization only in θtas in Problem 2.1.
In the author’s view, the reason for this neglect is that the e xistence of minimizers for θ
and θLis easy to check if one proposes a variational formulation th at considers gand h.
For example, if one imposes the L2-regularization |θ|2+|θL|2for the parameters θ∈Rd×d
and θL∈RdY×dassociated with g(•,θ):x/ma√sto→x0and h(•,θL):xT/ma√sto→yrespectively, the
existence of minimizers θ∗∈Rd×dand θ∗
L∈RdY×dfollows immediately by virtue of
the direct method of the Calculus of Variations; a minimizin g sequence of ((θn,θn
L))nhas
a convergent subsequence thanks to the Bolzano–Weierstras s theorem. Hence, only the
ODE ˙xt=v(xt,θt)in (2.2) is sometimes referred to as ODE-Net. On the other hand, gand
hshould notbe ignored when we explore the learning process, that is, the dynamics of
solving the problem with mathematical optimization method s such as GD. It is reported
that singular values of a parameter deﬁning gand haffect the convergence of GD [ 7, The-
orem 2].
On the other hand, for Problem 2.1, Bonnet et al. brought a measure-theoretical for-
mulation inspired by mean-ﬁeld optimal control problems [ 10, Section 1.4]. A trick used
in their formulation is that laws µt,t∈(0,T), of random variables (xt,y)subject to ( 2.4)
satisfy the following continuity equation:
/braceleftBigg
∂tµt+div x(µt(x,y)v(x,θt)) = 0,(x,y)∈Rd×Y,t∈(0,T),
µt|t=0=µ0,
in the sense of distributions deﬁned in Deﬁnition 3.7. They utilized this trick to translate
Problem 2.1into the following Problem 2.3in the case of L(x,θ) =λ|θ|2:
Problem 2.3 (Measure-theoretical learning problem [ 10, Equation 1.8]) .Letλ>0 be con-
stants and let v:Rd×Rm→Rdandℓ:Rd×Y → R+be continuous. For a given input
data µ0∈ P c(Rd×Rd), the learning problem constrained by ODE-Net is posed as theJ. Mach. Learn., 1(2):1-xx 9
following constrained minimization problem:
minimize
θ∈L2(0,T;Rm)/braceleftBigg/integraldisplay
Rd×Rdℓ(x,y)dµT(x,y)+λ/integraldisplayT
0|θt|2dt/bracerightBigg
, (2.5)
subject to/braceleftBigg
∂tµt+div x(v(x,θt)µt)=0,(x,y)∈Rd×Rd,t∈(0,T),
µt|t=0=µ0.(2.6)
In addition, µbelongs to Cw([0,T];Pc(Rd×Rd))which is the space of narrowly continu-
ous curves (see also Deﬁnition 3.2).
As for Problem 2.3, Bonnet et al. studied the unique existence of a minimizer θ∗under the
assumption that λ>0 issufﬁciently large and the neural network v(x,θ)is bounded for θ
[12, Theorem 3.2]. In practice, however, in order to minimize th e loss, the regularization
parameter λis usually set to be a sufﬁciently small positive number rather than a large one.
The difﬁculty in obtaining existence theorems to Problem 2.3is attributed to the varia-
tional formulation. From ( 2.5) and ( 2.6), we observe that the learning of ODE-Net has the
following aspects:
(i) the objective functional Jin (2.5) is minimized over an inﬁnite-dimensional space
L2(0,T;Rm), and
(ii) the minimization is constrained by the continuity equa tion ( 2.6) which is a differen-
tial equation on the inﬁnite-dimensional space of probabil ity measures P(Rd×Y).
When one tries to show the existence of a minimizer for a varia tional problem such as
Problem 2.3by using the direct method of the Calculus of Variations, it i s difﬁcult to obtain
the strong compactness of minimizing sequences due to the in ﬁnite dimensionality in (i).
In addition, even if minimizing sequences converge, it is no t generally obvious whether
limits satisfy the continuity equation ( 2.6) mentioned in (ii).
3 Preliminaries
This section presents fundamental mathematical tools.
3.1 Compactness lemma
ForT>0, we denote by C([0,T];X)the set of continuous mappings from [0,T]to a
topological space Xwith the uniform convergence topology.
Lemma 3.1 (Ascoli–Arzel´ a’s theorem) .Let(X,d)be a metric space. Then, a family F ⊂
C([0,T];X)is relatively compact in the uniform convergence topology i f and only if
•for each t ∈[0,T], the set{x∈Xx=f(t)for some f ∈ F}is relatively compact in X,
and
•Fis equi-continuous.
Proof. A more general version of the above lemma in the case where Xis a uniform space
is proved in, e.g., [ 32, Chapter 7.17] /squaresolidJ. Mach. Learn., 1(2):1-xx 10
3.2 Probability measures and the Wasserstein space
Hereinafter, P(X)denotes the set of Borel probability measures on a separable metric
space X. Here, we review some deﬁnitions and lemmas regarding prope rties and conver-
gence of probability measures, as well as properties of the W asserstein space.
Deﬁnition 3.2. Letp≥1 and let (X,d)be a Polish space, i.e., a complete and separable
metric space.
(i) (narrow convergence) A sequence (µn)inP(X)is said to be narrowly convergent to
µ∈ P(X)asn→∞if
lim
n→∞/integraldisplay
Xfdµn=/integraldisplay
Xfdµfor every function f∈Cb(X),
where Cb(X)is the space of continuous and bounded real functions deﬁned onX. A
topology induced by the convergence is said to be the narrow t opology.
(ii) (uniformly integrable p-moments) A subset KinP(X)hasuniformly integrable p-
moments if
lim
R→∞sup
µ∈K/integraldisplay
X\BX(R,x)d(x,x)pdµ(x) =0 for some x∈X,
where BX(R,x)is the open ball of radius Rand center xinX.
(iii) (ﬁnite p-th moment) A probability measure µ∈ P(X)is said to have the ﬁnite p-th
moment if
/integraldisplay
Xd(x,x)pdµ(x)<∞for some x∈X,
and the set of probability measures on Xwith the ﬁnite p-th moment is denoted by
Pp(X).
(iv) (function of p-growth) A function f:X→Ris said to have p-growth if there exist
A,B≥0 and x∈Xsuch that |f(x)| ≤A+B(d(x,x))pfor all x∈X.
(v) (Wasserstein distance) The ( Lp-)Wasserstein distance between µ1,µ2∈ P p(X)is de-
ﬁned by
Wp(µ1,µ2):=inf/braceleftBigg/parenleftbigg/integraldisplay
X2d(x1,x2)pdπ(x1,x2)/parenrightbigg1/p
π∈Γ(µ1,µ2)/bracerightBigg
,
where Γ(µ1,µ2)denotes the set of all Borel probability measures πonX2such that
for any measurable subset A⊂X,
π[A×X]=µ1[A],π[X×A]=µ2[A].
By using the H¨ older inequality, one easily getsJ. Mach. Learn., 1(2):1-xx 11
Corollary 3.3. Let1≤p<q<∞, let X be a Polish space and let µ1,µ2∈ P q(X). Then
Wp(µ1,µ2)≤Wq(µ1,µ2).
Lemma 3.4 (Kantrovich–Rubinstein duality [ 60, Thoerem 1.14]) .Let(X,d)be a Polish space
and let ρ0,ρ1∈ P 1(X). Then
W1(ρ0,ρ1) =sup/braceleftBigg/integraldisplay
Xϕd(ρ1−ρ0)ϕ∈L1(|ρ1−ρ0|), Lip
X(ϕ):=sup
x/\e}atio\slash=y∈X|ϕ(x)−ϕ(y)|
d(x,y)≤1/bracerightBigg
.
Proof. See [ 21, Section 11.8]. /squaresolid
A sufﬁcient condition for a family with the uniformly integr able p-moments is known,
and the proof of the following lemma is given for the sake of th e reader’s convenience.
Lemma 3.5 ([4, Subsection 5.1.1]) .Let p≥1. If a subset K ⊂ P(X)satisﬁes
sup
µ∈K/integraldisplay
Xd(x,x)p1dµ(x)<+∞,
for some p 1>p and x∈X, then K has uniformly integrable p-moments.
The following lemma shows a ﬁne criterion that reveals wheth er a sequence (µn)⊂
P(X)has the uniformly integrable p-moments.
Lemma 3.6 (Narrow convergence for p-growth functions) .A sequence (µn)inP(X)has
uniformly integrable p-moments if and only if
(i)the sequence is narrowly convergent to µ∈ P(X), and
(ii)for every continuous function f :X→Rof p-growth,
limn→∞/integraldisplay
Xfdµn=/integraldisplay
Xfdµ
Proof. See [ 4, Lemma 5.1.7]. /squaresolid
By Lemma 3.6and [ 4, Proposition 7.1.5], convergence in Wpand narrow convergence
forp-growth functions are equivalent.
3.3 Continuity equation
The following deﬁnition and lemma are based on a famous text [ 4, Chapter 4], to which
we refer the reader who wants a general discussion of the cont inuity equations.
Deﬁnition 3.7 (Solutions in the sense of distributions) .LetT>0. A continuous curve
µ∈Cw([0,T];P(Rd×Y))is called a solution to the continuity equation
(3.1) ∂tµt+div x(vtµt)=0 in(0,T)×Rd×Y,J. Mach. Learn., 1(2):1-xx 12
in the sense of distribution, if
(3.2)/integraldisplayT
0/integraldisplay
Rd×Y(∂tψt(x,y)+∇xψt(x,y)·vt(x))dµt(x,y)dt=0
for every ψ∈C∞
c((0,T)×Rd×Y). Here a mapping vt:Rd∋x/ma√sto→vt(x)∈Rd,t∈[0,T],
is a Borel vector ﬁeld.
In the following, we adopt Deﬁnition 3.7as the solution of the continuity equation ( 3.1)
with a vector ﬁeld v.
Lemma 3.8 (Representation formula for ( 3.1) [4, Proposition 8.1.8]) .Let T>0and let µ∈
Cw([0,T];P(Rd×Y))be a distributional solution of (3.1)with Borel vector ﬁelds v = (vt)t.
Assume that v satisﬁes that
/integraldisplayT
0/integraldisplay
Rd×Y|vt|dµtdt<∞, (3.3)
and
(3.4)/integraldisplayT
0/parenleftbigg
sup
K|vt|+Lip
K(vt)/parenrightbigg
dt<∞for every compact set K ⊂Rd.
Here LipK(vt)denotes a Lipschitz constant of the mapping v t:Rd→Rdon K, i.e.,
Lip
K(vt):=sup
x/\e}atio\slash=y∈K|vt(y)−vt(x)|
|y−x|.
Then, for µ0-a.e.(x,y)∈Rd×Y, there exists a unique solution X •(x)∈C([0,T];Rd)such that
X0(x) =x,
d
dtXt(x)=vt(Xt(x)).
Furthermore, the solution µtis represented as
(3.5) µt=(Xt×IdY)#µ0for all t∈[0,T],
where IdX:X→X is the identity mapping on X.
Proof. The existence result can be shown by the use of the standard ar gument of the Picard
iteration method. For the representation result, details a re proved in, e.g., [ 4, Proposition
8.1.8]. /squaresolid
The following lemma indicates the strong relation between t he Wasserstein distance
and the continuity equation.J. Mach. Learn., 1(2):1-xx 13
Lemma 3.9 (Benamou–Brenier formula [ 8]).Letρ0,ρ1∈ P 2(Rd). Then,
(3.6) W2(ρ0,ρ1)2=inf/braceleftBigg/integraldisplay1
0/integraldisplay
Rd|vt(x)|2dρt(x)dt(ρ,v)∈V(ρ0,ρ1)/bracerightBigg
,
where
V(ρ0,ρ1):=

(ρ,v)∈C([0, 1];P2(Rd))×L2(dρtdt)(3.1)holds in the sense of
Deﬁnition 3.7, and
ρt|t=0=ρ0,ρt|t=1=ρ1.

.
Proof. See [ 3, Theorem 17.2]. /squaresolid
4 Kinetic Regularization and an Existence Theorem
In this section, we discuss the existence of a minimizer to th ekinetic regularized learning
problem introduced in Problem 1.1. The section begins with some background on kinetic
regularization. Subsequently, We proceed to the proof of Th eorem 1.5. Throughout the
paper, we denote by Ca generic non-negative constant which may vary from line to l ine.
4.1 Kinetic regularization
In general, to argue minimizers of a functional Jas in ( 1.1) via the direct method of
the Calculus of Variations, a minimizing sequence ((µn,θn))nof the functional needs to
be compact in an appropriate topological space. Moreover, t he topology must be sufﬁ-
ciently strong to lead to a (lower semi)continuity of the fun ctional. Driven by this ne-
cessity, some previous studies have tried to strengthen the topology of the space of the
parameter θ:[0,T]→Rmin [10,29,57]. However, this strong topology leads to un-
usual assumptions, as reviewed in Subsection 2.2. Instead, we seek for compactness of
the continuous curves µn:[0,T]→ P(Rd×Y), rather than of the parameters θn(n∈N).
This idea is rarely seen in Machine Learning but often in the M FG theory (see, e.g., [ 43,
Theorem 6.6.] and [ 11, Theorem 6]). To illustrate this idea, we need the following lemma
derived from the Benamou–Brenier formula (Lemma 3.9):
Lemma 4.1 (Uniform continuity estimate) .Letµ∈Cw([0,T];P(Rd×Y))be a distributional
solution to the continuity equation (3.1)with Borel vector ﬁelds v t:Rd∋x/ma√sto→vt(x)∈Rd,
t∈[0,T]. Then it holds that
W2(µt,µs)2≤(s−t)/integraldisplays
t/integraldisplay
Rd×Y|v(τ,x)|2dµτ(x,y)dτ
for0≤t<s≤T.
In the rest of the paper, we often abbreviate/integraltext
Rd×Yf(x)dµ(x,y)to/integraltext
Rdfdµfor a function
f:Rd→Rindependent of y∈ Y.J. Mach. Learn., 1(2):1-xx 14
Proof. From Lemma 3.9, we have
W2(µt,µs)2
≤inf
ρ,w/braceleftBigg/integraldisplay1
0/integraldisplay
Rd×Y|wt(x)|2dρt(x,y)dt∂tρ+div x(wρ)=0,ρ0=µt,ρ1=µs./bracerightBigg
=infρ,w/braceleftbigg/integraldisplays
t/integraldisplay
Rd|wτ|2dρτdτ
s−t(s−t)∂τρ+div x(wρ)=0,ρt=µt,ρs=µs./bracerightbigg
=(s−t)inf/braceleftbigg/integraldisplays
t/integraldisplay
Rd|wτ|2dρτdτ∂τρ+div x(wρ)=0,ρt=µt,ρs=µs./bracerightbigg
≤(s−t)/integraldisplays
t/integraldisplay
Rd|vτ|2dµτdτ
for 0≤t<s≤T. /squaresolid
This lemma readily leads to the following:
Corollary 4.2. Let n∈Nand let µn∈Cw([0,T];Pc(Rd×Y))be a distributional solution to
the continuity equation (3.1)corresponding to Borel vector ﬁelds (vn
t)t∈[0,T]. If
sup
n∈N/integraldisplay1
0/integraldisplay
Rd|vn
t|2dµtdt<∞,
then the family (µn)is equi-continuous.
To use Corollary 4.2explicitly, we add a term
(4.1)λ
2|v(x,θ)|2,λ>0,
to the objective functional J(1.1) in Problem 1.1. This regularization term |v(x,θ)|2is
reported to be effective in generative models in [ 25]. Here, we use kinetic regularization
for simplicity, but in fact, one can prove the existence of a m inimizer without a kinetic
regularization term. See Remark 4.6for details.
4.2 Existence theorem
Our strategy is to use the direct method of the Calculus of Var iations, containing the
following three steps:
(i) take a minimizing sequence ((µn,θn))nand extract a convergent subsequence in suit-
able topologies,
(ii) check that Jis lower semicontinuous with respect to those topologies an d
(iii) verify that the limits of convergent subsequences sat isfy the constraint ( 1.2).J. Mach. Learn., 1(2):1-xx 15
As for a minimizing sequence, we get a weakly convergent subs equence of (θn)inL2(0,T;Rm)
and the strongly convergent subsequence of (µn)inC([0,T];(P2(Rd×Y),W2))by virtue
of Corollary 4.2and the Ascoli–Arzel´ a theorem. From these convergences an d Assump-
tion 1.4, we observe that the functional Jis lower semicontinuous in (µ,θ). Also, we can
verify that the limits solve the continuity equation again. This is why we impose the ki-
netic regularization term onto the functional J.
For the proof of Theorem 1.5, we need a lemma on the boundedness of the support of
µtuniformly in t∈[0,T].
Lemma 4.3. Letθ∈L2(0,T;Rm)and let µ∈C([0,T];(P2(Rd×Y),W2))be a distributional
solution of (1.2)corresponding to vector ﬁelds (v(•,θt))t∈[0,T]. If Assumption 1.4holds, there
exists a radius R∗=R∗/parenleftBig
µ0,f,T,/ba∇dblθ/ba∇dblL2(0,T;Rm)/parenrightBig
>0such that
supp µt⊂BRd×Y(R∗, 0) =:BRd×Y(R∗)for all t∈[0,T].
Proof. To use Lemma 3.8, we check the assumption ( 3.3) and ( 3.4). By Assumption 1.4and
the Lipschitz continuity of f, we have
/integraldisplayT
0/integraldisplay
Rd×Y|v(x,θt)|dµtdt≤/integraldisplayT
0/integraldisplay
Rd×Y|θt||f(x)|dµtdt
≤/ba∇dblθ/ba∇dblL2(0,T;Rm)/radicalBigg/integraldisplayT
0/parenleftbigg/integraldisplay
Rd×Y|f(x)|dµt(x,y)/parenrightbigg2
dt
≤√
T/ba∇dblθ/ba∇dblL2(0,T;Rm)/radicalBigg
sup
t∈[0,T]/integraldisplay
Rd×Y|f(x)|2dµt(x,y)
≤C/ba∇dblθ/ba∇dblL2(0,T;Rm)/parenleftBigg
1+sup
t∈[0,T]/radicalBigg/integraldisplay
Rd×Y|x|2dµt(x,y)/parenrightBigg
<∞
Similarly, it holds that
/integraldisplayT
0/parenleftbigg
sup
K|v(•,θt)|+Lip
K|v(•,θt)|/parenrightbigg
dt≤C(T+/ba∇dblθ/ba∇dbl2
L2(0,T;Rm))<∞,
for every compact set K⊂Rd×Y. We thus ﬁnd from Lemma 3.8that µtcan be repre-
sented as µt=(Xt, IdY)#µ0where Xt:Rd→Rdis the ﬂow maps of the corresponding
ODE satisfying


dXt(x)
dt=v(Xt(x),θt)fort∈(0,T),
X0(x) =x,
for almost all (x,y)∈supp µ0. By Gr¨ onwall’s inequality and Assumption 1.4, we have
|Xt(x)|≤/parenleftBigg
|x|+C/integraldisplayT
0|θs|ds/parenrightBigg
exp/parenleftBigg
C/integraldisplayT
0|θs|ds/parenrightBiggJ. Mach. Learn., 1(2):1-xx 16
≤C/parenleftBig
diam(supp µ0)+T+√
T/ba∇dblθ/ba∇dblL2(0,T;Rm)/parenrightBig
exp/parenleftBig
C√
T/ba∇dblθ/ba∇dblL2(0,T;Rm)/parenrightBig
≤R∗
for some R∗>0 independent of tsince θ∈L2(0,T;Rm), whence follows supp µt⊂B(R∗)
for all t∈[0,T]. /squaresolid
Remark 4.4.Assumption 1.4can be generalised to Assumption 6.1when one only proves
Lemma 4.3. See also Lemma A.3.
With Lemma 4.3, one can now proceed with the proof of Theorem 1.5.
Proof of Theorem 1.5.Set
S=/braceleftBig
(µ,θ)∈C/parenleftBig
[0,T];(P2(Rd×Y),W2)/parenrightBig
×L2(0,T;Rm)(µ,θ)satisﬁes ( 1.2)/bracerightBig
.
Since(µ0, 0)∈Sis a trivial and regular solution to the continuity equation , we see that
S/\e}atio\slash=∅. It is also obvious that 0 ≤infJ<+∞because the integrand ℓis non-negative.
Then, there exists a minimizing sequence ((µn,θn))∞
n=1⊂Ssuch that J(µn,θn)→infSJas
n→∞. For the sequence, there exists a constant C>0 independent of nsuch that
λ
2/integraldisplayT
0/integraldisplay
Rd|θn
tf(x)|2dµn
t(x)dt≤C, (4.2)
ǫ
2/integraldisplayT
0|θn
t|2dt≤C. (4.3)
From Lemma 4.1and ( 4.2), we have for 0 ≤t<s≤T,
W2(µn
t,µn
s)2≤(s−t)/integraldisplays
t/integraldisplay
Rd|θn
τf(x)|2dµn
τ(x)dτ≤2C
λ(s−t).
Hence it follows that (µn)⊂C([0,T];(P2(Rd× Y),W2))is equi-continuous. Also, by
Lemma 4.3and ( 4.3), there exists a constant R∗>0 independent of nand tsuch that
µn
t∈/braceleftBig
µ∈ P c(Rd×Y)supp µ⊂BRd×Y(R∗)/bracerightBig
for all n∈Nand all t∈[0,T].
In addition, the set/braceleftBig
µ∈ P c(Rd×Y)supp µ⊂BRd×Y(R∗)/bracerightBig
is compact with respect to
L2-Wasserstein topology because of [ 4, Proposition 7.1.5]. Hence, Lemma 3.1and( 4.3)
imply that there exist a subsequence of (n), still denoted by n,µ∗∈C([0,T];(P2(Rd×
Y),W2))and θ∗∈L2(0,T;Rm)such that
µn→µ∗strongly in C/parenleftBig
[0,T];/parenleftBig
P2(Rd×Y),W2/parenrightBig/parenrightBig
, (4.4)
θn→θ∗weakly in L2(0,T;Rm). (4.5)
By the following Claim 4.5, we can deduce that (µ∗,θ∗)solves ( 1.2) in the sense of distri-
bution.J. Mach. Learn., 1(2):1-xx 17
Claim 4.5. For the limits µ∗andθ∗, it holds that
(4.6)/integraldisplayT
0/integraldisplay
Rd×Y(∂tζt+∇xζt·v(•,θ∗
t))dµ∗
tdt=0,
for all ζ∈C∞c/parenleftBig
(0,T)×Rd×Y/parenrightBig
. Moreover, supp µ∗⊂BRd×Y(R)for some R >0.
Proof. We already know that
(4.7)/integraldisplayT
0/integraldisplay
Rd×Y∂tζtdµn
tdt+/integraldisplayT
0/integraldisplay
Rd×Y∇xζt·v(x,θn
t)dµn
tdt=0,
for all ζ∈C∞
c/parenleftBig
(0,T)×Rd×Y/parenrightBig
and n∈N. It follows that
0=/integraldisplayT
0/integraldisplay
Rd×Y∂tζtd(µn
t−µ∗
t)dt
+/integraldisplayT
0/integraldisplay
Rd×Y∇xζt(x)·v(x,θ∗
t)d(µn
t−µ∗
t)(x)dt
+/integraldisplayT
0/integraldisplay
Rd×Y∇xζt(x)·(v(x,θn
t)−v(x,θ∗
t))dµ∗
t(x)dt
+/integraldisplayT
0/integraldisplay
Rd×Y∇xζt(x)·(v(x,θn
t)−v(x,θ∗
t))d(µn
t(x)−µ∗
t(x))dt
+/integraldisplayT
0/integraldisplay
Rd×Y∂tζtdµ∗
tdt+/integraldisplayT
0/integraldisplay
Rd×Y∇xζt(x)·v(x,θ∗
t)dµ∗
t(x)dt
=:I1+I2+I3+I4+I5.(4.8)
It follow from ( 4.4) that
|I1|≤/integraldisplayT
0Lip
Rd×Y(∂tζt)/integraldisplay
Rd×Y∂tζt
LipRd×Y(∂tζt)d(µn
t−µ∗
t)dt
≤C/integraldisplayT
0W1(µn
t,µ∗
t)dt
≤C/integraldisplayT
0W2(µn
t,µ∗
t)dt
≤CT sup
t∈[0,T]W2(µn
t,µ∗
t)→0,
asn→∞by the Kantrovich–Rubinstein duality (Lemma 3.4) and Corollary 3.3. By As-
sumption 1.4, the function ∂tζt(x)v(x,θ∗
t)is Lipschitz continuous in xand yover Rd×Y,
and thus we see again from Lemma 3.4and Corollary 3.3that
|I2|≤C/integraldisplayT
0W1(µn
t,µ∗
t)dtJ. Mach. Learn., 1(2):1-xx 18
≤C/integraldisplayT
0W2(µn
t,µ∗
t)dt
≤CT sup
t∈[0,T]W2(µn
t,µ∗
t)→0,
asn→∞.
ForI3, we use Assumption 1.4to apply ( 4.5). We set
ϕ•:=/integraldisplay
Rd×Y∇xζ•f⊤dµ∗
•∈L2(0,T;Rm).
In fact, it is shown that
/ba∇dblϕ/ba∇dbl2
L2(0,T;Rm)≤/integraldisplayT
0/integraldisplay
Rd×Y/vextendsingle/vextendsingle/vextendsingle∇xζtf⊤/vextendsingle/vextendsingle/vextendsingle2
dµn
tdt≤C/parenleftBigg
1+sup
t∈[0,T]W2(µn
t,δ0)/parenrightBigg
<∞.
Then, we can deduce that
I3=/integraldisplayT
0/a\}b∇acketle{tϕt,θn
t−θ∗
t/a\}b∇acket∇i}htdt→0 as n→∞,
where/a\}b∇acketle{tA,B/a\}b∇acket∇i}ht:=Tr/parenleftbig
A⊤B/parenrightbig
,A,B∈Rd×pis the inner product on Rd×p.
As for I4, it follows from ( 4.4) and ( 4.5) that
|I4|=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/integraldisplayT
0/angbracketleftbigg
θn
t−θ∗
t,/integraldisplay
Rd×Y∇xζ•fTd(µn
t−µ∗
t)/angbracketrightbigg
dt/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤C/integraldisplayT
0|θn
t−θ∗
t|W1(µn
t,µ∗
t)dt
≤C/ba∇dblθn−θ∗/ba∇dblL2(0,T;Rm)sup
t∈[0,T]W2(µn
t,µ∗
t)
≤Csup
t∈[0,T]W2(µn
t,µ∗
t)→0.
Passing to the limit as n→∞in (4.8), we get ( 4.6).
By the lower semicontinuity of the L2-norm, we have /ba∇dblθ∗•/ba∇dblL2(0,T;Rm)≤supn∈N/ba∇dblθn•/ba∇dblL2(0,T;Rm)<
∞. Then Lemma 4.3implies that there exists R>0 such that supp µ∗
t⊂BRd×Y(R)for all
t∈[0,T], whence follows the conclusion. /squaresolid
We resume the proof of Theorem 1.5. From Claim 4.5, we have (µ∗,θ∗)∈S. We then
show that J(µn,θn)→J(µ∗,θ∗)asn→∞. First, from ( 4.4), Assumption 1.3, Lemma 3.6
and [ 4, Proposition 7.1.5], it follows that
(4.9)/integraldisplay
Rd×Yℓdµn
T→/integraldisplay
Rd×Yℓdµ∗
T,J. Mach. Learn., 1(2):1-xx 19
asn→∞. We next estimate the regularization term. Again from ( 4.3) and ( 4.4), we infer
that
(4.10)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/integraldisplayT
0/integraldisplay
Rd|θn
tf(x)|2dµn
t(x)dt−/integraldisplayT
0/integraldisplay
Rd|θn
tf(x)|2dµ∗
t(x)dt/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/integraldisplayT
0/angbracketleftbigg
θn
t/integraldisplay
Rdf f⊤d(µn
t−µ∗
t),θn
t/angbracketrightbigg
dt/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤/integraldisplayT
0/vextendsingle/vextendsingle/vextendsingle/vextendsingle/integraldisplay
Rdf f⊤d(µn
t−µ∗
t)/vextendsingle/vextendsingle/vextendsingle/vextendsingle|θn
t|2dt≤2C
ǫmax
t∈[0,T]/vextendsingle/vextendsingle/vextendsingle/vextendsingle/integraldisplay
Rdf f⊤d(µn
t−µ∗
t)/vextendsingle/vextendsingle/vextendsingle/vextendsingle→0 as n→∞.
Now we set
~θ~2
L2(0,T;Rm):=/integraldisplayT
0/angbracketleftbigg
θt/parenleftbigg
λ/integraldisplay
Rdf f⊤dµ∗
t+ǫ/parenrightbigg
,θt/angbracketrightbigg
dt,
forθ∈L2(0,T;Rm). Since λ/integraldisplay
Rdf f⊤dµ∗
t+ǫis positive deﬁnite matrix, the function
~•~L2(0,T;Rm)deﬁnes an equivalent norm of L2(0,T;Rm). Hence, it follows from [ 14,
Proposition 3.5] that ~•~L2(0,T;Rm)is weakly lower semicontinuous. Thus, we conclude
that
inf
SJ=lim inf
n→∞J(µn,θn)
=/integraldisplay
Rd×Yℓdµ∗
T+lim inf
n→∞/integraldisplayT
0/integraldisplay
Rd/parenleftbiggλ
2|θn
tf(x)|2+ǫ
2|θn
t|2/parenrightbigg
dµn
t(x)dt
=/integraldisplay
Rd×Yℓdµ∗
T+lim infn→∞/integraldisplayT
0/integraldisplay
Rd/parenleftbiggλ
2Tr/parenleftBig
θn
tf(x)f(x)⊤θn
t⊤/parenrightBig
+ǫ
2|θn
t|2/parenrightbigg
dµn
t(x)dt
=/integraldisplay
Rd×Yℓdµ∗
T+lim infn→∞/integraldisplayT
0/integraldisplay
Rd/parenleftbiggλ
2/angbracketleftBig
θn
tf(x)f(x)⊤,θn
t/angbracketrightBig
+ǫ
2/a\}b∇acketle{tθn
t,θn
t/a\}b∇acket∇i}ht/parenrightbigg
dµn
t(x)dt
=/integraldisplay
Rd×Yℓdµ∗
T+1
2lim infn→∞/integraldisplayT
0/angbracketleftbigg
θn
t/parenleftbigg
λ/integraldisplay
Rdf f⊤dµn
t+ǫ/parenrightbigg
,θn
t/angbracketrightbigg
dt
≥/integraldisplay
Rd×Yℓdµ∗
T+1
2lim infn→∞~θn~2
L2(0,T;Rm)
+λ
2lim infn→∞/parenleftBigg/integraldisplayT
0/integraldisplay
Rd|θn
tf(x)|2dµn
t(x)dt−/integraldisplayT
0/integraldisplay
Rd|θn
tf(x)|2dµ∗
t(x)dt/parenrightBigg
≥/integraldisplay
Rd×Yℓdµ∗
T+1
2~θ∗~2
L2(0,T;Rm)+0
=J(µ∗,θ∗)≥inf
SJ,(4.11)
i.e.,J(µ∗,θ∗) =infSJ, and the proof of Theorem 1.5is complete. /squaresolidJ. Mach. Learn., 1(2):1-xx 20
Remark 4.6 (The case of λ=0).If one only wants to show the existence of a minimizer, it
is sufﬁcient to use only ǫ|θ|2/2 as the regularization term. In other words, we can prove
the theorem when λ=0. Indeed, by Lemma 4.3it is apparent that
/integraldisplay
Rd|x|2dµn
t≤(R∗)2
holds for every t∈[0,T]and n∈N, where R∗>0 is the same as the radius in Lemma 4.3.
Thus, we obtain
W2(µn
t,µn
s)2≤(s−t)/integraldisplays
t/integraldisplay
Rd|θn
τf(x)|2dµn
τ(x)dτ
≤(s−t)/parenleftBigg
/ba∇dblθ/ba∇dbl2
L2(s,t;Rm)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/integraldisplay
Rd|f(x)|2dµn
•(x)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
L∞(s,t)/parenrightBigg
≤2C(s−t)
ǫ/vextenddouble/vextenddouble/vextenddouble/vextenddouble/integraldisplay
Rd|f(x)|2dµn
•(x)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
L∞(s,t)
≤2C(s−t)
ǫ/vextenddouble/vextenddouble/vextenddouble/vextenddouble(Lipf)2/integraldisplay
Rd|x|2dµn
•(x)+|f(0)|2/vextenddouble/vextenddouble/vextenddouble/vextenddouble
L∞(s,t)
≤2C/parenleftBig
(R∗Lipf)2+|f(0)|2/parenrightBig
(s−t)
ǫ
from Lemma 4.1and Assumption 1.4, or ( 6.1), and ( 4.3). Here Lip f≥0 is a Lipschitz
constant of f. Consequently, we can guarantee the equi-continuity of the curve µwithout
the kinetic regularization term. Then, we complete the proo f using an argument similar
to the one above. In this case, the proof of the lower semicont inuity ( 4.11) becomes rather
simple. It is noteworthy, however, that even in this case, de riving the convergence of I3
in the proof of Claim 4.5from the weak convergence of θis difﬁcult without imposing
Assumption 1.4. In this sense, it seems essential under L2-regularization that the neural
network is linear with respect to the parameters. If continu ity of θcan be obtained, e.g.,
byH1-regularization, then the convergence of I3can be easily shown. See the proof of
Theorem A.2.
Remark 4.7 (Uniqueness of a minimizer) .When a regularization parameter ǫis sufﬁciently
large, the uniqueness of the minimizer is proved by Bonnet et al. [10, Theorem 3.2]. For
self-containdness, we will present details in Appendix B. On the contrary, we do not refer
to the uniqueness in Theorem 1.5since ǫmight be small in most practical cases.
5 Ideal Learning Problem
This section discusses the existence of a minimizer to the ideal learning problem as
introduced in Problem 1.6. At the beginning of this section, we explain why we consider
this problem before we prove the main theorem (Theorem 1.7).J. Mach. Learn., 1(2):1-xx 21
5.1 Idealization of learning problems
Neural networks vin (1.2) have, in general, a complex structure, while Assumption 1.4
imposes the simplicity of linearity of vinθto prove Theorem 1.5. However, it is difﬁcult
to show Claim 4.5under the general assumption because the trick described in (4.11) is
unavailable.
Thus, we assume that v(•,θ)can be any square-integrable vector ﬁeld. This assump-
tion might be justiﬁed by universal approximation properties resulting from the complex-
ity. Universal approximation means that the set of function s expressed by a neural net-
work/braceleftBig
v(•,θ):Rd→Rdθ∈Rm/bracerightBig
is dense in appropriate function spaces (for example,
Lebesgue spaces Lp(Rd)). We refer the reader to [ 18,30,44] for details. In light of these
results, one can infer that
/braceleftbig
Rd×[0,T]∋(x,t)/ma√sto−→ v(x,θt)∈Rdθ∈L2(0,T;Rm)/bracerightbig/ba∇dbl•/ba∇dblL2(dµtdt)=L2(dµtdt),
holds where, by abuse of notation, we set for a ﬁxed µ∈C([0,T];P(Rd))
(5.1)
L2(dµtdt)
:=/braceleftBigg
(vt)t∈[0,T]is a family of Borel vector ﬁelds on Rd/ba∇dblv/ba∇dbl2
L2(dµtdt):=/integraldisplayT
0/integraldisplay
Rd|vt|2dµtdt<+∞/bracerightBigg
.
This abuse is referred to in the notation in Villani’s text [ 60, Equation (8.6)]. Furthermore,
ifǫ=0 in ( 1.1),θonly appears via vin Problem 1.1.
Therefore, we can regard Problem 1.1as a problem about vector ﬁelds (neural network)
v, rather than parameters θ. From the above, we consider Problem 1.6as the further ideal-
ized learning problem. The problem is similar to the variati onal form of MFG introduced
by Lasry and Lions [ 34,35]. We also refer the reader to more comprehensive lecture not es
by Santambrosio [ 50, Subsection 2.2].
5.2 Proof of the existence via the Lagrangian framework
We then consider the existence of a minimizer in Problem 1.6. For this problem, we
want to apply a similar argument to Theorem 1.5. However, unlike the previous problem,
the space L2(dµtdt)depends on µ, rendering it intractable. In such cases, it is helpful
to rewrite the problem with “probability measure on curves” as the variable instead of
“curve on probability measures” or µ∈C([0,T];P(Rd))as the variable. That is, we
consider Q∈ P(Rd×C([0,T];Rd))as presented in Proposition 5.1. Here AC2([0,T];Rd)
denotes the set of an absolutely continuous curve γ:[0,T]→Rdsuch that there exists
m∈L2(0,T)satisfying |γ(s)−γ(t)|≤/integraltextt
sm(τ)dτfor all s,t∈[0,T],s<t.
Proposition 5.1 (Probabilistic representation) .Letµ∈C([0,T];(P2(Rd),W2))satisfy the
continuity equation ∂tµt+div(vtµt)=0in the distributional sense for a Borel vector ﬁeld v t
such that
(5.2)/integraldisplayT
0/integraldisplay
Rd|vt|2dµtdt<+∞.J. Mach. Learn., 1(2):1-xx 22
Then, there exists Q ∈ P(Rd×C([0,T];Rd))such that
(i) Q is concentrated on the set of pairs (x,γ)such that γ∈AC2([0,T],Rd)is an absolutely
continuous solution of ˙γ(t) =vt(γ(t))for a.a. t∈(0,T)with γ(0) =x;
(ii)µt=µQ
tfor any t∈[0,T], where µQ
tis deﬁned as
(5.3)/integraldisplay
RdϕdµQ
t:=/integraldisplay
Rd×C([0,T];Rd)ϕ(γ(t))dQ(x,γ),
for all ϕ∈Cb(Rd).
Conversely, if Q ∈ P(Rd×C([0,T];Rs))satisﬁes (i)and
(5.4)/integraldisplay
Rd×C([0,T];Rd)T/integraldisplay
0|˙γ(t)|2dtdQ(x,γ)<+∞,
then there exists µQ∈C([0,T];(P2(Rd),W2))induced via (5.3), which is a solution of the conti-
nuity equation with the following vector ﬁeld
˜vt(x):=/integraldisplay
{γ∈C([0,T];Rd)γ(t)=x}˙γ(t)dQx(γ)∈L2(dµQ
tdt)forµQ
t-a.e. x∈Rd,
where Q xis the disintegrated measures with respect to the evaluatio n mapet:Rd×C([0,T];Rd)∋
(x,γ)/ma√sto−→ γ(t)∈Rd.
Proof. See proofs of [ 4, Theorem 8.2.1], [ 2, Theorem 5.3], and [ 36, Theorems 4 and 5]. /squaresolid
Recently, in studies for MFG [ 9,50], considering µinstead of Qis called the Lagrange
perspective. This perspective is summed up in the slogan “Th ink Eulerian, prove La-
grangian” in [ 59, Chapter 15], which is widely applied in, e.g., [ 51]. We refer the reader to
[4, Subsection 8.2] and [ 36] for a more general theory.
Referring to the formulation in MFG, we rewrite the ideal Pro blem 1.6in terms of Q.
Before starting a more ideal problem, we introduce an evalua tion map
et:(y,γ)∈ Y × C([0,T];Rd)/ma√sto−→(γ(t),y)∈Rd×Y, for t∈[0,T].
With the above, one can rewrite Problem 1.6as follows:
Problem 5.2 (Ideal learning problem in the Lagrangian framework) .Letλ>0 be a con-
stant, let ℓ:Rd×Y → R+be continuous, and let µ0∈ P c(Rd×Y)be a given input data.
Set
/tildewideJ(Q):=/integraldisplay
Y×C([0,T];Rd)/parenleftBigg
ℓ(γ(1),y)+/integraldisplayT
0λ
2|˙γ(t)|2dt/parenrightBigg
dQ(y,γ), (5.5)J. Mach. Learn., 1(2):1-xx 23
forQ∈ P(Y × C([0,T];Rd)). Then, the ideal learning problem in the Lagrangian frame-
work is posed as the following constrained minimization pro blem:
inf/braceleftBig
/tildewideJ(Q)Q∈ P(Y × C([0,T];Rd))such that (e0)#Q=µ0/bracerightBig
.
Comparing Problem 1.6and Problem 5.2, the functional /hatwideJin (1.4) and/tildewideJin (5.5) have
the correspondence such that
/integraldisplay
Rd×Yℓ(x,y)dµT(x,y)⇐⇒/integraldisplay
Y×C([0,T];Rd)ℓ(γ(1),y)dQ(y,γ),
/integraldisplayT
0/integraldisplay
Rd×Yλ
2|v(x,t)|2dµt(x,y)dt⇐⇒/integraldisplay
Y×C([0,T];Rd)/integraldisplayT
0λ
2|˙γ(t)|2dtdQ(y,γ).
We see that Problem 5.2has fewer constraints and fewer variables than Problem 1.6. This
is because, according to Problem 1.6,µand vsatisfying the continuity equation ( 1.2) can
be recovered as long as Qis obtained. This fact leads us to the existence of a minimize r for
Problem 5.2.
Lemma 5.3 (Existence result for Problem 5.2).Under Assumption 1.3, there exists a minimizer
Q∈ P(Y × C([0,T];Rd))for Problem 5.2.
Proof. Set
S=/braceleftBig
Q∈ P(Y × C([0,T];Rd))(e0)#Q=µ0/bracerightBig
,
here the probability measures P(Y × C([0,T];Rd))are endowed with the narrowly con-
vergence topology. We can easily check that a measure (eY×c•)#µ0belongs to S, where
we seteY:Rd×Y ∋(x,y)/ma√sto−→ y∈ Y andc:Rd×Y ∋(x,y)/ma√sto−→/braceleftBig
[0,T]∋t/ma√sto−→ x∈Rd/bracerightBig
∈
C/parenleftBig
[0,T];Rd/parenrightBig
. It is clear that 0 ≤/tildewideJ<+∞since the integrand ℓis non-negative. Thus, we
take a minimizing sequence (Qn)n⊂Ssuch that/tildewideJ(Qn)→infJ∈Rasn→∞. From the
second term of ( 5.5), there exists a constant Cindependent of nsuch that
(5.6)λ
2/integraldisplay
Y×C([0,T];Rd)T/integraldisplay
0|˙γ(t)|2dtdQn(y,γ)≤C.
Next, we claim that (Qn)nis tight. We choose the maps r1and r2deﬁned on Y ×
C/parenleftBig
[0,T];Rd/parenrightBig
as
r1:(y,γ)/ma√sto−→ y∈ Y,r2:(y,γ)/ma√sto−→ γ∈C/parenleftBig
[0,T];Rd/parenrightBig
.
It is clear that/parenleftbig
r1
#Qn/parenrightbig
nis tight because of Assumption 1.3and Prokhorov’s theorem. InJ. Mach. Learn., 1(2):1-xx 24
addition, the functional
(5.7)
A:C([0,T];Rd)∋γ/ma√sto−→

/integraldisplayT
0λ
2|˙γ(t)|2dt/parenleftbiggifγis an absolutely continuous curve
with|˙γ|∈L2(0,T)and γ(0)∈supp µ0./parenrightbigg
+∞ (otherwise )
has a compact sublevel sets in C/parenleftBig
[0,T];Rd/parenrightBig
because of the Ascoli–Arzel´ a theorem. Hence
we can see that/parenleftbig
r2
#Qn/parenrightbig
nis also tight thanks to an integral condition for tightness [ 4, Re-
mark 5.1.5] and ( 5.6). Then, we obtain the tightness of (Qn)nby applying a tightness
criterion [ 4, Lemma 5.2.2] for the maps r1and r2.
Therefore, there exists a subsequence (n), still denoted by n, and Q∗∈ P(Y× C([0,T];Rd))
such that
Qn⇀Q∗inP(Y × C([0,T];Rd)),
by Prokhorov’s theorem.
It remains to be veriﬁed that the limit Q∗satisﬁes(e0)#Q=µ0and/tildewideJ(Q∗)=inf/tildewideJ(=
lim n→∞/tildewideJ(Qn)). The former is obtained by the continuity of the evaluation m apet,t∈
[0,T]. The latter is shown as follows. By the continuity of ℓandeT, we obtain that
limn→∞/integraldisplay
Rd×Yℓd(eT)#Qn=limn→∞/integraldisplay
Rd×Ymin/braceleftbigℓ◦eT,C′/bracerightbig
dQn
=/integraldisplay
Rd×Ymin/braceleftbigℓ◦eT,C′/bracerightbig
dQ∗
=/integraldisplay
Rd×Yℓd(eT)#Q∗
for a sufﬁciently large constant C′>0. In addition, the functional Ain (5.7) is lower
semicontinuous, and we can choose Ak∈Cb/parenleftBig
C([0,T];Rd)/parenrightBig
,k=1, 2, . . . , such that Akր
Aask→∞by [3, Theorem 10.2]. Then, we get that for each k∈N,
lim infn→∞/integraldisplay
C([0,T];Rd)AdQn≥lim infn→∞/integraldisplay
C([0,T];Rd)AkdQn=/integraldisplay
C([0,T];Rd)AkdQ∗.
Hence, passing to the limit as k→∞in the above inequality, we obtain
lim infn→∞/integraldisplay
C([0,T];Rd)AdQn≥lim
k→∞/integraldisplay
C([0,T];Rd)AkdQ∗=/integraldisplay
C([0,T];Rd)AdQ∗,
by virtue of Fatou’s lemma. /squaresolid
From Lemma 5.3and Proposition 5.1we immediately obtain Theorem 1.7.J. Mach. Learn., 1(2):1-xx 25
Proof of Theorem 1.7.LetQ∗denote the minimizer of /tildewideJ. From Proposition 5.1, we can
getµQ∗∈C([0,T];P2(Rd×Y))satisfying ( 5.4) and ˜v∈L2(dµQ∗
tdt). By [ 36, Theorem
5], we have/tildewideJ(Q∗) =/hatwideJ(µQ∗,˜v). From this equality and Proposition 5.1, it follows that
/hatwideJ(µQ∗,˜v)≤/hatwideJ(µ,v)for any µ∈C([0,T];P2(Rd×Y))and v∈L2(dµtdt). /squaresolid
6 Conclusion
In this paper, we introduced the kinetic regularized learni ng problem (Problem 1.1)
and proved the existence of its minimizer in Theorem 1.5. A key idea in the proof is
to show that a sequence of curves (µn)⊂C([0,T];(P2(Rd×Y),W2)), rather than a pa-
rameter(θn)⊂L2(0,T;Rm), converges strongly. Furthermore, we attempted to idealiz e
Problem 1.1as Problem 1.6, although the relationship between this idealization and t he
existing neural network is unclear. However, considering t he minimizers of Problem 1.6
will provide essential clues for understanding deep learni ng in the future.
Our results can be further developed through a generalizati on of neural networks and
regularization terms. The directions of each generalizati on are described below and will
be subjects of future work.
6.1 For general neural network architectures
It remains to establish an existence result for neural netwo rks more general than As-
sumption 1.4. A general l-layer neural network vis a continuous vector ﬁeld satisfying
the following assumptions:
Assumption 6.1 (General l-layer neural network) .There exists C>0, it holds that
|v(x,θ)|≤C|θ|l(1+|x|), for x∈Rd, (6.1)
|v(x1,θ)−v(x2,θ)|≤C|θ|l|x1−x2|, for x1,x2∈Rd, (6.2)
forθ∈Rm.
Note that we also assume that v:Rd×Rm→Rdis continuous in Problem 1.1.
For example, vsatisfying Assumption 1.4is a 1-layer neural network. In practice, 2, 3-
layer neural network is often used. We mentioned in Subsecti on1.2that the nonlinearity of
such l-layer neural networks hinders the proof of existence theor ems, especially Claim 4.5.
To relax this nonlinearity, it may be effective to consider a mean-ﬁeld neural network
(6.3) V(x,ϑ) =/integraldisplay
Rmv(x,θ)dϑ(θ),
where ϑis a learnable probability measure on Rm. This assumption has long been known
as the Young measure [ 15,45] in optimal control theory, but it has recently been recogni zed
again as a helpful approach to shallow neural networks[ 1,17,40] and ODE-Nets [ 20,31,
38]. The author is in the process of conducting further theoret ical research using this net-
workV.J. Mach. Learn., 1(2):1-xx 26
AH1-Regularization
We discuss the existence of a minimizer in the same problem se tting as [ 57].
Problem A.1 (H1-regularized learning problem) .Letλ>0 be a constant, let Ybe a subset
ofRdand let v:Rd×Rm→Rdandℓ:Rd×Y → R+be continuous. Let µ0∈ P c(Rd×Y)
a given input data. Set
(A.1) JH1(µ,θ):=/integraldisplay
Rd×YℓdµT+λ
2/ba∇dblθ/ba∇dbl2
H1(0,T;Rm)
forµ∈C/parenleftBig
[0,T];(P2(Rd×Y),W2)/parenrightBig
and θ∈H1(0,T;Rm). The H1-regularized learn-
ing problem constrained by ODE-Net is posed as the following constrained minimization
problem:
inf/braceleftBig
JH1(µ,θ)µ∈C/parenleftBig
[0,T];(P2(Rd×Y),W2)/parenrightBig
,θ∈H1(0,T;Rm)/bracerightBig
,
subject to
/braceleftbigg∂tµt+div x(v(•,θt)µt)=0,
µt|t=0=µ0,(A.2)
We note that the constraint ( A.2) is the same as ( 1.2).
For Problem A.1, we can obtain an existence result without Assumption 1.4.
Theorem A.2 (Existence theorem for Problem A.1).Under Assumptions 1.3and6.1, there ex-
ists a minimizer (µ,θ)∈C/parenleftBig
[0,T];(P2(Rd×Y),W2)/parenrightBig
×H1(0,T;Rm)of(A.1)in Problem A.1.
Before the proof of Theorem A.2, we prepare a lemma similar to Lemma 4.3.
Lemma A.3. Letθ∈H1(0,T;Rm)and let µ∈C([0,T];(P2(Rd×Y),W2))be a distributional
solution of (A.2)corresponding to a vector ﬁelds (v(•,θt))t. If Assumption 6.1holds, there exists
a radius R∗=R∗(µ0,f,T,/ba∇dblθ/ba∇dblH1(0,T;Rm))>0such that
supp µt⊂BRd×Y(R∗)for all t∈[0,T].
Proof. The strategy of the proof is the same as that of the proof of Lem ma4.3. By Assump-
tion 6.1and Sobolev inequality, we have
/integraldisplayT
0/integraldisplay
Rd×Y|v(x,θt)|dµtdt
≤C/integraldisplayT
0/integraldisplay
Rd×Y|θt|l(1+|x|)dµtdt
≤C/ba∇dblθ/ba∇dbll
Ll(0,T;Rm)/parenleftBigg
1+sup
t∈[0,T]/integraldisplay
Rd×Y|x|dµt/parenrightBiggJ. Mach. Learn., 1(2):1-xx 27
≤C/ba∇dblθ/ba∇dbll
H1(0,T;Rm)/parenleftBigg
1+sup
t∈[0,T]/radicalBigg/integraldisplay
Rd×Y|x|2dµt/parenrightBigg
=C/ba∇dblθ/ba∇dbll
H1(0,T;Rm)/parenleftBigg
1+sup
t∈[0,T]W2(µt,δ)/parenrightBigg
<∞.
Also, again using Assumption 6.1and Sobolev inequality, we can estimate |Xt(x)|in the
proof of Lemma 4.3by/ba∇dblθ/ba∇dblH1(0,T;Rm). /squaresolid
Proof of Theorem A.2.Set
S=/braceleftBig
(µ,θ)∈C/parenleftBig
[0,T];(P2(Rd×Y),W2)/parenrightBig
×H1(0,T;Rm)(µ,θ)satisﬁes ( A.2)/bracerightBig
.
It is obvious that S/\e}atio\slash=∅and 0≤JH1<∞onS. Then, we can take a minimizing sequence
((µn,θn))∞
n=1⊂Ssuch that JH1(µn,θn)→infSJH1asn→∞. By the second term of ( A.1),
there exists a constant C>0 such that
(A.3)λ
2/ba∇dblθn/ba∇dbl2
H1≤C,
for all n∈N. From Lemma 4.1, Assumption 6.1, (A.3) and the Sobolev inequality, we
have for 0 ≤t<s≤T,
W2(µn
t,µn
s)2≤(s−t)/integraldisplays
t/integraldisplay
Rd|v(x,θn
τ)|2dµn
τ(x)dτ
≤C(s−t)/integraldisplays
t/integraldisplay
Rd|θτ|2l/parenleftBig
1+|x|2/parenrightBig
dµn
τ(x)dτ
≤C(s−t)/integraldisplays
t/integraldisplay
Rd|θτ|2l/parenleftBig
1+R∗2/parenrightBig
dµn
τ(x)dτ
≤C/ba∇dblθ/ba∇dbl2l
L2l(0,T;Rm)(s−t)
≤C/ba∇dblθ/ba∇dbl2l
H1(0,T;Rm)(s−t)
≤C(s−t),
where R∗>0 is the constant appeared in Lemma A.3. Hence, there exist a subsequence
(n′):=(n(k))∞
k=1⊂Z>0and(µ∗,θ∗)∈C/parenleftBig
0,T;(P(Rd×Y),W2)/parenrightBig
×H1(0,T;Rm)such
that
θn′→θ∗weakly in H1(0,T;Rm), (A.4)
θn′→θ∗strongly in C(0,T;Rm), (A.5)
µn′→µ∗strongly in C/parenleftBig
0,T;(P(Rd×Y),W2)/parenrightBig
. (A.6)
Here, we used the Sobolev embedding theorem in ( A.5). By the above, we can deduce the
following claim:J. Mach. Learn., 1(2):1-xx 28
Claim A.4. The limits µ∗andθ∗satidfy (4.6)for all ζ∈C∞
c/parenleftBig
(0,T)×Rd×Y/parenrightBig
.
Proof. As in the proof of Claim 4.5, the proof is completed by taking the limits of I1toI4
in (4.8). From the proof of Claim 4.5,I1,I2→0 as n→∞. Also, I3→0 as n→∞by
the uniform convergence ( A.5), and the continuity of v(x,θ)with respect to θ. For I4, it
follows from ( 6.2), (A.4) and ( A.6) that
|I4|≤/integraldisplayT
0Lip(∇xζt·(v(•,θn
t)−v(•,θ∗
t)))W1(µn
t,µ∗
t)dt
≤C/parenleftBig
/ba∇dblθn/ba∇dbll
Ll(0,T;Rm)+/ba∇dblθ∗/ba∇dbll
Ll(0,T;Rm)/parenrightBig
sup
t∈[0,T]W1(µn
t,µ∗
t)
≤C/parenleftBig
/ba∇dblθn/ba∇dbll
H1(0,T;Rm)+/ba∇dblθ∗/ba∇dbll
H1(0,T;Rm)/parenrightBig
sup
t∈[0,T]W2(µn
t,µ∗
t)
≤Csup
t∈[0,T]W2(µn
t,µ∗
t)→0 as n→∞.
Thus we obtain the conclusion. /squaresolid
We resume the proof of Theorem A.2. From Claim A.4, we have (µ∗,θ∗)∈S. In
addition, JH1is lower semicontinuous from ( 4.9) and the weak lower semi-continuity of
theH1-norm/ba∇dbl•/ba∇dblH1(0,T;Rm). The proof is complete. /squaresolid
B Convexity Assumptions
For comparison, using the proof technique by Bonnet et al. [ 10], we show that a unique
minimizer to Problem 1.1exists. This proof technique makes use of the idea that we can re-
gard the functional Jas a univariate functional /tildewideJ(θ):=J(µθ,θ), where µθ∈C([0,T];P(Rd×
Y))is a solution of ( 1.2) for a given θ∈L2(0,T;Rm). The existence and uniqueness of the
solution can be proved by showing the convexity of /tildewideJ. For this purpose, we evaluate the
Lipschitz constant of the Fr´ echet derivative ∇θ/tildewideJof/tildewideJ.
First, recall from Lemma 3.8that µθis represented as µθ
t=/parenleftbig
Φθ(0,t;•)×IdY/parenrightbig
#µ0using
a ﬂow map Φθ:[0,T]2×Rd→Rd,θ∈L2(0,T;Rm)according to the ODE
(B.1)

∂tΦθ(t0,t;x)=v/parenleftBig
Φθ(t0,t;x),θt/parenrightBig
,
Φθ(t0,t0;x) =x.
We note here that from ( 1.3) it can be veriﬁed that the Lipschitz continuity assumption
(3.4) is satisﬁed, as in the proof of Lemma 4.3. The derivative of Φwith respect to θcan be
described by a linearization of ( B.1).J. Mach. Learn., 1(2):1-xx 29
Lemma B.1 (Taylor expansion of Φθ).Suppose that the neural network v satisﬁes Assump-
tion 1.4and f :Rd→Rpis differentiable. Then, for every θ,ϑ∈L2(0,T;Rm), the Taylor
expansion
Φθ+ǫϑ(t0,t;x) =Φθ(t0,t;x)+ǫ/integraldisplayt
0∆θ
(s,t)(x)ϑsf/parenleftBig
Φθ(t,s;x)/parenrightBig
ds+o(ǫ)
holds in C/parenleftBig
[0,t]×supp µ0;Rd×Y/parenrightBig
, where, for (t0,x)∈[0,T]×Rd, the map [0,T]∋t/ma√sto−→
∆θ
(t0,t)(•)∈C/parenleftBig
Rd;Rd×d/parenrightBig
is the unique solution of the linearized Cauchy problem
(B.2)

∂t∆θ
(t0,t)(x)=θtJ f/parenleftBig
Φθ(t0,t;x)/parenrightBig
∆θ
(t0,t)(x),
∆θ
(t0,t0)(x) =IdRd,
where J f :Rd→Rpdenotes the Jacobian matrix of f .
Proof. See [ 13, Theorem 3.2.6]. /squaresolid
To evaluate the Lipscitz continuity of ∇θ/tildewideJ, we need to estimate the variation of ∆θ
(0,t)
with respect to θ. This evaluation requires us to impose a further assumption onv(x,θ) =
θf(x)in addition to Assumption 1.4. In the following, we will denote R∗(/ba∇dblθ/ba∇dbl)the same
radius as in Lemma 4.3.
Assumption B.2 (Strong smoothness on v).The function fis twice continuously differen-
tiable, and for given θ∈L2(0,T;Rm)and(x,y)∈B(R∗(/ba∇dblθ/ba∇dbl)),fsatisﬁes/ba∇dblf/ba∇dblC1(Rp;Rd)<
∞.
This assumption correspond to [ 10, Assumption 2]. Under this assumption, we can
show the Lipschitz continuity by the same argument as in the p roof of [ 10, Lemma 3.1].
Lemma B.3 (Fr´ echet-differentiablity of the loss functional) .The sum of loss and kinetic regu-
larization
Jℓ:θ/ma√sto−→/integraldisplay
Rd×Yℓdµθ
T+λ
2/integraldisplayT
0/integraldisplay
Rd|v(x,θ)|2dµθ
tdt
is Fr´ echet-differentiable. In addition, for θ1,θ2∈L2(0,T;Rm), there exists C/parenleftbig
λ,/vextenddouble/vextenddoubleθ1/vextenddouble/vextenddouble,/vextenddouble/vextenddoubleθ2/vextenddouble/vextenddouble/parenrightbig>0
such that /vextenddouble/vextenddouble/vextenddouble∇Jℓ(θ1)−∇ Jℓ(θ2)/vextenddouble/vextenddouble/vextenddouble≤C/parenleftBig
λ,/vextenddouble/vextenddouble/vextenddoubleθ1/vextenddouble/vextenddouble/vextenddouble,/vextenddouble/vextenddouble/vextenddoubleθ2/vextenddouble/vextenddouble/vextenddouble/parenrightBig/vextenddouble/vextenddouble/vextenddoubleθ1−θ2/vextenddouble/vextenddouble/vextenddouble.
From Lemma B.3, the following corollary follows immediately.
Corollary B.4 (Semiconvexity for the parameter θ).The functional
(B.3)/tildewideJ:θ/ma√sto−→ J(µθ,θ) =/integraldisplay
Rd×Yℓdµθ
T+/integraldisplayT
0/integraldisplay
Rd/parenleftbiggλ
2|v(x,θt)|2+ǫ
2|θt|2/parenrightbigg
dµθ
t(x)dtJ. Mach. Learn., 1(2):1-xx 30
satisﬁes
/tildewideJ/parenleftBig
(1−ζ)θ1+ζθ2/parenrightBig
≤(1−ζ)/tildewideJ/parenleftBig
θ1/parenrightBig
+ζ/tildewideJ/parenleftBig
θ2/parenrightBig
−/parenleftBig
ǫ−C/parenleftBig
λ,/vextenddouble/vextenddouble/vextenddoubleθ1/vextenddouble/vextenddouble/vextenddouble,/vextenddouble/vextenddouble/vextenddoubleθ2/vextenddouble/vextenddouble/vextenddouble/parenrightBig/parenrightBigζ(1−ζ)
2/vextenddouble/vextenddouble/vextenddoubleθ1−θ2/vextenddouble/vextenddouble/vextenddouble2
,
for any θ1,θ2∈L2(0,T;Rm)andζ∈[0, 1]. Here C/parenleftbig
λ,/vextenddouble/vextenddoubleθ1/vextenddouble/vextenddouble,/vextenddouble/vextenddoubleθ2/vextenddouble/vextenddouble/parenrightbig
is the same positive number
as in Lemma B.3.
By this corollary, /tildewideJis strongly convex on a L2ball if ǫis sufﬁciently large compared to
other parameters such as λand T. This plays an essential role in the proof of Theorem B.5
below.
Theorem B.5 (Existence and uniqueness of minimizer of /tildewideJ).Suppose that Assumptions 1.4
andB.2Ifǫ>0in(B.3)is sufﬁciently large, there exists θ∈L2(0,T;Rm)which minimize /tildewideJ, and
θis a unique minimizer of /tildewideJ.
The proof is carried out using the direct method of Calculus o f Variations as in Theo-
rem 1.5.
Proof. It is clear that 0 ≤inf/tildewideJ<+∞, then we can take a minimizing sequence (θn)∞
n=1⊂
L2(0,T;Rm)such that/tildewideJ(θn)→inf/tildewideJasn→∞. Thus, there exists C>0 independent of
nsuch that ( 4.3) holds. Then there exists a subsequence (n′)⊂Z>0such that ( 4.5). In
addition, by Corollary B.4, we see that there exists ǫ>0 such that/tildewideJis convex on B(2C/ǫ).
Therefore, by Mazur’s lemma, there exists another minimizi ng sequence (ˆθn)∞
n=1such that
ˆθn→θinL2(0,T;Rm). Because/tildewideJis lower semicontinuous, we conclude that
J(θ)≤lim infn→∞J(ˆθn) = inf
L2(0,T;Rm)/tildewideJ,
i.e.,θis a minimizer of /tildewideJ. The uniqueness is immediately obtained from the strong con -
vexity of/tildewideJ. /squaresolid
Acknowledgement
The author, N.I., would like to thank his supervisor, Norika zu Saito, for his encourage-
ment and advice during the preparation of the paper.
References
[1] S. Akiyama and T. Suzuki. “On Learnability via Gradient M ethod for Two-Layer
Relu Neural Networks in Teacher-Student Setting.” Proceedings of the 38th Interna-
tional Conference on Machine Learning .139. 2021, pp. 152–162 (cit. on p. 25).
[2] L. Ambrosio. “Transport Equation and Cauchy Problem for BV Vector Fields.” In-
ventiones mathematicae 158.2 2004, pp. 227–260 (cit. on p. 22).J. Mach. Learn., 1(2):1-xx 31
[3] L. Ambrosio, E. Bru´ e, and D. Semola. “Lectures on Optima l Transport.” 1st ed. UNI-
TEXT. Cham: Springer International Publishing, 2021 (cit. on pp. 13,24).
[4] L. Ambrosio, N. Gigli, and G. Savar´ e. “Gradient Flows in Metric Spaces and in the
Space of Probability Measures.” 2nd ed. Lectures in Mathema tics ETH Z ¨ urich. Basel:
Birkh¨ auser Verlag, 2008 (cit. on pp. 11,12,16,18,22,24).
[5] V . I. Arnol’d. “On Functions of Three Variables.” Dokl. Akad. Nauk SSSR 1141957,
pp. 679–681 (cit. on p. 3).
[6] G. Baravdish, G. Eilertsen, R. Jaroudi, B. T. Johansson, L. Mal ´y, and J. Unger. “Learn-
ing via Nonlinear Conjugate Gradients and Depth-Varying Ne ural ODEs.” 2022.
arXiv:2202.05766v1[cs.LG] (cit. on p. 7).
[7] R. Barboni, G. Peyr´ e, and F.-X. Vialard. “On Global Conv ergence of ResNets: From
Finite to Inﬁnite Width Using Linear Parameterization.” Advances in Neural Informa-
tion Processing Systems .35. 2022, pp. 16385–16397 (cit. on pp. 3,8).
[8] J.-D. Benamou and Y. Brenier. “A Computational Fluid Mec hanics Solution to the
Monge-Kantorovich Mass Transfer Problem.” Numerische Mathematik 84.3 2000, pp. 375–
393 (cit. on p. 13).
[9] J.-D. Benamou, G. Carlier, and F. Santambrogio. “Variat ional Mean Field Games.”
Active Particles, Volume 1 : Advances in Theory, Models, and Applications . Ed. by N.
Bellomo, P . Degond, and E. Tadmor. Cham: Springer Internati onal Publishing, 2017,
pp. 141–171 (cit. on pp. 5,22).
[10] B. Bonnet, C. Cipriani, M. Fornasier, and H. Huang. “A Me asure Theoretical Ap-
proach to the Mean-Field Maximum Principle for Training Neu rODEs.” Nonlinear
Analysis 2272023, p. 113161 (cit. on pp. 4,5,8,13,20,28,29).
[11] B. Bonnet and H. Frankowska. “Differential Inclusions in Wasserstein Spaces: The
Cauchy-Lipschitz Framework.” Journal of Differential Equations 2712021, pp. 594–637
(cit. on p. 13).
[12] B. Bonnet and H. Frankowska. “On the Properties of the Va lue Function Associated
to a Mean-Field Optimal Control Problem of Bolza Type.” 2021 60th IEEE Conference
on Decision and Control . 2021, pp. 4558–4563 (cit. on p. 9).
[13] A. Bressan and B. Piccoli. “Introduction to the Mathema tical Theory of Control.”
American Institute of Mathematical Sciences, 2007 (cit. on pp.8,29).
[14] H. Brezis. “Functional Analysis, Sobolev Spaces and Pa rtial Differential Equations.”
New York, NY: Springer New York, 2011 (cit. on p. 19).
[15] C. Charles, F. Paul Raynaud, and M. Valadier. “Young Mea sures on Topological
Spaces: With Applications in Control Theory and Probabilit y Theory.” Dordrecht:
Springer Netherlands, 2004 (cit. on p. 25).
[16] R. T. Q. Chen, Y. Rubanova, J. Bettencourt, and D. K. Duve naud. “Neural Ordinary
Differential Equations.” Advances in Neural Information Processing Systems .31. 2018,
pp. 6572–6583 (cit. on p. 6).J. Mach. Learn., 1(2):1-xx 32
[17] L. Chizat and F. Bach. “On the Global Convergence of Grad ient Descent for over-
Parameterized Models Using Optimal Transport.” Advances in Neural Information
Processing Systems .31. 2018 (cit. on p. 25).
[18] G. Cybenko. “Approximation by Superpositions of a Sigm oidal Function.” Mathe-
matics of Control, Signals and Systems 2.4 1989, pp. 303–314 (cit. on pp. 3,21).
[19] Z. Ding, S. Chen, Q. Li, and S. Wright. “On the Global Conv ergence of Gradient De-
scent for Multi-Layer ResNets in the Mean-Field Regime.” 20 21. arXiv: 2110.02926v2
[cs.LG] (cit. on p. 7).
[20] Z. Ding, S. Chen, Q. Li, and S. J. Wright. “Overparameter ization of Deep ResNet:
Zero Loss and Mean-Field Analysis.” Journal of Machine Learning Research 23.48 2022,
pp. 1–65 (cit. on pp. 7,25).
[21] R. M. Dudley. “Real Analysis and Probability.” 2nd ed. C ambridge Studies in Ad-
vanced Mathematics. Cambridge University Press, 2002 (cit . on p. 11).
[22] W. E. “A Proposal on Machine Learning via Dynamical Syst ems.” Communications in
Mathematics and Statistics 5.1 2017, pp. 1–11 (cit. on pp. 1,6,7).
[23] W. E, J. Han, and Q. Li. “A Mean-Field Optimal Control For mulation of Deep Learn-
ing.” Research in the Mathematical Sciences 6.1 2019, p. 10 (cit. on pp. 5,7,8).
[24] C. Esteve, B. Geshkovski, D. Pighin, and E. Zuazua. “Lar ge-Time Asymptotics in
Deep Learning.” 2021. arXiv: 2008.02491[math.OC] (cit. on p. 4).
[25] C. Finlay, J.-H. Jacobsen, L. Nurbekyan, and A. Oberman . “How to Train Your Neu-
ral ODE: The World of Jacobian and Kinetic Regularization.” Proceedings of the 37th
International Conference on Machine Learning .119. 2020, pp. 3154–3164 (cit. on pp. 2,5,
8,14).
[26] T. Haarnoja, H. Tang, P . Abbeel, and S. Levine. “Reinfor cement Learning with Deep
Energy-Based Policies.” Proceedings of the 34th International Conference on Machin e
Learning .70. 2017, pp. 1352–1361 (cit. on p. 8).
[27] E. Haber and L. Ruthotto. “Stable Architectures for Dee p Neural Networks.” Inverse
Problems 34.1 2017, p. 014004 (cit. on pp. 1,6).
[28] K. He, X. Zhang, S. Ren, and J. Sun. “Deep Residual Learni ng for Image Recogni-
tion.” 2016 IEEE Conference on Computer Vision and Pattern Recogni tion (CVPR) . 2016,
pp. 770–778 (cit. on pp. 3,6).
[29] M. Herty, A. Th ¨ unen, T. Trimborn, and G. Visconti. “Con tinuous Limits of Resid-
ual Neural Networks in Case of Large Input Data.” Communications in Applied and
Industrial Mathematics 13.1 2022, pp. 96–120 (cit. on pp. 4,13).
[30] K. Hornik. “Approximation Capabilities of Multilayer Feedforward Networks.” Neu-
ral Networks 4.2 1991, pp. 251–257 (cit. on p. 21).
[31] J.-F. Jabir, D. ˇSiˇ ska, and Ł. Szpruch. “Mean-Field Neural ODEs via Relaxed Optimal
Control.” 2021. arXiv: 1912.05475v3[math.PR] (cit. on pp. 7,8,25).
[32] J. L. Kelley. “General Topology.” New York: Springer Ne w York, 1975 (cit. on p. 9).J. Mach. Learn., 1(2):1-xx 33
[33] A. N. Kolmogorov. “On the Representation of Continuous Functions of Many Vari-
ables by Superposition of Continuous Functions of One Varia ble and Addition.”
Dokl. Akad. Nauk SSSR 1141957, pp. 953–956 (cit. on p. 3).
[34] J.-M. Lasry and P .-L. Lions. “Jeux `A Champ Moyen. Ii – Horizon Fini Et Contrˆ ole
Optimal.” Comptes Rendus Mathematique 343.10 2006, pp. 679–684 (cit. on p. 21).
[35] J.-M. Lasry and P .-L. Lions. “Mean Field Games.” Japanese Journal of Mathematics 2.1
2007, pp. 229–260 (cit. on pp. 5,21).
[36] S. Lisini. “Characterization of Absolutely Continuou s Curves in Wasserstein Spaces.”
Calculus of Variations and Partial Differential Equations 28.1 2007, pp. 85–120 (cit. on
pp.22,25).
[37] E. Lorin. “Derivation and Analysis of Parallel-in-Tim e Neural Ordinary Differential
Equations.” Annals of Mathematics and Artiﬁcial Intelligence 88.10 2020, pp. 1035–1059
(cit. on p. 6).
[38] Y. Lu, C. Ma, Y. Lu, J. Lu, and L. Ying. “A Mean Field Analys is of Deep ResNet and
Beyond: Towards Provably Optimization via Overparameteri zation from Depth.”
Proceedings of the 37th International Conference on Machin e Learning .119. 2020, pp. 6426–
6436 (cit. on pp. 7,25).
[39] S. Massaroli, M. Poli, J. Park, A. Yamashita, and H. Asam a. “Dissecting Neural ODEs.”
Advances in Neural Information Processing Systems .33. 2020, pp. 3952–3963 (cit. on
p.7).
[40] S. Mei, A. Montanari, and P .-M. Nguyen. “A Mean Field Vie w of the Landscape of
Two-Layer Neural Networks.” Proceedings of the National Academy of Sciences 115.33
2018, E7665–E7671. eprint: https://www.pnas.org/doi/pdf/10.1073/pnas.180657911 5
(cit. on p. 25).
[41] K. P . Murphy. “Machine Learning : A Probabilistic Persp ective.” Cambridge, Mass.
[u.a.]: MIT Press, 2013 (cit. on p. 7).
[42] V . Nair and G. E. Hinton. “Rectiﬁed Linear Units Improve Restricted Boltzmann
Machines.” Proceedings of the 27th International Conference on Intern ational Conference
on Machine Learning . 2010, pp. 807–814 (cit. on p. 6).
[43] C. Orrieri, A. Porretta, and G. Savar´ e. “A Variational Approach to the Mean Field
Planning Problem.” Journal of Functional Analysis 277.6 2019, pp. 1868–1957 (cit. on
p.13).
[44] A. Pinkus. “Approximation Theory of the MLP Model in Neu ral Networks.” Acta
Numerica 81999, pp. 143–195 (cit. on p. 21).
[45] N. Pogodaev. “Optimal Control of Continuity Equations .”Nonlinear Differential Equa-
tions and Applications 23.2 2016, p. 21 (cit. on pp. 5,25).
[46] A. Queiruga, N. B. Erichson, L. Hodgkinson, and M. W. Mah oney. “Stateful ODE-
Nets Using Basis Function Expansions.” Advances in Neural Information Processing
Systems .34. 2021, pp. 21770–21781 (cit. on p. 6).J. Mach. Learn., 1(2):1-xx 34
[47] L. Ruthotto, S. J. Osher, W. Li, L. Nurbekyan, and S. W. Fu ng. “A Machine Learning
Framework for Solving High-Dimensional Mean Field Game and Mean Field Con-
trol Problems.” Proceedings of the National Academy of Sciences 117.17 2020, pp. 9183–
9193 (cit. on p. 5).
[48] M. E. Sander, P . Ablin, M. Blondel, and G. Peyr´ e. “Momen tum Residual Neural Net-
works.” Proceedings of the 38th International Conference on Machin e Learning .139. 2021,
pp. 9276–9287 (cit. on p. 3).
[49] M. E. Sander, P . Ablin, and G. Peyr´ e. “Do Residual Neura l Networks Discretize Neu-
ral Ordinary Differential Equations?” Advances in Neural Information Processing Sys-
tems . Ed. by A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho. 2022 (cit . on pp. 3,
7).
[50] F. Santambrogio. “Lecture Notes on Variational Mean Fi eld Games.” Mean Field Games:
Cetraro, Italy 2019 . Ed. by P . Cardaliaguet and A. Porretta. Cham: Springer Inte rna-
tional Publishing, 2020, pp. 159–201 (cit. on pp. 5,21,22).
[51] C. Sarrazin. “Lagrangian Discretization of Variation al Mean Field Games.” SIAM
Journal on Control and Optimization 60.3 2022, pp. 1365–1392. eprint: https://doi.org/10.1137/20M1377 
(cit. on p. 22).
[52] A. Scagliotti. “Ensembles of Afﬁne-Control Systems wi th Applications to Deep Learn-
ing.” PhD thesis. Scuola Internazionale Superiore di Studi Avanzati, 2022 (cit. on
p.3).
[53] A. Scagliotti. “Optimal Control of Ensembles of Dynami cal Systems.” ESAIM: COCV
292023, p. 22 (cit. on p. 5).
[54] S. Sonoda and N. Murata. “Double Continuum Limit of Deep Neural Networks.”
ICML Workshop Principled Approaches to Deep Learning .1740 . 2017 (cit. on pp. 1,6).
[55] D. A. Sprecher. “On the Structure of Continuous Functio ns of Several Variables.”
Trans. Amer. Math. Soc. 1151965, pp. 340–355 (cit. on p. 3).
[56] T. Teshima, K. Tojo, M. Ikeda, I. Ishikawa, and K. Oono. “ Universal Approxima-
tion Property of Neural Ordinary Differential Equations.” 2020. arXiv: 2012.02414
[cs.LG] (cit. on p. 7).
[57] M. Thorpe and Y. van Gennip. “Deep Limits of Residual Neu ral Networks.” Research
in the Mathematical Sciences 10.1 2023, p. 6 (cit. on pp. 4,5,7,13,26).
[58] F.-X. Vialard, R. Kwitt, S. Wei, and M. Niethammer. “A Sh ooting Formulation of
Deep Learning.” Advances in Neural Information Processing Systems .33. 2020, pp. 11828–
11838 (cit. on p. 3).
[59] C. Villani. “Optimal Transport: Old and New.” A Series o f Comprehensive Studies
in Mathematics. Springer Berlin Heidelberg, 2009 (cit. on p .22).
[60] C. Villani. “Topics in Optimal Transportation (Gradua te Studies in Mathematics).”
American Mathematical Society, 2003 (cit. on pp. 11,21).
[61] G. Yang, D. Yu, C. Zhu, and S. Hayou. “Tensor Programs VI: Feature Learning in In-
ﬁnite Depth Neural Networks.” The Twelfth International Conference on Learning Rep-
resentations . 2024 (cit. on p. 3).J. Mach. Learn., 1(2):1-xx 35
[62] Y. D. Zhong, B. Dey, and A. Chakraborty. “Symplectic ODE -Net: Learning Hamil-
tonian Dynamics with Control.” International Conference on Learning Representations .
2020 (cit. on p. 6).