1
Task-oriented Explainable Semantic
Communications
Shuai Ma, Weining Qiao, Youlong Wu, Hang Li, Guangming Shi, Fellow, IEEE ,
Dahua Gao, Yuanming Shi, Shiyin Li, and Naofal Al-Dhahir, Fellow, IEEE
Abstract
Semantic communications utilize the transceiver computing resources to alleviate scarce transmis-
sion resources, such as bandwidth and energy. Although the conventional deep learning (DL) based
designs may achieve certain transmission efÔ¨Åciency, the uninterpretability issue of extracted features
is the major challenge in the development of semantic communications. In this paper, we propose an
explainable and robust semantic communication framework by incorporating the well-established bit-
level communication system, which not only extracts and disentangles features into independent and
semantically interpretable features, but also only selects task-relevant features for transmission, instead of
all extracted features. Based on this framework, we derive the optimal input for rate-distortion-perception
theory, and derive both lower and upper bounds on the semantic channel capacity. Furthermore, based
on the -variational autoencoder ( -V AE), we propose a practical explainable semantic communication
system design, which simultaneously achieves semantic features selection and is robust against semantic
channel noise. We further design a real-time wireless mobile semantic communication proof-of-concept
prototype. Our simulations and experiments demonstrate that our proposed explainable semantic com-
munications system can signiÔ¨Åcantly improve transmission efÔ¨Åciency, and also verify the effectiveness
of our proposed robust semantic transmission scheme.
Index Terms
Explainable semantic communications, feature selection, semantic communications prototype
I. I NTRODUCTION
With the advent of augmented reality (AR), virtual reality (VR), holographic communications,
autonomous vehicular networks, and industrial Internet of Things (IIoT), it is envisioned that
Shuai Ma is with Pengcheng Laboratory, Shenzhen, 518066, China (e-mail: mash01@pcl.ac.cn).arXiv:2302.13560v1  [eess.SP]  27 Feb 20232
existing networks may soon reach a resource bottleneck due to stringent requirements [1], [2],
such as ultra-high data rate, ultra-reliability, and low latency. To meet the above-mentioned
requirements, investigations on the sixth generation communications (6G) are well underway
and promise more powerful capacities than the Ô¨Åfth-generation communications (5G) [3]. From
the Ô¨Årst generation communications (1G) to 5G, the communication networks primarily focus
on Ô¨Ånding new resources and technologies to expand the channel capacity [4]. One approach is
to seek the usage of large bandwidth, such as terahertz (THz) communications and visible light
communication (VLC). Another approach is to explore the spatial domain, like ultra-massive
MIMO and intelligent metasurfaces. However, given the hardware and physical limitations, the
channel capacity may not keep increasing at the rate we desire to satisfy the aforementioned
beyond-5G applications [5], [6].
In recent years, semantic communications, in which only task-relevant information is extracted
and transmitted to the receiver, have received increasing attention by both academia and the
industry [7]‚Äì[13]. Rather than increasing the channel capacity as in the conventional techniques,
semantic communications exploit the computing power at the transceivers to alleviate the cost of
transmission resources. The classic Shannon information theory focuses on ‚ÄúHow accurately can
the symbols be transmitted?‚Äù, which ignores the meaning of the transmitted messages. Instead,
semantic communications [14] consider ‚ÄúHow precisely do the transmitted symbols convey the
desired meaning?‚Äù Thus, it is possible to improve the system efÔ¨Åciency at the semantic level,
not only at the pure bit level.
The classical separation theorem [15] states that, as the data size goes to inÔ¨Ånity, separating
source coding and channel coding can achieve the optimal performance over a memoryless
communication channel. However, for Ô¨Ånite number of bits transmission, the performance of
such separated structure will degrade. This issue also arises in semantic communications. Various
deep learning (DL) based joint source-channel coding (JSCC) schemes have been investigated for
text [12], [16], [17], image [18]‚Äì[23], speech [24], [25], and multimodal data [26] transmission.
SpeciÔ¨Åcally, for text semantic transmission, the JSCC schemes have been designed by exploiting
architectures like the recurrent neural network (RNN) [16], Transformer [17], [27], autoencoder
(AE) [28], adaptive Universal Transformer [29], and deep neural network (DNN) [30]. For
image semantic transmission, a masked auto-encoder (MAE) architecture with Transformer was
designed in [18] to combat adversarial samples noise. Convolutional neural networks (CNNs)
based JSCC schemes were designed for the time-invariant and fading wireless channels in [19].3
Neural error correcting and source trimming (NECST) codes were studied in [22]. For Ô¨Ånite
bit transmission, an attention DL based JSCC method was designed in [23]. By exploring
the channel output feedback, an AE-based JSCC scheme was developed in [20] to improve
the quality of image transmission. By combining an AE with orthogonal frequency division
multiplexing (OFDM), a JSCC wireless image transmission scheme was presented in [21] over
multipath fading channels. By leveraging reinforcement learning (RL), a joint semantics-noise
coding (JSNC) mechanism was designed in [31]. A DNN based JSCC scheme was designed
in [32] for adaptive rate control in wireless image transmission. Based on AE, a SNR-adaptive
deep JSCC scheme is proposed in [33] for multi-user wireless image transmission. To tackle the
variational information bottleneck, the authors in [34] investigated task-oriented communication
for edge inference, where a low-end edge device extracts the feature vector of a local data
sample and transmits to a powerful edge server for processing. Besides, for the speech semantic
transmission, AE based wave-to-vector architecture and squeeze-and-excitation (SE) attention
network have been developed in [24] and [25], respectively. For visual question answering,
the memory-attention-composition neural network was designed in [26] for multi-modal data
semantic communications.
However, most of the existing works on semantic communications [16]‚Äì[26] are based on DL
techniques, in which the DL model is basically a black box. Thus, the extracted semantic feature
vectors in these works are unexplainable (hidden) representations, and the uninterpretability of the
extracted features restricts further processing and exploitation of semantic features. For example,
due to the uninterpretability, the unintended features will also be transmitted to the receiver, which
wastes transmission resources and reduces the efÔ¨Åciency of semantic communications.
Moreover, most of the existing semantic communication investigations [16]‚Äì[26] completely
redesign the source and channel module over the conventional system, which are impractical and
not compatible with the existing communication networks. Because there is a large number of
practical standards and hardware for 5G physical layer, it will lead to a huge waste of resources
and costs by replacing physical layer techniques with DL-based semantic JSCC techniques.
Therefore, how to design efÔ¨Åcient and 5G-compatible practical semantic communications is a
critical issue.
To address the above the two key challenges of the semantic communications, we propose an
explainable and robust semantic communication framework in this paper, which is compatible
with existing communication systems. We show that the proposed framework can achieve a4
higher transmission efÔ¨Åciency than the existing inexplicable semantic communication systems.
The main contributions of this paper are summarized as follows:
We propose an explainable and easy-to-implement semantic communication framework
based on the bit-level communication systems, which includes a novel semantic encoder, as
well as the corresponding decoder, feature selection and semantic channel. The innovation
of the proposed framework is threefold: i) The semantic encoder/decoder aims to, not only
extract the independent and explainable semantic information as semantic source coding ,
but also alleviate the ambiguity of the semantic information inÔ¨Çuenced by the quantization
and channel noise as semantic channel coding ; ii) The feature selection module follows
the semantic encoder, to choose only the task-relevant features for transmission, which can
further reduce the transmission load; iii) The framework has an explicit deÔ¨Ånition of seman-
tic channels, which incorporates the key modules of the bit-level communication systems.
SpeciÔ¨Åcally, the semantic channel takes both quantization error (or noise) and physical
channel noise into account since those noise sources may lead to semantic information
ambiguity, and the semantic channel capsulizes the conventional bit-level communication
systems, which implies that the proposed framework can be more easily implemented
compared to the JSCC schemes.
Then, we propose two information-theoretic metrics for our semantic communication frame-
work. In terms of the information compression of the semantic encoder, we derive the the
optimal distribution of the reconstruction signal of the rate-distortion-perception function
for semantic information extraction. Moreover, to quantify the semantic information trans-
mission, we derive both upper and lower bounds for the semantic channel capacity, which
are shown to be tight when the quantization noise tends to zero.
Based on our framework, we further propose a feasible design of the explainable seman-
tic communication system. SpeciÔ¨Åcally, this design includes a robust -V AE lightweight
unsupervised learning network, where a weighted parameter is added to the Kullback-
Leibler (KL) divergence term of the variational autoencoder (V AE) network loss function,
in order to make the latent representations effectively disentangled. Moreover, to enhance
transmission robustness, the semantic channel noise is added to the extracted features during
semantic networks training.
Finally, we implement the above semantic communication design, and propose a wireless5
mobile semantic communication proof-of-concept prototype. Applying the portable Rasp-
berry Pi 4 Model B and Wi-Fi, the developed prototype can run the proposed robust -V AE
semantic system in real time. Our experiments demonstrate that our proposed semantic
communication system can achieve better performance than existing benchmarks.
The rest of this paper is organized as follows. The explainable semantic communications
framework is presented in Section II. Section III provides the information-theoretic metrics of
semantic communications. In Section IV , we propose a -V AE based robust and explainable
semantic communications system. In Section V , we present the semantic communication system
prototype design and implementation. In Section VI, we evaluate the proposed explainable
semantic communication system. Finally, we conclude the paper in Section VII. Table I and
II presents the means of the key notations and key acronyms in this paper, respectively.
TABLE I: Key Notations and Meanings
Variables Meanings
S=fskgK
k=1Semantic information with Kfeatures
sk Thekth semantic feature
X Source data
Z=fzlgL
l=1The extracted semantic feature vector with Lfeatures
zl Thei-th extracted semantic feature
L Semantic feature index set
Lsel Selected semantic feature index set
Xs=fzlgl2LselSelected semantic features
Ys=fbzlgl2Lsel Estimated semantic features
bZ Reconstructed feature set
bX Decoded data
TABLE II: Key Acronyms and Meanings
Acronyms Meanings
JSCC Joint source-channel coding
V AE Variational autoencoder
KL Kullback- Leibler
ANGC Additive non-Gaussian noise channel
ELBO Evidence lower bound
GPU Graphics processing unit
PSNR Peak signal-to-noise ratio6
II. E XPLAINABLE SEMANTIC COMMUNICATION FRAMEWORK
Most existing studies replace the traditional source coding and channel coding modules by
deep learning-based studies source-channel coding, which greatly changes the structure of the
existing communication systems. In this paper, we propose a semantic communication framework
incorporating the key modules of the conventional communication system (e.g., 5G).
Channel 
Encoder Source 
Encoder  Semantic 
Encoder Semantic 
 Source 
Data 
Feature 
Selection 
Channel 
Decoder Source 
Decoder Physical 
 Channel Semantic Channel 
Channel 
Encoder Source 
Encoder Channel 
Decoder Source 
Decoder Physical 
Channel Semantic Channel Disentangled 
Features  Semantic 
Decoder Semantic 
Destination 
Feature 
Completion X
sXsYZ ÀÜZÀÜX
+
PN+
QN
) Quan ¬∑Disentangled 
Features Data 
TXDQWL]HU Quantizer Sender 
Knowledge 
Base Receiver 
Knowledge 
Base Knowledge Bases 
Fig. 1: Explainable and robust semantic communications framework
As shown in Fig. 1, the proposed explainable semantic communication framework includes a
semantic source, sender knowledge base, semantic encoder, semantic channel, receiver knowledge
base, semantic decoder, and semantic destination. Note that the proposed framework introduces
a semantic-level transmission on the top of bit-level transmission. Clearly, such a framework
does not require the extra redesign over the existing physical standards, protocols and products,
which makes the application of semantic communications more practical. Next, we will describe
each module in detail.
A. Knowledge Bases
The knowledge base contains all the necessary information that can facilitate the communica-
tion at the semantic level. SpeciÔ¨Åcally, the knowledge base includes background knowledge and
training dataset. The background knowledge is used to facilitate the semantic feature extraction
and selection in the semantic transmitter. The training dataset is used for training the parameters
of the semantic encoder and decoder. The sender may choose different semantic knowledge bases
according to different tasks, scenarios and recipients. For example, when the communication is7
triggered between people in different countries, it may be necessary to sample multiple language
databases. In general, the sender and the receiver share some common knowledge, which may
act as a special kind of side information to improve coding efÔ¨Åciency.
B. Semantic Sources
The semantic source produces original data, such as pictures, videos, voices, and texts. The
generated data contains certain semantic information to be shared with the semantic destination.
Assume that the semantic information includes KfeaturesS=fskgK
k=1psou(s)(data gener-
ative factors), where skdenotes the kth semantic feature, and the joint probability distribution is
psou(s). Further, assume that the KfeaturesfskgK
k=1are independent, i.e., psou(s) =KQ
k=1psou(sk),
wherepsou(sk)denotes the probability distribution of sk. Thus, the entropy of the semantic source
is given as
H(S) = KX
k=1psou(sk) log2psou(sk): (1)
The semantic source can generate the data Xpdata(x), which can be images, text, sound
or video. Generally, the generated data need to include both the intended features and some
redundant features to make the whole semantic data complete. Thus, the data generation is
deÔ¨Åned asps2d
xjfskgK
k=1
, and the probability distribution function (PDF) of data Xis given
as
pdata(x) =X
s1;:::;sKps2d
xjfskgK
k=1KY
k=1psou(sk): (2)
The entropy of the semantic data xis given as
H(X) = X
xpdata(x) log2pdata(x): (3)
Based on (1) and (2), H(X)can be further expressed as
H(X) =H(S) +H(XjS) H(SjX): (4)
C. Semantic Encoder
Based on the knowledge base, the generated message Xwill be processed by the semantic
encoder, which is a joint semantic source and channel encoder. More speciÔ¨Åcally, it extracts8
semantic information or the semantic features of the message X, and outputs the disentangled
and explainable features Z, which can be viewed as a semantic source encoder. On the other
hand, in order to reduce the ambiguity incurred by the quantization error and channel noise, the
semantic encoder needs to improve the robustness against the semantic channel noise, which
can be viewed as a semantic channel encoder.
The semantic encoder extracts a low-dimensional semantic features vector Zpfea(z)from
the dataX. Letpd2f(zjx)denote the conditional PDF of the feature zgiven datax. Thus, the
PDF of the extracted feature (sub-vectors) pfea(z)is given as
pfea(z) =X
xpd2f(zjx)pdata(x): (5)
The encoder is required to regulate the extracted features into Lindependent features Z=
fzlgL
l=1, which satisfy
pfea(z) =LY
l=1pfea(zl); (6)
wherep(zl)denote the PDF for feature zl. In summary, the extracted feature zis required to
haveLdisentangled interpretable semantic features fzlgL
l=1, whose corresponding neural network
output is explainable and understandable by the human. For convenience, we let L=f1;:::;Lg
denote the index set of the disentangled semantic features. Note that such a requirement can
be met if the semantic encoder is designed in a sophisticated manner. In Section IV , we will
introduce a feasible system design that has such capability.
D. Feature Selection
It should be noted that the obtained features Zcould contain more information than what the
receiver is interested in. Thus, after extracting the disentangled features fzlgL
l=1, only the subset
of featuresfzlgL
l=1that are of interest to the receiver should be transmitted, and the rest of the
features can be viewed as the ‚Äúredundancy‚Äù. We will present more discussions of this issue via
experiments in Section VI.
Given the task requirement, let LselL denote the selected feature index set, then the selected
set of features is given as
Xs=fzlgl2Lsel: (7)9
Thus, feature selection will reduce the amount of data sent, and the corresponding reduction
isfzlgl2LnL sel. Then,Xswill be sent to the semantic channel.
E. Semantic Channel
After the feature selection module, the task-oriented features are selected and ready to send.
Since the quantization error and channel noise both could incur semantic information ambiguity,
we deÔ¨Åne the semantic channel with channel law p(ysjxs)as a virtual channel including the signal
quantizer and the bit-level communication system, as shown in Fig. 1. Here, Ys=fbzlgl2Lsel
pr(ys)represents the set of estimated features after the transmissions over the semantic channel,
andbzldenotes the estimated feature of zl.
Generally, the semantic noise could include various factors including source errors, feature
extraction errors, knowledge base ambiguities, adversarial injections, quantization noise, physical
channel noise, etc. In our framework, the semantic channel noise Nsis the distortion between
the selected semantic feature Xs=fzlgl2Lseland the estimated semantic feature Ys=fbzlgl2Lsel,
which mainly depends on the quantization noise and physical channel noise.
1) Quantization Noise: The quantization noise is caused by the traditional communication
operation modules, such as the source encoder (or decoder) or channel encoder (or decoder),
which may also lead to semantic ambiguity. In order to reduce the number of transmitted bits,
the semantic feature xswill be converted to a compressible binary stream using few bits. To
represent xswith a Ô¨Ånite number of bits, we need to map it to a discrete space. SpeciÔ¨Åcally, a
Ô¨Ånite quantizer maps the semantic feature xstoxb, whose values are then quantized to Mlevels
C=fc1;:::;cMg, i.e.,
xb= Quan (xs); (8)
where Quan ()is a quantization operator. Since the number of dimensions dim (xb)and the
number of levels Lare Ô¨Ånite, the entropy of quantized semantic data is given as
H(xb)dim (xb) log2M: (9)
In this paper, we consider the uniform distributed quantization noise nQ, i.e.,
pNQ(x) =1
b a;axb; (10)10
whereaandbare the lower and upper bounds of quantization noise NQ.
2) Physical Channel Noise: The physical channel noise exists ubiquitously in physical com-
munications and is caused by physical channel impairments, such as additive white Gaussian
noise (AWGN), interference, etc. It is noted that the errors caused by channel propagation usually
occur before channel decoding and can be corrected by channel decoding. Assume that the
physical channel noise NPfollows a Gaussian distribution with zero-mean and variance 2
P, i.e.,
pNP(x) =1
Pp
2exp
 x2
22
P
: (11)
F . Feature Completion
After obtaining the estimated features Ys=fbzlgl2Lselthrough the semantic channel transmis-
sion, the destination will use the estimated features and side information in the knowledge base,
to compute the target function of the task. Although the unintended features subset fzlgl2LnL selare
not transmitted, the receiver may generate the corresponding unintended features fbzlgl2LnL selby
exploiting the knowledge base. Then, by combining intended features fbzlgl2Lseland unintended
featuresfbzlgl2LnL sel, we may obtain the completed semantic features bZ=f^zlgl2Lwith distri-
butionprfea(bz). For example, considering a semantic communication system for staff clothing
image transmission, the intended semantic features of the receiver are clothing features, and the
receiver is not interested in the staff‚Äôs gender, skin color, and hairstyle. Therefore, the receiver
can generate unintended semantic features based on the shared knowledge base, such as the
staff‚Äôs gender, skin color and hairstyle. Note that, the generated unintended semantic features at
the receiver may be different from the corresponding features of the image at the transmitter.
Then, the receiver combines the received clothing features with its own generated unintended
features.
G. Semantic Decoder
The semantic decoder aims to recover the data from the disentangled features bZthat are
semantic explainable, which is the inverse function of the semantic encoding. Again, this inverse
function needs the help of the knowledge base for model training such that the decoder can
‚Äúunderstand‚Äù the features bZ.11
Similar to the encoding process, we use conditional PDF pf2d(bxjbz)to describe the semantic
decoding process. The PDF of the decoded data is prdata(bx), and the decoded data is bx. The
data reconstruction for a given feature vector is given as
prdata(^x) =X
bzpf2d(^xjbz)prfea(bz): (12)
H. Semantic Destination
Finally, the receiver recovers the semantic information based on the decoded data bX, and the
corresponding process can be described by pd2s(bsjbx), where the Ô¨Ånal semantic information is
denoted bybs. At last, the probability of such semantic information can be written as
pdes(bs) =pd2s(bsjbx)prdata(bx): (13)
So far, we have presented the complete semantic communication framework. The key modules
are the semantic encoder and the feature selection. Their functions can be realized by the careful
model design. We will present a detailed system design in Section IV , which is a feasible
realization of this framework.
III. I NFORMATION -THEORETIC METRICS OF SEMANTIC COMMUNICATIONS
In this section, we propose two metrics for the framework illustrated by Fig. 1. Here, we
focus on two procedures: the encoding and the transmission.
A. Rate-Distortion-Perception Function
The semantic encoding may include many different tasks, and these tasks may have relevant
or different criteria. For example, there is data distortion for the traditional data reconstruction
task, and distribution distortion for generative learning tasks.
Letp(x)be the distribution of the input source, r(bx)be the distribution of the reconstruction
signal, and q(bxjx)be a conditional distribution on XX . The information rate-distortion-
perception function R(D;P )[35] for a source Xp(x)is deÔ¨Åned as
R(D;P ) = min
q(bxjx)I(X;bX) (14a)
s:t:E[ (x;bx)]D; (14b)
d(p(x);r(bx))P; (14c)12
X
bxq(bxjx) = 1;8x2X: (14d)
where the distortion function  :XX! R+satisfying (x;bx) = 0 ifx=bx, and perception
functiond 
p(x);r(bx)
is a non-negative divergence between probability distributions p(x)and
r(bx)satisfyingd(p;q) = 0 ifp(x) =r(x).
So far, for a general source, the optimal distribution of the reconstruction signal r(bx)of
problem (14) has not been derived yet. For a binary source, the three-way tradeoff between rate,
distortion, and perception was investigated in [35] with Hamming distance distortion and total-
variation distance perception. While for a Gaussian source, the achievable distortion-perception
region was established in [36] under squared error distortion and squared Wasserstein-2 distance.
Hence, we investigate how to Ô¨Ånd the optimal of R(D;P )for a general source under the mean
square distortion (i.e., (x;bx) =jx bxj2) and KL divergence perception (i.e., d(p(x);r(bx)) =
dKL(p(x);r(bx)),P
xp(x) logp(x)
r(x)). We Ô¨Årst introduce the following lemma.
Lemma 1. Consider the mean square distortion (i.e., (x;bx) =jx bxj2) and KL divergence
perception (i.e., d(p(x);r(bx)) =P
xp(x) logp(x)
r(x)). The corresponding optimal distribution
q(bxjx)to problem (14) for a given output distribution r(bx)>0is
q(bxjx) =r(bx)
e(x)exp
p(x)
r(bx) (x bx)2
; (15)
wheree(x) =P
bxr(bx) exp
p(x)
r(bx) (x bx)2
. The corresponding optimal distribution r(bx)
to(14) for a given conditional distribution q(bxjx)>0is
r(bx) =X
xp(x)q(bxjx): (16)
Proof: Please Ô¨Ånd the proof in Appendix A.
Using Lemma 1, we can apply a process of alternating minimization, called the Blahut‚ÄìArimoto
algorithm [37]. SpeciÔ¨Åcally, in the initialization setup, choose some positive values ; and the
initial output distribution r(0)(bx). In each iteration k, compute the optimal q(k)(bxjx)according
to (15) for given r(k 1)(x), and then compute the optimal r(k)(x)according to (16).
B. Lower and Upper Bounds on Semantic Channel Capacity
The channel capacity quantiÔ¨Åes the maximum rate of information transmission for the con-
sidered system. According to the framework in Fig. 1, we deÔ¨Åne the semantic channel capacity13
as the maximum semantic information that can be transferred through the semantic channel
p(ysjxs). Following the standard achievability and converse proof techniques, we obtain the
semantic channel capacity in our framework as:
Cs= max
p(xs)I(Xs;Ys): (17)
In the conventional bit-level wireless communication system, the channel capacity is usually
represented by the Shannon capacity formula with additive Gaussian distributed noise. In our
framework, the semantic channel noise Nsmainly depends on the quantization noise and physical
channel noise, and follows non-Gaussian distribution in general. Thus, in our framework, the
semantic channel is an additive non-Gaussian noise channel (ANGC), and we assume that the
estimated semantic features Yscan be represented as
Ys= X s+Ns: (18)
Although the speciÔ¨Åc distribution of Nsis unknown, the variance of the semantic noise nscan
be obtained by measurement. In this paper, we assume that the covariance of the semantic noise
nsis2
s.
Due to the non-Gaussian distributed semantic noise ns, the classic Shannon capacity formula
(based on Gaussian distributed noise) cannot be directly applied to the semantic channel. To
derive the semantic channel capacity, we Ô¨Årst deÔ¨Åne equivalent Gaussian distributed semantic
channel noise NsN (0;2
s)with the same variance as Ns. Then, based on the equivalent
semantic channel noise Ns, the received signal of semantic channel Ysis given as
Ys=Xs+Ns: (19)
Therefore, the channel capacity of the equivalent semantic channel is given as
Cs;eq=1
2log
1 +Pxs
2
s
; (20)
wherePxsdenote the power of transmitted semantic data Xs.
Proposition 1 (Lower and upper bounds on the semantic channel capacity) .With the non-
Gaussian distributed channel noise, the semantic channel capacity Csis bounded by [38]
Cs;eqCsCs;eq+dKL(pns(x);pns(x)); (21)14
wheredKL(pns(x);pns(x)) =R1
 1pns(x) logpns(x)
pns(x)dx.
-4 -2 0 2 4 6 8 10 120.20.40.60.811.21.41.61.822.2
(a)
-4 -2 0 2 4 6 8 10 120.20.40.60.811.21.41.61.822.2 (b)
Fig. 2: (a) Lower and upper bounds of semantic channel capacity versus SNR with a= 1,b= 1 and2
P= 0:01; (b) Lower
and upper bounds of semantic channel capacity versus SNR with a= 0:3,b= 0:3and2
P= 0:01.
At last, we illustrate our theoretical results on the semantic channel capacity via numerical
simulation. Fig. 2 (a) and (b) show the lower bound and the upper bound in (22) on semantic
channel capacity versus SNR with semantic noise parameters a= 1,b= 1 and2
P= 0:01,
and semantic noise parameters a= 0:3,b= 0:3and2
P= 0:01, respectively. Fig. 2 (b) shows
that the gap between the upper bound and lower bound is less than that in Fig. 2 (a). The reason
is that when the the KL divergence between semantic noise Nsand the equivalent semantic
channel noise nstends to 0, i.e.,dKL(pns(x);pns(x))!0, the gap between the lower bound
and the upper bound in (22) tends to 0.
IV.-VAE BASED ROBUST AND EXPLAINABLE SEMANTIC COMMUNICATION SYSTEM
In this section, we present a feasible and efÔ¨Åcient system design based on the proposed
framework given in Fig. 1. Here, we propose a robust -V AE based semantic communications
system, as shown in Fig. 3, which disentangles the hidden representation vector into multiple
independent and semantically interpretable of features.
A. Robust -VAE based Semantic Encoder/Decoder
By exploiting a generative V AE model [39], we Ô¨Årst optimize the semantic encoder q(zjx)
with parameter set , and the semantic decoder p(bxjbz)for the receiver with parameter set .15
Semantic 
nosie 
¬∑¬∑¬∑ 
¬∑¬∑¬∑ 
¬∑¬∑¬∑ ¬∑¬∑¬∑ Feature 
completion Feature  
selection Feature 
completion Feature   
selection x ÀÜx
sn{ }llz√é
{ }
sel llz√é
sel sel sel sel sel { }ÀÜllz√é
{ }
sel ÀÜllz√é
sel sel sel sel sel 
( )|q z x f
 ( )|p x z q
z¬∑¬∑¬∑ 
Semantic encoder Semantic decoder 
Fig. 3: Proposed -V AE based explainable semantic communication system
Mathematically, we aim to jointly optimize parameters andto maximize the log-likelihood
of dataXas follows
max
;logp(x): (22)
To efÔ¨Åciently handle optimization problem (22), we optimize the lower bound of the objective
function logp(x)[40]. SpeciÔ¨Åcally, logp(x)is lower bounded by
logp(x) =Z
zq(zjx) logp(x)dz (23a)
=Z
zq(zjx) logp(z;x)
p(zjx)dz (23b)
=Z
zq(zjx) logp(z;x)
q(zjx)dz+Z
zq(zjx) logq(zjx)
p(zjx)dz (23c)
=Z
zq(zjx) logp(z;x)
q(zjx)dz+dKL(q(zjx)jjp(zjx)) (23d)
Z
zq(zjx) logp(z;x)
q(zjx)dz; (23e)
=Z
zq(zjx) logp(xjz)p(z)
q(zjx)dz (23f)
=Z
zq(zjx) logp(xjz)dz+Z
zq(zjx) logp(z)
q(zjx)dz (23g)
=Eq(zjx)[logp(xjz)] dKL(q(zjx)jjp(z)) (23h)
where equation (23a) holds for the arbitrary distribution q(zjx), and inequality (23e) holds due
todKL(q(zjx)jjp(zjx))0.16
Unfortunately, maximizing the lower bound in (23h) directly cannot achieve interpretable
and robust semantic communication systems design. To address this challenge, we multiply
dKL(q(zjx)jjp(z))by a weighting parameter to obtain a disentangling and explainable
semantic representation z[39], for >1. Furthermore, to combat semantic noise and achieve
robust semantic communication systems design, we replace p(zjx)withp(xjbz), wherebz=
gz+ns,gdenotes fading channel gain, and nsdenotes the semantic noise. SpeciÔ¨Åcally, the
log-likelihood maximization problem (22) is reformulated as follows
max
;Eq(xjz)[logp(xjbz)] dKL(q(zjx)jjp(z)); (24)
where the prior distribution p(z)is assumed to follow a standard Gaussian distribution, i.e.,
p(z) =N(0;I).Note that, in (24), the Ô¨Årst term Eq(xjz)[logp(xjz+ns)]is the expected
likelihood with the cross entropy form, which can be regarded as reconstruction loss, while
the second term regularizes q(zjx)to be close to prior p(z), which can be regarded as
regularization loss. To further enhance the robustness of the variational inference, we exploit
-cross entropy c(p(x)jjp(xj^z))[41], [42] as the reconstruction loss, instead of the cross
entropy Eq(xjz)[logp(xjz+ns)], where
c(p(x)jjp(xj^z)) = + 1
Z
p(x)dx+Z
p(xj^z)1+dx: (25)
SpeciÔ¨Åcally, the objective function of the proposed robust semantic communication system is
given as
max
;c(p(x)jjp(xj^z)) dKL(q(zjx)jjp(z)): (26)
Thus, the robust -V AE training objective (26) encourages the latent distribution q(zjx)to
efÔ¨Åciently represent semantic information about the data xby jointly maximizing the -cross
entropyc(p(x)jjp(xj^z))and minimizing the -weighted KL term dKL(q(zjx)jjp(z))via
unsupervised learning.
More speciÔ¨Åcally, we jointly optimize the semantic encoder parameter and semantic decoder
parameterto maximize the objective function (26). The Ô¨Årst term of (26) is the probability
of reconstructing the input data x, which corresponds to reconstruction loss. The second term
is minimizing the KL divergence, which is the distance between the approximated posterior
q(zjx)and the Ô¨Åxed Gaussian distribution p(z) =N(0;I). By adopting the well chosen17
values of the parameter (usually > 1), the posterior q(zjx)is encouraged to match
the Gaussian distribution p(z) =N(0;I), which disentangles the hidden representation into
multiple independent and semantically meaningful features fzlgl2L. The parameter balances
reconstruction accuracy and learned disentanglement quality. In general, a higher value of will
produce a more disentangled representation, but may lead to lower reconstruction accuracy [39].
Semantic encoder 
Semantic 
noise 
¬∑¬∑¬∑ 
Semantic decoder 
¬∑¬∑¬∑ Feature 
completion ¬∑¬∑¬∑ ¬∑¬∑¬∑ 
¬∑¬∑¬∑ Feature  
selection x
ÀÜxsn
¬∑¬∑¬∑ 
¬∑¬∑¬∑ ¬∑¬∑¬∑ ¬∑¬∑¬∑ Conv2D
32@32*32 Conv2D
32@16*16 Conv2D
64@8*8Conv2D
64@4*4
¬∑¬∑¬∑ Dense 
1*256 Dense 
1*32 
1*32 
ConvT2D
3@32*32 ConvT2D
32@16*16 ConvT2D
32@8*8ConvT2D
64@4*4Dense 
1*256 Dense 
1*32 
¬∑¬∑¬∑ ¬∑¬∑¬∑ 
¬∑¬∑¬∑ 
¬∑¬∑¬∑ ¬∑¬∑¬∑ { }32 
1llz=
1z
{ }32 
1ÀÜllz=
1ÀÜz
Fig. 4: Proposed robust -V AE based architecture for the explainable semantic communication system
Note that, in the robust -V AE network, we let flgL
l=1andflgL
l=1denote the mean and the
corresponding standard deviation of the approximate posterior q(zjx), respectively. Moreover,
a reparametrization trick [39] is applied to estimate gradients of the objective function (24) with
respect to the parameter , where random independent variables f"lgL
l=1are sampled from a
standard Gaussian distribution, i.e., "lN (0;1). Then, the output features of the semantic
encoderfzlgL
l=1are given as follows
zl=l+l"l; l= 1;:::;L: (27)
Thus, the feature zlis equivalent to being sampled from distribution N(l;2
l), wherel=
1;:::;L .18
B. Feature Selection and Completion
With the disentangled and explainable features, the proposed semantic communications system
further performs feature selection and completion at the transmitter and receiver, respectively.
SpeciÔ¨Åcally, since the receiver may only be interested in some of the features, the transmitter
only sends the intended features fzlgl2Lselaccording to their semantic meanings, rather than all of
the extracted features fzlgl2L, which can further reduce the amount of information transmission.
For the receiver, the proposed semantic source and channel decoder include semantic feature
completion and feature reconstruction. SpeciÔ¨Åcally, for the unintended features subset are not
transmittedfzlgl2LnL sel, the receiver generated the corresponding features fbzlgl2LnL selbased on
the receiver knowledge base, where both the dimensions and value ranges of sets zlandbzlare
the same.
Then, according to the completed semantic features bZ=f^zlgl2L, the feature reconstruction
module recovers the original data bX.
C. Proposed Architecture
The proposed lightweight semantic communication architecture includes a semantic encoder
network and a semantic decoder network, as shown in Fig. 4, where the notation Conv2D
32@32*32 means that the network has 32 2-D convolutional Ô¨Ålters of size 32*32, and Dense
1*256 represents a dense layer with 256 neurons. The details of the semantic encoder and the
decoder network architectures are given as:
1) Semantic encoder architecture: : Conv2D 32 @32*32!Conv2D 32 @16*16!Conv2D
64@8*8!Conv2D 64 @4*4!Dense 1*256!2 parallel Dense 1*32 !fzlg32
l=1!fzlgl2Lsel;
2) Semantic decoder architecture: :fbzlgl2Lsel!fbzlg32
l=1!Dense 1*32!Dense 1*256!
ConvT2D 64 @4*4!ConvT2D 32 @8*8!ConvT2D 32 @16*16!ConvT2D 3 @32*32.
Note that, based on the feature selection, the proposed semantic communication system only
needs to send the features fzlgl2Lselthat the receiver is interested in, instead of sending all
featuresfzlg32
l=1.
V. P ROTOTYPE AND IMPLEMENTATIONS
The proposed architecture and hardware platform design of the semantic communication sys-
tem prototype are shown in Fig. 5 (a) and (b), which can be used to implement the proposed robust
and explainable semantic communications system in Fig. 3. The prototype includes two semantic19
communication mobile users A and B. The trained robust -V AE network is implemented at
the portable RaspberryPi 4 Model B processors to realize the semantic encoding/decoding and
the feature selection/completion functions. The integrated Wi-Fi module fulÔ¨Ålls the bit-level
transmission. The decoded data can be shown through the display.
The detailed parameters of the prototype are provided in Table III. The Raspberry Pi is installed
with an ARM Cortex-A72 @quad-core 1.5GHz CPU and 4GB of DDR4 RAM, and is equipped
with Pytorch-CPU and torchvision software. The communication between Raspberry Pi A and
B is realized through WiFi, where the socket is used to send and receive data, and Visdom is
used to realize visual communication.
Raspberry-Pi 4 
Model B 
Wi-Fi 
Screen 
Raspberry-Pi 4 
Model B 
Wi-Fi 
Screen 
Semantic Communication 
Mobile User A Semantic Communication 
Mobile User B 
(a)
Screen 
Raspberry-Pi 4
Model B 
Wi-Fi 
Raspberry-Pi 4
Model B Wi-Fi 
Screen Semantic Communication 
Mobile User A Semantic Communication 
Mobile User B 
(b)
Fig. 5: (a) The architecture of the semantic communication system prototype; (b) The hardware platform of the semantic
communication system prototype.20
VI. E XPERIMENTS AND DISCUSSIONS
In this section, we evaluate the proposed explainable semantic communications system using a
graphics processing unit (GPU) and Raspberry Pi prototype, respectively. The GPU experiments
in this work have been performed on 32 GB RAM i5-12600H, and 8 GB Nvidia GeForce 3060Ti
GTX graphics card with Pytorch powered with CUDA 11.3. The experiments are performed via
two standard datasets, i.e., MNIST Dataset and CelebA Dataset.
A. Demonstration via GPU
First, we evaluate the robustness of the proposed semantic communication system. SpeciÔ¨Åcally,
the peak signal-to-noise ratio (PSNR) performance of the proposed robust -V AE scheme with
SNR train= 4dB andSNR train= 8dB are demonstrated over the two channel models: the ANGC
and a slow Rayleigh fading channel, where SNR train= 4dB andSNR train= 8dB mean that the
trained SNRs of the schemes are 4dB and8dB, respectively. Moreover, the PSNR performance
of the deep joint source-channel coding (Deep-JSCC) scheme [19], -V AE scheme, and the
JPEG compression scheme are presented for comparisons.
Fig. 6 (a) shows PSNR versus different test SNRs of the four schemes over ANGC, where
semantic noise parameters a= 0:1,b= 0:1and2
P= 1. We observe that the PSNR of JPEG
compression is the lowest among the Ô¨Åve schemes, and the PSNR of the robust -V AE schemes
are higher than those of both Deep-JSCC and -V AE. In the low SNR regions, the PSNR of
the robust-V AE with SNR train= 4dB is the highest, and the PSNR of the robust -V AE with
SNR train= 8dB is the higher than that of -V AE, which veriÔ¨Åes the robustness of our proposed
design. Since the training noise of SNR train= 4dB is higher than that of SNR train= 8dB , the
performance of SNR train= 4dB is more robust, and thus the PSNR of SNR train= 4dB is higher.
In the high SNR regions, the PSNR of -V AE, and robust -V AE models tend to be the same.
The reason is that the effect of noise at high SNR can be ignored.
TABLE III: Hardware parameters of the semantic communication prototype.
GPU 500MHz VideoCore VI
CPU quad-core Cortex-A72
System on Chip Broadcom BCM2711 @1.5GHz
memory 4GB DDR4
Wi-Fi 2.4=5.0 GHz IEEE 802.11ac wireless
Screen 800480display21
-4 -2 0 2 4 6 8 10 12
SNRtest(dB)024681012141618PSNR(dB)
Deep-JSCC
-VAE
Robust -VAE(SNRtrain=4dB)
Robust -VAE(SNRtrain=8dB)
JPEG
(a)
-4 -2 0 2 4 6 8 10 12
SNRtest(dB)024681012141618PSNR(dB) Deep-JSCC
-VAE
Robust -VAE(SNRtrain=4dB)
Robust -VAE(SNRtrain=8dB)
JPEG (b)
Fig. 6: (a) PSNR of Deep-JSCC, -V AE, JPEG compression, and robust -V AE with SNR train= 4dB andSNR train= 8dB
over ANGC with semantic noise a= 0:1,b= 0:1and2
P= 1; (b) PSNR of Deep-JSCC, -V AE, JPEG compression, and
robust-V AE with SNR train= 4dB andSNR train= 8dB over Rayleigh fading channels with 2
h= 1,a= 0:1,b= 0:1
and2
P= 1.
Fig. 6 (b) illustrates PSNR versus different test SNRs of the Ô¨Åve schemes over the Rayleigh
fading channel. Similar to Fig. 6 (a), the PSNR of JPEG compression is the lowest among the
four schemes, and the PSNR of the robust -V AE with SNR train= 4dB andSNR train= 8dB
are higher than those of both Deep-JSCC and -V AE. Note that for SNR test= 8dB , the PSNR
of the robust -V AE with SNR train= 8dB is the higher than that of SNR train= 8dB . This
because the training SNR of SNR train= 8dB is also 8dB. Comparing Fig. 6 (a) with ANGC,
the PSNRs of the schemes in Fig. 6 (b) are lower due to Rayleigh random fading.
Table IV illustrates the transmission performance of JPEG compression, -V AE, and robust -
V AE with SNR train= 4dB andSNR train= 8dB over ANGC with semantic noise parameters a=
 0:1,b= 0:1and2
P= 1. The second column of Table IV shows the transmission performance
of the JPEG compression scheme, where the transmitted semantics cannot be recognized from the
received image. The third column shows the results of the -V AE scheme, where the transmission
semantics can be recognized from the received image. The fourth and Ô¨Åfth columns show received
images of the robust -V AE scheme with SNR train= 4dB andSNR train= 8dB , and the quality
is better than that of the -V AE scheme.
Table V illustrates transmission performance of the four schemes over the Rayleigh fading
channel with semantic noise parameters a= 0:1,b= 0:1and2
P= 1. The second column
of table IV shows the transmission performance of the JPEG compression scheme, where the
transmission semantics cannot be recognized from the received image. The third column shows22
TABLE IV: Transmission performance comparison over ANGC
Test SNR DataX JPEG-V AERobust-V AE
SNR train= 4dBRobust-V AE
SNR train= 8dB
SNR test=4dB
SNR test=8dB
SNR test=4dB
SNR test=8dB
TABLE V: Transmission performance comparison over Rayleigh fading channel
Test SNR DataX JPEG-V AERobust-V AE
SNR train= 4dBRobust-V AE
SNR train= 8dB
SNR test=4dB
SNR test=8dB
SNR test=4dB
SNR test=8dB
the transmission performance of the -V AE scheme, where the transmission semantics can
be recognized from the received image. The fourth and Ô¨Åfth columns show the transmission
performance of the robust -V AE scheme with SNR train= 4dB andSNR train= 8dB , and the
received image is better than that of the -V AE scheme.23
TABLE VI: Proposed semantic communication with feature selection
Intended
FeatureSkin colorFace
orientationGender Hairstyle
Source
dataX
Receiver
Knowledge
Base
Decoded
data ^X
(GPU)
Decoded
data ^X
(Raspberry)
B. Demonstration via Prototype
In this subsection, we demonstrate that the proposed explainable semantic communication
system with feature selection can improve the transmission efÔ¨Åciency via our prototype.
Table VI shows the performance of the proposed explainable semantic communication system
with feature selection. From Column 2 to Column 5, we present four examples to show how the
explainable encoder and feature selection work in the transmission. In the second column, the
intended feature to send is skin color. The proposed semantic communication system performs
feature extraction on the input white-skinned women picture, and then only selects the white skin
color feature for transmission. Although the woman in the receiving knowledge base has darker
skin, the reconstructed image is changed to white-skin. In the third column, the intended feature
is face orientation. The proposed semantic communication system can successfully reconstruct
a picture with the same face orientation at the receiver. Similarly, the intended features of
the third and fourth columns are gender and hairstyle, respectively, and the proposed semantic
communication system can also recover the correct feature at the receiver.
Table VII compares a compression ratio, transmission time, PSNR and reconstructed image of
the original image transmission scheme, JPEG compression scheme, -V AE scheme, and robust
-V AE scheme over our proposed semantic communication prototype on MNIST dataset with
high SNR. From Table VII, we observe that the compression ratio of the -V AE scheme and24
robust-V AE scheme is 78:4which is signiÔ¨Åcantly higher than those of the JPEG compression
scheme (1.81) and original image transmission scheme. Thus, the transmission time of the
-V AE scheme and robust -V AE scheme is about 0:3ms, which is signiÔ¨Åcantly lower than
those of the JPEG compression scheme (10.88ms) and original image transmission scheme
(18.86ms). Therefore, the proposed semantic communication system can signiÔ¨Åcantly reduce the
transmission load and time. Moreover, the PSNR of the robust -V AE scheme is close to that
of the JPEG compression scheme, and is higher than that of the -V AE scheme. Comparing of
reconstructed images, we can clearly and accurately identify the number ‚Äú 7‚Äù from the recovered
images using the proposed robust -V AE scheme.
TABLE VII: Performance of the proposed semantic communication prototype on MNIST dataset
Transmission
time (ms)Compression
ratioPSNRReconstructed
image
Original image 18.86 1 100
JPEG 10.88 1.81 17.49
-V AE 0.30 78.4 15.79
Robust-V AE 0.30 78.4 16.07
Table VIII compares a compression ratio, transmission time, PSNR and reconstructed image of
the original image transmission scheme, JPEG compression scheme, -V AE scheme, and robust
-V AE scheme over our proposed semantic communication prototype on CelebA dataset with
high SNR. Similar to Table VII, the compression ratio of the -V AE scheme and robust -V AE
scheme is 384which is signiÔ¨Åcantly higher than those of the JPEG compression scheme (4.49)
and original image transmission scheme. Thus, the transmission time of the -V AE scheme and
robust-V AE scheme is about 0:18ms, which is signiÔ¨Åcantly lower than those of the JPEG
compression scheme (9.53ms) and original image transmission scheme (28.38ms). Therefore,
the proposed semantic communication system can signiÔ¨Åcantly reduce the transmission load
and time. Moreover, the PSNR of the robust -V AE scheme is close to that of the JPEG25
TABLE VIII: Performance of the proposed semantic communication prototype on CelebA dataset
Transmission
time (ms)Compression
ratioPSNRReconstructed
image
Original image 28.38 1 100
JPEG 9.53 4.49 30.17
-V AE 0.18 384 17.66
Robust-V AE 0.18 384 19.73
compression scheme, and is higher than that of the -V AE scheme. Note that, although the
effect of the reconstructed image of proposed robust -V AE scheme is a bit blurry, the three
main semantic features of the original image: female, white skin color and long hair, are all
accurately transmitted, which veriÔ¨Åes the validity and accuracy of the proposed task-oriented
semantic communication scheme.
VII. C ONCLUSIONS
In this paper, we propose an explainable and easy-to-implement semantic communication
framework that is compatible with conventional communication systems. In this new frame-
work, the semantic encoder can extract feature vectors, disentangle the semantic information,
and improve robustness against semantic information ambiguity. To further reduce the com-
munication cost, we apply feature selection to choose only task-related semantic information
to transmit. Then, we present two information theoretic metrics, namely, the rate-distortion-
perception function and semantic channel capacity to characterize the semantic information
compression and transmission, respectively. To quantify the semantic information transmission
with the additive quantization noise and physical channel noise, we further derive upper and
lower bounds on the semantic channel capacity. Then, we propose a feasible design of the
explainable semantic communication system, which includes a robust -V AE lightweight un-
supervised learning network. Finally, we develop a wireless mobile semantic communication26
proof-of-concept prototype to implement the semantic communication design. Our experiments
demonstrate that the proposed semantic communication system signiÔ¨Åcantly outperforms the
state-of-the-art methods, and shows robustness against various noise levels on two benchmark
datasets. This work attempts to provide frameworks and theoretic metrics to explain and analyze
the black-box semantic communications problem, and to provide guidelines on implementing
the semantic communication in practical communication systems.
VIII. A PPENDICES
APPENDIX A
PROOF OF LEMMA 1
We Ô¨Årst derive the optimal conditional distribution q(bxjx)in (14) for a given output distribution
r(x). The mutual information I(X;bX) =P
xP
bxp(x)q(bxjx) logq(bxjx)
r(bx)is convex in q(bxjx)for Ô¨Åxed
p(x), and the KL divergence dKL(p(x);r(bx)) =P
xp(x) logp(x)
r(x)is also convex in q(bxjx)for
Ô¨Åxedp(x). Thus, problem (14) is convex in q(bxjx). Then, the Lagrangian function of problem
(14) is given by
L(q(bxjx)) =I
X;bX
+X
xX
bxp(x)q(bxjx) (x bx)2
+X
xp(x) logp(x)
r(bx)+X
x(x)X
bxq(bxjx); (28)
where0,0and(x)0are Lagrange multipliers attached with constraints (14b),
(14c) and (14d), respectively. For given r(bx), the derivative of (28) with respect to q(bxjx)is
given as
@L 
q(bxjx)
@q(bxjx)=p(x) 
logq(bxjx)
r(bx)+(x bx)2 p(x)
r(bx)+(x)
p(x)
: (29)
Let@L(q(bxjx))
@q(bxjx)= 0, then we obtain the optimal q(bxjx)as
q(bxjx) =r(bx) exp
p(x)
r(bx) (x bx)2 (x)
p(x)
(30a)
=r(bx)
e(x)exp
p(x)
r(bx) (x bx)2
; (30b)
wheree(x)= exp
(x)
p(x)
.27
SinceP
bxq(bxjx) = 1 , we have
X
bxr(bx)
e(x)exp
p(x)
r(bx) (x bx)2
= 1: (31)
Furthermore, we obtain
e(x) =X
bxr(bx) exp
p(x)
r(bx) (x bx)2
: (32)
Substituting (32) into (30b), we obtain the optimal q(bxjx)as given in Lemma 1.
From [37] , we Ô¨Ånd that given a Ô¨Åxed conditional distribution q(bxjx), the optimal output
distribution r(bx)isr(x),P
xp(x)q(bxjx). We rewrite the proof below.
I(X;Z) =X
x;bxp(x)q(bxjx) logp(x)q(bxjx)
p(x)r(bx)(33)
 X
x;bxp(x)q(bxjx) logp(x)q(bxjx)
p(x)r(bx)(34)
=X
bxr(x) logr(bx)
r(bx)0; (35)
where the last inequality holds because of the non-negative property of KL divergence.
REFERENCES
[1] L. Knud, ‚ÄúState of the IoT 2020: 12 billion IoT connections, surpassing non-IoT for the Ô¨Årst time,‚Äù https://iot-analytics.
com/state-of-the-iot-2020-12-billion-iot-connections-surpassing-non-iot/, 2020.
[2] J. Antoniou, ‚ÄúQuality of experience and emerging technologies: Considering features of 5G, IoT, cloud and AI,‚Äù in Quality
of Experience and Learning in Information Systems , pp. 1‚Äì8. Springer, 2021.
[3] W. Saad, M. Bennis, and M. Chen, ‚ÄúA vision of 6G wireless systems: Applications, trends, technologies, and open research
problems,‚Äù IEEE Netw. , vol. 34, no. 3, pp. 134‚Äì142, Oct. 2020.
[4] E. Calvanese Strinati, S. Barbarossa, J. L. Gonzalez-Jimenez, D. Ktenas, N. Cassiau, L. Maret, and C. Dehos, ‚Äú6G: The
next frontier: From holographic messaging to artiÔ¨Åcial intelligence using subterahertz and visible light communication,‚Äù
IEEE Veh. Technol. Mag. , vol. 14, no. 3, pp. 42‚Äì50, Oct. 2019.
[5] B. Mao, F. Tang, Y . Kawamoto, and N. Kato, ‚ÄúAI models for green communications towards 6G,‚Äù IEEE Commun. Surveys
Tuts. , vol. 24, no. 1, pp. 210‚Äì247, Nov. 2022.
[6] K. Niu, J. Dai, S. Yao, S. Wang, Z. Si, X. Qin, and P. Zhang, ‚ÄúTowards semantic communications: A paradigm shift,‚Äù
arXiv preprint arXiv:2203.06692 , 2022.
[7] P. Zhang, W. Xu, H. Gao, K. Niu, X. Xu, X. Qin, C. Yuan, Z. Qin, H. Zhao, J. Wei, et al., ‚ÄúToward wisdom-evolutionary
and primitive-concise 6G: A new paradigm of semantic communication networks,‚Äù Engineering , 2022.
[8] M. Kountouris and N. Pappas, ‚ÄúSemantics-empowered communication for networked intelligent systems,‚Äù IEEE Commun.
Mag. , vol. 59, no. 6, pp. 96‚Äì102, Jan. 2021.28
[9] M. Sana and E. Calvanese Strinati, ‚ÄúLearning semantics: An opportunity for effective 6G communications,‚Äù arXiv preprint
arXiv:2202.11958 , 2021.
[10] Y . L. G. Shi Y . Xiao. and X. Xie, ‚ÄúFrom semantic communication to semantic-aware networking: Model, architecture,
and open problems,‚Äù IEEE Commun. Mag. , vol. 59, no. 8, pp. 44‚Äì50, Aug. 2021.
[11] X. Luo, H.-H. Chen, and Q. Guo, ‚ÄúSemantic communications: Overview, open issues, and future research directions,‚Äù
IEEE Wirel. Commun. , pp. 1‚Äì10, Jan. 2022.
[12] J. Bao, P. Basu, M. Dean, C. Partridge, A. Swami, W. Leland, and J. A. Hendler, ‚ÄúTowards a theory of semantic
communication,‚Äù in Proc. IEEE Netw. Sci. Workshop , pp. 110‚Äì117, Jun. 2011.
[13] A. Y . B. G ¬®uler and A. Swami, ‚ÄúThe semantic communication game,‚Äù IEEE Trans. Cogn. Commun. Netw. , vol. 4, no. 4,
pp. 787‚Äì802, Dec. 2018.
[14] W. Weaver, ‚ÄúRecent contributions to the mathematical theory of communication,‚Äù ETC: a review of general semantics ,
pp. 261‚Äì281, Sep. 1949.
[15] C. E. Shannon, ‚ÄúA mathematical theory of communication,‚Äù Bell Syst. Tech. J. , vol. 27, no. 3, pp. 623‚Äì656, Jul. 1948.
[16] N. Farsad, M. Rao, and A. Goldsmith, ‚ÄúDeep learning for joint source-channel coding of text,‚Äù in Proc.(ICASSP) , pp.
2326‚Äì2330, Apr. 2018.
[17] H. Xie, Z. Qin, L. Geoffrey Ye., and B.-H. Juang, ‚ÄúDeep learning enabled semantic communication systems,‚Äù IEEE Trans.
Signal Process. , vol. 69, pp. 2663‚Äì2675, Apr. 2021.
[18] Q. Hu, G. Zhang, Z. Qin, Y . Cai, and G. Yu, ‚ÄúRobust semantic communications against semantic noise,‚Äù arXiv preprint
arXiv:2202.03338 , Feb. 2022.
[19] E. Bourtsoulatze, D. B. Kurka, and D. G ¬®und¬®uz, ‚ÄúDeep joint source-channel coding for wireless image transmission,‚Äù IEEE
Trans. Cognit.Commun. Netw. , vol. 5, no. 3, pp. 567‚Äì579, May. 2019.
[20] D. B. Kurka and D. G ¬®und¬®uz, ‚ÄúDeepjscc-f: Deep joint source-channel coding of images with feedback,‚Äù IEEE J. Sel. Areas
Inf. Theory , vol. 1, no. 1, pp. 178‚Äì193, Apr. 2020.
[21] M. Yang, C. Bian, and H.-S. Kim, ‚ÄúOFDM-guided deep joint source channel coding for wireless multipath fading channels,‚Äù
IEEE Trans. Cognit. Commun. Netw. , Feb. 2022.
[22] K. Choi, K. Tatwawadi, A. Grover, T. Weissman, and S. Ermon, ‚ÄúNeural joint source-channel coding,‚Äù in Proc. Int. Conf.
Mach. Learn.(ICML) . PMLR, pp. 1182‚Äì1192, Jun. 2019.
[23] J. Xu, B. Ai, W. Chen, A. Yang, P. Sun, and M. Rodrigues, ‚ÄúWireless image transmission using deep source channel
coding with attention modules,‚Äù IEEE Trans. Circuits Syst. Video Technol. , May. 2021.
[24] H. Tong, Z. Yang, S. Wang, Y . Hu, W. Saad, and C. Yin, ‚ÄúFederated learning based audio semantic communication over
wireless networks,‚Äù in Proc. IEEE Global Commun. Conf. (GLOBECOM) , pp. 1‚Äì6, Feb. 2021.
[25] Z. Weng and Z. Qin, ‚ÄúSemantic communication systems for speech transmission,‚Äù IEEE J. Sel. Areas Commun. , vol. 39,
no. 8, pp. 2434‚Äì2444, Aug. 2021.
[26] H. Xie, Z. Qin, and G. Y . Li, ‚ÄúTask-oriented multi-user semantic communications for VQA,‚Äù IEEE Wirel. Commun. Lett. ,
vol. 11, no. 3, pp. 553‚Äì557, Dec. 2021.
[27] Q. Zhou, R. Li, Z. Zhao, Y . Xiao, and H. Zhang, ‚ÄúAdaptive bit rate control in semantic communication with incremental
knowledge-based HARQ,‚Äù arXiv preprint arXiv:2203.06634 , 2022.
[28] P. Jiang, C.-K. Wen, S. Jin, and G. Y . Li, ‚ÄúDeep source-channel coding for sentence semantic transmission with HARQ,‚Äù
arXiv preprint arXiv:2106.03009 , 2021.
[29] Z. Z. C. P. Q. Zhou R. Li. and H. Zhang, ‚ÄúSemantic communication with adaptive universal transformer,‚Äù IEEE Wirel.
Commun. Lett. , vol. 11, no. 3, pp. 453‚Äì457, Dec. 2021.29
[30] H. Xie and Z. Qin, ‚ÄúA lite distributed semantic communication system for internet of things,‚Äù IEEE J. Sel. Areas Commun. ,
vol. 39, no. 1, pp. 142‚Äì153, Jan. 2021.
[31] K. Lu, R. Li, X. Chen, Z. Zhao, and H. Zhang, ‚ÄúReinforcement learning-powered semantic communication via semantic
similarity,‚Äù arXiv preprint arXiv:2108.12121 , 2021.
[32] M. Yang and H.-S. Kim, ‚ÄúDeep joint source-channel coding for wireless image transmission with adaptive rate control,‚Äù
arXiv preprint arXiv:2110.04456 , 2021.
[33] M. Ding, J. Li, M. Ma, and X. Fan, ‚ÄúSNR-adaptive deep joint source-channel coding for wireless image transmission,‚Äù
inProc. IEEE Int. Conf. Acoust., Speech, Signal Process.(ICASSP) , pp. 1555‚Äì1559, May. 2021.
[34] Y . M. J. Shao and J. Zhang, ‚ÄúLearning task-oriented communication for edge inference: An information bottleneck
approach,‚Äù IEEE J. Sel. Areas Commun. , vol. 40, no. 1, pp. 197‚Äì211, Jan. 2022.
[35] Y . Blau and T. Michaeli, ‚ÄúRethinking lossy compression: The rate-distortion-perception tradeoff,‚Äù in International
Conference on Machine Learning . PMLR, pp. 675‚Äì685, 2019.
[36] G. Zhang, J. Qian, J. Chen, and A. Khisti, ‚ÄúUniversal rate-Distortion-Perception representations for lossy compression,‚Äù
Advances in Neural Information Processing Systems , vol. 34, 2021.
[37] T. M. Cover and J. A. Thomas, Elements of information theory, 2nd ed. , New York, NY , USA: Wiley, 2006.
[38] S. Ihara, ‚ÄúOn the capacity of channels with additive non-Gaussian noise,‚Äù Inform. Contr. , vol. 37, no. 1, pp. 34‚Äì39, Sep.
1978.
[39] I. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot, M. Botvinick, S. Mohamed, and A. Lerchner, ‚Äúbeta-vae: Learning
basic visual concepts with a constrained variational framework,‚Äù Proc. ICLR , pp. 1‚Äì12, 2017.
[40] D. P. Kingma and M. Welling, ‚ÄúAuto-encoding variational bayes,‚Äù arXiv preprint arXiv:1312.6114 , 2013.
[41] S. Eguchi and S. Kato, ‚ÄúEntropy and divergence associated with power function and the statistical application,‚Äù Entropy ,
vol. 2, pp. 262?274, Dec. 2010.
[42] F. Futami, I. Sato, and M. Sugiyama, ‚ÄúVariational inference based on robust divergences,‚Äù arXiv preprint arXiv:1710.06595 ,
2017.