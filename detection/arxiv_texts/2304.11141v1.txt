1
H2TF for Hyperspectral Image Denoising: Where
Hierarchical Nonlinear Transform Meets
Hierarchical Matrix Factorization
Jiayi Li, Jinyu Xie, Yisi Luo, Xile Zhao, Jianli Wang
Abstract ‚ÄîRecently, tensor singular value decomposition (t-
SVD) has emerged as a promising tool for hyperspectral image
(HSI) processing. In the t-SVD, there are two key building
blocks: (i) the low-rank enhanced transform and (ii) the accom-
panying low-rank characterization of transformed frontal slices.
Previous t-SVD methods mainly focus on the developments of
(i), while neglecting the other important aspect, i.e., the exact
characterization of transformed frontal slices. In this letter, we
exploit the potentiality in both building blocks by leveraging the
Hierarchical nonlinear transform and the H ierarchical matrix
factorization to establish a new T ensor F actorization (termed as
H2TF). Compared to shallow counter partners, e.g., low-rank
matrix factorization or its convex surrogates, H2TF can better
capture complex structures of transformed frontal slices due to its
hierarchical modeling abilities. We then suggest the H2TF-based
HSI denoising model and develop an alternating direction method
of multipliers-based algorithm to address the resultant model.
Extensive experiments validate the superiority of our method
over state-of-the-art HSI denoising methods.
Index Terms ‚ÄîHyperspectral denoising, t-SVD, ADMM.
I. I NTRODUCTION
HYPERSPECTRAL images (HSIs) inevitably contain
mixed noise due to sensor failures or complex imaging
conditions [1], [2], which seriously affects subsequent appli-
cations. Traditional hand-crafted HSI denoising methods, e.g.,
low-rankness [3], total variation (TV) [4], sparse representa-
tions [5], and non-local self-similarity [6], utilize interpretable
domain knowledge to design generalizable regularizations for
HSI denoising. Their representation abilities may be inferior
to data-driven methods using deep neural networks (DNNs)
[7]‚Äì[9], which can learn representative denoising mappings
via supervised learning with abundant training pairs. However,
supervised deep learning methods mostly neglect the prior
information of HSIs, which sometimes results in generalization
issues over different HSIs and various types of noise.
More recently, tensor singular value decomposition (t-SVD)
attracts much attention in HSI denoising [10], [11]. The t-
SVD views HSI as an implicit low-rank tensor and exploits
the low-rankness in the transformed domain, which can more
vividly characterize the structures of HSIs since it is Ô¨Çexible to
select appropriate transforms and the accompanying low-rank
characterization of the transformed frontal slices. Under such a
framework, there are naturally two key building blocks: (i) The
selection of the low-rank enhanced transform. A suitable trans-
form can obtain a lower-rank transformed tensor and enhance
The authors are with the School of Mathematical Sciences, University of
Electronic Science and Technology of China, Chengdu, China.the recovery quality [12], [13]. (ii) The characterization of
low-rankness of transformed frontal slices. The implicit low-
rankness of HSIs is exploited by the low-rank modeling of
frontal slices in the transformed domain.
Classical t-SVD-based methods mainly focused on the Ô¨Årst
building blocks, i.e., the design of different transforms. For
example, the discrete Fourier transform (DFT) [14] was Ô¨Årst
used in the t-SVD, and then the discrete cosine transform
(DCT) [15] was employed. Later methods exploited more
representative and Ô¨Çexible transforms such as non-invertible
transforms [16] and data-dependent transforms [17] to en-
hance the low-rankness of transformed frontal slices. These
methods have achieved increasingly satisfactory results for
HSI denoising [10], [11]. Nevertheless, these t-SVD methods
pay less attention to the second building block, i.e., the exact
characterization of transformed frontal slices. SpeciÔ¨Åcally, they
all employ shallow representations such as low-rank matrix
factorization (MF) [13], QR factorization [18], and nuclear
norm [12], [16] to characterize the transformed frontal slices.
In this work, we exploit a more representative formulation
to capture complex structures of transformed frontal slices.
SpeciÔ¨Åcally, we leverage the hierarchical matrix factorization
(HMF), which tailors a hierarchical formulation of learnable
matrices along with nonlinear layers to capture each frontal
slice in the transformed domain. The hierarchical modeling
ability of HMF makes it more representative to capture the
complex structures of HSIs. Meanwhile, we leverage the
hierarchical nonlinear transform (HNT) to enhance the low-
rankness of transformed frontal slices. With the H ierarchical
nonlinear transform and H ierarchical matrix factorization, we
develop a new T ensor F actorization method (termed as H2TF)
under the t-SVD framework. Correspondingly, we develop the
H2TF-based HSI denoising model. Attributed to the stronger
representation abilities of HMF than shallow MF or its surro-
gates, our H2TF-based model can better capture Ô¨Åne details
of the underlying clean HSI than conventional t-SVD-based
methods. Thus, our model is expected to deliver better HSI
denoising results. Meanwhile, the parameters of H2TF can
be inferred from the observed noisy HSI in an unsupervised
manner. In summary, the contributions of this letter are:
(i)We propose a new tensor factorization, i.e., the H2TF,
which leverages the expressive power of two key building
blocks‚Äîthe HNT and the HMF, to respectively enhance the
low-rankness of transformed data and characterize complex
structures of transformed frontal slices. By virtue of their
hierarchical modeling abilities, H2TF can faithfully capturearXiv:2304.11141v1  [cs.CV]  21 Apr 20232
Ô¨Åne details of the clean HSI, and thus is beneÔ¨Åcial for
effectively removing heavy noise in the HSI.
(ii)We suggest an unsupervised H2TF-based HSI denoising
model and develop an alternating direction method of mul-
tipliers (ADMM)-based algorithm. Extensive experiments on
simulated and real-world data validate the superiority of our
method over state-of-the-art (SOTA) HSI denoising methods,
especially for details preserving and heavy noise removal.
II. T HEPROPOSED H2TF
A. The t-SVD framework
We Ô¨Årst introduce the general formulation of t-SVD. Sup-
pose that the noisy HSI Y2RhwbadmitsY=X+N,
whereXdenotes the clean HSI and Ndenotes noise. To
infer the underlying clean HSI Xfrom the observed Y, t-SVD
method generally formulates the following model:
min
Z;L(Y;X) +X
k (Z(k));whereX=(Z):(1)
Here,L(Y;X)denotes the Ô¨Ådelity term and  (Z(k))repre-
sents the low-rank characterization of Z(k)(which denotes
thek-th frontal (spatial) slice of Z2Rhwb[16]).() :
Rhwb!Rhwbdenotes a transform with learnable
parameters, which transforms the low-rank representation
Zinto the original domain. Sometimes the transform ()
may not be learnable (e.g., the Ô¨Åxed DFT [14]), and in those
situations the optimization variable only includes Z.
The philosophy of the t-SVD model (1) is to minimize the
rank in the transformed domain, which can model the implicit
low-rankness of HSI. There are naturally two key building
blocks for exactly modeling the implicit low-rankness, i.e.,
the selection of the transform ()and the exact low-rank
characterization  ()of the transformed frontal slice Z(k).
Most t-SVD-based methods focus on the design of different
transforms()(see examples in [13], [16], [17]), but all
of them pay less attention to the exact characterization of
the transformed frontal slice. They mostly adopt shallow
representations to characterize Z(k), e.g., MF [13], [19], QR
factorization [18], and nuclear norm [15], [16]. However,
these shallow representations may not be expressive enough
to capture Ô¨Åne details of the clean HSI. Therefore, more rep-
resentative methods are desired to enhance the representation
abilities of the model in the transformed domain.
B. HMF for Characterizing Z(k)
To cope with this challenge, we leverage the HMF (hierar-
chical matrix factorization) to characterize Z(k). The hierar-
chical modeling ability of HMF helps it more faithfully capture
complex structures of the transformed frontal slice Z(k)than
shallow counter partners, e.g., SVD, MF, and QR factorization.
The standard MF used in previous t-SVD methods [13], [19]
decomposes a low-rank matrix Z2Rhwinto two factors as
Z=W2W1, where W22Rhr,W12Rrw, andris the
rank. To model the hierarchical structures of Z, we extend the
MF to the product of multiple matrix factors fWdgl
d=1:
Z=WlWl 1W1; (2)
whereWd2Rrdrd 1,rl=h, andr0=w. It was shown in
[20] that such a linear HMF can induce an implicit low-rankregularization on Zwhen using gradient-based optimization.
Generally, the larger lis (i.e., adding depth to the HMF),
the tendency towards low-rank solutions goes stronger and
oftentimes leads to better matrix recovery performances. Thus,
the HMF is suitable to play the role of low-rank regularization
in the t-SVD model (1).
Nevertheless, the linear HMF (2) may not be sufÔ¨Åcient
to capture nonlinear interactions inside HSIs. It motivates
us to utilize the nonlinear HMF [21], [22] to model the
low-rank matrix ZviaZ=Wl(Wl 1W3(W2W1)),
where()is a nonlinear scalar function. Classical HMF-
based methods [20], [21] only utilize HMF to tackle the two-
dimensional matrix. However, matrixing the HSI inevitably
destroys its high-dimensional data structures. Therefore, we
suggest tailoring bnonlinear HMFs to model the transformed
tensorZby using each HMF to represent one of the frontal
slices ofZ. Formally, we represent each frontal slice of Zby
Z(k)=W(k)
l(W(k)
l 1W(k)
3(W(k)
2W(k)
1));k= 1;2;;b:
The above HMFs can be equivalently formulated as the ten-
sor formulationZ=Wl(Wl 1W 3(W2W1)),
where is the tensor face-wise product [23] and fWd2
Rrdrd 1bgl
d=1are some factor tensors.
Compared to shallow counter partners, e.g., MF, QR fac-
torization, and nuclear norm, the above nonlinear HMF can
better capture complex hierarchical structures of HSIs due to
its nonlinear hierarchical modeling abilities, which helps to
better recover Ô¨Åne details of HSI and remove heavy noise.
C. The Proposed H2TF
Next, we introduce our H2TF. Recall that two key building
blocks in the t-SVD are the selection of the transform ()
and the characterization of the transformed frontal slice Z(k).
We have leveraged the HMF to characterize Z(k), and we
further leverage the HNT (hierarchical nonlinear transform)
as the Ô¨Årst building block ():
(Z) :=((Z 3H1)3 3Hm 1)3Hm;
where()is a nonlinear scalar function, :=fHp2
Rbbgm
p=1are learnable parameters of HNT, and 3is the
mode-3 tensor-matrix product [24]. It was throughout demon-
strated [13] that the HNT can effectively enhance the low-
rankness of transformed tensor and thus obtain a better low-
rank representation than shallow transforms (e.g., DFT [14]
and DCT [15]), which beneÔ¨Åts the implicit low-rank modeling.
DeÔ¨Ånition 1 (H2TF) .Finally, we can deÔ¨Åne the following
factorization modality of a certain low-rank tensor Xparam-
eterized byfWdgl
d=1andfHpgm
p=1:
X= 
Wl(Wl 1W 3(W2W1))| {z }
Hierarchical matrix factorization
;
(Z) :=((Z 3H1)3 3Hm 1)3Hm| {z }
Hierarchical nonlinear transform;(3)
which we call the H2TF representation of X.
A general illustration of the proposed H2TF is shown in Fig.
1. H2TF beneÔ¨Åts from the HMF to exploit complex hierarchi-
cal information of transformed frontal slices and the HNT to3
PSNR 23.43 ( l = 2)
 PSNR 23.65 ( l = 3) PSNR 24.15 ( l = 4) PSNR 24.34 ( l = 5)
PSNR 17.41 ( m = 1) PSNR 23.95 ( m = 2) PSNR 23.98 ( m = 3) PSNR 24.34 ( m = 4)
)
‚Ä¶ Hierarchical  matrix factorization 
Hierarchical nonlinear transform
‚Ä¶ 
)
)
‚Ä¶ Hierarchical  matrix factorization 
Hierarchical  nonlinear transform
‚Ä¶ 
)
)
‚Ä¶ Hierarchical  matrix factorization 
Hierarchical  nonlinear transform
‚Ä¶ 
)
Fig. 1. A general illustration of the H2TF representation of a tensor X. The
nonlinear layer ()is omitted for space consideration.
enhance the low-rankness in the transformed domain. With the
hierarchical modeling abilities of H2TF, the characterization
of HSIs would be more accurate. Therefore, H2TF can more
faithfully capture Ô¨Åne details and rich textures of HSIs and
remove heavy mixed noise. Now, we discuss the connections
between H2TF and some popular matrix/tensor factorizations.
Remark 1. By changing the layer number of hierarchical
matrix factorization (i.e., l) and the layer number of hier-
archical nonlinear transform (i.e., m), H2TF includes many
matrix/tensor factorizations as special cases:
(i)Whenl= 2, i.e., the HMF degenerates into the MF ,
our H2TF degenerates into the hierarchical low-rank tensor
factorization [13]. (ii)Whenm= 1 andHmis an identity
matrix (i.e., the transform ()is an identical mapping), our
H2TF degenerates into the plain HMFs [21], [22] applied on
each frontal slice of the tensor separately. In the following,
we interpret this case as ‚Äú m= 0‚Äù since the transform is
neglected. (iii)Whenl= 2andm= 1withHmbeing the Ô¨Åxed
inverse DFT matrix, our H2TF degenerates into the classical
low-tubal-rank tensor factorization [19], [25].
Moreover, H2TF can explicitly preserve the low-rankness of
the tensor when omitting some nonlinearity, as stated below.
Lemma 1. Suppose thatX= 
Wl(Wl 1W1)
2
Rhwb, wherefWd2Rrdrd 1bgl
d=1(rl=handr0=w)
are factor tensors, (Z) :=Z 3F 1is the inverse DFT,
andF 1is the inverse DFT matrix (which is a special case
of H2TF). Then we have rankt(X)minfr0;r1;;rlg,
where rankt()denotes the tensor tubal-rank [12]‚Äì[14].
Lemma 1 indicates that H2TF can preserve the low-rankness
in the linear special case, where the degree of low-rankness
(the upper bound of tubal-rank) is conditioned on the sizes of
factor tensors. Therefore, we can readily control the degree of
low-rankness by tuning the sizes of factor tensors in H2TF.
D. H2TF for HSI Denoising
H2TF is a potential tool for multi-dimensional data analysis
and processing. We consider HSI denoising as a representative
real-world application. By applying the H2TF representation
(3) into (1), we can obtain the following HSI denoising model:
min
fWdgl
d=1;fHpgm
p=1L(Y;X);
whereX= 
Wl(Wl 1W 3(W2W1))
:
In the HSI denoising problem, we consider the Ô¨Ådelity term as
L(Y;X) =kY X Sk2
F+1kSk`1, wherekk2
Fdenotes the
Frobenius norm and we introduce S2Rhwbto represent
sparse noise (often contains impulse noise and stripes). The
`1-norm enforces the sparsity on Sso that the sparse noise
can be eliminated. Here, 1is a trade-off parameter.
Meanwhile, our H2TF can be readily combined with other
proven techniques to enhance the denoising abilities. Here,we consider the hybrid spatial-spectral TV (HSSTV) regu-
larization [26] to further capture spatial and spatial-spectral
local smoothness of HSIs. The HSSTV is formulated as
kXk HSSTV :=2kXk TV+3kXk SSTV , wherekXk TV:=
krxXk`1+kryXk`1,kXk SSTV :=krx(rzX)k`1+
kry(rzX)k`1, andi(i= 2;3) are trade-off parameters.
Here, the derivative operators are deÔ¨Åned as (rxX)(i;j;k):=
X(i+1;j;k) X(i;j;k),(ryX)(i;j;k):=X(i;j+1;k) X(i;j;k), and
(rzX)(i;j;k):=X(i;j;k+1) X (i;j;k), whereX(i;j;k)denotes
the(i;j;k )-th element ofX.
Based on the formulations of Ô¨Ådelity term and HSSTV , the
proposed H2TF-based HSI denosing model is formulated as
min
fWdgl
d=1;fHpgm
p=1;SkY X Sk2
F+1kSk`1+kXk HSSTV;
whereX= 
Wl(Wl 1W 3(W2W1))
:
(4)
Compared to previous t-SVD-based HSI denoising methods
[10], [11], H2TF has powerful representation abilities brought
from the hierarchical structures and thus could better capture
Ô¨Åne details of HSIs. Besides, the parameters of H2TF are
unsupervisedly inferred from the noisy HSI by optimizing (4)
without the requirement of training process.
E. ADMM-Based Algorithm
To tackle the problem (4), we develop an ADMM-based al-
gorithm. By introducing auxiliary variables Vi(i= 1;2;3;4),
(4) can be equivalently formulated as
min
fWdgl
d=1;fHpgm
p=1;
S;fVig4
i=1kY X Sk2
F+1kSk`1+2kV1k`1+
2kV2k`1+3kV3k`1+3kV4k`1;
s:t:V1=rxX;V2=ryX;V3=rx(rzX);V4=ry(rzX);
whereX= 
Wl(Wl 1W 3(W2W1))
. The
corresponding augmented Lagrangian function is
L(fWdgl
d=1;fHpgm
p=1;S;fVig4
i=1;fig4
i=1)
=kY X Sk2
F+1kSk`1+2kV1k`1+2kV2k`1+
3kV3k`1+3kV4k`1+
2krxX+1
 V 1k2
F+

2kryX+2
 V 2k2
F+
2krx(rzX) +3
 V 3k2
F+

2kry(rzX) +4
 V 4k2
F;
whereis the penalty parameter, i(i= 1;2;3;4)are
multipliers, andXis deÔ¨Åned as in (3). The joint minimization
problem can be decomposed into easier subproblems, followed
by the update of Lagrangian multipliers.
TheVi(i= 1;2;3;4)subproblems are
8
>>>><
>>>>:minV1
2krxXt+t
1
 V 1k2
F+2kV1k`1
minV2
2kryXt+t
2
 V 2k2
F+2kV2k`1
minV3
2krx(rzXt) +t
3
 V 3k2
F+3kV3k`1
minV4
2kry(rzXt) +t
4
 V 4k2
F+3kV4k`1;
which can be exactly solved by Vt+1
1=Soft2
(rxXt+t
1
),
Vt+1
2=Soft2
(ryXt+t
2
),Vt+1
3=Soft3
(rx(rzXt)+
t
3
), andVt+1
4 =Soft3
(ry(rzXt) +t
4
), where 
Softv(X)
(i;j;k):=sign(X(i;j;k)) maxfjX(i;j;k)j v;0g.4
TheSsubproblem is minSkY Xt Sk2
F+1kSk`1, which
can be exactly solved by St+1=Soft1
2(Y Xt).
TheXsubproblem is
min
fWdgl
d=1;fHpgm
p=1kY X Stk2
F+
2(krxX Dt
1k2
F+
kryX Dt
2k2
F+krx(rzX) Dt
3k2
F+kry(rzX) Dt
4k2
F);
whereDt
i:=Vt
i t
i
(i= 1;2;3;4). The clean HSI Xis
parameterized byfWdgl
d=1andfHpgm
p=1, as presented in Eq.
(3). To tackle the nonlinear and nonconvex Xsubproblem, we
apply the adaptive moment estimation (Adam) algorithm [27].
In each iteration of the ADMM-based algorithm, we employ
one step of the Adam to update fWdgl
d=1andfHpgm
p=1.
Finally, the Lagrange multipliers are updated by t+1
1=
t
1+(rxXt Vt
1),t+1
2=t
2+(ryXt Vt
2),t+1
3=t
3+
(rx(rzXt) Vt
3), andt+1
4=t
4+(ry(rzXt) Vt
4).
III. E XPERIMENTS
A. Experimental Settings
We compare H2TF with SOTA model-based methods LRT-
DTV [28], SSTV-LRTF [11], RCTV [4], and HLRTF [13]
and deep learning methods HSID-CNN [9] and SDeCNN [8].
We use the pre-trained models of HSID-CNN and SDeCNN
provided by authors. All hyperparameters of these methods are
carefully adjusted based on authors‚Äô suggestions to achieve the
best results. We report the peak-signal-to-noise-ratio (PSNR)
and structural similarity (SSIM). For more implementation
details, please refer to supplementary materials.
We include four HSIs and three multi-spectral images
(MSIs) as simulated datasets. The HSIs are WDC (256256
32),PaviaC (25625632),PaviaU (25625632), and
Indian (14514532). The MSIs are Beads (25625631),
Cloth (25625631), and Cups (25625631) in the
CA VE dataset [29]. The noise settings of simulated data are
explained as below. Case 1 : All bands are added with Gaussian
noise of standard deviation 0.2. Case 2 : The Gaussian noise
for Case 1 is kept. Besides, all bands are added with impulse
noise with sampling rate 0.1. Case 3 : The same as Case 2
plus 50% of bands corrupted by deadlines. The number of
deadlines for each chosen band is generated randomly from 6
to 10, and their spatial width is chosen randomly from 1 to 3.
Case 4 : The same as Case 2 plus 40% of bands corrupted by
stripes. The number of stripes in each corrupted band is chosen
randomly from 6 to 15. Case 5 : The same as Case 2 plus both
the deadlines in Case 3 and the stripes in Case 4. To test our
method in real scenarios, we choose two real-world noisy HSIs
Shanghai (30030032) and Urban (30730732) as
real-world experimental datasets.
B. Experimental Results
1) Results: The quantitative results on simulated data are
reported in Table I. Our H2TF obtains better quantitative re-
sults than other competitors. H2TF outperforms other TV and
tensor factorization-based methods (LRTDTV , SSTV-LRTF,
RCTV , and HLRTF), which shows the stronger representation
abilities of H2TF than existing shallow tensor factorizations
thanks to the hierarchical structures of H2TF. Some visual
results on simulated and real data are shown in Figs. 2-3.TABLE I
AVERAGE QUANTITATIVE DENOISING RESULTS BY DIFFERENT METHODS .
Dataset MethodCase 1 Case 2 Case 3 Case 4 Case 5
PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM
HSIs
WDC
PaviaC
PaviaU
IndianLRTDTV 30.88 0.888 29.55 0.849 28.29 0.825 29.35 0.843 28.31 0.824
SSTV-LRTF 30.77 0.887 30.35 0.879 28.32 0.839 29.51 0.859 27.42 0.812
HSID-CNN 29.61 0.863 22.89 0.691 21.98 0.661 22.16 0.669 21.22 0.635
SDeCNN 30.26 0.873 23.97 0.735 23.33 0.725 23.41 0.723 22.63 0.714
RCTV 29.53 0.853 29.05 0.839 26.75 0.782 28.47 0.826 26.34 0.772
HLRTF 30.12 0.868 29.65 0.855 29.58 0.853 29.15 0.846 29.03 0.841
H2TF 32.51 0.919 31.41 0.900 31.34 0.899 30.83 0.893 30.84 0.892
MSIs
Beads
Cloth
CupsLRTDTV 28.85 0.889 27.05 0.838 26.31 0.828 26.83 0.830 26.13 0.819
SSTV-LRTF 27.64 0.878 27.48 0.864 26.25 0.855 26.79 0.844 25.11 0.823
HSID-CNN 25.86 0.827 21.22 0.660 20.97 0.645 20.68 0.646 20.34 0.626
SDeCNN 28.43 0.886 22.04 0.715 22.32 0.709 21.53 0.706 21.70 0.698
RCTV 28.15 0.869 27.49 0.866 25.77 0.839 26.98 0.854 25.46 0.829
HLRTF 29.21 0.884 28.73 0.886 28.67 0.884 28.10 0.870 28.03 0.868
H2TF 31.51 0.940 29.46 0.906 29.47 0.901 29.22 0.908 29.03 0.896
H2TF generally outperforms other competitors in two aspects.
First, H2TF can more effectively remove heavy mixed noise.
Second, H2TF preserves Ô¨Åne details of HSIs better than other
methods. The superior performances of H2TF are mainly due
to its hierarchical modeling abilities, which help to better
characterize Ô¨Åne details of HSI and robustly capture the
underlying structures of HSI under extremely heavy noise.
More visual results can be found in supplementary.
2) Discussions: The HMF is an important building block
in H2TF. We test the inÔ¨Çuence of the layer number of HMF
(i.e.,l); see Fig. 4 (a). A suitable layer number of HMF
(e.g.,l= 5 ) can obtain both good performances and a
lightweight model. The HNT is another important building
block. We change the layer number of HNT to test its
inÔ¨Çuence; see Fig. 4 (b). Also, a proper layer number of HNT
(e.g.,m= 2) can bring good performances. According to
Lemma 1, the sizes of factor tensors in HMF, i.e., frdg4
d=1,
determine the degree of low-rankness. Hence, we test such
connections by changing the sizes of factor tensors; see Fig.
4 (c) (Here, r0andr5are Ô¨Åxed as the sizes of observed
data andfrdg4
d=1are selected inf(1;2;4;8);(2;4;8;16);
(3;6;12;24);;(20;40;80;160)g). When the sizes (rank)
are too small, the model lacks representation abilities and
when the sizes (rank) are too large, the model overÔ¨Åts.
Nevertheless, our method is quite robust w.r.t. frdg4
d=1.
IV. C ONCLUSIONS
We propose the H2TF for HSI denoising. Our H2TF simul-
taneously leverages the hierarchical matrix factorization and
the hierarchical nonlinear transform to compactly represent
HSIs with powerful representation abilities, which can more
faithfully capture Ô¨Åne details of HSIs than classical tensor
factorization methods. Comprehensive experiments validate
the superiority of H2TF over SOTA methods, especially for
HSI details preserving and heavy noise removal.
REFERENCES
[1] N. Liu, W. Li, R. Tao, and J. E. Fowler, ‚ÄúWavelet-domain low-
rank/group-sparse destriping for hyperspectral imagery,‚Äù IEEE Trans-
actions on Geoscience and Remote Sensing , vol. 57, no. 12, pp. 10310‚Äì
10321, 2019.
[2] W. He, H. Zhang, L. Zhang, W. Philips, and W. Liao, ‚ÄúWeighted sparse
graph based dimensionality reduction for hyperspectral images,‚Äù IEEE
Geoscience and Remote Sensing Letters , vol. 13, no. 5, pp. 686‚Äì690,
2016.
[3] F. Xu, Y . Chen, C. Peng, Y . Wang, X. Liu, and G. He, ‚ÄúDenoising
of hyperspectral image using low-rank matrix factorization,‚Äù IEEE
Geoscience and Remote Sensing Letters , vol. 14, no. 7, pp. 1141‚Äì1145,
2017.5
Original Observed LRTDTV SSTV-LRTF HSID-CNN SDeCNN RCTV HLRTF H2TF
Fig. 2. Pseudo-color images of HSI denoising results by different methods on simulated data PaviaC Case 4 (Ô¨Årst row) and Cups Case 5 (second row).
Observed LRTDTV SSTV-LRTF HSID-CNN SDeCNN RCTV HLRTF H2TF
Fig. 3. Pseudo-color images of HSI denoising results by different methods on real-world data Shanghai (Ô¨Årst row) and Urban (second row).
Fig. 4. Results on Beads Case 5 with (a) different layer number of HMF, (b)
different layer number of HNT, and (c) different sizes of factor tensors.
[4] J. Peng, H. Wang, X. Cao, X. Liu, X. Rui, and D. Meng, ‚ÄúFast
noise removal in hyperspectral images via representative coefÔ¨Åcient total
variation,‚Äù IEEE Transactions on Geoscience and Remote Sensing , 2022.
doi=10.1109/TGRS.2022.3229012.
[5] B. Rasti, M. O. Ulfarsson, and P. Ghamisi, ‚ÄúAutomatic hyperspectral im-
age restoration using sparse and low-rank modeling,‚Äù IEEE Geoscience
and Remote Sensing Letters , vol. 14, no. 12, pp. 2335‚Äì2339, 2017.
[6] W. He, Q. Yao, C. Li, N. Yokoya, Q. Zhao, H. Zhang, and L. Zhang,
‚ÄúNon-local meets global: An iterative paradigm for hyperspectral im-
age restoration,‚Äù IEEE Transactions on Pattern Analysis and Machine
Intelligence , vol. 44, no. 4, pp. 2089‚Äì2107, 2022.
[7] Y . Chang, L. Yan, H. Fang, S. Zhong, and W. Liao, ‚ÄúHSI-DeNet:
Hyperspectral image restoration via convolutional neural network,‚Äù IEEE
Transactions on Geoscience and Remote Sensing , vol. 57, no. 2, pp. 667‚Äì
682, 2019.
[8] A. Maffei, J. M. Haut, M. E. Paoletti, J. Plaza, L. Bruzzone, and
A. Plaza, ‚ÄúA single model CNN for hyperspectral image denoising,‚Äù
IEEE Transactions on Geoscience and Remote Sensing , vol. 58, no. 4,
pp. 2516‚Äì2529, 2020.
[9] Q. Yuan, Q. Zhang, J. Li, H. Shen, and L. Zhang, ‚ÄúHyperspectral image
denoising employing a spatial‚Äìspectral deep residual convolutional neu-
ral network,‚Äù IEEE Transactions on Geoscience and Remote Sensing ,
vol. 57, no. 2, pp. 1205‚Äì1218, 2019.
[10] Y .-B. Zheng, T.-Z. Huang, X.-L. Zhao, T.-X. Jiang, T.-H. Ma, and T.-Y .
Ji, ‚ÄúMixed noise removal in hyperspectral image via low-Ô¨Åbered-rank
regularization,‚Äù IEEE Transactions on Geoscience and Remote Sensing ,
vol. 58, no. 1, pp. 734‚Äì749, 2020.
[11] H. Fan, C. Li, Y . Guo, G. Kuang, and J. Ma, ‚ÄúSpatial-spectral total
variation regularized low-rank tensor decomposition for hyperspectral
image denoising,‚Äù IEEE Transactions on Geoscience and Remote Sens-
ing, vol. 56, no. 10, pp. 6196‚Äì6213, 2018.
[12] J.-L. Wang, T.-Z. Huang, X.-L. Zhao, T.-X. Jiang, and M. K. Ng, ‚ÄúMulti-
dimensional visual data completion via low-rank tensor representation
under coupled transform,‚Äù IEEE Transactions on Image Processing ,
vol. 30, pp. 3581‚Äì3596, 2021.
[13] Y . Luo, X. Zhao, D. Meng, and T. Jiang, ‚ÄúHLRTF: Hierarchical low-rank
tensor factorization for inverse problems in multi-dimensional imaging,‚Äù
inIEEE/CVF Conference on Computer Vision and Pattern Recognition ,
pp. 19281‚Äì19290, 2022.
[14] C. Lu, J. Feng, Y . Chen, W. Liu, Z. Lin, and S. Yan, ‚ÄúTensor robust
principal component analysis with a new tensor nuclear norm,‚Äù IEEETransactions on Pattern Analysis and Machine Intelligence , vol. 42,
no. 4, pp. 925‚Äì938, 2020.
[15] C. Lu, X. Peng, and Y . Wei, ‚ÄúLow-rank tensor completion with a
new tensor nuclear norm induced by invertible linear transforms,‚Äù in
IEEE/CVF Conference on Computer Vision and Pattern Recognition ,
pp. 5989‚Äì5997, 2019.
[16] T.-X. Jiang, M. K. Ng, X.-L. Zhao, and T.-Z. Huang, ‚ÄúFramelet
representation of tensor nuclear norm for third-order tensor completion,‚Äù
IEEE Transactions on Image Processing , vol. 29, pp. 7233‚Äì7244, 2020.
[17] H. Kong, C. Lu, and Z. Lin, ‚ÄúTensor Q-rank: new data dependent
deÔ¨Ånition of tensor rank,‚Äù Machine Learning , vol. 110, pp. 1867‚Äì1900,
2021.
[18] Y . Zheng and A.-B. Xu, ‚ÄúTensor completion via tensor QR decom-
position and L2,1-norm minimization,‚Äù Signal Processing , vol. 189,
p. 108240, 2021.
[19] P. Zhou, C. Lu, Z. Lin, and C. Zhang, ‚ÄúTensor factorization for low-rank
tensor completion,‚Äù IEEE Transactions on Image Processing , vol. 27,
no. 3, pp. 1152‚Äì1163, 2018.
[20] S. Arora, N. Cohen, W. Hu, and Y . Luo, ‚ÄúImplicit regularization in deep
matrix factorization,‚Äù in Advances in Neural Information Processing
Systems , vol. 32, pp. 7413‚Äì7424, 2019.
[21] Z. Li, T. Sun, H. Wang, and B. Wang, ‚ÄúAdaptive and implicit regu-
larization for matrix completion,‚Äù SIAM Journal on Imaging Sciences ,
vol. 15, no. 4, pp. 2000‚Äì2022, 2022.
[22] J. Fan and J. Cheng, ‚ÄúMatrix completion by deep matrix factorization,‚Äù
Neural Networks , vol. 98, pp. 34‚Äì41, 2018.
[23] E. Kernfeld, M. Kilmer, and S. Aeron, ‚ÄúTensor-tensor products with in-
vertible linear transforms,‚Äù Linear Algebra and its Applications , vol. 485,
pp. 545‚Äì570, 2015.
[24] M. E. Kilmer, K. Braman, N. Hao, and R. C. Hoover, ‚ÄúThird-order
tensors as operators on matrices: A theoretical and computational
framework with applications in imaging,‚Äù SIAM Journal on Matrix
Analysis and Applications , vol. 34, no. 1, pp. 148‚Äì172, 2013.
[25] X.-Y . Liu, S. Aeron, V . Aggarwal, and X. Wang, ‚ÄúLow-tubal-rank
tensor completion using alternating minimization,‚Äù IEEE Transactions
on Information Theory , vol. 66, no. 3, pp. 1714‚Äì1737, 2019.
[26] S. Takeyama, S. Ono, and I. Kumazawa, ‚ÄúMixed noise removal for
hyperspectral images using hybrid spatio-spectral total variation,‚Äù in
IEEE International Conference on Image Processing , pp. 3128‚Äì3132,
2019.
[27] D. Kingma and J. Ba, ‚ÄúADAM: A method for stochastic optimization,‚Äù
inInternational Conference on Learning Representations , 2014.
[28] Y . Wang, J. Peng, Q. Zhao, Y . Leung, X.-L. Zhao, and D. Meng,
‚ÄúHyperspectral image restoration via total variation regularized low-rank
tensor decomposition,‚Äù IEEE Journal of Selected Topics in Applied Earth
Observations and Remote Sensing , vol. 11, no. 4, pp. 1227‚Äì1243, 2017.
[29] F. Yasuma, T. Mitsunaga, D. Iso, and S. K. Nayar, ‚ÄúGeneralized assorted
pixel camera: Postcapture control of resolution, dynamic range, and
spectrum,‚Äù IEEE Transactions on Image Processing , vol. 19, no. 9,
pp. 2241‚Äì2253, 2010.