A two-way heterogeneity model for dynamic networks
Binyan Jiang
Department of Applied Mathematics, The Hong Kong Polytechnic University
Hong Kong, by.jiang@polyu.edu.hk
Chenlei Leng
Department of Statistics, University of Warwick
United Kindom, C.Leng@warwick.ac.uk
Ting Yan
Department of Statistics, Central China Normal University
China, tingyanty@mail.ccnu.edu.cn
Qiwei Yao
Department of Statistics, London School of Economics
United Kindom, q.yao@lse.ac.uk
Xinyang Yu
Department of Applied Mathematics, The Hong Kong Polytechnic University
Hong Kong, xinyang.yu@connect.polyu.hk
Abstract
Dynamic network data analysis requires joint modelling individual snapshots and time dynamics.
This paper proposes a new two-way heterogeneity model towards this goal. The new model equips
each node of the network with two heterogeneity parameters, one to characterize the propensity
of forming ties with other nodes and the other to differentiate the tendency of retaining existing
ties over time. Though the negative log-likelihood function is non-convex, it is locally convex in
a neighbourhood of the true value of the parameter vector. By using a novel method of moments
estimator as the initial value, the consistent local maximum likelihood estimator (MLE) can be
obtained by a gradient descent algorithm. To establish the upper bound for the estimation error
of the MLE, we derive a new uniform deviation bound, which is of independent interest. The
usefulness of the model and the associated theory are further supported by extensive simulation and
the analysis of some real network data sets.
Keywords : Degree heterogeneity, Dynamic networks, Maximum likelihood estimation, Uniform
deviation bound
1 Introduction
Network data featuring prominent interactions between subjects arise in various areas such as biology,
economics, engineering, medicine, and social sciences [27, 20]. As a rapidly growing field of active
research, statistical modelling of networks aims to capture and understand the linking patterns in these
data. A large part of the literature has focused on examining these patterns for canonical, static networks
that are observed at a single snapshot. Due to the increasing availability of networks that are observed
multiple times, models for dynamic networks evolving in time are of increasing interest now. These
1arXiv:2305.12643v2  [stat.ME]  12 Apr 2024models typically assume, among others, that networks observed at different time are independent [28, 1],
independent conditionally on some latent processes [4, 23], or drawn sequentially from an exponential
random graph model conditional on the previous networks [11, 10, 21].
One of the stylized facts of real-life networks is that their nodes often have different tendencies to form
ties and may evolve differently over time. The former is manifested by the fact that the so-called hub
nodes have many links while the peripheral nodes have small numbers of connections in, for example,
a big social network. The latter becomes evident when some individuals are more active in seeking
new ties/friends than the others. In this paper, we refer to these two kinds of heterogeneity as static
heterogeneity and dynamic heterogeneity respectively. Also known as degree heterogeneity in the static
network literature, static heterogeneity has featured prominently in several popular models widely used
in practice including the stochastic block model and its degree-corrected generalization [17]. See also
[15, 29, 16, 19], and the references therein. Another common and natural approach to capture the static
heterogeneity is to introduce node-specific parameters, one for each node. For single networks, this is
often conducted via modelling the logit of the link probability between each pair of nodes as the sum
of their heterogeneity parameters. Termed as the β-model [2], this model and its generalizations have
been extensively studied when a single static network is observed [39, 18, 7, 35, 3, 30].
Withnobserved networks each having pnodes, the goal of this study is two-fold: (i) We propose a
dynamic network model named the two-way heterogeneity model that captures both static heterogeneity
and dynamic heterogeneity, and develop the associate inference methodology; (ii) We establish new
generic asymptotic results that can be applied or extended to different models with a large number of
parameters (in relation to p). We focus on the scenario that the number of nodes pgoes to infinity. Our
asymptotic results hold when np→∞ , thoughnmay be fixed. The main contributions of our paper
can be summarized as follows.
•We introduce a reparameterization of the general autoregressive network model [14] to accommo-
date variations in both node degree and dynamic fluctuations. This novel approach can be regarded
as an extension of the β-model [2] to a dynamic framework. It encompasses two sets of parameters
for heterogeneity: one governs static variations, akin to those in the standard β-model, while the
other addresses dynamic fluctuations. Unlike the general model in [14], which necessitates a large
number of network observations (i.e. n→∞ ), we demonstrate the validity of our formulation even
in scenarios where nis small but pis large.
•The formulation of our model gives rise to a high-dimensional non-convex loss function based on
likelihood. By establishing the local convexity of the loss function in a neighborhood of the true
parameters, we compute the local MLE by a standard gradient descent algorithm using a newly
proposed method of moments estimator (MME) as its initial value. To our best knowledge, this is
the first result in network data analysis for solving such a non-convex optimization problem with
algorithmic guarantees.
•Furthermore, to characterize the local MLE, we have derived its estimation error bounds in the
ℓ2norm and the ℓ∞norm when np→∞ in whichn≥2 can be finite. Due to the dynamic
structure of the data, the Hessian matrix of the loss function exhibits a complex structure. As
a result, existing analytical approaches, such as the interior point theorem [6, 37] developed for
static networks, are no longer applicable; see Section 3.1 for further elaboration. We derive a novel
locally uniform deviation bound in a neighborhood of the true parameters with a diverging radius.
Based on this we first establish ℓ2norm consistency of the MLE, which paves the way for the
uniform consistency in ℓ∞norm.
•In establishing the locally uniform deviation bound, we have provided a general result for functions
of the form L(θ) =1
p/summationtext
1≤i̸=j≤pli,j(θi,θj)Yi,jas defined in (4.11) below. This result explores
the sparsity structure of L(θ) in the sense that most of its higher order derivatives are zero – the
condition which our model satisfies, and provides a new bound that substantially extends the scope
2of empirical processes for the M-estimators [32] for the models with a fixed number of parameters
to those with a growing number of parameters. The result here is of independent interest as it can
be applied to any model with an objective function taking the form of L.
The rest of the paper is organized as follows. We introduce in Section 2 the new two-way heterogeneity
model and present its properties. The estimation of its local MLE in a neighborhood of the truth and the
associated theoretical properties are presented in Section 3. The development of these properties relies
on new local deviation bounds which are presented in Section 4. Simulation studies and an analysis of
ants interaction data are reported in Section 5. We conclude the paper in Section 6. All technical proofs
are relegated to Appendix A. Additional numerical results showcasing the effectiveness of our method
in aiding community detection within stochastic block structures, along with an application aimed at
understanding dynamic protein-protein interaction networks, are provided in Appendix B.
2 Two-way Heterogeneity Model
Consider a dynamic network defined on pnodes which are unchanged over time. Denote by a p×p
matrix Xt= (Xt
i,j) its adjacency matrix at time t, i.e.Xt
i,j= 1 indicates the existence of a connection
between nodes iandjat timet, and 0 otherwise. We focus on undirected networks without self-loops,
i.e.,Xt
i,j=Xt
j,ifor all (i,j)∈J≡{ (i,j) : 1≤i<j≤p}, andXt
i,i= 0 for 1≤i≤p, though our
approach can be readily extended to directed networks.
To capture the autoregressive pattern in dynamic networks, [14] proposed to model the network
process via the following stationary AR(1) framework:
Xt
i,j=Xt−1
i,jI(εt
i,j= 0) +I(εt
i,j= 1), t≥1,
whereI(·) denotes the indicator function, and the εt
i,j, (i,j)∈J are independent innovations satisfying
P(εt
i,j= 1) =αi,j, P (εt
i,j=−1) =βi,j, P (εt
i,j= 0) = 1−αi,j−βi,j,
for some positive parameters αi,jandβi,j. This general model opts to neglect the inherent nature of
the networks and chooses to estimate each pair ( αi,j,βi,j) independently. As a result, there are p(p−1)
parameters and consistent model estimation requires n→∞ . Conversely, in numerous real-world sce-
narios, it is frequently noted that the number of network observations nis modest, while the number
of nodespcan significantly exceed n. Under such a scenario of small- n-large-p, the conventional model
outlined in [14] may not be suitable. To address this and to effectively capture node heterogeneity in
dynamic networks, as well as accommodate small- n-large-pnetworks, we propose the following reparam-
eterization for the general AR(1) model mentioned above. This reparameterization not only accounts
for inherent node heterogeneity but also reduces the parameter count from p(p−1) to 2p.
Definition 1.Two-way Heterogeneity Model (TWHM) . The data generating process satisfies
(2.1) Xt
i,j=I(εt
i,j= 0) +Xt−1
i,jI(εt
i,j= 1), (i,j)∈J,
where theεt
i,j, for (i,j)∈J andt≥1 are independent innovations with their distributions satisfying
(2.2) P(εt
i,j=r) =eβi,r+βj,r
1 +/summationtext1
k=0eβi,k+βj,kforr= 0,1, P (εt
i,j=−1) =1
1 +/summationtext1
k=0eβi,k+βj,k.
TWHM defined above is a reparametrization of the AR(1) network model [14] as it reduces the total
number of parameters from 2 p2therein to 2 p. By Proposition 1 of [14], the matrix process {Xt,t≥1}
is strictly stationary with
(2.3) P(Xt
i,j= 1) =eβi,0+βj,0
1 +eβi,0+βj,0= 1−P(Xt
i,j= 0),
31 1;0
2
2;0
3
3;0
4 4;01
2
3
41
2
3
41;1
2;1
3;1
4;1Time t Time t+ 1 Time t+ 2Figure 1: A schematic depiction of TWHM: βi,0,i= 1,...,4, are parameters to characterize the static
heterogeneity of nodes, while βi,1characterize their dynamic heterogeneity.
provided that we activate the process with X0= (X0
i,j) also following this stationary marginal distribu-
tion.
Furthermore,
E(Xt
i,j) =eβi,0+βj,0
1 +eβi,0+βj,0, Var(Xt
i,j) =eβi,0+βj,0
(1 +eβi,0+βj,0)2,
(2.4) ρi,j(|t−s|)≡Corr(Xt
i,j,Xs
i,j) =/parenleftigg
eβi,1+βj,1
1 +/summationtext1
r=0eβi,r+βj,r/parenrightigg|t−s|
.
Note that the connection probabilities in (2.3) depend on β0= (β1,0,···,βp,0)⊤only, and are of the
same form as the (static) β-model [2]. Hence we call β0the static heterogeneity parameter. Proposition
1 below confirms that means and variances of node degrees in TWHM also depend on β0only, and that
different values of βi,0reflect the heterogeneity in the degrees of nodes.
Under TWHM, it holds that
(2.5)P(Xt
i,j= 1|Xt−1
i,j= 0) =eβi,0+βj,0
1 +/summationtext1
k=0eβi,k+βj,k,P(Xt
i,j= 0|Xt−1
i,j= 1) =1
1 +/summationtext1
k=0eβi,k+βj,k.
Hence the dynamic changes (over time) of network Xtdepend on, in addition to β0,β1≡(β1,1,···,βp,1)⊤:
the largerβi,1is, the more likely Xt
i,jwill retain the value of Xt−1
i,jfor allj. Thus we call β1the dynamic
heterogeneity parameter, as its components reflect the different dynamic behaviours of the pnodes. A
schematic description of the model can be seen from Figure 1 where three snapshots of a dynamic
network with four nodes are depicted.
From now on, let {Xt}∼Pθdenote the stationary TWHM with parameters θ= (β⊤
0,β⊤
1)⊤, and
dt
i=/summationtextp
j=1Xt
i,jbe the degree of node iat timet. The proposition below lists some properties of the
node degrees.
Proposition 1. Let{Xt}∼Pθ. Then{(dt
1,...,dt
p),t= 0,1,2,···} is a strictly stationary process.
Furthermore for any 1≤i<j≤pandt,s≥0,
E(dt
i) =p/summationdisplay
k=1,k̸=ieβi,0+βk,0
1 +eβi,0+βk,0, Var(dt
i) =p/summationdisplay
k=1,k̸=ieβi,0+βk,0
(1 +eβi,0+βk,0)2,
ρd
i,j(|t−s|)≡Corr(dt
i,ds
j)
4=

Ci,ρ/summationtextp
k=1,k̸=i/parenleftbigg
eβi,1+βk,1
1+/summationtext1
r=0eβi,r+βk,r/parenrightbigg|t−s|
eβi,0+βk,0
(1+eβi,0+βk,0)2ifi=j,
0 if i̸=j,
whereCi,ρ=/parenleftig/summationtextp
k=1,k̸=ieβi,0+βk,0
(1+eβi,0+βk,0)2/parenrightig−1
.
Proposition 1 implies that when there exist constants β0andβ1such thatβi,0≈β0andβi,1≈β1
for alli, the degree sequence {dt
i,t= 1,...,n}is approximately AR(1).
3 Parameter Estimation
We introduce some notation first. Denote by Ipthep×pidentity matrix. For any s∈R,spdenotes
thep×1 vector with all its elements equal to s. For a= (a1,...,ap)⊤∈RpandA= (Ai,j)∈Rp×p, let
∥a∥q= (aq
i)1/qfor anyq≥1,∥a∥∞= maxi|ai|, and∥A∥∞= maxi/summationtextp
j=1|Ai,j|. Furthermore, let ∥A∥2
denote the spectral norm of Awhich equals its largest eigenvalue. For a random matrix W∈Rp×p
with E ( W) =0, define its matrix variance as Var( W) = max/braceleftbig
∥E/parenleftbig
WW⊤/parenrightbig
∥2,∥E/parenleftbig
W⊤W/parenrightbig
∥2/bracerightbig
. The
notationx≲ymeans that there exists a constant c1>0 such that|x|≤c1|y|, while notation x≳y
means there exists a constant c2>0 such that|x|≥c2|y|. Denote by B∞(x,r) ={y:∥y−x∥∞≤r}
the ball centred at xwithℓ∞radiusr. Letc,c0,c1,...,C,C 0,C1,...denote some generic constants that
may be different in different places. Let θ∗= (β∗⊤
0,β∗⊤
1)⊤= (β∗
1,0,···,β∗
p,0,β∗
1,1,···,β∗
p,1)⊤be the true
unknown parameters. We assume:
(A1) There exists a constant Ksuch that for any i= 1,2,···,p, the true parameters satisfy β∗
i,1−
max/parenleftbig
β∗
i,0,0/parenrightbig
<K.
Condition (A1) ensures that the autocorrelation functions (ACFs) in (2.4) are bounded away from 1 for
any (i,j)∈J. It is worth noting that both β∗
i,1andβ∗
i,0are allowed to vary with p, thus accommodating
sparse networks in our analysis. In practical terms, β∗
i,0, which reflects the sparsity of the stationary
network, tends to be very small for large networks. Consequently, condition (A1) holds when β∗
i,1is
bounded from above.
3.1 Maximum likelihood estimation
With the available observations X0,···,Xn, the log-likelihood function conditionally on X0is of the
formL(θ;Xn,···,X1|X0) =/producttextn
t=1L(θ;Xt|Xt−1). Note{Xt
i,j}for different ( i,j)∈J are independent
with each other. By (2.5), a (normalized) negative log-likelihood admits the following form:
l(θ) =−1
npL(θ;Xn,Xn−1,···,X1|X0) (3.6)
=−1
p/summationdisplay
1≤i<j≤plog/parenleftig
1 +eβi,0+βj,0+eβi,1+βj,1/parenrightig
+1
np/summationdisplay
1≤i<j≤p/braceleftigg
(βi,0+βj,0)n/summationdisplay
t=1Xt
i,j
+ log/parenleftbig
1 +eβi,1+βj,1/parenrightbign/summationdisplay
t=1/parenleftbig
1−Xt
i,j/parenrightbig/parenleftbig
1−Xt−1
i,j/parenrightbig
+ log/parenleftbig
1 +eβi,1+βj,1−βi,0−βj,0/parenrightbign/summationdisplay
t=1Xt
i,jXt−1
i,j/bracerightigg
.
For brevity, write
(3.7) ai,j=n/summationdisplay
t=1Xt
i,j, bi,j=n/summationdisplay
t=1Xt
i,jXt−1
i,j, di,j=n/summationdisplay
t=1/parenleftig
1−Xt
i,j/parenrightig/parenleftig
1−Xt−1
i,j/parenrightig
.
5Then the Hessian matrix of l(θ) is of the form
V(θ) =∂2l(θ)
∂θ∂θ⊤=
∂2l(θ)
∂β0∂β⊤
0∂2l(θ)
∂β0∂β⊤
1
∂2l(θ)
∂β1∂β⊤
0∂2l(θ)
∂β1∂β⊤
1
:=/bracketleftbiggV1(θ)V2(θ)
V2(θ)V3(θ)/bracketrightbigg
,
where fori̸=j,
∂2l(θ)
∂βi,0∂βj,0=1
peβi,0+βj,0(1 +eβi,1+βj,1)
(1 +eβi,0+βj,0+eβi,1+βj,1)2−1
npbi,jeβi,0+βj,0+βi,1+βj,1
(eβi,0+βj,0+eβi,1+βj,1)2,
∂2l(θ)
∂βi,0∂βj,1=−1
peβi,0+βj,0+βi,1+βj,1
(1 +eβi,0+βj,0+eβi,1+βj,1)2+1
npbi,jeβi,0+βj,0+βi,1+βj,1
(eβi,0+βj,0+eβi,1+βj,1)2,
∂2l(θ)
∂βi,1∂βj,1=1
peβi,1+βj,1(1 +eβi,0+βj,0)
(1 +eβi,0+βj,0+eβi,1+βj,1)2−1
npdi,jeβi,1+βj,1
(1 +eβi,1+βj,1)2
−1
npbi,jeβi,0+βj,0+βi,1+βj,1
(eβi,0+βj,0+eβi,1+βj,1)2.
Note that matrix V2(θ) is symmetric. Furthermore, the three matrices V1(θ),V2(θ) and V3(θ) are
diagonally balanced [12] in the sense that their diagonal elements are the sums of their respective rows,
namely,
(Vk(θ))i,i=p/summationdisplay
j=1,j̸=i(Vk(θ))i,j, k = 1,2,3.
Unfortunately the Hessian matrix V(θ) is not uniformly positive-definite. Hence l(θ) is not convex;
see Section 5.1 for an example. Therefore, finding the global MLE by minimizing l(θ) would be
infeasible, especially given the large dimensionality of θ. To overcome the obstacle, we propose the
following roadmap to search for the local MLE over a neighbourhood of the true parameter values θ∗.
(1) First we show that l(θ) is locally convex in a neighbourhood of θ∗(see Theorem 1 below). Towards
this end, we first prove that E( V(θ)) is positive definite in a neighborhood of θ∗. Leveraging on
some newly proved concentration results, we show that V(θ) converges to E( V(θ)) uniformly over
the neighborhood.
(2) Denote by /hatwideθthe local MLE in the neighbourhood identified above. We derive the bounds for
/hatwideθ−θ∗respectively in both ℓ2andℓ∞norms (see Theorems 2 and 3 below). The ℓ2convergence
is established by providing a uniform upper bound for the local deviation between l(θ)−E(l(θ))
andl(θ∗)−E(l(θ∗)) (see Corollary 4 in Section 4). The ℓ∞convergence of /hatwideθis established by
further exploiting the special structure of the objective function.
(3) We propose a new method of moments estimator (MME) which is proved to lie asymptotically in
the neighbourhood specified in (1) above. With this MME as the initial value, the local MLE /hatwideθ
can be simply obtained via a gradient decent algorithm.
The main technical challenges in the roadmap above can be summarized as follows.
Firstly, to establish the upper bounds as stated in (2) above, we need to evaluate the uniform local
deviations of the loss function. While the theoretical framework for deriving similar deviations of M-
estimators has been well established in, for example, [32, 31], classical techniques in empirical process for
establishing uniform laws [33] are not applicable because the number of parameters in TWHM diverges.
Secondly, for the classical β-model, proving the existence and convergence of its MLE relies strongly
on the interior point theorem [6]. In particular, this theorem is applicable only because the Hessian
matrix of the β-model admits a nice structure, i.e. it is diagonally dominant and all its elements are
positive depending on the parameters only [2, 39, 37, 8]. However the Hessian matrix of l(θ) for TWHM
6depends on random variables Xt
i,j’s in addition to the parameters, making it impossible to verify if the
score function is uniformly Fr´ echet differentiable or not, a key assumption required by the interior point
theorem.
Lastly, the higher order derivatives of l(θ) may diverge as the order increases. To see this, notice that
for any integer k, thek-th order derivatives of l(θ) is closely related to the ( k−1)-th order derivatives
of the Sigmoid function S(x) =1
1+e−xin that∂kS(x)
∂xk=/summationtextk−2
m=0−A(k−1,m)(−ex)m+1
(1+ex)k , whereA(k−1,m) is
the Eulerian number [26]. Some of the coefficients A(k−1,m) can diverge very quickly as kincreases.
Thus, loosely speaking, l(θ) is not smooth. This non-smoothness and the need to deal with a growing
number of parameters make various local approximations based on Taylor expansions highly non-trivial;
noting that the consistency of MLEs in many finite-dimensional models is often established via these
approximations.
In our proofs, we have made great use of the special sparse structure of the loss function in the form
(4.11) below. This sparsity structure stems from the fact that most of its higher order derivatives are
zero. Based on the uniform local deviation bound obtained in Section 4, we have established an upper
bound for the error of the local MLE under the l2norm. Utilizing the structure of the marginalized
functions of the loss we have further established an upper bound for the estimation error under the l∞
norm thanks to an iterative procedure stated in Section 3.3.
3.2 Existence of the local MLE
To establish the convexity of l(θ) in a neighborhood of θ∗, we first show that such a local convexity
holds for E( V(θ)).
Proposition 2. LetAbe a 2p×2pmatrix defined as A=/bracketleftbigg
A1A2
A2A3/bracketrightbigg
,where A1,A2,A3arep×p
symmetric matrices. Then Ais positive (negative) definite if −A2,A2+A3,A2+A1are all positive
(negative) definite.
Proof. Consider any nonzero x= (x⊤
1,x⊤
2)⊤∈R2pwhere x1,x2∈Rp, we have:
xTAx =x⊤
1A1x1+x⊤
2A3x2+ 2x⊤
1A2x2
=x⊤
1(A1+A2)x1+x⊤
2(A3+A2)x2−(x1−x2)⊤A2(x1−x2).
This proves the proposition.
Noting that−V2(θ),V2(θ) +V3(θ) and V2(θ) +V1(θ) are all diagonally balanced matrices, with
some routine calculations it can be shown that −EV2(θ∗),E(V2(θ∗)+V3(θ∗)) and E( V2(θ∗)+V1(θ∗))
have only positive elements, and thus are all positive definite. Therefore, E V(θ∗) is positive definite
by Proposition 2. By continuity, when θis close enough to θ∗, EV(θ) is also positive definite, and
hence El(θ) is strongly convex in a neighborhood of θ∗. Next we want to show the local convexity
ofl(θ) whose second order derivatives depend on the sufficient statistics bi,j=/summationtextn
t=1Xt
i,jXt−1
i,j, and
di,j=/summationtextn
t=1/parenleftig
1−Xt
i,j/parenrightig/parenleftig
1−Xt−1
i,j/parenrightig
. By noticing that the network process is α-mixing with an exponential
decaying mixing coefficient, we first obtain the following concentration results for bi,janddi,j, which
ensure element-wise convergence of V(θ) to E V(θ) for a given θwhennp→∞ .
Lemma 1. Suppose{Xt}∼Pθfor some θ= (β1,0,···,βp,0,β1,1,···,βp,1)⊤satisfying condition (A1).
Then for any (i,j)∈J,{Xt
i,j,t≥1}isα-mixing with exponential decaying rates. Moreover, for any
positive constant c >0, by choosing c1>0to be large enough, it holds with probability greater than
1−(np)−cthat
max
1≤i<j≤p/braceleftigg
n−1/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglen/summationdisplay
t=1/braceleftbig
Xt
i,j−E/parenleftbig
Xt
i,j/parenrightbig/bracerightbig/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle,n−1|bi,j−E(bi,j)|,n−1|di,j−E(di,j)|/bracerightigg
≤c1rn,p,
7wherern,p=/radicalbig
n−1log(np) +n−1log (n) log log (n) log (np).
The following lemma provides a lower bound for the smallest eigenvalue of E( V(θ)).
Lemma 2. Let{Xt}∼Pθ∗,B∞(θ∗,r) :={θ:∥θ−θ∗∥∞≤r}andB(κ0,κ1) :=/braceleftig
(β0,β1) :∥β0∥∞≤
κ0,∥β1∥∞≤κ1/bracerightig
. Under condition (A1), for any κ0,κ1andr=cre−4κ0−4κ1withcr>0being a small
enough constant, there exists a constant C > 0such that
inf
θ∈B∞(θ∗,r)∩B(κ0,κ1);∥a∥2=1a⊤E (V(θ))a≥Ce−4κ0−4κ1.
Examining the proof indicates that the lower bound in Lemma 2 is attained when β0= (κ0,...,κ 0)⊤
andβ1= (−κ1,...,−κ1)⊤. Hence the smallest eigenvalue of E ( V(θ)) can decay exponentially in κ0
andκ1. Consequently, an upper bound for the radius κ0andκ1must be imposed so as to ensure the
positive definiteness of the sample analog V(θ). Moreover, Lemma 2 also indicates that the positive
definiteness of E ( V(θ)) can be guaranteed when θis within the ℓ∞ballB∞(θ∗,r). To establish the
existence of the local MLE in the neighborhood, we need to evaluate the closeness of E ( V(θ)) and V(θ)
in terms of the operator norm. Intuitively, for some appropriately chosen κ0,κ1, if∥E (V(θ))−V(θ)∥2
has a smaller order than e−4κ0−4κ1uniformly over the parameter space {θ:∥β0∥∞≤κ0,∥β1∥∞≤κ1
andθ∈B∞(θ∗,r)}, the positive definiteness of V(θ) can be concluded.
Note that V2(θ)−EV2(θ),V2(θ)+V3(θ)−E (V2(θ) +V3(θ)) and V2(θ)+V1(θ)−E (V2(θ) +V1(θ))
are all centered and diagonally balanced matrices which can be decomposed into sums of independent
random matrices. The following lemma provides a bound for evaluating the moderate deviations of these
centered matrices.
Lemma 3. LetZ= (Zi,j)1≤i,j≤pbe a symmetric p×prandom matrix such that the off-diagonal elements
Zi,j,1≤i<j≤pare independent of each other and satisfy
Zi,i=n/summationdisplay
j=1,j̸=iZi,j,E (Zi,j) = 0,Var (Zi,j)≤σ2,andZi,j≤balmost surely .
Then it holds that
P(∥Z∥2>ϵ)≤2pexp/parenleftbigg
−ϵ2
2σ2(p−1) + 4bϵ/parenrightbigg
.
Proposition 2, Lemma 2 and Lemma 3 imply the theorem below.
Theorem 1. Let condition (A1) hold, assume {Xt}∼Pθ∗, andκr:=∥β∗
r∥∞wherer= 0,1withκ0+
κ1≤clog(np)for some small enough constant c>0. Then asnp→∞ withn≥2, we have that, with
probability tending to one, there exists a unique MLE in the ℓ∞ballB∞(θ∗,r) ={θ:∥θ−θ∗∥∞≤r}
for somer=cre−4κ0−4κ1, wherecr>0is a constant.
In the proof of Theorem 1, we have shown that with probability tending to 1, l(θ) is convex in the
convex and closed set B∞(θ∗,r). Consequently, we conclude that there exists a unique local MLE in
B∞(θ∗,r). From Theorem 1 we can also see that when κ0+κ1becomes larger, the radius rwill be
smaller, and when κ0+κ1is bounded away from infinity, rhas a constant order. From the proof we
can also see that the constant crcan be larger if the smallest eigenvalue of the expected Hessian matrix
E(V(θ)) is larger. Further, by allowing the upper bound of ∥β∗
0∥∞to grow to infinity, our theoretical
analysis covers the case where networks are sparse. Specifically, under the condition that ∥β∗
0∥∞≤κ0,
from (2) we can obtain the following lower bound (which is achievable when β∗
1,0=...=β∗
p,0=−κ0)
for the density of the stationary network:
ρ:=2
p(p−1)/summationdisplay
1≤i<j≤pP/parenleftbig
Xt
i,j= 1/parenrightbig
≥e−2κ0
1 +e−2κ0=O/parenleftbig
e−2κ0/parenrightbig
.
8In particular, when κ0≤clog(np) for some constant c>0, we have ρ≥1
[1+(np)2c]. Thus, compared to
full dense network processes where the total number of edges for each network is of the order p2, TWHM
allows the networks with much fewer edges.
3.3 Consistency of the local MLE
In the previous subsection, we have proved that with probability tending to one, l(θ) is convex in
B∞(θ∗,r), wherer=cre−4κ0−4κ1is defined in Theorem 1. Denote by /hatwideθthe (local) MLE in B∞(θ∗,r).
We now evaluate the ℓ2andℓ∞distances between /hatwideθand the true value θ∗.
Based on Theorem 5 we obtain a local deviation bound for l(θ) as in Corollary 4 in Section 4, from
which we establish the following upper bound for the estimation error of /hatwideθunder theℓ2norm:
Theorem 2. Let condition (A1) hold, assume {Xt}∼Pθ∗, andκr:=∥β∗
r∥∞wherer= 0,1with
κ0+κ1≤clog(np)for some small enough constant c>0. Then asnp→∞ withn≥2, it holds with
probability converging to one that
1√p/vextenddouble/vextenddouble/vextenddouble/hatwideθ−θ∗/vextenddouble/vextenddouble/vextenddouble
2≲e4κ0+4κ1/radicaligg
log(np)
np/parenleftbigg
1 +log(np)√p/parenrightbigg
.
We discuss the implication of this theorem. When n→∞ andpis finite, that is, when we have a fixed
number of nodes but a growing number of network snapshots, Theorem 2 indicates that/vextenddouble/vextenddouble/vextenddouble/hatwideθ−θ∗/vextenddouble/vextenddouble/vextenddouble
2=
Op/parenleftbigg/radicalig
log3n
ne4κ0+4κ1/parenrightbigg
=op(1) whencis small enough. On the other hand, when n,κ0andκ1are finite,
Theorem 2 indicates that as the number of parameters pincreases, the ℓ2error bound of /hatwideθincreases at
a much slower rate O/parenleftbig√logp/parenrightbig
.
Although Theorem 2 indicates that1√p/vextenddouble/vextenddouble/vextenddouble/hatwideθ−θ∗/vextenddouble/vextenddouble/vextenddouble
2=op(1) asnp→∞ , it does not guarantee the
uniform convergence of all the elements in /hatwideθ. To prove the uniform convergence in the ℓ∞norm, we
exploit a special structure of the loss function and the ℓ2norm bound obtained in Theorem 2. Specifically,
denotel(θ) in (3.6) as l(θ) =l(θ(i),θ(−i)) where θ(i):= (βi,0,βi,1)⊤, and θ(−i)contains the remaining
elements of θexcept θ(i). Using this notation, we can analogously define θ∗
(i)andθ∗
(−i)for the true
parameter θ∗, and/hatwideθ(i)and/hatwideθ(−i)for the local MLE /hatwideθ. We then have that θ∗
(i)is the mimizer of
El/parenleftig
·,θ∗
(−i)/parenrightig
while/hatwideθ(i)is the minimizer of l/parenleftig
·,/hatwideθ(−i)/parenrightig
. The error of /hatwideθ(i)in estimating θ∗
(i)then relies
on the distance between E l/parenleftig
·,θ∗
(−i)/parenrightig
andl/parenleftig
·,/hatwideθ(−i)/parenrightig
, which on the other hand depends on both the
ℓ2bound of∥/hatwideθ−θ∗∥2and the uniform local deviation bound of l/parenleftbig
θ(i),θ(−i)/parenrightbig
. Based on Theorem 2,
Corollary 4 in Section 4, and a sequential approach (see equations (A.28) and (A.29) in the appendix),
we obtain the following bound for the estimation error under the l∞norm.
Theorem 3. Let condition (A1) hold, assume {Xt}∼Pθ∗, andκr:=∥β∗
r∥∞wherer= 0,1with
κ0+κ1≤clog(np)for some small enough constant c > 0. Then as np→∞,n≥2, it holds with
probability converging to one that
/vextenddouble/vextenddouble/vextenddouble/hatwideθ−θ∗/vextenddouble/vextenddouble/vextenddouble
∞≲e8κ0+8κ1log log(np)/radicaligg
log(np)
np/parenleftbigg
1 +log(np)√p/parenrightbigg
.
Theorem 3 indicates that/vextenddouble/vextenddouble/vextenddouble/hatwideθ−θ∗/vextenddouble/vextenddouble/vextenddouble
∞=op(1) asnp→∞ . Thus all the components of /hatwideθconverge
uniformly. On the other hand, when κ=clog(np) for some small enough positive constant c, we have
9e8κ0+8κ1log log(np)/radicalig
log(np)
np/parenleftig
1 +log(np)√p/parenrightig
≤o(cre−4κ0−4κ1). Compared with Theorem 1, we observe
that although the radius rin Theorem 1 already tends to zero when ∥β∗
0∥∞≤κ0,∥β∗
1∥∞≤κ1and
κ0+κ1≤clog(np) for some small enough constant c>0, theℓ∞error bound of /hatwideθhas a smaller order
asymptotically and thus gives a tighter convergence rate.
We remark that in the MLE, β∗
0andβ∗
1are estimated jointly. As we can see from the log-likelihood
function, the information related to βi,0is captured by Xt
i,jandXt
i,jXt−1
i,j,t= 1,...,n,j̸=i, while that
related toβi,1is captured by (1 −Xt
i,j)(1−Xt−1
i,j) andXt
i,jXt−1
i,j,t= 1,...,n,j̸=i. This indicates that
the effective “sample sizes” for estimating βi,0andβi,1are both of the order O(np). While the theorems
we have established in this section is for /hatwideθ= (/hatwideβ⊤
0,/hatwideβ⊤
1)⊤jointly, we would expect /hatwideβ0and/hatwideβ1to have the
same rate of convergence.
3.4 A method of moments estimator
Having established the existence of a unique local MLE in B∞(θ∗,r) and proved its convergence, we
still need to specify how to find this local MLE. To this end, we propose an initial estimator lying in
this neighborhood. Consequently we can adopt any convex optimization method such as the coordinate
descent algorithm to locate the local MLE, thanks to the convexity of the loss function in this neighbor-
hood. Based on (2.3), an initial estimator of β0denoted as ˜β(0)can be found by solving the following
method of moments equations
(3.8)/summationtextn
t=1/summationtextp
j=1,j̸=iXt
i,j
n−p/summationdisplay
j=1,j̸=ieβi,0+βj,0
1 +eβi,0+βj,0= 0, i= 1,···,p.
These equations can be viewed as the score functions of the pseudo loss function f(β0) :=/summationtext
1≤i,j≤plog{1+
eβi,0+βj,0}−n−1/summationtextp
i=1{βi,0/summationtextn
t=1/summationtextp
j=1,j̸=iXt
i,j}. Since the Hessian matrix of f(β0) is diagonally bal-
anced with positive elements, the Hessian matrix is positive definite, and, hence, f(β0) is strongly
convex. With the strong convexity, the solution of (3.8) is the minimizer of f(·) which can be easily
obtained using any standard algorithms such as the gradient descent. On the other hand, note that
E(Xt
i,jXt−1
i,j) =eβi,0+βj,0
1 +eβi,0+βj,0/parenleftbigg
1−1
1 +eβi,0+βj,0+eβi,1+βj,1/parenrightbigg
,
which motivates the use of the following estimating equations to obtain ˜β1, the initial estimator of β1,
(3.9)n/summationdisplay
t=1p/summationdisplay
j=1,j̸=i/braceleftigg
Xt
i,jXt−1
i,j−e˜βi,0+˜βj,0
1 +e˜βi,0+˜βj,0/parenleftbigg
1−1
1 +e˜βi,0+˜βj,0+eβi,1+βj,1/parenrightbigg/bracerightigg
= 0,
withi= 1,···,p. Similar to (3.8), we can formulate a pseudo loss function such that given ˜β0, its
Hessian matrix corresponding to the score equations (3.9) is positive definite, and hence (3.9) can also be
solved via the standard gradient descent algorithm. Since ˜θ= (˜β⊤
0,˜β⊤
1)⊤is obtained by solving two sets
of moment equations, we call it the method of moments estimator (MME). An interesting aspect of our
construction of these moment equations is that the equations corresponding to the estimation of β0and
β1are decoupled. While the estimator error in estimating β0propagates clearly in that of estimating
β1, we have the following existence, uniqueness, and a uniform upper bound for the estimation error of
˜θ. Our results build on a novel application of the classical interior mapping theorem [6, 38, 37].
Theorem 4. Let condition (A1) hold, and {Xt}∼Pθ∗. The MME ˜θdefined by equations (3.8) and
(3.9) exists and is unique in probability. Further, assume that κr:=∥β∗
r∥∞wherer= 0,1withκ0+κ1≤
clog(np)for some small enough constant c>0. Then asnp→∞ andn≥2, it holds that
/vextenddouble/vextenddouble˜θ−θ∗/vextenddouble/vextenddouble
∞≤Op/parenleftigg
e14κ0+6κ1/radicaligg
log(n) log(p)
np/parenrightigg
.
10Whennp→∞ andκ0,κ1are finite, Theorem 4 gives/vextenddouble/vextenddouble˜θ−θ∗/vextenddouble/vextenddouble
∞=Op/parenleftbigg/radicalig
log(n) log(p)
np/parenrightbigg
. When
κ0+κ1≍log(np), we see that the upper bound for the local MLE in Theorem 3 is dominated by the
upper bound of the MME in Theorem 4. Moreover, when κ0+κ1≤clog(np) for some small enough
constantc >0, we have ˜θ∈B∞(θ∗,r), whereris defined in Theorem 1. Thus, ˜θis in the small
neighborhood of θ∗as required.
3.5 The sparse case
In the previous results, the estimation error bounds depend on κ0andκ1, i.e., the upper bounds on
∥β∗
0∥∞and∥β∗
1∥∞. Clearly, the larger κ0is, the more sparse the networks could be, and the larger κ1
is, the lag-one correlations (c.f. equation (2.4)) could be closer to one, indicating fewer fluctuations in
the network process. To further characterize the effect of network sparsity, in this section, we derive
further properties under a relatively sparse scenario where −κ0≤β∗
i,0≤Cκand−κ1≤β∗
i,1≤κ1for all
i= 1,...,p andCκ>0 here is a constant. Under this case, there exist constants C > 0 andC1>0 such
thatCe−2κ0≤E/parenleftbig
Xt
i,j/parenrightbig
≤C1<1.In the most sparse case where β0,i=−κ0,i= 1,...,p , the density
of the stationary network is of the order O(e−2κ0). Similar to Lemma 2 and Theorem 1, the following
corollary provides a lower bound for the smallest eigenvalue of E( V(θ)) and the existence of the MLE.
Corollary 1. Let{Xt}∼Pθ∗,B∞(θ∗,r) ={θ:∥θ−θ∗∥∞≤r}for somer=cre−2κ0−4κ1where
cr>0is a small enough constant. Denote B′(κ0,κ1) :=/braceleftig
(β0,β1) :−κ0≤βi,0≤Cκ,i= 1,...,p,
∥β1∥∞≤κ1/bracerightig
for some constant Cκ>0. Then, under condition (A1), there exists a constant C > 0
such that
inf
θ∈B∞(θ∗,r)∩B′(κ0,κ1);∥a∥2=1a⊤E (V(θ))a≥Ce−2κ0−4κ1.
Further, assume that θ∗∈B′(κ0,κ1)andκ0+ 2κ1< clog(np)for some positive constant c < 1/6.
Then, asnp→∞ withn≥2, with probability tending to 1, there exists a unique MLE in B∞(θ∗,r).
Following Theorems 2-4, we also establish the estimation errors for the MLE and MME in the
subsequent corollaries below.
Corollary 2. Let condition (A1) hold. Assume {Xt}∼Pθ∗,∥β∗
1∥∞≤κ1, and−κ0≤β∗
i,0≤Cκfor
i= 1,...,p , and some constant Cκ>0. Then asnp→∞ withn≥2, it holds with probability tending
to one that
1√p/vextenddouble/vextenddouble/vextenddouble/hatwideθ−θ∗/vextenddouble/vextenddouble/vextenddouble
2≤Ce2κ0+4κ1/radicaligg
log(np)
np/parenleftbigg
1 +log(np)√p/parenrightbigg
,
and/vextenddouble/vextenddouble/vextenddouble/hatwideθ−θ∗/vextenddouble/vextenddouble/vextenddouble
∞≤Ce4κ0+8κ1log log(np)/radicaligg
log(np)
np/parenleftbigg
1 +log(np)√p/parenrightbigg
.
Corollary 3. Let condition (A1) hold. Assume {Xt}∼Pθ∗,∥β∗
1∥∞≤κ1, and−κ0≤β∗
i,0≤Cκfor
i= 1,...,p and some constant Cκ>0. Then asnp→∞ withn≥2, it holds with probability tending to
one that the MME ˜θdefined in equations (3.8) and(3.9) exists uniquely, and when κ0+ 2κ1<clog(np)
for some constant c<1/12, it holds that
/vextenddouble/vextenddouble˜θ−θ∗/vextenddouble/vextenddouble
∞≤Op/parenleftigg
e4κ0+6κ1/radicaligg
log(n) log(p)
np/parenrightigg
.
11From Corollary 2, we can see that when κ1≍O(1), the MLE is consistent when κ0≤clog(np) for
some positive constant c<1/8, with the corresponding lower bound in the density as O(e−2clog(np))≻
O((np)−1/4). Similarly, from Corollary 3 we can see that when κ1≍O(1), the density of the networks
can be as small as O(e−2clog(np)) for some constant c <1/12, i.e., the density has a larger order than
(np)−1/6for the estimation of the MME. Further, when 6 κ0+ 10κ1≤c1log(np) for some constant
c1<1/2, we have ˜θ∈B∞(θ∗,r), whereris defined in Corollary 1. This implies the validity of using ˜θ
as an initial estimator for computing the local MLE.
4 A uniform local deviation bound under high dimensionality
As we have discussed, a key to establish the consistency of the local MLE is to evaluate the magnitude
of/vextendsingle/vextendsingle[l(θ)−El(θ)]−[l(θ∗)−El(θ∗)]/vextendsingle/vextendsinglefor all θ∈B∞(θ∗,r) withrspecified in Theorem 1. Such local
deviation bounds are important for establishing error bounds for general M-estimators in the empirical
processes [32]. Note that
l(θ)−El(θ) =−1
p/summationdisplay
1≤i<j≤p/braceleftigg
(βi,0+βj,0)/parenleftigai,j−E(ai,j)
n/parenrightig
(4.10)
+ log/parenleftig
1 +e(βi,1+βj,1)/parenrightig/parenleftigdi,j−E(di,j)
n/parenrightig
+ log/parenleftig
1 +e(βi,1−βi,0)+(βj,1−βj,0)/parenrightig/parenleftigbi,j−E(bi,j)
n/parenrightig/bracerightigg
whereai,j,bi,janddi,jare defined in (3.7). The three terms on the right-hand side all admit the following
form
(4.11) L(θ) =1
p/summationdisplay
1≤i̸=j≤pli,j(θi,θj)Yi,j,
for some functions L:Rp→R,li,j:R2→R, and centered random variables Yi,j(1≤i,j≤p).
Instead of establishing the uniform bound for each term in (4.10) separately, below we will establish a
unified result for bounding |L(θ)−L/parenleftbig
θ′/parenrightbig
|over a local ℓ∞ball defined as θ∈B∞(θ′,·) for a general
Lfunction as in (4.11). We remark that in general without further assumptions on L, establishing
uniform deviation bounds is impossible when the dimension of the problem diverges. For our TWHM
however, the decomposition (4.10) is of a particularly appealing structure in the sense that only two-
way interactions between parameters θiexist. Based on this “sparsity” structure, we develop a novel
reformulation (c.f. equation (A.41)) for the main components of the Taylor series of L(θ) satisfying the
following two conditions.
(L-A1) There exists a constant α > 0, such that for any 1 ≤i̸=j≤p, any positive integer k, and any
non-negative integer s≤k, we have:
∂kli,j(θi,θj)
∂θs
i∂θk−s
j≤(k−1)!
αk.
(L-A2) Random variables Yi,j,1≤i̸=j≤pare independent satisfying E ( Yi,j) = 0,|Yi,j|≤b(p)and
Var (Yi,j)≤σ2
(p)for anyiandj, whereb(p)andσ2
(p)are constants depending on nandpbut
independent of iandj.
Loosely speaking, Condition (L-A1) can be seen as a smoothness assumption on the higher order
derivatives of li,j(θi,θj) so that we can properly bound these derivatives when Taylor expansion is
12applied. On the other hand, the upper bound for these derivatives is mild as it can diverge very
quickly askincreases. For our TWHM, it can be verified that (L-A1) holds for li,j(θi,θj) =θi+θj
andli,j(θi,θj) = log(1 + eθi+θj); see (3.6). For the latter, note that the first derivative of function
l(x) = log(1 + ex) is seen as the Sigmoid function:
S(x) =ex
1 +ex=1
1 +e−x.
By the expression of the higher order derivatives of the Sigmoid function [26], the k-th order derivative
oflis
∂kl(x)
∂xk=/summationtextk−2
m=0−A(k−1,m) (−ex)m+1
(1 +ex)k,
wherek≥2 andA(k−1,m) is the Eulerian number. Now for any x, we have
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationtextk−2
m=0−A(k−1,m) (−ex)m+1
(1 +ex)k/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤k−2/summationdisplay
m=0A(k−1,m) = (k−1)!.
Therefore,/vextendsingle/vextendsingle/vextendsingle/vextendsingle∂kl(x)
∂xk/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤(k−1)!
holds for all x∈Randk≥2. With extra arguments using the chain rule, this in return implies that
(L-A1) is satisfied with α= 1 whenli,j(θ) = log/parenleftbig
1 +eθi+θj/parenrightbig
.
Condition (L-A2) is a regularization assumption for the random variables Yi,j,1≤i,j≤p, and
the bounds on their moments are imposed to ensure point-wise concentration. For our TWHM, from
Lemma 1 and Lemma 5, we have that there exist large enough constants C > 0 andc >0 such that
with probability greater than 1 −(np)−c, the random variablesai,j−E(ai,j)
n,bi,j−E(bi,j)
nanddi,j−E(di,j)
nall
satisfy condition (L-A2) with b(p)=C/radicalbig
n−1log(np) +Cn−1log (n) log log (n) log (np) andσ2
(p)=Cn−1.
We present the uniform upper bound on the deviation of L(θ) below.
Theorem 5. Assume conditions (L-A1) and (L-A2). For any given θ′∈Rpandα0∈(0,α/2), there
exist large enough constants C > 0andc>0which are independent of θ′, such that, as np→∞ , with
probability greater than 1−(np)−c,
/vextendsingle/vextendsingleL(θ)−L/parenleftbig
θ′/parenrightbig/vextendsingle/vextendsingle≤Cb(p)log(np) +σ(p)/radicalbig
plog(np)
p/vextenddouble/vextenddoubleθ−θ′/vextenddouble/vextenddouble
1
holds uniformly for all θ∈B∞/parenleftbig
θ′,α0/parenrightbig
.
One of the main difficulties in analyzing L(θ) defined in (4.11) is that li,j(θi,θj) andYi,jare coupled,
giving rise to complex terms involving both in the Taylor expansion of L(θ). When Taylor expansion
with order Kis used, condition (L-A1) can reduce the number of higher order terms from O(pK) to
O(p22K). On the other hand, by formulating the main terms in the Taylor series into a matrix form in
(A.41), the uniform convergence of the sum of these terms is equivalent to that of the spectral norm of
a centered random matrix, which is independent of the parameters. Further details can be found in the
proofs of Theorem 5.
Define the marginal functions of L(θ) as
Li(θ) =1
pp/summationdisplay
j=1,j̸=ili,j(θi,θj)Yi,j, i= 1,...,p,
by retaining only those terms related to θi. Similar to Theorem 5, we state the following upper bound
for these marginal functions. With some abuse of notation, let θ−i:= (θ1,···,θi−1,θi+1,···,θp)⊤be
the vector containing all the elements in θexceptθi.
13Theorem 6. If conditions (L-A1) and (L-A2) hold, then for any given θ′∈Rpandα0∈(0,α/2), there
exist large enough constants C > 0andc>0which are independent of θ′, such that, as np→∞ , with
probability greater than 1−(np)−c,
/vextendsingle/vextendsingleLi(θ)−Li/parenleftbig
θ′/parenrightbig/vextendsingle/vextendsingle
≤Cb(p)
p/vextenddouble/vextenddoubleθ−i−θ′
−i/vextenddouble/vextenddouble
1+C/parenleftbig/vextenddouble/vextenddoubleθ−i−θ′
−i/vextenddouble/vextenddouble
1+ 1/parenrightbig
|θi−θ′
i|b(p)log(np) +σ(p)/radicalbig
plog(np)
p
holds uniformly for all θ∈B∞/parenleftbig
θ′,α0/parenrightbig
, andi= 1,···,p.
Similar to (4.10), we can also decompose l/parenleftbig
θ(i),θ(−i)/parenrightbig
−El/parenleftbig
θ(i),θ(−i)/parenrightbig
into the sum of three compo-
nents taking the form (4.11). Consequently, by setting θ′in Theorems 5 and 6 to be the true parameter
θ∗, we can obtain the following upper bounds.
Corollary 4. For any given 0<α0<1/4, there exist large enough positive constants c1,c2,andCsuch
that
(i) with probability greater than 1−(np)−c1,
(4.12)/vextendsingle/vextendsingle/parenleftbig
l(θ)−l(θ∗)/parenrightbig
−/parenleftbig
El(θ)−El(θ∗)/parenrightbig/vextendsingle/vextendsingle≤C1/parenleftbigg
1 +log(np)√p/parenrightbigg/radicalbigg
log(np)
n∥θ−θ∗∥2
holds uniformly for all θ∈B∞(θ∗,α0)with some constant α0<1/2;
(ii) with probability greater than 1−(np)−c2,
(4.13)/vextendsingle/vextendsingle/vextendsinglel/parenleftig
θ(i),θ∗
(−i)/parenrightig
−l/parenleftig
θ∗
(i),θ∗
(−i)/parenrightig
−/bracketleftig
El/parenleftig
θ(i),θ∗
(−i)/parenrightig
−El/parenleftig
θ∗
(i),θ∗
(−i)/parenrightig/bracketrightig/vextendsingle/vextendsingle/vextendsingle
≤C2/parenleftbigg
1 +log(np)√p/parenrightbigg/radicalbigg
log(np)
n/vextenddouble/vextenddouble/vextenddoubleθ(i)−θ∗
(i)/vextenddouble/vextenddouble/vextenddouble
2
holds uniformly for all θ(i)∈B∞/parenleftig
θ∗
(i),α0/parenrightig
with some constant α0<1/2.
In (4.12) and (4.13) we have replaced the ℓ1norm based upper bounds in Theorems 5 and 6 with ℓ2
norm based upper bounds using the fact that for all x∈Rp,∥x∥1≤√p∥x∥2. It is recognized that net-
works often exhibit diverse characteristics, including dynamic changes, node heterogeneity, homophily,
and transitivity, among others. In this paper, our primary emphasis is on addressing node heterogene-
ity within dynamic networks. When integrated with other stylized features, the objective function may
adopt a similar structure to the L(θ) function defined in Equation (4.11). Moreover, many other models
that incorporate node heterogeneity can express their log-likelihood functions in a form analogous to
Equation (4.11). For instance, the general category of network models with edge formation probabili-
ties represented as f(αi,βj), wheref(·) is a density or probability mass function, and ( αi,βi) denote
node-specific parameters for node i. This encompasses models such as the p1model [13], the directed
β-model [36], and the bivariate gamma model [8]. Additionally, in the domain of ranking data analy-
sis, it is common to introduce individual-specific parameters or scores for ranking, as observed in the
classical Bradley-Terry model and its variants [9]. Our discoveries have the potential for application in
the theoretical examination of these models or their modifications when considering additional stylized
features alongside node heterogeneity.
5 Numerical study
In this section, we assess the performance of the local MLE. For comparison, we have also computed a
regularized MME that is numerically more stable than the vanilla MME in (3.9). Specifically, for the
14Table 1: Signs of the smallest eigenvalues of the Hessian matrices of l(θ) and El(θ) evaluated at θ=θ∗
or02pwhen different values of θ∗= (β∗⊤
0,β∗⊤
1)⊤are used to generate data.
Sign of the smallest eigenvalue of l(θ∗) Sign of the smallest eigenvalue of E l(θ∗)
β∗
0=0.2pβ∗
0=0.5pβ∗
0=1p β∗
0=0.2pβ∗
0=0.5pβ∗
0=1p
β∗
1=0.2p + + + β∗
1=0.2p + + +
β∗
1=0.5p + + + β∗
1=0.5p + + +
β∗
1=1p + + + β∗
1=1p + + +
Sign of the smallest eigenvalue of l(02p) Sign of the smallest eigenvalue of E l(02p)
β∗
0=0.2pβ∗
0=0.5pβ∗
0=1p β∗
0=0.2pβ∗
0=0.5pβ∗
0=1p
β∗
1=0.2p + + − β∗
1=0.2p + + −
β∗
1=0.5p− − − β∗
1=0.5p + + −
β∗
1=1p− − − β∗
1=1p− − −
former, we solve
(5.14)−1
npn/summationdisplay
t=1p/summationdisplay
j=1,j̸=i/braceleftigg
Xt
i,jXt−1
i,j−e˜βi,0+˜βj,0
1 +e˜βi,0+˜βj,0/parenleftbigg
1−1
1 +e˜βi,0+˜βj,0+eβi,1+βj,1/parenrightbigg/bracerightigg
+λβi,1= 0,
withi= 1,···,p, whereλβi,1can be seen as a ridge penalty with λ>0 as the regularization parameter.
Denote the regularized MME as ˜θλ. Similar to Theorem 4, by choosing λ=Cλe2κ/radicalig
log(np)
npfor some
constantCλ, we can show that/vextenddouble/vextenddouble˜θλ−θ∗/vextenddouble/vextenddouble
∞≤Op/parenleftbigg
e26κ/radicalig
log(n) log(p)
np/parenrightbigg
.In our implementation we take
λ=/radicalig
log(np)
np. The MLE of TWHM is obtained via gradient descent using ˜θλas the initial value.
5.1 Non-convexity of l(θ)and El(θ)
Given the form of l(θ), it is intuitively true that it may not be convex everywhere. We confirm this
via a simple example. Take ( n,p) = (2,1000) and set β∗
0,β∗
1to be 0.2p,0.5por1p. We evaluate the
smallest eigenvalue of the Hessian matrix of l(θ) and its expectation E l(θ) at the true parameter value
θ∗= (β∗⊤
0,β∗⊤
1)⊤, or at θ=02pin one experiment. From the top half of Table 1 we can see that, when
evaluated at θ∗, the Hessian matrices are all positive definite. However, when evaluated at θ=02p,
from the bottom half of the table we can see that the Hessian matrices are no longer positive definite
when θ∗is far away from 02p. Even when the Hessian matrix of E l(θ) is so at θ=02pwith θ∗=0.52p,
the corresponding Hessian matrix of l(θ) at this point has a negative eigenvalue. Thus, E l(θ) andl(θ)
are not globally convex.
5.2 Parameter estimation
We first evaluate the error rates of the MLE and MME under different combinations of nandp. We set
n= 2,5,10,or 20 andp∈⌊200×1.20:6⌋={200,240,288,346,415,498,598}, which results in a total of
28 different combinations of ( n,p). For each ( n,p), the data are generated such that {Xt}∼Pθ∗where
the parameters β∗
i,0andβ∗
i,1(1≤i≤p) are drawn independently from the uniform distribution with
parameters in (−1,1). Each experiment is repeated 100 times under each setting. Denote the estimator
(which is either the MLE or the MME) as /hatwideθ, and the true parameter value as θ∗. We report the average
ℓ2error in terms of∥/hatwideθ−θ∗∥2√pand the average ℓ∞error∥/hatwideθ−θ∗∥∞in Figure 2. From this figure, we can
see that the errors in terms of the ℓ∞norm and the ℓ2norm decrease for MME and MLE as norp
increases, while the errors of MLE are smaller across all settings. These observations are consistent with
our findings in the main theory.
15n=2 n=5 n=10 n=20l2 l∞
2002402883464154985982002402883464154985982002402883464154985982002402883464154985980.10.20.3
0.51.01.5
pErrorestimator
MLE
MMEFigure 2: Mean errors of MME and MLE in terms of the ℓ2andℓ∞norm.
Next, we provide more numerical simulation to evaluate the performance of MLE and MME by
imposing different structures on β∗
0andβ∗
1. In particular, we want to evaluate how the estimation
accuracy changes by varying the sparsity of the networks as well as varying the correlations of the
network sequence. Note that the expected density of the stationary distribution of the network process
is simply
1
p(p−1)E
/summationdisplay
1≤i̸=j≤pXt
i,j
=1
p(p−1)
/summationdisplay
1≤i̸=j≤peβ∗
i,0+β∗
j,0
1 +eβ∗
i,0+β∗
j,0
.
In the sequel, we will use two parameters aandbto generate β∗
r,r= 0,1, according to the following
four settings:
Setting 1.{a}: all the elements in β∗
rare set to be equal to a.
Setting 2.{a,b}: the first 10% elements of β∗
rare set to be equal to a, while the other elements are set to be
equal tob.
Setting 3.L(a,b): the parameters take values in a linear form as β∗
i,r=a+(a−b)∗(i−1)/(p−1),i= 1,···,p.
Setting 4.U(a,b): thepelements in β∗
rare generated independently from the uniform distribution with
parameters aandb.
In Table 2, we generate β∗
1using Setting 1 with a= 0, and generate β∗
0using Setting 2 with different
choices foraandbto obtain networks with different expected density. In Table 3, we generate β∗
0andβ∗
1
using combinations of these four settings with different parameters such that the resulting networks have
expected density either around 0.05 (sparse) or 0.5 (dense). The number of networks in each process
and the number of nodes in each network are set as ( n,p) = (20,200),(20,500),(50,200) or (50 ,500).
The errors for estimating θ∗in terms of the ℓ∞andℓ2norms are reported via 100 replications. To
further compare the accuracy for estimating β∗
0andβ∗
1, in Table 4, we have conducted experiments
16Table 2: The estimation errors of MME and MLE under Setting 1 and Setting 2 for β∗
0by setting
β∗
1=0p.
np β∗
0 MME,ℓ2MME,ℓ∞ MLE,ℓ2MLE,ℓ∞
20 200{0} 0.074 0.219 0.071 0.212
50 200{0} 0.046 0.138 0.045 0.136
20 500{0} 0.046 0.150 0.045 0.146
50 500{0} 0.029 0.093 0.028 0.092
20 200{0.5,−0.5} 0.092 0.222 0.091 0.217
50 200{0.5,−0.5} 0.058 0.140 0.058 0.139
20 500{0.5,−0.5} 0.058 0.154 0.057 0.148
50 500{0.5,−0.5} 0.036 0.095 0.036 0.093
20 200{1,−1} 0.120 0.305 0.117 0.284
50 200{1,−1} 0.074 0.186 0.074 0.177
20 500{1,−1} 0.075 0.200 0.073 0.190
50 500{1,−1} 0.038 0.125 0.036 0.119
20 200{1.5,−1.5} 0.164 0.436 0.156 0.397
50 200{1.5,−1.5} 0.102 0.255 0.097 0.236
20 500{1.5,−1.5} 0.103 0.287 0.097 0.262
50 500{1.5,−1.5} 0.065 0.178 0.061 0.164
under Settings 3 and 4, and reported the estimation errors for β∗
0andβ∗
1separately. We summarize the
simulation results below:
•The effect of ( n,p). Similar to what we have observed in Figure 2, the estimation errors become
smaller when norpbecomes larger. Interestingly, from Tables 2–3 we can observe that, under
the same setting, the errors in ℓ2norm when ( n,p) = (50,200) are very close to those when
(n,p) = (20,500). This is to some degree consistent with our finding in Theorem 2 where the
upper bound depends on ( n,p) through their product np.
•The effect of sparsity. From Table 2 we can see that, as the expected density decreases, the
estimation errors increase in almost all the cases. On the other hand, even though the parameters
take different values in Table 3, the errors in the sparse cases are in general larger than those in
the dense cases.
•The impact of κ0:=∥β∗
0∥∞. Typically, estimation errors tend to increase with larger values of κ0,
as evidenced in Table 2. Additionally, when maintaining the same overall sparsity level, larger κ0
values are correlated with greater estimation errors, as illustrated in Table 3.
•MLE vs MME. In general, the estimation errors of the MLE are smaller than those of the MME
in most cases as can be seen in Tables 2 and Table 3. In Table 4 where the estimation errors for
β∗
0andβ∗
1are reported separately, we can see that the estimation errors of the MME of β∗
1are
generally larger than those of the MLE of β∗
1, especially when nis large.
5.3 Real data
In this section, we apply our TWHM to a real dataset to examine an insect interaction network process
[25]. We focus on a subset of the data named insecta-ant-colony4 that contains the social interactions of
102 ants in 41 days. In this dataset, the position and orientation of all the ants were recorded twice per
second to infer their movements and interactions, based on which 41 daily networks were constructed.
More specifically, Xt
i,jis 1 if there is an interaction between ants iandjduring day t, and 0 otherwise.
17Table 3: The average estimation errors of MME and MLE under combinations of different settings
Density = 0.05
np β∗
0 β∗
1 MME,ℓ2MME,ℓ∞ MLE,ℓ2MLE,ℓ∞
20 200L(−4,0)U(−1,1) 0.419 1.833 0.392 1.8
50 200L(−4,0)U(−1,1) 0.253 0.913 0.227 0.82
20 500L(−4,0)U(−1,1) 0.246 1.119 0.218 0.9
50 500L(−4,0)U(−1,1) 0.170 0.626 0.148 0.621
20 200L(−4,0){0} 0.275 1.452 0.280 1.516
50 200L(−4,0){0} 0.161 0.771 0.162 0.774
20 500L(−4,0){0} 0.160 0.892 0.162 0.904
50 500L(−4,0){0} 0.098 0.506 0.099 0.507
20 200{−1.47}U(−1,1) 0.187 0.588 0.161 0.514
50 200{−1.47}U(−1,1) 0.116 0.351 0.099 0.305
20 500{−1.47}U(−1,1) 0.114 0.387 0.099 0.339
50 500{−1.47}U(−1,1) 0.073 0.246 0.062 0.208
20 200{−1.47}{0} 0.150 0.482 0.151 0.484
50 200{−1.47}{0} 0.93 0.289 0.093 0.29
20 500{−1.47}{0} 0.93 0.309 0.093 0.311
50 500{−1.47}{0} 0.058 0.195 0.058 0.195
Density = 0.5
20 200L(−2,2)U(−0.1,0.1) 0.132 0.415 0.012 0.318
50 200L(−2,2)U(−0.1,0.1) 0.080 0.238 0.069 0.194
20 500L(−2,2)U(−0.1,0.1) 0.080 0.272 0.068 0.217
50 500L(−2,2)U(−0.1,0.1) 0.050 0.168 0.043 0.135
20 200L(−1,1)U(−1,1) 0.107 0.324 0.095 0.264
50 200L(−1,1)U(−1,1) 0.067 0.194 0.060 0.163
20 500L(−1,1)U(−1,1) 0.071 0.267 0.061 0.205
50 500L(−1,1)U(−1,1) 0.044 0.156 0.039 0.130
20 200L(−2,2)U(−1,1) 0.137 0.478 0.112 0.329
50 200L(−2,2)U(−1,1) 0.084 0.274 0.070 0.205
20 500L(−2,2)U(−1,1) 0.087 0.352 0.071 0.250
50 500L(−2,2)U(−1,1) 0.054 0.211 0.044 0.150
In the ACF and PACF plots of the degree sequences of selected ants (c.f. Figure 1 in Appendix B.1),
we can observe patterns similar to those of a first-order autoregressive model with long memory. This
motivates the use of TWHM for the analysis of this dataset.
In [25], the 41 daily networks were split into four periods with 11, 10, 10, and 10 days respectively,
because the corresponding days separating these periods were identified as change-points. By excluding
ants that did not interact with others, we are left with p= 102 nodes in period one, p= 73 nodes in
period two, p= 55 nodes in period three and p= 35 nodes in period four. Thus we take the networks
on day 1, day 12, day 22 and day 32 as the initial networks and fit four different TWHMs, one for each
of the four periods.
To appreciate how TWHM captures static heterogeneity, we present a subgraph of 10 nodes during the
fourth period ( t= 32–41), 5 of which have the largest and 5 have the smallest fitted βi,0values. The edges
of this subgraph are drawn to represent aggregated static connections defined as ( X32+···+X42)/10
between these ants. We can see from the left panel of Figure 3 that the magnitudes of the fitted static
heterogeneity parameters agree in principle with the activeness of each ant making connections. On
the other hand, we examine how TWHM can capture dynamic heterogeneity. Towards this, we plot a
18Table 4: The means and standard deviations of the errors of MME and MLE for estimating β∗
0andβ∗
1.
n 20 100 20 100
p 200 200 500 500
β∗
0∼L (−1,1)andβ∗
1∼U(0,2)
MME, ℓ2β∗
00.163(0.010) 0.096(0.006) 0.099(0.004) 0.057(0.002)
β∗
10.177(0.010) 0.084(0.005) 0.104(0.004) 0.050(0.002)
MME, ℓ∞β∗
00.570(0.103) 0.367(0.085) 0.395(0.070) 0.241(0.042)
β∗
10.658(0.137) 0.421(0.079) 0.438(0.076) 0.214(0.037)
MLE, ℓ2β∗
00.211(0.013) 0.091(0.006) 0.121(0.005) 0.054(0.002)
β∗
10.166(0.011) 0.072(0.005) 0.096(0.004) 0.043(0.002)
MLE, ℓ∞β∗
00.809(0.180) 0.354(0.076) 0.532(0.098) 0.232(0.041)
β∗
10.617(0.116) 0.265(0.052) 0.399(0.065) 0.172(0.028)
β∗
0∼L (−2,0)andβ∗
1∼U(0,2)
MME, ℓ2β∗
00.133(0.012) 0.080(0.007) 0.081(0.004) 0.047(0.002)
β∗
10.093(0.006) 0.053(0.004) 0.056(0.003) 0.032(0.002)
MME, ℓ∞β∗
00.568(0.104) 0.365(0.087) 0.394(0.071) 0.241(0.042)
β∗
10.387(0.069) 0.236(0.043) 0.258(0.037) 0.162(0.025)
MLE, ℓ2β∗
00.176(0.016) 0.076(0.007) 0.100(0.006) 0.044(0.002)
β∗
10.116(0.009) 0.051(0.004) 0.068(0.003) 0.031(0.002)
MLE, ℓ∞β∗
00.809(0.181) 0.351(0.078) 0.531(0.099) 0.232(0.041)
β∗
10.513(0.088) 0.227(0.047) 0.348(0.058) 0.158(0.024)
−1.84−1.82−0.55 −0.45
−0.41
1.73
1.99
2.10 2.112.47
−7.96−5.61−1.29 −0.75
−0.17
0.41
2.91
3 4.094.19
Figure 3: The aggregated networks of 10 selected ants during the fourth period reflect static heterogeneity
(Left) and dynamic heterogeneity (Right) respectively. The thickness of each edge is proportional to the
aggregation. The number in the nodes are the fitted βi,0(Left) andβi,1(Right).
subgraph of the 10 nodes having the smallest fitted βi,0values in Figure 3(b), where edges represent
the magnitude of/summationtext41
t=33I/parenleftbig
Xt
i,j=Xt−1
i,j/parenrightbig
/9 which is a measure of the extent that an edge is preserved
across the whole period and hence dynamic heterogeneity. Again, we can see an agreement between the
fitted β∗
1and how likely these nodes will preserve their ties.
To evaluate how TWHM performs when it comes to making prediction, we further carry out the
following experiments:
19(i) From (1), given the MLE {/hatwideβi,r,i= 1,...,p,r = 0,1}and the network at time t−1, we can estimate
the conditional expectation of node i’s degree as
˜dt
i:=p/summationdisplay
j=1,j̸=iE/parenleftig
Xt
i,j/vextendsingle/vextendsingle/vextendsingleXt−1
i,j,/hatwideθ/parenrightig
=p/summationdisplay
j=1,j̸=i/parenleftigg
e/hatwideβi,0+/hatwideβj,0
1 +e/hatwideβi,0+/hatwideβj,0+e/hatwideβi,1+/hatwideβj,1+e/hatwideβi,1+/hatwideβj,1
1 +e/hatwideβi,0+/hatwideβj,+e/hatwideβi,1+/hatwideβj,1Xt−1
i,j/parenrightigg
.
We can then compare the density of the estimated degree sequence {˜dt
i,i= 1,...,p}with that
of the observed degree sequence {dt
i,i= 1,...,p}at timet. To provide a comparison, we treat
networks in each period as i.i.d. observations and utilize the classical β-model to derive the degree
sequence estimator {ˇdt}for the four periods. The fitted degree distributions are depicted in Figure
4, revealing a close resemblance between the estimated and observed densities. This observation
suggests that the TWHM demonstrates strong performance in one-step-ahead prediction.
To further assess the similarity between the estimated degree sequences {˜dt},{ˇdt}, and the true
degree sequence{dt}, we compute the Kolmogorov-Smirnov (KS) distance and conduct the KS
test fort= 2,..., 41. The mean and standard deviation of the KS distances, the p-values of the
KS test, and the rejection rate are summarized in Table 5. Notably, at a significance level of 0.05,
out of the 40 KS tests, we fail to reject the null hypothesis that {˜dt}and{dt}originate from
the same distribution in 38 instances, resulting in a rejection rate consistent with the significance
level. Conversely, for the degree sequence estimators based on the β-model{ˇdt}, 8 out of the 40
tests were rejected. These findings indicate that our model exhibits highly promising performance
in recovering the degree sequences.
Table 5: The mean and standard deviation of the KS distances, the p-values of KS test, and the rejection
rate between the true degree sequence {dt}, and the THWM based estimator ˜dtand theβ-model based
estimator ˇdtover the 40 networks ( t= 2,..., 41) in the ant dataset.
KS distance KS test p-value Rejection rate
˜dtvsdt0.179(0.058) 0.361(0.267) 0.05
ˇdtvsdt0.192(0.061) 0.298(0.246) 0.20
(ii) By incorporating network dynamics, TWHM naturally enables one-step-ahead link prediction via
(5.15) P/parenleftig
/hatwideXt
i,j= 1/vextendsingle/vextendsingle/vextendsingleXt−1
i,j/parenrightig
=e/hatwideβi,0+/hatwideβj,0
1 +e/hatwideβi,0+/hatwideβj,0+e/hatwideβi,1+/hatwideβj,1+e/hatwideβi,1+/hatwideβj,1
1 +e/hatwideβi,0+/hatwideβj,0+e/hatwideβi,1+/hatwideβj,1Xt−1
i,j.
To transform these probabilities into links, we threshold them by setting /hatwideXt
i,j= 1 when P/parenleftig
/hatwideXt
i,j= 1/parenrightig
≥ci,jand/hatwideXt
i,j= 0 when P/parenleftig
/hatwideXt
i,j= 1/parenrightig
<ci,jfor some cut-off constants ci,j. As an illustration, we
first consider simply setting ci,j= 0.5 for all 1≤i < j≤pfor predicting links. We shall denote
this approach as TWHM 0.5.
As an alternative, owing to the fact that networks may change slowly, for a given parameter ω, we
also consider the following adaptive approach for choosing ci,j:
(5.16) ˜Xt
i,j:=I{ωP/parenleftig
/hatwideXt
i,j= 1/parenrightig
+ (1−ω)Xt−1
i,j>0.5}.
200204060800.000.020.04degree density at t= 2
DegreeDensity
0204060800.000.020.04degree density at t= 3
DegreeDensity
0204060800.000.020.04degree density at t= 4
DegreeDensity
0204060800.000.020.04degree density at t= 5
DegreeDensity
0204060800.000.020.04degree density at t= 6
DegreeDensity
0204060800.000.020.04degree density at t= 7
DegreeDensity
0204060800.000.020.04degree density at t= 8
DegreeDensity
0204060800.000.020.04degree density at t= 9
DegreeDensity
0204060800.000.020.04degree density at t= 10
DegreeDensity
0204060800.000.020.04degree density at t= 11
DegreeDensity
02040600.00 0.04 0.08degree density at t= 12
DegreeDensity
0204060800.00 0.04 0.08degree density at t= 13
DegreeDensity
0204060800.00 0.04 0.08degree density at t= 14
DegreeDensity
0204060800.00 0.04 0.08degree density at t= 15
DegreeDensity
0204060800.00 0.04 0.08degree density at t= 16
DegreeDensity
0204060800.00 0.04 0.08degree density at t= 17
DegreeDensity
0204060800.00 0.04 0.08degree density at t= 18
DegreeDensity
02040600.00 0.04 0.08degree density at t= 19
DegreeDensity
02040600.00 0.04 0.08degree density at t= 20
DegreeDensity
02040600.00 0.04 0.08degree density at t= 21
DegreeDensity
01030500.000.040.08degree density at t= 22
DegreeDensity
010203040500.000.040.08degree density at t= 23
DegreeDensity
010203040500.000.040.08degree density at t= 24
DegreeDensity
01030500.000.040.08degree density at t= 25
DegreeDensity
01030500.000.040.08degree density at t= 26
DegreeDensity
01030500.000.040.08degree density at t= 27
DegreeDensity
01030500.000.040.08degree density at t= 28
DegreeDensity
01030500.000.040.08degree density at t= 29
DegreeDensity
01030500.000.040.08degree density at t= 30
DegreeDensity
01030500.000.040.08degree density at t= 31
DegreeDensity
01020300.000.100.20degree density at t= 32
DegreeDensity
01020300.000.100.20degree density at t= 33
DegreeDensity
051525350.000.100.20degree density at t= 34
DegreeDensity
01020300.000.100.20degree density at t= 35
DegreeDensity
01020300.000.100.20degree density at t= 36
DegreeDensity
01020300.000.100.20degree density at t= 37
DegreeDensity
0102030400.000.100.20degree density at t= 38
DegreeDensity
01020300.000.100.20degree density at t= 39
DegreeDensity
01020300.000.100.20degree density at t= 40
DegreeDensity
051525350.000.100.20degree density at t= 41
DegreeDensityFigure 4: The observed and estimated degree distributions. X-axis: the node degrees; Red curves: the
smoothed degree distributions of the estimated degree sequences; Black curves: the smoothed degree
distributions of the observed degree sequences.21Table 6: The prediction accuracy of TWHM with 0 .5 as a cut-off point, TWHM with adaptive cut-off
points, and the naive estimator Xt−1.
ntrain Period TWHM 0.5TWHMadaptive Naive
2One 0.773 0.800 0.749
Two 0.817 0.817 0.780
Three 0.837 0.837 0.806
Four 0.824 0.831 0.807
Overall 0.811 0.822 0.784
5One 0.789 0.807 0.759
Two 0.826 0.823 0.779
Three 0.846 0.849 0.805
Four 0.833 0.842 0.805
Overall 0.822 0.829 0.786
8One 0.795 0.800 0.759
Two 0.832 0.832 0.778
Three 0.855 0.845 0.823
Four 0.831 0.863 0.779
Overall 0.825 0.831 0.782
It can be shown that the above estimator is equivalent to the prediction rule I/braceleftig
P/parenleftig
/hatwideXt
i,j= 1/parenrightig
>ci,j/bracerightig
with cut-off values specified as
ci,j=0.5e/hatwideβi,1+/hatwideβj,1+ (1−w)e/hatwideβi,0+/hatwideβj,0
(1−w) +e/hatwideβi,1+/hatwideβj,1+ (1−w)e/hatwideβi,0+/hatwideβj,0,1≤i<j≤p.
This method is denoted as TWHM adaptive . Lastly, as a benchmark, we have also considered a
naive approach that simply predicts XtasXt−1.
In this experiment, we set the number of training samples to be ntrain = 2,5 or 8. For a given
training sample size ntrain and a period with nnetworks, we predict the graph Xntrain +ibased on
the previous ntrain networks{Xt,t=i,...,ntrain +i−1}fori= 1,...,n−ntrain. That is, over
the four periods in the data, we have predicted 33, 21 and 9 networks, with 5151 edges in each
network in the first period, 2628 in the second period, 1485 in the third period, and 595 in the
fourth period for our choices of ntrain. Theωparameter employed in TWHM adaptive is selected as
follows. For prediction in each period, we choose the value in a sequence of ωvalues that produces
the highest prediction accuracy in predicting Xntrain +i−1for predicting Xntrain +i. For example,
in the first period with n= 11 networks, when ntrain = 8, we used{Xt,t=i,···,i+ 7}to predict
Xi+8fori= 1,2,3. For each i, let ˜Xi+7be defined as in (5.16). A set of candidate values for ωwere
used to compute ˜Xi+7, and the one that returns the smallest misclassification rate (in predicting
Xi+7) was used in TWHM adaptive for predicting Xi+8. The mean of the chosen ωis 0.936 when
n= 2, 0.895 whenn= 5, and 0.905 whenn= 8. The prediction accuracy of the above-mentioned
methods, defined as the percentages of correctly predicted links, are reported in Table 6. We can
see that TWHM 0.5and TWHM adaptive both perform better than the naive approach in all the
cases. On the other hand, TWHM coupled with adaptive cut-off points can improve the prediction
accuracy of TWHM with a cur-off value 0.5 in most periods.
6 Summary and Discussion
We have proposed a novel two-way heterogeneity model that utilizes two sets of parameters to explicitly
capture static heterogeneity and dynamic heterogeneity. In a high-dimension setup, we have provided
22the existence and the rate of convergence of its local MLE, and proposed a novel method of moments
estimator as an initial value to find this local MLE. To the best of our knowledge, this is the first model
in the network literature that the local MLE is obtained for a non-convex loss function. The theory of
our model is established by developing new uniform upper bounds for the deviation of the loss function.
While we have focused on the estimation of the parameters in this paper, how to conduct statistical
inference for the local MLE is a natural next step for research. In our setup, we assume that the
parameters are time invariant but this need not be the case. A future direction is to allow the static
heterogeneity parameter β0and/or the dynamic heterogeneity parameter β1to depend on time, giving
rise to non-stationary network processes. In case when these parameters change smoothly over time, we
may consider estimating the parameters βτ
i,0,βτ
i,1at timeτby kernel smoothing, that is, by maximizing
the following smoothed log-likelihood:
˜L(τ,Xn,Xn−1,···,X1|X0)
=n/summationdisplay
t=1wt/summationdisplay
1≤i<j≤p/braceleftigg
−log/parenleftig
1 +eβi,0+βj,0+eβi,1+βj,1/parenrightig
+ (βi,0+βj,0)Xt
i,j/parenleftbig
1−Xt−1
i,j/parenrightbig
+/parenleftbig
1−Xt
i,j/parenrightbig/parenleftbig
1−Xt−1
i,j/parenrightbig
log/parenleftbig
1 +eβi,1+βj,1/parenrightbig
+Xt
i,jXt−1
i,jlog/parenleftbig
eβi,0+βj,0+eβi,1+βj,1/parenrightbig/bracerightigg
,
withwt=K(h−1|t−τ|)/summationtextn
t=1K(h−1|t−τ|),whereK(·) is a kernel function and his the bandwidth parameter. As
another line of research, note that TWHM is formulated as an AR(1) process. We can extend it by
including more time lags. For example, we can extend TWHM to include lag- kdependence by writing
Xt
i,j=I(εt
i,j= 0) +k/summationdisplay
r=1Xt−r
i,jI(εt
i,j=r),
where the innovations εt
i,jare independent such that
P(εt
i,j=r) =eβi,r+βj,r
1 +/summationtextk
s=0eβi,s+βj,sforr= 0,···,k;P(εt
i,j=−1) =1
1 +/summationtextk
s=0eβi,s+βj,s,
with parameter β0= (β1,0,...,βp,0)⊤denoting node-specific static heterogeneity and β= (βi,r)1≤i≤p;1≤r≤k
∈Rp×kdenoting lag- kdynamic fluctuation. Other future lines of research include adding covariates to
model the tendency of nodes making connections [35] and exploring additional structures [3].
23References
[1] Bhattacharjee, M., Banerjee, M., and Michailidis, G. (2020). Change point estimation in a dynamic
stochastic block model. Journal of Machine Learning Research , 21(107):1–59.
[2] Chatterjee, S., Diaconis, P., and Sly, A. (2011). Random graphs with a given degree sequence. The
Annals of Applied Probability , 21(4):1400–1435.
[3] Chen, M., Kato, K., and Leng, C. (2021). Analysis of networks via the sparse β-model. Journal of
the Royal Statistical Society: Series B (Statistical Methodology) , 83(5).
[4] Durante, D., Dunson, D. B., et al. (2016). Locally adaptive dynamic networks. The Annals of Applied
Statistics , 10(4):2203–2232.
[5] Fu, D. and He, J. (2022). Dppin: A biological repository of dynamic protein-protein interaction
network data. In 2022 IEEE International Conference on Big Data (Big Data) , pages 5269–5277.
IEEE.
[6] Gragg, W. and Tapia, R. (1974). Optimal error bounds for the newton–kantorovich theorem. SIAM
Journal on Numerical Analysis , 11(1):10–13.
[7] Graham, B. S. (2017). An econometric model of network formation with degree heterogeneity.
Econometrica , 85(4):1033–1063.
[8] Han, R., Chen, K., and Tan, C. (2020). Bivariate gamma model. Journal of Multivariate Analysis ,
180:104666.
[9] Han, R., Xu, Y., and Chen, K. (2023). A general pairwise comparison model for extremely sparse
networks. Journal of the American Statistical Association , 118(544):2422–2432.
[10] Hanneke, S., Fu, W., and Xing, E. P. (2010). Discrete temporal models of social networks. Electronic
journal of statistics , 4:585–605.
[11] Hanneke, S. and Xing, E. P. (2007). Discrete temporal models of social networks. In Statistical
network analysis: models, issues, and new directions: ICML 2006 workshop on statistical network
analysis, Pittsburgh, PA, USA, June 29, 2006, Revised Selected Papers , pages 115–125. Springer.
[12] Hillar, C. J., Lin, S., and Wibisono, A. (2012). Inverses of symmetric, diagonally dominant positive
matrices and applications. arXiv preprint arXiv:1203.6812 .
[13] Holland, P. W. and Leinhardt, S. (1981). An exponential family of probability distributions for
directed graphs. Journal of the American Statistical Association , 76(373):33–50.
[14] Jiang, B., Li, J., and Yao, Q. (2023). Autoregressive networks. Journal of Machine Learning
Research , 24(227):1–69.
[15] Jin, J. (2015). Fast community detection by score. The Annals of Statistics , 43(1):57–89.
[16] Jin, J., Ke, Z. T., Luo, S., and Wang, M. (2022). Optimal estimation of the number of network
communities. Journal of the American Statistical Association , pages 1–16.
[17] Karrer, B. and Newman, M. E. (2011). Stochastic blockmodels and community structure in net-
works. Physical review E , 83(1):016107.
[18] Karwa, V., Slavkovi´ c, A., et al. (2016). Inference using noisy degrees: Differentially private beta-
model and synthetic graphs. The Annals of Statistics , 44(1):87–112.
24[19] Ke, Z. T. and Jin, J. (2022). The score normalization, especially for highly heterogeneous network
and text data. arXiv preprint arXiv:2204.11097 .
[20] Kolaczyk, E. D. and Cs´ ardi, G. (2020). Statistical analysis of network data with R , volume 65.
Springer, 2 edition.
[21] Krivitsky, P. N. and Handcock, M. S. (2014). A separable model for dynamic networks. Journal of
the Royal Statistical Society. Series B, Statistical Methodology , 76(1):29.
[22] Lin, Z. and Bai, Z. (2011). Probability inequalities . Springer Science & Business Media.
[23] Matias, C. and Miele, V. (2017). Statistical clustering of temporal networks through a dynamic
stochastic block model. Journal of the Royal Statistical Society, B, 79(4):1119–1141.
[24] Merlev` ede, F., Peligrad, M., Rio, E., et al. (2009). Bernstein inequality and moderate deviations
under strong mixing conditions. In High dimensional probability V: the Luminy volume , pages 273–292.
Institute of Mathematical Statistics.
[25] Mersch, D. P., Crespi, A., and Keller, L. (2013). Tracking individuals shows spatial fidelity is a key
regulator of ant social organization. Science , 340(6136):1090–1093.
[26] Minai, A. A. and Williams, R. D. (1993). On the derivatives of the sigmoid. Neural Networks ,
6(6):845–853.
[27] Newman, M. (2018). Networks . Oxford university press.
[28] Pensky, M. (2019). Dynamic network models and graphon estimation. Annals of Statistics ,
47(4):2378–2403.
[29] Sengupta, S. and Chen, Y. (2018). A block model for node popularity in networks with community
structure. Journal of the Royal Statistical Society: Series B (Statistical Methodology) , 80(2):365–386.
[30] Stein, S. and Leng, C. (2020). A sparse β-model with covariates for networks. arXiv preprint
arXiv:2010.13604 .
[31] Van der Vaart, A. W. (2000). Asymptotic statistics , volume 3. Cambridge university press.
[32] Van Der Vaart, A. W. and Wellner, J. A. (1996). Weak convergence. In Weak convergence and
empirical processes , pages 16–28. Springer.
[33] Wainwright, M. J. (2019). High-dimensional statistics: A non-asymptotic viewpoint , volume 48.
Cambridge University Press.
[34] Wu, X., Zhu, L., Guo, J., Zhang, D.-Y., and Lin, K. (2006). Prediction of yeast protein–protein inter-
action network: insights from the gene ontology and annotations. Nucleic acids research , 34(7):2137–
2150.
[35] Yan, T., Jiang, B., Fienberg, S. E., and Leng, C. (2019). Statistical inference in a directed network
model with covariates. Journal of the American Statistical Association , 114(526):857–868.
[36] Yan, T., Leng, C., and Zhu, J. (2015). Supplement to “asymptotics in directed exponential random
graph models with an increasing bi-degree sequence”. Annals of Statistics .
[37] Yan, T., Leng, C., Zhu, J., et al. (2016). Asymptotics in directed exponential random graph models
with an increasing bi-degree sequence. Annals of Statistics , 44(1):31–57.
[38] Yan, T. and Xu, J. (2012). Approximating the inverse of a balanced symmetric matrix with positive
elements. arXiv preprint arXiv:1202.1058 .
[39] Yan, T. and Xu, J. (2013). A central limit theorem in the β-model for undirected random graphs
with a diverging number of vertices. Biometrika , 100(2):519–524.
25A Technical proofs
For brevity, we denote α0,i,j:=eβi,0+βj,0andα1,i,j:=eβi,1+βj,1, and define α∗
0,i,j,α∗
1,i,jand/hatwideα0,i,j,/hatwideα1,i,j
similarly based on the true parameter θ∗and the MLE /hatwideθ.
A.1 Some technical lemmas
Before presenting the proofs for our main results, we first provide some technical lemmas which will
be used from time to time in our proofs. Lemmas 4 and 5 below provide further properties about the
process{Xt}.
Lemma 4. Let{Xt}∼PθWe have:
(i){Xt◦Xt−1,t= 0,1,2,···} where◦is the Hadamard product operator, is strictly stationary. Further-
more for any 1≤i<j≤p,1≤ℓ<m≤pand|t−s|≥1, we have
E/parenleftbig
Xt
i,jXt−1
i,j/parenrightbig
=α0,i,j(α0,i,j+α1,i,j)
(1 +α0,i,j)(1 +α0,i,j+α1,i,j),
Var/parenleftbig
Xt
i,jXt−1
i,j/parenrightbig
=α0,i,j(α0,i,j+α1,i,j)(2α0,i,j+α1,i,j+ 1)
(1 +α0,i,j)2(1 +α0,i,j+α1,i,j)2,
Cov(Xt
i,jXt−1
i,j,Xs
l,mXs−1
l,m) =


/parenleftig
α1,i,j
1+α0,i,j+α1,i,j/parenrightig|t−s|−1α0,i,j(α0,i,j+α1,i,j)2
(1+α0,i,j)2(1+α0,i,j+α1,i,j)2,(i,j) = (l,m),
0, (i,j)̸= (l,m).
(ii){(1−Xt)◦/parenleftbig
1−Xt−1/parenrightbig
,t= 0,1,2,···} is strictly stationary. Furthermore for any 1≤i<j≤p,1≤
ℓ<m≤pand|t−s|≥1, we have
E/parenleftig/parenleftig
1−Xt
i,j/parenrightig/parenleftig
1−Xt−1
i,j/parenrightig/parenrightig
=1 +α1,i,j
(1 +α0,i,j)(1 +α0,i,j+α1,i,j),
Var/parenleftig/parenleftig
1−Xt
i,j/parenrightig/parenleftig
1−Xt−1
i,j/parenrightig/parenrightig
=α0,i,j(1 +α1,i,j)(α0,i,j+α1,i,j+ 2)
(1 +α0,i,j)2(1 +α0,i,j+α1,i,j)2,
Cov/parenleftig/parenleftig
1−Xt
i,j/parenrightig/parenleftig
1−Xt−1
i,j/parenrightig
,(1−Xs
l.m)(1−Xs−1
l,m)/parenrightig
=


/parenleftig
α1,i,j
1+α0,i,j+α1,i,j/parenrightig|t−s|−1α0,i,j(1+α1,i,j)2
(1+α0,i,j)2(1+α0,i,j+α1,i,j)2,(i,j) = (l,m),
0, (i,j)̸= (l,m).
Proof. (i) Denoteµi,j= E/parenleftbig
Xt
i,jXt−1
i,j/parenrightbig
,γi,j(k) = Cov/parenleftbig
Xt
i,jXt−1
i,j,Xt−k
i,jXt−k−1
i,j/parenrightbig
andρi,j(k) =γi,j(k)/γi,j(1)
(k≥1). For every i<j , we have
E/parenleftbig
Xt
i,jXt−1
i,j/parenrightbig
=P/parenleftig
Xt
i,j= 1/vextendsingle/vextendsingle/vextendsingleXt−1
i,j= 1/parenrightig
P/parenleftbig
Xt−1
i,j= 1/parenrightbig
=α0,i,j(α0,i,j+α1,i,j)
(1 +α0,i,j)(1 +α0,i,j+α1,i,j),
Var/parenleftbig
Xt
i,jXt−1
i,j/parenrightbig
= E/parenleftbigg/parenleftig
Xt
i,jXt−1
i,j/parenrightig2/parenrightbigg
−E/parenleftbig
Xt
i,jXt−1
i,j/parenrightbig2
=/parenleftbig
1−E/parenleftbig
Xt
i,jXt−1
i,j/parenrightbig/parenrightbig
E/parenleftbig
Xt
i,jXt−1
i,j/parenrightbig
=α0,i,j(α0,i,j+α1,i,j)(2α0,i,j+α1,i,j+ 1)
(1 +α0,i,j)2(1 +α0,i,j+α1,i,j)2,
and
γi,j(1) = E/parenleftbig
Xt
i,j(Xt−1
i,j)2Xt−2
i,j/parenrightbig
−µ2
i,j=P/parenleftbig
Xt
i,jXt−1
i,jXt−2
i,j= 1/parenrightbig
−µ2
i,j
26=P/parenleftig
Xt
i,j/vextendsingle/vextendsingle/vextendsingleXt−1
i,jXt−2
i,j= 1/parenrightig
P/parenleftbig
Xt−1
i,jXt−2
i,j= 1/parenrightbig
−µ2
i,j
= E/parenleftig
Xt
i,j/vextendsingle/vextendsingle/vextendsingleXt−1
i,j= 1/parenrightig
µi,j−µ2
i,j
=α0,i,j(α0,i,j+α1,i,j)2
(1 +α0,i,j)2(1 +α0,i,j+α1,i,j)2.
Fork≥2, by Proposition 1, we have,
γi,j(k) = E/parenleftbig
Xt
i,jXt−1
i,jXt−k
i,jXt−k−1
i,j/parenrightbig
−µ2
i,j
=P/parenleftig
Xt
i,jXt−1
i,j= 1/vextendsingle/vextendsingle/vextendsingleXt−k
i,jXt−k−1
i,j = 1/parenrightig
P/parenleftbig
Xt−k
i,jXt−k−1
i,j = 1/parenrightbig
−µ2
i,j
=P/parenleftig
Xt
i,j= 1/vextendsingle/vextendsingle/vextendsingleXt−1
i,j= 1/parenrightig
P/parenleftig
Xt−1
i,j= 1/vextendsingle/vextendsingle/vextendsingleXt−k
i,j= 1/parenrightig
µi,j−µ2
i,j
=P/parenleftig
Xt
i,j= 1/vextendsingle/vextendsingle/vextendsingleXt−1
i,j= 1/parenrightig
P/parenleftbig
Xt−1
i,jXt−k
i,j= 1/parenrightbig
P/parenleftbig
Xt−k
i,j= 1/parenrightbig−1µi,j−µ2
i,j
=P/parenleftig
Xt
i,j= 1/vextendsingle/vextendsingle/vextendsingleXt−1
i,j= 1/parenrightig
P/parenleftbig
Xt−1
i,jXt−k
i,j= 1/parenrightbig
P/parenleftig
Xt−k+1
i,j = 1/vextendsingle/vextendsingle/vextendsingleXt−k
i,j= 1/parenrightig
−µ2
i,j
=P/parenleftbig
Xt−1
i,jXt−k
i,j= 1/parenrightbig
P/parenleftig
Xt
i,j= 1/vextendsingle/vextendsingle/vextendsingleXt−1
i,j= 1/parenrightig2
−µ2
i,j
=/parenleftig
P/parenleftbig
Xt−1
i,jXt−k
i,j= 1/parenrightbig
−E/parenleftbig
Xt
i,j/parenrightbig2/parenrightig
P/parenleftig
Xt
i,j= 1/vextendsingle/vextendsingle/vextendsingleXt−1
i,j= 1/parenrightig2
= Cov/parenleftbig
Xt−1
i,j,Xt−k
i,j/parenrightbig(α0,i,j+α1,i,j)2
(1 +α0,i,j+α1,i,j)2
=/parenleftbiggα1,i,j
1 +α0,i,j+α1,i,j/parenrightbiggk−1α0,i,j
(1 +α0,i,j)2(α0,i,j+α1,i,j)2
(1 +α0,i,j+α1,i,j)2
=/parenleftbiggα1,i,j
1 +α0,i,j+α1,i,j/parenrightbiggk−1
γi,j(1).
This proves (i).
(ii) Letµ′
i,j= E/parenleftig/parenleftig
1−Xt
i,j/parenrightig/parenleftig
1−Xt−1
i,j/parenrightig/parenrightig
,γ′
i,j(k) = Cov/parenleftigg/parenleftig
1−Xt
i,j/parenrightig/parenleftig
1−Xt−1
i,j/parenrightig
,/parenleftbig
1−Xt−k
i,j/parenrightbig/parenleftbig
1−Xt−k−1
i,j/parenrightbig/parenrightigg
andρ′
i,j(k) =γ′
i,j(k)/γ′
i,j(1) (k≥1). Similarly, for every i<j , we have
E/parenleftig/parenleftig
1−Xt
i,j/parenrightig/parenleftig
1−Xt−1
i,j/parenrightig/parenrightig
=P/parenleftig
Xt
i,j= 0/vextendsingle/vextendsingle/vextendsingleXt−1
i,j= 0/parenrightig
P/parenleftbig
Xt−1
i,j= 0/parenrightbig
=1 +α1,i,j
(1 +α0,i,j)(1 +α0,i,j+α1,i,j),
Var/parenleftig/parenleftig
1−Xt
i,j/parenrightig/parenleftig
1−Xt−1
i,j/parenrightig/parenrightig
=/parenleftbig
1−µ′
i,j/parenrightbig
µ′
i,j=α0,i,j(1 +α1,i,j)(α0,i,j+α1,i,j+ 2)
(1 +α0,i,j)2(1 +α0,i,j+α1,i,j)2,
and
γ′
i,j(1) = Cov/parenleftig/parenleftig
1−Xt
i,j/parenrightig/parenleftig
1−Xt−1
i,j/parenrightig
,/parenleftbig
1−Xt−1
i,j/parenrightbig/parenleftbig
1−Xt−2
i,j/parenrightbig/parenrightig
= E/parenleftbigg/parenleftig
1−Xt
i,j/parenrightig/parenleftig
1−Xt−1
i,j/parenrightig2/parenleftbig
1−Xt−2
i,j/parenrightbig/parenrightbigg
−E/parenleftig/parenleftig
1−Xt
i,j/parenrightig/parenleftig
1−Xt−1
i,j/parenrightig/parenrightig2
=P/parenleftbigg/parenleftig
1−Xt
i,j/parenrightig/parenleftig
1−Xt−1
i,j/parenrightig2/parenleftbig
1−Xt−2
i,j/parenrightbig
= 1/parenrightbigg
−E/parenleftig/parenleftig
1−Xt
i,j/parenrightig/parenleftig
1−Xt−1
i,j/parenrightig/parenrightig2
=P/parenleftbig
(1−Xt−1
i,j)(1−Xt−2
i,j) = 1/parenrightbig
P/parenleftig
Xt
i,j= 0/vextendsingle/vextendsingle/vextendsingleXt−1
i,j= 0/parenrightig
−/parenleftbig
µ′
i,j/parenrightbig2
= (1 +α0,i,j)/parenleftbig
µ′
i,j/parenrightbig2−/parenleftbig
µ′
i,j/parenrightbig2
27=α0,i,j/parenleftbig
µ′
i,j/parenrightbig2
=α0,i,j(1 +α1,i,j)2
(1 +α0,i,j)2(1 +α0,i,j+α1,i,j)2.
Fork≥2 we have,
γ′
i,j(k) = E/parenleftig/parenleftig
1−Xt
i,j/parenrightig/parenleftig
1−Xt−1
i,j/parenrightig/parenleftbig
1−Xt−k
i,j/parenrightbig/parenleftbig
1−Xt−k−1
i,j/parenrightbig/parenrightig
−/parenleftbig
µ′
i,j/parenrightbig2
=P/parenleftig/parenleftig
1−Xt
i,j/parenrightig/parenleftig
1−Xt−1
i,j/parenrightig
= 1/vextendsingle/vextendsingle/vextendsingle/parenleftbig
1−Xt−k
i,j/parenrightbig/parenleftbig
1−Xt−k−1
i,j/parenrightbig
= 1/parenrightig
µ′
i,j−/parenleftbig
µ′
i,j/parenrightbig2
=P/parenleftig
Xt
i,j= 0/vextendsingle/vextendsingle/vextendsingleXt−1
i,j= 0/parenrightig
P/parenleftig
Xt−1
i,j= 0/vextendsingle/vextendsingle/vextendsingleXt−k
i,j= 0/parenrightig
µ′
i,j−/parenleftbig
µ′
i,j/parenrightbig2
=P/parenleftig
Xt
i,j= 0/vextendsingle/vextendsingle/vextendsingleXt−1
i,j= 0/parenrightig
P/parenleftbig
Xt−1
i,j= 0,Xt−k
i,j= 0/parenrightbig
P/parenleftbig
Xt−k
i,j= 0/parenrightbig−1µ′
i,j−/parenleftbig
µ′
i,j/parenrightbig2,
with
P/parenleftbig
Xt−1
i,j= 0,Xt−k
i,j= 0/parenrightbig
= Cov/parenleftbig
1−Xt−1
i,j,1−Xt−k
i,j/parenrightbig
+ E/parenleftbig
1−Xt
i,j/parenrightbig2
= Cov/parenleftbig
Xt−1
i,j,Xt−k
i,j/parenrightbig
+1
(1 +α0,i,j)2
=/parenleftbiggα1,i,j
1 +α0,i,j+α1,i,j/parenrightbiggk−1α0,i,j
(1 +α0,i,j)2+1
(1 +α0,i,j)2
=1
(1 +α0,i,j)2/parenleftigg/parenleftbiggα1,i,j
1 +α0,i,j+α1,i,j/parenrightbiggk−1
α0,i,j+ 1/parenrightigg
,
and
P/parenleftig
Xt
i,j= 0/vextendsingle/vextendsingle/vextendsingleXt−1
i,j= 0/parenrightig1
(1 +α0,i,j)2P/parenleftbig
Xt−k
i,j= 0/parenrightbig−1=1 +α1,i,j
(1 +α0,i,j)(1 +α0,i,j+α1,i,j).
Thus,
γ′
i,j(k)
=P/parenleftig
Xt
i,j= 0/vextendsingle/vextendsingle/vextendsingleXt−1
i,j= 0/parenrightig
P/parenleftbig
Xt−1
i,j= 0,Xt−k
i,j= 0/parenrightbig
P/parenleftbig
Xt−k
i,j= 0/parenrightbig−1µ′
i,j−/parenleftbig
µ′
i,j/parenrightbig2
=/parenleftig
P/parenleftig
Xt
i,j= 0/vextendsingle/vextendsingle/vextendsingleXt−1
i,j= 0/parenrightig
P/parenleftbig
Xt−1
i,j= 0,Xt−k
i,j= 0/parenrightbig
P/parenleftbig
Xt−k
i,j= 0/parenrightbig−1−1/parenrightig/parenleftbig
µ′
i,j/parenrightbig2
=P/parenleftig
Xt
i,j= 0/vextendsingle/vextendsingle/vextendsingleXt−1
i,j= 0/parenrightig/parenleftbiggα1,i,j
1 +α0,i,j+α1,i,j/parenrightbiggk−1α0,i,j
(1 +α0,i,j)2P/parenleftbig
Xt−k
i,j= 0/parenrightbig−1µ′
i,j
=/parenleftbiggα1,i,j
1 +α0,i,j+α1,i,j/parenrightbiggk−1α0,i,j(1 +α1,i,j)2
(1 +α0,i,j)2(1 +α0,i,j+α1,i,j)2
=/parenleftbiggα1,i,j
1 +α0,i,j+α1,i,j/parenrightbiggk−1
γ′
i,j(1).
This proves (ii).
Lemma 5. Let{Xt}∼Pθhold. Under condition (A1) we have,
sup
1≤i<j≤p/braceleftigg
Var/parenleftiggn/summationdisplay
t=1Xt
i,jXt−1
i,j/parenrightigg
,Var/parenleftiggn/summationdisplay
t=1Xt
i,j/parenrightigg/bracerightigg
=O(n).
28Proof. LetY1,Y2,...be a sequence of Bernoulli random variables with E Yi=µ, Var(Yi) =σ2for all
i= 1,2..., and assume that Cov ( Yi,Yj)≤σ2ρ|i−j|for some 0≤ρ<1. We have
Var/parenleftiggn/summationdisplay
i=1Yi/parenrightigg
=
n/summationdisplay
i=1Var (Yi) +/summationdisplay
1≤i̸=j≤nCov (Yi,Yj)

≤σ2/parenleftbig
n+ 2ρ(n−1) + 2ρ2(n−2) +···+ 2ρn−1/parenrightbig
≤2σ2/parenleftbig
n+ρ(n−1) +ρ2(n−2) +···+ρn−1/parenrightbig
≤2nσ2
1−ρ.
Lemma 5 then follows directly from Proposition 1, Lemma 4, condition (A1), and the above inequality.
Lemma 6. SupposeZi,i= 1,···,pare independent random variables with
E (Zi) = 0,Var (Zi)≤σ2,
andZi≤balmost surely. We have, for any constant c>0, there exists a large enough constant C > 0
such that, with probability greater than 1−(p)−c,
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglep/summationdisplay
i=1Zi/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤C[/radicalbig
plog(p)σ+blog(p)].
Proof. This is a direct result of Bernstein’s inequality [22].
A.2 Proof of Proposition 1
Proof.
(i) follows directly from Proposition 1 of [14]. For (ii), we have,
E/parenleftbig
dt
i/parenrightbig
=p/summationdisplay
k=1,k̸=iE/parenleftbig
Xt
i,k/parenrightbig
=p/summationdisplay
k=1,k̸=ieβi,0+βk,0
1 +eβi,0+βk,0,
Var/parenleftbig
dt
i/parenrightbig
=p/summationdisplay
k=1,k̸=iVar/parenleftbig
Xt
i,k/parenrightbig
=p/summationdisplay
k=1,k̸=ieβi,0+βk,0
(1 +eβi,0+βk,0)2,
Cov/parenleftbig
dt
i,ds
i/parenrightbig
=p/summationdisplay
k=1,k̸=iCov/parenleftbig
Xt
i,k,Xs
i,k/parenrightbig
=p/summationdisplay
k=1,k̸=i/parenleftigg
eβi,1+βk,1
1 +/summationtext1
r=0eβi,r+βk,r/parenrightigg|t−s|
eβi,0+βk,0
(1 +eβi,0+βk,0)2.
Thus,
ρd
i,j(|t−s|)≡Corr(dt
i,ds
j)
=

Ci,ρ/summationtextp
k=1,k̸=i/parenleftbigg
eβi,1+βk,1
1+/summationtext1
r=0eβi,r+βk,r/parenrightbigg|t−s|
eβi,0+βk,0
(1+eβi,0+βk,0)2ifi=j,
0 if i̸=j,
whereCi,ρ=/parenleftig/summationtextp
k=1,k̸=ieβi,0+βk,0
(1+eβi,0+βk,0)2/parenrightig−1
.
29A.3 Proof of Lemma 1
Proof. By Proposition 3 of [14] and Lemma 4, we have that the process {Xt,t= 1,2,...}isα-mixing
with exponential decaying rate. Since the mixing property is hereditary, the processes {(1−Xt)◦/parenleftbig
1−Xt−1/parenrightbig
,t= 1,2,...}and{Xt◦Xt−1,t= 1,2,...}are alsoα-mixing with exponential decaying
rate. From Theorem 1 in [24], we obtain the following concentration inequalities: there exists a positive
constantCsuch that,
P/parenleftigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglen/summationdisplay
t=1/braceleftbig
Xt
i,j−E/parenleftbig
Xt
i,j/parenrightbig/bracerightbig/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle>ϵ/parenrightigg
≤exp/parenleftbigg−Cϵ2
n+ϵlog (n) log log (n)/parenrightbigg
,
P/parenleftigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglen/summationdisplay
t=1/braceleftbig
Xt
i,jXt−1
i,j−E/parenleftbig
Xt
i,jXt−1
i,j/parenrightbig/bracerightbig/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle>ϵ/parenrightigg
≤exp/parenleftbigg−Cϵ2
n+ϵlog (n) log log (n)/parenrightbigg
,
P/parenleftigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglen/summationdisplay
t=1/braceleftbig
(1−Xt
i,j)(1−Xt−1
i,j)−E/parenleftbig
(1−Xt
i,j)(1−Xt−1
i,j)/parenrightbig/bracerightbig/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle>ϵ/parenrightigg
≤exp/parenleftbigg−Cϵ2
n+ϵlog (n) log log (n)/parenrightbigg
,
hold for all 1≤i < j≤p. For any positive constant c > 0, by setting ϵ=c1/radicalbig
nlog(np) +
c1log (n) log log (n) log (np) with a big enough constant c1>0, we have, with probability greater than
1−(np)−c,
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglen/summationdisplay
t=1/braceleftbig
Xt
i,j−E/parenleftbig
Xt
i,j/parenrightbig/bracerightbig/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤ϵ,
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglen/summationdisplay
t=1/braceleftbig
Xt
i,jXt−1
i,j−E/parenleftbig
Xt
i,jXt−1
i,j/parenrightbig/bracerightbig/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤ϵ,
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglen/summationdisplay
t=1/braceleftbig
(1−Xt
i,j)(1−Xt−1
i,j)−E/parenleftbig
(1−Xt
i,j)(1−Xt−1
i,j)/parenrightbig/bracerightbig/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤ϵ,
hold for all 1≤i<j≤p.
A.4 Proof of Lemma 2
Proof. Note that for a,b,c,d∈[0,1], we have
|ab−cd|≤|ab−cb|+|cb−cd|=|a−c|b+|b−d|c≤|a−c|+|b−d|.
With−κ0≤θi,0≤κ0and−κ1≤θi,1≤κ1, for all 1≤i≤pandκr= max (κr,κr) forr= 0,1, we then
have, there exist positive constants C1,C2such that, for all 1 ≤i̸=j≤pand θ∈B∞/parenleftbig
θ∗,cre−4κ0−4κ1/parenrightbig
wherecr>0 is a small enough constant,
E(V2(θ) +V1(θ))i,j=1
pα0,i,j
(1 +α0,i,j+α1,i,j)2≥C1e−4κ1−2κ0
p,
E(−V2(θ))i,j (A.17)
=1
p/parenleftbiggα0,i,jα1,i,j
(1 +α0,i,j+α1,i,j)2−E(bi,j)α0,i,jα1,i,j
(α0,i,j+α1,i,j)2/parenrightbigg
=1
pα0,i,jα1,i,j
(α0,i,j+α1,i,j)2/parenleftbigg(α0,i,j+α1,i,j)2
(1 +α0,i,j+α1,i,j)2−E(bi,j)/parenrightbigg
30=1
pα0,i,jα1,i,j
(α0,i,j+α1,i,j)2/braceleftigg/parenleftbigg(α0,i,j+α1,i,j)2
(1 +α0,i,j+α1,i,j)2−α0,i,j(α0,i,j+α1,i,j)
(1 +α0,i,j)(1 +α0,i,j+α1,i,j)/parenrightbigg
−/parenleftigg
α∗
0,i,j(α∗
0,i,j+α∗
1,i,j)
(1 +α∗
0,i,j)(1 +α∗
0,i,j+α∗
1,i,j)−α0,i,j(α0,i,j+α1,i,j)
(1 +α0,i,j)(1 +α0,i,j+α1,i,j)/parenrightigg/bracerightigg
≥1
pα0,i,jα1,i,j
(α0,i,j+α1,i,j)2/braceleftigg
α1,i,j(α0,i,j+α1,i,j)
(1 +α0,i,j)(1 +α0,i,j+α1,i,j)2
−/parenleftigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleα∗
0,i,j
1 +α∗
0,i,j−α0,i,j
1 +α0,i,j/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle+/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleα∗
0,i,j+α∗
1,i,j
1 +α∗
0,i,j+α∗
1,i,j−α0,i,j+α1,i,j
1 +α0,i,j+α1,i,j/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/parenrightigg/bracerightigg
≥1
pα0,i,jα1,i,j
(α0,i,j+α1,i,j)2/braceleftigg
α1,i,j(α0,i,j+α1,i,j)
(1 +α0,i,j)(1 +α0,i,j+α1,i,j)2
−
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleα0,i,j/parenleftigα∗
0,i,j
α0,i,j−1/parenrightig
/parenleftbig
1 +α0,i,j/parenrightbig/parenleftbig
1 +α∗
0,i,j/parenrightbig/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle+/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleα0,i,j/parenleftigα∗
0,i,j
α0,i,j−1/parenrightig
+α1,i,j/parenleftigα∗
1,i,j
α1,i,j−1/parenrightig
/parenleftbig
1 +α∗
0,i,j+α∗
1,i,j/parenrightbig/parenleftbig
1 +α0,i,j+α1,i,j/parenrightbig/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
/bracerightigg
≥1
pα0,i,jα1,i,j
(α0,i,j+α1,i,j)2/braceleftigg
α1,i,j(α0,i,j+α1,i,j)
(1 +α0,i,j)(1 +α0,i,j+α1,i,j)2−/parenleftig
2/vextendsingle/vextendsingle/vextendsingleα∗
0,i,j
α0,i,j−1/vextendsingle/vextendsingle/vextendsingle+/vextendsingle/vextendsingle/vextendsingleα∗
1,i,j
α1,i,j−1/vextendsingle/vextendsingle/vextendsingle/parenrightig
1 +α∗
0,i,j+α∗
1,i,j/bracerightigg
≥C1e−6κ0−4κ1
p,
and
E(V2(θ) +V3(θ))i,j (A.18)
=1
p/parenleftbiggα1,i,j
(1 +α0,i,j+α1,i,j)2−E(di,j)α1,i,j
(1 +α1,i,j)2/parenrightbigg
=1
pα1,i,j
(1 +α1,i,j)2/parenleftbigg(1 +α1,i,j)2
(1 +α0,i,j+α1,i,j)2−E(di,j)/parenrightbigg
=1
pα1,i,j
(1 +α1,i,j)2/braceleftigg
(1 +α1,i,j)2
(1 +α0,i,j+α1,i,j)2−1 +α1,i,j
(1 +α0,i,j)(1 +α0,i,j+α1,i,j)
−/parenleftigg
1 +α∗
1,i,j
(1 +α∗
0,i,j)(1 +α∗
0,i,j+α∗
1,i,j)−1 +α1,i,j
(1 +α0,i,j)(1 +α0,i,j+α1,i,j)/parenrightigg/bracerightigg
=1
pα1,i,j
(1 +α1,i,j)2/braceleftigg
α0,i,jα1,i,j(1 +α1,i,j)
(1 +α0,i,j)(1 +α0,i,j+α1,i,j)2
−/parenleftigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
1 +α∗
0,i,j−1
1 +α0,i,j/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle+/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1 +α∗
1,i,j
1 +α∗
0,i,j+α∗
1,i,j−1 +α1,i,j
1 +α0,i,j+α1,i,j/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/parenrightigg/bracerightigg
=1
pα1,i,j
(1 +α1,i,j)2/braceleftigg
α0,i,jα1,i,j(1 +α1,i,j)
(1 +α0,i,j)(1 +α0,i,j+α1,i,j)2
−/parenleftigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleα0,i,j−α∗
0,i,j/parenleftbig
1 +α0,i,j/parenrightbig/parenleftbig
1 +α∗
0,i,j/parenrightbig/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle+/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleα0,i,j−α∗
0,i,j+α0,i,jα∗
1,i,j−α∗
0,i,jα1,i,j/parenleftbig
1 +α∗
0,i,j+α∗
1,i,j/parenrightbig/parenleftbig
1 +α0,i,j+α1,i,j/parenrightbig/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/parenrightigg/bracerightigg
=1
pα1,i,j
(1 +α1,i,j)2/braceleftigg
α0,i,jα1,i,j(1 +α1,i,j)
(1 +α0,i,j)(1 +α0,i,j+α1,i,j)2−/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleα0,i,j−α∗
0,i,j/parenleftbig
1 +α0,i,j/parenrightbig/parenleftbig
1 +α∗
0,i,j/parenrightbig/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
31−/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleα0,i,j−α∗
0,i,j+α0,i,jα∗
1,i,j−α∗
0,i,jα∗
1,i,j+α∗
0,i,jα∗
1,i,j−α∗
0,i,jα1,i,j/parenleftbig
1 +α∗
0,i,j+α∗
1,i,j/parenrightbig/parenleftbig
1 +α0,i,j+α1,i,j/parenrightbig/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/bracerightigg
=1
pα1,i,j
(1 +α1,i,j)2/braceleftigg
α0,i,jα1,i,j(1 +α1,i,j)
(1 +α0,i,j)(1 +α0,i,j+α1,i,j)2−/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleα0,i,j/parenleftigα∗
0,i,j
α0,i,j−1/parenrightig
/parenleftbig
1 +α0,i,j/parenrightbig/parenleftbig
1 +α∗
0,i,j/parenrightbig/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
−/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleα0,i,j/parenleftigα∗
0,i,j
α0,i,j−1/parenrightig
−α0,i,jα∗
1,i,j/parenleftigα∗
0,i,j
α0,i,j−1/parenrightig
+α∗
0,i,jα1,i,j/parenleftigα∗
1,i,j
α1,i,j−1/parenrightig
/parenleftbig
1 +α∗
0,i,j+α∗
1,i,j/parenrightbig/parenleftbig
1 +α0,i,j+α1,i,j/parenrightbig/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/bracerightigg
≥C11
pα1,i,j
(1 +α1,i,j)2/braceleftigg
α0,i,jα1,i,j(1 +α1,i,j)
(1 +α0,i,j)(1 +α0,i,j+α1,i,j)2
−/parenleftbigg
2/vextendsingle/vextendsingle/vextendsingle/vextendsingleα∗
0,i,j
α0,i,j−1/vextendsingle/vextendsingle/vextendsingle/vextendsingle+/vextendsingle/vextendsingle/vextendsingle/vextendsingleα∗
1,i,j
α1,i,j−1/vextendsingle/vextendsingle/vextendsingle/vextendsingle/parenrightbigg/bracerightigg
≥C1e−4κ0−4κ1
p.
Notice that the elements in −EV2(θ),E(V2(θ) +V3(θ)) and E( V2(θ) +V1(θ)) are all positive. Denote
z= (z⊤
1,z⊤
2)⊤withz1= (z1,1,...,z 1,p)⊤∈Rpandz2= (z2,1,...,z 2,p)⊤∈Rp. Then there exists a
constantC > 0 such that,
∥E(V(θ))∥2
≥ inf
∥z∥2=1/parenleftigg/summationdisplay
1≤i<j≤p/parenleftig
E (V1(θ) +V2(θ))i,j(z1,i+z1,j)2
+E (V3(θ) +V2(θ))i,j(z2,i+z2,j)2−E (V2(θ))i,j(z1,i+z1,j−z2,i−z2,j)2/parenrightig/parenrightigg
≥ inf
∥z∥2=1/parenleftigg/summationdisplay
1≤i<j≤pE (V1(θ) +V2(θ))i,j(z1,i+z1,j)2
+E (V3(θ) +V2(θ))i,j(z2,i+z2,j)2/parenrightigg
≥C2e−4κ0−4κ1
pinf
∥z∥2=1/summationdisplay
1≤i<j≤p/parenleftbig
(z1,i+z1,j)2+ (z2,i+z2,j)2/parenrightbig
≥Ce−4κ0−4κ1.
Here in the last step we have used the fact that for any a= (a1,...,ap)⊤∈Rp,/summationtext
1≤i<j≤p(ai+aj)2=
a⊤Cawhere C= (p−2)Ip+1p1⊤
p, and the fact that the eigenvalues of Cis greater or equal to p−2.
A.5 Proof of Lemma 3
Proof. Define a series of matrices {Yi,j}(1≤i<j≤p). For Yi,j, the (i,j), (j,i), (i,i), (j,j) elements
areZi,jwhile other elements are set to be zero. Then all the Yi,jmatrices are independent and
/summationdisplay
1≤i<j≤pYi,j=Z.
Since Yi,jare symmetric and centered random matrices, we have Var ( Yi,j) = E ( Yi,jYi,j). Further, by
the definition of Yi,j, we know that the ( i,j)th, (j,i)th, (i,i)th and (j,j)th elements of Var( Yi,j) are
32all equal to 2Var ( Zi,j), while all other elements are zero. Consequently,
∥Yi,j∥2≤bsup
∥a∥2=1/parenleftbig
(ai+aj)2/parenrightbig
≤2b,
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/summationdisplay
1≤i<j≤pVar(Yi,j)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2= sup
∥a∥2=1
/summationdisplay
1≤i<j≤p2Var (Zi,j) (ai+aj)2

≤max
i,j/parenleftig
2Var/parenleftig
Zi,j/parenrightig/parenrightig
sup
∥a∥2=1
/summationdisplay
1≤i<j≤p(ai+aj)2

≤4σ2(p−1).
Using the Matrix Bernstein inequality (c.f. Theorem 6.17 of [33]), we have
P(∥Z∥2>ϵ) =P
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/summationdisplay
1≤i<j≤pYi,j/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2>ϵ
≤2pexp/parenleftbigg
−ϵ2
4σ2(p−1) + 4bϵ/parenrightbigg
.
A.6 Proof of Theorem 1
Proof. Note that for any θandi̸=j, we have
(V2(θ) +V1(θ))i,j=1
pα0,i,j
(1 +α0,i,j+α1,i,j)2>0,
(V2(θ) +V1(θ))i,i=p/summationdisplay
k=1,k̸=i(V2(θ) +V1(θ))i,k.
Therefore, V2(θ) +V1(θ) is always positive definite. Next we prove that all U(θ)∈{−V2(θ),V2(θ) +
V3(θ)}are positive definite (in probability) by showing that with probability tending to 1,
inf
∥a∥2=1/parenleftbig
a⊤(E(U(θ)))a/parenrightbig
>sup
∥a∥2=1/parenleftbig
a⊤(U(θ)−E(U(θ)))a/parenrightbig
,
holds uniformly for all θ∈B∞(θ∗,r). Note that
∥U(θ)−E(U(θ))∥2
≤ ∥U(θ∗)−E(U(θ∗))∥2+∥U(θ)−U(θ∗) + E( U(θ∗))−E(U(θ))∥2.
We consider∥U(θ∗)−E(U(θ∗))∥2first. By setting ϵ=c1/parenleftbig/radicalbig
2σ2(p−1) log(np) +blog(np)/parenrightbig
with some
big enough constant c1>0 in Lemma 3, we have
P(∥Z∥2>ϵ)
≤2pexp/parenleftigg
−c2
12σ2(p−1) log(np) + 2b/radicalbig
2σ2(p−1) log3/2(np) +b2log2(np)
2σ2(p−1) + 4c1b/radicalbig
2σ2(p−1) log(np) + 4c1b2log(np)/parenrightigg
≤2pexp (−c1log(np)/4)
= 2p(np)−c1/4.
Asnp→∞ , we have with probability greater than 1 −2p(np)−c1/4,
(A.19) ∥Z∥2≤c1/parenleftig
σ/radicalbig
2plog(np) +blog(np)/parenrightig
.
33By Lemma 1 and Lemma 5, we have, uniformly for all θand 1≤i̸=j≤p, there exist positive constants
C1,c2such that, with probability greater than 1 −(np)−c2,
|U(θ)i,j−E(U(θ))i,j| ≤C1/parenleftigg/radicaligg
log(np)
np2+log (n) log log (n) log (np)
np/parenrightigg
;
Var (U(θ)i,j)≤supi̸=j/braceleftbig
Var/parenleftbig/summationtextn
t=1Xt
i,jXt−1
i,j/parenrightbig
,Var/parenleftbig/summationtextn
t=1Xt
i,j/parenrightbig/bracerightbig
n2p2
≤C1
np2.
Consequently, from (A.19) we have
∥U(θ∗)−E(U(θ∗))∥2=Op
/radicaligg
log(np)
np+/radicaligg
log3(np)
np2+log (n) log log (n) log2(np)
np
.
Next we derive uniform upper bounds for ∥U(θ)−U(θ∗) + E( U(θ∗))−E(U(θ))∥2. When U(θ) =
−V2(θ), by Lemma 1, we have, there exist positive constants C2andc3such that with probability
greater than 1−(np)−c3,
∥U(θ)−U(θ∗) + E( U(θ∗))−E(U(θ))∥2
= sup
∥a∥2=1/summationdisplay
1≤i,j≤p[U(θ)−U(θ∗) + E( U(θ∗))−E(U(θ))]i,jaiaj
= sup
∥a∥2=1/summationdisplay
1≤i<j≤p[U(θ)−U(θ∗) + E( U(θ∗))−E(U(θ))]i,j(ai+aj)2
≤ max
1≤i<j≤p/vextendsingle/vextendsingle/vextendsingle[U(θ)−U(θ∗) + E( U(θ∗))−E(U(θ))]i,j/vextendsingle/vextendsingle/vextendsinglesup
∥a∥2=1/summationdisplay
1≤i<j≤p(ai+aj)2
≤ max
1≤i<j≤p/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleV2(θ)i,j−V2(θ)∗
i,j+ E/parenleftbig
V2(θ)∗
i,j/parenrightbig
−E (V2(θ)i,j)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2 (p−1)
=2 (p−1)
npmax
1≤i<j≤p/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle(bi,j−E(bi,j))/parenleftigg
α0,i,jα1,i,j
(α0,i,j+α1,i,j)2−α∗
0,i,jα∗
1,i,j
(α∗
0,i,j+α∗
1,i,j)2/parenrightigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤ max
1≤i<j≤p2|bi,j−E(bi,j)|
nmax
1≤i<j≤p/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/parenleftbig
α∗
0,i,jα1,i,j−α0,i,jα∗
1,i,j/parenrightbig/parenleftbig
α∗
0,i,jα0,i,j−α1,i,jα∗
1,i,j/parenrightbig
(α0,i,j+α1,i,j)2(α∗
0,i,j+α∗
1,i,j)2/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
= max
1≤i<j≤p2|bi,j−E(bi,j)|
nmax
i̸=j/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/parenleftbiggα∗
0,i,j
α0,i,j−α∗
1,i,j
α1,i,j/parenrightbiggα0,i,jα1,i,j/parenleftbig
α∗
0,i,jα0,i,j−α1,i,jα∗
1,i,j/parenrightbig
(α0,i,j+α1,i,j)2(α∗
0,i,j+α∗
1,i,j)2/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤C2min/braceleftigg
1,/radicalbigg
log(np)
n+log (n) log log (n) log (np)
n/bracerightigg
max
1≤i<j≤p/vextendsingle/vextendsingle/vextendsingle/vextendsingleα∗
0,i,j
α0,i,j−α∗
1,i,j
α1,i,j/vextendsingle/vextendsingle/vextendsingle/vextendsingle,
holds uniformly for all θ. Similarly, when U(θ) =V2(θ) +V3(θ), by Lemma 1, we have there exist
positive constants C3andc4such that, with probability greater than 1 −(np)−c4,
∥U(θ)−U(θ∗) + E( U(θ∗))−E(U(θ))∥2
≤ max
1≤i<j≤p/vextendsingle/vextendsingle/vextendsingle[U(θ)−U(θ∗) + E( U(θ∗))−E(U(θ))]i,j/vextendsingle/vextendsingle/vextendsinglesup
∥a∥2=1/summationdisplay
1≤i<j≤p(ai+aj)2
≤ max
1≤i<j≤p/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/braceleftigg
V2(θ)i,j+V3(θ)i,j−V2(θ)∗
i,j−V3(θ)∗
i,j+ E/parenleftbig
V2(θ)∗
i,j/parenrightbig
+ E/parenleftbig
V3(θ)∗
i,j/parenrightbig
34−E (V2(θ)i,j)−E (V3(θ)i,j)/bracerightigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2 (p−1)
=2 (p−1)
npmax
1≤i<j≤p/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle(bi,j−E(bi,j))/parenleftigg
α0,i,jα1,i,j
(α0,i,j+α1,i,j)2−α∗
0,i,jα∗
1,i,j
(α∗
0,i,j+α∗
1,i,j)2/parenrightigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
+2 (p−1)
npmax
1≤i<j≤p/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle(di,j−E(di,j))/parenleftigg
α1,i,j
(1 +α1,i,j)2−α∗
1,i,j
(1 +α∗
1,i,j)2/parenrightigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤2 max
1≤i<j≤p|bi,j−E(bi,j)|
nmax
1≤i<j≤p/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/parenleftbiggα∗
1,i,j
α1,i,j−α∗
0,i,j
α0,i,j/parenrightbiggα0,i,jα1,i,j/parenleftbig
α0,i,jα∗
0,i,j−α1,i,jα∗
1,i,j/parenrightbig
(α0,i,j+α1,i,j)2(α∗
0,i,j+α∗
1,i,j)2/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
+2 max
1≤i<j≤p|di,j−E(di,j)|
nmax
1≤i<j≤p/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/parenleftbigg
1−α∗
1,i,j
α1,i,j/parenrightbiggα1,i,j/parenleftbig
1−α1,i,jα∗
1,i,j/parenrightbig
(1 +α1,i,j)2(1 +α∗
1,i,j)2/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤C3min/braceleftigg
1,/radicalbigg
log(np)
n+log (n) log log (n) log (np)
n/bracerightigg
×/parenleftbigg
max
1≤i<j≤p/vextendsingle/vextendsingle/vextendsingle/vextendsingleα∗
0,i,j
α0,i,j−α∗
1,i,j
α1,i,j/vextendsingle/vextendsingle/vextendsingle/vextendsingle+ max
1≤i<j≤p/vextendsingle/vextendsingle/vextendsingle/vextendsingle1−α∗
1,i,j
α1,i,j/vextendsingle/vextendsingle/vextendsingle/vextendsingle/parenrightbigg
,
holds uniformly for all θ. Consequently, there exist positive constants C4andc5such that, with proba-
bility greater than 1 −(np)−c5,
sup
θ∈B∞(θ∗,r)∥U(θ)−U(θ∗) + E( U(θ∗))−E(U(θ))∥2
≤max{C3,C2}min/braceleftigg
1,/radicalbigg
log(np)
n+log (n) log log (n) log (np)
n/bracerightigg
× sup
θ∈B∞(θ∗,r)/parenleftbigg
max
1≤i<j≤p/vextendsingle/vextendsingle/vextendsingle/vextendsingleα∗
0,i,j
α0,i,j−α∗
1,i,j
α1,i,j/vextendsingle/vextendsingle/vextendsingle/vextendsingle+ max
1≤i<j≤p/vextendsingle/vextendsingle/vextendsingle/vextendsingle1−α∗
1,i,j
α1,i,j/vextendsingle/vextendsingle/vextendsingle/vextendsingle/parenrightbigg
≤max{C3,C2}min/braceleftigg
1,/radicalbigg
log(np)
n+log (n) log log (n) log (np)
n/bracerightigg
× sup
θ∈B∞(θ∗,r)/parenleftbigg
max
1≤i<j≤p/vextendsingle/vextendsingle/vextendsingle/vextendsingleα∗
0,i,j
α0,i,j−1/vextendsingle/vextendsingle/vextendsingle/vextendsingle+ 2 max
1≤i<j≤p/vextendsingle/vextendsingle/vextendsingle/vextendsingleα∗
1,i,j
α1,i,j−1/vextendsingle/vextendsingle/vextendsingle/vextendsingle/parenrightbigg
≤C4min/braceleftigg
1,/radicalbigg
log(np)
n+log (n) log log (n) log (np)
n/bracerightigg
r.
On the other hand, by inequalities (A.17) and (A.18) we have, there exists a positive constant C5such
that
inf
∥a∥2=1/parenleftbig
a⊤E(U(θ))a/parenrightbig
= inf
∥a∥2=1/summationdisplay
1≤i<j≤pE(U(θ))i,j(ai+aj)2
≥C5e−4κ0−4κ1.
holds uniformly for any θ∈B∞/parenleftbig
θ∗,cre−4κ0−4κ1/parenrightbig
wherecr>0 is a small enough constant. Thus, as
np→∞ ,κ0+κ1≤clognp
log(np)for some small enough constant c>0 andr=cre−4κ0−4κ1for some
small enough constant 0 <cr<C 5/(2C4), we have, there exist big enough positive constants C6,c6such
that with probability greater than 1 −(np)−c6,
∥U(θ)−E(U(θ))∥2
35≤ ∥U(θ∗)−E(U(θ∗))∥2+∥U(θ)−U(θ∗) + E( U(θ∗))−E(U(θ))∥2
≤C6
/radicaligg
log(np)
np+/radicaligg
log3(np)
np2+log (n) log log (n) log2(np)
np
+crC4e−4κ0−4κ1
≤2crC4e−4κ0−4κ1
< C 5e−4κ0−4κ1
≤ inf
∥a∥2=1/parenleftbig
a⊤(E(U(θ)))a/parenrightbig
,
holds uniformly for all θ∈B∞(θ∗,cre−4κ0−4κ1). The statements in Theorem 1 can then be concluded.
A.7 Proof of Theorem 2
Proof. Recall that
l(θ) :=1
p/summationdisplay
1≤i<j≤plog/parenleftig
1 +eβi,0+βj,0+eβi,1+βj,1/parenrightig
−1
np/summationdisplay
1≤i<j≤p/braceleftigg
(βi,0+βj,0)n/summationdisplay
t=1Xt
i,j+
log/parenleftbig
1 +eβi,1+βj,1/parenrightbign/summationdisplay
t=1/parenleftbig
1−Xt
i,j/parenrightbig/parenleftbig
1−Xt−1
i,j/parenrightbig
+ log/parenleftbig
1 +eβi,1+βj,1−βi,0−βj,0/parenrightbign/summationdisplay
t=1Xt
i,jXt−1
i,j/bracerightigg
,
and writelE(θ) = El(θ); that is,
lE(θ) :=1
p/summationdisplay
1≤i<j≤plog/parenleftig
1 +eβi,0+βj,0+eβi,1+βj,1/parenrightig
−1
np/summationdisplay
1≤i<j≤p/braceleftigg
(βi,0+βj,0)n/summationdisplay
t=1E/parenleftbig
Xt
i,j/parenrightbig
+ log/parenleftbig
1 +eβi,1+βj,1/parenrightbign/summationdisplay
t=1E/parenleftbig
1−Xt
i,j/parenrightbig/parenleftbig
1−Xt−1
i,j/parenrightbig
+ log/parenleftbig
1 +eβi,1+βj,1−βi,0−βj,0/parenrightbign/summationdisplay
t=1E/parenleftbig
Xt
i,jXt−1
i,j/parenrightbig/bracerightigg
,
Define Dn,p:=/braceleftig
ˇθ:l/parenleftig
ˇθ/parenrightig
−l(θ∗)≤0 and∥ˇθ−θ∗∥∞≤cre−4κ0−4κ1/bracerightig
. Note that when cris small
enough,cre−4κ0−4κ1< α 0. By Corollary 4, there exist big enough positive constants C1,c1such that,
with probability greater than 1 −(np)−c1,
/vextendsingle/vextendsingle/vextendsingle/parenleftbig
l(θ∗)−l/parenleftig
ˇθ/parenrightig/parenrightbig
−/parenleftbig
lE(θ∗)−lE/parenleftig
ˇθ/parenrightig/parenrightbig/vextendsingle/vextendsingle/vextendsingle≤C1/parenleftbigg
1 +log(np)√p/parenrightbigg/radicalbigg
log(np)
n/vextenddouble/vextenddouble/vextenddoubleˇθ−θ∗/vextenddouble/vextenddouble/vextenddouble
2,
holds uniformly for all random variable ˇθ∈Dn,p.
Note that θ∗is the minimizer of lE(·). By Lemma 2, there exists a constant C2>0, such that for all
ˇθ∈Dn,p, we have,
/parenleftbig
l(θ∗)−l/parenleftig
ˇθ/parenrightig/parenrightbig
−/parenleftbig
lE(θ∗)−lE/parenleftig
ˇθ/parenrightig/parenrightbig
≥lE/parenleftig
ˇθ/parenrightig
−lE(θ∗)
=/parenleftig
ˇθ−θ∗/parenrightig⊤
∇2lE(cθθ∗+ (1−cθ)ˇθ)/parenleftig
ˇθ−θ∗/parenrightig
36≥/vextenddouble/vextenddouble/vextenddoubleˇθ−θ∗/vextenddouble/vextenddouble/vextenddouble2
2inf
∥a∥2≤1/parenleftig
a⊤∇2lE(cθˇθ+ (1−cθ)θ∗)a/parenrightig
≥C2
e4κ0+4κ1/vextenddouble/vextenddouble/vextenddoubleˇθ−θ∗/vextenddouble/vextenddouble/vextenddouble2
2.
Here 0≤cθ≤1 is a random scalar that may depend on ˇθ. Consequently, as np→∞ , with probability
greater than 1−(np)−c1, there exists a constant C3>0 such that
(A.20) sup
ˇθ∈Dn,p/vextenddouble/vextenddouble/vextenddoubleˇθ−θ∗/vextenddouble/vextenddouble/vextenddouble
2≤C3e4κ0+4κ1/radicalbigg
log(np)
n/parenleftbigg
1 +log(np)√p/parenrightbigg
.
Note that the MLE /hatwideθsatisfies that /hatwideθ∈B∞/parenleftbig
θ∗,cre−4κ0−4κ1/parenrightbig
andl(/hatwideθ)≤l(θ∗). Therefore we have
/hatwideθ∈Dn,p, and from (A.20) we can conclude that as np→∞ , with probability tending to 1,
1√p/vextenddouble/vextenddouble/vextenddouble/hatwideθ−θ∗/vextenddouble/vextenddouble/vextenddouble
2≤C3e4κ0+4κ1/radicaligg
log(np)
np/parenleftbigg
1 +log(np)√p/parenrightbigg
.
A.8 Proof of Theorem 3
Before presenting the proof, we first introduce a technical lemma.
Lemma 7. For all a,b,c,d∈RKs.t. max{∥a−d∥∞,∥b−c∥∞}≤ 1and any positive z1,z2s.t.
z1z2≥1/4, we have:
/parenleftig/summationtextK
k=1eak+bk/parenrightig/parenleftig/summationtextK
k=1eck+dk/parenrightig
/parenleftig/summationtextK
k=1eak+ck/parenrightig/parenleftig/summationtextK
k=1ebk+dk/parenrightig≤(e−1)2k2/productdisplay
1≤j,s≤Kez1|ak−dk|2+z2|bs−cs|2.
Hereak,bk,ckanddkare thekth element of a,b,candd, respectively.
Proof. Note that for all 0 <x≤1, we have 1≤(ex−1)/x≤e−1 and for all y, we have|ey−1|≤e|y|−1.
Consequently, for all a,b,c,d∈RKs.t. max{∥a−d∥∞,∥b−c∥∞}≤1, we have,
/parenleftig/summationtextK
k=1eak+bk/parenrightig/parenleftig/summationtextK
k=1eck+dk/parenrightig
/parenleftig/summationtextK
k=1eak+ck/parenrightig/parenleftig/summationtextK
k=1ebk+dk/parenrightig
= 1 +/summationdisplay
1≤j<s≤Keaj+bj+cs+ds−eaj+bs+cj+ds+eas+bs+cj+dj−eas+bj+cs+dj
/parenleftig/summationtextK
k=1eak+ck/parenrightig/parenleftig/summationtextK
k=1ebk+dk/parenrightig
= 1 +/summationdisplay
1≤j<s≤Keas+bs+cj+dj
/parenleftig/summationtextK
k=1eak+ck/parenrightig/parenleftig/summationtextK
k=1ebk+dk/parenrightig/parenleftbig
eaj+ds−as−dj−1/parenrightbig/parenleftbig
ebj+cs−bs−cj−1/parenrightbig
≤1 +/summationdisplay
1≤j<s≤Keas+bs+cj+dj
/parenleftig/summationtextK
k=1eak+ck/parenrightig/parenleftig/summationtextK
k=1ebk+dk/parenrightig/parenleftig
e|aj−dj|+|as−ds|−1/parenrightig/parenleftig
e|bj−cj|+|bs−cs|−1/parenrightig
≤1 + (e−1)2/summationdisplay
1≤j<s≤Keas+bs+cj+dj(|aj−dj|+|as−ds|) (|bj−cj|+|bs−cs|)/parenleftig/summationtextK
k=1eak+ck/parenrightig/parenleftig/summationtextK
k=1ebk+dk/parenrightig
≤1 + (e−1)2/summationdisplay
1≤j<s≤Keas+bs+cj+dj/parenleftbig
e(|aj−dj|+|as−ds|)(|bj−cj|+|bs−cs|)−1/parenrightbig
/parenleftig/summationtextK
k=1eak+ck/parenrightig/parenleftig/summationtextK
k=1ebk+dk/parenrightig
37≤(e−1)2/summationdisplay
1≤j,s≤ke|aj−dj||bs−cs|
≤(e−1)2/summationdisplay
1≤j,s≤kez1|aj−dj|2+z2|bs−cs|2
≤(e−1)2k2/productdisplay
1≤j,s≤kez1|aj−dj|2+z2|bs−cs|2
holds for any positive z1,z2s.t.z1z2≥1/4.
Next we proceed to the proof of Theorem 3.
Proof of Theorem 3
Proof. Recall the functions lE/parenleftbig
θ(i),θ(−i)/parenrightbig
andl/parenleftbig
θ(i),θ(−i)/parenrightbig
defined in the proof of Theorem 2. We
denote the MLE studied in Theorem 2 as /hatwideθ/parenleftbig
the local minimizer of l(θ) inB∞/parenleftbig
θ∗,cre−4κ0−4κ1/parenrightbig/parenrightbig
. Also,
let/hatwideθ(s)= (/hatwideβ(s)
1,0,...,/hatwideβ(s)
p,0,/hatwideβ(s)
1,1,...,/hatwideβ(s)
p,1)⊤be the local minimizer of l(θ) in theℓ∞ballB∞(θ∗,rs) where
rs=e8κ0+8κ1log log(np)/radicalbig
log(np)
(np)s/parenleftbigg
1 +log(np)√p/parenrightbigg
, (A.21)
for some given constants s≥0. Let
s0=12 (κ0+κ1) + log log(np)/2 + log log log( np) + log/parenleftig
1 +log(np)√p/parenrightig
−log(cr)
log(np). (A.22)
We then have /hatwideθ(s0)=/hatwideθ. Under the condition that κ0+κ1≤clog(np) for some positive constant c, we
haves0<1/2 whennpis large enough.
Next we will show that with probability tending to 1, uniformly for all s∈[s0,1/2],/hatwideθ(s), the local
MLE for B∞(θ∗,rs), is also the local MLE in a smaller ball B∞(θ∗,rs′) where
rs′= (np)2s−1
4rs=e8κ0+8κ1log log(np)/radicalbig
log(np)
(np)s′/parenleftbigg
1 +log(np)√p/parenrightbigg
,
ands′=2s+1
4<1/2. Note that θ∗
(i)is the minimizer of lE/parenleftig
·,θ∗
(−i)/parenrightig
. Given θ∗
(−i), the Hessian matrix
oflE/parenleftig
·,θ∗
(−i)/parenrightig
evaluated at θ(i)is a 2×2 matrix given as:
EV(i)/parenleftbig
θ(i)/parenrightbig
:=/bracketleftigg
EV(i)
1/parenleftbig
θ(i)/parenrightbig
EV(i)
2/parenleftbig
θ(i)/parenrightbig
EV(i)
2/parenleftbig
θ(i)/parenrightbig
EV(i)
3/parenleftbig
θ(i)/parenrightbig/bracketrightigg
.
Following the calculations in Lemma 2, there exists a constant C1>0 which is independent of θ(i), such
that, for all θ(i)∈B∞/parenleftig
θ∗
(i),cre−4κ0−4κ1/parenrightig
, we have
EV(i)
2/parenleftbig
θ(i)/parenrightbig
+ EV(i)
1/parenleftbig
θ(i)/parenrightbig
=p/summationdisplay
j=1,j̸=i1
peβi,0+β∗
j,0
(1 +eβi,0+β∗
j,0+eβi,1+β∗
j,1)2≥C1e−2κ0−4κ1,
and
−EV(i)
2/parenleftbig
θ(i)/parenrightbig
≥C1e−4κ0−4κ1; E V(i)
2/parenleftbig
θ(i)/parenrightbig
+ EV(i)
3/parenleftbig
θ(i)/parenrightbig
≥C1e−4κ0−4κ1.
38Then we have for all θ(i)∈B∞/parenleftig
θ∗
(i),cre−4κ0−4κ1/parenrightig
,
/vextenddouble/vextenddouble/vextenddoubleEV(i)/parenleftbig
θ(i)/parenrightbig/vextenddouble/vextenddouble/vextenddouble
2
= inf
∥z∥2=1/parenleftigg/parenleftig
EV(i)
2/parenleftbig
θ(i)/parenrightbig
+ EV(i)
1/parenleftbig
θ(i)/parenrightbig/parenrightig
z2
1+/parenleftig
EV(i)
2/parenleftbig
θ(i)/parenrightbig
+ EV(i)
3/parenleftbig
θ(i)/parenrightbig/parenrightig
z2
2
−EV(i)
2/parenleftbig
θ(i)/parenrightbig
(z1−z2)2/parenrightigg
≥ inf
∥z∥2=1/braceleftigg/parenleftig
EV(i)
2/parenleftbig
θ(i)/parenrightbig
+ EV(i)
1/parenleftbig
θ(i)/parenrightbig/parenrightig
z2
1+/parenleftig
EV(i)
2/parenleftbig
θ(i)/parenrightbig
+ EV(i)
3/parenleftbig
θ(i)/parenrightbig/parenrightig
z2
2/bracerightigg
≥C1
e4κ0+4κ1inf
∥z∥2=1/braceleftigg
z2
1+z2
2/bracerightigg
=C1
e4κ0+4κ1.
Consequently, for any iands∈[s0,1/2], there exists a random scalar θ(ξ)
(i), s.t.
l/parenleftbigg
θ∗
(i),/hatwideθ(s)
(−i)/parenrightbigg
−l/parenleftbigg
/hatwideθ(s)
(i),/hatwideθ(s)
(−i)/parenrightbigg
−/bracketleftbigg
lE/parenleftig
θ∗
(i),θ∗
(−i)/parenrightig
−lE/parenleftbigg
/hatwideθ(s)
(i),θ∗
(−i)/parenrightbigg/bracketrightbigg
(A.23)
≥lE/parenleftbigg
/hatwideθ(s)
(i),θ∗
(−i)/parenrightbigg
−lE/parenleftig
θ∗
(i),θ∗
(−i)/parenrightig
=1
2/parenleftbigg
θ∗
(i)−/hatwideθ(s)
(i)/parenrightbigg⊤
l′′
E/parenleftig
θ(ξ)
(i),θ∗
(−i)/parenrightig/parenleftbigg
θ∗
(i)−/hatwideθ(s)
(i)/parenrightbigg
≥ ∥ θ∗
(i)−/hatwideθ(s)
(i)∥2
2/parenleftbigg
inf
∥θ(i)∥∞<κ/vextenddouble/vextenddouble/vextenddoubleEV(i)/parenleftig
θ(ξ)
(i),θ∗
(−i)/parenrightig/vextenddouble/vextenddouble/vextenddouble
2/parenrightbigg
≥C1∥θ∗
(i)−/hatwideθ(s)
(i)∥2
∞
e−4κ0−4κ1.
On the other hand, notice that
l/parenleftbigg
θ∗
(i),/hatwideθ(s)
(−i)/parenrightbigg
−l/parenleftbigg
θ(i),/hatwideθ(s)
(−i)/parenrightbigg
−/bracketleftig
lE/parenleftig
θ∗
(i),θ∗
(−i)/parenrightig
−lE/parenleftig
θ(i),θ∗
(−i)/parenrightig/bracketrightig
≤/vextendsingle/vextendsingle/vextendsingle/vextendsinglel/parenleftbigg
θ∗
(i),/hatwideθ(s)
(−i)/parenrightbigg
−l/parenleftbigg
θ(i),/hatwideθ(s)
(−i)/parenrightbigg
−/bracketleftig
l/parenleftig
θ∗
(i),θ∗
(−i)/parenrightig
−l/parenleftig
θ(i),θ∗
(−i)/parenrightig/bracketrightig/vextendsingle/vextendsingle/vextendsingle/vextendsingle
+/vextendsingle/vextendsingle/vextendsinglel/parenleftig
θ∗
(i),θ∗
(−i)/parenrightig
−l/parenleftig
θ(i),θ∗
(−i)/parenrightig
−/bracketleftig
lE/parenleftig
θ∗
(i),θ∗
(−i)/parenrightig
−lE/parenleftig
θ(i),θ∗
(−i)/parenrightig/bracketrightig/vextendsingle/vextendsingle/vextendsingle.
By Corollary 4, there exist large enough positive constants C2andc1which are independent of θ(i)such
that, with probability greater than 1 −(np)−c1,
/vextendsingle/vextendsingle/vextendsinglel/parenleftig
θ∗
(i),θ∗
(−i)/parenrightig
−l/parenleftig
θ(i),θ∗
(−i)/parenrightig
−/bracketleftig
lE/parenleftig
θ∗
(i),θ∗
(−i)/parenrightig
−lE/parenleftig
θ(i),θ∗
(−i)/parenrightig/bracketrightig/vextendsingle/vextendsingle/vextendsingle (A.24)
≤C2/parenleftbigg
1 +log(np)√p/parenrightbigg/radicaligg
log(np)
np/vextenddouble/vextenddouble/vextenddoubleθ(i)−θ∗
(i)/vextenddouble/vextenddouble/vextenddouble
2
≤√
2C2/parenleftbigg
1 +log(np)√p/parenrightbigg/radicaligg
log(np)
np/vextenddouble/vextenddouble/vextenddoubleθ(i)−θ∗
(i)/vextenddouble/vextenddouble/vextenddouble
∞,
39holds uniformly for any θ(i)∈B∞/parenleftig
θ∗
(i),α0/parenrightig
whereα0<1/4. By Lemma 7, as np→∞ , there exist big
enough positive constants C3andc2which are independent of θ(i)such that, with probability greater
than 1−(np)−c2,
/vextendsingle/vextendsingle/vextendsingle/vextendsinglel/parenleftbigg
θ∗
(i),/hatwideθ(s)
(−i)/parenrightbigg
−l/parenleftbigg
θ(i),/hatwideθ(s)
(−i)/parenrightbigg
−/bracketleftig
l/parenleftig
θ∗
(i),θ∗
(−i)/parenrightig
−l/parenleftig
θ(i),θ∗
(−i)/parenrightig/bracketrightig/vextendsingle/vextendsingle/vextendsingle/vextendsingle
=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
npp/summationdisplay
j=1,j̸=i/bracketleftigg
−nlog/parenleftigg
1 +eβ∗
i,0+β∗
j,0+eβ∗
i,1+β∗
j,1
1 +eβ∗
i,0+/hatwideβ(s)
j,0+eβ∗
i,1+/hatwideβ(s)
j,1×1 +eβi,0+/hatwideβ(s)
j,0+eβi,1+/hatwideβ(s)
j,1
1 +eβi,0+β∗
j,0+eβi,1+β∗
j,1/parenrightigg
+ log/parenleftigg
1 +eβ∗
i,1+β∗
j,1
1 +eβ∗
i,1+/hatwideβ(s)
j,1×1 +eβi,1+/hatwideβ(s)
j,1
1 +eβi,1+β∗
j,1/parenrightigg
di,j
+ log/parenleftigg
eβ∗
i,0+β∗
j,0+eβ∗
i,1+β∗
j,1
eβ∗
i,0+/hatwideβ(s)
j,0+eβ∗
i,1+/hatwideβ(s)
j,1×eβi,0+/hatwideβ(s)
j,0+eβi,1+/hatwideβ(s)
j,1
eβi,0+β∗
j,0+eβi,1+β∗
j,1/parenrightigg
bi,j/bracketrightigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤C3
pp/summationdisplay
j=1,j̸=i/parenleftigg
z1/vextendsingle/vextendsingleβ∗
i,0−βi,0/vextendsingle/vextendsingle2+z1/vextendsingle/vextendsingleβ∗
i,1−βi,1/vextendsingle/vextendsingle2+z2/vextendsingle/vextendsingle/vextendsingleβ∗
j,0−/hatwideβ(s)
j,0/vextendsingle/vextendsingle/vextendsingle2
+z2/vextendsingle/vextendsingle/vextendsingleβ∗
j,1−/hatwideβ(s)
j,1/vextendsingle/vextendsingle/vextendsingle2/parenrightigg
≤2C3z1∥θ(i)−θ∗
(i)∥2
∞+C3z2
p∥/hatwideθ(s)
(−i)−θ∗
(−i)∥2
2
≤2C3z1e16κ0+16κ1[log log(np)]2log(np)
(np)2s/parenleftbigg
1 +log(np)√p/parenrightbigg2
+C3z2
pe8κ0+8κ1log(np)
n/parenleftbigg
1 +log(np)√p/parenrightbigg2
,
holds uniformly for all for any positive z1,z2s.t.z1z2≥1/4,s∈[s0,1/2] and θ(i)∈B∞/parenleftig
θ∗
(i),rs/parenrightig
.
Here in the last step we have used inequality (A.20) and the fact that for all s,/hatwideθ(s)∈Dn,p. Let
z1= 0.5e−4κ0−4κ1[log log(np)]−1(np)s−1/2andz2= 0.5e4κ0+4κ1log log(np)(np)1/2−s, we have, there
exists a big enough constant C4>0, with probability greater than 1 −(np)−c2,
/vextendsingle/vextendsingle/vextendsingle/vextendsinglel/parenleftbigg
θ∗
(i),/hatwideθ(s)
(−i)/parenrightbigg
−l/parenleftbigg
θ(i),/hatwideθ(s)
(−i)/parenrightbigg
−/bracketleftig
l/parenleftig
θ∗
(i),θ∗
(−i)/parenrightig
−l/parenleftig
θ(i),θ∗
(−i)/parenrightig/bracketrightig/vextendsingle/vextendsingle/vextendsingle/vextendsingle(A.25)
≤C4e12κ0+12κ1log log(np) log(np)
(np)s+1/2/parenleftbigg
1 +log(np)√p/parenrightbigg2
,
holds uniformly for all s∈[s0,1/2] and θ(i)∈B∞/parenleftig
θ∗
(i),rs/parenrightig
. Combining the inequalities (A.24) and
(A.25), we have, with probability greater than 1 −(np)−c3for a large enough constant c3>0,
/vextendsingle/vextendsingle/vextendsingle/vextendsinglel/parenleftbigg
θ∗
(i),/hatwideθ(s)
(−i)/parenrightbigg
−l/parenleftbigg
θ(i),/hatwideθ(s)
(−i)/parenrightbigg
−/bracketleftig
lE/parenleftig
θ∗
(i),θ∗
(−i)/parenrightig
−lE/parenleftig
θ(i),θ∗
(−i)/parenrightig/bracketrightig/vextendsingle/vextendsingle/vextendsingle/vextendsingle(A.26)
≤√
2C2/parenleftbigg
1 +log(np)√p/parenrightbigg/radicaligg
log(np)
np/vextenddouble/vextenddouble/vextenddoubleθ(i)−θ∗
(i)/vextenddouble/vextenddouble/vextenddouble
∞
+C4e12κ0+12κ1log log(np) log(np)
(np)s+1/2/parenleftbigg
1 +log(np)√p/parenrightbigg2
,
holds uniformly for all s∈[s0,1/2], alli= 1,...,p andθ(i)∈B∞/parenleftig
θ∗
(i),rs/parenrightig
.
40Combining the inequalities (A.23) and (A.26), we have, with probability greater than 1 −(np)−c3,
C1
e4κ0+4κ1∥/hatwideθ(s)
(i)−θ∗
(i)∥2
∞≤√
2C2/parenleftbigg
1 +log(np)√p/parenrightbigg/radicaligg
log(np)
np/vextenddouble/vextenddouble/vextenddouble/vextenddouble/hatwideθ(s)
(i)−θ∗
(i)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
∞
+C4e12κ0+12κ1log log(np) log(np)
(np)s+1/2/parenleftbigg
1 +log(np)√p/parenrightbigg2
,
holds uniformly for all s∈[s0,1/2] and alli= 1,...,p . Notice that the constants C1,C2,C3,C4,c2
andc3are all independent of rsands. This indicates that there exists a big enough constant C5>0
which is independent of rsandss.t.
∥/hatwideθ(s)−θ∗∥∞ (A.27)
≤C5e4κ0+4κ1/radicaligg
log(np)
np/parenleftbigg
1 +log(np)√p/parenrightbigg
+C5e8κ0+8κ1/radicalbig
log(np) log log(np)
(np)2s+1
4/parenleftbigg
1 +log(np)√p/parenrightbigg
≤2C5e8κ0+8κ1/radicalbig
log(np) log log(np)
(np)2s+1
4/parenleftbigg
1 +log(np)√p/parenrightbigg
≤e8κ0+8κ1log log(np)/radicalbig
log(np)
(np)2s+1
4/parenleftbigg
1 +log(np)√p/parenrightbigg
.
Recall that/hatwideθ(s)is the local maximizer of l(θ) inB∞(θ∗,rs) with
rs=e8κ0+8κ1log log(np)/radicalbig
log(np)
(np)s/parenleftbigg
1 +log(np)√p/parenrightbigg
.
Thus far we have proved that: with probability greater than 1 −(np)−c3for some large enough constant
c3>0, uniformly for all s∈[s0,1/2],/hatwideθ(s)is also within the ball B∞(θ∗,rs′) with
(A.28) rs′= (np)2s−1
4rs=e8κ0+8κ1log log(np)/radicalbig
log(np)
(np)2s+1
4/parenleftbigg
1 +log(np)√p/parenrightbigg
.
Now define a series {si;i= 0,1,···} s.t.s0is defined as in equation (A.22) and sk=sk−1/2 + 1/4.
We have:
(A.29) sk−1
2=1
2/parenleftbigg
sk−1−1
2/parenrightbigg
=1
2k/parenleftbigg
s0−1
2/parenrightbigg
.
Then, we have sk−1<sk<1/2 fork>1 and lim k→∞sk→1/2.
LetK=⌊log2(log(np))⌋+1 where⌊·⌋is the smallest integer function. Beginning with /hatwideθ(s0)=/hatwideθ, and
repeatedly using the result in (A.27) for Ktimes, we have, with probability greater than 1 −(np)−c3, we
can sequentially reduce the upper bound from ∥/hatwideθ−θ∗∥∞≤e8κ0+8κ1log log(np)√
log(np)
(np)1
2+2s0−1
2/parenleftig
1 +log(np)√p/parenrightig
to:
∥/hatwideθ−θ∗∥∞≤e8κ0+8κ1log log(np)/radicalbig
log(np)
(np)1
2+2s0−1
2K/parenleftbigg
1 +log(np)√p/parenrightbigg
= (np)1−2s0
2Ke8κ0+8κ1log log(np)/radicaligg
log(np)
np/parenleftbigg
1 +log(np)√p/parenrightbigg
41≤e1−2s0e8κ0+8κ1log log(np)/radicaligg
log(np)
np/parenleftbigg
1 +log(np)√p/parenrightbigg
.
Here in the last step of above inequality, we have used the fact that when K=⌊log2(log(np))⌋+ 1>
log2(log(np)),
(np)1−2s0
2K≤/parenleftig
(np)log(2)
log(np)/parenrightig1−2s0
log(2)= 21−2s0
log(2) =e1−2s0.
Thus, assuming that np→∞,n≥2,κ0+κ1≤clog(np) for some positive constant c, we have, with
probability tending to 1,
/vextenddouble/vextenddouble/vextenddouble/hatwideθ−θ∗/vextenddouble/vextenddouble/vextenddouble
∞≲e8κ0+8κ1log log(np)/radicaligg
log(np)
np/parenleftbigg
1 +log(np)√p/parenrightbigg
.
A.9 Proof of Theorem 4
For brevity, we denote ˜ αr,i,j:=e˜βi,r+˜βi,r, withi,j= 1,···,pandr= 0,1. We use the interior mapping
theorem [6] to establish the existence and uniform consistency of the moment estimator. The interior
mapping theorem is presented in Lemma 8.
Lemma 8. (Interior mapping theorem). LetF(x) = (F1(x),···,Fp(x))⊤be a vector function on an
open convex subset DofRpand∥F′(x)−F′(y)∥∞≲γ∥x−y∥. Assume that x0∈D s.t.
/vextenddouble/vextenddouble/vextenddoubleF′(x0)−1/vextenddouble/vextenddouble/vextenddouble
∞≤N,/vextenddouble/vextenddouble/vextenddoubleF′(x0)−1F(x0)/vextenddouble/vextenddouble/vextenddouble
∞≤δ, h = 2Nγδ≤1,
t∗≡2
h/parenleftig
1−√
1−h/parenrightig
δ,B∞(x0,t∗)⊂D,
where N and δare positive constants that may depend on x0andp. Then the Newton iterates xn+1≡
xn−F′(xn)−1F(xn)exists and xn∈B∞(x0,t∗)⊂D for alln≥0. Moreover, x∗= limn→∞xnexists,
x∗∈B∞(x0,t∗)⊂D, whereAdenotes the closure of set AandF(x∗) = 0 .
Proof of Theorem 4.
Proof. Recall that the ˜θ(0)is defined as the solution of
n/summationdisplay
t=1p/summationdisplay
j=1,j̸=iXt
i,j=np/summationdisplay
j=1,j̸=ieβi,0+βj,0
1 +eβi,0+βj,0, i= 1,···,p.
For any x∈Rp, define a system of random functions G(x):
Gi(x) :=−1
npn/summationdisplay
t=1p/summationdisplay
j=1,j̸=i/parenleftbigg
Xt
i,j−exi+xj
1 +exi+xj/parenrightbigg
, i= 1,···,p;
G(x) := ( G1(x),···,Gp(x))⊤.
Asnp→∞ andκ=clog(np) with a small enough constant c >0, there exist big enough constants
C1>0,c1>0 such that with probability greater than 1 −(np)c1,
∥G′(x)−G′(y)∥∞≤C1∥x−y∥∞, (A.30)/vextenddouble/vextenddouble/vextenddouble/vextenddoubleG′/parenleftig
θ∗
(0)/parenrightig−1/vextenddouble/vextenddouble/vextenddouble/vextenddouble
∞≤C1e2κ0,
42/vextenddouble/vextenddouble/vextenddouble/vextenddoubleG′/parenleftig
θ∗
(0)/parenrightig−1
G/parenleftig
θ∗
(0)/parenrightig/vextenddouble/vextenddouble/vextenddouble/vextenddouble
∞≤C1e2κ0/radicaligg
log(n) log(p)
np,
hold for all x,y∈B∞(0,κ0). For brevity, the proof of inequalities in (A.30) is provided independently
in Section A.10. Let x0=θ∗
(0)andD= Int ( B∞(0,κ0)). Here the notation Int( A) denotes the interior
of a given set A. We then have:
N=C1e2κ0, γ =C1, δ =C1e2κ0/radicaligg
log(n) log(p)
np,
h= 2Nγδ = 2C3
1e4κ0/radicaligg
log(n) log(p)
np=o(1),
t∗≡2C1
h/parenleftig
1−√
1−h/parenrightig
e2κ0/radicaligg
log(n) log(p)
np,
B∞/parenleftig
θ∗
(1),t∗/parenrightig
⊂D.
Note that∀h∈(0,1), 1−√
1−h<1−(1−h) =h, we have
t∗≡2C1
h/parenleftig
1−√
1−h/parenrightig
e2κ0/radicaligg
log(n) log(p)
np<4e4κ0/radicaligg
log(n) log(p)
np.
Consequently, by Lemma 8, we have that, with probability tending to 1,
(A.31)/vextenddouble/vextenddouble˜θ(0)−θ(0)/vextenddouble/vextenddouble
∞≲/radicaligg
log(n) log(p)e4κ0
np.
Next, we derive the error bound of ˜θ(1)based on (A.31). For x∈Rp, define a system of random
functions F/parenleftbig
x;˜θ(0)/parenrightbig
:
Fi/parenleftbig
x;˜θ(0)/parenrightbig
:=−1
npn/summationdisplay
t=1p/summationdisplay
j=1,j̸=i/braceleftbigg
Xt
i,jXt−1
i,j−˜α0,i,j
1 + ˜α0,i,j/parenleftbigg
1−1
1 + ˜α0,i,j+exi+xj/parenrightbigg/bracerightbigg
,
F/parenleftbig
x;˜θ(0)/parenrightbig
:=/parenleftbig
F1/parenleftbig
x;˜θ(0)/parenrightbig
,···,Fp/parenleftbig
x;˜θ(0)/parenrightbig/parenrightbig⊤.
Asnp→∞ andκ0=clog(np) with a small enough constant c >0, there exist big enough constants
C2>0,c2>0 such that with probability greater than 1 −(np)c2,
/vextenddouble/vextenddoubleF′/parenleftbig
x;˜θ(0)/parenrightbig
−F′/parenleftbig
y;˜θ(0)/parenrightbig/vextenddouble/vextenddouble
∞≤C2∥x−y∥∞, (A.32)
/vextenddouble/vextenddouble/vextenddouble/vextenddoubleF′/parenleftig
θ∗
(1);˜θ(0)/parenrightig−1/vextenddouble/vextenddouble/vextenddouble/vextenddouble
∞≤C2e12κ0+6κ1,
/vextenddouble/vextenddouble/vextenddouble/vextenddoubleF′/parenleftig
θ∗
(1);˜θ(0)/parenrightig−1
F/parenleftig
θ∗
(1);˜θ(0)/parenrightig/vextenddouble/vextenddouble/vextenddouble/vextenddouble
∞≤C2e14κ0+6κ1/radicaligg
log(n) log(p)
np,
hold for all x,y∈B∞(0,κ1). For brevity, the proof of inequalities in (A.32) is provided independently
in Section A.11. Let x0=θ∗
(1)andD= Int ( B∞(0,κ)). We then have:
N=C2e12κ0+6κ1, γ =C2, δ =C2e14κ0+6κ1/radicaligg
log(n) log(p)
np,
43h= 2Nγδ = 2C3
2e26κ0+12κ1/radicaligg
log(n) log(p)
np=o(1),
t∗≡2C2
h/parenleftig
1−√
1−h/parenrightig
e14κ0+6κ1/radicaligg
log(n) log(p)
np,
B∞/parenleftig
θ∗
(1),t∗/parenrightig
⊂D.
Note that∀h∈(0,1), 1−√
1−h<1−(1−h) =h, we have
t∗≡2C2
h/parenleftig
1−√
1−h/parenrightig
e14κ0+6κ1/radicaligg
log(n) log(p)
np<4C2e14κ0+6κ1/radicaligg
log(n) log(p)
np.
Consequently, by Lemma 8, we have that, with probability tending to 1,
/vextenddouble/vextenddouble/vextenddouble˜θ(1)−θ∗
(1)/vextenddouble/vextenddouble/vextenddouble
∞≤t∗≤4C2e14κ0+6κ1/radicaligg
log(n) log(p)
np.
Combining with (A.31), the theorem is proved.
A.10 Proof of (A.30)
Proof. Note that G′(x) is a balanced symmetric matrix s.t.
G′(x)i,j=1
pα0,i,j
(1 +α0,i,j)2, G′(x)i,i=p/summationdisplay
j=1,j̸=iG′(x)i,j,
fori= 1,···,pandi̸=j. Following [38], we construct a matrix S= (Si,j) to approximate the inverse
ofG′(x). Specifically, for any i̸=j, we set
Si,j=−1
T, Si,i=
p/summationdisplay
j=1,j̸=i1
pα0,i,j
(1 +α0,i,j)2
−1
−1
T,T=/summationdisplay
1≤i̸=j≤p1
pα0,i,j
(1 +α0,i,j)2.
Note that
e−2κ0
4p≤1
pα0,i,j
(1 +α0,i,j)2≤1
4p,
we have
T ∈/parenleftbigg(p−1)e−2κ0
4,(p−1)
4/parenrightbigg
.
By Theorem 1 in [38], with m=e−2κ0/(4p) andM= 1/(4p), there exists a big enough constant C1>0,
we have that
/vextenddouble/vextenddouble/vextenddoubleG′(x)−1−S/vextenddouble/vextenddouble/vextenddouble
max≤M
m2pM+ (p−2)m
2(p−2)m1
(p−1)2+1
2m(p−1)2
=1
2m(p−1)2/parenleftbigg
1 +M
m+M2
m2p
p−2/parenrightbigg
.
≤C1e6κ0
p2,
44where∥A∥max= maxi,jAi,j. Then, there exists a big enough constant C2>0,
/vextenddouble/vextenddouble/vextenddoubleG′(x)−1/vextenddouble/vextenddouble/vextenddouble
∞≤ ∥G′(x)−S∥∞+∥S∥∞
≤p/vextenddouble/vextenddouble/vextenddoubleG′(x)−1−S/vextenddouble/vextenddouble/vextenddouble
max+p
T+ max
i
p/summationdisplay
j=1,j̸=i1
pα0,i,j
(1 +α0,i,j)2
−1
≤pC1e6κ0
p2+ 8e2κ0
< C 2e2κ0.
By Lemma 1 and Lemma 6, there exist big positive constants C3,c1such that, with probability greater
than 1−(np)−c1,
/vextenddouble/vextenddouble/vextenddoubleG/parenleftig
θ∗
(0)/parenrightig/vextenddouble/vextenddouble/vextenddouble
∞(A.33)
= max
i/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
npn/summationdisplay
t=1p/summationdisplay
j=1,j̸=i/parenleftigg
Xt
i,j−eβ∗
i,0+β∗
j,0
1 +eβ∗
i,0+β∗
j,0/parenrightigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
= max
i/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
npn/summationdisplay
t=1p/summationdisplay
j=1,j̸=i/parenleftbig
Xt
i,j−E/parenleftbig
Xt
i,j/parenrightbig/parenrightbig/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤1
npmax
i/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglep/summationdisplay
j=1,j̸=i/parenleftiggn/summationdisplay
t=1/parenleftbig
Xt
i,j−E/parenleftbig
Xt
i,j/parenrightbig/parenrightbig/parenrightigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤C3
np/parenleftig/radicalbig
nplog(p) +/radicalbig
nlog(np) + log (n) log log (n) log (np)/parenrightig
<3C3/radicaligg
log(n) log(p)
np.
Then, for any y∈Rpwith∥y∥∞<κ0, we have that
(A.34)/vextenddouble/vextenddouble/vextenddouble/vextenddoubleG′/parenleftig
θ∗
(0)/parenrightig−1
G/parenleftig
θ∗
(0)/parenrightig/vextenddouble/vextenddouble/vextenddouble/vextenddouble
∞≤/vextenddouble/vextenddouble/vextenddouble/vextenddoubleG′/parenleftig
θ∗
(0)/parenrightig−1/vextenddouble/vextenddouble/vextenddouble/vextenddouble
∞/vextenddouble/vextenddouble/vextenddoubleG/parenleftig
θ∗
(0)/parenrightig/vextenddouble/vextenddouble/vextenddouble
∞≤2C2C3e2κ0/radicaligg
log(n) log(p)
np.
There exists a big enough constant C4>0, we have, for every x,y∈B∞(0,κ),
∥G′(x)−G′(y)∥∞ (A.35)
≤max
i/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
npn/summationdisplay
t=1p/summationdisplay
j=1,j̸=i/parenleftbigg
Xt
i,j−exi+xj
1 +exi+xj/parenrightbigg
−1
npn/summationdisplay
t=1p/summationdisplay
j=1,j̸=i/parenleftbigg
Xt
i,j−eyi+yj
1 +eyi+yj/parenrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤1
pmax
i/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglep/summationdisplay
j=1,j̸=iexi+xj
1 +exi+xj−eyi+yj
1 +eyi+yj/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤1
pmax
i/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglep/summationdisplay
j=1,j̸=iezi+zj
1 +ezi+zj(xi+xj−yi−yj)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤C4∥x−y∥∞
wherezi,j:= (1−ci,j) (xi+xj) +ci,j(yi+yj) with a series of constants ci,j∈(0,1). Combining the
inequalities (A.33), (A.34) and (A.35), we finish the proof of (A.30).
45A.11 Proof of (A.32)
Proof. For brevity, F/parenleftbig
x;˜θ(0)/parenrightbig
is denoted by F(x). Moreover, as all the conclusions hold uniformly for
all˜θ(0), the argument “uniformly for all ˜θ(0)” are also omitted in what follows.
Note that F′(x) is a balanced symmetric matrix s.t.
F′(x)i,j=1
p˜α0,i,j
1 + ˜α0,i,jexi+xj
(1 + ˜α0,i,j+exi+xj)2,F′(x)i,i=p/summationdisplay
j=1,j̸=iF′(x)i,j,
fori= 1,···,pandi̸=j. Following [38], we construct a matrix S= (Si,j) to approximate the inverse
ofF′(x). Specifically, for all i̸=j, we set
Si,j=−1
T, Si,i=
p/summationdisplay
j=1,j̸=i1
p˜α0,i,j
1 + ˜α0,i,jexi+xj
(1 + ˜α0,i,j+exi+xj)2
−1
−1
T,
T=/summationdisplay
1≤i̸=j≤p1
p˜α0,i,j
1 + ˜α0,i,jexi+xj
(1 + ˜α0,i,j+exi+xj)2.
Note that there exists a big enough constant C1>0 s.t., for any i̸=j,
C1
pe4κ0+2κ1≤1
p˜α0,i,j
1 + ˜α0,i,jexi+xj
(1 + ˜α0,i,j+exi+xj)2<1
4p,
we have
(A.36)T=/summationdisplay
1≤i̸=j≤p1
p˜α0,i,j
1 + ˜α0,i,jexi+xj
(1 + ˜α0,i,j+exi+xj)2∈/parenleftig
C1pe−4κ0−2κ1,p
4/parenrightig
.
By Theorem 1 in [38], with m=C1//parenleftbig
pe4κ0+2κ1/parenrightbig
andM=C1/(4p), there exists big enough constant
C2>0, we have that
/vextenddouble/vextenddouble/vextenddoubleF′(x)−1−S/vextenddouble/vextenddouble/vextenddouble
max≤M
m2pM+ (p−2)m
2(p−2)m1
(p−1)2+1
2m(p−1)2
=1
2m(p−1)2/parenleftbigg
1 +M
m+M2
m2p
p−2/parenrightbigg
.
≤C2e12κ0+6κ1
p2,
where∥A∥max= maxi,jAi,j. Then we have that
/vextenddouble/vextenddouble/vextenddoubleF′(x)−1/vextenddouble/vextenddouble/vextenddouble
∞(A.37)
≤ ∥S∥∞+/vextenddouble/vextenddouble/vextenddoubleF′(x)−1−S/vextenddouble/vextenddouble/vextenddouble
∞
≤max
i
p/summationdisplay
j=1,j̸=i1
p˜α0,i,j
1 + ˜α0,i,jexi+xj
(1 + ˜α0,i,j+exi+xj)2
−1
+p
T+p/vextenddouble/vextenddouble/vextenddoubleF′(x)−1−S/vextenddouble/vextenddouble/vextenddouble
max
<2C2e12κ0+6κ1.
Moreover, we have that
/vextenddouble/vextenddouble/vextenddoubleF/parenleftig
θ∗
(1)/parenrightig/vextenddouble/vextenddouble/vextenddouble
∞
46= max
i/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle−1
npn/summationdisplay
t=1p/summationdisplay
j=1,j̸=i/braceleftigg
Xt
i,jXt−1
i,j−˜α0,i,j
1 + ˜α0,i,j/parenleftigg
1−1
1 + ˜α0,i,j+α∗
1,i,j/parenrightigg/bracerightigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤max
i/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
npn/summationdisplay
t=1p/summationdisplay
j=1,j̸=i/braceleftigg
Xt
i,jXt−1
i,j−α∗
0,i,j
1 +α∗
0,i,j/parenleftigg
1−1
1 +α∗
0,i,j+α∗
1,i,j/parenrightigg/bracerightigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
+ max
i/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
npn/summationdisplay
t=1p/summationdisplay
j=1,j̸=i/braceleftigg
α∗
0,i,j
1 +α∗
0,i,jα∗
0,i,j+α∗
1,i,j
1 +α∗
0,i,j+α∗
1,i,j−˜α0,i,j
1 + ˜α0,i,j˜α0,i,j+α∗
1,i,j
1 + ˜α0,i,j+α∗
1,i,j/bracerightigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
=L1+L2.
By Lemma 1 and Lemma 6, there exist big positive constants C3,c2s.t., with probability greater than
1−(np)−c2,
L1=1
npmax
i/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglep/summationdisplay
j=1,j̸=in/summationdisplay
t=1Xt
i,jXt−1
i,j−E/parenleftbig
Xt
i,jXt−1
i,j/parenrightbig/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤1
npmax
i/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglep/summationdisplay
j=1,j̸=i/braceleftiggn/summationdisplay
t=1Xt
i,jXt−1
i,j−E/parenleftbig
Xt
i,jXt−1
i,j/parenrightbig/bracerightigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤C3
np/parenleftig/radicalbig
nplog(p) +/radicalbig
nlog(np) + log (n) log log (n) log (np)/parenrightig
<3C3/radicaligg
log(n) log(p)
np.
Moreover, we have
L2
=1
npmax
i/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglen/summationdisplay
t=1p/summationdisplay
j=1,j̸=i/braceleftigg
α∗
0,i,j
1 +α∗
0,i,j/parenleftigg
1−1
1 +α∗
0,i,j+α∗
1,i,j/parenrightigg
−˜α0,i,j
1 + ˜α0,i,j/parenleftigg
1−1
1 + ˜α0,i,j+α∗
1,i,j/parenrightigg/bracerightigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤max
i,j,βξ/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/parenleftbig
β∗
i,0+β∗
j,0−˜βi,0−˜βj,0/parenrightbigeβξ,i,j
(1 +eβξ,i,j)2
+eβξ,i,j/parenleftbig
1 +eβξ,i,j/parenrightbig/parenleftbig
1 +eβξ,i,j+α∗
1,i,j/parenrightbig
−eβξ,i,j/parenleftbig/parenleftbig
2 +α∗
1,i,j/parenrightbig
eβξ,i,j+ 2e2βξ,i,j/parenrightbig
(1 +eβξ,i,j)2/parenleftbig
1 +eβξ,i,j+α∗
1,i,j/parenrightbig2/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤max
i,j,βξ/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/parenleftbig
β∗
i,0+β∗
j,0−˜βi,0−˜βj,0/parenrightbig/parenleftigg
eβξ,i,j
(1 +eβξ,i,j)2+eβξ,i,j/parenleftbig
1 +α∗
1,i,j−e2βξ,i,j/parenrightbig
(1 +eβξ,i,j)2/parenleftbig
1 +eβξ,i,j+α∗
1,i,j/parenrightbig2/parenrightigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤2/vextenddouble/vextenddouble/vextenddouble˜θ(0)−θ∗
(0)/vextenddouble/vextenddouble/vextenddouble
∞,
whereβξ,i,j:= (1−ci,j)/parenleftbig
β∗
i,0+β∗
j,0/parenrightbig
+ci,j/parenleftbig˜βi,0+˜βj,0/parenrightbig
with a series of constants ci,j∈(0,1). Then, by
equation (A.31), there exists a big enough constant C4>0, we have, with probability tending to 1,
L2≤2/vextenddouble/vextenddouble/vextenddouble˜θ(0)−θ∗
(0)/vextenddouble/vextenddouble/vextenddouble
∞≤C4/radicaligg
log(n) log(p)e4κ0
np.
47We can conclude that, with probability tending to 1,
/vextendsingle/vextendsingle/vextendsingleF/parenleftig
θ∗
(1)/parenrightig/vextendsingle/vextendsingle/vextendsingle
∞≤1
np(L1+L2)
≤3C3/radicaligg
log(n) log(p)
np+C4/radicaligg
log(n) log(p)e4κ0
np
≤C5/radicaligg
log(n) log(p)e4κ0
np,
hold uniformly for any iat the same time. Consequently, we have
/vextenddouble/vextenddouble/vextenddouble/vextenddoubleF′/parenleftig
θ∗
(1)/parenrightig−1
F/parenleftig
θ∗
(1)/parenrightig/vextenddouble/vextenddouble/vextenddouble/vextenddouble
∞≤/vextenddouble/vextenddouble/vextenddouble/vextenddoubleF′/parenleftig
θ∗
(1)/parenrightig−1/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleF/parenleftig
θ∗
(1)/parenrightig/vextenddouble/vextenddouble/vextenddouble
∞(A.38)
≤2C2C5e12κ0+6κ1/radicaligg
log(n) log(p)e4κ0
np.
There exists an big enough constant C6>0, we have, for every x,y∈B∞(0,κ1),
∥F′(x)−F′(y)∥∞(A.39)
≤max
i/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2p/summationdisplay
j=1,j̸=i/parenleftigg
1
p˜α0,i,j
1 + ˜α0,i,jexi+xj
(1 + ˜α0,i,j+exi+xj)2−1
p˜α0,i,j
1 + ˜α0,i,jeyi+yj
(1 + ˜α0,i,j+eyi+yj)2/parenrightigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
=2
pmax
i/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglep/summationdisplay
j=1,j̸=i˜α0,i,j
1 + ˜α0,i,j/parenleftigg
exi+xj
(1 + ˜α0,i,j+exi+xj)2−eyi+yj
(1 + ˜α0,i,j+eyi+yj)2/parenrightigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
=1
pmax
i/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglep/summationdisplay
j=1,j̸=i˜α0,i,j
1 + ˜α0,i,jezi,j
(1 + ˜α0,i,j+ezi,j)3|xi+xj−yi−yj|/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤C6∥x−y∥∞,
wherezi,j:= (1−di,j) (xi+xj) +di,j(yi+yj) with a series of constants di,j∈(0,1). Combining the
inequalities (A.37), (A.38) and (A.39), we finish the proof of (A.32).
A.12 Proof of Corollary 1
Proof. When the condition −κ0≤βi,0≤Cκand∥β1∥∞≤κ1, whereCκ=O(1), is satisfied, there
exists a constant C1>0 such that, for all 1 ≤i̸=j≤pandθ∈B∞(θ∗,r) withr=cre−2κ0−4κ1for a
small enough constant cr>0, it holds
E(V2(θ) +V1(θ))i,j≥C1e−4κ1
p,E(V2(θ) +V3(θ))i,j≥C1e−4κ1−2κ0
p,
and
E(V2(θ))i,j≥C1e−4κ1−2κ0
p.
Then there exists a constant C2>0, for all θ∈B∞(θ∗,r) such that we have
∥E(V(θ))∥2≥C2e−4κ1−2κ0.
With similar arguments as in the proofs of Lemma 2 and Theorems 1 and 2, we can prove Corollary
1.
48A.13 Proof of Corollary 2
Proof. The first inequality can be proved using the results in Corollary 1, along with analogous reasoning
employed in the proof of Theorem 1. Replace equations (A.21) and (A.22) by
rs=e4κ0+8κ1log log(np)/radicalbig
log(np)
(np)s/parenleftbigg
1 +log(np)√p/parenrightbigg
;
s0=6κ0+ 12κ1+ log log(np)/2 + log log log( np) + log/parenleftig
1 +log(np)√p/parenrightig
−log(cr)
log(np).
Similar to the proof of Theorem 3, let z1= 0.5e−2κ0−4κ1[log log(np)]−1(np)s−1/2andz2= 0.5e2κ0+4κ1
log log(np)(np)1/2−s. We can assert the existence of a sufficiently large constant C1>0 such that with
probability greater than 1 −(np)−c2,
/vextendsingle/vextendsingle/vextendsingle/vextendsinglel/parenleftbigg
θ∗
(i),/hatwideθ(s)
(−i)/parenrightbigg
−l/parenleftbigg
θ(i),/hatwideθ(s)
(−i)/parenrightbigg
−/bracketleftig
l/parenleftig
θ∗
(i),θ∗
(−i)/parenrightig
−l/parenleftig
θ(i),θ∗
(−i)/parenrightig/bracketrightig/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤C1e6κ0+12κ1log log(np) log(np)
(np)s+1/2/parenleftbigg
1 +log(np)√p/parenrightbigg2
holds uniformly for all s∈[s0,1/2] and θ(i)∈B∞/parenleftig
θ∗
(i),rs/parenrightig
. Following similar steps, we can ascertain
that asnp→∞ withn≥2, with probability converging to one it holds that
/vextenddouble/vextenddouble/vextenddouble/hatwideθ−θ∗/vextenddouble/vextenddouble/vextenddouble
∞≤Ce4κ0+8κ1log log(np)/radicaligg
log(np)
np/parenleftbigg
1 +log(np)√p/parenrightbigg
.
A.14 Proof of Corollary 3
Proof. There exist positive constants C1andC2such that
/summationdisplay
1≤i̸=j≤p1
p˜α0,i,j
1 + ˜α0,i,jexi+xj
(1 + ˜α0,i,j+exi+xj)2∈/parenleftig
C1pe−2κ0−2κ1,p
4/parenrightig
,
for all∥x∥∞≤κ1. Then the inequalities in (A.32) can be updated to
/vextenddouble/vextenddoubleF′/parenleftbig
x;˜θ(0)/parenrightbig
−F′/parenleftbig
y;˜θ(0)/parenrightbig/vextenddouble/vextenddouble
∞≤C2∥x−y∥∞,
/vextenddouble/vextenddouble/vextenddouble/vextenddoubleF′/parenleftig
θ∗
(1);˜θ(0)/parenrightig−1/vextenddouble/vextenddouble/vextenddouble/vextenddouble
∞≤C2e2κ0+6κ1,
/vextenddouble/vextenddouble/vextenddouble/vextenddoubleF′/parenleftig
θ∗
(1);˜θ(0)/parenrightig−1
F/parenleftig
θ∗
(1);˜θ(0)/parenrightig/vextenddouble/vextenddouble/vextenddouble/vextenddouble
∞≤C2e4κ0+6κ1/radicaligg
log(n) log(p)
np,
for all∥x∥∞≤κ1,∥y∥∞≤κ1. Consequently, with Lemma 8, similar to the proof of Theorem 4, we can
prove Corollary 3.
A.15 Proof of the Theorem 5
Proof. To simplify the notations, in what follows we shall denote li,j(θi,θj) asli,j(θ) instead. Let
K=⌊c0log(np)⌋+ 1 for some big enough constant c0>0, where⌊·⌋is the smallest integer function.
49By the Taylor expansion with Lagrange remainder we have, for any θ∈B∞(θ′,α0), there exists a
θξ∈B∞(θ′,α0) dependent on θs.t.,
L(θ)−L/parenleftbig
θ′/parenrightbig
=p/summationdisplay
I1=1/parenleftbig
θI1−θ′
I1/parenrightbig∂L/parenleftbig
θ′/parenrightbig
∂θI1
+1
2!p/summationdisplay
I1=1p/summationdisplay
I2=1/parenleftbig
θI1−θ′
I1/parenrightbig/parenleftbig
θI2−θ′
I2/parenrightbig∂2L/parenleftbig
θ′/parenrightbig
∂θI2∂θI2
+···
+1
K!p/summationdisplay
I1=1p/summationdisplay
I2=1···p/summationdisplay
IK=1/parenleftiggK/productdisplay
ℓ=1/parenleftbig
θIℓ−θ′
Iℓ/parenrightbig/parenrightigg
∂KL/parenleftbig
θ′/parenrightbig
∂θI1∂θI2···∂θIK
+1
(K+ 1)!p/summationdisplay
I1=1p/summationdisplay
I2=1···p/summationdisplay
IK+1=1/parenleftiggK+1/productdisplay
ℓ=1/parenleftbig
θIℓ−θ′
Iℓ/parenrightbig/parenrightigg∂K+1L/parenleftig
θξ/parenrightig
∂θI1∂θI2···∂θIK+1
=1
pK/summationdisplay
k=1

1
k!/summationdisplay
1≤i̸=j≤p/parenleftigg
Yi,jk/summationdisplay
s=0/parenleftbigk
s/parenrightbig
(θi−θ′
i)s/parenleftbig
θj−θ′
j/parenrightbigk−s∂kli,j/parenleftbig
θ′/parenrightbig
∂θs
i∂θk−s
j/parenrightigg


+1
p1
(K+ 1)!/summationdisplay
1≤i̸=j≤p
Yi,jK+1/summationdisplay
s=0/parenleftbigK+1
s/parenrightbig
(θi−θ′
i)s/parenleftbig
θj−θ′
j/parenrightbigK+1−s∂K+1li,j/parenleftig
θξ/parenrightig
∂θs
i∂θK+1−s
j

≤/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleK/summationdisplay
k=11
p1
k!/summationdisplay
1≤i̸=j≤pYi,j/parenleftigg
(θi−θ′
i)k∂kli,j/parenleftbig
θ′/parenrightbig
∂θk
i+/parenleftbig
θj−θ′
j/parenrightbigk∂kli,j/parenleftbig
θ′/parenrightbig
∂θk
j/parenrightigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
+/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleK/summationdisplay
k=21
p1
k!/summationdisplay
1≤i̸=j≤p/parenleftigg
Yi,jk−1/summationdisplay
s=1/parenleftbigk
s/parenrightbig
(θi−θ′
i)s/parenleftbig
θj−θ′
j/parenrightbigk−s∂kli,j/parenleftbig
θ′/parenrightbig
∂θs
i∂θk−s
j/parenrightigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
+1
p/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
(K+ 1)!/summationdisplay
1≤i̸=j≤p
Yi,jK+1/summationdisplay
s=0/parenleftbigK+1
s/parenrightbig
(θi−θ′
i)s/parenleftbig
θj−θ′
j/parenrightbigK+1−s∂K+1li,j/parenleftig
θξ/parenrightig
∂θs
i∂θK+1−s
j
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
=S(1)+S(2)+S(3).
We first consider S(1). By Lemma 6, there exist big enough constants C1>0,c1>0 such that, uniformly
for all θ∈B∞/parenleftbig
θ′,α0/parenrightbig
and allk= 1,···,K, we have, with probability greater than 1 −(np)−c1,
1
p1
k!/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationdisplay
1≤i̸=j≤pYi,j/parenleftigg
(θi−θ′
i)k∂kli,j/parenleftbig
θ′/parenrightbig
∂θk
i+/parenleftbig
θj−θ′
j/parenrightbigk∂kli,j/parenleftbig
θ′/parenrightbig
∂θk
j/parenrightigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤2
p1
k!p/summationdisplay
i=1/vextendsingle/vextendsingle/vextendsingle/vextendsingleθi−θ′
i
α/vextendsingle/vextendsingle/vextendsingle/vextendsinglek/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglep/summationdisplay
j̸=i,j=1αkYi,j∂kli,j/parenleftbig
θ′/parenrightbig
∂θk
j/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤2
p1
k!max
i/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglep/summationdisplay
j̸=i,j=1αkYi,j∂kli,j/parenleftbig
θ′/parenrightbig
∂θk
j/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglep/summationdisplay
i=1/vextendsingle/vextendsingle/vextendsingle/vextendsingleθi−θ′
i
α/vextendsingle/vextendsingle/vextendsingle/vextendsinglek
≤C1
p1
k!/parenleftig
b(p)log(np) +σ(p)/radicalbig
plog(np)/parenrightig
(k−1)!/vextenddouble/vextenddouble/vextenddouble/vextenddoubleθ−θ′
α/vextenddouble/vextenddouble/vextenddouble/vextenddoublek
k
50=C1/vextenddouble/vextenddouble/vextenddouble/vextenddoubleθ−θ′
α/vextenddouble/vextenddouble/vextenddouble/vextenddoublek
kb(p)log(np) +σ(p)/radicalbig
plog(np)
kp.
Consequently, with probability greater than 1 −(np)−c1we have:
S(1)≤K/summationdisplay
k=11
p1
k!/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationdisplay
1≤i̸=j≤pYi,j/parenleftigg
(θi−θ′
i)k∂kli,j/parenleftbig
θ′/parenrightbig
∂θk
i+/parenleftbig
θj−θ′
j/parenrightbigk∂kli,j/parenleftbig
θ′/parenrightbig
∂θk
j/parenrightigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤C1K/summationdisplay
k=1/vextenddouble/vextenddouble/vextenddouble/vextenddoubleθ−θ′
α/vextenddouble/vextenddouble/vextenddouble/vextenddoublek
kb(p)log(np) +σ(p)/radicalbig
plog(np)
kp
≤C1b(p)log(np) +σ(p)/radicalbig
plog(np)
pK/summationdisplay
k=11
k/vextenddouble/vextenddouble/vextenddouble/vextenddoubleθ−θ′
α/vextenddouble/vextenddouble/vextenddouble/vextenddoublek
k,
holds uniformly for all θ∈B∞/parenleftbig
θ′,α0/parenrightbig
. Note that for any x= (x1,···,xp)⊤s.t.∥x∥∞≤a <1 for
some constant a>0, we have
(A.40)K/summationdisplay
k=11
k∥x∥k
k=K/summationdisplay
k=1p/summationdisplay
i=11
kxk
i=p/summationdisplay
i=1xiK/summationdisplay
k=1xk−1
i
k≤∥x∥1K/summationdisplay
k=1ak−1
k≤∥x∥1/parenleftbigg
−log(1−a)
a/parenrightbigg
.
Here in last step we have used −log(1−a) =/summationtext∞
k=1ak/k. With the fact that/vextenddouble/vextenddouble/vextenddoubleθ−θ′
α/vextenddouble/vextenddouble/vextenddouble
∞<α 0/α< 1/2
holds for any θ∈B∞/parenleftbig
θ′,α0/parenrightbig
, we have, with probability greater than 1 −(np)−c1,
S(1)≤ −log(1/2)
1/2C1b(p)log(np) +σ(p)/radicalbig
plog(np)
p/vextenddouble/vextenddoubleθ−θ′/vextenddouble/vextenddouble
1
≤2C1b(p)log(np) +σ(p)/radicalbig
plog(np)
p/vextenddouble/vextenddoubleθ−θ′/vextenddouble/vextenddouble
1.
Next we derive an upper bound for S(2). Define a series of random p×pmatrices{Ys
k:k=
2,...,K,s = 1,···,k−1}with the (i,j)-th elements of Ys
kgiven as
(Ys
k)i,j=Yi,jαk∂kli,j/parenleftbig
θ′/parenrightbig
∂θs
i∂θk−s
j,1≤i̸=j≤p.
Further for any k= 2,...,K , define a random ( k−1)p×(k−1)pmatrix Wkas:
Wk=
0 Y1
k
Y2
k
...
Yk−2
k
Yk−1
k0
.
Also we define a series of p×1 vectors,{z(s)
k:k= 2,...,K,s = 1,···,k−1}withi-th element of z(s)
k
given as:/parenleftig
z(s)
k/parenrightig
i= (0.5α)−s(θi−θ′
i)s/radicalig/parenleftbigk
s/parenrightbig
.
For anyk= 2,...,K , by denoting ˜zkas:
˜zk=
z(1)
k...
z(k−1)
k
,
51we have:
1
p1
k!/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationdisplay
1≤i̸=j≤p/parenleftigg
Yi,jk−1/summationdisplay
s=1/parenleftbigk
s/parenrightbig
(θi−θ′
i)s/parenleftbig
θj−θ′
j/parenrightbigk−s∂kli,j/parenleftbig
θ′/parenrightbig
∂θs
i∂θk−s
j/parenrightigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle(A.41)
=1
p1
2kk!/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglek−1/summationdisplay
s=1/parenleftig
˜z(k−s)
k/parenrightig⊤
Ys
k˜z(s)
k/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
=1
p1
2kk!/vextendsingle/vextendsingle˜z⊤
kWk˜zk/vextendsingle/vextendsingle
≤1
p1
2kk!∥˜zk∥2
2∥Wk∥2.
We remark that by formulating the confounding terms in S(2)via{z(s)
k,Wk}, we have established in
(A.41) an upper bound that depends on the parameters in {z(s)
k,k= 2,...,K}and the random matrices
{Wk,2,...,K}separately. Using the fact that/summationtext∞
l=0/parenleftbigl+s
s/parenrightbig
0.5l+s+1= 1, we have
K/summationdisplay
k=s+1/parenleftbigk
s/parenrightbig
0.5k<∞/summationdisplay
l=0/parenleftbigl+s
s/parenrightbig
0.5l+s= 2.
Consequently, there exists a big enough constant C2>0 such that, for all θ∈B∞/parenleftbig
θ′,α0/parenrightbig
, we have
K/summationdisplay
k=20.5k
k∥˜zk∥2
2=K/summationdisplay
k=20.5k
kp/summationdisplay
i=1k−1/summationdisplay
s=1(0.5α)−2s(θi−θ′
i)2s/parenleftbigk
s/parenrightbig
(A.42)
=K/summationdisplay
k=20.5k
kk−1/summationdisplay
s=1/parenleftbigk
s/parenrightbig
(0.5α)−2s∥θ−θ′∥2s
2s
≤K−1/summationdisplay
s=11
s/vextenddouble/vextenddouble/vextenddouble/vextenddoubleθ−θ′
α/2/vextenddouble/vextenddouble/vextenddouble/vextenddouble2s
2s/parenleftiggK/summationdisplay
k=s+1/parenleftbigk
s/parenrightbig
0.5k/parenrightigg
<2K−1/summationdisplay
s=11
s/vextenddouble/vextenddouble/vextenddouble/vextenddoubleθ−θ′
α/2/vextenddouble/vextenddouble/vextenddouble/vextenddouble2s
2s
<2K−1/summationdisplay
s=11
s/vextenddouble/vextenddouble/vextenddouble/vextenddoubleθ−θ′
α/2/vextenddouble/vextenddouble/vextenddouble/vextenddoubles
s
≤C2/vextenddouble/vextenddoubleθ−θ′/vextenddouble/vextenddouble
1.
Here the last step follows from (A.40). With similar arguments as in the proof of the matrix Bernstein
inequality (c.f. Lemma 3), we can show that uniformly for all k≤K=⌊c0log(np)⌋+ 1, there exist big
enough constants C3>0 andc2>0, such that with probability greater than 1 −(np)c2,
(A.43) ∥Wk∥2≤C3(k−1)!/parenleftig
b(p)log(np) +σ(p)/radicalbig
plog(np)/parenrightig
.
For brevity, the proof of inequality (A.43) is provided independently in Section (A.16). Consequently,
combining (A.41), (A.42) and (A.43) and K=⌊c0log(np)⌋+1, we conclude that with probability greater
than 1−(np)−c2,
S(2)≤K/summationdisplay
k=21
p1
k!/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationdisplay
1≤i̸=j≤p/parenleftigg
Yi,jk−1/summationdisplay
s=1/parenleftbigk
s/parenrightbig
(θi−θ′
i)s/parenleftbig
θj−θ′
j/parenrightbigk−s∂kli,j/parenleftbig
θ′/parenrightbig
∂θk
i/parenrightigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
52≤K/summationdisplay
k=21
p1
2kk!∥˜zk∥2
2∥Wk∥2
≤C3b(p)log(np) +σ(p)/radicalbig
plog(np)
pK/summationdisplay
k=21
k2k∥˜zk∥2
2
≤C2C3b(p)log(np) +σ(p)/radicalbig
plog(np)
p/vextenddouble/vextenddoubleθ−θ′/vextenddouble/vextenddouble
1,
uniformly for all θ∈B∞/parenleftbig
θ′,α0/parenrightbig
. Finally, we derive an upper bound for S(3). By condition (L-A1),
whenc0is chosen to be big enough, there exists a big enough constant c3>1, such that, uniformly for
allθ∈B∞(θ′,α0) and θξ, we have
S(3)
=1
p/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
(K+ 1)!/summationdisplay
1≤i̸=j≤p
Yi,jK+1/summationdisplay
s=0/parenleftbigK+1
s/parenrightbig
(θi−θ′
i)s/parenleftbig
θj−θ′
j/parenrightbigK+1−s∂K+1li,j/parenleftig
θξ/parenrightig
∂θs
i∂θK+1−s
j
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤1
p1
K+ 1/summationdisplay
1≤i̸=j≤p|Yi,j|/parenleftigg
|θi−θ′
i|
α+/vextendsingle/vextendsingleθj−θ′
j/vextendsingle/vextendsingle
α/parenrightiggK+1
≤pb(p)
K+ 1/parenleftigg
2/vextenddouble/vextenddoubleθ−θ′/vextenddouble/vextenddouble
∞
α/parenrightiggK+1
≤pb(p)
K+ 1/parenleftbigg2α0
α/parenrightbiggc0log(np)
≤b(p)(np)−c3.
Here the last step will hold if we choose c0to be large enough such that (2 α0/α)c0/(c3+1)<e−1. When
np→∞ , andc0,c3are chosen to be large enough, this bound will be dominated by the upper bounds
forS(1)andS(2).
Combining the upper bound on S(1),S(2)andS(3), we conclude that, for any given θ′, there exist
large enough constants C > 0,c >0 which are independent of θ′such that for any θ∈B∞/parenleftbig
θ′,α0/parenrightbig
whereα0∈(0,α/2) at the same time with probability greater than 1 −(np)−c,
/vextendsingle/vextendsingleL(θ)−L/parenleftbig
θ′/parenrightbig/vextendsingle/vextendsingle≤Cb(p)log(np) +σ(p)/radicalbig
plog(np)
p/vextenddouble/vextenddoubleθ−θ′/vextenddouble/vextenddouble
1.
A.16 Proof of inequality (A.43)
Proof. For anyk= 2,...,K and 1≤i̸=j≤p, letWk,i,jbe defined by keeping the ( i,j)th element
of all the Ys
k,s= 1,···,k−1 inWkunchanged, and setting all other elements to be zero. Then the
random matrices Wk,i,j, 1≤i̸=j≤pare independent, and
/summationdisplay
1≤i̸=j≤pWk,i,j=Wk.
For any a∈R(k−1)p, we can write it as
a=
a(1)
...
a(k−1)
,
53witha(s)= (a(s)
1,...,a(s)
p)⊤∈Rp,r= 1,2,···,k−1. Then for any k= 2,...,K and 1≤i̸=j≤p, we
have,
∥Wk,i,j∥2= sup
∥a∥2=1∥Wk,i,ja∥2
= sup
∥a∥2=1
k−1/summationdisplay
s=1/parenleftigg
αkYi,j∂kli,j/parenleftbig
θ′/parenrightbig
∂θs
i∂θk−s
ja(k−s)
j/parenrightigg2
1/2
≤max
1≤s≤k/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleαkYi,j∂kli,j/parenleftbig
θ′/parenrightbig
∂θs
i∂θk−s
j/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglesup
∥a∥2=1/parenleftiggk−1/summationdisplay
s=1/parenleftig
a(k−s)
j/parenrightig2/parenrightigg1/2
≤(k−1)!b(p).
On the other hand,
max

/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/summationdisplay
1≤i̸=j≤pE/parenleftbig
Wk,i,jW⊤
k,i,j/parenrightbig/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2,/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/summationdisplay
1≤i̸=j≤pE/parenleftbig
W⊤
k,i,jWk,i,j/parenrightbig/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2


= max/braceleftigg
sup
∥a∥2=1/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglea⊤
/summationdisplay
1≤i̸=j≤pE/parenleftbig
Wk,i,jW⊤
k,i,j/parenrightbig
a/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle,
sup
∥a∥2=1/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglea⊤
/summationdisplay
1≤i̸=j≤pE/parenleftbig
W⊤
k,i,jWk,i,j/parenrightbig
a/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/bracerightigg
= max/braceleftigg
sup
∥a∥2=1/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationdisplay
1≤i̸=j≤pk−1/summationdisplay
s=1
Var (Yi,j)α2k/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle∂kli,j/parenleftbig
θ′/parenrightbig
∂θs
i∂θk−s
j/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2/parenleftig
a(k−s)
j/parenrightig2
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle,
sup
∥a∥2=1/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationdisplay
1≤i̸=j≤pk−1/summationdisplay
s=1
Var (Yi,j)α2k/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle∂kli,j/parenleftbig
θ′/parenrightbig
∂θs
i∂θk−s
j/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2/parenleftig
a(s)
i/parenrightig2
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/bracerightigg
≤max
i,j,s
Var (Yi,j)α2k/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle∂kli,j/parenleftbig
θ′/parenrightbig
∂θs
i∂θk−s
j/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
sup
∥a∥2=1/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationdisplay
1≤i̸=j≤pk−1/summationdisplay
s=1/parenleftig
a(k−s)
i/parenrightig2
+/parenleftig
a(s)
j/parenrightig2/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤2p((k−1)!)2σ2
(p).
Using the general Matrix Bernstein inequality (c.f. Theorem 6.17 and equation (6.43) of [33]), we have,
P(∥Wk∥2>ϵ) =P
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/summationdisplay
1≤i<j≤pWk,i,j/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2>ϵ

≤2pexp/parenleftigg
−ϵ2
(p−1) ((k−1)!)2σ2
(p)+ 2 (k−1)!b(p)ϵ/parenrightigg
.
Consequently, there exist big enough constants C2>0 andc2>0 s.t. by choosing
ϵ=C2(k−1)!/parenleftig
b(p)log(np) +σ(p)/radicalbig
plog(np)/parenrightig
,
we have, with probability greater than 1 −(np)c2,
∥Wk∥2≤C2(k−1)!/parenleftig
b(p)log(np) +σ(p)/radicalbig
plog(np)/parenrightig
,
holds for all k≤K=c0log(np) wherec0>0 is big enough constant.
54A.17 Proof of Theorem 6
Proof. For any vector x∈Rp, we define x−i:= (x1,···,xi−1,xi+1,···,xp)⊤. With similar arguments
as in the proof of Theorem 5, we have that there exists a θξsuch that
Li(θ)−Li/parenleftbig
θ′/parenrightbig
=1
p∞/summationdisplay
k=1
1
k!p/summationdisplay
j=1,j̸=i/parenleftigg
Yi,jk/summationdisplay
s=0/parenleftbigk
s/parenrightbig
(θi−θ′
i)s/parenleftbig
θj−θ′
j/parenrightbigk−s∂kli,j/parenleftbig
θ′/parenrightbig
∂θs
i∂θk−s
j/parenrightigg

≤/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleK/summationdisplay
k=11
p1
k!p/summationdisplay
j=1,j̸=iYi,j/parenleftig
(θi−θ′
i)k+/parenleftbig
θj−θ′
j/parenrightbigk/parenrightig∂kli,j/parenleftbig
θ′/parenrightbig
∂θk
i/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
+/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleK/summationdisplay
k=21
p1
k!p/summationdisplay
j=1,j̸=i/parenleftigg
Yi,jk−1/summationdisplay
s=1/parenleftbigk
s/parenrightbig
(θi−θ′
i)s/parenleftbig
θj−θ′
j/parenrightbigk−s∂kli,j/parenleftbig
θ′/parenrightbig
∂θs
i∂θk−s
j/parenrightigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
+1
p/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
(K+ 1)!p/summationdisplay
j=1,j̸=i
Yi,jK+1/summationdisplay
s=0/parenleftbigK+1
s/parenrightbig
(θi−θ′
i)s/parenleftbig
θj−θ′
j/parenrightbigK+1−s∂K+1li,j/parenleftig
θξ/parenrightig
∂θs
i∂θK+1−s
j
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
=S(1)
i+S(2)
i+S(3)
i,
whereK=⌊c0log(np)⌋+ 1 for some large enough constant c0. First consider S(1)
i. There exist big
enough constants C1>0,c1>0 such that, uniformly for all i= 1,···,p,k= 1,2,···,Kand all
θ∈B∞/parenleftbig
θ′,α0/parenrightbig
, we have, with probability greater than 1 −(np)−c1,
1
p1
k!/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglep/summationdisplay
j=1,j̸=iYi,j/parenleftig
(θi−θ′
i)k+/parenleftbig
θj−θ′
j/parenrightbigk/parenrightig∂kli,j/parenleftbig
θ′/parenrightbig
∂θk
i/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
=1
p1
k!/vextendsingle/vextendsingle/vextendsingle/vextendsingleθi
α−θ′
i
α/vextendsingle/vextendsingle/vextendsingle/vextendsinglek/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglep/summationdisplay
j̸=i,j=1Yi,j∂kli,j/parenleftbig
θ′/parenrightbig
∂θk
iαk/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
+1
p1
k!p/summationdisplay
j̸=i,j=1/vextendsingle/vextendsingle/vextendsingle/vextendsingleθj
α−θ′
j
α/vextendsingle/vextendsingle/vextendsingle/vextendsinglek/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleYi,j∂kli,j/parenleftbig
θ′/parenrightbig
∂θk
iαk/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤1
p1
k!/vextendsingle/vextendsingle/vextendsingle/vextendsingleθi−θ′
i
α/vextendsingle/vextendsingle/vextendsingle/vextendsinglek
max
i/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglep/summationdisplay
j̸=i,j=1Yi,j∂kli,j/parenleftbig
θ′/parenrightbig
∂θk
iαk/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
+1
p1
k!/vextenddouble/vextenddouble/vextenddouble/vextenddoubleθ−i−θ′
−i
α/vextenddouble/vextenddouble/vextenddouble/vextenddoublek
kmax
i,j/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleYi,j∂kli,j/parenleftbig
θ′/parenrightbig
∂θk
iαk/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤C1/parenleftigg/vextendsingle/vextendsingle/vextendsingle/vextendsingleθi−θ′
i
α/vextendsingle/vextendsingle/vextendsingle/vextendsinglekb(p)log(np) +σ(p)/radicalbig
plog(np)
kp+/vextenddouble/vextenddouble/vextenddouble/vextenddoubleθ−i−θ′
−i
α/vextenddouble/vextenddouble/vextenddouble/vextenddoublek
kb(p)
kp/parenrightigg
.
Then, from inequality (A.40) we have, there exist big enough constants C2>0 andc2>0, such that
with probability greater than 1 −(np)−c2,
S(1)
i≤K/summationdisplay
k=11
p1
k!/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglep/summationdisplay
j=1,j̸=iYi,j/parenleftig
(θi−θ′
i)k+/parenleftbig
θj−θ′
j/parenrightbigk/parenrightig∂kli,j/parenleftbig
θ′/parenrightbig
∂θk
i/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤C1K/summationdisplay
k=1/parenleftigg/vextendsingle/vextendsingle/vextendsingle/vextendsingleθi−θ′
i
α/vextendsingle/vextendsingle/vextendsingle/vextendsinglekb(p)log(np) +σ(p)/radicalbig
plog(np)
kp+/vextenddouble/vextenddouble/vextenddouble/vextenddoubleθ−i−θ′
−i
α/vextenddouble/vextenddouble/vextenddouble/vextenddoublek
kb(p)
kp/parenrightigg
55≤C1b(p)log(np) +σ(p)/radicalbig
plog(np)
p/parenleftiggK/summationdisplay
k=11
k/vextendsingle/vextendsingle/vextendsingle/vextendsingleθi−θ′
i
α/vextendsingle/vextendsingle/vextendsingle/vextendsinglek/parenrightigg
+C1b(p)
pK/summationdisplay
k=11
k/vextenddouble/vextenddouble/vextenddouble/vextenddoubleθ−i−θ′
−i
α/vextenddouble/vextenddouble/vextenddouble/vextenddoublek
k
≤C2b(p)log(np) +σ(p)/radicalbig
plog(np)
p|θi−θ′
i|+C2b(p)
p/vextenddouble/vextenddoubleθ−i−θ′
−i/vextenddouble/vextenddouble
1,
holds uniformly for i= 1,···,pandθ∈B∞/parenleftbig
θ′,α0/parenrightbig
.
Next we derive an upper bound for S(2)
i. With the fact that Y2
i,j≤b2
(p)and
Var/parenleftbig
Y2
i,j/parenrightbig
≤E/parenleftbig
Y4
i,j/parenrightbig
≤b2
(p)E/parenleftbig
Y2
i,j/parenrightbig
=b2
(p)σ2
(p),
by Lemma 6 we have, there exist big enough constants C3,c3>0 such that with probability greater
than 1−(np)−c3,
max
i
p/summationdisplay
j=1,j̸=iY2
i,j
1/2
≤C3/parenleftig
b2
(p)log(np) +σ(p)b(p)/radicalbig
plog(np)/parenrightig1/2
≤C3/parenleftig
(plog(np))1/4/radicalig
σ(p)b(p)+b(p)/radicalbig
log(np)/parenrightig
.
Consequently, there exists a big enough constant C4>0 such that, uniformly for all i= 1,···,pand
θ∈B∞/parenleftbig
θ′,α0/parenrightbig
, we have, with probability greater than 1 −(np)−c3,
S(2)
i≤K/summationdisplay
k=21
p1
k!p/summationdisplay
j=1,j̸=i/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/parenleftigg
Yi,jk−1/summationdisplay
s=1/parenleftbigk
s/parenrightbig
(θi−θ′
i)s/parenleftbig
θj−θ′
j/parenrightbigk−s∂kli,j/parenleftbig
θ′/parenrightbig
∂θs
i∂θk−s
j/parenrightigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤K/summationdisplay
k=21
p1
k!0.5k
p/summationdisplay
j=1,j̸=iY2
i,jk−1/summationdisplay
s=1/parenleftbiggθi−θ′
i
α/2/parenrightbigg2s
1/2

p/summationdisplay
j=1,j̸=ik−1/summationdisplay
s=1/parenleftigg
/parenleftbigk
s/parenrightbig/parenleftbiggθj−θ′
j
α/2/parenrightbiggk−s
αk∂kli,j/parenleftbig
θ′/parenrightbig
∂θs
i∂θk−s
j/parenrightigg2
1/2
≤K/summationdisplay
k=21
p1
k!0.5k/parenleftigg
|θi−θ′
i|2
α2/4−|θi−θ′
i|2/parenrightigg1/2
max
i
p/summationdisplay
j=1,j̸=iY2
i,j
1/2

k−1/summationdisplay
s=1/parenleftbigk
s/parenrightbig2p/summationdisplay
j=1,j̸=i/parenleftbiggθj−θ′
j
α/2/parenrightbigg2(k−s)
1/2
max
j,s,k/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleαk∂kli,j/parenleftbig
θ′/parenrightbig
∂θs
i∂θk−s
j/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤C4|θi−θ′
i|(plog(np))1/4/radicalbigσ(p)b(p)+b(p)/radicalbig
log(np)
p
K/summationdisplay
k=20.5k
k
k−1/summationdisplay
s=1/parenleftbigk
s/parenrightbig2p/summationdisplay
j=1,j̸=i/parenleftbiggθj−θ′
j
α/2/parenrightbigg2(k−s)
1/2
≤C4|θi−θ′
i|(plog(np))1/4/radicalbigσ(p)b(p)+b(p)/radicalbig
log(np)
p
×K/summationdisplay
k=2k−1/summationdisplay
s=1p/summationdisplay
j=1,j̸=i0.5k
s/parenleftbigk
s/parenrightbig/vextendsingle/vextendsingle/vextendsingle/vextendsingleθj−θ′
j
α/2/vextendsingle/vextendsingle/vextendsingle/vextendsinglek−s
56≤C4|θi−θ′
i|(plog(np))1/4/radicalbigσ(p)b(p)+b(p)/radicalbig
log(np)
p
×K−1/summationdisplay
s=1/parenleftigg
1
s/vextenddouble/vextenddouble/vextenddouble/vextenddoubleθ−i−θ′
−i
α/2/vextenddouble/vextenddouble/vextenddouble/vextenddoubles
sK/summationdisplay
k=s+1/parenleftbigk
s/parenrightbig
0.5k/parenrightigg
≤2C4|θi−θ′
i|(plog(np))1/4/radicalbigσ(p)b(p)+b(p)/radicalbig
log(np)
pK−1/summationdisplay
s=11
s/vextenddouble/vextenddouble/vextenddouble/vextenddoubleθ−i−θ′
−i
α/2/vextenddouble/vextenddouble/vextenddouble/vextenddoubles
s
≤4C4/vextenddouble/vextenddoubleθ−i−θ′
−i/vextenddouble/vextenddouble
1|θi−θ′
i|(plog(np))1/4/radicalbigσ(p)b(p)+b(p)/radicalbig
log(np)
p.
Here in the above inequalities we have used that fact that/summationtextK
k=s+1/parenleftbigk
s/parenrightbig
0.5k<2, and the last step follows
from the inequality (A.40).
Finally, we derive an upper bound for S(3)
i. By condition (L-A1), by choosing K=⌊c0log(np)⌋+ 1
withc0to be a large enough constant, there exists a big enough constant c4>0 such that, uniformly
for alli= 1,···,p,θξandθ∈B∞/parenleftbig
θ′,α0/parenrightbig
, we have
S(3)
i =1
p/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
(K+ 1)!p/summationdisplay
j=1,j̸=i
Yi,jK+1/summationdisplay
s=0/parenleftbigK+1
s/parenrightbig
(θi−θ′
i)s/parenleftbig
θj−θ′
j/parenrightbigK+1−s∂K+1li,j/parenleftig
θξ/parenrightig
∂θs
i∂θK+1−s
j
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤1
p1
K+ 1p/summationdisplay
j=1,j̸=i|Yi,j|/parenleftigg
|θi−θ′
i|
α+/vextendsingle/vextendsingleθj−θ′
j/vextendsingle/vextendsingle
α/parenrightiggK+1
≤b(p)
K+ 1/parenleftigg
2/vextenddouble/vextenddoubleθ−θ′/vextenddouble/vextenddouble
∞
α/parenrightiggK+1
≤b(p)
K+ 1/parenleftbigg2α0
α/parenrightbiggK+1
≤b(p)(np)−c4.
Here the last step will hold if we choose c0to be large enough such that (2 α0/α)c0/c3< e−1. When
np→∞ , andc0,c3are chosen to be large enough, this bound will be dominated by the upper bounds
forS(1)andS(2).
Consequently, by choosing K=⌊c0log(np)⌋+ 1 withc0to be a large enough constant, we conclude
that, for any given θ′, there exist large enough constants C,c > 0 such that uniform for any θ∈
B∞/parenleftbig
θ′,α0/parenrightbig
and 1≤i≤p, with probability greater than 1 −(np)−c,
/vextendsingle/vextendsingleLi(θ)−Li/parenleftbig
θ′/parenrightbig/vextendsingle/vextendsingle
≤C2b(p)log(np) +σ(p)/radicalbig
plog(np)
p|θi−θ′
i|+C2b(p)
p/vextenddouble/vextenddoubleθ−i−θ′
−i/vextenddouble/vextenddouble
1
+C4/vextenddouble/vextenddoubleθ−i−θ′
−i/vextenddouble/vextenddouble
1|θi−θ′
i|(plog(np))1/4/radicalbigσ(p)b(p)+b(p)/radicalbig
log(np)
p
≤Cb(p)
p/vextenddouble/vextenddoubleθ−i−θ′
−i/vextenddouble/vextenddouble
1+C/parenleftbig/vextenddouble/vextenddoubleθ−i−θ′
−i/vextenddouble/vextenddouble
1+ 1/parenrightbig
|θi−θ′
i|b(p)log(np) +σ(p)/radicalbig
plog(np)
p.
57B Additional numerical results
B.1 Informal justification of the use of TWHM in analyzing the insecta-ant-
colony4 dataset
To motivate the use of the proposed TWHM for the analysis of the insecta-ant-colony4 dataset, we
plot the autocorrelation function (ACF) and the partial autocorrelation function (PACF) of the degree
sequences of two ants. These ants are selected based on their respective highest and lowest values at
timet= 1 according to
n/summationdisplay
t=1/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglep−1
2−p/summationdisplay
j=1,j̸=iXt
i,j/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle.
Upon examining Figure 5, it becomes evident that both degree sequences exhibit patterns reminiscent
of a first-order autoregressive model with long memory. This observation serves as a strong motivation
for employing the TWHM methodology.
B.2 Community detection under stochastic block structures
We conduct additional numerical studies to assess the efficacy of community detection using the estimated
β-parameters. To implement stochastic block structures within the TWHM framework, we partitioned
thepnodes into kcommunities of equal size, ensuring that the parameters ( βi,0,βi,1) for all nodes i
within the same community were identical. Furthermore, we explored scenarios where the networks
were independently generated from a Stochastic Block Model (SBM). Specifically, we have considered
the following settings:
Setting 1: The networks are generated under TWHM with βi,r=−0.2,0,0.2 (r= 0,1) for all
nodesiin communities 1 ,2 and 3 respectively.
Setting 2: The networks are generated under TWHM with βi,r=−0.4,0,0.4 (r= 0,1) for all
nodesiin communities 1 ,2 and 3 respectively.
Setting 3: The networks are independently generated under SBM, with the probability matrix
among different communities specified as
/bracketleftigg0.26 0 .1 0 .1
0.1 0 .2 0 .1
0.1 0 .1 0.14/bracketrightigg
.
Setting 4: The networks are independently generated under SBM, with the probability matrix
among different communities given by
/bracketleftigg0.4 0 .3 0 .2
0.3 0.225 0 .15
0.2 0 .15 0 .1/bracketrightigg
.
Setting 5: The networks are independently generated under SBM, with the probability matrix
among different communities given by
/bracketleftigg0.9 0.5 0 .3
0.5 0.3 0 .2
0.3 0.2 0.15/bracketrightigg
.
580 5 10 15−0.20.00.20.40.60.81.0
LagACFACF of the degree sequence of the 26th ant
5 10 15−0.4−0.20.00.20.4
LagPartial ACFPACF of the degree sequence of the 26th ant
0 5 10 15−0.20.00.20.40.60.81.0
LagACFACF of the degree sequence of the 93rd ant
5 10 15−0.20.00.20.40.60.8
LagPartial ACFPACF of the degree sequence of the 93rd antFigure 5: The ACF and PACF plots of the degree sequences of two selected ants.
Networks in Settings 1-2 are generated with autoregressive dependence using our TWHM model, while
networks in Setting 3 are independent samples following the classical SBM structure. In Settings 4-5,
networks are also generated with SBM structures, but the probability formation matrices are not full-
rank.
Once theβ-parameters are estimated, we apply k-means clustering to these parameters to cluster the p
nodes, denoting this method as ”TWHM-Cluster” . For comparison, we apply spectral clustering, widely
used for SBM, on the averaged networks, denoting this method as ”SBM-Spectral”. All experiments are
repeated 100 times, and the clustering accuracy is reported in Table 7 below.
From Table 7, we observe that community detection using the β-parameters performs significantly better
under Settings 1 and 2, where data were generated from our TWHM model. This improvement is
attributed to the fact that parameter estimation has considered the autoregressive structure of the
networks.
When networks were independently generated from SBM under Setting 3, the performance of TWHM-
59Cluster is comparable to that of SBM-Cluster. However, when the probability matrix of SBM is not
full-rank (Settings 4 and 5), the TWHM model still demonstrates promising performance, while classical
spectral clustering can be much less satisfactory.
Table 7: Mean clustering accuracy of TWHM and SBM over 100 replications. Here k,n,p denote the
number of communities, the number of network observations, and the number of nodes, respectively.
(k,n,p ) TWHM-Cluster SBM-Spectral
Setting 1(3,2,300) 68.6% 37.1%
(3,10,300) 95.1% 37.0%
(3,50,300) 99.5% 38.7%
Setting 2(3,2,300) 92.2% 39.7%
(3,10,300) 95.6% 48.6%
(3,50,300) 100.0% 63.0%
Setting 3(3,10,300) 92.1% 99.8%
(3,30,300) 99.4% 100.0%
(3,50,300) 99.3% 100.0%
Setting 4(3,10,500) 97.0% 37.1%
(3,30,500) 93.9% 37.0%
(3,50,500) 100% 37.5%
Setting 5(3,10,500) 80.0% 71.2%
(3,30,500) 80.2% 70.7%
(3,50,500) 83.8% 72.8%
B.3 Dynamic protein-protein interaction networks
In this section, we applied the proposed TWHM to 12 dynamic protein-protein interaction networks
(PPIN) of yeast cells examined in [5]. Each dynamic network comprises 36 network observations. The
objective of investigating protein-protein interactions is to glean valuable insights into the cellular func-
tion and machinery of a proteome [34]. To provide an overview of these datasets, we present selected
statistics in Table 8. We have employed our method for linkage prediction on these PPINs. Similar to
Table 8: Some statistics of the 12 protein-protein interaction network datasets.
Dataset # of Nodes Mean degree Density
DPPIN-Uetz 922 4.68 0.22%
DPPIN-Ito 2856 6.05 0.07%
DPPIN-Ho 1548 54.55 0.13%
DPPIN-Gavin 2541 110.22 0.08%
DPPIN-Krogan (LCMS) 2211 77.01 0.09%
DPPIN-Krogan (MALDI) 2099 74.60 0.10%
DPPIN-Yu 1163 6.19 0.17%
DPPIN-Breitkreutz 869 90.33 0.23%
DPPIN-Babu 5003 44.56 0.04%
DPPIN-Lambert 697 19.09 0.29%
DPPIN-Tarassov 1053 9.17 0.19%
DPPIN-Hazbun 143 27.40 1.40%
the main paper, we utilized TWHM with either a fixed cutoff point of 0 .5 (TWHM 0.5) or the adaptive
rule (TWHM adaptive ). For comparison, we used a naive estimator Xt−1to predict Xt(Naive). Our
60training time slot size was set to n= 10, and the results are presented in Table 9 below. As evident
from the table, our approach shows significant promise for accurate link prediction.
Table 9: Comparison of TWHM 0.5, TWHM adaptive , and Naive in terms of misclassification rate of links
(×10−5).
Dataset TWHM 0.5TWHM adaptive Naive
DPPIN-Uetz 55 55 88
DPPIN-Ito 28 29 50
DPPIN-Ho 108 111 194
DPPIN-Gavin 139 142 250
DPPIN-Krogan(LCMS) 133 141 222
DPPIN-Krogan(MALDI) 124 127 215
DPPIN-Yu 54 55 86
DPPIN-Breitkreutz 297 305 493
DPPIN-Babu 27 28 45
DPPIN-Lambert 293 298 489
DPPIN-Tarassov 72 75 133
DPPIN-Hazbun 759 772 1198
61