Grounded learning for compositional vector semantics
Martha Lewis
January 17, 2024
Abstract
Categorical compositional distributional semantics is an approach to modelling
language that combines the success of vector-based models of meaning with the
compositional power of formal semantics. However, this approach was developed
without an eye to cognitive plausibility. Vector representations of concepts and con-
cept binding are also of interest in cognitive science, and have been proposed as a
way of representing concepts within a biologically plausible spiking neural network.
This work proposes a way for compositional distributional semantics to be imple-
mented within a spiking neural network architecture, with the potential to address
problems in concept binding, and give a small implementation. We also describe a
means of training word representations using labelled images.
1 Introduction
Vector representations of word meaning have proved extremely successful at modelling
language in recent years, both as static word embeddings [Mikolov et al., 2013, Pen-
nington et al., 2014] and as contextual embeddings which take surrounding words into
account [Devlin et al., 2019]. Vectors have also been used in cognitive science, both at
a fairly abstract level representing concepts via collections of features, and at a more
mechanistic level representing concepts via patterns of neural activation.
In all cases we have an interest in describing how words or concepts combine. In the
case of contextual word embeddings, composition is effected by the artificial neural net-
work architecture, and this works very well, although in an opaque manner and arguably
in a way that does not generalise effectively to other tasks [Talman and Chatzikyriakidis,
2019, Bernardy and Chatzikyriakidis, 2019] or which leverages specific characteristics of
the dataset [McCoy et al., 2019].
In the case of static word embeddings, compositional distributional semantics de-
scribes methods to both build vector representations of words and combine them to-
gether so that phrases and sentences can be represented as vectors. Mitchell and Lapata
[2010] describe some quite general approaches to composition, and give implementa-
tions focussed on pointwise, potentially weighted, combinations, such as vector addition
or pointwise multiplication. Grammatically informed neural approaches are given in
[Socher et al., 2013, Bowman et al., 2015] where artificial neural networks for compos-
ing word vectors are built that use the grammatical structure of a sentence. Finally
1arXiv:2401.06808v1  [cs.CL]  10 Jan 2024tensor-based approaches were proposed and developed in Coecke et al. [2010], Baroni
and Zamparelli [2010], Paperno et al. [2014]. In these approaches words are modelled in
different vector spaces depending on their grammatical type, and composition is given by
tensor contraction. This will be described in more detail in section 2.1. Compositional
distributional semantic approaches are in general used to model text only, although some
multi-modal approaches have been used. This leads to the question of whether we can
develop a means of learning compositional vector-based representations in a grounded
way.
On the cognitive side, we focus here on the idea of vectors as representing patterns
of neural activation. One means of considering how vectors combine in this context
is given by vector symbolic architectures (VSAs) [Smolensky, 1990, ?, Gayler, 2003].
VSAs represent symbols as vectors, and provide a means of binding symbols together,
grouping them, and unbinding them as needed. More detail is given in section 2.2.
VSAs have been posited as a way of modelling how symbols can be represented and
manipulated in a neural substrate. This has been implemented in e.g. Eliasmith et al.
[2012] and investigated and discussed in e.g. Hummel [2011], Martin and Doumas [2020].
In Martin and Doumas [2020] the argument is made that additive binding , i.e. combining
vectors via addition, is more faithful to how humans combine concepts than conjunctive
binding , i.e. using something like a tensor product. Since VSAs have been investigated
and implemented within more biologically realistic neural networks, the question arises
of whether we can use these methods in developing a grounded learning model for tensor-
based compositional semantics, all the more so since the model of Coecke et al. [2010]
was inspired by Smolensky’s model originally.
Aims
•To build a model for generating grounded representations within a compositional
distributional semantics
•To draw out links between Smolensky’s theory on one hand and compositional
distributional semantics on the other
•To develop a way in which compositional distributional models can be implemented
within biologically plausible neural network models and thereby investigate a wider
range of composition methods than e.g. vector addition or tensor binding.
2 Background
2.1 Compositional Distributional Semantics
Compositional distributional semantics was developed as a way of generating meanings
above the word level via the composition of individual word meanings. The genre of
model we concentrate on here can be termed tensor-based compositional distributional
models [Coecke et al., 2010, Baroni and Zamparelli, 2010, Paperno et al., 2014]. Words
2are modelled in different vector spaces according to their grammatical type, and com-
position is modelled as tensor contraction. Specifically:
•Nouns are modelled as vectors in a noun space N, sentences in a sentence space S.
•Adjectives are modelled as matrices on N, i.e. linear maps adj:N→Nor
elements of the space N⊗N.
•Intransitive verbs are modelled as matrices from NtoS, i.s. linear maps iv:N→S
or elements of the space N⊗S
•Transitive verbs are modelled as tensors in N⊗S⊗N, or bilinear maps tv:
N⊗N→S
So, for example, an adjective like red is modelled as a matrix redand applied to a noun
carby matrix multiplication, giving back a vector red car .
In the original formulation, vectors for words were presumed to be inferred from
large text corpora, and so far there have been limited proposals for how to ground
these representations using images or other forms of input. Compositional distributional
semantics has also been developed with limited consideration of cognitive or neural
plausibility. In contrast, vector symbolic architectures, discussed in the following section,
have been considered by some cognitive scientists as offering a good basis for modelling
language and concept combination.
2.2 Vector-symbolic architectures
Prior to tensor-based distributional semantics, there has been a large amount of research
into vector symbolic architectures (VSAs), specifically, how symbolic structures can be
encoded into vector-based representations. These include Smolensky [1990], Plate [1994],
Gayler [2003] amongst others.
Smolensky [1990] proposes that structures like sentences are modelled as a sum of
role-filler bindings. Suppose we have a set of roles {agent ,patient ,verb}and a set of
fillers {Junpa ,Jen,loves}. Symbolically the binding of a role to a filler is represented by
/, and the sentence Junpa loves Jen can be represented as a set of role filler bindings
{Junpa /agent ,Jen/patient ,loves/verb}. In Smolensky [1990] these are mapped over to a
vector space model by mapping each role and each filler to a vector, mapping the binding
to tensor product, and mapping the collection of the role-filler bindings as their sum:
Junpa loves Jen 7→ {Junpa /agent ,Jen/patient ,loves/verb} (1)
7→Junpa ⊗agent +Jen⊗patient +loves⊗verb (2)
More generally, a sentence sconsisting of a set of role-filler bindings {ri/fi}iis
realized as:
s=X
iri⊗fi (3)
3Questions can be asked of a given statement via an unbinding mechanism . We may
want to extract individual elements of a given sentence. This is done using unbinding
vectors , defined as vectors dual to the role vectors. Each role vector rihas an unbinding
vector uisuch that ⟨ri,ui⟩= 1. Note that if the role vectors are an orthonormal set, each
role vector is its own unbinding vector. To unbind a particular role from a sentence, we
take the partial inner product of the unbinding vector with the sentence representation.
Suppose that
s=Junpa ⊗agent +Jen⊗patient +loves⊗verb (4)
andagent ,patient ,verb form an orthonormal set. If we want to know who the agent
is ins, we take the partial inner product of sandagent giving:
s·agent = (Junpa ⊗agent +Jen⊗patient +loves⊗verb )·agent
=Junpa ⊗agent ·agent +Jen⊗patient ·agent +loves⊗verb·agent
=Junpa
VSAs have been posited as a potential means for representing symbolic thought in a
neural substrate [Hummel, 2011, Doumas and Hummel, 2012, Calmus et al., 2020, Martin
and Doumas, 2020]. Holographic reduced representations [ ?] have similar properties to
Smolensky’s theory but without the drawback of needing the increased space for bound
representations. Eliasmith [2013] have shown how HRRs can be implemented within a
spiking neural network model which is designed to be biologically plausible.
Whilst VSAs such as Smolensky’s and Plate’s have the benefits outlined above, of
being composable and potentially biologically plausible, they have been argued to have
some drawbacks in representing how words combine, or how concepts compose. Hummel
[2011], Doumas and Hummel [2012], Martin and Doumas [2020] make the following
observation. Assuming that similarity is measured by cosine similarity, that is, the
cosine of the angle between two vectors, then the similarity of two role-filler bindings is
dependent only on the similarity of the pair of roles and the pair of fillers.
Martin and Doumas [2020] set up an experiment to investigate whether conjunctive
binding (via tensor product or circular convolution) or additive binding (via vector ad-
dition) is a better predictor of human similarity judgements. They consider a role to
be a predicate, such as fluffy , which can be bound to a filler, such as cat. Then, the
representations for fluffy dog andfluffy cat are exactly as similar as the representations
fordogandcatare.
fluffy ⊗dog·fluffy ⊗cat=fluffy ·fluffy ⊗dog·cat (5)
= 1·dog·cat=dog·cat (6)
This is undesirable, as the predicate fluffy should make cats and dogs more similar
to each other - see Figure 1 for example.
4(a) Dog1
(b) Cat2
(c) Fluffy Dog3
(d) Fluffy Cat4
Figure 1: Fluffy dogs and fluffy cats are more similar than dogs and cats
Alternatively, roles might be bound to fillers using additive binding . In this case, we
do obtain the result that fluffy dogs and fluffy cats are more similar than dogs and cats.
This property also holds if we model fluffy as an adjective, dogandcatas nouns, and
combine the representations as in equation 3 above. Bearing in mind that we consider
adjandnoun to be orthogonal, and assume we have a similarity of around 0.5 for cats
1https://commons.wikimedia.org/wiki/File:Wayfield%27s_Young_Argos,_the_Labrador_
Retriever.jpg
Attribution: Andrew Skolnick, en:User:Askolnick, CC BY-SA 3.0 ¡ http://creativecommons.org/
licenses/by-sa/3.0/>,viaWikimediaCommons
2https://commons.wikimedia.org/wiki/File:Cat_November_2010-1a.jpg
Attribution: Alvesgaspar, CC BY-SA 3.0 https://creativecommons.org/licenses/by-sa/3.0 , via
Wikimedia Commons
3https://commons.wikimedia.org/wiki/File:Cute_dog_on_beach,_Los_Angeles,_California_
LCCN2013634674.tif
Attribution: Carol M. Highsmith, Public domain, via Wikimedia Commons
4https://commons.wikimedia.org/wiki/File:White_Persian_Cat.jpg
Attribution: Optional at the Persian language Wikipedia, GFDL http://www.gnu.org/copyleft/
fdl.html , via Wikimedia Commons
5and dogs, we have:
fluffy cat =1√
2(fluffy ⊗adj+cat⊗noun ) (7)
fluffy dog =1√
2(fluffy ⊗adj+dog⊗noun ) (8)
where the factor of1√
2normalises the length of the resulting vectors. Then:
sim(fluffy cat ,fluffy dog ) =1
2⟨fluffy ⊗adj+cat⊗noun ,fluffy ⊗adj+dog⊗noun⟩
(9)
=1
2(⟨fluffy ⊗adj,fluffy ⊗adj⟩+⟨fluffy ⊗adj,dog⊗noun⟩
(10)
+⟨cat⊗noun ,fluffy ⊗adj⟩+⟨cat⊗noun ,dog⊗noun⟩)
(11)
=1
2(⟨fluffy ⊗adj,fluffy ⊗adj⟩+⟨fluffy ,dog⟩⟨adj,noun⟩
(12)
+⟨fluffy ,cat⟩⟨adj,noun⟩+⟨cat⊗noun ,dog⊗noun⟩)
(13)
=1
2(1 + 0 + 0 + ⟨cat,dog⟩) = 0.75 (14)
so applying fluffy has boosted the similarity of cats and dogs.
Why we want more than additive binding Whilst, as can be seen in Martin
and Doumas [2020], additive binding works for many examples, there are arguably a
number of predicates for which we don’t want an increase in similarity to occur. Some
words are ambiguous, and we do not want to say that bright light andbright student
are more similar than light andstudent are. One argument might be that we should
disambiguate words and that these different senses will be represented by different vec-
tors. However, some ambiguities can be subtle: compare Nishi opened the book with
Nishi opened the jar .
Boleda [2020] argues that tensor-based compositional distributional models of mean-
ing can reflect polysemy, and this assertion is borne out by the experimental results of ?,
where a matrix-based method is compared with (amongst others) an additive model. A
hand-crafted example of this is given in Grefenstette et al. [2010] where they show how
to design a representation for catch so that the similarity of the phrases catch ball and
catch disease is not boosted by the word catch . Coecke and Lewis [2015] look at the pet
fish problem, and show how a representation for petcan be developed that sends fishto
goldfish but leaves dogandcatmostly unchanged.
The compositional distributional model is focussed on corpus-based semantics, and
was not developed with any eye to neural plausibility or to learning language in a
6grounded fashion. We would like to be able to combine the greater flexibility of tensor-
based compositional models with the neural plausibility of VSA-based architectures, and
with the possibility of learning meanings that are grounded in an input outside of text.
We provide a mapping from the compositional distributional model into a Smolensky-
based architecture. We show how meanings of words can be learnt in a way that is
grounded in inputs outside of text.
Relations to quantum models of concepts The compositional distributional se-
mantic model of meaning proposed in Coecke et al. [2010] has its roots in quantum
theory. There is a wide range of interest in quantum modelling of concepts, examin-
ing how concepts behave in composition, and their application in artificial intelligence.
Widdows et al. [2021] provides a thorough review of applications of quantum theory to
artificial intelligence, part of which is to do with the representation of concepts or words
by vectors, together with an analysis of the use of VSA-like architectures that we cover
here. In the area of concept composition, it is proposed that concept composition can
be well modelled as interference between two quantum states [Aerts, 2009, Aerts et al.,
2012]. Aspects such as emergent meaning and vagueness are addressed in Bruza et al.
[2012], Blutner et al. [2013], and reviews of the use of quantum theory in cognitive sci-
ence are given in Pothos and Busemeyer [2013] and Lewis [2021]. Further consideration
of whether these phenomena can be well modelled within neural networks is an area of
future work.
2.3 Semantic Pointers for Concept Representation
Whilst deep neural architectures have had huge success in recent years, they are bio-
logically implausible in the structure of the individual units, the overall architecture of
networks, and in the learning algorithm implemented. There has therefore been interest
in implementing more biologically plausible networks. One of these is Nengo [Eliasmith
et al., 2012]. Within this architecture, symbolic structure is implemented using the Se-
mantic Pointer Architecture (SPA) [Eliasmith et al., 2012, Blouw et al., 2016]. Semantic
pointers can be thought of as vectors that are instantiated by patterns of neurons firing
in a spiking neural network. Semantic pointers can be bound together using circular
convolution, and unbound using circular correlation [Plate, 1994]. A Smolensky-style
form of concept composition can readily be implemented - with the caveat that these
representations and binding and unbinding operations are noisy.
In what follows, we propose a way to view tensor-based compositional distributional
semantics within a Smolensky-style framework, and thereby propose a way for tensor-
based compositional distributional semantics to be implemented within a spiking neural
network architecture.
73 Compositional distributional semantics in the Nengo frame-
work
The Nengo framework uses the Semantic Pointer Architecture to represent concepts.
Semantic pointers can be thought of as noisy vectors encoded by the dynamics of a
spiking neural network. Semantic pointers are bound together using circular convolution,
and can be unbound using circular correlation [Plate, 1994]. A usual proposal for the
representation of concepts in this kind of framework is to consider features of a concept
as roles, the value of those features as fillers, and form the representation of a concept
as a sum of role-filler binding. However, this then leads to the question of how function
words like adjectives and verbs should be represented, and how they might be combined
with noun concepts.
3.1 A first proposal
Recall that a key aspect of vector symbolic architectures is the existence of a binding
operator and an unbinding operator. In Smolenksy’s ICS these are respectively tensor
product and inner product, and in Plate [1994] these are circular convolution and circular
correlation.
In compositional distributional semantics, we also make use of the inner product (or
more generally tensor contraction) as a composition operator, and function words such
as verbs and adjectives are tensors or matrices, i.e. weighted sums of tensor products of
basis vectors. We therefore have an immediate way of mapping to the semantic
pointer architecture needed for implementation in Nengo , by viewing inner
product as an unbinding operator and tensor product as a binding operator.
In a little more detail: given a matrix redand a vector car, we form the composition
red car via matrix multiplication. Writing this out explicitly, if we have a noun space
Nwith basis {ei}i, then
car=X
iciei, red=X
irijei⊗ej (15)
red car =X
ijrijei⟨ej,car⟩=X
ijkrijckei⟨ej,ek⟩=X
ijrijcjei (16)
i.e., we can think of this operation as unbinding the vector carfrom the adjective red.
Now, to move to the semantic pointer setting, we map tensor product to circular
convolution, and inner product to circular correlation. We view each basis vector as a
semantic pointer, and encode a noun as a weighted sum of semantic pointers, and an
adjective as a weighted sum of convolved pairs of semantic pointers:
car=X
icipi, red=X
irijpi⊛pj (17)
red car =X
ijrijpi(pj⊘car) =X
ijkrijckpi(pj⊘pk) =X
ijrijcjpi+ noise (18)
8The last step in the above relies on the semantic pointers pibeing approximately or-
thogonal, which they are by design.
A toy implementation of this is available at https://github.com/marthaflinderslewis/
nengo-disco . We use the ‘pet fish’ problem as an example. In the pet fish problem, we
want the adjective ‘pet’ to modify animals in certain ways. A ‘pet fish’ should modify
‘fish’ to make it similar to a goldfish, however, the representation of ‘cat’, and ‘dog’
should stay pretty similar: cats and dogs are already pretty archetypal pets. To im-
plement a model of the ‘pet fish’ problem, we take inspiration from Coecke and Lewis
[2015]. We choose some features to describe our animals: cared-for ,vicious ,fluffy ,scaly,
lives-house ,lives-sea ,lives-zoo . These are rendered as semantic pointers and we use the
following notation: cared-for :c,vicious :v,fluffy :f,scaly:s,lives-house :h,lives-sea :
e,lives-zoo :z. Each animal is rendered as a weighted sum of semantic pointers with
weights as in table 1. We interpret these weights as the importance of each feature to
the noun. Note that vectors are normalised.
Fish Goldfish Cat Dog Shark Lion
c0.13 0.44 0.57 0.67 0.00 0.19
v0.51 0.00 0.13 0.37 0.57 0.62
f0.00 0.00 0.57 0.37 0.00 0.44
s0.63 0.62 0.00 0.00 0.57 0.00
e0.51 0.00 0.00 0.00 0.57 0.00
h0.19 0.62 0.57 0.52 0.00 0.00
z0.19 0.19 0.00 0.00 0.11 0.62
Table 1: Weights for semantic pointer representations of nouns
We then design an adjective as the following sum of convolved semantic pointers:
pet=c⊛(c+v+f+s+e+h+z) +v⊛v+f⊛f+s⊛s+h⊛(h+e+z) (19)
which in matrix format looks as in table 2 We can interpret these weights as follows.
c v f s e h z
c1 1 1 1 1 1 1
v0 1 0 0 0 0 0
f0 0 1 0 0 0 0
s0 0 0 1 0 0 0
e0 0 0 0 0 0 0
h0 0 0 0 1 1 1
z0 0 0 0 0 0 0
Table 2: Weights for sum of convolved semantic pointers
The first row of the matrix is essentially saying that no matter what the features of
9the animal, after application of the petadjective, the animal should be cared for. The
next three rows are just identity. The rows corresponding to lives-sea andlives-zoo are
zero: after application of the petadjective, the pet animal should not have any weight
on these features. Lastly, we see that the row corresponding to lives-home moves weight
from other features to this feature.
In Nengo, the nouns and adjective are implemented as weighted sums of semantic
pointers or convolved semantic pointers, and the nouns and adjective are composed
using the unbinding mechanism: the nouns are unbound from the adjective. Each
adjective-noun combination is then queried against the nouns to retrieve the noun
that is most similar. We wish that ‘pet fish’ is most similar to ‘goldfish’, ‘pet cat’
is most similar to ‘cat’, and so on. A video of the system can be seen at https:
//github.com/marthaflinderslewis/nengo-disco .
The above goes to show that compositional distributional semantics can be imple-
mented within the semantic pointer architecture, but does not give any indication about
how features or weights could be learnt. In the following section, we give an alterna-
tive formulation which has the potential to provide a learning mechanism from labelled
stimuli.
3.2 Compositional distributional semantics as a role-filler model of
meaning
We now provide a slightly different perspective on compositional distributional semantics
within a semantic pointer architecture. As we described in section 2.2, equation (2), a
semantic representation in ICS consists of a sum of role-filler pairs:
s=X
iri⊗fi (20)
In order to map this representation to the compositional vector representation we con-
sider the following. In the case of a noun, we say that the roles riare a set of basis
vectors spanning the noun space, and then the fillers are simply scalars attached to each
role.
n=X
iniri (21)
We view an adjective as a set of fillers bound to roles where the roles are possible
nouns and the fillers are vectors corresponding to the adjective-noun combination:
adj=X
iani⊗ni (22)
We view intransitive verbs as a set of fillers bound to roles where the roles are possible
nouns and the fillers are the resulting sentences:
in-verb =X
ini⊗sent i (23)
10Transitive verbs are a set of fillers bound to roles where the roles are pairs of possible
nouns and the fillers are the resulting sentences:
tr-verb =X
ijni⊗sent ij⊗nj (24)
The composition of an adjective and a noun is then found by unbinding the noun
role from the adjective, and similarly for verb-noun composition. In the adjective-noun
example, unbinding is just matrix-vector multiplication. For a very toy example, suppose
we have some kind of vector representations of: red car =rc,red apple =ra,red wine =
rw,car=c,apple =a,wine =w
Then,
red=rc⊗c+ra⊗a+rw⊗w (25)
Computing red car asred·cwe obtain:
red·c=rc⟨c,c⟩+ra⟨a,c⟩+rw⟨w,c⟩ (26)
=rc+ noise (27)
assuming that cars are not very similar to apples or wine.
Recall that in equations (17) and (18) we argued that in order to implement a
distributional semantic model within the spiking neural architecture, we could map from
tensor product as binding operator and inner product as unbinding operator, to circular
convolution as binding operator and circular correlation as unbinding operator. We use
the same methodology to obtain a representation of nouns, adjectives, and verbs within
the semantic pointer architecture. For example, in the semantic pointer architecture,
red=rc⊛c+ra⊛a+rw⊛w (28)
and
red⊘c=rc(c⊘c) +ra(a⊘c) +rw(w⊘c) (29)
=rc+ more noise (30)
again, assuming that cars are not very similar to apples or wine.
This perspective on compositional distributional semantics as a role-filler binding
architecture also gives us a potential way of learning representations for words.
3.3 Learning strategy
We consider adjective-noun composition. Suppose we have a set of images labelled with
adjective-noun combinations, and assume that within our semantic pointer architecture
we have some kind of vision system that can produce semantic pointers for the images
themselves.
11Supervised learning situation We have a set of labelled inputs. Let’s assume they
are all of the form adj-noun. Suppose the system has a convolved semantic pointer
representation of each adjective it has learnt so far, call them {Ai}i, and a semantic
pointer representation of each noun it has learnt so far, call them {ni}i. Suppose an
input is labelled Ajnkand the system has both these words in its vocabulary. We assume
the system has a vector representation aniof each image.
If we assume that the adjective has the form adj=P
iani⊛nithen we get a simple
update rule for the adjective, by mixing the current adjective with the convolution of
anandn:
Aj7→(1−h)Aj+hani⊛nk (31)
where his some small value in [0 ,1]. In order to update the noun, we propose unbinding
theanfiller from Ato get the nrole, giving us
nk7→(1−h)nk+h(ani⊘Aj) (32)
In the cases where the system does not yet have a representation of the adjective, we
can initialise it as an⊗anand where there is no representation of the noun, it can be
initialised as an.
We can make a similar proposal for intransitive verbs. We assume that the verb
has the form verb =P
ini⊛nvi, where nvis a vectors representing an intransitive
sentence like ‘Junpa walks’. Given an input labelled njVkcorresponding to a vector
representation nvi, the verb Vk=P
ini⊛nviis then updated by:
Vk7→(1−h)Vk+hnj⊛nvi (33)
and the noun is updated by:
nj7→(1−h)nj+h(Vk⊘nvi) (34)
We have started to investigate the approach outlined above within a standard neural
network model to learn compositional word representations from labelled images [Lewis
et al., 2023], and future work will go on to extend this to the Nengo architecture.
4 Conclusions and future work
Compositional distributional semantics has a set of powerful machinery that can be used
for composition. However, it does not have any particular cognitive grounding. In this
paper, we have given a proposal for implementation of compositional distributional se-
mantics within the cognitive architecture Nengo. This architecture uses a biologically
(more) realistic substrate to represent concepts as semantic pointers, and is integrated
with decision making, vision, and other modules. This therefore has the potential to
provide compositional distributional semantics with an environment in which meanings
can be grounded. We have given a toy implementation to show that our proposal is pos-
sible, and presented an alternative formulation of the approach together with a strategy
for learning word representations.
12Future work in this area is of course to take forward possible implementations of
these ideas. We already have a strategy to begin implementation within the Nengo
framework. Once implementation within this kind of architecture has been carried out,
there is potential to examine what kind of representation best model human behaviour
- whether compositional distributional semantics is a useful representation. We gave
arguments in section 2.2 to argue that there is a need for more flexible composition than
additive binding, so there is potential here.
We would also like to implement these ideas within a dialogue setting. Whilst a
supervised learning setting has been described, the kinds of representation proposed are
very amenable to being learnt in a self-supervised fashion between two or more agents.
References
Diederik Aerts. Quantum structure in cognition. Journal of Mathematical Psychology ,
53(5):314–348, October 2009. ISSN 00222496. doi: 10.1016/j.jmp.2009.04.005. URL
https://linkinghub.elsevier.com/retrieve/pii/S0022249609000558 .
Diederik Aerts, Jan Broekaert, Liane Gabora, and Tomas Veloz. The Guppy Effect as
Interference. In Jerome R. Busemeyer, Fran¸ cois Dubois, Ariane Lambert-Mogiliansky,
and Massimo Melucci, editors, Quantum Interaction , Lecture Notes in Computer Sci-
ence, pages 36–47, Berlin, Heidelberg, 2012. Springer. ISBN 978-3-642-35659-9. doi:
10.1007/978-3-642-35659-9 4.
Marco Baroni and Roberto Zamparelli. Nouns are vectors, adjectives are matrices:
Representing adjective-noun constructions in semantic space. In Proceedings of the
2010 Conference on Empirical Methods in Natural Language Processing , EMNLP ’10,
pages 1183–1193, USA, October 2010. Association for Computational Linguistics.
Jean-Philippe Bernardy and Stergios Chatzikyriakidis. What Kind of Natural Lan-
guage Inference are NLP Systems Learning: Is this Enough? In Special Ses-
sion on Natural Language Processing in Artificial Intelligence , volume 2, pages
919–931. SCITEPRESS, February 2019. ISBN 978-989-758-350-6. doi: 10.5220/
0007683509190931. URL http://www.scitepress.org/Papers/2019/76835 .
Peter Blouw, Eugene Solodkin, Paul Thagard, and Chris Eliasmith. Concepts as Seman-
tic Pointers: A Framework and Computational Model. Cognitive Science , page 35,
2016.
Reinhard Blutner, Emmanuel M. Pothos, and Peter Bruza. A Quantum Probability
Perspective on Borderline Vagueness. Topics in Cognitive Science , pages n/a–n/a,
September 2013. ISSN 17568757. doi: 10.1111/tops.12041. URL http://doi.wiley.
com/10.1111/tops.12041 .
Gemma Boleda. Distributional Semantics and Linguistic Theory. arXiv:1905.01896 [cs] ,
March 2020. doi: 10.1146/annurev-linguistics-011619-030303. URL http://arxiv.
org/abs/1905.01896 .
13Samuel R. Bowman, Christopher Potts, and Christopher D. Manning. Recursive Neural
Networks Can Learn Logical Semantics. In Proceedings of the 3rd Workshop on Con-
tinuous Vector Space Models and Their Compositionality , pages 12–21, Beijing, China,
July 2015. Association for Computational Linguistics. doi: 10.18653/v1/W15-4002.
URL https://www.aclweb.org/anthology/W15-4002 .
P. D. Bruza, K. Kitto, B. Ramm, L. Sitbon, D. Song, and S. Blomberg. Quantum-like
non-separability of concept combinations, emergent associates and abduction. Logic
Journal of the IGPL , 20(2):445–457, April 2012. ISSN 1367-0751. doi: 10.1093/jigpal/
jzq049. URL https://doi.org/10.1093/jigpal/jzq049 .
Ryan Calmus, Benjamin Wilson, Yukiko Kikuchi, and Christopher I. Petkov. Structured
sequence processing and combinatorial binding: Neurobiologically and computation-
ally informed hypotheses. Philosophical Transactions of the Royal Society B: Biologi-
cal Sciences , 375(1791):20190304, February 2020. doi: 10.1098/rstb.2019.0304. URL
https://royalsocietypublishing.org/doi/full/10.1098/rstb.2019.0304 .
Bob Coecke and Martha Lewis. A Compositional Explanation of the Pet Fish Phe-
nomenon. arXiv:1509.06594 [cs, math] , September 2015. URL http://arxiv.org/
abs/1509.06594 .
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen Clark. Mathematical Foundations for
a Compositional Distributional Model of Meaning. arXiv:1003.4394 [cs, math] , March
2010. URL http://arxiv.org/abs/1003.4394 .
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-
training of Deep Bidirectional Transformers for Language Understanding. In Pro-
ceedings of the 2019 Conference of the North American Chapter of the Associa-
tion for Computational Linguistics: Human Language Technologies, Volume 1 (Long
and Short Papers) , pages 4171–4186, Minneapolis, Minnesota, June 2019. Associ-
ation for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https:
//aclanthology.org/N19-1423 .
Leonidas A. A. Doumas and John E. Hummel. Computational models of higher cogni-
tion. In The Oxford Handbook of Thinking and Reasoning , Oxford Library of Psychol-
ogy, pages 52–66. Oxford University Press, New York, NY, US, 2012. ISBN 978-0-19-
973468-9.
C. Eliasmith, T. C. Stewart, X. Choo, T. Bekolay, T. DeWolf, Y. Tang, and D. Ras-
mussen. A Large-Scale Model of the Functioning Brain. Science , 338(6111):1202–1205,
November 2012. ISSN 0036-8075, 1095-9203. doi: 10.1126/science.1225266. URL
https://www.sciencemag.org/lookup/doi/10.1126/science.1225266 .
Chris Eliasmith. How to Build a Brain: A Neural Architecture for Biolog-
ical Cognition . Oxford University Press, 2013. ISBN 978-0-19-934523-6.
URL https://oxford.universitypressscholarship.com/view/10.1093/acprof:
oso/9780199794546.001.0001/acprof-9780199794546 .
14Ross W Gayler. Vector Symbolic Architectures Answer Jackendoff’s Challenges for Cog-
nitive Neuroscience. In Joint International Conference on Cognitive Science , page 6,
July 2003. URL https://arxiv.org/abs/cs/0412059 .
Edward Grefenstette, Mehrnoosh Sadrzadeh, Stephen Clark, Bob Coecke, and Stephen
Pulman. Concrete Sentence Spaces for Compositional Distributional Models of Mean-
ing.arXiv:1101.0309 [cs] , December 2010. URL http://arxiv.org/abs/1101.0309 .
John E. Hummel. Getting symbols out of a neural architecture. Connection Science , 23
(2):109–118, June 2011. ISSN 0954-0091. doi: 10.1080/09540091.2011.569880. URL
https://doi.org/10.1080/09540091.2011.569880 .
Martha Lewis. Quantum Computing and Cognitive Simulation, March 2021. URL
https://psyarxiv.com/hvbgt/ .
Martha Lewis, Nihal V. Nayak, Peilin Yu, Qinan Yu, Jack Merullo, Stephen H. Bach,
and Ellie Pavlick. Does CLIP Bind Concepts? Probing Compositionality in Large
Image Models, March 2023. URL http://arxiv.org/abs/2212.10537 .
Andrea E. Martin and Leonidas A. A. Doumas. Tensors and compositionality in
neural systems. Philosophical Transactions of the Royal Society B: Biological Sci-
ences , 375(1791):20190306, February 2020. doi: 10.1098/rstb.2019.0306. URL
https://royalsocietypublishing.org/doi/10.1098/rstb.2019.0306 .
Tom McCoy, Ellie Pavlick, and Tal Linzen. Right for the Wrong Reasons: Diag-
nosing Syntactic Heuristics in Natural Language Inference. In Proceedings of the
57th Annual Meeting of the Association for Computational Linguistics , pages 3428–
3448, Florence, Italy, July 2019. Association for Computational Linguistics. doi:
10.18653/v1/P19-1334. URL https://www.aclweb.org/anthology/P19-1334 .
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient Estimation of
Word Representations in Vector Space. arXiv:1301.3781 [cs] , September 2013. URL
http://arxiv.org/abs/1301.3781 .
Jeff Mitchell and Mirella Lapata. Composition in Distributional Models of Se-
mantics. Cognitive Science , 34(8):1388–1429, November 2010. ISSN 03640213.
doi: 10.1111/j.1551-6709.2010.01106.x. URL http://doi.wiley.com/10.1111/j.
1551-6709.2010.01106.x .
Denis Paperno, Nghia The Pham, and Marco Baroni. A practical and linguistically-
motivated approach to compositional distributional semantics. In Proceedings of the
52nd Annual Meeting of the Association for Computational Linguistics (Volume 1:
Long Papers) , pages 90–99, Baltimore, Maryland, June 2014. Association for Com-
putational Linguistics. doi: 10.3115/v1/P14-1009. URL https://www.aclweb.org/
anthology/P14-1009 .
15Jeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global Vec-
tors for Word Representation. In Proceedings of the 2014 Conference on Empirical
Methods in Natural Language Processing (EMNLP) , pages 1532–1543, Doha, Qatar,
2014. Association for Computational Linguistics. doi: 10.3115/v1/D14-1162. URL
http://aclweb.org/anthology/D14-1162 .
Tony A. Plate. Distributed Representations and Nested Compositional Structure . PhD
thesis, 1994.
Emmanuel M. Pothos and Jerome R. Busemeyer. Can quantum probability pro-
vide a new direction for cognitive modeling? Behavioral and Brain Sci-
ences , 36(3):255–274, June 2013. ISSN 0140-525X, 1469-1825. doi: 10.
1017/S0140525X12001525. URL https://www.cambridge.org/core/product/
identifier/S0140525X12001525/type/journal_article .
Paul Smolensky. Tensor product variable binding and the representation of symbolic
structures in connectionist systems. Artificial Intelligence , 46(1-2):159–216, Novem-
ber 1990. ISSN 00043702. doi: 10.1016/0004-3702(90)90007-M. URL https:
//linkinghub.elsevier.com/retrieve/pii/000437029090007M .
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning,
Andrew Ng, and Christopher Potts. Recursive Deep Models for Semantic Com-
positionality Over a Sentiment Treebank. In Proceedings of the 2013 Conference
on Empirical Methods in Natural Language Processing , pages 1631–1642, Seattle,
Washington, USA, October 2013. Association for Computational Linguistics. URL
https://www.aclweb.org/anthology/D13-1170 .
Aarne Talman and Stergios Chatzikyriakidis. Testing the Generalization Power of Neural
Network Models across NLI Benchmarks. In Proceedings of the 2019 ACL Workshop
BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP , pages 85–94,
Florence, Italy, August 2019. Association for Computational Linguistics. doi: 10.
18653/v1/W19-4810. URL https://aclanthology.org/W19-4810 .
Dominic Widdows, Kirsty Kitto, and Trevor Cohen. Quantum Mathematics in Artificial
Intelligence. Journal of Artificial Intelligence Research , 72:1307–1341, December 2021.
ISSN 1076-9757. doi: 10.1613/jair.1.12702. URL https://www.jair.org/index.
php/jair/article/view/12702 .
16