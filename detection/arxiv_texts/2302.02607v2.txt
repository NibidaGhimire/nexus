Target-based Surrogates for Stochastic Optimization
Jonathan Wilder Lavington* 1Sharan Vaswani* 2Reza Babanezhad3Mark Schmidt1 4Nicolas Le Roux5 6
Abstract
We consider minimizing functions for which it
is expensive to compute the (possibly stochastic)
gradient. Such functions are prevalent in rein-
forcement learning, imitation learning and adver-
sarial training. Our target optimization framework
uses the (expensive) gradient computation to con-
struct surrogate functions in a target space (e.g.
the logits output by a linear model for classifica-
tion) that can be minimized efficiently. This al-
lows for multiple parameter updates to the model,
amortizing the cost of gradient computation. In
the full-batch setting, we prove that our surro-
gate is a global upper-bound on the loss, and can
be (locally) minimized using a black-box opti-
mization algorithm. We prove that the resulting
majorization-minimization algorithm ensures con-
vergence to a stationary point of the loss. Next,
we instantiate our framework in the stochastic set-
ting and propose the SSO algorithm, which can
be viewed as projected stochastic gradient descent
in the target space. This connection enables us to
prove theoretical guarantees for SSO when mini-
mizing convex functions. Our framework allows
the use of standard stochastic optimization algo-
rithms to construct surrogates which can be mini-
mized by any deterministic optimization method.
To evaluate our framework, we consider a suite of
supervised learning and imitation learning prob-
lems. Our experiments indicate the benefits of
target optimization and the effectiveness of SSO.
*Equal contribution1University of British Columbia2Simon
Fraser University3Samsung - SAIT AI Lab, Montreal4
Canada CIFAR AI Chair (Amii)5Microsoft Research6Canada
CIFAR AI Chair (MILA). Correspondence to: Jonathan
Wilder Lavington <wilderlavington@gmail.com>, Sharan Vaswani
<vaswani.sharan@gmail.com>.
Proceedings of the 40thInternational Conference on Machine
Learning , Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright
2023 by the author(s).1. Introduction
Stochastic gradient descent (SGD) (Robbins and Monro,
1951) and its variants (Duchi et al., 2011; Kingma and
Ba, 2015) are ubiquitous optimization methods in machine
learning (ML). For supervised learning, iterative first-order
methods require computing the gradient over individual
mini-batches of examples, and that cost of computing the
gradient often dominates the total computational cost of
these algorithms. For example, in reinforcement learning
(RL) (Williams, 1992; Sutton et al., 2000) or online imita-
tion learning (IL) (Ross et al., 2011), policy optimization
requires gathering data via potentially expensive interactions
with a real or simulated environment.
We focus on algorithms that access the expensive gradi-
ent oracle to construct a sequence of surrogate functions.
Typically, these surrogates are chosen to be global upper-
bounds on the underlying function and hence minimizing
the surrogate allows for iterative minimization of the origi-
nal function. Algorithmically, these surrogate functions can
be minimized efficiently without additional accesses to the
gradient oracle , making this technique advantageous for the
applications of interest. The technique of incrementally con-
structing and minimizing surrogate functions is commonly
referred to as majorization-minimization and includes the
Expectation-Maximization (EM) algorithm (Dempster et al.,
1977) as an example. In RL, common algorithms (Schulman
et al., 2015; 2017) also rely on minimizing surrogates.
Typically, surrogate functions are constructed by using
the convexity and/or smoothness properties of the under-
lying function. Such surrogates have been used in the
stochastic setting (Mairal, 2013; 2015). The prox-linear
algorithm (Drusvyatskiy, 2017) for instance, uses the com-
position structure of the loss function and constructs sur-
rogate functions in the parametric space. Unlike these
existing works, we construct surrogate functions over a
well-chosen target space rather than the parametric space,
leveraging the composition structure of the loss functions
prevalent in ML to build better surrogates. For example,
in supervised learning, typical loss functions are of the
form h(θ) =ℓ(f(θ)), where ℓis (usually) a convex loss
(e.g. the squared loss for regression or the logistic loss
for classification), while fcorresponds to a transforma-
tion (e.g. linear or high-dimensional, non-convex as in
1arXiv:2302.02607v2  [cs.LG]  8 Jun 2023Target-based Surrogates for Stochastic Optimization
the case of neural networks) of the inputs. Similarly, in
IL,ℓmeasures the divergence between the policy being
learned and the ground-truth expert policy, whereas fcor-
responds to a specific parameterization of the policy being
learned. More formally, if Θis the feasible set of parameters,
f: Θ→ Z is a potentially non-convex mapping from the
parametric space Θ⊆Rdto the target space Z ⊆Rpand
ℓ:Z →Ris a convex loss function. For example, for lin-
ear regression, h(θ) =1
2∥Xθ−y∥2
2,z=f(θ) =Xθand
ℓ(z) =1
2∥z−y∥2
2. In our applications of interest, comput-
ing∇zℓ(z)requires accessing the expensive gradient oracle,
but∇θf(θ)can be computed efficiently. Unlike Nguyen
et al. (2022) who exploit this composition structure to prove
global convergence, we will use it to construct surrogate
functions in the target space. Johnson and Zhang (2020)
also construct surrogate functions using the target space, but
require access to the (stochastic) gradient oracle for each
model update, making the algorithm proposed inefficient in
our setting. Moreover, unlike our work, Johnson and Zhang
(2020) do not have theoretical guarantees in the stochas-
tic setting. Concurrently with our work, Woodworth et al.
(2023) also consider minimizing expensive-to-evaluate func-
tions, but do so by designing “proxy” loss function which
is similar to the original function. We make the following
contributions.
Target smoothness surrogate : In Section 3, we use the
smoothness of ℓwith respect to zin order to define the
target smoothness surrogate and prove that it is a global
upper-bound on the underlying function h. In particular,
these surrogates are constructed using tighter bounds on
the original function. This ensures that additional progress
can be made towards the minimizer using multiple model
updates before recomputing the expensive gradient.
Target optimization in the deterministic setting : In Sec-
tion 3, we devise a majorization-minimization algorithm
where we iteratively form the target smoothness surrogate
and then (locally) minimize it using any black-box algo-
rithm. Although forming the target smoothness surrogate
requires access to the expensive gradient oracle, it can be
minimized without additional oracle calls resulting in mul-
tiple, computationally efficient updates to the model. We
refer to this framework as target optimization . This idea of
constructing surrogates in the target space has been recently
explored in the context of designing efficient off-policy al-
gorithms for reinforcement learning (Vaswani et al., 2021).
However, unlike our work, Vaswani et al. (2021) do not
consider the stochastic setting, or provide theoretical con-
vergence guarantees. In Algorithm 1, we instantiate the
target optimization framework and prove that it converges
to a stationary point at a sublinear rate (Lemma C.1 in Ap-
pendix C).
Stochastic target smoothness surrogate : In Section 4, weconsider the setting where we have access to an expensive
stochastic gradient oracle that returns a noisy, but unbiased
estimate of the true gradient. Similar to the deterministic
setting, we access the gradient oracle to form a stochastic
target smoothness surrogate. Though the surrogate is con-
structed by using a stochastic gradient in the target space,
it is a deterministic function with respect to the parame-
ters and can be minimized using any standard optimization
algorithm. In this way, our framework disentangles the
stochasticity in ∇zℓ(z)(in the target space) from the poten-
tial non-convexity in f(in the parametric space).
Target optimization in the stochastic setting : Similar to
the deterministic setting, we use msteps of GD to mini-
mize the stochastic target smoothness surrogate and refer to
the resulting algorithm as stochastic surrogate optimization
(SSO). We then interpret SSO as inexact projected SGD in
the target space. This interpretation of SSO allows the use
of standard stochastic optimization algorithms to construct
surrogates which can be minimized by any deterministic
optimization method. Minimizing surrogate functions in
the target space is also advantageous since it allows us to
choose the space in which to constrain the size of the up-
dates. Specifically, for overparameterized models such as
deep neural networks, there is only a loose connection be-
tween the updates in the parameter and target space. In order
to directly constrain the updates in the target space, methods
such as natural gradient (Amari, 1998; Kakade, 2001) in-
volve computationally expensive operations. In comparison,
SSO has direct control over the updates in the target space
and can also be implemented efficiently.
Theoretical results for SSO: Assuming h(θ)to be smooth,
strongly-convex, in Section 4.1, we prove that SSO (with
a constant step-size in the target space) converges linearly
to a neighbourhood of the true minimizer (Theorem 4.2).
In Proposition 4.3, we prove that the size of this neighbour-
hood depends on the noise ( σ2
z) in the stochastic gradients in
the target space and an error term ζ2that depends on the dis-
similarity in the stochastic gradients at the optimal solution.
In Proposition 4.4, we provide a quadratic example that
shows the necessity of this error term in general. However,
as the size of the mini-batch increases or the model is over-
parameterized enough to interpolate the data (Schmidt and
Le Roux, 2013; Vaswani et al., 2019a), ζ2
tbecomes smaller.
In the special case when interpolation is exactly satisfied,
we prove that SSO withO(log( 1/ϵ))iterations is sufficient
to guarantee convergence to an ϵ-neighbourhood of the true
minimizer. Finally, we argue that SSO can be more efficient
than using parametric SGD for expensive gradient oracles
and common loss functions (Section 4.2).
Experimental evaluation : To evaluate our target optimiza-
tion framework, we consider online imitation learning (OIL)
as our primary example. For policy optimization in OIL,
2Target-based Surrogates for Stochastic Optimization
computing ∇zℓinvolves gathering data through interaction
with a computationally expensive simulated environment.
Using the Mujoco benchmark suite (Todorov et al., 2012)
we demonstrate that SSO results in superior empirical per-
formance (Section 5). We then consider standard supervised
learning problems where we compare SSO with different
choices of the target surrogate to standard optimization meth-
ods. These empirical results indicate the practical benefits
of target optimization and SSO.
2. Problem Formulation
We focus on minimizing functions that have a composition
structure and for which the gradient is expensive to compute.
Formally, our objective is to solve the following problem:
minθ∈Θh(θ) := ℓ(f(θ))where Θ⊆Rd,Z ⊆Rp,f:
Θ→ Z andℓ:Z →R. Throughout this paper, we will
assume that hisLθ-smooth in the parameters θand that
ℓ(z)isL-smooth in the targets z. For all generalized linear
models including linear and logistic regression, f=X⊤θ
is a linear map in θandℓis convex in z.
Example : For logistic regression with features
X∈Rn×dand labels y∈ {− 1,+1}n,h(θ) =Pn
i=1log (1 + exp( −yi⟨Xi, θ⟩)). If we “target” the
logits1, then Z={z|z=Xθ} ⊆ Rnandℓ(z) =Pn
i=1log (1 + exp( −yizi)). In this case, Lθis the
maximum eigenvalue of XTX, whereas L=1
4. A similar
example follows for linear regression.
In settings that use neural networks, it is typical for the
function fmapping Xtoyto be non-convex, while for the
lossℓto be convex. For example in OIL, the target space
is the space of parameterized policies, where ℓis the cu-
mulative loss when using a policy distribution π:=f(θ)
who’s density is parameterized by θ. Though our algorith-
mic framework can handle non-convex ℓandf, depending
on the specific setting, our theoretical results will assume
thatℓ(orh) is (strongly)-convex in θandfis an affine map.
For our applications of interest, computing ∇zℓ(z)is com-
putationally expensive, whereas f(θ)(and its gradient) can
be computed efficiently. For example, in OIL, comput-
ing the cumulative loss ℓ(and the corresponding gradient
∇zℓ(z)) for a policy involves evaluating it in the environ-
ment. Since this operation involves interactions with the
environment or a simulator, it is computationally expensive.
On the other hand, the cost of computing ∇θf(θ)only de-
pends on the policy parameterization and does not involve
additional interactions with the environment. In some cases,
it is more natural to consider access to a stochastic gra-
dient oracle that returns a noisy, unbiased gradient ∇˜ℓ(z)
1The target space is not unique. For example, we could di-
rectly target the classification probabilities for logistic regression
resulting in different fandℓ.such that E[∇˜ℓ(z)] =∇ℓ(z). We consider the effect of
stochasticity in Section 4.
If we do not take advantage of the composition structure
nor explicitly consider the cost of the gradient oracle, itera-
tive first-order methods such as GD or SGD can be directly
used to minimize h(θ). At iteration t∈[T], the paramet-
ric GD update is: θt+1=θt−η∇ht(θt)where ηis the
step-size to be selected or tuned according to the properties
ofh. Since hisLθ-smooth, each iteration of parametric
GD can be viewed as exactly minimizing the quadratic
surrogate function derived from the smoothness condi-
tion with respect to the parameters. Specifically, θt+1:=
arg min gp
t(θ)where gp
tis the parametric smoothness surro-
gate:gp
t(θ) :=h(θt) +⟨∇h(θt), θ−θt⟩+1
2η∥θ−θt∥2
2.
The quadratic surrogate is tight at θt, i.eh(θt) = gp
t(θt)
and becomes looser as we move away from θt. For
η≤1
Lθ, the surrogate is a global (for all θ) upper-
bound on h, i.e. gp
t(θ)≥h(θ). Minimizing the global
upper-bound results in descent on hsince h(θt+1)≤
gp
t(θt+1)≤gp
t(θt) =ht(θt). Similarly, the parametric
SGD update consists of accessing the stochastic gradient
oracle to obtain ˜h(θ),∇˜h(θ)
such that E[˜h(θ)] = h(θ)
andE[∇˜h(θ)] =∇h(θ), and iteratively constructing the
stochastic parametric smoothness surrogate ˜gpt(θ). Specif-
ically, θt+1= arg min ˜gpt(θ), where ˜gpt(θ) := ˜h(θt) +
⟨∇˜h(θt), θ−θt⟩+1
2ηt∥θ−θt∥2
2. Here, ηtis the iteration
dependent step-size, decayed according to properties of
h(Robbins and Monro, 1951). In contrast to these methods,
in the next section, we exploit the smoothness of the losses
with respect to the target space and propose a majorization-
minimization algorithm in the deterministic setting.
3. Deterministic Setting
We consider minimizing ℓ(f)in the deterministic setting
where we can exactly evaluate the gradient ∇zℓ(z). Similar
to the parametric case in Section 2, we use the smoothness
ofℓ(z)w.r.t the target space and define the target smooth-
ness surrogate around ztas:ℓ(zt) +⟨∇zℓ(zt), z−zt⟩+
1
2η∥z−zt∥2
2, where ηis the step-size in the target space
and will be determined theoretically. Since z=f(θ), the
surrogate can be expressed as a function of θas:gt(θ) :=h
ℓ(zt) +⟨∇zℓ(zt), f(θ)−zt⟩+1
2η∥f(θ)−zt∥2
2i
, which
in general is not quadratic in θ.
Example : For linear regression, f(θ) =X⊤θandgt(θ) =
1
2∥Xθt−y∥2
2+
[Xθt−y], X(θ−θt)
+1
2η∥X(θ−θt)∥2
2.
Similar to the parametric smoothness surrogate, we see that
h(θt) =ℓ(f(θt)) =gt(θt). IfℓisL-smooth w.r.t the target
spaceZ, then for η≤1
L, we have gt(θ)≥ℓ(f(θ)) =h(θ)
for all θ, that is the surrogate is a global upper-bound on h.
Since gtis a global upper-bound on h, similar to GD, we can
3Target-based Surrogates for Stochastic Optimization
minimize hby minimizing the surrogate at each iteration i.e.
θt+1= arg minθgt(θ). However, unlike GD, in general,
there is no closed form solution for the minimizer of gt, and
we will consider minimizing it approximately.
Algorithm 1 (Stochastic) Surrogate optimization
Input :θ0(initialization), T(number of iterations), mt
(number of inner-loops), η(step-size for the target space), α
(step-size for the parametric space)
fort= 0toT−1do
Access the (stochastic) gradient oracle to construct
˜gt(θ)
Initialize inner-loop: ω0=θt
fork←0tomt−1do
ωk+1=ωk−α∇ω˜gt(ωk)
end for
θt+1=ωm ;zt+1=f(θt+1)
end for
Return θT
This results in the following meta-algorithm: for each t∈
[T], at iterate θt, form the surrogate gtand compute θt+1
by (approximately) minimizing gt(θ). This meta-algorithm
enables the use of any black-box algorithm to minimize the
surrogate at each iteration. In Algorithm 1, we instantiate
this meta-algorithm by minimizing gt(θ)using m≥1steps
of gradient descent. For m= 1, Algorithm 1 results in the
following update: θt+1=θt−α∇gt(θt) =θt−α∇h(θt),
and is thus equivalent to parametric GD with step-size α.
Example : For linear regression, instantiating Algorithm 1
withm= 1recovers parametric GD on the least squares ob-
jective. On the other hand, minimizing the surrogate exactly
(corresponding to m=∞) to compute θt+1results in the
following update: θt+1=θt−η(XTX)−1[XT(Xθt−y)]
and recovers the Newton update in the parameter space. In
this case, approximately minimizing gtusing m∈(1,∞)
steps of GD interpolates between a first and second-order
method in the parameter space. In this case, our framework
is similar to quasi-Newton methods (Nocedal and Wright,
1999) that attempt to model the curvature in the loss without
explicitly modelling the Hessian. Unlike these methods, our
framework does not have an additional memory overhead.
In Lemma C.1 in Appendix C, we prove that Algorithm 1
with any value of m≥1and appropriate choices of αand
ηresults in an O(1/T)convergence to a stationary point of
h. Importantly, this result only relies on the smoothness
ofℓandgz
t, and does not require either ℓ(z)orf(θ)to be
convex. Hence, this result holds when using a non-convex
model such as a deep neural networks, or for problems with
non-convex loss functions such as in reinforcement learning.
In the next section, we extend this framework to consider the
stochastic setting where we can only obtain a noisy (thoughunbiased) estimate of the gradient.
4. Stochastic Setting
In the stochastic setting, we use the noisy but unbiased
estimates (˜ℓ(z),∇˜ℓ(z))from the gradient oracle to
construct the stochastic target surrogate . To simplify the
theoretical analysis, we will focus on the special case where
ℓis a finite-sum of losses i.e. ℓ(z) =1
nPn
i=1ℓi(z). In this
case, querying the stochastic gradient oracle at iteration t
returns the individual loss and gradient corresponding to the
loss index iti.e. ˜ℓ(z),∇˜ℓ(z)
= (ℓit(z),∇ℓit(z)). This
structure is present in the use-cases of interest, for example,
in supervised learning when using a dataset of ntraining
points or in online imitation learning where multiple
trajectories are collected using the policy at iteration t.
In this setting, the deterministic surrogate is gt(θ) :=
1
nhP[ℓi(z) +⟨∇ℓi(zt), f(θ)−zt⟩] +1
2ηt∥f(θ)−zt∥2
2i
.
In order to admit an efficient implementation of the stochas-
tic surrogate and the resulting algorithms, we only con-
sider loss functions that are separable w.r.t the target space,
i.e. for z∈ Z , ifzi∈Rdenotes coordinate iofz,
thenℓ(z) =1
nP
iℓi(zi). For example, this structure is
present in the loss functions for all supervised learning
problems where Z ⊆Rnandzi=fi(θ) := f(Xi, θ).
In this setting,∂ℓi
∂zj= 0 for all iandj̸=iand the
stochastic target surrogate is defined as ˜gt(θ) :=ℓit(zt) +
∂ℓit(zt)
∂zit
fit(θ)−zit
t
+1
2ηt
fit(θ)−zit
t2, where ηtis the
target space step-size at iteration t. Note that ˜gt(θ)only de-
pends on itand only requires access to ∂ℓit(zt)(meaning it
can be constructed efficiently). We make two observations
about the stochastic surrogate: (i) unlike the parametric
stochastic smoothness surrogate ˜gpthat uses itto form the
stochastic gradient, ˜gt(θ)usesit(the same random sam-
ple) for both the stochastic gradient ∂ℓit(zt)and the reg-
ularization
fit(θ)−zit
t2; (ii) while Eit[˜gt(θ)] = gt(θ),
Eit[arg min ˜ gt(θ)]̸= arg min gt(θ), in contrast to the para-
metric case where E[arg min ˜ gp
t(θ)] = arg min gp
t(θ).
Example : For linear regression, zi=fi(θ) =Xiθand
ℓi(zi) =1
2(zi−yi)2. In this case, the stochastic target
surrogate is equal to ˜gt(θ) =1
2(Xitθt−yit)2+ [Xitθt−
yit]·Xit(θ−θt) +1
2ηt∥Xit(θ−θt)∥2
2.
Similar to the deterministic setting, the next iterate can be
obtained by (approximately) minimizing ˜gt(θ). Algorithmi-
cally, we can form the surrogate ˜gtat iteration tand min-
imize it approximately by using any black-box algorithm.
We refer to the resulting framework as stochastic surrogate
optimization (SSO). For example, we can minimize ˜gtus-
ingmsteps of GD. The resulting algorithm is the same
as Algorithm 1 but uses ˜gt(the changes to the algorithm are
highlighted in green). Note that the surrogate depends on
4Target-based Surrogates for Stochastic Optimization
the randomly sampled itand is therefore random. However,
once the surrogate is formed, it can be minimized using any
deterministic algorithm, i.e. there is no additional random-
ness in the inner-loop in Algorithm 1. Moreover, for the
special case of m= 1, Algorithm 1 has the same update
as parametric stochastic gradient descent. Previous work
like the retrospective optimization framework in Newton
et al. (2021) also considers multiple updates on the same
batch of examples. However, unlike Algorithm 1, which
forces proximity between consecutive iterates in the target
space, Newton et al. (2021) use a specific stopping criterion
in every iteration and consider a growing batch-size.
In order to prove theoretical guarantees for SSO, we inter-
pret it as projected SGD in the target space. In particular,
we prove the following equivalence in Appendix D.1.
Lemma 4.1. The following updates are equivalent:
(1) ˜θt+1= arg min
θ˜gt(θ); (SSO)
˜z(1)
t+1=f(˜θt+1)
(2) zt+1/2=zt−ηt∇zℓit(zt);(Target-space SGD)
˜z(2)
t+1= arg min
z∈Z1
2zt+1/2−z2
Pt
where, Pt∈Rp×pis a random diagonal matrix such that
Pt(it, it) = 1 andPt(j, j) = 0 for all j̸=it. That is, SSO
(1) and target space SGD (2), result in the same iterate in
each step i.e. if zt=f(θt), then ˜zt+1:= ˜z(1)
t+1= ˜z(2)
t+1.
The second step in target-space SGD corresponds to the
projection (using randomly sampled index it) ontoZ2.
Using this equivalence enables us to interpret the inexact
minimization of ˜gt(for example, using msteps of GD in Al-
gorithm 1) as an inexact projection onto Zand will be
helpful to prove convergence guarantees for SSO. The
above interpretation also enables us to use the existing
literature on SGD (Robbins and Monro, 1951; Li et al.,
2021; Vaswani et al., 2019b) to specify the step-size se-
quence {ηt}T
t=1in the target space, completing the instan-
tiation of the stochastic surrogate. Moreover, alternative
stochastic optimization algorithms such as follow the reg-
ularized leader (Abernethy et al., 2009) and adaptive gra-
dient methods like AdaGrad (Duchi et al., 2011), online
Newton method (Hazan et al., 2007), and stochastic mirror
descent (Bubeck et al., 2015, Chapter 6) in the target space
result in different stochastic surrogates (refer to Appendix B)
that can then be optimized using a black-box deterministic
algorithm. Next, we prove convergence guarantees for SSO
when ℓ(z)is a smooth, strongly-convex function.
2For linear parameterization, zit=⟨Xit, θ⟩and the set Zis
convex. For non-convex f, the set Zcan be non-convex and the
projection is not well-defined. However, ˜gtcan still be minimized,
albeit without any guarantees on the convergence.4.1. Theoretical Results
For the setting where ℓ(z)is a smooth and strongly-convex
function, we first analyze the convergence of inexact pro-
jected SGD in the target space (Section 4.1.1), and then
bound the projection errors in in Section 4.1.2.
4.1.1. C ONVERGENCE ANALYSIS
For the theoretical analysis, we assume that the choice
offensures that the projection is well-defined (e.g. for
linear parameterization where f=XTθ). We define
¯zt+1:=f(¯θt+1), where ¯θt+1:= arg min ˜ qt(θ)and˜qt:=
ℓit(zt) +∂ℓit(zt)
∂zit
fit(θ)−zit
t
+1
2η′
t∥f(θ)−zt∥2
2. In or-
der to ensure that E[˜qt(θ)] =gt(θ), we will set η′
t=ηtn.
Note that ˜qtis similar to ˜gt, but there is no randomness in
the regularization term. Analogous to ˜zt+1,¯zt+1can be
interpreted as a result of projected (where the projection is
w.r.tℓ2-norm) SGD in the target space (Lemma D.1). Note
that˜qtis only defined for the analysis of SSO.
For the theoretical analysis, it is convenient to define
ϵt+1:=∥zt+1−¯zt+1∥2as the projection error at itera-
tiont. Here, zt+1=f(θt+1)where θt+1is obtained by
(approximately) minimizing ˜gt. Note that the projection
error incorporates the effect of both the random projection
(that depends on it) as well as the inexact minimization of
˜gt, and will be bounded in Section 4.1.2. In the following
lemma (proved in Appendix D.2), we use the equivalence
in Lemma 4.1 to derive the following guarantee for two
choices of ηt: (a) constant and (b) exponential step-size (Li
et al., 2021; Vaswani et al., 2022).
Theorem 4.2. Assuming that (i) ℓitisL-smooth and convex,
(ii)ℓisµ-strongly convex, (iii) z∗= arg min z∈Zℓ(z)(iv)
and that for all t,ϵt≤ϵ,Titerations of SSO result in the
following bound for zT=f(θT),
E∥zT+1−z∗∥ ≤YT
i=1ρi
∥z1−z∗∥2
2
+ 2σ2XT
t=1YT
i=t+1ρiη′
t21/2
+ 2ϵXT
t=1YT
i=t+1ρi,
where ρt= (1−µη′
t),σ2:=Eh
∥∇ℓ(z∗)− ∇ℓt(z∗)∥2
2i
,
(a)Constant step-size : When ηt=1
2L nand for ρ= 1−µ
2L,
E∥zT+1−z∗∥ ≤ ∥ z1−z∗∥
1−1
2κT
2
+σ√µL+2ϵ
1−q
1−1
2κ.
(b)Exponential step-size : When ηt=1
2L nαtforα=
5Target-based Surrogates for Stochastic Optimization
(β
T)1
T,
E∥zT+1−z∗∥ ≤c1exp
−T
4κα
ln(T/β)
∥z1−z∗∥
+4κc1(ln(T/β))
Leα√
Tσ+ 2ϵ c2.
where c1= exp
1
4κ2β
ln(T/β)
, and c2= exp
βln(T)
2κln(T/β)
In the above result, σ2is the natural analog of the noise
in the unconstrained case. In the unconstrained case,
∇ℓ(z∗) = 0 and we recover the standard notion of noise
used in SGD analyses (Bottou et al., 2018; Gower et al.,
2019). Unlike parametric SGD, both σ2andκdo not depend
on the specific model parameterization, and only depend on
the properties of ℓin the target space. For both the constant
and exponential step-size, the above lemma generalizes the
SGD proofs in Bottou et al. (2018) and Li et al. (2021);
Vaswani et al. (2022) to projected SGD and can handle in-
exact projection errors similar to Schmidt et al. (2011). For
constant step-size, SSO results in convergence to a neigh-
bourhood of the minimizer where the neighbourhood de-
pends on σ2and the projection errors ϵ2
t. For the exponential
step-sizes, SSO results in a noise-adaptive (Vaswani et al.,
2022) O 
exp −T
κ
+σ2
T
convergence to a neighbourhood
of the solution which only depends on the projection errors.
In the next section, we bound the projection errors.
4.1.2. C ONTROLLING THE PROJECTION ERROR
In order to complete the theoretical analysis of SSO, we
need to control the projection error ϵt+1=∥¯zt+1−zt+1∥2
that can be decomposed into two components as follows:
ϵt+1≤ ∥¯zt+1−˜zt+1∥2+∥˜zt+1−zt+1∥2, where ˜zt+1=
f(˜θt+1)and˜θt+1is the minimizer of ˜gt. The first part
of this decomposition arises because of using a stochastic
projection that depends on it(in the definition of ˜zt+1)
versus a deterministic projection (in the definition of ¯zt+1).
The second part of the decomposition arises because of the
inexact minimization of the stochastic surrogate, and can
be controlled by minimizing ˜gtto the desired tolerance. In
order to bound ϵt+1, we will make the additional assumption
thatfisLf-Lipschitz in θ.3The following proposition
(proved in Appendix D.3) bounds ϵt+1.
Proposition 4.3. Assuming that (i) fisLf-Lipschitz
continuous, (ii) ˜gtisµgstrongly-convex and Lgsmooth
with κg:=Lg/µg4, (iii) ˜qtisµqstrongly-convex (iv)
ζ2
t:=8
min{µg,µq}([min{Eit[˜gt]} −Eit[min{˜gt}]])
+
8
min{µg,µq}[min{Eit[˜qt]} −Eit[min{˜qt}]]
, (v)
3For example, when using a linear parameterization, Lf=
∥X∥This property is also satisfied for certain neural networks.
4We consider strongly-convex functions for simplicity, and it
should be possible to extend these results to when ˜gis non-convex
but satisfies the PL inequality, or is weakly convex.σ2
z:= min z∈Z{Eit[ℓit(z)]} −Eit[min z∈Z{ℓit(z)}],
if˜gtis minimized using mtinner-loops of GD
with the appropriate step-size, then, E[ϵ2
t+1]≤
L2
fζ2
t+4L2
f
µg
exp (−mt/κg)
E[ℓ(zt)−ℓ(z∗)] +σ2
z
.
Note that the definition of both ζ2
tandσ2
zis similar to
that used in the SGD analysis in Vaswani et al. (2022),
and can be interpreted as variance terms. In particular,
using strong-convexity, min{Eit[˜gt]} −Eit[min{˜gt}]≤
2
µgE[∇˜gt(θ′
t+1)−E[∇˜gt(θ′
t+1)]2
2]which is the variance
in∇˜gt(θ′
t+1). Similarly, we can interpret the other term
in the definition of ζ2
t. The first term L2
fζ2
tbecause of
the stochastic versus deterministic projection, whereas the
second term 4L2
f/µgexp (−mt/κg)
E[ℓ(zt)−ℓ(z∗)] +σ2
z
arises because of the inexact minimization of the stochastic
surrogate. Setting mt=O(log(1 /ϵ))can reduce the second
term to O(ϵ). When sampling a mini-batch of examples in
each iteration of SSO, both ζ2
tandσ2
zwill decrease as the
batch-size increases (because of the standard sampling-with-
replacement bounds (Lohr, 2019)) becoming zero for the
full-batch. Another regime of interest is when using over-
parameterized models that can interpolate the data (Schmidt
and Le Roux, 2013; Ma et al., 2018; Vaswani et al., 2019a).
The interpolation condition implies that the stochastic gra-
dients become zero at the optimal solution, and is satisfied
for models such as non-parametric regression (Liang and
Rakhlin, 2018; Belkin et al., 2019) and over-parametrized
deep neural networks (Zhang et al., 2017). Under this condi-
tion,ζ2
t=σ2
z= 0(Vaswani et al., 2020; Loizou et al., 2021).
From an algorithmic perspective, the above result implies
that in cases where the noise dominates, using large mfor
SSO might not result in substantial improvements. However,
when using a large batch-size or over-parameterized models,
using large mcan result in the superior performance.
Given the above result, a natural question is whether
the dependence on ζ2
tis necessary in the general (non-
interpolation) setting. To demonstrate this, we construct
an example (details in Appendix D.4) and show that even
for a sum of two one-dimensional quadratics, when min-
imizing the surrogate exactly i.e. m=∞and for any
sequence of convergent step-sizes, SSO will converge to a
neighborhood of the solution.
Proposition 4.4. Consider minimizing the sum h(θ) :=
h1(θ)+h2(θ)
2of two one-dimensional quadratics, h1(θ) :=
1
2(θ−1)2andh2(θ) =1
2(2θ+1/2)2, using SSO withmt=
∞andηt=c αtfor any sequence of αtand any constant
c∈(0,1].SSO results in convergence to a neighbourhood
of the solution, specifically, if θ∗is the minimizer of hand
θ1>0, then, E(θT−θ∗)≥min 
θ1,3
8
.
In order to show the above result, we use the fact that for
quadratics, SSO (with m=∞) is equivalent to the sub-
sampled Newton method and in the one-dimensional case,
6Target-based Surrogates for Stochastic Optimization
we can recover the example in Vaswani et al. (2022). Since
the above example holds for m=∞, we conclude that
this bias is not because of the inexact surrogate minimiza-
tion. Moreover, since the example holds for all step-sizes
including anydecreasing step-size, we can conclude that
the optimization error is not a side-effect of the stochas-
ticity. In order to avoid such a bias term, sub-sampled
Newton methods use different batches for computing the
sub-sampled gradient and Hessian, and either use an increas-
ing batch-size (Bollapragada et al., 2019) or consider using
over-parameterized models (Meng et al., 2020).
Agarwal et al. (2020) also prove theoretical guarantees when
doing multiple SGD steps on the same batch. In contrast
to our work, their motivation is to analyze the performance
of data-echoing (Choi et al., 2019). From a technical per-
spective, they consider (i) updates in the parameteric space,
and (ii) their inner-loop step-size decreases as mincreases.
Finally, we note our framework and subsequent theoretical
guarantees would also apply to this setting.
4.2. Benefits of Target Optimization
In order to gain intuition about the possible benefits of target
optimization, let us consider the simple case where each
hiisµθ-strongly convex, Lθ-smooth and κθ=Lθ/µθ. For
convenience, we define ζ2:= max t∈[T]ζ2
t. Below, we
show that under certain regimes, for ill-conditioned least
squares problems (where κθ>>1), target optimization has
a provable advantage over parametric SGD.
Example : For the least squares setting, κθis the
condition number of the XTXmatrix. In order
to achieve an ϵsub-optimality, assuming complete
knowledge of σ2
θ:=E∥∇hi(θ∗)∥2
2and all problem-
dependent constants, parametric SGD requires Tparam =
O 
max
κθlog 
1/ϵ
,σ2
θ/µ2ϵ	
iterations (Gower et al.,
2019, Theorem 3.1) where the first term is the bias term
and the second term is the effect of the noise. In order
to achieve an ϵsub-optimality for SSO, we require that
ζ2≤ϵ(for example, by using a large enough batch-
size) and O(κθlog(1/ϵ))inner iterations. Similar to the
parametric case, the number of outer iterations for SSO
isTtarget=O 
max
log 
1/ϵ
,σ2/ϵ	
, since the condition
number w.r.t the targets is equal to 1.
In order to model the effect of an expensive gradient oracle,
let us denote the cost of computing the gradient of ℓasτand
the cost of computing the gradient of the surrogate as equal
to1. When the noise in the gradient is small and the bias
dominates the number of iterations for both parametric and
target optimization, the cost of parametric SGD is dominated
byτTparam=O(κθτ), whereas the cost of target optimiza-
tion is given by Ttarget×[τ+κθlog(1/ϵ)] = O(τ+κθ).
Alternatively, when the noise dominates, we need to com-
pare the O 
τσ2
θ/µ2ϵ
cost for the parametric case against theO σ2
ϵ[τ+κθlog(1/ϵ)]
cost for target optimization. Using
the definition of the noise, σ2
θ=Ei∥XT
i(Xiθ∗−yi)∥2
2≤
Lσ2. By replacing σ2
θbyLσ2, we again see that the com-
plexity of parametric SGD depends on O(κθτ), whereas
that of SSO depends on O(κθ+τ).
A similar property can also be shown for the logistic regres-
sion. Similarly, we could use other stochastic optimization
algorithms to construct surrogates for target optimization.
See Appendix B and Appendix E for additional discussion.
5. Experimental Evaluation
We evaluate the target optimization framework for online
imitation learning and supervised learning5. In the subse-
quent experiments, we use either the theoretically chosen
step-size when available, or the default step-size provided
by Paszke et al. (2019). We do not include a decay sched-
ule for any experiments in the main text, but consider both
the standard 1/√
tschedule (Bubeck et al., 2015), as well
as the exponential step-size-schedule (Vaswani et al., 2022;
Orabona, 2019) in Appendix E. For SSO, since optimization
of the surrogate is a deterministic problem, we use the stan-
dard back-tracking Armijo line-search (Armijo, 1966) with
the same hyper-parameters across all experiments. For each
experiment, we plot the average loss against the number of
calls to the (stochastic) gradient oracle. The mean and the
relevant quantiles are reported using three random seeds.
Online Imitation Learning: We consider a setting in which
the losses are generated through interaction with a simulator.
In this setting, a behavioral policy gathers examples by ob-
serving a state of the simulated environment and taking an
action at that state. For each state gathered through the in-
teraction, an expert policy provides the action that it would
have taken. The goal in imitation learning is to produce
a policy which imitates the expert. The loss measures the
discrepancy between the learned policy πand the expert
policy. In this case, z=πwhere πis a distribution over ac-
tions given states, ℓit(z) =Est[KL(π(·|st)||πexpert(·|st))]
where the expectation is over the states visited by the behav-
ioral policy and f(θ)is the policy parameterization. Since
computing ℓitrequires the behavioural policy to interact
with the environment, it is expensive. Furthermore, the KL
divergence is 1-strongly convex in the ℓ1-norm, hence OIL
satisfies all our assumptions. When the behavioral policy
is the expert itself, we refer to the problem as behavioral
cloning . When the learned policy is used to interact with the
environment (Florence et al., 2022), we refer to the problem
asonline imitation learning (OIL) (Lavington et al., 2022;
Ross et al., 2011).
5The code is available at http://github.com/
WilderLavington/Target-Based-Surrogates-For-
Stochastic-Optimization .
7Target-based Surrogates for Stochastic Optimization
Adagrad Adam SLS SSO-1 SSO-10 SSO-100 SSO-1000
6.26.36.46.56.6Log-LossBehavioral Cloning: 
6.06.36.66.97.2Online Imitation: 
0 10k 20k 30k 40k 50k
T otal-Examples5.45.76.06.36.6Log-Loss
0 10k 20k 30k 40k 50k
T otal-Examples5.55.96.36.77.1
(a) Hopper-v2 Environment
6.66.87.07.27.4Log-LossBehavioral Cloning: 
6.97.27.57.88.1
LinearOnline Imitation: 
0 10k 20k 30k 40k 50k
T otal-Examples5.76.16.56.97.3Log-Loss
0 10k 20k 30k 40k 50k
T otal-Examples7.07.37.67.98.2
Neural (b) Walker2d-v2 Environment
Figure 1: Comparison of log policy loss (mean-squared error between the expert labels and the mean action produced
by the policy model) incurred by SGD,SLS,Adam ,Adagrad , andSSO as a function of the total interactions (equal to t
in Algorithm 1). SSO-m in the legend indicates that the surrogate has been minimized for mGD steps. The bottom row
shows experiments where the policy is parameterized by a neural network, while the top row displays an example where the
policy is parameterized by a linear model. Across all environments, behavioral policies, and model types, SSO outperforms
all other online-optimization algorithms. Additionally, as m increases, so does the performance of SSO.
SGD Adam SLS Adagrad SSO-1 SSO-10 SSO-20
100
101
102
LossBatch-Size: 25
100
101
102
Batch-Size: 125
100
101
102
Batch-Size: 625
100
101
SSO-SGDFull-Batch
00.5 1.0 1.5 2.0 2.5 33.5 4.0
Optimization-Steps (105)100
101
102
Loss
0 1 2 3 4 5 6 7 8
Optimization-Steps (104)100
101
102
0 2 4 6 8 10 12 14 16
Optimization-Steps (103)100
101
102
0 1 2 3 4 5
Optimization-Steps (102)100
101
SSO-SLS
Figure 2: Comparison of SGD and its SSO variant (top row), SLS and it SSO variant (bottom row) over the rcv1
dataset (Chang and Lin, 2011) using a logistic loss. Adam andAdagrad are included as baselines. All plots are in
log space, where the x-axis defines optimization steps (equal to tin Algorithm 1). We note that SGD with its theoretical
step-size is outperformed by more sophisticated algorithms like SLS orAdam . In contrast, SSO with the theoretical step-size
is competitive with both SLS andAdam with default hyper-parameters. Notably, the SSO variant of both SLS and SGD
outperforms its parametric counterpart across both m and batch-size.
SGD SLS SSO-1 SSO-10 SSO-100
01020304050Sample-Size: 10
0123456Sample-Size: 100
0.00.51.01.52.02.53.0Sample-Size: 1000
Figure 3: Comparison of run-times normalized with re-
spect to the cost of a single SGD update) between SSO
and relevant baselines. These plots illustrate that when data
collection is expensive, SSO will be as fast as SGD, even
for relatively large m. For Fig. 1, τ≈1000 , and the ratio of
the time required to compute ℓvsfis approximately 0.001.In Fig. 1, we consider continuous control environments from
the Mujoco benchmark suite (Todorov et al., 2012). The
policy corresponds to a standard normal distribution whose
mean is parameterized by either a linear function or a neural
network. For gathering states, we sample from the stochastic
(multivariate normal) policy in order to take actions. At each
round of environment interaction 1000 states are gathered,
and used to update the policy. The expert policy, defined by
a normal distribution and parameterized by a two-layer MLP
is trained using the Soft-Actor-Critic Algorithm (Haarnoja
et al., 2018). Fig. 1 shows that SSO with the theoretical
step-size drastically outperforms standard optimization al-
gorithms in terms the log-loss as a function of environment
interactions (calls to the gradient oracle). Further, we see
that for both the linear and neural network parameterization,
8Target-based Surrogates for Stochastic Optimization
the performance of the learned policy consistently improves
asmincreases.
Runtime Comparison: In Fig. 3, we demonstrate the relative
run-time between algorithms. Each column represents the
average run-time required to take a single optimization step
(sample states and update the model parameters) normal-
ized by the time for SGD. We vary the number of states
gathered (referred to as sample-size) per step and consider
sample-sizes of 10,100and1000 . The comparison is per-
formed on the Hopper-v2 environment using a two layer
MLP. We observe that for small sample-sizes, the time it
takes to gather states does not dominate the time it takes
to update the model (for example, in column 1, SSO-100
takes almost 50 times longer than SGD). On the other hand,
for large sample-sizes, the multiple model updates made by
SSO are no longer a dominating factor (for example, see
the right-most column where SSO-100 only takes about
three times as long as SGD but results in much better em-
pirical performance). This experiment shows that in cases
where data-access is the major bottleneck in computing the
stochastic gradients, target optimization can be beneficial,
matching the theoretical intuition developed in Section 4.2.
For applications with more expensive simulators such as
those for autonomous vehicle (Dosovitskiy et al., 2017),
SSO can result in further improvements.
Supervised Learning : In order to explore using other opti-
mization methods in the target optimization framework, we
consider a simple supervised learning setup. In particular,
we use the the rcv1 dataset from libsvm (Chang and Lin,
2011) across four different batch sizes under a logistic-loss.
We include additional experiments over other data-sets, and
optimization algorithms in Appendix E.6
Extensions to the Stochastic Surrogate : We consider us-
ing a different optimization algorithm – stochastic line-
search (Vaswani et al., 2019b) ( SLS) in the target space,
to construct a different surrogate. We refer to the re-
sulting algorithm as SSO-SLS . For SSO-SLS , at every
iteration, we perform a backtracking line-search in the
target space to set ηtthat satisfies the Armijo condition:
ℓit(zt−ηt∇zℓit(zt))≤ℓit(zt)−ηt
2∥∇zℓit(zt)∥2
2. We use
the chosen ηtto instantiate the surrogate ˜gtand follow Al-
gorithm 1. In Fig. 2, we compare both SSO-SGD (top row)
andSSO-SLS (bottom row) along with its parametric vari-
ant. We observe that the SSO variant of both SGD and SLS
(i) does as well as its parametric counterpart across all m,
and (ii), improves it for m sufficiently large.
6We also compared SSO against SVRG (Johnson and Zhang,
2013), a variance reduced method, and found that SSO consistently
outperformed it across the batch-sizes and datasets we consider.
For an example of this behavior see Fig. 15 in Appendix E.6. Discussion
In the future, we aim to extend our theoretical results to a
broader class of functions, and empirically evaluate target
optimization for more complex models. Since our frame-
work allows using any optimizer in the target space, we will
explore other optimization algorithms in order to construct
better surrogates. Another future direction is to construct
better surrogate functions that take advantage of the addi-
tional structure in the model. For example, Taylor et al.
(2016); Amid et al. (2022) exploit the composition structure
in deep neural network models, and construct local layer-
wise surrogates enabling massive parallelization. Finally,
we also aim to extend our framework to applications such
as RL where ℓis non-convex.
7. Acknowledgements
We would like to thank Frederik Kunstner and Reza
Asad for pointing out mistakes in the earlier version of
this paper. This research was partially supported by the
Canada CIFAR AI Program, the Natural Sciences and
Engineering Research Council of Canada (NSERC) Discov-
ery Grants RGPIN-2022-03669 and RGPIN-2022-04816.
References
Abernethy, J. D., Hazan, E., and Rakhlin, A. (2009). Com-
peting in the dark: An efficient algorithm for bandit linear
optimization. COLT . (cited on 5)
Agarwal, N., Anil, R., Koren, T., Talwar, K., and Zhang,
C. (2020). Stochastic optimization with laggard data
pipelines. Advances in Neural Information Processing
Systems , 33:10282–10293. (cited on 7)
Amari, S. (1998). Natural gradient works efficiently in
learning. Neural Computation . (cited on 2)
Amid, E., Anil, R., and Warmuth, M. (2022). Locoprop: En-
hancing backprop via local loss optimization. In Camps-
Valls, G., Ruiz, F. J. R., and Valera, I., editors, Proceed-
ings of The 25th International Conference on Artificial
Intelligence and Statistics , volume 151 of Proceedings of
Machine Learning Research , pages 9626–9642. PMLR.
(cited on 9)
Armijo, L. (1966). Minimization of functions having lips-
chitz continuous first partial derivatives. Pacific Journal
of mathematics , 16(1):1–3. (cited on 7)
Belkin, M., Rakhlin, A., and Tsybakov, A. B. (2019). Does
data interpolation contradict statistical optimality? In
AISTATS . (cited on 6)
9Target-based Surrogates for Stochastic Optimization
Bollapragada, R., Byrd, R. H., and Nocedal, J. (2019). Exact
and inexact subsampled newton methods for optimiza-
tion. IMA Journal of Numerical Analysis , 39(2):545–578.
(cited on 7)
Bottou, L., Curtis, F. E., and Nocedal, J. (2018). Opti-
mization methods for large-scale machine learning. Siam
Review , 60(2):223–311. (cited on 6)
Bubeck, S. et al. (2015). Convex optimization: Algorithms
and complexity. Foundations and Trends ®in Machine
Learning , 8(3-4):231–357. (cited on 5, 7)
Chang, C.-C. and Lin, C.-J. (2011). Libsvm: A library for
support vector machines. ACM transactions on intelligent
systems and technology (TIST) , 2(3):1–27. (cited on 8, 9,
30, 36)
Choi, D., Passos, A., Shallue, C. J., and Dahl, G. E. (2019).
Faster neural network training with data echoing. arXiv
preprint arXiv:1907.05550 . (cited on 7)
Dempster, A. P., Laird, N. M., and Rubin, D. B. (1977).
Maximum likelihood from incomplete data via the em
algorithm. Journal of the Royal Statistical Society: Series
B (Methodological) , 39(1):1–22. (cited on 1)
Dosovitskiy, A., Ros, G., Codevilla, F., Lopez, A., and
Koltun, V . (2017). CARLA: An open urban driving sim-
ulator. In Proceedings of the 1st Annual Conference on
Robot Learning , pages 1–16. (cited on 9)
Drusvyatskiy, D. (2017). The proximal point method revis-
ited. arXiv preprint arXiv:1712.06038 . (cited on 1)
Duchi, J., Hazan, E., and Singer, Y . (2011). Adaptive sub-
gradient methods for online learning and stochastic opti-
mization. JMLR . (cited on 1, 5)
Florence, P., Lynch, C., Zeng, A., Ramirez, O. A., Wahid,
A., Downs, L., Wong, A., Lee, J., Mordatch, I., and
Tompson, J. (2022). Implicit behavioral cloning. In Faust,
A., Hsu, D., and Neumann, G., editors, Proceedings of
the 5th Conference on Robot Learning , volume 164 of
Proceedings of Machine Learning Research , pages 158–
168. PMLR. (cited on 7)
Gower, R. M., Loizou, N., Qian, X., Sailanbayev, A.,
Shulgin, E., and Richtárik, P. (2019). Sgd: General anal-
ysis and improved rates. In International Conference on
Machine Learning , pages 5200–5209. PMLR. (cited on
6, 7)
Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S. (2018).
Soft actor-critic: Off-policy maximum entropy deep re-
inforcement learning with a stochastic actor. In Interna-
tional conference on machine learning , pages 1861–1870.
PMLR. (cited on 8)Hazan, E., Agarwal, A., and Kale, S. (2007). Logarithmic re-
gret algorithms for online convex optimization. Machine
Learning , 69(2):169–192. (cited on 5)
Johnson, R. and Zhang, T. (2013). Accelerating stochas-
tic gradient descent using predictive variance reduction.
InAdvances in Neural Information Processing Systems,
NeurIPS . (cited on 9, 36)
Johnson, R. and Zhang, T. (2020). Guided learning of
nonconvex models through successive functional gradient
optimization. In International Conference on Machine
Learning , pages 4921–4930. PMLR. (cited on 2)
Kakade, S. M. (2001). A natural policy gradient. In Ad-
vances in Neural Information Processing Systems 14
[Neural Information Processing Systems: Natural and
Synthetic, NIPS 2001, December 3-8, 2001, Vancouver,
British Columbia, Canada] . (cited on 2)
Kingma, D. and Ba, J. (2015). Adam: A method for stochas-
tic optimization. In ICLR . (cited on 1, 30)
Lavington, J. W., Vaswani, S., and Schmidt, M. (2022). Im-
proved policy optimization for online imitation learning.
arXiv preprint arXiv:2208.00088 . (cited on 7, 37)
Li, X., Zhuang, Z., and Orabona, F. (2021). A second
look at exponential and cosine step sizes: Simplicity,
adaptivity, and performance. In International Conference
on Machine Learning , pages 6553–6564. PMLR. (cited
on 5, 6)
Liang, T. and Rakhlin, A. (2018). Just interpolate: Ker-
nel" ridgeless" regression can generalize. arXiv preprint
arXiv:1808.00387 . (cited on 6)
Lohr, S. L. (2019). Sampling: Design and Analysis: Design
and Analysis . Chapman and Hall/CRC. (cited on 6)
Loizou, N., Vaswani, S., Laradji, I. H., and Lacoste-Julien,
S. (2021). Stochastic polyak step-size for sgd: An adap-
tive learning rate for fast convergence. In International
Conference on Artificial Intelligence and Statistics , pages
1306–1314. PMLR. (cited on 6)
Ma, S., Bassily, R., and Belkin, M. (2018). The power of
interpolation: Understanding the effectiveness of SGD in
modern over-parametrized learning. In ICML . (cited on
6)
Mairal, J. (2013). Stochastic majorization-minimization
algorithms for large-scale optimization. Advances in
Neural Information Processing Systems , 26. (cited on 1)
Mairal, J. (2015). Incremental majorization-minimization
optimization with application to large-scale machine
learning. SIAM Journal on Optimization , 25(2):829–855.
(cited on 1)
10Target-based Surrogates for Stochastic Optimization
Meng, S. Y ., Vaswani, S., Laradji, I. H., Schmidt, M., and
Lacoste-Julien, S. (2020). Fast and furious convergence:
Stochastic second order methods under interpolation. In
International Conference on Artificial Intelligence and
Statistics , pages 1375–1386. PMLR. (cited on 7)
Nesterov, Y . (2003). Introductory lectures on convex opti-
mization: A basic course , volume 87. Springer Science
& Business Media. (cited on 22)
Newton, D., Bollapragada, R., Pasupathy, R., and Yip, N. K.
(2021). Retrospective approximation for smooth stochas-
tic optimization. arXiv preprint arXiv:2103.04392 . (cited
on 5)
Nguyen, L. M., Tran, T. H., and van Dijk, M. (2022). Finite-
sum optimization: A new perspective for convergence
to a global solution. arXiv preprint arXiv:2202.03524 .
(cited on 2)
Nocedal, J. and Wright, S. J. (1999). Numerical optimiza-
tion. Springer. (cited on 4)
Orabona, F. (2019). A modern introduction to online learn-
ing. arXiv preprint arXiv:1912.13213 . (cited on 7, 30)
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J.,
Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga,
L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Rai-
son, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang,
L., Bai, J., and Chintala, S. (2019). Pytorch: An imper-
ative style, high-performance deep learning library. In
Advances in Neural Information Processing Systems 32 ,
pages 8024–8035. Curran Associates, Inc. (cited on 7,
33, 34, 35, 38)
Robbins, H. and Monro, S. (1951). A stochastic approxima-
tion method. The annals of mathematical statistics , pages
400–407. (cited on 1, 3, 5)
Ross, S., Gordon, G., and Bagnell, D. (2011). A reduction of
imitation learning and structured prediction to no-regret
online learning. Journal of machine learning research ,
pages 627–635. (cited on 1, 7)
Schmidt, M. and Le Roux, N. (2013). Fast convergence of
stochastic gradient descent under a strong growth condi-
tion. arXiv preprint arXiv:1308.6370 . (cited on 2, 6)
Schmidt, M., Roux, N., and Bach, F. (2011). Convergence
rates of inexact proximal-gradient methods for convex
optimization. Advances in neural information processing
systems , 24. (cited on 6, 28)
Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz,
P. (2015). Trust region policy optimization. In Interna-
tional Conference on Machine Learning (ICML) , pages
1889–1897. (cited on 1)Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and
Klimov, O. (2017). Proximal policy optimization algo-
rithms. CoRR , abs/1707.06347. (cited on 1)
Sutton, R. S., McAllester, D. A., Singh, S. P., and Mansour,
Y . (2000). Policy gradient methods for reinforcement
learning with function approximation. In Advances in
Neural Information Processing Systems (NeurIPS) , pages
1057–1063. (cited on 1, 38)
Taylor, G., Burmeister, R., Xu, Z., Singh, B., Patel, A.,
and Goldstein, T. (2016). Training neural networks with-
out gradients: A scalable admm approach. In Balcan,
M. F. and Weinberger, K. Q., editors, Proceedings of The
33rd International Conference on Machine Learning , vol-
ume 48 of Proceedings of Machine Learning Research ,
pages 2722–2731, New York, New York, USA. PMLR.
(cited on 9)
Todorov, E., Erez, T., and Tassa, Y . (2012). Mujoco:
A physics engine for model-based control. In 2012
IEEE/RSJ International Conference on Intelligent Robots
and Systems . (cited on 3, 8, 37)
Vaswani, S., Bach, F., and Schmidt, M. (2019a). Fast and
faster convergence of sgd for over-parameterized models
and an accelerated perceptron. In The 22nd international
conference on artificial intelligence and statistics , pages
1195–1204. PMLR. (cited on 2, 6)
Vaswani, S., Bachem, O., Totaro, S., Müller, R., Garg, S.,
Geist, M., Machado, M. C., Castro, P. S., and Roux,
N. L. (2021). A general class of surrogate functions for
stable and efficient reinforcement learning. arXiv preprint
arXiv:2108.05828 . (cited on 2)
Vaswani, S., Dubois-Taine, B., and Babanezhad, R. (2022).
Towards noise-adaptive, problem-adaptive (accelerated)
stochastic gradient descent. In International Conference
on Machine Learning , pages 22015–22059. PMLR. (cited
on 5, 6, 7, 20, 25, 30)
Vaswani, S., Laradji, I., Kunstner, F., Meng, S. Y ., Schmidt,
M., and Lacoste-Julien, S. (2020). Adaptive gradi-
ent methods converge faster with over-parameterization
(but you should do a line-search). arXiv preprint
arXiv:2006.06835 . (cited on 6)
Vaswani, S., Mishkin, A., Laradji, I., Schmidt, M., Gidel, G.,
and Lacoste-Julien, S. (2019b). Painless stochastic gra-
dient: Interpolation, line-search, and convergence rates.
InAdvances in Neural Information Processing Systems ,
pages 3727–3740. (cited on 5, 9, 30, 33, 34, 35, 38)
Ward, R., Wu, X., and Bottou, L. (2020). Adagrad stepsizes:
Sharp convergence over nonconvex landscapes. The Jour-
nal of Machine Learning Research , 21(1):9047–9076.
(cited on 34)
11Target-based Surrogates for Stochastic Optimization
Williams, R. J. (1992). Simple statistical gradient-following
algorithms for connectionist reinforcement learning. Ma-
chine learning , 8(3-4):229–256. (cited on 1)
Woodworth, B., Mishchenko, K., and Bach, F. (2023). Two
losses are better than one: Faster optimization using a
cheaper proxy. arXiv preprint arXiv:2302.03542 . (cited
on 2)
Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O.
(2017). Understanding deep learning requires rethinking
generalization. In ICLR . (cited on 6)
12Target-based Surrogates for Stochastic Optimization
Organization of the Appendix
A Definitions
B Algorithms
C Proofs in the Deterministic Setting
D Proofs in the Stochastic Setting
E Additional Experimental Results
A. Definitions
Our main assumptions are that each individual function fiis differentiable, has a finite minimum f∗
i, and is Li-smooth,
meaning that for all vandw,
fi(v)≤fi(w) +⟨∇fi(w), v−w⟩+Li
2∥v−w∥2
2, (Individual Smoothness)
which also implies that fisL-smooth, where Lis the maximum smoothness constant of the individual functions. A
consequence of smoothness is the following bound on the norm of the stochastic gradients,
∥∇fi(w)− ∇f∗
i∥2≤2L(fi(w)−f∗
i− ⟨∇ f∗
i, w−w∗
i⟩). (1)
We also assume that each fiis convex, meaning that for all vandw,
fi(v)≥fi(w) +⟨∇fi(w), v−w⟩, (Convexity)
Depending on the setting, we will also assume that fisµstrongly-convex, meaning that for all vandw,
f(v)≥f(w) +⟨∇f(w), v−w⟩+µ
2∥v−w∥2
2, (Strong Convexity)
B. Algorithms
In this section, we will formulate the algorithms beyond the standard SGD update in the target space. We will do so in two
ways – (i) extending SGD to the online Newton step that uses second-order information in Appendix B.1 and (ii) extend
SGD to the more general stochastic mirror descent algorithm in Appendix B.2. For both (i) and (ii), we will instantiate the
resulting algorithms for the squared and logistic losses.
B.1. Online Newton Step
Let us consider the online Newton step w.r.t to the targets. The corresponding update is:
zt+1/2=zt−ηt[∇2
zℓt(zt)]−1∇zℓt(zt);¯zt+1= arg min
z∈Z1
2z−zt+1/22
Pt(2)
zt+1=f(θt+1);θt+1= arg min
θ
⟨∇zℓt(zt), f(θ)−zt⟩+1
2ηt∥f(θ)−zt∥2
∇2ℓt(zt)
(3)
where ∇2
zℓt(zt)is the Hessian of example of the loss corresponding to sample itw.r.tz. Let us instantiate this update for
the squared-loss. In this case, ℓt(z) =1
2∥z−yt∥2
2, and hence, ∇ℓt(z) =z−yt,[∇2ℓt(z)]it,it= 1and[∇2ℓt(z)]j,j= 0
for all j̸=it. Hence, for the squared loss, Eq. (2) is the same as GD in the target space.
For the logistic loss, ℓt(z) = log (1 + exp ( −ytz)). Ifitis the loss index sampled at iteration t, then, [∇ℓt(z)]j= 0for all
j̸=it. Similarly, all entries of ∇2ℓt(z)except the [it, it]are zero.
[∇ℓt(z)]it=−yt
1 + exp( ytzt);[∇2ℓ(z)]it,it,=1
1 + exp( ytzt)1
1 + exp( −ytzt)= (1−pt)pt,
13Target-based Surrogates for Stochastic Optimization
where, pt=1
1+exp( −ytzt)is the probability of classifying the example itto have the +1label. In this case, the surrogate
can be written as:
˜gz
t(θ) =−yt
1 + exp( ytzt) 
fit(θ)−zit
t
+(1−pt)pt
2ηt 
fit(θ)−zit
t2(4)
As before, the above surrogate can be implemented efficiently.
B.2. Stochastic Mirror Descent
Ifϕis a differentiable, strictly-convex mirror map, it induces a Bregman divergence between xandy:Dϕ(y, x) :=
ϕ(y)−ϕ(x)− ⟨∇ ϕ(x), y−x⟩. For an efficient implementation of stochastic mirror descent, we require the Bregman
divergence to be separable, i.e. Dϕ(y, x) =Pp
j=1Dϕj(yj, xj) =Pp
j=1ϕj(yj)−ϕj(xj)−∂ϕj(x)
∂xj[yj−xj]. Such a
separable structure is satisfied when ϕis the Euclidean norm or negative entropy. We define stochastic mirror descent update
in the target space as follows,
∇ϕ(zt+1/2) =∇ϕ(zt)−ηt∇zℓt(zt);¯zt+1= arg min
z∈ZpX
j=1I(j=it)Dϕj(zj, zj
t+1/2)) (5)
=⇒¯zt+1= arg min
z∈Z
⟨∇zℓt(zt), z−zt⟩+1
ηtpX
j=1I(j=it)Dϕj(zj, zj
t+1/2)
 (6)
where Iis an indicator function and itcorresponds to the index of the sample chosen in iteration t. For the Euclidean mirror
map, ϕ(z) =1
2∥z∥2
2,Dϕ(z, zt) =1
2∥z−zt∥2
2and we recover the SGD update.
Another common choice of the mirror map is the negative entropy function: ϕ(x) =PK
i=1xilog(xi)where xiis coordinate
iof the x∈RK. This induces the (generalized) KL divergence as the Bregman divergence,
Dϕ(x, y) =KX
k=1xklogxk
yk
+KX
k=1xk−KX
k=1yk.
If both xandycorrespond to probability distributions i.e.PK
k=1xk=PK
k=1yk= 1, then the induced Bregman divergence
corresponds to the standard KL-divergence between the two distributions. For multi-class classification, Z ⊆Rp×Kand
eachzi∈∆Kwhere ∆KisK-dimensional simplex. We will refer to coordinate jofzias[zi]j. Since zi∈∆K,[zi]k≥0
andPK
k=1[zi]k= 1.
Let us instantiate the general SMD updates in Eq. (5) when using the negative entropy mirror map. In this case, for z∈∆K,
[∇ϕ(z)]k= 1 + log([ z]k). Denoting ∇t:=∇zℓt(zt)and using [∇t]kto refer to coordinate kof the K-dimensional vector
∇t. Hence, Eq. (5) can be written as:
[zit
t+1/2]k= [ztit]kexp (−ηt[∇t]k) (7)
For multi-class classification, yi∈ {0,1}Kare one-hot vectors. If [yi]krefers to coordinate kof vector yi, then correspond-
ing log-likelihood for nobservations can be written as:
ℓ(z) =nX
i=1KX
k=1[yi]klog([zi]k).
In our target optimization framework, the targets correspond to the probabilities of classifying the points into one of the
classes. We use a parameterization to model the vector-valued function fi(θ) :Rd→RK. This ensures that for all i,PK
k=1[fi(θ)]k= 1. Hence, the projection step in Eq. (5) can be rewritten as:
min
z∈ZDϕ(z, zt+1/2) =KX
k=1[fit(θ)]klog 
[fit(θ)]k
[zit
t+1/2]k!
where [zit
t+1/2]kis computed according to Eq. (7). Since the computation of [zit
t+1/2]kand the resulting projection only
depends on sample it, it can be implemented efficiently.
14Target-based Surrogates for Stochastic Optimization
C. Proofs in the Deterministic Setting
Lemma C.1. Assuming that gz
t(θ)isβ-smooth w.r.t. the Euclidean norm and η≤1
L, then, for α=1/β, iteration t
of Algorithm 1 guarantees that h(θt+1)≥h(θt)for any number m≥1of surrogate steps. In this setting, under the
additional assumption that his lower-bounded by h∗, then Algorithm 1 results in the following guarantee,
min
t∈{0,...,T−1}∥∇h(θt)∥2
2≤2β[h(θ0)−h∗]
T.
Proof. Using the update in Algorithm 1 with α=1
βand the β-smoothness of gz
t(θ), for all k∈[m−1],
gz
t(ωk+1)≤gz
t(ωk)−1
2β∥∇gz
t(ωk)∥2
2
After msteps,
gz
t(ωm)≤gz
t(ω0)−1
2βm−1X
k=0∥∇gz
t(ωk)∥2
2
Since θt+1=ωmandω0=θtin Algorithm 1,
=⇒gz
t(θt+1)≤gz
t(θt)−1
2β∥∇gz
t(θt)∥2
2−m−1X
k=1∥∇gz
t(ωk)∥2
2
Note that h(θt) =gz
t(θt)and if η≤1
L, then h(θt+1)≤gz
t(θt+1). Using these relations,
h(θt+1)≤h(θt)−
1
2β∥∇gz
t(θt)∥2
2+m−1X
k=1∥∇gz
t(ωk)∥2
2
| {z }
≥0
=⇒h(θt+1)≤h(θt).
This proves the first part of the Lemma. SincePm−1
k=1∥∇gz
t(ωk)∥2
2≥0,
h(θt+1)≤h(θt)−1
2β∥∇gz
t(θt)∥2
2=⇒ ∥∇ h(θt)∥2
2≤2β[h(θt)−h(θt+1)] (Since ∇h(θt) =∇gz
t(θt))
Summing from k= 0toT−1, and dividing by T,
∥∇h(θt)∥2
2
T≤2β[h(θt)−h∗]
T=⇒ min
t∈{0,...,T−1}∥∇h(θt)∥2
2≤2β[h(θt)−h∗]
T
15Target-based Surrogates for Stochastic Optimization
D. Proofs in the Stochastic Setting
D.1. Equivalence of SSO and SGD in Target Space
Lemma 4.1. The following updates are equivalent:
(1) ˜θt+1= arg min
θ˜gt(θ); (SSO)
˜z(1)
t+1=f(˜θt+1)
(2) zt+1/2=zt−ηt∇zℓit(zt); (Target-space SGD)
˜z(2)
t+1= arg min
z∈Z1
2zt+1/2−z2
Pt
where, Pt∈Rp×pis a random diagonal matrix such that Pt(it, it) = 1 andPt(j, j) = 0 for all j̸=it. That is, SSO (1)
and target space SGD (2), result in the same iterate in each step i.e. if zt=f(θt), then ˜zt+1:= ˜z(1)
t+1= ˜z(2)
t+1.
Proof. Since ℓis separable, if itis the coordinate sampled from z, we can rewrite the target-space update as follows:
zt+1/2it=ztit−ηt∂ℓit(zt)
∂zit
zt+1/2j=ztjwhen j̸=it.
Putting the above update in the projection step we have,
˜zt+1= arg min
z∈Z1
2{zt+1/2it−zit2
2}
= arg min
z∈Z1
2{ztit−ηt∂ℓit(zt)
∂zit−zit2
2}
= arg min
z∈Z∂ℓit(zt)
∂zit[zit−zit
t] +1
2ηtzit−zit
t2
2
(Due to separability of ℓ)
Since for all z∈ Z,z=f(θ)andzi=fi(θ)for all i. Hence ˜zt+1=f(˜θt+1)such that,
˜θt+1= arg min
θ∈Θ∂ℓit(zt)
∂zit[fit(θ)−fit(θt)] +1
2ηt∥fit(θ)−fit(θt)∥2
2
= arg min
θ∈Θ˜gz
t(θ)
Lemma D.1. Consider the following updates:
¯θt+1= arg min
θ˜qt(θ);¯z(1)
t+1=f(¯θt+1) (SSO)
zt+1/2=zt−η′
t∇zℓit(zt); (Target-space SGD)
¯z(2)
t+1= arg min
z∈Z1
2zt+1/2−z2
2
SSO and target space SGD result in the same iterate in each step i.e. if zt=f(θt), then ¯zt+1:= ¯z(1)
t+1= ¯z(2)
t+1.
Proof.
¯zt+1= arg min
z∈Z1
2{zt+1/2−z2
2}
= arg min
z∈Z1
2{∥zt−η′
t∇zℓit(zt)−z∥2
2}
= arg min
z∈Z∂ℓit(zt)
∂zit[zit−zit
t] +1
2η′
t∥z−zt∥2
2
(Due to separability of ℓ)
16Target-based Surrogates for Stochastic Optimization
Since for all z∈ Z,z=f(θ). Hence ¯zt+1=f(¯θt+1)such that,
¯θt+1= arg min
θ∈Θ∂ℓit(zt)
∂zit[fit(θ)−fit(θt)] +1
2η′
t∥f(θ)−f(θt)∥2
2
= arg min
θ∈Θ˜qt(θ)
D.2. Proof for Strongly-convex Functions
We consider the case where ℓ(z)is strongly-convex and the set Zis convex. We will focus on SGD in the target space, and
consider the following updates:
zt+1/2=zt−η′
t∇ℓt(zt)
¯zt+1= ΠZ[zt+1/2] := arg min
z∈Z1
2z−zt+1/22
2
∥zt+1−¯zt+1∥ ≤ϵt+1
Lemma D.2. Bounding the suboptimality (to z∗) ofzt+1based on the sub-optimality of ¯zt+1andϵt+1, we get that
∥zt+1−z∗∥2
2≤ ∥¯zt+1−z∗∥2
2+ 2ϵt+1∥zt+1−z∗∥. (8)
Proof.
∥zt+1−z∗∥2
2=∥zt+1−¯zt+1+ ¯zt+1−z∗∥2
2
=∥zt+1−¯zt+1∥2
2+∥¯zt+1−z∗∥2
2+ 2⟨zt+1−¯zt+1,¯zt+1−z∗⟩
=∥zt+1−¯zt+1∥2
2+∥¯zt+1−z∗∥2
2+ 2⟨zt+1−¯zt+1,¯zt+1−zt+1+zt+1−z∗⟩
=∥zt+1−¯zt+1∥2
2+∥¯zt+1−z∗∥2
2+ 2⟨zt+1−¯zt+1, zt+1−z∗⟩ −2∥zt+1−¯zt+1∥2
2
≤ ∥¯zt+1−z∗∥2
2+ 2∥zt+1−¯zt+1∥∥zt+1−z∗∥
≤ ∥¯zt+1−z∗∥2
2+ 2ϵt+1∥zt+1−z∗∥
Now we bound the exact sub-optimality at iteration t+ 1by the inexact sub-optimality at iteration tto get a recursion.
Lemma D.3. Assuming (i) each ℓtisL-smooth and (ii) ℓisµ-strongly convex and (iii) η′
t≤1
2L, we have
E∥¯zt+1−z∗∥2
2≤(1−µη′
t)E∥zt−z∗∥2
2+ 2η′
t2σ2(9)
where σ2=E∥∇ℓ(z∗)− ∇ℓt(z∗)∥2
2.
Proof. Since z∗∈ Z and optimal, z∗= ΠZ[z∗−η′
t∇ℓ(z∗)].
∥¯zt+1−z∗∥2
2=∥ΠZ[zt−η′
t∇ℓt(zt)]−ΠZ[z∗−η′
t∇ℓ(z∗)]∥2
2
≤ ∥[zt−η′
t∇ℓt(zt)]−[z∗−η′
t∇ℓ(z∗)]∥2
2(Since projections are non-expansive)
=∥[zt−η′
t∇ℓt(zt)]−[z∗−η′
t∇ℓt(z∗)] +η′
t[∇ℓ(z∗)− ∇ℓt(z∗)]∥2
2
=∥zt−z∗∥2
2+η′
t2∥∇ℓ(z∗)− ∇ℓt(z∗)∥2
2+η′
t2∥∇ℓt(zt)− ∇ℓt(z∗)∥2
2+ 2η′
t⟨zt−z∗,∇ℓ(z∗)− ∇ℓt(z∗)⟩| {z }
At
−2η′
t⟨zt−z∗,∇ℓt(zt)− ∇ℓt(z∗)⟩+ 2η′
t2⟨∇ℓt(z∗)− ∇ℓ(z∗),∇ℓt(zt)− ∇ℓt(z∗)⟩| {z }
Bt
≤ ∥zt−z∗∥2
2+ 2η′
t2∥∇ℓ(z∗)− ∇ℓt(z∗)∥2
2+ 2η′
t2∥∇ℓt(zt)− ∇ℓt(z∗)∥2
2
+At−2η′
t⟨zt−z∗,∇ℓt(zt)− ∇ℓt(z∗)⟩ (Young inequality on Bt)
17Target-based Surrogates for Stochastic Optimization
Taking expectation w.r.t it, knowing that EAt= 0and using that σ2=E∥∇ℓ(z∗)− ∇ℓt(z∗)∥2
2.
E∥¯zt+1−z∗∥2
2≤E∥zt−z∗∥2
2+ 2η′
t2E∥∇ℓt(zt)− ∇ℓt(z∗)∥2
2−2η′
t⟨zt−z∗,∇ℓ(zt)− ∇ℓ(z∗)⟩+ 2η′
t2σ2
≤E∥zt−z∗∥2
2+ 4η′
t2LE{ℓt(zt)−ℓt(z∗)− ⟨∇ ℓt(z∗), zt−z∗⟩} −2η′
t⟨zt
−z∗,∇ℓ(zt)− ∇ℓ(z∗)⟩+ 2η′2
tσ2(10)
≤E∥zt−z∗∥2
2+ 2η′
t{ℓ(zt)−ℓ(z∗)− ⟨∇ ℓ(z∗), zt−z∗⟩} −2η′
t⟨zt−z∗,∇ℓ(zt)− ∇ℓ(z∗)⟩+ 2η′2
tσ2
(η′
t<1
2L)
≤E∥zt−z∗∥2
2+ 2η′
t{ℓ(zt)−ℓ(z∗)− ⟨∇ ℓ(zt), zt−z∗⟩}+ 2η′2
tσ2
≤E∥zt−z∗∥2
2−µη′
tE∥zt−z∗∥2
2+ 2η′2
tσ2(strong convexity of ℓ)
≤(1−µη′
t)E∥zt−z∗∥2
2+ 2η′2
tσ2
where in Eq. (10) we use the smoothness of ℓt.
Theorem 4.2. Assuming that (i) ℓitisL-smooth and convex, (ii) ℓisµ-strongly convex, (iii) z∗= arg min z∈Zℓ(z)(iv) and
that for all t,ϵt≤ϵ,Titerations of SSO result in the following bound for zT=f(θT),
E∥zT+1−z∗∥ ≤YT
i=1ρi
∥z1−z∗∥2
2
+ 2σ2XT
t=1YT
i=t+1ρiη′
t21/2
+ 2ϵXT
t=1YT
i=t+1ρi,
where ρt= (1−µη′
t),σ2:=Eh
∥∇ℓ(z∗)− ∇ℓt(z∗)∥2
2i
,
(a)Constant step-size : When ηt=1
2L nand for ρ= 1−µ
2L,
E∥zT+1−z∗∥ ≤ ∥ z1−z∗∥
1−1
2κT
2
+σ√µL+2ϵ
1−q
1−1
2κ.
(b)Exponential step-size : When ηt=1
2L nαtforα= (β
T)1
T,
E∥zT+1−z∗∥ ≤c1exp
−T
4κα
ln(T/β)
∥z1−z∗∥
+4κc1(ln(T/β))
Leα√
Tσ+ 2ϵ c2.
where c1= exp
1
4κ2β
ln(T/β)
, and c2= exp
βln(T)
2κln(T/β)
Proof. Using Lemma D.2 and Lemma D.3 we have
E∥zt+1−z∗∥2
2≤E∥¯zt+1−z∗∥2
2+ 2E[ϵt+1∥zt+1−z∗∥]
≤(1−µη′
t)|{z}
ρtE∥zt−z∗∥2
2+ 2η′2
tσ2+ 2E[ϵt+1∥zt+1−z∗∥]
Recursing from t= 1toT,
E[∥zT+1−z∗∥2
2]≤ TY
t=1ρt!
∥z1−z∗∥2
2+ 2σ2TX
t=1TY
i=t+1ρiη′
t2+ 2TX
t=1TY
i=t+1ρiE[ϵt+1∥zt+1−z∗∥]
18Target-based Surrogates for Stochastic Optimization
Denote ut:=E∥zt−z∗∥. By applying Jensen’s inequality we know that u2
T≤E[∥zT−z∗∥2
2].
=⇒u2
T+1≤ TY
t=1ρt!
u2
1+ 2σ2TX
t=1TY
i=t+1ρiη′
t2+ 2ϵTX
t=1TY
i=t+1ρiut+1
Dividing both sides in the previous inequality byQT
i=1ρileads to:
 TY
i=1ρi!−1
u2
T+1≤u2
1+ 2σ2 TY
i=1ρi!−1TX
t=1η′
t2 TY
i=t+1ρi!
+ 2ϵ TY
i=1ρi!−1TX
t=1ut+1 TY
i=t+1ρi!
Simplify the above inequality for a generic τ≤T,

 τY
i=1ρi!−1
2
uτ+1
2
≤u2
1+ 2σ2τX
t=1η′
t2 tY
i=1ρi!−1
+τX
t=12ϵ tY
i=1ρi!−1
2
 tY
i=1ρi!−1
2
ut+1

Letvτ:= (Qτ
i=1ρi)−1
2uτ+1andSτ:=u2
1+ 2σ2Pτ
t=1η′
t2Qt
i=1ρi−1
. Let us also denote λt:= 2ϵQt
i=1ρi−1
2.
Observe that S0=u2
1=v2
0andSτ+1=Sτ+2σ2η′
t2
τ+1Qτ+1
i=1ρi−1
. Therefore Sτis an increasing sequence. Re-writing
the previous inequality using the new variables leads to the following inequality:
v2
τ≤Sτ+τX
t=1λtvt
Using the result from Lemma D.10 we have:
vτ≤1
2τX
t=1λt+
Sτ+ 
1
2τX
t=1λt!2
1
2
≤τX
t=1λt+p
Sτ ( using√
a+b≤√a+√
bfora, b≥0)
Writing the inequality above using the original variables results in:
 τY
i=1ρi!−1
2
uτ+1≤τX
t=12ϵ tY
i=1ρi!−1
2
+
u2
1+ 2σ2τX
t=1η′
t2 tY
i=1ρi!−1
1
2
uτ+1≤2ϵτX
t=1 tY
i=1ρi!−1
2 τY
i=1ρi!1
2
+u1 τY
i=1ρi!1
2
+√
2σ
τX
t=1η′
t2 tY
i=1ρi!−1
1
2 τY
i=1ρi!1
2
(a)Constant step size : Choosing a constant step size η′
t=η=1
2Limplies ρi=ρ= 1−µη= 1−µ
2L. Plugging this into
the previous inequality leads to:
uτ+1≤2ϵρτ
2τX
t=1ρ−t
2+u1ρτ
2+√
2ση ρτ
2vuutτX
t=1ρ−t
≤2ϵ
1−√ρ+u1ρτ
2+√
2ση√1−ρ(applying the formula for finite geometric series)
E∥zT+1−z∗∥ ≤ ∥ z1−z∗∥
1−1
2κT
2
+σ√µL+2ϵ
1−q
1−1
2κ.
19Target-based Surrogates for Stochastic Optimization
(b)Exponential step size .
Starting from Eq. (10) and using the proof from Vaswani et al. (2022), we have
E∥¯zt+1−z∗∥2
2≤E∥zt−z∗∥2
2+ 4η′2
tLE{ℓt(zt)−ℓt(z∗)− ⟨∇ ℓt(z∗), zt−z∗⟩} −2η′
t⟨zt−z∗,∇ℓ(zt)
− ∇ℓ(z∗)⟩+ 2η′2
tσ2
≤E∥zt−z∗∥2
2+α2t
LE{ℓt(zt)−ℓt(z∗)− ⟨∇ ℓt(z∗), zt−z∗⟩} −αt
L⟨zt−z∗,∇ℓ(zt)
− ∇ℓ(z∗)⟩+ 2η′2
tσ2( using η′
t=αt
2L)
≤E∥zt−z∗∥2
2+αt
L{ℓ(zt)−ℓ(z∗)− ⟨∇ ℓ(z∗), zt−z∗⟩} −αt
L⟨zt−z∗,∇ℓ(zt)− ∇ℓ(z∗)⟩+ 2η′2
tσ2
( using αt≤1)
=E∥zt−z∗∥2
2+αt
L{ℓ(zt)−ℓ(z∗)− ⟨∇ ℓ(zt), zt−z∗⟩}+ 2η′2
tσ2
≤E∥zt−z∗∥2
2−µαt
2LE∥zt−z∗∥2
2+ 2η′2
tσ2=
1−1
2καt
E∥zt−z∗∥2
2+ 2η′2
tσ2
(using strong convexity of ℓ)
Combining the above with Lemma D.2 we get:
E∥zt+1−z∗∥2
2≤
1−1
2καt
E∥zt−z∗∥2
2+ 2η′2
tσ2+ 2E[ϵt+1∥zt+1−z∗∥]
≤exp
−1
2καt
E∥zt−z∗∥2
2+σ2
2L2α2t+ 2E[ϵt+1∥zt+1−z∗∥] (1−x≤exp(−x))
≤exp
−1
2καt
E∥zt−z∗∥2
2+σ2
2L2α2t+ 2ϵE[∥zt+1−z∗∥] (ϵt+1≤ϵ)
Unrolling the recursion starting from t= 1toT, denoting ut:=E∥zt−z∗∥and applying Jensen’s inequality to deduce
that that u2
T+1≤E∥zT+1−z∗∥2
2, we get that,
u2
T+1≤u2
1exp
−1
2κTX
t=1αt
+σ2
2L2TX
t=1α2texp
−1
2κTX
i=t+1αi
+ 2ϵTX
t=1exp
−1
2κTX
i=t+1αi
ut+1
By multiplying both sides by exp
1
2κPT
t=1αt
we have:

exp1
4κTX
t=1αt
uT+12
≤u2
1+σ2
2L2TX
t=1α2texp1
2κtX
i=1αi
+ 2ϵTX
t=1exp1
2κtX
i=1αi
ut+1.
Now let us define vτ:= exp
1
4κPτ
t=1αt
uτ+1,Sτ:=u2
1+σ2
2L2Pτ
t=1α2texp
1
2κPt
i=1αi
andλt:=
2ϵexp
1
4κPt
i=1αi
. Note that Sτis increasing and S0=u2
1=v2
0≥0andλt>0,vτ>0. By applying Lemma D.10,
similar to the fixed step-size case, for τ=T, we get:
exp1
4κTX
t=1αt
uT+1≤
u2
1+σ2
2L2TX
t=1α2texp1
2κtX
i=1αi 1/2
+ 2ϵTX
t=1exp1
4κtX
i=1αi
20Target-based Surrogates for Stochastic Optimization
Multiplying both sides by exp
−1
4κPT
t=1αt
gives us
uT+1≤
exp
−1
2κTX
t=1αt
|{z}
:=A
u2
1+σ2
2L2TX
t=1α2texp1
2κTX
i=t+1αi
| {z }
:=BT1/2
+ 2ϵTX
t=1exp
−1
4κTX
i=t+1αi
| {z }
CT
=
u2
1exp
−1
2κA
+σ2
2L2BT1/2
+ 2ϵ CT
To bound A, we use Lemma D.6 and get
u2
1exp
−1
2κA
≤ ∥z1−z∗∥2
2exp1
2κ2β
ln(T/β)
| {z }
:=c2
1exp
−T
2κα
ln(T/β)
To bound BTwe use Lemma D.7
BT≤16κ2c2
1(ln(T/β))2
e2α2T
Finally using Lemma D.8 to bound CTwe get
E∥zT+1−z∗∥ ≤
c2
1exp
−T
2κα
ln(T/β)
∥z1−z∗∥2
2+16κ2c2
1(ln(T/β))2
2L2e2α2Tσ21/2
+ 2ϵexpβln(T)
2κln(T/β)
| {z }
c2
=⇒E∥zT+1−z∗∥ ≤c1exp
−T
4κα
ln(T/β)
∥z1−z∗∥+4κc1(ln(T/β))
Leα√
Tσ+ 2ϵ c2.
21Target-based Surrogates for Stochastic Optimization
D.3. Controlling the Projection Error
Let us recall the following definitions for the theoretical analysis:
˜θt+1:= arg min
θ˜gt(θ);˜gt(θ) :=ℓit(zt) +∂ℓit(zt)
∂zit
fit(θ)−zit
t
+1
2ηt
fit(θ)−zit
t2;˜zt+1=f(θt+1)
¯θt+1:= arg min
θ˜qt(θ);˜qt(θ) :=ℓit(zt) +∂ℓit(zt)
∂zit
fit(θ)−zit
t
+1
2η′
t∥f(θ)−zt∥2
2;¯zt+1=f(¯θt+1)
θ′
t+1:= arg min
θgt(θ);gt(θ) :=1
n""X
iℓi(zt) +⟨∇ℓi(zt), f(θ)−zt⟩#
+1
2ηt∥f(θ)−zt∥2
2#
zt+1=f(θt+1),
where θt+1is obtained by running mtiterations of GD on ˜gt(θ). We will use these definitions to prove the following
proposition to control the projection error in each iteration.
Proposition 4.3. Assuming that (i) fisLf-Lipschitz continuous, (ii) ˜gtisµgstrongly-convex and Lgsmooth
with κg:=Lg/µg7, (iii) ˜qtisµqstrongly-convex (iv) ζ2
t:=8
min{µg,µq}([min{Eit[˜gt]} −Eit[min{˜gt}]])
+
8
min{µg,µq}[min{Eit[˜qt]} −Eit[min{˜qt}]]
, (v) σ2
z:= min z∈Z{Eit[ℓit(z)]} −Eit[min z∈Z{ℓit(z)}], if
˜gtis minimized using mtinner-loops of GD with the appropriate step-size, then, E[ϵ2
t+1]≤L2
fζ2
t+
4L2
f
µg
exp (−mt/κg)
E[ℓ(zt)−ℓ(z∗)] +σ2
z
.
Proof. Since we obtain θt+1by minimizing ˜gt(θ)using mtiterations of GD starting from θt, using the convergence
guarantees of gradient descent (Nesterov, 2003),
˜θt+1−θt+12
2≤exp (−mt/κg)˜θt+1−θt2
2
θt+1−¯θt+12
2=θt+1−˜θt+1+˜θt+1−¯θt+12
2≤2θt+1−˜θt+12
2+ 2˜θt+1−¯θt+12
2
(∥a+b∥2
2≤2∥a∥2
2+ 2∥b∥2
2)
≤2 exp ( −mt/κg)˜θt+1−θt2
2+ 2˜θt+1−¯θt+12
2
≤4
µgexp (−mt/κg) [˜gt(θt)−˜gt(˜θt+1)] + 2˜θt+1−¯θt+12
2
Taking expectation w.r.t it,
Eitθt+1−¯θt+12
2≤4
µgh
exp (−mt/κg)Eit[˜gt(θt)−˜gt(˜θt+1)]i
+ 2E˜θt+1−¯θt+12
2
Let us first simplify E˜θt+1−¯θt+12
2.
E˜θt+1−¯θt+12
2=E˜θt+1−θ′
t+1+θ′
t+1−¯θt+12
2
≤2E˜θt+1−θ′
t+12
2+ 2Eθ′
t+1−¯θt+12
2
≤4
µgE[˜gt(θ′
t+1)−˜gt(˜θt+1)] +4
µqE[˜qt(θ′
t+1)−˜qt(¯θt+1)]
=4
µg[min{Eit[˜gt]} −Eit[min{˜gt}]] +4
µq[min{Eit[˜qt]} −Eit[min{˜qt}]]
(Since E[˜q] =E[˜g] =g)
7We consider strongly-convex functions for simplicity, and it should be possible to extend these results to when ˜gis non-convex but
satisfies the PL inequality, or is weakly convex.
22Target-based Surrogates for Stochastic Optimization
2E˜θt+1−¯θt+12
2≤8
min{µg, µq}([min{Eit[˜gt]} −Eit[min{˜gt}]] + [min {Eit[˜qt]} −Eit[min{˜qt}]])
| {z }
:=ζ2
t
=⇒2E˜θt+1−¯θt+12
2≤ζ2
t
Using the above relation,
Eitθt+1−¯θt+12
2≤4
µgh
exp (−mt/κg)Eit[˜gt(θt)−˜gt(˜θt+1)]i
+ζ2
t
≤4
µgh
exp (−mt/κg)Eit[ht(θt)−ht(˜θt+1)]i
+ζ2
t
(Since ˜gt(θt) =ht(θt)and˜gt(θ)≥ht(θ)for all θ)
=4
µgh
exp (−mt/κg)Eit[ht(θt)−h∗
t+h∗
t−ht(˜θt+1)]i
+ζ2
t (h∗
t:= min θht(θ))
≤4
µg
exp (−mt/κg)Eit[ht(θt)−h∗
t] +ζ2
t
(Since h∗
t≤ht(θ)for all θ)
=4
µg[exp ( −mt/κg) [Eit[ht(θt)−ht(θ∗)] +Eit[ht(θ∗)−h∗
t]]] +ζ2
t
=4
µg[exp ( −mt/κg) [Eit[ℓt(zt)−ℓt(z∗)] +Eit[ℓt(z∗)−ℓ∗
t]]] +ζ2
t
(Since h(θ) =ℓ(f(θ)) =ℓ(z))
Eit[θt+1−¯θt+12
2]≤4
µg[exp ( −mt/κg) [Eit[ℓt(zt)−ℓt(z∗)] +Eit[ℓt(z∗)−ℓ∗
t]]] +ζ2
t
≤4
µg
exp (−mt/κg)
[ℓ(zt)−ℓ(z∗)] +Eit[ℓt(z∗)−ℓ∗
t]|{z }
:=σ2z

+ζ2
t
(Since both ztandz∗are independent of the randomness in ℓtandEit[ℓt] =ℓ)
=⇒E[θt+1−¯θt+12
2]≤4
µg
exp (−mt/κg)
ℓ(zt)−ℓ(z∗) +σ2
z
+ζ2
t
Now, we will bound E[ϵ2
t+1]by using the above inequality and the Lipschitzness of f.
E[ϵ2
t+1] =∥zt+1−¯zt+1∥2
2=f(θt+1)−f(¯θt+1)2
2≤L2
fθt+1−¯θt+12
2(Since fisLf-Lipschitz)
=⇒E[ϵ2
t+1]≤4L2
f
µg
exp (−mt/κg)
ℓ(zt)−ℓ(z∗) +σ2
z
+L2
fζ2
t
Taking expectation w.r.t the randomness from iterations k= 0tot,
E[ϵ2
t+1]≤4L2
f
µg
exp (−mt/κg)
E[ℓ(zt)−ℓ(z∗)] +σ2
z
+L2
fζ2
t
23Target-based Surrogates for Stochastic Optimization
D.4. Example to show the necessity of ζ2term
Proposition 4.4. Consider minimizing the sum h(θ) :=h1(θ)+h2(θ)
2of two one-dimensional quadratics, h1(θ) :=1
2(θ−1)2
andh2(θ) =1
2(2θ+1/2)2, using SSO withmt=∞andηt=c αtfor any sequence of αtand any constant c∈(0,1].
SSO results in convergence to a neighbourhood of the solution, specifically, if θ∗is the minimizer of handθ1>0, then,
E(θT−θ∗)≥min 
θ1,3
8
.
Proof. Let us first compute θ∗:= arg min h(θ).
h(θ) =1
4(θ−1)2+1
4
2θ+1
22
=5
4θ2+1
4+1
16⇒θ∗= 0
Forh1,
ℓ1(z) =1
2(z−1)2where z=θ
˜gt(θ) :=1
2(θt−1)2+ (θt−1) (θt−θ) +1
2ηt(θ−θt)2
Ifmt=∞,SSO will minimize ˜gtexactly. Since ∇˜gt(θt+1) = 0 ,
=⇒1
ηt(θt+1−θt) =−(θt−1) =⇒θt+1=θt−ηt(θt−1) =⇒θt+1=θt−ηt∇h1(θt)
Similarly, for h2,
ℓ2(z) =1
2(z+1/2)2where z= 2θ
˜gt(θ) :=1
2(2θt+1/2)2+ (2θt+1/2) (2θt−2θ) +1
2ηt(2θ−2θt)2
Ifmt=∞,SSO will minimize ˜gtexactly. Since ∇˜gt(θt+1) = 0 ,
=⇒4
ηt(θt+1−θt) =−(4θt+ 1) = ⇒θt+1=θt−ηt(θt+1/4) =⇒θt+1=θt−ηt
4∇h2(θt)
Ifit= 1andη=c αt
θt+1=θt−c αt(θt−1) = c αt+ (1−cαt)θt
Ifik= 2,
θt+1=θt−c αt2
4(2θt+1
2) = (1 −c αt)θt−1
4c αt
Then
Eθt+1= (1−c αt)θt+1
2c αt−1
8c αt= (1−c αt)θt+3
8c αt
and
EθT=E(θT−θ∗) = (θ1−θ∗)TY
t=1(1−c αt) +3
8TX
t=12(1−c)αtTY
i=t+1(1−c αi)
Using Lemma D.9 and the fact that c αt≤1for all t, we have that if θ1−θ∗=θ1>0, then,
E(θT−θ∗)≥min
θ1,3
8
.
24Target-based Surrogates for Stochastic Optimization
D.5. Helper Lemmas
The proofs of Lemma D.4, Lemma D.6, and Lemma D.7 can be found in (Vaswani et al., 2022).
Lemma D.4. For all x >1,
1
x−1≤2
ln(x)
Proof. Forx >1, we have
1
x−1≤2
ln(x)⇐⇒ ln(x)<2x−2
Define f(x) = 2 x−2−ln(x). We have f′(x) = 2−1
x. Thus for x≥1, we have f′(x)>0sofis increasing on [1,∞).
Moreover we have f(1) = 2 −2−ln(1) = 0 which shows that f(x)≥0for all x >1and ends the proof.
Lemma D.5. For all x, γ > 0,
exp(−x)≤γ
exγ
Proof. Letx >0. Define f(γ) = γ
exγ−exp(−x). We have
f(γ) = exp ( γln(γ)−γln(ex))−exp(−x)
and
f′(γ) =
γ·1
γ+ ln( γ)−ln(ex)
exp (γln(γ)−γln(ex))
Thus
f′(γ)≥0⇐⇒ 1 + ln( γ)−ln(ex)≥0⇐⇒ γ≥exp (ln( ex)−1) = x
Sofis decreasing on (0, x]and increasing on [x,∞). Moreover,
f(x) =x
exx
−exp(−x) =1
ex
−exp(−x) = 0
and thus f(γ)≥0for all γ >0which proves the lemma.
Lemma D.6. Assuming α <1we have
A:=TX
t=1αt≥αT
ln(T/β)−2β
ln(T/β)
Proof.
TX
t=1αt=α−αT+1
1−α=α
1−α−αT+1
1−α
We have
αT+1
1−α=αβ
T(1−α)=β
T·1
1/α−1≤β
T·2
ln(1/α)=β
T·2
1
Tln(T/β)=2β
ln(T/β)(11)
25Target-based Surrogates for Stochastic Optimization
where in the inequality we used Lemma D.4 and the fact that 1/α>1. Plugging back into Awe get,
A≥α
1−α−2β
ln(T/β)
≥α
ln(1/α)−2β
ln(T/β)(1−x≤ln(1
x))
=αT
ln(T/β)−2β
ln(T/β)
Lemma D.7. Forα=
β
T1/T
and any κ >0,
TX
t=1α2texp 
−1
2κTX
i=t+1αi!
≤16κ2c2(ln(T/β))2
e2α2T
where c2= exp
1
2κ2β
ln(T/β
Proof. First, observe that,
TX
i=t+1αi=αt+1−αT+1
1−α
We have
αT+1
1−α=αβ
T(1−α)=β
T·1
1/α−1≤β
T·2
ln(1/α)=β
T·2
1
Tln(T/β)=2β
ln(T/β)
where in the inequality we used D.4 and the fact that 1/α>1. These relations imply that,
TX
i=t+1αi≥αt+1
1−α−2β
ln(T/β)
=⇒exp 
−1
2κTX
i=t+1αi!
≤exp
−1
2καt+1
1−α+1
2κ2β
ln(T/β)
=c2exp
−1
2καt+1
1−α
We then have
TX
t=1α2texp 
−1
2κTX
i=t+1αi!
≤c2TX
t=1α2texp
−1
2καt+1
1−α
≤c2TX
t=1α2t2(1−α)2κ
eαt+12
(Lemma D.5)
=16κ2c2
e2α2T(1−α)2
≤16κ2c2
e2α2T(ln(1/α))2
=16κ2c2(ln(T/β))2
e2α2T
26Target-based Surrogates for Stochastic Optimization
Lemma D.8. Forα=
β
T1/T
and any ζ >0,
TX
t=1exp 
−1
ζTX
i=tαi!
≤exp2βln(T)
ζln(T/β)
Proof. First, observe that,
TX
i=tαi=αt−αT+1
1−α≥ −αT+1
1−α
We have
αT+1
1−α=αβ
T(1−α)=β
T·1
1/α−1≤β
T·2
ln(1/α)=β
T·2
1
Tln(T/β)=2β
ln(T/β)
where in the inequality we used Lemma D.4 and the fact that 1/α>1. Using the above bound we have
TX
t=1exp 
−1
ζTX
i=tαi!
≤TX
t=1exp2β
ζln(T/β)
= exp2βln(T)
ζln(T/β)
Lemma D.9. For any sequence αt
TY
t=1(1−αt) +TX
t=1αtTY
i=t+1(1−αi) = 1
Proof. We show this by induction on T. For T= 1,
(1−α1) +α1= 1
Induction step:
T+1Y
t=1(1−αt) +T+1X
t=1αtT+1Y
i=t+1(1−αi) = (1 −αT+1)TY
t=1(1−αt) + 
αT+1+TX
t=1αtT+1Y
i=t+1(1−αi)!
= (1−αT+1)TY
t=1(1−αt) + 
αT+1+ (1−αT+1)TX
t=1αtTY
i=t+1(1−αi)!
= (1−αT+1)
TY
t=1(1−αt) +TX
t=1αtTY
i=t+1(1−αi)
| {z }
=1
+αT+1
(Induction hypothesis)
= (1−αT+1) +αT+1= 1
27Target-based Surrogates for Stochastic Optimization
Lemma D.10. (Schmidt et al., 2011, Lemma 1). Assume that the non-negative sequence {vτ}forτ≥1satisfies the
following recursion:
v2
τ≤Sτ+τX
t=1λtvt,
where {Sτ}is an increasing sequence, such that S0≥v2
0andλt≥0. Then for all τ≥1,
vτ≤1
2τX
t=1λt+
Sτ+ 
1
2τX
t=1λt!2
1/2
.
Proof. We prove this lemma by induction. For τ= 1we have
v2
1≤S1+λ1v1=⇒(v1−λ1
2)2≤S1+λ2
1
4=⇒v1≤ 
S1+1
2λ12!1/2
+1
2λ1
Inductive hypothesis : Assume that the conclusion holds for τ∈ {1, . . . , k }. Specifically, for τ=k,
vk≤1
2kX
t=1λt+
Sk+ 
1
2kX
t=1λt!2
1/2
.
Now we show that the conclusion holds for τ=k+ 1. Define ψk:=Pk
t=1λt. Note that ψk+1≥ψksince λt≥0for all t.
Hence, vk≤1
2ψk+
Sk+ 1
2ψk21/2
. Define k∗:= arg maxτ∈{0,...,k}vτ. Using the main assumption for τ=k+ 1,
v2
k+1≤Sk+1+k+1X
t=1λtvt
=⇒v2
k+1−λk+1vk+1≤Sk+1+kX
t=1λtvt
=⇒
vk+1−λk+1
22
≤Sk+1+λ2
k+1
4+kX
t=1λtvt
≤Sk+1+λ2
k+1
4+vk∗kX
t=1λt (since vk∗is the maximum)
=Sk+1+λ2
k+1
4+vk∗ψk (based on the definition for ψk)
Since k∗≤k, by the inductive hypothesis,

vk+1−λk+1
22
≤Sk+1+λ2
k+1
4+ψk

1
2ψk∗+ 
Sk∗+1
2ψk∗2!1/2


=Sk+1+λ2
k+1
4+1
2ψkψk∗+ψk 
Sk∗+1
2ψk∗2!1/2
≤Sk+1+λ2
k+1
4+1
2ψ2
k+ψk 
Sk∗+1
2ψk∗2!1/2
(Since {ψτ}is non-decreasing and k∗≤k)
28Target-based Surrogates for Stochastic Optimization
Furthermore, since {Sτ}is increasing and k∗< k+ 1,

vk+1−λk+1
22
≤Sk+1+λ2
k+1
4+1
2ψ2
k+ψk 
Sk+1+1
2ψk∗2!1/2
≤Sk+1+λ2
k+1
4+1
2ψ2
k+ψk 
Sk+1+1
2ψk+12!1/2
(Since {ψτ}is non-decreasing and k∗< k+ 1)
=Sk+1+λ2
k+1
4+1
4ψ2
k+ψk 
Sk+1+1
2ψk+12!1/2
+1
4ψ2
k
≤Sk+1+1
4ψ2
k+1+ψk 
Sk+1+1
2ψk+12!1/2
+1
4ψ2
k
(a2+b2≤(a+b)2fora=ψk>0andb=λk+1>0)
=Sk+1+1
4ψ2
k+1
|{z }
:=x2+ψk 
Sk+1+1
2ψk+12!1/2
| {z }
=2xy+1
4ψ2
k
|{z}
:=y2
=
 
Sk+1+1
2ψk+12!1/2
+1
2ψk
2
=⇒vk+1−λk+1
2≤ 
Sk+1+1
2ψk+12!1/2
+1
2ψk
=⇒vk+1≤ 
Sk+1+1
2ψk+12!1/2
+1
2ψk+λk+1
2
= 
Sk+1+1
2ψk+12!1/2
+1
2ψk+1
where replacing ψk+1with its definition gives us the required result.
29Target-based Surrogates for Stochastic Optimization
E. Additional Experimental Results
Supervised Learning: We evaluate our framework on the LibSVM benchmarks (Chang and Lin, 2011), a standard suite
of convex-optimization problems. Here consider two datasets – mushrooms , and rcv1 , two losses – squared loss and
logistic loss, and four batch sizes – {25, 125, 625, full-batch} for the linear parameterization ( f=X⊤θ). Each optimization
algorithm is run for 500epochs (full passes over the data).
E.1. Stochastic Surrogate Optimization
Comparisons of SGD,SLS,Adam ,Adagrad , andSSO evaluated on three SVMLib benchmarks mushrooms ,ijcnn , and
rcv1 , two losses – squared loss and logistic loss, and four batch sizes – {25, 125, 625, full-batch}. Each run was evaluated
over three random seeds following the same initialization scheme. All plots are in log-log space to make trends between
optimization algorithms more apparent. All algorithms and batch sizes are evaluated for 500 epochs and performance is
represented as a function of total optimization steps. We compare stochastic surrogate optimization ( SSO), against SGD
with the standard theoretical 1/2Lθstep-size, SGD with the step-size set according to a stochastic line-search (Vaswani et al.,
2019b) SLS, and finally Adam (Kingma and Ba, 2015) using default hyper-parameters. Since SSO is equivalent to projected
SGD in the target space, we set η(in the surrogate definition) to 1/2Lwhere Lis the smoothness of ℓw.r.tz. For squared loss,
Lis therefore set to 1, while for logistic it is set to 2. These figures show (i) SSO improves over SGD when the step-sizes are
set theoretically, (ii) SSO is competitive with SLS orAdam , and (iii) as mincreases, on average, the performance of SSO
improves as projection error decreases. For further details see the attached code repository. Below we include three different
step-size schedules: constant,1√
t(Orabona, 2019), and (1/T)t/T(Vaswani et al., 2022).
103104105102
100batch-size: 25
102103104102
100batch-size: 125
101102103102
100batch-size: 625
100101102102
100
mushroomsfull-batch
104105101
7×102
8×102
9×102
103104105101
7×102
8×102
9×102
102103104101
100101102101
ijcnn
103104105103
102
101
103104105103
102
101
102103104103
102
101
100101102101
rcv1
SGD Adam SLS Adagrad SSO-1 SSO-5 SSO-10 SSO-20
Figure 4: Constant step-size: comparison of optimization algorithms under a mean squared error loss . We note, SSO
significantly outperforms its parametric counterpart, and maintains performance which is on par with both SLS andAdam .
Additionally we note that taking additional steps in the surrogate generally improves performance.
30Target-based Surrogates for Stochastic Optimization
103104105103
100batch-size: 25
102103104103
100batch-size: 125
101102103103
100batch-size: 625
100101102103
101
mushroomsfull-batch
1041052×101
3×101
4×101
6×101
1031041052×101
3×101
4×101
6×101
1021031041001011022×101
3×101
4×101
6×101
ijcnn
103104105102
101
103104105102
101
102103104102
101
100101102101
2×101
3×101
4×101
6×101
rcv1
SGD Adam SLS Adagrad SSO-1 SSO-5 SSO-10 SSO-20
Figure 5: Constant step-size: comparison of optimization algorithms under a average logistic loss . We note, SSO
significantly outperforms its parametric counterpart, and maintains performance which is on par with both SLS andAdam .
Additionally we note that taking additional steps in the surrogate generally improves performance
103104105102
100batch-size: 25
102103104102
100batch-size: 125
101102103102
100batch-size: 625
100101102102
101
100
mushroomsfull-batch
104105101
7×102
8×102
9×102
103104105101
7×102
8×102
9×102
102103104101
100101102101
7×102
8×102
9×102
ijcnn
103104105103
102
101
103104105103
102
101
102103104103
102
101
100101102101
rcv1
SGD Adam SLS Adagrad SSO-1 SSO-5 SSO-10 SSO-20
Figure 6: Decreasing step-size: comparison of optimization algorithms under a mean squared error loss . We compare
examples which include a decaying step size of1√
talongside both SSO as well as SGD andSLS.Adam (andAdagrad ).
Again, we note that taking additional steps in the surrogate generally improves performance. Additionally the decreasing
step-size seems to help maintain strict monotonic improvement.
31Target-based Surrogates for Stochastic Optimization
103104105103
100batch-size: 25
102103104103
100batch-size: 125
101102103102
101
batch-size: 625
100101102103
100
mushroomsfull-batch
1041052×101
3×101
4×101
6×101
1031041052×101
3×101
4×101
6×101
1021031041001011022×101
3×101
4×101
6×101
ijcnn
103104105102
101
103104105102
101
102103104102
101
1001011022×101
3×101
4×101
6×101
rcv1
SGD Adam SLS Adagrad SSO-1 SSO-5 SSO-10 SSO-20
Figure 7: Decreasing step-size: comparison of optimization algorithms under a logistic loss . We compare examples which
include a decaying step size of1√
talongside both SSO as well as SGD andSLS.Adam (andAdagrad ). Again, we note
that taking additional steps in the surrogate generally improves performance. Additionally the decreasing step-size seems to
help maintain strict monotonic improvement.
103104105102
100batch-size: 25
102103104102
100batch-size: 125
101102103102
100batch-size: 625
100101102102
101
100
mushroomsfull-batch
104105101
7×102
8×102
9×102
103104105101
7×102
8×102
9×102
102103104101
100101102101
7×102
8×102
9×102
ijcnn
103104105103
102
101
103104105103
102
101
102103104103
102
101
100101102101
rcv1
SGD Adam SLS Adagrad SSO-1 SSO-5 SSO-10 SSO-20
Figure 8: Exponential step-size: comparison of optimization algorithms under a mean squared error loss . We compare
examples which include a decaying step size of (1
T)t/Talongside both SSO as well as SGD andSLS.Adam . Again, we
note that taking additional steps in the surrogate generally improves performance. Additionally the decreasing step-size
seems to help maintain strict monotonic improvement. Lastly, because of a less aggressive step size decay, the optimization
algorithms make more progress then their stochastic1√
tcounterparts.
32Target-based Surrogates for Stochastic Optimization
103104105102
100batch-size: 25
102103104102
100batch-size: 125
101102103102
100batch-size: 625
100101102102
101
100
mushroomsfull-batch
104105101
7×102
8×102
9×102
103104105101
7×102
8×102
9×102
102103104101
100101102101
7×102
8×102
9×102
ijcnn
103104105103
102
101
103104105103
102
101
102103104103
102
101
100101102101
rcv1
SGD Adam SLS Adagrad SSO-1 SSO-5 SSO-10 SSO-20
Figure 9: Exponential step-size: comparison of optimization algorithms under a logistic loss . We compare examples which
include a decaying step size of (1
T)t/Talongside both SSO as well as SGD andSLS.Adam remains the same as aboves.
Again, we note that taking additional steps in the surrogate generally improves performance. Additionally the decreasing
step-size seems to help maintain strict monotonic improvement. Lastly, because of a less aggressive step size decay, the
optimization algorithms make more progress then their stochastic1√
tcounterparts.
E.2. Stochastic Surrogate Optimization with a Line-search
Comparisons of SGD,SLS,Adam ,Adagrad , and SSO-SLS evaluated on three SVMLib benchmarks mushrooms ,
ijcnn , andrcv1 . Each run was evaluated over three random seeds following the same initialization scheme. All plots are
in log-log space to make trends between optimization algorithms more apparent. As before in all settings, algorithms use
either their theoretical step-size when available, or the default as defined by (Paszke et al., 2019). The inner-optimization
loop are set according to line-search parameters and heuristics following Vaswani et al. (2019b). All algorithms and batch
sizes are evaluated for 500 epochs and performance is represented as a function of total optimization steps.
103104105102
100batch-size: 25
102103104102
100batch-size: 125
101102103102
100batch-size: 625
100101102102
100
mushroomsfull-batch
104105101
7×102
8×102
9×102
103104105101
7×102
8×102
9×102
102103104101
100101102101
7×102
8×102
9×102
ijcnn
103104105103
102
101
103104105103
102
101
102103104103
102
101
100101102101
rcv1
SGD Adam SLS Adagrad SSO-1 SSO-5 SSO-10 SSO-20
Figure 10: Constant step-size: comparison of optimization algorithms under a mean squared error loss . We note,
SSO-SLS outperforms its parametric counterpart, and maintains performance which is on par with both SLS andAdam .
Additionally we note that taking additional steps in the surrogate generally improves performance, especially in settings
with less noise (full-batch and batch-size 625).
33Target-based Surrogates for Stochastic Optimization
103104105103
100batch-size: 25
102103104103
100batch-size: 125
101102103104
101
batch-size: 625
100101102103
101
mushroomsfull-batch
1041052×101
3×101
4×101
6×101
1031041052×101
3×101
4×101
6×101
1021031041001011022×101
3×101
4×101
6×101
ijcnn
103104105102
101
103104105102
101
102103104102
101
100101102101
rcv1
SGD Adam SLS Adagrad SSO-1 SSO-5 SSO-10 SSO-20
Figure 11: Constant step-size: comparison of optimization algorithms under a average logistic loss . We note, SSO-SLS
outperforms its parametric counterpart, and maintains performance which is on par with both SLS andAdam . Additionally
we note that taking additional steps in the surrogate generally improves performance.
E.3. Combining Stochastic Surrogate Optimization with Adaptive Gradient Methods
Comparisons of SGD,SLS,Adam ,Adagrad , andSSO-Adagrad evaluated on three SVMLib benchmarks mushrooms ,
ijcnn , andrcv1 . Each run was evaluated over three random seeds following the same initialization scheme. All plots are
in log-log space to make trends between optimization algorithms more apparent. As before in all settings, algorithms use
either their theoretical step-size when available, or the default as defined by (Paszke et al., 2019). The inner-optimization
loop are set according to line-search parameters and heuristics following Vaswani et al. (2019b). All algorithms and batch
sizes are evaluated for 500 epochs and performance is represented as a function of total optimization steps. Here we update
theηaccording to the same schedule as scalar Adagrad (termed AdaGrad-Norm in Ward et al. (2020)). Because Adagrad
does not have an easy to compute optimal theoretical step size, for our setting we set the log learning rate (the negative of
logη) to be 2.. For further details see the attached coding repository.
103104105102
100batch-size: 25
102103104102
100batch-size: 125
101102103102
100batch-size: 625
100101102102
100
mushroomsfull-batch
104105101
7×102
8×102
9×102
103104105101
7×102
8×102
9×102
102103104101
100101102101
7×102
8×102
9×102
ijcnn
103104105103
102
101
103104105103
102
101
102103104103
102
101
100101102101
rcv1
SGD Adam SLS Adagrad SSO-1 SSO-5 SSO-10 SSO-20
Figure 12: Comparison in terms of average MSE loss of SGD,SLS,Adam , and SSO-Adagrad evaluated under a
mean squared error loss . These plots show that SSO-Adagrad outperforms its parametric counterpart, and maintains
performance which is on par with both SLS andAdam . Additionally, we again find that taking additional steps in the
surrogate generally improves performance.
34Target-based Surrogates for Stochastic Optimization
103104105103
100batch-size: 25
102103104103
100batch-size: 125
101102103103
100batch-size: 625
100101102103
101
mushroomsfull-batch
1041052×101
3×101
4×101
6×101
1031041052×101
3×101
4×101
6×101
1021031041001011022×101
3×101
4×101
6×101
ijcnn
103104105102
101
103104105102
101
102103104102
101
1001011022×101
3×101
4×101
6×101
rcv1
SGD Adam SLS Adagrad SSO-1 SSO-5 SSO-10 SSO-20
Figure 13: Comparison in terms of average MSE loss of SGD,SLS,Adam , andSSO-Adagrad evaluated under a logistic
loss. These plots show that SSO-Adagrad outperforms its parametric counterpart, and maintains performance which is on
par with both SLS andAdam . Additionally, we again find that taking additional steps in the surrogate generally improves
performance.
E.4. Combining Stochastic Surrogate Optimization With Online Newton Steps
Comparisons of SGD,SLS,Adam ,Adagrad , andSSO-Newton evaluated on three SVMLib benchmarks mushrooms ,
ijcnn , andrcv1 . Each run was evaluated over three random seeds following the same initialization scheme. All plots are
in log-log space to make trends between optimization algorithms more apparent. As before, in all settings, algorithms use
either their theoretical step-size when available, or the default as defined by (Paszke et al., 2019). The inner-optimization
loop are set according to line-search parameters and heuristics following Vaswani et al. (2019b). All algorithms and batch
sizes are evaluated for 500 epochs and performance is represented as a function of total optimization steps. Here we update
theηaccording to the same schedule as Online Newton . We omit the MSE example as SSO-Newton in this setting is
equivalent to SSO. In the logistic loss setting however, which is displayed below, we re-scale the regularization term by
(1−p)pwhere p=σ(f(x))where σis this sigmoid function, and fis the target space. This operation is done per-data
point, and as can be seen below, often leads to extremely good performance, even in the stochastic setting. In the plots below,
if the line vanishes before the maximum number of optimization steps have occurred, this indicates that the algorithm has
converged to the minimum and is no longer executed. Notably, SSO-Newton achieves this in for multiple data-sets and
batch sizes.
103104105105
101
batch-size: 25
102103104103
100batch-size: 125
101102103104
101
batch-size: 625
100101102103
101
mushroomsfull-batch
104105100
2×101
3×101
4×101
6×101
1031041052×101
3×101
4×101
6×101
102103104100101102101103
ijcnn
103104105102
101
103104105102
101
102103104102
101
100101102102
101
rcv1
SGD Adam SLS Adagrad SSO-1 SSO-5 SSO-10 SSO-20
Figure 14: Comparison in terms of average logistic loss of SGD,SLS,Adam , andSSO-Newton evaluated on the logistic
loss. This plot displays that significant improvement can be made at no additional cost by re-scaling the regularization term
correctly. Note that in the case of mushrooms, SSO-Newton in all cases for m= 20 reaches the stopping criteria before
the 500th epoch. Second, even in many stochastic settings, SSO-Newton outperforms both SLS andAdam .
35Target-based Surrogates for Stochastic Optimization
E.5. Comparison with SVRG
Below we include a simple supervised learning example in which we compare the standard variance reduction
algorithm SVRG (Johnson and Zhang, 2013) to the SSO algorithm in the supervised learning setting. Like
the examples above, we test the optimization algorithms using the Chang and Lin (2011) rcv1 dataset, under
an MSE loss. This plot shows that SSO can outperform SVRG for even small batch-size settings. For addi-
tional details on the implementation and hyper-parameters used, see https://github.com/WilderLavington/
Target-Based-Surrogates-For-Stochastic-Optimization . We note that it is likely, that for small enough
batch sizes SVRG may become competitive with SSO, however we leave such a comparison for future work.
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0
Optimization-Steps (105)102
101
Lossbatch-size: 25
0 1 2 3 4 5 6 7 8
Optimization-Steps (104)102
101
batch-size: 125
0 2 4 6 8 10 12 14 16
Optimization-Steps (103)102
101
batch-size: 625
0 1 2 3 4 5
Optimization-Steps (102)101
SSO-SGDfull-batch
SGD SVRG SSO-1 SSO-10 SSO-20
Figure 15: Comparison of the average mean-squared error between SGD,SVRG , andSSO-SGD . This plot displays that even
in settings where the batch-size is small, SSO can even improve over variance reduction techniques like SVRG.
36Target-based Surrogates for Stochastic Optimization
E.6. Imitation Learning
0 10000 20000 30000 40000 500005.756.006.256.506.757.007.257.50Behavioral Cloning: 
0 10000 20000 30000 40000 500006.757.007.257.507.758.008.25
Log-Policy-LossOnline Imitation: 
0 10000 20000 30000 40000 50000
T otal-Examples0100200300400500600
0 10000 20000 30000 40000 50000
T otal-Examples0100200300400500600
Policy-Return
(a) Walker2d-v2 Environment
0 10000 20000 30000 40000 500005.005.255.505.756.006.256.506.75Behavioral Cloning: 
0 10000 20000 30000 40000 500005.56.06.57.07.5
Log-Policy-LossOnline Imitation: 
0 10000 20000 30000 40000 50000
T otal-Examples100200300400500600
0 10000 20000 30000 40000 50000
T otal-Examples0100200300400500600
Policy-ReturnAdagrad
Adam
SLS
SSO-1
SSO-10
SSO-100
SSO-1000 (b) Hopper-v2 Environment
Figure 16: Comparison of policy return, and log policy loss incurred by SGD,SLS,Adam ,Adagrad , andSSO as a function
of the total interactions. Unlike Section 5, the mean of the policy is parameterized by a neural network model . In both
environments, for both behavioral policies, SSO outperforms all other online-optimization algorithms. Additionally, as m in
increases, so to does the performance of SSO in terms of both the return as well as the loss.
0 10000 20000 30000 40000 500006.66.87.07.27.47.6Behavioral Cloning: 
0 10000 20000 30000 40000 500007.07.27.47.67.88.08.2
Policy-LossOnline Imitation: 
0 10000 20000 30000 40000 50000
T otal-Examples0100200300400500
0 10000 20000 30000 40000 50000
T otal-Examples0100200300400
Policy-Return
(a) Walker2d-v2 Environment
0 10000 20000 30000 40000 500006.06.26.46.66.8Behavioral Cloning: 
0 10000 20000 30000 40000 500006.06.26.46.66.87.07.27.4
Policy-LossOnline Imitation: 
0 10000 20000 30000 40000 50000
T otal-Examples50100150200250300350
0 10000 20000 30000 40000 50000
T otal-Examples50100150200250300350
Policy-ReturnAdagrad
Adam
SLS
SSO-1
SSO-10
SSO-100
SSO-1000 (b) Hopper-v2 Environment
Figure 17: Comparison of policy return, and log policy loss incurred by SGD,SLS,Adam ,Adagrad , andSSO as a function
of the total interactions. Unlike Section 5, the mean of the policy is parameterized by a linear model . In both environments,
for both behavioral policies, SSO outperforms all other online-optimization algorithms. Additionally, as m in increases, so
to does the performance of SSO in terms of both the return as well as the loss.
Comparisons of Adagrad ,SLS,Adam , and SSO evaluated on two Mujoco (Todorov et al., 2012) imitation learning
benchmarks (Lavington et al., 2022), Hopper-v2 , and Walker-v2 . In this setting training and evaluation proceed in
rounds. At every round, a behavioral policy samples data from the environment, and an expert labels that data. The goal
is guess at the next stage (conditioned on the sampled states) what the expert will label the examples which are gathered.
Here, unlike the supervised learning setting, we receive a stream of new data points which can be correlated and drawn
from following different distributions through time. Theoretically this makes the optimization problem significantly more
difficult, and because we must interact with a simulator, querying the stochastic gradient can be expensive. Like the example
in Appendix E, in this setting we will interact under both the experts policy distribution (behavioral cloning), as well as the
policy distribution induced by the agent (online imitation). We parameterize a standard normal distribution whose mean is
learned through a mean squared error loss between the expert labels and the mean of the agent policy. Again, the expert is
trained following soft-actor-critic. All experiments were run using an NVIDIA GeForce RTX 2070 graphics card. with a
AMD Ryzen 9 3900 12-Core Processor.
37Target-based Surrogates for Stochastic Optimization
0 100k 200k 300k 400k 500k
T otal-Examples6.66.87.07.27.4Log-LossBehavioral Cloning: 
0 100k 200k 300k 400k 500k
T otal-Examples7.07.27.47.67.88.08.2
LinearOnline Imitation: 
Adagrad Adam SLS SSO-1 SSO-10 SSO-100 SSO-1000
Figure 18: Comparison of log policy loss incurred by SGD,SLS,Adam ,Adagrad , and SSO as a function of the total
interactions for the Walker2d gym environment. Unlike Section 5, the mean of the policy is parameterized by a linear
model . Unlike Fig. 17 and Fig. 16,this plot displays 500 thousand iterations instead of only 50. We again see that SSO
outperforms all other online-optimization algorithms, even for a very large number of iterations. Additionally, as m in
increases, we again see the performance of SSO log loss improves as well.
In this this setting we evaluate two measures: the per-round log-policy loss, and the policy return. The log policy loss is as
described above, while the return is a measure of how well the imitation learning policy actually solves the task. In the
Mujoco benchmarks, this reward is defined as a function of how quickly the agent can move in space, as well as the power
exerted to move. Imitation learning generally functions by taking a policy which has a high reward (e.g. can move through
space with very little effort in terms of torque), and directly imitating it instead of attempting to learn a cost to go function as
is done in RL (Sutton et al., 2000).
Each algorithm is evaluated over three random seeds following the same initialization scheme. As before, in all settings,
algorithms use either their theoretical step-size when available, or the default as defined by (Paszke et al., 2019). The
inner-optimization loop is set according to line-search parameters and heuristics following Vaswani et al. (2019b). All
algorithms and batch sizes are evaluated for 50 rounds of interaction (accept in the case of Fig. 18, which is evaluated for
500) and performance is represented as a function of total interactions with the environment. Below we learn a policy which
is parameterized by a two layer perception with 256 hidden units and relu activations (as was done in the main paper). For
further details, please see the attached code repository.
38