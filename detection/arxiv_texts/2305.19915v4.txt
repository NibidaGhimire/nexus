Source Code Data Augmentation for Deep Learning: A Survey
Terry Yue Zhuo1,2†, Zhou Yang3, Zhensu Sun3,
Yufei Wang4,Li Li5,Xiaoning Du1,Zhenchang Xing2,6,David Lo3
1Monash University2CSIRO’s Data613Singapore Management University
4Huawei Noah’s Ark Lab5Beihang University6Australian National University
terry.zhuo@monash.edu
Abstract
The increasingly popular adoption of deep
learning models in many critical source code
tasks motivates the development of data aug-
mentation (DA) techniques to enhance training
data and improve various capabilities (e.g., ro-
bustness and generalizability) of these models.
Although a series of DA methods have been
proposed and tailored for source code models,
there lacks a comprehensive survey and exam-
ination to understand their effectiveness and
implications. This paper fills this gap by con-
ducting a comprehensive and integrative survey
of data augmentation for source code, wherein
we systematically compile and encapsulate ex-
isting literature to provide a comprehensive
overview of the field. We start with an intro-
duction of data augmentation in source code
and then provide a discussion on major repre-
sentative approaches. Next, we highlight the
general strategies and techniques to optimize
the DA quality. Subsequently, we underscore
techniques useful in real-world source code
scenarios and downstream tasks. Finally, we
outline the prevailing challenges and potential
opportunities for future research. In essence,
we aim to demystify the corpus of existing lit-
erature on source code DA for deep learning,
and foster further exploration in this sphere.
Complementing this, we present a continually
updated GitHub repository that hosts a list of
update-to-date papers on DA for source code
modeling, accessible at https://github.
com/terryyz/DataAug4Code .
1 Introduction
Data augmentation (DA) is a technique used to
increase the variety of training examples without
collecting new data. It has gained popularity in
recent machine learning (ML) research, with meth-
ods like back-translation (Sennrich et al., 2015;
†Corresponding author.Shiri et al., 2022), Mixup (Zhang et al., 2018),
and synthetic audio (Asyrofi et al., 2021) being
widely adopted in natural language processing
(NLP), computer vision (CV), and speech recog-
nition. These techniques have significantly im-
proved the performance of data-centric models in
low-resource domains. For example, Fadaee et al.
(2017) obtain substantial improvements for low-
resource machine translation via DA, where the
translation system is trained with the bilingual pairs
synthesized from a limited training corpus.
However, DA has not yet been fully explored
in source code modeling, which is the intersec-
tion of ML and software engineering (SE). Source
code modeling is an emerging area that applies ML
techniques to solve various source code tasks such
as code completion (Yin and Neubig, 2017), code
summarization (McBurney and McMillan, 2014),
and defect detection (Wang et al., 2016), by training
models on a vast amount of data available in open-
source repositories (Allamanis et al., 2017). Source
code data typically has two modalities: the pro-
gramming language (e.g., Python and Java) and the
natural language (e.g., doc-strings and code com-
ments), which complement each other. Such dual-
modality nature of source code data presents unique
challenges in tailoring DA for NLP to source code
models. For example, the context of a sentence
can be relatively standalone or derived from a few
surrounding sentences in many NLP tasks (Feng
et al., 2021). However, in source code, the context
can span across multiple functions or even different
files, due to the widespread use of function calls,
object-oriented programming, and modular design.
Therefore, we argue that DA methods for source
code would need to take this extended context into
account, to avoid introducing errors or changing
the original program’s behavior. In addition, source
code follows strict syntactic rules that are specified
using context-free grammar. Consequently, con-
ventional NLP data augmentation methods, such asarXiv:2305.19915v4  [cs.CL]  13 Nov 20232019 2020 2021 2022 2023
Year05101520253035Number of Publications
26162936Publications per YearFigure 1: Yearly publications on the topic of
“Source Code DA for Deep Learning”. Data
Statistics as of November 2023.
ArXiv
15.9%
ICSE10.2%
EMNLP6.8%
ACL5.7%
ICLR4.5%Publications per VenueFigure 2: Venue Distribution of the collected
publications.
token substitution with similar words, may make
the augmented source code fail to compile and in-
troduce erroneous knowledge for training models.
Despite such challenges, there has been increas-
ing interest and demand for DA for source code
modeling. With the growing accessibility of large,
off-the-shelf, pre-trained source code models via
learning from large-scale corpora (Chen et al.,
2021; Li et al., 2023b; Allal et al., 2023), there
is a growing focus on applying these models to
real-world software development. For instance,
(Husain et al., 2019) observed that many program-
ming languages are low-resource, emphasizing the
importance of DA to improve model performance
and robustness on unseen data.
This study aims to bring attention from both
ML and SE communities to this emerging field.
As depicted in Figure 1, the relevant publications
have been increasing in the recent five years. More
precisely, we have compiled a list of 89 core pa-
pers from the past five years, mainly from premier
conferences and journals in both the ML and SE
disciplines as shown in Figure 2 (with 62 out of
89 papers published in Core Rank A/A* venues1).
Given the escalating interest and burgeoning re-
search in this domain, it is timely for our survey
to (1) provide a comprehensive overview of DA
for source code models, and (2) pinpoint key chal-
lenges and opportunities to stimulate and guide
further exploration in this emerging field. To the
best of our awareness, our paper constitutes the first
1We refer to the venues listed at http://portal.
core.edu.au/conf-ranks/ and http://portal.
core.edu.au/jnl-ranks/ .comprehensive survey offering an in-depth exami-
nation of DA techniques for source code models.
The structure of this paper is organized as fol-
lows:
•Section 3 offers a thorough review of three
categories of DA for source code modeling:
rule-based (3.1), model-based (3.2), and ex-
ample interpolation-based (3.3) techniques.
•Section 4 provides a summary of prevalent
strategies and techniques designed to enhance
the quality of augmented data, encompassing
method stacking (4.1) and optimization (4.2).
•Section 5 articulates various beneficial source
code scenarios for DA, including adversarial
examples for robustness (5.1), low-resource
domains (5.2), retrieval augmentation (5.3),
and contrastive learning (5.4).
•Section 6 delineates DA methodologies for
common source code tasks, such as code
authorship attribution (6.1), clone detection
(6.2), defect detection and repair (6.3), code
summarization (6.4), code search (6.5), code
completion (6.6), code translation (6.7), code
question answering (6.8), problem classifica-
tion (6.9), method name prediction (6.10), and
type prediction (6.11).
•Section 7 expounds on the challenges and fu-
ture prospects in the realm of DA for source
code modeling.Through this work, we hope to emulate prior sur-
veys which have analyzed DA techniques for other
data types, such as text (Feng et al., 2021), time
series (Wen et al., 2020), and images (Shorten and
Khoshgoftaar, 2019). Our intention is to pique fur-
ther interest, spark curiosity, and encourage further
research in the field of data augmentation, specifi-
cally focusing on its application to source code.
2 Background
2.1 What are source code models?
Source code models are trained on large-scale cor-
pora of source code and therefore able to model
the contextual representations of given code snip-
pets (Allamanis et al., 2017). In the early stage,
researchers have attempted to leverage deep learn-
ing architectures like LSTM (Gu et al., 2016) and
Seq2Seq (Yin and Neubig, 2017) to model the
source code like plain text, and shown that these
models can achieve great performance on specific
downstream tasks of source code. With the de-
velopment of pre-trained language models in NLP,
many pre-trained source code models are proposed
to enhance the source code representations and ef-
ficiently be scaled to any downstream tasks (Feng
et al., 2020; Guo et al., 2021; Nijkamp et al., 2023).
Some of these models incorporate the inherent
structure of code. For example, instead of tak-
ing the syntactic-level structure of source code like
ASTs, Guo et al. (2021) consider program data
flow in the pre-training stage, which is a semantic-
level structure of code that encodes the relation of
“where-the-value-comes-from” between variables.
In this survey, we focus DA methods designed for
all the deep-learning-based source code models.
2.2 What is data augmentation?
Data augmentation (DA) techniques aim to im-
prove the model’s performance in terms of various
aspects (e.g., accuracy and robustness) via increas-
ing training example diversity with data synthesis.
Besides, DA techniques can help avoid model over-
fitting in the training stage, which maintains the
generability of the model. In CV , DA techniques
with predefined rules are commonly adopted when
training models, such as image cropping, image
flipping, and color jittering (Shorten and Khosh-
goftaar, 2019). These techniques can be classi-
fied as rule-based DA. Furthermore, some attempts
like Mixup have been made to create new exam-
ples by fusing multiple examples together, whichis categorized as example interpolation DA. Com-
pared to CV , DA techniques for NLP greatly rely
on language models that can help paraphrase the
given context by word replacing or sentence rewrit-
ing (Feng et al., 2021). As most of these language
models are pre-trained and can capture the seman-
tics of inputs, they serve as reasonable frameworks
to modify or paraphrase the plain text. We denote
such DA methods as model-based DA.
2.3 How does data augmentation work in
source code?
Compared to images and plain texts, source code
is less flexible to be augmented due to the nature
of strict programming syntactic rules. Hence, we
observe that most DA approaches in source code
must follow the predetermined transformation rules
in order to preserve the functionality and syntax of
the original code snippets. To enable the complex
processing of the given source code, a common ap-
proach is to use a parser to build a concrete syntax
tree from the code, which represents the program
grammar in a tree-like form. The concrete syntax
tree will be further transformed into an abstract
syntax tree (AST) to simplify the representation
but maintain the key information such as identi-
fiers, if-else statements, and loop conditions. The
parsed information is utilized as the basis of the
rule-based DA approaches for identifier replace-
ment and statement rewrite (Quiring et al., 2019).
From a software engineering perspective, these DA
approaches can emulate more diverse code repre-
sentation in real-world scenarios and thus make
source code models more robust by training with
the augmented data (Yefet et al., 2020).
3 Source Code Data Augmentation
Methods for Deep Learning
This section categorizes the mainstream DA tech-
niques specifically designed for source code mod-
els into three parts: rule-based, model-based, and
example-interpolation techniques. We explain stud-
ies of different branches as follows.
3.1 Rule-based Techniques
A large number of DA methods utilize predeter-
mined rules to transform the programs without
breaking syntax rules and semantics. Specifically,
these rules mainly implicitly leverage ASTs to
transform the code snippets. The transformations
can include operations such as replacing variableFigure 3: Rule-based DA to transform code snippets,
Wang et al. (2022d).
names, renaming method names, and inserting dead
code. Besides the basic program syntax, some
code transformations consider deeper structural in-
formation, such as control-flow graph (CGF) and
use-define chains (UDG) (Quiring et al., 2019). Ad-
ditionally, a small part of rule-based DA techniques
focuses on augmenting the natural language con-
text in the code snippets, including doc-strings and
comments (Bahrami et al., 2021; Song et al., 2022;
Park et al., 2023). We illustrate a rule-based DA
example relying on program grammars in Figure 3.
Zhang et al. (2020a) propose MHM, a method of
iteratively renaming identifiers in the code snip-
pets. Considered as the approach to generate ex-
amples for adversarial training, MHM greatly im-
proves the robustness of source code models. Later,
Srikant et al. consider program obfuscations as
adversarial perturbations, where they rename pro-
gram variables in an attempt to hide the program’s
intent from a reader. By applying these perturbed
examples to the training stage, the source code
models become more robust to the adversarial at-
tack. Instead of just renaming identifiers, BUGLAB-
Aug (Allamanis et al., 2021) contains more rules
to augment code snippets, emphasizing both the
programming language and natural language, such
as comment deletion, comparison expression mir-
roring, and if-else branch swapping. The evalua-
tion on BUGLAB-Aug demonstrates that DA meth-
ods can be exploited for self-supervised bug detec-
tion and repair. Similarly, Jain et al. (2021) use
compiler transforms as data augmentation, called
Transpiler , automatically generating a dataset
of equivalent functions. Specifically, they define
11 compiler transforms by exploiting ASTs of the
programs. Rule-based DA later has been widely
used for source code models to capture code repre-sentation effectively via contrastive learning (Ding
et al., 2021; Liu et al., 2023b).
Brockschmidt et al. (2019) present a genera-
tive source code model by augmenting the given
AST with additional edges to learn diverse code
expressions. Instead of the direct augmentation
on AST, Quiring et al. (2019) propose three dif-
ferent augmentation schemes via the combination
of AST and CFG, UDG and declaration-reference
mapping (DRM), named as Control Trans-
formations ,Declaration Transforma-
tions andAPI Transformations .Con-
trol Transformations rewrite control-flow
statements or modify the control flow between func-
tions. In total, the family contains 5 transforma-
tions. This transformation involves passing vari-
ables as function arguments, updating their values,
and changing the control flow of the caller and
callee. Declaration Transformations
consist of 14 transformers that modify, add or
remove declarations in source code. Declara-
tion Transformations make DA necessary
to update all usages of variables which can be el-
egantly carried out using the DRM representation.
API Transformations contain 9 transforma-
tions and exploits the fact that various APIs can be
used to solve the same problem. Programmers are
known to favor different APIs and thus tampering
with API usage is an effective strategy for changing
stylistic patterns.
Another line of work is augmenting the natural
language context in source code. QRA (Huang et al.,
2021) augments examples by rewriting natural lan-
guage queries when performing code search and
code question answering. It rewrites queries with
minor rule-based modifications that share the same
semantics as the original one. Specifically, it con-
sists of three ways: randomly deleting a word, ran-
domly switching the position of two words, and ran-
domly copying a word. Inspired by this approach,
Park et al. (2023) recently devised KeyDAC with
an emphasis on the query keywords. KeyDAC aug-
ments on both natural language and programming
language. For natural language query, it follows
the rules in QRA but only modifies non-keywords.
In terms of programming language augmentation,
KeyDAC simply uses ASTs to rename program
variables, similar to the aforementioned works.3.2 Model-based Techniques
A series of DA techniques for source code tar-
get training various models to augment data.
Intuitively, Mi et al. (2021) utilize Auxiliary
Classifier Generative Adversarial Networks ( AC-
GAN) (Odena et al., 2017) to generate augmented
programs. Similarly, Wang et al. (2023a) trained a
generative adversarial network to boost code gener-
ation and code search at the same time. To increase
the training data for code summarization, CDA-
CS(Song et al., 2022) uses the pre-trained BERT
model (Devlin et al., 2019) to replace synonyms for
non-keywords in code comments, which benefits
the source code downstream tasks.
While these methods largely adapt the existing
model-based DA techniques for general purposes,
most DA approaches are specifically designed for
source code models. Li et al. (2022f) introduce
IRGen , a genetic-algorithm-based model using
compiler intermediate representation (LLVM IR)
to augment source code embeddings, where IR-
Gen generates a piece of source code into a range
of semantically identical but syntactically distinct
IR codes for improving model’s contextual under-
standing. Several works (Roziere et al., 2021; Ah-
mad et al., 2023; Silva et al., 2023) have investi-
gated the suitability of the multilingual generative
source code models for unsupervised programming
language translation via Back-translation ,
in the similar scope of the one for NLP (Sen-
nrich et al., 2016). However, unlike the one in
NLP, Back-translation here is defined as
translating between two programming languages
via the natural language as an intermediate lan-
guage. Pinku et al. (2023) exploit another genera-
tive source code model, Transcoder (Roziere et al.,
2020), to perform source-to-source translation for
augmenting cross-language source code.
3.3 Example Interpolation Techniques
Another category of data augmentation (DA) tech-
niques, originated by Mixup (Zhang et al., 2018),
involves interpolating the inputs and labels of two
or more actual examples. For instance, given that a
binary classification task in CV and two images of
a dog and a cat, respectively, these DA approaches
likeMixup can blend these two image inputs and
their corresponding labels based on a randomly se-
lected weight. This collection of methods is also
termed Mixed Sample Data Augmentation. Despite
trials in the context of text classification problems,
Problem: Write a program which printsmultiplication tables in the following format:1x1=1 1x2=2 . . 9x8=72 9x9=81public static void main(String[] args){Scanner scan = new Scanner(System.in);int[] heights = new int[10];for(inti =0; i <10; i++){heights[i] =scan.nextInt();}Arrays.sort(heights);for(inti =9; i >=7; i--){System.out.println(heights[i]);}}Problem: Write a program which prints heights of thetop three mountains in descending order.public static void main(String[] a) throws IOException{BufferedReader input = new BufferedReader(newInputStreamReader(System.in));String s;while((s =input.readLine())!=null){String[] num =s.split(" ",0);intx =parseInt(num[0]);inty =parseInt(num[1]);if(0<= x &&x <=1000000&&0<=y &&y <=1000000){intz =x +y;System.out.println(Integer.toString(z).length());}}}(Program A)(Program B)𝑋":tensor([[-0.0568, 0.0262, -0.2367,  ..., 0.1004, -0.4510, 0.6271],...,[-0.0656, 0.0323, -0.2346,  ..., 0.0892, -0.4559, 0.6335]])tensor([[0.0734,  0.1239, -0.3143,  ...,  0.1549, -0.4469,  0.6513],...,[0.1298,  0.0127,  0.3154,  ..., -0.6284, -0.7741,  1.0450]])tensor([[0.0474,  0.1044, -0.2988,  ...,  0.1440, -0.4477,  0.6465],...,[0.0908,  0.0166,  0.2054,  ..., -0.4849, -0.7104,  0.9627]])tensor([[0.2000,  0.8000,  0.0000,  ...,0.0000,  0.0000,  0.0000]])𝑋#:X	:Y	:𝑌":𝑌#:tensor([[1.0000,  0.0000,  0.0000,  ...,0.0000,  0.0000,  0.0000]])tensor([[0.0000,  1.0000,  0.0000,  ...,0.0000,  0.0000,  0.0000]])DataADataBDataCFigure 4: MixCode , Dong et al. (2023a).
such methods are hard to be deployed in the realm
of source code, as each code snippet is constrained
by its unique program grammar and functionality.
In contrast to the aforementioned surface-level
interpolation, the majority of example-interpolation
DA methods are enhanced to fuse multiple real
examples into a single input via model embed-
dings (Feng et al., 2021). As an illustration in
Figure 4, Dong et al. (2023a) merge rule-based
techniques for source code models with Mixup
to blend the representations of the original code
snippet and its transformation. This approach is
commonly regarded as the linear interpolation tech-
nique deployed in NLP classification tasks.
Li et al. (2022a) introduce two novel interpola-
tion techniques for source code models, namely
Binary Interpolation andLinear Ex-
trapolation .Binary Interpolation
serves as a data augmentation strategy, which inter-
changeably swaps features between samples using
elements acquired from a Bernoulli distribution.
On the other hand, Linear Extrapolation is
another data augmentation approach that generates
new data points beyond the existing feature space
by extending current features in accordance with a
uniform distribution.
4 Strategies and Techniques
In real-world applications, the design and efficacy
of DA techniques for source code models are influ-
enced by a variety of factors, such as computing
cost, example diversity, and models’ robustness.
This section highlights these factors, offering in-
sights and techniques for devising and optimizing
suitable DA methods.4.1 Method Stacking
As discussed in Section 3, numerous DA strategies
are proposed concurrently in a single work, aiming
to enhance the models’ performance. [Add one
sentence to define method stacking] Typically, the
combination entails two types: same-type DA or
a mixture of different DA methods. The former
is typically applied in rule-based DA techniques,
stemming from the realization that a single code
transformation cannot fully represent the diverse
code style and implementation found in the real
world.
Several works (Shi et al., 2023; Huang et al.,
2021) demonstrate that merging multiple types
of DA techniques can enhance the performance
of source code models. Mi et al. (2021) com-
bined rule-based code transformation schemes
with model-based DA using AC-GAN to create
an augmented corpus for model training. Instead
of augmenting on programming language, CDA-
CS(Song et al., 2022) encompasses two kinds of
DA techniques: rule-based non-keyword extraction
and model-based non-keyword replacement. Em-
pirical evidence from Chen and Lampouras (2023)
shows that combining Back-translation and
variable renaming can result in improved code com-
pletion performance.
4.2 Optimization
In certain scenarios such as enhancing robustness
and minimizing computational cost, optimally se-
lecting specific augmented example candidates is
crucial. We denote such goal-oriented candidate
selections in DA as optimization . Subsequently,
we introduce three types of strategies: probabilis-
tic, model-based, and rule-based selection. Prob-
abilistic selection is defined as the optimization
via sampling from a probability distribution, while
model-based selection is guided by the model to
select the most proper examples. In terms of rule-
based selection, it is an optimization strategy where
specific predetermined rules or heuristics are used
to select the most suitable examples.
4.2.1 Probabilistic Selection
We introduce three representative probabilistic se-
lection strategies, MHM,QMDP , and BUGLAB-Aug .
MHM (Zhang et al., 2020a) adopts the Metropolis-
Hastings probabilistic sampling method, which
is a Markov Chain Monte Carlo technique, to
choose adversarial examples via identifier replace-
ment. Similarly, QMDP (Tian et al., 2021) uses aQ-learning approach to strategically select and ex-
ecute rule-based structural transformations on the
source code, thereby guiding the generation of ad-
versarial examples. In BUGLAB-Aug , Allamanis
et al. (2021) model the probability of applying a
specific rewrite rule at a location in a code snippet
similar to the pointer net (Merity et al.).
4.2.2 Model-based Selection
Several DA techniques employing this strategy use
the model’s gradient information to guide the se-
lection of augmented examples. An emblematic
approach is the DAMP method (Yefet et al., 2020),
which optimizes based on the model loss to select
and generate adversarial examples via variable re-
naming. Another variant, SPACE (Li et al., 2022c),
performs selection and perturbation of code identi-
fiers’ embeddings via gradient ascent, targeting to
maximize the model’s performance impact while
upholding semantic and grammatical correctness of
the programming language. A more complex tech-
nique, ALERT (Yang et al., 2022b), uses a genetic
algorithm in its gradient-based selection strategy.
It evolves a population of candidate solutions itera-
tively, guided by a fitness function that calculates
the model’s confidence difference, aiming to iden-
tify the most potent adversarial examples.
4.2.3 Rule-based Selection
Rule-based selection stands as a powerful ap-
proach, featuring predetermined fitness functions
or rules. This method often relies on evaluation
metrics for decision-making. For instance, IR-
Gen (Li et al., 2022f) utilizes a Genetic-Algorithm-
based optimization technique with a fitness func-
tion based on IR similarity. On the other hand,
ACCENT (Zhou et al., 2022) and RADAR apply eval-
uation metrics such as BLEU (Papineni et al., 2002)
and CodeBLEU (Ren et al., 2020) respectively
to guide the selection and replacement process,
aiming for maximum adversarial impact. Finally,
STRATA (Springer et al., 2021) employs a rule-
based technique to select high-impact subtokens
that significantly alter the model’s interpretation of
the code.
5 Scenarios
This section delves into several commonplace sce-
narios of source code scenarios, where DA ap-
proaches can be applied.DA Method Category PL NL Optimization Preprocess Parsing Level TA LA
ComputeEdge (Brockschmidt et al., 2019) Rule ✓ ✗ — — AST AST ✓ ✓
RefineRepresentation (Bielik and Vechev, 2020) Rule ✓ ✗ Model — AST AST ✓ ✓
Control Transformations (Quiring et al., 2019) Rule ✓ ✗ Prob — AST+CFG+UDG Input ✓ ✗
Declaration Transformations (Quiring et al., 2019) Rule ✓ ✗ Prob — AST+DRM Input ✓ ✗
API Transformations (Quiring et al., 2019) Rule ✓ ✗ Prob — AST+CFG+DRM Input ✓ ✗
DAMP (Yefet et al., 2020) Rule ✓ ✗ Model — AST Input ✓ ✓
IBA (Huang et al., 2021) Rule ✗ ✓ — Tok — Embed ✗ ✓
QRA (Huang et al., 2021) Rule ✓ ✗ — Tok — Input ✗ ✓
MHM (Zhang et al., 2020a) Rule ✗ ✓ Prob — AST Input ✓ ✗
Mossad (Devore-McDonald and Berger, 2020) Rule ✓ ✗ Rule Tok AST Input ✓ ✓
AugmentedCode (Bahrami et al., 2021) Rule ✓ ✗ — Tok — Input ✗ ✓
QMDP (Tian et al., 2021) Rule ✓ ✗ Prob Tok AST Input ✓ ✗
Transpiler (Jain et al., 2021) Rule ✓ ✗ Prob — AST Input ✓ ✗
BUGLAB-Aug (Allamanis et al., 2021) Rule ✓ ✗ Prob Tok AST Input ✗ ✓
SPAT (Yu et al., 2022b) Rule ✓ ✗ Model — AST Input ✓ ✗
RoPGen (Li et al., 2022d) Rule ✓ ✗ Model — AST Input ✓ ✗
ACCENT (Zhou et al., 2022) Rule ✓ ✗ Rule — AST Input ✓ ✓
SPACE (Li et al., 2022c) Rule ✓ ✗ Model Tok AST Embed ✓ ✓
ALERT (Yang et al., 2022b) Rule ✓ ✗ Model Tok AST Input ✓ ✓
IRGen (Li et al., 2022f) Rule ✓ ✗ Rule — AST+IR IR ✓ ✓
Binary Interpolation (Li et al., 2022a) EI ✓ ✓ — — — Embeb ✓ ✓
Linear Extrapolation (Li et al., 2022a) EI ✓ ✓ — — — Embeb ✓ ✓
Gaussian Scaling (Li et al., 2022a) Rule ✓ ✓ Model — — Embeb ✓ ✓
CodeTransformator (Zubkov et al., 2022) Rule ✓ ✗ Rule — AST Input ✓ ✗
RADAR (Yang et al., 2022a) Rule ✓ ✗ Rule — AST Input ✓ ✗
AC-GAN (Mi et al., 2021) Model ✓ ✗ — — — Input ✓ ✓
CDA-CS (Song et al., 2022) Model ✗ ✓ Model KWE — Input ✗ ✓
srcML-embed (Li et al., 2022e) Rule ✓ ✗ — — AST Embed ✓ ✗
MultIPA (Orvalho et al., 2022) Rule ✓ ✗ — — AST Input ✓ ✗
ProgramTransformer (Rabin and Alipour, 2022) Rule ✓ ✗ — — AST Input ✓ ✗
Back-translation (Ahmad et al., 2023) Model ✓ ✗ — Tok — Input ✗ ✓
MixCode (Dong et al., 2023a) Rule+EI ✓ ✓ — — — Embed ✓ ✓
NP-GD (Shen et al.) Model ✓ ✗ Model Tok — Embed ✓ ✓
ExploitGen (Yang et al., 2023a) Rule ✗ ✓ — — — Input ✓ ✗
SoDa (Shi et al., 2023) Model ✓ ✓ — — AST Input ✓ ✓
Transcompiler (Pinku et al., 2023) Model ✓ ✗ — — — Input ✓ ✗
STRATA (Springer et al., 2021) Rule ✓ ✗ Model Tok AST Input ✓ ✓
KeyDAC (Park et al., 2023) Rule ✓ ✓ — KWE AST Embed ✗ ✓
Simplex Interpolation (Zhang et al., 2022) EI ✓ ✗ — — AST+IR Embed ✗ ✓
Table 1: Comparing a selection of DA methods by various aspects relating to their applicability, dependencies,
and requirements. PL,NL,TA,LA,EI,Prob ,Tok, and KWE stand for Programming Language, Natural Language,
Example Interpolation, Probability, Tokenization, Keyword Extraction, Task-Agnostic, and Language-Agnostic. PL
andNLdetermine if the DA method is applied to the programming language or natural language context. Preprocess
denotes preprocessing required besides the program parsing. Parsing refers to the type of feature used by the DA
method during program parsing. Level denotes the depth at which data is modified by the DA. TAandLArepresent
whether the DA method can be applied to different tasks or programming languages. As most papers do not clearly
state if their DA methods are TAandLA, we subjectively denote the applicability.
5.1 Adversarial Examples for Robustness
Robustness presents a critical and complex dimen-
sion of software engineering, necessitating the cre-
ation of semantically-preserved adversarial exam-
ples to discern and mitigate vulnerabilities within
source code models. There is a surge in designing
more effective DA techniques for generating these
examples in recent years. Several studies (Yefet
et al., 2020; Li et al., 2022d; Srikant et al.; Li et al.,
2022c; Anand et al.; Henke et al., 2022; Tian et al.,
2023; Yang et al., 2023b; Li et al., 2023c; Gu et al.,
2023; Gao et al., 2023) have utilized various DA
methods for testing and enhancing model robust-
ness. Wang et al. (2023b) have gone a step further
to consolidate universally accepted code transfor-mation rules to establish the first benchmark for
source code model robustness.
5.2 Low-Resource Domains
In the domain of software engineering, the re-
sources of programming languages are severely
imbalanced (Orlanski et al., 2023). While some
most popular programming languages like Python
and Java play major roles in the open-source repos-
itories, many languages like Rust (Wu et al., 2023)
are starkly low-resource. As source code mod-
els are trained on open-source repositories and fo-
rums, the programming language resource imbal-
ance can adversely impact their performance on
the resource-scarce programming languages. Fur-
thermore, the application of DA methods withinlow-resource domains is a recurrent theme within
the CV and NLP communities (Shorten and Khosh-
goftaar, 2019; Feng et al., 2021). Yet, this scenario
remains underexplored within the source code dis-
cipline.
In order to increase data in the low-resource do-
main for representation learning, Li et al. (2022f)
tend to add more training data to enhance source
code model embeddings by unleashing the power
of compiler IR. Ahmad et al. (2023) propose
to use source code models to perform Back-
translation DA, taking into consideration
the scenario of low-resource programming lan-
guages. Meanwhile, Chen and Lampouras (2023)
underscore the fact that source code datasets
are markedly smaller than their NLP equivalents,
which often encompass millions of instances. As
a result, they commence investigations into code
completion tasks under this context and experiment
withBack-translation and variable renam-
ing. Shen et al. contend that the generation of bash
comments is hampered by a dearth of training data
and thus explore model-based DA methods for this
task.
5.3 Retrieval Augmentation
Increasing interest has been observed in the ap-
plication of DA for retrieval augmentation within
NLP (Mialon et al., 2023) and source code (Lu
et al., 2022). These retrieval augmentation frame-
works for source code models incorporate retrieval-
augmented examples from the training set when
pre-training or fine-tuning source code models.
This form of augmentation enhances the param-
eter efficiency of models, as they are able to store
less knowledge within their parameters and instead
retrieve it. It is shown as a promising application
of DA in various source code downstream tasks,
such as code summarization (Zhang et al., 2020b;
Liu et al.; Yu et al., 2022a; Choi et al., 2023; Ye
et al., 2023), code completion (Parvez et al., 2021;
Tang et al., 2023) and program repair (Nashid et al.,
2023; Jin et al., 2023; Wang et al., 2023c).
5.4 Contrastive Learning
Another source code scenario to deploy DA meth-
ods is contrastive learning, where it enables mod-
els to learn an embedding space in which similar
samples are close to each other while dissimilar
ones are far apart (Chen et al., 2022; Wang et al.,
2022c; Zhang et al., 2022; Yang et al., 2023c; Ding
et al., 2023). As the training datasets commonlycontain limited sets of positive samples, DA meth-
ods are preferred to construct similar samples as
the positive ones. Liu et al. (2023b) make use
of contrastive learning with DA to devise supe-
rior pre-training paradigms for source code mod-
els, while some works study the advantages of
this application in some source code tasks like
defect detection (Cheng et al., 2022), clone de-
tection (Zubkov et al., 2022; Wang et al., 2022a)
and code search (Shi et al., 2022b, 2023; Li et al.,
2022b).
6 Downstream Tasks
In this section, we discuss several DA works for
common source code tasks and evaluation datasets.
6.1 Code Authorship Attribution
Code authorship attribution is the process of identi-
fying the author of a given code, usually achieved
by source code models. Yang et al. (2022b) initially
investigate generating adversarial examples on the
Google Code Jam (GCJ) dataset, which effectively
fools source code models to identify the wrong
author of a given code snippet. By training with
these augmented examples, the model’s robustness
can be further improved. Li et al. (2022d) propose
another DA method called RoPGen for the adver-
sarial attack and demonstrate its efficacy on GCJ.
Dong et al. (2023b) empirically study the effective-
ness of several existing DA approaches for NLP
on several source code tasks, including authorship
attribution on GCJ.
6.2 Clone Detection
Code clone detection refers to the task of identify-
ing if the given code snippet is cloned and modified
from the original sample, and can be called plagia-
rism detection in some cases (Zubkov et al., 2022;
Pinku et al., 2023; Hasija et al., 2023). This is a
challenging downstream task as it needs the source
code model to understand the source code both
syntactically and semantically. Jain et al. (2021)
propose correct-by-construction DA via compiler
information to generate many variants with equiva-
lent functionality of the training sample and show
its effectiveness of improving the model robust-
ness on BigCloneBench (Svajlenko et al., 2014)
and a self-collected JavaScript dataset. Jia et al.
(2023) show that when training with adversarial
examples via obfuscation transformation, the ro-
bustness of source code models can be significantlyimproved. Zubkov et al. (2022) provide the compar-
ison of multiple contrastive learning frameworks,
combined with rule-based transformations for the
clone detection task. Pinku et al. (2023) later
useTranscompiler to translate between lim-
ited source code in Python and Java and increase
the training data for cross-language code clone de-
tection.
6.3 Defect Detection and Repair
Defect Detection and Repair, in other words, bug
or vulnerability detection and fix, is to capture
the bugs in given code snippets (Cheng et al.,
2022; Dong et al., 2023a) and generate repaired
versions (Drain et al., 2021; Yasunaga and Liang,
2021; Wang et al., 2022b). The task can be consid-
ered as the binary classification task, where the la-
bels are either true or false. Allamanis et al. (2021)
implement BUGLAB-Aug , a DA framework of self-
supervised bug detection and repair. BUGLAB-
Aug has two sets of code transformation rules,
one is a bug-inducing rewrite and the other one
is rewriting as DA. Their approach boosts the per-
formance and robustness of source code models
simultaneously. Cheng et al. (2022) present a path-
sensitive code embedding technique called Con-
traFlow , which uses self-supervised contrastive
learning to detect defects based on value-flow
paths. ContraFlow utilizes DA to generate con-
trastive value-flow representations of three datasets
(namely D2A (Zheng et al., 2021), Fan (Fan et al.,
2020) and FFMPeg+Qemu (Zhou et al., 2019)) to
learn the (dis)-similarity among programs. Ding
et al. (2021) present a novel self-supervised model
focusing on identifying (dis)similar functionalities
of source code, which outperforms the state-of-the-
art models on REVEAL (Chakraborty et al., 2022)
andFFMPeg+Qemu (Zhou et al., 2019). Specifi-
cally, they design code transformation heuristics to
automatically create bugged programs and similar
code for augmenting pre-training data.
6.4 Code Summarization
Code summarization is considered as a task that
generates a comment for a piece of the source code,
and is thus also named code comment generation.
(Zhang et al., 2020c) apply MHM to perturb train-
ing examples and mix them with the original ones
for adversarial training, which effectively improves
the robustness of source code models in summariz-
ing the adversarial code snippets. (Zhang et al.,
2020b) develop a retrieval-augmentation frame-work for code summarization, relying on similar
code-summary pairs to generate the new summary
onPCSD andJCSD datasets (Miceli-Barone and
Sennrich, 2017; Hu et al., 2018). Based on this
framework, (Liu et al.) leverage Hybrid GNN to
propose a novel retrieval-augmented code summa-
rization method and use it during model training
on the self-collected CCSD dataset. (Zhou et al.,
2022) generate adversarial examples of a Python
dataset (Wan et al., 2018) and JSCD to evaluate
and enhance the source code model robustness.
6.5 Code Search
Code search, or code retrieval, is a text-code task
that searches code snippets based on the given nat-
ural language queries. The source code models on
this task need to map the semantics of the text to
the source code (Li et al., 2022a, 2023a; Huang
et al., 2023; Ma et al., 2023). Bahrami et al. (2021)
increase the code search queries by augmenting the
natural language context such as doc-string, code
comments and commit messages. Shi et al. (2022b)
use AST-focused DA to replace the function and
variable names of the data in CodeSearchNet (Hu-
sain et al., 2019) and CoSQA (Huang et al., 2021).
Shi et al. (2023) introduce soft data augmentation
(SoDa ), without external transformation rules on
code and text. With SoDa , the model predicts to-
kens based on dynamic masking or replacement
when processing CodeSearchNet . Instead of ap-
plying rule-based DA techniques, Li et al. (2022a)
manipulate the representation of the input data by
interpolating examples of CodeSearchNet .
6.6 Code Completion
Code completion requires source code models to
generate lines of code to complete given program-
ming challenges (Brockschmidt et al., 2019; Lu
et al., 2022; Wang et al., 2022e). Anand et al.
suggest that source code models are vulnerable
to adversarial examples which are perturbed with
transformation rules. (Lu et al., 2022) propose a
retrieval-augmented code completion framework
composed of the rule-based DA module to gener-
ate on PY150 (Raychev et al., 2016) and GitHub
Java Corpus datasets (Allamanis and Sutton, 2013).
Wang et al. (2023b) customize over 30 transforma-
tions specifically for code on docstrings, function
and variable names, code syntax, and code format
and benchmark generative source code models on
HumanEval (Chen et al., 2021) and MBPP (Austin
et al., 2021). Yang et al. (2022a) devise transfor-mations on functional descriptions and signatures
to attack source code models and show that their
performances are susceptible.
6.7 Code Translation
Similar to neural machine translation in
NLP (Stahlberg, 2020), the task is to trans-
late source code written in a specific programming
language translation to another one. Ahmad
et al. (2023) apply data augmentation through
back-translation to enhance unsupervised code
translation. They use pre-trained sequence-to-
sequence models to translate code into natural
language summaries and then back into code
in a different programming language, thereby
creating additional synthetic training data to
improve model performance. Chen and Lampouras
(2023) utilize Back-translation and variable
augmentation techniques to yield the improvement
in code translation on CodeTrans (Lu et al., 2021).
Recently, (Xie et al., 2023) have proposed two
novel data augmentation methods to generate code
translation pairs with similar functionality, via
model training and back-translation.
6.8 Code Question Answering (CQA)
CQA can be formulated as a task where the source
code models are required to generate a textual an-
swer based on given a code snippet and a question.
Huang et al. (2021) incorporate two rule-base DA
methods on code and text to create examples for
contrastive learning. Li et al. (2022c) explore the ef-
ficacy of adversarial training on the continuous em-
bedding space with rule-based DA on CodeQA (Liu
and Wan, 2021), a free-form CQA dataset. Park
et al. (2023) evaluate KeyDAC , a framework using
query writing and variable renaming as DA, on
WebQueryTest of CodeXGLUE (Lu et al., 2021).
Different from CodeQA ,WebQueryTest is a CQA
benchmark only containing Yes/No questions.
6.9 Code Classification
The task performs the categorization of programs
regarding their functionality (Zhang et al., 2020a;
Dong et al., 2023a) or readability (Mi et al., 2021,
2022a,b). Wang et al. (2022c) propose a novel AST
hierarchy representation for contrastive learning
with the graph neural network. Specifically, they
augment the node embeddings in AST paths on
OJ, a dataset containing 104 classes of programs.
Zhang et al. (2022) incorporate simplex interpo-
lation, an example-interpolation DA approach onIR, to create intermediate embeddings on POJ-104
from CodeXGLUE (Lu et al., 2021). Dong et al.
(2023a) also explore the example-interpolation DA
to fuse the embeddings of code snippets. They
evaluate the method on two datasets, JAVA250 and
Python800 (Puri et al., 2021).
6.10 Method Name Prediction
The goal of method name prediction is to predict
the name of a method given the program. Yefet
et al. (2020) attack and defense source code models
by using variable-name-replaced adversarial pro-
grams on the Code2Seq dataset (Alon et al., 2019).
Pour et al. (2021) propose a search-based testing
framework specifically for adversarial robustness.
They generate adversarial examples of Java with
ten popular refactoring operators widely used in
Java. Rabin et al. (2021) and Yu et al. (2022b) both
implement data augmentation frameworks and vari-
ous transformation rules for processing Java source
code on the Code2Seq dataset.
6.11 Type Prediction
Type prediction, or type interference, aims to pre-
dict parameter and function types in programs.
Bielik and Vechev (2020) conduct adversarial at-
tacks on source code models with examples of
transformed ASTs. They instantiate the attack to
type prediction on JavaScript and TypeScript. Jain
et al. (2021) apply compiler transforms to gener-
ates many variants of programs in DeepTyper (Hel-
lendoorn et al., 2018), with equivalent function-
ality with 11 rules. Li et al. (2022e) incorporate
srcML (Collard et al., 2013) meta-grammar embed-
dings to augment the syntactic features of examples
in three datasets, DeepTyper ,Typilus Data (Allama-
nis et al., 2020) and CodeSearchNet (Husain et al.,
2019).
7 Challenges and Opportunities
When it comes to source code, DA faces significant
challenges. Nonetheless, it’s crucial to acknowl-
edge that these challenges pave the way for new
possibilities and exciting opportunities in this area
of work.
Discussion on theory. Currently, there’s a no-
ticeable gap in the in-depth exploration and the-
oretical understanding of DA methods in source
code. Most existing research on DA is centered
around image processing and natural languagefields, viewing data augmentation as a way of ap-
plying pre-existing knowledge about data or task
invariance (Dao et al., 2019; Wu et al., 2020; Shi
et al., 2022a). When shifting to source code, much
of the previous work introduces new methods or
demonstrates how DA techniques can be effective
for subsequent tasks. However, these studies often
overlook why and how particularly from a mathe-
matical perspective. With source code being dis-
crete by nature, having a theoretical discussion be-
comes even more important. It allows us to under-
stand DA from a broader perspective, not just by
looking at experimental results. By exploring DA
in this way, we can better understand its underly-
ing principles without being solely dependent on
experimental validation.
More study on pre-trained models. In recent
years, pre-trained source code models have been
widely applied in source code, containing rich
knowledge through self-supervision on a huge scale
of corpora (Feng et al., 2020; Guo et al., 2021;
Zhuo, 2023). Numerous studies have been con-
ducted utilizing pre-trained source code models
for the purpose of DA, yet, most of these attempts
are confined to mask token replacement (Shi et al.,
2023), direct generation after fine-tuning (Ahmad
et al., 2023; Pinku et al., 2023). An emergent re-
search opportunity lies in exploring the potential of
DA in the source code domain with the help of large
language models (LLMs) trained on a large amount
of text and source code (Chen et al., 2021; Li et al.,
2023b). LLMs have the capability of context gener-
ation based on prompted instructions and provided
examples, making them a choice to automate the
DA process in NLP (Yoo et al., 2021; Wang et al.,
2021a). Different from the previous usages of pre-
trained models in DA, these works open the era of
“prompt-based DA”. In contrast, the exploration of
prompt-based DA in source code domains remains
a relatively untouched research area. Another direc-
tion is to harness the internal knowledge encoded
in pre-trained source code models. For example,
Karmakar and Robbes (2021); Wan et al. (2022)
show that ASTs and code semantics can be induced
from these models without the static analysis tools.
As most DA methods for source code models tend
to predefine the code transformation rules via pro-
gram analysis, it is expected that the programming
knowledge inside these pre-trained source code
models can automate the rule designs.Working with domain-specific data. Our pa-
per focuses on surveying DA techniques for com-
mon downstream tasks involving processing source
code. However, we are aware that there are a few
works on other task-specific data in the field of
source code. For instance, API recommendation
and API sequence generation can be considered
a part of source code tasks (Huang et al., 2018;
Gu et al., 2016). DA methods covered by our sur-
vey can not be directly generalized to these tasks,
as most of them only target program-level aug-
mentation but not API-level. We observe a gap
of DA techniques between these two different lay-
ers (Treude and Robillard, 2016; Xu et al., 2020;
Wang et al., 2021b), which provides opportuni-
ties for future works to explore. Additionally, the
source code modeling has not fully justified DA for
out-of-distribution generalization. Previous stud-
ies (Hajipour et al., 2022; Hu et al., 2022) assume
the domain as the programs with different com-
plexity, syntax, and semantics. We argue that this
definition is not natural enough. Similar to the
subdomains in NLP, like biomedical and financial
texts, the application subdomains of source code
can be diverse. For example, the programs to solve
data science problems can significantly differ from
those for web design. We encourage SE and ML
communities to study the benefits of DA when ap-
plied to various application subdomains of source
code.
More exploration on project-level source code
and low-resource programming languages.
The existing methods have made sufficient progress
in function-level code snippets and common pro-
gramming languages. The emphasis on code snip-
pets at the function level fails to capture the intri-
cacies and complexities of programming in real-
world scenarios, where developers often work with
multiple files and folders simultaneously. There-
fore, we highlight the importance of exploring
DA approaches on the project level. The DA
on source code projects can be distinct from the
function-level DA, as it may involve more infor-
mation such as the interdependencies between dif-
ferent code modules, high-level architectural con-
siderations, and the often intricate relationship be-
tween data structures and algorithms used across
the project (Mockus et al., 2002). At the same
time, limited by data resources (Husain et al., 2019;
Orlanski et al., 2023), augmentation methods of
low-resource languages are scarce, although theyhave more demand for DA. Exploration in these
two directions is still limited, and they could be
promising directions.
Mitigating social bias. As source code models
have advanced software development, they may be
used to develop human-centric applications such
as human resources and education, where biased
programs may result in unjustified and unethical
decisions for underrepresented people (Zhuo et al.,
2023a). While social bias in NLP has been well
studied and can be mitigated with DA (Feng et al.,
2021), the social bias in source code has not been
brought to attention. For example, Zhuo et al.
(2023a) and Liu et al. (2023c) find that LLMs of
source code have server bias in various demograph-
ics such as gender, sexuality, and occupation when
performing code generation based on the natural
language queries. To make these models more re-
sponsible in source code, we urge more research on
mitigating bias. As prior works in NLP suggested,
DA may be an effective technique to make source
code models more responsible.
Few-shot learning. In few-shot scenarios, mod-
els are required to achieve performance that rivals
that of traditional machine learning models, yet
the amount of training data is extremely limited.
DA methods provide a direct solution to the prob-
lem. However, limited works in few-shot scenarios
have adopted DA methods (Nashid et al., 2023).
Mainstream pre-trained source code models obtain
rich semantic knowledge through language model-
ing. Such knowledge even covers, to some extent,
the semantic information introduced by traditional
paraphrasing-based DA methods. In other words,
the improvement space that traditional DA meth-
ods bring to pre-trained source code models has
been greatly compressed. Therefore, it is an inter-
esting question how to provide models with fast
generalization and problem-solving capability by
generating high-quality augmented data in few-shot
scenarios.
Multimodal applications. It is important to note
that the emphasis on function-level code snippets
does not accurately represent the intricacies and
complexities of real-world programming situations.
In such scenarios, developers often work with mul-
tiple files and folders simultaneously.s have also
been developed. Wang et al. (2021b) and Liu et al.
(2023a) explore the chart derendering with an em-
phasis on source code and corresponding APIs.Surís et al. (2023) propose a framework to gener-
ate Python programs to solve complex visual tasks
including images and videos. Although such mul-
timodal applications are more and more popular,
no study has yet been conducted on applying DA
methods to them. A potential challenge for the mul-
timodal source code task technique is to effectively
bridge between the embedding representations for
each modality in source code models, which has
been investigated in vision-language multimodal
tasks (Ray et al., 2019; Tang et al., 2020; Hao et al.,
2023).
Lack of unification. The current body of liter-
ature on data augmentation (DA) for source code
presents a challenging landscape, with the most
popular methods often being portrayed in a supple-
mentary manner. A handful of empirical studies
have sought to compare DA methods for source
code models (de Paula Rodrigues et al., 2023;
Dong et al., 2023b). However, none of these works
leverages most of the existing advanced DA meth-
ods for source code models. Whereas there are
well-accepted frameworks for DA for CV (e.g. de-
fault augmentation libraries in PyTorch, RandAug-
ment (Cubuk et al., 2020)) and DA for NLP (e.g.
NL-Augmenter (Dhole et al., 2021)), a correspond-
ing library of generalized DA techniques for source
code models is conspicuously absent. Furthermore,
as existent DA methods are usually evaluated with
various datasets, it is hard to determine the efficacy.
Therefore, we posit that the progression of DA
research would be significantly facilitated by the
establishment of standardized and unified bench-
mark tasks, along with datasets, for the purpose
of contrasting and evaluating the effectiveness of
different augmentation methods. This would pave
the way towards a more systematic and compara-
tive understanding of the benefits and limitations
of these methods.
8 Conclusion
Our paper comprehensively analyzes data augmen-
tation techniques in the context of source code.
We first explain the concept of data augmentation
and its function. We then examine the primary
data augmentation methods commonly employed
in source code research and explore augmentation
approaches for typical source code applications and
tasks. Finally, we conclude by outlining the cur-
rent challenges in the field and suggesting potential
directions for future source code research. In pre-senting this paper, we aim to assist source code
researchers in selecting appropriate data augmenta-
tion techniques and encourage further exploration
and advancement in this field.
Limitations
While the work presents in this paper has its
merits, we acknowledge the several limitations.
Firstly, our work only surveys imperative program-
ming languages used for general-purpose program-
ming. Therefore, some DA methods for declar-
ative languages (Zhuo et al., 2023b) or minor
downstream tasks like cryptography misuse detec-
tion (de Paula Rodrigues et al., 2023), including
SQL. Secondly, our focus has been primarily on
function-level DA within the source code context.
As such, there remains a need for future develop-
ment in project-level DA methods. Nonetheless,
this paper offers a valuable collection of general-
purpose DA techniques for source code models,
and we hope that it can serve as an inspiration for
further research in this area. Thirdly, given the
page limits, the descriptions presented in this sur-
vey are essentially brief in nature. Our approach
has been to offer the works in meaningful struc-
tured groups rather than unstructured sequences, to
ensure comprehensive coverage. This work can be
used as an index where more detailed information
can be found in the corresponding works. Lastly, it
is worth noting that this survey is purely qualitative
and does not include any experiments or empirical
results. To provide more meaningful guidance, it
would be helpful to conduct comparative experi-
ments across different DA strategies. We leave this
as a suggestion for future work.
References
Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray,
and Kai-Wei Chang. 2023. Summarize and generate
to back-translate: Unsupervised translation of pro-
gramming languages. In Proceedings of the 17th
Conference of the European Chapter of the Asso-
ciation for Computational Linguistics , pages 1528–
1542, Dubrovnik, Croatia. Association for Computa-
tional Linguistics.
Loubna Ben Allal, Raymond Li, Denis Kocetkov,
Chenghao Mou, Christopher Akiki, Carlos Muñoz
Ferrandis, Niklas Muennighoff, Mayank Mishra,
Alexander Gu, Manan Dey, Logesh Kumar Uma-
pathi, Carolyn Jane Anderson, Yangtian Zi, J. Poirier,
Hailey Schoelkopf, Sergey Mikhailovich Troshin,
Dmitry Abulkhanov, Manuel Romero, Michael FranzLappert, Francesco De Toni, Bernardo Garc’ia del
R’io, Qian Liu, Shamik Bose, Urvashi Bhattacharyya,
Terry Yue Zhuo, Ian Yu, Paulo Villegas, Marco Zocca,
Sourab Mangrulkar, David Lansky, Huu Nguyen,
Danish Contractor, Luisa Villa, Jia Li, Dzmitry Bah-
danau, Yacine Jernite, Sean Christopher Hughes,
Daniel Fried, Arjun Guha, Harm de Vries, and Lean-
dro von Werra. 2023. Santacoder: don’t reach for the
stars! ArXiv , abs/2301.03988.
Miltiadis Allamanis, Earl T. Barr, Premkumar T. De-
vanbu, and Charles Sutton. 2017. A survey of ma-
chine learning for big code and naturalness. ACM
Computing Surveys (CSUR) , 51:1 – 37.
Miltiadis Allamanis, Earl T Barr, Soline Ducousso, and
Zheng Gao. 2020. Typilus: Neural type hints. In
Proceedings of the 41st acm sigplan conference on
programming language design and implementation ,
pages 91–105.
Miltiadis Allamanis, Henry Jackson-Flux, and Marc
Brockschmidt. 2021. Self-supervised bug detection
and repair. Advances in Neural Information Process-
ing Systems , 34:27865–27876.
Miltiadis Allamanis and Charles Sutton. 2013. Min-
ing source code repositories at massive scale using
language modeling. In 2013 10th working confer-
ence on mining software repositories (MSR) , pages
207–216. IEEE.
Uri Alon, Omer Levy, and Eran Yahav. 2019. code2seq:
Generating sequences from structured representa-
tions of code. In International Conference on Learn-
ing Representations .
Mrinal Anand, Pratik Kayal, and Mayank Singh. Ad-
versarial robustness of program synthesis models. In
Advances in Programming Languages and Neurosym-
bolic Systems Workshop .
Muhammad Hilmi Asyrofi, Zhou Yang, Jieke Shi,
Chu Wei Quan, and David Lo. 2021. Can differ-
ential testing improve automatic speech recognition
systems? In 2021 IEEE International Conference
on Software Maintenance and Evolution (ICSME) ,
pages 674–678.
Jacob Austin, Augustus Odena, Maxwell Nye, Maarten
Bosma, Henryk Michalewski, David Dohan, Ellen
Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021.
Program synthesis with large language models. arXiv
preprint arXiv:2108.07732 .
Mehdi Bahrami, NC Shrikanth, Yuji Mizobuchi, Lei
Liu, Masahiro Fukuyori, Wei-Peng Chen, and Kazuki
Munakata. 2021. Augmentedcode: Examining the
effects of natural language resources in code retrieval
models. arXiv preprint arXiv:2110.08512 .
Pavol Bielik and Martin Vechev. 2020. Adversarial
robustness for code. In International Conference on
Machine Learning , pages 896–907. PMLR.Marc Brockschmidt, Miltiadis Allamanis, Alexander L.
Gaunt, and Oleksandr Polozov. 2019. Generative
code modeling with graphs. In International Confer-
ence on Learning Representations .
Saikat Chakraborty, Rahul Krishna, Yangruibo Ding,
and Baishakhi Ray. 2022. Deep learning based vul-
nerability detection: Are we there yet? IEEE Trans-
actions on Software Engineering , 48(9):3280–3296.
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming
Yuan, Henrique Ponde, Jared Kaplan, Harrison Ed-
wards, Yura Burda, Nicholas Joseph, Greg Brockman,
Alex Ray, Raul Puri, Gretchen Krueger, Michael
Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin,
Brooke Chan, Scott Gray, Nick Ryder, Mikhail
Pavlov, Alethea Power, Lukasz Kaiser, Moham-
mad Bavarian, Clemens Winter, Philippe Tillet, Fe-
lipe Petroski Such, David W. Cummings, Matthias
Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel
Herbert-V oss, William H. Guss, Alex Nichol, Igor
Babuschkin, S. Arun Balaji, Shantanu Jain, Andrew
Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan
Morikawa, Alec Radford, Matthew M. Knight, Miles
Brundage, Mira Murati, Katie Mayer, Peter Welinder,
Bob McGrew, Dario Amodei, Sam McCandlish, Ilya
Sutskever, and Wojciech Zaremba. 2021. Evaluat-
ing large language models trained on code. ArXiv ,
abs/2107.03374.
Pinzhen Chen and Gerasimos Lampouras. 2023. Ex-
ploring data augmentation for code generation tasks.
InFindings of the Association for Computational
Linguistics: EACL 2023 , pages 1497–1505.
Qibin Chen, Jeremy Lacomis, Edward J Schwartz, Gra-
ham Neubig, Bogdan Vasilescu, and Claire Le Goues.
2022. Varclr: Variable semantic representation pre-
training via contrastive learning. In Proceedings of
the 44th International Conference on Software Engi-
neering , pages 2327–2339.
Xiao Cheng, Guanqin Zhang, Haoyu Wang, and Yulei
Sui. 2022. Path-sensitive code embedding via con-
trastive learning for software vulnerability detection.
InProceedings of the 31st ACM SIGSOFT Interna-
tional Symposium on Software Testing and Analysis ,
pages 519–531.
YunSeok Choi, CheolWon Na, Hyojun Kim, and Jee-
Hyong Lee. 2023. Readsum: Retrieval-augmented
adaptive transformer for source code summarization.
IEEE Access .
Michael L Collard, Michael John Decker, and Jonathan I
Maletic. 2013. srcml: An infrastructure for the ex-
ploration, analysis, and manipulation of source code:
A tool demonstration. In 2013 IEEE International
conference on software maintenance , pages 516–519.
IEEE.
Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and
Quoc V Le. 2020. Randaugment: practical auto-
mated data augmentation with a reduced search space.
InProceedings of the 34th International Conferenceon Neural Information Processing Systems , pages
18613–18624.
Tri Dao, Albert Gu, Alexander Ratner, Virginia Smith,
Chris De Sa, and Christopher Ré. 2019. A kernel the-
ory of modern data augmentation. In International
Conference on Machine Learning , pages 1528–1537.
PMLR.
Gustavo Eloi de Paula Rodrigues, Alexandre M Braga,
and Ricardo Dahab. 2023. Detecting cryptography
misuses with machine learning: Graph embeddings,
transfer learning and data augmentation in source
code related tasks. IEEE Transactions on Reliability .
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. ArXiv , abs/1810.04805.
Breanna Devore-McDonald and Emery D Berger. 2020.
Mossad: Defeating software plagiarism detection.
Proceedings of the ACM on Programming Languages ,
4(OOPSLA):1–28.
Kaustubh D. Dhole, Varun Gangal, Sebastian
Gehrmann, Aadesh Gupta, Zhenhao Li, Saad Ma-
hamood, Abinaya Mahendiran, Simon Mille, Ashish
Srivastava, Samson Tan, Tongshuang Wu, Jascha
Sohl-Dickstein, Jinho D. Choi, Eduard Hovy, On-
drej Dusek, Sebastian Ruder, Sajant Anand, Na-
gender Aneja, Rabin Banjade, Lisa Barthe, Hanna
Behnke, Ian Berlot-Attwell, Connor Boyle, Car-
oline Brun, Marco Antonio Sobrevilla Cabezudo,
Samuel Cahyawijaya, Emile Chapuis, Wanxiang
Che, Mukund Choudhary, Christian Clauss, Pierre
Colombo, Filip Cornell, Gautier Dagan, Mayukh Das,
Tanay Dixit, Thomas Dopierre, Paul-Alexis Dray, Su-
chitra Dubey, Tatiana Ekeinhor, Marco Di Giovanni,
Rishabh Gupta, Rishabh Gupta, Louanes Hamla,
Sang Han, Fabrice Harel-Canada, Antoine Honore,
Ishan Jindal, Przemyslaw K. Joniak, Denis Kleyko,
Venelin Kovatchev, Kalpesh Krishna, Ashutosh Ku-
mar, Stefan Langer, Seungjae Ryan Lee, Corey James
Levinson, Hualou Liang, Kaizhao Liang, Zhexiong
Liu, Andrey Lukyanenko, Vukosi Marivate, Gerard
de Melo, Simon Meoni, Maxime Meyer, Afnan Mir,
Nafise Sadat Moosavi, Niklas Muennighoff, Timo-
thy Sum Hon Mun, Kenton Murray, Marcin Namysl,
Maria Obedkova, Priti Oli, Nivranshu Pasricha, Jan
Pfister, Richard Plant, Vinay Prabhu, Vasile Pais,
Libo Qin, Shahab Raji, Pawan Kumar Rajpoot, Vikas
Raunak, Roy Rinberg, Nicolas Roberts, Juan Diego
Rodriguez, Claude Roux, Vasconcellos P. H. S.,
Ananya B. Sai, Robin M. Schmidt, Thomas Scialom,
Tshephisho Sefara, Saqib N. Shamsi, Xudong Shen,
Haoyue Shi, Yiwen Shi, Anna Shvets, Nick Siegel,
Damien Sileo, Jamie Simon, Chandan Singh, Ro-
man Sitelew, Priyank Soni, Taylor Sorensen, William
Soto, Aman Srivastava, KV Aditya Srivatsa, Tony
Sun, Mukund Varma T, A Tabassum, Fiona Anting
Tan, Ryan Teehan, Mo Tiwari, Marie Tolkiehn,
Athena Wang, Zijian Wang, Gloria Wang, Zijie J.
Wang, Fuxuan Wei, Bryan Wilie, Genta Indra Winata,
Xinyi Wu, Witold Wydma ´nski, Tianbao Xie, UsamaYaseen, M. Yee, Jing Zhang, and Yue Zhang. 2021.
Nl-augmenter: A framework for task-sensitive natu-
ral language augmentation.
Yangruibo Ding, Luca Buratti, Saurabh Pujar, Alessan-
dro Morari, Baishakhi Ray, and Saikat Chakraborty.
2021. Towards learning (dis)-similarity of source
code from program contrasts. In Annual Meeting of
the Association for Computational Linguistics .
Yangruibo Ding, Saikat Chakraborty, Luca Buratti,
Saurabh Pujar, Alessandro Morari, Gail Kaiser, and
Baishakhi Ray. 2023. Concord: Clone-aware con-
trastive learning for source code.
Zeming Dong, Qiang Hu, Yuejun Guo, Maxime Cordy,
Mike Papadakis, Zhenya Zhang, Yves Le Traon, and
Jianjun Zhao. 2023a. Mixcode: Enhancing code
classification by mixup-based data augmentation. In
2023 IEEE International Conference on Software
Analysis, Evolution and Reengineering (SANER) ,
pages 379–390. IEEE.
Zeming Dong, Qiang Hu, Yuejun Guo, Zhenya Zhang,
Maxime Cordy, Mike Papadakis, Yves Le Traon, and
Jianjun Zhao. 2023b. Boosting source code learning
with data augmentation: An empirical study. arXiv
preprint arXiv:2303.06808 .
Dawn Drain, Colin B Clement, Guillermo Serrato, and
Neel Sundaresan. 2021. Deepdebug: Fixing python
bugs using stack traces, backtranslation, and code
skeletons. arXiv preprint arXiv:2105.09352 .
Marzieh Fadaee, Arianna Bisazza, and Christof Monz.
2017. Data augmentation for low-resource neural
machine translation. In Annual Meeting of the Associ-
ation for Computational Linguistics , pages 567–573.
Association for Computational Linguistics (ACL).
Jiahao Fan, Yi Li, Shaohua Wang, and Tien N Nguyen.
2020. Ac/c++ code vulnerability dataset with code
changes and cve summaries. In Proceedings of the
17th International Conference on Mining Software
Repositories , pages 508–512.
Steven Y Feng, Varun Gangal, Jason Wei, Sarath Chan-
dar, Soroush V osoughi, Teruko Mitamura, and Ed-
uard Hovy. 2021. A survey of data augmentation
approaches for nlp. In Findings of the Association
for Computational Linguistics: ACL-IJCNLP 2021 ,
pages 968–988.
Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xi-
aocheng Feng, Ming Gong, Linjun Shou, Bing Qin,
Ting Liu, Daxin Jiang, et al. 2020. Codebert: A
pre-trained model for programming and natural lan-
guages. In Findings of the Association for Computa-
tional Linguistics: EMNLP 2020 , pages 1536–1547.
Fengjuan Gao, Yu Wang, and Ke Wang. 2023. Discrete
adversarial attack to models of code. Proceedings of
the ACM on Programming Languages , 7(PLDI):172–
195.Xiaodong Gu, Hongyu Zhang, Dongmei Zhang, and
Sunghun Kim. 2016. Deep api learning. In Proceed-
ings of the 2016 24th ACM SIGSOFT international
symposium on foundations of software engineering ,
pages 631–642.
Yafeng Gu, Yiheng Shen, Xiang Chen, Shaoyu Yang,
Yiling Huang, and Zhixiang Cao. 2023. Apicom:
Automatic api completion via prompt learning and
adversarial training-based data augmentation. In Pro-
ceedings of the 14th Asia-Pacific Symposium on In-
ternetware , pages 259–269.
Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng,
Duyu Tang, Shujie LIU, Long Zhou, Nan Duan,
Alexey Svyatkovskiy, Shengyu Fu, Michele Tufano,
Shao Kun Deng, Colin Clement, Dawn Drain, Neel
Sundaresan, Jian Yin, Daxin Jiang, and Ming Zhou.
2021. Graphcode{bert}: Pre-training code represen-
tations with data flow. In International Conference
on Learning Representations .
Hossein Hajipour, Ning Yu, Cristian-Alexandru Staicu,
and Mario Fritz. 2022. Simscood: Systematic anal-
ysis of out-of-distribution behavior of source code
models. arXiv preprint arXiv:2210.04802 .
Xiaoshuai Hao, Yi Zhu, Srikar Appalaraju, Aston
Zhang, Wanqian Zhang, Bo Li, and Mu Li. 2023.
Mixgen: A new multi-modal data augmentation. In
Proceedings of the IEEE/CVF Winter Conference on
Applications of Computer Vision , pages 379–389.
Krishnam Hasija, Shrishti Pradhan, Manasi Patward-
han, Raveendra Kumar Medicherla, Lovekesh Vig,
and Ravindra Naik. 2023. Neuro-symbolic zero-shot
code cloning with cross-language intermediate repre-
sentation. arXiv preprint arXiv:2304.13350 .
Vincent J Hellendoorn, Christian Bird, Earl T Barr, and
Miltiadis Allamanis. 2018. Deep learning type in-
ference. In Proceedings of the 2018 26th acm joint
meeting on european software engineering confer-
ence and symposium on the foundations of software
engineering , pages 152–162.
Jordan Henke, Goutham Ramakrishnan, Zi Wang, Aws
Albarghouth, Somesh Jha, and Thomas Reps. 2022.
Semantic robustness of models of source code. In
2022 IEEE International Conference on Software
Analysis, Evolution and Reengineering (SANER) ,
pages 526–537. IEEE.
Qiang Hu, Yuejun Guo, Xiaofei Xie, Maxime Cordy,
Lei Ma, Mike Papadakis, and Yves Le Traon.
2022. Codes: A distribution shift benchmark
dataset for source code learning. arXiv preprint
arXiv:2206.05480 .
Xing Hu, Ge Li, Xin Xia, David Lo, Shuai Lu, and Zhi
Jin. 2018. Summarizing source code with transferred
api knowledge. In Proceedings of the 27th Inter-
national Joint Conference on Artificial Intelligence ,
pages 2269–2275.Junjie Huang, Duyu Tang, Linjun Shou, Ming Gong,
Ke Xu, Daxin Jiang, Ming Zhou, and Nan Duan.
2021. Cosqa: 20,000+ web queries for code search
and question answering. In Proceedings of the 59th
Annual Meeting of the Association for Computational
Linguistics and the 11th International Joint Confer-
ence on Natural Language Processing (Volume 1:
Long Papers) , pages 5690–5700.
Qiao Huang, Xin Xia, Zhenchang Xing, David Lo,
and Xinyu Wang. 2018. Api method recommenda-
tion without worrying about the task-api knowledge
gap. In Proceedings of the 33rd ACM/IEEE Interna-
tional Conference on Automated Software Engineer-
ing, pages 293–304.
Xiangbing Huang, Yingwei Ma, Haifang Zhou, Zhijie
Jiang, Yuanliang Zhang, Teng Wang, and Shanshan
Li. 2023. Towards better multilingual code search
through cross-lingual contrastive learning. In Pro-
ceedings of the 14th Asia-Pacific Symposium on In-
ternetware , pages 22–32.
Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis
Allamanis, and Marc Brockschmidt. 2019. Code-
searchnet challenge: Evaluating the state of semantic
code search. arXiv preprint arXiv:1909.09436 .
Paras Jain, Ajay Jain, Tianjun Zhang, Pieter Abbeel,
Joseph Gonzalez, and Ion Stoica. 2021. Contrastive
code representation learning. In Proceedings of the
2021 Conference on Empirical Methods in Natural
Language Processing , pages 5954–5971.
Jinghan Jia, Shashank Srikant, Tamara Mitrovska,
Chuang Gan, Shiyu Chang, Sijia Liu, and Una-May
O’Reilly. 2023. Clawsat: Towards both robust and ac-
curate code models. In 2023 IEEE International Con-
ference on Software Analysis, Evolution and Reengi-
neering (SANER) , pages 212–223. IEEE.
Matthew Jin, Syed Shahriar, Michele Tufano, Xin
Shi, Shuai Lu, Neel Sundaresan, and Alexey Svy-
atkovskiy. 2023. Inferfix: End-to-end program repair
with llms. arXiv preprint arXiv:2303.07263 .
Anjan Karmakar and Romain Robbes. 2021. What
do pre-trained code models know about code? In
2021 36th IEEE/ACM International Conference on
Automated Software Engineering (ASE) , pages 1332–
1336. IEEE.
Haochen Li, Chunyan Miao, Cyril Leung, Yanxian
Huang, Yuan Huang, Hongyu Zhang, and Yanlin
Wang. 2022a. Exploring representation-level aug-
mentation for code search. In Proceedings of the
2022 Conference on Empirical Methods in Natu-
ral Language Processing , pages 4924–4936, Abu
Dhabi, United Arab Emirates. Association for Com-
putational Linguistics.
Haochen Li, Xin Zhou, Luu Anh Tuan, and Chunyan
Miao. 2023a. Rethinking negative pairs in code
search.Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas
Muennighoff, Denis Kocetkov, Chenghao Mou, Marc
Marone, Christopher Akiki, Jia Li, Jenny Chim,
Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo,
Thomas Wang, Olivier Dehaene, Mishig Davaadorj,
Joel Lamy-Poirier, João Monteiro, Oleh Shliazhko,
Nicolas Gontier, Nicholas Meade, Armel Zebaze,
Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu,
Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo
Wang, Rudra Murthy, Jason Stillerman, Siva Sankalp
Patel, Dmitry Abulkhanov, Marco Zocca, Manan
Dey, Zhihan Zhang, Nourhan Fahmy, Urvashi Bhat-
tacharyya, W. Yu, Swayam Singh, Sasha Luccioni,
Paulo Villegas, Maxim Kunakov, Fedor Zhdanov,
Manuel Romero, Tony Lee, Nadav Timor, Jennifer
Ding, Claire Schlesinger, Hailey Schoelkopf, Jana
Ebert, Tri Dao, Mayank Mishra, Alexander Gu,
Jennifer Robinson, Carolyn Jane Anderson, Bren-
dan Dolan-Gavitt, Danish Contractor, Siva Reddy,
Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Car-
los Muñoz Ferrandis, Sean M. Hughes, Thomas Wolf,
Arjun Guha, Leandro von Werra, and Harm de Vries.
2023b. Starcoder: may the source be with you!
ArXiv , abs/2305.06161.
Xiaonan Li, Yeyun Gong, Yelong Shen, Xipeng Qiu,
Hang Zhang, Bolun Yao, Weizhen Qi, Daxin Jiang,
Weizhu Chen, and Nan Duan. 2022b. Coderetriever:
A large scale contrastive pre-training method for code
search. In Proceedings of the 2022 Conference on
Empirical Methods in Natural Language Processing ,
pages 2898–2910.
Yanzhou Li, Shangqing Liu, Kangjie Chen, Xiaofei Xie,
Tianwei Zhang, and Yang Liu. 2023c. Multi-target
backdoor attacks for code pre-trained models.
Yiyang Li, Hongqiu Wu, and Hai Zhao. 2022c.
Semantic-preserving adversarial code comprehen-
sion. In Proceedings of the 29th International Con-
ference on Computational Linguistics , pages 3017–
3028.
Zhen Li, Guenevere Chen, Chen Chen, Yayi Zou, and
Shouhuai Xu. 2022d. Ropgen: Towards robust code
authorship attribution via automatic coding style
transformation. In Proceedings of the 44th Inter-
national Conference on Software Engineering , pages
1906–1918.
Zhiming Li, Xiaofei Xie, Haoliang Li, Zhengzi Xu,
Yi Li, and Yang Liu. 2022e. Cross-lingual transfer
learning for statistical type inference. In Proceedings
of the 31st ACM SIGSOFT International Symposium
on Software Testing and Analysis , pages 239–250.
Zongjie Li, Pingchuan Ma, Huaijin Wang, Shuai Wang,
Qiyi Tang, Sen Nie, and Shi Wu. 2022f. Unleashing
the power of compiler intermediate representation to
enhance neural program embeddings. In Proceedings
of the 44th International Conference on Software
Engineering , pages 2253–2265.
Chenxiao Liu and Xiaojun Wan. 2021. Codeqa: A
question answering dataset for source code compre-hension. In Findings of the Association for Computa-
tional Linguistics: EMNLP 2021 , pages 2618–2632.
Fangyu Liu, Francesco Piccinno, Syrine Krichene,
Chenxi Pang, Kenton Lee, Mandar Joshi, Yasemin
Altun, Nigel Collier, and Julian Martin Eisenschlos.
2023a. Matcha: Enhancing visual language pretrain-
ing with math reasoning and chart derendering. In
Proceedings of the 61th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers) .
Shangqing Liu, Yu Chen, Xiaofei Xie, Jing Kai Siow,
and Yang Liu. Retrieval-augmented generation for
code summarization via hybrid gnn. In International
Conference on Learning Representations .
Shangqing Liu, Bozhi Wu, Xiaofei Xie, Guozhu Meng,
and Yang Liu. 2023b. Contrabert: Enhancing code
pre-trained models via contrastive learning.
Yan Liu, Xiaokang Chen, Yan Gao, Zhe Su, Fengji
Zhang, Daoguang Zan, JianGuang Lou, PinYuChen,
and TsungYiHo. 2023c. Uncovering and quantify-
ingsocialbiases incodegeneration.
Shuai Lu, Nan Duan, Hojae Han, Daya Guo, Seung-
won Hwang, and Alexey Svyatkovskiy. 2022. Reacc:
A retrieval-augmented code completion framework.
InProceedings of the 60th Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers) , pages 6227–6240.
Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey
Svyatkovskiy, Ambrosio Blanco, Colin Clement,
Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Li-
dong Zhou, Linjun Shou, Long Zhou, Michele Tu-
fano, MING GONG, Ming Zhou, Nan Duan, Neel
Sundaresan, Shao Kun Deng, Shengyu Fu, and Shu-
jie LIU. 2021. CodeXGLUE: A machine learning
benchmark dataset for code understanding and gener-
ation. In Thirty-fifth Conference on Neural Informa-
tion Processing Systems Datasets and Benchmarks
Track (Round 1) .
Yingwei Ma, Yue Yu, Shanshan Li, Zhouyang Jia, Jun
Ma, Rulin Xu, Wei Dong, and Xiangke Liao. 2023.
Mulcs: Towards a unified deep representation for
multilingual code search. In 2023 IEEE Interna-
tional Conference on Software Analysis, Evolution
and Reengineering (SANER) , pages 120–131. IEEE.
Paul W McBurney and Collin McMillan. 2014. Au-
tomatic documentation generation via source code
summarization of method context. In Proceedings
of the 22nd International Conference on Program
Comprehension , pages 279–290.
Stephen Merity, Caiming Xiong, James Bradbury, and
Richard Socher. Pointer sentinel mixture models. In
International Conference on Learning Representa-
tions .
Qing Mi, Yiqun Hao, Maran Wu, and Liwei Ou. 2022a.
An enhanced data augmentation approach to supportmulti-class code readability classification. In Interna-
tional conference on software engineering and knowl-
edge engineering .
Qing Mi, Luo Wang, Lisha Hu, Liwei Ou, and Yang
Yu. 2022b. Improving multi-class code readabil-
ity classification with an enhanced data augmen-
tation approach (130). International Journal of
Software Engineering and Knowledge Engineering ,
32(11n12):1709–1731.
Qing Mi, Yan Xiao, Zhi Cai, and Xibin Jia. 2021. The
effectiveness of data augmentation in code readability
classification. Information and Software Technology ,
129:106378.
Grégoire Mialon, Roberto Dessì, Maria Lomeli, Christo-
foros Nalmpantis, Ram Pasunuru, Roberta Raileanu,
Baptiste Rozière, Timo Schick, Jane Dwivedi-Yu,
Asli Celikyilmaz, et al. 2023. Augmented language
models: a survey. arXiv preprint arXiv:2302.07842 .
Antonio Valerio Miceli-Barone and Rico Sennrich.
2017. A parallel corpus of python functions and
documentation strings for automated code documen-
tation and code generation. In Proceedings of the
Eighth International Joint Conference on Natural
Language Processing (Volume 2: Short Papers) ,
pages 314–319.
Audris Mockus, Roy T Fielding, and James D Herbsleb.
2002. Two case studies of open source software de-
velopment: Apache and mozilla. ACM Transactions
on Software Engineering and Methodology (TOSEM) ,
11(3):309–346.
Noor Nashid, Mifta Sintaha, and Ali Mesbah. 2023.
Retrieval-based prompt selection for code-related
few-shot learning. In Proceedings of the 45th In-
ternational Conference on Software Engineering
(ICSE’23) .
Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan
Wang, Yingbo Zhou, Silvio Savarese, and Caiming
Xiong. 2023. Codegen: An open large language
model for code with multi-turn program synthesis.
ICLR .
Augustus Odena, Christopher Olah, and Jonathon
Shlens. 2017. Conditional image synthesis with aux-
iliary classifier gans. In International conference on
machine learning , pages 2642–2651. PMLR.
Gabriel Orlanski, Kefan Xiao, Xavier Garcia, Jeffrey
Hui, Joshua Howland, Jonathan Malmaud, Jacob
Austin, Rishah Singh, and Michele Catasta. 2023.
Measuring the impact of programming language dis-
tribution. arXiv preprint arXiv:2302.01973 .
Pedro Orvalho, Mikoláš Janota, and Vasco Manquinho.
2022. Multipas: applying program transformations
to introductory programming assignments for data
augmentation. In Proceedings of the 30th ACM Joint
European Software Engineering Conference and Sym-
posium on the Foundations of Software Engineering ,
pages 1657–1661.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Annual Meeting of
the Association for Computational Linguistics .
Shinwoo Park, Youngwook Kim, and Yo-Sub Han. 2023.
Contrastive learning with keyword-based data aug-
mentation for code search and code question answer-
ing. In Conference of the European Chapter of the
Association for Computational Linguistics .
Md Rizwan Parvez, Wasi Ahmad, Saikat Chakraborty,
Baishakhi Ray, and Kai-Wei Chang. 2021. Retrieval
augmented code generation and summarization. In
Findings of the Association for Computational Lin-
guistics: EMNLP 2021 , pages 2719–2734.
Subroto Nag Pinku, Debajyoti Mondal, and Chanchal K
Roy. 2023. Pathways to leverage transcompiler based
data augmentation for cross-language clone detection.
arXiv preprint arXiv:2303.01435 .
Maryam Vahdat Pour, Zhuo Li, Lei Ma, and Hadi Hem-
mati. 2021. A search-based testing framework for
deep neural networks of source code embedding. In
2021 14th IEEE Conference on Software Testing, Ver-
ification and Validation (ICST) , pages 36–46. IEEE.
Ruchir Puri, David Kung, Geert Janssen, Wei Zhang, Gi-
acomo Domeniconi, Vladimir Zolotov, Julian Dolby,
Jie Chen, Mihir Choudhury, Lindsey Decker, et al.
2021. Codenet: A large-scale ai for code dataset for
learning a diversity of coding tasks. In Annual Con-
ference on Neural Information Processing Systems .
Erwin Quiring, Alwin Maier, Konrad Rieck, et al. 2019.
Misleading authorship attribution of source code us-
ing adversarial learning. In USENIX Security Sympo-
sium , pages 479–496.
Md Rafiqul Islam Rabin and Mohammad Amin Alipour.
2022. Programtransformer: A tool for generating
semantically equivalent transformed programs. Soft-
ware Impacts , 14:100429.
Md Rafiqul Islam Rabin, Nghi DQ Bui, Ke Wang, Yijun
Yu, Lingxiao Jiang, and Mohammad Amin Alipour.
2021. On the generalizability of neural program
models with respect to semantic-preserving program
transformations. Information and Software Technol-
ogy, 135:106552.
Arijit Ray, Karan Sikka, Ajay Divakaran, Stefan Lee,
and Giedrius Burachas. 2019. Sunny and dark out-
side?! improving answer consistency in vqa through
entailed question generation. In Proceedings of the
2019 Conference on Empirical Methods in Natu-
ral Language Processing and the 9th International
Joint Conference on Natural Language Processing
(EMNLP-IJCNLP) , pages 5860–5865.
Veselin Raychev, Pavol Bielik, and Martin Vechev. 2016.
Probabilistic model for code with decision trees.
ACM SIGPLAN Notices , 51(10):731–747.Shuo Ren, Daya Guo, Shuai Lu, Long Zhou, Shujie Liu,
Duyu Tang, Neel Sundaresan, Ming Zhou, Ambrosio
Blanco, and Shuai Ma. 2020. Codebleu: a method
for automatic evaluation of code synthesis. arXiv
preprint arXiv:2009.10297 .
Baptiste Roziere, Marie-Anne Lachaux, Lowik Chanus-
sot, and Guillaume Lample. 2020. Unsupervised
translation of programming languages. Advances in
Neural Information Processing Systems , 33:20601–
20611.
Baptiste Roziere, Jie Zhang, Francois Charton, Mark
Harman, Gabriel Synnaeve, and Guillaume Lample.
2021. Leveraging automated unit tests for unsuper-
vised code translation. In International Conference
on Learning Representations .
Rico Sennrich, Barry Haddow, and Alexandra Birch.
2015. Improving neural machine translation
models with monolingual data. arXiv preprint
arXiv:1511.06709 .
Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Improving neural machine translation models
with monolingual data. In Proceedings of the 54th
Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers) , pages 86–96,
Berlin, Germany. Association for Computational Lin-
guistics.
Yiheng Shen, Xiaolin JU, Xiang Chen, and Guang Yang.
Bash comment generation via data augmentation
and semantic-aware codebert. Available at SSRN
4385791 .
Ensheng Shi, Yanlin Wang, Wenchao Gu, Lun Du,
Hongyu Zhang, Shi Han, Dongmei Zhang, and Hong-
bin Sun. 2023. Cocosoda: Effective contrastive learn-
ing for code search. In Proceedings of the 45th Inter-
national Conference on Software Engineering .
Yiwen Shi, Taha ValizadehAslani, Jing Wang, Ping
Ren, Yi Zhang, Meng Hu, Liang Zhao, and Hualou
Liang. 2022a. Improving imbalanced learning by
pre-finetuning with data augmentation. In Fourth In-
ternational Workshop on Learning with Imbalanced
Domains: Theory and Applications , pages 68–82.
PMLR.
Zejian Shi, Yun Xiong, Xiaolong Zhang, Yao Zhang,
Shanshan Li, and Yangyong Zhu. 2022b. Cross-
modal contrastive learning for code search. In 2022
IEEE International Conference on Software Mainte-
nance and Evolution (ICSME) , pages 94–105. IEEE.
Fatemeh Shiri, Terry Yue Zhuo, Zhuang Li, Shirui Pan,
Weiqing Wang, Reza Haffari, Yuan-Fang Li, and Van
Nguyen. 2022. Paraphrasing techniques for maritime
qa system. In 2022 25th International Conference on
Information Fusion (FUSION) , pages 1–8. IEEE.
Connor Shorten and Taghi M. Khoshgoftaar. 2019. A
survey on image data augmentation for deep learning.
Journal of Big Data , 6:1–48.André Silva, João F Ferreira, He Ye, and Martin
Monperrus. 2023. Mufin: Improving neural re-
pair models with back-translation. arXiv preprint
arXiv:2304.02301 .
Zixuan Song, Xiuwei Shang, Mengxuan Li, Rong Chen,
Hui Li, and Shikai Guo. 2022. Do not have enough
data? an easy data augmentation for code summariza-
tion. In 2022 IEEE 13th International Symposium on
Parallel Architectures, Algorithms and Programming
(PAAP) , pages 1–6. IEEE.
Jacob M. Springer, Bryn Marie Reinstadler, and Una-
May O’Reilly. 2021. Strata: Simple, gradient-free
attacks for models of code.
Shashank Srikant, Sijia Liu, Tamara Mitrovska, Shiyu
Chang, Quanfu Fan, Gaoyuan Zhang, and Una-May
O´Reilly. Generating adversarial computer programs
using optimized obfuscations. In International Con-
ference on Learning Representations .
Felix Stahlberg. 2020. Neural machine translation: A
review. Journal of Artificial Intelligence Research ,
69:343–418.
Dídac Surís, Sachit Menon, and Carl V ondrick. 2023.
Vipergpt: Visual inference via python execution for
reasoning. arXiv preprint arXiv:2303.08128 .
Jeffrey Svajlenko, Judith F Islam, Iman Keivanloo,
Chanchal K Roy, and Mohammad Mamun Mia. 2014.
Towards a big data curated benchmark of inter-project
code clones. In 2014 IEEE International Conference
on Software Maintenance and Evolution , pages 476–
480. IEEE.
Ruixue Tang, Chao Ma, Wei Emma Zhang, Qi Wu, and
Xiaokang Yang. 2020. Semantic equivalent adversar-
ial data augmentation for visual question answering.
InComputer Vision–ECCV 2020: 16th European
Conference, Glasgow, UK, August 23–28, 2020, Pro-
ceedings, Part XIX 16 , pages 437–453. Springer.
Ze Tang, Jidong Ge, Shangqing Liu, Tingwei Zhu, Tong-
tong Xu, Liguo Huang, and Bin Luo. 2023. Domain
adaptive code completion via language models and
decoupled domain databases.
Junfeng Tian, Chenxin Wang, Zhen Li, and Yu Wen.
2021. Generating adversarial examples of source
code classification models via q-learning-based
markov decision process. In 2021 IEEE 21st Inter-
national Conference on Software Quality, Reliability
and Security (QRS) , pages 807–818. IEEE.
Zhao Tian, Junjie Chen, and Zhi Jin. 2023. Adversarial
attacks on neural models of code via code difference
reduction. arXiv preprint arXiv:2301.02412 .
Christoph Treude and Martin P Robillard. 2016. Aug-
menting api documentation with insights from stack
overflow. In Proceedings of the 38th International
Conference on Software Engineering , pages 392–
403.Yao Wan, Wei Zhao, Hongyu Zhang, Yulei Sui, Guan-
dong Xu, and Hai Jin. 2022. What do they capture?
a structural analysis of pre-trained language models
for source code. In Proceedings of the 44th Interna-
tional Conference on Software Engineering , pages
2377–2388.
Yao Wan, Zhou Zhao, Min Yang, Guandong Xu,
Haochao Ying, Jian Wu, and Philip S Yu. 2018. Im-
proving automatic source code summarization via
deep reinforcement learning. In Proceedings of the
33rd ACM/IEEE international conference on auto-
mated software engineering , pages 397–407.
Deze Wang, Zhouyang Jia, Shanshan Li, Yue Yu, Yun
Xiong, Wei Dong, and Xiangke Liao. 2022a. Bridg-
ing pre-trained models and downstream tasks for
source code understanding. In Proceedings of the
44th International Conference on Software Engineer-
ing, pages 287–298.
Jianzong Wang, Shijing Si, Zhitao Zhu, Xiaoyang Qu,
Zhenhou Hong, and Jing Xiao. 2022b. Leveraging
causal inference for explainable automatic program
repair. In 2022 International Joint Conference on
Neural Networks (IJCNN) , pages 1–6. IEEE.
Shangwen Wang, Bo Lin, Zhensu Sun, Ming Wen,
Yepang Liu, Yan Lei, and Xiaoguang Mao. 2023a.
Two birds with one stone: Boosting code generation
and code search via a generative adversarial network.
Proceedings of the ACM on Programming Languages ,
7(OOPSLA2):486–515.
Shiqi Wang, Zheng Li, Haifeng Qian, Chenghao Yang,
Zijian Wang, Mingyue Shang, Varun Kumar, Sam-
son Tan, Baishakhi Ray, Parminder Bhatia, Ramesh
Nallapati, Murali Krishna Ramanathan, Dan Roth,
and Bing Xiang. 2023b. Recode: Robustness eval-
uation of code generation models. In Proceedings
of the 61th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) .
Shuohang Wang, Yang Liu, Yichong Xu, Chenguang
Zhu, and Michael Zeng. 2021a. Want to reduce label-
ing cost? gpt-3 can help. In Findings of the Associ-
ation for Computational Linguistics: EMNLP 2021 ,
pages 4195–4205.
Song Wang, Taiyue Liu, and Lin Tan. 2016. Automati-
cally learning semantic features for defect prediction.
InProceedings of the 38th International Conference
on Software Engineering , pages 297–308.
Weishi Wang, Yue Wang, Shafiq Joty, and Steven CH
Hoi. 2023c. Rap-gen: Retrieval-augmented patch
generation with codet5 for automatic program repair.
Xiao Wang, Qiong Wu, Hongyu Zhang, Chen Lyu,
Xue Jiang, Zhuoran Zheng, Lei Lyu, and Songlin
Hu. 2022c. Heloc: Hierarchical contrastive learn-
ing of source code representation. In Proceedings
of the 30th IEEE/ACM International Conference on
Program Comprehension , pages 354–365.Xin Wang, Xiao Liu, Pingyi Zhou, Qixia Liu, Jin Liu,
Hao Wu, and Xiao Cui. 2022d. Test-driven multi-
task learning with functionally equivalent code trans-
formation for neural code generation. Proceedings
of the 37th IEEE/ACM International Conference on
Automated Software Engineering .
Xin Wang, Xiao Liu, Pingyi Zhou, Qixia Liu, Jin Liu,
Hao Wu, and Xiaohui Cui. 2022e. Test-driven multi-
task learning with functionally equivalent code trans-
formation for neural code generation. In Proceedings
of the 37th IEEE/ACM International Conference on
Automated Software Engineering , pages 1–6.
Zeyu Wang, Sheng Huang, Zhongxin Liu, Meng Yan,
Xin Xia, Bei Wang, and Dan Yang. 2021b. Plot2api:
recommending graphic api from plot via semantic
parsing guided neural network. In 2021 IEEE Inter-
national Conference on Software Analysis, Evolution
and Reengineering (SANER) , pages 458–469. IEEE.
Qingsong Wen, Liang Sun, Xiaomin Song, Jing Gao,
Xue Wang, and Huan Xu. 2020. Time series data
augmentation for deep learning: A survey. In Inter-
national Joint Conference on Artificial Intelligence .
Sen Wu, Hongyang Zhang, Gregory Valiant, and
Christopher Ré. 2020. On the generalization effects
of linear transformations in data augmentation. In In-
ternational Conference on Machine Learning , pages
10410–10420. PMLR.
Xingbo Wu, Nathanaël Cheriere, Cheng Zhang, and
Dushyanth Narayanan. 2023. Rustgen: An augmen-
tation approach for generating compilable rust code
with large language models.
Yiqing Xie, Atharva Naik, Daniel Fried, and Carolyn
Rose. 2023. Data augmentation for code translation
with comparable corpora and multiple references.
Frank F Xu, Zhengbao Jiang, Pengcheng Yin, Bogdan
Vasilescu, and Graham Neubig. 2020. Incorporating
external knowledge through pre-training for natural
language to code generation. In Proceedings of the
58th Annual Meeting of the Association for Compu-
tational Linguistics , pages 6045–6052.
Guang Yang, Yu Zhou, Xiang Chen, Xiangyu Zhang,
Tingting Han, and Taolue Chen. 2023a. Exploit-
gen: Template-augmented exploit code generation
based on codebert. Journal of Systems and Software ,
197:111577.
Guang Yang, Yu Zhou, Wenhua Yang, Tao Yue, Xi-
ang Chen, and Taolue Chen. 2022a. How impor-
tant are good method names in neural code genera-
tion? a model robustness perspective. arXiv preprint
arXiv:2211.15844 .
Guang Yang, Yu Zhou, Xiangyu Zhang, Xiang Chen,
Tingting Han, and Taolue Chen. 2023b. Assessing
and improving syntactic adversarial robustness of pre-
trained models for code translation. arXiv preprint
arXiv:2310.18587 .Hongyu Yang, Jianma Hui, Min Hou, Shuanghong Shen,
and Enhong Chen. 2023c. A pre-training method for
enhanced code representation based on multimodal
contrastive learning. Journal of Software , 35(4):0–0.
Zhou Yang, Jieke Shi, Junda He, and David Lo. 2022b.
Natural attack for pre-trained models of code. In
Proceedings of the 44th International Conference on
Software Engineering , pages 1482–1493.
Michihiro Yasunaga and Percy Liang. 2021. Break-
it-fix-it: Unsupervised learning for program repair.
InInternational Conference on Machine Learning ,
pages 11941–11952. PMLR.
Tong Ye, Lingfei Wu, Tengfei Ma, Xuhong Zhang,
Yangkai Du, Peiyu Liu, Wenhai Wang, and Shouling
Ji. 2023. Tram: A token-level retrieval-augmented
mechanism for source code summarization. arXiv
preprint arXiv:2305.11074 .
Noam Yefet, Uri Alon, and Eran Yahav. 2020. Adversar-
ial examples for models of code. Proceedings of the
ACM on Programming Languages , 4(OOPSLA):1–
30.
Pengcheng Yin and Graham Neubig. 2017. A syntactic
neural model for general-purpose code generation.
InProceedings of the 55th Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers) , pages 440–450.
Kang Min Yoo, Dongju Park, Jaewook Kang, Sang-Woo
Lee, and Woomyoung Park. 2021. Gpt3mix: Lever-
aging large-scale language models for text augmen-
tation. In Findings of the Association for Computa-
tional Linguistics: EMNLP 2021 , pages 2225–2239.
Chi Yu, Guang Yang, Xiang Chen, Ke Liu, and Yanlin
Zhou. 2022a. Bashexplainer: Retrieval-augmented
bash code comment generation based on fine-tuned
codebert. In 2022 IEEE International Conference
on Software Maintenance and Evolution (ICSME) ,
pages 82–93. IEEE.
Shiwen Yu, Ting Wang, and Ji Wang. 2022b. Data
augmentation by program transformation. Journal of
Systems and Software , 190:111304.
Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and
David Lopez-Paz. 2018. mixup: Beyond empirical
risk minimization. In International Conference on
Learning Representations .
Huangzhao Zhang, Zhuo Li, Ge Li, Lei Ma, Yang Liu,
and Zhi Jin. 2020a. Generating adversarial exam-
ples for holding robustness of source code processing
models. In Proceedings of the AAAI Conference on
Artificial Intelligence , volume 34, pages 1169–1176.
Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun,
and Xudong Liu. 2020b. Retrieval-based neural
source code summarization. In Proceedings of the
ACM/IEEE 42nd International Conference on Soft-
ware Engineering , pages 1385–1397.Xiaoqing Zhang, Yu Zhou, Tingting Han, and Taolue
Chen. 2020c. Training deep code comment genera-
tion models via data augmentation. In Proceedings
of the 12th Asia-Pacific Symposium on Internetware ,
pages 185–188.
Yifan Zhang, Chen Huang, Yueke Zhang, Kevin Cao,
Scott Thomas Andersen, Huajie Shao, Kevin Leach,
and Yu Huang. 2022. Combo: Pre-training repre-
sentations of binary code using contrastive learning.
arXiv preprint arXiv:2210.05102 .
Yunhui Zheng, Saurabh Pujar, Burn Lewis, Luca Buratti,
Edward Epstein, Bo Yang, Jim Laredo, Alessandro
Morari, and Zhong Su. 2021. D2a: A dataset built
for ai-based vulnerability detection methods using
differential analysis. In 2021 IEEE/ACM 43rd Inter-
national Conference on Software Engineering: Soft-
ware Engineering in Practice (ICSE-SEIP) , pages
111–120. IEEE.
Yaqin Zhou, Shangqing Liu, Jingkai Siow, Xiaoning Du,
and Yang Liu. 2019. Devign: Effective vulnerability
identification by learning comprehensive program
semantics via graph neural networks. Advances in
neural information processing systems , 32.
Yu Zhou, Xiaoqing Zhang, Juanjuan Shen, Tingting
Han, Taolue Chen, and Harald Gall. 2022. Ad-
versarial robustness of deep code comment gener-
ation. ACM Transactions on Software Engineering
and Methodology (TOSEM) , 31(4):1–30.
Terry Yue Zhuo. 2023. Large language models are
state-of-the-art evaluators of code generation. arXiv
preprint arXiv:2304.14317 .
Terry Yue Zhuo, Yujin Huang, Chunyang Chen, and
Zhenchang Xing. 2023a. Exploring ai ethics of
chatgpt: A diagnostic analysis. arXiv preprint
arXiv:2301.12867 .
Terry Yue Zhuo, Zhuang Li, Yujin Huang, Fatemeh
Shiri, Weiqing Wang, Gholamreza Haffari, and Yuan-
Fang Li. 2023b. On robustness of prompt-based se-
mantic parsing with large pre-trained language model:
An empirical study on codex. In Proceedings of the
17th Conference of the European Chapter of the As-
sociation for Computational Linguistics , pages 1090–
1102.
Maksim Zubkov, Egor Spirin, Egor Bogomolov, and
Timofey Bryksin. 2022. Evaluation of contrastive
learning with various code representations for code
clone detection. arXiv preprint arXiv:2206.08726 .