Honeycomb: ordered key-value store acceleration on an FPGA-based SmartNIC
Junyi Liu
MicrosoftAleksandar Dragojevi ´c
Citadel SecuritiesShane Flemming
AMDAntonios Katsarakis
Huawei Research
Dario Korolija
ETH ZurichIgor Zablotchi
Mysten LabsHo-cheung Ng
Imperial College LondonAnuj Kalia
MicrosoftMiguel Castro
Microsoft
Abstract
In-memory ordered key-value stores are an important building
block in modern distributed applications. We present Hon-
eycomb, a hybrid software-hardware system for accelerat-
ing read-dominated workloads on ordered key-value stores
that provides linearizability for all operations including scans.
Honeycomb stores a B-Tree in host memory, and executes
SCAN and GET on an FPGA-based SmartNIC, and PUT,UP-
DATE and DELETE on the CPU. This approach enables large
stores and simpliﬁes the FPGA implementation but raises
the challenge of data access and synchronization across the
slow PCIe bus. We describe how Honeycomb overcomes this
challenge with careful data structure design, caching, request
parallelism with out-of-order request execution, wait-free read
operations, and batching synchronization between the CPU
and the FPGA. For read-heavy YCSB workloads, Honey-
comb improves the throughput of a state-of-the-art ordered
key-value store by at least 1:8. For scan-heavy workloads
inspired by cloud storage, Honeycomb improves throughput
by more than 2. The cost-performance, which is more im-
portant for large-scale deployments, is improved by at least
1:5on these workloads.
1 Introduction
In-memory key-value stores are an important building block
in modern distributed applications [7,9,14,42,50]. They store
data in servers that provide a simple interface ( PUT(K,V) ,
V=GET(K),UPDATE (K,V) and DELETE (K)) to clients across
the network. Ordered key-value stores expand the set of sup-
ported applications by providing an efﬁcient SCAN operation
to retrieve the key-value pairs whose keys are within a speci-
ﬁed range. For example, distributed ﬁle systems can use SCAN
to map ranges of logical ﬁle offsets to the nodes storing the
data [9]. It is also used to query graph stores [7, 50] and the
popular Redis [14] offers sorted sets. We describe Honey-
comb, a system that provides hardware acceleration for an
in-memory ordered key-value store.
*This work was done when afﬁliated with Microsoft
†This work has been submitted to the IEEE for possible publication.
Copyright may be transferred without notice, after which this version may
no longer be accessible.There is a large body of research on improving the perfor-
mance of ordered key-value stores, e.g., [12, 18, 29, 38, 41,
52, 56]. Recent research has shown how to leverage modern
networks to achieve high throughput and low latency with an
RPC-based system [29]. Other research has explored using
one-sided RDMA reads to bypass the server CPU for GET
and SCAN operations [12, 18, 41, 52, 56]. Since RDMA NICs
only provide simple one-sided reads of contiguous memory
regions, these systems require at least two RDMA reads per
operation when supporting variable-sized keys or values, and
they require client-side caching to avoid additional RDMAs
when traversing the tree data structures stored by the servers.
Honeycomb accelerates an ordered key-value store using an
FPGA-based SmartNIC [10,45] attached to a host CPU. These
SmartNICs are widely deployed in data centers [10,20 –22,45].
They enable effective CPU ofﬂoad by avoiding the function-
ality limitations of RDMA and the performance problems of
SmartNICs based on general-purpose, low-power cores [8].
Previous work used a similar SmartNIC to accelerate an un-
ordered key-value store [35]. Honeycomb solves a harder
problem because ordered key-value stores use more complex
data structures whose operations perform O(log(n)) memory
accesses instead of the O(1) accesses performed by hash table
operations in unordered key-value stores.
Honeycomb implements the ordered key-value store using
a B-Tree [13] that supports variable-sized keys and values
that are stored inline in B-Tree nodes to improve the per-
formance of scans. It uses multiversion-concurrency control
(MVCC [5]) to provide linearizability [25] for scans. This is
important to provide strong consistency, e.g., in the distributed
ﬁle system example above, clients may fail to read previously
written data when scans are not linearizable.
Honeycomb stores the B-Tree in host memory. It acceler-
ates GET and SCAN operations by executing them on the
FPGA, but PUT,UPDATE and DELETE operations are ex-
ecuted by the host CPU. Since most workloads are read-
dominated [4, 15, 17, 34, 50], there is less beneﬁt in accel-
erating write operations and the cost of accelerating them in
hardware is high because they may require complex splitting
and merging of B-Tree nodes. This hybrid approach enables
much larger stores than would be possible using the limited
DRAM attached to the FPGA and it reduces the complexity of
1arXiv:2303.14259v2  [cs.DC]  6 Apr 2023the FPGA implementation. However, it brings the challenge
of data access and synchronization across the slow PCIe bus.
We overcome this challenge with careful co-design of the
software running on the CPU and the FPGA hardware image.
Honeycomb uses several techniques to reduce data accesses
over PCIe. It caches B-Tree nodes on both FPGA SRAM
and on-board DRAM. It also uses large B-Tree nodes with
shortcuts , a list of sorted keys and offsets into the node that
divide it into segments of similar size. This allows requests
to fetch only the relevant segments of each node they access,
which reduces the number of bytes accessed to search for a
key and the total amount of memory required to cache interior
nodes compared to simply using smaller nodes.
To use available off-chip FPGA bandwidth efﬁciently, Hon-
eycomb exploits request-level parallelism and avoids head of
line blocking with out-of-order request execution. To use all
available off-chip bandwidth, Honeycomb also implements a
dynamic load balancer that directs some requests that hit on
the cache to host memory if there is no bandwidth available
to on-board DRAM and there is unused bandwidth over PCIe.
Honeycomb reduces the cost of synchronization across
PCIe by making GET and SCAN operations wait free [24]. It
also divides each leaf B-Tree node into a sorted block with
sorted key-value pairs and a small log block that logs recent
PUTand DELETE operations. The log block is merged into the
sorted block when its size excedes a threshold. This ensures
reads traverse mostly sorted items while avoiding sorting the
node and synchronizing the CPU and the FPGA on each write.
A comparison with a state-of-the-art ordered key-value
store, which uses eRPC [29] and Masstree [38], shows
that Honeycomb improves both peak performance and cost-
performance, which is more important in large-scale data
centers [28], signiﬁcantly for scan-dominated workloads. For
example, in uniform workloads with inserts and short scans,
Honeycomb improves the throughput by 1:2for workloads
with 50% reads and by more than 2:3for workloads with
at least 80% reads. Most importantly, Honeycomb improves
throughput per watt of Termal Design Power (TDP), which is
a proxy for total cost of ownership [28], by more than 1:9
for workloads with at least 80% reads. We expect these gains
to increase with future hardware because newer FPGAs have
more on-chip memory to cache B-Tree nodes and they are
connected to the host using PCIe Gen5 that has four times
higher bandwidth than the PCIe Gen3 used in our system.
We describe the architecture of Honeycomb in Section 2,
the algorithms and data structures used to implement the B-
Tree in Section 3, and how these are implemented in hardware
in Sections 4 and 5. Section 6 presents the evaluation.
2 Honeycomb architecture
Figure 1 shows the architecture of Honeycomb: an FPGA-
based SmartNIC that connects a host server to the datacenter
network. The Honeycomb accelerator is built on the Catapult
Host Machine FPGA
Ethernet
(50 Gbps)CPU
(put, update, 
delete)
Onboard
DRAM (4 GB)
Onboard
DRAM (4 GB)
Host Memory 
(64 GB)
Host Memory 
(64 GB)
B-Tree Accelerator
(scan, get)PCIe
DMA
Memory Subsystem
Cache PageTable
Memory Subsystem
Cache PageTable
Memory Subsystem
Cache PageTableNetworking
Transport RDMA
Networking
Transport RDMA
Networking
Transport RDMAFigure 1: Hardware system architecture.
FPGA infrastructure [10, 45], which provides a shell with IO
support. The core FPGA image has three subsystems: the
B-Tree accelerator, networking, and memory.
The B-Tree is stored in host DRAM because it scales to
much larger capacities than on-board DRAM in FPGAs. The
B-Tree accelerator implements GET and SCAN operations and
PUT and DELETE operations are executed by the CPU.
The networking subsystem implements LTL [10], a UDP-
based transport protocol similar to RoCE v2 [55] on top of
50 Gbps Ethernet, and an RDMA engine to enable efﬁcient
host-to-FPGA communication. The RDMA engine supports
custom commands for GET and SCAN in addition to general
commands like READ ,WRITE ,SEND , and RECEIVE . These
enable kernel bypass at the clients and at the servers for PUT,
UPDATE and DELETE . They also enable the B-Tree accelera-
tor to completely bypass the CPU for GET and SCAN requests
by receiving these commands directly from the network sub-
system, processing them, and sending back replies.
The memory subsystem interfaces with host memory over
PCIe and with on-board DRAM. It maintains a cache and
a page table. The cache stores interior B-Tree nodes in on-
board DRAM and the root of the B-Tree in on-chip SRAM.
The page table simpliﬁes atomic updates to the B-Tree (Sec-
tion 3.4) by adding indirection [34]. It is stored in on-board
DRAM and maps logical identiﬁers (LIDs) for B-Tree nodes
to their physical addresses in host memory.
The FPGA is connected over two PCIe Gen 3 8to the
host. We measured a peak throughput of 13 GB/s and a latency
of more than 1 µs (depending on load). For comparison, the
bandwidth between host CPU and DRAM is up to 64 GB/s
with an order of magnitude lower latency, and CPU cores have
large caches that improve performance further. Therefore,
Honeycomb must address the challenge of data access and
synchronization across the PCIe bus to achieve performance
and cost-performance gains relative to CPU-only systems.
The next sections describe how we overcame this challenge
with careful hardware-software co-design.
2Header
Shortcuts Sorted Log
Header
Shortcuts Sorted LogFigure 2: Honeycomb node layout.
3 B-Tree
Honeycomb implements a B+ tree [13], i.e., key-value pairs
(also called items) are stored only in leaf nodes while interior
nodes store keys and pointers to child nodes. Honeycomb
guarantees linearizability [25] for all operations including
scans.
3.1 Node layout
B-Tree nodes are ﬁxed-size, which is conﬁgured to 8 KB in
this paper. They are allocated in pinned host memory to allow
the FPGA to use physical memory addresses directly. B-Tree
nodes do not store addresses of child or sibling nodes directly.
Instead, they store 6-byte logical identiﬁers (LIDs) [34]. The
mapping from a LID to the virtual and physical addresses of
a node is maintained in a page table.
The node layout is shown in Figure 2. The node header
is stored in the ﬁrst 48 bytes. It contains ﬁelds specifying
the node type (interior or leaf), the number of bytes used,
a lock word, and a version number to synchronize accesses
to the node. The header of an interior node stores the LID
of the leftmost child. The header of a leaf node stores the
LIDs of the siblings to improve scan performance. Leaf nodes
store keys and values in variable-size blobs. Each blob has
a 2-byte header that speciﬁes its size. Keys and values are
stored inline in B-Tree nodes to improve performance. In the
current conﬁguration, the maximum key length is 460 bytes
and values larger than 469 bytes are stored outside the node.
Leaf nodes are split in two blocks: sorted and log. The
pointer to the boundary between them is stored in the node’s
header. The sorted block stores items in ascending lexico-
graphic order of keys and the log block stores a log of recent
changes. When the log block grows larger than a threshold
(currently set to 512 bytes), it gets merged with the sorted
block. Therefore, read operations beneﬁt from accessing
mostly sorted data and write operations avoid the overhead of
sorting the node on every change. It also reduces synchroniza-
tion across PCIe because it requires one update to the FPGA
page table per-merge instead of one on every write operation.
Since interior nodes change infrequently, the cost of storing
a log block and accessing it on reads outweighs the beneﬁts.
Therefore, interior nodes do not have log blocks.
Entries in the log block are either newly inserted or updated
items or delete markers. Each entry stores a 2-byte pointer to
an item in the sorted block. For a newly inserted item, it pointsto the ﬁrst item with key greater than the item’s key. For other
entries, it points to the deleted or updated item. In addition,
each entry records the 1-byte index in the sorted order of log
items at the time of insertion. These ﬁelds are used to sort all
node items, when searching the leaf, without comparing keys
and with O(1) cost per log block item (Section 4.3).
To optimize the search for a key in a node, we split the
sorted block into segments of roughly equal size. We store
the keys at the boundaries of segments and the pointers to the
boundaries in the shortcut block, which is stored immediately
after the node header. We currently reserve 48 bytes for the
header and 464 bytes for the shortcut block. The shortcut keys
are selected during the merge of the sorted and log blocks.
They are stored only in the shortcut block, i.e., the start of
the segment for boundary key K contains the value associated
with K rather than another copy of K followed by the value.
The search for a key starts by traversing the shortcut block
to identify the segment that contains the key. The search
examines only that segment and the log block, which reduces
the amount of data read both from host memory and the cache.
Since Honeycomb’s bottleneck is FPGA off-chip bandwidth,
this optimization signiﬁcantly improves performance. For
small keys with the current conﬁguration, a search reads at
most 1.5 KB of data from an 8 KB node, which is a 5
performance improvement relative to reading the whole node.
Analysis shows that using large nodes with shortcuts re-
duces the number of bytes accessed to search for a key and
the total amount of memory required to cache interior nodes
compared to simply using smaller nodes. For the example of
a ﬁve-level tree with 8 KB nodes, 16-byte keys and values
at 55% occupancy, searching the tree with this optimization
accesses fewer than 75% of the bytes accessed searching a
simple tree with 512-byte nodes with a similar number of
items. It also requires approximately four times less space to
cache all the interior nodes. The intuition is that the FPGA
can use the information in the header to fetch a sorted block
segment or a log block with a single DMA that reads only the
bytes in use, whereas it would need to read the whole node or
issue more dependent DMAs for the simple tree. Also using
smaller nodes increases overhead because it requires more
storage for pointers to children in interior nodes. Using log
blocks reduces the impact of sorting the large node on writes.
3.2 Synchronization
GETand SCAN operations executed by the accelerator are wait-
free [24] – they never block or retry due to PUT orDELETE
operations executed by the CPU. This reduces the overhead of
synchronization over PCIe and provides predictable latency.
We use multi-version concurrency control (MVCC) [5] to
ensure that read operations can always access a consistent
snapshot of data. Honeycomb maintains two shared 64-bit
version numbers: the global write version and the global read
version . A write operation performs an atomic fetch and add
3on the global write version to obtain its write version, which is
used to version any items it creates. Writers release changes
to readers in version order: a writer sets the global read ver-
sion to its write version when it becomes the writer with the
smallest write version, and updates the copy of the global
read version maintained by the accelerator over PCIe. We
delay responses to write requests until this update completes.
Writers synchronize with each other using node locks, which
are ignored by readers. Read operations read the global read
version maintained by the accelerator to get their read version.
They ignore items with versions greater than this value.
All items in the sorted block have the same version that is
stored in the node header and is called the node version . The
node version is the write version of the write operation that
created the version of the node. Nodes also store a pointer to
the state of the node as of the previous version. A reader that
observes a node version greater than its read version follows
the chain of old version pointers until it reaches a node with
a version less than or equal to its read version. The version
of an item in the log block is stored in the item header. To
reduce the size of these headers, we store a 5-byte version
delta from the node version instead the entire 8-byte version.
If a write would cause the delta version to wrap around, it
merges the sorted and log blocks. Readers ignore all items in
the log block with versions greater than their read version.
To ensure operations observe a consistent snapshot, Hon-
eycomb does not reclaim old versions of nodes while at least
one inﬂight operation can access a snapshot to which the old
version belongs. To garbage collect old versions that are not
accessible anymore, Honeycomb uses an epoch-based mem-
ory manager [39]. Each CPU thread executes operations one
at a time, assigns a sequence number to each inﬂight opera-
tion, and exposes its current operation sequence number to the
memory manager. Similarly, the B-Tree accelerator assigns a
sequence number to each operation and exposes the sequence
numbers of the newest and the oldest inﬂight operation to the
memory manager over PCIe. When a CPU thread makes a
change that removes old versions of nodes from the tree, it
puts the old versions into a garbage collection list tagged with
a vector timestamp. This vector timestamp has entries for the
current operation sequence numbers of all CPU threads and
the newest operation sequence number on the accelerator.
Periodically, a thread scans the garbage collection list and
reclaims all node versions that are not reachable by CPU
threads or the accelerator, i.e., node versions with a vector
timestamp where the entry for each CPU thread is smaller
than the current operation sequence number for the thread,
and the entry for the accelerator is smaller than the current
oldest inﬂight operation on the accelerator. If a writer fails to
allocate memory for a new node and the garbage collection
list is not empty, the operation is aborted and retried.
The page table is stored both in host DRAM and in FPGA
on-board DRAM. When a new node mapping is created or
when a node’s mapping is changed, the CPU updates the tablein host memory and issues a PCIe command to the FPGA to
update the copy of the table in FPGA on-board memory.
Honeycomb can be conﬁgured with MVCC off by using
zero as the version number for all nodes and log items, and by
not setting old version pointers. This improves write perfor-
mance a little by avoiding updates to the global read version
on the FPGA. The atomic tree updates using the page table
still ensure linearizability for GET,PUT, and DELETE , but not
forSCAN because sibling pointers are not updated atomically.
3.3 Scan and get
Honeycomb implements SCAN (Kl, Ku)that ﬁnds the largest
keyKsless than or equal to Kl, and returns a sorted list with
all key value pairs with keys greater than or equal to Ksand
less than or equal to Ku. We implement this version of SCAN
to support mapping from a range of logical ﬁle offsets to
the nodes storing the data in distributed ﬁle systems [9]. In
this application, the key-value pairs represent variable-sized
chunks of ﬁle data where the key is the logical offset of the
ﬁrst byte in the chunk and the value points to the servers
storing the data. Since the start of a logical ﬁle offset range
in a client request can be in the middle of a chunk, the SCAN
semantics we implement are necessary for correctness.
SCAN (Kl, Ku)traverses the tree from the root to a leaf.
It fetches the node header and shortcut block of each node
it visits (currently stored in the ﬁrst 512 bytes). It follows
old version pointers if needed to ﬁnd a node with version
less than or equal to the operation’s read version. Then it
searches for the largest shortcut key less than or equal to Kl,
and fetches the associated sorted block segment. When SCAN
visits an interior node, it searches the segment for the item
with the largest key less than or equal to Kl(which could be
the shortcut key), and obtains the LID of the next node to visit
from the item.
When SCAN reaches the leaf, it fetches the log block in
parallel with searching the shortcuts. Then it searches both
the sorted and log blocks for the largest key Ksless than or
equal to Kl. It scans forward from Ksuntil it reaches a key
greater than Kuor the end of the tree. It ignores any items with
versions greater than the read version. If it ﬁnds one or more
items with the same key, it returns the value associated with
the item with the latest version unless that item is a delete.
Otherwise, it ignores the item. SCAN uses sibling pointers to
move to the next leaf if needed.
The accelerator optimizes scan performance by determin-
ing a sorted order for the items in the log block of a leaf, and
uses the back pointers from log block items to sorted block
items to determine a sorted order across all the items in the
leaf (see Section 4.3). This enables efﬁcient scanning by over-
lapping data access with key comparisons, generating results
that are already sorted, and avoiding key comparisons with
all items in the log block.
Honeycomb implements GET(K)asSCAN (K,K) and post-
4a) The tree before the insert into 
the rightmost nodeb) Allocate a new buffer and merge into 
it. Insert the old pointer (dashed line) to 
the old node.
c) Update the page table to point to the new bufferInsertInsertMerge in
new buffer
Old
ptrMerge in
new buffer
Old
ptr
Old
ptr GCOld
ptr GCFigure 3: Merging the sorted and log blocks.
processes the result to return the value associated with K, or
return not found if K is not in the result.
3.4 Insert
APUT(K,V) operation inserts a new item if there is no item
with key Kin the tree. It traverses the tree as described for
SCAN (K,K) without acquiring any locks, but it reads the
latest version of each node to implement linearizability. The
operation locks the leaf node before modifying it to ensure
the state matches the one observed during the traversal.
The node header has a 32-bit lock word with a lock bit and
a 31-bit sequence number, which is incremented on writes to
the node. PUT tries to acquire the leaf’s lock using an atomic
compare-and-swap instruction that checks if the sequence
number matches the one observed during the traversal. If the
compare-and-swap fails, the PUT operation restarts.
Fast-path insert: In the common case, the operation sim-
ply adds a new item to the log block: it copies the item to the
log block; acquires the write version with a fetch-and-add on
the global write version; sets the version of the item in the log
block; updates the node size; increments the node sequence
number; and unlocks the node. The node size and the lock
word are packed into a 64-bit word. So that the last three
steps can be performed with a single instruction. Concurrent
operations either observe the node without the item or with
the item correctly written, and readers ignore the item until
the change is released (Section 3.2).
Sorted and log block merge: If the size of the log block
exceeds a threshold (currently 512 bytes), the operation
merges the sorted and log blocks as shown in Figure 3. It
allocates a new memory buffer for the node and sorts all the
items into a single sorted block in the new buffer. While
sorting, it selects the shortcut keys. For each item, it decides
whether to put the key in the shortcut block based on the num-
ber of bytes copied so far, the remaining bytes to copy, and the
average size of keys and values. It attempts to maximize the
number of shortcut keys while ensuring that segments have
similar sizes, a minimum size (currently 256 bytes), and that
NSplit NL NR
Split N into N L and N RCopy into new buffer and modify
Old ptr
Old ptr
Old ptrNSwap
NSplit NL NR
Split N into N L and N RCopy into new buffer and modify
Old ptr
Old ptr
Old ptrNSwap
GCNSwapOld ptr
Old ptr
Old ptrGC
GCNSwapOld ptr
Old ptr
Old ptrGCa) Before split. The rightmost 
node is getting split. The root of 
the split is the root of the tree.b) Allocated two new nodes for the split and a 
new buffer for swapping the root of the split.
c) The mapping of the root of the split is updated 
and the split nodes are put into GC listSplitSplitSplitFigure 4: Splitting a node during insert.
shortcut keys ﬁt in the shortcut block.
The operation sets the node version to the write version,
and the old version pointer to the address of the old buffer
(Figure 3b). Then it updates the LID mapping with the address
of the new buffer in the page tables of both CPU and FPGA
(Figure 3c). The parent node does not need to be changed
because the LID of the child remains the same. The operation
also puts the old buffer in the garbage collection list and
releases the changes to make them visible to readers. This
ensures concurrent operations observe the change atomically.
Node splitting: If there is not enough space in the leaf for
the merged block, the operation splits the leaf in two. This
requires inserting a new item in the parent node as shown
in Figure 4. If the parent is full, the operation also splits the
parent. Splitting can propagate to the root. The last interior
node that is updated but not split is called the root of the split .
The operation acquires locks on all interior nodes to split
and on the root of the split. If locking fails due to a sequence
number mismatch, it releases all acquired locks and restarts.
The operation also allocates two new nodes (new LID and
memory buffer) for each node that it splits and a memory
buffer (same LID) for the root of the split ( NL,NR, and Nswap
in Figure 4b). Then it processes nodes from the leaf up split-
ting the items in each node evenly into the two newly allo-
cated nodes. When it gets to the root of the split, the operation
copies the items into the new buffer and adds the LIDs of the
new child nodes and the key at the boundary between them.
To swap in the new subtree atomically, the operation up-
dates the LID mapping for the root of the split in both CPU
and FPGA page tables to point to the new buffer (Figure 4c). It
also locks the sibling leaves and updates their sibling pointers
to point to the new leaves. This is sufﬁcient to ensure lineariz-
ability for all operations but scans, because the updates to the
sibling leaves and the subtree swap are not atomic.
Linearizable scans are important for applications like the
distributed ﬁle system [9]. Without them, a client Ccould
write data Dto a ﬁle at a logical offset range R, and a later
5Request Management
Request
Preprocess
Epoch
ManagerKey Buffers
LB Key
BufferUB Key
BufferB-Tree Accelerator
Leaf -node Scan Engine
Interior -node Search Engine
 Request
InputResponse
OutputMemory 
Access
GeneratorMemory 
Access
GeneratorBuffer
Slot
MSI MSI MSI MSIKey Search Array
Key
Search
UnitKey
Search
UnitRange Scan Array
Range
Scan
UnitRange
Scan
Unit
SC Buffer L Buffer
Result Buffer
MSI
AdapterMSI
AdapterMSI
AdapterMSI
AdapterMSI
AdapterMSI
AdapterMSI
AdapterMSI
AdapterS BufferFigure 5: B-Tree accelerator architecture.
Key Search UnitData
BufferKey
BufferKey
Alignment
Key
Comparison
Pipeline
Result
OutputResult
GeneratorRequest
InputRequest
KeyBlock
Data Figure 6: Key seach unit
read from a range containing Rcould fail to return D. This can
happen if the B-Tree node with the mapping for the servers
that store Dis split and the sibling pointers are not updated
before the scan to ﬁnd the servers storing the data being read.
To provide linearizable scans, the operation sets the ver-
sions in all newly allocated buffers to its write version, and
sets the old version pointer in the new root of the split to
the address of the old buffer (Figure 4b). This ensures that
operations with older read versions traverse the old subtree.
It also sets the old version pointers of the new leaves to the
address of the old leaf. This ensures that scans with older
read versions, which may reach the new leaves using sibling
pointers, traverse the old leaf. The operation also puts the
LIDs and memory buffers of all nodes that got split, and the
old memory buffer of the root of the split in the garbage col-
lection list. The operation then unlocks all nodes and releases
the changes to readers (Section 3.2). This ensures readers
observe all changes made during the split, or none of them.
If the tree root must be split, the operation grows the tree
by allocating a LID and a memory buffer for the new root,
and updating the root and tree height in the accelerator.
3.5 Delete and update
The implementations of UPDATE and DELETE are similar to
INSERT .UPDATE appends the updated key-value pair to the
log block, which points to the previous version of the item
in the sorted or log block. For DELETE , a delete marker is
appended in the same way to indicate that readers should
ignore the deleted item. The space for stale and deleted items
is reclaimed when merging the sorted and log blocks. Nodes
whose occupancy drops below a threshold are merged and
nodes that become empty are deleted. We use similar tech-
niques to ensure atomicity as for splits but omit the details.
4 B-Tree accelerator implementation
The B-Tree accelerator has three components (Figure 5): re-
quest management, interior-node search engine, and leaf-node
scan engine. Request management parses requests and assignsthem sequence numbers and read versions. The interior-node
search engine is responsible for traversing the tree from the
root to a leaf. The leaf-node scan engine traverses the leaf or,
forSCAN operations, leaves using sibling pointers.
4.1 Request management
The time to complete requests is variable, e.g., due to different
size keys or cache misses. Therefore, the B-Tree accelera-
tor exploits request-level parallelism and avoids head of line
blocking by supporting out-of-order request execution. This
maximizes the utilization of compute resources, and on-board
DRAM and PCIe bandwidth.
The request pre-process module extracts the lower (LB)
and upper bound (UB) keys from the request and stores them
in centralized key buffers. These buffers support multiple
read ports to enable the interior-node search engine and the
leaf-node scan engine to read multiple keys in parallel. When
a request completes, its key buffers are freed.
The request pre-process module also initializes request
metadata that includes: identiﬁers of the buffers holding the
keys, the LID of the node being traversed, the node’s level, the
block type being traversed (shortcut, sorted, or log), and the
start offset of the block/segment within the node. The request
metadata ﬂows over a wide data path through the accelerator.
Request management is also responsible for storing the
accelerator’s copy of the 64-bit global read version, which
is updated by the CPU over PCIe (Section 3.2). It reads the
global read version to assign a read version to each incoming
request. Requests are also assigned a 64-bit sequence num-
ber by incrementing a counter Snewmaintained by the epoch
manager. These are part of the request metadata.
The epoch manager also monitors request completion to
keep track of Sold, the sequence number of the oldest inﬂight
request. It exposes SoldandSnewover PCIe to the memory
manager running on the CPU to enable reclaiming mem-
ory when it is no longer accessible by inﬂight requests (Sec-
tion 3.2). Accesses to SoldandSnewover PCIe are infrequent.
6c) Element 60 inserted a) Empty log blockb) Element 90 insertedSorted 9090606000
Sorted 906000
d) Element 30 inserted e) Element 45 insertedSortedSorted Sorted 90900
Sorted 900
Sorted 90906060303045450001
Sorted 906030450001
Sorted 909060603030000
Sorted 906030000Figure 7: Example of order hints in the log block.
a) Processing item 90 b) Processing item 60
d) Processing item 45 e) Final state of the indirection arrayc) Processing item 309090
909060
9060
6060909030
609030
45
30306060909045
306090 3030454560609090 30456090 Figure 8: Sorting the log indirection array with order hints.
4.2 Interior-node search engine
The interior-node search engine is responsible for travers-
ing interior nodes from the root as described in Section 3.3.
It uses a ring architecture to search node blocks iteratively
while overlapping memory reads with compute (Figure 5).
The architecture exploits request-level parallelism by using
multiple memory subsystem interface (MSI) adapters and key
search units (KSU). Additionally, each MSI adapter supports
out-of-order data transfer to fully utilize off-chip bandwidth.
For each interior node visited by a GET orSCAN request,
the memory access generator ﬁrst generates a memory access
request to one of the MSI adapters to fetch the node’s header
and shortcut block. The adapter issues the request to the mem-
ory subsystem, waits for the response, and sends the response
to one of the KSU. The KSU ﬁrst checks if the version in
the node header is greater than the request’s read version. If
so, it sends the updated request metadata back to the memory
access generator to visit the old version of the node. If not, the
KSU searches the shortcuts for the sorted block segment to
fetch. Then sends the updated metadata back to the memory
access generator that generates a memory access request to
one of the MSI adapters to fetch the sorted block segment.
When the adapter gets the segment, it is sent to one of the
KSU. The KSU searches the segment for the LID of the next
node to visit. If the next node is a leaf, the request is passed on
to the leaf-node scan engine. Otherwise, the request is passed
to the memory access generator to continue with the child.
The architecture of a KSU is shown in Figure 6. It uses
the request input metadata to fetch the request’s lower bound
key. Then it processes the block data (header and shortcut, or
sorted block segment) to ﬁnd the largest key that is smaller
than or equal to the request key. Since keys are variable size,
KSU have to process keys sequentially. They process keys
in a streaming fashion overlapping data transfer with key
comparisons. The key comparison pipeline implements com-
parison logic equivalent to the C++ memcmp() function. It
compares key fragments stored in registers with conﬁgurable
width (currently 16 bytes). Key alignment uses barrel shifters
to stream keys into the comparison pipeline. The result gener-
ator outputs updated request metadata.
4.3 Leaf-node scan engine
The leaf-node scan engine is responsible for traversing leaf
nodes as described in Section 3.3. As shown in Figure 5, it also
uses a ring architecture to scan leaf node blocks iteratively,and exploits request-level parallelism by using multiple MSI
adapters and range scan units (RSU).
Unlike the key search array, the range scan array includes
several buffers: result buffer, SC buffer, S buffer , and L buffer .
The result buffer is used to accumulate partial results because
the leaf-node scan engine produces a sorted list of key-value
pairs spread across different blocks in one or more leaves. The
SC buffer, S buffer, and L buffer are used to buffer data from
shortcut, sorted, and log blocks in a leaf, respectively. This
enables iteration over keys in the three blocks in a sorted order
to generate sorted results. The range scan array also includes
different RSU variants to scan each type of block and ﬁnite
state machines (FSM) that coordinate scans across the three
RSU variants. The buffers are separate from RSUs to enable
overlapping of compute with data buffering. The buffers are
divided into request slots that also include the FSM state. The
memory access generator maintains a list of available slots. It
assigns slots to requests or pushes back if no slot available.
The architecture of the RSU is similar to the KSU (Fig-
ure 6) but it has two parallel key comparison pipelines to
compare keys with both the lower and upper bound keys in
theSCAN operation. Additionally, the RSU variant responsi-
ble for log block processing must ﬁrst sort the log block to
enable iterating over the key-value pairs in order.
We leverage hardware-software co-design to optimize log
block sorting. Sorting does not compare keys and costs only
O(1)cycles per item. It uses 1-byte order hints stored in
log block items by inserts. Figure 7 illustrates an example
sequence of insertions into the log block. For clarity, the
number in the item represents its key. The number above the
item represents its order hint. The log block is initially empty.
Item 90 is inserted ﬁrst and is assigned order hint 0. Next,
item 60 is inserted. Since it is the smallest item in the log
block, it is assigned order hint 0. The order hints in existing
items are not changed. Then, item 30 is inserted with order
hint 0 since it is the smallest in the log block again. The ﬁnal
item is 45 and its order hint is 1.
Sorting creates an indirection array with offsets of items
in the log block in ascending key order. Figure 8 illustrates
sorting of the log block in Figure 7. It shows keys instead
of item offsets for clarity. Items are processed in the order
in which they are stored. When the item with order hint iis
processed, its offset in the log block is inserted into position
iin the indirection array and all the elements at positions
jiare shifted to the right. Since the indirection array is
stored in a large shift register, insertion costs one cycle and
7Memory Subsystem
Cache
Meta
Data
TableNode 
Address
Table
PCIe
(Host Memory)Onboard
DRAM
Response
Output
ArrayRequest
Input
ArrayLoad
BalancerResponse
Crossbar
Page
TableWrite
BackRoot
CacheFigure 9: Memory Subsystem
can be overlapped with fetching the next item. Any items with
version greater than the operation’s read version are ﬁltered
out. After sorting, the RSU uses the indirection array to stream
keys to the key comparison pipelines in order.
5 Memory subsystem
As shown in Figure 9, the memory subsystem includes the
cache and the page table. It caches B-Tree nodes to improve
performance by reducing off-chip memory accesses, reducing
memory accesses over PCIe, and maximizing the use of avail-
able off-chip bandwidth. The root node, which is accessed
by all requests, is cached in on-chip memory to save off-
chip bandwidth. Other interior nodes are cached in on-board
DRAM to reduce PCIe accesses, which both improves request
throughput and latency. The load balancer directs some mem-
ory accesses over PCIe even when they hit in the cache to use
all available off-chip memory bandwidth.
The memory subsystem supports multiple input/output in-
terfaces for parallel access from the interior-node search and
leaf-node scan engines. It also maintains a page table map-
ping node LIDs to node physical addresses. This is exposed
over PCIe for write operations to replace subtrees atomically
by changing mappings (Section 3.4). When the slow path for
a write operation changes a page table mapping, the cache
entry for the node with that LID is invalidated. Leaf nodes
are not cached to avoid the need for cache invalidations over
PCIe for every write operation to the B-Tree.
The metadata table keeps track of cached B-Tree nodes in
on-board DRAM. It implements a four-way set associative
cache indexed by LIDs. Each entry in a set records the node
LID, the physical address, and a 32-bit occupancy map where
each bit represents 256 bytes of node data (with 8 KB nodes).
The metadata table itself is stored in on-board DRAM. Honey-
comb uses a small on-chip metadata cache, which has capacity
for 1K entries in the current implementation, to minimize the
trafﬁc to on-board DRAM.
On cache misses, the cache module fetches an integral num-
ber of 256-byte chunks over PCIe and attempts to write themback to the cache. The write back module implements a lock-
ing scheme to avoid conﬂicts when the cache is invalidated on
page table updates or during B-Tree root updates. The write
back operation allocates space for a new node in the cache
only if the missing access was to the header and shortcut block
of the node. The write back module ﬁlls other chunks in the
allocated cache space when it reads sorted block segments for
the node. When interior nodes cannot ﬁt in the cache, we evict
a random node from the same set. We leave more complex
cache replacement policies for future investigation.
The metadata table determines if incoming memory read
requests are hits or misses, and sends them to the node address
table (NAT). The NAT is responsible for ensuring each request
has a consistent view of each node it visits. For example,
without the NAT, a request could read the shortcut block from
the cache and an inconsistent sorted block segment from host
memory after the page table entry for the node is updated.
The NAT maps the request sequence number to the physical
address of the node version ﬁrst accessed by the request. The
NAT entry can be written by both the metadata table and
the page table depending on whether the ﬁrst access by the
request to a node is a hit or a miss. When an access to a sorted
block hits in the cache, the physical address in the cache entry
must match the physical address recorded in the NAT for
the request. In case of a mismatch or cache miss, the sorted
block segment is always loaded from host memory using the
physical address from the NAT for the request.
With large caches, the accelerator can become bottlenecked
on accesses to on-board DRAM while underutilizing PCIe
bandwidth. To use all available off-chip memory bandwidth,
the load balancer directs some memory accesses that hit in the
cache to host memory over PCIe. It constantly monitors the
number of inﬂight operations and the total number of bytes
being read by those operations on both DRAM and PCIe
interfaces. Then balances load across the two interfaces.
6 Evaluation
This section compares Honeycomb with eRPC-Masstree [29,
38], a state-of-the-art ordered key-value store that supports
variable-size keys and values (we discuss why we chose this
baseline in Section 7). It also evaluates the impact of several
optimizations on performance.
6.1 Experimental setup
Experiments ran one server on one machine and clients on
another. They ran on a single socket of a dual-socket machine
because this provided the best cost-performance for both sys-
tems. Each socket had a 10-core Intel Xeon E5-2660 v3 @
2.89 GHz and four channels to 64 GB of DDR4-2133 DRAM.
The FPGA accelerator card had an Intel Arria 10 1150 FPGA
with two channels to 4 GB on-board DDR4-2133 DRAM,
two PCIe Gen3 x8 channels, and 50-Gbps Ethernet. Each
8A B C D E F
YCSB Workloads02468101214Throughput (M reqs/sec)UniformeRPC-Masstree Honeycomb
A B C D E F
YCSB Workloads020406080Efficiency (K reqs/sec/W)UniformeRPC-Masstree Honeycomb
A B C D E F
YCSB Workloads02468101214Throughput (M reqs/sec)Zipfian/LatesteRPC-Masstree Honeycomb
A B C D E F
YCSB Workloads020406080Efficiency (K reqs/sec/W)Zipfian/LatesteRPC-Masstree HoneycombFigure 10: Comparison of throughput and efﬁciency for YCSB workloads using uniform and Zipﬁan/latest distributions.
Logic Register Block RAM
Shell 14.0% 8.9% 14.6%
Networking 6.9% 2.4% 21.0%
B-Tree accelerator 33.0% 11.5% 35.6%
Memory subsystem 7.3% 2.9% 7.7%
Total 61.2% 25.7% 78.8%
Table 1: FPGA resource usage of Honeycomb.
A B C D E F
Read
OperationLOOKUP LOOKUP LOOKUP LOOKUPSCAN
(1 to 100)LOOKUP
Write
OperationUPDATE UPDATE - INSERT INSERTRD-MOD -
WR
Rd-Wr
Ratio (%)50:50 95:5 100:0 95:5 95:550:50
(66.6% LOOKUP )
Table 2: YCSB workloads.
machine had a separate 40-Gbps Ethernet ConnectX-3 NIC.
We used the socket that is attached to both the FPGA and
the NIC. We ran 10 software threads, each pinned to a differ-
ent core on the socket. Honeycomb ran on Windows Server
2016 Datacenter using the FPGA, and eRPC-Masstree ran on
Ubuntu 20.04 with DPDK 19.11.5 using the ConnectX-3 NIC.
Machines were connected to a DELL Z9100-ON switch. The
different bandwidths did not affect the comparison because
eRPC-Masstree was never bottlenecked on the network.
We conﬁgured Honeycomb with MVCC by default even
though eRPC-Masstree does not provide linearizability for
scans. The modular design of the Honeycomb accelerator en-
ables the user to choose conﬁgurations with different numbers
of KSUs, RSUs, and MSIs. This is critical to achieving good
power efﬁciency because it allows the user to trade off perfor-
mance for power and hardware resource efﬁciency. We used a
simple analytic performance model and tuning experiments to
select the minimum number of these units to achieve a target
SCAN operation throughput of approximately 10 Mops/s. We
used 14 KSUs, 4 shortcut-RSUs, 5 log-RSUs, 5 sorted-RSUs,
and 4 MSI adapters in all the experiments described in this pa-
per. The accelerator is clocked at 220 MHz. The breakdown
of FPGA resource usage is shown in Table 1. The FPGA
design tool [26] reports a TDP of 34.9 W.
We ran each experiment three times and present the averageof the results. The range of the results was below 4% of the
average for all experiments.
6.2 Workloads
We used two sets of workloads in the evaluation YCSB [1, 15]
and a cloud-storage workload representative of the distributed
ﬁle system application [9] discussed throughout the paper. We
ran all six YCSB workloads (see Table 2) with both uniform
and skewed access patterns. The read-modify-write operation
in YCSB-F is a combination of a LOOKUP followed by an
UPDATE . The cloud-storage workload is similar to YCSB-E
but uses shorter scans and we varied the percentage of scans
from 50% to 100% to characterize the range of read-write
ratios for which Honeycomb is beneﬁcial. We also varied the
number of key-value pairs returned by scans and the size of
keys and values. In all workloads, insert keys were generated
randomly with uniform distribution as in [52]. We used both
uniform and Zipﬁan [15] ( q=0:99) distributions for lookup
keys and the start keys of scans.
All experiments used the uniform distribution, and 16-byte
keys and values unless speciﬁed otherwise. The store had 128
million key-value pairs in the initial state for all experiments,
which are stored in a 4-level tree in Honeycomb. We observed
that eRPC-Masstree consumes 3more memory than the
total size of the key-value pairs to store the whole B-Tree,
whereas Honeycomb only consumes about 1 :44.
6.3 Comparison with eRPC-Masstree
Cost-performance is the key metric to optimize in large scale
data centers. We use TDP as a proxy for total cost of own-
ership (TCO) as in [28]. We use a single-socket server for
Honeycomb because adding another socket increases TDP
without increasing the throughput of the hardware accelera-
tor. We ran experiments with two sockets for eRPC-Masstree
but they resulted in worse cost-performance, e.g., it achieved
1:34better cost-performance with one socket than with
two for read-only 3-item scan workloads. Therefore, we also
present single socket results for eRPC-Masstree. We compute
TDP by adding numbers for each component from published
documentation. The server TDP is 127 W for eRPC-Masstree
950 60 70 80 90 95 100
Read Percentage024681012Throughput (M reqs/sec)UniformeRPC-Masstree Honeycomb
50 60 70 80 90 95 100
Read Percentage010203040506070Efficiency (K reqs/sec/W)UniformeRPC-Masstree Honeycomb
50 60 70 80 90 95 100
Read Percentage024681012Throughput (M reqs/sec)ZipfianeRPC-Masstree Honeycomb
50 60 70 80 90 95 100
Read Percentage010203040506070Efficiency (K reqs/sec/W)ZipfianeRPC-Masstree HoneycombFigure 11: Comparison of throughput and efﬁciency for cloud-storage workloads with 50% to 100% reads using uniform and
Zipﬁan distributions. The read operation is SCAN with 3 to 4 items in the range.
0 2 4 6 8 10 12
Throughput (M reqs/sec)0510152025Median Latency (us)
eRPC-Masstree
Honeycomb
Figure 12: Latency-throughput.
1246810 15 24
Number of items in scan range02468101214Throughput (M reqs/sec)
Honeycomb
eRPC-Masstree Figure 13: Varying SCAN size.
8 16 24 32 Mixed
Key Size (Bytes)02468101214Throughput (M reqs/sec)eRPC-Masstree Honeycomb Figure 14: Varying key sizes.
50 60 70 80 90
Read Percentage024681012Throughput (M reqs/sec)MVCC-On MVCC-Off Figure 15: Impact of MVCC.
and 157.9 W for Honeycomb. The TDP for Honeycomb is
larger because it uses a 40-W FPGA accelerator board instead
of a 10-W ConnectX-3 NIC [40].
Both the throughput and the TDP should scale by a factor
of two when using an additional CPU socket connected to
an additional NIC (for eRPC-Masstree) or SmartNIC (for
Honeycomb). Therefore, the performance per watt of TDP
for such a system would be similar to the results we present
with a single socket.
YCSB throughput and efﬁciency: Figure 10 compares
both average throughput (Mreqs/s) and throughput per watt of
TDP (Kreqs/s/W) of eRPC-Masstree and Honeycomb. Since
Honeycomb is not optimized for write-heavy workloads, it
is less efﬁcient running YCSB-A and F. For YCSB-B, C and
D, Honeycomb improves throughput per watt by 1:5with
both uniform and Zipﬁan/latest distributions. It also improves
the throughput for these workloads by 1:9with uniform
and1:8with Zipﬁan/latest distributions. Since Honeycomb
is optimized for scans, it does particularly well in YCSB-E
improving throughout by up to 2:9and efﬁciency by up
2:3. Honeycomb is bottlenecked on the network for large
scans while eRPC-Masstree is always bottlenecked on the
CPU.
Cloud storage throughput and efﬁciency: Figure 11
compares performance and efﬁciency of eRPC-Masstree and
Honeycomb running the cloud-storage workload. In this ex-
periment, the boundary keys in Honeycomb SCAN (Kl, Ku)
requests were chosen to return exactly three items when exe-
cuted on the inital key-value store. However, they can return
between three and four items because of newly-inserted items.Since eRPC-Masstree provides a different SCAN (K,N) op-
eration that returns the Nitems following K, we selected a
value of Nbetween three and four to ensure SCAN operations
return the same average number of items in both systems. For
high read percentages, Honeycomb also uses CPU cores to
execute SCAN operations. We conﬁgured eRPC-Masstree to
enable any thread to execute SCAN s.
The results show that Honeycomb improves both perfor-
mance and cost-performance signiﬁcantly for scan-mostly
workloads. Since Honeycomb accelerates only read opera-
tions, the improvement grows with the read percentage. In
the worst case of 100% writes, Honeycomb achieves only
58% of the throughput of eRPC-Masstree (47% of the cost-
performance) because there is no hardware acceleration and
the B-Tree is optimized for hybrid CPU-FPGA execution.
However, even for workloads with 50% writes, Honeycomb
achieves a similar throughput per watt as eRPC-Masstree and
better throughput ( 1:2better with uniform distribution). For
workloads with at least 80% reads, Honeycomb improves
throughput per watt by 1:9with uniform and 1:6with Zip-
ﬁan distributions. It also improves the throughput for these
workloads by 2:3with uniform and 2:0with Zipﬁan dis-
tributions.
As in YCSB workloads, the gains from acceleration are
lower with the Zipﬁan distribution. Whereas eRPC-Masstree
can leverage better locality with CPU caching, the current
implementation of Honeycomb does not cache leaf nodes,
which prevents it from caching leaves containing popular
items. We plan to explore leaf caching in the future.
With modern cloud storage server designs that leverage
100RT163264128256    NoLB
Cache Size (MB)0246810Throughput (M reqs/sec)Throughput
020406080100
Hit Rate (%)
Cache Hit Rate(a) Performance and cache hit rate.
 RT 16 32 64 128 256 NoLB  
Cache Size (MB)02468101214Bandwidth (GB/sec)Host PCIe-RD
Cache DRAM-RD
Metadata DRAM-RDPage T able DRAM-RD
Cache DRAM-WR
Metadata DRAM-WR (b) FPGA memory bandwidth breakdown.
 RT 16 32 64 128 256 NoLB  
Cache Size (MB)051015202530354045IOPS (M ops/sec)Host PCIe-RD
Cache DRAM-RD
Metadata DRAM-RDPage T able DRAM-RD
Cache DRAM-WR
Metadata DRAM-WR (c) FPGA memory IOPS breakdown.
Figure 16: Performance impact of caching and load balancing on Honeycomb (FPGA only) 1-item SCAN .
0 256 512 768
Leaf-node Log Block Size (Bytes)02468101214Throughput (M reqs/sec)Insert 1-Item Scan
Figure 17: Performance impact of log block size.
NVMe SSDs [44, 49] and fast networks to provide tens of
millions of IOPS per server, indexing metadata requires pow-
erful CPUs that account for a large fraction of the overall
TCO, e.g., CPUs, DRAM, and the NIC account for half the
TDP in Open Compute Project’s Poseidon [31] storage server.
Therefore, Honeycomb can signiﬁcantly increase overall per-
formance per TCO for these storage servers, e.g., we estimate
improvements around 20% for server designs similar to Po-
seidon. This is very signiﬁcant for large-scale data center
deployments.
We expect Honeycomb’s cost-performance gains to in-
crease with future hardware because newer FPGAs have more
on-chip memory to cache B-Tree nodes and use PCIe Gen5
that has 4the bandwidth of PCIe Gen3. Despite the band-
width improvement, we expect PCIe to remain the bottleneck
because we can use conﬁgurations with more KSUs, RSUs,
and MSIs to increase parallelism. Therefore, the techniques
proposed in this paper will continue to be important.
End-to-end latency: Figure 12 shows throughput-latency
curves for median latency measured at the client in a read-only
workload where each SCAN returns exactly three items. Hon-
eycomb can provide better throughput than eRPC-Masstree’s
throughput at lower latency. However, eRPC-Masstree has
lower latency at low load. This is mostly because Honeycomb
memory accesses over PCIe and to on-board DRAM havehigher latency than CPU memory accesses.
Scan size: Figure 13 shows the throughput of both systems
for a read-only workload when varying the number of items re-
turned by SCAN s. The gains of acceleration increase with scan
size, for example, Honeycomb has 4:0better throughput and
3:2better throughput per watt with 24-item scans. Since
eRPC-Masstree must follow pointers to each item in the scan
range, these random memory accesses become a bottleneck.
Honeycomb can amortize PCIe accesses to a leaf node over
many items because it inlines variable-sized items in leaves.
With longer scans Honeycomb becomes network bound while
eRPC-Masstree remains CPU bound (as observed in YCSB-
E).
Key size: Figure 14 shows the throughput of both systems
running 1-item SCAN on the initial store when the size of
keys and values increases (equal key and value sizes). We
use 1-item SCAN to better isolate the impact of increasing
key sizes on tree traversal. The performance of both systems
drops as key size increases. eRPC-Masstree has deeper trees
to traverse with larger keys. The depth of the Honeycomb
does not change but the accelerator must fetch larger sorted
block segments. We also compared both systems on a store
with a mix of key and value sizes chosen uniformly from
multiples of 8B less than or equal to 32B. These demonstrate
that Honeycomb performs well with variable-sized KV pairs.
6.4 Impact of optimizations
We ran experiments to investigate the performance impact of
using MVCC, log blocks, the cache, and the load balancer.
Cost of MVCC: All experiments ran with MVCC to pro-
vide linearizable scans. Since eRPC-Masstree does not pro-
vide linearizable scans, we also ran experiments to evaluate
the impact of MVCC on throughput. Figure 15 shows that
turning off MVCC improves Honeycomb performance on
cloud-storage workloads by up to 14% when the workload
is bottlenecked by INSERT s. The overhead on read-heavy
workloads is negligible.
11Log block: Figure 17 shows throughput of 1-item scans
in a read-only workload and of inserts in a write-only work-
load with varying log block size. Using log blocks improves
insert performance because it avoids adding the new item
to the sorted block, adjusting shortcuts, and updating the ac-
celerator page table on every operation. However, it reduces
SCAN performance because it increases the amount of data
accessed over PCIe. Using a 512-byte block achieves most of
the beneﬁt while decreasing SCAN throughput by only 8%.
Accelerator cache and load balancer: Figure 16 shows
the impact of caching on 1-item SCAN throughput in a read-
only workload using only the FPGA (no CPU). Figure 16a
shows request throughput and cache hit rate for interior node
accesses. Caching the root node in on-chip memory (RT) im-
proves performance by 30% over the no-cache case. Adding
an on-board DRAM cache with 256 MB (which can hold
all interior nodes) improves performance by 2:5. The load
balancer directs some cache hits to host memory over PCIe to
maximize off-chip bandwidth utilization. Removing the load
balancer (NoLB) decreases throughput by 13%.
Figures 16b and 16c break down bandwidth and IOPS
for PCIe and on-board DRAM trafﬁc. For small cache sizes,
the bottleneck is PCIe bandwidth and there are writes to on-
board DRAM due to cache replacement. With a 256 MB
cache (100% hit rate) and no load balancing (NoLB), on-
board DRAM bandwidth is the bottleneck while 4 GB/s of
PCIe bandwidth is left unused. The load balancer shifts trafﬁc
to PCIe to maximize off-chip bandwidth utilization increasing
throughput from 8.0 to 9.1 Mreqs/s. Page table and cache
metadata reads consume a small fraction of available on-
board DRAM bandwidth but a signiﬁcant fraction of IOPS.
For small cache sizes, there are no metadata reads from on-
board DRAM because all metadata ﬁts in the on-chip cache.
7 Related work
We build on a large body of work on indices for in-memory or-
dered key-value stores (e.g., [33,34,38,51,54]). A comparison
study [51] showed that the Adaptive Radix Tree (ART) [33]
performed best followed by Masstree [38] except on scan-
dominated workloads where a B+ tree [13] variant was better.
We chose Masstree [38] as a baseline because it has better
scan performance than ART and it supports variable-size
keys unlike the B+ tree variant in [51]. Cuckoo Trie [54]
exploits memory level parallelism to improve multi-core per-
formance but it has worse scan performance than ART. None
of these indices supports linearizable scans. Given our focus
on scan-dominated workloads, Honeycomb implements a B+
tree variant with support for variable-size keys and MVCC.
Many of the optimizations in recent work exploit the memory
hierarchy of modern server-class CPUs and are not applicable
to Honeycomb where the index is accessed over PCIe.
Ordered key-value stores shard an index across servers
and provide access to remote clients across the network. Re-cent research has shown how to leverage kernel-bypass using
DPDK [27] to implement eRPC [29], a fast RPC mechanism.
eRPC and Masstree have been used to implement a high
throughput, low-latency, key-value store [29] that we use as
our baseline. Other research has explored using one-sided
RDMA reads to bypass the server CPU for GET and SCAN op-
erations [12, 18, 41, 52, 56]. Since RDMA NICs only provide
simple reads of contiguous memory, these systems require at
least two RDMA reads per operation to support variable-sized
keys or values, and they use client-side caching to avoid addi-
tional RDMA reads when traversing the index. XStore [52]
uses a learned cache, a compact client-side cache design in-
spired by the work on learned indices [16, 32], but it does not
support variable-sized keys or linearizable SCAN s.
CliqueMap [48] implements a hybrid in-memory key-value
caching system with both RPC and remote memory access
(RMA). Like Honeycomb, it accelerates GET with RMA and
executes SETvia RPC to save CPU cost and improve perfor-
mance. However, this implementation is based on a hash table
and does not provide support for efﬁcient SCAN s.
There are proposals to extend RDMA, e.g., [2,8], but these
have yet to be implemented in NIC hardware.
SmartNICs avoid the functionality limitations of RDMA
by providing programmable processing engines in the NIC.
There are two types: SoC-based with general purpose cores
and accelerators for common functionality like compres-
sion and encryption (see [37] for a survey); and FPGA-
based [10, 45]. Since the later offer the promise of better
performance and energy efﬁciency at the cost of being harder
to program, Honeycomb leverages an FPGA-based SmartNIC
but implements complex split and merge of B-Tree nodes on
the host CPU.
SmartNICs have been used as ofﬂoads for accelerating
networking, e.g., [3, 6, 19 –21, 36, 43], AI inference [22],
distributed ﬁle systems [30], transactions [47], unordered
key-value stores [11, 19, 35, 46], and ordered key-value
stores [23, 37, 53].
KV-direct [35], which is the best unordered key-value store
ofﬂoad, stores the index in host memory and implements
hash table reads and writes, which are much simpler than B-
Tree writes, in the FPGA. It achieves better performance than
Honeycomb because this reduces synchronization across PCIe
and hash table operations only require O(1) memory accesses,
but it does not provide SCAN s. The best ordered key-value
store ofﬂoad, HeteroKV [53], implements a B-Tree where the
leaves are hash tables, which results in poor scan performance.
It cannot support large stores because it stores the B-Tree in
on-chip FPGA memory, and it supports only ﬁxed-size keys.
Honeycomb provides support for large stores, variable-sized
keys and values, fast scans, and strong consistency.
128 Conclusion
In-memory ordered key-value stores are an important building
block in modern distributed applications. We presented Hon-
eycomb, a system that leverages an FPGA-based SmartNIC to
accelerate these stores with a focus on scan-dominated work-
loads. It stores a B-Tree in host memory, and executes SCAN
and GET operations on the FPGA, and PUT,UPDATE , and
DELETE operations on the CPU. This approach enables large
stores and simpliﬁes the FPGA implementation but raises
the challenge of data access and synchronization across slow
PCIe. We described how Honeycomb addresses this challenge
by using large B-Tree nodes with shortcuts; caching in on-
chip and on-board FPGA memory; exploiting request-level
parallelism with out-of-order execution; making SCAN and
GET operations wait free; and using a log in B-Tree nodes to
batch synchronization across PCIe. Honeycomb is evalutated
against a state-of-the-art ordered key-value store using both
YCSB and a cloud-storage-inspired workload. The compar-
ison shows that Honeycomb improves throughput by 1:9
for uniform read-dominated workloads in YCSB and by 2:3
for uniform cloud-storage-inspired workloads with more than
80% SCAN operations. Most importantly, Honeycomb im-
proves cost-performance, which is the key metric to optimize
in large-scale data centers; it improves throughput per watt
of TDP (a proxy for TCO) by more than 1:5and1:9for
these two set of workloads respectively.
Acknowledgments
We thank Vadim Makhervaks, Lukasz Tomczyk, Prahasaran
Asokan, and Ankit Agrawal for their help and discussions on
exploring Honeycomb in cloud storage services. We thank
Andrew Putnam for his support and help with onboarding
Catapult FPGA platform. We thank Nikita Lazarev, David
Sidler, Mikhail Asiatici and Fabio Maschi who worked on
related projects during their internships at Microsoft Research.
We thank the members of Cloud System Futures team in
Micrsoft Research Cambridge for their help and feedback.
References
[1]Github reporsitory of YCSB. https://github.com/
brianfrankcooper/YCSB .
[2]Marcos K Aguilera, Kimberly Keeton, Stanko No-
vakovic, and Sharad Singhal. Designing far memory
data structures: Think outside the box. In Proceedings
of the Workshop on Hot Topics in Operating Systems ,
pages 120–126, 2019.
[3]Mina Tahmasbi Arashloo, Alexey Lavrov, Manya
Ghobadi, Jennifer Rexford, David Walker, and David
Wentzlaff. Enabling programmable transport protocolsin high-speed nics. In 17th USENIX Symposium on Net-
worked Systems Design and Implementation (NSDI 20) ,
pages 93–109, 2020.
[4]Berk Atikoglu, Yuehai Xu, Eitan Frachtenberg, Song
Jiang, and Mike Paleczny. Workload analysis of a large-
scale key-value store. In Proceedings of the 12th ACM
SIGMETRICS/PERFORMANCE joint international con-
ference on Measurement and Modeling of Computer
Systems , pages 53–64, 2012.
[5]Philip A Bernstein, Vassos Hadzilacos, and Nathan
Goodman. Concurrency control and recovery in
database systems , volume 370. Addison-wesley Read-
ing, 1987.
[6]Marco Spaziani Brunella, Giacomo Belocchi, Marco
Bonola, Salvatore Pontarelli, Giuseppe Siracusano,
Giuseppe Bianchi, Aniello Cammarano, Alessandro
Palumbo, Luca Petrucci, and Roberto Bifulco. hxdp:
Efﬁcient software packet processing on fFPGAgnics.
In14thfUSENIXgSymposium on Operating Systems
Design and Implementation ( fOSDIg20), pages 973–
990, 2020.
[7]Chiranjeeb Buragohain, Knut Magne Risvik, Paul Brett,
Miguel Castro, Wonhee Cho, Joshua Cowhig, Nikolas
Gloy, Karthik Kalyanaraman, Richendra Khanna, John
Pao, et al. A1: A distributed in-memory graph database.
InProceedings of the 2020 ACM SIGMOD International
Conference on Management of Data , pages 329–344,
2020.
[8]Matthew Burke, Shannon Joyner, Adriana Szekeres, Ja-
cob Nelson, Irene Zhang, and Dan RK Ports. Prism:
Rethinking the rdma interface for distributed systems.
InProceedings of the ACM SIGOPS 28th Symposium on
Operating Systems Principles CD-ROM , pages 228–242,
2021.
[9]Brad Calder, Ju Wang, Aaron Ogus, Niranjan Nilakan-
tan, Arild Skjolsvold, Sam McKelvie, Yikang Xu, Shash-
wat Srivastav, Jiesheng Wu, Huseyin Simitci, et al. Win-
dows azure storage: a highly available cloud storage
service with strong consistency. In Proceedings of the
Twenty-Third ACM Symposium on Operating Systems
Principles , pages 143–157, 2011.
[10] Adrian M. Caulﬁeld, Eric S. Chung, Andrew Put-
nam, Hari Angepat, Jeremy Fowers, Michael Haselman,
Stephen Heil, Matt Humphrey, Puneet Kaur, Joo-Young
Kim, Daniel Lo, Todd Massengill, Kalin Ovtcharov,
Michael Papamichael, Lisa Woods, Sitaram Lanka,
Derek Chiou, and Doug Burger. A cloud-scale accelera-
tion architecture. In The 49th Annual IEEE/ACM Inter-
national Symposium on Microarchitecture , MICRO-49,
Taipei, Taiwan, 2016. IEEE Press.
13[11] Sai Rahul Chalamalasetti, Kevin Lim, Mitch Wright,
Alvin AuYoung, Parthasarathy Ranganathan, and Martin
Margala. An fpga memcached appliance. In Proceed-
ings of the ACM/SIGDA international symposium on
Field programmable gate arrays , pages 245–254, 2013.
[12] Yanzhe Chen, Xingda Wei, Jiaxin Shi, Rong Chen, and
Haibo Chen. Fast and general distributed transactions
using rdma and htm. In Proceedings of the Eleventh
European Conference on Computer Systems , pages 1–
17, 2016.
[13] Douglas Comer. Ubiquitous b-tree. ACM Computing
Surveys (CSUR) , 11(2):121–137, 1979.
[14] Redis community. redis. https://redis.io/ .
[15] Brian F Cooper, Adam Silberstein, Erwin Tam, Raghu
Ramakrishnan, and Russell Sears. Benchmarking cloud
serving systems with ycsb. In Proceedings of the 1st
ACM symposium on Cloud computing , pages 143–154,
2010.
[16] Jialin Ding, Umar Farooq Minhas, Jia Yu, Chi Wang,
Jaeyoung Do, Yinan Li, Hantian Zhang, Badrish Chan-
dramouli, Johannes Gehrke, Donald Kossmann, et al.
Alex: an updatable adaptive learned index. In Proceed-
ings of the 2020 ACM SIGMOD International Confer-
ence on Management of Data , pages 969–984, 2020.
[17] Siying Dong, Mark Callaghan, Leonidas Galanis,
Dhruba Borthakur, Tony Savor, and Michael Strum. Op-
timizing space ampliﬁcation in rocksdb. In CIDR , vol-
ume 3, page 3, 2017.
[18] Aleksandar Dragojevi ´c, Dushyanth Narayanan, Ed-
mund B Nightingale, Matthew Renzelmann, Alex
Shamis, Anirudh Badam, and Miguel Castro. No com-
promises: Distributed transactions with consistency,
availability, and performance. In Proceedings of the
25th symposium on operating systems principles , pages
54–70, 2015.
[19] Haggai Eran, Lior Zeno, Maroun Tork, Gabi Malka, and
Mark Silberstein.fNICAg: An infrastructure for inline
acceleration of network applications. In 2019 USENIX
Annual Technical Conference (USENIX ATC 19) , pages
345–362, 2019.
[20] Daniel Firestone.fVFPg: A virtual switch platform for
hostfSDNgin the public cloud. In 14thfUSENIXg
Symposium on Networked Systems Design and Imple-
mentation (fNSDIg17), pages 315–328, 2017.
[21] Daniel Firestone, Andrew Putnam, Hari Angepat, Derek
Chiou, Adrian Caulﬁeld, Eric Chung, Matt Humphrey,
Kalin Ovtcharov, Jitu Padhye, Doug Burger, Dave
Maltz, Albert Greenberg, Sambhrama Mundkur, AlirezaDabagh, Mike Andrewartha, Vivek Bhanu, Harish Ku-
mar Chandrappa, Somesh Chaturmohta, Jack Lavier,
Norman Lam, Fengfen Liu, Gautham Popuri, Shachar
Raindel, Tejas Sapre, Mark Shaw, Gabriel Silva, Mad-
han Sivakumar, Nisheeth Srivastava, Anshuman Verma,
Qasim Zuhair, Deepak Bansal, Kushagra Vaid, and
David A. Maltz. Azure accelerated networking: Smart-
nics in the public cloud. In 15th USENIX Symposium on
Networked Systems Design and Implementation (NSDI) ,
April 2018.
[22] Jeremy Fowers, Kalin Ovtcharov, Michael Papamichael,
Todd Massengill, Ming Liu, Daniel Lo, Shlomi Alka-
lay, Michael Haselman, Logan Adams, Mahdi Ghandi,
Stephen Heil, Prerak Patel, Adam Sapek, Gabriel Weisz,
Lisa Woods, Sitaram Lanka, Steve Reinhardt, Adrian
Caulﬁeld, Eric Chung, and Doug Burger. A conﬁgurable
cloud-scale dnn processor for real-time ai. In Proceed-
ings of the 45th International Symposium on Computer
Architecture, 2018 . ACM, June 2018.
[23] Dennis Heinrich, Stefan Werner, Marc Stelzner, Christo-
pher Blochwitz, Thilo Pionteck, and Sven Groppe. Hy-
brid fpga approach for a b+ tree in a semantic web
database system. In 2015 10th International Symposium
on Reconﬁgurable Communication-centric Systems-on-
Chip (ReCoSoC) , pages 1–8. IEEE, 2015.
[24] Maurice Herlihy. Wait-free synchronization. ACM
Transactions on Programming Languages and Systems
(TOPLAS) , 13(1):124–149, 1991.
[25] Maurice P Herlihy and Jeannette M Wing. Linearizabil-
ity: A correctness condition for concurrent objects. ACM
Transactions on Programming Languages and Systems
(TOPLAS) , 12(3):463–492, 1990.
[26] Intel. Intel Quartus Prime Pro Edition User Guide:
Power Analysis and Optimization. https://www.
intel.com/content/www/us/en/programmable/
documentation/osq1513989409475.html , 2021.
[27] DPDK Intel. Data plane development kit, 2014.
[28] Norman P Jouppi, Doe Hyun Yoon, Matthew Ashcraft,
Mark Gottscho, Thomas B Jablin, George Kurian, James
Laudon, Sheng Li, Peter Ma, Xiaoyu Ma, et al. Ten
lessons from three generations shaped google’s tpuv4i:
Industrial product. In 2021 ACM/IEEE 48th Annual
International Symposium on Computer Architecture
(ISCA) , pages 1–14. IEEE, 2021.
[29] Anuj Kalia, Michael Kaminsky, and David Andersen.
Datacenter rpcs can be general and fast. In 16th
fUSENIXgSymposium on Networked Systems Design
and Implementation ( fNSDIg19), pages 1–16, 2019.
14[30] Jongyul Kim, Insu Jang, Waleed Reda, Jaeseong Im,
Marco Canini, Dejan Kosti ´c, Youngjin Kwon, Simon
Peter, and Emmett Witchel. Linefs. In Proceedings
of the ACM SIGOPS 28th Symposium on Operating
Systems Principles CD-ROM . ACM, 2021.
[31] Jungsoo Kim and Alan Chang. Po-
seidon V1 E1.S SSD Storage System.
https://www.opencompute.org/documents/
poseidon-v1-reference-system-spec-pdf , 2022.
[32] Tim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and
Neoklis Polyzotis. The case for learned index structures.
InProceedings of the 2018 International Conference on
Management of Data , pages 489–504, 2018.
[33] Viktor Leis, Alfons Kemper, and Thomas Neumann. The
adaptive radix tree: Artful indexing for main-memory
databases. In 2013 IEEE 29th International Conference
on Data Engineering (ICDE) , pages 38–49. IEEE, 2013.
[34] Justin J. Levandoski, David B. Lomet, and Sudipta Sen-
gupta. The bw-tree: A b-tree for new hardware plat-
forms. In 2013 IEEE 29th International Conference on
Data Engineering (ICDE) , pages 302–313, 2013.
[35] Bojie Li, Zhenyuan Ruan, Wencong Xiao, Yuanwei Lu,
Yongqiang Xiong, Andrew Putnam, Enhong Chen, and
Lintao Zhang. Kv-direct: High-performance in-memory
key-value store with programmable nic. In Proceedings
of the 26th Symposium on Operating Systems Principles ,
pages 137–152, 2017.
[36] Bojie Li, Kun Tan, Layong Luo, Yanqing Peng, Ren-
qian Luo, Ningyi Xu, Yongqiang Xiong, Peng Cheng,
and Enhong Chen. Clicknp: Highly ﬂexible and high
performance network processing with reconﬁgurable
hardware. In Proceedings of the 2016 ACM SIGCOMM
Conference , pages 1–14, 2016.
[37] Ming Liu, Tianyi Cui, Henry Schuh, Arvind Krishna-
murthy, Simon Peter, and Karan Gupta. Ofﬂoading dis-
tributed applications onto smartnics using ipipe. In
Proceedings of the ACM Special Interest Group on Data
Communication , pages 318–333. 2019.
[38] Yandong Mao, Eddie Kohler, and Robert Tappan Morris.
Cache craftiness for fast multicore key-value storage. In
Proceedings of the 7th ACM european conference on
Computer Systems , pages 183–196, 2012.
[39] Paul E McKenney and Jonathan Walpole. What is rcu,
fundamentally? Linux Weekly News (LWN. net) , 2007.
[40] Mellanox. ConnectX-3 VPI Single and
Dual QSFP Port Adapter Card User Manual.
https://www.mellanox.com/related-docs/user_
manuals/ConnectX-3_VPI_Single_and_Dual_QSFP_Port_Adapter_Card_User_Manual.pdf ,
2013.
[41] Christopher Mitchell, Kate Montgomery, Lamont Nel-
son, Siddhartha Sen, and Jinyang Li. Balancing
fCPUgand network in the cell distributed b-tree
store. In 2016fUSENIXgAnnual Technical Confer-
ence (fUSENIXgfATCg16), pages 451–464, 2016.
[42] Rajesh Nishtala, Hans Fugal, Steven Grimm, Marc
Kwiatkowski, Herman Lee, Harry C Li, Ryan McElroy,
Mike Paleczny, Daniel Peek, Paul Saab, et al. Scaling
memcache at facebook. In 10thfUSENIXgSympo-
sium on Networked Systems Design and Implementation
(fNSDIg13), pages 385–398, 2013.
[43] Phitchaya Mangpo Phothilimthana, Ming Liu, Antoine
Kaufmann, Simon Peter, Rastislav Bodik, and Thomas
Anderson. Floem: A programming system for nic-
accelerated network applications. In 13thfUSENIXg
Symposium on Operating Systems Design and Imple-
mentation (fOSDIg18), pages 663–679, 2018.
[44] Open Compute Project. Samsung PM9A3 NVMe PCIe
SSD. https://www.opencompute.org/products/
262/samsung-pm9a3-nvme-pcie-ssd .
[45] Andrew Putnam, Adrian M. Caulﬁeld, Eric S. Chung,
Derek Chiou, Kypros Constantinides, John Demme,
Hadi Esmaeilzadeh, Jeremy Fowers, Gopi Prashanth
Gopal, Jan Gray, Michael Haselman, Scott Hauck,
Stephen Heil, Amir Hormati, Joo-Young Kim, Sitaram
Lanka, James Larus, Eric Peterson, Simon Pope, Aaron
Smith, Jason Thong, Phillip Yi Xiao, and Doug Burger.
A reconﬁgurable fabric for accelerating large-scale data-
center services. In Proceeding of the 41st Annual Inter-
national Symposium on Computer Architecuture , ISCA
’14, page 13–24, Minneapolis, Minnesota, USA, 2014.
IEEE Press.
[46] Yuchen Ren, Jinyu Xie, Yunhui Qiu, Hankun Lv, Wenbo
Yin, Lingli Wang, Bowei Yu, Hua Chen, Xianjun He,
Zhijian Liao, et al. A low-latency multi-version key-
value store using b-tree on an fpga-cpu platform.
In2019 29th International Conference on Field Pro-
grammable Logic and Applications (FPL) , pages 321–
325. IEEE, 2019.
[47] Henry N Schuh, Weihao Liang, Ming Liu, Jacob Nel-
son, and Arvind Krishnamurthy. Xenic: Smartnic-
accelerated distributed transactions. In Proceedings
of the ACM SIGOPS 28th Symposium on Operating
Systems Principles , pages 740–755, 2021.
[48] Arjun Singhvi, Aditya Akella, Maggie Anderson, Rob
Cauble, Harshad Deshmukh, Dan Gibson, Milo M. K.
Martin, Amanda Strominger, Thomas F. Wenisch, and
15Amin Vahdat. Cliquemap: Productionizing an rma-
based distributed caching system. In Proceedings of the
2021 ACM SIGCOMM 2021 Conference , SIGCOMM
’21, page 93–105, New York, NY , USA, 2021. Associa-
tion for Computing Machinery.
[49] Ross Stenfort, Ta-Yu Wu, Lee Prewitt, Paul
Kaler, David Derosa, William Lynn, and Austin
Bolen. Datacenter NVMe SSD Speciﬁcation.
https://www.opencompute.org/documents/
datacenter-nvme-ssd-specification-v2-0r21-pdf ,
2021.
[50] Venkateshwaran Venkataramani, Zach Amsden, Nathan
Bronson, George Cabrera III, Prasad Chakka, Peter Di-
mov, Hui Ding, Jack Ferris, Anthony Giardullo, Jeremy
Hoon, et al. Tao: how facebook serves the social graph.
InProceedings of the 2012 ACM SIGMOD International
Conference on Management of Data , pages 791–792,
2012.
[51] Ziqi Wang, Andrew Pavlo, Hyeontaek Lim, Viktor Leis,
Huanchen Zhang, Michael Kaminsky, and David G An-
dersen. Building a bw-tree takes more than just buzz
words. In Proceedings of the 2018 International Con-
ference on Management of Data , pages 473–488, 2018.
[52] Xingda Wei, Rong Chen, and Haibo Chen. Fast rdma-
based ordered key-value store using remote learned
cache. In 14thfUSENIXgSymposium on Operating Sys-
tems Design and Implementation ( fOSDIg20), pages
117–135, 2020.
[53] Haichang Yang, Zhaoshi Li, Jiawei Wang, Shouyi Yin,
Shaojun Wei, and Leibo Liu. Heterokv: A scalable line-
rate key-value store on heterogeneous cpu-fpga plat-
forms. In 2021 Design, Automation & Test in Europe
Conference & Exhibition (DATE) , pages 834–837. IEEE,
2021.
[54] Adar Zeitak and Adam Morrison. Cuckoo trie: Exploit-
ing memory-level parallelism for efﬁcient dram index-
ing. In Proceedings of the ACM SIGOPS 28th Sympo-
sium on Operating Systems Principles , pages 147–162,
2021.
[55] Yibo Zhu, Haggai Eran, Daniel Firestone, Chuanxiong
Guo, Marina Lipshteyn, Yehonatan Liron, Jitendra Pad-
hye, Shachar Raindel, Mohamad Haj Yahia, and Ming
Zhang. Congestion control for large-scale rdma deploy-
ments. ACM SIGCOMM Computer Communication
Review , 45(4):523–536, 2015.
[56] Tobias Ziegler, Sumukha Tumkur Vani, Carsten Bin-
nig, Rodrigo Fonseca, and Tim Kraska. Designingdistributed tree-based index structures for fast rdma-
capable networks. In Proceedings of the 2019 Inter-
national Conference on Management of Data , pages
741–758, 2019.
16