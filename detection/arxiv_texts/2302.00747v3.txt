UNIVERSAL SOLDIER: USING UNIVERSAL ADVERSARIAL PERTURBATIONS FOR
DETECTING BACKDOOR ATTACKS
Xiaoyun Xu⋆Oguzhan Ersoy⋆Hamidreza Tajalli⋆Stjepan Picek⋆
⋆Digital Security Group, Radboud University, The Netherlands
{xiaoyun.xu, oguzhan.ersoy, hamidreza.tajalli, stjepan.picek }@ru.nl
ABSTRACT
This paper proposes a backdoor detection method by utilizing
a particular type of adversarial attack, universal adversarial
perturbation (UAP), and its similarities with a backdoor trig-
ger. We observe an intuitive phenomenon: UAPs generated
from backdoored models need fewer perturbations to mislead
the model than UAPs from clean models. UAPs of back-
doored models tend to exploit the shortcut from all classes
to the target class, built by the backdoor trigger. We pro-
pose a novel method called Universal Soldier for Backdoor
detection ( USB) and reverse engineering potential backdoor
triggers via UAPs. Experiments on 240 models trained on
several datasets show that USB effectively detects the injected
backdoor and provides comparable or better results than state-
of-the-art methods.
Index Terms —Universal Adversarial Perturbation, Back-
door Attack, Detection
1. INTRODUCTION
There have been several proposals to detect backdoor at-
tacks [1, 2, 3] by analyzing the pre-trained model. In particu-
lar, reverse engineering, such as Neural Cleanse (NC) [1] and
TABOR [3], aims to reconstruct the trigger through for every
class. The backdoored class behaves differently compared to
clean classes. However, previous methods may capture the
unique feature of the target class (for example, wings of the
airplane class) instead of the trigger [2]. Both class features
and triggers can lead a backdoored model to the target class.
Reverse engineering decides whether there is a backdoor ac-
cording to the size (for example, L1norm ) of reconstructed
triggers of every class. If the difference between the unique
class feature and the trigger is not particularly large concern-
ing size, reverse engineering may not generate the trigger,
see Fig. 3 as an example. Furthermore, these methods work
well against patch based triggers, such as BadNet [4], but
may fail under non-patch-wised attacks (see Tab. 3), such as
Input-aware dynamic backdoor attack (IAD) [5]. The reason
is that reverse engineering usually starts from a random point.
The content in random points can be very different from
triggers designed by more advanced attacks, and NC-style
Random TriggerUAP (backdoored)UAP (clean)
NC Optimized Pattern
Fig. 1 : The random point is barely updated by NC.
methods only optimize the mask without directly updating
trigger patterns, see Fig. 1. Therefore, it is difficult for NC-
style methods to generate specifically designed triggers. In
addition, NC-style methods also need lots of data to perform
the optimization with a larger number of iterations.
This paper presents a novel detection mechanism that
does not suffer from the aforementioned concerns. More
specifically, we investigate an inference-time defense that
only requires a small amount of clean data. To avoid using
the class unique feature as a trigger, we utilize the similarities
between backdoor attacks and adversarial attacks, especially
universal adversarial perturbations (UAP) [6]. The UAP ef-
fectively fools the victim model on any inputs because it
captures the correlations among different regions of the de-
cision boundary [6]. With similar reasoning, we conjecture
that UAP can also capture the feature of backdoor neurons,
resulting in slighter perturbations, see Fig. 1. USB requires
less iteration and data as we directly use UAP to capture the
potential backdoor. As UAP can be generalized across differ-
ent networks, we only need to generate UAP once for similar
models, greatly reducing the time requirement.
USB is evaluated on 240 models (150 on CIFAR-10 [7] by
ResNet-18 [8], 45 on ImageNet [9] by Efficientnet-B0 [10],
45 on CIFAR-10 [7] by VGG-16 [11] with stronger attacks),
andUSB achieves better backdoor detection performance than
previous methods on these datasets and architectures. Our
main contributions are summarized as follows:
• We propose a novel detection method USB that utilizes
the similarities between backdoor and UAPs.
• Our method USB can detect stronger backdoor trig-
gers for both patch-wised (BadNet and Latent) and
non-patch-wised (Input Aware Dynamic) backdoors.
• We conduct experiments on 240 models to evaluate the
effectiveness and limitations of our method.arXiv:2302.00747v3  [cs.LG]  24 Aug 2023Algorithm 1 Computation of targeted UAP.
Input: Data points X, target class t, victim model f, desired lpnorm of the
perturbation δ, desired error rate θ
Output: Targeted UAP v
1: Initialize v←0
2:while Err(X+v)≤edo
3: forxiinXdo
4: iff(xi+v)̸=tthen
5: Minimal perturbation that send xi+vto class t:
6: △vi←arg min
r∥r∥2s.t.f (xi+v+r) =t
7: v← △vi ▷Update the perturbation under limitation
8: end if
9: end for
10:end while
2. PRELIMINARIES
Inference-time Defenses. Inference-time defenses refer to
defenses with access to the pre-trained model and a certain
amount of clean data, including detection by reverse engineer-
ing of the backdoor trigger [1, 2, 12], pruning [13], and ma-
chine unlearning to remove the backdoor [1]. The pruning and
machine unlearning aim to remove the backdoor by directly
modifying the victim model, while the reverse engineering
conducts detection and reconstructs the backdoor trigger. Re-
verse engineering methods, such as NC [1] and TABOR [3],
use the behavioral characteristics of the backdoor itself. The
backdoor builds a shortcut from within regions of the space
belonging to each label into the region belonging to the tar-
get. For backdoored models, transforming input features of
any class into features of the target class requires less pertur-
bation than transforming into clean classes.
3. PROPOSED METHOD
3.1. Threat Model.
We consider defense against a backdoor attack under the Ma-
chine Learning as a Service (MLaaS) scenario. In MLaaS,
users with insufficient computation and training resources re-
sort to remote computation to train high-performance mod-
els. The adversary (attacker) controls (or has access to) the
MLaaS platform and aims to inject backdoors into models for
malicious purposes. The defender aims to detect these back-
doors. In this paper, we consider all-to-one backdoor attacks.
3.2. Targeted UAP
To work in all-to-one situation, we modify the algorithm
from [6] to generate targeted UAP. We formalize the algo-
rithm, i.e., Alg. 1. Let us assume a pre-trained deep learning
model fandKentries of training data D={(xi, yi)}K−1
0
where xi∈RdXandyi∈ { 0,1}N. N is the number of
classes, and dXis the input dimension. The objective of
the targeted UAP algorithm is to find a perturbation vector
vthat misleads the model fon most of the data points in DAlgorithm 2 Updating of targeted UAP.
Input: Data points X, target class t, victim model f, UAP v, Maximum
iteration number m, learning rate lr
Output: Updated UAP v′=trigger ×mask
1: Initialize trigger and mask by v:trigger ×mask =v
2:fori= 0 tomdo
3: x⊆X ▷ Take a batch of data, x, from Xin order
4: x′=x×(1−mask ) +trigger ×mask ▷ Apply trigger and
mask to get perturbed input
5: output =f(x′)
6:L=L(output, t )−SSIM (x, x′) +norm L1(mask )
7: Backward loss Lto update mask andtrigger
8: mask :mask ←mask−lr× ∇mask
9: trigger :trigger ←trigger −lr× ∇trigger
10: v′=trigger ×mask
11:end for
to a target class t. To work in a more realistic situation, we
use a very small number of data points Xsampled from the
same distribution as Dfor the algorithm. Empirically, a size
smaller than 1% of Dcan be enough for X.
The details are described in Alg. 1. The algorithm itera-
tively goes through every data point in Xto update UAP from
scratch. At each iteration, the algorithm searches for the min-
imal perturbation that sends xi+vto the target class. Then,
the error rate of inputting X+vtofshould be larger than the
desired threshold θ. This is feasible by solving the following
optimization problem:
△vi←arg min
r∥r∥2s.t.f (xi+v+r) =t.
Following the algorithm in [6], this search optimization is im-
plemented by deepfool [14].
3.3. UAP Optimization
Targeted UAP might be enough to contain the feature related
to the backdoor trigger. Then we further analyze the potential
trigger by an optimization phase to update the targeted UAP.
The optimization objective is formalized as a loss function:
L=Lce(output, t )−SSIM (x, x′)+norm L1(mask ),(1)
whereLcerefers to the cross-entropy loss. The structural sim-
ilarity index measure (SSIM) is a measure of the similarity
between images [15].
The details are provided in Alg. 2. The optimization
achieves two goals: (1) make the targeted UAP focus on more
important pixels, and (2) ensure that the UAP can mislead the
victim model. As mentioned before, misleading a backdoored
model to the target class needs smaller perturbation compared
to untarget classes. Therefore, if fis backdoored on class tb,
the size of v′
tbwill be smaller than other UAPs in {v′
i}N−1
0.
For example, for a ResNet-18 model with a BadNet backdoor
on class 0, the L1norm v′
0generated by USB is 4.49, and the
average L1norm of the other classes is 53.76.Model Accuracy ASR MethodReversed Trigger Model Detection Target Class Detection
L1norm Clean Backdoored Correct Correct Set Wrong
Clean 85.38 N/ANC 51.59 50 0 N/A N/A N/A
TABOR 55.09 50 0 N/A N/A N/A
USB 48.99 50 0 N/A N/A N/A
Backdoored
(2×2 trigger)83.43 95.04NC 8.72 5 45 44 1 0
TABOR 9.26 5 45 44 1 0
USB 9.83 1 49 45 4 0
Backdoored
(3×3 trigger)83.59 97.57NC 8.89 2 48 48 0 0
TABOR 10.06 3 47 47 0 0
USB 12.02 1 49 49 0 0
Table 1 : Detection evaluation on CIFAR-10 where each case consists of 50 trained models.CIFAR-10Original
 NC
 TABOR
 USB
ImageNet
 ImageNet
Fig. 2 : Examples of original triggers and reversed triggers by
NC, TABOR, and USB for CIFAR-10 and ImageNet.
4. EVALUATION
In this section, we provide the experimental results for USB as
well as its comparison with NC [1] and TABOR [3], which are
the typical and state-of-the-art methods. Experiments are con-
ducted with TrojanZoo [16], as they provide a widely-used
implementation of various attacks and defense technologies.
We use different random seeds for every trained model.
4.1. Experimental Setup
Model, Datasets, and Backdoor. We use ResNet-18 [8],
VGG-16 [11] on CIFAR-10 [7], and Efficientnet-B0 [10] on
ImageNet [9]. We use BadNet [4], Latent Backdoor [17] and
IAD attack [5] to inject backdoor into victim models. The
poisoning percent is 0.01. The triggers are generated in dif-
ferent positions and random colors.
Hyperparameters. To generate UAP, we use Hyperparame-
ters following experiments in [6], and we set the desired error
rate to θ= 0.6. Alg. 1 only relies on a small number, i.e., 300,
of data points for X, while NC and TABOR use the entire
training set as their input. Then, the maximum iteration num-
ber of Alg. 2 is m= 500 , the learning rate (lr) is lr= 0.1, and
the optimizer is Adam (for detection) with beta = (0.5,0.9).
(a) 2×2
 (b) NC
 (c) TABOR
 (d)USB
Fig. 3 : An example visualization of the original trigger and
reversed triggers by NC, TABOR, and USB for CIFAR-10.
The hyperparameters to train clean and backdoored models
are default ones from TrojanZoo [16]: batch size=96, lr=0.01,
epoch=50 for CIFAR-10 and ImageNet.
Evaluation. Following the previous work in [12], we design
two metrics to evaluate the defense performance: model de-
tection and target class detection. We check whether a model
is correctly identified as a clean or backdoored model. Then,
for backdoored models, we check whether reverse engineer-
ing correctly identifies the target class. In Tab. 1, Tab. 2, and
Tab. 3, Clean andBackdoored under Model Detection refer
to the cases whether the detection identifies a model as clean
or backdoored. For Target Class Detection , we have three
categories: (i) Correct means the detection method identifies
the true target class of a backdoored model, (ii) Correct Set
refers to the case where the detection method identifies mul-
tiple backdoors on different classes, including the true target
class, and (iii) Wrong refers to the case where the detection
method successfully identifies a backdoored model but with
wrong target class(es).
4.2. Results
Tab. 1 shows the detection results for CIFAR-10. In CIFAR-
10, class may contain features from other classes. For exam-
ple, “cat” and “dog” share the feature of four limbs. For the
backdoored models, USB achieves a higher accuracy (98%)
on detecting backdoored models compared to NC (93%) and
TABOR (92%). We believe that the misclassifications in NC
and TABOR are caused by capturing the class’s unique fea-
tures rather than the trigger, illustrated in Fig. 3.Model Accuracy ASR MethodReversed Trigger Model Detection Target Class Detection
L1norm Clean Backdoored Correct Correct Set Wrong
Backdoored
(20×20 trigger)70.94 76.67NC 276.78 0 15 14 1 0
TABOR 271.83 0 15 12 2 1
USB 461.32 0 15 14 1 0
Backdoored
(25×25 trigger)69.7 78.46NC 347.48 0 15 13 2 0
TABOR 341.47 2 13 13 0 0
USB 547.56 0 15 15 0 0
70.91 80.02NC 396.72 1 14 14 0 0
TABOR 406.1 3 12 12 0 0
USB 621.0 1 14 14 0 0
Table 2 : Detection evaluation on ImageNet where each case consists of 15 trained models.
Model Accuracy ASR MethodReversed Trigger Model Detection Target Class Detection
L1norm Clean Backdoored Correct Correct Set Wrong
Clean 91.59 N/ANC 40.78 15 0 N/A N/A N/A
TABOR 48.53 14 1 0 0 1
USB 47.53 15 0 N/A N/A N/A
Latent Backdoor
(4×4 trigger)87.20 99.66NC 19.71 4 11 10 1 0
TABOR 20.68 4 11 11 0 0
USB 12.37 1 14 13 1 0
Input Aware
Dynamic
(32×32 trigger)89.46 90.43NC 0.0 15 0 N/A N/A N/A
TABOR 1.8 15 0 N/A N/A N/A
USB 0.13 0 15 15 0 0
Table 3 : Detection evaluation by stronger backdoor attacks on VGG-16 trained with CIFAR-10.
As ImageNet contains a vast number of images, it is hard
to train a lot of models on it. Thus we use a subset of Ima-
geNet, which contains ten classes. Each class has 1301 im-
ages. Tab. 2 shows the results of detecting backdoors for
Efficientnet-B0 [10] trained with the subset of ImageNet [9].
Due to the larger image size and model architecture, we use
500 images for data points Xin Alg. 1 and Alg. 2.
4.3. Stronger Backdoor Attacks
Tab. 3 shows the detection results on Latent Backdoor [17]
and IAD attack [5]. The trigger size for Latent Backdoor is
4×4×3. Due to IAD attack’s characteristics, we use 32×
32×3trigger size (the size of input images). The motivation
is to show the generalization of USB under stronger attacks
besides the BadNets [4], especially IAD is non-patch-wised.
IAD trigger changes with different inputs.
According to Tab. 3, NC and TABOR show worse perfor-
mance compared to detection results on BadNets, while USB
still precisely detects most of the backdoored models. Espe-
cially, NC and TABOR do not work under the IAD attack, but
USB detects such backdoors with the true target class. The
reason is that NC-style methods do not directly optimize the
Trigger patterns . They mainly optimize the mask that will
be applied to the pattern. Moreover, IAD attacks designs sub-
tle triggers with specific features related to inputs, which is
more difficult for optimization from random points.4.4. Time Consumption
As mentioned before, NC and TABOR require a large amount
iterations to conduct detection. Therefore, we evaluate the
time consumption of NC, TABOR and USB when conduct-
ing detection with Efficientnet-B0 on ImageNet. When de-
tecting backdoored models with 20×20trigger, the average
time consumption (in second) for NC, TABOR and USB are:
1154.02, 2.129.40 and 267.12, respectively. It is clear that
USB costs much less time when optimizing reverse engineer-
ing the potential triggers. Although USB needs to generate
targeted UAP, the UAP can be used for different models with
similar architecture [6]. We only need to generate it once.
5. CONCLUSIONS AND FUTURE WORK
This paper proposes USB to detect potential backdoors. USB
uses targeted UAP to capture sensitive features created by
backdoors. Then the UAP is optimized to focus on pixels
sensitive to backdoors. The aim is to avoid using the class fea-
tures as the backdoor triggers. We run extensive experiments
on several datasets to evaluate the performance of our method.
Among the 160 backdoored models, we successfully identi-
fied 157 backdoored ones and outperform baselines. Finally,
further investigation is needed regarding optimizing UAP ac-
cording to backdoored neurons in the backddoored model,
which can greatly reduce the optimization time.6. REFERENCES
[1] Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li,
Bimal Viswanath, Haitao Zheng, and Ben Y . Zhao,
“Neural cleanse: Identifying and mitigating backdoor
attacks in neural networks,” in 2019 IEEE Symposium
on Security and Privacy (SP) , 2019, pp. 707–723.
[2] Yingqi Liu, Wen-Chuan Lee, Guanhong Tao, Shiqing
Ma, Yousra Aafer, and Xiangyu Zhang, “Abs: Scanning
neural networks for back-doors by artificial brain stimu-
lation,” in Proceedings of the 2019 ACM SIGSAC Con-
ference on Computer and Communications Security ,
New York, NY , USA, 2019, CCS ’19, p. 1265–1282,
Association for Computing Machinery.
[3] Wenbo Guo, Lun Wang, Yan Xu, Xinyu Xing, Min
Du, and Dawn Song, “Towards inspecting and elimi-
nating trojan backdoors in deep neural networks,” in
2020 IEEE International Conference on Data Mining
(ICDM) , 2020, pp. 162–171.
[4] Tianyu Gu, Kang Liu, Brendan Dolan-Gavitt, and Sid-
dharth Garg, “Badnets: Evaluating backdooring attacks
on deep neural networks,” IEEE Access , vol. 7, pp.
47230–47244, 2019.
[5] Tuan Anh Nguyen and Anh Tran, “Input-aware dynamic
backdoor attack,” in Advances in Neural Information
Processing Systems , H. Larochelle, M. Ranzato, R. Had-
sell, M.F. Balcan, and H. Lin, Eds. 2020, vol. 33, pp.
3454–3464, Curran Associates, Inc.
[6] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi,
Omar Fawzi, and Pascal Frossard, “Universal adver-
sarial perturbations,” in Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition
(CVPR) , July 2017.
[7] Alex Krizhevsky, Geoffrey Hinton, et al., “Learning
multiple layers of features from tiny images,” 2009.
[8] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun, “Deep residual learning for image recognition,” in
Proceedings of the IEEE Conference on Computer Vi-
sion and Pattern Recognition (CVPR) , June 2016.
[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei, “Imagenet: A large-scale hierarchical
image database,” in 2009 IEEE Conference on Com-
puter Vision and Pattern Recognition , 2009, pp. 248–
255.
[10] Mingxing Tan and Quoc Le, “EfficientNet: Rethink-
ing model scaling for convolutional neural networks,”
inProceedings of the 36th International Conference
on Machine Learning , Kamalika Chaudhuri and Rus-
lan Salakhutdinov, Eds. 09–15 Jun 2019, vol. 97 ofProceedings of Machine Learning Research , pp. 6105–
6114, PMLR.
[11] Karen Simonyan and Andrew Zisserman, “Very deep
convolutional networks for large-scale image recogni-
tion,” in 3rd International Conference on Learning Rep-
resentations, ICLR 2015, San Diego, CA, USA, May 7-
9, 2015, Conference Track Proceedings , Yoshua Bengio
and Yann LeCun, Eds., 2015.
[12] Yinpeng Dong, Xiao Yang, Zhijie Deng, Tianyu Pang,
Zihao Xiao, Hang Su, and Jun Zhu, “Black-box detec-
tion of backdoor attacks with limited information and
data,” in Proceedings of the IEEE/CVF International
Conference on Computer Vision (ICCV) , October 2021,
pp. 16482–16491.
[13] Dongxian Wu and Yisen Wang, “Adversarial neuron
pruning purifies backdoored deep models,” in Advances
in Neural Information Processing Systems , M. Ranzato,
A. Beygelzimer, Y . Dauphin, P.S. Liang, and J. Wortman
Vaughan, Eds. 2021, vol. 34, pp. 16913–16925, Curran
Associates, Inc.
[14] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi,
and Pascal Frossard, “Deepfool: A simple and accurate
method to fool deep neural networks,” in Proceedings
of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR) , June 2016.
[15] Zhou Wang, A.C. Bovik, H.R. Sheikh, and E.P. Simon-
celli, “Image quality assessment: from error visibility
to structural similarity,” IEEE Transactions on Image
Processing , vol. 13, no. 4, pp. 600–612, 2004.
[16] Ren Pang, Zheng Zhang, Xiangshan Gao, Zhaohan Xi,
Shouling Ji, Peng Cheng, and Ting Wang, “Trojan-
zoo: Towards unified, holistic, and practical evaluation
of neural backdoors,” in Proceedings of IEEE European
Symposium on Security and Privacy (Euro S&P) , 2022.
[17] Yuanshun Yao, Huiying Li, Haitao Zheng, and Ben Y .
Zhao, “Latent backdoor attacks on deep neural net-
works,” in Proceedings of the 2019 ACM SIGSAC Con-
ference on Computer and Communications Security ,
New York, NY , USA, 2019, CCS ’19, p. 2041–2055,
Association for Computing Machinery.
[18] J. Stallkamp, M. Schlipsing, J. Salmen, and C. Igel,
“Man vs. computer: Benchmarking machine learning
algorithms for traffic sign recognition,” Neural Net-
works , , no. 0, pp. –, 2012.
[19] Yann LeCun, “The mnist database of handwritten dig-
its,” http://yann. lecun. com/exdb/mnist/ , 1998.(a)2×2
 (b) NC
 (c) TABOR
 (d)USB
(e)3×3
 (f) NC
 (g) TABOR
 (h)USB
Fig. 4 : An example visualization of the original triggers and
reversed triggers. Fig. 4(a) and Fig. 4(e) are original triggers
and followed by three reversed triggers. More examples in
Appendix.
A. APPENDIX
A.1. Experimental Results on More Settings
We demonstrate the generalization of USB by considering
more settings in Appendix:
• Stronger backdoor attacks. We study the Latent Back-
door [17] and Input-Aware Dynamic Backdoor At-
tack [5] besides BadNets [4].
• Different model architectures. We study the VGG-
16 [11] besides ResNet-18 [8] and Efficientnet-B0 [10].
• More datasets. We also include detection results with
GTSRB [18] besides MNIST [19], CIFAR-10 [7], and
ImageNet [9].
A.2. MNIST
We compare USB with NC and TABOR while they use the
whole clean training set to conduct optimization. Tab. 5
shows detection results, including average accuracy on clean
data, attack success rates (ASRs), average L1norm of the
reversed trigger, and detection evaluation. According to
detection results, neither our method nor NC and TABOR
mistake clean models for backdoor models. The L1norm
values of triggers generated from clean models for every
class are close to each other. There are no outliers among
these norm values. On backdoored models, all three methods
can identify the majority of backdoors. In the wrong cases,
USB can still detect backdoors, although the target class is
not the true target. NC and TABOR tend to classify wrong
cases as clean models.A.3. Detection Results on VGG-16
In Tab. 4, we show the results of detecting backdoors for
VGG-16 models trained with CIFAR-10. We use the same
experimental settings as that in the experiment section. We
also study Latent Backdoor [17] beside BadNet attack.
A.4. Stronger Backdoor Attacks
The basic idea of the NC-style method is optimizing the Mask
so that the final trigger ( Trigger pattern ×Mask ) can mis-
lead the classifier to the target class. The optimization starts
from a random point. However, NC-style methods do not di-
rectly optimize the Trigger patterns . In Fig. 1, it is clear
that the optimized trigger pattern is almost the same as the
random start point. Moreover, the optimization process can
be more difficult to generate correct triggers when the ran-
dom start point is far away from optimum. USB uses simi-
lar optimization to find potential triggers, but we separate the
process into two different parts. In the first part (Section 3),
we search for a targeted UAP as the starting point for reverse
engineering of potential triggers. The UAP already includes
latent features of the model to be detected, while a random
start point includes lots of noise, see Fig. 1 as an example.
In other words, if the original trigger is not included in the
random start, it will be difficult for NC-style methods to be
successful. Stronger attacks, such as Input-Aware attacks, de-
sign subtle triggers with specific features related to inputs.
Random starting points cannot include these specific features.
Therefore, NC and TABOR cannot detect the backdoor under
an Input-Aware attack.
A.5. GTSRB
The results for GTSRB are shown in Tab. 6. On clean models,
USB, NC, and TABOR all have incorrect results, as the num-
ber of classes in GTSRB is significantly larger than that of
MNIST and CIFAR-10. Compared to the L1norm of NC and
TABOR, USB provides a much smaller norm value because
the reversed trigger is optimized from targeted UAP. Since
GTSRB has more classes than MNIST and CIFAR-10, ini-
tialization with targeted UAP helps the trigger optimization
(Alg. 2) to avoid local optima and find as small as possible
triggers. However, USB generates more wrong cases than NC
and TABOR. The reason is that USB uses only 300 entries
of clean data to conduct reverse engineering, which is to be
consistent with experiments on MNIST and CIFAR-10. On
MNIST and CIFAR-10, there are around 30 entries for each
class, but less than 10 entries for each class of GTSRB. Such
a small amount of data cannot provide enough features and
information. This gives a simple option to improve the per-
formance of our method by adding more data.Model Accuracy ASR MethodReversed Trigger Model Detection Target Class Detection
L1norm Clean Backdoored Correct Correct Set Wrong
Clean 91.59 N/ANC 40.78 15 0 N/A N/A N/A
TABOR 48.53 14 1 0 0 1
USB 47.53 15 0 N/A N/A N/A
Backdoored
(2×2 trigger)88.28 99.39NC 5.43 0 15 14 1 0
TABOR 5.32 0 15 15 0 0
USB 3.5 0 15 15 0 0
Backdoored
(3×3 trigger)88.30 99.77NC 6.60 1 14 13 1 0
TABOR 6.98 0 15 14 1 0
USB 7.0 0 15 14 1 0
Table 4 : Detection evaluation on VGG-16 trained with CIFAR-10 where each case consists of 15 trained models.
Model Accuracy ASR MethodReversed Trigger Model Detection Target Class Detection
L1norm Clean Backdoored Correct Correct Set Wrong
Clean 98.93 N/ANC 91.50 50 0 N/A N/A N/A
TABOR 95.31 50 0 N/A N/A N/A
USB 44.23 50 0 N/A N/A N/A
Backdoored
(2×2 trigger)98.82 94.43NC 6.37 1 49 49 0 0
TABOR 6.57 1 49 49 0 0
USB 9.22 0 50 49 0 1
Backdoored
(3×3 trigger)99.00 99.53NC 7.89 1 49 49 0 0
TABOR 6.04 0 50 50 0 0
USB 8.11 0 50 49 1 0
Table 5 : Detection evaluation on MNIST where each case consists of 50 trained models.
Fig. 5 :USB reverse engineering for 10 classes on MNIST.
The true backdoor target is class 1. The first one is the clean
image carrying the trigger. The rest are reversed triggers from
class 0 to 9.
A.6. Discussion
To explain our method, we analyze triggers reversed from ev-
ery class under a simple case. We use MNIST and a sim-
ple CNN architecture (see Appendix A.7 for details) with two
convolutional layers and two fully connected layers. We re-
move the constraint on the mask size to search for as pow-
erful features as possible. We replace Lin Alg. 2 by: L=
Lce(output, t )−SSIM (x, x′).Under this setting, we train
a backdoored model with BadNet. Then, we conduct reverse
engineering for all classes. According to results in Fig. 5, the
optimization with the loss Ltends to learn unique class fea-
tures for the clean class and the trigger features for the back-door class. This is expected as we only have a backdoor on
the target class, i.e., class 1. In this simplified situation, for
clean classes without backdoors, only the unique class fea-
tures allow the model to recognize that an input belongs to
the class. For the class injected with the backdoor, the model
will recognize the input as the backdoor target based on the
unique feature of the target and the feature of the backdoor
trigger.
Reverse engineering requires a choice between the unique
class feature and the feature of the backdoor trigger. Regard-
ing relatively simple features, the trigger feature is stronger
than class features when training with poisoned data. Reverse
engineering can find a small perturbation with strong features
enough to mislead the model according to learning objectives
in the loss function. However, in scenarios such as training
with GTSRB or larger datasets, there might be strong features
that can generate perturbations with a similar size to backdoor
triggers. This is why NC, Tabor, and USB provide more in-
correct results in Tab. 6, compared to results for MNIST and
CIFAR-10.
A.7. Details of the Basic Model
To reduce the impact of complex features and many model pa-
rameters, we use MNIST and a basic CNN architecture with
two convolutional layers (followed by the ReLU activation
function and a 2D average pooling layer) and two fully con-Model Accuracy ASR MethodReversed Trigger Model Detection Target Class Detection
L1norm Clean Backdoored Correct Correct Set Wrong
Clean 83.96 N/ANC 181.17 12 3 N/A N/A N/A
TABOR 185.21 13 2 N/A N/A N/A
USB 39.8 12 3 N/A N/A N/A
Backdoored
(2×2 trigger)80.85 85.06NC 13.36 0 15 13 2 0
TABOR 37.02 0 15 13 2 0
USB 10.86 3 12 12 0 0
Backdoored
(3×3 trigger)80.24 93.52NC 14.78 0 15 13 2 0
TABOR 15.11 0 15 13 2 0
USB 12.02 2 13 13 0 0
Table 6 : Detection evaluation on GTSRB where each case consists of 15 trained models.
nected layers (denoted as the Basic model). The input chan-
nel, output channel, and kernel size for the two convolutional
layers are (1, 16, 5) and (16, 32, 5). The input and output
channels for the two fully connected layers are (512, 512)
and (512, 10). The model is trained using the same hyperpa-
rameters as Resnet-18 on MNIST in Section A.2 with batch
size=128, epochs=40, and poisoned rate=0.05.
A.8. Datasets
We use four popular datasets: MNIST [19], CIFAR-10 [7],
GTSRB [18] and ImageNet [9].
• The MNIST contains 60,000 28 ×28×1 training images
and 10,000 28 ×28×1 testing images in 10 classes.
• The CIFAR-10 contains 60,000 32 ×32×3 images in 10
classes, with 6,000 images per class, 50,000 for train-
ing, and 10,000 for testing.
• The GTSRB contains 51,840 traffic sign images in 43
categories. 39,210 for training and validation (80:20
split), 12,630 for testing (without labels). In experi-
ments, the image size is fixed to 32 ×32×3.
• The ImageNet spans 1,000 classes and contains 1,281,167
training images, 50,000 validation images, and 100,000
test images. In experiments, the image size is fixed to
224×224×3.NC
 TABOR
 USB
Fig. 6 : Reversed triggers from class 0 to 9.
Model MethodGPU Time [m:s] in every class
0 1 2 3 4 5 6 7 8 9
Backdoored
(20×20 trigger)NC 23:16 24:18 24:32 23:39 24:48 23:35 23:15 23:34 23:41 24:10
TABOR 33:54 37:24 34:19 35:51 33:59 36:45 34:23 36:47 35:4 36:23
USB 4:26 4:26 4:27 4:30 4:26 4:26 4:26 4:26 4:26 4:26
Backdoored
(25×25 trigger)NC 23:35 24:38 25:11 24:3 24:58 24:19 24:29 23:54 24:29 23:6
TABOR 48:23 47:24 48:48 48:41 48:38 47:41 48:27 49:10 48:48 47:45
USB 4:44 4:42 4:44 4:44 4:49 4:48 4:48 4:54 4:50 4:45
NC 19:1 18:22 20:47 18:5 18:48 21:27 18:54 18:30 20:26 17:57
TABOR 48:52 47:16 49:0 48:39 48:54 47:40 48:34 48:55 48:55 47:50
USB 4:27 4:26 4:27 4:30 4:26 4:26 4:26 4:26 4:26 4:25
Table 7 : Running time results of backdoor detection for Efficientnet-B0 [10]. Each result is the average of detection on 15
models.