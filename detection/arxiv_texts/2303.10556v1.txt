arXiv:2303.10556v1  [eess.AS]  19 Mar 2023The Graph feature fusion technique for speaker
recognition based on wav2vec2.0 framework⋆
Zirui Ge, Haiyan Guo, Tingting Wang, Zhen Yang∗
School of Communication and Information Engineering, Nanj ing University of Posts and
Telecommunications, Nanjing 2100023, China
Abstract
Pre-trained wav2vec2.0 model has been proved its eﬀectiveness f or speaker
recognition. However, currentfeature processingmethods are focusingon classi-
cal pooling on the output features of the pre-trained wav2vec2.0 model, such as
mean pooling, max pooling etc. That methods take the features as t he indepen-
dent and irrelevant units, ignoring the inter-relationship among all t he features,
and do not take the features as an overall representation of a sp eaker. Gated
Recurrent Unit (GRU), as a feature fusion method, can also be con sidered as
a complicated pooling technique, mainly focuses on the temporal info rmation,
which may show poor performance in some situations that the main inf orma-
tion is not on the temporal dimension. In this paper, we investigate t he graph
neural network (GNN) as a backend processing module based on wa v2vec2.0
framework to provide a solution for the mentioned matters. The GN N takes
all the output features as the graph signal data and extracts th e related graph
structure information of features for speaker recognition. Spe ciﬁcally, we ﬁrst
give a simple proof that the GNN feature fusion method can outperf orm than
the mean, max, random pooling methods and so on theoretically. The n, we
model the output features of wav2vec2.0 as the vertices of a gra ph, and con-
struct the graph adjacency matrix by graph attention network ( GAT). Finally,
⋆This work has been supported by National Natural Science Fou ndations of China
(No.62071242)
∗Corresponding author
1E-mail address: yangz@njupt.edu.cn (Z. Yang)
Preprint submitted to Journal of L ATEX Templates March 21, 2023we follow the message passing neural network (MPNN) to design our message
function, vertex update function and readout function to trans form the speaker
features into the graph features. The experiments show our per formance can
provide a relative improvement compared to the baseline methods. C ode is
available at xxx.
Keywords: speaker recognition, wav2vec2.0, graph neural network, pooling ,
feature fusion
1. Introduction
Automaticspeakerrecognition(ASR)isthetaskofauthenticating theclaimed
identity using the speaker’s voiceprint. As a means of using bio-metr ics, ASR
has attracted considerable attention from many researchers du e to its accessi-
bility and uniqueness. With the development of deep neural network , speaker
recognition models based on deep neural networks are being more c omplicated,
and need more larger quantities of labeled training data.
However, producing large and high quality labeled data is hard and exp en-
sive, and only learning from the labeled samples also seems to be incons istent
with the process of language acquisition of the infants, i.e., self-lear ning from
listening and watching, supervise learning from training and testing w ith in-
structors. Self-supervision learning on unlabeled data and ﬁne-tu ning on the
pre-trained models is similar to the mentioned two stages and have be en proved
successful for natural language processing such as BERT [1], GP T-3 [2]etc.
In the ﬁeld of speech signal processing, wav2vec2.0[3] also applies to the two
stages learning process. Wav2vec2.0 shows an excellent performa nce on speech
recognition, and it ﬁrst learns the speech representation from th e unlabeled
speech audio dataset and ﬁne-tune the pre-trained weights on th e labeled data.
There are mainly four modules in wav2vec2.0framework, i.e., a multi-lay er con-
volution feature encoder, a Transformergroup module, the quan tization module
and contrastive loss. More speciﬁcally, wav2vec2.0 ﬁrst encodes t he raw audio
signal into latent speech representations via the multi-layer convo lution feature
2encoder. Then, the masked latent speech representations are f ed into the Trans-
former module group to capture the contextualized representat ions from the
entire sequence. Meanwhile, the quantization module converts the unmasked
latent speech representation into its discrete version via product quantization.
Finally, thediscreterepresentationofquantizationmodule andthe outputofthe
Transformer group are put into the contrastive loss to identify th e true quan-
tized latent speech representation. Wav2vec2.0 framework has a chieved 1.8/3.3
word error rate (WER) on the clean/other test sets using all labele d data of
Librispeech dataset. When using more less labeled data, wav2vec2.0 still out-
performs the state of the art at that time. The excellent perform ance shows
that the phonemic constructions are well learned during the pre-t raining and
the downstream modules can ﬁnish their tasks via ﬁne-tuning the pr e-training
weights.
Pre-trained wav2vec2.0 as the upstream model also works well in sp eaker
recognition. The authors in [4] ﬁrst applied the wav2vec2.0 to multi-t ask learn-
ing, i.e., speaker recognition and language identiﬁcation task, and inv estigated
wav2vec2.0 as the audio encoder to extract the speaker and langu age features
[4]. The work of multi-task learning in [4] ﬁrst demonstrated the eﬀec tiveness of
wav2vec2.0 on the speaker recognition and language identiﬁcation t ask. At the
same time, the authors in [5] took pre-trained wav2vec2.0 models to implement
the speech emotion recognition task, and propose to weight the ou tput of sev-
eral layers from the pre-trained model using trainable weights whic h are learned
jointly with the downstream model. The authors in [6] applied wav2ve c2.0
framework to speaker recognition task, and investigated eﬀectiv eness of diﬀer-
ent pooling methods. [6] further proposed the ﬁrst&cls pooling met hod that
inserts a ”start token” (all values are +1) in the input sequence of encoder,
and selected the ﬁrst output token as the speaker embedding. Th ese litera-
tures completed diﬀerent speech related tasks based on wav2vec 2.0 framework
via using mean, max, mean&std pooling methods etc. Though these c lassical
methods have been proven their eﬀectiveness, they only focus on some simple
information of features, for example, the mean and mean&max poo ling mainly
3consider the distribution of features, and max pooling mainly focuse s on the
”texture information”. Besides, these methods consider the out put features as
many independent elements in the regular (Euclidean) space. They d o not view
these output features as an entirety feature of a speaker ident ity and do not
consider more complicate relationship among these features. Thou gh GRU as
a complicate feature fusion method can focus the temporal inform ation, it may
be uncompetitive when meet the features lacking the temporal info rmation. For
example, these features have been processed by some other mod els that can also
extract the temporal information, such as the Transformer mod ule [7]. There-
fore, when features require to be viewed as an entirety or not mer ely extracted
the temporal information, a new data structure or signal proces sing technique
require to be introduced.
Some literatures [8, 9, 10] have shown that the speech signals can b e refor-
mulated as a graph signal and processed using graph signal proces sing theory
(GSP) [11, 12] in the irregular space, i.e., graph domain, and a better perfor-
mance can be obtained compared to regular space. In above literat ures, speech
features are considered as an entirety, i.e., a graph signal, not mer ely a set of
diﬀerent independent features. Graph neural networks (GNN) a s a nonlinear
form of GSP that corporate the advantages of the graph struct ure and deep
neural networks [13, 14] have shown the excellent performance on image classi-
ﬁcation [15, 16, 17] which is mainly processed in the regular (Euclidean ) space
previously. Thus, exploring the combination of GNN and speaker rec ognition is
well-founded.
The authors in [18] ﬁrst proposed the concept of GNN and extende d the
neural networks for processing the data resided in graph domains . With the
development of GNN, there are some major variants of GNN in the wo rld in-
cluding the message passing neural network (MPNN) [19], graph at tention net-
work (GAT) etc. MPNN contains two phases, a message passing pha se and a
readout phase. The message passing phase mainly include hidden sta tes updat-
ing and graph vertex aggregating, and the readout phase is aim to o btain the
whole graph representation using the readout function. GAT incor porates the
4self-attention mechanism [7] into the propagation step and obtains new node
features via weighting neighborhood node features using attentio n coeﬃcients.
[20, 21] ﬁrst applied GNN as the backend feature fusion method of R esNet
and RawNet2 model to speaker recognition task. [21] uses the GAT mechanism
to design an undirected graph with asymmetric weight matrix to depic t the
relationship between diﬀerent graph vertex pairs, and take the gr aph U-Net ar-
chitectureto obtainthe ﬁnal graphrepresentation. The GNNrela tedframework
in[19]takesapairofutterancesasthe enrollmentandthe testutt erancetotrain
their similarity score. [22] also took the same GNN architecture in [20] to im-
plement speaker anti-spooﬁng task. The both GNN backend models obtain the
improved performance. However, that mentioned utterance-pa ir classiﬁcation
format is inferior to the single-utterance classiﬁcation for speake r recognition
based on wav2vec2.0 framework [6]. Besides, both works do not talk about the
relationship between the GNN fusion method and classical fusion met hod such
as mean, max, random, etc., and why the GNN based feature fusion models can
outperform the compared GRU model.
Under this background, we ﬁrst discuss the relationship between G NN or
GSP and classical pooling methods, such as mean, max, random, etc ., and show
that why GNN based feature fusion models can outperform the mea n, max, ran-
dom pooling methods theoretically. Then we propose GNN as the down stream
processing framework to explore the output features of wav2ve c2.0 framework
in non-Euclidean space. Speciﬁcally, we model the set of output fea tures as a
graph signal, i.e., each feature is considered as a graph vertex’s fea ture, and the
number of graph vertices is equal to the number of output featur es. Then we
use the graph attention mechanism to construct the symmetric we ight matrix
to capture the similarity between diﬀerent vertex pairs. The atten tion weight
matrix is applied to our designed message passing graph neural netw ork to ob-
tain the graph embedding which is also the ﬁnal speaker embedding. W hen the
downstreamprocessingframeworkis constructed, we take the w av2vec2.0as the
upstream audio feature extractor and ﬁne-tune its pre-training weights. We im-
plement our experimentson the VoxCelebdatasets [21, 22], and the experiments
5show that proposed graph pooling method can obtain better perfo rmance than
the classical feature pooling methods. We also explain why GNN can ob tain
the top performance compared to other feature fusion methods . To the best of
our knowledge, this study is the ﬁrst to apply a GNN to wav2vec2.0 fr amework
for the speaker recognition task.
2. Related Work
In this section, we ﬁrst review the pre-training of the wav2vec 2.0 a nd how
to apply the pre-training to downstream. Then, we introduce grap h message
passing neural network.
2.1. Wav2vec 2.0 pretraining and ﬁne-tuning
The main body of the model consists a CNN based feature encoder, a
Transformer-based context network, a quantization module and a contrastive
loss. The feature encoder consists of 7 blocks and each block cont ains a tem-
poral convolution with 512 channels with respective kernel sizes of (10, 3, 3, 3,
3, 2, 2) and stride (5, 2, 2, 2, 2, 2, 2) followed by a layer normalizatio n and a
GELU activation function [22]. As the Figure.1 left depicted, the CNN feature
extractor takes as input raw audio Xand and outputs latent speech represen-
tationsZ. Then, the latent speech representations are projected into a n ew
dimension, before fed to the following modules.
The context network contains 12 Transformer blocks and a residu al 2-layer
feed forward network with 3072 and 768 units. The relative position al embed-
dingsinsteadofﬁxedpositionalembeddingsareﬁrstaddedtothem askedspeech
representations, before the masked representations are input to the context net-
work. Transformer then contextualizes the masked representa tions and ﬁnally
generates context representations C. The outputs of 12 Transformer blocks and
projected speech representation are considered as hidden feat ures.
The quantization module is to discretize the output of the feature e ncoder
to a ﬁnite set of speech representations via product quantization , where the
6Fig. 1. An overview of the pre-training and ﬁne-tuning. The right su bﬁgure shows the output
shape of diﬀerent model for a given input.
product quantizationis aim to choosingquantized representations frommultiple
codebooks and concatenating them. The number of codebooks is e qual to 2 and
there are 320 elements and respective size is 128 in each codebook. The gumbel
softmax function [23] is also used to enable choosing discrete codeb ook entries
in a fully diﬀerentiable way.
The objective function is the weighted sum of the contrastive loss a nd di-
versity loss. The contrastive loss requires to identify the true qua ntized latent
speech representation for a masked time step within a set of distra ctors. The
diversity loss is designed to encourage using the codebook entries e qually.
Fig.1 right shows the ﬁne-tuning stage, the models are identical to t he train-
ing stage, except the quantization modules and extra output layer s. We take a
speech audio with 48000samples as the input, and show the output s hape of dif-
ferent modules. In the ﬁne-tuning stage, how to select the hidden feature has an
important eﬀect on downstream tasks [5], therefore we consider t wo approaches
to select hidden feature, one way is taking the output of last Trans former block,
the other is weighting all the hidden features.
72.2. Message Passing Neural Networks
LetG= (V,E,A) be agraphand X∈RF×Nbe input vertexfeatures, where
V={v1,...,vN}is the set of N=|V|vertices, Fis the dimension of one input
feature.E={ei,j}i,j∈Vis the set of edges between vertices, such that ei,j= 1 if
there is a link from node jto nodei, otherwise ei,j= 0.Ais theN×Nweight
adjacency matrix and its entries are the edge weights ai,j, fori,j= 1,...,N.
The MPNN contains two phases, a message passing phase and a read out
phase. The message phase runs for Ttime steps and contains two functions
that are message function Mtand vertex update function Ut. The hidden state
htof each vertex at the ttime step can be written as:
mt+1
v=/summationdisplay
w∈N(v)Mt/parenleftbig
ht
v,ht
w,evw/parenrightbig
, (1)
ht+1
v=Ut/parenleftbig
ht
v,mt+1
v/parenrightbig
, (2)
whereN(v) denotes the neighbors of vertex vin graph G. The readout phase
computes the ﬁnal graph feature vector via the following readout function R
ˆy=R/parenleftbig/braceleftbig
hT
v|v∈G/bracerightbig/parenrightbig
, (3)
The message function Mt, vertex update function Utand readout function R
are all learned diﬀerential functions.
3. METHODOLOGY
In this section, we ﬁrst show that some classical pooling methods ca n be
represented by GNN or GSP, and introduce the graph neural netw ork as the
feature pooling method. The proposed module is located after the c ontext
network and takes as input the hidden states of wav2vec2.0. Ther e are three
components in ourproposed model: 1) the graph attention layer; 2 ) the message
and vertex update function; and 3) the readout function. Speciﬁ cally, we ﬁrst
showhow to obtain the graphweight adjacency matrix from the gra phattention
layer. Then we show how to aggregate neighbor vertices informatio n in term of
8one vertex, and update the status of every vertex. Finally, we pr opose a readout
function to obtain the whole graph feature embedding. The overall scheme is
illustrated in Figure 2.
3.1. Reformulating some classical pooling methods with GNN
We reformulate some pooling methods using GSP and GNN. Speciﬁcally, we
use the GSP to reformulate the linear pooling, including mean, random , ﬁrst,
middle, last pooling method, and use the GNN to reformulate the nonlin ear
pooling, i.e., the max pooling method.
In the graph signal processing theory, the weight adjacency mat rixAis also
is also named as the shift operator, and the notion of graph shift op erator is
deﬁned as a local operation that replaces a graph signal feature w ith the linear
combination of features at the neighbors of that vertex [24]. The g raph shift
operation can be expressed as
Y=AXT, (4)
whereX∈RD×N.
We take the abovegraph shift operatorto reformulate the mean a nd random
pooling methods. When
A=
1/N···1/N
.........
1/N···1/N
, (5)
the expression ATcan be considered as the mean pooling method, and for
A=
0 1 0
.........
0 1 0
, (6)
the expression ATcan be considered as the random pooling method. When the
random pooling method selects the ith feature, the entries of the ith column in
Ain are all 1. When the iin random pooling is always 1, N, or⌊N/2⌋, the
pooling method can be considered as the ﬁrst, last, and middle pooling .
9The max pooling method as a nonlinear operation is impossible to be refo r-
mulated by the linear operation. We take the MLP as an approximation for the
max pooling method, thanks to the universal approximation theore m [25], and
it can be expressed as
Y=MLP/parenleftbig
AXT/parenrightbig
, (7)
Therefore, some classical pooling methods can be reformulated wit h GSP or
GNN, and we may obtain a better performance theoretically, if we dir ectly use
GSP or GNN as our pooling methods.
3.2. Graph attention layer
We ﬁrst formulate a graph using the output features of wav2vec2 .0 frame-
work. Specially, each output feature is considered as a vertex of a graph. Due
to these feature does not have an obvious graph structure, we n eed to specify
the edge set and weight adjacency matrix. For this model, we argue that every
vertex pair has an edge to link each other, such that the entries ei,jin edge set
Eare all 1. Hence set Emeans that the entire output features are interacted
each other. Next, we show how to obtain the weight adjacency mat rix from the
graph attention mechanism. Specially, we consider the weights in the adjacency
matrix as the degree of similarity between pairs of vertices. There a re two main
approaches to graph attention mechanism. One approach leverag es the explicit
attention mechanism to obtain the attention weights such as the co sine similar-
ity between diﬀerent vertex pairs [26]. The other approach does no t rely on any
prior information and leverages complete parameter learning to gain attention
weight [27]. In this study, we take the ﬁrst approach as our graph a ttention
layer. Compared to [27] used in [21], the symmetric weight adjacency matrix
learned in [26] can save half the computation in the graph attention la yer. The
GAT process can be described as:
bi=Wxi, (8)
a(i,j) =exp(βcos(bi,bj))/summationtextN
k=1exp(βcos(bi,bj)), (9)
10wherexi∈RFis the vertex feature, and W∈RF′×Fis the projection matrix,
βis the learnable parameter. In the GAT layer, the vertex feature is ﬁrst
projected into F′dimensional space via multiplying W. Then the attention
score is obtained by equation (9).
3.3. Message and vertex update function
In this study, we formulate the message function as
Mt=AH⊤
t−1, (10)
whereHt∈RF′×Nis the set of the hidden states h,Mtis the aggregated states
set, and the subscript tis thetth massage aggregation, ⊤is the transpose
symbol. (10) can also be considered as the one order graph shift. W hent= 1,
Ht−1is the projected graph vertex features set B={bi},i= 1,...,N.
The vertex update function is deﬁned as the nonlinear transforma tion of
aggregated representation:
Ht=σ(LN(MLPt(Mt))), (11)
whereσ(·) is the non-linear activation, LN(·) is the layer normalization func-
tion,MLP(·) is the multi-layer perceptron (MLP) with a set of learnable pa-
rameters.
3.4. Readout function
When all the hidden states are suﬃciently updated, they are aggre gated to
a graph-level representation for the speaker voice print featur e, based on which
the ﬁnal prediction is produced. We deﬁne the readout function as :
HT=MLPθ/parenleftbig
HT−1/parenrightbig
⊙sigmoid/parenleftbig
MLPϕ/parenleftbig
HT−1/parenrightbig/parenrightbig
, (12)
hG=1
|V|T/summationdisplay
i=0/summationdisplay
v∈VhT
v+ Maxpooling/parenleftbig
HT/parenrightbig
, (13)
where⊙istheelement-wisemultiplication. Intheequation(13),1
|V|/summationtextT−1
i=0/summationtext
v∈VhT
v
denotes the residual connection in GNNs. In the residual connect ion, we extract
the distribution information of early representation as the supplem ent for the
ﬁnal representation. The MPNN framework is shown in Figure 2.
11Fig. 2. The MPNN framework.
4. EXPERIMENTS
4.1. Dataset
All experiments are conducted on the VoxCeleb1&2 datasets [28, 29]. Vox-
Celeb2 development set contains over a million utterances from 5994 celebrates
from the YouTobe, and the average duration of a signal speaker is about 7.2
seconds. The reported performance in terms of equal error rat es (EERs) is eval-
uated on extended (vox1-o, vox1-e, vox1-h) test sets from th e VoxCeleb12. The
pretrained weights1 used in the experiments with the wav2vec2.0fr ameworkare
released on Hugging-Face [30].
4.2. Model description and implementation details
We conduct all experiments using the PyTorch framework, on a 309 0 GPU,
and take the cosine similarity as the back-end performance evaluat ion tool.
Baselines. In the experiments, we take some released works [4, 6, 31] for
speaker recognition based on wav2vec2.0framework and feature fusion methods
as our baselines. The authors in [6] considered diﬀerent pooling meth ods, and
they are mean, max, mean&std, quantile, ﬁrst&cls, middle, last, ﬁr st, random.
In this study, we use new symbols to represent them, i.e., w2v2-mea n, w2v2-
max, w2v2-mean&std, w2v2-quantile, w2v2-ﬁrst&cls, w2v2-midd le, w2v2-last,
w2v2-ﬁrst, w2v2-random. The mean pooling is also used in [4] for spe aker
12Fig. 3. The weights of diﬀerent hidden features.
recognition task, which is restamp as w2v2-mean. Graph U-Net and GRU are
also considered as the baseline which are used as a pooling method for speaker
recognition in [21, 31].
Proposed method. In the graph attention layer, we set F′=F, do not
change the dimension of input features, and just project the inpu t features into
another space. In the vertex update and readout function, we s etT= 2, the
number of hidden layer of MLPt,t= 1,2,MLPθandMLPϕis 1, and the
dimension of hidden layer is 1024 for all the MLPs.
Implementation details. In each experiment, we set batch size is 48,
and every sample’s duration is 3 seconds sampling from the audio ﬁles. In
Table II, we take the pretraining weights of w2v2-mean as the initial weights
of wav2vec2.0 for the other models to accelerate the convergenc e process of the
models. We train 30epochs forw2v2-mean,and train5epochs foro thermodels.
The optimizer is Adam [32] with a OneCycle learning rate schedule [34], an d
the loss function is angular additive softmax (AAM) loss function [33 , 34]. We
also propose a thin version of our model, i.e., the is removed in this vers ion.
13Fig. 4. The weight matrices of diﬀerent speakers.
Similar to the [5], we also weight all the hidden features as:
x=/summationtext13
i=1wixi/summationtext13
i=1wi, (14)
where the trainable weights wiare initialized with 1.0, and the xiis the out-
put feature of diﬀerent hidden modules, where x1s the projected speaker rep-
resentation, and x2,···,x13is the 12 Transformer modules’ output features
successively.
14Table 1: The number of parameter in diﬀerent pooling methods .
wav2vec2 pooling loss fn
our 94.4 M 6.9 M 4.6 M
our/thin 94.4 M 5.3 M 4.6 M
Graph U-Net 94.4 M 3.7 M 4.6 M
GRU 94.4 M 7.9 M 6.1 M
4.3. Results
Table I shows the paraments of diﬀerent module in each model. From t he
Table I. we can obtain that our method can reduce the paraments c ompared to
the GRU, but still possessesmore paramentsthan graph U-Net po oling method.
Table II shows the performance for diﬀerent pooling methods base d on the
wav2vec2.0 framework so that the beneﬁts of proposed methods are assessed.
From the Table II, we can obtain that our method can provide a bett er perfor-
mance compared to other fusion methods. Our thin model provides a compa-
rable performance and reduce about 23% parameters compared t o the original
version.
We also notice that if the only ﬁnal feature is used as the output fea ture,
the performance of our models and GRU will be degraded, especially in our
original model. However, these phenomena are not obvious in other models,
and some models even show the opposite results, such as mean, max , graph U-
net model. These modelsall haslessparametersthanourmodels and GRU. The
numberofparameterscan determinethe upper limit ofthe express ivepowerofa
model. We believe that the additional information of weighted featur es beyond
the capacity of these models, the extra information may confuse t he backend
classiﬁer and degrade the performance.
Figure 2 shows diﬀerent weights of each hidden feature, we notice t hat the
ﬁnal output feature possesses the largest weight compared to o ther output fea-
tures in all the models, which is diﬀerent from the [5]. In the [5], the lar ger
15Table 2: The performance of diﬀerent pooling methods.
voxceleb1 voxceleb-e voxceleb-h
ourallfeatures 1.79 1.75 3.2
ﬁnalfeature 2.29 2.33 3.9
our/thinallfeatures 1.83 1.88 3.22
ﬁnalfeature 1.96 1.84 3.36
Graph U-Netallfeatures 2.07 2.2 3.99
ﬁnalfeature 1.9 1.84 3.36
GRUallfeatures 2.1 2.07 3.92
ﬁnalfeature 2.2 2.17 4.05
maxallfeatures 2.09 2.03 3.65
ﬁnalfeature 2.04 2.03 3.65
meanallfeatures 2.14 1.9 3.89
ﬁnalfeature 1.89 1.85 3.42
weights are clustered in the middle features, which means that the e motion fea-
tures are diﬀerent from the identity features of a speaker and th at is consistent
with common sense.
It’s worth noting that the GRU based backend classiﬁer does not sh ow the
competitive performance compared to other methods. That’s an in teresting
phenomenon. GRU fusion technique has shown an excellent perform ance in
many models, including speech recognition [35, 36, 37], speech emot ion recogni-
tion [38, 39], speech enhancement [40, 41], speaker recognition [31 , 42], etc. We
notice that most of these models put the GRU modules right behind co nvolu-
tion modules. That’s reasonable and eﬀective, the convolution modu les extract
latent speech representation and GRU modules further fused the information
in temporal dimension. However, in the wav2vec2.0 framework, the extracted
latent representationsfrom the convolution modules have been fe d to the Trans-
former modules, which can also focus the temporal information and has been
proved to be excellent at this [7]. Therefore, it is diﬃcult for the GRU m odules
16to extract extra temporal information from the output feature s of Transformer
modules. That is the main reason for GRU possesses the largest num ber of
parameters, but do not show the competitive performance.
Mean and max fusion methods mainly focus on the statistical informa tion of
output features and the representative features, respective ly. These two meth-
ods both provide extra information in non-temporal dimension. Tha t is the
reason why these two methods can outperform than GRU.
GNN uses graph message passing and vertex updating mechanism to obtain
the graph structure information in the non-Euclidean space. In Fig ure 4, each
subﬁgure shows the graph adjacency matrix in diﬀerent utteranc es. From the
Figure 4, we can obtain that each center feature has high weights w ith its adja-
cent features, which means that our GNN can extract the tempor al information.
Besides, it’s worth noting that the high similarity has no obvious distan ce prop-
erty, i.e., the high weights also appear in some features that are far away from
the center features. That means that GNN can not only fuse its ad jacency
features, but also some important features even in a long distance . Compared
to mean, middle, ﬁrst and random fusion method that allocate equal weight to
each feature and the maximum weight to a random feature, our met hod can
treat diﬀerent features with diﬀerent weights. In subsection 3.1, we have proved
that the mean, max and random fusion method are the special case of GNN
and Figure 3 further concretes this experimentally.
5. CONCLUSION
In this paper, we mainly take the wav2vec2.0 framework as the spea ker fea-
ture extractor to apply to speaker recognition task and then inve stigate the
graph neural network as the backend processing tool to aggreg ate the speaker
features. Speciﬁcally, we ﬁrst show that our motivation is reasona bleby proving
some classical pooling methods can be expressedin the GSP form or G NN form.
Then, we obtain the graph structure by GAT and we follow the MPNN f rame-
work to design our GNN including message function, vertex update f unction
17and readout function. Finally, we evaluate our model on Voxceleb1 d ataset, the
experiments show the GNN can obtain better performance than th e classical
pooling method, GRU feature fusion method and the other GNN applie d to
the speaker recognition task. In experiment results, we show why our proposed
method can obtain the best performance compared to other meth ods.
References
[1] J. Devlin, M. Chang, K. Lee, K. Toutanova,
BERT: pre-training of deep bidirectional transformers for langua ge understanding,
in: J. Burstein, C. Doran, T. Solorio (Eds.), Proceedings of the 201 9
Conference of the North American Chapter of the Association for Com-
putational Linguistics: Human Language Technologies, NAACL-HLT
2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Sho rt
Papers), Association for Computational Linguistics, 2019, pp. 41 71–4186.
doi:10.18653/v1/n19-1423 .
URLhttps://doi.org/10.18653/v1/n19-1423
[2] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariw al,
A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Her bert-
Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J . Wu,
C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess ,
J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, D. Amodei,
Language models are few-shot learners, in: Proceedings of the 34 th Inter-
national Conference on Neural Information Processing Systems , NIPS’20,
Curran Associates Inc., Red Hook, NY, USA, 2020.
[3] A. Baevski, Y. Zhou, A. Mohamed, M. Auli,
wav2vec 2.0: A framework for self-supervised learning of speech r epresentations,
in: H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, H. Lin (Eds.), Ad -
vances in Neural Information Processing Systems 33: Annual Con ference
on Neural Information Processing Systems 2020, NeurIPS 2020, December
186-12, 2020, virtual, 2020.
URLhttps://proceedings.neurips.cc/paper/2020/hash/92d1 e1eb1cd6f9fba3227870bb6d7f07-Abstract.html
[4] Z.Fan,M.Li,S.Zhou,B.Xu,Exploring wav2vec 2.0 on speaker ve riﬁcation and language identiﬁcation,
in: H. Hermansky, H. Cernock´ y, L. Burget, L. Lamel, O. Scharen -
borg, P. Motl´ ıcek (Eds.), Interspeech 2021, 22nd Annual Conf er-
ence of the International Speech Communication Association, Brn o,
Czechia, 30 August - 3 September 2021, ISCA, 2021, pp. 1509–15 13.
doi:10.21437/Interspeech.2021-1280 .
URLhttps://doi.org/10.21437/Interspeech.2021-1280
[5] L. Pepino, P. Riera, L. Ferrer, Emotion Recognition from Speech Using
wav2vec 2.0 Embeddings, in: Proc. Interspeech 2021, 2021, pp. 3 400–3404.
doi:10.21437/Interspeech.2021-703 .
[6] N.Vaessen,D.A.vanLeeuwen,Fine-tuning wav2vec2 for spea ker recognition,
in: IEEE International Conference on Acoustics, Speech and Sign al Pro-
cessing, ICASSP 2022, Virtual and Singapore, 23-27 May 2022, IE EE,
2022, pp. 7967–7971. doi:10.1109/ICASSP43922.2022.9746952 .
URLhttps://doi.org/10.1109/ICASSP43922.2022.9746952
[7] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
L. Kaiser, I. Polosukhin, Attention is all you need, in: I. Guyon, U. v on
Luxburg, S. Bengio, H. M. Wallach, R. Fergus, S. V. N. Vishwanatha n,
R. Garnett (Eds.), Advances in Neural Information Processing Sy stems
30: Annual Conference on Neural Information Processing Syste ms 2017,
December 4-9, 2017, Long Beach, CA, USA, 2017, pp. 5998–6008 .
URLhttps://proceedings.neurips.cc/paper/2017/hash/3f5e e243547dee91fbd053c1c4a845aa-Abstract.html
[8] X. Yan, Z. Yang, T. Wang, H. Guo,
An iterative graph spectral subtraction method for speech enha ncement,
SpeechCommun.123(2020)35–42. doi:10.1016/j.specom.2020.06.005 .
URLhttps://doi.org/10.1016/j.specom.2020.06.005
19[9] T. Wang, H. Guo, Q. Zhang, Z. Yang,
A new multilayer graph model for speech signals with graph learning,
Digit. Signal Process. 122 (2022) 103360.
doi:10.1016/j.dsp.2021.103360 .
URLhttps://doi.org/10.1016/j.dsp.2021.103360
[10] T. Wang, H. Guo, X. Yan, Z. Yang,
Speech signal processing on graphs: The graph frequency analys is and an improved graph wiener ﬁltering method,
SpeechCommun.127(2021)82–91. doi:10.1016/j.specom.2020.12.010 .
URLhttps://doi.org/10.1016/j.specom.2020.12.010
[11] A. Sandryhaila, J. M. F. Moura, Discrete signal processing on g raphs,
IEEE Trans. Signal Process. 61 (7) (2013) 1644–1656.
doi:10.1109/TSP.2013.2238935 .
URLhttps://doi.org/10.1109/TSP.2013.2238935
[12] A.Sandryhaila,J.M.F.Moura,Discrete signal processing on g raphs: Frequency analysis,
IEEE Trans. Signal Process. 62 (12) (2014) 3042–3054.
doi:10.1109/TSP.2014.2321121 .
URLhttps://doi.org/10.1109/TSP.2014.2321121
[13] L.Ruiz,F.Gama,A.Ribeiro,Graph neural networks: Architec tures, stability, and transferability,
Proc. IEEE 109 (5) (2021) 660–682. doi:10.1109/JPROC.2021.3055400 .
URLhttps://doi.org/10.1109/JPROC.2021.3055400
[14] M. Cheung, J. Shi, O. Wright, L. Y. Jiang, X. Liu, J. M. F. Moura,
Graph signal processing and deep learning: Convolution, pooling, an d topology,
IEEE Signal Process. Mag. 37 (6) (2020) 139–149.
doi:10.1109/MSP.2020.3014594 .
URLhttps://doi.org/10.1109/MSP.2020.3014594
[15] V. Vasudevan, M. Bassenne, M. T. Islam, L. Xing,
Image classiﬁcation using graph neural network and multiscale wave let superpixels,
CoRR abs/2201.12633 (2022). arXiv:2201.12633 .
URLhttps://arxiv.org/abs/2201.12633
20[16] N.Goyal,D.Steiner,Graph neural networks for image classiﬁc ation and reinforcement learning using graph representations,
CoRR abs/2203.03457 (2022). arXiv:2203.03457 ,
doi:10.48550/arXiv.2203.03457 .
URLhttps://doi.org/10.48550/arXiv.2203.03457
[17] V. Vasudevan, M. Bassenne, M. T. Islam, L. Xing,
Image classiﬁcation using graph neural network and multiscale wave let superpixels,
CoRR abs/2201.12633 (2022). arXiv:2201.12633 .
URLhttps://arxiv.org/abs/2201.12633
[18] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, G. Monfardin i,
The graph neural network model, IEEE Trans. Neural Networks 2 0 (1)
(2009) 61–80. doi:10.1109/TNN.2008.2005605 .
URLhttps://doi.org/10.1109/TNN.2008.2005605
[19] J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, G. E. Dahl,
Neural message passing for quantum chemistry, in: D. Precup, Y. W. Teh
(Eds.), Proceedingsofthe34thInternationalConferenceonMa chineLearn-
ing, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, Vol. 70 of
Proceedings of Machine Learning Research, PMLR, 2017, pp. 1263 –1272.
URLhttp://proceedings.mlr.press/v70/gilmer17a.html
[20] J. Jung, H. Heo, H. Yu, J. S. Chung,
Graph attention networks for speaker veriﬁcation, in: IEEE Inte rna-
tional Conference on Acoustics, Speech and Signal Processing, I CASSP
2021, Toronto, ON, Canada, June 6-11, 2021, IEEE, 2021, pp. 6 149–6153.
doi:10.1109/ICASSP39728.2021.9414057 .
URLhttps://doi.org/10.1109/ICASSP39728.2021.9414057
[21] H. Shim, J. Heo, J. Park, G. Lee, H. Yu,
Graph attentive feature aggregation for text-independent spe aker veriﬁcation,
in: IEEE International Conference on Acoustics, Speech and Sign al Pro-
cessing, ICASSP 2022, Virtual and Singapore, 23-27 May 2022, IE EE,
212022, pp. 7972–7976. doi:10.1109/ICASSP43922.2022.9746257 .
URLhttps://doi.org/10.1109/ICASSP43922.2022.9746257
[22] H. Tak, J. Jung, J. Patino, M. Todisco, N. W. D. Evans,
Graph attention networks for anti-spooﬁng, in: H. Hermansky, H . Cer-
nock´ y, L. Burget, L. Lamel, O. Scharenborg, P. Motl´ ıcek (Eds .), Inter-
speech 2021, 22nd Annual Conference of the International Spe ech Commu-
nication Association, Brno, Czechia, 30 August - 3 September 2021 , ISCA,
2021, pp. 2356–2360. doi:10.21437/Interspeech.2021-993 .
URLhttps://doi.org/10.21437/Interspeech.2021-993
[23] E.Jang,S.Gu, B.Poole,Categorical reparameterization with gumbel-softmax,
in: 5th International Conference on Learning Representations, ICLR 2017,
Toulon, France, April 24-26, 2017, Conference Track Proceedin gs, Open-
Review.net, 2017.
URLhttps://openreview.net/forum?id=rkE3y85ee
[24] A.Gavili,X.Zhang,On the shift operator, graph frequency, a nd optimal ﬁltering in graph signal processing,
IEEE Trans. Signal Process. 65 (23) (2017) 6303–6318.
doi:10.1109/TSP.2017.2752689 .
URLhttps://doi.org/10.1109/TSP.2017.2752689
[25] K. Hornik, M. B. Stinchcombe, H. White,
Multilayer feedforward networks are universal approximators, N eural
Networks 2 (5) (1989) 359–366. doi:10.1016/0893-6080(89)90020-8 .
URLhttps://doi.org/10.1016/0893-6080(89)90020-8
[26] K. K. Thekumparampil, C. Wang, S. Oh, L. Li,
Attention-based graph neural network for semi-supervised lear ning,
CoRR abs/1803.03735 (2018). arXiv:1803.03735 .
URLhttp://arxiv.org/abs/1803.03735
[27] J. B. Lee, R. A. Rossi, S. Kim, N. K. Ahmed, E. Koh,
Attention models in graphs: A survey, ACM Trans. Knowl. Discov. Da ta
2213 (6) (2019) 62:1–62:25. doi:10.1145/3363574 .
URLhttps://doi.org/10.1145/3363574
[28] A. Nagrani, J. S. Chung, A. Zisserman,
Voxceleb: A large-scale speaker identiﬁcation dataset, in: F. Lace rda
(Ed.), Interspeech 2017, 18th Annual Conference of the Inter national
Speech Communication Association, Stockholm, Sweden, August 20 -24,
2017, ISCA, 2017, pp. 2616–2620.
URLhttp://www.isca-speech.org/archive/Interspeech_2017 /abstracts/0950.html
[29] J. S. Chung, A. Nagrani, A. Zisserman,
Voxceleb2: Deep speaker recognition, in: B. Yegnanarayana (Ed.) ,
Interspeech 2018, 19th Annual Conference of the Internation al Speech
Communication Association, Hyderabad, India, 2-6 September 201 8,
ISCA, 2018, pp. 1086–1090. doi:10.21437/Interspeech.2018-1929 .
URLhttps://doi.org/10.21437/Interspeech.2018-1929
[30] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue,
A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz, J. Davi-
son, S. Shleifer, P. von Platen, C. Ma, Y. Jernite, J. Plu,
C. Xu, T. L. Scao, S. Gugger, M. Drame, Q. Lhoest, A. M.
Rush, Transformers: State-of-the-art natural language pro cessing, in:
Q. Liu, D. Schlangen (Eds.), Proceedings of the 2020 Conference
on Empirical Methods in Natural Language Processing: System
Demonstrations, EMNLP 2020 - Demos, Online, November 16-20,
2020, Association for Computational Linguistics, 2020, pp. 38–45 .
doi:10.18653/v1/2020.emnlp-demos.6 .
URLhttps://doi.org/10.18653/v1/2020.emnlp-demos.6
[31] J. Jung, H. Heo, J. Kim, H. Shim, H. Yu,
Rawnet: Advanced end-to-end deep neural network using raw wa veforms for text-independent speaker veriﬁcation,
in: G. Kubin, Z. Kacic (Eds.), Interspeech 2019, 20th Annual Con-
ference of the International Speech Communication Association,
23Graz, Austria, 15-19 September 2019, ISCA, 2019, pp. 1268–12 72.
doi:10.21437/Interspeech.2019-1982 .
URLhttps://doi.org/10.21437/Interspeech.2019-1982
[32] D. P. Kingma, J. Ba, Adam: A method for stochastic optimization , in:
Y. Bengio, Y. LeCun (Eds.), 3rd International Conference on Lea rning
Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Confer-
ence Track Proceedings, 2015.
URLhttp://arxiv.org/abs/1412.6980
[33] J. Deng, J. Guo, J. Yang, N. Xue, I. Kotsia, S. Zafeiriou,
Arcface: Additive angular margin loss for deep face recognition, IE EE
Trans. Pattern Anal. Mach. Intell. 44 (10) (2022) 5962–5979.
doi:10.1109/TPAMI.2021.3087709 .
URLhttps://doi.org/10.1109/TPAMI.2021.3087709
[34] J. Deng, J. Guo, N. Xue, S. Zafeiriou,
Arcface: Additive angular margin loss for deep face recognition, in: IEEE
Conference on Computer Vision and Pattern Recognition, CVPR 201 9,
Long Beach, CA, USA, June 16-20, 2019, Computer Vision Foundat ion /
IEEE, 2019, pp. 4690–4699. doi:10.1109/CVPR.2019.00482 .
URLhttp://openaccess.thecvf.com/content_CVPR_2019/html /Deng_ArcFace_Additive_Angular_Margin_Loss_for_Deep _Face_Recognition_CVPR_2019_paper.html
[35] M. Ravanelli, P. Brakel, M. Omologo, Y. Bengio,
Light gated recurrent units for speech recognition, IEEE
Trans. Emerg. Top. Comput. Intell. 2 (2) (2018) 92–102.
doi:10.1109/TETCI.2017.2762739 .
URLhttps://doi.org/10.1109/TETCI.2017.2762739
[36] S. Reza, M. C. Ferreira, J. J. M. Machado, J. M. R. Tavares, A customized
residual neural network and bi-directional gated recurrent unit -based auto-
matic speech recognition model, Expert Systems with Applications (2 022).
[37] A.Moumen,T.Parcollet,Stabilising and accelerating light gated r ecurrent units for automatic speech recognition,
CoRR abs/2302.10144 (2023). arXiv:2302.10144 ,
24doi:10.48550/arXiv.2302.10144 .
URLhttps://doi.org/10.48550/arXiv.2302.10144
[38] Z.Zhu,W.Dai,Y.Hu,J.Li,Speech emotion recognition model ba sed on bi-gru and focal loss,
Pattern Recognition Letters 140 (2020) 358–365.
doi:https://doi.org/10.1016/j.patrec.2020.11.009 .
URLhttps://www.sciencedirect.com/science/article/pii/S 0167865520304141
[39] M. R. Ahmed, S. Islam, A. K. M. M. Islam, S. Shatabda,
An ensemble 1d-cnn-lstm-gru model with data augmentation for sp eech emotion recognition,
CoRR abs/2112.05666 (2021). arXiv:2112.05666 .
URLhttps://arxiv.org/abs/2112.05666
[40] Y. Wang, J. Han, T. Zhang, D. Qing,
Speech enhancement from fused features based on deep neural network and gated recurrent unit network,
EURASIP J. Adv. Signal Process. 2021 (1) (2021) 104.
doi:10.1186/s13634-021-00813-8 .
URLhttps://doi.org/10.1186/s13634-021-00813-8
[41] X.Cui, Z.Chen,F.Yin, Speech enhancement based on simple rec urrent unit network,
AppliedAcoustics157(2020)107019. doi:https://doi.org/10.1016/j.apacoust.2019.107019 .
URLhttps://www.sciencedirect.com/science/article/pii/S 0003682X19303743
[42] J. Jung, S. Kim, H. Shim, J. Kim, H. Yu,
Improved rawnet with feature map scaling for text-independent s peaker veriﬁcation using raw waveforms,
in: H. Meng, B. Xu, T. F. Zheng (Eds.), Interspeech 2020, 21st An nual
Conference of the International Speech Communication Associat ion,
Virtual Event, Shanghai, China, 25-29 October 2020, ISCA, 2020 , pp.
1496–1500. doi:10.21437/Interspeech.2020-1011 .
URLhttps://doi.org/10.21437/Interspeech.2020-1011
25