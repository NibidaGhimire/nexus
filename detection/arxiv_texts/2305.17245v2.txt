Error Estimation for
Single-Image Human Body Mesh Reconstruction
Hamoon Jafarian
Ontario Tech University
Oshawa, Ontario, Canada
hamoon.jafarian@ontariotechu.caFaisal Z. Qureshi
Ontario Tech University
Oshawa, Ontario, Canada
faisal.qureshi@ontariotechu.ca
Abstract
Human pose and shape estimation methods continue
to suffer in situations where one or more parts of the
body are occluded. More importantly, these methods can-
not express when their predicted pose is incorrect. This
has serious consequences when these methods are used in
human-robot interaction scenarios, where we need meth-
ods that can evaluate their predictions and flag situations
where they might be wrong. This work studies this prob-
lem. We propose a method that combines information
from OpenPose and SPIN—two popular human pose and
shape estimation methods—to highlight regions on the pre-
dicted mesh that are least reliable. We have evaluated
the proposed approach on 3DPW, 3DOH, and Human3.6M
datasets, and the results demonstrate our model’s effective-
ness in identifying inaccurate regions of the human body
mesh. Our code is available at https://github.com/
Hamoon1987/meshConfidence .
1. Introduction
The applications of Human Body Mesh Recovery
(HMR) are diverse and numerous. HMR, for example,
is useful for Human-Robot Interaction (HRI) scenarios,
where accurate 3D mesh representation is essential for en-
suring safe interactions [18]. Drones, self-driving cars,
and human-robot collaborative manufacturing systems are
some examples where a three-dimensional understanding
of the environment and humans is critical for reliable op-
eration [16]. Additionally, the animation and movie indus-
tries can benefit significantly from HMR by simplifying the
process of character motion capture (MOCAP) and reduc-
ing the costs involved [26]. Other areas such as part and
foreground segmentation, computer-assisted coaching, and
virtual try-on can also leverage the capabilities of 3D mesh
recovery to enhance their outcomes [25].
The task of estimating a human body mesh from a singleInput SPIN Our method
Figure 1: Human body mesh recovery using the SPIN
model vs. our method. The input images are in the first
column; the recovered meshes using SPIN are shown in the
second column. Our method identifies whether or not the
recovered mesh corresponds to a partially occluded human
and highlights regions of the mesh that are unreliable since
these represent parts of the human body that are not visible
in the image.arXiv:2305.17245v2  [cs.CV]  31 May 2023Figure 2: Pixel Locations based Occlusion Sensitivity Anal-
ysis. A 40×40square (gray) occluder is moved across
the image and MPJPE values are computed (for each) for
SPIN (top-row) and OpenPose (bottom-row) models. The
heatmaps highlight locations in the image that strongly af-
fects the performance of the two models. Both models are
sensitive to occlusions in regions shown in red. Image size
is224×224and the stride is selected to be 20.
RGB image is an active area of research that has garnered
significant interest in the field of computer vision. Kolo-
toures and colleagues [13] proposed SPIN that achieves im-
pressive results on single image human body mesh recov-
ery. SPIN represents a significant improvement in human
pose and shape estimation over prior methods, and it is now
a widely adopted baseline in the field. A number of re-
cent methods attempt to recover human body mesh in the
presence of occlusions [31, 12, 11]. None of these meth-
ods, however, provide a confidence score for the recovered
mesh. The ability to tell whether or not the recovered mesh
is correct or to identify parts of the mesh that may be in-
accurate is particularly relevant for human-robot interaction
scenarios. A robot, for example, can choose to halt its op-
eration if it deems that the recovered mesh is not reliable.
Alternatively a robot may adjust its viewpoint to achieve a
better reconstruction if it identifies one or more parts of the
mesh to be unreliable.
Here we tackle the problem of estimating the error in the
reconstructed human body meshes. We propose a method
that fuses information from SPIN and OpenPose [2] to high-
light regions of the recovered mesh that may be inaccurate
(Figure 1). OpenPose estimates human joints’ keypoints,
and it is able to identify joints that are not visible in the im-
age. The proposed method leverages the observation that
SPIN and OpenPose agree when the person is visible in the
image; whereas, these two methods disagree when the per-
son is partially occluded. We have used sensitivity analysis
to quantify the disagreement between SPIN and OpenPose
models under occluded settings. The differences between
the joints’ keypoints estimated by OpenPose and those con-
structed by projecting the human body mesh recovered by
SPIN are fed into two multi-layer perceptron networks to3DPW H36M
min: 101 - max: 130 min: 111 - max: 141 min: 69 - max: 96 min: 113 - max: 141
Figure 3: Joints based Occlusion Sensitivity Analysis. For
every image in 3DPW and H36M datasets, a square oc-
cluder is pasted over each joint in turn and MPJPE values
are computed for SPIN (left) and OpenPose (right) models.
MPJPE errors for each joint are visualized by highlighting
the vertices (of the mesh) that correspond to each joint. The
figures depict model performance if one of the joints is oc-
cluded. This figure is best viewed in color.
compute an error estimate for each region of the mesh. In
Figure 1 (last column from right) regions shown in red de-
pict mesh parts with the lowest reliability. Note that these
regions correspond to the parts of the human body that are
not visible in the image.
To the best of our knowledge, this work represents the
first attempt at estimating error in single-image 3D human
body mesh reconstructions. The contributions of the work
presented here are: 1) location-based and joint-based occlu-
sion sensitivity analysis to quantify the relationship between
the disagreement of OpenPose and SPIN joint location esti-
mates and the “true” error; 2) a mesh classifier that identifies
whether or not the recovered mesh is reliable; and 3) a worst
joint classifier that selects the least reliable joint. This work
represents a significant step towards improving the safety
and reliability of those human-robot interactions that rely
upon accurate reconstructions of human body mesh by pro-
viding additional information about the confidence and re-
liability of the estimated mesh.
2. Related Work
2D Keypoint Estimation. 2D keypoint estimation aims
to localize body joints within an image. Joints’ keypoint
estimation comes in two flavours: regression-based meth-
ods [24, 17, 30] and detection-based methods [23, 3]. Top-
down approaches achieve better accuracy in multi-person
scenarios; however, bottom-up approaches are often faster
and are better suited for real-world applications that demand
real-time performance [5]. Pishcgulin et al. [21] proposed
DeepCut, a CNN-based body part detector, and Insafutdi-
nov et al. [7] improved the DeepCut model by employing
a ResNet-based deep part detector. Cao et al. [2] intro-
duced Part Affinity Fields (PAFs) that encode the position
and orientation of human body parts and propose Open-
Poase, an accurate, fast, and robust model for multi-personFigure 4: Overview of the Proposed Framework. The input image Iis passed through the SPIN and OpenPose models. Then,
the estimated SPIN mesh ( M) is regressed and projected into 2D joint coordinates. Comparing the results with the OpenPose
predicted 2D joint positions, Estimation Difference (ED) is obtained. Afterward, ED is employed to train the Mesh Classifier
(MC) and Worst Joint Classifier (WJC) that decide the SPIN mesh quality and detect the least reliable parts of the mesh,
respectively.
joints’ keypoints estimation.
3D Pose and Shape Estimation. Broadly speaking 3D
pose and shape estimation methods are divided into two
classes: optimization-based methods that deform a canon-
ical pose to match the image [1, 28, 14] and regression-
based methods that directly estimate the mesh from the im-
age [9, 19, 20]. Optimization-based methods achieve good
results; however, these are slow and require careful initial-
ization. The regression-based methods, on the other hand,
are difficult to train to achieve high-quality meshes [25].
Kolotoures et al. [13] proposed SPIN method for human
body mesh reconstruction that employs optimization to pro-
vide explicit 3D supervision to train a regressor to construct
high-quality meshes. Hybrid models achieve state-of-the art
performance. These benefit from the 3D pose and shape es-
timation models’ ability to capture the realistic body struc-
ture and combine it with the higher accuracy of keypoint
estimation models [15].
Occlusion Handling. Inspired by random erasing [32]
and synthetic occlusion [4] techniques exploited in classi-
fication and object detection tasks, some researchers sug-
gest that data augmentation could be a suitable solution
against occlusion. In this scenario, the images are occluded
throughout the training process, and the model is taught to
perform better against occlusion [22, 10]. Others modified
the model architecture to improve the model’s robustness
against occlusion. Zhang et al. [31] uses a partial UV map
model to convert the occluded human body to an image
inpainting problem. Georgakis et al. [6] develop a prior-
informed regressor that knows the hierarchical structure of
the human body, and the experiments show that this method
improves the model performance against occluded cases.
Kocabas et al. [12] implemented the soft attention mecha-
nism for the HMR problem, resulting in a considerable im-provement of the model’s robustness against occlusion. The
developed part attention regressor (PARE) learns to rely on
visible body parts to reason about the occluded parts.
3. Occlusion Sensitivity Analysis
We experiment with two approaches to visualize and un-
derstand the effects of partial occlusions of the human body
on the performance of SPIN and OpenPose models. The
first approach captures the sensitivity (of both methods) to
occluded regions for a given image. The second approach,
on the other hand, shows the sensitivity to an occluded joint
over the entire dataset.
The first approach is inspired by [29, 12], where a square
occluder is pasted onto different pixel locations in the im-
age. Both the size and the stride of the occluder can be
changed. Similar to [29], we use a grey colored square. The
occluded images are passed to SPIN and OpenPose mod-
els and the errors are recorded. The performance of both
models is measured using the Mean per Joint Position Error
(MPJPE) that is defined as the mean value of the Euclidean
distance between the ground truth and the predicted loca-
tions of all the joints. SPIN model recovers an SMPL mesh
M. Using a pre-trained regressor W, it is possible to es-
timate 3D joint locations X=WM , where X∈RK×3.
K= 14 refers to the number of joints. MPJPE for SPIN
model is
MPJPESPIN
(m,n )=Mean
k∥X(m,n )−Xgt∥. (1)
Here X(m,n )denotes 3D joint locations when occluder is
centered at location (m, n).Xgtdenotes ground truth 3D
joint locations. For the OpenPose model, which estimatesRegular
 Occluded
 Regular
 Occluded
Figure 5: Pearson Correlation Coefficient. We calculate the correlation coefficient for each joint throughout the 3DPW
dataset. The average value in the absence of occlusions is r= 0.67. This value jumps to 0.735for the occluded version of
the 3DPW dataset. These values suggest a positive correlation between ED and SE. Four regions (a, b, c and d) are indicated
in the top-right plot. Region a denotes False Positive scenarios, i.e., the estimated joint location is inaccurate, however, the
proposed model has failed to identify it. Region d denotes False Negative scenarios where the estimated joint location is
erroneously labelled inaccurate. Combining information from multiple joints helps deal with these scenarios.
2D joint locations x∈RK×2,
MPJPEOP
(m,n )=Mean
k∥x(m,n )−xgt∥, (2)
where x(m,n )andxgtare 2d joint estimates when occluder
is centered at (m, n)and ground truth 2d locations, respec-
tively. Figure 2 plots MPJPE scores for both models using a
heatmap. The figure shows how partial occlusions affect the
performance of the two methods as measured by MPJPE.
For the second approach, the square occluder is used to
hide specific joints through the entire dataset. Where as the
first approach captures the occlusions sensitivity to partic-
ular image locations, the second approach finds occlusions
sensitivity to different joints. In this case
MPJPESPIN
k=Mean
iMean
k∥Xi,k−Xgt
i∥, (3)
where iindices over images, kindices over images, Xi,k
denotes 3D joints’ locations estimations for image iwhen
occluder is centered on joint k.Xgt
iis ground truth 3D joint
locations for image i. Similarly,
MPJPEOP
k=Mean
iMean
k∥xi,k−xgt
i∥. (4)Here xi,krefers to OpenPose joint estimates for image i
when the occluder is centered at joint kandxgt
idenotes
ground truth 2D joints for image i. Figure 3 visualizes
MPJPE values for both methods on an SMPL mesh. Ev-
ery vertex of a joint is associated with one or more joints,
and each vertex is assigned a color using MPJPE kvalues,
where kbelongs to the set of joints associated with this ver-
tex. These colors visualize the sensitivity of the two meth-
ods to an occluded joint.
4. Method
Figure 4 illustrates the proposed method for assigning an
error estimate to different regions of the reconstructed hu-
man body mesh. It comprises three steps: 1) SPIN model is
used to estimate “2D” joint locations, 2) OpenPose model
is used to recover 2D joint locations, 3) The difference be-
tween the 2D joint estimates for SPIN and OpenPose is used
to assign a confidence score to the mesh. When SPIN and
OpenPose models correctly estimate a joint position, the es-
timated coordinates are close to each other and adjacent to
the ground truth. However, based on the sensitivity analysis,Input Output Input Output
Figure 6: The right wrist is occluded in the first input image,
making both Openpose and SPIN models misestimate the
right wrist’s position. However, these wrong estimations are
adjacent. The green dot shows the ground truth position,
and the red dot represents the OpenPose estimation of the
right wrist. In the second case, OpenPose is confused by
the other person’s right wrist and makes a wrong estimation,
while the SPIN model accurately estimates the right wrist.
These are two samples that negatively affect the correlation
between ED and SE.
when the models’ estimated positions are inaccurate, we ex-
pect the joint position estimations to be dissimilar. Hence,
the distance between the models’ outputs
EDi=∥xSPIN
i−xOP
i∥, (5)
can be considered as a proxy for the confidence in the re-
covered human body mesh. Here xOP
iare 2D joint estimates
for OpenPose and xSPIN
iareprojected 2D joint estimates for
SPIN. ED i∈RKandirefers to the image.
To investigate the hypothesis that ED is a useful proxy
for the confidence in the recovered mesh, we calculate the
correlation between the ED and the SPIN model’s error
SEi=∥xSPIN
i−xgt
i∥. (6)
The Pearson correlation coefficient of joint kwhich is
shown by rkis calculated using
rk=Corr([ED0,k, ...,EDn,k],[SE0,k, ...,SEn,k]),(7)
where nstands for the number of images in the dataset.
Since the OpenPose model provides 2D estimates, it can
only be compared to the 2D projection of the SPIN model
output. Hence, SE only captures the 2D error of the SPIN
model. Additionally, the OpePose model does not provide
any estimations for the undetected joints, which forces us
to ignore those points for calculating the correlation. The
computed correlation coefficient for the 3DPW test dataset
for each joint is presented in Figure 5. The average coef-
ficient r= 0.67indicates a strong correlation between ED
and SE. This suggests that the differences in the estimated
joint positions by SPIN and OpenPose models capture the
error of SPIN model with respect to the ground truth. We
leverage this information and explore three techniques that
use ED to estimate confidence for the recovered mesh.Without Occlusion With OcclusionInput
 Output
Figure 7: The error distribution on the estimated mesh
changes when part of the human is occluded. For exam-
ple, when a squared occluder is pasted onto the left hand,
the model successfully identifies that is the least reliable re-
gion of the mesh (red regions on the mesh).
4.1. Using Raw EDValues
For a given image, ED is a K-dimensional vector that
stores the differences between joints’ location estimates
from SPIN and OpenPose models. We can use these val-
ues to decide whether or not the mesh is “good” as follows
ymesh=(
good if max ED≤threshold
bad otherwise.(8)
We can use a similar argument to identify the worst joint:
kworst= arg max
kED. (9)
4.2. Using Linear Regression
Plots shown in Figure 5 suggest a positive correlation
between SE and ED (for all 14joints), which suggests that it
is possible to estimate SE given ED for a given joint. We are
interested in estimating SE, since it represents the true SPIN
error as computed using ground truth data. We do not have
ground truth data at inference time, so instead we estimate
SE using ED, which we can easily compute using SPIN and
OpenPose models. Therefore, we fit a linear regressor
SEk= (mk)(EDk) +ck (10)
that predicts SE .,kgiven observation ED .,k, where k∈
[1, K]. Given a new image, 1) compute ED, 2) use the
trained linear regressor in Eq. 10 to estimate SE ∈RK, and
3) use the estimated SE to decide whether or not mesh is
“good” or to identify “good” and “bad” joints using the ap-
proach discussed in the previous section. Just substitute SE
in place of ED.Dataset PCC Mesh WJ-R1 WJ-R3
Regular/Occluded R O R O R O R O
3DPW 0.67 0.735 79.2% 86.2% 42.2% 45.4% 70.6% 73.8%
3DOH 0.665 0.707 81.6% 88% 37.3% 40.5% 64.2% 66.8%
H36M-P1 0.492 0.545 71.9% 82.1% 42.4% 43.9% 76.4% 75.1%
Table 1: Model Evaluation. Pearson Coefficient Correlation (PCC), model accuracy in separating accurate and faulty meshes
(Mesh), and model performance on detecting the least reliable joints, i.e., worst joints (WJ), are presented in this table. Model
is allowed a single guess for Rank 1 (R1) and it is allowed three guesses for Rank 3 (R3).
4.3. Classifiers
The previous two approaches of using ED to classify re-
covered human body meshes and joints treat each joint sep-
arately. We now propose an approach that looks at all K
joints simultaneously to classify the mesh and identify the
worst joint. Specifically, we use two multi-linear percep-
tron networks that use ED to classify mesh and identify the
worst joint, respectively.
The Mesh Classifier (MC) network is a binary classifier
containing three hidden linear layers that contain 10,8, and
6neurons respectively with ReLU activation functions. In-
put to MC is ED and it outputs whether or not the recov-
ered mesh is reliable, i.e., all parts of the human body are
visible in the image. MC network is trained using binary
cross-entropy. The ground truth data for training MC is
constructed using SE scores—if SE .,k≥threshold for any
kthen the mesh is deemed unreliable, where SE .,kis the SE
score for joint k. Under this regime
ymesh=fMC(ED). (11)
The Worst Joint Classifier (WJC) network is a K-class
classification network. It comprises three hidden layers
containing 28,56and28neurons, respectively. Hidden lay-
ers use ReLU activation functions. ED is fed into WJC, and
WJC is trained using cross-entropy. The ground truth data
for training WJC is constructed from SE. We simply encode
SE using one-hot-encoded form with 1atarg max
kSE and
0elsewhere. Using WJC,
kworst=fWJC(ED). (12)
5. Experiments and Results
We use 3DPW [27] and Human3.6M [8] (S9 and S11)
datasets for model training and testing. In addition, we use
3DOH [31] dataset for testing only. The threshold used in
Section 4 is set at 10mm, i.e., if the difference between
an estimated joint location and the ground truth joint loca-
tion is higher than 10mm, the mesh recovered by the SPIN
model is labelled inaccurate. We also created occluded ver-
sions of the three datasets where a randomly selected joint
is occluded using a square occluder in each image.Figure 5 (rows 1 and 3) shows scatter plots of SE vs ED
for every joint for the unoccluded 3DPW dataset. These
plots also show Pearson correlation coefficient for each
joint, which suggests that ED is positively correlated with
SE. This is good news, since it suggests that in the absence
of SE, which is not available at inference time, we can use
ED to compute an error estimate for the recovered mesh.
Consider the ED vs. SE plot for right-wrist joint in Figure 5
(first row, right most figure). The plot identifies four regions
labelled (a), (b), (c) and (d). Points in the regions (a) and (d)
have a negative effect on the correlation. Points in region (a)
suggest that there are several situations where both models
are inaccurate, but that they agree with each other. Thus,
we conclude that when the OpenPose and SPIN estimates
are close to each other, it does not necessarily mean that the
recovered human mesh is accurate. Rather, it may be that
joint estimates from both models are close to each other but
far from the ground truth locations. Figure 6 (input/output
pair on the left) depicts such a case. Here both models are
in agreement with each other, however, both models fail to
detect the right wrist due to self-occlusion and the presence
of other people. Points in region (d) represent cases where
although the estimated values of SPIN and OpenPose model
are different, the SPIN model is accurate. In other words,
in some cases, a measurable difference in OpenPose and
SPIN outputs does not indicate an inaccurate mesh recon-
struction by the SPIN model. The right input/output pair
in Figure 6 shows an example of such a case. The SPIN
model is successful in estimating the right wrist of the per-
son, however, OpenPose model makes a mistake and selects
the other person’s hand position as the correct location for
the right wrist. Despite the points in regions (a) and (d),
the average Pearson correlation coefficient for all joints is
r= 0.67, indicating a strong correlation between ED and
SE for all the joints. This confirms our intuition that ED is
a good proxy for SE.
We performed a similar analysis as shown in Figure 5
(rows 2 and 4) for occluded dataset, where a square oc-
cluder is pasted on a randomly selected joint. The average
Pearson correlation coefficient obtained under these settings
isr= 0.735, which is even higher than the value com-
puted for the unonccluded case. This suggests two things:Input MC MC-GT WJC WJC-GT
Figure 8: Qualitative Results. Input images are shown in
the left column. The next two columns contain the mesh
classifier output and the ground truth. Unreliable meshes
are shown in light pink. The fourth column highlights the
least reliable joints. Red regions on the mesh correspond to
the least reliable joints. The last column shows the ground
truth for the least reliable joints.
1) that the proposed model is robust to occlusions and 2) ED
is even more positively correlated with SE. Table 1 shows
the Pearson correlation coefficient for different test datasets,
and it shows Pearson correlation coefficient is higher for
occluded datasets. In addition, 3DPW and 3DOH datasets
have higher coefficient values since these exhibit higher
occlusion levels. Figure 7 illustrates two instances of the
model’s behavior towards occlusion. Our model predicts
that the recovered mesh is correct when there are no occlu-
sions, however, the model correctly identifies the left wrist
region of the recovered mesh to be unreliable when a square
occluder is used to hide this joint in the input image.
We exploit the positive correlation between ED and SE
to estimate the error in the human body mesh recovered by
SPIN. The proposed method also highlights the least reli-
able region of the recovered mesh. Table 1 lists our model’s
performance at identifying an inaccurate mesh. Addition-
ally, this table also includes model’s performance at iden-Datasets Metric ED L. Regressor Classifier3DPWRMesh 71.2 75.3 79.2
WJ-R1 27.8 38.2 42.2
WJ-R3 61.5 68.8 70.6
OMesh 82.7 81.2 86.2
WJ-R1 30.7 42.17 45.4
WJ-R3 65.5 72.2 73.83DOHRMesh 80.8 82.9 81.6
WJ-R1 22 30.4 37.3
WJ-R3 54.5 70.2 64.2
OMesh 88.6 88 88
WJ-R1 22.5 31.7 40.5
WJ-R3 55.2 67.5 66.8H36M-P1RMesh 66.5 67.8 71.9
WJ-R1 17.8 29 42.4
WJ-R3 58.6 66.5 76.4
OMesh 79.9 78.2 82.1
WJ-R1 22.9 36 43.9
WJ-R3 64.1 69.5 75.1
Table 2: Ablation Study. Comparing the method that uses
raw ED values (column 3), linear regressor (column 4), and
classifier based method (column 5) for classifying unreli-
able meshes and identifying the least reliable joints. Mesh
refers to mesh reliability classification results, WJ-R1 refers
to the results for identifying the worst joint (least reliable)
when a single guess is allowed, and WJ-R3 refers to results
for identifying the worst joint in three guesses.
tifying the least reliable joint. There is no baseline, since,
to the best of our knowledge, ours is the first attempt at
performing error estimation for single-image human body
mesh reconstruction scenarios. For example, while the
model was never trained on 3DOH dataset, it is able to iden-
tify an inaccurate mesh with 88% accuracy. The model is
also able to identify the least reliable joint 40.5% accuracy.
This number jumps to 66.8% when the model is allowed
three guesses for the least reliable joint. These numbers are
considerably higher than randomly selecting the least reli-
able joint. A similar trend is visible for 3DPW and H36M-
P1 datasets.
Consider Figure 8 that presents some qualitative results.
The first four rows show cases where the proposed model
performed correctly. Here MC denotes output from the
mesh classifier and MC-GT denotes the ground truth. WJC
highlights the least reliable joint(s) and WJC-GT shows the
least reliable joint ground truth. The bottom two rows show
failure cases. Here, while the model correctly predicts that
the recovered mesh is unreliable, it is unable to identify the
least reliable joint correctly. Figure 9 shows an application
of our method on video data. Here the top row shows input
frames, the second row shows whether or not the recoveredInput
 MC
 WJC
 Input
 MC
 WJC
Figure 9: Application on Videos. The first row shows the input video frames. The second row shows mesh reliability
classification results. Light pink indicates an unreliable mesh. The last row shows least reliable joints. Here the meshes are
rotated by 90degrees for better visualization. The red regions on the mesh highlight the least reliable joint.
mesh is reliable, and the last row includes a visualization of
the least reliable joint. The meshes shown in the last row are
rotated to better see the least reliable joints. Our model cor-
rectly handles self-occlusions (top three rows) and occlu-
sions due to other objects in the scene (bottom three rows).
Check the last row where the model correctly predicts that
the left foot is the least reliable region of the recovered mesh
since it is not visible in the image (it is occluded by the ta-
ble). The decision to decide if the recovered mesh is “reli-
able” when only left foot is not visible in the image is ap-
plication specific. For example, say a robot is simply nav-
igating around this person then perhaps it is okay to deem
the recovered mesh to be reliable. However, if this same
robot is carrying out a task that involves the left foot of this
person then it is best to consider this mesh unreliable.
5.1. Ablation Study
We now compare the performance of the three ap-
proaches discussed in Section 4. All three approaches lever-
age the positive correlation between ED and SE. Table 2
shows the results obtained for each approach on the three
datasets in both unoccluded and occluded cases. The re-
sults confirm that the classifier-based approach that com-
bines ED information from different joints outperforms the
other two methods. Method that uses raw ED values posts
the worst performance. What is interesting to note is thatusing a classifier dramatically increases the performance of
identifying the least reliable joint, both when the model is
allowed a single guess and when it is allowed three guesses.
This suggests that it is beneficial to consider alljoints’ er-
rors when selecting the least reliable joint. For mesh clas-
sification, however, the improvement obtained by using a
classifier-based approach over using the method that relies
on raw ED values is not nearly as significant.
6. Conclusion
This work develops a method for estimating the error in
the human body meshes reconstructed by the SPIN model.
The model is not only able to decide whether or not a mesh
is unreliable, it is also able to highlight the least reliable,
i.e., having the highest error, regions on the mesh. The pro-
posed model uses the disagreement between joint location
estimates between OpenPose and SPIN model to compute
error values for the recovered mesh. Pearson correlation co-
efficient studies on 3DPW dataset show this disagreement
is a good proxy for the “true” error. Evaluations on 3DPW,
3DPH, and H36M-P1 confirm that the model is able to es-
timate error in the SPIN based single-image human body
mesh reconstructions in the presence of occlusions. Fur-
thermore, it is able to correctly estimate the error in SPIN
meshes even when OpenPose estimates are incorrect. The
model is also able to identify the least reliable joints. Theability to estimate the error in the recovered meshes is par-
ticularly important when these meshes are used in human-
robot interaction scenarios. To the best of our knowledge,
ours is the first method to estimate error in single-image 3D
human body mesh reconstruction.
References
[1] Federica Bogo, Angjoo Kanazawa, Christoph Lassner, Peter
Gehler, Javier Romero, and Michael J Black. Keep it smpl:
Automatic estimation of 3d human pose and shape from a
single image. In European conference on computer vision ,
pages 561–578. Springer, 2016.
[2] Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh.
Realtime multi-person 2d pose estimation using part affinity
fields. In Proceedings of the IEEE conference on computer
vision and pattern recognition , pages 7291–7299, 2017.
[3] Yu Chen, Chunhua Shen, Xiu-Shen Wei, Lingqiao Liu, and
Jian Yang. Adversarial posenet: A structure-aware convolu-
tional network for human pose estimation. In Proceedings
of the IEEE International Conference on Computer Vision ,
pages 1212–1221, 2017.
[4] Nikita Dvornik, Julien Mairal, and Cordelia Schmid. Mod-
eling visual context is key to augmenting object detection
datasets. In Proceedings of the European Conference on
Computer Vision (ECCV) , pages 364–380, 2018.
[5] Miniar Ben Gamra and Moulay A Akhloufi. A review of
deep learning techniques for 2d and 3d human pose estima-
tion. Image and Vision Computing , 114:104282, 2021.
[6] Georgios Georgakis, Ren Li, Srikrishna Karanam, Terrence
Chen, Jana Ko ˇseck´a, and Ziyan Wu. Hierarchical kinematic
human mesh recovery. In European Conference on Com-
puter Vision , pages 768–784. Springer, 2020.
[7] Eldar Insafutdinov, Leonid Pishchulin, Bjoern Andres,
Mykhaylo Andriluka, and Bernt Schiele. Deepercut: A
deeper, stronger, and faster multi-person pose estimation
model. In European conference on computer vision , pages
34–50. Springer, 2016.
[8] Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian
Sminchisescu. Human3. 6m: Large scale datasets and pre-
dictive methods for 3d human sensing in natural environ-
ments. IEEE transactions on pattern analysis and machine
intelligence , 36(7):1325–1339, 2013.
[9] Angjoo Kanazawa, Michael J Black, David W Jacobs, and
Jitendra Malik. End-to-end recovery of human shape and
pose. In Proceedings of the IEEE conference on computer
vision and pattern recognition , pages 7122–7131, 2018.
[10] Lipeng Ke, Ming-Ching Chang, Honggang Qi, and Siwei
Lyu. Multi-scale structure-aware network for human pose
estimation. In Proceedings of the european conference on
computer vision (ECCV) , pages 713–728, 2018.
[11] Rawal Khirodkar, Shashank Tripathi, and Kris Kitani. Oc-
cluded human mesh recovery. In Proceedings of the
IEEE/CVF conference on computer vision and pattern
recognition , pages 1715–1725, 2022.
[12] Muhammed Kocabas, Chun-Hao P Huang, Otmar Hilliges,
and Michael J Black. Pare: Part attention regressor for 3dhuman body estimation. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , pages 11127–
11137, 2021.
[13] Nikos Kolotouros, Georgios Pavlakos, Michael J Black, and
Kostas Daniilidis. Learning to reconstruct 3d human pose
and shape via model-fitting in the loop. In Proceedings of
the IEEE/CVF International Conference on Computer Vi-
sion, pages 2252–2261, 2019.
[14] Christoph Lassner, Javier Romero, Martin Kiefel, Federica
Bogo, Michael J Black, and Peter V Gehler. Unite the peo-
ple: Closing the loop between 3d and 2d human representa-
tions. In Proceedings of the IEEE conference on computer
vision and pattern recognition , pages 6050–6059, 2017.
[15] Jiefeng Li, Chao Xu, Zhicun Chen, Siyuan Bian, Lixin Yang,
and Cewu Lu. Hybrik: A hybrid analytical-neural inverse
kinematics solution for 3d human pose and shape estimation.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 3383–3393, 2021.
[16] Hongyi Liu and Lihui Wang. Human motion prediction for
human-robot collaboration. Journal of Manufacturing Sys-
tems, 44:287–294, 2017.
[17] Diogo C Luvizon, Hedi Tabia, and David Picard. Human
pose regression by combining indirect part detection and
contextual information. Computers & Graphics , 85:15–22,
2019.
[18] Angel Mart ´ınez-Gonz ´alez, Michael Villamizar, Olivier
Can´evet, and Jean-Marc Odobez. Real-time convolutional
networks for depth-based human pose estimation. In 2018
IEEE/RSJ International Conference on Intelligent Robots
and Systems (IROS) , pages 41–47. IEEE, 2018.
[19] Mohamed Omran, Christoph Lassner, Gerard Pons-Moll, Pe-
ter Gehler, and Bernt Schiele. Neural body fitting: Unify-
ing deep learning and model based human pose and shape
estimation. In 2018 international conference on 3D vision
(3DV) , pages 484–494. IEEE, 2018.
[20] Georgios Pavlakos, Luyang Zhu, Xiaowei Zhou, and Kostas
Daniilidis. Learning to estimate 3d human pose and shape
from a single color image. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition , pages
459–468, 2018.
[21] Leonid Pishchulin, Eldar Insafutdinov, Siyu Tang, Bjoern
Andres, Mykhaylo Andriluka, Peter V Gehler, and Bernt
Schiele. Deepcut: Joint subset partition and labeling for
multi person pose estimation. In Proceedings of the IEEE
conference on computer vision and pattern recognition ,
pages 4929–4937, 2016.
[22] Istv ´an S ´ar´andi, Timm Linder, Kai O Arras, and Bastian
Leibe. How robust is 3d human pose estimation to occlu-
sion? arXiv preprint arXiv:1808.09316 , 2018.
[23] Ke Sun, Cuiling Lan, Junliang Xing, Wenjun Zeng, Dong
Liu, and Jingdong Wang. Human pose estimation using
global and local normalization. In Proceedings of the IEEE
international conference on computer vision , pages 5599–
5607, 2017.
[24] Xiao Sun, Jiaxiang Shang, Shuang Liang, and Yichen Wei.
Compositional human pose regression. In Proceedings of the
IEEE International Conference on Computer Vision , pages
2602–2611, 2017.[25] Yating Tian, Hongwen Zhang, Yebin Liu, and Limin Wang.
Recovering 3d human mesh from monocular images: A sur-
vey. arXiv preprint arXiv:2203.01923 , 2022.
[26] Hsiao-Yu Tung, Hsiao-Wei Tung, Ersin Yumer, and Katerina
Fragkiadaki. Self-supervised learning of motion capture. Ad-
vances in Neural Information Processing Systems , 30, 2017.
[27] Timo V on Marcard, Roberto Henschel, Michael J Black,
Bodo Rosenhahn, and Gerard Pons-Moll. Recovering ac-
curate 3d human pose in the wild using imus and a mov-
ing camera. In Proceedings of the European Conference on
Computer Vision (ECCV) , pages 601–617, 2018.
[28] Andrei Zanfir, Elisabeta Marinoiu, and Cristian Sminchis-
escu. Monocular 3d pose and shape estimation of mul-
tiple people in natural scenes-the importance of multiple
scene constraints. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition , pages 2148–
2157, 2018.
[29] Matthew D Zeiler and Rob Fergus. Visualizing and under-
standing convolutional networks. In European conference on
computer vision , pages 818–833. Springer, 2014.
[30] Feng Zhang, Xiatian Zhu, Hanbin Dai, Mao Ye, and Ce Zhu.
Distribution-aware coordinate representation for human pose
estimation. In Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition , pages 7093–7102,
2020.
[31] Tianshu Zhang, Buzhen Huang, and Yangang Wang. Object-
occluded human shape and pose estimation from a single
color image. In Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition , pages 7376–7385,
2020.
[32] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and
Yi Yang. Random erasing data augmentation. In Proceedings
of the AAAI conference on artificial intelligence , volume 34,
pages 13001–13008, 2020.