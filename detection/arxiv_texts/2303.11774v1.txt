arXiv:2303.11774v1  [cs.LG]  21 Mar 2023Exact Non-Oblivious Performance of
Rademacher Random Embeddings
Maciej Skorski[0000−0003−2997−7539]and Alessandro
Temperoni[0000−0003−0272−6596]
University of Luxembourg, 4365 Esch-sur-Alzette, Luxembo urg@uni.lu
Abstract. This paper revisits the performance of Rademacher random
projections, establishing novel statistical guarantees t hat are numerically
sharp and non-oblivious with respect to the input data.
More speciﬁcally, the central result is the Schur-concavit y property of
Rademacher random projections with respect to the inputs. T his oﬀers
a novel geometric perspective on the performance of random p rojec-
tions, while improving quantitatively on bounds from previ ous works.
As a corollary of this broader result, we obtained the improv ed perfor-
mance on data which is sparse or is distributed with small spr ead. This
non-oblivious analysis is a novelty compared to techniques from previ-
ous work, and bridges the frequently observed gap between th eory and
practise.
The main result uses an algebraic framework for proving Schu r-concavity
properties, which is a contribution of independentinteres t and an elegant
alternative to derivative-based criteria.
Keywords: Johnson-LindenstraussLemma ·RademacherRandomPro-
jections ·Schur-convexity
1 Introduction
1.1 Background and Related Work
The seminal result of Johnson and Lindenstrauss [ 25] quantiﬁes the amazing
performance of random linear maps as embeddings : they map input data into
a much smaller dimension (compression) while nearly maintaining the sca lar
products (almost isometrically). The strong quantitative guarantees , a distinc-
tive feature of this technique, enabled its application to a range of p roblems
and areas, including nearest-neighbour search [ 27], clustering [ 15,12,28], outlier
detection [ 5], ensemble learning [ 10], adversarial machine learning [ 35], feature
hashinginmachinelearning[ 24],numericallinearalgebra[ 30,12,11],convexopti-
mization [ 36], diﬀerential privacy [ 7], neuroscience [ 21] and numerous others; for
a comprehensive literature review we refer the reader to the rece nt survey [ 20].
The focus of this work is on numerical guarantees for the almost distance-
preserving property, which is formally stated as
/bardblΦx/bardbl2≈/bardblx/bardbl2with high probability , (1)2 M. Skorski and A. Temperoni
where an appropriately sampled matrix Φ∈Rm×nrepresents the projection of
ann-dimensional vector xtomdimensions ( m≪nis desired), and the relative
approximation error in ( 1) is referred to as distortion .
The long-line of research [ 19,22,2,3,29,14,26,24,32,33] has incrementally es-
tablished various guarantees for ( 1), in the form of distortion-conﬁdence trade-
oﬀs: while a small distortion ensures that the analytical task can be per formed
with a similar eﬀect over the embedded data, high conﬁdence guaran tees with
what probability it will happen. Yet the quantitative analysis of the pr operty (1)
has remained a diﬃcult challenge, resulting in complex proofs simpliﬁed m any
times [13,23], crude statistical bounds (for example, sparse variants have an
exponential gap with respect to the sharp no-go results [ 9]), and a lack of ﬁnite-
dimensional insights (bounds are input-oblivious which widens the gap between
theory predictions and empirical performance [ 34,18]).
This workaddressesthe aforementionedgap by revisitingthe most promising
construction of Rademacher random projections , which uses the following matrix
Φk,j=1√mrk,i, rk,i∼IID{−1,+1}. (2)
More speciﬁcally, this paper solves the following problem:
Give a precise, non-asymptotic, non-oblivious analysis of random pr ojec-
tions (2).
Asrecently[ 9]showed,Rademacherrandomprojectionsareasymptotically dimension-
optimal with exact constant ; this result improves upon a previous suboptimal
bound of Kane and Nelson [ 26]. The statistical performance of Rademacher
projections is superior to the sparse ones, as demonstrated emp irically in 1.
Furthermore, the theoretical bounds for Rademacher random p rojections are
much better than those available for sparse analogues [ 13]. The best, prior to
this paper, analysis of ( 2) is given by Achlioptas in [ 2]. It is worth noting that
RademacherprojectionsarealsosuperiortotheirGaussiancount erparts;indeed,
we know that they are dominated by the gaussian-based projectio ns [2]. The re-
lation of statistical performance and input structure has not bee n understood
in-depth yet; as for conceptually similar research, we note that re cent results
show that for sparse data one can improve the sparsity of random projections,
gaining in computing time [ 24,33].
1.2 Our Contribution
Our study of the stochastic behavior of ( 1) oﬀers the following novel contribu-
tions:
(a) non-oblivious insights , by quantifying the dependency on input spread-
ness. Loosely speaking, we prove that more spread-out input data lead toExact Non-Oblivious Performance of Rademacher Random Embe ddings 3
−1 0 110−310−210−1100
threshold ǫP{distortion > ǫ }p= 0.10
p= 0.25
p= 1.00
Fig.1: The distortion tail (empirical CCDF) w.r.t. the embedding dens ity pa-
rameter pwhich equals the fraction of non-zero elements in the matrix. Dense
Rademacher projections ( p= 1) are numerically superior to their sparse coun-
terparts ( p <1). The comparison was done on a toy dataset.
heavier tails in the distributed distortion. In particular1, we obtain the im-
proved performance on sparse input data .
(b) Schur-concavity framework , used to provide the missing geometric in-
tuitions for the performance of random projections.
(c) numerically optimal bounds , which precisely capture the extreme be-
havior and cannot be improved by any constant.
(d) benchmarking against previous bounds, both theoretically and empiri-
cally. By means of distortion, we measured how high-dimensional vec tors
are projected with diﬀerent density to prove the strength of our bounds.
1.3 Organization
The remainderofthe paper is organizedasfollows: Section 2 introduces notation
and technical notions used throughout the paper; then Section 3 discusses and
benchmarks novel results of this paper, and Section 4 concludes the work. Parts
of the theoretical analysis appear in the Appendix A.
2 Preliminaries
We start by recalling some basic concepts from probability theory, a lgebra, com-
binatorics and optimization.
Throughout the paper we work basic probability distributions: norm al, bi-
nomial, and Rademacher. Given a vector xwe denote by/bardblx/bardblits euclidean norm
1We can think of sparse input as being the extreme opposite of t he ”well-spread”
property.4 M. Skorski and A. Temperoni
0.2 0 .4 0 .6 0 .8 110−310−210−1100
embedding density pquadratic chaos momentsparsity = 0
sparsity = 0.2
sparsity = 0.3
sparsity = 0.4
sparsity = 0.5
Fig.2: Data-aware insights. We measured the theoretical moment s against input
data sparsity . The plot shows that increased sparsity in the input data leads to
smaller values of the moments. This result becomes more evident whe n related
to the embeddings density parameter p.
andby/bardblx/bardbl0the numberofitsnon-zerocomponents;wealsosaythat xisℓ-sparse
withℓ=/bardblx/bardbl0, for example x=/parenleftbig1
4,1
4,1
4,0/parenrightbig
is 3-sparse.
Partitions represent a positive integer qas a sum of positive integers. A non-
decreasingandnon-negativesequence λisapartitionof q,denotedas λ⊢q,when/summationtext
iλi=q; in thefrequency notation distinct parts are assigned frequencies, so
thatλ= 1f1...qfqwhere/summationtext
iifi=q. For example, we have λ= 122 = (2,1,1)⊢
4.
Monomial symmetric functions for a given partition λ= (λ1,...,λ k)⊢qare
deﬁned as the sum of all distinct monomials with exponent λ, that is mλ(x) =/summationtext
i1,...,ikxλ1
i1···xλk
ikwherei1,...,ikand monomials xλ1
i1···xλk
ikare distinct. For
example, we have m(2,1,1,1)(x1,x2,x3,x4) =x2
1x2x3x4+x1x2
2x3x4+x1x2x2
3x4+
x1x2x3x2
4.
Elementary symmetric functions aredeﬁnedas ek(x) =/summationtext
i1<...<i kxi1xi2···xik.
Both monomial and elementary polynomials are a base of symmetric po lynomi-
als. For more details, we refer to the monographs [ 4].
Themajorization order onn-dimensional vectors is deﬁned as follows: we
say that xis dominated by y, denoted by x≺y, if for their non-decreasing
rearrangements ( x↓
i) and (y↓
i) we have the inequality/summationtextk
i=1x↓
i/greaterorequalslant/summationtextk
i=1y↓
ifor
k= 1,...,nwith equalitywhen k=n;equivalently, xcanbeproducedfrom yby
a sequence of Robin-Hood operations which replace xi> xjbyxi←xi−ǫ,xj←
xj+ǫforǫ∈/parenleftBig
0,xi−xj
2/parenrightBig
.Intuitively, xbeingdominatedby ymeansthat xismore
spread-out/dispersed than y. For example, we have/parenleftbig1
4,1
4,1
4,1
4/parenrightbig
≺/parenleftbig1
3,1
3,0,1
3/parenrightbig
(it
takes 3 Robin-Hood transfers).Exact Non-Oblivious Performance of Rademacher Random Embe ddings 5
TheSchur-convexity of a function f:Rn→Ris the following property:
x≺yimpliesf(x)/lessorequalslantf(y); we speak of Schur-concavity when the inequality is
reversed.Schur-convexorSchur-concavefunctionsarenece ssarysymmetric;sym-
metric function is Schur-convex if/parenleftBig
∂f
∂xi−∂f
∂xj/parenrightBig
(xi−xj)/greaterorequalslant0 (Schur-Ostrowski
criterion). For example, power sums/summationtext
ixq
iforq/greaterorequalslant1 are Schur-convex. For more
onmajorization andSchur-convexity we refer to [ 6,31].
We deﬁne the moment domination of a random variable YoverX, denoted
asX≺mY, by requiring EXq/lessorequalslantEYqfor all positive integers q. In particular,
it implies that MGF of Ydominates the MGF of X, the majorization in the
Lorentz stochastic order [ 6].
3 Results
3.1 Main Result
In the followingresult,we providethe promisednumericallysharp,no n-oblivious
and geometrically insightful bounds for Rademacher random proje ctions. In the
(particularly interesting) case of sparse input, we obtain more exp licit formulas
involving binomial distributions.
Theorem 1 (Sharp Moment Bounds for Rademacher Random Projec -
tions). LetΦbe sampled according to the Rademacher scheme (2), and deﬁne
the distortion as
E(x)/defines/bardblΦx/bardbl2
/bardblx/bardbl2−1. (3)
Then the following holds true:
(a)E(x)has moments that are Schur-concave polynomials in (x2
i)
(b)E(x)is moment-dominated by E∗deﬁned as
E∗=1
mm/summationdisplay
i=1(Z2
i−1) (4)
whereZiare standardized binomial r.v.s.:
Zi∼IIDB−EB/radicalbig
Var[B], B∼Binom/parenleftbigg
/bardblx/bardbl0,1
2/parenrightbigg
. (5)
Equivalently,
EE(x)q/lessorequalslantEEq
∗ (6)
holds for q= 2,3,...with equality when all components of the input xare
equal.
We brieﬂy overview the proof of Theorem 1 (seeFigure 3): it starts by a
reduction to the dimension m= 1, and writing the distortion as a Rademacher
chaos of order 2; we then ﬁnd extreme values of its moments geome trically,
by means of Schur optimization . Finally, these extreme values can be found
explicitly and eﬃciently by linking them to binomial moments.6 M. Skorski and A. Temperoni
Distortion E(x) =/bardblΦx/bardbl2
2−/bardblx/bardbl2
2
Distortion E(x) =/bardblΦx/bardbl2
2−/bardblx/bardbl2
2form= 1
Rademacher chaos: E(x) =/summationtext
i/negationslash=jxixjrirjform= 1
Flat vectors xyield heaviest moments/tail bounds
Explicit formula for Rademacher chaos on ﬂat vectorsaveraging IID
rewritting
Robin-Hood (Schur) optimization
combinatorial identities
Fig.3: The proof roadmap for Theorem 1 .
3.2 Techniques: Proving Schur Convexity
We present a useful framework for proving Schur convexity prop erties. It makes
repeateduseoffewauxiliaryfactstoeventuallyreducethetaskt oa2-dimensional
problem. This is often easier than the classical approach of evaluat ing derivative
tests.
Lemma 1. The class of non-negative Schur-convex(or concave) functions forms
a semi-ring.
Lemma 2. A multivariate function is Schur-convex (respectively, Sc hur-concave)
if and only if it is symmetric and Schur-convex (respectivel y, Schur-concave) with
respect to each pair of variables.
To demonstrate the usefulness of these facts, we sketch an alte rnative proof
of a reﬁned version of celebrated Khintchine’s Inequality, due to Ef ron. This
reﬁnement plays an important role in statistics, namely in proving pro perties of
popular Student-t tests.
Corollary 1 (Reﬁned Khintchine Inequality [ 17]).The mapping
x→E(/summationdisplay
ixiri)q
is a Schur-concave function of (x2
i). As a consequence for σ=/bardblx/bardbl2we have
E(n/summationdisplay
i=1xiri)q/lessorequalslantE/parenleftBigg
σ√nn/summationdisplay
i=1ri/parenrightBiggq
/lessorequalslantENorm(0,σ2)q.Exact Non-Oblivious Performance of Rademacher Random Embe ddings 7
Proof.The symmetry with respect to ( xi) is obvious. Applying the multino-
mial expansion to (/summationtext
ixiri)q, taking the expectation and using the symmetry
of Rademacher random variables, we conclude that E(/summationtext
ixiri)qis polynomial
in variables ( x2
i). ByLemma 2 , it suﬃces to prove the Schur-concavity property
forx2
1,x2
2. By the binomial formula and the independence of r1,r2from (ri)i>2,
we see that E(/summationtext
ixiri)q=/summationtext
k/parenleftbigq
k/parenrightbig
E(/summationtext
i>2xiri)q−k·E(x1r1+x2r2)kis a combi-
nation of expressions E(x1r1+x2r2)kwith coeﬃcients ck=/parenleftbigq
k/parenrightbig
E(/summationtext
i>2xiri)q−k
that are independent of x1,x2and non-negative due to the symmetry of ri. By
Lemma 1 , it suﬃces to prove that Fk/definesE(x1r1+x2r2)kis a Schur-concave
function of x2
1,x2
2. Deﬁne Gk/definesE(x1r1+x2r2)kx1x2r1r2. By (x1r1+x2r2)k=
(x1r1+x2r2)k−2(x2
1+x2
2+ 2x1x2r1r2) we have Fk= (x2
1+x2
2)Fk−2+ 2Gk−2
andGk= (x2
1+x2
2)Gk−2+2x2
1x2
2Fk−2. Sincex2
1+x2
2andx2
1x2
2are both Schur-
concave in x2
1,x2
2, the Schur-concavity property of Fk,Gkis proven when it is
proven for k:=k−2. By mathematical induction, it suﬃces to realize that
F0= 1,F1= 0,G1= 1,G2=x2
1x2
2are Schur-concave in x2
1,x2
2.
Let1nbe the vector of nones. The ﬁrst inequality follows then by/summationtextn
i=1x2
i
n·
1n≺(x2
1,...,x2
n), and is clearly sharp. Since1
n+11n+1≺1
n1nand the Schur-
concavity implies that E(/summationtextn
i=1ri/√n)qincreases with n, the second inequality
follows by the CLT.
3.3 Techniques: Rademacher Chaoses
Of independent interests are the techniques used in this work. The ﬁrst result
analyzes the quadratic Rademacherchaosgeometrically. It is similar in the spirit
of the results of Efron [ 17] and Eaton [ 16], which however concern only a ﬁrst-
order Rademacher chaos.
Theorem 2 (Schur-concavity of Rademacher chaoses). Let(ri)be a
sequence of independent Rademacher random variables. Then the Rademacher
chaos moment
Rq(x)/definesE
/summationdisplay
i/ne}ationslash=jxixjrirj
q
(7)
is a Schur-concave function of (x2
i)for every positive integer q.
The second result is a recipe for explicitly computing the extreme mom ent
values:
Theorem 3 (Extreme moments of Rademacher Chaos). For any xand
K=/bardblx/bardbl0the following holds:
Rq(x)/lessorequalslantRq(x∗), x∗=/parenleftbigg/bardblx/bardbl2√
K,...,/bardblx/bardbl2√
K/parenrightbigg
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
Ktimes, (8)8 M. Skorski and A. Temperoni
Author Result
[9]maxxP[|E(x)|> ǫ]/greaterorequalslant2exp/parenleftBig
−mǫ2(1+o(1))
4)/parenrightBig
whenm≫ǫ−2,n≫1
[1]P[|E(x)|> ǫ]/lessorequalslant2exp/parenleftBig
−mǫ2
4/parenleftbig
1−2
3ǫ/parenrightbig/parenrightBig
this paper E(x)≺m/summationtextm
i=1Z2
i−1
m,Zi∼IIDB−EB√
Var[B, B∼Binom/parenleftbig
/bardblx/bardbl0,1
2/parenrightbig
Table 1: Bounds from this work ( Theorem 1 ) compared with the best prior
bounds [ 1] and the sharp no-go results [ 9]. Our bounds imply those from prior
work by ”normal majorization” arguments (see the supplementar y material).
and furthermore the explicit value of this bound equals
Rq(x∗) =/bardblx/bardbl2q
2·E¯B(¯B2−1)q, (9)
where¯B=B−K/2√
K/4standardizes the symmetric binomial distribution with K
trialsB.
3.4 Numerical Comparison
Thepresentedresultis numerically optimal andcaptures input sparsity .Itshould
be compared against the bounds from [ 1] and the no-go result from [ 9], as il-
lustrated in Table 1. To see that our bounds are better than those in [ 1], it
suﬃces to use the Gaussian majorization argument to obtain a weak er bound
E(x)≺m/summationtextm
i=1(N2
i−1)
mwhereNiare independent standard normal random vari-
ables, and use known sub-gamma tail bounds for chi-square distrib utions (for
example, those developed in the monograph on concentration inequ alities [8]).
To validate our ﬁndings, we performed the following experiments on b oth
synthetic and real-world datasets.
Synthetic dataset Figure 4 andFigure 5 demonstrate numerical improve-
ments. The input sparsity is the key: random projections are seen less distorted
when input data is sparse .
Real-world datasets Figure 6 andFigure 7 show our ﬁndings. The results
are validated with experiments performed on datasets from the Su iteSparse Ma-
trix Collection (formerly the University of Florida Sparse Matrix Collec tion)
available at https://sparse.tamu.edu/ . For these experiments, we selected
matrices from machine learning datasets.
4 Conclusions
WerevisitedtheperformanceofRademacherrandomprojections ,connectingthe
statisticalguaranteeswith the input structure:forspreadnes sand,a specialcase,Exact Non-Oblivious Performance of Rademacher Random Embe ddings 9
2 5 10 15 20100102104106108
/bardblx/bardbl0maxxRq(x)q= 4
q= 6
q= 8
q= 10
Fig.4: The more spread-out the input (controlled by sparsity /bardblx/bardbl0), the more
distorted the projected output (captured by Rq(x), the Rademacher chaos mo-
ment). Utilizing the input dispersion improves probability bounds by or ders of
magnitude. Note: for normalization purposes, we assume /bardblx/bardbl2= 1.
5·10−20.1 0 .15 0 .2 0 .2510−710−510−310−1
ǫbound on P[|E(x)|> ǫ]new(ℓ= 10)
new(ℓ= 20)
new(ℓ= 50)
old bounds
Fig.5: Capturing input-sparsity ( ℓ=/bardblx/bardbl0) improvesthe bounds on Rademacher
random projections, as demonstrated by distortion probability ta ils.
sparsity. The main result of this paper proves Schur-concavity pr operties, which
makesthe boundsnumericallysharpand dataaware(non-obliviuos) while giving
a geometric perspective to the performance of the projections. We benchmarked
our bounds both theoretically and empirically by measuring the distor tion of the
projected vectors against the original input data. As a result, de nse projections
are preferred, and they work incredibly well with sparse input data . We believe10 M. Skorski and A. Temperoni
0.1 0.2 0.4 0.6 0.8 110−0.1100100.1
embedding density pdistortionmnist
optdigits
semeion
usps
Fig.6: The distortion measured w.r.t. the density of the embeddings shows that
sparse data result in better bounds and proves that Rademacher projections are
superior to sparse ones.
0 2 4 6 810−310−210−1100
ǫbound on P[|E(x)|> ǫ]mnist
plantstexture
micromass
Ecoli
Glass
Fig.7: Measuring the distortion tail probability on real-world datase ts conﬁrms
our theoretical ﬁndings: capturing the input-sparsity improves t he bounds on
Rademacher random projections. The datasets, from left to righ t, are displayed
from the most sparse (mnist) to the least one (Glass).
that our ﬁndings are of broader interest for a variety of statistic al-inference
applications.
References
1. Achlioptas, D.: Database-friendly random projections. In: Proceedings of the twen-
tieth ACM SIGMOD-SIGACT-SIGARTsymposium on Principles of database sys-Exact Non-Oblivious Performance of Rademacher Random Embe ddings 11
tems. pp. 274–281 (2001)
2. Achlioptas, D.: Database-friendly random projections: Johnson-lindenstrauss with
binary coins. Journal of computer and System Sciences 66(4), 671–687 (2003)
3. Ailon, N., Chazelle, B.: Approximate nearest neighbors a nd the fast johnson-
lindenstrauss transform. In: Proceedings of the thirty-ei ghth annual ACM sym-
posium on Theory of computing. pp. 557–563 (2006)
4. Alexandersson, P.: The symmetric functions catalog (202 0),
https://www.symmetricfunctions.com
5. Aouf, M., Park, L.A.: Approximate document outlier detec tion using random spec-
tral projection. In: Australasian Joint Conference on Arti ﬁcial Intelligence. pp.
579–590. Springer (2012)
6. Arnold, B., Sarabia, J.: Majorization and the Lorenz Orde r with Ap-
plications in Applied Mathematics and Economics. Statisti cs for So-
cial and Behavioral Sciences, Springer International Publ ishing (2018),
https://books.google.pl/books?id=ibhmDwAAQBAJ
7. Blocki, J., Blum, A., Datta, A., Sheﬀet, O.: The johnson-l indenstrauss transform
itself preserves diﬀerential privacy. In: 2012 IEEE 53rd An nual Symposium on
Foundations of Computer Science. pp. 410–419. IEEE (2012)
8. Boucheron, S., Lugosi, G., Bousquet, O.: Concentration i nequalities. In: Summer
school on machine learning. pp. 208–240. Springer (2003)
9. Burr, M., Gao, S., Knoll, F.: Optimal bounds for johnson-l indenstrauss transfor-
mations. The Journal of Machine Learning Research 19(1), 2920–2941 (2018)
10. Cannings, T.I., Samworth, R.J.: Random-projection ens emble classiﬁcation. Jour-
nal of the Royal Statistical Society: Series B (Statistical Methodology) 79(4), 959–
1035 (2017)
11. Clarkson, K.L., Woodruﬀ, D.P.: Low-rank approximation and regression in input
sparsity time. Journal of the ACM (JACM) 63(6), 1–45 (2017)
12. Cohen, M.B., Elder, S., Musco, C., Musco, C., Persu, M.: D imensionality reduction
for k-means clustering and low rank approximation. In: Proc eedings of the forty-
seventh annual ACM symposium on Theory of computing. pp. 163 –172 (2015)
13. Cohen, M.B., Jayram, T., Nelson, J.: Simple analyses of t he sparse johnson-
lindenstrauss transform. In: 1st Symposium on Simplicity i n Algorithms (SOSA
2018). Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik (2018)
14. Dasgupta, A., Kumar, R., Sarl´ os, T.: A sparse johnson: L indenstrauss transform.
In: Proceedings of the forty-second ACM symposium on Theory of computing. pp.
341–350 (2010)
15. Dasgupta, S.: Learning mixtures of gaussians. In: 40th A nnual Symposium on
FoundationsofComputerScience(Cat.No.99CB37039). pp.6 34–644. IEEE(1999)
16. Eaton, M.L.: A note on symmetric bernoulli random variab les. The annals of math-
ematical statistics 41(4), 1223–1226 (1970)
17. Efron, B.: Student’s t-test under non-normal condition s. Tech. rep., HARVARD
UNIV CAMBRIDGE MA DEPT OF STATISTICS (1968)
18. Fedoruk, J., Schmuland, B., Johnson, J., Heo, G.: Dimens ionality reduction via
the johnson–lindenstrauss lemma: theoretical and empiric al bounds on embedding
dimension. The Journal of Supercomputing 74(8), 3933–3949 (2018)
19. Frankl, P., Maehara, H.: The johnson-lindenstrauss lem ma and the sphericity of
some graphs. Journal of Combinatorial Theory, Series B 44(3), 355–362 (1988)
20. Freksen,C.B.: Anintroduction tojohnson-lindenstrau ss transforms. arXiv preprint
arXiv:2103.00564 (2021)12 M. Skorski and A. Temperoni
21. Ganguli, S., Sompolinsky, H.: Compressed sensing, spar sity, and dimensionality in
neuronal information processing and data analysis. Annual review of neuroscience
35, 485–508 (2012)
22. Indyk, P., Motwani, R.: Approximate nearest neighbors: towards removing the
curse of dimensionality. In: Proceedings of the thirtieth a nnual ACM symposium
on Theory of computing. pp. 604–613 (1998)
23. Jagadeesan, M.: Simple analysis of sparse, sign-consis tent jl. In: Approxima-
tion,Randomization,andCombinatorial Optimization. Alg orithmsandTechniques
(APPROX/RANDOM 2019). Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik
(2019)
24. Jagadeesan, M.: Understanding sparse JL for feature has hing. In: NeurIPS. pp.
15177–15187 (2019), (Full version: https://arxiv.org/pdf/1903.03605.pdf )
25. Johnson, W.B., Lindenstrauss, J.: Extensions of lipsch itz mappings into a hilbert
space. Contemporary mathematics 26(189-206), 1 (1984)
26. Kane, D.M., Nelson, J.: Sparser johnson-lindenstrauss transforms. Journal of the
ACM (JACM) 61(1), 1–23 (2014)
27. Kleinberg,J.M.: Twoalgorithms fornearest-neighbors earchinhighdimensions.In:
Proceedings of the twenty-ninth annual ACM symposium on The ory of computing.
pp. 599–608 (1997)
28. Makarychev, K., Makarychev, Y., Razenshteyn, I.: Perfo rmance of johnson-
lindenstrauss transform for k-means and k-medians cluster ing. In: Proceedings of
the 51st Annual ACM SIGACT Symposium on Theory of Computing. pp. 1027–
1038 (2019)
29. Matouˇ sek, J.: On variants of the johnson–lindenstraus s lemma. Random Structures
& Algorithms 33(2), 142–156 (2008)
30. Sarlos, T.: Improved approximation algorithms for larg e matrices via random pro-
jections. In: 2006 47th annual IEEE symposium on foundation s ofcomputer science
(FOCS’06). pp. 143–152. IEEE (2006)
31. Shi, H., Technology, H.: Schur-Convex Functions and Ine qualities: Volume 1:
Concepts, Properties, and Applications in Symmetric Funct ion Inequalities. De
Gruyter (2019), https://books.google.at/books?id=VrztDwAAQBAJ
32. Skorski, M.: Johnson-lindenstrauss transforms with be st conﬁdence. In: Conference
on Learning Theory. pp. 3989–4007. PMLR (2021)
33. Skorski, M., Alessandro, T., Martin, T.: Robust and prov able guarantees for sparse
random embeddings. In: Advances in Knowledge Discovery and Data Mining. pp.
211–223 (2022)
34. Venkatasubramanian, S., Wang, Q.: The johnson-lindens trauss transform: an em-
pirical study. In: 2011 Proceedings of the Thirteenth Works hop on Algorithm En-
gineering and Experiments (ALENEX). pp. 164–173. SIAM (201 1)
35. Vinh, N.X., Erfani, S., Paisitkriangkrai, S., Bailey, J ., Leckie, C., Ramamohanarao,
K.: Training robust models using random projection. In: 201 6 23rd International
Conference on Pattern Recognition (ICPR). pp. 531–536. IEE E (2016)
36. Zhang, L., Mahdavi, M., Jin, R., Yang, T., Zhu, S.: Recove ring the optimal solu-
tion by dual random projection. In: Conference on Learning T heory. pp. 135–157.
PMLR (2013)Exact Non-Oblivious Performance of Rademacher Random Embe ddings 13
A Proofs
A.1 Proof of Lemma 1
Consider two non-negativefunctions f,gand inputs x≺y. Consider the identity
f(y)g(y)−f(x)g(x) = (f(y)−f(x))·g(y) +f(x)·(g(y)−g(x)).(10)
Iff,gare Schur-convex then f(y)−g(x)/greaterorequalslant0 andg(y)−g(x)/greaterorequalslant0 and the whole
expression is non-negative when f,gare non-negative. This shows that f·gis
also Schur-convex. The claim for Schur-concave functions follows analogously
(the expression is then non-positive).
A.2 Proof of Lemma 2
The proof follows from the fact that xis dominated by yif and only if xcan
be produced from yby a sequence of Robin-Hood operations , and the fact that
Robin-Hood operations change only two ﬁxed components of vecto rs.
A.3 Proof of Theorem 2
Proof.Note that Rqis a polynomial in x2
iwith integer coeﬃcients, and thus a
well-deﬁnedfunction of( x2
i). Thisfollowsbyapplyingthe multinomialexpansion
and noticing that monomials with odd exponents have expectation ze ro due
to the symmetry of Rademacher distribution. Rqis obviously symmetric. By
Lemma 2 it now suﬃces to validate the Schur-concativity for x2
1,x2
2and any
ﬁxed choice of ( xi)j>2. Deﬁne the following expressions
P=/summationdisplay
i/ne}ationslash∈{1,2}xiri
R=/summationdisplay
i,j/ne}ationslash∈{1,2}xixjrirj,(11)
then our task is to prove the Schur-concativity of the function
Rq/definesE(P(x1r1+x2r2)+x1x2r1r2+R)q, (12)
with respect to x2
1,x2
2.
By the multinomial expansion we ﬁnd that
Rq/defines/summationdisplay
q1+q2+q3=q/parenleftbiggq
q1,q2,q3/parenrightbigg/bracketleftbigg
E[Pq1Rq3]E[(x1r1+x2r2)q1(x1x2r1r2)q2]/bracketrightbigg
,
(13)
where we used the independence of r1,r2on (ri)i>2and thus also on P,R.
Observethat Pq1Rq3is,bydeﬁnitionandourassumption xi/greaterorequalslant0,apolynomialin14 M. Skorski and A. Temperoni
symmetric random variables riwith non-negative coeﬃcients. This observation
shows that
E[Pq1Rq3]/greaterorequalslant0, (14)
and by Lemma 1 it suﬃces to prove that
F/definesE[(x1r1+x2r2)q1(x1x2r1r2)q2] (15)
is Schur-concave as a function of x2
1,x2
2for any non-negative integers q1,q2.
To see that Fis indeed a well-deﬁned function of x2
1,x2
2, note that it equals
the expectation of a polynomial in the symmetric random variables yi=xiri;
thusonlymonomialswitheven-degreescontribute,andtheresult is apolynomial
iny2
i=x2
i. In fact, Fequals the sum of even-degree monomials in the expanded
polynomial ( x1+x2)q1(x1x2)q2.
We next observe that
F=/braceleftBigg
(x1x2)q2E[(x1r1+x2r2)q1] q2even
(x1x2)q2−1E[(x1r1+x2r2)q1x1x2r1r2]q2odd.(16)
Note that x1x2is Schur-concave in non-negative x1,x2; indeed, the identity
(x1+ǫ)(x2−ǫ) =x1x2+ǫ(x2−x1−ǫ) shows that Robin-Hood transfers increase
the value. By Lemma 1 we conclude that ( x1x2)kis Schur concave in x2
1,x2
2for
non-negative even k. Thus, by Equation (16) and Lemma 1 we conclude that it
suﬃces to consider the case q2= 1, that is, to prove the Schur-concavity of the
following two functions:
Gk/definesE/bracketleftbig
(x1r1+x2r2)k/bracketrightbig
(17)
Hk/definesE/bracketleftbig
(x1r1+x2r2)kx1x2r1r2/bracketrightbig
. (18)
with respect to x2
1,x2
2for any non-negative integer k.
Using the identity ( x1r1+x2r2)k= (x1r1+x2r2)k−2(x2
1+x2
2+2x1x2r1r2),
we ﬁnd the following recurrence relation
Gk= (x2
1+x2
2)Gk−2+2Hk−2 (19)
Hk= 2x2
1x2
2Gk−2+(x2
1+x2
2)Hk−1, (20)
valid for k/greaterorequalslant2. Sincex2
1+x2
2andx2
1x2
2are Schur-concave as functions of x2
1,x2
2,
by Lemma 1 the concavity property proven for k−2 implies that it is valid also
fork. By induction, it suﬃces to verify the case k= 0 and k= 1. But we see
that
G0= 1
G1= 0
H0= 1
H1= 2x2
1x2
2(21)
are all Schur-concave as functions of x2
1,x2
2. This completes the proof.Exact Non-Oblivious Performance of Rademacher Random Embe ddings 15
A.4 Proof of Theorem 3
Without loss of generality, we assume that /bardblx/bardbl2= 1. From Theorem 2 and the
fact that ( x2
i) majorizes ( x∗
i2) we obtain
max
/bardblx/bardbl0/lessorequalslantKE
/summationdisplay
i<jxixirirj
q
=E
/summationdisplay
i<jx∗
ix∗
irirj
q
=E
1
K/summationdisplay
1/lessorequalslanti<j/lessorequalslantKrirj
q
,
(22)
Observe that ri= 1−2biwhere (bi) is a sequence of independent Bernoulli
random variables with parameter1
2. Therefore,
E/parenleftBiggK/summationdisplay
i=1ri/parenrightBiggq
=
=(a)/summationdisplay
k∈Zkq·P/braceleftBiggK/summationdisplay
i=1ri=k/bracerightBigg
=(b)/summationdisplay
kkq·P/braceleftBiggK/summationdisplay
i=1bi=K−k
2/bracerightBigg
=(c)K/summationdisplay
i=0(K−2i)q·P/braceleftbigg
Binom/parenleftbigg
K,1
2/parenrightbigg
=i/bracerightbigg
=(d)1
2KK/summationdisplay
i=0/parenleftbiggK
i/parenrightbigg
(K−2i)q
=(e)1
2K/summationdisplay
i/parenleftbiggK
i/parenrightbigg
(−K+2i)q,(23)
where in (a) we use the fact that/summationtext
iritakes integer values, (b) follows by the
identity ri= 1−2bi, (c) follows by Binom(K,1/2)∼/summationtextK
i=1bi, (d) uses the
explicit formula on the binomial probability mass function, and ﬁnally in ( e) we
substitute i:=K−iand use the symmetry of binomial coeﬃcients/parenleftbigK
i/parenrightbig
=/parenleftbigK
K−i/parenrightbig
.16 M. Skorski and A. Temperoni
Using the above formula, we further calculate
E
/summationdisplay
1/lessorequalslanti/ne}ationslash=j/lessorequalslantKrirj
q
=
=(a)E
/parenleftBiggK/summationdisplay
i=1ri/parenrightBigg2
−K/summationdisplay
i=1r2
i
q
=(b)/summationdisplay
j/parenleftbiggq
j/parenrightbigg
(−K)q−jE/parenleftBiggK/summationdisplay
i=1ri/parenrightBigg2j
=(c)1
2K/summationdisplay
i,j/parenleftbiggq
j/parenrightbigg/parenleftbiggK
i/parenrightbigg
(−K+2i)2j(−K)q−j
=(d)(−K)q
2K/summationdisplay
i/parenleftbiggK
i/parenrightbigg/parenleftbigg
1−(−K+2i)2
K/parenrightbiggq
,(24)
where (a) follows by the square sum completion, (b) follows by the bin omial
formula and r2
i= 1, (c) follows directly by Equation (23) , and (d) is obtained
by algebraic rearrangements.
Inserting Equation (23) intoEquation (22) , we arrive at
max
x:/bardblx/bardbl0/lessorequalslantKE
/summationdisplay
1/lessorequalslanti/ne}ationslash=j/lessorequalslantKxixjrirj
q
=1
2KK/summationdisplay
i=0/parenleftbiggK
i/parenrightbigg/parenleftbigg(−K+2i)2
K−1/parenrightbiggq
.(25)
To simplify further, let Z∼Binom(K,1
2)−K
2√
K
4be the standardization of the
symmetric binomial distribution. Denoting i∼Binom/parenleftbig
K,1
2/parenrightbig
we have Z2∼
(i−K
2)2
K
4=(−K+2i)2
K, and we can rewrite Equation (25) as follows:
max
x:/bardblx/bardbl0/lessorequalslantKE
/summationdisplay
1/lessorequalslanti/ne}ationslash=j/lessorequalslantKxixjrirj
q
=EZ/parenleftbig
Z2−1/parenrightbigq, (26)
which ﬁnishes the proof.
A.5 Proof of Theorem 1
We have to prove that for the distortion E(·) deﬁned as in
E(x)/defines/bardblΦx/bardbl2
/bardblx/bardbl2−1. (27)
the following inequality holds true:
E(x)/lessorequalslantE(y),(y2
i)≺(x2
i). (28)Exact Non-Oblivious Performance of Rademacher Random Embe ddings 17
The proof goes through several reduction steps until Schur-co ncavity of few
simple functions.
We ﬁrst observe that it suﬃces to prove that the moments of the e xpression
x→/bardblΦx/bardbl2−/bardblx/bardbl2, (29)
are Schur-concavity with respect to ( x2
i). Indeed, since ( y2
i)≺(x2
i) implies
/bardblx/bardbl2=/summationtext
ix2
i=/summationtext
iy2
i=/bardbly/bardbl2we have EE(x)q/lessorequalslantEE(y)qif and only if
E(/bardblΦx/bardbl2−/bardblx/bardbl2)q/lessorequalslantE(/bardblΦy/bardbl2−/bardbly/bardbl2)q, by the deﬁnition of E.
We ﬁrst prove that the distortion of m-dimensional projections is the average
ofmIID distortions of 1-D projections. Observe that
/bardblΦx/bardbl2−/bardblx/bardbl2=m/summationdisplay
k=1/parenleftbig
(Φkx)2−E(Φkx)2/parenrightbig
, (30)
whereΦkis thek-th row of Φ; this follows by E(Φkx)2=/summationtext
ix2
iVar[Φk,i] =
1
m/bardblx/bardbl2. Furthermore, the summands in ( 30) are independent and identically
distributed:
(Φkx)2−E(Φkx)2∼1
m/summationdisplay
i/ne}ationslash=jxixjrirj. (31)
Then we note that the Schur-concativity test can be done on the 1 -D case.
Thisfollowsbecause,duetothemultinomialexpansionappliedto Equation (31) ,
theq-th moment of m-dimensional distortion is a multivariate polynomial in 1-D
distortion moments of order k/lessorequalslantq, with non-negative coeﬃcients; the distortion
moments are themselves non-negative, and by Lemma 1 and Theore m 2 we
obtain the ﬁrst part of the theorem.
Finally, applying Theorem 3 proves the second part.