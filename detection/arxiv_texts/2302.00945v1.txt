arXiv:2302.00945v1  [math.DS]  2 Feb 2023A Renormalization Scheme for Semi-Regular Continued Fract ions
Niels Langeveld and David Ralston
February 3, 2023
1 Introduction
In this article we will study a renormalization scheme with which we ﬁnd a ll semi-regular continued fractions
of a number in a natural way. In Section 2we deﬁne two maps, ˆTslowandˆTfast: these maps are deﬁned for
(x,y)∈[0,1]2, wherexis the number for which a semi-regular continued fraction represen tation is developed
byˆTslowaccording to the parameter y. Importantly, the set of all possible semi-regular continued fract ion
representationsof xare bijectively constructed as the parameter yvaries (Theorem 4.1), making ˆTslowa natural
setup for discussing these representations of x. The map ˆTfastis a “sped-up” version of the map ˆTslow, and we
show that ˆTfastis ergodic with respect to a probability measure which is mutually absolu tely continuous with
Lebesgue measure on [0 ,1]2(Theorem 2.6). In contrast, ˆTslowpreserves no such measure, but does preserve an
inﬁnite,σ-ﬁnite measure mutually absolutely continuous with Lebesgue measu re (Corollary 2.7).
In Section 3we show that the maps ˆTslowandˆTfastapplied to the point ( x,y) can generate a sequence of
substitutions which generate a symbolic coding of the orbit of ywith respect to the intervals [0 ,1−x], [1−x,1].
These substitutions are shown to naturally relate to ﬁnding a seque nce of those n∈Z+such that−nxmod 1
best approximates yfor all−ixmod 1, 1≤i≤n(Lemma 3.3. Ergodicity of ˆTfastthen leads to statements
of a generic growth rate for this sequence (Corollary 3.8).
Finally, in Section 4, we highlight how our scheme can be used to generate semi-regular c ontinued fractions,
explicitly mentioning regular continued fractions [ DK02], backward continued fractions [ R´57],α-continued
fractions [ Nak81], a natural counterpart of α-continued fractions [ KLMM20 ], and Lehner continued fractions
[Leh94]. Ergodicity of ˆTfastand lack of ergodicity of ˆTslowalso leads to a statement regarding the generic
growth rate of denominators of convergents of xacross our parameterization of semi-regular continued fractions
(Theorem 4.2).
2 The maps ˆTslowandˆTfast
LetXbe the unit circle R/Z, but with all y∈Xhaving a “left” and “right” version y−andy+, which satisfy
y−< y+. Then we write X= [0+,1−]. We give Xthe topology generated by all open sets of the form ( a+,b−)
for 0≤a < b≤1. Forx∈R, letRxdenote rotation by x, i.e.Rx(y±) = (y+x)±mod 1. Observethat the sets
[0+,(1−x)−] and [(1−x)+,1−] form a partition of Xinto two disjoint compact sets. In studying properties of
Rx, there is no meaningful distinction between Rn
x(y+) andRn
x(y−) wheny /∈xZmod 1, so we identify such
points with one another. We will therefore suppress the notation r equired by X, simply considering y∈[0,1].
Intervals always begin at the right-sided version of a number and en d at the left-sided version if an endpoint is
somenxmod 1: e.g. [0 ,1−x] and [1−x,1] are disjoint under this convention.
Circle rotations are convenient to represent as the simplest nontr ivial type of interval exchange transforma-
tion, orIET, a bijective orientation-preserving piecewise isometry of the inter val [0,1]. Speciﬁcally, rotations
can be represented as exchanges of two intervals (a 2-IET). See Figure1for a presentation of Rxin this way.
Note that
Rx(y) =/braceleftBigg
y+x, y∈[0,1−x],
y+x−1, y∈[1−x,1].
0
01
11−x
xRx
Figure 1: Rotation by xpresented as a 2-IET.
1Next, let A= [1−x,1]. For any y∈A, deﬁne the return time ofyto beN(y), where
N(y) = min/braceleftbig
n∈Z+:Rn
x(y)∈A/bracerightbig
.
Theﬁrst-return map on Ais given by RN(y)
x(y). It is well-known that the ﬁrst-return map on Ahas exactly
two return times. Namely, for any y∈Awe either have N(y) =aorN(y) =a+ 1 where ais the largest
positive integer so that ax <1,i.e.a=⌊1/x⌋, the ﬁrst partial quotient of x. Fory∈[−(a+1)x,1] (modulo
one) we have N(y) =aand fory∈[1−x,−(a+1)x] we have N(y) =a+1; see Figure 2. The ﬁrst return map
is therefore another 2-IET given by
ˆRˆx(y) =/braceleftBigg
y+ ˆx, y∈[1−x,−(a+1)xmod 1],
y+ ˆx−1, y∈[−(a+1)xmod 1,1];
where ˆx= (a+1)x−1. This map is either a rotation by ( a+1)x−1 or rotation by 1 −ax: the two choices
are isomorphic, diﬀering only by a choice of orientation. We adopt the convention that the shorter return time
determines the orientation and rotation amount of the ﬁrst- return map . Our motivation for this choice is as
follows:
1−x1−x0
11
ax−(a+1)xmod 1
RN(y)
x
Figure 2: We construct the ﬁrst-return map on the interval [1 −x,1]. We obtain another 2-IET (i.e. rotation)
with two diﬀerent return times; a(solid lines) and a+1 (dotted lines).
Lemma 2.1. The ﬁrst-return map on A= [1−x,1], after rescaling and subject to the convention that shorter
return times dictate the choice of direction, is isomorphic to rotation by T(x), where
T(x) =1
x−/floorleftbigg1
x/floorrightbigg
is the Gauss map.
The map τ: [1−x,1]/ma√sto→[0,1]given by
τ(y) =1−y
x=T(x)−y
xmod 1 (1)
is an orientation-reversing isomorphism between the ﬁrst r eturn map on [1−x,1]and rotation by T(x)on[0,1].
Proof.We see in Figure 2that within A, the shorter return time is subtraction of (1 −ax). After rescaling by
1/xto present this map as acting on an interval of length one, we have s ubtraction of 1 /x−a=T(x). Since
the direction of rotation is also determined by this shorter return t ime, we reverse orientation and conclude
that the ﬁrst-return map may be presented as RT(x). Referring again to Figure 2, we see that we need τto be
a linear function which maps the interval [1 −x,1] to [0,1] with reversed orientation: this determines τ.
CertainlyLemma 2.1is well-knownregardingthe rotationamount ofthe induced map; we p resent it because
we will utilize the speciﬁc isomorphism τ.
What happens if we construct the ﬁrst-return map on [0 ,1−x] instead? We may reconsider rotation by x
in one direction as rotation by 1 −xin the other, so that by reversing orientation, now the interval [0 ,1−x] is
of length equal to the rotation amount. Applying Lemma 2.1then yields:
Corollary 2.2. The ﬁrst-return map on [0,1−x]is isomorphic to rotation by T(1−x), where the isomorphism
is given by
τ(y) =y
1−x. (2)
Note that in contrast to Lemma 2.1, in this scenario the isomorphism τis orientation-preserving. We
reversed orientation once to consider the original map Rxto instead be R1−x, and then the ﬁrst-return map
reversed orientation again by applying Lemma 2.1.
We present both ﬁrst-return maps simultaneously in Figure 3: on [1−x,1] we obtain rotation by T(x)
with orientation reversed, while on [0 ,1−x] we obtain rotation by T(1−x) with orientation preserved. Here
N′(y) = min{n∈Z+:Rn
x(y)∈[0,1−x]}.The lengths of the red and green arrows represent the rotation
20 1−x 1
0 1−x
1−x 1(1−x)T(1−x)
xT(x)RN(y)
xRN′(y)
x
Figure 3: The two diﬀerent ﬁrst-return maps, on [0 ,1−x] on the left and on [1 −x,1] on the right.
amount of this ﬁrst-return map (not yet rescaled to be in an interv al of length one), where the arrows begin
at the point which is mapped to the origin by the isomorphism τand point in the direction of the induced
rotation.
We can now deﬁne our ﬁrst new function: ˆTslowacts on the pair ( x,y), where xrepresentsa rotationamount
andy∈X. Depending on whether y∈[0,1−x] ory∈[1−x,1] we take the ﬁrst-return map on the appropriate
interval (either RN′(y)
xorRN(y)
x, respectively) along with the speciﬁed isomorphism τto obtain a new rotation
amount and point in the space X:
ˆTslow(x,y) =/braceleftBigg/parenleftBig
T(1−x),y
1−x/parenrightBig
y∈[0,1−x],
/parenleftbig
T(x),1−y
x/parenrightbig
y∈[1−x,1].(3)
For iterations of this map we present a useful visual aid. Let x0=xbe our rotation amount and y0=y∈
[0,1], as before. Generate the sequences {xn},{yn},{ˆan}as (xn,yn) =ˆTn
slow(x0,y0) and ˆanthe return time
used to deﬁne the n-th ﬁrst-return map. Then we may equivalently follow the algorithm g iven in Figure 4.
START
n= 0, x0=x, y0=y
Where is yn?
ˆan+1=⌊1/xn⌋
xn+1=T(xn)
yn+1=1−yn
xnˆan+1=⌊1/(1−xn)⌋
xn+1=T(1−xn)
yn+1=yn
1−xn
n←n+1yn∈[1−xn,1] yn∈[0,1−xn]
Figure 4: An informal graph algorithm showing the action of ˆTslow. The choice of green versus red arrows
aligns with the color scheme in Figure 3.
At ﬁrst glance the presentation in Figure 4does not add anything beyond the deﬁnition of ˆTslowin Equa-
tion (3). The two colored arrows, however, provide a clear way to distingu ish when ˆTslowacts on the ﬁrst
coordinate xnvia the Gauss map (the green arrow) or a modiﬁed version thereof ( the red arrow), and in both
cases forces us to compute the shorter return times ˆ a, which we will make use of later. The following lemma
is a technical necessity, but also introduces a critical link between t he mapˆTslowand well-known techniques in
the study of continued fractions:
Lemma 2.3. For any initial choice of (x0,y0), any execution of the algorithm in Figure 4must have inﬁnitely
many edges which are either green, or red with ˆan≥2; equivalently, no (x0,y0)enters into an inﬁnite loop of
following the red arrow with ˆan= 1.
Proof.The red edge involves computing
ˆan+1=/floorleftbigg1
1−xn/floorrightbigg
, x n+1=1
1−xn−ˆan+1.
3Suppose the regular continued fraction expansion of some xnis given by
xn=1
k1+1
k2+1
k3+...= [k1,k2,k3,...].
Then we see that
1−xn=/braceleftBigg
[k2+1,k3,...] (k1= 1),
[1,k1−1,k2,...] (k1/\e}atio\slash= 1).(4)
So ifk1= 1 and we follow the red edge
ˆa1=/floorleftbigg1
1−x/floorrightbigg
=k2+1≥2,
while ifk1≥2 and we follow the red edge
ˆa1=/floorleftbigg1
1−x/floorrightbigg
= 1,
we will have
xn+1=T(1−x) = [k1−1,k2,...].
It is therefore only possible to follow the red edge with ˆ an= 1 at most k1consecutive times.
Equation ( 4) gives us a relation with what are called singularizations and insertions of the standard con-
tinued fraction expansion of x. These terms will be more precisely deﬁned and elaborated upon in Se ction4.
We now construct a natural Markov partition for ˆTslow. The line y= 1−xis used to determine how ˆTslow
acts, and then one must determine either ⌊1/x⌋or⌊1/(1−x)⌋, so the trapezoids bounded for n= 1,2,3,...
either by
•1−x≤y≤1, 1/(n+1)≤x≤1/n, or
•0≤y≤1−x, (1−1/n)≤x≤(1−1/(n+1))
form exactly that partition we are seeking; see Figure 5.
The map ˆTslowmay be extended to act on the segments x= 0 and x= 1:
ˆTslow(0,y) = (0,y),ˆTslow(1,y) = (0,1−y).
As these segments have empty interior (in [0 ,1]2), they are included to complete the partition but will have no
eﬀect on our dynamical statements later.
We ask, then, if the map ˆTslowis ergodic with respect to a probability measure mutually absolutely co ntinu-
ouswithrespecttoLebesguemeasure. Observe,however,that forxclosetozero,wehave1 /(1−x)−1alsosmall;
the map ˆTslowacts “more and more like identity” the closer we get to x= 0. Since,d
dx(1/(1−x)−1) =1
(1−x)2,
forx= 0 any y∈[0,1] is an indiﬀerent ﬁxed point of the map ˆTslow. This observation suggests that ˆTslow
does not preserve any ﬁnitemeasures mutually absolutely continuous to Lebesgue measure. We will indeed
eventually show this result (Corollary 2.7), but for now we create a “sped-up” version of ˆTslowto avoid this
problem.
DeﬁneF={(x,y) :y≥1−xorx >1/2}andNF(x,y) = min{n≥0 :ˆTn
slow(x,y)∈F}. The fast map
is then given by ˆTfast(x,y) =ˆTNF(x,y)+1
slow(x,y). It follows from Lemma 2.3that this map is well-deﬁned: we
applyˆTslowuntil we either have y≥1−x(in which case we will follow the green edge) or until we have y≤1−x
but with x >1/2, in which case following the red edge will have ˆ a≥2. In other words, the map ˆTfastiterates
ˆTslowuntil entering into F, then applies ˆTslowone more time. Equivalently, all consecutive occurrences of ‘red
edge with ˆ a= 1’ under ˆTsloware absorbed into the next occurrence of ‘green edge, or red edg e with ˆa≥2.’
We begin the study of ˆTfastwith a lemma which describes the action of ˆTslowon the interval y∈[0,1−x]
forx <1/2.
Lemma 2.4. Withx <1/2ﬁxed,ˆTslowlinearly maps{x}×[0,1−x]onto{T(1−x)}×[0,1],T(1−x) =x/(1−x),
and⌊1/T(1−x)⌋=⌊1/x⌋−1.
40 1 1
22
33
41
21
31
4
Figure 5: A Markov partition for ˆTslow.
Proof.ThatˆTslowacts linearly on the second coordinate is immediate: when y≤1−x,ˆTslow(x,y) = (T(1−
x),y/(1−x)), andxis ﬁxed. When x <1/2 we have⌊1
1−x⌋= 1 which gives
T(1−x) =1
1−x−/floorleftbigg1
1−x/floorrightbigg
=1
1−x−1 =x
1−x.
The last claim follows from reciprocating the above.
So in the event that x <1/2 andy /∈[1−x,1] (e.g.exactly the points in [0 ,1]2whereˆTfastis not just
deﬁned as ˆTslow), Figure 6shows the action of ˆTslowon ally∈[0,1−x]. Speciﬁcally, if 1 −(i+1)x < y <1−ix,
then 1−ixs< ys<1−(i−1)xs, whereˆTslow(x,y) = (xs,ys).
01−ax1−(a−1)x
1−(a−2)x 1−3x1−2x1−x1
01−(a−1)xs
1−(a−2)xs1−(a−3)xs 1−2xs1−xs1
Figure 6: The action of ˆTslowon the second coordinate when x <1
2andy∈[0,1−x]. Herea=⌊1/x⌋≥2 and
xs=T(1−x).
Corollary 2.5. Forx <1/2andy <1−x, partition [0,1]with the points 1−ixfori= 1,2,...,a, where
a=⌊1/x⌋. For each i= 1,2,...,(a−1),ˆTfastmaps{x}×[1−(i+1)x,1−ix]linearly in its second coordinate
onto{T(x)}×[0,1]with a reversal of orientation in the second coordinate. Als o,ˆTfastmaps{x}×[0,1−ax]
linearly in its second coordinate onto {T2(x)}×[0,1]with no reversal of orientation in the second coordinate.
Proof.The proof follows from viewing iterations of ˆTslowin light of Lemma 2.4. Ify∈[1−(i+1)x,1−ix],
then after iapplications of ˆTslow“following the red edge” (in the presentation of Figure 4), we will arrive
5intoybelonging to the top-most interval, i.e. eventually y≥1−x, and we will “follow the green edge.” The
assumption that y≥1−axensures that every time we follow the red edge we do so with ⌊1/(1−x)⌋= 1, and
furthermore our rotation after applying ˆTslowexactlyiconsecutive such times will be given by
x∗=1
(a−i)+T(x),
and in following the green edge we will now have rotation by T(x∗) =T(x).
The situation for y∈[0,1−ax] is similar, except that we only “follow the red edge.” After a−1 iterations
ofˆTslow, the rotation amount is given by
x∗=1
a−(a−1)+T(x)=1
1+T(x),
and then in the ﬁnal step we compute
1−x∗=T(x)
1+T(x)
1
1−x∗= 1+1
T(x)
T(1−x∗) =T(T(x)).
The comments regarding orientation are simply due to considering ho w many times ˆTslowwould involve a
reversal of orientation: exactly once for those y≥1−ax(computing ˆTfastwill terminate in one application of
a “green edge”), and none for those y≤1−ax(ˆTfastwill instead terminate with a “red edge with ˆ a≥2”).
Corollary 2.5has another interpretation: after partitioning [0 ,1] using the points 1 −x,1−2x,...,1−ax
(wherea=⌊1/x⌋regardless of y), the map ˆTfastcan be considered to simply be the ﬁrst return map on the
interval of this partition containing y. When this interval is of length x, the induced rotation will be of length
T(x). When this interval is of length 1 −ax, the induced rotation will be of length T2(x). Then, yis linearly
scaled accordingly within this interval, reversing orientation when th e interval is of length x, but not when the
interval is of length 1 −ax. Altogether, then, we have the following:
ˆTfast(x,y) =/braceleftBigg/parenleftbig
T(x),1−y
xmod 1/parenrightbig
y≥1−⌊1/x⌋x,/parenleftBig
T2(x),y
1−⌊1/x⌋x/parenrightBig
y≤1−⌊1/x⌋x.(5)
We construct a graph presentation similar to Figure 4for the action of ˆTfastin Figure 7.
START
n= 0
x0=x, y0=y
Where is yn?
xn+1=T(xn)
yn+1=1−yn
xnmod 1xn+1=T2(xn)
yn+1=y
1−⌊1
xn⌋xnmod 1
n←n+1yn∈/bracketleftBig
1−/floorleftBig
1
xn/floorrightBig
xn,1/bracketrightBig
yn∈/bracketleftBig
0,1−/floorleftBig
1
xn/floorrightBig
xn/bracketrightBig
Figure 7: The graph version of ˆTfast.
We now describe a natural Markov partition for ˆTfast. SinceˆTfastacts the same as ˆTslowonF, we use the
same trapezoids in this region as we did for ˆTslow. Speciﬁcally, we notate them as GnandRnas trapezoids
6bounded by the given lines:
Gn:/braceleftbigg1
n+1≤x≤1
n,1−x≤y≤1/bracerightbigg
, n≥1,
Rn:/braceleftbigg
1−1
n+1≤x≤1−1
n+2,0≤y≤1−x/bracerightbigg
, n≥1.
In Figure 4the trapezoids Gncorrespond to “follow the green edge with ˆ ak+1=n”, while Rncorrespond
to “follow the red edge with ˆ ak+1=n+ 1.” The index k+ 1 here refers to a generic “next index” and is
non-speciﬁc. Also, for all x∈Gn, the standard continued fraction representation of xbegins with x= [n,...],
while for x∈Rnthe standard continued fraction representation begins with x= [1,n,...]. For convenience,
denoteG=∪iGiandR=∪iRi:Gis the triangle bounded by y= 1,x= 1, and y= 1−x, whileRis the
triangle bounded by x= 1/2,y= 0, and y= 1−x. The remaining trapezoid will be partitioned in the following
way
PGi,n:/braceleftbigg1
n+1≤x≤1
n,1−(i+1)x≤y≤1−ix/bracerightbigg
, n≥2, i= 1,...,n−1,
Tn:/braceleftbigg1
n+1≤x≤1
n,0≤y≤1−nx/bracerightbigg
, n≥2.
For consistent labeling, we also let PG0,n=GnandT1=R.
Directly from Lemma 2.4we get that the map ˆTslowacts on the trapezoids PGi,nas follows:
•Fori= 1,2,...,n−1 andn≥2,ˆTslow:PGi,n/ma√sto→PGi−1,n−1.
•Forn≥2,ˆTslow:Tn/ma√sto→Tn−1.
All of the maps as written are 1 : 1.
So we see that each of the trapezoids PGi,nwill orbit under ˆTslowintoGn−iafter exactly iapplications
ofˆTslow, wherei≤n−1. If we update our notation to set Gn=PG0,n, then we have the trapezoids PGi,n,
with 0≤i < n, are acted on by ˆTfastbyiapplications of ˆTslowof “red edge with ˆ ak+1= 1” followed by a
single instance of “green edge with ˆ ak+1= (n−i).” The notation “ PG” for trapezoids refers to them being
“pre-green.”
In contrast, the triangles Tnwill map into Raftern−1 applications of ˆTslow, at which point ˆTslowacts as
“red edge, but with ˆ ak+1≥2”. So we accordingly partition each Tninto which trapezoid Rnit will orbit into.
More speciﬁcally, let the trapezoids PRi,nbe bounded by the lines
PRi,n:/braceleftbiggi
in+1≤x≤i+1
(i+1)n+1,0≤y≤1−nx./bracerightbigg
n≥2, i= 1,...,n−1.
Note that ˆTslow:PRi,n/ma√sto→PRi,n−1, which is consistent if we relabel our original Ri=PRi,0. The trapezoid
PRi,nis acted on by ˆTslowas “n−1 red edges with ˆ ak+1= 1, followed by a single red edge with ˆ ak+1=i+1.”
These trapezoids are accordingly labeled as “pre-red.” In summary :
•The trapezoid PGi,nrefers to a region in which ˆTfastis given by iiterations of ˆTslowwith “red edge,
ˆak+1= 1” followed by one iteration of ˆTslowwith “green edge, ˆ ak+1=n−i.” Here 0≤i≤n−1.
•InPGi,n,nis the ﬁrst partial quotient of x, whileirefers to information about the ycoordinate:
1−(i+1)x≤y≤1−ix.
•In contrast, the trapezoid PRi,nrefers to a region in which ˆTfastis given by n−1 iterations of ˆTslowwith
“red edge, ˆ ak+1= 1” followed by one iteration of ˆTslowwith “red edge, ˆ ak+1≥2.” Here 1≤i <∞.
•InPRi,n, thex-coordinates are deﬁned by i/(ni+1)≤x≤(i+1)/(n(i+1)+1); the ﬁrst two partial
quotients of xaren,i, whileymust satisfy y≤1−nx.
See Figure 8to see this Markov partition; arrows show how successive applicatio ns ofˆTsloweventually map
trapezoids into either GorR, where they are mapped bijectively back to the unit square. So ˆTfastbijectively
maps each of these trapezoids to the unit square. The green trap ezoids account for those ywhich are not “very
small” by partitioning the xinto cylinders of length one for the standard continued fraction ex pansion, while
the red trapezoids account for those smaller values of y, but at the expense of partitioning the xinto cylinders
of length two for the standard continued fraction expansion.
We now present the central claim of this section:
7G=∪nPG0,n
PG0,1PG0,2
PG1,2PG0,3
PG1,3
PG2,3
T3=∪iPRi,3
T2=∪iPRi,2R=∪iPRi,1PR1,1
PR2,1
PR3,1
Figure 8: A Markov partition for ˆTfastshowing the action of ˆTslow.
Theorem 2.6. There is a probability measure ˆµon the space [0,1]2, mutually absolutely continuous with respect
to Lebesgue measure µ, for which the map ˆTfastis ergodic.
Proof.Recall that formally the second coordinate contains left/right sep aration of points y=nx; we take µto
be the natural extension of Lebesgue measure to this space. In o rder to prove our theorem we will show that
the conditions in [ BMD05, Theorem 1.1] are satisﬁed. First, we note that our system
/parenleftBig
[0,1]2,ˆTfast,µ/parenrightBig
is atower system . The ‘base’ of our tower system is the space [0 ,1]2, with all points having return time one
(i.e. the towers are all of height one, as ˆTfastmaps this space to itself). There are four conditions to verify,
and we provide the terminology of our reference for easy comparis on, though we do not necessarily redeﬁne all
terms herein:
First: “summability of upper ﬂoors”
With the return time R(x,y) = 1 for all ( x,y) we immediately verify
/summationdisplay
ℓ∈Nµ/parenleftbig/braceleftbig
(x,y)∈[0,1]2|R(x,y)> ℓ/bracerightbig/parenrightbig
=µ/braceleftbig
(x,y)∈[0,1]2|R(x,y) = 1/bracerightbig
+0
=µ([0,1]2)
= 1.
Second: the areas PGi,n,PRi,nform a generating partition R.
This condition follows from a veriﬁcation that some power of ˆTfastisuniformly expanding (has norm
bounded below by a number larger than one). We revisit Equation ( 5) as:
ˆTfast(x,y) =

/parenleftbig1
x−n,1−y
x−i/parenrightbig
(x,y)∈PGi,n,
/parenleftBig
x(ni+1)−i
1−nx,y
1−nx/parenrightBig
(x,y)∈PRi,n,(6)
8from which we can more easily compute the Jacobian DˆTfast:
DˆTfast(x,y) =

/bracketleftBigg
−1
x20
−(1−y)
x2−1
x/bracketrightBigg
(x,y)∈PGi,n,
/bracketleftBigg1
(1−nx)20
ny
(1−nx)21
(1−nx)/bracketrightBigg
(x,y)∈PRi,n.
As these matrices are triangular with distinct real numbers along th e diagonal, the smallest modulus of an
eigenvalue is easy to compute as either 1 /xor 1/(1−nx). The latter is bounded away from one, so ˆTfastis
uniformly expanding on PRi,n. OnPGi,n, however, this modulus is not bounded away from one. Also note
that in both cases there are two distinct real eigenvalues, both of which are larger than one in absolute value;
the map can never contract anywhere.
But we need only verify that some power ofˆTfastis uniformly expanding, so we consider ˆT2
fast. We have
already veriﬁed that ˆT2
fastis uniformly expanding on any PRi,norˆT−1
fastPRi,n(it is uniformly expanding on
PRi,nand nowhere-contracting in the PGi,n), so consider instead only the areas PGi1,n1∩ˆT−1
fast(PGi2,n2). For
such (x,y), the continued fraction expansion of xbeginsx= [n1,n2,...], and via the chain rule:
D/parenleftBig
ˆT2
fast(x,y)/parenrightBig
=DˆTfast(x,y)·DˆTfast/parenleftBig
ˆTfast(x,y)/parenrightBig
=/bracketleftbigg−1
x20
⋆−1
x/bracketrightbigg
·/bracketleftBigg
−1
T(x)20
⋆1
T(x)/bracketrightBigg
=/bracketleftBigg1
(xT(x))20
⋆1
xT(x)/bracketrightBigg
.
We did not explicitly compute the lower-left entries because they are not relevant for computing the norm of
these triangular matrices: in these regions we now see that the sma ller modulus of an eigenvalue is given by
1/(xT(x))>2. Therefore ˆT2
fastis uniformly expanding, and our partition generates the topology of our space.
Third: summable variation
The third condition is that /summationdisplay
n∈Nωn<∞,
whereωnis given by
ωn= sup
C∈Rnsup
(x1,y1),(x2,y2)∈ClogdetDˆTfast(x1,y1)
detDˆTfast(x2,y2),
withRn=/logicalortextn−1
i=0ˆT−i
fast(R). We see that the determinant is given by
det/parenleftBig
DˆTfast(x,y)/parenrightBig
=/braceleftBigg
x−3(x,y)∈PGi,n,
(xT(x))−3(x,y)∈PRi,n.
Observe that this determinant is independent of y. Furthermore, the areas PGi,nare deﬁned through a
length one cylinder in the standard continued expansion of x, and then xis mapped to T(x). ThePRi,nare
deﬁned through a length two cylinder in the same way, where xis mapped to T2(x). Therefore the cylinders
ofˆTn
fastare bounded in the x-coordinate by length kcylinders in the standard continued fraction expansion,
wheren≤k≤2n, depending on how many times we applied TversusT2.
So ourωnare in fact a supremum over all ( a1,a2,...,a k)∈Zk, over all x1,x2whose continued fraction
expansion begins with [ a1,a2,...,a k], of the quantity
max/braceleftbigg
3log/parenleftbiggx1
x2/parenrightbigg
,3log/parenleftbiggx1T(x1)
x2T(x2)/parenrightbigg/bracerightbigg
.
Ifx1,x2belong to the same cylinder of length k, thenT(x1),T(x2) belong to the same cylinder of length
k−1. Therefore it suﬃces to show summability only when ﬁnding the supr emum of log/parenleftBig
x1
x2/parenrightBig
. Furthermore, the
supremum is found by taking x1,x2to be the endpoints of the cylinders. The distance between the end points
is given by1
qk(qk+qk−1)whereqkis the denominator of [ a1,...,a k]. So the supremum occurs for the cylinders
with smallest denominators, i.e.a1=a2=···=ak= 1. We let ϕ= [1,1,1,...] so that 1 /ϕis the golden
mean. We ﬁnd x1=Fk/Fk+1andx2=Fk+1/Fk+2(forkodd; otherwise switch the choice of x1,x2). Here
9(Fk)k∈Nis the Fibonacci sequence. The ratio is largest when kis smallest, and since n≤k≤2n, we letk=n.
Finally, we have
|x1−ϕ|<1
F2
n+1,|x2−ϕ|<1
F2
n+2.
Regardless of whether kis even or odd, from the above we may derive that for some C <1 (speciﬁcally for
any 1> C > ϕ ) and for all suﬃciently large nwe have
/vextendsingle/vextendsingle/vextendsingle/vextendsinglex1
x2−1/vextendsingle/vextendsingle/vextendsingle/vextendsingle< Cn,1<x1
x2<1+Cn,0<log/parenleftbiggx1
x2/parenrightbigg
< Cn
from which summability of the ωnfollows.
Fourth: Large Image Properties
The fourth and ﬁnal condition of [ BMD05, Theorem 1.1] is that for each element of our partition, the image
underˆTfastis comprised of elements of our partition, and that the inﬁmum of the measure of these images is
positive. Since ˆTfastmaps each element of the partition to the entire space [0 ,1]2, this condition is trivially
satisﬁed.
The theorem we cite here also provides estimates on the rate of mixin g for this preserved measure: as the
ωnare exponentially decaying, the mixing rate of the system is exponen tial.
Corollary 2.7. The map ˆTslowdoes not preserve anyabsolutely continuous (withrespect t oLebesgue) probability
measures with bounded Radon-Nikodym derivative. Rather, ˆTslowpreserves a measure µwhich is mutually
absolutely continuous with respect to Lebesgue measure, bu t for which µ/parenleftbig
[0,1]2/parenrightbig
=∞.
Proof.The results of [ BMD05] additionally show that the Radon-Nikodym derivative dν/d(x,y), where νis
the invariant probability measure for ˆTfastwhich is absolutely continuous with respect to Lebesgue measure
d(x,y), is both bounded and bounded away from zero. There is an addition alaperiodicity requirement in that
reference which refers to the partition elements; since each eleme nt of our partition maps to the entire square,
this condition is met.
As there is a straightforward presentation of ˆTslowas a tower over ˆTfast, we see then that if ˆTslowpreserved
such a measure, then the νmeasure of the of the base would be proportional (according to th e bounds on the
derivative dµ/d(x,y)) to the sum of the µ-measures of the towers. Since our towers are of height nover a base
of area on the order of 1 /n2, no such ﬁnite measure can exist. If we simply deﬁne a measure µ, however, to
be on each level of the tower the ν-measure of the base, then we will have constructed an inﬁnite (bu t still
σ-ﬁnite) measure which is preserved by ˆTslow.
3 Canonical Approximations
As we successively constructed ﬁrst return maps in Section 2we repeatedly rescaled the resulting interval to be
of length one; this renormalization allowed us to construct the map ˆTfaston the space [0 ,1]2. In this section,
however, we keep continued track of the corresponding shrinking intervals containing yin the original space.
We simultaneously develop substitutions which encode of the orbits o f the endpoints of these intervals through
their ﬁrst returns.
It is clear from the iterative construction of ﬁrst-return maps in S ection2that we have a natural sequence
of intervals Islow(k) which are nested ( Ik+1⊂Ik) with intersection exactly {y}. Speciﬁcally, Islow(0) = [0,1],
and then if Islow(k) is partitioned by the ﬁrst pre-image of the origin which appears in th e interior, Islow(k+1)
is which of the two resulting intervals contains y; recall that for y∈xZwe have left- and right-sided versions
ofy, so this choice is always unique. These are therefore the intervals in the original circle [0 ,1] on which the
mapsˆTslowsuccessively computed the ﬁrst-return map. Observe trivially tha t both endpoints of Islow(k) are of
the form−nxfor some non-negative integer n. We deﬁne those natural numbers nso that−nxis an endpoint
of someIslow(k), listed in increasing order, to be the slow approximating sequence of ywith respect to x. We
formalize these notions below:
Lemma 3.1. If{nk}are the slow approximating sequence of ywith respect to x, then for all k∈N,−nkxand
−nk−1xare the endpoints of Islow(k)(under the convention that n−1=n0= 0). If we let{a,b}={nk,nk+1}
so that0≤y+ax < y+bx≤1 mod 1 , then either y+nk+2x < y+axmod 1ory+bx < y+nk+2xmod 1.
Furthermore, nk+2is the least positive integer which satisﬁes either of those inequalities.
Proof.Both endpoints of Islow(0) = [0,1] are of the form −0x, so the base case of the ﬁrst claim is immediate.
Our construction of the intervals Islow(k) is such that the endpoints form the sequence −nkxwhich are succes-
sively the closest points in the backwards orbit of the origin which are closest to y. If the points−nkmod 1
are the successively closest points to y, theny+nkmod 1 are the successively closest points to the origin.
10There is therefore a subsequence of these intervals, Ifast(k) which are the intervals on which the ﬁrst return
map is a scaled version of ˆTk
fast; recall that
ˆTfast(x,y) =/braceleftBiggˆTi+1
slow(x,y) ((x,y)∈PGi,n),
ˆTn
slow(x,y) ((x,y)∈PRi,n).
So we construct a subsequence of kbeginning with k0= 0, and then
kj+1=

kj+i+1/parenleftBig
ˆTj
fast(x,y)∈PGi,n/parenrightBig
,
kj+n/parenleftBig
ˆTj
fast(x,y)∈PRi,n/parenrightBig
.
In this way, letting Ifast(j) =Islow(kj), these are the intervals in the original system on which ˆTfastis
constructing ﬁrst-return maps.
We deﬁne those nkjso that−nkjxis an endpoint of some Ifast(kj) to be the fast approximating sequence of
ywith respect to x. These numbers, then, represent a “sped-up” sequence of nkalong which y+nkjxmod 1
converges to the origin. What we have called the slow and fast appro ximating sequences of ywith respect to x
represent those points in the orbit of ywhich are close to the origin: −nxmod 1 is close to y, soy+nxmod 1
is close to 0 or 1. What is more typical is to consider those points in the orbit of the origin which are close to y.
In [IN88] these are called the canonical approximating sequence forywith respect to x. Our deﬁnition is similar
but not identical; the diﬀerence amounts to approximating ywith points in the forward orbit of the origin (the
canonical sequence) versus the backwards orbit of the origin (ou r fast approximating sequence). Therefore the
fast approximating sequence for ywith respect to xwould be the canonical approximating sequence for ywith
respect to 1−x, and vice versa.
LetA={A,B}andA∗be the free monoid on A. Let the function Ω : [0 ,1]2/ma√sto→ANbe deﬁned by
Ω(x,y)n=/braceleftBigg
A(y+nxmod 1≥1−x)
B(y+nxmod 1≤1−x).
Then Ω(x,y) is called the symbolic encoding of the orbit of yunder rotation by x, and the preﬁx Ω( x,y)[0,n)is
called the symbolic encoding of length n. For a given pair of coordinates ( x,y), we will also simply refer to the
symbolic coding of the pair (x,y) in place of the symbolic coding of yunder rotation by x.
We deﬁne the homomorphisms σx,τonA∗by:
σx(A) =ABaτ(A) =B
σx(B) =ABa−1τ(B) =A
wherea=⌊1/x⌋as before. These can be naturally extended to be homomorphisms o nAN. Then let
σslow(x,y) =/braceleftBigg
σx (y≥1−x)
τ◦σ1−x(y≤1−x).(7)
Representing any word in A∗withnoccurrences of Aandmoccurrences of Bby the column vector [ n,m]T,
we see that the substitutions may be studied through their matrice sMslow(x,y)
Mslow(x,y) =

/bracketleftBigg
1 1
ˆa(ˆa−1)/bracketrightBigg
(y≥1−x,ˆa=⌊1/x⌋)
/bracketleftBigg
ˆa(ˆa−1)
1 1/bracketrightBigg
(y≤1−x,ˆa=⌊1/(1−x)⌋).(8)
Of particular interest is when y≤1−x, and 1−x≥1/2 (i.e. ˆa= 1), in which case σ(A) =BA,σ(B) =B,
i.e. those x,yfor which
Mslow(x,y) =/bracketleftbigg
1 0
1 1/bracketrightbigg
. (9)
Note that/bracketleftbigg1 0
1 1/bracketrightbiggd
=/bracketleftbigg1 0
d1/bracketrightbigg
. (10)
Finally, deﬁne the ergodic composition recursively by
σslow(x,y,k+1) =σslow(x,y,k)◦σslow/parenleftBig
ˆTk
slow(x,y)/parenrightBig
11with the conventionthat σslow(x,y,0) is the identity substitution, andwith the matrices Mslow(x,y,k) similarly
deﬁned.
Recall that ˆTfastwas equivalent to composing all occurrences ˆTslowwherey≤1−xand 1−x≥1/2, the
“red edges with ˆ ak+1= 1,” into the next application of ˆTslow. InPGi,nthere are exactly isuch occurrences
(with 0≤i≤n−1), and in PRi,nthere are exactly n−1. In other words, we may deﬁne σfast(x,y) as
σfast(x,y) =/braceleftBigg
σslow(x,y,i+1) (x,y)∈PGi,n
σslow(x,y,n) (x,y)∈PRi,n.(11)
We also deﬁne the matrices Mfast(x,y),Mfast(x,y,k) similarly. By combining Equation ( 8), Equation ( 9),
Equation ( 10), Equation ( 11):
Mfast(x,y,k) =

/bracketleftBigg
1 1
ˆa(ˆa−1)/bracketrightBigg
·/bracketleftBigg
1 0
i1/bracketrightBigg
((xk,yk)∈PGi,n),ˆa=⌊1/xn⌋)
/bracketleftBigg
ˆa(ˆa−1)
1 1/bracketrightBigg
·/bracketleftBigg
1 0
(n−1) 1/bracketrightBigg
((xk,yk)∈PRi,n,ˆa=⌊1/(1−xn)⌋).
Thevaluesofˆ acanbedeterminedasfollows: in PGi,n, aftericonsecutive“rededgewith ˆ ak+1= 1”applications
ofˆTslow, we will “follow the green edge” with ˆTslowbut where the ﬁrst partial quotient is n−i. So inPGi,n,
we use ˆa=n−i. In thePRi,n, aftern−1 applications of “red edge with ˆ ak+1= 1” we will follow the red edge
for a rotation whose standard continued fraction begins with x′= [1,i,...], so ˆa=⌊1/(1−x′)⌋=i+1. Then
Mfast(x,y,k) is the matrix of the substitution σfast(xk,yk); the single substitution generated after kiterations
ofˆTfast. Altogether:
Mfast(x,y) =

/bracketleftBigg
i+1 1
((n−i)(i+1)−i) (n−i−1)/bracketrightBigg
((xk,yk)∈PGi,n)
/bracketleftBigg
(ni+1)i
n1/bracketrightBigg
((xk,yk)∈PRi,n).(12)
These matrices have two distinct real eigenvalues, one positive and one negative: deﬁne λ±(x,y) to be these
eigenvalues. From ( 12) we directly compute
λ±=n±√
n2+4
2for (x,y)∈PGi,n
λ±=(ni+2)±/radicalbig
(ni+2)2+4
2for (x,y)∈PRi,n.
We can now state:
Theorem 3.2. Bothlog+/⌊ard⌊lMfast(x,y)/⌊ard⌊landlog+/⌊ard⌊lM−1
fast(x,y)/⌊ard⌊lare Lebesgue integrable on [0,1]2, wherelog+(t) =
max{0,log(t)}and the norm is the operator norm.
Proof.We begin with several claims about these norms:
1. In either PGi,norPRi,n, both of/⌊ard⌊lMfast/⌊ard⌊l,/⌊ard⌊lM−1
fast/⌊ard⌊l≥1. That is: the log+function will always simply
evaluate as a logarithm.
2. InPGi,n,/⌊ard⌊lMfast/⌊ard⌊l≤n+1, while in PRi,n,/⌊ard⌊lMfast/⌊ard⌊l≤ni+3.
3. InPGi,n,/⌊ard⌊lM−1
fast/⌊ard⌊l≤2n, while in PRi,n,/⌊ard⌊lM−1
fast/⌊ard⌊l≤2(ni+2).
The matrix Mfast(x,y) has two distinct eigenvalues of the form
λ±=ζ±/radicalbig
ζ2+4
2,
whereζis a positive integer. Speciﬁcally, ζ=ninPGi,nandζ=ni+2 inPRi,n. It follows that
/⌊ard⌊lMfast(x,y)/⌊ard⌊l=ζ+/radicalbig
ζ2+4
2,/⌊ard⌊lM−1
fast(x,y)/⌊ard⌊l=2/radicalbig
ζ2+4−ζ. (13)
The inequality ζ >0 establishes that these norms are at least one and completes our ﬁ rst claim.
12For the upper bound on /⌊ard⌊lMfast(x,y)/⌊ard⌊l, the elementary estimate ζ2+4<(ζ+2)2yields the desired result.
For the upper bound on the norm of the inverse operator, ( ζ+1/ζ)2< ζ2+4 shows that/⌊ard⌊lM−1
fast(x,y)/⌊ard⌊l<2ζ:
ζ=norζ=ni+2 in the PGi,nandPRi,nrespectively complete this step.
We now proceed with the proof of integrability. The set PGi,nis a trapezoid with width 1 /n−1/(n+1)<
1/n2andlongerheight1 /n. Theset PRi,nisatrapezoidwithwidth ( i+1)/(n(i+1)+1)−i/(ni+1)<1/(ni+1)2
and longer height 1 /(ni+1). So
Area(PGi,n)<1
n3,
Area(PRi,n)<1
(ni+1)3<1
(ni)3.
Recall that our Markov partition is given by PGi,nforn= 1,2,...and 0≤i≤n−1, andPRi,nfor
i,n= 1,2,.... With our earlier claims, then:
/integraldisplay
[0,1]2log+/⌊ard⌊lMfast(x,y)/⌊ard⌊ldxdy <∞/summationdisplay
n=1n−1/summationdisplay
i=0/integraldisplay
PGi,nlog(n+1)dxdy+∞/summationdisplay
i=1∞/summationdisplay
n=1/integraldisplay
PRi,nlog(ni+3)dxdy
<∞/summationdisplay
n=1n−1/summationdisplay
i=0log(n+1)
n3+∞/summationdisplay
i=1∞/summationdisplay
n=1log(ni+3)
(ni)3
<∞/summationdisplay
n=1n−1/summationdisplay
i=0log(n+1)
n3+∞/summationdisplay
i=1∞/summationdisplay
n=1log((n+2)(i+2))
(ni)3
<∞/summationdisplay
n=1log(n+1)
n2+/parenleftBigg∞/summationdisplay
i=11
i3/parenrightBigg/parenleftBigg∞/summationdisplay
n=1log(n+2)
n3/parenrightBigg
+/parenleftBigg∞/summationdisplay
n=11
n3/parenrightBigg/parenleftBigg∞/summationdisplay
i=1log(i+2)
i3/parenrightBigg
.
Summability of all terms is elementary. Integrability of log+/⌊ard⌊lM−1
fast/⌊ard⌊lis handled analogously; observe the
similarity in the bounds derived.
Theorem 3.2will be one ofour primarytools in studying the growthrate ofthe slow approximatingsequence
and fast approximating sequence. We again begin with the slow situat ion before moving on the fast. Observe
that between Islow(n) andIslow(n+ 1), one endpoint is always shared between these two intervals, an d one
endpoint is not. Deﬁne a sequence of ǫslow,nto track the number of times that orientation has been reversed
in applying ˆTslow:
ǫslow,0= 1, ǫ slow,n+1=/braceleftBigg
−ǫslow,nˆTn
slow(x,y)∈G
ǫslow,nˆTn
slow(x,y)∈R.
That is: we reverse the value of ǫslow,nwhen we “follow the green edge,” but not when we “follow the red
edge.”
Deﬁne a sequence of words ρslow,0,kandρslow,1,kas follows, where ω1·ω2simply refers to the concatenation
of two words and erefers to the empty word; this notation will be helpful for words wh ich are deﬁned with
numerous subscripts and functions:
ρslow,0,0=e, ρ slow,1,0=e
ρslow,0,k+1=

ρslow,0,k ǫslow,k= 1,ˆTk
slow(x,y)∈R
σslow(x,y,k)(A)·ρslow,0,kǫslow,k= 1,ˆTk
slow(x,y)∈G
σslow(x,y,k)(B)·ρslow,0,kǫslow,k=−1,ˆTk
slow(x,y)∈R
ρslow,0,k ǫslow,k=−1,ˆTk
slow(x,y)∈G
ρslow,1,k+1=

σslow(x,y,k)(B)·ρslow,1,kǫslow,k= 1,ˆTk
slow(x,y)∈R
ρslow,1,k ǫslow,k= 1,ˆTk
slow(x,y)∈G
ρslow,1,k ǫslow,k=−1,ˆTk
slow(x,y)∈R
σslow(x,y,k)(A)·ρslow,1,kǫslow,k=−1,ˆTk
slow(x,y)∈G.(14)
The choice of j= 0 orj= 1 inρslow,j,krelates to whether these words encode the orbit of endpoints of
Islow(k) until they reach 0+or 1−, respectively:
Lemma 3.3. The substitution σslowmaps the symbolic coding of the pair ˆTslow(x,y)to the symbolic coding of
the pair(x,y):
Ω(x,y) =σslow(x,y)/parenleftBig
Ω(ˆTslow(x,y))/parenrightBig
.
The left endpoint and right endpoint of Islow(k)are given by−/⌊ard⌊lρslow,0,k/⌊ard⌊lxand−/⌊ard⌊lρslow,1,k/⌊ard⌊lx, respectively,
where/⌊ard⌊lω/⌊ard⌊lrefers to the length of the word ω.
13Proof.The ﬁrst claim follows from the fact that we are constructing ﬁrst- return maps on intervals. When
y∈[1−x,1], the ﬁrst-return map on an interval of length xhas two return times; aanda+1, where a=⌊1/x⌋.
Furthermore, since the interval on which we are constructing the map is the interval labeled A, it follows that
points in Ahave their ﬁrst-returns encoded either by the words ABa−1(when the return time is a) orABa
(when the return time is a+1); points begin in this interval labeled A, and the remainder of the orbit until
returning to the interval labeled Ais spent in the complement labeled B. These words are exactly those used
byσslow(x,y) wheny≥1−x. The argument is similar for y≤1−x, we simply note that we are constructing
the ﬁrst-return map on the interval labeled B, but we consider the rotation to be of length 1 −xin the opposite
orientation. This explains both composition with the substitution τand the return times being dictated by
⌊1/(1−x)⌋.
Turning our attention to the words ρslow,i,k, fork= 0 the result is immediate. We begin with ρslow,0,0=
e=ρslow,1,0, a word of length zero, and both endpoints are of the form −0x. For the recursive procedure, let
us consider the situation with ǫslow,k= 1. See Figure 3for both possibilities for ǫslow,k= 1 (we begin with a
map in the forward orientation); when ˆTk
slow(x,y)∈R, we use the red arrow to construct a ﬁrst-return map
in the interval [0 ,1−x]. In this case, the left endpoint stays the same, so ρslow,0,k+1=ρslow,0,k. But the new
right endpoint of 1 −xwas previously encoded by Bin the prior system. The word Btherefore encodes this
endpoint through one step in the previous system, at which point it w ould be sent to 1. The ﬁrst result of this
lemma therefore gives that σslow(x,y,k)(B) encodes the orbit of this new right endpoint until orbiting onto the
previous rightendpoint, so concatenationwith ρslow,1,kwill give the word which encodes the new right endpoint
until it arrives at 1 in the starting system, rotation by x0inIslow(0):σslow(x,y,k)(B)·ρslow,1,k=ρslow,1,k+1.
The green arrow in the same ﬁgure informs how to consider the situa tionǫslow,k= 1,ˆTk
slow(x,y)∈G.
In this case the right endpoint stays the same, so ρslow,1,k+1=ρslow,1,k. But the left endpoint is new,
having been encoded by Ain the previous step and being mapped to the previous left endpoint in one step:
ρslow,0,k+1=σslow(x,y,k)(A)·ρslow,0,k. In situations where ǫslow,k=−1, the diagrams in Figure 3need only
be reversed in orientation and considered similarly.
The words ρslow,i,ktherefore encode the orbits of the left and right endpoints of Islow(k) through their
eventual returns to the endpoints of Islow(0), and therefore the lengths of these words are the return tim es
necessary to return to the origin: the endpoints are given by −/⌊ard⌊lρslow,0,k/⌊ard⌊lxand−/⌊ard⌊lρslow,1,k/⌊ard⌊lx.
Corollary 3.4. Forℓ= 0,1, exactly one of ρslow,ℓ,k +1=ρslow,ℓ,k, while for the other, either ρslow,ℓ,k +1=
σslow(x,y,k)(A)·ρslow,ℓ,korρslow,ℓ,k +1=σslow(x,y,k)(B)·ρslow,ℓ,k.
Proof.This statement follows immediately from the more precise formulation in Lemma 3.3. One may imme-
diately derive this statement, however, by noting that we always co nstruct our ﬁrst return map by sharing one
endpoint with the previous system, and the other endpoint was eith er encoded by AorBwith a return time
of one.
In constructing the sequences of words ρslow,i,k, consecutive applications of these “red edges with ˆ ak+1= 1”
correspond to consecutive terms where ǫslow,k=ǫslow,k+1. In Equation ( 14), this condition exactly describes
when the words σslow(x,y,n)(B) are considered. However, the individual substitutions being gene rated map
B/ma√sto→Bin this special situation.
In other words, for ( x,y)∈PGi,n, to determine the action of ˆTfastas iterations of ˆTslow, the map ˆTslow
would successively act as “red edge, a= 1” a total of iconsecutive times, but for each j= 1,...,iwe will have
σslow(x,y,j)(B) =B
because the ﬁrst isubstitutions applied will all map B/ma√sto→B. Similarly, if ( x,y)∈PRi,n, then as we have
exactlyn−1 consecutive such applications of “red edge, a= 1”, for all j= 1,...,n−1 we also have
σslow(x,y,j)(B) =B.
After these edges, we follow a single application of ˆTsloweither with a “green edge,” or a “red edge with
ˆak+1≥2,” and we have determined how substitutions help encode a single suc h instance.
Altogether, we use these observations to deﬁne the sequences ρfast,0,nandρfast,1,nwhich encode the orbits
of the left and right endpoints of Ifast(n). Let the ǫfast,n=±1 similarly to the ǫslow,n; these values track the
number of times we have reversed orientation due to ˆTi
fast(x,y) belonging to some PGi,n. Speciﬁcally:
ǫfast,0= 1, ǫ fast,n+1=/braceleftBigg
ǫfast,nˆTn
fast(x,y)∈PRi,n
−ǫfast,nˆTn
fast(x,y)∈PGi,n.
14Begin again with ρfast,0,0=eandρfast,1,0=e, and then
ρfast,0,k+1=

ρfast,0,k ǫfast,k= 1,ˆTk
fast(x,y)∈PRi,n
σfast(x,y,k)(BiA)·ρfast,0,kǫfast,k= 1,ˆTk
fast(x,y)∈PGi,n
σfast(x,y,k)(Bn)·ρfast,0,kǫfast,k=−1,ˆTk
fast(x,y)∈PRi,n
σfast(x,y,k)(Bi)·ρfast,0,kǫfast,k=−1,ˆTk
fast(x,y)∈PGi,n
ρfast,1,k+1=

σfast(x,y,k)(Bn)·ρfast,1,kǫfast,k= 1,ˆTk
fast(x,y)∈PRi,n
σfast(x,y,k)(Bi)·ρfast,1,kǫfast,k= 1,ˆTk
fast(x,y)∈PGi,n
ρfast,1,k ǫfast,k=−1,ˆTk
fast(x,y)∈PRi,n
σfast(x,y,k)(BiA)·ρfast,1,kǫfast,k=−1,ˆTk
fast(x,y)∈PGi,n.(15)
Lemma 3.5. The substitution σfastmaps the symbolic coding of the pair ˆTfast(x,y)to the symbolic coding of
the pair(x,y):
Ω(x,y) =σfast(x,y)/parenleftBig
Ω(ˆTfast(x,y))/parenrightBig
.
The words σfast(x,y,n)(A)andσfast(x,y,n)(B)give encodings of the endpoints of Ifast(n)through their length.
The left endpoint and right endpoint of Ifast(n)are given by−/⌊ard⌊lρfast,0,n/⌊ard⌊lxand−/⌊ard⌊lρfast,1,n/⌊ard⌊lx, respectively.
Proof.The ﬁrst statement is immediate in light of Lemma 3.3and Equation ( 11). The information about the
ρfast,i,kfollows from a case-by-caseanalysis, determining the encoding oft he left/right endpoints of the interval
on which we are going to construct our ﬁrst-return map until it rea ches the previous left/right endpoints.
For example, consider the situation of ǫfast,k= 1, so we are rotating from left to right. Suppose that
ˆTk
fast(x,y)∈PGi,n. Equivalently, if we let ( x′,y′) =ˆTk
fast(x,y), then we will be constructing our ﬁrst-return
map on the (normalized) interval [1 −(i+ 1)x′,1−ix′]. The left endpoint takes ( i+ 1) steps to return to
the origin. In the interim it will have iconsecutive values less than 1 −x′, then a single value equal to (the
right-sided version of) 1 −x′. So this portion of the orbit is encoded by BiA, after which we have arrived at
the (right-sided version of the) origin, so the orbit continues with ρfast,0,k. Overall:
ρfast,0,k+1=σfast(x,y,k)(BiA)·ρfast,0,k.
In contrast, the right endpoint of 1 −ix′will orbit to the (left-sided version of the) origin after isteps,
having its orbit encoded by Bi:
ρfast,1,k+1=σfast(x,y,k)(Bi)·ρfast,1,k.
Other cases are handled similarly.
Corollary 3.6. Between ℓ= 0,1, ifˆTfast(x,y,k)∈PRi,n, exactly one of the words ρfast,ℓ,k +1is given by
ρfast,ℓ,k, while the other is given by σfast(x,y,k)(Bn)·ρfast,ℓ,k.
IfˆTfast(x,y,k)∈PGi,n, then one of the words ρfast,ℓ,k +1is given by σfast(x,y,k)(Bi·ρfast,ℓ,kwhile the
other is given by σfast(x,y,k)(BiA)·ρfast,ℓ,k.
Proof.Just as Corollary 3.4followed immediately from Lemma 3.3, this result follows immediately from
Lemma3.5.
Wewishtostudythegrowthrateoftheslowandfastapproximating sequences,sobyLemma 3.3,Lemma 3.5,
we equivalentlywish to study the growthratesofconcatenationso fvariouswordsgeneratedbythe substitutions
σslow(x,y,k) andσfast(x,y,k). Ourﬁrststep is to showthat up to alinearfactor, the growthra teis determined
by the new term being concatenated, and not the previous terms g enerated. Deﬁne
Nslow(x,y,k) = max{/⌊ard⌊lρslow,0,k/⌊ard⌊l,/⌊ard⌊lρslow,1,k/⌊ard⌊l},
MAX slow(x,y,k) = max{/⌊ard⌊lσslow(x,y,k)(A)/⌊ard⌊l,/⌊ard⌊lσslow(x,y,k)(B)/⌊ard⌊l},
minslow(x,y,k) = min{/⌊ard⌊lσslow(x,y,k)(A)/⌊ard⌊l,/⌊ard⌊lσslow(x,y,k)(B)/⌊ard⌊l}
and then also deﬁne
Nfast(x,y,k) = max{/⌊ard⌊lρfast,0,k/⌊ard⌊l,/⌊ard⌊lρfast,1,k/⌊ard⌊l},
MAX fast(x,y,k) = max{/⌊ard⌊lσfast(x,y,k)(A)/⌊ard⌊l,/⌊ard⌊lσfast(x,y,k)(B)/⌊ard⌊l},
minfast(x,y,k) = min{/⌊ard⌊lσfast(x,y,k)(A)/⌊ard⌊l,/⌊ard⌊lσfast(x,y,k)(B)/⌊ard⌊l}.
15Lemma 3.7. For any x,y, we have
minslow(x,y,k)≤Nslow(x,y,k+1)≤(k+1)·MAX slow(x,y,k+1),
minfast(x,y,k)≤Nfast(x,y,k+1)≤(k+1)·MAX fast(x,y,k+1).
Proof.Thelowerboundsareimmediate; inlightofCorollary 3.4, atleastoneofthewords ρslow,0,k+1,ρslow,1,k+1
will involve concatenation with σslow(x,y,k)(A) orσslow(x,y,k)(B) so whichever of the two ρslowis larger,
Nslow(x,y,k+1) will be at least as large as the smaller of those two words. The res ult is similar for the lower
bound on the Nfast; by Corollary 3.6someσfast(x,y,k)(A) orσfast(x,y,k)(B) will always be concatenated to
one of the previous ρfast.
For the upper bound on Nslow, we prove by induction. Note that for k= 0 we are considering Nslow(x,y,1),
which is the length of either ρslow,0,1orρslow,1,1, whichever is larger. But those words refer to the encoding of
the endpoints of the ﬁrst interval on which we construct a ﬁrst re turn map; those points are either 0 ,1−xor
1−x,1. In either case, one is encoded with a word of length zero, and the other with a word of length one.
That is: Nslow(x,y,1) = 1 always: our base case is shown.
So assume for some k−1 that the upper inequality holds: Nslow(x,y,k)≤kMAX slow(x,y,k). Then in
light of Corollary 3.4, the longer of the two words ρslow,0,k+1,ρslow,1,k+1is no longer than the concatenation
of either σslow(x,y,k)(A) orσslow(x,y,k)(B) with whichever of ρslow,0,k,ρslow,1,kwas longer:
Nslow(x,y,k+1)≤MAX slow(x,y,k)+Nslow(x,y,k)
≤MAX slow(x,y,k)+kMAX slow(x,y,k)
≤(k+1)MAX slow(x,y,k)
≤(k+1)MAX slow(x,y,k+1)
The last line is included only to make the inequality appear similar to the ine qualities for Nfast; the upper
bound on the Nfastfollows from a similar argument. For k= 0, we have Nfast(x,y,1) representing the length
of the encoding of some point in the partition {0,1−ax,1−(a−1)x,...,1−x,1}, so the length of that word
is no larger than a=⌊1/x⌋. On the other hand, 1 ·MAX fast(x,y,1) is the length of some σ(A) orσ(B),
which are seen to be of length at least a. By Corollary 3.6,Nfast(x,y,k+1) is no larger than concatenating
the longest possible word to a word of length Nfast(x,y,k+1). But the word we concatenate is of the form
σfast(x,y,k) of some factor of the single substitution generated by ( xk,yk) (see Equation ( 15)). Therefore the
word we concatenate is no larger than MAX fast(x,y,k+1) (the relevant substitution for this term ends with
the substitution generated by ( xk,yk), explaining the appearance of one larger index than naturally occu rred
in the “slow” situation, where single letters were always fed into subs titutions):
Nfast(x,y,k+1)≤MAX fast(x,y,k+1)+Nfast(x,y,k).
The upper bound now follows inductively as it did in the case of Nslow.
We remarkthat the upper bound in both casesis likely far from optima l, but will be suﬃcient for ourdesired
claims. By deﬁnition, the slow and fast approximating sequences are given by Nslow(x,y,k) andNfast(x,y,k),
and the orbit of yis encoded through these lengths by the relevant words ρ. We may now state the objective
of this section:
Corollary 3.8. There is a generic growth rate of the fast-approximating tim esNfast(x,y,k): there exists some
C >1so that for almost every choice of x,y, we have
lim
k→∞1
klog(Nfast(x,y,k)) =C.
However, the slow-approximating times Nslow(x,y,k)have no such generic growth rate: for any C >1, for
almost all x,ywe have either
liminf
k→∞Nslow(x,y,k)
Ck= 0orlimsup
k→∞Nslow(x,y,k)
Ck=∞.
Proof.By the multiplicative ergodic theorem, Theorem 2.6, and Theorem 3.2, there is some C >1 so that
almost surely both
lim
k→∞1
klog/⌊ard⌊lσfast(x,y,k)(A)/⌊ard⌊l=C= lim
k→∞1
klog/⌊ard⌊lσfast(x,y,k)(B)/⌊ard⌊l,
from which we conclude that
lim
k→∞1
klogminfast(x,y,k) =C= lim
k→∞1
klogMAX fast(x,y,k).
16We then recall the estimates of Lemma 3.7, and the ﬁrst result is shown.
The second claim is then a standard result in theory of transformat ions which preserve an inﬁnite measure,
see e.g [AS97, Theorem 2.4.2].
It is proved in [ IN88] thatC=π2/(12log2), the almost-sure base of the growth rate of the denomin ators
qnin the convergents of the standard continued fraction represen tation of x. Recall that what we call the fast
approximating sequence for ( x,y) is what in that reference is called the canonical approximating sequ ence for
(x,1−y), but results for generic rates of growth are equivalent.
4 Relationships to Continued Fractions
In this section we do not ﬁrst pick a yand follow the algorithm to see when to follow the green edge and when
to follow the red edge. Rather, we follow certain methods for choos ing whether to construct the ﬁrst-return
map on [0 ,1−x], or whether to reverse orientation and construct the ﬁrst-ret urn map on [1−x,1]. Diﬀerent
systems for making these choices will be shown to correspond to diﬀ erent continued fraction systems. The
unique point of intersection of these nested intervals therefore p rovides a special point whose orbit is encoded
by substitutions that are related to these continued fraction sys tems. To avoid unnecessary ambiguity we
assume that x∈[0,1]\Q.
Suppose the regular continued fraction expansion of xis given by
x=1
a1+1
a2+1
a3+...= [a1,a2,a3,...].
Then we see that
1−x=/braceleftBigg
[a2+1,a3,...] (a1= 1),
[1,a1−1,a2,...] (a1/\e}atio\slash= 1).(16)
Accordingly, since certainly x= 1−(1−x), we ﬁnd that “following the red edge” exactly corresponds to
performing singularization or insertion on the continued fraction ex pansion of x; singularization when some
an= 1, insertion when an/\e}atio\slash= 1. To be more precise, a singularization is obtained by using the follow ing formula
fora,b∈N>0andε=±1
a+ε
1+1
b+ξ=a+ε+−ε
b+1+ξ. (17)
Takinga= 1 and ε=−1 we ﬁnd the continued fraction of 1 −x. We see that T2(x) =T(1−x) so inducing
on the interval [ x,1] will ‘go faster’ in the sense that the associated return times are larger. This corresponds
to the fact that we are constructing the return map on the smaller interval ( a1= 1 gives x >1
2). When a1>1
we get insertion. The formula for insertion where a,b∈N>0withb≥2 andξ∈[0,1] is given by
a+1
b+ξ=a+1+−1
1+1
b−1+ξ. (18)
Takinga= 0 we get the continued fraction of xon the left hand side and 1 −(1−x) on the right hand side
whena1/\e}atio\slash= 1. Here we see that T(x) =T2(1−x) which means we ‘went slower’ in the sense of smaller return
times: we constructed the return map on the larger interval. By us ing singularizations and insertions one can
ﬁnd all semi-regular continued fractions of a number [ DK00,Kra91]. That is, in case of irrational numbers,
all expansions with numerators εn∈{−1,1}, digits ˆan∈Nand for all n, ˆan+εn≥1, and inﬁnitely often
ˆan+εn≥2. Each such representation of xcorresponds to a certain choice of y; they∈[0,1] for which our
algorithm would transform the standard continued fraction expan sion ofxinto the desired form: ˆ anin the
modiﬁed continued fraction are exactly the return times ˆ anfrom Figure 4. Conversely, all such ygenerate a
semi-regular continued fraction representation of x. We therefore have a canonical correspondence between all
semi-regular continued fraction representations of some x(with numerators ±1) and the interval [0 ,1] (with
left/right versions of those y∈xZ):
Theorem 4.1. Everyy∈[0,1]generates exactly one semi-regular continued fraction of xand for every semi-
regular continued fraction of xthere is exactly one y∈[0,1]that generates it.
Proof.The proof is clear from the previous discussion.
17Another way of generating semi-regular continued fractions is to p ick the numerators εnat random. This is
done, for example, in [ KKV17]. Note that heuristic arguments about ‘random’ choices of εnwill not necessarily
agree with almost-sure results for ˆTfast, as singularization/insertion are performed by ˆTslowfor anyy≤1−x.
When the ﬁrst partial quotient of xis very large, the probability of singularization/insertion under ˆTslowis
correspondingly very large.
Singularizationsandinsertionsarelocaloperationsthatcanbeapp liedtoanysemi-regularcontinuedfraction
ofx. If
x= [1/d1,ε1/d2,...,ε n−1/dn,εn/1,1/dn+2,...]
then applying a singularization at dn+1= 1 gives
x= [1/d1,ε1/d2,...,ε n−1/(dn+εn),−1/dn+2,...].
On the other hand when
x= [1/d1,ε1/d2,...,ε n−1/dn,1/dn+1,εn+1/dn+2,...]
then, when dn+1≥2, insertion after dngives us
x= [1/d1,ε1/d2,...,ε n−1/(dn+1),−1/1,1/(dn+1−1),εn+1/dn+2,...].
Furthermore,theseoperationsaﬀecttheconvergentsinthefo llowingway. Writepn(x)
qn(x)= [1/d1,ε1/d2,...,ε n−1/dn].
Then performing a singularization at place nwill result in deleting the elementpn
qnin the list of convergents, i.e.
p1
q1,p2
q2,...pn−1
qn−1,pn+1
qn+1,.... On the other hand insertion will add a convergentp1
q1,p2
q2,...pn−1
qn−1,pn−1+pn
qn−1+qn,pn
qn,....
The numerators pnand denominators qnfor the altered, semi-regular expansion of x, satisfy the following
convergent relations
p−1= 1, p0= 0, pn=dnpn−1+εn−1pn−2,
q−1= 0, q0= 1, qn=dnqn−1+εn−1qn−2.(19)
Deciding when to do an insertion or an insertion is dictated by y∈[0,1]. With this in mind we deﬁne qn,y(x)
to be the denominators of the sequence of convergents of xwhere the places of insertion and singularization
are described by the orbit of ( x,y) when iterating over ˆTslow. Furthermore, we can look at those nksuch that
correspond to the accelerated system Tfast; let this be qk,y(x) =qnk,y(x). Observe that as ˆTfastis deﬁned as
iterating ˆTslowuntil either y≥(1−x) (which corresponded to regular progression in the convergents ofx) or
y≤1−xandx≥1/2 (which corresponded to singularization), the qk,y(x) correspond to those qn,y(x) which
arenotdue to insertion. We are interested in how fast qn,y(x) andqn,y(x) typically grow. To this end, let us
look at the inverse branches of ˆTslowas M¨ obius transformations acting on the matrix with the converge nts. To
be more precise,/bracketleftbigga b
c d/bracketrightbigg
(x) :=ax+b
cx+d.
Then the inverse branches of ˆTsloware given by
Ax,y:=

/bracketleftBigg
0 1
1 ˆa1(x,y)/bracketrightBigg
wheny∈[1−x,1]
/bracketleftBigg
1 ˆa1(x,y)−1
1 ˆa1(x,y)/bracketrightBigg
wheny∈[0,1−x].
Let
M0,x,y=/bracketleftbigg1 0
0 1/bracketrightbigg
=/bracketleftbiggp−1p0
q−1q0/bracketrightbigg
andMn+1,x,y=Mn,x,yATn
slow(x,y)forn≥1. Now when yn∈[1−xn,1] we ﬁnd
Mn+1,x,y=/bracketleftbiggpn−1pn
qn−1qn/bracketrightbigg/bracketleftbigg0 1
1 ˆan+1(x,y)/bracketrightbigg
=/bracketleftbiggpnpn+1
qnqn+1/bracketrightbigg
just like in the regular case (since we applied T(x)). Note that the index of the convergents may not be the
same as for the convergents of it’s regular continued fraction.
18Now when yn∈[0,1−xn], if ˆan+1(x,y) = 1 we ﬁnd
Mn+1,x,y=/bracketleftbigg
pn−1pn
qn−1qn/bracketrightbigg/bracketleftbigg
1 0
1 1/bracketrightbigg
=/bracketleftbigg
pn−1+pnpn
qn−1+qnqn/bracketrightbigg
.
We see that we did not update the second column but instead replace d the ﬁrst by the mediant (corresponding
to adding the mediant in the list of convergents). To illustrate what w ould happen in case of a singularization,
for ease of notation assume that the matrix Mn,x,yhas the convergents of the regular continued fraction as
columns for n−1 andnand we will be using the n+1’th digit of xin the next step. When an+1(x) = 1 then
an+1(1−x) =an+2(x)+1. We ﬁnd
/bracketleftbiggpn−1pn
qn−1qn/bracketrightbigg/bracketleftbigg1an+2(x)
1an+2(x)+1/bracketrightbigg
=/bracketleftbiggpn−1+pnan+2(x)pn−1+(an+2(x)+1)pn
qn−1+qnan+2(x)qn−1+(an+2(x)+1)qn/bracketrightbigg
=/bracketleftbiggpn+1pn+2
qn+1qn+2/bracketrightbigg
.
Here we see that we skipped a convergent because of a singularizat ion. (Here the index might have shifted as
well.) Since we now know how to relate the inverse branches to the seq uence of convergents qn,y(x) and, in the
same way, qn,y(x) we are in a position to proving the following:
Theorem 4.2. There is a generic growth rate of qn,y(x): there exists some C >1so that for almost every
choice of x,y, we have
lim
n→∞1
nlog(qn,y(x)) =C.
However, for qn,y(x)have no such generic growth rate: for any C >1, for almost all x,ywe have
liminf
n→∞qn,y(x)
Cn= 0.
Proof.For the fast map, for ( x,y)∈PGi,n, letn=a1(x) =⌊1/x⌋. InPGi,nwe ﬁrst do iinsertions and then
apply the Gauss map, so we ﬁnd the following matrix multiplication
Ax,y:=/bracketleftbigg
1 0
1 1/bracketrightbiggi/bracketleftbigg
0 1
1n−i/bracketrightbigg
=/bracketleftbigg
0 1
1n/bracketrightbigg
which is the same as applying the Gauss map directly. In this case we ﬁn d the eigenvalues
n±√
n2+4
2.
When (x,y)∈PRi,n, then we have n−1 insertions followed by a singularization. Note that a2(x) =i. We ﬁnd
the following matrix multiplication
Ax,y:=/bracketleftbigg
1 0
1 1/bracketrightbiggn−1/bracketleftbigg
1i
1i+1/bracketrightbigg
=/bracketleftbigg
1i
n ni+1/bracketrightbigg
.
The eigenvalues are
(ni+2)±/radicalbig
(ni+2)2−4
2.
We see that we are almost in the same situation as for Mfast(x,y), see (13). Here, though, the principal
eigenvalues are smaller in case ( x,y)∈PRi,n. This gives us the integrability of log+/⌊ard⌊lAx,y/⌊ard⌊l. The second
eigenvalue is larger than the case of Mfast(x,y) and therefore the norm of the inverse is smaller. This gives
us integrability of log+/⌊ard⌊lA−1
x,y/⌊ard⌊l. The integrability of the logarithm of these norms, the multiplicative e rgodic
theorem, and the relation of the matrices with the sequences qn,y(x) gives us the desired result. The second
statement in the theorem follows from the fact that Tslowhas an inﬁnite invariant measure. Informally put,
ˆTslowwill perform ‘too many insertions of mediants’ to permit any generic e xponential growth rate.
4.1 Standard Continued Fractions - Always Follow the Green Edge
When we construct the ﬁrst-return map on [1 −x,1], we get a rotation by T(x), the Gauss map. It follows that
ify0is chosen so that ynisalwaysin the interval [1 −xn,1], thenxn=Tn(x0) for every n.
Our procedure then amounts to “within the interval whose length is the rotation amount x, we construct the
ﬁrst-return map on the interval which is the ﬁrst T(x) proportion, and on this interval we reverse orientation.”
Let us let Inbe the interval in the original interval [0 ,1] on which we are constructing a ﬁrst-return map after
nconsecutive iterations: the ﬁrst-return map on Inis therefore isomorphic to rotation by Tn(x).
19So beginning with the interval
I1= [1−x,1]
with forward orientation, our next interval would be of length xT(x) and sharing the left endpoint with the
previous interval:
I2= [1−x,1−x+xT(x)].
In our ﬁrst-return map our orientation has been reversed, so ou r third interval will therefore be of length
xT(x)T2(x), but sharing the rightendpoint of the previous interval:
I3= [1−x+xT(x)−xT(x)T2(x),1−x+xT(x)].
Next, the interval will be of length xT(x)T2(x)T3(x), but sharing the leftendpoint, for a result of
I4= [1−x+xT(x)−xT(x)T2(x),1−x+xT(x)−xT(x)T2(x)+xT(x)T2(x)T3(x)].
Continuing in this fashion, we see inductively that Inis of length xT(x)···Tn−1(x), and that if we let ygbe
the unique point of intersection, then
yg=∞/intersectiondisplay
i=1In= 1−x+xT(x)−xT(x)T2(x)+xT(x)T2(x)T3(x)−···
Soygis the unique point in [0 ,1] whose orbit is exactly encoded by
ω= lim
n→∞σ[1,n)(A),
where each
σi:/braceleftbigg
A/ma√sto→ABai
B/ma√sto→ABai−1,
and theaiare given by the standard continued fraction representation x= [0;a1,a2,...]. Also note that this
is the unique ysuch that ˆTn
slow(x,y)∈Gfor alln∈N.
In the special case where xhas a purely periodic continued fraction expansion with period one, t his point
and the substitutions are fairly easy to compute. Suppose that
x=1
a+1
a+1
a+...= [a,a,a,... ] =√
a2+4−a
2.
The point ygcan then be found to be given by
yg= 1−x+x2−x3+x4+···=1
1+x
Then the only substitution is
σ:/braceleftbigg
A/ma√sto→ABa
B/ma√sto→ABa−1,
and the orbit of ygis encoded by
lim
n→∞σn(A).
Furthermore, for almost every xwe have
lim
n→∞1
nlog(qn,yg) =π2
12log(2)=1
2hµ(T)
wherehµ(T) is the measure theoretic entropy with respect to the invariant me asureµ. This is a classical result
and can be found for example in [ DK02,DK21] Note that we also have ˆTn
fast(x,yg) =ˆTn
slow(x,yg) so that
qn,yg(x) =qn,yg(x) for allx∈[0,1]\Q.
4.2 Other continued fractions
For other choices of ywe will get diﬀerent semi-regular continued fractions for x. By choosing yin a speciﬁc
manner according to the desired sequence of ‘red edges’ and ‘gree n edges,’ we specify a sequence of desired
insertions/singularizations, and therefore we can pick yto produce certain well-studied continued fraction
expansions. We brieﬂy discuss several examples.
204.2.1 The backward continued fractions - Always follow the red edge!
When always following the red edge we ﬁnd that the ﬁrst return map o nInis isomorphic to the rotation by
˜Tn(x) where ˜T(x) = 1−T(x). The map ˜Tis isomorphic to the map that generates the backward continued
fractions introduced in 1957 by R´ enyi [ R´57], for which the numerators are always −1. Because we never reverse
orientation in this scheme, our intervals Inwill always share their left endpoint with In−1. The point on
intersection is therefore the left endpoint of the original interval: we are producing substitutions which encode
the orbit of the point y= 0. Another way to determine yis by looking at the map ˆTslow. When for every n∈N
we have that ˆTn
slow(x,y)∈[0,1]×[0,1−x] then we always follow the red edge. For y= 0 we have ˆTn
slow(x,y) =
T(1−T(1−T(1−···T(1−x))),0). Likewise, we ﬁnd for y= 0 we have ˆTn
fast(x,y) = (T2n(x),0) for every
n∈N. Thefastapproximating sequence is therefore the sequence qn,y(x) =q2n(x), the denominators of the
even-index convergents of the standard continued fraction exp ansion of x. Equivalently, Islow(n) = [0,/⌊ard⌊lq2nx/⌊ard⌊l].
Note that for y= 1 we have ˆTslow(x,1) = (T(x),0). Therefore, we ﬁnd qn,1(x) =q2n+1(x), all odd-index
convergents, and Islow(n) = [1−/⌊ard⌊lq2n+1x/⌊ard⌊l,1].
4.2.2α-Continued Fractions - Follow the Green Edge Unless xn> α!
Letα∈[0,1]. In Figure 4, follow the green arrow if xn≤α, otherwise follow the red arrow. Correspondingly,
we perform singularization/insertion exactly when xn> α, as in [DHKM12 ]. This results in the α-continued
fraction of x, introduced in [ Nak81]. Note that when α= 1 we always follow the green edge and ﬁnd the
regular continued fraction and in case α= 0 we always follow the red edge and we ﬁnd the backward continued
fractions. Write yα, which also depends on x, for the corresponding yvalue. This is the unique ysuch that
ˆTn
slow(x,y)∈[0,α]×[1−x,1]∪(α,1]×[0,1−x]. Furthermore, for almost all x∈[0,1] we have
lim
n→∞1
nlog(qn,yα) =1
2hµα(Tα)
whereTαis Nakada’s α-continued fraction map and µαit’s invariant measure that is absolutely continuous
with respect to the Lebesgue measure. The map h(α) =hµα(Tα) is very well studied, see for example [ CT12,
CT13,KSS12,LM08,NN08].
4.2.3 Nearest Integer CF - Follow the Edge with the Shorter Interval!
Forα= 1/2, the above reduces to “follow the unique arrow for which xn+1<1/2.” This is equivalent to “pick
the substitutions whose word length grows the fastest” or “cons truct the ﬁrst-return map on whichever interval
is smaller.” The corresponding continued fraction representation o fxis the unique such expansion for which no
partial quotient is ever one. We ﬁnd the unique ysuch that ˆTn
slow(x,y)∈[0,1
2]×[1−x,1]∪(1
2,1]×[0,1−x].
For almost all x∈[0,1] we ﬁnd
lim
n→∞1
nlog(qn,y1
2) =1
2hµ1
2(T1
2) =π2
12log/parenleftBig√
5−1
2/parenrightBig,
see for example [ CT13].
4.2.4 A natural counterpart to α-Continued Fractions - Follow the Green Edge Unless x < α!
Now instead of following the green arrow in Figure 4whenx≤αwe follow the red arrow in that case. These
continued fractions are studied in [ KLMM20 ]. The digits of the resulting continued fraction expansions are
bounded from above. The corresponding yis the unique ysuch that ˆTn
slow(x,y)∈[0,α]×[0,x]∪(α,1]×[1−x,1].
Such continued fraction expansion generally convergeslowly: for v ery small xwe will perform many consecutive
insertions of mediants. For almost all x∈[0,1] we ﬁnd
lim
n→∞1
nlog(qn,y) = 0.
4.2.5 Lehner Continued Fractions - Follow the Edge with the Longer Interval!
Do the reverse of the nearest integer continued fraction: follow t he unique arrow for which xn+1>1/2. This
is equivalent to “pick the substitutions where the word length grows the slowest” or “construct the ﬁrst-
return map on whichever interval is longer.” The eﬀect is to always pe rform insertions whenever possible, but
never singularizations. The corresponding continued fraction rep resentation of xis the unique such expansion
for which all partial quotients are one or two. To be more precise, t hey will be the Lehner continued fraction
21introduced in [ Leh94] and studied in for example[ DK00]. The slowapproximatingsequence includes all possible
mediants within this scheme:
q0,2q0,...,a 1q0,q2,2q2,...,a 3q2,q4,...
Note that since the qnof all previous subsections necessarily appear as a subsequence o f theqnhere. Since the
previous subsection for almost every xhas sub-exponential growth, so do these. For almost every x∈[0,1] we
ﬁnd
lim
n→∞1
nlog(qn,y) = 0.
References
[AS97] J. Aaronson and American Mathematical Society. An Introduction to Inﬁnite Ergodic Theory .
Mathematical surveys and monographs. American Mathematical S ociety, 1997.
[BMD05] J´ erˆ ome Buzzi and V´ eronique Maume-Deschamps. Deca y of correlations on towers with non-h¨ older
jacobianandnon-exponentialreturntime. Discrete and Continuous Dynamical Systems , 12(4):639–
656, 2005.
[CT12] Carlo Carminati and Giulio Tiozzo. A canonical thickening of Qand the entropy of α-continued
fraction transformations. Ergodic Theory Dynam. Systems , 32(4):1249–1269, 2012.
[CT13] Carlo Carminati and Giulio Tiozzo. Tuning and plateaux for the e ntropy of α-continued fractions.
Nonlinearity , 26(4):1049–1070, 2013.
[DHKM12] K. Dajani, D. Hensley, C. Kraaikamp, and V. Masarotto. A rithmetic and ergodic properties of
‘ﬂipped’ continued fraction algorithms. Acta Arith. , 153(1):51–79, 2012.
[DK00] Karma Dajani and Cor Kraaikamp. “The mother of all continu ed fractions”. Colloq. Math. ,
84/85(part 1):109–123, 2000. Dedicated to the memory of Anzelm Iwanik.
[DK02] Karma Dajani and Cor Kraaikamp. Ergodic theory of numbers , volume 29 of Carus Mathematical
Monographs . Mathematical Association of America, Washington, DC, 2002.
[DK21] K. Dajani and C. Kalle. A First Course in Ergodic Theory . Mathematical surveysand monographs.
CRC Press, 2021.
[IN88] Sh. Ito and H. Nakada. Approximations of real numbers by t he sequence{nα}and their metrical
theory.Acta Math. Hungar. , 52(1-2):91–100, 1988.
[KKV17] Charlene Kalle, Tom Kempton, and Evgeny Verbitskiy. The ra ndom continued fraction transfor-
mation. Nonlinearity , 30(3):1182–1203, 2017.
[KLMM20] Charlene Kalle, Niels Langeveld, Marta Maggioni, and Sara Mu nday. Matching for a family of
inﬁnite measure continued fraction transformations. Discrete Contin. Dyn. Syst. , 40(11):6309–
6330, 2020.
[Kra91] Cor Kraaikamp. A new class of continued fraction expansion s.Acta Arith. , 57(1):1–39, 1991.
[KSS12] Cor Kraaikamp, Thomas A. Schmidt, and Wolfgang Steiner. N atural extensions and entropy of
α-continued fractions. Nonlinearity , 25(8):2207–2243, 2012.
[Leh94] Joseph Lehner. Semiregular continued fractions whose pa rtial denominators are 1 or 2. In The
mathematical legacy of Wilhelm Magnus: groups, geometry an d special functions (Brooklyn, NY,
1992), volume 169 of Contemp. Math. , pages 407–410. Amer. Math. Soc., Providence, RI, 1994.
[LM08] Laura Luzzi and Stefano Marmi. On the entropy of Japanes e continued fractions. Discrete Contin.
Dyn. Syst. , 20(3):673–711, 2008.
[Nak81] Hitoshi Nakada. Metrical theory for a class of continued f raction transformations and their natural
extensions. Tokyo J. Math. , 4(2):399–426, 1981.
[NN08] Hitoshi Nakada and Rie Natsui. The non-monotonicity of the e ntropy of α-continued fraction
transformations. Nonlinearity , 21(6):1207–1225, 2008.
[R´57] Alfr´ ed R´ enyi. On algorithms for the generation of real numbe rs.Magyar Tud. Akad. Mat. Fiz.
Oszt. K¨ ozl. , 7:265–293, 1957.
22