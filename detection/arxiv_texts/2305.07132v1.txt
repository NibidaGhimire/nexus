UNDER SUBMISSION 1
Tackling Interpretability in Audio ClassiÔ¨Åcation
Networks with Non-negative Matrix Factorization
Jayneel Parekh, Sanjeel Parekh, Pavlo Mozharovskyi, Ga ¬®el Richard, Florence d‚ÄôAlch ¬¥e-Buc
Abstract ‚ÄîThis paper tackles two major problem settings for
interpretability of audio processing networks, post-hoc and by-
design interpretation. For post-hoc interpretation, we aim to in-
terpret decisions of a network in terms of high-level audio objects
that are also listenable for the end-user. This is extended to
present an inherently interpretable model with high performance.
To this end, we propose a novel interpreter design that incor-
porates non-negative matrix factorization (NMF). In particular,
an interpreter is trained to generate a regularized intermediate
embedding from hidden layers of a target network, learnt as time-
activations of a pre-learnt NMF dictionary. Our methodology
allows us to generate intuitive audio-based interpretations that
explicitly enhance parts of the input signal most relevant for a
network‚Äôs decision. We demonstrate our method‚Äôs applicability
on a variety of classiÔ¨Åcation tasks, including multi-label data for
real-world audio and music.
Index Terms ‚ÄîAudio interpretability, explainability, by-design
interpretable models, audio convolutional networks, non-negative
matrix factorization
I. I NTRODUCTION
Deep learning models, while state-of-the-art for several
tasks in domains such as computer vision, natural language
processing and audio, are typically not interpretable. Their
increasing use, in decision-critical domains, in every-day ap-
plications, raises the problem of interpreting their decisions.
This issue has been grappled with in two different ways by
the research community. The Ô¨Årst one relies on the availability
of a predictive model trained and optimized for performance
but not for interpretability and aims to devise an additional
approach to interpret the given model. This problem setting is
usually referred to post-hoc interpretation . The other setting
aims to build an interpretable predictive model from the
data. The challenge here is to demonstrate high classiÔ¨Åcation
performance while maintaining interpretability in the same
model. This setting is often referred to as by-design interpreta-
tion problem. Real-world scenarios of utilizing interpretabil-
ity of networks can occur under variety of constraints and
demands regarding deployment, level of interpretability and
performance. Thus, from a practical standpoint, both problem
settings hold independent value.
In this paper, our aim is to address both problems for audio
classiÔ¨Åcation networks while proposing a system more suited
for understanding interpretations for audio modality than other
common methods in literature.
Manuscript received April, 10 2023; Jayneel Parekh, Sanjeel Parekh, Pavlo
Mozharovskyi, Ga ¬®el Richard, Florence d‚ÄôAlch ¬¥e-Buc are with the Laboratoire
de Traitement et Communication de l‚ÄôInformation (LTCI), T ¬¥el¬¥ecom
Paris, Institut Polytechnique de Paris, 91120 Palaiseau, France. (e-mail:
fjayneel.parekh,pavlo.mozharovskyi,Ô¨Çorence.dalche,gael.richard g@telecom-
paris.fr, sanjeelparekh@gmail.com)An ideal interpreter is supposed to offer insights about a
model‚Äôs decision in an understandable fashion to humans. In
the case of audio classiÔ¨Åcation, there are certain desirable
traits for an interpreter that effectively help to fulÔ¨Ål this
purpose. Firstly, we advocate that the interpretations should
be generated in terms of high-level audio objects. Even more
importantly, the interpretation should be listenable for an end-
user. The rationale behind posing these traits as desirable is as
follows: Audio scenes are often composed of multiple high-
level audio objects [1]. Moreover, understanding events/scenes
through the notion of audio objects also aligns with cognitive
development in human and animals [2], [3]. Listenability is
essential since it is signiÔ¨Åcantly more intuitive and easier
to listen to an interpretation rather than visualizing it in its
time-frequency representation (eg. spectrogram). Usefulness
for both the traits can be reinforced through an example.
Imagine an audio-based surveillance system for a house raising
an alarm for break-in. An interpreter can be expected to be
able to localize the event among a host of concurrent events
that triggered the alarm. If for example ‚Äôglass-breaking‚Äô is the
triggering event that the interpreter recognizes in the input, a
human would Ô¨Ånd it easier to understand, if they can hear the
interpretation rather than visualize it on a spectrogram.
To this end, we propose an interpreter that relies on pro-
cessing selected hidden representations of the classiÔ¨Åer by a
neural network to extract an intermediate embedding. This
intermediate encoding is regularized in multiple ways, the two
essential ones being: (i) Mimicking the classiÔ¨Åer output to be
able to interpret its decisions, and (ii) Reconstruct the input
through the help of a dictionary of spectral patterns. The latter
loss and its design is strongly inspired by the structure in Non-
negative Matrix Factorization (NMF, [4]), known to provide
part-based decompositions. The loss is crucial in imposing a
highly understandable meaning of ‚Äútime activations‚Äù on the
intermediate embedding. This decomposition structure also
allows the interpreter to beneÔ¨Åt from Ô¨Åltering information from
the input. It‚Äôs worth emphasizing that audio interpretability
is not the same as classical tasks of separation or denoising.
These tasks involve recovering complete object of interest in
the output audio. On the other hand, a classiÔ¨Åer network might
focus more on salient regions. When interpreting its decision
and making it listenable we expect to uncover such regions
and not necessarily the complete object of interest.
This paper is an extension of our work on post-hoc in-
terpretability [5]. It differs in two signiÔ¨Åcant ways: (a) In
addition to post-hoc, we present a by-design interpretable
network that has state-of-the-art classiÔ¨Åcation performance
among interpretable models for audio. To do so, we modifyarXiv:2305.07132v1  [cs.SD]  11 May 2023UNDER SUBMISSION 2
the training procedure to allow the hidden layers of the
classiÔ¨Åer to be Ô¨Åne-tuned, and apply a classiÔ¨Åcation loss at
its output. The interpreter‚Äôs architecture and objectives remain
the same for both problems. (b) We demonstrate the utility of
the proposed methods on music data with results on largest
publicly available polyphonic music instrument recognition
dataset, OpenMIC-2018 [6].
In summary, we make the following contributions:
We build a holistic approach that generates listenable
concept-based interpretations to tackle post-hoc and by-
design interpretability for audio classiÔ¨Åcation networks.
We present an original formulation that constrains the
interpreter encoding through two loss functions, one for
input reconstruction through NMF dictionary and the
other for Ô¨Ådelity to the network‚Äôs decision. From a
learning perspective, we show a new way to link NMF
with deep neural networks, especially for generating
interpretations.
We extensively evaluate on three popular audio event
analysis benchmarks, tackling both multi‚Äìclass and
multi‚Äìlabel classiÔ¨Åcation tasks. The dataset for the latter
is very challenging due to its collection in noisy real‚Äì
world settings. Our method‚Äôs design allows us to simulate
feature removal and perform faithfulness evaluation.
II. R ELATED WORKS
We organize the discussion about literature in three parts.
We Ô¨Årst cover interpretability works in a broad sense, followed
by audio speciÔ¨Åc methods. We conclude the section by dis-
cussion on applications of NMF to audio signal processing.
A. General Interpretability literature
Feature attribution : The vast majority of interpretability
literature is covered under feature attribution methods. They
are a class of methods which offer interpretations through
input feature importance or selection. In case of post-hoc
interpretation, this includes perturbation based approaches [7]‚Äì
[9], as well as saliency map based approaches [10]‚Äì[14].
Perturbation based approaches rely on observing model output
on many locally perturbed versions of the input to determine
importance of each individual feature for the decision. Part of
the research challenge for these methods is to deÔ¨Åne ‚Äúmean-
ingful‚Äù perturbations [15]. Saliency map based approaches
typically generate interpretations through modiÔ¨Åed gradient
backpropagation [10], [13], but some also utilize upsampled
versions of intermediate activation maps [12], [14]. This form
of interpretation is also common for by-design interpretable
models. Common ways of training such models involve mod-
ifying the architecture, loss function (or both) to incorporate
interpretability in the model.
Interpretation beyond attribution : Feature attribution
methods have been under scrutiny for their robustness and
faithfulness [16], [17]. This has overlapped with increasing
amount of research for both post-hoc and by-design inter-
pretation models to develop systems that offer interpretations
through different means. Prior research has now proposed
systems that provide interpretations via logical rules [18],counterfactuals [19] and even natural language [20]. Each
of them can potentially be a more suited choice for certain
problem domains and use-cases. Our approach uses high-
level audio objects for interpretation. This aspect renders it
closer to prototype and concept-based approaches. Prototype-
based approaches [21], [22] tackle by-design interpretability
by learning a predeÔ¨Åned number of embeddings which are
encouraged to represent singular training datapoints. The Ô¨Ånal
classiÔ¨Åcation decision is made based on similarity of given test
sample to all the prototypes. Concept-based approaches aim to
represent high-level concepts explicitly and subsequently offer
interpretation in terms of them. An interesting approach is that
of FLINT [23] with whom we share the idea of utilizing the
hidden layers and loss functions to encourage interpretability.
However we crucially differ from FLINT and other related
approaches in concept representation and their applicability
for audio interpretations. FLINT represents concepts by a
dictionary of attribute functions over input space. The learnt
concepts are not obviously comprehensible to a user, requiring
a separate visualization pipeline to get insights. Approaches
based on TCA V [24], such as ACE [25], ConceptSHAP [26],
deÔ¨Åne concept using a set of images and learn a representation
for it in terms of hidden layers of the network, termed as
concept activation vector (CA V). These designs for concepts
are not related to our NMF-inspired dictionary representation.
Importantly, none of the above mentioned approaches can
generate listenable interpretations which is key for understand-
ability of audio processing networks.
B. Audio Interpretability
Compared to literature in other domains like text or images,
work on interpretability with audio signals is considerably
sparse. Usefulness of saliency-map based methods for audio
interpretability has been illustrated by prior works. Becker et
al. [27] demonstrate the use of popular Layerwise Relevance
Propagation (LRP) [13] algorithm for audio digit recognition
task. Won et al. [28] highlight the use of attention for music
tagging task. Muckenhirn et al. [29] use GuidedBackprop [10]
to analyze CNNs operating on 1D waveform. However, these
methods do not address the issue of listenability of interpre-
tations and moreover are limited to post-hoc interpretation. A
few works based on the LIME algorithm [7] have attempted
to address this issue. SLIME [30], [31] proposed to segment
the input along time or frequency. The input is perturbed by
switching ‚Äùon/off‚Äù the individual segments. AudioLIME [32],
[33] proposed to separate the input using predeÔ¨Åned sources
to create the simpliÔ¨Åed representation. AudioLIME arguably
generates more meaningful interpretations than SLIME as it
relies on audio objects readily listenable for end-users. How-
ever, it suffers from limited applicability, requiring existence
of known and meaningful predeÔ¨Åned sources that compose the
input audio. More recently, [34] extended the idea of TCA V to
represent concepts in music data. The supervised approach re-
quires the overhead of human annotation of concepts, whereas
the unsupervised approached based on non-negative tensor
decomposition faces the challenge of meaningful learning of
concepts. APNet [35] proposes an interpretable system by-
design that extends prototypical networks [21], [22] for audioUNDER SUBMISSION 3
input by deÔ¨Åning a more suitable distance measure for audio
prototypes. An advantage of our approach over all the previous
methods is its ability to address both post-hoc and by-design
problem settings. Another unique feature of the approach is
its means of generating interpretations, relying on a dictionary
of NMF components for the same. From the point of view of
interpretability, this is a novel strategy to gain insights about
a model.
C. NMF applications for audio
NMF is a data decomposition technique popularized by Lee
and Seung [4] as a method to learn ‚Äúparts of an object‚Äù.
It has since been used widely within the audio community
to tackle source separation [36], denoising [37], inpainting
[38] and transcription [39], [40]. Typically, a nonnegative
time-frequency audio representation is decomposed into two
nonnegative factors namely, spectral patterns or dictionary
matrix and their time activations. Its traditional usage as
a supervised dictionary or feature learning method involves
learning class-wise dictionaries over training data [41]. Time
activations, the so-called features, can then be generated for
any input by projecting it onto the learnt dictionaries. These
features can subsequently be used for downstream tasks such
as classiÔ¨Åcation. In this section we focus on prior works that
combine NMF with deep architectures.
Bisot et al. [42] couple NMF-based features with neural
networks to boost performance of acoustic scene classiÔ¨Åcation.
NMF has also been successfully employed with audio‚Äìvisual
deep learning models for separation [43] and classiÔ¨Åcation
[44].
Iterations of NMF optimization algorithms can be unfolded
as novel deep neural networks. This observation has led
to development of ‚ÄúDeep NMF‚Äù methods. In particular, Le
Roux et al. [45] unfold the multiplicative updates of NMF
parameters into a deep network for speech separation. Wisdom
et al. [46] apply this strategy to iterative soft thesholding
algorithm to propose deep recurrent NMF.
While these works share with us the high-level idea of com-
bining neural networks and NMF, there is no overlap between
our goals and methodologies. Unlike aforementioned studies,
we wish to investigate a classiÔ¨Åer‚Äôs decision using NMF as a
regularizer. Furthermore, to our best knowledge, attempting
to regress temporal activations of a Ô¨Åxed NMF dictionary
by accessing intermediate layers of an audio classiÔ¨Åcation
network is novel even within the NMF literature.
III. S YSTEM DESIGN
We organize this section as follows: We start with a brief
note on notation used throughout the paper. We describe the
setup of our framework to address post-hoc interpretation in
section III-B. This is extended to address by-design interpreta-
tion in section III-C. We expound on the speciÔ¨Åc architectural
details common to both problem settings in section III-D and
conclude the section by detailing how we generate interpreta-
tions with our design in section III-E.A. Data Notation
We denote a training dataset by S:= (x;y)N
i=1, wherex2X
is the time domain audio signal and y2Y, a label vector. The
label vector could be a one-hot or binary encoding depending
upon a multi-class or multi-label dataset, respectively. For
listenable interpretations through NMF, we favor a represen-
tation ofxthat can be easily inverted back to the time-
domain and use a log‚Äìmagnitude spectrogram X2RFT
that is computed by applying an element-wise transformation
x0!log(1 +x0)on the magnitude spectrogram with F
frequency bins and Ttime frames. This is preferred over
using magnitude spectrograms as it corresponds more closely
to human perception of sound intensity [47]. A deep neural
network classiÔ¨Åer for post-hoc interpretations is denoted as
f:X!Y .
B. Post-hoc Interpretation
When addressing the problem of post-hoc interpretation, the
classiÔ¨Åerfwill be pre-trained and then Ô¨Åxed throughout. We
describe now the components of the interpreter and what are
its inputs and outputs.
Overview The system design is illustrated in Ô¨Ågure 1. The
interpreter is designed to have access to hidden representations
of the classiÔ¨Åer and is tasked to produce an intermediate
embedding through the function 	. This embedding is placed
under certain constraints via function and a pre-learnt
dictionary of NMF components W. These constraints impose
a highly meaningful structure on the embedding and help
in interpreting the decision f(x). We discuss the constrains,
which form the core of our approach, in this subsection. The
precise architectures of 	andand optimization problem
used to pre-learn Ware covered later in Sec. III-D.
SpeciÔ¨Åcally, hidden layer outputs of the classiÔ¨Åer f, taken
as input by the interpreter, are denoted as fI(x)2Z. They are
processed through the function 	 :Z!RKT
+ , modelled as
a neural network. This produces an intermediate encoding. For
simplicity, we will denote this encoding generated from hidden
layers as HI(x) = 	fI(x), a function over input x. The
constraints on this encoding, implemented as loss functions
are as follows:
Loss 1 (Fidelity loss): To be able to identify the relevant
signal for interpretation, we constrain HI(x)to approximate
classiÔ¨Åers output probabilities f(x)through the function  :
RKT
+! Y . The term (HI(x))is also referred to as
interpreter‚Äôs output. We implement this constraint as a loss
function by minimizing the generalized cross-entropy loss
between (HI(x))andf(x). We refer to it as the Ô¨Ådelity
lossLFID. Denoting the parameters of 	;asV	;V, for
multi-class classiÔ¨Åcation the loss can be written as,
LFID(x;V	;V) = f(x)|log(( HI(x))) (1)
On the other hand, for multi-label classiÔ¨Åcation this loss reads,
LFID(x;V	;V) = X
f(x)log(( HI(x)))
+ (1 f(x))log(1 (HI(x))):(2)
Heredenotes element-wise multiplication.UNDER SUBMISSION 4
Fig. 1: System overview : The core design common to both post-hoc and by-design interpretation. The interpreter (indicated in
blue) accesses hidden layer outputs of the classiÔ¨Åer. These are used to predict an intermediate encoding. Through regularization
terms, we encourage this encoding to both mimic the classiÔ¨Åer‚Äôs output and also serve as the time activations of a pre-learnt
NMF dictionary. In post-hoc interpretation, the classiÔ¨Åer is pre-trained and Ô¨Åxed, and only the interpreter is trained. For
by-design interpretation we train both jointly and make Ô¨Ånal predictions using output of interpreter.
Loss 2 (Reconstruction loss): We additionally constrain
HI(x)to be able to reconstruct the input audio using pre-
learnt dictionary W2RFK
+ . This constraint asks to decom-
pose input log-magnitude spectrogram as XWHI(x), that
is, a product of two non-negative matrices. This loss is based
on popular non-negative matrix factorization. Crucially, this
allows us to consider HI(x)as a time activation matrix for
W. We refer to this as the reconstruction loss, denoted as
LNMF.
LNMF(x;V	) =kX WHI(x)k2
2: (3)
Loss 3 (Sparsity loss): In addition toLFIDandLNMF,
we impose `1regularization on HI(x)to encourage well-
behavedness, especially for large dictionary sizes [48].
Training optimization . The complete loss function over our
training datasetScan thus be given as:
L(V	;V) =X
x2SLFID(x;V	;V)+LNMF(x;V	)+jjHI(x)jj1
(4)
where;0are loss hyperparameters. All the parameters
of the system are constituted in the functions 	;and dictio-
naryW. Since Wis pre-learnt and Ô¨Åxed, the training loss Lis
optimized only w.r.t V	;V. As a reminder, when training the
interpreter for post-hoc analysis, the classiÔ¨Åer network is kept
Ô¨Åxed. The Ô¨Ånal optimization problem addressed for post-hoc
interpretation writes as follows:
^	;^ = arg min
	;L(V	;V) (5)
C. By-design Interpretation
Interestingly, the same framework can also be utilized to
train an inherently interpretable model. As a Ô¨Årst step, we
propose the following function to be used for making Ô¨Ånal
predictions
g:X!Y;g(x) = 	fI(x)which is a mixture of interpreter and classiÔ¨Åer layers. One
might be tempted to employ the same training mechanism
for by-design problem as done for post-hoc interpretation.
However, there is a difference in the problem setting we need
to adapt for. Namely, the classiÔ¨Åer layers are not trained for
prediction as before. This implies that we cannot simply aim
to generate meaningful representations from it as is.
To remedy the difÔ¨Åculty, we modify the training in two
different ways: (i) Layers of fare now modiÔ¨Åed by back-
propagating all interpreter losses to them, and thus are now
jointly trained with the interpreter. (ii) We modify the loss
function to include an additional prediction loss on the output
f(x)to train all the layers in f. The training loss function and
optimization problem write as following:
Lf(x;Vf) = y|log(f(x))
LNMF(x;V	;Vf) =kX WHI(x)k2
2
LFID(x;V	;V;Vf) = f(x)|log(( HI(x)))
L(V	;V;Vf) =X
x2SLf(x;Vf) +LFID(x;V	;V;Vf)
+LNMF(x;V	;Vf) +jjHI(x)jj1
^	;^;^f= arg min
	;;fL(V	;V;Vf)
A reader might question the need for applying a prediction
loss at output of fwhen the function gdescribed above is
proposed to make Ô¨Ånal predictions. This is indeed a reasonable
variant of our current choice and we resolve this issue by
comparing the performance of both systems in experiments in
section V-D
D. Filling the gaps
It should be noted that the network architectures and other
implementation details remain the same in both problemUNDER SUBMISSION 5
settings. We now cover the remaining architectural details of
	;and the algorithm for pre-learning W.
Design of 	. The network 	is tasked with producing
the encoding HI(x)2RKT
+ from the set of convolutional
feature maps of the classiÔ¨Åer, given by fI(x). These feature
maps potentially originate from different layers and thus can
be of different resolutions. To perform joint processing on
them, each one is Ô¨Årst appropriately transformed to ensure
same width and height dimensions. The subsequent layers
process these maps through some convolutional (with ReLU
activation) and resampling layers. However, this composition
is based on certain important aspects. Firstly, audio feature
maps of CNNs with spectrogram-like inputs contain the notion
of time and frequency along the width and height dimensions.
Secondly, our goal with this network is to process a 3D rep-
resentation of feature patterns across time and frequency, and
convert it to a 2D intermediate encoding that can serve as time
activation matrix of size KT. To achieve this, the subsequent
convolutional layers continuously decrease resolution on the
frequency axis and increase resolution the time axis to T
frames. Furthermore, the input axis for number of feature maps
corresponds to the axis of number of components Kin output
of	, equal to the number of components in dictionary W.
Design of . The goal of this network is to mimic the output
f(x)by processing HI(x)This directly helps in shaping
HI(x)to interpret f(x). An important consideration for
designing was to keep its operations on HI(x)interpretable.
This helps during the interpretation phase in easily quantifying
how different parts of HI(x)inÔ¨Çuence the interpreters output.
It is thus composed of two parts. The Ô¨Årst part pools activations
HI(x)across time. This pooling can be implemented in
multiple ways, for eg. max or average pooling. However, we
opt for an intermediate style of attention‚Äìbased pooling [49],
i.e.,z=PT
t=1HI(x)a, where a2RTare the attention
weights and z2RKis the pooled vector. The pooled
representation vector is passed through a linear layer. This
is followed by an appropriate activation function to convert
its output to probabilities, that is, softmax for multi-class
classiÔ¨Åcation and sigmo ¬®ƒ±d for multi-label classiÔ¨Åcation.
Pre-learning W. The non-negative matrix Wforms an
integral part of the interpreter design. It is pre-learnt from
the input data, and essential in formulating the reconstruction
lossLNMF. We employ Sparse-NMF [48] for the pre-learning.
The following optimization problem is solved through multi-
plicative updates to pre-learn W:
min D(XtrainjWH ) +kHk1
subject to W0;H0;
kwkk= 1;8k:(6)
where Xtrainis a subset of the training data S. Note that its
construction is dataset dependent and will be covered in exper-
iments. Here D(:j:)is a divergence cost function. In practice,
euclidean distance is used. Training audio Ô¨Åles are converted
into log-magnitude spectrogram space for factorization.
E. Generating Interpretations
Having described the goals and details of all components
of our framework, we Ô¨Ånally discuss how the interpretationsare generated. To generate audio that interprets the classiÔ¨Åer‚Äôs
decision for a sample xand a predicted class c, we follow
a two-step procedure: The Ô¨Årst step consists of identifying
the components which are considered ‚Äúimportant‚Äù for the
prediction. This is determined by estimating their relevance
using the pooled time activations in and the weights for
linear layer. Precisely, given a sample x, the pooled activations
are computed as z=HI(x)a. Denoting the weights for
classcin the linear layer as w
c, the relevance of component
kis estimated as rk;c;x =(zkw
c;k)
maxljzlw
c;lj. This is essentially
the normalized contribution of component kin the output
logit for class c. To select the ‚Äúimportant‚Äù components, we
simply threshold the relevance via a parameter 2(0;1)as,
Lc;x=fk:rk;c;x>g.
The second step consists of estimating a time domain signal
for each relevant component k2Lc;xand also for set Lc;xas
a whole. In this paper, we refer to the latter as the generated
interpretation audio, xint. For certain classes, it may also be
meaningful to listen to each individual component, xk. As
discussed earlier under NMF basics, estimating time domain
signals from spectral patterns and their activations typically
involves a soft‚Äìmasking and inverse STFT procedure. We
detail this step with appropriate equations in Algorithm 1.
Algorithm 1 Audio interpretation generation
1:Input: log-magnitude spectrogram X, input phase Px
components W =fw1;:::;wKg, time activations
HI(x) = [ hI
1(x);:::;hI
K(x)]|, set of selected compo-
nentsLc;x=fk1;:::;kBg.
2:for allk2Lc;xdo
3:Xk wkhI
k(x)|
PK
l=1wlhI
l(x)|Xf// Soft maskingg
4:xk=INV(Xk;Px)f// Inverse STFTg
5:end for
6:Xint P
k2Lc;xXk
7:xint=INV(Xint;Px)
8:Output:fxk1;:::;xkBg,xint
IV. E XPERIMENTAL DESIGN
Most of the experimental settings remain the same for
post-hoc and by-design interpretations since the underlying
architecture and the loss functions directly affecting interpreter
are identical. Thus, the datasets, audio representation used
by the network and the learnt dictionaries remain unchanged.
However, there are some differences in training and evaluation
that will be discussed explicitly. We start by covering the above
details in section IV-A-IV-B. We discuss the interpretation
evaluation strategies relevant for both problems in section
IV-C, including all the systems evaluated.
A. Datasets
We experiment with three datasets covering different types
of learning tasks, source data etc. We discuss each of them in
greater detail below.UNDER SUBMISSION 6
1) ESC50: ESC-50 [50] is a popular benchmark for envi-
ronmental sound classiÔ¨Åcation task. It is a multi-class dataset
that contains 2000 audio recordings of 50 different envi-
ronmental sounds. The classes are broadly arranged in Ô¨Åve
categories namely, animals, natural soundscapes/water sounds,
human/non-speech sounds, interior/domestic sounds, exte-
rior/urban noises. Each clip is Ô¨Åve-seconds long and extracted
from publicly available recordings on the freesound.org
project. The dataset is prearranged into 5 folds.
2) SONYC-UST: The DCASE task used a very challenging
real-world dataset called Sounds of New York City-Urban
Sound Tagging (SONYC-UST) [51]. It contains audio col-
lected from multiple sensors placed in the New York City
to monitor noise pollution. It consists of eight coarse-level
and 20 Ô¨Åne-level labels. We opt for the coarse-level labeling
task that involves multi-label classiÔ¨Åcation into: ‚Äòengine‚Äô,
‚Äòmachinery-impact‚Äô, ‚Äònon-machinery-impact‚Äô, ‚Äòpowered-saw‚Äô,
‚Äòalert-signals‚Äô, ‚Äòmusic‚Äô, ‚Äòhuman-voice‚Äô, ‚Äòdog‚Äô. This task is
highly challenging for several reasons: (i) since it is real-world
audio, the samples contain a very high level of background
noise, (ii) the audio sources corresponding to the classes are
often weak in intensity, as they are not necessarily close to
the sensors, (iii) some classes may also be highly localized in
time and more challenging to detect, (iv) lastly, noisy audio
also makes it difÔ¨Åcult to annotate, leading to labeling noise.
This is especially true for training data labeled by volunteers.
3) OpenMIC-2018: The OpenMIC-2018 dataset [6] is com-
posed of 20000 polyphonic audio recordings annotated with
weak labels from among 20 instrument classes. The dataset
was created by querying the content available on Free Music
Archive under the Creatives Commons license with AudioSet
concept ontology and using a multi-instrument estimator
model trained on AudioSet data to suggest candidates for
annotation. Each recording/clip is 10 seconds long. A single
sample generally consists of weak labels of only a small subset
of classes. Each instrument class has at least 500 positive
and 1500 total annotated samples. Compared to SONYC-
UST, the number of positive samples intra class and inter
class are considerably more balanced. It is currently the only
large publicly available dataset with multi-label annotation for
polyphonic audio.
B. Implementation details
1) ClassiÔ¨Åcation network: We interpret a VGG-style con-
volutional neural network proposed by Kumar et al. [52]. This
network was chosen due to its popularity and applicability for
various audio scene and event classiÔ¨Åcation tasks. It can pro-
cess variable length audio and has been pretrained on AudioSet
[53], a large-scale weakly labeled dataset for sound events.
It takes as input a log-mel spectrogram. The architecture
broadly consists of six convolutional blocks (B1‚ÄìB6) followed
by a convolutional layer with pooling for Ô¨Ånal prediction.
Most convolutional blocks consist of two sets of conv2D
+ batch norm + ReLU layers followed by a max pooling
layer. We Ô¨Åne-tune this network on each dataset separately
before training our system for any post-hoc interpretations.
For ESC-50, we modify only fully-connected layers after theESC-50 (in %) SONYC-UST OpenMIC-2018
System top-1 macro-AUPRC avg-weighted-F1
Human accuracy [50] 81.3  
ESC50-CNN baseline [50] 64.5  
Arnault et al. [54]  0.649
Koutini et al. [55]   0.822
VGGish [6], [56]  0.510 0.785
Current-f 82.5 0.601 0.831
TABLE I: Benchmarking performance of pre-trained classiÔ¨Åer
ffor post-hoc interpretation.
convolutional blocks while for SONYC-UST and OpenMIC-
2018, we modify all the layers during Ô¨Åne-tuning.
ClassiÔ¨Åer performance . On ESC-50, the classiÔ¨Åer is evalu-
ated using 5-fold cross-validation. It achieves an accuracy of
82:51:9% over the 5 folds, higher than the average human
accuracy of 81.3%. SONYC-UST is an unbalanced multi-
label dataset. The evaluation is done using AUPRC based
metrics. Our Ô¨Åne-tuned classiÔ¨Åer achieves a macro-AUPRC
(ofÔ¨Åcial metric for DCASE 2020 challenge) of 0:601. This
is higher than the DCASE baseline performance of 0.510
and comparable to the best performing system macro-AUPRC
of 0.649 [54]. Note that it is obtained without use of data
augmentation or additional strategies to improve performance.
OpenMIC-2018 is a relatively balanced multi-label dataset. To
evaluate our trained classiÔ¨Åer, we use the weighted average
F1-score metric, proposed in the original paper. The metric
computes for each class a weighted average of F1-scores
over the positive and negative samples. The Ô¨Ånal score is the
average over 20 classes. Our classiÔ¨Åer achieves Ô¨Ånal score of
0.83, better than the VGGish based baseline score of 0.78
and competitive with other recent models. These details are
tabulated in Tab. I. As noted earlier, the pre-training is only
executed for post-hoc interpretations.
2) Audio time-frequency representation: For both the tasks,
we perform the same audio pre-processing steps. All audio
Ô¨Åles are sampled at 44.1kHz. STFT is computed with a
1024-pt FFT and 512 sample hop size, which corresponds to
about 23ms window size and 11.5ms hopsize. The log-mel
spectrogram is extracted using 128 mel-bands.
3) Dictionary learning: The matrix on which we apply
sparse-NMF to learn W,Xtrain, is constructed differently
for each dataset due to their speciÔ¨Åc properties. For ESC-
50,Xtrainis constructed by concatenating the log‚Äìmagnitude
spectrograms corresponding to each sample in the training
data of the cross-validation fold (1600 samples for each
fold). SONYC-UST however, is an imbalanced multilabel
dataset with very strong presence of background noise. A
procedure to learn components, as for ESC-50, yields many
components capturing signiÔ¨Åcant background noise, affecting
understandability of interpretations. Hence, we process this
dataset differently. We Ô¨Årst learn Wnoise, that is, a set of 10
components to model noise using training samples with no
positive label. Then, for each class, we randomly select 700
positively-labeled samples from all training data and learn
10 new components (per class) with Wnoise held Ô¨Åxed for
noise modeling. All 108 = 80 components are stackedUNDER SUBMISSION 7
Dataset   K # of epochs
ESC-50 10.0 0.8 100 35
SONYC-UST 10.0 0.8 80 21
OpenMIC-2018 5.0 0.2 300 21
TABLE II: Hyperparameters for all datasets for post-hoc
interpretation
Dataset    K # of epochs
ESC-50 1.0 3.0 0.2 100 51
SONYC-UST 1.0 4.0 0.2 80 21
OpenMIC-2018 1.0 3.0 0.2 300 21
TABLE III: Hyperparameters for all datasets for by-design
interpretation
column-wise to build our dictionary W. While this strategy
helps us reduce the number of noise-like components in the
Ô¨Ånal dictionary, it does not completely avoid it. OpenMIC is
instead a balanced multilabel dataset for rare noise presence.
We simply select random 500 positively labeled samples for
each of the 20 classes and learn 15 components. All of them
are stacked together to create Xtrain.
4) Hyperparameters: The hidden layers input to the in-
terpreter module are selected from the convolutional block
outputs. As is often the case with CNNs, the latter layers are
expected to capture higher-order features. We thus select the
last three convolutional block outputs as input to the network
	. The loss weights and number of components used for post-
hoc interpretation are summarized in table II. Ablation studies
about all the hyperparameters and justiÔ¨Åcation of their choices
will be presented in the next section. The hyperparameters
for by-design interpretation are guided by choices in post-hoc
interpretation and are tabulated in table III.
5) Optimization: All the networks are optimized using
Adam [57] with learning rate 210 4.
C. Evaluating Interpretations
Quantifying different aspects of interpretability has been a
challenging research question recently. This challenge stems
from the inherent subjectivity involved in its deÔ¨Ånition. Our
unique style of ‚Äúconcept-like‚Äù basis for interpretation and
global approximation of the base model results in a testing
situation to conduct its evaluation, wherein no other method
can be directly compared to it. We resolve this hurdle by
evaluating different aspects of the interpretation separately. We
Ô¨Årst discuss quantitative metrics for post-hoc and by-design
interpretation along with their goals, followed by discussion
on subjective evaluation of interpretations.
Metrics and baselines (Post-hoc) . The simplest aspect to
evaluate is how well does the interpreter agree with the
classiÔ¨Åer‚Äôs output. We refer to this metric as the Ô¨Ådelity metric.
To do so for any given task, we utilize the same metric
used to evaluate the classiÔ¨Åer performance but instead treat
classiÔ¨Åers output as ground truth and evaluate the interpreter‚Äôs
approximation (HI(x))w.r.t to it. Thus, for multi-class
classiÔ¨Åcation, this is done by computing fraction of samples
where the class predicted by fis among the top- kclassespredicted by the interpreter, referred to as top-kÔ¨Ådelity . For
multi-label classiÔ¨Åcation tasks with unbalanced number of
positive samples of classes, we compute Area Under Precision-
Recall Curve (AUPRC) based metrics. In case of balanced
classes, we compute F1-score based metrics. We denote our
proposed Listen to Interpret (L2I) system, with attention based
pooling in by L2I w/ ATT. The most suitable baselines to
benchmark its Ô¨Ådelity are post-hoc methods that approximate
the classiÔ¨Åer over input space with a single surrogate model.
We select two state-of-the-art systems, FLINT [23] and VIBI
[58]. A variant of our own proposed method, L2I w/ MAX,
is also evaluated. Herein, attention is replaced with 1D max-
pooling operation.
We also conduct a faithfulness evaluation for our interpre-
tations. In general for any interpretability method, faithfulness
tries to assess if the features identiÔ¨Åed to be of high rele-
vance are truly important in classiÔ¨Åer‚Äôs prediction [59]. Since
a ‚Äúground-truth‚Äù importance measure for features is rarely
available, attribution based methods evaluate faithfulness by
performing feature removal (generally by setting feature value
to 0) and observing the change in classiÔ¨Åer‚Äôs output [59].
However, it is hard to conduct such evaluation for non-
attribution or concept based interpretation methods on data
modalities like image/audio, as simulating feature removal
from input is not evident in these cases.
Interestingly, our interpretation module design allows us to
simulate removal of a set of components from the input. Given
any sample xwith predicted class c, we remove the set of
relevant components Lc;x=fk:rk;c;x> gby creating a
new time domain signal x2=INV(X2;Px), where X2=
X P
l2Lc;xXl. We deÔ¨Åne faithfulness of the interpretation
to classiÔ¨Åer ffor samplexwith:
FFx=f(x)c f(x2)c (7)
wheref(x)c;f(x2)cdenote the output probabilities for class
c. It should be noted that this strategy to simulate removal may
introduce artifacts in the input that can affect the classiÔ¨Åer‚Äôs
output unpredictably. Also, interpretations on samples with
poor Ô¨Ådelity can lead to negative FF x. Both of these obser-
vations point to the potential instability and outlying values
for this metric. Thus, we report the Ô¨Ånal faithfulness of the
system as median of FF xover test set, denoted by FF median . A
positive FF median would signify that interpretations generally
tend to be faithful to the classiÔ¨Åer.
As already discussed, it is not possible to measure faithful-
ness for concept-based post-hoc interpretability approaches.
While measurement for input attribution based approaches
is possible, the interpretations themselves and the feature
removal strategies are different, making comparisons with our
system signiÔ¨Åcantly less meaningful. We thus compare our
faithfulness against a Random Baseline , wherein the less-
important components, those not present in Lc;x, are randomly
removed. To compare fairly, we remove the same number of
components that are present in Lc;xon average. This would
validate that, if the interpreter selects truly important compo-
nents for the classiÔ¨Åer‚Äôs decision, then randomly removing the
less important ones should not cause a drop in the predicted
class probability.UNDER SUBMISSION 8
We also emphasize at this point that works related to
audio interpretability (see Sec. II-B), are not suitable for
comparison on these metrics. Particularly, APNet [35] is not
designed for post-hoc interpretations. AudioLIME [32] is not
applicable on our tasks as it requires known predeÔ¨Åned audio
sources. Moreover, SLIME [31] and AudioLIME still rely on
LIME [7] for interpretations. It is a feature-attribution method
that approximates a classiÔ¨Åer for each sample separately. As
discussed before, these characteristics are not suitable for
comparison on our metrics.
Separate from the quantitative metrics, we conducted a
subjective evaluation to evaluate quality and understandability
of interpretations. Our design for the same was based on
qualitative understanding of saliency maps for images. Attribu-
tion maps in images are qualitatively judged by observing the
visual overlap in input with the given class being interpreted.
In similar spirit, our design was based on providing the user
with input and class being interpreted and asking them to
rate auditory overlap of the interpretation and part of input
audio corresponding to the class. Further details and results
are covered in the next section. Apart from evaluating under-
standability, we also extensively analyze our interpretations
qualitatively.
Metrics and baselines (By-design) . For by-design interpre-
tation, the faithfulness metric is much less signiÔ¨Åcant. This
is because the Ô¨Ånal classiÔ¨Åcation output is generated by the
interpreter itself and thus faithfulness is ensured by-design.
The classiÔ¨Åcation performance of the interpreter is the primary
metric, similar in spirit to Ô¨Ådelity evaluation for post-hoc
interpretations. We compare this with several baselines to (i)
benchmark performance of our by-design interpretable net-
work, and (ii) to evaluate the two key modiÔ¨Åcations introduced
in the learning problem while extending from post-hoc to by-
design interpretation (section III-C). SpeciÔ¨Åcally, the hidden
layers offare not pre-trained on the given dataset in by-
design problem and updated jointly with interpreter layers.
And secondly, applying an additional classiÔ¨Åcation loss on
f(x)to affect the hidden layers. The various baselines and
the reasons to include them are the following:
Audio prototypical networks (APNet) [35] act as a pri-
mary baseline from literature. It is an audio processing
by-design interpretable network. While it generates inter-
pretation differently from us, it is the only system in the
literature addressing by-design interpretation for audio
modality. Note that the dedicated post-hoc interpretation
systems VIBI and SLIME are not relevant for this prob-
lem. For fair comparison, we use the same number of
prototypes in their network as our number of components.
In order to ascertain that using a CNN based repre-
sentation for NMF offer advantage over typical NMF
based representations in terms of prediction performance,
we also evaluate performance of two NMF variants:
Unsupervised NMF based classiÔ¨Åcation and the Task-
driven Dictionary Learning (TDL)-NMF model [42]. The
unsupervised NMF model simply learns a dictionary on
training data, computes average time activations on test
samples and makes predictions using a linear model.
The TDL-NMF model instead updates the initial learntdictionary with classiÔ¨Åcation loss from the linear model
and thus learns them jointly. For both the systems, we
experiment with use of two data types to learn NMF-
dictionaries. The Ô¨Årst is log-magnitude spectrograms and
second is power mel-spectrogram (with a square root
transformation). We vary dictionary sizes from 64 to 512
components and report results for best performance.
Given the framework level similarities between FLINT
and L2I, we also evaluate the performance of the FLINT
interpreter when trained for by-design interpretation. As
before, we again emphasize that FLINT is not suitable for
audio interpretations, but provides a interpretable network
design for comparison of performance.
Variants of L2I: We denote our proposed version of
L2I for by-design interpretation as L2I BDw/ATT. We
further evaluate two variants of our proposed classiÔ¨Å-
cation network g(x). The Ô¨Årst variant ‚ÄúL2I BD-NoPred‚Äù
does not include a classiÔ¨Åcation loss applied to f(x)and
instead applies it directly to g(x). The second variant
‚ÄúL2I-PostHoc‚Äù is simply the interpreter trained for post-
hoc interpretation. We compare with these variants to
gain perspective on effect of differences between our
formulations of post-hoc and by-design problems.
The implementation details of all the baselines (post-hoc and
by-design) can be found on our companion website.1
V. R ESULTS AND DISCUSSION
A. Post-hoc Interpretation
1) Fidelity: As discussed previously, to quantify Ô¨Ådelity, we
use the same respective metrics as done to benchmark classiÔ¨Åer
performance but evaluate them for interpreter output w.r.t
classiÔ¨Åer output. For ESC-50, mean and standard deviation
of top-kÔ¨Ådelity is calculated over the 5 folds. We show
these results for k= 1;5. For SONYC-UST, we report the
macro-AUPRC, micro-AUPRC and max-F1 for the interpreter
output w.r.t classiÔ¨Åer. For fairness, we ignore the class ‚Äònon-
machinery impact‚Äô from all class-wise evaluations involved in
Ô¨Ådelity ( i.e.macro-AUPRC) or faithfulness. This is because
the classiÔ¨Åer predicts only one sample in test set with positive
label for this class, causing AUPRC scores to vary widely
for different interpreters. For OpenMIC-2018, we report the
Fidelity weighted F1-score for each system. All the above
results are available in Tab. IV.
Among the four systems, VIBI performs the worst in terms
of Ô¨Ådelity. This is very likely because it treats the classiÔ¨Åer as
a black-box, while the other three systems access its hidden
representations. This strongly indicates that accessing hidden
layers can be beneÔ¨Åcial for Ô¨Ådelity of interpreters. While
on ESC50, FLINT achieves the best Ô¨Ådelity, L2I w/ ATT
outperforms all systems on the other datasets. It should be
noted that our system variants distinctly hold the advantage
of generating listenable interpretations over FLINT and VIBI.
Nevertheless, these systems form strong baselines for Ô¨Ådelity
and the results demonstrate that our interpreter can generate
high-Ô¨Ådelity post-hoc interpretations. Moreover, its design is
Ô¨Çexible w.r.t different pooling functions.
1https://jayneelparekh.github.io/listen2interpretV2/UNDER SUBMISSION 9
ESC-50 (in %) SONYC-UST OpenMIC-2018
System top-1 top-5 macro-AUPRC micro-AUPRC avg-weighted-F1
L2I w/ ATT 65.72.8 88.21.7 0.9090.011 0.9170.008 0.9200.004
L2I w/ MAX 73.32.3 92.71.2 0.8660.014 0.9130.012 0.9060.004
FLINT [23] 73.52.3 93.40.9 0.8160.013 0.9070.011 0.9070.004
VIBI [58] 27.7 2.3 53.01.8 0.6080.027 0.5750.019 0.5810.037
TABLE IV: Fidelity results for the interpreter w.r.t classiÔ¨Åer‚Äôs output on all datasets. We report top-1 and top-5 Ô¨Ådelity (in
%) for ESC-50 (all Ô¨Åve folds), AUPRC-based metrics for SONYC-UST and weighted F1-score averaged over all classes for
OpenMIC-2018. All results contain mean and variance over three runs. Values in bold indicate maximum of the metric among
all the evaluated systems (incl. baselines).
System Threshold  FFmedian
L2I w/ ATT= 0:9 0.002
= 0:7 0.004
= 0:5 0.012
= 0:3 0.040
= 0:1 0.113
Random Baseline = 0:1<10 4
TABLE V: Faithfulness results on ESC-50 for different thresh-
olds,. We report FF median for proposed L2I w/ ATTand the
Random Baseline.
2) Faithfulness: In Table V, we report median faithfulness
FFmedian on ESC-50 for our primary system L2I w/ ATTat
different thresholds averaged over the Ô¨Åve folds. Smaller
corresponds to higher jLc;xj, which denotes the number of
components being used for generating interpretations. Thus,
for Random Baseline, we report FF median at the lowest thresh-
old= 0:1, to ensure removal of maximal number of
components. To recall the deÔ¨Ånition of Random Baseline,
please refer to Sec. IV-C. FF median for L2I w/ ATTis positive
for all thresholds. It is also signiÔ¨Åcantly higher than the
Random Baseline, indicating faithfulness of interpretations.
The results for class-wise faithfulness on SONYC-UST and
OpenMIC are illustrated in Fig. 2 and 3 respectively. We show
FFmedian (absolute drop in probability) for our system and
the Random Baseline. For most classes, interpretations can
be considered faithful, with a signiÔ¨Åcantly positive median
compared to random baseline results, which are very close
to 0.
3) Subjective evaluation: The test was conducted with 15
participants. Each participant was provided with 10 input
samples, a predicted class by the classiÔ¨Åer for each sample and
the corresponding interpretation audios from SLIME and L2I.
They were asked to rate the interpretations on a scale of 0-100
for the following question: ‚Äú How well does the interpretation
correspond to the part of input audio associated with the
given class? ‚Äù. The 10 samples were randomly selected from
a set of 36 (5-6 random test examples per class). For each
sample, we ensured that the predicted class was both, present
in the ground-truth and audible in input. Class-wise preference
Fig. 2: Faithfulness (absolute drop in probability value) results
for SONYC-UST arranged class-wise for threshold, = 0:1
Fig. 3: Faithfulness (absolute drop in probability value) results
for OpenMIC-2018 arranged class-wise for threshold, = 0:1
results and average ratings are shown in Fig. 4. L2I is preferred
for ‚Äômusic‚Äô, ‚Äôdog‚Äô & ‚Äôalert-signal‚Äô, SLIME is preferred for
‚Äômachinery-impact‚Äô, no clear preference for others.
B. Qualitative analysis of interpretations
Qualitatively we observe that our interpretations are capable
of emphasizing the object of interest and are insightful for an
end-user to understand the classiÔ¨Åer‚Äôs prediction. We share
multiple examples on our companion website.1Samples in
case of SONYC-UST and OpenMIC are often already chal-
lenging with the presence of other sources of audio. In case of
ESC50, to create more interesting and challenging scenarios
we devise an experiment described below
Audio corruption experiment: interpretability illustra-
tion. For ESC50, we generate interpretations after corrupting
the testing data for fold‚Äì1 in two different ways (i) eitherUNDER SUBMISSION 10
Fig. 4: Subjective evaluation results. Average scores for L2I
and SLIME and fraction of votes in favour of each system
with white noise at 0dB SNR (signal-to-noise ratio), (ii) or
mixing it with a sample of a different class. It should be
noted that in both these cases the system is exactly the
same as before and nottrained with corrupted samples. Some
examples, covering both types of corruptions are shared on
our companion website.1.
For SONYC-UST, we observe good interpretations for
classes ‚Äòalert-signal‚Äô, ‚Äòdog‚Äô and ‚Äòmusic‚Äô. For them, the back-
ground noise is signiÔ¨Åcantly suppressed and the interpretations
mainly focus on the object of interest. Interpretations for class
‚Äòhuman‚Äô are also able to suppress noise to a certain extent and
focus on parts of human voices. However, for this class, we
found presence of some signal from other audio sources too.
For the remaining classes, namely ‚ÄòEngine‚Äô, ‚ÄòPowered-saw‚Äô
and ‚ÄôMachinery-impact‚Äô the quality of the interpretation is
more sample dependent. This is due to their acoustic similarity
with the background noise. We provide example interpretations
for SONYC-UST on our companion website.1
The third dataset OpenMIC-2018, offers challenges under
unique scenarios. Unlike SONYC-UST while it does not
face issue of noise in data, it faces the hurdle of a strong
overlap between instruments. This is because their onsets are
often aligned by beats of the musical piece. This increases
difÔ¨Åculty of Ô¨Åltering the signal of interest. Even with the
greater complexity, the interpretations in many cases are able
to emphasize the class of interest. Classes with relatively
unique sounds such as ‚ÄòBass‚Äô or ‚ÄòMallet-percussion‚Äô are very
well extracted. String like instruments including Violin and
Guitar are also generally emphasized well.
Coherence of interpretations . We visualize interpretations
generated on the test set for SONYC-UST and OpenMIC-2018
by clustering relevance vectors. SpeciÔ¨Åcally, we compute the
vectorrc;x2RKwhich contains relevances of all components
in prediction for class cfor samplex. The relevance vectors
are collected for each test sample xand its predicted class
c. We then apply a t-SNE [60] transformation to 2D for
visualization. This is shown in Fig. 5. Each point is la-
beled/colored according to the class for which we generate the
interpretation. Interpretations for any single class are coherent
and similar to each other. This is to some extent a positive
consequence of global weight matrix in . Moreover, globally
it can be observed that classes like ‚ÄôMachinery-impact‚Äô and
‚ÄôPowered-Saw‚Äô have similar relevances which are to someextent close to ‚ÄôEngine‚Äô. This is to be expected as these classes
are acoustically similar. ‚ÄôDog‚Äô and ‚ÄôMusic‚Äô are also close in
this space, likely due to the often periodic nature of barks or
beats. The visualization for OpenMIC is arguably even more
interesting because of larger number of classes and several
inter-class relationships. Various sets of similar instruments
end-up as clusters in proximity of each other. The examples
include ‚ÄòCello-Violin‚Äô, ‚ÄòDrums-Cymbals‚Äô, ‚ÄòClarinet-Flute‚Äô,
‚ÄòUkulele-Mandolin-Banjo‚Äô, ‚ÄòTrombone-Trumpet-Saxophone‚Äô.
Moreover, the meaningfulness of clustering also extends to
higher-level of grouping. For example, the data is partitioned
so as the string-based, wind-based, or percussion instruments
are close to each other within their respective groups. This
indicates that the interpreter‚Äôs representations of what consti-
tutes sound of an instrument aligns to some extent to human
understanding.
Fig. 5: Visualized relevances (following a t-SNE transforma-
tion) of generated interpretations on test sets of SONYC-UST
(top) and OpenMIC-2018 (bottom), colour-coded according to
interpreted class. For clarity in case of OpenMIC, we show up
to random 25 interpretations of a class.
C. Ablation studies
Tab. VI and Tab. VII present ablation studies for loss
hyperparameters and choice of hidden layers. The values in
bold indicate our current choices for post-hoc interpretation.
The metrics and loss values given here are for a single run.UNDER SUBMISSION 11
ConvBlocksLNMFLFID top-1
B3 0.104 1.788 53.0
B6 0.118 1.698 57.8
B2+B3 0.093 1.966 51.8
B5+B6 0.103 1.572 61.5
B4+B5+B6 0.079 1.546 65.5
Input 0.102 2.384 34.5
TABLE VI: Ablation study for hidden layers: loss values on
ESC50 (fold 1) test set for different subsets of hidden layers.
Current choice indicated in bold.
 LNMFLFID macro-AUPRC
10.0 0.8 0.028 0.386 0.900
10.0 8.0 0.048 0.386 0.879
10.0 0.08 0.028 0.388 0.876
1.0 0.8 0.045 0.375 0.921
100.0 0.8 0.027 0.445 0.612
TABLE VII: Ablation study for loss hyperparameters: loss
values on SONCY-UST test set for different weights of loss
functions. Current choice indicated in bold.
Selecting the hidden layers of the classiÔ¨Åer that should be
accessed by the interpreter is an important choice. At Ô¨Årst
glance, this model selection task might appear to be computa-
tionally too expensive as total possible choices is exponential
in number of hidden layers. However, practical considerations
can heavily reduce the search space. An upper bound to the
number of layers could be set depending upon the desired
size of interpreter. In our experiments throughout the paper,
we limited ourselves to at most 3 layers. Crucially, layers
close to the output are more favourable, for multiple reasons.
They generally result in better Ô¨Ådelity and inherently tie the
interpreter much closer to the output of classiÔ¨Åer. Moreover,
the latter layers are also expected to capture higher level
features. We illustrate how selecting different subsets of hidden
layers affects optimization of our Ô¨Ådelity and reconstruction
loss by doing an ablation study. It‚Äôs results are reported in table
VI. The classiÔ¨Åer consists of 6 major convolutional blocks
(B1‚ÄìB6).
Loss weights . We illustrate the effect of varying loss weights
on optimization in table VII. Too high emphasis on LNMF, that
is, highcan hurt the Ô¨Ådelity of interpreter while a high 
(sparsity loss) can result in poorer reconstruction. Importantly,
there is a good range of values wherein the system can be
regarded as operating reasonably.
Number of components . ChoosingK, also known as order
estimation, is typically data and application dependent. It
controls the granularity of the discovered audio spectral pat-
terns. Determining the optimal value has been a long standing
problem within the NMF community [61]. Our choice for this
parameter was guided by three main factors:
Choices made previously in literature for similar pre-
learning of W[42], who demonstrated reasonable acous-
Fig. 6: Ablation study for number of components. Loss values
on test data for ESC-50 and OpenMIC-2018.
tic scene classiÔ¨Åcation results with a dictionary size of
K= 128 . We used this as a reference to guide our choice.
Dataset speciÔ¨Åc details which include number of classes,
samples for each class, variability of recordings etc. For
eg. acoustic variability of ESC-50 (larger number of
classes), prompted us to use a dictionary of larger size
compared to SONYC-UST. We use highest number of
components for OpenMIC, which has largest dataset size
among the three and reasonably high acoustic variability.
When tracking loss values for different K, we observed a
plateauing effect for larger dictionary sizes as illustrated
in Fig. 6 for ESC-50 and OpenMIC-2018. In case of
OpenMIC, this effect is prominent for reconstruction loss
LNMF. The Ô¨Ådelity remains high even for small K.
D. By-design Interpretation
The performance of all systems is given in Tab. VIII. We
compute the same metrics as used to evaluate the classiÔ¨Åers
for each dataset. Mean performance along with variance across
3 runs is reported. We make the following key observations:
Among the interpretable neural networks for audio ,
L2I BDw/ATTclearly outperforms APNet. The size of
the models plays an important role in this. L2I learns
with the help of a network architecture that feeds it with
higher quality representations for prediction compared to
architecture in APNet. It is generally able to sustain a
comparable performance w.r.t the base network BASE-
fwhile imposing an interpretable structure for Ô¨Ånal
prediction model.
Comparison with NMF baselines . While TDL-NMF
performs better than unsupervised-NMF, L2I variants are
noticeably better than both. This highlights a unique
advantage of combining NMF representations with deep
neural network representations, wherein, the NMF struc-
ture leads to interpretability and using deep networks as
source provides higher prediction performance compared
to directly using NMF activations generated from input.
We also validate our design of training procedure for
by-design interpretable network g(x), by comparing it
with the two variants of proposed system, L2I-PostHoc
and L2I BD-NoPred. The performance of L2I BDw/ATT
compared to L2I-Posthoc highlights that g(x)tends to
perform better when hidden layers of fare trained jointly
with interpreter. L2I BD-NoPred performs the worst among
the three, emphasizing the beneÔ¨Åts of updating the hiddenUNDER SUBMISSION 12
ESC-50 (in %) SONYC-UST OpenMIC-2018
System accuracy macro-AUPRC avg-weighted-F1
L2I BDw/ATT 70.11.5 0.581 + 0.008 0.825 0.005
APNet [35] 63.6 1.7 0.4220.012 0.5630.025
Unsupervised-NMF 39.4 2.3 0.3730.006 0.6590.018
TD-NMF [42] 46.7 2.7 0.4310.018 0.6990.012
L2I‚ÄìPosthoc 65.4 3.4 0.5670.007 0.8250.003
L2I BD‚ÄìNoPred 64.4 1.1 0.5630.004 0.7460.006
FLINT [23] 75.3 3.6 0.5560.008 0.8270.002
BASE-f 82.5 0.601 0.831
TABLE VIII: ClassiÔ¨Åcation performance for by-design interpretation. The evaluated systems include our proposed by-design
interpretable network, denoted as L2I BDw/ATT, its variant with modiÔ¨Åed loss function (L2I BD‚ÄìNoPred), interpreter trained
for post-hoc interpretation (L2I‚ÄìPosthoc), classiÔ¨Åcation models based on traditional NMF (unsupervised NMF and TD-NMF)
and audio prototypical network APNet. The base classiÔ¨Åcation network used for post-hoc interpretation (BASE- f) and FLINT
are used as references for high performance networks not suitable for audio interpretation.
layers offwith classiÔ¨Åcation loss imposed on f(x)rather
than ong(x).
VI. C ONCLUSION
We have presented a framework to tackle both post-hoc
and by-design for audio classiÔ¨Åcation networks. To this end,
a novel interpreter is designed with the key idea of using
an NMF-inspired regularizer. This enables listenable concept-
based interpretations. We motivate listenability as an important
attribute for audio interpretability. EfÔ¨Åcacy of the proposed
framework is established through extensive qualitative and
quantitative experimentation. In particular, we quantitatively
evaluate both post-hoc and by-design interpretations on three
popular datasets pertaining to audio event and music instru-
ment recognition tasks. We perform a user-study to conÔ¨Årm
usefulness of our interpretations. In addition, through a visual-
ization of the generated interpretations, we show that they are
coherent across samples from different classes and cluster in
a fashion that aligns well with human understanding of sound.
Further works concern the extension of this framework to other
machine learning audio-based tasks.
REFERENCES
[1] A. S. Bregman, Auditory scene analysis: The perceptual organization of
sound . MIT press, 1994.
[2] T. D. GrifÔ¨Åths and J. D. Warren, ‚ÄúWhat is an auditory object?‚Äù Nature
Reviews Neuroscience , vol. 5, no. 11, pp. 887‚Äì892, 2004.
[3] B. J. Dyson and C. Alain, ‚ÄúRepresentation of concurrent acoustic objects
in primary auditory cortex,‚Äù The Journal of the Acoustical Society of
America , vol. 115, no. 1, pp. 280‚Äì288, 2004.
[4] D. Lee and H. S. Seung, ‚ÄúAlgorithms for non-negative matrix factoriza-
tion,‚Äù in Advances in Neural Information Processing Systems , T. Leen,
T. Dietterich, and V . Tresp, Eds., vol. 13. MIT Press, 2001.
[5] J. Parekh, S. Parekh, P. Mozharovskyi, F. d‚ÄôAlch ¬¥e Buc, and G. Richard,
‚ÄúListen to interpret: Post-hoc interpretability for audio networks with
nm,‚Äù in NeurIPS , 2022.
[6] E. Humphrey, S. Durand, and B. McFee, ‚ÄúOpenmic-2018: An open data-
set for multiple instrument recognition.‚Äù in ISMIR , 2018, pp. 438‚Äì444.
[7] M. T. Ribeiro, S. Singh, and C. Guestrin, ‚ÄúWhy should i trust you?:
Explaining the predictions of any classiÔ¨Åer,‚Äù in Proceedings of the 22nd
ACM SIGKDD international conference on knowledge discovery and
data mining . ACM, 2016, pp. 1135‚Äì1144.[8] S. M. Lundberg and S.-I. Lee, ‚ÄúA uniÔ¨Åed approach to interpreting model
predictions,‚Äù in Advances in Neural Information Processing Systems ,
2017, pp. 4765‚Äì4774.
[9] H. Lakkaraju, E. Kamar, R. Caruana, and J. Leskovec, ‚ÄúFaithful and cus-
tomizable explanations of black box models,‚Äù in AAAI/ACM Conference
on AI, Ethics, and Society , 2019, pp. 131‚Äì138.
[10] J. T. Springenberg, A. Dosovitskiy, T. Brox, and M. Riedmiller,
‚ÄúStriving for simplicity: The all convolutional net,‚Äù arXiv preprint
arXiv:1412.6806 , 2014.
[11] D. Smilkov, N. Thorat, B. Kim, F. Vi ¬¥egas, and M. Wattenberg,
‚ÄúSmoothgrad: removing noise by adding noise,‚Äù arXiv preprint
arXiv:1706.03825 , 2017.
[12] K. Schulz, L. Sixt, F. Tombari, and T. Landgraf, ‚ÄúRestricting the Ô¨Çow:
Information bottlenecks for attribution,‚Äù in ICLR , 2019.
[13] G. Montavon, W. Samek, and K.-R. M ¬®uller, ‚ÄúMethods for interpreting
and understanding deep neural networks,‚Äù Digital Signal Processing ,
vol. 73, pp. 1‚Äì15, 2018.
[14] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and
D. Batra, ‚ÄúGrad-cam: Visual explanations from deep networks via
gradient-based localization,‚Äù in ICCV , 2017, pp. 618‚Äì626.
[15] D. Alvarez-Melis and T. S. Jaakkola, ‚ÄúA causal framework for explain-
ing the predictions of black-box sequence-to-sequence models,‚Äù arXiv
preprint arXiv:1707.01943 , 2017.
[16] P.-J. Kindermans, S. Hooker, J. Adebayo, M. Alber, K. T. Sch ¬®utt,
S. D ¬®ahne, D. Erhan, and B. Kim, ‚ÄúThe (un) reliability of saliency
methods,‚Äù in Explainable AI: Interpreting, Explaining and Visualizing
Deep Learning . Springer, 2019, pp. 267‚Äì280.
[17] D. Alvarez-Melis and T. S. Jaakkola, ‚ÄúOn the robustness of interpretabil-
ity methods,‚Äù arXiv preprint arXiv:1806.08049 , 2018.
[18] R. Kusters, Y . Kim, M. Collery, C. d. S. Marie, and S. Gupta, ‚ÄúDiffer-
entiable rule induction with learned relational features,‚Äù arXiv preprint
arXiv:2201.06515 , 2022.
[19] P. Jacob, ¬¥E. Zablocki, H. Ben-Younes, M. Chen, P. P ¬¥erez, and M. Cord,
‚ÄúSteex: steering counterfactual explanations with semantics,‚Äù in ECCV .
Springer, 2022, pp. 387‚Äì403.
[20] L. A. Hendricks, Z. Akata, M. Rohrbach, J. Donahue, B. Schiele, and
T. Darrell, ‚ÄúGenerating visual explanations,‚Äù in ECCV . Springer, 2016,
pp. 3‚Äì19.
[21] O. Li, H. Liu, C. Chen, and C. Rudin, ‚ÄúDeep learning for case-
based reasoning through prototypes: A neural network that explains its
predictions,‚Äù in AAAI Conference on ArtiÔ¨Åcial Intelligence , vol. 32, 2018.
[22] C. Chen, O. Li, D. Tao, A. Barnett, C. Rudin, and J. K. Su, ‚ÄúThis looks
like that: deep learning for interpretable image recognition,‚Äù in NeurIPS ,
2019, pp. 8928‚Äì8939.
[23] J. Parekh, P. Mozharovskyi, and F. d‚ÄôAlch ¬¥e Buc, ‚ÄúA framework to learn
with interpretation,‚Äù in NeurIPS , 2021.
[24] B. Kim, M. Wattenberg, J. Gilmer, C. Cai, J. Wexler, F. Viegas,
and R. Sayres, ‚ÄúInterpretability beyond feature attribution: Quanti-
tative testing with concept activation vectors (tcav),‚Äù arXiv preprint
arXiv:1711.11279 , 2017.UNDER SUBMISSION 13
[25] A. Ghorbani, J. Wexler, J. Y . Zou, and B. Kim, ‚ÄúTowards automatic
concept-based explanations,‚Äù in NeurIPS , 2019, pp. 9277‚Äì9286.
[26] C.-K. Yeh, B. Kim, S. O. Arik, C.-L. Li, P. Ravikumar, and T. PÔ¨Åster,
‚ÄúOn concept-based explanations in deep neural networks,‚Äù arXiv preprint
arXiv:1910.07969 , 2019.
[27] S. Becker, M. Ackermann, S. Lapuschkin, K.-R. M ¬®uller, and W. Samek,
‚ÄúInterpreting and explaining deep neural networks for classiÔ¨Åcation of
audio signals,‚Äù arXiv preprint arXiv:1807.03418 , 2018.
[28] M. Won, S. Chun, and X. Serra, ‚ÄúToward interpretable music tagging
with self-attention,‚Äù arXiv preprint arXiv:1906.04972 , 2019.
[29] H. Muckenhirn, V . Abrol, M. Magimai-Doss, and S. Marcel, ‚ÄúUnder-
standing and visualizing raw waveform-based CNNs.‚Äù in Interspeech ,
2019, pp. 2345‚Äì2349.
[30] S. Mishra, B. L. Sturm, and S. Dixon, ‚ÄúLocal interpretable model-
agnostic explanations for music content analysis.‚Äù in ISMIR , 2017, pp.
537‚Äì543.
[31] S. Mishra, E. Benetos, B. L. Sturm, and S. Dixon, ‚ÄúReliable local
explanations for machine listening,‚Äù in IJCNN , 2020, pp. 1‚Äì8.
[32] V . Haunschmid, E. Manilow, and G. Widmer, ‚Äúaudiolime: Listenable
explanations using source separation,‚Äù arXiv preprint arXiv:2008.00582 ,
2020.
[33] S. Chowdhury, V . Praher, and G. Widmer, ‚ÄúTracing back music emotion
predictions to sound sources and intuitive perceptual qualities,‚Äù arXiv
preprint arXiv:2106.07787 , 2021.
[34] F. Foscarin, K. Hoedt, V . Praher, A. Flexer, and G. Widmer, ‚ÄúConcept-
based techniques for‚Äù musicologist-friendly‚Äù explanations in a deep
music classiÔ¨Åer,‚Äù ISMIR , 2022.
[35] P. Zinemanas, M. Rocamora, M. Miron, F. Font, and X. Serra, ‚ÄúAn
interpretable deep learning model for automatic sound classiÔ¨Åcation,‚Äù
Electronics , vol. 10, no. 7, p. 850, 2021.
[36] P. Smaragdis, ‚ÄúNon-negative matrix factor deconvolution; extraction of
multiple sound sources from monophonic inputs,‚Äù in ICA. Springer,
2004, pp. 494‚Äì499.
[37] K. W. Wilson, B. Raj, P. Smaragdis, and A. Divakaran, ‚ÄúSpeech de-
noising using nonnegative matrix factorization with priors,‚Äù in ICASSP .
IEEE, 2008, pp. 4029‚Äì4032.
[38] J. Le Roux, H. Kameoka, N. Ono, A. De Cheveigne, and S. Sagayama,
‚ÄúComputational auditory induction as a missing-data model-Ô¨Åtting prob-
lem with bregman divergence,‚Äù Speech Communication , vol. 53, no. 5,
pp. 658‚Äì676, 2011.
[39] C. Dittmar and D. G ¬®artner, ‚ÄúReal-time transcription and separation of
drum recordings based on nmf decomposition.‚Äù in DAFx , 2014, pp. 187‚Äì
194.
[40] N. Bertin, R. Badeau, and G. Richard, ‚ÄúBlind signal decompositions
for automatic transcription of polyphonic music: Nmf and k-svd on the
benchmark,‚Äù in ICASSP , vol. 1. IEEE, 2007, pp. I‚Äì65.
[41] C. F ¬¥evotte, E. Vincent, and A. Ozerov, ‚ÄúSingle-channel audio source
separation with nmf: divergences, constraints and algorithms,‚Äù Audio
Source Separation , pp. 1‚Äì24, 2018.
[42] V . Bisot, R. Serizel, S. Essid, and G. Richard, ‚ÄúFeature learning with
matrix factorization applied to acoustic scene classiÔ¨Åcation,‚Äù IEEE/ACM
TASLP , vol. 25, no. 6, pp. 1216‚Äì1229, 2017.
[43] R. Gao, R. Feris, and K. Grauman, ‚ÄúLearning to separate object sounds
by watching unlabeled video,‚Äù in ECCV , 2018, pp. 35‚Äì53.
[44] S. Parekh, A. Ozerov, S. Essid, N. Q. Duong, P. P ¬¥erez, and G. Richard,
‚ÄúIdentify, locate and separate: Audio-visual object extraction in large
video collections using weak supervision,‚Äù in WASPAA . IEEE, 2019,
pp. 268‚Äì272.
[45] J. Le Roux, J. R. Hershey, and F. Weninger, ‚ÄúDeep nmf for speech
separation,‚Äù in ICASSP . IEEE, 2015, pp. 66‚Äì70.
[46] S. Wisdom, T. Powers, J. Pitton, and L. Atlas, ‚ÄúDeep recurrent nmf
for speech separation by unfolding iterative thresholding,‚Äù in WASPAA .
IEEE, 2017, pp. 254‚Äì258.
[47] J. Goldstein, ‚ÄúAuditory nonlinearity,‚Äù The Journal of the Acoustical
Society of America , vol. 41, no. 3, pp. 676‚Äì699, 1967.
[48] J. Le Roux, F. J. Weninger, and J. R. Hershey, ‚ÄúSparse nmf‚Äìhalf-baked
or well done?‚Äù Mitsubishi Electric Research Labs (MERL), Cambridge,
MA, USA, Tech. Rep., no. TR2015-023 , vol. 11, pp. 13‚Äì15, 2015.
[49] M. Ilse, J. Tomczak, and M. Welling, ‚ÄúAttention-based deep multiple
instance learning,‚Äù in ICML . PMLR, 2018, pp. 2127‚Äì2136.
[50] K. J. Piczak, ‚ÄúEsc: Dataset for environmental sound classiÔ¨Åcation,‚Äù in
ACM Multimedia , 2015, pp. 1015‚Äì1018.
[51] M. Cartwright, A. E. M. Mendez, J. Cramer, V . Lostanlen, G. Dove,
H.-H. Wu, J. Salamon, O. Nov, and J. Bello, ‚ÄúSONYC urban sound
tagging (SONYC-UST): A multilabel dataset from an urban acoustic
sensor network,‚Äù in DCASE , October 2019, pp. 35‚Äì39.[52] A. Kumar, M. Khadkevich, and C. F ¬®ugen, ‚ÄúKnowledge transfer from
weakly labeled audio using convolutional neural network for sound
events and scenes,‚Äù in ICASSP . IEEE, 2018, pp. 326‚Äì330.
[53] J. F. Gemmeke, D. P. Ellis, D. Freedman, A. Jansen, W. Lawrence,
R. C. Moore, M. Plakal, and M. Ritter, ‚ÄúAudio set: An ontology and
human-labeled dataset for audio events,‚Äù in ICASSP . IEEE, 2017, pp.
776‚Äì780.
[54] A. Arnault and N. Riche, ‚ÄúCRNNs for urban sound tagging with
spatiotemporal context,‚Äù DCASE2020 Challenge, Tech. Rep., October
2020.
[55] K. Koutini, H. Eghbal-Zadeh, V . Haunschmid, P. Primus, S. Chowdhury,
and G. Widmer, ‚ÄúReceptive-Ô¨Åeld regularized cnns for music classiÔ¨Åca-
tion and tagging,‚Äù arXiv preprint arXiv:2007.13503 , 2020.
[56] M. Cartwright, J. Cramer, A. E. M. Mendez, Y . Wang, H.-H. Wu,
V . Lostanlen, M. Fuentes, G. Dove, C. Mydlarz, J. Salamon et al. ,
‚ÄúSonyc-ust-v2: An urban sound tagging dataset with spatiotemporal
context,‚Äù arXiv preprint arXiv:2009.05188 , 2020.
[57] D. P. Kingma and J. Ba, ‚ÄúAdam: A method for stochastic optimization,‚Äù
arXiv preprint arXiv:1412.6980 , 2014.
[58] S. Bang, P. Xie, H. Lee, W. Wu, and E. Xing, ‚ÄúExplaining a black-
box by using a deep variational information bottleneck approach,‚Äù in
AAAI Conference on ArtiÔ¨Åcial Intelligence , vol. 35, no. 13, 2021, pp.
11 396‚Äì11 404.
[59] D. Alvarez-Melis and T. Jaakkola, ‚ÄúTowards robust interpretability with
self-explaining neural networks,‚Äù in NeurIPS , 2018, pp. 7775‚Äì7784.
[60] M. LJPvd and G. Hinton, ‚ÄúVisualizing high-dimensional data using t-
sne,‚Äù J Mach Learn Res , vol. 9, no. 2579-2605, p. 9, 2008.
[61] V . Y . Tan and C. F ¬¥evotte, ‚ÄúAutomatic relevance determination in
nonnegative matrix factorization with the/spl beta/-divergence,‚Äù IEEE
TPAMI , vol. 35, no. 7, pp. 1592‚Äì1605, 2012.