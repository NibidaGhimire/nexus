BCSSN: Bi-direction Compact Spatial Separable Network for Collision
Avoidance in Autonomous Driving
Haichuan Li Liguo Zhou Alois Knoll
Chair of Robotics, Artiﬁcial Intelligence and Real-Time Systems
Technical University of Munich
haichuan.li@tum.de, liguo.zhou@tum.de, knoll@in.tum.de
Abstract
Autonomous driving has been an active area of re-
search and development, with various strategies being ex-
plored for decision-making in autonomous vehicles. Rule-
based systems, decision trees, Markov decision processes,
and Bayesian networks have been some of the popular
methods used to tackle the complexities of trafﬁc condi-
tions and avoid collisions. However, with the emergence
of deep learning, many researchers have turned towards
CNN-based methods to improve the performance of colli-
sion avoidance. Despite the promising results achieved by
some CNN-based methods, the failure to establish correla-
tions between sequential images often leads to more col-
lisions. In this paper, we propose a CNN-based method
that overcomes the limitation by establishing feature corre-
lations between regions in sequential images using variants
of attention. Our method combines the advantages of CNN
in capturing regional features with a bi-directional LSTM
to enhance the relationship between different local areas.
Additionally, we use an encoder to improve computational
efﬁciency. Our method takes ”Bird’s Eye View” graphs gen-
erated from camera and LiDAR sensors as input, simulates
the position (x, y) and head offset angle (Yaw) to generate
future trajectories. Experiment results demonstrate that our
proposed method outperforms existing vision-based strate-
gies, achieving an average of only 3.7 collisions per 1000
miles of driving distance on the L5kit test set. This signiﬁ-
cantly improves the success rate of collision avoidance and
provides a promising solution for autonomous driving.
1. Introduction
Trajectory prediction [1, 2, 3] is a critical task in au-
tonomous driving for collision avoidance, as it requires
the autonomous vehicle (A V) to accurately estimate the fu-
ture motion of surrounding objects and plan its own motion
(a) Front Collision
(b) Side Collision
(c) Rear Collision
Figure 1. Different collision cases during driving
accordingly shows in Fig 1. This task is challenging due
to the uncertainty of the surrounding environment and the
complexity of the motion patterns of other vehicles, pedes-
trians, and cyclists. To improve the accuracy of trajectory
prediction, various techniques have been proposed. For in-
stance, multi-modal prediction [4, 5] and uncertainty esti-
mation [6, 7] have been utilized to consider multiple pos-
sible future trajectories and their probabilities. Attention
mechanisms [8, 9] and graph-based models [5, 10] have
been proposed to capture the importance and relationships
among different objects. Convolutional Neural Network
(CNN) has made outstanding contributions to vision tasks
and has been widely applied to trafﬁc scenes due to its ex-
cellent regional feature extraction capabilities. Based on
this advantage, CNN will obtain the local feature informa-
tion of sequential related frames in our network. Addition-
ally, since the motion trajectory is planned for A V , whicharXiv:2303.06714v1  [cs.RO]  12 Mar 2023Figure 2. Overview of Bi-direction Compact Spatial Separable Network
means each position point has a sequential relation (the later
points depend on former points). It is necessary to establish
the relationship between each local feature of the image ob-
tained by CNN. To address this, some strategies use CNN
plus RNN to deal with sequential graphs as input, such as
STDN [11], CRNN [12], LSTM-CNN [13], RCNN [14].
Although the above strategies have performed well in a
large number of vision tasks, their performances are still
far inferior to similar-sized convolutional neural networks
counterparts, such as EfﬁcientNets [15] and RepVGG [16]
in Fig.3. We believe this is due to the following aspects.
First, the huge differences between the sequential tasks of
NLP and the image tasks of CV are ignored. For exam-
ple, when the local feature information acquired in a two-
dimensional image is compressed into one-dimensional
time series information, how to achieve accurate mapping
becomes a difﬁcult problem. Second, it is difﬁcult to keep
the original information of inputs since after RNN layers,
we need to recover the dimension from one to three. Be-
sides, due to the several transformations between different
dimensions, that process becomes even harder, especially
our input size is 224×224×5. Third, the computational and
memory requirement of switching between layers are ex-
tremely heavy tasks, which also becomes a tricky point for
the algorithm to run. Higher hardware requirements as well
as more running time arise when running the attention part.
In this paper, we propose a new network structure based
on CNN, Bi-LSTM, encoder, and attention to trajectory-
generating tasks in autonomous driving. The new network
structure overcomes these problems by using Bi-direction
Compress Sequential Spatial Network (BCSSN). As shownin Fig. 2, input Bird’s Eye View (BEV) images ﬁrst will
be divided into three sub-graphs. Then go through bi-
directional frame-related(BIFR) blocks which consist of
ﬂatten layer, Bi-LSTM layer, and full connect layers. After
that, information will be fed into the main stem, the con-
volution stem for ﬁne-grained feature extraction, and are
then fed into a stack of SSN (Sequential Spatial Network)
blocks in Fig. 4 for further processing. The Upsampling
Convolutional Decreasing (UCD) blocks are introduced for
the purpose of local information enhancement by deep con-
volution, and in SSN block of features generated in the ﬁrst
stage can be less loss of image resolution, which is crucial
for the subsequent trajectory adjustment task.
In addition, we adopt a staged architecture design using
three convolutional layers with different kernel sizes and
steps gradually decreasing the resolution (sequence length)
and ﬂexibly increasing the dimensionality. Such a design
helps to extract local features of different scales and, since
the ﬁrst stage retains high resolution, our design can ef-
fectively reduce the resolution of the output information
in the ﬁrst layer at each convolutional layer, thus reduc-
ing the computational effort of subsequent layers. The Re-
inforcement Region Unit (RRU) and the Fast MultiHead
Self-Attention (FMHSA) in the SSN block can help obtain
global and local structural information within the intermedi-
ate features and improve the normalization capability of the
network. Finally, average pooling is used to obtain better
trajectory tuning.
Extensive experiments on the lykit dataset [17] demon-
strate the superiority of our BCSSN network in terms of
accuracy. In addition to image classiﬁcation, SSN blockcan be easily transferred to other vision tasks and serve as a
versatile backbone.
2. Related Works
Rule-based systems[18, 19, 20]: Rule-based systems are
a type of artiﬁcial intelligence that uses a set of rules to
make decisions. These rules are typically expressed in if-
then statements and are used to guide the system’s behavior.
Rule-based systems have been used in a variety of applica-
tions, including expert systems, decision support systems,
and automated planning systems.
Decision trees[21, 22, 23]: Decision trees are a type of
machine learning algorithm that is used for classiﬁcation
and regression analysis. They are built using a tree struc-
ture, where each internal node represents a decision based
on a feature, and each leaf node represents a prediction. De-
cision trees are widely used in various ﬁelds, including busi-
ness, medicine, and engineering.
Markov Decision Processes[24, 25, 26, 27] (MDPs):
Markov Decision Processes are a mathematical framework
for modeling sequential decision-making problems, where
an agent interacts with an environment in discrete time
steps. The framework involves a set of states, actions, re-
wards, and transition probabilities that govern the agent’s
behavior. MDPs have been used in a wide range of applica-
tions, such as robotics, ﬁnance, healthcare, and transporta-
tion.
Bayesian networks[28, 29, 30]: A Bayesian network is
a probabilistic graphical model that represents a set of ran-
dom variables and their conditional dependencies using a
directed acyclic graph. It is a powerful tool for probabilistic
inference, learning from data, and decision making under
uncertainty.
Except for the mentioned four methods, there are some
popular strategies as well. Over the past decade, au-
tonomous driving has ﬂourished in the wave of deep learn-
ing, where a large number of solution strategies are based
on computer vision algorithms, using images as the primary
input. The prevailing visual neural networks are typically
built on top of a basic block in which a series of convo-
lutional layers are stacked sequentially to capture local in-
formation in intermediate features. However, the limited
receptive ﬁeld of the small convolution kernel makes it dif-
ﬁcult to obtain global information, which hinders the high
performance of the network on highly feature-dependent
tasks (such as trajectory prediction and planning). In view
of this dilemma, many researchers have begun to deeply
study self-attention-based [31] networks with the ability to
capture long-distance information. Here, we brieﬂy review
traditional CNN and recently proposed visual networks.
Convolutional neural network. The ﬁrst standard CNN was
proposed by LeCun [32] et al. and was used for handwrit-
ten character recognition. Based on this foundation, a largenumber of visual models have achieved cross-generational
success in a variety of tasks with images as the main in-
put. Google Inception Net [33] and DenseNet [34] showed
that deep neural networks consisting of convolutional and
pooling layers can yield adequate results in recognition.
ResNet [35] in Fig.3 is a classic structure that has a bet-
ter generalization ability by adding shortcut connections to
the underlying network. To alleviate the limited acceptance
domain in previous studies, some studies used the attention
mechanism as an operator for adapting patterns.
Besides, several novel visual networks have been pro-
posed recently, which have achieved remarkable perfor-
mance in various computer vision tasks. For example,
the Mask R-CNN [36] proposed by He et al. extends the
Faster R-CNN framework with an additional mask branch
to perform instance segmentation. SENet [37] and Mo-
bileNetV3 [38] demonstrate the effectiveness of multiple
paths within a basic block. The DenseNet [34] proposed by
Huang et al. introduced dense connections between lay-
ers to improve feature reuse and alleviate the vanishing-
gradient problems. Moreover, the Transformers-based net-
works, such as ViT [39] in Fig.3 proposed by Dosovitskiy
et al. and DETR [40] proposed by Carion et al., have
achieved state-of-the-art performance on image classiﬁca-
tion and object detection tasks, respectively, by leveraging
the self-attention mechanism. These novel visual networks
have shown promising results and have opened up new re-
search directions in the ﬁeld of computer vision.
3. Method
(a) ResNet-50
 (b) RepVGG
 (c) ViT
Figure 3. Three popular network structures in vision areas. The
structure of ResNet-50 is shown in (a). The structure of RepVGG
is shown in (b). The structure of ViT is shown in (c).
3.1. Trajectory Prediction
Trajectory prediction is an essential task in autonomous
driving for collision avoidance. In this task, the goal is to
predict the future trajectory of the vehicle based on its cur-
rent and past states. This prediction allows the autonomous
vehicle to plan its future path and avoid potential collisions
with other objects in the environment. What we obtain fromFigure 4. Structure of Sequential Spatial Network (SSN)
our model and dataset are X, Y axis position, and yaw.
Since the frame slot is known, we can easily get the veloc-
ity that A V needs. Besides, after combing ﬁction between
ground and wheels, wind resistance, and other physic pa-
rameters, we can calculate the acceleration that A V needs so
that we can control the motor force just like human driving
via an accelerograph. Moreover, yaw can provide a driv-
ing wheel adjustment as well. These processes compose the
basic requirements for autonomous driving.
Some mathematical models such as kinematic or dy-
namic models can show these processes. The kinematic
model assumes that the vehicle moves in a straight line with
a constant velocity and acceleration. The equations of mo-
tion for the kinematic model can be represented as:
x(t) =x0+v0cos(0)t+1
2axt2;
y(t) =y0+v0sin(0)t+1
2ayt2;
(t) =0+!t;(1)
wherex(t)andy(t)are the position of the vehicle at time
tin thexandyaxes, respectively, v0is the initial velocity,
0is the initial orientation, axandayare the acceleration in
thexandyaxes, respectively, and !is the angular velocity.
On the other hand, the dynamic model takes into account
the forces acting on the vehicle, such as friction, air resis-
tance, and gravity. Wind resistance is an important factor to
consider when predicting the trajectory of an autonomous
vehicle. The force of wind resistance can be calculated us-
ing the following formula:
Fwind =1
2v2CdA; (2)
whereFwind is the force of wind resistance, is the density
of air,vis the velocity of the vehicle, Cdis the drag coefﬁ-cient, andAis the cross-sectional area of the vehicle. The
drag coefﬁcient and cross-sectional area can be experimen-
tally determined for a speciﬁc vehicle.
To incorporate wind resistance into the trajectory predic-
tion model, we can use the above formula to calculate the
additional force that the vehicle must overcome. This can
then be used to adjust the predicted trajectory accordingly.
The equations of motion for the total dynamic model can
be represented as:
md2x
dt2=Fx Ffriction  Fwind;
md2y
dt2=Fy Fg Ffriction  Fwind;(3)
wheremis the mass of the vehicle, FxandFyare the
forces acting on the vehicle in the xandyaxes, respectively,
Ffriction andFwind are the forces due to friction and wind
resistance, respectively, and Fgis the force due to gravity.
To predict the future trajectory of an autonomous vehi-
cle, we need to estimate the parameters of the kinematic
or dynamic model based on the current and past states of
the vehicle. This can be done using machine learning tech-
niques such as regression or neural networks.
A popular neural network model for trajectory predic-
tion in autonomous driving is the Long Short-Term Memory
(LSTM) network. LSTM networks are a type of recurrent
neural network that can capture long-term dependencies in
sequential data. The LSTM network can take as input the
current and past states of the vehicle and predict its future
trajectory.
Given a sequence of input states s1;s2;:::;sT, wherest
represents the state of the vehicle at time t, an LSTM net-
work can predict the future states ^st+ 1;^st+ 2;:::;^st+k,
wherekis the number of future time steps to predict.
The LSTM network consists of an input layer, a hidden
layer, and an output layer. The input layer takes the se-
quence of input states as input, and the hidden layer con-
tains recurrent connections that allow the network to main-
tain a memory of past states. The output layer produces the
predicted future states. The LSTM network can be trained
using backpropagation through time to minimize the pre-
diction error.
Several studies have proposed different variations of the
LSTM network for trajectory prediction in autonomous
driving, such as the Social LSTM [41], the ConvL-
STM [42], and the TrajGRU [43]. These models have
shown promising results in predicting the future trajectory
of the vehicle and avoiding potential collisions with other
objects in the environment.
Recent works have also explored the use of self-attention
mechanisms to capture both local and global information
for trajectory prediction in autonomous driving [44]. These
models, such as the proposed SSN architecture, combine(a) former case
(b) later case
Figure 5. Two different cases of predicted points can not reach
CNN and self-attention to capture spatiotemporal features
from input data and improve the ability of sequentially re-
lated inputs.
In summary, trajectory prediction for collision avoidance
in autonomous driving is a crucial task that can be addressed
using recurrent neural networks such as LSTM or hybrid
models that incorporate self-attention mechanisms to cap-
ture both local and global information.
3.2. BIFR Block
To address this sequential relation problem, we also pro-
pose a bi-directional frame-related (BIFR) block to build
up relations between sequential frames, as shown in Fig. 2.
This block consists of one ﬂatten layer, one bi-directional
LSTM, and two fully connected layers. By utilizing bi-
directional LSTMs, the information of both past and fu-
ture frames can be incorporated into the current frame,
which can effectively solve the problem of unreachability
and safety of the predicted trajectory for A V .
The trajectory planning process is based on several way-
points, and these waypoints have a sequential relationship.
The position of the later waypoints depends on the former
waypoints, and the position of the former waypoints should
consider the later waypoints as well. The potential danger
of not considering the sequential relationship between way-
points can be observed from the examples shown in ﬁgures.
The red circles represent the actual positions of the vehicle,
and the blue circles represent the predicted positions based
on the current waypoints. As can be seen from the ﬁgures
shows in Fig. 5, the former waypoints are reachable in the
predicted trajectory but the latter waypoints are not, or vice
versa. Therefore, it is essential to incorporate the sequential
relationship between waypoints into the trajectory planning
process to ensure the safety and feasibility of the trajectory.3.3. Network Structure Overview
Our strategy is to take advantage of CNN, encoder, Bi-
LSTM, and attention by building a hybrid network. CNN
has excellent regional feature extraction capabilities, allow-
ing the network to accurately and quickly capture important
visual features within an image.
hi=f(X
j= 1Nwjxi j+b)hi=f(X
j= 1Nwjxi j+b)
(4)
Encoder can efﬁciently compress and encode the input data,
reducing the computational complexity of the neural net-
work.
Encoder =fenc(x1;x2;:::;xT): (5)
LSTM can learn and model long-term dependencies
within sequential data, making it particularly effective in
modeling and predicting temporal information.
ft=g(Wfxt+Ufht 1+bf);
it=g(Wixt+Uiht 1+bi);
~Ct=c(Wcxt+Ucht 1 +bc);
Ct=ftCt 1+it~Ct;
ot=g(Woxt+Uoht 1 +bo);
ht=oth(Ct):(6)
Attention mechanisms can dynamically weight different
regions of an image or sequence, allowing the network to
selectively focus on the most important information and im-
proving overall performance.
ht=TxX
i=1t;ihi;
ht=TxX
i=1t;ihi;
t;i=exp(et;i)PTx
j=1exp(et;j);
et;i=f(st 1;hi):(7)
The combination of these techniques can allow the net-
work to effectively model complex and dynamic relation-
ships within a scene, allowing for more accurate and robust
predictions.
In particular, the use of attention mechanisms can help
the network to better handle variable and unpredictable in-
puts, allowing it to adapt to a wider range of scenarios and
improve generalization performance.
The bi-directional frame-related(BIFR) block is one of
the fundamental building blocks in our networks, especially
aims to capture sequential information in image data. The
ﬁrst step in the BIFR is to ﬂatten the input sub-graphs,which converts the input data to a one-dimensional format
that can be processed by the following layers. Next, the Bi-
LSTM layer plays a central role in capturing local sequen-
tial information. By recording the temporal dependencies
between the input data, the Bi-LSTM layer enables the net-
work to learn the long-term dependencies between the in-
put and output. To enhance the computational efﬁciency of
the network, only the output value is kept, and the hidden
layer parameters are ignored. This allows the subsequent
layers to process the output value more efﬁciently. Finally,
two fully connected layers are used to map the output value
produced by the Bi-LSTM layer as inputs for later blocks,
which further process the information to extract higher-level
features. Overall, the BIFR is a powerful tool for capturing
sequential information in image data and has the potential
ability widely used in many state-of-the-art computer vision
networks, especially for sequentially related inputs.
3.4. SSN Blcok
Resnet-50 is a powerful architecture that has proven to be
effective for various image classiﬁcation tasks. Its design
comprises of ﬁve stages, with the initial stage0 consisting
of a convolutional layer, a batch normalization layer, and a
maxpooling layer. The bottleneck blocks used in stages 1-4
allow for efﬁcient feature extraction and are followed by a
fully connected layer for classiﬁcation. One of the advan-
tages of this design is that it enables efﬁcient classiﬁcation
between different images. However, stage0 only uses one
convolutional layer with a large kernel size, which can limit
the capture of local information. To address this limitation,
the main input block is introduced, which is composed of
three different kernel sizes and steps convolutional layers.
The main objective of the input block is to extract input in-
formation quickly when the output size is large and then use
the smaller kernel sizes to capture local information as the
input size decreases. This approach helps to improve the
overall performance of our model.
The proposed SSN block consists of a Reinforcement
Region Unit (RRU), a Fast Multi-Head Self-Attention
(FMHSA) module and an Information Reﬁnement Unit
(IRU), as shown in Fig. 4. We will describe these four
components in the following. After taking into account
the fast processing and local information processing of the
main input block, the input information is transferred to the
subsequent blocks for subsequent processing. Furthermore,
between each block, we add a UCD layer, which consists
of a convolutional layer with 1×1 kernel size and a down-
sampling layer which a sampling ratio is 0.5. The UCD
layer allows us to speed up the network without reducing
the amount of information in the input but maintaining the
ratio between the information, and the size of the input is
reduced to half of the original size after the UCD layer. Af-
terward, the feature extraction is performed by a networklayer composed of different numbers of SSN blocks, while
maintaining the same resolution of the input. Due to the
existence of the self-attention mechanism, SSN can cap-
ture the correlation of different local information features,
so as to achieve mutual dependence between local informa-
tion and global information. Finally, the results are out-
put through an average pooling layer and a projection layer
as well as a classiﬁer layer. The reinforcement region unit
(RRU) was proposed as a data augmentation technique for
our network. Data augmentation technique can generate
high-quality augmented samples that maintain the spatial
correlation and semantic information of the original image
by selectively amplifying the informative regions of the in-
put images. This technique can improve the robustness and
generalization of the model, and it has been applied to vari-
ous computer vision tasks such as object detection, seman-
tic segmentation, and image classiﬁcation. Moreover, RRU
can be easily integrated with existing models and does not
require additional computational resources, making it an
efﬁcient and practical data augmentation method for deep
learning models.
In other words, a good model should maintain effective
operating output for similar but variant data as well so that
the model has better input acceptability. However, the ab-
solute position encoding used in the common attention was
originally designed to exploit the order of the tokens, but
it breaks the input acceptability because each patch adds a
unique position encoding to it [45]. Furthermore, the con-
catenation between the local information obtained by the in-
formation capture module at the beginning of the model and
the structural information inside the patch [46] is ignored.
In order to maintain input acceptability, the Reinforcement
Region Unit (RRU) is designed to extract the local informa-
tion from the input to the ”SSN” module, deﬁned as:
RRU (X) =Conv (Conv (X)): (8)
The FMHSA module in our model is designed to en-
hance the connection between local information obtained
from the RRU. With the use of a convolutional layer, a linear
layer, and multi-head self-attention, we can effectively cap-
ture the correlation between different local features. This
is particularly useful for the collision avoidance task, where
the trajectory prediction is based on continuously predicted
positions that are sequentially related. The FMHSA module
allows for the transfer of local information between differ-
ent areas, making it suitable for solving this problem. By
improving the connection between local information, our
model is able to achieve outstanding results in this task. Ad-
ditionally, the sequential relationship between predicted po-
sitions is taken into account, ensuring that the autonomous
driving vehicle arrives at each target position before moving
on to the next one.
The Information Reﬁnement Unit (IRU) is used to efﬁ-ciently extract the local information obtained by FMHSA,
and after processing by this unit, the extracted local infor-
mation is fed into the pooling and classiﬁer layers. The orig-
inal FFN proposed in ViT consists of two linear layers sep-
arated by the GELU activation [47]. First, expand the input
dimension by 4 times, and then scale down the expanded
dimension:
FFN (X) =GELU (XW1 +b1)W2 +b2: (9)
This has the advantage of using a linear function for for-
ward propagation before using the GELU() function, which
greatly improves the efﬁciency of the model operation. The
proposed design concept is to address the performance sac-
riﬁce issue of using the linear function for forward propa-
gation in the FFN of ViT. The solution is to use a combina-
tion of convolutional layers with different kernel sizes and a
linear function layer to achieve both operational efﬁciency
and model performance. First, a larger convolutional ker-
nel is used to capture the input information’s characteristics
with a large ﬁeld of view. Then, the linear function layer
is applied for fast propagation of the input information. Fi-
nally, a smaller convolutional kernel is used to reﬁne the ob-
tained information. By using this approach, the model can
efﬁciently process the input information while maintaining
high performance during fast propagation. This strategy is
particularly useful for vision tasks such as image classiﬁca-
tion and object detection, where the model’s performance
and operational efﬁciency are crucial.
IRU (X) =Conv (L(Conv (X))); (10)
where L(X)=WX+b. After designing the above three unit
modules, the SSN block can be formulated as:
A=RRU (X);
B=FMHSA (A);
C=IRU (B) +B:(11)
In the experiment part, we will prove the efﬁciency of
SSN network. In addition to its ability to capture the tem-
poral dependencies of sequential data, SSN also has versa-
tility in handling different downstream tasks. Speciﬁcally,
SSN employs a self-attention mechanism to capture the re-
lationships between the different temporal feature maps.
This mechanism can effectively encode the dependencies
between different timesteps and extract the most relevant
information for the downstream tasks. Furthermore, SSN
is designed to handle the sparsity and irregularity of the se-
quential data, which is common in real-world driving sce-
narios. These features make SSN a promising candidate for
various applications in autonomous driving, such as trajec-
tory prediction, behavior planning, and decision-making.4. Experiment
The autonomous driving obstacle avoidance task is cru-
cial in ensuring safe driving in real-life scenarios. To eval-
uate the effectiveness of the BCSSN architecture, experi-
ments were conducted using a driving map as the main in-
put. We use the Earlystopping function with 100 patience
for ﬁnding the best parameters’ value. Besides, we ap-
ply cosine dynamic learning rate [48] adjustment for tuning
suitable values at each iteration. To implement the cosine
dynamic learning rate adjustment, we use the following for-
mula:
t=max1 + cos(t
T)
2(12)
wheremax is the maximum learning rate, tis the current
iteration, and Tis the total number of iterations.
The EarlyStopping function is used to monitor the val-
idation loss and stop the training process when the val-
idation loss does not improve for a certain number of
epochs. Speciﬁcally, we use the patience parameter, which
determines how many epochs to wait for the validation
loss to improve before stopping the training process. The
EarlyStopping function is deﬁned as follows:
EarlyStopping (Vloss;patience ) (13)
whereVlossis the validation loss and patience is the number
of epochs to wait for the validation loss to improve. In the
experiments, the proposed BCSSN architecture was com-
pared with other popular models. The experimental results
in Table 1 were then analyzed to draw conclusions on the
performance of the BCSSN architecture. To assess the per-
formance of the BCSSN architecture, collisions were de-
ﬁned into three categories: front collision, rear collision,
and side collision. These collisions were caused by differ-
ent unsuitable physical parameters, and they are depicted
in Fig. 1. The experiments aimed to test the ability of the
BCSSN architecture to detect and avoid these collisions ef-
fectively. The results of the experiments were analyzed, and
the performance of the BCSSN architecture was compared
with other popular models to determine its effectiveness in
addressing the obstacle avoidance task. The analytical con-
clusions drawn from these experiments provide valuable in-
sights into the efﬁciency and efﬁcacy of the proposed BC-
SSN architecture.
4.1. Dataset and Description
The l5kit dataset is a vast collection of data, which serves
as the primary source for this study. It is an extensive dataset
containing over 1,000 hours of data collected over a four-
month period by a ﬂeet of 20 autonomous vehicles that fol-
lowed a ﬁxed route in Palo Alto, California. The dataset
comprises 170,000 scenes, where each scene lasts for 25
seconds. Each scene captures the perception output of theself-driving system, which provides precise positions and
motions of nearby vehicles, cyclists, and pedestrians over
time. This information is invaluable for training and testing
autonomous driving models.
In addition to the scene data, the l5kit dataset also con-
tains a high-deﬁnition semantic map with 15,242 labeled
elements, which includes information about the lane mark-
ings, trafﬁc signs, and other relevant information. This se-
mantic map is used to help the autonomous vehicle under-
stand the environment and make informed decisions. Fur-
thermore, the dataset also includes a high-deﬁnition aerial
view of the area, which provides a top-down perspective of
the environment.
Overall, the l5kit dataset is an invaluable resource for
researchers and developers working in the ﬁeld of au-
tonomous driving. With its vast collection of data, including
the perception output of the self-driving system, semantic
map, and aerial view, the dataset provides a comprehen-
sive understanding of the driving environment, which can
be used to train and test autonomous driving models.
4.2. Data Preprocessing
The l5kit dataset has a well-deﬁned structure, consist-
ing of three main concepts: Scenes, Frames, and Agents.
Scenes are identiﬁed by the host vehicle that collected them
and a start and end time. Each scene consists of multiple
frames, which are snapshots captured at discretized time in-
tervals. The frames are stored in an array, and the scene
datatype stores a reference to its corresponding frames in
terms of the start and end index within the frames array. All
the frames in between these indices correspond to the scene,
including the start index, but excluding the end index.
Each frame captures all the information observed at a
particular time, including the timestamp, which the frame
describes. The frame also includes data about the ego ve-
hicle itself, such as its position and rotation. Additionally,
the frame contains a reference to other agents (such as ve-
hicles, cyclists, and pedestrians) that were detected by the
ego’s sensors. The frame also includes a reference to all
trafﬁc light faces for all visible lanes.
An agent in the l5kit dataset is an observation made by
the autonomous vehicle of another detected object. Each
agent entry describes the object in terms of its attributes,
such as position and velocity, and gives the agent a tracking
number to track it over multiple frames (but only within the
same scene!) and its most probable label.
The input of this dataset is images of the Ego car, which
is one of the properties of the EgoDataset. The output of the
model is the position and yaw, which are also properties of
the EgoDataset. By using this method, it is possible to sim-
ulate vehicles’ driving as human driving actions. During the
human driving process, drivers control the accelerator and
the driving wheels to move the vehicles, where the accel-erator is used for velocity and the driving wheel for yaw.
Similarly, the output of the model is also the velocity and
yaw, which can be used to simulate the trajectories of vehi-
cles and avoid collisions during driving.
4.3. Result
We conducted experiments using four different network
structures and presented their results in Table 1. We com-
pared our model’s performance with other transformer-
based and convolution-based models and found that our
model achieved better accuracy and faster processing speed.
The results highlighted the signiﬁcance of our proposed
BCSSN for capturing both local and global information,
which resulted in a signiﬁcant reduction of front collision
errors. Speciﬁcally, our model achieved 2.6 times fewer
front collision errors compared to RepVGG, which is 13.6
times less than RepVGG’s error rate. Similarly, our model
achieved 5.8 times fewer front collision errors than ViT and
12.6 times less than ResNet50. Other comparisons can ob-
serve form result table 1 directly as well. These results
demonstrate the effectiveness of the BCSSN in capturing
both local and global information, leading to signiﬁcant im-
provements in the model’s accuracy.
BCSSN consistently outperformed other models by a
large margin across all the test scenarios. These ﬁndings in-
dicate that the proposed BCSSN block is a promising tech-
nique for improving the performance of autonomous driv-
ing systems by effectively capturing both local and global
information.
Table 1. Average collision times per 1000 miles.
Method Front Side Rear Total
BC [49] 79 395 997 1471
BC-perturb [49] 14 73 678 765
MS Prediction [49] 18 55 125 198
Urban Driver [49] 15 46 101 162
ML Planner [50] - - - 91.5
L5Kit
(ResNet-50) [51]15.2 20.7 8.3 44.2
RepVGG [16] 16.2 11.7 10.6 38.5
Vit [39] 8.4 7.7 9.2 25.3
SafetyNet [50] - - - 4.6
BCSSN (Ours) 0.6 1.1 2.0 3.7(a) Bird’s eye view of virtual city
(b) First person view of virtual city
Figure 6. Autonomous driving vehicle in Unity virtual city
5. Feature Work
To integrate our algorithm model with the Unity vir-
tual world, we need to establish communication between
them. We accomplish this by using Unity’s built-in API and
message-passing techniques, a ﬁrst-person view, and bird
eye view ﬁgure shown in Fig. 6 . The communication pro-
tocol is established such that the algorithm sends the input
information to the virtual world, and the virtual world re-
turns the output information to the algorithm.
To ensure accurate physical modeling and simulation, we
need to perform coordinate transformation between the im-
age coordinates and the world coordinates. The process in-
volves mapping the image coordinates to the virtual world
coordinates using the camera parameters and the intrinsic
and extrinsic calibration matrices. The resulting coordi-
nates are then transformed to the physical world coordi-
nates, where the physical model can calculate the necessary
forces and physical parameters for the autonomous driving
system.
This transformation can be mathematically represented
as:2
4Xworld
Yworld
Zworld3
5=R2
4Xcamera
Ycamera
Zcamera3
5+T (14)where Ris the rotation matrix, Tis the translation vector,
and[Xcamera;Ycamera;Zcamera ]are the coordinates in the
camera frame.
To obtain accurate values for RandT, we need to per-
form calibration using various methods such as checker-
board pattern or laser scanning techniques [52]. The result-
ing calibration parameters can then be used to transform the
image coordinates to world coordinates for accurate physi-
cal modeling and simulation.
6. Conclusion
In this paper, we introduce a novel hybrid architecture
named BCSSN that is designed to improve the ability of
vision-based autonomous driving tasks as well as other vi-
sion tasks. The BCSSN architecture combines the strengths
of several different techniques, including CNN, Bi-LSTM,
encoders, and self-attention, to capture both local and global
information from sequentially related inputs.
The CNN are used to extract local information from the
input images, while the Bi-LSTM is used to model the tem-
poral dependency between the frames. The encoder is used
to convert the input images into a high-level feature repre-
sentation, which is then passed to the self-attention mech-
anism to capture the long-range dependencies and context
information.
To evaluate the effectiveness of the proposed BCSSN
architecture, we conducted extensive experiments on the
Lykit dataset, which is a challenging dataset for vision-
based autonomous driving tasks. The results of these ex-
periments demonstrate the superiority of the proposed BC-
SSN architecture compared to other state-of-the-art models.
Speciﬁcally, the BCSSN architecture achieved better per-
formance in terms of accuracy, speed, and generalization
ability.
Overall, BCSSN architecture offers a promising solution
for vision-based autonomous driving tasks and other vision
tasks that require the ability to capture both local and global
information from sequentially related inputs. Its hybrid de-
sign allows it to take advantage of the strengths of several
different techniques, resulting in improved performance and
generalization ability.
References
[1] Zhenyu Wang, Zhiyong Li, Jinye Li, Wentong Cai, and Xi-
aoqing Liu. Trajectory prediction for interactive virtual char-
acters. Multimedia Tools and Applications , 77(1):191–212,
2018.
[2] Kai Liu, Huan Zhang, Jingjing Li, Li Li, Liyan Li, Yu Li,
Wei Song, and Chen Wang. A hybrid model for trajectory
prediction of moving objects in a multi-sensor system. Sen-
sors, 18(6):1753, 2018.
[3] Luis Rodrigues, David Goncalves, Lu ´ıs Lopes, and
Lu´ıs Paulo Reis. Multiagent system for trajectory predic-tion of multiple football players. In Progress in Artiﬁcial
Intelligence , volume 10423, pages 375–386. Springer, 2017.
[4] Navneet Deo, Srikant Sahu, Mayank Vatsa, and Richa Singh.
End-to-end multi-modal multi-task learning for emotion
recognition. Expert Systems with Applications , 149:113244,
2020.
[5] Peng Zhao, Hanbing Yan, Yinhuan Guo, Yongqiang Cheng,
and Peng Huang. Multi-task learning with dynamic weight-
ing for diabetic retinopathy diagnosis. Computers in biology
and medicine , 109:179–187, 2019.
[6] Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian
approximation: Representing model uncertainty in deep
learning. In International Conference on Machine Learning ,
pages 1050–1059. PMLR, 2016.
[7] Abhishek Bhattacharyya, Ujjwal Chaudhuri, and Dipti
Swain. Latent space regularized generative adversarial net-
works for hyperspectral image synthesis. IEEE Journal of
Selected Topics in Applied Earth Observations and Remote
Sensing , 13:1683–1696, 2020.
[8] Dong-Hee Kim, Youngsun Kwon, and Hyunwoo Park.
Deterministic and probabilistic deep learning approaches
for prediction of trafﬁc ﬂow using temporal features.
IEEE Transactions on Intelligent Transportation Systems ,
20(1):219–228, 2019.
[9] Chao Ma, Xiaojie Huang, Zhihao Yang, Yafei Wang, Chunx-
iao Yang, Dengshi Huang, Zhiheng Wang, Wayne Wang, To-
bias Weyand, Wei Sun, et al. Trafﬁcpredict: Trajectory pre-
diction for heterogeneous trafﬁc-agents. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 1307–1317, 2019.
[10] Changqing Yu, Yabo Dong, Jiangnan Pang, Xiaohu Huang,
Xiaowei Zhang, and Jian Sun. Grnet: Gridding residual
network for dense point cloud completion. arXiv preprint
arXiv:2103.16287 , 2021.
[11] Huaxiu Yao, Xianfeng Tang, Hua Wei, Guanjie Zheng, Yan-
wei Yu, and Zhenhui Li. Modeling spatial-temporal dynam-
ics for trafﬁc prediction. arXiv preprint arXiv:1803.01254 ,
1:9, 2018.
[12] Andrew Shin, Seunghyun Lee, Hyun-Woo Lee, and Hyun-
woong Kim. Deep convolutional neural networks and re-
current neural networks for detection of fetal hypoxia from
cardiotocography. IEEE Journal of Biomedical and Health
Informatics , 21(1):274–282, 2017.
[13] Nicolas Ballas, Li Yao, Christopher Pal, and Aaron
Courville. Delving deeper into convolutional net-
works for learning video representations. arXiv preprint
arXiv:1511.06432 , 2015.
[14] Liang-Chieh Chen and Yi Yang. Recurrent convolutional
neural network for object recognition. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 3367–3375, 2017.
[15] Mingxing Tan and Quoc Le. Efﬁcientnet: Rethinking model
scaling for convolutional neural networks. In International
conference on machine learning , pages 6105–6114. PMLR,
2019.[16] Xiaohan Ding, Xiangyu Zhang, Ningning Ma, Jungong Han,
Guiguang Ding, and Jian Sun. Repvgg: Making vgg-style
convnets great again. In Proceedings of the IEEE/CVF con-
ference on computer vision and pattern recognition , pages
13733–13742, 2021.
[17] John Houston, Guido Zuidhof, Luca Bergamini, Yawei
Ye, Long Chen, Ashesh Jain, Sammy Omari, Vladimir
Iglovikov, and Peter Ondruska. One thousand and one hours:
Self-driving motion prediction dataset. In Jens Kober, Fabio
Ramos, and Claire Tomlin, editors, Proceedings of the 2020
Conference on Robot Learning , volume 155 of Proceedings
of Machine Learning Research , pages 409–418. PMLR, 16–
18 Nov 2021.
[18] Richard W Harper, Michael W Lehman, James E Miller,
Kevin B Johnson, James L Myers, Thomas R Martin,
Thomas A Platts-Mills, Peter J Sterk, Vito G Brusasco, et al.
Rule-based expert systems for diagnosis and management of
asthma. In AMIA Annual Symposium Proceedings , volume
1993, page 370. American Medical Informatics Association,
1993.
[19] Ihsan Yilmaz. Rules versus decision trees for classiﬁcation
of medical data. Journal of medical systems , 28(2):125–130,
2004.
[20] Franziska Hinkelmann, Michael Brandon, Bonnie Guang,
and Hans V Westerhoff. Rule-based modeling and simula-
tion of biochemical systems with rulemonkey. In Interna-
tional Workshop on Computational Methods in Systems Bi-
ology , pages 182–191. Springer, 2004.
[21] Leo Breiman, Jerome H Friedman, Richard A Olshen,
and Charles J Stone. Classiﬁcation and regression trees.
Wadsworth International Group , 1984.
[22] Bhavani Mehta and Rakesh Agrawal. Using decision trees
to improve case-based learning. In Proceedings of the Thir-
teenth International Conference on Machine Learning , pages
327–335. Morgan Kaufmann Publishers Inc., 1996.
[23] Konstantinos Veropoulos, Colin Campbell, and Nello Cris-
tianini. Controlling the complexity of decision trees by prun-
ing and visualization. Proceedings of the British Machine
Vision Conference , 1:129–138, 1999.
[24] Martin L Puterman. Markov decision processes: discrete
stochastic dynamic programming. John Wiley & Sons , 2014.
[25] V olodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza,
Alex Graves, Timothy Lillicrap, Tim Harley, David Silver,
and Koray Kavukcuoglu. Asynchronous methods for deep
reinforcement learning. In International conference on ma-
chine learning , pages 1928–1937. PMLR, 2016.
[26] St ´ephane Ross, Geoffrey J Gordon, and J Andrew Bag-
nell. Reinforcement and imitation learning via interactive
no-regret learning. Foundations and Trends® in Machine
Learning , 7(1):1–129, 2014.
[27] Oliver Lemon and Shimon Whiteson. Multi-agent reinforce-
ment learning: A selective overview of theories and algo-
rithms. arXiv preprint arXiv:1904.00943 , 2019.
[28] Daphne Koller and Nir Friedman. Probabilistic graphical
models: principles and techniques. MIT press , 2009.[29] Finn V Jensen. Bayesian networks and decision graphs. In
Springer , pages 17–39. Springer, 2001.
[30] David Heckerman. Learning bayesian networks: The combi-
nation of knowledge and statistical data. Machine learning ,
20(3):197–243, 1995.
[31] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. Advances in neural
information processing systems , 30, 2017.
[32] Yann LeCun, Bernhard Boser, John S Denker, Donnie
Henderson, Richard E Howard, Wayne Hubbard, and
Lawrence D Jackel. Backpropagation applied to handwrit-
ten zip code recognition. Neural computation , 1(4):541–551,
1989.
[33] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,
Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent
Vanhoucke, and Andrew Rabinovich. Going deeper with
convolutions. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pages 1–9, 2015.
[34] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kil-
ian Q Weinberger. Densely connected convolutional net-
works. In Proceedings of the IEEE conference on computer
vision and pattern recognition , pages 4700–4708, 2017.
[35] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition , pages 770–778, 2016.
[36] Kaiming He, Georgia Gkioxari, Piotr Doll ´ar, and Ross Gir-
shick. Mask r-cnn. In Proceedings of the IEEE international
conference on computer vision , pages 2980–2988, 2017.
[37] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation net-
works. In Proceedings of the IEEE conference on computer
vision and pattern recognition , pages 7132–7141, 2018.
[38] Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh
Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu,
Ruoming Pang, Vijay Vasudevan, et al. Searching for mo-
bilenetv3. In Proceedings of the IEEE/CVF international
conference on computer vision , pages 1314–1324, 2019.
[39] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Trans-
formers for image recognition at scale. arXiv preprint
arXiv:2010.11929 , 2020.
[40] Nicolas Carion, Francisco Massa, Alexander Kirillov, and
Ross Girshick. End-to-end object detection with transform-
ers. In European conference on computer vision , pages 213–
229. Springer, 2020.
[41] Alexandre Alahi, Kratarth Goel, Vignesh Ramanathan,
Alexandre Robicquet, Li Fei-Fei, and Silvio Savarese. So-
cial lstm: Human trajectory prediction in crowded spaces.
InProceedings of the IEEE Conference on Computer Vision
and Pattern Recognition , pages 961–971, 2016.[42] SHI Xingjian, Zhourong Chen, Hao Wang, Dit-Yan Yeung,
Wai-Ki Wong, and Wing-Kin Woo. Convolutional lstm net-
work: A machine learning approach for precipitation now-
casting. arXiv preprint arXiv:1506.04214 , 2015.
[43] Agrim Gupta, Justin Johnson, Li Fei-Fei, Silvio Savarese,
and Alexandre Alahi. Social gan: Socially acceptable tra-
jectories with generative adversarial networks. In European
Conference on Computer Vision (ECCV) , 2018.
[44] Yingfeng Li, Zeyu Qin, Wenyu Sun, Guanbin Li, and
Liang Wang. A multi-head self-attention network for au-
tonomous driving trajectory prediction. In 2021 IEEE In-
ternational Conference on Robotics and Automation (ICRA) ,
pages 12271–12277. IEEE, 2021.
[45] Xiangxiang Chu, Zhi Tian, Bo Zhang, Xinlong Wang, Xi-
aolin Wei, Huaxia Xia, and Chunhua Shen. Conditional po-
sitional encodings for vision transformers. arXiv preprint
arXiv:2102.10882 , 2021.
[46] Md Amirul Islam, Sen Jia, and Neil DB Bruce. How much
position information do convolutional neural networks en-
code? arXiv preprint arXiv:2001.08248 , 2020.
[47] Dan Hendrycks and Kevin Gimpel. Gaussian error linear
units (gelus). arXiv preprint arXiv:1606.08415 , 2016.
[48] Leslie N Smith. A disciplined approach to neural network
hyper-parameters: Part 1–learning rate, batch size, momen-
tum, and weight decay. arXiv preprint arXiv:1803.09820 ,
2018.
[49] Oliver Scheel, Luca Bergamini, Maciej Wolczyk, Bła ˙zej
Osi´nski, and Peter Ondruska. Urban driver: Learning to
drive from real-world demonstrations using policy gradients.
InConference on Robot Learning , pages 718–728. PMLR,
2022.
[50] Matt Vitelli, Yan Chang, Yawei Ye, Ana Ferreira, Maciej
Wołczyk, Bła ˙zej Osi ´nski, Moritz Niendorf, Hugo Grimmett,
Qiangui Huang, Ashesh Jain, et al. Safetynet: Safe planning
for real-world self-driving vehicles using machine-learned
policies. In 2022 International Conference on Robotics and
Automation (ICRA) , pages 897–904. IEEE, 2022.
[51] John Houston, Guido Zuidhof, Luca Bergamini, Yawei
Ye, Long Chen, Ashesh Jain, Sammy Omari, Vladimir
Iglovikov, and Peter Ondruska. One thousand and one hours:
Self-driving motion prediction dataset. In Conference on
Robot Learning , pages 409–418. PMLR, 2021.
[52] Jia Zhang and Sanjiv Singh. An evaluation of camera calibra-
tion methods for autonomous driving. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion Workshops , pages 2344–2353, 2018.