W orking Paper. Do not distribute. (2021) RESEARCH NOTE
When can Regression-Adjusted Control Variates Help?
Rare Events, Sobolev Embedding and Minimax Optimality
Jose Blanchet,*,1,2,3Haoxuan Chen,*,1,3Yiping Lu,*,1,3and Lexing Ying*,1,3,4
1alphabetical order
2Department of Management Science & Engineering, Stanford University, CA, USA
3ICME, Stanford University, CA, USA
4Department of Mathematics, Stanford University, CA, USA
*Corresponding author. Email: jose.blanchet@stanford.edu; haoxuanc@stanford.edu; yplu@stanford.edu; lexing@stanford.
edu
Abstract
This paper studies the use of a machine learning-based estimator as a control variate for mitigating the
variance of Monte Carlo sampling. Specifically, we seek to uncover the key factors that influence the
efficiency of control variates in reducing variance. We examine a prototype estimation problem that
involves simulating the moments of a Sobolev function based on observations obtained from (random)
quadrature nodes. Firstly, we establish an information-theoretic lower bound for the problem. We then
study a specific quadrature rule that employs a nonparametric regression-adjusted control variate to reduce
the variance of the Monte Carlo simulation. We demonstrate that this kind of quadrature rule can improve
the Monte Carlo rate and achieve the minimax optimal rate under a sufficient smoothness assumption.
Due to the Sobolev Embedding Theorem, the sufficient smoothness assumption eliminates the existence
of rare and extreme events. Finally, we show that, in the presence of rare and extreme events, a truncated
version of the Monte Carlo algorithm can achieve the minimax optimal rate while the control variate
cannot improve the convergence rate.
Keywords: Monte Carlo, Sobolev Embedding, Rare Events, Minimax Optimality, Control Variate
1. Introduction
In this paper, we consider a nonparametric quadrature rule on (random) quadrature points based on
regression-adjusted control variate (Asmussen and Glynn 2007; Davidson and MacKinnon 1992;
Oates and Girolami 2016; Hickernell, Lemieux, and Owen 2005). To construct the quadrature rule,
we partition our available data into two halves. The first half is used to construct a nonparametric
estimator, which is then utilized as a control variate to reduce the variance of the Monte Carlo
algorithm implemented over the second half of our data. Traditional and well-known results
Asmussen and Glynn 2007, Chapter 5.2 show that the optimal linear control variate can be obtained
via Ordinary Least Squares regression. In this paper, we investigate a similar idea for constructing a
quadrature rule (Oates and Girolami 2016; Assaraf and Caffarel 1999; Mira, Solgi, and Imparato 2013;
Oates, Girolami, and Chopin 2017; Oates et al. 2019; South et al. 2018; Holzmüller and Bach 2023),
which uses a non-parametric machine learning-based estimator as a regression-adjusted control
variate. We aim to answer the following two questions:
Is using optimal nonparametric machine learning algorithms to construct control variates
an optimal way to improve Monte Carlo methods? What are the factors that determine the
©Working Paper 2021. This is an Open Access article, distributed under the terms of the Creative Commons Attribution licence
(http://creativecommons.org/licenses/by/4.0/), which permits unrestricted re-use, distribution, and reproduction in any medium, provided the
original work is properly cited.arXiv:2305.16527v1  [math.ST]  25 May 20232 Jose Blanchet et al.
effectiveness of the control variate?
xxYY(a)(b)Rare and extreme event
Figure 1. According to the Sobolev Embedding Theorem (Adams and Fournier 2003), the Sobolev space Ws,pcan be
embedded in Lp∗, where1
p∗=1
p–s
d. When sis large enough, as shown in (a), the smoothness assumption can rule out the
existence of rare and extreme events. When sis not sufficiently large, specifically s<2dq–dp
2pq, there may exist a peak ( a.k.a rare
and extreme event) that makes the Monte Carlo simulation hard. Under such circumstances, the function’s 2q-th moment is
unbounded.
To understand the two questions, we consider a basic but fundamental prototype problem
of estimating moments of a Sobolev function from its values observed on (random) quadrature
nodes, which has a wide range of applications in Bayesian inference, the study of complex systems,
computational physics, and financial risk management (Asmussen and Glynn 2007). Specifically, we
estimate the q-th momentR
Ωf(x)qdxoffbased on values f(x1),···,f(xn) observed on n(random)
quadrature nodes x1,···,xn∈Ωfor a function fin the Sobolev space Ws,p(Ω), where Ω⊂Rd.
The parameter qhere is introduced to characterize the rare events’ extremeness for estimation. To
verify the effectiveness of the non-parametric regression adjusted quadrature rule, we first study
the statistical limit of the problem by providing a minimax information-theoretic lower bound of
magnitude nmax{(1
p–s
d)q–1,–s
d–1
2}.
We also provide matching upper bounds for different levels of function smoothness. Under the
sufficient smoothness assumption that s>d(2q–p)
2pq, we find that the non-parametric regression adjusted
control variateˆfcan improve the rate of classical Monte Carlo algorithm and help us attain a minimax
optimal upper bound. In (7) below, we bound varianceR
Ω(fq–ˆfq)2of the Monte Carlo target
by the sum of the semi-parametric influence partR
Ωf2q–2(f–ˆf)2and the propagated estimation
errorR
Ω(f–ˆf)2q. Although the optimal algorithm in this regime remains the same, we need to
consider three different cases to derive an upper bound on the semi-parametric influence part, which
is the main contribution of our proof. We propose a new proof technique that embeds the square of
the influence function ( qfq–1)2and estimation error ( f–ˆf)2in appropriate spaces via the Sobolev
Embedding Theorem (Adams and Fournier 2003). The two norms used for evaluating ( fq–1)2and
(f–ˆf)2should be dual norms of each other. Also, we should select the norm for evaluating ( f–ˆf)2in
a way that it’s easy to estimate funder the selected norm, which helps us control the error induced
by (f–ˆf)2. A detailed explanation of how to select the proper norms in different cases via the Sobolev
Embedding Theorem is exhibited in Figure 2. In the first regime when s>d
p, we can directly embed
finL∞(Ω) and attain a final convergence rate of magnitude n–s
d–1
2. For the second regime when
d(2q–p)
p(2q–2)<s<d
p, the smoothness parameter sis not large enough to ensure that f∈L∞(Ω). Thus,
we evaluate the estimation error ( f–ˆf)2under the Lp
2norm and embed the square of the influence
function ( qfq–1)2in the dual space of Lp
2(Ω). Here the validity of such embedding is ensured byW orking Paper. Do not distribute. 3
the lower boundd(2q–p)
p(2q–2)ons. Moreover, the semi-parametric influence part is still dominant in the
second regime, so the final convergence rate is the same as that of the first case. In the third regime,
whend(2q–p)
2pq<s<d(2q–p)
p(2q–2), the semi-parametric influence no longer dominates and the final converge
rate transits from n–s
d–1
2tonq(1
p–s
d)–1.
When the sufficient smoothness assumption breaks, i.e. s <d(2q–p)
2pq, according to the Sobolev
Embedding Theorem (Adams and Fournier 2003), the Sobolev space Ws,pis embedded in Ldp
d–sp
anddp
d–sp< 2q. This indicates that rare and extreme events might be present, and they are not even
guaranteed to have bounded L2qnorm, which makes the Monte Carlo estimate of the q-th moment
have infinite variance. Under this scenario, we consider a truncated version of the Monte Carlo
algorithm, which can be proved to attain the minimax optimal rate of magnitude nq(1
p–s
d)–1. In
contrast, the usage of regression-adjusted control variates does not improve the convergence rate
under this scenario. Our results reveal how the existence of rare events will change answers to the
questions raised at the beginning of the section.
We also use the estimation of a linear functional as an example to investigate the algorithm’s
adaptivity to the noise level. In this paper, we provide minimax lower bounds for estimating the
integral of a fixed function with a general assumption on the noise level. Specifically, we consider
all estimators that have access to observations { xi,f(xi) +ϵi}n
i=1of some function fthat is s-Hölder
smooth, where xii.i.d∼Uniform ([0, 1]d) and ϵii.i.d∼n–γN(0, 1) for some γ> 0. Based on the method
of two fuzzy hypotheses, we present a lower bound of magnitude nmax{–1
2–γ,–1
2–s
d}, which exhibits a
smooth transition from the Monte Carlo rate to the Quasi-Monte Carlo rate. At the same time, our
information-theoretic lower bound also matches the upper bound built for quadrature rules taking
use of non-parametric regression-adjusted control variates.
1.1 Related Work
Regression-adjusted Control Variate Regression-adjusted control variates have shown both theo-
retical and empirical improvements in a broad range of applications, including the construction of
confidence intervals (Angelopoulos et al. 2023; Romano, Patterson, and Candes 2019), randomized
trace-estimation, (Meyer et al. 2021; Sobczyk and Luisier 2022; Lin 2017), dimension reduction
(Sobczyk and Luisier 2022), causal inference (Liu and Yang 2020), estimation of the normalizing
factor (Holzmüller and Bach 2023) and gradient estimation (Shi et al. 2022; Liu et al. 2017). It is also
used as a technique used for proving the approximation bounds on two-layer neural networks in the
Barron space (Siegel and Xu 2022).
In connection to the related literature to our work, we mention (Oates and Girolami 2016;
Oates, Girolami, and Chopin 2017; Oates et al. 2019; Holzmüller and Bach 2023), which also study
the use of nonparametric control variate estimator. However, the theoretical analysis in (Oates
and Girolami 2016; Oates, Girolami, and Chopin 2017) does not provide a specific convergence
rate in the Reproducing Kernel Hilbert Space, which requires a high level of smoothness for the
underlying function. In contrast to prior work, our research delves into the effectiveness of a non-
parametric regression-adjusted control variate in boosting convergence rates across various degrees
of smoothness assumptions and identifies the key factor that determines the effectiveness of these
control variates.
Quadrature Rule There is a long literature on building quadrature rules in the Reproducing Kernel
Hilbert Space, including Bayes–Hermite quadrature (O’Hagan 1991; Kanagawa, Sriperumbudur, and
Fukumizu 2016; Bach 2017; Karvonen and Sarkka 2018; Kanagawa and Hennig 2019), determinantal
point processes (Belhadji, Bardenet, and Chainais 2019; Belhadji 2021; Bardenet and Hardy 2020;4 Jose Blanchet et al.
Smoothness s
Truncate Monte CarloRegression-adjusted Control Variatemax{(1p−sd)q−1,−12−sd}1p=sd1p−sd=12qEmbed f in L∞Embed f in Lpdd−spLp
Minimax rateL2pq−2pp−2Estimation in L2p*p*+2−2qEstimation in L2−1/2Choose an embedding good for both evaluating the semi-parametric hardness and function estimation
Figure 2. We summarize the minimax optimal rates and the corresponding optimal algorithms with respect to the function
smoothness here. When the function is smooth enough, regression-adjusted control variates can improve the Monte Carlo
rate. However, when there exist rare and extreme events that are hard to simulate, truncating the Monte Carlo estimate
directly yields a minimax optimal algorithm. Above the transition point of algorithm selection is s=d(2q–p)
2pq, while the
transition point of the optimal convergence rate is s=d(2q–p)
p(2q–2). To build the optimal convergence guarantee for any algorithm
that utilizes a regression-adjusted control variateˆf, we need to embed the square of the influence function (qfq–1)2in an
appropriate space via the Sobolev Embedding Theorem and evaluate the estimation error (f–ˆf)2under the dual norm of
the norm associated with the chosen space, which allows us to achieve optimal semi-parametric efficiency. Our selections
of the metrics in different regimes are shown in this figure.W orking Paper. Do not distribute. 5
Gautier, Bardenet, and Valko 2019), Nyström approximation (Hayakawa, Oberhauser, and Lyons
2021, 2023), kernel herding(Chen, Welling, and Smola 2012; Lacoste-Julien, Lindsten, and Bach
2015; Huszár and Duvenaud 2012) and kernel thinning (Chen et al. 2018; Dwivedi and Mackey
2021b, 2021a). Nevertheless, the quadrature points chosen in these studies all have the ability to
reconstruct the function’s information, which results in a suboptimal rate for estimating the moments.
Functional Estimation There are also lines of research that investigated the optimal rates of esti-
mating both linear (Oates et al. 2019; Novak 2006; Traub et al. 1994; Novak and Wozniakowski
2008; Novak and W oźniakowski 2008; Bakhvalov 2015; Hinrichs et al. 2014; Novak 2016; Hinrichs
et al. 2020; Hinrichs et al. 2022; Krieg and Sonnleitner 2020; Krieg, Novak, and Sonnleitner 2022)
and nonlinear (Birgé and Massart 1995; Donoho and Nussbaum 1990; Donoho 1988; Donoho and
Liu 1991a, 1991b; Robins et al. 2008; Jiao et al. 2015; Krishnamurthy et al. 2014; Mathé 1991;
Heinrich 2009a, 2009b; Han, Jiao, and Mukherjee 2020; Lepski, Nemirovski, and Spokoiny 1999;
Heinrich 2018) functionals, such as integrals and the Lqnorm. However, as far as the authors know,
previous works on this topic have assumed sufficient smoothness, which rules out the existence of
rare and extreme events that are hard to simulate. Additionally, existing proof techniques are only
applicable in scenarios where there is either no noise or a constant level of noise present. We have
developed a novel and unified proof technique that leverages the method of two fuzzy hypotheses,
which allows us to account for not only rare and extreme events but also different levels of noise.
1.2 Contribution
•We determine all the regimes when a quadrature rule utilizing a nonparametric estimator as a
control variate to reduce the Monte Carlo estimate’s variance can boost the convergence rate of
estimating the moments of a Sobolev function. Under sufficient smoothness assumption, which
rules out the existence of rare and extreme events due to the Sobolev Embedding Theorem, the
regression-adjusted control variate improves the convergence rate and achieves the minimax
optimal rate. The major technical difficulty in building the convergence guarantee in this
regime is determining the right evaluation metric for function estimation. In our work, we
bring a new proof technique to select such a metric by embedding the influence function into an
appropriate space via the Sobolev Embedding Theorem and evaluating the function estimation
in the corresponding dual norm to achieve optimal semi-parametric efficiency. The selection of
the metric is shown in Figure 2.
•Without the sufficient smoothness assumption, however, there may exist rare and extreme events
that are hard to simulate. In this circumstance, we discover that a truncated version of the Monte
Carlo method is minimax optimal, while regression-adjusted control variate can’t improve the
convergence rate. As far as the authors know, our paper is the first work that considers this
problem beyond the sufficient smoothness regime.
•To study how the regression adjusted control variate adapts to the noise level, we examine
the linear functionals, i.e.the definite integral. We prove that this method is minimax optimal
regardless of the level of noise present in the observed data.
1.3 Notations
Let ||·|| be the standard Euclidean norm and Ω= [0, 1]dbe the unit cube in Rdfor any fixed d∈N.
Also, let 1= 1{·} denote the indicator function, i.e, for any event Awe have 1{A} = 1 if Ais true and
1{A} = 0 otherwise. For any region R⊆Ω, we use V(R) :=R
Ω1{x∈R}dxto denote the volume
ofR. Let C(Ω) denote the space of all continuous functions f:Ω→Rand⌊·⌋be the rounding6 Jose Blanchet et al.
function. For any s> 0 and f∈C(Ω), we define the Hölder norm || ·||Cs(Ω)by
||f||Cs(Ω):= max
|k|≤⌊s⌋||Dkf||L∞(Ω)+ max
|k|=⌊s⌋sup
x,y∈Ω,x̸=y|Dkf(x) –Dkf(y)|
||x–y||s–⌊s⌋. (1)
The corresponding Hölder space is defined as Cs(Ω) :=n
f∈C(Ω) : ||f||Cs(Ω)<∞o
. When s= 0,
we have that the two norms || ·||C0(Ω)and ||·||L∞(Ω)are equivalent and C0(Ω) =L∞(Ω). Let
N0:=N∪{0} be the set of all non-negative integers. For any s∈N0and 1≤p≤ ∞ , we define the
Sobolev space Ws,p(Ω) by
Ws,p(Ω) :=n
f∈Lp(Ω) :Dαf∈Lp(Ω),∀α∈Nd
0satisfying | α|≤so
. (2)
Let (c)+denote max{c, 0} for any c∈R. Fix any two non-negative sequences { an}∞
n=1and { bn}∞
n=1.
We write an≲bn, oran=O(bn), to denote that an≤Cbnfor some constant Cindependent of n.
Similarly, we write an≳bn, oran=ω(bn), to denote that an≥cbnfor some constant cindependent
ofn. We use an=Θ(bn) to denote that an=O(bn) and an=ω(bn).
2. Information-Theoretic Lower Bound on Moment Estimation
Problem Setup To understand how the non-parametric regression-adjusted control variate im-
proves the Monte Carlo estimator’s convergence rate, we consider a prototype problem that estimates
a function’s q-th moment. For any fixed q∈Nandf∈Ws,p(Ω), we want to estimate the q-th
moment Iq
f:=R
Ωfq(x)dxwith nrandom quadrature points { xi}n
i=1⊂Ω. On each quadrature point
xi(i= 1,···,n), we can observe the function value yi:=f(xi).
In this section, we study the information-theoretic limit for the problem above via the method of
two fuzzy hypotheses (Tsybakov 2004). We have the following information-theoretic lower bound
on the class Hf,q
nthat contains all estimators ˆHq:Ωn×Rn→Rof the q-th moment Iq
f.
Theorem 1 (Lower Bound on Estimating the Moment) .When p > 2and q <p< 2q, letHf
ndenote the
class of all the estimators that use n quadrature points {xi}n
i=1and observed function values {yi=f(xi)}n
i=1to
estimate the q-th moment of f , where {xi}n
i=1are independently and identically sampled from the uniform
distribution on Ω. Then we have
inf
ˆHq∈Hf,q
nsup
f∈Ws,p(Ω)E{xi}n
i=1,{yi}n
i=1"ˆHq
{xi}n
i=1, {yi}n
i=1
–Iq
f#
≳nmaxn
–q
s
d–1
p
–1,–1
2–s
do
. (3)
Proof Sketch Here we give a sketch for our proof of Theorem 1. Our proof is based on the method
of two fuzzy hypotheses, which is a generalization of the traditional Le Cam’s two-point method. In
fact, each hypothesis in the generalized method is constructed via a prior distribution. In order to
attain a lower bound of magnitude ∆via the method of two fuzzy hypotheses, one needs to pick
two prior distributions µ0,µ1on the Sobolev space Ws,p(Ω) such that the following two conditions
hold. Firstly, the estimators Iq
fdiffer by ∆with constant probability under the two priors. Secondly,
the TV distance between the two corresponding distributions P0andP1of data generated by µ0
andµ1is of constant magnitude. In order to prove the two lower bounds given in (3), we pick two
different pairs of prior distributions as follows:
Below we set m=Θ(n1
d) and divide the domain Ωintomdsmall cubes Ω1,Ω2,···,Ωmd, each
of which has side length m–1. For any p∈(0, 1), we use vp,wpto denote the discrete random variables
satisfying P(vp= 0) = P(wp= –1) = pandP(vp= 1) = P(wp= 1) = 1 – p.W orking Paper. Do not distribute. 7
(I) For the first lower bound in (3), we construct some bump function g∈Ws,p(Ω) satisfying
supp(g)⊆Ω1andIq
g=R
Ω1g(x)dx=Θ(mq(–s+d
p)–d). Now let’s take some sufficiently small constant
ϵ∈(0, 1) and pick µ0,µ1to be discrete measures supported on the two finite setsn
v1+ϵ
2go
and
n
v1–ϵ
2go
. On the one hand, the difference between the q-th moments under µ0andµ1can be lower
bounded by Θ(nq(1
p–s
d)–1) with constant probability. On the other hand, KL(P0||P1) can be upper
bounded by the KL divergence between v1+ϵ
2andv1–ϵ
2, which is of constant magnitude.
(II) For the second lower bound in (3), we set M> 0 to be some sufficiently large constant and
κ=Θ(1√n). For any 1 ≤j≤md, we construct bump functions fj∈Ws,p(Ω) satisfying supp(fj)⊆Ωj
andIk
fj=R
Ωjfj(x)dx=Θ(m–ks–d) for any 1 ≤j≤mdand 1≤k≤s. Now let’s pick µ0,µ1to be
discrete measures supported on the two finite setsn
M+Pmd
j=1w(0)
jfjo
andn
M+Pmd
j=1w(1)
jfjo
, where
{w(0)
j}md
j=1and { w(1)
j}md
j=1are independent and identical copies of w1+κ
2andw1–κ
2respectively. On the
one hand, applying Hoeffding’s inequality yields that the q-th moments under µ0andµ1differ by
Θ(n–s
d–1
2) with constant probability. On the other hand, note that KL(P0||P1) can be bounded by the
KL divergence between two multivariate discrete distributions ( w(0)
j1,···,w(0)
jn) and ( w(1)
j1,···,w(1)
jn),
where { w(0)
ji}n
i=1and { w(1)
ji}n
i=1are independent and identical copies of w1+κ
2andw1–κ
2respectively.
Hence, KL(P0||P1) is of constant magnitude.
Combining the two cases above gives us the minimax lower bound in (3). We defer a complete
proof of Theorem 1 to Appendix Appendix 2.2.
3. Minimax Optimal Estimators for Moment Estimation
This section is devoted to constructing minimax optimal estimators of the q-th moment. We show
that under the sufficient smoothness assumption, a regression-adjusted control variate is essential for
building minimax optimal estimators. However, when the given function is not sufficiently smooth,
we demonstrate that a truncated version of the Monte Carlo algorithm is minimax optimal, and
control variates cannot give any improvement.
3.1 Sufficient Smoothness Regime: Non-parametric Regression-Adjusted Control Variate
This subsection is devoted to building a minimax optimal estimator of the q-th moment under the
assumption thats
d>1
p–1
2q, which guarantees that functions in the space Ws,pare sufficiently smooth.
From the Sobolev Embedding theorem, we know that the sufficient smoothness assumption implies
Ws,p(Ω)⊂Lp∗(Ω)⊂L2q(Ω), where1
p∗=1
p–s
d. Given any function f∈Ws,p(Ω) along with n
uniformly sampled quadrature points { xi}n
i=1and corresponding observations { yi=f(xi)}n
i=1off,
the key idea behind the construction of our estimator ˆHq
Cis to build a nonparametric estimationˆfof
fbased on a sub-dataset and useˆfas a control variate for Monte Carlo simulation. Consequently, it
takes three steps to compute the numerical estimation of Iq
ffor any estimator ˆHq
C:Ωn×Rn→R.
The first step is to divide the observed data into two subsets S1:= {(xi,yi)}n
2
i=1,S2:= {(xi,yi)}n
i=n
2+1
of equal size and use a machine learning algorithm to compute a nonparametric estimationˆf1:n
2of
fbased on S1. Without loss of generality, we may assume that the number of data points is even.
Secondly, we treatˆf1:n
2as a control variate and compute the q-th moment Iq
ˆf. Using the other dataset8 Jose Blanchet et al.
S2, we may obtain a Monte Carlo estimate of Iq
f–Iq
ˆf1:n
2as follows: Iq
f–Iq
ˆf1:n
2≈2
nPn
i=n
2+1
yq
i–ˆfq
1:n
2(xi)
.
Finally, combining the estimation of the q-th moment Iq
ˆf1:n
2=R
Ωˆfq
1:n
2(x)dxwith the estimation of
Iq
f–Iq
ˆf1:n
2gives us the numerical estimation returned by ˆHq
C:
ˆHq
C
{xi}n
i=1, {yi}n
i=1
:=Z
Ωˆfq
1:n
2(x)dx+2
nnX
i=n
2+1
yq
i–ˆfq
1:n
2(xi)
. (4)
We assume that our function estimationˆfis obtained from ann
2-oracle Kn
2:Ωn
2×Rn
2→Ws,p(Ω)
satisfying Assumption 3.1. For example, there are lines of research (Krieg and Sonnleitner 2020;
Krieg, Novak, and Sonnleitner 2022; Mathé 1991; Heinrich 2009a, 2009b) considering how the
moving least squares method (Wendland 2001, 2004) can achieve the convergence rate in (5).
Assumption 3.1 (Optimal Function Estimator as an Oracle) .Given any function f ∈Ws,p(Ω)and
n∈N, let{xi}n
i=1be n data points sampled independently and identically from the uniform distribution on Ω.
Assume that there exists an oracle K n:Ωn×Rn→Ws,p(Ω)that estimates f based on the n points {xi}n
i=1
along with the n observed function values {f(xi)}n
i=1and satisfies the following bound for any r satisfying
1
r∈(d–sp
pd, 1]:

E{xi}n
i=1h
||Kn({xi}n
i=1, {f(xi)}n
i=1) –f||r
Lr(Ω)i 1
r≲n–s
d+(1
p–1
r)+. (5)
Based on the oracle above, we can obtain the following upper bound that matches the information-
theoretic lower bound in Theorem 1.
Theorem 2 (Upper Bound on Moment Estimation with Sufficient Smoothness) .Assume that p > 2,
q<p< 2q and s >2dq–dp
2pq. Let {xi}n
i=1be n quadrature points independently and identically sampled from the
uniform distribution on Ωand{yi:=f(xi)}n
i=1be the corresponding n observations of f ∈Ws,p(Ω). Then
the estimator ˆHq
Cconstructed in (4) above satisfies
E{xi}n
i=1,{yi}n
i=1"ˆHq
C
{xi}n
i=1, {yi}n
i=1
–Iq
f#
≲nmax{– q(s
d–1
p)–1,–s
d–1
2}, (6)
Proof Sketch Given a non-parametric estimatorˆfof the function f, we may bound the variance of
the Monte Carlo process by ( fq–ˆfq)2and further upper bound it by the sum of the following two
terms:
|fq–ˆfq|2≲ |fq–1(f–ˆf)|2
|{z}
semi-parametric influnce+ |( f–ˆf)q|2
|{z}
estimation error propagation.(7)
The first term above represents the semi-parametric influence part of the problem, as qfq–1is the
influence function for the estimation of the q-th moment fq. The second term characterizes how
function estimation affects functional estimation. If we consider the special case of estimating the
mean instead of a general q-th moment, i.e, q= 1, the semi-parametric influence term will disappear.
Consequently, the convergence rate won’t transit from n–1
2–s
dton–q(s
d–1
p)–1in the special case.
Although the algorithm remains unchanged in the sufficient smooth regime, we need to consider
three separate cases to obtain an upper bound on the integral of the semi-parametric influence term
|fq–1(f–ˆf)|2in (7). An illustration of the three cases is given in Figure 2.W orking Paper. Do not distribute. 9
From Hölder’s inequality, we know thatR
Ωf2q–2(x)(f(x) –ˆf(x))2dxcan be upper bounded by
||f2q–2||Lr′(Ω)||(f–ˆf)2||Lr∗(Ω), where || ·||Lr′(Ω)and ||||Lr∗(Ω)are dual norms. Therefore, the main
difficulty here is to embed the function fin different spaces via the Sobolev Embedding Theorem
under different assumptions on the smoothness parameter s. When the function is smooth enough, i.e.
s>d
p, we embed the function finL∞(Ω) and evaluate the estimation error f–ˆfunder the L2norm.
Then our assumption on the oracle (5) gives us an upper bound of magnitude n–2s
don ||f–ˆf||2
L2(Ω),
which helps us further upper bound the semi-parametric influence partR
Ωf2q–2(x)(f(x) –ˆf(x))2dx
byn–2s
dup to constants. Whend(2q–p)
p(2q–2)<s<d
p, we embed the function finL2pq–2p
p–2(Ω)⊆Lpd
d–sp(Ω)
and evaluate the estimation error f–ˆfunder the Lpnorm. Applying our assumption on the oracle
(5) again implies that the semi-parametric influence partR
Ωf2q–2(x)(f(x) –ˆf(x))2dxcan be upper
bounded by n–2s
dup to constants. Whend(2q–p)
2pq<s<d(2q–p)
p(2q–2), we embed the function finLp∗and
evaluate the error of the oracle in L2p∗
p∗+2–2q, where1
p∗=1
p–s
d. Similarly, we can use (5) to upper
bound the semi-parametric influence partR
x∈Ωf2q–2(x)(f(x) –ˆf(x))2dxbyn2q(1
p–s
d)–1.
The upper bound on the propagated estimation errorR
x∈Ω(f(x) –ˆf(x))2qdxin (7) can be derived
by evaluating the error of the oracle under the L2qnorm. i.e,by picking r= 2qin (5) above, which
yields an upper bound of magnitude n2q(1
p–s
d)–1.
The obtained upper bounds on the semi-parametric influence part and the propagated estimation
error above provide us with a clear view of the upper bound on the variance of fq–ˆfq, which is
the random variable we aim to simulate via Monte-Carlo in the second stage. Using the standard
Monte-Carlo algorithm to simulate the expectation of fq–ˆfqthen gives us an extra n–1
2factor for
the convergence rate, which helps us attain the final upper bounds given in (6). A complete proof of
Theorem 2 is given in Appendix Appendix 3.1.
3.2 Beyond the Sufficient Smoothness Regime: Truncated Monte Carlo
In this subsection, we study the case when the sufficient smoothness assumption breaks, i.e.s
d<1
p–1
2q.
According to the Sobolev Embedding theorem, we have that Wp
sis embedded in Ldp
d–sp. Since
1
p–s
d>1
2qimpliesdp
d–sp< 2q, the underlying function fis not guaranteed to have bounded L2qnorm,
which indicates the existence of rare and extreme events. Consequently, the Monte Carlo estimate of
f’sq-th moment must have infinite variance, which makes it hard to simulate. Here we present a
truncated version of the Monte Carlo algorithm that can achieve the minimax optimal convergence
rate. For any fixed parameter M> 0, our estimator is designed as follows:
ˆHq
M
{xi}n
i=1, {yi}n
i=1
:=1
nnX
i=1maxn
min{ yi,M}, –Moq
. (8)
In Theorem 3, we provide the convergence rate of the estimator (8) by choosing the truncation
parameter Min an optimal way.
Theorem 3 (Upper Bound on Moment Estimation without Sufficient Smoothness) .Assuming that
p> 2, q<p< 2q and s <2dq–dp
2pq, we pick M =Θ(n1
p–s
d). Let {xi}n
i=1be n quadrature points independently
and identically sampled from the uniform distribution on Ωand{yi:=f(xi)}n
i=1be the corresponding n10 Jose Blanchet et al.
observations of f ∈Ws,p(Ω). Then we have that the estimator ˆHq
Mconstructed in (8) above satisfies
E{xi}n
i=1,{yi}n
i=1"ˆHq
M
{xi}n
i=1, {yi}n
i=1
–Iq
f#
≲n–q(s
d–1
p)–1. (9)
Proof Sketch The error can be decomposed into bias and variance parts. The bias part is caused
by the truncation in our algorithm, which is controlled by the parameter Mand can be bounded
byR
{x:|f(x)|>M}|f|qdx. According to the Sobolev Embedding Theorem, Ws,p(Ω) can be embedded
in the space Lp∗, where1
p∗=1
p–s
d. As | f(x)| >Mimplies | f(x)|q<Mq–p∗|f(x)|p∗, the bias can be
upper bounded by Mq–p∗. Similarly, the variance is controlled by Mand can be upper bounded by
Mq–p∗
2. Combining the bias and variance bound, we can bound the final error as Mq–p∗+Mq–p∗
2√n. By
selecting M=Θ(n1
p∗) =Θ(n1
p–s
d), we obtain the final convergence rate n–q(s
d–1
p)–1. A complete proof
of Theorem 3 is given in Appendix Appendix 3.2.
Remark 1. (Heinrich 2018) has shown that the convergence rate of the optimal non-parametric regression-
based estimation is n–s
d+1
p–1
q, which is slower than the convergence rate of the truncated Monte Carlo estimator
that we show above.
4. Adapting to the Noise Level: a Case Study for Linear Functional
In this section, we study how the regression-adjusted control variate adapts to different noise levels.
Here we consider the linear functional, i.e.estimating a function’s definite integral via low-noise
observations at random points.
Problem Setup We consider estimating If=R
Ωf(x)dx, the integral of foverΩ, for a fixed function
f∈Cs(Ω) with uniformly sampled quadrature points { xi}n
i=1⊂Ω. On each quadrature point
xi(i= 1,···,n), we have a noisy observation yi:=f(xi) +ϵi. Here the ϵi’s are independently and
identically distributed Gaussian noises sampled from N(0,n–2γ), where γ∈[0,∞].
4.1 Information-Theoretic Lower Bound on Mean Estimation
In this subsection, we present a minimax lower bound (Theorem 4) for all estimators ˆH:Ωn×Rn→R
of the integral Ifof a function f∈Cs(Ω) when one can only access noisy observations.
Theorem 4 (Lower Bound for Integral Estimation) .LetHf
ndenote the class of all the estimators that
use n quadrature points {xi}n
i=1and noisy observations {yi=f(xi) +ϵi}n
i=1to estimate the integral of f , where
{xi}n
i=1and{ϵi}n
i=1are independently and identically sampled from the uniform distribution on Ωand the
normal distribution N(0,n–2γ)respectively. Assuming that γ∈[0,∞]and s > 0, we have
inf
ˆH∈Hf
nsup
f∈Cs(Ω)E{xi}n
i=1,{yi}n
i=1"ˆH
{xi}n
i=1, {yi}n
i=1
–If#
≳nmax{–1
2–γ,–1
2–s
d}. (10)
Remark 2. Functional estimation is a well-studied problem in the literature of nonparametric statistics.
However, current information-theoretic lower bounds for functional estimation (Birgé and Massart 1995;
Donoho and Nussbaum 1990; Donoho 1988; Robins et al. 2008; Jiao et al. 2015; Krishnamurthy et al. 2014;
Tsybakov 2004; Han et al. 2020) assume a constant level of noise on the observed function values. One
essential idea for proving these lower bounds is to leverage the existence of the observational noise, which enablesW orking Paper. Do not distribute. 11
us to upper bound the amount of information required to distinguish between two reduced hypotheses. In
contrast, we provide a minimax lower bound that is applicable for noises at any level by constructing two priors
with overlapping support and assigning distinct probabilities to the corresponding Bernoulli random variables,
which separates the two hypotheses. A comprehensive proof of Theorem 4 is given in Appendix Appendix 4.2.
4.2 Optimal Nonparametric Regression-Adjusted Quadrature Rule
In the discussion below, we use the nearest-neighbor method as an example. For any k∈{1, 2,···,n
2},
thek-nearest neighbor estimatorˆfk-NN offis given byˆfk-NN(z) :=1
kPk
j=1yi(z)
j, where { xi(z)
j}n
2
j=1is
a permutation of the quadrature points { xi}n
2
i=1such that || xi(z)
1–z||≤||xi(z)
2–z||≤ ··· ≤ ||xi(z)
n
2–z||
holds for any z∈Ω. Moreover, we use Tk,z:= {xi(z)
j}k
j=1to denote the collection of the knearest
neighbors of zamong { xi}n
2
i=1for any z∈Ω. For any 1 ≤i≤n
2, we take Di⊂Ωto be the region
formed by all the points whose knearest neighbors contain xi,i.e, D i:=n
z∈Ω:xi∈ Tk,zo
. Our
estimator ˆHk-NN can be formally represented as
ˆHk-NN
{xi}n
i=1, {yi}n
i=1
=n
2X
i=1V(Di)
kyi
|{z}R
Ωˆfk-NN(x)dx+2
nnX
i=n
2+1yi–2
nnX
i=n
2+11
kn
2X
j=11{xi∈Dj}yj
| {z }
2
nPn
i=n
2+1
yi–ˆfk-NN(xi).
In the following theorem, we present an upper bound on the expected risk of the estimator
ˆHk-NN:
Theorem 5 (Matching Upper Bound for Integral Estimation) .Let{xi}n
i=1be n quadrature points
independently and identically sampled from the uniform distribution on Ωand{yi:=f(xi) +ϵi}n
i=1be the
corresponding n noisy observations of f ∈Cs(Ω), where {ϵi}n
i=1are independently and identically sampled
from the normal distribution N(0,n–2γ). Assuming that γ∈[0,∞]and s∈(0, 1), we have that there exists
k∈Nsuch that the estimator ˆHk-NN constructed above satisfies
E{xi}n
i=1,{yi}n
i=1"ˆHk-NN
{xi}n
i=1, {yi}n
i=1
–If#
≲nmax{–1
2–γ,–1
2–s
d}. (11)
Remark 3. Our upper bound in Theorem 5 matches our minimax lower bound in Theorem 4, which
indicates that the regression-adjusted quadrature rule associated with the nearest neighbor estimator is minimax
optimal. When the noise level is high ( γ<s
d), the control variate helps to improve the rate from n–1
2(the
Monte Carlo rate) to n–1
2–γvia eliminating allthe effects of simulating the smooth function. When the noise
level is low ( γ>s
d), we show that our estimator ˆHk-NN can achieve the optimal rate of quadrature rules
(Novak 2016). W e defer a complete proof of Theorem 5 to Appendix Appendix 4.3.
5. Discussion and Conclusion
In this paper, we have investigated whether a non-parametric regression-adjusted control variate can
improve the rate of estimating functionals and if it is minimax optimal. Using the Sobolev Embedding
Theorem, we discover that the existence of rare and extreme events will change the answer to this
question. We show that when rare and extreme events are present, using a non-parametric machine12 Jose Blanchet et al.
learning algorithm as a control variate does not help, and truncated Monte Carlo is minimax optimal.
Investigating how to apply importance sampling under this scenario may be of future interest.
Also, the study of how regression-adjusted control variates adapt to the noise level for non-linear
functionals (Han, Jiao, and Mukherjee 2020; Lepski, Nemirovski, and Spokoiny 1999) is left as future
work. Another interesting direction is to analyze how to use the data distribution’s information
(Oates and Girolami 2016; Oates, Girolami, and Chopin 2017) to achieve both better computational
trackability and convergence rate (Oates et al. 2019).
Acknowledgement
Yiping Lu is supported by the Stanford Interdisciplinary Graduate Fellowship (SIGF). Jose Blanchet
is supported in part by the Air Force Office of Scientific Research under award number FA9550-
20-1-0397. Lexing Ying is supported is supported by National Science Foundation under award
DMS-2208163.W orking Paper. Do not distribute. 13
References
Adams, Robert A, and John JF Fournier. 2003. Sobolev spaces. Elsevier.
Angelopoulos, Anastasios N, Stephen Bates, Clara Fannjiang, Michael I Jordan, and Tijana Zrnic. 2023. Prediction-powered
inference. arXiv preprint arXiv:2301.09633.
Asmussen, Søren, and Peter W Glynn. 2007. Stochastic simulation: algorithms and analysis. V ol. 57. Springer.
Assaraf, Roland, and Michel Caffarel. 1999. Zero-variance principle for monte carlo algorithms. Physical review letters 83 (23):
4682.
Bach, Francis. 2017. On the equivalence between kernel quadrature rules and random feature expansions. The Journal of
Machine Learning Research 18 (1): 714–751.
Bakhvalov, Nikolai Sergeevich. 2015. On the approximate calculation of multiple integrals. Journal of Complexity 31 (4):
502–516.
Bardenet, Rémi, and Adrien Hardy. 2020. Monte carlo with determinantal point processes. Annals of Applied Probability.
Belhadji, Ayoub. 2021. An analysis of ermakov-zolotukhin quadrature using kernels. Advances in Neural Information Processing
Systems 34:27278–27289.
Belhadji, Ayoub, Rémi Bardenet, and Pierre Chainais. 2019. Kernel quadrature with dpps. Advances in Neural Information
Processing Systems 32.
Biau, Gérard, and Luc Devroye. 2015. Lectures on the nearest neighbor method. V ol. 246. Springer.
Birgé, Lucien, and Pascal Massart. 1995. Estimation of integral functionals of a density. The Annals of Statistics 23 (1): 11–29.
Chen, Wilson Y e, Lester Mackey, Jackson Gorham, François-Xavier Briol, and Chris Oates. 2018. Stein points. In International
conference on machine learning, 844–853. PMLR.
Chen, Yutian, Max Welling, and Alex Smola. 2012. Super-samples from kernel herding. arXiv preprint arXiv:1203.3472.
Davidson, Russell, and James G MacKinnon. 1992. Regression-based methods for using control variates in monte carlo
experiments. Journal of Econometrics 54 (1-3): 203–222.
Donoho, David L. 1988. One-sided inference about functionals of a density. The Annals of Statistics, 1390–1420.
Donoho, David L, and Richard C Liu. 1991a. Geometrizing rates of convergence, ii. The Annals of Statistics, 633–667.
. 1991b. Geometrizing rates of convergence, iii. The Annals of Statistics, 668–701.
Donoho, David L, and Michael Nussbaum. 1990. Minimax quadratic estimation of a quadratic functional. Journal of Complexity
6 (3): 290–323.
Dwivedi, Raaz, and Lester Mackey. 2021a. Generalized kernel thinning. arXiv preprint arXiv:2110.01593.
. 2021b. Kernel thinning. arXiv preprint arXiv:2105.05842.
Gautier, Guillaume, Rémi Bardenet, and Michal Valko. 2019. On two ways to use determinantal point processes for monte
carlo integration. Advances in Neural Information Processing Systems 32.
Han, Yanjun, Jiantao Jiao, and Rajarshi Mukherjee. 2020. On estimation of Lr-norms in gaussian white noise models. Probability
Theory and Related Fields 177 (3-4): 1243–1294.
Han, Yanjun, Jiantao Jiao, Tsachy Weissman, and Yihong Wu. 2020. Optimal rates of entropy estimation over lipschitz balls.
The Annals of Statistics 48 (6): 3228–3250.
Hayakawa, Satoshi, Harald Oberhauser, and Terry Lyons. 2021. Positively weighted kernel quadrature via subsampling. arXiv
preprint arXiv:2107.09597.
. 2023. Sampling-based nyström approximation and kernel quadrature. arXiv preprint arXiv:2301.09517.
Heinrich, Stefan. 2009a. Randomized approximation of sobolev embeddings, ii. Journal of Complexity 25 (5): 455–472.
. 2009b. Randomized approximation of sobolev embeddings, iii. Journal of Complexity 25 (5): 473–507.
. 2018. On the complexity of computing the Lqnorm. Journal of Complexity 49:1–26.
Hickernell, Fred J, Christiane Lemieux, and Art B Owen. 2005. Control variates for quasi-monte carlo. Statistical Science 20
(1): 1–31.14 Jose Blanchet et al.
Hinrichs, Aicke, David Krieg, Erich Novak, Joscha Prochno, and Mario Ullrich. 2020. On the power of random information.
Multivariate Algorithms and information-based complexity 27:43–64.
Hinrichs, Aicke, David Krieg, Erich Novak, and Jan Vybıéral. 2022. Lower bounds for integration and recovery in L2.Journal
of Complexity 72:101662.
Hinrichs, Aicke, Erich Novak, Mario Ullrich, and H Woźniakowski. 2014. The curse of dimensionality for numerical
integration of smooth functions. Mathematics of Computation 83 (290): 2853–2863.
Holzmüller, David, and Francis Bach. 2023. Convergence rates for non-log-concave sampling and log-partition estimation.
arXiv preprint arXiv:2303.03237.
Huszár, Ferenc, and David Duvenaud. 2012. Optimally-weighted herding is bayesian quadrature. arXiv preprint arXiv:1204.1664.
Jiao, Jiantao, Kartik Venkat, Yanjun Han, and Tsachy Weissman. 2015. Minimax estimation of functionals of discrete
distributions. IEEE Transactions on Information Theory 61 (5): 2835–2885.
Kanagawa, Motonobu, and Philipp Hennig. 2019. Convergence guarantees for adaptive bayesian quadrature methods. Advances
in Neural Information Processing Systems 32.
Kanagawa, Motonobu, Bharath K Sriperumbudur, and Kenji Fukumizu. 2016. Convergence guarantees for kernel-based
quadrature rules in misspecified settings. Advances in Neural Information Processing Systems 29.
Karvonen, Toni, and Simo Sarkka. 2018. Fully symmetric kernel quadrature. SIAM Journal on Scientific Computing 40 (2):
A697–A720.
Krieg, David, Erich Novak, and Mathias Sonnleitner. 2022. Recovery of sobolev functions restricted to iid sampling. Mathematics
of Computation 91 (338): 2715–2738.
Krieg, David, and Mathias Sonnleitner. 2020. Random points are optimal for the approximation of sobolev functions. arXiv
preprint arXiv:2009.11275.
Krishnamurthy, Akshay, Kirthevasan Kandasamy, Barnabas Poczos, and Larry Wasserman. 2014. Nonparametric estimation
of renyi divergence and friends. In International conference on machine learning, 919–927. PMLR.
Lacoste-Julien, Simon, Fredrik Lindsten, and Francis Bach. 2015. Sequential kernel herding: frank-wolfe optimization for
particle filtering. In Artificial intelligence and statistics, 544–552. PMLR.
Lepski, Oleg, Arkady Nemirovski, and Vladimir Spokoiny. 1999. On estimation of the Lrnorm of a regression function.
Probability theory and related fields 113:221–253.
Lin, Lin. 2017. Randomized estimation of spectral densities of large matrices made accurate. Numerische Mathematik 136:183–
213.
Liu, Hanzhong, and Yuehan Yang. 2020. Regression-adjusted average treatment effect estimates in stratified randomized
experiments. Biometrika 107 (4): 935–948.
Liu, Hao, Yihao Feng, Yi Mao, Dengyong Zhou, Jian Peng, and Qiang Liu. 2017. Action-depedent control variates for
policy optimization via stein’s identity. arXiv preprint arXiv:1710.11198.
Mathé, Peter. 1991. Random approximation of sobolev embeddings. Journal of Complexity 7 (3): 261–281.
Meyer, Raphael A, Cameron Musco, Christopher Musco, and David P W oodruff. 2021. Hutch++: optimal stochastic trace
estimation. In Symposium on simplicity in algorithms (sosa), 142–155. SIAM.
Mira, Antonietta, Reza Solgi, and Daniele Imparato. 2013. Zero variance markov chain monte carlo for bayesian estimators.
Statistics and Computing 23:653–662.
Novak, E, and H W ozniakowski. 2008. Tractability of multivariate problems, volume i: linear information, european math.
Soc., Zürich 2 (3).
Novak, Erich. 2006. Deterministic and stochastic error bounds in numerical analysis. V ol. 1349. Springer.
. 2016. Some results on the complexity of numerical integration. Monte Carlo and Quasi-Monte Carlo Methods: MCQMC,
Leuven, Belgium, April 2014, 161–183.
Novak, Erich, and Henryk W oźniakowski. 2008. Tractability of multivariate problems: standard information for functionals. V ol. 2.
European Mathematical Society.
O’Hagan, Anthony. 1991. Bayes–hermite quadrature. Journal of statistical planning and inference 29 (3): 245–260.W orking Paper. Do not distribute. 15
Oates, Chris, and Mark Girolami. 2016. Control functionals for quasi-monte carlo integration. In Artificial intelligence and
statistics, 56–65. PMLR.
Oates, Chris J, Jon Cockayne, François-Xavier Briol, and Mark Girolami. 2019. Convergence rates for a class of estimators
based on stein’s method. Bernoulli 25 (2): 1141–1159.
Oates, Chris J, Mark Girolami, and Nicolas Chopin. 2017. Control functionals for monte carlo integration. Journal of the
Royal Statistical Society. Series B (Statistical Methodology), 695–718.
Robins, James, Lingling Li, Eric Tchetgen, Aad van der Vaart, et al. 2008. Higher order influence functions and minimax
estimation of nonlinear functionals. Probability and statistics: essays in honor of David A. Freedman 2:335–421.
Romano, Yaniv, Evan Patterson, and Emmanuel Candes. 2019. Conformalized quantile regression. Advances in neural information
processing systems 32.
Shi, Jiaxin, Yuhao Zhou, Jessica Hwang, Michalis Titsias, and Lester Mackey. 2022. Gradient estimation with discrete stein
operators. Advances in Neural Information Processing Systems 35:25829–25841.
Siegel, Jonathan W, and Jinchao Xu. 2022. High-order approximation rates for shallow neural networks with cosine and
relukactivation functions. Applied and Computational Harmonic Analysis 58:1–26.
Sobczyk, Aleksandros, and Mathieu Luisier. 2022. Approximate euclidean lengths and distances beyond johnson-lindenstrauss.
arXiv preprint arXiv:2205.12307.
South, Leah F, CJ Oates, A Mira, and C Drovandi. 2018. Regularised zero-variance control variates. arXiv preprint arXiv:1811.05073.
Traub, Joseph F, GW Wasilkowski, H W ozniakowski, and Erich Novak. 1994. Information-based complexity. SIAM Review
36 (3): 514–514.
Tsybakov, Alexandre B. 2004. Introduction to nonparametric estimation, 2009. URL https://doi. org/10.1007/b13794. Revised
and extended from the 9 (10).
Wendland, Holger. 2001. Local polynomial reproduction and moving least squares approximation. IMA Journal of Numerical
Analysis 21 (1): 285–300.
. 2004. Scattered data approximation. V ol. 17. Cambridge university press.16 Jose Blanchet et al.
Appendix
The appendix is organized as follows:
•In Appendix A, we list some notations and standard lemmas used in our proofs.
•Appendix B contains a comprehensive proof of the information-theoretic lower bound on the
estimation of q-th moments, which is established in Theorem 1.
•In Appendix C, we provide a detailed proof of Theorem 2 and 3, which gives us the minimax
optimal upper bound on estimating q-th moments.
•Appendix D consists of our proof for the information-theoretic lower bounds and minimax
optimal upper bounds on integral estimation and function estimation, which are listed in
Theorem 4 and 5.
Appendix 1. Preliminaries and Basic Tools
Appendix 1.1 Preliminaries
This subsection is devoted to presenting some basic notations used in our proofs. For any fixed convex
function f:R+→Rsatisfying f(1) = 0, we use Df(·||·) to denote the corresponding f-divergence,
i.e, Df(P||Q) =R
Yf
dP
dQ
dQfor any two probability distributions PandQover some fixed space
Y. In particular, when f(x) =1
2|x– 1|,Df(·||·) is the total variation (TV) distance TV(·||·). When
f(x) =xlogx,Df(·||·) coincides with the Kullback–Leibler (KL) divergence KL(·||·). Moreover, for
anya∈R, we use δa(·) to denote the Dirac delta distribution at point a,i.e,R∞
–∞f(x)δa(x) =f(a) for
any function f:R→R.
Appendix 1.2 Basic Lemmas
In this subsection, we list some basic lemmas that serve as essential tools in our proofs.
Lemma 1 (Sobolev Embedding Theorem (Adams and Fournier 2003)) .For some fixed dimension
d∈N, we have that
(I) For any s ,t∈N0and p ,q∈Rsatisfying s >t, p<d and 1≤p<q≤ ∞ , we have Ws,p(Rd)⊆Wt,q(Rd)
when the relation1
p–s
d=1
q–t
dholds. In the special case when t = 0, we have Ws,p(Rd)⊆Lq(Rd)for any
s∈Nand p ,q∈Rsatisfying 1≤p<q≤ ∞ and1
p–s
d≤1
q.
(II) For any α∈(0, 1), letβ=d
1–α∈(d,∞]. Then we have C1(Rd)∩W1,β(Rd)⊆Cα(Rd).
Lemma 2 (Hölder’s Inequality) .For any fixed domain Ωand p ,q∈[1,∞]satisfying1
p+1
q= 1, we
have that ||fg||L1(Ω)≤||f||Lp(Ω)||g||Lq(Ω)holds for any f ∈Lp(Ω),g∈Lq(Ω).
Lemma 3 (Hoeffding’s Inequality) .Let X 1,X2,···,Xnbe independent random variables satisfying
Xi∈[ai,bi]for any 1≤i≤n. Then for any ϵ> 0, the sum S n:=Pn
i=1Xiof these n random variables
satisfies the following inequality:
P(Sn≥E[Sn] +t)≤exp
–2t2
Pn
i=1(bi–ai)2
P(Sn≤E[Sn] –t)≤exp
–2t2
Pn
i=1(bi–ai)2(12)
Lemma 4 (Data Processing Inequality) .Given some Markov Chain X →Z, where X and Z are two
random variables the measurable spaces (X,µ)and(Z,ν)respectively. Let K be the transition kernel of the
Markov Chain above, i.e,for any x ∈ X, the probability distribution of Z is given by K (·,x)when conditionedW orking Paper. Do not distribute. 17
on X =x. For any two fixed two distributions P ,Q over Xwith probability density functions p ,q, we use K P(·)
and K Q(·)to denote the corresponding marginal distributions respectively, i.e,KP(·) :=R
XK(·,x)p(x)dµ(x)
and K Q(·) =R
XK(·,x)q(x)dµ(x). Then we have Df(KP||KQ)≤Df(P||Q)holds for any f -divergence
Df(·||·).
Appendix 2. Proof of Lower Bounds in Section 2
Appendix 2.1 A Key Lemma for Building Minimax Optimal Lower Bounds
In this subsection, we firstly present the method of two fuzzy hypotheses, which turns out to be the
most essential tool for establishing all the minimax optimal lower bounds in our paper, before giving
our complete proof of Theorem 1.
Lemma 5 (Method of Two Fuzzy Hypotheses: Theorem 2.15 (i), (Tsybakov 2004)) .Let F :ˆ→R
be some continuous functional defined on the measurable space (ˆ,U)and taking values in (R,B(R)), where
B(R)denotes the Borel σ-algebra on R. Suppose that each parameter θ∈ˆ is associated with a distribution
Pθ, which together form a collection {Pθ:θ∈ˆ}of distributions.
For any fixed θ∈ˆ, assume that our observation Xis distributed as Pθ. Let ˆFbe an arbitrary estimator of
F(θ)based on X. Letµ0,µ1be two prior measures on ˆ. Assume that there exist constants c ∈R,∆∈(0,∞)
andβ0,β1∈[0, 1) , such that:
µ0(θ∈ˆ:F(θ)≤c–∆)≥1 –β0,
µ1(θ∈ˆ:F(θ)≥c+∆)≥1 –β1.(13)
For j∈{0, 1} , we use Pj(·) :=R
Pθ(·)µj(dθ)to denote the marginal distribution Pjassociated with the prior
distribution µj. Then we have the following lower bound:
inf
ˆFsup
θ∈ΘPθ(|ˆF–F(θ)|≥∆)≥1 –TV(P0||P1) –β0–β1
2. (14)
Appendix 2.2 Proof of Theorem 1 (Information-Theoretic Lower Bound on Moment Estimation)
In this subsection, we give a detailed proof of the two minimax lower bounds established in Theorem
1 above via the method of two fuzzy hypotheses (Lemma 5). We start off by introducing some
preliminary tools used in our proof. Consider the function K0defined as follows:
K0(x) :=dY
i=1exp
–1
1 –x2
i
1(|xi|≤1),∀x= (x1,x2,···,xd)∈Rd. (15)
Moreover, we pick some function Ksatisfying
K(x) :=K0(2x),∀x∈Rd, (16)
From our construction of KandK0above, we have that K0is inC∞(Rd) and compactly supported
on [–1
2,1
2]d. Furthermore, we set m= (200 n)1
dand divide the domain Ωintomdsmall cubes
Ω1,Ω2,···,Ωmd, each of which has side length m–1. For any 1 ≤j≤md, we use cjto denote
the center of the cube Ωj. Similar to the proof sketch of Theorem 1, below we again use wpto
denote the discrete random variable satisfying P(wp= –1) = pandP(wp= 1) = 1 – pfor any
p∈(0, 1). Furthermore, we use ⃗x:= (x1,x2,···,xn) and ⃗y:= (y1,y2,···,yn) to denote the two n-
dimensional vectors formed by the quadrature points and observed function values, After introducing
all preliminaries above, let’s present the essential parts of our proof. Given that our lower bound in
Theorem 1 consists of two terms, our proof is also divided into two parts:18 Jose Blanchet et al.
(Case I) For the first lower bound in (3), let’s consider two functions g0andg1defined as follows:
g0(x)≡0 (∀x∈Ω),
g1(x) =(
m–s+d
pK(m(x–c1)) (x∈Ω1),
0 (otherwise).(17)
Clearly we have g0∈Ws,p(Ω) and Iq
g0= 0. Now let’s verify that g1∈Ws,p(Ω) for any m. Note that
the following bound holds for any t∈Nd
0satisfying | t|≤s:
||Dtg1||Lp(Ω)= Z
Ω1m–s+d
pm|t|(DtK)(m(x–c1))p
dx!1
p
=m|t|–s+d
p Z
[–1
2,1
2]d(DtK)(y)p1
mddy!1
p
=m|t|–s||DtK||Lp([–1
2,1
2]d)≲1.
This implies g1∈Ws,p(Ω) for any m, as desired. Moreover, computing the q-th moment of g1yields
Iq
g1=Z
Ωgq
1(x)dx=Z
Ω1(m–s+d
pK(m(x–c1)))qdx
=m–q(s–d
p)Z
[–1
2,1
2]d(K(y))q1
mddy=m–q(s–d
p)–d||K||q
Lq([–1
2,1
2]d).(18)
Now let us take ϵ=1
2and pick two discrete measures µ0,µ1supported on the finite set
{g0,g1}⊂Ws,p(Ω) as below:
µ0({g0}) =1 +ϵ
2,µ0({g1}) =1 –ϵ
2,
µ1({g0}) =1 –ϵ
2,µ1({g1}) =1 +ϵ
2.(19)
On the one hand, by taking c=∆=1
2Iq
g1andβ0=β1=1–ϵ
2, we may use (19) to deduce that
µ0(f∈Ws,p(Ω) :Iq
f≤c–∆) =µ0(Iq
f≤0)≥1 +ϵ
2= 1 –β0,
µ1(f∈Ws,p(Ω) :Iq
f≥c+∆) =µ1(Iq
f≥Iq
g1)≥1 +ϵ
2= 1 –β1.(20)
Hence, we have that (13) holds true. On the other hand, recall that the quadrature points { x1,···,xn}
are identical and independent samples from the uniform distribution on Ω, which enables us to write
the marginal distributions in an explicit form as follows:
P0(⃗x,⃗y) =1 +ϵ
2Y
i:xi∈Ω1δ0(yi) +1 –ϵ
2Y
i:xi∈Ω1δg1(xi)(yi)
·mdY
j=2Y
i:xi∈Ωjδ0(yi)
,
P1(⃗x,⃗y) =1 –ϵ
2Y
i:xi∈Ω1δ0(yi) +1 +ϵ
2Y
i:xi∈Ω1δg1(xi)(yi)
·mdY
j=2Y
i:xi∈Ωjδ0(yi)
.(21)W orking Paper. Do not distribute. 19
In particular, we have P0=P1when the set { i:xi∈Ω1} is empty. Combing this fact with (21)
above allows us to compute the KL divergence between P0andP1as below
KL(P0||P1) =Z
Ω···Z
ΩZ∞
–∞···Z∞
–∞logP0(⃗x,⃗y)
P1(⃗x,⃗y)
P0(⃗x,⃗y)dy1···dyn
dx1···dxn
=Z
Ω···Z
Ω Z∞
–∞···Z∞
–∞log1+ϵ
2Q
i:xi∈Ω1δ0(yi) +1–ϵ
2Q
i:xi∈Ω1δg1(xi)(yi)
1–ϵ
2Q
i:xi∈Ω1δ0(yi) +1+ϵ
2Q
i:xi∈Ω1δg1(xi)(yi)
·1 +ϵ
2Y
i:xi∈Ω1δ0(yi) +1 –ϵ
2Y
i:xi∈Ω1δg1(xi)(yi)
·mdY
j=2Y
i:xi∈Ωjδ0(yi)nY
i=1dyi!nY
i=1dxi
=Z
Ω···Z
Ω Z∞
–∞···Z∞
–∞log1+ϵ
2Q
i:xi∈Ω1δ0(yi) +1–ϵ
2Q
i:xi∈Ω1δg1(xi)(yi)
1–ϵ
2Q
i:xi∈Ω1δ0(yi) +1+ϵ
2Q
i:xi∈Ω1δg1(xi)(yi)
·1 +ϵ
2Y
i:xi∈Ω1δ0(yi) +1 –ϵ
2Y
i:xi∈Ω1δg1(xi)(yi)Y
i:xi∈Ω1dyi!nY
i=1dxi
=
log1 +ϵ
1 –ϵ1 +ϵ
2+ log1 –ϵ
1 +ϵ1 –ϵ
2
P
{i:xi∈Ω1}̸=∅
=ϵlog1 +ϵ
1 –ϵ
P
{i:xi∈Ω1}̸=∅
.(22)
Moreover, since the probability that { i:xi∈Ω1} =∅equals to (md–1
md)n= (md–1
md)md
200, we have
P
{i:xi∈Ω1}̸=∅
= 1 –
1 –1
mdmd
200≤1 – 
1
e
1 –1
md! 1
200
≤1 – (2 e)–1
200. (23)
Now we may combine (22), (23) and Pinkser’s inequality to upper bound the TV distance
between P0andP1as below:
TV(P0||P1)≤r
1
2KL(P0||P1)≤s
1 – (2 e)–1
200
2ϵlog1 +ϵ
1 –ϵ
≤r
3
100ϵ=√
3
10ϵ. (24)
Finally, by substituting (18), (24), ∆=1
2Iq
g1andβ0=β1=1–ϵ
2=1
4into (14) and applying
Markov’s inequality, we obtain the final lower bound
inf
ˆHq∈Hf,q
nsup
f∈Ws,p(Ω)E{xi}n
i=1,{yi}n
i=1"ˆHq
{xi}n
i=1, {yi}n
i=1
–Iq
f#
≥∆inf
ˆHq∈Hf,q
nsup
f∈Ws,p(Ω)P{xi}n
i=1,{yi}n
i=1"ˆHq
{xi}n
i=1, {yi}n
i=1
–Iq
f≥∆#
≥1
2Iq
g11 –TV(P0||P1) –β0–β1
2≥1
4
1 –√
3
10
ϵIq
g1
=1
8
1 –√
3
10
(200n)–q
d(s–d
p)–1||K||q
Lq([–1
2,1
2]d)≳n–q(s
d–1
p)–1,(25)
which is exactly the first term in the RHS of (3).20 Jose Blanchet et al.
(Case II) Now let us proceed to prove the second lower bound in (3). For any 1 ≤j≤md,
consider first some function fjdefined as follows
fj(x) =(
m–sK(m(x–cj)) (x∈Ωj),
0 (otherwise),(26)
which satisfies supp(fj)⊆Ωj,fj∈C∞(Ω) and fj(x)≥0 (∀x∈Ω). We further pick two constants
α,Msatisfying α:= ||K||L∞([–1
2,1
2]d)andM= 3α. Now consider the following finite set of 2md
functions:
S:=n
M+mdX
j=1ηjfj:ηj∈{±1},∀1≤j≤mdo
. (27)
We will proceed to verify that any element in Smust be in Ws,p(Ω) for any m. Note that for any
ηj∈{±1} (1≤j≤md) and any t∈Nd
0satisfying | t|≤s, we have
|Dt
M+mdX
j=1ηjfj|p
Lp(Ω)≤ 
M+|mdX
j=1ηj(Dtfj)|Lp(Ω)!p
≤2p 
Mp+|mdX
j=1ηj(Dtfj)|p
Lp(Ω)!
≲Mp+mdX
j=1||Dtfj||p
Lp(Ωj)
=Mp+mdX
j=1Z
Ωjm–s+|t|(DtK)(m(x–cj))p
dx
=Mp+m(|t|–s)pmdX
j=1Z
[–1
2,1
2]d(DtK)(y)p1
mddy
≤Mp+ ||DtK||p
Lp([–1
2,1
2]d)≲1.
This gives us that S ⊂Ws,p(Ω) for any m, as desired. Now let’s pick κ=1
3q
2
3nand take { w(0)
j}md
j=1
and { w(1)
j}md
j=1to be independent and identical copies of w1+κ
2andw1–κ
2respectively. Then we define
µ0,µ1to be two discrete measures supported on the finite set Ssuch that the following condition
holds for any ηj∈{±1} (1≤j≤md):
µkn
M+mdX
j=1ηjfjo
=mdY
j=1P(w(k)
j=ηj),k∈{0, 1}. (28)
In order to determine the separation distance ∆between the two priors µ0andµ1, we need to
define two quantities A:=R
Ωj(M+fj(x))qdxandB:=R
Ωj(M–fj(x))qdx, which both remain the same
for any 1 ≤j≤md. Now consider deriving a lower bound on the quantity ∆′:=A–B> 0. Note
that for any fixed j∈{1, 2,···,md}, we have M> 2α≥2m–s||K||L∞([–1
2,1
2]d)= 2||fj||L∞(Ωj), which
implies M+y>1
2M> 0 for any y∈[–||fj||L∞(Ωj), ||fj||L∞(Ωj)]. This helps us obtain the followingW orking Paper. Do not distribute. 21
lower bound on ∆′:
∆′=Z
Ωj(M+fj(x))qdx–Z
Ωj(M–fj(x))qdx=Z
ΩjZfj(x)
–fj(x)q(M+y)q–1dy
dx
≥Z
ΩjZfj(x)
–fj(x)q(1
2M)q–1dy
dx=q
2q–1Mq–1Z
Ωj
2fj(x)
dx≳Z
Ωjfj(x)dx
=Z
Ωjm–sK(m(x–cj))dx=m–sZ
[–1
2,1
2]dK(y)1
mddy=m–s–d||K||L1([–1
2,1
2]d).(29)
Moreover, let us pick λ=1
2and apply Hoeffding’s Inequality (Lemma 3) to the bounded random
variables { w(0)
j}md
j=1and { w(1)
j}md
j=1to deduce that
PmdX
j=1w(0)
j≥–(1 –λ)mdκ
≤exp
–2(λmdκ)2
4md
= exp
–1
2λ2κ2md
,
PmdX
j=1w(1)
j≤(1 –λ)mdκ
≤exp
–2(λmdκ)2
4md
= exp
–1
2λ2κ2md
.(30)
By taking c:=md
2(A+B),∆:= (1 – λ)κmd(A–B) = (1 – λ)κmd∆′andβ0=β1=exp
–1
2λ2κ2md
,
we may combine (29) and (30) justified above to get that
µ0(f∈Ws,p(Ω) :Iq
f≤c–∆) =PmdX
j=1Iq
M+w(0)
jfj≤1 – (1 – λ)κ
2mdA+1 + (1 – λ)κ
2mdB
≥PmdX
j=1w(0)
j≤–(1 –λ)mdκ
= 1 –PmdX
j=1w(0)
j≥–(1 –λ)mdκ
≥1 – exp
–1
2λ2κ2md
= 1 –β0,
µ1(f∈Ws,p(Ω) :Iq
f≥c+∆) =PmdX
j=1Iq
M+w(1)
jfj≥1 + (1 – λ)κ
2mdA+1 – (1 – λ)κ
2mdB
≥PmdX
j=1w(1)
j≥(1 –λ)mdκ
= 1 –PmdX
j=1w(0)
j≤(1 –λ)mdκ
≥1 – exp
–1
2λ2κ2md
= 1 –β1,(31)
which indicates that (13) holds true. Now let’s consider bounding the KL divergence between the two
marginal distributions P0,P1associated with µ0,µ1, respectively. Using the fact that { x1,···,xn}
are identical and independent samples from the uniform distribution on Ωagain allows us to write22 Jose Blanchet et al.
the marginal distributions in an explicit form as follows:
P0(⃗x,⃗y) =mdY
j=11 +κ
2Y
i:xi∈ΩjδM–fj(xi)(yi) +1 –κ
2Y
i:xi∈ΩjδM+fj(xi)(yi)
,
P1(⃗x,⃗y) =mdY
j=11 –κ
2Y
i:xi∈ΩjδM–fj(xi)(yi) +1 +κ
2Y
i:xi∈ΩjδM+fj(xi)(yi)
.(32)
Furthermore, for any nquadrature points { xi}n
i=1, we use Jnto denote the set of all indices jsatisfying
thatΩjcontains at least one of the points in { xi}n
i=1,i.e,
Jn:=Jn(x1,···,xn) =n
j: 1≤j≤mdandΩj∩{x1,···,xn}̸=∅o
(33)
Given that md= 200 n>n, we have | Jn|≤nfor any nquadrature points { xi}n
i=1. Using this upper
bound on | Jn| allows us to bound the KL divergence between P0andP1in the following way:
KL(P0||P1) =Z
Ω···Z
ΩZ∞
–∞···Z∞
–∞logP0(⃗x,⃗y)
P1(⃗x,⃗y)
P0(⃗x,⃗y)dy1···dyn
dx1···dxn
=Z
Ω···Z
Ω Z∞
–∞···Z∞
–∞logmdY
j=11+κ
2Q
i:xi∈ΩjδM–fj(xi)(yi) +1–κ
2Q
i:xi∈ΩjδM+fj(xi)(yi)
1–κ
2Q
i:xi∈ΩjδM–fj(xi)(yi) +1+κ
2Q
i:xi∈ΩjδM+fj(xi)(yi)
·mdY
j=11 +κ
2Y
i:xi∈ΩjδM–fj(xi)(yi) +1 –κ
2Y
i:xi∈ΩjδM+fj(xi)(yi)nY
i=1dyi!nY
i=1dxi
=Z
Ω···Z
Ω Z∞
–∞···Z∞
–∞logY
j∈Jn1+κ
2Q
i:xi∈ΩjδM–fj(xi)(yi) +1–κ
2Q
i:xi∈ΩjδM+fj(xi)(yi)
1–κ
2Q
i:xi∈ΩjδM–fj(xi)(yi) +1+κ
2Q
i:xi∈ΩjδM+fj(xi)(yi)
·Y
j∈Jn1 +κ
2Y
i:xi∈ΩjδM–fj(xi)(yi) +1 –κ
2Y
i:xi∈ΩjδM+fj(xi)(yi)nY
i=1dyi!nY
i=1dxi
=Z
Ω···Z
Ω X
j∈JnZ∞
–∞···Z∞
–∞log1+κ
2Q
i:xi∈ΩjδM–fj(xi)(yi) +1–κ
2Q
i:xi∈ΩjδM+fj(xi)(yi)
1–κ
2Q
i:xi∈ΩjδM–fj(xi)(yi) +1+κ
2Q
i:xi∈ΩjδM+fj(xi)(yi)
·1 +κ
2Y
i:xi∈ΩjδM–fj(xi)(yi) +1 –κ
2Y
i:xi∈ΩjδM+fj(xi)(yi)Y
i:xi∈Ωjdyi!nY
i=1dxi
=Z
Ω···Z
Ω|Jn|
log1 +κ
1 –κ1 +κ
2+ log1 –κ
1 +κ1 –κ
2nY
i=1dxi≤nκlog1 +κ
1 –κ
.
(34)
Now we may combine (34) and Pinkser’s inequality to upper bound the TV distance between
P0andP1as below:
TV(P0||P1)≤r
1
2KL(P0||P1)≤r
nκ
2log1 +κ
1 –κ
≤r
3n
2κ=1
3. (35)
Finally, by substituting (29), (35), ∆= (1 – λ)κmd∆′andβ0=β1=exp
–1
2λ2κ2md
=W orking Paper. Do not distribute. 23
exp(–50
27) <1
6into (14) and applying Markov’s inequality, we obtain the final lower bound
inf
ˆHq∈Hf,q
nsup
f∈Ws,p(Ω)E{xi}n
i=1,{yi}n
i=1"ˆHq
{xi}n
i=1, {yi}n
i=1
–Iq
f#
≥∆inf
ˆHq∈Hf,q
nsup
f∈Ws,p(Ω)P{xi}n
i=1,{yi}n
i=1"ˆHq
{xi}n
i=1, {yi}n
i=1
–Iq
f≥∆#
≥(1 –λ)κmd∆′1 –TV(P0||P1) –β0–β1
2≥1
2√
2
3√
3n·(200n)·∆′
6
≳√n∆′≳√n(200n)–s+d
d||K||L1([–1
2,1
2]d)≳n–s
d–1
2,(36)
which is exactly the second term in the RHS of (3). Combining the two lower bounds proved in (25)
and (36) concludes our proof of Theorem 1.
Appendix 3. Proof of Upper Bounds in Section 3
Appendix 3.1 Proof of Theorem 2 (Regression-Adjusted Control Variate)
In this subsection, we present a detailed proof of Theorem 2. With the first half of the quadrature
points { xi}n
2
i=1and observed function values { yi}n
2
i=1as inputs, we pick the regression adjusted control
variateˆf1:n
2to be the estimator returned by the oracle Kn
2specified in Assumption 3.1. Moreover, we
use the following expression to denote the variance of the functionˆfq
1:n
2(x) –fq(x) with respect to the
uniform distribution on Ω:
Var(ˆfq
1:n
2–fq) :=Z
Ω(fq(x) –ˆfq
1:n
2(x))2dx–Z
Ω(fq(x) –ˆfq
1:n
2(x))dx2
. (37)
By plugging in the expression of ˆHq
C,Iq
fand using the fact that { xi}n
i=1are identical and independent
copies of the uniform random variable over Ω, we have
E{xi}n
i=1,{yi}n
i=1"ˆHq
C
{xi}n
i=1, {yi}n
i=1
–Iq
f2#
=E{xi}n
i=1"Z
Ωˆfq
1:n
2(x)dx+2
nnX
i=n
2+1
fq(xi) –ˆfq
1:n
2(xi)
–Z
Ωfq(x)dx2#
=E
{xi}n
2
i=1"
E{xi}n
i=n
2+1"1
n
2nX
i=n
2+1
fq(xi) –ˆfq
1:n
2(xi) –Z
Ω(fq(x) –ˆfq
1:n
2(x))dx2##
=E
{xi}n
2
i=1"
4
n2nX
i=n
2+1Exi"
fq(xi) –ˆfq
1:n
2(xi) –Z
Ω(fq(x) –ˆfq
1:n
2(x))dx2##
=E
{xi}n
2
i=1h4
n2nX
i=n
2+1Var(ˆfq
1:n
2–fq)i
=2
nE
{xi}n
2
i=1h
Var(ˆfq
1:n
2–fq)i
.(38)
From the identity above, we know that it suffices to upper bound the term E
{xi}n
2
i=1h
Var(ˆfq
1:n
2–fq)i
.
Letg1:n
2:=ˆf1:n
2–fdenote the difference between the estimatorˆf1:n
2and underlying function f. Then24 Jose Blanchet et al.
we may further upper bound the expression E
{xi}n
2
i=1h
Var(ˆfq
1:n
2–fq)i
as follows:
E
{xi}n
2
i=1h
Var(ˆfq
1:n
2–fq)i
=E
{xi}n
2
i=1hZ
Ω(fq(x) –ˆfq
1:n
2(x))2dx–Z
Ω(fq(x) –ˆfq
1:n
2(x))dx2i
≤E
{xi}n
2
i=1hZ
Ω
fq(x) –ˆfq
1:n
2(x)2
dxi
=E
{xi}n
2
i=1"Z
Ω
(f(x) +g1:n
2(x))q–fq(x)2
dx#
=E
{xi}n
2
i=1"Z
ΩZg1:n
2(x)
0q(f(x) +y)q–1dy2
dx#
≤E
{xi}n
2
i=1"Z
ΩZg1:n
2(x)
01dyZg1:n
2(x)
0q2(|f(x) +y|2)q–1dydx#
≲E
{xi}n
2
i=1"Z
Ω|g1:n
2(x)|·|g1:n
2(x)| maxn
|f2q–2(x)|, |g2q–2
1:n
2(x)|o
dx#
≲E
{xi}n
2
i=1"Z
Ω|g2q
1:n
2(x)|dx#
+E
{xi}n
2
i=1"Z
Ω|g2
1:n
2(x)f2q–2(x)|dx#
.(39)
Now let’s proceed to bound from above the two expected integrals in the last line of (39). For the
first expected integral, since s>2dq–dp
2pq⇒1
2q>d–sp
pd, we may apply (5) in Assumption 3.1 to deduce
that
E
{xi}n
2
i=1"Z
Ω|g2q
1:n
2(x)|dx#
=E
{xi}n
2
i=1h
||ˆf1:n
2–f||2q
L2q(Ω)i
≲((n
2)–s
d+(1
p–1
2q)+)2q≲n2q(–s
d+1
p–1
2q)=n2q(1
p–s
d)–1,(40)
where the last equality above follows from the given assumption that p< 2q. Now let’s proceed to
bound from above the second expected integral in (39). Here we define p∗= (max{1
p–s
d, 0})–1,
i.e, p∗=pd
d–spwhen s<d
pandp∗=∞otherwise. From Sobolev Embedding Theorem (Lemma 1),
we have that Ws,p(Ω)⊆Lp∗(Ω). Based on the value of the smoothness parameter s, we have three
separate cases as below:
(Case I) When s∈(d
p,∞), we have p∗=∞andf∈Ws,p(Ω)⊂L∞(Ω). Sinceˆf1:n
2andf
are both in the Sobolev space Ws,p(Ω)⊆L∞(Ω), we may further deduce that g1:n
2=ˆf1:n
2–f∈
Ws,p(Ω)⊆L∞(Ω)⊆L2(Ω). By picking r= 2 in in (5) of Assumption 3.1, we may use the facts that
p> 2 and f∈L∞(Ω) to deduce that
E
{xi}n
2
i=1"Z
Ω|g2
1:n
2(x)f2q–2(x)|dx#
≲E
{xi}n
2
i=1"Z
Ω|g2
1:n
2(x)|dx#
=E
{xi}n
2
i=1h
||ˆf1:n
2–f||2
L2(Ω)i
≲
n–s
d+(1
p–1
2)+2
=n–2s
d,(41)
which is our final upper bound on the second expected integral in (39) under the assumption that
s∈(d
p,∞).W orking Paper. Do not distribute. 25
(Case II) When s∈(d(2q–p)
p(2q–2),d
p), we have p∗=pd
d–sp>pd
d–pd(2q–p)
p(2q–2)=p(2q–2)
p–2, which implies f∈
Ws,p(Ω)⊆Lp∗(Ω)⊆Lp(2q–2)
p–2(Ω)⊆Lp(Ω). Given thatp
p–2> 1, we can further deduce that
f2q–2∈Lp
p–2(Ω) . Moreover, sinceˆf1:n
2∈Ws,p(Ω)⊆Lp(Ω), we have that g1:n
2=ˆf1:n
2–f∈Lp(Ω).
Given that p> 2, we can further deduce that g2
1:n
2∈Lp
2(Ω). Then we may apply Hölder’s inequality
(Lemma 2) to g2
1:n
2∈Lp
2(Ω) and f2q–2∈Lp
p–2(Ω) to obtain that
E
{xi}n
2
i=1"Z
Ω|g2
1:n
2(x)f2q–2(x)|dx#
=E
{xi}n
2
i=1h
||g2
1:n
2f2q–2||L1(Ω)i
≤E
{xi}n
2
i=1h|g2
1:n
2|
Lp
2(Ω)|f2q–2|
Lp
p–2(Ω)i
≤|f|2q–2
Lp(2q–2)
p–2(Ω)E
{xi}n
2
i=1h|g1:n
2|2
Lp(Ω)i
.(42)
Note that the function h(t) =t2
pis concave and1
p∈(d–sp
pd, 1] when p> 2. Hence, applying Jensen’s
inequality and picking r=pin (5) of Assumption 3.1 further allows us to upper bound the last term
in (42) as follows:
E
{xi}n
2
i=1h
||g1:n
2||2
Lp(Ω)i
=E
{xi}n
2
i=1h
||g1:n
2||p
Lp(Ω)2
pi
≤E
{xi}n
2
i=1h
||g1:n
2||p
Lp(Ω)i2
p=E
{xi}n
2
i=1h
||ˆf1:n
2–f||p
Lp(Ω)i2
p
≲
(n
2)–s
d+(1
p–1
p)+2
≲n–2s
d.(43)
Substituting (43) into (42) then gives us the final upper bound on the second expected integral in
(39) under the assumption that s∈(d(2q–p)
p(2q–2),∞):
E
{xi}n
2
i=1"Z
Ω|g2
1:n
2(x)f2q–2(x)|dx#
≲n–2s
d. (44)
(Case III) When s∈(d(2q–p)
2pq,d(2q–p)
p(2q–2)), we have that s<d
p, which indicates that p∗=pd
d–spsatisfies
2q<p∗<p(2q–2)
p–2. Given that p∗> 2q> 2q– 2 and f∈Ws,p(Ω)⊆Lp∗(Ω), we can deduce that
f2q–2∈Lp∗
2q–2(Ω). Furthermore, note that p∗> 2qimplies2p∗
p∗+2–2q<p∗andp∗<p(2q–2)
p–2implies
2p∗
p∗+2–2q>p. Sinceˆf1:n
2andfare both in the Sobolev space Ws,p(Ω)⊆Lp∗(Ω), we may further
deduce that g1:n
2=ˆf1:n
2–f∈Ws,p(Ω)⊆Lp∗(Ω)⊆L2p∗
p∗+2–2q(Ω). Given that q≥1⇒p∗
p∗+2–2q≥1, we
have g2
1:n
2∈Lp∗
p∗+2–2q(Ω). Then we may apply Hölder’s inequality (Lemma 2) to g2
1:n
2∈Lp∗
p∗+2–2q(Ω)
andf2q–2∈Lp∗
2q–2(Ω), which yields the following upper bound:26 Jose Blanchet et al.
E
{xi}n
2
i=1"Z
Ω|g2
1:n
2(x)f2q–2(x)|dx#
=E
{xi}n
2
i=1h
||g2
1:n
2f2q–2||L1(Ω)i
≤E
{xi}n
2
i=1"|g2
1:n
2|
Lp∗
p∗+2–2q(Ω)|f2q–2|
Lp∗
2q–2(Ω)#
≤|f|2q–2
Lp∗(Ω)E
{xi}n
2
i=1"|g1:n
2|2
L2p∗
p∗+2–2q(Ω)#
.(45)
Note that the function ω(t) =tp∗+2–2q
p∗is concave since q≥1. Moreover, using the given assumption
s∈(d(2q–p)
2pq,d(2q–p)
p(2q–2)) we get thatpd
d–sp> 2q, which further yields
p∗+ 2 – 2 q
2p∗=pd
d–sp+ 2 – 2 q
2pd
d–sp>2
2pd
d–sp=d–sp
pd,
i.e,(p∗+2–2q)
2p∗∈(d–sp
pd, 1]. Hence, we may apply Jensen’s inequality and (5) in Assumption 3.1 to
upper-bound the last term in (45) as follows:
E
{xi}n
2
i=1"|g1:n
2|2
L2p∗
p∗+2–2q(Ω)#
=E
{xi}n
2
i=1" |g1:n
2|2p∗
p∗+2–2q
L2p∗
p∗+2–2q(Ω)!p∗+2–2q
p∗#
≤E
{xi}n
2
i=1"|g1:n
2|2p∗
p∗+2–2q
L2p∗
p∗+2–2q(Ω)#(p∗+2–2q)
p∗
=E
{xi}n
2
i=1"|ˆf1:n
2–f|2p∗
p∗+2–2q
L2p∗
p∗+2–2q(Ω)#(p∗+2–2q)
p∗
≲
(n
2)–s
d+(1
p–p∗+2–2q
2p∗)+2
≲n–2s
d+2(1
p–p∗+2–2q
2p∗)+.(46)
In order to simplify the last expression in (46), let’s recall the fact that p∗∈(2q,p(2q–2)
p–2) proved above.
This gives us that p∗(p– 2) < p(2q– 2)⇒2p∗>p(p∗+ 2 – 2 q),i.e,1
p>p∗+2–2q
2p∗. Then we may simplify
the power term in the last expression of (46) as follows:
–2s
d+ 21
p–p∗+ 2 – 2 q
2p∗
+= –2s
d+2
p–
1 +2
p∗–2q
p∗
=2q
p∗– 1 = 2 q1
p–s
d
– 1.
Now let’s substitute (46) into (45), which gives us the final upper bound on the second expected
integral in (39) under the assumption that s∈(d(2q–p)
2pq,d(2q–p)
p(2q–2)):
E
{xi}n
2
i=1"Z
Ω|g2
1:n
2(x)f2q–2(x)|dx#
≲n2q(1
p–s
d)–1. (47)W orking Paper. Do not distribute. 27
Combining the upper bounds derived in (40), (41), (44) and (47) finally allows us to upper bound the
expected variance E
{xi}n
2
i=1h
Var(ˆfq
1:n
2–fq)i
as below:
E
{xi}n
2
i=1h
Var(ˆfq
1:n
2–fq)i
≲n2q(1
p–s
d)–1+ max{ n–2s
d,n2q(1
p–s
d)–1}. (48)
Finally, substituting (48) into 38) derived at the beginning gives us the final upper bound:
E{xi}n
i=1,{yi}n
i=1"ˆHq
C
{xi}n
i=1, {yi}n
i=1
–Iq
f#
≤vuutE{xi}n
i=1,{yi}n
i=1"ˆHq
C
{xi}n
i=1, {yi}n
i=1
–Iq
f2#
=s
2
nE
{xi}n
2
i=1h
Var(ˆfq
1:n
2–fq)i
≲n–1
2q
n2q(1
p–s
d)–1+ max{ n–2s
d,n2q(1
p–s
d)–1}≲max{ n–s
d–1
2,n–q(s
d–1
p)–1}.(49)
This concludes our proof of Theorem 2.
Appendix 3.2 Proof of Theorem 3 (Truncated Monte Carlo)
In this subsection, we provide a complete proof of Theorem 3. For any fixed parameter M> 0, we
may divide Ωinto the following two regions:
Ω+
M:= {x∈Ω: |f(x)|≥M},Ω–
M:= {x∈Ω: |f(x)| <M}, (50)
where Ω+
M∩Ω–
M=∅andΩ+
M∪Ω–
M=Ω. Let fM(x) :=maxn
min{f(x),M}, –Mo
(∀x∈Ω)
denote a truncated version of the given function f, where Mis the threshold. Also, we use the
following expression to denote the expectation of the q-th power of the truncated function fMwith
respect to the uniform distribution on Ω:
E(fq
M(x)) =Z
Ωmaxn
min{ f(x),M}, –Moq
dx=Z
Ω+
MMqdx+Z
Ω–
Mf(x)qdx, (51)
where the last identity in (51) above follows from our definition of the two regions defined in (50).
In a similar way, we can define the variance of the function fq
Mas below:
Var(fq
M(x)) =E(f2q
M(x)) –E(fq
M(x))2
=Z
Ωmaxn
min{ f(x),M}, –Mo2q
dx–Z
Ωmaxn
min{ f(x),M}, –Moq
dx2
.(52)
Furthermore, as { xi}n
i=1are identical and independent samples of the uniform distribution on Ω, we
have that for any 1 ≤i≤n, the following identity holds
E{xi}n
i=1,{yi}n
i=1h
ˆHq
M
{xi}n
i=1, {yi}n
i=1i
=E{xi}n
i=1,{yi}n
i=1h1
nnX
i=1maxn
min{ yi,M}, –Moqi
=Exih
maxn
min{ f(xi),M}, –Moqi
=Exi[fq
M(xi)] =E(fq
M(x)).(53)28 Jose Blanchet et al.
Now we may use (53) and the bias-variance decomposition to derive an upper bound on the squared
expected risk of the estimator ˆHq
Mas follows:
E{xi}n
i=1,{yi}n
i=1"ˆHq
M
{xi}n
i=1, {yi}n
i=1
–Iq
f2#
=E{xi}n
i=1,{yi}n
i=1"ˆHq
M
{xi}n
i=1, {yi}n
i=1
–E(fq
M(x)) +E(fq
M(x)) –Iq
f2#
≤2E{xi}n
i=1,{yi}n
i=1"1
nnX
i=1maxn
min{ yi,M}, –Moq
–E(fq
M(x))2#
+ 2E{xi}n
i=1,{yi}n
i=1"E(fq
M(x)) –Iq
f2#(54)
where the first and the second term in the last line of (54) above denotes the variance and the bias
part, respectively. Again, we define p∗= (max{1
p–s
d, 0})–1,i.e, p∗=pd
d–spwhen s<d
pandp∗=∞
otherwise. Under the assumption that s<2dq–dp
2pq<d
p, we have p∗=pd
d–sp∈(p, 2q). Moreover, from
Sobolev Embedding Theorem (Lemma 1), we have that f∈Ws,p(Ω)⊆Lp∗(Ω).
On the one hand, since p< 2q, we can deduce that | f(x)|2q≤M2q–p∗|f(x)|p∗for any x∈Ω–
M
andM2q≤M2q–p∗|f(x)|p∗for any x∈Ω+
M, which helps us upper bound the variance part as below:
E{xi}n
i=1,{yi}n
i=1"1
nnX
i=1maxn
min{ yi,M}, –Moq
–E(fq
M(x))2#
=E{xi}n
i=1"1
nnX
i=1
fq
M(xi) –Exih
fq
M(xi)i2#
=1
n2nX
i=1Exih
fq
M(xi) –Exih
fq
M(xi)i2i
=1
nVar(fq
M(x))
≤1
nE(f2q
M(x)) =1
nZ
Ω+
MM2qdx+Z
Ω–
Mf(x)2qdx
≤1
nZ
Ω+
MM2q–p∗|f(x)|p∗dx+Z
Ω–
MM2q–p∗|f(x)|p∗dx
=1
nZ
ΩM2q–p∗|f(x)|p∗dx≲M2q–p∗
n,(55)
where the last step of (55) above follows from the fact that f∈Ws,p(Ω)⊆Lp∗(Ω).
On the other hand, using the fact that p∗>p>q⇒|f(x)|q≤Mq–p∗|f(x)|p∗for any x∈Ω+
M,W orking Paper. Do not distribute. 29
we may upper-bound the bias part as follows:
E{xi}n
i=1,{yi}n
i=1"E(fq
M(x)) –Iq
f2#
=Z
Ω+
MMqdx+Z
Ω–
Mf(x)qdx–Z
Ω+
Mfq(x)dx–Z
Ω–
Mf(x)qdx2
=Z
Ω+
M
Mq–fq(x)
dx2
≤Z
Ω+
MMq–fq(x)dx2
≤Z
Ω+
M
Mq+ |f(x)|q
dx2
≤2Z
Ω+
M|f(x)|qdx2
≲Z
Ω+
MMq–p∗|f(x)|p∗dx2
≤M2q–2p∗Z
Ω|f(x)|p∗dx2
≲M2q–2p∗,(56)
where the last step above again follows from the fact that f∈Ws,p(Ω)⊆Lp∗(Ω). By substituting
(55) and (56) into (54), we obtain that
E{xi}n
i=1,{yi}n
i=1"ˆHq
M
{xi}n
i=1, {yi}n
i=1
–Iq
f#
≤vuutE{xi}n
i=1,{yi}n
i=1"ˆHq
M
{xi}n
i=1, {yi}n
i=1
–Iq
f2#
≲s
M2q–p∗
n+M2q–2p∗.(57)
By balancing the variance partM2q–p∗
nand the bias part M2q–2p∗above, we may get the optimal
choice of Mas follows:M2q–p∗
n=M2q–2p∗⇒M=Θ(n1
p∗). Plugging in the optimal choice of Mgives
us the final upper bound:
E{xi}n
i=1,{yi}n
i=1"ˆHq
M
{xi}n
i=1, {yi}n
i=1
–Iq
f#
≲q
n2q–2p∗
p∗=nq
p∗–1=n–q(s
d–1
p)–1, (58)
which finishes our proof of Theorem 3.
Appendix 4. Proof of Minimax Lower and Upper Bounds in Section 4
This section is organized as follows. The first subsection consists of one important lemma used in our
proof. In the second subsection, we provide complete proof for the minimax optimal lower bound
on the estimation of integrals under any level of noise. In the third subsection, a complete proof for
the upper bound on the estimation of integrals is given.
Appendix 4.1 A Key Lemma for Establishing the Upper Bound on Integral Estimation
Lemma 6 (Bound on the Expected k-Nearest Neighbor Distance: Theorem 2.4, (Biau and Devroye
2015)) .Assume that x 1,x2,···,xnare independent and identical samples from the uniform distribution
on the domain Ω= [0, 1]d. For any k ∈{1, 2,···,n}and z∈Ω, we use xi(z)
kto denote the k-th nearest
neighbor of z among {xi}n
i=1. When z is also uniformly distributed over the domain Ω, we have the following
upper bound on the expected distance between z and xi(z)
k:
Ez,{xi}n
i=1h
||z–xi(z)
k||2i
≲(k
n)2
d. (59)30 Jose Blanchet et al.
Appendix 4.2 Proof of Theorem 4 (Lower Bound on Integral Estimation)
Here we present a comprehensive proof of the two lower bounds given in Theorem 4 above by
applying the method of two fuzzy hypotheses (Lemma 5). Below we again use ⃗x:= (x1,x2,···,xn)
and⃗y:= (y1,y2,···,yn) to denote the two n-dimensional vectors formed by the quadrature points
and observed function values. Since our lower bound in Theorem 4 consists of two terms, we need
to prove the two bounds in the following two separate cases:
(Case I) For the first lower bound in (10), let’s consider two constant functions g0andg1defined
as follows:
g0(x)≡0 (∀x∈Ω),g1(x)≡n–γ–1
2(∀x∈Ω) (60)
Clearly we have g0,g1∈Cs(Ω). Then let’s take µkto be a Dirac delta measure supported on the set
{gj},i.e,µk({gk}) = 1, for k∈{0, 1}. By picking c=∆=1
2Ig1=1
2n–γ–1
2andβ0=β1= 0, we then
obtain that
µ0(f∈Ws,p(Ω) :If≤c–∆) =µ0(If≤0) = 1 = 1 – β0,
µ1(f∈Ws,p(Ω) :If≥c+∆) =µ1(If≥Ig1) = 1 = 1 – β1,(61)
which indicates that (13) holds true. Now let’s consider bounding the KL divergence between the two
marginal distributions P0,P1associated with µ0,µ1, respectively. Given that the quadrature points
{xi}n
i=1and the observational noises { ϵi}n
i=1are independent and identical samples from the uniform
distribution on Ωand the normal distribution N(0,n–2γ), we can write the marginal distributions
in an explicit form as follows:
P0(⃗x,⃗y) =nY
i=11√
2πn–γe–1
2n–2γy2
i
,P1(⃗x,⃗y) =nY
i=11√
2πn–γe–1
2n–2γ(yi–n–γ–1
2)2
. (62)
From (62) we can see that P0andP1are two n-dimensional normal distributions having the
same covariance matrix but different mean vectors. Computing the KL divergence between them
and applying Pinsker’s inequality then give us that
TV(P0||P1)≤r
1
2KL(P0||P1) =s
n(n–γ–1
2)2
4n–2γ=1
2. (63)
Substituting (63), ∆=1
2Ig1=1
2n–γ–1
2andβ0=β1= 0 into (14) and applying Markov’s inequality
yield the final lower bound
inf
ˆH∈Hf
nsup
f∈Cs(Ω)E{xi}n
i=1,{yi}n
i=1"ˆH
{xi}n
i=1, {yi}n
i=1
–If#
≥∆inf
ˆH∈Hf
nsup
f∈Cs(Ω)P{xi}n
i=1,{yi}n
i=1"ˆH
{xi}n
i=1, {yi}n
i=1
–If≥∆#
≥1
2Ig11 –TV(P0||P1) –β0–β1
2≥1
8n–γ–1
2≳n–γ–1
2,(64)
which is exactly the first term in the RHS of (10).
(Case II) For the second lower bound in (10), our proof is similar to the proof of the second
lower bound in Theorem 1 presented in Appendix Appendix 2.2 above. Again, we select m= (200 n)1
d
and divide the domain Ωintomdsmall cubes Ω1,Ω2,···,Ωmd, each of which has side length m–1.
For any 1 ≤j≤md, we use cjto denote center of the cube Ωj. Then let’s consider the same bumpW orking Paper. Do not distribute. 31
function Kdefined in (15) and (16) above, which satisfies supp(K)⊆[–1
2,1
2]dandK∈C∞([–1
2,1
2]d).
In an analogous way, for any 1 ≤j≤md, we associate each cube Ωjwith a bump function fjdefined
as follows:
fj(x) =(
m–sK(m(x–cj)) (x∈Ωj),
0 (otherwise),(65)
where supp(fj)⊆Ωj,fj∈C∞(Ω) and fj(x)≥0 (∀x∈Ω). Then let’s consider the following finite
set of 2mdfunctions:
S:=nmdX
j=1ηjfj:ηj∈{±1},∀1≤j≤mdo
. (66)
We will first verify that S ⊆Cs(Ω). Fix any element f∗=Pmd
j=1ηjfj∈ S. On the one hand, from our
construction of the fj’s given in (65) above, we have
max
|t|≤⌊s⌋||Dtf∗||L∞(Ω)= max
|t|≤⌊s⌋m–s+|t|||DtK||L∞([–1
2,1
2]d)
≤max
|t|≤⌊s⌋||DtK||L∞([–1
2,1
2]d).(67)
On the other hand, for any 1 ≤i̸=j≤md, we consider the function ψifi+ψjfj, where the scalars
ψj,ψj∈{0,±1}. Now let’s may pick β:=d
1–{s}, where { s} =s–⌊s⌋ ∈(0, 1) denotes the fractional
part of s. Given that fj∈C∞(Ω), we may upper bound the Sobolev norm || ·||W1,βof the function
Dt(ψifi+ψjfj) for any t∈Nd
0satisfying | t| =⌊s⌋as follows:
|Dt(ψifi+ψjfj)|β
W1,β(Ω)= |ψi|β|Dtfi|β
W1,β(Ωi)+ |ψj|β|Dtfj|β
W1,β(Ωj)
≤|Dtfi|β
Lβ(Ωi)+dX
r=1|∂
∂xrDtfi|β
Lβ(Ωi)+|D⌊s⌋fj|β
Lβ(Ωj)+dX
r=1|∂
∂xrDtfj|β
Lβ(Ωj)
=X
l∈{i,j}Z
Ωl
m–s+|t|DtK(m(x–cl))β
dx
+X
l∈{i,j}dX
r=1Z
Ωl
m–s+|t|+1∂
∂xrDtK(m(x–cl))β
dx.(68)
From our choice of βand assumption on the bump function K, we may further upper bound32 Jose Blanchet et al.
the Sobolev norm|Dt(ψifi+ψjfj)|W1,β(Ω)as below:
|Dt(ψifi+ψjfj)|β
W1,β(Ω)≤X
l∈{i,j}m–β{s}Z
[–1
2,1
2]d
DtK(y)β1
mddy
+X
l∈{i,j}dmβ(1–{s})sup
|t′|≤⌊s⌋+1 Z
[–1
2,1
2]d
Dt′K(y)β1
mddy!
≤2m–β{s}–d|DtK|β
Lβ([–1
2,1
2]d)+ 2dmβ(1–{s})–d·sup
|t′|≤⌊s⌋+1|Dt′K|β
Lβ([–1
2,1
2]d)
≲|DtK|β
Lβ([–1
2,1
2]d)+ sup
|t′|≤⌊s⌋+1|Dt′K|β
Lβ([–1
2,1
2]d),(69)
where the last inequality above follows from our choice of β. From (69) and the second part of
the Sobolev Embedding Theorem (Lemma 1), we can deduce that Dt(ψifi+ψjfj)∈C1(Ω)∩
W1,d
1–{s}(Ω)⊆C{s}(Ω) and the following inequality holds:
|Dt(ψifi+ψjfj)|C{s}(Ω)≲|Dt(ψifi+ψjfj)|W1,β(Ω)
≲ 
sup
|t′|=⌊s⌋|Dt′K|β
Lβ([–1
2,1
2]d)+ sup
|t′|=⌊s⌋+1|Dt′K|β
Lβ([–1
2,1
2]d)!1
β
,(70)
Furthermore, combining (70) with our construction of the fj’s given in (65) above gives us that
max
|t|=⌊s⌋sup
x,y∈Ω,x̸=y|Dtf∗(x) –Dtf∗(y)|
||x–y||s–⌊s⌋
≤ max
1≤i̸=j≤k
ψi,ψj∈{0,±1}max
|t|=⌊s⌋sup
x̸=y∈Ω|Dt(ψifi+ψjfj)(x) –Dt(ψifi+ψjfj)(x)|
||x–y||{s}
≤ max
1≤i̸=j≤k,|t|=⌊s⌋
ψi,ψj∈{0,±1}|Dt(ψifi+ψjfj)|C{s}(Ω)
≲ 
sup
|t′|=⌊s⌋|Dt′K|β
Lβ([–1
2,1
2]d)+ sup
|t′|=⌊s⌋+1|Dt′K|β
Lβ([–1
2,1
2]d)!1
β(71)
Finally, adding the two inequalities (67) and (71) gives us that for any f∗∈ S, we have
||f∗||Cs(Ω)= max
|t|≤⌊s⌋||Dtf∗||L∞(Ω)+ max
|t|=⌊s⌋sup
x,y∈Ω,x̸=y|Dtf∗(x) –Dtf∗(y)|
||x–y||s–⌊s⌋
≲max
|t|≤⌊s⌋||DtK||L∞([–1
2,1
2]d)
+ 
sup
|t′|=⌊s⌋|Dt′K|β
Lβ([–1
2,1
2]d)+ sup
|t′|=⌊s⌋+1|Dt′K|β
Lβ([–1
2,1
2]d)!1
β
≲1.(72)
From the arbitrariness of f∗, we can then deduce that S ⊆Cs(Ω), as desired. For any p∈(0, 1), below
we again use wpto denote the discrete random variable satisfying P(wp= –1) = pandP(wp= 1) = 1– p.W orking Paper. Do not distribute. 33
Now let’s pick κ=1
3q
2
3nand take { w(0)
j}md
j=1and { w(1)
j}md
j=1to be independent and identical copies of
w1+κ
2andw1–κ
2respectively. Then we define µ0,µ1to be two discrete measures supported on the
finite set Ssuch that the following condition holds for any ηj∈{±1} (1≤j≤md):
µkn mdX
j=1ηjfjo
=mdY
j=1P(w(k)
j=ηj),k∈{0, 1}. (73)
Then we proceed to determine the separation distance ∆between the two priors µ0andµ1. Similar to
what we did in the proof of Theorem 1, we need to first define the following quantity C:=R
Ωjfj(x)dx,
which remains the same for any 1 ≤j≤md. Moreover, applying (65) helps us evaluate the quantity
Cdirectly as follows
C=Z
Ωjfj(x)dx=Z
Ωjm–sK(m(x–cj))dx
=m–sZ
[–1
2,1
2]dK(y)1
mddy=m–s–d||K||L1([–1
2,1
2]d).(74)
Moreover, by picking λ=1
2, we may apply Hoeffding’s Inequality (Lemma 3) to the bounded
random variables { w(0)
j}md
j=1and { w(1)
j}md
j=1to deduce that
PmdX
j=1w(0)
j≥–(1 –λ)mdκ
≤exp
–2(λmdκ)2
4md
= exp
–1
2λ2κ2md
,
PmdX
j=1w(1)
j≤(1 –λ)mdκ
≤exp
–2(λmdκ)2
4md
= exp
–1
2λ2κ2md
.(75)
By taking c:= 0,∆:= (1 – λ)κmdCandβ0=β1=exp
–1
2λ2κ2md
, we may use (75) justified above
to get that
µ0(f∈Cs(Ω) :If≤c–∆) =PmdX
j=1Iw(0)
jfj≤1 – (1 – λ)κ
2mdC–1 + (1 – λ)κ
2mdC
≥PmdX
j=1w(0)
j≤–(1 –λ)mdκ
= 1 –PmdX
j=1w(0)
j≥–(1 –λ)mdκ
≥1 – exp
–1
2λ2κ2md
= 1 –β0,
µ1(f∈Cs(Ω) :If≥c+∆) =PmdX
j=1Iw(1)
jfj≥1 + (1 – λ)κ
2mdC–1 – (1 – λ)κ
2mdC
≥PmdX
j=1w(1)
j≥(1 –λ)mdκ
= 1 –PmdX
j=1w(0)
j≤(1 –λ)mdκ
≥1 – exp
–1
2λ2κ2md
= 1 –β1,(76)34 Jose Blanchet et al.
which indicates that (13) holds true. Now let’s consider bounding the KL divergence between
the two marginal distributions P0,P1associated with µ0,µ1, respectively. Applying the fact that
{x1,···,xn} and { ϵ1,···,ϵn} are identical and independent samples from the uniform distribution
onΩand the normal distribution N(0,n–2γ) allows us to write the marginal distributions in an
explicit form as follows:
P0(⃗x,⃗y) =mdY
j=11 –κ
2Y
i:xi∈Ωj1√
2πn–γe–(yi–fj(xi))2
2n–2γ+1 +κ
2Y
i:xi∈Ωj1√
2πn–γe–(yi+fj(xi))2
2n–2γ
,
P1(⃗x,⃗y) =mdY
j=11 +κ
2Y
i:xi∈Ωj1√
2πn–γe–(yi–fj(xi))2
2n–2γ+1 –κ
2Y
i:xi∈Ωj1√
2πn–γe–(yi+fj(xi))2
2n–2γ
.(77)
Furthermore, for any nfixed quadrature points ⃗x= (x1,x2,···,xn), we use Pk(·|⃗x) to denote
the marginal distribution of the observed function values ⃗y= (y1,y2,···,yn) conditioned on ⃗xfor
k∈{0, 1}. Since { xi}n
i=1are identically and independently sampled from the uniform distribution
onΩ, we have that the two probability densities Pk(⃗x,⃗y) andPk(⃗y|⃗x) have the same mathematical
expression for any k∈{0, 1}. Then we may further rewrite the KL divergence between the two
marginal distributions P0,P1as follows:
KL(P0||P1) =Z
Ω···Z
ΩZ∞
–∞···Z∞
–∞logP0(⃗x,⃗y)
P1(⃗x,⃗y)
P0(⃗x,⃗y)dy1···dyn
dx1···dxn
=Z
Ω···Z
ΩZ∞
–∞···Z∞
–∞logP0(⃗y|⃗x)
P1(⃗y|⃗x)
P0(⃗y|⃗x)dy1···dyn
dx1···dxn
=Z
Ω···Z
Ω 
KL
P0(·|⃗x)||P1(·|⃗x)!
dx1···dxn.(78)
It now remains to upper bound the KL divergence between the two conditional distributions P0(·|⃗x)
andP1(·|⃗x) for any fixed ⃗x= (x1,···,xn). In order to derive such an upper bound, we need to
introduce the following notations first. For any nquadrature points { xi}n
i=1, we use Jnto denote the
set of all indices jsatisfying that Ωjcontains at least one of the points in { xi}n
i=1,i.e,
Jn:=Jn(⃗x) =n
j: 1≤j≤mdandΩj∩{x1,···,xn}̸=∅o
. (79)
Moreover, we use ⃗ω(k)
Jnto denote | Jn|-dimensional vector formed by the random variables { ω(k)
j:
j∈ J n} and p(k)
Jn(·) to denote the probability density function of ⃗ω(k)
Jn, where k∈{0, 1}. From
our assumption on the distribution of the weights { w(0)
j}n
j=1and { w(1)
j}n
j=1, we have that for any
⃗ωJn∈{±1}|Jn|,
p(0)
Jn(⃗ωJn) =Y
j∈Jn1 +κ
21
2(1–ωj)1 –κ
21
2(1+ωj)
p(1)
Jn(⃗ωJn) =Y
j∈Jn1 +κ
21
2(1+ωj)1 –κ
21
2(1–ωj)(80)
Furthermore, for any fixed quadrature points ⃗x= (x1,···,xn) and weights ⃗ωJn:= {ωj:j∈ Jn}⊆W orking Paper. Do not distribute. 35
{±1}|Jn|, we may define the transition kernel G(⃗x,⃗ωJn) as below
G(⃗x,⃗ωJn) :=Y
j∈JnY
i:xi∈Ωj1√
2πn–γe–(yi+ωjfj(xi))2
2n–2γ
(81)
Combining the expressions in (77),(80) and (81) allows us to rewrite the two conditional distributions
Pk(·|⃗x) as below:
Pk(⃗y|⃗x) =Pk(⃗x,⃗y) =Z
{±1}|Jn|G(⃗x,⃗ωJn)p(k)
Jn(⃗ωJn)d⃗ωJn (82)
where k∈{0, 1}. Applying the data processing inequality (Lemma 4) to (82) above then enables us
to derive the following upper bound on KL
P0(·|⃗x)||P1(·|⃗x)
for any nfixed quadrature points
⃗x= (x1,···,xn):
KL
P0(·|⃗x)||P1(·|⃗x)
≤KL
p(0)
Jn||p(1)
Jn
= |Jn|
log1 +κ
1 –κ1 +κ
2+ log1 –κ
1 +κ1 –κ
2
≤nκlog1 +κ
1 –κ(83)
where the equality in (83) above follows from the fact that { w(0)
j}md
j=1and { w(1)
j}md
j=1are independent
and identical copies of w1+κ
2andw1–κ
2respectively. The last inequality of (83) above, however, is
deduced from the fact that md= 200 n>n, which implies | Jn|≤nfor any nquadrature points { xi}n
i=1.
Substituting (83) into (78) and applying Pinkser’s inequality yields the final upper bound on the TV
distance between P0andP1:
TV(P0||P1)≤r
1
2KL(P0||P1)≤sZ
Ω···Z
Ωnκ
2log1 +κ
1 –κ
dx1···dxn
=r
nκ
2log1 +κ
1 –κ
≤r
3n
2κ=1
3.(84)
Finally, by substituting (74), (84), ∆= (1 – λ)κmdCandβ0=β1=exp
–1
2λ2κ2md
=
exp(–50
27) <1
6into (14) and applying Markov’s inequality, we obtain the final lower bound
inf
ˆH∈Hf
nsup
f∈Cs(Ω)E{xi}n
i=1,{yi}n
i=1"ˆH
{xi}n
i=1, {yi}n
i=1
–If#
≥∆inf
ˆH∈Hf
nsup
f∈Cs(Ω)P{xi}n
i=1,{yi}n
i=1"ˆH
{xi}n
i=1, {yi}n
i=1
–If≥∆#
≥(1 –λ)κmdC1 –TV(P0||P1) –β0–β1
2≥1
2√
2
3√
3n·(200n)·C
6
≳√nC≳√n(200n)–s+d
d||K||L1([–1
2,1
2]d)≳n–s
d–1
2,(85)
which is exactly the second term in the RHS of (10). Combining the two lower bounds proved in
(64) and (85) concludes our proof of Theorem 436 Jose Blanchet et al.
Appendix 4.3 Proof of Theorem 5 (Upper Bound on Integral Estimation)
Before proving the upper bound on integral estimation, we need to derive an upper bound on the
expected error of the k-nearest neighbor estimatorˆfk-NN, which is built based on the first half of the
given dataset {( xi,yi)}n
i=1, with respect to the L2norm. From our construction ofˆfk-NN given in
Section 4.2, we have that for any fixedn
2quadrature points { xi}n
2
i=1,z∈Ωandk∈{1, 2,···,n
2},
the expected value ofˆfk-NN(z) with respect to the observational noises { ϵi}n
2
i=1is given by
E
{ϵi}n
2
i=1hˆfk-NN(z)i
=1
kkX
j=1E
{ϵi}n
2
i=1h
f(xi(z)
j) +ϵi(z)
ji
=1
kkX
j=1f(xi(z)
j), (86)
where { xi(z)
j}k
j=1above are the knearest neighbors of zamong { xi}n
2
i=1. Now let’s consider using the
bias-variance decomposition to upper bound the error ||ˆfk-NN(z) –f(z)||2
L2(Ω). Based on the expected
value computed in (86) above, we may decompose the functionˆfk-NN–fas a sum of the bias part
and the variance part as follows:
B(z) :=E
{ϵi}n
2
i=1hˆfk-NN(z)i
–f(z) =1
kkX
j=1f(xi(z)
j) –f(z) =1
kkX
j=1
f(xi(z)
j) –f(z)
, (87)
V(z) :=ˆfk-NN(z) –E
{ϵi}n
2
i=1hˆfk-NN(z)i
=ˆfk-NN(z) –1
kkX
j=1f(xi(z)
j) =1
kkX
j=1ϵi(z)
j, (88)
where the function Bcorresponds to the bias part and the function Vcorresponds to the variance
part. Using the decompositionˆfk-NN–f=B+Vallows us to upper bound the expected error of
ˆfk-NN with respect to the L2norm as below:
E
{xi}n
2
i=1,{yi}n
2
i=1h
||ˆfk-NN–f||2
L2(Ω)i
=E
{xi}n
2
i=1,{yi}n
2
i=1h
||B+V||2
L2(Ω)i
≤E
{xi}n
2
i=1,{yi}n
2
i=1"
||B||L2(Ω)+ ||V||L2(Ω)2#
≤E
{xi}n
2
i=1,{yi}n
2
i=1h
2||B||2
L2(Ω)+ 2||V||2
L2(Ω)i
≲E
{xi}n
2
i=1,{yi}n
2
i=1h
||V||2
L2(Ω)i
+E
{xi}n
2
i=1,{yi}n
2
i=1h
||B||2
L2(Ω)i
=E
z,{xi}n
2
i=1,{ϵi}n
2
i=1h
|V(z)|2i
+E
z,{xi}n
2
i=1,{ϵi}n
2
i=1h
|B(z)|2i
,(89)
where zabove is uniformly distributed over the domain Ωand independent of xifor any 1 ≤i≤n
2.
On the one hand, using the expression of the variance part Vderived in (88) above and the fact that
{ϵi}n
2
i=1are independent and identical distributed noises, we may compute the first term in (89) aboveW orking Paper. Do not distribute. 37
as follows:
E
z,{xi}n
2
i=1,{ϵi}n
2
i=1h
|V(z)|2i
=Ez"
E
{xi}n
2
i=1,{ϵi}n
2
i=1"1
kkX
j=1ϵi(z)
j2##
=Ez"
1
k2E
{xi}n
2
i=1,{ϵi}n
2
i=1"kX
j=1ϵ2
i(z)
j##
=Ezhn–2γk
k2i
=n–2γ
k.(90)
On the other hand, since s∈(0, 1) and the given function fiss-Hölder smooth, we have that
the inequality | f(x) –f(y)|≲||x–y||sholds true for any x,y∈Ω. Combining this inequality with
the expression of the bias part Bderived in (88) above helps us upper bound the second term in (89)
as below:
E
z,{xi}n
2
i=1,{ϵi}n
2
i=1h
|B(z)|2i
=Ez"
E
{xi}n
2
i=1,{ϵi}n
2
i=1"1
kkX
j=1
f(xi(z)
j) –f(z)2##
≤1
kEz"
E
{xi}n
2
i=1"kX
j=1f(xi(z)
j) –f(z)2##
≲1
kEz"
E
{xi}n
2
i=1"kX
j=1xi(z)
j–z2s##
≤E
z,{xi}n
2
i=1"xi(z)
k–z2s#
≤ 
E
z,{xi}n
2
i=1"xi(z)
k–z2#!s
≲k
n2s
d.(91)
The second least inequality follows from the fact that ω(t) :=tsis a concave function when s∈(0, 1),
while the last inequality is obtained by plugging in (59) given in Lemma 6. Substituting (90) and
(91) into (89) then yields that for any k∈{1, 2,···,n
2}, the expected error ofˆfk-NN with respect to
theL2norm can be upper bounded as follows:
E
{xi}n
2
i=1,{yi}n
2
i=1h
||ˆfk-NN–f||2
L2(Ω)i
≲n–2γ
k+k
n2s
d. (92)
Furthermore, from our construction of the integral estimator ˆHk-NN given in Section 4.2, we may
upper bound the expectation of the estimator ˆHk-NN’s squared error via the expected error ofˆfk-NN38 Jose Blanchet et al.
with respect to the L2norm as below:
E{xi}n
i=1,{yi}n
i=1"ˆHk-NN
{xi}n
i=1, {yi}n
i=1
–If2#
=E{xi}n
i=1,{yi}n
i=1"Z
Ωˆfk-NN(x)dx+2
nnX
i=n
2+1
yi–ˆfk-NN(xi)
–Z
Ωf(x)dx2#
≲E
{xi}n
2
i=1,
{yi}n
2
i=1"
E{xi}n
i=n
2+1,
{yi}n
i=n
2+1"1
n
2nX
i=n
2+1
f(xi) –ˆfk-NN(xi) –Z
Ω(f(x) –ˆfk-NN(x))dx2##
+E{xi}n
i=1,{yi}n
i=1"2
nnX
i=n
2+1ϵi2#
=E
{xi}n
2
i=1,{yi}n
2
i=1"
4
n2nX
i=n
2+1Exi"
f(xi) –ˆfk-NN(xi) –Z
Ω(f(x) –ˆfk-NN(x))dx2##
+4
n2nX
i=n
2+1Exi,yih
ϵ2
ii
≲1
n 
E
{xi}n
2
i=1,{yi}n
2
i=1h
||ˆfk-NN–f||2
L2(Ω)i
+n–2γ!
≲1
nn–2γ
k+k
n2s
d
+n–2γ–1.(93)
Based on the magnitude of the noises, we have the following two cases for the final upper bound:
When γ∈[0,s
d), the optimal kis determined by balancing the two termsn–2γ
kand
k
n2s
din (93),
which yieldsn–2γ
k=
k
n2s
d⇒k=Θ(n2(s–γd)
d+2s). The corresponding upper bound is given by
1
nn–2γ
k+k
n2s
d
+n–2γ–1≲1
nn–2γ–2(s–γd)
d+2s+n–1–2γ=n–2s(1+2γ)
2s+d–1+n–2γ–1
≲max{ n–2s(1+2γ)
2s+d–1,n–2γ–1} =n–2γ–1.(94)
When γ∈[s
d,∞], we note that k∈{1, 2,···,n
2} must be of at least constant level. Therefore, the
optimal kis determined by balancing the two termsn–2γ–1
kandn–2γ–1, which yields that k=Θ(1) is
of constant level. The corresponding upper bound is given by
1
nn–2γ
k+k
n2s
d
+n–2γ–1≲n–2s
d–1+n–2γ–1
≲max{ n–2s
d–1,n–2γ–1} =n–2s
d–1.(95)W orking Paper. Do not distribute. 39
Finally, substituting (94) and (95) into (93) gives us the final upper bound:
E{xi}n
i=1,{yi}n
i=1"ˆHk-NN
{xi}n
i=1, {yi}n
i=1
–If#
≤vuutE{xi}n
i=1,{yi}n
i=1"ˆHk-NN
{xi}n
i=1, {yi}n
i=1
–If2#
≲s
1
nn–2γ
k+k
n2s
d
+n–2γ–1≲q
max{ n–2s
d–1,n–2γ–1}
=nmax{–1
2–γ,–1
2–s
d},(96)
which concludes our proof of Theorem 5.