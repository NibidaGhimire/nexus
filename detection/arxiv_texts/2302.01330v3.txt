1
SceneDreamer: Unbounded 3D Scene
Generation from 2D Image Collections
Zhaoxi Chen, Guangcong Wang, and Ziwei Liu
Abstract —In this work, we present SceneDreamer , an unconditional generative model for unbounded 3D scenes, which synthesizes
large-scale 3D landscapes from random noise. Our framework is learned from in-the-wild 2D image collections only, without any 3D
annotations. At the core of SceneDreamer is a principled learning paradigm comprising 1)an efficient yet expressive 3D scene
representation, 2)a generative scene parameterization, and 3)an effective renderer that can leverage the knowledge from 2D images.
Our approach begins with an efficient bird’s-eye-view (BEV) representation generated from simplex noise, which includes a height field
for surface elevation and a semantic field for detailed scene semantics. This BEV scene representation enables 1) representing a 3D
scene with quadratic complexity, 2) disentangled geometry and semantics, and 3) efficient training. Moreover, we propose a novel
generative neural hash grid to parameterize the latent space based on 3D positions and scene semantics, aiming to encode
generalizable features across various scenes. Lastly, a neural volumetric renderer, learned from 2D image collections through
adversarial training, is employed to produce photorealistic images. Extensive experiments demonstrate the effectiveness of
SceneDreamer and superiority over state-of-the-art methods in generating vivid yet diverse unbounded 3D worlds. Project Page is
available at https://scene-dreamer.github.io/. Code is available at https://github.com/FrozenBurning/SceneDreamer.
Index Terms —Neural Rendering, GAN, 3D Generative Model, Unbounded Scene Generation
✦
1 I NTRODUCTION
SCENE GENERATION has raised considerable attention
in recent years, addressing the growing need for 3D
creative tools in the metaverse. At the core of 3D content
creation is inverse graphics, which aims to recover 3D
representations from 2D observations. Given the cost and
labor for creating 3D assets, the ultimate goal of 3D content
creation would be learning a generative model from in-
the-wild 2D images. Recent work on 3D-aware generative
models tackles the problem to some extent, by learning to
generate object-level content ( e.g.,faces [1], [2], [3], bodies [4]
or common objects [5], [6]) from curated 2D image data.
However, the observation space is in a bounded domain and
the generated target occupies a limited region of Euclidean
space. It is highly desirable to learn 3D generative models
for unbounded scenes from in-the-wild 2D images, e.g., a
vivid landscape that covers an arbitrarily large region (as
shown in Fig. 1), which we aim to tackle in this paper.
To generate unbounded 3D scenes from in-the-wild 2D
images, intuitively, three critical issues must be addressed:
the unbounded range of scenes, unaligned content in scale
and coordinate, and in-the-wild 2D images without knowl-
edge of camera pose. Specifically, a successful unbounded
scene generation model should overcome the following
challenges: 1)Lack of efficient 3D representation for un-
bounded 3D scenes. Unbounded scenes can occupy an
arbitrarily large region of Euclidean space, necessitating the
design of an efficient 3D representation; 2)Lack of content
alignment. Given a set of in-the-wild 2D images, objects
with different semantics could be captured in varying scales,
3D locations, and orientations. The unaligned content often
•Z. Chen, G. Wang, and Z. Liu are affiliated with S-Lab, Nanyang
Technological University.leads to unstable training; 3)Lack of priors on camera pose
distributions. In-the-wild 2D images may originate from
non-overlapping views or different image sources, making it
difficult to use structure-from-motion to estimate the camera
poses on in-the-wild 2D images due to the absence of
reliable correspondence between different images.
Existing methods only offer partial solutions to these
three challenges, as summarized in Table 1. For the first
problem, object-centric 3D GANs [1], [2], [7] warp the scene
to a bounded domain within the camera view. However,
the space and scale of occupancy are not determined for
unbounded scenes. Plus, instead of capturing the entire
scene, observation space in unbounded scenes is often a
partial and small portion, which makes it a waste of capacity
to model the scene as a whole. Though efficient representa-
tions, e.g., tri-plane [1], 2D manifolds [8], [9], sparse voxel
grid [10] have been proposed for object-centric 3D GANs,
they model the scene as a whole which is not practical for
large-scale unbounded scenes. For the second problem, ex-
isting 3D GANs either need ground truth camera poses [11]
or require a camera pose prior [1]. However, we cannot
estimate camera pose distribution on in-the-wild images
collected from non-overlapping views or different image
sources. For the third problem, current 3D GANs are trained
on curated and aligned datasets [12], [13], [14] where all
objects are in the same scale, 3D location, and orientation
and the camera covers the entire scene. However, in-the-
wild scene-level content may lead to unstable training due
to its extremely high diversity. It is crucial for 3D generative
models to align in-the-wild 2D content with 3D semantics.
Given the aforementioned challenges, we propose a
principled learning paradigm, SceneDreamer, that learns to
generate unbounded 3D scenes from in-the-wild 2D image
collection without camera parameters. To facilitate that,arXiv:2302.01330v3  [cs.CV]  7 Dec 20232
In-the-wild 
Image CollectionsPhotorealistic
Unbounded 3D Scenes
Multi -view consistent Well-defined geometry
Diverse scenes and styles
Fig. 1. SceneDreamer learns to generate unbounded 3D scenes from in-the-wild 2D image collections. Our method can synthesize diverse
landscapes across different styles, with 3D consistency, well-defined depth, and free camera trajectory.
our framework consists of three modules, an efficient yet
expressive 3D scene representation, a generative scene pa-
rameterization, and a volumetric renderer that can leverage
knowledge from 2D images.
First, we propose a BEV scene representation that is
both efficient and expressive for 3D unbounded scenes
(Challenge 1). Given a simplex noise as input, we generate
a height field and a semantic field to represent a unique
3D scene from a bird’s-eye-view. The height field represents
the surface elevation of the scene, while the semantic field
provides detailed scene semantics. The advantages of such
a representation are three folds. 1)It enables learning 3D
from in-the-wild images without any knowledge of camera
poses by explicitly sampling camera views in constructed
3D worlds. 2)It supports a sufficient level of detail both in
geometry and semantics. 3)It represents a 3D scene with
quadratic space complexity, which enables efficient training
and 3D points sampling.
Furthermore, we introduce a novel semantic-aware neu-
ral hash grid for adversarial training to parameterize the
space-varied and scene-varied latent features. Different
from the hash grid as neural graphics primitives [15], we
propose a semantic-conditional hash grid that extends the
vanilla version into a generative setting by conditioning
on BEV scene representation. It enables our model to learn
generalizable latent features across scenes while preserving
impressive capacity and efficiency. The proposed generative
hash grid is semantic-aware, which further helps the model
to align content with 3D semantics (Challenge 2).
Lastly, we employ a style-based volumetric renderer [16]
to produce photorealistic images via adversarial training.
Given latent features sampled from the hash grid, the ren-
derer learns to blend them into realistic images by style-
modulated volume rendering [2], [17]. It guarantees the 3D
consistency of the generated images while also enablinglearning from in-the-wild 2D image collections (Challenge
3). Once training is done, we can unconditionally generate
diverse unbounded 3D scenes by sampling different simplex
noise and style codes.
Quantitative and qualitative experiments are performed
against various state-of-the-art 3D generative models,
demonstrating the capability of SceneDreamer in generat-
ing large-scale and diverse 3D scenes. In conclusion, we
summarize our contributions as follows: 1)To the best of
our knowledge, we are the first to achieve unbounded 3D
scene generation from in-the-wild 2D image collections; 2)
We propose a BEV scene representation for an expressive
scene modeling and efficient 3D GAN training; 3)We ex-
tend the neural hash grid parameterization into the gen-
erative setting for the first time; 4)We demonstrate useful
applications of SceneDreamer, including unconditional 3D
landscape generation, perpetual view generation [18], and
interpolation between different scenes and styles.
2 R ELATED WORK
Neural scene representation. Immense progress has been
witnessed in neural scene representations [15], [17], [20],
[21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31],
[32], [33], [34], [35], [36] , which can be optimized from 2D
multi-view images via differentiable neural rendering [17].
Implicit representations [17], [28], typically coordinate net-
works, employ large MLPs to model the 3D scenes. It
offers potential advantages in memory efficiency. However,
densely querying the network during training is slow, es-
pecially for large-scale unbounded scenes. Explicit repre-
sentations [31], [35], [37], i.e.,voxels, are inherently fast to
query. Yet, they are memory-intensive, which makes them
difficult to scale up and contain sufficient level of detail
in large-scale scenes. Hybrid representations [1], [11], [15],3
TABLE 1
Comparisons of Existing Methods on 3D Scene Generation. Our aim is to learn a feed-forward generative model of unbounded 3D scenes
from in-the-wild 2D image collections. “Training Data” denotes the source of training images; “Rendering Resolution” denotes the maximum
resolution of rendered images during inference. “Camera DOF” denotes the degree of freedom of camera movement at inference time, where
“XYZ” is for translation (x, y, z) and “RPY” is for rotation (row, pitch, yaw). “Unbounded” means whether the scene can be generated in arbitrary
scales. “3D consistent” indicates whether the renderings have 3D consistency. “Feed-Forward” denotes that the method does not need test-time
optimization to generate novel scenes.†Posed 3D data with image and camera pose pairs.
Methods Training Data Rendering Resolution Camera DOF Unbounded? 3D Consistent? Feed-Forward?
GANcraft [16] in-the-wild 2D images 1920×1080 XYZRPY ✗ ✓ ✗
EG3D [1] curated 2D images 512×512 RPY ✗ ✓ ✓
GSN [11]†3D trajectories 256×256 XYZRPY ✗ ✓ ✓
Inf-Nature [18]†3D trajectories 160×256 XYZ ✓ ✗ ✓
Inf-Zero [19] in-the-wild 2D images 512×512 XYZ ✓ ✗ ✓
Ours in-the-wild 2D images 3840×2160 XYZRPY ✓ ✓ ✓
[36], [38] combine the benefits of both explicit and implicit
representations to strike a balance between efficiency and
cost. In terms of balancing the trade-off, the proposed BEV
scene representation shares a common goal with them, but
ours is unique in its careful design. We explicitly model the
geometry and semantics of unbounded scenes within this
representation, which is key to efficient training and seman-
tic alignment of content for in-the-wild scene generation.
3D-aware GANs. Generative adversarial network (GAN)
[39] has been a great success in recent years, especially in
2D image generation [13], [40]. Extending the capability
of GANs to 3D space has emerged as well. A bunch of
work [41], [42], [43] intuitively extend the CNN backbone
used in 2D to 3D with a voxel-based representation. How-
ever, the prohibitively high computational and memory cost
of voxel grids and 3D convolution makes them difficult to
model unbounded 3D scenes. With recent advances in neu-
ral radiance field (NeRF) [17], many [1], [2], [3], [4], [7], [8],
[9], [10], [44], [45], [46], [47], [48] have incorporated volume
rendering as the key inductive bias to make GANs be 3D-
aware, which enables GANs to learn 3D representations
from 2D images. However, most of them are trained on
curated datasets for bounded scenes, e.g., human faces [13],
human bodies [12], and objects [14], where the occupancy of
the target in Euclidean space is limited and the camera view
captures a complete observation of the scene instead of a
partial one. The problem of unbounded 3D scene generation
from 2D images is still underexplored. As a concurrent
work, Persistent Nature [49] leverages persistent layout
grids to generate unbounded 3D worlds. Yet, the inefficiency
of latent layout grids limits the rendering resolution to
256×256. Meanwhile, InfiniCity [50] learns to generate 3D
urban scenes with the reliance on 3D CAD datasets.
Scene-level image synthesis. Different from impressive 2D
generative models which focus on a single category or com-
mon objects, generating scene-level content is a challenging
task, given the extremely high diversity of scenes. Semantic
image synthesis [16], [51], [52], [53] is the most promising
way for generating scene-level content in the wild. The
model is conditioned on pixel-wise dense correspondence,
e.g., semantic segmentation maps or depth maps, effectively
helping the model to find the bijective mapping between
input condition and output textures. It is worth mentioned
that some [16], [18], [19], [53], [54], [55] have even succeeded
in 3D-aware scene synthesis as well. However, they are
neither fully 3D consistent nor support feed-forward gen-eration on novel worlds. Another bunch of work [11], [56],
[57], [58] focuses on indoor scene synthesis using expensive
3D datasets [59], [60] or CAD retrieval [61], which strictly
limits the diversity of their results.
3 S CENE DREAMER
Our aim is to learn a feed-forward generative model for
unbounded 3D scenes from in-the-wild 2D image collections
only. We first derive our BEV scene representation (Sec. 3.1),
which consists of a height field and a semantic field, for
the aim of efficient 3D point sampling and training. Then,
we use a generative neural hash grid to parameterize the
hyperspace of space-varied and scene-varied latent features
(Sec. 3.2), learning generalizable 3D features across scenes.
Finally, a style-based generator is employed to blend latent
features of 3D points and render 2D images through volume
rendering (Sec. 3.3). An overview is shown in Fig. 2.
3.1 BEV Scene Representation
The learning of unbounded 3D scene GAN requires the
underlying 3D representation to be: 1)efficient, i.e.,we can
represent a 3D world with relatively low complexity to
enable tractable training; 2)expressive, i.e.,we can sample
3D points with corresponding semantics from the repre-
sentation for ease of semantic content alignment. Existing
representations [1], [2], [8], [10] are designed for object-
centric 3D GANs in bounded scenes, and cannot well fit the
large scale and diverse content for unbounded scenes. To
this end, we propose a novel bird’s-eye-view (BEV) scene
representation consisting of a height field and a semantic
field for large-scale scenes.
Height Field. The height field Mh:x− →h(x)represents
the level of height of surface points in a 3D scene ( x∈R3).
Intuitively, it is similar to the elevation map used in geogra-
phy, which records the distance of surfaces above zero level.
Given a level of detail D,i.e.,the resolution of a tiny voxel,
we can further discretize the height field into a height map
ˆMhwith a resolution of ND×ND×1. Note that, the spatial
resolution NDcan be any positive integer for unbounded
scenes and can vary given the budget of resources.
Semantic Field. The semantic field Ms:x− →li(x)rep-
resents the semantic label of surfaces in a 3D scene, where
i∈ {1, . . . , C s}andCsis the number of semantic classes
(e.g., rivers, mountains, grasslands). Similar to the height
map, we can derive a semantic map ˆMsfrom the semantic4
𝓏~𝑝scene𝓏style~𝑝style
Mapping
Volumetric
RendererEncoder 
𝐹𝜃𝑠
(Sec 3.1)
BEV Scene Representation(Sec 3.2)
Generative Hash Grid(Sec 3.3)
Unbounded 3D Scene GANSemantic field 𝑀𝑠
Height field 𝑀ℎ
Local scene window
Mod.
Camera ~𝑆𝐸(3)Semantic label𝑙(𝒙)
Arbitrary large -scale sceneScene feature
𝑙(𝒙)
Mod.
𝒙,𝑙(𝒙)Hyperspace𝒇s
𝒄,𝝈
DiscriminatorCond.Real
Fake𝒇𝒙Indexed 
feature
Fig. 2. Overview of SceneDreamer . Given a simplex noise z∼pscene and a style code zstyle∼pstyle as input, our model is capable of synthesizing
large-scale 3D scenes where the camera can move freely and get realistic renderings. We first derive our BEV scene representation which consists
of a height field and a semantic field. Then, we use a generative neural hash grid to parameterize the hyperspace of space-varied and scene-varied
latent features given scene semantics fsand 3D position x. Finally, a style-modulated renderer is employed to blend latent features fxand render
2D images via volume rendering.
field via discretization, with a resolution of ND×ND×Cs.
The semantic map stores the one-hot encoding of semantic
labels for corresponding surface points in the height map.
With such a representation, we can easily increase mem-
ory efficiency during training while preserving a satisfied
level of detail by constructing local 3D volumes. Given a
query of any 3D position x∈R3, we can first crop a local
scene window with a resolution of ND
w×ND
wfrom the
D-th level of ˆMhand ˆMs. Then, the local 3D voxels Vw
with a resolution of ND
w×ND
w×HD
ware constructed using
the 3D coordinates from height map and semantic labels
from the semantic map. The trade-off between memory and
capacity during volume rendering (Sec. 3.3) can be achieved
by tuning NDandND
w.
In addition, such a representation facilitates a feed-
forward design for large-scale scene generation, where the
input latent code z∼pscene can be transformed to the
scene representation through a mapping: z− →(Mh, Ms).
This mapping can be either parameterized by networks
or parameter-free. As for the former way, one can achieve
this by learning from satellite images or real terrain data,
which we leave for future work. Our instantiation of this
mapping is a parameter-free way, which enables us to scale
up the training data for free. Inspired by the fact that the
simplex noise can mimic patterns of nature [62], we let zbe
a randomized simplex noise as the input, and generate the
height field and the semantic field accordingly. Please refer
to Sec. 4 for implementation details.
3.2 Generative Neural Hash Grid
To enable generalizable 3D representation learning across
scenes and align content with 3D semantics, we need to
parameterize the scene representation into latent space forease of adversarial learning. Given the large scale of un-
bounded scenes and the fact that only the surface points
matter for appearance, it is a waste of capacity to model
the entire 3D space, e.g., using 3D convolutions [41] or tri-
plane [1]. Plus, the observation space involved in a camera
view is often a small portion of the unbounded scene, which
suggests that an ideal parameterization should be able to
activate the related subspace instead of the whole scene.
Inspired by the success of neural hash grid [15] in single-
scene reconstruction, we take advantage of this flexible yet
compact parameterization and extend it to learn general-
izable features across scenes by modeling the hyperspace
beyond 3D space.
The vanilla neural hash grid represents a hash-based
encoding function enc(x;θH) :R3− →RCH, which maps the
input xto a unique indexed feature with CHdimensions
in the trainable encoding parameters θH. We extend it to a
generative setting by introducing a hyperspace, aiming to
learn space-varied and scene-varied features. In specific, we
first encode a sampled scene (ˆMh,ˆMs), using an encoder
Fθswith trainable parameters θs:
fs=Fθs(ˆMh,ˆMs)∈Rd, (1)
where fsis the scene feature, which compactly represents
a 3D scene. Consider a space-varied and scene-varied latent
space, i.e.,given a 3D position x∈R3and a scene feature
fs∈Rd, we parameterize it into a hyperspace, i.e.,R3+d− →
RCH, using a neural hash function with trainable encoding
parameters θH∈RL×T×CH:
FθH(x,fs) =H(x,fs;θH), (2)
H(x,fs) =dM
i=1fi
sπi3M
j=1xjπj
mod T, (3)5
Fig. 3. Diverse samples of SceneDreamer . Our model can synthesize a large variety of 3D scenes with diverse styles, from winter to summer and
dawn to dusk. Please check the supplementary and project page for 3D consistent videos.
where ⊕denotes the bit-wise XOR operation and πi, πjare
large and unique prime numbers. Note that, we construct
Llevels multiresolution hash grids to represent multiscale
features, Tis the maximum number of entries per level,
andCHis the number of channels of each unique feature
vector. We set L= 16, T= 219, CH= 8. Specifically, the two
inputs (x,fs)are concatenated before feeding into the hash
grid. As the dimension of the scene feature is set to d= 2,
therefore the input to the hash grid is five-dimensional. The
unique prime numbers in Eq. 2 are set accordingly1.
3.3 Unbounded 3D Scene GAN in the Wild
In this section, we introduce our adversarial training frame-
work which learns to generate unbounded 3D scenes from
in-the-wild 2D image collections without camera parame-
ters. The key is the semantic alignment of content through-
out the whole pipeline.
Generator. The generator Gis a neural volumetric ren-
derer [17], defined as
G(z,zstyle,o,d; ΦG) =R(FΦG(z,zstyle),o,d), (4)
where z∼pscene is sampled from simplex noise (Sec. 3.1)
to represent different scenes, i.e.,z− →(ˆMh,ˆMs). The style
codezstyle∼pstyle is sampled from normal distribution
to represent different styles, e.g., different weathers and
illuminations within a given scene. And Ris a volumetric
renderer based on a conditional neural radiance field:
FΦG(z,zstyle|x∈R3) ={c(fx,zstyle, l(x)),σ(fx)},(5)
where cis color and σis volume density. Given a 3D
position xin the local volume Vw, we can obtain its semantic
label l(x)by sampling from ˆMs, and the indexed latent
feature fxvia Eq. 2. As shown in Fig. 2, the renderer
1.π1= 1 , π2= 2654435761 , π3= 805459861 , π4=
3674653429 , π5= 2097192037 . The first prime number is set to 1 for
better cache coherence while keeping pseudo-independence [63] of the
dimensions on the hashed value.takes fxas input, and is modulated by l(x)and style
codezstyle. Then, the output image is generated via volume
rendering. Given a sampled camera view with origin at o
and view direction from d, we cast a ray r(t) =o+td, the
corresponding pixel value C(r)is obtained by an integral:
C(r) =Z∞
0T(t)c(fr(t),zstyle, l(r(t)))σ(fr(t))dt, (6)
where T(t) = exp( −Rt
0σ(fr(s))ds).
Discriminator. To reinforce the semantic alignment of con-
tent, we employ a semantic-aware discriminator [64] to
discriminate between real and fake images. For each per-
spective camera view at (o,d), we obtain the segmentation
mapSfof the generated images If=G(z,zstyle,o,d; ΦG)
by accumulating semantic labels l(x)sampled from the se-
mantic field along each ray. Then, the fake pairs (If,Sf)and
the real pairs (Ir,Sr)are fed into discriminator D(I|S; ΦD)
for adversarial training.
4 I MPLEMENTATION DETAILS
4.1 BEV Scene Representation Generation
As introduced in Sec. 3.1, the feed-forward mapping from
random noise zto the BEV scene representation, i.e.,z− →
(Mh, Ms), can be either learned from data or parameter-free.
Our instantiation is a parameter-free way via procedural
generation, which enables us to scale up the training data
for free. We let zbe a simplex noise [62], [65] which is
commonly used to generate continuous and infinite terrains
in computer graphics, and generate the height field Mhand
the semantic field Mswith any resolution respectively.
Simplex noise [65] is an n-dimensional noise function
based on gradient grids, which is an improved version of
Perlin noise [62] that has lower computational complexity
and fewer directional artifacts. Specifically, we consider z
to be a 2D simplex noise in SceneDreamer, i.e.,n= 2. The
procedure to generate the height field and the semantic field
is presented in Fig. 5.6
Correspondence
 Sliding Local Scene Window
 Camera Trajectory
Fig. 4. Sliding window mechanism to generate unbounded scenes beyond training resolution . Given a scene with its BEV scene represen-
tation size of 10240×10240 , we first generate the BEV maps for the entire world, then bind the local scene window (highlighted rectangles) to the
camera position (orange). Given a fly-through camera trajectory, the local scene window slides accordingly to render coherent frames (bottom).
Lloyd’s relaxation on Voronoi diagramBezier Interpolation
Semantic Map ෡𝑴𝒔
 Height Map ෡𝑴𝒉
Fine ෡𝑀ℎ1
Coarse ෡𝑀ℎ2
Temp. Map ෡𝑀𝑡 Prec. Map ෡𝑀𝑝Query
Fig. 5. Procedural Generation of BEV Scene Representation . The
feed-forward mapping from random noise zto the BEV scene repre-
sentation (Sec. 3.1), i.e.,z− →(Mh, Ms), can be either learned from
data or parameter-free. Our instantiation starts from 2D simplex noises
(highlighted in orange background ). Please refer to Sec. 4.1 for details.
Height Field Generation. As for the height field Mh, we
first sample two 2D simplex noise maps with a fixed random
seed yet different levels of the octave as ˆM1
h,ˆM2
h. The level
of the octave controls the fine-grained details in the noise
map, which leads to a smoother appearance with a lower
level of the octave. One can observe that a higher level of
the octave leads to sufficient details but noisy heights while
a lower level of the octave results in flat and smooth terrain
but lacks diversity. To strike a balance in between, obtaining
smooth transitions across different types of landscape, we
apply bezier-based interpolation between ˆM1
h,ˆM2
hto get the
final height map ˆMh. Note that, the height field Mhis the
continuous form of height map ˆMh, which is approximated
by increasing the resolution of the height map.
Too dark Nearly in grayscale
Uncommon ratio
𝐻
𝑊𝑊
𝐻>4Fig. 6. Dataset Preprocessing . We apply rule-based filters to remove
images that are too dark, nearly in grayscale, and with uncommon ratios.
The filters are highlighted in red box. We remove dark images whose
values in HSV color space are lower than the threshold. Grayscale
images are identified by computing average grayscale in RGB color
space, and filtered based on the L2 distance to the gray axis.
Semantic Field Generation. As for the semantic field Ms,
we derive it from another two 2D simplex noise maps with
intermediate outputs as temperature map ˆMtand precipi-
tation map ˆMprespectively. Specifically, we sample two 2D
simplex noise maps with varied seeds (to ensure they have
different patterns) and the same level of the octave. The first
noise map is termed as temperature map ˆMtwhich denotes
the spatially varied distribution of the temperature. The
second noise map is termed as precipitation map ˆMpwhich
represents the level of precipitation. We are inspired by
geography that temperature and precipitation are two main
factors that affect the semantics of landscapes. Therefore, on
top of ˆMt,ˆMp, we can define the semantics of the landscape
using a 2D lookup table ILUT, which is an image with
a resolution of Nt×Npthat stores the corresponding se-
mantic label given a certain temperature and precipitation.
NtandNpare the range resolution of the temperature
and precipitation respectively, both of which we set to
256 in our implementation. Totally, there are 9 different7
Fig. 7. RGB renderings and corresponding depth maps of SceneDreamer . Our method can synthesize different 3D landscapes in different
styles with free camera trajectories, where both the appearance and geometry are of promising quality.
landscapes in ILUT, including desert, savanna, woodland,
tundra, seasonal forest, rain forest, taiga, temperate forest,
and grassland.
Label Regularization. For our semantic-conditioned GAN
training framework, we introduce a new set of 12 classes
of semantic labels as a higher level of abstraction: sky, tree,
dirt, flower, grass, gravel, water, rock, stone, sand, snow,
and others. For instance, the desert landscape would be full
of sand labels, the tundra landscape consists of a mixture
of grass, dirt, rock, and stone, and the region with negative
height is treated as water. To prevent noisy edges and unre-
alistic holes within different landscapes, we use the Voronoi
diagram followed by Lloyd’s relaxation to regularize the
semantic map ˆMs. Specifically, the semantic label of each
point is quantized to most of the semantic labels within a
Voronoi cell. This regularization ensures the consistency and
smoothness of the semantic field.
4.2 Training
Camera pose sampling. Different from existing 3D-aware
generative models [1], [2], [4], which either assume a camera
pose distribution or estimate the camera pose using off-the-
shelf tools, our framework supports freely sampling camera
pose with 6 DoF. By constructing the local scene window
Vw, one can explicitly sample perspective camera in SE(3) .
To further improve the sampling efficiency during training,
we employ rejection sampling technique [16] which rejects
any camera pose whose rendering produces a low mean
depth or a low entropy of semantic labels.
End-to-end learning. SceneDreamer is directly learned on
in-the-wild 2D images collections in an end-to-end manner.For ease of semantic alignment, besides the semantic-aware
discriminator, we use off-the-shelf models [51] for semantic
image synthesis to generate pseudo ground truth [16] from
Sfas the reconstruction target. We train our model with
a hybrid objective which consists of a reconstruction loss
and an adversarial learning loss. In specific, we leverage a
combination of GAN loss [66], mean squared error (MSE)
loss, and perceptual loss [67], with their weights of 0.5, 10.0,
and 10.0, respectively.
During training, we randomly sample scenes with the
level of detail ND= 2048 . The ray casting and 3D
points sampling is executed within the local scene window
which has a resolution of ND
w×ND
w×HD
w. We choose
ND
w= 1024 , HD
w= 256 in our implementation. In volume
rendering, the resolution of generated images is 256×256
with 24 points sampled along each camera ray. Totally,
we have {θs, θH,ΦG,ΦD}as trainable parameters in our
model, which are the parameter of the scene encoder, the
generative hash grid, the volumetric renderer, and the dis-
criminator, respectively. We adopt Adam [68] optimizer with
β= (0,0.999) for training. We use a learning rate of 5×10−4
forθs,1×10−4forθHandΦG,4×10−4forΦDrespectively.
Our model is trained on 8 NVIDIA A100 GPUs for 600k
iterations with a batch size of 8, which takes approximately
80 hours to converge. The scene encoder Fθsis implemented
as a convolutional network, and the dimension of the scene
feature fsis set to d= 2.
4.3 Inference
Unbounded scene generation. Despite training on a fixed
size of local scene window, our model allows a flexible8
(a)  GANcraft (b)  EG3D (d)  Ours
 (c)  Inf-Zero
N/A
Fig. 8. Qualitative comparison . Both the 2D renderings and 3D depths (last column) generated by ours achieve the best visual quality. (a)
GANcraft [16]. (b) EG3D [1]. (c) Inf-Zero [19]. (d) Ours.
spatial size of the entire scene, enabling generating scenes
in arbitrary scales. To create scenes in an unbounded do-
main, we employ our model in a sliding-window fashion,
as illustrated in Fig. 4, taking a scene ( ND= 10240 ) ten
times larger than the training scene size as an example. We
initially generate the BEV scene representation of the entire
world, then bind the local scene window to the camera
position. Note that, fsis the scene feature encoded by the
scene encoder Fθson the entire BEV maps, which is already
seamless prior to feeding in volumetric rendered. Given a
camera trajectory to be rendered, the local scene window
slides according to the camera positions, extracting the scene
feature fsfor each frame. For each queried 3D point during
rendering, we take its global normalized coordinate instead
of the coordinate within the local scene window as the 3D
position x. Subsequently, the camera views are rendered
with continuously updated (x,fs)along the trajectory.
High-resolution renderings. Thanks to our expressive and
compact 3D representation, our framework can render im-
ages much higher than training during testing time. We can
render images beyond the training resolution by specifying
the camera intrinsic and the level of detail ND. For example,
renderings with a resolution of 1920×1080 are presented in
Fig. 7. Furthermore, fly-through videos in 4K ( 3840×2160 )
are shown in the supplementary video. Approximately, ren-
dering a frame with a resolution of 960×540 on an A100
GPU takes 2 seconds.
5 E XPERIMENTS
5.1 Datasets
To enable learning from rich and diverse in-the-wild scene-
level content, we collect a large-scale dataset of 1,135,662TABLE 2
Quantitative results on unbounded 3D scene generation. The top
three techniques are highlighted in red, orange, and yellow,
respectively.†Methods adapted to fit our setting.
Methods FID ↓ KID↓ Depth ↓ CE↓ SfM rate ↑
†GANcraft [16] 93.73 4.82 0.464 0.058 0.450
†EG3D [1] 103.86 6.20 0.993 1.178 0.475
Inf-Nature [18] - - - 1.555 0.380
Inf-Zero [19] 79.99 4.51 - 1.213 0.250
Ours 76.73 4.52 0.277 0.021 0.935
natural images from the Internet2with keywords as “land-
scape”. However, the raw dataset is too noisy to support
scene-level 3D-aware adversarial learning. Therefore, we
use a rule-based filter to remove images that are too dark,
with uncommon ratios, or in grayscale. The details of data
preprocessing are presented in Fig. 6.
The short side of images is rescaled to 896 while keeping
the original ratio. To obtain real pairs (Ir,Sr)for the dis-
criminator, we extract the corresponding segmentation map
for each image using ViT-Adapter3[69] for adversarial train-
ing. We randomly set apart 15,000 images for evaluation.
5.2 Evaluation Protocols
During the evaluation, we sample 1024 different scenes by
randomizing the simplex noise z. For each scene, we sample
20 different styles by randomizing the style code zstyle. Each
sample is a fly-through video with 40 frames (with a resolu-
tion of 960×540) with any possible camera trajectory. Then,
we randomly select frames from those video sequences for
evaluation. The evaluation metrics are listed as follows.
2. https://flickr.com/
3. https://github.com/czczup/ViT-Adapter9
(a)  GANcraft (b)  EG3D (c)  Inf-Nature (d)  Inf-Zero (e)  Ours
Fig. 9. Visualization of camera trajectory for CE and SfM rate evaluation . The red trajectory is the ground truth camera trajectory defined as
Eq. 7. And the green trajectory is successfully recovered camera poses by each method using structure-from-motion (SfM). The camera error (CE)
is computed as the scale-invariant L2 distance between estimated cameras and ground truth. SfM rate is defined as the proportion of successfully
recovered camera poses relative to the total number of frames. (a) GANcraft [16]. (b) EG3D [1]. (c) Inf-Nature [18]. (d) Inf-Zero [19]. (e) Ours.
FID and KID. Fr´echet Inception Distance (FID) [70] and
Kernel Inception Distance (KID) [71] are metrics for the
quality of generated images. We use the publicly available
tool4to compute FID and KID between 15,000 generated
frames and our evaluation set.
Depth. We follow a similar practice in EG3D [1] for the
evaluation of 3D geometry. We use a pre-trained model5
[72] for monocular depth estimation to generate a pseudo
ground truth depth map for each generated frame. The
predicted depth map is generated in volume rendering by
accumulating density σ. Then, the “ground truth” depth
and the predicted depth are normalized to zero mean and
unit variance to remove scale ambiguity. Finally, the depth
error is computed as the L2 distance between the two. We
compute depth error on 100 frames for each method.
CE and SfM rate. The camera error (CE) is raised to
evaluate the multi-view consistency of renderings by com-
puting the camera position error between the inference
camera trajectory and the estimated camera trajectory by
a good structure-from-motion (SfM) algorithm. We use
COLMAP [73] to perform SfM to estimate the camera pose
from the rendered sequence. Note that estimation errors of
the camera pose occur when a video sequence is inconsistent
in a 3D space, and some frames with severe pose errors
are discarded by SfM due to incoherent 3D corresponding
points. We choose a circular trajectory (shown in the sup-
plementary video) for computing CE. Specifically, given a
local scene window with a resolution of ND
w×ND
w×HD
w,
the camera position is sampled from the circle defined as
(x, y, z ) s.t.(
(x−0.5ND
w)2+ (y−0.5ND
w)2= (0.4ND
w)2
z= 0.2HD
w.
(7)
The rotation of the camera is set to keep the camera look-
ing at the center. This ground truth camera trajectory is
showcased in Fig. 9. The camera error (CE) is computed as
the scale-invariant normalized L2 distance between recon-
structed camera poses and generated poses. Moreover, we
select the success rate of structure-from-motion (SfM rate)
as an additional measure of 3D consistency, defined as the
proportion of successfully recovered camera poses relative
to the total frame count.
4. https://github.com/toshas/torch-fidelity
5. https://github.com/isl-org/MiDaS5.3 Comparisons
We compare SceneDreamer against four methods:
GANcraft [16] is a 3D world-to-world translation model
that aims to synthesize multi-view-consistent photorealistic
images given 3D semantic voxels. It needs to train on a per-
world basis for 4 days with 8 V100 GPUs, which means that
it cannot generalize across scenes and does not support feed-
forward scene generation. That is, given a Minecraft world
of a new scene, GANcraft has to re-train the 3D translation
model again. For a fair comparison that enables GANcraft
to get rid of per-scene optimization, we need to modify
the official code6. In specific, to implement the adapted
GANcraft (Table 2), we replace the voxel-aligned features
with a feature codebook conditioned on the semantic label
of the voxel. It enables the adapted baseline to optimize
scene-agnostic latent features that generalize across scenes.
EG3D [1] is a 3D-aware GAN for faces that incorporates
a hybrid tri-plane representation to enable efficient GAN
training. As it requires an estimated camera pose distribu-
tion as input, which does not fit our setting (unbounded
scenes and free camera poses), we adapt the official code7
and only utilize the tri-plane representation. Specifically, we
use random noise vector as scene feature fsand replace the
generative hash grid with a StyleGAN2 [40] backbone which
outputs tri-planes. The latent feature fxis computed from
tri-planes via interpolation. Both the rendering and training
strategies are kept the same as ours.
Inf-Zero [19] aims to synthesize perpetual views given
a single image as input. It enables learning from in-the-
wild images for 3D-aware scene-level synthesis, but does
not ensure the 3D consistency and semantic consistency of
foreground contents due to the lack of 3D representation.
We adapt the officially released code8.
Inf-Nature [18] addresses the same problem as Inf-Zero, but
is trained on camera trajectories with known camera poses.
Therefore, it is impossible to retrain in our setting (using in-
the-wild 2D images without known camera poses), which
makes it unfair to compare visual quality. Thus, we only
compare CE and SfM rate to evaluate 3D consistency.
6. https://github.com/NVlabs/imaginaire
7. https://github.com/NVlabs/eg3d
8. https://github.com/google-research/google-research/tree/
master/infinite nature zero10
(d)  Full Model (a)  w/o BEV
 (b)  w/o Semantic
 (c)  w/o Height
Fig. 10. Ablation study of BEV scene representation . Without this
representation, our model will produce dull outputs due to the lack of
global semantics, which is crucial for the scene parameters to learn vivid
and detailed appearance across different scenes. (a) w/o BEV: trained
with no scene encoder. (b) w/o Semantic: trained with no semantic field
as input. (c) w/o Height: trained with no height field as input. (d) Ours.
Qualitative Results. Fig. 8 provides qualitative comparisons
against baselines. While GANcraft synthesizes high-quality
images, reliance on coarse voxels produces blocky geometry.
Plus, the way of parameterizing large-scale scenes using
voxel-aligned features prohibits its generalizability across
different scenes, resulting in artifacts. The adapted EG3D
struggles with realistic renderings due to the incapability
of tri-plane for parameterizing large-scale space. Inf-Zero
cannot synthesize high-resolution content with 3D consis-
tency. Without any 3D representation, it cannot produce
depth maps along with the renderings (shown as N/A).
Our method generates not only images that are more view-
consistent and realistic across different scenes but also plau-
sible 3D depth. Please refer to the supplementary video for
more visual results and comparisons.
Quantitative Results. Table 2 presents quantitative metrics
of the proposed approach against baselines. Our method
demonstrates significant improvements in FID and KID,
which is consistent with visual comparisons. Although Inf-
Zero achieves comparable image quality as ours, it is ex-
tremely 3D inconsistent due to the lack of 3D representation
(indicated by the high CE and low SfM rate). High CE
and low SfM rate are also observed for Inf-Nature, which
validates the vital role of incorporating 3D representation
in 3D scene generation. In contrast, SceneDreamer shows
the ability to keep the correctness of 3D geometry and view
consistency when producing photorealistic images, which is
indicated by the lowest error in depth and CE compared
with baselines. Furthermore, we visualize the camera trajec-
tory for evaluating CE and SfM rate in Fig. 9. It is obvious
that our method succeeds in recovering most of the camera
poses with low errors.
5.4 Ablation Study
Effectiveness of BEV scene representation. Without BEV
scene representation, the model will not have a sense of
the global semantics of the entire scene within the localTABLE 3
Effectiveness of BEV scene representation. The top three
techniques are highlighted in red, orange, and yellow, respectively.
Methods FID ↓ KID↓ Depth ↓ CE↓ SfM rate ↑
w/o BEV 80.62 4.68 0.314 0.044 0.930
w/o semantic 83.20 4.77 0.401 0.046 0.910
w/o height 79.80 4.61 0.356 0.051 0.890
Ours 76.73 4.52 0.277 0.021 0.935
TABLE 4
Memory efficiency of BEV scene representation.
Methods Max. Supported LOD ↑ FID↓ KID↓
Voxels 512×512×256 - -
Semantic Voxels 512×512×256 93.73 4.82
BEV ( Ours ) 2048×2048×256 76.73 4.52
scene window. We measure this effect by training our model
without the scene encoder Fθsand scene feature fs. Quanti-
tative results are presented in Table 3, where both the image
quality and 3D consistency are dropped. In addition, as
shown in Fig. 10, the global semantics is key to ensuring
vivid and realistic results by our full model (bottom row).
Without this representation fed into the hash grid, the
model will produce dull and unrealistic outputs (top row).
Moreover, the effect of the semantic field and height field
is ablated respectively. For example, “w/o semantic” means
the model trained without semantic field as input to the
scene encoder Fθs. Both of them benefit the visual quality,
leading to improvements in FID and KID.
Besides, we report the efficiency of BEV scene represen-
tation in Table 4. With a set GPU memory budget of 40GB,
we record the highest achievable level of detail (LOD),
which corresponds to the maximum size of the local scene
window. The two alternatives, ”Voxels” and ”Semantic Vox-
els,” explicitly represent the scene as occupied 3D voxels,
exhibiting cubic complexity. The “Voxels” baseline stores
per-voxel latent features which cannot generalize across
scenes while “Semantic Voxels” is the representation used in
the adapted GANcraft baseline. The quadratic complexity of
BEV scene representation significantly boosts our capacity
in modeling larger scenes.
Effectiveness of Generative Hash Grid. We argue that
scene parameterization is also critical to the success of
unbounded 3D scene generation. We ablate different ways
of parameterization, i.e.,how to implement FθH: (x,fs)− →
fx. We illustrated these implementations in Fig. 12. We
compare all five variants with our generative neural hash
grid, presenting the results in Table 5. None of the methods
succeeds in learning to synthesize unbounded 3D scenes
from 2D images, indicated by poor FID and depth metrics.
Furthermore, qualitative results are shown in Fig. 11, where
our full model synthesizes vivid and appealing images.
Using pure MLP leads to blocky artifacts while applying
positional encoding [17] produces sinusoidal patterns on
the scene surface, as shown in Fig. 11(a, b). “HyperNet”
and “Modulation” baselines fail to converge, yielding dull
appearances. And tri-plane-based method struggles to syn-
thesize meaningful foreground textures. We attribute the
failure of these baselines to the incapability of sparsely and
compactly parameterizing the scene. In terms of unbounded11
(f)  Ours
 (e)  Tri -plane (c)  HyperNet
 (a)  Pure MLP
 (b)  MLP w/ enc
 (d)  Modulation
Fig. 11. Ablation study of different generative scene parameteri-
zation . Instead of the generative hash grid in our model, the function
FθH: (x,fs)− →fxcan be replaced with other parameterization
techniques. Please refer to Fig. 12 for details of each method.
𝒙𝒙𝒇𝒇𝒙𝒙𝒇𝒇s
𝒇𝒇s
𝒙𝒙𝒇𝒇𝒙𝒙𝒙𝒙𝒇𝒇𝒙𝒙𝒇𝒇s
Mod.𝒙𝒙𝒇𝒇s
𝒇𝒇𝒙𝒙 𝒙𝒙𝒇𝒇s
𝒇𝒇𝒙𝒙
Positional Encoding𝒙𝒙𝒇𝒇s
𝒇𝒇𝒙𝒙
(a) Pure MLP (b) MLP w/ enc (c) HyperNet
(d) Modulation (e) Tri -plane (f) OursStyleGAN2Hash Grid Hash Grid
Fig. 12. Different design choices of generative scene parameteriza-
tion, i.e.,FθH: (x,fs)− →fx.
scenes, the observation space (image captured by camera
view) always occupies a small portion of the entire scene.
Thus, the parameterization should be sparsely activated,
i.e.,only the parameter space that is related to the rendered
view is sampled. In contrast, these methods model the scene
as a whole, injecting instabilities in adversarial training.
Effectiveness of Camera Sampling Strategy. We report the
effects of different camera pose sampling strategies during
training in Table 6. Specifically, “Random” refers to theTABLE 5
Effectiveness of different generative scene parameterization. The
top three techniques are highlighted in red, orange, and yellow.
Methods FID ↓ KID↓ Depth ↓ CE↓ SfM rate ↑
Pure MLP 88.76 5.07 0.433 0.194 0.650
MLP w/ enc 83.14 4.70 0.522 0.106 0.815
HyperNet 117.64 6.07 0.781 1.032 0.525
Modulation 120.88 6.94 0.994 1.331 0.325
Tri-plane 103.86 6.20 0.993 1.178 0.475
Ours 76.73 4.52 0.277 0.021 0.935
(a)  Random (b)  Area (c)  Ours
Fig. 13. Ablation study of different camera sampling strategies
during training . (a) Random: sample camera pose in SE(3) . (b) Area:
assume cameras are distributed within a height interval. (c) Ours: rejec-
tion sampling based on mean depth and entropy of camera view.
naive strategy of sampling camera poses in SE(3) , without
any constraint and assumption. “Area” indicates a sampling
technique that assumes the training cameras are distributed
within a height interval, but no rejection sampling is lever-
aged. Our rejection sampling strategy notably enhances
rendering quality, as evidenced by the lowest FID and KID
scores, as well as improved geometry correctness. Moreover,
it is apparent that the two baseline sampling strategies result
in blurry artifacts and washed-out patterns, as shown in
Fig. 13. Our sampling approach eliminates out camera views
with low mean depth or low entropy of semantic labels both
of which lead to the low diversity of visible semantic labels
in image patches. Consequently, this benefits adversarial
training as the distribution of training patches should be
as semantically rich as real images, validated by the vivid
appearance and low FID of our method.
5.5 Further Analysis
User Study. To better evaluate the 3D consistency and
quality of unbounded 3D scene generation, we conduct an
output evaluation [74] as the user study, which is a survey
that involves asking participants to rate alternatives. For
each generated camera trajectory, a total of 20 volunteers
are asked to score: 1)the perceptual quality of the imagery,
2)the degree of 3D realism, and 3)3D view consistency.
All scores are in the range of 5, and 5 indicates the best.
The results are presented in Fig. 15. The proposed method
outperforms baselines by a large margin.
View Consistency. To demonstrate the multi-view-
consistent renderings of SceneDreamer, we employ
COLMAP [73], [75] to perform structure-from-motion and12
Fixed Style, Scene Interpolation
Scene & Style Interpolation
Fixed Scene, Style Interpolation
Fig. 14. Interpolation along two different dimensions, scene and style, which are controlled by zandzstyle respectively.
TABLE 6
Ablation of different camera sampling strategy.
Methods FID ↓ KID↓ Depth ↓ CE↓ SfM rate ↑
Random 102.60 6.30 0.456 0.093 0.833
Area 96.90 6.13 0.429 0.094 0.865
Ours 76.73 4.52 0.277 0.021 0.935
Fig. 15. User study on unbounded 3D scene generation . All scores
are in the range of 5, and 5 indicates the best.
dense reconstruction from a synthesized video sequence. We
render a video sequence of 600 frames (with a resolution of
960×540), taken from a circle camera trajectory orbiting in
the scene given a fixed height and looking at the center (like
the sequence we show in the supplementary video). Only
images are used for reconstruction, even without specifying
camera parameters. As shown in Fig. 16, the estimated
camera poses exactly match our sampled trajectory, and the
resulting point cloud is well-defined and dense. Note that,
comparison methods failed during dense reconstruction.
It further validates that SceneDreamer can generate 3D
consistent large-scale scenes.
Structure from motion & Dense reconstruction
Corresponding rendered v iew Reconstructed Poisson meshCamera poses
Fig. 16. COLMAP [73], [75] reconstruction of a 600-frame synthe-
sized video which followed a circle trajectory . The estimated camera
poses (red) and well-defined point clouds demonstrate highly multi-view-
consistent renderings of SceneDreamer.
5.6 More Applications
Large-scale Landscape Generation. The strong capability of
our model in generating large-scale landscapes is validated
in Fig. 7. SceneDreamer can synthesize diverse scenes with
different styles and well-defined depths, where the camera
can freely move.
Perpetual View Generation [18]. Given a single RGB im-
age, the goal is to synthesize a video depicting a scene
captured from a forward flying camera with an arbitrary
long trajectory. Though the input is different, SceneDreamer
can also do the task of generating videos of infinite 3D
scenes unconditionally, by sliding the local scene window
on an arbitrary large semantic map (Fig. 4). Please refer
to the supplementary for our generated video with long13
a) Text-driven 3D Scene Stylization - “a fantasy land”
b) 3D Urban Scene Generation
Fig. 17. Scalability of SceneDreamer to other scene generative
tasks. We demonstrate the scalability of our work in two ways: a)text-
driven 3D scene stylization and b)3D urban scene generation. We only
change the training data and configuration to train these models without
significant modifications to our framework.
trajectories.
Interpolation. As shown in Fig. 14, we are able to in-
terpolate along two different dimensions, scene and style,
which are controlled by zandzstyle respectively. The first
row demonstrates a linear interpolation between two style
codes to generate a smooth transitional path within the
latent space, highlighting that the learned style space is
semantically meaningful. The second row shows that we
can interpolate between scenes to smoothly change the
scene configuration while keeping the style unchanged. The
bottom row showcases when the scene code and style code
are interpolated simultaneously.
Scalability to other generative tasks for 3D scenes. As
shown in Fig. 17, we can easily extend SceneDreamer to
other large-scale scene generative tasks with promising re-
sults by changing the training data and configuration only.
For text-driven 3D scene stylization, as shown in Fig. 17(a),
we fine-tune our model on the images generated by Con-
trolNet [76]. For urban scene generation shown in Fig. 17(b),
we train our model on image collections of urban scenery.
We hope this will shed light on future work regarding 3D
generative models for unbounded scenes.
6 D ISCUSSION
To conclude, we present SceneDreamer for unbounded 3D
scene generation, which aims to synthesize large-scale 3D
landscapes. We propose an efficient yet expressive BEV
scene representation to model large-scale 3D scenes. Be-
sides, we propose a novel generative neural hash grid to
parameterize space-varied and scene-varied latent features,
aiming to learn compact and generalizable 3D representa-
tion across scenes. By learning from in-the-wild 2D image
collections with a volumetric renderer, our method takes
significant steps toward generating scene-level 3D content
without 3D annotations. We hope this may inspire future
work in scene understanding and 3D-aware synthesis.
Limitations. 1) Our BEV scene representation assumes the
scene surface to be convex, which indicates that concave
geometry, such as caves and tunnels, cannot be modeled
and generated. 2)Though our model does not require
prior knowledge of the camera pose, further explorations
are needed to increase the robustness of camera sampling
during training. A naively random strategy sometimes leadsto model collapse. 3)Our model requires millions of training
images and takes several days to converge. Future work on
reducing the training cost would be fruitful.
ACKNOWLEDGMENTS
This work is supported by the National Research Founda-
tion, Singapore under its AI Singapore Programme (AISG
Award No: AISG2-PhD-2021-08-019), NTU NAP , MOE
AcRF Tier 1 (RG13/21), MOE AcRF Tier 2 (T2EP20221-0012),
and under the RIE2020 Industry Alignment Fund - Industry
Collaboration Projects (IAF-ICP) Funding Initiative, as well
as cash and in-kind contribution from the industry part-
ner(s).
REFERENCES
[1] E. R. Chan, C. Z. Lin, M. A. Chan, K. Nagano, B. Pan, S. D.
Mello, O. Gallo, L. Guibas, J. Tremblay, S. Khamis, T. Karras, and
G. Wetzstein, “Efficient geometry-aware 3D generative adversarial
networks,” in CVPR , 2022.
[2] J. Gu, L. Liu, P . Wang, and C. Theobalt, “Stylenerf: A style-
based 3d aware generator for high-resolution image synthesis,”
inInternational Conference on Learning Representations , 2022.
[3] R. Or-El, X. Luo, M. Shan, E. Shechtman, J. J. Park, and
I. Kemelmacher-Shlizerman, “StyleSDF: High-Resolution 3D-
Consistent Image and Geometry Generation,” in Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR) , June 2022, pp. 13 503–13 513.
[4] F. Hong, Z. Chen, Y. Lan, L. Pan, and Z. Liu, “Eva3d: Compo-
sitional 3d human generation from 2d image collections,” arXiv
preprint arXiv:2210.04888 , 2022.
[5] Y. Xue, Y. Li, K. K. Singh, and Y. J. Lee, “Giraffe hd: A high-
resolution 3d-aware generative model,” in CVPR , 2022.
[6] J. Gao, T. Shen, Z. Wang, W. Chen, K. Yin, D. Li, O. Litany,
Z. Gojcic, and S. Fidler, “Get3d: A generative model of high quality
3d textured shapes learned from images,” in Advances In Neural
Information Processing Systems , 2022.
[7] E. R. Chan, M. Monteiro, P . Kellnhofer, J. Wu, and G. Wetzstein,
“pi-GAN: Periodic Implicit Generative Adversarial Networks for
3D-Aware Image Synthesis,” arXiv:2012.00926 [cs] , Apr. 2021,
arXiv: 2012.00926. [Online]. Available: http://arxiv.org/abs/2012.
00926
[8] Y. Deng, J. Yang, J. Xiang, and X. Tong, “Gram: Generative ra-
diance manifolds for 3d-aware image generation,” in IEEE/CVF
Conference on Computer Vision and Pattern Recognition , 2022.
[9] J. Xiang, J. Yang, Y. Deng, and X. Tong, “GRAM-HD: 3D-
Consistent Image Generation at High Resolution with Generative
Radiance Manifolds,” arXiv, Tech. Rep. arXiv:2206.07255, Jun.
2022, arXiv:2206.07255 [cs] type: article. [Online]. Available:
http://arxiv.org/abs/2206.07255
[10] K. Schwarz, A. Sauer, M. Niemeyer, Y. Liao, and A. Geiger, “Vox-
graf: Fast 3d-aware image synthesis with sparse voxel grids,” in
Advances in Neural Information Processing Systems (NeurIPS) , 2022.
[11] T. DeVries, M. A. Bautista, N. Srivastava, G. W. Taylor,
and J. M. Susskind, “Unconstrained Scene Generation with
Locally Conditioned Radiance Fields,” in 2021 IEEE/CVF
International Conference on Computer Vision (ICCV) . Montreal, QC,
Canada: IEEE, Oct. 2021, pp. 14 284–14 293. [Online]. Available:
https://ieeexplore.ieee.org/document/9710863/
[12] Z. Liu, P . Luo, S. Qiu, X. Wang, and X. Tang, “Deepfashion: Power-
ing robust clothes recognition and retrieval with rich annotations,”
inProceedings of IEEE Conference on Computer Vision and Pattern
Recognition (CVPR) , June 2016.
[13] T. Karras, S. Laine, and T. Aila, “A Style-Based
Generator Architecture for Generative Adversarial Networks,”
arXiv:1812.04948 [cs, stat] , Mar. 2019, arXiv: 1812.04948. [Online].
Available: http://arxiv.org/abs/1812.04948
[14] A. X. Chang, T. Funkhouser, L. Guibas, P . Hanrahan, Q. Huang,
Z. Li, S. Savarese, M. Savva, S. Song, H. Su, J. Xiao, L. Yi, and F. Yu,
“ShapeNet: An Information-Rich 3D Model Repository,” Stanford
University — Princeton University — Toyota Technological Insti-
tute at Chicago, Tech. Rep. arXiv:1512.03012 [cs.GR], 2015.14
[15] T. M ¨uller, A. Evans, C. Schied, and A. Keller, “Instant neural
graphics primitives with a multiresolution hash encoding,” ACM
Trans. Graph. , vol. 41, no. 4, pp. 102:1–102:15, Jul. 2022. [Online].
Available: https://doi.org/10.1145/3528223.3530127
[16] Z. Hao, A. Mallya, S. Belongie, and M.-Y. Liu, “GANcraft:
Unsupervised 3D Neural Rendering of Minecraft Worlds,” in
2021 IEEE/CVF International Conference on Computer Vision (ICCV) .
Montreal, QC, Canada: IEEE, Oct. 2021, pp. 14 052–14 062. [On-
line]. Available: https://ieeexplore.ieee.org/document/9710945/
[17] B. Mildenhall, P . P . Srinivasan, M. Tancik, J. T. Barron, R. Ra-
mamoorthi, and R. Ng, “Nerf: Representing scenes as neural
radiance fields for view synthesis,” in ECCV , 2020.
[18] A. Liu, R. Tucker, V . Jampani, A. Makadia, N. Snavely, and
A. Kanazawa, “Infinite nature: Perpetual view generation of nat-
ural scenes from a single image,” in Proceedings of the IEEE/CVF
International Conference on Computer Vision (ICCV) , October 2021.
[19] Z. Li, Q. Wang, N. Snavely, and A. Kanazawa, “Infinitenature-zero:
Learning perpetual view generation of natural scenes from single
images,” in ECCV , 2022.
[20] A. Yu, V . Ye, M. Tancik, and A. Kanazawa, “pixelNeRF: Neural
radiance fields from one or few images,” in CVPR , 2021.
[21] M. Tancik, V . Casser, X. Yan, S. Pradhan, B. Mildenhall, P . P .
Srinivasan, J. T. Barron, and H. Kretzschmar, “Block-nerf: Scalable
large scene neural view synthesis,” in Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR) , June
2022, pp. 8248–8258.
[22] M. Niemeyer, J. T. Barron, B. Mildenhall, M. S. M. Sajjadi,
A. Geiger, and N. Radwan, “Regnerf: Regularizing neural radiance
fields for view synthesis from sparse inputs,” in Proc. IEEE Conf.
on Computer Vision and Pattern Recognition (CVPR) , 2022.
[23] C. Reiser, S. Peng, Y. Liao, and A. Geiger, “Kilonerf: Speeding up
neural radiance fields with thousands of tiny mlps,” in Interna-
tional Conference on Computer Vision (ICCV) , 2021.
[24] A. Pumarola, E. Corona, G. Pons-Moll, and F. Moreno-Noguer, “D-
NeRF: Neural Radiance Fields for Dynamic Scenes,” in Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recogni-
tion, 2021.
[25] B. Mildenhall, P . Hedman, R. Martin-Brualla, P . Srinivasan,
and J. T. Barron, “NeRF in the Dark: High Dynamic Range
View Synthesis from Noisy Raw Images,” arXiv:2111.13679
[cs, eess] , Nov. 2021, arXiv: 2111.13679. [Online]. Available:
http://arxiv.org/abs/2111.13679
[26] K. Park, U. Sinha, P . Hedman, J. T. Barron, S. Bouaziz, D. B. Gold-
man, R. Martin-Brualla, and S. M. Seitz, “Hypernerf: A higher-
dimensional representation for topologically varying neural radi-
ance fields,” ACM Trans. Graph. , vol. 40, no. 6, dec 2021.
[27] M. Johari, Y. Lepoittevin, and F. Fleuret, “Geonerf: Generalizing
nerf with geometry priors,” in Proceedings of the IEEE International
Conference on Computer Vision and Pattern Recognition (CVPR) , 2022.
[28] J. T. Barron, B. Mildenhall, D. Verbin, P . P . Srinivasan, and P . Hed-
man, “Mip-nerf 360: Unbounded anti-aliased neural radiance
fields,” CVPR , 2022.
[29] M. Niemeyer, L. Mescheder, M. Oechsle, and A. Geiger, “Differen-
tiable volumetric rendering: Learning implicit 3d representations
without 3d supervision,” in Proc. IEEE Conf. on Computer Vision
and Pattern Recognition (CVPR) , 2020.
[30] A. Chen, Z. Xu, A. Geiger, J. Yu, and H. Su, “Tensorf: Tensorial
radiance fields,” in European Conference on Computer Vision (ECCV) ,
2022.
[31] S. Fridovich-Keil, A. Yu, M. Tancik, Q. Chen, B. Recht, and
A. Kanazawa, “Plenoxels: Radiance fields without neural net-
works,” in Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR) , June 2022, pp. 5501–5510.
[32] P . Wang, L. Liu, Y. Liu, C. Theobalt, T. Komura, and W. Wang,
“Neus: Learning neural implicit surfaces by volume rendering for
multi-view reconstruction,” NeurIPS , 2021.
[33] J. T. Barron, B. Mildenhall, M. Tancik, P . Hedman, R. Martin-
Brualla, and P . P . Srinivasan, “Mip-nerf: A multiscale represen-
tation for anti-aliasing neural radiance fields,” ICCV , 2021.
[34] J. Li, Z. Feng, Q. She, H. Ding, C. Wang, and G. H. Lee, “Mine: To-
wards continuous depth mpi with nerf for novel view synthesis,”
inICCV , 2021.
[35] C. Sun, M. Sun, and H. Chen, “Direct voxel grid optimization:
Super-fast convergence for radiance fields reconstruction,” in
CVPR , 2022.
[36] L. Liu, J. Gu, K. Z. Lin, T.-S. Chua, and C. Theobalt, “Neural sparse
voxel fields,” NeurIPS , 2020.[37] A. Yu, R. Li, M. Tancik, H. Li, R. Ng, and A. Kanazawa, “PlenOc-
trees for real-time rendering of neural radiance fields,” in ICCV ,
2021.
[38] J. N. P . Martel, D. B. Lindell, C. Z. Lin, E. R. Chan, M. Monteiro,
and G. Wetzstein, “Acorn: Adaptive coordinate networks for neu-
ral scene representation,” ACM Trans. Graph. (SIGGRAPH) , vol. 40,
no. 4, 2021.
[39] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,
S. Ozair, A. Courville, and Y. Bengio, “Generative adversarial
nets,” in Advances in Neural Information Processing Systems ,
Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and
K. Weinberger, Eds., vol. 27. Curran Associates, Inc., 2014.
[Online]. Available: https://proceedings.neurips.cc/paper/2014/
file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf
[40] T. Karras, S. Laine, M. Aittala, J. Hellsten, J. Lehtinen, and T. Aila,
“Analyzing and Improving the Image Quality of StyleGAN,”
arXiv:1912.04958 [cs, eess, stat] , Mar. 2020, arXiv: 1912.04958.
[Online]. Available: http://arxiv.org/abs/1912.04958
[41] T. Nguyen-Phuoc, C. Li, L. Theis, C. Richardt, and Y.-L. Yang,
“Hologan: Unsupervised learning of 3d representations from nat-
ural images,” in The IEEE International Conference on Computer
Vision (ICCV) , Nov 2019.
[42] M. Gadelha, S. Maji, and R. Wang, “3d shape induction
from 2d views of multiple objects,” 2016. [Online]. Available:
https://arxiv.org/abs/1612.05872
[43] J. Wu, C. Zhang, T. Xue, W. T. Freeman, and J. B. Tenenbaum,
“Learning a probabilistic latent space of object shapes via 3d
generative-adversarial modeling,” in Proceedings of the 30th Inter-
national Conference on Neural Information Processing Systems , ser.
NIPS’16. Red Hook, NY, USA: Curran Associates Inc., 2016, p.
82–90.
[44] M. Niemeyer and A. Geiger, “Giraffe: Representing scenes as
compositional generative neural feature fields,” in Proc. IEEE Conf.
on Computer Vision and Pattern Recognition (CVPR) , 2021.
[45] K. Schwarz, Y. Liao, M. Niemeyer, and A. Geiger, “Graf: Genera-
tive radiance fields for 3d-aware image synthesis,” in Advances in
Neural Information Processing Systems (NeurIPS) , 2020.
[46] I. Skorokhodov, S. Tulyakov, Y. Wang, and P . Wonka, “Epigraf:
Rethinking training of 3d gans,” arXiv preprint arXiv:2206.10535 ,
2022.
[47] K. Sun, S. Wu, Z. Huang, N. Zhang, Q. Wang, and H. Li, “Con-
trollable 3d face synthesis with conditional generative occupancy
fields,” in Advances in Neural Information Processing Systems , A. H.
Oh, A. Agarwal, D. Belgrave, and K. Cho, Eds., 2022.
[48] J. Sun, X. Wang, Y. Shi, L. Wang, J. Wang, and Y. Liu, “Ide-3d:
Interactive disentangled editing for high-resolution 3d-aware
portrait synthesis,” ACM Trans. Graph. , vol. 41, no. 6, nov 2022.
[Online]. Available: https://doi.org/10.1145/3550454.3555506
[49] L. Chai, R. Tucker, Z. Li, P . Isola, and N. Snavely, “Persistent
nature: A generative model of unbounded 3d worlds,” in Pro-
ceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , 2023.
[50] C. H. Lin, H.-Y. Lee, W. Menapace, M. Chai, A. Siarohin, M.-H.
Yang, and S. Tulyakov, “InfiniCity: Infinite-scale city synthesis,”
arXiv preprint arXiv:2301.09637 , 2023.
[51] T. Park, M.-Y. Liu, T.-C. Wang, and J.-Y. Zhu, “Semantic image
synthesis with spatially-adaptive normalization,” in Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition ,
2019.
[52] P . Esser, R. Rombach, and B. Ommer, “Taming transformers for
high-resolution image synthesis,” in Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR) , June
2021, pp. 12 873–12 883.
[53] A. Mallya, T.-C. Wang, K. Sapra, and M.-Y. Liu, “World-consistent
video-to-video synthesis,” in Proceedings of the European Conference
on Computer Vision , 2020.
[54] Z. Shi, Y. Shen, J. Zhu, D.-Y. Yeung, and Q. Chen, “3d-aware indoor
scene synthesis with depth priors,” in ECCV , 2022.
[55] X. Ren and X. Wang, “Look outside the room: Synthesizing a
consistent long-term 3d scene video from a single image,” in
Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR) , 2022.
[56] D. Paschalidou, A. Kar, M. Shugrina, K. Kreis, A. Geiger, and
S. Fidler, “Atiss: Autoregressive transformers for indoor scene
synthesis,” in Advances in Neural Information Processing Systems
(NeurIPS) , 2021.15
[57] X. Wang, C. Yeshwanth, and M. Nießner, “Sceneformer: Indoor
scene generation with transformers,” in 2021 International Confer-
ence on 3D Vision (3DV) , 2021, pp. 106–115.
[58] M. A. Bautista, P . Guo, S. Abnar, W. Talbott, A. Toshev,
Z. Chen, L. Dinh, S. Zhai, H. Goh, D. Ulbricht, A. Dehghan,
and J. Susskind, “Gaudi: A neural architect for immersive
3d scene generation,” in NeurIPS , 2022. [Online]. Available:
https://arxiv.org/abs/2207.13751
[59] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, and
M. Nießner, “Scannet: Richly-annotated 3d reconstructions of
indoor scenes,” in Proc. Computer Vision and Pattern Recognition
(CVPR), IEEE , 2017.
[60] J. Straub, T. Whelan, L. Ma, Y. Chen, E. Wijmans, S. Green, J. J.
Engel, R. Mur-Artal, C. Ren, S. Verma, A. Clarkson, M. Yan,
B. Budge, Y. Yan, X. Pan, J. Yon, Y. Zou, K. Leon, N. Carter,
J. Briales, T. Gillingham, E. Mueggler, L. Pesqueira, M. Savva,
D. Batra, H. M. Strasdat, R. D. Nardi, M. Goesele, S. Lovegrove,
and R. Newcombe, “The Replica dataset: A digital replica of
indoor spaces,” arXiv preprint arXiv:1906.05797 , 2019.
[61] H. Fu, B. Cai, L. Gao, L.-X. Zhang, J. Wang, C. Li, Q. Zeng,
C. Sun, R. Jia, B. Zhao et al. , “3d-front: 3d furnished rooms with
layouts and semantics,” in Proceedings of the IEEE/CVF International
Conference on Computer Vision , 2021, pp. 10 933–10 942.
[62] K. Perlin, “An image synthesizer,” ACM Siggraph Computer Graph-
ics, vol. 19, no. 3, pp. 287–296, 1985.
[63] D. H. Lehmer, “Mathematical methods in large-scale computing
units,” Proc. of 2nd Symp. on Large-Scale Digital Calculating Machin-
ery, vol. 26, pp. 141–146, 1949.
[64] E. Sch ¨onfeld, V . Sushko, D. Zhang, J. Gall, B. Schiele,
and A. Khoreva, “You only need adversarial supervision
for semantic image synthesis,” in International Conference
on Learning Representations , 2021. [Online]. Available: https:
//openreview.net/forum?id=yvQKLaqNE6M
[65] M. Olano, J. C. Hart, W. Heidrich, B. Mark, and K. Perlin, “Real-
time shading languages,” in SIGGRAPH 2002 Course 36 Notes , ser.
SIGGRAPH ’02, 2002.
[66] J. H. Lim and J. C. Ye, “Geometric gan,” 2017. [Online]. Available:
https://arxiv.org/abs/1705.02894
[67] J. Johnson, A. Alahi, and L. Fei-Fei, “Perceptual losses for real-time
style transfer and super-resolution,” in Computer Vision – ECCV
2016 , B. Leibe, J. Matas, N. Sebe, and M. Welling, Eds. Cham:
Springer International Publishing, 2016, pp. 694–711.
[68] D. P . Kingma and J. Ba, “Adam: A method for stochastic
optimization,” in 3rd International Conference on Learning
Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015,
Conference Track Proceedings , Y. Bengio and Y. LeCun, Eds., 2015.
[Online]. Available: http://arxiv.org/abs/1412.6980
[69] Z. Chen, Y. Duan, W. Wang, J. He, T. Lu, J. Dai, and Y. Qiao,
“Vision transformer adapter for dense predictions,” arXiv preprint
arXiv:2205.08534 , 2022.
[70] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and
S. Hochreiter, “Gans trained by a two time-scale update
rule converge to a local nash equilibrium,” in Advances in
Neural Information Processing Systems , I. Guyon, U. V . Luxburg,
S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and
R. Garnett, Eds., vol. 30. Curran Associates, Inc., 2017.
[Online]. Available: https://proceedings.neurips.cc/paper/2017/
file/8a1d694707eb0fefe65871369074926d-Paper.pdf
[71] M. Bi ´nkowski, D. J. Sutherland, M. Arbel, and A. Gretton, “De-
mystifying mmd gans,” arXiv preprint arXiv:1801.01401 , 2018.
[72] R. Ranftl, K. Lasinger, D. Hafner, K. Schindler, and V . Koltun,
“Towards robust monocular depth estimation: Mixing datasets
for zero-shot cross-dataset transfer,” IEEE Transactions on Pattern
Analysis and Machine Intelligence , vol. 44, no. 3, 2022.
[73] J. L. Sch ¨onberger and J.-M. Frahm, “Structure-from-motion re-
visited,” in Conference on Computer Vision and Pattern Recognition
(CVPR) , 2016.
[74] Z. Bylinskii, L. Herman, A. Hertzmann, S. Hutka, and Y. Zhang,
“Towards better user studies in computer graphics and vision,”
2022. [Online]. Available: https://arxiv.org/abs/2206.11461
[75] J. L. Sch ¨onberger, E. Zheng, M. Pollefeys, and J.-M. Frahm,
“Pixelwise view selection for unstructured multi-view stereo,” in
European Conference on Computer Vision (ECCV) , 2016.
[76] L. Zhang and M. Agrawala, “Adding conditional control to text-
to-image diffusion models,” arXiv preprint arXiv:2302.05543 , 2023.
Zhaoxi Chen is currently a Ph.D. student at
MMLab@NTU, Nanyang Technological Univer-
sity, supervised by Prof. Ziwei Liu. He got his
bachelor’s degree from Tsinghua University in
2021. He received the AISG PhD Fellowship
in 2021. His research interests include inverse
rendering and 3D generative models. He has
published several papers in CVPR, ICCV, ECCV,
ICLR and TOG. He also served as a reviewer for
CVPR, ICCV, NeurIPS, TOG and IJCV. He is a
member of IEEE.
Guangcong Wang is currently a research fel-
low in the School of Computer Science and
Engineering at Nanyang Technological Univer-
sity, Singapore. He received a Ph.D. degree in
the School of Computer Science and Engineer-
ing, Sun Y at-sen University, Guangzhou, China,
in 2020. His research interests include genera-
tive models, 3D, person re-identification, semi-
supervised learning, and unsupervised learning.
He has published some papers, such as TOG,
TNNLS, TIP , TCSVT, ICCV, CVPR, ECCV, KDD,
and AAAI. He has served as a reviewer of TPAMI, IJCV, TIP , TCSVT,
TOMM, ICML, NeurIPS, ICCV, CVPR, ECCV, ICLR, and AAAI. He is a
member of IEEE.
Ziwei Liu (Member, IEEE) is currently a
Nanyang Assistant Professor at Nanyang Tech-
nological University, Singapore. His research re-
volves around computer vision machine learn-
ing and computer graphics. He has published
extensively on top-tier conferences and journals
in relevant fields, including CVPR, ICCV, ECCV,
NeurlPS, ICLR, ICML, TPAMI, TOG and Nature
- Machine Intelligence. He is the recipient of
Microsoft Y oung Fellowship, Hong Kong PhD
Fellowship, ICCV Y oung Researcher Award, HK-
STP Best Paper Award and WAIC Yunfan Award. He serves as an Area
Chair of CVPR, ICCV, NeurlPS and ICLR, as well as an Associate Editor
of IJCV.