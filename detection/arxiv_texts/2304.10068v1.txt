Fruit Picker Activity Recognition with Wearable
Sensors and Machine Learning
Joel Janek Dabrowski
Data61, CSIRO
Brisbane, Australia
joel.dabrowski@data61.csiro.auAshfaqur Rahman
Data61, CSIRO
Hobart, Australia
ashfaqur.rahman@data61.csiro.au
Abstract —In this paper we present a novel application of de-
tecting fruit picker activities based on time series data generated
from wearable sensors. During harvesting, fruit pickers pick
fruit into wearable bags and empty these bags into harvesting
bins located in the orchard. Once full, these bins are quickly
transported to a cooled pack house to improve the shelf life of
picked fruits. For farmers and managers, the knowledge of when
a picker bag is emptied is important for managing harvesting
bins more effectively to minimise the time the picked fruit is left
out in the heat (resulting in reduced shelf life). We propose a
means to detect these bag-emptying events using human activity
recognition with wearable sensors and machine learning methods.
We develop a semi-supervised approach to labelling the data.
A feature-based machine learning ensemble model and a deep
recurrent convolutional neural network are developed and tested
on a real-world dataset. When compared, the neural network
achieves 86% detection accuracy.
Index Terms —human activity recognition, convolutional neural
network, recurrent neural network, deep learning, agriculture,
time-series
I. I NTRODUCTION
It is estimated that up to 25% of all fruit and vegetable
produce go to waste before ever leaving the farm, costing
Australian farmers $2:84billion annually [1]. Waste may be
caused by pests, disease, weather events, or damage inﬂicted
during harvesting. In harvesting, fruit are picked and placed in
harvesting bins located throughout the paddocks or orchards,
and transported to a packing house for processing prior to
market distribution. The picked fruit are spoiled when they
are left in the harvesting bins exposed to heat for extended
periods of time. Farmers and managers must thus ensure that
the pickers ﬁll the bins quickly and that the bins are transported
to the cooled packing house as soon as they are full.
In general, the machine learning paradigm provides promis-
ing approaches to address challenges in harvesting, however
examples of such applications are scarce in the literature
[2]. This study contributes to this gap in the literature and
provides a means to monitor fruit pickers and the ﬁlling of
the harvesting bins to allow for more effective management
of the harvesting process.
As illustrated in Fig. 1, fruit pickers are equipped with a
fruit picker bag. Fruit are picked and placed into these bags,
This work is a product of a collaboration with Grow Logic, who provided
the dataset, photographs, and the context to the problem. The work was
supported by the Systems program in Agriculture & Food, CSIRO.
SensorBins
Bag
Fig. 1. Top: Photograph of a fruit picker, a picker bag, sensor, and
harvesting bin. The bin is mounted on a trailer, which is typically pulled by
a tractor. There are typically many bins scattered across the orchard. Bottom:
Photograph of bag-emptying events where two fruit pickers empty the bags
into a bin.
and once the bag is full, the fruit is emptied into a harvesting
bin. This is referred to as a bag-emptying event . Detecting
bag-emptying events provides the means to simultaneously
monitor the fruit picker productivity and bin levels. The bag-
emptying event rate is a direct measurement of the fruit
picker productivity. Furthermore, given the known capacity
of a picker bag and the knowledge of which bin the fruit is
emptied into, bag-emptying events provide a measure of the
ﬁlling rate of the bins. With this knowledge, farmers are able
to more effectively manage pickers and bins.
Given recent success in the application of deep and machine
learning approaches to human activity recognition [3], we
propose a novel application of these approaches to detect bag-
emptying events using wearable sensors. A wearable sensor
is placed on a picker’s bag strap as illustrated in Fig. 1 to
measure the picker’s movements and their proximity to the
harvest bins. Given the sensor measurements, bag-emptying
events are detected using machine learning models. We con-
sider two models: a feature-based machine learning ensemblearXiv:2304.10068v1  [cs.LG]  20 Apr 2023classiﬁer and a deep Recurrent Convolutional Neural Network
(RCNN). The models are compared on a dataset collected
during avocado harvesting.
In addition to bag-emptying event detection, we address a
data labelling problem where the duration of bag-emptying
events are unknown. For this, the K-means clustering algo-
rithm is used to perform semi-supervised labelling of the data.
Approximate times of when the events occurred and several
features of the data are used to learn the duration of bag-
emptying events.
To our knowledge, fruit picker activity recognition based on
time series data from wearable sensors is a novel application.
The main contributions of this study include: (1) we use
a wearable accelerometer sensor for measuring fruit picker
activity, (2) we used machine learning to understand fruit
picker productivity, and (3) we develop a new methodology
that uses a combination of semi-supervised labelling and
supervised learning to detect bag-emptying events. The key
advantages of our approach is it does not interfere with the
harvesting process and it is autonomous.
This article begins with a discussion on related work in
Section II. The dataset and data labelling approaches are
discussed in Sections III and IV respectively. The models are
described in Section V and our methodology is provided in
Section VI. Results are presented in Section VII and the article
is concluded in Section VIII.
II. R ELATED WORK
A. Machine Learning in Agriculture
Machine learning has been applied to various agricultural
problems and several literature surveys have been produced
[2], [4], [5]. Most applications relate to crop management
and include yield prediction, disease detection, weed detection,
crop quality prediction, and water management. Literature on
the application of machine learning approaches speciﬁcally to
harvesting and the harvesting process are scarce.
B. Fruit Picker Productivity
In relation to fruit picker activity recognition, several studies
have considered tracking pickers for yield mapping, e.g., see
[6]–[10]. Tracking pickers can be challenging due to Global
Positioning System (GPS) signal losses through foliage. Var-
ious alternatives to GPS have been thus been proposed [11],
[12]. However, even accurate picker tracking does not neces-
sarily provide any direct information on the picker’s activity.
In our work, we are directly measuring the fruit picker’s
activity, which to our knowledge has not been considered in
the literature before.
A more direct approach to measuring picker productivity is
to have a ﬁxed platform located at the bin containing some
form of digital scale and a device to identify the picker (such
as RFID or a bar code scanner) [13]–[16]. The picker identiﬁes
them self, weighs the picked fruit, and releases the weighed
fruit into the bin. Drawbacks of this approach include the
additional time that is required for weighing bags and the
additional supervisors required to ensure that the weighingprocesses is being conducted correctly. Our approach does
not interfere with the harvesting process and does not require
additional supervisors. Furthermore, our approach considers
time-series data rather than data at a single point in time.
C. Human Activity Recognition and Machine Learning
Detecting fruit picker bag-emptying events can be consid-
ered as a Human Activity Recognition (HAR) problem. Gen-
erally, HAR involves using some form of classiﬁer to predict
an activity given data from a sensor that directly monitors
human movement, such as wearable sensors. Surveys on HAR
using wearable sensors [17] deep learning approaches to HAR
[3] have been conducted. Applications with wearable sensors
in agriculture include human–robot interaction [18] and the
assessment of vibration risk with agricultural machinery [19].
To our knowledge HAR has not been applied to monitor fruit
pickers.
A wide range of wearable sensors exist [17] including:
accelerometers, global positioning systems (GPS), radio fre-
quency identiﬁcation (RFID), environmental sensors (such as
temperature), and physiological sensors (such as heart rate
monitors). A survey has been conducted on sensor positioning
on the body [20]. Sensor positions may include the waist,
arms, wrist, ankle, and the torso. The chest is suggested to
be the preferred location for medium-level activities such as
walking and house-work. The actions in such activities are
similar to that of fruit picking. The chest was thus chosen as
the location for this study.
Many HAR models begin by processing the accelerome-
ter data using a sliding window [17]. Various features are
extracted from the data in the window as it is slid across
the dataset. Features may include mean, standard deviation,
minimum, maximum, energy, main frequency component,
root mean square of the derivative, and correlation between
axes [20], [21]. The features are fed into a classiﬁer, which
classiﬁes the activity type. Various classiﬁers such as decision
trees, neural networks, Bayesian models, Markov models, and
classiﬁer ensembles have may be considered [17]. In this study,
such a feature-based ensemble classiﬁer is compared with a
deep neural network model.
Feature selection and design can be a tedious task that typi-
cally requires domain knowledge. Deep learning algorithms
are often designed to be end-to-end methods that take the
raw input data and output a prediction. Features are learned
within the multiple layers of network. A survey of various
deep learning architectures that have been applied to sensor-
based activity detection problems has been conducted [22].
These architectures include the convolutional neural network
(CNN) and the recurrent neural network (RNN).
The CNN is a neural network that applies a convolution
operation on the data presented to its inputs [23]. CNNs
exploit local interactions in the data and provide scale invariant
features [24]. In wearable sensor HAR problems, several CNN
based models have been proposed [24]–[26].
The RNN is a neural network that is designed for sequential
applications [23], [27]. It contains a neural network that isreplicated over time where the replications are sequentially
connected. Like the CNN, the RNN has been also applied to
several HAR problems [28]–[30].
RNN and CNN models have been compared on HAR
tasks [31]. It is found that RNNs perform well for activities
where long term dependency is required, such as opening a
door. CNNs perform well when long-term dependencies are
not required such as gait analysis. Given that RNNs and
CNNs each have their own advantages, combining the two
architectures may provide a more widely applicable and more
powerful model. The combination of the architectures forms a
recurrent convolutional neural network (RCNN) and provide a
promising framework for HAR [32], [33], and is an approach
we consider in this study.
III. D ATASET
A. Sensors and Data Collection
The Haltian Nexus Prototype sensors were used in the wear-
able sensor. These sensors comprise a ST LIS2DH accelerom-
eter, a Nordic nRF52832 Wirepas radio gateway module, and
a data logger. Accelerometer readings for 3 axes were logged
at a frequency of 50Hz with a measurement range of 4 G
and sensitivity of 8 mG. The radio logged its Received Signal
Strength Indicator (RSSI) value with respect to a Haltian
Thingsee POD2 Prototype node every second. The POD2 node
was located at the picker’s bin.
B. Avocado Farm Trial Dataset
A trial was conducted on an avocado farm. The data was
acquired for two different pickers over several hours. A sensor
was attached to each pickers bag strap and positioned on
the picker’s chest as illustrated in Fig. 1(a). Bag-emptying
event times were manually recorded by a human observer.
The dataset comprises 580986 samples of data with 64 bag-
emptying events.
Each of the 3 data streams from the 3-axis accelerometer
are ﬁltered with a bandpass ﬁlter. The high-cut frequency is
set to reduce aliasing. The low-cut frequency is set to remove
any offsets caused by gravitational effects. The three ﬁltered
accelerometer data streams and the RSSI data stream are
combined to form a dataset of four data streams, which are
scaled to a range of [0;1].
C. Dataset Balancing
The dataset is imbalanced where only 28% of the samples
are associated with bag-emptying events. This imbalance can
create an undesirable bias in the classiﬁers. Resampling is used
to form a balanced dataset. Sequences of samples are extracted
to ensure the sequential nature of the data is maintained. To
introduce some form of randomness in the sampling, the length
of the extracted sequence is selected according to a normal
distribution. The mean length and variance of the bag-
emptying event sequences are calculated from the sequence
length of the manual label bag-emptying events. For each bag-
emptying event sequence, only the n2N(;)preceding
non-bag-emptying event samples are preserved. All remainingnon-bag-emptying event samples are removed. The result is
a dataset with sequence lengths that are similarly distributed
between classes.
IV. B AG-EMPTYING EVENT LABELLING
The supervised machine learning algorithms require labels
of the bag-emptying events for training. Although the times
that the bag-emptying events occurred were recorded, the
duration of the events were not recorded. Furthermore, owing
to human error, the recoded bag-emptying times are only
considered to be approximations. The bag-emptying event
times are thus required to be reﬁned and the bag-emptying
event durations are required to be determined. For this, we
consider two approaches: (1) a manual labelling approach
using expert knowledge and (2) a semi-supervised approach
based on K-means clustering.
A. Manual Labelling of Bag-Emptying Events
To manually determine the bag-emptying event times and
durations, the following bag-drop process is noted: (1) A
pickers gait changes under the strain of a full bag as they
walk from the trees to the bin; (2) the picker lifts the bag into
the bin; (3) a ﬂap at the bottom of the bag is opened to release
the fruit (e.g. see Fig. 1); (4) the bag is shaken and tugged to
empty it; and (5) once the bag is empty, it is removed from
the bin and the bottom ﬂap is reattached.
The scaled accelerometer and RSSI data surrounding a bag-
emptying event are illustrated in Fig. 2. The plot indicates a
change in the dynamics as the picker transitions from normal
picking activity to the bag-emptying event. The RSSI increases
as the picker approaches the bin and the accelerometer signal
level increases due to gait change. A spike in the accelerometer
data occurs as the picker lifts the bag into the bin (this spike is
not evident in every bag-emptying event). The accelerometer
signal level remains high as the bag is shaken, removed and
reassembled. The RSSI and the accelerometer signal decrease
as the picker returns to fruit picking activities. Based on these
observations the bag-emptying times were manually reﬁned
and the duration of the bag-emptying events were deﬁned.
The average bag-emptying event duration is 50 seconds with
the emptying of the bag typically lasting between 10 and 20
seconds.
B. Semi-supervised labelling of Bag-Emptying Events
The K-means clustering algorithm is used to perform semi-
supervised learning of the bag-emptying event labels1. The
approach is to pre-deﬁne the duration of bag-emptying events
around the logged bag-emptying times. The K-means algo-
rithm is then used to reﬁne the duration by clustering samples
according to statistical features of bag-emptying event and
non-bag-emptying event data.
Data sample labels are initialised by deﬁning the duration
of all bag-emptying events to be 1200 samples in length,
with 500 samples before and 700 samples after the manually
1The K-means algorithm is selected due to its computational efﬁciency,
however more complex clustering algorithms could also be considered.0.550.600.65RSSI
01
Bag-
emptying
0.00.5Acc. X01
Bag-
emptying
0.500.75Acc. Y01
Bag-
emptying
518000 520000 5220000.51.0Acc. Z01
Bag-
emptyingFig. 2. Plot of the sensor data with a bag-emptying event. Sensor data is
plotted in blue. The manually deﬁned bag-emptying event is plotted in orange
with a value of 1 indicating a bad drop event.
logged bag-drop event time. Samples within this window are
associated with bag-emptying events and samples outside this
window are associated with normal fruit picking activity.
A 256-sample sliding window is shifted sample-by-sample
over the data sequences. For each shift, a set of statistical fea-
tures are extracted to produce a feature vector associated with
each sample. This feature vector comprises the mean, standard
deviation, minimum, maximum, and standard deviation of the
derivative for the associated sample. Additionally, to capture
the sequential structure of the data, an indicator is included to
specify whether the neighbouring samples labels are associated
with bag-emptying events.
K-means algorithm is initialised by grouping the feature
vectors into two clusters according to the initial bag-emptying
event labels. The mean values of each cluster form the initial
values for the K-means clustering algorithm. These clusters
are reﬁned using the K-means algorithm, which corresponds
to reﬁning the labels of each data sample. The algorithm was
typically run over 10 iterations, where the number of iterations
can affect how much the clusters change from the predeﬁned
settings.
The K-means clustering algorithm can produce false pos-
itives and false negatives, where false positives are spurious
bag-emptying events predicted far from the logged time and
false negatives are spurious non-bag-emptying events pre-
dicted near logged bag-emptying event times.
The results of the K-means algorithm are thus ﬁltered
using the intersection and union operators of mathematical set
theory. Predicted bag-emptying event sequences are compared
with the predeﬁned bag-emptying event sequence as illustrated
in Fig. 3. To reduce false positives an intersection operator is
used. If none of the samples in the predicted sequence overlap
with the predeﬁned sequence, the prediction is rejected. As
illustrated in the bottom plot of Fig. 3, the predictions at the
beginning and end of the sequence are removed as they do not
overlap with the predeﬁned bag-emptying event sequence. To
reduce false negatives, a union operation is applied between
the predicted and predeﬁned sequences. As illustrated in the
bottom plot of Fig. 3, the false negative is removed. Note that
01Predef.
01K-mn
0 2000 4000 6000 800001Filt.Fig. 3. Plots of the process of semi-supervised bag-emptying event labelling.
The predeﬁned labels, the K-means clustering results, and the ﬁltered K-means
clustering results are plotted in the top, middle and bottom ﬁgures respectively.
Algorithm 1 Semi-supervised Labelling of bag-emptying
events using the K-means algorithm.
Require: The dataset X, a vector of bag-emptying event start
timeststart, a vector of bag-emptying event end times tend,
and the dataset labels Y.
1:Deﬁne the window size q= 256
2:Deﬁne an empty set of dataset labels ^Y=?
3:foreach bag-emptying event, ido
4: Extract dataset sequence surrounding the ithevent
A=Xtend
i 1:tstart
i+1
5: Extract the labels associated with A
B=Ytend
i 1:tstart
i+1
6: foreach indexjof sample in Ado
7: Extract the window of data associated with sample j
W=Aj:j+q
8: Compute the feature vector for sample j
fj=features (W)
9: Set the class of sample jaccording to the window
cj=(
01
qPq
k=1Bj+k<0:5
11
qPq
k=1Bj+k0:5
10: end for
11: Update the classes using the K-means algorithm
c kmeans (f;c)
12: Filter the labels using mathematical set theory
c ﬁlter(c;B)
13: Append the ﬁltered labels cto the new label set ^Y.
14:end for
15:return The updated labels ^Y.
this operator is only applied to join predicted bag-emptying
events. It is not permitted to extend the predicted bag-emptying
event duration. This operation can however result in an un-
reasonably long bag-emptying event sequence if it joins two
long sequences of positive samples. If a predicted sequence is
longer than twice the predeﬁned bag-emptying event sequence,
it is rejected and the predeﬁned bag-emptying event sequence
is used instead.
The overall algorithm for the semi-supervised labelling
approach is presented in Algorithm 1.
V. M ODELS
Detecting a bag-emptying event from the wearable sensor
data is a challenging task. As described in Section IV, theRSSI Acc. Z Acc. Y Acc. XFeature vector
(std., energy, RMS( dx=dt ), RMS( d2x=dt2),
mean( dx=dt ), mean( d2x=dt2), min, max)Naive Bayes ANN (512)P
60% 40%
Input
dataFeature
vectorEnsembleClass
Fig. 4. Architecture of the ensemble model. A feature vector is assembled
from a window of data. A naive Bayes and an ANN perform a classiﬁcation
given the feature vector. The outputs of these classiﬁers are weighted and
summed to determine the class.
events comprises various sub-activities involved in the bag-
emptying event. Furthermore, the bag-drop signals may vary
according to pickers and the environment. The models are
required to handle these variations.
A. Feature Based Ensemble Model
A traditional feature-based ensemble model is applied for
detecting bag-emptying events. The model is illustrated in
Fig. 4. A 256 sample sliding window with zero overlap is
applied to the data. The following features are extracted in
each window: standard deviation, energy, the RMS ﬁrst and
second derivative, mean ﬁrst and second derivative, minimum
value, and maximum value [20], [21].
The features are provided as inputs to an ensemble classiﬁer
comprising a Gaussian Naive Bayes classiﬁer and a neural
network. These are two commonly used classiﬁers in HAR
[17] and are sufﬁciently different from each other to provide
diversity in the ensemble. The neural network comprises a
single hidden layer with 512 neurons with hyperbolic tangent
activation functions. The ADAM algorithm [34] is used to
train the neural network. The parameters of the Gaussian naive
Bayes classiﬁer are estimated using maximum likelihood. Both
classiﬁers output a prediction in the form of a probability
of a bag-emptying event. These predictions are combined
in the ensemble through a weighted summation. The Naive
Bayes classiﬁer is weighted with 60% of the vote (which was
determined through cross validation).
B. Recurrent Convolutional Neural Network (RCNN)
The architecture of the RCNN used in this study is illus-
trated in Fig. 5 and is based on the models presented in [32]
and [33]. For each data stream, a set of samples are extracted
using a 256 sample sliding window with zero overlap. The
windows of samples from the 4 sensor streams are combined
into a tensor, where each stream represents channel for the
input of the CNN portion of the model. A key advantage ofRSSIAcc. ZAcc. YAcc. XConv. 1 (ﬁlters = 32, kernel size = 32)Pool 1 (pool size = 8, stride = 4)Conv. 2 (ﬁlters = 16, kernel size = 16)Pool 2 (pool size = 4, stride = 2)Dense (512 neurons)
Input
tensorCNNRNNOutputs
Fig. 5. Architecture of the RCNN model for a single sequence sample (based
on [32] and [33]). The input contains a tensor comprising a window of samples
from each data stream. The CNN comprises two convolutional layers, two
pooling layers, and a densely (fully) connected neural network. The CNN
output is passed to an LSTM cell of the RNN. The LSTM cell output is
passed to a sigmoidal neuron which outputs the class of the input window
sample. The LSTM models temporal dynamics over sample windows.
the RCNN over the ensemble model is that it is an end-to-end
model and does not require feature engineering.
The CNN portion of the RCNN performs feature extraction
and the RNN portion of the RCNN models temporal dynamics
over the sample windows. The RCNN outputs the class of the
input window sample. The rectiﬁed linear unit (ReLU) is used
as the activation function in hidden layers, the ﬁlter and layer
sizes in the network were determined through trial and error,
and the ADAM algorithm [34] is used for training.
VI. M ETHODOLOGY
Both the models use a sliding window with zero overlap.
The window of data is presented to the classiﬁer. The set of
data in the window is classiﬁed to belong either to a bag-
emptying event or a non-bag-drop event. Each data sample
within the window is associated with this predicted class.
During training, a window may be slid to a position where it
partially falls within a bag-emptying event. The ground-truth
class label for the window is calculated as the average true
class of all the samples in the window.
To validate models and results, a 6-fold cross validation test
is performed. The dataset is split into 6 equal data segments.
Each data segment is a continuous time series of 96831
samples. The model is trained on 5 of the data segments and
tested on the remaining segment. This is repeated such that
the model is tested on each data segment of the dataset.TABLE I
FEATURE -BASED ENSEMBLE MODEL AND RCNN MODEL MEDIAN VALUE
RESULTS FOR THE PREDEFINED LABELS ,MANUALLY DEFINED LABELS ,
AND THE LEARNED LABELS .
Model Measure Predeﬁned Manual Learned
EnsembleAccuracy 74% 80% 80%
Precision 81% 89% 83%
Recall 63% 66% 71%
F-score 71% 77% 77%
RCNNAccuracy 79% 76% 86%
Precision 86% 80% 89%
Recall 71% 72% 83%
F-score 77% 75% 84%
Accuracy, precision, recall, and F-score are used to measure
the performance of the classiﬁers presented in this study.
Accuracy describes the ratio of the number of correct clas-
siﬁcations to the total number of classiﬁed samples. Precision
describes the ratio of correct classiﬁcations to the total number
of classiﬁcations made for the particular class. It considers the
number of incorrectly predicted bag-emptying events and thus
provides a measure of the classiﬁer quality. Recall describes
the ratio of correct classiﬁcations to the total number of items
which truly belong to the predicted class. It considers the
number of bag-emptying event samples that were missed.
Recall thus provides a measure of the probability of correctly
classifying the bag-emptying event. Finally, the F-score is
deﬁned as the harmonic mean between the precision and recall.
It provides a measure to describe both the precision and recall
together.
VII. R ESULTS
A. Comparison of Labelling Approaches
The median value of the accuracy, precision, recall, and F-
score across the 6-fold cross validations for the predeﬁned
labels, manually deﬁned labels, and the learned labels are
provided in Table I.
The ensemble model produces the poorest results with the
predeﬁned labels. Improved performance is obtained with
the manually and learned labels. The performance for the
manually and learned labels are similar. This is a key result
as it validates the proposed approach to learning the dataset
labels. The manual labels produce a higher precision than the
learned labels. However, the learned labels provide a higher
recall than the manual labels. The recall is considered to a
more important measure as it relates to the probability of
detecting bag-emptying events.
The best results for the RCNN are obtained with the learned
labels. This reinforces the validation of the semi-supervised
label learning approach. The RCNN produces results that are
much higher than those produced by the ensemble model. The
ensemble model’s precision for the manual labels matches the
capability of the RCNN. This however is achieved at the cost
of a low recall value of 66%. Along with the high precision
value, the RCNN produces a recall of 83%. These comparisons
Accuracy Precision Recall F-score5060708090100%(a) Ensemble model.
Accuracy Precision Recall F-score5060708090100%
(b) RCNN model.
Fig. 6. Box whisker plot of the cross validation results.
are reiterated by the 7% difference in F-score results between
the two models.
B. Results with the Learned Labels
A box whisker plot of the results over the 6-fold cross
validation test are presented in Fig. 6. The RCNN performs
better than the ensemble model for all performance measures.
The precision box of the RCNN reaches high values. However,
the box is the largest of all boxes in the diagram. A larger box
indicates that there is more variability in the precision results.
The quality of the model is however still high considering
that the range of the box remains above 80%. The ensemble
model’s recall box is large with whiskers that extend to
low values. This indicates a high level of uncertainty in the
ensemble models recall results. The recall box of the RCNN
is narrow indicating high certainty in the RCNN recall results.
The range of the box remains above 80% indicating good
prediction ability of the RCNN model. Both models produce
narrow F-score boxes. The range of the RCNN F-score box
is higher than the ensemble model indicating superior perfor-
mance overall.
A plot of the predictions and data for one of the cross
validation folds is presented in Fig. 7. The learned labels
correspond well with the changes in the accelerometer and
RSSI data. The predictions of the ensemble model are sporadic
resulting in several false positives and false negatives. The
RCNN model predictions are smoother over time. The RCNN
however misses the third bag-emptying event. This is possibly
due to the RSSI level unexpectedly dropping to a minimum
during this bag-emptying event. Unlike the RCNN, the ensem-
ble model is able to detect the third bag-emptying event. This
seems to indicate that the RCNN relies more on RSSI data and
the ensemble model relies more on accelerometer data. Note
that the last bag-emptying event is not missed by the RCNN.
It is detected in the following cross validation fold.
The plots of the predictions for the remaining cross valida-
tion folds are illustrated in Fig. 8. The ensemble model results
are more sporadic and the predictions are more conﬁdent. The
RCNN is less conﬁdent and the predictions are smoother over
time. This is preferred when false positives have a high risk.01Bag-
emptying
0.500.75RSSI
0.51.0Acc. X
0.250.500.75Acc. Y
0 10000 20000 300000.250.500.75Acc. Z(a) Ensemble model.
01Bag-
emptying
0.500.75RSSI
0.51.0Acc. X
0.250.50Acc. Y
0 10000 20000 300000.250.500.75Acc. Z (b) RCNN model.
Fig. 7. bag-emptying event predictions and dataset for the convolutional recurrent neural network. The blue curves plot the dataset. The orange curve the top
ﬁgure plots the model predictions. A value of 1 indicates a bag-emptying event.
31000 41000 51000 6100001
31000 41000 51000 6100001
62000 72000 82000 9200001
62000 72000 82000 9200001
93000 103000 113000 12300001
93000 103000 113000 12300001
124000 134000 144000 15400001
124000 134000 144000 15400001
155000 165000 175000 18500001
155000 165000 175000 18500001
Fig. 8. Bag-emptying event predictions for the second to sixth cross validation folds along the rows. Plots of the ensemble model are presented in the left
column and plots of the RCNN model are presented in the right column. The horizontal axis is the sample number. The vertical axis is the probability of a
bag-emptying event. The blue curve plots the manual bag-emptying events and orange curve plots the predicted bag-emptying events.
VIII. D ISCUSSION AND CONCLUSION
In this study, we present a novel application on measuring
fruit picker productivity. The picker productivity is measured
by detecting bag-emptying events from wearable sensor data.
A traditional feature-based ensemble model and a deep con-
volutional recurrent neural network are applied to predict bag-
emptying events from the wearable sensor data. Furthermore,a semi-supervised method for learning the bag-emptying event
labels is presented.
Results indicate that both models are able to successfully
detect the bag-emptying events. The RCNN model is more
accurate than the ensemble model but it is less conﬁdent,
which results in 3 of the 64 bag-emptying events being missed.
The ensemble model has at least one positive detection within
each bag-emptying event suggesting that all bag-emptyingevents. As the ensemble model does not directly model
any temporal relationship between samples, its predictions
are noisy resulting in multiple detections for a single bag-
emptying event. The RCNN models temporal dynamics to
produces smooth predictions over time and more accurate
predictions of bag-emptying event durations.
In future work, the RCNN could be improved by increasing
its capacity and training it on more data. The capacity of
the RCNN can be increased by introducing more CNN ﬁlters
and by increasing the CNN depth. Such improvements may
provide the model with the capability to learn more advanced
features. The RNN can be improved by adding multiple layers
and by using bi-directional RNNs. More complex models
however generally require more data. Collecting more data is
thus a priority for future work. Other than improving models,
the data captured by the sensors provide information relating
to other problems such as health and safety. For example,
the accelerometer sensor can be used to detect excessive bag
weight or falls.
REFERENCES
[1] L. Fedunik-Hofman, “Food waste: preventing a multi-billion dollar
problem,” https://www.science.org.au/curious/earth-environment/
food-waste-preventing-multi-billion-dollar-problem, 2023.
[2] M. Altalak, M. Ammad uddin, A. Alajmi, and A. Rizg, “Smart agricul-
ture applications using deep learning technologies: A survey,” Applied
Sciences , vol. 12, no. 12, 2022.
[3] S. Zhang, Y . Li, S. Zhang, F. Shahabi, S. Xia, Y . Deng, and N. Alshurafa,
“Deep learning in human activity recognition with wearable sensors: A
review on advances,” Sensors , vol. 22, no. 4, p. 1476, 2022.
[4] L. Benos, A. C. Tagarakis, G. Dolias, R. Berruto, D. Kateris, and
D. Bochtis, “Machine learning in agriculture: A comprehensive updated
review,” Sensors , vol. 21, no. 11, 2021.
[5] R. Sharma, S. S. Kamble, A. Gunasekaran, V . Kumar, and A. Kumar,
“A systematic literature review on machine learning applications for
sustainable agriculture supply chain performance,” Computers & Oper-
ations Research , vol. 119, p. 104926, 2020.
[6] F. K. Anjom, S. G. V ougioukas, and D. C. Slaughter, “Development and
application of a strawberry yield-monitoring picking cart,” Computers
and Electronics in Agriculture , vol. 155, pp. 400 – 411, 2018.
[7] J. Whitney, Q. Ling, W. Miller, and T. A. Wheaton, “A dgps yield mon-
itoring system for ﬂorida citrus,” Applied Engineering in Agriculture ,
vol. 17, no. 2, p. 115, 2001.
[8] D. Thomas, C. Perry, G. Vellidis, J. Durrence, L. Kutz, C. Kvien,
B. Boydell, and T. Hamrita, “Development and implementation of a
load cell yield monitor for peanut,” Applied Engineering in Agriculture ,
vol. 15, no. 3, p. 211, 1999.
[9] G. Pelletier and S. K. Upadhyaya, “Development of a tomato load/yield
monitor,” Computers and Electronics in Agriculture , vol. 23, no. 2, pp.
103 – 117, 1999.
[10] W. Miller and J. Whitney, “Evaluation of weighing systems for citrus
yield monitoring,” Applied Engineering in Agriculture , vol. 15, no. 6, p.
609, 1999.
[11] Y . Ampatzidis, S. V ougioukas, and M. Whiting, “A wearable module
for recording worker position in orchards,” Computers and Electronics
in Agriculture , vol. 78, no. 2, pp. 222 – 230, 2011.
[12] R. Arikapudi, S. G. V ougioukas, F. Jim ´enez-Jim ´enez, and F. K. Anjom,
“Estimation of fruit locations in orchard tree canopies using radio signal
ranging and trilateration,” Computers and Electronics in Agriculture , vol.
125, pp. 160 – 172, 2016.
[13] Y . Ampatzidis, L. Tan, R. Haley, and M. D. Whiting, “Cloud-based
harvest management information system for hand-harvested specialty
crops,” Computers and Electronics in Agriculture , vol. 122, pp. 161 –
167, 2016.
[14] Y . G. Ampatzidis, M. D. Whiting, B. Liu, P. A. Scharf, and F. J.
Pierce, “Portable weighing system for monitoring picker efﬁciency
during manual harvest of sweet cherry,” Precision Agriculture , vol. 14,
no. 2, pp. 162–171, Apr 2013.[15] Y . G. Ampatzidis, M. D. Whiting, P. A. Scharf, and Q. Zhang, “Devel-
opment and evaluation of a novel system for monitoring harvest labor
efﬁciency,” Computers and Electronics in Agriculture , vol. 88, pp. 85 –
94, 2012.
[16] Y . Ampatzidis and S. V ougioukas, “Field experiments for evaluating
the incorporation of rﬁd and barcode registration and digital weighing
technologies in manual fruit harvesting,” Computers and Electronics in
Agriculture , vol. 66, no. 2, pp. 166 – 172, 2009.
[17] O. D. Lara and M. A. Labrador, “A survey on human activity recogni-
tion using wearable sensors,” IEEE Communications Surveys Tutorials ,
vol. 15, no. 3, pp. 1192–1209, Third 2013.
[18] A. Anagnostis, L. Benos, D. Tsaopoulos, A. Tagarakis, N. Tsolakis,
and D. Bochtis, “Human activity recognition through recurrent neural
networks for human–robot interaction in agriculture,” Applied Sciences ,
vol. 11, no. 5, 2021.
[19] G. Aiello, P. Catania, M. Vallone, and M. Venticinque, “Worker safety
in agriculture 4.0: A new approach for mapping operator’s vibration
risk through machine learning activity recognition,” Computers and
Electronics in Agriculture , vol. 193, p. 106637, 2022.
[20] L. Atallah, B. Lo, R. King, and G. Yang, “Sensor positioning for
activity recognition using wearable accelerometers,” IEEE Transactions
on Biomedical Circuits and Systems , vol. 5, no. 4, pp. 320–329, Aug
2011.
[21] N. Ravi, N. Dandekar, P. Mysore, and M. L. Littman, “Activity recog-
nition from accelerometer data,” in Proceedings of the 17th Conference
on Innovative Applications of Artiﬁcial Intelligence - Volume 3 , ser.
IAAI’05. AAAI Press, 2005, pp. 1541–1546.
[22] J. Wang, Y . Chen, S. Hao, X. Peng, and L. Hu, “Deep learning for sensor-
based activity recognition: A survey,” Pattern Recognition Letters , 2018.
[23] I. Goodfellow, Y . Bengio, and A. Courville, Deep learning . MIT press,
2016.
[24] M. Zeng, L. T. Nguyen, B. Yu, O. J. Mengshoel, J. Zhu, P. Wu, and
J. Zhang, “Convolutional neural networks for human activity recognition
using mobile sensors,” in 6th International Conference on Mobile
Computing, Applications and Services , Nov 2014, pp. 197–205.
[25] S.-M. Lee, S. M. Yoon, and H. Cho, “Human activity recognition from
accelerometer data using convolutional neural network,” in 2017 IEEE
International Conference on Big Data and Smart Computing (BigComp) ,
Feb 2017, pp. 131–134.
[26] J. B. Yang, M. N. Nguyen, P. P. San, X. L. Li, and S. Krishnaswamy,
“Deep convolutional neural networks on multichannel time series for
human activity recognition,” in Proceedings of the 24th International
Conference on Artiﬁcial Intelligence , ser. IJCAI’15. AAAI Press, 2015,
pp. 3995–4001.
[27] A. Graves, Supervised Sequence Labelling with Recurrent Neural Net-
works . Berlin, Heidelberg: Springer Berlin Heidelberg, 2012.
[28] A. Murad and J.-Y . Pyun, “Deep recurrent neural networks for human
activity recognition,” Sensors , vol. 17, no. 11, 2017.
[29] M. Inoue, S. Inoue, and T. Nishida, “Deep recurrent neural network for
mobile human activity recognition with high throughput,” Artiﬁcial Life
and Robotics , vol. 23, no. 2, pp. 173–185, Jun 2018.
[30] Y . Guan and T. Pl ¨otz, “Ensembles of deep lstm learners for activity
recognition using wearables,” Proc. ACM Interact. Mob. Wearable
Ubiquitous Technol. , vol. 1, no. 2, pp. 11:1–11:28, Jun. 2017.
[31] N. Y . Hammerla, S. Halloran, and T. Pl ¨otz, “Deep, convolutional,
and recurrent models for human activity recognition using wearables,”
inProceedings of the Twenty-Fifth International Joint Conference on
Artiﬁcial Intelligence , ser. IJCAI’16. AAAI Press, 2016, pp. 1533–
1540.
[32] F. J. Ord ´o˜nez and D. Roggen, “Deep convolutional and lstm recurrent
neural networks for multimodal wearable activity recognition,” Sensors ,
vol. 16, no. 1, 2016.
[33] S. Yao, S. Hu, Y . Zhao, A. Zhang, and T. Abdelzaher, “Deepsense:
A uniﬁed deep learning framework for time-series mobile sensing data
processing,” in Proceedings of the 26th International Conference on
World Wide Web , ser. WWW ’17. Republic and Canton of Geneva,
Switzerland: International World Wide Web Conferences Steering Com-
mittee, 2017, pp. 351–360.
[34] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”
arXiv preprint arXiv:1412.6980 , 2014.