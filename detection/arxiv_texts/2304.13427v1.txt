TRAINING-FREE LOCATION-AWARE TEXT-TO-IMAGE SYNTHESIS
Jiafeng Mao
The University of Tokyo
Dept. of Information and Communication Eng.
mao@hal.t.u-tokyo.ac.jpXueting Wang
CyberAgent, Inc.
AI Lab
wang xueting@cyberagent.co.jp
ABSTRACT
Current large-scale generative models have impressive ef-
ﬁciency in generating high-quality images based on text
prompts. However, they lack the ability to precisely con-
trol the size and position of objects in the generated image.
In this study, we analyze the generative mechanism of the
stable diffusion model and propose a new interactive gener-
ation paradigm that allows users to specify the position of
generated objects without additional training. Moreover, we
propose an object detection-based evaluation metric to as-
sess the control capability of location aware generation task.
Our experimental results show that our method outperforms
state-of-the-art methods on both control capacity and image
quality.
Index Terms —diffusion model, text-to-image synthesis
1. INTRODUCTION
Text-to-image synthesis is a rapidly evolving ﬁeld aimed
at generating natural and realistic images based on textual
prompts. The emergence of generative diffusion models [1]
and related research [2, 3, 4, 5, 6, 7] has signiﬁcantly im-
proved the quality of generated images. Recent research has
focused on enhancing the quality of generated images and
diverse ﬁne-grained control on the editability of image con-
tent by diffusion models [8, 9, 10, 11, 12, 13], for example,
Compositional diffusion [8] allows user to give multiple con-
cepts and generate images with feature combinations, and
Imagic [9] allows user to edit semantic attributes on input
images by speciﬁed test. However, seldom attention has been
paid to ﬁne-grained control of object location and size within
the generated content.
This work aims to make generated images more precisely
aligned with user expectations. Whereas typical text-guided
diffusion models accept textual prompts, we propose a new
task setting that also accepts the speciﬁc location and size of
objects described in the prompt and generates images where
each object is positioned in the location speciﬁed by users as
they want, as shown in Fig. 1. This setting contribute to the
accurate user-aligned generation and more stable generation
performance of multiple objects.
Fig. 1 . Concept of our task setting. Users can provide both
prompt and exact region guidance of objects. Then the gen-
eration model aims at generating images where each object is
positioned in its speciﬁed location.
Existing research [11] has observed that the cross-attention
layer [14] of the U-Net [15] in diffusion models controls the
layout and structure of generated images. Therefore, we
propose controlling the position of individual objects in gen-
erated images by manipulating the values of cross-attention
layers without additional training. Currently, no evaluation
metric is designed to measure the control efﬁciency for the
object-wise location-guided generation. We also propose a
comprehensive evaluation method based on object detectors
to calculate the consistency of object information between
generated images and guidance information. We conducted
experiments using the MS COCO dataset [16] to show the
effectiveness of our proposal on both object-wise control ef-
ﬁciency and image quality. We summarize the contributions
of this paper as follows:
• We propose an intuitive yet effective method to con-
trol the region of objects on generated images, allow-
ing users to specify the precise position and size of the
objects they wish to generate.
• We propose an evaluation method to estimate the
object-wise consistency between the generated im-
ages and the guidance information for location-aware
text-to-image generation tasks.arXiv:2304.13427v1  [cs.CV]  26 Apr 2023Fig. 2 . Illustration of our proposal. The text prompt is processed into a text embedding using CLIP, which is then used to
compute the attention map. Each entry of the attention map corresponds to a word in the text prompt. The object-wise guidance
is processed into a soft mask (Section 3.3.1) with out-of-region suppression (Section 3.3.2) and added to the attention map to
increase the likelihood of the corresponding concept features appearing in speciﬁc locations.
2. RELATED WORKS
Some methods have been proposed to improve the control-
lability of diffusion models by making it easier for users to
specify the content of generated images. Imagic [9] focuses
on attribute editing, designed to generate edited images ac-
cording to the speciﬁed text. Compositional diffusion [8]
takes multiple text conditions and outputs an image contain-
ing features of all text inputs. Unlike these works, our focus
is on generating images where objects are in the speciﬁc lo-
cations users want them to be. Paint-with-Word [13] shows
that the location of generated objects can be manipulated by
changing the attention maps. However, according to our ex-
periments, its mask can harm the generation quality. Inspired
by it, we propose a ﬁnely designed method of manipulating
attention maps to control the region of generated objects.
3. PROPOSED METHOD
3.1. Problem Setting
In our task, we aim to generate images not only according to
a given text prompt P, which is an image-wise caption, but
also complying with the object-wise guidance fB;Cg, where
Bdenotes a list of bounding boxes indicating the desired spa-
tial location of each object and Cis a list of the object category
mentioned in prompt P.
3.2. Stable Diffusion
Our approach and experiments utilize the state-of-the-art text-
to-image model, Stable Diffusion [4] as the baseline model.
This model achieves high performance by projecting noisy
images into a latent space, using a diffusion model to remove
the noise. The denoised images in the latent space are then de-
coded as the ﬁnal output. To accomplish this, Stable Diffusionemploys a modiﬁed U-Net for noise estimation and a frozen
CLIP [17] text encoder to convert text inputs into embedding
sequences. The relationship between the image and text is es-
tablished through multiple cross-attention layers in the down-
sampling and up-sampling stages. During a generation of an
image, the CLIP encoder encodes provided prompt Pas an
embedding sequence, which is subsequently processed into
keys and values K;V2R(n;l)in each cross-attention block
by linear layers. The latent Xof the current image is also
projected into a query Q2R(n;[w;h])in each cross-attention
block, and the attention maps M2R([w;h];l)are calculated
byQandKas follows,
M=Softmax (QKT
p
d): (1)
The attention map Mcontrols the spatial distribution of val-
uesV, which contains rich semantic information about the
image content speciﬁed by the prompt P. It is indicated that
each attention map entry highly corresponds to a single word
in the prompt [11]. In an attention map, each entry corre-
sponds to a word in the text prompt. When the attention map
for a particular word has a high value in a speciﬁc region, this
region will likely present the features of the concept associ-
ated with that word.
3.3. Location-Aware Text-to-Image Synthesis
A straightforward way to manipulate the location of gener-
ated objects is by increasing the values of speciﬁed concepts
in the speciﬁed area on the attention maps. Given a prompt P
and a object-wise guidance fb;cg2fB;Cg, since thec2C
is a word mentioned in P, there is a particular entry of atten-
tion mapMccorresponding to c. We proposed to increase the
value in area bon theMcby a gaussian-based soft mask G
so that image tokens in the region bare encouraged to attendmore to the text token of c. As a result, the semantic con-
cept corresponding to cis more likely to appear in the area b.
Speciﬁcally, the calculation of attention maps is modiﬁed as
follows,
M=Softmax (QKT+wSp
d) (2)
whereSis our proposed soft mask and has the same shape as
QKT. Values inSthat are speciﬁed by fB;Cgare calculated
by a gaussian mask G, while others are set to  1, as shown
in Fig. 2. Speciﬁcally, for each item fb;cgin object-wise
guidancefB;Cg, the maskScis calculated as follows,
Sc(x;y) =(
Gc(x;y)If(x;y)insideb
 1 Otherwise(3)
wis the weight of the mask, which is calculated as follows,
w=w0f(T)max(QKT); (4)
wheref(T)is a linear function of the denoising step Tthat
becomes smaller as the noise level decreases. w0is a user-
speciﬁed scalar. We then introduce the soft mask Scin detail
as followings.
3.3.1. Gaussian-based Mask
By adding values to the speciﬁed area, Paint-with-words [13]
makes the speciﬁed words dominate the area and prevents
attention maps for other words from working in that area.
This approach impacts the internal distribution of attention
maps and therefore affects the quality of generated images,
particularly when the size of the speciﬁed object is large.
Our proposal can effectively alleviate the negative impact
caused by additional masks while maintaining the control
efﬁciency, as shown in Fig. 3. We model our soft mask
based on a Gaussian probability density function. Similar to
a Gaussian distribution, our soft mask will have high values
in the central region and low values at the edges. Thus, our
mask can retain the intensity of the guided generation us-
ing the high values in the central region while reducing the
impact on surrounding regions. Speciﬁcally, for each item
fb;cgin object-wise guidance fB;Cg, where b is denoted
asfxmin;ymin;xmax;ymaxg, our soft maskGc(x;y)for
conceptcis calculated as follows,
Gc(x;y) =1
2e 1
2(D(x)2+D(y)2): (5)
where
D(x) =2x (xmax +xmin)
(xmax xmin)s; (6)
and
D(y) =2y (ymax +ymin)
(ymax ymin)s; (7)
wheresis a hyper-parameter to control the softness of the
mask and is set to 2 experimentally. Larger swill lead to
smaller values on the edge of the mask.
Fig. 3 . Generated images under object-wise guidance. The
left image is guided by Paint-with-words [13], and the right
image is guided by our soft mask.
methods w0IoU"Rsuc%" FID#
Stable [4] - 0:19 12:14 20:78
Paint [13]0:05 0:22 14:86 21:45
0:1 0:24 17:56 22:72
0:15 0:22 15:38 27:07
0:2 0:16 11:34 47:73
Ours0:15 0:25 19:24 20:40
0:2 0:27 22 :31 21:35
Table 1 . Results on selected MS COCO dataset. Our proposal
signiﬁcantly outperformed the state-of-the-art on control efﬁ-
ciency and image quality. Columns of IoU and Rsucindicate
the average recorded IoU and the ratio of successfully gener-
ated objects to all objects, respectively.
3.3.2. Out-of-region Suppression
Using a speciﬁc concept’s mask to enhance the value of spe-
ciﬁc areas in the attention map can increase the likelihood of
this concept appearing in this area. However, when the atten-
tion map already has high values outside the speciﬁed area,
only enhancing the values of the speciﬁc area cannot prevent
the objects from being generated outside the area. Thus, we
set the values of the mask outside the area to  1 to suppress
the generation of out-of-region objects. After softmax, the
values outside the speciﬁed area are calculated as 0so that
the generated objects are restricted to the speciﬁed area.
4. EXPERIMENTS
4.1. Datasets
We use the MS COCO dataset, which contains image caption
and object annotation, to validate our proposed method. We
selected samples from the MS COCO dataset that meet the
requirements of our task, i.e., samples whose object-wise an-methods subset IoU IoU s IoUm IoUlRsuc%Rs
suc%Rm
suc%Rl
suc%
Stable [4]near 0:29 0:07 0:27 0:41 24:20 0:86 12 :26 45:09
mid 0:19 0:06 0:23 0:35 10:23 0:50 8:84 30 :71
far 0:09 0:04 0:15 0:28 2:32 0:39 3:99 22 :08
All 0:19 0:05 0:21 0:38 12:14 0:50 8:11 39 :41
Paint [13]near 0:34 0:09 0:33 0:48 31:10 1:01 18 :39 56:01
mid 0:24 0:09 0:28 0:43 16:17 1:42 14 :15 47:05
far 0:13 0:07 0:22 0:38 5:65 1:10 10 :47 37:66
All 0:24 0:08 0:27 0:46 17:56 1:16 14 :05 52:34
oursnear 0:37 0 :12 0 :37 0 :50 37:03 3 :74 27 :82 60 :92
mid 0:27 0 :10 0 :32 0 :45 20:58 2 :59 22 :48 49 :32
far 0:17 0 :09 0 :26 0 :40 9:75 2 :97 17 :94 41 :56
All 0:27 0 :10 0 :31 0 :48 22:31 2 :95 22 :38 56 :24
Table 2 . Control efﬁciency of guidance on different positions. Our method achieved the best performance on all subsets.
notations are all mentioned in the image caption (prompt).
Since most of the image captions involve only one or two
signiﬁcant objects in the image, we randomly selected 6000
images, where 3000 contain one object and the other 3000
contain two objects. All images are resized to 5122.
4.2. Evaluation Metrics
Object-wise Consistancy We propose an object detector-
based evaluation method to quantify the effectiveness in
controlling the positions of generated objects. We gener-
ate images using image captions as prompt and object-wise
bounding boxes from the dataset as object-wise guidance.
Then, we use a YOLOR [18] to perform object detection
on each generated image and obtain the class labels and
bounding boxes of the objects in the generated images. The
consistency of the detection results with the object-wise
guidance information can represent the effectiveness of the
object-wise controlling, speciﬁcally, given a generated image
and the object-wise guidance fB;Cgit uses, for each entry
fb;cg 2 fB;Cg, we ﬁrst ﬁnd out all detected objects with
the class label cand record the maximum IoU. If no object
with the label cis detected, IoU corresponding to fb;cgis
recorded as 0. If the recorded IoU is more signiﬁcant than
0:5,fb;cgis regarded as a successful control.
We notice that the size and the position of the guidance
box have a signiﬁcant impact on the control efﬁciency. We
categorize all objects into three subsets based on the size of
the guidance bounding box. Speciﬁcally, the subset Scon-
tains all objects with an area less than 1502, the subset L
contains all objects with an area greater than 3002, while the
other objects are channeled into subset M. On the other hand,
We evenly divide all guidance boxes into three subsets, near ,
mid, andfar, based on the distance between the center of
the guidance box and the center of the image and evaluate the
control effectiveness of each subset.
Image Quality FID [19] is a measure of similarity between
two datasets of images. We evaluate the quality of generated
images by their FID with the ground truth images in datasets,
and a smaller FID indicates a better image quality.5. RESULTS AND ANALYSIS
We ﬁrst show overall results in Table 1. Our proposal outper-
forms the stable diffusion on object-wise consistency (IoU,
Rsuc) thanks to the designed soft-mask while maintaining
or even improving the image quality (FID). However, Paint
[13] controls the position of the generated object at the cost
of image quality. Moreover, our proposal ( w0= 0:15and
w0= 0:2) outperforms Paint on all settings of w0. We then
show the evaluation results on the control effectiveness of dif-
ferent subsets considering guidance size and position in Ta-
ble 2, from which we can ﬁnd that our proposal outperforms
the related works on all conditions.
Discussion on Guidance Size The guidance size signiﬁcantly
affects the control’s effectiveness. According to the results for
each subsets;m;l , larger guidance bounding boxes will have
larger IoUs and are more likely to be successful controlled.
In contrast, the guidance of generating small objects is much
more difﬁcult.
Discussion on Guidance Position Although stable diffusion
cannot accept object-wise guidance, generated objects some-
times overlap with the guidance box by chance when the ob-
ject guidance box is relatively large and near to the center
of image, because the model tends to generate objects in the
center of the image. For objects far from the center, the like-
lihood of the stable diffusion accidentally generating objects
at the speciﬁed location decreases signiﬁcantly. In contrast,
our proposed method maintains higher control over all sub-
sets than the related works.
6. CONCLUSIONS
This paper proposes an effective object generation position
guiding method for stable diffusion and a method to evaluate
object-wise generation consistency. We introduce a Gaussian
distribution-based soft-mask to reduce the negative impact on
the attention map while preserving the control efﬁciency. Ex-
perimental results show that our method achieves state-of-the-
art performance on both control efﬁciency and image quality.7. REFERENCES
[1] Jonathan Ho, Ajay Jain, and Pieter Abbeel, “Denoising
diffusion probabilistic models,” Advances in Neural In-
formation Processing Systems , vol. 33, pp. 6840–6851,
2020.
[2] Prafulla Dhariwal and Alexander Nichol, “Diffusion
models beat gans on image synthesis,” Advances in Neu-
ral Information Processing Systems , vol. 34, pp. 8780–
8794, 2021.
[3] Jiaming Song, Chenlin Meng, and Stefano Ermon, “De-
noising diffusion implicit models,” in International
Conference on Learning Representations , 2021.
[4] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¨orn Ommer, “High-resolution im-
age synthesis with latent diffusion models,” in Proceed-
ings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , 2022, pp. 10684–10695.
[5] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey
Chu, and Mark Chen, “Hierarchical text-conditional
image generation with clip latents,” arXiv preprint
arXiv:2204.06125 , 2022.
[6] Alexander Quinn Nichol and Prafulla Dhariwal, “Im-
proved denoising diffusion probabilistic models,” in In-
ternational Conference on Machine Learning . PMLR,
2021, pp. 8162–8171.
[7] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya
Ramesh, Pranav Shyam, Pamela Mishkin, Bob Mcgrew,
Ilya Sutskever, and Mark Chen, “Glide: Towards photo-
realistic image generation and editing with text-guided
diffusion models,” in International Conference on Ma-
chine Learning . PMLR, 2022, pp. 16784–16804.
[8] Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, and
Joshua B Tenenbaum, “Compositional visual genera-
tion with composable diffusion models,” in Computer
Vision–ECCV 2022: 17th European Conference, Tel
Aviv, Israel, October 23–27, 2022, Proceedings, Part
XVII . Springer, 2022, pp. 423–439.
[9] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Hui-
wen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani,
“Imagic: Text-based real image editing with diffusion
models,” arXiv preprint arXiv:2210.09276 , 2022.
[10] Weixi Feng, Xuehai He, Tsu-Jui Fu, Varun Jam-
pani, Arjun Akula, Pradyumna Narayana, Sugato Basu,
Xin Eric Wang, and William Yang Wang, “Training-free
structured diffusion guidance for compositional text-to-
image synthesis,” arXiv preprint arXiv:2212.05032 ,
2022.[11] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kﬁr Aber-
man, Yael Pritch, and Daniel Cohen-Or, “Prompt-
to-prompt image editing with cross attention control,”
arXiv preprint arXiv:2208.01626 , 2022.
[12] Dong Huk Park, Samaneh Azadi, Xihui Liu, Trevor
Darrell, and Anna Rohrbach, “Benchmark for com-
positional text-to-image synthesis,” in Thirty-ﬁfth
Conference on Neural Information Processing Systems
Datasets and Benchmarks Track (Round 1) , 2021.
[13] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vah-
dat, Jiaming Song, Karsten Kreis, Miika Aittala, Timo
Aila, Samuli Laine, Bryan Catanzaro, et al., “edifﬁ:
Text-to-image diffusion models with an ensemble of ex-
pert denoisers,” arXiv preprint arXiv:2211.01324 , 2022.
[14] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser,
and Illia Polosukhin, “Attention is all you need,” Ad-
vances in neural information processing systems , vol.
30, 2017.
[15] Olaf Ronneberger, Philipp Fischer, and Thomas Brox,
“U-net: Convolutional networks for biomedical im-
age segmentation,” in Medical Image Computing and
Computer-Assisted Intervention–MICCAI 2015: 18th
International Conference, Munich, Germany, October
5-9, 2015, Proceedings, Part III 18 . Springer, 2015, pp.
234–241.
[16] Tsung-Yi Lin, Michael Maire, Serge Belongie, James
Hays, Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and
C Lawrence Zitnick, “Microsoft coco: Common ob-
jects in context,” in Computer Vision–ECCV 2014: 13th
European Conference, Zurich, Switzerland, September
6-12, 2014, Proceedings, Part V 13 . Springer, 2014, pp.
740–755.
[17] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-
try, Amanda Askell, Pamela Mishkin, Jack Clark, et al.,
“Learning transferable visual models from natural lan-
guage supervision,” in International conference on ma-
chine learning . PMLR, 2021, pp. 8748–8763.
[18] Chien-Yao Wang, I-Hau Yeh, and Hong-Yuan Mark
Liao, “You only learn one representation: Uni-
ﬁed network for multiple tasks,” arXiv preprint
arXiv:2105.04206 , 2021.
[19] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,
Bernhard Nessler, and Sepp Hochreiter, “Gans trained
by a two time-scale update rule converge to a local nash
equilibrium,” Advances in neural information process-
ing systems , vol. 30, 2017.