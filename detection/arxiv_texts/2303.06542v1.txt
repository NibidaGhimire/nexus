1
StereoTac: a Novel Visuotactile Sensor that
Combines Tactile Sensing with 3D Vision
Etienne Roberge1, Guillaume Fornes2, Jean-Philippe Roberge1
Abstract ‚ÄîCombining 3D vision with tactile sensing could
unlock a greater level of dexterity for robots and improve several
manipulation tasks. However, obtaining a close-up 3D view of the
location where manipulation contacts occur can be challenging,
particularly in conÔ¨Åned spaces, cluttered environments, or with-
out installing more sensors on the end effector. In this context,
this paper presents StereoTac , a novel vision-based sensor that
combines tactile sensing with 3D vision. The proposed sensor
relies on stereoscopic vision to capture a 3D representation of
the environment before contact and uses photometric stereo to
reconstruct the tactile imprint generated by an object during
contact. To this end, two cameras were integrated in a single
sensor, whose interface is made of a transparent elastomer coated
with a thin layer of paint with a level of transparency that can be
adjusted by varying the sensor‚Äôs internal lighting conditions. We
describe the sensor‚Äôs fabrication and evaluate its performance
for both tactile perception and 3D vision. Our results show that
the proposed sensor can reconstruct a 3D view of a scene just
before grasping and perceive the tactile imprint after grasping,
allowing for monitoring of the contact during manipulation.
Index Terms ‚ÄîTactile Sensing, Vision-Based Sensors, 3D vision,
Perception for Manipulation and Grasping.
I. I NTRODUCTION
TACTILE sensing is often an important capability for
robots that interact with their environment and that
perform tasks such as grasping, manipulating and assem-
bling objects. In recent years, signiÔ¨Åcant progress has been
made in developing tactile sensors that can assist robots
with determining a myriad of physical attributes related to
the objects to manipulate such as their pose, shape and
even their texture at a very Ô¨Åne scale. These sensors, which
are made from various technologies including piezoelectric,
capacitive, and optical sensing, can help robots to handle a
wide range of objects‚Äîfrom small and delicate to large and
irregular‚Äîwhich, by extension, contribute to increase their
overall dexterity. When complementing vision, tactile sensing
can also unlock the ability of achieving new, complex tasks
that require additional information not provided by either
sensing modality alone.
However, a common challenge in tactile sensing and visual
perception during robotic manipulation is the reduced object
visibility that will typically occur when a robot reaches for an
1Etienne Roberge and Jean-Philippe Roberge are with the Command
and Robotics Laboratory, ¬¥Ecole de technologie sup ¬¥erieure, Montreal,
Quebec, H3C1K3, Canada (e-mail: etienne.roberge.1@ens.etsmtl.ca; jean-
philippe.roberge@etsmtl.ca).
2Guillaume Fornes is with the Bordeaux Institute of Technology,
ENSEIRB-MATMECA, 1 avenue du Dr Albert Schweitzer B.P. 99 33402
Talence, France (e-mail: gfornes@enseirb-matmeca.fr).
Supported by the Natural Sciences and Engineering Research Council of
Canada (NSERC) under grant award RGPIN-2022-04884.
Tactile Mode
Vision ModeFig. 1. A photo showing StereoTac , a visuotactile sensor combining photo-
metric stereo with stereoscopic vision.
object and/or when the gripper encompasses it as it performs
its grasp. This is particularly true when the manipulator is used
for reaching objects in conÔ¨Åned spaces, such as cabinets or
boxes, or when manipulation occurs in cluttered environments.
In these scenarios, the robot will often create obstructions to
cameras that are statically afÔ¨Åxed to the robotic cell. This
can limit the accuracy and reliability of the grasp, as the
vision system cannot continuously and unobstructedly track
the object, which may however be prone to moving as the robot
approaches. Alternatively, vision system(s) can be mounted
directly to the robot wrist to get a closer perspective to where
manipulation contacts will be generated. However, this gene-
rally increases the size of the tool at the end effector, which can
in turn lead to additional constraints on movement / reduced
motion and an overall augmented bulkiness of the system. This
is particularly true when 3D vision is needed‚Äîeven though
small 3D vision systems exist, time-of-Ô¨Çight, structured light
and stereoscopic vision to name a few, will generally require
more internal components and more space than what is typi-
cally needed by 2D cameras. Combining 3D vision with tactile
sensing could be an advantage in several manipulation tasks,
but acquiring complete, unobstructed tridimensional view of
the object close to where manipulation actually happens is
often still considered a challenge.
To address this issue, we propose a novel vision-based
tactile sensor that combines stereoscopic vision (3D vision)
with photometric stereo (tactile), as displayed in Fig. 1. The
proposed sensor uses a semi-transparent soft skin and twoarXiv:2303.06542v1  [cs.RO]  12 Mar 20232
a) b) c)
RB LEDs
2 x 2D CamerasElastomer}Adjustable semi-
transparent coating
Fig. 2. An example of a use case scenario with StereoTac. a) StereoTac‚Äôs main components; b) Here, two sensors are integrated to robotic hand and provide
a stitched 3D close-up view for grasp planning; c) After an objet is picked up, the sensor allows the reconstruction of the tactile imprint.
cameras to enable high-resolution, multi-modal tactile sensing
for robotic manipulation. The sensor is inspired by the idea
ofwhole-body vision [1] which consists of having the ability
to perceive tactile stimuli as well as being able to see through
the skin. In our proposed implementation, the contact interface
is made of a transparent elastomer covered with a thin layer
of reÔ¨Çective paint, which allows two cameras to capture the
deformation of the surface induced by the contact while still
being able to see through the skin by varying internal lighting
conditions. This allows the sensor to acquire 3D images of
the scene, even when the manipulator is close (from 5 to
60 cm) from the grasping site, and to get tactile imprints
after contacting the object, as illustrated in Fig. 2. The main
contributions of this paper are:
The design of StereoTac , a novel visuotactile sensor that
combines tactile sensing with 3D vision. To the best of
our knowledge, it is the Ô¨Årst time a vision-based tactile
sensor that uses photometric stereo with stereoscopic
vision was designed to capture both visual (3D) and
tactile data from the same location;
An analysis of the impact the level of transparency has
on both tactile imprint reconstruction and 3D perception;
Experimental results that demonstrate the sensor‚Äôs ability
to capture both tactile and 3D vision. The stereoscopic
camera is compared to a similar off-the-shelf product and
the whole sensor is integrated to a robotic arm for 3D
object reconstruction (shown in the accompanyingvideo).
In the next sections, we Ô¨Årst provide background infor-
mation about vision-based sensors, with a focus on sensors
that have the ability to acquire external vision as well as
tactile stimuli. In section III, we discuss StereoTac‚Äôs operating
principles as well as its fabrication process. In sections III-B
and III-C, we present experimental results related to the
performance of both tactile and visual data, and use of the
sensor in the real world. Finally, we discuss the main results
this work generated as well as future work opportunities.
II. R ELATED WORK
A. Vision-Based Sensors
Considerable efforts have been devoted, during the last
decades, to the development of high-performance tactile sen-
sors based on different transduction principles. Chi et al. [2]review the most common transduction types in tactile sen-
sors, which include piezoresistive, piezoelectric, capacitive,
inductive, magnetic, barometric and optical sensors. A sub-
category of optical sensors includes those based on vision [3],
sometimes also referred to as ‚Äùvisuotactile sensors‚Äù. Vision-
based tactile sensors have brought signiÔ¨Åcant interest in the
last years, partly due to the fact that their data processing
can leverage well-known computer vision techniques. Further-
more, these sensors beneÔ¨Åt from recent progress in the Ô¨Åeld
of cameras and are generally considered as high resolution,
affordable and reliable sensors, when broadly compared to
other transduction techniques [4]. Several approaches exist
for the fabrication of vision-based tactile sensors. However,
in most occurrences and although these sensing devices are
equipped with camera(s), the visual perception is normally
limited to the sensor‚Äôs internal chamber.
Vision-based tactile sensors generally employ at least one
camera that captures the deformation of an elastomer while
external pressure is exerted on its surface. From the images
captured by the camera, the goal is usually to create a
depth map that reÔ¨Çects the penetration of the object into the
elastomer, to determine the force distribution on the sensor,
or to infer a light reÔ¨Çection-to-pressure mapping. To meet
such objectives, some have relied on frustrated total internal
reÔ¨Çection (FTIR) by using an elastomer sitting on a light-
conductive material [5], [6]. In this setting, the elastomer has
to be non-transparent to generate a perceptible reÔ¨Çection to the
camera, therefore limiting the camera‚Äôs view to the sensor‚Äôs
internal chamber. Other approaches involve tracking pins [7],
scattered particles [8] or markers [1] displacement, which can
be used to measure normal force, shear and torque. However,
the presence of such particles in the elastomer generally hin-
ders efforts for external perception. Another popular principle
in the literature is photometric stereo [9], which consists of
generating a depth map by illuminating the elastomer coated
with reÔ¨Çective pigments from different directions. Following
this method, Gelsight sensors [10], [11], Digit [12] and
others [13], [14] use distinct red, green and blue lighting
from different directions to illuminate an elastomer that allows
the generation of a depth map from a single image. Using
photometric stereo with markers have also been proposed [14],3
[15], which allow geometric reconstruction along with the
determination of force distribution on the contact medium.
Classical stereophotometric approaches usually involve the
application of a non-transparent specular or matte coating on
the contact medium, which limits the camera‚Äôs perception to
the inside of the sensor. Other types of vision-based tactile
sensors have been proposed, such as stereoscopic-based sen-
sors that get 3D images of a membrane deformation without
constrained illumination [16], [17], sensors using a mix of
photometric stereo and structured light [18], and sensors based
on illumination contrast and difference images [19]. However,
all of the aforementioned sensors also suffer from the same
limitation of using camera(s) that are able to capture only the
inside portion of the sensor.
B. Vision-Based Sensors with Proximity/3D Geometry Sensing
Nevertheless, a sensor that could combine both tactile and
external vision would be a valuable asset for a wide variety of
tasks, such as path and grasp planning, pose estimation, object
reconstruction and many more. For this reason, researchers
have sought ways to integrate both of these sensing modalities
in a compact device. One well-known example of such sensor
is Fingervision [20], which combines tactile sensing through
marker tracking with 2D vision. By analyzing multiple 2D
images with optical Ô¨Çow, the authors were able to estimate
the proximity of objects. Fingervision have been successfully
used for cutting vegetables [21], however, the markers printed
on the external layer create noise in the images and their use
of a single camera limited perception to 2D. Conversely, a rare
occurrence where a single sensor combines both tactile sensing
with 3D vision is the work of Shimonomura et al. [22], which
combines FTIR sensing with stereoscopic vision. However, the
sensor used FTIR technology with a rigid acrylic surface as
the contact medium, which means it only provides information
on the position and area of contact without capturing grasping
forces or enabling contact imprint reconstruction. On the
other hand, Hogan et al. [23] showed that it was possible
to see through a semi-reÔ¨Çective coating, used for photometric
stereo by varying the sensor‚Äôs internal lighting conditions. The
authors have demonstrated the feasibility of estimating object
proximity using an approach that required artiÔ¨Åcial vision
techniques and moving the robot on a predetermined linear
path to acquire images [24]. However, this method used a
single camera, which prevented the sensor from perceiving the
3D geometry of objects. Recently, Luu et al. [25] introduced
ProTac, a sensor with a contact medium made of polymer
dispersed liquid crystal (PLDC) Ô¨Ålm with markers, which
allowed the active control of the membrane‚Äôs transparency.
However, the markers generated noise in the 2D images and
the two cameras integrated by the authors were facing each
others in the current implementation, which would not be well-
suited for acquiring 3D features.
In order to combine tactile and visual data, we rather
show the development of a novel visuotactile sensor that
combines 3D vision with tactile sensing. The sensor relies on
stereoscopic vision for 3D imagery and photometric stereo,
that beneÔ¨Åts from using an adjustable-transparency-through-
lighting elastomer, for tactile sensing.
Fig. 3. Exploded view of the proposed sensor.
III. D ESIGN AND SENSING PRINCIPLES
A. Fabrication
The design objective is to create a visuotactile sensor that
integrates stereoscopic vision to capture a 3D representation of
the nearby environment before contact and uses photometric
stereo to acquire a 3D depth map (tactile imprint) of the con-
tact. This capability could complement existing tactile-based
manipulation approaches in cluttered environments (e.g.: [26]),
where the ability to accurately perceive the surrounding envi-
ronment and the object being manipulated is often crucial.
Fig. 3 provides an exploded view of the elements used in the
proposed design. The design involves several key components,
including two 2D cameras, the contact elastomer and the semi-
transparent layer. In the following subsections, we describe
how these elements were fabricated and assembled, along with
speciÔ¨Åc design considerations.
1) Elastomer and Coatings: As per prior studies [11], [23],
P-595 silicone elastomer from Silicones Inc. was employed as
the base material for all considered membranes, as depicted in
Fig. 4. The elastomers were produced using a mold made of
three 1/8 inch acrylic sheets, with one sheet cut to the desired
membrane shape and the remaining two employed to compress
the gel and Ô¨Çatten the surfaces.
The transparent and semi-transparent reÔ¨Çective membranes
studied in this paper, which correspond to Fig. 4-#1 and
Fig. 4-#2, were designed in accordance with the speciÔ¨Åcations
outlined in [23]. To achieve the desired transparency, multiple
layers of mirror-type spray paint (Rust-Oleum 267727) were
applied onto the elastomer. A silicone protective layer was
then sprayed over the paint layer to safeguard it. To create
the protective layer, the same gel used for the membrane was
mixed with a silicone thinner (Smooth-On NOVOCS Gloss) in
a 2:1 ratio, resulting in a highly Ô¨Çuid gel that could be easily
spread over the paint layer.
Fig. 4. The elastomers used in the comparative study: 1) Completely
transparent, 2) Semi-transparent reÔ¨Çective, 3) Semi-transparent matte, 4)
Opaque reÔ¨Çective, and 5) Opaque matte. It is important to note that membranes
1-3 are the focus of the study, while membranes 4-5 are not see-through and
only used for comparison purposes in section III-C.4
The semi-transparent matte membrane (Fig. 4-#3) was gen-
erated by incorporating 0.5% white silicone dye (Smooth-
On Silc Pig White) by weight into the protective layer.
Subsequently, the opaque membranes (Fig. 4-#5 and Fig. 4-
#5) were produced by adding 3% white dye by weight to the
protective layer. To create the reÔ¨Çective opaque membrane,
a mirror paint layer was applied, similar to that of the
semi-transparent reÔ¨Çective membrane, prior to coating with
the opaque protective layer. The opacity of each membrane
depicted in Fig. 4 was approximated using a stable light source
and a lux meter (AP-881D from AOPUTTRIVER) positioned
at a Ô¨Åxed distance. The sensing portion of the lux meter was
covered by the membrane placed on an adapter, which only
allowed light to pass through the membrane to reach the lux
meter. The lux reading obtained in this manner and estimated
opacity are shown in Table I.
TABLE I
APPROXIMATED OPACITY FOR EACH MEMBRANE
Membrane Type Total Light (Lux) Approx. Opacity (%)
No membrane 466 n/a
Transparent 442 5.15
Semi-Trans. ReÔ¨Çective 352 24.46
Semi-Trans. Matte 363 22.10
Opaque ReÔ¨Çective 93 80.04
Opaque Matte 141 69.74
2) Lighting and acrylic support: For the internal illumina-
tion of the sensor, NeoPixel LED strips from Adafruit were po-
sitioned on the walls of the sensor enclosure at the same level
as the acrylic membrane support. This conÔ¨Åguration provides
parallel illumination of the membrane surface, effectively
reducing light leakage from the sensor. In particular, when
using transparent or semi-transparent membranes, we aim to
minimize light escaping from the sensor to prevent premature
object illumination before contact. Further discussion on this
topic can be found in section IV-C. These lights are operated
using an Arduino Micro communicating with a ROS Melodic
package that we developed for StereoTac.
To minimize internal reÔ¨Çections of the lights on the acrylic
support block, a gray Ô¨Ålter (VViViD Smoke Black Gloss
Vinyl) was applied to the sides, as demonstrated in [15].
Additionally, a 1624K57 LED diffuser from McMaster-Carr
is employed to promote light diffusion instead of specularity.
3) Cameras: The prototype uses Odseven 160¬∞ variable
Focus cameras, which offer a large Ô¨Åeld of view to capture the
entire membrane. The cameras are particularly well-suited to
this research due to their depth of Ô¨Åeld, which enables the fo-
cus to be on the membrane deformations while also capturing
objects up to approximately 60 cm in front of the sensor. These
cameras are operated by an Arducam USB3.0 Camera Shield
capable of simultaneously acquiring images from two cameras.
However, the cameras‚Äô circuitry required a modiÔ¨Åcation for
stereoscopic usage. SpeciÔ¨Åcally, to synchronize the camera
images, the clock component was removed from one camera‚Äôs
electrical circuit and that camera was connected to the clock
of the second camera. This ensured precise synchronization
between the cameras, allowing simultaneous image acquisition
by the acquisition card. The cameras are positioned 14 mm
apart, providing better depth resolution for close-up shots near
the sensor, but decreasing resolution for more distant objects,
which is not a focus of this study.B. Stereoscopic Vision
Although the 14 mm spacing between the cameras limits
the depth perception resolution at longer distances, it allows
the sensor to be more compact and to perceive objects in
a closer range, which is helpful for Ô¨Åne manipulation and
more suited to conÔ¨Åned and/or cluttered spaces (e.g.: [26]).
To calibrate both the intrinsic and extrinsic parameters of
the cameras, the stereo camera calibration utility available
in the camera calibration package [27] on ROS was used,
given its ease of use and its visual feedback during the
calibration process. The calibration was performed using an
8x6 checkerboard with 17mm tiles. The information obtained
from calibration enables image rectiÔ¨Åcation. Indeed, due to
the cameras having wide-angle lenses, the initial images are
distorted and require rectiÔ¨Åcation prior to utilization. Using the
Q Matrix obtained from calibration, the Stereo Block Matching
utility provided by OpenCV is utilized to generate the disparity
map and 3D projection of the scene. Finally, the obtained
points are Ô¨Åltered by statistically removing outliers using the
Open3D [28] library before publishing the Ô¨Ånal point cloud.
C. Tactile Sensing
In tactile mode, the visuotactile sensor utilizes photometric
stereo to capture the deformation of the membrane in response
to the pressure and force applied by an object. Similar to other
tactile sensors such as GelSight [14], membrane deformation
is captured using arrays of LEDs to illuminate the membrane
from multiple angles, and a camera is then used to reconstruct
the 3D shape of the object.
To ensure that only the deformation of the membrane is
captured while StereoTac is operated in tactile mode, the
exposure of the cameras is reduced and the LED arrays
inside the sensor are illuminated to make the luminosity high
enough to capture reÔ¨Çections. Moreover, to eliminate any
potential contamination from external lighting in tactile mode,
a HSV Ô¨Ålter is applied to isolate only the red and blue tones
corresponding to the LEDs‚Äô illumination. The LED arrays are
placed on the four sides of the square-shaped lens, providing
a range of angles to illuminate the membrane and enabling
the detection of Ô¨Åne surface details. When the goal is only
to obtain the shape of the contacts on the membrane, all the
LED arrays can be illuminated to observe the reÔ¨Çection on
the membrane‚Äôs surface with the camera. However, when the
goal is to perform 3D reconstruction of the contact on the
membrane, directed light can be used to capture the reÔ¨Çection
gradient and create a 3D representation of the tactile imprint.
The following sub-sections present the 3D contact recon-
struction method with StereoTac. The approach involves cap-
turing contact gradients on a membrane utilizing photometric
stereo technique. Additionally, we will describe the sensor
calibration procedure, speciÔ¨Åcally regarding its ability to read
gradients on a semi-transparent membrane.
1)Capturing contact gradients from membrane illumi-
nation :By utilizing the reÔ¨Çectance map of the elastomer, it
is possible to determine the gradient at each pixel based on
the intensity of the measured colors. Regarding the StereoTac
sensor, the membrane is illuminated by placing the sensor
lights at the elastomer‚Äôs perimeter in a square conÔ¨Åguration,5
12
3
4
12
3
4
Fig. 5. The 2-step gradient capture method. 1) Gradient dy is obtained
by illuminating the membrane with LED array 2 and 4 with blue and red
respectively. 2) Gradient dx is obtained by illuminating the membrane with
LED array 1 and 3 with blue and red respectively.
as displayed in Fig. 3. This setup provides four feasible
illumination angles. However, a three-angle approach using
RGB colors on three distinct axes, as is frequently done
with photometric stereo-based sensors, would not be sufÔ¨Åcient
to observe the complete contact gradients. SpeciÔ¨Åcally, the
utilization of three colors simultaneously on three different,
orthogonal angles would only provide a fraction of the in-
formation in the fourth direction where no directed light is
employed. To address this limitation, we utilized the placement
of the illumination axes to illuminate the elastomer in two
steps, as shown in Fig. 5. Given the parallel arrangement of
the LED rows, the x-gradient (dx) information is obtained by
illuminating rows 1 and 3 with distinct colors. Similarly, the y-
gradient (dy) information is obtained from rows 2 and 4. Since
illumination is done from only two directions at the same time,
we employed blue-red pairs to facilitate color segmentation
during image processing. The dx and dy gradients were
captured sequentially by alternating the illuminated LED pairs,
with dx and dy LEDs alternating at a rate of 4 Hz. Image
capture from the cameras was synchronized to obtain two
distinct images of membrane illumination. An example of the
resulting images for the semi-transparent reÔ¨Çective membrane
(Fig. 4-#2) can be observed in Fig. 6.
After acquiring the dx and dy gradient images, an HSV Ô¨Ålter
is applied to retain only the pixels corresponding to the red
and blue tones of the LEDs used. This step eliminates potential
contamination captured by the cameras originating from exter-
nal lighting. As different membranes are investigated during
this study, and their exact reÔ¨Çectance maps are unknown,
a simple, 3-hidden layers neural network is employed, as
Fig. 6. Process for the 3D reconstruction of contacts. A) The example-
object (D20). B) D20 contacting the reÔ¨Çective semi-transparent membrane.
C.1-2) Images obtained during the two-step illumination dx-dy: note that the
white/green spot is a ceiling light in the room where the image was taken (339
Lux). D.1-2) Images obtained after Ô¨Åltering C.1-2 using a color Ô¨Ålter. E1-2)
3D reconstruction obtained with gradients calculated using images D.1-2.
Fig. 7. Examples of 3D contact reconstruction made with the semi-transparent
reÔ¨Çective membrane. Left, the real objects (M5 screw, wood screw, D12 and
D0). Center, top view of the 3D imprints. Right, isometric view of the imprints.
proposed by Wang et al. [15]. Calibration is performed for
each membrane by capturing 30 images, with a calibration
ball of known diameter pressed at various positions on the
membrane. After using the HSV mask to eliminate non-contact
information from the image, and by knowing the diameter of
the ball and the exact center position of the ball in the image,
the dx and dy values of each pixel in contact with the ball can
be easily determined using a simple distance relationship with
its center. For example: dx= sin 1((px cx)=r), where pis
the position of the pixel, cis the position of the center of the
ball in pixels and ris the radius of the ball in pixels.
All non-zero pixels obtained from this acquisition are
utilized to train the MLP neural network. The input to the
network consists of the R-B values and the x-y position of
each pixel. Including x-y values aims to mitigate the light
attenuation problem, where pixels further away from the light
source have lower brightness. This issue arises because, for
the same contact gradient, the red brightness of a pixel closer
to the red light source is higher than that of a pixel on the
opposite side of the membrane.
2)Reconstructing the 3D Tactile Imprint using gx-gy :
As demonstrated by Wang et al. [15], it is possible to use
a 2D Fast-Poisson solver to compute the depth ( z) of each
pixel given the gx and gy values of each pixel from an image
capture. This method is particularly useful when dealing with
noisy or incomplete gradient data, as it generally produces
smoother results during 3D reconstruction. In our case, as
the gradients are estimated by our neural network, we found
that this reconstruction method provided a more accurate
tactile imprint with Ô¨Åner details. Finally, by knowing the
number of pixels per mm in the obtained images (in our6
case, 15 pixels/mm), we can determine the measured depth
in millimeters. We used the open-source Python code from
Doerner [29] to compute the Fast Poisson algorithm. Fig. 7
provides an overview of the reconstruction process for several
objects with different shapes.
IV. E XPERIMENTS
A. Evaluation of Visual Depth through different membranes
We assess the depth perception capability by utilizing var-
ious evaluation metrics. In particular, the performance of the
sensor in vision mode is evaluated using Z-accuracy ,RMS
error (spatial noise) and temporal noise . These metrics are
commonly used in the evaluation of depth cameras and cor-
respond to some of the recommended metrics in the ‚ÄùCamera
Depth Testing Methodology‚Äù from Intel [30].
To perform empirical experiments on the membranes (#1,
#2 and #3 in Fig. 4), we positioned the sensor at different
distances (10 cm, 15 cm, 20 cm, 25 cm, and 30 cm) away
from a completely Ô¨Çat surface. At each position, ten RGB-
D images were captured with each of the membranes. Addi-
tionally, the same experiments were repeated using the Intel
RealSense D405 camera, which is an off-the-shelf short-range
stereoscopic camera, for comparison purposes.
1)Z-accuracy :Z-accuracy measures the depth accuracy
by taking a pre-measured ground truth as a reference. The
metric expresses the accuracy of depth data on a per-pixel basis
relative to the ground truth for each captured depth image at
a Ô¨Åxed distance. The depth is calculated relative to a best-Ô¨Åt
plane in the point cloud to isolate camera positioning errors.
The Z-accuracy is then obtained by calculating
Zacc=median (D(x; y) GT)
GT100% ; (1)
where D(x; y)is the calculated depths at pixel positions (x; y)
and GT is the ground-truth depth value. The resulting Z-
Accuracy is then a ratio of the error and the actual distance.
The experiment results can be observed in Fig. 8.
Fig. 8. Z-Accuracy measured on a Ô¨Çat surface using different membranes at
different distances.
The precision of depth measurement was found to be af-
fected by the use of a semi-transparent membrane. As expected
from the presence of distortions created by using a semi-
transparent interface, the presence of matte or reÔ¨Çective Ô¨Åni-
shes created more variations in depth perception and resultedin a generally less accurate median values ( 0:5 9%) in the
distribution, when compared to the Intel camera or StereoTac
equipped with a transparent membrane.
2)RMS error :The Root Mean Square (RMS) error and
spatial noise of StereoTac‚Äôs depth measurements are evaluated.
While the variation in depth values of each pixel within a
ROI does not directly measure accuracy, it is an important
metric for assessing the quality of depth measurements, as it
provides information on the consistency and repeatability of
the measurements within a speciÔ¨Åc area. The results obtained
during this experiment can be observed in table II.
TABLE II
RMSE V ALUES OBTAINED ON A FLATSURFACE BY DIFFERENT
MEMBRANES AT DIFFERENT DISTANCES , (%) [ :]
Dist. Intel D405 TransparentSemi-trans.
matteSemi-trans.
reÔ¨Çective
10 cm. 0.43 : 0.01 4.1 : 1.66 2.18 : 0.1 8.06 : 1.43
15 cm. 0.6 : 0.02 2.3 : 0.7 3.52 : 0.59 4.70 : 0.79
20 cm. 0.86 : 0.03 2.23 : 0.06 5.91 : 3.19 6.65 : 0.48
25 cm. 1.01 : 0.04 1.92 : 0.52 3.08 : 0.06 4.76 : 0.43
30 cm. 1.15 : 0.02 1.89 : 0.18 3.74 : 0.13 6.15 1.35
Table II reveals that the reÔ¨Çective semi-transparent mem-
brane exhibits a higher RMSE error, consequently accounting
for the higher Z-Accuracy error. The precision and reliability
of the whole system are indeed dependent on the intrinsic
variation of measurements on a single image, explaining the
correlation between the RMSE and Z-Accuracy values.
3)Temporal Noise :The changes in depth values for each
pixel within the same ten images were monitored over time
for each distance. To quantify the temporal noise, the standard
deviation of depth values across the ten images of a Ô¨Çat
surface was determined. The mean of the standard deviations
of all pixels within the ROI was calculated and used as the
measure of temporal noise. The results of this experiment can
be observed in the table III.
TABLE III
TEMPORAL ERRORS OBTAINED ON A FLATSURFACE BY DIFFERENT
MEMBRANES AT DIFFERENT DISTANCES . [%]
Dist. Intel D405 Trans.Semi-trans.
matteSemi-trans.
reÔ¨Çective
10 cm. 0.29 1.25 0.75 3.72
15 cm. 0.46 0.81 1.42 3.48
20 cm. 0.69 0.72 1.84 3.12
25 cm. 0.81 1.17 1.02 2.86
30 cm. 0.96 1.64 0.89 4.57
The results show that the use of a matte semi-transparent
membrane does not signiÔ¨Åcantly affect the noise level over
time. However, the use of the reÔ¨Çective semi-transparent
membrane adds observable noise. For example, at 10cm, the
temporal noise would be around 3.72 mm. This increase
may be due to the clarity of the membrane, where the reÔ¨Çective
semi-transparent membrane has tiny visible paint spots when
viewed up close by the cameras.
B. Tactile Imprint Experiments
To determine the accuracy and stability of depth measure-
ment when the sensor is in tactile mode, empirical experiments
were conducted by pressing a known-sized Ô¨Çat object onto the
sensor at a repeatable depth. To do this, a 13mm diameter disk
mounted on a Mark-10 Manual Lever Operated Test Stand was
used to ensure perpendicular pressure onto the membrane and
capture a truly Ô¨Çat image. To ensure the penetration depth7
of the disk on the membrane, a Mitutoyo Absolute 543-693
vertical vernier, mounted on the test stand, was employed.
To obtain a reliable estimation of the depth reconstruction
error for each membrane, pressure disks were consecutively
pressed onto the sensor 30 times at random locations to a
depth of 1 mm. The methods discussed in section III-C1 were
then used to reconstruct the disk. Depth measurements for each
trial were obtained by taking the average of the depth values
of the Ô¨Çat surface of the disk in the image. Table IV provides
the mean and standard deviation values for each membrane.
TABLE IV
MEAN AND STANDARD DEVIATION OF DEPTH MEASUREMENTS ‚Äî1 MM
INDENTATION WITH A DISK ONTO EACH MEMBRANE 30TIMES . [MM.]
Membrane type Mean Std
Clear 0.915 0.179
Semi-ReÔ¨Çective 0.837 0.085
Semi-Matte 0.6143 0.121
Opaque-ReÔ¨Çective 1.094 0.071
Opaque-Matte 0.853 0.091
The mean values are generally below 1mm for all mem-
branes except for those obtained using the Opaque-ReÔ¨Çective
membrane. This may be due to several factors. For example,
since a neural network was employed to obtain depth mea-
surements, the calibration process of the membranes plays a
crucial role in determining the accuracy of the depth estimates.
The calibration was performed using a sphere with a textured
surface, and it is possible that the estimation of depth on
the smooth surface of the disk altered the perspective of
the measurements. However, it is worth noting that the two
reÔ¨Çective membranes yielded the smallest standard deviations
in measurement, indicating that depth measurement using them
resulted in more consistent outcomes.
C. Discussion
Fig. 9 showcases a qualitative comparison of StereoTac‚Äôs
3D vision capabilities using the three see-through membranes.
Stereoscopic assessments have noted that semi-reÔ¨Çective mem-
branes generally undermine depth measurement reliability.
The errors detected are typically limited to a maximum of 2
centimeters over a distance of 30 cm. While these errors bear
signiÔ¨Åcance in the context of precise robotic grasping, real-
time depth readings are still feasible to estimate the actual
depth required to reach a target. Moreover, the utilization
of stereoscopy for depth sensing is highly inÔ¨Çuenced by
ambient brightness in the scene. Depending on the brightness
variability of the task, integrating external LEDs to the sensor
could adjust the ambient brightness for uniform readings.
Additionally, the membrane‚Äôs cleanliness can impact the depth
reading‚Äôs precision. For example, handling oily or dirty objects
may leave residue on the membrane, which could potentially
hinder reliable readings.
Concerning the tactile properties of the membranes, it can
be qualitatively observed from Fig. 7 that semi-transparent
membranes offer 3D reconstructions suitable for object recog-
nition during grasping. The empirical results of experiments
presented in Table IV indicate that the use of reÔ¨Çective
semi-transparent membranes provides the best reconstruction
reliability for our sensor type.
However, it is important to note that the use of transparent
or semi-transparent membranes comes with noise effects that
TransparentSemi-trans. 
reflectiveSemi-trans. matte2D 3DFig. 9. Comparison of 2D and 3D perception through different membranes.
For a more comprehensive understanding of the sensor‚Äôs 3D vision capabili-
ties, the reader is invited to refer to the accompanyingvideo.
would not be present with opaque membranes. A fraction of
the sensor‚Äôs internal illumination escapes through the semi-
transparency of the membrane, and depending on the object‚Äôs
color, incident angle or reÔ¨Çectivity, depth readings may be
altered. The red or blue color rays emitted from the sensor
can reÔ¨Çect off the object and return to the sensor, marginally
modifying the illumination readings, as exempliÔ¨Åed in Fig. 10.
Diffuser*
Fig. 10. The effect of semi-transparent / transparent coating on a soft
elastomer. In the top-right corner: for an elastomer coated with opaque
reÔ¨Çective paint, only the red light ray will be reÔ¨Çected to the camera,
while only the orange light ray will be seen when a completely transparent
membrane is used. When the coating is semi-transparent, both red and orange
rays will be seen by the camera.
This phenomenon was created on purpose and is observable
in Fig. 11. In this scenario, we suspended a reÔ¨Çective bolt 18
Fig. 11. Example of contact measuring error with a reÔ¨Çective object held at
1mm. from the membrane. 1) ReÔ¨Çective bolt. 2) Transparent membrane. 3)
Semi-transparent opaque membrane. 4) Semi-transparent reÔ¨Çective membrane.
mm above the membrane, which corresponds to the longest
distance away from the sensor at which gradients could be
perceived. The red and blue lights emitted from the sensor
hit the reÔ¨Çective surfaces of the bolt and returned to the
sensor, causing an inaccurate measurement. The transparent
membrane is obviously more susceptible to this issue, but
semi-transparent membranes are also slightly affected.
V. C ONCLUSION
This article presented the development of StereoTac, a
novel tactile sensor with 3D vision capabilities. The feasibility
of combining multiple modalities for robotic manipulation
in a single sensor was demonstrated by incorporating a se-
cond camera inside a visuo-tactile sensor and using a semi-
transparent contact membrane. Reliable 3D reconstruction of
tactile imprints was shown to be achievable despite the semi-
transparency of the membrane by capturing contact gradients
in a sequential manner, as demonstrated by the results.
Nevertheless, several areas of improvement will be explored
as future work to improve different aspects of the sensor. For
example, tactile imprints were reconstructed using only one
camera in this work. However, using the two available cameras
of the sensor for this task could potentially increase the accu-
racy and conÔ¨Ådence level of the reconstruction. Investigating
the use of stereoscopy for contact reconstruction, as recently
done in [16], [17], would be interesting and promising.
Furthermore, it was observed that the 3D view of the sensor
environment was less reliable when using reÔ¨Çective semi-
transparent membranes. Although no advanced preprocessing
of the images from the two cameras was performed, it is
possible that the use of image Ô¨Åltering as well as image
reconstruction methods such as autoencoders could reduce
the noise caused by the membrane and improve the depth
estimates. Additionally, depth post-processing techniques like
temporal Ô¨Åltering or edge-preserving Ô¨Åltering [31] would be
beneÔ¨Åcial in reducing the point cloud distortions generated by
semi-transparent membranes.
REFERENCES
[1] A. Yamaguchi and C. Atkeson, ‚ÄúOptical skin for robots: Tactile sensing
and whole-body vision,‚Äù in Carnegie Mellon University , 07 2017.
[2] C. Chi, X. Sun, N. Xue, T. Li, and C. Liu, ‚ÄúRecent progress in
technologies for tactile sensors,‚Äù Sensors , vol. 18, no. 4, 2018.
[3] K. Shimonomura, ‚ÄúTactile image sensors employing camera: A review,‚Äù
Sensors , no. 18, 2019.
[4] A. Yamaguchi and C. G. Atkeson, ‚ÄúRecent progress in tactile sensing
and sensors for robotic manipulation: can we turn tactile sensing into
vision?,‚Äù Advanced Robotics , vol. 33, no. 14, 2019.
[5] S. Begej, ‚ÄúPlanar and Ô¨Ånger-shaped optical tactile sensors for robotic
applications,‚Äù IEEE Journal on Robotics and Automation , vol. 4, no. 5,
pp. 472‚Äì484, 1988.[6] M. Ohka, H. Kobayashi, J. Takata, and Y . Mitsuya, ‚ÄúAn experimental
optical three-axis tactile sensor featured with hemispherical surface,‚Äù
Journ. of Adv. Mech. Des. Syst. and Man. , vol. 2, pp. 860‚Äì873, 2008.
[7] B. Ward-Cherrier, N. Pestell, L. Cramphorn, B. Winstone, M. E. Gian-
naccini, J. Rossiter, and N. F. Lepora, ‚ÄúThe tactip family: Soft optical
tactile sensors with 3d-printed biomimetic morphologies,‚Äù Soft Robotics ,
vol. 5, no. 2, pp. 216‚Äì227, 2018. PMID: 29297773.
[8] C. Sferrazza and R. D‚ÄôAndrea, ‚ÄúDesign, motivation and evaluation of a
full-resolution optical tactile sensor,‚Äù Sensors , vol. 19, no. 4, 2019.
[9] R. J. Woodham, ‚ÄúPhotometric Method For Determining Surface Orien-
tation From Multiple Images,‚Äù Opt. Eng. , vol. 19, no. 1, 1980.
[10] M. K. Johnson and E. H. Adelson, ‚ÄúRetrographic sensing for the
measurement of surface texture and shape,‚Äù in 2009 IEEE Conference
on Computer Vision and Pattern Recognition , pp. 1070‚Äì1077, 2009.
[11] E. Donlon, S. Dong, M. Liu, J. Li, E. Adelson, and A. Rodriguez,
‚ÄúGelslim: A high-resolution, compact, robust, and calibrated tactile-
sensing Ô¨Ånger,‚Äù in 2018 IEEE/RSJ International Conference on Intel-
ligent Robots and Systems (IROS) , pp. 1927‚Äì1934, 2018.
[12] M. Lambeta, P.-W. Chou, S. Tian, B. Yang, B. Maloon, V . R. Most,
D. Stroud, R. Santos, A. Byagowi, G. Kammerer, D. Jayaraman, and
R. Calandra, ‚ÄúDIGIT: A novel design for a low-cost compact high-
resolution tactile sensor with application to in-hand manipulation,‚Äù IEEE
Robotics and Automation Letters , vol. 5, pp. 3838‚Äì3845, jul 2020.
[13] O. C. Kara, N. Ikoma, and F. Alambeigi, ‚ÄúHysense: A hyper-sensitive
and high-Ô¨Ådelity vision-based tactile sensor,‚Äù 2022.
[14] W. Yuan, S. Dong, and E. H. Adelson, ‚ÄúGelsight: High-resolution robot
tactile sensors for estimating geometry and force,‚Äù Sensors , vol. 17,
no. 12, 2017.
[15] S. Wang, Y . She, B. Romero, and E. H. Adelson, ‚ÄúGelsight wedge:
Measuring high-resolution 3d contact geometry with a compact robot
Ô¨Ånger,‚Äù CoRR , vol. abs/2106.08851, 2021.
[16] J. Hu, S. Cui, S. Wang, C. Zhang, R. Wang, L. Chen, and Y . Li,
‚ÄúGelstereo palm: a novel curved visuotactile sensor for 3d geometry
sensing,‚Äù IEEE Transactions on Industrial Informatics , pp. 1‚Äì10, 2023.
[17] S. Cui, R. Wang, J. Hu, J. Wei, S. Wang, and Z. Lou, ‚ÄúIn-hand object
localization using a novel high-resolution visuotactile sensor,‚Äù IEEE
Trans. on Industrial Electronics , vol. 69, no. 6, pp. 6015‚Äì6025, 2022.
[18] H. Sun, K. J. Kuchenbecker, and G. Martius, ‚ÄúA soft thumb-sized
vision-based sensor with accurate all-round force perception,‚Äù CoRR ,
vol. abs/2111.05934, 2021.
[19] C. Lin, Z. Lin, S. Wang, and H. Xu, ‚ÄúDtact: A vision-based tactile sensor
that measures high-resolution 3d geometry directly from darkness,‚Äù
2022.
[20] A. Yamaguchi and C. G. Atkeson, ‚ÄúImplementing tactile behaviors
using Ô¨Ångervision,‚Äù in 2017 IEEE-RAS 17th International Conference
on Humanoid Robotics (Humanoids) , pp. 241‚Äì248, 2017.
[21] A. Yamaguchi and C. G. Atkeson, ‚ÄúCombining Ô¨Ånger vision and optical
tactile sensing: Reducing and handling errors while cutting vegetables,‚Äù
in2016 IEEE-RAS 16th International Conference on Humanoid Robots
(Humanoids) , pp. 1045‚Äì1051, 2016.
[22] K. Shimonomura, H. Nakashima, and K. Nozu, ‚ÄúRobotic grasp control
with high-resolution combined tactile and proximity sensing,‚Äù in 2016
IEEE Int. Conf. on Rob. and Aut. (ICRA) , pp. 138‚Äì143, 2016.
[23] F. R. Hogan, M. Jenkin, S. Rezaei-Shoshtari, Y . Girdhar, D. Meger,
and G. Dudek, ‚ÄúSeeing through your skin: Recognizing objects with a
novel visuotactile sensor,‚Äù in Proc. of the IEEE/CVF Winter Conf. on
Applications of Computer Vision (WACV) , pp. 1218‚Äì1227, January 2021.
[24] F. R. Hogan, J.-F. Tremblay, B. H. Baghi, M. Jenkin, K. Siddiqi, and
G. Dudek, ‚ÄúFinger-sts: Combined proximity and tactile sensing for
robotic manipulation,‚Äù IEEE R-A Letters , pp. 1‚Äì8, 2022.
[25] Q. K. Luu, D. Q. Nguyen, N. H. Nguyen, and V . A. Ho, ‚ÄúSoft robotic
link with controllable transparency for vision-based tactile and proximity
sensing,‚Äù 2022.
[26] R. Thomasson, E. Roberge, M. Cutkosky, and J. Roberge, ‚ÄúGoing in
blind: Object motion classiÔ¨Åcation using distributed tactile sensing for
safe reaching in clutter,‚Äù in 2022 IEEE International Conference on
Intelligent Robots and Systems (IROS) , 2022.
[27] ‚Äúcamera calibration - package summary.‚Äù http://wiki.ros.org/camera
calibration. Accessed: 2023-03-06.
[28] Q.-Y . Zhou, J. Park, and V . Koltun, ‚ÄúOpen3D: A modern library for 3D
data processing,‚Äù arXiv:1801.09847 , 2018.
[29] J. Doerner, ‚ÄúFast poisson reconstruction in python.‚Äù
https://gist.github.com/jackdoerner/b9b5e62a4c3893c76e4c.
[30] Intel, ‚ÄúCamera depth testing methodology.‚Äù https://dev.intelrealsense.
com/docs/camera-depth-testing-methodology, Jan 2021.
[31] Intel, ‚ÄúDepth post-processing for intel realsense depth cameras.‚Äù
https://dev.intelrealsense.com/docs/depth-post-processing, Jan 2021.