Logistic Regression Equivalence: A Framework for
Comparing Logistic Regression Models Across
Populations
Guy Ashiri-Prossner1,2and Yuval Benjamini1
1Department of Statistics and Data Science, The Hebrew University of Jerusalem
2National Institute for Testing & Evaluation
February 28, 2022
Abstract
In this paper we discuss how to evaluate the diﬀerences between ﬁtted logistic regression
models across sub-populations. Our motivating example is in studying computerized
diagnosis for learning disabilities, where sub-populations based on gender may or may not
require separate models. In this context, signiﬁcance tests for hypotheses of no diﬀerence
between populations may provide perverse incentives, as larger variances and smaller
samples increase the probability of not-rejecting the null. We argue that equivalence
testing for a prespeciﬁed tolerance level on population diﬀerences incentivizes accuracy in
the inference. We develop a cascading set of equivalence tests, in which each test addresses
a diﬀerent aspect of the model: the way the phenomenon is coded in the regression
coeﬃcients, the individual predictions in the per example log odds ratio and the overall
accuracy in the mean square prediction error. For each equivalence test, we propose a
strategy for setting the equivalence thresholds. The large-sample approximations are
validated using simulations. For diagnosis data, we show examples for equivalent and
non-equivalent models.
1arXiv:2303.13330v1  [stat.ME]  23 Mar 20231 Introduction
A common challenge in developing psychometric tools is deciding when to design separate
modelsortestsforspeciﬁcsub-populationsordemographicgroups. Responsestotestsoften
vary along demographic covariates such as sex, age, mother tongue and socio-economical
status (Byrne (1988), Collins and Gleaves (1998)). These variations usually persist even
after conditioning on the concept to be measured (Byrne and van de Vijver 2014). The
population response distribution may also change over time, potentially requiring model
recalibration. Ideally, for a given ability level, the distribution of test scores should be
identical across sub-populations. However, from an organizational standpoint, introducing
and validating tests for any potential sub-population may not be feasible: beyond the
resources required to develop, calibrate and administer multiple exams, there also costs in
having experts trained to interpret the results from multiple models. In high-stakes exams,
the question of model calibration for sub-populations can aﬀect access to higher education
or to jobs in the public sector. It is therefore important to identify transparent metrics
and criteria for similarity between populations in the context of such examinations.
In this paper we focus on diagnostic tests with binary outcomes. We are motivated by
automated exams for detecting learning disabilities in Israeli higher education. Such
diagnostic exams are constructed in a supervised manner, trying to match an annotation
considered to be the ground truth, for example one based on expert assessment. Due to its
popularity, we restrict the analysis to a logistic regression-based classiﬁer (Cramer 2002).
For logistic regression (simply as a classiﬁer, unrelated to the popular DIF detection
method by Swaminathan and Rogers (1990)), a simple way to measure invariance is by
including interaction terms for the inputs to the regression, and comparing the resulting
models (Hosmer Jr et al. 2013).
The inﬂuence of demographic covariates on diagnostic outcomes has been studied ex-
tensively in educational statistics. Under item response theory, Steinberg and Thissen
(2006) developed methods for detecting bias in individual items using eﬀect size, with
respect to sub-populations. Given an item and its responses data for two (or more)
distinct populations, these methods compare (either graphically or by computation) the
diﬀerences between the two estimated item functions. Many other diﬀerential item func-
tioning (DIF) detection methods are available, see Özdemir (2015), Holland and Wainer
2(2012), Magis and De Boeck (2011) and Martinková et al. (2017) for a review. More
generally, the measurement invariance (Meredith 1993) framework is concerned with a
systematic study of the multivariate distributions of latent factors derived from test-scores.
Measurement invariance requires that the association between items or test-scores and
the latent variables derived from them would not depend on demographic covariates
(van de Schoot et al. 2015). Because independence can break at various points along
the analysis, the assessment of measurement invariance follows a cascade of statistical
tests (Putnick and Bornstein (2016), Vandenberg and Lance (2000)) sensitive to ﬁnd
diﬀerences in distribution of the test-scores, the loadings of the factors, biases in the factor
distributions and more.
Consider a prediction model trained on data from sub-population Aand a distinct dataset
from sub-population B. There are two major related problems with using signiﬁcance
tests for deciding whether to use the model ‘as is’, re-ﬁt the model (by adding data from
sub-population B) or ﬁt a separate model. The ﬁrst is that the null hypothesis is probably
never exactly correct: for any meaningful partitioning of a population into sub-populations,
there will be some diﬀerence, perhaps small, in the distribution of test scores given the
categorical classiﬁcation. This is known as the null hypothesis fallacy (Brenner 1985).
As sample-sizes increase, invariance tests become powerful to detect even such minor
diﬀerences. Examining recent developments in the measurement invariance literature we
see that the criterion is usually relaxed by using Bayesian methods for estimating the
biases (Verhagen and Fox 2013) or by using indiﬀerence regions. The second problem is
that often the test-developers incur costs by rejecting invariance. Under the signiﬁcance
testing framework, their incentive may hypothetically be to use a smaller sample size or
less precise measurements so that the null hypothesis would not be rejected.
We propose addressing these two challenges with equivalence testing. Equivalence tests
are designed to ascertain that the diﬀerence between sub-populations does not exceed
a predetermined acceptable threshold. There are several recent works in psychometrics
using equivalence testing for comparing models across groups. Casabianca and Lewis
(2018) oﬀer an equivalence test for the Mantel-Haenszel test of diﬀerential item functioning
(Holland and Thayer 1988); Weigold et al. (2016) use equivalence testing for comparing
the results of paper-and-pencil surveys against computer-administered surveys. Outside
3of psychometrics, equivalence tests have been used to compare coeﬃcients in linear or
generalized linear regression models (Counsell and Cribbie (2015), Jonkman and Sidik
(2009)). Equivalence tests for individual sample predictions have been proposed for the
predicted expected value (W. Liu et al. (2009) and Wei Liu (2010)), log odds (Siqueira
et al. 2008) and probability in binary regressions (Stevens and Anderson-Cook (2017a),
Stevens and Anderson-Cook (2017b)). Most similar to our approach, Dette et al. (2018)
proposes bootstrap-based equivalence test for comparing nonlinear regression models in
terms of the L1ofL2distance between predictions.
In this paper, we develop a more comprehensive approach for comparing logistic regression
diagnostic tests across sub-populations. We identify three distinct stages of equivalence:
1.Descriptive equivalence of models is achieved when two models describe the
relation between predictors and outcome in a similar manner. We will check for
descriptive equivalence by comparing the regression coeﬃcient vectors. Descriptive
equivalence (DE) implies the eﬀect of the corresponding coeﬃcients is suﬃciently
similar.
2.Individual predictive equivalence of models is achieved when two models yield
the similar predictions for a set of observations. We will check for this equivalence by
comparing the log-odds produced by the models on a new set of examples. Individual
predictive equivalence (IPE) implies the outputted predictions remain stable, within
a predeﬁned threshold, even if the models are exchanged.
3.Performance equivalence of models is achieved when the prediction accuracy of
the two models is similar. We will check for this equivalence by comparing the Brier
scores. Performance equivalence (PE) implies that the prediction accuracy would
not be hurt if the models are exchanged.
The three equivalences capture diﬀerent aspects in the development of predictive models.
The choice of equivalence method depends on the particular needs of a researcher or an
organization: Should they wish to test whether the two coeﬃcient vectors are similar,
descriptive equivalence would ﬁt. In case they want to test whether the models produce
similar predictions for a speciﬁc test set, individual predictive equivalence would ﬁt (even
if descriptive equivalence is not achieved or tested for). If they want to test only for
4similar overall accuracy of prediction, performance equivalence would ﬁt (regardless of
the two other methods).
We prove that at appropriate thresholds these equivalences form a cascade, meaning that
descriptive equivalence implies individual predictive equivalence. Similarly, individual pre-
dictive equivalence implies performance equivalence. In practice, the diﬀerent equivalences
correspond to diﬀerent usages of the models, and we therefore expect that equivalence
bounds would be determined separately for each stage. We propose methods for choosing
the equivalence bounds for each stage of the cascade.
The rest of the paper is organized as follows. The remainder of the section will be devoted
to a brief introduction of equivalence testing. In Section 2 we derive equivalence tests
for the three stages in our cascade. For each stage we discuss how the threshold should
be parameterized. Section 3 shows in simulations that the equivalence methods work as
stated. In Section 4, we study the eﬀects of sex on the diagnostic test learning disabilities
used by the Israel Higher Education Council. The data analysed in this section was
re-randomized.
1.1 Equivalence Testing
Equivalence testing (see Wellek (2010) for an introduction) develops statistical test (e.g. for
the diﬀerence between two groups) to demonstrate with high probability that an eﬀect
size is negligible. Equivalence testing provides a conceptual change compared to usual
signiﬁcance testing, as it moves the burden of proof (Dolado et al. 2014). Instead of
assuming no eﬀect and proving that an eﬀect exists, the test inverts the direction of the
hypotheses. Acknowledging that very small deviations from the null cannot be disproven,
equivalence testing requires the researcher to decide on a maximal acceptable diﬀerence
between the populations. Then, the null hypothesis will be that the eﬀect size is at least
/epsilon1. It is up to the researcher to prove the alternative that the eﬀect is smaller.
For example, consider samples of size nfrom two populations: XA
i∼N (µA,1),XB
i∼
N(µB,1)anddenotethediﬀerenceofexpectations µ=µA−µB. Ifthegoaloftheresearcher
is to prove that expectations are not equal, the appropriate hypotheses for signiﬁcance test-
ing areH0:µ= 0, H 1:µ/negationslash= 0and the test is of the form/braceleftBig/vextendsingle/vextendsingle/vextendsingle¯XA−¯XB/vextendsingle/vextendsingle/vextendsingle>z 1−α/2·/radicalBig
2/n/bracerightBig
.
On the other hand, if the goal us to prove that the eﬀect size is smaller than /epsilon1, the
5equivalence testing hypotheses are H0:|µ|≥/epsilon1, H 1:|µ|</epsilon1and the test is of the form
(Wellek (2010), Chapter 4):
/braceleftbigg
|¯xn|</radicalBig
χ2
1,α(n·/epsilon12/2)//radicalBig
n/2/bracerightbigg
. (1)
As Robinson et al. (2005) observes, the burden of proof for determining equivalence shifts
to the scientist to reject this hypothesis, or in other words, the scientist needs to show
the diﬀerence between models is small enough to be acceptable. By setting this null
hypothesis, the parameter-region of equivalence grows, rather than narrows, as sample size
increases. In standard (null-eﬀect) hypothesis testing we assume the population means
(for example) to be equal. Then, we use the data to disprove our null hypothesis of no
diﬀerence. In equivalence testing we assume that the population means diﬀer (by at least
/epsilon1). Then, we use the data to prove equality. Moreover, equivalence testing changes the
way a study should be designed, as explained by Walker and Nowacki (2011): The need
to determine the equivalence threshold /epsilon1beforeany data is collected might be the most
important change. By inverting the hypotheses direction, equivalence testing also bounds
the probability of mistakenly ﬁnding the populations to be equivalent (Barker et al. 2002).
2 Methods
The goal of this work is to provide a framework for comprehensive comparison of two
logistic regression models, overcoming common problems. We propose a cascading set of
equivalence tests, inspired by the measurement invariance framework. The equivalence
tests we develop for the coeﬃcient vectors and the log-odds-ratio in the logistic regression
are based on their asymptotic distributions (see Peng et al. (2002) for an introduction).
2.1 Data Structure and Notations
For a sample of nindividuals, let (X1,...,Xp−1)i∈Rp−1be the vector of covariates for
individuali,X0i= 1the intercept and Yi∈{0,1}the binary response of interest, for
i= 1,...,n. The logistic regression model is deﬁned as
6E[Yi|Xi=xi] =P(Yi= 1|Xi=xi) =eβTxi
1 +eβTxi.
For a given ˆβwe denote the linear predictor by ˆθi=ˆβTxi, and the predicted probability
byˆπi=eˆθi
1+eˆθi.
We assume that our data consists of samples from two sub-populations, denoted by Aand
B. The samples of population Aare split into distinct trainandtestsets (In practice,
these sets may be collected separately). A logistic regression model MAis ﬁt using the
training data ( Xtrain
A,ytrain
A). The model can be described by its coeﬃcient vector ˆβAand
its coeﬃcients’ covariance matrix VA. The data of a population B(XB,yB) will yield
modelMBthat can be described by ˆβB,VB.
For comparing the predictions, we look to the vectors of linear predictors and binary
predictions that are associated with two diﬀerent coeﬃcient vectors and a single test
population. We use the superscript to denote the coeﬃcient vector, and the subscript to
denote the test population. Hence, the vector of linear predictors outputted by using MA
on thetestset of population A(Xtest
A) is denoted ˆθA
A, the vector of output probabilities
is denoted ˆπA
Aand the vector of binary predictions is denoted ˆyA
A. By using MBon the
sametestset of population A(Xtest
A) we will obtain ˆθB
A,ˆπB
A,ˆyB
A. Although we do not use
ˆyin our methods, it is notes here for the sake of completeness.
We suggest the following comparisons:
1.Descriptive equivalence - Compare the coeﬃcient vector estimates ˆβA,ˆβBin order to
compare the eﬀects of the diﬀerent covariates on the predictor ˆθ. The test accounts
for the sampling distribution of the training sets, characterized by the covariances
VAandVB.
2.Individual predictive equivalence - Compare the linear predictors of two trained
models on a single test population. ˆθA
A,ˆθB
A. The test accounts for the sampling
distribution of the test sets, conditional on the estimated models.
3.Performance equivalence - Compare the overall prediction accuracy (yi−ˆπi)of
the two models for a single test population. The test accounts for the sampling
distribution of the test sets, conditional on the estimated models.
7Figure 1: Data structure. Solid lines indicate splitting of population data; double lines indicate
data used for training a model; dashed lines indicate data used as model input; dotted lines
indicate model outputs. Subscript indicated population and superscript indicates model.
The three tests oﬀer diﬀerent perspectives of model equivalence. Much like in the
measurement equivalence framework, they do follow a cascading order:
Proposition 2.1. Descriptive equivalence implies individual predictive equivalence, using
a proper equivalence threshold.
More speciﬁcally, descriptive equivalence of models with equivalence threshold /epsilon1βimplies
individual predictive equivalence over test set Xof sizemwith equivalence threshold
/epsilon1θ=/epsilon1β√λ1/radicalBig
µXTµX+tr(ΣX), whereµXis the expected value vector of the test set
covariates and ΣXis its covariance matrix.
Proposition 2.2. Individual predictive equivalence implies performance equivalence, using
a proper equivalence threshold.
More speciﬁcally, individual predictive equivalence of models over test set Xof sizem
wirh equivalence threshold /epsilon1θimplies performance equivalence over the same test set with
equivalence threshold /epsilon1B= exp(2/epsilon1θ).
The proofs appear in Section 6.1.
We will use equivalence tests in order to provide robust performance with large sample
8sizes. In addition to specifying the required signiﬁcance level α, equivalence testing
requires specifying an equivalence threshold /epsilon1. As can be seen in (1), the/epsilon1value should
be selected in terms of the test statistic distribution rather than the data itself, which
might not be intuitive to the practitioner. We therefore identify for each test a sensitivity
levelparameterδthat can be set externally, and a conversion function /epsilon1=f(δ)to set the
equivalence threshold for the test. Each of the suggested methods is accompanied by a
suggestion for choosing a proper δvalue (which in turn yields an equivalence threshold /epsilon1).
2.2 Descriptive Equivalence: Testing Regression Coeﬃcients
In the logistic regression model, the regression coeﬃcients code the eﬀects of each covariate
on the outcome. The coeﬃcient vectors are therefore the strongest indication that the
prediction models describe the relation of the response to the covariates in a similar
manner. Let βAandβBbe the coeﬃcient vectors obtained from logistic regression models
on populations AandB. Denote by q=βA−βBthe diﬀerence between these vectors.
We say models MAandMBobtain descriptive equivalence if the size of qis suﬃciently
small. We describe here a chi-square-based equivalence-test, sensitive to detect dense
diﬀerences between the coeﬃcient vectors. It is not hard to construct a similar test (or
test-family) that is sensitive to diﬀerences in an individual coordinate; see Wells et al.
(2009) and Casabianca and Lewis (2018).
Our test examines the diﬀerence between the logistic regression coeﬃcient vectors that
were estimated on samples from the two populations. In large samples, the maximum
likelihood estimator (MLE) for βis approximately normally distributed around the true
parameter ˆβ˙∼N(β,V). See Hosmer Jr et al. (2013) for further details. The estimated
vectors ˆβAand ˆβBare asymptotically normally distributed. Therefore, the observed
diﬀerence vector ˆq=ˆβA−ˆβBis also normal, ˆq·∼N(q,Vq). The covariance matrix Vq
is the sum of the variance-covariance matrices VA=Cov/parenleftBigˆβA/parenrightBig
andVB=Cov/parenleftBigˆβB/parenrightBig
.
In practice, with large enough samples, VAandVBcan be estimated from the logistic
regression and then Sq=ˆVq=/hatwidestCov(ˆq) =ˆVA+ˆVB.
The equivalence test should reject the null when the observed diﬀerence vector ˆqis small.
We measure the size of the vector using the squared Mahalanobis norm /bardblv/bardbl2
Σ=vTΣ−1v,
where we set Σ =Vq. The size of ˆqis therefore the Wald statistic, W=ˆqTS−1
qˆq;W
9follows a non-centralized chi-square distribution with non-centrality parameter /epsilon12
β=/bardblq/bardbl2
Vq.
To set an equivalence threshold, we set an allowed diﬀerence per coeﬃcient δβi, then
write¯δβ= (δβ0,...,δβp−1)T∈Rp. We substitute Vqwith its consistent estimator Sq, the
equivalence threshold can then be set to the Mahalanobis Sqsize of¯δβ
/epsilon12
β=/bardbl¯δβ/bardbl2
Sq,
and the equivalence region is the set of vectors/braceleftBig
k∈Rp:/bardblk/bardblSq</bardbl¯δβ/bardbl2
Sq/bracerightBig
.
Given the required equivalence threshold /epsilon12
β, we can identify the following hypotheses:
H0:/bardblβA−βB/bardbl2
Vq≥/epsilon12
β, H 1:/bardblβA−βB/bardbl2
Vq</epsilon12
β.
To ensure an α-level test, we set the critical value to the α-percentile of the non-centralized
chi-squared distribution with non-centrality parameter /epsilon12
β:
βAis equivalent to βBifW <χ2
α,p(/epsilon12
β).
2.2.1 Choosing δβ
As covariates might vary in scale and scientiﬁc importance, it is up to the researcher
to specify an appropriate sensitivity level for each covariate. A possible alternative is
scaling of each covariate, then either specifying the sensitivity level in terms of standard
deviations or using a single sensitivity level for all covariates:¯δβ= (δβ,...,δβ)T.
2.3 Individual Predictive Equivalence: Testing Log-Odds Vec-
tors
The response probabilities for each individual can be coded as odds. The logistic regression
model has the odds for an individual ideﬁned asP(yi=1|xi)
P(yi=0|xi)=eθi. The odds ratio for x1,x2
is then deﬁned aseθ2
eθ1=e(x2−x1)Tβ. This could also be used when comparing two estimates
for the parameter vector ( ˆβAagainst ˆβB):eˆθA
i
eˆθB
i=exT
i(ˆβA−ˆβB)=eξi.
Recall that individual predictive equivalence is achieved when two models yield similar
predictions for a ﬁxed set of observations. For logistic regression models, we compare the
10log-odds produced by the two models. The models obtain equivalence if the expectation
of the absolute log-odd diﬀerence is signiﬁcantly smaller than the predeﬁned threshold.
Given a testing population Xtestof sizem, we compare the observed log-odds obtained
from models MAandMB. Usually,Xtestwould be associated with one of the populations
(without loss of generality, population A). In that case, we would consider the log-odds
vector of ˆθA
Ato be the “gold-standard” for Xtest; the test assesses the eﬀect of replacing ˆθA
A
with predictions obtained by applying model MB, which is based on a diﬀerent population,
toXtest. Note that the equivalence test does not privilege MAorMB. However, our
proposalforsettingtheequivalencethreshold, wepreferusingthegold-standardpredictions
(those ofMA).
In the following we assume that the observations in Xtestare a simple random sample
from our population of interest, and that they are independent from the observations used
for training MAandMB. The analysis is conditional on the two models (i.e. on ˆβA,ˆβB),
so that all variability is due to the sampling of Xtest.
For a sample xi∈Xtest,i= 1,...,n, modelMAyields log-odds ˆθA
iand model MByields
ˆθB
i. The odds ratio for xiwould beeˆθA
i−ˆθB
i=eξiand the absolute log-odds ratio is
ξi=/vextendsingle/vextendsingle/vextendsingleˆθA
i−ˆθB
i/vextendsingle/vextendsingle/vextendsingle=/vextendsingle/vextendsingle/vextendsinglexT
iˆq/vextendsingle/vextendsingle/vextendsingle= max

log
eˆθA
i
eˆθB
i
,log
eˆθB
i
eˆθA
i


. (2)
Conditional on ˆβA,ˆβB, the values of the absolute log-odds ratio ξiare independent and
identicallydistributed duetosamplingof individualstotest set. Wedenotethis conditional
distribution G, and letµξandσξbe its mean and standard deviation. Denoting /epsilon1θas
the allowed expected diﬀerence in log-odds ratio, we would like to verify whether µξ</epsilon1θ
using the equivalence testing framework. Assuming the sample is large enough, we use a
one-sided t-test on the absolute diﬀerence of log odds.
As we are interested in equivalence testing, the relevant hypotheses are
H0:µξ≥/epsilon1θ, H 1:µξ</epsilon1θ.
Although the conditional distribution Gis unknown, we rely on the Central Limit Theorem
to claim that the mean absolute log-odds value ¯ξis normally distributed. As its variance
11is unknown, we should use a t-test. The level- αequivalence test is:


√m/parenleftBig¯ξ−/epsilon1θ/parenrightBig
/radicalBig
/hatwidestVar(ξ)<tα,m−1

. (3)
Figure 2 describes our procedure: Two sets of paired ˆθvalues provide us with a vector of log
odd ratios and of absolute log odd ratios. The mean absolute log odds ratio is eventually
compared against the equivalence threshold. Data was simulated using ˆθA
i∼N(0,1)and
ˆθB
i∼N(ˆθA
i,1).
−202
−2−1012
θ^
iAθ^
iBθ^  Scatter  Plot
0.00.10.20.30.4
−2−1012
θ^
iA
−θ^
iBDensityθ^A
−θ^B
  Histogram
ξεθ
 Non− 
 Equivalence 
 Area
0.00.20.40.60.8
0123
ξiDensity|θ^A
−θ^B|  Histogram
Figure 2: (Left) A scatter plot comparing the linear predictors from model B ( ˆθB
i) and model
A (ˆθA
i). (Center) the density of the corresponding diﬀerence in log odds ( ˆθA
i−ˆθB
i). (Right) A
density plot of the absolute diﬀerences ξi=/vextendsingle/vextendsingle/vextendsingleˆθB
i−ˆθB
i/vextendsingle/vextendsingle/vextendsingleincluding the equivalence threshold and
the non-equivalence area. If ¯ξ</epsilon1θ, we would like the test to reject the inequivalence hypothesis
H0.
2.3.1 Choosing δθ
To set the sensitivity level for the individual predictive equivalence δθ, we propose looking
at the distances between the estimated log odds and the classiﬁcation threshold.
Consider vectors ˆθA
A,ˆθB
B, as presented in Figure 1. These are the predicted log-odds for
test setsXtest
A,Xtest
B. These log-odds are obtained using models trained on data from
the same population: Model MAwas trained using Xtrain
A, and its predicted log-odds for
Xtrain
AareˆθA
A.
Consider ˆθto be such vector of mestimated log-odd values, without loss of generality we
choose ˆθ=ˆθA
A. In a calibrated logistic regression, the classiﬁcation of the ithsubject by
12the model ( ˆyi) is 1 when ˆθi>0and 0 otherwise. The absolute log-odds/vextendsingle/vextendsingle/vextendsingleˆθi/vextendsingle/vextendsingle/vextendsingleis minimal
change to the log-odds that could change to the classiﬁcation. We therefore propose
setting the equivalence threshold /epsilon1θto be a small quantile δθ(sayδθ= 0.1) of the observed
distribution of absolute log-odds
/epsilon1θ=/vextendsingle/vextendsingle/vextendsingleˆθ/vextendsingle/vextendsingle/vextendsingle
(⌈δθ·m⌉).
Remark. One way to view the formulation of /epsilon1θis that we set an upper bound on
the fraction of subjects that would change in their classiﬁcation if log-odd estimates of
MAwere replaced by the log-odd estimates of MB. Note that this interpretation is not
guaranteed to hold, because the equivalence test compares the mean absolute log-odds
diﬀerence to /epsilon1θ. If variation across individuals is large, we can still hypothetically get a
larger fraction of individuals that change their classiﬁcation.
Remark. We note that the deﬁnition of individual predictive equivalence does not depend
on the linear logistic regression. Any two prediction models that output results in terms
of probabilities or log-odds can be compared using this framework.
2.4 Performance Equivalence : Testing Brier Scores
Sometimes, a suﬃcient criterion to retain a model ﬁtted to diﬀerent population is its
overall performance in prediction. Two models obtain performance equivalence if the
diﬀerences in their expected prediction loss is suﬃciently small. To evaluate performance
equivalence, we generate prediction values for the same test set from each model. These
values are compared to expert-labeled or ground-truth outcomes using the Brier score.
Similar to the previous section, the performance equivalence test is conditional on the
ﬁtted models.
The Brier score (Brier (1950), Benedetti (2010)) is deﬁned as
BS=1
mm/summationdisplay
i=1(yi−ˆπi)2.
For a suﬃciently large sample size, the CLT states that the sampling distribution of the
Brier score converges to a normal distribution, with Var(BS) =1
mVar((y−π)2)(Bradley
13et al. 2008). Consider two Brier scores BSA,BSB>0for models MA,MB, calculated
over the same dataset Xtestof sizem. We would like to set equivalence thresholds for
their ratio. Without loss of generality we choose the bound the ratioBSB
BSAwith lower and
upper thresholds 0</epsilon1L<1</epsilon1U, so that/epsilon1L<BSB
BSA</epsilon1U. The corresponding equivalence
hypotheses are (Hauschke et al. 2007):
H0:BSB
BSA≤/epsilon1LorBSB
BSA≥/epsilon1U, H 1:/epsilon1L<BSB
BSA</epsilon1U.
According to Hauschke et al. (1999), we reject the null hypothesis if
BSB−/epsilon1L·BSA
/radicalBig
s2
L>tα,m−1andBSB−/epsilon1U·BSA
/radicalBig
s2
U<−tα,m−1
wheres2
Lis the sample variance of BSB−/epsilon1L·BSAands2
Uis the sample variance of
BSB−/epsilon1U·BSA. We use these sample variances (rather than pooled variance) in order to
compensate for possible correlation caused by both Brier scores computed for the same
test set.
In addition to 0</epsilon1L<1</epsilon1U, we further assume /epsilon1U=/epsilon1−1
L=/epsilon1B, similar to the 80/125
rule (see Chow and Liu (2008) for introduction) widely used in bioequivalence. Our
equivalence hypotheses can be written as
H0:E[BSB]
E[BSA]≤/epsilon1−1
BorE[BSB]
E[BSA]≥/epsilon1B, H 1:/epsilon1−1
B<E[BSB]
E[BSA]</epsilon1B.
and the level- αequivalence test is


BSB−/epsilon1−1
B·BSA
/radicalBig
/hatwidestVar(BSB−/epsilon1−1
B·BSA)>t1−α,m−1andBSB−/epsilon1B·BSA
/radicalBig
/hatwidestVar(BSB−/epsilon1B·BSA)<−t1−α,m−1

.
(4)
2.4.1 Choosing δB
To set the sensitivity level δBfor the performance equivalence, we suggest looking at the
distances between the estimated probabilities and the real value of the dependent variable.
14The ratioBSB
BSAreﬂects the change in Brier score caused by using model MBon testing
data from population A. It may be more natural for the user to set the sensitivity level
in terms of the absolute diﬀerence rather than the square diﬀerence. Hence, denoting
δB>1the acceptable score degradation or improvement,/vextendsingle/vextendsingle/vextendsingleyi−ˆπB
i/vextendsingle/vextendsingle/vextendsingle≤δB/vextendsingle/vextendsingle/vextendsingleyi−ˆπA
i/vextendsingle/vextendsingle/vextendsingleor
/vextendsingle/vextendsingle/vextendsingleyi−ˆπB
i/vextendsingle/vextendsingle/vextendsingle≤1
δB/vextendsingle/vextendsingle/vextendsingleyi−ˆπA
i/vextendsingle/vextendsingle/vextendsingle. That is, we take BSA,BSBas equivalent ifBSB
BSA∈/parenleftbigg
1
δ2
B,δ2
B/parenrightbigg
.
The relevant equivalence threshold is then /epsilon1B=δ2
B.
3 Simulation Study
Our simulation demonstrates the performance of the three equivalence tests (descriptive
equivalence, individual predictive equivalence and performance equivalence) for logistic
regression models. We simulate data with diﬀerent eﬀects, sample sizes and eﬀect sizes,
then run the tests with diﬀerent equivalence thresholds.
Our simulation is based on two populations AandB, each characterised by a pair (X,y)
of a covariate matrix and a binary response vector. Descriptive equivalence is compared
directly between the regression models, whereas individual predictive equivalence and
performance equivalence are tested with respect to a third dataset (Xtest,ytest). Each
equivalence test is compared to a standard null-hypothesis signiﬁcance test. For each
method, we measure the proportion for trials in which it had identiﬁed models MA,MBas
equivalent (i.e rejecting the null in equivalence tests, not rejecting the null in signiﬁcance
tests).
The descriptive equivalence method is compared to the deviance test: Use the design
matricesXGr=
1XA
1XB
,XG=
1 1XA0
1 0 0 XB
for thereducedand thefullmodels
respectively, with a gender indicator in the full model. For each of those, a logistic
regression model is ﬁtted and ˆβreduced,ˆβfullare found. The respective likelihood function
values are denoted l(ˆβreduced ),l(ˆβfull)and the test statistic is D= 2(l(ˆβreduced )−l(ˆβfull)).
Denotingdas the diﬀerence in degrees of freedom between the models (in this case d=p),
we get that D∼χ2
d.
The individual predictive equivalence method is compared to the Hosmer-Lemeshow test
(Hosmer Jr et al. 2013): Given the samples (xi,yi), we can classify them to Gdistinct
groups according to the ﬁtted probabilities ˆπi. Next, for each group g∈{1,...,G}we can
15calculate the following: nggroup size; O1gthe number of observed yi= 1events;E1gthe
expected number of yi= 1events; ¯πgthe average ﬁtted probability. The Hosmer-Lemeshow
test statistic is then calculated as H=/summationtextG
g=1{(O1g−E1g)2/(ng¯πg(1−¯πg)}. Using this
test withGgroups, we get that H∼χ2
G−2. The Hosmer-Lemeshow test is conducted
using ResourceSelection (Lele et al. 2019).
The performance equivalence method is compared to a t-test on the diﬀerence of the
Brier scores. Using the terms deﬁned in Section 2.4, let BSA,BSBbe the Brier scores,
we get that the two-sample t-test statistic is T=√m(BSB−BSA)√
Var(BSB−BSA)and the level- α t-test is
/braceleftBig
|T|<t1−α/2,m−1/bracerightBig
.
Simulation Settings
We sample XA,XB,Xtestof lengthn, wherexA
i,xB
i,xtest
i∼N (0,Ip)andp= 3. For
population Awe set the regression coeﬃcients to 1, so that the linear predictors are
θA
i= 1 +/summationtextp
j=1xA
ij, the probabilities πA
i=eθA
i
1+eθA
iand the dependent variable yA
i∼Ber(πA
i).
We set the data for the testpopulation in the same manner. We simulate diﬀerent eﬀects
by settingθB
iandπB
iin diﬀerent ways.
All simulations use n={100,200,...,1000,1500,2000,2500,5000,7500,10000}andα=
0.05, as well as the following sensitivity levels: δβ={0.1,0.25,0.5,1}for descriptive
equivalence, δθ={0.025,0.05,0.1,0.2}for individual predictive equivalence and δB=
{1.01,1.05,1.1,1.2}for performance equivalence. Each combination of sample size n,
eﬀect type and eﬀect size kis simulated 1000 times.
3.1 Log-Odds Multiplicative Eﬀect
In this simulation we set a multiplicative eﬀect on the log-odds, θB
i=ki·/parenleftBig
1 +/summationtextp
j=1xB
ij/parenrightBig
withki∼N(k,0.1). We use eﬀect sizes k={1.01,1.05,1.1,1.25}. This eﬀect is intended
for assessing the sensitivity of the descriptive equivalence method. The results of this
simulation are depicted in Figure 3.
AsθA
i= 1+/summationtextp
j=1xA
ij, we expect the coeﬃcients vector to have the form q= (k−1,...,k−1).
We can see that the descriptive equivalence (DE) method identiﬁes equivalence in most
cases when using δβ>k−1(that is, sensitivity level larger than the actual eﬀect) and
as the sample size is large enough. On the other hand, when using δβ<k−1it fails to
16k=1.01 k=1.05 k=1.1 k=1.25
1021031041021031041021031041021031040%25%50%75%100%
Sample SizeIdentifying Equivalence, %
Method DE, δβ = 0.1 DE, δβ = 0.25 DE, δβ = 0.5 DE, δβ = 1Deviance testDescriptive Equivalence
k=1.01 k=1.05 k=1.1 k=1.25
1021031041021031041021031041021031040%25%50%75%100%
Sample SizeIdentifying Equivalence, %
Method IPE, δθ = 0.025 IPE, δθ = 0.05 IPE, δθ = 0.1 IPE, δθ = 0.2 Hosmer−LemeshowIndividual Predictive Equivalence
k=1.01 k=1.05 k=1.1 k=1.25
1021031041021031041021031041021031040%25%50%75%100%
Sample SizeIdentifying Equivalence, %
Method PE, δB = 1.01 PE, δB = 1.05 PE, δB = 1.1 PE, δB = 1.2 t−testPerformance EquivalenceLog−Odds Multiplicative EffectFigure 3: Testing the performance of diﬀerent comparison methods against sample size, under
log-odds multiplicative eﬀect. DE stands for descriptive equivalence test (with diﬀerent δβ
values), IPE stands for individual predictive equivalence test (with diﬀerent δθvalues), PE
stands for performance equivalence test (with diﬀerent δBvalues).
identify equivalence. The deviance test fails to identify equivalence (i.e not rejecting the
null hypothesis) for large eﬀect size and large sample size.
As the three equivalence methods have a cascading form, we expect that identiﬁcation
of descriptive equivalence (DE) would also imply the identiﬁcation of individual predic-
tive equivalence (IPE) and performance equivalence (PE), upon choosing appropriate
equivalence thresholds. We can see that this is the case for both IPE and PE. We can
17also see that the equivalence methods’ ratio of identifying equivalence (using appropriate
thresholds) increases as sample size grows, while the ratio of identifying equivalence for
the corresponding signiﬁcance tests decreases. Overall, for large eﬀects and/or sample
sizes, the equivalence-based methods perform better than the signiﬁcance-based methods
(in terms of successfully identifying model equivalence).
3.2 Log-Odds Additive Eﬀect
In this simulation we set a additive eﬀect on the log-odds, θB
i=ki+/parenleftBig
1 +/summationtextp
j=1xB
ij/parenrightBig
with
ki∼N(k,0.1). We use eﬀect sizes k={0.05,0.1,0.25,0.5}. This eﬀect is intended for
assessing the sensitivity of the individual predictive equivalence method. The results of
this simulation are depicted in Supplementary Figure 5.
AsθA
i= 1 +/summationtextp
j=1xA
ij, we expect the log-odds ratio to have the form ξi=k. We can see
that the individual predictive equivalence (IPE) method identiﬁes equivalence in most
cases when using a large δθvalues. On the other hand, when using small δθvalues it
fails to identify equivalence. The Hosmer-Lemeshow test fails to identify equivalence (i.e
not rejecting the null hypothesis) for almost all large eﬀect sizes and large sample sizes.
Due to the cascading form of the equivalence methods, we expect that identiﬁcation of
individual predictive equivalence (IPE) would also imply the identiﬁcation of performance
equivalence (PE), upon choosing appropriate equivalence thresholds. We can see that this
is indeed the case.
The additive eﬀect allows us to compare the equivalence boundaries against the actual
MAD ofξ=θB−θA, as depicted in Figure 4. As sample size grows, all the quantiles of
|θA|converge, as well as ¯ξ. The limit of ¯ξis the eﬀect size k, as expected. For a large eﬀect
size such as k= 0.5, we should choose a very lenient equivalence boundary for identifying
individual predictive equivalence (in the above example equivalence is identiﬁed if we let
20% of the samples ﬂip their prediction).
3.3 Probability Multiplicative Eﬀect
In this simulation we set a multiplicative eﬀect on the predicted probabilities. Let
θB
i= 1 +/summationtextp
j=1xB
ij, we setπB
i=ki·eθB
i
1+eθB
iwithki∼N (k,0.1). We use eﬀect sizes
k={1.01,1.1,1.25,1.5}. For the (very rare) cases where πB
i>1we setπB
i= 1−10−6
18k=0.05 k=0.1 k=0.25 k=0.5
1021031041021031041021031041021031040.250.500.75
Sample Sizeθ
 θ  ( 0.025 ) θ  ( 0.05 ) θ  ( 0.1 ) θ  ( 0.2 )ξQuantiles of  θAFigure 4: Diﬀerent quantiles of |θA|and the MAD of θB−θA.
and forπB
i<0we setπB
i= 10−6. This eﬀect is intended for assessing the sensitivity
of the performance equivalence method. The results of this simulation are depicted in
Supplementary Figure 6.
Using a large enough equivalence threshold, the performance equivalence method (PE)
successfully identiﬁes equivalence even for large eﬀect sizes. This is not the case with the
descriptive equivalence (DE) and individual predictive equivalence (IPE) methods, which
fail to identify equivalence under large eﬀect sizes. The t-test fails to identify equivalence
(i.e not rejecting the null hypothesis) for large eﬀect sizes and large sample sizes.
3.4 Error Rate Control
In this simulation we test the three equivalence methods for their error rate control,
under two conditions: no eﬀect at all (equal models) and a log-odds multiplicative
eﬀect of size 1.5 (unequal models). Each condition is simulated 1000 times for n=
{100,1000,10000}andα={0.05,0.1}, using the following sensitivity levels: 0.2 for the
descriptive equivalence method, 0.05 for the individual predictive equivalence method and
1.005 for the performance equivalence method. These levels were chosen for being fairly
strict. The results appear in Table 1. We can see that under no eﬀect (equal models),
the three equivalence methods identify model equivalence when the sample size is large
enough. On the other hand, we see that for unequal models the error rate of the methods
is kept below α.
19Table 1: Error Rates
Equal Models Unequal Models
α= 0.05 α= 0.1 α= 0.05 α= 0.1
DE IPE PE DE IPE PE DE IPE PE DE IPE PE
n= 100 0.06 0.01 0.00 0.10 0.01 0.00 0.02 0 0 0.04 0.01 0
n= 1000 0.26 0.16 0.12 0.46 0.18 0.13 0.00 0 0 0.00 0.00 0
n= 10000 1.00 0.98 1.00 1.00 0.99 1.00 0.00 0 0 0.00 0.00 0
*DE stands for Descriptive Equivalence, IPE for Individual Predictive Equivalence,
PE for Performance Equivalence.
4 Learning Disabilities and MATAL Data Results
A learning disability is deﬁned as a “neurodevelopmental disorder [...] characterized by
persistent and impairing diﬃculties with learning foundational academic skills in reading,
writing, and/or math” (American Psychiatric Association and others 2013). MATAL
(Ben-Simon (2007), Ben-Simon et al. (2008), Ben-Simon (2013)) is a computer-based test
battery for the diagnosis of learning disabilities (dyslexia, dysgraphia, and dyscalculia)
and Attention Deﬁcit & Hyperactivity Disorder (ADHD) for applicants to Israeli higher
education institutions and for currently enrolled students. MATAL was developed by the
Israeli NationalInstitute for Testing & Evaluation incooperationwith the IsraeliCouncil of
Higher Education, as part of an endeavor to develop policy and procedure for standardizing
and regulating the diagnosis of learning disabilities in higher education and the provision
of test accommodations. Since its inauguration in 2007, the MATAL system has diagnosed
more than 40,000 enrolled students and applicants to higher education (around 5,000
yearly). It became the standard diagnosis tool for most Israeli higher education institutions.
The MATAL system consists of 22 standardized neuropsychological tests, whose results
are summarized into 10 functioning scores. The system provides its predictions regarding
each disability using logistic regression models, with the functioning scores as covariates.
Experts are trained to incorporate the computerized diagnosis with the various test scores
and the background information provided in reaching their decision.
A system such as MATAL provides a prime example where any change to the system
would involve a great scientiﬁc cost: Experts have been trained and gained experience
incorporating results of the system in their decision making process. The diagnosis
data collected has been used for further neuropsychological research regarding learning
20disabilities and cognitive functioning. Changing the model might impair the reliability of
previous diagnoses and long-term studies. It will also require the experts to relearn how
to interpret the outcomes. Therefore, any change to the underlying diagnosis model would
involve both scientiﬁc and inter-organization costs. We will examine the decision-making
process when determining whether a new population is similar enough for there to be no
need of re-estimating the model.
We will use the MATAL data to study the regression models for two sub-populations:
male and female students. For two disabilities, dysgraphia and dyscalculia, we will ﬁt
separate models for each sub-population. We will then use the diﬀerent Equivalence tests
to verify or refute the equivalence of the two populations for these disabilities.
Data Generation
The original MATAL data incorporates personal and proprietorial information, so the
following results are based on data regenerated according to the estimated distributions.
For each response and each gender, we estimated the joint distribution of 5 functioning
scores using a Gaussian copula (Joe 2014) with Gamma marginal distributions. The
full regeneration process is explained in Section 6.3. We resampled data points for each
category, using ntrain
female =ntrain
male= 3000for two disabilities: dysgraphia and dyscalculia.
Each sub-sample was used to construct logistic regression models for dysgraphia and dyscal-
culia. The prediction of each disability is made using 10 functioning scores as predictors.
Each disability uses diﬀerent predictors, according to their relevance (e.g. quantitative skill
is used only for predicting dyscalculia, whereas verbal ﬂuency is used only for predicting
dysgraphia). The following comparisons relate to multiple combinations of disability and
gender. We use α= 0.05and sensitivity levels δβ= 0.1for the descriptive equivalence,
δθ= 7.5%for the individual predictive equivalence method and δB= 1.1for the perfor-
mance equivalence. The individual predictive equivalence and performance equivalence
methods were tested with resampled testing datasets ( ntest
female =ntest
male= 1000).
Results
The coeﬃcients for the female and male dysgraphia models are presented in Table 2,
whereas the coeﬃcients for dyscalculia are in Table 3. The Brier scores for all models are
21Table 2: Coeﬃcients for Dysgraphia Models
(Intercept) x1x2x3
Female model -3.022 -0.090 0.248 -0.263
Male model -3.261 -0.049 -0.042 -0.197
ˆq 0.240 -0.042 0.290 -0.066
Table 3: Coeﬃcients for Dyscalculia Models
(Intercept) x4x5
Female model -3.189 -0.430 -7.027
Male model -2.121 -0.387 -5.614
ˆq -1.068 -0.043 -1.414
presented in Table 4. Dysgraphia models were found to be equivalent: The descriptive
equivalence test, the individual predictive equivalence test and the performance equivalence
test have found the models equivalent for the female data, as well as the t-test for the
Brier scores. For Dyscalculia, we found only performance equivalence. A brief discussion
of the results follows, along with their compliance to previous ﬁndings. The full result are
found in Tables 5 and 6.
Dysgraphia
For dysgraphia, we ﬁnd a repeating pattern in which the equivalence tests ﬁnd equivalence,
whereas the usual signiﬁcance tests sometimes rejected the null-hypothesis of no-diﬀerence.
The descriptive equivalence test for the selected δβandαvalues does reject the null
hypothesis H0:/vextenddouble/vextenddouble/vextenddoubleˆβmale−ˆβfemale/vextenddouble/vextenddouble/vextenddouble
Σ≥λβ, meaning the models describe dysgraphia in an
equivalent manner. The Mahalanobis distance between the dysgraphia coeﬃcient vectors
is 87.5, while the allowed distance for our choice of δβ= 0.1is 505.5. The deviance test for
the gender variable rejects the hypothesis “ xgenderdoes not improve the model”, meaning
the models diﬀer for our choice of α= 0.05.
Table 4: Brier Scores for Dysgraphia and Dyscalculia Models
Dysgraphia Dyscalculia
Male Model Female Model Male Model Female Model
Male Data 0.1060 0.1164 0.1103 0.1259
Female Data 0.1216 0.1086 0.1133 0.1066
22Table 5: Summary of Tests for Dysgraphia Models
Method Type Test Set /epsilon1 C αTest Stat. P-value Models Diﬀer?
DE (δβ= 0.1) Equiv. - - 437.148 87.481 0.000 No
Deviance Test Signif. - - 9.488 91.548 0.000 Yes
IPE (δθ= 7.5%) Equiv. Female 0.528 -1.646 -1.983 0.024 No
Hosmer-Lemeshow Signif. Female - 15.507 255.914 0.000 Yes
IPE (δθ= 7.5%) Equiv. Male 0.435 -1.646 14.464 1.000 Yes
Hosmer-Lemeshow Signif. Male - 15.507 552.907 0.000 Yes
PE (δB= 1.1),tLEquiv. Female 1.21 1.962 2.869 0.002 No
PE (δB= 1.1),tUEquiv. -1.962 -10.150 0.000
Briert-test Signif. Female - 1.646 -7.718 0.000 Yes
PE (δB= 1.1),tLEquiv. Male 1.21 1.962 8.303 0.000 No
PE (δB= 1.1),tUEquiv. -1.962 -3.195 0.001
Briert-test Signif. Male - 1.646 11.410 0.000 Yes
*DE stands for Descriptive Equivalence, IPE for Individual Predictive Equivalence, PE for
Performance Equivalence.
Table 6: Summary of Tests for Dyscalculia Models
Method Type Test Set /epsilon1 C αTest Stat. P-value Models Diﬀer?
DE (δβ= 0.1) Equiv. - - 11.783 112.715 1.000 Yes
Deviance Test Signif. - - 7.815 120.510 0.000 Yes
IPE (δθ= 7.5%) Equiv. Female 0.273 -1.646 74.159 1.000 Yes
Hosmer-Lemeshow Signif. Female - 15.507 345.777 0.000 Yes
IPE (δθ= 7.5%) Equiv. Male 0.338 -1.646 93.567 1.000 Yes
Hosmer-Lemeshow Signif. Male - 15.507 604.883 0.000 Yes
PE (δB= 1.1),tLEquiv. Female 1.21 1.962 4.366 0.000 No
PE (δB= 1.1),tUEquiv. -1.962 -9.463 0.000
Briert-test Signif. Female - 1.646 3.406 0.000 Yes
PE (δB= 1.1),tLEquiv. Male 1.21 1.962 9.518 0.000 No
PE (δB= 1.1),tUEquiv. -1.962 -2.759 0.003
Briert-test Signif. Male - 1.646 -0.419 0.338 No
*DE stands for Descriptive Equivalence, IPE for Individual Predictive Equivalence, PE for
Performance Equivalence.
When testing the male model’s ﬁt for the female data, we observe diﬀerences between the
signiﬁcance test and equivalence test: The individual predictive equivalence test ﬁnds the
models equivalent for the female data, while Hosmer-Lemeshow goodness-of-ﬁt test rejects
the null hypothesis (that the male model ﬁts the female data). Our choice of δθ= 7.5%
yields/epsilon1θ= 0.528while ¯ξ= 0.498. This means a negative numerator and a negative test
23statistic, so the inequivalence hypothesis is rejected.
When testing the female model’s ﬁt for the male data, both the individual predictive
equivalence test and the Hosmer-Lemeshow goodness-of-ﬁt test have found the models
inequivalent. We got δθ= 0.435and¯ξ= 0.678. This means our choice of δθ= 7.5% is too
strict for the male data.
The performance equivalence testhas foundthe models equivalentfor both male andfemale
testing datasets, while the t-test for the Brier scores did not ﬁnd ﬁnd any equivalence. We
have setδB= 1.1as the sensitivity level for the performance equivalence test, meaning
we tolerate Brier ratios in [0.826,1.21]. The observed Brier ratio/parenleftBig
BSmale
BSfemale/parenrightBig
is 1.12 for the
female test data and by 0.911 for the male test data. It comes as no surprise that the
Brier scores were found equivalent in both cases.
Overall, itseemsthatthemodelsareequivalentindescribingthephenomenonofdysgraphia
and perform equivalently in terms of prediction for the female data.
Berninger and O’Malley May (2011) has found males to be “consistently more impaired”
than females in orthographic skills. The latter is known as a key factor in dysgraphia.
fMRI studies in that research have found gender diﬀerence in brain activation only in
regions associated with orthographic processing. However, no gender diﬀerences in brain
activation were observed on other writing tasks. This might imply some gender similarities
in dysgraphia, which explains the descriptive equivalence and predictive equivalence
achieved for the female data.
Dyscalculia
The descriptive equivalence test for the selected δβandαvalues does reject the null
hypothesis H0:/vextenddouble/vextenddouble/vextenddoubleˆβmale−ˆβfemale/vextenddouble/vextenddouble/vextenddouble
Σ≥λβ, meaning the models describe dyscalculia in a
diﬀerent manner. The deviance test for the gender variable does not reject the hypothesis
“xgenderdoes not improve the model”, meaning the models diﬀer for our choice of α= 0.05.
The individual predictive equivalence test ﬁnds the models ineqiuvalent for both the
female and male data. This means our choice of δθ= 7.5% might be too strict for the
data. When testing the male model’s ﬁt for the female data, the Hosmer-Lemeshow
goodness-of-ﬁt test rejects the null hypothesis (that the male model ﬁts the female data).
The same result is obtained when testing the female model’s ﬁt for the male data.
24The performance equivalence tests for both the male and female datasets has found the
models equivalent, as well as the t-test for the Brier scores over the male test data. This
means our choice of δB= 1.1% might be too lenient for the data.
Overall, it seems that the models are describing the phenomenon of dyscalculia in a
diﬀerent manner and diﬀer in their individual predictions. The equivalence of Brier scores
might be caused by the limited range of Brier scores.
The gender inequivalence for dyscalculia complies with Devine et al. (2013), which found
that using discrepancy thresholds “signiﬁcantly more girls than boys could be deﬁned as
having developmental dyscalculia”.
5 Discussion
This work addresses the problem of assessing the suitability of using a single logistic regres-
sion model for diﬀerent populations. Although no two populations are identical, ﬁtting
a diﬀerent model for each population requires considerable scientiﬁc and administrative
costs. Because identifying equivalent models is the goal of this investigation, we argue
that the burden should be on the scientist to show that the models are equivalent up
to a speciﬁed sensitivity level. We develop an equivalence testing framework for logistic
regression models ﬁtted to two diﬀerent populations. This framework consists of three
diﬀerent equivalence tests: (a) descriptive equivalence, which compares the models’ coeﬃ-
cient vectors; (b) individual predictive equivalence, which compares the models’ log-odds
estimate vectors; and (c) performance equivalence, which compares the models’ average
prediction accuracy using Brier scores. The proposed tests are based on asymptotic
normality and are formed by adding equivalence regions and inverting the direction of the
rejection regions.
The usage of the proposed methods is subject to the research goal of the scientist: if it is
to show that the models describe a certain phenomenon in a similar manner, then the
descriptive equivalence is the appropriate method; if it is to show that the models produce
equal predictions for a given dataset, then the individual predictive equivalence is the
appropriate method; if it is to show that the overall prediction accuracy is similar, then
the performance equivalence is the appropriate method.
25The three methods also form a cascade, as discussed in Section 2.1. Achieving a certain
level of equivalence implies ‘weaker’ forms of equivalence (see Section 6.1). On the
other hand, ﬁnding two models inequivalent (that is, not rejecting the null hypothesis
of an equivalence test) does not necessarily mean that ‘weaker’ forms of equivalence are
unobtainable. An example of such case can be found in Section 4: The male and female
models for diagnosing dyscalculia only achieve performance equivalence.
These three methods aren’t a full framework, and more comparison steps should be
suggested. Currently it is up for the researcher to decide what is an acceptable equivalence
threshold, which was identiﬁed as a “key methodological issue” of equivalence testing by
Greene et al. (2008). When using more than one method, it might be advised to apply a
multiplicity correction to the signiﬁcance level, such as the Bonferroni correction or the
Holm-Bonferroni method.
The methods described in this paper could be compared to some recent work: Unlike the
equivalence methodology proposed by Dette et al. (2018), we use asymptotic parametric
tests in this work (rather than bootstrap-based). In addition, the tests suggested use dif-
ferent metrics: The descriptive equivalence and individual predictive equivalence methods
compare models by using Mahalanobis and L1-distances (respectively) rather than L2or
L∞. Moreover, we show cascading order between the distances we use.
The Mantel-Haenszel test counts cases according to the predicted binary output versus
the actual output, much like the accuracy score1
m/summationtextm
i=1I{ˆyi=yi}. One advantage of the
performance equivalence method over the Mantel-Haenszel equivalence (Casabianca and
Lewis 2018) is the usage of proper scoring rule, and another possible advantage is the use
of a parametric test.
For individual predictive equivalence, we proposed a method for choosing the threshold by
looking at the ﬁtted data. Although this choice is a bit unorthodox, we believe it allow
greater ﬂexibility for the investigator. Moreover, as diﬀerences of logits or Brier scores have
no meaning outside context (unlike boundaries set for the descriptive equivalence method),
using percentile-based boundaries make these methods usable even for investigators who
are not very familiar with statistical methodology. It is much easier to state “I am willing
to let 5% of the samples ﬂip their prediction” than to use an allowed diﬀerence in logits,
or use a pre-set boundary. The same applies for the performance equivalence method - “I
26will tolerate deviations of up to 20% in the Brier scores” is easier that specifying a value.
One limitation of this work is that it is based on the normal approximation of the logistic
regression estimates for the coeﬃcients and the log-odds. Another one is the normal
approximation of the Brier score, based on its MSE structure, although its full sampling
distribution is known. Section 4 results also suggest that the log-odds equivalence method
might be too strict. The proposed equivalence methods are compared to very common
signiﬁcance tests, while there might be other comparable methods out there with better
performance.
Despite these drawbacks, this work does present a new approach towards to comparison of
logistic regression models: The combination of diﬀerent methods oﬀers a comprehensive
view of the models, and the usage of equivalence testing encourages using large sample
sizes. Even as is, this framework could help researchers perform meta-analysis of results,
as measurement invariance is used for factor analysis results (for example Byrne et al.
(1989); Collins and Gleaves (1998)). The ideas presented in this work could be used
for some possible further developments: (a) introduction of new comparison methods
between logistic regression models, such as comparison of the outputted probabilities;
(b) incorporating equivalence tests in the context of assessing measurement invariance
between two factor analysis models; (c) extending the proposed methods for comparison
of other classiﬁer types; (d) using the proposed methods in the psychometric context to
provide new insights regarding diﬀerential item functioning (DIF).
Acknowledgements
The authors would like to thank Anat Ben-Simon and Yoel Rapp for their long-term
support of this research, as well as Henry Braun for helpful discussions. The authors
would also like to express their gratitude towards three reviewers, who made constructive
comments on an earlier version of this work.
Equivalence methods conducted using LogRegEquiv (Ashiri-Prossner 2022). Figures
created using ggplot2 (Wickham et al. 2019), gridExtra (Auguie 2017) and latex2exp
(Meschiari 2021). Tables were created using kableExtra (Zhu 2019). Document authoring
using rmarkdown (Allaire et al. 2019), knitr(Xie 2019a) and bookdown (Xie 2019b).
276 Supplements
6.1 Cascading Order of Equivalence
Proposition. Descriptive equivalence implies individual predictive equivalence, using a
proper equivalence threshold.
Proof.Let models MA,MBachieve descriptive equivalence. That is, there exists some
/epsilon1β>0for which/bardblˆβA−ˆβB/bardbl2
Sq< /epsilon12
β, where ˆq=ˆβA−ˆβBandSq=Cov(ˆq). We can
write/vextendsingle/vextendsingle/vextendsingleˆθA
i−ˆθB
i/vextendsingle/vextendsingle/vextendsingle=/vextendsingle/vextendsingle/vextendsinglexT
iˆq/vextendsingle/vextendsingle/vextendsingle. GivenSq, we denote λ1> ... > λ pas its eigenvalues. We can
then bound the squared Mahalanobis norm using the L2norm: Let c∈Rp, we get
/bardblc/bardbl2≤λ1/bardblc/bardbl2
Sq(Jensen 1997). Using the power norm inequality we get
|c|≤/bardbl c/bardbl≤/radicalBig
λ1/bardblc/bardblSq
then using the Cauchy-Schwarz inequality, we get
/vextendsingle/vextendsingle/vextendsinglexT
iˆq/vextendsingle/vextendsingle/vextendsingle≤/bardblxi/bardbl·/bardblˆq/bardbl≤/bardblxi/bardbl/radicalBig
λ1/bardblˆq/bardblSq≤/epsilon1β/radicalBig
λ1/bardblxi/bardbl.
Using Jensen’s inequality we can write
/vextendsingle/vextendsingle/vextendsingleθA
i−θB
i/vextendsingle/vextendsingle/vextendsingle=/vextendsingle/vextendsingle/vextendsingleE/bracketleftBig
xT
iˆq/bracketrightBig/vextendsingle/vextendsingle/vextendsingle≤E/bracketleftBig/vextendsingle/vextendsingle/vextendsinglexT
iˆq/vextendsingle/vextendsingle/vextendsingle/bracketrightBig
≤/epsilon1β/radicalBig
λ1E[/bardblxi/bardbl].
The function g(x) =x2is strictly convex, so using Jensen’s inequality we can write
E[/bardblxi/bardbl]2≤E/bracketleftBig
/bardblxi/bardbl2/bracketrightBig
and ﬁnally
E/bracketleftBig/vextendsingle/vextendsingle/vextendsingleθA
i−θB
i/vextendsingle/vextendsingle/vextendsingle/bracketrightBig
≤/epsilon1β/radicalBig
λ1/radicalbigg
E/bracketleftBig
/bardblxi/bardbl2/bracketrightBig
.
Assuming test set Xof sizem, denoteµXas its expected value vector and ΣXas its
covariance matrix, we get
E/bracketleftBig/vextendsingle/vextendsingle/vextendsingleθA−θB/vextendsingle/vextendsingle/vextendsingle/bracketrightBig
≤/epsilon1β/radicalBig
λ1/radicalBig
µXTµX+tr(ΣX),
meaning individual predictive equivalence is achieved. 
28Proposition. Individual predictive equivalence implies performance equivalence, using a
proper equivalence threshold.
Proof.Assume test set Xtestof sizemand let models MA,MBachieve individual predictive
equivalence with respect to Xtest. That is, for a given signiﬁcance level αthere exists
some/epsilon1θ>0for which the test in Equation (3) does reject the null hypothesis. This means
we can bound the mean absolute diﬀerence of log-odds E[|ξi|]</epsilon1θ, which can also be
written asE/bracketleftBig/vextendsingle/vextendsingle/vextendsinglelogit(πB
i)−logit(πA
i)/vextendsingle/vextendsingle/vextendsingle/bracketrightBig
</epsilon1θ. The diﬀerence of logits can be written as
E/bracketleftBig/vextendsingle/vextendsingle/vextendsingle(log(πB
i) + log(1−πA
i))−(log(πA
i) + log(1−πB
i))/vextendsingle/vextendsingle/vextendsingle/bracketrightBig
=E/bracketleftBig/vextendsingle/vextendsingle/vextendsinglelogit(πB
i)−logit(πA
i)/vextendsingle/vextendsingle/vextendsingle/bracketrightBig
</epsilon1θ
Using reverse triangle inequality
E/bracketleftBig/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglelog(πB
i) + log(1−πA
i)/vextendsingle/vextendsingle/vextendsingle−/vextendsingle/vextendsingle/vextendsinglelog(πA
i) + log(1−πB
i)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/bracketrightBig
<E/bracketleftBig/vextendsingle/vextendsingle/vextendsinglelogit(πB
i)−logit(πA
i)/vextendsingle/vextendsingle/vextendsingle/bracketrightBig
.
As both log(πB
i),log(1−πA
i)are negative, we get
E/bracketleftBig/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglelog(πB
i)/vextendsingle/vextendsingle/vextendsingle−/vextendsingle/vextendsingle/vextendsinglelog(πA
i) + log(1−πB
i)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/bracketrightBig
<E/bracketleftBig/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglelog(πB
i) + log(1−πA
i)/vextendsingle/vextendsingle/vextendsingle−/vextendsingle/vextendsingle/vextendsinglelog(πA
i) + log(1−πB
i)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/bracketrightBig
,
using the triangle inequality
E/bracketleftBig/vextendsingle/vextendsingle/vextendsingle|log(πB
i)|−|log(πA
i)|−|log(1−πB
i)|/vextendsingle/vextendsingle/vextendsingle/bracketrightBig
<E/bracketleftBig/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglelog(πB
i)/vextendsingle/vextendsingle/vextendsingle−/vextendsingle/vextendsingle/vextendsinglelog(πA
i) + log(1−πB
i)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/bracketrightBig
and eventually (this can be shown numerically)
E/bracketleftBig/vextendsingle/vextendsingle/vextendsingle|log(πB
i)|−|log(πA
i)|/vextendsingle/vextendsingle/vextendsingle/bracketrightBig
<E/bracketleftBig/vextendsingle/vextendsingle/vextendsingle|log(πB
i)|−|log(πA
i)|−|log(1−πB
i)|/vextendsingle/vextendsingle/vextendsingle/bracketrightBig
</epsilon1θ.
Next,E/bracketleftBig
|log(πB
i)|−|log(πA
i)|/bracketrightBig
≤E/bracketleftBig/vextendsingle/vextendsingle/vextendsingle|log(πB
i)|−|log(πA
i)|/vextendsingle/vextendsingle/vextendsingle/bracketrightBig
and using linearity
−E/bracketleftBig
|log(πA
i)|/bracketrightBig
≤/epsilon1θ−E/bracketleftBig
|log(πB
i)|/bracketrightBig
. As probabilities lie in (0,1), we get
−E/bracketleftBig
|log(πA
i)|/bracketrightBig
=−E/bracketleftBigg
log/parenleftBigg1
πA
i/parenrightBigg/bracketrightBigg
=E/bracketleftBigg
−log/parenleftBigg1
πA
i/parenrightBigg/bracketrightBigg
=E/bracketleftBig
log(πA
i)/bracketrightBig
soE/bracketleftBig
log(πA
i)/bracketrightBig
≤/epsilon1θ+E/bracketleftBig
log(πB
i)/bracketrightBig
,2E/bracketleftBig
log(πA
i)/bracketrightBig
≤2/epsilon1θ+ 2E/bracketleftBig
log(πB
i)/bracketrightBig
andE/bracketleftBig
(πA
i)2/bracketrightBig
≤
exp(2/epsilon1θ)E/bracketleftBig
(πB
i)2/bracketrightBig
.
29Using the symmetry of absolute values we write
E/bracketleftBig/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglelog(1−πB
i) + log(πA
i)/vextendsingle/vextendsingle/vextendsingle−/vextendsingle/vextendsingle/vextendsinglelog(1−πA
i) + log(πB
i)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/bracketrightBig
<E/bracketleftBig/vextendsingle/vextendsingle/vextendsinglelogit(πB
i)−logit(πA
i)/vextendsingle/vextendsingle/vextendsingle/bracketrightBig
,
then apply the same steps as before to get E/bracketleftBig
(1−πA
i)2/bracketrightBig
≤exp(2/epsilon1θ)E/bracketleftBig
(1−πB
i)2/bracketrightBig
.
This can also be written as E/bracketleftBig
(yi−πA
i)2/vextendsingle/vextendsingle/vextendsingleyi= 1/bracketrightBig
≤exp(2/epsilon1θ)E/bracketleftBig
(yi−πB
i)2/vextendsingle/vextendsingle/vextendsingleyi= 1/bracketrightBig
.
Multiplying both sides by P(Y= 1)we getE/bracketleftBig
(yi−πA
i)2/vextendsingle/vextendsingle/vextendsingleyi= 1/bracketrightBig
P(Y= 1)≤
exp(2/epsilon1θ)E/bracketleftBig
(yi−πB
i)2/vextendsingle/vextendsingle/vextendsingleyi= 1/bracketrightBig
P(Y= 1). The bound E/bracketleftBig
(πA
i)2/bracketrightBig
≤exp(2/epsilon1θ)E/bracketleftBig
(πB
i)2/bracketrightBig
can
be written as E/bracketleftBig
(yi−πA
i)2/vextendsingle/vextendsingle/vextendsingleyi= 0/bracketrightBig
P(Y= 0)≤exp(/epsilon1θ)E/bracketleftBig
(yi−πB
i)2/vextendsingle/vextendsingle/vextendsingleyi= 0/bracketrightBig
P(Y= 0).
We add up these two bounds to get
E/bracketleftBig
(yi−πA
i)2/vextendsingle/vextendsingle/vextendsingleyi= 0/bracketrightBig
P(Y= 0) +E/bracketleftBig
(yi−πA
i)2/vextendsingle/vextendsingle/vextendsingleyi= 1/bracketrightBig
P(Y= 1)≤
exp(2/epsilon1θ)E/bracketleftBig
(yi−πB
i)2/vextendsingle/vextendsingle/vextendsingleyi= 0/bracketrightBig
P(Y= 0) + exp(2 /epsilon1θ)E/bracketleftBig
(yi−πB
i)2/vextendsingle/vextendsingle/vextendsingleyi= 1/bracketrightBig
P(Y= 1)
then using Law of Total Expectation:
E/bracketleftBig
(yi−πA
i)2/bracketrightBig
≤exp(2/epsilon1θ)E/bracketleftBig
(yi−πB
i)2/bracketrightBig
and ﬁnally
E/bracketleftBig
(yi−πA
i)2/bracketrightBig
E[(yi−πB
i)2]≤exp(2/epsilon1θ).
In a similar manner, a lower bound can be obtained and we get that the Brier scores ratio
is bounded
exp(−2/epsilon1θ)≤E/bracketleftBig
(yi−πA
i)2/bracketrightBig
E[(yi−πB
i)2]≤exp(2/epsilon1θ),
meaning performance equivalence is achieved. 
306.2 Simulation Study Results
k=0.05 k=0.1 k=0.25 k=0.5
1021031041021031041021031041021031040%25%50%75%100%
Sample SizeIdentifying Equivalence, %
Method DE, δβ = 0.1 DE, δβ = 0.25 DE, δβ = 0.5 DE, δβ = 1Deviance testDescriptive Equivalence
k=0.05 k=0.1 k=0.25 k=0.5
1021031041021031041021031041021031040%25%50%75%100%
Sample SizeIdentifying Equivalence, %
Method IPE, δθ = 0.025 IPE, δθ = 0.05 IPE, δθ = 0.1 IPE, δθ = 0.2 Hosmer−LemeshowIndividual Predictive Equivalence
k=0.05 k=0.1 k=0.25 k=0.5
1021031041021031041021031041021031040%25%50%75%100%
Sample SizeIdentifying Equivalence, %
Method PE, δB = 1.01 PE, δB = 1.05 PE, δB = 1.1 PE, δB = 1.2 t−testPerformance EquivalenceLog−Odds Additive Effect
Figure 5: Testing the performance of diﬀerent comparison methods against sample size, under
log-odds additive eﬀect. DE stands for descriptive equivalence test (with diﬀerent δβvalues),
IPE stands for individual predictive equivalence test (with diﬀerent δθvalues), PE stands for
performance equivalence test (with diﬀerent δBvalues).
31k=1.01 k=1.1 k=1.25 k=1.5
1021031041021031041021031041021031040%25%50%75%100%
Sample SizeIdentifying Equivalence, %
Method DE, δβ = 0.1 DE, δβ = 0.25 DE, δβ = 0.5 DE, δβ = 1Deviance testDescriptive Equivalence
k=1.01 k=1.1 k=1.25 k=1.5
1021031041021031041021031041021031040%25%50%75%100%
Sample SizeIdentifying Equivalence, %
Method IPE, δθ = 0.025 IPE, δθ = 0.05 IPE, δθ = 0.1 IPE, δθ = 0.2 Hosmer−LemeshowIndividual Predictive Equivalence
k=1.01 k=1.1 k=1.25 k=1.5
1021031041021031041021031041021031040%25%50%75%100%
Sample SizeIdentifying Equivalence, %
Method PE, δB = 1.01 PE, δB = 1.05 PE, δB = 1.1 PE, δB = 1.2 t−testPerformance EquivalenceProbability Multiplicative EffectFigure 6: Testing the performance of diﬀerent comparison methods against sample size, under
probability multiplicative eﬀect. DE stands for descriptive equivalence test (with diﬀerent δβ
values), IPE stands for individual predictive equivalence test (with diﬀerent δθvalues), PE
stands for performance equivalence test (with diﬀerent δBvalues).
6.3 MATAL Data Generation Process
The data used to design MATAL comes from two datasets:
1.Norms research dataset – participants with no learning disabilities, participated in
the national norms building for the MATAL tasks.
2.Revalidation dataset – participants who have applied for MATAL-based diagnosis
between the years 2008 and 2011.
32Each dataset has the following variables: Gender (m/f); clinical diagnosis for dysgraphia
(binary); clinical diagnosis for dyscalculia (binary); ﬁve exam scores x1,...,x 5. Each exam
score is a linear transformation of a gamma variable: Let zi∼Γ(αi,βi), thenxi=Ci−zi
withCibeing some non-negative constant. As the ﬁve exam scores are correlated, we
should use a multivatiate gamma distribution to generate them. The datasets are of
similar size, and are balanced with respecr to gender.
For a given dataset, the following procedure was used for regeneration:
Estimation
1. Invert each variable using zi=max(xi)−xi(soziis gamma-distributed).
2.µ1,...,µ 5is taken as the vector of means and Σ = (σij)as the covariance matrix.
3.Gamma distribution parameters were estimated using the method of moments:
ˆαi=µ2
i
σ2
ii,ˆβ=µi
σ2
ii.
Sampling from model
4.Multivariate data matrix Ywas generated using the multivariate normal distribution,
with input parameters µandΣ.
5.Marginal gamma variables zr
iwere created using the gamma distribution quantile
function with parameters ˆα,ˆβand the probabilities vector Piof each regenerated
variableyr
i.
6. Inverted variables were created using xr
i=max(xi)−zr
i
The Norms dataset was split according to gender, then each subgroup (total of 2 subgroups)
was regenerated with n= 2000so overall the regenerated Norms dataset has n= 4000
samples. The Revalidation dataset was split according to gender and combination of
disabilities (total of 8 subgroups). Each subgroup was regenerated with n= 500so overall
the regenerated Revalidation dataset has n= 4000samples.
Regenerated datasets were combined, then split again by gender. Each gender-based
dataset (n= 4000) was split to trainandtestsets with a 3:1 ratio. The ﬁnal dataset sizes
arentrain
female =ntrain
male= 3000,ntest
female =ntest
male= 1000.
33References
Allaire, J., Xie, Y., McPherson, J., Luraschi, J., Ushey, K., Atkins, A., et al. (2019).
rmarkdown: Dynamic documents for R . https://CRAN.R-project.org/package=rmar
kdown
American Psychiatric Association and others. (2013). Diagnostic and statistical manual
of mental disorders (DSM-5 ®). American Psychiatric Pub.
Ashiri-Prossner, G. (2022). LogRegEquiv: Logistic regression equivalence . https://CRAN
.R-project.org/package=LogRegEquiv
Auguie, B. (2017). gridExtra: Miscellaneous functions for "grid" graphics . https://CRAN
.R-project.org/package=gridExtra
Barker, L. E., Luman, E. T., McCauley, M. M., & Chu, S. Y. (2002). Assessing equivalence:
An alternative to the use of diﬀerence tests for measuring disparities in vaccination
coverage. American Journal of Epidemiology ,156(11), 1056–1061.
Benedetti, R. (2010). Scoring rules for forecast veriﬁcation. Monthly Weather Review ,
138(1), 203–211.
Ben-Simon, A. (2007). MATAL: A computerized test battery for the diagnosis of learning
disabilities . Jerusalem, Israel: National Institute for Testing & Evaluation.
Ben-Simon, A. (2013). MATAL: User guide . Jerusalem, Israel: National Institute for
Testing & Evaluation.
Ben-Simon, A., Beyth-Marom, R., Inbar-Weiss, N., & Cohen, Y. (2008). Regulating the
diagnosis of learning disability and the provision of test accommodations in institutions
of higher education. In 34th conference of the association for educational assessment
cambridge, UK.[google scholar] .
Berninger, V. W., & O’Malley May, M. (2011). Evidence-based diagnosis and treatment
for speciﬁc learning disabilities involving impairments in written and/or oral language.
Journal of Learning Disabilities ,44(2), 167–183.
Bradley, A.A., Schwartz, S.S., &Hashino, T.(2008). Samplinguncertaintyandconﬁdence
intervals for the brier score and brier skill score. Weather and Forecasting ,23(5),
992–1006.
Brenner, C. H. (1985). Evidence, probability, and paternity. American journal of human
genetics,37(4), 826.
Brier, G. W. (1950). Veriﬁcation of forecasts expressed in terms of probability. Monthly
34Weather Review ,78(1), 1–3.
Byrne, B. M. (1988). Measuring adolescent self-concept: Factorial validity and equivalency
of the SDQ III across gender. Multivariate Behavioral Research ,23(3), 361–375.
Byrne, B. M., Shavelson, R. J., & Muthén, B. (1989). Testing for the equivalence of
factor covariance and mean structures: The issue of partial measurement invariance.
Psychological bulletin ,105(3), 456.
Byrne, B. M., & van de Vijver, F. J. (2014). Factorial structure of the family values scale
from a multilevel-multicultural perspective. International Journal of Testing ,14(2),
168–192.
Casabianca, J. M., & Lewis, C. (2018). Statistical equivalence testing approaches for
mantel–haenszel DIF analysis. Journal of Educational and Behavioral Statistics ,43(4),
407–439.
Chow, S.-C., & Liu, J. (2008). Design and analysis of bioavailability and bioequivalence
studies. CRC Press.
Collins, J. M., & Gleaves, D. H. (1998). Race, job applicants, and the ﬁve-factor model of
personality: Implications for black psychology, industrial/organizational psychology,
and the ﬁve-factor theory. Journal of Applied Psychology ,83(4), 531.
Counsell, A., & Cribbie, R. A. (2015). Equivalence tests for comparing correlation and
regression coeﬃcients. British Journal of Mathematical and Statistical Psychology ,
68(2), 292–309.
Cramer, J. S. (2002). The origins of logistic regression.
Dette, H., Möllenhoﬀ, K., Volgushev, S., & Bretz, F. (2018). Equivalence of regression
curves.Journal of the American Statistical Association ,113(522), 711–729.
Devine, A., Soltész, F., Nobes, A., Goswami, U., & Szűcs, D. (2013). Gender diﬀerences
in developmental dyscalculia depend on diagnostic criteria. Learning and Instruction ,
27, 31–39.
Dolado, J. J., Otero, M. C., & Harman, M. (2014). Equivalence hypothesis testing in
experimental software engineering. Software Quality Journal ,22(2), 215–238.
Greene, C. J., Morland, L. A., Durkalski, V. L., & Frueh, B. C. (2008). Noninferiority
and equivalence designs: Issues and implications for mental health research. Journal
of traumatic stress ,21(5), 433–439.
Hauschke, D., Kieser, M., Diletti, E., & Burke, M. (1999). Sample size determination for
35proving equivalence based on the ratio of two means for normally distributed data.
Statistics in Medicine ,18(1), 93–105.
Hauschke, D., Steinijans, V., & Pigeot, I. (2007). Bioequivalence studies in drug develop-
ment: Methods and applications (Vol. 60). John Wiley & Sons.
Holland, P. W., & Thayer, D. T. (1988). Diﬀerential item performance and the mantel-
haenszel procedure. Test validity , 129–145.
Holland, P. W., & Wainer, H. (2012). Diﬀerential item functioning . Routledge.
Hosmer Jr, D. W., Lemeshow, S., & Sturdivant, R. X. (2013). Applied logistic regression .
John Wiley & Sons.
Jensen, D. (1997). Bounds on mahalanobis norms and their applications. Linear algebra
and its applications ,264, 127–139.
Joe, H. (2014). Dependence modeling with copulas . CRC press.
Jonkman, J. N., & Sidik, K. (2009). Equivalence testing for parallelism in the four-
parameter logistic model. Journal of biopharmaceutical statistics ,19(5), 818–837.
Lele, S. R., Keim, J. L., & Solymos, P. (2019). ResourceSelection: Resource selection
(probability) functions for use- availability data . https://CRAN.R-project.org/packag
e=ResourceSelection
Liu, Wei. (2010). Simultaneous inference in regression . CRC Press.
Liu, W., Bretz, F., Hayter, A., &Wynn, H.(2009). Assessingnonsuperiority, noninferiority,
or equivalence when comparing two regression models over a restricted covariate region.
Biometrics ,65(4), 1279–1287.
Magis, D., & De Boeck, P. (2011). Identiﬁcation of diﬀerential item functioning in multiple-
group settings: A multivariate outlier detection approach. Multivariate Behavioral
Research ,46(5), 733–755.
Martinková, P., Drabinová, A., Liaw, Y.-L., Sanders, E. A., McFarland, J. L., & Price, R.
M. (2017). Checking equity: Why diﬀerential item functioning analysis should be a
routine part of developing conceptual assessments. CBE—Life Sciences Education ,
16(2), rm2.
Meredith, W. (1993). Measurement invariance, factor analysis and factorial invariance.
Psychometrika ,58(4), 525–543.
Meschiari, S. (2021). latex2exp: Use LaTeX expressions in plots . https://CRAN.R-
project.org/package=latex2exp
36Özdemir, B. (2015). A comparison of IRT-based methods for examining diﬀerential item
functioning in TIMSS 2011 mathematics subtest. Procedia-Social and Behavioral
Sciences,174, 2075–2083.
Peng, C.-Y. J., Lee, K. L., & Ingersoll, G. M. (2002). An introduction to logistic regression
analysis and reporting. The journal of educational research ,96(1), 3–14.
Putnick, D. L., & Bornstein, M. H. (2016). Measurement invariance conventions and
reporting: The state of the art and future directions for psychological research. Devel-
opmental Review ,41, 71–90.
Robinson, A. P., Duursma, R. A., & Marshall, J. D. (2005). A regression-based equivalence
test for model validation: Shifting the burden of proof. Tree physiology ,25(7), 903–913.
Siqueira, A. L., Whitehead, A., & Todd, S. (2008). Active-control trials with binary data:
A comparison of methods for testing superiority or non-inferiority using the odds ratio.
Statistics in medicine ,27(3), 353–370.
Steinberg, L., & Thissen, D. (2006). Using eﬀect sizes for research reporting: Examples
using item response theory to analyze diﬀerential item functioning. Psychological
methods,11(4), 402.
Stevens, N. T., & Anderson-Cook, C. M. (2017a). Comparing the reliability of related
populations with the probability of agreement. Technometrics ,59(3), 371–380.
Stevens, N. T., & Anderson-Cook, C. M. (2017b). Quantifying similarity in reliability
surfaces using the probability of agreement. Quality Engineering ,29(3), 395–408.
Swaminathan, H., & Rogers, H. J. (1990). Detecting diﬀerential item functioning using
logistic regression procedures. Journal of Educational measurement ,27(4), 361–370.
van de Schoot, R., Schmidt, P., De Beuckelaer, A., Lek, K., & Zondervan-Zwijnenburg, M.
(2015). Measurement invariance. Frontiers in psychology ,6, 1064.
Vandenberg, R. J., & Lance, C. E. (2000). A review and synthesis of the measurement
invariance literature: Suggestions, practices, and recommendations for organizational
research. Organizational research methods ,3(1), 4–70.
Verhagen, A., & Fox, J. (2013). Bayesian tests of measurement invariance. British Journal
of Mathematical and Statistical Psychology ,66(3), 383–401.
Walker, E., & Nowacki, A. S. (2011). Understanding equivalence and noninferiority testing.
Journal of general internal medicine ,26(2), 192–196.
Weigold, A., Weigold, I. K., Drakeford, N. M., Dykema, S. A., & Smith, C. A. (2016).
37Equivalence of paper-and-pencil and computerized self-report surveys in older adults.
Computers in Human Behavior ,54, 407–413.
Wellek, S. (2010). Testing statistical hypotheses of equivalence and noninferiority . Chap-
man; Hall/CRC.
Wells, C. S., Cohen, A. S., & Patton, J. (2009). A range-null hypothesis approach for
testing DIF under the rasch model. International Journal of Testing ,9(4), 310–332.
Wickham, H., Chang, W., Henry, L., Pedersen, T. L., Takahashi, K., Wilke, C., et al.
(2019).ggplot2: Create elegant data visualisations using the grammar of graphics .
https://CRAN.R-project.org/package=ggplot2
Xie, Y. (2019b). bookdown: Authoring books and technical documents with R markdown .
https://github.com/rstudio/bookdown
Xie, Y. (2019a). knitr: A general-purpose package for dynamic report generation in R .
https://CRAN.R-project.org/package=knitr
Zhu, H. (2019). kableExtra: Construct complex table with ’kable’ and pipe syntax . https:
//CRAN.R-project.org/package=kableExtra
38