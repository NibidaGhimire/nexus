arXiv:2302.12456v1  [cs.LG]  24 Feb 2023Logarithmic Switching Cost in Reinforcement Learning
beyond Linear MDPs
Dan Qiao1, Ming Yin1,2, and Yu-Xiang Wang1
1Department of Computer Science, UC Santa Barbara
2Department of Statistics and Applied Probability, UC Santa Barbara
{danqiao,ming yin}@ucsb.edu ,yuxiangw@cs.ucsb.edu
Abstract
In many real-life reinforcement learning (RL) problems, deploying ne w policies is costly. In
those scenarios, algorithms must solve exploration (which requires adaptivity) while switching
the deployed policy sparsely (which limits adaptivity). In this paper, w e go beyond the existing
state-of-the-art on this problem that focused on linear Markov D ecision Processes (MDPs) by
considering linear Bellman-complete MDPs with low inherent Bellman erro r. We propose the
ELEANOR-LowSwitching algorithm that achieves the near-optimal r egret with a switching cost
logarithmic in the number of episodes and linear in the time-horizon Hand feature dimension
d. We also prove a lower bound proportional to dHamong all algorithms with sublinear regret.
In addition, we show the “doubling trick” used in ELEANOR-LowSwitch ing can be further
leveraged for the generalized linear function approximation, under which we design a sample-
eﬃcient algorithm with near-optimal switching cost.
1Contents
1 Introduction 3
1.1 Related works . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
2 Problem setup 5
2.1 Low inherent Bellman error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
3 Main algorithm 8
4 Main results 10
5 Proof sketch 11
5.1 Upper bounds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .11
5.2 Lower bound . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .13
6 Extension to generalized linear function approximation 13
6.1 Problem setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .13
6.2 Low switching algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
6.3 Main results of Algorithm 2. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
7 Conclusion and future work 15
A Proof of Theorem 4.1 19
A.1 Proof of switching cost bound . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
A.2 Proof of regret bound . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
B Proof of Theorem 4.2 22
C Proof for Section 6 23
C.1 Proof of upper bounds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
C.2 Proof of lower bound . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
D Assisting technical lemmas 25
21 Introduction
In many real-world reinforcement learning (RL) tasks, limi ted computing resources make it chal-
lenging to apply fully adaptive algorithms that continuall y update the exploration policy. As a
surrogate, it is more cost-eﬀective to collect data in large b atches using the current policy and
make changes to the policy after the entire batch is complete d. For example, in a recommenda-
tion system [ Afsar et al. ,2021], it is easier to gather new data quickly, but deploying a new policy
takes longer as it requires signiﬁcant computing and human r esources. Therefore, it’s not feasible
to switch policies based on real-time data, as typical RL alg orithms would require. A practical
solution is to run several experiments in parallel and make d ecisions on policy updates only after
the entire batch has been completed. Similar limitations oc cur in other RL based applications such
as healthcare [ Yu et al. ,2021], robotics [ Kober et al. ,2013], and new material design [ Zhou et al. ,
2019], where the agent must minimize the number of policy updates while still learning an eﬀective
policy using a similar number of trajectories as fully-adap tive methods. On the theoretical side,
Bai et al. [2019] brought up the deﬁnition of switching cost , which measures the number of policy
updates. In this paper, we measure the adaptivity of online r einforcement learning algorithms via
global switching cost , and we leave the formal deﬁnition to Section 2.
In recent years, there has been a growing interest in designi ng online reinforcement learning algo-
rithms with low switching costs [ Bai et al. ,2019,Zhang et al. ,2020,Qiao et al. ,2022,Gao et al. ,
2021,Wang et al. ,2021,Kong et al. ,2021,Velegkas et al. ,2022]. While much progress has been
made in achieving near-optimal results, most of the researc h has focused on the tabular MDP
setting and the slightly more general linear MDP setting [ Yang and Wang ,2019,Jin et al. ,2020].
However, linear MDP is still a restrictive model, and subseq uent works have proposed a variety
of more general settings, such as low inherent Bellman error [Zanette et al. ,2020], generalized lin-
ear function approximation [ Wang et al. ,2019], low Bellman rank [ Jiang et al. ,2017], low rank
[Agarwal et al. ,2020], and low Bellman eluder dimension [ Jin et al. ,2021]. Therefore, it is natural
to question whether reinforcement learning with low switch ing cost is achievable under these more
general MDP settings.
Our contributions. In this paper, we extend previous results underlinear MDP to its two natural
extensions, linear Bellman-complete MDPs with low inheren t Bellman error [ Zanette et al. ,2020]
and MDP with genaralized linear function approximation [ Wang et al. ,2019]. Under both settings,
we design algorithms with near optimal regret and switching cost. Our contributions are three-fold
and summarized as below.
•A new algorithm (Algorithm 1) based on “doubling trick” for regret minimization under th e
low inherent Bellman error setting that achieves global swi tching cost of O(/summationtextH
h=1dhlogK)
and regret of /tildewideO/parenleftBig/summationtextH
h=1dh√
K+/summationtextH
h=1√dhIK/parenrightBig
, wheredhis the dimension of feature map
for theh-th layer, Iis the inherent Bellman error and Kis the number of episodes (Theorem
4.1). The regret bound is known to be minimax optimal [ Zanette et al. ,2020].
•When the inherent Bellman error I= 0, we prove a nearly matching switching cost lower
bound (Theorem 4.2) Ω(/summationtextH
h=1dh) for any algorithm with sub-linear regret bound, which
implies that the switching cost of our Algorithm 1is optimal up to log Kfactor. When
applied to linear MDP, Algorithm 1achieves the same switching cost and better regret bound
compared to the previous results [ Gao et al. ,2021,Wang et al. ,2021].
3Algorithms for regret minimization Setting Regret bound Switching cost bound
Our Algorithm 1 (Theorem 4.1)†Low IBE /tildewideO/parenleftBig/summationtextH
h=1dh√
K/parenrightBig
O(/summationtextH
h=1dhlogK)
Our Algorithm 2 (Theorem 6.4)⋆GLM /tildewideO/parenleftBig
H√
d3K/parenrightBig
O(dHlogK)
Algorithm 1 of Gao et al. [2021]‡Linear MDP /tildewideO(√
d3H4K) O(dHlogK)
UCB-Advantage [ Zhang et al. ,2020]Tabular MDP /tildewideO(√
H3SAK)O(H2SAlogK)∗
APEVE [ Qiao et al. ,2022]Tabular MDP /tildewideO(√
H5S2AK)O(HSAloglogK)
Lower bound (Theorem 4.2) Low IBE If “no-regret” Ω(/summationtextH
h=1dh)
Lower bound (Theorem 6.5) GLM If “no-regret” Ω(dH)
Table 1: Comparison of our results (in blue) to existing works regarding regret bound and (global)
switching cost bound. “Low IBE” is short for low inherent Bel lman error while “GLM” represents
generalized linear function approximation, where both set tings generalize linear MDP. For both
“Low IBE”and“GLM”settings, weassumethetotal rewardisbo undedby1. Inparticular, weshow
the regret bound for “Low IBE” assuming the inherent Bellman error is 0 while the detailed result
is shown in Theorem 4.1. We highlight that our switching cost upper bounds under bot h settings
match the corresponding lower bounds up to logarithmic fact ors.†: Heredhis the dimension of
feature map for the h-th layer and Kis the number of episodes. When applied to linear MDP, there
will be an additional factor of Hin the regret bound while dh=dfor allh. Therefore, regret bound
and switching cost bound will be /tildewideO(√
d2H4K) andO(dHlogK), respectively. ⋆: When applied to
linear MDP, there will be an additional factor of Hin the regret bound, and the regret bound will
be/tildewideO(√
d3H4K).‡: This result is generalized by Wang et al. [2021] whose algorithm has a same
switching cost bound under this regret bound. ∗: The switching cost here is local switching cost
(deﬁned in Bai et al. [2019]), which is speciﬁed to tabular MDP.
4•We leverage the “doubling trick” used in Algorithm 1under the generalized linear function
approximation setting and propose Algorithm 2which achieves switching cost of O(dHlogK)
and regret of /tildewideO/parenleftBig
H√
d3K/parenrightBig
, wheredis the dimension of feature map (Theorem 6.4). We also
prove a nearly matching switching cost lower bound of Ω( dH) for any algorithm with sub-
linear regret bound (Theorem 6.5). The pair of results strictly generalize previous results
under linear MDP [ Gao et al. ,2021,Wang et al. ,2021].
1.1 Related works
There is a large and growing body of literature on the statist ical theory of reinforcement learn-
ing that we will not attempt to thoroughly review. Detailed c omparisons with existing work on
reinforcement learning with low switching cost [ Gao et al. ,2021,Wang et al. ,2021,Zhang et al. ,
2020,Qiao et al. ,2022] are given in Table 1. Notably, the settings we consider are more general
than the well studied tabular or linear MDP, while our result s for regret and switching cost are
comparable or better than the best known results under linea r MDP [Gao et al. ,2021,Wang et al. ,
2021]. While there are low adaptive algorithms under other more g eneral settings than linear MDP,
they either consider only pure exploration (without regret guarantee) [ Jiang et al. ,2017,Sun et al. ,
2019], or suﬀer from sub-optimal results comparing to our results [Kong et al. ,2021,Velegkas et al. ,
2022].
In addition to switching cost, there are other measurements of adaptivity. The closest measure-
ment is batched learning, which requires decisions about po licy updates to be made at only a
few (often predeﬁned) checkpoints but does not constrain th e number of policy switches. Batched
learning has been considered both under bandits [ Perchet et al. ,2016,Gao et al. ,2019] and RL
[Wang et al. ,2021,Qiao et al. ,2022,Zhang et al. ,2022b] while the settings are restricted to tabu-
lar MDP or linear MDP. Meanwhile, Matsushima et al. [2020] proposed the notion of deployment
eﬃciency , which is similar to batched RL with additional requirement that each policy deployment
should have similar size. Deployment eﬃcient RL is studied b y some following works [ Huang et al. ,
2022,Qiao and Wang ,2022,Modi et al. ,2021]. However, as pointed out by Qiao and Wang [2022],
deployment complexity is not a good measurement of adaptivi ty when studying regret minimiza-
tion.
Technically speaking, we directly base on ELEANOR [ Zanette et al. ,2020] and Algorithm 1 of
Wang et al. [2019], which admit fully adaptive structure. We apply “doubling trick” when deciding
whether to update the exploration policy, in order to achiev e low switching cost. In particular,
we show that the “information gain” used in previous works un der linear MDP [ Gao et al. ,2021,
Wang et al. ,2021]: the determinant of empirical covariance matrix can beext ended to more general
MDPs with linear approximation. Therefore, we only update t he exploration policy when the
“information gain” doubles, and the switching cost depends only logarithmically on the number of
episodes K.
2 Problem setup
Notations. Throughout the paper, for n∈Z+, [n] ={1,2,···,n}. We denote /bardblx/bardblΛ=√
x⊤Λx.
For matrix X∈Rd×d,/bardbl·/bardbl2, det(·),λmin(·),λmax(·) denote theoperator norm, determinant, smallest
eigenvalue and largest eigenvalue, respectively. In addit ion, we use standard notations such as O
5and Ω to absorb constants while /tildewideOand/tildewideΩ suppress logarithmic factors.
Markov Decision Processes. We consider ﬁnite-horizon episodic Markov Decision Processes
(MDP)withnon-stationarytransitions, denotedbyatuple M= (S,A,H,Ph,rh)[Sutton and Barto ,
1998], where Sis the state space, Ais the action space and His the horizon. The non-stationary
transition kernel has the form Ph:S ×A×S /ma√sto→ [0,1] withPh(s′|s,a) representing the probability
of transition from state s, actionato next state s′at time step h. In addition, rh(s,a)∈∆([0,1])
denotes the corresponding distribution of reward.1Without loss of generality, we assume there is a
ﬁxed initial state s1.2A policy can be seen as a series of mapping π= (π1,···,πH), where each πh
maps each state s∈ Sto a probability distribution over actions, i.e.πh:S →∆(A),∀h∈[H]. A
random trajectory ( s1,a1,r1,···,sH,aH,rH,sH+1) is generated by the following rule: s1is ﬁxed,
ah∼πh(·|sh),rh∼rh(sh,ah),sh+1∼Ph(·|sh,ah),∀h∈[H]. For normalization, we assume that/summationtextH
h=1rh∈[0,1] almost surely.
Q-values, Bellman operator. Given a policy πand anyh∈[H], the value function Vπ
h(·) and Q-
value function Qπ
h(·,·) are deﬁned as: Vπ
h(s) =Eπ[/summationtextH
t=hrt|sh=s],Qπ
h(s,a) =Eπ[/summationtextH
t=hrt|sh,ah=
s,a],∀s,a∈ S ×A.Besides, the value function and Q-value function with respe ct to the optimal
policyπ⋆is denoted by V⋆
h(·) andQ⋆
h(·,·). Then the Bellman operator Thapplied to Qh+1is deﬁned
as
Th(Qh+1)(s,a) =rh(s,a)+Es′∼Ph(·|s,a)max
a′Qh+1(s′,a′).
Regret. We measure the performance of online reinforcement learnin g algorithms by the regret.
The regret of an algorithm over Kepisodes is deﬁned as
Regret(K) :=K/summationdisplay
k=1[V⋆
1(s1)−Vπk
1(s1)],
whereπkis the policy it deploys at episode k. Besides, we denote the total number of steps by
T:=KH.
Switching cost. We adopt the global switching cost [ Bai et al. ,2019], which simply measures how
many times the algorithm changes its policy:
Nswitch:=K−1/summationdisplay
k=11{πk/ne}ationslash=πk+1}.
Global switching cost is a widely applied measurement of the adaptivity of an online RL algorithm
both under the tabular setting [ Bai et al. ,2019,Zhang et al. ,2020,Qiao et al. ,2022] and the linear
MDP setting [ Gao et al. ,2021,Wang et al. ,2021]. Similar to previous works, our algorithm also
uses deterministic policies only.
1We overload the notation rso thatralso denotes the expected (immediate) reward function.
2The generalized case where the initial distribution is an ar bitrary distribution can be recovered from this setting
by adding one layer to the MDP.
62.1 Low inherent Bellman error
In this part, we introduce the linear function approximatio n, the deﬁnition of inherent Bellman
error [Zanette et al. ,2020] and the connection between the low inherent Bellman error s etting and
the linear MDP setting [ Jin et al. ,2020].
To encode linear function approximation of the state space S, a common approach is to deﬁne
a feature map φh:S × A → Rdh, which can be diﬀerent across diﬀerent timestep. Then the
Q-value functions are represented as linear functions of φh,i.e.,Qh(s,a) =φh(s,a)⊤θhfor some
θh∈Rdh.
The feasible parameter class for timestep his deﬁned as
Bh:={θh∈Rdh| |φh(s,a)⊤θh| ≤1,∀(s,a)},
which is consistent with our assumption that Qπ
h(s,a)≤1.
For each feasible parameter θ∈ Bh, the corresponding Q-value function and value function are
deﬁned as
Qh(θ)(s,a) =φh(s,a)⊤θ, Vh(θ)(s) = max
aφh(s,a)⊤θ.
Meanwhile, the associated function spaces are
Qh:={Qh(θh)|θh∈ Bh},Vh:={Vh(θh)|θh∈ Bh}.
Similar to Zanette et al. [2020], we make the following normalization assumption, which is without
loss of generality.
/bardblφh(s,a)/bardbl2≤1,∀(h,s,a)∈[H]×S ×A.
/bardblθh/bardbl2≤/radicalbig
dh,∀h∈[H], θh∈ Bh.
Inherent Bellman error. For provably eﬃcient learning, completeness assumption is widely
adopted [ Zanette et al. ,2020,Wang et al. ,2020,Jin et al. ,2021]. In this paper, we characterize
the completeness by assuming an upper bound of the projectio n error when we project ThQh+1
(Qh+1∈ Qh+1) toQh. Formally, we have the following deﬁnition of inherent Bell man error.
Deﬁnition 2.1. The inherent Bellman error of an MDPwith a known linear featur e map{φh(·,·)}h∈[H]
is deﬁned as the maximum over the timesteps h∈[H]of
sup
θh+1∈Bh+1inf
θh∈Bhsup
s,a|φh(s,a)⊤θh−(ThQh+1(θh+1))(s,a)|.
Similarto Zanette et al. [2020], weassumetheinherentBellman erroroftheMDP is upperbou nded
by some (known) constant I ≥0. Below we will show that this setting strictly generalizes the linear
MDP setting [ Jin et al. ,2020].
Connections to linear MDP. Since linear MDP admits transition kernel and reward functi on
that is linear in a known feature map φ, for any function V(·) :S →R,ThV(·,·) is a linear
function of φ(·,·) [Jin et al. ,2020]. Therefore, a linear MDP with feature map φand dimension
dis a special case of the low inherent Bellman error setting wi thI= 0,φ1=···=φH=φand
7d1=···=dH=d(if ignoring the scale of rewards). More importantly, it is s hown that an MDP
with zero inherent Bellman error ( I= 0) may not be a linear MDP [ Zanette et al. ,2020], which
means that the setting in this paper is strictly more general and technically demanding than linear
MDP. For more discussions about the low inherent Bellman err or setting and relavent comparisons,
please refer to Section 3 in Zanette et al. [2020].
3 Main algorithm
In this section, we propose our main algorithm: ELEANOR-Low Switching (Algorithm 1) and the
low switching design for global optimism-based algorithms .
We begin with the standard LSVI technique. At the beginning o f thek-th episode, assume the
parameter for the ( h+1)-th layer is ﬁxed to be θh+1. Then LSVI minimizes the following objective
function with respect to θ:
k−1/summationdisplay
τ=1/parenleftBig
(φτ
h)⊤θ−rτ
h−Vh+1(θh+1)(sτ
h+1)/parenrightBig2
+λ/bardblθ/bardbl2
2, (1)
whereφτ
his short for φh(sτ
h,aτ
h) andrτ
his the reward encountered at layer hof theτ-th episode.
The minimization problem ( 1) has a closed form solution:
/hatwideθh= (Σk
h)−1k−1/summationdisplay
τ=1φτ
h/bracketleftbig
rτ
h+Vh+1(θh+1)(sτ
h+1)/bracketrightbig
, (2)
where Σk
h=/summationtextk−1
τ=1φτ
h(φτ
h)⊤+λIdhis the empirical covariance matrix.
Based on the standard LSVI, we introduce the global optimist ic planning below, wherean optimiza-
tion problem is solved to derive the most optimistic estimat e of the Q-value function at the initial
state. At each episode where the policy is updated, Algorith m1solves the following problem.
Deﬁnition 3.1 (Optimistic planning) .
max
{¯ξh}h∈[H],{/hatwideθh}h∈[H],{¯θh}h∈[H]max
aφ1(s1,a)⊤¯θ1subject to
/hatwideθh= (Σk
h)−1k−1/summationdisplay
τ=1φτ
h/parenleftbig
rτ
h+Vh+1(¯θh+1)(sτ
h+1)/parenrightbig
,
¯θh=/hatwideθh+¯ξh;/bardbl¯ξh/bardblΣk
h≤/radicalBig
αk
h;¯θh∈ Bh.
Deﬁnition 3.1optimizes over the perturbation ¯ξhadded to the least square solution /hatwideθh. The
constraint on ¯ξhis
/bardbl¯ξh/bardblΣk
h≤/radicalBig
αk
h:=/tildewideO(/radicalbig
dh+dh+1)+√
kI, (3)
where the deﬁnition of/radicalBig
αk
hwill be speciﬁed in Appendix A.2. As will be shown in the analysis,
the ﬁrst term accounts for the estimation error of the LSVI, w hile the second term accounts for
8the model misspeciﬁcation (recall that Iis inherent Bellman error). Finally, with high probability ,
there will be a valid solution of the optimization problem (d etails in Appendix A.2), and therefore
Algorithm 1is well posed.
About global optimism. We highlight that the optimization problem aims at being opt imistic
only at the initial state instead of choosing a value functio n everywhere optimistic, as in LSVI-UCB
[Jin et al. ,2020]. Such global optimism eﬀectively keeps the linear structur e of our function class
and reduces the dimension of the covering set, since we do not need to cover the quadratic bonus
as inJin et al. [2020].
Algorithmic design. We present the whole learning process in Algorithm 1. For linear function
approximation, wecharacterize the“information gain”(th einformation welearned frominteracting
with the MDP) through the determinant of the empirical covar iance matrix Σk
h(line 5). To achieve
low switching cost, we only update the exploration policy wh en the “information gain” doubles for
some layer h∈[H] (line 7), and each update means the information about some l ayer has doubled.
As will be shown later, such “doubling schedule” will lead to a switching cost depending only
logarithmically on K, in stark contrast to its fully adaptive counterpart: ELEAN OR [Zanette et al. ,
2020]. When an update occurs, Algorithm 1solves the optimization problem to derive {¯θh}h∈[H]
ensuring global optimism (line 9), takes the greedy policy w ith respect to φh(·,·)⊤¯θk
h(line 10) and
updates the empirical covariance matrix (line 11).
Algorithm 1 ELEANOR-LowSwitching
1:Input: Number of episodes K, regularization λ= 1, feature map {φh(·,·)}h∈[H], failure proba-
bilityδ, initial state s1and inherent Bellman error I.
2:Initialize : Σh= Σ0
h=λIdh, for allh∈[H].
3:fork= 1,2,···,Kdo
4:forh= 1,2,···,Hdo
5:Σk
h=/summationtextk−1
τ=1φh(sτ
h,aτ
h)φh(sτ
h,aτ
h)⊤+λIdh.
6:end for
7:if∃h∈[H], det(Σk
h)≥2det(Σ h)then
8:Set¯θk
H+1=/hatwideθk
H+1=¯ξk
H+1= 0.
9:Solve the optimization problem in Deﬁnition 3.1.
10:Setπk
h(s) = argmax aφh(s,a)⊤¯θk
h,∀h∈[H].
11:Set Σh= Σk
h,∀h∈[H].
12:else
13:Setπk
h=πk−1
hfor allh∈[H].
14:end if
15:Deploy policy πk= (πk
1,···,πk
H) and get trajectory ( sk
1,ak
1,rk
1,···,sk
H+1).
16:end for
Generalization over previous algorithms. If we remove the update rule in Algorithm 1and
solve Deﬁnition 3.1at all episodes, our Algorithm 1will degenerate to ELEANOR [ Zanette et al. ,
2020]. Compared to ELEANOR, our Algorithm 1achieves the same regret bound (shown later)
and near optimal switching cost. Meanwhile, Algorithm 1also strictly generalizes the RARELY
SWITCHING OFUL algorithm [ Abbasi-Yadkori et al. ,2011] designed for linear bandits. Taking
H= 1, both our Algorithm 1and our guarantees (for regret and switching cost) strictly subsumes
9the RARELY SWITCHING OFUL. In conclusion, we show that low sw itching cost is possible for
RL algorithms with global optimism.
Computational eﬃciency. Although Algorithm 1is shown to be near optimal both in regret
and switching cost, the implementation of the optimization problem is ineﬃcient in general. This is
because the max operator breaks the quadratic structure of t he constraints. Such issue also exists
for our fully adaptive counterpart: ELEANOR [ Zanette et al. ,2020], and other algorithms based
on global optimism [ Jiang et al. ,2017,Sun et al. ,2019,Jin et al. ,2021]. We leave the improvement
of computation as future work.
4 Main results
In this section, we present our main results. We begin with th e upper bounds for regret and
switching cost. Recall that we assume/summationtextH
h=1rh∈[0,1] almost surely, while dhrepresents the
dimension of the feature map for the h-th layer and Iis inherent Bellman error.
Theorem 4.1 (Main theorem) .The global switching cost of Algorithm 1is bounded by O(/summationtextH
h=1dh·
logK). In addition, with probability 1−δ, the regret of Algorithm 1overKepisodes is bounded by
Regret(K)≤/tildewideO/parenleftBiggH/summationdisplay
h=1dh√
K+H/summationdisplay
h=1/radicalbig
dhIK/parenrightBigg
.
The proof of Theorem 4.1is sketched in Section 5.1with details in the Appendix, below we discuss
several interesting aspects of Theorem 4.1.
Near-optimal switching cost. Our algorithm achieves a switching cost that depends logari th-
mically on K, which improves the O(K) switching cost of ELEANOR [ Zanette et al. ,2020]. We
also prove the following information-theoretic limit whic h says that the switching cost of Algorithm
1is optimal up to logarithmic factors. Since it is impossible to get sub-linear regret bound with
positive inherent Bellman error, we only consider the case w hereI= 0.
Theorem 4.2 (Lower boundfor no-regret learning) .Assume that the inherent Bellman error I= 0
anddh≥3for allh∈[H], for any algorithm with sub-linear regret bound, the global switching cost
is at least Ω(/summationtextH
h=1dh).
The proof of Theorem 4.2is sketched in Section 5.2with details in the Appendix.
Application tolinear MDP. AsdiscussedinSection 2.1,linearMDPwithdimension disaspecial
case of the low inherent Bellman error setting with I= 0,d1=d2=···=dH=d. Therefore,
when applied to linear MDP, our Algorithm 1will have switching cost bounded by O(dHlogK)
and regret bounded by /tildewideO(√
d2H3T), where T=KH.3Compared to current algorithms achieving
low switching cost under linear MDP [ Gao et al. ,2021,Wang et al. ,2021], we achieve the same
switching cost and a regret bound better by a factor of√
d. The improvement on regret bound
results from global optimism and a smaller linear function c lass. More importantly, low inherent
Bellman error setting is indeed a harder setting than linear MDP. According to Theorem 2 in
3When transferring Theorem 4.1to linear MDP, we need to rescale the reward function by H, and therefore there
will be an additional factor of Hin our regret bound.
10Zanette et al. [2020], the regret of our Algorithm 1is minimax optimal. Together with the lower
bound of switching cost (Theorem 4.2), Theorem 4.1is generally not improvable both in regret and
global switching cost.
Application to misspeciﬁed linear bandits. TakingH= 1, anMDPwithlowinherentBellman
error will become a linear bandit[ Lattimore and Szepesv´ ari ,2020] with model misspeciﬁcation. For
simplicity, we only consider the case where there is no missp eciﬁcation ( i.e.I= 0), as studied in
Abbasi-Yadkori et al. [2011]. Our result is summarized in the following corollary.
Corollary 4.3 (Results under linear bandit) .Suppose H= 1andI= 0, then the MDP reduces
to a linear bandit with dimension d. Our Algorithm 1will reduce to the RARELY SWITCHING
OFUL algorithm (Figure 3 in Abbasi-Yadkori et al. [2011]) and is computationally eﬃcient. The
global switching cost of Algorithm 1isO(dlogK), while the regret can be bounded by /tildewideO(d√
K)with
high probability.
The above corollary is derived by directly plugging H= 1 and d1=din Theorem 4.1. Note that
our Corollary 4.3matches the results in Abbasi-Yadkori et al. [2011], and our Algorithm 1can be
applied under the more general case with model misspeciﬁcat ion. Therefore, our results can be
seen as strict generalization of Abbasi-Yadkori et al. [2011].
5 Proof sketch
Due to the space constraint, we sketch the proof in this secti on while more details are deferred to
the Appendix. We begin with the proof overview of Theorem 4.1.
5.1 Upper bounds
Upper bound of switching cost. Let{k1,k2,···,kN}be the episodes where the algorithm
updates the policy (N is the global switching cost), and we al so deﬁne k0= 0.
According to the update rule (line 7 of Algorithm 1), every time the policy is updated, at least one
det(Σk
h) doubles, which implies that ΠH
h=1det(Σki+1
h)≥2ΠH
h=1det(Σki
h) for alli∈[N]. This further
implies
ΠH
h=1det(ΣkN
h)≥2NΠH
h=1det(Σk0
h).
Since the left hand side can be upper bounded by K/summationtextH
h=1dh(details in Lemma D.2) and the
right hand side is just 2N(from deﬁnition), the global switching cost ( i.e.N) is bounded by
O(/summationtextH
h=1dhlogK).
Below we give a proof overview of the regret bound.
Upper bound of regret. We denote ¯Qk
h(·,·) =Qh(¯θk
h)(·,·) =φh(·,·)⊤¯θk
h, where¯θk
his the solution
of Deﬁnition 3.1at thek-th episode. Similarly, ¯Vk
h(·) =Vh(¯θk
h)(·). In addition, let bkdenote the
last policy update before episode k, for allk∈[K].
Based on concentration inequalities of self-normalized pr ocesses, we can show that with high prob-
ability, the “best feasible” approximant parameter θ⋆(Deﬁnition A.3) is a feasible solution of
Deﬁnition 3.1. Therefore, the ¯Vk
1(s1) is always a nearly optimistic estimate of V⋆
1(s1) (summarized
11in Lemma A.5) and we only need to bound
Regret(K) =K/summationdisplay
k=1/parenleftBig
V⋆
1(s1)−Vπbk
1(s1)/parenrightBig
≤HKI+K/summationdisplay
k=1/parenleftBig
¯Vbk
1(s1)−Vπbk
1(s1)/parenrightBig
.(4)
Meanwhile, thepointwiseBellmanerrorcanbeboundedas(th isresultisstatedinLemmaA.6)
/vextendsingle/vextendsingle/vextendsingle/parenleftBig
¯Qbk
h−Th¯Qbk
h+1/parenrightBig
(s,a)/vextendsingle/vextendsingle/vextendsingle≤ I+2/bardblφh(s,a)/bardbl(Σbk
h)−1/radicalBig
αbk
h,
where/radicalBig
αk
h≤√
KI+/tildewideO(/radicalbig
dh+dh+1).
As a result, applying regret decomposition accross diﬀerent layersh∈[H] and bounding the
martingale diﬀerence by Azuma-Hoeﬀding inequality (Lemma D. 1), we have
K/summationdisplay
k=1/parenleftBig
¯Vbk
1(s1)−Vπbk
1(s1)/parenrightBig
≤K/summationdisplay
k=1H/summationdisplay
h=1/parenleftbigg
I+2/vextenddouble/vextenddouble/vextenddoubleφh(sk
h,ak
h)/vextenddouble/vextenddouble/vextenddouble
(Σbk
h)−1/radicalBig
αbk
h/parenrightbigg
+Sum of bounded martingale diﬀerence
≤K/summationdisplay
k=1H/summationdisplay
h=12/vextenddouble/vextenddouble/vextenddoubleφh(sk
h,ak
h)/vextenddouble/vextenddouble/vextenddouble
(Σbk
h)−1/radicalBig
αbk
h
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
(a)
+HKI+/tildewideO(H/summationdisplay
h=1/radicalbig
dhK).(5)
Due to our update rule based on det(Σk
h), we have
(a)≤H/summationdisplay
h=12/radicalBig
αK
h·/radicaltp/radicalvertex/radicalvertex/radicalbtKK/summationdisplay
k=1/vextenddouble/vextenddoubleφh(sk
h,ak
h)/vextenddouble/vextenddouble2
(Σbk
h)−1
≤H/summationdisplay
h=12/radicalBig
αK
h·/radicaltp/radicalvertex/radicalvertex/radicalbt2KK/summationdisplay
k=1/vextenddouble/vextenddoubleφh(sk
h,ak
h)/vextenddouble/vextenddouble2
(Σk
h)−1
≤/tildewideO/parenleftBiggH/summationdisplay
h=1(√
KI+/radicalbig
dh+dh+1)·/radicalbig
Kdh/parenrightBigg
≤/tildewideO(H/summationdisplay
h=1/radicalbig
dhKI+H/summationdisplay
h=1dh√
K),(6)
12where the second inequality holds because of Lemma D.3 and ou r update rule. The third inequality
is from elliptical potential lemma (Lemma D.4).
Finally, the regret bound results from plugging ( 6) into (5).
5.2 Lower bound
In this part, we sketch the proof of Theorem 4.2.
We construct a hard MDP case with zero inherent Bellman error (I= 0), which has deterministic
transition kernel. Therefore, deploying some determinist ic policy will lead to a deterministic tra-
jectory, like pulling an “arm” in the multi-armed bandits (M AB) setting. We further show that
the number of such “arms” is at least Ω(/summationtextH
h=1dh). Together with the lower bounds of switching
cost in multi-armed bandits [ Qiao et al. ,2022], we can derive the Ω(/summationtextH
h=1dh) lower bound under
the low inherent Bellman error setting.
6 Extension to generalized linear function approximation
In this section, we consider low adaptive reinforcement lea rning with generalized linear function
approximation [ Wang et al. ,2019]. We show that the same “doubling schedule” for updating
exploration policy (line 7 of Algorithm 1) can be leveraged under this setting, which enables the
designofprovablyeﬃcient algorithms. Webeginwiththeint roductionofgeneralized linearfunction
approximation.
6.1 Problem setup
Diﬀerent from the low inherent Bellman error setting which ch aracterizes Q⋆using linear functions,
we use a function class of generalized linear models (GLMs) t o modelQ⋆. We denote the dimension
of feature map by dand deﬁne Bd={x∈Rd:/bardblx/bardbl2≤1}.
Deﬁnition 6.1 (GLM [Wang et al. ,2019]).For a known feature map φ:S × A → Bdand a
known link function f: [−1,1]→[−1,1], the class of generalized linear models is G={(s,a)→
f(/an}bracketle{tφ(s,a),θ/an}bracketri}ht) :θ∈Bd}.
Similar to Wang et al. [2019], we make the following standard assumption which is withou t loss of
generality.
Assumption 6.2. f(·)is either monotonically increasing or decreasing. Further more, there exist
absolute constants 0< κ1< κ2<∞andM <∞such that κ1≤ |f′(z)| ≤κ2and|f′′(z)| ≤M, for
all|z| ≤1.
This assumption is naturally satisﬁed by the identical map f(z) =zand also includes other non-
linear maps such as the logistic map f(z) = 1/(1+e−z).
To characterize completeness under this function class, Wang et al. [2019] assumes the function
class is closed with respect to the Bellman operator Th(deﬁned in Section 2). Similarly, we make
the same optimistic closure assumption below. Note that for a ﬁxed constant Γ >04, the enlarged
function class is deﬁned as
4Γ will be set to depend polynomially on dand logK.
13Gup={(s,a)→min{1,f(/an}bracketle{tφ(s,a),θ/an}bracketri}ht) +γ/bardblφ(s,a)/bardblA}:
θ∈Bd,0≤γ≤Γ, A/followsorcurly0,/bardblA/bardbl2≤1}.
Then the optimistic closure assumption is stated below.
Assumption 6.3. For allh∈[H]andg∈ Gup, we have Th(g)∈ G.
According to Proposition 1 of Wang et al. [2019], this assumption strictly generalizes the standard
linear MDP setting by allowing link functions with more expr essivity.
6.2 Low switching algorithm
We present our Algorithm 2below. Intuitively speaking, the algorithmic idea is to app ly doubling
schedule to Algorithm 1 of Wang et al. [2019]. Similar to Algorithm 1, we only update the explo-
ration policy when the “information gain” with respect to so me layer has doubled (line 7). When
the policy is updated, the LSVI step calculates an estimate o fθ⋆(the parameter w.r.t. the real
Q⋆function) iteratively from the H-th layer to the ﬁrst layer through minimizing ( 7). Then the
optimistic Qvalue function is constructed by adding a bonus term γ/bardblφ(·,·)/bardbl(Σk
h)−1to the empirical
estimate f(φ(·,·)⊤θk
h) (line 11). Finally, the greedy policy is deployed for colle cting data (line 12,
18).
6.3 Main results of Algorithm 2
In this part, we state the main results about Algorithm 2. We begin with the upper bounds for
regret and switching cost. Recall that we still assume/summationtextH
h=1rh∈[0,1] almost surely, while d
represents the dimension of the feature map.
Theorem 6.4 (Main results) .The global switching cost of Algorithm 2is bounded by O(dH·logK).
In addition, with probability 1−δ, the regret of Algorithm 2overKepisodes is bounded by
Regret(K)≤/tildewideO/parenleftBig
H√
d3K/parenrightBig
.
The proof of Theorem 6.4is deferred to Appendix Cdue to space limit, below we discuss several
interesting aspects of Theorem 6.4.
Near-optimal switching cost. Our algorithm achieves a switching cost that depends logari thmi-
cally onK, which improves the O(K) switching cost of Algorithm 1 in Wang et al. [2019]. We also
prove the following information-theoretic limit which say s that the switching cost of Algorithm 2
is optimal up to logarithmic factors.
Theorem 6.5 (Lower boundforno-regretlearning) .For any algorithm with sub-linear regret bound,
the global switching cost is at least Ω(dH).
Theorem 6.5isadaptedfromthelower boundforglobal switchingcostund erlinearMDP[ Gao et al. ,
2021], and we leave the proof to Appendix C.
14Algorithm 2 Low-switching-cost LSVI-UCB with generalized linear func tion approximation
1:Input: Numberof episodes K, featuremap {φ(·,·)}, failureprobability δ, parameters κ1,κ2,M,
universal constant C.
2:Initialize : Σh= Σ0
h=Id, for allh∈[H].γ=Cκ2κ−1
1/radicalBig
1+M+κ2+d2log(1+κ2+Γ
δ).
3:fork= 1,2,···,Kdo
4:forh= 1,2,···,Hdo
5:Σk
h=/summationtextk−1
τ=1φ(sτ
h,aτ
h)φ(sτ
h,aτ
h)⊤+Id.
6:end for
7:if∃h∈[H], det(Σk
h)≥2det(Σ h)then
8:SetQk
H+1(·,·) = 0.
9:forh=H,···,1do
10: Solve the empirically optimal estimate of θ⋆.
θk
h= arg min
/bardblθ/bardbl2≤1k−1/summationdisplay
τ=1(f(/an}bracketle{tφ(sτ
h,aτ
h),θ/an}bracketri}ht)−rτ
h−max
a′∈AQk
h+1(sτ
h+1,a′))2. (7)
11: Construct the Q value function: Qk
h(·,·) = min{1,f(φ(·,·)⊤θk
h)+γ/bardblφ(·,·)/bardbl(Σk
h)−1}.
12: Setπk
h(s) = argmax a∈AQk
h(s,a).
13: Set Σh= Σk
h.
14:end for
15:else
16:Setπk
h=πk−1
hfor allh∈[H].
17:end if
18:Deploy policy πk= (πk
1,···,πk
H) and get trajectory ( sk
1,ak
1,rk
1,···,sk
H+1).
19:end for
Generalization over previous results. The closest result to our Algorithm 2is the fully adap-
tive Algorithm 1 of Wang et al. [2019], which achieves the same /tildewideO/parenleftBig
H√
d3K/parenrightBig
regret bound. In
comparison, our Algorithm 2favors near optimal global switching cost at the same time, w hich
saves computation and accelerates the learning process.
When applying our Algorithm 2to the linear MDP case, our Theorem 6.4will imply a regret bound
of/tildewideO(√
d3H3T)5(T=KH) and a global switching cost of O(dHlogK), which recovers the results
inGao et al. [2021],Wang et al. [2021]. Therefore, our result can be considered as generalizatio n
of these two results since GLMs allow more general function c lasses.
7 Conclusion and future work
This paper studied the well motivated problem of online rein forcement learning with low switch-
ing cost. Under linear Bellman-complete MDP with low inhere nt Bellman error, we designed an
algorithm (Algorithm 1) with near optimal regret bound of /tildewideO/parenleftBig/summationtextH
h=1dh√
K+/summationtextH
h=1√dhIK/parenrightBig
and
5The identical link function corresponds to κ1=κ2= 1 and M= 0. In addition, due to rescaling of reward
functions, there will be an additional Hfactor in the regret bound of Theorem 6.4.
15global switching cost boundof O(/summationtextH
h=1dh·logK). In addition, we prove a (nearly) matching global
switching cost lower bound Ω(/summationtextH
h=1dh) for any algorithm with sub-linear regret. At the same time,
we leverage the same “doubling trick” under the generalized linear function approximation setting,
and designed a sample-eﬃcient algorithm (Algorithm 2) with near optimal switching cost.
AlthoughbeingmoregeneralthanlinearMDP, thetwosetting s weconsiderarenotthemostgeneral
ones. The low Bellman eluder dimension setting [ Jin et al. ,2021] and MDP with diﬀerentiable
function approximation [ Zhang et al. ,2022a] can be considered as generalization of the two settings
in this paper, respectively. Therefore, our results can be c onsidered as a middle step towards low
switching reinforcement learningundermoregeneral MDP se ttings. For furtherextension, it will be
interesting to ﬁnd out whether low switching cost RL is possi ble under more general MDP settings
(e.g., low Bellman eluder dimension [ Jin et al. ,2021], diﬀerentiable function class [ Zhang et al. ,
2022a,Yin et al. ,2023]), and we leave these as future work.
Acknowledgments
The research is partially supported by NSF Award #2007117.
References
Yasin Abbasi-Yadkori, D´ avid P´ al, and Csaba Szepesv´ ari. Improved algorithms for linear stochastic
bandits. In Advances in Neural Information Processing Systems , pages 2312–2320, 2011.
M Mehdi Afsar, Traﬀord Crump, and Behrouz Far. Reinforcement learning based recommender
systems: A survey. arXiv preprint arXiv:2101.06286 , 2021.
Alekh Agarwal, Sham Kakade, Akshay Krishnamurthy, and Wen S un. Flambe: Structural com-
plexity and representation learning of low rank mdps. Advances in neural information processing
systems, 33:20095–20107, 2020.
Yu Bai, Tengyang Xie, Nan Jiang, and Yu-Xiang Wang. Provably eﬃcient q-learning with low
switching cost. Advances in Neural Information Processing Systems , 32, 2019.
Minbo Gao, Tianle Xie, Simon S Du, and Lin F Yang. A provably eﬃ cient algorithm for linear
markov decision process with low switching cost. arXiv preprint arXiv:2101.00494 , 2021.
Zijun Gao, Yanjun Han, Zhimei Ren, and Zhengqing Zhou. Batch ed multi-armed bandits problem.
Advances in Neural Information Processing Systems , 32, 2019.
Jiawei Huang, Jinglin Chen, Li Zhao, Tao Qin, Nan Jiang, and T ie-Yan Liu. Towards deployment-
eﬃcient reinforcement learning: Lower bound and optimalit y. InInternational Conference on
Learning Representations , 2022.
Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langf ord, and Robert E Schapire. Con-
textual decision processes with low bellman rank are pac-le arnable. In International Conference
on Machine Learning-Volume 70 , pages 1704–1713, 2017.
Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Pr ovably eﬃcient reinforcement
16learning with linear function approximation. In Conference on Learning Theory , pages 2137–
2143. PMLR, 2020.
Chi Jin, Qinghua Liu, and Sobhan Miryooseﬁ. Bellman eluder d imension: New rich classes of rl
problems, and sample-eﬃcient algorithms. Advances in neural information processing systems ,
34:13406–13418, 2021.
Jens Kober, J Andrew Bagnell, and Jan Peters. Reinforcement learning in robotics: A survey. The
International Journal of Robotics Research , 32(11):1238–1274, 2013.
Dingwen Kong, Ruslan Salakhutdinov, Ruosong Wang, and Lin F Yang. Online sub-sampling for
reinforcement learning with general function approximati on.arXiv preprint arXiv:2106.07203 ,
2021.
Tor Lattimore and Csaba Szepesv´ ari. Bandit algorithms . Cambridge University Press, 2020.
Tatsuya Matsushima, Hiroki Furuta, Yutaka Matsuo, Oﬁr Nach um, and Shixiang Gu. Deployment-
eﬃcient reinforcement learning via model-based oﬄine opti mization. In International Conference
on Learning Representations , 2020.
Aditya Modi, Jinglin Chen, Akshay Krishnamurthy, Nan Jiang , and Alekh Agarwal. Model-free
representation learning and exploration in low-rank mdps. arXiv preprint arXiv:2102.07035 ,
2021.
Vianney Perchet, Philippe Rigollet, Sylvain Chassang, and Erik Snowberg. Batched bandit prob-
lems.The Annals of Statistics , 44(2):660–681, 2016.
Dan Qiao and Yu-Xiang Wang. Near-optimal deployment eﬃcien cy in reward-free reinforcement
learning with linear function approximation. arXiv preprint arXiv:2210.00701 , 2022.
Dan Qiao, Ming Yin, Ming Min, and Yu-Xiang Wang. Sample-eﬃci ent reinforcement learning with
loglog(T) switching cost. In International Conference on Machine Learning , pages 18031–18061.
PMLR, 2022.
Wen Sun, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, an d John Langford. Model-based
rl in contextual decision processes: Pac bounds and exponen tial improvements over model-free
approaches. In Conference on learning theory , pages 2898–2933. PMLR, 2019.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction , volume 1. MIT
press Cambridge, 1998.
Grigoris Velegkas, Zhuoran Yang, and Amin Karbasi. The best of both worlds: Reinforcement
learning with logarithmic regret and policy switches. arXiv preprint arXiv:2203.01491 , 2022.
Ruosong Wang, Russ R Salakhutdinov, and Lin Yang. Reinforce ment learning with general value
function approximation: Provably eﬃcient approach via bou nded eluder dimension. Advances in
Neural Information Processing Systems , 33:6123–6135, 2020.
Tianhao Wang, Dongruo Zhou, and Quanquan Gu. Provably eﬃcie nt reinforcement learning with
linear function approximation under adaptivity constrain ts.Advances in Neural Information
Processing Systems , 34, 2021.
17YiningWang, RuosongWang, SimonSDu, andAkshay Krishnamur thy. Optimisminreinforcement
learning with generalized linear function approximation. arXiv preprint arXiv:1912.04136 , 2019.
Lin Yang and Mengdi Wang. Sample-optimal parametric q-lear ning using linearly additive features.
InInternational Conference on Machine Learning , pages 6995–7004. PMLR, 2019.
Ming Yin, Mengdi Wang, and Yu-Xiang Wang. Oﬄine reinforceme nt learning with diﬀerentiable
function approximation is provably eﬃcient. International Conference on Learning Representa-
tions, 2023.
Chao Yu, Jiming Liu, Shamim Nemati, and Guosheng Yin. Reinfo rcement learning in healthcare:
A survey. ACM Computing Surveys (CSUR) , 55(1):1–36, 2021.
Andrea Zanette, Alessandro Lazaric, Mykel Kochenderfer, a nd Emma Brunskill. Learning near op-
timal policies with low inherent bellman error. In International Conference on Machine Learning ,
pages 10978–10989. PMLR, 2020.
Ruiqi Zhang, Xuezhou Zhang, Chengzhuo Ni, and Mengdi Wang. O ﬀ-policy ﬁtted q-evaluation
with diﬀerentiable function approximators: Z-estimation a nd inference theory. In International
Conference on Machine Learning , pages 26713–26749. PMLR, 2022a.
Zihan Zhang, Yuan Zhou, and Xiangyang Ji. Almost optimal mod el-free reinforcement learning
via reference-advantage decomposition. Advances in Neural Information Processing Systems , 33:
15198–15207, 2020.
Zihan Zhang, Yuhang Jiang, Yuan Zhou, and Xiangyang Ji. Near -optimal regret bounds for multi-
batch reinforcement learning. arXiv preprint arXiv:2210.08238 , 2022b.
Zhenpeng Zhou, Steven Kearnes, Li Li, Richard N Zare, and Pat rick Riley. Optimization of
molecules via deep reinforcement learning. Scientiﬁc reports , 9(1):1–10, 2019.
18A Proof of Theorem 4.1
In this section, we prove our main theorem. We ﬁrst restate Th eorem4.1below, and then prove
the bounds for switching cost and regret in Section A.1and Section A.2, respectively.
Theorem A.1 (Restate Theorem 4.1).The global switching cost of Algorithm 1is bounded by
O(/summationtextH
h=1dh·logK). In addition, with probability 1−δ, the regret of Algorithm 1overKepisodes
is bounded by
Regret(K)≤/tildewideO/parenleftBiggH/summationdisplay
h=1dh√
K+H/summationdisplay
h=1/radicalbig
dhIK/parenrightBigg
.
A.1 Proof of switching cost bound
Proof of switching cost bound. Let{k1,k2,···,kN}be the episodes where the algorithm updates
the policy, and we also deﬁne k0= 0.
According to the update rule (line 7 of Algorithm 1), for all i∈[N], there exists some hi∈[H]
such that
det(Σki+1
hi)≥2det(Σki
hi).
In addition, for all h,i∈[H]×[N], we have
det(Σki+1
h)≥det(Σki
h).
Combining these two results, we have for all i∈[N],
ΠH
h=1det(Σki+1
h)≥2ΠH
h=1det(Σki
h). (8)
Therefore, it holds that
K/summationtextH
h=1dh≥ΠH
h=1det(ΣkN
h)≥2NΠH
h=1det(Σk0
h) = 2N, (9)
wherethe ﬁrstinequality is becauseof Lemma D.2and our choice that λ= 1. Thesecond inequality
is due to recursive application of ( 8). The last equation holds since we have Σk0
h=Idhfor allh.
Solving( 9), wehave N≤/summationtextH
h=1dhlogK
log2=O(/summationtextH
h=1dhlogK),andthereforetheproofiscomplete.
A.2 Proof of regret bound
We ﬁrst state some technical lemmas from Zanette et al. [2020]. We begin with the following bound
on failure probability.
Lemma A.2 (Lemma 2 of Zanette et al. [2020]).With probability at least 1−δ/2, for allk∈[K],
h∈[H],Vh+1∈ Vh+1,
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublek−1/summationdisplay
i=1φi
h/parenleftBig
ri
h−rh(si
h,ai
h)+Vh+1(si
h+1)−Es′∼Ph(·|si
h,ai
h)Vh+1(s′)/parenrightBig/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
(Σk
h)−1≤/radicalBig
βk
h,(10)
where/radicalBig
βk
h:=/radicalBig
dhlog(1+k/dh)+2dh+1log(1+4√kdh)+log(2KH
δ)+1 =/tildewideO(/radicalbig
dh+dh+1).
19Next, we deﬁne the “best” feasible parameters θ⋆that well approximate the Q⋆values, and such
parameters are going to be a feasible solution for the optimi zation problem (Deﬁnition 3.1). Then
we state the accuracy bound of θ⋆.
Deﬁnition A.3 (Best feasible approximant, Deﬁnition 4 of Zanette et al. [2020]).We recursively
deﬁne the best approximant parameter θ⋆
hforh∈[H]as:
θ⋆
h= arg min
θ∈Bhsup
(s,a)/vextendsingle/vextendsingle/vextendsingleφh(s,a)⊤θ−(ThQh+1(θ⋆
h+1))(s,a)/vextendsingle/vextendsingle/vextendsingle (11)
with ties broken arbitrarily and θ⋆
H+1= 0.
Lemma A.4 (Accuracy Bound of θ⋆, Lemma 6 of Zanette et al. [2020]).It holds that for all
h∈[H]:
sup
(s,a)|Q⋆
h(s,a)−φh(s,a)⊤θ⋆
h| ≤(H−h+1)I. (12)
For notational simplicity, for ¯θhwhich is the solution of Deﬁnition 3.1, we denote ¯Qh(·,·) =
Qh(¯θh)(·,·) =φh(·,·)⊤¯θh. Besides, ¯Qk
hrepresents Qh(¯θk
h) where ¯θk
his the solution at the k-th
episode. Similarly, ¯Vh(·) =Vh(¯θh)(·) and¯Vk
h(·) =Vh(¯θk
h)(·). In addition, let bkdenote the last
policy update before episode k, for allk∈[K].
Lemma A.5 (Optimism, Lemma 7 of Zanette et al. [2020]).Under the high probability case in
LemmaA.2, if we choose/radicalBig
αk
h=/radicalBig
βk
h+√
kI+√dh=√
kI+/tildewideO(/radicalbig
dh+dh+1), then¯θh=θ⋆
h, for
allh∈[H]is a feasible solution of the optimization problem (Deﬁniti on 3.1). Therefore, for all
k∈[K], the optimistic value function satisﬁes
¯Vbk
1(s1)≥V⋆
1(s1)−HI. (13)
In addition to optimism, we also have the following upper bou nd of Bellman error.
Lemma A.6 (Bound of Bellman error, Lemma 1 of Zanette et al. [2020]).Under the high proba-
bility case in Lemma A.2, it holds that for all (k,h,s,a)∈[K]×[H]×S ×A,
/vextendsingle/vextendsingle/vextendsingle/parenleftBig
¯Qbk
h−Th¯Qbk
h+1/parenrightBig
(s,a)/vextendsingle/vextendsingle/vextendsingle≤ I+/bardblφh(s,a)/bardbl(Σbk
h)−1/parenleftbigg/radicalbig
bkI+/radicalBig
βbk
h+/radicalbig
dh+/radicalBig
αbk
h/parenrightbigg
=I+2/bardblφh(s,a)/bardbl(Σbk
h)−1/radicalBig
αbk
h.(14)
Now we are ready to present the regret analysis of Algorithm 1.
Proof of regret bound. We prove based on the high probability case in Lemma A.2.
20First of all, the regret over Kepisodes can be decomposed as
Regret(K) =K/summationdisplay
k=1(V⋆
1(s1)−Vπk
1(s1))
=K/summationdisplay
k=1/parenleftBig
V⋆
1(s1)−Vπbk
1(s1)/parenrightBig
=K/summationdisplay
k=1/parenleftBig
V⋆
1(s1)−¯Vbk
1(s1)/parenrightBig
+K/summationdisplay
k=1/parenleftBig
¯Vbk
1(s1)−Vπbk
1(s1)/parenrightBig
≤HKI+K/summationdisplay
k=1/parenleftBig
¯Vbk
1(s1)−Vπbk
1(s1)/parenrightBig
,(15)
where the last inequality results from Lemma A.5.
Note that ¯Vbk
h(sk
h) =¯Qbk
h(sk
h,ak
h) due to our choice of πk, it holds that for all k,h∈[K]×[H],
/parenleftBig
¯Vbk
h−Vπbk
h/parenrightBig
(sk
h) =¯Qbk
h(sk
h,ak
h)−Th¯Qbk
h+1(sk
h,ak
h)+Th¯Qbk
h+1(sk
h,ak
h)−Vπbk
h(sk
h)
≤I+2/vextenddouble/vextenddouble/vextenddoubleφh(sk
h,ak
h)/vextenddouble/vextenddouble/vextenddouble
(Σbk
h)−1/radicalBig
αbk
h+Es′∼Ph(·|sk
h,ak
h)/parenleftBig
¯Vbk
h+1−Vπbk
h+1/parenrightBig
(s′),(16)
where the inequality holds because of Lemma A.6.
Plugging ( 16) into (15), we have with probability 1 −δ,
Regret(K)≤HKI+K/summationdisplay
k=1/parenleftBig
¯Vbk
1(s1)−Vπbk
1(s1)/parenrightBig
≤HKI+K/summationdisplay
k=1H/summationdisplay
h=1/parenleftbigg
I+2/vextenddouble/vextenddouble/vextenddoubleφh(sk
h,ak
h)/vextenddouble/vextenddouble/vextenddouble
(Σbk
h)−1/radicalBig
αbk
h/parenrightbigg
+H/summationdisplay
h=1K/summationdisplay
k=1/parenleftBig
Es′∼Ph(·|sk
h,ak
h)/parenleftBig
¯Vbk
h+1−Vπbk
h+1/parenrightBig
(s′)−/parenleftBig
¯Vbk
h+1−Vπbk
h+1/parenrightBig
(sk
h+1)/parenrightBig
≤HKI+K/summationdisplay
k=1H/summationdisplay
h=1/parenleftbigg
I+2/vextenddouble/vextenddouble/vextenddoubleφh(sk
h,ak
h)/vextenddouble/vextenddouble/vextenddouble
(Σbk
h)−1/radicalBig
αbk
h/parenrightbigg
+/tildewideO(H/summationdisplay
h=1/radicalbig
dhK)
=2HKI+/tildewideO(H/summationdisplay
h=1/radicalbig
dhK)+K/summationdisplay
k=1H/summationdisplay
h=12/vextenddouble/vextenddouble/vextenddoubleφh(sk
h,ak
h)/vextenddouble/vextenddouble/vextenddouble
(Σbk
h)−1/radicalBig
αbk
h,(17)
where the second inequality is because of ( 16). The last inequality holds with high probability due
to Azuma-Hoeﬀding inequality (Lemma D.1) and the fact that /bardbl¯Vbk
h+1/bardbl∞≤ /bardbl¯θbk
h+1/bardbl2≤/radicalbig
dh+1for
anyk∈[K].
21Finally, it holds that
Regret(K)≤2HKI+/tildewideO(H/summationdisplay
h=1/radicalbig
dhK)+K/summationdisplay
k=1H/summationdisplay
h=12/vextenddouble/vextenddouble/vextenddoubleφh(sk
h,ak
h)/vextenddouble/vextenddouble/vextenddouble
(Σbk
h)−1/radicalBig
αbk
h
≤2HKI+/tildewideO(H/summationdisplay
h=1/radicalbig
dhK)+2H/summationdisplay
h=1/radicalBig
αK
h·/radicaltp/radicalvertex/radicalvertex/radicalbtKK/summationdisplay
k=1/vextenddouble/vextenddoubleφh(sk
h,ak
h)/vextenddouble/vextenddouble2
(Σbk
h)−1
≤/tildewideO(HKI+H/summationdisplay
h=1/radicalbig
dhK)+2H/summationdisplay
h=1/radicalBig
αK
h·/radicaltp/radicalvertex/radicalvertex/radicalbtKK/summationdisplay
k=12/vextenddouble/vextenddoubleφh(sk
h,ak
h)/vextenddouble/vextenddouble2
(Σk
h)−1
≤/tildewideO(HKI+H/summationdisplay
h=1/radicalbig
dhK)+2H/summationdisplay
h=1/radicalBig
αK
h·/radicalbig
2K·2dhlog(1+K)
≤/tildewideO(HKI+H/summationdisplay
h=1/radicalbig
dhK)+/radicalbig
Klog(1+K)·/tildewideO/parenleftBiggH/summationdisplay
h=1/radicalbig
KdhI+/radicalbig
dh(dh+dh+1)/parenrightBigg
≤/tildewideO(H/summationdisplay
h=1/radicalbig
dhKI+H/summationdisplay
h=1dh√
K),(18)
where the second inequality holds according to Cauchy-Schw arz inequality and the fact that αk
his
non-decreasing in k. The third inequality results from Lemma D.3and the fact that det((Σbk
h)−1) =
det(Σbk
h)−1≤2det(Σk
h)−1= 2det((Σk
h)−1). The forth inequality is because of elliptical potential
lemma (Lemma D.4). The ﬁfth inequality is derived by the deﬁnition of αK
h(from Lemma A.5).
The last inequality comes from direct calculation.
The regret analysis is complete.
B Proof of Theorem 4.2
In this section, we prove our lower bound of switching cost.
Theorem B.1 (Restate Theorem 4.2).Assume that the inherent Bellman error I= 0anddh≥3
for allh∈[H], for any algorithm with sub-linear regret bound, the global switching cost is at least
Ω(/summationtextH
h=1dh).
We ﬁrst brieﬂy discuss about our assumptions. We assume zero inherent Bellman error ( i.e.I=
0) since it is possible to derive sub-linear regret bounds on ly ifI= 0, and we want to derive
lower bounds of switching cost for algorithms with sub-line ar regret. Otherwise, the regret bound
will always be linear in K. Also, the assumption on dh≥3 for all h∈[H] is without loss of
generality.
Proof of Theorem B.1.We ﬁrst construct an MDP with two states, the initial state s1and the
absorbing state s2.
For absorbing state s2, the choice of action is only a0, while for initial state s1, the choice of actions
22at layerhis{a1,a2,···,adh−1}. Then we deﬁne the dh-dimensional feature map for the h-th layer:
φh(s2,a0) = (1,0,0,···,0), φh(s1,ai) = (0,···,0,1,0,···),
where for s1,ai(i∈[dh−1]), the ( i+1)-th element is 1 while all other elements are 0.
We now deﬁne the transition kernel and reward function as Ph(s2|s2,a0) = 1,rh(s2,a0) = 0,
Ph(s1|s1,a1) = 1,rh(s1,a1) = 0 for all h∈[H]. Besides, Ph(s2|s1,ai) = 1,rh(s1,ai) =rh,ifor all
h∈[H] and 2≤i≤dh, whererh,i’s are unknown non-zero values. Note that such MDP has zero
inherent Bellman error ( I= 0) since the function class {φh(s,a)⊤θh|θh∈ Bh}includes all possible
Q-value functions.
Therefore, for any deterministic policy, the only possible case is that the agent takes action a1and
stays ats1for the ﬁrst h−1 steps, then at step hthe agent takes action ai(i≥2) and transitions
tos2with reward rh,i, later the agent always stays at s2with no more reward. For this trajectory,
the total reward will be rh,i. Also, for any deterministic policy, the trajectory is ﬁxed , like pulling
an “arm” in multi-armed bandits setting. Note that the total number of such “arms” with non-zero
unknown reward is at least/summationtextH
h=1(dh−2) = Ω(/summationtextH
h=1dh) due to our assumption that dh≥3. Even
if the transition kernel is known to the agent, this MDP is sti ll as diﬃcult as a multi-armed bandits
problem with Ω(/summationtextH
h=1dh) arms. Together will Lemma B.2below, the proof is complete.
Lemma B.2 (Lemma H.4 of Qiao et al. [2022]).For any algorithm with sub-linear regret bound
underK-armed bandit problem, the switching cost is at least Ω(K).
C Proof for Section 6
In this section, we prove the theorems regarding our Algorit hm2under the generalized linear func-
tion approximation setting. We begin with the upper bounds f or switching cost and regret.
C.1 Proof of upper bounds
Theorem C.1 (Restate Theorem 6.4).The global switching cost of Algorithm 2is bounded by
O(dH·logK). In addition, with probability 1−δ, the regret of Algorithm 2overKepisodes is
bounded by
Regret(K)≤/tildewideO/parenleftBig
H√
d3K/parenrightBig
.
Proof of switching cost bound. Since the feature map in Algorithm 2satisﬁes that for all s,a∈
S ×A,φ(s,a)∈Bd={x∈Rd:/bardblx/bardbl2≤1}, we have /bardblφ(s,a)/bardbl2≤1. Therefore, the conclusion of
LemmaD.2still holds, with dh=dfor allh∈[H]. In addition, because our policy update rule
(line 7 of Algorithm 2) is identical to Algorithm 1, theO(dH·logK) upper bound of switching cost
results from identical proof as in Section A.1, with all dhreplaced by d.
Before we prove the upper bound of regret, we state some techn ical lemmas from Wang et al.
[2019].
23Lemma C.2 (Corollary 3 of Wang et al. [2019]).We denote the estimated Q value function of
layerhat thek-th episode by Qk
h(·,·). Suppose there exists a function confk
h:S × A → R+such
that for all (k,h,s,a)∈[K]×[H]×S ×A,
Q⋆
h(s,a)≤Qk
h(s,a)≤ Th(Qk
h+1)(s,a)+confk
h(s,a), (19)
(whereThis Bellman operator) and the policy πkis the greedy policy with respect to Qk
h, then with
probability at least 1−δ,
Regret(K)≤K/summationdisplay
k=1H/summationdisplay
h=1confk
h(sk
h,ak
h)+O(H/radicalbig
Klog(1/δ)).
LemmaC.2is a standard regret decomposition which will be used to boun d the regret of Algorithm
2. Below we give a valid choice of the conﬁdence bound confk
h. Note that we deﬁne bkto be the
last policy update before episode k, for allk∈[K]. Therefore, Qk
h=Qbk
hfor allk∈[K].
Lemma C.3 (Adapted from Lemma 6 of Wang et al. [2019]).With probability 1−δ, it holds that
for allk,h,s,a∈[K]×[H]×S ×A,
/vextendsingle/vextendsingle/vextendsinglef(/an}bracketle{tφ(s,a),θk
h/an}bracketri}ht)−Th(Qk
h+1)(s,a)/vextendsingle/vextendsingle/vextendsingle≤γ/bardblφ(s,a)/bardbl(Σbk
h)−1,
whereγis deﬁned in Algorithm 2.
Therefore, optimism is straightforward.
Lemma C.4 (Corollary 5 of Wang et al. [2019]).Under the high probability case in Lemma C.3,
for allk,h,s,a∈[K]×[H]×S ×A,Qk
h(s,a)≥Q⋆
h(s,a).
Combining optimism (Lemma C.4) with Lemma C.3, we have that Qk
hin Algorithm 2satisﬁes
condition ( 19) with confk
h(s,a) =γ/bardblφ(s,a)/bardbl(Σbk
h)−1. Below we boundthe summation of bonus.
Lemma C.5. Assume that confk
h(s,a) =γ/bardblφ(s,a)/bardbl(Σbk
h)−1, then it holds that
K/summationdisplay
k=1H/summationdisplay
h=1confk
h(sk
h,ak
h) =K/summationdisplay
k=1H/summationdisplay
h=1γ/bardblφ(sk
h,ak
h)/bardbl(Σbk
h)−1≤Hγ/radicalbig
4Kdlog(1+K).(20)
Proof of Lemma C.5.
K/summationdisplay
k=1H/summationdisplay
h=1γ/bardblφ(sk
h,ak
h)/bardbl(Σbk
h)−1≤H/summationdisplay
h=1γ/radicaltp/radicalvertex/radicalvertex/radicalbtKK/summationdisplay
k=1/vextenddouble/vextenddoubleφ(sk
h,ak
h)/vextenddouble/vextenddouble2
(Σbk
h)−1
≤H/summationdisplay
h=1γ/radicaltp/radicalvertex/radicalvertex/radicalbtKK/summationdisplay
k=12/vextenddouble/vextenddoubleφ(sk
h,ak
h)/vextenddouble/vextenddouble2
(Σk
h)−1
≤H/summationdisplay
h=1γ/radicalbig
2K·2dlog(1+K)
=Hγ/radicalbig
4Kdlog(1+K),(21)
24where the ﬁrst inequality holds according to Cauchy-Schwar z inequality. The second inequality re-
sults from Lemma D.3and the fact that det((Σbk
h)−1) = det(Σbk
h)−1≤2det(Σk
h)−1= 2det((Σk
h)−1).
The third inequality is because of elliptical potential lem ma (Lemma D.4).
Now we are ready to present the proof of the regret upper bound .
Proof of regret upper bound. The ﬁnal/tildewideO(H√
d3K) regret upper bound is derived by combining
LemmaC.2, Lemma C.5and the deﬁnition that γ=/tildewideO(d).
C.2 Proof of lower bound
Finally, we present the proof of the lower bound.
Theorem C.6 (Restate Theorem 6.5).For any algorithm with sub-linear regret bound, the global
switching cost is at least Ω(dH).
Proof of Theorem C.6.Since linear MDP is a special case of generalized linear func tion approxima-
tion, the Ω( dH) lower bound of global switching cost in Gao et al. [2021] holds here.
D Assisting technical lemmas
Lemma D.1 (Azuma-Hoeﬀding inequality) .LetXibe a martingale diﬀerence sequence such that
Xi∈[−A,A]for some A >0. Then with probability at least 1−δ, it holds that:
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglen/summationdisplay
i=1Xi/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤/radicalbigg
2A2nlog(1
δ).
Lemma D.2 (Lemma C.1 of Wang et al. [2021]).Let{Σk
h}(h,k)∈[H]×[K]be as deﬁned in Algorithm
1. Then for all h∈[H]andk∈[K], we have det(Σk
h)≤(λ+k−1
dh)dh.
Lemma D.3 (Lemma 12 of Abbasi-Yadkori et al. [2011]).Suppose A,B∈Rd×dare two positive
deﬁnite matrices satisfying that A/followsorcurlyB, then for any x∈Rd, we have
/bardblx/bardbl2
A
/bardblx/bardbl2
B≤det(A)
det(B).
Lemma D.4 (Elliptical Potential Lemma, Lemma 26 of Agarwal et al. [2020]).Consider a se-
quence of d×dpositive semi-deﬁnite matrices X1,···,XTwithmaxtTr(Xt)≤1and deﬁne
M0=I,···,Mt=Mt−1+Xt. Then
T/summationdisplay
t=1Tr(XtM−1
t−1)≤2dlog(1+T
d).
25