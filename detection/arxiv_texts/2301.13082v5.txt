PaCaNet: A Study on CycleGAN with Transfer Learning for
Diversifying Fused Chinese Painting and Calligraphy
Zuhao Yang *
yang0756@e.ntu.edu.sg
School of Computer Science and
Engineering
Nanyang Technological University
SingaporeHuajun Bai *
hb364@cornell.edu
Seafog AI
ChinaZhang Luo
luozhang@seafogai.com
Seafog AI
China
Yang Xu
yang.xu@sdsu.edu
Department of Computer Science
San Diego State University
USAWei Pang
w.pang@hw.ac.uk
School of Mathematical and Computer
Sciences
Heriot-Watt University
UKYue Wang
wang.yue.f07@kyoto-u.jp
Graduate School of Advanced
Integrated Studies in Human
Survivability
Kyoto University
Japan
Yisheng Yuan
yyuan01@risd.edu
Graphic Design Department
Rhode Island School of Design
USAYeqi Hu
huyeqi@stu.ouc.edu.cn
College of Information Science and
Engineering
Ocean University of China
ChinaYingfang Yuan†
y.yuan@hw.ac.uk
School of Mathematical and Computer
Sciences
Heriot-Watt University
UK
Figure 1: Generated artistic calligraphy fused with various Chinese paintings via our PaCaNet (pre-training stage) framework.
ABSTRACT
AI-Generated Content (AIGC) has recently gained a surge in popu-
larity, powered by its high efficiency and consistency in production,
and its capability of being customized and diversified. The cross-
modality nature of the representation learning mechanism in most
AIGC technology allows for more freedom and flexibility in explor-
ing new types of art that would be impossible in the past. Inspired by
the pictogram subset of Chinese characters, we proposed PaCaNet,
*Equal contribution
†Corresponding authora CycleGAN-based pipeline for producing novel artworks that fuse
two different art types, traditional Chinese painting andcalligraphy .
In an effort to produce stable and diversified output, we adopted
three main technical innovations: (1) Using one-shot learning to in-
crease the creativity of pre-trained models and diversify the content
of the fused images. (2) Controlling the preference over generated
Chinese calligraphy by freezing randomly sampled parameters in
pre-trained models. (3) Using a regularization method to encourage
the models to produce images similar to Chinese paintings. Further-
more, we conducted a systematic study to explore the performancearXiv:2301.13082v5  [cs.CV]  21 May 2023of PaCaNet in diversifying fused Chinese painting and calligraphy,
which showed satisfying results. In conclusion, we provide a new di-
rection of creating arts by fusing the visual information in paintings
and the stroke features in Chinese calligraphy. Our approach creates
a unique aesthetic experience rooted in the origination of Chinese
hieroglyph characters. It is also a unique opportunity to delve deeper
into traditional artwork and, in doing so, to create a meaningful
impact on preserving and revitalizing traditional heritage.
CCS CONCEPTS
•Applied computing →Fine arts ;•Computing methodologies
→Neural networks .
KEYWORDS
AI-generated content, neural networks, transfer learning
1 INTRODUCTION
AI-generated content (AIGC) refers to all types of content (e.g., text,
images, videos, and audio) that are created using artificial intelli-
gence algorithms rather than being produced by humans [ 19]. AIGC
can be used for a variety of purposes, such as creating stories [ 24],
creating posts on social media [ 22], generating literature reviews
[2], and producing creative advertisements [ 27]. People have been
investigating AIGC ever since it emerged as a technology. Over the
years, research and development in this field have grown signifi-
cantly. The reason why AIGC is becoming increasingly prevalent
is that AI can efficiently produce interesting and creative content
based on human knowledge. However, the level of sophistication
and quality of AI-generated content can vary greatly depending on
the selected algorithms and the real scenarios applied. Our study
focuses on diversifying fused Chinese ink painting (we will use
the Chinese painting for brevity in the following paragraphs) and
Chinese calligraphy.
Chinese painting is a classic form of Chinese art that has been
practiced in China for thousands of years. Chinese paintings are
well-known for their unique aesthetics and techniques. Over time,
the art has evolved to encompass a variety of subjects, such as
landscapes, figures, and animals. Chinese painting is also renowned
for employing symbolism and metaphor to convey spiritual and
philosophical ideals.
Chinese painting and Chinese calligraphy share similar stylistic
elements and tools. The brushstrokes used in paintings are also seen
in calligraphy, highlighting the close relationship between the two
art forms. However, Chinese calligraphy, which adopts a unique
pictographic writing system, has received widespread attention due
to its beauty and elegance [ 20,31]. Through continuous use for thou-
sands of years, this system with such ordering of the various parts
and harmony of proportions that the experienced and knowledge-
able people will recognize such composition as a work of art [ 21].
Unlike the alphabetic writing system, Chinese calligraphy requires
to achieve almost perfect proportions within specific grid spaces.
Specifically, the structure of Chinese calligraphy is an important
aspect that separates it from paintings. Various strokes that are ar-
ranged in a particular order form different Chinese characters. The
character’s structure is crucial since it can alter the character’s mean-
ing. Therefore, Chinese calligraphy could provide core guidance on
Figure 2: Output images generated by a pre-trained CycleGAN
model. Only a small proportion of features are fused.
composing specific spatial layouts. Another important characteristic
of Chinese calligraphy is the use of rhythm and movement, which
is shown through brushstrokes. This can help convey the mood and
atmosphere of the scene.
As mentioned above, Chinese painting and calligraphy share
many similarities, yet they also possess distinctive characteristics. It
would be intriguing to create images that combine the structure of
Chinese calligraphy with the features of Chinese painting, concep-
tually similar to how hieroglyphs represent both a character and an
image. As shown in Fig. 4b, the fused image on the right side retains
the structure and brushstrokes of the Chinese calligraphy character
from its adjacent image on the left side. At the same time, the fused
image also learns the pictorial quality of Chinese painting.
In our research, we proposed a new approach, termed Chinese
Painting and Calligraphy Network (PaCaNet), to diversify images
that fuse the structure of Chinese characters from calligraphy with
diverse elements of Chinese paintings. PaCaNet is based on Cycle-
Consistent Adversarial Networks (CycleGAN) [ 32] which was orig-
inally designed for image-to-image translations [14]. The core idea
of CycleGAN is to learn two respective distributions to support map-
ping between two domains. In our experiments, we discovered that
the original design of CycleGAN had limitations in generating a
diverse range of images. The majority of the generated images were
similar in style, which would have a detrimental effect on the orig-
inality of AI-generated fused paintings and calligraphy. As shown
in Fig. 1 and Fig. 2, we leveraged thousands of distinct unpaired
Chinese paintings and calligraphy characters to train the model.
However, the model is so restricted that it can only produce similar
images that incorporate only a small number of features (e.g. leaf
and flower) from thousands of different others present in the training
set. This goes against the key criterion of creativity in AIGC [ 28].
Furthermore, training a CycleGAN consumes a lot of computational
resources, and thus it is not cost-effective to generate a batch of sim-
ilar images with respect to AIGC. Therefore, we proposed PaCaNet
to resolve this dilemma.
In our study, PaCaNet starts with a source task that is to pre-train
a CycleGAN model using unpaired calligraphy images (domain 𝐴)
and Chinese landscape paintings (domain 𝐵). Domain𝐴contains
traditional paintings of many subjects, for example, mountains, flow-
ers, and rivers, but animals are excluded. Once the training process
is completed, the generator 𝐺𝐴can map the image 𝑎∈𝐴to𝐵while
𝐺𝐵can map the image 𝑏∈𝐵to𝐴, and𝐺𝐴can be used to gener-
ate fused images (Fig. 4). We suppose that 𝐺𝐴and𝐺𝐵are capable
of performing feature representation for images from 𝐴and𝐵, re-
spectively. Then, we employ three techniques: one-shot learning,
parameter freezing, and regularization to improve the creativity of
the fused images. Firstly , we diversify the fused images by adding
2a target task to learn a new mapping from 𝐴to𝐵′(Chinese ani-
mal paintings). In this way, the style of fused images will change
since more elements from 𝐵′will be added. Animal and landscape
paintings are different, but both belong to Chinese painting, which
means that shared characteristics can be processed by pre-trained
𝐺𝐴, and exclusive characteristics can be learned by one-shot learn-
ing with a tiny computational budget. By sampling different animal
images𝑏∈𝐵′as target tasks, we can cost-effectively obtain various-
style generators capable of producing diversified fused images for
arbitrary calligraphic input. Secondly , to preserve the structure of
calligraphy characters in the fused images, we proposed an approach
that freezes parameters of the pre-trained model based on random
sampling. Thirdly , we introduced a regularization method to en-
hance the capability of the generator 𝐺′
𝐴to produce images similar
to𝑏∈𝐵′. In fact,𝐵′can be generalized to other types of images with-
out limitation to animals. A more detailed description of PaCaNet
will be given in Section 3. Furthermore, we conducted a systematic
study to explore the performance of PaCaNet, and the results will be
discussed in Section 4.
In summary, the main contributions of our work are as follows:
•We proposed PaCaNet as a pioneering framework for explor-
ing the combination of two traditional Chinese cultural her-
itages. The generated images combine the structural features
of Chinese characters and the pictorial features of Chinese
paintings.
•We collected and organized public datasets for Chinese paint-
ing and calligraphy. Our curated dataset will be released
shortly to support other research.
•We confirmed that freezing randomly sampled parameters
performs better than layer freezing when performing transfer
learning.
•We introduced a regularization method that enables PaCaNet
to learn from the specific target, and thus improved the diver-
sity of output.
2 RELATED WORK
2.1 Cycle-Consistent Adversarial Networks
Cycle-Consistent Adversarial Networks (CycleGAN) is a type of
generative model that is capable of performing image-to-image trans-
lations without the need for paired training data. The CycleGAN
generators are trained using a cycle consistency loss, which ensures
that the translated image can be transformed back to its original do-
main while still being similar to the original image. There are several
applications that emerge afterward, such as Cycle-MedGAN [ 1] for
medical image translation, CycleGAN for music genre transfer [ 3],
and CycleGAN for the generation of handwritten Chinese charac-
ters [ 4], etc. From our viewpoint, even if CycleGAN has achieved
promising performance on various tasks, it still lacks exploration on
AIGC.
2.2 AIGC for Chinese Painting and Calligraphy
AIGC is the application of machine learning techniques to generate
creative content. It includes using deep learning models such as
Generative Adversarial Networks (GANs) [ 9] or Latent Diffusion
Model (LDM) [ 25] to produce new works of art. LDM has beenwidely explored and proved to be a state-of-the-art model in gener-
ating creative and aesthetic images from prompt tuning. However,
applying LDM to our task may lead to some semantic information
loss, for example, original structure of the calligraphic characters
may be discarded [ 25]. According to our literature review, we no-
ticed that an equally important yet underexplored problem is how to
fuse the artistic style of Chinese paintings with Chinese calligraphy.
Most of the earliest ancient Chinese characters are pictograms (象
形文), which are highly stylized and simplified pictures of objects.
This unique property makes pictogram characters perfect candidates
for creating visual arts, because it is natural to consider them as an
abstract representation that contains partial visual information of the
object being denoted, which can potentially have similar patterns
with paintings of the same object. Related to this idea is the fact
that raw visual information in Chinese characters and glyph have
been proven useful in language modeling [ 18,26], but we believe
their potentials in AIGC have not been explored. One of the major
challenges in AIGC for fused Chinese painting and calligraphy is the
high level of complexity and variation in the style and technique of
traditional Chinese art. We overcome this by using a large selected
dataset, which includes various types of high-quality traditional Chi-
nese paintings (e.g., animals, landscapes, and flowers). The AIGC of
the traditional Chinese cultural heritage is still worthy of our study.
2.3 Transfer Learning
Transfer learning is a technique in machine learning in which a
model developed for one task is reused as a starting point for another
task. The idea is to transfer the knowledge learned from the first task
to the second task, which can be useful in cases where the second
task has limited data or is related to the first task.
One-shot learning is a type of machine learning algorithm in
which the model is trained to assess the similarity and difference
between two images based on a single example. [ 7] shows rather
than learning from scratch, one can take advantage of the knowledge
gathered from previously learned source images to learn about a
single example in the target domain. [ 17] implemented one-shot
learning to learn strokes in novel characters from previous knowl-
edge of how handwritten characters are composed from strokes. The
one-shot learning model outperforms a competing state-of-the-art
character model and provides a good fit to human perceptual data.
Layer freezing in transfer learning refers to the process of keep-
ing certain parameters of network layers of a pre-trained model fixed
while allowing other parameters to be updated during training. It
helps prevent the model from forgetting the features it has learned
during the initial pre-training phase. In the study of malware classifi-
cation [ 23], a model consisting of ResNet-50 layers [ 11] pre-trained
on ImageNet [ 5] is trained by freezing the first 49 layers and leaving
the last layer to be trainable. It achieves high accuracy and shows that
the knowledge obtained in the ImageNet classification task can be
successfully transferred to malware classification tasks. In addition,
adaptive transfer learning approaches are proposed to better fine-
tune the pre-trained model on the new task. For example, SpotTune
[10] automatically decides the optimal set of layers, and stochastic
depth [ 13] proposes a linearly decayed survival probability to drop a
subset of layers in residual blocks [11].
3Figure 3: Overall framework of PaCaNet. The red arrow in the transfer learning stage indicates that the discriminator 𝐷′
𝐵is regular-
ized with a penalty term 𝑃. Green arrows indicate that the images 𝐺𝐴(𝑎)and𝐺′
𝐴(𝑎)are respectively generated by generators 𝐺𝐴and
𝐺′
𝐴. Orange arrows denote that parameters of 𝐺′
𝐴and𝐺′
𝐵are partially frozen in our one-shot learning method during the transfer
learning stage.
3 METHOD
The proposed PaCaNet framework is shown in Fig. 3. 𝐴and𝐵
respectively represent two domains for Chinese landscape painting
and Chinese calligraphy. 𝐺𝐴and𝐺𝐵denote the generators for two
mappings𝐺𝐴:𝐴→𝐵and𝐺𝐵:𝐵→𝐴. In general, PaCaNet consists
of two stages. The first stage, called the pre-training stage, aims to
produce a model that processes landscape paintings and calligraphy.
Before training, landscape images will be cropped and resized, as
the sizes of most Chinese painting images are large and vary widely,
which causes an overwhelming computational cost. Meanwhile, the
calligraphy dataset will be resized and polarized. Further details of
polarization will be discussed in our experimental part (Section 4.1).
During the second stage (transfer learning stage), the user is
allowed to add a preferred pair of images (e.g., an arbitrary Chinese
animal painting 𝑏′with any Chinese calligraphy character 𝑎) to
perform one-shot learning on the pre-trained model obtained from
the previous stage, which involves a new generator 𝐺′
𝐴for mapping
𝐴→𝐵′where𝐵′denotes the domain of Chinese animal paintings.
It is essential to keep in mind that during the transfer learning
phase, the discriminators 𝐷𝐴and𝐷𝐵will be updated to 𝐷′
𝐴and𝐷′
𝐵,
with all their parameters being modified accordingly. Meanwhile,
𝐺𝐴and𝐺𝐵will be partially frozen to become𝐺′
𝐴and𝐺′
𝐵. To en-
courage𝐺′
𝐴to learn more features from 𝑏′∈𝐵′, we proposed a
regularization method that adds a penalty term 𝑃to the mapping
𝐴→𝐵′. Once transfer learning has been completed, 𝐺′
𝐴can gener-
ate images that combine features of 𝐵′with arbitrary calligraphic,
producing more diversified outputs than those of original 𝐺𝐴.
In the following paragraphs, the use of transfer learning and
regularization will be discussed in detail.3.1 Transfer Learning for Learning Features of
Calligraphy
The original CycleGAN can generate fused images, but with lim-
ited creativity for different calligraphic input, as shown in Fig. 2.
Towards the goal of introducing more creativity (i.e., generating
images with distinctive artistic styles for different Chinese charac-
ters as input), we applied two transfer learning techniques to our
pre-trained CycleGAN model, which are: one-shot learning and
parameter freezing .
One-shot learning involves a pair of Chinese calligraphy character
𝑎and Chinese animal painting 𝑏′. Both as datasets for Chinese paint-
ings,𝐵mostly includes landscapes, while 𝐵′contains only animals.
Our pre-trained CycleGAN model has generalization abilities for
Chinese paintings. Adding one-shot learning will develop its ability
to integrate features of animal paintings. One-shot learning occurs
when𝐺′
𝐴learns how to map 𝑎to𝑏′, with other network architectures
of CycleGAN 𝐺′
𝐵,𝐷′
𝐴and𝐷′
𝐵iterating normally as the training
stage. The learning process only requires one pair of ( 𝑎,𝑏′) to fine-
tune the target task. In the end, we will obtain a generator 𝐺′
𝐴that is
capable of generating the fused image 𝐺′
𝐴(𝑎)with both the features
from Chinese landscape paintings 𝐵those from the animal image 𝑏′.
However, we discovered that calligraphic characteristics and land-
scape characteristics of 𝐵are lost with one-shot learning. Parameter
freezing is a solution we adopted in transfer learning stage to make
the model produce better calligraphy handwriting and retain the
features learned in the pre-training stage. One common way is to
freeze the parameters by layers [ 5]: freeze all the parameters of one
ResNet block of the generator network (details in Section 4.3). We
found a better way to freeze the parameters by randomly sampling
all network parameters with a freezing rate 𝑟∈[0,1], as illustrated
in Algorithm 1. A freezing rate of 𝑟=0.9means that 90% of all
parameters of the generator networks will not be trainable during the
one-shot learning stage. The fused images generated in this way are
4Algorithm 1 Parameter freezing algorithm of generators
Input : Generators 𝐺𝐴and𝐺𝐵.
Hyperparameter : Freezing rate 𝑟𝑓∈[0,1].
Output : Generators 𝐺′
𝐴and𝐺′
𝐵.
1:for𝑔in{𝐺′
𝐴,𝐺′
𝐵}do
2: for all parameters𝑝in𝑔.𝑝𝑎𝑟𝑎𝑚𝑒𝑡𝑒𝑟𝑠()do
3: Generate a random number 𝑟∈[0,1].
4: if𝑟<𝑟𝑓then
5: Freeze parameter 𝑝.
6: else
7: Make parameter 𝑝trainable.
8: end if
9: end for
10:end for
as good as those generated by freezing layers. It also offers users an
adjustable freezing rate 𝑟to control the level of features learned in
the pre-training stage and the character remembrance for the output
images.
3.2 Regularization for Learning Features of
Paintings
CycleGAN aims to learn mapping functions between 𝐴and𝐵given
unpaired training data. The original loss function of CycleGAN is
proposed as Eq. 1:
L(𝐺𝐴,𝐺𝐵,𝐷𝐵,𝐷𝐴)=LGAN(𝐺𝐴,𝐷𝐵,𝐴,𝐵)
+LGAN(𝐺𝐵,𝐷𝐴,𝐵,𝐴)
+𝜆Lcyc(𝐺𝐴,𝐺𝐵),(1)
whereLGAN stands for adversarial loss [ 9];Lcycrepresents cycle
consistency loss [ 32]; and𝜆is a coefficient that controls the relative
weighting of the above two objectives.
We expect the generated images to combine Chinese calligraphy
with traditional Chinese paintings. The transfer learning approaches
mentioned in Section 3.1 have already fulfilled the need to fuse cal-
ligraphy characters. However, according to our experiments, we dis-
covered that CycleGAN with transfer learning still underperformed
in learning high-level features from 𝐵′.
To ameliorate this issue, we introduced a penalty term 𝑃applied to
the original CycleGAN. Initially, we chose Root Mean Square Error
(RMSE) and Multi-scale Structural Similarity Index Measure (MS-
SSIM) [ 30] to calculate the pixel-level similarity between 𝐺′
𝐴(𝑎)
and𝑏′for an arbitrary image 𝑎∈𝐴and its paired image 𝑏′∈𝐵′
during the one-shot learning process. Empirically, we found that
using RMSE only actually reconstructed better 𝐵′-like images that
preserved most of the features and looked extremely similar to the
image𝑏we used during the transfer learning stage. Nevertheless,
what we desire is that the fused images exhibit as many features
from Chinese paintings as possible without completely losing the
handwriting of the calligraphy characters. Consequently, we only
adopted𝑀𝑆−𝑆𝑆𝐼𝑀 as a penalty term 𝑃that is used to define Eq. 2:
Lreg(𝐺′
𝐴,𝐵′)=−(𝑀𝑆−𝑆𝑆𝐼𝑀(𝐺′
𝐴(𝑎),𝑏′)+𝐶), (2)whereLregdenotes the loss produced by the penalty term; 𝐶denotes
a constant term that is empirically set to 1 to avoid a negative loss
value.
Here,Lregis solely related to 𝐷′
𝐵because we only focus on
improving the discriminator ability that distinguishes between the
fused image and its paired image from 𝐵′.Lregplays a critical role
in driving the model to learn more characteristics from 𝐵′. Thus, the
full objective during the transfer learning stage becomes:
L 𝐺′
𝐴,𝐺′
𝐵,𝐷′
𝐵,𝐷′
𝐴=LGAN 𝐺′
𝐴,𝐷′
𝐵,𝐴,𝐵′
+LGAN 𝐺′
𝐵,𝐷′
𝐴,𝐵′,𝐴
+𝜆1Lcyc 𝐺′
𝐴,𝐺′
𝐵
+𝜆2Lreg 𝐺′
𝐴,𝐵′,(3)
where𝜆1and𝜆2are two hyperparameters that control the relative
importance ofL𝑐𝑦𝑐andL𝑟𝑒𝑔, respectively. Our aim is to find out:
𝐺′
𝐴∗,𝐺′
𝐵∗=arg min
𝐺′
𝐴,𝐺′
𝐵max
𝐷′
𝐵,𝐷′
𝐴L(𝐺′
𝐴,𝐺′
𝐵,𝐷′
𝐵,𝐷′
𝐴). (4)
4 EXPERIMENTAL RESULTS
4.1 Datasets and Input Pre-processing
Our experiments strictly followed the workflow presented in Fig. 3.
We initially collected 40,000 landscape paintings of several Chinese
artists (e.g., Baishi Qi, Beihong Xu, and Binhong Huang). Further-
more, we selected 4,745 of them as domain 𝐵by manual evaluation.
We also downloaded 4,745 Chinese calligraphy images from [ 29] as
domain𝐴to make up our own unpaired dataset for the CycleGAN
model, called PaCa dataset. 80% data of the PaCa dataset is used to
pre-train the model with 200 epochs. In addition, we prepared 100
animal paintings as domain 𝐵′for transfer learning.
During the pre-processing phase, we resized every image to 256×
256pixels by pixels. Moreover, we polarized our calligraphic input
so that every pixel intensity value turns into either 0 or 255 (in
grayscale). Intuitively, the handwriting should be pure black, and the
background should be entirely white. Empirically, we found that the
polarization removed all the noise around the handwriting (shown
in the left half of Fig. 4a and Fig. 4b). Meanwhile, we noticed that
the fused images without polarization showed a much worse effect
(Fig. 4a). In contrast, it can be seen in Fig. 4b that some features,
such as the calligraphic inscription and the ripple effect of the water,
are well preserved.
(a) An example of pre-training
result given unpolarized input.
(Left): Original calligraphy.
(Right): Fused image.
(b) An example of pre-training
result given polarized input.
(Left): Original calligraphy.
(Right): Fused image.
Figure 4: Pre-training results given unpolarized (left) and po-
larized (right) input.
54.2 Comparison Experiment on Naïve CycleGAN
and CycleGAN with One-shot Learning
In the following experiments, we set naïve CycleGAN as our base-
line model and explore how one-shot learning upon our pre-trained
model contributes to the creativity and diversity of CycleGAN. We
started our study with a Chinese “horse” painting. The hyperparame-
ters are set as follows: the number of epochs with initial learning rate
(𝛼=0.0002 ) was 100 and the number of epochs with linearly de-
cayed learning rate was 100; the Adam optimizer [ 16] was adopted
for training. Convergence occurred around the 50𝑡ℎepoch of the
learning rate decay phase.
Figure 5: Synthesis results of simplified Chinese calligraphy
“horse” character ( 2𝑛𝑑image) with horse painting (leftmost)
generated by naïve CycleGAN ( 3𝑟𝑑image) and CycleGAN with
one-shot learning (rightmost). Here, “ 马” is the Chinese charac-
ter corresponding to “horse”.
We observe in Fig. 5 that the simplified Chinese calligraphy
character of “horse” fails to produce a decent result of style fusion.
Gird-like noise is produced in the fused image. In addition, we can
barely see the features learned from the original horse painting. In
this paper, we applied the one-shot learning method that uses only
one pair of training images (e.g., calligraphy character of “horse”
and traditional horse painting) to adjust the fusion details over the
images generated by our pre-trained model.
We extended our case study of the horse to three more animals,
i.e., cat (“ 猫”), rooster (“鸡”), and bird (“鸟”). We performed ex-
periments for both simplified and traditional Chinese calligraphy
characters. As can be observed in Fig. 6a, the horse’s mane, body,
and tail are portrayed vividly. When it comes to the cat, its yellow
eyes and black spots are captured. With well-preserved original
handwriting, the fusion of traditional calligraphy characters gains
a better effect than their simplified counterparts. For instance, the
traditional “rooster” character generates a cockscomb right above
the image, in which the cockscomb of the original rooster painting
resides. Nevertheless, the fused result of the simplified “rooster”
only exhibits some feather-like effect. Meanwhile, the traditional
“bird” character maintains the bird’s beak after the transfer learning
stage, while its simplified counterpart does not.
Obviously, one-shot learning based on our pre-trained model
yields drastically better fusion effects compared to the naïve Cy-
cleGAN. This is due to the fact that our baseline model adopts
normal weight initialization [ 8], whereas our proposed CycleGAN
model with the one-shot learning method continues training from a
pre-trained model on a training set of Chinese landscape paintings.
Nonetheless, the one-shot learning approach tends to overfit the
animal paintings, leading to the loss of calligraphic structures and
brushstrokes.4.3 CycleGAN with One-shot Learning and
Parameter Freezing
From the experimental results in Section 4.2, we note that Cycle-
GAN with one-shot learning can learn representations of the animal
task. Yet, it loses information learned in the pre-training stage: the
fused images show little pictorial elements of Chinese landscape
paintings. Moreover, the structure of the Chinese calligraphy char-
acter is weakened during the one-shot learning stage. To keep the
image features learned at both stages, we introduced a method that
performs one-shot learning with parameter freezing for the network.
Inspired by the previous study [ 13], we decided to freeze part of
the PaCaNet parameters and leave the rest trainable. The network
structure is as follows: the generators 𝐺𝐴and𝐺𝐵of our network con-
sisting of 2 downsampling layers, 9 ResNet blocks, and 2 upsampling
layers, sequentially [ 15]. We conducted two sets of experiments as
follows.
In the first set, we investigated the effect of image fusion by freez-
ing one ResNet block but on different indexes of layers. As shown in
Fig. 8, the results of the four groups look similar. We hypothesized
that as long as the total number of freezing parameters remains the
same, the results should look alike. Since one ResNet block takes up
about 10% of the total parameters in the entire generator network, we
introduced the approach that freezes 10% of the total parameters as
described in Section 3.1. Subsequently, we conducted an experiment
with the freezing rate 𝑟=0.1, which produced a similar outcome to
the freezing of one ResNet block, which validated our hypothesis.
We further carried out another set of experiments to investigate
the effect of changing the freezing rate 𝑟. As shown in Fig. 7, a trend
of the mixture of features of Chinese landscape paintings and horse
painting is displayed. With fewer epochs or more frozen parameters,
the model will remember more features from the pre-trained stage,
i.e., features of Chinese landscape paintings, preserving the structure
of the calligraphy character of “horse” at the same time. Thereby, on
one hand, the more towards the upper right corner, the more features
of Chinese landscape paintings can be found. On the other hand,
with more training epochs or fewer frozen parameters, the model
tends to learn more features of the animal paintings. Hence, the
farther away the bottom left corner, the more features of the animal
painting can be observed.
To summarize, the freezing rate allows us to adjust the level
of blending of characteristics from Chinese landscape and animal
paintings in the fused image.
4.4 CycleGAN with One-shot Learning and
Regularization
Parameter freezing approach discussed in Section 4.3 addressed
the issue of missing calligraphic features. However, it degrades the
performance of CycleGAN in learning features from 𝐵′. Hence,
we applied the regularization technique (penalty term) during the
transfer learning stage together with the one-shot learning introduced
in Section 4.2.
Empirically, we verified that the generated images became more
unlike the original handwritten character after regularization. Mean-
while, the fused images managed to learn more features from the
animal paintings, as suggested in Section 3.2. Referring to Fig. 6b,
the “horse”example presents an artificial hieroglyph, in which the
6(a) Synthesis results of simplified and traditional Chinese calligra-
phy characters generated by CycleGAN with one-shot learning ap-
proach. (Left): Animal paintings. (Middle): Simplified Chinese cal-
ligraphy character and corresponding fused image. (Right): Tradi-
tional Chinese calligraphy character and corresponding fused im-
age.
(b) Synthesis results of simplified and traditional Chinese calligra-
phy characters generated by CycleGAN with one-shot learning ap-
proach and regularization. (Left): Simplified Chinese calligraphy
character and corresponding fused image. (Right): Traditional Chi-
nese calligraphy character and corresponding fused image.
Figure 6: Synthesis results. (Left): CycleGAN with one-shot learning. (Right): CycleGAN with one-shot learning as well as regular-
ization. In our study, we investigated Chinese calligraphy character of animals and paired them with their corresponding animal
painting.
Figure 7: Generated images through testing on different epochs
(vertically) with different freezing rates (horizontally). The
number of epochs from top to bottom is 10, 20, 60, 120, 200.
Freezing rate from left to right is 0.1, 0.2, 0.5, 0.8, 0.9.
handwriting of the original character is further weakened. Besides,
the horse’s hoof and the lower side of its body take the positions of
vertical and horizontal stroke, respectively.
Using the set of traditional calligraphy characters as input still
outperforms its simplified counterpart. For example, the fused bird
Figure 8: Generated images by freezing different ResNet blocks.
From left to right, the figure shows results of freezing only the
3𝑟𝑑,5𝑡ℎ,7𝑡ℎand9𝑡ℎunit of 9 ResNet blocks in 𝐺′
𝐴and𝐺′
𝐵of
PaCaNet.
produced by the simplified “bird” character only maintains the bird’s
head and beak from the target image, whereas its traditional coun-
terpart preserves birds’ bodies well on top of that. We believe there
are two major aspects that contribute to the better synthesis effect
of traditional calligraphy characters. First, most traditional charac-
ters possess more complex structures, which provides more features
to learn. Second, traditional characters are visually closer to hiero-
glyphs, which caters to our aesthetic idea of fusing handwritten
strokes with Chinese paintings.
4.5 Ablation Study
In this section, we will provide a brief summary of the performance
of our previous experimental groups. In particular, we will focus
on quantitative evaluation and qualitative analysis to distinguish
7between four models: original CycleGAN (CycleGAN Naïve ), Cycle-
GAN with one-shot learning (CycleGAN OSL), CycleGAN with one-
shot learning combined with parameter freezing (CycleGAN OSL+PF ),
and CycleGAN with one-shot learning incorporating both parameter
freezing and regularization (CycleGAN OSL+PF+REG ).
We selected 100 unseen animal paintings and paired them with
their corresponding calligraphy image to perform one-shot learning
as before. The model was still trained on a 200-epoch basis to
produce 100 generators ( 𝐺′
𝐴) with distinctive styles. Afterwards,
1,000 unused calligraphy images were leveraged to run inferences
on these generators, which consequently produced 100 sets of test
results. We respectively computed the Fréchet Inception Distance
(FID) [ 12] and Fréchet Pre-train Distance (FPD) [ 6] scores of the
fused results regarding 𝐴and𝐵′. Through calculating the average
value of these two evaluation metrics over 100 sets of fused images,
the overall performance of the original CycleGAN and our proposed
CycleGAN variants can be determined.
Method 𝐹𝐼𝐷𝐴𝐹𝑃𝐷𝐴𝐹𝐼𝐷𝐵′𝐹𝑃𝐷𝐵′
CycleGAN Naïve 400.904 2.002 422.825 0.291
CycleGAN OSL 477.531 1.863 373.592 0.221
CycleGAN OSL+PF 458.155 1.775 465.507 1.812
CycleGAN OSL+PF+REG 460.299 1.794 369.597 0.280
Table 1: Overall evaluation using quantitative metrics for our
CycleGANs (with transfer learning).
We aim to generate fused images integrating the characteristics
of animals with the structure of calligraphy characters. This can
be verified from the quantitative results shown in Table. 1. Our
proposed model, PaCaNet (CycleGAN OSL+PF+REG ), performed best
in terms of𝐹𝐼𝐷𝐵′compared to other approaches. It also had similar
performance to CycleGAN Naïve and CycleGAN OSL in terms of
𝐹𝑃𝐷𝐵′, indicating that PaCaNet effectively learned the features from
𝐵′. CycleGAN Naïve yielded the best score in terms of 𝐹𝐼𝐷𝐴, since it
generated images that were almost identical to the characters in 𝐴
without learning any features from 𝐵′. When it comes to 𝐹𝐼𝐷𝐴and
𝐹𝑃𝐷𝐴, PaCaNet was neither the best nor the worst, which is in line
with our expectations as we did not want the generated images to
display calligraphy characters only. Overall, while CycleGAN OSL
performed similarly to PaCaNet on 𝐵′, PaCaNet behaved better
with respect to both 𝐹𝐼𝐷𝐴and𝐹𝑃𝐷𝐴, making it the top-performing
model.
Figure 9: Synthesis results of CycleGAN combined with
our proposed parameter freezing and regularization methods.
(Left): Horse. (Middle): Cat. (Right): Bird.As shown in Fig. 9, clearer calligraphic strokes of “cat” and “bird”
characters can be observed compared to the results we obtained
using CycleGAN OSL(Fig. 6a) and CycleGAN OSL+REG (Fig. 6b).
Meanwhile, the pictorial elements of 𝐵′are presented elegantly.
4.6 Human Evaluation
To complete the qualitative analysis of the outputs from PaCaNet.
We invited two artists to evaluate the value of the generated images.
Reviewer 1 : In this study, by combining animal imagery in Chi-
nese painting with its corresponding Chinese calligraphy, the output
results (Fig. 6) demonstrate that simple character structures can help
make the picture more compact and show a more stable composition
in the given space. Although complex structures may not exhibit
the advantages of composition well in the output, by rearranging
the distribution of blocks according to the character structure, if AI
can learn composition through Chinese calligraphy with a specific
purpose, it may be able to output a more stable and harmonious
visual structure more effectively in the future.
In addition, incorporating texture can achieve the expression of
subjective cultural intentions. Through learning the color blocks and
texture of Chinese painting, the output results show a similar atmo-
sphere, which retains the characteristics of Chinese paintings and
completes the cultural expression to some extent. Considering the
connotation of Chinese pictographic characters, their font structure
itself is a conversion of the form and meaning of external objects.
In other words, the character structure of Chinese characters has a
set of developed logic for decoding external objects. By repeatedly
comparing and learning this logic, it may be possible to develop
a new visual artistic structural innovation capability that can help
non-physical intentions (such as emotions, logic, etc.) to decode
with Chinese graphics. This implies that the character structure of
Chinese calligraphy is not only a means of conveying written lan-
guage but also a representation of external objects. The logic behind
the formation of Chinese characters can serve as a foundation for
developing new and innovative visual art structures that can facil-
itate the decoding and interpretation of non-physical intentions in
Chinese graphics.
Reviewer 2 : In my humble opinion, AI is, in the first place, a
tool for productivity. The trained AI model described in this paper is
capable of mass-calculating and mass-creating an abundance of ma-
terials considered the fusion of Chinese calligraphy and ink paintings
based on patterns. By that, we see how human control is inserted
into artistic production (under loose definition) that takes advantage
of existing historical and cultural assets to meet a certain extent of
expectations. It requires very little previous formal artistic training.
By examining the results closer, for instance, the horse (“ 马”) and
bird (“鸟”) cases in Fig. 6, we can notice how the image-production
logic reveals the characteristics of Chinese pictograms (e.g., char-
acters resemble the forms and gestures of the animals depicted). It
serves as an entrance to the further study of archeology on signs,
philology on writing, and linguistics through the artistic landscapes
provided by the paintings. From prehistoric signage to the revolution
of simplified Chinese characters, with the assistance of visual art
as the pictorial representation, I see the potential of utilizing these
models in the aforementioned areas of study to address the logocen-
trism that still prevails. I would also like to emphasize that, from the
8above case, AI-generated outcomes can be entry points but not the
final product; references but not answers; and guides but not canons.
5 CONCLUSIONS AND FUTURE WORK
In this study, we introduce PaCaNet, a cutting-edge transfer learning
model designed to generate diverse fusion of Chinese painting and
calligraphy. The generated output images present new aesthetic ef-
fects that have not been created in previous generative models. By
incorporating one-shot learning, random parameter freezing, and a
novel regularization technique, PaCaNet demonstrates superior per-
formance compared to other methods through extensive experiments.
Our results showcase the effectiveness of PaCaNet in producing
unique and high-quality fused images. Additionally, the artistic style
fusion will open a new avenue for exploring the traditional cultural
heritage. In the future, we will try to examine the capability of our
PaCaNet framework to combine both visual and textual inputs in the
image fusion process. Furthermore, we may adapt our approach to
other artistic domains, such as neoclassicism andimpressionism .
REFERENCES
[1]Karim Armanious, Chenming Jiang, Sherif Abdulatif, Thomas Kustner, Sergios
Gatidis, and Bin Yang. 2019. Unsupervised Medical Image Translation Using
Cycle-MedGAN. In 2019 27th European Signal Processing Conference (EU-
SIPCO) . IEEE. https://doi.org/10.23919/eusipco.2019.8902799
[2]Ömer Aydın and Enis Karaarslan. 2022. OpenAI ChatGPT Generated Literature
Review: Digital Twin in Healthcare. Available at SSRN 4308687 (2022).
[3]Gino Brunner, Yuyi Wang, Roger Wattenhofer, and Sumu Zhao. 2018. Symbolic
Music Genre Transfer with CycleGAN. https://doi.org/10.48550/ARXIV .1809.
07575
[4]Bo Chang, Qiong Zhang, Shenyi Pan, and Lili Meng. 2018. Generating Handwrit-
ten Chinese Characters Using CycleGAN. In 2018 IEEE Winter Conference on
Applications of Computer Vision (WACV) . IEEE. https://doi.org/10.1109/wacv.
2018.00028
[5]Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009.
ImageNet: A large-scale hierarchical image database. In 2009 IEEE Conference
on Computer Vision and Pattern Recognition . 248–255. https://doi.org/10.1109/
CVPR.2009.5206848
[6]Yifan Ding, Liqiang Wang, and Boqing Gong. 2021. Analyzing Deep Neural
Network’s Transferability via Fréchet Distance. In 2021 IEEE Winter Conference
on Applications of Computer Vision (WACV) . 3931–3940. https://doi.org/10.1109/
WACV48630.2021.00398
[7]Li Fei-Fei, Robert Fergus, and Pietro Perona. 2006. One-shot learning of object
categories. IEEE transactions on pattern analysis and machine intelligence 28, 4
(2006), 594–611.
[8]Xavier Glorot and Yoshua Bengio. 2010. Understanding the difficulty of training
deep feedforward neural networks. In Proceedings of the Thirteenth International
Conference on Artificial Intelligence and Statistics (Proceedings of Machine
Learning Research, Vol. 9) , Yee Whye Teh and Mike Titterington (Eds.). PMLR,
Chia Laguna Resort, Sardinia, Italy, 249–256. https://proceedings.mlr.press/v9/
glorot10a.html
[9]Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-
Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative
Adversarial Networks. https://doi.org/10.48550/ARXIV .1406.2661
[10] Yunhui Guo, Honghui Shi, Abhishek Kumar, Kristen Grauman, Tajana Rosing,
and Rogerio Feris. 2019. Spottune: transfer learning through adaptive fine-tuning.
InProceedings of the IEEE/CVF conference on computer vision and pattern
recognition . 4805–4814.
[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep Residual
Learning for Image Recognition. In 2016 IEEE Conference on Computer Vision
and Pattern Recognition (CVPR) . 770–778. https://doi.org/10.1109/CVPR.2016.
90
[12] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and
Sepp Hochreiter. 2017. GANs Trained by a Two Time-Scale Update Rule Con-
verge to a Local Nash Equilibrium. (2017). https://doi.org/10.48550/ARXIV .
1706.08500
[13] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. 2016.
Deep networks with stochastic depth. In Computer Vision–ECCV 2016: 14th
European Conference, Amsterdam, The Netherlands, October 11–14, 2016, Pro-
ceedings, Part IV 14 . Springer, 646–661.[14] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros. 2016. Image-to-
Image Translation with Conditional Adversarial Networks. https://doi.org/10.
48550/ARXIV .1611.07004
[15] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. 2016. Perceptual losses for real-
time style transfer and super-resolution. In European Conference on Computer
Vision .
[16] Diederik P. Kingma and Jimmy Ba. 2014. Adam: A Method for Stochastic
Optimization. https://doi.org/10.48550/ARXIV .1412.6980
[17] Brenden Lake, Ruslan Salakhutdinov, Jason Gross, and Joshua Tenenbaum. 2011.
One shot learning of simple visual concepts. In Proceedings of the annual meeting
of the cognitive science society , V ol. 33.
[18] Yuxian Meng, Wei Wu, Fei Wang, Xiaoya Li, Ping Nie, Fan Yin, Muyu Li,
Qinghong Han, Xiaofei Sun, and Jiwei Li. 2019. Glyce: Glyph-vectors for chinese
character representations. Advances in Neural Information Processing Systems 32
(2019).
[19] Pat Pataranutaporn, Valdemar Danry, Joanne Leong, Parinya Punpongsanon, Dan
Novy, Pattie Maes, and Misha Sra. 2021. AI-generated characters for supporting
personalized learning and well-being. Nature Machine Intelligence 3, 12 (2021),
1013–1022.
[20] Gao Pengcheng, Gu Gang, Wu Jiangqin, and Wei Baogang. 2017. Chinese calli-
graphic style representation for recognition. International Journal on Document
Analysis and Recognition (IJDAR) 20 (2017), 59–68.
[21] Ralph H. Pinder-Wilson, Ruth Barbour, and Robert Williams. 2023. Calligraphy.
https://www.britannica.com/art/calligraphy
[22] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
Sutskever. 2019. Language Models are Unsupervised Multitask Learners. OpenAI
(2019). https://cdn.openai.com/better-language-models/language_models_are_
unsupervised_multitask_learners.pdf
[23] Edmar Rezende, Guilherme Ruppert, Tiago Carvalho, Fabio Ramos, and Paulo
De Geus. 2017. Malicious software classification using transfer learning of resnet-
50 deep neural network. In 2017 16th IEEE International Conference on Machine
Learning and Applications (ICMLA) . IEEE, 1011–1014.
[24] Melissa Roemmele. 2016. Writing stories with help from recurrent neural net-
works. In Proceedings of the AAAI Conference on Artificial Intelligence , V ol. 30.
[25] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn
Ommer. 2021. High-Resolution Image Synthesis with Latent Diffusion Models.
arXiv:2112.10752 [cs.CV]
[26] Xinlei Shi, Junjie Zhai, Xudong Yang, Zehua Xie, and Chao Liu. 2015. Radical
embedding: Delving deeper to chinese radicals. In Proceedings of the 53rd Annual
Meeting of the Association for Computational Linguistics and the 7th International
Joint Conference on Natural Language Processing (Volume 2: Short Papers) . 594–
598.
[27] Sümeyra Terzio ˘glu, Kevser Nur Ço ˘galmı¸ s, and Ahmet Bulut. 2022. Ad creative
generation using reinforced generative adversarial network. Electronic Commerce
Research (2022), 1–17.
[28] Aarni Tuomi. 2023. AI-Generated Content, Creative Freelance Work and Hospi-
tality and Tourism Marketing. In Information and Communication Technologies
in Tourism 2023: Proceedings of the ENTER 2023 eTourism Conference, January
18-20, 2023 . Springer, 323–328.
[29] Yuanhao Wang. 2020. Chinese calligraphy styles by calligraphers.
https://www.kaggle.com/datasets/yuanhaowang486/chinese-calligraphy-
styles-by-calligraphers
[30] Z. Wang, E.P. Simoncelli, and A.C. Bovik. 2003. Multiscale structural similarity
for image quality assessment. In The Thrity-Seventh Asilomar Conference on
Signals, Systems & Computers, 2003 , V ol. 2. 1398–1402 V ol.2. https://doi.org/10.
1109/ACSSC.2003.1292216
[31] Huang Xing and Xu Feng. 2016. The romanization of Chinese language. (2016).
[32] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. 2017. Unpaired
Image-to-Image Translation using Cycle-Consistent Adversarial Networks. In
Computer Vision (ICCV), 2017 IEEE International Conference on .
9