ReSync: Riemannian Subgradient-based
Robust Rotation Synchronization
Huikang Liu
School of Information Management and Engineering
Shanghai University of Finance and Economics
liuhuikang@shufe.edu.cn
Xiao Li
School of Data Science
The Chinese University of Hong Kong, Shenzhen
lixiao@cuhk.edu.cn
Anthony Man-Cho So
Department of Systems Engineering and Engineering Management
The Chinese University of Hong Kong
manchoso@se.cuhk.edu.hk
Abstract
This work presents ReSync , a Riemannian subgradient-based algorithm for solving
the robust rotation synchronization problem, which arises in various engineering
applications. ReSync solves a least-unsquared minimization formulation over
the rotation group, which is nonsmooth and nonconvex, and aims at recovering
the underlying rotations directly. We provide strong theoretical guarantees for
ReSync under the random corruption setting. Specifically, we first show that the
initialization procedure of ReSync yields a proper initial point that lies in a local
region around the ground-truth rotations. We next establish the weak sharpness
property of the aforementioned formulation and then utilize this property to derive
the local linear convergence of ReSync to the ground-truth rotations. By combining
these guarantees, we conclude that ReSync converges linearly to the ground-truth
rotations under appropriate conditions. Experiment results demonstrate the effec-
tiveness of ReSync .
1 Introduction
Rotation synchronization ( RS) is a fundamental problem in many engineering applications. For
instance, RS(also known as “rotation averaging”) is an important subproblem of structure from
motion (SfM) and simultaneous localization and mapping (SLAM) in computer vision [ 20,22,17],
where the goal is to compute the absolute orientations of objects from relative rotations between pairs
of objects. RShas also been applied to sensor network localization [ 41,12], signal recovery from
phaseless observations [2], digital communications [36], and cryo-EM imaging [35, 33].
Practical measurements of relative rotations are often incomplete andcorrupted , leading to the
problem of robust rotation synchronization ( RRS) [28,21,22,39,9,31]. The goal of RRS is to
reconstruct a set of ground-truth rotations X⋆
1,···,X⋆
i,···,X⋆
n∈SO(d)from measurements of
37th Conference on Neural Information Processing Systems (NeurIPS 2023).arXiv:2305.15136v2  [math.OC]  6 Dec 2023relative rotations represented as
Yij=

X⋆
iX⋆⊤
j,(i, j)∈ A,
Oij, (i, j)∈ E \ A ,
0, (i, j)∈ Ec,with (i, j)∈

A, with ratio pq,
E \ A ,with ratio (1−p)q,
Ec, otherwise .(1)
Here, SO(d) :=
R∈Rd×d:R⊤R=I,det(R) = 1	
denotes the rotation group (also known
as the special orthogonal group), Erepresents the indices of all available observations, Adenotes
the indices of true observations, Ac:=E \ A is the indices of outliers ,Oij∈SO(d)is an outlying
observation, and the missing observations are set to be 0by convention; see, e.g., [ 23, section 2.1]. We
useq∈(0,1)to denote the observation ratio and p∈(0,1)to denote the ratio of true observations.
Related works. Due to the vast amount of research in this field, our overview will necessarily focus
on theoretical investigations of RS. In the case where no outliers exist in the measurement model (1),
i.e.,p= 1, a natural formulation is to minimize a smooth least-squares functionP
(j,j)∈E∥XiX⊤
j−
Yij∥2
FoverXi∈SO(d),1≤i≤n. Spectral relaxation and semidefinite relaxation (SDR) are
typical approaches for addressing this problem [ 34,3,6,5,4,30], where they provide strong recovery
guarantees. However, these results cannot be directly applied to the corrupted model (1)due to
the existence of outliers (i.e., p <1) and the sensitivity of the least-squares solution to outlying
observations.
Theoretical understanding of RRS is still rather limited. One typical setting for theoretical analysis
ofRRS is the random corruption model ( RCM ); see Section 2.2. The work [ 39] introduces a least-
unsquared formulation and applies the SDR method to tackle it. Under the RCM and in the full
observation case where q= 1, it is shown that the minimizer of the SDR reformulation exactly
recovers the underlying Gram matrix (hence the ground-truth rotations) under the conditions that
the true observation ratio p≥0.46forSO(2) (andp≥0.49forSO(3) ) and n→ ∞ . In [ 23], the
authors established the relationship between cycle-consistency and exact recovery and introduced a
message-passing algorithm. Their method is tailored to find the corruption level in the graph, rather
than recovering the ground-truth rotations directly. They provided linear convergence guarantees for
their algorithm once the ratios satisfy p8q2= Ω(log n/n)under the RCM . However, it is unclear how
this message-passing algorithm is related to other optimization procedures for solving the problem.
Let us mention that they also provided guarantees for other compact groups and corruption settings.
Following partly the framework established in [ 23], the work [ 32] presents an interesting nonconvex
quadratic programming formulation of RRS. It is shown that the global minimizer of the nonconvex
formulation recovers the true corruption level (still not the ground-true rotations directly) when
p2q2= Ω(log n/n)under the RCM . Unfortunately, the work does not provide a concrete algorithm
that provably finds a global minimizer of the nonconvex formulation. In [ 29], the authors introduced
and analyzed a depth descent algorithm for recovering the underlying rotation matrices. In the context
of the RCM , they showed asymptotic convergence of their algorithm to the underlying rotations
without providing a specific rate. The result is achieved under the conditions that the algorithm
is initialized near X⋆,q≥ O(logn/n), and p≥1−1/(d(d−1) + 2) . The latter requirement
translates to p≥3/4forSO(2) andp≥7/8forSO(3) . It is important to note, however, that the
primary focus of their research lies in the adversarial corruption setup rather than the RCM .
Main contributions. Towards tackling the RRS problem under the measurement model (1), we
consider the following least-unsquared formulation, which was introduced in [ 39] as the initial step
for applying the SDR method:
minimize
X∈Rnd×df(X) :=X
(i,j)∈E∥XiX⊤
j−Yij∥F
subject to Xi∈SO(d),1≤i≤n.(2)
Note that this problem is nonsmooth andnonconvex due to the unsquared Frobenius-norm loss and
the rotation group constraint, respectively. We design a Riemannian Subgradient s ynchronization
algorithm ( ReSync ) for addressing problem (2); see Algorithm 1. ReSync will first call an initializa-
tion procedure named SpectrIn (see Algorithm 2), which is a spectral relaxation method. Then, it
implements an iterative Riemannian subgradient procedure. ReSync targets at directly recovering the
ground-truth rotations X⋆∈SO(d)nrather than the Gram matrix or the corruption level. Under the
RCM (see Section 2.2), we provide the following strong theoretical guarantees for ReSync :
2(S.1) Initialization . The first step of ReSync is to call SpectrIn for computing the initial point X0.
Theoretically, we establish that X0can be relatively close to X⋆depending on pandq; see
Theorem 2.
(S.2) Weak sharpness . We then establish a problem-intrinsic property of the formulation (2)called
weak sharpness; see Theorem 3. This property characterizes the geometry of problem (2)
and is of independent interest.
(S.3) Convergence analysis . Finally, we derive the local linear rate of convergence for ReSync
based on the established weak sharpness property; see Theorem 4.
The main idea is that the weak sharpness property in (S.2) helps to show linear convergence of
ReSync toX⋆in (S.3). However, this result only holds locally . Thus, we need the initialization
guarantee in (S.1) to initialize our algorithm in this local region and then argue that it will not leave
this region once initialized. We refer to Sections 3.1 to 3.3 for more technical challenges and our
proof ideas. Combining the above theoretical results yields our overall guarantee: ReSync converges
linearly to the ground-truth rotations X⋆when p7q2= Ω(log n/n); see Theorem 1.
Notation. Our notation is mostly standard. We use Rnd×d∋X= (X1;. . .;Xn)∈SO(d)nto
represent the Cartesian product of all the variables Xi∈SO(d),1≤i≤n. The same applies to the
ground-truth rotations X⋆= (X⋆
1;···;X⋆
n). LetEi={j|(i, j)∈ E} ,Ai={j|(i, j)∈ A} , and
Ac
i=Ei\ Ai. We also define Aij=Ai∩ Ajfor simplicity. For a set S, we use |S|to denote its
cardinality. For any matrix X,Y∈Rnd×d, we define the following distance up to a global rotation:
dist (X,Y) =∥X−Y R⋆∥F,where R⋆= arg min
R∈SO(d)∥XR−Y∥2
F=PSO(d)(X⊤Y).
Besides, we introduce the following distances up to the global rotation R⋆defined above:
dist1(X,Y) =nX
i=1∥Xi−YiR⋆∥F,dist∞(X,Y) = max
1≤i≤n∥Xi−YiR⋆∥F.
2 Algorithm and Setup
2.1 ReSync: Algorithm Development
In this subsection, we present ReSync for tackling the nonsmooth nonconvex formulation (2); see
Algorithm 1. Our algorithm has two main parts, i.e., initialization and an iterative Riemannian
subgradient procedure.
Initialization. ReSync first calls a procedure SpectrIn (see Algorithm 2) for initialization.
SpectrIn is a spectral relaxation-based initialization technique. SpectrIn computes the first d
leading unit eigenvectors of the data matrix to form Φ∈Rnd×d. We multiply√nto those
eigenvectors to ensure that its norm matches that of SO(d)n. We also construct Ψ, which re-
verses the sign of the last column of Φso that the determinants of ΦandΨdiffer by a sign.
400 600 800 10001020304050
SpectrIn
Naive SpectrIn
Figure 1: The average under
100 simulations of the ini-
tial distance dist(X0,X⋆)
computed by Algorithm 2
versus naive spectral ini-
tialization (i.e., outputting
X0=eΦdirectly) with p=
0.2, q= 0.2andd= 3.Then, we compute the projection of ΦandΨonto SO(d)n. The
projection is computed in a block-wise manner, namely
eΦi=PSO(d)(Φi),1≤i≤n,
where Φi,eΦi∈Rd×dare the i-th block of ΦandeΦ, respectively.
The projection can be explicitly evaluated as
eΦi=(
PiQ⊤
i,ifdet(Φi)>0,
bPiQ⊤
i,otherwise ,1≤i≤n.
Here,Pi,Qi∈Rd×dare the left and right singular vectors of Φi(with
descending order of singular values), respectively, and bPiis obtained
by reversing the sign of the last column of Pi. The initial point X0
is chosen as eΦoreΨ, depending on which is closer to SO(d)n.
Let us mention that the computation of eΨand Steps 5 - 9 in SpectrIn
can practically improve the approximation error dist(X0,X⋆). We
demonstrate such a phenomenon in Figure 1, in which “Naive SpectrIn ”
refers to outputing X0=eΦdirectly in Algorithm 2.
3Riemannian subgradient update. ReSync then implements an iterative Riemannian subgradient
procedure after obtaining the initial point X0. The key is to compute the search direction (Riemannian
subgradient) e∇Rf(Xk
i)and the retraction RetrXk
i(·)ontoSO(d)for1≤i≤n. Towards providing
concrete formulas for the Riemannian subgradient update, let us impose the Euclidean inner product
⟨A,B⟩= trace( A⊤B)as the inherent Riemannian metric. Consequently, the tangent space to
SO(d)atR∈SO(d)is given by TR:={RS:S∈Rd×d,S+S⊤= 0}. The Riemannian
subgradient e∇Rf(Xi)can be computed as [40, Theorem 5.1]
e∇Rf(Xi) =PTXi(e∇f(Xi)),1≤i≤n, (3)
where the projection can be computed as PTXi(B) =Xi 
X⊤
iB−B⊤Xi
/2for any B∈Rd×d
ande∇f(Xi)is the Euclidean subgradient of fwith respect to the i-th block variable Xi. Let us
define fi,j(X) :=∥XiX⊤
j−Yij∥F. The Euclidean subdifferential ∂f(Xi)with respect to the
block variable Xiis given by
∂f(Xi) = 2X
j:(i,j)∈E∂fi,j(Xi),with ∂fi,j(Xi) =(Xi−YijXj
∥XiX⊤
j−Yij∥F,if∥XiX⊤
j−Yij∥F̸= 0,
V∈Rd×d,∥V∥F≤1,otherwise.
Algorithm 1 ReSync : Riemannian Subgradient
Synchronization
Require: Initialize X0=SpectrIn (Y)(Algo-
rithm 2), where Y∈Rnd×ndand its (i, j)-
th block is Yi,j∈Rd×d;
1:Set iteration count k= 0;
2:while stopping criterion not met do
3: Update the step size µk;
4: Riemannian subgradient update:
Xk+1
i= RetrXk
i
−µke∇Rf(Xk
i)
for1≤i≤n;
5: Update iteration count k=k+ 1;
6:end whileAlgorithm 2 SpectrIn : Spectral Initialization
1:Input: Y∈Rnd×nd;
2:Compute the dleading unit eigenvectors of
Y:{u1, . . . ,ud};
3:SetΦ=√n[u1,u2, . . . ,ud]∈Rnd×dand
Ψ=√n[u1,u2, . . . ,ud−1,−ud];
4:Compute eΦ=PSO(d)n(Φ)andeΨ=
PSO(d)n(Ψ);
5:if∥eΦ−Φ∥F≤ ∥eΨ−Ψ∥Fthen
6:X0=eΦ;
7:else
8:X0=eΨ;
9:end if
10:Output: Initial point X0.
Any element e∇f(Xi)∈∂f(Xi)is called a Euclidean subgradient. In ReSync , one can choose an
arbitrary subgradient e∇f(Xi)∈∂f(Xi)atXi.
Mimicking the gradient method to update along the search direction e∇Rf(Xi)provides a point
X+
i=Xi−µe∇Rf(Xi)on the tangent space TXiatXi, which may violate the manifold constraint
“X+
i∈SO(d)”. One common approach in Riemannian optimization is to employ a retraction
operator to address the feasibility issue. For SO(d), we can use a QR decomposition-based retraction
and implement the Riemannian subgradient step as
X+
i= Retr Xi
−µe∇Rf(Xi)
= Qr
Xi−µe∇Rf(Xi)
,1≤i≤n. (4)
Here, Qr(B)returns the Q-factor in the thin QR decomposition of B, while the diagonal entries of
the R-factor are restricted to be positive [7].
Finally, setting Xi=Xk
i,Xj=Xk
jfor all jsuch that (i, j)∈ E,µ=µkin(3)and(4)yields a
concrete implementation of Step 4 in ReSync and leads to SO(d)∋Xk+1
i=X+
ifor1≤i≤n.
This completes the description of one full iteration of ReSync . Note that the per-iteration complexity
of the Riemannian subgradient procedure is O(n2q), and Algorithm 2 has computational cost O(n3).
2.2 RCM Setup for Theoretical Analysis
We develop our theoretical analysis of ReSync by adopting the random corruption model ( RCM ).
TheRCM was previously used in many works to analyze the performance of various synchronization
algorithms; see, e.g., [ 39,19,23,32]. Specifically, we can represent our measurement model (1)on
a graph G(V,E), where Vis a set of nnodes representing {X⋆
1,···,X⋆
n}andEis a set of edges
4containing all the available measurements {Yi,j,(i, j)∈ E} . We assume that the graph Gfollows the
well-known Erd ¨os-R ´enyi model G(n, q), which implies that each edge (i, j)∈ Eis observed with
probability q, independently from every other edge. Each edge (i, j)∈ Eis a true observation (i.e.,
(i, j)∈ A) with probability pand an outlier (i.e., (i, j)∈ Ac) with probability 1−p. Furthermore,
the outliers {Oi,j}(i,j)∈Acare assumed to be independently and uniformly distributed on SO(d).
3 Main Results
In this section, we present our theoretical results for ReSync . Our main results are summarized in
the following theorem, which states that our proposed algorithm can converge at a linear rate to the
underlying rotations X⋆. Our standing assumption in this section is stated below.
All our theoretical results in this section are based on the RCM ; see Section 2.2.
Theorem 1 (overall) .Suppose that the ratios pandqsatisfy
p7q2= Ωlogn
n
.
With probability at least 1−O(1/n),ReSync withµk=µ0γk, where µ0= Θ( p2/n)andγ= 1−pq
16,
converges linearly to the ground-truth rotations X⋆(up to a global rotation), i.e.,
dist 
Xk,X⋆
≤ξ0γk,dist∞ 
Xk,X⋆
≤δ0γk,∀k≥0.
Here, ξ0= Θ(p
np5q)andδ0= Θ( p2).
The basic idea of the proof is to establish the problem-intrinsic property of weak sharpness and then
use it to derive a linear convergence result. However, the result only holds locally. Thus, we develop
a procedure to initialize the algorithm in this local region and argue that ReSync will not leave this
region afterwards. In the remaining parts of this section, we implement the above ideas and highlight
the challenges and approaches to overcoming them.
3.1 Analysis of SpectrIn with Leave-One-Out Technique
Theorem 2 (initialization) .LetX0be generated by SpectrIn (see Algorithm 2). Suppose that the
ratios pandqsatisfy
p2q= Ωlogn
n
.
Then, with probability at least 1− O(1/n), we have
dist(X0,X⋆) =O√logn
p√q
and dist∞(X0,X⋆) =O√logn
p√nq
. (5)
The works [ 34] and [ 11] show that exact reconstruction of X⋆is information-theoretically possible
if the condition p2q= Ω (log n/n)holds for the cases d= 2 andd= 3, respectively. Though
Theorem 2 does not provide exact recovery, it achieves an optimal sample complexity for recon-
structing an approximate solution in the infinity norm. Specifically, Theorem 2 shows that, as long
asp2q≥Clogn/n for some constant C >0large enough, the ℓ∞-distance dist∞(X0,X⋆)(i.e.,
max 1≤i≤ndist(Xi,X⋆
i)) can be made relatively small. However, the ℓ2-distance dist(X0,X⋆)is
of the order Ω(√n)under such a sample complexity.
The work [ 26] considers orthogonal and permutation group synchronization and shows that spectral
relaxation-based methods achieve near-optimal performance bounds. Our result differs from that of
[26] in twofold: 1) Our approach follows the standard leave-one-out analysis based on the standard
“Dist” (up to O(d)invariance) defined above Lemma 3 in the Appendix. Nonetheless, we have to
transfer the results to “ dist” due to the structure of SO(d)in Lemma 5, which is a nontrivial step due
to the specific structure of SO(d). 2) Our result can handle incomplete observations (i.e., q <1). In
the case of incomplete observations, the construction in (17) in the Appendix becomes more intricate;
it has the additional third column, rendering the analysis of our Lemma 2 more involved.
We prove Theorem 2 with some matrix concentration bounds and the leave-one-out technique. We
provide the proof sketch below and refer to Appendix A for the full derivations.
5Proof outline of Theorem 2 . According to (1)and the fact E(Oij) =0since outliers are assumed
to be independently and uniformly distributed on SO(d)in the RCM (see Appendix A), we know
thatE(Yij) =pqX⋆
iX⋆⊤
jfor all (i, j)∈[n]×[n]. This motivates us to introduce the noise matrix
Wij=Yij−pqX⋆
iX⋆⊤
j, i.e.,
Y=pqX⋆X⋆⊤+W. (6)
The condition p2q= Ω(log n/n)in Theorem 2 ensures that the expectation pqX⋆X⋆⊤will dominate
the noise matrix Win the decomposition (6).
We first discuss how to bound dist(X0,X⋆). Notice that X0andX⋆are the dleading eigenvectors
ofY(after projection onto SO(d)n) and pqX⋆X⋆⊤, respectively. We can then use the matrix
perturbation theory (see Lemma 3) to bound dist(X0,X⋆). Towards this end, we need to estimate the
operator norm ∥W∥2, which could be done by applying the standard matrix Bernstein concentration
inequality [ 38] since the blocks {Wij}are i.i.d. white noise with bounded operator norms and
variances; see Lemma 2.
We next turn to bound the initialization error in the infinity norm, i.e., dist∞(X0,X⋆). Let us
use(WX0)m∈Rd×dto denote the m-th block of WX0∈Rnd×dfor1≤m≤n. The main
technical challenge lies in deriving a sharp bound for the term max 1≤m≤n∥(WX0)m∥F, as it
involves two dependent random quantities, i.e., the noise matrix Wand the initial X0that is obtained
by projecting the first dleading eigenvectors of YontoSO(d)n. To overcome such a statistical
dependence, we utilize the leave-one-out technique. This technique was utilized in [ 42] to analyze
the phase synchronization problem and was later applied to many other synchronization problems
[1, 10, 15, 18, 26]. Let us define
Y(m)=pqX⋆X⋆⊤+W(m)with W(m)
kl=Wkl·1{k̸=m}·1{l̸=m}. (7)
That is, we construct W(m)∈Rnd×ndby setting the m-th block-wise row and column of Wto
be0. Then, it is easy to see that Y(m)is statistically independent of W⊤
m∈Rd×nd, where the
latter denotes the m-th block-wise row of W. LetX(m)be the dleading eigenvectors of Y(m).
Consequently, X(m)is also independent of W⊤
m. Based on the above discussions, we can bound
each∥(WX0)m∥Fin the following way:
∥(WX0)m∥F=∥W⊤
mX0∥F≤ ∥W⊤
mX(m)∥F+∥W⊤
m(X0−X(m))∥F. (8)
The first term ∥W⊤
mX(m)∥Fcan be bounded using an appropriate concentration inequality due to
the statistical independence between W⊤
mandX(m). The second term can be bounded as
∥W⊤
m(X0−X(m))∥F≤ ∥Wm∥2· ∥X0−X(m)∥F,
in which ∥Wm∥2can be further bounded by matrix concentration inequality (see Lemma 2) and
∥X0−X(m)∥Fcan be bounded using standard matrix perturbation theory (see Lemma 4).
3.2 Weak Sharpness and Exact Recovery
We next present a property that is intrinsic to problem (2) in the following theorem.
Theorem 3 (weak sharpness) .Suppose that the ratios pandqsatisfy
p2q2= Ωlogn
n
.
Then, with probability at least 1− O(1/n), for any X∈SO(d)nsatisfying dist∞(X,X⋆) =O(p),
we have
f(X)−f(X⋆)≥npq
8dist1(X,X⋆).
Some remarks on Theorem 3 are in order. This theorem shows that problem (2)possesses the weak
sharpness property [ 8], which is intrinsic to the problem and independent of the algorithm used to
solve it. It is known that with this property, various subgradient-type methods can achieve linear
convergence [ 14,25]. We will establish a similar linear convergence result for ReSync in the next
subsection based on Theorem 3.
The weak sharpness property shown in Theorem 3 is of independent interest, as it could be helpful
when analyzing other optimization algorithms (not just ReSync ) for solving problem (2). Currently,
6only a few applications are known to produce sharp optimization problems, such as robust low-rank
matrix recovery [ 25], robust phase retrieval [ 16], and robust subspace recovery [ 24]. Furthermore,
sharp instances of manifold optimization problems are especially scarce. Hence, Theorem 3 extends
the list of optimization problems that possess the weak sharpness property and contributes to the
growing literature on the geometry of structured nonsmooth nonconvex optimization problems.
It is worth noting that Theorem 3 also establishes the exact recovery property of the formulation
(2). Specifically, up to a global rotation, the ground-truth X⋆is guaranteed to be the unique global
minimizer of fover the region SO(d)n∩ {X: dist ∞(X,X⋆) =O(p)}. Consequently, recovering
the underlying X⋆reduces to finding the global minimizer of fover the aforementioned region. As
we will show in the next subsection, ReSync will converge linearly to the global minimizer X⋆when
initialized in this region. However, the initialization requirement is subject to the stronger condition
p4q= Ω (log n/n)on the ratios pandq, which is ensured by Theorem 2.
We list our main ideas for proving Theorem 3 below. The full proof can be found in Appendix B.
Proof outline of Theorem 3 . Note that the objective function fcan be decomposed into two parts:
f(X) =g(X)
X
(i,j)∈A∥X⊤
iXj−X⋆⊤
iX⋆
j∥F+h(X)
X
(i,j)∈Ac∥X⊤
iXj−Oij∥F. (9)
It is easy to see that g(X⋆) = 0 andg(X)≥0. Based on the fact that the true observation is
uniformly distributed in all the indices, we have E(g(X)) =pqP
1≤i,j≤n∥X⊤
iXj−X⋆⊤
iX⋆
j∥F≥
npq
2dist1(X,X⋆); see Appendix B.2 for the last inequality. A traditional way to lower bounding
g(X)usingE(g(X))for all X∈SO(d)is to apply concentration inequality and an epsilon-net
covering argument. Unfortunately, the sample complexity condition p2q2= Ω(log n/n)does not
lead to a high probability result in this way. Instead, our approach is to apply the concentration theory
on the cardinalities of index sets rather than on Xdirectly; see the following lemma.
Lemma 1 (concentration of cardinalities of index sets) .Given any ϵ= Ω√logn√npq
, with probability
at least 1− O(1/n), we have
(1−ϵ)nq≤ |E i| ≤(1 +ϵ)nq, (1−ϵ)npq≤ |A i| ≤(1 +ϵ)npq,
(1−ϵ)npq2≤ |E i∩ Aj| ≤(1 +ϵ)npq2, (1−ϵ)np2q2≤ |A ij| ≤(1 +ϵ)np2q2
for any 1≤i, j≤n. See Section 1 for the notation.
We then provide a sharp lower bound on g(X)based on Lemma 1.
Proposition 1. Under the conditions of Theorem 3, with probability at least 1− O(1/n), we have
g(X)≥3npq
16dist1(X,X⋆),∀X∈SO(d)n. (10)
Next, to lower bound h(X)−h(X⋆) =P
(i,j)∈Ac 
∥X⊤
iXj−Oij∥F− ∥X⋆⊤
iX⋆
j−Oij∥F
we
first bound
h(X)−h(X⋆)≥X
(i,j)∈Ac*
X⋆⊤
iX⋆
j−Oij
∥X⋆⊤
iX⋆
j−Oij∥F,X⊤
iXj−X⋆⊤
iX⋆
j+
,
where the inequality comes from the convexity of the norm function U7→ ∥U∥Fwhenever
X⋆⊤
iX⋆
j−Oij̸=0. Then, using the orthogonality of each block of X⋆, we further have
h(X)−h(X⋆)≥X
(i,j)∈Ac*
I−X⋆
iOijX⋆⊤
j
∥I−X⋆
iOijX⋆⊤
j∥F,X⋆
iX⊤
iXjX⋆⊤
j−I+
. (11)
Recall that since the outliers {Oi,j}(i,j)∈Acare independently and uniformly dis-
tributed on SO(d), so are {X⋆
iOijX⋆⊤
j}(i,j)∈Ac. This observation indicates that
I−X⋆
iOijX⋆⊤
j/∥I−X⋆
iOijX⋆⊤
j∥F	
(i,j)∈Acare i.i.d. random matrices. Hence, by in-
voking concentration results that utilize the randomness of the outliers {Oi,j}(i,j)∈Acand the
cardinalities (i, j)∈ Ac, we obtain the following result.
7Proposition 2. Under the conditions of Theorem 3, with probability at least 1− O(1/n), we have
h(X)−h(X⋆)≥ −npq
16dist1(X,X⋆) (12)
for all X∈SO(d)nsatisfying dist∞(X,X⋆) =O(p).
Combining Proposition 1 and Proposition 2 gives Theorem 3.
3.3 Convergence Analysis and Proof of Theorem 1
Let us now turn to utilize the weak sharpness property shown in Theorem 3 to establish the local
linear convergence of ReSync . As a quick corollary of Theorem 3, we have the following result.
Corollary 1. Under the conditions of Theorem 3, with probability at least 1− O(1/n), for any
X∈SO(d)nsatisfying dist∞(X,X⋆) =O(p), we haveD
e∇Rf(X),X⋆−XE
≤ −npq
16dist1(X,X⋆),∀e∇Rf(X)∈∂Rf(X). (13)
This condition indicates that any Riemannian subgradient e∇Rf(X)provides a descent direction
pointing towards X⋆. However, it only holds for X∈SO(d)nsatisfying dist∞(X,X⋆) =O(p).
Our key idea for establishing local convergence is to show that the Riemannian subgradient update in
ReSync is a contraction operator in both the Euclidean and infinity norm-induced distances using
Corollary 1, i.e., if Xklies in the local region, then Xk+1also lies in the region. This idea motivates
us to define two sequences of neighborhoods as follows:
Nk
F={X|dist(X,X⋆)≤ξk}andNk
∞={X|dist∞(X,X⋆)≤δk}. (14)
Here, ξk=ξ0γk, δk=δ0γk, where ξ0,δ0, and γ∈(0,1)will be specified later. Thus, these two
sequences of sets {Nk
F}and{Nk
∞}will linearly shrink to the ground-truth. It remains to show that if
Xk∈ Nk
F∩ Nk
∞, thenXk+1∈ Nk+1
F∩ Nk+1
∞, which is summarized in the following theorem.
Theorem 4 (convergence analysis) .Suppose that δ0=O(p2)andξ0=O(√npqδ 0). Setγ= 1−pq
16
andµk=δk/ninReSync . IfXk∈ Nk
F∩ Nk
∞for any k≥0, then with probability at least
1− O(1/n), we have
Xk+1∈ Nk+1
F∩ Nk+1
∞.
Proof outline of Theorem 4. The proof consisted of two parts. On the one hand, we need to show
thatXk+1∈ Nk+1
F, which can be achieved by applying Corollary 1. On the other hand, in order to
show that Xk+1∈ Nk+1
∞, we need a good estimate of each block of e∇Rf(X). See Appendix C.
Having developed the necessary tools, we are now ready to prove Theorem 1.
Proof of Theorem 1. Based on Theorem 2, we know that X0∈ N0
FTN0
∞ifξ0andδ0satisfy
ξ0=O√logn
p√q
and δ0=O√logn
p√nq
. (15)
According to Theorem 4, by choosing δ0= Θ( p2)andξ0= Θ(p
np5q), condition (15) holds when
p7q2= Ω (log n/n). This completes the proof of Theorem 1.
4 Experiments
In this section, we conduct experiments on ReSync for solving the RRS problem on both synthetic and
real data, providing empirical support for our theoretical findings. Our experiments are conducted on
a personal computer with a 2.90GHz 8-core CPU and 32GB memory. All our experiment results are
averaged over 20 independent trials. Our code is available at https://github.com/Huikang2019/
ReSync .
4.1 Synthetic Data
We consider the rotation group SO(3) in all our experiments. We generate X⋆
1, . . . ,X⋆
nby first
generating matrices of the same dimension with i.i.d. standard Gaussian entries and then projecting
80 100 200 30010-5100(a)n= 400
0 100 200 30010-5100 (b)n= 600
0 100 200 30010-5100 (c)n= 800
0 100 200 30010-5100 (d)n= 1000
Figure 2: Convergence of ReSync withp=q= (log n/n)1/3.
0.2 0.4 0.6 0.8 100.511.522.5
MPLS
CEMP_GCW
IRLS_L12
DESC
ReSync
LUD
(a)q= 0.2, σ= 0.0
0.2 0.4 0.6 0.8 100.511.522.5 (b)q= 0.2, σ= 1.0
0.2 0.4 0.6 0.8 100.511.522.5
q (c)p= 0.2, σ= 0.0
0.2 0.4 0.6 0.8 10.511.522.5
q (d)p= 0.2, σ= 1.0
Figure 3: Comparison with state-of-the-art synchronization algorithms.
each of them onto SO(3) . The underlying graph, outliers, and relative rotations in the measurement
model (1)are generated according to the RCM as described in Section 2.2. In our experiments,
we also consider the case where the true observations are contaminated by additive noise, namely,
{Yi,j}(i,j)∈Ain (1) is generated using the formula
Yi,j=PSO(3) 
X⋆⊤
iX⋆
j+σGi,j
for (i, j)∈ A, (16)
where Gi,jconsists of i.i.d. entries following the standard Gaussian distribution and σ≥0controls
the variance level of the noise.
Convergence verification of ReSync. We evaluate the convergence performance of ReSync with
the noise level σ= 0 in(16). We set p=q= (log n/n)1/3in the measurement model (1),
which satisfies p2q= log n/n. We use the initial step size µ0= 1/npq and the decaying factor
γ∈ {0.7,0.8,0.85,0.90,0.95,0.98}inReSync . We test the performance for various nselected from
{400,600,800,1000}. Figure 2 displays the experiment results. It can be observed that (i) ReSync
converges linearly to ground-truth rotations for a wide range of γand (ii) a smaller γoften leads to
faster convergence speed. These corroborate our theoretical findings. However, it is worth noting
that excessively small γvalues may result in an early stopping phenomenon (e.g., γ≤0.8when
n= 400 ). In addition, ReSync performs better with a larger n, as it allows for a smaller γ(e.g.,
γ= 0.7when n= 1000 ) and hence converges to the ground-truth rotations faster.
Comparison with the state-of-the-arts. We next compare ReSync with state-of-the-art syn-
chronization algorithms, including IRLS L12 [ 9], MPLS [ 31], CEMP GCW [ 23,32], DESC
[32], and LUD [ 39]. We obtain the implementation of the first four algorithms from https:
//github.com/ColeWyeth/DESC , while LUD’s implementation is obtained through private com-
munication with its authors. In our comparisons, we use their default parameter settings. For ReSync ,
we set the initial step size to µ0= 1/npq and the decaying factor to γ= 0.95, as suggested by the
previous experiment. We fix n= 200 and vary the true observation ratio p(or the observation ratio
q) while keeping q= 0.2(orp= 0.2) fixed. We display the experiment results for σ= 0andσ= 1
in Figures 3a and 3b, respectively, where pis selected from {0.2,0.3,0.4, . . . , 1}. When σ= 0,
ReSync achieves competitive performance compared to other robust synchronization algorithms.
When the additive noise level is σ= 1,ReSync outperforms other algorithms. In Figures 3c and 3d,
we present the results with varying qchosen from {0.2,0.3,0.4, . . . , 1}for noise-free ( σ= 0) and
noisy ( σ= 1) cases, respectively. In the noise-free case, DESC performs best when q <0.5, while
ReSync slightly outperforms others when q≥0.5. In the noisy case, it is clear that ReSync achieves
the best performance for a large range of q.
94.2 Real Data
We consider the global alignment problem of three-dimensional scans from the Lucy dataset, which
is a down-sampled version of the dataset containing 368 scans with a total number of 3.5 million
triangles. We refer to [ 39] for more details about the experiment setting. We apply three algorithms
LUD [ 39], DESC [ 32] and our ReSync on this dataset since they have the best performance on noisy
synthetic data. As Figure 4 shows, ReSync outperforms the other two methods.
Figure 4: Histogram of the unsquared residuals of LUD, DESC, and ReSync for the Lucy dataset.
5 Conclusion and Discussions on Limitations
In this work, we introduced ReSync , a Riemannian subgradient-based algorithm with spectral
initialization for solving RRS. We established strong theoretical results for ReSync under the
RCM . In particular, we first presented an initialization guarantee for SpectrIn , which is a procedure
embedded in ReSync for initialization. Then, we established a problem-intrinsic property called weak
sharpness for our nonsmooth nonconvex formulation, which is of independent interest. Based on
the established weak sharpness property, we derived linear convergence of ReSync to the underlying
rotations once it is initialized in a local region. Combining these theoretical results demonstrates that
ReSync converges linearly to the ground-truth rotations under the RCM .
Limitations. Our overall guarantee in Theorem 1 requires the sample complexity of p7q2=
Ω(log n/n), which does not match the currently best known lower bound p2q= Ω(log n/n)for
exact recovery [ 34,11]. We showed in Theorem 2 that approximate recovery with an optimal
sample complexity is possible. Moreover, we showed in Theorem 3 that exact recovery with
p2q2= Ω(log n/n)is possible if we have a global minimizer of the objective function of problem
(2) within a certain local region. However, due to the nonconvexity of problem (2), it is non-trivial to
obtain the said minimizer. We circumvented this difficulty by establishing the linear convergence of
ReSync to a desired minimizer in Theorem 4. Nevertheless, a strong requirement on initialization is
needed, which translates to the weaker final complexity result of p7q2= Ω(log n/n).
Although our theory allows for p→0asn→ ∞ , our argument relies heavily on the randomness of
the outliers {Oi,j}and the absence of additive noise. In practice, adversarial outliers that arbitrarily
corrupt a measurement and additive noise contamination are prevalent. It remains unknown how well
ReSync performs in such scenarios.
The above challenges are significant areas for future research and improvements.
10Acknowledgments and Disclosure of Funding
The authors thank Dr. Zengde Deng (Cainiao Network) and Dr. Shixiang Chen (University of
Science and Technology of China) for providing helpful advice. They also thank the reviewers for
their insightful comments, which have helped greatly to improve the quality and presentation of the
manuscript.
Huikang Liu is supported in part by the National Natural Science Foundation of China (NSFC) Grant
72192832. Xiao Li is supported in part by the National Natural Science Foundation of China (NSFC)
under grants 12201534 and 72150002, and in part by the Shenzhen Science and Technology Program
under grant RCBS20210609103708017. Anthony Man-Cho So is supported in part by the Hong
Kong Research Grants Council (RGC) General Research Fund (GRF) project CUHK 14205421.
References
[1]Emmanuel Abbe, Jianqing Fan, Kaizheng Wang, and Yiqiao Zhong. Entrywise eigenvector
analysis of random matrices with low expected rank. Annals of Statistics , 48(3):1452–1474,
2020.
[2]Boris Alexeev, Afonso S Bandeira, Matthew Fickus, and Dustin G Mixon. Phase retrieval with
polarization. SIAM Journal on Imaging Sciences , 7(1):35–66, 2014.
[3]Mica Arie-Nachimson, Shahar Z Kovalsky, Ira Kemelmacher-Shlizerman, Amit Singer, and
Ronen Basri. Global motion estimation from point matches. In 2012 2nd International
Conference on 3D Imaging, Modeling, Processing, Visualization & Transmission , pages 81–88.
IEEE, 2012.
[4]Afonso S Bandeira. Random Laplacian matrices and convex relaxations. Foundations of
Computational Mathematics , 18(2):345–379, 2018.
[5]Afonso S Bandeira, Nicolas Boumal, and Amit Singer. Tightness of the maximum likelihood
semidefinite relaxation for angular synchronization. Mathematical Programming , 163(1–2):145–
167, 2017.
[6]Afonso S Bandeira, Amit Singer, and Daniel A Spielman. A Cheeger inequality for the graph
connection Laplacian. SIAM Journal on Matrix Analysis and Applications , 34(4):1611–1630,
2013.
[7]N. Boumal, B. Mishra, P.-A. Absil, and R. Sepulchre. Manopt, a Matlab toolbox for optimization
on manifolds. Journal of Machine Learning Research , 15(1):1455–1459, 2014.
[8]James V Burke and Michael C Ferris. Weak sharp minima in mathematical programming. SIAM
Journal on Control and Optimization , 31(5):1340–1359, 1993.
[9]Avishek Chatterjee and Venu Madhav Govindu. Robust relative rotation averaging. IEEE
Transactions on Pattern Analysis and Machine Intelligence , 40(4):958–972, 2017.
[10] Yuxin Chen, Jianqing Fan, Cong Ma, and Kaizheng Wang. Spectral method and regularized
MLE are both optimal for top- Kranking. Annals of Statistics , 47(4):2204–2235, 2019.
[11] Yuxin Chen and Andrea J Goldsmith. Information recovery from pairwise measurements. In
2014 IEEE International Symposium on Information Theory , pages 2012–2016. IEEE, 2014.
[12] Mihai Cucuringu, Yaron Lipman, and Amit Singer. Sensor network localization by eigenvector
synchronization over the Euclidean group. ACM Transactions on Sensor Networks , 8(3):1–42,
2012.
[13] Chandler Davis and W. M. Kahan. The rotation of eigenvectors by a perturbation. III. SIAM
Journal on Numerical Analysis , 7(1):1–46, 1970.
[14] Damek Davis, Dmitriy Drusvyatskiy, Kellie J MacPhee, and Courtney Paquette. Subgradient
methods for sharp weakly convex functions. Journal of Optimization Theory and Applications ,
179(3):962–982, 2018.
[15] Shaofeng Deng, Shuyang Ling, and Thomas Strohmer. Strong consistency, graph Laplacians,
and the stochastic block model. Journal of Machine Learning Research , 22(1):5210–5253,
2021.
11[16] John C Duchi and Feng Ruan. Solving (most) of a set of quadratic equalities: Composite
optimization for robust phase retrieval. Information and Inference: A Journal of the IMA ,
8(3):471–529, 2019.
[17] Anders Eriksson, Carl Olsson, Fredrik Kahl, and Tat-Jun Chin. Rotation averaging with the
chordal distance: Global minimizers and strong duality. IEEE Transactions on Pattern Analysis
and Machine Intelligence , 43(1):256–268, 2019.
[18] Jianqing Fan, Weichen Wang, and Yiqiao Zhong. An l∞eigenvector perturbation bound
and its application to robust covariance estimation. Journal of Machine Learning Research ,
18(207):1–42, 2018.
[19] Tingran Gao and Zhizhen Zhao. Multi-frequency phase synchronization. In Proceedings of the
36th International Conference on Machine Learning , pages 2132–2141. PMLR, 2019.
[20] Venu Madhav Govindu. Lie-algebraic averaging for globally consistent motion estimation. In
CVPR 2004 , volume 1, pages I–I. IEEE, 2004.
[21] Richard Hartley, Khurrum Aftab, and Jochen Trumpf. L1 rotation averaging using the Weiszfeld
algorithm. In CVPR 2011 , pages 3041–3048. IEEE, 2011.
[22] Richard Hartley, Jochen Trumpf, Yuchao Dai, and Hongdong Li. Rotation averaging. Interna-
tional Journal of Computer Vision , 103(3):267–305, 2013.
[23] Gilad Lerman and Yunpeng Shi. Robust group synchronization via cycle-edge message passing.
Foundations of Computational Mathematics , 22(6):1665–1741, 2022.
[24] Xiao Li, Shixiang Chen, Zengde Deng, Qing Qu, Zhihui Zhu, and Anthony Man-Cho So.
Weakly convex optimization over Stiefel manifold using Riemannian subgradient-type methods.
SIAM Journal on Optimization , 31(3):1605–1634, 2021.
[25] Xiao Li, Zhihui Zhu, Anthony Man-Cho So, and Ren ´e Vidal. Nonconvex robust low-rank
matrix recovery. SIAM Journal on Optimization , 30(1):660–686, 2020.
[26] Shuyang Ling. Near-optimal performance bounds for orthogonal and permutation group
synchronization via spectral methods. Applied and Computational Harmonic Analysis , 60:20–
52, 2022.
[27] Huikang Liu, Man-Chung Yue, and Anthony Man-Cho So. A unified approach to synchroniza-
tion problems over subgroups of the orthogonal group. Applied and Computational Harmonic
Analysis , 66:320–372, 2023.
[28] Daniel Martinec and Tomas Pajdla. Robust rotation and translation estimation in multiview
reconstruction. In CVPR 2007 , pages 1–8. IEEE, 2007.
[29] Tyler Maunu and Gilad Lerman. Depth descent synchronization in SO (D).International
Journal of Computer Vision , 131(4):968–986, 2023.
[30] David M Rosen, Luca Carlone, Afonso S Bandeira, and John J Leonard. SE-Sync: A certifiably
correct algorithm for synchronization over the special euclidean group. International Journal of
Robotics Research , 38(2–3):95–125, 2019.
[31] Yunpeng Shi and Gilad Lerman. Message passing least squares framework and its application
to rotation synchronization. In Proceedings of the 37th International Conference on Machine
Learning , pages 8796–8806. PMLR, 2020.
[32] Yunpeng Shi, Cole M Wyeth, and Gilad Lerman. Robust group synchronization via quadratic
programming. In Proceedings of the 39th International Conference on Machine Learning , pages
20095–20105. PMLR, 2022.
[33] Yoel Shkolnisky and Amit Singer. Viewing direction estimation in cryo-EM using synchroniza-
tion. SIAM Journal on Imaging Sciences , 5(3):1088–1110, 2012.
[34] Amit Singer. Angular synchronization by eigenvectors and semidefinite programming. Applied
and Computational Harmonic Analysis , 30(1):20–36, 2011.
[35] Amit Singer, Zhizhen Zhao, Yoel Shkolnisky, and Ronny Hadani. Viewing angle classification
of cryo-electron microscopy images using eigenvectors. SIAM Journal on Imaging Sciences ,
4(2):723–759, 2011.
[36] Anthony Man-Cho So. Probabilistic analysis of the semidefinite relaxation detector in digital
communications. In Proceedings of the 21st annual ACM-SIAM Symposium on Discrete
Algorithms , pages 698–711. SIAM, 2010.
12[37] Gilbert W Stewart and Ji-guang Sun. Matrix Perturbation Theory . Academic Press, 1990.
[38] Joel A Tropp. An Introduction to Matrix Concentration Inequalities. Foundations and Trends ®
in Machine Learning , 8(1–2):1–230, 2015.
[39] Lanhui Wang and Amit Singer. Exact and stable recovery of rotations for robust synchronization.
Information and Inference: A Journal of the IMA , 2(2):145–193, 2013.
[40] Wei Hong Yang, Lei-Hong Zhang, and Ruyi Song. Optimality conditions for the nonlinear
programming problems on Riemannian manifolds. Pacific Journal of Optimization , 10(2):415–
434, 2014.
[41] Stella Yu. Angular embedding: A robust quadratic criterion. IEEE Transactions on Pattern
Analysis and Machine Intelligence , 34(1):158–173, 2011.
[42] Yiqiao Zhong and Nicolas Boumal. Near-optimal bounds for phase synchronization. SIAM
Journal on Optimization , 28(2):989–1016, 2018.
13Contents
1 Introduction 1
2 Algorithm and Setup 3
2.1 ReSync: Algorithm Development . . . . . . . . . . . . . . . . . . . . . . . . . . 3
2.2 RCM Setup for Theoretical Analysis . . . . . . . . . . . . . . . . . . . . . . . . . 4
3 Main Results 5
3.1 Analysis of SpectrIn with Leave-One-Out Technique . . . . . . . . . . . . . . . . 5
3.2 Weak Sharpness and Exact Recovery . . . . . . . . . . . . . . . . . . . . . . . . . 6
3.3 Convergence Analysis and Proof of Theorem 1 . . . . . . . . . . . . . . . . . . . 8
4 Experiments 8
4.1 Synthetic Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
4.2 Real Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
5 Conclusion and Discussions on Limitations 10
A Full Proof of Theorem 2 15
A.1 Initialization Error in Euclidean Norm . . . . . . . . . . . . . . . . . . . . . . . . 16
A.2 Initialization Error in Infinity Norm . . . . . . . . . . . . . . . . . . . . . . . . . 17
B Full Proof of Theorem 3 18
B.1 Proof of Lemma 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
B.2 Proof of Proposition 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
B.3 Proof of Proposition 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
C Full Proof of Theorem 4 21
C.1 Proof of Corollary 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
C.2 Proof of Contraction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
14A Full Proof of Theorem 2
In this section, we present the full proof of Theorem 2. We will use the notation defined in the proof
outline of Theorem 2 in Section 3.1.
Firstly, we know that
Wij=

(1−pq)X⋆
iX⋆⊤
j,with probablity pq,
Oij−pqX⋆
iX⋆⊤
j,with probablity (1−p)q,
−pqX⋆
iX⋆⊤
j, otherwise .(17)
SinceOijis assumed to be uniformly distributed on SO(d)in the RCM, given any matrix Q∈SO(d),
it is easy to see that OijQis also uniformly distributed on SO(d), so we have
E(Oij) =E(OijQ) =E(Oij)·Q,∀Q∈SO(d).
LetEkl∈SO(d), k̸=ldenote the diagonal matrix whose k-th and l-th diagonal entries are −1and
others are 1, then we have E(Oij) =E(Oij)·Ekl, which implies
dE(Oij) =E(Oij)·(E12+E23+···+Ed1) = (d−4)E(Oij).
Thus, we have E(Oij) =0. Then, it is easy to see that E(Wij) = 0 and
Var(Wij) = (1 −pq)2I+ (1−q)p2q2I+ (1−p)q(1 +p2q2)I=q(1−p2q)I. (18)
Based on the above calculations, we can derive the following Lemma, which is a direct result of the
matrix Bernstein inequality [ 38]. Similar results can also be found in [ 42, Lemma 9], [ 27, Proposition
3], and [26, Eq. (5.12)].
Lemma 2. With probability at least 1− O(1/n), the following holds for any m∈[n]:
∥W∥2=Op
nqlogn
,∥W(m)∥2=Op
nqlogn
,∥Wm∥2=Op
nqlogn
.
Proof. Note that W=P
i<jW(ij), where W(ij)∈Rnd×nddenotes the matrix with the (i, j)
and(j, i)-th block equal to WijandWjiand others equal to 0. So {W(ij)}are i.i.d. centered and
bounded random matrices. Besides, we have
E(WW⊤)
2=X
i<jE(W(ij)(W(ij))⊤)
2= 2(n−1)∥Var(Wij)∥2=O(nq).
According to the matrix Bernstein inequality [ 38], we have that ∥W∥2=O √nqlogn
holds with
probability at least 1−O(1/n2). The above argument also holds for each W(m), then taking a union
bound over the choice of m∈[n]yields the second result. For Wm, we have
∥W∥2= max
∥u∥2=1∥Wu∥2≥max
∥u∥2=1∥Wmu∥2=∥Wm∥2,
which gives the last result.
The following lemma follows from [ 13]; see also [ 26, Theorem A.2]. [ 42, Lemma 11] is a special
case where d= 1. Before that, we need to introduce the distance up to a global orthogonal matrix:
Dist (X,Y) =∥X−Y R⋆∥F,where R⋆= arg min
R∈O(d)∥XR−Y∥2
F=PO(d)(X⊤Y).
Lemma 3 (Davis-Kahan sin Θ Theorem) .Suppose that A,E∈ Cn×nare Hermitian matrices and
˜A=A+E. Letδ=λd(A)−λd+1(A)>0be the gap between the d-th eigenvalue and d+ 1-th
eigenvalue of Afor some 1≤d≤n−1. Furthermore, let U,˜Ube the d-leading eigenvectors of A
and˜A, respectively, which are normalized such that ∥U∥F=∥˜U∥F=√
nd. Then, we have
Dist(U,˜U)≤√
2∥EU∥F
δ− ∥E∥2. (19)
15A.1 Initialization Error in Euclidean Norm
Based on Lemma 2 and Lemma 3, we have the following bound on Dist. Note that the notations
Φ,Ψ,eΦ,eΨused in the following analysis are defined in SpectrIn (i.e., Algorithm 2).
Lemma 4. LetΦandΦ(m)be the d-leading eigenvectors of YandY(m), respectively, which are
normalized such that ∥Φ∥F=∥Φ(m)∥F=√
nd. Then, we have
Dist(Φ,X⋆) =O√logn
p√q
and Dist(Φ,Φ(m)) =O(1) (20)
hold with probability at least 1− O(1/n).
Proof. Let us Choose A=Y(m)andE= ∆W(m)=W−W(m)in Lemma 3, then ˜A=Y,
U=Φ(m)and˜U=Φ. Since Φ(m)is independent of ∆W(m), similar to Lemma 2, we apply the
matrix Bernstein inequality [38] to obtain, with probability at least 1− O(1/n2), that
∥EU∥F=∥∆W(m)Φ(m)∥F=O(p
nqlogn).
In addition, Y(m)=pqX⋆X⋆⊤+W(m)implies that
δ=λd(Y(m))−λd+1(Y(m))≥λd(pqX⋆X⋆⊤)− ∥W(m)∥2≥npq− O(p
nqlogn).
where the second inequality holds due to λd(X⋆X⋆⊤) = nand the last inequality is from
λd(X⋆X⋆⊤) = nand Lemma 2. Based on the condition that p2q= Ω
logn
n
, as long as
p2q≥Clogn
nfor some large enough constant C, we have
δ− ∥E∥2≥npq− O(p
nqlogn)− ∥E∥2≥npq− O(p
nqlogn)≥1
2npq,
where the second inequality holds because of ∥E∥2≤ ∥W∥2+∥W(m)∥2=O(√nqlogn). Hence,
by applying Lemma 3, we get
Dist(Φ,Φ(m))≤√
2∥EU∥F
δ− ∥E∥2≤O(√nqlogn)
npq=O(1) (21)
where the last inequality is because of p√q= Ω(p
logn/n). Similarly, by choosing A=
pqX⋆X⋆⊤andE=W, we can show that
Dist(Φ,X⋆)≤√
2∥WX⋆∥F
npq− O(√nqlogn)≤√
2∥W∥2∥X⋆∥F
1
2npq=O√logn
p√q
.
Here, the last inequality holds because ∥W∥2=O(√nqlogn)(see Lemma 2) and the fact that
∥X⋆∥F=√
nd.
Following the same analysis as in Lemma 4, we are also able to show that Dist(Ψ,X⋆) =
O√logn
p√q
, where Ψreverses the sign of the last column of Φso that the determinants of Φ
andΨdiffer by a sign. However, Lemma 4 only provides the upper bound on “ Dist”, i.e., the
distance up to an orthogonal matrix. The following lemma translates the result to that on “ dist”.
Lemma 5. Suppose that ∥eΦ−Φ∥F≤ ∥eΨ−Ψ∥F, where eΦ=PSO(d)n(Φ)andeΨ=PSO(d)n(Ψ),
then we have
dist(Φ,X⋆) = Dist( Φ,X⋆) =O√logn
p√q
.
Proof. Based on the structure of O(d)andSO(d), it is easy to see that
Dist(Φ,X⋆) = min {dist(Φ,X⋆),dist(Ψ,X⋆)}.
Our remaining task is to prove Dist(Φ,X⋆) = dist( Φ,X⋆)based on the condition that ∥eΦ−Φ∥F≤
∥eΨ−Ψ∥F. IfDist(Φ,X⋆) = dist( Ψ,X⋆), then we have
O√logn
p√q
= Dist( Φ,X⋆) = dist( Ψ,X⋆)≥ ∥eΨ−Ψ∥F≥ ∥eΦ−Φ∥F
16where the first inequality holds because eΨis the projection of ΨonSO(d)n. It is easy to see that
∥eΦ−Φ∥F+∥eΨ−Ψ∥F=∥Φ− P SO(d)n(Φ)∥F+∥Φ− P (O(d)\SO(d))n(Φ)∥F≥2√n.
The equality holds because the mapping that reverses the sign of the last column of a matrix is a
bijection between SO(d)andO(d)\SO(d), and the inequality holds since the minimum distance
between SO(d)andO(d)\SO(d)is 2. Under the condition that p2q= Ω
logn
n
, as long as
p2q≥Clogn
nfor some large enough constant C, we could have ∥eΦ−Φ∥F+∥eΨ−Ψ∥F=
O√logn
p√q
<2√n, which contradict to the above inequality. Thus, we have
dist(Φ,X⋆) = Dist( Φ,X⋆) =O√logn
p√q
.
A.2 Initialization Error in Infinity Norm
Next, based on the Davis-Kahan theorem, we can bound the distance between fX0andX⋆in the
infinity norm, which is stated in the following result.
Lemma 6. LetΦbe the d-leading eigenvectors of Y. Then, we have
dist∞(Φ,X⋆) =O√logn
p√nq
(22)
holds with probability at least 1− O(1/n).
Proof. LetΠm=argminΠ∈SO(d)∥Φ−Φ(m)Π∥F. We can compute
∥(WΦ)m∥F=∥W⊤
mΦ∥F≤ ∥W⊤
mΦ(m)Πm∥F+∥W⊤
m(Φ−Φ(m)Πm)∥F
≤ ∥W⊤
mΦ(m)∥F+∥Wm∥2∥Φ−Φ(m)Πm∥F.(23)
The fact that Φ(m)is independent from Wmimplies ∥W⊤
mΦ(m)∥F=O(√nqlogn), so we have
∥(WΦ)m∥F=O(p
nqlogn) (1 + O(1)) = O(p
nqlogn). (24)
Next, let Π⋆=argminΠ∈SO(d)∥Φ−X⋆Π∥F, then we have
(YΦ)m=pqX⋆
mX⋆⊤Φ+ (WΦ)m=npqX⋆
mΠ⋆+pqX⋆
mX⋆⊤(Φ−X⋆Π⋆) + (WΦ)m.
Since Φis the d-leading eigenvector of Y, we have YΦ=ΦΣ withΣ∈Rd×dconsisting of the
d-leading eigenvalues of Y. Applying the standard eigenvalue perturbation theory, e.g., [ 37, Theorem
4.11], we have
∥Σ−npqI∥2=O(∥W∥2) =O(p
nqlogn).
Based on the assumption that p2q= Ω
logn
n
, we can show that Σ≥3
4npqI. Note that YΦ=ΦΣ
implies Φm= (YΦ)mΣ−1for each m∈[n], then we have
∥Φm∥F≤4
3npq∥(YΦ)m∥F≤4
3npq∥pqX⋆
mX⋆⊤Φm∥F+4
3npq∥(WΦ)m∥F≤2√
d,
where the last inequality holds because4
3npq∥pqX⋆
mX⋆⊤Φm∥F=4
3n∥X⋆⊤Φm∥F≤4√
d
3and
(24). Therefore, for each m∈[n],
npq(Φm−X⋆
mΠ⋆) =Φm(npqI−Σ) +ΦmΣ−npqX⋆
mΠ⋆
=Φm(npqI−Σ) + (YΦ)m−npqX⋆
mΠ⋆.
This further implies
npq∥Φm−X⋆
mΠ⋆∥F≤∥Σ−npqI∥2∥Φm∥F+pq∥X⋆
mX⋆⊤(Φ−X⋆Π⋆)∥F+∥(WΦ)m∥F
≤2√
d∥W∥2+√npq∥Φ−X⋆Π⋆∥F+Op
nq(logn)
=O(p
nqlogn),
where the last inequality holds because of Lemma 2 and Lemma 4. This completes the proof.
17Finally, since X0=PSO(d)n(Φ), according to Lemma 2 in [27], we have
dist(X0,X⋆)≤2 dist( Φ,X⋆)and dist∞(X0,X⋆)≤2 dist ∞(Φ,X⋆),
which complete the proof of Theorem 2.
B Full Proof of Theorem 3
In this section, we provide the full proof of Lemma 1, Proposition 1, and Proposition 2, which finishes
the proof of Theorem 3. To simplify the notation and the theoretical derivations, we assume without
loss of generality that X⋆
i=Ifor all 1≤i≤nas one can separately rotate the space that each
variable Xilies in such that the corresponding ground-truth X⋆
iis rotated to identity [ 39, Lemma
4.1]. Consequently, we have Yij=Ifor(i, j)∈ A.
B.1 Proof of Lemma 1
For each 1≤i≤n,|Ei|=P
1≤j≤n1Ei(j), where 1Ei(·)denotes the indicator function w.r.t Ei.
Based on our model,P
1≤j≤n1Ei(j)follows the binomial distribution B(n, q). According to the
Bernstein inequality [38], for any constant ϵ∈(0,1), we have
Pr
X
1≤j≤n1Ei(j)−nq≥ϵnq
≤2 exp 
−1
2ϵ2n2q2
P
1≤j≤nE{12
Ei(j)}+ϵnq/3!
= 2 exp
−1
2ϵ2n2q2
nq+ϵnq/3
≤2 exp
−3
8ϵ2nq
.
The last inequality holds because of ϵ <1. Therefore,
Pr
∪
1≤i≤n{|Ei| −nq| ≥ϵnq}
≤2nexp
−3
8ϵ2nq
≤2
n2, (25)
where the last inequality holds because we assume that ϵ≥√8 logn√npq. Similarly, we have
Pr
∪
1≤i≤n{|Ai| −npq| ≥ϵnpq}
≤2nexp
−3
8ϵ2npq
≤2
n2, (26)
Pr
∪
1≤i,j≤nEi∩ Aj| −npq2≥ϵnpq2	
≤2n2exp
−3
8ϵ2npq2
≤2
n, (27)
and
Pr
∪
1≤i,j≤nAij| −np2q2≥ϵnp2q2	
≤2n2exp
−3
8ϵ2np2q2
≤2
n. (28)
Hence, we complete the proof of Lemma 1 once n≥4.
B.2 Proof of Proposition 1
We can first compute
X
1≤i,j≤n∥Xi−Xj∥F≤X
1≤i,j≤n1
|Aij|X
k∈Aij(∥Xi−Xk∥F+∥Xj−Xk∥F)
≤1
(1−ϵ)np2q2X
1≤i,j≤nX
k∈Aij(∥Xi−Xk∥F+∥Xj−Xk∥F).(29)
Here, the first inequality comes from the triangle inequality, while the second one follows from
Lemma 1. Now, invoking Lemma 1, which tells |Ak| ≤(1 +ϵ)npq, gives
X
1≤i,j≤nX
k∈Aij∥Xi−Xk∥F=X
1≤i≤nX
k∈AiX
j∈Ak∥Xi−Xk∥F
≤(1 +ϵ)npqX
1≤i≤nX
k∈Ai∥Xi−Xk∥F
18= (1 + ϵ)npqX
(i,k)∈A∥Xi−Xk∥F.
By symmetry, we conclude that
X
1≤i,j≤nX
k∈Aij(∥Xi−Xk∥F+∥Xj−Xk∥F)≤2(1 + ϵ)npqX
(i,j)∈A∥Xi−Xj∥F. (30)
Furthermore, we claim the following bound for any X∈SO(d)n:
X
1≤i,j≤n∥Xi−Xj∥F≥n
2dist1(X,X⋆). (31)
Combining (29), (30), (31), and the fact g(X) =P
(i,j)∈A∥Xi−Xj∥Festablishes Proposition 1.
Hence, it remains to show (31). First of all, according to the triangle inequality, we have
X
1≤i,j≤n∥Xi−Xj∥F≥X
1≤i≤nnXi−X
1≤j≤nXj
F=nX
1≤i≤n∥Xi−X∥F, (32)
where X=1
nP
1≤j≤nXjcan be taken as a diagonal matrix (since the Frobenius norm is invariant
up to a global rotation) with its first (d−1)diagonal entries being positive. Finally, applying the
following lemma to (32) provides (31).
Lemma 7. For any A∈SO(d)andB=Diag(b1, . . . , b d)satisfying b1, . . . , b d−1∈[0,1]and
bd∈[−1,1], we have
∥A−B∥F≥1
2∥A−I∥F.
Proof. It is equivalent to show that ∥A−B∥2
F≥1
4∥A−I∥2
F. Since A∈SO(d), we simplify as
d+⟨A,I−4B⟩+ 2∥B∥2
F=X
1≤i≤d1 +Aii(1−4bi) + 2b2
i≥0. (33)
To prove (33) holds for all A∈SO(d), we choose ¯A=argminA∈SO(d)⟨A,I−4B⟩=PSO(d)(4B−
I).It is easy to see that ¯Ais also a diagonal matrix. For any 1≤i≤d−1, we have
1 +Aii(1−4bi) + 2b2
i=
2(bi−1)2,Aii= 1;
2b2
i+ 4bi,Aii=−1.
So it is always nonnegative since bi≥0for any 1≤i≤d−1. For the last summation in (33),
on the one hand, if ¯Add= 1, then 1 +Add(1−4bd) + 2 b2
d= 2(bd−1)2≥0. On the other
hand, if ¯Add=−1, then there exist another 1≤k≤d−1such that ¯Akk=−1. So we have
(1 +Akk(1−4bk) + 2b2
k) + (1 + Add(1−4bd) + 2b2
d) = 2 b2
k+ 2b2
d+ 4(bk+bd)≥0.The last
inequality holds because |bd| ≤bk. We complete the proof.
B.3 Proof of Proposition 2
Based on (11) and our simplification that X⋆
i=I,1≤i≤n, we have
h(X)−h(X⋆)≥X
(i,j)∈AcI−Oij
∥I−Oij∥F,X⊤
iXj−I
. (34)
LetA=En
I−Oij
∥I−Oij∥Fo
,Zij=I−Oij
∥I−Oij∥F·1Ac(ij)−(1−p)qA, andZ∈Rnd×ndcollects each
Zijin its (i, j)-th block. We can computeX
(i,j)∈AcI−Oij
∥I−Oij∥F,X⊤
iXj−I=X
1≤i,j≤n
Zij+ (1−p)qA,X⊤
iXj−I
≤X
1≤i,j≤n
(1−p)qA,X⊤
iXj−I(35)
19+X
1≤i,j≤n
Zij,(Xi−I)⊤(Xj−I) + (Xi−I)⊤+ (Xj−I).
To bound the first term in (35), we can proceed asX
1≤i,j≤n
(1−p)qA,X⊤
iXj−I=*
(1−p)qA, nX
i=1Xi!⊤ nX
i=1Xi!
−n2I+
=*
(1−p)qA, nX
i=1Xi+nI!⊤ nX
i=1Xi−nI!+
≤(1−p)qA nX
i=1Xi+nI!
FnX
i=1Xi−nI
F
≤ ∥(1−p)qA∥FnX
i=1Xi+nI
2nX
i=1Xi−nI
F
≤2(1−p)qn∥A∥FnX
i=1Xi−nI
F.
(36)
Here, the second equality is true sincePn
i=1Xican be taken as a diagonal matrix. According to [ 39,
Lemma A.1], we know that ∥A∥F=En
I−Oij
∥I−Oij∥Fo
F≤1√
2for all d≥2. On the other hand,
we know that
dist(X,X⋆)2=nX
i=1∥Xi−I∥2
F= 2 trace 
nI−nX
i=1Xi!
≥2nX
i=1Xi−nI
F, (37)
where the last inequality holds because nI−Pn
i=1Xiis a nonnegative diagonal matrix. Therefore,
we obtainX
1≤i,j≤n
(1−p)qA,X⊤
iXj−I≤(1−p)qn√
2dist(X,X⋆)2
≤(1−p)qn√
2max
i∥Xi−I∥FnX
i=1∥Xi−I∥F.(38)
To further bound the second term in (35), we can proceed asX
1≤i,j≤n
Zij,(Xi−I)⊤(Xj−I) + (Xi−I)⊤+ (Xj−I)
≤(X−In)Z(X−In)⊤+ 2X
1≤i≤n*X
1≤j≤nZij,Xi−I+
≤∥Z∥op∥X−In∥2
F+ 2X
1≤i≤nX
1≤j≤nZij
F∥Xi−I∥F
≤
2√
d∥Z∥op+ 2 max
1≤i≤nX
1≤j≤nZij
F
X
1≤i≤n∥Xi−I∥F,(39)
where In∈Rnd×dcollects nidentity matrix together and the last inequality holds because ∥X−
In∥2
F=P
1≤i≤n∥Xi−I∥2
F≤P
1≤i≤n(∥Xi∥F+∥I∥F)∥Xi−I∥F= 2√
dP
1≤i≤n∥Xi−I∥F.
20Using the randomness of Oij, we claim that with probability at least 1−4d/n, we have
∥Z∥op≤p
8n(1−p)qlogn, and max
1≤i≤nX
1≤j≤nZij
F≤p
8n(1−p)qlogn. (40)
Combining the above bounds gives
h(X)−h(X⋆)
≥ −
2(√
d+ 1)p
8n(1−p)qlogn+(1−p)qn√
2max
i∥Xi−I∥FX
1≤i≤n∥Xi−I∥F
≥ −npq
16·dist1(X,X⋆),(41)
where the last inequality holds because we assume p2q2= Ω
logn
n
,max i∥Xi−I∥F=
dist∞(X,X⋆) =O(p), andP
1≤i≤n∥Xi−I∥F= dist 1(X,X⋆).
Finally, it remains to show that the two inequalities in (40) holds with probability at least 1−4d/n.
It is quick to verify that E(Zij) =0,∥Zij∥op≤1 + (1 −p)q∥A∥F≤2, and
E(Z2) =BlkDiag
X
jE(Z1jZ⊤
1j), . . . ,X
jE(ZnjZ⊤
nj)

=X
jE(Z1jZ⊤
1j)⊗In
=n(1−p)qE(I−Oij)(I−Oij)⊤
∥I−Oij∥2
F−(1−p)2q2AA⊤
⊗In,
where BlkDiag (·)means the block diagonal matrix, ⊗means the Kronecker product and Indenotes
then-by-nidentity matrix. Thus, we have ∥E(Z2)∥op≤n(1−p)q. According to the Matrix
Bernstein inequality [38], we have
Pr
∥Z∥op≤p
8n(1−p)qlogn
≥1−2ndexp 
−4n(1−p)qlogn
n(1−p)q+ 2p
8n(1−p)qlogn/3!
≥1−2d
n.
(42)
Here, the last inequality holds because we assume 2p
8n(1−p)qlogn/3≤n(1−p)q, which is
true as long as p= Ω(log n/n).
For any fixed 1≤i≤n, a similar argument based on Matrix Bernstein inequality [38] shows that
Pr
∥X
jZij∥F≤p
8n(1−p)qlogn
≥1−2dexp 
−4n(1−p)qlogn
n(1−p)q+p
8n(1−p)qlogn/3!
≥1−2d
n2,
(43)
which implies Pr
max i∥P
jZij∥F≤p
8n(1−p)qlogn
≥1−2d/n.
C Full Proof of Theorem 4
C.1 Proof of Corollary 1
Combining the convexity of fand Theorem 3, we have
−npq
8X
1≤i≤n∥Xi−X⋆
i∥F≥f(X⋆)−f(X)≥D
e∇f(X),X⋆−XE
,∀e∇f(X)∈∂f(X).
(44)
21for all X∈SO(d)nsatisfying dist∞(X,X⋆) =O(p). For any e∇⊥
Rf(X)∈∂⊥
Rf(X), we can
further computeD
e∇⊥
Rf(X),X−X⋆E
=X
1≤i≤nD
e∇⊥
Rf(Xi),PT⊥
Xi(Xi−X⋆
i)E
≤X
1≤i≤n∥e∇⊥
Rf(Xi)∥F· ∥PT⊥
Xi(Xi−X⋆
i)∥F.
On the one hand, notice that
PT⊥
Xi(Xi−X⋆
i) =Xi−X⋆
i− P TXi(Xi−X⋆
i)
=Xi 
X⊤
i(Xi−X⋆
i) + (Xi−X⋆
i)⊤Xi
/2
=Xi(Xi−X⋆
i)⊤(Xi−X⋆
i)/2,
which implies
∥PT⊥
X(Xi−X⋆
i)∥F=1
2∥Xi(Xi−X⋆
i)⊤(Xi−X⋆
i)∥F≤1
2∥Xi−X⋆
i∥2
F.
On the other hand, according to Lemma 1, with probability at least 1− O(1/n), for any i∈[n],
∥e∇⊥
Rf(Xi)∥F≤ ∥e∇f(Xi)∥F=X
j∈Eie∇fi,j(Xi)
F≤X
j∈Eie∇fi,j(Xi)
F≤ |E i| ≤2nq,
wheree∇fi,j(Xi)∈∂fi,j(Xi)satisfies ∥e∇fi,j(Xi)∥F≤1. Hence, we have
D
e∇⊥
Rf(X),X−X⋆E
≤nqX
1≤i≤n∥Xi−X⋆
i∥2
F≤npq
16X
i∥Xi−X⋆
i∥F
for any Xsuch that dist∞(X,X⋆)≤p
16. Invoking the above bounds into (44) yields the desired
resultD
e∇Rf(X),X−X⋆E
=D
e∇f(X)−e∇⊥
Rf(X),X−X⋆E
≥npq
16X
1≤i≤n∥Xi−X⋆
i∥F.
C.2 Proof of Contraction
Let us first present some preliminary results, which will be used in our later derivations. By noticing
thate∇f(Xi) =P
j∈Eie∇fi,j(Xi)wheree∇fi,j(Xi)∈∂fi,j(Xi), we define
e∇g(Xi) =X
j∈Aie∇fi,j(Xi),e∇h(Xi) =X
j∈Ei\Aie∇fi,j(Xi).
Recall that e∇Rg(Xi) =PTXi(e∇g(Xi))ande∇Rh(Xi) =PTXi(e∇h(Xi)). Similarly, we have
e∇f(Xi) =e∇g(Xi) +e∇h(Xi)ande∇Rf(Xi) =e∇Rg(Xi) +e∇Rh(Xi). Furthermore, the QR
decomposition-based retraction satisfies the second-order boundedness property, i.e., there exists
some M≥1such that
∥Xk+1
i−X⋆
i∥F=∥RetrXk
i
−µke∇Rf(Xk
i)
−X⋆
i∥F
≤ ∥Xk
i−µke∇Rf(Xk
i)−X⋆
i∥F+M·µ2
k∥e∇Rf(Xk
i)∥2
F.(45)
Recall that e∇Rf(Xk
i) =e∇Rg(Xk
i) +e∇Rh(Xk
i), we have
∥e∇Rf(Xk
i)∥F≤ ∥e∇Rg(Xk
i)∥F+∥e∇Rh(Xk
i)∥F
≤5p
2δ0nq+ (1 + ϵ)npq≤(1 + 2 ϵ)npq,(46)
where the second inequality comes from
∥e∇Rg(Xk
i)∥F=X
j∈Aie∇Rfi,j(Xi)
F≤X
j∈Aie∇Rfi,j(Xi)
F≤ |A i| ≤(1 +ϵ)npq,
22and Lemma 10 and the last inequality is due to the choice δ0≤ϵ2p2/50(i.e.,δ0=O(p2)). Thus, by
choosing µk=O(δk
n), and δ0=O(p2), the second-order term M·µ2
k∥e∇Rf(Xk
i)∥2
F=O(p6q2),
which is a very high-order error. In the following analysis, we will ignore this term to simplify our
derivations.
Using the above preliminaries and Corollary 1, we are ready to establish two key lemmas, which
show that if Xk∈ Nk
F∩ Nk
∞, thenXk+1∈ Nk+1
F (Lemma 8) and Xk+1∈ Nk+1
∞ (Lemma 9),
respectively. This completes the proof of Theorem 4.
Lemma 8. With high probability, suppose that Xk∈ Nk
F∩ Nk
∞,µk=O(δk
n), and
δ0=O(p2),and ξ0= Θ(√npqδ 0), (47)
thenXk+1∈ Nk+1
F.
Proof. By ignoring the high-order error term in (45), in order to bound ∥Xk+1−X⋆∥2
Fwe can first
compute
∥Xk+1−X⋆∥2
F=X
1≤i≤n∥Xk+1
i−X⋆
i∥2
F≤X
1≤i≤n∥Xk
i−µke∇Rf(Xk
i)−X⋆
i∥2
F
=∥Xk−X⋆∥2
F−2µkD
e∇Rf(Xk),Xk−X⋆E
+µ2
k∥e∇Rf(Xk)∥2
F.
Then, according to Corollary 1, we haveD
e∇Rf(Xk),Xk−X⋆E
≥npq
16X
1≤i≤n∥Xk
i−X⋆
i∥F≥npq
16δkX
1≤i≤n∥Xk
i−X⋆
i∥2
F
=npq
16δk∥Xk−X⋆∥2
F,
where the second inequality holds because ∥Xk
i−X⋆
i∥F≤δk(i.e.,Xk∈ Nk
∞). Combining the
above two inequalities gives
∥Xk+1−X⋆∥2
F≤
1−µk·npq
8δk
∥Xk−X⋆∥2
F+µ2
k∥e∇Rf(Xk)∥2
F
≤
1−pq
8
∥Xk−X⋆∥2
F+ (1 + 2 ϵ)2n3p2q2µ2
k,
where the last inequality is due to (46). Since µk=O(δk
n)andξ0= Θ(√npqδ 0)(i.e., ξk=
Θ(√npqδ k), we have n3p2q2µ2
k=O(pqξ2
k), which implies
∥Xk+1−X⋆∥2
F≤
1−pq
8
ξ2
k+pq
16ξ2
k=
1−pq
16
ξ2
k=ξ2
k+1.
This completes the proof.
Lemma 9. With high probability, suppose that Xk∈ Nk
F∩ Nk
∞,µk=O(δk
n), and
δ0=O(p2)and ξ0=O(√npqδ 0), (48)
thenXk+1∈ Nk+1
∞.
Proof. As stated at the beginning of Appendix B, we can assume without loss of generality that
R⋆=IandX⋆
i=Ifor all 1≤i≤n. We divide the index set [n]into three sets
I1=
i| ∥Xk
i−I∥F≤δk
4
,I2=
i|δk
4<∥Xk
i−I∥F≤3δk
4
,
andI3=
i|3δk
4<∥Xk
i−I∥F≤δk
.
For any i∈ I1SI2, we have
∥Xk
i−µke∇Rf(Xk
i)−I∥F≤ ∥Xk
i−I∥+µk∥e∇Rf(Xk
i)∥ ≤3δk
4+ 2µknpq≤δk+1,
(49)
where the last inequality holds because we choose µk=O(δk
n)≤δk
16n.
23It remains to consider the case i∈ I3. Firstly, it is easy to see that
dist(Xk,X⋆)2≥X
i∈I2SI3∥Xk−X⋆(δk)∥2
F≥δ2
k
16|I2[
I3|.
Note that we have |I2SI3| ≤dist(Xk,X⋆)2
δ2
k/16≤16ξ2
k
δ2
k=O(npq)according to the assumption
ξ0=O(√npqδ 0). Hence, for any i∈ I3, we have
e∇f(Xi) =X
j∈Ai∩I1Xi−Xj
∥Xi−Xj∥F+X
j∈Ai\I1Xi−Xj
∥Xi−Xj∥F+e∇h(Xi). (50)
Since|I2SI3|=O(npq), we can choose ξ0properly such that |I2SI3| ≤ϵnpq . Then, we have
|Ai\ I1| ≤ |I 2[
I3| ≤ϵnpq and |Ai∩ I1| ≥ |A i| − |A i\ I1| ≥(1−2ϵ)npq.
Let us use e∇g1(Xi)to denote the first term on the RHS of (50). We have
e∇g1(Xi) =X
j∈Ai∩I1Xi−Xj
∥Xi−Xj∥F=X
j∈Ai∩I1Xi−I
∥Xi−Xj∥F+I−Xj
∥Xi−Xj∥F
=σ(Xi−I) +X
j∈A∩I 1I−Xj
∥Xi−Xj∥F,(51)
where σ=P
j∈Ai∩I11
∥Xi−Xj∥F. For the last term in the above equation, we have
X
j∈Ai∩I1I−Xj
∥Xi−Xj∥F
F≤X
j∈Ai∩I1∥I−Xj∥F
∥Xi−Xj∥F≤δk
4X
j∈Ai∩I11
∥Xi−Xj∥F=δkσ
4.
In addition, the projection of Xi−Ionto the cotangent space can be bounded as
∥PT⊥
Xi(Xi−I)∥F=∥(Xi−I)∥2
F/2≤δ2
k/2,
which implies that e∇Rg1(Xi) =PTXi(e∇g1(Xi))satisfies
∥e∇Rg1(Xi)−σ(Xi−I)∥F≤δkσ
4+δ2
kσ
2≤δkσ
2.
Then, the fact e∇Rf(Xi) =e∇Rg1(Xi) +PT⊥
XiP
j∈Ai\I1Xi−Xj
∥Xi−Xj∥F
+e∇Rh(Xi)implies
∥e∇Rf(Xi)−σ(Xi−I)∥F≤ ∥e∇Rg1(Xi)−σ(Xi−I)∥F+|Ai\ I1|+e∇Rh(Xi)
F
≤δkσ
2+ϵnpq + 5p
2δ0nq.
Next, motivated by the update of ReSync , we can construct
Xk
i−µke∇Rf(Xk
i)−I= (1−µkσ)(Xk
i−I) +µk(e∇Rf(Xi)−σ(Xi−I)),
which implies
∥Xk
i−µke∇Rf(Xk
i)−I∥F≤(1−µkσ)∥Xk
i−I∥+µkδkσ
4+δ2
kσ
2+ϵnpq + 5p
2δ0nq
≤(1−µkσ)δk+µkδkσ
2+ϵnpq + 5p
2δ0nq
=δk−µkδkσ
2−ϵnpq−5p
2δ0nq
.
(52)
In order to further upper bound the above inequality, we can compute
σ=X
j∈Ai∩I11
∥Xi−Xj∥F≥X
j∈Ai∩I11
∥Xi−I∥F+∥Xj−I∥F
≥4
5δk|Ai∩ I1|=4(1−2ϵ)npq
5δk.
24where the second inequality holds because ∥Xi−I∥F+∥Xj−I∥F≤5δk/4asj∈ I1. It implies
δkσ
2−ϵnpq−5p
2δ0nq≥2(1−2ϵ)npq
5−ϵnpq−5p
2δ0nq≥npq
4.
By plugging the above bound into (52) and ignoring the high-order error term in (45), we complete
the proof.
Finally, we present Lemma 10 and its proof.
Lemma 10. With high probability, the following holds for all X0∈ N0
∞:e∇Rh(Xi)
F≤5p
2δ0nq∀i∈[n]. (53)
Proof of Lemma 10. Firstly, define ˜Oij=OijXjX⊤
i, then
e∇h(Xi) =X
j∈Ei/AiXi−OijXj
∥Xi−OijXj∥F=X
j∈Ei/AiI−OijXjX⊤
i
∥Xi−OijXj∥FXi=X
j∈Ei/AiI−˜Oij
∥I−˜Oij∥FXi
The fact that X∈ N0
∞implies that ∥Xi−Xj∥F≤2δ0, i.e.,∥I−XjX⊤
i∥F≤2δ0. For any
∥Oij−I∥ ≥√2δ0, we haveI−˜Oij
∥I−˜Oij∥F−I−Oij
∥I−Oij∥F
F≤I−˜Oij
∥I−˜Oij∥F−I−˜Oij
∥I−Oij∥F
F
+I−˜Oij
∥I−Oij∥F−I−Oij
∥I−Oij∥F
F≤2p
2δ0
where the last inequality holds becauseI−˜Oij
∥I−˜Oij∥F−I−˜Oij
∥I−Oij∥F
F=∥I−˜Oij∥F1
∥I−˜Oij∥F−1
∥I−Oij∥F
=1−∥I−˜Oij∥F
∥I−Oij∥F
=∥I−Oij∥F− ∥I−˜Oij∥F
∥I−Oij∥F≤∥Oij−˜Oij∥F
∥I−Oij∥F
=∥I−XjX⊤
i∥F
∥I−Oij∥F≤p
2δ0,
andI−˜Oij
∥I−Oij∥F−I−Oij
∥I−Oij∥F
F=∥Oij−˜Oij∥F
∥I−Oij∥F=∥I−XjX⊤
i∥F
∥I−Oij∥F≤p
2δ0.
LetΦi={j∈ Ei/Ai| ∥Oij−I∥ ≤√2δ0}. According to the fact that |Ei/Ai| ≤(1 +ϵ)nqand
the randomness of Oij, it is easy to show that |Φi| ≤√2δ0nqhold for all 1≤i≤nwith high
probability. Thus, by splitting the sumP
j∈Ei/Aiin to two parts: j∈Φiandj /∈Φi, we have
X
j∈Ei/AiI−˜Oij
∥I−˜Oij∥F−X
j∈Ei/ΩiI−Oij
∥I−Oij∥F
F≤4p
2δ0nq. (54)
Besides, since Oijis uniformly distributed on SO(d), according to Lemma A.1 in [ 39], we know
thatEn
I−Oij
∥I−Oij∥Fo
=c(d)I. Then, the matrix Bernstein’s inequality [ 38] tells us that, with high
probability,X
j∈Ei/ΩiI−Oij
∥I−Oij∥F− |E i/Ωi| ·c(d)I
F≤p
2δ0nq. (55)
25This, together with (54), implies thatX
j∈Ei/AiI−˜Oij
∥I−˜Oij∥F− |E i/Ωi| ·c(d)I
F≤5p
2δ0nq.
The fact that e∇h(Xi) =P
j∈Ei/AiI−˜Oij
∥I−˜Oij∥FXiimplies that
e∇h(Xi)− |E i/Ωi| ·c(d)Xi
F≤5p
2δ0nq. (56)
We complete the proof by taking the projection operator e∇Rh(Xi) =PTXi(e∇h(Xi))and the fact
thatPTXi(Xi) = 0 .
26