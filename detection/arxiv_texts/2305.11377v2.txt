GraphFC: Customs Fraud Detection with Label Scarcity
Karandeep Singh∗
Institute for Basic Science
Daejeon, Korea
ksingh@ibs.re.krYu-Che Tsai∗
National Taiwan University
Taipei, Taiwan
roytsai27@gmail.comCheng-Te Li
National Cheng Kung University
Tainan, Taiwan
chengte@ncku.edu.tw
Meeyoug Cha
Data Science Group, IBS & School of
Computing, KAIST
Daejeon, Korea
meeyoungcha@kaist.ac.krShou-De Lin
National Taiwan University
Taipei, Taiwan
sdlin@csie.ntu.edu.tw
ABSTRACT
Custom officials across the world encounter huge volumes of trans-
actions. With increased connectivity and globalization, the cus-
toms transactions continue to grow every year. Associated with
customs transactions is the customs fraud - the intentional manip-
ulation of goods declarations to avoid the taxes and duties. With
limited manpower, the custom offices can only undertake manual
inspection of a limited number of declarations. This necessitates
the need for automating the customs fraud detection by machine
learning (ML) techniques. Due the limited manual inspection for
labeling the new-incoming declarations, the ML approach should
have robust performance subject to the scarcity of labeled data.
However, current approaches for customs fraud detection are not
well suited and designed for this real-world setting. In this work,
we propose GraphFC (Graph neural networks for Customs Fraud),
a model-agnostic, domain-specific, semi-supervised graph neural
network based customs fraud detection algorithm that has strong
semi-supervised and inductive capabilities. With upto 252% relative
increase in recall over the present state-of-the-art, extensive exper-
imentation on real customs data from customs administrations of
three different countries demonstrate that GraphFC consistently
outperforms various baselines and the present state-of-art by a
large margin.
KEYWORDS
Graph Neural Network, Frauds detection, Multi-task learning
1 INTRODUCTION
Customs is an authority in a country responsible for collecting
tariffs and controlling the flow of goods into and out of country.
According to the World Trade Organization, the world merchandise
trade volume for exports and imports in 2019 alone was about 38
trillion dollars1. With globalization and increased connectivity, the
trade volumes continue to grow further.
Customs cover a wide range of trade facilitation issues aimed at
greater transparency, effectiveness, and efficiency of government
services and regulations with regard to the clearance of import,
∗Both authors contributed equally to this research.
1Source: World Integrated Trade Solution,
https://wits.worldbank.org/CountryProfile/en/WLD
Figure 1: Distribution of physical inspection ratio among
90 countries across the world according to [ 5]. Around 72%
countries have inspection rate less than 30%.
export and transit transactions across international borders. In addi-
tion to these aspects, customs also include facilitation measures re-
lating to cross-border trade and supply-chain security. International
trade and commerce via customs encounter malicious transaction
declarations that involve intentional manipulation of trade invoices
to avoid ad valorem taxes and duties [ 8,19,27]. Administrators
inspect trading goods and invoices to secure revenue generation
from trades and develop automated systems to detect fraudulent
transactions. This is an important undertaking as with customs
transactions running into millions, even a slight increase in the de-
tection accuracy will result in collection of hundreds of thousands
of dollars of additional revenue. However, limited manpower and
the colossal scale of trade volumes make the manual inspection of
all transactions practically infeasible. In fact, as per the World Bank
reports, administrations end up inspecting only a small portion,
typically 5% or lower, of the total transactions [ 5,14]. To select the
most suspicious transactions to be inspected, customs administra-
tors often leverage rule-based algorithms or expert-systems with
the domain know-how and empirical knowledge to determine thearXiv:2305.11377v2  [cs.LG]  19 Aug 2023GraphFC: Customs Fraud Detection with Label Scarcity, , Singh and Tsai, et al.
Figure 2: Performance w.r.t. manual inspection ratio (%).
illicitness of transactions [ 37]. Recently, the World Customs Organi-
zation (WCO)2and its partner countries have made a great deal of
progress in developing machine learning models, which can help of-
ficials identify and investigate any suspicious transactions that yield
maximum revenue. For instance, India and China Customs Services
applied decision tree and neural network based algorithms that out-
perform the traditional rule-based system [ 24,34]. Moreover, the
WCO had released DATE [ 20], a deep learning-based approach that
jointly considers the information of importers and their trade goods
and achieves state-of-the-art performance in detecting fraudulent
transactions.
Although there are various approaches proposed for customs
fraud detection, none of the existing work has addressed a critical
issue: low inspection ratios and thereof lack of labeled ground-truth
data for training the ML algorithms.
Figure 1 presents the distribution of physical inspection rate
of 90 different countries across the world. Intuitively, we notice
that around 40% of countries facilitate physical inspection less than
5% and up to 60% of countries has inspection rate less than 10%.
Therefore, with low inspection ratio comes with the label scarcity.
Scarcity of labeled data could severely hamper the performance of
existing ML models for customs fraud detection models. We notice
that most of the previous works and DATE [ 20,24,34], focus on
a supervised learning paradigm that requires an adequately large
amount of labeled data, which might become infeasible or less
effective in fraud detection when dealing with limited labeled data.
To understand the impact of performance of existing approaches
under different inspection ratios, we randomly sample n% of trans-
actions from a fully labeled dataset ( ∼2M labeled data) collected
from a partner country of WCO (country B) and train supervised
model learning models (e.g.,DATE and XGBoost). The results are
presented in Figure 2. We notice that supervised models achieve
reasonably stable performance when the inspection rate is greater
than 30%. However, in the case of inspection ratio lower than 30%,
large number of data points become unlabeled and the performance
drops significantly.
While the performance of a supervised ML model is limited
by the scarcity of labeled data, the potential of utilizing heaps of
unlabeled data in the customs domain has not been explored yet.
2http://www.wcoomd.org/Thus, in this work, we aim to mitigate the problem of label scarcity
for ML driven customs fraud detection approaches by adopting a
semi-supervised modeling approach.
To mitigate the difficulties arising from low inspection ratios, we
propose a graph neural network (GNN) based customs fraud detec-
tion model and tackle this problem from the graph perspective. The
setup can be viewed as a graph where the individual transactions
act as nodes and form the network, and a pre-defined connectivity
measure connects nodes, forming links in the network. For instance,
if transaction 𝑡𝑥𝑛1was made by importer 𝑖𝑚𝑝 1with the item coded
ℎ𝑠1, and importer id and item codes are defined to be the connec-
tivity measures, links can be formed between transaction nodes as
shown in Figure 3).
GNNs have been proven to have strong representational abilities
in the semi-supervised setting [ 16,22,39,46]. Moreover, self- and
unsupervised pretraining approaches in GNNs enable learning of
hidden relations and dynamics of interacting entities [ 6,21,40].
While learning the transactions representations, supervised ap-
proaches like XGBoost and DATE are not only incapable of such
an unsupervised pretraining, but also of accommodating the in-
formation from neighboring transactions. In customs setting, the
representations of transactions learnt by GNNs are enriched by
amalgamating information from the linked transactions as well as
the structure of the network.
Motivated by this, we propose Graph neural networks for Customs
Fraud ( GraphFC ), a GNN-based customs fraud detection model op-
erating on the structured customs data. The strength of GraphFC
lies in its two-stage design. In the first stage, we employ self-
supervised learning technique that jointly learns the representation
among labeled and unlabeled data. The self-supervision process
allows GraphFC understands the distribution of transactions and
its related transactions throughout the message passing operation
of GNN and the higher order connectivity in the transaction graph.
Afterwards, in the second stage, we fine-tuned GraphFC with la-
beled data under a dual-task learning scheme which predicts the
illicitness and the expected revenue simultaneously to make sure
the most suspicious and valuable transaction could be detected and
retrieved. The key finding of leveraging GNNs for fraud detection
is it not only learns the knowledge from unlabeled data through
self-supervised learning, but also capable of referring other associ-
ated transaction when making predictions. For instance, tradition
models predict the illicitness of 𝑡𝑥𝑛1solely based on the features
derived from 𝑡𝑥𝑛1, while GraphFC takes the features of 𝑡𝑥𝑛1and
its relevant transaction 𝑡𝑥𝑛2,𝑡𝑥𝑛 3into account. We found that such
design could substantially ease the performance drop with label
scarcity and offer major improvements over strong baselines like
XGBoost and the present state-of-the-art DATE model [ 20]. The
model code is made publicly available. We summarize our contri-
butions as follows:
•We develop Graph neural networks for Customs Fraud ( GraphFC )
and showcase its strength in real-world customs fraud detection.
•GraphFC adopts self-supervised and semi-supervised learning
techniques to extract rich information from unlabeled data, which
substantially alleviate the performance degradation due to label
scarcity.GraphFC: Customs Fraud Detection with Label Scarcity GraphFC: Customs Fraud Detection with Label Scarcity, ,
•GraphFC optimizes on the dual of maximizing the detection of
fraudulent transactions and maximizing the collection of associ-
ated revenue. Hence, it prioritizes fraudulent items that are likely
to yield maximum penalty surcharge.
•Extensive experimentation on multi-year customs data from
three countries shows that GraphFC outperforms the previous
state-of-the-art and various other baselines.
2 RELATED WORK
Most of the customs administrations are using legacy rule-based
systems [ 25]. Perhaps the biggest advantage of rule-based systems
is their simplicity and interpretability. But rule systems are hard to
maintain and are heavily dependent on expert knowledge [ 23,28].
In general, advances in data science have led to the development
of various fraud-detection models. A popular choice is tree-based
models [1–3, 44].
Due to the non-availability of customs data in the public do-
main, the published literature on customs fraud is understand-
ably scarce. However, there are some known efforts, like Belgium,
where customs have tested an ensemble method of a support vec-
tor machine-based learner in a confidence-rated boosting algo-
rithm [ 36]; Columbia has demonstrated the use of unsupervised
spectral clustering in detecting tax fraud with limited labels [ 11];
Indonesia customs have proposed an ensemble of tree-based ap-
proaches, support vector machine and neural network [ 7]. Similarly,
Netherlands’ customs fraud detection model is built based on the
Bayesian network and neural networks [ 35]. Another research em-
ployed a deep learning model to segregate high-risk and low-risk
consignment on randomly selected 200,000 data from Nepal Cus-
toms of the year 2017 [ 31]. Other countries have developed their
customs fraud detection systems such as Brazil [13].
GNNs are being used in fraud detection. For instance [ 48] ad-
dressed the homogeneity and heterogeneity of issue of networks
for fraudulent invitation detection, anomaly detection on dynamic
graphs with sudden bouts of increased and decreased activities is
proposed in [ 10], [41] proposed a semi-supervised approach and
an attention-based approach for fraud detection in Alipay. In [ 30],
authors designed a dynamic heterogeneous graph from the users’
registrations to prevent the suspicious massive account registra-
tions, [ 12] designs a GNN based fraud detection model to detect
the camouflaged fraudsters, [ 42] designs a graph convolutional
network for fraudster detection in online app review system, [ 29]
undertakes the task of fraud detection in credit cards by learning
temporal and location-based graph features.
Graph structure can also be defined as homogeneous, where
nodes have the same set of features, such as the original GCN (Graph
convolution network) [ 22], where all the nodes (club members) have
identical set features or are heterogeneous with nodes possessing
a different set of features [ 29,33]. While these approaches made
advances in the GNN methodologies and applications, their appli-
cation in customs domain is completely unexplored.
3 PROBLEM SETTING
3.1 Customs Selection
There are variety of customs fraud, but undervaluation is the most
common type. Importers or exporters declare the value of their
𝑙=1𝑡𝑥𝑛!𝑡𝑥𝑛"𝑡𝑥𝑛#𝑖𝑚𝑝!ℎ𝑠!𝑖𝑚𝑝"
ℎ𝑠"𝑡𝑥𝑛!𝑖𝑚𝑝!ℎ𝑠!𝑡𝑥𝑛"𝑡𝑥𝑛#𝑙=2𝑖𝑚𝑝!ℎ𝑠"
𝑖𝑚𝑝"ℎ𝑠!𝑙=3Labeled TransactionimporterItem codeUnlabeled TransactionFigure 3: Transaction graph construction in GraphFC
trade goods lower than the actual value, mainly to avoid ad valorem
customs duties and taxes [ 32]. For this work, we limit the definition
of an illicit customs transaction to that of undervaluation. Given
the volume of transactions and limited resources to inspect them,
the customs administration prioritizes items that entail a maximum
penalty. In this light, we formulate the problem of customs selection
as follows:
Problem: Given a transaction 𝑜𝑖with its importer ID 𝑖𝑚𝑝𝑖and HS-
code𝑐𝑖of the goods, the goal is to predict both the fraud score ˆ𝑦𝑐𝑙𝑠
𝑖
and the raised revenue ˆ𝑦𝑟𝑒𝑣
𝑖obtainable by inspecting transaction 𝑜𝑖.
By selecting two predicted values, ˆ𝑦𝑐𝑙𝑠
𝑖and ˆ𝑦𝑟𝑒𝑣
𝑖in descending
order from all transactions 𝑂={𝑜1,...,𝑜𝑛}, customs adminis-
tration could identify the most suspicious transactions 𝑂𝐹⊂𝑂
to be inspected. Meanwhile, since we focus on mitigating the low
inspection ratio problem in this work, we assume we have the
labeled set𝐿={(𝑥𝑖,𝑦𝑐𝑙𝑠
𝑖,𝑦𝑟𝑒𝑣
𝑖),∀𝑖=1,...,𝑛}and unlabeled set
𝑈={(𝑥𝑖),∀𝑖=𝑛+1,...,𝑚}, where|𝐿|≪|𝑈|.
Note that we also exhibit the model performance in the super-
vised setting and leave the results in Appendix, although a very
high inspection rate( >90%) is a rarity for customs administrations.
3.2 Transactions Graph from Tabular Data
One of the key insights of this work is to represent the customs
transactions and their interactions as a graph and use GNNs to learn
transaction-level embeddings by aggregating the neighborhood
information. Traditional fraud detection methods [ 13,20,36] judge
the illicitness of a transaction solely depending on its features 𝑥𝑖
while the information of similar transactions is not considered.
We set up links (edges) between transactions (nodes) to capture
any implicit patterns they might exhibit. These links could either
be based on common categorical features, binned numerical fea-
tures, or any other pre-defined rule. However, with even a relatively
smaller customs office running into millions of transactions, we im-
mediately run into a critical issue to exploding space complexity. A
homogeneous graph formed by above rule suffers from a complex-
ity of𝑂(𝑛2)due to the pairwise relational nature, where 𝑛denotes
the number of transactions. To handle this issue, we introduce a set
ofvirtual nodesVCthat are initially represented by the uniquely
identifiable values/features (such as the importer id). Each node in
VCwill only connect to transactions that share that feature value
with it, and thus bringing down the space complexity from 𝑂(𝑛2)
to𝑂(𝑛).GraphFC: Customs Fraud Detection with Label Scarcity, , Singh and Tsai, et al.
Import 
Transaction 𝑡𝑡𝑡𝑡𝑡𝑡1
Feature
Vector 𝐱𝐱
GBDTTree -based 
Cross -Feature Embedding
GBDT
GBDT
HS Code ℎ𝑠𝑠1
Importer 𝑖𝑖𝑖𝑖𝑖𝑖1Transaction RepresentationIllicitness 
Classification
Revenue
Prediction
Initialize: Zero VectorsDual -Task
Learning
�𝑦𝑦𝑐𝑐𝑐𝑐𝑐𝑐
�𝑦𝑦𝑟𝑟𝑟𝑟𝑟𝑟
Task -Specific 
Layersℎ𝑠𝑠1
ℎ𝑠𝑠2𝑖𝑖𝑖𝑖𝑖𝑖1
𝑖𝑖𝑖𝑖𝑖𝑖2𝑡𝑡𝑡𝑡𝑡𝑡1
𝑡𝑡𝑡𝑡𝑡𝑡2
𝑡𝑡𝑡𝑡𝑡𝑡3ℎ𝑠𝑠1𝑖𝑖𝑖𝑖𝑖𝑖1
𝑡𝑡𝑡𝑡𝑡𝑡1
𝑖𝑖𝑖𝑖𝑖𝑖1
ℎ𝑠𝑠1𝑡𝑡𝑡𝑡𝑡𝑡1
𝑡𝑡𝑡𝑡𝑡𝑡2
𝑡𝑡𝑡𝑡𝑡𝑡1
𝑡𝑡𝑡𝑡𝑡𝑡3Layer 1Layer 2Layer 3
Layer 1Layer 2Layer 3
Layer 1Layer 2Layer 3Self-Supervised
GNN Pre- TrainingSemi -Supervised
GNN Fine- Tuning
Transaction 
Graph
𝛼𝛼
𝛼𝛼
𝛼𝛼Neighbor Preservation & Negative Sampling𝐖𝐖1(𝑘𝑘)
𝐖𝐖2(𝑘𝑘)ℎ𝑠𝑠1𝑖𝑖𝑖𝑖𝑖𝑖1
𝑡𝑡𝑡𝑡𝑡𝑡1
𝑖𝑖𝑖𝑖𝑖𝑖1
ℎ𝑠𝑠1𝑡𝑡𝑡𝑡𝑡𝑡1
𝑡𝑡𝑡𝑡𝑡𝑡2
𝑡𝑡𝑡𝑡𝑡𝑡1
𝑡𝑡𝑡𝑡𝑡𝑡3Layer 1Layer 2Layer 3
Layer 1Layer 2Layer 3
Layer 1Layer 2Layer 3𝛼𝛼
𝛼𝛼
𝛼𝛼High -order Message Passing in the Graph
Layer -wise
Model Weights
Figure 4: Model architecture of GraphFC . Cross features extracted from GBDT step act as node features in the transaction
graph. In the pre-training stage, GraphFC learns the model weights and refine the transaction representations. Afterwards, the
model is fine-tuned with labeled data with dual-task learning framework to predict the illicitness and the additional revenue.
Based on previous works like DATE [ 20], we choose importer
ID and HS-code to be the categorical features defining the links
between transactions. Thus, in this work, VCis composed of the
importer ID and HS-code with the uniquely identifiable values
as their node IDs, as illustrated in the left half of Fig. 3. In line
with this, when a prediction is made for a transaction txn1, the
features of other transactions that share the same importer ID( txn2)
and HS-code( txn2) would be propagated to txn1to form a new
representation, as demonstrated in the right side of Fig. 3. The
detailed propagation rule will be described in Sec. 4. The transaction
nodesVTconsist of both labeled and unlabeled transactions, which
benefits the aggregation of additional information under a self- and
semi-supervised setting.
The transaction graph could be represented as G=(V,E)where
V=VT∪VCis the node-set that denotes the combined individual
transactions and the virtual nodes, and Edenotes the connections
between them based on the shared feature values. Let D∈Rℎ×𝑚
be the feature matrix that consists of h=|V|nodes with their
associated mfeatures. It is worth noting that in the testing phase,
only the historical transactions are considered for the inferring the
transaction embedding. Also, the cross features from GBDT step
become the features of nodes in VT, and the features of nodes in
VCare initialized with zero vectors ®0∈R𝑚.
4 METHODOLOGY
The proposed model GraphFC synergizes the strengths the GBDTs
and GNNs. Model architecture for GraphFC is illustrated in Fig. 4,
highlighting its multi-stage design.3
3Model code: https://github.com/k-s-b/gnn_wco. Due to its proprietary nature, the
raw data cannot be shared. Synthetic customs data for experimentation is provided.4.1 Cross Features from GBDT
Tree-based methodologies are a popular choice for building ma-
chine learning models on tabular data. Tabular data is known to
have hyperplane-like decision boundaries, and tree-based models
allow for hyperplane-based recursive partitioning of data space,
such that points in the partitioned space share the same class. In
this work, we follow the feature extraction method using GBDTs
as [20]. Gradient Boosting Decision Trees (GBDTs) methodology
can be understood as an ensemble technique where multiple weak
learner models are combined to provide a powerful overall learner.
The decision path in the fitted trees could be seen as a new set of
features, with each path representing a new cross feature made
from the original features. Assume 𝐶and𝐸𝑇respectively represent
internal nodes and their connecting paths in a decision tree 𝐷𝑇.
𝐶further can be viewed as consisting of three sets of nodes, the
root node{𝑐𝑅}, internal nodes 𝐶𝐼, and the leaf nodes 𝐶𝐸, where
𝐶={𝑐𝑅}∪𝐶𝐼∪𝐶𝐸. Nodes in𝑐𝐼∈𝐶𝐼splits features into some deci-
sion space. After passing through internal nodes, an input feature
vector x∈Xgets assigned to a leaf node 𝑙𝑒. Then, a decision rule
is the path connecting the root node to the leaf node. For instance,
“gross.weight >100kg ∧quantity < 5 ” is a two-node decision path
or a cross-feature comprising of features gross.weight andquantity .
Inspired by some recent studies such as [ 18,43], we utilize state-
of-the-art XGBoost model [ 9] and extract cross features from the
fitted model.
Let us assume 𝑊represents the total number of leaves in the
ensemble. An input vector xends up in one of the leaf nodes accord-
ing to the decision rules of each fitted decision tree. Each activated
leaf node in the 𝑡-th decision tree can be represented as a one-hot
encoding vectorF𝑡. We concatenate these together and produce a
multi-hot vector p∈R𝑊, where 1indicates the activated leaves
and0, the non-activated ones. After the extraction of cross features,GraphFC: Customs Fraud Detection with Label Scarcity GraphFC: Customs Fraud Detection with Label Scarcity, ,
we apply transformation for all data points to obtain a new feature
matrix M∈R𝑛×𝑊.
4.2 Transaction Interaction Learning with
Message Passing
The strengths of learning latent representations of transactions via
graph neural networks are mainly threefold. First, given a transac-
tion node4𝑣𝑡∈VTand it’s representation vector 𝑚𝑘
𝑡at𝑘𝑡ℎlayer,
graph neural network updates 𝑚𝑘
𝑡with the feature vector of adja-
cent nodes (i.e., 𝑚𝑘
𝑗∀𝑗∈𝑁(𝑣𝑡)). Hence the nearby transactions in
Gcould be included to form the representation for 𝑣𝑡. Second, the
aggregation process takes the neighboring labeled andunlabeled
transactions into account, a critical step in semi-supervised settings.
Lastly, existing work [ 20] fails to extract informative patterns if
either a new importer or an unseen item made the transaction.
On the contrary, the inductive GNN generalizes to unseen nodes
by exploring the node neighborhood. We elaborate the detailed
framework in the following paragraphs.
In this work, GraphFC adopts GNN architecture inspired by
GAT [ 38], that has shown strong representation learning abilities
among different graph learning tasks. Thanks to the flexible na-
ture of our proposed approach, the backbone GNN architecture
could be replaced with any GNN model . We demonstrate this by
experimenting with GraphSAGE [ 17] and RGCN [ 33] (refer Table 2).
We initialize the node embedding for VCwith zero vectors then
update it via the propagation from neighboring transaction features.
4.2.1 Local subgraph extraction via neighbor sampling. Training
transductive GNNs such as GCN [ 22] usually requires loading the
entire graph data and the intermediate states of all nodes into mem-
ory. The full-batch training algorithm for GNNs suffers significantly
from the memory overflow problem, especially when dealing with
large scale graphs containing millions of nodes. Also, the addition
of new nodes would require retraining of the entire model. To mit-
igate these issues, the neighborhood sampling technique [ 17] is
applied to extract a local subgraph from a target node for inductive
mini-batch training. Specifically, it samples a tree rooted at each
node by recursively expanding the root node’s neighborhood by
𝐾steps with a fixed sample size of {𝑇1,𝑇2,...,𝑇𝐾}, where𝑇𝑖is the
number of nodes sampled in 𝑖-th step. Note that we alternately
sample𝑇𝑖nodes from either VCorVTat each step to form the
subgraph. Afterwards, the desired message passing operation could
be applied to the subgraph.
4.2.2 Message passing with graph neural network . Intuitively, the
interaction between nodes can be considered as an embedding prop-
agation (i.e., message passing ) from adjacent nodes to the source
node. By stacking multiple message passing operations, the node
representations collect information from a local subgraph, as illus-
trated in Fig. 3 and Sec. 3.2.
The process of message construction derives the joint represen-
tation of a node by passing its features through a neural network.
The message aggregation process updates all of the neighboring
messages obtainable from the previous message construction stage.
4In context of embeddings, we use the terms “node(s)” and “transaction(s)” invariably
hereafter.It unifies them into a single representation as the final node embed-
ding.
Message Construction. Given set of node pair (𝑚,𝑗), where
one of{𝑚,𝑛}falls inVTand the other belongs to VC. We first
define the message from node 𝑛to node𝑚as:
𝑔𝑚←𝑗=𝑓(𝑠𝑚,𝑠𝑗), (1)
where𝑔𝑚←𝑗is the message embedding(i,e, information to be propa-
gated) and𝑠𝑘denotes the𝑘𝑡ℎrow in M.𝑓(·)is the message encoding
function which takes the node embeddings of {𝑚,𝑗}and output
their joint representation, 𝑓(·)is implemented as follows:
𝑓(𝑠𝑚,𝑠𝑗)=𝛼𝑚𝑗𝑠𝑗, (2)
where𝛼𝑚𝑗is the attention score that represents the similarity
between nodes which is given as:
𝛼𝑚𝑗=exp 𝜎 r⊤[W1𝑠𝑗∥W1𝑠𝑚]
Í
𝑛∈𝑁(𝑚)exp 𝜎 r⊤[W1𝑠𝑗∥W1𝑠𝑗], (3)
where ris the learnable vector that projects the embedding into
a scalar,∥denotes the concatenation operation, 𝜎represents non-
linear activation function(we use LeakyReLU [ 26] in this work),
andW1is a learnable weight matrix.
It is worth pointing out here that different GNN methods have
various strategies to discriminate edges when generating node
embeddings. In this work, we use attention-based aggregation to
assign different importance weights to each edge so that the model
can better distinguish transactions with different importers and
with different HS-codes.
For example, RGCN [ 33] exploits discriminate edges based on
their relations, and SAGE [ 17] does not discriminate edges but treat
all links equal.
Thus, any newer aggregation methodologies can be employed
in future for enhancing the performance even further.
Message Aggregation. In this stage, messages obtainable from
node𝑚and all of it’s neighboring nodes {𝑗∈𝑁(𝑚)}are aggregated
into an unified representation. Aggregation function is defined as:
𝑠(1)
𝑚=𝑅𝑒𝐿𝑈©­
«𝑔𝑚←𝑚+∑︁
𝑗∈𝑁(𝑚)W2𝑔𝑚←𝑗ª®
¬, (4)
where𝑔𝑚←𝑚=𝑊2𝑠𝑚and𝑠(1)
𝑚denotes the new representation of
node𝑚after the first propagation process and 𝑊2is a learnable
weight matrix.
Given the first-order propagation rule (i.e., Eq. 4.2.2), we could
further capture the higher-order transaction signals by stacking 𝑘
embedding propagation layer. Formally, the node representation at
𝑘-th layer could be derived as:
𝑠(𝑘)
𝑚=𝑅𝑒𝐿𝑈©­
«𝑔(𝑘)
𝑚←𝑚+∑︁
𝑗∈𝑁(𝑚)W(𝑘)
2𝑔(𝑘)
𝑚←𝑗ª®
¬,
=𝑅𝑒𝐿𝑈©­
«∑︁
𝑗∈𝑁(𝑚)∪{𝑚}W(𝑘)
2𝛼(𝑘)
𝑚𝑗𝑠(𝑘−1)
𝑗ª®
¬(5)
where𝑊(𝑘)
2is a trainable matrix and 𝛼(𝑘)
𝑚𝑗is defined similar as
Eq. 3 which 𝑊1is replaced by 𝑊(𝑘)
1denotes the learnable weightGraphFC: Customs Fraud Detection with Label Scarcity, , Singh and Tsai, et al.
matrix at the 𝑘-th layer. In this work, we use the embeddings at
last layer (i.e., 𝑠(𝑘)
𝑚) as the final representation for each transaction.
4.2.3 Order of Message Passing. In the message passing operation,
one of the key components of GraphFC is to make use of the virtual
nodeVCto aggregate features from its neighboring transactions,
withVCbeing connected to both labeled and unlabeled transac-
tions. As the node embeddings of virtual node VCis initialized with
zero vectors, it is critical to decide the order of message passing
to avoid collecting information from an empty vector. We address
this issue by updating embeddings of VCandVTin the following
manner: In step 1, the embeddings for VCare updated by collecting
information fromVTvia E.q 1. It is followed by step 2 that updates
embeddings ofVTas per new representations of 𝑉𝐶. Step 1 and
step 2 performed alternately to obtain the final embeddings.
4.3 Self-Supervised Pretraining
Unsupervised pretraining approaches in GNNs enable learning of
hidden relational patterns and structural properties of the under-
lying network. The fact that such information can be learnt solely
from observational data like node and edge features, without the
need for ground-truth labels, lends an amazing flexibility to the rep-
resentational learning of the GNNs [ 6,21,40]. Inspired by this, we
adopt a self-supervised GNN pretraining scheme that learns trans-
actions embeddings by using the given features of both the labeled
and unlabeled transactions data.. Thereafter, we utilize these embed-
dings as a prior for the semi-supervised learning stage of GraphFC .
The objective of the self-supervised step is to learn the embeddings
of a node such that its representation in the latent space is closer to
its neighboring nodes. By forcing the embedding of nearby nodes
closer, GNN learns to preserve the graph structure level information
as discussed in [ 15]. Besides, it improves the generalization ability
by aligning the distribution of unlabeled data closer to labeled data
and thus reduce the extrapolation phenomenon.
We utilize the following self-supervised pretraining objective
function [17]:
L𝑝𝑟𝑒(𝑠(𝑘)
𝑢)=−1
|𝑁(𝑢)|∑︁
𝑣∈𝑁(𝑢)log
𝜙
𝑠(𝑘)⊤
𝑢𝑠(𝑘)
𝑣
+1
𝑅∑︁
𝑣𝑛∈𝑃𝑛log
𝜙
𝑠(𝑘)⊤
𝑢𝑠(𝑘)
𝑣𝑛 (6)
where𝑣is the immediate neighbor of node 𝑢,𝑃𝑛denotes the neg-
ative sampling distribution that gives a set of negative sampled
nodes, and𝑅is the number of negative samples. By minimizing the
objective function in Eq. 6, GraphFC is forced to lower the distance
of representation of nearby nodes while pushing away the negative
sampled nodes. Note that the pretraining operation is applied for
all nodes inVwhich implies that both labeled and unlabeled infor-
mation would be used to learn robust GNN parameters. Lastly, the
trained parameters are saved and fine-tuned on the labeled dataset
with Eq. 8.
4.4 Prediction and Optimization
To determine the illicitness of a given transaction, we propose
to map the graph-based representation into scalars and train the
neural network parameters via the following objective function.Table 1: Brief data description of the three datasets.
A-Data B-Data C-Data
Starting Date 2016-01-01 2014-01-01 2017-01-01
Ending Date 2019-01-01 2017-01-01 2019-01-01
# Transactions 1895222 1428397 2385367
# importers 8699 165202 132893
# HS codes 5491 5953 13387
Overall Illicit rate 1.21% 4.12% 8.16%
Inspired by [ 20], the dual-task learning framework is used to achieve
two goals: 1. provide the probability of a transaction being illicit 2.
predict the additional revenue (i.e., taxes) after inspecting suspicious
transactions. Hence, we use the transaction representation (i.e., 𝑠(𝑘)
𝑚)
for both tasks of binary illicit classification and maximizing revenue
prediction. Given the transaction feature 𝑠(𝑘)
𝑚, we introduce the
task-specific layer:
ˆ𝑦𝑐𝑙𝑠(𝑠(𝑘)
𝑚)=𝜙
r⊤
1𝑠(𝑘)
𝑚+b1
,
ˆ𝑦𝑟𝑒𝑣(𝑠(𝑘)
𝑚)=r⊤
2𝑠(𝑘)
𝑚+b2,(7)
where r1,r2∈R𝑑denotes the hidden vectors of task-specific layers
that project 𝑠(𝑘)
𝑚into the prediction tasks of binary illicitness and
raised revenue, respectively. 𝜙is the sigmoid function. ˆ𝑦𝑐𝑙𝑠(𝑠(𝑘)
𝑚)is
the predicted probability of a transaction being illicit, and ˆ𝑦𝑟𝑒𝑣(𝑠(𝑘)
𝑚)
is the predicted raised revenue value of a transaction. The final
objective functionLGraphFC is given by:
LGraphFC =L𝑐𝑙𝑠+𝛼L𝑟𝑒𝑣+𝜆∥Θ∥2, (8)
where Θdenotes all learnable model parameters, L𝑐𝑙𝑠(refer Eq. 9)
is the cross-entropy loss for binary illicitness classification, L𝑟𝑒𝑣
(refer Eq. 9) is the mean-square loss for raised revenue prediction.
The hyperparameter 𝛼is used to balance the contributions.
Finally, we use mini-batch gradient descent to optimize the ob-
jective functionLGraphFC , along with the Ranger [45] optimizer.
5 EVALUATION
5.1 Experimental Setup
We employed multi-year transaction-level import data from three
partner countries, released by WCO for research usage. Because
the data contains only the import transactions and those reported
with undervaluation, we are only working with limited data from a
larger dataset. Due to data confidentiality policies, we abbreviate
these countries as A, B, and C. Subject to data availability, the three
datasets have a different number of records, details of which are
summarized in Table 1. Further, as data belongs to different customs
administrations, it varies slightly in the features. Nonetheless, the
most relevant features are the same across the datasets. Among
others, these include the HS-code, importer ids, the notional value of
the transaction, date, taxes paid, the quantity of the goods. We also
include a binary feature representing historic fraudulent behavior
or total taxes paid per unit value.
Data Split. To mimic the actual setting, we split the train, valid,
and test data temporally. Since we have multi-year data, we treat
the data from the most recent year as the test, and the older data is
split as train and validation sets.GraphFC: Customs Fraud Detection with Label Scarcity GraphFC: Customs Fraud Detection with Label Scarcity, ,
Because of our proposed design of introducing the virtual nodes
for HS-codes and importer ids as presented in Sec 3.2, the number
of edges in the resultant graph structures of each of the datasets
would be twice the number of transactions in it.
Evaluation Criteria. The customs administrations can manually
inspect only a limited number of transactions; we imitate this set-
ting by evaluating the model performance on top-n% (we demon-
strate results for 1%, and 5%) of the suspicious transactions sug-
gested by the model. Thus, e.g., precision at 1% is equivalent to
classification precision when top 1% of transactions suggested by
GraphFC aremanually inspected and the ground-truth for those
transactions is obtained. Same as DATE [ 20], we use precision (Pre.),
recall (Rec.) as evaluation metrics to inspect the model performance.
AsGraphFC supports the dual-task learning, we also report revenue
(Rev.) collected by correct identification of the illicit transactions.
This metric is reported as the ratio of the total revenue collected if
all fraudulent transactions were identified. In a live setting, an in-
spection of top-n% items suggested by the model equates to customs
officers examining the transactions that would yield the maximum
revenue (refer Eq. 10).
We run an elaborate set of experiments that aim to answer the
following: (a)Subject to the limited inspection rate by the customs
officers, how well can GraphFC identify the fraudulent transac-
tions? (b)Similarly, what additional revenue can GraphFC gener-
ate by inspection of items suggested by it? (c)How unlabeled data
helps in semi-supervised learning? (d)Do different components in
GraphFC contribute to making better predictions?, and finally, (e)
Is the model robust to different backbone networks and inspection
rates.
5.2 Performance Comparison
GraphFC performance is compared with 4 baseline methods:
•XGBoost [9]: XGBoost is a tree-based model which is widely
used for modeling tabular data.
•Tabnet [4]: Tabnet is a self-supervised learning-based framework
especially designed for tabular data.
•VIME [47]: VIME adopts self-and semi-supervised learning tech-
niques for tabular data modeling.
•DATE [20]: This fraud detection method which utilizes tree-
aware embeddings and attention network.
Performance under Low Inspection Ratio. To verify the effec-
tiveness of the proposed model subject to label scarcity, we assume
a 5% inspection rate for each country and randomly sample 5%
of the transactions from the train set as labeled and mask labels
for the remaining 95% of the transactions. Results for this setting
are presented in Table 2. Among the baseline methods, the current
state-of-the-art customs fraud detection method, DATE, shows su-
periority due to its network design that considers the joint behavior
between importer id and item codes. On the other hand, we no-
tice that although VIME and Tabnet both utilize unlabeled data to
perform self- and semi-supervised learning, the performance are
still relatively lower than DATE and GraphFC . The reason being
design of both DATE and GraphFC that explicitly consider the
information of transactions made by different importers and trade
goods. GraphFC shows a consistent and remarkable improvementTable 2: Performance under 5% inspection rate. The 5%
ground truth labels are used (95% of labels are masked). n
denotes the top n% transaction suggested by the model.
A-Data
n = 1% n=5%
Model Pre. Rec. Rev. Pre. Rec. Rev.
XGB 0.026 0.021 0.040 0.017 0.070 0.096
Tabnet 0.031 0.024 0.031 0.016 0.061 0.049
VIME 0.021 0.018 0.030 0.007 0.030 0.038
DATE 0.045 0.037 0.056 0.033 0.129 0.187
GraphFC SAGE 0.040 0.032 0.039 0.026 0.104 0.105
GraphFC RGCN 0.050 0.040 0.064 0.039 0.152 0.189
GraphFC 0.045 0.035 0.057 0.030 0.118 0.170
B-Data
XGB 0.151 0.061 0.109 0.045 0.092 0.184
Tabnet 0.149 0.060 0.089 0.046 0.085 0.171
VIME 0.064 0.024 0.052 0.043 0.087 0.208
DATE 0.152 0.061 0.105 0.057 0.115 0.210
GraphFC SAGE 0.327 0.132 0.143 0.201 0.405 0.305
GraphFC RGCN 0.259 0.104 0.127 0.179 0.362 0.334
GraphFC 0.401 0.162 0.171 0.200 0.405 0.304
C-Data
XGB 0.671 0.070 0.120 0.445 0.234 0.359
Tabnet 0.715 0.050 0.081 0.452 0.231 0.334
VIME 0.725 0.076 0.116 0.471 0.249 0.362
DATE 0.803 0.085 0.158 0.472 0.249 0.380
GraphFC SAGE 0.788 0.083 0.135 0.535 0.282 0.442
GraphFC RGCN 0.819 0.086 0.146 0.525 0.277 0.424
GraphFC 0.869 0.092 0.170 0.535 0.282 0.445
over the baselines, which verifies the effectiveness of the proposed
model. It is worth emphasizing that the significant improvement of
GraphFC over VIME and Tabnet further validates the superiority
of the proposed model.
Model Robustness. We demonstrate model robustness from two
aspects: with different backbone GNNs, and by varying inspection
rates. Specifically, we use the following backbone GNNs:
•GraphFC SAGE : AGraphFC variant using GraphSAGE [ 17] ag-
gregator in message passing.
•GraphFC RGCN : AGraphFC variant using RGCN [ 33] aggrega-
tor designed for relational and heterogeneous graphs.
•GraphFC : Proposed model with GAT [39] based aggregator.
Inspection rates are varied in {1%,2%,5%,10%,20%}, representing
customs in different countries. Results with different backbone net-
works are presented in Table 2. Fig. 5 shows the model performance
under different inspection rates. It can be observed that though
the detection performance drops as the inspection rate decreases,
GraphFC and its variants still consistently outperform all the base-
lines. These results firmly establish that GraphFC is robust against
different degrees of label scarcity, and generalize well in different
countries. Additionally, model robustness with different backbone
GNNs is a proof that any desired aggregation strategy can be used
inGraphFC as per the features in the target data.
5.3 Effectiveness of Unlabeled Data
GraphFC improves its representation ability by utilizing the un-
labeled data in the pretraining and fine-tuning stage. The key to
making use of unlabeled data lies in the construction of transaction
graph as presented in Fig. 3. The transaction graph comprises ofGraphFC: Customs Fraud Detection with Label Scarcity, , Singh and Tsai, et al.
1.0 2.0 5.0 10.0 20.0
A-Data0.020.040.06Pre@1
1.0 2.0 5.0 10.0 20.0
B-Data0.00.20.4
1.0 2.0 5.0 10.0 20.0
C-Data0.000.250.500.75
XGB
TabnetVIME
DATEGraphFC
GraphFCSAGEGraphFCRGCN
Figure 5: Performance comparison by varying inspection rate. The x-axis denotes the different inspection rate.
Table 3: Performance comparison on different training
graphs under 5% inspection rate. n denotes the top n% trans-
action suggested by the model.
B-Data
n = 1% n=5%
Model Pre. Rec. Rev. Pre. Rec. Rev.
Q(G𝐿) + F(G𝐿)0.245 0.099 0.134 0.147 0.298 0.259
Q(G𝑈) + F(G𝐿)0.278 0.112 0.141 0.186 0.377 0.304
Q(G) + F(G𝐿) 0.286 0.115 0.141 0.180 0.363 0.285
Q(G𝐿) + F(G) 0.292 0.118 0.149 0.163 0.329 0.269
Q(G𝑈) + F(G) 0.402 0.162 0.155 0.173 0.349 0.286
Q(G) + F(G) 0.402 0.162 0.172 0.200 0.406 0.305
𝑉𝑇and𝑉𝐶, where𝑉𝑇includes both labeled and unlabeled trans-
actions. To verify the effectiveness of unlabeled data in 𝑉𝑇, two
variants ofGare made with the following rule: (a)G𝐿: keeps only
the labeled transaction and its corresponding edges with 𝑉𝐶.(b)
G𝑈: keeps only the unlabeled transaction and its corresponding
edges with𝑉𝐶. We then compare the results using different graphs
in the pretraining(Q) and fine-tuning (F) stage With 6 combinations
and list the performance with a semi-supervised setting of B-Data
in Table 3. Each row represents the graphs used in pretraining and
fine-tuning, for example, 𝑄(G𝐿)+𝐹(G𝑈)means pretraining with
graphG𝐿and fine-tune on graph G𝑈.
First, it is worth noticing that 𝑄(G𝐿)+𝐹(G𝐿)is the only variant
that solely uses labeled data without inclusion of any unlabeled data
in the training process. The results show that it performs the worst
among all test cases. The takeaway here is that the using unlabeled
data in the pretraining or fine-tuning process leads to a significant
improvement in the model performance. Second, we compare the
result of fine-tuning on G𝐿andG. The results show that fine-tuning
with unlabeled data generally performs better, which again shows
the advantage of jointly considering labeled and unlabeled informa-
tion. Lastly, we observe the difference between 𝑄(G𝐿),𝑄(G𝑈)and
𝑄(G).𝑄(G𝐿)is significantly worse than the others while 𝑄(G𝑈)
and𝑄(G)usually gives similar performance. This final result adds
to the already evident effectiveness of adding unlabeled data in
(a) w/o unlabeled data.
 (b) w/o pretraining.
 (c) Full model.
Figure 6: TSNE visualization of variants of GraphFC . Each
transaction is mapped to a point in 2D space. Red, blue and
grey dots denote the illicit, legitimate, and unlabeled trans-
actions respectively.
the pretraining stage. It not only provides more training instances
for learning the weight matrices (i.e., W(𝑘)
1,W(𝑘)
2) but makes the
nearby transaction closer in embedding space, leading to better
predictive capabilities.
To further explore the effect of including unlabeled data in
GraphFC , we train two variants of GraphFC and mapped their
transaction embedding obtainable before the output layer into 2-
dimensional space using TSNE, and present the result in Figure 6.
Figure 6a presents the variant of GraphFC where we remove all
of the unlabeled data in G(refer to the setting of 𝑄(G𝐿)+𝐹(G𝐿)),
whereas for Figure 6b, the pretraining step is ommited, and Fig-
ure 6c is the result of GraphFC . Figure 6c is the best performing
case where legitimate and illicit transactions are separated well into
two tightly clustered sets. In Figure 6a, we can observe diverse pat-
terns that are not clearly separable. Figure 6b has better separation
but data points are more dispersed than as compared to Figure 6c.
To sum up, this analysis shows the importance of unlabeled data in
learning high quality embedding vectors that played a key role for
mitigating effects of label scarcity.
5.4 Prediction Performance for Unseen
Importers
Inductive learning can be understood as the ability of the model to
learn from the historic data and generalize it to new, unseen cases.
Given the huge volume of transactions, the model will consistently
come across input data that has unseen features, such as new itemsGraphFC: Customs Fraud Detection with Label Scarcity GraphFC: Customs Fraud Detection with Label Scarcity, ,
and importers. In the previous work DATE [ 20], the representations
for every importer and HS-code were learnt, however, in a semi-
supervised setting, such a learning becomes infeasible. GraphFC
provides a novel solution for inductive learning by linking the
newly observed importer and item to the target transaction and
its representation could be updated via multiple messages passing
operation. To measure the degree of the inductive phenomenon, we
introduce Out-of-Sample-Ratio (OSR), which is defined as: 𝑂𝑆𝑅=
|𝑃𝑢𝑛|/(|𝑃𝑠|+|𝑃𝑢𝑛|),where|𝑃𝑠|and|𝑃𝑢𝑛|denote the number of
unique states (refer inductive setting in 3.1 for definition) observed
and unobserved in the training data, respectively. The higher OSR
is, the more unseen data states are present in the data. In this work,
we focus on analyzing the behavior of importer and HS-code.
Table 4: Inductive setting - Importer
A-Data , OSR: 9.89%
n = 1% n=5%
Model Pre. Rec. Rev. Pre. Rec. Rev.
GraphFC 0.0486 0.0168 0.0161 0.0396 0.0686 0.0672
DATE 0.0660 0.0229 0.0271 0.0501 0.0866 0.0953
B-Data , OSR: 49.43%
GraphFC 0.5386 0.1963 0.2236 0.2055 0.3744 0.3846
DATE 0.0655 0.0239 0.0482 0.0549 0.1000 0.1580
C-Data , OSR: 5.09%
GraphFC 0.9052 0.0483 0.0847 0.7805 0.2081 0.3396
DATE 0.7730 0.0413 0.0616 0.6418 0.1711 0.2605
Table 5: Inductive setting - HSCode
A-Data , OSR: 2.75%
n = 1% n=5%
Model Pre. Rec. Rev. Pre. Rec. Rev.
GraphFC 0.0560 0.0329 0.0219 0.0471 0.1382 0.1213
DATE 0.0373 0.0219 0.0104 0.0396 0.1162 0.0964
B-Data , OSR: 3.64%
GraphFC 0.4554 0.1053 0.1256 0.1577 0.1808 0.1889
DATE 0.2772 0.0641 0.1002 0.0978 0.1121 0.1960
C-Data , OSR: 4.74%
GraphFC 0.7320 0.0574 0.1143 0.4277 0.1675 0.2620
DATE 0.5876 0.0461 0.0933 0.3946 0.1545 0.2142
Inductive analysis in semi-supervised learning We evalu-
ate the performance of inductive learning by selecting a subset of
transactions from testing data where the importer ID or HS-code
were not observed in training data. The result is presented in and
presents the result in Table 4 and 5. We compare GraphFC with
DATE as DATE being the state-of-the-art method for transductive
learning. In Table 4, it demonstrates the performance on predict-
ing transactions made by unseen importers. GraphFC outperforms
DATE with a significant improvement as OSR increases. Especially
in B-data, the OSR is 49.43% and GraphFC gains 7x improvement
in terms of Pre@1%. On the other hand, Table 5 also shows the
superiority against DATE on unseen HS-code. Additionally, in Fig-
ure 7 we also show the effect of varying OSR rates by controlling
HS codes and importer id. GraphFC outperforms the baselines in
all cases.Table 6: Component Analysis.
A-Data
n = 1% n=5%
Model Pre. Rec. Rev. Pre. Rec. Rev.
GraphFC semi 0.030 0.024 0.034 0.023 0.092 0.127
GraphFC joint 0.025 0.019 0.037 0.023 0.090 0.117
GraphFC only 0.032 0.024 0.034 0.017 0.067 0.081
GraphFC sparse 0.030 0.024 0.037 0.018 0.070 0.096
GraphFC 0.045 0.035 0.057 0.030 0.118 0.170
B-Data
GraphFC semi 0.281 0.113 0.162 0.197 0.398 0.367
GraphFC joint 0.218 0.076 0.092 0.105 0.203 0.212
GraphFC only 0.068 0.027 0.069 0.034 0.069 0.157
GraphFC sparse 0.067 0.070 0.120 0.044 0.234 0.359
GraphFC 0.401 0.162 0.171 0.200 0.405 0.304
C-Data
GraphFC semi 0.829 0.087 0.143 0.548 0.289 0.434
GraphFC joint 0.807 0.061 0.135 0.491 0.249 0.401
GraphFC only 0.764 0.080 0.115 0.490 0.258 0.382
GraphFC sparse 0.712 0.075 0.123 0.473 0.250 0.376
GraphFC 0.869 0.092 0.170 0.535 0.282 0.445
5.5 Component Analysis
To establish the contribution of different components in GraphFC ,
we ablate its core components and evaluate its performance. Specif-
ically, we evaluate the following settings:
•GraphFC : Use full model.
•No unsupervised pretraining (GNN semi): Skip the unsuper-
vised pre-train step.
•Joint training (GNN joint): Remove pretraining and jointly opti-
mizing E.q 6 and Eq. 8.
•No GBDT (GNN only): Remove the GBDT step of the model and
utilize the data from transactions directly.
•No item/importer nodes (GNN sparse ): Remove one of the cat-
egorical nodes utilized for building the graph structure and build
asparser graph.
Results of different variants are demonstrate in Table 6. It can
be noticed that removing any component in GraphFC leads to
degradation of performance. The only exception is for GraphFC semi
that delivers similar performance in C-data. Among these variants,
GraphFC sparse andGraphFC onlyperform poorly which indicates
that both the GBDT step, and utilizing the importer and HS-code
for constructing the transaction graph play an important role in
GraphFC .
In general, neither GraphFC semi norGraphFC joint improves
GraphFC which verifies the necessity our pretraining step. Specifi-
cally, the difference between GraphFC andGraphFC semi,GraphFC joint
are significant when 𝑛is small (e.g. 𝑛=1%) which is important
since customs usually maintain a low inpsection rate lower than
5%.
6 DISCUSSION AND CONCLUSION
With increased connectivity, development, and globalization, cus-
toms administrations across the world have to process an increas-
ingly large number of transactions. This calls for an automatedGraphFC: Customs Fraud Detection with Label Scarcity, , Singh and Tsai, et al.
Figure 7: Performance comparison by varying OSR (%). (a) & (b) & (c) (controls importer id), (d) & (e) & (f) (controls HS-code). In
all cases, GraphFC outperforms the baselines.
5.0 10.0 20.0 30.0
OSR0.860.880.900.920.940.960.98Pre@1
GraphFC
DATEGraphFCSAGE
GraphFCRGCN
(a) N-data
5.0 10.0 20.0 30.0
OSR0.700.750.800.850.900.951.00Pre@1
GraphFC
DATEGraphFCSAGE
GraphFCRGCN (b) T-data
5.0 10.0 20.0 30.0
OSR0.0650.0700.0750.0800.085Pre@1
GraphFC
DATEGraphFCSAGE
GraphFCRGCN (c) C-data
5.0 10.0 20.0 30.0
OSR0.860.880.900.920.940.960.98Pre@1
GraphFC
DATEGraphFCSAGE
GraphFCRGCN
(d) N-data
5.0 10.0 20.0 30.0
OSR0.800.820.840.860.880.900.92Pre@1
GraphFC
DATEGraphFCSAGE
GraphFCRGCN (e) T-data
5.0 10.0 20.0 30.0
OSR0.0600.0650.0700.0750.0800.0850.090Pre@1
GraphFC
DATEGraphFCSAGE
GraphFCRGCN (f) C-data
system for detecting fraudulent transactions, as manual inspection
of such an astronomical volume of transactions is infeasible. In this
work, we develop GraphFC , a GNN based fraud detection model
that transforms the customs tabular data into a graph and maxi-
mizes the identification of malicious customs transactions while
also maximizing the additional revenue collected from these trans-
actions. We propose the given approach and exhibit its effectiveness
by developing a model and testing it on real data from multiple
countries. In our experiments, we show that GraphFC is capable
of making use of the precious unlabeled data and further substan-
tially alleviate the performance drop under different levels of label
scarcity. The proposed model provides a remedy to any country
who is trying to facilitate automated customs fraud detection even
if the administration did not collect huge labeled data.
REFERENCES
[1]Aisha Abdallah, Mohd Aizaini Maarof, and Anazida Zainal. 2016. Fraud detection
system: A survey. Journal of Network and Computer Applications 68 (2016).
[2]Aderemi O Adewumi and Andronicus A Akinyelu. 2017. A survey of machine-
learning and nature-inspired based credit card fraud detection techniques. Inter-
national Journal of System Assurance Engineering and Management 8, 2 (2017).
[3]Mohiuddin Ahmed, Abdun Naser Mahmood, and Md. Rafiqul Islam. 2016. A
survey of anomaly detection techniques in financial domain. Future Generation
Computer Systems 55 (2016), 278–288.
[4]Sercan O Arik and Tomas Pfister. 2019. Tabnet: Attentive interpretable tabular
learning. arXiv preprint arXiv:1908.07442 (2019).
[5]Jean-François Arvis, Lauri Ojala, Christina Wiederer, Ben Shepherd, Anasuya
Raj, Karlygash Dairabayeva, and Tuomas Kiiski. 2018. Connecting to compete
2018. (2018).[6]Aleksandar Bojchevski and Stephan Günnemann. 2018. Deep Gauss-
ian Embedding of Graphs: Unsupervised Inductive Learning via Ranking.
arXiv:1707.03815 [stat.ML]
[7]Canrakerta, Achmad Nizar Hidayanto, and Yova Ruldeviyani. 2020. Application of
business intelligence for customs declaration: A case study in Indonesia. Journal
of Physics: Conference Series 1444 (2020), 012028.
[8]Andrea Cerioli, Lucio Barabesi, Andrea Cerasa, Mario Menegatti, and Domenico
Perrotta. 2019. Newcomb–Benford law and the detection of frauds in international
trade. Proceedings of the National Academy of Sciences 116, 1 (2019), 106–115.
[9]Tianqi Chen and Carlos Guestrin. 2016. XGBoost: A scalable tree boosting system.
InKDD . 785–794.
[10] Zhe Chen and Aixin Sun. 2020. Anomaly Detection on Dynamic Bipartite Graph
with Burstiness. In 2020 IEEE International Conference on Data Mining (ICDM) .
966–971.
[11] Daniel de Roux, Boris Perez, Andrés Moreno, Maria del Pilar Villamil, and César
Figueroa. 2018. Tax fraud detection for under-reporting declarations using an
unsupervised machine learning approach. In KDD . 215–222.
[12] Yingtong Dou, Zhiwei Liu, Li Sun, Yutong Deng, Hao Peng, and Philip S. Yu. 2020.
Enhancing Graph Neural Network-Based Fraud Detectors against Camouflaged
Fraudsters. In Proceedings of the 29th ACM International Conference on Information
& Knowledge Management . 315–324.
[13] Jorge Jambeiro Filho and Jacques Wainer. 2008. HPB: A model for handling BN
nodes with high cardinality parents. JMLR 9 (2008), 2141–2170.
[14] Anne-Marie Geourjon, Bertrand Laporte, Ousmane Coundoul, Massene Gadiaga,
T Cantens, R Ireland, and G Raballand. 2013. Inspecting Less to Inspect Better.
The Use of Data Mining for Risk Management by Customs Administrations’.
Reform by Numbers. Measurement Applied to Customs and Tax Administrations in
Developing Countries, Washington DC: World Bank (2013).
[15] Aditya Grover and Jure Leskovec. 2016. node2vec: Scalable feature learning for
networks. In Proceedings of the 22nd ACM SIGKDD international conference on
Knowledge discovery and data mining . 855–864.
[16] Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive representation
learning on large graphs. In proc. of the NeurIPS . 1024–1034.
[17] William L Hamilton, Rex Ying, and Jure Leskovec. 2017. Inductive representation
learning on large graphs. arXiv preprint arXiv:1706.02216 (2017).GraphFC: Customs Fraud Detection with Label Scarcity GraphFC: Customs Fraud Detection with Label Scarcity, ,
[18] Xinran He, Junfeng Pan, Ou Jin, Tianbing Xu, Bo Liu, Tao Xu, Yanxin Shi, Antoine
Atallah, Ralf Herbrich, Stuart Bowers, et al .2014. Practical lessons from predicting
clicks on ads at facebook. In ADKDD . 1–9.
[19] Michael Keen. 2003. Changing Customs: Challenges and Strategies for the Reform
of Customs Administration . International Monetary Fund, USA.
[20] Sundong Kim, Yu-Che Tsai, Karandeep Singh, Yeonsoo Choi, Etim Ibok, Cheng-Te
Li, and Meeyoung Cha. 2020. DATE: Dual Attentive Tree-aware Embedding for
Customs Fraud Detection. In Proceedings of the 26th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining .
[21] Thomas N. Kipf and Max Welling. 2016. Variational Graph Auto-Encoders.
arXiv:1611.07308 [stat.ML]
[22] Thomas N. Kipf and Max Welling. 2017. Semi-Supervised Classification with
Graph Convolutional Networks. In Proc. of the ICLR .
[23] Maria Krivko. 2010. A hybrid model for plastic card fraud detection systems.
Expert Systems with Applications 37, 8 (2010), 6070–6076.
[24] Anuj Kumar and Vishnuprasad Nagadevara. 2006. Development of hybrid classifi-
cation methodology for mining skewed data sets: A case study of Indian customs
data. (2006).
[25] Yiğit Kültür and Mehmet Ufuk Çağlayan. 2017. Hybrid approaches for detecting
credit card fraud. Expert Systems 34, 2 (2017), e12191.
[26] Andrew L Maas, Awni Y Hannun, and Andrew Y Ng. 2013. Rectifier nonlinearities
improve neural network acoustic models. In Proc. icml , Vol. 30. Citeseer, 3.
[27] Kunio Mikuriya and Thomas Cantens. 2020. If algorithms dream of Customs,
do customs officials dream of algorithms? A manifesto for data mobilization in
Customs. World Customs Journal 14, 2 (2020), 3–22.
[28] Andrea Dal Pozzolo, Giacomo Boracchi, Olivier Caelen, Cesare Alippi, and Gi-
anluca Bontempi. 2018. Credit Card Fraud Detection: A Realistic Modeling and
a Novel Learning Strategy. IEEE Transactions on Neural Networks and Learning
Systems 29, 8 (2018), 3784–3797.
[29] Susie Xi Rao, Shuai Zhang, Zhichao Han, Zitao Zhang, Wei Min, Zhiyao Chen, Yi-
nan Shan, Yang Zhao, and Ce Zhang. 2020. xFraud: Explainable Fraud Transaction
Detection on Heterogeneous Graphs. arXiv:2011.12193 [cs.LG]
[30] Susie Xi Rao, Shuai Zhang, Zhichao Han, Zitao Zhang, Wei Min, Mo Cheng, Yinan
Shan, Yang Zhao, and Ce Zhang. 2020. Suspicious Massive Registration Detection
via Dynamic Heterogeneous Graph Neural Networks. arXiv:2012.10831 [cs.LG]
[31] Ram Hari Regmi and Arun K. Timalsina. 2018. Risk Management in customs
using Deep Neural Network. In IEEE International Conference on Computing,
Communication and Security . 133–137.
[32] Jean-Paul Rodrigue, Claude Comtois, and Brian Slack. 2016. The geography of
transport systems . Routledge.
[33] Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne Van Den Berg, Ivan
Titov, and Max Welling. 2018. Modeling relational data with graph convolutional
networks. In European semantic web conference . Springer, 593–607.
[34] Hua Shao, Hong Zhao, and Gui-Ran Chang. 2002. Applying data mining to detect
fraud behavior in customs declaration. In Proceedings. International Conference
on Machine Learning and Cybernetics , Vol. 3. IEEE, 1241–1244.
[35] Ron Triepels, Hennie Daniels, and Ad Feelders. 2018. Data-driven fraud detection
in international shipping. Expert Systems with Applications 99 (2018), 193–202.
[36] Jellis Vanhoeyveld, David Martens, and Bruno Peeters. 2019. Customs fraud
detection: Assessing the value of behavioural and high-cardinality data under
the imbalanced learning issue. Pattern Analysis and Applications (2019).
[37] Jellis Vanhoeyveld, David Martens, and Bruno Peeters. 2020. Customs fraud
detection. Pattern Analysis and Applications 23, 3 (2020), 1457–1477.
[38] Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro
Lio, and Yoshua Bengio. 2017. Graph attention networks. arXiv preprint
arXiv:1710.10903 (2017).
[39] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro
Liò, and Yoshua Bengio. 2018. Graph Attention Networks. In Proc. of the ICLR .
[40] Petar Veličković, William Fedus, William L. Hamilton, Pietro Liò, Yoshua Bengio,
and R Devon Hjelm. 2018. Deep Graph Infomax. arXiv:1809.10341 [stat.ML]
[41] Daixin Wang, Jianbin Lin, Peng Cui, Quanhui Jia, Zhen Wang, Yanming Fang,
Quan Yu, Jun Zhou, Shuang Yang, and Yuan Qi. 2019. A Semi-Supervised Graph
Attentive Network for Financial Fraud Detection. In 2019 IEEE International
Conference on Data Mining (ICDM) . 598–607.
[42] Jianyu Wang, Rui Wen, Chunming Wu, Yu Huang, and Jian Xion. 2019. FdGars:
Fraudster Detection via Graph Convolutional Networks in Online App Review
System. In Companion Proceedings of The 2019 World Wide Web Conference .
310–316.
[43] Xiang Wang, Xiangnan He, Fuli Feng, Liqiang Nie, and Tat-Seng Chua. 2018. TEM:
Tree-enhanced embedding model for explainable recommendation. In WWW .
1543–1552.
[44] Jarrod West and Maumita Bhattacharya. 2016. Intelligent financial fraud detection:
A comprehensive review. Computers & Security 57 (2016), 47–66.
[45] Less Wright. 2019. Ranger - a synergistic optimizer. https://github.com/lessw2020/
Ranger-Deep-Learning-Optimizer.
[46] Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L. Hamilton,
and Jure Leskovec. 2018. Graph Convolutional Neural Networks for Web-Scale
Recommender Systems. In Proceedings of the 24th ACM SIGKDD InternationalConference on Knowledge Discovery &and Data Mining . 974–983.
[47] Jinsung Yoon, Yao Zhang, James Jordon, and Mihaela van der Schaar. 2020. Vime:
Extending the success of self-and semi-supervised learning to tabular domain.
Advances in Neural Information Processing Systems 33 (2020), 11033–11043.
[48] Yong-Nan Zhu, Xiaotian Luo, Yu-Feng Li, Bin Bu, Kaibo Zhou, Wenbin Zhang,
and Mingfan Lu. 2020. Heterogeneous Mini-Graph Neural Network and Its
Application to Fraud Invitation Detection. In 2020 IEEE International Conference
on Data Mining (ICDM) . 891–899.
A APPENDIX
A.1 Algorithm and Complexity Analysis
Algorithm 1: Training algorithm for GraphFC
Input: GraphG(V,E); node feature matrix D; depth𝐾;
learnable weight matrices
Θ={W(𝑘)
1,W(𝑘)
2,∀𝑘∈{1,...,𝐾}
,r1,r2}; labeled data 𝐿=(𝑥𝑖,𝑦𝑐𝑙𝑠
𝑖,𝑦𝑟𝑒𝑣
𝑖)
Output: Trained weights Θ
1Initialize Dwith zero matrix Train XGBoost model with 𝐿
using(𝑥𝑖,𝑦𝑐𝑙𝑠
𝑖)
2Update transaction feature in Dwith leaf indices from
XGBoost
3𝑠(0)
𝑣←𝑑𝑣,∀𝑑𝑣∈D
4forstep∈{1,2,...,pretraining epochs} do
5 for𝑘=1..𝐾do
6 for𝑣∈Vdo
7 Update𝑠(𝑘)
𝑣with E.q. 4
8 end
9 end
10 Update𝑊(𝑘)
1,𝑊(𝑘)
2,∀𝑘∈{1,...,𝐾}with E.q. 6
11end
12obtain the set of labeled nodes V𝐿∈V
13forstep∈{1,2,...,finetuning epochs} do
14 for𝑘=1..𝐾do
15 for𝑣∈V𝐿do
16 Update𝑠(𝑘)
𝑣with E.q. 4
17 end
18 end
19 Obtain predictions ˆ𝑦𝑐𝑙𝑠
𝑖and ˆ𝑦𝑟𝑒𝑣
𝑖with E.q. 7
20 Update Θwith E.q. 8
21end
22output trained weight Θ
For a better understanding of the training procedure of GraphFC ,
we elaborate the pseudo-code in Algorithm 1. First, we train the
XGBoost model and obtain the leaf indices in lines 1 and 2. After-
wards, line 4 to 10 describes the pretraining stage and learn the
weight matrices{W(𝑘)
1,W(𝑘)
2. Finally, lines 13 to 22 presents the
fine-tuning stage that trains all the parameters Θwith Eq. 8 and
outputs the trained parameters.
For space complexity, we have 𝑂(|V|+|E|) as we need to keep all
the nodes and edges in track. Since each transaction is connected to
𝑙virtual nodes in maximum, the size of Ecould be replaced with 𝑛×𝑙,
where𝑛is the number of transactions. Thus the space complexityGraphFC: Customs Fraud Detection with Label Scarcity, , Singh and Tsai, et al.
could be further derive as 𝑂(𝑛(𝑙+2)), where|V|=𝑛(𝑙+1),|E|=𝑛𝑙
For time complexity, the layer-wise message passing rule is the main
operation. Without the sampling method mentioned in Sec. 4.2.1,
the memory and expected runtime of a single batch is unpredictable
and, in the worst case, could be 𝑂(|V|) . In contrast, as we sample
a fixed number of 𝑇𝑖in𝑘-th propagation layer, the per-batch space
and time complexity for GraphFC is fixed at𝑂(Î𝐾
𝑖=1𝑇𝑖).
A.2 Hyperparameters
GraphFC has two sets of hyperparameters at the macro level: GBDT
and GNN. For GBDT, we use 100trees and a maximum depth of
4(same as DATE). For unsupervised and semi-supervised GNNs,
the number of GNN layers are kept at 2with neighborhood sample
sizes𝑇1=25,𝑇2=10. The embedding dimension is searched in
{8,16,32,64}and eventually set to 32, and 𝜆is1𝑒−4. The balancing
factor𝛼is searched in{0.1,1,10}and finally set as 10. Similarly, the
batch size for the GNN models is 512. Additionally, the learning rate
is searched in{0.05,0.01,0.005,0.001}and set at 0.005for C-data
and A-data, and 0.05 for B data. For XGBoost model, we used the
default 100 trees and max depth 4 and kept it same for all other
experiments. We pre-trained Tabnet in an unsupervised fashion
with an entire training set - including both labeled and unlabeled
data and then fine-tuned the model with labeled data. Embedding
dimensions for Tabnet and DATE were searched in {8,16,32,64},
and learning rate in {0.0001,0.001,0.01}. For VIME, we tune the
learning rate in{0.0001,0.001,0.01}with Adam optimizer and the
hidden dimension in {256,512,1024}and select the best-performing
parameter on validation set.
A.3 Design Decisions
Table 7: GraphFC Performance under different design choices
A-Data
n = 1% n=5%
Model Pre. Rec. Rev. Pre. Rec. Rev.
GNN dense 0.030 0.024 0.037 0.017 0.069 0.100
GNN deep 0.033 0.026 0.045 0.022 0.085 0.128
GraphFC 0.038 0.030 0.044 0.025 0.097 0.182
B-Data
GNN dense 0.211 0.085 0.123 0.148 0.299 0.295
GNN deep 0.195 0.079 0.120 0.079 0.158 0.227
GraphFC 0.245 0.099 0.131 0.118 0.238 0.283
C-Data
GNN dense 0.837 0.088 0.174 0.556 0.293 0.457
GNN deep 0.876 0.092 0.165 0.583 0.308 0.479
GraphFC 0.869 0.092 0.170 0.535 0.282 0.445
In this section, we explore the effect of choice of most notewor-
thy hyper-, and other parameters on the model performance. For
instance, we utilize two of the categorical features in the data to
build the graph structure from the customs data. It would be inter-
esting to evaluate the effect of the inclusion of other categorical
features as well. Other parameters such as layer depth of GNNs are
directly linked to the overall architecture and hence, could directly
affect the model performance. We vary these parameters as follows:•GNN depth : Even though we designed the graph structure in
our model to be less dense , the number of nodes and edges can
nonetheless run into the order of multi-millions. To reduce the
training time, and to avoid the over-smoothing of GNNs, we
restrict the layer depth of GNNs to be 2. In this experiment,
we explore whether increasing the layer depth would offer any
additional performance gains.
•GNN dense : During the GBDT runs, we noticed that some of the
designed features designed during the feature engineering phase
prove to be most effective in building the GBDT model. In this
experimental setup, we utilize additional categorical information
(a combination of importer and office IDs) to build a denser graph.
A.4 Supervised Learning
Table 8: Model performance with a 100% inspection rate (su-
pervised setting).
A-Data
n = 1% n=5%
Model Pre. Rec. Rev. Pre. Rec. Rev.
XGB 0.077 0.061 0.015 0.051 0.201 0.341
Tabnet 0.054 0.036 0.070 0.053 0.196 0.339
DATE 0.081 0.064 0.129 0.056 0.219 0.378
GraphFC 0.087 0.069 0.121 0.056 0.222 0.334
B-Data
XGB 0.896 0.362 0.249 0.373 0.754 0.604
Tabnet 0.921 0.372 0.245 0.387 0.782 0.675
DATE 0.937 0.379 0.265 0.410 0.829 0.619
GraphFC 0.963 0.389 0.271 0.396 0.801 0.670
C-Data
XGB 0.844 0.089 0.158 0.539 0.284 0.421
Tabnet 0.824 0.070 0.157 0.500 0.261 0.402
DATE 0.810 0.085 0.178 0.532 0.280 0.426
GraphFC 0.912 0.096 0.188 0.587 0.310 0.450
This setting pertains to the availability of 100% of the ground-
truth labels for the historical customs data. Availability of ground-
truth labels for the entirety of the customs transactions is relatively
rare and infeasible. Our main focus in the paper is semi-supervised
setting but nonetheless, we test the model performance in full
supervised setting as we have fully labeled import transactions
from three countries. Table 8 presents the performance comparison
with baselines. GraphFC outperforms all the baselines consistently,
inspite of the fact that baselines are already performing quite well.
As an example, for B-data, the Pre@1 increases from DATE’s already
impressive 0.937 to 0.963 and Rec@1 from 0.379 to 0.389. Furthering
performance gain from such a level is quite impressive. For C-data,
we observe a gain of about 12% and 13% for Pre.@1 and Rec.@1
respectively. For A-data, the overall behavior remains same as that
of semi-supervised, with results being same or slightly better than
a strong baseline.
A.5 Classification and regression losses
We define the losses for classification ( L𝑐𝑙𝑠) and regression loss
L𝑟𝑒𝑣for dual task leanring as follows:GraphFC: Customs Fraud Detection with Label Scarcity GraphFC: Customs Fraud Detection with Label Scarcity, ,
L𝑐𝑙𝑠=−1
𝑛∑︁
𝑖𝑦𝑐𝑙𝑠
𝑖log(ˆ𝑦𝑐𝑙𝑠
𝑖(𝑠(𝑘)
𝑚)+(1−𝑦𝑐𝑙𝑠
𝑖)log(1−ˆ𝑦𝑐𝑙𝑠
𝑖(𝑠(𝑘)
𝑚),
L𝑟𝑒𝑣=1
𝑛∑︁
𝑖
𝑦𝑟𝑒𝑣
𝑖−ˆ𝑦𝑟𝑒𝑣
𝑖(𝑠(𝑘)
𝑚)2
,
(9)
where𝑦𝑐𝑙𝑠
𝑖and𝑦𝑟𝑒𝑣
𝑖are the ground-truth illicitness class and raised
revenue of transactions 𝑡𝑖, respectively, 𝜆is the regularization hy-
perparameter to prevent overfitting, and 𝑛is the number of trainingsamples. Further, the revenue collection can be defined as:
c𝑟𝑒𝑣=Í
𝑖𝑆𝑖𝑦𝑐𝑙𝑠
𝑖𝜁ˆ𝑦𝑐𝑙𝑠
𝑖Í
𝑖𝑆𝑖𝑦𝑐𝑙𝑠
𝑖(10)
where c𝑟𝑒𝑣is the total revenue collected by inspecting a certain
fraction of items over a set of test items 𝑖,𝑆𝑖is the penalty surcharge
an importer has to pay on that transaction if a fraudulent entry is
detected,𝑦𝑐𝑙𝑠
𝑖is the ground-truth label for a transaction, and 𝜁ˆ𝑦𝑐𝑙𝑠
𝑖
is a boolean flag valued true if the model’s prediction ˆ𝑦𝑐𝑙𝑠
𝑖equates
to a true positive, and false otherwise.