High Speed Human Action Recognition using a Photonic Reservoir Computer
Enrico Piccoa,∗, Piotr Antonikb, Serge Massara
aLaboratoire d’Information Quantique, CP 224, Universit´ e Libre de Bruxelles (ULB), B-1050, Bruxelles, Belgium,
bMICS EA-4037 Laboratory, CentraleSup´ elec, F-91192 Gif-sur-Yvette, France
,
Abstract
The recognition of human actions in videos is one of the most active research fields in computer vision. The canonical
approach consists in a more or less complex preprocessing stages of the raw video data, followed by a relatively simple
classification algorithm. Here we address recognition of human actions using the reservoir computing algorithm,
which allows us to focus on the classifier stage. We introduce a new training method for the reservoir computer, based
on “Timesteps Of Interest”, which combines in a simple way short and long time scales. We study the performance of
this algorithm using both numerical simulations and a photonic implementation based on a single non-linear node and
a delay line on the well known KTH dataset. We solve the task with high accuracy and speed, to the point of allowing
for processing multiple video streams in real time. The present work is thus an important step towards developing
efficient dedicated hardware for video processing.
Keywords: Reservoir computing, Computer vision, Human action recognition, Photonics
1. Introduction
Human Action Recognition (HAR) is nowadays con-
sidered a milestone in the field of Computer Vision
(CV). During the past decades it has become a pro-
lific research area due both to the importance of video
processing in everyday life and to the tremendous ad-
vances in artificial intelligence. Initially considered for
surveillance and human-machine interaction [1], HAR
also finds use in areas such as autonomous driving, med-
ical rehabilitation, content-based video indexing on so-
cial platforms, as well as education [2, 3]. To target
these applications, it is necessary to o ffer reliable au-
tonomous scene-analysis using embedded hardware to
process data in real time and with high energy e fficiency.
However HAR remains a challenging task due to the na-
ture of the actions which must be recognized, as well as
the varying contexts in which they take place involv-
ing for instance variations in lighting and illumination,
camera scale and angle variations, frame resolution, etc.
The usual approach of Computer Vision systems for
HAR recognition is to separate the optimization into
two major steps [4]. The first step consists in providing
a more concise and easily parsed representation of the
∗Corresponding author
Email address: enrico.picco@ulb.be (Enrico Picco)video. To this end preprocessing techniques and algo-
rithms are used to extract a set of relevant characteristics
(also called features) such as the various homogeneous
regions present in the image, texture, movement, color,
etc. The preprocessing step has been the main topic of
research in this area. The second step is to classify
the preprocessed data with more complex algorithms
which allow the analysis and identification of activi-
ties in video sequences [5]. In this respect deep learn-
ing is by now well established as a powerful classifica-
tion tool for human action recognition [6]. However it
comes with several drawbacks such as time-consuming
and power-ine fficient training procedures, and the need
for dedicated high-end hardware. For a review of dif-
ferent preprocessing and classification techniques, see
[2, 7, 8].
One of the most widely used datasets for HAR is the
KTH dataset, first introduced in [9, 10] and then used
in hundreds of publications [11] such as [12, 13, 14, 15,
16, 17, 18, 19, 20, 21, 22, 23, 24, 25]; the reader can
refer to [2] for a detailed comparison between some rel-
evant works on this dataset.
In this paper, we adopt a di fferent approach, focus-
ing our attention on the classifier stage. To this end
we use Reservoir Computing (RC) for the HAR task,
both in software and in an opto-electronic implementa-
Preprint submitted to Neural Networks June 21, 2023arXiv:2305.15283v2  [cs.CV]  19 Jun 2023tion. Reservoir Computing is a set of Machine Learn-
ing algorithms for designing and training artificial neu-
ral networks [26, 27, 28] that are highly e fficient for
processing time series. Reservoir Computers are sig-
nificantly easier to train than other Neural Networks.
Indeed, they consist of an untrained random recurrent
neural network which is driven by the time series to
analyse, and a trained readout layer. Reservoir comput-
ing has been applied to a variety of problems, includ-
ing for instance speech recognition [29] and time series
prediction[26, 30].
Reservoir computers (RCs) have been implemented
physically using e.g. spintronics [31, 32], laser-based
systems, [33, 34, 35, 36, 37], microresonators [38, 39],
free-space optics [40, 41], VCSELs [42, 43], memris-
tors [44, 45], integrated photonics [46, 47], and op-
toelectronic systems [48, 49]. These implementations
have been successfully tested on benchmark tasks in-
cluding hand-written digits [50], speech [51, 52] and
video [25, 53] recognition, prediction of future evolu-
tion of financial [54] and chaotic [55] time series, with
performances comparable to digital Machine Learning
algorithms. Photonic RC in particular is a promising
path towards the development of real-time and energy-
efficient information processing systems.
As a video sequence is a time domain signal, classify-
ing video signals seems particularly well adapted to the
Reservoir Computing approach. Furthermore, classify-
ing video sequences is rather complex and will therefore
test the performance of RC on a more challenging task
than often used in the literature.
RC implementations have been used previously to
target the HAR task. Zheng et al. report using microfab-
ricated MEMS resonators for HAR [56]. However, they
reported problems in reservoir size scalability and lim-
ited the size of their implementation to N =200 nodes.
Lu et al. use micro-Doppler radar signal processing for
HAR [53]. However, they achieved classification accu-
racy going only up to 85% which is behind the state-
of-the-art for HAR. Antonik et al. used a large-scale
photonic computer using free-space optics. Their im-
plementation can scale the size of the reservoir from
several tens to hundreds of thousands of nodes [25].
However, their system was rather slow, and free-space
optical experiments can su ffer from environmental in-
fluences such as noise and temperature variations.
Here we apply the RC architecture based on a delay
loop and a single nonlinear node to HAR. This architec-
ture has been shown to provide excellent performance
while being simple to implement, both in software [57]
and in hardware [49, 52, 58, 59]. To benchmark our
results, we use the aforementioned KTH dataset.On the conceptual side, our main advance is the in-
troduction of a novel architecture for the output layer,
inspired by Ref. [50, 60] where a similar approach was
used for static image recognition, which we call using
Timesteps Of Interest (TOI). This allows to increase the
dimensionality of the output layer without modifying
the size of the reservoir, while simultaneously introduc-
ing a new time scale which depends on which time-
frames are used.
We report results on an optoelectronic RC and on
simulations thereof. The optoelectronic RC is based
on off-the-shelf optical components and a Field Pro-
grammable Gate Array (FPGA) board, following our
previous works (see e.g. [55]). Our study shows that,
despite its simplicity, a small-scale delay RC is capa-
ble of solving the complex HAR task, achieving per-
formance comparable to the state-of-the-art digital ap-
proaches. We experimentally achieved a classification
accuracy value of 90.83% for a reservoir size of N =
600. Because of their simplicity, both the numerical and
experimental implementations are considerably faster
than previous systems, reaching more than 100 frames
per second (fps). Our work therefore provides a very
promising approach for real time, low energy consump-
tion, video processing.
Our paper is structured as follows. We first introduce
the KTH database and the dataprocessing pipelines usu-
ally used for HAR. We then introduce Reservoir Com-
puting and the concept of Timesteps Of Interest, fol-
lowed by a description of our experimental system. In
the results section we present classification acccuracy
as a function of the size of the reservoir, the number of
TOI, the input data variability kept after Principle Com-
ponennt Analysis (PCA), and we compare our results
with those obtained in previously in the literature.
2. Data Processing and the Experiment
2.1. Human Action Recognition
2.1.1. KTH Human Actions Dataset
The KTH Human Actions dataset is provided by the
Royal Institute of Technology in Stockholm [9, 10]. It
constitutes a well-known benchmark in the Machine
Learning community to compare the performance of
different learning algorithms for recognition of human
actions [7, 8]. The dataset consists of video sequences
divided into six di fferent classes of human actions,
namely walking, jogging, running, boxing, hand wav-
ing, and hand clapping. All the sequences are stored
using A VI file format and are available online ( https:
//www.csc.kth.se/cvap/actions/ ). The actions
2are performed by 25 subjects recorded in four di fferent
scenarios, outdoors s1, outdoors with scale variation s2,
outdoors with di fferent clothes s3, and indoors s4. Each
action is repeated 4 times by each subject. The video
sequences are taken with a 25fps frame rate, down-
sampled to the spatial resolution of 160 x 120 pixels,
and have a time length of 4 seconds on average. Their
length varies between 24 and 362 frames [9]. Fig. 1
shows a few samples of the data.
Figure 1: Samples of the KTH Human Actions dataset showing six
human actions output classes and four scenarios (s1: outdoors, s2:
outdoors with scale variation, s3: outdoors with di fferent clothes, s4:
indoors) [9]
Due to some missing sequences, the original dataset
contains a total of 2391 video sequences, instead of
25×6×4×4=2400 sequences for every combination of
25 subjects, 6 actions, 4 scenarios, and 4 repetitions.
For ease of use, we increase the number of sequences
to 2400 by filling in missing action sequences with a
copy of the existing ones taken from the same subject
and scenario. Consequently, the modified dataset used
in this study contains 2400 video sequences. The sub-
datasets corresponding to four di fferent scenarios con-
tain 600 videos each.
2.1.2. Data Pre-Processing
Usually, the deep-learning-based approach for
human-action recognition consists of the following
steps: (i) the preprocessing of the input data that might
include, for instance, the removal of the background,
(ii) feature extraction via a series of convolutional and
pooling layers, (iii) formation of the feature descriptors
to numerically encode interesting information, and (iv)
the actual classification using various classification al-
gorithms (cf. Fig. 2(a)) [8]. In this chain of actions,
the formation of feature descriptors is realised using a
feature-description algorithm and can be a time- and
calculation-consuming task. As the reservoir computer
is per se designed to process time-dependent signals,a separate descriptor-formation stage is not necessary.
This simplifies the overall data processing workflow and
contributes to the increase of the data processing speed
(cf. Fig. 2(b)) .
Figure 2: Process flow in human action recognition: (a) canonical
approach (adapted from [8]); and (b) our approach.
In more detail the preprocessing and feature extrac-
tion we use consists of the following steps. First we
subsample each video sequence to extract 10 keyframes
to allow the classification to work on an equal number
of frames for each video sequence (Appendix A). Then,
we use an attention mechanism to generate binary sil-
houettes by discarding the background and horizontally
centering the silhouettes in the frame [61] (Appendix
B). This removes any unnecessary information (such as
background clutter or shadows) and focuses the atten-
tion of the classifier on the pose of the subject disregard-
ing its spatial position in the frame. The feature extrac-
tion step consists in the computation of the histograms
of oriented gradients (HOG) to automatically detect the
silhouettes in the frames (Appendix C) [62]. Finally,
to simplify the computation, we reduced the number of
HOG features using the principal component analysis
(PCA) (cf. Sec. 3.3) [63, 64, 65]. The resulting features
are then injected into RC for classification.
2.2. Reservoir Computing
2.2.1. Basic Principles
In general form, a RC contains Ninternal variables
xi(n) that are often named “nodes” or “neurons” as de-
rived from the biological origins of artificial neural net-
works. The nodes are concatenated in a state vector
x(n)=xi∈0...N−1(n). The evolution of the neurons in dis-
crete time n∈Zcan be expressed as
x(n+1)=f(Wx(n)+Winu(n)) (1)
where fis a nonlinear scalar function, Wis an NxN
matrix representing the interconnectivity between the
nodes of the network, u(n) is the K-dimensional input
signal injected into the system, and Winis the NxK in-
ternal weight matrix, also called mask. The entries of
WandWinare time-independent coe fficients with val-
ues drawn from a random distribution with zero mean
3and variance adjusted to determine the dynamics of the
reservoir and obtain the best performance on the inves-
tigated task.
Here, we use a ring-topology reservoir with a nearest-
neighbor coupling between the nodes. Although quite
simple, this architecture has shown equivalent per-
formance to more complex reservoir implementations,
both numerically [57] and experimentally [36, 49, 59,
58, 66]. The nonlinearity that we use here is sinusoidal
(cf. [49, 59]) and can be easily implemented experi-
mentally [67] (cf. Sec. 2.2.4). In terms of mathematical
description, our topology changes Eq. 1 to:
x0(n+1)=sin(αxN−1(n−1)+βM0u(n))
xi(n+1)=sin(αxi−1(n)+βMiu(n)) i=1,..,N−1
(2)
The interconnectivity matrix W is now reduced to a sin-
gle feedback parameter α. The input term is calcu-
lated by multiplication of a NxK mask Miwith a K-
dimensional input signal u.The input strength parame-
terβtunes the reservoir dynamics .
In this work, the RC is implemented using an opti-
cal fiber loop with time delay τ, illustrated schemati-
cally in Fig. 3. In this work we use desynchnization
between the input and the reservoir, as introduced in
[49] To this end the input signal u(t) is sampled and
held for a period of time τinwhich is di fferent from the
delay loop period τ. Desynchronization is a technique
for coupling the neurons in the reservoir and create in-
teraction between them: without desynchronization, the
system would not be a network of interconnected neu-
rons, but only a set of independent variables. In the
present work the time duration of one neuron is taken
to beθ=τ/(N+1)=τin/N. It follows that θis the time
difference between τandτin. After sample and holding,
the input signal is multiplied by the mask Mi(t), a pe-
riodic signal of period τin, with each period containing
Nrandom values of duration θtaken over the interval
[−1,+1]. The masked input signal Mi(t)u(t) is then in-
jected into the reservoir, i.e. injected into the fiber spool.
The output of the reservoir y(n) is given by
y(n)=NX
i=0wixi(n) (3)
where wiare the weights of the linear readout layer.
Contrary to the traditional RNNs where all intercon-
nections between the neurons are trained, RC only trains
its output layer, making the computation faster and al-
ways convergent [28, 52]. The dataset is split in a train
set and a test set. In both train and test phase, the sys-
tem is driven by the input u(t) to excite the reservoir
Figure 3: Basic scheme of a time-multiplexed reservoir computer. The
product of input u(t) and mask Mi(t) generates the masked input that
drives the reservoir states x(t), which are then used for training and
testing.
states x(t). Furhermore, in this work we reset the reser-
voir states prior to each input video sequence, to en-
hance the performance: more considerations and quan-
titative analysis on the impact of reservoir state reset
can be found in Appendix D. The reservoir states re-
lated to the training set are then stored in the train state
matrix X(t)=(x(t1),...,x(tKtr)),X∈RNxK trwith Ktris
the size of the train dataset. The most often used train-
ing method to obtain the weights wiis regularized linear
(ridge) regression [68]:
w=(XTX+λI)−1XT˜y (4)
whereλis the regularization parameter and ˜ yis the de-
sired output.
For classification problems, such as speech recogni-
tion or HAR, the usual approach [69] is to train as many
outputs output yj(n)=wjx(n),j=1,...,C, as their are
output classes C, with wjthe output weights for class
j. If the current input u(n) belongs to class c∈C, then
yc(n) is trained to take a high value, say 1, and the other
outputs yj(n) (j,c) are trained to take a low value, say
0. Thus for the HAR task, as there are 6 classes, one
would train 6 sets of output weights to take either 0 or
1 values. During the test phase, one computes which
output, averaged over the duration of the input, has the
highest value, and the input is assigned to the corre-
sponding class, through a winner-takes-all approach.
2.2.2. Training using Timesteps Of Interest
In the present work we use a novel training and clas-
sification procedure which is inspired by Ref. [50, 60]
where a similar approach was used for static image
recognition. However, the method has never been ap-
plied to a temporal task like video stream processing.
Here the concatenated states are related to di fferent
frames of the video, rather than di fferent sections of a
static image.
4To introduce the method, recall that the input is sub-
sampled to contain 10 keyframes. That is both the input
u(n) and all of the reservoir states xi(n) have a dura-
tion of length 10: n=1,...,10. From these 10 time
steps, we select a subset which we call the Timesteps
Of Interest (TOI). For instance these could consist of
time frames 2 ,8,10. We then concatenate the reservoir
states at the time frames of interest. In the above ex-
ample this would yield a vector Xof size 3 Nequal to
X=(x(2),x(8),x(10)).
We then train as many outputs as their are classes (in
the present case 6 outputs corresponding to the 6 classes
of the HAR task)
yj=wjX (5)
where the wjare vectors of size 3 N(since 3 is the num-
ber of time frames of interest in the above example).
Thewjare trained using ridge regression, see Eq. (4),
so that if the current input u(n) belongs to class c∈C,
then yctake a high value, say 1, and the other outputs yj
(j,c) take a low value, say 0. During the test phase,
one computes all the outputs yjand assigns the input to
the class cwhose output has the highest value, through
a winner-takes-all approach. This classification proce-
dure is illustrated in Fig. 4.
A first advantage of the present method is that it in-
creases the number of output weights wj, i.e. the num-
ber of trained parameters, without increasing the size
of the reservoir. A second advantage is that the reser-
voir output now takes into account both the short term
memory present in the reservoir itself, and a long term
memory which is implemented through the concatena-
tion of the key frames of interest at selected times. Gen-
erally speaking, one of the trade-o ffs in tuning a reser-
voir’s hyperparameters involves memory and nonlinear-
ity [70]: linear memory capacity decreases for increas-
ingly nonlinear activation functions. Here, by combin-
ing short-term and long-term memories with the TOIs,
we artificially achieve a highly nonlinear system with
long memory, thus avoiding the need to compromise.
The disadvantage of the present method is that one
needs to choose the TOI. Indeed, since increasing the
number of TOI increases the complexity of the output
layer, one will try to choose the minimum number of
TOI compatible with good performance. The benefits
of this technique and the optimal number of training
TOIs are discussed further in 3.2. Overall, using the
TOI yields an increase in performance, as we demon-
strate below.2.2.3. Hyperparameters and Bayesian Optimization
The RC dynamics depend on task-dependent coe ffi-
cients called hyperparameters. The choice of these co-
efficients is a critical step in the design of a RC. In order
to maximize the RC performance, the hyperparameters
need to be tuned for every benchmark task. Our RC has
three hyperparameters that need to be optimized:
•the feedback parameter αrepresents the strength of
the recurrence of the network.
•the input strength βis the scale factor of the in-
put in Eq. 2. It defines the amplitude of the input
signal and, thus, the degree of nonlinearity of the
system response. The value of βby itself is not
meaningful, as it depends on the scale used for the
input u(n). In the table below we give the standard
deviation of the input σβMiu(n)=p
VAR(βMiu(n)).
•the ridge parameter λis the coe fficient used in
Eq. 4, it adds regularization to the training of the
linear regression model. The value of λby itself is
not meaningful, as it depends on the scale used for
the neurons xi(n). In the table below we give the
rescaled value λ/VAR(x(n)).
In the RC community, the hyperparameters are usu-
ally determined via a grid search. The grid search tests
every possible combination of hyperparameters over an
interval with a pre-defined precision. This method is
simple, but the processing time quickly explodes in the
case of slow experimental setups and large number of
hyperparameter combinations. To counterbalance the
problem of exploding processing time, we use Bayesian
optimization [71, 72] to find optimal parameters. Its
effectiveness in the case of RCs has been shown in
Ref. [73]. This method creates a surrogate model of the
classification accuracy as a function of the hyperparam-
eters using Gaussian Process regression [74]. The hy-
perparameter space is then e fficiently scanned by an ac-
quisition function by making a trade-o ffbetween the ex-
ploration and exploitation of the regions with more po-
tential for improvement (cf. Appendix E). To compare,
a grid search would need a few thousand iterations to
find the optimum hyperparameters in our setup whereas
the Bayesian optimization takes less than a hundred it-
erations to converge. In practical terms, finding opti-
mal hyperparameters for our experimental system re-
duces from 3 weeks (grid search) to roughly half a day
(Bayesian optimization).
In our simulations and experiments, given the limited
size of the dataset, the hyperparameters were optimised
on the whole dataset instead of using a dedicated vali-
dation set. Table 1 gives the optimum hyperparameters
5Figure 4: Illustration of the use of reservoir computer for human action recognition. First, 10 keyframes are selected from the subsampled video
sequence, then histograms of oriented gradients (HOG) are extracted from binary (black-and-white) silhouettes to generate the input features (see
Apps. A-C). After injection in the RC, each keyframe induces one state vector of the network. The state vectors at the Timesteps Of Interest are
concatenated into one vector. In the figure we illustrate this for the cases where a single TOI is used (corresponding to time n=10), two TOI are
used (corresponding to times n=2 and n=10, and three TOI are used (corresponding to times n=2,8,10. Six outputs (one for each output class)
are trained, corresponding to 6 N, 12N, or 18 Noutput weights depending on how many TOI are used. Finally the input is assigned to the output
class whose corresponding value is largest.
found for our simulations and for the experiment, for
two di fferent reservoir sizes ( N=200 and N=600).
In experiments, the attenuation in the fiber loop corre-
sponds to the hyperparameter α(cf. Sec. 2.2.4), but it
is not immediately related to the value of α. For this
reason, the optimal experimental values of αare not in-
cluded in the table. The values given in Table 1 were
used throughout this work when other parameters (such
as the number of TOI) were varied. Note that the opti-
mal rescaled values of beta and lambda are not the same
in the simulations and experiment. In the case of the
ridge parameter lambda, this is presumably because the
experiment is a ffected by noise.
Nα β σ βMuλ λ/ VAR(x)
Simulation 200 1.90 0.0004 0.0016 0.00013 0.0029
600 1.50 0.0032 0.014 0.22 4.07
Experiment 200 / / 0.078 / 68.29
600 / / 0.065 / 16.57
Table 1: Optimal hyperparameters for simulation and experiment for
different number of neurons. These values are used throughout this
work.
The rescaled values σβMuandλ/VAR(x) do not de-
pend on the scales used for the input and internal vari-
ables. For the experimental implementation, we do not
give the values of α,β,λas these depend on the scales
used.
2.2.4. Experimental Reservoir Computer
Our experimental setup is shown in Fig. 5. It con-
sists of two main parts: an optoelectronic reservoir (cf.
Fig. 3) and an FPGA board (cf. [49, 55, 59]). The op-
toelectronic part starts with a superluminescent diode(Thorlabs SLD1550P-A40). The output of the diode
passes through a Mach-Zehnder intensity Modulator
(MZM) (EOSPACE AX-2X2-0MSS-12) to implement
the sinusoidal nonlinearity of Eq. 2. A 10% fraction of
the MZM output is sent to a photodiode (TTI TIA-525I)
for the neurons’ readout. The remaining 90% is sent to
an optical attenuator (JDS HA9) to tune the feedback
parameterαin Eq. 2.
The ring-topology of the reservoir is implemented us-
ing time-multiplexing: the delay that the light needs to
travel through the fiber spool is divided by the desired
number of neurons. This is done by selecting the clock
frequency (CLK) of the electronic part of the system so
that every neuron is encoded in light intensity for a du-
rationθ=τ/(N+1), as discussed in section 2.2.1.
The neurons are collected from the spool with a sec-
ond photodiode, electrically combined with the masked
input Miu(n), and sent back to the modulator after an
amplification stage of +27 dB (ZHL-32A +coaxial am-
plifier) to span the entire V πinterval of the MZM.
A power supply (Hameg HMP4040) provides the bias
voltages for the amplifier and the MZM.
The electronic part of the setup is based on a FPGA
board. Its reconfigurability allows us to flexibly try out
different experimental configurations. In our setup, the
FPGA interfaces with the reservoir using an analog-
to-digital converter (ADC) and a digital-to-analog con-
verter (DAC), and also communicates with a PC. The
ADC collects the states from the readout photodiode so
that they can be used for training and testing, whereas
the DAC sends the masked inputs Miu(n) to the reser-
voir. The FPGA-PC link is realized with a custom-
6designed PCIe interface that provides a higher band-
width than o ff-the-shelf Ethernet-based solutions. A
more detailed description of the FPGA design can be
found in Appendix F.
Figure 5: Schematic of the optoelectronic experimental setup. The
photonic part of the reservoir is shown in yellow and is composed
of an incoherent light source (LED), a Mach-Zehnder Modulator
(MZM), an optical attenuator (Att), a fiber spool, and two photodi-
odes (PD). The electronic part is shown in blue and consists of an
FPGA board connected to a PC, a clock generator (Clock /CLK) to
drive the analog-to-digital converter (ADC) and digital-to-analog con-
verter (DAC), a resistive combiner (Comb), and an amplifier (Amp).
The setup is tested with two di fferent reservoir sizes,
N=200 and N=600. To do so, we used two di ffer-
ent fiber spools of respectively 1.6 km ( τ=7.94µs) and
10 km (τ=49.27µs). For the HAR task, each input
video frame is a K-dimensional vector after preprocess-
ing. Therefore, the mask Miis a matrix of size NxK.
In this way, the input-mask matrix product reshapes an
1xK input into a 1xN vector, ready to be fed to the N-
sized reservoir.
3. Results and Discussion
Here, we present the numerical and experimental re-
sults of our system on the HAR task. We focus on how
the reservoir size, number of TOIs, and dimensionality
reduction after PCA influence the classification accu-
racy. In Sec. 3.4, our system is compared with other
published schemes. In Appendix G we show that us-
ing a reservoir significantly improves performance com-
pared to simply carrying out linear regression on the
preprocessed inputs.
3.1. Performance vs. Reservoir Size N
The reservoir size N is generally the most important
parameter to take into account in the design of a RC.
Usually, a larger network has more computational capa-
bility and memory which results in better performance.
First, we investigate the impact of N (reservoir size
/number of nodes) on the classification accuracy both,numerically and experimentally, using scenario s1 that
comprises boxing, clapping, waving, jogging, running,
and walking outdoors. (We focus only on one scenario
to reduce the overall computational time).
The network has been simulated for N =50, 100, 200,
600, and 1000. As seen in Fig. 6, the performance
sharply increases with the network size of up to 200
nodes (blue squares). Then, it continues improving. Ex-
perimentally, we use an RC with N =200 and N =600 (cf.
Sec. 2.2.4). The experimental results are depicted (red
squares) in Fig. 6, and are in a good agreement with the
numerical ones.
For this figure, we ran the RC with 5 di fferent input
masks, and used K-fold cross validation (with K =4) to
make up for the limited size of the dataset [69]. The
obtained results with their standard deviation are shown
in Fig. 6.
Figure 6: Impact of the reservoir size N on its performance stud-
ied numerically (blue) and experimentally (red) for scenario s1 (box-
ing, clapping, waving, jogging, running, and walking performed out-
doors), using 5 Timesteps of Interest. Error bars are standard deviation
as described in the main text.
Next we performed experimental studies for all sce-
narios (s1-s4) as well as the whole dataset for N=200
andN=600. Tab. 2 summarizes the results. As ex-
pected, the system performs better for a higher num-
ber of N for separate scenarios as well as for the whole
dataset. The best classification accuracy we achieved
experimentally on the complete dataset is 90.83% for
N=600. Tab. 2 also shows that the video sequences of
Scenario 2 are more di fficult to classify because they are
shot with di fferent zoom and scale variations. This trend
will be evident in the next sections.
3.2. Reservoir Computer Performance vs. Timesteps of
Interest
The number of TOIs is the second major parameter
to impact the classification performance. As we feed the
7Dataset N=200 N =600
Scenario 1 95,33% 96,67%
Scenario 2 82,67% 87,33%
Scenario 3 94,00% 95,33%
Scenario 4 91,33% 92,00%
Full KTH dataset 86,83% 90.83%
Table 2: Experimental results on individual scenarios (s1, s2, s3, s4)
and on the full database for two di fferent reservoir sizes, N =200 and
N=600, using 5 Timesteps of Interest.
reservoir with 10 keyframes for each video sequence, 10
is also the maximum number of TOIs that we can use.
To find the optimal number of TOIs, we perform numer-
ical and experimental tests. Fig. 7 shows a comparison
of simulations and experiments for N =600 performed
for scenario s1. The classification accuracy steeply in-
creases for a number of TOIs up to 4, remains stable,
and then slightly decreases for larger numbers of TOIs,
probably due to the fact that the data base is not large
enough to fully train using 10 TOIs. The best experi-
mental accuracy (96.67%) is obtained using 4 to 6 TOIs.
Figure 7: Impact of number of TOIs on performances, for N =600, on
scenario 1. Numerical results are in magenta and experimental results
in blue. The presented results are for the optimal choice of TOIs.
Apart from the number of TOIs, the question of what
TOIs to concatenate is of particular importance. For
example, when concatenating 7 TOIs, the optimal per-
formance is always obtained by including the 10thTOI.
The 1stTOI is the second most important TOI, appear-
ing in 95% of the top 20 results. TOI 9 is in third posi-
tion (80%) followed by TOIs 4, 5 and 6 (75%) and TOI
2 (65%). In other words, TOIs can be ordered depend-
ing on their contribution to the classification accuracy.
Based on these results, a good choice if we use 3 TOI is
to concatenate the last (10thor 9th), the first (1stor 2nd)
and middle (4th, 5th, or 6th) TOIs to obtain the best per-formance. It’s easy to understand why these TOIs are
the optimal ones if we think that every TOI represents
the states associated to a video frame. Since the reser-
voir computer has some memory capability, it is best
to sample the video sequences in the beginning, in the
middle, and in the end. That way, the system is fed with
a good representation of the video over its entire length.
Our experimental results, for each scenario and for
the whole database, for N =600 are shown in Fig. 8.
Also here it is clear that the optimal number of TOIs
lies between 4 and 6.
Figure 8: Impact of number of TOIs on performances, for N =600. Ex-
perimental results for the complete dataset (red) and for each individ-
ual scenario (s1-blue, s2-black, s3-green, s4-magenta). The presented
results are for the optimal choice of TOIs.
From Fig. 6 and Fig. 7 it is evident that our experi-
mental results are in excellent agreement with the nu-
merical ones. This is not only clear when taking into
account the overall accuracy, but also when considering
the classification of the individual action class: see the
confusion matrices in Fig. 9 (for N =600 and scenario 1)
which are very similar.
The enhancement of the performance given by the
concatenation of multiple TOIs comes at a price: the
training time increases with the number of TOIs. Thus,
for a network size of N =600 and the whole dataset, the
training time increases from 3.88s (using 1 TOI) to 4.2s
(using 7 TOIs) in simulations and from ∼9 minutes to
∼12 minutes in experiments. In this case, the amount
of additional training time is not too alarming ( +8.2%
for simulation and +33% for experiment) but it can eas-
ily explode in case of larger networks where the matrix
multiplications become the bottleneck of the computa-
tion speed. This aspect has to be taken into account in
future when designing new schemes. Additional con-
8Figure 9: Confusion matrix on test set for N =600, numerical (left) and experimental (right) results for scenario s1. (Note that the errors in the
confusion matrix are multiples of 4% as the test set of scenario s1 contains 150 video sequences, and each class contains 150 /6=25 video
sequences; the minimum error is therefore 1 /24=4%).
siderations on the improvement of the experiment speed
can be found in Sec. 4.
3.3. Dimensionality Reduction via Principle Compo-
nent Analysis
As mentioned in Sec. 2.1.2, the last preprocessing
step is the input dimensionality reduction using PCA.
We use it because too many features can degrade the
system performance. A degradation happens when the
neural network correlates an output class to some fea-
tures that do not relate to the class itself but to some
noise in the data, due to the finite size of the data base.
Further, reducing the input size has also the e ffect of
simplifying computations. For these reasons, we reduce
the number of features using PCA by keeping the prin-
cipal components with the largest eigenvalues, i.e. the
ones that account for the most variability in the data.
Fig. 10 shows the classification accuracy as a function
of PCA data variability. The accuracy increases steadily
up to 45% of the variability and remains stable until
around 85% when it starts to slightly decrease (prob-
ably because the database is not large enough to fully
train on all the features). Using these results, we set our
optimum point at 75% of variability and use this value
through all of this work. The variability of 75% corre-
sponds to 118 features (out of 1361).
3.4. Comparison with Literature
In table 3 we compare our work with some represen-
tative results from the vast (hundreds of publications)
literature that use the same database. We selected works
that provide information on the network size and /or the
time required for classification, but which employ dif-
ferent feature extraction and classifier methods.
Concerning accuracy, we achieved the highest re-
ported accuracy for prediction of the first (s1) sce-
nario values (96.67%) (however most publications do
not report this) and middle-range accuracy on the whole
Figure 10: Impact of the data variability kept after PCA, for N =200.
Experimental results for the complete dataset (red) and for every indi-
vidual scenario (s1-blue, s2-black, s3-green, s4-magenta)
dataset (90.83%). Reported results on the whole dataset
go from less than 80% accuracy to essentially perfect
(99,3% accuracy). Note that the prediction accuracy
of our system could be increased further by choosing
a larger reservoir size N (Sec. 3.1). This could be
achieved by increasing the operating frequency or us-
ing a longer fiber spool for the optoelectronic reservoir.
The main focus of this work was to introduce and
study a delay based RC as a classifier for HAR. There-
fore, we chose a quite simple approach, namely HOG,
for feature extraction during the data preprocessing step.
HOG was introduced in 2005 [62]. Since then, more
advanced feature extraction techniques or combinations
thereof have been published that allow for a consider-
able increase of the classification accuracy [8]. The ac-
curacy of our system could thus probably be improved
by using these more sophisticated feature extraction
methods.
Concerning complexity and processing time, our ar-
chitecture stands out as being low complexity with only
9N=600 nodes. This low complexity implies fast pro-
cessing rates. Indeed after training, our architecture pro-
cesses video frames at rates of 160 frames per second
(fps) experimentally and 143 fps numerically. It can
therefore classify 25 fps video sequences in real time.
Note that for the speed analysis we took into account
the preprocessing time which for the whole dataset ac-
counts to roughly 29 seconds. But the preprocessing is a
small fraction of the time required to run the RC on the
whole dataset at the maximum speed of 160 fps (1494
seconds), hence it is almost negligible when calculat-
ing the system processing speed. Furthermore, the pre-
processing time could be reduced by using a high-end
computer.
Because of its speed, the system could be used to
process di fferent video streams at the same time, thus,
implementing a multi-channel video processing system.
For future works, an additional speed gain of at least
one order of magnitude could be reached by attaching
an external RAM to the FPGA evaluation board. In-
deed opto-electronic reservoir computers based on de-
lay loops can be made considerably faster as demon-
strated in [52]. (However if the speed gain was greater
than two orders of magnitude the preprocessing time
could become the bottleneck of the system).
Finally, recall that other groups (except for [25] and
[18]) use a classifiers that are not explicitly suited
for temporal signals. Therefore, they have to form a
descriptor from the extracted features to classify the
videos. The search for the optimal descriptor consti-
tutes a whole area of research with several open direc-
tions. Reservoir computing as in our case does not re-
quire descriptors, thus, simplifying and speeding up the
data processing.
4. Conclusion
In the present work we applied a reservoir computer
architecture based on a ring topology with a single non-
linear node to the task of Human Action Recognition.
The task we studied, namely Human Actions Recogni-
tion using the KTH video-sequence benchmark dataset,
is considered to be highly challenging in the field of
neuromorphic computing.
We implemented this architecture both numerically,
and using an experimental system consisting in an op-
tolectronic reservoir and a field-programmable gate ar-
ray. The ring-topology reservoir uses a standard tele-
com fiber fed with a time-multiplexed and masked in-
put to create the desired number of neurons and influ-
ence the reservoir dynamics. The FPGA masks the in-
put signal and interfaces with the reservoir and a PC.The system is reconfigurable and allows to flexibly try
out di fferent experimental configurations.
We introduce a new way to use RC for classification
of time series, which we called the Timesteps Of Inter-
est. These allow to increase the size of the output layer
without increasing the size of the reservoir itself, and
to combine both the short term memory of the reservoir
with the longer term memory obtained by keeping rele-
vant TOIs. We studied the reservoir computers perfor-
mance as a function of reservoir size, number of TOIs,
number of principal components (to reduce the dimen-
sionality of the input during the preprocessing step).
Contrary to other popular video processing ap-
proaches, our system does not require a descriptor for-
mation to transform the temporal dimension into a spa-
tial dimension. This considerably reduces the complex-
ity of the preprocessing stage.
Our experiment reached 90.83% in prediction accu-
racy on the whole dataset, and similar results were ob-
tained in numerical simulations. This is in the middle
of the range of previous reported results. Our system
stands out for its low complexity (only N=600 neu-
rons) and speed (160fps). This exceeds the typical video
frame rate of 25 fps by a factor of 6, and implies that this
system could process multiple video streams in parallel.
In conclusion, the results reported here show that
reservoir computing, whether implemented numerically
or experimentally, is a highly promising approach for
video processing, particularly notable for its speed and
low complexity. Future works can focus on the process-
ing of video sequences more realistic than the bench-
mark KTH database. Further improvements in speed
and accuracy should be achievable, enabling multi-
channel real-time video processing.
Appendix A. Subsampling and Keyframes
Video sequences of the KTH database have di ffer-
ent lengths due to di fferent time duration typical of
different classes of actions. They span, on average,
from 49 frames for the shortest action (running) to 129
frames for the longest (hand waving). This di fference
in timescales can potentially confuse the classifier or
advantage certain action classes to the detriment of the
others. Further, not all the frames carry equal amounts
of information. First, most of the walking, jogging, and
running sequences contain several frames at the begin-
ning and at the end where the subject does not appear.
Second, for slow-paced motions, consecutive frames in
a 25 fps video include little variation in the subject’s
stance. Therefore, video sequences could in principle
10Publication Preprocess MethodNetwork
sizeProcessing
speedAccuracy
(scenario 1)Accuracy
(Full database)
Sharif (2017) [12] LBP+HOG +Haralick +Euclidean distance +PCA Multi-class SVM - 0.5 fps - 99.30%
Khan (2020) [13] PDaUM Deep CNN ∼195 millions - - 98.30%
Rahman (2014) [14] Background segmentation +shadow elimination Nearest Neighbor - 12 fps - 94.49%
Rathor (2022) [15] Frame /pose/joints detection +features extraction +PCA DNN ∼5000 6 fps - 93.9%
Shu (2014) [16] Feature extraction with mean firing rate SNN+SVM 24000 - 95.30% 92.30%
Jahagirdar (2018) [17] HOG +PCA KNN - - - 91.83%
Jhuang (2007) [18] hierarchy of feature detectors SVM - 0.4 fps 96% 91.60%
Ramya (2020) [19] silhouette extraction +distance & entropy features NN 4310 - - 91.4%
This work (experimental) HOG +PCA photonic RC 600 160 fps 96.67% 90.83%
Grushin et al (2013) [20] space-time interest points +HOF LSTM ∼29000 12-15 fps - 90.70%
Ji (2010) [21] CNN 3D CNN 295458 - - 90.20%
Xie (2014) [22] Star Skeleton Detector SNN 10800 - - 87.47%
This work (numerical) HOG +PCA delay RC 600 143 fps 96.67% 87.33%
Liu (2022) [23] spiking coding LSM (RC +SNN) +EA 2000 - - 86.30%
Begampure (2021) [24] resizing +gray scale +normalization CNN ∼5 millions - - 86.21%
Antonik (2019) [25] HOG +PCA photonic RC 16384 2-7 fps 91.30% -
Schuldt (2004) [9] local features +HistLF +HistSTG SVM - - - 71.83%
Table 3: Comparison with state-of-the-art literature. SVM: Support Vector Machine. CNN: Convolutional Neural Network. SNN: Spiking Neural
Network. KNN: K-Nearest Neighbors. LSTM: Long Short-Term Memory. LSM: Liquid State Machine. EA: Evolutionary Algorithm.
be subsampled without noticeable performance degra-
dation. It means only a few frames are required to guess
the correct action. It can be seen in Fig. 1 that some
action classes, such as boxing or hand waving, can be
determined correctly using a only single frame. Other
classes, such as running and jogging, are more subtle to
distinguish since the single frames are similar between
the two classes, and the relevant information is carried
by the time relationship between the frames. This idea
has been studied in detail in [75], where the authors have
shown that snippets of 5-7 frames are enough to achieve
a performance similar to the one obtainable with the en-
tire video sequence. Our approach combines subsam-
pling and selection of keyframes. First, each video se-
quence is subsampled by a factor of 3 meaning that only
one frame out of 3 is kept. Then, from the subsampled
sequence, we select 10 frames from its middle part and
save them as keyframes for the given sequence. In this
way, we achieve that (i) each video sequence has the
same number of frames and (ii) we keep the middle of
the video sequence where most of the action happens
and thus the relevant information is present, while the
beginning and the end are discarded without a loss in
performance. Sequences shorter than 30 frames – i.e.
those containing less than 10 frames after the subsam-
pling – are extended with empty frames (all black pix-
els) in the beginning.
Appendix B. Silhouette Segmentation and Center-
ing
Attention mechanisms [76, 77, 78] have recently
shown great success in natural language processing and
other fields [79, 80]. They are e ffective and e fficient
because they focus on the principal features of the in-
put data [53] instead of blending them without distinc-tion. The extraction of binary silhouettes requires the
subtraction of the background from 10 keyframes we
obtain after subsampling. Several techniques can be
found in the literature, such as e.g. Gaussian mixture
models [81], grassfire [82], the Self-Organizing Back-
ground Subtraction (SOBS) algorithm [83], or based on
the GLCM features [84]. In this work, for the sake of
simplicity, we literally removed the background image
from video frames, which is the simplest method for
background subtraction. This approach has the advan-
tage of being computationally simple and e fficient, but
can only be applied when videos are recorded with a
stable camera so that a reliable background image can
be extracted (for instance: surveillance cameras). This
is the situation of most of the KTH dataset videos, ex-
cept for the ones of scenario 2 which present scale vari-
ations. However, since every video was shot against
a uniform background, the simple background subtrac-
tion produces acceptable results even in this more com-
plicated situation.
The silhouette segmentation was implemented in
Matlab and consists of the following steps, illustrated
in Figs. B.11(a)–B.11(f):
1) From the raw keyframe (Fig. B.11(a)), the back-
ground image is selected(Fig. B.11(b)). Most of the
recordings of the walking, jogging, and running actions
start with an empty background since the subject has
not yet entered the frame. This frame is the perfect
candidate for background subtraction. As for in-place
actions (boxing, hand-waving, and hand-clapping), we
used a background image from the running sequence of
the same subject, in the same scenario.
2) Gaussian smoothing (Fig. B.11(c)) is applied to all
the keyframes: they are fully blurred using a Gaussian
filter, implemented with the imgaussfilt function in
Matlab. Blurring the images allows to reduce the noise
11Figure B.11: The Preprocessing stage consists in silhouette segmen-
tation (a)-(f), silhouette centering (g)-(h), and feature extraction via
HOG(i). After HOG, data are compressed using PCA.
and significantly improves the quality of the resulting
silhouettes [85].
3) Background subtraction (see Fig. B.11(d)) is per-
formed pixel-wise.
4) Foreground binarization (Fig. B.11(e)). The
imbinarize function with a threshold of 0.15 is ap-
plied to the resulting images to discriminate the fore-
ground and the background pixels.
5) Removal of small blobs (Fig. B.11(f)). Despite the
Gaussian smoothing, background noise such as changes
in illumination or shadows sometimes produces small
objects, or blobs, in the binarised foreground images
(notice the two small white dots in the left-hand side
of Fig. B.11(e)). To get rid of them, we use the function
bwareopen with a threshold of 10, thereby removing
any disconnected binary object of 10 pixels or less.
6) The last preprocessing step is the centering stage,
illustrated in Fig B.11(g) and B.11(h). A horizontal pro-
jection histogram of the image is traced, to locate the
silhouette horizontally; then its maximum is located (we
consider the first one in cases of multiple maxima) and
shifted to the center of the frame. Finally, the empty
space is padded with black columns, to keep the frame
size of 160x120 pixels.
Appendix C. Feature Extraction
In the ML community, numerous feature extraction
methods have been used to represent human actionsfrom the input data [8]. In general, they belong to
one of the two categories: local- or global-part based.
The methods based on local features have a long suc-
cessful history in many applications, given their robust-
ness to disturbances and noise, such as background clut-
ter and illumination changes. Some examples are the
scale-invariant feature transform (SIFT) [86], speeded-
up robust features (SURF) [87], spatio-temporal inter-
est points (STIPs) [88], dense sampling [89], and tex-
ture based such as local binary patterns [90] and GLCM
[91]. In global-based feature methods, a human shape
is derived from a silhouette, a skeleton, or from depth
information, and it is then used to capture the action.
In this work, similarly to another previous study [25],
we use Histograms of Oriented Gradients (HOG). This
method is classified as local features for which we do
not take into account the global shapes of the silhouettes
extracted from the keyframes. In other words, silhouette
extraction is here employed as an attention mechanism.
The HOG method, introduced by Dalal and Triggs [62],
is based on Scale-Invariant Features transform (SIFT)
descriptors [92]. First, horizontal and vertical gradients
are computed by filtering the image with the following
kernels [93]:
Gx=(−1,0,1),
Gy=(−1,0,1)T.(C.1)
Then, magnitude m(x,y) and orientation θ(x,y) of gra-
dients are computed for every pixel:
m(x,y)=q
D2x+D2y,
θ(x,y)=arctan( Dy/Dx).(C.2)
where DxandDyare the approximations of horizontal
and vertical gradients, respectively.
Next, the image is divided into small cells (typically
8×8 pixels), each of them is assigned a histogram of
typically 9 bins, corresponding to angles 0, 20, 40, .
. . 160, and containing the sums of magnitudes of
the gradients within the cell. This operation provides
a compact, yet truthful description of each patch of the
image: each cell is described with 9 numbers instead
of 64. The procedure is completed with block normal-
ization that allows compensation for the sensitivity of
the gradients to overall lighting. The histograms are di-
vided by their euclidean norm computed over bigger-
sized blocks. In practice, the computation of HOG
features was performed in Matlab, using the built-in
extractHOGFeatures function, individually for each
keyframe of every sequence, with a cell size of 8 ×8 and
a block size of 2 ×2. Given the frame size of 160 ×120
12pixels, the function returns 19 ×14×4×9=9576 fea-
tures per frame. Fig. 10(i) illustrates the resulting gra-
dients superimposed on top of the silhouette obtained
in Appendix B. To reduce the input dimensionality, we
first remove the zero features, i.e. the gradients equal
to zero for all frames in the database, since a large por-
tion of binarized input frames (see Fig. 10(h))) remains
empty (black). This simple operation alone reduces the
dimensionality approximately by half. In the next step,
we apply the principal component analysis (PCA) [63],
[64] based on the covariance method [65]. For it, we
tune the amount of variability we want to keep. Tab. C.4
contains the variability and the number of features gen-
erated for the four individual scenarios setting the vari-
ability to the maximum (99%) or the optimal (75%, see
section 3.3) value. The whole data preprocessing, from
the raw video frames to the PCA-compressed inputs,
takes 7.25 seconds for every scenario, and so 29 sec-
onds for the whole dataset. Additional considerations
on how the preprocessing time a ffects the overall pro-
cessing speed can be found in 3.4.
Feature set Variability Number of features
Scenario 1 99% 1361
75% 118
Scenario 2 99% 1288
75% 109
Scenario 3 99% 1437
75% 134
Scenario 4 99% 1475
75% 131
Table C.4: Features statistics generated from di fferent scenarios by ap-
plying PCA with maximum variability of 99% or using optimal vari-
ability of 75%
Appendix D. Reservoir state reset
A stream of di fferent video sequences in series can
lead to the degradation of the reservoir performance as
the states related to the initial keyframes can be influ-
enced by the last keyframes of the previous sequence.
Therefore, we investigate numerically the reset of the
states after each video sequence. In practice, the reser-
voir is driven with a null input in between the video se-
quences, for enough timesteps to ensure that the first
nodes related to a sequence are not influenced by the last
inputs of the previous sequence. The results are shown
in Tab. D.5. They indicate that the reset considerably
reduces the error. We conclude that the state reset can
lead to better classification accuracy and use it in our
experiment.Configuration Performance
N TOIs Reset No Reset
200 1 77,17% 68,83%
200 3 80,16% 76,33%
600 1 82,83% 72,00%
600 3 85,00% 81,33%
Table D.5: Impact of resetting the reservoir states at the beginning of
each input video sequence, numerical results.
Appendix E. Bayesian Optimization
Bayesian optimization uses Gaussian Process (GP)
regression [74] to build a surrogate model of the cost
function. This model can be used to search the regions
most potential for improvement in the hyperparameter
space. We implemented the Bayesian optimization on
the RC model (Eq. 1) using Matlab built-in functions.
The GP model generation was done by the function
fitrgp using a squared exponential kernel and auto-
matic optimization of the hyperparameters. We chose
expected improvement function [73] as the acquisition
function. It evaluates the expected amount of improve-
ment in the objective function ignoring values that cause
an increase in the objective. All three hyperparameters
discussed in Sec. 2.2.3 were simultaneously optimized
within certain intervals. The starting set of observations,
i.e. the initial evaluations of the system performance,
consisted of 27 samples, comprising all possible com-
binations of initial values. In our case, these values lie
at the extremes and the middle of the hyperparameters
intervals. After initial 27 observations, the optimization
process was run for 200 observations to obtain the hy-
perparameters we used for simulations and experiments
in this work.
Appendix F. Field-Programmable Gate Array
(FPGA)
A Field-Programmable Gate Array (FPGA) is an
electronic integrated circuit. Its main feature is recon-
figurability which means it is possible to reprogram the
FPGA using Hardware Description Languages (HDLs).
In our setup, we use Xilinx Virtex-7 XC7VX485T-
FF1761 chip together with the Xilinx VC707 evaluation
board. An FPGA Mezzanine Card (FMC) is used to in-
terface with the experiment: we use the 4DSP FMC 151
which contains a dual channel 14-bit ADC and a dual
channel 16-bit DAC, with respective bandwidth of 250
MHz and 800 MHz. The board is connected to a PC
through a custom PCIe link, which allows communi-
cation up to 2 GBps and single data transfer bursts up
13to 48 GB. In order not to be constrained by the lim-
ited frequencies available on-board, the clock tree is
driven externally by a Hewlett Packard 8648 A signal
generator. For our experiments, we used a frequency
of 205 MHz. The FPGA design is written in standard
IEEE 1076-1993 VHDL language [94] and compiled
with the Xilinx Vivado 2021 suite. The design is shown
in Fig. F.12: each FPGA block represents an entity of
the VHDL project. Entities are responsible for various
tasks that range from interfacing with the experiment
(FPGAtoRC ,RCtoFPGA ), implementing and accessing
memories ( MskRAM ,InpRAM ) to communicating with
the PC ( FPGAtoPCI ,PCIctrl ,PCIe interface ). All data
and commands for driving the experiment are sent from
the PC using a Matlab in-house script.
Figure F.12: FPGA schematic with the entities of the project.
Appendix G. Role of the reservoir computer in the
classification
The aim of this section is to quantify the impact of the
RC in the classification. As a benchmark for compari-
son, we evaluate the performance reached if the reser-
voir is taken out and the linear regression is performed
directly on the preprocessed inputs. To this end, we per-
formed numerical simulations on scenario 1: the results
are shown in Fig. G.13.
Figure G.13: Performance of a reservoir with N =200 nodes (pink
curve) and performance obtained by taking out the reservoir and per-
forming the linear regression directly on the pre-processed inputs
(blue curve).
We compare the performance of a reservoir with
N=200 nodes (pink curve) with the performance ob-
tained by taking out the reservoir and performing the
linear regression directly on the pre-processed inputs
(blue curve). In both cases, we use the same pre-
processed data and the same post-processing for the
concatenation of TOIs. We observe that for all the
possible combinations of TOIs, the configuration with
the reservoir outperforms the configuration without the
reservoir, with di fference in accuracy ranging from
27.3% (for 1 TOI) to 10.67% (for 9 TOIs). When con-
sidering the best overall performance, i.e. 95,33% of
accuracy with the reservoir and 84% without the reser-
voir, we observe a 11.33% di fference in accuracy. In
other words, the classification error is reduced from
16% (without reservoir) to 4.77% (with reservoir). We
can conclude that the reservoir has a major role in the
classification.
Acknowledgements
The authors would like to express their very great
appreciation to Dr. Marina Zajnulina for her valuable
and constructive suggestions during the preparation and
writing of this research work.
The authors acknowledge financial support from
the H2020 Marie Skłodowska-Curie Actions (Project
POSTDIGITAL Grant number 860830); and from the
Fonds de la Recherche Scientifique - FNRS.
References
[1] T. Moeslund, E. Granum, A survey of computer vision-based
human motion capture, Computer Vision and Image Under-
14standing 81 (2001) 231–268. doi:10.1006/cviu.2000.
0897 .
[2] P. Singh, S. Kundu, T. Adhikary, R. Sarkar, D. Bhattachar-
jee, Progress of human action recognition research in the last
ten years: A comprehensive survey, Archives of Computa-
tional Methods in Engineering 29 (11 2021). doi:10.1007/
s11831-021-09681-9 .
[3] Z. Sun, Q. Ke, H. Rahmani, M. Bennamoun, G. Wang, J. Liu,
Human action recognition from various data modalities: A re-
view, IEEE Transactions on Pattern Analysis and Machine In-
telligence (2022) 1–20 doi:10.1109/TPAMI.2022.3183112 .
[4] S. Ji, W. Xu, M. Yang, K. Yu, 3d convolutional neural networks
for human action recognition, IEEE transactions on pattern anal-
ysis and machine intelligence 35 (1) (2012) 221–231.
[5] V . Wiley, T. Lucas, Computer vision and image processing: A
paper review, International Journal of Artificial Intelligence Re-
search 2 (2018) 22. doi:10.29099/ijair.v2i1.42 .
[6] D. Wu, N. Sharma, M. Blumenstein, Recent advances in video-
based human action recognition using deep learning: A review,
in: 2017 International Joint Conference on Neural Networks
(IJCNN), 2017, pp. 2865–2872. doi:10.1109/IJCNN.2017.
7966210 .
[7] R. Poppe, A survey on vision-based human action recognition,
Image and vision computing 28 (6) (2010) 976–990.
[8] S. A. R. Abu Bakar, Advances in human action recognition -
an update survey, IET Image Processing 13 (11 2019). doi:
10.1049/iet-ipr.2019.0350 .
[9] C. Schuldt, I. Laptev, B. Caputo, Recognizing human actions:
a local svm approach, in: Proceedings of the 17th International
Conference on Pattern Recognition, 2004. ICPR 2004., V ol. 3,
2004, pp. 32–36 V ol.3. doi:10.1109/ICPR.2004.1334462 .
[10] I. Laptev, T. Lindeberg, Local descriptors for spatio-temporal
recognition, in: International Workshop on Spatial Coherence
for Visual Motion Analysis, Springer, 2004, pp. 91–103.
[11] J. M. Chaquet, E. J. Carmona, A. Fern ´andez-Caballero, A sur-
vey of video datasets for human action and activity recognition,
Computer Vision and Image Understanding 117 (6) (2013) 633–
659.
[12] M. Sharif, M. A. Khan, T. Akram, M. Y . Javed, T. Saba,
A. Rehman, A framework of human detection and action recog-
nition based on uniform segmentation and combination of eu-
clidean distance and joint entropy-based features selection,
EURASIP Journal on Image and Video Processing 2017 (1)
(2017) 1–18.
[13] M. A. Khan, Y .-D. Zhang, S. A. Khan, M. Attique, A. Rehman,
S. Seo, A resource conscious human action recognition frame-
work using 26-layered deep convolutional neural network, Mul-
timedia Tools and Applications 80 (28) (2021) 35827–35849.
[14] S. A. Rahman, I. Song, M. K. Leung, I. Lee, K. Lee, Fast action
recognition using negative space features, Expert Systems with
Applications 41 (2) (2014) 574–587.
[15] S. Rathor, N. Garg, P. Verma, S. Agrawal, Video event classifica-
tion and recognition using ai and dnn, in: International Confer-
ence on Innovative Computing and Communications, Springer,
2022, pp. 435–443.
[16] N. Shu, Q. Tang, H. Liu, A bio-inspired approach modeling
spiking neural networks of visual cortex for human action recog-
nition, in: 2014 international joint conference on neural net-
works (IJCNN), IEEE, 2014, pp. 3450–3457.
[17] A. Jahagirdar, M. Nagmode, Silhouette-based human action
recognition by embedding hog and pca features, in: Intelli-
gent Computing and Information and Communication, Springer,
2018, pp. 363–371.
[18] H. Jhuang, T. Serre, L. Wolf, T. Poggio, A biologically inspired
system for action recognition, in: 2007 IEEE 11th internationalconference on computer vision, Ieee, 2007, pp. 1–8.
[19] P. Ramya, R. Rajeswari, Human action recognition using dis-
tance transform and entropy based features, Multimedia Tools
and Applications 80 (6) (2021) 8147–8173.
[20] A. Grushin, D. D. Monner, J. A. Reggia, A. Mishra, Ro-
bust human action recognition via long short-term memory, in:
The 2013 International Joint Conference on Neural Networks
(IJCNN), IEEE, 2013, pp. 1–8.
[21] S. Ji, W. Xu, M. Yang, K. Yu, 3d convolutional neural networks
for human action recognition, IEEE transactions on pattern anal-
ysis and machine intelligence 35 (1) (2012) 221–231.
[22] X. Xie, H. Qu, G. Liu, L. Liu, Recognizing human actions by
using the evolving remote supervised method of spiking neural
networks, in: International Conference on Neural Information
Processing, Springer, 2014, pp. 366–373.
[23] C. Liu, H. Wang, N. Liu, Z. Yuan, Optimizing the neural struc-
ture and hyperparameters of liquid state machines based on
evolutionary membrane algorithm, Mathematics 10 (11) (2022)
1844.
[24] S. Begampure, P. Jadhav, Enhanced video analysis framework
for action detection using deep learning, International Journal
of Next-Generation Computing (2021) 218–228.
[25] P. Antonik, N. Marsal, D. Brunner, D. Rontani, Human ac-
tion recognition with a large-scale brain-inspired photonic com-
puter, Nature Machine Intelligence 1 (11 2019). doi:10.1038/
s42256-019-0110-8 .
[26] H. Jaeger, H. Haas, Harnessing nonlinearity: Predicting chaotic
systems and saving energy in wireless communication, Science
(New York, N.Y .) 304 (2004) 78–80. doi:10.1126/science.
1091277 .
[27] W. Maass, T. Natschl ¨ager, H. Markram, Real-time computing
without stable states: A new framework for neural computation
based on perturbations, Neural computation 14 (2002) 2531–60.
doi:10.1162/089976602760407955 .
[28] M. Luko ˇseviˇcius, H. Jaeger, Jaeger, h.: Reservoir computing ap-
proaches to recurrent neural network training. computer science
review 3, 127-149, Computer Science Review 3 (2009) 127–
149. doi:10.1016/j.cosrev.2009.03.005 .
[29] D. Verstraeten, B. Schrauwen, D. Stroobandt, Reservoir-based
techniques for speech recognition, in: The 2006 IEEE Interna-
tional Joint Conference on Neural Network Proceedings, IEEE,
2006, pp. 1050–1053.
[30] J. Pathak, B. Hunt, M. Girvan, Z. Lu, E. Ott, Model-free pre-
diction of large spatiotemporally chaotic systems from data: A
reservoir computing approach, Physical review letters 120 (2)
(2018) 024102.
[31] N. Akashi, Y . Kuniyoshi, S. Tsunegi, T. Taniguchi, M. Nishida,
R. Sakurai, Y . Wakao, K. Kawashima, K. Nakajima, A cou-
pled spintronics neuromorphic approach for high-performance
reservoir computing, Advanced Intelligent Systems (09 2022).
doi:10.1002/aisy.202200123 .
[32] J. Torrejon, M. Riou, F. A. Araujo, S. Tsunegi, G. Khalsa,
D. Querlioz, P. Bortolotti, V . Cros, K. Yakushiji, A. Fukushima,
H. Kubota, S. Yuasa, M. Stiles, J. Grollier, Neuromorphic
computing with nanoscale spintronic oscillators (547) (07
2017). doi:https://doi.org/10.1038/nature23011 .
URL https://tsapps.nist.gov/publication/get_
pdf.cfm?pub_id=922599
[33] A. Akrout, A. Bouwens, F. Duport, Q. Vinckier, M. Haelter-
man, S. Massar, Parallel photonic reservoir computing using fre-
quency multiplexing of neurons (12 2016).
[34] L. Butschek, A. Akrout, E. Dimitriadou, M. Haelterman,
S. Massar, Parallel photonic reservoir computing based on fre-
quency multiplexing of neurons (08 2020).
[35] Q. Vinckier, F. Duport, A. Smerieri, K. Vandoorne, P. Bienst-
15man, M. Haelterman, S. Massar, High performance photonic
reservoir computer based on a coherently driven passive cavity,
Optica 2 (01 2015). doi:10.1364/OPTICA.2.000438 .
[36] D. Brunner, M. Soriano, C. Mirasso, I. Fischer, Parallel photonic
information processing at gigabyte per second data rates using
transient states, Nature communications 4 (2013) 1364. doi:
10.1038/ncomms2368 .
[37] F. Duport, B. Schneider, A. Smerieri, M. Haelterman, S. Mas-
sar, All-optical reservoir computing, Optics express 20 (2012)
22783–95. doi:10.1364/OE.20.022783 .
[38] D. Bazzanella, S. Biasi, M. Mancinelli, L. Pavesi, A microring
as a reservoir computing node: Memory /nonlinear tasks and ef-
fect of input non-ideality (03 2022).
[39] M. Borghi, S. Biasi, L. Pavesi, Reservoir computing based on a
silicon microring and time multiplexing for binary and analog
operations (01 2021).
[40] J. Bueno, S. Maktoobi, L. Froehly, I. Fischer, M. Jacquot,
L. Larger, D. Brunner, Reinforcement learning in a large scale
photonic recurrent neural network (11 2017).
[41] J. Dong, M. Rafayelyan, F. Krzakala, S. Gigan, Optical reservoir
computing using multiple light scattering for chaotic systems
prediction, IEEE Journal of Selected Topics in Quantum Elec-
tronics PP (2019) 1–1. doi:10.1109/JSTQE.2019.2936281 .
[42] A. Skalli, J. Robertson, D. OWEN-NEWNS, M. Hejda,
X. Porte Parera, S. Reitzenstein, A. Hurtado, D. Brunner, Pho-
tonic neuromorphic computing usingvertical cavity semicon-
ductor lasers, Optical Materials Express 12 (04 2022). doi:
10.1364/OME.450926 .
[43] J. Vatin, D. Rontani, M. Sciamanna, Experimental reservoir
computing using vcsel polarization dynamics, Optics Express
27 (2019) 18579. doi:10.1364/OE.27.018579 .
[44] Z. Tong, R. Nakane, A. Hirose, G. Tanaka, A Simple Memris-
tive Circuit for Pattern Classification Based on Reservoir Com-
puting, International Journal of Bifurcation and Chaos 32 (Jul.
2022). doi:10.1142/S0218127422501413 .
[45] G. Tanaka, R. Nakane, T. Yamane, S. Takeda, D. Nakano,
S. Nakagawa, A. Hirose, Waveform classification by memris-
tive reservoir computing, 2017, pp. 457–465. doi:10.1007/
978-3-319-70093-9_48 .
[46] K. Vandoorne, P. Mechet, T. Van Vaerenbergh, M. Fiers,
G. Morthier, D. Verstraeten, B. Schrauwen, J. Dambre, P. Bien-
stman, Experimental demonstration of reservoir computing on a
silicon photonics chip, Nature communications 5 (2014) 3541.
doi:10.1038/ncomms4541 .
[47] K. Takano, C. Sugano, M. Inubushi, K. Yoshimura, S. Sunada,
K. Kanno, A. Uchida, Compact reservoir computing with a pho-
tonic integrated circuit., Optics express 26 22 (2018) 29424–
29439.
[48] R. Martinenghi, S. Rybalko, M. Jacquot, Y . Chembo, L. Larger,
Photonic nonlinear transient computing with multiple-delay
wavelength dynamics, Physical Review Letters 108 (2012)
244101. doi:10.1103/PhysRevLett.108.244101 .
[49] Y . Paquot, F. Duport, A. Smerieri, J. Dambre, B. Schrauwen,
M. Haelterman, S. Massar, Optoelectronic reservoir computing,
Scientific Reports 2 (11 2011). doi:10.1038/srep00287 .
[50] P. Antonik, N. Marsal, D. Rontani, Large-scale spatiotemporal
photonic reservoir computer for image classification, IEEE Jour-
nal of Selected Topics in Quantum Electronics PP (2019) 1–1.
doi:10.1109/JSTQE.2019.2924138 .
[51] F. Triefenbach, A. Jalalvand, B. Schrauwen, j.-p. Martens,
Phoneme recognition with large hierarchical reservoirs., 2010,
pp. 2307–2315.
[52] L. Larger, A. Baylon, R. Martinenghi, V . Udaltsov, Y . Chembo,
M. Jacquot, High-speed photonic reservoir computing using a
time-delay-based architecture: Million words per second clas-sification, Physical Review X 7 (02 2017). doi:10.1103/
PhysRevX.7.011015 .
[53] L. Zhang, X. Feng, K. Ye, C. Lou, X. Suo, Y . Song, X. Pang,
O. Ozolins, X. Yu, Human recognition with the optoelectronic
reservoir computing based micro-doppler radar signal process-
ing, Applied Optics 61 (06 2022). doi:10.1364/AO.462299 .
[54] The 2006 /07 forecasting competition for neural networks &
computational intelligence,.
URL http://www.neural-forecasting-competition.
com/NN3/
[55] P. Antonik, M. Haelterman, S. Massar, Brain-inspired photonic
signal processor for generating periodic patterns and emulating
chaotic systems, Physical Review Applied 7 (05 2017). doi:
10.1103/PhysRevApplied.7.054014 .
[56] T. Zheng, W. Yang, J. Sun, Z. Liu, K. Wang, X. Zou, Pro-
cessing imu action recognition based on brain-inspired comput-
ing with microfabricated mems resonators, Neuromorphic Com-
puting and Engineering 2 (2) (2022) 024004. doi:10.1088/
2634-4386/ac5ddf .
URL https://dx.doi.org/10.1088/2634-4386/ac5ddf
[57] A. Rodan, P. Tino, Minimum complexity echo state network,
IEEE Transactions on Neural Networks 22 (1) (2011) 131–144.
doi:10.1109/TNN.2010.2089641 .
[58] L. Appeltant, M. Soriano, G. Van der Sande, J. Danckaert,
S. Massar, J. Dambre, B. Schrauwen, C. Mirasso, I. Fischer, In-
formation processing using a single dynamical node as complex
system, Nature communications 2 (2011) 468. doi:10.1038/
ncomms1476 .
[59] L. Larger, M. Soriano, D. Brunner, L. Appeltant, J. Guti ´errez,
L. Pesquera, C. Mirasso, I. Fischer, Photonic information pro-
cessing beyond turing: An optoelectronic implementation of
reservoir computing, Optics express 20 (2012) 3241–9. doi:
10.1364/OE.20.003241 .
[60] N. Schaetti, M. Salomon, R. Couturier, Echo state networks-
based reservoir computing for mnist handwritten digits recogni-
tion, 2016. doi:10.1109/CSE-EUC-DCABES.2016.229 .
[61] H. Lu, Z. Deng, X. Liu, Semantic image segmentation based on
attentions to intra scales and inner channels, in: 2018 Interna-
tional Joint Conference on Neural Networks (IJCNN), 2018, pp.
1–8. doi:10.1109/IJCNN.2018.8489254 .
[62] N. Dalal, B. Triggs, Histograms of oriented gradients for hu-
man detection, in: 2005 IEEE Computer Society Conference on
Computer Vision and Pattern Recognition (CVPR’05), V ol. 1,
2005, pp. 886–893 vol. 1. doi:10.1109/CVPR.2005.177 .
[63] K. P. F.R.S., Liii. on lines and planes of closest fit to systems of
points in space, Philosophical Magazine Series 1 2 559–572.
[64] H. Hotelling, Analysis of a complex of statistical variables into
principal components., Journal of Educational Psychology 24
(1933) 498–520.
[65] L. I. Smith, A tutorial on principal component analysis, Tech.
Rep. (2002).
[66] F. Duport, A. Smerieri, A. Akrout, M. Haelterman, S. Massar,
Fully analogue photonic reservoir computer, Scientific Reports
6 (2016) 22381. doi:10.1038/srep22381 .
[67] P. Antonik, M. Hermans, M. Haelterman, S. Massar, Pattern and
frequency generation using an opto-electronic reservoir com-
puter with output feedback, in: A. Hirose, S. Ozawa, K. Doya,
K. Ikeda, M. Lee, D. Liu (Eds.), Neural Information Processing,
Springer International Publishing, Cham, 2016, pp. 318–325.
[68] A. N. Tikhonov, A. V . Goncharsky, V . V . Stepanov, A. G.
Yagola, Numerical methods for the solution of ill-posed prob-
lems, 1995.
[69] D. Verstraeten, B. Schrauwen, M. d’Haene, D. Stroobandt, An
experimental unification of reservoir computing methods, Neu-
ral networks 20 (3) (2007) 391–403.
16[70] D. Verstraeten, J. Dambre, X. Dutoit, B. Schrauwen, Memory
versus non-linearity in reservoirs, in: The 2010 international
joint conference on neural networks (IJCNN), IEEE, 2010, pp.
1–8.
[71] J. Mockus, Application of Bayesian approach to numerical
methods of global and stochastic optimization, Journal of
Global Optimization 4 (4) (1994) 347–365. doi:10.1007/
BF01099263 .
URL https://doi.org/10.1007/BF01099263
[72] E. Brochu, V . M. Cora, N. De Freitas, A tutorial on bayesian op-
timization of expensive cost functions, with application to active
user modeling and hierarchical reinforcement learning, arXiv
preprint arXiv:1012.2599 (2010).
[73] P. Antonik, N. Marsal, D. Brunner, D. Rontani, Bayesian opti-
misation of large-scale photonic reservoir computers (04 2020).
[74] R. C. E., C. K. I. Williams, Gaussian Processes for Machine
Learning, MIT press, 2006.
[75] K. Schindler, L. van Gool, Action snippets: How many frames
does human action recognition require?, in: 2008 IEEE Con-
ference on Computer Vision and Pattern Recognition, 2008, pp.
1–8. doi:10.1109/CVPR.2008.4587730 .
[76] V . Mnih, N. Heess, A. Graves, K. Kavukcuoglu, Recurrent mod-
els of visual attention, in: Proceedings of the 27th International
Conference on Neural Information Processing Systems - V ol-
ume 2, NIPS’14, MIT Press, Cambridge, MA, USA, 2014, p.
2204–2212.
[77] J. Ba, V . Mnih, K. Kavukcuoglu, Multiple object recognition
with visual attention, CoRR abs /1412.7755 (2015).
[78] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,
A. N. Gomez, L. Kaiser, I. Polosukhin, Attention is All you
Need, in: I. Guyon, U. V . Luxburg, S. Bengio, H. Wallach,
R. Fergus, S. Vishwanathan, R. Garnett (Eds.), Advances
in Neural Information Processing Systems, V ol. 30, Curran
Associates, Inc., 2017.
URL https://proceedings.neurips.cc/paper/2017/
file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf
[79] T. Xiao, Y . Xu, K. Yang, J. Zhang, Y . Peng, Z. Zhang, The appli-
cation of two-level attention models in deep convolutional neu-
ral network for fine-grained image classification, in: Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2015.
[80] M. Ren, R. S. Zemel, End-to-end instance segmentation with
recurrent attention, in: Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), 2017.
[81] K. Goyal, J. Singhai, Review of background subtraction meth-
ods using gaussian mixture model for video surveillance sys-
tems, Artificial Intelligence Review 50 (2) (2018) 241–259.
[82] G. Goudelis, A. Tefas, I. Pitas, Automated facial pose extrac-
tion from video sequences based on mutual information, IEEE
Transactions on Circuits and Systems for Video Technology
18 (3) (2008) 418–424.
[83] L. Maddalena, A. Petrosino, A self-organizing approach to
background subtraction for visual surveillance applications,
IEEE Transactions on image processing 17 (7) (2008) 1168–
1177.
[84] D. K. Vishwakarma, C. Dhiman, A unified model for human ac-
tivity recognition using spatial distribution of gradients and dif-
ference of gaussian kernel, The Visual Computer 35 (11) (2019)
1595–1613.
[85] M. R. Stevens, J. B. Pollak, S. Ralph, M. S. Snorrason, Video
surveillance at night, in: Acquisition, Tracking, and Pointing
XIX, V ol. 5810, SPIE, 2005, pp. 128–136.
[86] D. G. Lowe, Object recognition from local scale-invariant fea-
tures, in: Proceedings of the seventh IEEE international confer-
ence on computer vision, V ol. 2, Ieee, 1999, pp. 1150–1157.[87] H. Bay, T. Tuytelaars, L. V . Gool, Surf: Speeded up robust fea-
tures, in: European conference on computer vision, Springer,
2006, pp. 404–417.
[88] Y . Li, R. Xia, Q. Huang, W. Xie, X. Li, Survey of spatio-
temporal interest point detection algorithms in video, IEEE Ac-
cess 5 (2017) 10323–10331.
[89] R. Sicre, T. Gevers, Dense sampling of features for image re-
trieval, in: 2014 IEEE International Conference on Image Pro-
cessing (ICIP), IEEE, 2014, pp. 3057–3061.
[90] X. Wang, T. X. Han, S. Yan, An hog-lbp human detector with
partial occlusion handling, in: 2009 IEEE 12th international
conference on computer vision, IEEE, 2009, pp. 32–39.
[91] R. M. Haralick, K. Shanmugam, I. H. Dinstein, Textural features
for image classification, IEEE Transactions on systems, man,
and cybernetics (6) (1973) 610–621.
[92] D. G. Lowe, Distinctive image features from scale-invariant key-
points, International journal of computer vision 60 (2) (2004)
91–110.
[93] H. Bahi, Z. Mahani, A. Zatni, S. Saoud, A robust system for
printed and handwritten character recognition of images ob-
tained by camera phone, Published in WSEAS Transactions on
Signal Processing 11 (2015) 9–22.
[94] Ieee standard vhdl language reference manual, ANSI /IEEE
Std 1076-1993 (1994) 1–288 doi:10.1109/IEEESTD.1994.
121433 .
17