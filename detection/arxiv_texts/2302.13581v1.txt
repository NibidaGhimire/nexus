SALIENCY-DRIVEN HIERARCHICAL LEARNED IMAGE CODING FOR MACHINES
Kristian Fischer, Fabian Brand, Christian Blum, and Andr ´e Kaup
Multimedia Communications and Signal Processing
Friedrich-Alexander-Universit ¨at Erlangen-N ¨urnberg (FAU)Accepted for publication in 2023 International Conference on Acoustics, Speech, and Signal Processing (ICASSP). Personal use of this material is permitted.
Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or
promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in
other works.
ABSTRACT
We propose to employ a saliency-driven hierarchical neural image
compression network for a machine-to-machine communication sce-
nario following the compress-then-analyze paradigm. By that, dif-
ferent areas of the image are coded at different qualities depending
on whether salient objects are located in the corresponding area. Ar-
eas without saliency are transmitted in latent spaces of lower spatial
resolution in order to reduce the bitrate. The saliency information is
explicitly derived from the detections of an object detection network.
Furthermore, we propose to add saliency information to the train-
ing process in order to further specialize the different latent spaces.
All in all, our hierarchical model with all proposed optimizations
achieves 77.1 % bitrate savings over the latest video coding standard
VVC on the Cityscapes dataset and with Mask R-CNN as analysis
network at the decoder side. Thereby, it also outperforms traditional,
non-hierarchical compression networks.
Index Terms —Neural image compression, Hierarchical cod-
ing, Machine-to-machine communication, Coding for machines
1. INTRODUCTION
In today’s modern world of communication, the number of applica-
tions and processes where machines and devices are communicat-
ing with each other has tremendously increased. Such communica-
tion is often referred to as machine-to-machine (M2M) communica-
tion, taking mostly place in Internet of things (IoT) scenarios. From
this raise, it can be followed that suitable image and video compres-
sion schemes are necessary. This has been targeted by MPEG since
2019 [1], referring to this special type of video or image coding as
video coding for machines (VCM) .
This work focuses on improving the image coding performance
for instance segmentation networks as information sink following
the compress-then-analyze paradigm [2]. For such scenarios, pre-
vious approaches [3–5] for standard hybrid codecs mainly utilized
saliency coding by separating the image into salient and non-salient
areas. The latter ones are coded with reduced bitrate without harm-
ing the accuracy of the analysis network. More recent work [6–8]
employed neural image compression networks (NCNs) as codecs,
which allows for an end-to-end training of the whole VCM frame-
work with the analysis network as discriminator. By that, the NCNs
are adapted towards the characteristics of the analysis network and
the input data which eventually results in a superior coding perfor-
mance outperforming VVC [9] for the tasks of object detection and
segmentation as it has been shown in [7] and [8].
However, the methodology in [6, 7] has the shortcoming that
the network has to implicitly derive between salient areas where a
The authors gratefully acknowledge that this work has been funded by
the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation)
under project number 426084215.
Mask
Generation
fenc16↓
LSU 1
EncoderLSU 2
EncoderLSU 3
EncoderConv 2 ↓
Conv 2 ↓
LSU 1
DecoderLSU 3
DecoderLSU 2
DecoderTConv 2 ↑ TConv 2 ↑
TConv 2 ↑
fdec16↑
ˆxx
u1
u2
ym,1u3
ym,2 ym,3
v1v3 v2mHW
HW
W/16
H/16
CW/32
H/32
CW/64
H/64
CFig. 1 . Proposed hierarchical neural image coding framework based
on the RDONet structure [10, 11] for VCM scenarios. The input
imagexis separated into salient (green) and non-salient (red) areas
that are transmitted in different latent spaces yn. White latent ele-
ments correspond to a value of zero since these latents are masked
out. Dotted lines indicate that this information is transmitted via the
channel.#sand"sindicate a down-upscaling factor of s.Cde-
notes the number of latent space channels. Please note that xandyn
are not in scale for better visualization.
high quality is required, and non-salient areas where no potential
objects are located. Due to the limited ﬁeld of view of the NCNs,
this decision has to be drawn on a rather small amount of pixels.
Alleviation to this was made in our previous work [8], where we
introduced a latent space masking network to mask possibly non-
salient areas in the latent space in order to reduce the required bitrate.
Nevertheless, also this information was only implicitly derived from
the features of the analysis network.
To overcome these issues, we propose to employ a hierarchical
NCN, as it is depicted in Fig. 1, which utilizes multiple latent spaces
yn. These are used to compress different areas of the image with
different quality. By that, the latent spaces are specialized for either
transmitting salient or non-salient areas. In Fig. 1, this means that
areas including objects of interest, e.g. cars, are transmitted with
higher spatial resolution and thus a higher quality, whereas the non-
salient areas, e.g. trees, are transmitted with less spatial resolution
requiring less bitrate. In our approach, the saliency information is
explicitly derived from an external object detection network and used
to steer the NCN. We propose to employ our existing rate-distortion
optimization network (RDONet) [10, 11] as the core network struc-arXiv:2302.13581v1  [eess.IV]  27 Feb 2023un
Conv 192/5 /2↓
un+1ConcatConv 192/5 /1Masking mChannelTConv 192/5 /2↑
Concatvn
vn+1ynym,nˆym,nFig. 2 .n-th LSU structure with attendant convolutional layers based
on RDONet [10]. The encoder and decoder part (ref. Fig 1) are
framed in blue and purple color, respectively. Conv C=k=s denotes
a convolutional layer with Coutput channels, a kernel size of kk,
and a subsampling factor of s. TConv denotes an analogous trans-
posed convolutional layer.
ture. This NCN allows for hierarchical coding by transmitting the
image data in three latent spaces of different spatial resolutions. To
the best of our knowledge, we are the ﬁrst to propose a learned com-
pression framework exploiting VCM-based saliency information.
All in all, our paper provides the following contributions: First,
we show that the hierarchical RDONet published in [11] outperforms
the latest standard hybrid video codec VVC and a comparable NCN
architecture with only one latent space, when being trained in an end-
to-end manner similar to [6–8]. Second, we further improve the cod-
ing performance of RDONet for VCM scenarios by proposing a new
RDOnet masking criterion, which allows to explicitly add saliency
information to the coding process during the inference. Third, we
show that the overall coding performance is further improved by
adding saliency information to the training process to specialize the
different latent spaces in coding salient or non-salient areas.
2. HIERARCHICAL NEURAL IMAGE COMPRESSION
Today, end-to-end trained neural image compression networks are
mainly based on the pioneering work by Ball ´e et al. [12]. There, the
authors proposed a variational autoencoder fthat transforms the in-
put imagexby an encoder network into a latent space yof reduced
spatial dimensionality that is quantized and losslessly transmitted to
the decoder side. The corresponding decoder network reconstructs
the image from the transmitted latent space resulting in the deterio-
rated output image ^x. To train the network weights , a lossLHVS
combining the required rate Rand the distortion Dbetween the in-
putxand the output ^xis utilized
LHVS=D(x;^x) +R(fenc(xj)); (1)
wheresteers between the two competing goals of a low bitrate and
a low distortion.
In order to provide additional transmission options, RDONet [10,
11] adds extra latent spaces ynto the NCN structure proposed
in [13], which allows compressing different areas of xat differ-
ent spatial resolutions. The spatial resolution is halved with every
deeper latent space such that one element in the latent space covers
more pixels in the image domain. Thus, more bitrate can be saved
and the weights can be adapted correspondingly. Thereby, the ex-
ternal maskmsteers which image area is coded by which latentspace. Each area of xis transmitted by exactly one latent space. The
non-selected areas are zeroed-out (white latents in Fig. 1).
The RDONet coding order is that the deepest latent space y3is
coded ﬁrst and the coding process of each latent space ynis condi-
tioned on the before transmitted latent space yn+1. First, the image
xwith sizeHWis fed intofencconsisting of four convolutional
layers with a stride of two in order to reduce the spatial resolution.
This results in the intermediate latent space u1. The three latent
spacesynare generated from feeding u1into three cascaded latent
space units (LSUs) [10]. The structure of the n-th LSU is depicted in
Fig. 2. First, the incoming data unis spatially downscaled by a con-
volution with a stride of two. The resulting features are concatenated
with the output of the deeper LSU n+1vn+1and fed into a convo-
lutional layer to obtain the latent space yn. Subsequently, elements
ofynthat are not transmitted in this latent space are zeroed-out de-
pending onm. The channel to transmit the masked latents ym;nis
similar to [13]. ym;nis quantized, coded with the help of a condi-
tional hyperprior including a context model, and transmitted to the
decoder side. There, the received latent space ^ym;nis concatenated
with the result from the deeper LSU n+1vn+1, spatially upscaled by
a transposed convolution, and fed forward into the next LSU. After
the last latent space y1has been transmitted, the decoder network
fdecreconstructs the output image ^xfrom the output of LSU 1v1.
3. OPTIMIZING HIERARCHICAL NCN FRAMEWORK
FOR VCM SCENARIOS
This section discusses the adaptations that are proposed in order to
adapt RDONet for the compression of images in the VCM context.
As analysis network at the decoder side, the state-of-the-art instance
segmentation network Mask R-CNN [14] is chosen.
3.1. End-to-end Training for Analysis Network
In general, NCNs are optimized for the task of coding for the human
visual system (HVS). Originally, the weights of RDONet are thus
trained on a distortion DHVS mixing MS-SSIM and MSE [10]:
DHVS=DMSE+ 0:1DMS SSIM: (2)
To optimally adapt RDONet to the Mask R-CNN as informa-
tion sink, we end-to-end train its weights with the analysis network
as discriminator in the training loop similar to the work in [6–8].
Therefore, we substitute DHVS by the Mask R-CNN task loss
LMRCNN [14] to obtain the following VCM-optimized loss:
LVCM=LMRCNN (^x) +R(f(xj)): (3)
The analysis network weights are not adapted during this training.
3.2. VCM-Optimized Mask Generation for Inference
When applying RDOnet for the HVS, [11] showed that deriving the
masks based on the variance of each block is a decent compromise
between rate-distortion performance and runtime. Fig 3c shows such
a variance-based mask. From this we can easily follow that the gen-
erated mask is sub-optimal, since a lot of highly structured content
such as the trees or road markings would still be encoded in the ﬁrst
latent spacey1requiring a lot of bitrate, despite being not relevant
for the analysis network at the decoder side.
In order to obtain optimal masks for inference when coding for
the task of instance segmentation, we propose to apply an object de-
tection network to the input data xinspired by our previous worka) Input image with
YOLO detectionsb) Annotated ground
truth (GT) data
c) Mask generated by variance
criterion (var mask) [11]d) Mask generated by proposed
YOLO criterion (YOLO mask)
Fig. 3 . Exemplary masks mfor Cityscapes input image frank-
furt000000 001236 leftImg8bit . The used color mapping is
green!y1, yellow!y2, and red!y3.
in [5]. There, YOLO [15] is applied to the input image to derive the
salient objects, to ultimately reduce the bitrate in non-salient coding
units of VVC. This successful criterion is transferred to our RDONet
approach by transmitting all image areas that are covered by the
bounding box of a YOLO detection in y1. All remaining areas are
transmitted iny3, since [5] revealed that the best coding performance
is achieved, when the non-salient areas are transmitted at the low-
est possible quality. Thus, y2is not utilized with this mask. Since
the mask signaling to the decoder is very cheap in RDOnet, the bi-
trate overhead when keeping this second latent representation in the
framework can be neglected. The mask generated from the YOLO
detections is depicted in Fig. 3d and shows that only the areas con-
taining relevant objects such as cars and pedestrians are transmitted
with the best quality.
3.3. VCM-Adapted Training with Ground-Truth Data
With the proposed mask criterion during inference, a discrepancy
arises between the mask generation in training, i.e. variance based
as proposed in [11], and inference, i.e. VCM-optimized. Hence,
the codec cannot optimally adapt its weights to the different tasks
of delivering a high quality for salient areas transmitted by y1, and
reducing the rate in non-salient areas transmitted in y3.
To mitigate this discrepancy, we propose to utilize the ground
truth data, which is commonly available when training with the task
loss of the analysis network. Analogous to the masking criterion in
inference with YOLO detections as presented in the previous Sec-
tion 3.2, we generate a mask based on the ground truth objects. If
a pixel ofxis located inside an annotated object, the corresponding
block is coded in y1. All other objects are coded in latent space y3.
Therewith, the network learns that all information that is transmitted
iny3does not inﬂuence the Mask R-CNN task loss LMRCNN , and
thus reduces the bitrate in such regions as far as possible.
4. ANALYTICAL METHODS
4.1. Training Procedure
When training our NCN models, we selected four parameters such
that the coding results are in a comparable bitrate range as the ref-
erence VVC test model (VTM-10.0) [16] with the four quantization
parameter (QP) values of 22, 27, 32, and 37. We trained the models
on the Cityscapes training dataset cropped to 5121024 patches
and a batch size of eight. As optimizer, we used Adam with a learn-ing rate of 0:0001 . First, we trained a reference RDONet model on
LHVS as in (2) with variance masks for 1500 epochs as described
in [11]. These weights were taken as initialization to further train
the models with the proposed VCM optimizations, i.e. the training
withLVCM and the training with the GT-based masks, for another
1000 epochs. To generate the VCM-optimized masks for inference,
we trained a YOLO-v5 network [17] on the Cityscapes training data
for 600 epochs with the standard conﬁguration from [17].
4.2. Evaluation Setup
To evaluate our proposed methods, we build up a coding framework
similar to [18] in line with the common testing conditions for VCM
proposed by MPEG [19]. As dataset, we compressed the 500 uncom-
pressed Cityscapes [20] validation images. The compressed images
were taken as input for the Mask R-CNN [14] instance segmenta-
tion network with ResNet50 backbone. Its weights trained on the
Cityscapes training data were taken off the shelf from the Detectron2
library [21]. The Mask R-CNN accuracy is measured by the average
precision (AP), which is the standard metric to evaluate instance seg-
mentation networks. To alleviate class imbalances, we calculate the
weighted AP (wAP) as in [18]. The resulting rate-wAP curves are
quantiﬁed by the Bjøntegaard delta rate (BDR) metric [22], measur-
ing the bitrate savings at the same detection accuracy by coding the
data with a certain codec over an anchor codec. As a reference, we
compare our methods against the VTM-10.0 and the NCN from [8]
with a similar codec structure but only one transmitted latent space.
5. EXPERIMENTAL RESULTS
5.1. Inﬂuence of End-to-end Training with Analysis Network
Fig. 4 shows the coding efﬁciency of the tested coding methods. The
RDONet model trained for the human visual system [11] (orange)
is performing worse than VTM-10.0 in terms of wAP-rate perfor-
mance. The reference NCN [8] with one latent space trained on
LVCM (blue) outperforms the reference VTM-10.0 codec by 41.4 %
BDR savings (cf. Tab. 1). The proposed approach of coding the data
with the hierarchical RDONet structure steered with the masks de-
rived from the basic variance criterion (green) results in even better
coding performance of 52.7 %.
5.2. Inﬂuence of Advanced Mask Generation for Inference
Next, Fig. 5 compares the coding performance employing different
masking criteria to obtain mfor the inference. Here, the trained
model remains the same and only the masks are changed during the
inference. The reference case (green) is the mask generated by the
variance criterion from [11], which is not optimized for VCM sce-
narios. When deriving the mask from the YOLO detections (red)
as proposed in Sec. 3.2, the required bitrate can further be reduced
compared to the reference case while even increasing the detection
accuracy. As an oracle test, we also conducted experiments with
optimal inference masks derived from the GT data (purple) result-
ing in a slightly higher detection accuracy at the same bitrate than
the VCM-optimized masks. This is due to the fact that YOLO does
not perfectly ﬁnd all salient objects in the Cityscapes dataset. Thus,
those missed objects are transmitted with a worse quality, which ulti-
mately leads to missed detections by the Mask R-CNN that is applied
to the coded images. From this we can follow that the detection ac-
curacy of the network taken to generate the masks is vital, as missed
detections can have severe impact on the whole framework. Despite
those possible misses, the Mask R-CNN detection accuracy is still0.05 0.1 0.15 0.2 0.25 0.3 0.350.250.30.350.4
Rate in bits per pixelwAP
VTM-10.0
NCN+ LVCM [8]
RDONet+ LHVS+var mask
RDONet+ LVCM+var mask
uncompressedFig. 4 . wAP-rate curves averaged over the 500 Cityscapes validation
images. NCN denotes the reference network with only one latent
space. The dotted line shows the accuracy when applying Mask R-
CNN to uncompressed images.
Table 1 . Bjøntegaard delta values with VTM-10.0 as anchor.
Marker CodecTrain
lossTrain
maskInf.
maskBDR
wAP
VTM-10.0 - - YOLO -62.0 %
NCN [8] LVCM - - -41.4 %
RDONet LHVS var var 21.5 %
RDONet LVCM var var -52.7 %
RDONet LVCM var YOLO -66.2 %
RDONet LVCM var GT -70.0 %
RDONet LVCM GT YOLO -77.1 %
RDONet LVCM GT GT -79.5 %
higher than for VVC-coded images at all investigated bitrates. All in
all, utilizing the proposed VCM-optimized mask generation method
results in 66.2 % bitrate savings over VTM-10.0. With an optimal
mask generator, 70.0 % of bitrate could be saved.
5.3. Inﬂuence of Improved Training with GT Masks
For the previously shown results, the networks were all trained with
the variance-based masks as proposed in [11]. The dashed lines in
Fig. 5 represent the coding behavior when the models are trained
with masks derived from the GT data as proposed in Sec. 3.3. The
curves show that training the models with the VCM-optimal masks
further increases the coding efﬁciency by reducing the bitrate in non-
salient areas. In terms of BDR, the model trained on GT-based masks
and executed with the YOLO mask during inference achieves bitrate
savings of 77.1 % over VTM-10.0. By that, the proposed framework
with RDOnet achieves 15.1 percentage points more BDR savings
compared to applying the saliency-driven method proposed in [5]
with YOLO as saliency detector on VTM-10.0 (brown). Our method
also clearly outperforms the network proposed in [7] (-33.7 % BDR
over VTM-8.2), and our previous LSMnet method [8], which adds
implicit saliency information derived from the Mask R-CNN fea-
tures to the coding process (-54.3 % BDR over VTM-10.0).
5.4. Visual Results
Fig. 6 gives a visual comparison. When coding with the proposed
YOLO mask (cf. Fig. 6c), non-relevant details such as street markers
or trees are coded in y3, and thus with lower quality requiring less
rate. If the model is also trained with explicit saliency information
(cf. Fig. 6d), the quality is drastically reduced in the non-salient
areas. The relevant objects in the image are transmitted with high
quality and can still be detected by the analysis network.
These visual results also show that the high VCM coding efﬁ-
ciency comes at a price. Considering a scenario where, e.g., a human
0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.20.340.360.380.4
Rate in bits per pixelwAPVTM-10.0 + YOLO mask
RDONet+ LVCM+var mask
RDONet+ LVCM+YOLO mask
RDONet+ LVCM+GT mask
uncompressedFig. 5 . Comparison of the RDONet model trained with LVCM
depending on the used masks during inference coding the 500
Cityscapes validation images. Solid and dashed lines symbolize that
the RDONet model was trained with variance and GT-based masks,
respectively.
a) VTM-10.0 (QP=37)
@ 0.057 bits per pixelb) Train: var mask, Inf.: var mask
@ 0.078 bits per pixel
c) Train: var mask, Inf.: YOLO
mask @ 0.059 bits per pixeld) Train: GT mask, Inf.: YOLO
mask @ 0.047 bits per pixel
Fig. 6 . Visual results for coding the exemplary Cityscapes image
frankfurt 000000 001236 leftImg8bit with different RDONet mod-
els and the corresponding Mask R-CNN detections. All models were
trained withLVCM on the same value. Corresponding masks are
depicted in Fig. 3. Best to be viewed enlarged on a screen.
supervisor is supposed to comprehend the detections of the analysis
network from the transmitted image. This would be possible in the
salient areas, but not whether there might be missed objects in ar-
eas that have been classiﬁed as non-salient during mask generation
at the encoder. Therefore, the proposed method can be regarded
as an intermediate step between image and feature coding for ma-
chines. Future research might add a HVS-based regularization term
toLVCM , in order to not let the quality in non-salient regions drop
below a certain visual quality depending on the practical use case.
6. CONCLUSION
In this paper we proposed to employ a hierarchical neural image
compression network for the VCM context, which transmits the in-
formation in multiple latent spaces. To adapt this network to a cod-
ing framework with an instance segmentation network as informa-
tion sink, we end-to-end trained the NCN with the analysis network
as discriminator, proposed a VCM-optimized saliency mask gener-
ation, and also utilized a mask derived from the GT data to opti-
mally adapt the different latent spaces during training. With all our
proposed optimizations, our RDOnet model is able to save 77.1 %
of bitrate over VTM-10.0 at the same detection accuracy. Thereby,
RDOnet also clearly outperforms existing NCN approaches with one
latent space and the reference case when applying the same saliency
criterion based on YOLO to VTM-10.0.7. REFERENCES
[1] Yuan Zhang and Patrick Dong, “MPEG-M49944: Report of
the AhG on VCM,” Tech. Rep., Moving Picture Experts Group
(MPEG) of ISO/IEC JTC1/SC29/WG11, Geneva, Switzerland,
Oct. 2019.
[2] Alessandro Redondi, Luca Barofﬁo, Lucio Bianchi, Matteo
Cesana, and Marco Tagliasacchi, “Compress-then-analyze ver-
sus analyze-then-compress: What is best in visual sensor net-
works?,” IEEE Transactions on Mobile Computing , vol. 15,
no. 12, pp. 3000–3013, Dec. 2016.
[3] Leonardo Galteri, Marco Bertini, Lorenzo Seidenari, and Al-
berto Del Bimbo, “Video compression for object detection
algorithms,” in Proc. International Conference on Pattern
Recognition (ICPR) , Aug. 2018, pp. 3007–3012.
[4] Hyomin Choi and Ivan V . Baji ´c, “High efﬁciency compression
for object detection,” in Proc. IEEE International Conference
on Acoustics, Speech and Signal Processing (ICASSP) , Apr.
2018, pp. 1792–1796.
[5] Kristian Fischer, Felix Fleckenstein, Christian Herglotz, and
Andr ´e Kaup, “Saliency-driven versatile video coding for neu-
ral object detection,” in Proc. IEEE International Conference
on Acoustics, Speech and Signal Processing (ICASSP) , May
2021, pp. 1505–1509.
[6] Lahiru D. Chamain, Fabien Racap ´e, Jean B ´egaint, Akshay
Pushparaja, and Simon Feltman, “End-to-end optimized image
compression for machines, a study,” in Proc. Data Compres-
sion Conference (DCC) , Mar. 2021, pp. 163–172.
[7] Nam Le, Honglei Zhang, Francesco Cricri, Ramin Ghaznavi-
Youvalari, and Esa Rahtu, “Image coding for machines:
an end-to-end learned approach,” in Proc. IEEE Interna-
tional Conference on Acoustics, Speech and Signal Processing
(ICASSP) , June 2021, pp. 1590–1594.
[8] Kristian Fischer, Fabian Brand, and Andr ´e Kaup, “Boost-
ing neural image compression for machines using latent space
masking,” accepted for IEEE Transactions on Circuits and
Systems for Video Technology , 2022.
[9] Benjamin Bross, Ye-Kui Wang, Yan Ye, Shan Liu, Jianle Chen,
Gary J. Sullivan, and Jens-Rainer Ohm, “Overview of the ver-
satile video coding (VVC) standard and its applications,” IEEE
Transactions on Circuits and Systems for Video Technology ,
vol. 31, no. 10, pp. 3736–3764, Oct. 2021.
[10] Fabian Brand, Kristian Fischer, and Andr ´e Kaup, “Rate-
distortion optimized learning-based image compression using
an adaptive hierachical autoencoder with conditional hyper-
prior,” in Proc. IEEE/CVF Conference on Computer Vision
and Pattern Recognition Workshops (CVPRW) , June 2021, pp.
1885–1889.
[11] Fabian Brand, Kristian Fischer, Alexander Kopte, and Andr ´e
Kaup, “Learning true rate-distortion-optimization for end-to-
end image compression,” in Proc. Data Compression Confer-
ence (DCC) , Apr. 2022.
[12] Johannes Ball ´e, Valero Laparra, and Eero P. Simoncelli, “End-
to-end optimized image compression,” in Proc. International
Conference on Learning Representations (ICLR) , Apr. 2017.
[13] David Minnen, Johannes Ball ´e, and George D Toderici, “Joint
autoregressive and hierarchical priors for learned image com-
pression,” Advances in Neural Information Processing Sys-
tems, vol. 31, pp. 10771–10780, Dec. 2018.[14] Kaiming He, Georgia Gkioxari, Piotr Doll ´ar, and Ross B. Gir-
shick, “Mask R-CNN,” in Proc. IEEE International Confer-
ence on Computer Vision (ICCV) , Oct. 2017, pp. 2980–2988.
[15] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali
Farhadi, “You only look once: Uniﬁed, real-time object de-
tection,” in Proc. IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR) , June 2016, pp. 779–788.
[16] Jinale Chen, Yan Ye, and Seung Hwan Kim, “JVET-
S2002: Algorithm description for versatile video coding and
test model 10 (VTM 10),” Tech. Rep., Joint Video Explo-
ration Team (JVET) of ITU-T SG 16 WP 3 and ISO/IEC
JTC 1/SC 29/WG 11, July 2020.
[17] Glenn Jocher, “Yolov5,” https://github.com/ultralytics/yolov5,
June 2020.
[18] Kristian Fischer, Christian Herglotz, and Andr ´e Kaup, “On
intra video coding and in-loop ﬁltering for neural object de-
tection networks,” in Proc. IEEE International Conference on
Image Processing (ICIP) , Oct. 2020, pp. 1147–1151.
[19] Shan Liu, Wen Gao, Xiaozhong Xu, Sheng-Po Wang, Ching-
Chieh Lin, and Tsung-Hua Li, “MPEG-M55583: [VCM]
common test conditions, evaluation methodology and report-
ing template for VCM,” Tech. Rep., Moving Picture Experts
Group (MPEG) of ISO/IEC JTC1/SC29/WG2, Oct. 2020.
[20] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Re-
hfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke,
Stefan Roth, and Bernt Schiele, “The cityscapes dataset for
semantic urban scene understanding,” in Proc. IEEE Con-
ference on Computer Vision and Pattern Recognition (CVPR) ,
June 2016, pp. 3213–3223.
[21] Yuxin Wu, Alexander Kirillov, Francisco Massa,
Wan-Yen Lo, and Ross Girshick, “Detectron2,”
https://github.com/facebookresearch/detectron2, 2019.
[22] Gisle Bjontegaard, “Calculation of average PSNR differences
between RD-curves,” ITU-T VCEG and ISO/IEC MPEG doc-
ument VCEG-MM33 , Apr. 2001.