Integrated Information, a Complexity Measure for optimal
partitions
Otavio Citton∗†and Nestor Caticha‡
Instituto de Fisica, Universidade de Sao Paulo
Sao Paulo, SP, Brazil
April 28, 2023
Abstract
Motivated by the possible applications that a better understanding of conscious-
ness might bring, we follow Tononi’s idea and calculate analytically a complexity index
for two systems of Ising spins with parallel update dynamics, the homogeneous and
a modular inﬁnite range models. Using the information geometry formulation of in-
tegrated information theory, we calculate the geometric integrated information index,
G()for a ﬁxed partition withKcomponents and  =max G()forK= 2or
3. For systems in the deep ferromagnetic phase, the optimal partition undergoes a
transition such that the smallest (largest) component is above (resp. below) its critical
temperature. The eﬀects of partitioning are taken into account by introducing site
dilution.
Keywords: Statistical Mechanics; Complexity Index; Integrated Information; Ising
model.
1 Introduction
There is no general agreement on what is meant by the term consciousness. The
impossibility, so far, of physical explanations of the ﬁrst person perception of pain or other
formsofawareness, mayleadtoeitherabandoningthephysicaldescriptionofconsciousness
or dismissing it as either an illusion or even non-existent, a folly of subjectivism. Taking
as a given that the only certainty we can have are the inner personal experiences, Tononi’s
IntegratedInformationTheory(IIT)[1]proposestoextractaquantitativesignature ,that
would betray the existence of an inner complexity that conscious systems ought to have.
As a working hypothesis, “time is what clocks measure”, may not be fully satisfying but
permitted great progress and the reﬁnement of ideas about time. Measuring or estimating
IIT’s has the potential of showing the direction of reﬁning the question of what is it that
science can say about consciousness. It may be that “consciousness is what measures”
is just a stepping stone to better questions, experiments and predictions. Denying that
a system lacks consciousness hinges on having a deﬁnition to assess whether the system
has it or not. There is a mountain to climb: from one side, obtaining consciousness as
the emergent property of a physical system; from the opposite route, the analysis of how
∗Current address: Bernoulli Institute for Mathematics, Computer Science and Artiﬁcial Intelligence,
University of Groningen, Nijenborgh 9, 9747 AG Groningen, The Netherlands
†o.c.citton@rug.nl
‡ncaticha@usp.br
1arXiv:2304.14316v1  [cond-mat.stat-mech]  27 Apr 2023we can identify whether it is conscious from its necessarily complex intrinsic properties.
No matter how high these two routes meet, they will not convince those that seek a ﬁrst
person description of how we experience the world.
There is also no consensus on the mathematical deﬁnition of and Tononi and col-
laborators and other groups have pressed on, presenting reﬁnements and variations on
the theme. In general, IIT proposes as a measure of casual inﬂuences of the diﬀerent
parts, the constituents of the system. This idea stems from the postulated irreducibility of
consciousness into some sum over the parts. A physical system that satisﬁes these prop-
erties would be called a Physical Substrate of Consciousness (PSC). See [2] and [3] for a
description of the current state of the Desiderata of IIT, which is rather a declaration of
what elements an acceptable theory ought to present, but not yet the basis for a unique
tractable mathematical theory and hence many possibilities arise when trying to translate
this declaration of purpose into something that can be calculated theoretically, measured
experimentally and compared. The merit lies in presenting the ﬁrst attempt to quantify
and therefore enable measuring consciousness. It is certainly an unﬁnished chapter in the
history of science.
Despite being incomplete in its logical structure, which is usual in an incipient theory,
several applications, see [4] for an extensive review, point to the utility from a clinical
perspective of being able to correlate clinical characterization to numerical estimates of
complexity. For example, in [5] the Perturbational Complexity Index (PCI) related to IIT,
is shown to serve as a quantitative characterization of the neural correlates of conscious-
ness, a marker that could identify and measure the level of consciousness of comatose,
anesthetized, sleeping or fully awake subjects. A timid approach to the subject would
claim that even if has no relation to consciousness, it derives its importance from being
a measure of the complexity of the dynamics.
The information geometry based framework to study complexity measures as “dis-
tances” between probability manifolds introduced by Oizumi et al.[6] to embody IIT
ideas, is central to this paper. The “geometric integrated information” Gis the relative
entropy of a full joint probability distribution relative to the product of marginals of dis-
connected subsets for a ﬁxed partition. In all deﬁnitions of , a concept that is always
present is that it is a measure of how diﬀerent a system is to a partially disconnected
version of itself, and the information geometry notion of distance (or divergence) seems
natural to describe such a measure. Diﬀerent measures of distance can be used, such as in
[7] and [8] which use the Wasserstein distance. There are, however, technical diﬃculties in
the calculation of either orGwhich have limited such characterizations to theoretical
models which may fail to qualify as complex. The ﬁrst application to a system in the
thermodynamic limit, by Aguilera and Di Paolo [8], deals with a kinetic Ising model to
calculate a complexity index based on the Wasserstein metric, which yields results diﬀerent
from the geometric approach we follow here.
In this paper we present a calculation of the geometric complexity index based on
the ideas of Oizumi, Tsuchiya and Amari [6]. We study the two versions of the inﬁnite
range parallel update Ising model. We can calculate the geometric complexity index for a
general partition with a given number of disconnected components, then Gis deﬁned as
the maximum value, per spin, over all those partitions. We present results for partitions
intotwoorthreecomponentsforthehomogeneousinﬁniterangemodel. Wealsolookatthe
model with a modular structure with two groups of spins, in a simple attempt to make an
analogy with the separation of a brain into hemispheres. It has a natural partition into two
components which is not necessarily the partition that maximizes integrated information.
In the thermodynamic limit of both models, Gpresents phase transitions.
2A detailed comparison between some versions of was made by Mediano et al.[9].
Although they have calculated those measures for a not so complex and small system,
namely an autoregressive Gaussian model, it is still interesting to note the large variability
of behaviors. Each deﬁnition of seems to capture diﬀerent aspects of the model, and
the problem of choosing a single deﬁnition that allows us to quantify consciousness, if at
all possible, remains open. This holds another motivation for studying diﬀerent measures
for a system in the thermodynamic limit, since the results may give insights on what
it is measuring and if it has some property that would be interesting as a signature of
consciousness.
2 Geometric Integrated Information
Consider a system with Ninteracting classical Ising spins, indexed on a set Z,
jj=N. We denote by X=fxiji2gandY=fyiji2gthe variables that characterize
the system’s state at two consecutive steps of a discrete time dynamics. The spatio-
temporal interactions between its elements are described by a distribution P(X;Y )1, the
full model. Let the distribution Q(X;Y )be a member of a probability manifold M,
deﬁned by imposing some constraints, e.g. some of the interactions are disconnected. The
diﬀerence between the full model and the disconnected model can be quantiﬁed by the
Kullback-Leibler divergence [6]:
DKL[PkQ] =X
X;YP(X;Y ) logP(X;Y )
Q(X;Y ); (1)
a measure of the strength of the inﬂuence in the complete system between the disconnected
elements. Let be a partition of the set into non-overlapping subsets and denote by
I;Jcomponents of partition . A variable XI=fxiji2Igstands for the set of variables
on a given component Iof.
Integratedinformationaimstoquantifytheamountof“synergistic” inﬂuencesthewhole
system exerts on its future in excess of what the independent parts of the system do.
Synergy is an unusual word in the context of Statistical Mechanics of phase transitions,
and it probably refers to interactions between the system’s degrees of freedom and their
eﬀect on emergent or collective properties. In the information geometry framework, this
can be achieved by considering the following disconnection:
Q(YIjX) =Q(YIjXI); (2)
for every subset I2. This deﬁnes the manifold
MG:=n
Q(X;Y )Q(YIjX) =Q(YIjXI);8I2o
; (3)
where the disconnected system lives.
The geometric integrated information [6] associated to a particular partition is then
deﬁned as
G() = min
Q2MGX
X;YP(X;Y ) logP(X;Y )
Q(X;Y ): (4)
This is a complexity index relative to the particular partition under consideration.
We start by showing a speciﬁc model calculation with the simplest partition, a bipar-
tition  =fI;Jgwith non-overlapping components IandJ, the complement of Iin
1Conditional information about the details of the model is not shown for notational simplicity.
3. We deﬁneMIJthe probability manifold whose elements satisfy the integrated infor-
mation disconnection constraint (2) for this bipartition. Every distribution QIJ2MIJ
must decompose as follows:
QIJ(X;Y ) =QIJ(X)QIJ(YIjXI)QIJ(YJjXJYI): (5)
The minimum of the KL divergence subject to such constraints is obtained ﬁrst, by sup-
posing it exits and then, the minimization of the Lagrangian
L=DKL[PkQIJ] + X
XQIJ(X) 1!
+X
XI(XI)0
@X
YIQIJ(YIjXI) 11
A+
+X
XJ;YI(XJ;YI)0
@X
YJQIJ(YJjXJYI) 11
A;(6)
is solved by
Q
IJ(X) =P(X); (7)
Q
IJ(YIjXI) =P(YIjXI); (8)
Q
IJ(YJjXJ;YI) =P(YJjXJ;YI): (9)
The integrated information G()for a bipartition  =fI;Jgis:
IJ=X
X;YP(X;Y ) logP(YjX)P(YIjXJ)
P(YjXJ)P(YIjXI); (10)
and the geometric integrated information per spin
G= max
lim
N!11
NIJ: (11)
3 Long range kinetic Ising Model
To proceed we choose a particular version of the inﬁnite range Ising model. It is
formulated with an intrinsic discrete dynamics and deﬁned through the interaction at
two consecutive times, hence it ﬁts perfectly the information geometry description of the
integrated information index.
The Hamiltonian is
H(X;Y ) = X
i;j2Jijxjyi; (12)
and on the assumption that it is conserved, maximum entropy leads to the distribution
P(X;YjJ) =1
Zexp8
<
:X
i;jJijxjyi9
=
;; (13)
whereis an inverse temperature for the system and Z, the partition function, ensures
normalization. At this point the interaction Jijis simplyJ0=N, for the fully connected
system, but may take other values in order to implement a disconnection.
4From the product rule, the transition probability distribution is
P(YjX;J) =P(X;YjJ)
P(XjJ)=P(X;YjJ)P
YP(X;YjJ)=expn
P
i;jJijxjyio
Q
i2 cosh
P
jJijxj;(14)
a product of the transition distributions for each element,
P(YjX;J) =NY
i=1expfhiyig
2 cosh (hi)=NY
i=1P(yijX;J); (15)
where the local ﬁeld on site iishi=PN
j=1Jijxj.
We only consider the case with zero external ﬁeld, since h6= 0only diminishes com-
plexity. The partition function is
Z(;J 0;N) =X
X;Yexp8
<
:J0
NX
ijxiyj9
=
;: (16)
Introducing a  
~m 1
NP
ixi
 
~n 1
NP
iyi
into the partition function:
Z(;J 0;N) =Z
d~md^m
2Z
d~nd^n
2eJ0N~m~nei^m~mei^n~neNlogZ: (17)
Wehaveintroducedthesinglesitepartitionfunction Z=P
x;ye H, thecanonicalpartition
function associated with the Hamiltonian H= i
N( ^mx+ ^ny). The partition function can
be written as
Z(;J 0;N) =Z
e NF( ~m;~n;;J0)d~md~n (18)
F(;J 0) = lim
N!11
NlogZ(;J 0;N): (19)
F(;J 0)is the free energy of the system per spin and we call F( ~m;~n;;J 0)the free energy
functional, with some abuse of language. Then
F(;J 0) =min
~m;~nF( ~m;~n;;J 0) (20)
(m(;J 0;);n(;J 0)) =argmin
~m;~nF( ~m;~n;;J 0): (21)
For largeNthe saddle point integration leads to
Z(;J 0;N) =eJ0Nmnei^mmei^nneNlogZ(22)
where the values of ~m;~nand ^m;^nwere substituted by the saddle point equation solutions.
By imposing that the derivative of the exponent with respect to each one of them is equal
to zero, we ﬁnd:
^m=iNJ 0n; (23)
^n=iNJ 0m; (24)
thesaddlepointvaluesof ^mand^ndependonthevaluesof nandmrespectively. Evaluating
the^m;^nintegrals by constant phase integration:
H= (J0n)x+ (J0m)y; (25)
Z= exp (log 2 cosh( J0n) + log 2 cosh( J0m)): (26)
5The saddle point equations for the expected values are simply
m= tanh (J0n); (27)
n= tanh (J0m); (28)
and their stable solutions yield m=n. The free energy functional per spin is
F( ~m;~n;;J 0) =J0~m~n 1
log 4 cosh(J0~n) cosh(J0~m): (29)
When the order parameters are the solutions of the saddle point equations, the free
energy functional is at is minimum and yields the thermodynamic free energy
F() =J0m2 2
log(2 cosh(J0m)): (30)
TheentropyofthesystemcanbecalculateddirectlyfromtheShannonexpressionevaluated
at the saddle point solutions, or obtained using the thermodynamic relation, taking into
account that m() = tanh (J0m())
S[P(X;Y )] = N@F
@T(31)
= 2N 
 J0m()2+ log 2 cosh( J0m())
; (32)
For 1~m1, deﬁne the function
S( ~m;;J 0;N) = 2N 
 J0~m2+ log 2 cosh( J0~m)
; (33)
such that when ~m=m(), it gives the value of the entropy of the full model S(;J 0;N) :=
S[P(X;Y )], but it is deﬁned for any value of ~m.
A similar calculation leads to the conditional entropy in equilibrium
1
NS[P(YjX)] = J0m2+1
log 2 cosh(J0m): (34)
From equations (32) and (34) we see that the entropy of the joint distribution is simply
twice the entropy of the conditional on the past distribution. This is interesting since
S[P(X;Y )] =S[P(YjX)] +S[P(X)], it follows that S[P(X)] =S[P(YjX)]and since the
Hamiltonian is symmetric in XandY, usingS[P(X;Y )] =S[P(XjY)] +S[P(Y)], we
conclude that in equilibrium S[P(YjX)] =S[P(Y)], which means that, for calculating the
entropy, the past is not informative about the future. Of course, the past is informative
about the future before reaching equilibrium.
4 Implementing disconnections by Site Dilution
To evaluate IJ, expression (10), we need the disconnected transition probabilities
distributions. We deal with this problem by introducing a method to calculate them:
disconnect by site dilution. Given a bipartition of into subsets I;J, introduce the set of
dilution variables =figi2, withi= 1ifi2Iand0ifi2J. There is a one to one
correspondence between bipartitions and conﬁgurations, so =IJ. The interaction
matrix of the disconnected system is obtained with the auxiliary variables. For example,
for an interaction matrix Jiji(1 j), only when i2Iandj2J,iandjinteract.
6The transition probabilities densities needed in equation (10) are:
P(YjX;J;) =expn
P
i;jJijxjyio
Q
i2 cosh
P
jJijxj; (35)
P(YIjXI;J;) =expn
P
i;jJijijxjyio
Q
i2 cosh
P
jJijijxj; (36)
P(YIjXJ;J;) =expn
P
i;jJiji(1 j)xjyio
Q
i2 cosh
P
jJiji(1 j)xj; (37)
P(YjXJ;J;) =expn
P
i;jJij(1 j)xjyio
Q
i2 cosh
P
jJij(1 j)xj: (38)
Our expression for can be shown by explicit calculation to equal a diﬀerence of condi-
tional entropies
=S[P(YIjXI)] +S[P(YJjXJ)] S[P(YjX)]: (39)
This means that, for this type of simple model, the geometric Gis equal to the
stochastic interaction complexity (see [6] and [10]), and can be interpreted as the amount
of information gained when the components IandJare allowed to communicate with each
other, in order to predict the future state. A word of warning about notation: S[P(YIjXI)]
is not the entropy of an Ising model deﬁned on component I, but rather of the
distribution in expression (36), which is deﬁned in the whole .
Withmthe magnetization of the full system and =P
i2i=N, the fraction of spins
in setI, the conditional entropies in equilibrium are
S[P(YIjXI)] = J02Nm2+Nlog 2 cosh(J0m); (40)
S[P(YJjXJ)] = J0(1 )2Nm2+N(1 ) log 2 cosh( J0(1 )m;)(41)
S[P(YjX)] = J0Nm2+Nlog 2 cosh(J0m); (42)
1
N= 2J0(1 )m2+ log(cosh(J0m))(cosh1 (J0(1 )m))
cosh(J0m):
It is important to stress that the value of mis the magnetization of the Nunits full model
with interaction J0=Nand entropy given by S(;J0;N). Hence, using deﬁnition (33) we
can write
=
2S(m; ;J0;N) +1 
2S(m;(1 );J0;N) 1
2S(;J0;N):(43)
Duetothelackofspatialgeometryoftheoriginalspinsystem, theinﬂuenceofthepartition
is only through its size and not on the particular sets (I;J). For a general partition into
Knon overlapping subsets of fractional sizes fag, withPK
a=1a= 1
=1
2KX
a=1aS(m;a;J0;N) 1
2S(;J0;N): (44)
70.0
0.2
0.4
0.6
0.8
1.0
0
1
2
3
4
5
6
7G
0.000.020.040.060.080.100.120.14
(a)
0.0 0.2 0.4 0.6 0.8 1.0
02468101214
G
=0
c()=1, critical  for size N
Large >1/2
Small  <1/2
 (b)
Figure 1: Fig 1a: IJ(;). The dark circles show the maximum values of IJper spin
for ﬁxed. The ﬂoor shows its projection into the (;)plane. Fig 1b: The optimal
bipartition in the (;)plane. Symmetry breaks for  > 2, with the optimal partition,
given by the inverse of B(). The small component remains in the (gray) paramagnetic
region, below c()and the large component is in a ferromagnetic state, above c().
4.1 Bipartition
In ﬁgure 1a we show IJfor a general bipartition. In the paramagnetic phase, the
index is zero, which shows that the paramagnetic phase for the full system, is similar to
that of the disconnected system. Statistically, the full model in the paramagnetic phase
behaves as if eﬀectively disconnected. As soon as  1becomes smaller than  1
c= 1, the
critical temperature of the full system measured in units of J0,IJis diﬀerent from zero
and has a single maximum, at = 1=2. However, as goes above 2, both disconnected
systems with = 1=2can present ferromagnetic order, and the value of whereIJis
maximum, bifurcates away from 1=2. This reﬂects the fact that the small component of
the partition is paramagnetic and the large component is ferromagnetic. If both were
ordered, the KL divergence to the full model would be smaller. At that temperature, both
components can’t be disordered. Hence, the disconnection into equal sets doesn’t yield the
largest integrated information loss to the full system. For a given , the bipartition into 
and1 is optimal for (see ﬁgure 1b):
=1
2+A( c)1
2; (45)
with values c= 2andA 0:400for >2andA= 0for <0. Details are shown in
[11].
4.2 Tripartition
Panel 2 shows when has three components: IJK. Call the fractional sizes a,
and choose their labels such that 123, withP
ii= 1. For a ﬁxed a1
3, we
calculate the value of bin the interval [0;1 a]that maximizes given by equation 44.
Then1=max(a;b),2=max(min(a;b);1 a b)and3= 1 1 2. Again,
81,2,3
0.000
0.111
0.222
0.333
0.444
0.556
0.667
0.778
0.889
1.000
0
2
4
6
8
10
12
14IJK
0.000.050.100.150.200.250.300.35
1
2
3
(a)
0.0 0.2 0.4 0.6 0.8 1.0
02468101214
Large 1>1/3
Small 2,3<1/3
c()=1, critical  for size N
 (b)
Figure 2: Fig 2a: IJK(;) =max2IJK(1;2;3;)for a partition into three subsets.
The ﬂoor shows, as a function of the values of 1 3that yield the maximum value.
Before the bifurcation at = 3, alli= 1=3. For larger , one subset increases ( 1) and
the other two decrease, but remain equal. Fig 2b: The optimal tripartition in the (;)
plane. There is a discontinuous partially symmetry breaking transition at  > 3. The
small components, with fractions less than 1=3, remain in the (gray) paramagnetic region,
belowc()and the large component is in a ferromagnetic state, above c().
in the paramagnetic region IJK = 0. For 13, the system is equi-partitioned
1=2=3= 1=3. For > 3, a component with 11=3can be ordered, leading to
a symmetry breaking transition with 2=3= (1 1)=2<1=3. The symmetry is only
partially broken since there is still a remnant symmetry, with the two small components
equal in size.
Partitioning the system into three subsets allows for a larger than for partitions
into two subsets. This is to be expected, since the KL divergences from a ferromagnetic
state to a partitioned system increases with the size of the paramagnetic component of
the partitioned system. It is expected, then that in general Ggrows with the number of
components of .
5 Modular Ising model
Inthepreviousexamplesthereisnonaturalpartitioningofthesystemintocomponents.
It is possible that this index of complexity ﬁnds its utility when applied to neural systems
which present a natural partition. For example, see ﬁgure 3, a system might be naturally
divided into broad groups such as the hemispheres of a brain, or into a larger set of
components, such as Brodmann areas. Of course, we are not ready to work with realistic
models of a brain. We thus study the parallel updating version of a modular Ising model,
similar to one studied by [12] in the context of super-paramagnetic clustering.
The system has NIsing units, and again, the state of the system is given at time t 1
byX=fxigN
i=1, and byY=fyigN
i=1att. The Hamiltonian is:
HM(X;Y ) = J1
NMX
i;j=1xjyi J2
NNX
i;j=M+1xjyi J21
NNX
i=M+1MX
j=1xjyi J12
NMX
i=1NX
j=M+1xjyi:(46)
911
IJ
2I2J
∧∧
∧∧Figure 3: A system with a natural separation into modules 1and2can be divided, for
calculation of IJinto arbitrary subsets IandJ, giving rise to four types of units.
The ﬁrstMunits belong to group 1. They interact with their future through J1=Nand
with thefuture ofthe other N Munits, belonging to group 2, throughJ21=N. TheN M
elements interact with their future through J2=Nand with group 1units future through
J12=N. The equilibrium is described by a Gibbs distribution. Call ~1=M=N; ~2= 1 ~1,
m= m1
m2
,n= n1
n2
and
J=J1J12
J21J2
: (47)
Using the same methods as in the previous section, we obtain for the free energy functional:
FM(m;n;;J;~) =nTJm 1
X
i=1;2~ilog 4 cosh
 
nTJ
i
cosh [(Jm)i];(48)
and the free energy
FM(;J;~) = lim
N!11
NlogZ; (49)
and the saddle point equations, for i= 1;2:
ni= ~itanh [(Jm)i] (50)
mi= ~itanh
(nTJ)i
(51)
(52)
Again we obtain the entropy function from the derivative of the free-energy
SM(m;n;;J;~) = nTJm+X
i=1;2~ilog 4 cosh
 
nTJ
i
cosh [(Jm)i]:(53)
10While the modular Ising has a natural partition into groups 1and2, the partition con-
sidered for the integrated information can, in principle, be diﬀerent. We consider the
partitionsIandJof sizeNand(1 )Nrespectively. Introduce the mixing parame-
ters:, the fraction of elements of module 1in partition Iand= ~
1 ~, the fraction of
elements of module 2in partition I. When= ~= 1=2, thenaturalpartition occurs for
= 1 and= 0or1.
This divides the units into four groups, of sizes:
j1^Ij j2^Ij
j1^Jj j2^Jj
=~ (1 ~)
(1 )~(1 )(1 ~)
: (54)
The expression for , as shown in [11] is
=N= 
nT
JJmI+nT
IJmJ
+
 ~log 2 cosh [(Jm)1] (1 ~) log 2 cosh [ (Jm)2] +
+~log 2 cosh [(JmI)1] + (1 ~) log 2 cosh [ (JmI)2] +
+ (1 )~log 2 cosh [(JmJ)1] + (1 )(1 ~) log 2 cosh [ (JmJ)2](55)
The vectors mI= m1
m2
andnI= n1
n2
represent the partial magnetizations for the
fraction of the system that belongs to the component Iand, analogously, mJandnJare
the partial magnetizations for the elements that belong to the component J, such that
m=mI+mJandn=nI+nJ. The measure of complexity is not symmetric in
the present and past variables, but with the approach to equilibrium their expected values
become the same.
This expression is analyzed in panel 4, which presents as a function of the mixing
parameterand the inverse temperature , for ﬁxedand~. We show results in 4a to
4e when the intra-group connections are equal J1=J2and the inter-group connection are
either both higher J12;J21>J1, both smaller J12;J21<J1, or one smaller and one higher
than the intra-group J12>J1>J21.
For large inter-group connections, the partition along the natural partition causes the
largest diﬀerence, and thus there is no mixing: = 0or1and= 1or0for~== 1=2,
see 4a. All members of one module are in the same partition. However, for ~== 1=4,
which is not shown, there is a small and a large partition, as well as a small and a large
module. Then = 0and= 1=3can’t be zero and is as small as possible. The inter-group
connections are large and the partition that maximizes separates the modules as much
as possible.
Figure 4b shows the case when the inter-group couplings take values larger and smaller
thanJ1;J2and the partitions are not equal in size, ~= ~6= 1=2. Within the ferromagnetic
region, but still low , the partition separates the modules as much as possible as in the
previous case. However, for suﬃciently high there is a discontinuous transition into a
mixing state. But when the partitions are symmetric, ~= ~= 1=2, see 4c, in addition
to the discontinuous transition to the = = 1=2state, there is a second continuous
transition into a 6= 1=2state. The bifurcated curve shows the evolution of the values of
and. Again the reason for the bifurcation is that all the disconnected sets would enter
the ferromagnetic phase, and the KL is reduced by preventing that, by reducing the size
of two of the four subsets.
For small inter-group connections the mixing for small  > 1goes to one half, for
lower temperatures and then it changes. As increases for the asymmetric partitions
~=6= 1=2, 4d, changes into larger mixing of the small component. However, it bifurcates
symmetrically when ~== 1=2, 4e, as in the case of the homogeneous system.
11(a)J1=J2= 1,J12=J21= 2. There is no
mixing for strong cross couplings. Similar
behavior for = ~= 1=2(shown) and for
1=4(not shown)
(b)2 =J21> J1=J2= 1> J12=:5. For
asymmetric cross couplings and = ~= 1=4
there is discontinuous transitions
(c)2 =J21> J1=J2= 1> J12=:5. For
asymmetric cross couplings and = ~= 1=2
there are two transitions.
(d)J1=J2= 1> J 12=J21= 0:4. Weak
cross couplings. Mixing symmetry breaking
has a preference, = ~= 0:45.
(e)J1=J2= 1> J 12=J21= 0:4. Weak
cross couplings. Symmetry breaking in the
mixing,= ~= 1=2.
(f) Connections that inﬂuence group 1are
small and group 2are large:J1=J12=:5
andJ2=J21= 2.~== 1=4
Figure 4: The dark circles follow as a function of , the maximum of , with respect to .
The light circles in the ﬂoor show the projection of the maxima to the (;)plane.
12Finally, 4f shows the case J12=J1< J 2=J21, where the couplings to the future
of group 1is smaller than the corresponding couplings to group 2and~== 1=4.
changesabruptlyfrom 0to1, while goesfrom 1=3to0whichisthenaturalpartition. The
transition occurs when the small partition I, containing only members of group 2becomes
ferromagnetic. Then it is more disrupting to disconnect the inter-group couplings.
6 Conclusions
The main contribution of the IIT program is laying a road map to overcoming the bar-
rier between a physical approach to a deduction of consciousness and the characterization
of the degree to which an information processing system may be deemed conscious. While
the ﬁrst seems to be currently a vaguely posed problem, the second may lead to operational
improvements of what it is meant by the many diﬀerent aspects of consciousness, and even
illuminating the ﬁrst. Sadi Carnot motivated, specially by his father’s failures to prove
that no perpetuum mobile exists, just postulated their impossibility. The rewards were
immense. Decades later, the probabilistic approach of Statistical Mechanics furnished a
rational explanation for Carnot’s leap, with the reﬁnement of questions by the introduction
of concepts like probability, entropy and later, the quantiﬁcation of information. The study
of complexity indices for diﬀerent systems may lead to better questions about conscious-
ness, and even if not, the improvement of the computational techniques and understanding
of complex systems is worthy of the eﬀorts.
Inthiswork, wehaveanalyzedacomplexityindex, inspiredbythegeometricalapproach
in integrated information theory, for physical systems, in an attempt to better understand
how it can be used to assess complexity. Our main experimental motivation was the
application by Casali et al.[5] of related indices as markers of consciousness in patients
whose brains are subject to diﬀerent clinical conditions. From a theoretical perspective, we
follow the information geometric framework developed by Oizumi et al.[6]. By restricting
the analysis to partitions into 2and3sets, we present the calculation of the geometric
integrated information index in two models. An optimal partition is one in which the
loss in information is greatest by removing the coupling between the components. A
phase transition in the size of the optimal partition was found. As the temperature is
lowered in the ferromagnetic phase (below Tc=2for bipartitions), symmetry is broken and
an asymmetric bipartion becomes optimal. Going from the paramagnetic phase to the
ferromagnetic phase increases. In the case of tripartitions, the symmetry is partially
broken and the two small sets remain of the same size.
We have addressed the question about what partition should be considered. For the
homogeneous model, it seems natural to consider the partition which at a given value of
, maximizes the KL divergence to the full system. However, interesting systems, serious
candidates that deserve to be deemed conscious, are far from homogeneous. Informa-
tion processing systems, arising from evolution, have a modular architecture of specialized
macroscopic units and may suggest a particular partition as natural. There is a high degree
of localization for diﬀerent cognitive tasks in the brain, and it seems interesting to consider
the partition of the system into something resembling the functional partition. There is
no general theory to decide in this classiﬁcation of diﬀerent areas, in part because of the
high connectivity among them. Thus, we look at the modular model which implements
the idea of functional partitioning and explore the possibility of a diﬀerent partitioning for
IIT purposes, where the similarity of the functional and the IIT partition is measured by
the mixing parameter . We deﬁned as the maximum over all partitions, while ﬁxing K
the number of components of the partition. This invites the question of about its behavior
13as a function of the number of components; the restricted scenario of K= 2;3supports
that this may be monotonic in K.
It was shown, in [6], that the manifold of interest for the calculation of G,MG,
contains a submanifold, MS, of the disconnected distributions whose components evolve
independent of each other. The complexity measure calculated using the later manifold
was called stochastic interaction, SI[10], and is always equal to the same diﬀerence of
entropies we found in equation (39). We know from the hierarchical relation MSMG,
thatGSI. For the models analyzed here we have an equality, but the necessary
conditions for this are still unclear, and an analysis of how the integrated information
index and the stochastic interactions complexity measures diﬀers from one another may
yield interesting results.
The deﬁnition of is dynamic and encompasses two consecutive states in time, so
our equilibrium analysis only captures a small part of its phase space and more will be
learned by studying the system out of equilibrium, since it is diﬃcult to make the case for
consciousness in a system in equilibrium.
Acknowledgments
We thank Leonardo S. Barbosa, Marcus V. Baldo and O. Kinouchi for discussions.
This work was partially supported by CAPES as part of project 88887.612147/2021-00.
References
[1] Giulio Tononi. An information integration theory of consciousness. BMC Neuro-
science, 5(1):42, November 2004.
[2] Leonardo S. Barbosa, William Marshall, Sabrina Streipert, Larissa Albantakis, and
Giulio Tononi. A measure for intrinsic information. Scientiﬁc Reports , 10(1), 2020.
[3] LeonardoS.Barbosa, WilliamMarshall, LarissaAlbantakis, andGiulioTononi. Mech-
anism integrated information. Entropy, 23(3), 2021.
[4] Simone Sarasso, Adenauer Girardi Casali, Silvia Casarotto, Mario Rosanova, Corrado
Sinigaglia, and Marcello Massimini. Consciousness and complexity: a consilience of
evidence. Neuroscience of Consciousness , 08 2021. niab023.
[5] Adenauer G Casali, Olivia Gosseries, Mario Rosanova, Mélanie Boly, Simone Sarasso,
Karina R Casali, Silvia Casarotto, Marie-Aurélie Bruno, Steven Laureys, Giulio
Tononi, and Marcello Massimini. A theoretically based index of consciousness inde-
pendent of sensory processing and behavior. Sci Transl Med , 5(198):198ra105, August
2013.
[6] Masafumi Oizumi, Naotsugu Tsuchiya, and Shun-ichi Amari. Uniﬁed framework for
information integration based on information geometry. Proceedings of the National
Academy of Sciences , 113(51):14817–14822, 2016.
[7] Oizumi M., Albantakis L, and Tononi G. From the phenomenology to the mechanisms
of consciousness: Integrated information theory 3.0. PLoS Computational Biology ,
pages 10(5), e1003588, 2014.
[8] Aguilera M. and E. Di Paolo. Integrated information in the thermodynamic limit.
Neural Networks , 114:136–146, 2019.
14[9] Pedro A.M. Mediano, Anil K. Seth, and Adam B. Barrett. Measuring integrated
information: Comparison of candidate measures in theory and simulation. Entropy,
21(1), 2019.
[10] SosukeIto, MasafumiOizumi, andShun-ichiAmari. Uniﬁedframeworkfortheentropy
production and the stochastic interaction based on information geometry. Phys. Rev.
Res., 2:033048, Jul 2020.
[11] See Supplemental Material at [URL will be inserted by publisher], 2023.
[12] Shai Wiseman, Marcelo Blatt, and Eytan Domany. Superparamagnetic clustering of
data.Phys. Rev. E , 57:3767–3783, Apr 1998.
15A Supplementary Material
A.1 bifurcation
Hereweshowequation45. With mgivenbym= tanhm, write 0 =1
N@
@=I1+I2+I3
with
I1= 2(1 2)m2(56)
I2= log cosh m log cosh(1 )m (57)
I3=m(tanh(m) (1 ) tanh((1 )m)) (58)
The transition occurs near = 2and=1
2, so deﬁne= 2(1 +),= (1 +)=2, so
that 1 =1 
2. Then=a+band(1 ) =a b, witha= 1 +O(1)and
b=+O()is small near the transition. Substitution leads to
I1= 4bm2: (59)
Write for short ta= tanh(am)andtb= tanh(bm)and expand tb:tanh(x) =x x3=3, and
forb6= 0
I22tabm 2m3b3
3ta(1 t2
a) +O(b4) (60)
I32m 
ta+am(1 t2
a)
b 2m3(1 t2
a)
ta amt2
a+1
3am
b3+O(b4)(61)
The maximum with :
0 =1
N@
@= 
 4m2+ 2tam+ 2m 
ta+am(1 t2
a)
b+
 2m3(1 t2
a)4
3ta amt2
a+1
3am
b3: (62)
Forb=a=
26= 0,= 2( 1=2)
=1
2+A( c)1
2 (63)
where
c=4(m ta)
m(1 t2a)= 2 (64)
A= 
1
221 4
3ta amt2a+1
3am!1
2
(65)
To prove 64 use that m= tanhmis the magnetization of the full system, and =
2(1 +) = 2a, thenta= tanh(am) = tanh(m=2)
tanh(m) =m=2ta
1 +t2a=ta+ta1 t2
a
1 +t2a(66)
4(m ta)
(1 t2a)=4ta
1 +t2a= 2m (67)
proving 64. Numerically, at = 2,A= 0:40000.
16A.2 for the modular Ising model
Here are presented the steps that lead to equation 55, for the Modular Ising Model.
Writingas a diﬀerence of conditional entropies explicitly
=X
X;YP(X;YjJ) 
NX
i=1yihijD NX
i=1log 2 cosh(hi) +NX
i=1log 2 cosh(hijS)!
;(68)
where we introduce the quantities hijSand its complementary hijD, deﬁned as
hijS=8
<
:P
jJijjxj; fori2I
P
jJij(1 j)xj;fori2J; (69)
hijD=8
<
:P
jJij(1 j)xj;fori2I
P
jJijjxj; fori2J; (70)
as the ﬁelds generated on iby the elements in the samepartition and in the diﬀerent
partition, respectively. We immediately note that the total ﬁeld is hi=hijS+hijD.
Consider the 3 averages separately and call then A,BandC, given by
A=X
X;YP(X;YjJ)NX
i=1yihijD; (71)
B= X
X;YP(X;YjJ)NX
i=1log 2 cosh(hi); (72)
C=X
X;YP(X;YjJ)NX
i=1log 2 cosh(hijS): (73)
The procedure we follow for the calculation of these averages is to consider auxiliary
distributions. The distributions we are interested are those that are parameterized in such
a way that the original distribution for the modular Ising model, can be recovered by a
particular choice of the parameters. To calculate A,BandC, consider, respectively,
PA(X;YjJ;;S;D) =1
ZAexp(X
i 
ShijS+DhijD
yi)
; (74)
PB(XjJ;;; ) =1
ZBexp8
<
:X
ilog 2 cosh0
@X
jJijxj1
A9
=
;; (75)
PC(X;YjJ;;; S) =1
ZCexp8
<
:X
i;jJijxjyi+SX
ilog 2 cosh0
@X
jJijS
ijxj1
A9
=
;:(76)
We note that by setting S=D=,= 1andS= 0they all become the equilibrium
distribution for the modular Ising model2, and the desired averages can be easily calculated
2With exception of PBwhich, for = 0, becomes the marginalized distribution P(XjJ) =P
YP(X;Y jJ), which is not a problem, since Bonly depends on the variables X.
17as derivatives of the free energies:
A=@
@DlogZA
S=D=; (77)
B= @
@logZB
=1; (78)
C=@
@SlogZC
S=0: (79)
A.2.1 logZAcalculation
ZA=X
X;Yexp8
<
:X
i;jJijxjyi 
SS
ij+DD
ij9
=
;; (80)
we introduce here the projectors S
ij=ij+(1 i)(1 j)andD
ij=i(1 j)+(1 i)j,
that take into account the cases where iandjbelongs to the same component and to
diﬀerent components, respectively.
Introducing integrals over Dirac delta distributions and using its Fourier representation
ZA=ZdmI
1d^mI
1
2=NZdmJ
1d^mJ
1
2=NZdmI
2d^mI
2
2=NZdmJ
2d^mJ
2
2=NZdnI
1d^nI
1
2=NZdnJ
1d^nJ
1
2=NZdnI
2d^nI
2
2=NZdnJ
2d^nJ
2
2=N
expMX
i=1log(1)
i+NX
i=M+1log(2)
i+SJ1N 
mI
1nI
1+mJ
1nJ
1
+DJ1N 
mI
1nJ
1+mJ
1nI
1
+
+SJ2N 
mI
2nI
2+mJ
2nJ
2
+DJ2N 
mI
2nJ
2+mJ
2nI
2
+SJ12N 
mI
2nI
1+mJ
2nJ
1
+
+DJ12N 
mI
2nJ
1+mJ
2nI
1
+SJ21N 
mI
1nI
2+mJ
1nJ
2
+DJ21N 
mI
1nJ
2+mJ
1nI
2
+
+iN 
mI
1^mI
1+mJ
1^mJ
1+mI
2^mI
2+mJ
2^mJ
2+nI
1^nI
1+nJ
1^nJ
1+nI
2^nI
2+nJ
2^nJ
2
; (81)
where
(1)
i=X
xi;yie i(^mI
1i+ ^mJ
1(1 i))xi i(^nI
1i+^nJ
1(1 i))yi; (82)
(2)
i=X
xi;yie i(^mI
2i+ ^mJ
2(1 i))xi i(^nI
2i+^nJ
2(1 i))yi: (83)
Note that the integrand is an exponential with exponent proportional to N, thus we
can use the saddle point integration method and obtain
logZA= SJ1N 
mI
1nI
1+mJ
1nJ
1
 DJ1N 
mI
1nJ
1+mJ
1nI
1
 SJ2N 
mI
2nI
2+mJ
2nJ
2
+
 DJ2N 
mI
2nJ
2+mJ
2nI
2
 SJ12N 
mI
2nI
1+mJ
2nJ
1
 DJ12N 
mI
2nJ
1+mJ
2nI
1
+
 SJ21N 
mI
1nI
2+mJ
1nJ
2
 DJ21N 
mI
1nJ
2+mJ
1nI
2
+
+~Nlog 2 cosh
J1 
SnI
1+DnJ
1
+J21 
SnI
2+DnJ
2
+
+ (1 )~Nlog 2 cosh
J1 
SnJ
1+DnI
1
+J21 
SnJ
2+DnI
2
+
+~Nlog 2 cosh
J1 
SmI
1+DmJ
1
+J12 
SmI
2+DmJ
2
+
+ (1 )~Nlog 2 cosh
J1 
SmJ
1+DmI
1
+J12 
SmJ
2+DmI
2
+
+ ( ~)Nlog 2 cosh
J2 
SnI
2+DnJ
2
+J12 
SnI
1+DnJ
1
+
+ (1  (1 )~)Nlog 2 cosh
J2 
SnJ
2+DnI
2
+J12 
SnJ
1+DnI
1
+
+ ( ~)Nlog 2 cosh
J2 
SmI
2+DmJ
2
+J21 
SmI
1+DmJ
1
+
+ (1  (1 )~)Nlog 2 cosh
J2 
SmJ
2+DmI
2
+J21 
SmJ
1+DmI
1
;(84)
18where we deﬁne as the fraction of elements of the group 1 that belongs to the partition I.
The fractional sizes for the other components can be obtained by imposing the constraint
that the fractional size of the component Imust beand the whole system must sum up
to 1.
The saddle point equations are
mI
1=~tanh
J1 
SnI
1+DnJ
1
+iJ21 
SnI
2+DnJ
2
; (85)
mJ
1= (1 )~tanh
J1 
SnJ
1+DnI
1
+iJ21 
SnJ
2+DnI
2
; (86)
mI
2= (1 ~) tanh
J2 
SnI
2+DnJ
2
+iJ12 
SnI
1+DnJ
1
; (87)
mJ
2= (1 )(1 ~) tanh
J2 
SnJ
2+DnI
2
+iJ12 
SnJ
1+DnI
1
;(88)
nI
1=~tanh
J1 
SmI
1+DmJ
1
+iJ12 
SmI
2+DmJ
2
; (89)
nJ
1= (1 )~tanh
J1 
SmJ
1+DmI
1
+iJ12 
SmJ
2+DmI
2
; (90)
nI
2= (1 ~) tanh
J2 
SmI
2+DmJ
2
+iJ21 
SmI
1+DmJ
1
; (91)
nJ
2= (1 )(1 ~) tanh
J2 
SmJ
2+DmI
2
+iJ21 
SmJ
1+DmI
1
;(92)
where we deﬁne
= ~
1 ~; (93)
as the fraction of elements of the group 2 that belongs to the component I.
Taking the derivative
@
@DlogZA=NJ1 
mI
1nJ
1+mJ
1nI
1
+NJ21 
mI
1nJ
2+mJ
1nI
2
+
+NJ12 
mI
2nJ
1+mJ
2nI
1
+NJ2 
mI
2nJ
2+mJ
2nI
2
; (94)
where we used the equations of state to substitute the tanhfor the correspondent order
parameter.
TakingS=D=we have:
mI
1=m1; nI
1=n1; (95)
mJ
1= (1 )m1; nJ
1= (1 )n1; (96)
mI
2= m2; nI
2= n2; (97)
mJ
2= (1 )m2; nJ
2= (1 )n2; (98)
(99)
wherem1,m2,n1andn2are now the order parameter of the modular Ising model.
And the value of A
A=2(1 )NJ 1m1n1+ [(1 ) + (1 )]NJ 21m1n2+
+ [(1 ) +(1 )]NJ 12m2n1+ 2(1 )NJ 2m2n2:(100)
A.2.2 logZBcalculation
ZB=X
Xexp8
<
:X
ilog 2 cosh0
@X
jJijxj1
A9
=
;: (101)
19Introducing integrals over delta distributions 
wi iP
jJijxj
and its Fourier rep-
resentation
ZB=X
XZY
idwid^wi
2exp8
<
:X
ilog 2 cosh ( iwi) +iX
i^wiwi+X
i^wiX
jJijxj9
=
;:(102)
Introducing more integrals over delta distributions and using its Fourier representation
for the order parameters:
ZB=Zdm1d^m1
2=NZdm2d^m2
2=NZdn1d^n1
2=NZdn2d^n2
2=N
exp
J1Nm 1n1+J21Nm 1n2+J12Nm 2n1+J2Nm 2n2+
+iN(m1^m1+m2^m2+n1^n1+n2^n2) + ~NlogZ1+ (1 ~)NlogZ2
;(103)
where
Z1= 2 cosh( i^m1) [2 cosh( i^u1)]; (104)
Z2= 2 cosh( i^m2) [2 cosh( i^u2)]: (105)
Using the saddle point method of integration
logZB= J1Nm 1n1 J21Nm 1n2 J12Nm 2n1 J2Nm 2n2+
+ ~Nlog 2 cosh[(J1u1+J21u2)] +~Nlog 2 cosh[(J1m1+J21m2)]+
+ (1 ~)Nlog 2 cosh[(J12u1+J2u2)] +(1 ~)Nlog 2 cosh[(J21m1+J2m2)]:
(106)
The saddle point equations are:
m1= ~tanh [(J1u1+J21u2)]; (107)
m2= (1 ~) tanh [(J12u1+J2u2)]; (108)
u1= ~tanh [(J1m1+J21m2)]; (109)
u2= (1 ~) tanh [(J21m1+J2m2)]: (110)
Taking the derivative with respect to , we obtain:
B= ~Nlog 2 cosh [(J1m1+J21m2)] (1 ~)Nlog 2 cosh [(J21m1+J2m2)]:(111)
A.2.3 logZCcalculation
ZC=X
X;Yexp8
<
:X
i;jJijxjyi+SX
ilog 2 cosh0
@X
jJijS
ijxj1
A9
=
;:(112)
Again we introduce integrals over 
wi iP
jJijS
ijxj
, and use its Fourier repre-
sentation
ZC=X
X;YZY
idwid^wi
2exp8
<
:X
i;jJijxjyi+SX
ilog 2 cosh ( iwi) +
+iX
i^wiwi+X
i^wiX
jJijS
ijxj9
=
;;(113)
20where,againweusedtheprojector S
ij=ij+(1 i)(1 j)thatselectsalltheinteractions
between the same component.
Introducing integrals over delta distribution for each parameter
ZC=Zdm1d^m1
2=NZdm2d^m2
2=NZdn1d^n1
2=NZdn2d^n2
2=NZdmI
1d^mI
1
2=NZdmJ
1d^mJ
1
2=N
ZdmI
2d^mI
2
2=NZdmJ
2d^mJ
2
2=NZduI
1d^uI
1
2=NZduJ
1d^uJ
1
2=NZduI
1d^uI
1
2=NZduJ
1d^uJ
1
2=N
exp(
J1Nm 1n1+J21Nm 1n2+J12Nm 2n1+J2Nm 2n2+J1N 
mI
1uI
1+mJ
1uJ
1
+
+J21N 
mI
1uI
2+mJ
1uJ
2
+J12N 
mI
2uI
1+mJ
2uJ
1
+J2N 
mI
2uI
2+mJ
2uJ
2
+
+iN 
m1^m1+m2^m2+n1^n1+n2^n2+mI
1^mI
1+mJ
1^mJ
1+mI
2^mI
2+mJ
2^mJ
2+
+uI
1^uI
1+uJ
1^uJ
1+uJ
2^uJ
2+uJ
2^uJ
2
+MX
i=1log(1)
i+NX
i=M+1log(2)
i)
; (114)
where
(1)
i=2 cosh ( i^n1)
2 cosh 
 i 
i^uI
1+ (1 i)^uJ
1S
2 cosh 
 i 
^m1+i^mI
1+ (1 i) ^mJ
1
; (115)
(2)
i=2 cosh ( i^n2)
2 cosh 
 i 
i^uI
2+ (1 i)^uJ
2S
2 cosh 
 i 
^m2+i^mI
2+ (1 i) ^mJ
2
: (116)
The saddle point integration method gives us
logZC=J1Nm 1n1+J21Nm 1n2+J12Nm 2n1+J2Nm 2n2+
+J1N 
mI
1uI
1+mJ
1uJ
1
+J21N 
mI
1uI
2+mJ
1uJ
2
+
+J12N 
mI
2uI
1+mJ
2uJ
1
+J2N 
mI
2uI
2+mJ
2uJ
2
+
+iN 
m1^m1+m2^m2+n1^n1+n2^n2+mI
1^mI
1+mJ
1^mJ
1+
+mI
2^mI
2+mJ
2^mJ
2+uI
1^uI
1+uJ
1^uJ
1+uJ
2^uJ
2+uJ
2^uJ
2
+
+ ~Nlog 2 cosh ( i^n1) + (1 ~)Nlog 2 cosh ( i^n2) +
+MX
i=1log 2 cosh 
 i 
^m1+i^mI
1+ (1 i) ^mJ
1
+
+SMX
i=1log 2 cosh 
 i 
i^uI
1+ (1 i)^uJ
1
+
+NX
i=M+1log 2 cosh 
 i 
^m2+i^mI
2+ (1 i) ^mJ
2
+
+SNX
i=M+1log 2 cosh 
 i 
i^uI
2+ (1 i)^uJ
2
: (117)
21With the saddle point equations
^m1=i(J1n1+J21n2); ^n1=i(J1m1+J12m2); (118)
^m2=i(J12n1+J2n2); ^n2=i(J21m1+J2m2); (119)
^mI
1=i 
J1uI
1+J21uI
2
; ^uI
1=i 
J1mI
1+J12mI
2
; (120)
^mJ
1=i 
J1uJ
1+J21uJ
2
; ^uJ
1=i 
J1mJ
1+J12mJ
2
; (121)
^mI
2=i 
J12uI
1+J2uI
2
; ^uI
2=i 
J21mI
1+J2mI
2
; (122)
^mJ
2=i 
J12uJ
1+J2uJ
2
; ^uJ
2=i 
J21mJ
1+J2mJ
2
; (123)
n1= ~tanh ( i^n1); (124)
n2= (1 ~) tanh ( i^n2); (125)
m1=MX
i=1tanh
 i 
^m1+i^mI
1+ (1 i) ^mJ
1
; (126)
mI
1=MX
i=1itanh
 i 
^m1+i^mI
1+ (1 i) ^mJ
1
; (127)
mJ
1=MX
i=1(1 i) tanh
 i 
^m1+i^mI
1+ (1 i) ^mJ
1
; (128)
m2=NX
i=M+1tanh
 i 
^m2+i^mI
2+ (1 i) ^mJ
2
; (129)
mI
2=NX
i=M+1itanh
 i 
^m2+i^mI
2+ (1 i) ^mJ
2
; (130)
mJ
2=NX
i=M+1(1 i) tanh
 i 
^m2+i^mI
2+ (1 i) ^mJ
2
; (131)
uI
1=SMX
i=1itanh
 i 
i^uI
1+ (1 i)^uJ
1
; (132)
uJ
1=SMX
i=1(1 i) tanh
 i 
i^uI
1+ (1 i)^uJ
1
; (133)
uI
2=SNX
i=M+1itanh
 i 
i^uI
2+ (1 i)^uJ
2
; (134)
uJ
2=SNX
i=M+1(1 i) tanh
 i 
i^uI
2+ (1 i)^uJ
2
: (135)
Now we are able to take the derivative of logZCwith respect to S:
@
@SlogZC=MX
i=1log 2 cosh 
 i 
i^uI
1+ (1 i)^uJ
1
+
+NX
i=M+1log 2 cosh 
 i 
i^uI
2+ (1 i)^uJ
2
: (136)
22TakingS= 0, we notice that the parameters uI
1,uJ
1,uI
2anduJ
2all are zero. Thus, in
this case, the expression for the order parameters become
n1= ~tanh ( i^n1); (137)
n2= (1 ~) tanh ( i^n2); (138)
m1= ~tanh ( i^m1); (139)
m2= (1 ~) tanh ( i^m2); (140)
mI
1=MX
i=1itanh ( i^m1); (141)
mJ
1=MX
i=1(1 i) tanh ( i^m1); (142)
mI
2=NX
i=M+1itanh ( i^m2); (143)
mJ
2=NX
i=M+1(1 i) tanh ( i^m2); (144)
(145)
and we notice that m1,m2,n1andn2are the order parameters of the full modular Ising
model, and by using a similar notation we used before for A, we can write this order
parameters in terms of same variable that denotes the fraction of ﬁrst Melements that
belongs to the component I:
mI
1=m1; (146)
mJ
1= (1 )m1; (147)
mI
2= m2; (148)
mJ
2= (1 )m2; (149)
wherem1,m2,n1andn2are the order parameters for the modular Ising model and, is,
again, given by
= ~
1 ~: (150)
Finally we have:
C=~Nlog 2 cosh [(J1m1+J12m2)] +
+ (1 )~Nlog 2 cosh [(J1(1 )m1+J12(1 )m2)] +
+ (1 ~)Nlog 2 cosh [(J21m1+J2m2)] +
+ (1 )(1 ~)Nlog 2 cosh [(J21(1 )m1+J2(1 )m2)]:(151)
Deﬁning the vectors
mI=m1
m2
;mJ=(1 )m1
(1 )m2
;nI=n1
n2
;nJ=(1 )n1
(1 )n2
;(152)
we obtain equation (55).
23