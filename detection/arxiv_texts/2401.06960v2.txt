Noname manuscript No.
(will be inserted by the editor)
Transformer for Object Re-Identification: A Survey
Mang Ye, Shuoyi Chen, Chenyue Li, Wei-Shi Zheng,
David Crandall, Bo Du
the date of receipt and acceptance should be inserted later
Abstract Object Re-identification (Re-ID) aims to
identify specific objects across different times and
scenes, which is a widely researched task in com-
puter vision. For a prolonged period, this field has
been predominantly driven by deep learning technol-
ogy based on convolutional neural networks. In re-
cent years, the emergence of Vision Transformers has
spurred a growing number of studies delving deeper
into Transformer-based Re-ID, continuously breaking
performance records and witnessing significant progress
in the Re-ID field. Offering a powerful, flexible, and
unified solution, Transformers cater to a wide array of
Re-ID tasks with unparalleled efficacy. This paper pro-
vides a comprehensive review and in-depth analysis of
the Transformer-based Re-ID. In categorizing existing
works into Image/Video-Based Re-ID, Re-ID with lim-
ited data/annotations, Cross-Modal Re-ID, and Spe-
cial Re-ID Scenarios, we thoroughly elucidate the ad-
vantages demonstrated by the Transformer in address-
ing a multitude of challenges across these domains.
Considering the trending unsupervised Re-ID, we pro-
pose a new Transformer baseline, UntransReID, achiev-
ing state-of-the-art performance on both single/cross
modal tasks. For the under-explored animal Re-ID,
we devise a standardized experimental benchmark and
conduct extensive experiments to explore the applica-
bility of Transformer for this task and facilitate future
research. Finally, we discuss some important yet under-
M. Ye, SY. Chen, CY. Li and B. Du are with the National
Engineering Research Center for Multimedia Software, School
of Computer Science, Hubei Luojia Laboratory, Wuhan Uni-
versity, Wuhan, China.
WS. Zheng is with the School of Data and Computer Science,
Sun Yat-sen University, Guangzhou, China.
D. Crandall is with the Luddy School of Informatics, Com-
puting, and Engineering, Indiana University.investigated open issues in the large foundation model
era, we believe it will serve as a new handbook for re-
searchers in this field. A periodically updated website
will be available at https://github.com/mangye16/
ReID-Survey .
Keywords Object Re-Identification, Transformer,
Survey, Person Re-Identification, Deep Learning
1 Introduction
The object re-identification (Re-ID), which aims at
matching the same object (person (Gray et al., 2007),
vehicles (Sun et al., 2004), etc) across multiple differ-
ent views(Hermans et al., 2017; Zhong et al., 2017).
Over the years, object Re-ID has attracted consider-
able attention as a significant research area (Ye et al.,
2021c; Zheng et al., 2015; Ahmed et al., 2015), expand-
ing the application scope of tasks such as object de-
tection, tracking, and recognition. It holds substantial
practical application value in domains such as intelli-
gent surveillance, smart cities, and the preservation of
natural ecosystems. In recent years, research in the Re-
ID field, particularly concerning subjects like persons
and vehicles, has undergone profound development and
has achieved notable success in conventional settings.
Moreover, Re-ID encompasses a diverse array of ob-
ject categories, including animals, buildings, and more.
In order to better address real-world application de-
mands, existing Re-ID research is gradually shifting its
focus towards open-world scenarios. This entails tack-
ling challenges such as managing large-scale data with
limited annotations (Xuan and Zhang, 2021; Zhang
et al., 2022c; Yu et al., 2019), diverse data modalities
(Ye et al., 2021b; Wu and Ye, 2023), generalization ofarXiv:2401.06960v2  [cs.CV]  22 Oct 20242 Mang Ye, Shuoyi Chen, Chenyue Li, Wei-Shi Zheng, David Crandall, Bo Du
20192 0202 0212 0222 0232 024405060708090mAP 
CNN 
TransformerB oT(CVPRW)STF(ICCV)OSNet(ICCV)ABDNet(ICCV)R
GA-SC(CVPR)C
BN(ECCV)AGW(TPAMI)CDNet(CVPR)HAT(ACM MM)PFD(AAAI)RotTrans(ACM MM)PHA(CVPR)T
ransReID(ICCV)PTCR(ACM MM)SOLIDER(CVPR)C
LIP-ReID(AAAI)PromptSG(CVPR)
USLT ext-ImageEnd-to-endU AVC loth-change20304050607080mAP 
CNN 
Transformer  TransReID-
SSL(arxiv21)D
CMIP(ICCV23)I
SE(CVPR22)P
PLR(CVPR22)MLLM(CVPR24) 
  VITAA(
ECCV20) 
   AXM-Net 
(ACM MM22) 
 AlignPS(
CVPR21)PSTR(CVPR22)    RotTrans 
(ACM MM22)T
ransReID(ICCV21) 
   AGW(
TPAMI21)  AD-ViT(
AVSS22) 
    SCNet(
ACM MM23)C
CFA(CVPR23)IRRA(CVPR23)
Fig. 1: (1) We show the performance of recent state-of-the-art methods on the widely-used person Re-ID dataset
MSMT17 (left). The transformer-based methods have achieved a comprehensive lead in accuracy since 2021, while
the CNN-based method for single-modal image Re-ID has not been studied. (2) We show state-of-the-art results of
representative works in different Re-ID tasks: unsupervised (USL) Re-ID on MSMT17 (Wei et al., 2018) dataset,
Text-Image on CUHK-PEDES (Li et al., 2017), end-to-end person search on PRW (Zheng et al., 2017a), Re-ID
in UAVs on PRAI-1581 (Zhang et al., 2020a), and cloth-changing Re-ID on LTCC (Qian et al., 2020).
unknown scenarios (Jin et al., 2020; Zhao et al., 2021),
as well as tackling specialized applications like long-
term Re-ID or group Re-ID (Chen et al., 2022c; Rao
et al., 2021; Xiao et al., 2018). These emerging research
directions hold rich potential for further advancing Re-
ID and facilitating its practical deployment.
Benefiting from the development of deep learning
(Cheng et al., 2023c, 2024), the studies in the Re-ID
field have been dominated by deep Convolutional Neu-
ral Networks (CNNs) for a long time (Ye et al., 2021c;
Zheng et al., 2016b). Nevertheless, the emergence of
the Vision Transformer (Vaswani et al., 2017; Doso-
vitskiy et al., 2020) has changed this situation. Trans-
former is a network architecture dispensing with recur-
rence and convolutions that relies entirely on attention
mechanisms to model the global dependencies between
inputs and outputs. Transformer has been introduced
into the field of computer vision as a breakthrough,
demonstrating remarkable performance in various vi-
sual tasks, including Re-ID. Intuitively, some works try
to directly replace CNNs with existing vision trans-
formers (Dosovitskiy et al., 2020; Liu et al., 2021b;
Wang et al., 2021b) as the feature extractor, which sig-
nificantly improves the accuracy of Re-ID (He et al.,
2021a; Chen et al., 2022c; Cao et al., 2022; Zhang
et al., 2022a; Comandur, 2022). Unlike CNNs, Vision
Transformer (ViT) (Dosovitskiy et al., 2020) architec-
ture imposes little structural bias to guide representa-
tion learning, which allows for diverse learning strat-
egy design and broad task applicability (Walmer et al.,
2023). This allows the Transformer to be flexibly ap-plied to various scenarios including unsupervised Re-
ID, multimodal Re-ID, etc. Furthermore, ViT treats
an image as a sequence of patch tokens rather than
processing images pixel-by-pixel, which allows inputs of
various sizes to be accepted without additional adjust-
ments. The tokenization feature exhibits strong com-
patibility for personalized design and offers flexibility
in organizing information (Xu et al., 2023; Han et al.,
2022; Naseer et al., 2021). These advantages facilitate
the seamless integration of Transformers with Re-ID-
specific designs, and novel research ideas for Re-ID that
leverage the unique properties of transformers continue
to emerge (Jiang and Ye, 2023; Luo et al., 2021; Rao
and Miao, 2023; Li et al., 2022b; Chen et al., 2022c).
In recent years, Transformer-based Re-ID research has
consistently set new records in recognition accuracy,
showing a trend of being significantly superior to CNN-
based methods in many aspects, as shown in Fig. 1.
Due to the rapid proliferation of transformer-based Re-
ID models, it is becoming progressively challenging to
stay abreast of the latest advancements. Consequently,
there is an urgent need for a comprehensive survey of
existing Transformer-based works, which would greatly
benefit the community in the new era.
Existing Re-ID surveys (Ye et al., 2021c; Zheng
et al., 2016b; Khan and Ullah, 2019; Wang et al., 2019d)
predominantly focus on deep learning methods based
on CNNs and tend to narrow their scope to specific ob-
jects, with a primary emphasis on persons or vehicles.
On the contrary, this survey is mainly oriented towards
the application of emerging transformer technology inTransformer for Object Re-Identification: A Survey 3
1
Transformer in 
Object Re -ID
Re-ID with Limited 
Data/Annotations
Cross -modal Re -ID
Special Re -ID 
ScenariosImage/Video Re -IDBackgroundNew Baseline :
UntransReID
Visible -infrared 
Cross -modal Re -IDSingle -modal Re -IDAnimal Re -ID
Animal Re -ID 
MethodsSpecies -specific 
DatasetFuture Prospects
Unified Re -ID ModelRe-ID Meets VLM
Efficient DeploymentRe-ID before 
Transformer
A Unified 
BenchmarkOverview of the 
Transformer
Superiority 
In-depth Analysis
Fig. 2: An overview of the framework structure for the survey, illustrating key sections and their interrelationships.
Re-ID and covers a wider range of objects (persons, ve-
hicles, and animals), which is more innovative and com-
prehensive. Recognizing the significant potential and
promise demonstrated by numerous Transformer-based
studies in various vision applications, we systemati-
cally organize and review the growing research works on
Transformer for Re-ID in recent years to gain valuable
insights. Differing from existing surveys, the primary
contributions of our survey are as follows:
–We conduct an in-depth analysis of the strengths
of Transformer and summarize the research efforts
since its introduction into the Re-ID field across
four extensively studied Re-ID directions, includ-
ing image/video-based Re-ID, Re-ID with limited
data/annotations, cross-modal Re-ID, and special
Re-ID scenarios. It demonstrates the success of
Transformer in Re-ID and underscores its potential
for future advancements.
–We introduce a Transformer-based unsupervised
baseline for trending unsupervised Re-ID, leverag-
ing the Transformer architecture, which remains
relatively underexplored in existing works. Our
proposed method demonstrates competitive perfor-
mance across both single- and cross-modal unsuper-
vised Re-ID tasks.
–We particularly delve into animal re-identification,
an area that has received significantly less atten-
tion compared to persons and vehicles. It presents
numerous challenges and unresolved issues. We de-
velop unified experimental standards for animal Re-
ID and evaluate the feasibility of employing Trans-
former in this context, laying a solid foundation for
future research.
The rest of this survey is organized as follows: In §2,
we briefly review the development of the Re-ID field be-
fore the Transformer era while introducing the Trans-
former in vision and providing a detailed analysis of
its numerous advantages. A comprehensive analysis of
Transformer in Re-ID is presented in §3. A powerfulTransformer baseline for single/cross-modal unsuper-
vised Re-ID is proposed in §4. The progress in Animal
Re-ID and the evaluation of the applicability of Trans-
former to this task are introduced in §5. In Fig. 2, we
present the overall framework structure of this survey,
outlining the key sections and their interconnections.
2 Background
2.1 A Brief Review of Re-ID Before Transformers
This subsection begins by summarizing the fundamen-
tal definition and challenges of object Re-ID, followed
by an introduction to commonly used datasets and eval-
uation metrics ( §2.1.1). Then, we give a general review
of previous CNN-dominated Re-ID research works and
discuss the limitations of CNN-based Re-ID methods in
some aspects ( §2.1.2).
2.1.1 Object Re-Identification
Definition. Given a query q, the goal of object re-
identification is to retrieve specific objects from a
gallery set G={gi|i= 1,2,···, N}ofNdescriptors.
An important feature of Re-ID is to identify the ob-
jects in different cameras with non-overlapping views.
The query qcan be an image, a video sequence, a text
description, a sketch, a combination of different forms,
etc. (Liu et al., 2016b; Zheng et al., 2016b; Liu et al.,
2016a; Chen et al., 2017; Ye et al., 2021b; Jiang and Ye,
2023; Chen et al., 2023a). The identity of the query q
can be formulated as:
I= arg min
gidis(q, gi), gi∈ G, (1)
where dis(·,·) is an arbitrary distance metric.
Challenges. The flow of a basic Re-ID system is
shown in Fig. 3. To provide a detailed overview of the4 Mang Ye, Shuoyi Chen, Chenyue Li, Wei-Shi Zheng, David Crandall, Bo Du
Image
Various typesof queriesText
RGBIRSketch
Video
Search
Person ReIDVehicle ReIDAnimal ReID
image input
tokenized inputTransformer······discriminative featuresCNN-based
Transformer-basedReIDModel
Rank list
Fig. 3: General object Re-ID process. Given a query that can be any type of image, text, video, etc., the goal
of Re-ID is to search for the specific object from gallery data collected by different cameras.
challenges in Re-ID tasks, we have divided the discus-
sion into two main categories. (1) Object Types. The
objects that are widely studied currently include per-
sons and vehicles. Person Re-ID involves the challenge
of individuals frequently changing clothes, accessories,
and appearance. This variability can result in signifi-
cant differences in visual appearance, making it diffi-
cult for Re-ID models to consistently identify the same
individual over time. Therefore, models are required to
disregard changes in clothing and focus on more in-
variant features, such as body shape or gait. Addition-
ally, human body posture can vary considerably due to
movement and camera angles. Discriminative regions,
such as the face, are often difficult to capture under
these circumstances. In vehicle Re-ID, the primary chal-
lenge lies in the significant intra-class similarity. Vehi-
cles of the same brand, model, and color often appear
nearly identical. Even minor differences, such as subtle
scratches or decals, may be crucial for distinguishing
one vehicle from another, but these differences can be
difficult to detect or may not always be visible. Addi-
tionally, vehicles can look different when viewed from
various angles (e.g., front, rear, or side). Unlike per-
sons whose body shape remains relatively constant, the
geometry and key identifying features of a vehicle can
vary substantially depending on the viewpoint, making
it challenging for Re-ID systems to generalize across
multiple perspectives. (2) Application Scenarios. The
early Re-ID tasks mainly focused on the pre-detected
object bounding boxes obtained from the original image
or video and used the appearance information to match
the corresponding individual. Due to the complex con-
ditions of image acquisition, the difficulty of Re-ID in
this period involves occlusion, illumination variation,
resolution difference, camera view variation, and back-ground clutter (Zheng et al., 2016b; Khan and Ullah,
2019; Ahmed et al., 2015). Furthermore, new challenges
continue to emerge with the urgent growth of practical
application requirements (Wu and Ye, 2023; Yang et al.,
2022; Yu et al., 2019). For example, with the widespread
application of drone surveillance recently (Zhang et al.,
2020a; Wang et al., 2019b; Teng et al., 2021), discrimi-
native information has been greatly reduced under ex-
treme bird’s-eye view angles (Li et al., 2021d; Kumar
et al., 2020). In the data processing stage, for special
cases where conventional visible light images are not
available, valid object information might be represented
by different modalities such as infrared images, text
descriptions, sketches, and depth images (Chen et al.,
2023a; Ye et al., 2023; Zhai et al., 2022b). The cross-
modal gaps of object Re-ID in the modal heterogeneous
scene lead to great intra-class differences across varying
modalities (Li et al., 2017; Wang et al., 2020c). In ad-
dition, due to the artificial cross-camera correlation of
objects, high labeling costs (Li et al., 2019a; Fu et al.,
2021; Cheng et al., 2022a) and unavoidable noise prob-
lems (Ye et al., 2021a) make it difficult to achieve large-
scale expansion (Ye et al., 2020b; Lin et al., 2019b; Cho
et al., 2022). In the retrieval phase, the varying real en-
vironmental domains may cause the inapplicability of
the Re-ID model (Bai et al., 2021a; Ni et al., 2022),
and long-term Re-ID suffers from appearance informa-
tion changes (Qian et al., 2020; Fan et al., 2020).
Datasets. In Table 1, we provide a comprehensive
summary of widely utilized datasets for various Re-ID
tasks. These datasets encompass a range of challenges
specific to different object types, such as persons and
vehicles, offering diverse conditions for evaluating Re-
ID algorithms. By aggregating key details, including
the number of identities, images, and cameras, this ta-Transformer for Object Re-Identification: A Survey 5
ble serves as a valuable reference for understanding the
scale, diversity, and complexity of datasets frequently
employed in Re-ID research.
Evaluation Metrics. In the evaluation of Re-ID
models, two commonly adopted metrics are the Cumu-
lative Matching Characteristic (CMC) and the mean
Average Precision (mAP). The CMC curve measures
the probability that a correct match for a given query
appears within the top-K ranked results. It is partic-
ularly effective in ranking-based evaluation scenarios,
providing insights into how well a model retrieves the
correct identity from a gallery. The CMC at rank-1 is
often emphasized, as it reflects the model’s ability to
correctly identify the target in the top position, mak-
ing it a critical metric for real-world Re-ID applications
where quick and accurate identification is crucial. On
the other hand, mAP provides a more comprehensive
evaluation by considering both precision and recall over
the entire ranked list. It computes the average preci-
sion for each query and then calculates the mean across
all queries. Unlike CMC, mAP is sensitive to both the
ranking and the completeness of retrieved results, mak-
ing it a robust metric for cases where the correct iden-
tity might appear lower in the ranking list. This metric
is particularly valuable in scenarios where it is impor-
tant not only to rank the correct match highly but also
to retrieve all relevant matches with high precision. To-
gether, CMC and mAP provide a well-rounded assess-
ment of Re-ID models, reflecting their performance in
ranking accuracy and retrieval quality.
Besides, mINP (mean Inverse Negative Penalty) (Ye
et al., 2021c) is a newly proposed metric designed to en-
hance the evaluation of Re-ID systems by focusing on
the rank position of the hardest correct match, which is
crucial for effective tracking in multi-camera networks.
Unlike traditional metrics like CMC and mAP, which
may not accurately reflect the challenges of identify-
ing all correct matches, mINP quantifies the penalty
incurred when searching for the hardest match. This
metric is computationally efficient and can be easily
integrated into existing CMC/mAP evaluation frame-
works. While it may exhibit smaller value differences
with larger gallery sizes, mINP still effectively indicates
the relative performance of a Re-ID model, serving as
a valuable complement to conventional metrics.
2.1.2 CNN-based Re-ID Methods
Under the mainstream trend of deep learning, the ob-
ject Re-ID steps are generalized as different steps, in-
cluding data processing, model training, and descrip-
tor matching. Most existing methods take training a
strong Re-ID model as the core goal. In fact, CNNshave dominated Re-ID studies for a long period. In this
section, we focus on reviewing the progress of object
Re-ID which is highly related to CNNs. Considering dif-
ferent application requirements, Ye et al. proposed to
divide Re-ID technology into two subsets, closed-world
and open-world (Ye et al., 2021c).
Closed-world refers to supervised learning methods
based on well-labeled visible images captured by com-
mon video surveillance (Liu et al., 2016a). With the aid
of labels, many approaches model Re-ID as a classifi-
cation, verification, or metric learning problem, using
CNNs ( i.e., ResNet(He et al., 2016)) to learn discrim-
inative feature representations from the training data
(Zheng et al., 2017b; Luo et al., 2019; Liu et al., 2016b).
On the basis, learning local features such as image slices
(Sun et al., 2018; Park and Ham, 2020), semantic pars-
ing (Meng et al., 2020; Kalayeh et al., 2018), pose es-
timation (Suh et al., 2018; Su et al., 2017), region of
interest (He et al., 2019) and key points (Wang et al.,
2020a; Khorramshahi et al., 2019) to further mine fine-
grained information are typical ideas in Re-ID. At the
CNN backbone level, some people try to directly im-
prove the convolutional layer and residual block (Zhou
et al., 2019), and a large number of studies introduce
attention modules in CNNs to capture the relationship
between different convolutional channels, feature maps,
and local regions (Guo et al., 2019; Ye et al., 2021c; Li
et al., 2018; Zhang et al., 2020b; Chen et al., 2019). On
the other hand, for video sequence input with tempo-
ral information, the major limitation of CNNs is that it
can only process spatial dimension information. Some
video Re-ID works introduce RNN or LSTM for se-
quence modeling (McLaughlin et al., 2016; Yan et al.,
2016; Liu et al., 2017).
Open-world technologies usually target more com-
plex and difficult scenarios, including cross-modal Re-
ID, unsupervised learning, domain generalization, etc.
(1)Cross-modal Re-ID. In recent years, cross-modal
Re-ID of visible-infrared (Ye et al., 2021b; Li et al.,
2020; Yang et al., 2023b; Cheng et al., 2023a) and
text-image (Niu et al., 2020; Wu et al., 2021; Ding
et al., 2021) has received more and more interests. For
visible-infrared Re-ID, researchers design single-stream
(Ye et al., 2020a), dual-stream (Ye et al., 2019b, 2018;
Zhang et al., 2021b) and other different structures (Wu
et al., 2017) based on CNN to learn modality-sharing
and modality-specific feature representations. To re-
duce the difference between modalities, a class of meth-
ods implements modal conversion or style transforma-
tion through GAN (Wang et al., 2019a,c, 2020b) or
special augmentation strategies (Ye et al., 2021b) and
then performs subsequent CNN-based feature represen-
tation learning. Text-to-image Re-ID mainly focuses on6 Mang Ye, Shuoyi Chen, Chenyue Li, Wei-Shi Zheng, David Crandall, Bo Du
Table 1: Summary of Commonly Used Datasets for Diverse Re-ID Tasks.
Dataset Year Images/Boxes Identities Object Type Characterization
Image/Video Based Re-ID
MSMT17(Wei et al., 2018) 2018 126,441 4,101 Person Image
DukeMTMC-ReID(Zheng et al., 2017c) 2017 36,441 1,812 Person Image
Market1501(Zheng et al., 2015) 2015 32,217 1,501 Person Image
CUHK03(Li et al., 2014) 2014 13,164 1,467 Person Image
MARS(Zheng et al., 2016a) 2016 1,191,003 1,261 Person Video
LPW(Song et al., 2018) 2018 592,438 2,731 Person Video
CityFlow(Tang et al., 2019) 2019 56,277 666 Vehicle Image
VERI-Wild(Lou et al., 2019) 2019 416,314 40,671 Vehicle Image
PKU-VD(VD1/VD2)(Yan et al., 2017) 2017 846,358/807,260 1,232/1,112 Vehicle Image
VehicleID(Zapletal and Herout, 2016) 2016 221,763 26,267 Vehicle Image
VeRi-776(Liu et al., 2016b) 2016 49,357 776 Vehicle Image
Cross-modality Re-ID
CUHK-PEDES(Li et al., 2017) 2017 40,206 (image), 80,422 (text) 13,003 Person Image-text
ICFG-PEDES(Ding et al., 2021) 2021 54,522 (image), 54,522 (text) 4,102 Person Image-text
RSTPReid(Zhu et al., 2021a) 2021 20,505 (image), 41,010 (text) 4,101 Person Image-text
RegDB (Nguyen et al., 2017) 2017 4,120 (RGB), 4,120 (thermal) 412 Person Visible-thermal
SYSU-MM01(Wu et al., 2017) 2017 20,284 (RGB), 9,929 (infrared) 296 Person Visible-infrared
PKU SketchRe-ID(Pang et al., 2018) 2018 400 200 Person Sketch
ReID with Limited Data/Annotations
LUPerson(Fu et al., 2021) 2021 4M 200K Person Unlabelled Image
VehicleX(Yao et al., 2020) 2020 ∞ 1,362 Vehicle Synthetic data
PersonX(Sun and Zheng, 2019) 2019 45,576 1,266 Person Synthetic data
UnrealPerson(Zhang et al., 2021d) 2021 120,000 3,000 Person Synthetic data
WePerson(Li et al., 2021b) 2021 4,000,000 1,500 Person Synthetic data
Special Re-ID Scenarios
UAV-Human (Li et al., 2021d) 2021 41,290 1,144 Person UAV
PRAI-1581 (Zhang et al., 2020a) 2020 39,461 1,581 Person UAV
VRAI(Wang et al., 2019b) 2019 137,613 13,022 Vehicle UAV
UAV-VeID (Teng et al., 2021) 2021 41,917 4,601 Vehicle UAV
Partial-REID(He et al., 2021c) 2021 600 60 Person Occluded
Occluded-DukeMTMC(Miao et al., 2019) 2019 35,489 2,331 Person Occluded
Occluded-REID (Zhuo et al., 2018) 2018 2,000 200 Person Occluded
DeepChange(Xu and Zhu, 2023) 2023 178K 1,121 Person Cloth-changing
LTCC(Qian et al., 2020) 2020 17,119 152 Person Cloth-changing
PRCC(Yang et al., 2019) 2019 33,698 221 Person Cloth-changing
CUHK-SYSU(Xiao et al., 2017) 2017 18,184 8,432 Person Person Search
PRW(Zheng et al., 2017a) 2017 5,704 482 Person Person Search
CSG(Yan et al., 2020) 2020 3,839 1,558(group classes) Person Group Re-lD
DukeMTMC Group(Lin et al., 2019a) 2019 354 177(group classes) Person Group Re-lD
RoadGroup(Lin et al., 2019a) 2019 324 162(group classes) Person Group Re-lD
the cross-modal alignment module design based on the
visual and text features extracted from each modal-
ity backbone (Zhang and Lu, 2018; Sarafianos et al.,
2019). In addition, many works also introduce atten-
tion mechanisms to enhance local information match-
ing (Shao et al., 2022; Farooq et al., 2022). (2) Unsu-
pervised learning. This approach alleviates the label in-
sufficiency issue (Ge et al., 2020), which now has been
a trending topic due to its benefits in large-scale ap-
plications. It mainly includes two categories: unsuper-
vised domain adaptation (Dai et al., 2021; Bai et al.,
2021b; Zheng et al., 2021) and pure unsupervised learn-
ing (Lin et al., 2020; Wang and Zhang, 2020). In ad-
dition to transferring knowledge from labeled sourcedatasets to unlabeled target datasets (Wei et al., 2018;
Deng et al., 2018), most of the existing methods learn
feature representations purely from unlabeled images
(Zhang et al., 2022c). The core idea is to use the features
extracted by CNNs to perform clustering to obtain
pseudo-labels as label supervision, some of which focus
on generating high-quality pseudo-labels (Cho et al.,
2022; Zhang et al., 2021f; Wu et al., 2022b), and oth-
ers improve clustering algorithms and training strate-
gies (Lin et al., 2019b; Ge et al., 2020; Dai et al., 2022).
(3)Other open scenes. In recent years, an increasing
number of research efforts have shifted towards open
scenarios, such as cloth-changing Re-ID and domain-
generalizable Re-ID. To facilitate the research, manyTransformer for Object Re-Identification: A Survey 7
new cloth-changing datasets (Qian et al., 2020; Yang
et al., 2019) have been introduced. The key to address-
ing the cloth-changing problem lies in learning clothing-
agnostic features, and straightforward approaches in-
volve augmenting the data by introducing a variety of
clothing types (Jia et al., 2022b; Xu et al., 2021). Many
works try to utilize auxiliary information, such as hu-
man parsing (Liu et al., 2023a; Guo et al., 2023), gait
(Jin et al., 2022), shape (Hong et al., 2021) to guide
the CNN model to focus on identity-related features.
Domain generalization is also highly aligned with prac-
tical application requirements. Some research endeav-
ors focus on creating large-scale and diverse synthetic
data (Li et al., 2021b), while others seek to enhance
the generalization capabilities of CNN models through
meta-learning (Choi et al., 2021; Ni et al., 2022) or dis-
entanglement techniques (Jin et al., 2020).
2.2 Understanding and Analysis of Transformer
The introduction of Vision Transformer opens novel di-
rections for Re-ID studies, especially in challenging sce-
narios. In this subsection, we first give the basic con-
cept of the Transformer ( §2.2.1). In order to demon-
strate the superiority of the Transformer, we provide
a comprehensive comparison between the Transformer
and CNN and analyze it in terms of network architec-
ture, modeling capabilities, scalability, flexibility, and
special properties ( §2.2.2).
2.2.1 Transformer Concepts
Original Transformer. The original Transformer
(Vaswani et al., 2017) was proposed in the field of
natural language processing (NLP), which is the first
sequence transduction model based on the attention
mechanism. It completely abandons the dominant se-
quence transduction models based on complex recur-
rent and convolutional neural networks and achieves
new state-of-the-art levels in multiple NLP tasks. The
transformer is essentially an encoder-decoder structure,
in which both the encoder and decoder are composed
of multiple stacked transformer layers (Vaswani et al.,
2017; Han et al., 2022; Xu et al., 2023). Each trans-
former layer consists of two sub-layers: a multi-head
self-attention mechanism and a position-wise fully con-
nected feed-forward network. Self-attention plays a cru-
cial role in Transformer, which enables each element
to learn to gather from other tokens in the sequence.
Multi-head self-attention can create multiple attention
matrices in a layer, and with multi-head, the self-
attention layer will create multiple outputs to guar-
antee diverse capability. The two sub-layers performresidual connection (He et al., 2016) for stability, fol-
lowed by layer normalization. The transformer accepts
tokenized sequences as input. To make efficient use of
sequence order, an optional positional encoding (rela-
tive or absolute) needs to be added. Transformers are
used in machine translation tasks, where the encoder
extracts features from input with positional encodings,
and the decoder uses these features to produce out-
put. Since Transformer was proposed, it has gradually
become mainstream and most of the subsequent NLP
research has been reprocessed on its basis (Devlin et al.,
2018).
Vision Transformer. The emergence of Vision
Transformer (ViT) (Dosovitskiy et al., 2020) is a sig-
nificant breakthrough in the field of computer vision
(Cheng et al., 2023b). Different from the previous work
that embeds the attention module in the CNN, it ap-
plies the pure Transformer to the image patches with a
simple idea and has achieved remarkable results. Specif-
ically, given an image x∈RH×W×C,H×WandCrep-
resent the image resolution and the number of channels
respectively. In order to adapt to the tokenized sequence
input of the Transformer, ViT designs a patch embed-
ding operation that divides an image into Npatches,
where the size of each patch is P×P. These patches
are projected into the D-dimensional space after lin-
ear transformation as the input of ViT, which is a se-
quence composed of N D-dimensional vectors, denoted
asx∈RN×D. A special learnable embedding called
class token is set for classification, which is directly con-
catenated to the patch embedding. Following a similar
line of the original Transformer, the position embed-
ding is also added to each patch embedding to preserve
the spatial position information of the image which is
represented as Epos∈R(N+1)×D. ViT adopts the same
structure as the encoder of the original Transformer as
a feature extractor. Following the ViT paradigm, a se-
ries of subsequent ViT variants are proposed for various
vision tasks, leading to significant advancements (Han
et al., 2022).
2.2.2 Superiority of Vision Transformer
We provide a detailed analysis of the strengths of Trans-
former from the vision perspective to facilitate the sub-
sequent elaboration of its robust performance in ad-
dressing complex and dynamic Re-ID scenarios.
Powerful Modeling Capabilities. Different from
the standard CNNs, which are limited to the local re-
ceptive field, it is extremely difficult to establish long-
distance relationships at the early stage. As a result,
the performance of CNNs is limited for challenging
scenarios. In contrast, the powerful modeling ability8 Mang Ye, Shuoyi Chen, Chenyue Li, Wei-Shi Zheng, David Crandall, Bo Du
of Transformer is reflected in the local-global duality
(Walmer et al., 2023). Specifically, the modeling of im-
ages mainly involves pixel level and object level in vision
tasks with images or videos. The attention mechanism
of the Transformer is flexibly designed to process in-
formation from any image region and can model any
relationships between pixels-pixels, objects-pixels and
objects-objects. Moreover, CNN can only build hier-
archical representations from local to global, whereas
Transformer has the flexibility to integrate global in-
formation at any stage (Naseer et al., 2021; Liu et al.,
2021b). For Re-ID, both global and local modeling are
essential for learning discriminative features to distin-
guish high-similar inter-class objects.
Diverse Unsupervised Learning Paradigms.
Due to the expensive and time-consuming nature of ac-
quiring large amounts of high-quality annotated data,
unsupervised learning can develop more generalized fea-
ture representations without relying on annotations.
The great success of Transformers in NLP has largely
benefited from self-supervised learning, which provides
a solid foundation for the self-supervised research in
Vision Transformer. Unsupervised learning in the field
of computer vision has primarily been centered around
contrastive learning, and the introduction of Trans-
former has made it feasible to incorporate mainstream
generative learning approaches from NLP, such as
masked autoencoders (He et al., 2022). Besides, dis-
criminative self-supervised methods reveal some new
characteristics in Transformer models, such as clear ob-
ject boundaries (Caron et al., 2021). In general, unsu-
pervised learning with its cost-effectiveness and gen-
eralization capabilities, emerges as a future trend, and
transformers hold a unique advantage within this trend.
Multi-modal Uniformity and Versatility. In
practical applications, single-modal data may lack in-
formation or be ambiguous. Multi-modal learning al-
lows models to leverage diverse types of data, enabling
them to capture rich and comprehensive information
for a better understanding of complex real-world sce-
narios. Compared to CNN, which is primarily designed
for processing image modalities, the Transformer ex-
hibits significant versatility across multiple modalities.
Multi-modal information can be transformed into a to-
ken sequence or latent space features within the same
semantic space and input into Transformer for encod-
ing. Transformer can be considered as a fully connected
graph, where each token embedding is represented as a
node in the graph and the relationships between these
embeddings can be described by edges. This property
enables Transformer to function within a modality-
agnostic pipeline that is compatible with various modal-
ities (Xu et al., 2023). Especially in the combination ofvision and language, Transformer promotes many new
ideas for solving cross-modal Re-ID challenges.
High Scalability and Generalization. With the
continuous increase in data, the future demand for
highly scalable models becomes increasingly urgent to
adapt effectively to the growing scale of data. More-
over, the generalization capability is crucial for stable
performance in dynamic and unknown environments.
Numerous recent studies have shown that the powerful
scalability of Transformer in terms of large models and
big data has achieved incredible results (Brown et al.,
2020; Dehghani et al., 2023). Zhai et al. (Zhai et al.,
2022a) successfully trained a Vision Transformer model
with 2 billion parameters, achieving a new record of
90.45% top-1 accuracy on ImageNet. Transformers hold
immense potential for larger and more versatile mod-
els. The encoder-decoder structure of the Transformer,
coupled with the joint learning of decoder embeddings
and positional encoding, can seamlessly unify various
tasks. Additionally, its powerful cross-modal learning
capabilities offer further possibilities for expanding Re-
ID applications.
3 Transformer in Object Re-ID
In this section, we comprehensively review the latest
research on Transformer-based Re-ID. Considering the
different types of challenges and diverse applications of
Re-ID tasks, we divided the existing research into four
scenarios: regular images or videos with annotations
(§3.1), limited data or limited annotations ( §3.2), multi-
modal data ( §3.3), and special settings ( §3.4) to demon-
strate the advantages of Transformer respectively.
3.1 Transformer in Image/Video Based Re-ID
In this subsection, we summarize the progress of trans-
formers under the general supervised setting of image-
based ( §3.1.1) and video-based ( §3.1.2) Re-ID. For
image-based Transformer Re-ID methods, we first re-
view different structural designs at the backbone level
for discriminative Re-ID feature extraction. In addi-
tion, the tokenized embeddings and attention mecha-
nism in Transformer provide strong flexibility for repre-
sentation learning. We comprehensively summarize the
methods of exploiting Transformer properties for Re-
ID-specific design. In video Re-ID, Transformer-based
methods have shown great superiority over CNNs on
modeling the spatio-temporal cues.Transformer for Object Re-Identification: A Survey 9
[32], etc., divide an image into several stripes and extract
local features for each stripe. Using parsing or keypoint
estimation to align different parts or two objects has also
been proven effective for both person and vehicle ReID
[25, 30, 47, 31].
Side Information. For images captured in a cross-
camera system, large variations exist in terms of pose,
orientation, illumination, resolution, etc. caused by different
camera setup and object viewpoints. Some works [61, 7]
use side information such as camera ID or viewpoint
information to learn invariant features. For example,
Camera-based Batch Normalization (CBN) [61] forces
the image data from different cameras to be projected
onto the same subspace, so that the distribution gap
between inter- and intra- camera pairs is largely diminished.
Viewpoint/Orientation-invariant feature learning [7, 60] is
also important for both person and vehicle ReID.
2.2. Pure Transformer in Vision
The Transformer model is proposed in [41] to handle
sequential data in the ﬁeld of natural language processing
(NLP). Many studies also show its effectiveness for
computer-vision tasks. Han et al . [11] and Salman et
al. [18] have surveyed the application of the Transformer
in the ﬁeld of computer vision.
Pure Transformer models are becoming more and more
popular. For example, Image Processing Transformer
(IPT) [3] takes advantage of transformers by using
large scale pre-training and achieves the state-of-the-art
performance on several image processing tasks like super-
resolution, denoising and de-raining. ViT [8] is proposed
recently which applies a pure transformer directly to
sequences of image patches. However, ViT requires a
large-scale dataset to pretrain the model. To overcome
this shortcoming, Touvron et al. [40] propose a framework
called DeiT which introduces a teacher-student strategy
speciﬁc for transformers to speed up ViT training without
the requirement of large-scale pretraining data.
3. Methodology
Our object ReID framework is based on transformer-
based image classiﬁcation, but with several critical
improvements to capture robust feature (Sec. 3.1). To
further boost the robust feature learning in the context
of transformer, a jigsaw patch module (JPM) and a side
information embeddings (SIE) are carefully devised in
Sec. 3.2 and Sec. 3.3. The two modules are jointly trained
in an end-to-end manner and shown in Figure 4.
3.1. Transformer-based strong baseline
We build a transformer-based strong baseline for object
ReID, following the general strong pipeline for object
ReID [27, 44]. Our method has two main stages, i.e., feature
Linear Projection of Flattened PatchesTransformer LayerTransformer Layer⋮!0 1278
Position EmbeddingID LossTriplet LossBN
⋯⋯****Extra learnable[cls] embedding!Figure 3: Transformer-based strong baseline framework (a non-
overlapping partition is shown). Output [cls] token marked with ∗
is served as the global feature f. Inspired by [27], we introduce
the BNNeck after the f.
extraction and supervision learning. As shown in Figure 3.
Given an image x∈RH×W×C, whereH,W,Cdenote
its height, width, and number of channels, respectively, we
split it into Nﬁxed-sized patches {xi
p|i= 1,2,···,N}.
An extra learnable [cls] embedding token denoted as xcls
is prepended to the input sequences. The output [cls]
token serves as a global feature representation f. Spatial
information is incorporated by adding learnable position
embeddings. Then, the input sequences fed into transformer
layers can be expressed as:
Z0= [xcls;F(x1
p);F(x2
p);···;F(xN
p)] +P,(1)
whereZ0represents input sequence embeddings and
P ∈ R(N+1)×Dis position embeddings. Fis a
linear projection mapping the patches to Ddimensions.
Moreover,ltransformer layers are employed to learn
feature representations. The limited receptive ﬁeld
problem of CNN-based methods is addressed, because all
transformer layers have a global receptive ﬁeld. There
are also no downsampling operations, so the detailed
information is preserved.
Overlapping Patches. Pure transformer-based models
(e.g. ViT, DeiT) split the images into non-overlapping
patches, losing local neighboring structures around the
patches. Instead, we use a sliding window to generate
patches with overlapping pixels. Denoting the step size as
S, size of the patch as P(e.g.16) , then the shape of the
area where two adjacent patches overlap is (P−S)×P.
An input image with a resolution H×Wwill be split into
Npatches.
N=NH×NW=⌊H+S−P
S⌋×⌊W+S−P
S⌋(2)
Fig. 4: The first pure transformer baseline for object
Re-ID (He et al., 2021a). The Vision Transformer back-
bone (Dosovitskiy et al., 2020) is adopted as a feature
extractor, optimized with ID loss and triplet loss (Her-
mans et al., 2017) widely used in Re-ID.
3.1.1 Transformer in Image-based Re-ID
Architecture Improvements. A number of recent
studies have shown that applying Vision Transformer
as a feature extractor in Re-ID can achieve high accu-
racy (He et al., 2021a; Cao et al., 2022; Zhang et al.,
2022a; Li et al., 2023b). Many recent Re-ID methods
are dedicated to designing special Transformer architec-
tures to build stronger backbones (Li et al., 2022b; Shen
et al., 2023a; Zhang et al., 2021a). The first to introduce
Vision Transformer in the Re-ID field is TransReID
(He et al., 2021a), which preserves two advantages of
Transformer at the architectural level. Compared with
CNNs, the multi-head self-attention scheme in Trans-
former captures long-distance dependencies so that dif-
ferent object/body parts can be better focused. In ad-
dition, Transformer retains more detailed information
without down-sampling operators. Therefore, it builds
a pure Transformer baseline for supervised image-based
single modality object Re-ID, following in a similar way
to ViT (Dosovitskiy et al., 2020), as shown in Fig. 4.
Even simply replacing the feature extraction network in
basic Re-ID methods with vision transformers, the per-
formance on multiple vehicle and person Re-ID datasets
is comparable to state-of-the-art methods, reflecting the
strong potential of Transformers for Re-ID tasks. In-
spired by this, some subsequent methods design special
transformer architectures such as pyramid structure (Li
et al., 2022b), hierarchical aggregation (Zhang et al.,
2021a; Tan et al., 2023), graph structure (Shen et al.,
2023a), etc., while some other methods intent to im-prove the attention mechanisms (Chen et al., 2021b;
Zhu et al., 2022a; Tian et al., 2022; Shen et al., 2023b).
Considering the mutual cooperation of CNN and
Transformer, Li et al. (Li et al., 2022b) develop a pyra-
midal transformer structure like CNN to learn multi-
scale features and improve the patch embedding pro-
cess, utilizing convolution with the anti-aliasing block
to capture translation-invariant information. Similarly,
based on hierarchical features extracted by CNN, HAT
(Zhang et al., 2021a) is proposed to aggregate fea-
tures of different scales in a global view with the help
of Transformer. GiT (Shen et al., 2023a) introduces
graphs in transformers to mine relationships of nodes
within the patch.
For the improvement of attention schemes in Trans-
former, Zhu et al. (Zhu et al., 2022a) present a dual
cross-attention learning strategy by emphasizing the
interaction between the global image and local high-
response regions and the interaction between image
pairs. Considering the impact of variable appearances
of the same identity, Shen et al. (Shen et al., 2023b) in-
troduce cross-attention in the Transformer encoder to
merge information from different instances. From the
perspective of improving Transformer efficiency in Re-
ID, Tian et al. (Tian et al., 2022) present a hierarchical
walking attention, by introducing a prior as an indicator
to decide whether to skip or calculate a region’s atten-
tion matrix in the image patch. For lightweight Re-ID
models, Mao et al. (Mao et al., 2023) design an atten-
tion map-guided Transformer pruning method, so that
the models can be deployed on edge devices with lim-
ited resources. It removes redundant tokens and heads
in a hardware-friendly manner, achieving the goal of re-
ducing the inference complexity and model size without
sacrificing the accuracy of Re-ID.
Re-ID-specific Design. For different objects and
Re-ID-specific challenges, many works explore the ap-
plication of Transformer to make Re-ID-specific adapta-
tions (Zhu et al., 2021b; Lai et al., 2021; Li et al., 2021e).
For the most crucial local discriminative information
mining for Re-ID, Vision Transformer naturally has at-
tention and patch embeddings, which can be easily used
to capture local discriminative information to enhance
the representation (He et al., 2021a; Li et al., 2021e; Lai
et al., 2021; Zhu et al., 2021b). Furthermore, the disen-
tanglement of some key information can be modeled by
the encoder-decoder structure in Transformer (Li et al.,
2021e; Wang et al., 2022c; Zhou et al., 2022b). Besides,
the structure prior (Chen et al., 2022a) or task special-
ties (Chen et al., 2022c) of different objects are also
important for Transformer design.
The effectiveness of learning local feature represen-
tation has been proven by extensive Re-ID research,10 Mang Ye, Shuoyi Chen, Chenyue Li, Wei-Shi Zheng, David Crandall, Bo Du
…TransformerTransformerCNNTransformer
Transformer
…TransformerTransformerTransformerTransformer…Patch to NodeGraph(a) basic structure(b) pyramid structure(c) hierarchical aggregation(d) graph structure
Fig. 5: Different Transformer architectures designed for image-based Re-ID. (a) The basic Re-ID baseline
based on Vision Transformer (He et al., 2021a). (b) Pyramid Transformer for learning multi-scale features (Li et al.,
2022b). (c) Transformer and CNN hybrid architecture for aggregating hierarchical features (Zhang et al., 2021a).
(d) Combination of graph structure and Transformer (Shen et al., 2023a).
and adding an attention mechanism to the CNN to fo-
cus on more discriminative information is a mainstream
practice in previous studies ( §2.1.2). However, many
new ideas that use the special properties of Transform-
ers to learn local features are also emerging and grow-
ing rapidly (Qian et al., 2022). TransReID (He et al.,
2021a) designs shift and patch shuffling operations on
the transformer baseline to learn local features, which is
conducive to enhancing disturbance invariance and ro-
bustness. Zhu et al. developed an Auto-Aligned Trans-
former (AAformer) (Zhu et al., 2021b) to adaptively
locate the human parts. It learns local representations
by introducing learnable part tokens to the transformer
and integrates part alignment into self-attention. Zhang
et al. revealed that self-attention leads to the inevitable
dilution of high-frequency components of images. To en-
hance the feature representation of high-frequency com-
ponents that are important to Re-ID, they proposed to
use Discrete Haar Wavelet Transform (DHWT) (Mal-
lat, 1989) to split the patches of high-frequency compo-
nents as the auxiliary information (Zhang et al., 2023a).
For vehicle Re-ID, feature misalignment caused by pose
and viewpoint variations is a key challenge. Previous
methods addressed this by aligning features based on
additional annotations of vehicle parts. To make feature
decomposition more flexible and unstructured, (Qian
et al., 2022) leverages Transformers to decouple vehi-
cle features across the spatial dimension, enabling fine-
grained feature learning on a global scale. Moreover,
Transformers are highly effective in establishing inter-
actions between semantic knowledge related to Re-ID
and visual features. MsKAT (Li et al., 2022a) intro-
duces a state elimination Transformer to remove inter-
ference from cameras and viewpoints, as well as an at-
tribute aggregation Transformer to gather information
on vehicle attributes such as color and type.
3.1.2 Transformer in Video-based Re-ID
Transformer for Post-processing. Video Re-ID
aims to fully exploit the temporal and spatial inter-actions of frame sequences to extract more discrimi-
native representations (Wu et al., 2022a). Compared
with CNN-based methods that require additional mod-
els to encode time information, Transformer is pro-
posed as a powerful architecture for processing sequence
data, which has inherent advantages. The global atten-
tion mechanism in Transformer can be easily adapted
to video data to capture spatio-temporal dependen-
cies (Tang et al., 2022). A group of Transformer-based
video Re-ID methods are hybrid architectures (Liu
et al., 2021a; Zhang et al., 2021c; He et al., 2021b).
They typically refer to processing the extracted fea-
tures from other models (such as convolutional neural
networks) before further processing them using a Trans-
former model. The primary use of Transformer lies in
its self-attention mechanism, which captures long-term
dependencies and contextual information within the se-
quence. Zhang et al. (Zhang et al., 2021c) designed
a two-stage spatio-temporal transformer module for
patch tokens converted from CNN feature maps, where
the spatial transformer focuses on object regions with
different backgrounds, while the subsequent temporal
transformer focuses on video sequences to exclude noisy
frames. Also based on the features extracted by CNN,
Liuet al. (Liu et al., 2021a) present a multi-stream
Transformer architecture that emphasizes three per-
spectives of video features via spatial Transformer, tem-
poral Transformer, and spatio-temporal Transformer. A
cross-attention based strategy is designed to fuse multi-
view cues to obtain enhanced features. Additionally,
some studies consider the complementary learning of
CNN and Transformer in space and time. Specifically,
DCCT (Liu et al., 2023b) introduces self-attention and
cross-attention to the features extracted by two sepa-
rate networks to establish a spatial complementary re-
lationship, and designs hierarchical aggregation based
on a temporal Transformer to integrate two temporal
features. DenseIL (He et al., 2021b) is a hybrid archi-
tecture consisting of a CNN encoder and a Transformer
decoder with dense interaction, where the CNN encoder
extracts discriminative spatial features while the de-Transformer for Object Re-Identification: A Survey 11
Table 2: Representative Transformer methods based on image/video Re-ID.
Category Focal Point Object Transformer Type Method Publication
Image-based Re-ID
Architecture
DesignPure Transformer Re-ID baseline Vehicle&Person ViT TransReID (He et al., 2021a) ICCV
Hierarchical feature aggregation Person Hybrid HAT (Zhang et al., 2021a) ACM MM
Introducing the benefits of CNN Person PVT PTCR (Li et al., 2022b) ACM MM
Improvements to attention Vehicle&Person ViT,DeiT DCAL (Zhu et al., 2022a) CVPR
Integrating graph structure Vehicle ViT GiT (Shen et al., 2023a) TIP
Re-ID-specific
DesignPartial representation learning Person Decoder PAT (Li et al., 2021e) CVPR
Introducing auxiliary information Person Encoder-Decoder PFD (Wang et al., 2022c) AAAI
Modeling relationships between individuals Person Transformer NFormer (Wang et al., 2022a) CVPR
Learning rotation-invariant features Vehicle&Person ViT RotTrans (Chen et al., 2022c) ACM MM
High-frequency augmentation Person ViT PHA (Zhang et al., 2023a) CVPR
Video-based Re-ID
Combination
of CNN &
TransformerTransformer for post-processing Person Decoder DenseIL(He et al., 2021b) ICCV
Coupled CNN-Transformer Person ViT/Swin/DeiT DCCT(Liu et al., 2023b) TNNLS
Pure
TransformerSpatial-temporal aggregation Person ViT MSTAT (Tang et al., 2022) TMM
Spatial-temporal joint modeling Person ViT CAViT (Wu et al., 2022a) ECCV
coder aims to densely model spatio-temporal interac-
tions across frames.
Pure Transformer. The hybrid architecture
makes it difficult to overcome the intrinsic limitations
of CNN for perceiving long-distance information. Some
recent work attempts to explore the application of
pure Transformer architecture to video Re-ID (Tang
et al., 2022; Wu et al., 2022a). Tang et al. (Tang
et al., 2022) designed a multi-stage Transformer frame-
work by taking advantage of Vision Transformer’s class
token to facilitate the aggregation of various infor-
mation. At different stages, the learning of attribute-
associated information, identity-associated information
and attribute-identity-associated information is guided
respectively. Besides, different from the mainstream
divide-and-conquer strategy that tackles feature rep-
resentation and feature aggregation separately that fail
to simultaneously solve temporal dependence, attention
and spatial misalignment, a contextual alignment Vi-
sion Transformer (CAViT) (Wu et al., 2022a) is pro-
posed for spatial-temporal joint modeling. To jointly
model spatio-temporal cues, it replaces self-attention
with temporal-shift attention based on a pure Trans-
former architecture to align objects in adjacent frames.
In video person Re-ID, occlusion remains a major chal-
lenge, as traditional convolution-based methods often
struggle to effectively handle occlusion and the mis-
alignment of adjacent frames, leading to a drop in recog-
nition performance. To address this issue, TCViT (Wu
et al., 2024) leverages Transformers, utilizing their at-
tention mechanisms to focus on the relative motion
and completeness of frame-level features, aligning the
frames and improving the visibility of the target per-son. This approach significantly enhances the model’s
ability to handle occlusion.
3.2 Transformer in ReID with Limited
Data/Annotations
In this survey, limited annotation usually corresponds
to unsupervised learning Re-ID technology ( §3.2.1),
while limited data mainly focuses on domain general-
ization in Re-ID ( §3.2.2). In fact, Transformer is still
in the preliminary exploration of such Re-ID scenar-
ios, with a small number of works but showing great
potential.
3.2.1 Transformer in Unsupervised Re-ID
Self-supervised Pre-training. Generally, the exist-
ing unsupervised Re-ID methods mainly rely on the fea-
tures extracted by CNN to cluster and generate pseudo-
labels as label supervision, in which CNN is supervis-
edly pre-trained on ImageNet (Zhang et al., 2022c; Dai
et al., 2022). However, supervised pre-training focuses
on coarse category-level distinction, which reduces the
rich visual fine-grained information in images. These
fine-grained cues are crucial for Re-ID tasks with a large
intra-class variation. A class of studies of Transformer
in unsupervised Re-ID emphasizes self-supervised pre-
training to obtain a better initialization model and re-
duce the domain difference between ImageNet data and
Re-ID data (Zhu et al., 2022b; Luo et al., 2021). The
success of self-supervised Transformers in vision tasks
provides a lot of modeling and training experience for12 Mang Ye, Shuoyi Chen, Chenyue Li, Wei-Shi Zheng, David Crandall, Bo Du
Table 3: Comparison of state-of-the-art supervised and unsupervised methods based on CNN and Transformer on
two widely used datasets Market-1501 (Zheng et al., 2015) and MSMT17 (Wei et al., 2018) in Person Re-ID. The
performance of different pre-training conditions is reported. The supervised TransReID-SSL results are obtained
by basic Transformer baseline (He et al., 2021a) fine-tuning and the unsupervised TransReID-SSL results are
obtained by Cluster-Contrast (Dai et al., 2022) fine-tuning. TransReID-SSL* refers to the results reproduced as a
baseline in our experiments.
Pre-training Conditions Market-1501 MSMT17
Method Venue Backbone Data Supervision mAP Rank-1 mAP Rank-1
State-of-the-art methods for supervised Re-ID
AGW (Ye et al., 2021c) TPAMI CNN ImageNet Supervised 87.8 95.1 49.3 68.3
CDNet (Li et al., 2021a) CVPR CNN ImageNet Supervised 86.0 95.1 54.7 78.9
TransReID (He et al., 2021a) ICCV Transformer ImageNet Supervised 89.5 95.2 69.4 86.2
PHA (Zhang et al., 2023a) CVPR Transformer ImageNet Supervised 90.2 96.1 68.9 86.1
TransReID-SSL (Luo et al., 2021) Arxiv Transformer LUPerson SSL 93.2 96.7 75.0 89.5
State-of-the-art methods for unsupervised Re-ID
ICE (Chen et al., 2021a) ICCV CNN ImageNet Supervised 82.3 93.8 38.9 70.2
ISE (Zhang et al., 2022c) CVPR CNN ImageNet Supervised 85.3 94.3 37.0 67.6
Cluster-Contrast (Dai et al., 2022) ACCV CNN ImageNet Supervised 83.0 92.9 31.2 61.5
Cluster-Contrast (Dai et al., 2022) ACCV CNN LUPerson SSL 84.0 94.3 31.4 58.8
PASS (Zhu et al., 2022b) ECCV Transformer LUPerson SSL 88.5 94.9 41.0 67.0
TransReID-SSL (Luo et al., 2021) Arxiv Transformer LUPerson SSL 89.6 95.3 50.6 75.0
TransReID-SSL* (Luo et al., 2021) Arxiv Transformer LUPerson SSL 89.9 95.2 48.2 72.8
UntransReID (Ours) - Transformer LUPerson SSL 90.7 95.7 51.1 75.7
unsupervised Re-ID research (Han et al., 2022). The ad-
vantages of Vision Transformer in unsupervised learn-
ing are reflected in several aspects: 1) The strong scal-
ability of the Transformer model for large-scale unla-
beled data. Self-supervised learning can fully use the
representation ability of the models with Transformer
architecture (Caron et al., 2021). 2) The flexibility of
the Transformer structure provides more diverse self-
supervised paradigms, which are extremely challenging
for CNNs to complete (He et al., 2022).
With the emergence of LUPerson (Fu et al., 2021), a
large-scale unlabeled dataset specifically for person Re-
ID, Luo et al. (Luo et al., 2021) began to initially ex-
plore effective Transformer self-supervised pre-training
paradigms for Re-ID, achieving significant results. They
first conduct extensive experiments to investigate the
performance of CNNs and Transformers using different
Self-Supervised Learning (SSL) methods on ImageNet
and LUPerson pre-training datasets. In addition, they
promote the stability and domain invariance of Trans-
former by designing the IBN-based convolution stem to
replace the standard patchify stem in ViT to enhance
the local feature learning. The conclusion is that Trans-
former is ahead of CNN in terms of pre-training. No-
tably, under the fully unsupervised condition of usingDINO (Caron et al., 2021) to pre-train the Transformer
on LUPerson and fine-tuning with a common unsuper-
vised Re-ID method (Dai et al., 2022), the performance
of Re-ID is even competitive with the state-of-the-art
supervised Re-ID method. It can be regarded as a ma-
jor breakthrough in the field of unsupervised Re-ID.
On this basis, the later PASS (Zhu et al., 2022b) fur-
ther integrated Re-ID-specific part-aware properties in
the self-supervised Transformer pre-training. Inspired
by DINO (Caron et al., 2021), which develops a sim-
ple strategy for label-free self-distillation, PASS in-
troduces several learnable tokens to extract part-level
features, further reinforcing fine-grained learning for
Re-ID. Specifically, it divides the image into several
fixed overlapping local regions and randomly crops lo-
cal views from them, while the global view is randomly
cropped from the whole image. In knowledge distilla-
tion, all views are passed through the student and only
the global view is passed through the teacher.
The pre-trained Transformer serves as a powerful
initialization model compared with previously widely-
used ImageNet pretraining. It can be fine-tuned with
different Re-ID supervised learning or unsupervised
learning methods in downstream tasks. As shown in
Table 3, the performances of corresponding methodsTransformer for Object Re-Identification: A Survey 13
have been greatly improved. We believe these research
efforts will be an advancement for the Re-ID commu-
nity, allowing future work to be performed with better
pre-trained models.
Unsupervised Domain Adaptation. Trans-
former has received limited attention for another widely
studied unsupervised domain adaptation (UDA) prob-
lem in unsupervised Re-ID, with a small amount of
work on vehicles and persons respectively (Wang et al.,
2022d; Wei et al., 2022). Different from the previous Re-
ID method based on domain alignment to guide feature
learning to achieve distribution consistency at the do-
main level or identity level, Wang et al. (Wang et al.,
2022d) oriented person Re-ID to achieve fine-grained
domain alignment between different body parts with
the help of Transformer. They embed the transformer
layer into the feature extraction backbone and discrim-
inators respectively, where the backbone obtains the
class tokens representing each body part and the dis-
criminators extract the domain information contained
in each body part. Dual adversarial learning is in-
troduced in the backbone and discriminator to align
each class token of a target domain sample with the
corresponding class token in the source domain. An-
other vehicle-oriented Transformer work belongs to the
clustering-based UDA solution (Wei et al., 2022). The
core idea is to make the Transformer adaptively focus
on the discriminative part of the vehicle in each domain
through a joint training strategy. To achieve dynamic
knowledge transfer, both source and target images are
simultaneously fed into a shared CNN to obtain fea-
ture maps, and the Transformer encoder-decoder archi-
tecture is subsequently introduced to generate a global
feature representation integrating contextual informa-
tion from the feature maps. Based on Transformer, a
learnable domain encoding module similar to positional
encoding is added to better utilize the specific charac-
teristics of each domain.
3.2.2 Transformer in Generalized Re-ID
The application of the Transformer promotes new ideas
of Re-ID in the challenging problem of domain gener-
alization (DG) (Liao and Shao, 2021). Completely dif-
ferent from mainstream research that uses Transformer
for feature representation learning, TransMatcher (Liao
and Shao, 2021) studies Transformer for image match-
ing and metric learning for a given image pair from
the perspective of generalizability. For Re-ID, a typi-
cal image matching and metric learning problem, the
Transformer encoder can only facilitate the feature in-
teraction between different positions within an image
but fails to realize the interaction between different im-ages. Liao et al . (Liao and Shao, 2021) also demon-
strate that directly applying vanilla vision Transformer
ViT through a classification training pipeline will re-
sult in poor generalization to different datasets. In-
spired by the cross-attention module in the Trans-
former decoder that enables cross-interaction between
query and encoded memory, they attempt to use ac-
tual image queries instead of learnable query embed-
dings as the input to the decoder to gather informa-
tion across query-key pairs, effectively boosting perfor-
mance. Further, TransMatcher is designed as a simpli-
fied decoder more suitable for image matching, which
discards all attention implementations with softmax
weighting and only keeps query-key similarity compu-
tation. This study demonstrates that Transformer can
be effectively adapted to image matching and metric
learning tasks with strong potential, and now it has
been widely used in later research (Ni et al., 2023; Wang
et al., 2023c) to improve the generalizability.
In addition, researchers try to investigate the gen-
eralization ability of Transformer in Re-ID. Ni et al.
(Ni et al., 2023) employed different Transformers and
CNNs as the backbone to assess the cross-domain per-
formance from Market (Zheng et al., 2015) to MSMT
(Wei et al., 2018). The results indicate that Vision
Transformers outperform CNNs significantly. On this
basis, a proxy task is introduced which mines local sim-
ilarities shared by different IDs based on part aware
attention, to promote the Transformer to learn gener-
alized features without using ID annotations.
In terms of generalization, Transformers have shown
great promise by effectively focusing on the object of in-
terest and learning domain-invariant features that can
transfer well across different environments. This abil-
ity to capture robust, generalized features is one of
the key strengths of Transformers, making them suit-
able for complex Re-ID tasks across various settings.
However, when it comes to generalizing across different
types of objects, the limitations of Transformers be-
come more apparent. Their attention mechanism may
struggle to adapt to significant variations between ob-
ject categories, particularly when the intra-class varia-
tions are subtle, but the inter-class differences are sub-
stantial.
3.3 Transformer in Cross-modal Re-ID
In this subsection, we summarize the Transformer
progress of three types of cross-modal problems that
have received more attention in Re-ID: visible-image
(§3.3.1), text-image ( §3.3.2) and sketch-image ( §3.3.3).
Recently, Transformer has made a lot of novel works
and influential breakthroughs in multi-modal learning14 Mang Ye, Shuoyi Chen, Chenyue Li, Wei-Shi Zheng, David Crandall, Bo Du
in the field of vision. The primary advantage of the
Transformer is that its input can include multiple se-
quences of tokens, each with distinct attributes, facili-
tating the association of different modalities via the at-
tention mechanism without necessitating any changes
to the architecture (Xu et al., 2023).
3.3.1 Visible-Infrared Re-ID
Visible-infrared Re-ID is a cross-modal retrieval task
that aims at matching the daytime visible and night-
time infrared images (Wu et al., 2017; Ye et al., 2019a).
The major challenge of visible-infrared Re-ID is the
modality gap between two types of images. The ap-
plication of the Transformer provides many benefits
to the visible-infrared Re-ID problem. For example,
Transformers tend to learn shape and structure infor-
mation, while CNNs rely on local texture information
(Naseer et al., 2021). Due to the lack of colors and light-
ing conditions in infrared images, Vision Transformer
can better capture modality-invariant information and
has stronger robustness. On the other hand, the vision
transformer structure and attention enable local cross-
modal associations to be easily established at the patch
token level, which is essential for fine-grained properties
of Re-ID, especially under a large modality gap.
The mainstream approach in existing visible-
infrared Re-ID is to learn modality-shared features,
which decouple features into modality-specific and
shared-modal features, and then focus on modality
alignment at the feature level. Jiang et al. (Jiang et al.,
2022) started trying to adopt Transformer’s encoder-
decoder architecture for modal feature enhancement
and compensation to promote better alignment of RGB
and IR modalities. They separately construct two sets
of learnable prototypes for RGB and IR modalities to
represent global modality information. In the Trans-
former decoder, the IR prototype is regarded as a query
for the RGB modality and the part features at the token
level of the RGB samples are used as keys and values.
Compensated IR modal features are obtained by aggre-
gating partial features through the correspondence of
cross-attention between partial features and modal pro-
totypes. In contrast, Liang et al. (Liang et al., 2023) in-
troduce learnable embeddings to mine modality-specific
features in Transformer in a manner similar to posi-
tional encoding, and employ a modality removal process
to subtract the learned modality-specific features.
Considering the specificity of the body part position
of the person object, Chen et al. (Chen et al., 2022b) be-
lieve that position interaction can discover the underly-
ing structural relationship between regions and provide
more stable invariance for pose changes. A structure-Table 4: Comparison of the state-of-the-art image-text
Re-ID methods based on Transformer.
- CUHK-PEDES ICFG-PEDES
Method mAP R1 mAP R1
w/o CLIP
LGUR (Shao et al., 2022) - 65.3 - 59.0
IVT (Shu et al., 2022) - 65.7 - 56.0
UniPT (Shao et al., 2023) - 68.5 - 60.1
w/ CLIP
TP-TPS (Wang et al., 2023a) 66.3 70.2 42.8 60.6
UNIReID Chen et al. (2023a) - 68.7 - 61.3
C-Fine(Yan et al., 2022) - 69.6 - 60.8
IRRA (Jiang and Ye, 2023) 66.1 73.4 38.1 63.5
TBPS-C (Cao et al., 2024) 65.4 73.5 39.8 65.1
MALS (Yang et al., 2023c) 66.6 74.1 38.9 64.4
MLLM(Tan et al., 2024) 69.6 76.8 41.5 67.0
aware position transformer (SPOT) is proposed to ex-
tract modality-shared representations. It exploits the
attention mechanism to learn structure-related features
guided by human key points and adaptively combines
partially recognizable cues by modeling context and po-
sition relations through a transformer encoder. Addi-
tionally, Feng et al. (Feng et al., 2022) focus on the in-
teraction of local features across modalities, where they
leverage attention to enrich the feature representation
of each patch token by interacting with patch tokens
from other modalities. Yang et al. (Yang et al., 2023a)
argue that each token in the self-attention mechanism
in ViT is connected to a class token, where the atten-
tion score can be intuitively interpreted as a measure of
token importance. To better align features of different
modalities, they select top-k important visual patches
from each attention head for localizing important image
regions. Focusing on the modal invariant information of
shallow features such as texture or contour information,
Zhao et al. (Zhao et al., 2022) utilize Transformer to en-
code the spatial information of each convolution stage
of CNNs to fuse shallow and deep features to enhance
the representation.
3.3.2 Text-Image Re-ID
Text-Image Re-ID refers to a cross-modal retrieval task,
which aims at identifying the target object (person or
vehicle) from an image gallery based on a given textual
query, describing the target appearance (Ye et al., 2015;
Li et al., 2017).
CLIP in Re-ID. As a milestone work of Trans-
former in multimodal applications, the proposal of Con-
trastive Language-Image Pre-training (CLIP) (Rad-
ford et al., 2021) opened up a new era of large-scale
pre-training for text-image communication. CLIP uses
text information to supervise the self-training of vision
tasks, which turns a classification task into an image-Transformer for Object Re-Identification: A Survey 15
text matching task. During the training process, a two-
stream network, including an image encoder and a text
encoder processes text and image data respectively, and
contrastive learning is used to learn the matching re-
lationship between text-image pairs. The pre-trained
model directly performs zero-shot image classification
without any training data, achieving comparable su-
pervised accuracy. Recently, CILP has become a pow-
erful tool for downstream text-image Re-ID tasks. Some
Re-ID works directly introduce CLIP as a pre-training
model with good generalization (Han et al., 2021) or
further expand the design of cross-modal association
mining (Jiang and Ye, 2023; Yan et al., 2022; Zuo et al.,
2023), and some works focus more on the utilization of
Re-ID related textual information in CLIP to better
assist downstream Re-ID tasks (Li et al., 2023a; Wang
et al., 2023a).
Considering the effectiveness of directly fine-tuning
CLIP, Yan et al. (Yan et al., 2022) explore the trans-
fer of CLIP models to text-image Re-ID. Based on the
pre-trained CLIP, they further capture the relation-
ship between image patches and words to build fine-
grained cross-modal associations. Inspired by CLIP,
Zuoet al. (Zuo et al., 2023) propose a language-image
pre-training framework PLIP that is more suitable
for person objects. To explicitly establish fine-grained
cross-modal relations, a large-scale person dataset con-
structed with stylish generated text descriptions is pro-
posed and three pretext tasks are introduced. The first
is semantic-fused image coloring, which recovers the
color information of gray-scale person images given a
textual description. The second is visual-fused attribute
prediction, which predicts masked attribute phrases in
text descriptions through paired images. The last is
visual-language matching. Instead, with CLIP as the
initialization model, IRRA (Jiang and Ye, 2023) de-
signs a cross-modal implicit relation reasoning mod-
ule to efficiently construct the relation between visual
and textual representations through self-attention and
cross-attention mechanisms. This fused representation
is used to perform masked language modeling (MLM)
task without any additional supervision and inference
costs, achieving the purpose of effective inter-modal
relation learning. He et al. (He et al., 2023b) devel-
oped a CLIP-driven framework focusing on fine-grained
cross-modal feature alignment. They proposed a Vision-
Guided Semantic Grouping Network, which mitigates
the misalignment of fine-grained cross-modal features
by semantically grouping textual features and aligning
them with visual concepts. Additionally, Wang et al.
(Wang et al., 2023a) aim to enhance text-based person
search by leveraging the dual generalization capabilities
of Vision-Language Pre-training (VLP) models. Thepaper focuses on fully exploring the potential of tex-
tual representations, utilizing pre-trained transferable
knowledge in text, and proposes two strategies tailored
to descriptive corpora. Compared with the previous
method which utilizes single-modal pre-trained external
knowledge and lacks multi-modal corresponding infor-
mation, these CLIP-based text-image Re-ID methods
have achieved significant performance improvements.
In addition to text-image Re-ID, some works also
leverage CLIP to provide text-based auxiliary informa-
tion to enhance image-based Re-ID (Li et al., 2023a;
Yang et al., 2024). Compared with the one-hot label
of image classification, CLIP-Re-ID (Li et al., 2023a)
demonstrates that more detailed image text descrip-
tions can help the visual encoder learn better image
features, especially for fine-grained tasks such as Re-ID
that lack precise descriptions. Inspired by the learnable
prompt used by CoOp (Zhou et al., 2022a), CLIP-Re-
ID designs a two-stage training strategy. It combines
ID-specific learnable tokens to give ambiguous textual
descriptions in the first stage and these tokens together
with the text encoder provide constraints for optimiz-
ing the image encoder in the second stage. Building on
this, Yang et al. (Yang et al., 2024) argue that prede-
fined soft prompts may not be sufficient to capture the
full visual context and are difficult to generalize to un-
seen categories. They design an end-to-end PromptSG
framework, instead of a two-stage learning process, to
leverage CLIP’s inherent rich semantics. By utilizing
an inversion network to learn representations of spe-
cific, more detailed appearance attributes, the frame-
work can create more personalized descriptions for in-
dividuals, further enhancing image-based Re-ID.
Considering that manually annotating textual de-
scriptions limits the dataset scale, Tan et al. (Tan
et al., 2024) are the first to explore the transferable
text-to-image ReID problem. Specifically, they lever-
age large-scale training data obtained through mul-
timodal large language models (MLLM) to train the
model and directly deploy it for evaluation across var-
ious datasets. They propose a method to build large-
scale datasets with diverse textual descriptions by using
MLLM’s multi-turn dialogues to generate captions for
images based on various templates. To address the issue
of MLLM potentially generating incorrect descriptions,
they leverage a Transformer-based approach to auto-
matically identify words in the description that do not
correspond to the image. This is achieved by analyzing
the similarity between the textual content and all patch
token embeddings within the image.16 Mang Ye, Shuoyi Chen, Chenyue Li, Wei-Shi Zheng, David Crandall, Bo Du
3.3.3 Sketch/Skeleton Re-ID
Sketch-to-photo Re-ID represents a cross-modal match-
ing problem whose query sets are sketch images pro-
vided by artists or amateurs (Yang et al., 2019), while
the query images in skeleton Re-ID are generated by
pose estimation (Rao and Miao, 2023; Rao et al., 2024).
These two tasks share similar spirits in large informa-
tion asymmetry.
Sketch-image Re-ID. The significant difference
in region-level information between sketches and im-
ages is a challenge due to the abstraction and iconog-
raphy of sketch images. The correlation between object
shape and local information plays an important role
in sketch-image Re-ID. The advantage of Transformer
in learning global-level feature representations shows
excellent discriminative ability in sketch photo recog-
nition (Chen et al., 2022a; Hong et al., 2021). Chen
et al. (Chen et al., 2022a) experimentally verified that
the method using ViT as the backbone has a signifi-
cant performance improvement over most CNN-based
sketch-image Re-ID methods. Therefore, they construct
a strong baseline based on a vision transformer for
sketch-image Re-ID. In order to narrow the gap between
sketches and images, Zhang et al. (Zhang et al., 2022d)
designed a token-level cross-modal exchange strategy in
Transformer under the guidance of identity consistency
to learn modality-compatible features. Local tokens of
different modalities are classified into different groups
and assigned specific semantic information to construct
a semantically consistent global representation.
In particular, a new, challenging, and modality-
agnostic person Re-ID problem has recently been pro-
posed (Chen et al., 2023a). It comprehensively consid-
ers descriptive queries such as supplementary text or
sketch modalities for general images to achieve multi-
modal unified re-identification. Benefiting from CLIP,
UNIRe-ID (Chen et al., 2023a) developed a simple dual-
encoder transformer architecture for multimodality fea-
ture learning and designed a task-aware dynamic train-
ing strategy that adaptively adjusts the training focus
according to the difficulty of the task. This work demon-
strates the power of Transformer in multi-modal learn-
ing and also opens up the direction for the future pro-
motion of Re-ID.
Re-ID with Skeleton Data. Person Re-ID via
3D skeletons differs from traditional Re-ID, which re-
lies on visual appearance features such as color, out-
line, etc., and mainly utilizes the 3D locations of key
body joints to model unique body and motion repre-
sentations. TranSG (Rao and Miao, 2023) is proposed
as a general Transformer paradigm for learning feature
representations from skeleton graphs for person Re-ID.Its core idea is to model the 3D skeleton as a graph
and use Transformer for full-relational learning of body
joint nodes, which simultaneously aggregates key rela-
tionship features of body structure and motion into a
graph representation.
Discussion. For cross-modal Re-ID, Transformer-
based methods combined with large language models
have made significant progress, particularly in image-
text Re-ID, where many breakthroughs have been
achieved. However, for other cross-modal Re-ID tasks,
such as visible-infrared Re-ID, the challenges remain
more pronounced. While Transformers excel at cap-
turing complex relationships between modalities, cross-
modal learning in such tasks typically requires exten-
sive amounts of paired data to learn effective feature
alignment across domains. Unfortunately, such large-
scale datasets are often unavailable or difficult to col-
lect, limiting the scalability and generalization of these
methods.
3.4 Transformer in Special Re-ID Scenarios
We investigate that the vision Transformer is also ap-
plied to some more open and complex task settings in
Re-ID. The more special Re-ID types than the scenar-
ios mentioned above are summarized in this subsection
for discussion.
Occluded Re-ID. Occluded Re-ID is a variant of
object Re-ID that deals with the challenge of partial oc-
clusions in images (Jia et al., 2022a; Wang et al., 2022c;
Xu et al., 2022; Ye et al., 2022). In occluded Re-ID, part
of the person or object is blocked by obstacles (e.g.,
other people, objects, or structures), making it harder
for models to extract full identity information. Recently
Transformer-based methods have made great contribu-
tions to address the occlusion challenge of Re-ID (Wang
et al., 2023b; Mao et al., 2023; Zhou et al., 2022b; Cheng
et al., 2022b). Extracting partial region representation
features is also a good solution to the challenge of object
occlusion in Re-ID. Part-Aware Transformer (PAT) (Li
et al., 2021e) is proposed to exploit the Transformer
encoder-decoder architecture to capture different dis-
criminative body parts, where the encoder is used to
obtain pixel context-aware feature maps and the de-
coder is used to generate part-aware masks. To ad-
dress the issues of misalignment and occlusion, He et al.
(He et al., 2023a) proposes using CLIP (Radford et al.,
2021) to capture both discriminative and invariant re-
gional features. A region generation module is designed
to automatically search for and locate more discrimi-
native regions. For the widely studied person object,
due to the occlusion noise or the occlusion region is
similar to the target, an intuitive solution is to guideTransformer for Object Re-Identification: A Survey 17
(b) UAVRe-ID
Changing Group Members
Different Clothing(a) Occluded Re-ID(c) Group Re-ID(d) Cloth-changingRe-ID
ImageDetectionOccluded by a personOccluded by an umbrella
GroundUAV
Fig. 6: Description of the characteristics of special Re-ID scenarios.
local feature learning with the help of human pose in-
formation. Wang et al. (Wang et al., 2022c) also adopt
the transformer encoder-decoder structure to present
a pose-guided feature disentangling method. With the
keypoint information captured by the pose estimator,
a set of learnable semantic views are introduced into
the decoder to implicitly enhance the disentangled body
part features. Similarly, assisted by motion information,
Zhou et al. (Zhou et al., 2022b) utilize keypoint detec-
tion and part segmentation for Transformer encoder-
decoder modeling. Besides, some transformer studies
analyze occlusion problems from other perspectives (Xu
et al., 2022; Cheng et al., 2022b). Xu et al. (Xu et al.,
2022) design a feature recovery Transformer (FRT) to
recover occluded features using nearby target informa-
tion. Considering the similarity between the local in-
formation of each semantic feature in k-nearest neigh-
bors and the query, FRT filters out noise to restore
the occluded query feature. Cheng et al. (Cheng et al.,
2022b) utilize knowledge learned from different source
datasets to generate reliable semantic clues to alleviate
domain differences between off-the-shelf semantic mod-
els and Re-ID data. Transformer allows human pars-
ing results to be embedded as learnable tokens into
the input, where a weighted sum operation is employed
to integrate parsed information from multiple sources.
Besides, given that the self-attention in Transformers
primarily emphasizes low-level feature correlations, it
inherently limits higher-order relations among different
body parts or regions, which are particularly crucial
for occluded person Re-ID. To address this, Li et al.
(Li et al., 2024b) introduces a second-order attention
module, which extracts contextual information from at-
tention weights using spectral clustering techniques.
Cloth-changing Re-ID. It is a challenging Re-
ID task in long-term scenarios where a person may
change clothes in an unknown pattern. Cloth-changing
Re-ID is a challenge unique to persons, which is a diffi-
cult but more practical problem (Liu et al., 2023a). In
this scenario, the discriminative feature representationdominated by the visual appearance of clothing will be
invalid. Existing research tackling this more intricate
challenge has initiated initial investigations into the ap-
plication of Transformers. Lee et al. (Lee et al., 2022)
evaluate different backbones in the cloth-changing Re-
ID scenario, and Transformer demonstrated notable
performance advantages when compared to CNNs. On
this basis, in order to further eliminate the influence
of characteristics related to clothing or accessories, an
attribute de-biasing module is designed. The core idea
is to use the generated attribute labels for person in-
stances as auxiliary information and adopt a gradient
reversal mechanism based on adversarial learning to
learn attribute-agnostic representations.
Human-centric Tasks. The success of the general
large model built by Transformer lies in its ability to
handle multiple tasks. Recent work has attempted to
focus on human-centric general model design to facil-
itate the Re-ID task. Human-centric perception inte-
grates visual tasks such as pedestrian detection, pose
estimation, attribute recognition, and human parsing.
Person Re-ID is one of the human-centric tasks. These
tasks all have in common that they rely on the ba-
sic structure of the human body and the properties of
body parts. Previous studies have experimentally veri-
fied that training human-centric tasks together can ben-
efit each other (Ci et al., 2023). It is challenging to
unify large-scale multiple tasks into a scalable model
due to the different structure and granularity of anno-
tations and expected outputs of different tasks requir-
ing separate output headers for each task. UniHCP (Ci
et al., 2023) presents a flexible Transformer encoder-
decoder structure to avoid task-specific output heads.
The core idea is to define task-specific queries in the de-
coder and design a task-guided interpreter to interpret
each query token independently. Outputs of the same
modality share the same output unit, enabling maxi-
mum parameter sharing among all tasks while learning
human-centric knowledge at different granularities. Ad-
ditionally, Tang et al. (Tang et al., 2023) established a18 Mang Ye, Shuoyi Chen, Chenyue Li, Wei-Shi Zheng, David Crandall, Bo Du
large-scale dataset HumanBench, specifically designed
for human-centric pre-training. To address task con-
flicts arising from diverse annotations in supervised pre-
training, a projector-assisted hierarchical pre-training
method is proposed. The core idea involves constructing
a hierarchical structure: sharing the weights of the back-
bone across all datasets, while restricting the weights of
the projector to be shared only among datasets of the
same tasks, and the weights of the head to be shared
for a single dataset.
On the other hand, unlabeled person data is plen-
tiful, and researchers use Transformer’s strong scala-
bility for large-scale self-supervised training to learn
human-centric representations. Considering methods
such as contrastive learning or masked image model-
ing that failed to explicitly learn semantic information,
SOLIDER (Chen et al., 2023b) uses Transformer to
generate pseudo semantic labels for every token based
on prior knowledge of human images and introduces a
token-level semantic classification pretext task to learn
a stronger human semantic representation. With the
mutual promotion of large-scale learning in multiple
human-centric tasks, SOLIDER can achieve superior
performance compared with the state-of-the-art unsu-
pervised pre-training methods (Luo et al., 2021; Zhu
et al., 2022b) for Re-ID, which can be regarded as a
further advance in the Re-ID community.
Person Search. Person search is an end-to-end
method that aims to jointly solve the two sub-problems
of detection and person Re-ID using more efficient
multi-task learning methods. Since the goals of per-
son detection and person Re-ID are conflicting and it
is difficult to jointly learn a unified feature representa-
tion, Yu et al. (Yu et al., 2022) propose to decompose
feature learning into successive steps in the T stage
of a multi-scale Transformer to gradually learn from
coarse to fine embedding. Unlike some existing multi-
scale Transformers that learn different scale informa-
tion based on patches of different sizes, they leverage
a series of convolutional layers with different kernels
to generate multi-scale tokens. Furthermore, to pro-
duce more occlusion-robust representations, they de-
sign to exchange partial tokens of instances in mini-
batches, and then compute occlusion attention based on
mixed tokens. Different from the shuffling and regroup-
ing strategy in TransReID (He et al., 2021a), they are
for partial tokens in a single instance. PSTR (Cao et al.,
2022) is also designed as a person search multi-scale
learning scheme of the Transformer architecture. It de-
velops a PSS module consisting of a detection encoder-
decoder and a discriminative re-identification decoder,
where the detection encoder-decoder employs backbone
features and three cascaded decoders are employed. The
…Patch-level Mask
FeaturesPseudo Labels
Vision Transformer
Negative Sample…PositiveSampleFig. 7: The proposed unsupervised Transformer base-
line for Re-ID enhanced with a patch-level mask learn-
ing strategy.
Re-ID decoder takes a feature query from one of the
three detection decoders as input, and a multi-level su-
pervision scheme is designed to provide different input
Re-ID feature queries and box sampling locations. In
order to achieve multi-scale expansion, the features of
different layers use PSS modules and are concatenated
to perform instance-level matching with queries.
Group Re-ID. It is a group-level Re-ID by using
the contextual information to match a small number
of individuals in a group together (Zheng et al., 2009).
Since people usually have group and social attributes,
group actions are preferred in most real-world scenarios.
Group Re-ID has gradually attracted the attention of
researchers, which needs to deal with challenges such as
membership and layout changes. Existing group Re-ID
methods are mainly based on the combined framework
of CNN and GNN. However, these structures are de-
ficient in position modeling and have weak ability to
describe group layout characteristics. Inspired by the
position embedding in the transformer, Zhang et al.
(Zhang et al., 2022b) design the second-order Trans-
former model SOT to deal with the layout features
in group Re-ID. It consists of intra-member and inter-
member modules, where each member in the group im-
age is first cropped, and then each member is segmented
into multiple sub-patches. The intra-member module
extracts first-order labels as per-member features by
modeling the relationship between sub-patches through
a transformer. The member-to-member module models
the relationship between members through uncertainty
and extracts second-order tokens through transformers.
Re-ID in UAVs. Object Re-ID in UAVs involves
identifying specific objects within a multitude of aerial
images captured from a dynamic bird’s-eye view (Or-
ganisciak et al., 2021). It also has broad application
prospects in different scenarios. Re-ID using aerial
images captured by UAVs is an under-explored sce-
nario. Unlike the widely used fixed cameras, the im-Transformer for Object Re-Identification: A Survey 19
ages captured by UAVs are more complex than fixed
city cameras. Unavoidable continuous rapid movement
and height changes lead to large differences in image
viewing angles. Chen et al. (Chen et al., 2022c) ana-
lyzed the vehicles and persons in the bird’s-eye view
and concluded that Re-ID faces two key challenges in
UAV scenarios: bounding boxes with significant size dif-
ferences and objects with uncertain rotation direction.
Benefiting from the corresponding relationship between
image patches and token-level features in Vision Trans-
former, the insight of this work is to simulate rotation
operations on the initially learned patch features to
generate enhanced diversity rotation features. Another
study (Ferdous et al., 2022) explores the enhancement
of the Pyramid Vision Transformer, leveraging multi-
scale features for object Re-ID in UAV scenarios. In ad-
dition to fully drone-view Re-ID, some research focuses
on the cross-view matching problem between aerial and
ground perspectives. (Zhang et al., 2024) proposed a
view-decoupled transformer to specifically address the
significant view discrepancy, aiming to decouple view-
related and view-independent components.
4 New Unsupervised Transformer Baseline
After conducting a thorough review of Transformer’s
work in Re-ID, we are confident that large-scale pre-
trained Transformers hold substantial promise for unsu-
pervised Re-ID and warrant further exploration. Most
previous Re-ID works are pre-trained on ImageNet,
due to the lack of large-scale person datasets. In fact,
pre-training on person datasets, such as LUPerson (Fu
et al., 2021), is better suited for Re-ID task and aligns
with future development trends. Our survey reveals
that some studies (Luo et al., 2021; Zhu et al., 2022b)
verify the evident advantages of using Transformer pre-
training on LUPerson. In order to further promote
the progress of the Re-ID community, we propose a
single/multi-modal general unsupervised Re-ID base-
line. Specifically, our baseline follows the Re-ID method
(Dai et al., 2022) of contrastive learning of pseudo-
labels generated by clustering, and uses the TransReID-
SSL (Luo et al., 2021) pre-trained Transformer as a
powerful initialization model. On this basis, leveraging
the characteristics of Transformer, we have devised the
following design for UntransReID.
Single-modal Unsupervised Re-ID. Inspired
by existing Transformer-based masked image model-
ing self-supervised methods (He et al., 2022; Xie et al.,
2022), we design a patch-level mask enhancement strat-
egy integrated into the unsupervised training process.
Our core idea is to adopt a series of learnable tokensTable 5: Evaluation results of our Transformer-based
visible-infrared cross-modal unsupervised Re-ID base-
line on RegDB (Nguyen et al., 2017) and SYSU-MM01
(Wu et al., 2017).
RegDB SYSU-MM01
Method V-T T-V All Search Indoor Search
mAP R1 mAP R1 mAP R1 mAP R1
OTLA 29.7 32.9 28.6 32.1 27.1 29.9 38.8 29.8
ADCA 64.1 67.2 63.8 68.5 42.7 45.5 59.1 50.6
ACCL 65.4 69.5 65.2 69.9 51.8 57.3 62.7 56.2
UntransReID 69.9 76.3 69.3 76.8 52.5 51.9 66.0 57.5
to mask part of the image patches as an augmenta-
tion and establish the relationship between the original
features and the mask features during the training pro-
cess as a supervisory signal to guide model learning.
On the other hand, aligning the mask features with
the original features inherently encourages the model
to learn local fine-grained information. For input im-
ages, we define the set Xg={xg
i|i= 1,2,···, n}and
setXl={xl
i|i= 1,2,···, n}respectively as the orig-
inal input and mask enhancement input. Patch em-
bedding operations are used to get preliminary tokens
xg
i∈RN×Dandxl
i∈RN×D, where N and D rep-
resent the number of patches and the dimension of
the token. We initialize a set of learnable mask tokens
Ml={ml
i|i= 1,2,···, m}, randomly replacing pof
the tokens in Xlas the final input. The corresponding
output class tokens after Transformer model learning
are{fg
i|i= 1,2,···, n}and{fl
i|i= 1,2,···, n}. We
calculate the contrastive loss between the original im-
age features and mask-enhanced features as:
L=−logexp(fg
i·fl
i/τ)
Pk
j=1exp(fg
i·fj/τ), (2)
where krepresents the batch size and fjrepresents the
original features within the batch.
Cross-modal Unsupervised Re-ID. For the
transformer-based unsupervised visible-infrared cross-
modal Re-ID, we devise a dual-path transformer that
adopts two modality-specific patch embedding layers
and a modality-shared transformer. Each modality-
specific patch embedding layer comprises an IBN-
based Convolution Stem (ICS) (Luo et al., 2021) to
capture modality-specific information. The modality-
shared transformer is introduced to learn a multi-
modality sharable space. On the basis of (Dai et al.,
2022), two modality-specific memories are constructed
for mining inter- and intra-class information within
each modality with contrastive learning. To further en-
sure the modality generalization capability, we adopt
random channel augmentation following (Ye et al.,
2021b) as an extra input to the visible stream for joint
learning.20 Mang Ye, Shuoyi Chen, Chenyue Li, Wei-Shi Zheng, David Crandall, Bo Du
Analysis of Results. Table 3 and Table 5 re-
spectively present the evaluation results for our single-
modal and cross-modal unsupervised Re-ID baselines.
For single-modal unsupervised Re-ID, the structural
characteristics of the Transformer allow us to gener-
ate augmented samples by applying local masks at
the patch level, enabling the construction of supervi-
sory signals. On the powerful Transformer backbone
pretrained on LUPerson (Luo et al., 2021), our base-
line combined with the enhancement strategy and con-
trastive learning (Dai et al., 2022), achieves perfor-
mance comparable to state-of-the-art methods. For
cross-modal Re-ID, three methods OTLA (Wang et al.,
2022b), ADCA (Yang et al., 2022), and ACCL (Wu
and Ye, 2023), are compared. Existing state-of-the-
art methods are based on CNNs and require complex
cross-modal association designs, whereas our Trans-
former baseline achieves state-of-the-art performance
with a simple design across several infrared-visible Re-
ID datasets.
5 Animal Re-Identification
In addition to objects such as persons and vehicles,
which are currently widely studied in the field of Re-
ID, the demand for wildlife protection makes animal
re-identification gradually attract the attention of re-
searchers (Kuncheva et al., 2022; Schneider et al., 2019).
Animal Re-ID has important applications in many
fields such as ecology, conservation biology, environ-
mental monitoring, popular science education, and agri-
culture. It provides a powerful tool for understanding
nature, protecting biodiversity, and maintaining ecolog-
ical balance. Considering that the existing Re-ID sur-
veys are all for persons or vehicles, our survey will cover
a wider range of Re-ID objects including diverse ani-
mals to promote Re-ID research. This section provides
an overview of recent work concerning animal Re-ID.
Different from the mature development of per-
son and vehicle re-identification technology, animal re-
identification technology is still in a relatively early
stage. One of the most intuitive challenges is that the
uncontrollable environmental factors in the wild make
it very complicated to collect animal images and la-
bel their individual information. Compared to humans,
animal species exhibit a diverse array of unique charac-
teristics across different species. In the supplementary,
we provide a more visually intuitive comparison of Re-
ID across different species. The core problem of animal
individual re-identification is to mine and analyze the
discriminative features specific to a species. Our survey
summarizes the animal Re-ID datasets released in re-cent years in §5.1. In addition, deep learning-based an-
imal re-identification techniques are introduced in §5.2.
5.1 Animal Re-ID Datasets
Due to the diversity of environments and ways in which
different animals live, the collection of animal data is
not as simple as that of persons and vehicles. In addi-
tion to using surveillance cameras and ordinary digital
cameras, camera traps, drones, and infrared thermogra-
phy are also important devices. Therefore, most animal
Re-ID datasets focus on annotating identity informa-
tion in order to complete specific individual recognition,
without a clear definition of camera. The emergence of
more and more animal Re-ID datasets in recent years
has promoted research progress (Li et al., 2019b; Zhang
et al., 2021e; Nepovinnykh et al., 2022). As shown in Ta-
ble 6, we provide an overview of animal Re-ID datasets
in recent years. We present key features of different
species in Re-ID, which are also special challenges in
the animal Re-ID task. In addition, some animal data
such as long-lived species are collected over a period of
several years. This kind of data with a long time span
provides more comprehensive information and supports
deeper analysis for long-term ecological research and
species protection (Papafitsoros et al., 2022). This im-
plies that Re-ID will be more challenging since the ap-
pearance of animals can change dramatically over time.
We also report the time span for each dataset.
Our survey collects animal Re-ID datasets from dif-
ferent sources in recent years to promote Re-ID re-
search, some of which are paper publications (Ko-
rschens and Denzler, 2019; Nepovinnykh et al., 2022;
Zuerl et al., 2023; Wang et al., 2021a; Zhang et al.,
2021e), and some in the form of competitions (Howard
et al., 2022; Li et al., 2019b). This is mainly due to the
fact that the datasets of many papers are not publicly
available, and datasets from diverse sources allow ad-
vanced research to be conducted on them. First, some
domestic or laboratory animal datasets are introduced.
Bergamini et al. (Bergamini et al., 2018) collect cat-
tle head images in farms for Re-ID, considering that
the cattle heads can show sufficient texture, shape and
patch characteristics. Li et al. (Li et al., 2021c) shot
at a real cattle farm and built a dataset of 13 cows.
The Cows2021 dataset (Gao et al., 2021) contains 186
Holstein-Friesian cattle, which took a month to capture
from a bird’s-eye view on the farm. For cattle, their per-
sonalized black and white coat pattern patches are an
important distinguishing characteristic of individuals.
YakRe-ID-103 (Zhang et al., 2021e) is a Yak dataset of
highland pasture scenes. Yaks mostly have black fur and
are typically texture-less animals, making it difficult toTransformer for Object Re-Identification: A Survey 21
coat pattern local details tusks, injury marks
discriminative features(a) ELPephants
iPanda -50
tails, unique markings
Images of the same individual
Images of different individuals
(b) ATRW (c) iPanda -50 (d) HappyWhale
Fig. 8: Images of different species in Animal Re-ID. Unlike the widely studied person and vehicle Re-ID,
animal individuals of the same species have extremely similar appearances. Different species have their own unique
discriminative characteristics, such as (a) the tusks and injury marks of elephants (Korschens and Denzler, 2019),
(b) the coat pattern of amur tiger(Li et al., 2019b), (c) the eyes of giant pandas (Wang et al., 2021a), (d) the
dorsal fin, back, and flank of whales (Howard et al., 2022).
Table 6: Summary of animal Re-ID datasets from recent years.
Dataset Species IDs Images Feature Span Source Available
CattleRe-ID(Bergamini et al., 2018) cattle - - face - farm ✕
DolphinRe-ID(Bouma et al., 2018) dolphin 185 3544 fin 12 years - ✕
Elpephants(Korschens and Denzler, 2019) elephant 276 2078 body, tusk 15 years national park ✕
ATRW(Li et al., 2019b) Amur tiger 92 3649 stripe - wild zoos ✓
zebrafishRe-ID(Bruslund Haurum et al., 2020) zebrafish 6 2224 side view - laboratory ✓
CowRe-ID(Li et al., 2021c) cow 13 3772 coat pattern - farm ✓
YakRe-ID-103(Zhang et al., 2021e) yak 103 2247 horn - highland pastures ✕
Cows2021 (Gao et al., 2021) cattle 182 13784 coat pattern 1 month farm ✓
iPanda-50 (Wang et al., 2021a) giant panda 50 6874 local - Panda Channel ✓
SealID(Nepovinnykh et al., 2022) seal 57 2080 pelage pattern 10 years Lake Saimaa ✓
FiveVideos (Kuncheva et al., 2022) pigeon,fish,pig 93 20490 - - Pixabay ✓
BelugaID (bel, 2022) beluga whale 788 5902 scarring pattern 4 years Cook Inlet ✓
Honeybee (Chan et al., 2022) honeybee 181 8962 abdomen multiple weeks colony entrance ✕
HappyWhale (Howard et al., 2022) 30 species 15587 51033 fin,head,flank very long 28 organizations ✓
SeaTurtleID (Papafitsoros et al., 2022) sea turtle 400 7774 - 12 years Laganas Bay ✓
LeopardID(leo, 2022) African leopard 430 6795 spot pattern 11 years - ✓
HyenaID (hye, 2022) spotted hyena 256 3104 spot pattern - - ✓
PolarBearVidID (Zuerl et al., 2023) polar bear 13 138,363 - - zoo ✓
Wildlife-71 (Jiao et al., 2023) 71 species ≈2059 ≈108,808 - - internet ✓
distinguish individuals. The most unique features are
the thickness, bending and direction of the horn. Be-
sides, a dataset of six zebrafish recorded in a labora-
tory setting is presented by (Bruslund Haurum et al.,
2020). They propose reliable re-identification through
the stripes of zebrafish from the side view.
Re-ID for wild animals is relatively more challeng-
ing due to complex animal habits and uncontrollable
environments. ATRW (Li et al., 2019b) is a dataset
containing 92 Amur tigers collected in multiple large
wild zoos with bounding boxes, pose key points and
identity annotations. Korschens et al. (Korschens and
Denzler, 2019) collected images of forest elephants in
national parks and constructed a data set containing2078 images of 276 elephant individuals. The dataset
spans approximately 15 years, which reflects some of
the aging effects and dramatic changes in the physique
of elephants. The most identifiable tusks and scars of
elephants also change over time, exacerbating the dif-
ficulty of Re-ID. Chan et al. (Chan et al., 2022) con-
structed a short-term and long-term dataset for honey-
bee re-identification. They mainly focused on the ab-
domen of honeybee for individual discrimination, and
the time span of the long-term dataset reached 13 days.
iPanda-50 (Wang et al., 2021a) is a giant panda Re-ID
dataset collected through giant panda streaming videos,
which contains 50 giant pandas of different ages includ-
ing cubs, juveniles, and adults. The Saimaa ringed seal22 Mang Ye, Shuoyi Chen, Chenyue Li, Wei-Shi Zheng, David Crandall, Bo Du
is an endangered subspecies found only in Lake Saimaa,
Finland. Individual ringed seals have unique fur pat-
terns, and individual re-identification is of great value
in monitoring endangered animals (Nepovinnykh et al.,
2022). SealID (Nepovinnykh et al., 2022) is a bench-
mark for Saimaa ringed seal re-identification, which
takes into account challenges such as the deformable
nature of seals and low contrast between the ring pat-
tern. SeaTurtleID (Papafitsoros et al., 2022) is a large-
scale dataset containing images of sea turtles captured
in the wild. This dataset is time-stamped and spans
up to 12 years. Considering the impact of timestamps
for unbiased evaluation of animal Re-ID methods, the
dataset also provides time-aware partitioning of refer-
ence and query sets. Happywhale (Howard et al., 2022)
is an open-source platform to facilitate the identifica-
tion of individual marine mammals. PolarBearVidID
(Zuerl et al., 2023) provides a video-based dataset of po-
lar bears, which is challenging because individuals lack
significant unique visual features. While the majority of
existing datasets are tailored to a single species, recent
research has introduced large-scale Re-ID datasets that
encompass multiple species. The Wildlife-71 dataset
(Jiao et al., 2023), proposed as a dataset that aggre-
gates existing datasets and partial web data, includes
Re-ID data from 71 different wildlife categories. Indeed,
it is observed that many existing animal datasets are
relatively small in scale, and aggregating multi-species
data proves advantageous for deep learning technolo-
gies. In addition, we observe that the animal Re-ID is
much less explored compared with other objects. Rec-
ognizing that animals suffer from additional severe oc-
clusions and viewpoint changes compared with persons.
5.2 Animal Re-ID Methods
In this survey, we mainly focus on advanced deep learn-
ing methods for animal Re-ID due to their power-
ful performance compared with other traditional solu-
tions. Our survey broadly categorizes these methods
into three groups: learning with global animal images,
learning with key local body areas, and learning with
auxiliary information. Note that Transformer is seldom
explored in this area.
Global Image Based Methods. Many existing
studies draw upon the conventional approaches of per-
son Re-ID, directly feeding entire animal images into
deep neural networks to acquire reliable feature repre-
sentations (Bouma et al., 2018). Taking cues from per-
son Re-ID methodologies like local maximal occurrence
(Liao et al., 2015), Bruslund et al. (Bruslund Haurum
et al., 2020) introduce two feature descriptors consist-
ing of color and texture to reliably re-identify zebrafishfrom side-view. Considering that the patterns on manta
rays are usually in uncertain positions, Moskvyak et al.
(Moskvyak et al., 2021) devise a loss function to mini-
mize the distance between the same individual observed
from various viewpoints, guiding the learning of pose-
invariant features. Porrello et al. (Porrello et al., 2020)
propose a general view knowledge distillation method
for Re-ID tasks. The core idea is to use the diversity of
the target in different views as a teaching signal, allow-
ing students to use fewer views to restore it and learn
more robust features. For giant panda re-identification,
Wang et al. (Wang et al., 2021a) design a multi-stream
structure to learn local and global features. In order to
mine local fine-grained information from the global im-
age, a patch detector is adopted to automatically cap-
ture the most discriminative local patches without ad-
ditional part annotations.
Local Area Based Methods. Among the related
work on animal Re-ID, some research focuses on specific
parts of the animal. They extract the most discrimina-
tive areas of the original image during the data collec-
tion stage, such as the head of a cow (Bergamini et al.,
2018), elephant ears (Weideman et al., 2020), whale
tails (Cheeseman et al., 2022), dolphin fins (Bouma
et al., 2018; Weideman et al., 2017; Konovalov et al.,
2018), etc. Bergamini et al. (Bergamini et al., 2018)
employ CNNs for direct feature extraction from self-
collected cattle head datasets and utilize KNN (k-
nearest neighbors) for classification. For fine-grained
images of elephant ears and whale tails, Weideman et
al.(Weideman et al., 2020) designed their approach
to extract boundary information from color and tex-
ture transitions, along with intensity variations, to ef-
fectively discern the outlines of critical regions.
Auxiliary Information Based Methods. Zhang
et al. (Zhang et al., 2021e) utilize a simplified definition
of the pose of the yak’s right or left head as an auxiliary
supervision signal to enhance feature learning. Li et al.
(Li et al., 2019b) employed the results of pose key point
estimation to model the tiger image into 7 parts includ-
ing the trunk, front legs, and hind legs to learn local
features. In order to learn the unique body markings of
animal individuals with similar appearance, Moskvyak
et al. (Moskvyak et al., 2020) proposed a heat map en-
hancement method to display the location information
of introduced animal landmarks in the Re-ID model.
When dealing with species exhibiting similar pelage or
fur patterns, Nepovinnykh et al. (Nepovinnykh et al.,
2020) employed the Sato tubeness filter to extract the
fur pattern from the image, mitigating the impact of in-
terfering factors like lighting. Siamese networks (Koch
et al., 2015) trained with triplet loss are used for sub-
sequent matching.Transformer for Object Re-Identification: A Survey 23
5.3 A Unified Benchmark for Animal Re-ID
In fact, existing deep learning-based animal Re-ID
methods are still in the early stages of development,
and we generally summarize their main limitations: (1)
Unclear Task Boundaries. Many animal-related studies
do not have clear task definitions, some of which are re-
garded as fine-grained recognition or individual classifi-
cation (Wang et al., 2021a). They typically concentrate
solely on distinguishing between different individuals
and pay little attention to whether they can be reliably
re-identified across various settings. However, in this
survey, we emphasize animal re-identification, with the
goal of accurately identifying the same individual across
different timeframes, environments, or viewpoints. (2)
Limited method applicability. Many existing methods
leverage the distinctive traits of particular species to de-
velop their approaches, with some specifically focusing
on curating datasets for certain body parts of the ani-
mals (Weideman et al., 2017; Nepovinnykh et al., 2020;
Bergamini et al., 2018; Chan et al., 2022). These ap-
proaches prove challenging to adapt for broader appli-
cation in Re-ID across different species and exhibit lim-
ited scalability. (3) Inconsistent experimental settings.
Existing animal Re-ID methods adopt varying experi-
mental settings. Some of the work is conducted experi-
mentally in a closed-world setting, which involves iden-
tifying objects within known and limited categories. In
most cases, the Re-ID system is not aware of all pos-
sible categories during training, necessitating its abil-
ity to handle unforeseen categories. Some research has
also been conducted in scenarios that better align with
the open-set nature of Re-ID tasks. This makes perfor-
mance comparison between different methods a chal-
lenging task.
To further advance research and realize the full po-
tential of animal re-identification for practical appli-
cations, it is critical to establish standardized bench-
marks and develop more robust, scalable techniques.
In this survey, we conducted extensive animal Re-ID
experiments using multiple state-of-the-art general Re-
ID methods to address the aforementioned issues. Our
work in this section covers a unified evaluation setting,
a comparison of different backbone methods, and an
analysis of the Transformer’s suitability for animal Re-
ID. The code will be publicly available.
5.3.1 Animal Re-ID experiments.
Datasets. We chose datasets featuring various species,
such as giant pandas (Wang et al., 2021a), elephants
(Korschens and Denzler, 2019), seals (Nepovinnykh
et al., 2022), giraffes (Parham et al., 2017), zebrasTable 7: Evaluation results of state-of-the-art Re-ID
methods on multiple animal datasets. Three methods,
BoT (Luo et al., 2019), TransReID (He et al., 2021a)
and RotTrans (Chen et al., 2022c), are compared.
- BoT TransReID RotTrans
Dataset mAP R1 mINP mAP R1 mINP mAP R1 mINP
iPanda-50 28.4 72.5 9.8 37.9 88.8 10.5 42.6 91.7 12.9
ELPephants 15.8 32.3 4.5 15.3 40.2 3.1 30.2 56.0 9.7
SealID 49.1 82.2 7.2 42.6 82.8 6.3 48.3 83.5 7.4
ATRW 65.2 98.4 32.5 64.1 98.3 33.0 66.9 97.9 35.4
GZGC-G 47.4 46.7 38.4 49.1 48.9 39.0 48.9 47.8 40.4
GZGC-Z 13.7 23.5 5.6 16.3 26.0 7.4 16.2 26.7 7.2
LeopardID 27.3 60.1 9.9 31.6 63.7 12.5 32.5 63.0 13.3
(Parham et al., 2017), leopards (leo, 2022) and tigers(Li
et al., 2019b) for our evaluation. Since the datasets only
provide original images and corresponding identity an-
notations, we uniformly divide them into training sets
and test sets for the Re-ID task. Specifically, we divide
each data set into 70% of all identities as training data,
and the remaining 30% as test data. Ensure that the
identities of the test set have not appeared in the train-
ing set. In the testing phase, we regard each image in
the test set as a query, and all images in the test set ex-
cept the query image constitute the gallery. The results
for FiveVideos in Table 7 are obtained using only pig
data. The results for GZGC-G and GZGC-Z are using
giraffe data and zebra data, respectively.
Evaluation Metrics. The performance is evalu-
ated by two widely used metrics in Re-ID tasks: Cumu-
lative Matching Characteristic (CMC) and the mean
Average Precision (mAP). It’s worth noting that in the
context of person and vehicle Re-ID, correctly matched
objects captured by the same camera are typically ex-
cluded from the evaluation, while only objects captured
by different cameras are considered. However, in ani-
mal Re-ID, where explicit camera information is often
lacking in most animal datasets, we calculate all cor-
rectly matched objects uniformly. In many cases, simple
samples with small viewing angle changes will lead to
high Rank-k accuracy. Therefore, we calculate the met-
ric mean Inverse Negative Penalty (mINP) (Ye et al.,
2021c), which reflects the cost of finding the hardest
matching sample.
Analysis of Results. To evaluate the perfor-
mance of different backbones in animal Re-ID, two
Re-ID methods that are generally applicable to vari-
ous objects, CNN-based BoT (Luo et al., 2019) and
Transformer-based TransReID (He et al., 2021a), are
employed in our experiments. As shown in Table 7,
the mean accuracy of existing state-of-the-art Re-ID
methods applied directly to animals is generally low.
This also underscores that Animal Re-ID, distinct from
the widely studied Re-ID objects, poses unique chal-
lenges and requires more targeted solutions in the fu-24 Mang Ye, Shuoyi Chen, Chenyue Li, Wei-Shi Zheng, David Crandall, Bo Du
ture. The Transformer method performs better in most
cases. In addition, considering some characteristics of
animal Re-ID that are different from conventional per-
son Re-ID such as camera view and diverse orienta-
tions, we choose a state-of-the-art Transformer-based
method of object Re-ID in UAVs which is mentioned
in§3.4, RotTrans (Chen et al., 2022c), for evaluation.
We believe that images of different species (e.g., marine
and terrestrial animals) will exhibit a variety of rota-
tion angles rather than being in a standing position
as persons. Consequently, RotTrans demonstrates su-
periority in most animal Re-ID scenarios as a method
that helps to learn rotation-invariant representations.
Recently, researchers have proposed the development of
a Re-ID model capable of handling any unseen wildlife
category (Jiao et al., 2023). The concept is similar to
the domain generalization problem in conventional Re-
ID tasks, and their solution involves leveraging larger
scale and more diverse data. Differently, our benchmark
is designed for the general animal Re-ID task, specifi-
cally aiming at the methodological level of multi-species
applicability.
6 Conclusion and Future Prospects
6.1 Under-Investigated Future Prospects
Re-ID Meets Large Language Models. The inte-
gration of large language models (LLMs) into Re-ID
tasks has emerged as a promising trend. In this context,
Re-ID benefits from the advanced capabilities of LLMs
in generating or processing textual descriptions that
complement image data. In fact, several preliminary
studies have already explored this direction: (1) Textual
Assistance. By generating or understanding textual de-
scriptions of visual data, LLMs provide more detailed
contextual information to enhance image-based Re-ID
performance (Li et al., 2023a; Yang et al., 2024). (2)
Cross-modal Image-Text Re-ID. LLMs leverage their
dual strengths in both the visual and textual modal-
ities to align visual features with natural language de-
scriptions, creating more robust representations for im-
proved identification (Jiang and Ye, 2023; He et al.,
2023b). (3) Unlabeled Data Utilization. LLMs can au-
tomatically generate captions or labels for images in
large datasets, reducing the need for manual annotation
and increasing the dataset size for more effective Re-ID
model training (Tan et al., 2024; Zuo et al., 2024). (4)
Semantic Understanding. LLMs enhance fine-grained
semantic understanding of image regions, especially in
challenging scenarios such as occlusion or low-quality
data. (5) Model Generalization. LLMs possess stronggeneralization capabilities, enabling them to handle un-
seen categories more effectively, further improving the
robustness of Re-ID systems (Tan et al., 2024). The ad-
vanced exploratory work related to these developments
is detailed in §3.3.2. Looking ahead, LLMs hold even
greater potential for various Re-ID scenarios, offering
opportunities to further improve the adaptability, accu-
racy, and scalability of Re-ID systems across different
applications.
Unified Large-scale Foundation Model for
Re-ID. To meet the practical application demands of
Re-ID, it primarily involves the utilization of a unified
large-scale model that accommodates multi-modality
and multi-object scenarios. In cross-modal Re-ID, ex-
isting research often concentrates on just two specific
modalities. However, the sources of query cues are
highly diverse, and exploring how to integrate infor-
mation from various senses or data sources to create a
genuinely modality-agnostic universal Re-ID model is
a matter of significance. Transformer shows great po-
tential in this problem, owing to its flexible handling of
multi-modal inputs, robust relationship modeling capa-
bilities, and scalability for processing large-scale data.
The latest research reveals that Transformer contin-
ues to make breakthroughs in constructing multi-modal
large models. For instance, Meta-Transformer (Zhang
et al., 2023b) is proposed to understand 12 modal in-
formation and offer a borderless multi-modal fusion
paradigm. In the field of Re-ID, there have been some
preliminary explorations into multi-modal unified mod-
els (Li et al., 2024a; He et al., 2024). Besides, unifying
various tasks related to Re-ID that target the same ob-
jective is a development direction with practical signif-
icance. In our survey, it is evident that several recent
studies have made breakthroughs by utilizing Trans-
former models to unify diverse vision tasks centered
around humans (Tang et al., 2023; Ci et al., 2023; Chen
et al., 2023b). Furthermore, constructing a universal
model for multiple objects in Re-ID poses a significant
challenge. This implies that many methods rely on spe-
cific information face difficulties. Particularly in ani-
mal Re-ID, the creation of a multi-species robust model
holds great importance for practical applications.
Efficient Transformer Deployment for Re-ID.
Our survey demonstrates that Transformers indeed ex-
hibit powerful performance in the Re-ID field. How-
ever, due to the substantial computational support re-
quired for self-attention calculations, the associated re-
source consumption is relatively high. In practical ap-
plications, such as video surveillance and intelligent se-
curity, there is an increasing demand for real-time per-
formance and lightweight deployment of Re-ID mod-
els (Mao et al., 2023). Balancing the preservation ofTransformer for Object Re-Identification: A Survey 25
the Transformer’s robust performance in Re-ID with
the imperative to reduce computational complexity be-
comes a crucial direction for future research. Besides,
many pre-trained large-scale general foundations have
been developed in general areas and how to efficiently
transfer the general knowledge to the specific Re-ID
tasks is also worth studying (Ding et al., 2023). Con-
sidering the catastrophic forgetting problem in large-
scale dynamic updated camera network, how to effi-
ciently fine-tune the previously learned Re-ID models
to downstream scenarios is another important direction
to explore (Pu et al., 2023; Gu et al., 2023).
6.2 Summary
From our survey, it is evident that in the past three
years, Transformer has experienced rapid development
in the Re-ID field, particularly demonstrating strong
advantages in more challenging scenarios such as multi-
modal and unsupervised settings. We provide an in-
depth analysis of the advantages of Vision Transformer
in four aspects, corresponding to four Re-ID scenarios:
1.Transformer in Image/Video Based Re-ID: At the
backbone level, the Transformer entirely relies on
the attention mechanism, providing it with uni-
versal modeling capabilities for global, local, and
spatio-temporal relationships. This inherent capa-
bility facilitates the effortless extraction of global,
fine-grained, and spatio-temporal information in
regular image and video Re-ID tasks.
2.Transformer in Re-ID with Limited Data or An-
notations: The emergence of Transformer provides
more possibilities for unsupervised learning. Beyond
conventional discriminative learning approaches,
such as contrastive learning, a broader spectrum of
self-supervised paradigms ( e.g. masked image mod-
eling), has gained widespread attention and explo-
ration. Furthermore, the Transformer exhibits su-
perior adaptability to large-scale data, facilitating
extensive self-supervised pre-training of more pow-
erful and generalized models for addressing Re-ID
with limited data or annotations.
3.Transformer in Cross-modal Re-ID: Transformer
provides a unified architecture to effectively han-
dle data of different modalities, especially the con-
nection of vision and language. The multi-head at-
tention mechanism possesses the capability to ag-
gregate features across various feature spaces and
global contexts, and the highly adaptable encoder-
decoder structure is capable of accommodating di-
verse types of inputs and outputs. Consequently, the
Transformer is particularly well-suited for establish-ing inter-modal associations and facilitating the fu-
sion of multi-modal information in cross-modal Re-
ID tasks.
4.Transformer in Special Re-ID Scenarios: Driven by
the demands of practical applications, the Re-ID
field has given rise to a range of specialized and
challenging scenarios, such as cloth-changing Re-
ID, end-to-end Re-ID, group Re-ID, Re-ID in UAVs,
and human-centric tasks. The initial exploratory ef-
forts of Transformer in tackling these intricate chal-
lenges have showcased remarkable scalability and
adaptability.
This survey predominantly encompasses
transformer-based Re-ID papers, primarily focus-
ing on widely studied objects like persons and vehicles.
Considering the application of Transformer in single-
modal/cross-modal unsupervised Re-ID, which has not
been fully explored by existing research, we present a
Transformer-based baseline that achieves state-of-the-
art performance on multiple single-modal/cross-modal
Re-ID datasets. In particular, we explore the field of
animal Re-ID, an area that continues to encounter
challenges and unresolved issues. We develop unified
experimental standards for animal Re-ID and eval-
uate the feasibility of employing Transformer in this
context, laying a solid foundation for future research.
Additionally, we delve into the future prospects of
Transformers in Re-ID, aiming to further stimulate
subsequent research.
Declarations
Availability of data The authors declare that pub-
licly available datasets are utilized for evaluating object
Re-ID methods. The data supporting the experiments
conducted in this study can be found in the paper. All
publicly available datasets about person-related data
used in this study are obtained and utilized following
appropriate permissions and ethical guidelines.
Acknowledgement. This work is supported by Na-
tional Natural Science Foundation of China (62176188,
62361166629, 62225113)).
References
(2022) Beluga id 2022. URL https://lila.science/
datasets/beluga-id-2022/
(2022) Hyena id 2022. URL https://lila.science/
datasets/hyena-id-2022/
(2022) Leopard id 2022. URL https://lila.science/
datasets/leopard-id-2022/26 Mang Ye, Shuoyi Chen, Chenyue Li, Wei-Shi Zheng, David Crandall, Bo Du
Ahmed E, Jones M, Marks TK (2015) An im-
proved deep learning architecture for person re-
identification. In: CVPR, pp 3908–3916
Bai Y, Jiao J, Ce W, Liu J, Lou Y, Feng X, Duan
LY (2021a) Person30k: A dual-meta generalization
network for person re-identification. In: CVPR, pp
2123–2132
Bai Z, Wang Z, Wang J, Hu D, Ding E (2021b) Unsu-
pervised multi-source domain adaptation for person
re-identification. In: CVPR, pp 12914–12923
Bergamini L, Porrello A, Dondona AC, Del Negro E,
Mattioli M, D’alterio N, Calderara S (2018) Multi-
views embedding for cattle re-identification. In: IEEE
SITIS, pp 184–191
Bouma S, Pawley MD, Hupman K, Gilman A (2018)
Individual common dolphin identification via metric
embedding learning. In: IEEE IVCNZ, pp 1–6
Brown T, Mann B, Ryder N, Subbiah M, Kaplan JD,
Dhariwal P, Neelakantan A, Shyam P, Sastry G,
Askell A, et al. (2020) Language models are few-shot
learners. NeurIPS 33:1877–1901
Bruslund Haurum J, Karpova A, Pedersen M,
Hein Bengtson S, Moeslund TB (2020) Re-
identification of zebrafish using metric learning. In:
WACV Workshop, pp 1–11
Cao J, Pang Y, Anwer RM, Cholakkal H, Xie J, Shah
M, Khan FS (2022) Pstr: End-to-end one-step person
search with transformers. In: CVPR, pp 9458–9467
Cao M, Bai Y, Zeng Z, Ye M, Zhang M (2024) An
empirical study of clip for text-based person search.
In: AAAI, vol 38, pp 465–473
Caron M, Touvron H, Misra I, J´ egou H, Mairal J,
Bojanowski P, Joulin A (2021) Emerging properties
in self-supervised vision transformers. In: ICCV, pp
9650–9660
Chan J, Carri´ on H, M´ egret R, Agosto-Rivera JL, Giray
T (2022) Honeybee re-identification in video: New
datasets and impact of self-supervision. In: VISI-
GRAPP (5: VISAPP), pp 517–525
Cheeseman T, Southerland K, Park J, Olio M, Flynn
K, Calambokidis J, Jones L, Garrigue C, Frisch Jor-
dan A, Howard A, et al. (2022) Advanced image
recognition: a fully automated, high-accuracy photo-
identification matching system for humpback whales.
Mammalian Biology 102(3):915–929
Chen B, Deng W, Hu J (2019) Mixed high-order atten-
tion network for person re-identification. In: ICCV,
pp 371–381
Chen C, Ye M, Qi M, Du B (2022a) Sketch trans-
former: Asymmetrical disentanglement learning from
dynamic synthesis. In: ACM MM, pp 4012–4020
Chen C, Ye M, Qi M, Wu J, Jiang J, Lin CW (2022b)
Structure-aware positional transformer for visible-infrared person re-identification. IEEE TIP 31:2352–
2364
Chen C, Ye M, Jiang D (2023a) Towards modality-
agnostic person re-identification with descriptive
query. In: CVPR, pp 15128–15137
Chen H, Lagadec B, Bremond F (2021a) Ice: Inter-
instance contrastive encoding for unsupervised per-
son re-identification. In: ICCV, pp 14960–14969
Chen S, Ye M, Du B (2022c) Rotation invariant trans-
former for recognizing object in uavs. In: ACM MM,
pp 2565–2574
Chen W, Xu X, Jia J, Luo H, Wang Y, Wang F, Jin
R, Sun X (2023b) Beyond appearance: a semantic
controllable self-supervised learning framework for
human-centric visual tasks. In: CVPR, pp 15050–
15061
Chen X, Xu C, Cao Q, Xu J, Zhong Y, Xu J, Li Z, Wang
J, Gao S (2021b) Oh-former: Omni-relational high-
order transformer for person re-identification. arXiv
preprint arXiv:210911159
Chen YC, Zhu X, Zheng WS, Lai JH (2017) Person
re-identification by camera correlation aware feature
augmentation. IEEE TPAMI 40(2):392–408
Cheng D, Zhou J, Wang N, Gao X (2022a) Hybrid dy-
namic contrast and probability distillation for unsu-
pervised person re-id. IEEE TIP 31:3334–3346
Cheng D, Huang X, Wang N, He L, Li Z, Gao X (2023a)
Unsupervised visible-infrared person reid by collab-
orative learning with neighbor-guided label refine-
ment. In: ACM MM, pp 7085–7093
Cheng D, Wang G, Wang B, Zhang Q, Han J, Zhang
D (2023b) Hybrid routing transformer for zero-shot
learning. Pattern Recognition 137:109270
Cheng D, Wang G, Wang N, Zhang D, Zhang Q,
Gao X (2023c) Discriminative and robust attribute
alignment for zero-shot learning. IEEE TCSVT
33(8):4244–4256
Cheng D, Li Y, Zhang D, Wang N, Sun J, Gao X (2024)
Progressive negative enhancing contrastive learning
for image dehazing and beyond. IEEE TMM
Cheng X, Jia M, Wang Q, Zhang J (2022b) More is
better: Multi-source dynamic parsing attention for
occluded person re-identification. In: ACM MM, pp
6840–6849
Cho Y, Kim WJ, Hong S, Yoon SE (2022) Part-based
pseudo label refinement for unsupervised person re-
identification. In: CVPR, pp 7308–7318
Choi S, Kim T, Jeong M, Park H, Kim C (2021) Meta
batch-instance normalization for generalizable person
re-identification. In: CVPR, pp 3425–3435
Ci Y, Wang Y, Chen M, Tang S, Bai L, Zhu F, Zhao
R, Yu F, Qi D, Ouyang W (2023) Unihcp: A unified
model for human-centric perceptions. In: CVPR, ppTransformer for Object Re-Identification: A Survey 27
17840–17852
Comandur B (2022) Sports re-id: Improving re-
identification of players in broadcast videos of team
sports. arXiv preprint arXiv:220602373
Dai Y, Liu J, Sun Y, Tong Z, Zhang C, Duan LY (2021)
Idm: An intermediate domain module for domain
adaptive person re-id. In: ICCV, pp 11864–11874
Dai Z, Wang G, Yuan W, Zhu S, Tan P (2022) Cluster
contrast for unsupervised person re-identification. In:
ACCV, pp 1142–1160
Dehghani M, Djolonga J, Mustafa B, Padlewski P, Heek
J, Gilmer J, Steiner AP, Caron M, Geirhos R, Alab-
dulmohsin I, et al. (2023) Scaling vision transformers
to 22 billion parameters. In: ICML, PMLR, pp 7480–
7512
Deng W, Zheng L, Ye Q, Kang G, Yang Y, Jiao J (2018)
Image-image domain adaptation with preserved self-
similarity and domain-dissimilarity for person re-
identification. In: CVPR, pp 994–1003
Devlin J, Chang MW, Lee K, Toutanova K (2018)
Bert: Pre-training of deep bidirectional transform-
ers for language understanding. arXiv preprint
arXiv:181004805
Ding N, Qin Y, Yang G, Wei F, Yang Z, Su Y, Hu S,
Chen Y, Chan CM, Chen W, et al. (2023) Parameter-
efficient fine-tuning of large-scale pre-trained lan-
guage models. Nature Machine Intelligence 5(3):220–
235
Ding Z, Ding C, Shao Z, Tao D (2021) Semantically self-
aligned network for text-to-image part-aware person
re-identification. arXiv preprint arXiv:210712666
Dosovitskiy A, Beyer L, Kolesnikov A, Weissenborn D,
Zhai X, Unterthiner T, Dehghani M, Minderer M,
Heigold G, Gelly S, et al. (2020) An image is worth
16x16 words: Transformers for image recognition at
scale. In: ICLR
Fan L, Li T, Fang R, Hristov R, Yuan Y, Katabi D
(2020) Learning longterm representations for person
re-identification using radio signals. In: CVPR, pp
10699–10709
Farooq A, Awais M, Kittler J, Khalid SS (2022) Axm-
net: Implicit cross-modal feature alignment for per-
son re-identification. In: AAAI, vol 36, pp 4477–4485
Feng Y, Yu J, Chen F, Ji Y, Wu F, Liu S, Jing XY
(2022) Visible-infrared person re-identification via
cross-modality interaction transformer. IEEE TMM
Ferdous SN, Li X, Lyu S (2022) Uncertainty aware mul-
titask pyramid vision transformer for uav-based ob-
ject re-identification. In: ICIP, IEEE, pp 2381–2385
Fu D, Chen D, Bao J, Yang H, Yuan L, Zhang L, Li H,
Chen D (2021) Unsupervised pre-training for person
re-identification. In: CVPR, pp 14750–14759Gao J, Burghardt T, Andrew W, Dowsey AW,
Campbell NW (2021) Towards self-supervision for
video identification of individual holstein-friesian
cattle: The cows2021 dataset. arXiv preprint
arXiv:210501938
Ge Y, Zhu F, Chen D, Zhao R, et al. (2020) Self-paced
contrastive learning with hybrid memory for domain
adaptive object re-id. NeurIPS 33:11309–11321
Gray D, Brennan S, Tao H (2007) Evaluating appear-
ance models for recognition, reacquisition, and track-
ing. In: PETS, vol 3, pp 1–7
Gu J, Luo H, Wang K, Jiang W, You Y, Zhao J
(2023) Color prompting for data-free continual un-
supervised domain adaptive person re-identification.
arXiv preprint arXiv:230810716
Guo H, Zhu K, Tang M, Wang J (2019) Two-level atten-
tion network with multi-grain ranking loss for vehicle
re-identification. IEEE TIP 28(9):4328–4338
Guo P, Liu H, Wu J, Wang G, Wang T (2023) Semantic-
aware consistency network for cloth-changing person
re-identification. arXiv preprint arXiv:230814113
Han K, Wang Y, Chen H, Chen X, Guo J, Liu Z, Tang
Y, Xiao A, Xu C, Xu Y, et al. (2022) A survey on
vision transformer. IEEE TPAMI 45(1):87–110
Han X, He S, Zhang L, Xiang T (2021) Text-based per-
son search with limited data. 2110.10807
He B, Li J, Zhao Y, Tian Y (2019) Part-regularized
near-duplicate vehicle re-identification. In: CVPR, pp
3997–4005
He K, Zhang X, Ren S, Sun J (2016) Deep residual
learning for image recognition. In: CVPR, pp 770–
778
He K, Chen X, Xie S, Li Y, Doll´ ar P, Girshick R (2022)
Masked autoencoders are scalable vision learners. In:
CVPR, pp 16000–16009
He S, Luo H, Wang P, Wang F, Li H, Jiang W
(2021a) Transreid: Transformer-based object re-
identification. In: ICCV, pp 15013–15022
He S, Chen W, Wang K, Luo H, Wang F, Jiang W,
Ding H (2023a) Region generation and assessment
network for occluded person re-identification. IEEE
TIFS
He S, Luo H, Jiang W, Jiang X, Ding H (2023b) Vgsg:
Vision-guided semantic-group network for text-based
person search. IEEE TIP 33:163–176
He T, Jin X, Shen X, Huang J, Chen Z, Hua XS (2021b)
Dense interaction learning for video-based person re-
identification. In: ICCV, pp 1490–1501
He T, Shen X, Huang J, Chen Z, Hua XS (2021c) Par-
tial person re-identification with part-part correspon-
dence learning. In: CVPR, pp 9105–9115
He W, Deng Y, Tang S, Chen Q, Xie Q, Wang Y, Bai
L, Zhu F, Zhao R, Ouyang W, et al. (2024) Instruct-28 Mang Ye, Shuoyi Chen, Chenyue Li, Wei-Shi Zheng, David Crandall, Bo Du
reid: A multi-purpose person re-identification task
with instructions. In: CVPR, pp 17521–17531
Hermans A, Beyer L, Leibe B (2017) In defense of the
triplet loss for person re-identification. arXiv preprint
arXiv:170307737
Hong P, Wu T, Wu A, Han X, Zheng WS (2021)
Fine-grained shape-appearance mutual learning for
cloth-changing person re-identification. In: CVPR,
pp 10513–10522
Howard A, Ken i, Southerland, Holbrook R, Cheeseman
T (2022) Happywhale - whale and dolphin identi-
fication. URL https://kaggle.com/competitions/
happy-whale-and-dolphin
Jia M, Cheng X, Lu S, Zhang J (2022a) Learning dis-
entangled representation implicitly via transformer
for occluded person re-identification. IEEE TMM
25:1294–1305
Jia X, Zhong X, Ye M, Liu W, Huang W (2022b) Com-
plementary data augmentation for cloth-changing
person re-identification. IEEE TIP 31:4227–4239
Jiang D, Ye M (2023) Cross-modal implicit relation
reasoning and aligning for text-to-image person re-
trieval. In: CVPR, pp 2787–2797
Jiang K, Zhang T, Liu X, Qian B, Zhang Y, Wu
F (2022) Cross-modality transformer for visible-
infrared person re-identification. In: ECCV, Springer,
pp 480–496
Jiao B, Liu L, Gao L, Wu R, Lin G, Wang P, Zhang Y
(2023) Toward re-identifying any animal. In: NeurIPS
Jin X, Lan C, Zeng W, Chen Z, Zhang L (2020) Style
normalization and restitution for generalizable per-
son re-identification. In: CVPR, pp 3143–3152
Jin X, He T, Zheng K, Yin Z, Shen X, Huang Z, Feng
R, Huang J, Chen Z, Hua XS (2022) Cloth-changing
person re-identification from a single image with gait
prediction and regularization. In: CVPR, pp 14278–
14287
Kalayeh MM, Basaran E, G¨ okmen M, Kamasak ME,
Shah M (2018) Human semantic parsing for person
re-identification. In: CVPR, pp 1062–1071
Khan SD, Ullah H (2019) A survey of advances in
vision-based vehicle re-identification. CVIU 182:50–
63
Khorramshahi P, Kumar A, Peri N, Rambhatla SS,
Chen JC, Chellappa R (2019) A dual-path model
with adaptive attention for vehicle re-identification.
In: ICCV, pp 6132–6141
Koch G, Zemel R, Salakhutdinov R, et al. (2015)
Siamese neural networks for one-shot image recog-
nition. In: ICML workshop, Lille, vol 2
Konovalov DA, Hillcoat S, Williams G, Birtles RA,
Gardiner N, Curnock MI (2018) Individual minke
whale recognition using deep learning convolutionalneural networks. Journal of Geoscience and Environ-
ment Protection 6:25–36
Korschens M, Denzler J (2019) Elpephants: A fine-
grained dataset for elephant re-identification. In:
ICCV Workshop, pp 0–0
Kumar S, Yaghoubi E, Das A, Harish B, Proen¸ ca
H (2020) The p-destre: a fully annotated dataset
for pedestrian detection, tracking, re-identification
and search from aerial devices. arXiv preprint
arXiv:200402782
Kuncheva LI, Williams F, Hennessey SL, Rodr´ ıguez
JJ (2022) A benchmark database for animal re-
identification and tracking. In: IEEE IPAS, IEEE,
pp 1–6
Lai S, Chai Z, Wei X (2021) Transformer meets
part model: Adaptive part division for person re-
identification. In: ICCV, pp 4150–4157
Lee KW, Jawade B, Mohan D, Setlur S, Govindaraju
V (2022) Attribute de-biased vision transformer (ad-
vit) for long-term person re-identification. In: IEEE
AVSS, IEEE, pp 1–8
Li H, Li C, Zhu X, Zheng A, Luo B (2020) Multi-
spectral vehicle re-identification: A challenge. In:
AAAI, vol 34, pp 11345–11353
Li H, Wu G, Zheng WS (2021a) Combined depth space
based architecture search for person re-identification.
In: CVPR, pp 6729–6738
Li H, Ye M, Du B (2021b) Weperson: Learning a gener-
alized re-identification model from all-weather virtual
data. In: ACM MM, pp 3115–3123
Li H, Li C, Zheng A, Tang J, Luo B (2022a) Mskat:
Multi-scale knowledge-aware transformer for vehicle
re-identification. IEEE TITS 23(10):19557–19568
Li H, Ye M, Wang C, Du B (2022b) Pyrami-
dal transformer with conv-patchify for person re-
identification. In: ACM MM, pp 7317–7326
Li H, Ye M, Zhang M, Du B (2024a) All in one frame-
work for multimodal re-identification in the wild. In:
CVPR, pp 17459–17469
Li M, Zhu X, Gong S (2019a) Unsupervised tracklet
person re-identification. IEEE TPAMI 42(7):1770–
1782
Li S, Xiao T, Li H, Zhou B, Yue D, Wang X (2017)
Person search with natural language description. In:
CVPR, pp 1970–1979
Li S, Li J, Tang H, Qian R, Lin W (2019b) Atrw:
a benchmark for amur tiger re-identification in the
wild. arXiv preprint arXiv:190605586
Li S, Fu L, Sun Y, Mu Y, Chen L, Li J, Gong H
(2021c) Individual dairy cow identification based on
lightweight convolutional neural network. Plos one
16(11):e0260510Transformer for Object Re-Identification: A Survey 29
Li S, Sun L, Li Q (2023a) Clip-reid: Exploiting vision-
language model for image re-identification without
concrete text labels. In: AAAI, vol 37, pp 1405–1413
Li T, Liu J, Zhang W, Ni Y, Wang W, Li Z (2021d)
Uav-human: A large benchmark for human behav-
ior understanding with unmanned aerial vehicles. In:
CVPR, pp 16266–16275
Li W, Zhao R, Xiao T, Wang X (2014) Deepreid:
Deep filter pairing neural network for person re-
identification. In: CVPR, pp 152–159
Li W, Zhu X, Gong S (2018) Harmonious attention
network for person re-identification. In: CVPR, pp
2285–2294
Li W, Zou C, Wang M, Xu F, Zhao J, Zheng R, Cheng
Y, Chu W (2023b) Dc-former: Diverse and com-
pact transformer for person re-identification. arXiv
preprint arXiv:230214335
Li Y, He J, Zhang T, Liu X, Zhang Y, Wu F
(2021e) Diverse part discovery: Occluded person
re-identification with part-aware transformer. In:
CVPR, pp 2898–2907
Li Y, Liu Y, Zhang H, Zhao C, Wei Z, Miao D (2024b)
Occlusion-aware transformer with second-order at-
tention for person re-identification. IEEE TIP
Liang T, Jin Y, Liu W, Li Y (2023) Cross-modality
transformer with modality mining for visible-infrared
person re-identification. IEEE TMM
Liao S, Shao L (2021) Transmatcher: Deep image
matching through transformers for generalizable per-
son re-identification. NeurIPS 34:1992–2003
Liao S, Hu Y, Zhu X, Li SZ (2015) Person re-
identification by local maximal occurrence represen-
tation and metric learning. In: CVPR, pp 2197–2206
Lin W, Li Y, Xiao H, See J, Zou J, Xiong H, Wang
J, Mei T (2019a) Group reidentification with multi-
grained matching and integration. IEEE transactions
on cybernetics 51(3):1478–1492
Lin Y, Dong X, Zheng L, Yan Y, Yang Y (2019b) A
bottom-up clustering approach to unsupervised per-
son re-identification. In: AAAI, vol 33, pp 8738–8745
Lin Y, Xie L, Wu Y, Yan C, Tian Q (2020) Unsuper-
vised person re-identification via softened similarity
learning. In: CVPR, pp 3390–3399
Liu F, Ye M, Du B (2023a) Dual level adaptive weight-
ing for cloth-changing person re-identification. IEEE
TIP
Liu H, Jie Z, Jayashree K, Qi M, Jiang J, Yan S, Feng J
(2017) Video-based person re-identification with ac-
cumulative motion context. IEEE transactions on cir-
cuits and systems for video technology 28(10):2788–
2802
Liu X, Liu W, Ma H, Fu H (2016a) Large-scale vehi-
cle re-identification in urban surveillance videos. In:ICME, IEEE, pp 1–6
Liu X, Liu W, Mei T, Ma H (2016b) A deep
learning-based approach to progressive vehicle re-
identification for urban surveillance. In: ECCV,
Springer, pp 869–884
Liu X, Zhang P, Yu C, Lu H, Qian X, Yang X (2021a)
A video is worth three views: Trigeminal transform-
ers for video-based person re-identification. arXiv
preprint arXiv:210401745
Liu X, Yu C, Zhang P, Lu H (2023b) Deeply cou-
pled convolution–transformer with spatial–temporal
complementary learning for video-based person re-
identification. IEEE TNNLS
Liu Z, Lin Y, Cao Y, Hu H, Wei Y, Zhang Z, Lin S,
Guo B (2021b) Swin transformer: Hierarchical vision
transformer using shifted windows. arXiv preprint
arXiv:210314030
Lou Y, Bai Y, Liu J, Wang S, Duan L (2019) Veri-
wild: A large dataset and a new method for vehicle
re-identification in the wild. In: CVPR, pp 3235–3243
Luo H, Jiang W, Gu Y, Liu F, Liao X, Lai S, Gu J
(2019) A strong baseline and batch normalization
neck for deep person re-identification. IEEE TMM
22(10):2597–2609
Luo H, Wang P, Xu Y, Ding F, Zhou Y, Wang
F, Li H, Jin R (2021) Self-supervised pre-training
for transformer-based person re-identification. arXiv
preprint arXiv:211112084
Mallat SG (1989) A theory for multiresolution sig-
nal decomposition: the wavelet representation. IEEE
TPAMI 11(7):674–693
Mao J, Yao Y, Sun Z, Huang X, Shen F, Shen HT
(2023) Attention map guided transformer pruning
for occluded person re-identification on edge device.
IEEE TMM
McLaughlin N, Del Rincon JM, Miller P (2016) Re-
current convolutional network for video-based person
re-identification. In: CVPR, pp 1325–1334
Meng D, Li L, Liu X, Li Y, Yang S, Zha ZJ, Gao X,
Wang S, Huang Q (2020) Parsing-based view-aware
embedding network for vehicle re-identification. In:
CVPR, pp 7103–7112
Miao J, Wu Y, Liu P, Ding Y, Yang Y (2019) Pose-
guided feature alignment for occluded person re-
identification. In: ICCV, pp 542–551
Moskvyak O, Maire F, Dayoub F, Baktashmotlagh M
(2020) Learning landmark guided embeddings for an-
imal re-identification. In: WACV Workshop, pp 12–
19
Moskvyak O, Maire F, Dayoub F, Armstrong AO, Bak-
tashmotlagh M (2021) Robust re-identification of
manta rays from natural markings by learning pose
invariant embeddings. In: DICTA, IEEE, pp 1–830 Mang Ye, Shuoyi Chen, Chenyue Li, Wei-Shi Zheng, David Crandall, Bo Du
Naseer M, Ranasinghe K, Khan S, Hayat M, Khan FS,
Yang MH (2021) Intriguing properties of vision trans-
formers. arXiv preprint arXiv:210510497
Nepovinnykh E, Eerola T, Kalviainen H (2020) Siamese
network based pelage pattern matching for ringed
seal re-identification. In: WACV Workshop, pp 25–
34
Nepovinnykh E, Eerola T, Biard V, Mutka P, Niemi
M, Kunnasranta M, K¨ alvi¨ ainen H (2022) Sealid:
Saimaa ringed seal re-identification dataset. Sensors
22(19):7602
Nguyen DT, Hong HG, Kim KW, Park KR (2017) Per-
son recognition system based on a combination of
body images from visible light and thermal cameras.
Sensors 17(3):605
Ni H, Song J, Luo X, Zheng F, Li W, Shen HT (2022)
Meta distribution alignment for generalizable person
re-identification. In: CVPR, pp 2487–2496
Ni H, Li Y, Gao L, Shen HT, Song J (2023) Part-aware
transformer for generalizable person re-identification.
In: Proceedings of the IEEE/CVF International Con-
ference on Computer Vision, pp 11280–11289
Niu K, Huang Y, Ouyang W, Wang L (2020) Im-
proving description-based person re-identification by
multi-granularity image-text alignments. IEEE TIP
pp 5542–5556
Organisciak D, Poyser M, Alsehaim A, Hu S, Isaac-
Medina BK, Breckon TP, Shum HP (2021) Uav-
reid: A benchmark on unmanned aerial vehicle
re-identification in video imagery. arXiv preprint
arXiv:210406219
Pang L, Wang Y, Song YZ, Huang T, Tian Y (2018)
Cross-domain adversarial feature learning for sketch
re-identification. In: ACM MM, pp 609–617
Papafitsoros K, Adam L, ˇCerm´ ak V, Picek L (2022)
Seaturtleid: A novel long-span dataset highlight-
ing the importance of timestamps in wildlife re-
identification. arXiv preprint arXiv:221110307
Parham J, Crall J, Stewart C, Berger-Wolf T, Ruben-
stein DI (2017) Animal population censusing at scale
with citizen science and photographic identification.
In: AAAI
Park H, Ham B (2020) Relation network for person re-
identification. In: AAAI, vol 34, pp 11839–11847
Porrello A, Bergamini L, Calderara S (2020) Robust
re-identification by multiple views knowledge distil-
lation. In: ECCV, Springer, pp 93–110
Pu N, Zhong Z, Sebe N, Lew MS (2023) A memoriz-
ing and generalizing framework for lifelong person
re-identification. IEEE TPAMI
Qian W, Luo H, Peng S, Wang F, Chen C, Li H
(2022) Unstructured feature decoupling for vehicle
re-identification. In: ECCV, Springer, pp 336–353Qian X, Wang W, Zhang L, Zhu F, Fu Y, Xiang T,
Jiang YG, Xue X (2020) Long-term cloth-changing
person re-identification. In: ACCV
Radford A, Kim JW, Hallacy C, Ramesh A, Goh G,
Agarwal S, Sastry G, Askell A, Mishkin P, Clark J,
et al. (2021) Learning transferable visual models from
natural language supervision. In: ICML, PMLR, pp
8748–8763
Rao H, Miao C (2023) Transg: Transformer-based
skeleton graph prototype contrastive learning with
structure-trajectory prompted reconstruction for
person re-identification. In: CVPR, pp 22118–22128
Rao H, Wang S, Hu X, Tan M, Guo Y, Cheng J, Liu
X, Hu B (2021) A self-supervised gait encoding ap-
proach with locality-awareness for 3d skeleton based
person re-identification. IEEE TPAMI 44(10):6649–
6666
Rao H, Leung C, Miao C (2024) Hierarchical skeleton
meta-prototype contrastive learning with hard skele-
ton mining for unsupervised person re-identification.
IJCV 132(1):238–260
Sarafianos N, Xu X, Kakadiaris IA (2019) Adversarial
representation learning for text-to-image matching.
In: ICCV, pp 5814–5824
Schneider S, Taylor GW, Linquist S, Kremer SC (2019)
Past, present and future approaches using computer
vision for animal re-identification from camera trap
data. Methods in Ecology and Evolution 10(4):461–
470
Shao Z, Zhang X, Fang M, Lin Z, Wang J, Ding C
(2022) Learning granularity-unified representations
for text-to-image person re-identification. In: ACM
MM, pp 5566–5574
Shao Z, Zhang X, Ding C, Wang J, Wang J (2023)
Unified pre-training with pseudo texts for text-to-
image person re-identification. In: ICCV, pp 11174–
11184
Shen F, Xie Y, Zhu J, Zhu X, Zeng H (2023a)
Git: Graph interactive transformer for vehicle re-
identification. IEEE TIP 32:1039–1051
Shen L, He T, Guo Y, Ding G (2023b) X-reid: Cross-
instance transformer for identity-level person re-
identification. arXiv preprint arXiv:230202075
Shu X, Wen W, Wu H, Chen K, Song Y, Qiao R, Ren B,
Wang X (2022) See finer, see more: Implicit modality
alignment for text-based person retrieval. In: ECCV,
Springer, pp 624–641
Song G, Leng B, Liu Y, Hetang C, Cai S (2018) Region-
based quality estimation network for large-scale per-
son re-identification. In: AAAI, vol 32
Su C, Li J, Zhang S, Xing J, Gao W, Tian Q (2017)
Pose-driven deep convolutional model for person re-
identification. In: ICCV, pp 3960–3969Transformer for Object Re-Identification: A Survey 31
Suh Y, Wang J, Tang S, Mei T, Lee KM (2018)
Part-aligned bilinear representations for person re-
identification. In: ECCV, pp 402–419
Sun CC, Arr GS, Ramachandran RP, Ritchie SG (2004)
Vehicle reidentification using multidetector fusion.
IEEE TITS 5(3):155–164
Sun X, Zheng L (2019) Dissecting person re-
identification from the viewpoint of viewpoint. In:
CVPR, pp 608–617
Sun Y, Zheng L, Yang Y, Tian Q, Wang S (2018)
Beyond part models: Person retrieval with refined
part pooling (and a strong convolutional baseline).
In: ECCV, pp 480–496
Tan B, Xu L, Qiu Z, Wu Q, Meng F (2023) Mfat: A
multi-level feature aggregated transformer for person
re-identification. In: ICASSP, IEEE, pp 1–5
Tan W, Ding C, Jiang J, Wang F, Zhan Y, Tao D (2024)
Harnessing the power of mllms for transferable text-
to-image person reid. In: CVPR, pp 17127–17137
Tang S, Chen C, Xie Q, Chen M, Wang Y, Ci Y, Bai L,
Zhu F, Yang H, Yi L, et al. (2023) Humanbench: To-
wards general human-centric perception with projec-
tor assisted pretraining. In: CVPR, pp 21970–21982
Tang Z, Naphade M, Liu MY, Yang X, Birchfield S,
Wang S, Kumar R, Anastasiu D, Hwang JN (2019)
Cityflow: A city-scale benchmark for multi-target
multi-camera vehicle tracking and re-identification.
In: CVPR, pp 8797–8806
Tang Z, Zhang R, Peng Z, Chen J, Lin L (2022)
Multi-stage spatio-temporal aggregation transformer
for video person re-identification. IEEE TMM
Teng S, Zhang S, Huang Q, Sebe N (2021) Viewpoint
and scale consistency reinforcement for uav vehicle
re-identification. IJCV 129:719–735
Tian X, Liu J, Zhang Z, Wang C, Qu Y, Xie Y, Ma
L (2022) Hierarchical walking transformer for object
re-identification. In: ACM MM, pp 4224–4232
Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L,
Gomez AN, Kaiser  L, Polosukhin I (2017) Attention
is all you need. NeurIPS 30
Walmer M, Suri S, Gupta K, Shrivastava A (2023)
Teaching matters: Investigating the role of supervi-
sion in vision transformers. In: CVPR, pp 7486–7496
Wang D, Zhang S (2020) Unsupervised person
re-identification via multi-label classification. In:
CVPR, pp 10981–10990
Wang G, Zhang T, Cheng J, Liu S, Yang Y, Hou
Z (2019a) Rgb-infrared cross-modality person re-
identification via joint pixel and feature alignment.
In: ICCV, pp 3623–3632
Wang G, Yang S, Liu H, Wang Z, Yang Y, Wang S, Yu
G, Zhou E, Sun J (2020a) High-order information
matters: Learning relation and topology for occludedperson re-identification. In: CVPR, pp 6449–6458
Wang G, Yu F, Li J, Jia Q, Ding S (2023a) Exploit-
ing the textual potential from vision-language pre-
training for text-based person search. arXiv preprint
arXiv:230304497
Wang GA, Zhang T, Yang Y, Cheng J, Chang J, Liang
X, Hou ZG (2020b) Cross-modality paired-images
generation for rgb-infrared person re-identification.
In: AAAI, vol 34, pp 12144–12151
Wang H, Shen J, Liu Y, Gao Y, Gavves E (2022a)
Nformer: Robust person re-identification with neigh-
bor transformer. In: CVPR, pp 7297–7307
Wang J, Zhang Z, Chen M, Zhang Y, Wang C, Sheng
B, Qu Y, Xie Y (2022b) Optimal transport for label-
efficient visible-infrared person re-identification. In:
ECCV, Springer, pp 93–109
Wang L, Ding R, Zhai Y, Zhang Q, Tang W, Zheng
N, Hua G (2021a) Giant panda identification. IEEE
TIP 30:2837–2849
Wang P, Jiao B, Yang L, Yang Y, Zhang S, Wei W,
Zhang Y (2019b) Vehicle re-identification in aerial
imagery: Dataset and approach. In: ICCV, pp 460–
469
Wang T, Liu H, Song P, Guo T, Shi W (2022c) Pose-
guided feature disentangling for occluded person re-
identification based on transformer. In: AAAI, vol 36,
pp 2540–2549
Wang T, Liu H, Li W, Ban M, Guo T, Li Y (2023b)
Feature completion transformer for occluded person
re-identification. arXiv preprint arXiv:230301656
Wang W, Xie E, Li X, Fan DP, Song K, Liang D, Lu T,
Luo P, Shao L (2021b) Pyramid vision transformer: A
versatile backbone for dense prediction without con-
volutions. arXiv preprint arXiv:210212122
Wang X, Wang X, Jiang B, Luo B (2023c) Few-
shot learning meets transformer: Unified query-
support transformers for few-shot classification.
IEEE TCSVT
Wang Y, Qi G, Li S, Chai Y, Li H (2022d) Body part-
level domain alignment for domain-adaptive person
re-identification with transformer framework. IEEE
TIFS 17:3321–3334
Wang Z, Wang Z, Zheng Y, Chuang YY, Satoh S
(2019c) Learning to reduce dual-level discrepancy for
infrared-visible person re-identification. In: CVPR,
pp 618–626
Wang Z, Wang Z, Zheng Y, Wu Y, Zeng W, Satoh
S (2019d) Beyond intra-modality: A survey of het-
erogeneous person re-identification. arXiv preprint
arXiv:190510048
Wang Z, Fang Z, Wang J, Yang Y (2020c) Vitaa: Visual-
textual attributes alignment in person search by nat-
ural language. In: ECCV, Springer, pp 402–42032 Mang Ye, Shuoyi Chen, Chenyue Li, Wei-Shi Zheng, David Crandall, Bo Du
Wei L, Zhang S, Gao W, Tian Q (2018) Person transfer
gan to bridge domain gap for person re-identification.
In: CVPR, pp 79–88
Wei R, Gu J, He S, Jiang W (2022) Transformer-based
domain-specific representation for unsupervised do-
main adaptive vehicle re-identification. IEEE TITS
24(3):2935–2946
Weideman H, Stewart C, Parham J, Holmberg J, Flynn
K, Calambokidis J, Paul DB, Bedetti A, Henley M,
Pope F, et al. (2020) Extracting identifying contours
for african elephants and humpback whales using a
learned appearance model. In: WACV, pp 1276–1285
Weideman HJ, Jablons ZM, Holmberg J, Flynn K,
Calambokidis J, Tyson RB, Allen JB, Wells RS, Hup-
man K, Urian K, et al. (2017) Integral curvature rep-
resentation and matching algorithms for identifica-
tion of dolphins and whales. In: ICCV Workshop, pp
2831–2839
Wu A, Zheng WS, Yu HX, Gong S, Lai J (2017) Rgb-
infrared cross-modality person re-identification. In:
ICCV, pp 5380–5389
Wu J, He L, Liu W, Yang Y, Lei Z, Mei T, Li SZ (2022a)
Cavit: Contextual alignment vision transformer for
video object re-identification. In: ECCV, Springer,
pp 549–566
Wu L, Liu D, Zhang W, Chen D, Ge Z, Boussaid F,
Bennamoun M, Shen J (2022b) Pseudo-pair based
self-similarity learning for unsupervised person re-
identification. IEEE TIP 31:4803–4816
Wu P, Wang L, Zhou S, Hua G, Sun C (2024) Tem-
poral correlation vision transformer for video person
re-identification. In: AAAI, vol 38, pp 6083–6091
Wu Y, Yan Z, Han X, Li G, Zou C, Cui S (2021) Lap-
score: language-guided person search via color rea-
soning. In: ICCV, pp 1624–1633
Wu Z, Ye M (2023) Unsupervised visible-infrared per-
son re-identification via progressive graph matching
and alternate learning. In: CVPR, pp 9548–9558
Xiao H, Lin W, Sheng B, Lu K, Yan J, Wang J, Ding
E, Zhang Y, Xiong H (2018) Group re-identification:
Leveraging and integrating multi-grain information.
In: ACM MM, pp 192–200
Xiao T, Li S, Wang B, Lin L, Wang X (2017) Joint de-
tection and identification feature learning for person
search. In: CVPR, pp 3415–3424
Xie Z, Zhang Z, Cao Y, Lin Y, Bao J, Yao Z, Dai Q, Hu
H (2022) Simmim: A simple framework for masked
image modeling. In: CVPR, pp 9653–9663
Xu B, He L, Liang J, Sun Z (2022) Learning fea-
ture recovery transformer for occluded person re-
identification. IEEE TIP 31:4651–4662
Xu P, Zhu X (2023) Deepchange: A long-term person
re-identification benchmark with clothes change. In:ICCV, pp 11196–11205
Xu P, Zhu X, Clifton DA (2023) Multimodal learning
with transformers: A survey. IEEE TPAMI
Xu W, Liu H, Shi W, Miao Z, Lu Z, Chen F (2021) Ad-
versarial feature disentanglement for long-term per-
son re-identification. In: IJCAI, pp 1201–1207
Xuan S, Zhang S (2021) Intra-inter camera similarity
for unsupervised person re-identification. In: CVPR,
pp 11926–11935
Yan K, Tian Y, Wang Y, Zeng W, Huang T (2017) Ex-
ploiting multi-grain ranking constraints for precisely
searching visually-similar vehicles. In: ICCV, pp 562–
570
Yan S, Dong N, Zhang L, Tang J (2022) Clip-driven
fine-grained text-image person re-identification.
arXiv preprint arXiv:221010276
Yan Y, Ni B, Song Z, Ma C, Yan Y, Yang X (2016)
Person re-identification via recurrent feature aggre-
gation. In: ECCV, Springer, pp 701–716
Yan Y, Qin J, Ni B, Chen J, Liu L, Zhu F, Zheng
WS, Yang X, Shao L (2020) Learning multi-attention
context graph for group-based re-identification. IEEE
TPAMI 45(6):7001–7018
Yang B, Ye M, Chen J, Wu Z (2022) Augmented dual-
contrastive aggregation learning for unsupervised
visible-infrared person re-identification. In: ACM
MM, pp 2843–2851
Yang B, Chen J, Ye M (2023a) Top-k visual tokens
transformer: Selecting tokens for visible-infrared per-
son re-identification. In: ICASSP, IEEE, pp 1–5
Yang B, Chen J, Ye M (2023b) Towards grand uni-
fied representation learning for unsupervised visible-
infrared person re-identification. In: ICCV, pp
11069–11079
Yang Q, Wu A, Zheng WS (2019) Person re-
identification by contour sketch under moderate
clothing change. IEEE TPAMI 43(6):2029–2046
Yang S, Zhou Y, Zheng Z, Wang Y, Zhu L, Wu Y
(2023c) Towards unified text-based person retrieval:
A large-scale multi-attribute and language search
benchmark. In: ACM MM, pp 4492–4501
Yang Z, Wu D, Wu C, Lin Z, Gu J, Wang W (2024)
A pedestrian is worth one prompt: Towards lan-
guage guidance person re-identification. In: CVPR,
pp 17343–17353
Yao Y, Zheng L, Yang X, Naphade M, Gedeon T (2020)
Simulating content consistent vehicle datasets with
attribute descent. In: ECCV, Springer, pp 775–791
Ye M, Liang C, Wang Z, Leng Q, Chen J, Liu J (2015)
Specific person retrieval via incomplete text descrip-
tion. In: ACM ICMRl, pp 547–550
Ye M, Lan X, Li J, Yuen P (2018) Hierarchical dis-
criminative learning for visible thermal person re-Transformer for Object Re-Identification: A Survey 33
identification. In: AAAI, vol 32
Ye M, Cheng Y, Lan X, Zhu H (2019a) Improving night-
time pedestrian retrieval with distribution alignment
and contextual distance. IEEE TII 16(1):615–624
Ye M, Lan X, Wang Z, Yuen PC (2019b) Bi-directional
center-constrained top-ranking for visible thermal
person re-identification. IEEE TIFS 15:407–419
Ye M, Shen J, Shao L (2020a) Visible-infrared per-
son re-identification via homogeneous augmented tri-
modal learning. IEEE TIFS 16:728–739
Ye M, Shen J, Zhang X, Yuen PC, Chang SF (2020b)
Augmentation invariant and instance spreading fea-
ture for softmax embedding. IEEE TPAMI
Ye M, Li H, Du B, Shen J, Shao L, Hoi SC (2021a) Col-
laborative refining for person re-identification with
label noise. IEEE TIP 31:379–391
Ye M, Ruan W, Du B, Shou MZ (2021b) Channel aug-
mented joint learning for visible-infrared recognition.
In: ICCV, pp 13567–13576
Ye M, Shen J, Lin G, Xiang T, Shao L, Hoi SCH (2021c)
Deep learning for person re-identification: A survey
and outlook. IEEE TPAMI pp 1–1
Ye M, Wu Z, Chen C, Du B (2023) Channel aug-
mentation for visible-infrared re-identification. IEEE
TPAMI (01):1–16
Ye Y, Zhou H, Yu J, Hu Q, Yang W (2022) Dynamic
feature pruning and consolidation for occluded per-
son re-identification. arXiv preprint arXiv:221114742
Yu HX, Zheng WS, Wu A, Guo X, Gong S, Lai JH
(2019) Unsupervised person re-identification by soft
multilabel learning. In: CVPR, pp 2148–2157
Yu R, Du D, LaLonde R, Davila D, Funk C, Hoogs A,
Clipp B (2022) Cascade transformers for end-to-end
person search. In: CVPR, pp 7267–7276
Zapletal D, Herout A (2016) Vehicle re-identification for
automatic video traffic surveillance. In: CVPR Work-
shop, pp 25–31
Zhai X, Kolesnikov A, Houlsby N, Beyer L (2022a) Scal-
ing vision transformers. In: CVPR, pp 12104–12113
Zhai Y, Zeng Y, Cao D, Lu S (2022b) Trireid: Towards
multi-modal person re-identification via descriptive
fusion model. In: ICMR, pp 63–71
Zhang B, Liang Y, Du M (2022a) Interlaced percep-
tion for person re-identification based on swin trans-
former. In: IEEE ICIVC, pp 24–30
Zhang G, Zhang P, Qi J, Lu H (2021a) Hat: Hierarchical
aggregation transformers for person re-identification.
In: ACM MM, pp 516–525
Zhang G, Zhang Y, Zhang T, Li B, Pu S
(2023a) Pha: Patch-wise high-frequency augmenta-
tion for transformer-based person re-identification.
In: CVPR, pp 14133–14142Zhang Q, Lai JH, Feng Z, Xie X (2022b) Uncertainty
modeling with second-order transformer for group re-
identification. In: AAAI, vol 36, pp 3318–3325
Zhang Q, Wang L, Patel VM, Xie X, Lai J (2024) View-
decoupled transformer for person re-identification
under aerial-ground camera network. In: CVPR, pp
22000–22009
Zhang S, Zhang Q, Yang Y, Wei X, Wang P, Jiao B,
Zhang Y (2020a) Person re-identification in aerial im-
agery. IEEE TMM 23:281–291
Zhang S, Yang Y, Wang P, Liang G, Zhang X, Zhang Y
(2021b) Attend to the difference: Cross-modality per-
son re-identification via contrastive correlation. IEEE
TIP 30:8861–8872
Zhang T, Wei L, Xie L, Zhuang Z, Zhang Y, Li
B, Tian Q (2021c) Spatiotemporal transformer for
video-based person re-identification. arXiv preprint
arXiv:210316469
Zhang T, Xie L, Wei L, Zhuang Z, Zhang Y, Li B,
Tian Q (2021d) Unrealperson: An adaptive pipeline
towards costless person re-identification. In: CVPR,
pp 11506–11515
Zhang T, Zhao Q, Da C, Zhou L, Li L, Jiancuo
S (2021e) Yakreid-103: A benchmark for yak re-
identification. In: IEEE IJCB, IEEE, pp 1–8
Zhang X, Ge Y, Qiao Y, Li H (2021f) Refining pseudo
labels with clustering consensus over generations for
unsupervised object re-identification. In: CVPR, pp
3436–3445
Zhang X, Li D, Wang Z, Wang J, Ding E, Shi JQ,
Zhang Z, Wang J (2022c) Implicit sample extension
for unsupervised person re-identification. In: CVPR,
pp 7369–7378
Zhang Y, Lu H (2018) Deep cross-modal projection
learning for image-text matching. In: ECCV, pp 686–
701
Zhang Y, Wang Y, Li H, Li S (2022d) Cross-compatible
embedding and semantic consistent feature construc-
tion for sketch re-identification. In: ACM MM, pp
3347–3355
Zhang Y, Gong K, Zhang K, Li H, Qiao Y, Ouyang
W, Yue X (2023b) Meta-transformer: A unified
framework for multimodal learning. arXiv preprint
arXiv:230710802
Zhang Z, Lan C, Zeng W, Jin X, Chen Z (2020b)
Relation-aware global attention for person re-
identification. In: CVPR, pp 3186–3195
Zhao J, Wang H, Zhou Y, Yao R, Chen S, El Saddik
A (2022) Spatial-channel enhanced transformer for
visible-infrared person re-identification. IEEE TMM
Zhao Y, Zhong Z, Yang F, Luo Z, Lin Y, Li S, Sebe
N (2021) Learning to generalize unseen domains via
memory-based multi-source meta-learning for person34 Mang Ye, Shuoyi Chen, Chenyue Li, Wei-Shi Zheng, David Crandall, Bo Du
re-identification. In: CVPR, pp 6277–6286
Zheng K, Liu W, He L, Mei T, Luo J, Zha ZJ (2021)
Group-aware label transfer for domain adaptive per-
son re-identification. In: CVPR, pp 5310–5319
Zheng L, Shen L, Tian L, Wang S, Wang J, Tian
Q (2015) Scalable person re-identification: A bench-
mark. In: ICCV, pp 1116–1124
Zheng L, Bie Z, Sun Y, Wang J, Su C, Wang S, Tian
Q (2016a) Mars: A video benchmark for large-scale
person re-identification. In: ECCV, Springer, pp 868–
884
Zheng L, Yang Y, Hauptmann AG (2016b) Person
re-identification: Past, present and future. arXiv
preprint arXiv:161002984
Zheng L, Zhang H, Sun S, Chandraker M, Yang Y,
Tian Q (2017a) Person re-identification in the wild.
In: CVPR, pp 1367–1376
Zheng W, Gong S, Xiang T (2009) Associating groups
of people. In: BMVC, pp 1–11
Zheng Z, Zheng L, Yang Y (2017b) A discriminatively
learned cnn embedding for person reidentification.
ACM TOMM 14(1):1–20
Zheng Z, Zheng L, Yang Y (2017c) Unlabeled samples
generated by gan improve the person re-identification
baseline in vitro. In: ICCV, pp 3754–3762
Zhong Z, Zheng L, Cao D, Li S (2017) Re-ranking per-
son re-identification with k-reciprocal encoding. In:
CVPR, pp 1318–1327
Zhou K, Yang Y, Cavallaro A, Xiang T (2019) Omni-
scale feature learning for person re-identification. In:
ICCV, pp 3702–3712
Zhou K, Yang J, Loy CC, Liu Z (2022a) Learn-
ing to prompt for vision-language models. IJCV
130(9):2337–2348
Zhou M, Liu H, Lv Z, Hong W, Chen X (2022b)
Motion-aware transformer for occluded person re-
identification. arXiv preprint arXiv:220204243
Zhu A, Wang Z, Li Y, Wan X, Jin J, Wang T, Hu
F, Hua G (2021a) Dssl: Deep surroundings-person
separation learning for text-based person retrieval.
In: ACM MM, pp 209–217
Zhu H, Ke W, Li D, Liu J, Tian L, Shan Y (2022a) Dual
cross-attention learning for fine-grained visual cate-
gorization and object re-identification. In: CVPR, pp
4692–4702
Zhu K, Guo H, Zhang S, Wang Y, Huang G, Qiao
H, Liu J, Wang J, Tang M (2021b) Aaformer:
Auto-aligned transformer for person re-identification.
arXiv preprint arXiv:210400921
Zhu K, Guo H, Yan T, Zhu Y, Wang J, Tang M
(2022b) Pass: Part-aware self-supervised pre-training
for person re-identification. In: ECCV, Springer Na-
ture Switzerland Cham, pp 198–214Zhuo J, Chen Z, Lai J, Wang G (2018) Occluded person
re-identification. In: ICME, IEEE, pp 1–6
Zuerl M, Dirauf R, Koeferl F, Steinlein N, Sueskind J,
Zanca D, Brehm I, Fersen Lv, Eskofier B (2023) Po-
larbearvidid: A video-based re-identification bench-
mark dataset for polar bears. Animals 13(5):801
Zuo J, Yu C, Sang N, Gao C (2023) Plip: Language-
image pre-training for person representation learning.
arXiv preprint arXiv:230508386
Zuo J, Zhou H, Nie Y, Zhang F, Guo T, Sang N,
Wang Y, Gao C (2024) Ufinebench: Towards text-
based person retrieval with ultra-fine granularity. In:
CVPR, pp 22010–22019