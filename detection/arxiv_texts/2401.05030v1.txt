1
An event-based implementation of saliency-based
visual attention for rapid scene analysis
Camille Simon Chane1, Ernst Niebur2, Ryad Benosman3, Sio-Hoi Ieng3
1ETIS UMR 8051, CY Cergy Paris University, ENSEA, CNRS
2Dept. of Electrical and Computer Engineering and Dept. of Neuroscience, Johns Hopkins University
3Dept. of electrical engineering, Sorbonne University
Abstract —Selective attention is an essential mechanism to filter
sensory input and to select only its most important components,
allowing the capacity-limited cognitive structures of the brain
to process them in detail. The saliency map model, originally
developed to understand the process of selective attention in
the primate visual system, has also been extensively used in
computer vision. Due to the wide-spread use of frame-based
video, this is how dynamic input from non-stationary scenes is
commonly implemented in saliency maps. However, the temporal
structure of this input modality is very different from that of the
primate visual system. Retinal input to the brain is massively
parallel, local rather than frame-based, asynchronous rather than
synchronous, and transmitted in the form of discrete events,
neuronal action potentials (spikes). These features are captured
by event-based cameras. We show that a computational saliency
model can be obtained organically from such vision sensors, at
minimal computational cost. We assess the performance of the
model by comparing its predictions with the distribution of overt
attention (fixations) of human observers, and we make available
an event-based dataset that can be used as ground truth for
future studies.
I. I NTRODUCTION
The primate visual system is capable of dealing with ex-
tremely large amounts of data coming from the eyes without
being overwhelmed. Visual information is pre-processed in
the retina and then sent to the brain viathe optic nerves.
A simple calculation shows that, seen as an information
transmission channel in the Shannon sense, each optic nerve
has a capacity of ≈108bits per second [1], [2]. The channel
is discretized since information is coded in terms of the
presence and absence of discrete events, the action potentials
of retinal ganglion cells. The brain deals with this deluge of
information by the mechanisms of visual selective attention
that allocate resources to process preferentially information
deemed relevant, and suppresses or discards the remainder.
The concept of visual selective attention also plays a crit-
ical role in computer vision, machine learning, and robotic
research, which face the same problem of overwhelming
amounts of data arriving from high-throughput vision sensors.
The ability to process all these data at the rate of their
acquisition is particularly critical for autonomous systems that
are subject to tight power and computational constraints. How
a human brain is capable to process all of the incoming
information and achieve complex tasks by consuming as little
as 20W is not understood, but selective attention is likely one
of the reasons: early selection of relevant information anddiscarding all other presumably saves costly computational
resources.
Selective attention is a complex process that plays a crucial
role at many levels of perception and cognition. It is useful
to distinguish between top-down and bottom-up attention.
The latter can be described as data-driven: where bottom-up
attention is directed is determined by the visual input alone. In
contrast, top-down attention also depends on the internal states
of the observer, for instance their (or, in the case of machine
vision, its) goals. While progress is made in understanding
mechanisms of top-down attention [3]–[6], it is more difficult
to study because it is usually much easier to control visual
input than the internal states of an observer. This is one of the
reasons why bottom-up attention has been studied in much
more detail and why it is also the focus of the present study.
Highly influential conceptual models of visual selective
attention were developed in the 1980s which introduced Fea-
ture Integration Theory [7] and the concept of the saliency
map [8]. The latter was moved from the stage of a conceptual
idea to a quantitative, testable computational model [9]–[11];
review: [12], which is part of the pedigree of the present study.
These early models, as well as many contemporary versions,
were designed for static visual scenes (although an early
version of a dynamic saliency map was already introduced
in 1996 [9]). In the real world, however, agents are interacting
with a continuously changing environment, making the inte-
gration of temporal information in the computation of saliency
a necessity. It is therefore not surprising that saliency map
models have incorporated the effects of visual changes on
the control of selective attention, e.g. [13]–[16]. The vast
majority of such models are frame-based, being applied to
the standard representation of dynamic scenes (e.g. in video)
in terms of series of image ”frames” shown at a sufficiently
high rate to result in the perception of smooth movement.
Of course, primate biology has no concept of image frames.
Instead, sensory input is dynamic and sent asynchronously
from different retinal ganglion cells to the brain.
In the present study, visual input is not represented in the
form of standard frame-based video cameras but, instead, gen-
erated by event-based vision sensors such as the Dynamic Vi-
sion Sensor [17]. Event-based vision sensors asynchronously
capture spatio-temporal constrast changes, akin to the mag-
nocellular pathway in primate vision. They are sensitive to
changes due to motion, onsets and offsets. Visual information
going beyond the detection of changes, providing informationarXiv:2401.05030v1  [eess.IV]  10 Jan 20242
about the local intensity at each pixel is needed for tasks
like object recognition and many others. In the primate visual
system, this is mainly represented in the parvocellular pathway.
We use a variant of event-based cameras, the Asynchronous
Time-based Image Sensor (ATIS) [18] that represents not
only the time when a change is detected (loosely related to
the magnocellular pathway as discussed) but, using a pulse-
width modulation code, also the pixel value at the time of
each change, loosely related to the parvocellular pathway.
These signals are, again asynchronous and binary, akin to
action potentials generated in retinal ganglion cells. In the
current study, a component of the parvocellular pathway, color
sensitivity, is missing, but newer versions of the ATIS sensor
do provide this feature [19], and future work can incorporate
it into saliency computations.
Of particular interest for the present study is that event-
based vision sensors natively capture fast changes in the
visual input which, we argue, is a highly efficient way to
compute saliency. While temporal change is only one of
several contributions to bottom-up saliency [9], [20], it is a
highly important one [14], [15] even though trained artificial
neural networks seem to weigh dynamical changes less than
static features [21].
One main contribution of this work is the creation of an
event-based pipeline used to compute a saliency score for each
event, at the same rate as events occur. This makes the event-
based sensor a native dynamical visual saliency identifier. An
event’s saliency score is the response of spatiotemporal filters
that average the number of events that occurred within a
spatiotemporal neighborhood of the current event. The model
has a bottom-up (feed-forward) architecture and, importantly,
it does not require any training. Since event-based encoding of
information incurs only minimal redundancy, we propose an
easy to implement and computationally highly efficient event-
based saliency map on a generic consumer-grade computer.
Our second main contribution is the compilation of a dataset
of event-based camera data obtained by recording dynamic
visual scenes. They are combined with eye-movement data
collected from human observers while they free-view the
corresponding visual scenes. We use these data to evaluate
the fixation-prediction performance of our model, as well as
that of other dynamical saliency models. We show that the
proposed model outperforms state-of-the-art saliency models
when applied to spatiotemporal or dynamic contents. Since, to
our knowledge, currently no such dataset has been published,
we make our dataset freely available.
II. R ELATED WORK
While most early saliency map models were designed for
still images [10], [11], the importance for temporal change
for attracting attention led to the development of computa-
tional models that use spatio-temporal filters [22], [23] or
localized temporal change (”flicker”) [9], [14] as components
contributing to local saliency. A related approach is the use
of statistical analyses to introduce dynamical components in
saliency models. Bayesian inference is the root of the method
proposed in [24], where the concept of Bayesian surprise isused to characterize spatio-temporal change. Motion features
can then be captured as elements of dynamic saliency. Another
related approach [15] relies on saliency generated from outliers
in a pre-established/learned distribution of motion features.
Feature Integration Theory [7] has spawned a large set
of saliency models that can be classified as ”feature-based”.
The basic elements in these models are elementary visual
features, e.g.oriented line segments, colored areas etc.It is
known, however, that primates use more complex concepts
to structure their visual input to increase efficiency of scene
understanding. Perceptual organization based on Gestalt psy-
chology [25] is the basis for models implementing these ideas.
An important concept is that of proto-objects, the non-semantic
precursors of object representations [26]. (Proto-)Object-based
scene organization has been exploited for a biologically
plausible saliency model and shown to exceed performance
of purely feature-based models in the prediction of overt
attention (fixations) [27]. While the original model [27] did
not include temporal information, a generalization [20], [28]
implements the use of spatio-temporal filters over a subset of
consecutive frames in a sliding window. Dynamic information
and the concept of motion then becomes part of the saliency
computation. The method is strictly feed-forward and does not
require any training which makes it possible to implement it
on dedicated hardware (FPGA) that can run as a standalone
real-time device [16].
Most of these methods are built on biological evidence
and are neuromorphic in this perspective; however, they are
still frame-based rather than acquired in an event-based, asyn-
chronous way. Computation of the saliency model requires
substantial pre-processing due to the redundant information
contained in frames. An alternative comes from event-based
vision sensors that are now experiencing a growing acceptance
in visual sensing. As we show in this work, they are new tools
for tackling the problem of implementing visual attention in
machines. As event-based vision sensors are designed to be
driven by spatio-temporal contrast changes, roboticists [29],
[30] have developed event-based forms of proto-object based
visual saliency.
III. M ODEL
A. Event-based sensors
Event-based sensors available on the market are mainly
variations of the Dynamic Vision Sensor [17], with steadily
improving spatial resolution in newer generations. The core
principle of these sensors is to detect intensity changes at the
level of each individual pixel. A supra-threshold change from
low-to-high intensity generates an ON event, and from high-to-
low an OFF event. Since information is only transmitted when
a temporal change occurs at any given pixel, both spatial and
temporal redundancy are largely reduced in the acquired visual
data. For this reason, the currently technologically available
bandwidth allows update rates in the microsecond range.
In addition to implementing this change detection algorithm,
the ATIS camera also captures brightness (luminance) infor-
mation [18]. Each pixel of this sensor has two detectors: The
first is the contrast change detector that produces pixel-wise3
ON/OFF events as described above. In addition, each event
of this detector triggers a second detector which represents
the luminance of the changed pixel by generating a pair of
events whose time difference encodes the pixel luminance.
The functionality of this mechanism is roughly analogous
to the intensity component of the parvocellular pathway in
the mammalian visual system, although the implementation
(temporal difference coding vs.labeled line coding) is very
different.
B. Saliency model
1) Basic saliency score: We postulate that an event-based
camera implementing temporal contrast change detection is a
native dynamic visual saliency sensor. By collecting events
in a two-dimensional spatial map and updating old values
as new events come in, we are generating a spatiotemporal
saliency map. A limitation is that with the currently used
ATIS sensor only one submodality (intensity) contributes while
traditional models combine multiple submodalities [10], [11],
[31]. Although this is an important constraint, it is not of a fun-
damental nature; it can be addressed by using other hardware
sensors, for instance those including color information [19],
[32].
Let us assume that e={x, y, t}is the current event, occur-
ring at time tand position (x, y). We define a spatiotemporal
history of the event by the spatial neighborhood of size rv
around (x, y)and the duration tubefore t. The saliency score
at that time and position is the response of a filter that sums
the number of events within the thus-defined spatiotemporal
history (rv, tu)as:
Su,v(x, y, t ) =X
i1σ(xi, yi, ti)
(1 + 2 rv)2, (1)
where
σ={ei|x−xi|+|y−yi| ≤rvandt−ti≤tu} (2)
and
1σ(xi, yi, ti) =1if(xi, yi, ti)∈σ
0otherwise, (3)
is the indicator function of the set σ. It is then ”normalized”
by the area of the spatial window of width (1 + 2 rv). We use
the tarsier framework [33] without a decay mechanism to store
the events in a 2D buffer where only the most recent event at
each pixel is stored. This buffer has the same purpose as the
one used in storing a ”time-surface” introduced in [34].
2) Saliency at multiple scales: Equation 1 defines the
saliency score computed for one spatial scale rvand one
temporal window tu. To capture changes at multiple spatiotem-
poral scales, we include six octaves in both space and time.
Formally, we define
rv∈ {2v}0≤v≤5 (4)
tu∈ {10×2u}0≤u≤5ms (5)
and use Su,vfrom eq 1 to define the spatiotemporal saliency
SSTat position (x, y)and time tas,
SST(x, y, t ) =1
STmax5X
u=05X
v=0Su,v(x, y, t ), (6)
tv= 10ms
<latexit sha1_base64="8ARKv5rWQwaOQDELe98wGEbkMxU=">AAAB8XicbVDLSgNBEOyNrxhfUY9eBoPgKexKQD0IQS8eI5gHJkuYncwmQ2Znl5neQAj5Cy8eFPHq33jzb5wke9DEgoaiqpvuriCRwqDrfju5tfWNza38dmFnd2//oHh41DBxqhmvs1jGuhVQw6VQvI4CJW8lmtMokLwZDO9mfnPEtRGxesRxwv2I9pUIBaNopSfsjsgN8dzIdIslt+zOQVaJl5ESZKh1i1+dXszSiCtkkhrT9twE/QnVKJjk00InNTyhbEj7vG2pohE3/mR+8ZScWaVHwljbUkjm6u+JCY2MGUeB7YwoDsyyNxP/89ophlf+RKgkRa7YYlGYSoIxmb1PekJzhnJsCWVa2FsJG1BNGdqQCjYEb/nlVdK4KHuV8vVDpVS9zeLIwwmcwjl4cAlVuIca1IGBgmd4hTfHOC/Ou/OxaM052cwx/IHz+QMlko/x</latexit>
tv= 20ms
<latexit sha1_base64="dxkKM4OZUxWmqf3ai94iYIZYv4Q=">AAAB8XicbVA9SwNBEJ2LXzF+RS1tFoNgFe5CQC2EoI1lBBODyRH2NnvJkr29Y3cuEEL+hY2FIrb+Gzv/jZvkCk18MPB4b4aZeUEihUHX/XZya+sbm1v57cLO7t7+QfHwqGniVDPeYLGMdSughkuheAMFSt5KNKdRIPljMLyd+Y8jro2I1QOOE+5HtK9EKBhFKz1hd0SuScWNTLdYcsvuHGSVeBkpQYZ6t/jV6cUsjbhCJqkxbc9N0J9QjYJJPi10UsMTyoa0z9uWKhpx40/mF0/JmVV6JIy1LYVkrv6emNDImHEU2M6I4sAsezPxP6+dYnjpT4RKUuSKLRaFqSQYk9n7pCc0ZyjHllCmhb2VsAHVlKENqWBD8JZfXiXNStmrlq/uq6XaTRZHHk7gFM7BgwuowR3UoQEMFDzDK7w5xnlx3p2PRWvOyWaO4Q+czx8nGY/y</latexit>
tv= 320ms
<latexit sha1_base64="7k/z45WYdeHR4OHwf2Wm9HTXDMk=">AAAB8nicbVBNSwMxEM36WetX1aOXYBE8ld1aUA9C0YvHCvYDtkvJptk2NJssyWyhLP0ZXjwo4tVf481/Y9ruQVsfDDzem2FmXpgIbsB1v5219Y3Nre3CTnF3b//gsHR03DIq1ZQ1qRJKd0JimOCSNYGDYJ1EMxKHgrXD0f3Mb4+ZNlzJJ5gkLIjJQPKIUwJW8qE3xrf4surGplcquxV3DrxKvJyUUY5Gr/TV7SuaxkwCFcQY33MTCDKigVPBpsVualhC6IgMmG+pJDEzQTY/eYrPrdLHkdK2JOC5+nsiI7Exkzi0nTGBoVn2ZuJ/np9CdB1kXCYpMEkXi6JUYFB49j/uc80oiIklhGpub8V0SDShYFMq2hC85ZdXSata8WqVm8dauX6Xx1FAp+gMXSAPXaE6ekAN1EQUKfSMXtGbA86L8+58LFrXnHzmBP2B8/kDm+2QLw==</latexit>
ru=8
<latexit sha1_base64="BYO30m84rWoN5F4sdCnIYCegC98=">AAAB7XicbVBNSwMxEJ2tX7V+VT16CRbBU9mVgvUgFL14rGA/oF1KNs22sdlkSbJCWfofvHhQxKv/x5v/xmy7B219MPB4b4aZeUHMmTau++0U1tY3NreK26Wd3b39g/LhUVvLRBHaIpJL1Q2wppwJ2jLMcNqNFcVRwGknmNxmfueJKs2keDDTmPoRHgkWMoKNldpqkFzXS4Nyxa26c6BV4uWkAjmag/JXfyhJElFhCMda9zw3Nn6KlWGE01mpn2gaYzLBI9qzVOCIaj+dXztDZ1YZolAqW8Kgufp7IsWR1tMosJ0RNmO97GXif14vMWHdT5mIE0MFWSwKE46MRNnraMgUJYZPLcFEMXsrImOsMDE2oCwEb/nlVdK+qHq16tV9rdK4yeMowgmcwjl4cAkNuIMmtIDAIzzDK7w50nlx3p2PRWvByWeO4Q+czx+hSY6H</latexit>
ru= 16
<latexit sha1_base64="WHzs5kBNmBj3EJHhrTPfOhe+awE=">AAAB7XicbVDLSgNBEOyNrxhfqx69DAbBU9iV4OMgBL14jGAekCxhdjKbjJmdWWZmhbDkH7x4UMSr/+PNv3GS7EETCxqKqm66u8KEM20879sprKyurW8UN0tb2zu7e+7+QVPLVBHaIJJL1Q6xppwJ2jDMcNpOFMVxyGkrHN1O/dYTVZpJ8WDGCQ1iPBAsYgQbKzVVL732z3tu2at4M6Bl4uekDDnqPfer25ckjakwhGOtO76XmCDDyjDC6aTUTTVNMBnhAe1YKnBMdZDNrp2gE6v0USSVLWHQTP09keFY63Ec2s4Ym6Fe9Kbif14nNdFlkDGRpIYKMl8UpRwZiaavoz5TlBg+tgQTxeytiAyxwsTYgEo2BH/x5WXSPKv41crVfbVcu8njKMIRHMMp+HABNbiDOjSAwCM8wyu8OdJ5cd6dj3lrwclnDuEPnM8f2VaOrA==</latexit>
ru= 32
<latexit sha1_base64="UhkyLdIFr8ONA4I931GEuA+SKIw=">AAAB7XicbVBNSwMxEJ2tX7V+VT16CRbBU9mthepBKHrxWMF+QLuUbJptY7PJkmSFsvQ/ePGgiFf/jzf/jWm7B219MPB4b4aZeUHMmTau++3k1tY3Nrfy24Wd3b39g+LhUUvLRBHaJJJL1QmwppwJ2jTMcNqJFcVRwGk7GN/O/PYTVZpJ8WAmMfUjPBQsZAQbK7VUP7m+qPSLJbfszoFWiZeREmRo9ItfvYEkSUSFIRxr3fXc2PgpVoYRTqeFXqJpjMkYD2nXUoEjqv10fu0UnVllgEKpbAmD5urviRRHWk+iwHZG2Iz0sjcT//O6iQkv/ZSJODFUkMWiMOHISDR7HQ2YosTwiSWYKGZvRWSEFSbGBlSwIXjLL6+SVqXsVctX99VS/SaLIw8ncArn4EEN6nAHDWgCgUd4hld4c6Tz4rw7H4vWnJPNHMMfOJ8/1lCOqg==</latexit>
tyx<latexit sha1_base64="XDUJ0ZYRcZC76dW2Qsw2/9xb+5Q=">AAACIHicbVDJSgNBEO1xjeMW9eilMQgeJMyI20UIevEYwSRCJoSeTk3S2NMzdNeIYcinePFXvHhQRG/6NXYWxO1Bw+O9quqqF6ZSGPS8d2dqemZ2br6w4C4uLa+sFtfW6ybJNIcaT2Sir0JmQAoFNRQo4SrVwOJQQiO8Phv6jRvQRiTqEvsptGLWVSISnKGV2sUjOHEDCREGuRuE0BUqjxlqcTtwb3dpf5eiG4DqfImBFt0eBoN2seSVvRHoX+JPSIlMUG0X34JOwrMYFHLJjGn6XoqtnGkUXIIdnBlIGb9mXWhaqlgMppWPDhzQbat0aJRo+xTSkfq9I2exMf04tJV20Z757Q3F/7xmhtFxKxcqzRAUH38UZZJiQodp0Y7QwFH2LWFcC7sr5T2mGUebqWtD8H+f/JfU98r+YfngYr9UOZ3EUSCbZIvsEJ8ckQo5J1VSI5zckQfyRJ6de+fReXFex6VTzqRng/yA8/EJ/Jei6w==</latexit>e= x, y, t …Fig. 1: Illustration of the spatio-temporal neighborhoods for
an event e= (x, y, t ), used to compute the response of
the sum of events filters. Past events (black dots) that pre-
cede eare summed over time windows of length tu∈
{10,20,40,160,320}ms and over spatial windows of radius
ru∈ {1,2,4,8,16,32}.
where the division by STmax = max x,y,t(SST)normalizes
SSTto the range [0,1].
Saliency values are updated asynchronously at each new
event, with all events that occurred in the immediately preced-
ing time windows defined by tuand within the spatial windows
defined by ruas in eq 6 and as illustrated in figure 1. We call
this the event-based Spatio-Temporal (evST) model.
IV. P ERFORMANCE EVALUATION TOOLS
A. Saliency metrics
To obtain an objective performance metric for computa-
tional models of covert selective attention, Parkhurst et al.
proposed to evaluate how well a model predicts eye move-
ments, i.e., overt attention [35]. This has become the standard
method in the field which we also adopt. The paradigm
allows for multiple metrics for the comparison between model
predictions and fixation locations. One class of metrics used
for this purpose are Location-based ; examples are the Area
Under the Curve (AUC) [36]–[38] and its variation. the
shuffled AUC (sAUC) [39]–[41], the Normalized Scanpath
Saliency (NSS) [24], [42], and information gain (IG) [43],
[44]. Another class of metrics are Distribution-based such
as the similarity metric (SIM), known also as histogram
intersection , the Pearson’s Correlation Coefficient (CC) [45],
and the Kullback-Leibler divergence (KL-Div) [46], [47].
It has been suggested [48], [49] that a reliable comparison
of computational models with human fixations requires to use
more than one of these metrics. We therefore evaluate our
model with two location based metrics (NSS and sAUC) and
two distribution based metrics (SIM and CC).4
B. Normalized Scanpath Saliency
The Normalized Scanpath Saliency metric [42] is a discrete
measure of the correlation between fixated locations and their
saliency. It is parameter-free and defined by
NSS =1
NNX
i=1Si−¯S
σs, (7)
where Si, i= 1, . . . , N are the locations of the Nfixations
and¯Sandσsare, respectively, the mean and the standard
deviation of the saliency map S. For NSS = 1, the salience
at fixation locations is on average one standard deviation
above the average while for NSS = 0, the saliency map
prediction is only as good as chance. Note that the smaller the
standard deviation of a saliency map, the better the NSS metric
predicts the fixations locations. Finally, due to the centering
of the NSS by subtracting the mean, the NSS is invariant to
linear transformations of the saliency map, e.g., if the map is
considered as a distribution, its normalization has little impact
on the NSS score [49].
C. Shuffled Area under the ROC Curve
Let us define the correct prediction of a fixation by a
computational attention model as a hit (true positive) and the
incorrect prediction that a non-fixated pixel is a fixation, while
in fact it is not, as a false alarm (false positive). Plotting the
true positive rate against the false positive rate is called the
Receiver Operating Characteristic (ROC). Its integral, or Area
Under the Curve (AUC) provides a natural saliency metric
that considers the saliency map as a classifier for pixels being
fixations or not [36]–[38]. To account for biases common to
all (or many) images, e.g. the tendency of many observers
to focus on the center of a scene, we use the shuffled AUC
(sAUC). Zhang et al. [50] constructed this metric by defining
the hits as defined above and the false alarms as the union of
all fixations of all subjects across all other images, except for
the hits in the currently viewed scene.
D. Similarity
The similarity metric (SIM) was introduced in image con-
tent based matching by Swain and Ballard [51] to quantify
the intersection of histograms. It was widely adopted due to
its simplicity:
SIM(S, FM ) =X
imin(Si, FM i), (8)
where, in our application, SiandFMiare respectively the i-
th pixels of the saliency and the fixation map to be compared,
and the sum runs over all pixels of the maps. Both maps are
normalized as distributions, i.e. the sums of all their elements
are equal to unity. Perfect agreement of SandFM results
in SIM (S, FM ) = 1 while completely different maps give
SIM= 0.E. Pearson’s correlation coefficient
Pearson’s correlation coefficient is a well established sta-
tistical metric used to quantify how closely two variables are
correlated. It is defined as:
CC(S, FM ) =cov(S, FM )
σ(S)×σ(FM), (9)
where covis the covariance and σis the standard deviation. S
and FM have the same definitions as in the similarity metric.
V. G ROUND -TRUTH DATA FOR OVERT ATTENTIONAL
SELECTION
A. Simultaneous acquisition of event based and frame based
dynamic scenes
Videos of indoors and outdoors scenes were acquired with
an ATIS camera which records both change events and gray
level events, defined as a pixel’s intensity when it undergoes
a change event. To generate the frame based videos shown to
the participants, gray level images are read from the gray level
events buffer at 100 frames per second, in agreement with the
temporal accuracy of the eye tracker. Simultaneously, for com-
parison with the frame-based saliency models in section VI,
saliency maps are also generated at 100 frames per second
from the saliency 2D-buffer.
All videos were recorded with a stationary ATIS camera
observing dynamic scenes of moving physical objects (no
simulations). Scenes from six categories were recorded; an
example of a still image of a scene from each category is
shown in figure 2.
(a) Foosball (138)
 (b) Bastille (67)
 (c) Objects (128)
(d) People (26)
 (e) Shapes (42)
 (f) Street (197)
Fig. 2: Sample frames from each type of video category.
Numbers in parentheses give the number of videos for each
category (total 598).
B. Eye-tracking
The video material was used to produce 598 short videos
that were displayed to a group of human observers while their
eye movements were tracked. Each participant watched 300
videos, randomly selected out of the corpus of 598. Twenty
participants (10 female, 10 male) with ages between 20 and 54
years (average = 35 years, std dev = 9.98 years) participated5
(a)
(b)
Fig. 3: Schematic view of eye-tracking acquisition setup. (a)
The left stool was for the volunteer while the operator sat on
the stool on the right, controlling the experiment. (b) The eye-
tracking cameras from the Eyelink II system were fixed to the
chin rest holder.
in the study which was approved by the Institutional Review
Board from the Sorbonne University and executed in the
StreetLab [52]. Participants had normal or corrected to normal
vision and gave written or verbal consent. To limit top-down
effects these clips were limited to a duration of eight seconds
each.
Participants were seated on a height-adjustable stool facing
a 168×94 cm screen, with their chin on a chin rest, figure 3.
This screen was 1.50 m from the chin rest and had a 100 Hz
refresh rate and a resolution of 1024 ×768 pixels. Since the
ATIS camera has a spatial resolution of 304 ×240 pixels only
the center area of the screen was used, with the surrounding
area dark (black).
Participants were naive to the goals of the study. The exper-
iment took approximately one hour and a half per participant
including breaks, an introduction to the free-viewing paradigm
and audio recordings (see below). Viewing time was divided
in ten seven-minute blocks. Each block started with a 9-point
calibration followed by 30 successive alternations of a central
fixation point and an eight second video.Between blocks, participants were encouraged to share three
things they saw in the videos, their answers were recorded
with a microphone. No other instructions were given to the
participants. The purpose of asking these questions was two-
fold: to provide a break in the viewing task, and to maintain
the participants’ attention directed to the videos. The audio
recordings could also be used determine any potential corre-
lations between the vocal responses and the fixation patterns.
This has not been explored in the current study.
While participants were watching the videos, their eye
positions were recorded using a modified Eyelink II (SR
Research, Ottawa, Canada) eye-tracking system. The Eyelink
headset has three cameras: one pointing towards each eye and
one mounted on the forehead to track the global head position.
For this experiment we removed the eye-tracking cameras
from the head-set and attached them to the chin rest. The
third (forward-facing) camera was not used.
VI. R ESULTS
We first quantitatively assess, in Section VI-A, to what ex-
tent participants overtly attend the videos. In Section VI-B, we
explore the influence of different temporal scales over which
events are integrated. In Section VI-C we use the four metrics
introduced above (NSS, sAUC, SIM and CC) to compare
the fixation-prediction performance of the event-based Spatio-
Temporal (evST) method with two models from the literature.
The first is the Itti et al saliency map model [10]. We
used the simplified implementation (”simpsal”) from [53]1and
augmented it with a motion channel and a flicker channel [9],
[13]. The second model is a state-of-the art proto-object
based saliency model [16]. By design this model includes
motion information through the use of spatiotemporal filters on
sequences of frames. We used the code provided by the authors
of that study2. Note that because frames are generated by the
gray level events from the ATIS, the videos watched by the
human participants were gray-level. Therefore, color channels
were inactive in all of the models, including the weakly phasic
channel in [16] that is based on color information.
A. Quantitative assessment of attention to stimuli
The fixations from the eye-tracker provide the ground truth
against which we compare three saliency models. We first
provide an analysis of the robustness of these ground truth
data.
1) Visual attention: Fixations were on average 413 ms long
(std dev = 458 ms ). Some were not on the video but on the
black area of the screen surrounding the video display, see
Section V-B. We consider these ”inattentive” fixations and
use them to compute a visual attention score and determine
whether a participant’s data should be included in the analysis.
We define an attention score as the ratio of attentive fixations
over total fixations. Figure 4 shows great variability in the fix-
ation scores between participants. However, even for the least
attentive participant there are videos for which all fixations are
1http://www.animaclock.com/harel/share/gbvs.php
2https://github.com/csmslab/dynamic-proto-object-saliency6
Fig. 4: Box plot of the variability in attention scores per video
for each participant. Participants are ordered by decreasing
mean attention score (red lines).
in the region of interest. Videos with low attention scores were
removed from the analysis (see below), but this did not result
in the removal of all data from any of the participants. Even
for the ”least attentive” participant, on average the majority of
fixations were on the video portion of the screen.
We decided to eliminate data from videos with low attention
scores. We must choose a compromise between a small but
high quality dataset (given by a high threshold) and a larger
but lower-quality dataset (low threshold). This is illustrated in
Figure 5: For instance, there are 240 videos that are seen by 10
or more participants (upper dashed line). If we remove those
with an attention score lower than 0.5, 152 remain (middle
dashed line); if we remove all videos with an attention score
lower than 0.9, only 11 remain (lower dashed line). We opt for
a high-quality data set with an attention threshold of 0.9, all
following results are obtained from this data set. It comprises
a total of 57,798 fixations from 20 participants in 598 videos.
There are a few other informal observations, not discussed
in detail here. First, the experiment took less time for the
participants who focused on the videos. For those whose
gaze drifted, more re-calibrations were necessary, increasing
the duration of the experiment. We also noticed that the
frequency of inattentive fixations increased over the course
of the experiment, presumably due to fatigue.
B. Integration over time
The impact of the time window on the saliency metrics
is expected to be significant as events are triggered by
scene contents. Scene dynamics, i.e. its changes and rates
of changes, can be correctly captured if the adequate time
window is used. This section evaluates the behavior of the
saliency metrics for increasing temporal window size tu
(eq 5) while summing over all spatial scales. Therefore,
for each value of tuwe compute a map for5X
v=0Su,vand
Fig. 5: Cumulative histogram showing the minimum number
of participants surpassing the plotted attention scores. At least
10 participants saw 240 videos (upper dashed line), 152 of
those were video clips with an attention score exceeding a
threshold of 0.5 (middle dashed line) and 11 with an attention
score of 0.9 (lower dashed line.
normalize it to the range [0,1[as before.
As mentioned in section III-B, the saliency computation
pipeline uses events stored in a 2D limited memory buffer that
inherently provides some temporal adaptation in the compu-
tation of the spatio-temporal saliency. By changing manually
tu, we can more explicitly examine the temporal contribution
of events by excluding the ones not consistent (e.g. too old)
with the stimulus dynamics.
Due to the large amount of saliency maps to be generated,
we conducted this test only over a subset of each category of
the database. We randomly selected 10% of videos from each
category for each of the six values of tu, increasing from 10ms
to 320ms and for the three models.
Table I summarizes the results on the four metrics and also
the results achieved by the full form of the evST model that
integrates over all the time windows. It shows that three of
the four metrics are increasing functions of tu, the exception
being sAUC where in several scene categories the model
performances was best for some shorter time scales than for
the longest one. However, for this and indeed for all four
metrics, the complete evST model (with integration over all
time scales) tends to either achieve the highest scores or is on
par with best score from fixed time windows. As the full evST
model provides in most cases the best performance, we will
use the full model. This is not adding much computational
complexity when compared to the use of only one time
window.
C. Performance comparisons between models
The four metrics are shown for each video and for each
saliency model in figures 6 to 9. Table II provides an assess-
ment of the average performance on each video category and
each metric.7
NSS sAUC
tu(ms) Foosball Bastille Objects People Shapes Street
10 0.725 0.456 0.665 0.544 0.260 0.394
20 0.781 0.466 0.748 0.615 0.372 0.401
40 0.818 0.475 0.831 0.686 0.451 0.434
80 0.873 0.490 0.900 0.683 0.535 0.458
160 0.914 0.507 0.950 0.755 0.661 0.496
320 0.952 0.509 0.998 0.775 0.810 0.534
evST 1.011 0.555 1.060 0.832 0.595 0.479tu(ms) Foosball Bastille Objects People Shapes Street
10 0.688 0.603 0.616 0.610 0.576 0.551
20 0.665 0.596 0.612 0.619 0.610 0.553
40 0.689 0.606 0.619 0.611 0.593 0.546
80 0.659 0.592 0.599 0.630 0.624 0.562
160 0.685 0.604 0.610 0.602 0.633 0.564
320 0.682 0.608 0.613 0.610 0.640 0.558
evST 0.715 0.603 0.620 0.646 0.661 0.564
SIM CC
tu(ms) Foosball Bastille Objects People Shapes Street
10 0.358 0.386 0.342 0.269 0.193 0.259
20 0.381 0.402 0.377 0.321 0.242 0.304
40 0.401 0.416 0.406 0.358 0.289 0.340
80 0.418 0.429 0.431 0.389 0.332 0.370
160 0.434 0.444 0.447 0.405 0.370 0.395
320 0.448 0.457 0.459 0.417 0.400 0.422
evST 0.398 0.436 0.494 0.433 0.385 0.400tu(ms) Foosball Bastille Objects People Shapes Street
10 0.195 0.174 0.296 0.208 0.056 0.145
20 0.216 0.185 0.334 0.237 0.072 0.168
40 0.238 0.193 0.373 0.255 0.089 0.188
80 0.257 0.201 0.409 0.271 0.106 0.206
160 0.274 0.211 0.436 0.284 0.122 0.222
320 0.294 0.219 0.457 0.294 0.138 0.243
evST 0.270 0.217 0.490 0.313 0.122 0.250
TABLE I: Impact on the four metrics of the temporal windows for tu∈[10,320] ms, tested over subsets of each categories.
Larger is better for all metrics. Best scores with a fixed time window are highlighted with boxes and best scores for each
metric are in bold font.
1) NSS: The NSS score, for almost all videos, is highest
for the evST saliency model, as shown in figure 6 and in the
averaged scores in Table II. There is only one video for which
the evST model gives a negative score. In contrast, the Proto
object and the Itti et al models have, respectively, 96 and 243
videos with negative scores. The evST model fares better in
82 % of the videos than the other two models.
The closest competition to the evST model is from the
Proto-object model in the ”Bastille” sequence. Here, there
are several videos where the performance of the Proto-object
model clearly exceeds that of the evST model. However, even
in this video category, the evST model shows overall clearly
superior performance, as is seen in Table II.
2) sAUC: For this metric as well, the evST model achieved
a better performance on average than the two other models as
shown in figure 7 and Table II. The evST model performs
better than the Proto object model and the Itti et al. model for
92 % of the videos.
3) SIM: This metric –and the CC metric as well– does not
directly evaluate saliency value at the pixel level, but instead
compares globally the saliency maps Sagainst the distribution
of fixations FM for each video and for all participants. By this
metric, the Proto object model is superior for all sequences but
”Foosball”, where the Itti et al. model achieves best, as shown
by figure 8 and Table II. The evST model does not achieve
the highest average score in any of the video categories.
4) CC: Each of the three models outperforms the others in
at least one video category by this metric, as shown by figure 9
and Table II. However, the evST model overall dominates the
results, with highest scores in the majority (4/6) of categories
while the other two models are best in only a single category
each.VII. D ISCUSSION AND CONCLUSION
We compare the predictive power of our newly developed
saliency model with that of two established computational
models. We selected these models because they were devel-
oped for explaining neuronal mechanisms of visual selective
attention, rather than being fine-tuned (trained) to perform
optimally in eye-movement prediction benchmark tests. In
fact, the two models we compare evST with were not trained
at all, and obviously no training is involved in the evST model
itself.
We find that evST is highly competitive with both of
these models. As Table II shows, evST shows the highest
performance in all scene categories for two of the metrics we
used (NSS and sAUC) and in the majority (four out of six)
of scene categories in a third one (CC). In the fourth category
(SIM), the Proto-object model dominates the other two but
evST is second-best in four of the six scene categories. Over
all scene categories and metrics, evST obtained the highest
score in 16 out of 24 categories, far surpassing both other
models (Proto-object: 6, Itti et al: 2).
We acknowledge that we had to use slightly simplified
versions of the two comparison models because both use
color contrast as one of their features. Since our recordings
are gray-scale, we simplified both models by removing the
corresponding color-based feature maps. We also point out
that the original Itti et al model [10] operates entirely on
static scenes, i.e., it considers image frames as independent
and ignores all temporal relationships between them, including
differences between frames. For this reason, we felt it would be
”unfair” (favoring the evST model) to compare the evST model
with the original model using dynamic scenes. Therefore,
we used an extended version of the Itti et al model which
is augmented by both a flicker and a motion component.
Nevertheless, even with these augmentations, the evST model
predicts eye fixations substantially better than the Itti et al8
NSS sAUC
Foosball Bastille Objects People Shapes Street
evST 0.721 0.575 0.873 0.731 0.694 0.689
Proto object 0.164 0.515 0.561 0.324 0.306 0.311
Itti 0.423 −0.233 0.203 0.130 0.127 0.115Foosball Bastille Objects People Shapes Street
evST 0.641 0.615 0.643 0.637 0.647 0.635
Proto object 0.521 0.476 0.559 0.562 0.553 0.549
Itti 0.529 0.482 0.522 0.539 0.496 0.542
SIM CC
Foosball Bastille Objects People Shapes Street
evST 0.401 0.452 0.435 0.422 0.418 0.406
Proto object 0.423 0.502 0.479 0.425 0.423 0.440
Itti 0.466 0.314 0.405 0.354 0.411 0.333Foosball Bastille Objects People Shapes Street
evST 0.183 0.262 0.332 0.281 0.238 0.269
Proto object 0.080 0.324 0.287 0.189 0.148 0.151
Itti 0.232 −0.190 0.096 0.039 0.038 0.002
TABLE II: All four metrics averaged scores per video for the three models. Larger is better and highest scores are in bold.
Fig. 6: NSS scores for all videos for all three models and each category. Scores are plotted in ascending order of the evST
model (blue). In red and black are shown results from the Proto Object model and of the Itti et al model, respectively. An
NSS score of zero corresponds to random choices.
Fig. 7: sAUC scores for all videos for all three models. Notations are as in Fig 6. An sAUC value of 0.5 means the saliency
model is close to random guesses.9
Fig. 8: SIM scores for all videos given for all three models. Notations as in Fig 6.
Fig. 9: CC scores for all videos for all three models. Notations as in Fig 6. Zero value means chance performance.
model. More surprising that this ”classic” model is surpassed
in performance by the evST model is that the latter overall
also exceeds the performance of a state-of-the-art proto-object
based model [16], though by a smaller margin. In addition
to taking into account Gestalt-laws of perceptual organization
to structure the static feature maps of earlier models, this
model also includes dynamic (motion) features. It does not,
however, include localized temporal differences (flicker), the
only feature present in the evST model. This feature was
introduced in a very early saliency map [9], [54] but, perhaps
surprisingly, omitted in many later models.
While our benchmarking results indicate a highly cred-
ible performance of evST for predicting human saccades,
as mentioned previously our emphasis is not on optimizing
benchmark numbers. Instead, our goal is to develop and
understand the mechanisms that control the deployment ofvisual selective attention, specifically bottom-up attention that
is determined by saliency. We feel that there are two reasons
why the event-based signal is highly promising for use as a
saliency indicator.
The first is efficiency. Compared to other feature maps,
computation of local change (flicker) requires few computa-
tional resources even in standard frame-based technology (e.g.,
different from many commonly used feature types contributing
to saliency, no convolutions are required to identify areas
with temporal change, only a pixel-wise difference between
frames). We do not generate frames of saliency maps, in
fact the notion of an image ”frame” has as little meaning
in our model as it has in biology. Forgoing all the related
complexities and constraints associated with standard image
frame-based technology we can update the saliency of each
pixel with microsecond precision.10
This advantage is highly amplified in event-based neuro-
morphic technology. Our saliency measure is obtained with
minimal computational effort, essentially counting local events
in the immediate event history followed by a simple normal-
ization step, eq 6. We thus propose that the raw event signal
originating in event-based vision sensors followed by minimal
computation can serve as a first approximation of a saliency
map. Saliency is thus obtained nearly ”for free” (for a related
idea in the domain of saliency in biological vision see [55]).
What is the price to pay for this nearly unsurpassable level
of efficiency? The most obvious is that temporal change is only
one component out of several that make a visual scene element
salient; other submodalities clearly contribute to saliency. This
is most obvious in still images where using time-invariant vi-
sual features exclusively is sufficient for computational models
of selective attention to make predictions that are significantly
better than chance [35]. However, in most naturally occurring
situations complete lack of motion/change is rare. Itti [14]
found that temporal change (and motion) are more reliable
predictors of human saccades than static visual features. While
best prediction performance is achieved when all features
(static and dynamic) are taken into account, and we therefore
pay a price in efficacy in the evST model by using only
temporal change, this price has to be weighed against the
efficiency of using the raw event signal directly, with only
minimal computational effort and memory load, as discussed
above.
The second, and likely related reason for focusing on
temporal change for saliency computation is its high ecolog-
ical relevance. The first explicit computational saliency map
models [10], [11] were developed for still images and based
on the differences of static features (intensity, color, depth
etc) in space . In contrast, saliency in the current study is
computed entirely from local contrast in time . Both temporal
and spatial contrast contribute to computational saliency, with
the former having more influence than the latter [14]. In many
areas, responses to transient stimuli are of great importance in
biological vision and in neuronal processing in general where
phasic (transient) responses are very common.
In the more narrow context of selective attention, which
is the focus of our research, it has been known for a long
time that rapid changes in the visual field attract attention.
For instance, abrupt visual onsets are known to ”capture” at-
tention [56]–[59], resulting in preferential processing of visual
input at their location, although this capture can be overridden
in some cases [60]. This is closely related to the phenomenon
of exogenous attention. While endogenous attention is guided
by symbolic cues, e.g. an arrowhead pointiing towards the to-
be-attended region, exogenous attention is typically controlled
by a peripheral cue e.g. a dot or a small bar that is briefly pre-
sented next to the target immediately preceding target onset.
This results in an involuntary, automatic, fast ( ≈hundred ms
vs several hundreds of ms for endogenous attention) direction
of attention to the target area [61].
We thus propose that there may be a strong connection
between the events we use in the evST model and the
behavioral (and neuronal) effects of exogenous attention.
This may explain the good performance of a model that isexclusively based on a very restricted amount of information,
the raw events indicating the locations where change occurs in
a scene. While there are many important aspects of the control
of visual selective attention (endogenous attention, mentioned
above, or any of the purely spatial features that have
dominated early saliency maps) which have not implemented
localized temporal change, our results indicate that this
extremely simple mechanism may capture a surprisingly large
part of a complex function of perception and cognitive. While
motivated in biology, we postulate that these mechanisms
are equally applicable to the implementation of machine
intelligence.
Acknowledgments Work supported by ONR grant N00014-
22-1-2699 and NSF grant 2223725; by French National Re-
search Agency (ANR) grant ANR-20-CE23-0021.
REFERENCES
[1] K. Koch, J. McLean, M. Berry, P. Sterling, V . Balasubramanian, and
M. Freed, “Efficiency of information transmission by retinal ganglion
cells,” Current biology , vol. 14, pp. 1523–1530, 2004.
[2] P. Le Callet and E. Niebur, “Visual Attention and Applications in
Multimedia Technologies,” IEEE Proceedings , vol. 101, no. 9, pp. 2058–
67, 2013, nIHMS539064.
[3] F. Baluch and L. Itti, “Mechanisms of top-down attention,” Trends in
neurosciences , vol. 34, no. 4, pp. 210–224, 2011.
[4] C. E. Connor, H. E. Egeth, and S. Yantis, “Visual attention: bottom-
up versus top-down,” Current biology , vol. 14, no. 19, pp. R850–R852,
2004.
[5] S. Mihalas, Y . Dong, R. von der Heydt, and E. Niebur, “Mechanisms
of perceptual organization provide auto-zoom and auto-localization for
attention to objects,” Proceedings of the National Academy of Sciences ,
vol. 108, no. 18, pp. 7583–8, 2011, pMC3088583.
[6] M. Usher and E. Niebur, “Modeling the Temporal Dynamics of IT
Neurons in Visual Search: A Mechanism for Top-Down Selective
Attention,” J. Cognitive Neuroscience , vol. 8, no. 4, pp. 311–327, 1996.
[7] A. Treisman and G. Gelade, “A feature integration theory of attention,”
Cognitive psychology , vol. 12, no. 1, pp. 97–136, 1980.
[8] C. Koch and S. Ullman, “Shifts in Selective Visual Attention: Towards
the Underlying Neural Circuitry,” Human neurobiology , vol. 4, no. 4,
pp. 219–227, 1985.
[9] E. Niebur and C. Koch, “Control of Selective Visual Attention: Modeling
the “Where” Pathway,” in Neural Information Processing Systems , D. S.
Touretzky, M. C. Mozer, and M. E. Hasselmo, Eds. Cambridge, MA:
MIT Press, 1996, vol. 8, pp. 802–808.
[10] L. Itti, C. Koch, and E. Niebur, “A Model of Saliency-Based Visual
Attention for Rapid Scene Analysis,” IEEE Transactions on Pattern
Analysis and Machine Intelligence , vol. 20, no. 11, pp. 1254–1259, 1998.
[11] L. Itti and C. Koch, “Computational modelling of visual attention,”
Nature Neuroscience , vol. 2, pp. 194–203, 2001.
[12] E. Niebur, “The Saliency Map,” Scholarpedia , vol. 2, no. 8, p. 2675,
2007.
[13] L. Itti, N. Dhavale, and F. Pighin, “Realistic avatar eye and head
animation using a neurobiological model of visual attention,” in
Proceedings of SPIE Vol. 5200, Applications and Science of Neural
Networks, Fuzzy Systems, and Evolutionary Computation VI , vol. 64,
2004. [Online]. Available: http://link.aip.org/link/?PSI/5200/64/1 {&}
Agg=doi
[14] L. Itti, “Quantifying the contribution of low-level saliency to human eye
movements in dynamic scenes,” Visual Cognition , vol. 12, pp. 1093–
1123, 2005.
[15] R. Rosenholtz, “A simple saliency model predicts a number of motion
popout phenomena,” Vision Research , vol. 39, no. 19, pp. 3157–3163,
1999.
[16] J. Molin, C. Thakur, E. Niebur, and R. Etienne-Cummings, “A neu-
romorphic proto-object based dynamic visual saliency model with a
hybrid implementation,” IEEE Transactions on Biomedical Circuits and
Systems , vol. 15, no. 3, pp. 580–594, 2021.
[17] P. Lichtsteiner, C. Posch, and T. Delbr ¨uck, “A 128 ×128 120 db 15 mus
latency asynchronous temporal contrast vision sensor,” IEEE Journal of
solid-state circuits , vol. 43, no. 2, pp. 566–576, 2008.11
[18] C. Posch, D. Matolin, and R. Wohlgenannt, “A qvga 143 db dynamic
range frame-free pwm image sensor with lossless pixel-level video
compression and time-domain cds,” IEEE Journal of solid-state circuits ,
vol. 46, no. 1, pp. 259–275, 2011.
[19] A. Marcireau, S.-H. Ieng, C. Simon-Chane, and R. B. Benosman,
“Event-based color segmentation with a high dynamic range sensor,”
Frontiers in neuroscience , vol. 12, p. 135, 2018.
[20] J. Molin, R. Etienne-Cummings, and E. Niebur, “How is Motion
Integrated into a Proto-Object Based Visual Saliency Model?” in 49th
Annual Conference on Information Sciences and Systems IEEE-CISS-
2015 . IEEE Press, 2015, pp. 1–6.
[21] M. Tangemann, M. K ¨ummerer, T. S. Wallis, and M. Bethge, “Measuring
the importance of temporal features in video saliency,” in European
Conference on Computer Vision . Springer, 2020, pp. 667–684.
[22] D. J. Parkhurst, “Selective attention in natural vision: using computa-
tional models to quantify stimulus driven attentional allocation,” Ph.D.
dissertation, Johns Hopkins University, April 2002.
[23] D. Gao, V . Mahadevan, and N. Vasconcelos, “On the plausibility of the
discriminant center-surround hypothesis for visual saliency,” Journal of
vision , vol. 8, no. 7, pp. 13–13, 2008.
[24] L. Itti and P. Baldi, “Bayesian surprise attracts human attention,”
Advances in neural information processing systems , pp. 547–554, 2005.
[25] E. Rubin, Visuell wahrgenommene Figuren . Copenhagen: Gyldendalske,
1921.
[26] R. A. Rensink, “The dynamic representation of scenes,” Visual Cogni-
tion, vol. 7, no. 1/2/3, pp. 17–42, 2000.
[27] A. F. Russell, S. Mihalas ¸, R. von der Heydt, E. Niebur, and R. Etienne-
Cummings, “A model of proto-object based saliency,” Vision Research ,
vol. 94, pp. 1–15, 2014.
[28] J. L. Molin, A. F. Russell, S. Mihalas, E. Niebur, and R. Etienne-
Cummings, “Proto-object based visual saliency model with a motion-
sensitive channel,” in Biomedical Circuits and Systems Conference
(BioCAS), 2013 IEEE . IEEE, 2013, pp. 25–28.
[29] M. Iacono, G. D’Angelo, A. Glover, V . Tikhanoff, E. Niebur, and
C. Bartolozzi, “Proto-object based saliency for event-driven cameras,” in
IEEE/RSJ International Conference on Intelligent Robots and Systems
(IROS) , 2019, pp. 805–812.
[30] P. L. A. Gabbott, K. A. C. Martin, and D. Whitteridge, “Connections
between pyramidal neurons in layer 5 of cat visual cortex (area 17),” J.
Comp. Neurol. , vol. 259, pp. 364–381, 1987.
[31] C. Koch and S. Ullman, “Shifts in selective visual attention: towards
the underlying neural circuitry,” Human Neurobiol. , vol. 4, pp. 219–227,
1985.
[32] R. Berner and T. Delbruck, “Event-based pixel sensitive to changes of
color and brightness,” IEEE Transactions on Circuits and Systems I:
Regular Papers , vol. 58, no. 7, pp. 1581–1590, 2011.
[33] A. Marcireau, S.-H. Ieng, and R. Benosman, “Sepia, Tarsier, and
Chameleon: A Modular C++ Framework for Event-Based Computer
Vision,” Frontiers in Neuroscience , vol. 13, Jan 2020. [Online].
Available: http://dx.doi.org/10.3389/fnins.2019.01338
[34] X. Lagorce, G. Orchard, F. Gallupi, B. Shi, and R. Benosman, “Hots:
A hierarchy of event-based time-surfaces for pattern recognition,” IEEE
Transactions on Pattern Analysis and Machine Intelligence , vol. 7, pp.
1346–1359, 2017.
[35] D. Parkhurst, K. Law, and E. Niebur, “Modelling the role of salience
in the allocation of visual selective attention,” Vision Research , vol. 42,
no. 1, pp. 107–123, 2002.
[36] D. M. Green and J. A. Swets, Signal detection theory and psychophysics .
Wiley New York, 1966.
[37] M. Emami and L. L. Hoberock, “Selection of a best metric and
evaluation of bottom-up visual saliency models,” Image and Vision
Computing , vol. 31, no. 10, pp. 796–808, 2013.
[38] U. Engelke, H. Liu, J. Wang, P. Le Callet, I. Heynderickx, H. J.
Zepernick, and A. Maeder, “Comparative study of fixation density
maps,” IEEE Transactions on Image Processing , vol. 22, no. 3, pp.
1121–1133, 2013.
[39] A. Borji, D. N. Sihite, and L. Itti, “Quantitative analysis of human-
model agreement in visual saliency modeling: a comparative study,”
IEEE Transactions on Image Processing , vol. 22, no. (1), pp. 55–69,
2013.
[40] A. Borji, H. R. Tavakoli, D. N. Sihite, and L. Itti, “Analysis of scores,
datasets, and models in visual saliency prediction,” Proceedings of the
IEEE International Conference on Computer Vision , pp. 921–928, 2013.
[41] J. Li, C. Xia, Y . Song, S. Fang, and X. Chen, “A data-driven metric
for comprehensive evaluation of saliency models,” Proceedings of the
IEEE International Conference on Computer Vision , vol. 2015 Inter, pp.
190–198, 2015.[42] R. J. Peters, A. Iyer, L. Itti, and C. Koch., “Components of bottom-up
gaze allocation in natural images.” Vision Research , vol. 45, no. 18, pp.
2397 – 2416, 2005.
[43] M. K ¨ummerer, T. Wallis, and M. Bethge, “How close are we
to understanding image-based saliency?” 2014. [Online]. Available:
http://arxiv.org/abs/1409.7686
[44] M. K ¨ummerer, T. S. A. Wallis, and M. Bethge, “Information-theoretic
model comparison unifies saliency metrics,” Proceedings of the National
Academy of Sciences , vol. 112, no. 52, pp. 16 054–16 059, 2015.
[45] K. Pearson, “Contributions to the mathematical theory of evolution.”
Philosophical Transactions of the Royal Society of London. A , vol. 185,
pp. 71–110, 1894.
[46] O. Le Meur, P. Le Callet, and D. Barba, “Predicting visual fixations
on video based on low-level visual features,” Vision Research , vol. 47,
no. 19, pp. 2483–2498, 2007.
[47] N. Wilming, T. Betz, T. C. Kietzmann, and P. K ¨onig, “Measures and
limits of models of fixation selection,” Plos One , vol. 6, no. 9, pp. 1–19,
2011.
[48] N. Riche, M. Duvinage, M. Mancas, B. Gosselin, and T. Dutoit,
“Saliency and Human Fixations : State-of-the-Art and Study of Compar-
ison Metrics,” in IEEE International Conference on Computer Vision ,
2013, pp. 1153–1160.
[49] Z. Bylinskii, T. Judd, A. Oliva, A. Torralba, and F. Durand, “What
do different evaluation metrics tell us about saliency models?” IEEE
Transactions on Pattern Analysis and Machine Intelligence , vol. 41,
no. 3, pp. 740–757, 2019.
[50] L. Zhang, M. H. Tong, T. K. Marks, H. Shan, and G. W. Cottrell, “Sun:
A bayesian framework for saliency using natural statistics,” Journal of
vision , vol. 8, no. 7, pp. 32–32, 2008.
[51] M. J. Swain and D. H. Ballar, “Color indexing,” International Journal
of Computer Vision , vol. 7, no. 1, 1991.
[52] http://www.streetlab-vision.com, accessed 2023-03-09.
[53] J. Harel, “A saliency implementation in matlab,” http://www.animaclock.
com/harel/share/gbvs.php.
[54] E. Niebur and C. Koch, “Modeling The “Where” Visual Pathway,” in
Proceedings of 2nd Joint Symposium on Neural Computation, Caltech–
UCSD , T. J. Sejnowski, Ed. La Jolla: Institute for Neural Computation,
1995, vol. 5, pp. 26–35.
[55] R. VanRullen, “Visual saliency and spike timing in the ventral visual
pathway,” Journal of Physiology Paris , vol. 97, no. 2-3, pp. 365–377,
2003.
[56] S. Yantis and J. Jonides, “Abrupt visual onsets and selective attention:
evidence from visual search,” J Exp Psychol Hum Percept Perform ,
vol. 10, pp. 601–621, Oct 1984.
[57] J. Jonides and S. Yantis, “Uniqueness of abrupt visual onset in capturing
attention,” Perception & Psychophysics , vol. 43, no. 4, pp. 346–354,
1988.
[58] D. Schreij, C. Owens, and J. Theeuwes, “Abrupt onsets capture attention
independent of top-down control settings,” Perception & psychophysics ,
vol. 70, pp. 208–218, 2008.
[59] E. Ruthruff, C. Hauck, and M.-C. Lien, “What do we know about
suppression of attention capture?” Visual Cognition , vol. 29, no. 9, pp.
604–607, 2021.
[60] W. F. Bacon and H. E. Egeth, “Overriding stimulus-driven attentional
capture,” Perception & Psychophysics , vol. 55, pp. 485–496, 1994.
[61] K. Anton-Erxleben and M. Carrasco, “Attentional enhancement of
spatial resolution: linking behavioural and neurophysiological evidence,”
Nature Reviews Neuroscience , vol. 14, no. 3, pp. 188–200, 2013.