JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 1
Another Vertical View: A Hierarchical Network
for Heterogeneous Trajectory Prediction via
Spectrums
Beihao Xia*, Conghao Wong*, Duanquan Xu, Qinmu Peng, and Xinge Y ou ( B),Senior Member, IEEE
Abstract —With the fast development of AI-related techniques, the applications of trajectory prediction are no longer limited to easier
scenes and trajectories. More and more trajectories with different forms, such as coordinates, bounding boxes, and even
high-dimensional human skeletons, need to be analyzed and forecasted. Among these heterogeneous trajectories, interactions
between different elements within a frame of trajectory, which we call “Dimension-wise Interactions”, would be more complex and
challenging. However, most previous approaches focus mainly on a specific form of trajectories, and potential dimension-wise
interactions are less concerned. In this work, we expand the trajectory prediction task by introducing the trajectory dimensionality M,
thus extending its application scenarios to heterogeneous trajectories. We first introduce the Haar transform as an alternative to Fourier
transform to better capture the time-frequency properties of each trajectory-dimension. Then, we adopt the bilinear structure to model
and fuse two factors simultaneously, including the time-frequency response and the dimension-wise interaction, to forecast
heterogeneous trajectories via trajectory spectrums hierarchically in a generic way. Experiments show that the proposed model
outperforms most state-of-the-art methods on ETH-UCY , SDD, nuScenes, and Human3.6M with heterogeneous trajectories, including
2D coordinates, 2D/3D bounding boxes, and 3D human skeletons.
✦
1 I NTRODUCTION
TRAJECTORY prediction aims at inferring agents’ possi-
ble future trajectories considering potential influencing
factors. It is an essential but challenging task, which can
be widely applied to behavior analysis [1], robot naviga-
tion and planning [2], autonomous driving [3], tracking
[4], detection [5], and many other tasks [6], [7]. Accord-
ing to previous works [8], [9], trajectory prediction mainly
involves homogeneous and heterogeneous trajectory pre-
diction. Different from homogeneous trajectory prediction,
heterogeneous trajectory prediction aims to handle agents
with different types and preferences simultaneously. With
the gradual enrichment of agents involved in this task ( e.g.,
pedestrians, bikers, and carts), more researchers start explor-
ing how to represent the interactions among heterogeneous
agents [8], [9], [10], which is a hot and exciting problem.
Forms of Trajectories. Unfortunately, although these
methods have achieved some success in complex scenar-
ios, most of them mainly focus on heterogeneous agents
but pay less attention to the heterogeneity of trajectories
caused by differences in trajectory representations. In detail,
most researchers in predicting trajectories from videos are
working on forecasting two-dimensional (2D) trajectories
rather than trajectories with different structures. Recently,
more datasets with full 3D or high-dimensional annotations
have become accessible [11]. Accordingly, various trajectory
forms could arise in different scenarios and applications. As
shown in Fig. 1, the trajectory could be formed with either
coordinates or bounding boxes, even skeletons. However,
* Equal contribution. Codes at https://github.com/cocoon2wong/E-Vertical.
The authors are with Huazhong University of Science and Technology, Wuhan,
Hubei, P .R.China. Email: xbh hust@hust.edu.cn, conghaowong@icloud.com,
{pengqinmu, xudq, youxg }@hust.edu.cn
(𝑥𝑙,𝑦𝑙)
(𝑥𝑟,𝑦𝑟)(𝑥𝑙,𝑦𝑙,𝜃𝑙=0)
(𝑥𝑟,𝑦𝑟,𝜃𝑟)
(𝑥1,𝑦1,𝑧1)
(𝑥2,𝑦2,𝑧2)
(𝑥𝑁,𝑦𝑁,𝑧𝑛) (𝑥2,𝑦2,𝑧2)
𝐪=𝑤+𝑥𝑖+𝑦𝑗+𝑧𝑘
(𝑥1,𝑦1,𝑧1)
(𝑥2,𝑦2,𝑧2)(𝑥1,𝑦1,𝑧1)
𝐩𝑡=𝑥1,𝑦1,𝑧1,𝑥2,𝑦2,𝑧2
𝑀=6
(c) 3D Box𝐩𝑡=𝑥1,𝑦1,𝑧1,𝑥2,𝑦2,𝑧2,𝑤,𝑥,𝑦,𝑧
𝑀=10
(d) 3D Box with Rotation𝐩𝑡=𝑥1,𝑦1,…,𝑥𝑁,𝑦𝑁
𝑀=3𝑁
(e) 3D Skeleton𝐩𝑡=𝑥𝑙,𝑦𝑙,𝑥𝑟,𝑦𝑟
𝑀=4
(a) 2D Box𝐩𝑡=𝑥𝑙,𝑦𝑙,𝑥𝑟,𝑦𝑟,𝜃𝑟
𝑀=5
(b) 2D Box w/ RotationFig. 1. Examples of several trajectory forms. Different forms of trajec-
tories may exist in complex scenarios regardless of agent categories.
Trajectories are no longer limited to 2D coordinate series.
most methods could only handle one of these trajectory
forms, especially mostly 2D trajectories. Thus, the main
focus of this manuscript is to find a “uniform” trajectory
prediction structure, thus making it available to forecast
trajectories with different forms according to different appli-
cation requirements. Referring to previous “heterogeneous
agents”, this manuscript defines trajectories with different
representation forms as “Heterogeneous Trajectories” .
Heterogeneous Trajectories and Challenges. In this
manuscript, we denote the length of a frame of recorded
trajectory vector at some moment as Mand call the cor-
responding trajectory the M-dimensional trajectory . The het-
erogeneous trajectories could be treated as a set of M-
dimensional trajectories, where M∈ {M1, M2, ...}. We par-
tially list heterogeneous trajectories with different dimen-
sionalities Min 2D and 3D scenes in Fig. 1. For example,
various bounding boxes can be treated as a trajectory frame.
Even the high-dimensional human skeleton could also be
considered as a frame of trajectory. With the development
of various sensing technologies, trajectories with more di-
mensions ( M) may also be required for prediction. Thus,
compared to homogeneous trajectories, the most significant
difference in heterogeneous trajectory prediction is thatarXiv:2304.05106v2  [cs.CV]  3 Dec 2024JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 2
𝑥!,𝑦!𝑥",𝑦"
𝑿#𝑿$
𝑿%𝑿&Interactions
Global PlanningsInteractive Preferences
1Nodes2 Nodes3 Nodes𝑁−2 Nodes𝑁−1 Nodes𝑁 Nodes…Reconstructwith IDFT(𝑛 points)……𝐴Φ𝑁𝑛𝑁𝑛DecomposewithDFT
Number of Used Frequency NodesLower-Frequency PortionsAdd More High-Frequency Portions
𝑡=1𝑡=2𝑡=3𝑡=𝑡!−1𝑡=𝑡!…
𝑥"!−𝑥#!𝑥""−𝑥#"𝑦"!−𝑦#!𝑦""−𝑦#"
𝑥"#−𝑥##𝑦"#−𝑦##Dimension-WiseInteractionsOverthe Observation Period…
𝑥"$%−𝑥#$%𝑦"$%−𝑦#$%
𝑥"$%&!−𝑥#$%&!𝑦"$%&!−𝑦#$%&!Time-FrequencyResponse(Within Single Trajectory-Dimension)Dimension-Wise Interactions(Among Trajectory-Dimensions)
……Social InteractionsScene Constraints
HeterogeneousTrajectories0\\\(𝑀Trajectory-Dimensions)
Fig. 2. Illustrations of challenges in heterogeneous trajectory prediction, i.e., “Time-Frequency Response” and “Dimension-wise Interaction”.
it considers different forms of trajectories when making
predictions, especially those trajectory forms with higher
dimensionalities. Accordingly, as shown in Fig. 2, two main
challenges have arisen corresponding to the higher dimen-
sionality, including the description of a single trajectory-
dimension and the interactions among dimensionalities.
Challenge A. Modeling of a Single Trajectory-
Dimension. The M-dimensional trajectory has M
trajectory-dimensions. Each dimension is a time sequence.
Thus, modeling and analyzing each dimension significantly
matters when forecasting heterogeneous trajectories.
However, most current approaches may lack the overall
analyses of all factors at different temporal scales. Most
agents, such as pedestrians, always simultaneously plan
their future activities at different “levels”. For example, they
may first plan their coarse motion trends and then make
fine decisions about interactive variations. Although some
methods employ networks with attention mechanisms [12]
(like Graph Attentive Networks [13] and Transformers [14],
[15]) as backbones to model agents’ status, they may have
difficulty directly representing agents’ detailed motions at
different temporal scales. In addition, capturing sudden
changes in trajectories is a problem that has not been well
addressed. Especially for heterogeneous trajectories, in
which a single dimension could also change dramatically
over time, not to mention all Mdimensions, modeling and
encoding these dimensions has become another challenge.
The classical theories of time-frequency analysis provide
us with ideas to address this challenge. The Fourier trans-
form (FT), and its variations have significantly succeeded
in signal processing and computer vision communities [16],
[17]. They decompose signals into a series of sinusoids
on different frequency portions to reflect the frequency
response of different frequency scales that may be difficult
to obtain in original signals directly. However, the Fourier
transform only knows which frequency components the
signal contains and cannot know the frequency information
of the signal at different times from the frequency domain,
which is unsuitable for analyzing a signal whose frequency
changes over time. Especially in heterogeneous trajectory
prediction, agents’ diverse motion patterns, differentiated
behavior preferences, and uncertain future decisions shift
trajectories irregularly over time, making Fourier transform
challenging to handle. Wavelet Transforms (WTs) have bet-ter capabilities in terms of multi-resolution and multi-scale
characteristics for adaptive time-frequency signal analyses
and achieve great success in numerous tasks [18], [19], [20],
prompting it to be selected as an alternative to the Fourier
transform to deal with rapidly changing trajectories. Similar
to the Fourier transform, signals can be represented by a
series of wavelets in the wavelet transform, where wavelets
can be localized in both time and frequency. Therefore, we
can choose different transformations to cope with diverse
heterogeneous trajectory prediction scenarios. They provide
a “vertical” view for processing and analyzing sequences,
thus presenting elusive features in original signals.
Interestingly, Becker et al. [21] find that the latest two
observed steps contribute to the predicted trajectory for a
surprising 88.3%. Similarly, Monti et al. [22] also indicate
that subsequent states contribute more when forecasting.
The above study shows that trajectory prediction can also
be achieved with a few trajectory points containing most of
the trajectory’s energy. Recently, some works have divided
trajectory prediction into a two-stage pipeline by consider-
ing the contribution of each future step, which we call the
“hierarchical prediction” strategy. For example, [23], [24],
[25], [26] have been proposed to predict agents’ keypoints,
waypoints, or destinations first and then interpolate the
complete trajectories under the chosen conditions. Inspired
by these approaches, a natural thought is to hierarchically
predict agents’ trajectories at different frequency scales , in-
cluding Global Plannings and Interactive Preferences . Corre-
spondingly, the low-frequency portions in trajectories could
reflect agents’ motion trends, and the high-frequency por-
tions as agents’ detailed interactions, shown in Fig. 2. Fore-
casting trajectories hierarchically via spectrums could better
describe multi-scale characteristics due to the flexibility of
splitting based on different frequency components without
specifying waypoints and destinations. At the same time,
when the trajectory is transferred from the time domain to
the frequency domain, the energy contained in the trajec-
tory will be redistributed. Therefore, a suitable keypoints
selection strategy is crucial. Too few keypoints will cause
excessive energy loss, resulting in large deviations in the re-
constructed trajectory; on the contrary, too many keypoints
cannot reflect the superiority of hierarchical prediction.
One natural thought is that transforms could be used
before the model implementing to learn agents’ behaviorsJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 3
at different scales via spectrums hierarchically. Considering
the properties of transforms, to model a single trajectory-
dimension, we employ Discrete Fourier Transform and
Discrete Haar Transform to get the corresponding time-
frequency joint representation, i.e., trajectory spectrums, to
capture agents’ detailed motion preferences at different fre-
quency scales, thus better modeling trajectory-dimensions.
Challenge B. Interactions among Dimensionalities.
Another important change is that heterogeneous trajecto-
ries may have higher dimensionality compared to homo-
geneous trajectories. Thus, “relations” between trajectory-
dimensions have become crucial when forecasting. It means
that when predicting the trajectory of the next moment in
one dimension, other dimensions and their relationships
need to be considered simultaneously. These relationships
are quite similar to social interactions, whereas the inter-
action participators have become different dimensions of
the heterogeneous trajectory rather than several neighbor
agents. Since these dimensional interactions happen “verti-
cally” to both the time and space axes, in this manuscript,
we will investigate the possible effects of these interactions
within heterogeneous trajectories by introducing “another
vertical view” in trajectory prediction. Similar to social in-
teraction, we call them the “Dimension-wise Interaction” .
In statistics, an interaction may arise when considering
the relationship among three or more variables and de-
scribes a situation in which the effect of one causal variable
on an outcome depends on the state of a second causal
variable. The social interaction considered in trajectory pre-
diction is a special case of the above interaction. Similarly,
we define dimension-wise interaction as the interactions
among two or more trajectory-dimensions within the same
agent’s trajectory when forecasting their future trajectories.
The dimension-wise interaction also happens among at
least three variables, but these variables all belong to the
same agent’s trajectory (at different time steps). Denote the
observed M-dimensional position (can be coordinates, 2D
or 3D bounding boxes, or even 3D skeletons) of an agent
at some moment taspt= p1
t, ..., pm
t, ..., pM
t⊤∈RM,
the dimension-wise interaction can be represented as the
following term Iwhen forecasting the next time step:
ˆpm
t+1= Net
pm
t, I
p1
t, p2
t, ..., pm
t, ..., pM
t
. (1)
As the dimensionality of the heterogeneous trajectories
rises, these inter-dimensional interactions will become non-
negligibly complex (at least O(M2)) when forecasting. For
example, there will be at least N(M) =M(M−1)/2paris
of interactions even if only considering them as undirected.
In addition, trajectory-dimensions on adjacent moments are
often highly correlated, like ˆpm
t+1andpm
tin Eq. 1. Nev-
ertheless, the time-frequency properties of each trajectory-
dimension still need to be considered when representing
these interactions. Therefore, there are also challenges in
finding an effective modeling way to fit this complex in-
teraction term I. Thus, how to model the above dimension-
wise interaction term I p1
t, p2
t, ..., pm
t, ..., pM
t
in heteroge-
neous trajectories has become our other concern.
In fact, although these interactions may be established
due to the use of two or more fully connected layers
when embedding trajectories, which is the first operationin most current approaches, these established ones are not
representative enough ( discussed in Sec. 4.4 ). It could be
difficult to model these dimension-wise interactions directly
because of either the higher computation complexity or
the frequency response characteristics of each trajectory-
dimension. Modeling these interactions from a different per-
spective relative to the original time sequences has become a
better option. As discussed above, transforms and trajectory
spectrums can help us locate the “buried” information. Like
a single trajectory-dimension, we can also model dimension-
wise interactions via trajectory spectrums to better capture
the time-frequency characteristics within these interactions.
Thus, our focus has turned to how to simulate dimension-
wise interaction Ivia spectrums while simultaneously con-
sidering how it varies over time or frequency.
Talking about interactions, a natural thought is to use
graphs to model the relations between interaction participa-
tors. Among dimension-wise interactions, each participator
is a single trajectory-dimension, which means that we need
to calculate the similarity of these participators as edges of
the graph structure. However, it is challenging to model
such a complex relation between two trajectory-dimensions
or the corresponding spectrums within a static graph since
both of them may change over time or frequency and own
specific time-frequency joint representation, not to mention
how to gather information on all adjacent edges into a node.
Thus, we need to find a powerful approach to simultane-
ously model “time-frequency response” and “dimension-
wise interactions” two factors with trajectory spectrums.
Fortunately, bilinear structure [27], [28] have been pro-
posed to model “two-factor” characteristics jointly like
“style” and “content” in images, and achieved great success
in many computer vision tasks [12], [28], [29], [30]. These
two factors may be connected in some undirect forms, which
could be difficult to represent directly. The bilinear structure
combines these two factors by utilizing outer-product and
pooling operations. During this process, two factors will
be simultaneously considered, and their relations will be
constructed through the outer product matrix when classi-
fying images, in which element-wise similarity computation
will be conducted to measure how each vector component
contributes adaptively. The bilinear form also simplifies
gradient computations to ensure both factors are equally
optimized while speeding up training [28].
Similar to these two image factors “style” and “content”,
our considered frequency response and the dimension-wise
interaction also represent heterogeneous trajectories from
two different perspectives. In detail, the frequency charac-
ters describe how each trajectory-dimension forwards and
changes over time, while the dimension-wise interactions
describe how other trajectory-dimensions affect or limit the
change of a specific trajectory-dimension. The outer-product
operation in the bilinear structure also provides a new
implementation to view and simulate interactions among
these trajectory dimensions. In this process, the dimension-
wise interactions will be established as the projections for
time-frequency representations of different dimensions onto
the others, thus representing interactions while efficiently
integrating time-frequency properties.
Thus, in this manuscript, we will take advantage of both
transforms and bilinear structures to encode and forecastJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 4
heterogeneous trajectories from another view ,i.e., modeling
each trajectory-dimension as well as the dimension-wise
interaction with trajectory spectrums, thus modeling the
interactions among trajectory-dimensions while simultane-
ously considering their time-frequency response characters,
achieving the refined extraction of agents’ behavior repre-
sentations while maintaining inter-dimensional interactions
within heterogeneous trajectories.
Contributions. This manuscript is an extension of our
previous conference paper [31]. The former proposed V2-
Net predicts trajectories in a “vertical” view, i.e., model-
ing and forecasting trajectories via the Fourier spectrums
instead of the original time series hierarchically. In this
work, the proposed E-V2-Net extends the existing V2-
Net from another vertical view by focusing on dimension-wise
interactions in the heterogeneous trajectories. Specifically,
this manuscript enhances V2-Net from the following ways.
On the one hand, we define and verify the existence of
dimension-wise interactions in heterogeneous trajectories,
which extends the structure and network settings of the
original network to be applied to forecast heterogeneous
trajectories. On the other hand, E-V2-Net introduces Wavelet
transform [32] with time-frequency localization properties
as an alternative to Fourier transform to obtain trajectory
spectrums to cope with diverse scenarios (such as rapidly
changing trajectories). Furthermore, we adopt an extra bilin-
ear structure [27] to model and fuse the above “dimension-
wise interactions” and “frequency response” two factors,
thus extracting refined behavior representations while main-
taining interactions within heterogeneous trajectories. Ex-
periments show that the proposed E-V2-Net outperforms
most state-of-the-art methods on the ETH-UCY benchmark,
Stanford Drone Dataset, nuScenes, Human3.6M with het-
erogeneous trajectories, including 2D coordinates, 2D and
3D bounding boxes, and 3D skeletons. In summary, the
contributions of this manuscript are listed as follows:
•We define the “Heterogeneous Trajectories” to extend
the current trajectory prediction task and propose a generic
prediction method E-V2-Net to handle trajectories with dif-
ferent representation forms.
•This work further investigates trajectories with differ-
ent spectrums to model the frequency responses of trajec-
tories at different frequency scales simultaneously to obtain
richer multi-scale characteristics.
•Potential interactions between different elements
within heterogeneous trajectories, which we call the
“Dimension-wise Interactions”, have been focused specifi-
cally, and a bilinear structure is adopted to model and fuse
“frequency response” and “dimension-wise interaction” fac-
tors when forecasting heterogeneous trajectories.
2 R ELATED WORK
Trajectory prediction has received increasing attention re-
cently. Alahi et al. [33] treat this task as sequence generation
and employ LSTMs to model and predict pedestrians’ posi-
tions in the next time step recurrently. Researchers like [14],
[15], [22], [34] have also designed different Transformers to
obtain better trajectory representations. In addition, several
factors, such as social/scene interactions [35], [36], [37],
agents’ motion preferences [1], [26], the goal distributions[38], [39], [40] and stochastic trajectory prediction [41], [42],
[43], [44], have been widely investigated. Notably, trajectory
prediction discussed in this manuscript focuses more on
the scenarios such as walking pedestrians and city streets,
and less on fast-changing scenarios like highway vehicles,
which means the safety concerns in autonomous-driving-
related tasks may be less considered. In contrast, we pay
more attention to the diversity of agents’ activities.
Heterogeneous Trajectory Prediction. According to the
classification methods of previous works like [8], [9], tra-
jectory prediction mainly involves homogeneous and het-
erogeneous trajectory prediction in real scenarios. Homoge-
neous trajectory prediction predicts future trajectories under
the same category ( e.g., only pedestrians). On the contrary,
heterogeneous predicts future trajectories of the Heteroge-
neous Agents under different categories ( e.g., pedestrians,
cars, and bikes). The heterogeneous trajectory prediction
considered in this manuscript further expands this defini-
tion to include trajectories with different forms, which we
call the Heterogeneous Trajectories . Yagi et al. [45] present an
approach to forecast agents’ 2D bounding box positions in
the first-person videos by taking into account agents’ post-
locations, poses, and ego-motions. Quan et al. [46] propose
the Holistic LSTM to predict 2D bounding boxes along with
the help of depth estimation and the optical flow. Saadat-
nejad et al. [47] design an LSTM-based method to predict
3D bounding boxes by encoding positions and velocities
together. Xu et al. [48] also propose a graph network to
forecast human skeletons. Although these methods have
achieved great performance enhancement for trajectories
with different forms, they often require additional model
inputs, like skeletons and optical flow information, making
them difficult to adapt their models to different trajectory
forms and scenarios. Few methods can handle heteroge-
neous trajectories with different forms, let alone interactions
and relations within heterogeneous trajectories at the same
time. Combining these strengths of existing methods, a
natural thought is to design a “generic” prediction network
to cope different forms of heterogeneous trajectories.
Applications of Transforms. As powerful tools, the Fourier
transform (FT) and its variations have been extensively
studied and achieved impressive success in various image
processing and computer vision tasks. Cheng et al. [49]
present a Fast Fourier Transform-based algorithm, which
brings high computational efficiency and reliability for the
multichannel interpolation in image super-resolution. Kaur
et al. [17] propose a Fractional Fourier Transform based
Riesz fractional derivative approach for edge detection and
apply it to enhance images. Komatsu et al. [50] construct
the 3-D mean-separation-type short-time DFT and apply it
to denoise moving images. It is worth noting that FTs have
also been widely applied in handling time-series forecasting
problems. Mao et al. [51], [52] employ DCT to help predict
human skeleton graphs in motion forecasting. Cao et al. [53],
[54] propose a spectral-temporal graph to model interactions
(not trajectories) when forecasting trajectories.
As an improvement to FTs, Wavelet Transforms (WTs)
have better capabilities in terms of multi-resolution and
multi-scale characteristics to achieve the adaptive time-JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 5
frequency signal analysis. Similar to FTs, WTs have also
succeeded in numerous tasks. Zhang et al. [18] present a
Dyadic Wavelet Transform-based edge detection method
by scale multiplication. Tang and You [19] construct a
novel Wavelet function to extract the skeleton of ribbon-
like shapes to detect edges. Cheng et al. [20] provide a
Discrete Wavelet Transform-based approach for detecting,
tracking, and identifying multiple moving objects. WTs are
also well-applied for time-series analysis. For example, Yang
et al. [55] propose a time-series prediction model based on
fuzzy cognitive maps and the Wavelet Transform.
Considering that a trajectory can also be treated as a
time series, the successful use of these powerful transforms
motivates us to model agents’ trajectories with trajectory
spectrums, therefore trying to obtain better representations.
Unfortunately, there seem to be no methods that directly
use either the Fourier transform or its variations to analyze
and forecast agents’ trajectories in the field of trajectory
prediction. This manuscript attempts to model and forecast
agents’ trajectories by adopting different transforms (DFT
and Haar wavelet) in the spectrum view for the first time.
Hierarchical Prediction and Bilinear Structures. More and
more researchers [38], [39], [40] have treated trajectory pre-
diction as a “two-stage” problem and predict trajectories
hierarchically . Mangalam et al. [23] infer trajectory end-points
to assist in long-range multi-modal trajectory prediction.
Tran et al. [25] attempt to obtain multi-modal goal propos-
als from the additional goal channel to generate multiple
predictions. Wong et al. [26] use a set of generators to gener-
ate multiple destination proposals, and then interpolate to
forecast multiple trajectories. Others like [24] also introduce
several “waypoints” to help better predict agents’ potential
future intentions rather than the only destination points.
These methods have made an initial decomposition of
the trajectory and the trajectory prediction task, where the
features to be focused on in each of the two prediction stages
can be distinguished, thus improving the efficiency of the
prediction network while reducing the difficulty of network
training. The transforms presented above could decompose
trajectories according to different frequency components,
similar to the hierarchical prediction methods introduced
here. They may further improve the efficiency of the predic-
tion network if the hierarchical prediction is implemented in
the frequency domain. Specifically, considering the Fourier
transform and its variations could decompose time series
into different frequency portions, a natural thought is to
predict agents’ future trajectories hierarchically on different
frequency scales rather than the “end-point” or “way-points”.
Along with the hierarchical predictions, the heteroge-
neous trajectories mentioned above need to be considered
simultaneously. In order to combine two different factors,
i.e., the frequency response and the dimension-wise interac-
tion within heterogeneous trajectories, a natural idea is to
use a bilinear structure to fuse these two factors. Bilinear
structures [27] are used to model two-factor variations such
as “style” and “content” in images, and they have been
widely applied in various tasks. Lin et al. [28] utilize the
bilinear structure in fine-grained visual recognition to obtain
discriminative features and have reached better classifica-
tion accuracy. Xu et al. [29] design a second-order bilinear
DFT
TrajectoryKeypointsEstimation
SpectrumInterpolationKeypointsTransformerT𝜔𝜔𝐴ΦSpectrumsInteraction RepresentationInterpolationTransformerTIDFTRGBImageNeighbor Trajectories
!
!"#MLPMLP𝜔𝜔$𝐴!"#%Φ!"#KeypointSpectrums𝜔𝜔$𝐴%ΦCompleteSpectrumsAverage KeypointsLoss!𝑦$"%∈ℝ&!"#×(
!𝑦∈ℝ)$×(TrainingOnly
Average Point-Wise LossIDFT𝑧
+Fig. 3. V2-Net Overview. It has keypoints estimation and spectrum
interpolation two sub-networks. It forecasts 2D coordinate trajectories
“from-coarse-to-fine” hierarchically via Fourier spectrums.
pooling module to effectively aggregate targets’ deep and
shallow features for better tracking. Guo et al. [30] propose
bilinear graph networks to model the context of joint em-
beddings of words and objects in visual question answering
to model relationships between words.
However, the combination of frequency responses in
trajectories and the potential dimension-wise interactions
through bilinear structures has rarely appeared in the field
of trajectory prediction, which is the main focus of this work.
3 M ETHOD
3.1 V2-Net ( MDimensions and Transform T)
As shown in Fig. 3, V2-Net has two main sub-networks, key-
points estimation sub-network and spectrum interpolation
sub-network. In our conference paper [31], V2-Net is used
to forecast 2D coordinate trajectories via Fourier spectrums.
Here, we expand dimension of forecasted trajectories into
Mand make it compatible with a generic transform T.
Problem Formulation. Letpi
t= (r1i
t, r2i
t, ..., r Mi
t)⊤∈
RMdenote the position of i-th agent at step t. We use
the time sequences of the above M-dimensional vectors to
represent one form of heterogeneous trajectories. Formally,
we denote agents’ N-point M-dimensional trajectory as
Xi(1≤t≤N, M ) = pi
1,pi
2, ...,pi
N⊤
= Xi
1,Xi
2, ...,Xi
M∈RN×M.(2)
Thus, the observed trajectory of agent iduring thobserva-
tion moments is denoted as Xi=Xi(1≤t≤th, M)∈
Rth×M. Given a video clip {I}that contains Naagents’
observed trajectories {Xi}Na
i=1, trajectory prediction focused
in this manuscript aims to forecast their possible future posi-
tions{ˆYi}Na
i=1during the future period (th+1≤t≤th+tf).
Here, ˆYi={ˆyk}K
k=1are multiple possible predictions for
agent i, where ˆyk=ˆXi(th+ 1≤t≤th+tf, M)∈Rtf×M
denotes one of the expected model prediction. Formally, we
aim at building and optimizing a network Net (·), such that
ˆYi=Net Xi,{Xj}j̸=i,{I}
. For a clearer representation,
agent superscripts ( i) will be omitted if there are no conflicts.
Keypoints Estimation Sub-network. The coarse-level
keypoints estimation sub-network aims to forecast agents’
keypoint spectrums with a lower spatio-temporal resolu-
tion. Similar to the original V2-Net, we first apply trans-
formToneach dimension of trajectory X(we call the
“trajectory-dimension”) to obtain the corresponding N-
pointM-dimensional spectrum S ∈RN×M,i.e.,
S=T(X) = (T(X1),T(X2), ...,T(XM)). (3)JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 6
Here,NandMare the number of points and dimensions
of the transformed trajectory spectrum S, which can be
represented by mapping functions f(T,n)andf(T,m):
N=f(T,n)(N, M ),M=f(T,m)(N, M ). (4)
In our experiments, we use two different transforms,
including Haar wavelet and DFT, to further verify the
impact of different transforms on the trajectory prediction
performance. The above mapping functions are determined
by different transforms themselves. Note that we regard
all values in this spectrum matrix as real numbers for the
convenience of the calculation in neural networks. If the
transform yields a set of complex values, we will split them
into two sets of real numbers. Take DFT as an example.
We will split the spectrum into two parts, like amplitudes
and phases or real and imaginary. Thus, we have N=N
andM= 2Mfor Fourier specturms. In addition, we have
N=N/2andM= 2Mfor Haar transform. Sepcifically,
we denote Nh=f(T,n)(th, M), andNf=f(T,n)(tf, M).
Then, we employ an embedding MLP (the MLP tin
Tab. 1) to embed agents’ observed trajectory spectrums into
the high-dimensional ft. We sample and embed the noise
vector z∼ N (0,I)and concatenate the corresponding
random representation fito the ftto generate stochastic
predictions. The encoder for these noise vectors (MLP i) has
the same structure as the MLP t, but their weights are not
shared. We combine the above representations to obtain the
embedded spectrum representation fe∈RNh×128. Formally,
fe= [ft,fi] = [ MLP t(S),MLP i(z)]. (5)
Then, we use a Transformer [56] T kto encode agents’
behaviors. The embedded vectors fewill be fed into the
Transformer Encoder to calculate self-attention features.
Transformer T kis used as a feature extractor and does not
contain the output layer. Spectrums Swill query these at-
tention features in the Transformer Decoder to compute the
final behavior representations. We employ another encoder
MLP (MLP e) to aggregate features at different frequency
nodes [26], thus inferring the behavior feature f. Formally,
f=MLP e(Tk(fe,S))∈RNh×128. (6)
TABLE 1
Architecture details of the enhanced V2-Net and E-V2-Net.
Layer Architecture Out Shape
Tran. X→ T (X) =S (Nh,M)
MLPt S → fc(64,ReLU )→fc(64,tanh)→ft (Nh,64)
MLPi z→fc(64,ReLU )→fc(64,tanh)→fi (Nh,64)
Tk[ft,fi]→TEncoder ( 128)→f′
k;
f′
k,S → TDecoder ( 128)→f′′
k(Nh,128)
MLPe f′′
k→fc(128,tanh )→fc(128)→f (Nh,128)
MLPdf→fc(128,ReLU )→fc(128,ReLU )
→fc(MNkey)
→Reshape (Nkey, M)→ˆSkey(Nkey,M)
MLPcC→MaxPool (5×5)→Flatten
→fc(64Nkey,tanh)
→Reshape( Nkey,64)→fc(Nkey,64)
Interp. S,ˆSkey→ˆSkey
l(Np,M)
Ti[fkey
t,fc]→TEncoder ( 128)→f′
i;
f′
i,ˆSkey
l→TDecoder ( M)→ˆS(Np,M)
Inv-Tran. ˆS → T−1(ˆS)[th:]→ˆY (tf, M)
MLPbi
(E-V2-Net)fα⊗fα→MaxPool (2×2)→Flatten
→fc(64,tanh)→fR(Nh,64)Given the number of keypoints Nkey, it finally utilizes
a decoder MLP (MLP d) to predict the Nkey-point keypoint
spectrums ˆSkey∈RNkey×M(Nkey=f(T,n)(Nkey, M)):
ˆSkey=MLP d(f), (7)
Note that we still call Nkey the number of spectrum
keypoints to avoid confusion. Then, the Inverse Transform
T−1will be applied to obtain the corresponding spatial
keypoints ˆykey∈RNkey×M, where
ˆykey=IDFT
ˆSkey
. (8)
When training the sub-network, the groundtruth key
spatial observations ykeywill be used as the supervision
to make it learn to predict the keypoint spectrums ˆSkey.
It is obtained by sampling the groundtruth yonNkeykey
temporal moments. Formally,
ykey=
ptkey
1,ptkey
2, ...,ptkey
Nkey⊤
∈RNkey×M. (9)
In our experiments, these key temporal moments will be
sampled at a uniform interval from the prediction period.
The network variables will be tuned by minimizing the
average Euclidean distance between ykeyand the predicted
ˆykey,i.e., the Average Keypoints Loss (LAKL):
LAKL=∥ˆykey−ykey∥2. (10)
Spectrum Interpolation Sub-network. The fine-level
spectrum interpolation sub-network reconstructs the com-
plete trajectory spectrums from the formerly predicted key-
point spectrums with a higher spatio-temporal resolution. It
takes the predicted Nkey-point keypoint spectrums ˆSkeyas
one of the primary inputs, thus learning possible interactive
behaviors from spectrum biases. Similar to Eq. 5, we have:
fkey
t=MLP t
ˆSkey
∈RNkey×64. (11)
Note that MLP tin Eq. 5 and Eq. 11 do not share weights.
Then, we use a context MLP (MLP c) to encode the interac-
tion representation C[57] (which encodes social interactions
and scene constraints together in an energy map way) into
the context feature fc=MLP c(C).
Then, this sub-network uses a similar Transformer
(called the Interpolation Transformer, T i) to forecast the
complete spectrums ˆS ∈RNp×M, which is supposed to
be the trajectory spectrums of the entire observed trajecto-
ries and forecasted trajectories on th+tftime steps, i.e.,
Np=f(T,n)(th+tf, M). In particular, to adapt this sub-
network to different keypoint locations, we linearly interpo-
late keypoint spectrums to let them have the same number
of frequency portions as the final predicted ones. This linear
interpolation layer is denoted as “Interp” in Tab. 1. We use
the interpolated spectrum ˆSkey
las the actual transformer
input. The fkey
e= [fkey
t,fc]will be fed into the Transformer
Encoder to calculate self-attention. After that, spectrums
ˆSkey
lwill query self-attention features in the Transformer
Decoder to infer the predicted spectrums. Formally,
ˆS=Ti
fkey
e,ˆSkey
l
∈RNp×M. (12)
Then, the inverse transform T−1will be applied to obtain
the final reconstructed trajectory ˆyofor all 1≤t≤th+tfJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 7
𝑥!,𝑦!𝑥",𝑦"T𝑿MaxPoolFlattenMLP!"
𝑀-dimensionalTrajectory(shown with𝑀=42D bounding boxes)𝓢#𝓢$𝓢%𝓢&KeypointsEstimation Sub-Network𝑧
+𝓢(𝒩#Sliced Vectors)Outer-ProductMatrices𝒩#×64×64
𝑿#𝑿$𝑿&𝑿%Spectrums 𝓢(After 𝑀1D Transforms)BilinearStructureSlice and Stack
𝒩!×64𝒇$∈ℝStack
MLP%𝒇%"𝒇%"BilinearFeature
Fig. 4. Bilinear Structure in E-V2-Net. It takes spectrums S ∈RNh×Mas the input, and finally outputs the refined spectrum features fR∈RNh×64.
time steps. By slicing this whole reconstructed trajectories,
we have the M-dimensional predictions ˆy∈Rtf×M:
ˆyo=T−1
ˆS
,ˆy=ˆyo[th:], (13)
where the [th:]indicates the slicing operation on tensors.
This sub-network learns to interpolate the “key” tra-
jectory spectrums into the complete trajectory spectrums,
thus reflecting agents’ fine interactive details by predicting
the remaining spectrum portions. Similar to the keypoints
estimation sub-network, its trainable variables will be tuned
through the Average Point-wise Loss (LAPL):
LAPL=∥ˆy−y∥2. (14)
3.2 E-V2-Net
V2-Net can not consider the interactions within heteroge-
neous trajectories, which we call “dimension-wise interac-
tions”. The E-V2-Net is proposed to address this limitation.
Dimension-wise Interaction and Its Graph View.
Heterogeneous trajectories are made up of multiple M-
dimensional trajectories. Each M-dimensional trajectory
consists of Mtrajectory-dimensions. However, most current
approaches have not explicitly focused on the relationships
between different trajectory dimensions. The dimension-
wise interactions are actually interactions between differ-
ent trajectory-dimensions, and each interaction participa-
tor is a single trajectory-dimension. Given an undirected
graph G(t) = ( V(t),E(t))with time variable t, where
V(t) ={gm(t)}M
m=1is the set of vertices that contains all M-
dimensional position information of an agent at time t, and
E(t)is the set of edges, which represents the connections be-
tween these vertices at time t, to establish these connections,
we actually need to learn the adjacency matrix
A(t) =
W1,1(t)···W1,M(t)
.........
WM,1(t)···WM,M(t)
, (15)
so that the effect of dimension-wise interactions onto the
future trajectories can be gathered into the vertex of each
single trajectory-dimension into the interactive g′
m(t),i.e.,
g′
m(t) =σ MX
n=1Wm,n(t)e(gm(t),gn(t))!
. (16)
Here, e(·)represents the calculation on edges, and σ(·)
denotes the non-linear gather operation. Thus, we need to
determine each gm,e,σ, and A(t)simultaneously to model
dimension-wise interactions, which is obversely challenging
since each of them may vary over time t.Vanilla Bilinear Structures. A natural thought is to
model these dimension-wise interactions in the frequency
domain so that the time-frequency properties of each
trajectory-dimension could be better represented. We intro-
duce the bilinear structure to model dimension-wise inter-
actions as well as fuse “dimension-wise interactions” and
“time-frequency response” factors simultaneously. Follow-
ing [28], a bilinear structure consists of a quadruple
B= (fA, fB,P,E). (17)
Here, fAandfBare feature functions, Pis the max pooling
function, and Erepresents an encoding function.
In the two-factor tasks, denote the set of features of one
factor as S1, and the other as S2. A feature function is a
mapping f:S1× S2→RK×D. It takes the two-factor rep-
resentations fs1∈ S 1andfs2∈ S 2as the input and finally
outputs the corresponding bilinear features fb∈RK×D.
Features for these two factors are combined at each location
using the matrix outer product. Formally,
fb=bilinear (fs1,fs2, fA, fB) =fA(fs1,fs2)⊗fB(fs1,fs2).
(18)
The final encoded bilinear feature fRis obtained by applying
the max pooling and encoding operations, i.e.,
fR=E(P(fb)). (19)
Bilinear Structure in E-V2-Net. Now we describe how to
apply the above vanilla bilinear structure in E-V2-Net. Fig. 4
shows its overall structure. In E-V2-Net, both S1andS2are
the spectral representation of the M-dimensional trajectory
X,i.e., we have fs1=fs2=S, and fA=fB=MLP t. Thus,
according to Eq. 18, we have the Outer-Product Matrix
R=fb=fα⊗fα∈RNh×64×64, (20)
where fα= (fα,1,fα,2, ...,fα,Nh)⊤=MLP t(S), (21)
andR[i] =fα,ifα,i⊤∈R64×64,1≤i≤ N h. (22)
Here,{fα,i}Nh
i=1are obtained by slicing the spectrum Sverti-
cally to the frequency axis and feeding them to MLP t. Dur-
ing this process, the element-wise outer product fα,ifα,i⊤
on the batch input {fα,i}could also measure similarities
of different dimensions, which plays similar roles to our
expected Aandesimultaneously in Eq. 16.
Then, we only need to construct the σfunction to model
relations between factors “time-frequency response” and
“dimension-wise interactions”. According to Eq. 19, the max
pooling and a new encoder MLP (named MLP bi, structures
in Tab. 1) are applied on the original bilinear feature fb
(also named the outer-product matrix R) to encode the final
interactive bilinear features fR. Formally,
fR=MLP bi(Flatten (MaxPool (R))). (23)JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 8
It can be treated as a “frequency-bilinear” version of Eq. 16,
which enables the network to express the time-frequency
characteristics of each trajectory-dimension better while
avoiding the complex computations in graph-like structures.
The E-V2-Net takes bilinear features fRto replace the
original ftin the keypoints estimation sub-network (Eq. 5)
to achieve the goal of modeling dimension-wise interac-
tions and time-frequency response simultaneously. Other
network computations are the same as the original V2-Net.
4 E XPERIMENTS
4.1 Settings
We evaluate models on multiple datasets: (a) ETH-UCY
(2D Coordinate). ETH [4]-UCY [58] contains several bird-
eye videos captured in pedestrian walking scenes, including
eth, hotel, univ, zara1, and zara2. The annotations are 2D
coordinates in meters. Researchers [33], [41] mostly train
and evaluate their models with the “leave-one-out” strategy
[33] on this dataset.1(b)Stanford Drone Dataset (2D Coor-
dinate). The Stanford Drone Dataset [60] (SDD) has 60 bird-
view videos captured by drones. More than 11,000 agents
with different categories are annotated with 2D bounding
boxes in pixels. Notably, most existing methods only use
the 2-dimensional center points to train and validate their
trajectory prediction models rather than the 4-dimensional
bounding box points. We train our model with 60% data (36
clips), validate and test with 20% (12 clips) respectively [61]
[62].2(c)Stanford Drone Dataset (2D Bounding Box). The
original SDD is labeled with bounding box positions. We
treat SDD labeled with 4-dimensional 2D bounding boxes as
a new dataset to validate the performance of 2D bounding
box prediction. Splits and settings are the same as SDD. (d)
nuScenes (3D Bounding Box). The nuScenes Dataset [11]
is a large-scale real-world dataset of 1000 driving scenes
collected in the urban cities of Boston and Singapore. This
dataset is used to validate the performance of 3D bounding
box prediction, different from previous approaches that
predict 2D coordinates like [15]. We only use two 3D corner
points when training and validating, i.e., forecasting a 6-
dimensional trajectory. Following [47], we use 550 scenes
to train, 150 to validate, and the other 150 to test. (e) Hu-
man3.6M (3D Skeleton). The Human3.6M [63], [64] dataset
is a large-scale dataset with 3.6 million 3D human poses,
performed by 11 professional actors in 15 scenarios (such
as discussion, smoking, taking photos, and talking on the
phone). Each person is annotated with 51 points (17 joints
in 3D). We use subjects {1,6,7,8,9}to train, subjects {11}
to validate, and subjects {5}to test [48].
Metrics. (a)ADE & FDE [4], [33]. ADE is the average
point-wise Euclidean distance between each groundtruth
and predicted point, and FDE is the Euclidean distance on
the last prediction step. For K M -dimensional predictions
1. Dataset files used to train and validate on ETH-UCY are the same
as [59]. In particular, the sampling interval on the eth subset is 6 frames
to match the actual “3.2s-to-4.8s” setting.
2. Dataset splits used to train and validate on SDD are the same as
[61], which includes all categories of agent annotations.{ˆyk}K
k=1to the same agent, where ˆykconsists of m2D or
3D points ˆpi
t, we have
ADE(y,{ˆyk}K
k=1) = min
k1
mtfth+tfX
t=th+1mX
i=1∥pi
t−ˆpki
t∥2,(24)
FDE(y,{ˆyk}K
k=1) = min
k1
mmX
i=1∥pi
th+tf−ˆpki
th+tf∥2.(25)
(b)AIoU & FIoU [47]. To better estimate 2D and 3D bound-
ing box prediction performance, we compare the volume
of the predicted bounding box and the ground truth, then
compute the Intersection volume Over Union between them
similar to ADE and FDE as AIoU and FIoU. (c) MPJPE. For
3D skeleton cases, we use FDE in Eq. 25 as the metric to
measure model performance ( m= 17 ). Note that in the field
of motion prediction, this metric is also more commonly
known as the Mean Per Joint Position Error (MPJPE).
Prediction types. Our focused trajectory types include
2D coordinates (“co”, M= 2 ), 2D bounding box (“bb”,
M= 4), 3D bounding box (“3Dbb”, M= 6), and 3D human
skeleton (“ske”, M= 51 ). We also provide “ SEP” variations
for ablation studies, which separately predict each element
(i.e., a single point in 2D coordinates, a single 2D point in
2D bboxes, a single 3D point in 3D bboxes or skeletons) and
then concatenating them to obtain final predictions.
Implementation details. For different prediction types,
E-V2-Nets’ structures are the same except for the number
of input dimensions M. We first introduce the common
settings, then specify different datasets’ prediction settings.
Notably, E-V2-Net and V2-Net will be short as “ EV” and
“V” to suit space limitations in some paragraphs.
(a)Common Settings. Following [59], each trajectory is
pre-processed by moving to the origin before the model
implementation. We train V2-Net and E-V2-Net with Adam
optimizer (learning rate = 0.0003 ) on one NVIDIA GeForce
GTX 1080Ti card. Models are trained with the batch size
bs= 2500 for 800 epochs on ETH-UCY and 150 epochs on
SDD, SDD (2D bounding box), and nuScenes. We employ
L= 4 layers of encoder-decoder structure with H= 8 at-
tention heads in each Transformer sub-network. The output
unit in attention layers is set to 128. Detailed Transformer
structures are listed in the supplemental material.
(b)Dataset-Wise Settings. (i) For ETH-UCY and SDD ,
we predict with M= 2 and type “co”. For SDD (2D
bounding box), we set M= 4 and predict with type
“bb”. On these datasets, we predict agents’ trajectories in
future tf= 12 frames according to their th= 8 frames’
observations. The frame rate is set to 2.5 fps when sampling
trajectories. In addition, we set the number of spectrum
keypoints Nkey= 3, and{tkey
1, tkey
2, tkey
3}={th+ 4, th+
8, th+ 12}for DFT variations. Similarly, we set Nkey= 4
and{tkey
1, tkey
2, tkey
3tkey
4}={th+ 3, th+ 6, th+ 9, th+ 12}
for Haar variations. (ii) For nuScenes (3D bounding box) ,
we set M= 6 and predict with type “3Dbb”. We set
{th, tf}={4,4}on this dataset when forecasting trajecto-
ries annotated with 3D bounding boxes with the frame rate
of 2fps. We set Nkey= 2 and{tkey
1, tkey
2}={th+ 2, th+ 4}
for all variations. Note that we do not use agents’ inter-
actions representations Cin this dataset due to the scale
differences. (iii) For Human3.6M (3D Skeleton) , we setJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 9
TABLE 2
Comparisons with the best-of-20 on ETH-UCY ( 2D coordinates ).
Metrics are shown in the format of “ADE/FDE” in meters.
Models eth↓ hotel↓ univ↓ zara1↓ zara2↓ Avg.↓
S-BiGAT [43] 0.69/1.29 0.49/1.01 0.55/1.32 0.30/0.62 0.36/0.75 0.48/1.00
TF [34] 0.61/1.12 0.18/0.30 0.35/0.65 0.22/0.38 0.17/0.32 0.31/0.55
PECNet [23] 0.54/0.87 0.18/0.24 0.35/0.60 0.22/0.39 0.17/0.30 0.29/0.48
STAR [14] 0.56/1.11 0.26/0.50 0.52/1.13 0.40/0.89 0.52/1.13 0.41/0.87
Trajectron++ [44] 0.43/0.86 0.12/0.19 0.22/0.43 0.17/0.32 0.12/0.25 0.20/0.39
TPNMS [65] 0.52/0.89 0.22/0.39 0.55/1.13 0.35/0.70 0.27/0.56 0.38/0.73
Introvert [66] 0.42/0.70 0.11/0.17 0.20/0.32 0.16/0.27 0.16/0.25 0.21/0.34
LB-EBM [67] 0.30/0.52 0.13/0.20 0.27/0.52 0.20/0.37 0.15/0.29 0.21/0.38
Agentformer [15] 0.26/0.39 0.11/0.14 0.26/0.46 0.15/0.23 0.14/0.23 0.18/0.29
Y-net (TTST) [24] 0.28/0.33 0.10/0.14 0.24/0.41 0.17/0.27 0.13/0.22 0.18/0.27
STC-Net [68] 0.64/1.18 0.33/0.54 0.39/0.74 0.29/0.49 0.26/0.45 0.38/0.68
E-SR-LSTM [59] 0.44/0.79 0.19/0.31 0.50/1.05 0.32/0.64 0.27/0.54 0.34/0.67
CSCNet [57] 0.51/1.05 0.22/0.42 0.36/0.81 0.31/0.68 0.47/1.02 0.37/0.79
MID [69] 0.39/0.66 0.13/0.22 0.22/0.45 0.17/0.30 0.13/0.27 0.21/0.38
SHENet [70] 0.41/0.61 0.13/0.20 0.25/0.43 0.21/0.32 0.15/0.26 0.23/0.36
V2-Net [31] 0.23/0.37 0.10/0.16 0.24/0.43 0.19/0.30 0.14/0.24 0.18/0.30
Social-SSL [37] 0.69/1.37 0.24/0.44 0.51/0.93 0.42/0.84 0.34/0.67 0.44/0.85
SEEM [71] 0.62/1.20 0.61/1.21 0.50/1.04 0.31/0.61 0.36/0.68 0.48/0.95
EqMotion [48] 0.40/0.61 0.12/0.18 0.23/0.43 0.18/0.32 0.13/0.23 0.21/0.35
E-V2-Net-DFT 0.25/0.38 0.11/0.16 0.23/0.42 0.19/0.30 0.13/0.24 0.18/0.30
E-V2-Net-Haar 0.25/0.41 0.10/0.15 0.23/0.43 0.18/0.31 0.13/0.24 0.18/0.30
TABLE 3
Comparisons with the best-of-20 on SDD ( 2D coordinates ).
Models ADE/FDE ↓ Models ADE/FDE ↓
SoPhie [42] 16.27/29.38 Multiverse [62] 14.78/27.09
MANTRA [72] 8.96/17.76 SimAug [61] 12.03/23.98
PECNet [23] 9.96/15.88 LB-EBM [67] 8.87/15.61
SpecTGNN [54] 8.21/12.41 Y-net (TTST) [24] 7.85/11.85
CSCNet [57] 14.63/26.91 MID [69] 7.91/14.50
SHENet [70] 9.01/13.24 V2-Net [31] 7.12/11.39
EV-DFT (Ours) 6.57/10.49 EV-Haar (Ours) 6.83/11.01
M= 51 with type “ske”. We sample observations with
the frequency of 25Hz ( i.e., the sample interval is 40ms)
and use th= 10 frames (400ms) of observations to predict
tf= 10 future frames (400ms). We set Nkey= 4 , and
{tkey
1, tkey
2, tkey
3, tkey
4}={th+ 1, th+ 4, th+ 7, th+ 10}. We
extend the feature dimension from 128 to 512 for each layer
in the network to expand the model capacity. Models are
set to predict one deterministic trajectory for each agent.
All social interaction and scene interaction modules are also
disabled since there is only one subject in each scene.
4.2 Comparisons to State-of-the-Art Methods
(a)ETH-UCY (2D Coordinate). As shown in Tab. 2, E-
V2-Nets perform at the same level as the state-of-the-art
works like Agentformer and Y-net. Compared to the newly-
published SHENet, E-V2-Net-DFT has improved 26.1% and
22.2% in ADE and FDE. A recent work EqMotion could
predict multiple tasks, including 2D trajectories and human
skeletons. The proposed models outperform EqMotion by
19.0% and 20.0% in ADE and FDE, respectively. In short,
the performances of V2-Net and E-V2-Nets are strongly
comparable to these state-of-the-art methods on ETH-UCY.
(b)SDD (2D Coordinate). As listed in Tab. 3, both V2-
Net and E-V2-Nets outperform most state-of-the-art meth-
ods. Compared with the current state-of-the-art method Y-
net, E-V2-Net-DFT achieves about 16.3% and 11.5% im-
provement in ADE and FDE, respectively. Compared to the
recent SHENet, E-V2-Net-DFT shows a significant improve-
ment of 27.1% in ADE and 20.8% in FDE. Although notTABLE 4
Comparisons ( best-of-20 ) of2D bounding boxes on SDD.
Models ADE↓ FDE↓ AIoU↑ FIoU↑
Linear 27.18 52.40 0.562 0.401
MSN (SEP) [26] 7.87 12.98 0.691 0.543
V2-Net (SEP) [31] 7.10 10.92 0.698 0.578
V2-Net 6.78 10.73 0.717 0.601
E-V2-Net-DFT 6.62 10.57 0.725 0.604
E-V2-Net-Haar 6.44 10.28 0.730 0.613
TABLE 5
Comparisons (forecasting k= 1deterministic trajectory for each agent
with 2 seconds’ prediction horizon) of 3D bounding boxes on nuScenes.
Models ADE↓ FDE↓ AIoU↑ FIoU↑
Zero-Vel [73] 4.364 6.952 0.084 0.065
P-LSTM [47] 1.094 1.985 0.251 0.110
PV-LSTM [47] 0.994 1.811 0.305 0.158
E-V2-Net-DFT ( k= 1) 0.529 0.919 0.602 0.445
E-V2-Net-Haar ( k= 1) 0.525 0.918 0.608 0.450
performing as well as E-V2-Net-DFT, E-V2-Net-Haar also
shows strong competitiveness. Compared to Y-net, it still
has a 13.0% better ADE and a 7.0% better FDE.
(c)SDD (2D Bounding Box). Since few researchers have
tested the 2D bounding box prediction performance on
SDD, we use the “SEP” variations to verify their effective-
ness in forecasting 2D bounding boxes. As shown in Tab. 4,
both V2-Net and E-V2-Net show better prediction perfor-
mance on 2D bounding boxes. Compared to MSN (SEP),
V2-Net (SEP) obtains a remarkable improvement of 9.8%
in ADE, 15.9% in FDE, and a slight improvement of 0.07
in AIoU and 0.035 in FIoU, which demonstrates its effec-
tiveness. Moreover, E-V2-Net-Haar performs better than V2-
Net with an improvement of 5.0% in ADE and 4.2% in FDE,
which shows unique advantages with different transforms.
(d)nuScenes (3D Bounding Box). As shown in Tab. 5,
compared to PV-LSTM, the deterministic E-V2-Net-DFT
(k= 1) has achieved significant improvement of 46.8% in
ADE, 49.3% in FDE, 0.297 in AIoU, and 0.287 in FIoU. In ad-
dition, the deterministic Haar variation slightly outperforms
the DFT variation with about 0.8% ADE, 1.0% AIoU, and
1.1% FIoU. These comparisons also indicate the superiority
of E-V2-Net in forecasting 3D bounding boxes.
(e)Human3.6M (3D Skeleton). We use this more chal-
lenging dataset to further evaluate the proposed models’
modeling capacity. As shown in Tab. 6, E-V2-Net-Haar has
achieved considerable performance compared to current
motion prediction methods, even though it does not con-
sider any prior about human structures or postures. Espe-
cially for the action “Sitting Down”, E-V2-Net-Haar outper-
forms EqMotion for up to 12.22% MPJPE at 400ms. In other
actions, the proposed V2-Net and E-V2-Net also perform
well, meeting or even outperforming several concurrent
state-of-the-art works. Although the proposed method does
not achieve the best performance on the average of this
dataset, which is roughly 5.09% worse than EqMotion, these
results have illustrated their effectiveness for processing
high dimensional trajectories such as human skeletons.JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 10
TABLE 6
Comparisons of 3D skeleton prediction ( th=tf= 10 ) on several Human3.6M subsets and the average metrics over all 15 subsets. Metrics are
shown in the format of MPJPE (FDE) at 80/160/320/400 ms in millimeters. All models only forecast one trajectory for each agent.
Models Sitting Down ↓ Phone Call ↓ Directions ↓ Smoking ↓ Discussion ↓ Average (15 actions) ↓
Traj-GCN [52] 16.1/31.1/61.5/75.5 10.2/21.0/42.5/52.3 9.0/19.9/43.4/53.7 7.9/16.2/31.9/38.9 12.5/27.4/58.5/71.7 26.1/52.3/63.5/63.5
DMGNN [74] 15.0/32.9/77.1/93.0 12.5/25.8/48.1/58.3 13.1/24.6/64.7/81.9 9.0/17.6/32.1/40.3 17.3/34.8/61.0/69.8 33.6/65.9/79.7/79.7
MSR-GCN [75] 16.1/31.6/62.5/76.8 10.1/20.7/41.5/51.3 8.6/19.7/43.3/53.8 8.0/16.3/31.3/38.2 12.0/26.8/57.1/69.7 12.1/25.6/51.6/62.9
PGBIG [76] 13.9/27.9/57.4/71.5 8.3/18.3/38.7/48.4 7.2/17.6/40.9/51.5 6.6/14.1/28.2/34.7 10.0/23.8/53.6/66.7 10.3/22.7/47.4/58.5
SPGSN [77] 14.2/27.7/56.8/70.7 8.7/18.3/38.7/48.5 7.4/17.2/39.8/50.3 6.7/13.8/28.0/34.6 10.4/23.8/53.6/67.1 10.4/22.3/47.1/58.3
EqMotion [48] 13.0/26.5/56.2/70.7 7.4/16.7/36.9/47.0 6.3/15.8/38.9/50.1 5.5/11.3/23.0/29.3 8.2/18.9/42.1/53.9 9.1/20.1/43.7/55.0
V2-Net-Haar 9.5/20.9/47.7/62.1 7.0/15.5/35.3/46.1 6.9/16.2/39.7/52.9 5.6/12.3/28.1/36.9 9.2/21.3/50.0/65.2 8.7/19.7/45.5/59.5
E-V2-Net-Haar 9.8/21.3/47.8/61.9 7.2/15.5/34.3/44.5 7.0/16.1/38.4/50.7 5.9/12.4/27.4/35.9 9.4/21.2/49.0/63.6 9.0/19.6/44.4/57.8
Fig. 5. The average energy percentages of the observed trajectories on
different moments and different frequency components.
TABLE 7
Ablation Studies (2D coordinate prediction) with best-of-20 on
ETH-UCY . Models with “(D)” only predict one deterministic trajectory.
“T” indicates the used transforms. “S2” indicates whether the variation
uses the second stage spectrum interpolation sub-network.
No. Model (T,Nkey ) S2 eth↓ hotel↓ univ1↓ zara1↓ zara2↓
CO1 V (D) (None, 0) - 0.65/1.30 0.25/0.44 0.77/1.39 0.48/0.97 0.40/0.77
CO2 V (D) (DFT, 0) - 0.62/1.15 0.22/0.37 0.55/1.10 0.45/0.92 0.36/0.74
CO3 V (D) (Haar, 0) - 0.63/1.15 0.23/0.42 0.59/1.17 0.43/0.90 0.33/0.70
CO4 V (None, 1) ✓ 0.29/0.47 0.12/0.18 0.23/0.38 0.25/0.37 0.17/0.29
CO5 V (DFT, 1) ✓ 0.23/0.39 0.12/0.17 0.23/0.37 0.22/0.32 0.16/0.26
CO6 V (DFT, 4) ✓ 0.24/0.38 0.12/0.17 0.22/0.36 0.21/0.32 0.16/0.26
CO7 V (DFT, 6) ✓ 0.29/0.49 0.13/0.17 0.23/0.38 0.26/0.37 0.21/0.31
CO8 V (SEP) (DFT, 3) ✓ 0.26/0.40 0.11/0.17 0.22/0.41 0.21/0.39 0.15/0.27
CO9 V (DFT, 3) ✓ 0.23/0.37 0.11/0.16 0.21/0.35 0.19/0.30 0.14/0.24
CO10 V (DFT, 3) × 0.24/0.37 0.12/0.16 0.23/0.35 0.21/0.30 0.15/0.25
CO11 EV (DFT, 3) ✓ 0.25/0.38 0.10/0.16 0.20/0.34 0.19/0.30 0.13/0.24
CO12 EV (DFT, 3) × 0.27/0.38 0.11/0.16 0.21/0.34 0.19/0.30 0.14/0.24
CO13 EV (Haar, 4) ✓ 0.25/0.41 0.10/0.15 0.20/0.35 0.18/0.31 0.13/0.24
CO14 EV (Haar, 4) × 0.27/0.41 0.10/0.16 0.21/0.35 0.19/0.32 0.14/0.24
4.3 Quantitative Analysis and Ablation Studies
Why Transform Trajectories? A central motivation of this
manuscript is to transform trajectories so that the trajectory
prediction task can be implemented in the corresponding
transformed domain. In this part, we discuss the quantita-
tive impacts of transforms when forecasting.
(a)Energy of Spectrums. We first discuss transforms
(DFT and Discrete Haar transform) from the energy per-
spective. In mathematical analysis, Parseval’s identity is a
fundamental result of the summability of the Fourier series
of a function. It could also apply to Haar transform. Simply,
in one-dimension, it says that the integral of the square ofthe Fourier transform of a function equals the integral of the
square of the function itself, i.e.,
Z∞
−∞|X(ξ)|2dξ=Z∞
−∞|x(t)|2dt, (26)
where X(ξ)is the Fourier transform of x(t). It states that the
energy (square sum) remains constant before and after the
transform, which means that the transform does not change
the amount of information but redistributes them at different
time-frequency moments. Thus, information “buried” in the
time domain may be easily noticed in the frequency domain.
We calculate and report the energy of trajectories and
their spectrums on different scenarios in Fig. 5. Note that we
shift the start points of trajectories to the origin and report
normalized energy for simple comparisons. The changes
in energy over time are relatively small. Take Fig. 5 (a)
as an example, the average energy on each time step
is(0,1%,3%,7%,12%,18%,26%,35%) , among which the
maximum gain between adjacent steps is about 9%. Differ-
ently, the energy has been significantly redistributed after
DFT. The energy on the fundamental frequency ( k= 0 )
reaches a staggering 70%, much greater than all adjacent
time steps. Besides, such an enormous energy difference,
like the almost 60% decrease from k= 0 tok= 1 in
the Fourier spectrum, may also bring advantages when
modeling agents’ preferences at different frequency scales.
The Haar transform redistributes energy in a way that
is different from DFT. In detail, as shown in Fig. 5, the
high-frequency portion XwH(b)in the Haar spectrums owns
higher percentages of energy than the Fourier spectrums.
This means the Haar transform could better capture these
changes when analyzing trajectories with rapidly changing
characteristics than the DFT. Another difference between the
Haar transform and DFT is that the Haar transform could
consider the time-frequency joint representation, which
means that the energy distribution of frequency portions in
Haar spectrums may change over time. Thus, the differenti-
ated low or high-frequency distribution in Haar spectrums
may facilitate modeling the detailed agent preferences.
In conclusion, transforms do not bring additional in-
formation but redistribute the existing information in the
trajectory. In this process of looking at the trajectory from
a new “vertical” view, the prediction network may be
able to discover different information that better responds
to agents’ behavioral preferences with different spectral
distributions. Hence, we apply DFT and Haar transforms
on trajectories in the proposed V2-Net and E-V2-Net, thus
forecasting trajectories hierarchically with spectrums.JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 11
Fig. 6. Performance comparisons of DFT (mark “ +”) and Haar (“ ×”) E-
V2-Net variations when forecasting different forms of trajectories. Lower
metrics (ADE/FDE/MPJPE) indicate better prediction performances.
(b)The Choose of Spectrum Keypoints. It can be seen
from Fig. 5 that most energy is concentrated in limited
frequency portions, whether in Fourier or Haar spectrums.
For example, 2 to 3 of all 8 lower-frequency portions could
occupy up to 90% energy in Fourier spectrums, and 4
frequency portions could also occupy at least 50% energy
in Haar spectrums, which are different from the energy dis-
tribution in the time domain. This phenomenon motivates
us to forecast trajectories with spectrums hierarchically, i.e.,
first coarsely predicting the lower-frequency portions that
occupy more spectral energy and then finely complement-
ing the left with less energy. According to Eq. 26, we can se-
lect the same percentages of temporal keypoints as the per-
centages of these energy-concentrated frequency portions
as an instruction, i.e., about 25% to 50% temporal points
as keypoints. Correspondingly, we sample the trajectory in
the time domain at uniform intervals so that the sampled
trajectory has a total of Nkeypoints, thus making sure that
the lower-frequency portions could occupy more energy in
the newly sampled Nkey-point spectrum, which is similar to
the reconstruct process in Fig. 2. Please refer to “Number of
Spectrum Keypoints” for detailed validations.
TABLE 8
Ablation Studies with best-of-20 on SDD. 2D coordinate variations are
labeled with subscript “ co”, and others are for 2D bounding boxes.
No. Model (T,Nkey) ADE↓ FDE↓ AIoU↑ FIoU↑
CO15 Vco(SEP) (DFT, 3) 7.19 12.13 N/A N/A
CO16 Vco (DFT, 3) 7.12 11.39 N/A N/A
CO17 Vco (Haar, 4) 7.01 11.62 N/A N/A
CO18 EVco (DFT, 3) 6.57 10.50 N/A N/A
CO19 EVco (Haar, 4) 6.83 11.01 N/A N/A
BB1 V (SEP) (DFT, 3) 7.06 10.91 0.698 0.578
BB2 V (DFT, 3) 6.78 10.73 0.717 0.601
BB3 V (DFT, 6) 7.07 11.37 0.709 0.587
BB4 EV (SEP) (DFT, 3) 6.78 10.75 0.708 0.582
BB5 EV (DFT, 3) 6.62 10.57 0.725 0.604
BB6 EV (DFT, 6) 6.81 10.91 0.716 0.594
BB7 EV (SEP) (Haar, 4) 6.99 11.35 0.699 0.563
BB8 EV (Haar, 4) 6.44 10.28 0.730 0.613
BB9 EV (Haar, 6) 6.90 10.82 0.713 0.599
(c)Comparisons of Fourier and Haar Transform acrossTABLE 9
Ablation studies on nuScenes (3D bounding box) with 2 seconds
prediction horizon ( th=tf= 4). Models with “(D)” only predict one
deterministic trajectory for each agent. Other models are validated with
best-of-20 , and we set Nkey= 2for all these models.
No. Model T ADE↓ FDE↓ AIoU↑ FIoU↑
3DBB1 V (D) DFT 0.534 0.930 0.597 0.439
3DBB2 EV (D) DFT 0.529 0.919 0.602 0.445
3DBB3 V (D) Haar 0.530 0.921 0.598 0.441
3DBB4 EV (D) Haar 0.525 0.918 0.608 0.450
3DBB5 V (SEP) Haar 0.210 0.306 0.761 0.683
3DBB6 V Haar 0.213 0.310 0.762 0.684
3DBB7 V (SEP) DFT 0.208 0.302 0.763 0.686
3DBB8 V DFT 0.207 0.299 0.764 0.687
3DBB9 EV None 0.213 0.308 0.760 0.681
3DBB10 EV (SEP) Haar 0.209 0.302 0.763 0.686
3DBB11 EV Haar 0.209 0.301 0.764 0.688
3DBB12 EV (SEP) DFT 0.207 0.299 0.763 0.687
3DBB13 EV DFT 0.203 0.291 0.766 0.691
TABLE 10
Ablation studies (3D skeleton prediction) on Human3.6M. The metrics
used are “FDE@ xms” (MPJPE) in millimeters.
No. Model (T,Nkey) 80↓ 160↓ 320↓ 400↓
SKE1 V (SEP) (Haar, 4) 6.02 16.73 46.58 63.29
SKE2 V (Haar, 4) 8.79 19.71 45.53 59.49
SKE3 EV (Haar, 4) 9.00 19.68 44.48 57.80
SKE4 V (SEP) (DFT, 4) 7.62 19.62 51.47 68.92
SKE5 V (DFT, 4) 13.03 26.68 55.04 69.49
SKE6 EV (DFT, 4) 12.18 25.08 52.54 66.62
SKE7 EV (None, 4) 8.68 24.71 50.67 64.56
SKE8 EV (Haar, 1) 9.45 21.65 46.70 63.58
SKE9 EV (Haar, 6) 9.21 20.47 45.38 59.23
Trajectory Forms. We begin with three minimal abla-
tion variations in Tab. 7: CO1, CO2, and CO3, where all
interaction-related networks have been removed, and only
the Transformer backbone is used to forecast 2D determin-
istic trajectory directly rather than hierarchically. Compared
to CO1, the average performance of DFT variation CO2 has
been improved by over 12.1%. Furthermore, Haar variation
CO3 also achieves a comparable performance with CO2,
which both greatly outperform the non-transform CO1,
indicating the usefulness of these transforms in 2D cases.
Haar transform could represent the time-frequency joint
character of the input sequence while Fourier transform
could not. It means that Haar variations may be suitable
for modeling with complex time-frequency changes. To
verify our thought, we visualize the quantitative metrics of
transform-related ablation variations across different forms
of trajectories in Fig. 6. Detailed metrics are reported in
Tabs. 7 to 10. We can see from Fig. 6 that E-V2-Net-DFT
(CO11) and E-V2-Net-Haar (CO13) perform almost the same
on 2D coordinate datasets. Specifically, the Haar variation
CO19 performs even worse than the DFT variation CO18
when forecasting 2D coordinates on SDD. The performance
differences between these transforms have gradually in-
creased as the dimensionality of the predicted trajectory
increases. Haar variations {BB8, SKE3 }outperform DFT
variations {BB5, SKE6 }on the more challenging 2D bound-
ing box and 3D skeleton datasets for up to 2.8% ADE and
13.2% MPJPE accordingly. On the contrary, DFT variationJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 12
SKE6 even performs worse than the none-transform SKE7
for about 3.2% quantitatively at 400ms when forecasting
3D skeletons. These results show that the Haar variation
performs better at predicting datasets (scenarios) that con-
tain more complex time-frequency changes, like the more
complex 2D bounding boxes and 3D skeletons.
Notably, Haar variations do not show huge performance
gains to DFT ones for 3D bounding box cases. When gen-
erating k= 20 trajectories, Haar variation 3DBB11 loses
about 1.4% ADE and 2.3% FDE relative to the DFT vari-
ation 3DBB13. We infer it is caused by the short observa-
tion period th= 4 in this case, which makes trajectories
more “smoother”. In other words, in the relatively short
observation period, the possibility of large changes in the
trajectory will be significantly reduced, considering these
agents’ physical movement constraints. The Haar transform
has a vanishing moment of 1, which means that the energy
is distributed more in the high-frequency portions in the
Haar spectrum, making it difficult to focus on the lower-
frequency portions, leading to a poor representation of the
smoother trajectories3. Nevertheless, both transforms still
facilitate the prediction. Compared to the none-transform
3DBB9, transformed variations {3DBB11, 3DBB13 }obtain at
least 1.9% better ADE and 2.3% better FDE.
In conclusion, DFT is more effective for simpler or
smoother trajectories, but Haar transform is more effec-
tive for trajectories with higher dimensionality or fast
time-frequency changing properties. These experiments also
demonstrate the different advantages of these transforms,
thus supporting the choice for different scenarios.
The Existence of Dimension-wise Interactions and Verifi-
cations of Bilinear Structures across Trajectory Forms. We
conduct the following discussions to analyze the existence of
dimension-wise interactions and how the proposed bilinear
structure helps model them across different trajectory forms.
(a)2D Coordinate Cases. We start by comparing vari-
ations CO9 and CO8 in Tab. 7. The “SEP” variation CO8
considers nothing about interactions between two dimen-
sions xtandytin a 2D coordinate. Especially in the zara1
dataset, CO8 performs worse than the other one for about
10.5% ADE and 30.0% FDE, showing a fairly non-negligible
performance drop. Results on SDD (in Tab. 8) also show sim-
ilar trends, including only about 1% ADE drop and a 6.1%
huge FDE drop between variations CO15 and CO16, which
demonstrates the existence of dimension-wise interactions
within 2D coordinates when forecasting trajectories. Com-
paring variations CO11 and CO9 (Tab. 7), the former (with
bilinear structure) obtains 1.13% better ADE on ETH-UCY.
Variation CO18 also outperforms variation CO16 (Tab. 8)
with a significant improvement of 7.7% ADE and 7.8% FDE,
which proves the usefulness of the bilinear structure in
modeling dimension-wise interactions in coordinate cases.
(b)2D Bounding Box Cases. Similar to the above SEP
variations, in Tab. 8, SEP variations {BB1, BB4, BB7 }have
been constructed to forecast 4-dimensional 2D bounding
boxes by forecasting two 2D corner points separately, which
means that interactions between these two 2D points have
not been considered. Variation BB1 obtains 4.12% and 1.68%
3. See more discussions and analyses in the supplemental material.worse ADE and FDE than BB2, similarly 2.41%/1.70% worse
between {BB4, BB5 }and 8.54%/10.40% worse between
{BB7, BB8 }. Regardless of transforms, these SEP variations
show a significant performance drop, which illustrates the
existence of dimension-wise interaction between two points
in 2D bounding box cases, not to mention the above-
validated interactions within each 2D point. In addition,
compared with BB2, DFT variation BB5 brings up to 2.4%
ADE and 1.5% FDE improvements. Haar variation BB8 also
shows similar improvements, verifying the usefulness of the
bilinear structure in 2D bounding box cases.
(c)3D Bounding Box Cases. We can see from Tab. 9
that SEP variations {3DBB7, 3DBB10, 3DBB12 }have lead to
uo 1.5% performance drops compared to their “interactive”
variations {3DBB8, 3DBB11, 3DBB13 }. Thus, the dimension-
wise interactions in 3D bounding box prediction cases can
be verified through these DFT variations (we will dis-
cuss variation 3DBB5 later). Furthermore, when generating
k= 20 trajectories, variation 3DBB13 shows better perfor-
mance than variation 3DBB8, including improvements of
8.3% in ADE and 10.4% in FDE. Haar variation 3DBB11
also outperforms 3DBB6 for about 1.9% ADE and 2.6% FDE.
From these results, the usefulness of the bilinear structure
has been validated when forecasting 3D bounding boxes.
(d)3D Skeleton Cases. Intuitively, the connections be-
tween different joints in the skeleton naturally become part
of the dimension-wise interactions. As shown in Tab. 10,
comparing Haar variations SKE1 and SKE2, separately fore-
casting each 3D point may lead to a 6.00% performance
drop (at 400ms). The results also indicate another interesting
trend, i.e., the SEP variations perform better than others over
a fairly short period. Also, the DFT variation SKE5 performs
worse compared to the SEP variation SKE4. We will dis-
cuss them later. After adding the bilinear structure, both
DFT and Haar variations (SKE6 and SKE3) are greatly im-
proved, including 3.33% and 8.67% at 400ms. Considering
that skeletons have higher dimensionality ( M= 51 ), these
results further demonstrate the existence of dimension-wise
interactions in heterogeneous trajectories and verify the
effectiveness of bilinear structures for modeling them.
(e)Discussions of Modeling Capacity and General-
izability. We can see that there is a clear difference in
the modeling capability of bilinear structures for model-
ing dimension-wise interactions between different trajectory
forms and transforms. For example, the SEP Haar variation
3DBB5 performs better than 3DBB6 in Tab. 9, while the DFT
variation SKE5 performs worse compared to the SEP varia-
tion SKE4 in Tab. 10. In the above analyses, we conclude that
the Fourier transform is better for smoother trajectories and
vice versa for the Haar transform. Thus, we can infer that
different ways to characterize each trajectory-dimension
could also directly affect or limit the generalizability.
In addition, we mentioned that the SEP variations per-
form better than others over a fairly short period, such
as the metrics under 80ms and 160ms in Tab. 10. Results
on 2D coordinate cases in Tab. 7 also indicate a similar
phenomenon that the impact of dimension-wise interactions
on trajectories further into the future is even more critical
since the changes in FDE are larger than ADE. It can be
seen from these results that there might be some “response
time” for the dimension-wise interaction to make an effortJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 13
ObservationsGroudtruths
Predictions
Neighbors
Observations (co)Groudtruths(co)Predictions (co)
Fig. 7. Visualized predictions given by E-V2-Net-Haar with 2D bounding
boxes and 2D coordinates. Each image contains 20 random predictions.
to determine future trajectories. In other words, dimension-
wise interactions may become more critical when predicting
longer-term trajectories4. In summary, the bilinear structure
will be more effective in modeling dimensional-wise interac-
tions in heterogeneous trajectories with more complex time-
frequency properties and longer prediction terms.
Number of Spectrum Keypoints ( Nkey).This part ana-
lyzes the quantitative performance changes brought by the
number of keypoints. Tab. 7 reports the results of DFT
variations (CO5, CO9, CO6, and CO7) under different Nkey
configurations (2D coordinate). Their results indicate that
variation CO9 ( Nkey= 3) outperforms CO5 ( Nkey= 1) with
8.3% better ADE and 6.0% better FDE. The performance
of CO7 ( Nkey= 6 ) reduces by 5.7% and 1.8% compared
to the base CO4. In Tab. 8, 2D bounding boxes results of
DFT variations (BB5 and BB6) and Haar variations (BB8 and
BB9) demonstrate that the prediction network may perform
worse with either a lower or higher number of keypoints,
where ADE and FDE of variation BB6 are 2.9% and 3.2%
worse than BB5, and about 7.1% and 5.3% ADE and FDE
drops for variation BB9 compared to BB8. The same phe-
nomenon also occurs when forecasting 3D skeletons. In
Tab. 10, Nkey= 6 SKE9 and Nkey= 1 SKE8 perform worse
than SKE6 ( Nkey= 4) with 10.0% and 2.5% in FDE@400ms,
respectively. As a result, too-low or too-high Nkeys may lead
to metric drops, consistent with all these trajectory forms.
Similar to the quantitative analysis in [31], we can describe
these phenomena as a smaller Nkeymight cause a looser
planning division that makes it challenging to reflect the
differences between agents’ similar future choices, while a
larger one may lead to a strict trends division, which could
be difficult for the subsequent network to reflect agents’
multiple uncertain future choices and motion preferences.
4.4 Qualitative Analysis
Visualization. As shown in Fig. 7 and Fig. 8, E-V2-Net-
Haar could predict heterogeneous trajectories considering
multiple interactions. For example, E-V2-Net gives a variety
of different future options for bikers, carts, and cars crossing
the intersection. Besides, when agents bypass the grass, it
considers the physical constraints of the environment to
provide acceptable predictions. Additionally, it also shows
strong adaptability in some unique prediction scenarios. For
instance, it gives scene-friendly predictions to the biker to
pass through the traffic circle: turning right or going ahead
4. More validations are included in the supplemental material.
ObservationsGroudtruthsPredictions
(a)(b)
Fig. 8. Visualized E-V2-Net-Haar predictions (3D bounding boxes) on
nuScenes. For a clear description, only 1 sampled prediction is drawn.
to leave the circle, and turning left to keep driving in the
circle. Notably, when predicting 2D and 3D bounding boxes,
the motion trends and the box structures are considered
together. In short, E-V2-Nets could provide heterogeneous
predicted trajectories adapted to different scenarios.
We also visualize skeleton predictions in Fig. 9. Un-
like coordinate or bounding box prediction, forecasting the
skeleton is a different challenge since human actions are
collaborative efforts of movements in every joint. Interest-
ingly, Fig. 9 (a) shows that the slight bend in the knee can be
accurately predicted by V2-Net-Haar. Is is also amzaing that
E-V2-Net-Haar can predict the postures of different joints
while satisfying the physical constraints of human bodies in
Fig. 9 (b), when predicting the action “Sitting Down”. The
detailed body movements, such as trunk rotations, bendings
and raisings of arms, and the lifting of the back, are all pre-
dicted vividly. Notably, it does not consider any additional
motion rules, which differs from most motion prediction
approaches. Therefore, these qualitative results demonstrate
the superiority and competitiveness of E-V2-Net in higher-
dimensional heterogeneous trajectory prediction.
Discussions of Dimension-wise Interactions in Visualized
Predictions. Next, we further qualitatively validate the ex-
istence of dimension-wise interactions and discuss how E-
V2-Net maintain these interactions when forecasting.
(a)Separate (SEP) Variations. 2D bounding box predic-
tions provided by the separate variation V2-Net-DFT (SEP)
are shown in Fig. 10 (p) to (t). Predictions in subfigures (p),
(q), (s), and (t) indicate that the SEP variation completely
fails to establish and reflect potential dimension-wise inter-
active factors, such as the preservation and change of the
shape of these boxes. For example, whether for the bottom
dark green prediction indicated by the arrow in Fig. 10
(t), or the dark green prediction indicated in (q), the SEP
variation does not hold their shapes at all, and the forecasted
destination box could even almost turns into a dot. We also
visualize 3D human skeleton predictions provided by the
V2-Net (SEP) in Fig. 9 (e) and (h). We can see that the
predicted skeletons are almost movements of each joint
individually, without being able to take into account the
relationships between the joints. As a result, the length of
the predicted forearms or legs seems anomalous, and their
synergistic movements appear uncoordinated.
(b)Non-Bilinear-Structure Variations. Compared with
the above SEP variation, V2-Net-DFT takes the 4-
dimensional bounding box as the input, which means that
interactions among dimensions could be partially estab-
lished due to the trajectory embedding operation. Its 2DJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 14
ObservationPrediction (EV)GroundtruthObservationObservationPrediction (EV)GroundtruthObservation40ms400ms40ms400ms
40ms400ms40ms400ms
ObservationPrediction (SEP)GroundtruthObservation40ms400ms40ms400msObservationPrediction (EV)Prediction (V)Observation40ms400ms40ms400ms
ObservationPrediction (SEP)GroundtruthObservation40ms400ms40ms400msObservationPrediction (EV)Prediction (V)Observation40ms400ms40ms400ms
ß(a)
(b)
(c)(d)(e)
(f)(g)(h)
Fig. 9. Visualized 3D skeleton predictions provided by E-V2-Net-Haar,
V2-Net-Haar, and V2-Net-Haar (SEP) on Human3.6M dataset.
bounding box predictions in Fig. 10 (k) to (o) exhibit better
shape preservation properties than the above SEP varia-
tion. For example, compared with the above-discussed dark
green predictions in Fig. 10 (q) and (t), the light green
prediction and the blue prediction focused in subfigures (l)
and (o) show the suppression of the unnatural changes of
the bounding box to a certain extent. Nevertheless, the dark
green prediction focused in subfigure (k) even gets bigger as
the prediction time grows, which indicates that it still fails to
model the detailed dimension-wise interactions. As for the
skeleton variation, predictions in Fig. 9 (d) also show that
leg movements, especially the left leg, are more natural and
acceptable compared to the SEP ones in Fig. 9 (e).
(c)Bilinear Structure Variations. As shown in the first
two columns of Fig. 10, E-V2-Net variations add additional
bilinear structures to focus on the dimension-wise inter-action. Compared with the non-bilinear variations, two E-
V2-Net variations present better predictions for 2D bound-
ing boxes qualitatively. Most importantly, the previously
discussed examples of hard-to-maintain shapes are largely
resolved. Besides, Haar variations also better predict the
motion tendency of bounding boxes. For example, in the
bottom pink prediction in Fig. 10 (j), these predicted boxes
become more slender when the biker turns and wider
after the turn, thus conforming to the real motion pattern
of the biker in the drone view, which is not reflected in
the other variations, thus also indicating the qualitative
improvement brought by Haar transform. Additionally, E-
V2-Net also shows better qualitative performance when
forecasting skeletons compared to the non-bilinear ones.
Compared with Fig. 9 (g), predictions in (f) present better
joint collaborations, like moving the right arm slightly up
and down in front of the chest and bending the knee. It
shows the competitiveness of the bilinear structure in mod-
eling dimension-wise interactions when forecasting high-
dimensional heterogeneous trajectories.
Analyses of Outer Product Matrices. Next, we visualize
several outer product matrices Rto analyze how bilinear
structures work in modeling dimension-wise interactions.
(a)2D Coordinate Cases. As shown in Fig. 11 (a) and (b),
there are multiple separate bright parts in the outer product
matrix. We call these bright parts the “high-value clusters”
for convenient analyses. It can be seen that the network
could capture potential interactions in the high-dimensional
fα, which are represented by high-value clusters in these
matrices, especially these non-diagonal elements, although
dimension-wise interactions are relatively easy in 2D coor-
dinate cases compared to other heterogeneous trajectories.
(b)2D Bounding Box Cases. As shown in Fig. 11 (d)
and (e), there are more high-value clusters in the outer
product matrices which indicates there may exist more
other potential dimension-wise interactions in 2D bounding
boxes ( M= 4). Intuitively, the dimension-wise interactions
in 2D bounding boxes include interactions between two
corner points (top-left and bottom-right points) so that the
shape of the bounding box could be maintained, leading to
more high-value clusters in outer product matrices. It also
indicates that E-V2-Net could focus on the hard-to-capture
slight temporal motion changes. In Fig. 11 (d) and (e), two
observed bounding box trajectories have similar shapes, but
their matrices Rare obversely different in the last frequency
portion. As a result, E-V2-Net forecast these samples in
completely different trends and changing properties.
(c)3D Skeleton Cases. It can be seen that the observed
skeletons in Fig. 12 basically remain static in the first four
observation frames, and after that, the hands start to rise till
the end. Correspondingly, as shown in Fig. 12, the outer
product matrices in k= 1 andk= 2 seem the same,
while some bright parts in k= 3 andk= 4 have become
darker. Interestingly, the outer product matrix in k= 5
is completely different from the others, which means that
dimension-wise interactions at this time-frequency portion
(corresponding to the last two observation frames) present
different frequency properties. This phenomenon could re-
flect the bilinear structure’s better time-frequency localiza-
tion characteristics (like the subtle and rapid changes).JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 15
V2-Net-DFT E-V2-Net-DFT V2-Net-DFT (SEP) V2-Net-DFT (SEP)(co) E-V2-Net-Haar
Fig. 10. Visualized predictions provided by different V2-Net or E-V2-Net variations on SDD (2D coordinates or bounding boxes).
(a)(b)(c)(d)(e)1234567𝒩!=8Frequency Nodes
Fig. 11. Visualized outer product matrices R=fα⊗fα∈R8×64×64in
different prediction cases. The horizontal axis (1 to Nh= 8) represents
different frequency components. Bright parts indicate higher values.
Discussions of Qualitative Generalizability. E-V2-Net is
designed to handle different trajectory forms. However,
the forecasted trajectories and their outer product matrices
present different styles or distributions for different forms
of trajectories. We first take V2-Net (SEP) as an example.
Its coordinate predictions (in Fig. 10 (u) - (y)) are about
acceptable, while the 2D bounding box predictions in (p)
- (t) have become weird, with some of the boxes’ abnormal
shapes. Its skeleton predictions could even be described as
“ridiculous” in Fig. 9 (e) for it forecasts trajectories that
were completely inconsistent with physical movements. We
can conclude that the dimension-wise interactions may be-
come more “influential” as the dimensionality of trajectories
increases. By adding the bilinear structure with different
transforms, predictions provided by the enhanced E-V2-
Net models have been greatly improved in these corre-
sponding Fig. 10 (a) to (j) or Fig. 9 (c) and (f) cases. However,
the degrees of these improvements are also clearly limited
to the dimensionalities of trajectories. For example, the qual-
itative enhancements in 2D bounding box predictions are
more straightforward, while there is still a more significant
shortage of skeleton predictions than their groundtruths.
It can be seen that the number of high-value clustersTABLE 11
Comparisons of inference times and parameter amount of 2D
coordinate prediction on ETH-UCY ( th= 8, tf= 12 ). Results are
obtained from [68] on one NVIDIA GeForce GTX 1080Ti card.
Models Time/Para. Models Time/Para.
Social-LSTM [33] 1180 ms/264K S-GAN [41] 97 ms/46.3K
SR-LSTM [36] 1179 ms/64.9K DAG-Net [78] 46 ms/2.35M
PECNet [23] 607 ms/2.10M STGCNN [35] 2.0 ms/7.6K
Next [79] 114 ms/360.3K STC-Net [68] 1.3 ms/0.7K
EV-DFT 43 ms/3.9M EV-DFT-L 21 ms/1.9M
EV-Haar 40 ms/3.9M EV-Haar-L 19 ms/1.9M
in outer product matrices also differs with trajectory forms
(dimensionalities). The more clusters, the more complex
dimension-wise interactions would be. For example, sce-
nario (c) is easier to handle since the agent almost stands
still during observation. Then, Fig. 11 (c) have fewer high-
value clusters (about only five clusters) than (a) and (b) (up
to 49 clusters). Instead, Fig. 11 (d) and (e) have more clusters
(up to 81), demonstrating the more complex dimension-wise
interactions within bounding boxes. The same phenomenon
also occurs in 3D human skeleton prediction, and there
are a considerable amount of high-value clusters (about
144 clusters) in Fig. 12, which means that dimension-wise
interactions have become “more and more” complex.
From the above analyses, although the prediction per-
formance improvement is relatively limited in higher di-
mensional skeletons, it still illustrates its effectiveness in
predicting heterogeneous trajectories, validating its quan-
titative and qualitative generalization performance.
4.5 Parameter Amount and Inference Speed
Comparisons with Other Baselines. We report the inference
speed and the number of parameters of different models in
Tab. 11. All results are measured on one NVIDIA GeForce
GTX 1080Ti GPU. Models with postfix “-L” indicate that
linear interpolation models have replaced their second-stage
spectrum interpolation networks to speed up calculation
(variations CO12 and CO14 in Tab. 7). Many fast calculation
methods like STC-Net [68] have been proposed recently.
Although the time efficiency of the proposed methods isJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 16
PredictionObservation1234𝑘=5Time-Frequency Nodes
Fig. 12. Visualized outer product matrices R∈R5×256×256with the corresponding visualized prediction given by E-V2-Net-Haar.
TABLE 12
Inference times τand memory usages µof V2-Net and E-V2-Net (with
linear S2) on one Apple M1 Mac mini. Results (as “ τms/µMB”) are
obtained by predicting k= 20 trajectories under th= 8, tf= 12 .
No. Model, T co bb 3Dbb ske
INF1 V , DFT 30 / 13.1 30 / 13 37 / 13 112 / 83.1
INF2 V , Haar 26 / 13.6 27 / 11.3 27 / 11.3 28 / 81.9
INF3 EV , DFT 31 / 25.9 34 / 25.7 37 / 25.8 112 / 147.9
INF4 EV , Haar 27 / 20.1 27 / 17.8 28 / 17.8 29 / 124.1
not better than theirs, we have greatly improved the quan-
titative prediction performance as a balance.
Efficiency Discussions. In Tab. 12, we report the infer-
ence time under different forms of predicted trajectories
on one Mac Mini with an Apple M1 chip (8GB memory)
(which could be easily accessed on mobile devices such as
iPads). It can be seen that despite the higher amount of train-
able parameters, models still have faster inference speeds.
Especially for Haar variations INF2 and INF4, they have
almost about 400% faster compared to DFT variations when
predicting 3D skeletons5. We also report memory usages
during test. In Tab. 12, E-V2-Net variations allocates more
memory when running tests. Interestingly, Haar variations
still exhibit better memory use properties, reducing memory
by about 15% compared to DFT variations, which also
illustrates another significant advantage of the Haar vari-
ation in dealing with complex heterogeneous trajectories,
demonstrating their better real-time prediction capability.
5 C ONCLUSION AND LIMITATIONS
This work aims to forecast heterogeneous trajectories via
spectrums hierarchically. Thus, trajectories to be analyzed
and forecasted in this manuscript could have multiple forms
like bounding boxes or skeletons. Also, the dimensionality
of trajectories could become larger. Naturally, the interac-
tions between the different dimensions of the trajectory
will become more complex and more non-negligible. One
of our main focuses has moved on to the potential inter-
actions among different trajectory-dimensions, which we
name dimension-wise interactions . Heterogeneous trajec-
tories also mean that each trajectory-dimension may have
more complex time-frequency properties. Thus, the other
focus lies on how to model these interactions by simulta-
neously considering the time-frequency response of each
trajectory-dimension when forecasting.
This work further extends V2-Net [31] in two aspects. On
the one hand, we enhance the structure and input-output
settings of the original network to be applied to trajecto-
ries with any dimensionalities, and employ an additional
bilinear structure to model and fuse factors “frequency re-
sponse” and “dimension-wise interaction” simultaneously.
5. See more ablation efficiency discussions in supplemental material.On the other hand, more transforms, such as the Haar
wavelet, are introduced to cope with different trajectory
forms. Multiple experiments show that E-V2-Net outper-
forms most state-of-the-art methods quantitatively on the
ETH-UCY benchmark, Stanford Drone Dataset, nuScenes,
and Human3.6M when forecasting different forms of hetero-
geneous trajectories, including 2D coordinates, 2D and 3D
bounding boxes, and 3D human skeletons. Ablation discus-
sions have also verified the usefulness of each component.
Despite the impressive performance of V2-Net and E-
V2-Net, there are still potentials for further improvement.
For example, the Haar transform is not good at handling
simpler or smoother trajectories despite its better time-
frequency localization characteristics. Moreover, V2-Net and
E-V2-Net do not consider any additional priors, leading
to some performance drop when encountering complex
trajectory forms like human skeletons. Therefore, the above
limitations will become the focus of our future research.
REFERENCES
[1] Y. Chai, B. Sapp, M. Bansal, and D. Anguelov, “Multipath: Multiple
probabilistic anchor trajectory hypotheses for behavior predic-
tion,” arXiv preprint arXiv:1910.05449 , 2019. 1, 4
[2] Y. Chen, B. Ivanovic, and M. Pavone, “Scept: Scene-consistent,
policy-based trajectory predictions for planning,” in Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition ,
2022, pp. 17 103–17 112. 1
[3] N. Lee, W. Choi, P . Vernaza, C. B. Choy, P . H. Torr, and M. Chan-
draker, “Desire: Distant future prediction in dynamic scenes with
interacting agents,” in Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition , 2017, pp. 336–345. 1
[4] S. Pellegrini, A. Ess, K. Schindler, and L. Van Gool, “You’ll never
walk alone: Modeling social behavior for multi-target tracking,” in
2009 IEEE 12th International Conference on Computer Vision . IEEE,
2009, pp. 261–268. 1, 8
[5] T. Fernando, S. Denman, S. Sridharan, and C. Fookes, “Soft+
hardwired attention: An lstm framework for human trajectory
prediction and abnormal event detection,” Neural networks , vol.
108, pp. 466–478, 2018. 1
[6] B. T. Morris and M. M. Trivedi, “Trajectory learning for activity
understanding: Unsupervised, multilevel, and long-term adaptive
approach,” IEEE transactions on pattern analysis and machine intelli-
gence , vol. 33, no. 11, pp. 2287–2301, 2011. 1
[7] D. Xie, T. Shu, S. Todorovic, and S.-C. Zhu, “Learning and inferring
“dark matter” and predicting human intents and trajectories in
videos,” IEEE transactions on pattern analysis and machine intelli-
gence , vol. 40, no. 7, pp. 1639–1652, 2017. 1
[8] F. Zheng, L. Wang, S. Zhou, W. Tang, Z. Niu, N. Zheng, and
G. Hua, “Unlimited neighborhood interaction for heterogeneous
trajectory prediction,” in Proceedings of the IEEE/CVF International
Conference on Computer Vision , 2021, pp. 13 168–13 177. 1, 4
[9] Y. Ma, X. Zhu, S. Zhang, R. Yang, W. Wang, and D. Manocha, “Traf-
ficpredict: Trajectory prediction for heterogeneous traffic-agents,”
inProceedings of the AAAI Conference on Artificial Intelligence , vol. 33,
2019, pp. 6120–6127. 1, 4
[10] R. Chandra, U. Bhattacharya, A. Bera, and D. Manocha, “Traphic:
Trajectory prediction in dense and heterogeneous traffic using
weighted interactions,” in Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition , 2019, pp. 8483–8492. 1
[11] H. Caesar, V . Bankiti, A. H. Lang, S. Vora, V . E. Liong, Q. Xu,
A. Krishnan, Y. Pan, G. Baldan, and O. Beijbom, “nuscenes:
A multimodal dataset for autonomous driving,” arXiv preprint
arXiv:1903.11027 , 2019. 1, 8JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 17
[12] R. Girdhar and D. Ramanan, “Attentional pooling for action recog-
nition,” Advances in neural information processing systems , vol. 30,
2017. 2, 3
[13] Y. Huang, H. Bi, Z. Li, T. Mao, and Z. Wang, “Stgat: Modeling
spatial-temporal interactions for human trajectory prediction,” in
Proceedings of the IEEE International Conference on Computer Vision ,
2019, pp. 6272–6281. 2
[14] C. Yu, X. Ma, J. Ren, H. Zhao, and S. Yi, “Spatio-temporal graph
transformer networks for pedestrian trajectory prediction,” in
European Conference on Computer Vision . Springer, 2020, pp. 507–
523. 2, 4, 9
[15] Y. Yuan, X. Weng, Y. Ou, and K. M. Kitani, “Agentformer: Agent-
aware transformers for socio-temporal multi-agent forecasting,”
inProceedings of the IEEE/CVF International Conference on Computer
Vision (ICCV) , 2021, pp. 9813–9823. 2, 4, 8, 9
[16] X. Zhu, G. T. Beauregard, and L. L. Wyse, “Real-time signal
estimation from modified short-time fourier transform magnitude
spectra,” IEEE Transactions on Audio, Speech, and Language Process-
ing, vol. 15, no. 5, pp. 1645–1653, 2007. 2
[17] K. Kaur, N. Jindal, and K. Singh, “Fractional fourier transform
based riesz fractional derivative approach for edge detection and
its application in image enhancement,” Signal Processing , vol. 180,
p. 107852, 2021. 2, 4
[18] L. Zhang and P . Bao, “Edge detection by scale multiplication in
wavelet domain,” Pattern Recognition Letters , vol. 23, no. 14, pp.
1771–1784, 2002. 2, 5
[19] Y. Y. Tang and X. You, “Skeletonization of ribbon-like shapes based
on a new wavelet function,” IEEE Transactions on pattern analysis
and machine intelligence , vol. 25, no. 9, pp. 1118–1133, 2003. 2, 5
[20] F.-H. Cheng and Y.-L. Chen, “Real time multiple objects tracking
and identification based on discrete wavelet transform,” Pattern
recognition , vol. 39, no. 6, pp. 1126–1139, 2006. 2, 5
[21] S. Becker, R. Hug, W. Hubner, and M. Arens, “Red: A simple
but effective baseline predictor for the trajnet benchmark,” in
Proceedings of the European Conference on Computer Vision (ECCV) ,
2018, pp. 0–0. 2
[22] A. Monti, A. Porrello, S. Calderara, P . Coscia, L. Ballan, and R. Cuc-
chiara, “How many observations are enough? knowledge distil-
lation for trajectory forecasting,” in Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition , 2022. 2, 4
[23] K. Mangalam, H. Girase, S. Agarwal, K.-H. Lee, E. Adeli, J. Malik,
and A. Gaidon, “It is not the journey but the destination: End-
point conditioned trajectory prediction,” in European Conference on
Computer Vision , 2020, pp. 759–776. 2, 5, 9, 15
[24] K. Mangalam, Y. An, H. Girase, and J. Malik, “From goals, way-
points & paths to long term human trajectory forecasting,” in
Proceedings of the IEEE/CVF International Conference on Computer
Vision , 2021, pp. 15 233–15 242. 2, 5, 9
[25] H. Tran, V . Le, and T. Tran, “Goal-driven long-term trajectory
prediction,” in Proceedings of the IEEE/CVF Winter Conference on
Applications of Computer Vision , 2021, pp. 796–805. 2, 5
[26] C. Wong, B. Xia, Q. Peng, W. Yuan, and X. You, “Msn: multi-style
network for trajectory prediction,” IEEE Transactions on Intelligent
Transportation Systems , vol. 24, pp. 9751 – 9766, 2023. 2, 4, 5, 6, 9
[27] J. B. Tenenbaum and W. T. Freeman, “Separating style and content
with bilinear models,” Neural computation , vol. 12, no. 6, pp. 1247–
1283, 2000. 3, 4, 5
[28] T.-Y. Lin, A. RoyChowdhury, and S. Maji, “Bilinear convolutional
neural networks for fine-grained visual recognition,” IEEE trans-
actions on pattern analysis and machine intelligence , vol. 40, no. 6, pp.
1309–1322, 2017. 3, 5, 7
[29] Q. Xu, Y. Mei, J. Liu, and C. Li, “Multimodal cross-layer bilinear
pooling for rgbt tracking,” IEEE Transactions on Multimedia , vol. 24,
pp. 567–580, 2021. 3, 5
[30] D. Guo, C. Xu, and D. Tao, “Bilinear graph networks for visual
question answering,” IEEE Transactions on neural networks and
learning systems , 2021. 3, 5
[31] C. Wong, B. Xia, Z. Hong, Q. Peng, W. Yuan, Q. Cao, Y. Yang, and
X. You, “View vertically: A hierarchical network for trajectory pre-
diction via fourier spectrums,” in European Conference on Computer
Vision . Springer, 2022, pp. 682–700. 4, 5, 9, 13, 16
[32] A. Haar, “Zur theorie der orthogonalen funktionensysteme,”
Mathematische Annalen , vol. 69, no. 3, pp. 331–371, 1910. 4
[33] A. Alahi, K. Goel, V . Ramanathan, A. Robicquet, L. Fei-Fei, and
S. Savarese, “Social lstm: Human trajectory prediction in crowded
spaces,” in Proceedings of the IEEE conference on computer vision and
pattern recognition , 2016, pp. 961–971. 4, 8, 15[34] F. Giuliari, I. Hasan, M. Cristani, and F. Galasso, “Transformer
networks for trajectory forecasting,” pp. 10 335–10 342, 2021. 4, 9
[35] A. Mohamed, K. Qian, M. Elhoseiny, and C. Claudel, “Social-
stgcnn: A social spatio-temporal graph convolutional neural net-
work for human trajectory prediction,” in Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition ,
2020, pp. 14 424–14 432. 4, 15
[36] P . Zhang, W. Ouyang, P . Zhang, J. Xue, and N. Zheng, “Sr-lstm:
State refinement for lstm towards pedestrian trajectory predic-
tion,” in Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition , 2019, pp. 12 085–12 094. 4, 15
[37] L.-W. Tsao, Y.-K. Wang, H.-S. Lin, H.-H. Shuai, L.-K. Wong, and
W.-H. Cheng, “Social-ssl: Self-supervised cross-sequence represen-
tation learning based on transformers for multi-agent trajectory
prediction,” in European Conference on Computer Vision . Springer,
2022, pp. 234–250. 4, 9
[38] C. Choi, S. Malla, A. Patil, and J. H. Choi, “Drogon: A trajectory
prediction model based on intention-conditioned behavior reason-
ing,” arXiv preprint arXiv:1908.00024 , 2019. 4, 5
[39] H. Girase, H. Gang, S. Malla, J. Li, A. Kanehara, K. Mangalam,
and C. Choi, “Loki: Long term and key intentions for trajectory
prediction,” in Proceedings of the IEEE/CVF International Conference
on Computer Vision , 2021, pp. 9803–9812. 4, 5
[40] N. Rhinehart, R. McAllister, K. Kitani, and S. Levine, “Precog:
Prediction conditioned on goals in visual multi-agent settings,” in
Proceedings of the IEEE International Conference on Computer Vision ,
2019, pp. 2821–2830. 4, 5
[41] A. Gupta, J. Johnson, L. Fei-Fei, S. Savarese, and A. Alahi, “Social
gan: Socially acceptable trajectories with generative adversarial
networks,” in Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition , 2018, pp. 2255–2264. 4, 8, 15
[42] A. Sadeghian, V . Kosaraju, A. Sadeghian, N. Hirose,
H. Rezatofighi, and S. Savarese, “Sophie: An attentive gan
for predicting paths compliant to social and physical constraints,”
inProceedings of the IEEE Conference on Computer Vision and Pattern
Recognition , 2019, pp. 1349–1358. 4, 9
[43] V . Kosaraju, A. Sadeghian, R. Mart ´ın-Mart ´ın, I. Reid,
H. Rezatofighi, and S. Savarese, “Social-bigat: Multimodal
trajectory forecasting using bicycle-gan and graph attention
networks,” in Advances in Neural Information Processing Systems ,
2019, pp. 137–146. 4, 9
[44] T. Salzmann, B. Ivanovic, P . Chakravarty, and M. Pavone, “Trajec-
tron++: Dynamically-feasible trajectory forecasting with heteroge-
neous data,” in Proceedings of the European conference on computer
vision (ECCV) . Springer, 2020, pp. 683–700. 4, 9
[45] T. Yagi, K. Mangalam, R. Yonetani, and Y. Sato, “Future person
localization in first-person videos,” in Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition , 2018, pp.
7593–7602. 4
[46] R. Quan, L. Zhu, Y. Wu, and Y. Yang, “Holistic lstm for pedestrian
trajectory prediction,” IEEE transactions on image processing , vol. 30,
pp. 3229–3239, 2021. 4
[47] S. Saadatnejad, Y. Z. Ju, and A. Alahi, “Pedestrian 3d bounding
box prediction,” arXiv preprint arXiv:2206.14195 , 2022. 4, 8, 9
[48] C. Xu, R. T. Tan, Y. Tan, S. Chen, Y. G. Wang, X. Wang, and Y. Wang,
“Eqmotion: Equivariant multi-agent motion prediction with in-
variant interaction reasoning,” in Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition , 2023, pp.
1410–1420. 4, 8, 9, 10
[49] H. Cheng, W. Liao, M. Y. Yang, B. Rosenhahn, and M. Sester,
“Amenet: Attentive maps encoder network for trajectory predic-
tion,” ISPRS Journal of Photogrammetry and Remote Sensing , vol. 172,
pp. 253–266, 2021. 4
[50] T. Komatsu, K. Tyon, and T. Saito, “3-d mean-separation-type
short-time dft with its application to moving-image denoising,” in
2017 IEEE International Conference on Image Processing (ICIP) , 2017,
pp. 2961–2965. 4
[51] W. Mao, M. Liu, and M. Salzmann, “History repeats itself: Human
motion prediction via motion attention,” in European Conference on
Computer Vision . Springer, 2020, pp. 474–489. 4
[52] W. Mao, M. Liu, M. Salzmann, and H. Li, “Learning trajectory
dependencies for human motion prediction,” in Proceedings of the
IEEE/CVF International Conference on Computer Vision , 2019, pp.
9489–9497. 4, 10
[53] D. Cao, Y. Wang, J. Duan, C. Zhang, X. Zhu, C. Huang, Y. Tong,
B. Xu, J. Bai, J. Tong et al. , “Spectral temporal graph neural net-JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 18
work for multivariate time-series forecasting,” Advances in Neural
Information Processing Systems , vol. 33, pp. 17 766–17 778, 2020. 4
[54] D. Cao, J. Li, H. Ma, and M. Tomizuka, “Spectral temporal graph
neural network for trajectory prediction,” in 2021 IEEE Interna-
tional Conference on Robotics and Automation (ICRA) . IEEE, 2021,
pp. 1839–1845. 4, 9
[55] S. Yang and J. Liu, “Time-series forecasting based on high-order
fuzzy cognitive maps and wavelet transform,” IEEE Transactions
on Fuzzy Systems , vol. 26, no. 6, pp. 3391–3402, 2018. 5
[56] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.
Gomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,”
inAdvances in neural information processing systems , 2017, pp. 5998–
6008. 6, 22
[57] B. Xia, C. Wong, Q. Peng, W. Yuan, and X. You, “Cscnet: Con-
textual semantic consistency network for trajectory prediction in
crowded spaces,” Pattern Recognition , p. 108552, 2022. 6, 9
[58] A. Lerner, Y. Chrysanthou, and D. Lischinski, “Crowds by exam-
ple,” Computer Graphics Forum , vol. 26, no. 3, pp. 655–664, 2007.
8
[59] P . Zhang, J. Xue, P . Zhang, N. Zheng, and W. Ouyang, “Social-
aware pedestrian trajectory prediction via states refinement lstm,”
IEEE transactions on pattern analysis and machine intelligence , vol. 44,
no. 5, pp. 2742–2759, 2022. 8, 9
[60] A. Robicquet, A. Sadeghian, A. Alahi, and S. Savarese, “Learn-
ing social etiquette: Human trajectory understanding in crowded
scenes,” in European conference on computer vision . Springer, 2016,
pp. 549–565. 8
[61] J. Liang, L. Jiang, and A. Hauptmann, “Simaug: Learning ro-
bust representations from simulation for trajectory prediction,” in
Proceedings of the European conference on computer vision (ECCV) ,
August 2020. 8, 9
[62] J. Liang, L. Jiang, K. Murphy, T. Yu, and A. Hauptmann, “The gar-
den of forking paths: Towards multi-future trajectory prediction,”
inProceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , 2020, pp. 10 508–10 518. 8, 9
[63] C. Ionescu, D. Papava, V . Olaru, and C. Sminchisescu, “Human3.
6m: Large scale datasets and predictive methods for 3d human
sensing in natural environments,” IEEE transactions on pattern
analysis and machine intelligence , vol. 36, no. 7, pp. 1325–1339, 2013.
8
[64] C. S. Catalin Ionescu, Fuxin Li, “Latent structured models for
human pose estimation,” in International Conference on Computer
Vision , 2011. 8
[65] R. Liang, Y. Li, X. Li, Y. Tang, J. Zhou, and W. Zou, “Temporal
pyramid network for pedestrian trajectory prediction with multi-
supervision,” vol. 35, no. 3, pp. 2029–2037, 2021. 9
[66] N. Shafiee, T. Padir, and E. Elhamifar, “Introvert: Human trajec-
tory prediction via conditional 3d attention,” in Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition ,
2021, pp. 16 815–16 825. 9
[67] B. Pang, T. Zhao, X. Xie, and Y. N. Wu, “Trajectory prediction with
latent belief energy-based model,” in Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition , 2021, pp.
11 814–11 824. 9
[68] S. Li, Y. Zhou, J. Yi, and J. Gall, “Spatial-temporal consistency
network for low-latency trajectory forecasting,” in Proceedings of
the IEEE/CVF International Conference on Computer Vision (ICCV) ,
October 2021, pp. 1940–1949. 9, 15
[69] T. Gu, G. Chen, J. Li, C. Lin, Y. Rao, J. Zhou, and J. Lu, “Stochastic
trajectory prediction via motion indeterminacy diffusion,” in Pro-
ceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , 2022, pp. 17 113–17 122. 9
[70] M. Meng, Z. Wu, T. Chen, X. Cai, X. Zhou, F. Yang, and D. Shen,
“Forecasting human trajectory from scene history,” Advances in
Neural Information Processing Systems , vol. 35, pp. 24 920–24 933,
2022. 9
[71] D. Wang, H. Liu, N. Wang, Y. Wang, H. Wang, and S. Mcloone,
“Seem: a sequence entropy energy-based model for pedestrian
trajectory all-then-one prediction,” IEEE transactions on pattern
analysis and machine intelligence , vol. 45, no. 1, pp. 1070–1086, 2023.
9
[72] F. Marchetti, F. Becattini, L. Seidenari, and A. D. Bimbo, “Mantra:
Memory augmented networks for multiple trajectory prediction,”
inProceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , 2020, pp. 7143–7152. 9
[73] J. Martinez, M. J. Black, and J. Romero, “On human motion
prediction using recurrent neural networks,” in Proceedings of theIEEE conference on computer vision and pattern recognition , 2017, pp.
2891–2900. 9
[74] M. Li, S. Chen, Y. Zhao, Y. Zhang, Y. Wang, and Q. Tian, “Dynamic
multiscale graph neural networks for 3d skeleton based human
motion prediction,” in Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition , 2020, pp. 214–223. 10
[75] L. Dang, Y. Nie, C. Long, Q. Zhang, and G. Li, “Msr-gcn: Multi-
scale residual graph convolution networks for human motion
prediction,” in Proceedings of the IEEE/CVF International Conference
on Computer Vision , 2021, pp. 11 467–11 476. 10
[76] T. Ma, Y. Nie, C. Long, Q. Zhang, and G. Li, “Progressively
generating better initial guesses towards next stages for high-
quality human motion prediction,” in Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition , 2022, pp.
6437–6446. 10
[77] M. Li, S. Chen, Z. Zhang, L. Xie, Q. Tian, and Y. Zhang, “Skeleton-
parted graph scattering networks for 3d human motion predic-
tion,” in European Conference on Computer Vision . Springer, 2022,
pp. 18–36. 10
[78] A. Monti, A. Bertugli, S. Calderara, and R. Cucchiara, “Dag-net:
Double attentive graph neural network for trajectory forecasting,”
in2020 25th International Conference on Pattern Recognition (ICPR) .
IEEE, 2021, pp. 2551–2558. 15
[79] J. Liang, L. Jiang, J. C. Niebles, A. G. Hauptmann, and L. Fei-
Fei, “Peeking into the future: Predicting future person activities
and locations in videos,” in Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition , 2019, pp. 5725–5734. 15
[80] Y. Sheng, D. Roberge, and H. H. Szu, “Optical wavelet transform,”
Optical Engineering , vol. 31, no. 9, pp. 1840–1845, 1992. 19
Beihao Xia received his Ph.D. degree in Huazhong University of Sci-
ence and Technology, Wuhan, China, in 2023. His research interests
include trajectory prediction, behavior analysis, and understanding.
Conghao Wong received the master’s degree from Huazhong Univer-
sity of Science and Technology, Wuhan, in 2022, where he is currently
pursuing the Ph.D. degree. His research interests include computer
vision and pattern recognition.
Duanquan Xu is currently an Associate Professor in Huazhong Univer-
sity of Science and Technology, Wuhan, China. He received his Ph.D.
degree from Huazhong University of Science and Technology in 2008.
His research interests include image processing, and computer vision.
Qinmu Peng is currently an Associate Professor in Huazhong Uni-
versity of Science and Technology, Wuhan, China. He received his
Ph.D. degree from the Department of Computer Science at Hong Kong
Baptist University in 2015. His research interests include medical image
processing, pattern recognition, machine learning, and computer vision.
Xinge You (Senior Member, IEEE) is currently a Professor in Huazhong
University of Science and Technology, Wuhan. He received his Ph.D.
degree from the Department of Computer Science, Hong Kong Baptist
University in 2004. His research have expounded in 200+ publications,
such as IEEE T -PAMI, T -IP , T -NNLS, T -CYB, CVPR, ECCV, ICCV. He
served/serves as an Associate Editor of the IEEE Transactions on
Cybernetics ,IEEE Transactions on Systems, Man, Cybernetics: Sys-
tems . His research interests include image processing, wavelet analysis,
pattern recognition, machine learning, and computer vision.JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 19
APPENDIX A
ADDITIONAL DISCUSSIONS ON THE HAAR TRANS -
FORM
The Advantages of Haar Transform in Time-Frequency
Analyses. According to Fourier expansion, a signal can be
expressed as the sum of a series of sine and cosine waves.
The Fourier transform of a function x(t)can be represented
as
X(k) =Z∞
−∞x(t)e−j2πktdt. (27)
However, a significant shortage of the Fourier spectrum
X(k)is that the time variable tis not directly or indirectly
included, which means that a sequence x(t)only has one
specific Fourier spectrum X(k), no matter how the time
variable tchanges. In other words, it has no time resolution ,
so we can not infer which frequency portions present a
specific time.
As the simplest variation of wavelets, the Haar transform
makes it easy to analyze the time-frequency joint repre-
sentation of signals. The main difference between wavelet
transforms and the Fourier transform is that wavelets are
localized in both time and frequency, whereas the standard
Fourier transform is only localized in frequency. Formally,
the continuous wavelet transform of a function x(t)at scale
a >0and translational value b∈Rcan be expressed as
Xw(a, b) =1√aZ∞
−∞x(t)¯ψt−b
a
dt, (28)
where ψ(t)is the continuous mother wavelet, and ¯ψ(t)
represents its complex conjugate. In Haar wavelet, we have
ψHaar(t) =

1,0≤t <1
2,
−1,1
2≤t <1,
0,Otherwise .(29)
(a)Time-Frequency Joint Representation. Compared to
the Fourier transform in Eq. 27, the Haar spectrum has
two variables, scale aand translation b. According to these
variables, the mother wavelet ψ(t)could generate a set of
basis functionsψ t−b
a	
, thus decomposing signals with
these series of functions (also called wavelets). Similar to
the Fourier transform, the scale arepresents frequencies.
Especially, the Fourier transform can be treated as a special
wavelet transform with the mother function
¯ψFT(t) =e−j2πt(30)
with scale a=1
kand translation b= 0 in Eq. 27. Differently,
the wavelet spectrum is located by both variables aandb,
where brepresents the “focused time”. In other words, we
can see the particular frequency response Xw(a, b=t0)at a
specific time t0from wavelet spectrums. This one property
is not available in the Fourier transform. It means that when
analyzing agents’ trajectories, especially considering the un-
certainty of agents’ behaviors, the wavelet spectrum could
better represent the changes in this spectral distribution over
time, especially in more complex dynamic scenarios. That
is the first reason we choose wavelet in V2-Net and E-V2-
Net in this manuscript.(b)Computational Efficiency. One of the other reasons
why we choose Haar transform from different wavelet
transforms is that it has a fairly simple calculation. Com-
paring Eq. 29 and Eq. 30, the Haar transform contains only
additions and no multiplications on x(t)in the computation
process when computing Eq. 28, rather than the complex
integrals (or sums for discrete cases)Rx(t)e−j2πktdt. In
addition, in trajectory prediction, trajectories are usually
not even-symmetric sequences, meaning that there must
be imaginary parts in their Fourier spectrums. It means
that additional channels are needed for neural networks to
handle the corresponding imaginary parts of the spectrum
(the additional channels are still needed even if they are
represented as amplitudes and phases), which leads to
an increase in the number of trainable parameters in the
network, also reduces the inference efficiency of the whole
prediction network. On the contrary, due to the simple
mother function (Eq. 29), calculating the Haar spectrum
requires only a simple addition of real numbers, thus saving
this unnecessary prediction network overhead.
Limitations of Haar Transform. Haar transform has its
limitations due to the properties of its mother function ψ(t).
One main limitation is that the Haar transform focuses more
on the high-frequency portions of the signal. It means that
a signal with more low-frequency portions may not be rep-
resented well by the Haar transform. In wavelet transforms,
thenth moment of a mother wavelet ψ(t)is defined as
Mn=Z
tnψ(t)dt. (31)
Simply, if there exists a positive integer psuch that M0=
M1=...=Mp−1= 0, then the mother wavelet ψ(t)hasp
Vanishing Moments . A mother function with more vanishing
moments means that the higher -frequency portions may take
up less energy in the corresponding transformed wavelet
spectrum. This conclusion can be intuitively explained with
a simple example. We can expand the Xw(a, b)(Eq. 28) into
the Taylor series at t= 0 with order nlike (the translation b
is set to 0 for a easier computation [80]):
Xw(a,0) =1√aZ
nX
p=0x(p)(0)tp
p!+O(n+ 1)
ψt
a
dt
(32)
=1√a
nX
p=0x(p)(0)Ztp
p!ψt
a
dt+O(n+ 1)
.
(33)
Considering the definition of Mkin Eq. 31, we have
Xw(a,0) =1√a
nX
p=0x(p)(0)
p!Mnap+1+O(n+ 1)
.(34)
Here, it can be seen that Xw(a,0)decays as fast as the first
non-zero termxn(0)
n!Mnan+1/2. The speed of convergence of
theXw(a,0)to zero with an increase of frequency variable
k=1
ais then determined by the first non-zero moment
Mnof the wavelet ψ(t). Thus, a wavelet ψ(t)with a higher
vanishing moment pmay lead to a higher decay rate for the
higher-frequency portions (higher k) of the correspondingJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 20
wavelet spectrum Xw(a=1
k, b), thus meaning a higher
percentage of energy distributed in the relatively lower
frequency portions in the entire spectrum.
However, the Haar mother wavelet ψHaar only has p= 1
vanishing moment, lower than most of the other wavelets,
thus leading to a slow decay with the increase of frequency
variable k=1
a. As a result, the lower-frequency portions
may occupy a relatively small amount of energy in the
Haar spectrum, making it more challenging to well charac-
terize signals x(t)with more lower-frequency components,
i.e., the smoother signals. In the trajectory prediction that
the manuscript focused on, the trajectories of the different
agents are the signals x(t)above. It means that smoother
trajectories, either because of the short observation time or
the smaller amplitudes of the trajectory changes, may lead
to a limited ability to encode with the Haar spectrum. Con-
sequently, the prediction performance of the corresponding
Haar transform model variations will also be limited.
Additional Experimental Validations. We have conducted
several ablation studies to validate the above advantages
and limitations of the Haar transform and its usefulness in
helping forecast heterogeneous trajectories, including the
quantitative validations of transforms and the validations
of model efficiencies. Here, we add more experimental
discussions to further verify our thoughts.
(a)Quantitative Validations of Transforms. We can
see from the ablation results from the main manuscript
that E-V2-Net-DFT and E-V2-Net-Haar variations perform
almost the same on 2D coordinate datasets. Specifically, the
Haar variation performs even worse than the DFT varia-
tion when forecasting 2D coordinates on SDD. However,
Haar variations outperform DFT variations on the more
challenging 2D bounding box and 3D skeleton datasets. We
infer from these results that the Haar variation performs
better at predicting datasets (scenarios) that contain more
complex time-frequency changes, like the more complex 2D
bounding box changes and significant changes in human
skeletons due to their pose changes.
Notably, Haar variations perform similarly to DFT ones
for 3D bounding box cases. This phenomenon is consistent
with the above-mentioned limitations of the Haar transform:
the trajectories in the nuScenes dataset appear smoother due
to the lower length of observations ( th= 4). In other words,
in the relatively short observation period, the possibility of
large changes in the trajectory will be significantly reduced,
considering these agents’ physical movement constraints.
The Haar transform has a vanishing moment of 1, which
means that the energy is distributed more in the high-
frequency portions in the Haar spectrum, making it difficult
to focus on the lower-frequency portions, leading to a poor
representation of the smoother trajectories.
We have also conducted another pair of ablation experi-
ments to validate our thoughts on how the sequence length
(or the “smoothness”) of the input sequence affects different
model variations’ performances. In Tab. 13, DFT and Haar
variations perform almost the same when th=tf= 4. The
Haar variation APP-A2 even performs a little worse than the
DFT variation APP-A2. However, by increasing the length
of the observed and predicted trajectories to th=tf= 10(by keeping the sample interval ∆t), the new Haar variation
APP-A4 exhibits better prediction results compared to the
corresponding DFT variation APP-A3, including 11.3% bet-
ter ADE and 13.1% FDE. This phenomenon aligns with our
results obtained on forecasting 3D human skeletons under
the same th=tf= 10 condition, where the Haar variation
outperforms DFT variation for about 13.2% MPJPE at 400ms,
thus validating the idea that the Haar transform above is
unsuitable for dealing with “smoother” trajectories.
From all these quantitative results, we can see that DFT
is more effective for simpler or smoother trajectories, but
Haar transform will be more effective for heterogeneous
trajectories with higher dimensionality or fast time-
frequency changing properties. These experiments also
demonstrate the different advantages of the different
transforms, thus supporting the choice of transforms for
different scenarios.
(b)Validations of Model Efficiencies. We mentioned in
the above theoretical analyses that the Haar transform has
better computation efficiency than DFT. We have conducted
several ablation variations of both V2-Net and E-V2-Net to
verify these properties in the main manuscript. Here, we
provide more efficiency-related experimental validations to
analyze how trajectory length, dimensionality (trajectory
types), and number (keypoints selection) affect the speed
and memory. Their results are reported in Tabs. 14 and 15. It
is easy to observe that the Haar variations are more efficient
(i.e., less inference time and memory) than DFT variations
in the same setting, e.g., comparing APP-A5 and APP-A6 or
APP-A31 and APP-A32.
Analyses of Inference Speed/Memory Influenced by
Trajectory Length. The trajectory length consists of two
parts: the observed trajectory length thand the predicted
trajectory length tf. Tab. 14 showes the results of inference
time and memory under different (th, tf)pairs under dif-
ferent bssettings, respectively. As shown in Tab. 14, when
bs= 1 , as the trajectory length increases, E-V2-Net and
V2-Net variations’ inference time and memory are similar,
comparing APP-A5, APP-A9, APP-A13 and APP-A17. As
the trajectory length increases, the inference time of models
maintains the same level, but the memory of E-V2-Net-DFT
has increased by up to 12.8%, comparing APP-A21, APP-
A17, APP-A25, and APP-A29. Moreover, comparing APP-
A13 ( (th= 8, tf= 16) ) and APP-A21 ( (th= 4, tf= 20) )
under the same trajectory length L= 24 , the inference
time is similar but APP-A23 takes up about 1%-3% more
memory than APP-A21 in trajectory form of 2D coordinate,
2D bounding box and 3D bounding box. In contrast, APP-
A21 takes up about 6.0% more memory than APP-A23 in
3D human skeleton prediction. As a result, the trajectory
length has a certain impact on model efficiency, especially
the longer the trajectory, the more obvious the impact.
Analyses of Inference Speed/Memory Influenced by
Trajectory Types. As shown in Tab. 14, as the trajectory
dimensionality rises, the inference time and memory also
increase. Taking APP-A13 as an example, comparing with
variation M= 2 , the inference time and memory of
DFT variations M= 4 ,M= 6 , and M= 51 increase
24.1%/1.3%, 31.0%/1.3%, and 286%/10.0%, respectively.
Correspondingly, Haar variations APP-A14 also increaseJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 21
TABLE 13
Ablation Studies on validating the modeling capacity of different transforms under different prediction lengths (3D bounding box cases, best-of-20 ).
No. Model T thtf ADE↓ FDE↓ AIoU↑ FIoU↑
APP-A1 E-V2-Net DFT 4 4 0.203 0.291 0.766 0.691
APP-A2 E-V2-Net Haar 4 4 0.209 0.301 0.764 0.688
APP-A3 E-V2-Net DFT 10 10 0.730 1.368 0.565 0.403
APP-A4 E-V2-Net Haar 10 10 0.647 1.202 0.575 0.407
TABLE 14
Inference time and memory of E-V2-Net and V2-Net (DFT and Haar, with linear S2) at different trajectory lengths (frames) on one Apple M1 chip.
b,L,th, and tfdenote “batch size”, “trajectory length”, “the observed trajectory length”, and “the predicted trajectory length”, respectively. Results
are obtained by generating k= 20 trajectories for each prediction sample. The reported results are shown as “Time (ms)/Para. (MB)”.
No. Model b,L,th→tf co(M= 2) bb(M= 4) 3dbb (M= 6) ske(M= 51)
APP-A5 EV-DFT
1, 16, 8→829 ms/8.0M 33 ms/8.0M 37 ms/8.0M 111 ms/8.4M
APP-A6 EV-Haar 26 ms/7.7M 27 ms/7.8M 27 ms/7.8M 27 ms/8.0M
APP-A7 V-DFT 27 ms/7.4M 29 ms/7.4M 36 ms/7.4M 111 ms/7.9M
APP-A8 V-Haar 25 ms/7.3M 25 ms/7.3M 26 ms/7.3M 26 ms/7.6M
APP-A9 EV-DFT
1, 20, 8→1229 ms/8.0M 34 ms/8.0M 37 ms/8.1M 112 ms/8.6M
APP-A10 EV-Haar 27 ms/7.7M 27 ms/7.8M 28 ms/7.8M 29 ms/8.1M
APP-A11 V-DFT 30 ms/7.4M 30 ms/7.4M 37 ms/7.5M 112 ms/8.1M
APP-A12 V-Haar 26 ms/7.3M 27 ms/7.3M 27 ms/7.3M 28 ms/7.7M
APP-A13 EV-DFT
1, 24, 8→1629 ms/8.0M 36 ms/8.1M 38 ms/8.1M 112 ms/8.8M
APP-A14 EV-Haar 26 ms/7.7M 27 ms/7.8M 27 ms/7.8M 28 ms/8.3M
APP-A15 V-DFT 28 ms/7.4M 32 ms/7.4M 38 ms/7.5M 112 ms/8.3M
APP-A16 V-Haar 26 ms/7.3M 26 ms/7.3M 27 ms/7.4M 28 ms/7.8M
APP-A17 EV-DFT
1, 28, 8→2029 ms/8.0M 36 ms/8.1M 38 ms/8.1M 113 ms/9.0M
APP-A18 EV-Haar 26 ms/7.8M 27 ms/7.8M 28 ms/7.8M 28 ms/8.4M
APP-A19 V-DFT 28 ms/7.4M 33 ms/7.5M 38 ms/7.5M 112 ms/8.5M
APP-A20 V-Haar 26 ms/7.3M 27 ms/7.4M 27 ms/7.4M 28 ms/8.0M
APP-A21 EV-DFT
1, 24, 4→2029 ms/7.8M 35 ms/7.8M 38 ms/7.9M 112 ms/8.9M
APP-A22 EV-Haar 26 ms/7.6M 27 ms/7.7M 28 ms/7.7M 28 ms/8.3M
APP-A23 V-DFT 27 ms/7.4M 33 ms/7.4M 38 ms/7.5M 109 ms/8.5M
APP-A24 V-Haar 26 ms/7.3M 27 ms/7.4M 28 ms/7.4M 28 ms/8.0M
APP-A25 EV-DFT
1, 30, 10→2029 ms/8.2M 36 ms/8.2M 39 ms/8.3M 113 ms/9.2M
APP-A26 EV-Haar 26 ms/7.9M 27 ms/7.9M 28 ms/7.9M 28 ms/8.4M
APP-A27 V-DFT 29 ms/7.5M 33 ms/7.6M 39 ms/7.6M 112 ms/8.6M
APP-A28 V-Haar 26 ms/7.4M 27 ms/7.4M 28 ms/7.4M 29 ms/8.0M
APP-A29 EV-DFT
1, 40, 20→2029 ms/8.8M 36 ms/8.9M 40 ms/8.9M 113 ms/9.7M
APP-A30 EV-Haar 26 ms/8.2M 27 ms/8.2M 28 ms/8.2M 29 ms/8.7M
APP-A31 V-DFT 29 ms/7.7M 35 ms/7.7M 40 ms/7.7M 112 ms/8.8M
APP-A32 V-Haar 26 ms/7.5M 27 ms/7.5M 27 ms/7.5M 29 ms/8.1M
TABLE 15
Inference time and memory of E-V2-Net and V2-Net (DFT and Haar, with linear S2) at different keypoints when (th= 8, tf= 20) on one Apple M1
chip. b,Nkeydenote “batch size”, and “the number of keypoints”, respectively. Results are obtained by generating k= 20 trajectories for each
prediction sample. The reported results are shown as “Time (ms)/Para. (MB)”.
No. Model bNkey co(M= 2) bb(M= 4) 3dbb (M= 6) ske(M= 51)
APP-A33 EV-DFT
1 228 ms/8.0M 33 ms/8.0M 35 ms/8.0M 109 ms/8.0M
APP-A34 EV-Haar 27 ms/7.7M 27 ms/7.7M 27 ms/7.7M 28 ms/7.9M
APP-A35 V-DFT 27 ms/7.4M 31 ms/7.4M 34 ms/7.4M 108 ms/7.5M
APP-A36 V-Haar 25 ms/7.3M 25 ms/7.3M 26 ms/7.3M 27 ms/7.4M
APP-A37 EV-DFT
1 429 ms/8.0M 36 ms/8.0M 38 ms/8.0M 113 ms/8.2M
APP-A38 EV-Haar 27 ms/7.7M 27 ms/7.7M 28 ms/7.7M 28 ms/7.9M
APP-A39 V-DFT 27 ms/7.4M 33 ms/7.4M 38 ms/7.4M 112 ms/7.7M
APP-A40 V-Haar 26 ms/7.4M 27 ms/7.4M 28 ms/7.4M 28 ms/7.5MJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 22
3.8%/0 ( M= 4), 3.8%/0 ( M= 6), 7.7%/7.8% ( M= 51 ) un-
der the same trajectory dimensionality settings, compared
with variation M= 2 . This phenomenon indicates that
Haar variations have better computation ability when the
trajectory dimensionality increases. According to the re-
sults, trajectory types significantly impact model efficiency,
especially the higher the trajectory dimension, the more
significant the impact.
Analyses of Inference Speed/Memory Influenced by
the Number of Keypoints. As shown in Tab. 15, com-
paring APP-A33 and APP-A37, the memory of models in
2D coordinate, 2D bounding box, and 3D bounding box
maintain the same level, but the inference time and memory
of variation Nkey= 4 increase 2.5% in 3D human skeleton
than variation Nkey= 2 . Moreover, the inference time
of variation Nkey= 4 increase 3.6%, 9.0%, 8.6%, 3.7%,
respectively,than variation Nkey= 2. In short, the number
of keypoints is limitedly affected by the trajectory length.
From the above efficiency comparisons, we can further
verify the advantages of the Haar transform to the DFT. In
particular, the Haar variations show better computational
speed and require less memory for high-dimensional het-
erogeneous trajectories. Considering the quantitative per-
formance of the Haar variations discussed above, we can
conclude that the Haar transform is more effective in these
high dimensional trajectory predictions relative to the DFT.
That is why we choose Haar transform as an alternative in
this manuscript.
APPENDIX B
TRANSFORMER DETAILS
We employ the Transformer [56] as the backbone to en-
code trajectory spectrums and the scene context in the two
proposed sub-networks. The Transformer used in the E-V2-
Net has two main parts, the Transformer Encoder and the
Transformer Decoder, both of which are made up of several
attention layers.
Attention Layers. Multi-Head Attention operations are ap-
plied in each of the attention layers. Following [56], each
layer’s multi-head dot product attention with Hheads is
calculated as:
Attention (q,k,v) =softmax 
qk⊤
√
d!
v, (35)
MultiHead (q,k,v) =
fc
concat ({Attention i(q,k,v)}H
i=1)
.(36)
Here, fc ()denotes one fully connected layer that concate-
nates all heads’ outputs. Query matrix q, key matrix k, and
value matrix vare the three layer inputs. Each attention
layer also contains an MLP (denoted as MLP a) to extract the
attention features further. It contains two fully connected
layers. ReLU activations are applied in the first layer. For-
mally, we have output feature foof this layer:
fo=ATT(q,k,v) =MLP a(MultiHead (q,k,v)). (37)
Transformer Encoder. The transformer encoder comprisesseveral encoder layers, and each encoder layer contains
an attention layer and an encoder MLP (MLP e). Residual
connections and normalization operations are applied to
prevent the network from overfitting. Let h(l+1)denote the
output of l-th encoder layer, and h(0)denote the encoder’s
initial input. For l-th encoder layer, the calculation of the
layer output h(l+1)can be written as:
a(l)=ATT(h(l),h(l),h(l)) +h(l),
a(l)
n=Normalization (a(l)),
c(l)=MLP e(a(l)
n) +a(l)
n,
h(l+1)=Normalization (c(l)).(38)
Transformer Decoder. Similar to the Transformer encoder,
the Transformer decoder comprises several decoder layers,
and each is stacked with two different attention layers. The
first attention layer in the Transformer decoder focuses on
the essential parts in the Transformer encoder’s outputs he
queried by the decoder’s input X. The second layer is the
same self-attention layer as in the encoder. Similar to Eq. 38,
we have the decoder layer’s output feature h(l+1):
a(l)=ATT(h(l),h(l),h(l)) +h(l),
a(l)
n=Normalization (a(l)),
a(l)
2=ATT(he,h(l),h(l)) +h(l),
a(l)
2n=Normalization (a(l)
2)
c(l)=MLP d(a(l)
2n) +a(l)
2n,
h(l+1)=Normalization (c(l)).(39)
Positional Encoding. Before feeding agents representations
or trajectory spectrums into the Transformer, we add the
positional coding to inform the relative position of each
timestep or frequency portion in the sequential inputs. The
position coding ft
eat step t(1≤t≤th)is obtained by:
ft
e= ft
e0, ..., ft
ei, ..., ft
ed−1∈Rd,
where ft
ei=

sin
t/10000d/i
, i is even ;
cos
t/10000d/(i−1)
, i is odd .(40)
Then, we have the positional coding matrix fethat describes
thsteps of sequences:
fe= (f1
e,f2
e, ...,fthe)⊤∈Rth×d. (41)
The final Transformer input XTis the addition of the
original sequential input Xand the positional coding matrix
fe. Formally,
XT=X+fe∈Rth×d. (42)
Layer Configurations. We employ L= 4 layers of encoder-
decoder structure with H= 8 attention heads in each
Transformer-based sub-networks. The MLP eand the MLP d
have the same shape. Both of them consist of two fully
connected layers. The first layer has 512 output units with
the ReLU activation, and the second layer has 128 but does
not use any activations. The output dimensions of fully
connected layers used in multi-head attention layers are set
tod= 128.JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 23
Difference between Dimension-wise Interactions and Po-
sitional Encoding. Transformers focus more on modeling
the relations of different moments (different steps) in the se-
quence. They use positional encoding to locate the position
of different time steps in the temporal sequence and then
adopt the attention mechanism to obtain the similarity of in-
formation in different time steps. It means that the positions
of some moments will be paid with more attention among
all moments in the temporal sequence in the Transformers.
However, the dimensional-wise interaction this
manuscript studied refers to the interaction within different
trajectory-dimensions at a certain time (frequency) step. We
take a simple example to explain the difference between the
Transformer and the dimension-wise interaction in Fig. 13
for easier understanding. Suppose there are three time
steps t∈ {1,2,3}, and four trajectory dimensions ( M= 4),
i.e., we have the trajectory matrix X∈R3×4. Thus, the
four trajectory-dimensions focused in this manuscript are
{X:,1,X:,2,X:,3,X:,4}, where each trajectory-dimension
X:,m∈R3, m∈ {1,2,3,4}.
When modeling the trajectory Xin Transformers, it will
be applied the positional encoding to locate the relative po-
sitions of different temporal moments t(the corresponding
trajectory is Xt∈R4), and then the attention mechanism
will be applied to obtain the similarity of information in
these three time steps X1,X2,X3. Simply, it can be treated
as a matrix with three rows and three columns to indicate
the relations between these steps. Then, the most attentive
steps (like Xt0) will be specifically taken into account when
encoding the trajectory X.
However, the bilinear structure aims to model the
dimension-wise interactions among the four trajectory-
dimensions {X:,1,X:,2,X:,3,X:,4}. Correspondingly, we
aim at building connections to describe the relations, for
example it can be simply represented by a matrix with four
rows and four columns. Finally, it aims at considering the
relations between different trajectory-dimensions, for exam-
ple, which dimensionality X:,m0would affect the forecasted
ˆX:,mthe most.
Thus, as shown in Fig. 13, the most significant dif-
ference between the dimension-wise interaction and the
positional encoding in Transformers is that they can con-
sider different “aspects” of the input sequence. Transformer
focuses on the relations between different time steps, and
the bilinear structure focuses on the interactions between
all trajectory-dimensions. In the above example (trajectory
X∈R3×4), dimension-wise interactions concern more
from the last tensor-dimension {X:,1,X:,2,X:,3,X:,4}while
the other one focuses from the first (temporal) tensor-
dimension {X1,X2,X3}. There are no such structures
in Transformers that could directly describe the relations
among trajectory-dimensions {X:,1,X:,2,X:,3,X:,4}. Thus,
we use the bilinear structure to represent the above re-
lations, i.e., dimension-wise interactions, upon the Trans-
former backbone to further capture their time-frequency
properties simultaneously.
𝐗!,!𝐗!,#𝐗!,$𝐗!,%𝐗#,!𝐗#,#𝐗#,$𝐗#,%𝐗$,!𝐗#,#𝐗#,$𝐗#,%𝑡=1
Dimension-wise Interaction @𝑡=1Attention Weightsin Transformers(Temporal Sequences)
𝑡=2
Dimension-wise Interaction @𝑡=2𝑡=3
Dimension-wise Interaction @𝑡=3123123
Dimension-Wise Interactions(Among Spatial Trajectory-Dimensions)WhichTemporal nodes are more important
WhichDimensionnodes are more importantFig. 13. Different roles of the Transformer and the dimension-wise
interaction, taking a time-sequence with three times steps t∈ {1,2,3}
and four trajectory dimensions ( M= 4) as an example.
APPENDIX C
LINEAR LEAST SQUARES TRAJECTORY PREDIC -
TION
The linear least squares trajectory prediction method aims
to minimize the mean square error between the predicted
and agents’ groundtruth trajectories. When predicting, we
perform a separate least squares operation for each dimen-
sion of the M-dimensional observed trajectory X. Simply,
we want to find the xm= (bm, wm)⊤∈R2(1≤m≤M),
such that
ˆY= (ˆY1,ˆY2, ...,ˆYm, ...,ˆYM)∈Rtf×M,
where ˆYm=Afxm=
1th+ 1
1th+ 2
... ...
1th+tf
bm
wm
.(43)
For one of agents’ observed M-dimensional trajectory
X∈Rth×M, we have the trajectory slice on the m-th
dimension
Xm= (rm1, rm2, ..., r mth)⊤. (44)
Suppose we have a coefficient matrix Ah, where
Ah=1 1 1 ...1
1 2 3 ... t h⊤
. (45)
We aim to find a xm∈R2, such that the mean square
∥Ahxm−Xm∥2
2could reach its minimum value. Under this
condition, we have
xm= (A⊤
hAh)−1A⊤
hXm. (46)JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 24
133 134 134 130 131 180 188 188 188 190 134 135 135 134 132 156 188 188 188 188 135 136 135 134 135 134 184 186 188 188 135 135 137 135 134 133 164 185 185 186 145 137 138 137 134 135 135 180 184 185 155 137 136 138 138 138 135 134 172 181 156 155 140 141 136 135 134 133 136 147 156 154 152 137 135 132 133 132 134 132 160 159 157 153 133 132 131 131 131 132 158 157 155 155 139 131 132 131 129 131 𝑡=1𝑡=2𝑡=3𝑡=4𝑡=5𝑡=6𝑚=1top-left-x101112141618𝑚=2top-left-y654321𝑚=3bottom-right-x141516171921𝑚=4bottom-right-y876543
ImagesTrajectories(a)(b)
Fig. 14. Matrices views of a trajectory (2D bounding box) and an image.
Then, we have the predicted m-th dimension trajectory
ˆYm=Afxm. (47)
The final M-dimensional predicted trajectory ˆYis ob-
tained by stacking all results. Formally,
ˆY=Af(x1,x2, ...,xM). (48)
APPENDIX D
2D DFT V.S. BILINEAR STRUCTURE
We apply different transforms on each dimension of the
trajectory to obtain the corresponding trajectory spectrums
either in V2-Net or the enhanced E-V2-Net. Moreover, con-
sidering that one of our main contributions is to establish
connections between trajectories (or spectrums) of different
dimensions, a more natural idea might be to apply some
2D transform directly to these trajectories. However, it ap-
pears to be less effective from both theoretical analyses and
experimental results. In this section, we will discuss the
discrepancy between the 2D transform and the proposed
bilinear structure in describing the two factors, including
the frequency response of the trajectory and the dimension-
wise interactions, from different perspectives, taking DFT as
an example.
DFT on Different Directions in Trajectories. The 2D DFT
can be decomposed into two consecutive 1D DFTs per-
formed in different directions of the target 2D matrix. The
M-dimensional trajectory X∈RN×Mis also a 2D matrix
similar to 2D grayscale images. Although the 2D DFT and its
variations have achieved impressive results in tasks related
to image processing, they might not be directly applied to
trajectories. We will analyze this problem specifically by
focusing on the different directions of the transforms in the
trajectory.
Fig. 14 shows an M= 4 2D bounding box trajectory
and an image with the matrix view. As shown in Fig. 14
(b), whether the image is sliced horizontally or vertically,
the resulting vector could reflect the change in grayscale
values in a particular direction. Therefore, when performing
the 2D transform, the first 1D transform will extract the
frequency response in a specific direction, while the second
1D transform will fuse it with the frequency response in the
vertical direction.
In contrast, different slice directions of the trajectory
may lead to different meanings. If the trajectories are sliced
according to the time dimension, then four 1D time series
will be obtained as shown in Fig. 14 (a). Applying 1D trans-
forms to these four sequences, we can obtain four trajectory
spectrums that could describe agents’ frequency responses
V2-Net (2D DFT)E-V2-Net (DFT,Bilinear structure)
Fig. 15. Visualized comparisons of 2D DFT bilinear structure.
and thus describe their motions from the global plannings
and interaction details at different scales. However, if the
trajectory is sliced from the dimensional direction, then N
(6 in the figure) 4-dimensional vectors will be obtained.
These vectors contain information about agents’ locations
and postures at a particular moment. In addition, the fo-
cused dimension-wise interactions are also contained in
these vectors. However, it should be noted that we are more
interested in the relationships between the data in these
vectors, i.e., the “edges” between the different data. If a 1D
transform is applied to these vectors, the resulting spectrum
may hardly have a clear physical meaning, because the tem-
poral or spatial adjacencies of these points are not reflected
in these 4-dimensional vectors.
For example, suppose we want to apply the 1D DFT
on the 4-dimensional (2D bounding box) vector x=
(xl, yl, xr, yr)⊤. Simply, we have:
X=DFT[x] =
1 1 1 1
1−j−1j
1−1 1 −1
1j−1−j

xl
yl
xr
yr

=
xl+yl+xr+yr
xl−xr−j(yl−yr)
xl−yl+xr−yr
xl−xr+j(yl−yr)
.(49)
Accordingly, we have its fundamental frequency portion
X[0] = xl+yl+xr+yrand the high-frequency portion
X[2] = xl−yl+xr−yr. However, since the four po-
sitions {xl, yl, xr, yr}do not have specific time-dependent
or space-dependent like time-sequences and images, these
frequency components may hardly reflect the specific fre-
quency response. For example, the fundamental frequencies
can represent their average value for a time series, yet
the values obtained by directly summing the 4 position
coordinates of the 2 points of the 2D bounding box would
be uninterpretable. In other words, each element in this
4-dimensional vector is relatively independent, and their
connection relationships are more like a graph rather than
a sequence where an order is required.
Quantitative Analyses. To verify our thoughts, we perform
ablation experiments on SDD and nuScenes to compare the
effects of 2D DFT and the bilinear structure quantitatively.
As shown in Tab. 16, the results of APP-B2 and APP-B3 (or
APP-B5 and APP-B6) show that 2D DFT does not improve
quantitative trajectory prediction performance as effectively
as bilinear structures. On the contrary, in the more complexJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 25
TABLE 16
Validation of 2D DFT and bilinear structures with best-of-20 on SDD (2D bounding box) the nuScenes (3D bounding box).
No. Model Type T Nkey BS Dataset ADE↓ FDE↓ AIoU↑ FIoU↑
APP-B1 V2-Net bb DFT 3 ×SDD
(2D bounding box)6.78 10.73 0.717 0.601
APP-B2 V2-Net bb 2D DFT 3 × 6.74 10.84 0.723 0.602
APP-B3 E-V2-Net bb DFT 3 ✓ 6.62 10.57 0.725 0.604
APP-B4 V2-Net 3dbb DFT 2 ×nuScenes
(3D bounding box)0.229 0.335 0.747 0.666
APP-B5 V2-Net 3dbb 2D DFT 2 × 0.234 0.341 0.739 0.656
APP-B6 E-V2-Net 3dbb DFT 2 ✓ 0.210 0.300 0.762 0.688
3D bounding boxes ( M= 6 ) prediction, using 2D DFT
instead degrades the prediction performance compared to
1D DFT when no bilinear structures are used. These ex-
perimental results validate our thoughts of not using 2D
transforms but bilinear structures.
Qualitative Analyses. We visualize the prediction results of
different models under the effect of 2D DFT and bilinear
structure qualitatively. As shown in Fig. 15, the V2-Net (2D
DFT) performs not as well as E-V2-Net in both the predic-
tion of agent motions and the interactions within the bound-
ing box. In detail, predictions given by V2-Net (2D DFT)
capture fewer path possibilities in the top left prediction
scenario. In addition, some predicted trajectories are with
less smoothness and naturalness. For example, the predic-
tion in color #DF6091 to the left of the bottom left prediction
scene gives a turn with a large angle to observation, which
could not be physical-acceptable in the actual scenario. In
contrast, predictions given by E-V2-Net have not shown
similar results in this scenario. On the other hand, as shown
in the traffic circle prediction scenario on the right, the shape
of the bounding box is not well maintained in V2-Net’s
predictions, such as the prediction in color #93C3CA to turn
right to across the street.
APPENDIX E
ADDITIONAL VALIDATIONS OF SOCIAL INTERA -
TIONS
When forecasting heterogeneous trajectories, interactions
are more complex, including social interactions and
dimension-wise interactions. To further validate the gener-
alizability, we conduct a series of experiments to explore the
contributions of social and dimension-wise interactions to
the predicted trajectories.
When forecasting agent- i’s position ˆpm
t+1at the next time
stept+ 1, the prediction process can be represented as:
ˆpm
t+1= Net ( pm
t, Isoc, Idwi). (50)
Here, pi
t= p1
t, p2
t, ..., pM
t⊤∈RMdenotes agent-
i’s positions during the observion period. Isoc =
I
pi
t,{pj
t}j̸=i
indicates the social interaction. Idwi=
I p1
t, p2
t, ..., pm
t, ..., pM
t
indicates the dimension-wise inter-
action. Thus, when forecasting trajectories, two extra factors,
the social interaction and the dimension-wise interaction,
are considered in the proposed methods simultaneously.
To analyze the respective contributions of the social
interaction and the dimension-wise interaction to the predic-
tion performance, we take the E-V2-Net-DFT as an exampleto conduct a series of experiments on heterogeneous tra-
jectories (2D coordinates and 2D and 3D bounding boxes).
Specifically, the corresponding contributions of dimension-
wise interactions Idwi(donate as Cdwi) and contributions of
social interactions Isoc(donate as Csoc) to E-V2-Net-DFT’s
prediction performance are obtained by calculating the ratio
of the sum of squares of dimension-wise interactions fea-
turesfdwiand social interactions features fc. Formally,
Csoc=f⊤
cfc
f⊤cfc+f⊤
dwifdwi, (51)
Cdwi= 1−Csoc. (52)
Notably, this is a simple metric to measure the contributions
of social interactions and dimension-wise interactions to the
prediction network, and it is only used for rough analyses.
We conduct a series of corresponding quantitative ex-
periments to analyze the factor that the prediction network
depends more on when forecasting heterogeneous trajecto-
ries, like trajectory dimensionality or other factors. The hu-
man skeleton dataset Human3.6M contains nothing about
social interactions among subjects. Thus, we only conduct
experiments to verify the contributions of these factors on
2D coordinate, 2D bounding box, and 3D bounding box
datasets. Their results are shown in Tab. 17. We can see that
as the dimension of a frame of trajectory vector Mincreases,
the contribution of dimension-wise interactions increases.
When forecasting 3D bounding boxes ( M= 6), dimension-
wise interactions contribute up to 83%. In other words, with
the increase of trajectory dimensionality, the dimension-
wise interaction may play a more significant role than social
interactions when forecasting heterogeneous trajectories. Es-
pecially comparing APP-C1 and APP-C2, which are both
tested on SDD, it shows that as trajectory dimensionality M
increases, the dimension-wise interaction contributes more,
although there are no social interaction differences and data
differences (they are both SDD).
TABLE 17
Quantitative results of contributions of the social interaction and the
dimension-wise interaction to the prediction performance in
E-V2-Net on heterogeneous trajectories. M,Idwi, and Isocdenote the
dimension of a frame of trajectory vector, dimension-wise interaction,
and social interactions, respectively.
No. Dataset Type M Idwi(%) Isoc(%)
APP-C1 SDD (co) co 2 61 39
APP-C2 SDD (bb) bb 4 67 33
APP-C3 nuScenes 3dbb 6 83 17