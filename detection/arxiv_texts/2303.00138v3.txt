Texture
-
Based
 
Input Feature Selection
 
for Action 
Recognition
 
Yalong Jiang
 
 
 
Abstract
—
The performance of video action recognition has 
been significantly boosted by 
using
 
motion representations 
within
 
a two
-
stream
 
Convolutional Neural Network 
(
CNN
)
 
architecture
.
 
However, 
there are a few
 
challenging 
problems
 
in 
action recognition 
in real scenarios, 
e.g.
,
 
the 
variations in 
viewpoint
s
 
and
 
pose
s
, and the 
changes in backgrounds.
 
The 
domain 
discrepancy
 
between 
the 
training data and 
the 
test data 
causes the
 
performance drop.
 
To 
improve the 
model
 
robustness, 
we 
propose
 
a novel method
 
to determine the
 
task
-
irrelevant
 
c
ont
ent
 
in inputs which 
increases the domain 
discrepancy
.
 
The 
method is based on a
 
human parsing model (HP 
m
odel) 
which
 
jointly
 
conduct
s
 
dense 
correspondence
 
labelling
 
and
 
semantic 
part segmentation.
 
The 
prediction
s
 
from
 
the
 
HP 
m
odel
 
also
 
function
 
as
 
re
-
rendering t
he
 
human regions
 
in 
each video using 
the
 
same
 
set of
 
textures
 
to make humans appearances in all 
classes be the same
.
 
A
 
revised
 
dataset
 
is
 
generated for training 
and testing and
 
make
s
 
the
 
action recognition
 
model
 
exhibit 
invariance
 
to the irrelevant 
c
on
t
ent in 
the 
inputs
.
 
Moreover,
 
the 
predictions from the HP 
m
odel are used to enrich 
the inputs to 
the 
AR model
 
during both training and testing
. Experimental 
results show that 
our
 
proposed 
model
 
is 
superior
 
to existing 
models for action recognition
 
on the HMDB
-
51 dataset
 
and the 
Penn Action 
d
ataset
.
 
Index Terms
—
Action Recognition, 
Dense Correspondence 
Labelling, Domain Discrepancy
, 
Task
-
irrelevant Content
.
 
I.
 
I
NTRODUCTION
 
c
t
ion
 
recognition 
has achieved an increasing popularity 
in the field of computer vision
 
due to its 
potential 
applications in person identification, behaviour analysis
, 
surveillance
 
and 
recommendation systems. 
Hand
-
designed 
features 
such as 
improved Dense Trajectory
 
(
iDT
)
 
[1]
, 
Dense 
Trajectory
 
(
DT
)
 
[2]
, 
Trajectory
-
Pooled 
D
eep
-
Convolutional 
D
escriptors (
TDD
)
 
[3]
 
and deep learning
-
based models 
[4]
 
[5]
 
[6]
 
[7]
 
[8]
 
have contributed to the considerable progress 
in video action recognition.
 
The first type of 
models
 
consider
s
 
action recognition as a 
classification  task  and 
conduct  frame
-
wise  predictions 
before  aggregating
 
the  resul
t
s  o
f
 
different  frames
.
 
[9]
 
samples one image from each video
. 
Temporal Segment 
Networks (TSN) 
p
roposed in 
[8]
 
samples 3 to 7 images per 
video before averaging the 
corresponding predictions.
 
Long 
Short
-
Term Memory (
LSTM
)
 
has
 
also 
been 
widely applied
 
to
 
action recognition 
[10]
 
[11]
 
[12]
 
[13]
. For example,
 
[11]
 
proposed to sum up frame
-
level 
predictions
 
by 
connecting 
LSTM
 
cells to the output of 
underlying
 
CNNs 
which conduct
 
frame
-
level  action  recognition
.
 
However, 
a 
partial 
observation 
of  videos  may  easily  result  in 
a
 
loss  of 
discriminative features.
 
 
The  second  type  of 
existing 
work
 
expl
ores
 
a
 
distinguishing ingredient 
f
or
 
action
 
recognition
, i.
e. 
the
 
temporal  features
. 
Comprehensive 
types  of 
temporal 
information 
have
 
been explored 
in
 
[14]
 
and
 
[15]
 
which 
obtain
 
long
-
range temporal information with 3D CNNs.
 
However,
 
the performance of
 
3D CNNs
 
and 
3DHOG
 
[16]
 
are
 
inferior to that of 
optical flows.
 
DTPP proposed in
 
[17]
 
utilized
 
video
-
level representation
s
 
i
n 
multiple temporal 
scales
 
to fully 
represent motion dynamics
 
and 
generate 
predictions
 
based
 
on both 
RGB images and optical flows
.
 
Additionally, 
[18]
 
introduced  a 
new  type  of  feature 
representation based on the 
spatial and temporal gradients 
of 
feature maps
.
 
T
he representation is orthogonal to 
optical 
flow and is computational
ly more
 
efficient.
 
However, 
none of the above
-
mentioned methods have 
paid attention to 
domain 
discrepancy
. The first
 
type of
 
domain 
discrepancy 
exists
 
between 
the 
training 
set
 
and 
the 
test 
set
 
while the second gap is 
among different datasets
. 
The 
former
 
hinders 
the performance of generalization and the 
latter requires one 
model to be trained for each dataset.
 
For 
instance, t
he most 
widely used dataset
s
 
UCF
-
101 
[19]
 
and 
HMDB
-
51 
[20]
 
only 
have
 
about 13
0
 
videos per class which 
are taken under constrained environments. 
The duration
s
 
of 
videos as well as 
the variations covered by 
the 
videos in each 
class
 
are much less than 
those in Youtube
-
8M 
[21]
 
whi
ch is
 
less constrained and better p
reserves complex real
-
world 
conditions.
 
The
refore
,
 
the
 
learned representations are 
difficult to generalize well.
 
However, datasets such as 
Youtube
-
8M 
[21]
 
require 
a huge amount of computational 
 
AR
G
B
.
.
.
 
.
.
.
P
S
P
N
e
t
-
5
0
 
b
a
s
e
d
 
n
e
t
w
o
r
k
1
-
c
l
a
s
s
 
M
a
s
k
C
o
n
c
a
t
e
n
a
t
i
o
n
D
e
n
s
e
 
c
o
r
r
e
s
p
o
n
d
e
n
c
e
 
l
a
b
e
l
l
i
n
g
 
b
r
a
n
c
h
H
u
m
a
n
 
p
a
r
s
i
n
g
 
b
r
a
n
c
h
C
o
n
c
a
t
e
n
a
t
i
o
n
U
V
 
C
o
o
r
d
i
n
a
t
e
s
M
a
p
p
i
n
g
 
t
o
 
t
e
x
t
u
r
e
s
 
o
f
 
2
4
 
p
a
r
t
s
S
t
a
g
e
1
S
t
a
g
e
2
D
i
l
a
t
e
d
 
C
o
n
v
o
l
u
t
i
o
n
s
.
.
.
 
.
.
.
P
S
P
N
e
t
-
5
0
 
b
a
s
e
d
 
n
e
t
w
o
r
k
(a)
 
.
.
.
 
.
.
.
T
e
x
t
u
r
e
 
E
x
t
r
a
c
t
i
o
n
M
a
p
p
i
n
g
.
.
.
 
.
.
.
(b)
 
Fig. 1. 
Our p
roposed way 
of 
data augmentation
 
based on texture transferring.
 
(
a
)
 
The model for
 
mapping
 
the 
pixels on 
a 
human 
image
 
to
 
the 
textures describing
 
24
 
semantic
 
person
 
parts. The UV coordinates
 
introduced in 
[22]
 
are used for 
dense correspondence labelling
. 
The model is composed of 
two
 
stages
.
 
The first 
stage 
performs
 
saliency prediction while the second 
stage jointly predicts 
part segmentation
 
masks
 
and 
UV coordinates.
 
(b) 
One single
 
frame
 
can
 
only
 
pro
vide
 
a 
partial view
 
about the 24 parts. 
Therefore
, we fill in the texture
 
detail
s using
 
different
 
views
 
which describe the same set of parts
.
 
resources. As a result, the research towards narrowing the 
domain discrepancy with limited data and computational 
resources 
is desired to cope with 
realistic challenges.
 
To  address  the  above
-
mentioned  challenges  and 
to 
facilitate 
a 
model towards reducing domain 
discrepancie
s,
 
we propose a new way
 
of determining irrelevant 
contents
 
in 
input images. 
The 
contents
 
have 
little
 
mutual information 
with 
the 
labels and do not contribute to 
predictions, as will 
be 
discuss
ed in Section III
-
B
.
 
To reduce the risk of over
-
fitting caused by the irrelevant 
contents
, 
an HP 
m
odel for 
human parsing and dense
ly
 
localizing 
human textures
 
is 
developed
 
and is shown in Fig. 1 (a)
.
 
The HP 
m
odel performs 
almost  as  well  as  the  state
-
of
-
the
-
art  model  with  less 
parameters.
 
It
 
maps 
the 
pixels in
 
an 
RGB image to a surface
-
based re
presentation of human bodies and
 
serves the purpose 
of 
re
-
rendering
 
e
ach
 
vide
o
 
in the HMDB
-
51 dataset 
[20]
 
using the textures collected
 
from 
another randomly sample
d
 
video
, as is illustrated in Fig. 1 (b)
.
 
The collection of textures 
is based on 
the 
s
ide views from different frames
 
which
 
offe
r 
complementary descriptions
. 
B
y
 
pairing 
the 
original videos 
with 
re
-
rendered ones 
for training, the 
AR model
 
achieves 
invariance to human appearances 
and reduces the domain 
discrepancy
.
 
Moreover,
 
a procedure is proposed to concatenate
 
the 
predictions from the 
HP model
 
with RGB frames to feed into 
the 
AR model
 
during both training and testing
.
 
T
he 
AR model
 
has the same number of parameters as the state
-
of
-
the
-
art model DTPP 
[17]
 
during testing
 
but 
perform
s be
t
ter.
 
The contributions of the paper 
are 
in 
three
 
aspects
: 
(
1) 
A 
human passing 
m
odel
 
(
HP model
)
 
is developed
 
to conduct
 
part segmentation and dense 
correspondence labelling
.
 
It 
achieves 
the
 
similar performance as the current state
-
of
-
the
-
art model 
[23]
 
and is used to 
collect textures from videos and
 
enrich  the 
HMDB
-
51  dataset 
[20]
. 
(2)  The  dense 
correspondences  predicted  by  the  HP  model  serve  the 
purpose of determining the task
-
irrelevant contents 
in inputs.
 
The benefits 
will
 
be
 
shown in Section 
IV
-
D.
 
(
3
)
 
A procedure
 
is proposed to 
enrich the inputs to
 
AR model
 
by integrating 
the predictions from 
the 
HP model
.
 
Th
e
 
advantages of the
 
AR model
 
will be shown in experi
m
ents
.
 
The rest of this paper is 
organized as follows. Section
 
2
 
surveys
 
t
he
 
methods that are related to our work.
 
Section 3 
discusses
 
our
 
proposed 
HP model
 
as well as the
 
strategies for
 
training and testing 
AR model
. Section 4 explains our 
implementation
 
details  and 
reports 
and  discusses
 
experimental results.
 
C
oncluding
 
remarks
 
are drawn
 
in 
Section 5
.
 
II.
 
R
ELATED 
W
ORK
 
Conventional 
methods. 
The most widely used hand
-
crafted 
features are 
DT
-
based
 
and iDT
-
based 
features
 
proposed in
 
[1]
 
and 
[2]
.
 
DT builds representations along
 
the
 
tra
jectories 
provided by
 
optical flow
 
estimation
s
. 
iDT 
compensates the 
motion of ca
meras to remove the 
influences 
brought by 
shaking
 
hands
. 
iDT  is  also  helpful  in  improving  the 
performance of deep learning models
, as 
wa
s discussed in
 
[17]
. 
However, 
deep features 
perform much better than 
hand
-
crafted ones.
 
Frame
-
level feature learning. 
Early deep learning models 
for action recognition fuse
d
 
the predictions on single frames 
to obtain video
-
level predictions. For instance,
 
[24]
 
divided 
video
s
 
into fragments with 
different 
numbers of frames
.
 
The 
prediction in each fragment is based on a single
-
frame,
 
and
 
early  fusion  and  late  fusion
 
were  also  proposed
. 
[11]
 
introduced
 
different ways of temporal pooling for feature 
fusion and 
could
 
process longer videos 
than those in
 
[24]
. 
[25]
 
evaluated the importance of different frames using rank
-
pooling
 
and
 
proposed an efficient
 
video
-
level descriptor
. 
S
imilarly,
 
[26]
 
sampled 100 
distinctive
 
frame
s
 
from every 
450  on
es  and 
conducted  average  pooling. 
ECO 
wa
s 
introduced in 
[27]
 
which 
combined 2D convolutions with 3D 
convolutions 
for 
representing
 
temporal  and  spatial 
relationships. 
However, 
frame
-
level 
feature representations
 
are easier to
 
suffer from
 
over
-
fit
ting
 
and cannot generalize 
as well as
 
video
-
level ones
 
without appropriate 
techniques 
for feature fusion
.
 
As a result, the long
-
range temporal 
features are required to characterize videos.
 
ConvLSTM.
 
For the purpose of capturing 
the patterns 
which describe long
-
range temporal relationships in videos
, 
Long Short
-
Term Memory (LSTM) module 
is integrated
 
in
to
 
an 
action recognition model
 
to encode the relations
hips
 
between 
different 
frames
 
[28]
 
[29]
 
[11]
. 
[11]
 
and 
[30]
 
proposed to use
 
a 5
-
layer LSTM with 512 hidden nodes in 
each 
layer to 
aggrega
te
 
the
 
features from 
the RGB stream 
with those from the optical flow stream
. 
[31]
 
i
n
troduced
 
the
 
end
-
to
-
en
d  Recurrent  Pose
-
Attention  N
etwork  (RPAN)
 
where a spatial attention mechanism based on poses 
wa
s 
used to integrate pose
-
relevant features in all frames into the 
feature representation of action 
recognition
.
 
This method 
facilitates an LSTM to learn the structure of movements 
along  time.
 
[32]
 
applied
 
temporal 
dilated  convolutions
 
which operated on multiple time steps
 
to 
capture
 
long
-
range 
temporal 
informat
i
on
.
 
[33]
 
proposed to sample task
-
relevant 
frames and aggregate
d
 
local features before classification. 
This mechanism is similar to temporal attention. However, 
LSTM or RNN
-
based architectures might bring additional 
parameters which in
creases the computational burden and 
the risk of over
-
fitting. Our proposed scheme increases the 
robustness of a model without increasing the number of 
learnable parameters.
 
 
3D Convolutions.
 
To make use of long
-
range temporal 
information without increasing complexity, 
3D CNNs are 
proposed. Different from LSTMs, each input is composed of 
multiple frames. As a result, the temporal information can 
be captured 
without segmenting 
frames
 
into snippe
ts
 
[4]
 
[34]
 
[15]
 
[35]
 
[14]
.
 
Early research on 3D CNNs include
 
[36]
 
which combined the features from
 
an RGB stream, an optical 
flow stream and a stream of gradients
. 
[15]
 
introduced the 
C3D framework 
which improved the efficiency of 3D CNNs
. 
[37]
 
decomposed 
each 3D convolution
 
into
 
a
 
spatial 
2D 
operation
 
and 
a
 
temporal
 
1D operation
. 
I3D proposed 
in 
[4]
 
integrated the two
 
convolutional operations in
 
a
 
two
-
stream 
a
rchite
cture into a 3D one
. 
Moreover, the spatial and 
temporal streams in
 
[4]
 
we
re trained independently. 
[38]
 
and 
[39]
 
proposed 
“
P3D ResNet
”
 
to efficiently decompose 
3x3x3 convolutions into 1x3x3 ones for spatial feature 
extraction and 3x1x1 temporal ones for temporal feature 
extraction
. 
[40]
 
proposed
 
a 
SMART block to learn spatial 
and temporal features 
independently
. 
[41]
 
demonstrated
 
that 
pre
-
training on the Kinetics dataset improves
 
the
 H
P
 
M
o
d
e
l
 

P
a
r
t
 
I
D
(
1
 
t
o
 
2
4
)
U
-
c
o
o
r
d
i
n
a
t
e
(
1
 
t
o
 
2
0
0
)
V
-
c
o
o
r
d
i
n
a
t
e
(
1
 
t
o
 
2
0
0
)
I
n
p
u
t
B
l
a
n
k
 
t
e
x
t
u
r
e
s
(
2
4
 
b
l
o
c
k
s
 
e
a
c
h
 
w
i
t
h
 
s
i
z
e
 
2
0
0
*
2
0
0
)
P
i
x
e
l
-
l
e
v
e
l
 
m
a
p
p
i
n
g
 
b
a
s
e
d
 
o
n
 
l
o
o
k
-
u
p
 
t
a
b
l
e
 
(
 
c
o
o
r
d
i
n
a
t
e
:
 
[
P
a
r
t
 
I
D
,
 
U
-
c
o
o
r
d
i
n
a
t
e
,
 
V
-
c
o
o
r
d
i
n
a
t
e
]
 
)
F
i
l
l
e
d
 
t
e
x
t
u
r
e
s
(
2
4
 
b
l
o
c
k
s
 
e
a
c
h
 
w
i
t
h
 
s
i
z
e
 
2
0
0
*
2
0
0
)
 
(a)                                              (b)                                (c)
 
Fig. 2. 
The procedure for sampling textures from continuous frames. 
Left column: input frames 
illustrat
ing a person from multiple point
-
of
-
views. 
[PartID, 
U
-
coordinates, V
-
coordinates] 
–
 
t
he p
redictions from the 
HP model
 
shown
 
in Fig. 1 (a)
,
 
and
 
the 
UV 
coordinate
s
 
map
 
the 
pixels on images to the 
24 
look
-
up 
table
s
 
of textures
,
 
each
 
one of the 24
 
set
s
 
of textures 
includ
ing
 
a
 
200*200 
block
. 
Right column: 
the look
-
up table filled by 
the 
textures collected
 
from input 
frames.
 
Each frame provide
s
 
one specific point
-
of
-
view of the same person.
 
performance of generalization. 
[42]
 
improved conventional 
3D CNNs by increasing the sizes of receptive fields, and the 
activations from spatial and temporal streams were fused to 
produce responses.
 
However, 
the performance of 3D CNNs is hindered by 
two
 
disadvantages:  (1) 
The  motion  representations 
developed are not as good as those learned by a two
-
stream 
method based on optical flows
. (2)
 
A
 
3D CNN 
is
 
quite 
memory
-
consuming
 
because it
 
take
s
 
all frames as inputs
, as 
was reported 
in
 
[17]
.
 
Our proposed scheme is based on a 
two
-
stream structure
.
 
Two
-
stream methods. 
A framework with a two
-
stream 
architecture
 
can be trained with offline
-
computed data such 
as  optical  flow  estimations  or  iDT  information
. 
[6]
 
demonstrated
 
that concatenating the convolutional features 
from two branches 
using
 
1x1 convolutions contributes to 
better performance. To reduce the
 
risk of
 
over
-
fitting in the 
spatial  stream, 
[43]
 
proposed  to 
multiply  the  feature 
representations from two branches
. 
[8]
 
introduced TSN 
which removed similar frames through dividing videos into 
snippets  and  uniformly  sampling  from  snippets.  The 
integration of RGB images, optical fl
ows as well as warped 
optical  flows  was  also  implemented
. 
[44]
 
proposed 
Action
VLAD 
which  jointly  encoded  both  spatial  and 
temporal features after aggregating them. However, the 
dimension  of ActionVLAD  representation  is  with  size 
3
2,768. It is of low efficiency in implementation
. Moreover, 
[45]
 
built a temporal conditional random fields (CRFs) for 
making predictions based on multiple aspects of actions
. 
[46]
 
implemented temporal 
attention mechanism by detecting 
critical  events  from  videos
.
 
End
-
to
-
end  spatiotemporal 
networks were proposed to fuse both types of features using 
a pyramid architecture
 
[47]
 
[5]
 
[48
]
 
[49]
.
 
Temporal attention 
based on a compact bilinear layer and spatial attention based 
on a spatiotemporal attention module are combined in the 
architecture.
 
[50]
 
reduced the redundancy from both 
data and 
CNNs
’ 
structures. However, the two
-
stream structure suffers 
from low efficiency. As a result, we propose to enhance the 
performance of a CNN model by upgrading the strategy of 
training. This  method  does  not  add  to  the  number  of 
learnable parameter
s.
 
III.
 
M
ETHODOLOGY
 
The 
work is divided into two parts:
 
T
he
 
first part involves 
the development of
 
an 
HP model
 
for
 
human parsing and 
dense  correspondence  labelling
.
 
T
he  predicted 
correspondences  map  colored  pixels
 
in  images
 
t
o
 
the 
locations  of
 
textures
 
in 
different
 
person
 
parts
.  T
he 
collections  of
 
textures  from
 
video
s
 
are
 
based  on  the 
correspondences
.
 
Different from conventional models for 
pose estimation, the 
HP model
 
also provides the point
-
of
-
view of each semantic part. For instance, 
even if the same
 
semantic
 
part appears in two frames, 
the first frame may 
show its 
frontal view 
while
 
the second 
frame shows its side view
.
 
The second part 
includes
 
the analysis about the task
-
irrelevant 
content
s
 
in  input  images
.
 
Moreover,  t
he  re
-
rendered 
videos are generated for 
improving the
 
robustness
 
of the
 
AR model
 
during training
. Finally, 
the method for 
enriching the inputs to the 
AR model
 
is 
propos
ed
.
 
A.
 
Nested CNNs for human parsing
 
and texture 
extraction
 
We  name  this  model  as 
HP  model
.  The  predicted 
correspondences 
map colored pixels on human images to the 
textures describing
 
the
 
24 semantic person parts.
 
Different 
from two
-
stage methods such as Dense
-
Pose 
[23]
, Faster
-
RCNN 
[51]
 
and M
ask
-
RCNN 
[52]
 
which involve over 100 
layers in feature extractors
, 
our
 
proposed 
HP model
 
has
 
much less parameters
.
 
As a result
,
 
the 
HP model
 
spends
 
less 
time during 
both training and testing
.
 
The  first  sub
-
net 
conducts  two
-
class  semantic 
segmentation 
(
human
s
 
and backgrounds
)
 
and 
it serves as a 
prior to the second stage, as is illustrated in Stage 1 of Fig. 1 
(a). We leverage the PSPNet
-
50
 
[53]
 
as
 
the 
backbone. The 
prediction is formulat
ed as 
'
H W C H W C
   
  
  
where 
H
 
and 
W
 
denote
,
 
respectively, 
the 
height and w
idth of input 
and output images with
 
3
C

 
and 
' 1
C

. 
The p
ixel
-
wise 
cross
-
entropy loss
 
function
 
is computed based on the binary 
pixel
-
wise annotations for training 
s
tage 1.
 
The second sub
-
net concatenates 
the 
original RGB images 
with the 
predictions
 
from the first sub
-
net as 
the 
input
s
. It 
estimates a fine
-
grained semantic parsing map with 24 labels 
and  UV  coordinates  for 
dense  correspondences
.  The 
predictions are formulated as 


'
''
H W C C
H W C
  
 
  
  
where 
'' 3
C

. As is shown in 
Fig. 1 (a) and 
Fig. 2, each output has 
3 channels: PartID, U
-
coordinates an
d V
-
coordinates. PartID 
maps each
 
pixel to its corresponding part and (U
-
coordinates, 
V
-
coordinates) 
is
 
the exact 
location
 
of the pixel within the 
part using t
he look
-
up table 
corresponding to
 
tha
t
 
part. Each 
part is parameterized by a unique look
-
up table
 
with size
200 200

, as is 
illus
t
rated
 
in Fig. 2
 
(b
) (
c
)
. Stage 2 has the 
same backbone as Stage 1
.
 
T
wo heads are built on top of the 
backbone, one for part segmentation and the 
other for 
coordinate regression.
 
Different from
 
[23]
 
which 
utilized
 
the
 
available
 
texture
s
 
from
 
the SURREAL 
dataset 
[54]
,
 
we 
generate 
textures
 
using 
color information
 
from 
video clips in the HMDB
-
51 dataset 
[20]
.
 
The procedure is demonstrated in Fig. 2
 
and is 
based 
on the SMPL format of dense correspondences
 
[22]
. 
24 
200x200 
plane
s
 
compose one set of textures describing one 
person
. 
P
ixel
s
 
in 
RGB frames are
 
mapped to the planes
 
with 
relative locations kept
. 
As 
there are only limited pixels 
available, 
interpolation
 
is conducted to 
i
n
-
paint
 
the texture 
details
 
that cannot be 
collected 
from video frames
. 
The filled 
textures are 
shown in Fig. 2
 
(c)
. 
As is shown in 
Fig. 2
 
(a)
, 
people  with different poses 
can provide 
the 
details of 
different parts
 
or different components of the same part
. 
For 
instance, the frontal pose shows
 
the textures of
 
chest while 
the view of the person
’
s back shows 
the details of
 
back. 
Only 
by 
collecting
 
the 
textural details
 
from different view
s 
can
 
the 
dense correspondences
 
of
 
all 
24 part
s
 
be 
obtained
.
 
B.
 
Input 
content
 
selection based on dense correspondences
 
In this sec
t
ion
,
 
we evaluate the correlation between human 
appearances and 
the task of action recognition. 
Different 
from the procedures for feature selection in machine learning 
[55]
, 
we directly evaluate the redundancy in inputs and 
remove the con
t
ents in
 
the
 
input data which are
 
irrelevant to 
the target task.
 
[55]
 
divided the selection of 
input 
features into two steps: 
subset search and subset evaluation based on
 
Eq.
 
(1).
 
 






1
v
V
v
v
D
Gain A Entropy D Entropy D
D

 

 
(1)
 
where 
D
 
denotes the dataset and
 
A
 
indicates the subset 
of features under evaluation. 
D
 
is divided by 
A
 
into 
V
 
subsets each of which share the same value on 
A
. 


Gain A
 
indicates
 
the 
contribution  made  by 
A
  
to  correct
ly 
classifying 
D
 .
 
Suppose
 
that
 
D
  
has 
N
  
class labels and 
the  proportion  of  input  samples  belonging  to  the 
 
 
,1
i th i N
  
 
class is 
i
p
.
 
 


1
1
log
N
i
i
i
Entropy D p
p



 
(2)
 
Similarly, we firstly divide input images into co
nt
ents and 
then evaluate 
the relevance
 
of 
human 
appearance con
t
ent
s
 
to 
the action recognition task.
 
The
 
dense 
correspondences 
between RGB images and the surface
-
based representations 
of 
24 
human 
part
s
 
provided by the 
HP model
 
in Fig. 1 (a)
 
enable
s
 
us 
to 
evaluate
 
the 
contribution  from
 
human 
appearances 
to classification
. 
F
irstly
,
 
Eq. 
(1) 
is reformulated 
as
 
 






















 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
1
log
1
 
 
 
 
 
 
 
log
y
x y
x
Gain X p Y y
p Y y
p X x p y X x
p y X x
H Y p X x H Y X x
H Y H Y X
  

 

   
 

 

(3)
 
where 
X
  
represents an
 
input sample
 
containing only human appearances
 
and 
Y
 
indicates 
a 
ground truth
 
label
 
in 
action recognition
. 
The first term in
 
Eq.
 
(3) denotes 
the 
entropy of the dataset. 
The second term 
measures  the
 
remaining
 
uncertainty  in  the 
data
set
 
if  provided
 
X
 .
 


Gain X
  
measures the 
contribution made by 
X
  
to
 
the
 
accura
t
e prediction of 
Y
 .
 
In another way, the mutual 
information 
between 
random variables 
X
  
and 
Y
  
with 
joint distribution 


,
p x y
 
can be expressed as
 
 


























; ,
,
, lo
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
g
, log
 
KL
x y
x y
I X Y D p x y p x p y
p x y
p x y
p x p y
p y x
p x y
p y
H Y H Y X
 

 
 

 
 
 
 

 
 
 
 


 
(4)
 
As a result, 
the 
relevance
 
of 
X
 
to 
the target task with
 
the 
label 
Y
  
is equivalent to 
the mutual information between 
X
  
and 
Y
 .
 
Under the assumption that 
a 
deep learning 
model is best at extracting the features in inputs that are 
informative about labels, 
w
e 
evaluate
 


;
I X Y
 
by
 
training a 
CNN 
model 
for predicting 
Y
 
given 
X
 
and 
compare 
the 
performance of predicting 
Y
  
using 
X
  
and randomly 
guessing 
Y
.
 
To 
encode only the information of human appearances, 
o
ur 
AR model
 
is trained on the 
inputs
 
which only 
contain the 
textures of 24 human parts
. 
The images are with size 1200
-
by
-
800 and an example is illustrated in Fig. 2 (c). 
The setting 
up is shown in Fig. 3.
 
 
Fig. 3. The 
AR model
 
is re
-
purposed to conduct single
-
frame classification. 
The feature
-
maps with 24 channels are predicted by the 
HP model
,
 
describing
 
the appearances of human parts
 
only
.
 
Each channel has a size of 
200

 200
.
 
The 
AR model
 
learns to predict action labels based on the 
feature
-
maps.
 
The accuracy over the training process 
will be
 
shown
 
in 
Section IV
-
D
.
 
C.
 
The 
method for training 
AR model
 
T
he
 
AR model
 
has 
the RGB stream
 
and flow stream
 
discussed in 
[17]
.
 
The 
backbone
 
is based on the BN
-
Inception Network 
[56]
. 
The
 
difference between 
the RGB 
stream
 
in our 
AR model
 
and that in 
DTPP Model
 
[17]
 
lies in 
the procedure of training
, as
 
is
 
illustrated 
in Fig. 
4
.
 
 
(a)
 
 
(b)
 
Fig. 
4
. The procedure for training 
our
 
AR model
. (a) Each input video is re
-
rendered using 
a set of
 
textures
 
which are
 
collected from another 
randomly 
sampled 
video from the HMDB
-
51 
d
ataset. The procedures for texture 
rendering have been illustrated in Fig. 1 (b) and Fig. 2. The 
network
 
inherits 
its structure from the DTPP
-
Net proposed in 
[17]
. 
Paired i
mages are located 
in the same batch to facilitate the ave
raging of gradients. (b) 
After being 
pre
-
trained according to (a), the 
AR model
 
is trained for a second time. 
Each 
input frame to our 
AR model
 
is composed of 6 channels which is the 
concatenation of the original RGB and the predictions from the proposed 
HP
 
model
. Each input frame is composed of 6 channels during both training 
and testing. 
 
Firstly
,
 
the humans in
 
all input videos are re
-
rendered 
using th
e method 
shown 
in Fig. 1 (b).
 
T
he source videos for 
extracting  textures  and 
re
-
rendered 
target  videos 
are 
randomly paired. 
The
 
first 
branch
 
in Fig. 
4 (a)
 
corresponds 
to 
an 
original 
video from HMDB
-
51 
d
ataset
 
while the second 
one 
shows the 
re
-
rendered
 
version
.
 
Both 
branches
 
share 
everything but 
the 
human appearances
 
in inputs
.
 
The second 
branch only exists d
uring training.
 
The
 
batch
 
size for training 
is 4
 
and two pairs of videos are 
involved in each batch.
 
The 
gradients from 
both 
the original branch and the re
-
rendered 
branch are averaged to 
update
 
the
 
parameters.
 
Alt
hough the 
input
s are different, the two 
branch
e
s are forced to learn the 
same 
feature  representations
 
which  is  invariant  to 
the 
changes of 
appearances
.
 
 
T
he
 
procedure for training
 
is divided into two steps
, as is 
described in Algorithm 1
. In the first step, 
the 
AR model
 
is 
trained
 
with 3 input channels, as illustrated in
 
Fig. 
4
 
(a). Then 
each input frame is expanded to be with 6 channels and 
The 
AR model
 
is fine
-
tuned on the expanded frames.
 
Algorithm 1
.
 
The p
rocedure
 
f
or
 
t
raining
 
Steps
 
Operations
 
1
 
Pair each 
video clip from the HMDB
-
51 
d
ataset 
with
 
its
 
9 re
-
rendered versions
. 
Train 
the spatial stream of the 
AR model
 
with pairs of videos according to Fig. 
4
 
(a). Each batch 
is 
composed of
 
two pairs of videos.
 
2
 
Collect the prediction
 
of the 
HP model
 
on each frame and 
concatena
te the  prediction
 
with
 
its  corresponding
 
RGB
 
frame
. 
Modify the first convolutional 
layer
 
of the spatial 
stream and train it on new inputs with 6 channels.
 
This
 
training 
procedure 
has two 
merit
s. 
Firstly
,
 
the 
AR 
model
 
develop
s
 
a
 
robus
t
 
feature representation and the 
training instances in the learned manifold have lower intra
-
class distances. The model achieves invariances to 
different 
human  appearances  which  are  irrelevant  to  action 
recognition. As a result, the learned feature e
xtractors tend 
to concentrate on more discriminative features, especially 
when faced with
 
confusing
 
data
.
 
The domain 
discrepancy
 
between  training  data  and  test  data  is  reduced  and 
performance is improved.
 
Secondly, 
involv
ing
 
segmentation 
masks as well as 
coordinates of textures as 
the 
inputs
 
provides
 
the descriptions of 
both 
poses
 
and 
the 
view
s
 
of 
semantic parts
.
 
For instance, the frontal view and back view 
of the same semantic part are distinguished by different 
textural coordinates within the same part.
 
A 
comparison 
between 
the 
pro
posed 
AR model
 
and
 
existing benchmarks
 
will be 
presented
 
in Section IV
-
E
 
and Section IV
-
F
.
 
IV.
 
E
VALUATION
 
A.
 
Introduction to 
Dataset
s
 
The 
HP model
 
and the 
AR model
 
we
re trained on different 
datasets. 
At Stage 1, t
he
 
HP model
 
wa
s
 
trained on 
t
he 
union
 
of
 
the
 
MHP
 
v2.0
 
d
ataset 
[57]
 
and 
the 
LIP 
d
ataset 
[58]
. At 
Stage
 
2
,
 
the 
HP model
 
wa
s trained on the 
Dense
-
pose COCO 
d
ataset 
[23]
.
 
The 
AR model
 
wa
s trained
 
and tested
 
on the 
HMDB
-
51 
d
ataset 
[20]
.
 
MHP
 
v2.0
 
D
ataset.
 
The 
dataset contains 25,403 images 
with 
58 fine
-
gr
ained semantic category labels
.
 
I
t is divided 
into 3 splits: 
15,403 for training, 5,000 for validation
,
 
and 
5,000 for 
test
ing
. In our experiments, the labels on
 
all
 
semantic person parts
 
are assigned t
he same
 
class
 
label
 
while 
the backgrounds are assigned the other 
label
. 
The evaluation 
metric for this 
dataset
 
include
s 
Average Precision
 
based on 
part
s
 
(
p
AP
)
, Average Precision based on region
s
 
(
r
AP
)
 
as 
well as 
Percentage of Correctly parsed 
Parts (
PCP
).
 
p
AP
 
compu
tes
 
the average of 
Intersection over Union (
IoU
) of 
all part categories.
 
r
AP
 
evaluates the average of 
IoU
 
of
 
all instances.
 
PCP
 
evaluates the proportion of parts that are 
parsed  with an 
IoU
  
over  a  certain threshold. 
In  our 
implementation, 
p
AP
 
is the same as
 
r
AP
 
because there is 
only one foreground category.
 
The
 
loss function of Stage 1 
in 
HP model
 
is based on 
r
AP
.
 
LIP Dataset. 
The 
dataset contains 50,462 images with 
pixel
-
wise annotations 
covering
 
19 semantic 
person
 
part 
labels
. 
We
 
ha
ve converted the 19 labels into 1
 
uniform 
foreground label
. The images
 
are with 
challenging poses and 
views, heavily occlusions, 
various appearances
,
 
and low
 
resolutions.
 
The metric for evaluation is the same as that in
 
the
 
MHP
 
dataset
.
 
Dense
-
pose COCO
 
Dataset
.
 
The 
HP model
 
wa
s trained 
on the 
Dense
-
pose COCO dataset 
proposed 
in
 
[23]
. 
The 
dataset involves
 
the 
annotations for
 
50
k
 
humans
 
with
 
over 5 
million
 
manually annotated
 
correspondences
 
in the form of 
UV coordinates
. 
The training set contains 48k humans 
and
 
the test set 
has 
1.5k images with 2.3k humans. 
The metric 
for evaluating performance on this dataset 
include
s 
A
rea 
U
nder the 
C
urve (AUC) and 
IoU
. 
 
HMDB
-
51 Dataset. 
The 
dataset
 
consists of
 
realistic 
videos from multiple 
sources, 
including
 
movies and web 
videos. 
I
t is composed of 
6,
766 video clips 
which cover
 
51 
action categories.
 
Standard 3 training/testing splits are used 
for evaluation. 
The evaluation metric 
adopted for 
this dataset 
is classifica
tion accuracy
 
(%)
.
 
We ha
ve 
re
-
rendered each 
video with 9 sets of 
textures which are randomly sampled 
from 
9 
other videos. T
he augmented dataset contains 67,660 
video clips.
 
Penn Action Dataset.
 
The dataset wa
s
 
proposed in
 
[59]
 
and includes
 
2,326
 
videos 
from 15 categories of actions.
 
For 
instance, 
“
baseball_swing
”,  “
pull_ups
” 
and 
“
strumming_guitar
”
 
are three 
of these 
classes.
 
The videos 
were 
obtained from 
multiple
 
online 
sourc
es. The 
number
 
of 
frames in each video
 
rang
e
s
 
from 18 to 663. 
The 
frames
 
in 
this dataset 
suffer from 
occlusions as well as significant variance
s
 
in scales.
 
Experiment
al result
s
 
on this dataset
 
are 
presented 
in Part E.
 
B.
 
I
mplementation
 
Details
 
HP 
M
odel
.
 
Different from both 
the end
-
to
-
end 
Fully 
Convolutional Network 
(
FCN
)
 
structure and the two
-
stage 
R
CNN structure 
[23]
, 
the two stages 
of the 
HP model
 
can be
 
trained together 
and the second stage takes both the heat
-
maps and 
the 
original RGB images as 
the 
in
puts. 
As
 
is
 
shown 
in Fig. 1
 
(a), 
Stage 2
 
is built on Stage 1
 
and conducts refined 
regression
 
based
 
on the predictions of
 
Stage 1
. 
The training 
of 
HP model
 
is divided into 
t
wo
 
steps
.
 
Firstly
,
 
at 
Stage 1
, it
 
is trained 
on itself
 
with
 
the initial learning rate
 
set to 
2.5e
-
4 
and
 
the 
input images cropped to 
473
-
by
-
473. 
After the 
validation accuracy saturates, we keep the 
learnt 
parameters 
from 
Stage 1 and concatenate the output heat
-
maps with 
the 
original images 
as the inputs
 
of 
Stage 2.
 
At 
Stage 2
,
 
the 
model
 
is trained
 
on itself
 
to map 
the inputs 
to
 
semantic part
 
labels
 
and  UV  coordinates. 
The 
part  labels  and  UV 
coordinates are u
tilized as discussed 
in 
Fig. 1 (b)
 
to
 
collect
 
texture
s
 
and re
-
render videos
.
 
Note that the
 
videos from 
the 
classes  such  as
 
“
brush_hair
”
, 
“
chew
”
, 
“
eat
”
, 
“
drink
”
, 
“
dribble
”
, 
“
kiss
”
, 
“
laugh
”
, 
“
smile
”
, 
“
smoke
”
,
 
and 
“
talk
”
 
part 
are not
 
r
e
-
rendered
 
because the 
details of
 
faces cannot be 
perfectly described by textures
.
 
The b
atch size 
wa
s set to 
12 
for
 
training
.
 
The implementation 
wa
s based on
 
the
 
Caffe
 
platform 
[60]
 
with 
two NVIDIA GEFORCE GTX 1080 Ti 
GPUs. 
 
AR 
M
odel
.
 
For the HMDB
-
51 
d
ataset, e
ach video 
wa
s 
split 
in
to 25 segmen
ts
 
during training and testing, i.e. 25 
frames 
we
re sampled from each video.
 
For the Penn Action 
d
ataset, 16 frames 
we
re sampled from each video 
clip.
 
Both 
the spatial stream and the temporal stream 
we
re trained with
 
the categorical 
soft
-
max
 
loss
.
 
T
he batch size 
was
 
set to 
4
.
 
The RGB
 
stream
 
and 
optical flow stream 
we
re trained on 
two GPUs independently
 
and tested together
.
 
The learning 
rate 
wa
s 
initialized to 
0.01
 
and
 
wa
s
 
divided by 10 each time 
when 
validation loss saturates until 0.00001. 
The momentum 
wa
s 0.9 and gradients 
we
re clipped at
 
a L2
-
norm of
 
40.
 
The 
dropout ratio 
wa
s set to 0.8.
 
As is 
illustr
ated
 
in
 
Fig. 
4
,
 
we 
ha
ve paired each training video with 
its 9 re
-
rendered 
versions
 
and  each  batch 
contains  2  video  pairs.
 
The 
implementation 
wa
s 
also 
based on the Caffe 
platform 
[60]
 
and two NVIDIA GEFORCE GTX 1080 Ti GPUs
 
we
re used
.
 
C.
 
Evalu
ation
 
of 
HP 
model
 
The metric for evaluating the accuracy of 
predictions on 
UV
-
coordinates
 
is AUC 
which is computed by
 
 


0
1
a
a
AUC f t dt
a


 
(5)
 
T
wo 
different values of 
a
(
10
  
 
or
 
30
  
)
 
are 
adopted 
to 
measure how many points are estimated with an error of less 
than 
10 cm or
 
30 cm. 
The AUC for t
h
e 
j th

  
estimated 
point is computed by
 
 




2
2
ˆ
,
exp
2
p p
g i i
Distance j

 

 

 
 
 
 
(6)
 
The
 
details in
 
human
 
images
 
are described by vertices 
using the SMPL model introduced in 
[22]
.
 
p
i
 
denotes the
 
location of the
 
vertex that is closest to the ground truth 
location of 
the 
j
-
th point
 
and 
ˆ
p
i
 
loca
tes the 
vertex that is 
closest to the
 
j
 -
th
 
estimated point.
 


,
g

  
measures the 
geodesic distance between two points on a surface.
 

 
is set 
to
 
0.255  when
 


0.5
Distance j

   
corresponds  to
 


30
AUC j cm

 . 

  
is set to 1 when 


0.96
Distance j

 
correspond
s
 
to 


30
AUC j cm

 . 

  
is set to 1.45 when 


0.5
Distance j

  
correspond
s
 
to 


170
AUC j cm

 .
 
In our 
experiments 

  
wa
s  set  to  0.255.
 
Table  1  shows 
a
 
comparison on performance.
 
Intersection over Union (IoU) 
proposed in 
[23]
 
is 
used for evaluating the 
performance of 
human parsing.
 
Table 1. 
Performance comparison on dense 
correspondence labelling. 
The 
performance of our 
HP model
 
is compared with 
that of
 
current state
-
of
-
the
-
art
 
DensePose RCNN and DensePose FCN 
[23]
 
on
 
the COCO 
Densepose 
d
ataset
 
[23]
. The images 
in the dataset 
contain 
multiple people with 
variances i
n poses, backgrounds and scales.
 
Method
s
 
10
AUC
 
30
AUC
 
I
oU
 
DensePose
-
FCN
 
0.253
 
0.418
 
0.66
 
DensePose
-
RCNN  (only 
points)
 
0.315
 
0.567
 
0.75
 
DensePose
-
RCNN 
(
distillation
)
 
0.381
 
0.645
 
0.79
 
DensePose
-
RCNN 
(
cascade
)
 
0.390
 
0.664
 
0.81
 
HP model
 
0.383
 
0.651
 
0.80
 
Human Performance
 
0.563
 
0.
835
 
 
From Table 1
,
 
we can see
 
that our p
roposed 
HP model
 performs almost as well as the current state
-
of
-
the
-
art model
 
(DensePose
-
FCN and DensePose
-
RCNN
 
[23]
)
 
with only 
52.7% 
as many 
parameters.
 
D.
 
Determining the task
-
irrelevant content using the 
predictions from the HP model
 
The accuracy
 
over the training process is shown in Fig. 
5
. 
It can be observed that the probability of labels provided by 
appearances 


p Y y X x
 
  
is almost equal to 


p Y y

 
because the number of classes is 51. A probability of 


0.0196
p Y y
 
  
can be achieved by random sampling 
without any information from 
X
 . According to Eq. (4), 


,
I X Y
 
approximates zero.
 
 
Fig.
 
5
. Training classification accuracy of the AR model shown in 
Fig. 3. 
The training was conducted for 45,000 iterations without obvious increase 
in accuracy.
 
To
 
further
 
demonstrate  that 
human  appearance  is 
irrelevant to action recognition,  we replace the 
human 
regions in images with the predictions f
rom
 
the HP model
.
 
In that way, humans 
in
 
the videos of all classes are with the 
same  appearance.
 
The revised  images  are 
used
 
during 
training and testing.
 
Fig. 6 shows 
an
 
example.
 
The
 
AR model is trained and tested on the revised images 
shown in the bottom rows in Fig. 6
 
(a) and Fig. 6 (b)
. The 
results in Table 2 demonstrate the effectiveness of removing 
task
-
irrelevant contents.
 
 
(
a)
 
 
(
b)
 
Fig.
 
6. 
The h
uman regions
 
are
 
replaced with the outputs from the HP model
.
 
(a) and (b) show t
he
 
frames
 
from
 
two videos
 
which have
 
different class 
labels
 
in
 
the HMDB
-
51 dataset.
 
Upon
 
the revision, humans in videos of 
different
 
classes are with the same appearance.
 
Table 
2
. 
The
 
effect  of  revising  training  and  testing  images  on
 
the
 
performance of the
 
AR model
. The experiment is 
conducted
 
on the three 
testing splits of HMDB
-
51
. Performance is
 
measured in classification 
accuracy (%).
 
Methods
 
Split 1
 
Split 2
 
Split 3
 
Spatial Stream
 
(AR model) 
train
ed and tested 
on 
the 
original images in HMDB
-
51
.
 
61.5
 
61.
2
 
6
0.5
 
Spatial Stream
 
(AR model) 
trained 
and tested 
on 
the revised 
images.
 
6
2.8
 
6
2.7
 
6
1
.
6
 
From
 
Table 2 it can be observed that revising the humans 
in all videos to be with the same appearance does not lead to 
any decrease in accuracy.
 
T
he method of enforcing the 
invariance to
 
human appearances will be conducted in Part 
E.
 
E.
 
Evaluation
 
of 
AR model
 
The performance of 
the 
AR model
 
is 
evaluated on the 
HMDB
-
51 dataset. 
During
 
both training and testing, 
25 
fram
es 
we
re sampled from each video, 
and 
the scores from 
the  temporal  stre
am  and  spatial  stream 
we
re  fused. 
A 
temporal pooling layer of 3 le
vels
 
proposed in
 
[17]
 
wa
s 
applied  in  both  streams.
 
The 
procedure  for  learning 
appearance
-
inva
riant representations and
 
enriching inputs 
wa
s applied
.
 
As the AR model is a modified version of the 
temporal stream in DTPP
 
which is
 
proposed in 
[17]
, we 
compare the
 
AR
 
model with DTPP in Table 
3
.
 
The 6,766 video
s are divided into three splits. T
he first 
split  has 
3,570  videos  for 
training
,
 
1,531
 
videos  for 
validation
 
and 
1
,
665 videos for test
ing
. 
The second split has 
3,570
 
videos for training,
 
1,532
 
videos for validation
 
and 
1,664 for test
ing
.
 
The third split has 
3,570
 
videos for training,
 
1,5
32
 
videos for validation
 
and 1,664 for test
ing
.
 
According 
to  the  strategy  of  data  augmentation  and  pairing,  we 
expand
ed
 
the three splits to be with 
35
7
,
0
0 training videos 
and 
15
,
310
 
validation videos, 
357,00
 
training videos and 
15,
3
2
0 validation videos, 
357,00
 
training videos and 
15,320
 
validation videos, respectively. 
Th
e results are shown in 
Table 
3
.
 
Ablation study 
wa
s conducted
 
to 
evaluat
e
 
the 
improvements brought by each
 
of the two
 
step
s
 
in 
Algorithm 
1
 
(Table 
3
)
.
 
Table 
3
. 
P
erformance
 
comparison between
 
the 
AR model
 
and the 
DTPP 
Model
 
[17]
 
on
 
the
 
three  testing  splits  of  HMDB
-
51,
 
m
easured  in 
classification accuracy
 
(%)
.
 
Methods
 
Split 1
 
Split 2
 
Split 3
 
Spatial Stream
 
(DTPP)
 
61.5
 
61
.
2
 
6
0.5
 
Spatial Stream
 
(
AR model
)
 
trained according to 
Fig. 
4
 
(a)
 
61.9
 
61
.
7
 
60.9
 
Spatial Stream
 
(
AR model
)
 
with 
6 input channels
 
shown in Fig. 
4
 
(b)
 
and trained according to Fig. 
4
 
(a)
 
63
.
4
 
6
3.2
 
62.
4
 
Temporal Stream
 
(DTPP)
 
66.3
 
69.2
 
68.8
 
Temporal Stream
 
(
AR model
)
 
66.3
 
69.2
 
68.8
 
Fusing  two  streams 
(0.4  for 
spatial, 0.6 for temporal)
 
(DTPP)
 
75.0
 
75.0
 
74.4
 
Fusing two streams 
(0.4 for 
spatial, 0.6 for temporal)
 
(
AR 
model
 
trained using the 
techniques in Fig. 
4
 
(a) and (b)
)
 
76.
4
 
7
6
.5
 
75.7
 
Fusing two streams
 
with MIFS
 
(DTPP)
 
76
.
9
 
76.
3
 
75.
9
 
Fusing two streams
 
with MIFS
 
(
AR model
 
trained using the 
techniques in Fig. 
4
 
(a) and (b)
)
 
77.
8
 
7
7
.2
 
76
.8
 
Fusing  two  streams  with  iDT
 
(
DTPP
)
 
76
.3
 
74.6
 
7
5
.1
 
Fusing two streams with iDT
 
(
AR model
 
trained using the 
techniques in Fig. 
4
 
(a) and (b)
)
 
77.0
 
75
.4
 
75
.
8
 
T
wo
 
conclusions can be drawn f
rom Table
 
3
.
 
Firs
t
ly
, 
S
tep
 
1 of Algorithm 1
 
enables the 
AR model
 
to develop the 
feature  representations  which  are  invariant  to 
different 
human appearances. 
A slig
ht improvement of about 0.
5
% 
demonstrates 
that
 
the invariance to 
human appearances
 
reduces  the  domain
 
discrepancy
. 
This  conclusion  is 
consistent with the 
discussion
 
in Section III
-
B
 
and Section 
IV
-
D
.
 
Besides 
developing
 
the 
invariance  to 
irrelevant 
contents
, 
Step 2 in Algorithm 1 also 
enriches inputs with 
the 
predictions from the HP model
.
 
The performance is further 
improved. 
As a result, 
the 
AR model
 
out
performs
 
the 
DTPP 
Model
 
[17]
 
both 
in
 
the
 
single RGB stream and in the
 
fusion 
of both streams.
 
Note that the MIFS 
m
odel 
[61]
 
stacks 
the 
features
 
which are
 
extracted using a family of differential 
filters parameterized 
with multiple time skips, 
while 
the
 
iDT 
m
odel 
[1]
 
cancels out camera motions 
from both streams
.
 
T
he predicted scores 
from 
MIFS and iDT
 
are available
 
online
 
[17]
. 
Besides the first convolutional layer
 
which 
involves 6 input channels
, t
he
 
AR model
 
has the same 
architecture as the 
DTPP Model
 
during testing
.
 
The
 
AR model
 
is also compared  with several 
other 
benchmark  models.  The  average  accurac
y
 
of  different 
models
 
on
 
the three splits 
of the dataset 
is shown in Table 
4
.
 
Table 
4
. 
Performance
 
comparison
 
of 
AR model
 
with existing benchmarks.
 
Model
s
 
Classification 
Accuracy 
on HMDB
-
51
 
(%)
 
iDT 
[1]
 
57.2
 
MoFAP
 
[62]
 
61.7
 
MIFS 
[61]
 
65.1
 
Two
-
stream 
[9]
 
59.4
 
TDD 
[3]
 
63.2
 
FstCN 
[37]
 
59.1
 
LTC 
[14]
 
64.8
 
TSN (
3 segments
)
 
[8]
 
70.7
 
TSN
 
(7 segments) 
[8]
 
71.0
 
DOVF
 
[33]
 
71.7
 
ActionVLAD
 
[44]
 
66.9
 
ST
-
ResNet 
[6]
 
66.4
 
ST
-
Multiplier 
[43]
 
68.9
 
ST
-
Pyramid 
Network 
[47]
 
68.9
 
TLE 
[5]
 
71.1
 
Four
-
Stream 
[63]
 
72.5
 
DTPP 
[17]
 
74.8
 
ST
-
ResNet + iDT
 
[6]
 
70.3
 
ST
-
Multiploer + 
iDT 
[43]
 
72.2
 
DOVF + MIFS 
[33]
 
75.0
 
ActionVLAD + 
iDT 
[44]
 
69.8
 
Four
-
Stream + 
iDT 
[63]
 
74.9
 
O
ur proposed 
AR model
 
76.2
 
From Table 
4
,
 
we can see
 
that 
our 
AR model
 
outperforms 
existing benchmark methods even without t
he augmentation 
of MIFS or iDT.
 
To 
have a better comparison
,
 
Fig. 
7
 
shows 
that o
ur 
AR model
 
outperforms
 
the 
DTPP 
m
odel
 
on
 
most
 
of all 51 classes in
 
HMDB
-
51 
d
ataset
 
[20]
.
 
 
Fig. 
7
. 
Classification accuracy for each action
 
class
 
on the test
 
set of 
HMDB
-
51 dataset 
[20]
.
 
F.
 
Evaluation on Additional Data
 
for A
ction 
R
ecognition
 
I
t 
has been
 
shown in Fig. 
4
 
(a),
 
(b) and Table 
3
 
that 
the 
integration of predictions from HP model contributes to 
improvements in action recognition. However, 
the dataset is 
not large enough to cover all types of cases.
 
Therefore, 
the 
proposed model is 
also 
evaluated on the 
Penn Action dataset
 
[59]
 
to show its benefits.
 
Similar to 
the scenario in
 
HMDB
-
51 
d
ataset, the 
HP 
model
 
wa
s firstly applied to 
build the dense correspondences 
between 
the pixels in RGB frames
 
from 
Penn Action 
d
ataset
 
[59]
 
and 
the 
surface
-
based representation of human bodies.
 
Then
 
optical flow estimation 
wa
s conducted 
on the frames 
provided  in 
the 
Penn Action 
d
ataset. 
T
he 
spatial  and 
temporal 
streams of the
 
AR model
 
we
re trained on 
RGB 
frames and optical flow frames, respectively. 
 
The frames in each video are divided using a
 
50/50 
train
ing/testing  split.
 
Performance  is  measured 
in
 
classification accuracy and is shown in 
Table 
5
.
 
Table 
5
. 
Classification accuracy 
on the Penn Action 
d
ataset.
 
Model
s
 
Accuracy
 
(%)
 
Joint action recognition and 
pose 
estimation 
[64]
 
85.5
 
Pose for action 
[65]
 
92.9
 
Body joint guided method 
[66]
 
95.3
 
Our
 
model
 
trained using the techniques 
in Fig. 4 (a)
 
and (b)
 
9
6
.8
 
From Table 
5
,
 
it can be 
observed
 
that 
our proposed method
 
outperforms
 
exis
t
ing
 
benchmark
 
models  for  action 
recognition
 
on 
the 
Penn Action dataset
 
[59]
.
 
V.
 
C
ONCLUSION
 
In this 
paper,
 
we
 
propose
 
the
 
HP model
 
for human parsing 
which conducts dense 
correspondence prediction
 
and part 
segmentation
. We also 
propose
 
a novel strategy for training
 
and testing
 
the 
AR model
. The 
HP model
 
performs almost as 
well as the current state
-
of
-
the
-
art
 
m
odel with a much 
simpler 
training scheme and less parameters
 
used
. 
The 
proposed training 
strategy
 
in Algorithm 1
 
reduces 
domain
 
discrepancie
s
 
by
 
enforcing the invariance 
to irrelevant clues
.
 
Moreover, 
the predictions from the 
HP model
 
enriches the 
input to the 
AR model
 
by providing 
dense localizations of 
human parts and textures.
 
T
he 
AR model
 
benefits from the 
novel strategies 
and
 
outperforms existing benchmark models 
with
 
a
 
higher robustness
 
and more stable predictions
.
 
VI.
 
W
ORKS 
C
ITED
 
 
[1] 
 
Wang, H. and Schmid, C., "Action recognition with improved 
trajectories," in 
IEEE Conference on Computer Vision and Pattern 
Recognition (CVPR)
, 2013, pp. 3551
-
3558. 
 
[2] 
 
Wang, H., Kläser, A., Schmid, C. and Liu, C. L., "Dense trajectories 
and  motion  boundary  descriptors  for  action  recognition," 
International journal of computer vision, 
vol. 103, no. 1, pp. 60
-
79, 
2013. 
 
[3] 
 
Wang, L., Qiao, Y. and Tang, X., "Action recogn
ition with trajectory
-
pooled  deep
-
convolutional descriptors," in 
IEEE conference on 
Computer Vision and Pattern Recognition (CVPR)
, 2015, pp. 4305
-
4314. 
 
[4] 
 
Carreira, J. and Zisserman, A., "Quo vadis, action recognition? a new 
model and the kinetics dat
aset," in 
IEEE Conference on Computer 
Vision and Pattern Recognition (CVPR)
, 2017, pp. 6299
-
6308. 
 
[5] 
 
Diba, A., Sharma, V. and Van Gool, L. , "Deep temporal linear 
encoding networks," in 
IEEE conference on Computer Vision and 
Pattern Recognition (CVPR)
,
 
2017, pp. 2329
-
2338. 
 
[6] 
 
Feichtenhofer, C., Pinz, A. and Zisserman, A., "Convolutional two
-
stream  network  fusion  for  video  action  recognition,"  in 
IEEE 
conference on Computer Vision and Pattern Recognition (CVPR)
, 
2016, pp. 1933
-
1941. 
 
[7] 
 
Ng, J. Y. 
H., Choi, J., Neumann, J. and Davis, L. S., "Actionflownet: 
Learning motion representation for action recognition," in 
IEEE 
Winter Conference on Applications of Computer Vision (WACV)
, 
2018, pp. 1616
-
1624. 
 
[8] 
 
Wang, L., Xiong, Y., Wang, Z., Qiao, Y., Li
n, D., Tang, X. and Van 
Gool, L., "Temporal segment networks: Towards good practices for 
deep action recognition," in 
European Conference on Computer 
Vision (ECCV)
, 2016. pp. 20
-
36. 
 
[9] 
 
Simonyan,  K.  and  Zisserman,  A.,  "Two
-
stream  convolutional 
networks for action recognition in videos," in 
Advances in neural 
information processing systems (NIPS)
, 2014, pp. 568
-
576. 
 
[10] 
Srivastava, N., Mansimov, E. and Salakhudinov, R., "Unsupervised 
l
earning of video representations using lstms," in 
International 
conference on machine learning (ICML)
, 2015, pp. 843
-
852. 
 
[11] 
Yue
-
Hei Ng, J. and Hausknecht, M., "Beyond short snippets: Deep 
networks for video classification," in 
IEEE conference on Comp
uter 
Vision and Pattern Recognition (CVPR)
, 2015, pp. 4694
-
4702. 
 
[12] 
Sharma, S., Kiros, R. and Salakhutdinov, R., "Action recognition 
using visual attention," in 
arXiv preprint 
, 2015, arXiv:1511.04119. 
 
[13] 
Ma, C. Y., Chen, M. H., Kira, Z. and AlRegib, G., "TS
-
LSTM and 
temporal
-
inception: Exploiting spatiotemporal dynamics for activity 
recognition," in 
Signal Processing: Image Communication
, 2019, 
Vol. 71, pp. 76
-
87. 
 
[14] 
Varol,  G.,  Laptev,  I.  and  Schmid, 
C.,  "Long
-
term  temporal 
convolutions for action recognition," 
IEEE transactions on pattern 
analysis and machine intelligence, , 
vol. 40, no. 6, pp. 1510
-
1517, 
2018. 
 
[15] 
Tran, D., Bourdev, L., Fergus, R., Torresani, L. and Paluri, M. , 
"Learning spatiot
emporal features with 3d convolutional networks," 
in 
IEEE international conference on computer vision (ICCV)
, 2015, 
pp. 4489
-
4497. 
 
[16] 
Klaser, A., Marszałek, M. and Schmid, C., "A spatil
-
Temporal 
descriptor  based  on  3D
-
Gradients,"  in 
British Machine 
Vision 
Conference (BMVC)
, 2008. 
 
[17] 
Zhu, J., Zhu, Z. and Zou, W., "End
-
to
-
end video
-
level representation 
learning  for  action  recognition.,"  in 
2018 24th International 
Conference on Pattern Recognition
, 2018. 
 
[18] 
Sun, S., Kuang, Z., Sheng, L., Ouyan
g, W. and Zhang, W., "Optical 
flow guided feature: a fast and robust motion representation for video 
action recognition," in 
IEEE conference on Computer Vision and 
Pattern Recognition (CVPR)
, 2018, pp. 1390
-
1399. 
 
[19] 
Soomro, K., Zamir, A. R. and Shah, 
M., "UCF101: A dataset of 101 
human actions classes from videos in the wild," in 
arXiv preprint 
, 
2012, arXiv:1212.0402. 
 
[20] 
Kuehne, H., Jhuang, H., Stiefelhagen, R. and Serre, T., "A large video 
database for human motion recognition," in 
High Performa
nce 
Computing in Science and Engineering 
‘12
, 2013, 
Vol. 4, No. 5, p. 6. 
[21] 
Abu
-
El
-
Haija, S., Kothari, N., Lee, J., Natsev, P., Toderici, G., 
Varadarajan, B. and Vijayanarasimhan, S., "Youtube
-
8m: A large
-
scale video classification benchmark," in 
arXi
v preprint 
, 2016, 
arXiv:1609.08675. 
 
[22] 
Loper, M., Mahmood, N., Romero, J., Pons
-
Moll, G. and Black, M. 
J., "SMPL: A skinned multi
-
person linear model," 
ACM transactions 
on graphics (TOG),, 
vol. 34, no. 6, p. 248, 2015. 
 
[23] 
Alp Güler, R., Neverova, N. and Kokkinos, I., "Densepose: Dense 
human  pose  estimation  in  the  wild,"  in 
IEEE Conference on 
Computer Vision and Pattern Recognition (CVPR)
, 2018, pp. 7297
-
7306. 
 
[24] 
Karpathy, A., Toderici, G., Shetty, S., Leung, T., Suktha
nkar, R. and 
Fei
-
Fei, L., "Large
-
scale video classification with convolutional 
neural networks," in 
IEEE conference on Computer Vision and 
Pattern Recognition (CVPR)
, 2014, pp. 1725
-
1732. 
 
[25] 
Fernando,  B.  and  Gould,  S.,  "Learning  end
-
to
-
end  video 
class
ification with rank
-
pooling," in 
International Conference on 
Machine Learning (ICML)
, 2016, pp. 1187
-
1196. 
 
[26] 
Wei, X. S., Zhang, C. L., Zhang, H. and Wu, J., "Deep bimodal 
regression of apparent personality traits from short video sequences," 
IEEE Tra
nsactions on Affective Computing, 
vol. 9, no. 3, pp. 303
-
315, 2018. 
 
[27] 
Zolfaghari, M., Singh, K. and Brox, T., "Eco: Efficient convolutional 
network for online video understanding," in 
European Conference on 
Computer Vision (ECCV)
, 2018, pp. 695
-
712. 
 
[28] 
Shi, Y., Tian, Y., Wang, Y., Zeng, W. and Huang, T., "Learning long
-
term dependencies for action recognition with a biologically
-
inspired 
deep network," in 
IEEE International Conference on Computer 
Vision (ICCV)
, 2017, pp. 716
-
725. 
 
[29] 
Sun, L.,
 
Jia, K., Chen, K., Yeung, D. Y., Shi, B. E. and Savarese, S., 
"Lattice Long Short
-
Term Memory for Human Action Recognition," 
in 
IEEE International Conference on Computer Vision (ICCV)
, 2017, 
pp. 2147
-
2156. 
 
[30] 
Donahue, J., Anne Hendricks, L., Guadarrama, S., Rohrbach, M., 
Venugopalan, S., Saenko, K. and Darrell, T., "Long
-
term recurrent 
convolutional networks for visual recognition and description," in 
IEEE conference on Computer Vision and Pattern Recognition 
(
CVPR)
, 2015, pp. 2625
-
2634. 
 
[31] 
Du, W., Wang, Y. and Qiao, Y., "RPAN: An end
-
to
-
end recurrent 
pose
-
attention network for action recognition in videos," in 
IEEE 
International Conference on Computer Vision (ICCV)
, 2017, pp. 
3725
-
3734. 
 
[32] 
Lea, C., Fl
ynn, M. D., Vidal, R., Reiter, A. and Hager, G. D. , 
"Temporal  convolutional networks for action  segmentation and 
detection," in 
IEEE Conference on Computer Vision and Pattern 
Recognition (CVPR)
, 2017, pp. 156
-
165. 
 
[33] 
Lan, Z., Zhu, Y., Hauptmann, A. G. and Newsam, S., "Deep local 
video  feature  for  action  recognition,"  in 
IEEE Conference on 
Computer Vision and Pattern Recognition Workshops (CVPRW)
, 2017, pp. 1
-
7. 
 
[34] 
Diba, A., Pazandeh, A. M. and Van Gool, L., "Effic
ient two
-
stream 
motion and appearance 3d cnns for video classification," in 
arXiv 
preprint 
, 2016, arXiv:1608.08851. 
 
[35] 
Tran, D., Ray, J., Shou, Z., Chang, S. F. and Paluri, M., "ConvNet 
Architecture Search for Spatiotemporal Feature Learning," in 
arX
iv 
preprint 
, 2017, arXiv:1708.05038. 
 
[36] 
Ji, S., Xu, W., Yang, M. and Yu, K., "3D convolutional neural 
networks for human action recognition," 
IEEE transactions on 
pattern analysis and machine intelligence,, 
vol. 35, no. 1, pp. 221
-
231, 2013. 
 
[37] 
Sun, L., Jia, K., Yeung, D. Y. and Shi, B. E., "Human action 
recognition  using  factorized  spatio
-
temporal  convolutional 
networks," in 
IEEE International Conference on Computer Vision 
(ICCV) 
, 2015, pp. 4597
-
4605. 
 
[38] 
Qiu, Z., Yao, T. and Mei, T., "Lear
ning spatio
-
temporal representation 
with  pseudo
-
3D  residual  networks,"  in 
IEEE International 
Conference on Computer Vision (ICCV)
, 2017, pp. 5533
-
5541. 
 
[39] 
Tran, D., Wang, H., Torresani, L., Ray, J., LeCun, Y. and Paluri, M., 
"A  closer  look  at  spatio
-
t
emporal  convolutions  for  action 
recognition," in 
IEEE conference on Computer Vision and Pattern 
Recognition (CVPR)
, 2018, pp. 6450
-
6459. 
 
[40] 
Wang, L., Li, W., Li, W. and Van Gool, L., "Appearance
-
and
-
relation 
networks for video classfication," in 
IEEE 
conference on Computer 
Vision and Pattern Recognition (CVPR)
, 2018, pp. 1430
-
1439. 
 
[41] 
Hara, K., Kataoka, H. and Satoh, Y. , "Can spatio
-
temporal 3D CNNs 
retrace the history of 2D CNNs and ImageNet?," in 
IEEE conference 
on Computer Vision and Pattern R
ecognition (CVPR)
, 2018, pp. 
6546
-
6555. 
 
[42] 
Wang, X., Girshick, R., Gupta, A. and He, K., "Non
-
local neural 
networks," in 
IEEE Conference on Computer Vision and Pattern 
Recognition (CVPR) 
, 2018. pp. 7794
-
7803. 
 
[43] 
Feichtenhofer, C., Pinz, A. and W
ildes, R. P., "Spatio
-
temporal 
multiplier  networks  for  video  action  recognition,"  in 
IEEE 
conference on Computer Vision and Pattern Recognition (CVPR)
, 
2017. pp. 4768
-
4777. 
 
[44] 
Girdhar, R., Ramanan, D., Gupta, A., Sivic, J. and Russell, B. , 
"ActionVLAD:  Learning spatio
-
temporal aggregation  for action 
recognition," in 
IEEE Conference on Computer Vision and Pattern 
Recognition (CVPR)
, 2017, pp. 971
-
980. 
 
[45] 
Sigurdsson,  G. A., 
Divvala,  S.,  Farhadi, A.  and  Gupta, A.  , 
"Asynchronous temporal fields for action recognition," in 
IEEE 
Conference on Computer Vision and Pattern Recognition (CVPR)
, 
2017, pp. 585
-
594. 
 
[46] 
Zhu, W., Hu, J., Sun, G., Cao, X. and Qiao, Y., "A key volume m
ining 
deep framework for action recognition," in 
IEEE Conference on 
Computer Vision and Pattern Recognition (CVPR)
, 2016, pp. 1991
-
1999. 
 
[47] 
Wang, Y., Long, M., Wang, J. and Yu, P. S., "Spatio
-
temporal pyramid 
network for video action recognition," in 
IEEE conference on 
Computer Vision and Pattern Recognition (CVPR)
, 2017, pp. 1529
-
1538 . 
 
[48] 
Girdhar,  R.  and Ramanan,  D.,  "Attentional pooling for action 
recognition," in 
Advances in Neural Information Processing Systems 
(NIPS)
, 2017, pp. 34
-
45. 
 
[49]
 
Cosmin Duta, I., Ionescu, B., Aizawa, K. and Sebe, N., "Spatio
-
temporal vector of locally max
-
pooled features for action recognition 
in videos," in 
IEEE Conference on Computer Vision and Pattern 
Recognition (CVPR)
, 2017, pp. 3205
-
3214. 
 
[50] 
Wu, C. Y.,
 
Zaheer, M., Hu, H., Manmatha, R., Smola, A. J. and 
Krähenbühl, P., "Compressed video action recognition," in 
IEEE 
Conference on Computer Vision and Pattern Recognition (CVPR)
, 
2018, pp. 6026
-
6035. 
 
[51] 
Ren, S., He, K., Girshick, R. and Sun, J., "Faster r
-
cnn: Towards real
-
time object detection with region proposal networks," in 
Advances in 
neural information processing systems (NIPS) 
, 2015, pp. 91
-
99. 
 
[52] 
He, K., G. Gkioxari, P. Dollar and R. Girshi
ck, "Mask R
-
CNN," in 
arXiv Preprint
, 2018, arXiv:1703.06870v3. 
 
[53] 
Zhao, H., Shi, J., Qi, X., Wang, X. and Jia, J., "Pyramid scene parsing 
network," in 
IEEE conference on Computer Vision and Pattern 
Recognition (CVPR)
, 2017, pp. 2881
-
2890. 
 
[54] 
Varo
l, G., Romero, J., Martin, X., Mahmood, N., Black, M. J., Laptev, 
I. and Schmid, C., "Learning from synthetic humans," in 
IEEE 
Conference on Computer Vision and Pattern Recognition (CVPR)
, 
2017, pp. 109
-
117. 
 
[55] 
Z. Zhou, Machine Learning, Peking: Qing hua da xue chu ban she, 
2016. 
 
[56] 
Ioffe, S. and Szegedy, C., "Batch normalization: Accelerating deep 
network training by reducing internal covariate shift," in 
arXiv 
preprint 
, 2015, arXiv:1502.03167. 
 
[57] 
Zha
o,  J.,  Li,  J.,  Cheng,  Y.,  Sim,  T.,  Yan,  S.  and  Feng,  J., 
"Understanding  Humans  in  Crowded  Scenes:  Deep  Nested 
Adversarial Learning and A New Benchmark for Multi
-
Human 
Parsing," in 
2018 ACM Multimedia Conference on Multimedia 
Conference
, 2018, pp. 792
-
800. 
 
[58] 
Gong, K., Liang, X., Zhang, D., Shen, X. and Lin, L., "Look into 
person:  Self
-
supervised  structure
-
sensitive  learning  and  a  new 
benchmark for human parsing," in 
IEEE Conference on Computer 
Vision and Pattern Recognition (CVPR)
, 2017, pp. 932
-
940 . 
 [59] 
Zhang, W., Zhu, M. and Derpanis, K. G., "From actemes to action: A 
strongly
-
supervised  representation  for  detailed  action 
understanding," in 
IEEE International Conference on Computer 
Vision (ICCV)
, 2013, pp. 2248
-
2255. 
 
[60] 
Jia, Y., Shelhamer, E
., Donahue, J., Karayev, S., Long, J., Girshick, 
R. and Darrell, T., "Caffe: Convolutional architecture for fast feature 
embedding,"  in 
Proceedings of the 22nd ACM international 
conference on Multimedia
, 2014, pp. 675
-
678. 
 
[61] 
Lan, Z., Lin, M., Li, X.,
 
Hauptmann, A. G. and Raj, B., "Beyond 
Gaussian  Pyramid:  Multi
-
skip  Feature  Stacking  for  Action 
Recognition," in 
IEEE conference on Computer Vision and Pattern 
Recognition (CVPR)
, 2015, pp. 204
-
212. 
 
[62] 
Wang,  L.,  Qiao,  Y.  and  Tang,  X.,  "MoFAP:  A  multi
-
level 
representation  for  action  recognition," 
International Journal of 
Computer Vision,, 
vol. 119, no. 3, pp. 254
-
271, 2016. 
 
[63] 
Bilen,  H.,  Fernando, B.,  Gavves,  E.  and Vedaldi, A.,  "Action 
recognition with dynamic image networks," 
IEEE transactions on
 
pattern analysis and machine intelligence,, 
vol. 40, no. 1, pp. 2799
-
2815, 2018. 
 
[64] 
Xiaohan Nie, B., Xiong, C. and Zhu, S. C., "Joint action recognition 
and pose estimation from video," in 
IEEE Conference on Computer 
Vision and Pattern Recognition (C
VPR)
, 2015, pp. 1293
-
1301 . 
 
[65] 
Iqbal, U., Garbade, M. and Gall, J., "Pose for action 
-
 
action for pose," 
in 
The 12th IEEE International Conference on Automatic Face and 
Gesture Recognition (FG 2017)
, 2017. 
 
[66] 
Cao, C., Zhang, Y., Zhang, C. and Lu, H., "Body joint guided 3
-
d 
deep  convolutional  descriptors  for  action  recognition," 
IEEE 
transactions on cybernetics, 
vol. 48, no. 3, pp. 1095
-
1108, 2017. 
 
 
 