Progressive Content-aware Coded Hyperspectral Compressive Imaging
Xuanyu Zhang1, Bin Chen1, Wenzhen Zou2, Shuai Liu3, Yongbing Zhang2, Ruiqin Xiong4, Jian Zhang1
1Peking University Shenzhen Graduate School2Harbin Institute of Technology (Shenzhen)
3Tsinghua University4Peking University
Abstract
Hyperspectral imaging plays a pivotal role in a wide
range of applications, like remote sensing, medicine, and
cytology. By acquiring 3D hyperspectral images (HSIs) via
2D sensors, the coded aperture snapshot spectral imaging
(CASSI) has achieved great success due to its hardware-
friendly implementation and fast imaging speed. However,
for some less spectrally sparse scenes, single snapshot and
unreasonable coded aperture design tend to make HSI re-
covery more ill-posed and yield poor spatial and spectral
fidelity. In this paper, we propose a novel Progressive
Content- Aware CASSI framework, dubbed PCA-CASSI ,
which captures HSIs with multiple optimized content-aware
coded apertures and fuses all the snapshots for reconstruc-
tion progressively. Simultaneously, by mapping the Range-
Null space Decomposition (RND) into a deep network with
several phases, an RND-HRNet is proposed for HSI recov-
ery. Each recovery phase can fully exploit the hidden phys-
ical information in the coded apertures via explicit R−N
decomposition and explore the spatial-spectral correlation
by dual transformer blocks. Our method is validated to sur-
pass other state-of-the-art methods on both multiple- and
single-shot HSI imaging tasks by large margins.
1. Introduction
Hyperspectral images (HSIs) embody rich spectral bands
and detailed information than the normal RGB images,
which have unprecedented demand in recent years [57, 2, 9,
19]. Inspired by the compressive sensing (CS) [67, 62, 25,
10, 59, 47, 46, 21], CASSI system aims to utilize 2D detec-
tor to capture 3D hyperspectral scenes. Due to the merits of
fast acquisition speed, low cost, and high data throughput, it
has played an indispensable role in a wealth of applications,
such as remote sensing, object detection, super-resolution,
and medical diagnosis [68, 38, 33, 24, 52, 15, 49, 1]. How-
ever, for some specific applications, the information re-
tained in a single snapshot is inadequate. For instance, mi-
Corresponding author: Jian Zhang (zhangjian.sz@pku.edu.cn).
Figure 1. Illustration of SD-CASSI and the proposed PCA-CASSI
system. SD-CASSI compresses the HSI via a single coded aper-
ture and tends to cause excessive information loss. However, our
PCA-CASSI captures the same HSI with multiple content-aware
coded apertures (produced by the mask predictor) progressively,
which significantly increases the information throughput.
croscopic imaging [4] has extremely high demands on the
textures and details of the HSIs. To guarantee the accuracy
of recovered images, capturing the same scene with multi-
ple shots is necessary and imperative. Simultaneously, with
the development of sampling devices, the improved digital
micromirror device (DMD) [51] and CCD sensors allow the
imaging system to record multiple snapshots and alter the
coded patterns with a limited increase in acquisition time.
Although previous works such as MS-CASSI [23] have
explored the potential of multiple shots, there are still sev-
eral bottlenecks to be solved. First , how to design optimal
multiple-coded apertures is the key to the enhancement of
multiple snapshot imaging systems. We focus on the fol-
lowing two aspects: 1)The coded apertures are expected
to exhibit excellent anisotropy and complementarity. The
complementary coded apertures promote the fusion and in-
teraction of the multiple snapshots, otherwise they may tend
to interfere with each other and undermine the beneficial
information; 2)Furthermore, the coded apertures are sup-
posed to be adjusted contextually. Prior information from
the previous shots enables coded apertures to perceive the
content of HSIs, thus increasing the flexibility and perfor-
mance of imaging systems. To the best of our knowledge,arXiv:2303.09773v1  [eess.IV]  17 Mar 2023these two aspects remain under-investigated. Second , ex-
isting reconstruction networks generally focus on the sin-
gle disperser CASSI system (SD-CASSI). The practical so-
lution for multiple-shot reconstruction is deficient. Mean-
while, how to utilize the coded aperture to retain the range
space and recover the null space of HSIs has been ignored.
To solve the above-mentioned issues, a novel
Progressive Content- Aware Coded Aperture Snapshot
Spectral Imaging framework, dubbed PCA-CASSI , is pro-
posed for progressive content-aware sampling and accurate
multiple-shot reconstruction (Fig. 1). From hardware per-
spective, PCA-CASSI can be easily implemented from the
original SD-CASSI [40] hardware with little modification.
From algorithm perspective, PCA-CASSI is composed of
the light-weight recovery network and mask predictor. Fur-
thermore, an Range- Null space Decomposition unfolding
Hyperspectral Reconstruction Network (RND-HRNet) is
proposed to fuse all the coded measurements adaptively.
Overall, our contributions are summarized as:
• A novel “Encoder + Decoder” framework, dubbed
PCA-CASSI, is proposed to capture HSIs contextu-
ally and reconstruct them progressively. Noted that the
“Encoder” corresponds to the HSI sampling while the
“Decoder” actually indicates the HSI reconstruction.
• A progressive content-aware sampling strategy is pro-
posed to perceive HSI contents and optimize the coded
aperture in the current shot from the measurement in
the previous shot.
• AnR−Ndecomposition-inspired network, dubbed
RND-HRNet, is presented to reconstruct HSIs, which
utilizes the range-null space decomposition module
(RNDM) to refine the null space of the HSIs iteratively
and adopts the spectral spatial fusion module (SSFM)
to exploit the non-local spatial-spectral information.
• Experiments exhibit that our method outperforms
other state-of-the-art methods on both multiple- and
single-shot HSI reconstruction tasks by large margins.
2. Related Works
2.1. Hyperspectral Imaging Systems
Conventional hyperspectral cameras usually make a
trade-off between temporal and spectral resolution. Inspired
by the theory of compressive sensing (CS), CASSI aims to
capture hyperspectral images (HSIs) within a snapshot time.
In what follows, we will review some representative works
on imaging optical path and coded aperture design.
Imaging Optical Path: The most representative CASSI
system [40] utilized a single disperser to encode spatial and
spectral information. To improve the imaging performance
and information throughput, Kittle et al. [23] captured thesame hyperspectral scenes via varied coded apertures. In
addition, by temporally aligning the single-shot CASSI sys-
tem with an RGB camera, Wang et al. [44] provided multi-
modal supervision and complementary information for HSI
reconstruction. Recently, Lin et al. [28, 27] utilized two
high speed spatial light modulators to realize dual-optical
coding. Although the optical path design of CASSI has
achieved remarkable results, ensuring high compression ra-
tio and excellent image quality is still a bottleneck.
Coded Aperture Design: Wuet al. [51] first introduced the
digital micromirror device (DMD) into the imaging systems
to realize flexible and fast dynamic coding. Benefiting from
the rapid response time of DMD, the theory of Restricted
Isometry Property (RIP) [17] was introduced to guide the
coded aperture optimization. To make the coded aperture
retain enough useful information and remove redundancy,
Zhang et al. [66] combined the coded aperture optimization
and HSI reconstruction with an united network to achieve
efficient mask optimization. However, as the complex scene
varies, designing adaptive, content-aware and task-driven
coded apertures is worthy of exploration.
2.2. HSI Reconstruction Methods
Model-based Methods: The traditional model-based meth-
ods employ the regularization term inspired by the image
prior to solve the ill-posed inverse problem iteratively with
widely-used optimization frameworks, such as ISTA [3],
GAP [56] and so on [31, 39]. Simultaneously, to improve
the representation ability of the model, TV [56], GMM
[54, 53], image sparsity representation [45] and rank min-
imization [29] are embedded into the above optimization
frameworks to provide prior information. Although these
methods produce decent results in specific scenarios, it is
difficult to design suitable hand-crafted priors for all scenes.
Deep Learning-based Methods: Owing to the powerful
representation capabilities of deep networks, learning-based
HSI reconstruction methods [36] have received increasing
attention. To improve the accuracy and perceptual qual-
ity, residual blocks [41], spatial-spectral attention modules
[34], long-short-term memory units [13] and Fourier do-
main constraint [20] are embedded into the structure of con-
volution neural networks (CNNs). Although they can cap-
ture the local image cues, CNNs fail to exploit the global
correlation and long-range dependencies. Recently, thanks
to the wide application of vision transformers, spatial-wise
and channel-wise transformers have been incorporated into
the multi-scale encoder-decoder architectures [6, 5]. To
alleviate the poor interpretability of these end-to-end net-
works, some researchers have attempted to combine opti-
mization algorithms and deep network priors, such as plug-
and-play frameworks [58, 69, 35]. Meanwhile, deep unfold-
ing methods [42, 43, 64, 8, 22, 66] have been prevalent for
its exquisite design and powerful performance. AlthoughFigure 2. Illustration of the proposed PCA-CASSI. (a) overall architecture, (b) HR(⋅), (c)H(i)
M(⋅).HR(⋅)takes multiple snapshots {yi}N
i=1
and coded apertures {Mi}N
i=1to generate the reconstructed HSI ˆx, which includes KRND-inspired recovery phases. It is also adapted to
single-shot reconstruction. H(i)
M(⋅)takes the snapshot yi−1from previous shot to predict the coded aperture Miin the current shot.
these methods have achieved great success, few attention
has been paid to the multiple-shot HSI reconstruction.
3. Proposed Method
3.1. Review of the CASSI system
In CASSI, the 3D hyperspectral cube is first modulated
via a coded aperture and then dispersed via a dispersive ele-
ment (Fig. 1). Given a HSI sequence {Xi}C
i=1∈RH×W, each
frame is modulated via a coded aperture M∈RH×W:
X′
i=M⊙Xi, (1)
where X′
iis the ithmodulated HSI frame and ⊙de-
notes the Hadamard product. The modulated HSI frames
{X′
i}C
i=1∈RH×Wwith different wavelengths are then shifted
spatially and summed in an element-wise manner, leading
to a coded measurement:
Y(m,n)=C
∑
i=1X′
i(m,n+di)+N, (2)
where(m,n)is the spatial coordinates, and didenotes the
shifting distance of the ithchannel. N∈RH×(W+C−1)and
Y∈RH×(W+C−1)denote the noise and the coded measure-
ment, respectively. The vectorized form of CASSI is:
y=Φx+n, (3)
where x,yandndenote the vectorized form of X,Yand
N, respectively. Φrepresents the sensing matrix.
The above imaging system can be extended to multiple
shots. For instance, if we capture the same scene Xvia
Nmultiple different code apertures {Mi}N
i=1, every shot
can be treated as an implementation of the CASSI system.
Hereby, the imaging scheme can be formulated as follows:
[y⊺
1,y⊺
2,...,y⊺
N]⊺=[Φ⊺
1,Φ⊺
2,...,Φ⊺
N]⊺x+n, (4)
where yiandΦi=Mask2Mat (Mi)denote the observedHSI measurement and sensing matrix of the ithshot, re-
spectively. Mask2Mat (⋅)transforms the physical mask Mi
to its equivalent sensing matrix form Φi. The degradation
process of multiple-snapshot compressive imaging remains
a linear model and can still be expressed as y=Φx+n.
3.2. Overview of the Proposed PCA-CASSI
As illustrated in Fig. 2, PCA-CASSI is a novel “Encoder
+ Decoder” framework, which aims to compress HSIs by
various content-aware optimal masks to increase the infor-
mation throughput and fuse all the captured snapshots for
HSI restoration cooperatively. Note that following [5, 6],
this work just focuses on hyperspectral image sampling and
restoration, and our contributions may be extensible to other
imaging systems like [40, 23, 65]. For software algorithm, it
is composed of two sub-modules, namely the mask predic-
tors{H(i)
M(⋅)}N
i=1and reconstruction network HR(⋅). The
mask predictors {H(i)
M(⋅)}N
i=1are capable of generating the
optimal mask contextually in the current shot from the mea-
surement in the previous shot, which is formulated as:
Mi=H(i)
M(yi−1), i∈{2,3,⋯}, (5)
where Midenotes the mask in the ithshot and yi−1repre-
sents the measurement in the (i−1)thshot.M1is a learn-
able parameter. We will elaborate this module in Sec. 3.3.
Simultaneously, HR(⋅)is designed to integrate all the coded
snapshots based on the R−NDecomposition (RND) the-
ory, which is formulated as follows:
ˆx=HR(y1,M1,y2,M2,...,yN,MN), (6)
where ˆxdenotes the reconstruction result. We will elabo-
rate it in Sec 3.4. For hardware realization, the proposed
PCA-CASSI can be easily implemented via the acquisition
devices of SD-CASSI [40]. Owing to the rapid responseTable 1. High-level functional comparison among three represen-
tative imaging systems and our proposed PCA-CASSI.
Method SD-CASSI [40] DCD [65] MS-CASSI [23] PCA-CASSI
Single CCD sensor
Multiple mask switch
Progressive sampling
Content-aware mask
time of 2D sensors and the fast prediction speed of H(i)
M(⋅),
the increased time caused by multiple shots is indeed lim-
ited. Considering its superior performance, it is still worth
sacrificing a little sampling speed for higher imaging per-
formance in some applications. As illustrated in Tab. 1, our
framework is distinguished from some previous works. Dif-
ferent from MS-CASSI [23] which utilizes parallel multi-
ple coded apertures in the optical path, the proposed PCA-
CASSI can achieve progressive sampling sequentially and
be easily extended to any number of shots. Different from
DCD [18, 65], which uses dual cameras to capture snap-
shots and grayscale images simultaneously, our framework
requires only one camera and is more flexible with respect
to the coded apertures. Hence, the proposed PCA-CASSI is
hardware-friendly in system implementation, content-aware
in coded aperture design, and effective in HSI recovery.
3.3. Progressive Content-aware Sampling
Apart from the fusion mechanism in the restoration pro-
cess, coded aperture design plays a pivotal role in PCA-
CASSI, which enables different measurements to con-
tain complementary information and maintain excellent
anisotropy. Considering that different HSIs have various
spectral correlations and spatial sparsities, the coded mea-
surement from the previous shot is utilized to optimize the
coded aperture in the current shot. Thereby, the optimized
coded apertures tend to perceive the photographed HSIs and
make content-aware adjustments adaptively. As the shot
epoch progresses, the optimized coded apertures tend to be
more reasonable with more accurate reconstruction results.
Specifically, as shown in Fig. 2 (a), the hyperspectral
scene xis firstly captured by the mask M1to obtain the
measurement y1. Then, the coarse compressed result y1is
adopted to furnish prior information and predict subsequent
maskM2. Due to the guidance of y1,M2is able to perceive
the content of the HSIs and refine the coded pattern. Gener-
ally, the coded aperture Miin the ithshot is generated from
the measurement yi−1via the mask predictor H(i)
M(⋅). To be
noted, the optimized mask in each shot is required to reflect
both the shared properties of the imaging system and the
independent characteristics of each HSI. Hence, the opti-
mized masks {Mi}N
i=1are decoupled into two components,
namely shared component and content-aware component.
The shared components {Mi}N
i=1are learnable parameters
and jointly optimized in the network. The content-aware
component is derived from the previous measurement yi−1
via a lightweight deep module H′(i)
M(⋅), composed of threeConv-ReLU layers, a normalization layer and a mean layer
(Fig. 2 (c)). The normalization layer transforms all pixels to
the interval [0, 1]. The mean layer calculates the channel-
wise means of features to produce the masks. The above
pipeline, corresponding to Eq. 5, is implemented as:
Mi=H(i)
M(yi−1)=Mi+η(i)⋅H′(i)
M(yi−1), (7)
where η(i)denotes a learnable parameter to stabilize the
network training. By means of our progressive content-
aware sampling, each snapshot tends to capture comple-
mentary physical information and spectral features.
3.4. Architecture of the Proposed RND-HRNet
The proposed RND-HRNet aims to recover the hyper-
spectral scene xfrom the snapshots {yi}N
i=1and use coded
apertures {Mi}N
i=1to guide the transmission of spectral-
spatial features. To start with the noise-free degradation
model y=Φx, the HSI reconstruction is formulated as:
ˆx=arg min
x1
2∥y−Φx∥2
2+λψ(x), (8)
where y=[y⊺
1,y⊺
2,...,y⊺
N]⊺denotes the concatenation of
all vectors of measurements. The first part is the data fi-
delity term, and the second part λψ(x)denotes the regu-
larization term. Following the traditional ISTA framework
[60, 61], Eq. 8 can be solved by iterating between the fol-
lowing gradient descent and proximal mapping steps:
r(k)=x(k−1)−ρΦ⊺(Φx(k−1)−y), (9)
x(k)=proxλψ(r(k)), (10)
where r(k)andx(k)denote the intermediate result and re-
construction image in the kthphase, respectively. kand
ρdenote the number of ISTA iterations and the step size,
respectively. The limitation of the above Eq. 9 is that gradi-
ent descent can only find an approximate sub-optimal solu-
tion to the data fidelity term in each iteration, which cannot
strictly satisfy y≡Φr(k). To tackle with the bottleneck of
gradient descent, inspired by [11, 12], the R−NDecompo-
sition (RND) is introduced to explicitly maintain the con-
sistency constraint of the data fidelity term. Given a sensing
matrix Φand its pseudo inverse matrix Φ†, which satisfies
ΦΦ†=I, we have the following theorem for arbitrary HSIs:
Theorem 1 ([11]) R−NDecomposition: Let Pr≜Φ†Φ
be the operator that projects the sample xfrom sample do-
main to the range of Φ†, and denote by Pn≜(I−Φ†Φ)
the operator that projects xto the null space of Φ. Then
∀x∈RH×W×C, there exists the unique decomposition:
x≡Pr(x)+Pn(x), (11)
wherePr(x)andPn(x)respectively denote the range and
null space of x. The HSI reconstruction can be treated as
solving these two components Pr(x)andPn(x). Recon-
structing the range space of the HSI signal is to ensure theFigure 3. Details of the proposed RNDM. It retains the range-space
component and recovers the null-space component of HSIs.
data consistency with respect to measurement y, while re-
fining the null space of the HSI signal aims to remove ar-
tifacts and enrich details. Therefore, the core advantage of
RND lies in its ability to improve perceptual quality while
maintaining reconstruction fidelity. Furthermore, substitut-
ingy=Φxinto Eq. 11, we have the following RND for x:
x≡Φ†y+(I−Φ†Φ)x. (12)
In HSI reconstruction, the range-space component Φ†y
can actually be calculated from y, while the null-space
component [(I−Φ†Φ)x]can be refined and estimated by
the network. To be noted, the combination of Φ†yand
any solution sfor the null-space component projected by
(I−Φ†Φ)strictly enjoys the exact data consistency, i.e.
∀s,Φ[Φ†y+(I−Φ†Φ)s]≡y. Thus, our motivation is to
estimate the null-space component [(I−Φ†Φ)x]only and
remain the clean range-space component unchanged to alle-
viate the recovery difficulty of networks. Furthermore, we
unfold the null-space learning in Eq. 12 into a deep network
and resort to the proximal mapping to refine the null-space
correlated component iteratively as follows.
z(k)=Φ†y+(I−Φ†Φ)x(k−1)
=x(k−1)+Φ†(y−Φx(k−1)),(13)
x(k)=proxλψ(z(k)), (14)
where z(k)andx(k)respectively denote the result of RND
and proximal mapping in the kthiteration. Inspired by
Eq. 13 and Eq. 14, the range-null decomposition module
(RNDM) and spatial-spectral fusion module (SSFM) are
proposed as follows for accurate HSI reconstruction.
3.4.1 Range-Null Space Decomposition Module
Inspired by Eq. 13, our RNDM aims to retain the range
space and refine the null space of the HSIs iteratively
(Fig. 3). Given the snapshots {yi}N
i=1and masks {Mi}N
i=1,
RNDM yields the intermediate result z(k)flexibly.
z(k)=RNDM(k)(x(k−1),y1,...,yN,M1,...,MN).
(15)Due to the cumulative errors caused by equipment de-
viation, noise corruption and alignment of the continuous
spectrum in real scenes, using physical masks {Mi}N
i=1di-
rectly may be imprecise. Therefore, a deep enhanced mod-
uleH(k)
EM(⋅)is introduced to produce two enhanced mask
representations F(k)
i∈RH×W×CandE(k)
i∈RH×W×Cfrom
the physical mask Mi∈RH×W, which corrects the bias be-
tween the degradation process and the physical mask via an
attention mechanism [6] as follows.
F(k)
i,E(k)
i=H(k)
EM(Mi). (16)
Furthermore, the crucial step of RNDM is to solve the
degradation operator Hd(⋅,⋅)and its pseudo-inverse opera-
torH†
d(⋅,⋅). Specifically, Hd(⋅,⋅)simulates the process of
mask modulation, dispersion, and compression in Sec. 3.1
with the current state x(k−1)and enhanced mask representa-
tionsF(k)
i.H†
d(⋅,⋅)is designed to provide an initialization
from 2D signals to 3D cubes with the enhanced represen-
tation E(k)
i.More details about these two operators are
presented in SM . To dynamically balance the contribution
of range- and null-space signals, the learnable parameter
ρ(k)is incorporated into the optimization process. Hence,
we can do R−Ndecomposition on each yito get the in-
termediate results {z(k)
i}N
i=1. Then, an 1×1convolution is
utilized to adaptively merge {z(k)
i}N
i=1intoz(k)as follows,
which is fed to the subsequent proximal mapping module.
v(k)
i=yi−Hd(x(k−1)
i,F(k)
i),
z(k)
i=x(k−1)
i+ρ(k)H†
d(v(k)
i,E(k)
i),
z(k)=Conv(k)([z(k)
1,...,z(k)
N]),(17)
where v(k)
iand[⋅]respectively denote the auxiliary variable
and the channel-wise concatenation operation. The mea-
surements compressed by various coded patterns reflect dif-
ferent HSI contents and fuse complementary information.
3.4.2 Spectral-Spatial Fusion Module
To implement Eq. 14 with deep networks, SSFM is adopted
to refine the fused intermediate result z(k)and yield the
reconstruction result x(k)(Fig. 5). Given that HSI rep-
resentations are spatially sparse and spectrally correlated,
capturing spatial interactions and spectral correlation are
just as important. Inspired by the previous works [6, 63],
the channel-wise and spatial-wise transformer blocks are
plugged into the U-shaped architecture. The spatial-wise
transformer block (STB) fuses the swin transformer block
and residual convolution blocks to integrate local and non-
local information. The channel-wise transformer block
(CTB) encodes each feature frame into a token and exploresTable 2. Comparison results of the proposed RND-HRNet and state-of-the-art HSI reconstruction methods on the single-shot reconstruction.
The best and second-best scores are highlighted and underlined .
GAP-TV [56] TSA-Net [34] DGSMP [22] SRN [41] HDNet [20] MST [6] MST++ [7] CST [5] RND-HRNet RND-HRNetTesting setICIP 2016 ECCV 2020 CVPR 2021 Arxiv 2021 CVPR 2022 CVPR 2022 CVPRW 2022 ECCV 2022 2 phases 10 phases
Scene01 26.82 / 0.754 32.03 / 0.892 33.26 / 0.915 34.85 / 0.937 35.14 / 0.935 35.40 / 0.941 35.80 / 0.943 35.16 / 0.938 36.15 / 0.948 37.29 /0.959
Scene02 22.89 / 0.610 31.00 / 0.858 32.09 / 0.898 35.11 / 0.935 35.67 / 0.940 35.87 / 0.944 36.24 / 0.947 35.60 / 0.942 37.34 / 0.956 40.07 /0.974
Scene03 26.31 / 0.802 32.25 / 0.915 33.06 / 0.925 35.89 / 0.949 36.03 / 0.943 36.51 / 0.953 37.39 / 0.957 36.57 / 0.953 38.47 / 0.963 41.48 /0.972
Scene04 30.65 / 0.852 39.19 / 0.953 40.54 / 0.964 42.12 / 0.975 42.30 / 0.969 42.27 / 0.973 43.85 / 0.973 42.29 / 0.972 43.95 / 0.975 45.59 /0.985
Scene05 23.64 / 0.703 29.39 / 0.884 28.86 / 0.882 32.53 / 0.944 32.69 / 0.946 32.77 / 0.947 33.41 / 0.952 32.82 / 0.948 34.57 / 0.959 36.08 /0.971
Scene06 21.85 / 0.663 31.44 / 0.908 33.08 / 0.937 34.59 / 0.955 34.46 / 0.952 34.80 / 0.955 35.43 / 0.957 35.15 / 0.956 35.82 / 0.961 37.34 /0.972
Scene07 23.76 / 0.688 30.32 / 0.878 30.74 / 0.886 33.52 / 0.924 33.67 / 0.926 33.66 / 0.925 34.35 / 0.934 33.85 / 0.927 35.37 / 0.943 37.27 /0.960
Scene08 21.98 / 0.655 29.35 / 0.888 31.55 / 0.923 32.63 / 0.947 32.48 / 0.941 32.67 / 0.948 33.71 / 0.953 33.52 / 0.952 33.95 / 0.957 35.55 /0.970
Scene09 22.63 / 0.682 30.01 / 0.890 31.66 / 0.911 35.04 / 0.944 34.89 / 0.942 35.39 / 0.949 36.67 / 0.953 35.28 / 0.946 37.57 / 0.961 39.99 /0.972
Scene10 23.10 / 0.584 29.59 / 0.874 31.44 / 0.925 31.99 / 0.938 32.38 / 0.937 32.50 / 0.941 33.38 / 0.945 32.84 / 0.940 33.46 / 0.945 34.69 /0.960
Average 24.36 / 0.669 31.46 / 0.894 32.63 / 0.917 34.82 / 0.945 34.97 / 0.943 35.18 / 0.948 36.02 / 0.951 35.31 / 0.947 36.66 / 0.957 38.54 /0.969
Figure 4. The visual comparison of the proposed RND-HRNet and other SOTA methods on the single-shot reconstruction.
Figure 5. Details of the proposed SSFM. It extracts the spectral-
spatial features with high-throughput transmission h(k−1).
spectral self-attention. To avoid information loss and model
degradation, a feature interaction model (FIM) [66] is incor-
porated into the SSFM to interact with the spatial-spectral
features in other phases. Furthermore, since feature maps
at each scale of the U-shaped architecture also have well-
preserved spatial information, the multi-scale features in
other layers are also utilized for feature fusion. Noted that
the concatenation of the spatial-spectral features and multi-
scale features in the kthphase are denoted by h(k). The
cascaded features in the previous phase are conducive to
the reconstruction of the current phase. More details are
presented in SM . Finally, our SSFM is summarized as:
h(k),x(k)=SSFM(k)(z(k),h(k−1)). (18)3.5. Network Training and Implementation
Without bells and whistles, the proposed reconstruc-
tion network HR(⋅)and mask predictors {H(i)
M(⋅)}N
i=1are
jointly optimized via a common MSE loss. Specifically,
given the training data {xi}Nd
i=1, the loss function is defined:
L(Θ)=1
HWCN dNd
∑
i=1∥ˆxi−xi∥2
2, (19)
where ˆxiandNdare the recovered result and train-
ing sample number. Θ=[⋃i∈{1,⋯,N}H(i)
M]∪HR
denotes the set of all learnable parameters with
H(i)
M={Mi,η(i),H′(i)
M},HR={RNDM(k),SSFM(k)}K
k=1,
where RNDM(k)={ρ(k),H(k)
EM,Conv(k)}. All the parame-
ters are indiscriminately learned in an end-to-end manner.
The Adam [16] optimizer is employed for the training of
200 epochs. The learning rate is initialized to 4×10−4and
scheduled to 1×10−6using cosine annealing.
4. Experimental Results
4.1. Experimental Settings
In this paper, the effectiveness of the proposed method
has been verified on both simulation datasets and the realTable 3. Comparison results of the proposed method and state-of-the-art HSI reconstruction methods on the two-shot reconstruction. The
best and second-best scores are highlighted and underlined . Here,HR(⋅)in PCA-CASSI and RND-HRNet both include 2 phases.
GAP-TV [56] ADMM-TV [56] TSA-Net [34] SRN [41] HDNet [20] MST [6] MST++ [7] CST [5] RND-HRNet PCA-CASSITesting setICIP 2016 ICIP 2016 ECCV 2020 Arxiv 2021 CVPR 2022 CVPR 2022 CVPRW 2022 ECCV 2022 Ours Ours
Scene01 27.62 / 0.739 27.85 / 0.768 33.62/0.912 34.79 / 0.930 35.20 / 0.941 35.50 / 0.942 35.89 / 0.947 35.57 / 0.939 37.49 / 0.961 39.49 /0.970
Scene02 25.92 / 0.665 25.65 / 0.684 32.51/0.893 35.09 / 0.920 36.00 / 0.945 36.00 / 0.939 36.70 / 0.946 35.99 / 0.940 39.78 / 0.969 43.52 /0.984
Scene03 23.65 / 0.762 23.84 / 0.791 33.89/0.932 34.50 / 0.930 35.01 / 0.936 36.38 / 0.943 36.44 / 0.947 35.94 / 0.943 39.01 / 0.957 40.91 /0.968
Scene04 34.20 / 0.872 33.56 / 0.886 40.40/0.964 40.94 / 0.955 41.19 / 0.968 43.16 / 0.975 43.56 / 0.975 42.32 / 0.965 44.49 / 0.980 47.44 /0.988
Scene05 23.90 / 0.708 23.94 / 0.732 31.05/0.917 32.21 / 0.925 32.76 / 0.946 33.31 / 0.946 33.18 / 0.948 33.25 / 0.945 35.62 / 0.965 38.45 /0.978
Scene06 23.97 / 0.670 23.85 / 0.698 33.04/0.930 35.63 / 0.938 36.10 / 0.958 35.91 / 0.954 36.52 / 0.960 36.36 / 0.955 38.47 / 0.970 40.15 /0.980
Scene07 23.46 / 0.682 23.60 / 0.712 32.06/0.902 33.63 / 0.919 33.44 / 0.917 33.70 / 0.916 33.96 / 0.918 34.02 / 0.921 36.27 / 0.945 38.72 /0.963
Scene08 23.68 / 0.654 23.93 / 0.695 31.37/0.919 34.45 / 0.933 34.96 / 0.958 35.07 / 0.951 35.58 / 0.961 35.62 / 0.954 37.58 / 0.971 39.48 /0.981
Scene09 25.18 / 0.708 25.04 / 0.743 32.27/0.913 34.27 / 0.930 34.42 / 0.937 36.15 / 0.944 35.89 / 0.945 35.52 / 0.942 38.69 / 0.963 40.35 /0.973
Scene10 24.22 / 0.589 24.54 / 0.603 30.42/0.887 33.82 / 0.942 33.95 / 0.958 34.50 / 0.954 34.67 / 0.958 34.83 / 0.956 36.54 / 0.971 38.78 /0.981
Average 25.58 / 0.705 25.58 / 0.731 33.06/0.917 34.93 / 0.932 35.30 / 0.947 35.97 / 0.946 36.24 / 0.951 35.94 / 0.946 38.39 / 0.965 40.73 /0.977
Figure 6. The visual comparison of the proposed method and other SOTA methods on the multiple-shot reconstruction.
dataset. Following [34], the simulation experiments are
conducted on the public HSI datasets CA VE [55] and
KAIST [14] with the size 256×256×28. Meanwhile, 5 real
compressive measurements [22] with the size of 660×714
are used for testing. The metrics of PSNR and SSIM [48]
are employed to evaluate the reconstruction quality.
4.2. Simulation Results
To demonstrate the effectiveness of the proposed PCA-
CASSI and RND-HRNet, we compare our methods with
other state-of-the-art (SOTA) methods, including model-
based methods [56], classical CNN-based methods [41, 34,
22, 20] and recent transformer networks [6, 5, 7].
Single-shot Reconstruction: Following the settings of pre-
vious works [34, 22], we conduct the single-shot recon-
struction on the KAIST dataset [14]. It can be clearly seen
that the proposed RND-HRNet with 10 phases yields 38.54
dB on PSNR and 0.969 on SSIM, which significantly out-
performs the most recent works [6, 5, 7] over 2dB . To be
noted, even with only 2 phases, the proposed RND-HRNet
can also achieve the current best performance. As exhibited
in Fig. 4, our method can recover sharper edges and more
realistic details, which indicates that the proposed RNDM
and SSFM can accurately maintain data consistency and ef-Table 4. Evaluation of computational complexity on multiple- and
single-shot reconstructions.“Ph” denotes the number of Phases.
Method Shot Testing time Params. PSNR SSIM
HDNet [20] #1 116.92ms 2.37M 34.97dB 0.943
MST [6] #1 176.36ms 2.03M 35.18dB 0.948
CST [5] #1 153.18ms 1.36M 35.31dB 0.947
RND-HRNet-2Ph #1 189.13ms 1.82M 36.66dB 0.957
RND-HRNet-10Ph #1 531.37ms 9.48M 38.54dB 0.969
HDNet [20] #2 117.45ms 2.37M 35.30dB 0.947
MST [6] #2 179.25ms 2.03M 35.97dB 0.946
CST [5] #2 159.21ms 1.36M 35.94dB 0.946
RND-HRNet-2Ph #2 199.21ms 1.82M 38.39dB 0.965
PCA-CASSI-2Ph #2 249.45ms 1.84M 40.73dB 0.977
fectively exploit the non-local correlation.
Multiple-shot Reconstruction: In this section, we extend
the RND-HRNet to multiple-shot with the proposed pro-
gressive content-aware sampling. For PCA-CASSI, two op-
timized masks are utilized to capture HSIs respectively. For
other methods, we adopt two fixed real masks, namely M
[34] and 1−M, to realize the two-shot imaging. To adapt ex-
isting methods to multiple-shot reconstruction, we utilize a
few1×1convolutions to fuse two measurements and masks
in learning-based methods (SRN, HDNet, MST, CST). Be-
sides, for model-based methods (GAP-TV , ADMM-TV),
we jointly solve these two sub-optimization problems.
As shown in Tab. 3, all reconstruction methods have
achieved corresponding improvements compared to single-Figure 7. Visualization results of the optimized M1andM2,
where M2=M2+η(2)⋅H′(2)
M(y1). For clearer presentation,
all the element values of H′(2)
M(y1)are magnified by 10.
Figure 8. Real data results of the proposed RND-HRNet and other
SOTA methods on the single-shot reconstruction.
shot results listed in Tab. 2. However, since the fusion
method does not fully utilize the complementary informa-
tion of multiple masks, some methods do not reach their
limit of performance. Owing to the proposed RND-inspired
fusion mechanism, RND-HRNet with 2 phases achieves
38.39dB on PSNR and 0.965 on SSIM, which surpasses
1.73 dB than the single-shot case. Noted that even utiliz-
ing the exact same system and masks, the proposed RND-
HRNet also outperforms the SOTA method MST++ [7] by
2.15 dB on PSNR. Considering the trade-off between com-
putational complexity and performance, RND-HRNet with
10 phases is not conducted on the multiple-shot case. Fur-
thermore, utilizing the content-aware optimized masks, the
proposed PCA-CASSI achieves very impressive results, i.e.
40.73dB on PSNR and 0.977 on SSIM. Compared with two
recent SOTA methods CST [5] and MST [6], our method
surpasses them by 4.79dB and 4.76dB on PSNR, which ver-
ifies the effect of the proposed imaging framework. As de-
picted in Fig. 6, the HSIs produced by our methods have
clearer spatial details and more accurate spectral consis-
tency. Meanwhile, Fig. 7 presents the content-aware opti-
mized masks. An interesting finding is that the optimized
mask M2contains a shared pattern M2and veils some
content-aware textures H′(2)
M(y1).
Computational complexity: As listed in Tab. 4, the pro-
posed RND-HRNet achieves greater performance than that
of CST and MST with the same level of average testing time
and even less parameters, which verifies the effect of the
proposed RND-HRNet. With the mask predictor, the pro-
posed method with 2 phases can obtain 2.34dB gains on
PSNR with only 0.02M increase in parameters, proving the
practicality of the progressive content-aware sampling.
4.3. Real Data Results
To objectively evaluate the proposed method on real
data, RND-HRNet is tested in 5 real measurements [22].Table 5. Evaluation of different mask combinations. Here, the re-
construction network includes 1 phase for simplicity. More results
of the N-shot cases ( N≥3) are presented in our SM.
Mask TypeCaseFixed mask Optimized maskContent-aware PSNR SSIM
1 1 0 35.04 0.948
2 0 1 36.33 0.957
3 1 1 38.29 0.967
4 2 0 35.73 0.953
5 0 2 39.19 0.972
6 0 2 39.62 0.974
To simulate the real imaging situations, 11-bit shot noise
is injected into the training data [34]. As shown in Fig. 8,
our results can reconstruct clearer HSI contents and detailed
textures with fewer artifacts and noise, especially in the
cropped regions such as flowers and strawberries, suggest-
ing the generalization ability and robustness of our method.
4.4. Ablation Study
To evaluate the contribution of the proposed PCA-
CASSI, we deploy the CASSI system on different mask
combinations. For convenience, the reconstruction network
is an one-phase RND-HRNet. Tab. 5 indicates the PSNR
and SSIM on different mask combinations.
Multiple-shot vs. Single-shot: Comparing case 1 and case
4 or case 2 and case 5, HSI reconstruction with two fixed
or optimized masks surpasses one-shot reconstruction by
0.69dB or 2.86dB on PSNR, suggesting that multiple com-
plementary coded apertures capture richer information.
Optimized vs. Fixed: As shown in Tab. 5, the more opti-
mized masks are used, the better the reconstruction perfor-
mance is, indicating that more optimized masks better re-
move redundancy and retain useful information (case 2,3,5).
Content-aware vs. Shared: Comparing case 5 and case 6,
we find that although randomly optimized masks are con-
ducive to information acquisition, the same coded apertures
are shared and non-adaptive for all HSI scenes, which in-
evitably leads to the loss of spectral anisotropy. With the
proposed progressive content-aware strategy, HSI recon-
struction with content-aware optimized masks can surpass
the results with shared optimized masks by 0.43dB, which
proves the effect of our mask optimization algorithm.
5. Conclusion
In this paper, we propose a novel spectral snapshot com-
pressive imaging framework, dubbed PCA-CASSI. It pro-
gressively compresses HSIs with content-aware optimized
coded apertures and fuses all the snapshots for reconstruc-
tion. Inspired by the R−Ndecomposition, an RND-HRNet
is proposed for accurate HSI reconstruction. To improve
its representation ability, a range-null space decomposition
module is proposed to refine the null-space component of
HSIs iteratively. Meanwhile, a spatial-spectral fusion mod-
ule is introduced to explore the non-local correlation viadual transformer blocks. Extensive experiments verify that
our method significantly outperforms the other SOTA meth-
ods on both multiple- and single-shot reconstruction tasks.
References
[1] Hamed Akbari, Yukio Kosugi, Kazuyuki Kojima, and Nao-
fumi Tanaka. Detection and analysis of the intestinal is-
chemia using visible and invisible hyperspectral imaging.
IEEE Transactions on Biomedical Engineering , 57(8):2011–
2017, 2010. 1
[2] Gonzalo R Arce, David J Brady, Lawrence Carin, Henry
Arguello, and David S Kittle. Compressive coded aperture
spectral imaging: an introduction. IEEE Signal Processing
Magazine , 31(1):105–115, 2013. 1
[3] Jos ´e M Bioucas-Dias and M ´ario AT Figueiredo. A new twist:
two-step iterative shrinkage/thresholding algorithms for im-
age restoration. IEEE Transactions on Image Processing ,
16(12):2992–3004, 2007. 2
[4] Andrew Bullen. Microscopic imaging techniques for drug
discovery. Nature Reviews Drug Discovery , 7(1):54–67,
2008. 1
[5] Yuanhao Cai, Jing Lin, Xiaowan Hu, Haoqian Wang, Xin
Yuan, Yulun Zhang, Radu Timofte, and Luc Van Gool.
Coarse-to-fine sparse transformer for hyperspectral image re-
construction. In Proceedings of the European Conference on
Computer Vision , 2022. 2, 3, 6, 7, 8, 14
[6] Yuanhao Cai, Jing Lin, Xiaowan Hu, Haoqian Wang, Xin
Yuan, Yulun Zhang, Radu Timofte, and Luc Van Gool.
Mask-guided spectral-wise transformer for efficient hy-
perspectral image reconstruction. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , 2022. 2, 3, 5, 6, 7, 8, 13, 14
[7] Yuanhao Cai, Jing Lin, Zudi Lin, Haoqian Wang, Yulun
Zhang, Hanspeter Pfister, Radu Timofte, and Luc Van Gool.
Mst++: Multi-stage spectral-wise transformer for efficient
spectral reconstruction. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
2022. 6, 7, 8
[8] Yuanhao Cai, Jing Lin, Haoqian Wang, Xin Yuan, Henghui
Ding, Yulun Zhang, Radu Timofte, and Luc Van Gool.
Degradation-aware unfolding half-shuffle transformer for
spectral compressive imaging. In Proceedings of the Ad-
vances in Neural Information Processing Systems , 2022. 2
[9] Xun Cao, Tao Yue, Xing Lin, Stephen Lin, Xin Yuan, Qiong-
hai Dai, Lawrence Carin, and David J Brady. Computa-
tional snapshot multispectral cameras: toward dynamic cap-
ture of the spectral world. IEEE Signal Processing Maga-
zine, 33(5):95–108, 2016. 1
[10] Bin Chen and Jian Zhang. Content-aware scalable deep com-
pressed sensing. IEEE Transactions on Image Processing ,
31:5412–5426, 2022. 1
[11] Dongdong Chen and Mike E Davies. Deep decomposition
learning for inverse imaging problems. In Proceedings of
the European Conference on Computer Vision , 2020. 4
[12] Dongdong Chen, Juli ´an Tachella, and Mike E Davies. Equiv-
ariant imaging: Learning beyond the range space. InProceedings of the IEEE/CVF International Conference on
Computer Vision , pages 4379–4388, 2021. 4
[13] Ziheng Cheng, Bo Chen, Ruiying Lu, Zhengjue Wang, Hao
Zhang, Ziyi Meng, and Xin Yuan. Recurrent neural net-
works for snapshot compressive imaging. IEEE Transactions
on Pattern Analysis and Machine Intelligence , 45(2):2264–
2281, 2023. 2
[14] Inchang Choi, MH Kim, D Gutierrez, DS Jeon, and G Nam.
High-quality hyperspectral reconstruction using a spectral
prior. ACM Transactions on Graphics , 36(6):1–13, 2017.
7
[15] Meng Ding, Xiao Fu, Ting-Zhu Huang, Jun Wang, and Xi-
Le Zhao. Hyperspectral super-resolution via interpretable
block-term tensor modeling. IEEE Journal of Selected Top-
ics in Signal Processing , 15(3):641–656, 2020. 1
[16] Kingma DP and Jimmy Ba. Adam: a method for stochastic
optimization. In Proceedings of the International Conference
for Learning Representations , 2015. 6
[17] Yonina C Eldar and Gitta Kutyniok. Compressed sensing:
theory and applications . Cambridge University Press, 2012.
2
[18] Ying Fu, Tao Zhang, Lizhi Wang, and Hua Huang. Coded
hyperspectral image reconstruction using deep external and
internal learning. IEEE Transactions on Pattern Analysis and
Machine Intelligence , 44(7):3404–3420, 2021. 4
[19] Wei He, Naoto Yokoya, and Xin Yuan. Fast hyperspec-
tral image recovery of dual-camera compressive hyperspec-
tral imaging via non-iterative subspace-based fusion. IEEE
Transactions on Image Processing , 30:7170–7183, 2021. 1
[20] Xiaowan Hu, Yuanhao Cai, Jing Lin, Haoqian Wang, Xin
Yuan, Yulun Zhang, Radu Timofte, and Luc Van Gool. Hd-
net: High-resolution dual-domain learning for spectral com-
pressive imaging. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition , 2022. 2, 6, 7, 14
[21] Yujie Hu, Yinhuai Wang, and Jian Zhang. Dear-gan:
Degradation-aware face restoration with gan prior. IEEE
Transactions on Circuits and Systems for Video Technology ,
2023. 1
[22] Tao Huang, Weisheng Dong, Xin Yuan, Jinjian Wu, and
Guangming Shi. Deep gaussian scale mixture prior for spec-
tral compressive imaging. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
2021. 2, 6, 7, 8, 12, 14
[23] David Kittle, Kerkil Choi, Ashwin Wagadarikar, and David J
Brady. Multiframe image estimation for coded aperture
snapshot spectral imagers. Applied Optics , 49(36):6824–
6833, 2010. 1, 2, 3, 4
[24] Jie Lei, Weiying Xie, Jian Yang, Yunsong Li, and Chein-I
Chang. Spectral-spatial feature extraction for hyperspectral
anomaly detection. IEEE Transactions on Geoscience and
Remote Sensing , 57(10):8131–8143, 2019. 1
[25] Weiqi Li, Bin Chen, and Jian Zhang. D3c2-net: Dual-domain
deep convolutional coding network for compressive sensing.
arXiv preprint arXiv:2207.13560 , 2022. 1
[26] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc
Van Gool, and Radu Timofte. Swinir: Image restoration us-
ing swin transformer. In Proceedings of the IEEE Interna-
tional Conference on Computer Vision , 2021. 13[27] Xing Lin, Yebin Liu, Jiamin Wu, and Qionghai Dai. Spatial-
spectral encoded compressive hyperspectral imaging. ACM
Transactions on Graphics (TOG) , 33(6):1–11, 2014. 2
[28] Xing Lin, Gordon Wetzstein, Yebin Liu, and Qionghai Dai.
Dual-coded compressive hyperspectral imaging. Optics Let-
ters, 39(7):2044–2047, 2014. 2
[29] Yang Liu, Xin Yuan, Jinli Suo, David J Brady, and Qionghai
Dai. Rank minimization for snapshot compressive imaging.
IEEE Transactions on Pattern Analysis and Machine Intelli-
gence , 41(12):2990–3006, 2018. 2
[30] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng
Zhang, Stephen Lin, and Baining Guo. Swin transformer:
Hierarchical vision transformer using shifted windows. In
Proceedings of the IEEE International Conference on Com-
puter Vision , 2021. 13
[31] Jiawei Ma, Xiao-Yang Liu, Zheng Shou, and Xin Yuan.
Deep tensor admm-net for snapshot compressive imaging.
InProceedings of the IEEE/CVF International Conference
on Computer Vision , 2019. 2
[32] Qing Ma, Junjun Jiang, Xianming Liu, and Jiayi Ma. Deep
unfolding network for spatiospectral image super-resolution.
IEEE Transactions on Computational Imaging , 8:28–40,
2021. 16
[33] Farid Melgani and Lorenzo Bruzzone. Classification of hy-
perspectral remote sensing images with support vector ma-
chines. IEEE Transactions on Geoscience and Remote Sens-
ing, 42(8):1778–1790, 2004. 1
[34] Ziyi Meng, Jiawei Ma, and Xin Yuan. End-to-end low
cost compressive spectral imaging with spatial-spectral self-
attention. In Proceedings of European Conference on Com-
puter Vision , 2020. 2, 6, 7, 8, 14
[35] Ziyi Meng, Zhenming Yu, Kun Xu, and Xin Yuan. Self-
supervised neural networks for spectral snapshot compres-
sive imaging. In Proceedings of the IEEE International Con-
ference on Computer Vision , 2021. 2
[36] Xin Miao, Xin Yuan, Yunchen Pu, and Vassilis Athitsos. λ-
net: reconstruct hyperspectral images from a snapshot mea-
surement. In Proceedings of the IEEE International Confer-
ence on Computer Vision , 2019. 2, 14
[37] Chong Mou, Qian Wang, and Jian Zhang. Deep generalized
unfolding networks for image restoration. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 17399–17410, 2022. 16
[38] Lujendra Ojha, Mary Beth Wilhelm, Scott L Murchie, Al-
fred S McEwen, James J Wray, Jennifer Hanley, Marion
Mass ´e, and Matt Chojnacki. Spectral evidence for hydrated
salts in recurring slope lineae on mars. Nature Geoscience ,
8(11):829–832, 2015. 1
[39] Jin Tan, Yanting Ma, Hoover Rueda, Dror Baron, and Gon-
zalo R Arce. Compressive hyperspectral imaging via approx-
imate message passing. IEEE Journal of Selected Topics in
Signal Processing , 10(2):389–401, 2015. 2
[40] Ashwin Wagadarikar, Renu John, Rebecca Willett, and
David Brady. Single disperser design for coded aperture
snapshot spectral imaging. Applied Optics , 47(10):B44–
B51, 2008. 2, 3, 4, 16[41] Jiamian Wang, Yulun Zhang, Xin Yuan, Yun Fu, and
Zhiqiang Tao. A new backbone for hyperspectral image re-
construction. arXiv preprint arXiv:2108.07739 , 2021. 2, 6,
7
[42] Lizhi Wang, Chen Sun, Ying Fu, Min H Kim, and Hua
Huang. Hyperspectral image reconstruction using a deep
spatial-spectral prior. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , 2019.
2
[43] Lizhi Wang, Chen Sun, Maoqing Zhang, Ying Fu, and Hua
Huang. Dnu: deep non-local unrolling for computational
spectral imaging. In Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition , 2020. 2
[44] Lizhi Wang, Zhiwei Xiong, Dahua Gao, Guangming Shi, and
Feng Wu. Dual-camera design for coded aperture snapshot
spectral imaging. Applied Optics , 54(4):848–858, 2015. 2
[45] Lizhi Wang, Zhiwei Xiong, Guangming Shi, Feng Wu,
and Wenjun Zeng. Adaptive nonlocal sparse representation
for dual-camera compressive hyperspectral imaging. IEEE
Transactions on Pattern Analysis and Machine Intelligence ,
39(10):2104–2111, 2016. 2
[46] Yinhuai Wang, Yujie Hu, Jiwen Yu, and Jian Zhang.
Gan prior based null-space learning for consistent super-
resolution. arXiv preprint arXiv:2211.13524 , 2022. 1
[47] Yinhuai Wang, Jiwen Yu, and Jian Zhang. Zero-shot im-
age restoration using denoising diffusion null-space model.
InProceedings of the International Conference on Learning
Representations , 2022. 1
[48] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Si-
moncelli. Image quality assessment: from error visibility to
structural similarity. IEEE Transactions on Image Process-
ing, 13(4):600–612, 2004. 7
[49] Xueling Wei, Wei Li, Mengmeng Zhang, and Qingli Li.
Medical hyperspectral image classification based on end-to-
end fusion deep neural network. IEEE Transactions on In-
strumentation and Measurement , 68(11):4481–4492, 2019.
1
[50] Zhuoyuan Wt, Jian Zhangt, and Chong Mou. Dense deep
unfolding network with 3d-cnn prior for snapshot compres-
sive imaging. In 2021 IEEE/CVF International Conference
on Computer Vision (ICCV) , 2021. 16
[51] Yuehao Wu, Iftekhar O Mirza, Gonzalo R Arce, and Den-
nis W Prather. Development of a digital-micromirror-device-
based multishot snapshot spectral imaging system. Optics
Letters , 36(14):2692–2694, 2011. 1, 2
[52] Weiying Xie, Tao Jiang, Yunsong Li, Xiuping Jia, and Jie
Lei. Structure tensor and guided filtering-based algorithm
for hyperspectral anomaly detection. IEEE Transactions on
Geoscience and Remote Sensing , 57(7):4218–4230, 2019. 1
[53] Jianbo Yang, Xuejun Liao, Xin Yuan, Patrick Llull, David J
Brady, Guillermo Sapiro, and Lawrence Carin. Compressive
sensing by learning a gaussian mixture model from measure-
ments. IEEE Transactions on Image Processing , 24(1):106–
119, 2014. 2
[54] Jianbo Yang, Xin Yuan, Xuejun Liao, Patrick Llull,
David J Brady, Guillermo Sapiro, and Lawrence Carin.
Video compressive sensing using gaussian mixture models.IEEE Transactions on Image Processing , 23(11):4863–4878,
2014. 2
[55] Fumihito Yasuma, Tomoo Mitsunaga, Daisuke Iso, and
Shree K Nayar. Generalized assorted pixel camera: post-
capture control of resolution, dynamic range, and spectrum.
IEEE Transactions on Image Processing , 19(9):2241–2253,
2010. 7
[56] Xin Yuan. Generalized alternating projection based total
variation minimization for compressive sensing. In Proceed-
ings of IEEE International Conference on Image Processing ,
2016. 2, 6, 7
[57] Xin Yuan, David J Brady, and Aggelos K Katsaggelos. Snap-
shot compressive imaging: theory, algorithms, and appli-
cations. IEEE Signal Processing Magazine , 38(2):65–88,
2021. 1
[58] Xin Yuan, Yang Liu, Jinli Suo, and Qionghai Dai. Plug-and-
play algorithms for large-scale snapshot compressive imag-
ing. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition , 2020. 2
[59] Jian Zhang, Bin Chen, Ruiqin Xiong, and Yongbing Zhang.
Physics-inspired compressive sensing: Beyond deep un-
rolling. IEEE Signal Processing Magazine , 40(1):58–72,
2023. 1
[60] Jian Zhang and Bernard Ghanem. Ista-net: interpretable
optimization-inspired deep network for image compressive
sensing. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , 2018. 4
[61] Jian Zhang, Chen Zhao, and Wen Gao. Optimization-
inspired compact deep compressive sensing. IEEE Journal of
Selected Topics in Signal Processing , 14(4):765–774, 2020.
4
[62] Jian Zhang, Chen Zhao, Debin Zhao, and Wen Gao. Im-
age compressive sensing recovery using adaptively learned
sparsifying basis via l0minimization. Signal Processing ,
103:114–126, 2014. 1
[63] Kai Zhang, Yawei Li, Jingyun Liang, Jiezhang Cao, Yulun
Zhang, Hao Tang, Radu Timofte, and Luc Van Gool. Prac-
tical blind denoising via swin-conv-unet and data synthesis.
arXiv preprint arXiv:2203.13278 , 2022. 5, 13
[64] Shipeng Zhang, Lizhi Wang, Lei Zhang, and Hua Huang.
Learning tensor low-rank prior for hyperspectral image re-
construction. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , 2021. 2
[65] Tao Zhang, Ying Fu, Lizhi Wang, and Hua Huang. Hyper-
spectral image reconstruction using deep external and inter-
nal learning. In Proceedings of the IEEE/CVF International
Conference on Computer Vision , 2019. 3, 4
[66] Xuanyu Zhang, Yongbing Zhang, Ruiqin Xiong, Qilin Sun,
and Jian Zhang. Herosnet: Hyperspectral explicable recon-
struction and optimal sampling deep network for snapshot
compressive imaging. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , 2022.
2, 6
[67] Chen Zhao, Siwei Ma, Jian Zhang, Ruiqin Xiong, and
Wen Gao. Video compressive sensing reconstruction via
reweighted residual sparsity. IEEE Transactions on Circuits
and Systems for Video Technology , 27(6):1182–1195, 2016.
1[68] Ji Zhao, Yanfei Zhong, Yunyun Wu, Liangpei Zhang, and
Hong Shu. Sub-pixel mapping based on conditional random
fields for hyperspectral remote sensing imagery. IEEE Jour-
nal of Selected Topics in Signal Processing , 9(6):1049–1060,
2015. 1
[69] Siming Zheng, Yang Liu, Ziyi Meng, Mu Qiao, Zhishen
Tong, Xiaoyu Yang, Shensheng Han, and Xin Yuan.
Deep plug-and-play priors for spectral snapshot compressive
imaging. Photonics Research , 9(2):B18–B29, 2021. 2Figure 9. Reconstructed results of the proposed methods. Simulation results of all 28 bands [22] are presented in the left column and the
real data results are shown in the right column.
In the supplementary materials, we demonstrate additional experimental results and implementation details as follows.
• Implementation details of some key modules are elaborated in Sec. A;
• More visualizations of the real reconstruction results, simulation results, and content-aware masks are shown in Sec. B;
• Ablation studies on the proposed RND-HRNet and N-shot reconstruction (with N>2) are conducted in Sec. C;
• The limitations and broader impacts of this work are discussed and presented in Sec. D.
Appendix A. Implementation Details of some Key Modules
A.1. The pseudo codes of Hd(⋅,⋅)andH†
d(⋅,⋅)in the RNDM
To further reflect the forward and inverse operation of the imaging system, the PyTorch-style pseudo-codes of Hd(⋅,⋅)and
H†
d(⋅,⋅)in the range-nullspace decomposition module (RNDM) are presented as follows.
d e f H d ( x , F ) : # s h i f t and sum
# i n p u t : x ; shape : (B , C , H, W) F ; shape : (B , C , H, W)
# o u t p u t : y ; shape : (B , 1 , H, W+(C−1) *s t e p )
s t e p = 2
z = x *F
y = z e r o s (B , 1 , H, W)
f o r i i n r a n g e (C) :
y [ : , : , : , i *s t e p : i *s t e p + W] += z [ : , i : i +1]
r e t u r n y
d e f H dpinv ( y , E ) : # r e c t i f i c a t i o n , c r o p p i n g and s l i d i n g
# i n p u t : y ; shape : (B , 1 , H, W+(C−1) *s t e p ) E ; shape : (B , C , H, W)
# o u t p u t : x ; shape : (B , C , H, W)
s t e p = 2
a l p h a = z e r o s (B , 1 , 1 , W+(C−1) *s t e p )
f o r i i n r a n g e (C) :
a l p h a [ : , : , : , i *s t e p : i *s t e p + W] += 1
a l p h a = 1 / a l p h a
z = y *a l p h a
x = [ ]
f o r i i n r a n g e (C) :
x . append ( z [ : , : , : , i *s t e p : i *s t e p + W] )
x = C o n c a t e n a t e ( x , dim =1) / E
r e t u r n xA.2. Details of the CTB and STB in the SSFM
In this subsection, we elaborate on the details of the CTB and STB in Fig. 10 to show how the SSFM fuses spatial and
spectral correlations adaptively. Following the previous work [6], the channel-wise transformer block (CTB) aims to extract
spectral correlation, which includes two-layer normalization (LN) layers, one channel-wise self-attention (CSA) block and
one feed-forward network (FFN). For convenience, we assume that the batch size is set to 1 respectively. Concretely, the
CSA block encodes each feature frame Icto the channel-wise tokens via three linear layers to obtain the value V∈RHW×C,
keyK∈RHW×Cand query Q∈RHW×Cas follows.
Q=IcWQ,K=IcWK,V=IcWV, (20)
where WQ,WKandWV∈RC×Crespectively denote the learnable parameters. Then, Q,KandVare respectively split
intoNheads along the channel dimension, namely Q=[Q1,...,QN],K=[K1,...,KN]andV=[V1,...,VN]. Note
that the channel dimension of each head is C/N. Then, we calculate the self-attention map Ajfor each head jas follows.
Aj=Vj⋅Softmax (σjK⊺
jQj), j∈{1,2,...,N}, (21)
where σj∈Ris the learnable parameter to represent the variation of spectral intensity. Finally, the above output is reshaped
and fused via a convolution and added with the results of position encoding Hpos(⋅).
Oc=CSA(Ic)=[A1,...,AN]W+Hpos(V), (22)
where W∈RC×Cis the learnable weight of the convolution. The position encoding Hpos(⋅)is composed of several depth-
wise convolutions and GELU activation. Meanwhile, inspired by the previous work [63], the spatial-wise transformer block
(STB) aims to introduce local biases and spatial cues for the long-range transformer blocks via the sliding-window-based
attention mechanism and residual blocks. Specifically, the input feature is firstly processed via an 1×1convolution and split
in the channel dimension as follows.
I(1)
s,I(2)
s=Split(Conv(Is)). (23)
One part of the split feature is fed to the Swin transformer block [26, 30] to explore the spatial correlation and non-local
information. Additionally, the other part of the feature is fed to the residual block to capture local image cues. Finally, the
two parts of features are fused adaptively via an 1×1convolution as follows.
O(1)
s,O(2)
s=SwinT(I(1)
s),RB(I(2)
s), (24)
Os=Conv([O(1)
s,O(2)
s])+Is, (25)
where SwinT(⋅)andRB(⋅)respectively denote the operation of sliding-window-based transformer block and residual block.
Figure 10. The details of channel-wise transformer block (CTB) and spatial-wise transformer block (STB) in the spectral-spatial fusion
module (SSFM). These two blocks allow the SSFM to explore spectral and spatial correlation and fuse local and non-local information.Appendix B. Visualization Results
B.1. Real Data and Simulation Results Visualization
To objectively evaluate the proposed RND-HRNet on real data, we present more visualization results with sufficient
comparison methods [36, 34, 22, 20, 6, 5] and more band numbers in 2 real measurements [22]. To simulate the real imaging
situations, 11-bit shot noise is injected into the training data [34]. As shown in Fig. 11, our results are more perception-
friendly with clearer HSI contents and fewer artifacts. For instance, our results in the cropped regions of the flowers have
smoother textures and our results in the cropped regions of the strawberries have less noise, which suggests the generalization
ability and robustness of our method. Meanwhile, we also illustrate the other 3 real scenes [22] in Fig. 9 (right column). It
can be clearly seen that our method can reconstruct high-quality hyperspectral images in different contents and degradations.
Furthermore, we present the two-shot simulation results of all the 28 bands [22] from 453.5nm to 648.0nm in Fig. 9 (right
column). It proves that our results have accurate spectral information and smooth spatial details in different wavelengths.
Figure 11. Real data results of the proposed RND-HRNet (Ours) and other SOTA methods on the single-shot reconstruction.
B.2. Mask Visualization
To show the structure and data distribution of optimized coded apertures in different scenarios, we visualize more content-
aware masks. As shown in Fig. 12, optimized masks M1andM2in three scenes are presented, where M1is shared by all
scenes and M2is content-aware. Concretely, M2is composed of the shared component M2and content-aware component
H′(2)
M(y1). We find that although M2remains a similar pattern, it veils some anisotropic HSI information of different scenes,
which is conducive to the multiple-shot reconstruction.
Table 6. Evaluation of the effectiveness of different components.
Case Index RNDM CTB STB PSNR SSIM
(a) 35.13 0.951
(b) 36.15 0.950
(c) 36.10 0.953
(d) 36.66 0.957Figure 12. The visualization of the optimized content-aware masks M1∈RH×WandM2∈RH×W. The content-aware mask M2learns
the anisotropic HSI information of different scenes adaptively with M2=M2+η(2)⋅H′(2)
M(y1), while the shared M1andM2∈RH×W
learn the general characteristics of the imaging system.
Appendix C. More Ablation Studies
C.1. Ablation Study on the Proposed RND-HRNet
To evaluate the contribution of different components in the proposed RND-HRNet, we conduct an ablation study on
the single-shot reconstruction with the two-phase reconstruction network. We mainly focus on our adopted three modules,
namely range-nullspace decomposition module (RNDM), channel-wise transformer block (CTB), and spatial-wise trans-
former block (STB). Tab. 6 illustrates the PSNR (dB) and SSIM on the different settings. Removing the RNDM, we retrained
a variant of the proposed model that contains only two proximal mapping modules. It can be clearly seen that the PSNR
has a decline of 1.43dB, proving the effectiveness of the proposed RNDM. Meanwhile, we respectively remove the CTB and
STB in the SSFM to implement two variant models, namely case (b) and case (c). Obviously, without CTB, the values of
the PSNR and SSIM have dropped by 0.51dB and 0.007 respectively. Without STB, the results of PSNR and SSIM have
decreased by 0.56dB and 0.004. It further proves that integrating the spectral and spatial correlation cooperatively in the
SSFM is necessary and significant for reconstruction quality.
C.2. Ablation Study on the N-shot Reconstruction
To explore the upper bound of the proposed progressive sampling in the multiple-shot reconstruction, we retrain the pro-
posed PCA-CASSI when the shot number Nis set to be 2, 3, 4, 5, and 6. The values of PSNR and SSIM are reported in
Fig. 13. As the shot number increases, the reconstruction performances of the proposed PCA-CASSI are improved corre-
spondingly and reach the peak when the shot number is set to 5. However, as the shot number increases above 5, the PSNR
and SSIM results decline slightly due to the increased parameters and the network overfitting. Furthermore, it can be clearly
seen from Fig. 14 that with the shot number increases, more HSI contents will be reflected in the coded apertures, which is
conducive to the HSI reconstruction.
Appendix D. Limitations and Broader Impacts
Limited by the quality of the hardware devices and the scarcity of real hyperspectral data, the core contribution of our paper
focuses on designing a novel high-quality imaging framework and the proposed progressive sampling is mainly verified in
the simulation setup, which will inspire future work in the community. However, we maintain that the proposed PCA-CASSIFigure 13. The values of PSNR and SSIM of the proposed PCA-CASSI on the N-shot reconstruction.
Figure 14. The visualization of five optimized content-aware masks on the 5-shot HSI reconstruction.
can be easily deployed on existing imaging systems such as [40]. We will realize the proposed PCA-CASSI in hardware
imaging devices and present some real data results in our future work.
The proposed PCA-CASSI contributes to the industrial application of hyperspectral compressive imaging and inspires
the design of deep unfolding networks in other image inverse problems such as video snapshot compressive imaging [50],
super-resolution [32], and general image restoration [37]. Meanwhile, benefiting from its fast imaging speed and accurate
HSI reconstruction, the proposed PCA-CASSI has the potential to promote closer integration of artificial intelligence in life
sciences and cytology.