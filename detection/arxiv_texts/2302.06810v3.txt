Learning from Noisy Labels with Decoupled Meta Label Puriﬁer
Yuanpeng Tu1*Boshen Zhang2*Yuxi Li2*Liang Liu2Jian Li2
Yabiao Wang2Chengjie Wang2Cai Rong Zhao1
1Dept. of Electronic and Information Engineering, Tongji Univeristy, Shanghai
2YouTu Lab, Tencent, Shanghai
{2030809, zhaocairong}@tongji.edu.cn
{boshenzhang, yukiyxli, leoneliu, swordli, caseywang, jasoncjwang}@tencent.com
Abstract
Training deep neural networks (DNN) with noisy labels
is challenging since DNN can easily memorize inaccurate
labels, leading to poor generalization ability. Recently,
the meta-learning based label correction strategy is widely
adopted to tackle this problem via identifying and correcting
potential noisy labels with the help of a small set of clean
validation data. Although training with puriﬁed labels can
effectively improve performance, solving the meta-learning
problem inevitably involves a nested loop of bi-level opti-
mization between model weights and hyper-parameters (i.e.,
label distribution). As compromise, previous methods resort
to a coupled learning process with alternating update. In
this paper, we empirically ﬁnd such simultaneous optimiza-
tion over both model weights and label distribution can not
achieve an optimal routine, consequently limiting the rep-
resentation ability of backbone and accuracy of corrected
labels. From this observation, a novel multi-stage label puri-
ﬁer named DMLP is proposed. DMLP decouples the label
correction process into label-free representation learning
and a simple meta label puriﬁer, In this way, DMLP can fo-
cus on extracting discriminative feature and label correction
in two distinctive stages. DMLP is a plug-and-play label
puriﬁer, the puriﬁed labels can be directly reused in naive
end-to-end network retraining or other robust learning meth-
ods, where state-of-the-art results are obtained on several
synthetic and real-world noisy datasets, especially under
high noise levels.
1. Introduction
Deep learning has achieved signiﬁcant progress on vari-
ous recognition tasks. The key to its success is the availabil-
Yuanpeng Tu, Boshen Zhang, Yuxi Li contribute equally to this work.
Fully -
supervision𝑥𝑡Update weights:𝑤𝑡′
𝐿𝑣𝑎𝑙Self-
supervision 𝑤𝑡∗Coupled 
optimization
loop
Decoupling𝐿𝑣𝑎𝑙
(a) Traditional
(b) DMLP𝑦𝑡
𝑥𝑡 𝑦𝑡𝑥𝑣𝑦𝑣
𝑥𝑣𝑦𝑣Updated label :𝑦𝑡′
𝑦𝑡∗𝑦𝑡∗
Fixed
weightsUpdate
labe lFigure 1. (a) Traditional coupled alternating update to solve meta
label puriﬁcation problem, and (b) the proposed DMLP method
that decouples the label puriﬁcation process into representation
learning and a simple non-nested meta label puriﬁer.
ity of large-scale datasets with reliable annotations. Collect-
ing such datasets, however, is time-consuming and expensive.
Easy ways to obtain labeled data, such as web crawling [31],
inevitably yield samples with noisy labels, which is not
appropriate to be directly utilized to train DNN since these
complex models are vulnerable to memorize noisy labels [2].
Towards this problem, numerous Learning with Noisy
Label (LNL) approaches were proposed. Classical LNL
methods focus on identifying the noisy samples and reduc-
ing their effect on parameter updates by abandoning [12]
or assigning smaller importance. However, when it comes
to extremely noisy and complex scenarios, such scheme
struggles since there is no sufﬁcient clean data to train a dis-
criminative classiﬁer. Therefore, label correction approaches
are proposed to augment clean training samples by revising
noisy labels to underlying correct ones. Among them, meta-
learning based approaches [9, 16, 25] achieve state-of-the-art
performance via resorting to a small clean validation setarXiv:2302.06810v3  [cs.CV]  17 Feb 2023and taking noisy labels as hyper-parameters, which provides
sound guidance toward underlying label distribution of clean
samples. However, such meta puriﬁcation inevitably in-
volves a nested bi-level optimization problem on both model
weight and hyper-parameters (shown as Fig. 1 (a)), which is
computationally infeasible. As a compromise, the alternat-
ing update between model weights and hyper-parameters is
adopted to optimize the objective [9, 16, 25], resulting in a
coupled solution for both representation learning and label
puriﬁcation.
Empirical observation. Intuitively, alternate optimiza-
tion over a large search space (model weight and hyper-
parameters) may lead to sub-optimal solutions. To investi-
gate how such approximation affects results in robust learn-
ing, we conduct empirical analysis on CIFAR-10 [14] with
recent label puriﬁcation methods MLC [40] and MSLC [9],
which consist of a deep model and a meta label correction
network, and make observation as Fig. 2.
Coupled optimization hinders quality of corrected la-
bels. We ﬁrst compare the Coupled meta corrector MLC
with its extremely Decoupled variant where the model
weights are ﬁrst optimized for 70epochs with noisy labels
and get ﬁxed, then labels are puriﬁed with the guidance of
validation set. We adopt the accuracy of corrected label
to measure the performance of puriﬁcation. From Fig. 2
(a), we can clearly observe that compared with Decoupled
counterpart, joint optimization yields inferior correction per-
formance, and these miscorrection will reversely affect the
representation learning in coupled optimization.
Coupled optimization hinders representation ability.
We investigate the representation quality by evaluating the
linear prob accuracy [6] of extracted feature in Fig. 2 (b).
We ﬁnd the representation quality of Coupled training is
much worse at the beginning, which leads to slow and un-
stable representation learning in the later stage. To further
investigate the effect on representation learning, we also
resort to a well pretrained backbone with self-supervised
learning [5] as initialization, recent research [39] shows pre-
trained representation is substantially helpful for LNL frame-
work. However, we ﬁnd this conclusion does not strictly
hold for coupled meta label correctors. As shown in Fig. 2
(c), by comparing the classiﬁcation accuracy from classi-
ﬁer of MLC/MSLC, we observe the pretrained model only
brings marginal improvement if model weights is still cou-
pled with hyper-parameters. In contrast, when the weight of
backbone is ﬁxed and decoupled from the label puriﬁcation
and classiﬁer, the improvement becomes more signiﬁcant.
Decoupled Meta Puriﬁcation. From the observation
above, we ﬁnd the decoupling between model weights and
hyperparameters of meta correctors is essential to label ac-
curacy and ﬁnal results. Therefore, in this paper, we aim
at detaching the meta label puriﬁcation from representation
learning and designing a simple meta label puriﬁer whichis more friendly to optimization of label distribution prob-
lem than existing complex meta networks [9, 40]. Hence
we propose a general multi-stage label correction strategy,
named Decoupled Meta Label Puriﬁer (DMLP). The core
of DMLP is a meta-learning based label puriﬁer, however,
to avoid solving the bi-level optimization with a coupled
solution, DMLP decouples this process into self-supervised
representation learning and a linear meta-learner to ﬁt un-
derlying correct label distribution (illustrated as Fig. 1 (b)),
thus simpliﬁes the label puriﬁcation stage as a single-level
optimization problem. The simple meta-learner is carefully
designed with two mutually reinforcing correcting processes,
named intrinsic primary correction (IPC) and extrinsic aux-
iliary correction (EAC) respectively. IPC plays the role of
purifying labels in a global sense at a steady pace, while EAC
targets at accelerating the puriﬁcation process via looking
ahead (i.e., training with) the updated labels from IPC. The
two processes can enhance the ability of each other and form
a positive loop of label correction. Our DMLP framework
is ﬂexible for application, the puriﬁed labels can either be
directly applied for naive end-to-end network retraining, or
exploited to boost the performance of existing LNL frame-
works. Extensive experiments conducted on mainstream
benchmarks, including synthetic (noisy versions of CIFAR)
and real-world (Clothing1M) datasets, demonstrate the supe-
riority of DMLP. In a nutshell, the key contributions of this
paper include:
We analyze the necessity of decoupled optimization
for label correction in robust learning, based on which we
propose DMLP, a ﬂexible and novel multi-stage label puriﬁer
that solves bi-level meta-learning problem with a decoupled
manner, which consists of representation learning and non-
nested meta label puriﬁcation;
In DMLP, a novel non-nested meta label puriﬁer
equipped with two correctors, IPC and EAC is proposed.
IPC is a global and steady corrector, while EAC accelerates
the correction process via training with the updated labels
from IPC. The two processes form a positive training loop
to learn more accurate label distribution;
Deep models trained with puriﬁed labels from DMLP
achieve state-of-the-art results on several synthetic and real-
world noisy datasets across various types and levels of label
noise, especially under high noise levels. Extensive ablation
studies are provided to verify the effectiveness.
2. Related Works
The existing LNL approaches that are related to our work
can be coarsely categorized into two groups: noisy sample
detection and label correction.
Noisy sample detection methods aim to identify and re-
duce the importance of suspicious false-labeled samples
during training. The detected noisy samples are aban-
doned [12, 23], assigned with smaller weights via a sam-Figure 2. The corrected label accuracy (a) and linear probe accuracy of representations (b) between coupled [40] and decoupled label
correction schemes during training under 50% and 90% symmetric label noise on CIFAR-10. (c) investigates the effect of representation
learning on ‘Ori’-original coupled network training from scratch, ‘SimCLR-Coupled’-initialization with stronger self-supervised pretrained
weights and ‘SimCLR-Decoupled’-further ﬁx the pretrained backbone during label puriﬁcation.
ple re-weight training scheme [20], or used to formulate
a semi-supervised learning problem by throwing away the
labels while keeping the unlabeled data [15,37]. These meth-
ods show robustness under certain noise levels, but struggle
when it comes to extremely noisy and complex scenarios
since there is no sufﬁcient clean data to train a classiﬁer.
Label correction approaches attempt to augment the
training set by ﬁnding and correcting noisy labels to their
underlying true ones. To do so, some works [22] try to
estimate the noise transition matrix. However, these meth-
ods usually assume that the noise type is class-dependent,
which may be inappropriate for more complex noise set-
tings, such as real-world noisy datasets [31]. Some other
works resort to exploiting the prediction of the network,
both soft [1, 13, 24, 35] and hard [27, 28] label correction
schemes are designed. However, the predictions of over-
parameterized backbone network can be unreliable since it
tends to ﬂuctuate during training in the presence of false-
labeled data ( [37]). Another line of works utilize robust
representations learned via unsupervised contrastive learn-
ing methods [11, 17, 37, 39] to eliminate the interference of
noisy labels, which provides a reliable initialization of deep
models. Recently, meta-learning based methods [9,16,25,40]
show great potential towards LNL problems with the help
of a small clean validation set to provide sound guidance to-
ward underlying label distribution of clean sample. However,
these approaches involves a bi-level optimization problem
on model weights and hyper-parameters, which is too com-
putationally expensive to optimize. As a compromise, the
one-step approximation is commonly adopted [9, 40] to con-
vert the nested objective into a coupled update procedure
between model weights and hyper-parameters, leading to
sub-optimal performance.
Accordingly, DMLP belongs to the label correction group
via meta-learning strategy, but unlike previous meta-learning
based methods, the learning process on model weights and
labels are decoupled into individual stages within DMLP.Together with the proposed non-nested meta-label puriﬁer,
DMLP yields more accurate labels than coupled label correc-
tion methods [9, 40] and further set the new state-of-the-art
on CIFAR and Clothing1M.
3. Method
3.1. Decoupled Solution to Meta Label Puriﬁcation
For convenience, notations within DMLP are clariﬁed
ﬁrst. The noisy training dataset is denoted as Dt=
f(xi;yi)j1iNtg, wherexi2RHW3;yi2f0;1gc
are the image and corrupted label of the i-th instance, cis
the class number. Similarly, a small clean validation dataset
is denoted as Dv=f(xi;yi)j1iNvg.Ndenotes the
dataset size, and NvNt. The subscripts t,vrepresent the
data is from training set or validation set.
Typical meta-learning based puriﬁcation requires solving
a bi-level optimization problem on both model weights and
hyper-parameters with objective:
min
E(xv;yv)2DvLval(xv;yv;w())
s.t.w() = arg min
wE(xt;yt)2DtLtrain(xt;yt;w;)
(1)
wherewanddenote the model weights and the meta
hyper-parameters respectively. In the classical label pu-
riﬁcation pipeline, is reparameterized as a function of
noisy label distribution, i.e. =g(yt).LvalandLtrain
are the loss function on different datasets. Since both loss
terms are not analytical and involve complicated forward
pass of DNN, solving the nested optimization objective is
computationally expensive. Meanwhile, alternating one-step
approximation [9,25] cannot guarantee that the optimization
direction is optimal due to coupled update.
In contrast, in order to avoid coupled optimization over
the large searching space of network parameters w, we re-𝑥𝑡
LR Solver
(Eq 6)𝑦𝑡𝐶(;𝑤𝑐)
𝑤∗(𝑌𝑡)
Share weights EMA update Gradient propagateCE / LNLIPCEAC𝑥𝑣
𝑦𝑣 𝐿𝑣𝑎𝑙(Eq 8)𝐿𝑐(Eq 10)𝑥𝑡
(b) Application(a) DMLP
𝐹(∙;𝜃)𝐺(∙;𝜃𝐺∗) 𝐺(;𝜃𝐺∗)Self-supervised 
weightsFigure 3. The overall framework of DMLP. (a) Decoupled meta label puriﬁcation (Sec. 3.2). (b) The puriﬁed labels can be applied in normal
network retraining (CE) or other LNL loss (Sec. 3.3).
formulate the meta-learning objective in DMLP as:
min
ytE(xv;yv)2DvLval(fv;yv;w(yt))
s.t.w(yt) = arg min
wE(xt;yt)2DtLtrain(ft;yt;w)
ft=G(xt;
G); fv=G(xv;
G)(2)
where a pretrained feature extractor G:RHW3!Rdis
designed to extract d-dimensional image representation f. In
order to obtain representation of high quality, these extracted
features are utilized in a contrastive self-supervised learning
framework [5, 6] to update the parameters Gas pretraining
without noisy labels. Subsequently, the established feature
extractor f=G(x;
G)can learn a noise-agnostic descriptor
of image data, which is also highly separable in high di-
mensional feature space [37]. In this manner, we detach the
representation learning from noisy label correction, while
keeping strong separability of features.
Further, since the feature is representative and separable,
the loss termLtrain andLvalcan be formulated with simple
risk estimation functions (e.g. linear discrimination) instead
of complex DNN forward pass, making it possible to solve
the problem of Eq. (2) in a non-nested manner with an
analytical solution , which will be introduced in Sec. 3.2.
3.2. Non-nested Meta Label Puriﬁer
To solve the puriﬁcation problem of Eq. (2), we propose
two mutually reinforced solutions to seek puriﬁed training
labels as shown in Fig. 3, the intrinsic primary correction
(IPC) and extrinsic auxiliary correction (EAC) processes.
Intrinsic Primary Correction. IPC aims at performing
global-wise label puriﬁcation at a slow and steady pace.
Speciﬁcally, as shown in Fig. 3 (a), the features and labels of
a batch ofbtraining data are gathered into matrix Ft2Rbd
andYt2Rbcas
Ft= [ ft;1;ft;2;;ft;b]T; Yt= [yt;1;yt;2;;yt;b]T:
(3)
Since the feature descriptors are representative, we as-
sume there exists a simple linear estimation transformw2Rdc, which accurately predict the categorical dis-
tribution with ridge regression:
min
wLtrain(Ft;Yt;w) =k(Yt) Ftwk2+kwk2;
(4)
where()is the softmax function along the categorical
dimension to meet the normalization constraint, and is
a scaling factor. By solving the linear regression problem
of Eq. (4) through least square method, we can obtain its
closed-form solution w(Yt)on the training batch (Ft;Yt)
and derive its optimal prediction via linear regression on
samples fv;ifrom validation set Dv:
y0
v;i(Yt) =w(Yt)Tfv;i
=(Yt)TFt 
FtTFt+I 1fv;i:(5)
Intuitively, the discrepancy between the predicted results y0
v;i
and the ground truth labels yv;iofDvis due to the potential
noise fromYt, therefore we take the prediction discrepancy
as objective for label puriﬁcation:
Lval(Yt) =1
NvNvX
i=1jjy0
v;i(Yt) yv;ijj2+H(y0
v;i(Yt));
(6)
whereH()represents the entropy of input distribution as a
regularization term to sharpen the predicted label distribution
as [35]. With Eq. (5) and Eq. (6), the validation loss can be
expressed analytically by training labels Ytin a batch, thus
the noisy labels can be corrected steadily with correction
rateIby the gradient back-propagated from Eq. (6) as
Ytp+1:=Ytp Ir(Lval(Yp
t)): (7)
Extrinsic Auxiliary Correction. To accelerate the label
correction process, an external correction process is fur-
ther proposed. Speciﬁcally, an accompanied linear classiﬁer
C(;wc) :Rd!Rcwith learnable parameter wcis trained
along with the updated labels from IPC:
Lc(wc) =Lce(C(ft;wc);y0
t) +H(C(ft;wc));(8)Table 1. Comparison with state-of-the-art methods on CIFAR-10/100 datasets with symmetric noise. “CE” is the standard ConvNet trained
with Cross-Entropy loss in an end-to-end manner. “Classiﬁer” means adopts the pre-trained SimCLR features to re-train a linear classiﬁer.
“Val” denotes using a small clean validation set. DivideMix* denotes training DivideMix with the same validation set as additional data.
Dataset CIFAR-10 CIFAR-100
Method Val Noise ratio 20% 50% 80% 90% 20% 50% 80% 90%
Cross-Entropy (CE)Best 86.8 79.4 62.9 42.7 62.0 46.7 19.9 10.1%Last 82.7 57.9 26.1 16.8 61.8 37.3 8.8 3.5
Co-teaching +[36]Best 89.5 85.7 67.4 47.9 65.6 51.8 27.9 13.7%Last 88.2 84.1 45.5 30.1 64.1 45.3 15.5 8.8
PENCIL [35]Best 92.4 89.1 77.5 58.9 69.4 57.5 31.1 15.3%Last 92.0 88.7 76.5 58.2 68.1 56.4 20.7 8.8
REED [37]Best 95.8 95.6 94.3 93.6 76.7 73.0 66.9 59.6%Last 95.7 95.4 94.1 93.5 76.5 72.2 66.5 59.4
Sel-CL+ [18]Best 95.5 93.9 89.2 81.9 76.5 72.4 59.6 48.8%Last 95.1 93.3 88.7 81.6 76.1 72.0 59.2 48.6
MOIT+ [21]Best 94.1 91.8 81.1 74.7 75.9 70.6 47.6 41.8%Last 93.8 91.3 80.6 74.0 75.2 70.1 46.9 41.2
C2D-DivideMix [39]Best 96.3 95.2 94.4 93.5 78.6 76.4 67.7 58.7%Last 96.2 95.1 94.1 93.4 78.3 76.0 67.4 58.4
DivideMix [15]Best 96.1 94.6 93.2 76.0 77.3 74.6 60.2 31.5%Last 95.7 94.4 92.9 75.4 76.9 74.2 59.6 31.0
Meta-Learning [16]Best 92.9 89.3 77.4 58.7 68.5 59.2 42.4 19.5"Last 92.0 88.8 76.1 58.3 67.7 58.0 40.1 14.3
MLC [40]Best 92.6 88.1 77.4 67.9 66.8 52.7 21.8 15.0"Last 91.8 87.5 77.1 67.0 66.5 52.4 18.9 14.2
MSLC [9]Best 93.4 89.9 69.8 56.1 72.5 65.4 24.3 16.7"Last 93.3 89.4 68.8 55.2 72.0 64.9 20.5 14.6
DivideMix* [15]Best 96.1 94.9 93.6 77.3 77.7 74.8 60.7 32.5"Last 95.9 94.6 93.0 76.5 77.1 74.3 60.5 32.2
DMLP-NaiveBest 94.7 94.2 93.5 92.8 72.7 68.0 63.5 61.3"Last 94.2 94.0 93.2 92.0 72.3 67.4 63.2 60.9
DMLP-DivideMixBest 96.3 95.8 94.5 94.3 79.9 76.8 68.6 65.8"Last 96.2 95.6 94.3 94.0 79.4 76.1 68.5 65.4
wherey0
tis the updated training label from IPC, and Lce
denotes the cross entropy loss function. Since the accompa-
nied linear classiﬁer is intrinsically robust to noisy labels in
y0
t[24], it can quickly achieve high correction accuracy. With
this intuition, the predicted results of the classiﬁer C(ft;wc)
are used to correct labels periodically, speciﬁcally, the update
rule ofYtswitches to momentum update every Titerations:
Yp+1
t:= (1 E)Yp
t+EC(Ft;wc)ifp=nT; (9)
whereTandEare the period and momentum for update,
andnis an arbitrary positive integer to denote the n-th
time of EAC update. In a global sense, after Titerations
of training, EAC can quickly achieve locally optimal label
estimation by mimic of gradually updated labels from IPC,
which reversely facilitates the label correction of IPC by
providing cleaner training labels. Subsequently, IPC and
EAC form a positive loop and mutually improve the qualityof label correction of each other.
3.3. Application of DMLP
The DMLP is a ﬂexible label puriﬁer, the corrected train-
ing labelsy
tcan be applied in different ways in robust
learning scenarios, as shown in Fig. 3.
Naive classiﬁcation network with DMLP. In a simple
and direct manner, we can take the puriﬁed labels to retrain
a neural network with simple cross-entropy loss (CE), here
we term this simple application as DMLP-Naive.
LNL framework boosted by DMLP. Considering that
there may still be a small number of noisy or miscorrected la-
bels after puriﬁcation, another effective way to apply DMLP
is to take the puriﬁed labels as new training samples for the
existing LNL framework. In this work, we extend some clas-
sic frameworks [12, 15, 19, 30] with DMLP, and the boosted
LNL methods are denoted with preﬁx of “DMLP-” ( e.g.Table 2. Evaluation results with asymmetric noise of different noisy
ratio on CIFAR-10. “Validation” denotes the method exploits a
small clean validation set.
MethodNoisy ratio
Validation20% 40%
Joint-Optim [28] % 92.8 91.7
PENCIL [35] % 92.4 91.2
M-correction [1] % - 86.3
Iterative-CV [4] % - 88.0
DivideMix [15] % 93.4 93.4
REED [37] % 95.0 92.3
C2D-DivideMix [39] % 93.8 93.4
Sel-CL+ [18] % 95.2 93.4
GCE [11] % 87.3 78.1
RRL [17] % - 92.4
Zhang, et al. [38] " 92.7 90.2
Meta-Learning [16] " - 88.6
MSLC [9] " 94.4 91.6
DMLP-Naive " 94.6 93.9
DMLP-DivideMix " 95.2 95.0
Table 3. Top-1 testing accuracy on Clothing-1M testset. “Valida-
tion” denotes using the validation provided by [31].
Method Validation Top-1 Accuracy
PENCIL [35] % 73.49
DivideMix [15] % 74.76
RRL [17] % 74.90
GCE [11] % 73.30
C2D-DivideMix [39] % 74.30
REED [37] % 75.81
Meta-Learning [16] " 73.47
Self-Learning [13] " 76.44
MLC [40] " 75.78
MSLC [9] " 74.02
Meta-Cleaner [10] " 72.50
Meta-Weight [8] " 73.72
FaMUS [34] " 74.40
MSLG [7] " 76.02
DMLP-Naive " 77.77
DMLP-DivideMix " 78.23
DMLP-DivideMix, DMLP-ELR+, etc.)
4. Experiments
4.1. Experimental Settings
CIFAR-10/100. For the self-supervised pre-training
stage, we adopt the popular SimCLR algorithm [5] with
ResNet as the backbone network. Classiﬁers in meta-learner
are trained for 100 epochs with the Adam optimizer. For
the ﬁnal DivideMix algorithm, ResNet18 is adopted for a
fair comparison. IandEare set as 0:01and1:0respec-
tively. The scaling factor is set as 1:0. To ensure fairevaluation, we follow previous meta-learning based LNL
methods [8, 9] to randomly separate 1,000 images as the
clean validation set for CIFAR-10/100 [14], leaving the rest
as training samples. We strictly follow the protocol in [12]
to generate label noise. Speciﬁcally, symmetric noise is
generated by replacing labels with one of the other classes
uniformly, while the labels in asymmetric noise are disturbed
to their similar classes to simulate label noise in real-world
scenarios. Our experiments are conducted under different
noisy rates:2f20%;50%;80%;90%gfor symmetric and
2f20%;40%gfor asymmetric noises.
Clothing1M. For the ﬁrst stage on the Clothing1M [32],
the ResNet50 is trained with the ofﬁcial MoCo-v2 [6] to
fully leverage its advantages on large-scale datasets. After-
ward, the meta-learner is trained for 50 epochs.For DMLP-
DivideMix, ResNet50 is adopted and initialized with weights
from previous stages and trained for 80 epochs. Due to the
space constraint, we put more detailed experimental settings
and descriptions of each compared LNL method in our sup-
plementary materials.
4.2. Experimental Results
Comparison with state-of-the-art methods. We com-
pare our method with multiple recent competitive methods
on CIFAR-10/100 under various noisy settings (detailed
descriptions of these methods are provided in the supplemen-
tary materials). Both test accuracy of the best and last epoch
are reported. As shown in Table 1, the simple DMLP-Naive
can already achieve competitive results to most methods, the
superiority is more obvious in extremely noisy cases, further,
the DMLP-DivideMix achieves state-of-the-art performance
across all the settings. It is worth noting that directly utiliz-
ing the validation data to train DivideMix (i.e., DivideMix*)
only brings marginal improvement, while when equipped
with the puriﬁed labels by DMLP, signiﬁcant improvements
are obtained, indicating that DMLP is effective in terms of
utilizing validation set towards LNL problem. On the other
hand, though there exist other meta-learning methods utiliz-
ing validation set [9, 16, 40], DMLP shows great advantages
over them. It is also noticeable that compared with the orig-
inal DivideMix, the puriﬁed version of DMLP-DivideMix
achieves better results by a large margin, indicating that the
puriﬁed label from our approach is more friendly to boost
LNL frameworks. Table 2 shows comparison with the recent
methods on asymmetric noisy CIFAR-10 dataset. DMLP-
DivideMix outperforms REED by 0.2% and 2.7% under
different noisy ratios and obtains greater improvements over
the rest methods, demonstrating the ability of DMLP in han-
dling harder semantic-related noise. Finally, DMLP-based
methods suffer less from increasing noisy ratio than other
competitors, indicating its robustness to variant noisy levels .
In addition to artiﬁcial noise, we also evaluate DMLP
on the large-scale real-world noisy dataset Clothing1M. AsTable 4. Comparison between the LNL methods and their DMLP applications with symmetric noise on CIFAR-10/100. Speciﬁcally, the
9-layer CNN is adopted as the backbone network of Co-teaching.
Dataset CIFAR-10 CIFAR-100
Method/Noise ratio 20% 50% 80% 90% 20% 50% 80% 90%
Co-teaching [12]Best 82.6 73.0 24.0 14.6 50.5 38.2 11.8 4.9
Last 81.9 72.6 23.5 11.7 50.3 38.0 11.3 4.3
DMLP-Co-teachingBest 85.8 85.8 85.4 84.6 51.2 49.8 48.1 45.3
Last 85.6 85.6 85.3 84.5 51.0 49.3 47.8 45.1
CDR [30]Best 90.4 85.0 47.2 12.3 63.3 39.5 29.2 8.0
Last 82.7 49.4 16.6 10.1 62.9 39.5 9.7 4.5
DMLP-CDRBest 91.4 91.2 91.2 90.2 69.2 64.8 61.4 58.5
Last 91.2 90.8 90.6 89.3 68.3 64.3 61.1 57.9
ELR+ [19]Best 94.6 93.8 91.1 75.2 77.5 72.4 58.2 30.8
Last 94.4 93.7 90.5 73.5 76.2 72.2 56.8 30.6
DMLP-ELR+Best 94.9 94.1 93.0 92.5 77.8 73.6 63.9 60.5
Last 94.6 94.0 92.7 92.1 77.1 73.4 63.6 60.5
Figure 4. Comparison of corrected label accuracy under symmetric-
20% (left), symmetric-80% (right) noise settings on CIFAR-10.
shown in Table 3, simple DMLP-Naive can outperform all
other methods by a large margin, and DMLP-DivideMix
further improves the accuracy by about 0:46%. The results
indicate that DMLP is more suitable for noise from real-
world situations .
Label correction accuracy. Fig. 4 compares the label
accuracy after correction in our meta-learner against coupled
puriﬁers MLC and MSLC on CIFAR-10. Speciﬁcally, the
one-hot form of the corrected pseudo-labels is compared
with the ground truth for evaluation. As Fig. 4 shows, labels
can be rapidly corrected to accuracy over 92% in low noise
cases. For severe label noise, DMLP can still improve label
accuracy similar to low-noise settings. The overall corrected
label accuracy within DMLP is superior against other com-
petitors across all noise settings (More detailed experimental
results can be found in the supplementary material).
Generality of DMLP. To validate the generalization abil-
ity of DMLP, other than DivideMix, another 3 popular LNL
methods, ELR+ [19], Co-teaching [12], and CDR [30] are
further adopted to work collaboratively with puriﬁed labels
of DMLP. As show in Table 4, all the applications of DMLP
perform consistently better over their corresponding base-
lines, especially under high-level noise cases. It is worth
noting that since CDR highly relies on the early stoppingTable 5. Ablation study for the effectiveness of IPC and EAC in
DMLP-Naive on CIFAR-10.
Component CIFAR-10Clothing1MIPC EAC 20% 50% 80% 90%
7 XBest 93.7 93.3 91.1 67.4 76.5
Last 93.0 92.9 90.6 66.5 76.1
X 7Best 87.8 85.7 79.9 76.0 76.8
Last 87.2 85.5 79.4 75.4 76.5
X XBest 94.7 94.2 93.5 92.8 77.7
Last 94.2 94.0 93.2 92.0 77.6
technique, it suffers from a severe memorization effect in
the training process, leading to a discrepancy between best
results and last performance. In contrast, when training CDR
with our puriﬁed labels, this discrepancy almost disappears,
demonstrating the labels output by DMLP have a better
quality to suppress the memorization effect, thus alleviating
the reliance on early stopping. Therefore, the results indi-
cate that the puriﬁed labels of DMLP are friendly to boost
LNL frameworks. More detailed experimental results can be
found in the supplementary materials.
4.3. Ablation Studies
Analysis on different label correction process. We ex-
plore the inﬂuence of IPC and EAC on the performance
of DMLP-Naive in Table 5, when one of the process is
excluded, the updated labels from the other are applied to
retrain a new model. It is observed that EAC performs well
in low noise cases due to its intrinsic robustness, as the noise
ratio increases, the performance drops rapidly. On the other
hand, IPC is more robust to high-level noise, but there exists
a large gap compared with the full DMLP pipeline due to its
slow optimization process. In contrast, when IPC and EAC
work collaboratively, DMLP can achieve optimal results.
Comparison against other coupled puriﬁers with pre-Table 6. Comparison with coupled meta label correction methods
MLC [40] and MSLC [9] on CIFAR-10. "*" denotes training with
SimCLR pretrained ResNet-18.
MethodNoisy ratio
20% 50% 80% 90%
MLC*Best 91.8 86.2 77.6 72.9
Last 91.6 85.9 77.5 72.6
MSLC*Best 92.0 87.7 78.0 67.8
Last 92.0 87.5 77.9 67.3
DMLP-Naive*Best 94.0 93.7 93.1 92.3
Last 93.9 93.4 92.9 91.9
MLC*-DivideMixBest 95.3 94.0 93.0 86.6
Last 95.0 93.6 92.7 86.5
MSLC*-DivideMixBest 95.7 94.9 93.8 83.0
Last 95.5 94.8 93.1 82.8
DMLP*-DivideMixBest 96.3 95.6 94.1 93.8
Last 96.0 95.2 94.0 93.6
Table 7. Investigation of the validation set size on Clothing1M.
 10% 20% 30% 40% 50% 100%
Accuracy (%) 75.50 76.40 76.61 77.00 77.30 77.31
training. To solely evaluate the inﬂuence of decoupled
puriﬁcation, we train two coupled meta label correction
methods MLC and MSLC with the same self-supervised
pretrained weights and apply their corrected labels to naive
training or DivideMix for fair comparisons. As shown in Ta-
ble. 6, though self-supervised weights can marginally boost
the performance of coupled label correctors, there still exists
a large gap between their performance and DMLP, especially
for high noisy cases. Moreover, when further applying the
corrected labels to mainstream LNL framework DivideMix,
our method can also consistently outperform the coupled
counterparts across all the noisy settings, demonstrating our
corrected labels are of better quality. Therefore, these results
verify that the superiority mainly attributes to decoupled
label correction instead of self-supervised pretraining .
Effect of different feature representation for puriﬁca-
tion. The quality of features plays a crucial role in the label
correction process of DMLP since the distribution of learned
features is closely related to the rationality behind the linear
estimation assumption in high-dimensional space. There-
fore, we study the inﬂuence of different features on perfor-
mance. Speciﬁcally, two types of features are investigated,
including features from the ResNet-18/50 which load the
self-supervised pre-trained weights. As the results in Table 8
show, the features from the ResNet-18 lead to slightly poor
performance, while it brings performance improvements
when using features from self-supervised ResNet-50. This
observation indicates that although feature representation of
higher quality beneﬁts the puriﬁcation results, DMLP is notTable 8. Ablation study for adopting different features in DMLP-
Naive on CIFAR-10, where "R18/50" denote "ResNet-18/50" and
"M/S" represent "MoCo/SimCLR".
Feature SourceNoisy ratio
20% 50% 80% 90%
R18 (M)Best 93.8 93.3 92.2 90.4
Last 93.7 92.7 92.1 90.0
R18 (S)Best 94.0 93.7 93.1 92.3
Last 93.9 93.4 92.9 91.9
R50 (S)Best 94.7 94.2 93.5 92.8
Last 94.2 94.0 93.2 92.0
Table 9. Comparison between recent semi-supervised methods and
DMLP-DivideMix on CIFAR-10/100 with 100% noisy ratio.
Method CIFAR-10 CIFAR-100
MeanTeacher 83.0 31.0
MixMatch 87.9 57.7
FixMatch 88.1 56.3
UDA 88.2 56.1
Ours 91.7 60.1
very sensitive to the representation ability of input feature.
Effect of validation size. We examine how the number
of validation set affect performance. Speciﬁcally, validation
sizes from 10% to 100% of the whole validation set are eval-
uated on Clothing1M for DMLP-Naive. As shown in Table
7, DMLP-Naive achieves similar performance regardless of
the validation size Nv, demonstrating DMLP is not sensitive
to the number of images. It is worth noting that even using
only 10% of the validation set (around 0.1% of training data),
DMLP-Naive still achieves high accuracy and outperforms
most methods in Table.3, indicating that the effectiveness of
DMLP is not heavily relied on validation size.
Performance under extremely noisy setting. In an ex-
tremely noisy scenario where all labels in the training set
are unreliable except the given clean validation set, the LNL
problem is converted into a partially-labeled semi-supervised
learning problem, therefore we further compare DMLP-
DivideMix with some state-of-the-art semi-supervised learn-
ing algorithms, including MeanTeacher [29], MixMatch [3],
FixMatch [26] and UDA [33], under the 100% symmetric
noise case on CIFAR-10 and CIFAR-100. From the results
in Table 9, DMLP-DivideMix performs optimally among all
methods when using the validation set as labeled samples
for training, indicating the proposed method is potential for
broader applications.
5. Conclusion
In this paper, we propose a ﬂexible and novel multi-stage
robust learning approach termed as DMLP. The core ofDMLP is a carefully-designed meta-learning based label
puriﬁer, which decouples the complex bi-level optimization
problem into representation and label distribution learning,
thus helping the meta-learner focus on correcting noisy la-
bels in a faster and more precise manner even under ex-
tremely noisy scenarios. Further, DMLP can be applied
either for naive retraining on noisy data or assistance of
existing LNL methods to boost performance. Extensive ex-
periments conducted on several synthetic and real-world
noisy datasets verify the superiority of the proposed method.
References
[1]Eric Arazo, Diego Ortego, Paul Albert, Noel E O’Connor,
and Kevin McGuinness. Unsupervised label noise modeling
and loss correction. In Proc. International Conference on
Machine Learning (ICML) , 2019.
[2]Devansh Arpit, Stanislaw Jastrzebski, Nicolas Ballas, David
Krueger, Emmanuel Bengio, Maxinder S. Kanwal, Tegan
Maharaj, Asja Fischer, Aaron C. Courville, Yoshua Bengio,
and Simon Lacoste-Julien. A closer look at memorization in
deep networks. In Proc. International Conference on Machine
Learning (ICML) , 2017.
[3]David Berthelot, Nicholas Carlini, Ian J. Goodfellow, Nicolas
Papernot, Avital Oliver, and Colin Raffel. Mixmatch: A holis-
tic approach to semi-supervised learning. In Proc. Advances
in Neural Information Processing Systems (NeurIPS) , 2019.
[4]Pengfei Chen, Benben Liao, Guangyong Chen, and Shengyu
Zhang. Understanding and utilizing deep neural networks
trained with noisy labels. In Proc. International Conference
on Machine Learning (ICML) , 2019.
[5]Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geof-
frey Hinton. A simple framework for contrastive learning of
visual representations. In Proc. International Conference on
Machine Learning (ICML) , pages 1597–1607. PMLR, 2020.
[6]Xinlei Chen, Haoqi Fan, Ross B. Girshick, and Kaiming
He. Improved baselines with momentum contrastive learning.
arXiv preprint arXiv:2003.04297 .
[7]Algan et al. Meta soft label generation for noisy labels. In
2020 25th International Conference on Pattern Recognition
(ICPR) , pages 7142–7148, 2021.
[8]Shu et al. Meta-weight-net: Learning an explicit mapping for
sample weighting. In NeurIPS , 2019.
[9]Wu et al. Learning to purify noisy labels via meta soft label
corrector. In AAAI , 2021.
[10] Zhang et al. Metacleaner: Learning to hallucinate clean
representations for noisy-labeled visual recognition. In CVPR ,
2019.
[11] Aritra Ghosh and Andrew Lan. Contrastive learning improves
model robustness under label noise. In Proc. IEEE Conference
on Computer Vision and Pattern Recognition (CVPR) , pages
2703–2708, 2021.
[12] Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu,
Weihua Hu, Ivor Tsang, and Masashi Sugiyama. Co-teaching:
Robust training of deep neural networks with extremely noisy
labels. In Proc. Advances in Neural Information Processing
Systems (NeurIPS) , 2018.[13] Jiangfan Han, Ping Luo, and Xiaogang Wang. Deep self-
learning from noisy labels. In Proc. IEEE International Con-
ference on Computer Vision (ICCV) , 2019.
[14] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple
layers of features from tiny images. 2009.
[15] Junnan Li, Richard Socher, and Steven CH Hoi. Dividemix:
Learning with noisy labels as semi-supervised learning. In
Proc. International Conference on Learning Representations
(ICLR) , 2020.
[16] Junnan Li, Yongkang Wong, Qi Zhao, and Mohan S. Kankan-
halli. Learning to learn from noisy labeled data. In Proc.
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR) , 2019.
[17] Junnan Li, Caiming Xiong, and Steven CH Hoi. Learning
from noisy data with robust representation learning. In Proc.
IEEE International Conference on Computer Vision (ICCV) ,
pages 9485–9494, 2021.
[18] Shikun Li, Xiaobo Xia, Shiming Ge, and Tongliang Liu.
Selective-supervised contrastive learning with noisy labels.
arXiv preprint arXiv:2203.04181 , 2022.
[19] Sheng Liu, Jonathan Niles-Weed, Narges Razavian, and Car-
los Fernandez-Granda. Early-learning regularization prevents
memorization of noisy labels. In Proc. Advances in Neural
Information Processing Systems (NeurIPS) , 2020.
[20] Tongliang Liu and Dacheng Tao. Classiﬁcation with noisy
labels by importance reweighting. IEEE Transactions on
Pattern Analysis and Machine Intelligence (TPAMI) , 2015.
[21] Diego Ortego, Eric Arazo, Paul Albert, Noel E O’Connor,
and Kevin McGuinness. Multi-objective interpolation training
for robustness to label noise. In Proc. IEEE Conference on
Computer Vision and Pattern Recognition (CVPR) , pages
6606–6615, 2021.
[22] Giorgio Patrini, Alessandro Rozza, Aditya Krishna Menon,
Richard Nock, and Lizhen Qu. Making deep neural networks
robust to label noise: A loss correction approach. In Proc.
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR) , 2017.
[23] Geoff Pleiss, Tianyi Zhang, Ethan R. Elenberg, and Kilian Q.
Weinberger. Identifying mislabeled data using the area under
the margin ranking. In Proc. Advances in Neural Information
Processing Systems (NeurIPS) , 2020.
[24] Scott E. Reed, Honglak Lee, Dragomir Anguelov, Christian
Szegedy, Dumitru Erhan, and Andrew Rabinovich. Training
deep neural networks on noisy labels with bootstrapping. In
Proc. International Conference on Learning Representations
(ICLR) , 2015.
[25] Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urtasun.
Learning to reweight examples for robust deep learning. In
Proc. International Conference on Machine Learning (ICML) ,
2018.
[26] Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao
Zhang, Han Zhang, Colin Raffel, Ekin Dogus Cubuk, Alexey
Kurakin, and Chun-Liang Li. Fixmatch: Simplifying semi-
supervised learning with consistency and conﬁdence. In
Proc. Advances in Neural Information Processing Systems
(NeurIPS) , 2020.[27] Hwanjun Song, Minseok Kim, and Jae-Gil Lee. Selﬁe: Refur-
bishing unclean samples for robust deep learning. In Proc. In-
ternational Conference on Machine Learning (ICML) , 2019.
[28] Daiki Tanaka, Daiki Ikami, Toshihiko Yamasaki, and Kiy-
oharu Aizawa. Joint optimization framework for learning
with noisy labels. In Proc. IEEE Conference on Computer
Vision and Pattern Recognition (CVPR) , 2018.
[29] Antti Tarvainen and Harri Valpola. Mean teachers are better
role models: Weight-averaged consistency targets improve
semi-supervised deep learning results. In Proc. Advances in
Neural Information Processing Systems (NeurIPS) , 2017.
[30] Xiaobo Xia, Tongliang Liu, Bo Han, Chen Gong, Nannan
Wang, Zongyuan Ge, and Yi Chang. Robust early-learning:
Hindering the memorization of noisy labels. In Proc. Inter-
national Conference on Learning Representations (ICLR) ,
2021.
[31] Tong Xiao, Tian Xia, Yi Yang, Chang Huang, and Xiaogang
Wang. Learning from massive noisy labeled data for image
classiﬁcation. In Proc. IEEE Conference on Computer Vision
and Pattern Recognition (CVPR) , 2015.
[32] Tong Xiao, Tian Xia, Yi Yang, Chang Huang, and Xiaogang
Wang. Learning from massive noisy labeled data for image
classiﬁcation. In Proc. IEEE Conference on Computer Vision
and Pattern Recognition (CVPR) , pages 2691–2699, 2015.
[33] Qizhe Xie, Zihang Dai, Eduard H. Hovy, Thang Luong, and
Quoc Le. Unsupervised data augmentation for consistency
training. In Proc. Advances in Neural Information Processing
Systems (NeurIPS) , 2020.
[34] Youjiang Xu, Linchao Zhu, Lu Jiang, and Yi Yang. Faster
meta update strategy for noise-robust deep learning. In Proc.
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR) , pages 144–153, 2021.
[35] Kun Yi and Jianxin Wu. Probabilistic end-to-end noise correc-
tion for learning with noisy labels. In Proc. IEEE Conference
on Computer Vision and Pattern Recognition (CVPR) , 2019.
[36] Xingrui Yu, Bo Han, Jiangchao Yao, Gang Niu, Ivor W Tsang,
and Masashi Sugiyama. How does disagreement help gen-
eralization against label corruption? In Proc. International
Conference on Machine Learning (ICML) , 2019.
[37] Hui Zhang and Quanming Yao. Decoupling representa-
tion and classiﬁer for noisy label learning. arXiv preprint
arXiv:2011.08145 , 2020.
[38] Z. Zhang, H. Zhang, S. Arik, H. Lee, and T. Pﬁster. Distill-
ing effective supervision from severe label noise. In Proc.
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR) , 2020.
[39] Evgenii Zheltonozhskii, Chaim Baskin, Avi Mendelson,
Alex M Bronstein, and Or Litany. Contrast to divide: Self-
supervised pre-training for learning with noisy labels. In Proc.
IEEE/CVF Winter Conference on Applications of Computer
Vision (WACV) , pages 1657–1667, 2022.
[40] Guoqing Zheng, Ahmed Hassan Awadallah, and Susan Du-
mais. Meta label correction for noisy label learning. In
Association for the Advancement of Artiﬁcial Intelligence
(AAAI) , 2021.