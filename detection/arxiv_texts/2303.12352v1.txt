TRAINING MULTILAYER PERCEPTRONS BY SAMPLING
WITH QUANTUM ANNEALERS
Frances Fengyi Yang, Michele Sasdelli & Tat-Jun Chin
School of Computer Science
The University of Adelaide
Adelaide, SA 5005, Australia
ffengyi.yang,michele.sasdelli,tat-jun.chin g@adelaide.edu.au
ABSTRACT
A successful application of quantum annealing to machine learning is training re-
stricted Boltzmann machines (RBM). However, many neural networks for vision
applications are feedforward structures, such as multilayer perceptrons (MLP).
Backpropagation is currently the most effective technique to train MLPs for su-
pervised learning. This paper aims to be forward-looking by exploring the training
of MLPs using quantum annealers. We exploit an equivalence between MLPs and
energy-based models (EBM), which are a variation of RBMs with a maximum
conditional likelihood objective. This leads to a strategy to train MLPs with quan-
tum annealers as a sampling engine. We prove our setup for MLPs with sigmoid
activation functions and one hidden layer, and demonstrated training of binary
image classiÔ¨Åers on small subsets of the MNIST and Fashion-MNIST datasets
using the D-Wave quantum annealer. Although problem sizes that are feasible
on current annealers are limited, we obtained comprehensive results on feasible
instances that validate our ideas. Our work establishes the potential of quantum
computing for training MLPs.
1 I NTRODUCTION
The encouraging progress in building quantum computers in the past decade has sparked interest in
quantum computing for computer vision (CV) [26; 11; 8; 9] and machine learning (ML) [10; 35;
34; 42]. Major foci include using quantum computers to speed up computations and learning more
expressive models using quantum representations.
There are two main quantum computing paradigms: gate quantum computing (GQC) and adiabatic
quantum computing (AQC). Currently, the practical realisation of AQC in the form of quantum
annealers has yielded machines with a larger number of qubits. The primary example is D-Wave
Advantage [28], whose quantum processing unit (QPU) contains >5,000 qubits. While this is still
far from a size that will make quantum computing truly revolutionary ( >1 M qubits), it can already
support exploratory research.
A notable example of using quantum annealers in ML is training Boltzmann machines (BM) [15;
41; 1], which are powerful generative models that can solve complex CV tasks such as classiÔ¨Åcation
[29], image labelling [24] and facial feature tracking [43]. BMs are usually trained via contrastive
divergence (CD) [12], which involves generating samples from proposal distributions. Computa-
tionally this requires matrix multiplications that scale cubically with the number of variables [13],
which can be costly for large problem sizes. AQC enables a workaround that samples directly from
the distribution in constant time with quantum annealers. In practice, quantum sampling is not
straightforward since current noisy intermediate-scale quantum (NISQ) devices are prone to noise,
and careful tuning of the control parameters such as the qubit biases, coupling strength and the
system temperature are required. Nonetheless, promising results from training BMs with quantum
sampling have been reported [1; 15].
Less explored is the training of feedforward neural networks (FNN) [38] with quantum annealers.
Unlike BMs which are usually used to model data distributions, FNNs are usually employed to learn
1arXiv:2303.12352v1  [cs.LG]  22 Mar 2023Hidden LayerOutput Layer
kVisible LayerWxInput Layer kyMultilayer Perceptron
Energy-Based ModelPegasus Embedding on Quantum Annealer
xkyVisible LayerHidden LayervHidden LayerRestricted Boltzmann MachineW(1)W(1)W(2)
W(2)Figure 1: RBM and EBM are generative models, where EBM is a variant of RBM with
maximum conditional likelihood objective. MLP is non-generative and is typically
employed for supervised learning. We establish the equivalence in training MLP and
EBM, which allows to embed the corresponding proposal distribution for MLP in the
QPU to achieve quantum sampling for training MLPs.
functions, e.g., for classiÔ¨Åcation or regression, which underpin many important CV applications.
Notable FNNs are multilayer perceptrons (MLP) [39], convolutional neural networks (CNN) [3]
and recurrent neural networks (RNN) [36]. The most successful method to train FNNs is backprop-
agation [32], which minimises a loss that is typically related to supervised learning. Since FNNs are
not generative, it is unclear how quantum sampling can be employed for training FNNs.
Sasdelli and Chin [33] leveraged the ability of quantum annealers to solve combinatorial problems
to train binary neural networks (BNN), which are a type of FNN. This requires formulating learning
as quadratic unconstrained binary optimisation (QUBO). However, the resulting QUBO consumes
a number of qubits that scales with the input dimensions and the number of weights in the BNN,
which restricts the method to low-dimensional problems and/or low capacity models on currently
limited quantum annealers.
1.1 C ONTRIBUTIONS
In this paper, we explore quantum sampling to train MLPs. We exploit an equivalence between train-
ing optimisation in MLPs and sampling proposal distributions in energy-based models (EBM) [27].
The latter are a variation to restricted Boltzmann machines (RBM) [37, Chap. 6] with a maximum
conditional likelihood objective; see Fig. 1.
By exploiting the equivalence above, we developed a novel quantum sampling method to train MLPs
for supervised learning. Our approach requires the number of qubits equal to the number of hidden
units plus one, and allows for non-binary weights and batch training. Compared to [33], our method
allows larger networks to be trained given the same qubit count. Though the network size is ulti-
mately limited by the capacity of current QPUs (see Sec. 1.2), comprehensive results on feasible
instances validate our ideas.
Our work differs from implementing and training neural networks on GQC, e.g., [5], where the
neural networks are inherently quantum, thus requiring quantum computers for training and infer-
ence. In contrast, our work employs quantum computers on the training optimisation problem, and
inference on the model can be performed classically.
21.2 S HORTCOMINGS AND OUTLOOK
Our method cannot yet beat backpropagation. The capacity of current QPUs limits the size of
feasible MLPs (note that the sparse topology of the QPU necessitates a minor embedding step that
increases the number of qubits consumed; see Fig. 1). We were able to train an MLP with one
hidden layer of 548 units and input dimension of 784 for the MNIST and Fashion-MNIST datasets.
Overheads in data transfer between CPU and QPU also add to training times that are not competitive
against classical solutions.
The value of our work lies in the conceptual innovations that bring quantum computing to bear on
a problem of interest in the CV and ML community. Thus, our work enables the community to
leverage the potential beneÔ¨Åts of quantum technology, which is improving rapidly.
2 R ELATED WORK
2.1 BM AND MLP
A BM is an undirected graphical neural network with stochastic variables in both the visible and
hidden layers, with full connections in between. The structure of a BM encodes a joint probability
distribution of the variables.
Unlike BMs, standard MLPs are non-generative and are usually employed to learn classiÔ¨Åcation
and regression functions [31]. MLP is a fundamental FNN structure that underpin more advanced
models such as CNN and RNN.
2.2 Q UANTUM ANNEALING IN CV AND ML
Quantum computing offers a new paradigm to solve challenging computational problems. The po-
tential for CV and ML was recognised quite early on [14; 22; 40; 21; 16]. Recent progress in building
quantum computers has reinvigorated interested in the topic. In particular, quantum annealers now
contain>5;000qubits [28], which allow small instances of practical problems to be examined. We
survey recent works on quantum annealing (QA) for CV and ML.
Combinatorial optimisation The ability of quantum annealers to solve combinatorial optimisa-
tion has been examined for graph matching [8], view synchronisation [11], robust geometric Ô¨Åtting
[17], object tracking [44] and training binary neural networks [33] on a small scale. However, real
quantum annealers are imperfect; fundamentally they do not fully satisfy the assumptions of the
adiabatic theorem [19]. Other constraints such as limited precision and hyperparameter tuning [17]
also affect solution optimality.
Quantum sampling By exerting some level of control over the annealing process, quantum an-
nealers can sample from Boltzmann-like distributions [6; 4; 30]. Benedetti et al. [7] built a hardware-
embedded generative model on a D-Wave quantum annealer with 1,152 qubits for image reconstruc-
tion, generation and classiÔ¨Åcation on simple input images of size 7 6. In the classiÔ¨Åcation task, the
generative model was only used for pre-training. Amin et al. [4] implemented the energy function
of BM on the quantum annealer, which led to a new quantum Boltzmann distribution of the problem
Hamiltonian. Their quantum Boltzmann Machine (QBM) was trained by introducing a lower-bound
to the log-likelihood to allow effective gradient estimation by sampling. Adachi and Henderson [1]
trained a deep belief net (DBN) on a 512-qubit D-Wave machine with 8 faulty qubits. The input
size is limited to 32 and classiÔ¨Åcation is achieved by adding a downstream classiÔ¨Åer. Dorband [18]
implemented a multi-layer semi-restricted BM on a D-Wave quantum annealer, where they allevi-
ate the qubit limitation by implementing a virtual quantum annealer that was conÔ¨Ågured and run
separately from the actual QPU.
As mentioned in Sec. 1.1, our work employs quantum annealer as a sampling engine. However,
unlike previous works that subscribe to this usage paradigm [6; 4; 30; 7; 18; 1], we use quantum
sampling to train an MLP, which is a non-generative model usually used for supervised learning.
33 E QUIVALENCE BETWEEN MLP AND EBM
In this section, we establish an equivalence between the training optimisation problem of the fol-
lowing models:
‚Ä¢ MLP with sigmoid activation, trained for a classiÔ¨Åcation task with a binary cross-entropy loss.
‚Ä¢ EBM with binary stochastic units and quadratic energy, trained by maximising the log-likelihood
of generating the correct label, conditioned on the input data.
The model structures are shown in Fig. 1. Later, Sec. 4 will discuss quantum sampling to train
EBMs.
3.1 M AIN IDEA
We show that the models are ‚Äúdual‚Äù up to Ô¨Årst-order approximations, in the sense that training one
implies training the other, and the optimised weights of one can be transferred to the other without
changing the decisions. The strategy is to show that by assuming small weights, the gradient of the
log-likelihood of conditional probability of the EBM equals the gradient of the binary cross entropy
of the MLP. The rest of this section will mathematically prove this equivalence, while Sec. 5 will
empirically verify it.
3.2 EBM
An EBM with one hidden layer deÔ¨Ånes the energy
E(x;k;y) =kTW(1)x+yTW(2)k; (1)
where xis a vector of Ninputs and yis a vector of Moutputs. Together, xandymake up the
visible layer. The hidden layer kis a vector of Kfeatures. We consider only inputs xthat are
binary (taking values of 0or1). The weights W(1)andW(2)are continuous. yandkare also
constrained to be binary. This energy corresponds to the physical energy of the system (for a correct
temperature) for a state deÔ¨Åned by ( x,k,y).
To simplify exposition, we omit the biases from (1), though biases can be included without affecting
our results; see Appendix A for details. Also, our derivations hold for M1output units, though
in our experiments (Sec. 5) we tested only on cases with M= 1.
From (1), there are 2N+M+Kpossible states and the probability of a speciÔ¨Åc state (x;k;y)is deÔ¨Åned
as:
P(x;k;y) =e E(x;k;y)
Z(2)
where the partition function Zis
Z=X
fx;k;yge E(x;k;y); (3)
which sums over the 2N+M+Kpossible combinations of (x;k;y). The probability of generating
outputs yand inputs xis obtained marginalising k
P(y;x) =X
fkgP(y;k;x); (4)
and the probability of generating an input xis
P(x) =X
fk;ygP(y;k;x): (5)
Given an EBM with trained weights W(1)andW(2), we predict the outputs (labels) yfor a given
set of input values xby calculating the conditional probability
P(yjx) =P(y;x)
P(x): (6)
43.3 T RAINING AN EBM
We aim to learn the weights from a training dataset D=f(x`;y`)gL
`=1. This is achieved by max-
imising the expectation of the log conditional probability
E^x;^y[logP(yjx)]; (7)
where E^x;^yindicates that the expectation is taken over the true underlying joint distribution PDof
xandy,i.e.,
E^x;^y[f(x;y)] =X
fx;ygf(x;y)PD(x;y) (8)
for an arbitrary f(x;y), which sums over all possible combinations of x;y. Equation (7) can be
expanded as
E^x;^y[logP(yjx)] =E^x;^y
logP(y;x)
P(x)
=E^x;^y[logP(y;x)] E^x;^y[logP(x)]
=E^x;^y[logP(y;x)] E^x[logP(x)];(9)
where E^xis the expectation taken over the true underlying distribution of variable xonly.
Following CD [12, Sec. 1], the partial differentiation of the terms in (9) against the (j;i)-th element
ofW(1)are
@W(1)j;iE^x;^y[logP(y;x)] =E^x;^y[kjxi] E[kjxi]; (10)
@W(1)j;iE^x[logP(x)] =E^x[kjxi] E[kjxi]; (11)
wherekjandxiare thej-th andi-th element of kandx,Eis the expectation over the model
distribution, i.e.,
E[f(x;y)] =X
fx;ygf(x;y)P(x;y); (12)
andE^xis the expectation over the true distribution of x
E^x[f(x;y)] =X
fx;ygf(x;y)P(yjx)PD(x) (13)
with the other variable ymarginalised over the conditional model distribution. Note that kjis
conditionally dependent on xandy, as per the structure of the EBM (see Fig. 1). Subtracting (11)
from (10) yields the gradient of our training objective
@W(1)j;iE^x;^y[logP(yjx)] =E^x;^y[kjxi] E^x[kjxi]: (14)
In a similar way, the gradient for W(2)is obtained as
@W(2)j;iE^x;^y[logP(yjx)] =E^x;^y[yjki] E^x[yjki]: (15)
To develop explicit formulae for the above, we use the deÔ¨Ånitions of the expectations to rewrite
E^x;^y[kjxi] =E^x;^y[E[kjjx;y]xi]; (16)
E^x[kjxi] =E^x[E[kjjx]xi]
=E^x2
4X
fyg(E[kjjx;y]P(yjx))xi3
5: (17)
Then, by factoring in the EBM structure, we have
E^x;^y[kjxi] =E^x;^yh
xi(W(1)x+W(2)Ty)ji
; (18)
E^x[kjxi] =E^x2
4xiX
fyg
(W(1)x+W(2)Ty)P(yjx)
j3
5; (19)
where(z) =1
1+e zis the logistic function. The expectation (18) can be estimated from the dataset
D, while samples from P(yjx)are required in addition to Dto estimate (19); the standard way to
sample is Gibbs sampling, which is replaced by quantum sampling in our method; more details in
Sec. 4. With the computed gradients (14) and (15), the weights W(1)andW(2)can be iteratively
updated using any gradient-based update rule.
53.4 C ONNECTING EBM TOMLP
To connect EBM to MLP, we begin by taking Ô¨Årst-order Taylor expansions of the expectations and
activation functions in (14) and (15). DeÔ¨Åne
f(y) =E[kjjx;y] =(W(1)x+W(2)Ty)j: (20)
We expandf(y)over the moments of the distribution of P(yjx)around E[yjx]:
E[f(y)]f(E[y]) +@2f(E[y])
2Var[y]; (21)
where the second derivative of fis quadratic in the weights, i.e., the second term of (21) is of order
O(W(2)2).
We assume that the weights are small, i.e.,jW(l)
i;jj  1forl= 1;2and for all (i;j). This is
justiÔ¨Åable since the weights of BMs are usually initialised to be small random values, e.g., from
a Gaussian distribution with zero mean and a standard deviation of 0.01 [23, Section 8], and they
remain typically smaller than one in absolute value during training. This was observed throughout
our experiments.
It follows (19) can be approximated as
E^x[kjxi]E^xh
xi(W(1)x+W(2)TE[yjx])j
; (22)
where the second-order term has been removed. We now perform Taylor expansions of the sigmoid
function around W(1)xin (18) and (22) as
E^x;^y[kjxi]E^x;^y
xi
(W(1)x) +W(2)Ty@(W(1)x)
j
E^xh
xi(W(1)x)ji
+E^x;^y
xi
W(2)Ty@(W(1)x)
j (23)
and
E^x[kjxi]E^x
xi
(W(1)x) +W(2)TE[yjx]@(W(1)x)
j
E^xh
xi(W(1)x)ji
+E^x
xi
W(2)TE[yjx]@(W(1)x)
j
;(24)
where in (23) the Ô¨Årst expectation can be done on ^xalone because the quantity does not depend on
y.
Taking Taylor expansion of E[yjx]over the moments of the distribution P(kjx)and ignoring second
order terms of W(2)again, we can rewrite
E[yjx] =X
fkgE[yjk]P(kjx)
=X
fkg(W(2)k)P(kjx)(W(2)E[kjx]): (25)
Factoring in the network structure to expand E[kjx], and further assuming kW(2)kF kW(1)kF,
which is justiÔ¨Åed because the second layer has fewer neurons than the Ô¨Årst, we can rewrite (25) as
(W(2)X
fyg((W(1)x+W(2)Ty)P(yjx)))(W(2)(W(1)x)): (26)
We collect all terms and substitute into (14). Since the argument of the expectation does not depend
ony, we replace the expectation E^xwithE^x;^yand rewrite (14) as
@W(1)j;iE^x;^y[logP(yjx)]E^x;^y
xi
@(W(1)x)W(2)T
y (W(2)(W(1)x))
j
: (27)
6Similarly, we can rewrite gradient (15) using the same stpdf; see Appendix A for details.
The output of an MLP with one hidden layer is
z(x) =(W(2)(W(1)x)): (28)
The standard sigmoid cross-entropy loss between ground-truth label yand output zis
L(y;z) = ylog (z) (1 y) log (1 z) (29)
The gradient of the loss for W(1)
j;iis
@W(1)j;iL(y;z) =@W(1)j;iL(y;(W(2)(W(1)x)))
=xi
@(W(1)x)W(2)T
y (W(2)(W(1)x))
j:(30)
Taking the average of (30) over the dataset Dyields (27). In a similar way, we can equate the
gradient ofLforW(2)
j;iwith gradient of the expectation of the conditional log-likelihood w.r.t. W(2)
j;i;
see Appendix A for details.
Typical image datasets have continuous pixel values. The relations derived in the current section are
also valid for xthat take continuous values by generalizing the discrete probability distributions to
a continuous domain for the variables x. In the next section, we show experiments where the input
xare images.
Linear Transformation &  Reformulation if RequiredBinary Quadratic Model Ising ModelQPUGenerated Quantum SamplesClassical ModelMinor  EmbeddingQuantum Annealing
0.0 0.2 0.4 0.6 0.8 1.0
Time30
20
10
0102030Eigenvalues of H(t)
Figure 2: Left: The quantum sampling process starts from reformulating the classical model to the
Ising model through BQM. Minor embedding brings the process onto the hardware. Sampling is
then conducted by measuring the output state of the process. Right : Quantum annealing schedule.
Time is normalised from the annealing time, which is 20 sby default in the D-Wave system.
4 T RAINING EBM WITH QUANTUM SAMPLING
In this section, we sketch how quantum annealers can be employed to generate samples from EBMs,
which in turn can be used to compute the gradient of the expectation of log-likelihood. The equiva-
lence of the gradients of our EBM and MLP established in Sec. 3 thus implies that quantum sampling
can used to train the MLP.
4.1 Q UANTUM SAMPLING PRELIMINARIES
Previous works show that QA can generate samples from Boltzmann-like distributions for training
BMs [1; 15]. Fig. 2 (left) shows the workÔ¨Çow of sampling on a quantum annealer; more details as
follows.
The evolution of QA in an nqubit system is summarised by the time-dependent Hamiltonian, which
in the D-Wave system is deÔ¨Åned as
H(t) = A(t)
2nX
i=1^(i)
x+B(t)
2 nX
i=1hi^(i)
z+X
i>jJi;j^(i)
z^(j)
z1
A; (31)
where ^(i)
xand^(i)
zare Pauli matrices operating on qubits with index iorjwith all other positions
being the identity matrix and outer products between them. The dimensions of H(t)is2n2n; see
[2] for details.
7Note that H(t)is the sum of two terms‚Äîthe initial Hamiltonian and the Ô¨Ånal Hamiltonian. As the
annealing functions A(t)andB(t)change with time, the energy of the system moves towards the
Ô¨Ånal Hamiltonian.
The Ô¨Ånal Hamiltonian derives from the model of interest, which is encoded as a binary quadratic
model (BQM)
E(q) =X
iQi;iqi+X
i<jQi;jqiqj=qTQq; (32)
where q2f0;1gnandqiis thei-th element of q, andQis annnupper triangular matrix with Qi;i
being linear coefÔ¨Åcients and the non-zero off-diagonal terms, and Qi;jbeing quadratic coefÔ¨Åcients;
Sec. 4.2 will deÔ¨Åne Qfor the model of interest. By substituting
qi= (si+ 1)=2 (33)
into (32), the BQM is converted to a model of spin variables, which is the Ising model with param-
etersJandh
E0(s) = X
ihisi X
iX
j<iJijsisj; (34)
where s2f  1;1gn,hiencodes the qubit bias of siand has a range of [ 2:0;2:0], andJijis
the coupling strength between siandsjwith a range [ 1:0;1:0]. The precision of the control
parameters depends on the QPU. After minor embedding, (34) becomes the Ô¨Ånal Hamiltonian.
The QA process is depicted on the right of Fig. 2, where the red dots are eigenvalues of the Hamil-
tonian, whose eigenstates compose a certain quantum state. The blue dots are values of the Hamil-
tonian calculated for the quantum state at each time step, which is obtained from integrating the
Schr ¬®odinger‚Äôs equation [2]. The blue dots can be interpreted as expectations of energy correspond-
ing to a certain quantum state and Hamiltonian, i.e., sums of red dots at the same time instance times
their corresponding probabilities. For sampling, we are interested in generating the target proba-
bility distribution that makes the blue dot at the Ô¨Ånal time instance by feeding in optimal control
parameters including the qubit biases h, coupling strength J, and effective temperature eff. The
output of the quantum annealer are the samples from the probability distribution that makes the Ô¨Ånal
blue dot.
Quantum annealers can sample models with complex correlations, which is computationally pro-
hibitive on classical computers. In D-Wave Advantage, one qubit can have a maximum of 15 con-
nections with other qubits, which is a richer connectivity than previous annealers. However, QA can
be affected by noise from the external environment such as temperature and magnetic Ô¨Åelds [20].
The hardware noise complicates the process of inferring the instance-dependent effective tempera-
ture, which is vital for effective training of the probabilistic model.
4.2 BQM FOR SAMPLING CONDITIONAL PROBABILITY
To sample from P(yjx)of the EBM using QA, we need to construct the corresponding BQM. Let
the biases in the hidden and output layers be bandc. The linear coefÔ¨Åcients of the BQM are thus
bq=W(1)x+b2RKand c2RM; (35)
where xis constant (clamped to a speciÔ¨Åc input). DeÔ¨Åning
B=diag(bq)and C=diag(c); (36)
the BQM coefÔ¨Åcient matrix Qis thus
Q=1
eff
B W(2)
0 C
; (37)
which is of size (K+M)(K+M).effat this point remains undetermined. Note that the
size of Qdoes not depend on the dimensionality of the input x. This is in stark contrast to Sasdelli
and Chin [33], where their QUBO to train BNNs scales with the feature dimensions andnumber of
connections/weights. Embedding weights as continuous coupling strength Jijallows bigger models
to Ô¨Åt on quantum hardware. Additionally, we can do batch training as we do not require embedding
all the data in the QUBO.
84.3 T RAINING ALGORITHM
Given a batch of training data T D , we wish to compute the gradients (14) and (15) to update the
weights. This is achieved via quantum sampling as follows:
1. Compute (18) by taking the average over T.
2. For each (x`;y`)2T
(a) Fix x=x`.
(b) Calculate Q(37) with an appropriate eff.
(c) Convert BQM (32) to Ising model (34).
(d)S S[f  samples from QA (34) g.
3.Scontains samples of both kandy; letf~yrgR
r=1be the samples of y. The expectation in
(19) is estimated to x`
iPR
r=1
(W(1)x`+W(2)T~yr)
j=R.
4. Calculate the gradient as in (14) and (15).
Throughout our experiments, we set   = 1k. The gradient is then utilised in the ADAM optimiser
for weight updating.
5 E XPERIMENTS
5.1 E QUIVALENCE BETWEEN MLP AND EBM
We Ô¨Årst verify the equivalence established in Sec. 3.
Datasets We utilised the MNIST dataset consisting of handwritten digits of 28 28 pixels. Each
test case involved random selection of two digits, which were used to create a binary classiÔ¨Åcation
task with all training and test data from the chosen classes in MNIST, i.e., approximately 12k training
images and 2k test images both with balanced classes.
Network size and training In both Sec. 5.1 and 5.2, we created an MLP/EBM with size
7845481,i.e.,N= 784 (image size), K= 548 andM= 1. Small network sizes were used to
stay within the qubit count of the QPU.
The MLP was trained with backpropagation (speciÔ¨Åcally ADAM optimiser) with cross-entropy loss,
while the EBM was trained with CD [12] to maximise log conditional likelihood (9). The weights
of the EBM were initialised from a Gaussian distribution with a zero-mean and standard deviation
of 0.01 [23]. Otherwise, the weights of MLP were initialised with the standard initialisation from
PyTorch. For both training algorithms, the batch size was 1k, while the learning rate was 0.001.
Results For each network, we monitored the training loss and test accuracy during training. For
each network, we also evaluated the metrics above with the weights inherited from the other network.
SpeciÔ¨Åcally, we evaluated
‚Ä¢ The cross-entropy loss and test accuracy for the MLP with weights inherited from the EBM.
‚Ä¢ The log conditional likelihood and test accuracy for the EBM with weights inherited from the
MLP.
Additionally, the symmetrised KL divergence measured in nats [25] is calculated between the sig-
moidal output of the MLP with weights trained by backpropagation and that of the MLP that has
inherited weights from the EBM.
Figs. 3 shows the case of classiÔ¨Åcation between digits 0 and 1. The metrics are highly similar
across the training iterations, and the output divergence measured with MLP approaches zero, which
supports the equivalence of the two training problems established in Sec. 3. Refer to Appendix C.1
for evaluation on more digit pairs.
5.2 T RAINING MLP WITH QUANTUM SAMPLING
We now present results for the algorithm in Sec. 4.3 with sampling conducted by a D-Wave Advan-
tage quantum annealer. Note that besides limited QPU capacity, our access time was also restricted.
The results presented here have fully exhausted our available quantum compute quota.
90 50 100 150 200 250
Step00.20.40.60.8Training Loss of MLPMLP
MLP with weights inherited from EBM
0 50 100 150 200 250
Step-0.3-0.2-0.10log P(y|x) of EBMEBM with weights inherited from MLP
EBM
0 50 100 150 200 250
Step0.50.60.70.80.9Test Accuracy of MLP 00.050.10.150.2
MLP
MLP with weights inherited from EBM
Symmetrised divergence of MLP output
0 50 100 150 200 250
Step0.40.60.81Test Accuracy of EBMEBM with weights inherited from MLP
EBMFigure 3: Equivalence of MLP and EBM on MNIST binary classiÔ¨Åcation. The top panels show the
cross-entropy loss and log conditional likelihood (trained and evaluated). The bottom panels show
the test accuracy. Symmetrised KL divergence is shown in the bottom left panel.
Datasets We experimented with randomly generated binary classiÔ¨Åcation problems on both
MNIST and Fashion-MNIST datasets. We trained on random subsets of the training data, speciÔ¨Å-
cally 20 training images for each case, and test data was generated in the same way as Sec. 5.1. The
classes were balanced in both sets.
The following methods and settings were executed:
‚Ä¢Classical-1: MLP with backpropagation (ADAM optimiser) to minimise cross-entropy loss. For
both datasets, a batch size of 5 and a learning rate of 0.1 were used.
‚Ä¢Classical-2: EBM trained with the algorithm in Sec. 4.3 but with Gibbs sampling (classical) to
generate the samples. SpeciÔ¨Åcally, an RBM [23] was created from kandyin the EBM. For both
datasets, a batch size of 5 and a learning rate of 0.1 were used.
‚Ä¢Quantum-1: EBM trained with the algorithm in Sec. 4.3 with quantum sampling. The other
settings were the same as in Classical-2. The qubit biases hare capped according to the hardware
constraints mentioned in Sec. 4.1.
In Classical-2 and Quantum-1, since our focus is on training MLP, the optimised weights for the
EBM were transferred to an MLP for test accuracy evaluation.
Results For Quantum-1, a grid search was conducted and the effective temperature of 16 was
found to be viable for MNIST and Fashion-MNIST. Fig. 4 illustrates the average test accuracy over
at least 3 successful trials (with identical setup) of all the methods above, with unbiased sample
standard deviation highlighted as the shaded areas. The randomness is due to batch generation,
weight initialisation, and the sampling processes involved. Refer to Appendix C.2 for plots on
more test cases. Tables 1, 2, and 3 present results over same sets of experimental trials for 11 digit
pairs from MNIST and 5 class pairs from Fashion-MNIST, typically 3‚Äì16trials for each case. The
numbers of test cases and trials were restricted by our available quantum compute quota. Table 1
shows the mean test accuracy. Table 2 reports the number of training stpdf at which the test accuracy
surpassed 70 %for the Ô¨Årst time. Table 3 displays the success rates of the methods, with successful
trials referring to those that exhibited a signiÔ¨Åcant increase in test accuracy during training, while
unsuccessful trials display a constant/volatile test accuracy curve.
In both datasets, Quantum-1 performed successfully; the mean test accuracy of the successful runs
is comparable to the classical counterparts across the training iterations.
5.3 R UNTIME ANALYSIS
In this experiment, we are interested to compare the processing time of quantum sampling to the
classical training methods. For fairness, a comparison is made between the computational time for
matrix multiplication between the hidden and output layer in MLP and the sampling time for the
hidden and visible layer in EBM trained with both classical and quantum sampling. The sampling
setups are consistent with those in Sec. 5. The classical models were running on the GPU (GeForce
RTX 3060) while the quantum sampling was done on the QPU (D-Wave Advantage 4.1 System). To
enable a direct comparison, the number of hidden nodes was kept 1 while the number of input/visible
10Figure 4: Avg test accuracy of MLP for binary classiÔ¨Åcation over at least 3
successful runs. The unbiased sample std. dev. is shown as the shaded area.
The model parameters were trained by MLP (Classical-1), EBM with Gibbs
sampling (Classical-2), and EBM with quantum sampling (Quantum-1). Top:
Digits 2 & 3 of MNIST; Bottom: ‚ÄúPullover‚Äù & ‚ÄúDress‚Äù of Fashion-MNIST.
Dataset Class Classical-1 Classical-2 Quantum-1
MNIST2-3 0.8277 0.7742 0.8586
3-9 0.9024 0.8619 0.9050
5-7 0.8921 0.8911 0.8447
4-8 0.7536 0.7720 0.8247
5-6 0.7656 0.7660 0.7752
5-9 0.7628 0.6937 0.6740
0-6 0.8572 0.8594 0.8436
6-8 0.9237 0.9254 0.9346
0-1 0.9965 0.9936 0.9986
1-3 0.9652 0.9288 0.9288
8-9 0.7376 0.6622 0.7202
Fashion-
MNISTPullover-Dress 0.9136 0.8909 0.9116
Dress-Ankle Boot 0.9917 0.9937 0.9970
Sandal-Sneaker 0.7047 0.7142 0.7046
Sandal-Shirt 0.9750 0.9635 0.9346
Coat-Bag 0.9158 0.8000 0.8997
Table 1: Test accuracy on random cases averaged over at least 3 successful runs.
Dataset Class Classical-1 Classical-2 Quantum-1
MNIST2-3 8 12 7
3-9 7 13 5
5-7 12 18 10
4-8 7 9 9
5-6 15 9 9
5-9 11 >20 >20
0-6 12 9 5
6-8 4 5 5
0-1 8 4 4
1-3 7 10 6
8-9 12 14 8
Fashion-
MNISTPullover-Dress 6 14 5
Dress-Ankle Boot 4 4 6
Sandal-Sneaker 11 10 7
Sandal-Shirt 7 6 5
Coat-Bag 8 15 6
Table 2: Number of stpdf at which the test accuracy surpasses 70 %for the Ô¨Årst time.
11Dataset Class Classical-1 Classical-2 Quantum-1
MNIST2-3 50 40 28
3-9 80 60 50
5-7 75 75 40
4-8 100 50 50
5-6 50 100 75
5-9 60 75 50
0-6 60 100 100
6-8 100 100 75
0-1 100 100 100
1-3 100 75 100
8-9 44 40 30
Fashion-
MNISTPullover-Dress 20 20 29
Dress-Ankle Boot 67 50 63
Sandal-Sneaker 33 75 50
Sandal-Shirt 50 63 50
Coat-Bag 60 21 38
Table 3: Success rate in percentage. Typically 3‚Äì16trials were run for each problem.
101102103104105106107
Model Size10-410-2100102Runtime (s)GPU-MLP
GPU-EBM
QPU
QPU Avg Time
QPU Max Size
Figure 5: Comparison of runtime for the three schemes. The model size
speciÔ¨Åes the number of input/visible nodes, while the number of hidden
nodes is always 1.
nodes was speciÔ¨Åed as the model size on the x-axis. The results are shown in Fig. 5 in a log
scale. As the model gets larger, linear growth is observed in the sampling time of both MLP and
classical EBM, while that of the quantum annealer Ô¨Çuctuates roughly at the same level. The average
processing time for quantum sampling is plotted as the dashed line. Provided the constant annealing
time used in our work, i.e., 20s, the runtime of the two classical models will eventually exceed that
of the quantum-assisted model. The capacity of the Zephyr architecture, which is the latest QPU
architecture under development by D-Wave is highlighted as the red dot in Fig. 5. This illustrates
great potential for the proposed quantum-assisted training method.
6 C ONCLUSIONS
We have presented a method for utilising QA to train MLPs. It is noted that only under certain
conditions can a quantum annealer sample from a Boltzmann-like distribution. We found proper
hyper-parameters for the quantum method, which achieved comparable performance to backpropa-
gation. Though model sizes were limited on the current QPUs, extensive results on feasible instances
illustrate excellent runtime scaling. An important future work is to explore more stable training per-
formance on more complex models through better tuning of the QA control parameters.
12REFERENCES
[1] Steven H Adachi and Maxwell P Henderson. Application of quantum annealing to training of deep neural
networks. arXiv preprint arXiv:1510.06356 , 2015.
[2] Tameem Albash and Daniel A Lidar. Adiabatic quantum computation. Reviews of Modern Physics ,
90(1):015002, 2018.
[3] Saad Albawi, Tareq Abed Mohammed, and Saad Al-Zawi. Understanding of a convolutional neural
network. In 2017 international conference on engineering and technology (ICET) , pages 1‚Äì6. Ieee, 2017.
[4] Mohammad H Amin, Evgeny Andriyash, Jason Rolfe, Bohdan Kulchytskyy, and Roger Melko. Quantum
boltzmann machine. Physical Review X , 8(2):021050, 2018.
[5] Kerstin Beer, Dmytro Bondarenko, Terry Farrelly, Tobias J Osborne, Robert Salzmann, Daniel Scheier-
mann, and Ramona Wolf. Training deep quantum neural networks. Nature communications , 11(1):1‚Äì6,
2020.
[6] Marcello Benedetti, John Realpe-G ¬¥omez, Rupak Biswas, and Alejandro Perdomo-Ortiz. Estimation of
effective temperatures in quantum annealers for sampling applications: A case study with possible appli-
cations in deep learning. Physical Review A , 94(2):022308, 2016.
[7] Marcello Benedetti, John Realpe-G ¬¥omez, Rupak Biswas, and Alejandro Perdomo-Ortiz. Quantum-
assisted learning of hardware-embedded probabilistic graphical models. Physical Review X , 7(4):041052,
2017.
[8] Marcel Seelbach Benkner, Vladislav Golyanik, Christian Theobalt, and Michael Moeller. Adiabatic quan-
tum graph matching with permutation matrix constraints. In 2020 International Conference on 3D Vision
(3DV) , pages 583‚Äì592. IEEE, 2020.
[9] Marcel Seelbach Benkner, Zorah L ¬®ahner, Vladislav Golyanik, Christof Wunderlich, Christian Theobalt,
and Michael Moeller. Q-match: Iterative shape matching via quantum annealing. In Proceedings of the
IEEE/CVF International Conference on Computer Vision , pages 7586‚Äì7596, 2021.
[10] Jacob Biamonte, Peter Wittek, Nicola Pancotti, Patrick Rebentrost, Nathan Wiebe, and Seth Lloyd. Quan-
tum machine learning. Nature , 549(7671):195‚Äì202, 2017.
[11] Tolga Birdal, Vladislav Golyanik, Christian Theobalt, and Leonidas J Guibas. Quantum permutation syn-
chronization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition ,
pages 13122‚Äì13133, 2021.
[12] Miguel A Carreira-Perpinan and Geoffrey Hinton. On contrastive divergence learning. In International
workshop on artiÔ¨Åcial intelligence and statistics , pages 33‚Äì40. PMLR, 2005.
[13] Thomas H Cormen, Charles E Leiserson, Ronald L Rivest, and Clifford Stein. Introduction to algorithms .
MIT press, 2022.
[14] Navneet Dalal and Bill Triggs. Histograms of oriented gradients for human detection. In 2005 IEEE
computer society conference on computer vision and pattern recognition (CVPR‚Äô05) , volume 1, pages
886‚Äì893. Ieee, 2005.
[15] Vivek Dixit, Raja Selvarajan, Muhammad A Alam, Travis S Humble, and Sabre Kais. Training restricted
boltzmann machines with a D-Wave quantum annealer. Front. Phys. , 2021.
[16] Thanh-Toan Do, Anh-Dzung Doan, and Ngai-Man Cheung. Learning to hash with binary deep neural
network. In European Conference on Computer Vision , pages 219‚Äì234. Springer, 2016.
[17] Anh-Dzung Doan, Michele Sasdelli, David Suter, and Tat-Jun Chin. A hybrid quantum-classical algo-
rithm for robust Ô¨Åtting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 417‚Äì427, 2022.
[18] John E Dorband. A boltzmann machine implementation for the D-Wave. In 2015 12th International
Conference on Information Technology-New Generations , pages 703‚Äì707. IEEE, 2015.
[19] Edward Farhi, Jeffrey Goldstone, Sam Gutmann, and Michael Sipser. Quantum computation by adiabatic
evolution. arXiv preprint quant-ph/0001106 , 2000.
[20] Bart≈Çomiej Gardas, Jacek Dziarmaga, Wojciech H Zurek, and Michael Zwolak. Defects in quantum
computers. ScientiÔ¨Åc reports , 8(1):1‚Äì10, 2018.
[21] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accurate
object detection and semantic segmentation. In Proceedings of the IEEE conference on computer vision
and pattern recognition , pages 580‚Äì587, 2014.
[22] Vladislav Golyanik, Sk Aziz Ali, and Didier Stricker. Gravitational approach for point set registration.
InProceedings of the IEEE conference on computer vision and pattern recognition , pages 5802‚Äì5810,
2016.
[23] Geoffrey E Hinton. A practical guide to training restricted boltzmann machines. In Neural networks:
Tricks of the trade , pages 599‚Äì619. Springer, 2012.
[24] Andrew Kae, Kihyuk Sohn, Honglak Lee, and Erik Learned-Miller. Augmenting crfs with boltzmann
machine shape priors for image labeling. In Proceedings of the IEEE conference on computer vision and
pattern recognition , pages 2019‚Äì2026, 2013.
[25] Solomon Kullback and Richard A Leibler. On information and sufÔ¨Åciency. The annals of mathematical
statistics , 22(1):79‚Äì86, 1951.
[26] Harashta Tatimma Larasati, Howon Kim, et al. Trends of quantum computing applications to computer
vision. In 2022 International Conference on Platform Technology and Service (PlatCon) , pages 7‚Äì12.
IEEE, 2022.
[27] Yann LeCun, Sumit Chopra, Raia Hadsell, M Ranzato, and Fujie Huang. A tutorial on energy-based
learning. Predicting structured data , 1(0), 2006.
13[28] Catherine McGeoch and Pau Farr ¬¥e. Advantage processor overview. Technical report, D-Wave, 01 2022.
[29] Siqi Nie, Ziheng Wang, and Qiang Ji. A generative restricted boltzmann machine based method for
high-dimensional motion data modeling. Computer Vision and Image Understanding , 136:14‚Äì22, 2015.
[30] Jack Raymond, Sheir Yarkoni, and Evgeny Andriyash. Global warming: Temperature estimation in
annealers. Frontiers in ICT , 3:23, 2016.
[31] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning internal representations by
error propagation. Technical report, California Univ San Diego La Jolla Inst for Cognitive Science, 1985.
[32] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning representations by back-
propagating errors. nature , 323(6088):533‚Äì536, 1986.
[33] Michele Sasdelli and Tat-Jun Chin. Quantum annealing formulation for binary neural networks. In Digital
Image Computing: Techniques and Applications (DICTA) , 2021.
[34] Maria Schuld and Francesco Petruccione. Supervised learning with quantum computers , volume 17.
Springer, 2018.
[35] Maria Schuld, Ilya Sinayskiy, and Francesco Petruccione. An introduction to quantum machine learning.
Contemporary Physics , 56(2):172‚Äì185, 2015.
[36] Alex Sherstinsky. Fundamentals of recurrent neural network (rnn) and long short-term memory (lstm)
network. Physica D: Nonlinear Phenomena , 404:132306, 2020.
[37] Paul Smolensky. Information processing in dynamical systems: Foundations of harmony theory. Techni-
cal report, Colorado Univ at Boulder Dept of Computer Science, 1986.
[38] Daniel Svozil, Vladimir Kvasnicka, and Jiri Pospichal. Introduction to multi-layer feed-forward neural
networks. Chemometrics and intelligent laboratory systems , 39(1):43‚Äì62, 1997.
[39] Hind Taud and JF Mas. Multilayer perceptron (mlp). In Geomatic approaches for modeling land change
scenarios , pages 451‚Äì455. Springer, 2018.
[40] Paul Viola and Michael Jones. Rapid object detection using a boosted cascade of simple features. In
Proceedings of the 2001 IEEE computer society conference on computer vision and pattern recognition.
CVPR 2001 , volume 1, pages I‚ÄìI. Ieee, 2001.
[41] Walter Winci, Lorenzo Buffoni, Hossein Sadeghi, Amir Khoshaman, Evgeny Andriyash, and Moham-
mad H Amin. A path towards quantum advantage in training deep generative models with quantum
annealers. Machine Learning: Science and Technology , 1(4):045028, 2020.
[42] Peter Wittek. Quantum machine learning: what quantum computing means to data mining . Academic
Press, 2014.
[43] Yue Wu, Zuoguan Wang, and Qiang Ji. Facial feature tracking under varying facial expressions and face
poses based on restricted boltzmann machines. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition , pages 3452‚Äì3459, 2013.
[44] Jan-Nico Zaech, Alexander Liniger, Martin Danelljan, Dengxin Dai, and Luc Van Gool. Adiabatic quan-
tum computing for multi object tracking. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 8811‚Äì8822, 2022.
14APPENDIX A P ROOF OF EQUIVALENCE
Biases To introduce biases we need to deÔ¨Åne the energy function as:
^E(x;k;y) =kTW(1)x+yTW(2)k+bTk+cTy; (38)
we deÔ¨Åne new weights and variables appending one extra dimension to xandk. We constraint these
new variables to 1. We can now identify with the bias bthe additional column of W(1)and with
biascthe additional row of W(2):
^x=2
6664x0
x1
:::
xN 1
13
7775;^k=2
6664k0
k1
:::
kK 1
13
7775;^W(1)=2
664W(1)
00W(1)
01::: b 0
W(1)
10W(1)
11::: b 1
::: ::: ::: :::
0 0 :::03
775;^W(2)=2
664W(2)
00W(2)
01:::
W(2)
10W(2)
11:::
::: ::: :::
c0c1:::3
775
(39)
we can reduce the case of biases to the simple case without biases:
^E(^x;^k;^y) =^kT^W(1)x+^yT^W(2)^k: (40)
Notice that we did not introduce biases on the inputs x, as these are always conditioned to the data
distribution.
Derivative of W(2)The gradient for W(2)in the EBM is obtained as
@W(2)j;iE^x;^y[logP(yjx)] =E^x;^y[yjki] E^x[yjki] (41)
we then rewrite:
E^x;^y[yjki] =E^x;^y[E[yjkijx;y]] =E^x;^y[yjE[kijx;y]]; (42)
E^x[yjki] =E^x[E[yjkijx]]
=E^x2
4X
fyg(E[yjkijx;y]P(yjx))3
5 (43)
=E^x2
4X
fyg(yjE[kijx;y]P(yjx))3
5: (44)
Then, the EBM structure allows us to rewrite:
E^x;^y[yjki] =E^x;^yh
yj(W(1)x+W(2)Ty)ii
; (45)
E^x[yjki] =E^x2
4X
fyg
yj(W(1)x+W(2)Ty)P(yjx)
i3
5: (46)
We can estimate the Ô¨Årst equation directly from the dataset and the second requires sampling from
P(yjx). Like before, we approximate (46) as
E^x[yjki]E^xh
E[yjjx](W(1)x+W(2)TE[yjx])i
: (47)
We now perform Taylor expansions of the sigmoid function around W(1)x:
E^x;^y[yjki]E^x;^yh
yj
(W(1)x) +W(2)Ty@(W(1)x)
ii
E^x;^yh
yj(W(1)x)ii
+E^x;^yh
yj
W(2)Ty@(W(1)x)
ii (48)
and
E^x[yjki]E^xh
E[yjjx]
(W(1)x) +W(2)TE[yjx]@(W(1)x)
ii
E^xh
E[yjjx](W(1)x)ii
+E^xh
E[yjjx]
W(2)TE[yjx]@(W(1)x)
ii
:(49)
15Figure 6: An RBM of size 654 1 embedded in the D-
Wave Advantage System with a Pegasus P16 graph. The
Visible nodes are highlihgted in blue, the hidden nodes
are highlighted in red, and the QPU graph is highlighted
in orange (edge) and yellow (vertices). The black lines in
the graph shows how the qubit chains are formed for the
hidden node (red).
As before, we approximate E[yjx](W(2)(W(1)x))and combine everything together. Differ-
ently from the gradient of the log-likelihood with respect to W(1), the leading orders do not cancel
out. Consequently, we obtain:
@W(2)j;iE^x;^y[logP(yjx)]E^x;^y
(W(1)x)i
y (W(2)(W(1)x))
j
; (50)
The function inside the expectation is equal to the gradient of LforW(2)
j;iin the MLP.
APPENDIX B M INOR EMBEDDING
Fig. 6 and Fig. 7 each plots the embedding of an RBM of maximum number of visible nodes given
one single hidden node in the respective D-Wave quantum processing unit (QPU) architecture, i.e.,
Pegasus P16 graph in the Advantage QPU and Zephyr Z15 in the Advantage2 Prototype. Each
vertex in a graph represents a quantum bit, or qubit and each edge represents a coupler between
two qubits that enables the interaction. SpeciÔ¨Åcally, the Pegasus graph contains 5760 qubits with 15
couplers per qubit while Zephyr scales up to 7440 qubits with 20 couplers per qubit. However, the
number of couplers per qubit is still signiÔ¨Åcantly smaller than the number of connections required
by the target models. The physical qubits in the same chain connected by the black lines represent
one single logical qubit in the target problem, i.e., the hidden node in this case. The chains help
to expand the number of connections allowed for the qubits, which is less restricted to the limited
connectivity in the QPU. It is observed that there is still a fair amount of spare qubits in both plots.
This is due to full connectivity being exploited for some physical qubit(s).
16Figure 7: An RBM of size 834 1 embedded in the D-
Wave Advantage2 System (under development) with a
Zephyr Z15 graph.The Visible nodes are highlihgted in
blue, the hidden nodes are highlighted in red, and the
QPU graph is highlighted in orange (edge) and yellow
(vertices). The black lines in the graph shows how the
qubit chains are formed for the hidden node (red).
APPENDIX C A DDITIONAL EXPERIMENTAL RESULTS
C.1 E QUIVALENCE BETWEEN MLP AND EBM
Please refer to Fig. 8‚Äì13 for more test cases regarding the equivalence analysis described in Sec. 5.1.
The results were gained on both MNIST and Fashion-MNIST. Note that the digit/class pairs were
generated randomly. The same performance metrics as mentioned in the paper have been applied:
‚Ä¢ The cross-entropy loss and test accuracy for the MLP with weights inherited from the EBM.
‚Ä¢ The log conditional likelihood and test accuracy for the EBM with weights inherited from the
MLP.
‚Ä¢ The symmetrised KL divergence calculated between the sigmoidal output of the MLP with
weights trained by backpropagation and that of the MLP that has inherited weights from the
EBM.
C.2 T RAINING MLP WITH QUANTUM SAMPLING
We present additional evaluation results of the three experimental setups introduced in Sec. 5.2. Fig.
14 and 15 contain evaluation results on more test cases plotted in the same way as Fig. 4. The plots
cover the test accuracy of the three setups on all the test cases summarised in Table 1‚Äì3.
170 50 100 150 200 250
Step00.20.40.60.8Training Loss of MLPMLP
MLP with weights inherited from EBM
0 50 100 150 200 250
Step-0.3-0.2-0.100.1log P(y|x) of EBMEBM with weights inherited from MLP
EBM
0 50 100 150 200 250
Step0.40.60.81Test Accuracy of MLP00.20.40.6
MLP
MLP with weights inherited from EBM
Symmetrised divergence of MLP output
0 50 100 150 200 250
Step0.50.60.70.80.9Test Accuracy of EBMEBM with weights inherited from MLP
EBMFigure 8: Equivalence of MLP and EBM on
MNIST binary classiÔ¨Åcation between digits 2
& 3. Panels 1 & 2 show cross-entropy loss and
log conditional likelihood (trained and evalu-
ated). Panels 3 & 4 show the test accuracy.
Symmetrised KL divergence is shown in Panel
3.
0 50 100 150 200 250
Step00.20.40.60.8Training Loss of MLPMLP
MLP with weights inherited from EBM
0 50 100 150 200 250
Step-0.3-0.2-0.100.1log P(y|x) of EBMEBM with weights inherited from MLP
EBM
0 50 100 150 200 250
Step0.50.60.70.80.9Test Accuracy of MLP 00.050.10.15
MLP
MLP with weights inherited from EBM
Symmetrised divergence of MLP output
0 50 100 150 200 250
Step0.40.60.81Test Accuracy of EBMEBM with weights inherited from MLP
EBMFigure 9: Equivalence of MLP and EBM on
MNIST binary classiÔ¨Åcation between digits 3
& 9. Panels 1 & 2 show cross-entropy loss and
log conditional likelihood (trained and evalu-
ated). Panels 3 & 4 show the test accuracy.
Symmetrised KL divergence is shown in Panel
3.
180 50 100 150 200 250
Step00.20.40.60.8Training Loss of MLPMLP
MLP with weights inherited from EBM
0 50 100 150 200 250
Step-0.3-0.2-0.10log P(y|x) of EBMEBM with weights inherited from MLP
EBM
0 50 100 150 200 250
Step0.50.60.70.80.9Test Accuracy of MLP 00.050.10.15
MLP
MLP with weights inherited from EBM
Symmetrised divergence of MLP output
0 50 100 150 200 250
Step0.40.60.81Test Accuracy of EBMEBM with weights inherited from MLP
EBMFigure 10: Equivalence of MLP and EBM on
MNIST binary classiÔ¨Åcation between digits 5
& 7. Panels 1 & 2 show cross-entropy loss and
log conditional likelihood (trained and evalu-
ated). Panels 3 & 4 show the test accuracy.
Symmetrised KL divergence is shown in Panel
3.
0 50 100 150 200 250
Step00.20.40.60.8Training Loss of MLPMLP
MLP with weights inherited from EBM
0 50 100 150 200 250
Step-0.3-0.2-0.10log P(y|x) of EBMEBM with weights inherited from MLP
EBM
0 50 100 150 200 250
Step0.40.60.81Test Accuracy of MLP00.050.1
MLP
MLP with weights inherited from EBM
Symmetrised divergence of MLP output
0 50 100 150 200 250
Step0.40.60.81Test Accuracy of EBMEBM with weights inherited from MLP
EBMFigure 11: Equivalence of MLP and EBM on
MNIST binary classiÔ¨Åcation between digits 4
& 8. Panels 1 & 2 show cross-entropy loss and
log conditional likelihood (trained and evalu-
ated). Panels 3 & 4 show the test accuracy.
Symmetrised KL divergence is shown in Panel
3.
190 50 100 150 200 250
Step00.20.40.60.8Training Loss of MLPMLP
MLP with weights inherited from EBM
0 50 100 150 200 250
Step-0.2-0.10log P(y|x) of EBMEBM with weights inherited from MLP
EBM
0 50 100 150 200 250
Step0.40.60.81Test Accuracy of MLP00.51
MLP
MLP with weights inherited from EBM
Symmetrised divergence of MLP output
0 50 100 150 200 250
Step0.40.60.81Test Accuracy of EBMEBM with weights inherited from MLP
EBMFigure 12: Equivalence of MLP and EBM on
Fashion-MNIST binary classiÔ¨Åcation between
class Pullover & Dress. Panels 1 & 2 show
cross-entropy loss and log conditional likeli-
hood (trained and evaluated). Panels 3 & 4
show the test accuracy. Symmetrised KL di-
vergence is shown in Panel 3.
0 50 100 150 200 250
Step00.20.40.60.8Training Loss of MLPMLP
MLP with weights inherited from EBM
0 50 100 150 200 250
Step-0.3-0.2-0.10log P(y|x) of EBMEBM with weights inherited from MLP
EBM
0 50 100 150 200 250
Step0.20.40.60.81Test Accuracy of MLP00.10.20.3
MLP
MLP with weights inherited from EBM
Symmetrised divergence of MLP output
0 50 100 150 200 250
Step0.40.60.81Test Accuracy of EBMEBM with weights inherited from MLP
EBMFigure 13: Equivalence of MLP and EBM on
Fashion-MNIST binary classiÔ¨Åcation between
class Dress & Ankle boot. Panels 1 & 2 show
cross-entropy loss and log conditional likeli-
hood (trained and evaluated). Panels 3 & 4
show the test accuracy. Symmetrised KL di-
vergence is shown in Panel 3.
20(a) 0-6
 (b) 8-9
(c) 3-9
 (d) 5-7
(e) 5-6
 (f) 4-8
(g) 5-9
 (h) 6-8
(i) 0-1
 (j) 1-3
Figure 14: Test accuracy of MLP for binary classiÔ¨Åcation of digit pairs randomly selected
from MNIST averaged over at least 3 successful runs. The unbiased sample std. dev. is
shown as the shaded area. The model parameters were trained by MLP (Classical-1), EBM
with Gibbs sampling (Classical-2), and EBM with quantum sampling (Quantum-1).
21(a) Dress-Ankle boot
 (b) Sandal-Sneaker
(c) Sandal-Shirt
 (d) Coat-Bag
Figure 15: Test accuracy of MLP for binary classiÔ¨Åcation of classes randomly selected from
Fashion-MNIST averaged over at least 3 successful runs. The unbiased sample std. dev. is
shown as the shaded area. The model parameters were trained by MLP (Classical-1), EBM
with Gibbs sampling (Classical-2), and EBM with quantum sampling (Quantum-1).
22