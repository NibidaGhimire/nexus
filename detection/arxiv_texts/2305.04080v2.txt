Robust Tensor CUR Decompositions: Rapid Low-Tucker-Rank
Tensor Recovery with Sparse Corruptions∗
HanQin Cai†, Zehan Chao‡, Longxiu Huang§,andDeanna Needell¶
Abstract. We study the tensor robust principal component analysis (TRPCA) problem, a tensorial extension
of matrix robust principal component analysis (RPCA), that aims to split the given tensor into an
underlying low-rank component and a sparse outlier component. This work proposes a fast algorithm,
calledRobustTensorCURDecompositions (RTCUR), for large-scale non-convex TRPCA problems
under the Tucker rank setting. RTCUR is developed within a framework of alternating projections
that projects between the set of low-rank tensors and the set of sparse tensors. We utilize the recently
developed tensor CUR decomposition to substantially reduce the computational complexity in each
projection. In addition, we develop four variants of RTCUR for different application settings. We
demonstrate the effectiveness and computational advantages of RTCUR against state-of-the-art
methods on both synthetic and real-world datasets.
Key words. Tensor CUR Decomposition, Robust Tensor Principal Component Analysis, Low-Rank Tensor
Recovery, Outlier Detection
1 Introduction Inourrealworld, high-dimensionaldata, suchasimages, videos, andDNA
microarrays, often reside approximately on low-dimensional manifolds [ 38]. This association
between low-dimensional manifolds and high-dimensional data has led mathematicians to
develop various dimension reduction methods, such as principal component analysis (PCA)
[1] and non-negative matrix factorization (NMF) [ 35] under the low-rank assumption. It
becomes increasingly important to study the low-rank structures for data in many fields
such as image processing [ 20,37,40], video processing [ 42,62], text analysis [ 18,45], and
recommendation system [ 54,60]. In reality, many higher-order data are naturally represented
by tensors [ 39,44,61], which are higher-order extensions of matrices. The tensor-related tasks
such as tensor compression and tensor recovery usually involve finding a low-rank structure
from a given tensor.
As in the matrix setting, but to an even greater extent, principal component analysis is
one of the most widely used methods for such dimension reduction tasks. However, standard
PCA is over-sensitive to extreme outliers [ 15]. To overcome this weakness, robust principal
component analysis (RPCA) has been proposed to tolerate the sparse outliers in data analysis
∗Received by the editors XXXX, 2023; accepted for publication (in revised form) XXXX XX, 2023; published
electronically XXXX XX, 2023. This paper is an extension of work originally presented in ICCVW [8].
Funding: The authors contributed equally. This work was partially supported by NSF DMS 2011140, NSF DMS
2108479, NSF DMS 2304489, and AMS Simons Travel Grant.
†Department of Statistics and Data Science and Department of Computer Science, University of Central Florida,
Orlando, FL 32816, USA. (email: hqcai@ucf.edu)
‡Department of Mathematics, University of California Los Angeles, Los Angeles, CA 90095, USA. (email:
zchao@math.ucla.edu)
§Department of Computational Mathematics, Science and Engineering and Department of Mathematics, Michigan
State University, East Lansing, MI 48823, USA. (email: huangl3@msu.edu)
¶Department of Mathematics, University of California Los Angeles, Los Angeles, CA 90095, USA. (email:
deanna@math.ucla.edu)
1arXiv:2305.04080v2  [math.NA]  10 Oct 20232 H.Q. CAI, Z. CHAO, L. HUANG, AND D. NEEDELL
[16]. In particular, RPCA aims to reconstruct a low-rank matrix L⋆and a sparse outlier
matrix S⋆from the corrupted observation
(1.1) X=L⋆+S⋆.
Existing studies on RPCA seek to find L⋆andS⋆by solving the following non-convex problem
[16], and we use LandSto denote the outcomes:
minimize
L,S∥X−L−S∥F
subject to Lis low-rank and Sis sparse.(1.2)
The term low-rank refers to the constraint that the rank of Lis much smaller than its size, and
the term sparserefers to the restriction on the number of non-zero entries in S(for example,
one possible restriction is to allow each column and row of Scontain at most 10%non-zero
entries). This RPCA model has been widely studied [ 3,5–7,9,11,12,28,48] and applied to
many applications, e.g., face modeling [ 57], feature identification [ 30], and video background
subtraction [ 36]. However, the original RPCA method can only handle 2-mode arrays (i.e.,
matrices), while real-world data are often more naturally represented by higher-dimensional
arrays (i.e., tensors). For instance, in the application of video background subtraction, a color
video is automatically a 4-mode tensor (height, width, frame, and color). To apply RPCA to
tensor data, one has to unfold the original tensor into a matrix along some specific mode(s).
Although the upper bound of the unfolded matrix rank depends on the original tensor rank, the
exact rank of the unfolded matrix remains unclear in some tensor rank settings. In addition,
we seek methods that utilize the structural information of tensor rather than ignoring the
information. Therefore, it is important to generalize the standard RPCA to tensor settings.
This task is called Tensor Robust Principal Component Analysis (TRPCA) [ 43]. Moving from
matrix PCA to the tensor setting could be challenging because some standard results known
in the matrix case may not be generalized to tensors smoothly [ 21]. For example, the rank
of a matrix is well and uniquely defined, but researchers have proposed several definitions of
tensor rank, such as Tucker rank [55], CP rank [17], and Tubal rank [59].
1.1 Notation and Definitions A tensor is a multi-dimensional array and its number
of dimensions is called the orderormode. The space of real tensors of order nand of size
(d1,···,dn)is denoted as Rd1×···×dn. In this section, we first bring in the tensor-related
notation and review some basic tensor properties, which will be used throughout the rest of
the paper. We denote tensors, matrices, vectors, and scalars in different typeface for clarity.
More specifically, calligraphic capital letters (e.g., X) are used for tensors, bold capital letters
(e.g.,X) are used for matrices, bold lower case letters (e.g., x) for vectors, and regular letters
(e.g.,x) for scalars. We use X(I,:)andX(:,J)to denote the row and column submatrices of
Xwith index sets IandJ, respectively.X(I1,···,In)denotes the subtensor of Xwith index
setsIkatk-th mode. A single element in a tensor is indexed as Xi1,···,in. Moreover,
∥X∥∞= max
i1,···,in|Xi1,···,in|and∥X∥F= /summationdisplay
i1,···,inX2
i1,···,inROBUST TENSOR CUR DECOMPOSITIONS 3
denote the max magnitude and Frobenius norm of a tensor, respectively. X†denotes the
Moore-Penrose pseudo-inverse of a matrix. The set of the first dnatural numbers is denoted
by[d] :={1,···,d}.
Definition 1.1 (Tensor matricization/unfolding ).Ann-mode tensorXcan be matricized,
or reshaped into a matrix, in nways by unfolding it along each of the nmodes. The mode- k
matricization/unfolding of tensor X∈Rd1×···×dnis the matrix denoted by
(1.3) X(k)∈Rdk×/producttext
j̸=kdj
whose columns are composed of all the vectors obtained from Xby fixing all indices except for
thek-th dimension. The mapping X∝⇕⊣√∫⊔≀→X (k)is called the mode- kunfolding operator.
Definition 1.2 (Mode-kproduct).LetX∈Rd1×···×dnandA∈RJ×dk. Thek-th mode
multiplication between XandAis denoted byY=X×kA, with
(1.4) Yi1,···,ik−1,j,ik+1,···,in=dk/summationdisplay
s=1Xi1,···,ik−1,s,ik+1,···,inAj,s.
This can be written as a matrix product by noting that Y(k)=AX(k). If we have multiple tensor
matrix products from different modes, we use the notation X×s
i=tAito denote the product
X×tAt×t+1···×sAs. We also use ‘tensor-matrix product’ to name this operation throughout
our paper.
In tensor analysis, Tucker rank [ 29] (also known as multilinear rank) is one of the most
essentialtensorranksrelatedtosubspaceestimation, compression, anddimensionalityreduction
[51].
Definition 1.3 (Tensor Tucker rank and Tucker decomposition ).The Tucker decomposition
of tensorXis defined as an approximation of a core tensor Cmultiplied by nfactor matrices
Ak(whose columns are usually orthonormal) along each mode, such that
(1.5) X≈C×n
i=1Ai.
If(1.5)becomes an equation and C∈Rr1×···×rn, then we say this decomposition is an exact
Tucker decomposition of X.
Note that higher-order singular value decomposition (HOSVD) [ 22] is a specific orthogonal
Tucker decomposition which is popularly used in the literature.
1.2 RelatedWork: TensorCURDecompostions Researchershavebeenactivelystudying
CUR decompositions for matrices in recent years [ 24,26]. For a matrix X∈Rd1×d2, letCbe
a submatrix consisting a subset of columns of Xwith column indices J,Rbe a submatrix
consisting a subset of rows of Xwith row indices I, andU=X(I,J). The theory of CUR
decompositions states that X=CU†Rifrank(U) =rank(X). The first extension of CUR
decompositions to tensors involved a single-mode unfolding of 3-mode tensors [ 46]. Later,
[14] proposed a different variant of tensor CUR that accounts for all modes. Recently, [ 10]
dubbed these decompositions with more descriptive monikers, namely Fiber and Chidori4 H.Q. CAI, Z. CHAO, L. HUANG, AND D. NEEDELL
CUR decompositions. In this paper, we will employ both Fiber CUR decomposition and
Chidori CUR decomposition (see Figures 1 and 2 for illustration) to accelerate an essential
step in the proposed algorithm. We state the Fiber CUR and Chidori CUR decomposition
characterizations below for the reader’s convenience.
Figure 1: ([ 10]). Illustration of the Fiber
CUR Decomposition of Theorem 1.4 in
whichJiis not necessarily related to Ii.
The lines correspond to rows of C2, and
red indices correspond to rows of U2. Note
thatthelinesmay(butdonothaveto)pass
through the core subtensor Routlined by
dotted lines. For the figure’s clarity, we do
not show fibers in C1andC3.
Figure 2: ([ 10]). Illustration of Chidori
CUR decomposition of a 3-mode tensor in
the case when the indices Iiare each an
interval and Ji=⊗j̸=iIj(see Theorem 1.4).
The matrix C1is obtained by unfolding
the red subtensor along mode 1, C2by
unfolding the green subtensor along mode
2, and C3byunfoldingtheyellowsubtensor
along mode 3. The dotted line shows the
boundaries ofR. In this case Ui=R(i)
for alli.
Theorem 1.4 ([10, Theorem 3.3]). LetA∈Rd1×···×dnwith Tucker rank (r1,...,rn). Let
Ii⊆[di]andJi⊆[/producttext
j̸=idj]. SetR=A(I1,···,In),Ci=A(i)(:,Ji)andUi=Ci(Ii,:). Then
the following statements are equivalent:
(i)A=R×n
i=1(CiU†
i),
(ii)rank(Ui) =ri,
(iii) rank(Ci) =rifor alliand the Tucker rank of Ris(r1,···,rn).
Remark 1.5.In particular, when Jiare sampled independently from Ii, Theorem 1.4(i) is
called Fiber CUR decomposition. When Ji=⊗j̸=iIj, Theorem 1.4(i) is called Chidori CUR
decomposition.
In addition, according to [ 27, Corollary 5.2], if one uniformly samples indices IiandJiwith
size|Ii|=O(rilog(di))and|Ji|=O rilog /producttext
j̸=idj, then rank(Ui) =riholds for all iwith
high probability under some mild assumptions. Thus, the tensor CUR decomposition holds
and its computational complexity is dominated by computing the pseudo-inverse of Ui. GivenROBUST TENSOR CUR DECOMPOSITIONS 5
the dimension of Ui, the computational complexity of the pseudo-inverse of Uiwith Fiber
sampling isO (n−1)r3log2d, thus Fiber CUR decomposition costs O nr3log2d.1The
Chidori CUR decomposition has a slightly larger |Ji|, which is/producttext
j̸=irilog(di)=O((rlogd)n−1),
thus the decomposition cost O rn+1lognd. By contrast, the computational complexity of
HOSVD is at least O(rdn).
1.3 Related Work: Tensor Robust Principal Component Analysis There is a long list
of studies on RPCA [ 56] and low-rank tensor approximation [ 44], so we refer readers to those
two review articles for the aforementioned topics and focus on TRPCA works in this section.
Consider a given tensor Xthat can represent a hypergraph network or a multi-dimensional
observation [ 13]; the general assumption of TRPCA is that Xcan be decomposed as the sum
of two tensors:
(1.6) X=L⋆+S⋆,
whereL⋆∈Rd1×···×dnis the underlying low-rank tensor and S⋆∈Rd1×···×dnis the underlying
sparse tensor. Compared to the exact low-rank tensor models, TRPCA model contains an
additional sparse tensor S, which accounts for potential model outliers and hence more stable
with sparse noise. Different from the well-defined matrix rank, there exist various definitions
of tensor decompositions that lead to various versions of tensor rank, and lead to different
versions of robust tensor decompositions. For example, [ 41,43,47] formulate TRPCA as a
convex optimization model based on the tubal rank [59].
Based on the Tucker rank, we aim to solve the non-convex optimization problem in this
work:
minimize
L,S∥X−L−S∥ F
subject toLis low-Tucker-rank and Sis sparse.(1.7)
Researchers have developed different optimization methods to solve (1.7)[13,23,31,53].
For example, the work in [ 13] integrated the Riemannian gradient descent (RGD) and gradient
pruning methods to develop a linearly convergent algorithm for (1.7). This RGD algorithm will
also serve as a guideline approach in our experiments. However, one of the major challenges in
solvingtheTuckerrankbasedTRPCAproblemisthehighcomputationalcostforcomputingthe
Tucker decomposition. If L⋆is rank- (r1,···,rn), the existing methods, e.g., [ 13,25,31,32,53],
have computational complexity at least O(ndnr)—they are thus computationally challenging
in large-scale problems. Thus, it is necessary to develop a highly efficient TRPCA algorithm
for time-intensive applications.
1.4 Contributions In this work, we consider the TRPCA problem under the Tucker rank
setting. Our main contributions are three-fold:
1.We provide theoretical evidence supporting the generality of Tensor Robust Principal
Component Analysis (TRPCA) model over the matrix robust PCA model obtained
from the unfolded tensor (see Section 2). That is, TRPCA requires a much weaker
1For notation simplicity, we assume the tensor has the same dand ralong each mode when we discuss
complexities. All logoperators used in this paper stand for natural logarithms.6 H.Q. CAI, Z. CHAO, L. HUANG, AND D. NEEDELL
sparsity condition on the outlier component. Our theoretical finds will be empirically
verified later in the numerical section.
2.We propose a novel non-convex approach, coined RobustTensorCURDecompositions
(RTCUR), for large-scale2TRPCA problems (see Section 3). RTCUR uses a framework
of alternating projections and employs a novel mode-wise tensor decomposition [ 10] for
fast low-rank tensor approximation. We present four variants of RTCUR with different
sampling strategies (see Subsection 3.4 for the details about sampling strategies). The
computational complexity of RTCUR is as low as O(n2dr2log2d)orO(ndrnlognd)
flops, depending on the sampling strategy, for an input n-mode tensor of size3Rd×···×d
with Tucker rank ( r,...,r ). Both computational complexities are substantially lower
than the state-of-the-art TRPCA methods. For instance, two state-of-the-art methods
[42,43] based on tensor singular value decomposition have computational costs at least
O(ndnr)flops.
3.We verify the empirical advantages of RTCUR with synthetic datasets and three real-
world applications (see Section 4), including robust face modeling, video background
subtraction, andnetworkclustering. WeshowthatRTCURhasnotonlyspeedefficiency
but also superior robustness compared to the state-of-the-arts. In particular, we provide
certain outlier patterns that can be detected by RTCUR but fails all matrix based
methods (see Subsection 4.2). This further verifies our theoretical finds in Section 2.
2 Characterizations of Sparse Outlier Tensor Tensor RPCA problem can be also solved
by using the RPCA method if we unfold the tensor into a matrix along a certain mode. To
solve the tensor-to-matrix RPCA problem successfully, the unfolded outlier tensor must satisfy
α-M-sparsity, a commonly used assumption in matrix RPCA problem. We state the definition
ofα-M-sparsity for matrix as follows.
Definition 2.1 ( α-M-sparsity for matrix). S∈Rd1×d2isα-M-sparse if
∥Sei∥0≤αd1and∥e⊤
jS∥0≤αd2
for alli= 1,···,d2andj= 1,···,d1.
However, when solving TRPCA directly with tensor-based methods, it is more natural to
generalizeα-M-sparsity to tensor setting. We consider the following sparsity condition for
outlier tensor [13].
Definition 2.2 ( α-T-sparsity for tensor). A tensorS∈Rd1×···×dnisα-T-sparse if
∥S×je⊤
kj,j∥0≤α/productdisplay
i̸=jdi,
for allkj= 1,···,dj, where{ekj,j}dj
kj=1is the standard basis of Rdj. Thus,S×je⊤
kj,j
corresponds to the kj-th slice ofSalong mode j.
2In our context, ‘large-scale’ refers to large d.
3For notation simplicity, we assume the tensor has the same dand ralong each mode when we discuss
complexities. All logoperators used in this paper stand for natural logarithms.ROBUST TENSOR CUR DECOMPOSITIONS 7
Figure 3: T-Sparsity vs. M-Sparsity. A black box represents an outlier entry and a white box
represents a good entry. The right-hand-side matrix is unfolded from the left-hand-side tensor.
We emphasize that Definitions 2.1 and 2.2 are not equivalent. In fact, from Figure 3, one
can see that unfolding an outlier tensor along a certain mode can result in much worse sparsity
in the unfolded matrix, with exact same outlier pattern. Note one can argue that unfolding
alone a different mode may result in a not-worse M-sparsity. Although this is true, in practice
the user usually does not have prior knowledge of outlier patterns, and thus cannot determine
the most robust mode to unfold alone. Hence, we claim that Definition 2.2 is a much weaker
condition than Definition 2.1. This is further verified by the following theorem.
Theorem 2.3. Suppose that an n-order tensorS∈Rd×···×dis generated according to the
Bernoulli distribution with expectationα
2, i.e.,Si1,···,in∼Ber(α
2), andαd>2nlogd
log 4−1. Then,S
isα-T-sparse with probability at least Moreover, by unfolding Sinto a matrix M∈Rdk×dn−k
along some particular modes with k∈[1,n−1],Misα-M-sparse with probability at least
1−dk−ndn−k−1−dn−k−ndk−1.
Remark 2.4.It is evident that for large-scale tensor, i.e., d≫1andn > 2, we have
1−ndn−2≪max{k−ndn−k−1,n−k−ndk−1}. Thus, the condition of α-T-sparsity forScan
be satisfied with much better probability than that of α-M-sparsity for the unfolders of S.
Proof of Theorem 2.3.SinceSi1,···,in∼Ber(α
2), we have E(Si1,...,in) =α
2. Let’s first
consider the sparsity of one slice of S: we useS(j)[k]to denote the k-th slice along mode j.
According to the multiplicative Chernoff bound, we have
(2.1) PÑ
/summationdisplay
i1,···,ij−1,ij+1,···,in[S(j)[k]]i1,···,ij−1,ij+1,···,in≥αdn−1é
<e
4αdn−1
2.
Taking the sparsity of all slices along all modes into account, then the probability that Sis
α-sparsity can be bounded by:
P(Sisα-T-sparse )≥Å
1−e
4αdn−1
2ãnd
≥1−nde
4αdn−1
2.
Whenαd>2nlogd
log 4−1, we thus have:
P(Sisα-T-sparse )≥1−nd1−ndn−2. (2.2)8 H.Q. CAI, Z. CHAO, L. HUANG, AND D. NEEDELL
Similarly, if we unfolder Sinto a matrix M∈Rdk×dn−k, we have that
P(Misα-M-sparse )≥1−dk−ndn−k−1−dn−k−ndk−1(2.3)
provided that2nlogd
log 4−1.
Remark 2.5.The purpose of the sparsity condition is to ensure a well-defined robust tensor
PCA problem, i.e., the low-rank and sparse tensors are separable. By matrix rank-sparsity
uncertainty principle [ 19],α-M-sparsity ensures that the matrix problem unfolded from tensor
is well-defined. However, with the newly developed tensor Tucker-rank-sparsity uncertainty
principle [ 58, Proposition 2], the more relaxed α-T-sparsity is enough for well-defined robust
tensor PCA problems. Moreover, to solve the problems with α-T-sparsity right, an algorithm
based directly on tensor structures is needed, like the one that will be proposed in the next
section. The unfolded matrix based algorithms will still require the more restricted α-M-sparsity
condition, thus more likely to fail.
3 Proposed Approach In this section, we propose an efficient approach, called Robust
TensorCURDecompositions (RTCUR), for the non-convex TRPCA problem (1.7). RTCUR
is developed in a framework of alternating projections: (I) First, we project X−L(k)onto the
space of sparse tensors to update the estimate of outliers (i.e., S(k+1)); (II) then we project the
less corrupted data X−S(k+1)onto the space of low-Tucker-rank tensors to update the estimate
(i.e.,L(k+1)). In our algorithm, the key to acceleration is using the tensor CUR decomposition
for inexact low-Tucker-rank tensor approximation in Step (II), which is proved to be much more
efficient than the standard HOSVD [ 10], in terms of computational complexity. Consequently,
in Step (I), this inexact approximation allows us to estimate only the outliers in the smaller
subtensors and submatrices involved in the tensor CUR decomposition. RTCUR is summarized
in Algorithm 3.1. Notice that there are two variants of tensor CUR decompositions which will
result in different Ji(see Remark 1.5), but the steps of Algorithm 3.1 will remain the same.
Therefore, we will not distinguish the two decomposition methods in Subsections 3.1 and 3.2
when discussing the details of Step (I) and (II). We will then show the computational complexity
for Algorithm 3.1 with both Fiber and Chidori CUR decompositions in Subsection 3.3.
3.1 Step (I): Update Sparse Component SWe consider the simple yet effective hard
thresholding operator HTζfor outlier estimation. The operator is defined as:
(3.1) (HTζ(X))i1,···,in=®
Xi1,···,in,|Xi1,···,in|>ζ;
0, otherwise.
As shown in [ 6,9,48], with a properly chosen thresholding value, HTζis effectively a
projection operator onto the support of S⋆. More specifically, we update
(3.2) S(k+1)= HTζ(k+1)(X−L(k)).
Ifζ(k+1)=∥L⋆−L(k)∥∞is chosen, then we have supp (S(k+1))⊆supp (S⋆)and∥S⋆−
S(k+1)∥∞≤2∥L⋆−L(k)∥∞. Empirically, we find that iteratively decaying thresholding values
(3.3) ζ(k+1)=γ·ζ(k)ROBUST TENSOR CUR DECOMPOSITIONS 9
Algorithm 3.1 R obustTensorCURDecompositions (RTCUR)
1:Input:X=L⋆+S⋆∈Rd1×···×dn: observed tensor; (r1,···,rn): underlying Tucker rank of
L⋆;ε: targeted precision; ζ(0),γ: thresholding parameters; {|Ii|}n
i=1,{|Ji|}n
i=1: cardinalities
for sample indices. // Jiis defined differently for different sampling strategies. See
Subsection 3.4 for details about Jiand sampling strategies.
2:Initialization:L(0)=0,S(0)=0,k= 0
3:Uniformly sample the indices {Ii}n
i=1,{Ji}n
i=1
4:whilee(k)>εdo //e(k)is defined in (3.7)
5:(Optional) Resample the indices {Ii}n
i=1,{Ji}n
i=1
6:// Step (I): Updating S
7:ζ(k+1)=γ·ζ(k)
8:S(k+1)= HTζ(k+1)(X−L(k))
9:// Step (II): Updating L
10:R(k+1)= (X−S(k+1))(I1,···,In)
11:fori= 1,···,ndo
12: C(k+1)
i = (X−S(k+1))(i)(:,Ji)
13: U(k+1)
i = SVDri(C(k+1)
i (Ii,:))
14:end for
15:L(k+1)=R(k+1)×n
i=1C(k+1)
iÄ
U(k+1)
iä†
16:k=k+ 1
17:end while
18:Output:R(k),C(k)
i,U(k)
ifori= 1,···,n: the estimates of the tensor CUR decomposition
ofL⋆.
provide superb performance with carefully tuned γandζ(0). Note that a favorable choice
ofζ(0)is∥L⋆∥∞, which can be easily estimated in many applications. The decay factor
γ∈(0,1)should be tuned according to the level of difficulty of the TRPCA problem, e.g.,
those problems with higher rank, more dense outliers, or large condition numbers are considered
to be harder. For successful reconstruction of L⋆andS⋆, the harder problems require larger γ.
When applying RTCUR on both synthetic and real-world data, we observe that γ∈[0.6,0.9]
generally performs well. Since real-world data normally leads to more difficult problems, we fix
γ= 0.7for the synthetic experiment and γ= 0.8for the real-world data studies in Section 4.
3.2 Step (II): Update Low-Tucker-rank Component LSVD is the most popular
method for low-rank approximation under matrix settings since SVD gives the best rank- r
approximation of given matrix X, both with respect to the operator norm and to the Frobenius
norm [2]. Similarly, HOSVD has been the standard method for low-Tucker-rank approximation
under tensor settings in many works [ 2,22,39,52]. However, the computational complexity
of HOSVD is at least O(rdn); hence computing HOSVD is very expensive when the problem
scale is large. As highlighted in [ 10, Sections 3.2 and 3.3], tensor CUR decomposition can
serve as an effective low-Tucker-rank approximation method, even with perturbations. As
such, we employ tensor CUR decomposition for accelerated inexact low-Tucker-rank tensor
approximations. Namely, we update the estimate of the low-Tucker-rank component Lby10 H.Q. CAI, Z. CHAO, L. HUANG, AND D. NEEDELL
setting
(3.4) L(k+1)=R(k+1)×n
i=1C(k+1)
iÄ
U(k+1)
iä†,
where
R(k+1)= (X−S(k+1))(I1,···,In),
C(k+1)
i = (X−S(k+1))(i)(:,Ji),
U(k+1)
i = SVDri(C(k+1)
i (Ii,:)).(3.5)
3.3 Computational Complexities As mentioned in Subsection 1.2, the complexity for
computing a tensor CUR decomposition is much lower than HOSVD, and the dominating steps
in RTCUR are the hard thresholding operator and the tensor/matrix multiplications. For
both Fiber and Chidori CUR decompositions, only the sampled subtensors and submatrices
are required when computing (3.5). Thus, we merely need to estimate the outliers on these
subtensors and submatrices, and (3.2)should not be fully executed. Instead, we only compute
S(k+1)(I1,···,In) = HTζ(k+1)((X−L(k))(I1,···,In)),
S(k+1)
(i)(:,Ji) = HTζ(k+1)((X−L(k))(i)(:,Ji))(3.6)
for alli. Not only can we save the computational complexity on hard thresholding but
also, much smaller subtensors of L(k)need to be formed in (3.6). We can form the required
subtensors from the saved tensor CUR components, which is much cheaper than forming and
saving the whole L(k).
In particular, for X ∈Rd×···×d,r1=···=rn=rand|I1|=···=|In|=O(rlogd),
computingL(k)(I1,···,In)requiresntensor-matrix product operations so the complexity
for computingL(k)(I1,···,In)isO(n(rlogd)n+1)flops for both Fiber and Chidori CUR
decompositions. The complexity for computing L(k)
(i)(:,Ji)with Fiber CUR is different from
the complexity of computing L(k)
(i)(:,Ji)with Chidori CUR. With Fiber CUR, we compute
each fiber inL(k)
(i)(:,Ji)independently and each fiber takes ntensor-matrix product operations.
The firstn−1operations transform the n-mode core tensor L(k)(I1,···,In)into a 1-mode
tensor, which is a vector of length O(rlogd), and the last operation transforms this vector into
another vector of length d. Since there are Ji=O(nrlogd)fibers in total, the complexity for
computingL(k)
(i)(:,Ji)with Fiber CUR decomposition is O(nrlogd((rlogd)n+drlogd))flops.
With Chidori CUR, we compute L(k)
(i)(:,Ji)as a complete unit using ntensor-matrix product
operations. The first n−1operations on the core tensor do not change its size, and the last
operation changes the size of the ith mode to d. Therefore the complexity for computing
L(k)
(i)(:,Ji)with Chidori CUR decomposition is O(n(rlogd)n+1+d(rlogd)n)flops.
Moreover, for time-saving purposes, we may avoid computing the Frobenius norm of the
full tensor when computing the relative error for the stopping criterion. In RTCUR, we adjust
the relative error formula to be
(3.7) e(k)=∥E(k)(I1,···,In)∥F+/summationtextn
i=1∥E(k)
(i)(:,Ji)∥F
∥X(I1,···,In)∥F+/summationtextn
i=1∥X(i)(:,Ji)∥F,ROBUST TENSOR CUR DECOMPOSITIONS 11
whereE(k)=X−L(k)−S(k), so that it does not use any extra subtensor or fiber but only
those we already have. We hereby summarize the computational complexity for each step from
Algorithm 3.1 in Table 3.1.
If we assume that the tensor size dis comparable with or greater than O((rlogd)n−1),
we can conclude that the total computational complexity is O(n2dr2log2d) flops for RTCUR
with Fiber CUR decomposition and O(ndrnlognd) flops for RTCUR with Chidori CUR
decomposition. Otherwise, the computational complexity would be O(n2rn+1logn+1d) flops
for RTCUR with Fiber CUR, and the complexity for RTCUR with Chidori CUR remains
unchanged. For all tensors tested in Section 4, the first case holds. Therefore, in Table 3.1, we
highlightedO(n2dr2log2d) andO(ndrnlognd) as the dominating terms.
Table 3.1: computational complexity for each step from Algorithm 3.1. The complexity
for computingS(I1,···,In)andRare the same as their size; the complexity for CiU†
iis
introduced in Subsection 1.2; the complexity for computing Land error term is introduced in
Subsection 3.3. The dominating terms are highlighted in bold.
Computational Complexity Fiber Sampling Chidori Sampling
Sparse subtensor S(I1,···,In)orR O(rnlognd) O(rnlognd)
AllS(i)(:,Ji)orCifornmodes O(n2rdlogd) O(nrn−1dlogn−1d)
AllU†
ifornmodes O(n2r3log2d) O(nrn+1lognd)
AllCiU†
ifornmodes O(n2r2dlog2dn2r2dlog2dn2r2dlog2d) O(nrndlogndnrndlogndnrndlognd)
Low-rank subtensor L(I1,···,In)O(nrn+1logn+1d) O(nrn+1logn+1d)
AllL(i)(:,Ji)fornmodesO(n2rn+1logn+1d+n2r2dlog2dn2r2dlog2dn2r2dlog2d)O(n2rn+1logn+1d+nrndlogndnrndlogndnrndlognd)
Error termE(k)(I1,···,In)andE(k)
(i)(:,Ji)O(rnlognd+n2drlogd)O(drn−1logn−1(d))
3.4 Four Variants of RTCUR In Subsection 1.2, we discussed two versions of tensor
CUR decomposition: Fiber CUR decomposition and Chidori CUR decomposition. Each of
the decomposition methods could derive two slightly different RTCUR algorithms depending
on if we fix sample indices through all iterations (see Algorithm 3.1). As a result of this, we
obtain four variants in total. We give different suffixes for each variant of RTCUR algorithm:
RTCUR-FF, RTCUR-FC, RTCUR-RF, and RTCUR-RC. We will showcase experimental
results for all variants in Section 4. The first letter in the suffix indicate whether we fix the
sample indices through all iterations: ‘F’ stands for ‘fix’, where the variant uses fixed sample
indices through all iterations; ‘R’ stands for ‘resampling’, where the variant resamples {Ii}n
i=1
and{Ji}n
i=1in each iteration. The second letter indicate which type of CUR decomposition
we use in RTCUR. ‘F’ represents that RTCUR is derived from Fiber CUR decomposition and
‘C’ stands for Chidori CUR. For Fiber CUR, the amount of fibers to be sampled refers to [ 27,
Corollary 5.2], i.e., |Ii|=υrilog(di),|Ji|=υrilog /producttext
j̸=idjandJiis sampled independently
fromIi. Here,υdenotes the sampling constant, a hyper-parameter that will be tuned in the
experiments. For Chidori CUR, |Ii|=υrilog(di)andJi=⊗j̸=iIj. Of these four variants,
RTCUR-FF requires minimal data accessibility and runs slightly faster than other variants.
The resampling variants access more data and take some extra computing; for example, the
denominator of (3.7)has to be recomputed per iteration. However, accessing more redundant
data means resampling variants have better chances of correcting any “unlucky” sampling over12 H.Q. CAI, Z. CHAO, L. HUANG, AND D. NEEDELL
Algorithm 3.2 Conversion from CUR to HOSVD
1:Input:R,Ci,Ui: CUR decomposition of the tensor A
2:[Qi,Ri] = qrÄ
CiU†
iä
fori= 1,···,n
3:T1=R× 1R1×2···×nRn
4:Compute HOSVD of T1to findT1=T× 1V1×2···×nVn
5:Output: JT;Q1V1,···,QnVnK: HOSVD decomposition of A
the iterations. Thus, we expect resampling variants to have superior outlier tolerance than
fixed sampling variants. And the fixed sampling variants have an efficiency advantage over the
resampling variants under specific conditions (e.g., when re-accessing the data is expansive).
The difference between Chidori variants and Fiber variants has similar properties: if we
choose the same υand let|Ii|=υrilog(di)for both Chidori and Fiber CUR described in
Subsection 1.2, the Chidori variants generally access more tensor entries compared to the
Fiber variants. Therefore, with the same sampling constant υ, Chidori variants requires
more computing time in each iteration. Nevertheless, Chidori variants can tolerate more
dense outliers with these extra entries than Fiber variants. We will further investigate their
computational efficiency and practical performance in Section 4.
Remark 3.1.The tensor CUR decomposition represented in Theorem 1.4(i) is also in Tucker
decomposition form. We can efficiently convert the tensor CUR decomposition to HOSVD
with Algorithm 3.2 [ 10]. In contrast, converting HOSVD to a tensor CUR decomposition is
not as straightforward.
4 Numerical Experiments This section presents a set of numerical experiments that
compare the empirical performance of RTCUR with several state-of-the-art robust matrix/ten-
sor PCA algorithms, including Riemannian gradient descent (RGD) [ 13], alternating direction
method of multipliers (ADMM) [ 43], accelerated alternating projections (AAP) [ 6], and itera-
tive robust CUR (IRCUR) [ 9]. RGD and ADMM are designed for the TRPCA task, while
AAP and IRCUR are designed for the traditional matrix RPCA task. Note that RGD is
Tucker-rank based and ADMM is tubal-rank based.
In each subsection, we evaluate the performance of all four proposed variants, RTCUR-FF,
RTCUR-RF, RTCUR-FC, and RTCUR-RC. However, for the network clustering experiment,
we only use fixed sampling (RTCUR-FF and RTCUR-FC). As the coauthorship network is
highly sparse, the resampling variants may diminish the core tensor with a high probability.
The rest of this section is structured as follows. In Subsection 4.1, we present two synthetic
experiments. Specifically, Subsection 4.1.1 examines the empirical relationship between outlier
tolerance and sample size for RTCUR, while Subsection 4.1.2 demonstrates the speed advantage
of RTCUR over state-of-the-art. In Subsections 4.2 and 4.3, we apply RTCUR to two real-world
problems, namely face modeling and color video background subtraction. In Subsection 4.4,
we apply RTCUR on network clustering applications and analyze the obtained results.
We obtain the codes for all compared algorithms from the authors’ websites and hand-tune
the parameters for their best performance. For RTCUR, we sample |Ii|=υrilog(di)(and
|Ji|=υrilog /producttext
j̸=idjfor Fiber variants) for all i, andυis called the sampling constant
through this section. All the tests are executed from Matlab R2020a on an Ubuntu work-ROBUST TENSOR CUR DECOMPOSITIONS 13
2 4 6 8 10 12 14 1690
70
50
30
10
2 4 6 8 10 12 14 1690
70
50
30
10
2 4 6 8 10 12 14 1690
70
50
30
10
2 4 6 8 10 12 14 1690
70
50
30
10
1 2 3 4 5 6 7 8 9 1090
70
50
30
10
1 2 3 4 5 6 7 8 9 1090
70
50
30
10
1 2 3 4 5 6 7 8 9 1090
70
50
30
10
1 2 3 4 5 6 7 8 9 1090
70
50
30
10
1 2 3 4 590
70
50
30
10
1 2 3 4 590
70
50
30
10
1 2 3 4 590
70
50
30
10
1 2 3 4 590
70
50
30
10
Figure 4: Empirical phase transition in corruption rate αand sampling constant υ. Left to
Right: RTCUR-FF, RTCUR-RF, RTCUR-FC, RTCUR-RC, Top:r= 3.Middle:r= 5.
Bottom :r= 10.
station with an Intel i9-9940X CPU and 128GB RAM. The relevant codes are available at
https://github.com/huangl3/RTCUR.
4.1 Synthetic Examples For the synthetic experiments, we use d:=d1=···=dn
andr:=r1=···=rn. The observed tensor Xis composed asX=L⋆+S⋆. To generate
n-modeL⋆∈Rd×···×dwith Tucker rank (r,···,r), we takeL⋆=Y× 1Y1×2···×nYnwhere
Y∈Rr×···×rand{Yi∈Rd×r}n
i=1are Gaussian random tensor and matrices with standard
normal entries. To generate the sparse outlier tensor S⋆, we uniformly sample ⌊αdn⌋entries to
be the support of S⋆and the values of the non-zero entries are uniformly sampled from the
interval [−E(|L⋆
i1,···,in|),E(|L⋆
i1,···,in|)].
4.1.1 Phase Transition We study the empirical relation between the outlier corruption
rateαand sampling constant υfor all four variants of RTCUR using 300×300×300(i.e.,n= 3
andd= 300) problems with Tucker rank (r,r,r ), wherer= 3,5,or10. We set the thresholding
parameters to ζ(0)=∥L∥∞andγ= 0.7, and use the stopping condition e(k)<10−5. A test
example is considered successfully solved if ∥L⋆−L(k)∥F/∥L⋆∥F≤10−3. For each pair of α
andυ, we generate 10test examples.
We summarize the experimental results in Figure 4, where a white pixel means all 10
test examples are successfully solved under the corresponding problem parameter setting,
and a black pixel means all 10test cases fail. Upon observation, one has that the Chidori14 H.Q. CAI, Z. CHAO, L. HUANG, AND D. NEEDELL
50 100 150 200 250 300100101102103
100 200 300 400 500 600 700050100150200
Figure 5: Runtime vs. dimension comparison among variants of RTCUR, RGD, IRCUR and
AAP on tensors with size d×d×dand Tucker rank (3,3,3). The RGD method proceeds
relatively slowly for larger tensors, so we only test the RGD runtime for tensors with a size
smaller than 300 for each mode.
0 5 10 1510-710-610-510-410-310-210-1
Figure 6: Runtime vs. relative error comparison among RTCUR-F, RTCUR-R, AAP, and
IRCUR on tensor with size 500×500×500and Tucker rank ( 3,3,3).
variants, RTCUR-FC, and RTCUR-RC, can recover the low-rank tensor with higher outlier
rate than the Fiber variants with the same sampling constant υ. This expected behavior can be
attributed to the fact that Chidori sampling accesses more data from the tensor with the same
υ, leading to more stable performance compared to Fiber sampling. Furthermore, RTCUR-RF
also outperforms RTCUR-FF, as resampling the fibers allows access to more tensor entries
throughout the process. This phenomenon holds true for the Chidori variants as well, where
RTCUR-FC and RTCUR-RC exhibit very similar phase transitions. Additionally, we observe
that smaller values of rtolerate more outliers, as larger values of rmake the TRPCA task
more complex. Increasing υimproves outlier tolerance, but at the cost of larger subtensors
and more fibers to be sampled, resulting in longer computational timeROBUST TENSOR CUR DECOMPOSITIONS 15
4.1.2 Computational Efficiency In this section, we present a comparative analysis of the
computational efficiency of RTCUR and the state-of-the-art tensor/matrix RPCA algorithms
mentioned in Section 4. To apply the matrix RPCA algorithms, we first unfold an n-mode
d×···×dtensor to a d×dn−1matrix and then solve the matrix RPCA problem by setting
the matrix rank as r. It is worth noting that we do not apply ADMM to this fixed-Tucker-rank
experiment, as the target tensor rank for ADMM is tubal-rank instead of Tucker-rank [ 43].
For all tests, we set the underlying low-Tucker-rank component as rank [3,3,3]and add 20%
corruption. We set parameters υ= 3,ζ(0)=∥L∥∞,γ= 0.7for all four variants of RTCUR.
The reported runtime is averaged over 20trials.
In Figure 5, we investigate the 3-mode TRPCA problem with varying dimension dand
compare the total runtime (all methods halt when relative error e(k)<10−5). It is evident that
all variants of RTCUR are significantly faster than the compared algorithms when dis large.
Next, we evaluate the convergence behavior of the tested algorithms in Figure 6. We exclude
RGD as well because running RGD on this experiment is too expensive. We find all the tested
algorithms converge linearly, and variants of RTCUR run the fastest. Moreover, as discussed
in Subsection 3.4, the Fiber sampling has a running time advantage over Chidori sampling
with the same sampling constant, and fixed sampling runs slightly faster than resampling in
all tests.
4.2 Robust Face Modeling In this section, we apply the four variants of RTCUR and
compare them with the aforementioned tensor/matrix RPCA algorithms on the robust face
modeling task using data from the UT Dallas database [ 50]. The dataset consists of a face
speech video of approximately 5 seconds with a resolution of 360 ×540. We extract 10
non-successive frames and mark a monochromatic block on different color channels for 10
distinct frames per color. This process results in a total of 40 color frames, including the
original 10unmarked frames. As a monochromatic frame usually does not have a low-rank
structure, we vectorize each color channel of each frame into a vector and construct a (height
·width)×3×frames tensor. The targeted Tucker rank is set as r= (3,3,3)for all videos.
For those matrix algorithms, including AAP and IRCUR, we unfold the tensor to a (height ·
width)×(3·frames) matrix and rank 3is used. We set RTCUR parameters υ= 2,ζ(0)= 255,
γ= 0.7in this experiment.
Figure 7 presents the test examples and visual results; Table 4.1 summarizes the runtime
for each method applied on this task. One can see that the matrix-based methods fail to
detect the monochromatic outlier blocks since they lose the structural connection between color
channels after matricization, albeit they spend less time on this task. In contrast, all variants
of RTCUR successfully detect the outlier blocks. The other two TPRCA methods, ADMM
and RGD, partially detect the outlier blocks. Since ADMM is based on tubal decomposition,
it is not a surprise to see different performances in this experiment. The empirical results in
this section verify our claim in Remark 2.5.
4.3 Color Video Background Subtraction We apply the four variants of RTCUR and
aforementioned tensor/matrix RPCA algorithms on the color video background subtraction
task. We obtain 5 color video datasets from various sources: Shoppingmall [34],Highway
[4],Crossroad [4],Port[4], andParking-lot [49]. Similar to Subsection 4.2, we vectorize each
frame and construct a (height·width )×3×framedata tensor. The tensor is unfolded to16 H.Q. CAI, Z. CHAO, L. HUANG, AND D. NEEDELL
Figure 7: Visual results for robust face modeling. The top row contains the corrupted faces,
thesecond and third rows are the recovered faces and detected outliers outputted by
RTCUR-FF; the fourth and fifth rows are results from RTCUR-RF; the sixth and seventh
rowsare results from RTCUR-FC; the eighth and ninth rows are results from RTCUR-RC;
thetenth and eleventh rows are results from ADMM; the twelfth and thirteenth rows
are results from AAP; the fourteenth and fifteenth rows are results from IRCUR; the
sixteenth and seventeenth rows are results from RGD.
a (height·width)×(3·frame) matrix for matrix RPCA methods. We use Tucker rank
(3,3,3)for tensors methods and rank 3 for matrix methods. We exclude RGD from this
experiment because the disk space required for RGD exceeds our server limit. Among the
tested videos, Shoppingmall ,Highway, andParking-lot are normal speed videos with almost
static backgrounds. Crossroad andPortare outdoor time-lapse videos, hence their backgroundROBUST TENSOR CUR DECOMPOSITIONS 17
Table 4.1: Runtime comparison (in seconds) for face modeling task. The matrix RPCA
approaches (AAP and IRCUR) meet the termination condition earlier with the unfolded tensor,
but they failed to detect the artificial noise in this task (see Figure 7).
Method Runtime Method Runtime
RTCUR-FF 2.247 ADMM 30.61
RTCUR-RF 2.289 AAP 1.754
RTCUR-FC 2.319 IRCUR 1.307
RTCUR-RC 2.701 RGD 1430.8
Table 4.2: Video information and runtime comparison (in seconds) for color video background
subtraction task.
Video size RTCUR-FF RTCUR-RF RTCUR-FC RTCUR-RC ADMM AAP IRCUR
Shoppingmall 256×320×3×1250 3.53 5.83 10.68 10.75 783.67 50.38 15.71
Highway 240×320×3×440 3.15 5.47 6.80 7.55 168.55 18.10 3.87
Crossroad 350×640×3×600 6.15 13.33 8.46 12.01 1099.3 97.85 35.47
Port 480×640×3×1000 11.04 18.34 26.63 27.93 2934.3 154.30 71.64
Parking-lot 360×640×3×400 3.79 4.52 6.62 8.14 854.50 34.70 17.38
colors change slightly between different frames. We observe that all tested algorithms perform
very similarly for videos with static backgrounds and produce visually desirable output. On the
other hand, the color of the extracted background varies slightly among different algorithms on
time-lapse videos. Since the color of the background keeps changing slightly for the time-lapse
videos, we cannot decide the ground-truth color of the background, hence we do not rank the
performance of different algorithms. The runtime for results along with video size information
are summarized in Table 4.2. By comparing the runtime of four variants of RTCUR, we
can observe that the experiment result generally agrees with the analysis on computational
efficiency in Subsection 3.4. All RTCUR variants accomplish the background subtraction task
faster than the guideline methods. In addition, we provide some selected visual results in
Figure 8.
4.4 Network Clustering In this section, we apply our RTCUR algorithm, and the TRPCA
algorithm RGD from [ 13] for the community detection task on the co-authorship network data
from [33] and compare their results and efficiency. This dataset contains 3248 papers with
a list of authors for each paper (3607 authors in total); hence could naturally serve as the
adjacency matrix for the weighted co-authorship graph. The original paper for this dataset
tests a number of community detection algorithms, including network spectral clustering,
profile likelihood, pseudo-likelihood approach, etc., on a selected subset with 236 authors and
542 papers. To obtain this subset, we generate an undirected graph for the 3607 authors
and put an edge between two authors if they have co-authored two or more papers. We then
take the largest connected component, and all the nodes in this component are the subset of
authors we are interested in. Since we aim to explore the higher-order interactions among this
co-authorship network, we convert this connected component into a 3-mode adjacency tensor
Tand apply TRPCA algorithms to obtain the low-rank component from L. We construct the18 H.Q. CAI, Z. CHAO, L. HUANG, AND D. NEEDELL
Original
 RTCUR-FF
 RTCUR-RF
 RTCUR-FC
 RTCUR-RC
 ADMM
 AAP
 IRCUR
Figure 8: Visual results for color video background subtraction. The first two rows are
separated backgrounds and foregrounds corresponding to a frame from Shoppingmall , the3rd
and 4th rows are separated backgrounds and foregrounds corresponding to a frame from
Highway, the5th and 6th are separated backgrounds and foregrounds corresponding to a
frame from Crossroad ,7th and 8th correspond to a frame from Port, and thelast two rows
correspond to a frame from Parking lot , except the first column which is the original frame.
adjacency tensor Twith the following rules:
•For any two connected authors (i,j), which means author iandjhave worked together
for at least two papers, we set TG(i,i,j)=TG(i,j,j)= 1
•For any three pairwisely connected authors (i,j,k ), we setTG(i,j,k)= 1. Notice that
these three authors may not appear in one paper at the same time, but each pair of
them have worked together for at least two papers.
HereG(S)denotes all permutations of the set S. Therefore the adjacency tensor TisROBUST TENSOR CUR DECOMPOSITIONS 19
symmetric. Now we apply RTCUR with different sampling constants as well as the TRPCA
algorithm RGD [ 13] to learn the low-rank component Lwith Tucker rank (4,4,4), which is
used to infer the communities in this network. Then we apply the SCORE algorithm [ 33] as
the clustering algorithm on the low-rank tensor L. We use SCORE instead of other traditional
clustering algorithms such as spectral clustering because SCORE could mitigate the influence
of node heterogeneity [33]. We plot the results from each TRPCA algorithm in Figure 9.
-25 -20 -15 -10 -5 0 5 10 15 20 25
First Principle Component of SCORE result-15-10-5051015Second Principle Component of SCORE resultXiaoyan ShiAbel Rodriguez
Yanyuan Ma
Nilanjan Chatterjee
Stefan SperlichGuang ChengByeong U Park
Tiejun Tong
Yuedong Wang
Amy H HerringAnna Maria Siega-Riz
Bradley S Peterson
David DunsonDebajyoti Sinha
Dipak K DeyHeping ZhangHongtu Zhu
Hsin-Cheng HuangJian Shi
Joseph G Ibrahim
Michael J ToddMing-Hui ChenNiansheng Tang
Qingxia ChenStuart R LipsitzSungduk Kim
Wang ZhouWeili LinXin-Bing Kong
Yimei LiYing YuanZhi Liu
Amnon NeemanHans-Georg Muller
Hua LiangJane-Ling Wang
Jiashun Jin
Lan ZhouMing-Yen ChengPeter Hall
Philip J Brown
Qiwei YaoRaymond J Carroll
Richard SamworthT Tony Cai
Xiang LiuYi-Hau Chen
Bo Li
Chenlei LengClifford LamDan Yu Lin
Daniela M Witten
Haibo ZhouHansheng Wang
Heng PengHui ZouJacob Bien
Jiancheng Jiang Jianqing Fan
Kung-Sik Chan
Lan Zhang
Noelle I SamiaRunze Li
Trevor J Hastie
Wei PanYacine Ait-Sahalia
Yichao WuYingcun Xia
Yingying FanZhezhen Jin
-15 -10 -5 0 5 10 15 20
PC 1 by SCORE and Multidimensional Scaling-10-5051015PC 2 by SCORE and Multidimensional ScalingAnnie QuChenlei LengChiung-Yu Huang
Dan Yu LinDaniela M WittenDebashis PaulDonglin ZengHansheng WangHeng PengHowell Tong Hui Zou
Jacob BienJiancheng Jiang
Jianqing FanJing Qin
Kani ChenKung-Sik ChanLan Zhang
Noelle I Samia
Rui SongRunze LiTrevor J HastieXiaotong ShenYacine Ait-SahaliaYang FengYichao Wu
Yingcun XiaYingying Fan Yong ZhouZhezhen Jin
Amy H Herring
Anirban Bhattacharya
Bing-Yi JingBrian J ReichDavid DunsonHeping ZhangHongtu ZhuHsin-Cheng Huang
J S MarronJoseph G Ibrahim
Lawrence CarinMichael J Todd
Ming-Hui Chen
Qi-Man ShaoQingxia Chen
Wang ZhouWei Pan
Abel RodriguezBo Li
Byeong U ParkCheng Yong Tang
Cun-Hui Zhang
David L DonohoFang YaoGrace Wahba
Guang ChengHans-Georg MullerHarrison H ZhouHongzhe LiHua Liang
Jian HuangLawrence D BrownMichael Sherman
Ming Yuan
Peter HallQiwei YaoRasmus Waagepetersen
Raymond J Carroll
Richard SamworthSong Xi Chen T Tony CaiYanyuan MaYi Lin
Ying Wei
Yongtao Guan
-10 -5 0 5 10 15 20 25 30 35 40
First Principle Component of SCORE result-25-20-15-10-50510152025Second Principle Component of SCORE resultChenlei Leng
Xiang Liu
Anirban BhattacharyaD Kuang
Alexander Meister
Nilanjan Chatterjee
Anna Maria Siega-Riz
Brian J ReichBruce G LindsayEric BairJianqing Fan
Li ChenRui SongYang Feng
Yichao WuBo Li
Clifford Lam
Dan Yu LinDaniela M Witten
Guang ChengHaibo Zhou
Hansheng WangHsin-Cheng Huang
Hua Liang
Hui Zou
Jacob BienJoseph G Ibrahim
Kung-Sik Chan
Lan Zhang
Ming-Hui ChenMing-Yen ChengNoelle I SamiaPhilip J Brown
Qingxia ChenQiwei YaoRichard SamworthRunze Li
Stefan Sperlich
T Tony CaiTiejun TongTrevor J Hastie
Wang ZhouWei PanYingcun Xia
Yingying FanYuedong WangZhezhen Jin
Amnon NeemanAnastasios A TsiatisAndrew O Finley
Arnab Maity
Aurore Delaigle
Byeong U ParkCeline VialChris C Holmes
Dipak K Dey
Enno MammenHans-Georg Muller
Heng Peng
Jane-Ling Wang
Jiancheng Jiang
Jianhua Z Huang
Jiashun JinJing Qin
Kyusang YuLan ZhouLawrence Carin
Malay GhoshNeil BathiaPeihua Qiu
Peter Hall
Raymond J CarrollTao YuYacine Ait-SahaliaYanyuan Ma
Yi-Hau Chen
-10 -5 0 5 10 15 20 25 30
First Principle Component of SCORE result-20-15-10-5051015202530Second Principle Component of SCORE resultClifford LamMing-Yen Cheng
Hans-Georg Muller
Christina Kendziorski
Hui Zou
Amnon NeemanBo LiChenlei LengDan Yu Lin
Daniela M Witten Guang ChengHaibo Zhou
Hansheng WangHeng Peng
Hsin-Cheng HuangHua Liang
Jacob BienJane-Ling Wang Jiancheng JiangJianqing Fan
Joseph G IbrahimKung-Sik ChanLan ZhangLan Zhou
Ming-Hui Chen
Noelle I SamiaPhilip J Brown
Qingxia ChenRunze LiStefan Sperlich
Tiejun TongTrevor J HastieWang Zhou
Wei PanXiang LiuYacine Ait-Sahalia
Yichao WuYingcun XiaYingying Fan
Yuedong WangZhezhen Jin
Byeong U ParkGrace WahbaHao Helen Zhang
Jiashun JinNilanjan Chatterjee
Peter Hall
Qiwei YaoRaymond J CarrollRichard Samworth
T Tony CaiYanyuan MaYi-Hau Chen
Anastasios A Tsiatis
Anirban BhattacharyaBani Mallick
-30 -20 -10 0 10 20 30 40
First Principle Component of SCORE result-25-20-15-10-50510152025Second Principle Component of SCORE resultAmnon Neeman
Anastasios A TsiatisArnab MaityBani Mallick
Bo LiD M TitteringtonDamla Senturk
Dan Yu LinDaniela M WittenDavid L Donoho
Fang YaoGuosheng Yin
Hans-Georg Muller
Jane-Ling WangJeffrey D Hart
Jeffrey S Morris
Jian HuangJianhua Z Huang
Jiashun Jin
Joseph G IbrahimKehui ChenKyusang Yu
Lan Zhou
Liang PengMing-Yen ChengNilanjan Chatterjee
Peter Hall
Raymond J CarrollRichard SamworthSuojin Wang
T Tony CaiTiejun TongXihong Lin
Yanyuan MaYuedong Wang
Chenlei Leng
Guang ChengHansheng Wang
Hsin-Cheng HuangJacob BienKung-Sik Chan
Lan ZhangMing-Hui Chen
Noelle I SamiaQingxia Chen
Stefan SperlichTrevor J HastieWang Zhou
Wei Pan
Xiang LiuYingcun Xia
Zhezhen Jin
Byeong U ParkChunming Zhang
Haibo ZhouHeng Peng
Hua Liang
Huazhen LinHui Zou
Jiancheng JiangJianqing FanJianwen Cai
Per Aslak Mykland
Qiwei YaoRui Song
Runze Li
Shaojun GuoTao Huang Tao YuYacine Ait-Sahalia
Yang FengYingying Fan
Yong Zhou
-30 -20 -10 0 10 20 30
First Principle Component of SCORE result-15-10-5051015Second Principle Component of SCORE resultBent NielsenBiao Zhang
D Kuang
Dan Yu Lin
Hao Helen ZhangHui Zou
Joseph G IbrahimMenggang Yu
Ming-Hui ChenMinggen Lu
Qingxia ChenSungkyu JungTrevor J Hastie
Wang ZhouXin-Bing KongZhi Liu
Bo KaiChiung-Yu HuangDaniela M WittenDebashis Paul
Eric BairHaibo ZhouHeng PengHsin-Cheng Huang
Jacob BienJerome H FriedmanJiancheng Jiang
Jianqing FanJunhui Wang
Kung-Sik Chan
Lan Zhang
Limin PengMichael R KosorokNoelle I Samia
Robert J TibshiraniShaojun GuoWei Pan
Xiaotong Shen
Yacine Ait-Sahalia
Yichao WuYing Zhang
Yingying FanYong ZhouYu ShenYufeng Liu
Zhezhen JinAmnon NeemanBo Li
Byeong U ParkChenlei LengClifford Lam
Guang ChengHans-Georg MullerHansheng WangHua Liang
Jane-Ling Wang
Jiashun JinLan Zhou
Ming-Yen ChengNilanjan Chatterjee
Peter HallPhilip J BrownQiwei Yao
Raymond J Carroll
Richard SamworthRunze Li
Stefan Sperlich
T Tony CaiTao Yu
Tiejun TongXiang Liu
Yanyuan Ma
Yi-Hau ChenYingcun Xia
Yuedong Wang
Figure 9: Three communities detected in the “High-Dimensional Data Analysis” co-authorship
network with SCORE[ 33], RGD [13], and RTCUR . Top left : Result from SCORE on original
tensor;top right : Result from RGD and SCORE; middle left : Result from RTCUR-FF
and SCORE, with υ= 6;middle right : Result from RTCUR-FC and SCORE, with υ= 2;
bottom left : Result from RTCUR-FC and SCORE, with υ= 6;bottom right : Result from
RTCUR-FC and SCORE, with υ= 11. All TRPCA methods are applied on the adjacency
tensorTwith Tucker rank = (4,4,4).
The clustering of the “High-Dimensional Data Analysis” co-authorship network is an20 H.Q. CAI, Z. CHAO, L. HUANG, AND D. NEEDELL
unsupervisedtask, whichmeansthegroundtruthoflabelinganauthorwithacertaincommunity
does not exist. Therefore, we do not focus on qualitatively evaluating each result, but we
present the new findings from higher-order interactions among co-authors and analyze the
results from different choices of parameters. Previous studies on this co-authorship network
generallyprovidethreeclusterswithnames: “Carroll–Hall”group, “NorthCarolina”community,
and “Fan and others” group [ 13,33]. Among them, “Carroll-Hall” group generally includes
researchers in nonparametric and semi-parametric statistics, functional estimation, and high-
dimensional statistics; “North Carolina” group generally includes researchers from Duke
University, University of North Carolina, and North Carolina State University; “Fan and
Others” group includes primarily the researchers collaborating closely with Jianqing Fan or
his co-authors, and other researchers who do not obviously belong to the first two groups [ 13].
For conciseness, we will make use of the same name of each group as in previous studies on
this co-authorship network.
The top two plots of Figure 9 are existing results from [ 13]. With SCORE as the clustering
method, the original tensor and the RGD output both successfully reveal the two groups: the
“Carroll–Hall” group and a “North Carolina” community, with slightly different clustering result
for researchers who do not obviously belong to one group, such as Debajyoti Sinha, Michael J
Todd, and Abel Rodriguez. One can observe that Fiber RTCUR detected the “Carroll–Hall”
group. However, Fiber RTCUR labels most authors not having a strong connection with
Peter Hall, Raymond Carroll, and Jianqing Fan as the “North Carolina” community. Similarly,
Chidori RTCUR with υ= 2generates the center of the “Carroll–Hall” group as one cluster
and categorizes most authors with a lower number of co-authorships and not co-authored
with kernel members into the “Fan and others” group. We infer that the tendency to cluster
most members into one group is due to insufficient sampling. The co-authorship tensor is very
sparse, with only about 2% entries being non-zero, so the feature of each node may not be
sufficiently extracted from Fiber sampling or Chidori sampling with small υ. From the middle
two plots, we can observe that most authors in the largest group have very close first and
second principal components in the two-dimension embedding, providing the evidence that the
algorithm ignored some non-zeros entries for nodes with fewer numbers of connections during
the sampling process.
Note that the sampling constant of Fiber sampling should be rlogdtimes the constant of
Chidori sampling in order to access the same amount of data from the original tensor, where d
denotes the number of authors in this experiment). So we only test the Chidori sampling with
a larger sampling constant on the co-authorship tensor Lfor efficiency. The result is shown
in the bottom: υ= 6for bottom-left and υ= 11for bottom-right of Figure 9. Both settings
generate the “Carroll–Hall” group with authors having strong ties to Peter Hall and Raymond
Carroll, such as Richard Samworth, Hans-Georg Muller, Anastasios Tsiatis, Yuanyuan Ma,
Yuedong Wang, Lan Zhou, etc. The “Fan and others” is also successfully detected, including
co-authors of high-degree node Jianqing Fan such as Hua Liang, Yingying Fan, Haibo Zhou,
Yong Zhou, Jiancheng Jiang, Qiwei Yao, etc. The sizes of the three clusters generated from
these two settings are more balanced than the result from RTCUR with smaller υ. Therefore
we can conclude that, in this real-world network clustering task, different choices of sampling
constant provide the same core members of each group, and the group size is more balanced
with a larger amount of sampling data at the cost of computation efficiency. Table 4.3 showsROBUST TENSOR CUR DECOMPOSITIONS 21
the runtime for each algorithm and sampling method.
Table 4.3: Runtime comparison (in seconds) of TRPCA algorithms: RGD and RTCUR.
Method Runtime
RGD [13] 120.5
RTCUR-FF with υ= 6 3.526
RTCUR-FC with υ= 2 0.571
RTCUR-FC with υ= 6 4.319
RTCUR-FC with υ= 11 7.177
5 Conclusion This paper presents a highly efficient algorithm RTCUR for large-scale
TRPCA problems. RTCUR is developed by introducing a novel inexact low-Tucker-rank tensor
approximation via mode-wise tensor decomposition. The structure of this approximation
significantly reduces the computational complexity, resulting in a computational complexity for
each iteration ofO(n2d(rlogd)2)with Fiber CUR and O(nd(rlogd)n)with Chidori CUR. This
is much lower than the minimum of O(rdn)required by HOSVD-based algorithms. Numerical
experiments on synthetic and real-world datasets also demonstrate the efficiency advantage
of RTCUR against other state-of-the-art tensor/matrix RPCA algorithms. Additionally, the
fixed sampling variants of RTCUR only require partial information from the input tensor for
the TRPCA task.
References
[1]H. Abdi and L. J. Williams ,Principal component analysis , Wiley interdisciplinary
reviews: computational statistics, 2 (2010), pp. 433–459.
[2]G. Bergqvist and E. G. Larsson ,The higher-order singular value decomposition:
Theory and an application [lecture notes] , IEEE signal processing magazine, 27 (2010),
pp. 151–154.
[3]T. Bouwmans, S. Javed, H. Zhang, Z. Lin, and R. Otazo ,On the applications
of robust PCA in image and video processing , Proceedings of the IEEE, 106 (2018),
pp. 1427–1457.
[4]T. Bouwmans, L. Maddalena, and A. Petrosino ,Scene background initialization:
A taxonomy , Pattern Recognition Letters, 96 (2017), pp. 3–11.
[5]H. Cai, J.-F. Cai, T. Wang, and G. Yin ,Accelerated structured alternating projections
for robust spectrally sparse signal recovery , IEEE Transactions on Signal Processing, 69
(2021), pp. 809–821.
[6]H. Cai, J.-F. Cai, and K. Wei ,Accelerated alternating projections for robust principal
component analysis , The Journal of Machine Learning Research, 20 (2019), pp. 685–717.
[7]H. Cai, J.-F. Cai, and J. You ,Structured gradient descent for fast robust low-rank
Hankel matrix completion , SIAM Journal on Scientific Computing, 45 (2023), pp. 1172–
1198.
[8]H. Cai, Z. Chao, L. Huang, and D. Needell ,Fast robust tensor principal component
analysis via Fiber CUR decomposition , in Proceedings of the IEEE/CVF International
Conference on Computer Vision Workshops, 2021, pp. 189–197.
[9]H. Cai, K. Hamm, L. Huang, J. Li, and T. Wang ,Rapid robust principal component
analysis: CUR accelerated inexact low rank estimation , IEEE Signal Processing Letters,22 H.Q. CAI, Z. CHAO, L. HUANG, AND D. NEEDELL
28 (2020), pp. 116–120.
[10]H. Cai, K. Hamm, L. Huang, and D. Needell ,Mode-wise tensor decompositions:
Multi-dimensional generalizations of CUR decompositions , The Journal of Machine Learn-
ing Research, 22 (2021), pp. 1–36.
[11]H. Cai, K. Hamm, L. Huang, and D. Needell ,Robust CUR decomposition: Theory
and imaging applications , SIAM Journal on Imaging Sciences, 14 (2021), pp. 1472–1503.
[12]H. Cai, J. Liu, and W. Yin ,Learned robust PCA: A scalable deep unfolding approach for
high-dimensional outlier detection , Advances in Neural Information Processing Systems,
34 (2021).
[13]J.-F. Cai, J. Li, and D. Xia ,Generalized low-rank plus sparse tensor estimation by
fast riemannian optimization , Journal of the American Statistical Association, (2022),
pp. 1–17.
[14]C. F. Caiafa and A. Cichocki ,Generalizing the column–row matrix decomposition to
multi-way arrays , Linear Algebra and its Applications, 433 (2010), pp. 557–573.
[15]E. Candes and J. Romberg ,Sparsity and incoherence in compressive sampling , Inverse
Problems, 23 (2007), p. 969.
[16]E. J. Candès, X. Li, Y. Ma, and J. Wright ,Robust principal component analysis? ,
Journal of the ACM (JACM), 58 (2011), pp. 1–37.
[17]J. D. Carroll and J.-J. Chang ,Analysis of individual differences in multidimensional
scaling via an n-way generalization of “eckart-young” decomposition , Psychometrika, 35
(1970), pp. 283–319.
[18]J. Chambua, Z. Niu, A. Yousif, and J. Mbelwa ,Tensor factorization method based
on review text semantic similarity for rating prediction , Expert Systems with Applications,
114 (2018), pp. 629–638.
[19]V. Chandrasekaran, S. Sanghavi, P. A. Parrilo, and A. S. Willsky ,Rank-
sparsity incoherence for matrix decomposition , SIAM Journal on Optimization, 21 (2011),
pp. 572–596.
[20]Z. Chao, L. Huang, and D. Needell ,HOSVD-based algorithm for weighted tensor
completion , Journal of Imaging, 7 (2021), p. 110.
[21]J. Chen and Y. Saad ,On the tensor SVD and the optimal low rank orthogonal
approximation of tensors , SIAM journal on Matrix Analysis and Applications, 30 (2009),
pp. 1709–1734.
[22]L. De Lathauwer, B. De Moor, and J. Vandewalle ,On the best rank-1 and
rank-(r1,...,rn) approximation of higher-order tensors , SIAM Journal on Matrix Analysis
and Applications, 21 (2000), pp. 1324–1342.
[23]H. Dong, T. Tong, C. Ma, and Y. Chi ,Fast and provable tensor robust principal
component analysis via scaled gradient descent , Information and Inference: A Journal of
the IMA, 12 (2023), p. iaad019.
[24]S. A. Gore ˘ınov, N. L. Zamarashkin, and E. E. Tyrtyshnikov ,Pseudo-skeleton
approximations , Doklay Akdemii Nauk, 343 (1995), pp. 151–152.
[25]Q. Gu, H.Gui, and J. Han ,Robust tensor decomposition with gross corruption , Advances
in Neural Information Processing Systems, 27 (2014), pp. 1422–1430.
[26]K. Hamm and L. Huang ,Perspectives on CUR decompositions , Applied and Computa-
tional Harmonic Analysis, 48 (2020), pp. 1088–1099.ROBUST TENSOR CUR DECOMPOSITIONS 23
[27]K. Hamm and L. Huang ,Stability of sampling for CUR decompositions , Foundations of
Data Science, 2 (2020), p. 83.
[28]K. Hamm, M. Meskini, and H. Cai ,Riemannian CUR decompositions for robust
principal component analysis , inTopological, AlgebraicandGeometricLearningWorkshops
2022, PMLR, 2022, pp. 152–160.
[29]F. L. Hitchcock ,Multiple invariants and generalized rank of a p-way matrix or tensor ,
Journal of Mathematical Physics, 7 (1928), pp. 39–79.
[30]Y. Hu, J.-X. Liu, Y.-L. Gao, and J. Shang ,DSTPCA: Double-sparse constrained
tensor principal component analysis method for feature selection , IEEE/ACM Transactions
on Computational Biology and Bioinformatics, (2019).
[31]Y. Hu and D. B. Work ,Robust tensor recovery with fiber outliers for traffic events ,
ACM Transactions on Knowledge Discovery from Data (TKDD), 15 (2020), pp. 1–27.
[32]B. Huang, C. Mu, D. Goldfarb, and J. Wright ,Provable low-rank tensor recovery ,
Optimization-Online, 4252 (2014), pp. 455–500.
[33]P. Ji and J. Jin ,Coauthorship and citation networks for statisticians , The Annals of
Applied Statistics, 10 (2016), pp. 1779–1812.
[34]E. L,Foreground detection in video sequences with probabilistic self-organizing maps ,
http://www.lcc.uma.es/~ezeqlr/fsom/fsom.html.
[35]D. D. Lee and H. S. Seung ,Learning the parts of objects by non-negative matrix
factorization , Nature, 401 (1999), pp. 788–791.
[36]L. Li, W. Huang, I. Y.-H. Gu, and Q. Tian ,Statistical modeling of complex back-
grounds for foreground object detection , IEEE Transactions on Image Processing, 13 (2004),
pp. 1459–1472.
[37]X. Li, D. Xu, H. Zhou, and L. Li ,Tucker tensor regression and neuroimaging analysis ,
Statistics in Biosciences, 10 (2018), pp. 520–545.
[38]Z. Lin and H. Zhang , in Low-Rank Models in Visual Analysis, Elsevier, 2017, pp. 1–2.
[39]J. Liu, P. Musialski, P. Wonka, and J. Ye ,Tensor completion for estimating missing
values in visual data , IEEE transactions on pattern analysis and machine intelligence, 35
(2012), pp. 208–220.
[40]T. Liu, M. Yuan, and H. Zhao ,Characterizing spatiotemporal transcriptome of the
human brain via low-rank tensor decomposition , Statistics in Biosciences, (2022), pp. 1–29.
[41]Y. Liu, L. Chen, and C. Zhu ,Improved robust tensor principal component analysis via
low-rank core matrix , IEEE Journal of Selected Topics in Signal Processing, 12 (2018),
pp. 1378–1389.
[42]C. Lu, J. Feng, Y. Chen, W. Liu, Z. Lin, and S. Yan ,Tensor robust principal
component analysis: Exact recovery of corrupted low-rank tensors via convex optimization ,
in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
2016, pp. 5249–5257.
[43]C. Lu, J. Feng, Y. Chen, W. Liu, Z. Lin, and S. Yan ,Tensor robust principal
component analysis with a new tensor nuclear norm , IEEE Transactions on Pattern
Analysis and Machine Intelligence, 42 (2019), pp. 925–938.
[44]H. Lu, K. N. Plataniotis, and A. N. Venetsanopoulos ,A survey of multilinear
subspace learning for tensor data , Pattern Recognition, 44 (2011), pp. 1540–1551.
[45]Y. Luo, Y. Xin, E. Hochberg, R. Joshi, O. Uzuner, and P. Szolovits ,Subgraph24 H.Q. CAI, Z. CHAO, L. HUANG, AND D. NEEDELL
augmented non-negative tensor factorization (SANTF) for modeling clinical narrative text ,
Journal of the American Medical Informatics Association, 22 (2015), pp. 1009–1019.
[46]M. W. Mahoney, M. Maggioni, and P. Drineas ,Tensor-CUR decompositions
for tensor-based data , SIAM Journal on Matrix Analysis and Applications, 30 (2008),
pp. 957–987.
[47]M. Mohammadpour Salut and D. Anderson ,Randomized tensor robust principal
component analysis , (2022).
[48]P. Netrapalli, N. U N, S. Sanghavi, A. Anandkumar, and P. Jain ,Non-convex
robust PCA , in Advances in Neural Information Processing Systems, vol. 27, 2014.
[49]S. Oh, A. Hoogs, A. Perera, N. Cuntoor, C.-C. Chen, J. T. Lee, S. Mukherjee,
J. Aggarwal, H. Lee, L. Davis, et al. ,A large-scale benchmark dataset for event
recognition in surveillance video , in CVPR 2011, IEEE, 2011, pp. 3153–3160.
[50]A. J. O’Toole, J. Harms, S. L. Snow, D. R. Hurst, M. R. Pappas, J. H. Ayyad,
and H. Abdi ,A video database of moving faces and people , IEEE Transactions on Pattern
Analysis and Machine Intelligence, 27 (2005), pp. 812–816.
[51]S. Rabanser, O. Shchur, and S. Günnemann ,Introduction to tensor decompositions
and their applications in machine learning , arXiv preprint arXiv:1711.10781, (2017).
[52]A. Rajwade, A. Rangarajan, and A. Banerjee ,Image denoising using the higher
order singular value decomposition , IEEE Transactions on Pattern Analysis and Machine
Intelligence, 35 (2012), pp. 849–862.
[53]S. E. Sofuoglu and S. Aviyente ,A two-stage approach to robust tensor decomposition ,
in 2018 IEEE Statistical Signal Processing Workshop (SSP), IEEE, 2018, pp. 831–835.
[54]T. Song, Z. Peng, S. Wang, W. Fu, X. Hong, and P. S. Yu ,Based cross-domain
recommendation through joint tensor factorization , in International conference on database
systems for advanced applications, Springer, 2017, pp. 525–540.
[55]L. R. Tucker ,Some mathematical notes on three-mode factor analysis , Psychometrika,
31 (1966), pp. 279–311.
[56]N. Vaswani and P. Narayanamurthy ,Static and dynamic robust PCA and matrix
completion: A review , Proceedings of the IEEE, 106 (2018), pp. 1359–1379.
[57]J. Wright, A. Y. Yang, A. Ganesh, S. S. Sastry, and Y. Ma ,Robust face
recognition via sparse representation , IEEE Transactions on Pattern Analysis and Machine
Intelligence, 31 (2008), pp. 210–227.
[58]D. Xia, M. Yuan, C.-H. Zhang, et al. ,Statistically optimal and computationally
efficient low rank tensor completion from noisy entries , Annals of Statistics, 49 (2021),
pp. 76–99.
[59]Z. Zhang, G. Ely, S. Aeron, N. Hao, and M. Kilmer ,Novel methods for multi-
linear data completion and de-noising based on tensor-SVD , in Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, 2014, pp. 3842–3849.
[60]X. Zheng, W. Ding, Z. Lin, and C. Chen ,Topic tensor factorization for recommender
system, Information Sciences, 372 (2016), pp. 276–293.
[61]H. Zhou, L. Li, and H. Zhu ,Tensor regression with applications in neuroimaging data
analysis, Journal of the American Statistical Association, 108 (2013), pp. 540–552.
[62]P. Zhou, C. Lu, Z. Lin, and C. Zhang ,Tensor factorization for low-rank tensor
completion , IEEE Transactions on Image Processing, 27 (2017), pp. 1152–1163.