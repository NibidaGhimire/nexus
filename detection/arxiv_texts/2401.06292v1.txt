The Effect of Value-Focused Discussions on Scientists’ Ethical Decision Making
Tyler Garcia, Bill Bridges, Caitlin Solis, Wyatt Jones, and James T. Laverty
Physics Department, Kansas State University
Caleb Linville and Scott Tanona
Philosophy Department, Kansas State University
Jonathan Herington
Department of Health Humanities and Bioethics, University of Rochester∗
(Dated: January 15, 2024)
Many scientists view science as value-free, despite the fact that both epistemic and non-epistemic
values structure scientific inquiry. Current ethics training usually focuses on transmitting knowl-
edge about high-level ethical concepts or rules and is widely regarded as ineffective. We argue that
ethics training will be more effective at improving ethical decision making if it focuses on connecting
values to science. We pull from philosophy and psychology to define ethical decision making using
the Four Component Model. This model states that in order to make an ethical decision someone
must consider four components: moral sensitivity, moral reasoning, moral motivation, and moral
implementation. We formed a moderated fellowship of fourteen science faculty from different disci-
plines who met for ten sessions over the course of a year, where they discussed the values embedded
in different scientific norms. We then conducted interviews before and after the year-long fellowship
that involved guided reflection of scenarios where there was some kind of ethical misconduct where
the scientific practice required value judgements (e.g using unpublished data in their own work). We
looked at how the fellowship affected the scientists’ ability to recognize ethical dimensions regarding
the scenarios. We found that this fellowship improved moral sensitivity, but their moral reasoning
does not improve. We outlined our approach on how to look at scientists’ ethical decision making
and made recommendations on how to improve our approach. This work can inform future ethical
training to align better with what scientists value and introduce useful concepts from philosophy
and psychology to education research in physics.
I. INTRODUCTION
In 2019 the American Physical Society (APS) updated
their Guidelines on Ethics to further clarify physicist’s
duties to avoid data fabrication, falsification, plagiarism,
and abuse [1]. These updates were in response to the
National Academies’ Fostering Integrity in Research re-
port, which identified a need to improve research integrity
training due to the growing importance of information
technology in research and the number of studies being
retracted because of irreproducible data [2]. While these
guidelines are an important step forward, we suggest that
to implement these guidelines appropriately, physicists
must cultivate their ethical decision-making [3].
Currently, physicists are primarily exposed to ethics
through Responsible Conduct of Research (RCR) train-
ing. This kind of training, often offered in self-paced
online modules, tends to focus on topics like authorship,
conflicts of interest, research misconduct, and the pro-
tection of human subjects [4]. Studies have shown that
this kind of online training is ineffective at improving
scientists’ ethical decision-making [5–7]. Specifically in
physics, Kirby and Houle found in 2004 that around 39%
of junior members of APS who responded to the survey
had observed or had personal knowledge of at least one
∗Also at Philosophy Department, Kansas State Universityethical violation, such as data falsification, plagiarism,
or authorship showing that ethical training is needed, in
physics[8]. This survey was re-administered in 2020 to
junior level physicists and grad students. The survey
showed mixed improvement: physicists were undergoing
more ethics training (whether at the institution level or
with their supervisors), but the number of misconduct
violations did not significantly change [9]. Those that
did notice violations reported more violations than in
2003 [9]. The 2020 study also found that harassment or
abuse (e.g. experiencing inappropriate remarks, treated
differently, etc.) of physicists is a common experience,
concentrated among women in physics [9]. In sum, there
is more to be done to improve physicists’ ethical conduct.
In our study of scientists’ ethical decision making, we
focus on the fact that scientists routinely make value
judgements in science. In this paper, we define “values in
science” to refer to aims, goals, or principles that direct or
influence scientific work and “value judgements” to refer
to evaluations of priorities between values (e.g., deciding
tradeoffs) or of how a value is to be implemented in a
decision or action (see Theory section for more detail).
The idea that scientists make value judgements in science
seems to contrast with the commonly held belief that
that social and ethical values ought not affect the agenda,
methodology or dissemination of research [3]. However,
more recently philosophers of science have argued per-
suasively that science can never be value free, and pre-
tending otherwise distorts scientist’s sense of their ownarXiv:2401.06292v1  [physics.ed-ph]  11 Jan 20242
ethical responsibilities [10, 11]. Ethics, after this recog-
nition of values in science, requires attention to those
values and reasoning well about them.
For instance, Douglas has argued that scientists need
to attend to non-epistemic values (e.g. consequences on
people’s lives) as well as epistemic values (e.g. gaining
knowledge) to address the “inductive risk” of making an
error in either accepting or rejecting a claim [11]. Sci-
entists often report the significance of statistical findings
by using a ‘p-value’ to indicate the evidential strength of
their data. The choice of a statistical significance level
for a p-value (i.e. p <0.01, p <0.05, etc.) implies a
tradeoff between the risk of a false positive result and
the risk of a false negative result. Moreover, choices in
experimental design, and sometimes during experiments
themselves, influence the chances of positive or negative
results. These tradeoffs are not merely epistemic, as false
positives and false negatives can differentially affect peo-
ples lives, directly or cumulatively, both within the sci-
entific community and in the public more broadly (e.g.,
consider the replication crisis, or the consequences of re-
quiring definitive evidence of a health risk before action).
This highlights both that scientists cannot avoid making
value judgements and that the scope of ethics in science
involves a broader range of topics than is traditionally
covered in RCR training. Building on this work, we ar-
gue that we can improve physicist’s ethical decision mak-
ing by focusing on the values invoked in actual scientific
practice.
In this paper, we define “ethics” to refer to the domain
of inquiry concerning ultimately doing the right thing [12],
and “ethical decision-making” as a process (individual or
collective) aimed at doing the right thing. While some
invoke a distinction between the adjectives “ethical” and
“moral”, in this paper we align with the convention in
the philosophical and psychological literature on ethical
decision-making that uses these words interchangeably.
The Four Component Model of ethical decision-making
[13] identifies four necessary components to making an
ethical decision: moral sensitivity (identifying morally
important ideas), moral reasoning (identifying morally
right action), moral motivation (intending to do the
morally right action), and moral implementation (act-
ing on morally right action) [14]. (Note that while
the original name for the second component is “moral
judgement”, in this paper we call the second component
“moral reasoning”, consistent with some of the literature
on this topic) [15]. In this paper we focused on the moral
sensitivity and moral reasoning components in ethical de-
cision making. These components and their definitions
will be further explained in the Theory section.
This paper reports on an intervention aimed at learn-
ing how to use the presence of value judgements in science
to help improve scientists’ ethical decision making. This
intervention took the form of a two-semester fellowship in
which scientists from different fields met together to talk
about scientific topics rooted in value judgements. To see
if their ethical decision making changed over the courseof the fellowship, we conducted (one-on-one) interviews
with the faculty before and after the fellowship to talk
about their own experience of research ethics and their
responses to vignettes describing situations with multiple
ethical concerns identified by RCR training.
The structure of this paper is a follows: First, we de-
scribe the theoretical basis for our intervention in the val-
ues in science literature and Rest’s theory of moral devel-
opment. We then describe the value-focused fellowship
and provide details of how we analyzed the interviews for
moral sensitivity and moral reasoning. We follow with re-
sults on how the fellowship changed the scientists’ moral
sensitivity and their moral reasoning. Finally, we end
with reflections on how our measures worked and ways
to improve these measures.
II. LITERATURE REVIEW
A. Values in Science
Science cannot be “value free”. In a recent study, El-
liot noted that decisions in science are necessarily made
with values in mind [16]. Values are involved in sci-
ence when choosing research questions, handling data,
drawing conclusions, communicating findings, etc. For
example, climate modelers have to consider values on
the local weather versus the global weather to optimize
their model to predict the weather. Scientists conducting
vaccine trials must make tradeoffs between production
speed, safety for human participants, and the reliability
of the efficacy data.
One of the main arguments that science should re-
main value free is that people will not trust science if
scientists place their own personal values in their re-
search [16, 17]. While this might be an ideal worth
striving for, it is simply not possible to conduct sci-
ence without making explicit and implicit value judge-
ments. Elliot mentions in their paper that there are mul-
tiple arguments on why science can never really be value
free with the gap argument (evidential gaps in data are
filled by value-laden background assumptions), the er-
ror argument (when scientists face epistemic risks they
ought to factor non-epistemic values into decisions), the
aims argument (achieving a non-epistemic aim in science
means taking into account non-epistemic values when us-
ing models, theories, and hypothesis), and the conceptual
argument (non-epistemic values are relevant to assessing
value-laden concepts) [16].
Findings from the developmental moral psychology lit-
erature show that changes in moral reasoning are corre-
lated with improvements in ethical conduct in the pro-
fession [18]. Currently most ethics training focuses on
applications of ethical theories or codes of ethics, instead
of discussing the values embedded in ordinary scientific
practice [5]. While scientists receive training in method-
ologies for distinguishing factual or theoretical questions,
they receive little training in how to adjudicate questions3
about values such as whether to replicate data through
a third party or spend more money training a graduate
student.
In a project looking at the values expressed by scien-
tists’ when discussing decisions within their own work,
Linville et al. found that scientists view the value judge-
ments in science as involving more than just ethical and
epistemic values [19]. For example, when tasked with re-
doing a study or publishing the study with omitted data,
the scientists considered possible funding (economic val-
ues) problems with redoing the study. The conclusions
for this paper were that ethical training must focus more
on the overall values in science rather than focusing on
ethical theories or guidelines.
B. Assessing Ethical Decision Making
Historically someone’s ethical decision making is mea-
sured through standardized assessments [15]. Despite
these assessments being accessible to everyone, these as-
sessments have been studied in the medical/dentistry
field with very few studies measuring scientists’ ethical
decision making.
There are a few assessments that gauge someone’s eth-
ical sensitivity. For example, there is the Dental Ethical
Sensitivity Test (DEST) and Hebert et al’s medical ethi-
cal sensitivity test [20, 21]. In both of these measures, the
person taking the test reads vignettes regarding ethical
scenarios and then is asked to identify the moral issues
in each vignette.
To gauge someone’s moral reasoning, moral psycholo-
gists have developed the Defining Issues Test (DIT) and
subsequently developed the DIT-2 [5, 18]. The DIT-2 is
the accepted test to measure moral reasoning, which con-
tains 5 moral dilemmas including ones like stealing food
from someone hoarding food and a doctor has to decide to
give a frail and suffering patient an overdose on medicine
[22]. Currently, participants in other moral reasoning
studies are only given the DIT-2 where they are given a
multiple choice format to rank a set of items based on the
scenarios [23]. Another way to measure ethical reasoning
capacity is to use the Ethical Decision Making Measure
(EDM). This assessment is similar to the DIT where the
participant is given summaries of ethical scenarios and
asked to rate the severity of the violation in the scenario
on a scale of 1 (low) to 7 (high) [24].
One example of a real life application of measuring
moral reasoning is when Klinker and Hackmann used a
modified version of the DIT-2 to measure school princi-
pal’s moral reasoning. The study looked at 64 different
state school principals with different years of employment
to see their level of reasoning when asked about scenarios
that could happen to them at their schools. Their rea-
soning was gauged through a multiple choice test where
the participants could pick both their responses and their
justifications for those responses [25]. Klinker and Hack-
mann found that the principals’ values is one of the mainfactors that they use in order to make ethical decisions
regarding students’ welfare.
Moral motivation has received less attention from the
psychology literature than the other components. You
and Bebeau measured moral motivation through the Pro-
fessional Role Orientation Inventory (PROI). The PROI
assesses a person’s commitment to prioritize professional
values over personal values by measuring their author-
ity/responsibility and a person’s perceptions of self-
efficacy by measuring their agency/autonomy [15, 26].
Moral implementation is the least investigated com-
ponent. The sole attempt at measurement was by You
and Bebeau, who used dental students’ scores in a profes-
sional problem solving (PPS) class. In the class students
were required to implement action for complex cases that
present difficult human interaction problems that can
arise in dental practice [15].
C. Approaches to Ethics Training
One of the main ways for scientists to receive ethi-
cal training is through self-paced online modules like the
CITI program [24]. There are also hybrid options avail-
able, but a recent study of university RCR training re-
quirements found that 82% of research universities with
available institutional plans for RCR training require
an online-only ethics training [27]. This training has
been found to be ineffective improving ethical decision-
making, likely because the online training is wholly di-
dactic, and focuses on applying regulatory rules to sim-
ple, context-free cases [7]. Current RCR training is often
restricted to passive online readings or lectures while it
has been found that active learning modalities are more
effective than passive learning modalities [28, 29]. While
it was found that longer ethical training did cause people
to have better ethical decision making, many institutions
do not have the resources to have these extended training
sessions [5].
Some studies have sought to change RCR training to be
more grounded in actual scientific practice instead of pas-
sively focusing on the rules and regulations of RCR. One
of these new methods is to have a virtue-based approach
to teaching ethics. This kind of training involves look-
ing at the ethical considerations scientists’ encounter on
a daily basis and connects RCR to this everyday ethical
decision making [30]. This would allow scientists to see
the connection between ethical values and the practice of
science, hopefully leading to more robust and consistent
ethical conduct.
One way of changing RCR training to be more
grounded in scientific practice is to see the causes of un-
ethical conduct first. Cairns et al. conducted a phe-
nomenography where scientists were asked what makes
other scientists act unethical. They found that the two
biggest perceived factors for unethical conduct are pres-
sure and personal gain [31]. These two factors they found
line up with what other literature says about what mo-4
tivates people to act unethically.
III. THEORY
A. Values
There are multiple definitions of the concept of
“value”. For this study, we adopt the view that an in-
dividual values something, if the individual prefers that
thing over some other thing [32]. Values can be “goals”,
“aims”, “principles” or anything else individuals want to
be realized. Things can be valued intrinsically, because
they are preferred on their own, or instrumentally, be-
cause they help individuals achieve other goals. Values
that might guide scientific decision-making include find-
ing the truth, getting a publishable result, saving time
or money, helping graduate students, providing benefit
to society, or a variety of other personal, professional,
epistemic, or ethical goals. In general, we distinguish
“values” from “beliefs” by suggesting that the latter in-
volves claims about purely factual matters (i.e. what is
the case), while the former involves claims about norma-
tive ideals (i.e. what ought to be the case). Note that
values can be held by a community as well as individuals,
and they need not be mere “opinion” or “taste”. Ethical
values are values, and at least some people hold some
ethical values to apply universally [33].
Our main focus in this paper is on the different val-
ues scientists invoke when discussing ethics in science.
While we should generally expect that scientists will be
attentive to aspects of their work relevant to knowledge
production, their ability to reason well depends also on
their attentiveness to ethical values. Therefore, we inves-
tigated the epistemic ,ethical , and other non-epistemic
values invoked by scientists during discussions about re-
search ethics. In this research we relied on the “values in
science” literature that notes the many ways decision-
making about scientific methods are laden with non-
epistemic (e.g. ethical, legal, or economic) values [10, 11].
B. Rest’s theory of moral development
The Four Component Model (FCM) states that in or-
der for someone to behave morally in a situation, they
must demonstrate each of the following components:
moral sensitivity, moral reasoning, moral motivation, and
moral implementation [13]. These components are not
steps that have to be followed in a linear sequence but
are rather mental processes that collectively and inter-
actively produce ethical action [34]. A summary of the
four components for ethical decision making are found in
Table I.
Moral sensitivity is the ability to identify the morally
salient features of the scenario. This component is where
the person making an ethical decision identifies what is
going on in a scenario and how a decision might affectComponent Description
Moral Sensitivity Identifying morally salient ideas
Moral Reasoning Identifying morally right action
Moral Motivation Intending to do morally right action
Moral Implementation Acting on morally right action
TABLE I. Brief definitions of the four components of ethical
decision-making
others [13]. For example when scientists want to repro-
duce their results for a study, they must evaluate the
ethical dimensions of different alternatives: training grad
students to reproduce the data or send the data to a third
party to reproduce the data and save money.
Moral reasoning is the capacity to identify the morally
right action (e.g: identifying the option to benefit society
instead of the option to benefit oneself) [34]. This compo-
nent interacts with moral sensitivity since once someone
has identified the morally relevant factors of the situation
(people possibly affected, actions available, etc), they
must decide which action or actions are morally justifi-
able [13]. Rest et al. developed a set of ”Neo-Kohlberg”
schemas to classify moral reasoning based on Kohlberg’s
theory of moral development [35]. Rest et. al. organized
moral reasoning into three schemas (1) personal interest,
(2) maintaining norms, and (3) post-conventional [35].
The personal interest schema focuses on the personal
gains and losses that result from moral action. The main-
taining norms schema focuses on following some set of
rules or guidelines from moral action [36]. The maintain-
ing norms schema contains the following qualities: gen-
erally accepted social norms as a collective, norms apply
society-wide, need for norms to be clear, norms establish
a reciprocity, and there is an establishment of a hierarchy
[37]. The post-conventional schema focuses on prioritiz-
ing what society idealizes (e.g: maximizing other people’s
welfare) [36]. The post-conventional schema contains the
following qualities: involves reasoning about moral obli-
gations based on shared ideals, fully reciprocal, and are
open to scrutiny [18, 37]. These three schema are sum-
marized in Table II. In Rest’s formulation of the DIT-2
there is a hierarchy between the schema; The DIT-2 is
scored based on someone’s N2 score which measures how
much someone prioritizes the post-conventional schema
over the personal interest schema [22], which is deter-
mined by the degree to which post-conventional items
are prioritized and personal interest items are depriori-
tized.
Moral motivation is the intent to do what is morally
right [34]. After someone identifies the morally justifiable
decision, they must want to act on this decision. This
means that someone does not always need moral reason-
ing or moral sensitivity to have moral motivation. Rest
argues that in order for someone to have a high moral
motivation, they must prioritize ethical values over other
values [13]. Someone with a low moral motivation might5
be able to identify the ethical values and reasoning, but
they might not prioritize these values which will not lead
to a morally justifiable action.
Moral implementation is where the person acts on their
plan of action. A person may be able to identify an ac-
tion, come up with a morally justifiable reason for the
action, and place a priority on moral values, but in the
end they could end up not doing any action because of
a low moral implementation (i.e. non-cognitive barriers
to action such as weakness of will, lack of time, anxiety,
post-traumatic stress, etc.) [13].
While these components can interact with each other,
these components are distinct from one another, meaning
that mastery in one of the components does not mean
mastery in other components. You and Bebeau found
that after testing students on their proficiency in each of
the four components, there was no correlation between
a student’s competence in one component compared to
another [15].
Neo-Kohlberg
SchemaDefinition
Personal Interest
SchemaDoing something in hopes of personal
gain
Maintaining
Norms SchemaDoing something to follow set guide-
lines (e.g. following the law, following
set science standards)
Post-Conventional
SchemaDoing something because it is ulti-
mately the right thing to do
TABLE II. Brief definitions of the schema used in moral
reasoning
C. Improving Ethical Decision Making
The main reason for using the FCM is that it provides
a schematic for identifying the components in a person’s
ethical decision-making. While we can identify values to
identify the connection someone makes between ethics
and science, we need to also use the FCM to identify
how someone makes a decision in science. Identifying a
person’s components of their ethical decision will allow
us to focus research and interventions to strengthen any
deficiencies in any of the components. For example, some
research has identified that an improvement in someone’s
moral sensitivity will lead to an overall improvement in
ethical decision making, indicating that training should
focus primarily on improving someone’s moral sensitivity
[38, 39].
We argue that someone needs to improve on all of the
ethical decision making components in order to improve
on ethical decision making. Narvaez and Rest claim that
a person who fails to make the morally right decision has
a deficiency in one or more of the ethical decision making
components [14]. For example, someone can identify allof the morally salient ideas in a scenario but ultimately
cannot come up with a reason to make a decision. Since
all components are a necessary part of the process for
making an ethical decision, an improvement in one com-
ponent does not guarantee an improvement in someone’s
ethical decision making [14]. We then define improving
in ethical decision making as improving in all of the com-
ponents, but an improvement in one or more of the com-
ponents is a step in the right direction towards morally
acceptable decisions.
We identify how scientists’ are improving with respect
to individual ethical decision making components. As
mentioned in the introduction, we focused on changes in
moral sensitivity and moral reasoning. Since Rest defined
moral sensitivity as identifying the morally salient ideas
in a scenario and the literature states that epistemic and
non-epistemic values are important for making decisions
in science [10, 11, 16].
We define an improvement in scientists’ moral sensi-
tivity as invoking a greater number of values when dis-
cussing ethically-laden cases. “More values” in this case
means quantitatively more values and more types values
(e.g. instead of invoking purely epistemic values, partic-
ipants should invoke ethical, epistemic, and other values
in their sensitivity).
We define an improvement in moral reasoning as rea-
soning with a higher level schema. As mentioned above,
in the DIT-2, the post-conventional schema is defined the
as the highest level reasoning schema, followed by the
conventional and the personal interest schemas [22]. For
example, someone’s initial reasoning for a scenario starts
in the self-interest schema but over time the reasoning
changes into the post-conventional schema. This means
that their moral reasoning improved. In Rest’s case he
applied these schema to multiple choice questions. In
this study we applied the same schema to participants’
open-ended responses to ethical vignettes.
We applied these theories to answer the following re-
search questions:
1. How do discussions of values embedded in science
affect scientists’ moral sensitivity ?
2. How do discussions of values embedded in science
impact scientists’ moral reasoning ?
3. How effective is the value-focused training in im-
proving scientists’ overall ethical decision making?
IV. METHODS
A. Fellowship
Our project is structured around interviews conducted
as part of a year-long fellowship originally for 15 faculty
members in the basic sciences at a midwestern research
university. One of the faculty members dropped out so
we had 14 faculty members for this study. Participants6
were recruited by email, word of mouth, and explicit in-
vitations by the fellowship organizers. Recruitment in-
tentionally tried to promote diversity in terms of gender
and academic status.
The fellowship consisted of 10 sessions over the course
of an academic year. Each fellowship session was one and
a half hours long, and centered on reading materials given
to participants prior to the session. The fellowship did
not explicitly teach the participants about ethics, but was
designed to foster directed exploration and discussion of
the role that values play in decisions made within science.
The contexts ranged from foundational methodological
decisions, such as the choice of statistical methods, to de-
cisions about the application of science, such as whether
and how to advocate for policy changes. Throughout the
fellowship, the participants were made aware that they
were explicitly addressing value questions. They knew
the context of the research project was about ethics in
science, but the research goals were not discussed.
Author Scott Tanona ran the sessions and helped guide
discussions. Each session focused on a particular topic
meant to highlight the role of values in scientific decision-
making. The sessions were designed to provide some ba-
sic conceptual tools for understanding and potentially
addressing issues. Participants were asked to read and
come prepared to discuss pre-readings they were provided
for each session. During the sessions, the participants
were prompted to discuss topics with open-ended, gen-
eral questions inviting them to react or discuss what they
thought was interesting, as well as with directed ques-
tions meant to elicit discussion of particular topics. At
the end of the session, they were asked to write a follow-
up reflection, which asked for general thoughts about the
session as well as for answers to specific follow-up ques-
tions. The 15 (eventually 14) fellowship participants were
split into two even groups for ease of scheduling and to
ensure more chances of discussion from each participant.
Halfway through the fellowship (after five sessions), the
groups switched some members for scheduling purposes
and to provide broader exposure to other faculty. Meet-
ings were held in person, until the COVID-19 pandemic
required meetings by Zoom for the last three sessions.
The first introductory session introduced participants
to ethical theories and other philosophical concepts (such
as descriptive and normative distinction). After that,
each session focused on a particular topic or two that
highlighted the potential role of values in scientific
decision-making, including a number of topics from the
“values in science” literature (e.g., inductive risk, choice
of statistical methods, social structure of science, repro-
ducibility) and topics that highlighted the connection be-
tween science and society (e.g., advocacy, diversity, pub-
lic participation in science, public sharing of pre-prints).
In the final sessions, discussion turned to the COVID-19
pandemic, during which previous concepts were applied.
Topics were chosen and sessions were designed to en-
courage participants to reflect on and communicate both
the immediate, intermediate, and long-term instrumentalgoals of their own work. These topics highlighted epis-
temic values (including choices between them, e.g., be-
tween avoiding type I or type II error, or between model-
ing for predictive accuracy or representational accuracy),
as well as relationships between epistemic goals and ethi-
cal values (e.g., in “inductive risk”). This allowed discus-
sions about the goals of science as a whole, disciplinary
standards, general disciplinary goals, and impacts on so-
ciety. This same fellowship is explored in other works
[19, 31].
B. Data Collection - Interviews
Our data for this project comes from 28 individual,
semi-structured interviews of fellowship participants con-
ducted both before and after the fellowship series, respec-
tively called the pre-interview and the post-interview.
All interviews were video and audio recorded. Due to
one of the fellowship participants dropping out and be-
ing unable to make time for a post-interview, we only
analyzed the pre- and post- interviews for 14 of the 15
participants. In these interviews, the interviewee was
asked to respond to a series of fictional vignettes involv-
ing research misconduct (named “RCR Vignettes”). In
these recordings, the interviewer asked follow questions
to the interviewee’s responses to the vignettes that were
supposed to elicit more detailed responses from the in-
terviewee (e.g. “Can you explain more about ...”). In
total, there are three vignettes with six different ethi-
cal dilemmas within these vignettes. Two of the three
vignettes are adapted from the Ethical Decision Making-
measures assessment (EDM)[24]. The names and brief
descriptions of the dilemmas can be found in Table III.
Note that while these vignettes are not specifically re-
lated to physics, these concerns are relevant to physicists
[8].
We transcribed all 28 (14 pre-interview plus 14 post-
interview) of the video and audio recordings using ot-
ter.ai as the initial transcriber. We then corrected the
responses starting from the first question of the inter-
view and ended as soon as the questions about vignettes
were done.
C. Data Analysis
In order to see what kinds of values scientists appealed
to when thinking about research ethics, we identified the
explicit and implicit content of the values they appealed
to when discussing the vignettes. We are primarily inter-
ested in the extent to which scientists invoke epistemic,
ethical or other non-epistemic and non-ethical values. We
first took all of the responses from the interviewee and
split them up into quotes. One quote was the part of the
response that the interviewee invoked a value or values.
We identified values from quotes by identifying expres-
sions that contained normative or evaluative judgements.7
Title Description
Informed
consentA professor wants to conduct an experiment
where he gives a placebo shock to partic-
ipants, but does not tell the participants
about the placebo. The graduate student
working on the project does not want this
project to go on.
Publishing
concernThe previous experiment ends up being con-
ducted and two groups are working on the
data. One group believes that their data is
incorrect, and both groups want to publish
the correct data.
Authorship
concernThe professor promises a graduate student
that they would be first author on the
project, but he then takes back this promise
because he needs more first authorship.
Review
similar
experimentA professor is asked to review a project, but
the project is very similar to the one he is
working on.
Unpublished
dataThe professor found out that one of his stu-
dents got a hold of unpublished data from
another student and is trying to use the data
in their own research.
Hiring
diversityA professor department is hiring a new fac-
ulty member. They can only bring in three
candidates, and they decide to bring in
three male candidates and leave out a female
candidate.
TABLE III. Brief description of the ethical dilemmas from
the interviews.
A feature of the statement was judged as valued in a
statement if it could be fit into a form such as “The in-
terviewee values...” or “the interviewee has a goal of ...”,
whether or not the person explicitly held that value them-
selves or was discussing someone else’s values or possible
values. An example of how we coded values from quotes
is found in the Moral Sensitivity section.
We identified improvement in moral sensitivity by
looking at counts of the values invoked. For an improve-
ment in moral reasoning we looked at percent distribu-
tion. Due to the independence of improvement on the
components (an improvement on one of the components
does not mean an improvement on another component),
we accept that we can analyze the two components for
improvements in different ways.
1. Moral Sensitivity
As mentioned in the theory section, we define an im-
provement in moral sensitivity as participants invoking
more epistemic and non-epistemic values. We focused on
changes in scientist’s “moral sensitivity” by looking at
the different values the scientists invoked since these are
the “morally salient features” of the scenarios that wegave them.
Originally we defined the types of values scientists can
invoke in ethical scenarios by using Kohlberg’s stages of
moral development. These values are ethical (appeal to
what is the right thing to do), legal (appeal to threat
or punishment by governing entity), communitarian (ap-
peal to social approval), and self interest (appeal that
benefits self) [40]. As we coded the quotes, we found
other emergent types of values. These values are epis-
temic (appeal to improving knowledge), RCR (appeal to
regulations and guidelines), economic (appeal on using
resources), and practical (appeal to actions that must
be done for science). Within the ethical and epistemic
categories are sub-categories that further defined the cat-
egories of these values. The definitions for the categories
and sub-categories are in Table IV.
We also investigated how participants invoked unique
values. We define unique values as distinct and separate
values that are not mentioned previously in the scenario.
We are identifying unique values since we are saying that
Moral Sensitivity is being able to identify the morally
salient features in a scenario, we looked to see if the par-
ticipants are invoking a wide range of values instead of
repeating the same value over again. We started by look-
ing at the quotes to see the sub-category of the value for
the quote. If there are two values with the same sub-
category within the same scenario, then we would return
to the original transcript and look at the context for the
value. For example, if someone values “taking care of
their students” followed by valuing “publishing accurate
data” and then said they valued “taking care of their
students” in the same scenario, there would be three to-
tal values since they are valuing three things in total,
but there would only be two unique values since they are
valuing “taking care of their students” twice.
For this analysis, we considered the unique values per
person , so if one of the participants said that they value
“taking care of their students” and another participant
said that they also value “taking care of their students”,
then these would be considered two different unique val-
ues when looking at the data since these values come
from two different people. Below are example quotes for
coding values:
Ethical Value Example: In this quote the inter-
viewee is responding to the question “What should Dr.
Kaylee do?” about trying to convince other people to
bring in a woman to the hiring process when discussing
the “Hiring diversity” dilemma:
“So I mean, I guess if they’re all roughly equal
I mean he should probably talk to the chair
and get the woman in as a candidate... It’s
kind of really important to have the
same sort of representation as the gen-
eral population . It’s gonna be problematic
if you don’t have that.” - Interviewee 2
In the interviewee’s response it shows that they think
that having a diverse representation in the department8
TABLE IV. This table shows the categories, sub-categories, and definitions for the values used in the project. This table is
inspired by Linville’s values table [19]
Category Subcategory Definition
Ethical Ethical, misc. Doing what is right. This subcategory was used when the interviewee valued
doing what is right, but it did not fit in the other subcategories
Rights Treating people in a certain way due to some intrinsic feature
Fairness Treating others in the same way
Social Good Doing something that will benefit society or a group of people
Virtue Fulfilling a character trait that the interviewee found desirable
Interpersonal Care Maximizing welfare for one specific person
Responsible Conduct
of Reasoning (RCR)RCR Favoring regulations set by RCR training (plagiarism, authorship, etc)
Legal Legal Worrying about threats by government regulations.
Communitarian Communitarian Doing something good for the desire for peer/social approval.
Epistemic Epistemic, misc Advancing or improving science in some way. This subcategory was used
when the interviewee valued improving science, but it did not fit in the other
subcategories
Alethic Pursuing or clarifying knowledge about something
Explanatory Understanding some process
Methodological Understanding a part of the scientific process
Aesthetic Having a theory or explanation that is pleasing to the interviewee
Predictive Conducting science to make predictions about the future
Empirical Disseminating data or proper use of data
Technological Using science to have some technological application
Economic Economic Using fixed resources (time, money, etc)
Self-Interest Self-Interest Action providing some benefit to the interviewee
Practical Practical Actions that must be done for science to be conducted
is “important” and that the department should bring in
the woman candidate. This shows that they value the
importance in having proper and equal representation in
a department and since this would be ultimately bene-
fiting other people in the department this would be an
ethical value with the subcategory of “fairness”.
Epistemic Value Example: This quote the inter-
viewee is responding to the interviewer’s statement “Tell
me why you shouldn’t throw out data” when following
up on the question “What should they do?” regarding
the discrepancy between the two group’s data:
“So having data sets that acknowledge all
of your data, how it was collected, and
then that it’s been thoroughly vetted so
that you’ve done your best effort possible to
make sure that there haven’t just been mis-
takes in the synthesis of these data sets. I
think it’s integral to getting that data .”
-Interviewee 14
The interviewee responded by saying that all data and
the collection and analysis of the data is “integral” and
should not be thrown out. This quote is coded as “epis-
temic” with the subcategory of “empirical” as we see that
the interviewee values having proper collection/analysis
of all data.We identify the difference in the counts of values be-
tween the pre-interviews and post-interviews. We did not
assume any hierarchy among the values, and thus focused
on how many values are invoked and how many values of
each category are invoked. An analysis of invoking epis-
temic/ethical values versus RCR values can be found in
Linville et. al. [19].
We used a paired t-test to determine if there are any
significant differences between the pre-interview values
invoked and post-interview values invoked. This test de-
termines if there is a mean difference between observa-
tions for two pairs of data (in our case the pairs would be
the pre and post values) [41]. Due to the low numbers of
scientists, we added up all of the scientists’ invoked values
per ethical dilemma for the pre and the post interviews.
We then ran a paired t-test on the total values invoked
per dilemma and unique values invoked per dilemma to
find the p-value for the differences in values invoked.
We also found Cohen’s d to further determine if there
are any significant differences in the values invoked. Co-
hen’s d determines our standardized effect size of our
data, which allows us to see the variation of the means
of values invoked between the pre-interviews and post-
interviews [42]. We then ran a program in R using the
“effsize” library that found the Cohen’s d between the
pre and post-interview values. The generally accepted9
value of Cohen’s d are as follows: d = 0.2 is a small ef-
fect size, d = 0.5 is a medium effect size, and d = 0.8 is
a large effect size [42].
2. Moral Reasoning
We focused on changes in scientist’s “moral reasoning”
by identifying the participants’ Neo-Kohlberg Schema in
their reasoning to questions about the vignettes. At first,
we coded each individual response. Identifying an indi-
vidual response is defined as a claim made by the par-
ticipants followed by the reasoning for that claim state-
ment. Note that this order of claim then reasoning could
be flipped and we needed both the claim and reasoning
to be considered an individual response.
An example of identifying a response is found in the
quote “they should stop the experiment because harming
people is not the right thing to do”. In this quote, the
claim made is that “they should stop the experiment”.
The reasoning following the claim is “harming people is
not the right thing to do”. We only applied the Neo-
Kohlberg Schema to the participants’ reasoning and not
the claims they were making.
At first, we coded each individual response. After
coding each of the individual participant’s responses, we
combined all of the interviewee responses to see how the
scientists’ moral reasoning as a whole changed. We iden-
tified the participants’ moral reasoning this way as we are
attempting to find a way of identifying moral reasoning
in a more asset based model, which the DIT and EDM
do not fit in. Below are examples of coding participant’s
moral reasoning:
Post-Conventional Reasoning Example: In this
quote the interviewee is responding to the question
“What should Fowler do?” about being first author on
the paper instead of his graduate student:
“In my opinion, this is one of the most impor-
tant things we do is build up our students and
giving them opportunities to grow and succeed
and to develop a CVs that will help them be
successful long term is just the highest impor-
tance to me.” - Interviewee 4
In this quote we classify the claim as “building up stu-
dents”. The reasoning is the “being successful in the
long term” they get from being first author. The inter-
viewee believes that helping graduate students with their
careers is the most important idea when it comes to who’s
first author and is an example of post-conventional rea-
soning (helping others, ultimately right thing to do).
Maintaining Norms Reasoning Example: In this
quote the interviewee is responding to the question
“What should Stavenick do?” regarding a grad student
using unpublished work in their research:
“So I mean, that would be a discussion along
the lines of you, you know, you can’t do thisbecause you’re not allowed to see this proposal
in the first place.” -Interviewee 12
The claim in the quote is “[the student] can’t do this”
study. The reasoning is the grad student is not allowed
to see the proposal in the first place. The use of “not
allowed” shows that the interviewee is referencing rules
that prevent the grad student from seeing the unpub-
lished data in the first place and is an example of main-
taining norms reasoning (following a set of rules set in
the science community).
Since we are identifying an improvement in moral rea-
soning as participants using higher level schema in their
reasoning after the fellowship, we analyzed the partici-
pants’ percent distribution of schemas to determine if the
participants’ reasoning changed. Since we are now look-
ing at which specific schemas the participants are using
due to the hierarchy of the schemas, a percentage break-
down of each schema the participants use will more ac-
curately reflect on their change in moral reasoning. We
do identify the counts of schema as a way to see how
the number of reasonings are effected by the number of
values.
We used Fisher’s exact test to compare the distri-
butions of schema between pre-interviews and post-
interviews [43]. Fisher’s exact test is used to compare
the distribution of a categorical variable in a group with
the same variable of another group [44]. In our case, the
categorical variable is the schema invoked and the two
groups we’re comparing are pre-interviews and the post-
interviews. If the Fisher’s exact test shows that there is
a significant difference in the distribution of schemas be-
tween the pre- and post-interviews, then we can say that
improvement was a result of the fellowship.
We identified any significant changes in the total num-
ber of schemas when we ran a paired t-test to find the
p-value for the differences in the schemas invoked. If the
p-value is less than 0.05, we will consider this differences
significant and conclude that the number of reasonings
between the interviews are different.
3. Inter-rater reliability
We validated the values and Neo-Kohlberg schema
through inter-rater reliability (IRR). Three people coded
the interviewee’s responses. One of the coders coded all
of the quotes for both values and responses for the Neo-
Kohlberg schema and is labeled the original coder . The
other coders received 120 quotes (around 20% of the total
quotes) to determine the values and 84 responses (around
20% of the total responses) for schema.
We used the Fleiss Kappa to determine the rater agree-
ment between the three researchers for the individual val-
ues [45]. Calculating the Fleiss Kappa gives a number be-
tween 1.0 and 0.0 where 1.0 means the coders had a 100%
agreement between codes and 0.0 means the coders had
a 0% agreement between codes. While the Fleiss Kappa
value is calculated by counting the agreement between10
the coders, there are many outside factors affecting value
like how long people have been in the project and how
many categories of values that are coded [46, 47].
For coding the values in general, the original coder sent
out around 20% of the total value quotes coded (around
121 quotes) and the other two researchers coded values
for these quotes. The Fleiss Kappa value for the reliabil-
ity is around 0.53, which according to Landis and Koch
would be a moderate agreement between the coders [48].
After the Fleiss Kappa value was calculated, the two re-
searchers met up with the original coder and discussed
the values they coded. After discussing the values the
coders agreed on the same value for each quote until there
was a 100% agreement between the coders for the values
for each quote that was sent to the other coders.
For the unique values, the coders went through each
ethical dilemma and discussed what they thought were
unique values. We looked for agreements/disagreements
by discussing what were the unique values in each
dilemma. After going through one of the interviewee
dilemmas that were sent out and the coders reached 100%
agreement on each of the unique values, the coders con-
tinued to the next one until 100% agreement was reached
for each unique value on the dilemmas that were sent to
the other coders.
For the IRR for the moral reasoning schema, we used
a similar method to the total number of values where
the original coder sent out around 20% of the total rea-
soning responses and the other two researchers coded
schema for these responses. The Fleiss Kappa value for
the reliability is around 0.39, which according to Landis
and Koch would be a fair agreement between the coders
[48]. After the Fleiss Kappa value was calculated, the
two researchers met up with the original coder and dis-
cussed the schema they coded. The coders then discussed
the agreed upon schema for a response. After discussion
there was a 100% agreement between the coders for each
schema for the send out responses.
D. Limitations
We have a relatively small number of faculty from a
single university in this study. This allows us to look at a
small section of the scientific community and we therefore
hesitate to make broad claims from this. Additionally, we
acknowledge that the values participants’ invoked could
have changed due to factors outside the fellowship.
We also acknowledge that due to the one-on-one nature
of the interviews, the participants may have displayed
some sort of social approval bias where they mainly
talked about doing what is right. This may raise the
total post-conventional reasonings, but likely did so in
both the pre-interviews and post-interviews.V. RESULTS
We found that the average interview length for the
vignettes was about thirty minutes for both the pre-
interviews and the post-interviews (the average interview
length for the pre-interviews was 29:41 while the average
interview length for the post-interviews was 30:30)
A. Moral Sensitivity
FIG. 1. Total invoked values for the pre-interviews and post-
interviews. We combined all participants into one graph since
we are looking at scientists as a whole.
Figure 1 shows that the total number of values invoked
increased in the post-interview (443 vs 308). The most
prevalent values to show up are the ethical values and the
epistemic values. While these two categories show up the
most, there is only a change between the pre-interviews
and the post-interviews for the ethical category. The
ethical category had the largest increase in number of
values invoked while every other category stayed roughly
the same.
We found that the paired t-test for the total values
gives us a p-value of 0.007 and a Cohen’s d of 1.13 (large
effect size). The difference between the post-interview
values invoked and pre-interview values invoked are sta-
tistically significant and substantial. More explicitly, the
number of post-interview values invoked are significantly
higher than the pre-interview values invoked.
Looking at unique values invoked in Figure 2, we see
that the participants are invoking a variety of values in-
stead of repeating the same values. The trends here are
similar to what we see when looking at the total number
of values invoked. There is an increase in the total num-
ber of unique values invoked (360 vs 270). Once again,
the biggest increase in unique values was in the ethics
category while the rest of the categories stayed relatively
the same.
We found that the paired t-test for the unique val-
ues gives us a p-value of 0.02 and a Cohen’s d of 0.9711
Ethical Dilemma Pre Values Post Values
Informed Consent 67 92
Publishing Concern 73 107
Authorship Concern 36 42
Review Similar Project 48 74
Unpublished Data 33 68
Hiring Diversity 51 60
TABLE V. Total values invoked per ethical dilemma from
the interviews. This data was used to run the t-test and to
find Cohen’s d
Ethical Dilemma Pre Values Post Values
Informed Consent 60 75
Publishing Concern 58 83
Authorship Concern 32 36
Review Similar Project 45 66
Unpublished Data 33 57
Hiring Diversity 42 43
TABLE VI. Unique values invoked per ethical dilemma from
the interviews. This data was used to run the t-test and to
find Cohen’s d for the unique values
(large effect size). There is a significant difference be-
tween the post-interview unique values and pre-interview
unique values.
FIG. 2. Chart looking at the difference between unique in-
voked values for the pre-interviews and post-interviews. We
combined all participants into one graph since we are looking
at scientists as a whole.
B. Moral Reasoning
Figure 3 and Figure 4 show there is no significant
change in the distribution of schemas between the pre-
FIG. 3. Pie chart for percent breakdown of schema for levels
of reasoning for the pre-interviews. We combined all partic-
ipants into one graph since we are looking at scientists as a
whole.
FIG. 4. Pie chart for percent breakdown of schema for levels
of reasoning for the post-interviews. We combined all partic-
ipants into one graph since we are looking at scientists as a
whole.
interviews and the post-interviews for the participants’
moral reasoning. The Self-Interest schema did not change
with staying at 7% for both the pre-interviews and post-
interviews, while the Maintaining Norms schema and the
Post-Conventional schema showed no significant change
between the interviews as the post-conventional schema
increased by 4% in the post-interviews.
The Fisher’s exact test for the distribution of schemas
invoked gives us a p-value of 0.70. The difference be-
tween the distribution of pre-interview schemas and post-
interview schemas is insignificant.
The paired t-test for the total number of schemas in-
voked gives us a p-value of 0.004 and a Cohen’s d of 1.34
(large effect size). The difference between the the total
post-interview schemas invoked and total pre-interview
schemas invoked are statistically significant. More ex-
plicitly, the number of post-interview schemas invoked
are significantly higher than the pre-interview schemas
invoked.12
FIG. 5. Bar chart for every reasoning categorized into the
different schemas for all of the scientists.
VI. DISCUSSION AND FUTURE WORK
To answer the question of “how do the discussions of
values embedded in science shift scientists’ ethical sensi-
tivity”, we claim that the value-focused training improves
scientists’ ethical sensitivity. The participants evoked
more values in the vignettes since there was a significant
increase in the number of ethical values invoked after the
fellowship.
The participants invoked a wider range of values after
the fellowship which is seen in the unique values invoked.
Since time is not a factor in the change in number of
values, the participants are noticing more values in the
vingettes after the fellowship sessions. Since we are say-
ing that the morally salient items in these scenarios are
the values, this means that the participants’ moral sensi-
tivity increased. This is a step in the right direction since
to start improving in ethical decision making people need
to improve on each individual component.
The fellowship did not appear to improve scientists’
moral reasoning according to our measure. From the
data, there is a significant change in the total number
of schemas invoked, but there is not a change in the dis-
tribution of schemas participants used. While there is
a significant increase in the number of distinct reasons
brought up from pre to post fellowship (as seen in Fig-
ure 5), the overall trend between the pre and the post-
interviews shows that the schema of reasoning used re-
mains about the same.
The way we measured the scientists’ moral reasoning
is not sensitive enough did not fully capture significant
changes in moral reasoning. Although the participants’
reasoning schema did not change, something about their
reasoning must have changed. Reasons for these specu-
lations are because of the increase in participants’ total
reasonings in the post-interviews and the unique values
invoked are higher in the post-interviews. This suggests
that the participants are able to come up with not only
more claims and reasonings for scenarios but they arereasoning with different ideas as well. One suggestion for
this is that looking at just the Neo-Kohlberg Schema is
not enough to fully analyze someone’s moral reasoning
as this does not fully encapsulate what is being said in
the scientists’ moral reasoning [49].
This raises the question “How do we look more in
depth at the participant’s reasoning?” Answering this
new question will allow us to ultimately answer how
the participants’ reasoning changed due to the fellow-
ship more accurately. While we believe that the Neo-
Kohlberg Schema should not be disregarded, we need
more than just the schema to categorize scientists’ rea-
soning. One part of the next step is to look at how sci-
entist’s identify people affected by other people’s actions
(acting stakeholder and effected stakeholder). We plan to
explore this, and other theories from outside of physics,
in future projects.
Looking at the effectiveness of the value-focused fel-
lowship on improving the overall ethical decision making
of the scientists, we see a start for improving scientists’
ethical decision making. We see an improvement in eth-
ical sensitivity and inconclusive results for improvement
in moral reasoning. More work will be needed in look-
ing at motivation and implementation in order for us to
fully claim that this value-focused training is effective at
improving ethical decision making.
One of the goals for this paper is to introduce the-
ories from philosophy and moral psychology to Physics
Education Research in order to deconstruct scientists’
ethical decision making. We found that one of the bet-
ter ways to categorize ethical decision making is by us-
ing the Four Component Model due to the distinct and
easy to recognize components. We believe that as inter-
est in research ethics education for physicists grows, the
Four Component Model (and theories like it) will provide
much-needed theoretical and methodological structure to
attempts to understand, measure, and improve research
ethics education.
Another goal for this paper is to analyze the affects of
a novel approach to ethics training by introducing value
focused discussions for different science topics. As dis-
cussed in Linville et. al’s paper, the scientists our study
invoked both epistemic and non-epistemic values when
discussing ethics in their work before the fellowship[19].
If the scientists truly valued RCR training, then we would
have seen more invoked RCR values from the partici-
pants. Scientists valuing values other than RCR values
is further highlighted in this paper as the participants
are invoking significantly more ethical values and not
RCR values when discussing vignettes instead of their
own work after the fellowship. This further highlights
the need to improve on current RCR training by focus-
ing more on values in science.
With this work we want to highlight that this kind
work required collaboration between physics and philos-
ophy. This is due to the fact that physics in general has
little work when it comes to making ethical decisions in
science. Because of this, we have to turn to the field13
of psychology and philosophy to learn how people make
ethical decisions and adapt this to science as they are the
experts in this field. We encourage future ethics work to
be interdisciplinary with philosophy to have a more solid
foundation in ethics work.
VII. CONCLUSION
We found that guided deliberations on the values im-
plicit in real examples of scientific practice shifted sci-
entists’ invoked values with respect to research ethics.
After scientists talked about values at their fellowship
meetings, we found that scientists’ moral sensitivity im-
proved: they not only noticed more values in the RCR
Vignettes, but also invoked more unique values. We did
not see an improvement in their moral reasoning though
we suspect there was a lack of sensitivity in our way to
measure moral reasoning. This study shows that their
moral sensitivity did increase, ultimately making the sci-
entists’ more ethically aware of the actions in their work.
Our fellowship was aimed at discussing how scien-
tific decision-making includes both epistemic and non-
epistemic values, and entanglements between them.
We know theoretically that scientific decision-making is
value-laden and there are calls to increase scientists’ un-
derstanding and appreciation of thisOne way to improve scientists’ moral behavior is to
improve the different moral capacities in the Four Com-
ponent Model. Since the four component model suggests
that people form their ethical decisions based on consid-
ering moral sensitivity, moral reasoning, moral motiva-
tion, and moral implementation, improving these steps
will ultimately lead to scientists being more ethically at-
tentive in their studies.
In the end we want scientists to be more attentive to-
wards ethical issues throughout their work and to ulti-
mately make better ethical judgements about both re-
search conduct and the broader consequences of their
work. Our findings suggest that they actually were more
attentive to ethical values after the fellowship. This sug-
gests there is promise in approaches to ethics training
that focus on the general role of values in scientific prac-
tice.
ACKNOWLEDGMENTS
We would like to thank the Kansas State University
Physics Department for their support. We would also
like to thank K-SUPER for their valuable insights in
our study. This material is based upon work supported
by the National Science Foundation under Grant No.
#1835366.
[1] 19.1 Guidelines on Ethics (Full Statement).
[2]Read ”Fostering Integrity in Research” at NAP.edu .
[3] M. J. Reiss, Teaching Ethics in Science, Studies in Science
Education 34, 115 (1999).
[4] Responsible Conduct of Research (RCR) Basic |CITI
Program.
[5] M. Mumford, Read ”Fostering Integrity in Research” at
NAP.edu .
[6] S. Powell, M. Allison, and M. Kalichman, Effectiveness
of a Responsible Conduct of Research Course: A Pre-
liminary Study, Science and engineering ethics 13, 249
(2007).
[7] A. L. Antes, S. T. Murphy, E. P. Waples, M. D.
Mumford, R. P. Brown, S. Connelly, and L. D.
Devenport, A Meta-Analysis of Ethics Instruction
Effectiveness in the Sciences, Ethics & Behav-
ior19, 379 (2009), publisher: Routledge, eprint:
https://doi.org/10.1080/10508420903035380.
[8] K. Kirby and F. A. Houle, Ethics and the Welfare of the
Physics Profession, Physics Today 57, 42 (2004), pub-
lisher: American Institute of Physics.
[9] F. A. Houle, K. P. Kirby, and M. P. Marder, Ethics in
physics: The need for culture change, Physics Today 76,
28 (2023), publisher: American Institute of Physics.
[10] M. J. Brown, Values in Science beyond Underdetermina-
tion and Inductive Risk, Philosophy of Science 80, 829
(2013).
[11] H. Douglas, Inductive Risk and Values in Science, Philos-
ophy of Science 67, 559 (2000), publisher: [The Univer-
sity of Chicago Press, Philosophy of Science Association].[12] M. J. B. Stokhof, Ethics and morality, principles and
practice, Zeitschrift f¨ ur Ethik und Moralphilosophie 1,
291 (2018).
[13] J. R. Rest and D. Narv´ aez, eds., Moral development in
the professions: Psychology and applied ethics , Moral de-
velopment in the professions: Psychology and applied
ethics (Lawrence Erlbaum Associates, Inc, Hillsdale, NJ,
US, 1994) pages: xii, 233.
[14] D. Narvaez and J. Rest, The four components of acting
morally. Moral behavior and moral development: An in-
troduction, Handbook of moral and character education
, 385 (1995).
[15] D. You and M. J. Bebeau, The independence of James
Rest’s components of morality: evidence from a profes-
sional ethics curriculum study, Ethics and Education 8,
202 (2013).
[16] K. C. Elliott, Values in Science, Elements in the Phi-
losophy of Science 10.1017/9781009052597 (2022), iSBN:
9781009052597 9781009055635 Publisher: Cambridge
University Press.
[17] L. K. Bright, Du Bois’ democratic defence of the value
free ideal, Synthese 195, 2227 (2018).
[18] J. Rest, D. Narvaez, S. Thoma, and M. Bebeau, DIT2:
Devising and Testing A Revised Instrument of Moral
Judgment, Journal of Educational Psychology 91, 644
(1999).
[19] C. L. Linville, A. C. Cairns, T. Garcia, B. Bridges, J. Her-
ington, J. T. Laverty, and S. Tanona, How Do Scientists
Perceive the Relationship Between Ethics and Science?
A Pilot Study of Scientists’ Appeals to Values, Science14
and Engineering Ethics 29, 15 (2023).
[20] M. Bebeau, J. Rest, and C. Yamoor, Measuring dental
students’ ethical sensitivity, Journal of dental education
49, 225 (1985).
[21] P. C. H´ ebert, E. M. Meslin, and E. V. Dunn, Measuring
the ethical sensitivity of medical students: a study at the
University of Toronto, Journal of Medical Ethics 18, 142
(1992).
[22] About the DIT.
[23] J. Rest, D. Narvaez, M. Bebeau, and S. Thoma, A neo-
Kohlbergian approach: The DIT and schema theory, Ed-
ucational Psychology Review 11, 291 (1999), place: Ger-
many Publisher: Springer.
[24] M. D. Mumford, L. D. Devenport, R. P. Brown, S. Con-
nelly, S. T. Murphy, J. H. Hill, and A. L. Antes, AR-
TICLES: Validation of Ethical Decision Making Mea-
sures: Evidence for a New Set of Measures, Ethics &
Behavior 16, 319 (2006), publisher: Routledge eprint:
https://doi.org/10.1207/s15327019eb1604 4.
[25] J. F. Klinker and D. G. Hackmann, An Analysis of Prin-
cipals’ Ethical Decision Making Using Rest’s Four Com-
ponent Model of Moral Behavior, Journal of School Lead-
ership 14, 434 (2004), publisher: SAGE Publications Inc.
[26] M. J. Bebeau, D. O. Born, and D. T. Ozar, The de-
velopment of a professional role orientation inventory,
The Journal of the American College of Dentists 60, 27
(1993).
[27] T. Phillips, F. Nestor, G. Beach, and E. Heitman, Amer-
ica Competes at 5 Years: An Analysis of Research-
Intensive Universities? Rcr Training Plans, Science and
Engineering Ethics 24, 227 (2018), publisher: Springer
Verlag.
[28] J. M. Dubois and J. M. Dueker, Teaching and Assessing
the Responsible Conduct of Research: A Delphi Consen-
sus Panel Report, The Journal of Research Administra-
tion40, 49 (2009).
[29] M. W. Kalichman, Responding to Challenges in Educat-
ing for the Responsible Conduct of Research, Academic
Medicine 82, 870 (2007).
[30] R. Pennock and M. O’Rourke, Developing a Scien-
tific Virtue-Based Approach to Science Ethics Training,
Science and engineering ethics 23, 10.1007/s11948-016-
9757-2 (2017).
[31] A. C. Cairns, C. Linville, T. Garcia, B. Bridges,
S. Tanona, J. Herington, and J. T. Laverty, A phe-
nomenographic study of scientists’ beliefs about the
causes of scientists’ research misconduct, Research Ethics
, 17470161211042658 (2021), publisher: SAGE Publica-
tions Ltd.
[32] D. M. Hausman, Preference, Value, Choice, and Welfare
(Cambridge University Press, 2011).
[33] S. Kagan, Normative Ethics (Routledge, 2018) google-
Books-ID: 7f EDwAAQBAJ.
[34] J. R. Rest, Research on Moral Development: Implica-
tions for Training Counseling Psychologists, The Coun-seling Psychologist 12, 19 (1984), publisher: SAGE Pub-
lications Inc.
[35] S. J. Thoma, The Defining Issues Test of moral judgment
development., Behavioral Development Bulletin 19, 55
(2014), publisher: US: Joseph D. Cautilli.
[36] J. Rest, D. Narvaez, M. J. Bebeau, and S. J. Thoma,
Postconventional moral thinking: A neo-Kohlbergian
approach , Postconventional moral thinking: A neo-
Kohlbergian approach (Lawrence Erlbaum Associates
Publishers, Mahwah, NJ, US, 1999) pages: ix, 229.
[37] S. J. Thoma, Measuring moral thinking from a neo-
Kohlbergian perspective, Theory and Research in Edu-
cation 12, 347 (2014).
[38] L. Myyry and K. Helkama, The Role of Value
Priorities and Professional Ethics Training in
Moral Sensitivity, Journal of Moral Education
31, 35 (2002), publisher: Routledge eprint:
https://doi.org/10.1080/03057240120111427.
[39] H. Clarkeburn, J. R. Downie, and B. Matthew,
Impact of an Ethics Programme in a Life Sci-
ences Curriculum, Teaching in Higher Educa-
tion 7, 65 (2002), publisher: Routledge eprint:
https://doi.org/10.1080/13562510120100391.
[40] L. Kohlberg, C. Levine, , and A. Hewer, Moral Stages: A
Current Formulation and a Response to Critics (1983),
accepted: 2016-01-08T18:51:08Z.
[41] H. Hsu and P. Lachenbruch, Paired t Test (2008).
[42] D. Lakens, Calculating and reporting effect sizes to fa-
cilitate cumulative science: a practical primer for t-tests
and ANOVAs, Frontiers in Psychology 4(2013).
[43] V. Bewick, L. Cheek, and J. Ball, Statistics review 8:
Qualitative data – tests of association, Critical Care 8,
46 (2004).
[44] H.-Y. Kim, Statistical notes for clinical researchers: Chi-
squared test and Fisher’s exact test, Restorative Den-
tistry & Endodontics 42, 152 (2017), publisher: The Ko-
rean Academy of Conservative Dentistry.
[45] J. L. Fleiss, Measuring nominal scale agreement among
many raters, Psychological Bulletin 76, 378 (1971), place:
US Publisher: American Psychological Association.
[46] K. A. Hallgren, Computing Inter-Rater Reliability for
Observational Data: An Overview and Tutorial, Tutori-
als in quantitative methods for psychology 8, 23 (2012).
[47] J. Sim and C. C. Wright, The Kappa Statistic in Re-
liability Studies: Use, Interpretation, and Sample Size
Requirements, Physical Therapy 85, 257 (2005).
[48] J. R. Landis and G. G. Koch, The Measurement of Ob-
server Agreement for Categorical Data, Biometrics 33,
159 (1977).
[49] T. Garcia, C. Solis, C. Linville, B. Bridges, W. Jones,
J. Herington, S. Tanona, and J. T. Laverty, Examin-
ing Physicists’ Ethical Reasoning: A New Methodology
(2022) pp. 200–205, iSSN: 2377-2379.