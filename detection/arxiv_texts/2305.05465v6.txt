THE EMERGENCE OF CLUSTERS IN SELF-ATTENTION
DYNAMICS
BORJAN GESHKOVSKI, CYRIL LETROUIT, YURY POLYANSKIY,
AND PHILIPPE RIGOLLET
Abstract. Viewing Transformers as interacting particle systems, we describe
the geometry of learned representations when the weights are not time depen-
dent. We show that particles, representing tokens, tend to cluster toward
particular limiting objects as time tends to infinity. Cluster locations are
determined by the initial tokens, confirming context-awareness of representa-
tions learned by Transformers. Using techniques from dynamical systems and
partial differential equations, we show that the type of limiting object that
emerges depends on the spectrum of the value matrix. Additionally, in the
one-dimensional case we prove that the self-attention matrix converges to a
low-rank Boolean matrix. The combination of these results mathematically
confirms the empirical observation made by Vaswani et al. [VSP`17] that
leadersappear in a sequence of tokens when processed by Transformers.
Contents
Part 1. Introduction and main results 1
1. Introduction 1
2. Asymptotic low-rankness of the self-attention matrix 6
3. Clustering toward vertices of convex polytopes 8
4. Clustering toward hyperplanes 10
5. A mix of hyperplanes and polytopes 13
Part 2. Proofs 13
6. Well-posedness 13
7. Proof of Theorem 2.1 20
8. Proofs of Theorems 3.1 and 8.5 28
9. Proof of Theorem 4.2 39
10. Proof of Theorem 5.2 44
11. Numerical experiments 46
Part 3. Discussion and open questions 50
12. Outlook 50
References 53
Part1.Introduction and main results
1.Introduction
The introduction of Transformers in 2017 [VSP`17] marked a turning point
in the AI revolution, powering breakthroughs in natural language modeling and
computer vision. With remarkable empirical success, Transformers enable large
language models to compute very powerful representations using the self-attentionarXiv:2305.05465v6  [cs.LG]  12 Feb 20242 GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
mechanism. Yet, little is known about the geometric structure of these represen-
tations. As the size of these models grows at an astonishing rate, the need to
understand their inner workings is becoming a pressing scientific challenge. In this
work, we make a first step in this direction by describing the geometry of learned
representations.
To provide a transparent presentation of our findings, we take a leaf out of the
literature on continuous-time dynamics such as neural ordinary differential equa-
tions (ODEs) [CRBD18, Wei17, HR17]. By viewing layers as a time variable, this
formalism has emerged as a flexible mathematical framework to implement and
study ResNets [HZRS16a] as particular discrete-time versions of a parametrized
dynamics of the form
9xptqâ€œfÎ¸pxptqq, tPr0, Ts.
Here Î¸is the trained parameter of a neural network and fÎ¸is characterized by the
precise architecture of the ResNet1. In turn, an input (e.g., an image) xp0qPRdis
mapped to its representation xpTq.
Unlike neural ODEs and ResNets, the representation map of Transformers is
not solely a function of an individual input xp0qPRdbut rather of a set/sequence
px1p0q, . . . , x np0qqofnÄ›1d-dimensional tokens. These tokens then evolve in
time by interacting with each other per the self-attention mechanism. Namely,
following [SABP22], we view tokens as particles, and the transformer dynamics as
an interacting particle system of the form
9xiptqâ€œnÃ¿
jâ€œ1PijptqV xjptq, tPr0,`8q, (1.1)
for any iPrns, where Pijptqare the entries of a nË†nstochastic matrix Pptq, given
by
Pijptq:â€œexQxiptq,Kxjptqy
Å™n
â„“â€œ1exQxiptq,Kxâ„“ptqy,pi, jqPrns2. (1.2)
Here the matrices Q(Query), K(Key), and V(Value) are learned from data.
Note that Q, Kneed not be square. The nË†nmatrix Pptqis called self-attention
matrix. The wording attention stems precisely from the fact that Pijptqcaptures
the attention given by token ito token jrelatively to all tokens â„“P rns. The
matrices QandKin (1.2) warp the geometry of the input tokens, so that a trained
attention matrix contains weights which indicate semantic relations between words.
Such conclusions have been drawn in the context of language processing tasks in
[VSP`17, Figures 3-5].
Our goal is to showcase the fact that self-attention, which itself is the core
novelty of Transformers, entails a clustering effect. To that end, we focus on the
pure self-attention dynamics described in (1.1). In particular, we do not model
variations such as multiple heads, feed-forward layers, and layer normalization that
are typically adjoined to self-attention dynamics of (1.1). However, on this last
point, wenotethatourtheoreticalfindingsindicatethatwithoutanynormalization,
the dynamics (1.1) can diverge in some (or even all) directions over time. We leave
these additional questions for future research; see Section 12.
1A classical choice is Î¸â€œpW, A, bqPRdË†dË†RdË†dË†RdandfÎ¸pxqâ€œWÏƒpAx`bqwhere Ïƒis
an elementwise nonlinearity such as the ReLU ([HZRS16b]).THE EMERGENCE OF CLUSTERS IN SELF-ATTENTION DYNAMICS 3
Figure 1. ForVâ€œI3tokens cluster
toward the vertices of a convex polytope
(Theorem 3.1).1.1.Organization of the paper and
summary of contributions. The goal of
this paper is to characterize clustered rep-
resentations of a trainedTransformer by
studying the asymptotic behavior of a se-
quence of tokens px1ptq, . . . , x nptqqas they
evolve through the layers of a transformer
architecture using the dynamics (1.1). In
this setup, a Transformer is completely de-
scribed by the weight matrices pQ, K, Vq
obtained during training. Note that we as-
sume that these three matrices are time-
independent . While this assumption is mo-
tivated by mathematical convenience, it is worth noting that such weight-sharing
scenarios are in fact used in practiceâ€”see, e.g., ALBERT [LCG`20]â€”as they dras-
tically reduce the number of parameters of a network.
With parameters pQ, K, Vqfixed, tokens are subject to collective dynamics that
we calltransformer dynamics . While these dynamics are reminiscent of existing
models for opinion dynamics and flocking, they present they own mathematical
challenges requiring ad-hoc tools to study their asymptotic behavior.
The main conclusion of our analysis is that the set of tokens tx1ptq, . . . , x nptqu,
appropriately rescaled, tends to a clustered configuration astÃ‘8. Our theoretical
findings justify the empirical observation made in [VSP`17] that leadersappear
in a sequence of tokens when processed by Transformers. We now list our main
contributions.
(i)As a warm-up to the geometric characterization of the limits of sequences of
tokens, we show in Section 2 that when dâ€œ1andVÄ…0, the self-attention
matrix Pptqconverges to a low-rank matrix with entries 0and1astÃ‘`8thus
revealing the emergence of a small number of leaders that drive the transformer
dynamics. The restriction dâ€œ1follows from technical considerations, and some
pathological phenomena may occur in higher dimensions (see Remark 7.9). The
proof may be found in Section 7 . But numerical experiments (as well as past
empirical work) indicate that the result may extend to higher dimensions for almost
all initial sequences of tokens.
(ii)InSection3 wefirstfocusonthecase Vâ€œIdasanaturalcanonicalchoicethat
enables us to establish some of the main tools of the paper. We introduce a time
re-scaling reminiscent of the layer normalization heuristics to alleviate the possible
divergence of tokens. We show that along this scale the tokens converge to the
boundary of a convex polytope. For almost all initial sequences they even converge
to the vertices of the polytope, the number of which is significantly smaller than n.
This elucidates the clustering phenomenon. (See Fig. 1.) When Vâ€œÂ´Id, all tokens
following the dynamics (1.1) collapse to 0. The proofs are given in Section 8 .
(iii)We build on these results and in Section 4 consider the case wherein Vis
only assumed to have a simple and positive leading eigenvalue. This setting is much
closer to reality and corresponds to actual learned matrices V(see Figure 10). We
show that along the particular timescale, tokens cluster toward one of at most three4 GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
hyperplanes which are determined by the corresponding eigenvector. The proof is
given inSection 9 .
(iv)InSection 5 we complete the results of Sections 3 and 4 by addressing the
case where the leading eigenvalue has multiplicity. This results in clustering toward
the vertices of a convex polytope in some directions, and a linear subspace in the
others. The proof is provided in Section 10 .
(v)We also prove the global existence and uniqueness of solutions of all dynamics
considered in this work (including the mean field limit). We refer the reader to
Section 6 for more details.
We also observed numerically that our conclusions extend to more compound ar-
chitectures (see Conjecture 4.3, Section 12 andSection 11 ).
Value Key and Query Limit geometry Reference
Vâ€œId QJKÄ…0 vertices of convex polytope Theorem 3.1
Î»1pVqÄ…0, simplexQÏ†1, KÏ† 1yÄ…0union of 3parallel hyperplanes Theorem 4.2
Vparanormal QJKÄ…0 polytopeË†subspaces Theorem 5.2
Vâ€œÂ´Id QJKâ€œId single cluster at originËšTheorem 8.5
Table 1. Summary of the clustering results of this work.ËšAll results
except for the case Vâ€œÂ´Idhold for the time-scaled dynamics (3.1).
Remark 1.1 (Discrete time) .While we focus on the idealized setting of self-
attention dynamics in continuous-time, this is solely done for convenience and all
of our methods are straightforwardly applicable to the discrete-time setting. (See
also Remark 3.4.) The discrete-time analog of (1.1)with time-step âˆ†tÄ…0(equal
to1in practice) is simply the forward Euler iteration
xippk`1qâˆ†tqâ€œxipkâˆ†tq`âˆ†tnÃ¿
jâ€œ1Ë†exQxipkâˆ†tq,Kxjpkâˆ†tqy
Å™n
â„“â€œ1exQxipkâˆ†tq,Kxâ„“pkâˆ†tqyË™
V xjpkâˆ†tq,(1.3)
forkPN.
1.2.Notation. We denote by xÂ¨,Â¨yand}Â¨}the Euclidean dot product and norm
respectively, and we use the shorthand rns:â€œt1, . . . , nu. For any matrix MPRdË†d,
we order its eigenvalues (repeated according to multiplicity) by decreasing order of
modulus:|Î»1pMq|Ä›. . .Ä›|Î»dpMq|. We denote by }M}optheâ„“2â€”operator norm
of the matrix M, equal to the largest singular value of M. Given a set SÄ‚Rd, we
define the distance of a point xPRdtoSasdistpx, Sq:â€œinfsPS}xÂ´s}, and by
convpSqthe convex hull of S.
1.3.Related work. Our study and results build on several different lines of work,
and we draw some parallels in what follows.
1.3.1.Analysis of attention-based models. Given the widespread use of Transform-
ers in natural language processing, there has been a surge of interest in under-
standing the function and significance of attention layers within these models. In
[YBR`20], the authors show that when treated as discrete-time systems with addi-
tional dense layers and multiple heads appended to the core attention mechanism,THE EMERGENCE OF CLUSTERS IN SELF-ATTENTION DYNAMICS 5
Transformers exhibit the universal approximation property. In [LLH`20], the au-
thors present, to the best of our knowledge, the first interacting particle systems
perspective on Transformers. They then leverage the similarities between Trans-
formers (with an additional feed-forward layer compared to (1.1)) and convection-
diffusion equations to slightly improve the performance of Transformers by em-
ploying a Strang-Marchuk splitting scheme for time discretization. In [SABP22],
the authors interpret system (1.1) as the characteristics of a continuity equation.
Drawing on the similarities between (1.1) and Sinkhorn iterations, they propose
a novel architecture dubbed Sinkformer , which possesses the desirable property of
being a Wasserstein gradient flow.
1.3.2.Quadratic complexity of Transformers. The major computational challenge
of Transformers is their high computational complexity, particularly when process-
ing long sequences. Transformers require quadratic time and space complexity to
process sequences, because each self-attention layer contains n2products of the
formxQxi, Kx jy(fori, jPrns). The empirical observation that the self-attention
matrix Pisclosetoalowrankmatrixâ€”see[LWLQ22, Section4.4]forreferencesâ€”is
citedastheinspirationbehind Linformers [WLK`20]andthefine-tuningalgorithm
LoRA [HysW`22]. For both approaches, the low-rank structure is imposed rather
than extracted from Pitself. Other methods called sparse attention andblock at-
tentionhave been proposed to reduce the quadratic complexityâ€”see [WLK`20,
Section 2.2] for references. In the spirit of these works, a foreshadowing of the
clustering mechanism was invoked in [VKF20], where queries are clustered into
groups, again in view of reducing the quadratic complexity of self-attention. We
point out that [DCL21] previously demonstrated that without skip connections, the
dynamics trivializes and all tokens quickly lump together into a single tight cluster.
Our work, in contrast, shows that in the presence of skip connections a rich cluster
structure emerges.
Compared to the usual BERT,ALBERT [LCG`20] uses parameter-sharing across
layers, meaning that the weight matrices Q, K, Vin (1.1)-(1.2) do not depend on
time, as in the present paper. This does not reduce the theoretical Opn2qcomplex-
ity of the original Transformer, but, quoting [LCG`20], it "significantly reduce[s]
the number of parameters for BERTwithout seriously hurting performance, thus
improving parameter-efficiency. An ALBERT configuration similar to BERT-large
has 18x fewer parameters and can be trained about 1.7x faster. The parameter
reduction techniques also act as a form of regularization that stabilizes the training
and helps with generalization".
1.3.3.Neural collapse. Ourresultsandconclusionsbeararesemblancetosomegeo-
metric aspects of neural collapse for classification tasks [PHD20]. A key geometric
aspect of neural collapse is the observation that, during the training of deep neural
networks, the representation of different classes in the later layers of the network
tends to form a tight cluster around the vertices of a simplex. The emergence of a
simplex structure in the representation space provides insights into how the neural
network organizes and separates the different classes.
1.3.4.Clustering in interacting particle systems. The transformer dynamics (1.1)
have a strong connection to the vast literature on nonlinear systems arising in the
modeling of opinion dynamics and flocking phenomena. In addition to the clas-
sical Kuramoto model describing synchronization/clustering of oscillators [Kur75,6 GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
ABV`05], the model which is most similar to (1.1) is the Krause model [Kra00]
9xiptqâ€œnÃ¿
jâ€œ1aijpxjptqÂ´xiptqq, a ijâ€œÏ•p}xiÂ´xj}2qÅ™n
kâ€œ1Ï•p}xiÂ´xk}2q.
whichisnon-symmetricingeneral( aijâ€°aji), muchlike(1.1). When Ï•iscompactly
supported, it has been shown in [JM14] that the particles xiptqassemble in several
clusters as tÃ‘ `8. Other models of opinion dynamics and flocking have been
proposed and studied, among which the Vicsek model [VCBJ`95], the Hegselmann-
Krause model [HK02] and the Cucker-Smale model [CS07]. These models may
also exhibit a clustering behavior under various assumptions (see [MT14, CHH`16,
HKPZ19] and the references therein). The transformer dynamics are also closely
related to the dynamics employed in mean-shift clustering [Che95], and this work
indirectly sheds some light on its theoretical properties.
The analysis of transformer dynamics presents unique mathematical challenges
that cannot be addressed using the tools developed for these more primitive models.
In particular, our work demonstrates how different choices for the parameters lead
to remarkably diverse clustering patterns. Much more remains to be discovered
and this work is a first attempt a rigorous mathematical analysis of these synthetic
dynamics.
Acknowledgments. We thank Pierre Ablin, LÃ©onard Boussioux, Enric Boix Ad-
sera, Gabriel PeyrÃ©, Yair Shenfeld and Emmanuel TrÃ©lat for helpful discussions.
C.L. was supported by the Simons Foundation Grant 601948, DJ. P.R. is supported
by NSF grants IIS-1838071, DMS-2022448, and CCF-2106377. Y.P. is supported
in part by the MIT-IBM Watson AI Lab.
2.Asymptotic low-rankness of the self-attention matrix
As mentioned in Section 1.3, numerical experiments in [WLK`20] show that
the self-attention matrix P, defined in (1.2), has an almost low-rank structure.
This observation has then been leveraged to reduce the quadratic complexity in the
sequence length nwhich is inherent to Transformers, resulting in a non-negligible
decrease in the cost of training.
As a warm-up to deriving complete geometric representations of the dynamics,
ourfirstresultshows, inthesimple 1dcasethat Pptqindeedconvergesexponentially
fast toward a matrix which is typically both Boolean and low-rank (see Fig. 3).
Although there are clear obstructions to a rigorous extension of this result to higher
dimensions (Remark 7.9), numerical experiments appear to show that this result
holds in greater generality, for almost all initial sequences (Section 11).
Tosetthisup,weintroducetheset PofnË†nmatriceshavingtheformillustrated
in Fig. 2, where the asterisks denote arbitrary non-negative real numbers which add
up to 1. The row of asterisks may actually be any row between the first and the
last one.
Theorem 2.1 (Self-attention matrix converges to a low-rank Boolean matrix) .
Letdâ€œ1. Suppose that the scalars pQ, K, Vqsatisfy VÄ…0andQKÄ…0. For
any initial sequence of pairwise distinct tokens px1p0q, . . . , x np0qqPRn, there exists
some PËšPPsuch that the self-attention matrix Pptqdefined in (1.2)converges to
PËšastÃ‘`8.THE EMERGENCE OF CLUSTERS IN SELF-ATTENTION DYNAMICS 7
P2
666666666641 0 ::: 0
............
1 0 ::: 0
  ::: 
0::: 0 1
............
0::: 0 13
77777777775
1
Figure 2. Elements in P,
where PÏƒiPRnË†nare some
permutation matrices, and as-
terisks denote arbitrary non-
negative reals which add to 1.The proof may be found in Section 7. The rate of
convergence toward PËšis in fact doubly exponen-
tial in tfor coefficients outside the row of asterisks
in Fig. 2. The proof the theorem also reveals that
for almost all initial sequences of pairwise distinct
tokens, PËšis actually of rank 1or2, i.e., the row
of asterisks is equal to either e1â€œ p1,0, . . . , 0qor
enâ€œp0, . . . , 0,1q.
The interpretation of Theorem 2.1 is that in the
1dcase, at most three tokens capture the atten-
tionof all tokens except at most one. Typically,
theseleadingtokens are those carrying the largest
amount of information. This is also illustrated in
Fig. 4. Since the tokens xihere evolve on R, the
right-most and left-most ones (which typically tend
towardË˜8) capture the attention of all the others.
0 5 10 15 20 25 30 35
0
5
10
15
20
25
30
35t= 0.0, rank= 11
0 5 10 15 20 25 30 35
0
5
10
15
20
25
30
35t= 3.0, rank= 23
0 5 10 15 20 25 30 35
0
5
10
15
20
25
30
35t= 5.0, rank= 14
0 5 10 15 20 25 30 35
0
5
10
15
20
25
30
35t= 10.0, rank= 2
Figure 3. An illustration of the asymptotics of Pptqentailed by Theo-
rem 2.1 for nâ€œ40tokens, with Qâ€œKâ€œ1andVâ€œ1. (See Section 11
for details on computing.) Increasing nhas no effect on this behavior of
Pptqâ€”see Fig. 11.
t= 0.0
t= 2.0
t= 9.0
Figure 4. ThecloudstKxiptquiPr20s(green)andtQxjptqujPr20s(purple)
fordâ€œ2wherepairwisepointsofcloudsareconnectedbyalineofwidth
equalto Pijptq. Here VÄ…0andQÄ…0arerandommatricesand Kâ€œI2.
The creation of clusters is reflected by the rank Ä2structure of the self-
attention matrix Pptq. This interaction echoes findings illustrated in the
original paper [VSP`17]â€”for instance, Figures 3-5 therein.8 GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
3.Clustering toward vertices of convex polytopes
In the rest of the paper, we seek to taxonomize various clustering results for the
solutions to (3.1) when tÃ‘ `8, depending the sign and the multiplicity of the
eigenvaluesof V. Webeginbyfocusingonwhatmayappeartobethemostnatural2
caseVâ€œId, as is also done in [SABP22]. In fact, we demonstrate (theoretically
and numerically) later on, clustering is a generic phenomenon which holds under
much less restrictive assumptions.
The transformer dynamics considered in (1.1) does not contain a layer normal-
ization mechanism typically encountered in practice [VSP`17]. In absence of such
a device, tokens may diverge to infinity as in Theorem 2.1. In fact, the norm of the
tokens xiptqtypically diverges exponentially toward `8for any d: this is expected,
by analogy with the non-trivial solutions to 9yptqâ€œyptq.
To remedy this situation, we take inspiration from the solution yptqâ€œetVyp0q
to9yptqâ€œV yptq. Namely, for any iPrnswe consider the rescaled tokens
ziptq:â€œeÂ´tVxiptq,
which solve
9ziptqâ€œnÃ¿
jâ€œ1Ëœ
exQetVziptq,KetVzjptqy
Å™n
kâ€œ1exQetVziptq,KetVzkptqyÂ¸
VpzjptqÂ´ziptqq, tPr0,`8q.(3.1)
The initial condition remains the same: xip0q â€œzip0qfor any iP rns. More im-
portantly, the coefficients of the self-attention matrix for the rescaled tokens ziptq
are the same as those for the original tokens xiptq. Whence, the conclusion of The-
orem 2.1 also applies to the dynamics (3.1). We see this rescaling of tokens as a
mathematically justified surrogate for the layer normalization.
The appearance of the exponential factor within the self-attention kernel facili-
tates the analysis of (3.1) compared to (1.1), and it is in fact instrumental in the
proofs of all results that follow. Each result on the rescaled tokens ziptqthen gives
information on the dynamics of the original tokens xiptqby virtue of the relation
xiptqâ€œetVziptq.
We are now able to state the main result of this section on the case Vâ€œId. The
following theorem shows that the tokens ziptqevolving per dynamics (3.1) converge
to the boundary of a convex polytope as tÃ‘`8. We present here a simplified but
weaker version of our result for convenience, and refer the reader to Theorem 8.1
for a complete statement.
Theorem 3.1 (Convergence to points on the boundary of a convex polytope) .Sup-
pose Vâ€œIdandQJKÄ…0. Then, for any initial sequence of tokens tzip0quiPrnsÄ‚
Rd, there exists a convex polytope KÄ‚Rdsuch that for any iPrns,ziptqconverges
either to 0or to some point on BKastÃ‘`8.
The convex polytope Kis completely determined by the initial sequence of to-
kens, and QJK(refer to Claim 1). Numerical experiments (e.g. Fig. 5) also lead
us to claim that for almost all initial sequences of tokens, one should expect con-
vergence of ziptq(iPrns) toward some vertex of K. (Furthermore, the number of
vertices of Kis often found to be significantly smaller than n.) It may however
2Note that the case Vâ€œÂ´Idmay appear equally natural. For such a choice of V, we show
in Section 8.2 that the dynamics converge to a single cluster located at the origin. Multiplicative
constants preserving the sign, i.e., Vâ€œË˜cId, cÄ…0trivially yield the same conclusions.THE EMERGENCE OF CLUSTERS IN SELF-ATTENTION DYNAMICS 9
happen that for initial sequences taken in some null set (not seen when tokens
are drawn at random) some tokens converge to other points of the boundary BK,
namely in the interior of facets. On the other hand, for generic choices of initial
sequences, we do not see a way to predict Kexplicitly besides running the full
dynamics.
âˆ’505âˆ’505âˆ’505
t= 0.0
âˆ’505âˆ’505âˆ’505
t= 1.0
âˆ’505âˆ’505âˆ’505
t= 2.0
âˆ’505âˆ’505âˆ’505
t= 5.0
Figure 5. A toy example illustrating Theorem 3.1 with nâ€œ40tokens
inR3. Here Qâ€œKâ€œI3. The tokens converge to one of the vertices
(leaders) of the limiting convex polytope.
Recall that the points xiptqâ€œetziptqwhen Vâ€œIdfollow the original dynamics
(1.1). Akin to Theorem 2.1, this result also shows the emergence of a set of leaders
(given by the vertices of K) attracting all tokens as tgrows. It has been experi-
mentally observed (first in [VSP`17]) that in trained Transformers, tokens focus
their attention on local leaders in a way that seems to reproduce the syntactic and
semantic structure of sentences.
The proof of Theorem 3.1 is postponed to Section 8, and amounts to a couple
of effects entailed by the dynamics. First of all, the convex hull of the particles is
shrinking over time (Proposition 8.2). This is due to the fact that the distance of
the particle nearest to any half-space (not containing the particles) increases with10 GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
time. On the other hand, the convex hull ought not collapse since particles which
have not concentrated near the boundary of the limiting polytope will continue to
increase in magnitude until they themselves reach this boundary (Step 2 in the
proof). This occurs due to the time-rescaling.
Remark 3.2. Assuming QJKÄ…0does not seem to be essential for our conclu-
sions; instead, it guides the direction of the proof. To emphasize the broader validity
of our conclusion beyond this specific assumption, we conducted additional experi-
ments (refer to Section 12.1) which suggest that Theorem 3.1 (as well as Theorems
4.2 and 5.2 stated below) holds in more generality.
Remark 3.3 (Rate of convergence) .Although Theorem 3.1 (as well as Theorems
4.2 and 5.2 stated below) does not specify a rate of convergence toward BK, we expect
(and observe through numerics) that convergence happens very quicklyâ€”after few
layers, most tokens are already clustered. What "few layers" means here necessarily
depends on the typical modulus of the initial tokens, since the dynamics (1.1)is not
invariant under multiplication of all initial conditions by a fixed real number.
Remark 3.4 (Discrete time) .As alluded to in Remark 1.1, all our results extend to
the discrete-time Transformers (1.3). Indeed, just as in the continuous-time case,
there is a natural rescaled dynamics, which is the discrete analogue of (3.1): if we
setRâ€œId`Vâˆ†t, and assume that Ris invertible (which is the case for sufficiently
small âˆ†t), then zipkâˆ†tqâ€œRÂ´kxipkâˆ†tq:â€œzrks
isatisfies
zrk`1s
iâ€œzrks
i`âˆ†tnÃ¿
jâ€œ1Ëœ
exQRkzrks
i,KRkzrks
jy
Å™n
â„“â€œ1exQRkzrks
i,KRkzrks
â„“yÂ¸
RÂ´1VÂ´
zrks
jÂ´zrks
iÂ¯
, kPN.
The proofs of Theorems 2.1, 8.5, 3.1, 4.2, and 5.2 carry through with straightforward
modifications.
Let us provide some comments on the proof of Theorem 3.1 in the discrete-time
setting, for the sake of completeness. First of all, Proposition 8.2 holds intuitively
because for all integers iPrnsandkÄ›1,
zrk`1s
iâ€œ1
1`âˆ†tËœ
zrks
i`âˆ†tnÃ¿
jâ€œ1Prks
ijzrks
jÂ¸
PconvË†!
zrks
j)
jPrnsË™
.
We then define the candidate set of limit points as in (8.6), and Claim 1 holds
without any change in the statement or in the proof. Then, just as in Steps 2 and 3
in the proof of 8.1, we can first show that if zrks
iis not already near some point in the
candidate limit set, it will keep moving toward the boundary of the convex polytope.
Finally, we can prove that tokens cannot circulate indefinitely between different
points on the boundary. The combination of these arguments would establish the
convergence of each token toward some point in the set given by (8.6).
4.Clustering toward hyperplanes
While being a natural example to consider, value matrices found empirically are
much more general than Vâ€œId, which we considered in the previous section. We
now turn our attention to a significantly more general setting of value matrices,
which we formalize as follows.
Definition 4.1. We callpQ, K, Vqagood triple if the two following conditions are
satisfied:THE EMERGENCE OF CLUSTERS IN SELF-ATTENTION DYNAMICS 11
â€šthe eigenvalue of Vwithlargest modulusis real, positive, and simple; namely,
Î»1pVqÄ…|Î»2pVq|Ä›. . .Ä›|Î»dpVq|.
â€š xQÏ†1, KÏ† 1yÄ…0for any Ï†1PRdlying on the line kerpVÂ´Î»1pVqIdq.
The second condition simply states that the quadratic form xQÂ¨, KÂ¨yis positive
definite along the eigenspace associated to the leading eigenvalue of V. Note also
that if all entries of Vare positive, the first condition is automatically satisfied by
virtue of the Perron-Frobenius theorem. In fact, this assumption is generic. On the
one hand, it is satisfied by some pre-trained value matrices for ALBERT (Figure
10). On the other hand, numerical experiments indicate that a constant fraction
(about 14%) of matrices from the real Ginibre ensemble in dimension dâ€œ128â€”this
proportion is known to vanish as dÃ‘8, albeit very slowly [RS14].
Our clustering result in the setting of good triples can be summarized as follows:
the coordinate xziptq,Ï†1
}Ï†1}yof any token ziptqalong the eigenspace spanned by Ï†1
converges, as tÃ‘`8, toward one among possibly 3real scalars. Consequently, all
the tokens ziptqconverge toward one among at most three parallel hyperplanes; see
Fig. 6 for an illustration.
Theorem 4.2 (Convergence toward Ä3hyperplanes) .Assume thatpQ, K, Vqis a
good triple in the sense of Definition 4.1. Then, for any initial sequence of tokens
tzip0quiPrnsÄ‚Rd, there exist at most three parallel hyperplanes in Rdsuch that for
anyiPrns, the distance of the solution ziptqto(3.1)to one of these hyperplanes
converges to 0astÃ‘`8.
0 10âˆ’10010
t= 0.0
0 10âˆ’10010
t= 1.0
0 10âˆ’10010
t= 5.0
0 10âˆ’10010
t= 15.0
Figure 6. IllustratingTheorem4.2with nâ€œ40tokensin R2. Here Qâ€œ
Kâ€œI2,Visarandomsymmetricmatrixwitheigenvalues t1.35,Â´0.07u,
andÏ†1â€œp0.76,0.65q. The components of the tokens in the direction of
Ï†1(orange arrow) cluster over time. (See Figures 13â€“14 for examples
inR3.) We also observe that tokens typically cluster toward only two
hyperplanesâ€”a third one (passing through the origin) may appear for
non-generic initial sequences. The hyperplanes are perpendicular to Ï†1
since Vis diagonalizable.
The proof may be found in Section 9. The important role played by Î»1pVqin the
dynamics may be seen in (3.1): the component of ziptqalong Ï†1determines the size
ofetVziptqin the exponent appearing in (3.1). The tokens zjptqattracting other
tokens ziptqare those for which this component along Ï†1is largest in modulus. This
attraction process forms the clusters. These leaders, as in all our results, have been
empirically observed to be the ones carrying the largest amount of information in
the sentence (see Supplementary material in [VSP`17]).12 GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
Furthermore, Theorem 4.2 can also be interpreted in more classical machine
learning terms. On the one hand, it can be seen as an instance of K-flats clus-
tering[BM00, Vid11]â€”points in the input sequence are clustered, based on their
intrinsic similarity, to at most 3"flats" of dimension dÂ´1. On the other hand, it
ensures that for a good triple pQ, K, Vq, (3.1) generates a linearly separable repre-
sentation of tokens.
Beyond a single direction? Numerical experiments (e.g., Fig. 7) indicate that
a similar phenomenon emerges for more complex V. We formulate following con-
jecture which is a natural generalization of Theorem 4.2.
Conjecture 4.3 (Codimension conjecture) .LetkÄ›1be the number of eigenvalues
ofVwith positive real part. Then there exist at most three parallel Euclidean
subspaces of Rdof codimension ksuch that for any iPrns, the distance of ziptqto
one of these subspaces converges to 0astÃ‘`8.
âˆ’505âˆ’505âˆ’505
t= 0.0
âˆ’10010âˆ’10010âˆ’505
t= 5.0
âˆ’40âˆ’20020âˆ’25025âˆ’10010
t= 10.0
âˆ’100âˆ’50050
âˆ’1000100âˆ’40âˆ’20020
t= 15.0
(a)Conjecture 4.3: low-dimensional case.
0.0 2.5 5.0 7.5 10.0 12.5 15.0
tâˆ’0.20.00.2Positive limits for clustered coordinates
0.0 2.5 5.0 7.5 10.0 12.5 15.0
tâˆ’0.75âˆ’0.50âˆ’0.250.000.250.50Negative limits for clustered coordinates(b)Conjecture 4.3: high-dimensional
case.
Figure 7. (a)nâ€œ40,dâ€œ3andQâ€œKâ€œI3with Va random matrix
with eigenvalues t1.96,Â´0.22,0.25u. The kâ€œ2positive eigenvalues of
Vgenerate attraction between the tokens and even convergence in the
corresponding eigenspacesâ€“this explains the codimension kstatement.
The negative eigenvalue generates a repulsive effect between the tokens,
andweseeadivergencealongtwolines(notethedifferentscalesbetween
the four figures). (b) nâ€œ256,dâ€œ128, withpQ, K, Vqfixed random
matrices and Vsymmetric. For each coordinate jcorresponding to a
positive eigenvalue, the variance of the set tÏ†Ëš
jpziptqq:iPrnsu(shaded
area) tends to 0with t, while the mean (solid lines) converges to one
among two real scalars: one positive (top figure), one negative (bot-
tom) figure. Coordinates corresponding to negative eigenvalues diverge
(Fig. 15).THE EMERGENCE OF CLUSTERS IN SELF-ATTENTION DYNAMICS 13
5.A mix of hyperplanes and polytopes
WenowturnourattentiontoanevenmoregeneralversionofTheorem4.2, which
does not require the leading eigenvalue of Vto be simple. The resulting theorem
can be viewed as a combination of Theorem 4.2 and Theorem 3.1. Specifically,
we assume that Vbehaves as the identity when acting on the eigenspace of the
leading eigenvalue. This property is automatically satisfied if Vis normalâ€”so that
its eigenvectors form an orthonormal basisâ€”so we call such a Vparanormal.
Definition 5.1. We callpQ, K, Vqagood triple with multiplicity if the following
conditions hold:
(i)QJKis positive definite: QJKÄ…0;
(ii)Vis paranormal: there exist two linear subspaces F,GÄ‚Rdwhich are
invariant under V, and such that Fâ€˜Gâ€œRd,V|Fâ€œÎ»IdforÎ»Ä…0, and
ÏpV|GqÄƒÎ», where ÏpÂ¨qdenotes the spectral radius (the maximal modulus of
eigenvalues).
An example of such a Vis used for Fig. 8. We may now state our main result in
the setting of good triples with multiplicity. The proof may be found in Section 10.
Theorem 5.2 (Clustering for Î»1with multiplicity) .Suppose that pQ, K, Vqis a
good triple with multiplicity in the sense of Definition 5.1. Then, for any initial
sequencetzip0quiPrnsÄ‚Rd, there exists a bounded convex polytope KÄ‚Fsuch that
setting H:â€œpBKYt0uqË†G, for any iPrns, we have distpziptq,HqÃ‘0astÃ‘`8.
Part2.Proofs
6.Well-posedness
We collect several facts regarding the global-in-time existence and uniqueness
of solutions to all systems under consideration. Throughout the remainder of the
paper, we use the terminology "tokens" and "particles" interchangeably.
Toprovetheseresults, weleveragetheunderlyingcontinuityequation(see(6.1)).
For the sake of future use, we prove a more general well-posedness result for the
continuity equation than what is needed in this paper.
6.1.Notation. We denote by PcpRdqthe set of compactly supported probability
measures on Rd, and by P2pRdqthe set of probability measures ÂµonRdhaving
finite second moment:ÅŸ
Rd}x}2dÂµpxqÄƒ`8. Let C0pR;PcpRdqqdenote the Banach
space of continuous curves RQtÃÃ‘ÂµptqPPcpRdq. Here PcpRdqis endowed with
the weak topology, which coincides with the topology induced by the Wasserstein
distance Wpfor any pPr1,`8q.
As seen below, for compactness purposes regarding solutions to the continuity
equation, we consider an additional property on the support of such curves, sum-
marized by the following definition.
Definition 6.1 (Equi-compactly supported curves) .The set C0
copR;PcpRdqqcon-
sists of all elements ÂµPC0pR;PcpRdqqsuch that for any t0, t1PR, there exists a
compact subset KÄ‚Rdsuch that supppÂµptqqÄ‚Kfor any tPrt0, t1s.
We emphasise that there exist elements in C0pR;PcpRdqqwhich do not satisfy
this property with regard to their supportâ€”e.g., Âµptqâ€œp1Â´eÂ´1
t2qÎ´0`eÂ´1
t2Î´1
t.14 GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
âˆ’505âˆ’505âˆ’505
t= 0.0
âˆ’505âˆ’505âˆ’50âˆ’250
t= 5.0
âˆ’505âˆ’505âˆ’5000
t= 10.0
âˆ’505âˆ’505âˆ’10000âˆ’50000
t= 15.0
Figure 8. Illustrating Theorem 5.2 with nâ€œ40tokens in R3. As be-
fore, Qâ€œKâ€œId, and we take Vâ€œdiagp1,1,Â´1
2q. A convex polytope
Kemerges before time 5, toward which two coordinates of the tokens
cluster, and persists throughout the evolution, while the tokens diverge
along the coordinate corresponding to the eigenvalue Â´1
2(note the dif-
ferent scales between the four figures).
6.2.Well-posedness of the ODEs. For any initial datum, i.e. a sequence of n
points in Rd, the dynamics (1.1) is well-posed, in the sense that it admits a unique
solution defined for all times.
Proposition 6.2. For any initial datum X0â€œpx0
1, . . . , x0
nqPpRdqn, there exists a
unique Lipschitz continuous function RQtÃÃ‘Xptqâ€œp x1ptq, . . . , x nptqqsuch that
xipÂ¨qsolves(1.1)and satisfies xip0qâ€œx0
ifor any iPrns.
We postpone the proof which is seen as a corollary of the well-posedness for
the corresponding continuity equation. It follows that the equation (3.1) is also
well-posed:
Proposition 6.3. For any initial datum Z0â€œpz0
1, . . . , z0
nqPpRdqn, there exists a
unique Lipschitz continuous function RQtÃÃ‘Zptq â€œ p z1ptq, . . . , z nptqqsuch that
zipÂ¨qsolves(3.1)and satisfies zip0qâ€œz0
ifor any iPrns.THE EMERGENCE OF CLUSTERS IN SELF-ATTENTION DYNAMICS 15
Proof of Proposition 6.3. Since the equations (1.1) and (3.1) are related by the
change of variables xiptqâ€œetVziptq, Proposition 6.3 is an immediate consequence
of Proposition 6.2. â–¡
6.3.The continuity equation. To prove Proposition 6.2, we show a more general
result concerning global existence and uniqueness of solutions to the corresponding
continuity equation3
#
BtÂµ`divpXrÂµsÂµqâ€œ0inp0,`8qË† Rd
Âµ|tâ€œ0â€œÂµ0 inRd,(6.1)
when XrÂµsis theattention kernel
XrÂµspxq:â€œÅ¼
RdexQx,KyyV ydÂµpyq
Å¼
RdexQx,KyydÂµpyq. (6.2)
We will make use of the following notion of solution.
Definition 6.4. FixÂµ0PPcpRdq. We say that tÃÃ‘Âµptqâ€œ:Âµtis a solution to the
Cauchy problem (6.1)ifÂµPC0
copR,PcpRdqq, the function
RQtÃÃ‘Å¼
RdgpxqdÂµtpxq
is absolutely continuous for every gPC8
cpRdq, and
Å¼
RdgpxqdÂµtpxqâ€œÅ¼
RdgpxqdÂµ0pxq`Å¼t
0Å¼
Rdxâˆ‡gpxq,XrÂµtspxqydÂµspxqds
holds for almost every tPR.
We will make use of the following lemma regarding (6.2).
Lemma 6.5. For any RÄ…0there exists a constant C1pRqÄ…0such that for any
Âµ, Î½PPcpRdqwith support in Bp0, Rq,
}XrÂµs}L8pRd;RdqÄ}V}opR, (6.3)
}âˆ‡xXrÂµs}L8pRd;RdË†dqÄ2}QJK}op}V}opR2(6.4)
}XrÂµspÂ¨qÂ´ XrÎ½spÂ¨q} L8pBp0,Rq;RdqÄC1pRqW2pÂµ, Î½q. (6.5)
Proof.We henceforth set Gpx, yq:â€œexQx,Kyy. To show (6.3), since GÄ…0we see
that for any xPRd,
}XrÂµspxq}â©½}V}opÅ¼
Bp0,RqGpx, yq}y}dÂµpyq
Å¼
Bp0,RqGpx, yqdÂµpyqâ©½}V}opR.
3which can be seen as a mean-field limit, and is sometimes also referred to as a Vlasov equation .16 GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
We now show (6.4). Note that âˆ‡xGpx, yqâ€œQJKyGpx, yq, thus, arguing as above,
we find
}âˆ‡xXrÂµspxq}â©½Å¼
Bp0,Rq}âˆ‡xGpx, yq}}V y}dÂµpyq
Å¼
Bp0,RqGpx, yqdÂµpyq
`}V}opÅ¼
Bp0,RqGpx, yq}y}dÂµpyq
Å¼
Bp0,RqGpx, yqdÂµpyqÅ¼
Bp0,Rq}âˆ‡xGpx, yq}dÂµpyq
Å¼
Bp0,RqGpx, yqdÂµpyq
Ä2}QJK}op}V}opR2.
We finally prove (6.5). Using the fact that
Å¼
RdGpx, yqdÂµpyqâ©¾Ë†
inf
px,yqPBp0,Rq2Gpx, yqË™
ÂµpBp0, Rqq,
â€“with an analogous bound for Î½â€“, we see that it suffices to bound
Ë‡Ë‡Ë‡Ë‡Å¼
RdGpx, yqV ydÂµpyqÅ¼
RdGpx, yqdÎ½pyqÂ´Å¼
RdGpx, yqV ydÎ½pyqÅ¼
RdGpx, yqdÂµpyqË‡Ë‡Ë‡Ë‡
from above. We rewrite this difference by making ÂµÂ´Î½appear artificially, and we
then use the triangle inequality along with the fact that bothÅŸ
RdGpx, yqV ydÂµpyq
andÅŸ
RdGpx, yqdÂµpyqare bounded from above (by e}QJK}opR2maxp1,}V}opRq). We
thus end up with the task of bounding from above the absolute values of
Å¼
RdGpx, yqpdÎ½Â´dÂµqpyqandÅ¼
RdGpx, yqV ypdÎ½Â´dÂµqpyq.(6.6)
For the first integral, from the Kantorovich-Rubinstein duality we deduce
Ë‡Ë‡Ë‡Ë‡Å¼
RdGpx, yqpdÎ½Â´dÂµqpyqË‡Ë‡Ë‡Ë‡Ä}Gpx,Â¨q}C0,1pBp0,RqqW1pÂµ, Î½q. (6.7)
We now recall the following inequality relating Wasserstein distances of different
orders: for any pÄ›1and any bounded set B, for all Radon measures Âµ, Î½supported
inB,
W1pÂµ, Î½qÄWppÂµ, Î½qÄdiampBq1Â´1
pW1pÂµ, Î½q1{p. (6.8)
Using (6.8)andthefactthattheLipschitzconstant }Gpx,Â¨q}C0,1pBp0,Rqqisuniformly
bounded for}x}ÄRby some CRÄ…0in (6.7), we end up with
Ë‡Ë‡Ë‡Ë‡Å¼
RdGpx, yqpdÎ½Â´dÂµqpyqË‡Ë‡Ë‡Ë‡ÄCRW2pÂµ, Î½q.
The same chain of inequalities applies to the second integral in (6.6) (with the
additional multiplier }V}opR), which finally leads us to (6.5). â–¡
The following existence and uniqueness result is adapted from [PRT15, Theo-
rem 2.3]. In fact, the result holds true for any vector field XrÂµsonRdsatisfying
conditions analog to those entailed by Lemma 6.5.THE EMERGENCE OF CLUSTERS IN SELF-ATTENTION DYNAMICS 17
Proposition 6.6. For any initial condition Âµ0PPcpRdq, the Cauchy problem (6.1)
admits a unique solution ÂµPC0
copR;PcpRdqqin the sense of Definition 6.4.
Furthermore, we have the following stability estimate for solutions: for any RÄ…0
andTÄ…0, there exists a constant CpT, RqÄ…0such that for any Âµ0, Î½0PPcpRdq
with support in Bp0, Rq,
W2pÂµptq, Î½ptqqÄeCpT,RqtW2pÂµ0, Î½0q (6.9)
for any tPr0, Ts, where ÂµptqandÎ½ptqsolve(6.1)with initial conditions Âµ0andÎ½0
respectively.
Results of this nature can be found in the literatureâ€”see for instance [PRT15].
They are however not sufficient for our purposes. We wrote Proposition 6.6 in the
W2setting instead of the usual W1setting (used for instance for the classical Do-
brushin estimate [Dob79,Gol13])becauseitallowstoextendtheresultsof[WHL19]
without difficulty from classical ResNets to self-attention dynamics. We recall that
the goal of [WHL19] is to import classical (mean-field) optimal control tools such
as the Pontryagin maximum principle and the analysis of Hamilton-Jacobi-Bellman
equations to deep learning, and relies heavily on W2estimates (e.g., in [WHL19,
Section 4]).
Proof of Proposition 6.6. To ease reading, we split the proof in three parts.
Part 1: Existence. Fix an arbitrary TÄ…0. For kÄ›1, set
Ï„k:â€œT
2k.
We define a sequence of curves Âµk:r0, TsÃ‘PcpRdqby the following scheme4:
(i)Âµkp0q:â€œÂµ0;
(ii)Âµkpâ„“Ï„k`tq:â€œÂ´
Î¦t
XrÂµkpâ„“Ï„kqsÂ¯
#Âµkpâ„“Ï„kqforâ„“Pt0, . . . , 2kÂ´1uandtPp0, Ï„ks,
where for any xPRd,Î¦t
XrÂµkpâ„“Ï„kqspxqis the unique solution to the Cauchy problem
#
9yptqâ€œXrÂµkpâ„“Ï„kqspyptqqonr0, Ï„ks
yp0qâ€œx.
(The above problem indeed has a unique solution for any xPRdby virtue of the
Cauchy-Lipschitz theorem, using (6.4).) By construction, ÂµkPC0pr0, Ts;PcpRdqq
for any kâ©¾1.
We begin by showing that there exists a radius Râ€œRpTqÄ…0independent of k
such that supppÂµkptqqÄ‚Bp0, Rqfor any kâ©¾1andtPr0, Ts. To this end, for any
tPr0, Tsandkâ©¾1, letRkptqÄ…0denote the smallest positive radius5such that
supppÂµkptqqÄ‚Bp0, Rkptqq. We will first look to show that
supppÂµkpâ„“Ï„k`tqqÄ‚Bp0, Rkpâ„“Ï„kq`t}V}opRkpâ„“Ï„kqq. (6.10)
LetxPsupppÂµkpâ„“Ï„k`tqq, thus Âµkpâ„“Ï„k`tqpBpx, Îµqq Ä… 0for any ÎµÄ…0. By the
change of variables formula, we find thatÅ¼
pÎ¦t
XrÂµkpâ„“Ï„kqsqÂ´1pBpx,ÎµqqdÂµkpâ„“Ï„kqpzqÄ…0.
4In other words we "freeze" the vector field Xon each interval of the form râ„“Ï„k,pâ„“`1qÏ„kq, and
during this time interval, we follow the flow generated by this vector field starting from Âµkpâ„“Ï„kq.
5This radius always exists, since Âµkptqis compactly supported.18 GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
Consequently pÎ¦t
XrÂµkpâ„“Ï„kqsqÂ´1pBpx, ÎµqqXsupppÂµkpâ„“Ï„kqqâ€°H, andlet zbeanelement
lying in this set. From the Duhamel formula, we gather that
Î¦t
XrÂµkpâ„“Ï„kqspzqâ€œ:yptqâ€œz`Å¼t
0XrÂµkpâ„“Ï„kqspypsqqds.
Since zPpÎ¦t
XrÂµkpâ„“Ï„kqsqÂ´1pBpx, Îµqq, we find that
â€ºâ€ºâ€ºâ€ºz`Å¼t
0XrÂµkpâ„“Ï„kqspypsqqdsÂ´xâ€ºâ€ºâ€ºâ€ºÄÎµ.
Usingthetriangleinequality,(6.3),andsince zPsupppÂµkpâ„“Ï„kqqimplies zPBp0, Rkpâ„“Ï„kqq,
we deduce that
}x}â©½Îµ`t}V}opRkpâ„“Ï„kq`Rkpâ„“Ï„kq.
Since ÎµÄ…0is arbitrary, this inequality yields (6.10). We now use (6.10) to prove
the original claim. Using the definition of the radius Rkptq, we evaluate (6.10) at
tâ€œÏ„kand find
Rkppâ„“`1qÏ„kqâ©½p1`}V}opÏ„kqRkpâ„“Ï„kq.
By induction, we deduce that
Rkpâ„“Ï„kqâ©½p1`}V}opÏ„kqâ„“Rkp0q,
whence
Rkpâ„“Ï„kqâ©½Ë†
1`}V}opT
2kË™2k
Rkp0qÄƒe}V}opTR0,
where R0Ä…0denotes the smallest positive radius such that supppÂµ0qÄ‚Bp0, R0q.
Since the above bound is independent of k, the claim follows, yielding the desired
radius Râ€œRpTqÄ…0bounding the support of every element in the sequence. In
turn, we also deduce that ÂµkPC0
copR;PcpRdqqfor any kâ©¾1.
Using the above fact, along with (6.3) and the definition of Âµkpâ„“Ï„k`tq, we find
that
W2`
Âµkpâ„“Ï„k`tq, Âµkpâ„“Ï„kqË˜
Ä}V}opRt
for any â„“P t0, . . . , 2kÂ´1u,tP p0, Ï„ksandkâ©¾1. Gluing these inequalities (for
different â„“andt) with the triangle inequality yields
W2`
Âµkptq, ÂµkpsqË˜
Ä}V}opR|tÂ´s|
for any tPr0, Ts. Since Âµkp0qâ€œÂµ0for any kÄ›1, and since P2pRdqis the comple-
tion of Pcfor the Wasserstein distance W2, the ArzelÃ -Ascoli theorem implies the
existence of a subsequence uniformly converging to some ÂµËšPC0pr0, Ts;P2pRdqq.
Since for any tP r0, Tsthe curves Âµkptqhave their support enclosed in Bp0, Rq
for any kâ©¾1, we even deduce that ÂµËšPC0
copR,PcpRdqq. Note moreover that
ÂµËšp0qâ€œÂµ0and that
W2pÂµËšptq, ÂµËšpsqqÄ} V}opR|tÂ´s|
for any t, sPr0, Ts.
Thefactthat ÂµËšisasolutionof (6.1)followsexactlyfromthesamecomputations
as in [PRT15, p. 4711-4712], starting from (A.2) therein. We do not reproduce here
this argument since the computations are the same word for word. The fact that
for any TÄ…0we have suptPr0,TsW1pÂµËšptq, Âµkptqq Ã‘ 0askÃ‘ `8, which is
instrumental in [PRT15, p. 4711-4712], follows in our case from the left-hand-side
of (6.8).THE EMERGENCE OF CLUSTERS IN SELF-ATTENTION DYNAMICS 19
Part 2: Uniqueness. Regarding uniqueness, we proceed as follows. We first
recall the following estimate from [PR16, Proposition 4]. Let pÄ›1, lettÄ›0, let
v, wPC0,1XL8pr0, tsË†Rd;Rdq(both with Lipschitz constant LÄ…0, say), and let
Âµ, Î½PPcpRdq. Then
Wp`
pÎ¦t
vq#Âµ,pÎ¦t
wq#Î½Ë˜
Äep`1
pLtWppÂµ, Î½q`eLt
ppeLtÂ´1q
L}vÂ´w}L8pr0,tsË†Rd;Rdq.
(6.11)
Now assume that there are two solutions ÂµandÎ½of (6.1), with a spatial support
that is locally bounded in time, and having the same initial condition. Define
vpt, xq:â€œXrÂµptqspxqandwpt, xq:â€œXrÎ½ptqspxq. Also set
t0:â€œinfttÄ›0:W2pÂµptq, Î½ptqqâ€°0u,
and assume that t0â€° `8. Fix TÄ…t0and take RÄ…0such that ÂµtandÎ½t
are supported in Bp0, Rqfor any tPr0, Ts. Using (6.11) with pâ€œ2, and setting
C2pRq:â€œ2}QJK}op}V}opR2in (6.4), we find
W2pÂµpt0`sq, Î½pt0`sqqÄe2C2pRqsW2pÂµpt0q, Î½pt0qq
`eC2pRqseC2pRqsÂ´1
C2pRqsup
Ï„Prt0,t0`ss}vpÏ„,Â¨qÂ´wpÏ„,Â¨q}L8pRdq.
Choose sÄ…0sufficiently small so that eC2pRqsÂ´1Ä2C2pRqs. Then, by virtue of
(6.5) and the fact that W2pÂµpt0q, Î½pt0qqâ€œ0, we deduce
W2pÂµpt0`sq, Î½pt0`sqqÄ2seC2pRqssup
Ï„Prt0,t0`ssW2pÂµpÏ„q, Î½pÏ„qq.(6.12)
We choose s1Ä…0satisfying both eC2pRqs1Â´1Ä2C2pRqs1and2s1eC2pRqs1Äƒ1.
Applying (6.12) to every sPr0, s1swe obtain
sup
sPr0,s1sW2pÂµpt0`sq, Î½pt0`sqqÄ2s1eC2pRqs1sup
Ï„Prt0,t0`s1sW2pÂµpÏ„q, Î½pÏ„qq
Äƒsup
sPr0,s1sW2pÂµpt0`sq, Î½pt0`sqq,
which is a contradiction. Therefore Âµptqâ€Î½ptqfor any tÄ›0, which proves unique-
ness, as desired.
Part 3: Stability. We do not detail the proof of estimate (6.9), which is very
similar to the proof of (2.3) in Theorem 2.3 of [PRT15]: it follows from (6.11) with
pâ€œ2, and the argument after (A.7) in [PRT15], with W2instead of W1. See also
[PR13, Theorem 3]. â–¡
We conclude this section with the proof of Proposition 6.2, which follows as a
corollary of the above derivations.
Proof of Proposition 6.2. We first show existence. We apply Proposition 6.6 with
Âµ0:â€œ1
nÅ™n
jâ€œ1Î´x0
i, which in turn yields a solution Âµptqto (6.1). Following the proof
of Proposition 6.6, we also know that this solution satisfies Âµptqâ€œp Î¦t
XrÂµptqsq#Âµ0
for any tPR, and the vector field XrÂµptqssatisfies the assumptions of the Cauchy-
Lipschitz theorem. In particular, Âµptqis of the form Âµptqâ€œ1
nÅ™n
jâ€œ1Î´xiptqfor some
Lipschitz curves RQtÃÃ‘xiptq, for iP rns. Then tÃÃ‘Âµptq â€œ1
nÅ™n
jâ€œ1Î´xiptqis a
solution to the Cauchy problem (6.1)-(6.2) in the sense of Definition 6.4.20 GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
Secondly, weshowuniqueness. Supposethat Xptqâ€œpx1ptq, . . . , x nptqqandXËšptq
are two Lipschitz solutions to (1.1), with the same initial conditions. Then for a.e.
tÄ›0, using the equation (1.1) and the fact that the attention matrix coefficients
Pijptqdefined in (1.2) belong to r0,1s, we obtain
1
2d
dtmax
iPrns}xiptq}2Ä}V}opmax
iPrns}xiptq}2
(and analogously for xËš
iptq). Using GrÃ¶nwallâ€™s inequality, we deduce the existence
of two constants c1, c2Ä…0such that for any tÄ…0and for any iP rns,}xiptq}
and}xËš
iptq}are bounded from above by c1ec2t. It then follows that the empirical
measures ÂµpÂ¨qâ€œ1
nÅ™n
jâ€œ1Î´xipÂ¨qandÂµËšpÂ¨qâ€œ1
nÅ™n
jâ€œ1Î´xËš
ipÂ¨qbelong to C0
copR,PcpRdqq.
Moreover, they satisfy Âµptq â€œ p Î¦t
XrÂµptqsq#Âµ0andÂµËšptq â€œ p Î¦t
XrÂµËšptqsq#Âµ0and are
thus solutions to (6.1). Using the uniqueness result of Proposition 6.6, we obtain
thatÂµâ€œÂµËšwhich concludes the proof. â–¡
7.Proof of Theorem 2.1
Throughout this section we focus on the following dynamics:
9xiptqâ€œnÃ¿
jâ€œ1Ë†exxiptq,xjptqy
Å™n
kâ€œ1exxiptq,xkptqyË™
xjptq. (7.1)
Note that for dâ€œ1, the dot products in (7.1) are just multiplications of scalars.
We begin with the following observation, which holds for any dÄ›1.
Lemma 7.1. For any x1, . . . , x nPRd, the function f:RdÃ‘Rdefined by
f:xÃÃ‘logËœnÃ¿
jâ€œ1exx,xjyÂ¸
(7.2)
is convex.
Proof.Using the elementary inequality pa`bqÄ›2pabq1
2for any a, bÄ›0, we have
exppfpxq`fpyqqâ€œËœnÃ¿
jâ€œ1exppxx, xjyqÂ¸ËœnÃ¿
jâ€œ1exppxy, xjyqÂ¸
â€œ1
2nÃ¿
jâ€œ1nÃ¿
kâ€œ1â€
exppxx, xjy`xy, xkyq`exppxx, xky`xy, xjyqÄ±
(7.3)
Ä›nÃ¿
jâ€œ1nÃ¿
kâ€œ1expË†Bx`y
2, xj`xkFË™
(7.4)
â€œexpË†
2fË†x`y
2Ë™Ë™
.
Taking the logon both sides yields the statement. â–¡
The following lemma also holds for any dÄ›1.
Lemma7.2. LetRQtÃÃ‘txiptquiPrnsbe a solution to (7.1). Then for any i, jPrns,
the map RQtÃÃ‘}xiptqÂ´xjptq}is non-decreasing.THE EMERGENCE OF CLUSTERS IN SELF-ATTENTION DYNAMICS 21
Proof.The dynamics (7.1) can be equivalently written as
9xiptqâ€œâˆ‡fpxiptqq
where fis as in (7.2). By convexity of f(Lemma 7.1),
1
2d
dt}xiptqÂ´xjptq}2â€œx9xiptqÂ´9xjptq, xiptqÂ´xjptqy
â€œxâˆ‡fpxiptqqÂ´âˆ‡fpxjptqq, xiptqÂ´xjptqyÄ›0,
as desired. â–¡
We now present the proof of Theorem 2.1, which assumes dâ€œ1. We recall that
in the statement, Vis a positive scalar, but by reparametrizing time we may assume
thatVâ€œ1, so the 1ddynamics under consideration is really given by (7.1). Also,
to ease notations we focus on QKâ€œ1, but the proof adapts straightforwardly to
the setting QKÄ…0assumed in the statement of Theorem 2.1.
AsseeninSection7.1,itisnotdifficulttoprovetheconvergenceofthecoefficients
Pijptqof the attention matrix for indices iPrnsfor which xiptqbecomes unbounded
astÃ‘`8. This is the case for at least nÂ´1of the particles xiptq(Lemma 7.6).
But should one particle xiptqremain bounded, proving the convergence of Pijptq
forjPrnsis slightly tedious (Section 7.2). Since dâ€œ1, up to relabeling, we can
order the initial collection of particles (which, we recall, are assumed distinct):
x1p0qÄƒ. . .Äƒxnp0q.
We set
c:â€œmin
iPrnÂ´1s|xi`1p0qÂ´xip0q|. (7.5)
According to Lemma 7.2, we have |xiptqÂ´xjptq|Ä›cfor any iâ€°jand any tÄ›0.
In particular, particles never "collide".
7.1.Results about unbounded particles. In this section we gather several re-
sults concerning the indices icorresponding to particles xiptqwhich are not uni-
formly bounded in time. In particular, in Lemma 7.4 we show that for such indices
i,Pijptqconverges toward 0or1for any jPrns.
Lemma 7.3. LetAÄ…0denote the unique positive real number satisfying A2â€œ
n2exppÂ´A2q. Ifxnpt0qÄ…Afor some time t0Ä›0, then there exists c1Ä…0such
thatxnptq Ä›c1etfor any sufficiently large tÄ…0. Similarly, if x1pt0q Äƒ Â´ Afor
some t0Ä›0, then x1ptqÄÂ´ c1etfor any sufficiently large tÄ…0.
Proof.The two cases are symmetric since the evolution (7.1) commutes with the
involution ofpRdqngiven bypx1, . . . , x nqÃÃ‘pÂ´ x1, . . . ,Â´xnq. We thus focus on the
casexnpt0qÄ…A.22 GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
IfxnptqÄ›0for some tÄ›0, then
9xnptqâ€œnÃ¿
jâ€œ1Ë†exnptqpxjptqÂ´xnptqq
Å™n
kâ€œ1exnptqpxkptqÂ´xnptqqË™
xjptq (7.6)
Ä›xnptq
1`pnÂ´1qeÂ´cxnptq`Ã¿
tjPrns:xjptqÄƒ0uexnptqpxjptqÂ´xnptqqxjptq(7.7)
Ä›xnptq
1`pnÂ´1qeÂ´cxnptqÂ´neÂ´xnptq2
xnptq(7.8)
Ä›xnptq
nÂ´neÂ´xnptq2
xnptq. (7.9)
We provide some detail on the above sequence of inequalities. First of all, to pass
from (7.6) to (7.7), we use
exnptqpxkptqÂ´xnptqqÄeÂ´cxnptq
forjâ€œnand for any kPrns(which holds by virtue of (7.5)), combined with the
fact thatnÃ¿
kâ€œ1exnptqpxkptqÂ´xnptqqÄ›1
for all indices jsuch that xjptqÄƒ0. To pass from (7.7) to (7.8), we use exnptqzzÄ›
Â´1
xnptq, which holds for any zÄ0.
For any BÄ…A, we clearly have
B
nÂ´neÂ´B2
BÄ…0.
We then deduce from (7.8) and the fact that xnpt0q Ä…Athat xnptq Ã‘ `8 as
tÃ‘`8. Moreover due to the fact that the expression in (7.9) is bounded from
below byxnptq
2nwhenever xnptqis sufficiently large, we deduce that
xnptqÄ›c0et
2n
for any sufficiently large tÄ…0.
Coming back to (7.8), we find that for sufficiently large tÄ…0,
9xnptqÄ›xnptqËœ
1
1`pnÂ´1qeÂ´cc0et
2nÂ´eÂ´c2
0et
nÂ¸
.
This implies that
d
dtlogpxnptqqÄ›1Â´OÂ´
eÂ´t
3nÂ¯
,
whence
logpxnptqqÄ›t`Op1q
for sufficiently large tÄ…0, as desired. â–¡
Here and in what follows, Î´jkdenotes the Kronecker symbol.
Lemma 7.4. IfiPrnsis such that xiptqis not uniformly bounded with respect to
tÄ…0, then xiptqconverges to either Â´8or`8astÃ‘`8. Moreover,
(1) if xiptqÃ‘`8, then for any jPrns,Pijptqconverges to Î´njastÃ‘`8,
with doubly exponential rate.THE EMERGENCE OF CLUSTERS IN SELF-ATTENTION DYNAMICS 23
(2) if xiptqÃ‘Â´8, then for any jPrns,Pijptqconverges to Î´1jastÃ‘`8,
with doubly exponential rate.
Proof.We assume that xiptqis not uniformly bounded with respect to tÄ…0.
Without loss of generality, we assume that there exists a sequence of positive times
ttku`8
kâ€œ1with tkÃ‘`8such that xiptkqÃ‘`8 . Necessarily, xnptkqÃ‘`8 . We
notice that if xiptq Ä…0for some tÄ›0, then, arguing as in (7.6)â€“(7.7)â€“(7.8), we
have
9xiptqâ€œnÃ¿
jâ€œ1Ë†exiptqpxjptqÂ´xnptqq
Å™n
kâ€œ1exiptqpxkptqÂ´xnptqqË™
xjptqÄ›xnptq
nÂ´n
xiptqeÂ´xiptqxnptq.(7.10)
For sufficiently large integers kÄ›1, from (7.10) we get 9xiptkqÄ…0and9xnptkqÄ…0.
But as xiandxnincrease, the lower bound in (7.10) becomes larger. It follows that
9xiptqÄ›xnptq
2nÄ›xiptq
2n
for sufficiently large t, implying that xiptqÃ‘`8 with exponential rate as tÃ‘`8.
We now prove point 1. regarding Pptq. We assume that xiptqÃ‘`8 astÃ‘`8.
In this case, for jâ€°n(namely jPrnÂ´1s),
Pijptqâ€œexiptqxjptq
nÃ¿
kâ€œ1exiptqxkptqÄexiptqpxjptqÂ´xnptqqÄeÂ´cxiptq,
thusPijptqconvergesto 0astÃ‘`8(withdoublyexponentialrate). Consequently,
we also deduce that
Pinptqâ€œ1Â´nÂ´1Ã¿
jâ€œ1Pijptq
converges to 1, also with doubly exponential rate, as tÃ‘`8.
The case where xiptqÃ‘Â´8 is symmetric. This concludes the proof. â–¡
Our last result is useful in the next section.
Lemma 7.5. For any iPrnssuch that xiptqis not uniformly bounded with respect
totÄ…0, there exists some Î³iPR,Î³iâ€°0such that xiptqâ€œÎ³iet`opetqastÃ‘`8.
Proof.Without loss of generality we assume that xiptq Ã‘ `8 astÃ‘ `8. For
jâ€°n, we find
Pijptqâ€œexiptqxjptq
nÃ¿
kâ€œ1exiptqxkptqâ€œexiptqpxjptqÂ´xnptqq
nÃ¿
kâ€œ1exiptqpxkptqÂ´xnptqqÄeÂ´cxiptq.
Consequently,
PinptqÄ›1Â´neÂ´cxiptq.
Therefore, using Lemma 7.3 and the fact that xiptqÄ›biet
2nfor some biÄ…0(thanks
to (7.10)), we gather that
9xiptqÄ›Â´
1Â´neÂ´cxiptqÂ¯
xnptqÂ´neÂ´cxiptqc1et
Ä›Ë†
1Â´neÂ´cbiet
2nË™
xnptqÂ´neÂ´cbiet
2nc1et(7.11)24 GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
for some c1Ä…0independent of t. We also notice that due to (7.1), 9xiptqÄxnptq.
Using (7.11), firstly for iâ€œn, together with the trivial upper bound xnptqÄCet
for some CÄ…0independent of t(immediately seen from (7.1)), we obtain
9xnptqâ€œxnptqË†
1`oË†
eÂ´cbiet
3nË™Ë™
astÃ‘`8, which yields
xnptqâ€œÎ³net`opetq
for some Î³nÄ…0. Now using (7.11) for the index i, we gather that
9xiptqâ€œxnptq`oË†
eÂ´cbiet
3nË™
,
and so we deduce that
xiptqâ€œÎ³net`opetq.
Similarly, if xiptqÃ‘Â´8, then xiptqâ€œÎ³1et`opetq. This proves Lemma 7.5 (and
shows that Î³iPtÎ³1, Î³nu). â–¡
7.2.Results about bounded particles. In this section we collect results con-
cerning particles which remain uniformly bounded in time. The following lemma
entails that there can be at most one particle with this property.
Lemma 7.6. Consider
B:â€œ!
iPrns:xipÂ¨qPL8pr0,`8qq)
.
Then #BPt0,1u.
Proof.We first prove that either x1ptqÃ‘Â´8 orxnptqÃ‘`8 astÃ‘`8. By con-
tradiction, if this is not the case, then by Lemma 7.3, px1ptq, . . . , x nptqqPrÂ´ A, Asn
for any tÄ›0. We denote by Ithe set of configurations pxËš
1, . . . , xËš
nqPrÂ´ A, Asn
such that|xËš
iÂ´xËš
j| Ä› | xip0qÂ´xjp0q| Ä… 0for any distinct i, jP rns. For any
XËšâ€œpxËš
1, . . . , xËš
nqPI, the function fdefined in (7.2) (with anchor points given
byXËš) is strictly convexâ€”the equality in the inequality between (7.3) and (7.4) is
never achieved. Therefore, the proof of Lemma 7.2 shows that if XËšis seen as an
initial datum for the dynamics (7.1), then
vpXËšq:â€œd
dt|tâ€œ0|xËš
1ptqÂ´xËš
nptq|Ä…0.
SinceIis compact, v0:â€œinfXËšPIvpXËšqÄ…0. Hence, tÃÃ‘|x1ptqÂ´xnptq|grows at
least linearly, which is a contradiction.
Wemaythereforeassumewithoutlossofgeneralitythat x1ptqÃ‘Â´8 astÃ‘`8.
We prove that xnptqconverges to either Â´8, or0, or`8, astÃ‘`8. We assume
in the sequel that xnptqdoes not converge to Â´8or0. For any iP rns, if there
exists ÎµÄ…0and a sequence of positive times tsku`8
kâ€œ1tending to`8such that
xipskq Ä Â´ Îµ, then it follows from (7.10) that xiptq Ã‘ Â´8 . Therefore, by our
assumptions, we have lim inf tÃ‘`8 xnptq Ä›0. Also, since xnptqâ†›0, there exists
ÎµÄ…0and a sequence of positive times ttku`8
kâ€œ1tending to`8such that xnptkqÄ›Îµ
for any integer kâ©¾1. For any tÄ›0such that xnptqÄ›Îµ, we introduce the set of
indices
Nptqâ€œtiPrns:xiptqÄƒ0u,THE EMERGENCE OF CLUSTERS IN SELF-ATTENTION DYNAMICS 25
and we write
9xnptqÄ›exnptq2xnptq
nÃ¿
kâ€œ1exnptqxkptq`Ã¿
jPNptqexjptqxnptqxjptq
nÃ¿
kâ€œ1exnptqxkptqÄ›Îµ
n`1
eÎµ2Ã¿
jPNptqeÎµxjptqxjptq.(7.12)
According to Lemma 7.4, any point xiptqwhich takes negative values for arbitrarily
largetimesanddoesnotconvergeto Â´8hastoconvergeto 0. Therefore, thesecond
term in the lowermost bound in (7.12) is lower bounded by Â´Îµ
2nfor sufficiently large
t. All in all, we gather that 9xnptqÄ›Îµ
2nandxnptqconverges to`8astÃ‘`8. If
it converges to 0, then necessarily xnÂ´1ptqÃ‘Â´8 by combining Lemma 7.2 with
Lemma 7.4. This proves Lemma 7.6 in this case.
From now on we assume that xnptq Ã‘ `8 . Using (7.10) we see that if there
exists ÎµÄ…0such that xiptq Ä… Îµfor an unbounded sequence of times t, then
xiptqÃ‘`8. The same is true symmetrically when xiptqÄƒÂ´ Îµfor an unbounded
sequence of times t. Thus if iPB, necessarily xiptqÃ‘0. By Lemma 7.2 this can
be true for at most one index i, which concludes the proof of Lemma 7.6. â–¡
IfBâ€œH, Theorem 2.1 follows from Lemma 7.4. From now on, we assume that
#Bâ€œ1, and we denote by i0Prnsits unique element. We distinguish two cases:
either i0Pt1, nu(Lemma 7.7), or i0Rt1, nu(Lemma 7.8).
Lemma 7.7. Ifxnptqis bounded as tÃ‘`8, then PnnptqÃ‘1, and PnjptqÃ‘0
for any jPrnÂ´1s, astÃ‘`8. Similarly, if x1ptqis bounded as tÃ‘`8, then
P11ptqÃ‘1, and P1jptqÃ‘0for any jPrnÂ´1s, astÃ‘`8.
Proof.The two cases ( xnpÂ¨qbounded or x1pÂ¨qbounded) are symmetric since the
evolution (7.1) commutes with the involution of pRdqngiven bypx1, . . . , x nq ÃÃ‘
pÂ´x1, . . . ,Â´xnq. Whence, we only address the first one: we assume that xnptqis
bounded as tÃ‘`8. We first notice that all particles xjptqforjPrnÂ´1stend to
Â´8astÃ‘`8due to Lemma 7.6. We now prove the following properties:
(1)xnptqÄ…0for any sufficiently large t;
(2)xnptqÃ‘0astÃ‘`8;
(3) for any jPrnÂ´1s,PnjptqÃ‘0astÃ‘`8.
Toprovepoint(1), wenoticethatforsufficientlylarge t,xiptqÄ0forany iPrnÂ´1s.
If in addition xnptqÄ0, then due to (7.1), all xiptq(iPrns) remain negative and
due to (7.1), xnptqÃ‘Â´8 astÃ‘`8, which is a contradiction.
For point (2), we fix ÎµÄ…0, and set
T`
Îµ:â€œttÄ›0:xnptqÄ›Îµu.
We prove that if T`
Îµis unbounded, then xnptq Ã‘ `8 astÃ‘ `8, which is a
contradiction. As a consequence, T`
Îµis bounded for any ÎµÄ…0, which implies (in
conjunction with point 1.) that xnptqÃ‘0astÃ‘`8. So let us assume that T`
Îµ
is unbounded. We notice that for any Î´Ä…0, iftPT`
Îµis sufficiently large then
Ë‡Ë‡Ë‡exnptqxjptqxjptqË‡Ë‡Ë‡ÄÎ´26 GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
for any jPrnÂ´1ssince xjptqÃ‘`8 astÃ‘`8. Therefore,
nÃ¿
jâ€œ1exnptqxjptqxjptqÄ›eÎµ2ÎµÂ´pnÂ´1qÎ´Ä›0,
where we took Î´Ä…0sufficiently small for the last inequality to hold. Consequently,
9xnptqâ€œnÃ¿
jâ€œ1exnptqxjptqxjptq
nÃ¿
jâ€œ1exnptqxjptqÄ›exnptq2xnptqÂ´pnÂ´1qÎ´
exnptq2`nÂ´1.
It is not difficult to see that this implies that xnptqÃ‘`8 astÃ‘`8, which is a
contradiction.
For point (3), we first notice that for any jâ€°n, since xjptqÃ‘Â´8,
9xjptqâ€œnÃ¿
kâ€œ1Ë†exjptqpxkptqÂ´xnptqq
Å™n
â„“â€œ1exjptqpxâ„“ptqÂ´xnptqqË™
xkptqÄx1ptq
n`n
ÎµeÂ´xjptqxnptq.
Using Lemma 7.3, we deduce the existence of some c2Ä…0such that
xjptqÄÂ´ c2et
for any sufficiently large tÄ…0. We now prove that for any jâ€°n,
xjptqxnptqÂ´xnptq2ÃÃ‘
tÃ‘`8Â´8. (7.13)
Due to the ordering of the particles, it is enough to prove (7.13) for jâ€œnÂ´1. Fix
jâ€œnÂ´1andÎºÄ…0, and assume that
xnptqxjptqÄ›xnptq2Â´Îº
for some tÄ›0. Then, using the fact that
xnptqxjptqÄ›xnptqxkptq
for any kPrnÂ´2s, we get
PnjptqÄ›exjptqxnptq
exnptq2`pnÂ´1qexnptqxjptqÄ›Îµ,
where Îµâ€œ1
n`eÎº. We obtain
9xnptqÄPnnptqxnptq`PnjptqxjptqÄxnptq`Îµxjptq,
hence
d
dt`
xnptqpxnptqÂ´xjptqqË˜
â€œ9xnptqp2xnptqÂ´xjptqqÂ´xnptq9xjptq
Äpxnptq`Îµxjptqqp2xnptqÂ´xjptqqÂ´xnptq9xjptq
â€œÂ´Îµxjptq2`xnptqp2Îµxjptq`2xnptqÂ´xjptqÂ´9xjptqq
ÄÂ´Îµxjptq2`xnptqp2xnptqÂ´2x1ptqq, (7.14)
where in the last line we used the fact that 9xjptqÄ›x1ptq, which is due to (7.1), and
thatx1ptqÄƒxjptq, which is due to the ordering of the particles. Since xjptqÄÂ´ c2et
andx1ptqÄ›Â´ c1et, the upper bound in (7.14) is negative if tis large enough. We
therefore conclude that for any fixed Îº, if there exist unbounded times tsuch that
xnptqxjptqÄ›xnptq2Â´Îº, then xnptqxjptqÄ›xnptq2Â´Îºfor any tlarge enough. ButTHE EMERGENCE OF CLUSTERS IN SELF-ATTENTION DYNAMICS 27
this is excluded since xnptq Ä…0andxjptq Ã‘ Â´8 astÃ‘ `8. This concludes
the proof of (7.13), and the lemma follows by plugging this information into the
definition of Pnjptq. â–¡
Lemma 7.8. Ifi0Rt1, nuandxi0ptqremains uniformly bounded in t, then for any
jPrnÂ´1s, there exists some Î±jPr0,1ssuch that Pi0jptqÃ‘Î±jastÃ‘`8.
Proof.Assume that i0Rt1, nu. Then x1ptqÃ‘Â´8 andxnptqÃ‘`8 astÃ‘`8.
Also, xi0ptq Ã‘ 0due to (7.10). We write xi0ptq â€œyi0ptqeÂ´t. Since Î³nÄ…0and
Î³1Äƒ0, we notice that the function
g:Î¸ÃÃ‘Ã¿
iPrnszti0ueÎ³iÎ¸Î³i
1`Ã¿
iPrnszti0ueÎ³iÎ¸
takes valueÂ´8atÂ´8, and`8at`8, and has a positive derivative. Thus, it
takes the value 0exactly once, and we denote this point by Î¸0. We prove that
yi0ptqÃ‘Î¸0astÃ‘`8. We observe that
exi0ptq2â€œ1`op1q.
Using Lemma 7.5 we have
9yi0ptqâ€œet9xi0ptqÂ´yi0ptq
â€œpPi0i0ptqÂ´1qyi0ptq
`e2tÃ¿
jPrnszti0uÂ¨
ËšËšËeyi0ptqpÎ³j`op1qq
1`op1q`Ã¿
kPrnszti0ueyi0ptqpÎ³k`op1qqË›
â€¹â€¹â€špÎ³j`op1qq.
We recognize that the sum in the above expression is roughly equal to gpyi0q. If the
latter is not close to 0for large times, then 9yi0ptqnecessarily have a huge magnitude
due to the e2tfactor, leading to a contradiction. Fix ÎµÄ…0. Ifyi0ptqÄ…Î¸0`Îµfor
some large time tÄ…0, then, noticing that
|yi0ptq|â€œet|xi0ptq|â€œopetq, (7.15)
we get
9yi0ptqâ€œopetq`e2tÂ´
g`
yi0ptq`opyi0ptqqË˜Â¯
.
Butgpyi0ptqqÄ›Î´â€œÎ´pÎµq, and hence
9yi0psqÄ›Î´
2e2s
for any larger time sÄ›t, which contradicts (7.15). We get a similar contradiction
ifyi0ptqÄƒÎ¸0Â´Îµfor large enough t. This concludes the proof that yi0ptqÃ‘Î¸0. As
a consequence, xi0ptqxiptqÃ‘Î¸0Î³ifor any iâ€°i0, and we deduce Lemma 7.8. â–¡28 GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
7.3.Concluding the proof of Theorem 2.1.
Proof of Theorem 2.1. By Lemma 7.6, there is at most one index i0Prnsfor which
the particle xi0ptqremains bounded for any tÄ…0. In turn, for any iPrnszti0u, we
may invoke Lemma 7.4 which entails that Pijptqconverges to either Î´1jorÎ´njas
tÃ‘`8(withdoublyexponentialrate). Andbyorderingoftheparticles,forindices
i1Äi2different from i0, and Pi1jptqÃ‘Î´njthen necessarily Pi2jptqÃ‘Î´njas well.
Consequently, all but at most one row of Pptqconverge to either e1â€œp1,0, . . . , 0qor
enâ€œp0, . . . , 0,1qastÃ‘`8. For the i0-th row, we may invoke either Lemma 7.7 or
Lemma 7.8. The former applies if i0Pt1, nu, and entails that the i0-th row of Pptq
converges either to e1oren, while the latter applies if i0Rt1, nu, and entails that
thei0-th row of Pptqconverges to some vector Î±PRdwith non-negative entries.
Finally, since the i0-th row of Pptqhas entries which sum up to 1, then so does Î±.
These conclusions lead us to a final limit matrix PËšwhich has precisely the form
indicated in Fig. 2 (namely, PËšPPq, as desired. â–¡
Remark 7.9 (Higher dimensions) .The extension of Theorem 2.1 to dÄ›2is not
straightforward due to rare pathological situations. For example, suppose dâ€œ2,
nâ€œ2, and the initial configuration x1p0qâ€œp 1, Îµqandx2p0qâ€œp 1,Â´Îµq. One can
check that xiptqÃ‘p 1,0qastÃ‘`8, for iâ€œ1,2, which means that a single cluster
appears. However, the self-attention matrix converges toward the identity (which
has rank 2). Therefore, it is not true in full generality that the rank of the limiting
self-attention matrix is equal to the number of clusters as tÃ‘ `8, although we
believe that the result is true for almost all initial conditions.
8.Proofs of Theorems 3.1 and 8.5
In this section, we focus on proving the result in the case
Vâ€œId.
We also provide a full picture of the behavior of the dynamics in the case Vâ€œÂ´Id
in Section 8.2.
8.1.Clustering towards vertices of convex polytopes: Theorem 3.1. In
this section, we prove Theorem 8.1â€”namely, we show that particles tziptquiPrns
following the rescaled dynamics
9ziptqâ€œnÃ¿
jâ€œ1Ëœ
ee2txAziptq,Azjptqy
Å™n
kâ€œ1ee2txAziptq,AzkptqyÂ¸
pzjptqÂ´ziptqq (8.1)
converge, as tÃ‘8, toward points lying on the boundary of a particular convex
polytope. In (8.1) we made use of the shorthand notation
A:â€œ`
QJKË˜1
2. (8.2)
The precise statement is the following:
Theorem 8.1. Suppose Vâ€œIdandQJKÄ…0. Then, for any initial datum
tzip0quiPrnsÄ‚Rd, the solution to (8.1)is such that its convex hull conv`
tziptquiPrnsË˜
converges to some convex polytope KÄ‚RdastÃ‘ `8. Furthermore, let Vâ€œ
tv1, . . . , v mu(mÄn) denote the set of vertices of K, and consider
S:â€œ"
xPK:}Ax}2â€œmax
jPrmsxAx, Av jy*
,THE EMERGENCE OF CLUSTERS IN SELF-ATTENTION DYNAMICS 29
with Adefined in (8.2). Then Shas finite cardinality, and VÄ‚SÄ‚ BKYt0u.
Finally, for any iPrnsthere exists a point Â¯zPSsuch that ziptqÃ‘Â¯zastÃ‘`8.
In particular, ziptqconverges either to some point on the boundary of K, or to 0.
8.1.1.The convex hull is shrinking. To prove Theorem 8.1, we begin with the fol-
lowing illustrative result.
Proposition 8.2. Suppose Vâ€œIdandQJKÄ…0. Then the solution tzipÂ¨quiPrns
to(8.1)is such that tÃÃ‘convptziptquiPrnsqis non-increasing in the sense of set-
inclusion.
Proof of Proposition 8.2. FixtÄ…0and let HÄ‚Rdbe a closed half-space which
does not contain any of the points ziptq. We define the map
Î±:sÃÃ‘min
iPrnsdistpzipsq, Hq
forsâ©¾0. We claim that
Î±is non-decreasing on rt,`8q. (8.3)
Before proving (8.3), let us show how to conclude the proof of Proposition 8.2
using this claim. It follows from (8.3) that if convptziptquiPrnsqXHâ€œ H, then
convptzipt1quiPrnsqXHâ€œHfor any t1Ä›t. Writing the convex set convptziptquiPrnsq
as
convptziptquiPrnsqâ€œÄ
H1open half-space
convptziptquiPrnsqÄ‚H1H1â€œÄ
Hclosed half-space
convptziptquiPrnsqXHâ€œHRdzH,
we get that convptzipt1quiPrnsqÄ‚convptziptquiPrnsqfor any t1Ä›t.
We now turn to the proof of the claim (8.3). Denoting by nthe unit outer normal
toHand by projHthe orthogonal projection onto the closed set H, we have
distpx, Hqâ€œxxÂ´projHpxq,ny.
IftÃÃ‘xptqis a differentiable curve, writing 9xptqâ€œx9xptq,nyn`vptqwhere vptqPH
we haved
dtpprojHpxptqqqâ€œ vptq, whence
d
dtdistpxptq, Hqâ€œx9xptq,ny. (8.4)
LetTÄ…tdenote the infimum of the times for which one of the points ziptqlies
inH. Now fix sPrt, Tq, and denote by Mpsqthe set of indices iPrnssuch that
distpzipsq, Hqis minimal. For hÃ‘0, we have
Î±ps`hqâ€œ min
iPMpsqdistpzips`hq, Hq
â€œmin
iPMpsqË†
distpzipsq, Hq`hd
dtdistpzipsq, Hq`ophqË™
â€œÎ±psq`hË†
min
iPMpsqd
dtdistpzipsq, HqË™
`ophq.
Consequently,
dÎ±
dtpsqâ€œ min
iPMpsqd
dtdistpzipsq, Hq.30 GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
Moreover, for any iPMpsq, one has
d
dtdistpzipsq, Hq(8.4)â€œ x9zipsq,nyâ€œnÃ¿
jâ€œ1PijpsqxzjpsqÂ´zipsq,nyÄ›0,
where the last inequality comes from the fact that each term in the sum is non-
negative, since iPMpsq. Thisproves(8.3)(and, asabyproduct, that Tâ€œ`8).â–¡
The following fact immediately ensues.
Corollary 8.3. For any iPrnsandtÄ›0,ziptqPconvptzip0quiPrnsq. In particular,
zipÂ¨qis uniformly bounded in time.
8.1.2.Proof of Theorem 8.1.
Proof of Theorem 8.1. AsaconsequenceofProposition8.2,theset convptziptquiPrnsq
converges as tÃ‘ `8toward some convex polytope K. In the remainder of the
proof, we look to show that the particles ziptqcan in fact converge only to some
well-distinguished points lying on the boundary of this polytope.
Step 1. The candidate set of limit points. We denote by Vâ€œtv1, . . . , v muthe
set of vertices of K. Writing any xPKas a convex combination of these vertices:
xâ€œÅ™m
jâ€œ1Î±jvjfor some weights Î±jÄ›0withÅ™m
jâ€œ1Î±jâ€œ1, we gather that
}Ax}2â€œC
Ax,mÃ¿
jâ€œ1Î±jAvjG
â€œmÃ¿
jâ€œ1Î±jxAx, Av jyÄmax
jPrmsxAx, Av jy.(8.5)
LetSÄ‚Kdenote the set of points wPKsuch that
}Aw}2â€œmax
jPrmsxAw, Av jy. (8.6)
The following holdsâ€”we postpone the proof to after that of the theorem.
Claim 1. VÄ‚S. Moreover, if 0PK, then 0PS. Finally, SÄ‚BKYt0u, andS
has finite cardinality.
Now, for Î´Ä…0, we define the set SÎ´of points in Kat distance at most Î´fromS:
SÎ´:â€œtxPK: distpx,SqÄÎ´u.
SinceSis finite, there exists a sufficiently small Î´0Ä…0such that for any Î´ÄÎ´0,
the set SÎ´hasM:â€œ#Sconnected components, with any two of these connected
components being separated by a distance of at least Î´0. Our goal is to prove that
for any iPrns, and for sufficiently large t, the particle ziptqremains in one of these
connected components. In the sequel, we fix iPrns.
Step 2. ziptqmust grow if it is not already in SÎ´.We now prove that there
exists some Î³â€œÎ³pKqÄ…0(depending only on the geometry of K) such that for any
Î´Pp0, Î´0s, there exists TpÎ´qÄ…0such that if tÄ›TpÎ´qandziptqRSÎ´, then
d
dt}Aziptq}2Ä›Î³Î´. (8.7)THE EMERGENCE OF CLUSTERS IN SELF-ATTENTION DYNAMICS 31
K
S
SÎ´
Figure 9. An example configuration of the sets SandSÎ´inR2. The
setSconsists of all green nodes along the boundary of BK, while SÎ´is
the union of all yellow "hemispheres". The latter are pairwise disjoint
and are the connected components of SÎ´, which we denote by Ck, for
kPrMs.
To this end, we observe that
1
2d
dt}Aziptq}2â€œxA9ziptq, Aziptqy
â€œnÃ¿
jâ€œ1Ëœ
exAziptq,Azjptqye2t
Å™n
kâ€œ1exAziptq,Azkptqye2tÂ¸
xApzjptqÂ´ziptqq, Aziptqy
â€œnÃ¿
jâ€œ1Ëœ
eajptqe2t
Å™n
kâ€œ1eakptqe2tÂ¸
ajptq
loooooooooooooomoooooooooooooon
:â€œbjptq(8.8)
where we have set
ajptq:â€œxApzjptqÂ´ziptqq, Aziptqy.
(Toobtainthelastequalityin(8.8), divideboththenumeratorandthedenominator
bye}Aziptq}2e2t.) The following holds.
Claim 2. There exists some constant Î³1â€œÎ³1pKqÄ…0depending only on the geom-
etry of Ksuch that the following holds. Fix Î´Pp0, Î´0s. There exists T1pÎ´qÄ…0such
that if tÄ›T1pÎ´qandziptqRSÎ´, then there exists jPrnssuch that ajptqÄ›Î³1Î´.
We postpone the proof of this claim to after that of the theorem. We seek to
use this claim in obtaining a lower bound of bjptqfor any j, whenever Î´is small
enough and tis large enough. Since by Corollary 8.3, for any jPrns,tÃÃ‘zjptqis
uniformly bounded on r0,`8q, we gather that ajpÂ¨qPL8p0,`8q. So, we may set
Îº:â€œmax
jPrnssup
tÄ›0|ajptq|.
LettÄ›0be fixed. We define
Bptq:â€œtjPrns:ajptqÄ›0u.32 GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
We pick an index j0ptqmaximizing ajptq, namely
j0ptqPargmaxjPrnsajptq.
Observe that j0ptqPBptqsince aj0ptqptqÄ›aiptqâ€œ0. Clearly
bjptqÄ›0for all jPBptq. (8.9)
In fact, we also have
bj0ptqptqÄ›aj0ptqptq
n. (8.10)
Now suppose that jRBptq; since ajptqÄ›Â´ Îº, and
eajptqe2t
nÃ¿
kâ€œ1eakptqe2tÄ1
nÃ¿
kâ€œ1eakptqe2tÄeÂ´aj0ptqe2t,
we gather that
bjptqÄ›Â´ ÎºeÂ´aj0ptqptqe2tfor all jPrnszBptq. (8.11)
Using (8.9), (8.10) and (8.11) in (8.8), we find
1
2d
dt}Aziptq}2â©¾aj0ptqptq
nÂ´ÎºneÂ´aj0ptqptqe2t.
TheaboveinequalityalongwithClaim2leadustodeducethatthereexists TpÎ´qÄ…0
(possibly larger than T1pÎ´q) such that (8.7) holds whenever tâ©¾TpÎ´q, with Î³â€œÎ³1
2n,
as desired.
Step 3. ziptqcannot circulate indefinitely between the connected com-
ponents of SÎ´.Since ziPL8pr0,`8qqby Corollary 8.3, from (8.1) we gather
that9ziPL8pr0,`8qqas well. And since any two connected components of SÎ´0are
separated by a distance at least Î´0, we deduce that it takes a time at least
T0:â€œÎ´0
}9zi}L8pr0,`8qq
forzito go from one connected component of SÎ´0to another one. Fix Î´Pp0, Î´0q
such that
Î´ÄƒT0Î³Î´0
8R}A}op, (8.12)
where R:â€œmax jPrns}zj}L8pr0,`8qq. Denote by
C1, . . . ,CM
the connected components of SÎ´, each of which being the intersection of Kwith a
Euclidean ball of radius Î´centered at some point of S(see Fig. 9). For any kPrMs,
sup
xPCk}Ax}2Â´inf
xPCk}Ax}2Ä4R}A}opÎ´. (8.13)
We introduce the following binary relation on rMs:
kÄ…â„“Ã°Ã± inf
xPCk}Ax}2Ä…sup
xPCâ„“}Ax}2,
whichistransitive. Theunderlyingideaisthefollowing: if tissufficientlylarge, and
ifzistartsfromsomeconnectedcomponent Câ„“,thentheonlyconnectedcomponents
Ckwhich ziis able to visit later on are those for which kÄ…â„“. This travel of ziTHE EMERGENCE OF CLUSTERS IN SELF-ATTENTION DYNAMICS 33
has to stop after some time since rMsis finite, Ä…is transitive, and for any â„“, the
relation â„“Ä…â„“does not hold.
LetTâ€œTpÎ´qbe as in Step 2. Suppose that t2Ä…t1Ä›Tandk1, k2PrMsare
distinct and such that zipt1qPCk1,zipt2qPCk2andziptqRSÎ´for any tPpt1, t2q.
Per Step 2 (more specifically, (8.7)),
}Azipt2q}2Ä›}Azipt1q}2`T0Î³Î´0.
Therefore using (8.13) twice and since Î´is chosen as in (8.12), we gather that
inf
xPCk2}Ax}2Ä›}Azipt2q}2Â´4R}A}opÎ´Ä›}Azipt1q}2`T0Î³Î´0Â´4R}A}opÎ´
Ä›inf
xPCk1}Ax}2`T0Î³Î´0Â´4R}A}opÎ´
Ä›sup
xPCk1}Ax}2`T0Î³Î´0Â´8R}A}opÎ´
Ä…sup
xPCk1}Ax}2.(8.14)
Whence k2Ä…k1. We therefore deduce that there exist some T1Ä›TandkPrMs
such that ziptqRSÎ´zCkfor any tÄ›T1.
Step 4. Conclusion. To conclude, it remains to be shown that ziptqstays in Ck
fortlarge enough. For this, in addition to (8.12), we impose
Î´1
4ÄƒÎ³T0
8R}A}opÎ´0. (8.15)
ForrÄ…0, we denote by Cr
kthe intersection of Kwith the closed Euclidean ball of
radius Î´rhaving the same center as Ck. In particular, C1
kâ€œCk. If, after time T1,
zitravels from Ckto the complement of C1
4
k, it spends a time at least
pÎ´1
4Â´Î´1
2q
}9zi}L8pr0,`8qq
inC1
4
kzC1
2
k. Per Step 2 (used with Î´1
2),}Azi}2has to increase by at least
Î³Î´1
2Â´
Î´1
4Â´Î´Â¯
}9zi}L8pr0,`8qqÄ›Î³Î´3
4
2}9zi}L8pr0,`8qqÄ…4R}A}opÎ´ (8.16)
during this travel (the last inequality in (8.16) stems from (8.15)). This implies
thatzicannot reenter Ckafter having reached the boundary of C1
4
k, due to (8.13).
Thus ziptqRSÎ´for any sufficiently large t, which is impossible due to Step 2 and
the uniform boundedness of tÃÃ‘}Aziptq}. Hence, for sufficiently large t,ziptqPC1
4
k.
Since Î´maybechosenarbitrarilysmall,thisconcludestheproofofTheorem8.1. â–¡
8.1.3.Proving Claims 1 and 2. We now address the proofs of the two claims which
were instrumental in what precedes (along with a sketch of the proof of VÄ‚S, as
implied).
Proof of Claim 1. The fact that 0PSif0PKis immediate. We now show that S
is finite and SÄ‚BKYt0u. Let wPSzt0u. As
wâ€œmÃ¿
jâ€œ1Î±jvj34 GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
for some Î±jâ©¾0withÅ™m
jâ€œ1Î±jâ€œ1, and since (8.6) holds by definition, it follows
thatÎ±jâ€œ0for any jnot attaining the maximum in (8.6). Let IÄ‚rmsdenote the
set of all such indices. We have
wâ€œÃ¿
jPIÎ±jvj
with}Aw}2â€œxAw, Av jyfor any jPI. Whence wis the orthogonal projection onto
spantvjujPIwith respect to xAÂ¨, AÂ¨y. This yields SÄ‚BK. Moreover, since for each
subset IÄ‚rmsthere exists a unique such projection w,Sis finite. â–¡
Sketch of proof of VÄ‚S.We notice that for any iPrnsand for tlarge enough, we
have
9ziptqâ€œnÃ¿
jâ€œ1Ëœ
ee2txAziptq,Azjptqy
Å™n
kâ€œ1ee2txAziptq,AzkptqyÂ¸
pzjptqÂ´ziptqq (8.17)
Â«Ã¿
jPMiptqËœ
ee2txAziptq,Azjptqy
Å™n
kâ€œ1ee2txAziptq,AzkptqyÂ¸
pzjptqÂ´ziptqq,(8.18)
where Miptqis the subset of rnscontaining all indices jsuch that
max
kPrnsxAziptq, AzkptqyÂ´x Aziptq, AzjptqyÄeÂ´t
(all other terms in the sum (8.17) are negligible). Due to the convergence of
convptziptquiPrnsqtoward K, we also know that for tlarge enough,
â€šall the points ziptqare contained in a small neighborhood of K,
â€šnear any element of V, there exists some particle ziptq.
Assume, for the sake of contradiction, that there exists a vertex vjPVsuch that
vjRS. Set C:â€œconvptviuiPrmsztjuq. In particular, distpvj,Cq Ä…0since vjis a
vertex of K. IfIÄ‚ rnsdenotes the set of indices isuch that ziptqlies near vj,
then MiptqXIâ€œ Hfor any iPI, since vjRS. For iPI, using (8.18), we find
that distpziptq,Cqdecays as tÃ‘`8as long as iRMiptqâ€”indeed, (8.18) implies
that ziptqis attracted by C. This implies that vjRconvptzkpt1qukPrnsqfort1large
enough. This isa contradictionsince KÄ‚convptzkptqukPrnsqforany tÄ›0according
to Proposition 8.2. â–¡
Proof of Claim 2. To simplify the notation, we only prove Claim 2 when Aâ€œId.
Assume that tÄ›0and that ziptqRSÎ´.
First case. Firstly, we prove the claim in the case where ziptqRSÎ´0. For this, we
notice that the function
f:xÃÃ‘max
jPrnsxvj, xyÂ´}x}2
is continuous, and by definition of S,fis strictly positive on the compact set
KzIntpSÎ´0q(the complement in Kof the interior of SÎ´0). Hence fpxqÄ›c1in this
set for some constant c1Ä…0. Setting
KÎµ:â€œtxPRd: distpx,KqÄÎµu,
by continuity we find that fpxqÄ›c1{2forxPKÎµzIntpSÎ´0qand for sufficiently small
ÎµÄ…0(fixed in the sequel). For sufficiently large t, we have ziptq PKÎµfor anyTHE EMERGENCE OF CLUSTERS IN SELF-ATTENTION DYNAMICS 35
iPrns, thus
max
jPrnsxziptq, zjptqÂ´ziptqyÄ› max
jPrmsxziptq, vjÂ´ziptqyÄ›c1
2.
Since c1is independent of Î´, we deduce the claim in this case (notice that it suffices
to prove the claim for sufficiently small Î´).
Second case. Secondly, we prove the claim when ziptqPSÎ´0zSÎ´. The proof mainly
relies on the following result:
Lemma 8.4. For any wPS, there exists Î²Ä…0such that if6xPKXBpw, Î´0q,
then
max
jPrmsxx, vjÂ´xyÄ›Î²}xÂ´w}. (8.19)
We postpone the proof of Lemma 8.4 and show how to conclude the proof of
Claim 2. Fix Î´Ä…0. We set
Î·:â€œÎ²Î´
6R
where
R:â€œmax
jPrns}zj}L8pRq.
Since convptzjptqujPrnsqconverges to KastÃ‘`8, there exists TpÎ´qÄ…0such that
for any tÄ›TpÎ´q, ifziptqPBpw, Î´0qzBpw, Î´qfor some wPS, then
}ziptqÂ´x}ÄÎ·
for some xPKXpBpw, Î´0qzBpw, Î´qq. Therefore, using Lemma 8.4,
max
jPrmsxziptq, vjÂ´ziptqyÄ› max
jPrmsxx, vjÂ´xyÂ´3RÎ·
Ä›Î²Î´Â´3RÎ·
â€œÎ²
2Î´.
To summarize, we have found that for any Î´Ä…0there exists TpÎ´qÄ…0such that if
tÄ›TpÎ´qandziptqPSÎ´0zSÎ´, then
max
jPrmsxziptq, vjÂ´ziptqyÄ›Î²
2Î´. (8.20)
Combining (8.20) with
max
jPrnsxziptq, zjptqÂ´ziptqyÄ› max
jPrmsxziptq, vjÂ´ziptqy
concludes the proof of Claim 2 in this second case. â–¡
Proof of Lemma 8.4. Let us first address the case where wâ€œ0. Writing any xP
Kzt0uas a convex combination of the vertices: xâ€œÅ™m
jâ€œ1Î±jvj, we find
0â€œC
x,mÃ¿
jâ€œ1Î±jpvjÂ´xqG
â€œmÃ¿
jâ€œ1Î±jxx, vjÂ´xy. (8.21)
6Here, Bpy, rqdenotes the closed ball with center yPRdand radius rÄ…0.36 GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
We can exclude having xx, vjÂ´xy â€œ0for all jP rms, as this would necessarily
imply that}x}2â€œ2Å™m
jâ€œ1Î±jxx, vjÂ´xyâ€œ0. We deduce from (8.21) that
max
jPrmsxx, vjÂ´xyÄ…0
for any xPKzt0u. Hence, it is sufficient to prove (8.19) for }x}small enough. We
notice that for any xPKzt0uwritten as above,
}x}2â€œmÃ¿
jâ€œ1Î±jxvj, xy.
Hence xÃÃ‘max jPrmsxvj, xyis positive for xPKzt0u. Since this function is contin-
uous and homogeneous in x, we deduce the existence of Î²Ä…0such that
max
jPrmsxvj, xyÄ›2Î²}x}
for any xPK. For xPKwith}x}sufficiently small, we obtain (8.19).
We now assume that wPSzt0u. We set
Iw:â€œâ£
jPrns:}w}2â€œxw, vjy(
and
A:â€œspan`â£
vjÂ´w:jPIw(Ë˜
,
which is orthogonal to w. We also introduce
R:â€œ`
Rwâ€˜AË˜K,
and we denote by Ï€Rthe orthogonal projection on R. We claim that there exists
some ÏÄ…0such that for any jPrms, we have
xwÂ´vj, wyÄ›Ï}Ï€Rvj}.
This follows from the observation that rmsis finite, and that }Ï€Rvj}Ä…0implies
xwÂ´vj, wyÄ…0. Therefore, for any xPK, writing xas a convex combination of
the vertices, namely xâ€œÅ™m
jâ€œ1Î±jvj, we find that
Ï}Ï€Rx}ÄmÃ¿
jâ€œ1Î±j}Ï€Rvj}ÄmÃ¿
jâ€œ1Î±jxwÂ´vj, wyâ€œxwÂ´x, wy.(8.22)
FixxPKXBpw, Î´0q. We write xâ€œw`Î´1uwith 0ÄÎ´1ÄÎ´0and}u}â€œ1. Then
we have the orthogonal decomposition
uâ€œbw`a`r (8.23)
where aPA,rPRandbPR. Since ais a convex combination of the form
aâ€œÃ¿
jPIwÎ²jpvjÂ´wq,
we have
}a}2â€œÃ¿
jPIwÎ²jxvjÂ´w, ay,
whence
max
jPIwxa, vjÂ´wyÄ›}a}2.THE EMERGENCE OF CLUSTERS IN SELF-ATTENTION DYNAMICS 37
We deduce that
max
jPIwxx, vjÂ´xyâ€œmax
jPIwxw`Î´1u,pvjÂ´wqÂ´Î´1uy
â€œÂ´Î´1b}w}2Â´Î´12`Î´1max
jPIwxa, vjÂ´wy
Ä›Â´Î´1b}w}2Â´Î´12`Î´1}a}2. (8.24)
Notice that bÄ0by combining (8.22) and (8.23). Since }u}â€œ1and using (8.22)
we have
1â€œb2`}a}2`}r}2Ä}a}2`Îºb2ÄÎºp}a}2`b2q
where Îº:â€œ1`ÏÂ´2}w}4. Wededucethateither }a}2Ä›p2ÎºqÂ´1orÂ´bâ€œ|b|Ä›p2ÎºqÂ´1
2.
Plugging this knowledge in (8.24) and using the fact that }w}Ä…0, we finally deduce
the existence of an Î±Ä…0(independent of Î´Ä…0andxPKXBpw, Î´0q) such that
max
jPrmsxx, vjÂ´xyÄ›Î±Î´1Â´Î´12â€œÎ±}xÂ´w}Â´}xÂ´w}2.
This proves (8.19) when }xÂ´w}ÄÎ±{2.
It thus remains to show that (8.19) holds for all xPKXpBpw, Î´0qzBpw,Î±
2qq. To
this end, we notice that xÃÃ‘max jPrmsxx, vjÂ´xyis continuous in the connected set
KXpBpw, Î´0qzBpw,Î±
2qq, non-negative according to (8.5), and it is nowhere 0(by
definition of S). Therefore, it is strictly positive, and denote by Î±1Ä…0some lower
bound. Then for xPKXpBpw, Î´0qzBpw,Î±
2qq, we have
max
jPrmsxx, vjÂ´xyÄ›Î±1Ä›Î±1
Î´0}xÂ´w}.
This concludes the proof of Lemma 8.4. â–¡
8.2.A cluster at the origin. We complete this section by addressing the case
Vâ€œÂ´Id, for which the convergence of the solutions of (1.1) is the simplest, since
a unique cluster forms at the origin. We also suppose that QJKâ€œId: in other
words, we consider the dynamics
9xiptqâ€œÂ´nÃ¿
jâ€œ1Ë†exxiptq,xjptqy
Å™n
kâ€œ1exxiptq,xkptqyË™
xjptq, tPr0,`8q,(8.25)
with a prescribed initial condition txip0quiPrnsÄ‚Rd.
Theorem 8.5 (Convergence toward the origin) .Suppose Vâ€œÂ´IdandQJKâ€œId.
Then, for any initial sequence of tokens txip0quiPrnsÄ‚Rd, and for any iPrns, we
have}xiptq}Ã‘ 0astÃ‘`8.
Remark 8.6. In the setting of Theorem 8.5, the self-attention matrix Pptqdefined
in(1.2)converges, as tÃ‘`8, to the nË†nmatrix with all entries equal to 1{n.
8.2.1.Proof of Theorem 8.5. We begin by showing that for any iPrns, the solu-
tion to (8.25) is uniformly bounded for all tÄ…0. In the sequel, we fix an initial
configuration txip0quiPrnsÄ‚Rd.
Lemma 8.7. The trajectories of (8.25)are uniformly bounded in timeâ€”namely,
there exists RÄ…0(depending solely on nand the initial configuration) such that
the solution xipÂ¨qto(8.25)satisfies}xiptq}â©½Rfor any iPrnsandtâ©¾0.38 GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
Proof of Lemma 8.7. We fix iPrns. For tÄ›0, we denote by Diptqthe set of points
xkptqsuch thatxxiptq, xkptqyÄ›0. We also set
Siptq:â€œÃ¿
kPDiptqexxiptq,xkptqyxxiptq, xkptqy,
and
Riptq:â€œnÃ¿
kâ€œ1exxiptq,xkptqy.
Since 1`xÄexwhence eÂ´xxÄ1, we deduce that
1
2d
dt}xiptq}2â€œÂ´nÃ¿
kâ€œ1exxiptq,xkptqyxxiptq, xkptqy
RiptqÄÂ´Siptq`n
Riptq.
Now since 1Â´xÄeÂ´xwhence exÄ1`exx, we find that Riptq Än`Siptq.
Consequently, if we assume that }xiptq}2Ä›2nthen SiptqÄ›2n, and therefore
1
2d
dt}xiptq}2ÄÂ´Siptq`n
n`SiptqÄÂ´1.
This shows that }xiptq}Ä maxt}xip0q},?
2nufor any tÄ›0, which concludes the
proof. â–¡
By virtue of Lemma 7.1, we are able to characterize the stationary configurations
for the dynamics (8.25)â€”namely, the set of points pÂ¯x1, . . . , Â¯xnqPpRdqnsatisfying
nÃ¿
jâ€œ1Ë†exÂ¯xi,Â¯xjy
Å™n
kâ€œ1exÂ¯xi,Â¯xkyË™
Â¯xjâ€œ0
for all iPrns.
Lemma 8.8. The only stationary configuration for the dynamics (8.25)isÂ¯x1â€œ
. . .â€œÂ¯xnâ€œ0.
Proof.Assume that pÂ¯x1, . . . , Â¯xnqPpRdqnis a stationary configuration for the dy-
namics (8.25). We consider f:RdÃ‘Rdefined as
f:xÃÃ‘logËœnÃ¿
jâ€œ1exx,Â¯xjyÂ¸
.
Per Lemma 7.1, fis convex, whence
fpxqÄ›fpÂ¯xiq`xâˆ‡fpÂ¯xiq, xÂ´Â¯xiy
forxPRdandiPrns. Since âˆ‡fpÂ¯xiqâ€œ0for any iPrns, we gather that fpxqÄ›
fpÂ¯xiq, whence Â¯xiis a global minimizer of ffor any iP rns. By convexity, fis
constant on convptÂ¯xiuiPrnsq. Since fis analytic on the affine space Espanned by
the points Â¯xi,iPrns, it is then constant on Eas well. Now assume that not all
of the points Â¯xiare equal, and pick an index i0P rnssuch that Â¯xi0is not equal
to the projection of the origin onto E. Then there exists some j0Prnssuch that
xÂ¯xi0Â´Â¯xj0,Â¯xi0yâ€°0. For any sPR, we set Ps:â€œÂ¯xj0`spÂ¯xi0Â´Â¯xj0qPE, and we
notice that fpPsq Ä› x Ps,Â¯xi0y, where the lower bound tends to `8either when
sÃ‘`8or when sÃ‘Â´8. This contradicts the fact that fis constant on E. We
conclude that the Â¯xiare all equal for iPrns. The only value they can then take is
necessarily 0. â–¡THE EMERGENCE OF CLUSTERS IN SELF-ATTENTION DYNAMICS 39
Lemma 8.9. The trajectories of (8.25)satisfyÅ¼`8
0}9xiptq}2dtÄƒ `8for any
iPrns.
Proof.The function
L:tÃÃ‘nÃ¿
iâ€œ1nÃ¿
jâ€œ1exxiptq,xjptqy
is non-increasing, as demonstrated by the following simple computation:
dLptq
dtâ€œ2nÃ¿
iâ€œ1nÃ¿
jâ€œ1exxiptq,xjptqyx9xiptq, xjptqyâ€œ2nÃ¿
iâ€œ1C
9xiptq,nÃ¿
jâ€œ1exxiptq,xjptqyxjptqG
â€œÂ´2nÃ¿
iâ€œ1nÃ¿
jâ€œ1exxiptq,xjptqy}9xiptq}2.
Being non-negative, Lptqthus converges as tÃ‘`8. Sincexxiptq, xjptqyÄ› Rfor
some (possibly negative) RPRby virtue of Lemma 8.7, we deduce that
Å¼`8
0}9xiptq}2dtÄeÂ´RÅ¼`8
0nÃ¿
iâ€œ1nÃ¿
jâ€œ1exxiptq,xjptqy}9xiptq}2dtâ€œeÂ´RpLp0qÂ´lim
tÃ‘`8Lptqq,
which concludes the proof. â–¡
We are now able to conclude the proof of Theorem 8.5.
Proof of Theorem 8.5. We set Xptq:â€œpx1ptq, . . . , x nptqqPpRdqn. IfXptqdoes not
converge to 0, the compactness provided by Lemma 8.7 implies that there is a
sequencettku`8
kâ€œ1with tkÃ‘ `8, and XËšâ€œ pxËš
1, . . . , xËš
nq P pRdqnzt0u, such that
Xptkq Ã‘XËšaskÃ‘ `8. To conclude the proof, it suffices to show that XËšis
a stationary configuration of the dynamics: this directly leads to a contradiction
per Lemma 8.8. Therefore, assume that XËšis not a stationary configuration of
the dynamics. We denote by XËšptq â€œ p xËš
1ptq, . . . , xËš
nptqqthe solution of (8.25)
with initial condition XËš. Then, there exists iP rnssuch that 9xËš
ip0q â€°0. We
setÎµâ€œ }9xËš
ip0q}. We select T0Ä…0(possibly small) such that }9xËš
iptq} Ä› Îµ{2for
tPr0, T0s. It follows from (6.9) (which is verified according to Corollary 6.6) that
for any Î´Ä…0there exists k0PNsuch that}Xptk`tqÂ´XËšptq}ÄÎ´for any tPr0, T0s
and any kÄ›k0. By (6.5) (which is verified according to Corollary 6.6), we obtain
that}9xiptk`tqÂ´9x0
iptq} Ä CÎ´fortP r0, T0sand any kÄ›k0. Choosing Î´Ä…0
sufficiently small, we obtain that }9xiptk`tq}Ä› Îµ{4fortPr0, T0sand any kÄ›k0.
This contradicts Lemma 8.9. â–¡
9.Proof of Theorem 4.2
To ensure clarity, we present the proof of Theorem 4.2 under the assumption that
Vis diagonalizable. However, this assumption is not necessary. In Remark 9.5, we
explain how the proof can be modified to accommodate for non-diagonalizable V.
Let us therefore assume that Vis diagonalizable. Let pÏ†1, . . . , Ï† dqbe an or-
thonormal basis of eigenvectors associated to eigenvalues pÎ»1, . . . , Î» dq, ordered in
a decreasing manner with respect to their modulus: |Î»1| Ä›. . .Ä› |Î»d|. (Starting
from this point and throughout, we use the symbol Î»exclusively to denote the
eigenvalues of V.) Except for Î»1PR, all the other eigenvalues (and eigenvectors)
may be complex. We denote by pÏ†Ëš
1, . . . , Ï†Ëš
dqthe dual basis of pÏ†1, . . . , Ï† dq.40 GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
9.1.Some monotonicity properties and bounds. To start, we present some
general facts that are prove useful in all subsequent sub-cases.
Lemma 9.1. Suppose kPrdsis such that Î»kÄ›0. Then tÃÃ‘max jPrnsÏ†Ëš
kpzjptqq
is a non-increasing and bounded function, and tÃÃ‘minjPrnsÏ†Ëš
kpzjptqqis a non-
decreasing and bounded function. In particular, tÃÃ‘Ï†Ëš
kpziptqqis uniformly bounded
as a function on r0,`8qfor any iPrns.
Proof.For any kPrdsand any tÄ›0, set
Î±kptqâ€œmin
jPrnsÏ†Ëš
kpzjptqq, Î² kptqâ€œmax
jPrnsÏ†Ëš
kpzjptqq.
LetiPrnsbe an index such that Î±kptqâ€œÏ†Ëš
kpziptqq. Then we have
d
dtÏ†Ëš
kpziptqqâ€œnÃ¿
jâ€œ1PijptqÏ†Ëš
kpVpzjptqÂ´ziptqqq
â€œÎ»knÃ¿
jâ€œ1PijptqpÏ†Ëš
kpzjptqqÂ´Ï†Ëš
kpziptqqqÄ› 0
where the last inequality stems from the fact that Î»kÄ›0and the choice of index
i. This proves that Î±kpÂ¨qis non-decreasing, as desired. Arguing similarly, one finds
that Î²kpÂ¨qis non-increasing. As a consequence, Î±kp0qÄÎ±kptqÄÎ²kptqÄÎ²kp0qfor
anytÄ›0, which shows that Î±kpÂ¨qandÎ²kpÂ¨qare bounded. â–¡
Corollary9.2. IfVonlyhas realnon-negative eigenvalues, then zipÂ¨qPL8pr0,`8qq.
Lemma 9.3. FixkPrdsandiPrns. Then there exists a constant CÄ…0such that
Ë‡Ë‡Ï†Ëš
k`
etVziptqË˜Ë‡Ë‡ÄCe|Î»k|t
holds for all tâ©¾0.
Proof.We naturally make use of the equation for xiptq:â€œetVziptq. Fix tâ©¾0. We
have
d
dt|Ï†Ëš
kpxiptqq|2â€œ2Â¨ReË†
Ï†Ëš
kpxiptqqd
dtÏ†Ëš
kpxiptqqË™
â€œ2Â¨ReËœnÃ¿
jâ€œ1PijptqÏ†Ëš
kpV xjptqqÏ†Ëš
kpxiptqqÂ¸
â€œ2Â¨ReËœnÃ¿
jâ€œ1PijptqÎ»kÏ†Ëš
kpxjptqqÏ†Ëš
kpxiptqqÂ¸
Ä2|Î»k|max
jPrns|Ï†Ëš
kpxjptqq|2.
Choosing iPrnsrunning over the set of indices such that |Ï†Ëš
kpxiptqq|is maximal,
we obtain
d
dtmax
jPrns|Ï†Ëš
kpxjptqq|2Ä2|Î»k|max
jPrns|Ï†Ëš
kpxjptqq|2.
We conclude the proof by applying GrÃ¶nwallâ€™s lemma. â–¡THE EMERGENCE OF CLUSTERS IN SELF-ATTENTION DYNAMICS 41
9.2.Proof of Theorem 4.2. We now prove Theorem 4.2. We again recall that
Î»1is simple and positive, and the eigenvalues of Vare ordered in decreasing order
of modulus: Î»1Ä…|Î»2|Ä›. . .Ä›|Î»d|.
Proof of Theorem 4.2. We look to prove that for any iPrns, the component of ziptq
along the principal eigenvector Ï†1, i.e. Ï†Ëš
1pziptqq, converges as tÃ‘`8. We also
show that there exists a set of at most 3real numbers (depending on the initial
datumpz1p0q, . . . , z np0qq) such that for any iPrnsthe limit of Ï†Ëš
1pziptqqbelongs to
this set. Theorem 4.2 directly follows from these facts.
LetiPrnsbe fixed. Recall from Lemma 9.1 that Ï†Ëš
1pziptqqis uniformly bounded
for any tPr0,`8q. We set
a:â€œlim
tÃ‘`8min
jPrnsÏ†Ëš
1pzjptqq, b :â€œlim
tÃ‘`8max
jPrnsÏ†Ëš
1pzjptqq.(9.1)
(Note that by Lemma 9.1, aÄ›minjPrnsÏ†Ëš
1pzjp0qqandbÄmax jPrnsÏ†Ëš
1pzjp0qq.) For
cPt0, a, bu, we define the candidate limiting hyperplanes for ziptq:
Hc:â€œtxPRd:Ï†Ëš
1pxqâ€œcu.
We show that ziptqconverges either to H0, toHaor to Hb. Ifaâ€œbâ€œ0, then
according to (9.1) all particles converge to H0and there is nothing left to prove.
We now distinguish two scenarios:
(i) either for any ÎµÄ…0,|Ï†Ëš
1pziptqq|Ä Îµfortlarge enoughâ€”in which case, we
deduce that ziptqconverges toward H0astÃ‘`8â€”,
(ii) or|Ï†Ëš
1pziptkqq|Ä… Îµ0for some Îµ0Ä…0and for some sequence of positive times
ttku`8
kâ€œ1with tkÃ‘`8.
Since case (i) is straightforward, let us handle case (ii). Without loss of generality,
we can extract a subsequence of times (which we do not relabel, for simplicity of
notation) along which
Ï†Ëš
1pziptkqqÄ…Îµ0. (9.2)
LetÎµPp0, Îµ0sbe fixed and to be chosen later. We set
wjptq:â€œ@
QetVziptq, KetVzjptqD
,
so that
1
Î»1d
dtÏ†Ëš
1pziptqqâ€œnÃ¿
jâ€œ1ewjptq
Å™n
kâ€œ1ewkptqpÏ†Ëš
1pzjptqqÂ´Ï†Ëš
1pziptqqq.(9.3)
We look to obtain a lower bound for the right-hand side in the above identity. Let
us use the shorthand
ckâ„“:â€œxQÏ†k, KÏ† â„“y
fork, â„“Prds. By assumption, c11Ä…0. We have Ï†Ëš
kpetVziptqqâ€œ etÎ»kÏ†Ëš
kpziptqqand
the following spectral expansion holds:
etVziptqâ€œdÃ¿
kâ€œ1etÎ»kÏ†Ëš
kpziptqqÏ†k.42 GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
Using this fact, as well as Lemma 9.3, we gather that
Ë‡Ë‡Ë‡wjptqÂ´c11e2Î»1tÏ†Ëš
1pziptqqÏ†Ëš
1pzjptqqË‡Ë‡Ë‡â€œË‡Ë‡Ë‡Ë‡Ë‡Ë‡Ã¿
pk,â„“qâ€°p1,1qckâ„“Ï†Ëš
kpetVziptqqÏ†Ëš
â„“`
etVzjptqË˜Ë‡Ë‡Ë‡Ë‡Ë‡Ë‡
ÄÃ¿
pk,â„“qâ€°p1,1q|ckâ„“|Ë‡Ë‡Ï†Ëš
k`
etVziptqË˜Ë‡Ë‡Ë‡Ë‡Ï†Ëš
â„“`
etVzjptqË˜Ë‡Ë‡
ÄC2}QJK}opÃ¿
pk,â„“qâ€°p1,1qep|Î»k|`|Î»â„“|qt
ÄC2}QJK}oppdÂ´1q2loooooooooooomoooooooooooon
â€œ:C1epÎ»1`|Î»2|qt(9.4)
holds for all tÄ›0andjPrns. Now since Î»1Ä…0, Lemma 9.1 implies that for any
tÄ›0there exists an index i0ptqPrnssuch that
Ï†Ëš
1pzi0ptqptqqÄ›b. (9.5)
With j0ptqPargmaxjPrnswjptq, using (9.4) and (9.5) we see that
wj0ptqptqÄ›wi0ptqptqÄ›c11Ï†Ëš
1pziptqqbe2Î»1tÂ´C1epÎ»1`|Î»2|qt. (9.6)
Now for any twithin the sequence ttku`8
kâ€œ1, combining the first inequality in (9.6)
with the fact that c11Ä…0, (9.2) and (9.4), we deduce that
Ï†Ëš
1pzj0ptqptqqÂ´Ï†Ëš
1pzi0ptqptqqÄ›Â´2C1
c11ÎµeÂ´pÎ»1Â´|Î»2|qt. (9.7)
AsÎ»1Ä… |Î»2|, for tlarge enough, we find that we can lower bound the above
expression by Â´Îµ
4. We now define the set of indices
Nptq:â€œtjPrns:Ï†Ëš
1pziptqqÂ´Ï†Ëš
1pzjptqqÄ›0u.
Take twithin the sequence ttku`8
kâ€œ1such that Ï†Ëš
1pziptqqÄbÂ´Îµand large enough so
that (9.7) is lower bounded by Â´Îµ
4(if such a tdoes not exist, we immediately con-
clude that Ï†Ëš
1pziptqqÃ‘ bastÃ‘`8). Using (9.5) and the subsequent derivations,
we deduce that
Ï†Ëš
1pzj0ptqptqqÂ´Ï†Ëš
1pziptqqÄ›3Îµ
4,
and since Ï†Ëš
1pzjptqqÂ´Ï†Ëš
1pziptqqÄ›0forjRNptq, we expand in (9.3) to get
1
Î»1d
dtÏ†Ëš
1pziptqqÄ›ewj0ptqptq
Å™n
kâ€œ1ewkptq3Îµ
4`Ã¿
jPNptqewjptq
Å™n
kâ€œ1ewkptqpÏ†Ëš
1pzjptqqÂ´Ï†Ëš
1pziptqqq.
(9.8)
On another hand, for jPNptq, we may use (9.4) to find
wjptqÄc11Ï†Ëš
1pziptqq2e2Î»1t`C1epÎ»1`|Î»2|qt. (9.9)
We set
C0:â€œmax
jPrnsÏ†Ëš
1pzjp0qqÂ´min
jPrnsÏ†Ëš
1pzjp0qq.THE EMERGENCE OF CLUSTERS IN SELF-ATTENTION DYNAMICS 43
Using the monotonicity properties from Lemma 9.1, as well as (9.9) in (9.8), we
obtain
1
Î»1d
dtÏ†Ëš
1pziptqqÄ›3Îµ
4nÂ´C0nexpÂ´
c11Ï†Ëš
1pziptqq2e2Î»1t`C1epÎ»1`|Î»2|qtÂ¯
expÂ´
c11Ï†Ëš
1pziptqqbe2Î»1tÂ´C1epÎ»1`|Î»2|qtÂ¯.
Given our choice of t, we have Ï†Ëš
1pziptqq2Â´bÏ†Ëš
1pziptqqÄÂ´ ÎµpbÂ´Îµq, so, we conclude
from the inequality just above that
1
Î»1d
dtÏ†Ëš
1pziptqqÄ›3Îµ
4nÂ´C0nexpÂ´
Â´c11ÎµpbÂ´Îµqe2Î»1t`2C1epÎ»1`|Î»2|qtÂ¯
.(9.10)
Since Î»1Ä…|Î»2|, it follows from (9.10) that there exists TÄ…0such that for any t
within the sequence ttku`8
kâ€œ1for which tÄ›TandÏ†Ëš
1pziptqqPr Îµ, bÂ´Îµs, there holds
d
dtÏ†Ëš
1pziptqqÄ›Î»1Îµ
2n.
This shows the existence of a larger time horizon T1Ä…Tsuch that Ï†Ëš
1pziptqqÄ›bÂ´Îµ
whenever tÄ›T1. And since Îµcan be taken arbitrarily small, we deduce that
Ï†Ëš
1pziptqqconverges toward b, namely that ziptqconverges toward Hb, astÃ‘`8.
Arguing in the same way as above, and assuming without loss of generality that
aÄƒ0, we may find that all indices iP rnsfor which Ï†Ëš
1pziptkqq Ä Â´ Îµ0for some
Îµ0Ä…0and some sequence tkÃ‘ `8, the particle ziptqconverges toward Haas
tÃ‘`8. This concludes the proof. â–¡
9.3.Remarks.
Remark 9.4. Theorem 4.2 establishes the convergence of Ï†Ëš
1pziptqqfor any iPrns
astÃ‘`8, but does not preclude the fact that }ziptq}may diverge toward `8(along
the hyperplane) as tÃ‘`8. This is indeed expected (and observed numericallyâ€”
see Fig. 6) when Vhas some negative eigenvalues. We also note that when all the
eigenvalues of Vare non-negative, Corollary 9.2 shows that all the ziptqremain
bounded.
Remark 9.5 (The case where Vis not diagonalizable) .IfVis not assumed to
be diagonalizable, Lemma 9.3 (or, at least the proof thereof) requires some modi-
fications. Let Î´:â€œÎ»1Â´|Î»2|Ä…0. Let ÎµÄ…0be fixed and to be chosen later. We
decompose Vin Jordan blocks, and we consider
Cdâ€œmÃ 
kâ€œ1Fk, (9.11)
whereFkis the span of the Jordan chain corresponding to the k-th Jordan block.
By a slight abuse of notation (solely for the purpose of this remark), we denote by
Î»kthe eigenvalue associated to the k-th Jordan block. We recall that we can choose
a basispÏ†k,1, . . . , Ï† k,jkqof each Fkin a way that V|Fkreads in this basis as7
Â»
â€”â€”â€”â€”â€“Î»kÎµ
......
...Îµ
Î»kfi
ffiffiffiffifl. (9.12)
7Recall that Jordan blocks are commonly written with a `1in the superdiagonal. This can be
replaced by any non-zero complex scalar as done hereâ€”see [HJ12, Chapter 3, Corollary 3.1.21].44 GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
We observe that if Îµis chosen sufficiently small (depending only on Î´), Lemma 9.3
may be replaced by the following estimate in each Fk:
DCÄ…0,@tÄ›0,@iPrns,â€ºâ€ºÏ€Fk`
etVziptqË˜â€ºâ€ºÄCep|Î»k|`Î´qt. (9.13)
Here, Ï€Fkdenotes the orthogonal projection onto Fk. To prove estimate (9.13), we
follow theproofof Lemma9.3, withd
dt}Ï€Fkpxiptqq}2playingtheroleofd
dt|Ï†Ëš
kpxiptqq|2.
The key observation is that combining (9.11)and(9.12)we obtain
}Ï€FkpV xiptqq}Äp| Î»k|`Î´q}Ï€Fkpxiptqq},
provided Îµis chosen sufficiently small. Then (9.13)follows as in Lemma 9.3.
With(9.11)at hand, the proof of Theorem 4.2 carries through, under the im-
pactless modification that CepÎ»1`|Î»2|`Î´qtreplaces (9.4)(and subsequent estimates
are modified in the same way).
10.Proof of Theorem 5.2
In this section, we establish the proof for Theorem 5.2. Since the proof is essen-
tially a combination of the proofs of Theorems 4.2 and 8.1, we may occasionally
skip certain details and refer to the proofs of these two results. As done throughout
this work, we set
A:â€œpQJKq1
2.
We denote by Ï€F:RdÃ‘Fthe projection onto Fparallel to G, and by Ï€G:RdÃ‘G
theprojectiononto Gparallelto F. Theset Ï€FpconvptziptquiPrnsqqisaconvexsubset
ofFwhich is non-increasing with respect to t(the proof of this fact is identical to
that of Proposition 8.2). It therefore converges toward some convex polytope Kas
tÃ‘`8.
FixiPrns. We have
Ï€Fp9ziptqqâ€œnÃ¿
jâ€œ1Ëœ
exAetVziptq,AetVzjptqy
Å™n
kâ€œ1exAetVziptq,AetVzkptqyÂ¸
Ï€FpVpzjptqÂ´ziptqqq
â€œnÃ¿
jâ€œ1Ëœ
exAetVziptq,AetVpzjptqÂ´ziptqqy
Å™n
kâ€œ1exAetVziptq,AetVpzkptqÂ´ziptqqyÂ¸
Ï€FpVpzjptqÂ´ziptqqq.
From this point on, we follow the proof of Theorem 8.1, and we solely highlight the
changes compared to the original proof. Roughly speaking, this new proof amounts
to adding projections Ï€Fat several places. We denote by SÄ‚Fthe set of points
wPKsuch that
}Ï€FpAwq}2â€œmax
jPrmsxÏ€FpAwq, Ï€FpAvjqy.
The fact that SÄ‚BKand that Shas finite cardinality is proved precisely as Claim
1 (in the proof of Theorem 8.1), simply by replacing all occurrences of AÂ¨byÏ€FpAÂ¨q.
Once again, SÎ´denotes the set of all points in Kat distanceÄÎ´to some point of
S.
Step 2 in the proof of Theorem 8.1 (i.e., (8.7)) is replaced by the following
statement:
Step 2â€™: There exists a constant Î³â€œÎ³pKqÄ…0(depending only on the geometry
ofK) such that for any Î´Pp0, Î´0s, there exists Tâ€œTpÎ´qÄ…0such that if tÄ›TandTHE EMERGENCE OF CLUSTERS IN SELF-ATTENTION DYNAMICS 45
Ï€FpziptqqRSÎ´, then
d
dt}Ï€FpAziptqq}2Ä›Î³Î´.
We now proceed in proving this statement.
Proof of Step 2â€™. We set
ajptq:â€œxÏ€FpAziptqq, Ï€FpApzjptqÂ´ziptqqy
and
rjptq:â€œ@
AetVziptq, AetVpzjptqÂ´ziptqqD
Â´ajptqe2Î»1t.
We find
1
2d
dt}Ï€FpAziptqq}2â€œxÏ€FpA9ziptqq, Ï€FpAziptqqy
â€œnÃ¿
jâ€œ1Ëœ
exAetVziptq,AetVzjptqy
Å™n
kâ€œ1exAetVziptq,AetVzkptqyÂ¸
xÏ€FpApzjptqÂ´ziptqqq, Ï€FpAziptqqy
â€œnÃ¿
jâ€œ1Ëœ
exAetVziptq,AetVpzjptqÂ´ziptqqy
Å™n
kâ€œ1exAetVziptq,AetVpzkptqÂ´ziptqqyÂ¸
xÏ€FpApzjptqÂ´ziptqqq, Ï€FpAziptqqy
â€œnÃ¿
jâ€œ1Ëœ
eajptqe2Î»1t`rjptq
Å™n
kâ€œ1eakptqe2Î»1t`rkptqÂ¸
ajptq
looooooooooooooooooomooooooooooooooooooon
â€œ:bjptq. (10.1)
We now make use of the following adaptation of Claim 2.
Claim 3. There exists some constant Î³1â€œÎ³1pKqÄ…0depending only on the geom-
etry of Ksuch that the following holds. Fix Î´Pp0, Î´0s. There exists Tâ€œTpÎ´qÄ…0
such that if tÄ›TandziptqRSÎ´Ë†G, then there exists jPrnssuch that ajptqÄ›Î³1Î´.
Compared to Step 2 in the proof of Theorem 8.1, we now have to estimate the
coefficients rjptq. To this end, setting yjptq:â€œAetVzjptqforjPrns, we notice that
rjptqâ€œP1ptq`P2ptq`P3ptqwhere
P1ptqâ€œxÏ€Fpyiptqq, Ï€GpyjptqÂ´yiptqqy,
P2ptqâ€œxÏ€Gpyiptqq, Ï€FpyjptqÂ´yiptqqy,
P3ptqâ€œxÏ€Gpyiptqq, Ï€GpyjptqÂ´yiptqqy.
By virtue of Lemma 9.3 we have |Ï€Fpyjptqq|Ä CeÎ»1tand|Ï€Gpyjptqq|Ä Cet|Î»2|for
anytÄ›0(orCet|Î»2|`ÎµifV|Gis not diagonalizableâ€”see Remark 9.5), hence
|rjptq|ÄCetpÎ»1`|Î»2|q. (10.2)
Since Ï€Fpzjptqqis uniformly bounded in tPr0,`8qfor any jPrnsdue to Corollary
8.3, we get ajpÂ¨qPL8p0,`8q. So, we may set
Îº:â€œmax
jPrnssup
tÄ›0|ajptq|.
LettÄ›0. We define
Bptq:â€œâ£
jPrns:ajptqe2Î»1t`rjptqÄ›0(
.
Letj0ptqPargmaxjPrnspajptqe2Î»1t`rjptqq. Note that j0ptqPBptqsince
aj0ptqe2Î»1t`rj0ptqÄ›aiptqe2Î»1t`riptqâ€œ0.46 GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
We notice the following three properties:
â€šForjâ€œj0ptq, we have bj0ptqptq Ä›aj0ptqptq
n(recall the definition of bjin
(10.1));
â€šfor any jPBptqztj0u, we have bjptqÄ›0;
â€šfor any jRBptq, we have
bjptqÄ›Â´ ÎºexpÂ´
Â´aj0ptqe2Î»1t`CepÎ»1`|Î»2|qtÂ¯
.
Indeed, using the fact that jPBptqand (10.2), we find
exp`
ajptqe2Î»1t`rjptqË˜
nÃ¿
kâ€œ1exp`
akptqe2Î»1t`rkptqË˜Ä1
nÃ¿
kâ€œ1exp`
akptqe2Î»1t`rkptqË˜
Ä1
exppaj0ptqe2Î»1t`rj0ptqq
ÄexpÂ´
Â´aj0ptqe2Î»1t`CepÎ»1`|Î»2|qtÂ¯
.
Making use of these properties in (10.1) yields the desired lower boundâ€”indeed,
iftis sufficiently large and ziptq RSÎ´Ë†G, we havetjP rns:ajptq Ä›Î³1Î´u â€° H
according to Claim 3, and so we deduce that
1
2d
dt}Aziptq}2Ä›Î³1Î´
nÂ´ÎºneÂ´Î³1Î´e2Î»1t`CepÎ»1`|Î»2|qt.
Taking tpossibly larger (and depending on Î´), we obtain the result of Step 2â€™. â–¡
Steps 3 and 4 in the proof of Theorem 8.1 are essentially unchangedâ€”we re-
place all the occurrences of }AÂ¨}by}Ï€FpAÂ¨q}(for instance in (8.13) and (8.14)).
Although}Aziptq}may not be uniformly bounded in t, it is important to note
that}Ï€FpAziptqq}is uniformly bounded. Similarly, while 9ziptqRL8pr0,`8qq, we
do have}d
dtÏ€FpzipÂ¨qq} L8pr0,`8qqÄƒ`8. The sets SÎ´,CkandCr
kare replaced by
SÎ´Ë†G,CkË†GandCr
kË†Grespectively. The conclusion is that }Ï€FpAziptqq}2has
to increase by at least
Î³Î´1
2pÎ´1
4Â´Î´q
}9zi}L8pr0,`8qqÄ›Î´3
4
2}9zi}L8pr0,`8qqÄ…4R}A}opÎ´
during a travel from CkË†Gto the complement of C1
4
kË†G. As in the proof of
Theorem 8.1 this implies that for any iP rnsthere exists sPSsuch that ziptq
remains at distance at most Î´away fromtsuË†G. This being true for any Î´Ä…0,
we obtain the desired result.
11.Numerical experiments
11.1.Setup.Unless indicated otherwise, all figures presented in this paper were
generated by discretizing the underlying dynamics (either (1.1) or (3.1)) using a
fourth order Runge-Kutta scheme with a step size of 0.1. All points in the initial
sequence were drawn independently from the uniform distribution over the hyper-
cuberÂ´5,5sd. Random matrices (e.g., Q, K, V) have entries drawn independently
from the uniform distribution on rÂ´1,1s. Codes and animated plots of all examples
may be found online atTHE EMERGENCE OF CLUSTERS IN SELF-ATTENTION DYNAMICS 47
https://github.com/borjanG/2023-transformers .
We now present some experiments which motivate some conjectures and claims
made in what precedes.
11.2.Eigenvalues of ALBERTâ€™s value matrices. In Figure 10 we illustrate the
eigenvaluesofthevaluematrices Vhforacoupleofheads hinapre-trained ALBERT
model. We focus on ALBERT-xlarge-v2 available online at
https://huggingface.co/albert-xlarge-v2 .
This version uses 16heads, with sequences of length nâ€œ256and tokens of di-
mension dâ€œ128. While not all value matrices Vhper head hP r16ssatisfy the
assumptions made in Section 4, we illustrate the eigenvalues of a couple of them
which do.
2
 1
 0 11.5
1.0
0.5
0.00.51.01.5
Eigenvalues of value matrix for head 5
2
 1
 0 11.5
1.0
0.5
0.00.51.01.5
Eigenvalues of value matrix for head 14
Figure 10. The eigenvalues of V5andV14in the pre-trained ALBERT
satisfy the eigenvalue assumption made in Definition 4.1. Furthermore,
the second assumption made in Definition 4.1 is satisfied by pQ5, K5q
andpQ14, K14q(the inner products evaluated along the eigenvector of
norm 1equal 1.3060and0.6719respectively). In other words, the triples
pQh, Kh, Vhqcorresponding to heads hâ€œ5andhâ€œ14inALBERT
satisfy all the assumptions made in the statement of Theorem 4.2.
11.3.Experiments related to Theorem 2.1. We begin with the setup of The-
orem 2.1, which we recall was proven to hold in the case dâ€œ1. Herein we present
a couple of examples (Figures 11 and 12) which elucidate the role that dandn
appear to play in this fact.
Notably, as seen in Fig. 4, we believe that the conclusion of Theorem 2.1 could
plausibly be extended to any dÄ…1, assuming VÄ…0.
11.4.Illustrating Theorem 4.2 in R3.To precisely illustrate the appearance of
at most three hyperplanes in the setting of Theorem 4.2, we gave an example in R2.
We expand on this and provide a couple of toy examples in R3for the purpose of
visualization (we recall that these are toy models, as Transformers in practice are
high-dimensional), and namely focus in both examples on the case where the two
latter eigenvalues are complex. In Fig. 14, we see the effect of having eigenvalues
with a negative real part, and the complementary case is illustrated in Fig. 13.48 GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
0 20 40 60 80
0
20
40
60
80t= 0.0, rank= 11
0 20 40 60 80
0
20
40
60
80t= 3.0, rank= 27
0 20 40 60 80
0
20
40
60
80t= 5.0, rank= 15
0 20 40 60 80
0
20
40
60
80t= 10.0, rank= 3
Figure 11. WeexpandonFig.3â€”forthesamesetup, consider nâ€œ100.
The sequence length ndoes not appear to influence the rank of Pptq,
which is expected since the rank of Pcorresponds to the number of
leaders.
0 5 10 15 20 25 30 35
0
5
10
15
20
25
30
35t= 0.0, rank= 40
0 5 10 15 20 25 30 35
0
5
10
15
20
25
30
35t= 0.2, rank= 40
0 5 10 15 20 25 30 35
0
5
10
15
20
25
30
35t= 1.0, rank= 2
0 5 10 15 20 25 30 35
0
5
10
15
20
25
30
35t= 10.0, rank= 2
0 5 10 15 20 25 30 35
0
5
10
15
20
25
30
35t= 0.0, rank= 40
0 5 10 15 20 25 30 35
0
5
10
15
20
25
30
35t= 0.5, rank= 7
0 5 10 15 20 25 30 35
0
5
10
15
20
25
30
35t= 5.0, rank= 2
0 5 10 15 20 25 30 35
0
5
10
15
20
25
30
35t= 10.0, rank= 2
0 5 10 15 20 25 30 35
0
5
10
15
20
25
30
35t= 0.0, rank= 40
0 5 10 15 20 25 30 35
0
5
10
15
20
25
30
35t= 0.5, rank= 4
0 5 10 15 20 25 30 35
0
5
10
15
20
25
30
35t= 1.0, rank= 2
0 5 10 15 20 25 30 35
0
5
10
15
20
25
30
35t= 10.0, rank= 2
Figure 12. We consider nâ€œ40,Qâ€œKâ€œIdand a random matrix
VÄ…0in dimensions dâ€œ10(first row), dâ€œ40(second row), and dâ€œ80
(third row). The conclusion of Theorem 2.1 appears to transfer to the
higher dimensional case, and this would actually follow from Conjecture
4.3 (should it hold).
11.5.Complementing Figure 7. In Figure 7, we illustrate the appearance of
clustering in high-dimension (the ALBERT setup: nâ€œ256anddâ€œ128) for generic
random matrices pQ, K, Vq. The value matrix Vin question has 65positive eigen-
values, and we show the conjectured convergence of the 65coordinates along the
corresponding eigenvectors to one of possibly 3(generically 2) real scalars. In Fig-
ure 15, we complement this illustration by showing the possible oscillatory and
divergent behavior of the remaining coordinates.THE EMERGENCE OF CLUSTERS IN SELF-ATTENTION DYNAMICS 49
âˆ’5 0 5âˆ’505âˆ’50510
t= 0.0
âˆ’5 0 5âˆ’505âˆ’50510
t= 5.0
âˆ’5 0 5âˆ’505âˆ’50510
t= 40.0
Figure 13. We consider nâ€œ25,Qâ€œKâ€œId, and Va random matrix
with positive entries and eigenvalues t1,0.1`0.08i,1Â´0.08iu. The
pair of complex eigenvalues have a positive real part. We not only
see convergence to one of two hyperplanes determined by the direction
Ï†1â€œp0.38,0.8,0.47q, but in fact, the particles appear to collapse to two
points. In other words, the "hyperplanes" are of codimension 3, which
is in line with Conjecture 4.3.
âˆ’10 0 10010âˆ’505
t= 0.0
âˆ’10 0 10010âˆ’505
t= 5.0
âˆ’10 0 10010âˆ’505
t= 10.0
âˆ’50050âˆ’2502550âˆ’2502550
t= 30.0
âˆ’50050âˆ’2502550âˆ’2502550
t= 35.0
âˆ’50050âˆ’2502550âˆ’2502550
t= 40.0
Figure 14. We consider nâ€œ25,Qâ€œKâ€œId, and Va random matrix
with positive entries and eigenvalues t1,Â´0.05`0.25i,Â´0.05Â´0.25iu.
The pair of complex eigenvalues have a negative real part, which en-
tails the rotation of the particles. We see that the particles rotate
within a couple of 2-dimensional hyperplanes determined by Ï†1â€œ
pÂ´0.3,Â´0.8,Â´0.45q, as implied by Theorem 4.2.50 GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
0 2 4 6 8 10 12 14
tâˆ’1014âˆ’1010âˆ’106âˆ’102010210610101014Non-clustered coordinates
Figure 15. We complement Figure 7 and plot the variance of the set
tÏ†Ëš
jpziptqq:iPrnsuof all coordinates jcorresponding to negative eigen-
values of V. We also show the mean along tokens of a couple of coor-
dinates (white lines). Coordinates diverge rapidly to Ë˜8over time t;
y-axis is in logscale.
Part3.Discussion and open questions
12.Outlook
Several important directions regarding the mathematical theory of Transformers
remain unexplored. An important extension of our work would amount to studying
multi-headed Transformersâ€”borrowingthenotationfromRemark3.4, theyamount
to:
xrk`1s
iâ€œxrks
i`âˆ†tHÃ¿
hâ€œ1nÃ¿
jâ€œ1Ëœ
exQhxrks
i,Khxrks
jy
Å™n
â„“â€œ1exQhxrks
i,Khxâ„“pkqyÂ¸
Vhxrks
j, kPN.
Foreach hPrHs(correspondingtoadifferent head), theweightmatrices Qh, Kh, Vh
are constant. Proofs regarding clustering or convergence of the self-attention ma-
trix for such dynamics is an open problem. Preliminary numerical investigations
seem to indicate that interesting clustering phenomena also occur in this context.
A characterization or properties of optimal weights by invoking the optimal con-
trol correspondence in the spirit of [Wei17] is also an interesting avenue for future
research.
We hereby list a couple of additional numerical experiments suggesting general-
izations of our results, which we leave as open problems.
12.1.Beyond QJKÄ…0in Theorems 3.1 and 5.2. As seen throughout all the
presented proofs, assumptions on the value matrix Vare significantly more rigid
than assumptions on the matrices QandK. For instance, should the eigenvalue
Î»with the largest real part of Vbe negative, all rescaled tokens will diverge to
infinity. Should Î»be complex, we do not expect any clustering to occur (for theTHE EMERGENCE OF CLUSTERS IN SELF-ATTENTION DYNAMICS 51
rescaledtokens). Yet, noneoftheconclusionsofTheorems3.1or5.2seemtochange
for generic choices of QJK. This is illustrated in Figures 16 and 17 respectively.
âˆ’5
0
5âˆ’505âˆ’505
t= 0.0
âˆ’5
0
5âˆ’505âˆ’505
t= 1.0
âˆ’5
0
5âˆ’505âˆ’505
t= 2.0
âˆ’5
0
5âˆ’505âˆ’505
t= 5.0
Figure 16. Here, Vâ€œId, while QJKviolates the PSD assumptionâ€“it
is a random matrix (with entries drawn from the uniform distribution
onrÂ´1,1s). Nonetheless, the clustering pattern entailed by Theorem 3.1
persists.
12.2.Beyond pure self-attention: adding a feed-forward layer. Practical
implementations of the Transformer architecture combine the self-attention mech-
anism with a feed-forward neural network. While extending the mathematical
analysis from this paper to such a broader setting would be challenging, we can
offer some numerical insights into the expected outcomes.
The feed-forward neural network which can be adjoined to the Transformer dy-
namics in one of two ways. The first way consists in running the pure self-attention
dynamics up to time tÄT(or equivalently, for OpTqlayers), and then applying a
pure feed-forward neural network to the concatenated vector of clustered features
at time T. This amounts to seeing the feed-forward network as a map from Rnd
toRm(for some mâ©¾1), which can be studied independently with existing theory.52 GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
âˆ’5
0
5âˆ’505âˆ’100âˆ’50050
t= 0.0
âˆ’5
0
5âˆ’505âˆ’100âˆ’50050
t= 5.0
âˆ’5
0
5âˆ’505âˆ’100001000
t= 10.0
âˆ’5
0
5âˆ’505âˆ’10000010000
t= 15.0
Figure 17. Here, Vis paranormal, while QJKviolates the PSD
assumptionâ€“it is a random matrix (with entries drawn from the uniform
distribution on rÂ´1,1s). Nonetheless, the clustering pattern entailed by
Theorem 5.2 persists.
The second way consists in using both the self-attention and feed-forward mech-
anisms in parallel at every layer t. In this case, clustering in the exact sense of
Theorems 3.1 and Theorems 5.2 would be difficult to anticipate since the weights
of the feed-forward network play the role of a value matrix V(as they can be ab-
sorbed within V), and the conclusions of these theorems strongly depend on the
identity-like structure.
In Figure 18, we focus on the second of the above-discussed examples, and il-
lustrate a possible generalization of Theorem 4.2 to this setup. For simplicity, we
focus on a 2-layer neural network: we apply a component-wise nonlinear activa-
tion function Ïƒ(either the ReLU or tanh) to the self-attention dynamics, and then
multiply by a weight matrix WPRdË†d. Namely, we consider
9ziptqâ€œWÏƒËœ
VnÃ¿
jâ€œ1Ëœ
exQetVziptq,KetVzjptqy
Å™n
kâ€œ1exQetVziptq,KetVzkptqyÂ¸
pzjptqÂ´ziptqqÂ¸
(12.1)THE EMERGENCE OF CLUSTERS IN SELF-ATTENTION DYNAMICS 53
foriPrnsandtÄ›0. A bias vector bPRd(whether inside or outside the activation
function) can also be included to allow for translations. The clustering property
appears to persist, the pattern depending on the weight matrix Wand on the
activation function Ïƒ. We leave this problem open to further investigation.
âˆ’5 0 5 10âˆ’50510
t= 0.0
âˆ’5 0 5 10âˆ’50510
t= 20.0
0 10 20âˆ’505101520
t= 40.0
0 50 100 150 200050100150
t= 100.0
âˆ’5 0 5âˆ’4âˆ’2024
t= 0.0
âˆ’5 0 5âˆ’4âˆ’2024
t= 1.0
âˆ’5 0 5âˆ’4âˆ’2024
t= 5.0
âˆ’5 0 5âˆ’4âˆ’2024
t= 10.0
âˆ’5 0 5âˆ’4âˆ’2024
t= 0.0
âˆ’5 0 5âˆ’4âˆ’2024
t= 0.1
âˆ’5 0 5âˆ’4âˆ’2024
t= 5.0
âˆ’5 0 5âˆ’4âˆ’2024
t= 10.0
Figure 18. The setup of Theorem 4.2 with a 2-layer neural network
appended to the dynamics (i.e., (12.1)). Top: Ïƒâ€œReLU with Wâ€œId.
Middle: Ïƒâ€œtanhwith Wâ€œId. Bottom: Ïƒâ€œReLU with Wbeing a
random matrix. In the first row, we see that the particles first evolve
as to reach the upper right quadrant pRÄ…0qd(due to the ReLU). Once
they reach it, every particle eventually follows one of three hyperplanes
determined by the spectrum of Vand the projection onto pRÄ…0qd. In
the other two cases, all particles appear to collapse to 0.
References
[ABV`05] Juan A AcebrÃ³n, Luis L Bonilla, Conrad J PÃ©rez Vicente, FÃ©lix Ritort, and Renato
Spigler. The Kuramoto model: A simple paradigm for synchronization phenomena.
Reviews of modern physics , 77(1):137, 2005.
[BM00] Paul S Bradley and Olvi L Mangasarian. K-plane clustering. Journal of Global opti-
mization , 16:23â€“32, 2000.
[Che95] Yizong Cheng. Mean shift, mode seeking, and clustering. IEEE Transactions on Pat-
tern Analysis and Machine Intelligence , 17(8):790â€“799, 1995.
[CHH`16] Junghee Cho, Seung-Yeal Ha, Feimin Huang, Chunyin Jin, and Dongnam Ko. Emer-
gence of bi-cluster flocking for the Cuckerâ€“Smale model. Mathematical Models and
Methods in Applied Sciences , 26(06):1191â€“1218, 2016.
[CRBD18] Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural
ordinary differential equations. Advances in Neural Information Processing Systems ,
31, 2018.54 GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
[CS07] Felipe Cucker and Steve Smale. Emergent behavior in flocks. IEEE Transactions on
Automatic Control , 52(5):852â€“862, 2007.
[DCL21] Yihe Dong, Jean-Baptiste Cordonnier, and Andreas Loukas. Attention is not all you
need: Pure attention loses rank doubly exponentially with depth. In International
Conference on Machine Learning , pages 2793â€“2803. PMLR, 2021.
[Dob79] Roland Lâ€™vovich Dobrushin. Vlasov equations. Funktsionalâ€™nyi Analiz i ego
Prilozheniya , 13(2):48â€“58, 1979.
[Gol13] FranÃ§ois Golse. Mean field kinetic equations. Course of Polytechnique , 2013.
[HJ12] Roger A Horn and Charles R Johnson. Matrix analysis . Cambridge University Press,
2012.
[HK02] Rainer Hegselmann and Ulrich Krause. Opinion dynamics and bounded confidence:
models, analysis and simulation. Journal of Artifical Societies and Social Simulation
(JASSS) , 5(3), 2002.
[HKPZ19] Seung-Yeal Ha, Jeongho Kim, Jinyeong Park, and Xiongtao Zhang. Complete clus-
ter predictability of the Cuckerâ€“Smale flocking model on the real line. Archive for
Rational Mechanics and Analysis , 231:319â€“365, 2019.
[HR17] Eldad Haber and Lars Ruthotto. Stable architectures for deep neural networks. In-
verse problems , 34(1), 2017.
[HysW`22] Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean
Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language
models. In International Conference on Learning Representations , 2022.
[HZRS16a] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning
for image recognition. In Proceedings of the IEEE conference on computer vision and
pattern recognition , pages 770â€“778, 2016.
[HZRS16b] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep
residual networks. In Computer Visionâ€“ECCV 2016: 14th European Conference,
Amsterdam, The Netherlands, October 11â€“14, 2016, Proceedings, Part IV 14 , pages
630â€“645. Springer, 2016.
[JM14] Pierre-Emmanuel Jabin and Sebastien Motsch. Clustering and asymptotic behavior
in opinion formation. Journal of Differential Equations , 257(11):4165â€“4187, 2014.
[Kra00] UlrichKrause.Adiscretenonlinearandnon-autonomousmodelofconsensus.In Com-
munications in Difference Equations: Proceedings of the Fourth International Con-
ference on Difference Equations , page 227. CRC Press, 2000.
[Kur75] Yoshiki Kuramoto. Self-entrainment of a population of coupled non-linear oscillators.
InInternational Symposium on Mathematical Problems in Theoretical Physics: Jan-
uary 23â€“29, 1975, Kyoto University, Kyoto/Japan , pages 420â€“422. Springer, 1975.
[LCG`20] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma,
and Radu Soricut. ALBERT: A Lite BERT for Self-supervised Learning of Language
Representations. In International Conference on Learning Representations , 2020.
[LLH`20] Yiping Lu, Zhuohan Li, Di He, Zhiqing Sun, Bin Dong, Tao Qin, Liwei Wang, and
Tie-YanLiu.Understandingandimprovingtransformerfromamulti-particledynamic
systempointofview.In International Conference on Learning Representations , 2020.
[LWLQ22] Tianyang Lin, Yuxin Wang, Xiangyang Liu, and Xipeng Qiu. A survey of transform-
ers.AI Open , 3:111â€“132, 2022.
[MT14] Sebastien Motsch and Eitan Tadmor. Heterophilious dynamics enhances consensus.
SIAM Review , 56(4):577â€“621, 2014.
[PHD20] Vardan Papyan, XY Han, and David L Donoho. Prevalence of neural collapse during
the terminal phase of deep learning training. Proceedings of the National Academy of
Sciences, 117(40):24652â€“24663, 2020.
[PR13] Benedetto Piccoli and Francesco Rossi. Transport equation with nonlocal velocity in
Wasserstein spaces: convergence of numerical schemes. Acta Applicandae Mathemat-
icae, 124:73â€“105, 2013.
[PR16] Benedetto Piccoli and Francesco Rossi. On properties of the generalized Wasserstein
distance. Archive for Rational Mechanics and Analysis , 222:1339â€“1365, 2016.
[PRT15] Benedetto Piccoli, Francesco Rossi, and Emmanuel TrÃ©lat. Control to flocking of the
kinetic Cuckerâ€“Smale model. SIAM Journal on Mathematical Analysis , 47(6):4685â€“
4719, 2015.THE EMERGENCE OF CLUSTERS IN SELF-ATTENTION DYNAMICS 55
[RS14] Brian Rider and Christopher D. Sinclair. Extremal laws for the real Ginibre ensemble.
The Annals of Applied Probability , 24(4):1621 â€“ 1651, 2014.
[SABP22] Michael E Sander, Pierre Ablin, Mathieu Blondel, and Gabriel PeyrÃ©. Sinkformers:
Transformers with doubly stochastic attention. In International Conference on Arti-
ficial Intelligence and Statistics , pages 3515â€“3530. PMLR, 2022.
[VCBJ`95] TamÃ¡sVicsek, AndrÃ¡sCzirÃ³k, EshelBen-Jacob, InonCohen, andOferShochet.Novel
type of phase transition in a system of self-driven particles. Physical Review Letters ,
75(6):1226, 1995.
[Vid11] RenÃ© Vidal. Subspace clustering. IEEE Signal Processing Magazine , 28(2):52â€“68,
2011.
[VKF20] Apoorv Vyas, Angelos Katharopoulos, and FranÃ§ois Fleuret. Fast transformers with
clustered attention. Advances in Neural Information Processing Systems , 33:21665â€“
21674, 2020.
[VSP`17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N
Gomez, Åukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in
Neural Information Processing Systems , 30, 2017.
[Wei17] E Weinan. A proposal on machine learning via dynamical systems. Communications
in Mathematics and Statistics , 1(5):1â€“11, 2017.
[WHL19] E Weinan, Jiequn Han, and Qianxiao Li. A mean-field optimal control formulation
of deep learning. Research in Mathematical Sciences , 6(1):10, 2019.
[WLK`20] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer:
Self-attention with linear complexity. arXiv preprint arXiv:2006.04768 , 2020.
[YBR`20] ChulheeYun, SrinadhBhojanapalli, AnkitSinghRawat, SashankJReddi, andSanjiv
Kumar. Are transformers universal approximators of sequence-to-sequence functions?
InInternational Conference on Learning Representations , 2020.
Borjan Geshkovski
Department of Mathematics
MIT
Cambridge, MA
02139 USA
e-mail: borjan@mit.eduCyril Letrouit
Department of Mathematics
MIT
Cambridge, MA
02139 USA
e-mail: letrouit@mit.edu
Yury Polyanskiy
Department of EECS
MIT
Cambridge, MA
02139 USA
e-mail: yp@mit.eduPhilippe Rigollet
Department of Mathematics
MIT
Cambridge, MA
02139 USA
e-mail: rigollet@mit.edu