THE EMERGENCE OF CLUSTERS IN SELF-ATTENTION
DYNAMICS
BORJAN GESHKOVSKI, CYRIL LETROUIT, YURY POLYANSKIY,
AND PHILIPPE RIGOLLET
Abstract. Viewing Transformers as interacting particle systems, we describe
the geometry of learned representations when the weights are not time depen-
dent. We show that particles, representing tokens, tend to cluster toward
particular limiting objects as time tends to infinity. Cluster locations are
determined by the initial tokens, confirming context-awareness of representa-
tions learned by Transformers. Using techniques from dynamical systems and
partial differential equations, we show that the type of limiting object that
emerges depends on the spectrum of the value matrix. Additionally, in the
one-dimensional case we prove that the self-attention matrix converges to a
low-rank Boolean matrix. The combination of these results mathematically
confirms the empirical observation made by Vaswani et al. [VSP`17] that
leadersappear in a sequence of tokens when processed by Transformers.
Contents
Part 1. Introduction and main results 1
1. Introduction 1
2. Asymptotic low-rankness of the self-attention matrix 6
3. Clustering toward vertices of convex polytopes 8
4. Clustering toward hyperplanes 10
5. A mix of hyperplanes and polytopes 13
Part 2. Proofs 13
6. Well-posedness 13
7. Proof of Theorem 2.1 20
8. Proofs of Theorems 3.1 and 8.5 28
9. Proof of Theorem 4.2 39
10. Proof of Theorem 5.2 44
11. Numerical experiments 46
Part 3. Discussion and open questions 50
12. Outlook 50
References 53
Part1.Introduction and main results
1.Introduction
The introduction of Transformers in 2017 [VSP`17] marked a turning point
in the AI revolution, powering breakthroughs in natural language modeling and
computer vision. With remarkable empirical success, Transformers enable large
language models to compute very powerful representations using the self-attentionarXiv:2305.05465v6  [cs.LG]  12 Feb 20242 GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
mechanism. Yet, little is known about the geometric structure of these represen-
tations. As the size of these models grows at an astonishing rate, the need to
understand their inner workings is becoming a pressing scientific challenge. In this
work, we make a first step in this direction by describing the geometry of learned
representations.
To provide a transparent presentation of our findings, we take a leaf out of the
literature on continuous-time dynamics such as neural ordinary differential equa-
tions (ODEs) [CRBD18, Wei17, HR17]. By viewing layers as a time variable, this
formalism has emerged as a flexible mathematical framework to implement and
study ResNets [HZRS16a] as particular discrete-time versions of a parametrized
dynamics of the form
9xptq“fθpxptqq, tPr0, Ts.
Here θis the trained parameter of a neural network and fθis characterized by the
precise architecture of the ResNet1. In turn, an input (e.g., an image) xp0qPRdis
mapped to its representation xpTq.
Unlike neural ODEs and ResNets, the representation map of Transformers is
not solely a function of an individual input xp0qPRdbut rather of a set/sequence
px1p0q, . . . , x np0qqofně1d-dimensional tokens. These tokens then evolve in
time by interacting with each other per the self-attention mechanism. Namely,
following [SABP22], we view tokens as particles, and the transformer dynamics as
an interacting particle system of the form
9xiptq“nÿ
j“1PijptqV xjptq, tPr0,`8q, (1.1)
for any iPrns, where Pijptqare the entries of a nˆnstochastic matrix Pptq, given
by
Pijptq:“exQxiptq,Kxjptqy
řn
ℓ“1exQxiptq,Kxℓptqy,pi, jqPrns2. (1.2)
Here the matrices Q(Query), K(Key), and V(Value) are learned from data.
Note that Q, Kneed not be square. The nˆnmatrix Pptqis called self-attention
matrix. The wording attention stems precisely from the fact that Pijptqcaptures
the attention given by token ito token jrelatively to all tokens ℓP rns. The
matrices QandKin (1.2) warp the geometry of the input tokens, so that a trained
attention matrix contains weights which indicate semantic relations between words.
Such conclusions have been drawn in the context of language processing tasks in
[VSP`17, Figures 3-5].
Our goal is to showcase the fact that self-attention, which itself is the core
novelty of Transformers, entails a clustering effect. To that end, we focus on the
pure self-attention dynamics described in (1.1). In particular, we do not model
variations such as multiple heads, feed-forward layers, and layer normalization that
are typically adjoined to self-attention dynamics of (1.1). However, on this last
point, wenotethatourtheoreticalfindingsindicatethatwithoutanynormalization,
the dynamics (1.1) can diverge in some (or even all) directions over time. We leave
these additional questions for future research; see Section 12.
1A classical choice is θ“pW, A, bqPRdˆdˆRdˆdˆRdandfθpxq“WσpAx`bqwhere σis
an elementwise nonlinearity such as the ReLU ([HZRS16b]).THE EMERGENCE OF CLUSTERS IN SELF-ATTENTION DYNAMICS 3
Figure 1. ForV“I3tokens cluster
toward the vertices of a convex polytope
(Theorem 3.1).1.1.Organization of the paper and
summary of contributions. The goal of
this paper is to characterize clustered rep-
resentations of a trainedTransformer by
studying the asymptotic behavior of a se-
quence of tokens px1ptq, . . . , x nptqqas they
evolve through the layers of a transformer
architecture using the dynamics (1.1). In
this setup, a Transformer is completely de-
scribed by the weight matrices pQ, K, Vq
obtained during training. Note that we as-
sume that these three matrices are time-
independent . While this assumption is mo-
tivated by mathematical convenience, it is worth noting that such weight-sharing
scenarios are in fact used in practice—see, e.g., ALBERT [LCG`20]—as they dras-
tically reduce the number of parameters of a network.
With parameters pQ, K, Vqfixed, tokens are subject to collective dynamics that
we calltransformer dynamics . While these dynamics are reminiscent of existing
models for opinion dynamics and flocking, they present they own mathematical
challenges requiring ad-hoc tools to study their asymptotic behavior.
The main conclusion of our analysis is that the set of tokens tx1ptq, . . . , x nptqu,
appropriately rescaled, tends to a clustered configuration astÑ8. Our theoretical
findings justify the empirical observation made in [VSP`17] that leadersappear
in a sequence of tokens when processed by Transformers. We now list our main
contributions.
(i)As a warm-up to the geometric characterization of the limits of sequences of
tokens, we show in Section 2 that when d“1andVą0, the self-attention
matrix Pptqconverges to a low-rank matrix with entries 0and1astÑ`8thus
revealing the emergence of a small number of leaders that drive the transformer
dynamics. The restriction d“1follows from technical considerations, and some
pathological phenomena may occur in higher dimensions (see Remark 7.9). The
proof may be found in Section 7 . But numerical experiments (as well as past
empirical work) indicate that the result may extend to higher dimensions for almost
all initial sequences of tokens.
(ii)InSection3 wefirstfocusonthecase V“Idasanaturalcanonicalchoicethat
enables us to establish some of the main tools of the paper. We introduce a time
re-scaling reminiscent of the layer normalization heuristics to alleviate the possible
divergence of tokens. We show that along this scale the tokens converge to the
boundary of a convex polytope. For almost all initial sequences they even converge
to the vertices of the polytope, the number of which is significantly smaller than n.
This elucidates the clustering phenomenon. (See Fig. 1.) When V“´Id, all tokens
following the dynamics (1.1) collapse to 0. The proofs are given in Section 8 .
(iii)We build on these results and in Section 4 consider the case wherein Vis
only assumed to have a simple and positive leading eigenvalue. This setting is much
closer to reality and corresponds to actual learned matrices V(see Figure 10). We
show that along the particular timescale, tokens cluster toward one of at most three4 GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
hyperplanes which are determined by the corresponding eigenvector. The proof is
given inSection 9 .
(iv)InSection 5 we complete the results of Sections 3 and 4 by addressing the
case where the leading eigenvalue has multiplicity. This results in clustering toward
the vertices of a convex polytope in some directions, and a linear subspace in the
others. The proof is provided in Section 10 .
(v)We also prove the global existence and uniqueness of solutions of all dynamics
considered in this work (including the mean field limit). We refer the reader to
Section 6 for more details.
We also observed numerically that our conclusions extend to more compound ar-
chitectures (see Conjecture 4.3, Section 12 andSection 11 ).
Value Key and Query Limit geometry Reference
V“Id QJKą0 vertices of convex polytope Theorem 3.1
λ1pVqą0, simplexQφ1, Kφ 1yą0union of 3parallel hyperplanes Theorem 4.2
Vparanormal QJKą0 polytopeˆsubspaces Theorem 5.2
V“´Id QJK“Id single cluster at origin˚Theorem 8.5
Table 1. Summary of the clustering results of this work.˚All results
except for the case V“´Idhold for the time-scaled dynamics (3.1).
Remark 1.1 (Discrete time) .While we focus on the idealized setting of self-
attention dynamics in continuous-time, this is solely done for convenience and all
of our methods are straightforwardly applicable to the discrete-time setting. (See
also Remark 3.4.) The discrete-time analog of (1.1)with time-step ∆tą0(equal
to1in practice) is simply the forward Euler iteration
xippk`1q∆tq“xipk∆tq`∆tnÿ
j“1ˆexQxipk∆tq,Kxjpk∆tqy
řn
ℓ“1exQxipk∆tq,Kxℓpk∆tqy˙
V xjpk∆tq,(1.3)
forkPN.
1.2.Notation. We denote by x¨,¨yand}¨}the Euclidean dot product and norm
respectively, and we use the shorthand rns:“t1, . . . , nu. For any matrix MPRdˆd,
we order its eigenvalues (repeated according to multiplicity) by decreasing order of
modulus:|λ1pMq|ě. . .ě|λdpMq|. We denote by }M}optheℓ2—operator norm
of the matrix M, equal to the largest singular value of M. Given a set SĂRd, we
define the distance of a point xPRdtoSasdistpx, Sq:“infsPS}x´s}, and by
convpSqthe convex hull of S.
1.3.Related work. Our study and results build on several different lines of work,
and we draw some parallels in what follows.
1.3.1.Analysis of attention-based models. Given the widespread use of Transform-
ers in natural language processing, there has been a surge of interest in under-
standing the function and significance of attention layers within these models. In
[YBR`20], the authors show that when treated as discrete-time systems with addi-
tional dense layers and multiple heads appended to the core attention mechanism,THE EMERGENCE OF CLUSTERS IN SELF-ATTENTION DYNAMICS 5
Transformers exhibit the universal approximation property. In [LLH`20], the au-
thors present, to the best of our knowledge, the first interacting particle systems
perspective on Transformers. They then leverage the similarities between Trans-
formers (with an additional feed-forward layer compared to (1.1)) and convection-
diffusion equations to slightly improve the performance of Transformers by em-
ploying a Strang-Marchuk splitting scheme for time discretization. In [SABP22],
the authors interpret system (1.1) as the characteristics of a continuity equation.
Drawing on the similarities between (1.1) and Sinkhorn iterations, they propose
a novel architecture dubbed Sinkformer , which possesses the desirable property of
being a Wasserstein gradient flow.
1.3.2.Quadratic complexity of Transformers. The major computational challenge
of Transformers is their high computational complexity, particularly when process-
ing long sequences. Transformers require quadratic time and space complexity to
process sequences, because each self-attention layer contains n2products of the
formxQxi, Kx jy(fori, jPrns). The empirical observation that the self-attention
matrix Pisclosetoalowrankmatrix—see[LWLQ22, Section4.4]forreferences—is
citedastheinspirationbehind Linformers [WLK`20]andthefine-tuningalgorithm
LoRA [HysW`22]. For both approaches, the low-rank structure is imposed rather
than extracted from Pitself. Other methods called sparse attention andblock at-
tentionhave been proposed to reduce the quadratic complexity—see [WLK`20,
Section 2.2] for references. In the spirit of these works, a foreshadowing of the
clustering mechanism was invoked in [VKF20], where queries are clustered into
groups, again in view of reducing the quadratic complexity of self-attention. We
point out that [DCL21] previously demonstrated that without skip connections, the
dynamics trivializes and all tokens quickly lump together into a single tight cluster.
Our work, in contrast, shows that in the presence of skip connections a rich cluster
structure emerges.
Compared to the usual BERT,ALBERT [LCG`20] uses parameter-sharing across
layers, meaning that the weight matrices Q, K, Vin (1.1)-(1.2) do not depend on
time, as in the present paper. This does not reduce the theoretical Opn2qcomplex-
ity of the original Transformer, but, quoting [LCG`20], it "significantly reduce[s]
the number of parameters for BERTwithout seriously hurting performance, thus
improving parameter-efficiency. An ALBERT configuration similar to BERT-large
has 18x fewer parameters and can be trained about 1.7x faster. The parameter
reduction techniques also act as a form of regularization that stabilizes the training
and helps with generalization".
1.3.3.Neural collapse. Ourresultsandconclusionsbeararesemblancetosomegeo-
metric aspects of neural collapse for classification tasks [PHD20]. A key geometric
aspect of neural collapse is the observation that, during the training of deep neural
networks, the representation of different classes in the later layers of the network
tends to form a tight cluster around the vertices of a simplex. The emergence of a
simplex structure in the representation space provides insights into how the neural
network organizes and separates the different classes.
1.3.4.Clustering in interacting particle systems. The transformer dynamics (1.1)
have a strong connection to the vast literature on nonlinear systems arising in the
modeling of opinion dynamics and flocking phenomena. In addition to the clas-
sical Kuramoto model describing synchronization/clustering of oscillators [Kur75,6 GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
ABV`05], the model which is most similar to (1.1) is the Krause model [Kra00]
9xiptq“nÿ
j“1aijpxjptq´xiptqq, a ij“ϕp}xi´xj}2qřn
k“1ϕp}xi´xk}2q.
whichisnon-symmetricingeneral( aij‰aji), muchlike(1.1). When ϕiscompactly
supported, it has been shown in [JM14] that the particles xiptqassemble in several
clusters as tÑ `8. Other models of opinion dynamics and flocking have been
proposed and studied, among which the Vicsek model [VCBJ`95], the Hegselmann-
Krause model [HK02] and the Cucker-Smale model [CS07]. These models may
also exhibit a clustering behavior under various assumptions (see [MT14, CHH`16,
HKPZ19] and the references therein). The transformer dynamics are also closely
related to the dynamics employed in mean-shift clustering [Che95], and this work
indirectly sheds some light on its theoretical properties.
The analysis of transformer dynamics presents unique mathematical challenges
that cannot be addressed using the tools developed for these more primitive models.
In particular, our work demonstrates how different choices for the parameters lead
to remarkably diverse clustering patterns. Much more remains to be discovered
and this work is a first attempt a rigorous mathematical analysis of these synthetic
dynamics.
Acknowledgments. We thank Pierre Ablin, Léonard Boussioux, Enric Boix Ad-
sera, Gabriel Peyré, Yair Shenfeld and Emmanuel Trélat for helpful discussions.
C.L. was supported by the Simons Foundation Grant 601948, DJ. P.R. is supported
by NSF grants IIS-1838071, DMS-2022448, and CCF-2106377. Y.P. is supported
in part by the MIT-IBM Watson AI Lab.
2.Asymptotic low-rankness of the self-attention matrix
As mentioned in Section 1.3, numerical experiments in [WLK`20] show that
the self-attention matrix P, defined in (1.2), has an almost low-rank structure.
This observation has then been leveraged to reduce the quadratic complexity in the
sequence length nwhich is inherent to Transformers, resulting in a non-negligible
decrease in the cost of training.
As a warm-up to deriving complete geometric representations of the dynamics,
ourfirstresultshows, inthesimple 1dcasethat Pptqindeedconvergesexponentially
fast toward a matrix which is typically both Boolean and low-rank (see Fig. 3).
Although there are clear obstructions to a rigorous extension of this result to higher
dimensions (Remark 7.9), numerical experiments appear to show that this result
holds in greater generality, for almost all initial sequences (Section 11).
Tosetthisup,weintroducetheset Pofnˆnmatriceshavingtheformillustrated
in Fig. 2, where the asterisks denote arbitrary non-negative real numbers which add
up to 1. The row of asterisks may actually be any row between the first and the
last one.
Theorem 2.1 (Self-attention matrix converges to a low-rank Boolean matrix) .
Letd“1. Suppose that the scalars pQ, K, Vqsatisfy Vą0andQKą0. For
any initial sequence of pairwise distinct tokens px1p0q, . . . , x np0qqPRn, there exists
some P˚PPsuch that the self-attention matrix Pptqdefined in (1.2)converges to
P˚astÑ`8.THE EMERGENCE OF CLUSTERS IN SELF-ATTENTION DYNAMICS 7
P2
666666666641 0 ::: 0
............
1 0 ::: 0
  ::: 
0::: 0 1
............
0::: 0 13
77777777775
1
Figure 2. Elements in P,
where PσiPRnˆnare some
permutation matrices, and as-
terisks denote arbitrary non-
negative reals which add to 1.The proof may be found in Section 7. The rate of
convergence toward P˚is in fact doubly exponen-
tial in tfor coefficients outside the row of asterisks
in Fig. 2. The proof the theorem also reveals that
for almost all initial sequences of pairwise distinct
tokens, P˚is actually of rank 1or2, i.e., the row
of asterisks is equal to either e1“ p1,0, . . . , 0qor
en“p0, . . . , 0,1q.
The interpretation of Theorem 2.1 is that in the
1dcase, at most three tokens capture the atten-
tionof all tokens except at most one. Typically,
theseleadingtokens are those carrying the largest
amount of information. This is also illustrated in
Fig. 4. Since the tokens xihere evolve on R, the
right-most and left-most ones (which typically tend
toward˘8) capture the attention of all the others.
0 5 10 15 20 25 30 35
0
5
10
15
20
25
30
35t= 0.0, rank= 11
0 5 10 15 20 25 30 35
0
5
10
15
20
25
30
35t= 3.0, rank= 23
0 5 10 15 20 25 30 35
0
5
10
15
20
25
30
35t= 5.0, rank= 14
0 5 10 15 20 25 30 35
0
5
10
15
20
25
30
35t= 10.0, rank= 2
Figure 3. An illustration of the asymptotics of Pptqentailed by Theo-
rem 2.1 for n“40tokens, with Q“K“1andV“1. (See Section 11
for details on computing.) Increasing nhas no effect on this behavior of
Pptq—see Fig. 11.
t= 0.0
t= 2.0
t= 9.0
Figure 4. ThecloudstKxiptquiPr20s(green)andtQxjptqujPr20s(purple)
ford“2wherepairwisepointsofcloudsareconnectedbyalineofwidth
equalto Pijptq. Here Vą0andQą0arerandommatricesand K“I2.
The creation of clusters is reflected by the rank ď2structure of the self-
attention matrix Pptq. This interaction echoes findings illustrated in the
original paper [VSP`17]—for instance, Figures 3-5 therein.8 GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
3.Clustering toward vertices of convex polytopes
In the rest of the paper, we seek to taxonomize various clustering results for the
solutions to (3.1) when tÑ `8, depending the sign and the multiplicity of the
eigenvaluesof V. Webeginbyfocusingonwhatmayappeartobethemostnatural2
caseV“Id, as is also done in [SABP22]. In fact, we demonstrate (theoretically
and numerically) later on, clustering is a generic phenomenon which holds under
much less restrictive assumptions.
The transformer dynamics considered in (1.1) does not contain a layer normal-
ization mechanism typically encountered in practice [VSP`17]. In absence of such
a device, tokens may diverge to infinity as in Theorem 2.1. In fact, the norm of the
tokens xiptqtypically diverges exponentially toward `8for any d: this is expected,
by analogy with the non-trivial solutions to 9yptq“yptq.
To remedy this situation, we take inspiration from the solution yptq“etVyp0q
to9yptq“V yptq. Namely, for any iPrnswe consider the rescaled tokens
ziptq:“e´tVxiptq,
which solve
9ziptq“nÿ
j“1˜
exQetVziptq,KetVzjptqy
řn
k“1exQetVziptq,KetVzkptqy¸
Vpzjptq´ziptqq, tPr0,`8q.(3.1)
The initial condition remains the same: xip0q “zip0qfor any iP rns. More im-
portantly, the coefficients of the self-attention matrix for the rescaled tokens ziptq
are the same as those for the original tokens xiptq. Whence, the conclusion of The-
orem 2.1 also applies to the dynamics (3.1). We see this rescaling of tokens as a
mathematically justified surrogate for the layer normalization.
The appearance of the exponential factor within the self-attention kernel facili-
tates the analysis of (3.1) compared to (1.1), and it is in fact instrumental in the
proofs of all results that follow. Each result on the rescaled tokens ziptqthen gives
information on the dynamics of the original tokens xiptqby virtue of the relation
xiptq“etVziptq.
We are now able to state the main result of this section on the case V“Id. The
following theorem shows that the tokens ziptqevolving per dynamics (3.1) converge
to the boundary of a convex polytope as tÑ`8. We present here a simplified but
weaker version of our result for convenience, and refer the reader to Theorem 8.1
for a complete statement.
Theorem 3.1 (Convergence to points on the boundary of a convex polytope) .Sup-
pose V“IdandQJKą0. Then, for any initial sequence of tokens tzip0quiPrnsĂ
Rd, there exists a convex polytope KĂRdsuch that for any iPrns,ziptqconverges
either to 0or to some point on BKastÑ`8.
The convex polytope Kis completely determined by the initial sequence of to-
kens, and QJK(refer to Claim 1). Numerical experiments (e.g. Fig. 5) also lead
us to claim that for almost all initial sequences of tokens, one should expect con-
vergence of ziptq(iPrns) toward some vertex of K. (Furthermore, the number of
vertices of Kis often found to be significantly smaller than n.) It may however
2Note that the case V“´Idmay appear equally natural. For such a choice of V, we show
in Section 8.2 that the dynamics converge to a single cluster located at the origin. Multiplicative
constants preserving the sign, i.e., V“˘cId, cą0trivially yield the same conclusions.THE EMERGENCE OF CLUSTERS IN SELF-ATTENTION DYNAMICS 9
happen that for initial sequences taken in some null set (not seen when tokens
are drawn at random) some tokens converge to other points of the boundary BK,
namely in the interior of facets. On the other hand, for generic choices of initial
sequences, we do not see a way to predict Kexplicitly besides running the full
dynamics.
−505−505−505
t= 0.0
−505−505−505
t= 1.0
−505−505−505
t= 2.0
−505−505−505
t= 5.0
Figure 5. A toy example illustrating Theorem 3.1 with n“40tokens
inR3. Here Q“K“I3. The tokens converge to one of the vertices
(leaders) of the limiting convex polytope.
Recall that the points xiptq“etziptqwhen V“Idfollow the original dynamics
(1.1). Akin to Theorem 2.1, this result also shows the emergence of a set of leaders
(given by the vertices of K) attracting all tokens as tgrows. It has been experi-
mentally observed (first in [VSP`17]) that in trained Transformers, tokens focus
their attention on local leaders in a way that seems to reproduce the syntactic and
semantic structure of sentences.
The proof of Theorem 3.1 is postponed to Section 8, and amounts to a couple
of effects entailed by the dynamics. First of all, the convex hull of the particles is
shrinking over time (Proposition 8.2). This is due to the fact that the distance of
the particle nearest to any half-space (not containing the particles) increases with10 GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
time. On the other hand, the convex hull ought not collapse since particles which
have not concentrated near the boundary of the limiting polytope will continue to
increase in magnitude until they themselves reach this boundary (Step 2 in the
proof). This occurs due to the time-rescaling.
Remark 3.2. Assuming QJKą0does not seem to be essential for our conclu-
sions; instead, it guides the direction of the proof. To emphasize the broader validity
of our conclusion beyond this specific assumption, we conducted additional experi-
ments (refer to Section 12.1) which suggest that Theorem 3.1 (as well as Theorems
4.2 and 5.2 stated below) holds in more generality.
Remark 3.3 (Rate of convergence) .Although Theorem 3.1 (as well as Theorems
4.2 and 5.2 stated below) does not specify a rate of convergence toward BK, we expect
(and observe through numerics) that convergence happens very quickly—after few
layers, most tokens are already clustered. What "few layers" means here necessarily
depends on the typical modulus of the initial tokens, since the dynamics (1.1)is not
invariant under multiplication of all initial conditions by a fixed real number.
Remark 3.4 (Discrete time) .As alluded to in Remark 1.1, all our results extend to
the discrete-time Transformers (1.3). Indeed, just as in the continuous-time case,
there is a natural rescaled dynamics, which is the discrete analogue of (3.1): if we
setR“Id`V∆t, and assume that Ris invertible (which is the case for sufficiently
small ∆t), then zipk∆tq“R´kxipk∆tq:“zrks
isatisfies
zrk`1s
i“zrks
i`∆tnÿ
j“1˜
exQRkzrks
i,KRkzrks
jy
řn
ℓ“1exQRkzrks
i,KRkzrks
ℓy¸
R´1V´
zrks
j´zrks
i¯
, kPN.
The proofs of Theorems 2.1, 8.5, 3.1, 4.2, and 5.2 carry through with straightforward
modifications.
Let us provide some comments on the proof of Theorem 3.1 in the discrete-time
setting, for the sake of completeness. First of all, Proposition 8.2 holds intuitively
because for all integers iPrnsandkě1,
zrk`1s
i“1
1`∆t˜
zrks
i`∆tnÿ
j“1Prks
ijzrks
j¸
Pconvˆ!
zrks
j)
jPrns˙
.
We then define the candidate set of limit points as in (8.6), and Claim 1 holds
without any change in the statement or in the proof. Then, just as in Steps 2 and 3
in the proof of 8.1, we can first show that if zrks
iis not already near some point in the
candidate limit set, it will keep moving toward the boundary of the convex polytope.
Finally, we can prove that tokens cannot circulate indefinitely between different
points on the boundary. The combination of these arguments would establish the
convergence of each token toward some point in the set given by (8.6).
4.Clustering toward hyperplanes
While being a natural example to consider, value matrices found empirically are
much more general than V“Id, which we considered in the previous section. We
now turn our attention to a significantly more general setting of value matrices,
which we formalize as follows.
Definition 4.1. We callpQ, K, Vqagood triple if the two following conditions are
satisfied:THE EMERGENCE OF CLUSTERS IN SELF-ATTENTION DYNAMICS 11
‚the eigenvalue of Vwithlargest modulusis real, positive, and simple; namely,
λ1pVqą|λ2pVq|ě. . .ě|λdpVq|.
‚ xQφ1, Kφ 1yą0for any φ1PRdlying on the line kerpV´λ1pVqIdq.
The second condition simply states that the quadratic form xQ¨, K¨yis positive
definite along the eigenspace associated to the leading eigenvalue of V. Note also
that if all entries of Vare positive, the first condition is automatically satisfied by
virtue of the Perron-Frobenius theorem. In fact, this assumption is generic. On the
one hand, it is satisfied by some pre-trained value matrices for ALBERT (Figure
10). On the other hand, numerical experiments indicate that a constant fraction
(about 14%) of matrices from the real Ginibre ensemble in dimension d“128—this
proportion is known to vanish as dÑ8, albeit very slowly [RS14].
Our clustering result in the setting of good triples can be summarized as follows:
the coordinate xziptq,φ1
}φ1}yof any token ziptqalong the eigenspace spanned by φ1
converges, as tÑ`8, toward one among possibly 3real scalars. Consequently, all
the tokens ziptqconverge toward one among at most three parallel hyperplanes; see
Fig. 6 for an illustration.
Theorem 4.2 (Convergence toward ď3hyperplanes) .Assume thatpQ, K, Vqis a
good triple in the sense of Definition 4.1. Then, for any initial sequence of tokens
tzip0quiPrnsĂRd, there exist at most three parallel hyperplanes in Rdsuch that for
anyiPrns, the distance of the solution ziptqto(3.1)to one of these hyperplanes
converges to 0astÑ`8.
0 10−10010
t= 0.0
0 10−10010
t= 1.0
0 10−10010
t= 5.0
0 10−10010
t= 15.0
Figure 6. IllustratingTheorem4.2with n“40tokensin R2. Here Q“
K“I2,Visarandomsymmetricmatrixwitheigenvalues t1.35,´0.07u,
andφ1“p0.76,0.65q. The components of the tokens in the direction of
φ1(orange arrow) cluster over time. (See Figures 13–14 for examples
inR3.) We also observe that tokens typically cluster toward only two
hyperplanes—a third one (passing through the origin) may appear for
non-generic initial sequences. The hyperplanes are perpendicular to φ1
since Vis diagonalizable.
The proof may be found in Section 9. The important role played by λ1pVqin the
dynamics may be seen in (3.1): the component of ziptqalong φ1determines the size
ofetVziptqin the exponent appearing in (3.1). The tokens zjptqattracting other
tokens ziptqare those for which this component along φ1is largest in modulus. This
attraction process forms the clusters. These leaders, as in all our results, have been
empirically observed to be the ones carrying the largest amount of information in
the sentence (see Supplementary material in [VSP`17]).12 GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
Furthermore, Theorem 4.2 can also be interpreted in more classical machine
learning terms. On the one hand, it can be seen as an instance of K-flats clus-
tering[BM00, Vid11]—points in the input sequence are clustered, based on their
intrinsic similarity, to at most 3"flats" of dimension d´1. On the other hand, it
ensures that for a good triple pQ, K, Vq, (3.1) generates a linearly separable repre-
sentation of tokens.
Beyond a single direction? Numerical experiments (e.g., Fig. 7) indicate that
a similar phenomenon emerges for more complex V. We formulate following con-
jecture which is a natural generalization of Theorem 4.2.
Conjecture 4.3 (Codimension conjecture) .Letkě1be the number of eigenvalues
ofVwith positive real part. Then there exist at most three parallel Euclidean
subspaces of Rdof codimension ksuch that for any iPrns, the distance of ziptqto
one of these subspaces converges to 0astÑ`8.
−505−505−505
t= 0.0
−10010−10010−505
t= 5.0
−40−20020−25025−10010
t= 10.0
−100−50050
−1000100−40−20020
t= 15.0
(a)Conjecture 4.3: low-dimensional case.
0.0 2.5 5.0 7.5 10.0 12.5 15.0
t−0.20.00.2Positive limits for clustered coordinates
0.0 2.5 5.0 7.5 10.0 12.5 15.0
t−0.75−0.50−0.250.000.250.50Negative limits for clustered coordinates(b)Conjecture 4.3: high-dimensional
case.
Figure 7. (a)n“40,d“3andQ“K“I3with Va random matrix
with eigenvalues t1.96,´0.22,0.25u. The k“2positive eigenvalues of
Vgenerate attraction between the tokens and even convergence in the
corresponding eigenspaces–this explains the codimension kstatement.
The negative eigenvalue generates a repulsive effect between the tokens,
andweseeadivergencealongtwolines(notethedifferentscalesbetween
the four figures). (b) n“256,d“128, withpQ, K, Vqfixed random
matrices and Vsymmetric. For each coordinate jcorresponding to a
positive eigenvalue, the variance of the set tφ˚
jpziptqq:iPrnsu(shaded
area) tends to 0with t, while the mean (solid lines) converges to one
among two real scalars: one positive (top figure), one negative (bot-
tom) figure. Coordinates corresponding to negative eigenvalues diverge
(Fig. 15).THE EMERGENCE OF CLUSTERS IN SELF-ATTENTION DYNAMICS 13
5.A mix of hyperplanes and polytopes
WenowturnourattentiontoanevenmoregeneralversionofTheorem4.2, which
does not require the leading eigenvalue of Vto be simple. The resulting theorem
can be viewed as a combination of Theorem 4.2 and Theorem 3.1. Specifically,
we assume that Vbehaves as the identity when acting on the eigenspace of the
leading eigenvalue. This property is automatically satisfied if Vis normal—so that
its eigenvectors form an orthonormal basis—so we call such a Vparanormal.
Definition 5.1. We callpQ, K, Vqagood triple with multiplicity if the following
conditions hold:
(i)QJKis positive definite: QJKą0;
(ii)Vis paranormal: there exist two linear subspaces F,GĂRdwhich are
invariant under V, and such that F‘G“Rd,V|F“λIdforλą0, and
ρpV|Gqăλ, where ρp¨qdenotes the spectral radius (the maximal modulus of
eigenvalues).
An example of such a Vis used for Fig. 8. We may now state our main result in
the setting of good triples with multiplicity. The proof may be found in Section 10.
Theorem 5.2 (Clustering for λ1with multiplicity) .Suppose that pQ, K, Vqis a
good triple with multiplicity in the sense of Definition 5.1. Then, for any initial
sequencetzip0quiPrnsĂRd, there exists a bounded convex polytope KĂFsuch that
setting H:“pBKYt0uqˆG, for any iPrns, we have distpziptq,HqÑ0astÑ`8.
Part2.Proofs
6.Well-posedness
We collect several facts regarding the global-in-time existence and uniqueness
of solutions to all systems under consideration. Throughout the remainder of the
paper, we use the terminology "tokens" and "particles" interchangeably.
Toprovetheseresults, weleveragetheunderlyingcontinuityequation(see(6.1)).
For the sake of future use, we prove a more general well-posedness result for the
continuity equation than what is needed in this paper.
6.1.Notation. We denote by PcpRdqthe set of compactly supported probability
measures on Rd, and by P2pRdqthe set of probability measures µonRdhaving
finite second moment:ş
Rd}x}2dµpxqă`8. Let C0pR;PcpRdqqdenote the Banach
space of continuous curves RQtÞÑµptqPPcpRdq. Here PcpRdqis endowed with
the weak topology, which coincides with the topology induced by the Wasserstein
distance Wpfor any pPr1,`8q.
As seen below, for compactness purposes regarding solutions to the continuity
equation, we consider an additional property on the support of such curves, sum-
marized by the following definition.
Definition 6.1 (Equi-compactly supported curves) .The set C0
copR;PcpRdqqcon-
sists of all elements µPC0pR;PcpRdqqsuch that for any t0, t1PR, there exists a
compact subset KĂRdsuch that supppµptqqĂKfor any tPrt0, t1s.
We emphasise that there exist elements in C0pR;PcpRdqqwhich do not satisfy
this property with regard to their support—e.g., µptq“p1´e´1
t2qδ0`e´1
t2δ1
t.14 GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
−505−505−505
t= 0.0
−505−505−50−250
t= 5.0
−505−505−5000
t= 10.0
−505−505−10000−50000
t= 15.0
Figure 8. Illustrating Theorem 5.2 with n“40tokens in R3. As be-
fore, Q“K“Id, and we take V“diagp1,1,´1
2q. A convex polytope
Kemerges before time 5, toward which two coordinates of the tokens
cluster, and persists throughout the evolution, while the tokens diverge
along the coordinate corresponding to the eigenvalue ´1
2(note the dif-
ferent scales between the four figures).
6.2.Well-posedness of the ODEs. For any initial datum, i.e. a sequence of n
points in Rd, the dynamics (1.1) is well-posed, in the sense that it admits a unique
solution defined for all times.
Proposition 6.2. For any initial datum X0“px0
1, . . . , x0
nqPpRdqn, there exists a
unique Lipschitz continuous function RQtÞÑXptq“p x1ptq, . . . , x nptqqsuch that
xip¨qsolves(1.1)and satisfies xip0q“x0
ifor any iPrns.
We postpone the proof which is seen as a corollary of the well-posedness for
the corresponding continuity equation. It follows that the equation (3.1) is also
well-posed:
Proposition 6.3. For any initial datum Z0“pz0
1, . . . , z0
nqPpRdqn, there exists a
unique Lipschitz continuous function RQtÞÑZptq “ p z1ptq, . . . , z nptqqsuch that
zip¨qsolves(3.1)and satisfies zip0q“z0
ifor any iPrns.THE EMERGENCE OF CLUSTERS IN SELF-ATTENTION DYNAMICS 15
Proof of Proposition 6.3. Since the equations (1.1) and (3.1) are related by the
change of variables xiptq“etVziptq, Proposition 6.3 is an immediate consequence
of Proposition 6.2. □
6.3.The continuity equation. To prove Proposition 6.2, we show a more general
result concerning global existence and uniqueness of solutions to the corresponding
continuity equation3
#
Btµ`divpXrµsµq“0inp0,`8qˆ Rd
µ|t“0“µ0 inRd,(6.1)
when Xrµsis theattention kernel
Xrµspxq:“ż
RdexQx,KyyV ydµpyq
ż
RdexQx,Kyydµpyq. (6.2)
We will make use of the following notion of solution.
Definition 6.4. Fixµ0PPcpRdq. We say that tÞÑµptq“:µtis a solution to the
Cauchy problem (6.1)ifµPC0
copR,PcpRdqq, the function
RQtÞÑż
Rdgpxqdµtpxq
is absolutely continuous for every gPC8
cpRdq, and
ż
Rdgpxqdµtpxq“ż
Rdgpxqdµ0pxq`żt
0ż
Rdx∇gpxq,Xrµtspxqydµspxqds
holds for almost every tPR.
We will make use of the following lemma regarding (6.2).
Lemma 6.5. For any Rą0there exists a constant C1pRqą0such that for any
µ, νPPcpRdqwith support in Bp0, Rq,
}Xrµs}L8pRd;Rdqď}V}opR, (6.3)
}∇xXrµs}L8pRd;Rdˆdqď2}QJK}op}V}opR2(6.4)
}Xrµsp¨q´ Xrνsp¨q} L8pBp0,Rq;RdqďC1pRqW2pµ, νq. (6.5)
Proof.We henceforth set Gpx, yq:“exQx,Kyy. To show (6.3), since Gą0we see
that for any xPRd,
}Xrµspxq}⩽}V}opż
Bp0,RqGpx, yq}y}dµpyq
ż
Bp0,RqGpx, yqdµpyq⩽}V}opR.
3which can be seen as a mean-field limit, and is sometimes also referred to as a Vlasov equation .16 GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
We now show (6.4). Note that ∇xGpx, yq“QJKyGpx, yq, thus, arguing as above,
we find
}∇xXrµspxq}⩽ż
Bp0,Rq}∇xGpx, yq}}V y}dµpyq
ż
Bp0,RqGpx, yqdµpyq
`}V}opż
Bp0,RqGpx, yq}y}dµpyq
ż
Bp0,RqGpx, yqdµpyqż
Bp0,Rq}∇xGpx, yq}dµpyq
ż
Bp0,RqGpx, yqdµpyq
ď2}QJK}op}V}opR2.
We finally prove (6.5). Using the fact that
ż
RdGpx, yqdµpyq⩾ˆ
inf
px,yqPBp0,Rq2Gpx, yq˙
µpBp0, Rqq,
–with an analogous bound for ν–, we see that it suffices to bound
ˇˇˇˇż
RdGpx, yqV ydµpyqż
RdGpx, yqdνpyq´ż
RdGpx, yqV ydνpyqż
RdGpx, yqdµpyqˇˇˇˇ
from above. We rewrite this difference by making µ´νappear artificially, and we
then use the triangle inequality along with the fact that bothş
RdGpx, yqV ydµpyq
andş
RdGpx, yqdµpyqare bounded from above (by e}QJK}opR2maxp1,}V}opRq). We
thus end up with the task of bounding from above the absolute values of
ż
RdGpx, yqpdν´dµqpyqandż
RdGpx, yqV ypdν´dµqpyq.(6.6)
For the first integral, from the Kantorovich-Rubinstein duality we deduce
ˇˇˇˇż
RdGpx, yqpdν´dµqpyqˇˇˇˇď}Gpx,¨q}C0,1pBp0,RqqW1pµ, νq. (6.7)
We now recall the following inequality relating Wasserstein distances of different
orders: for any pě1and any bounded set B, for all Radon measures µ, νsupported
inB,
W1pµ, νqďWppµ, νqďdiampBq1´1
pW1pµ, νq1{p. (6.8)
Using (6.8)andthefactthattheLipschitzconstant }Gpx,¨q}C0,1pBp0,Rqqisuniformly
bounded for}x}ďRby some CRą0in (6.7), we end up with
ˇˇˇˇż
RdGpx, yqpdν´dµqpyqˇˇˇˇďCRW2pµ, νq.
The same chain of inequalities applies to the second integral in (6.6) (with the
additional multiplier }V}opR), which finally leads us to (6.5). □
The following existence and uniqueness result is adapted from [PRT15, Theo-
rem 2.3]. In fact, the result holds true for any vector field XrµsonRdsatisfying
conditions analog to those entailed by Lemma 6.5.THE EMERGENCE OF CLUSTERS IN SELF-ATTENTION DYNAMICS 17
Proposition 6.6. For any initial condition µ0PPcpRdq, the Cauchy problem (6.1)
admits a unique solution µPC0
copR;PcpRdqqin the sense of Definition 6.4.
Furthermore, we have the following stability estimate for solutions: for any Rą0
andTą0, there exists a constant CpT, Rqą0such that for any µ0, ν0PPcpRdq
with support in Bp0, Rq,
W2pµptq, νptqqďeCpT,RqtW2pµ0, ν0q (6.9)
for any tPr0, Ts, where µptqandνptqsolve(6.1)with initial conditions µ0andν0
respectively.
Results of this nature can be found in the literature—see for instance [PRT15].
They are however not sufficient for our purposes. We wrote Proposition 6.6 in the
W2setting instead of the usual W1setting (used for instance for the classical Do-
brushin estimate [Dob79,Gol13])becauseitallowstoextendtheresultsof[WHL19]
without difficulty from classical ResNets to self-attention dynamics. We recall that
the goal of [WHL19] is to import classical (mean-field) optimal control tools such
as the Pontryagin maximum principle and the analysis of Hamilton-Jacobi-Bellman
equations to deep learning, and relies heavily on W2estimates (e.g., in [WHL19,
Section 4]).
Proof of Proposition 6.6. To ease reading, we split the proof in three parts.
Part 1: Existence. Fix an arbitrary Tą0. For kě1, set
τk:“T
2k.
We define a sequence of curves µk:r0, TsÑPcpRdqby the following scheme4:
(i)µkp0q:“µ0;
(ii)µkpℓτk`tq:“´
Φt
Xrµkpℓτkqs¯
#µkpℓτkqforℓPt0, . . . , 2k´1uandtPp0, τks,
where for any xPRd,Φt
Xrµkpℓτkqspxqis the unique solution to the Cauchy problem
#
9yptq“Xrµkpℓτkqspyptqqonr0, τks
yp0q“x.
(The above problem indeed has a unique solution for any xPRdby virtue of the
Cauchy-Lipschitz theorem, using (6.4).) By construction, µkPC0pr0, Ts;PcpRdqq
for any k⩾1.
We begin by showing that there exists a radius R“RpTqą0independent of k
such that supppµkptqqĂBp0, Rqfor any k⩾1andtPr0, Ts. To this end, for any
tPr0, Tsandk⩾1, letRkptqą0denote the smallest positive radius5such that
supppµkptqqĂBp0, Rkptqq. We will first look to show that
supppµkpℓτk`tqqĂBp0, Rkpℓτkq`t}V}opRkpℓτkqq. (6.10)
LetxPsupppµkpℓτk`tqq, thus µkpℓτk`tqpBpx, εqq ą 0for any εą0. By the
change of variables formula, we find thatż
pΦt
Xrµkpℓτkqsq´1pBpx,εqqdµkpℓτkqpzqą0.
4In other words we "freeze" the vector field Xon each interval of the form rℓτk,pℓ`1qτkq, and
during this time interval, we follow the flow generated by this vector field starting from µkpℓτkq.
5This radius always exists, since µkptqis compactly supported.18 GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
Consequently pΦt
Xrµkpℓτkqsq´1pBpx, εqqXsupppµkpℓτkqq‰H, andlet zbeanelement
lying in this set. From the Duhamel formula, we gather that
Φt
Xrµkpℓτkqspzq“:yptq“z`żt
0Xrµkpℓτkqspypsqqds.
Since zPpΦt
Xrµkpℓτkqsq´1pBpx, εqq, we find that
››››z`żt
0Xrµkpℓτkqspypsqqds´x››››ďε.
Usingthetriangleinequality,(6.3),andsince zPsupppµkpℓτkqqimplies zPBp0, Rkpℓτkqq,
we deduce that
}x}⩽ε`t}V}opRkpℓτkq`Rkpℓτkq.
Since εą0is arbitrary, this inequality yields (6.10). We now use (6.10) to prove
the original claim. Using the definition of the radius Rkptq, we evaluate (6.10) at
t“τkand find
Rkppℓ`1qτkq⩽p1`}V}opτkqRkpℓτkq.
By induction, we deduce that
Rkpℓτkq⩽p1`}V}opτkqℓRkp0q,
whence
Rkpℓτkq⩽ˆ
1`}V}opT
2k˙2k
Rkp0qăe}V}opTR0,
where R0ą0denotes the smallest positive radius such that supppµ0qĂBp0, R0q.
Since the above bound is independent of k, the claim follows, yielding the desired
radius R“RpTqą0bounding the support of every element in the sequence. In
turn, we also deduce that µkPC0
copR;PcpRdqqfor any k⩾1.
Using the above fact, along with (6.3) and the definition of µkpℓτk`tq, we find
that
W2`
µkpℓτk`tq, µkpℓτkq˘
ď}V}opRt
for any ℓP t0, . . . , 2k´1u,tP p0, τksandk⩾1. Gluing these inequalities (for
different ℓandt) with the triangle inequality yields
W2`
µkptq, µkpsq˘
ď}V}opR|t´s|
for any tPr0, Ts. Since µkp0q“µ0for any kě1, and since P2pRdqis the comple-
tion of Pcfor the Wasserstein distance W2, the Arzelà-Ascoli theorem implies the
existence of a subsequence uniformly converging to some µ˚PC0pr0, Ts;P2pRdqq.
Since for any tP r0, Tsthe curves µkptqhave their support enclosed in Bp0, Rq
for any k⩾1, we even deduce that µ˚PC0
copR,PcpRdqq. Note moreover that
µ˚p0q“µ0and that
W2pµ˚ptq, µ˚psqqď} V}opR|t´s|
for any t, sPr0, Ts.
Thefactthat µ˚isasolutionof (6.1)followsexactlyfromthesamecomputations
as in [PRT15, p. 4711-4712], starting from (A.2) therein. We do not reproduce here
this argument since the computations are the same word for word. The fact that
for any Tą0we have suptPr0,TsW1pµ˚ptq, µkptqq Ñ 0askÑ `8, which is
instrumental in [PRT15, p. 4711-4712], follows in our case from the left-hand-side
of (6.8).THE EMERGENCE OF CLUSTERS IN SELF-ATTENTION DYNAMICS 19
Part 2: Uniqueness. Regarding uniqueness, we proceed as follows. We first
recall the following estimate from [PR16, Proposition 4]. Let pě1, lettě0, let
v, wPC0,1XL8pr0, tsˆRd;Rdq(both with Lipschitz constant Lą0, say), and let
µ, νPPcpRdq. Then
Wp`
pΦt
vq#µ,pΦt
wq#ν˘
ďep`1
pLtWppµ, νq`eLt
ppeLt´1q
L}v´w}L8pr0,tsˆRd;Rdq.
(6.11)
Now assume that there are two solutions µandνof (6.1), with a spatial support
that is locally bounded in time, and having the same initial condition. Define
vpt, xq:“Xrµptqspxqandwpt, xq:“Xrνptqspxq. Also set
t0:“infttě0:W2pµptq, νptqq‰0u,
and assume that t0‰ `8. Fix Tąt0and take Rą0such that µtandνt
are supported in Bp0, Rqfor any tPr0, Ts. Using (6.11) with p“2, and setting
C2pRq:“2}QJK}op}V}opR2in (6.4), we find
W2pµpt0`sq, νpt0`sqqďe2C2pRqsW2pµpt0q, νpt0qq
`eC2pRqseC2pRqs´1
C2pRqsup
τPrt0,t0`ss}vpτ,¨q´wpτ,¨q}L8pRdq.
Choose są0sufficiently small so that eC2pRqs´1ď2C2pRqs. Then, by virtue of
(6.5) and the fact that W2pµpt0q, νpt0qq“0, we deduce
W2pµpt0`sq, νpt0`sqqď2seC2pRqssup
τPrt0,t0`ssW2pµpτq, νpτqq.(6.12)
We choose s1ą0satisfying both eC2pRqs1´1ď2C2pRqs1and2s1eC2pRqs1ă1.
Applying (6.12) to every sPr0, s1swe obtain
sup
sPr0,s1sW2pµpt0`sq, νpt0`sqqď2s1eC2pRqs1sup
τPrt0,t0`s1sW2pµpτq, νpτqq
ăsup
sPr0,s1sW2pµpt0`sq, νpt0`sqq,
which is a contradiction. Therefore µptq”νptqfor any tě0, which proves unique-
ness, as desired.
Part 3: Stability. We do not detail the proof of estimate (6.9), which is very
similar to the proof of (2.3) in Theorem 2.3 of [PRT15]: it follows from (6.11) with
p“2, and the argument after (A.7) in [PRT15], with W2instead of W1. See also
[PR13, Theorem 3]. □
We conclude this section with the proof of Proposition 6.2, which follows as a
corollary of the above derivations.
Proof of Proposition 6.2. We first show existence. We apply Proposition 6.6 with
µ0:“1
nřn
j“1δx0
i, which in turn yields a solution µptqto (6.1). Following the proof
of Proposition 6.6, we also know that this solution satisfies µptq“p Φt
Xrµptqsq#µ0
for any tPR, and the vector field Xrµptqssatisfies the assumptions of the Cauchy-
Lipschitz theorem. In particular, µptqis of the form µptq“1
nřn
j“1δxiptqfor some
Lipschitz curves RQtÞÑxiptq, for iP rns. Then tÞÑµptq “1
nřn
j“1δxiptqis a
solution to the Cauchy problem (6.1)-(6.2) in the sense of Definition 6.4.20 GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
Secondly, weshowuniqueness. Supposethat Xptq“px1ptq, . . . , x nptqqandX˚ptq
are two Lipschitz solutions to (1.1), with the same initial conditions. Then for a.e.
tě0, using the equation (1.1) and the fact that the attention matrix coefficients
Pijptqdefined in (1.2) belong to r0,1s, we obtain
1
2d
dtmax
iPrns}xiptq}2ď}V}opmax
iPrns}xiptq}2
(and analogously for x˚
iptq). Using Grönwall’s inequality, we deduce the existence
of two constants c1, c2ą0such that for any tą0and for any iP rns,}xiptq}
and}x˚
iptq}are bounded from above by c1ec2t. It then follows that the empirical
measures µp¨q“1
nřn
j“1δxip¨qandµ˚p¨q“1
nřn
j“1δx˚
ip¨qbelong to C0
copR,PcpRdqq.
Moreover, they satisfy µptq “ p Φt
Xrµptqsq#µ0andµ˚ptq “ p Φt
Xrµ˚ptqsq#µ0and are
thus solutions to (6.1). Using the uniqueness result of Proposition 6.6, we obtain
thatµ“µ˚which concludes the proof. □
7.Proof of Theorem 2.1
Throughout this section we focus on the following dynamics:
9xiptq“nÿ
j“1ˆexxiptq,xjptqy
řn
k“1exxiptq,xkptqy˙
xjptq. (7.1)
Note that for d“1, the dot products in (7.1) are just multiplications of scalars.
We begin with the following observation, which holds for any dě1.
Lemma 7.1. For any x1, . . . , x nPRd, the function f:RdÑRdefined by
f:xÞÑlog˜nÿ
j“1exx,xjy¸
(7.2)
is convex.
Proof.Using the elementary inequality pa`bqě2pabq1
2for any a, bě0, we have
exppfpxq`fpyqq“˜nÿ
j“1exppxx, xjyq¸˜nÿ
j“1exppxy, xjyq¸
“1
2nÿ
j“1nÿ
k“1”
exppxx, xjy`xy, xkyq`exppxx, xky`xy, xjyqı
(7.3)
ěnÿ
j“1nÿ
k“1expˆBx`y
2, xj`xkF˙
(7.4)
“expˆ
2fˆx`y
2˙˙
.
Taking the logon both sides yields the statement. □
The following lemma also holds for any dě1.
Lemma7.2. LetRQtÞÑtxiptquiPrnsbe a solution to (7.1). Then for any i, jPrns,
the map RQtÞÑ}xiptq´xjptq}is non-decreasing.THE EMERGENCE OF CLUSTERS IN SELF-ATTENTION DYNAMICS 21
Proof.The dynamics (7.1) can be equivalently written as
9xiptq“∇fpxiptqq
where fis as in (7.2). By convexity of f(Lemma 7.1),
1
2d
dt}xiptq´xjptq}2“x9xiptq´9xjptq, xiptq´xjptqy
“x∇fpxiptqq´∇fpxjptqq, xiptq´xjptqyě0,
as desired. □
We now present the proof of Theorem 2.1, which assumes d“1. We recall that
in the statement, Vis a positive scalar, but by reparametrizing time we may assume
thatV“1, so the 1ddynamics under consideration is really given by (7.1). Also,
to ease notations we focus on QK“1, but the proof adapts straightforwardly to
the setting QKą0assumed in the statement of Theorem 2.1.
AsseeninSection7.1,itisnotdifficulttoprovetheconvergenceofthecoefficients
Pijptqof the attention matrix for indices iPrnsfor which xiptqbecomes unbounded
astÑ`8. This is the case for at least n´1of the particles xiptq(Lemma 7.6).
But should one particle xiptqremain bounded, proving the convergence of Pijptq
forjPrnsis slightly tedious (Section 7.2). Since d“1, up to relabeling, we can
order the initial collection of particles (which, we recall, are assumed distinct):
x1p0qă. . .ăxnp0q.
We set
c:“min
iPrn´1s|xi`1p0q´xip0q|. (7.5)
According to Lemma 7.2, we have |xiptq´xjptq|ěcfor any i‰jand any tě0.
In particular, particles never "collide".
7.1.Results about unbounded particles. In this section we gather several re-
sults concerning the indices icorresponding to particles xiptqwhich are not uni-
formly bounded in time. In particular, in Lemma 7.4 we show that for such indices
i,Pijptqconverges toward 0or1for any jPrns.
Lemma 7.3. LetAą0denote the unique positive real number satisfying A2“
n2expp´A2q. Ifxnpt0qąAfor some time t0ě0, then there exists c1ą0such
thatxnptq ěc1etfor any sufficiently large tą0. Similarly, if x1pt0q ă ´ Afor
some t0ě0, then x1ptqď´ c1etfor any sufficiently large tą0.
Proof.The two cases are symmetric since the evolution (7.1) commutes with the
involution ofpRdqngiven bypx1, . . . , x nqÞÑp´ x1, . . . ,´xnq. We thus focus on the
casexnpt0qąA.22 GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
Ifxnptqě0for some tě0, then
9xnptq“nÿ
j“1ˆexnptqpxjptq´xnptqq
řn
k“1exnptqpxkptq´xnptqq˙
xjptq (7.6)
ěxnptq
1`pn´1qe´cxnptq`ÿ
tjPrns:xjptqă0uexnptqpxjptq´xnptqqxjptq(7.7)
ěxnptq
1`pn´1qe´cxnptq´ne´xnptq2
xnptq(7.8)
ěxnptq
n´ne´xnptq2
xnptq. (7.9)
We provide some detail on the above sequence of inequalities. First of all, to pass
from (7.6) to (7.7), we use
exnptqpxkptq´xnptqqďe´cxnptq
forj“nand for any kPrns(which holds by virtue of (7.5)), combined with the
fact thatnÿ
k“1exnptqpxkptq´xnptqqě1
for all indices jsuch that xjptqă0. To pass from (7.7) to (7.8), we use exnptqzzě
´1
xnptq, which holds for any zď0.
For any BąA, we clearly have
B
n´ne´B2
Bą0.
We then deduce from (7.8) and the fact that xnpt0q ąAthat xnptq Ñ `8 as
tÑ`8. Moreover due to the fact that the expression in (7.9) is bounded from
below byxnptq
2nwhenever xnptqis sufficiently large, we deduce that
xnptqěc0et
2n
for any sufficiently large tą0.
Coming back to (7.8), we find that for sufficiently large tą0,
9xnptqěxnptq˜
1
1`pn´1qe´cc0et
2n´e´c2
0et
n¸
.
This implies that
d
dtlogpxnptqqě1´O´
e´t
3n¯
,
whence
logpxnptqqět`Op1q
for sufficiently large tą0, as desired. □
Here and in what follows, δjkdenotes the Kronecker symbol.
Lemma 7.4. IfiPrnsis such that xiptqis not uniformly bounded with respect to
tą0, then xiptqconverges to either ´8or`8astÑ`8. Moreover,
(1) if xiptqÑ`8, then for any jPrns,Pijptqconverges to δnjastÑ`8,
with doubly exponential rate.THE EMERGENCE OF CLUSTERS IN SELF-ATTENTION DYNAMICS 23
(2) if xiptqÑ´8, then for any jPrns,Pijptqconverges to δ1jastÑ`8,
with doubly exponential rate.
Proof.We assume that xiptqis not uniformly bounded with respect to tą0.
Without loss of generality, we assume that there exists a sequence of positive times
ttku`8
k“1with tkÑ`8such that xiptkqÑ`8 . Necessarily, xnptkqÑ`8 . We
notice that if xiptq ą0for some tě0, then, arguing as in (7.6)–(7.7)–(7.8), we
have
9xiptq“nÿ
j“1ˆexiptqpxjptq´xnptqq
řn
k“1exiptqpxkptq´xnptqq˙
xjptqěxnptq
n´n
xiptqe´xiptqxnptq.(7.10)
For sufficiently large integers kě1, from (7.10) we get 9xiptkqą0and9xnptkqą0.
But as xiandxnincrease, the lower bound in (7.10) becomes larger. It follows that
9xiptqěxnptq
2něxiptq
2n
for sufficiently large t, implying that xiptqÑ`8 with exponential rate as tÑ`8.
We now prove point 1. regarding Pptq. We assume that xiptqÑ`8 astÑ`8.
In this case, for j‰n(namely jPrn´1s),
Pijptq“exiptqxjptq
nÿ
k“1exiptqxkptqďexiptqpxjptq´xnptqqďe´cxiptq,
thusPijptqconvergesto 0astÑ`8(withdoublyexponentialrate). Consequently,
we also deduce that
Pinptq“1´n´1ÿ
j“1Pijptq
converges to 1, also with doubly exponential rate, as tÑ`8.
The case where xiptqÑ´8 is symmetric. This concludes the proof. □
Our last result is useful in the next section.
Lemma 7.5. For any iPrnssuch that xiptqis not uniformly bounded with respect
totą0, there exists some γiPR,γi‰0such that xiptq“γiet`opetqastÑ`8.
Proof.Without loss of generality we assume that xiptq Ñ `8 astÑ `8. For
j‰n, we find
Pijptq“exiptqxjptq
nÿ
k“1exiptqxkptq“exiptqpxjptq´xnptqq
nÿ
k“1exiptqpxkptq´xnptqqďe´cxiptq.
Consequently,
Pinptqě1´ne´cxiptq.
Therefore, using Lemma 7.3 and the fact that xiptqěbiet
2nfor some bią0(thanks
to (7.10)), we gather that
9xiptqě´
1´ne´cxiptq¯
xnptq´ne´cxiptqc1et
ěˆ
1´ne´cbiet
2n˙
xnptq´ne´cbiet
2nc1et(7.11)24 GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
for some c1ą0independent of t. We also notice that due to (7.1), 9xiptqďxnptq.
Using (7.11), firstly for i“n, together with the trivial upper bound xnptqďCet
for some Cą0independent of t(immediately seen from (7.1)), we obtain
9xnptq“xnptqˆ
1`oˆ
e´cbiet
3n˙˙
astÑ`8, which yields
xnptq“γnet`opetq
for some γną0. Now using (7.11) for the index i, we gather that
9xiptq“xnptq`oˆ
e´cbiet
3n˙
,
and so we deduce that
xiptq“γnet`opetq.
Similarly, if xiptqÑ´8, then xiptq“γ1et`opetq. This proves Lemma 7.5 (and
shows that γiPtγ1, γnu). □
7.2.Results about bounded particles. In this section we collect results con-
cerning particles which remain uniformly bounded in time. The following lemma
entails that there can be at most one particle with this property.
Lemma 7.6. Consider
B:“!
iPrns:xip¨qPL8pr0,`8qq)
.
Then #BPt0,1u.
Proof.We first prove that either x1ptqÑ´8 orxnptqÑ`8 astÑ`8. By con-
tradiction, if this is not the case, then by Lemma 7.3, px1ptq, . . . , x nptqqPr´ A, Asn
for any tě0. We denote by Ithe set of configurations px˚
1, . . . , x˚
nqPr´ A, Asn
such that|x˚
i´x˚
j| ě | xip0q´xjp0q| ą 0for any distinct i, jP rns. For any
X˚“px˚
1, . . . , x˚
nqPI, the function fdefined in (7.2) (with anchor points given
byX˚) is strictly convex—the equality in the inequality between (7.3) and (7.4) is
never achieved. Therefore, the proof of Lemma 7.2 shows that if X˚is seen as an
initial datum for the dynamics (7.1), then
vpX˚q:“d
dt|t“0|x˚
1ptq´x˚
nptq|ą0.
SinceIis compact, v0:“infX˚PIvpX˚qą0. Hence, tÞÑ|x1ptq´xnptq|grows at
least linearly, which is a contradiction.
Wemaythereforeassumewithoutlossofgeneralitythat x1ptqÑ´8 astÑ`8.
We prove that xnptqconverges to either ´8, or0, or`8, astÑ`8. We assume
in the sequel that xnptqdoes not converge to ´8or0. For any iP rns, if there
exists εą0and a sequence of positive times tsku`8
k“1tending to`8such that
xipskq ď ´ ε, then it follows from (7.10) that xiptq Ñ ´8 . Therefore, by our
assumptions, we have lim inf tÑ`8 xnptq ě0. Also, since xnptq↛0, there exists
εą0and a sequence of positive times ttku`8
k“1tending to`8such that xnptkqěε
for any integer k⩾1. For any tě0such that xnptqěε, we introduce the set of
indices
Nptq“tiPrns:xiptqă0u,THE EMERGENCE OF CLUSTERS IN SELF-ATTENTION DYNAMICS 25
and we write
9xnptqěexnptq2xnptq
nÿ
k“1exnptqxkptq`ÿ
jPNptqexjptqxnptqxjptq
nÿ
k“1exnptqxkptqěε
n`1
eε2ÿ
jPNptqeεxjptqxjptq.(7.12)
According to Lemma 7.4, any point xiptqwhich takes negative values for arbitrarily
largetimesanddoesnotconvergeto ´8hastoconvergeto 0. Therefore, thesecond
term in the lowermost bound in (7.12) is lower bounded by ´ε
2nfor sufficiently large
t. All in all, we gather that 9xnptqěε
2nandxnptqconverges to`8astÑ`8. If
it converges to 0, then necessarily xn´1ptqÑ´8 by combining Lemma 7.2 with
Lemma 7.4. This proves Lemma 7.6 in this case.
From now on we assume that xnptq Ñ `8 . Using (7.10) we see that if there
exists εą0such that xiptq ą εfor an unbounded sequence of times t, then
xiptqÑ`8. The same is true symmetrically when xiptqă´ εfor an unbounded
sequence of times t. Thus if iPB, necessarily xiptqÑ0. By Lemma 7.2 this can
be true for at most one index i, which concludes the proof of Lemma 7.6. □
IfB“H, Theorem 2.1 follows from Lemma 7.4. From now on, we assume that
#B“1, and we denote by i0Prnsits unique element. We distinguish two cases:
either i0Pt1, nu(Lemma 7.7), or i0Rt1, nu(Lemma 7.8).
Lemma 7.7. Ifxnptqis bounded as tÑ`8, then PnnptqÑ1, and PnjptqÑ0
for any jPrn´1s, astÑ`8. Similarly, if x1ptqis bounded as tÑ`8, then
P11ptqÑ1, and P1jptqÑ0for any jPrn´1s, astÑ`8.
Proof.The two cases ( xnp¨qbounded or x1p¨qbounded) are symmetric since the
evolution (7.1) commutes with the involution of pRdqngiven bypx1, . . . , x nq ÞÑ
p´x1, . . . ,´xnq. Whence, we only address the first one: we assume that xnptqis
bounded as tÑ`8. We first notice that all particles xjptqforjPrn´1stend to
´8astÑ`8due to Lemma 7.6. We now prove the following properties:
(1)xnptqą0for any sufficiently large t;
(2)xnptqÑ0astÑ`8;
(3) for any jPrn´1s,PnjptqÑ0astÑ`8.
Toprovepoint(1), wenoticethatforsufficientlylarge t,xiptqď0forany iPrn´1s.
If in addition xnptqď0, then due to (7.1), all xiptq(iPrns) remain negative and
due to (7.1), xnptqÑ´8 astÑ`8, which is a contradiction.
For point (2), we fix εą0, and set
T`
ε:“ttě0:xnptqěεu.
We prove that if T`
εis unbounded, then xnptq Ñ `8 astÑ `8, which is a
contradiction. As a consequence, T`
εis bounded for any εą0, which implies (in
conjunction with point 1.) that xnptqÑ0astÑ`8. So let us assume that T`
ε
is unbounded. We notice that for any δą0, iftPT`
εis sufficiently large then
ˇˇˇexnptqxjptqxjptqˇˇˇďδ26 GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
for any jPrn´1ssince xjptqÑ`8 astÑ`8. Therefore,
nÿ
j“1exnptqxjptqxjptqěeε2ε´pn´1qδě0,
where we took δą0sufficiently small for the last inequality to hold. Consequently,
9xnptq“nÿ
j“1exnptqxjptqxjptq
nÿ
j“1exnptqxjptqěexnptq2xnptq´pn´1qδ
exnptq2`n´1.
It is not difficult to see that this implies that xnptqÑ`8 astÑ`8, which is a
contradiction.
For point (3), we first notice that for any j‰n, since xjptqÑ´8,
9xjptq“nÿ
k“1ˆexjptqpxkptq´xnptqq
řn
ℓ“1exjptqpxℓptq´xnptqq˙
xkptqďx1ptq
n`n
εe´xjptqxnptq.
Using Lemma 7.3, we deduce the existence of some c2ą0such that
xjptqď´ c2et
for any sufficiently large tą0. We now prove that for any j‰n,
xjptqxnptq´xnptq2ÝÑ
tÑ`8´8. (7.13)
Due to the ordering of the particles, it is enough to prove (7.13) for j“n´1. Fix
j“n´1andκą0, and assume that
xnptqxjptqěxnptq2´κ
for some tě0. Then, using the fact that
xnptqxjptqěxnptqxkptq
for any kPrn´2s, we get
Pnjptqěexjptqxnptq
exnptq2`pn´1qexnptqxjptqěε,
where ε“1
n`eκ. We obtain
9xnptqďPnnptqxnptq`Pnjptqxjptqďxnptq`εxjptq,
hence
d
dt`
xnptqpxnptq´xjptqq˘
“9xnptqp2xnptq´xjptqq´xnptq9xjptq
ďpxnptq`εxjptqqp2xnptq´xjptqq´xnptq9xjptq
“´εxjptq2`xnptqp2εxjptq`2xnptq´xjptq´9xjptqq
ď´εxjptq2`xnptqp2xnptq´2x1ptqq, (7.14)
where in the last line we used the fact that 9xjptqěx1ptq, which is due to (7.1), and
thatx1ptqăxjptq, which is due to the ordering of the particles. Since xjptqď´ c2et
andx1ptqě´ c1et, the upper bound in (7.14) is negative if tis large enough. We
therefore conclude that for any fixed κ, if there exist unbounded times tsuch that
xnptqxjptqěxnptq2´κ, then xnptqxjptqěxnptq2´κfor any tlarge enough. ButTHE EMERGENCE OF CLUSTERS IN SELF-ATTENTION DYNAMICS 27
this is excluded since xnptq ą0andxjptq Ñ ´8 astÑ `8. This concludes
the proof of (7.13), and the lemma follows by plugging this information into the
definition of Pnjptq. □
Lemma 7.8. Ifi0Rt1, nuandxi0ptqremains uniformly bounded in t, then for any
jPrn´1s, there exists some αjPr0,1ssuch that Pi0jptqÑαjastÑ`8.
Proof.Assume that i0Rt1, nu. Then x1ptqÑ´8 andxnptqÑ`8 astÑ`8.
Also, xi0ptq Ñ 0due to (7.10). We write xi0ptq “yi0ptqe´t. Since γną0and
γ1ă0, we notice that the function
g:θÞÑÿ
iPrnszti0ueγiθγi
1`ÿ
iPrnszti0ueγiθ
takes value´8at´8, and`8at`8, and has a positive derivative. Thus, it
takes the value 0exactly once, and we denote this point by θ0. We prove that
yi0ptqÑθ0astÑ`8. We observe that
exi0ptq2“1`op1q.
Using Lemma 7.5 we have
9yi0ptq“et9xi0ptq´yi0ptq
“pPi0i0ptq´1qyi0ptq
`e2tÿ
jPrnszti0u¨
˚˚˝eyi0ptqpγj`op1qq
1`op1q`ÿ
kPrnszti0ueyi0ptqpγk`op1qq˛
‹‹‚pγj`op1qq.
We recognize that the sum in the above expression is roughly equal to gpyi0q. If the
latter is not close to 0for large times, then 9yi0ptqnecessarily have a huge magnitude
due to the e2tfactor, leading to a contradiction. Fix εą0. Ifyi0ptqąθ0`εfor
some large time tą0, then, noticing that
|yi0ptq|“et|xi0ptq|“opetq, (7.15)
we get
9yi0ptq“opetq`e2t´
g`
yi0ptq`opyi0ptqq˘¯
.
Butgpyi0ptqqěδ“δpεq, and hence
9yi0psqěδ
2e2s
for any larger time sět, which contradicts (7.15). We get a similar contradiction
ifyi0ptqăθ0´εfor large enough t. This concludes the proof that yi0ptqÑθ0. As
a consequence, xi0ptqxiptqÑθ0γifor any i‰i0, and we deduce Lemma 7.8. □28 GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
7.3.Concluding the proof of Theorem 2.1.
Proof of Theorem 2.1. By Lemma 7.6, there is at most one index i0Prnsfor which
the particle xi0ptqremains bounded for any tą0. In turn, for any iPrnszti0u, we
may invoke Lemma 7.4 which entails that Pijptqconverges to either δ1jorδnjas
tÑ`8(withdoublyexponentialrate). Andbyorderingoftheparticles,forindices
i1ďi2different from i0, and Pi1jptqÑδnjthen necessarily Pi2jptqÑδnjas well.
Consequently, all but at most one row of Pptqconverge to either e1“p1,0, . . . , 0qor
en“p0, . . . , 0,1qastÑ`8. For the i0-th row, we may invoke either Lemma 7.7 or
Lemma 7.8. The former applies if i0Pt1, nu, and entails that the i0-th row of Pptq
converges either to e1oren, while the latter applies if i0Rt1, nu, and entails that
thei0-th row of Pptqconverges to some vector αPRdwith non-negative entries.
Finally, since the i0-th row of Pptqhas entries which sum up to 1, then so does α.
These conclusions lead us to a final limit matrix P˚which has precisely the form
indicated in Fig. 2 (namely, P˚PPq, as desired. □
Remark 7.9 (Higher dimensions) .The extension of Theorem 2.1 to dě2is not
straightforward due to rare pathological situations. For example, suppose d“2,
n“2, and the initial configuration x1p0q“p 1, εqandx2p0q“p 1,´εq. One can
check that xiptqÑp 1,0qastÑ`8, for i“1,2, which means that a single cluster
appears. However, the self-attention matrix converges toward the identity (which
has rank 2). Therefore, it is not true in full generality that the rank of the limiting
self-attention matrix is equal to the number of clusters as tÑ `8, although we
believe that the result is true for almost all initial conditions.
8.Proofs of Theorems 3.1 and 8.5
In this section, we focus on proving the result in the case
V“Id.
We also provide a full picture of the behavior of the dynamics in the case V“´Id
in Section 8.2.
8.1.Clustering towards vertices of convex polytopes: Theorem 3.1. In
this section, we prove Theorem 8.1—namely, we show that particles tziptquiPrns
following the rescaled dynamics
9ziptq“nÿ
j“1˜
ee2txAziptq,Azjptqy
řn
k“1ee2txAziptq,Azkptqy¸
pzjptq´ziptqq (8.1)
converge, as tÑ8, toward points lying on the boundary of a particular convex
polytope. In (8.1) we made use of the shorthand notation
A:“`
QJK˘1
2. (8.2)
The precise statement is the following:
Theorem 8.1. Suppose V“IdandQJKą0. Then, for any initial datum
tzip0quiPrnsĂRd, the solution to (8.1)is such that its convex hull conv`
tziptquiPrns˘
converges to some convex polytope KĂRdastÑ `8. Furthermore, let V“
tv1, . . . , v mu(mďn) denote the set of vertices of K, and consider
S:“"
xPK:}Ax}2“max
jPrmsxAx, Av jy*
,THE EMERGENCE OF CLUSTERS IN SELF-ATTENTION DYNAMICS 29
with Adefined in (8.2). Then Shas finite cardinality, and VĂSĂ BKYt0u.
Finally, for any iPrnsthere exists a point ¯zPSsuch that ziptqÑ¯zastÑ`8.
In particular, ziptqconverges either to some point on the boundary of K, or to 0.
8.1.1.The convex hull is shrinking. To prove Theorem 8.1, we begin with the fol-
lowing illustrative result.
Proposition 8.2. Suppose V“IdandQJKą0. Then the solution tzip¨quiPrns
to(8.1)is such that tÞÑconvptziptquiPrnsqis non-increasing in the sense of set-
inclusion.
Proof of Proposition 8.2. Fixtą0and let HĂRdbe a closed half-space which
does not contain any of the points ziptq. We define the map
α:sÞÑmin
iPrnsdistpzipsq, Hq
fors⩾0. We claim that
αis non-decreasing on rt,`8q. (8.3)
Before proving (8.3), let us show how to conclude the proof of Proposition 8.2
using this claim. It follows from (8.3) that if convptziptquiPrnsqXH“ H, then
convptzipt1quiPrnsqXH“Hfor any t1ět. Writing the convex set convptziptquiPrnsq
as
convptziptquiPrnsq“č
H1open half-space
convptziptquiPrnsqĂH1H1“č
Hclosed half-space
convptziptquiPrnsqXH“HRdzH,
we get that convptzipt1quiPrnsqĂconvptziptquiPrnsqfor any t1ět.
We now turn to the proof of the claim (8.3). Denoting by nthe unit outer normal
toHand by projHthe orthogonal projection onto the closed set H, we have
distpx, Hq“xx´projHpxq,ny.
IftÞÑxptqis a differentiable curve, writing 9xptq“x9xptq,nyn`vptqwhere vptqPH
we haved
dtpprojHpxptqqq“ vptq, whence
d
dtdistpxptq, Hq“x9xptq,ny. (8.4)
LetTątdenote the infimum of the times for which one of the points ziptqlies
inH. Now fix sPrt, Tq, and denote by Mpsqthe set of indices iPrnssuch that
distpzipsq, Hqis minimal. For hÑ0, we have
αps`hq“ min
iPMpsqdistpzips`hq, Hq
“min
iPMpsqˆ
distpzipsq, Hq`hd
dtdistpzipsq, Hq`ophq˙
“αpsq`hˆ
min
iPMpsqd
dtdistpzipsq, Hq˙
`ophq.
Consequently,
dα
dtpsq“ min
iPMpsqd
dtdistpzipsq, Hq.30 GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
Moreover, for any iPMpsq, one has
d
dtdistpzipsq, Hq(8.4)“ x9zipsq,ny“nÿ
j“1Pijpsqxzjpsq´zipsq,nyě0,
where the last inequality comes from the fact that each term in the sum is non-
negative, since iPMpsq. Thisproves(8.3)(and, asabyproduct, that T“`8).□
The following fact immediately ensues.
Corollary 8.3. For any iPrnsandtě0,ziptqPconvptzip0quiPrnsq. In particular,
zip¨qis uniformly bounded in time.
8.1.2.Proof of Theorem 8.1.
Proof of Theorem 8.1. AsaconsequenceofProposition8.2,theset convptziptquiPrnsq
converges as tÑ `8toward some convex polytope K. In the remainder of the
proof, we look to show that the particles ziptqcan in fact converge only to some
well-distinguished points lying on the boundary of this polytope.
Step 1. The candidate set of limit points. We denote by V“tv1, . . . , v muthe
set of vertices of K. Writing any xPKas a convex combination of these vertices:
x“řm
j“1αjvjfor some weights αjě0withřm
j“1αj“1, we gather that
}Ax}2“C
Ax,mÿ
j“1αjAvjG
“mÿ
j“1αjxAx, Av jyďmax
jPrmsxAx, Av jy.(8.5)
LetSĂKdenote the set of points wPKsuch that
}Aw}2“max
jPrmsxAw, Av jy. (8.6)
The following holds—we postpone the proof to after that of the theorem.
Claim 1. VĂS. Moreover, if 0PK, then 0PS. Finally, SĂBKYt0u, andS
has finite cardinality.
Now, for δą0, we define the set Sδof points in Kat distance at most δfromS:
Sδ:“txPK: distpx,Sqďδu.
SinceSis finite, there exists a sufficiently small δ0ą0such that for any δďδ0,
the set SδhasM:“#Sconnected components, with any two of these connected
components being separated by a distance of at least δ0. Our goal is to prove that
for any iPrns, and for sufficiently large t, the particle ziptqremains in one of these
connected components. In the sequel, we fix iPrns.
Step 2. ziptqmust grow if it is not already in Sδ.We now prove that there
exists some γ“γpKqą0(depending only on the geometry of K) such that for any
δPp0, δ0s, there exists Tpδqą0such that if těTpδqandziptqRSδ, then
d
dt}Aziptq}2ěγδ. (8.7)THE EMERGENCE OF CLUSTERS IN SELF-ATTENTION DYNAMICS 31
K
S
Sδ
Figure 9. An example configuration of the sets SandSδinR2. The
setSconsists of all green nodes along the boundary of BK, while Sδis
the union of all yellow "hemispheres". The latter are pairwise disjoint
and are the connected components of Sδ, which we denote by Ck, for
kPrMs.
To this end, we observe that
1
2d
dt}Aziptq}2“xA9ziptq, Aziptqy
“nÿ
j“1˜
exAziptq,Azjptqye2t
řn
k“1exAziptq,Azkptqye2t¸
xApzjptq´ziptqq, Aziptqy
“nÿ
j“1˜
eajptqe2t
řn
k“1eakptqe2t¸
ajptq
loooooooooooooomoooooooooooooon
:“bjptq(8.8)
where we have set
ajptq:“xApzjptq´ziptqq, Aziptqy.
(Toobtainthelastequalityin(8.8), divideboththenumeratorandthedenominator
bye}Aziptq}2e2t.) The following holds.
Claim 2. There exists some constant γ1“γ1pKqą0depending only on the geom-
etry of Ksuch that the following holds. Fix δPp0, δ0s. There exists T1pδqą0such
that if těT1pδqandziptqRSδ, then there exists jPrnssuch that ajptqěγ1δ.
We postpone the proof of this claim to after that of the theorem. We seek to
use this claim in obtaining a lower bound of bjptqfor any j, whenever δis small
enough and tis large enough. Since by Corollary 8.3, for any jPrns,tÞÑzjptqis
uniformly bounded on r0,`8q, we gather that ajp¨qPL8p0,`8q. So, we may set
κ:“max
jPrnssup
tě0|ajptq|.
Lettě0be fixed. We define
Bptq:“tjPrns:ajptqě0u.32 GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
We pick an index j0ptqmaximizing ajptq, namely
j0ptqPargmaxjPrnsajptq.
Observe that j0ptqPBptqsince aj0ptqptqěaiptq“0. Clearly
bjptqě0for all jPBptq. (8.9)
In fact, we also have
bj0ptqptqěaj0ptqptq
n. (8.10)
Now suppose that jRBptq; since ajptqě´ κ, and
eajptqe2t
nÿ
k“1eakptqe2tď1
nÿ
k“1eakptqe2tďe´aj0ptqe2t,
we gather that
bjptqě´ κe´aj0ptqptqe2tfor all jPrnszBptq. (8.11)
Using (8.9), (8.10) and (8.11) in (8.8), we find
1
2d
dt}Aziptq}2⩾aj0ptqptq
n´κne´aj0ptqptqe2t.
TheaboveinequalityalongwithClaim2leadustodeducethatthereexists Tpδqą0
(possibly larger than T1pδq) such that (8.7) holds whenever t⩾Tpδq, with γ“γ1
2n,
as desired.
Step 3. ziptqcannot circulate indefinitely between the connected com-
ponents of Sδ.Since ziPL8pr0,`8qqby Corollary 8.3, from (8.1) we gather
that9ziPL8pr0,`8qqas well. And since any two connected components of Sδ0are
separated by a distance at least δ0, we deduce that it takes a time at least
T0:“δ0
}9zi}L8pr0,`8qq
forzito go from one connected component of Sδ0to another one. Fix δPp0, δ0q
such that
δăT0γδ0
8R}A}op, (8.12)
where R:“max jPrns}zj}L8pr0,`8qq. Denote by
C1, . . . ,CM
the connected components of Sδ, each of which being the intersection of Kwith a
Euclidean ball of radius δcentered at some point of S(see Fig. 9). For any kPrMs,
sup
xPCk}Ax}2´inf
xPCk}Ax}2ď4R}A}opδ. (8.13)
We introduce the following binary relation on rMs:
kąℓðñ inf
xPCk}Ax}2ąsup
xPCℓ}Ax}2,
whichistransitive. Theunderlyingideaisthefollowing: if tissufficientlylarge, and
ifzistartsfromsomeconnectedcomponent Cℓ,thentheonlyconnectedcomponents
Ckwhich ziis able to visit later on are those for which kąℓ. This travel of ziTHE EMERGENCE OF CLUSTERS IN SELF-ATTENTION DYNAMICS 33
has to stop after some time since rMsis finite, ąis transitive, and for any ℓ, the
relation ℓąℓdoes not hold.
LetT“Tpδqbe as in Step 2. Suppose that t2ąt1ěTandk1, k2PrMsare
distinct and such that zipt1qPCk1,zipt2qPCk2andziptqRSδfor any tPpt1, t2q.
Per Step 2 (more specifically, (8.7)),
}Azipt2q}2ě}Azipt1q}2`T0γδ0.
Therefore using (8.13) twice and since δis chosen as in (8.12), we gather that
inf
xPCk2}Ax}2ě}Azipt2q}2´4R}A}opδě}Azipt1q}2`T0γδ0´4R}A}opδ
ěinf
xPCk1}Ax}2`T0γδ0´4R}A}opδ
ěsup
xPCk1}Ax}2`T0γδ0´8R}A}opδ
ąsup
xPCk1}Ax}2.(8.14)
Whence k2ąk1. We therefore deduce that there exist some T1ěTandkPrMs
such that ziptqRSδzCkfor any těT1.
Step 4. Conclusion. To conclude, it remains to be shown that ziptqstays in Ck
fortlarge enough. For this, in addition to (8.12), we impose
δ1
4ăγT0
8R}A}opδ0. (8.15)
Forrą0, we denote by Cr
kthe intersection of Kwith the closed Euclidean ball of
radius δrhaving the same center as Ck. In particular, C1
k“Ck. If, after time T1,
zitravels from Ckto the complement of C1
4
k, it spends a time at least
pδ1
4´δ1
2q
}9zi}L8pr0,`8qq
inC1
4
kzC1
2
k. Per Step 2 (used with δ1
2),}Azi}2has to increase by at least
γδ1
2´
δ1
4´δ¯
}9zi}L8pr0,`8qqěγδ3
4
2}9zi}L8pr0,`8qqą4R}A}opδ (8.16)
during this travel (the last inequality in (8.16) stems from (8.15)). This implies
thatzicannot reenter Ckafter having reached the boundary of C1
4
k, due to (8.13).
Thus ziptqRSδfor any sufficiently large t, which is impossible due to Step 2 and
the uniform boundedness of tÞÑ}Aziptq}. Hence, for sufficiently large t,ziptqPC1
4
k.
Since δmaybechosenarbitrarilysmall,thisconcludestheproofofTheorem8.1. □
8.1.3.Proving Claims 1 and 2. We now address the proofs of the two claims which
were instrumental in what precedes (along with a sketch of the proof of VĂS, as
implied).
Proof of Claim 1. The fact that 0PSif0PKis immediate. We now show that S
is finite and SĂBKYt0u. Let wPSzt0u. As
w“mÿ
j“1αjvj34 GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
for some αj⩾0withřm
j“1αj“1, and since (8.6) holds by definition, it follows
thatαj“0for any jnot attaining the maximum in (8.6). Let IĂrmsdenote the
set of all such indices. We have
w“ÿ
jPIαjvj
with}Aw}2“xAw, Av jyfor any jPI. Whence wis the orthogonal projection onto
spantvjujPIwith respect to xA¨, A¨y. This yields SĂBK. Moreover, since for each
subset IĂrmsthere exists a unique such projection w,Sis finite. □
Sketch of proof of VĂS.We notice that for any iPrnsand for tlarge enough, we
have
9ziptq“nÿ
j“1˜
ee2txAziptq,Azjptqy
řn
k“1ee2txAziptq,Azkptqy¸
pzjptq´ziptqq (8.17)
«ÿ
jPMiptq˜
ee2txAziptq,Azjptqy
řn
k“1ee2txAziptq,Azkptqy¸
pzjptq´ziptqq,(8.18)
where Miptqis the subset of rnscontaining all indices jsuch that
max
kPrnsxAziptq, Azkptqy´x Aziptq, Azjptqyďe´t
(all other terms in the sum (8.17) are negligible). Due to the convergence of
convptziptquiPrnsqtoward K, we also know that for tlarge enough,
‚all the points ziptqare contained in a small neighborhood of K,
‚near any element of V, there exists some particle ziptq.
Assume, for the sake of contradiction, that there exists a vertex vjPVsuch that
vjRS. Set C:“convptviuiPrmsztjuq. In particular, distpvj,Cq ą0since vjis a
vertex of K. IfIĂ rnsdenotes the set of indices isuch that ziptqlies near vj,
then MiptqXI“ Hfor any iPI, since vjRS. For iPI, using (8.18), we find
that distpziptq,Cqdecays as tÑ`8as long as iRMiptq—indeed, (8.18) implies
that ziptqis attracted by C. This implies that vjRconvptzkpt1qukPrnsqfort1large
enough. This isa contradictionsince KĂconvptzkptqukPrnsqforany tě0according
to Proposition 8.2. □
Proof of Claim 2. To simplify the notation, we only prove Claim 2 when A“Id.
Assume that tě0and that ziptqRSδ.
First case. Firstly, we prove the claim in the case where ziptqRSδ0. For this, we
notice that the function
f:xÞÑmax
jPrnsxvj, xy´}x}2
is continuous, and by definition of S,fis strictly positive on the compact set
KzIntpSδ0q(the complement in Kof the interior of Sδ0). Hence fpxqěc1in this
set for some constant c1ą0. Setting
Kε:“txPRd: distpx,Kqďεu,
by continuity we find that fpxqěc1{2forxPKεzIntpSδ0qand for sufficiently small
εą0(fixed in the sequel). For sufficiently large t, we have ziptq PKεfor anyTHE EMERGENCE OF CLUSTERS IN SELF-ATTENTION DYNAMICS 35
iPrns, thus
max
jPrnsxziptq, zjptq´ziptqyě max
jPrmsxziptq, vj´ziptqyěc1
2.
Since c1is independent of δ, we deduce the claim in this case (notice that it suffices
to prove the claim for sufficiently small δ).
Second case. Secondly, we prove the claim when ziptqPSδ0zSδ. The proof mainly
relies on the following result:
Lemma 8.4. For any wPS, there exists βą0such that if6xPKXBpw, δ0q,
then
max
jPrmsxx, vj´xyěβ}x´w}. (8.19)
We postpone the proof of Lemma 8.4 and show how to conclude the proof of
Claim 2. Fix δą0. We set
η:“βδ
6R
where
R:“max
jPrns}zj}L8pRq.
Since convptzjptqujPrnsqconverges to KastÑ`8, there exists Tpδqą0such that
for any těTpδq, ifziptqPBpw, δ0qzBpw, δqfor some wPS, then
}ziptq´x}ďη
for some xPKXpBpw, δ0qzBpw, δqq. Therefore, using Lemma 8.4,
max
jPrmsxziptq, vj´ziptqyě max
jPrmsxx, vj´xy´3Rη
ěβδ´3Rη
“β
2δ.
To summarize, we have found that for any δą0there exists Tpδqą0such that if
těTpδqandziptqPSδ0zSδ, then
max
jPrmsxziptq, vj´ziptqyěβ
2δ. (8.20)
Combining (8.20) with
max
jPrnsxziptq, zjptq´ziptqyě max
jPrmsxziptq, vj´ziptqy
concludes the proof of Claim 2 in this second case. □
Proof of Lemma 8.4. Let us first address the case where w“0. Writing any xP
Kzt0uas a convex combination of the vertices: x“řm
j“1αjvj, we find
0“C
x,mÿ
j“1αjpvj´xqG
“mÿ
j“1αjxx, vj´xy. (8.21)
6Here, Bpy, rqdenotes the closed ball with center yPRdand radius rą0.36 GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
We can exclude having xx, vj´xy “0for all jP rms, as this would necessarily
imply that}x}2“2řm
j“1αjxx, vj´xy“0. We deduce from (8.21) that
max
jPrmsxx, vj´xyą0
for any xPKzt0u. Hence, it is sufficient to prove (8.19) for }x}small enough. We
notice that for any xPKzt0uwritten as above,
}x}2“mÿ
j“1αjxvj, xy.
Hence xÞÑmax jPrmsxvj, xyis positive for xPKzt0u. Since this function is contin-
uous and homogeneous in x, we deduce the existence of βą0such that
max
jPrmsxvj, xyě2β}x}
for any xPK. For xPKwith}x}sufficiently small, we obtain (8.19).
We now assume that wPSzt0u. We set
Iw:“␣
jPrns:}w}2“xw, vjy(
and
A:“span`␣
vj´w:jPIw(˘
,
which is orthogonal to w. We also introduce
R:“`
Rw‘A˘K,
and we denote by πRthe orthogonal projection on R. We claim that there exists
some ρą0such that for any jPrms, we have
xw´vj, wyěρ}πRvj}.
This follows from the observation that rmsis finite, and that }πRvj}ą0implies
xw´vj, wyą0. Therefore, for any xPK, writing xas a convex combination of
the vertices, namely x“řm
j“1αjvj, we find that
ρ}πRx}ďmÿ
j“1αj}πRvj}ďmÿ
j“1αjxw´vj, wy“xw´x, wy.(8.22)
FixxPKXBpw, δ0q. We write x“w`δ1uwith 0ďδ1ďδ0and}u}“1. Then
we have the orthogonal decomposition
u“bw`a`r (8.23)
where aPA,rPRandbPR. Since ais a convex combination of the form
a“ÿ
jPIwβjpvj´wq,
we have
}a}2“ÿ
jPIwβjxvj´w, ay,
whence
max
jPIwxa, vj´wyě}a}2.THE EMERGENCE OF CLUSTERS IN SELF-ATTENTION DYNAMICS 37
We deduce that
max
jPIwxx, vj´xy“max
jPIwxw`δ1u,pvj´wq´δ1uy
“´δ1b}w}2´δ12`δ1max
jPIwxa, vj´wy
ě´δ1b}w}2´δ12`δ1}a}2. (8.24)
Notice that bď0by combining (8.22) and (8.23). Since }u}“1and using (8.22)
we have
1“b2`}a}2`}r}2ď}a}2`κb2ďκp}a}2`b2q
where κ:“1`ρ´2}w}4. Wededucethateither }a}2ěp2κq´1or´b“|b|ěp2κq´1
2.
Plugging this knowledge in (8.24) and using the fact that }w}ą0, we finally deduce
the existence of an αą0(independent of δą0andxPKXBpw, δ0q) such that
max
jPrmsxx, vj´xyěαδ1´δ12“α}x´w}´}x´w}2.
This proves (8.19) when }x´w}ďα{2.
It thus remains to show that (8.19) holds for all xPKXpBpw, δ0qzBpw,α
2qq. To
this end, we notice that xÞÑmax jPrmsxx, vj´xyis continuous in the connected set
KXpBpw, δ0qzBpw,α
2qq, non-negative according to (8.5), and it is nowhere 0(by
definition of S). Therefore, it is strictly positive, and denote by α1ą0some lower
bound. Then for xPKXpBpw, δ0qzBpw,α
2qq, we have
max
jPrmsxx, vj´xyěα1ěα1
δ0}x´w}.
This concludes the proof of Lemma 8.4. □
8.2.A cluster at the origin. We complete this section by addressing the case
V“´Id, for which the convergence of the solutions of (1.1) is the simplest, since
a unique cluster forms at the origin. We also suppose that QJK“Id: in other
words, we consider the dynamics
9xiptq“´nÿ
j“1ˆexxiptq,xjptqy
řn
k“1exxiptq,xkptqy˙
xjptq, tPr0,`8q,(8.25)
with a prescribed initial condition txip0quiPrnsĂRd.
Theorem 8.5 (Convergence toward the origin) .Suppose V“´IdandQJK“Id.
Then, for any initial sequence of tokens txip0quiPrnsĂRd, and for any iPrns, we
have}xiptq}Ñ 0astÑ`8.
Remark 8.6. In the setting of Theorem 8.5, the self-attention matrix Pptqdefined
in(1.2)converges, as tÑ`8, to the nˆnmatrix with all entries equal to 1{n.
8.2.1.Proof of Theorem 8.5. We begin by showing that for any iPrns, the solu-
tion to (8.25) is uniformly bounded for all tą0. In the sequel, we fix an initial
configuration txip0quiPrnsĂRd.
Lemma 8.7. The trajectories of (8.25)are uniformly bounded in time—namely,
there exists Rą0(depending solely on nand the initial configuration) such that
the solution xip¨qto(8.25)satisfies}xiptq}⩽Rfor any iPrnsandt⩾0.38 GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
Proof of Lemma 8.7. We fix iPrns. For tě0, we denote by Diptqthe set of points
xkptqsuch thatxxiptq, xkptqyě0. We also set
Siptq:“ÿ
kPDiptqexxiptq,xkptqyxxiptq, xkptqy,
and
Riptq:“nÿ
k“1exxiptq,xkptqy.
Since 1`xďexwhence e´xxď1, we deduce that
1
2d
dt}xiptq}2“´nÿ
k“1exxiptq,xkptqyxxiptq, xkptqy
Riptqď´Siptq`n
Riptq.
Now since 1´xďe´xwhence exď1`exx, we find that Riptq ďn`Siptq.
Consequently, if we assume that }xiptq}2ě2nthen Siptqě2n, and therefore
1
2d
dt}xiptq}2ď´Siptq`n
n`Siptqď´1.
This shows that }xiptq}ď maxt}xip0q},?
2nufor any tě0, which concludes the
proof. □
By virtue of Lemma 7.1, we are able to characterize the stationary configurations
for the dynamics (8.25)—namely, the set of points p¯x1, . . . , ¯xnqPpRdqnsatisfying
nÿ
j“1ˆex¯xi,¯xjy
řn
k“1ex¯xi,¯xky˙
¯xj“0
for all iPrns.
Lemma 8.8. The only stationary configuration for the dynamics (8.25)is¯x1“
. . .“¯xn“0.
Proof.Assume that p¯x1, . . . , ¯xnqPpRdqnis a stationary configuration for the dy-
namics (8.25). We consider f:RdÑRdefined as
f:xÞÑlog˜nÿ
j“1exx,¯xjy¸
.
Per Lemma 7.1, fis convex, whence
fpxqěfp¯xiq`x∇fp¯xiq, x´¯xiy
forxPRdandiPrns. Since ∇fp¯xiq“0for any iPrns, we gather that fpxqě
fp¯xiq, whence ¯xiis a global minimizer of ffor any iP rns. By convexity, fis
constant on convpt¯xiuiPrnsq. Since fis analytic on the affine space Espanned by
the points ¯xi,iPrns, it is then constant on Eas well. Now assume that not all
of the points ¯xiare equal, and pick an index i0P rnssuch that ¯xi0is not equal
to the projection of the origin onto E. Then there exists some j0Prnssuch that
x¯xi0´¯xj0,¯xi0y‰0. For any sPR, we set Ps:“¯xj0`sp¯xi0´¯xj0qPE, and we
notice that fpPsq ě x Ps,¯xi0y, where the lower bound tends to `8either when
sÑ`8or when sÑ´8. This contradicts the fact that fis constant on E. We
conclude that the ¯xiare all equal for iPrns. The only value they can then take is
necessarily 0. □THE EMERGENCE OF CLUSTERS IN SELF-ATTENTION DYNAMICS 39
Lemma 8.9. The trajectories of (8.25)satisfyż`8
0}9xiptq}2dtă `8for any
iPrns.
Proof.The function
L:tÞÑnÿ
i“1nÿ
j“1exxiptq,xjptqy
is non-increasing, as demonstrated by the following simple computation:
dLptq
dt“2nÿ
i“1nÿ
j“1exxiptq,xjptqyx9xiptq, xjptqy“2nÿ
i“1C
9xiptq,nÿ
j“1exxiptq,xjptqyxjptqG
“´2nÿ
i“1nÿ
j“1exxiptq,xjptqy}9xiptq}2.
Being non-negative, Lptqthus converges as tÑ`8. Sincexxiptq, xjptqyě Rfor
some (possibly negative) RPRby virtue of Lemma 8.7, we deduce that
ż`8
0}9xiptq}2dtďe´Rż`8
0nÿ
i“1nÿ
j“1exxiptq,xjptqy}9xiptq}2dt“e´RpLp0q´lim
tÑ`8Lptqq,
which concludes the proof. □
We are now able to conclude the proof of Theorem 8.5.
Proof of Theorem 8.5. We set Xptq:“px1ptq, . . . , x nptqqPpRdqn. IfXptqdoes not
converge to 0, the compactness provided by Lemma 8.7 implies that there is a
sequencettku`8
k“1with tkÑ `8, and X˚“ px˚
1, . . . , x˚
nq P pRdqnzt0u, such that
Xptkq ÑX˚askÑ `8. To conclude the proof, it suffices to show that X˚is
a stationary configuration of the dynamics: this directly leads to a contradiction
per Lemma 8.8. Therefore, assume that X˚is not a stationary configuration of
the dynamics. We denote by X˚ptq “ p x˚
1ptq, . . . , x˚
nptqqthe solution of (8.25)
with initial condition X˚. Then, there exists iP rnssuch that 9x˚
ip0q ‰0. We
setε“ }9x˚
ip0q}. We select T0ą0(possibly small) such that }9x˚
iptq} ě ε{2for
tPr0, T0s. It follows from (6.9) (which is verified according to Corollary 6.6) that
for any δą0there exists k0PNsuch that}Xptk`tq´X˚ptq}ďδfor any tPr0, T0s
and any kěk0. By (6.5) (which is verified according to Corollary 6.6), we obtain
that}9xiptk`tq´9x0
iptq} ď CδfortP r0, T0sand any kěk0. Choosing δą0
sufficiently small, we obtain that }9xiptk`tq}ě ε{4fortPr0, T0sand any kěk0.
This contradicts Lemma 8.9. □
9.Proof of Theorem 4.2
To ensure clarity, we present the proof of Theorem 4.2 under the assumption that
Vis diagonalizable. However, this assumption is not necessary. In Remark 9.5, we
explain how the proof can be modified to accommodate for non-diagonalizable V.
Let us therefore assume that Vis diagonalizable. Let pφ1, . . . , φ dqbe an or-
thonormal basis of eigenvectors associated to eigenvalues pλ1, . . . , λ dq, ordered in
a decreasing manner with respect to their modulus: |λ1| ě. . .ě |λd|. (Starting
from this point and throughout, we use the symbol λexclusively to denote the
eigenvalues of V.) Except for λ1PR, all the other eigenvalues (and eigenvectors)
may be complex. We denote by pφ˚
1, . . . , φ˚
dqthe dual basis of pφ1, . . . , φ dq.40 GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
9.1.Some monotonicity properties and bounds. To start, we present some
general facts that are prove useful in all subsequent sub-cases.
Lemma 9.1. Suppose kPrdsis such that λkě0. Then tÞÑmax jPrnsφ˚
kpzjptqq
is a non-increasing and bounded function, and tÞÑminjPrnsφ˚
kpzjptqqis a non-
decreasing and bounded function. In particular, tÞÑφ˚
kpziptqqis uniformly bounded
as a function on r0,`8qfor any iPrns.
Proof.For any kPrdsand any tě0, set
αkptq“min
jPrnsφ˚
kpzjptqq, β kptq“max
jPrnsφ˚
kpzjptqq.
LetiPrnsbe an index such that αkptq“φ˚
kpziptqq. Then we have
d
dtφ˚
kpziptqq“nÿ
j“1Pijptqφ˚
kpVpzjptq´ziptqqq
“λknÿ
j“1Pijptqpφ˚
kpzjptqq´φ˚
kpziptqqqě 0
where the last inequality stems from the fact that λkě0and the choice of index
i. This proves that αkp¨qis non-decreasing, as desired. Arguing similarly, one finds
that βkp¨qis non-increasing. As a consequence, αkp0qďαkptqďβkptqďβkp0qfor
anytě0, which shows that αkp¨qandβkp¨qare bounded. □
Corollary9.2. IfVonlyhas realnon-negative eigenvalues, then zip¨qPL8pr0,`8qq.
Lemma 9.3. FixkPrdsandiPrns. Then there exists a constant Cą0such that
ˇˇφ˚
k`
etVziptq˘ˇˇďCe|λk|t
holds for all t⩾0.
Proof.We naturally make use of the equation for xiptq:“etVziptq. Fix t⩾0. We
have
d
dt|φ˚
kpxiptqq|2“2¨Reˆ
φ˚
kpxiptqqd
dtφ˚
kpxiptqq˙
“2¨Re˜nÿ
j“1Pijptqφ˚
kpV xjptqqφ˚
kpxiptqq¸
“2¨Re˜nÿ
j“1Pijptqλkφ˚
kpxjptqqφ˚
kpxiptqq¸
ď2|λk|max
jPrns|φ˚
kpxjptqq|2.
Choosing iPrnsrunning over the set of indices such that |φ˚
kpxiptqq|is maximal,
we obtain
d
dtmax
jPrns|φ˚
kpxjptqq|2ď2|λk|max
jPrns|φ˚
kpxjptqq|2.
We conclude the proof by applying Grönwall’s lemma. □THE EMERGENCE OF CLUSTERS IN SELF-ATTENTION DYNAMICS 41
9.2.Proof of Theorem 4.2. We now prove Theorem 4.2. We again recall that
λ1is simple and positive, and the eigenvalues of Vare ordered in decreasing order
of modulus: λ1ą|λ2|ě. . .ě|λd|.
Proof of Theorem 4.2. We look to prove that for any iPrns, the component of ziptq
along the principal eigenvector φ1, i.e. φ˚
1pziptqq, converges as tÑ`8. We also
show that there exists a set of at most 3real numbers (depending on the initial
datumpz1p0q, . . . , z np0qq) such that for any iPrnsthe limit of φ˚
1pziptqqbelongs to
this set. Theorem 4.2 directly follows from these facts.
LetiPrnsbe fixed. Recall from Lemma 9.1 that φ˚
1pziptqqis uniformly bounded
for any tPr0,`8q. We set
a:“lim
tÑ`8min
jPrnsφ˚
1pzjptqq, b :“lim
tÑ`8max
jPrnsφ˚
1pzjptqq.(9.1)
(Note that by Lemma 9.1, aěminjPrnsφ˚
1pzjp0qqandbďmax jPrnsφ˚
1pzjp0qq.) For
cPt0, a, bu, we define the candidate limiting hyperplanes for ziptq:
Hc:“txPRd:φ˚
1pxq“cu.
We show that ziptqconverges either to H0, toHaor to Hb. Ifa“b“0, then
according to (9.1) all particles converge to H0and there is nothing left to prove.
We now distinguish two scenarios:
(i) either for any εą0,|φ˚
1pziptqq|ď εfortlarge enough—in which case, we
deduce that ziptqconverges toward H0astÑ`8—,
(ii) or|φ˚
1pziptkqq|ą ε0for some ε0ą0and for some sequence of positive times
ttku`8
k“1with tkÑ`8.
Since case (i) is straightforward, let us handle case (ii). Without loss of generality,
we can extract a subsequence of times (which we do not relabel, for simplicity of
notation) along which
φ˚
1pziptkqqąε0. (9.2)
LetεPp0, ε0sbe fixed and to be chosen later. We set
wjptq:“@
QetVziptq, KetVzjptqD
,
so that
1
λ1d
dtφ˚
1pziptqq“nÿ
j“1ewjptq
řn
k“1ewkptqpφ˚
1pzjptqq´φ˚
1pziptqqq.(9.3)
We look to obtain a lower bound for the right-hand side in the above identity. Let
us use the shorthand
ckℓ:“xQφk, Kφ ℓy
fork, ℓPrds. By assumption, c11ą0. We have φ˚
kpetVziptqq“ etλkφ˚
kpziptqqand
the following spectral expansion holds:
etVziptq“dÿ
k“1etλkφ˚
kpziptqqφk.42 GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
Using this fact, as well as Lemma 9.3, we gather that
ˇˇˇwjptq´c11e2λ1tφ˚
1pziptqqφ˚
1pzjptqqˇˇˇ“ˇˇˇˇˇˇÿ
pk,ℓq‰p1,1qckℓφ˚
kpetVziptqqφ˚
ℓ`
etVzjptq˘ˇˇˇˇˇˇ
ďÿ
pk,ℓq‰p1,1q|ckℓ|ˇˇφ˚
k`
etVziptq˘ˇˇˇˇφ˚
ℓ`
etVzjptq˘ˇˇ
ďC2}QJK}opÿ
pk,ℓq‰p1,1qep|λk|`|λℓ|qt
ďC2}QJK}oppd´1q2loooooooooooomoooooooooooon
“:C1epλ1`|λ2|qt(9.4)
holds for all tě0andjPrns. Now since λ1ą0, Lemma 9.1 implies that for any
tě0there exists an index i0ptqPrnssuch that
φ˚
1pzi0ptqptqqěb. (9.5)
With j0ptqPargmaxjPrnswjptq, using (9.4) and (9.5) we see that
wj0ptqptqěwi0ptqptqěc11φ˚
1pziptqqbe2λ1t´C1epλ1`|λ2|qt. (9.6)
Now for any twithin the sequence ttku`8
k“1, combining the first inequality in (9.6)
with the fact that c11ą0, (9.2) and (9.4), we deduce that
φ˚
1pzj0ptqptqq´φ˚
1pzi0ptqptqqě´2C1
c11εe´pλ1´|λ2|qt. (9.7)
Asλ1ą |λ2|, for tlarge enough, we find that we can lower bound the above
expression by ´ε
4. We now define the set of indices
Nptq:“tjPrns:φ˚
1pziptqq´φ˚
1pzjptqqě0u.
Take twithin the sequence ttku`8
k“1such that φ˚
1pziptqqďb´εand large enough so
that (9.7) is lower bounded by ´ε
4(if such a tdoes not exist, we immediately con-
clude that φ˚
1pziptqqÑ bastÑ`8). Using (9.5) and the subsequent derivations,
we deduce that
φ˚
1pzj0ptqptqq´φ˚
1pziptqqě3ε
4,
and since φ˚
1pzjptqq´φ˚
1pziptqqě0forjRNptq, we expand in (9.3) to get
1
λ1d
dtφ˚
1pziptqqěewj0ptqptq
řn
k“1ewkptq3ε
4`ÿ
jPNptqewjptq
řn
k“1ewkptqpφ˚
1pzjptqq´φ˚
1pziptqqq.
(9.8)
On another hand, for jPNptq, we may use (9.4) to find
wjptqďc11φ˚
1pziptqq2e2λ1t`C1epλ1`|λ2|qt. (9.9)
We set
C0:“max
jPrnsφ˚
1pzjp0qq´min
jPrnsφ˚
1pzjp0qq.THE EMERGENCE OF CLUSTERS IN SELF-ATTENTION DYNAMICS 43
Using the monotonicity properties from Lemma 9.1, as well as (9.9) in (9.8), we
obtain
1
λ1d
dtφ˚
1pziptqqě3ε
4n´C0nexp´
c11φ˚
1pziptqq2e2λ1t`C1epλ1`|λ2|qt¯
exp´
c11φ˚
1pziptqqbe2λ1t´C1epλ1`|λ2|qt¯.
Given our choice of t, we have φ˚
1pziptqq2´bφ˚
1pziptqqď´ εpb´εq, so, we conclude
from the inequality just above that
1
λ1d
dtφ˚
1pziptqqě3ε
4n´C0nexp´
´c11εpb´εqe2λ1t`2C1epλ1`|λ2|qt¯
.(9.10)
Since λ1ą|λ2|, it follows from (9.10) that there exists Tą0such that for any t
within the sequence ttku`8
k“1for which těTandφ˚
1pziptqqPr ε, b´εs, there holds
d
dtφ˚
1pziptqqěλ1ε
2n.
This shows the existence of a larger time horizon T1ąTsuch that φ˚
1pziptqqěb´ε
whenever těT1. And since εcan be taken arbitrarily small, we deduce that
φ˚
1pziptqqconverges toward b, namely that ziptqconverges toward Hb, astÑ`8.
Arguing in the same way as above, and assuming without loss of generality that
aă0, we may find that all indices iP rnsfor which φ˚
1pziptkqq ď ´ ε0for some
ε0ą0and some sequence tkÑ `8, the particle ziptqconverges toward Haas
tÑ`8. This concludes the proof. □
9.3.Remarks.
Remark 9.4. Theorem 4.2 establishes the convergence of φ˚
1pziptqqfor any iPrns
astÑ`8, but does not preclude the fact that }ziptq}may diverge toward `8(along
the hyperplane) as tÑ`8. This is indeed expected (and observed numerically—
see Fig. 6) when Vhas some negative eigenvalues. We also note that when all the
eigenvalues of Vare non-negative, Corollary 9.2 shows that all the ziptqremain
bounded.
Remark 9.5 (The case where Vis not diagonalizable) .IfVis not assumed to
be diagonalizable, Lemma 9.3 (or, at least the proof thereof) requires some modi-
fications. Let δ:“λ1´|λ2|ą0. Let εą0be fixed and to be chosen later. We
decompose Vin Jordan blocks, and we consider
Cd“mà
k“1Fk, (9.11)
whereFkis the span of the Jordan chain corresponding to the k-th Jordan block.
By a slight abuse of notation (solely for the purpose of this remark), we denote by
λkthe eigenvalue associated to the k-th Jordan block. We recall that we can choose
a basispφk,1, . . . , φ k,jkqof each Fkin a way that V|Fkreads in this basis as7
»
————–λkε
......
...ε
λkfi
ffiffiffiffifl. (9.12)
7Recall that Jordan blocks are commonly written with a `1in the superdiagonal. This can be
replaced by any non-zero complex scalar as done here—see [HJ12, Chapter 3, Corollary 3.1.21].44 GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
We observe that if εis chosen sufficiently small (depending only on δ), Lemma 9.3
may be replaced by the following estimate in each Fk:
DCą0,@tě0,@iPrns,››πFk`
etVziptq˘››ďCep|λk|`δqt. (9.13)
Here, πFkdenotes the orthogonal projection onto Fk. To prove estimate (9.13), we
follow theproofof Lemma9.3, withd
dt}πFkpxiptqq}2playingtheroleofd
dt|φ˚
kpxiptqq|2.
The key observation is that combining (9.11)and(9.12)we obtain
}πFkpV xiptqq}ďp| λk|`δq}πFkpxiptqq},
provided εis chosen sufficiently small. Then (9.13)follows as in Lemma 9.3.
With(9.11)at hand, the proof of Theorem 4.2 carries through, under the im-
pactless modification that Cepλ1`|λ2|`δqtreplaces (9.4)(and subsequent estimates
are modified in the same way).
10.Proof of Theorem 5.2
In this section, we establish the proof for Theorem 5.2. Since the proof is essen-
tially a combination of the proofs of Theorems 4.2 and 8.1, we may occasionally
skip certain details and refer to the proofs of these two results. As done throughout
this work, we set
A:“pQJKq1
2.
We denote by πF:RdÑFthe projection onto Fparallel to G, and by πG:RdÑG
theprojectiononto Gparallelto F. Theset πFpconvptziptquiPrnsqqisaconvexsubset
ofFwhich is non-increasing with respect to t(the proof of this fact is identical to
that of Proposition 8.2). It therefore converges toward some convex polytope Kas
tÑ`8.
FixiPrns. We have
πFp9ziptqq“nÿ
j“1˜
exAetVziptq,AetVzjptqy
řn
k“1exAetVziptq,AetVzkptqy¸
πFpVpzjptq´ziptqqq
“nÿ
j“1˜
exAetVziptq,AetVpzjptq´ziptqqy
řn
k“1exAetVziptq,AetVpzkptq´ziptqqy¸
πFpVpzjptq´ziptqqq.
From this point on, we follow the proof of Theorem 8.1, and we solely highlight the
changes compared to the original proof. Roughly speaking, this new proof amounts
to adding projections πFat several places. We denote by SĂFthe set of points
wPKsuch that
}πFpAwq}2“max
jPrmsxπFpAwq, πFpAvjqy.
The fact that SĂBKand that Shas finite cardinality is proved precisely as Claim
1 (in the proof of Theorem 8.1), simply by replacing all occurrences of A¨byπFpA¨q.
Once again, Sδdenotes the set of all points in Kat distanceďδto some point of
S.
Step 2 in the proof of Theorem 8.1 (i.e., (8.7)) is replaced by the following
statement:
Step 2’: There exists a constant γ“γpKqą0(depending only on the geometry
ofK) such that for any δPp0, δ0s, there exists T“Tpδqą0such that if těTandTHE EMERGENCE OF CLUSTERS IN SELF-ATTENTION DYNAMICS 45
πFpziptqqRSδ, then
d
dt}πFpAziptqq}2ěγδ.
We now proceed in proving this statement.
Proof of Step 2’. We set
ajptq:“xπFpAziptqq, πFpApzjptq´ziptqqy
and
rjptq:“@
AetVziptq, AetVpzjptq´ziptqqD
´ajptqe2λ1t.
We find
1
2d
dt}πFpAziptqq}2“xπFpA9ziptqq, πFpAziptqqy
“nÿ
j“1˜
exAetVziptq,AetVzjptqy
řn
k“1exAetVziptq,AetVzkptqy¸
xπFpApzjptq´ziptqqq, πFpAziptqqy
“nÿ
j“1˜
exAetVziptq,AetVpzjptq´ziptqqy
řn
k“1exAetVziptq,AetVpzkptq´ziptqqy¸
xπFpApzjptq´ziptqqq, πFpAziptqqy
“nÿ
j“1˜
eajptqe2λ1t`rjptq
řn
k“1eakptqe2λ1t`rkptq¸
ajptq
looooooooooooooooooomooooooooooooooooooon
“:bjptq. (10.1)
We now make use of the following adaptation of Claim 2.
Claim 3. There exists some constant γ1“γ1pKqą0depending only on the geom-
etry of Ksuch that the following holds. Fix δPp0, δ0s. There exists T“Tpδqą0
such that if těTandziptqRSδˆG, then there exists jPrnssuch that ajptqěγ1δ.
Compared to Step 2 in the proof of Theorem 8.1, we now have to estimate the
coefficients rjptq. To this end, setting yjptq:“AetVzjptqforjPrns, we notice that
rjptq“P1ptq`P2ptq`P3ptqwhere
P1ptq“xπFpyiptqq, πGpyjptq´yiptqqy,
P2ptq“xπGpyiptqq, πFpyjptq´yiptqqy,
P3ptq“xπGpyiptqq, πGpyjptq´yiptqqy.
By virtue of Lemma 9.3 we have |πFpyjptqq|ď Ceλ1tand|πGpyjptqq|ď Cet|λ2|for
anytě0(orCet|λ2|`εifV|Gis not diagonalizable—see Remark 9.5), hence
|rjptq|ďCetpλ1`|λ2|q. (10.2)
Since πFpzjptqqis uniformly bounded in tPr0,`8qfor any jPrnsdue to Corollary
8.3, we get ajp¨qPL8p0,`8q. So, we may set
κ:“max
jPrnssup
tě0|ajptq|.
Lettě0. We define
Bptq:“␣
jPrns:ajptqe2λ1t`rjptqě0(
.
Letj0ptqPargmaxjPrnspajptqe2λ1t`rjptqq. Note that j0ptqPBptqsince
aj0ptqe2λ1t`rj0ptqěaiptqe2λ1t`riptq“0.46 GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
We notice the following three properties:
‚Forj“j0ptq, we have bj0ptqptq ěaj0ptqptq
n(recall the definition of bjin
(10.1));
‚for any jPBptqztj0u, we have bjptqě0;
‚for any jRBptq, we have
bjptqě´ κexp´
´aj0ptqe2λ1t`Cepλ1`|λ2|qt¯
.
Indeed, using the fact that jPBptqand (10.2), we find
exp`
ajptqe2λ1t`rjptq˘
nÿ
k“1exp`
akptqe2λ1t`rkptq˘ď1
nÿ
k“1exp`
akptqe2λ1t`rkptq˘
ď1
exppaj0ptqe2λ1t`rj0ptqq
ďexp´
´aj0ptqe2λ1t`Cepλ1`|λ2|qt¯
.
Making use of these properties in (10.1) yields the desired lower bound—indeed,
iftis sufficiently large and ziptq RSδˆG, we havetjP rns:ajptq ěγ1δu ‰ H
according to Claim 3, and so we deduce that
1
2d
dt}Aziptq}2ěγ1δ
n´κne´γ1δe2λ1t`Cepλ1`|λ2|qt.
Taking tpossibly larger (and depending on δ), we obtain the result of Step 2’. □
Steps 3 and 4 in the proof of Theorem 8.1 are essentially unchanged—we re-
place all the occurrences of }A¨}by}πFpA¨q}(for instance in (8.13) and (8.14)).
Although}Aziptq}may not be uniformly bounded in t, it is important to note
that}πFpAziptqq}is uniformly bounded. Similarly, while 9ziptqRL8pr0,`8qq, we
do have}d
dtπFpzip¨qq} L8pr0,`8qqă`8. The sets Sδ,CkandCr
kare replaced by
SδˆG,CkˆGandCr
kˆGrespectively. The conclusion is that }πFpAziptqq}2has
to increase by at least
γδ1
2pδ1
4´δq
}9zi}L8pr0,`8qqěδ3
4
2}9zi}L8pr0,`8qqą4R}A}opδ
during a travel from CkˆGto the complement of C1
4
kˆG. As in the proof of
Theorem 8.1 this implies that for any iP rnsthere exists sPSsuch that ziptq
remains at distance at most δaway fromtsuˆG. This being true for any δą0,
we obtain the desired result.
11.Numerical experiments
11.1.Setup.Unless indicated otherwise, all figures presented in this paper were
generated by discretizing the underlying dynamics (either (1.1) or (3.1)) using a
fourth order Runge-Kutta scheme with a step size of 0.1. All points in the initial
sequence were drawn independently from the uniform distribution over the hyper-
cuber´5,5sd. Random matrices (e.g., Q, K, V) have entries drawn independently
from the uniform distribution on r´1,1s. Codes and animated plots of all examples
may be found online atTHE EMERGENCE OF CLUSTERS IN SELF-ATTENTION DYNAMICS 47
https://github.com/borjanG/2023-transformers .
We now present some experiments which motivate some conjectures and claims
made in what precedes.
11.2.Eigenvalues of ALBERT’s value matrices. In Figure 10 we illustrate the
eigenvaluesofthevaluematrices Vhforacoupleofheads hinapre-trained ALBERT
model. We focus on ALBERT-xlarge-v2 available online at
https://huggingface.co/albert-xlarge-v2 .
This version uses 16heads, with sequences of length n“256and tokens of di-
mension d“128. While not all value matrices Vhper head hP r16ssatisfy the
assumptions made in Section 4, we illustrate the eigenvalues of a couple of them
which do.
2
 1
 0 11.5
1.0
0.5
0.00.51.01.5
Eigenvalues of value matrix for head 5
2
 1
 0 11.5
1.0
0.5
0.00.51.01.5
Eigenvalues of value matrix for head 14
Figure 10. The eigenvalues of V5andV14in the pre-trained ALBERT
satisfy the eigenvalue assumption made in Definition 4.1. Furthermore,
the second assumption made in Definition 4.1 is satisfied by pQ5, K5q
andpQ14, K14q(the inner products evaluated along the eigenvector of
norm 1equal 1.3060and0.6719respectively). In other words, the triples
pQh, Kh, Vhqcorresponding to heads h“5andh“14inALBERT
satisfy all the assumptions made in the statement of Theorem 4.2.
11.3.Experiments related to Theorem 2.1. We begin with the setup of The-
orem 2.1, which we recall was proven to hold in the case d“1. Herein we present
a couple of examples (Figures 11 and 12) which elucidate the role that dandn
appear to play in this fact.
Notably, as seen in Fig. 4, we believe that the conclusion of Theorem 2.1 could
plausibly be extended to any dą1, assuming Vą0.
11.4.Illustrating Theorem 4.2 in R3.To precisely illustrate the appearance of
at most three hyperplanes in the setting of Theorem 4.2, we gave an example in R2.
We expand on this and provide a couple of toy examples in R3for the purpose of
visualization (we recall that these are toy models, as Transformers in practice are
high-dimensional), and namely focus in both examples on the case where the two
latter eigenvalues are complex. In Fig. 14, we see the effect of having eigenvalues
with a negative real part, and the complementary case is illustrated in Fig. 13.48 GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
0 20 40 60 80
0
20
40
60
80t= 0.0, rank= 11
0 20 40 60 80
0
20
40
60
80t= 3.0, rank= 27
0 20 40 60 80
0
20
40
60
80t= 5.0, rank= 15
0 20 40 60 80
0
20
40
60
80t= 10.0, rank= 3
Figure 11. WeexpandonFig.3—forthesamesetup, consider n“100.
The sequence length ndoes not appear to influence the rank of Pptq,
which is expected since the rank of Pcorresponds to the number of
leaders.
0 5 10 15 20 25 30 35
0
5
10
15
20
25
30
35t= 0.0, rank= 40
0 5 10 15 20 25 30 35
0
5
10
15
20
25
30
35t= 0.2, rank= 40
0 5 10 15 20 25 30 35
0
5
10
15
20
25
30
35t= 1.0, rank= 2
0 5 10 15 20 25 30 35
0
5
10
15
20
25
30
35t= 10.0, rank= 2
0 5 10 15 20 25 30 35
0
5
10
15
20
25
30
35t= 0.0, rank= 40
0 5 10 15 20 25 30 35
0
5
10
15
20
25
30
35t= 0.5, rank= 7
0 5 10 15 20 25 30 35
0
5
10
15
20
25
30
35t= 5.0, rank= 2
0 5 10 15 20 25 30 35
0
5
10
15
20
25
30
35t= 10.0, rank= 2
0 5 10 15 20 25 30 35
0
5
10
15
20
25
30
35t= 0.0, rank= 40
0 5 10 15 20 25 30 35
0
5
10
15
20
25
30
35t= 0.5, rank= 4
0 5 10 15 20 25 30 35
0
5
10
15
20
25
30
35t= 1.0, rank= 2
0 5 10 15 20 25 30 35
0
5
10
15
20
25
30
35t= 10.0, rank= 2
Figure 12. We consider n“40,Q“K“Idand a random matrix
Vą0in dimensions d“10(first row), d“40(second row), and d“80
(third row). The conclusion of Theorem 2.1 appears to transfer to the
higher dimensional case, and this would actually follow from Conjecture
4.3 (should it hold).
11.5.Complementing Figure 7. In Figure 7, we illustrate the appearance of
clustering in high-dimension (the ALBERT setup: n“256andd“128) for generic
random matrices pQ, K, Vq. The value matrix Vin question has 65positive eigen-
values, and we show the conjectured convergence of the 65coordinates along the
corresponding eigenvectors to one of possibly 3(generically 2) real scalars. In Fig-
ure 15, we complement this illustration by showing the possible oscillatory and
divergent behavior of the remaining coordinates.THE EMERGENCE OF CLUSTERS IN SELF-ATTENTION DYNAMICS 49
−5 0 5−505−50510
t= 0.0
−5 0 5−505−50510
t= 5.0
−5 0 5−505−50510
t= 40.0
Figure 13. We consider n“25,Q“K“Id, and Va random matrix
with positive entries and eigenvalues t1,0.1`0.08i,1´0.08iu. The
pair of complex eigenvalues have a positive real part. We not only
see convergence to one of two hyperplanes determined by the direction
φ1“p0.38,0.8,0.47q, but in fact, the particles appear to collapse to two
points. In other words, the "hyperplanes" are of codimension 3, which
is in line with Conjecture 4.3.
−10 0 10010−505
t= 0.0
−10 0 10010−505
t= 5.0
−10 0 10010−505
t= 10.0
−50050−2502550−2502550
t= 30.0
−50050−2502550−2502550
t= 35.0
−50050−2502550−2502550
t= 40.0
Figure 14. We consider n“25,Q“K“Id, and Va random matrix
with positive entries and eigenvalues t1,´0.05`0.25i,´0.05´0.25iu.
The pair of complex eigenvalues have a negative real part, which en-
tails the rotation of the particles. We see that the particles rotate
within a couple of 2-dimensional hyperplanes determined by φ1“
p´0.3,´0.8,´0.45q, as implied by Theorem 4.2.50 GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
0 2 4 6 8 10 12 14
t−1014−1010−106−102010210610101014Non-clustered coordinates
Figure 15. We complement Figure 7 and plot the variance of the set
tφ˚
jpziptqq:iPrnsuof all coordinates jcorresponding to negative eigen-
values of V. We also show the mean along tokens of a couple of coor-
dinates (white lines). Coordinates diverge rapidly to ˘8over time t;
y-axis is in logscale.
Part3.Discussion and open questions
12.Outlook
Several important directions regarding the mathematical theory of Transformers
remain unexplored. An important extension of our work would amount to studying
multi-headed Transformers—borrowingthenotationfromRemark3.4, theyamount
to:
xrk`1s
i“xrks
i`∆tHÿ
h“1nÿ
j“1˜
exQhxrks
i,Khxrks
jy
řn
ℓ“1exQhxrks
i,Khxℓpkqy¸
Vhxrks
j, kPN.
Foreach hPrHs(correspondingtoadifferent head), theweightmatrices Qh, Kh, Vh
are constant. Proofs regarding clustering or convergence of the self-attention ma-
trix for such dynamics is an open problem. Preliminary numerical investigations
seem to indicate that interesting clustering phenomena also occur in this context.
A characterization or properties of optimal weights by invoking the optimal con-
trol correspondence in the spirit of [Wei17] is also an interesting avenue for future
research.
We hereby list a couple of additional numerical experiments suggesting general-
izations of our results, which we leave as open problems.
12.1.Beyond QJKą0in Theorems 3.1 and 5.2. As seen throughout all the
presented proofs, assumptions on the value matrix Vare significantly more rigid
than assumptions on the matrices QandK. For instance, should the eigenvalue
λwith the largest real part of Vbe negative, all rescaled tokens will diverge to
infinity. Should λbe complex, we do not expect any clustering to occur (for theTHE EMERGENCE OF CLUSTERS IN SELF-ATTENTION DYNAMICS 51
rescaledtokens). Yet, noneoftheconclusionsofTheorems3.1or5.2seemtochange
for generic choices of QJK. This is illustrated in Figures 16 and 17 respectively.
−5
0
5−505−505
t= 0.0
−5
0
5−505−505
t= 1.0
−5
0
5−505−505
t= 2.0
−5
0
5−505−505
t= 5.0
Figure 16. Here, V“Id, while QJKviolates the PSD assumption–it
is a random matrix (with entries drawn from the uniform distribution
onr´1,1s). Nonetheless, the clustering pattern entailed by Theorem 3.1
persists.
12.2.Beyond pure self-attention: adding a feed-forward layer. Practical
implementations of the Transformer architecture combine the self-attention mech-
anism with a feed-forward neural network. While extending the mathematical
analysis from this paper to such a broader setting would be challenging, we can
offer some numerical insights into the expected outcomes.
The feed-forward neural network which can be adjoined to the Transformer dy-
namics in one of two ways. The first way consists in running the pure self-attention
dynamics up to time tďT(or equivalently, for OpTqlayers), and then applying a
pure feed-forward neural network to the concatenated vector of clustered features
at time T. This amounts to seeing the feed-forward network as a map from Rnd
toRm(for some m⩾1), which can be studied independently with existing theory.52 GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
−5
0
5−505−100−50050
t= 0.0
−5
0
5−505−100−50050
t= 5.0
−5
0
5−505−100001000
t= 10.0
−5
0
5−505−10000010000
t= 15.0
Figure 17. Here, Vis paranormal, while QJKviolates the PSD
assumption–it is a random matrix (with entries drawn from the uniform
distribution on r´1,1s). Nonetheless, the clustering pattern entailed by
Theorem 5.2 persists.
The second way consists in using both the self-attention and feed-forward mech-
anisms in parallel at every layer t. In this case, clustering in the exact sense of
Theorems 3.1 and Theorems 5.2 would be difficult to anticipate since the weights
of the feed-forward network play the role of a value matrix V(as they can be ab-
sorbed within V), and the conclusions of these theorems strongly depend on the
identity-like structure.
In Figure 18, we focus on the second of the above-discussed examples, and il-
lustrate a possible generalization of Theorem 4.2 to this setup. For simplicity, we
focus on a 2-layer neural network: we apply a component-wise nonlinear activa-
tion function σ(either the ReLU or tanh) to the self-attention dynamics, and then
multiply by a weight matrix WPRdˆd. Namely, we consider
9ziptq“Wσ˜
Vnÿ
j“1˜
exQetVziptq,KetVzjptqy
řn
k“1exQetVziptq,KetVzkptqy¸
pzjptq´ziptqq¸
(12.1)THE EMERGENCE OF CLUSTERS IN SELF-ATTENTION DYNAMICS 53
foriPrnsandtě0. A bias vector bPRd(whether inside or outside the activation
function) can also be included to allow for translations. The clustering property
appears to persist, the pattern depending on the weight matrix Wand on the
activation function σ. We leave this problem open to further investigation.
−5 0 5 10−50510
t= 0.0
−5 0 5 10−50510
t= 20.0
0 10 20−505101520
t= 40.0
0 50 100 150 200050100150
t= 100.0
−5 0 5−4−2024
t= 0.0
−5 0 5−4−2024
t= 1.0
−5 0 5−4−2024
t= 5.0
−5 0 5−4−2024
t= 10.0
−5 0 5−4−2024
t= 0.0
−5 0 5−4−2024
t= 0.1
−5 0 5−4−2024
t= 5.0
−5 0 5−4−2024
t= 10.0
Figure 18. The setup of Theorem 4.2 with a 2-layer neural network
appended to the dynamics (i.e., (12.1)). Top: σ“ReLU with W“Id.
Middle: σ“tanhwith W“Id. Bottom: σ“ReLU with Wbeing a
random matrix. In the first row, we see that the particles first evolve
as to reach the upper right quadrant pRą0qd(due to the ReLU). Once
they reach it, every particle eventually follows one of three hyperplanes
determined by the spectrum of Vand the projection onto pRą0qd. In
the other two cases, all particles appear to collapse to 0.
References
[ABV`05] Juan A Acebrón, Luis L Bonilla, Conrad J Pérez Vicente, Félix Ritort, and Renato
Spigler. The Kuramoto model: A simple paradigm for synchronization phenomena.
Reviews of modern physics , 77(1):137, 2005.
[BM00] Paul S Bradley and Olvi L Mangasarian. K-plane clustering. Journal of Global opti-
mization , 16:23–32, 2000.
[Che95] Yizong Cheng. Mean shift, mode seeking, and clustering. IEEE Transactions on Pat-
tern Analysis and Machine Intelligence , 17(8):790–799, 1995.
[CHH`16] Junghee Cho, Seung-Yeal Ha, Feimin Huang, Chunyin Jin, and Dongnam Ko. Emer-
gence of bi-cluster flocking for the Cucker–Smale model. Mathematical Models and
Methods in Applied Sciences , 26(06):1191–1218, 2016.
[CRBD18] Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural
ordinary differential equations. Advances in Neural Information Processing Systems ,
31, 2018.54 GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
[CS07] Felipe Cucker and Steve Smale. Emergent behavior in flocks. IEEE Transactions on
Automatic Control , 52(5):852–862, 2007.
[DCL21] Yihe Dong, Jean-Baptiste Cordonnier, and Andreas Loukas. Attention is not all you
need: Pure attention loses rank doubly exponentially with depth. In International
Conference on Machine Learning , pages 2793–2803. PMLR, 2021.
[Dob79] Roland L’vovich Dobrushin. Vlasov equations. Funktsional’nyi Analiz i ego
Prilozheniya , 13(2):48–58, 1979.
[Gol13] François Golse. Mean field kinetic equations. Course of Polytechnique , 2013.
[HJ12] Roger A Horn and Charles R Johnson. Matrix analysis . Cambridge University Press,
2012.
[HK02] Rainer Hegselmann and Ulrich Krause. Opinion dynamics and bounded confidence:
models, analysis and simulation. Journal of Artifical Societies and Social Simulation
(JASSS) , 5(3), 2002.
[HKPZ19] Seung-Yeal Ha, Jeongho Kim, Jinyeong Park, and Xiongtao Zhang. Complete clus-
ter predictability of the Cucker–Smale flocking model on the real line. Archive for
Rational Mechanics and Analysis , 231:319–365, 2019.
[HR17] Eldad Haber and Lars Ruthotto. Stable architectures for deep neural networks. In-
verse problems , 34(1), 2017.
[HysW`22] Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean
Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language
models. In International Conference on Learning Representations , 2022.
[HZRS16a] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning
for image recognition. In Proceedings of the IEEE conference on computer vision and
pattern recognition , pages 770–778, 2016.
[HZRS16b] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep
residual networks. In Computer Vision–ECCV 2016: 14th European Conference,
Amsterdam, The Netherlands, October 11–14, 2016, Proceedings, Part IV 14 , pages
630–645. Springer, 2016.
[JM14] Pierre-Emmanuel Jabin and Sebastien Motsch. Clustering and asymptotic behavior
in opinion formation. Journal of Differential Equations , 257(11):4165–4187, 2014.
[Kra00] UlrichKrause.Adiscretenonlinearandnon-autonomousmodelofconsensus.In Com-
munications in Difference Equations: Proceedings of the Fourth International Con-
ference on Difference Equations , page 227. CRC Press, 2000.
[Kur75] Yoshiki Kuramoto. Self-entrainment of a population of coupled non-linear oscillators.
InInternational Symposium on Mathematical Problems in Theoretical Physics: Jan-
uary 23–29, 1975, Kyoto University, Kyoto/Japan , pages 420–422. Springer, 1975.
[LCG`20] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma,
and Radu Soricut. ALBERT: A Lite BERT for Self-supervised Learning of Language
Representations. In International Conference on Learning Representations , 2020.
[LLH`20] Yiping Lu, Zhuohan Li, Di He, Zhiqing Sun, Bin Dong, Tao Qin, Liwei Wang, and
Tie-YanLiu.Understandingandimprovingtransformerfromamulti-particledynamic
systempointofview.In International Conference on Learning Representations , 2020.
[LWLQ22] Tianyang Lin, Yuxin Wang, Xiangyang Liu, and Xipeng Qiu. A survey of transform-
ers.AI Open , 3:111–132, 2022.
[MT14] Sebastien Motsch and Eitan Tadmor. Heterophilious dynamics enhances consensus.
SIAM Review , 56(4):577–621, 2014.
[PHD20] Vardan Papyan, XY Han, and David L Donoho. Prevalence of neural collapse during
the terminal phase of deep learning training. Proceedings of the National Academy of
Sciences, 117(40):24652–24663, 2020.
[PR13] Benedetto Piccoli and Francesco Rossi. Transport equation with nonlocal velocity in
Wasserstein spaces: convergence of numerical schemes. Acta Applicandae Mathemat-
icae, 124:73–105, 2013.
[PR16] Benedetto Piccoli and Francesco Rossi. On properties of the generalized Wasserstein
distance. Archive for Rational Mechanics and Analysis , 222:1339–1365, 2016.
[PRT15] Benedetto Piccoli, Francesco Rossi, and Emmanuel Trélat. Control to flocking of the
kinetic Cucker–Smale model. SIAM Journal on Mathematical Analysis , 47(6):4685–
4719, 2015.THE EMERGENCE OF CLUSTERS IN SELF-ATTENTION DYNAMICS 55
[RS14] Brian Rider and Christopher D. Sinclair. Extremal laws for the real Ginibre ensemble.
The Annals of Applied Probability , 24(4):1621 – 1651, 2014.
[SABP22] Michael E Sander, Pierre Ablin, Mathieu Blondel, and Gabriel Peyré. Sinkformers:
Transformers with doubly stochastic attention. In International Conference on Arti-
ficial Intelligence and Statistics , pages 3515–3530. PMLR, 2022.
[VCBJ`95] TamásVicsek, AndrásCzirók, EshelBen-Jacob, InonCohen, andOferShochet.Novel
type of phase transition in a system of self-driven particles. Physical Review Letters ,
75(6):1226, 1995.
[Vid11] René Vidal. Subspace clustering. IEEE Signal Processing Magazine , 28(2):52–68,
2011.
[VKF20] Apoorv Vyas, Angelos Katharopoulos, and François Fleuret. Fast transformers with
clustered attention. Advances in Neural Information Processing Systems , 33:21665–
21674, 2020.
[VSP`17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N
Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in
Neural Information Processing Systems , 30, 2017.
[Wei17] E Weinan. A proposal on machine learning via dynamical systems. Communications
in Mathematics and Statistics , 1(5):1–11, 2017.
[WHL19] E Weinan, Jiequn Han, and Qianxiao Li. A mean-field optimal control formulation
of deep learning. Research in Mathematical Sciences , 6(1):10, 2019.
[WLK`20] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer:
Self-attention with linear complexity. arXiv preprint arXiv:2006.04768 , 2020.
[YBR`20] ChulheeYun, SrinadhBhojanapalli, AnkitSinghRawat, SashankJReddi, andSanjiv
Kumar. Are transformers universal approximators of sequence-to-sequence functions?
InInternational Conference on Learning Representations , 2020.
Borjan Geshkovski
Department of Mathematics
MIT
Cambridge, MA
02139 USA
e-mail: borjan@mit.eduCyril Letrouit
Department of Mathematics
MIT
Cambridge, MA
02139 USA
e-mail: letrouit@mit.edu
Yury Polyanskiy
Department of EECS
MIT
Cambridge, MA
02139 USA
e-mail: yp@mit.eduPhilippe Rigollet
Department of Mathematics
MIT
Cambridge, MA
02139 USA
e-mail: rigollet@mit.edu