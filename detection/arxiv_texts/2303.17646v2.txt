XPert: Per ipheral Circuit & Neural Architecture
Co-search for Area and Energy-efficient X bar-based
Computing
Abhishek Moitra, Abhiroop Bhattacharjee, Youngeun Kim and Priyadarshini Panda
Yale University, New Haven, CT, 06511, USA
Abstract —The hardware-efficiency and accuracy of Deep Neu-
ral Networks (DNNs) implemented on In-memory Computing
(IMC) architectures primarily depend on the DNN architecture
and the peripheral circuit parameters. It is therefore essential
to holistically co-search the network and peripheral parameters
to achieve optimal performance. To this end, we propose XPert,
which co-searches network architecture in tandem with periph-
eral parameters such as the type and precision of analog-to-
digital converters, crossbar column sharing and the layer-specific
input precision using an optimization-based design space explo-
ration. Compared to VGG16 baselines, XPert achieves 10.24 ×
(4.7×) lower EDAP, 1.72 ×(1.62×) higher TOPS/W, 1.93 ×(3×)
higher TOPS/mm2at 92.46% (56.7%) accuracy for CIFAR10
(TinyImagenet) datasets. The code for this paper is available at
https://github.com/Intelligent-Computing-Lab-Yale/XPert.
Index Terms —Neural Architecture Search, In-memory Com-
puting, Peripheral Circuits, Crossbars, Deep Neural Networks
I. I NTRODUCTION
Deep Neural Networks (DNNs) have achieved state-of-the-
art performance in large-scale image recognition tasks [1], [2].
In the context of energy-efficient deep learning, In-Memory
Computing (IMC) architectures are a promising alternative
for overcoming the memory wall bottleneck of traditional
von-Neumann computing [3], [4]. However, in the recent
development of IMC architectures, it is found that peripheral
circuits are the significant contributors to the energy and area
overhead [5]–[8].
To this end, several hardware-algorithm co-design ap-
proaches have employed algorithm-level optimization tech-
niques to reduce the peripheral circuit overheads. In [5], [9],
the authors have employed pruning and quantization to reduce
the analog-to-digital-converter (ADC) precision and thereby,
decrease the ADC area and energy overhead. In [6], the
authors employ binary partial-sum quantization-aware training
to realise sense amplifier-based SRAM IMC implementations.
Other co-design based works [7], [8] have leveraged hardware-
level optimization in IMC architectures to reduce the periph-
eral circuit overhead. The authors in [7] propose compact
ADC architectures to achieve low area and high throughput
implementations. In [8], the authors reuse crossbar peripheral
circuits over multiple convolution layers to maximize energy
efficiency. Although the above mentioned works achieve good
hardware efficiency, they have optimized the DNN or hardware
in isolation without considering the co-dependence betweenTABLE I: Table showing empirical trends in energy, delay, area and
accuracy of IMC-implemented DNNs. ✘– metric is not affected.
Design Parameter Energy Delay Area Accuracy
Channel Depth (CD) ↑ ↑ ↑ ↑ ↑
ADC Precision (AP) ↑ ↑ ↑ ↑ ↑
Input Precision (IP) ↑ ↑ ↑ ✘ ↑
Column Sharing (CS) ↑ ↑ ↑ ↓ ✘
(a)(b)
Homogeneous VGG16Homogeneous VGG16
Fig. 1: EDAP and non-ideal (NI) accuracy comparison of our XPertNet
(having layer-specific peripheral circuit configurations) and VGG16 networks
(having homogeneous peripheral circuit configuration across all layers) for
(a) CIFAR10 and (b) TinyImagenet datasets. F- Flash ADC, CS8- 8 crossbar
columns shared per ADC. All networks are implemented on 64x64 4-bit
RRAM crossbars. VGG16 networks are implemented with AP=6 and IP=8.
the DNN architecture and the IMC hardware. Note, we use the
terminology DNN architecture ( DNNArch ) to denote different
layer configurations of a DNN such as channel depth, kernel
size, layer-depth among others.
To address this, a line of works such as [10], [11] have
employed IMC-aware DNNArch search. To achieve high en-
ergy efficiency, the authors in [10] co-searched DNNArch
along with layer-specific quantization without considering any
hardware-specific design parameters (for example, ADC pre-
cision, crossbar-size among others). A recent work NAX [11]
proposed co-searching DNNArch with layer-specific crossbar
sizes to maximize the energy-efficiency and DNN accuracy.
However, the above works have the following drawbacks:
1) They explore a very small hardware design space (e.g.
weight/activation quantization in [10] or crossbar size in
[11]) during the co-search which might result in sub-optimal
solutions. 2) They optimize for energy and accuracy without
considerations for area overhead.
Table I shows different DNNArch (layer-specific channel
depths (CD)) and crossbar peripheral circuit (ADC Precision
(AP), Input Precision (IP) and Column Sharing (CS)) param-
eters that majorly contribute to the energy, delay, area and
accuracy of any IMC-implemented DNN. We define columnarXiv:2303.17646v2  [cs.CV]  21 Nov 2023sharing as the number of crossbar columns shared by one
ADC (more details in Section II). In addition to the design
parameters shown in Table I, the ADC type (can be either
Successive Approximation Register (SAR) or Flash type) plays
an important role towards the hardware efficiency and hence
is also considered as a design parameter (see Section II). The
choice of each design parameter entails different cost and
benefits in terms of hardware-efficiency and DNN accuracy.
For example as seen in Table I, higher CD, AP and IP
values ( ↑) incur higher DNN accuracy ( ↑) while increasing
the energy/delay and area consumption ( ↑). A higher CS value
decreases area and increases energy/delay. Similarly, among
SAR and Flash type ADCs, Flash ADCs are more energy-
efficient while being area-inefficient compared to SAR ADCs.
Given the patent co-dependence between different design
parameters, it is essential to perform a holistic co-search in
theDNNArch and peripheral circuit design space to achieve
an optimal solution.
To this end, we propose XPert, a fast network architecture
and crossbar peripheral circuit co-search framework for area
and energy-efficient IMC-implementations. XPert uses a dual-
phase co-search approach wherein Phase1 is catered towards
searching a DNNmodel with optimal layer-specific CD, CS and
ADC Type (AT) parameters to achieve the lowest delay under a
given area constraint. To further optimize the energy-efficiency
and accuracy, in Phase2, XPert searches for layer-specific
AP and IP configurations on the Phase1-searched DNNmodel .
Note, we use the terminology DNNmodel to denote a DNN
with layer-specific architecture and hardware configuration
namely CD, CS, AT, AP and IP. Fig. 1 shows that XPertNet
(the optimal DNNmodel obtained from Phase1 and Phase2 co-
search) achieves the lowest energy-delay-area product (EDAP)
compared to different homogeneous VGG16 implementations.
In summary, we make the following contributions:
1) We develop the XPert framework to perform fast
DNNArch -peripheral circuit design space co-search to
achieve energy and area-efficient DNN implementations
with high classification accuracy. During the co-search,
XPert uses XPertSim-Pytorch for real-time area and
delay assisted DNNmodel search. XPerSim-Pytorch uses
a backend XPertSim-C++: a cycle-accurate hardware
evaluation platform to support IMC-implemented DNNs
with heterogeneous peripheral circuit configurations.
2) We perform extensive evaluations on CIFAR10 and
TinyImagenet datasets and observe that XPertNet (with
50mm2area) achieves 10.2 ×and 4.76 ×lower EDAP
compared to the most energy-efficient homogeneous
VGG16 network (F+CS16+IP8+AP6 in Fig. 1) for CI-
FAR10 and TinyImagenet, respectively. XPertNet also
achieves 9.8% (CIFAR10) and 4.7% (TinyImagenet)
higher NI accuracy (includes quantization and device
variation noise due to analog crossbar computing) com-
pared to baseline VGG16 implementations.
3) Furthermore, we show that XPert can attain opti-
mal accuracy and EDAP under extremely small area
Switch MatrixMultiplexerADCIMCIMC…..…..WL0WLXSL0SLXBL0IMCIMCIMCIMCBL1BLXI0IX10101110Input Precision(IP)T1T2T3T4T5T6L1L2L3(a)(b)ChannelDepth (CD)Column Sharing (CS)ADC Type (AT), ADC Precision (AP)Shift-and-AddMultiplexerADCShift-and-AddFig. 2: Figure showing (a) the DNN mapping on a tiled-IMC architecture.
Each Tile (T1-T6) contains several processing engines (PE) with each PE
containing multiple IMC crossbars [12] (b) the IMC crossbar with different
peripheral circuit parameters. The DNNArch and peripheral circuit parameters
enclosed in red boxes are co-searched in XPert.
1e5(b)(a)1e6
1e6(c)(d)1e5CS:8, AP:6,IP:8NT:9, AP:6,IP:8
NT:9, AP:6,CS:8NT:9, CS:8,IP:8
Fig. 3: Figure showing the variation of delay with different (a) NTat fixed
CS=8, AP=6 and IP=8 (shown inside the figure) (b) CS (c) IP and (d) AP
values. The implementations are based on 64 ×64 4-bit RRAM crossbars.
Note, the energy follows similar trends and not shown for brevity.
constraints. At 20mm2, XPertNet achieves an EDAP
(accuracy) of 4.96 mJ*ms*mm2(92.1%) and 40.4
mJ*ms*mm2(56.2%) on CIFAR10 and TinyImagenet
datasets, respectively.
II. B ACKGROUND AND MOTIVATION
The energy, delay, area and accuracy of IMC-implemented
DNNs majorly depend on the following DNNArch and periph-
eral circuit parameters.
1)Channel Depth (CD): In a standard IMC architecture
[12], [13], individual layers are mapped on multiple tiles
as shown in Fig. 2a. The number of tiles ( NT,l) required
for any layer lis majorly dictated by the layer’s input
(CDin,l) and output ( CDout,l) channel depths. NT,lalso
depends on factors like the kernel size ( kl), crossbar size
(X) and the number of crossbars (#Xbars) per tile as
shown in Eq. 1.
NT,l=ceil(CDin,l×k2
l
X)×ceil(CDout,l
X)
#Xbars/Tile. (1)
To capture representative features during training and
achieve high accuracy, higher CD is required [14]. How-
ever, higher CD entail higher NTcausing higher delay
and area as seen in Fig. 3a and Fig. 4a, respectively.
2)Column Sharing (CS): As seen in Fig. 2b multi-
plexers at the end of each crossbar facilitate sharing
of ADCs and Shift-and-Add circuits among multiple1e7
1e7
1e7(a)(b)(c)NT:9, AP:6,IP:8CS:8, AP:6, IP:8NT:9, CS:8,IP:8Fig. 4: Figure showing the variation of area with different (a) NTat fixed
CS=8, AP=6 and IP=8 (shown inside figure) (b) CS and (c) AP values. The
implementations are based on 64 ×64 4-bit RRAM crossbars.
crossbar columns. As shown in Fig. 4b, higher CS
requires less ADCs per crossbar and therefore reduces
the area. However, higher CS also increases the crossbar
read cycles [12] resulting in higher delay as in Fig. 3b.
3)Input Precision (IP): To reduce the overhead of digital-
to-analog converters (DAC), standard IMC architectures
[12] employ input bit-serialization wherein a multi-
bit input is encoded into a bit-stream processed over
multiple crossbar read cycles. For example, a 4-bit input
is processed over 4 cycles as seen in Fig. 2b. Higher
input precision leads higher accuracy and higher delay
due to higher crossbar read cycles as seen in Fig. 3c.
4)ADC Type (AT) and Precision (AP): In IMC archi-
tectures, two types of ADC are predominantly used:
Flash type and SAR type. A k-bit Flash-type ADC
is composed of 2k−1cascaded comparators. Flash
ADCs entail extremely low delay at the cost of higher
area overhead. A Successive Approximation Register
(SAR) ADC employs a single comparator and a DAC
to perform bit-by-bit comparison over multiple clock
cycles. Due to a single comparator, the area overhead of
SAR ADC is significantly less compared to Flash ADC
while the delay is much higher as seen in Fig. 3 and Fig.
4. Higher AP is essential for higher accuracy of IMC-
implemented DNNs. However, higher AP entails higher
cascading and clock cycles/DAC precision in case of
Flash and SAR ADCs, respectively. This increases both
the delay and area as shown in Fig. 3d and Fig. 4c.
Evidently, the energy, delay, area and accuracy of IMC-
implemented DNNs are functions of CD, CS, AT, AP and IP.
Therefore, to achieve an optimal solution, there is a need for
holistic DNNArch and peripheral circuit co-search.
III. M ETHODOLOGY
Fig. 5a shows the DNNArch -peripheral circuit design space
and a summary of XPert’s dual-phase co-search approach.
XPert’s DNNArch design space consists of layer-specific chan-
nel depths only as it significantly contributes to the EDAP and
accuracy of an IMC-implemented DNN (see Section II).
Phase1 Co-search: The Phase1 SuperNet consists of L
layers, with each layer containing different combinations of
CD, CS and AT values. In XPert, layer lin the Phase1
SuperNet contains 4 ×5×2= 40 nodes with associated archi-
tectural parameters αl,1-αl,40. In each search step, node iwith
the highest softmax (αl,i)value is sampled (shown with red
boxes in Fig. 5b) resulting in a candidate DNNmodel withlayer-specific CD, CS and AT configurations. Note, during
Phase1, the AP and IP configurations are 6 and 8, respectively
for all layers. The candidate DNNmodel is sent as an input to
the XPertSim-Pytorch tool. XPertSim-Pytorch contains GPU-
compatible hardware-accurate differentiable area and delay
functions for real-time loss L1computation and gradient
backpropagation. L1is a combination of the delay and mean
squared error loss (MSE) between the sampled DNNmodel’s
area and the user provided area constraint. The gradients∂L1
∂αl,iare used to optimize the αl,iusing standard SGD. Note, during
Phase1, we only update the αs while the weights are not
trained. At each step, if the area of any candidate DNNmodel
is nearly equal (within 2% margin) to the user provided area
constraint, the candidate DNNmodel and its delay are saved.
The Phase1 search occurs over N1steps.
Post Phase1 Co-search: As shown in Fig. 5c, among
the pool of candidate DNNmodels (saved during Phase1), we
select the candidate with the highest Hamming Distance (HD)
score [15] and lowest delay. We call this candidate the Phase1-
searched DNNmodel . The HD score measure is adopted from
a recent Neural Architecture Search work [15] which uses HD
as a fast and indirect metric to determine the classification ca-
pability of different subnetworks sampled from a supernetwork
without the need to train the architecture/weight parameters1.
The weights of the Phase1-searched DNNmodel are trained till
convergence using standard SGD.
Phase2 Co-search: The weights, CD, CS and AT configura-
tions of the trained Phase1-searched DNNmodel are frozen and
only the layer-specific AP and IP are co-searched. For each
layer of the Phase1-searched DNNmodel , there are 2 ×6=12
different combinations of AP and IP as shown in Fig. 5d
with architectural parameters βl,ifor layer land node i.
Like Phase1, the nodes with the highest softmax (βl,i)are
sampled. A forward propagation is performed on the trained
Phase1-searched DNNmodel with the selected layer-specific
AP and IP configurations. Since, during Phase2, the AP and
IP can be lower than 6 and 8, respectively (default values
in Phase1), quantization noise is induced during forward
propagation. Additionally, we also include device variation
noise during the forward propagation. To mitigate the quan-
tization/device variation noise, we use batchnorm adaptation
[16] wherein the running mean of the batch-normalization
layers are updated based on the noisy convolution outputs.
Through the forward propagation, the cross-entropy loss LCE
is computed. The batchnorm adaptation is performed over 20k
randomly sampled images from the training set. The delay
of the trained Phase1-searched DNNmodel is computed by
XPertSim-Pytorch based on the selected AP/IP configurations.
LCEand delay are used to compute the loss function L2.
Gradients∂L2
∂βl,iare used to optimize the βl,is using SGD.
Phase2 occurs over N2steps.
XPertSim-C++ is a cycle-accurate hardware evaluation
platform adopted from Neurosim [12] to support IMC-
1inputs are passed through an untrained DNN and the hamming distance
between unique binary code representations of the ReLU maps is carried out.
Higher hamming distance score represents higher representation capability.Phase1•Search CD, CS and ATOptimize Area and DelayTrain Phase1-searched DNNmodelfor AccuracyPhase2•Search AP and IP parametersOptimize Delay at Iso-accuracyFreeze WeightsFreeze WeightsTrain WeightsIso-Accuracy MaintainedXPertNet(a)
β2,1[6,3]β2,2 [6,4]β2,12 [5,8]Layer 1Layer LLayer 2…AP and IP Search Space…
Layer 1Layer LLayer 2…
AP:IP:556…435…Fwdwith Quantization and Device Variation Noise (Noise-aware BN Adaptation)XpertSimPytorchPhase2 Optimizer𝓛2= 𝓛CE+ 𝝀2DelayOptimize βs only. No Weights trained….…Layer 1CD:CS:AT:641285122432SSFCandidate DNNmodelXpertSimPytorchPhase1 OptimizerArea, DelayOptimize⍺s only. No Weights trained 𝓛1= 𝝀1Delay + MSE(Area, AC) If Area Constraint MetSave candidate DNNmodel and Delay⍺11 [64,2,S]⍺12 [64,4,S]⍺1,40 [512,32,S]…Layer 1Layer 1⍺L1 [64,2,S]⍺L2 [64,4,S]⍺L,40 [512,32,S]…Layer LPhase1 SuperNetN1Step SearchPhase1-searched DNNmodel:•Best hamming distance score and  •Lowest DelayTrain Weights with SGD(c)After Phase1(d)CD: 64, 128, 256, 512CS: 2,4,8,16, 32AT: Flash (F), SAR (S)AP: 5, 6IP: 3, 4, 5, 6, 7, 8DNNArch+PeripheralCircuit Design Space
Phase1 SearchPhase2 SearchWeights Frozen…Layer 1Layer 2Layer LTrained Phase1-searched DNNModel(b)
N2Step SearchFig. 5: (a) Overview of XPert’s dual-phase co-search (b) Phase1 co-search between CD, CS and AT to optimize area and delay. AC- user provided area
constraint. (c) Post-Phase1 training of the Phase1-searched DNNmodel (d) Phase2 co-search to optimize hardware accuracy and to further reduce the delay.
SARCS: 2AP: 5IP: 3CD:CS:AT:AP:IP:641285124322SFS565453………TTTTTTTTGAPRGBMUXCrossbar (XB)Switch MatrixSARShift-&-AddH-Tree Connected TilesDNNmodel
Layer-specific Trace GenerationLayer-specific Peripheral CircuitsEnergy, Delay, Area EvaluationPEPEPETBPEPEPETAXBXBXBPBXBXBXBPATile (T)Processing Engine (PE)Shift-&-AddMUXLayer-specific Mapping
Fig. 6: DNNmodel mapping on the XPertSim C++ platform and tool flow.
GB, TB, PB- Global, Tile and PE Buffers. GA, TA, PA- Global, Tile and
PE Accumulators, P- Pooling and R-ReLU modules. Switch matrix provide
voltage inputs to the crossbar. Shift-&-Add, PA, TA and GA accumulate partial
sum outputs. PB, TB and GB store intermediate MAC and activation values.
implemented DNNs with heterogeneous peripheral circuit con-
figurations. XPertSim-C++ serves as a backend for XPertSim-
Pytorch. Fig. 6 shows a DNNmodel with layer-specific CD,
CS, AT, AP and IP being mapped on a tiled architecture. The
Tiles, PEs and crossbars use H-Tree-based communication.
The crossbar’s configuration depends on the circuit parameters
of the mapped layer. Post mapping, XPertSim C++ performs
hardware-accurate energy, delay, area evaluation.
IV. E XPERIMENTS AND RESULTS
A. Experimental Setup
TABLE II: IMC Hardware Implementation Parameters.
Technology 32nm CMOS
Device 4-bit RRAM [17] ( σ/µ= 20%)
Ron(on/off ratio) 6 kΩ(150)
Crossbar Size (X), #Xbars/Tile 64, 64
Weight Quantization, Weight & Input Slicing 8-bit, 4-bit & 1-bit
We consider CIFAR10 and TinyImagenet datasets to evalu-
ate the efficacy of the XPert platform.
Design Space: The DNNArch +Peripheral circuit design
space is shown in Fig. 5a. XPert’s DNNArch has a VGG16
backbone with 13 convolution and 1 fully-connected (FC) lay-
ers with 3 ×3 kernel. The CDs are co-searched with peripheralcircuits. The DNNArch of the baseline, is a standard VGG16
network2with 13 convolution and 1 FC layer.
Phase1 and Phase2 Parameters: The Phase1 and Phase2
co-search is conducted for 2000 and 20 steps, respectively
on Pytorch 1.1.0 with Nvidia RTX-2080ti GPU backend.
Since the weights are not trained during co-search, the Phase1
and Phase2 require merely 5 and 10 GPU minutes which
is negligible compared to the training time of the Phase1-
searched DNNmodel .λ1andλ2are 0.01 and 0.001, respec-
tively. Learning rates for Phase1 and Phase2 SGD-based co-
search are 13 and 0.1, respectively. Post Phase1 co-search,
the Phase1-searched DNNmodel is trained for 200 epochs
on the respective dataset using SGD with a learning rate of
0.1 with cosine decay. Hardware evaluations of XPertNets
and VGG16 baselines are performed on the XPertSim C++
platform with parameters shown in Table II. Further, we
perform quantization/device noise-aware batchnorm adaptation
of VGG16 baselines to obtain competitive NI accuracy on IMC
implementation.
B. Comparison with Homogeneous VGG16 Implementations
Table III shows the hardware efficiency and accuracy results
of XPertNets and 4 homogeneous VGG16 implementations
with the lowest EDAP values. For CIFAR10, XPertNet C,50
(shown in Fig. 8a) having an area of 50mm2achieves
2.98×and 10.24 ×lower energy and EDAP, respectively than
VGG16 [F+CS16]. XPertNet C,50also achieves the best NI
accuracy of 92.46% (9.7% higher than VGG16 [F+CS16]).
XPertNet C,60(shown in Fig. 8a) achieves 1.72 ×and 1.93 ×
higher TOPS/W and TOPS/mm2than VGG16 [F+CS16].
For TinyImagenet, XPertNet T,50achieves 4.7% higher
NI accuracy at 4.7 ×lower EDAP than VGG16[F+CS16].
XPertNet T,60achieves 1.62 ×and 3×higher TOPS/W and
TOPS/mm2than VGG16 [F+CS16].
Energy Consumption Analysis From Fig. 7a and Fig. 7b,
we find that the XPertNet C,50andXPertNet C,60entail
significantly low ADC, Accumulation (Acc), data commu-
nication (H-Tree), storage (Buffers) and switch matrix en-
2adopted from https://github.com/kuangliu/pytorch-cifarTABLE III: Table comparing XPertNets with homogeneous VGG16 implementations for CIFAR10 and TinyImagenet datasets. The notations used are same
as in Fig. 1. All implementations are on 64 ×64 4-bit RRAM crossbars. For VGG16, AP/IP are 6 and 8, respectively. I- Ideal accuracy without any hardware
noise. NI- Non-ideal accuracy with quantization and device variation noise. XPertNet D,50- XPertNet searched for dataset D at 50mm2area constraint.
Configuration Area (mm2)Delay (ns) Energy (pJ)EDAP
(mJ×ms×mm2)TOPS/W TOPS/mm2 Accuracy
I NI
CIFAR10
VGG16 [F+CS16] 99 3.75e+06 8.02e+07 29.72 21.54 0.0062 93.97 82.7
VGG16 [F+CS8] 135 3.19e+06 7.01e+07 30.14 24.18 0.005 93.97 82.7
VGG16 [F+CS32] 81 5.04e+06 1.08e+08 43.74 16.09 0.0053 93.97 82.7
VGG16 [F+CS4] 203 4.27e+06 6.98e+07 60.50 22.63 0.0025 93.97 82.7
XPertNet C,50 50.2 2.16e+06 2.69e+07 2.9 35.7 0.01 93.6 92.46
XPertNet C,60 59.6 2.15e+06 3.32e+07 4.28 37.24 0.012 94.1 92.11
TinyImagenet
VGG16 [F+CS16] 95 9.01e+06 1.48e+08 126.5 16.1 0.0035 60 52
VGG16 [F+CS8] 137 7.76e+06 1.32e+08 140.3 17.57 0.002 60 52
VGG16 [F+CS32] 82.4 1.19e+07 1.95e+08 191.2 12.38 0.003 60 52
VGG16 [(F+CS4] 205 1.03e+07 1.34e+08 282 15.91 0.0014 60 52
XPertNet T,50 49.5 5.91e+06 8.98e+07 26.53 26.08 0.0096 58 56.7
XPertNet T,60 60.3 4.88e+06 9.28e+07 27.17 26.14 0.0107 59 56.3
(a)(b)
Fig. 7: Plots showing the (a) CIFAR10 energy (b) CIFAR10 delay of different
IMC components for VGG16, XPertNet C,50andXPertNet C,60DNN-
models . Acc- Accumulators and SM- Switch Matrices and Mux- Multiplexers.
ergy/delay which results in a high energy efficiency. Similar
observations are made in case of TinyImagenet.
C. Comparison with Prior Works
TABLE IV: Performance comparison of prior IMC-aware DNNArch
co-search works and XPert (results correspond to XPertNet C,50).
S-Searched Parameter, NS- Not Searched, X- crossbar size
Work Jiang et al. [10] NAX [11] XPert (Ours)
Backbone Network VGG11 ResNet20 VGG16
Channel Depth S NS S
Kernel Size S S NS (3×3)
Crossbar Size NS (64×64) S NS (64×64)
Weight Quantization S NS (8-b) NS (8-b)
Layer Input Precision S NS (8-b) S
ADC Precision NS (4-b) NSlog2(3X) S
ADC Type NS NS S
Column Sharing NS NS S
Device 4-b RRAM 2-b RRAM 4-b RRAM
Accuracy (CIFAR10) 93.12 92.7 92.46
EDAP (mJ ×ms×mm2) 2631 290 2.9
Baseline EDAP
Optimized EDAP0.24 1.2 10.24
Table IV compares different IMC-aware DNNArch co-
search works with XPert at iso-accuracy for CIFAR10 dataset.
As different works use different baselines and hardware
evaluation platforms, we use theBaseline EDAP
Optimized EDAPratio to
perform a fair hardware-efficiency evaluation. In [10], the
authors co-searched the channel depths, kernel size and layer-
specific quantization of a VGG11 backbone network to achieve
optimal accuracy. Although they achieved 3% higher CIFAR10
accuracy compared to the VGG11 baseline, the EDAP of
the optimized network was 4.16 ×higher than the baseline
(causingBaseline EDAP
Optimized EDAP<1). In NAX [11], the authors
performed kernel and IMC crossbar size co-search to achieve
high accuracy (92.7) with 1.2 ×lower EDAP than a ResNet20
baseline. Xpert ( XPertNet C,50), achieves aBaseline EDAP
Optimized EDAP
CD= 64CD= 128CD= 256CD= 512AT= FAT= S1616162828432324432841642323216232888443242216432164816XPertNetC,20161644824416322321644416243232161681616XPertNetC,50XPertNetC,60XPertNetC,80XPertNetC,100L1L2L3L4L5L6L7L8L9L10L11L12L13
AP= 6AP= 5IP= 4IP= 3IP= 5CD↑ CS↓ Flash ADC↑ 1616162828432324432XPertNetC,501616448244163223216XPertNetC,60(a)(b)Fig. 8: (a) XPertNet C,50andXPertNet C,60configurations. (b) XPert-
Net configurations (only convolution layers) under different area constraints
for CIFAR10 dataset. Numbers inside the shapes denote CS values.
of 10.24 ×at 92.46% accuracy attributed to the co-search of
DNNArch and peripheral circuit parameters.
D. Analysing XPertNets at Different Area Constraints
Interestingly, as seen in Fig. 8a, to achieve optimal accu-
racy, XPert assigns higher AP and IP in the shallow layers
(for better representation of input features) while reduces
the AP/IP configurations in the deeper layers to increase
energy-efficiency. The AP/IP trends are similar across other
XPertNets and hence not shown. Fig. 8b shows the CIFAR10-
based XPertNet configurations obtained under different area
constraints. As the area constraint increases, the CD and the
number of Flash ADCs in the XPertNets increase (reduction
in the number of circles and more lighter shades) while the CS
decreases. Similar trends are observed for XPertNets searched
for TinyImagenet dataset.
EDAP at Different Area Constraints: Interestingly, as
seen in Fig. 9a, the minimum EDAP is obtained at area
constraints of 50mm2and 60mm2. To explain this, we define
Ψ(plotted in Fig. 9c) as the product of average CS and SAR
ADC fraction (plotted in Fig. 9b). Ψempirically determines
the delay incurred due to the crossbar read. Higher number
of SAR ADCs and average CS in an XPertNet increases the
number of clock cycles for crossbar read and hence more delay
(also shown in Fig. 3b). At area ≤40mm2,NTis small but
theΨvalue is high due to high average CS and SAR ADC
fraction. This results in higher EDAP. For area ≥70mm2,
although Ψdecreases, the delay increases as XPertNets have
larger CDs (as seen in Fig. 8b) resulting in higher Total NT.(a)(b)(c)XPertNetC,60
XPertNetC,50
Fig. 9: (a) EDAP trend for CIFAR10-based XPertNets at iso-accuracy (b) SAR ADC fraction and average CS in XPertNets and (c) Ψand total NTof
XPertNets obtained at different area constraints. Total NTis the total number of tiles consumed by a particular XPertNet.
A-CD Only        B-CD+AT        C-CD+AT+CSD-CD+AT+CS+IP+AP
ABCDABCD(a)(b)
Fig. 10: Figure showing the area-wise (a) EDAP and (b) normalized accuracy
(with respect to the highest achieved accuracy) when different DNNArch and
peripheral circuit parameters are co-searched.
(a)(b)
Fig. 11: Comparison of (a) EDAP for different crossbar sizes (b) NI accuracy
under different device variations between XPertNet and VGG16[F+CS16].
E. Significance of Different Design Spaces
Fig. 10 shows the significance of co-search across different
DNNArch and peripheral circuit design spaces. In A, where
only CDs are searched with fixed peripheral circuit configura-
tions (CS=8, AT=F, AP=6 and IP=8), the DNNArchs sampled
at higher area constraint have high EDAP and accuracy
due to large DNNArch size that accommodates more weight
parameters. At low area constraints, the EDAP is low at the
cost of low accuracy. In B, when CD is co-searched with AT
(at CS=8, AP=6, IP=8), the accuracy at low area constraints
increases as more weight parameters can be accommodated
with the help of area-efficient SAR ADCs. In C, when CD,
AT and CS are co-searched (AP=6, IP=8), the accuracy and
EDAP at low area constraints further increases because SAR
ADCs are now combined with high CS values which helps
accommodate more weight parameters while also increasing
the EDAP. Interestingly, at higher areas, the EDAP is lowered
compared to design space B as smaller CS values are selected
to achieve low delay and energy. Finally in D, when CD, AT,
CS, AP and IP are holistically co-searched, the EDAP lowers
significantly at high accuracy across different area constraints.
F . Impact of Crossbar Sizes and Device Variation
Fig. 11a, shows that XPertNet C,50andXPertNet C,60
achieve significantly low EDAP compared to
VGG16[F+CS16] irrespective of the crossbar size used
for implementation. Additionally, XPertNets entail higher
robustness against memristive device variations compared
to VGG16[F+CS16] model as seen in Fig. 11b. This showsan interesting co-dependence between device variations and
XPert-searched DNNmodel configuration.
V. C ONCLUSION
In this work, we design XPert, a framework that performs
quick and optimized co-search in a large DNNArch -peripheral
circuit design space to obtain high accuracy, and low EDAP
DNNmodels . We show that a holistic co-search between CD,
CS, AT, AP and IP is essential to attain high accuracy at low
EDAP values. For CIFAR10 and TinyImagenet datasets, XPert
achieves 10.24 ×and 4.7 ×lower EDAP, respectively while
improving the NI-accuracy by 9.7% and 4.7%, respectively
compared to homogeneous VGG16 implementations.
ACKNOWLEDGEMENT
This work was supported in part by CoCoSys, a JUMP2.0 center
sponsored by DARPA and SRC, Google Research Scholar Award,
the National Science Foundation CAREER Award, TII (Abu Dhabi),
the DARPA AI Exploration (AIE) program, and the DoE MMICC
center SEA-CROGS (Award #DE-SC0023198).
REFERENCES
[1] Alzubaidi et al. , “Review of deep learning: Concepts, cnn architectures,
challenges, applications, future directions,” Journal of big Data , 2021.
[2] Guo et al. , “Deep learning for visual understanding: A review,” Neuro-
computing , 2016.
[3] Yu et al. , “Compute-in-memory chips for deep learning: Recent trends
and prospects,” IEEE Circuits and Systems Magazine , pp. 31–56, 2021.
[4] Sebastian et al. , “Memory devices and applications for in-memory
computing,” Nature nanotechnology , 2020.
[5] Yuan et al. , “Tinyadc: Peripheral circuit-aware weight pruning frame-
work for mixed-signal dnn accelerators,” in DATE, 2021 . IEEE.
[6] Saxena et al. , “Towards adc-less compute-in-memory accelerators for
energy efficient deep learning,” in DATE, 2022 . IEEE.
[7] Jiang et al. , “Analog-to-digital converter design exploration for compute-
in-memory accelerators,” IEEE Design & Test , pp. 48–55, 2021.
[8] Qiu et al. , “A peripheral circuit reuse structure integrated with a retimed
data flow for low power rram crossbar-based cnn,” in DATE, 2018 .
IEEE.
[9] Huang et al. , “Mixed precision quantization for reram-based dnn infer-
ence accelerators,” in ASP-DAC, 2021 . IEEE.
[10] Jiang et al. , “Device-circuit-architecture co-exploration for computing-
in-memory neural accelerators,” IEEE ToC , pp. 595–605, 2020.
[11] Negi et al. , “Nax: neural architecture and memristive xbar based
accelerator co-design,” in DAC, 2022 .
[12] Peng et al. , “Dnn+ neurosim: An end-to-end benchmarking framework
for compute-in-memory accelerators with versatile device technologies,”
inIEDM, 2019 . IEEE.
[13] Shafiee et al. , “Isaac: A convolutional neural network accelerator with
in-situ analog arithmetic in crossbars,” ACM SIGARCH Computer Ar-
chitecture News , 2016.
[14] Cai et al. , “Proxylessnas: Direct neural architecture search on target task
and hardware,” arXiv preprint arXiv:1812.00332 , 2018.
[15] Mellor et al. , “Neural architecture search without training,” in ICML .
PMLR, 2021.
[16] Tsai et al. , “Robust processing-in-memory neural networks via noise-
aware normalization,” arXiv preprint arXiv:2007.03230 , 2020.
[17] Hajri et al. , “Rram device models: A comparative analysis with experi-
mental validation,” IEEE Access , 2019.