Published as a conference paper at ICLR 2023
LIGHT SAMPLING FIELD AND BRDF R EPRESENTA -
TION FOR PHYSICALLY -BASED NEURAL RENDERING
Jing Yang, Hanyuan Xiao, Wenbin Teng , Yunxuan Cai , Yajie Zhao
Institute for Creative Technologies
University of Southern California
fjyang,hxiao,wteng,ycai,zhao g@ict.usc.edu
ABSTRACT
Physically-based rendering (PBR) is key for immersive rendering effects used
widely in the industry to showcase detailed realistic scenes from computer graph-
ics assets. A well-known caveat is that producing the same is computationally
heavy and relies on complex capture devices. Inspired by the success in qual-
ity and efÔ¨Åciency of recent volumetric neural rendering, we want to develop a
physically-based neural shader to eliminate device dependency and signiÔ¨Åcantly
boost performance. However, no existing lighting and material models in the
current neural rendering approaches can accurately represent the comprehensive
lighting models and BRDFs properties required by the PBR process. Thus, this
paper proposes a novel lighting representation that models direct and indirect light
locally through a light sampling strategy in a learned light sampling Ô¨Åeld. We also
propose BRDF models to separately represent surface/subsurface scattering de-
tails to enable complex objects such as translucent material (i.e., skin, jade). We
then implement our proposed representations with an end-to-end physically-based
neural face skin shader, which takes a standard face asset (i.e., geometry, albedo
map, and normal map) and an HDRI for illumination as inputs and generates a
photo-realistic rendering as output. Extensive experiments showcase the quality
and efÔ¨Åciency of our PBR face skin shader, indicating the effectiveness of our
proposed lighting and material representations.
1 I NTRODUCTION
Physically-based rendering (PBR) provides a shading and rendering method to accurately represent
how light interacts with objects in virtual 3D scenes. Whether working with a real-time rendering
system in computer graphics or Ô¨Ålm production, employing a PBR process will facilitate the creation
of images that look like they exist in the real world for a more immersive experience. Industrial PBR
pipelines take the guesswork out of authoring surface attributes like transparency since their method-
ology and algorithms are based on physically accurate formulae and resemble real-world materials.
This process relies on onerous artist tuning and high computational power in a long production cy-
cle. In recent years, academia has shown incredible success using differentiable neural rendering
in extensive tasks such as view synthesis (Mildenhall et al., 2020), inverse rendering (Zhang et al.,
2021a), and geometry inference (Liu et al., 2019). Driven by the efÔ¨Åciency of neural rendering, a
natural next step would be to marry neural rendering and PBR pipelines. However, none of the ex-
isting neural rendering representations supports the accuracy, expressiveness, and quality mandated
by the industrial PBR process.
A PBR workÔ¨Çow models both specular reÔ¨Çections, which refers to light reÔ¨Çected off the surface,
and diffusion or subsurface scattering, which describes the effects of light absorbed or scattered in-
ternally. Pioneering works of differentiable neural shaders such as Softras (Liu et al., 2019) adopted
the Lambertian model as BRDF representation, which only models the diffusion effects and results
in low-quality rendering. NeRF (Mildenhall et al., 2020) proposed a novel radiance Ô¨Åeld repre-
sentation for realistic view-synthesis under an emit-absorb lighting transport assumption without
explicitly modeling BRDFs or lighting, and hence is limited to a Ô¨Åxed static scene with no scope
for relighting. In follow-up work, NeRV (Srinivasan et al., 2020) took one more step by explicitly
modeling directional light, albedo, and visibility maps to make the Ô¨Åxed scene relightable. The indi-
rect illumination was achieved by ray tracing under the assumption of one bounce of incoming light.
Equal contributions. We would like to thank Marcel Ramos and Chinmay Chinara at Vision and Graphics
Lab (VGL), for their valuable help on data preparation and paper writing.
1arXiv:2304.05472v1  [cs.CV]  11 Apr 2023Published as a conference paper at ICLR 2023
However, this lighting model is computationally very heavy for real-world environment illumination
when more than one incoming directional lights exist. To address this problem, NeRD (Boss et al.,
2020) and PhySG (Zhang et al., 2021a) employ a low-cost global environment illumination mod-
eling method using spherical gaussian (SG) to extract parameters from HDRIs. Neural-PIL (Boss
et al., 2021) further proposed a pre-trained light encoding network for a more detailed global illu-
mination representation. However, it is still a global illumination representation assuming the same
value for the entire scene, which is not true in the real world, where illumination is subjected to
shadows and indirect illumination bouncing off of objects in different locations in the scene. Thus
it‚Äôs still an approximation but not an accurate representation of the environmental illumination. Re-
garding material (BRDF) modeling, all the current works adopt the basic rendering parameters (such
as albedo, roughness, and metalness) deÔ¨Åned in the rendering software when preparing the synthetic
training data. However, they will fail in modeling intricate real-world objects such as participating
media (e.g., smoke, fog) and translucent material (organics, skins, jade), where high scattering and
subsurface scattering cannot be ignored. Such objects require more effort and hence attract more
interest in research in their traditional PBR process.
In this work, we aim to design accurate, efÔ¨Åcient lighting/ illumination and BRDF representations
to enable the neural PBR process, which will support high-quality and photo-realistic rendering in
a fast and lightweight manner. To achieve this goal, we propose a novel lighting representation -
a Light Sampling Field to model both the direct and indirect illumination from HDRI environment
maps. Our Light Sampling Field representation faithfully captures the direct illumination (incom-
ing from light sources) and indirect illumination (summary of all indirect incoming lighting from
surroundings) given an arbitrary sampling location in a continuous Ô¨Åeld. Accordingly, we propose
BRDF representations in the format of surface specular, surface diffuse, and subsurface scattering
for modeling real-world object material. This paper mainly evaluates the proposed representations
with a novel volumetric neural physically-based shader for human facial skin. We trained with an
extensive high-quality database, including real captured ground truth images as well as synthetic im-
ages for illumination augmentation. We also introduce a novel way of integrating surface normals
into volumetric rendering for higher Ô¨Ådelity. Coupled with proposed lighting and BRDFs models,
our light transport module delivers pore-level realism in both on- and underneath-surface appearance
unprecedentedly. Experiments show that our Light Sampling Field is robust enough to learn illumi-
nation by local geometry. Such an effect usually can only be modeled by ray tracing. Therefore, our
method compromises neither efÔ¨Åciency nor quality with the Light Sampling Field when compared
to ray tracing.
The main contributions of this paper are as follows: 1) A novel volumetric lighting representation
that accurately encodes the direct and indirect illumination positionally and dynamically given an
environment map. Our local representation enables efÔ¨Åcient modeling of complicated shading ef-
fects such as inter-reÔ¨Çectance in neural rendering for the Ô¨Årst time as far as we are aware. 2) A BRDF
measurement representation that supports the PBR process by modeling specular, diffuse, and sub-
surface scattering separately. 3) A novel and lightweight neural PBR face shader that takes facial
skin assets and environment maps (HDRIs) as input and efÔ¨Åciently renders photo-realistic, high-
Ô¨Ådelity, and accurate images comparable to industrial traditional PBR pipelines such as Maya. Our
face shader is trained with an image database consisting of extensive identities and illuminations.
Once trained, our models will extract lighting models and BRDFs from input assets, which works
well for novel subjects/ illumination maps. Experiments show that our PBR face shader signiÔ¨Å-
cantly outperforms the state-of-the-art neural face rendering approaches with regard to quality and
accuracy, which indicates the effectiveness of the proposed lighting and material representations.
2 R ELATED WORK
Volumetric Neural Rendering V olumetric rendering models the light interactions with volume
densities of absorbing, glowing, reÔ¨Çecting, and scattering materials (Max, 1995). A neural volu-
metric shader trains a model from a set of images and queries rendered novel images. The recent
state-of-the-art was summarized in a survey (Tewari et al., 2020). In addition, Zhang et al. (2019)
introduced the radiance Ô¨Åeld as a differentiable theory of radiative transfer. Neural Radiance Field
(NeRF) (Mildenhall et al., 2020) further described scenes as differentiable neural representation and
the following raycasting integrated color in terms of the transmittance factor, volume density, and the
voxel diffuse color. Extensions to NeRF were developed for better image encoding (Yu et al., 2020),
ray marching (Bi et al., 2020a), network efÔ¨Åciency (Lombardi et al., 2021; Yariv et al., 2020), real-
istic shading (Suhail et al., 2022) and volumetric radiative decomposition (Bi et al., 2020b; Rebain
2Published as a conference paper at ICLR 2023
et al., 2020; Zhang et al., 2021c; Verbin et al., 2021). In particular, NeRV (Srinivasan et al., 2020),
NeRD (Boss et al., 2020; 2021) decomposed the reconstructed volume into geometry, SVBRDF, and
illumination given a set of images even under varying lighting conditions. RNR (Chen et al., 2020b)
assumed the environmental illumination as distant light and is able to decompose the scene into an
albedo map with a 10-order spherical harmonics (SH) of incoming directions.
Portrait and Face Relighting Early single-image relighting techniques utilize CNN-based image
translation (Nalbach et al., 2017; Thies et al., 2019). Due to the lack of 3D models, image trans-
lation approaches cannot recover surface materials or represent realistic high-Ô¨Ådelity details, thus
neural volumetric relighting approaches are widely adopted recently. Ma et al. (2021) proposed a
lightweight representation to decode only the visible pixels during rendering. Bi et al. (2021) The
face relighting utilized strong priors. Zhou et al. (2019) Ô¨Åtted a 3D face model to the input image
and obtained reÔ¨Åned normal to help achieve relighting. Chen et al. (2020a) relit the image by us-
ing spherical harmonics lighting on a predicted 3D face. Hou et al. (2022) introduced a shadow
mask estimation module to achieve novel face relighting with geometrically consistent shadows.
With high-quality volumetric capture in lightstage (Debevec et al., 2000) to obtain training data, this
trend achieved the following: regression of a one-light-at-a-time (OLAT) image for relighting (Meka
et al., 2019), encoding the feature tensors for a Phong shading into UV space and relighting using an
HDRI map (Meka et al., 2020), a neural renderer that can predict the non-diffuse residuals (Zhang
et al., 2021b). Bi et al. (2021) proposed neural networks to learn relighting implicitly but lacked
modeling both surface and subsurface reÔ¨Çectance properties following physical light transport. Sim-
ilar to our approach most, Sun et al. (2021) inferred both light transport and density, and enabled
relighting and view synthesis from a sparse set of input images.
3 M ETHODS
3.1 P RELIMINARIES
Figure 1: Skin Scattering Model. The incident light Liscattered
from the skin consists of an oil layer specular reÔ¨Çection (a) or pen-
etrates one or more scattering layers (b, c, d) at some point under-
neath the oil layer. The difference in melanin distribution or color
results in the diversity of skin appearance. In humans, the distribu-
tion and color of melanin is the primary determinant of skin appear-
ance. Darker and denser melanin in the epidermis leads to darker
skin color and vice versa.
We use the rendering equation Kajiya (1986) to estimate the radiance Lat a 3D point xwith the
outgoing direction !o:L(x;!o) =R
!i2
+f(x;!o;!i)Li(x;!i)(!inx)d!i, wheref(x;!o;!i)
is the BRDF representation and Li(x;!i)measures the radiance of incident light with direction !i.
Generally, the incident light could be categorized as direct vs. indirect light. Fig. 1 illustrates
an example of direct light (path (a), (b)) and indirect light (path (c)) in human face skin, where
subsurface scattering happens in the deeper dermis layer, causing the nearby area to receive indirect
light. Therefore, the rendering formulation can be split into separate components with direct and
indirect lighting:
L(x;!o)=Z
!i2
+fs(x;!o;!i)Ld
i(x;!i)(!inx)d!i+Z
!i2
fss(x;!o;!i)Lid
i(x;!i)(!inx)d!i(1)
wherefs,fssrepresent the BRDF evaluation of surface and subsurface, respectively. Following the
render equation, we design a lightweight physically-based rendering method by learning different
BRDF representations and modeling the direct and indirect lights, as we will introduce in the next
few sections.
3.2 I LLUMINATION AND BIDIRECTIONAL REFLECTANCE DISTRIBUTION LEARNING
We propose a lighting model and material model to construct the Light Sampling Field and estimate
the BRDF representations, as will be introduced in the next several sections.
3.2.1 L IGHTING MODEL
Considering the interaction between light and different skin layers, we propose a novel light model-
ing method using HDRIs that decomposes lighting into direct illumination and indirect illumination.
3Published as a conference paper at ICLR 2023
For direct illumination, We use light importance sampling to simulate external light sources, such as
light bulbs. And we implement ray tracing for the specular reÔ¨Çectance effect. For indirect illumina-
tion, we introduce a Light Sampling Field that models location-aware illumination using SH. This
learned local light-probe models subsurface scattering and inter-reÔ¨Çectance effects.
Uniform Spherical Sampling HDRI Image Importance Sampling
(a) Importance Light Sampling.
 (b) Light Sampling Field
Visualization
Figure 2: Lighting Model. (a) We use importance sampling to sample heavily on pixels with high
intensity. (b) We use various densities of sampling to bring attention to locations with data as in the
Light Sampling Field.
Importance Light Sampling for Direct Illumination. Direct radiance comes from the HDRI
map directly. In order to compute the contribution of pixels in HDRI to different points in our
radiance Ô¨Åeld, we use a SkyDome to represent direct illumination by projecting the HDRI environ-
ment map onto a sphere. Each pixel on the sphere is regarded as a distant directional light source.
Hence, direct lighting is identical at all locations in the radiance Ô¨Åeld. Such representation preserves
specular reÔ¨Çection usually achieved by ray tracing methods. We take two steps to construct the rep-
resentation. Firstly, we uniformly sample a point grid of size N= 800 on the sphere, where each
point is a light source candidate. Secondly, we apply our importance light sampling method to Ô¨Ålter
valid candidates by two thresholds: 1) an intensity threshold that clips the intensity of outliers with
extreme values; 2) an importance threshold that Ô¨Ålters out outliers in textureless regions. Fig. 2a
illustrates this process.
(a) Material Network
 (b) Light Sampling Field Network
Figure 3: Material and Light Sampling Field Network. (a) Our material network takes in 3D po-
sition and view direction as inputs and predicts the specular strength and skin scattering parameters
of our BRDF model. (b) The Light Sampling Field network applies similar differentiable network
but with the light sampling created by our importance light sampling technique as another input to
predict SH coefÔ¨Åcients.
Light Sampling Field for Indirect Illumination. Indirect illumination models the incoming
lights reÔ¨Çected or emitted from surrounding objects, which is achieved by ray tracing with the
assumption of limited bounce times in the traditional PBR pipeline. Inspired by the volumetric
lightmaps used in Unreal Engine (Karis & Games, 2013), which stores precomputed lighting in
sampled points and use them for interpolation at runtime for modeling indirect lighting of dynamic
and moving objects, We adopt a continuous Light Sampling Field for accurately modeling the il-
lumination variation in different scene positions. We use Spherical Harmonics (SH) to model the
total incoming lights at each sampled location separately. We compute the local SH by multiplying
Ô¨Åxed Laplace‚Äôs SH basis with predicted SH parameters. SpeciÔ¨Åcally, we use SH of degree l= 1for
each color channel (RGB). Therefore, we acquire 3(color channels)4(basis) = 12 -D vector as
local SH representation. We downsample the HDRI map to 100150resolution and project it to a
sphere. Each pixel on the map is considered an input lighting source. We use the direction and color
of each pixel as the lighting embedding to feed into a light Ô¨Åeld sampling network for inference of
4Published as a conference paper at ICLR 2023
coefÔ¨Åcients of local SH. We visualize our Light Sampling Field with selected discrete sample points
in Fig. 2b.
Learning Light Sampling Field. We design a network (Fig. 3b) to predict the spherical Harmon-
ics coefÔ¨Åcient Cm
kof a continuous light Ô¨Åeld. The inputs of this network are the lighting embedding
zl, the positional encoding of 3D location x, and view direction !. Conditioned on the lighting rep-
resentations, the network succeeds in predicting the accurate and location-aware lighting. Fig. 6b
evaluates our lighting model.
3.2.2 MATERIAL MODEL
The choice of reÔ¨Çectance parameters usually relies on artists‚Äô tuning and therefore requires high
computational power and results in a long production cycle, but sometimes not very ideal. To tackle
the problem, we propose a lightweight material network (Fig. 3a) to estimate the BRDF parameters
including specular strength 2Rand skin scattering 2R3through learnable parameters. The
parameters are crucial to represent the reÔ¨Çectance property of both surface and subsurface. Together
with input albedo , we can construct a comprehensive BRDF to model surface reÔ¨Çection and sub-
surface scattering. It consists of a surface specular component fss, a surface diffuse component fsd,
and a subsurface scattering component fsss. Refer to Sec. 3.3 for a detailed light transport and Fig.5
for an evaluation of modeling subsurface scattering.
3.3 L IGHT TRANSPORT
Light transport deÔ¨Ånes a light path from the luminaire to the receiver. We introduce the light trans-
port that connects our lighting model and material model in Fig. 4. We also detail the rendering
equation in this section to match the light transport along the light path.
R/gid00052/gid00079/gid00068/gid00066/gid00084/gid00075/gid00064/gid00081/gid00001
/gid00052/gid00083/gid00081/gid00068/gid00077/gid00070/gid00083/gid00071
/gid00052/gid00066/gid00064/gid00083/gid00083/gid00068/gid00081/gid00072/gid00077/gid00070
/gid00045/gid00072/gid00070/gid00071/gid00083/gid00001/gid00052/gid00064/gid00076/gid00079/gid00075/gid00072/gid00077/gid00070/gid00001/gid00039/gid00072/gid00068/gid00075/gid00067/gid00034/gid00075/gid00065/gid00068/gid00067/gid00078/gid00001/gid00046/gid00064/gid00079 /gid00047/gid00078/gid00081/gid00076/gid00064/gid00075/gid00001/gid00046/gid00064/gid00079
/gid00045/gid00072/gid00070/gid00071/gid00083/gid00079/gid00081/gid00078/gid00065/gid00068/gid00051/gid00064/gid00088/gid00001/gid00037/gid00072/gid00081/gid00068/gid00066/gid00083/gid00072/gid00078/gid00077/gid00064/gid00075/gid00001
/gid00038/gid00077/gid00066/gid00078/gid00067/gid00072/gid00077/gid00070
/gid00052/gid00064/gid00076/gid00079/gid00075/gid00068/gid00001/gid00049/gid00078/gid00082/gid00072/gid00083/gid00072/gid00078/gid00077/gid00064/gid00075/gid00001
/gid00038/gid00077/gid00066/gid00078/gid00067/gid00072/gid00077/gid00070
/gid00052/gid00079/gid00068/gid00066/gid00084/gid00075/gid00064/gid00081/gid00001/gid00035/gid00064/gid00075/gid00075
/gid00037/gid00072/gid00193/gid00084/gid00082/gid00068/gid00001/gid00035/gid00064/gid00075/gid00075/gid00041/gid00037/gid00051/gid00042 /gid00042/gid00076/gid00079/gid00078/gid00081/gid00083/gid00064/gid00077/gid00066/gid00068/gid00001/gid00052/gid00064/gid00076/gid00079/gid00075/gid00072/gid00077/gid00070/gid00055/gid00072/gid00068/gid00086/gid00001
/gid00037/gid00072/gid00081/gid00068/gid00066/gid00083/gid00072/gid00078/gid00077
/gid00052/gid00064/gid00076/gid00079/gid00075/gid00068/gid00067/gid00001/gid00049/gid00078/gid00072/gid00077/gid00083/gid00046/gid00064/gid00083/gid00068/gid00081/gid00072/gid00064/gid00075/gid00001/gid00047/gid00068/gid00083/gid00086/gid00078/gid00081/gid00074
/gid00045/gid00072/gid00070/gid00071/gid00083/gid00001/gid00052/gid00064/gid00076/gid00079/gid00075/gid00072/gid00077/gid00070/gid00001/gid00039/gid00072/gid00068/gid00075/gid00067
/gid00047/gid00068/gid00083/gid00086/gid00078/gid00081/gid00074 /gid00045/gid00072/gid00070/gid00071/gid00083/gid00001/gid00053/gid00081/gid00064/gid00077/gid00082/gid00079/gid00078/gid00081/gid00083/gid00039/gid00072/gid00077/gid00064/gid00075/gid00001/gid00051/gid00068/gid00077/gid00067/gid00068/gid00081/gid00072/gid00077/gid00070
/gid00040/gid00068/gid00078/gid00076/gid00068/gid00083/gid00081/gid00088
/gid00055/gid00078/gid00075/gid00084/gid00076/gid00068
/gid00035/gid00052/gid00037/gid00039
/gid00035/gid00052/gid00052/gid00051/gid00037/gid00039
/gid00042/gid00077/gid00079/gid00084/gid00083
/gid00037/gid00072/gid00081/gid00068/gid00066/gid00083/gid00001/gid00042/gid00075/gid00075/gid00084/gid00076/gid00072/gid00077/gid00064/gid00083/gid00072/gid00078/gid00077
/gid00042/gid00077/gid00067/gid00072/gid00081/gid00068/gid00066/gid00083/gid00001/gid00042/gid00075/gid00075/gid00084/gid00076/gid00072/gid00077/gid00064/gid00083/gid00072/gid00078/gid00077
R/gid00051/gid00068/gid00077/gid00067/gid00068/gid00081/gid00072/gid00077/gid00070/gid00001/gid00038/gid00080/gid00084/gid00064/gid00083/gid00072/gid00078/gid00077
Figure 4: Light Transport. Starting from sampling lights from HDRI and 3D points close to
geometry along the light path, we feed encoded locations, view direction to Material network and
Light Sampling Field network (with extra direct illumination code). The networks output material
and indirect illumination code. Our PBR equation generates radiance at each sample based on our
material model and light transport in the volume and further composites all the radiance on the light
path to the receiver.
Volume Casting. Our lighting model includes direct illumination and indirect illumination. Light
transports from explicit lights and casts along the light path. We, therefore, deÔ¨Åne the light transport
along the light path, p, in the volume by the following:
L(^ x;!o) =Z1
0(t)(p(t))L(p(t);!o)dt (2)
whereL(^ x;!o)is the total radiance at ^ xalong the light path direction !o.(x)is the volume
density, which is converted from the input geometry. (x) = exp( Rx
0(p(t))dt)is the visibility
that indicates whether the location xis visible on the light path, where p(t)represents the 3D
location on the light path along !ofrom ^ xand deÔ¨Åned as p(t) =^ x+t!o.
We use explicit geometry to construct a more reliable density Ô¨Åeld. More speciÔ¨Åcally, we locate
the intersection coordinates, x0, between any arbitrary light path and the input geometry. For any
point xalong the light path !, we deÔ¨Åne the density as the following central Gaussian distribution
5Published as a conference paper at ICLR 2023
function:(x) =exp ( dG(x)2=(22)), wheredG(x)is the distance between xand the
intersection x0,andare two scalars that determine the magnitude and standard deviation of the
gaussian distribution.
Material Scattering. The light transport between the scene objects and light sources is charac-
terized by the rendering equation. Eqn. 1 introduce the rendering covering direct illumination and
indirect illumination. Also, classiÔ¨Åed by reÔ¨Çection location, the reÔ¨Çected radiance L(x;!o)has two
components: surface reÔ¨Çectance and subsurface volume scattering. To have a comprehensive light
transport representation, we further develop the equation with the dissection of light as well as the
specialized BSSRDF components:
L(x;!o)=Z
!i2
+fss(x;!o;!i)Ld
i(x;!i)j!inxjd!i
| {z }
surface specular reÔ¨Çectance with direct light+Z
!i2
+fsd(x;!o;!i)Ld
i(x;!i)j!inxjd!i
| {z }
surface diffuse reÔ¨Çectance with direct light
+Z
!i2
fsss(x;!o;!i)Lid
i(x;!i)j!inxjd!i
| {z }
subsurface scattering with indirect light(3)
hereLd
i(x;!i)andLid
i(x;!i)are the incoming direct and indirect radiance from direction !iat
point x, respectively. fss,fsd, andfsssare the different counterparts of light transport parameterized
by material representations. We show the complete light transport in Eqn. 4 consists of our light and
material representation.
L(x;!o)=xZ
!i2
+Ld
i(x;!i)j!oR(!i;nx)jed!i
| {z }
surface specular reÔ¨Çectance with direct light+s
x
Z
!i2
+Ld
i(x;!i)j!inxjd!i
| {z }
surface diffuse reÔ¨Çectance with direct light
+ss
x+x
Z
!i2
Lid
i(x;!i)j!inxjd!i
| {z }
subsurface scattering with indirect light(4)
wherexandxare the predicted specular strength and scattering from material network at x.
R(!i;nx)denotes the reÔ¨Çection direction of !iat the surface with normal nxandedenotes the
specular exponent. Also, s
xandss
xare surface and subsurface albedo at xsampled from the input
albedo map correspondingly with geometry.
4 I MPLEMENTATION DETAILS
In order to construct the density Ô¨Åeld , we setandto be 10and0:5, respectively. We further
compare the rendering results of other values and visualize them in the Appendix. In the constructed
radiance Ô¨Åeld, to sample rays, we draw 1024 random rays per batch. Along each ray, we sample
64points for the shading model. The low-frequency location of 3D points and direction of rays
are transformed to high-frequency input via positional encoding and directional encoding respec-
tively (Mildenhall et al., 2020). The length of encoded position and view direction is 37and63
respectively in the material network and the Light Sampling Field network. Also, importance light
sampling takes 800light samples z2R3from the HDRI input for direct lighting. We further down-
sample the input HDRI and embedded all pixels as a light embedding zl2R615000to feed into
the Light Sampling Field network. We use an 8-layer MLP with 256 neurons in each layer for both
networks. For the material network, encoded sample locations are fed in the Ô¨Årst layer of the MLP,
while the encoded view direction is later fed in layer 4. The output of material MLP for each queried
3D point is specular strength 2R, and scattering 2R3. The Light Sampling Field network has
a similar network structure but also has direct light embedding in layer 4 as input and outputs en-
coded spherical coefÔ¨Åcients Cm
k2R12for indirect lighting. During light transport, we obtain a
weighted value for each ray based on distribution among the sampled points along the ray. After
introducing values from pre-processed albedo and normal maps, the value of each component on
rays is gathered and visualized as an image with pixels representing their intensities following our
rendering Eqn. 4. Finally, the rendered RGB values are constrained with ground truth by an MSE
loss. In our application, MLP modules can converge in 50;000iterations ( 2:6hour) on a single Tesla
V100, with decent results on the same level of detail as reference images.
6Published as a conference paper at ICLR 2023
5 E XPERIMENTS AND EVALUATION
5.1 T RAINING DATASET
Our training dataset is composed of a synthetic image dataset and a Lightstage-scanned image
dataset. In synthetic dataset , we used a professionally-tuned Maya face shader to render 40-view
colored images under all combinations between 21 face assets and 101 HDRI+86 OLAT illumi-
nation. Lightstage-scan dataset consists of 16-view captured colored images of 48 subjects in 27
expressions under white illumination. We carefully selected subjects in both dataset preparation to
cover diverse ages, skin colors, and gender. Further details can be found in Appendix. A.
5.2 E VALUATION AND ANALYSIS
Figure 5: Material Modeling. Under HDRI illumination in column (a), we show the appearance
of (b) Lambertian BRDF, (c) Cook-Torrance BRDF, and (d) ours (including direct diffuse, direct
specular, and indirect diffuse). We adjust the intensity of each component for visualization purposes.
(a) Appearance at three different locations under the same illumination
(b) Lighting Model Evaluation.
Figure 6: Light Evaluation. (a) Given HDRI illumination, we capture the appearance of a mirror ball
and a grey ball in the Light Stage at different locations. (b) We rendered a uniform white ball and a
grid of such balls with different lighting models: SH (degree l= 1), SG, Neural-PIL, and ours. We
separated diffuse lighting and specular lighting by turning off the corresponding component in the
rendering equation.
7Published as a conference paper at ICLR 2023
Material Model Evaluation. We conduct a comparison experiment with only surface scattering
(BRDF) in Fig. 5, which presents two BRDF materials (middle two columns) and our proposed
material with layer-by-layer decomposition (right four columns). From the comparison, Cook-
Torrance BRDF has a more specular effect than Lambertian yet saves rubber-alike appearance from
the uncanny valley. Apart from surface scattering, our method also predicts subsurface scattering to
achieve a vivid look around the nose tip and ears by depicting red blood cells and vessel color.
(a)
 (b)
 (c)
Figure 7: Evaluation of light and
material modeling. (a) Rendering
results using proposed materials and
location-aware local SH as lighting
model. (b) Rendering results us-
ing proposed materials with global
SH. (c) Rendering results with global
SH and without using the subsurface
scattering layer in the material. We
also demonstrate the inter-reÔ¨Çection
effects in the zoom-in boxes. Com-
pare to (c) and (b), (a) achieves soft
shadows.Lighting Model Evaluation. We show our captured real
data under a real Ô¨Åxed HDRI illumination in Fig. 6a. Fig. 6b
illustrates uniform white ball illumination within a scene
rendered by SH lighting (degree l= 1), Spherical Gaus-
sian (SG) Neural-PIL(Boss et al., 2021) and our method,
respectively. Compared with other models, our proposed
method delivers the highest Ô¨Ådelity of illumination with the
widest spectrum of light as well as a lighting Ô¨Åeld sensi-
tive to lighting distribution. We further provide an extensive
ablation study to validate our light sampling for modeling
direct illumination in Fig. 15.
Evaluation of Light and Material Modeling in indirect
illumination. We evaluate our light and material compo-
nents in Fig. 7. In (c), we use pre-calculated SH of degree
l= 1to model the global illumination and albedo as a dif-
fuse scattering to render the face, resulting in a face im-
age with strong shadows and an unnatural appearance. In
(b), we introduce learned subsurface scattering in material
but still use the same pre-calculated SH of degree l= 1
as global illumination, which results in color shift and arti-
facts. In (c), we further introduce local SH and infer a light
sampling Ô¨Åeld to replace the pre-calculated SH. Together
with the full spectrum of material layers, we achieve real-
istic rendering effects. In particular, we demonstrate inter-
reÔ¨Çection effects in the zoom-in box. The shadow is soft-
ened by modeling the scattering and positional illumination.
Inverse Rendering. Our method can also achieve high-
Ô¨Ådelity inverse rendering with multi-view images (under
various illumination) instead of geometry and texture maps
as input. To make this possible, we additionally imple-
mented MLP to predict a density Ô¨Åeld. We present our results under novel illumination in Fig. 8.
Figure 8: Inverse Rendering. (a) An
example input of a set of multi-view im-
ages. (b) Reference view under the in-
put illumination. (c) Inverse rendering
results in two novel environment illumi-
nations.
5.3 Q UALITATIVE RESULTS
Figure 9: General objects.Rendering on General Object Assets. We picked orange and
meat in addition to face subjects to show that our method is gen-
eralizable on diverse objects with multi-layered structures in Fig. 9.
Through testing on different organic materials, we show consis-
tent sharpness and realistic appearance, especially in accomplishing
specular reÔ¨Çection on orange and accurate soft shadow on meat.
Maya Comparison. We compare our renderings with Maya, an
industry-level rendering engine, under HDRI or OLAT (one-light-at-a-time) illumination and present
zoom-in pore-level details in Fig. 10. The zoom-in inspections show comparable or even better
8Published as a conference paper at ICLR 2023
(a) HDRI
(b) OLAT
Figure 10: Qualitative comparison between ours and Maya renderings with zoom-in under (a) HDRI
and (b) OLAT. The left images and green crops in each pair are Maya renderings. The right images
and yellow crops in each pair are ours. Our result achieves a realistic appearance of skin texture in
addition to accurate diffuse and ambient illumination effects.
rendering results in their sharpness or illumination. Skin wrinkles and forehead specularity are
particularly rich and sharp from the proposed method. At testing time, with the same rendering
assets and queries (2500 2500 in resolution), our method requires the training images with only
800800 in resolution. Under OLAT settings, our method casts hard shadows and soft shadows as
accurately as Maya. More comparisons and qualitative results can be found in Fig. 17 and Fig. 18.
Qualitative Comparison. In Fig. 16, we compare the rendering results of FRF (Tewari et al.,
2021), NeLF (Sun et al., 2021), SIPR (Sun et al., 2019), Neural-PIL (Boss et al., 2021), and our
methods under HDRI and OLAT illuminations. We present more testing performance of our trained-
once models on other novel subjects in other datasets in Fig.19.
5.4 Q UANTITATIVE RESULTS
We evaluated PSNR, SSIM, and LPIPS in quantitative measurements (Table 1) with Maya rendering
as a benchmark. SpeciÔ¨Åcally, LPIPS evaluates the perception on the VGG-19 network. Ours outper-
forms other baseline methods in all three metrics. Moreover, We do not provide the SSIM score of
NeLF due to the slight view difference.
Baseline FRF SIPR NeLF Neural-PIL Ours
PSNR" 29.83 33.43 33.80 33.22 36.69
SSIM" 0.4489 0.8507 N/A 0.9153 0.9250
LPIPS (VGG)#0.6017 0.1265 0.7538 0.1082 0.0664
Table 1: Facial rendering metrics at 800800resolution.
6 C ONCLUSION
We demonstrate that the prior neural rendering representation for physically-based rendering fails
to accurately model environment lighting or capture subsurface details. In this work, we propose a
differentiable light sampling Ô¨Åeld network that models dynamic illumination and indirect lighting in
a lightweight manner. In addition, we propose a Ô¨Çexible material network that models subsurface
scattering for complicated materials such as the human face. Experiments on both synthetic and real-
world datasets demonstrate that our light sampling Ô¨Åeld and material network collectively improve
the rendering quality under complicated illumination compared with prior works. In the future, we
will focus on modeling more complicated materials such as translucent materials and participated
media. We will also collect datasets based on general objects and apply them for extensive tasks
such as inverse rendering.
9Published as a conference paper at ICLR 2023
7 A CKNOWLEDGMENT
This research is sponsored by the U.S. Army Research Laboratory (ARL) under contract number
W911NF-14-D-0005. Army Research OfÔ¨Åce also sponsored this research under Cooperative Agree-
ment Number W911NF-20-2-0053. We would also would like to acknowledge Sony Corporation
of America R&D Center, US Lab for their support. Statements and opinions expressed, and content
included, do not necessarily reÔ¨Çect the position or the policy of the Government, and no ofÔ¨Åcial
endorsement should be inferred. Further, the views and conclusions contained in this document
are those of the authors and should not be interpreted as representing the ofÔ¨Åcial policies, either
expressed or implied, of the Army Research OfÔ¨Åce or the U.S. Government. The U.S. Govern-
ment is authorized to reproduce and distribute reprints for Government purposes notwithstanding
any copyright notation herein.
REFERENCES
Sai Bi, Zexiang Xu, Pratul Srinivasan, Ben Mildenhall, Kalyan Sunkavalli, Milo Àás Ha Àásan, Yannick
Hold-Geoffroy, David Kriegman, and Ravi Ramamoorthi. Neural reÔ¨Çectance Ô¨Åelds for appearance
acquisition. arXiv preprint arXiv:2008.03824 , 2020a.
Sai Bi, Zexiang Xu, Kalyan Sunkavalli, Milo Àás Ha Àásan, Yannick Hold-Geoffroy, David Kriegman,
and Ravi Ramamoorthi. Deep reÔ¨Çectance volumes: Relightable reconstructions from multi-view
photometric images. arXiv preprint arXiv:2007.09892 , 2020b.
Sai Bi, Stephen Lombardi, Shunsuke Saito, Tomas Simon, Shih-En Wei, Kevyn Mcphail, Ravi Ra-
mamoorthi, Yaser Sheikh, and Jason Saragih. Deep relightable appearance models for animatable
faces. ACM Transactions on Graphics (TOG) , 40(4):1‚Äì15, 2021.
Mark Boss, Raphael Braun, Varun Jampani, Jonathan T Barron, Ce Liu, and Hendrik Lensch. Nerd:
Neural reÔ¨Çectance decomposition from image collections. CoRR , 2020.
Mark Boss, Varun Jampani, Raphael Braun, Ce Liu, Jonathan Barron, and Hendrik Lensch. Neural-
pil: Neural pre-integrated lighting for reÔ¨Çectance decomposition. Advances in Neural Information
Processing Systems , 34, 2021.
Yajing Chen, Fanzi Wu, Zeyu Wang, Yibing Song, Yonggen Ling, and Linchao Bao. Self-supervised
learning of detailed 3d face reconstruction. IEEE Transactions on Image Processing , 29:8696‚Äì
8705, 2020a.
Zhang Chen, Anpei Chen, Guli Zhang, Chengyuan Wang, Yu Ji, Kiriakos N Kutulakos, and Jingyi
Yu. A neural rendering framework for free-viewpoint relighting. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition , pp. 5599‚Äì5610, 2020b.
Paul Debevec. The light stages and their applications to photoreal digital actors. SIGGRAPH Asia ,
2(4):1‚Äì6, 2012.
Paul Debevec, Tim Hawkins, Chris Tchou, Haarm-Pieter Duiker, Westley Sarokin, and Mark Sagar.
Acquiring the reÔ¨Çectance Ô¨Åeld of a human face. In Proceedings of the 27th annual conference on
Computer graphics and interactive techniques , pp. 145‚Äì156, 2000.
Abhijeet Ghosh, Graham Fyffe, Borom Tunwattanapong, Jay Busch, Xueming Yu, and Paul De-
bevec. Multiview face capture using polarized spherical gradient illumination. In Proceedings of
the 2011 SIGGRAPH Asia Conference , pp. 1‚Äì10, 2011.
Andrew Hou, Michel Sarkis, Ning Bi, Yiying Tong, and Xiaoming Liu. Face relighting with geo-
metrically consistent shadows. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pp. 4217‚Äì4226, 2022.
James T. Kajiya. The rendering equation. SIGGRAPH Comput. Graph. , 20(4):143‚Äì150, aug 1986.
ISSN 0097-8930. doi: 10.1145/15886.15902. URL https://doi.org/10.1145/15886.
15902 .
Brian Karis and Epic Games. Real shading in unreal engine 4. Proc. Physically Based Shading
Theory Practice , 4:3, 2013.
10Published as a conference paper at ICLR 2023
Shichen Liu, Tianye Li, Weikai Chen, and Hao Li. Soft rasterizer: A differentiable renderer for
image-based 3d reasoning. The IEEE International Conference on Computer Vision (ICCV) , Oct
2019.
Stephen Lombardi, Tomas Simon, Gabriel Schwartz, Michael Zollhoefer, Yaser Sheikh, and Ja-
son Saragih. Mixture of volumetric primitives for efÔ¨Åcient neural rendering. arXiv preprint
arXiv:2103.01954 , 2021.
Shugao Ma, Tomas Simon, Jason Saragih, Dawei Wang, Yuecheng Li, Fernando De La Torre, and
Yaser Sheikh. Pixel codec avatars. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pp. 64‚Äì73, 2021.
Wan-Chun Ma, Tim Hawkins, Pieter Peers, Charles-Felix Chabert, Malte Weiss, Paul E Debevec,
et al. Rapid acquisition of specular and diffuse normal maps from polarized spherical gradient
illumination. Rendering Techniques , 2007(9):10, 2007.
Nelson Max. Optical models for direct volume rendering. IEEE Transactions on Visualization and
Computer Graphics , 1(2):99‚Äì108, 1995.
Abhimitra Meka, Christian Haene, Rohit Pandey, Michael Zollh ¬®ofer, Sean Fanello, Graham Fyffe,
Adarsh Kowdle, Xueming Yu, Jay Busch, Jason Dourgarian, et al. Deep reÔ¨Çectance Ô¨Åelds: high-
quality facial reÔ¨Çectance Ô¨Åeld inference from color gradient illumination. ACM Transactions on
Graphics (TOG) , 38(4):1‚Äì12, 2019.
Abhimitra Meka, Rohit Pandey, Christian H ¬®ane, Sergio Orts-Escolano, Peter Barnum, Philip David-
Son, Daniel Erickson, Yinda Zhang, Jonathan Taylor, SoÔ¨Åen Bouaziz, et al. Deep relightable
textures: volumetric performance capture with neural rendering. ACM Transactions on Graphics
(TOG) , 39(6):1‚Äì21, 2020.
Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and
Ren Ng. Nerf: Representing scenes as neural radiance Ô¨Åelds for view synthesis. In European
Conference on Computer Vision , pp. 405‚Äì421. Springer, 2020.
Oliver Nalbach, Elena Arabadzhiyska, Dushyant Mehta, H-P Seidel, and Tobias Ritschel. Deep
shading: convolutional neural networks for screen space shading. In Computer graphics forum ,
volume 36, pp. 65‚Äì78. Wiley Online Library, 2017.
Daniel Rebain, Wei Jiang, Soroosh Yazdani, Ke Li, Kwang Moo Yi, and Andrea Tagliasacchi. Derf:
Decomposed radiance Ô¨Åelds. arXiv preprint arXiv:2011.12490 , 2020.
Pratul P Srinivasan, Boyang Deng, Xiuming Zhang, Matthew Tancik, Ben Mildenhall, and
Jonathan T Barron. Nerv: Neural reÔ¨Çectance and visibility Ô¨Åelds for relighting and view syn-
thesis. arXiv preprint arXiv:2012.03927 , 2020.
Mohammed Suhail, Carlos Esteves, Leonid Sigal, and Ameesh Makadia. Light Ô¨Åeld neural render-
ing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition ,
pp. 8269‚Äì8279, 2022.
Tiancheng Sun, Jonathan T Barron, Yun-Ta Tsai, Zexiang Xu, Xueming Yu, Graham Fyffe,
Christoph Rhemann, Jay Busch, Paul E Debevec, and Ravi Ramamoorthi. Single image portrait
relighting. ACM Trans. Graph. , 38(4):79‚Äì1, 2019.
Tiancheng Sun, Kai-En Lin, Sai Bi, Zexiang Xu, and Ravi Ramamoorthi. Nelf: Neural light-
transport Ô¨Åeld for portrait view synthesis and relighting. arXiv preprint arXiv:2107.12351 , 2021.
Ayush Tewari, Ohad Fried, Justus Thies, Vincent Sitzmann, Stephen Lombardi, Kalyan Sunkavalli,
Ricardo Martin-Brualla, Tomas Simon, Jason Saragih, Matthias Nie√üner, et al. State of the art on
neural rendering. In Computer Graphics Forum , volume 39, pp. 701‚Äì727. Wiley Online Library,
2020.
Ayush Tewari, Tae-Hyun Oh, Tim Weyrich, Bernd Bickel, Hans-Peter Seidel, Hanspeter PÔ¨Åster,
Wojciech Matusik, Mohamed Elgharib, Christian Theobalt, et al. Monocular reconstruction of
neural face reÔ¨Çectance Ô¨Åelds. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pp. 4791‚Äì4800, 2021.
11Published as a conference paper at ICLR 2023
Justus Thies, Michael Zollh ¬®ofer, and Matthias Nie√üner. Deferred neural rendering: Image synthesis
using neural textures. ACM Transactions on Graphics (TOG) , 38(4):1‚Äì12, 2019.
Dor Verbin, Peter Hedman, Ben Mildenhall, Todd Zickler, Jonathan T Barron, and Pratul P Srini-
vasan. Ref-nerf: Structured view-dependent appearance for neural radiance Ô¨Åelds. arXiv preprint
arXiv:2112.03907 , 2021.
Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. High-
resolution image synthesis and semantic manipulation with conditional gans. In Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition , 2018.
Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan Atzmon, Basri Ronen, and Yaron Lip-
man. Multiview neural surface reconstruction by disentangling geometry and appearance. Ad-
vances in Neural Information Processing Systems , 33, 2020.
Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelnerf: Neural radiance Ô¨Åelds from
one or few images. arXiv preprint arXiv:2012.02190 , 2020.
Cheng Zhang, Lifan Wu, Changxi Zheng, Ioannis Gkioulekas, Ravi Ramamoorthi, and Shuang
Zhao. A differential theory of radiative transfer. ACM Transactions on Graphics (TOG) , 38(6):
1‚Äì16, 2019.
Kai Zhang, Fujun Luan, Qianqian Wang, Kavita Bala, and Noah Snavely. Physg: Inverse rendering
with spherical gaussians for physics-based material editing and relighting. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 5453‚Äì5462, 2021a.
Xiuming Zhang, Sean Fanello, Yun-Ta Tsai, Tiancheng Sun, Tianfan Xue, Rohit Pandey, Sergio
Orts-Escolano, Philip Davidson, Christoph Rhemann, Paul Debevec, et al. Neural light transport
for relighting and view synthesis. ACM Transactions on Graphics (TOG) , 40(1):1‚Äì17, 2021b.
Xiuming Zhang, Pratul P Srinivasan, Boyang Deng, Paul Debevec, William T Freeman, and
Jonathan T Barron. Nerfactor: Neural factorization of shape and reÔ¨Çectance under an unknown
illumination. arXiv preprint arXiv:2106.01970 , 2021c.
Hao Zhou, Sunil Hadap, Kalyan Sunkavalli, and David W Jacobs. Deep single-image portrait re-
lighting. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pp.
7194‚Äì7202, 2019.
12Published as a conference paper at ICLR 2023
A D ATASETS
Figure 11: OLAT Mapping. (a) Area labels for OLAT mapping; (b) Spherical mapping; (c) OLAT
mapping and renderings.
Figure 12: Training Dataset. (a) Light Stage-scanned real image dataset, (b) Maya-rendered syn-
thetic image dataset
A.1 L IGHT STAGE
Face Capture. We use the Light Stage (Debevec, 2012) to capture data for training purposes. The
Light Stage features controllable lights and cameras, allowing us to capture multiview images using
polarized spherical gradient illumination (Ma et al., 2007; Ghosh et al., 2011). By decomposing
the specular and diffuse surface reÔ¨Çections from the captured images, we can generate image space
diffuse albedo and high-frequency normals. We further Ô¨Åt the reconstructed 3D face to our template
mesh for consistent UV space textures.
OLAT Mapping. To simulate OLAT illumination, we approximate a directional light source by
using an area light source positioned on a sphere. We then convert this area light into a High
Dynamic Range Image (HDRI) map using equirectangular mapping. We illustrate the segmentations
and corresponding OLAT mapping in Fig. 11.
A.2 T RAINING DATASET
Our training dataset consists of 1) a Light Stage-scanned multi-view colored image dataset under
white illumination shown in Fig. 12 (a), 2) a synthetic multi-view colored image dataset rendered
via a professionally-tuned Maya face shader shown in Fig. 12 (b). In the following sections, we
introduce data composition and settings for synthetic data rendering and lightstage scanning.
13Published as a conference paper at ICLR 2023
Synthetic Image Dataset Our input rendering assets to Maya renderer are 101 HDRI environment
maps, 86 OLAT environment maps, and 21 face assets: the HDRI data covers various illuminations
from outdoor open areas to small indoor areas; OLAT environment map cover 86 different regions
in SkyDome as directional light; 21 subjects cover a variety of skin color, age, and gender. Each
face asset consists of a coarse mesh, an albedo map, and a high-frequency normal map. We rendered
40 Ô¨Åxed-view RGBA-space images under all combinations of illumination and face assets. In total,
we acquired 37;160images in 800800-pixel resolution for our synthetic image dataset.
Lightstage-scanned Real Image Dataset Synthetic data is insufÔ¨Åcient to train a model with out-
put rendering close to the real world. We utilized lightstage to additionally capture multi-view
images for 48 subjects under uniform white illumination. Besides the diverse skin colors, gender,
and age of subjects, the lightstage-scanned dataset consists of 27 expressions of each subject. We set
up 16 Ô¨Åxed cameras covering different viewpoints of the frontal view. In total, we acquired 20;736
images in 30084096 -pixel resolution for our real image dataset.
B M ORE RESULTS AND ANALYSIS
Figure 13: Desnity Field Construction. We
present a comparison of hyperparameters ( and
) in density Ô¨Åeld construction.Density Field Construction. We show how
andaffect the constructed density Ô¨Åeld in
Fig. 13. Larger results in a more evenly dis-
tributed density at samples along the light path.
The constructed density Ô¨Åeld hence presents
a coarse boundary around the input geome-
try. Therefore, the rendered results tend to
have a blurry appearance. However, smaller
approaches Dirac delta distribution, where
only samples close to the intersections have
valid density values. The rendered results thus
have black stripes. Larger alleviates the
false density construction and results in accu-
rate density values around the input geometry.
High-frequency Surface Normal. To inves-
tigate the effectiveness of high-frequency sur-
face normal as input, we compare results by re-
placing high-frequency input normal maps with a low-frequency one extracted from coarse geom-
etry (Fig. 14). Given other input assets unchanged, rendered results with low-frequency normal
maps show the incapability of extracting high-frequency information from input images. Therefore,
high-frequency normal maps are crucial in achieving pore-level sharpness even though training data
contains high-frequency details on images.
Figure 14: High-frequency Surface NormalAblation Study on Light Sampling We val-
idate the Importance Light Sampling for sim-
ulating various external light sources with an
extensive ablation study in Fig. 15. Under the
same illumination conditions, the Importance
Light Sampling can generate soft and appropri-
ate diffuse reÔ¨Çection while preserving the accu-
rate lighting distribution from the input HDRI.
It ensures that the rendered images maintain the
high degree of realism and Ô¨Ådelity to the orig-
inal lighting conditions. In contrast, Uniform
Spherical Sampling, while capable of repre-
senting the lighting environment with the same
number of sampled lights, tends to produce hard shadows and may result in less detailed and over-
exposed images.
14Published as a conference paper at ICLR 2023
Figure 15: Ablation study on the impact of different light sampling techniques. We showcase the
layer-by-layer decomposition of the direct lighting model using (a) Importance Light Sampling, and
(b) Uniform Spherical Sampling. We adjust the intensity of each component equally for visualization
purposes.
MethodsInput Output Novel
SubjectsNovel
ViewpointViewsKnown
LightingDirectional
LightHDRI
SIPR Single-view SH X
FRF Single-view OLAT X X X
Neural-PIL Single-view OLAT X X X X
NeLF Multi-view X X
Ours Multi-view* HDRI X X X X
Table 2: Input, output, and functionality of our baselines. Multi-view* indicates that we Ô¨Årst
convert geometry and textures to multiview representation utilizing the ray-mesh intersection. Our
method is capable of rendering novel subjects from novel viewpoints using either directional light
or HDRI as queried illumination, without compromising on quality.
Qualitative Comparison. In Fig. 16, we compare the rendering results of FRF, NeLF, SIPR,
Neural-PIL, and our methods under HDRI and OLAT (one-light-at-a-time) illuminations, as well
as using multi-view images as input instead of geometry. To be more concrete, we identify different
methods‚Äô input, output, and functionality in Table 2. Our method stands out for its ability to pro-
duce clear hard shadows resulting from full occlusion by face geometry, and soft shadows caused
by indirect illumination. In contrast, Neural-PIL and NeLF do not model directional light and were
not trained on OLAT data, so we compare them only under HDRI illumination. SIPR is an image-
based relighting method that models the scene in 2D, and cannot be queried from novel viewpoints.
FRF, Neural-PIL, and NeLF, on the other hand, models in 3D. Neural-PIL inherits a per-scene-per-
train manner as NeRF and, thus is not generalizable on different subjects. In addition, we provide
rendering results from Maya, a top-notch industrial renderer, as a reference for comparison.
More Qualitative Results. Fig. 18 presents a comprehensive collection of qualitative results.
Each row showcases the rendering inputs, including the geometry, albedo map, and normal map,
followed by six rendered images. The Ô¨Årst three show different facial expressions under all-white
illumination, while the last three display neutral expressions under different lighting conditions. By
utilizing our photo-realistic neural renderer, we are able to render images at any resolution without
compromising quality.
Rendering Speed. In addition to the rendering results, we also compare the rendering speed in
Table 3. With the same speciÔ¨Åcation of output and environment, our method is able to achieve up to
47-49 times faster with engineering acceleration (e.g. multi-thread processing)
15Published as a conference paper at ICLR 2023
FRF
HDRI 1
HDRI 2
HDRI 3
HDRI 4
NeLF
 SIPR
 Neural-PIL
 Maya
 Ours
 Ours*
(a) HDRI illumination
OLAT 1
OLAT 2
OLAT 3
OLAT 4
FRF
SIPR
Ours
Maya
 (b) OLAT illumination
Figure 16: Qualitative comparison with baseline methods. (a) Under HDRI illumination, our
rendering results achieve the highest sharpness and less plastic/rubber-like appearance, where Ours
takes geometry and textures as input and Ours* takes multiview inputs; (b) OLAT illumination
simulates strong directional lighting. While our robust direct light modeling enables hard shadow
casting, our indirect light modeling realistically softens shadow at the boundary.
Resolution 800800 14401440 28802880 57605760
Maya 130s 370s 1857s 7848s
Ours 3s 8s 31s 158s
Table 3: Facial rendering speed benchmark on different resolution in seconds
Testing on Other Datasets. To demonstrate that high-quality training data does not determine the
robustness of the method, we evaluated our trained model over other available resources in Fig. 19.
We converted three datasets to match our input as follows,
‚Ä¢Triplegangers. We used FaceX 3DMM to Ô¨Åt and align geometries; albedo map was directly
transferred from source; normal map was inferred using (Wang et al., 2018).
‚Ä¢3D Scan Store. We used 3DMM to Ô¨Åt and align geometries; albedo and normal maps were
provided and not further processed.
‚Ä¢FaceScape. Geometries and albedo maps were provided and not further processed. The
normal map was unavailable and not used.
Our method delivers promising Ô¨Ådelity on all three testing datasets. First, our method is capable of
adapting to different mesh topologies. For example, meshes in the Triplegangers dataset has denser
vertices in the front face than behind while meshes in FaceScape have more uniform density, but
our trained-once model performs equally well in both testing datasets. Second, our method does
not sacriÔ¨Åce Ô¨Ådelity when input normal is unavailable during testing. Thanks to explicit geometry
volume and robust model, pixels in the albedo map are precisely mapped onto the surface and
therefore, yield no noise in rendering.
Finally, we acquired and tested our pre-trained model over the Generated Photos dataset containing
only low-frequency albedo and normal map in Fig. 20. The dataset is generated by some anonymous
method with only a single-view online image as input. Our method not only outputs a clear silhouette
but also shows realistic pre-level detail when high-frequency input is unavailable.
16Published as a conference paper at ICLR 2023
(a) HDRI
(b) OLATFigure 17: We show more qualitative comparisons of our method and Maya under (a) HDRI and (b)
OLAT, with zoom-in on the images.
17Published as a conference paper at ICLR 2023
Figure 18: Qualitative results. In each row, We present our input (col. 1), three novel view ren-
dering with neutral expression (col. 2), two other expressions under white illumination (col. 3, 4),
and three frontal view rendering with neutral expression under different HDRI illumination (col. 5,
6, 7). Geometry, albedo map and normal map in UV space are list in the Ô¨Årst column from top to
bottom.
18Published as a conference paper at ICLR 2023
Figure 19: Testing performance of our trained-once models on other open source datasets. To
demonstrate Ô¨Ådelity and generalization of our method, we further test our pre-trained model on
Triplegangers, FaceScape and 3D Store datasets. Besides accurate shading, our rendering illustrates
sharp detail in pore, wrinkle and hair.
19Published as a conference paper at ICLR 2023
Figure 20: Testing performance of our trained-once models on GeneratedPhotos dataset (in-the-
wild). Our model demonstrates high Ô¨Ådelity when accurate scans of albedo and normal map are
unavailable in this test.
20