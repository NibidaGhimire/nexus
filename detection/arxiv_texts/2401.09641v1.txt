arXiv:2401.09641v1  [cs.LG]  17 Jan 2024Functional Linear Non-Gaussian Acyclic Model for Causal Di scovery
Functional Linear Non-Gaussian Acyclic Model for Causal
Discovery
Tian-Le Yang yangtianle1996@gmail.com
Graduate School of Engineering Science
Osaka University
1-3, Machikaneyama, Toyonaka, Osaka, Japan
Kuang-Yao Lee kuang-yao.lee@temple.edu
Department of Statistics, Operations, and Data Science
Temple University
Philadelphia, PA 19122 United States
Kun Zhang kunz1@cmu.edu
Machine Learning Department
Carnegie Mellon University & Mohamed bin Zayed University o f Artiﬁcial Intelligence
Pittsburgh, PA 15213, USA & Masdar city, Abu Dhabi, United Ar ab Emirates
Joe Suzuki prof.joe.suzuki@gmail.com
Graduate School of Engineering Science
Osaka University
1-3, Machikaneyama, Toyonaka, Osaka, Japan
Abstract
In causal discovery, non-Gaussianity has been used to characte rize the complete conﬁgu-
ration of a Linear Non-Gaussian Acyclic Model (LiNGAM), encompass ing both the causal
ordering of variables and their respective connection strengths. However, LiNGAM can
only deal with the ﬁnite-dimensional case. To expand this concept, we extend the notion of
variables to encompass vectors and even functions, leading to the Functional Linear Non-
Gaussian Acyclic Model (Func-LiNGAM). Our motivation stems from t he desire to identify
causal relationships in brain-eﬀective connectivity tasks involving, for example, fMRI and
EEG datasets. We demonstrate why the original LiNGAM fails to hand le these inher-
ently inﬁnite-dimensional datasets and explain the availability of func tional data analysis
from both empirical and theoretical perspectives. We establish th eoretical guarantees of
the identiﬁability of the causal relationship among non-Gaussian ran dom vectors and even
random functions in inﬁnite-dimensional Hilbert spaces. To address the issue of sparsity
in discrete time points within intrinsic inﬁnite-dimensional functional d ata, we propose
optimizing the coordinates of the vectors using functional principa l component analysis.
Experimental results on synthetic data verify the ability of the pro posed framework to
identify causal relationships among multivariate functions using the observed samples. For
real data, we focus on analyzing the brain connectivity patterns d erived from fMRI data.
Keywords: LiNGAM, Causal Discovery, Functional Data, Darmois-Skitovich Th eorem,
Non-Gaussian, Gaussian Process
1 Introduction
Numerous empirical sciences strive to uncover and comprehe nd causal mechanisms that
underlie a wide range of natural phenomena and human social b ehavior. Causal discovery
1Yang, Lee, Zhang, Suzuki
✒✑✓✏
✒✑✓✏✒✑✓✏
X1X2X3
✡✡✡ ✣❏
❏❏ ❫
✒✑✓✏
✒✑✓✏✒✑✓✏
X1X2X3
✡
✡✡ ✢❏❏❏ ❪
✒✑✓✏
✒✑✓✏✒✑✓✏
X1X2X3
✡
✡✡ ✢❏
❏❏ ❫X2
X1ǫX1→X2
✲✻
✟✟✟✟✟✟ ✯ X1
X2ǫ′X2→X1
✲✻
✟✟✟✟✟✟ ✯
Figure 1: Structure learning methods like the PC algorithm cannot distinguish t hese causal graphs
that have the identical probability distribution P(X1X2)P(X2X3)/P(X2) (Left). But
LiNGAM can diﬀerentiate them via the non-Gaussian assumption (Righ t).
has a wide range of applications, including biology ( Sachs et al. ,2005), climate studies
(Ebert-Uphoﬀ and Deng ,2012), and healthcare ( Lucas et al. ,2004). When determining
the cause-and-eﬀect relationship between variables, such a sX1andX2, detecting their
dependence alone is insuﬃcient for determining the causal d irection, i.e., whether X1→X2
orX2→X1.
CausalanalysisbasedontheLiNGAM, proposedby Shimizu et al. (2006), addressesthis
challenge by identifying the causal directions in linear re lationships. Speciﬁcally, supposing
there is no latent common cause for X1andX2, it ﬁgures out the causal direction between
them bychecking which ofthefollowing two modelsholds: X2=aX1+ǫandX1=a′X2+ǫ′,
whereX1⊥ ⊥ǫandX2⊥ ⊥ǫ′anda,a′∈R.∗The suﬃcient and necessary condition of
the identiﬁability is that LiNGAM requires at most one of the noise terms (including the
root causes) to be non-Gaussian to make it possible to identi fy unique causal directions.
Notably, zero correlation is synonymous with independence in Gaussian variables, making
it impossible to distinguish between the two causal models w henX1andX2are Gaussian.
In this linear, Gaussian case, one can only end up with the so- called Markov equiva-
lence class (all members of the equivalence class have the sa me conditional independence
relations), even when adheringto faithfulness assumption (Spirtes et al. ,2000;Pearl,2000).
For instance, the three Directed Acyclic Graphs (DAGs) conn ecting three variables, such
asX1,X2,X3, in Fig. 1are Markov equivalent because they have the same distributi on in
the Gaussian case. Here faithfulness refers to the property where any independence rela-
tions observed in the data can be explained by the causal rela tionships represented in the
graphical model. However, this is not the case anymore in non -Gaussian cases. Due to
this signiﬁcant advancement, LiNGAM can uniquely determin e the causal ordering among
variables solely based on observational data, even without assuming faithfulness.
For the converse, the Darmois-Skitovich theorem (D-S) is em ployed to prove the iden-
tiﬁability of causal direction. From D-S, if at least one of t he variables X1andX2are
non-Gaussian, then only one unique direction of X1→X2andX2→X1exists. The
Darmois-Skitovich (D-S) theorem originally focused on one -dimensional Gaussian random
variables. Interestingly, Ghurye and Olkin (1962) expanded its application to random vec-
tors, while Myronyuk (2008)generalized ittoBanach spaces. Inourpaper,randomeleme nts
that take values in a Banach space are called random function s.
∗X1⊥ ⊥X2denotes the independence of XandY.
2Functional Linear Non-Gaussian Acyclic Model for Causal Di scovery
This paper establishes a novel functional framework for mod eling the causal structure of
multivariate functional data, which is therealization of r andomfunctions. It is important to
note that functional data is inherently inﬁnite-dimension al. Ifwe apply conventional models
suchasPCorLiNGAM directly, wemight incorrectly identify causal relationships, as shown
in Fig.2. To demonstrate the beneﬁts of functional data analysis ( Ramsay and Silverman ,
2005), we provide an example in Fig. 3, illustrating how smoothing the discrete points
enables us to capture missing information. Functional data analysis has gained prominence
in diverse ﬁelds, including neuroimaging ( Luo et al. ,2019), ﬁnance ( Tsay and Pourahmadi ,
2017), and genetics ( Wei and Li ,2008). Exploring causal relationships among random func-
tions presents a signiﬁcant challenge in multivariate func tional data analysis.
Thisresearch is motivated by brain-eﬀective connectivity ( Friston,2001), which explores
the directional eﬀects between neural systems. Learning bra in-eﬀective connectivity net-
works from electroencephalogram (EEG), functional magnet ic resonance imaging (fMRI),
and electrocorticographic imaging (ECoG) records is cruci al for understanding brain activi-
ties and neuron responses. Modeling these multivariate pro cesses and accurately estimating
eﬀective connections between brain areas pose signiﬁcant ch allenges due to the continuous
nature of the data and the need to treat the data as functions, considering the small time
intervals between adjacent sample points. Previous studie s, such as Qiao et al. (2019), has
explored the functional aspects of the Gaussian graphical m odel by estimating the inverse
covariance matrix. Lee and Li (2022) introduced the functional directional relationships
under Gaussian assumption, enabling the determination of a directed acyclic graph (DAG)
up to its equivalence class. The previous version of this pap erYang and Suzuki (2022)
discussed the identiﬁability without considering one impo rtant point for functional data:
the covariance operator’s non-invertibility. Moreover, t he previous algorithm for functional
data is not accurate because it only tests the independence o f every principal component
rather than the whole random vector. Zhou et al. (2022) developed a novel Bayesian net-
work model for multivariate functional data. Roy et al. (2023) considers the directed cyclic
modelforfunctional data. Incontrast topreviousworks, ou rapproach diﬀersinthatweﬁrst
establish the identiﬁability of random vectors. Subsequen tly, we demonstrate the identiﬁa-
bility of random functions considering the non-invertibil ity and extend it into multivariate
scenarios. Our contributions are as follows :
•We establish a framework for discovering causal orders for r andom vectors and func-
tions, moving beyond the traditional focus on random variab les.
•We theoretically prove that it is possible to identify the ca usal order under non-
Gaussianity for random vectors (Theorem 5).
•We further demonstrate the identiﬁability of the causal ord er for non-Gaussian pro-
cesses in inﬁnite-dimensional Hilbert spaces (Theorem 8).
•To verify the validity of our method, we performed extensive experiments with simu-
lated data as Table 1. Empirical results demonstrate the identiﬁability. The re sults
show that it performs worse as the number of functions increa ses, which is reasonable.
But as the sample size increases, it performs better. We need more data for larger
dimensions, but the required amounts are still reasonable.
3Yang, Lee, Zhang, Suzuki
xxy
y=g(x) =sin(x+1)
y=f(x) =sin(x)g(x) = 1·f(x+1)
=⇒Samplingg(x) = 0·f(x)
xxy
y=f(x) = 0y=g(x)
Figure 2: Illustration of why the original LiNGAM does not work . TheLeft Graph:
originaltwostochasticprocesseswiththeircausalrelati onships; TheRightGraph:
a possible situation where we sample the time series but miss the causal relation-
ship.
The structure of the paper is as follows. Section 2provides the necessary background
information to comprehend this paper. This includes introd ucing the LiNGAM, inﬁnite-
dimensional Hilbert spaces, and random elements (random fu nctions). Section 3and4
present the main theoretical results extending the LiNGAM a nd outlines the corresponding
procedure. Section 5and6present the experimental results. Section 7summarizes the key
points.
2 Background
2.1 Linear Non-Gaussian Acyclic Model (LiNGAM)
This section introduces the concept of the LiNGAM for inferr ing the causal relationships
among random variables.
Suppose two random variables X1,X2∈R, we want to identify the causal directions of
eitherX1→X2orX2→X1. More speciﬁcally, our analysis assumes that X1andX2are
linearly related and have zero means. Such as
X1=e1, X2=aX1+e2, (1)
X2=e′
1, X1=a′X2+e′
2 (2)
witha,a′∈RandE[ǫ] =E[ǫ′] = 0. To be simple, we let
a/\e}atio\slash= 0,ora′/\e}atio\slash= 0, (3)
to avoid X1⊥ ⊥X2. Speciﬁcally, in the context of LiNGAM, under the assumptio n of the
noise terms, denoted as ǫandǫ′, are independent of their respective covariates, X1andX2
in (1) and (2). Therefore, based on the condition of X1⊥ ⊥e2orX2⊥ ⊥e′
2, we determine
the true causal model to be either ( 1) or (2). It may initially appear that distinguishing
between ( 1) and (2) is not possible, in other words, X1andX2could satisfy both equations
for certain values of a,a′,e2, ande′
2, whereX1⊥ ⊥e2andX2⊥ ⊥e′
2. LiNGAM claims that
4Functional Linear Non-Gaussian Acyclic Model for Causal Di scovery
xxy
y=f(x) = 0y=g(x)y=g(x)
=⇒FPCAg(x) = 0·f(x)
xxy
y=f(x) = 0g(x)≈f(x+1)
Figure 3: Illustration of Why Func-LiNGAM Work . (Smoothing: Functional data
analysis)TheLeftGraph: withtheworstsituationwhenwesa mplethetimeseries
andmissthecausal relationship, whereweget gandfhavenocausalrelationship.
The Right Graph: we can complete the discrete points into smo oth curves with
the Functional data analysis, capturing extra information when choosing suitable
bases.
this inconvenience occurs if and only if X1andX2are Gaussian. In other words, we can
identify ( 1) and (2) if and only if at least one of X1andX2are non-Gaussian.
For the suﬃcient part, we show that if variables are both Gaus sian, then causal order
is unidentiﬁable. Suppose X1,X2both are normally distributed, and the model ( 1) with
X1⊥ ⊥e2is true for certain aandǫ. Letσ2
1,σ2
2be the variances of e1ande2. Then, from
E[e1e2] = 0, we have
e′
1=ae1+e2 (4)
e′
2=e1−a′e′
1=e1−a′(ae1+e2) = (1−a′a)e1−a′e2, (5)
andE[e′
1e′
2] = (1−a′a)σ2
1−a′σ2
2,which means that choosing
a′=aσ2
1
a2σ2
1+σ2
2(6)
will make the E[e′
1e′
2] = 0 too. We call WandZjointly Gaussian if the two random vari-
ables can be represented as/bracketleftbiggZ
W/bracketrightbigg
=A/bracketleftbiggU
V/bracketrightbigg
whereA∈R2×2andU,Vare independent
Gaussian.
The well-known property states that independence is equiva lent to zero correlation for
jointly Gaussian variables†. By checking e′
1ande′
2belonging to joint Gaussian distribution,
we can conclude that e′
1is independent of e′
2. Consequently,( 2) holds with X2⊥ ⊥ǫ′for the
corresponding a′,ǫ′.
For the necessary part, assume that X⊥ ⊥ǫfor (1) andY⊥ ⊥ǫ′for (2) both hold
simultaneously for certain a,a′,ǫ,ǫ′, wherea′satisﬁes ( 6). Therefore, this means that a,a′/\e}atio\slash=
0 due to ( 3) and (6). Now note the statement as follows:
†Suppose ZandWbe binary taking ±1 equiprobably and zero-mean Gaussian. Then, ZWandZare
not jointly Gaussian. Even though E[ZW·Z] =E[W]·E[Z2] = 0 but they are not independent.
5Yang, Lee, Zhang, Suzuki
Proposition 1 ( Skitivic (1953);Darmois (1953))Letm≥2 and independent ran-
dom variables ξ1,...,ξm∈R. Let two linear form L1=/summationtextm
i=1αiξiandL2=/summationtextm
i=1βiξi, if
L1⊥ ⊥L2, forα1,...,α m,β1,...,β m∈R. Then the random variable ξisuch that αiβi/\e}atio\slash= 0
belongs to Gaussian for i= 1,...,m.
Following ( 4)(5)(6) and the Proposition 1, then
(e1,e2,a,1,1−aa′,−a′) = (ξ1,ξ2,α1,α2,β1,β2)
= (X,ǫ,a,1,σ2
2
a2σ2
1+σ2
2,−aσ2
1
a2σ2
1+σ2
2)
By combining ( 3),X1,ǫbelong to Gaussian, then X2is also Gaussian-distributed.
Proposition 2 ( Shimizu et al. (2011))Assuming ( 3), we can identify the causal order
using LiNGAM if at least one of two random variables belongs t o non-Gaussian.
We can also identify the causal orders among multiple random variables. Suppose there
are three linearly related random variables X1,X2,X3with zero means. Then, six potential
causal orders exist, for instance, X2→X1→X3, andX3→X2→X1. First, we determine
the top of them. Assuming X1is independent of {X2−aX1,X3−a′X1}fora,a′∈R,
which means X1is the top variable. Furthermore, suppose that X2−aX1is independent
ofX3−a′X1−a′′(X2−aX1) for some a′′∈R, then regarding the X2as the middle and X3
as the bottom. We obtain the causal order X1→X2→X3. Following the steps, we can
identify the causal order for X1,X2,X3. Furthermore, we can estimate the causal order for
an arbitrary number of random variables like
Xi=i−1/summationdisplay
j=1bi,jXj+ei
wherebi,j∈Rand noise eiis non-Gaussian for prandom variables X1,...,X p.
2.2 Hilbert Spaces
A Banach space is a complete normed vector space where comple teness ensures that all
Cauchy sequences converge within the space. It combines lin earity, completeness, and the
norm to provide a framework for studying mathematical struc tures and functions. More
precisely, in our context, we consider the set of functions a s a Hilbert space, denoted by H.
A Hilbert space is a Banach space equipped with an inner produ ct that induces the norm,
ensuring completeness.
We deﬁne a linear operator T21:H1→H2overRas a mapping that satisﬁes the
linearity property: T21(αf+βg) =αT21f+βT21gforf,g∈H1andα,β∈R. Furthermore,
T21is said to be bounded if there exists a positive constant Csuch that /bardblT21f/bardbl2≤C/bardblf/bardbl1
holds for all f∈H1. Here,/bardbl·/bardbl1,/bardbl·/bardbl2denote the norms within H1,H2, respectively.
For any bounded operator T21:H1→H2, there exists its adjoint operator or dual
operator, a unique bounded linear operator T∗
21:H2→H1such that the following equality
holds:/a\}bracketle{tT21f1,f2/a\}bracketri}ht2=/a\}bracketle{tf1,T∗
21f2/a\}bracketri}ht1forf1∈H1andf2∈H2. The operator T∗
21is the adjoint
operator of T21. IfT21=T∗
21, we say that T21is self-adjoint. Moreover, if the dimension of
His ﬁnite, the self-adjoint operator T21is symmetric.
6Functional Linear Non-Gaussian Acyclic Model for Causal Di scovery
2.3 Random functions
Functional data analysis involves considering each indivi dual element of data as a random
function. These functions are deﬁned over a continuous phys ical continuum, which is typ-
ically time but can also be spatial location, wavelength, pr obability, or other dimensions.
Functionally, these data are inﬁnite-dimensional. Random functions can be interpreted as
random elements that take values in a Hilbert space or as stoc hastic processes. The former
approach provides mathematical convenience, while the lat ter is more suitable for practical
applications. These two perspectives align when the random functions are continuous and
satisfy a mean-squared continuity condition.
Formally speaking, if a mapping X: Ω→Ris measurable from a probability space
(Ω,F,µ) to (R,B(R)), then it is a random variable:
B∈ B(R) =⇒ {ω∈Ω|X(ω)∈B} ∈ F,
with the Borel sets B(R). Similarly, if χ: Ω→His measurable from (Ω ,F,µ) to
(H,B(H)), then it is a random function (or random element) in a Hilbe rt space H:
B∈ B(H) =⇒ {ω∈Ω|X(ω)∈B} ∈ F,
with the Borel sets B(H) w.r.t. the norm of H. LetEbe one set, we suppose that every
entryfofHis a function f:E∋x/mapsto→f(x)∈R.
The mean of the random function χis deﬁned using the Bochner integral‡as/integraltext
Ωχdµ,
under the condition that the expectation of /bardblχ/bardblis bounded. Moreover, if the means of
χ1,χ2inHarem, we give the deﬁnition of the covariance operator K:H→Hof
random functions χ1,χ2when H:=H1=H2:
/a\}bracketle{tKg1,g2/a\}bracketri}ht=/a\}bracketle{t/integraldisplay
Ω/a\}bracketle{tχ1−m,g1/a\}bracketri}ht(χ2−m)/a\}bracketri}htdµ,g2/a\}bracketri}ht=/integraldisplay
Ω/a\}bracketle{tχ1−m,g1/a\}bracketri}ht/a\}bracketle{tχ2−m,g2/a\}bracketri}htdµ ,
forg1,g2∈H. By using orthonormal bases {ei}inH, we can compute the covari-
ance values /a\}bracketle{tKei,ej/a\}bracketri}htfor all pairs of indices iandj. Generally, if χ1⊥ ⊥χ2, then we get
/a\}bracketle{tKg1,g2/a\}bracketri}ht= 0 forg1,g2∈H.
In the context where each element in His a mapping from EtoR, a random function
χ: Ω→Htakes values χ(ω,x)∈Rfor each ω∈Ω andx∈E. Furthermore, if we
ﬁxω∈Ω,χ(ω,·) represents a random function from EtoR. Henceforth, we adopt the
notation χ(·) to represent the random function χ(ω,·). This convention is analogous to
the simpliﬁcation employed for random variables, where X(ω) is denoted as X. Note that
a mean mrandom function χis referred as a Gaussian process if for any n≥1, the
random vector [ χ(x1),...,χ(xn)] of length nfollows a Gaussian distribution with mean
[m(x1),...,m(xn)],x1,...,x n∈E.
When the Hilbert space Hhas a ﬁnite dimension d, the covariance operator can be
represented by a covariance matrix, denoted as Σ ∈Rd×d. This matrix is positive deﬁ-
nite. Consequently, we can deﬁne the eigenvalues {λi}and eigenvectors {φi}of Σ. Each
vector in Hcan be expressed as a linear combination of the eigenvectors , speciﬁcally as/summationtextd
i=1/a\}bracketle{tX,φi/a\}bracketri}htφi. Moreover, for /a\}bracketle{tX,φi/a\}bracketri}ht, thevarianceisgiven by λi. Then,forrandomfunction
χ, ifHis an inﬁnite-dimensional function space,
‡See the deﬁnition of the Bochner integral in HSING and EUBANK (2015).
7Yang, Lee, Zhang, Suzuki
Proposition 3 ( HSING and EUBANK (2015))Let{λi}and{φi}denote the eigen-
values and eigenfunctions obtained from the eigenvalue pro blemKφi=λiφi,i= 1,2,....
With probability one, χcan be represented as:
χ=∞/summationdisplay
i=1/a\}bracketle{tχ,φi/a\}bracketri}htHφi,
where/a\}bracketle{tχ,φi/a\}bracketri}htHdenotes the inner product between χandφiinH. Additionally, mean of χ
is zero, and for /a\}bracketle{tχ,φi/a\}bracketri}htH, the variance is equal to λi.
It is important to note the close relationship between stoch astic processes and random
functions. A set of random variables {X(t)}t∈Ecan be considered a stochastic process if the
function X: Ω×E→Rismeasurablewithrespecttotheprobabilityspace(Ω ,F,µ) andthe
measurable space ( R,B(R)) for each t∈E. It is worth mentioning that certain stochastic
processes can also be regarded as random functions ( HSING and EUBANK ,2015).
3 Extension to Functional Data
In this section, we generalize the concept of LiNGAM from ran dom variables to encompass
both random vectors and random functions.
Previous works have extended theD-S to encompass various sc enarios. Theseextensions
include incorporating random vectors ( Ghurye and Olkin ,1962) and random functions in a
Banach space ( Myronyuk ,2008) as substitutes for random variables.
3.1 LiNGAM for Random Vectors
As shown by Shimizu et al. (2011), the identiﬁability of non-Gaussian random variables is
outlined in Proposition 2. However, this proposition does not extend to the case of ran dom
vectors or random functions. This section provides proof of identiﬁability for non-Gaussian
random vectors.
Proposition 4 (Multivariate Darmois-Skitovich ( Ghurye and Olkin ,1962))LetL1=/summationtextm
i=1AiξiandL2=/summationtextm
i=1Biξiwith mutually independent k-dimensional random vectors
ξiand invertible matrices Ai,Bifori= 1,...,m. IfL1andL2are mutually independent,
then allξiare Gaussian.
Now we consider the identiﬁability of the following model wh enx,y∈Rmand invertible
matrixA∈Rm×m,e1⊥ ⊥e2and zero means,
x=ǫ1, y =ǫ′
1,
y=Ax+ǫ2, x=A′y+ǫ′
2,
ǫ′
1=Aǫ1+ǫ2, ǫ′
2= (I−A′A)ǫ1−A′ǫ2.(7)
We assume
AorA′is invertible . (8)
Then, we have the following theorem.
8Functional Linear Non-Gaussian Acyclic Model for Causal Di scovery
Theorem 5 Assuming ( 8), which extends ( 3), we can identify the causal order between
random vectors X1,X2: Ω→Rmof dimension m∈[1,∞) if and only if at least one of
them is non-Gaussian.
ProofSinceǫ1⊥ ⊥ǫ2,Eǫ′
1=Eǫ′
2= 0, and they are Gaussian random vectors with covari-
ance matrix Σ 1,Σ2, respectively. Then the correlated coeﬃcient ρ= 0⇐⇒Cov(ǫ′
1,ǫ′
2) =
AΣ1/parenleftbig
I−ATA′T/parenrightbig
−Σ2A′T= 0⇐⇒ǫ′
1⊥ ⊥ǫ′
2, that is, when A′= Σ1AT(AΣ1AT+ Σ2)−1,
the causal relation between x,yis unable to be identiﬁed. This also satisﬁes the condition
ofǫ′
1⊥ ⊥ǫ′
2is that they follow the Gaussian distribution from the Propo sition4.
3.2 LiNGAM for Random Functions
In this subsection, we present results that demonstrate ide ntiﬁability can be achieved in
non-Gaussian scenarios in inﬁnite-dimensional Hilbert sp aces as Theorem 8. In extending
our approach to multivariate scenarios, we adopted methodo logies from Lemmas 1 and 2 of
DirectLiNGAM (refer to ( Shimizu et al. ,2011)). This involved identifying the exogenous
function (see Appendix) and using residuals for causal orde ring, paralleling the process
in Direct-LiNGAM. Owing to the procedural similarities wit h multivariate functions, we
omitted a detailed proof in our main text, choosing to apply t hese established principles to
our context. We have included the preliminary proof in Secti on3.3for clarity.
LetH1,H2be Hilbert spaces. Assume that there are two causal models fo rf1∈H1
andf2∈H2,
f1=h1, f2=T21f1+h2,
f2=h′
1, f1=T12f2+h′
2.(9)
where random functions {h1,h′
2} ∈H1and{h′
1,h2} ∈H2. We also assume the covariance
operator K11ofh1,K22ofh2have positive eigenvalues ( >0). The T12:H2→H1,
T21:H1→H2are linear bounded operators between H1,H2, and we identify the order
by examining whether h2⊥ ⊥f1orh1⊥ ⊥f2.
A bounded linear operator T:H1→H2is considered continuous if the set {T(f)|f∈
U} ⊆H2is open for any subset U⊆H1. Similarly, the inverse image Uis also open.
Furthermore, an operator T:H1→H2is said to be invertible if it is both one-to-one
(injective) and onto (surjective).
Let’s conﬁrm the statements before proceeding with our disc ussion:
•Proposition 6: There is an equivalence between independence and non-corr elation for
jointly Gaussian random functions. In other words, if χ1andχ2are jointly Gaussian
random functions, they are independent if and only if they ar e uncorrelated.
•Proposition 7: The Darmois-Skitovich (D-S) theorem can be extended to ran dom
functions in Banach spaces.
The following Proposition 6establishes the equivalence between independence and non-
correlation for random functions in Banach spaces, which al so includes Hilbert spaces as a
special case.
9Yang, Lee, Zhang, Suzuki
Proposition 6 ( van Neerven (2020))Supposeχ,χ′are joint Gaussian in Banach spaces.
Then,χ⊥ ⊥χ′if and only if they are uncorrelated.
Proposition 7 (Darmois-Skitovich in Banach Space( Myronyuk ,2008))Suppose that
n≥2, and random functions ξ1,...,ξnare in a Banach space. Let L1=/summationtextm
i=1Aiξi,L2=/summationtextm
i=1Biξiwith some continuous linear bounded operators A1,...,A m, andB1,...,B m. If
L1⊥ ⊥L2, thenξiis a Gaussian process for i= 1,...,mwith invertible Ai,Bi.
Theorem 8 (Causal Identiﬁability) If either T12orT21is invertible, the causal order
between random functions in inﬁnite-dimensional Hilbert s paces can be identiﬁed if and only
if at least one of them is a non-Gaussian process.
ProofFor the suﬃciency, from ( 9), we ﬁrst assume the f1=h1,f2=T21f1+h2, and
represent the noise functions h′
1,h′
2withh1,h2:
h′
1=f2=T21h1+h2
h′
2=f1−T12f2=h1−T12(T21h1+h2) = (I−T12T21)h1−T12h2.(10)
Because h′
1,h′
2are formed as the linear combinations of two independent Gau ssian random
functions h1,h2, we can conclude that h′
1andh′
2are jointly Gaussian ( van Neerven ,2020).
Then from Proposition 6, the zero-correlation implies independence. Since h1⊥ ⊥h2and
h1∈H1,h2∈H2, the cross-covariance operator K12is zero:
/a\}bracketle{tK12g1,g2/a\}bracketri}htH2=/integraldisplay
Ω/a\}bracketle{th1,g1/a\}bracketri}htH1/a\}bracketle{th2,g2/a\}bracketri}htH2= 0
for anyg1∈H1,g2∈H2. Then, the cross-covariance operator K′
12between h′
1andh′
2is
/a\}bracketle{tK′
12g1,g2/a\}bracketri}htH2=/integraldisplay
Ω/a\}bracketle{t(I−T12T21)h1−T12h2,g1/a\}bracketri}htH1/a\}bracketle{tT21h1+h2,g2/a\}bracketri}htH2dµ
=/integraldisplay
Ω/a\}bracketle{t(I−T12T21)h1,g1/a\}bracketri}htH1/a\}bracketle{tT21h1,g2/a\}bracketri}htH2dµ+/integraldisplay
Ω/a\}bracketle{t−T12h2,g1/a\}bracketri}htH1/a\}bracketle{th2,g2/a\}bracketri}htH2dµ
=/integraldisplay
Ω/a\}bracketle{th1,(I−T12T21)∗g1/a\}bracketri}htH1/a\}bracketle{th1,T∗
21g2/a\}bracketri}htH1dµ−/integraldisplay
Ω/a\}bracketle{th2,T∗
12g1/a\}bracketri}htH2/a\}bracketle{th2,g2/a\}bracketri}htH2dµ
=/a\}bracketle{tK11(I−T12T21)∗g1,T∗
21g2/a\}bracketri}htH1−/a\}bracketle{tK22T∗
12g1,g2/a\}bracketri}htH2
=/a\}bracketle{tT21K11(I−T∗
21T∗
12)g1,g2/a\}bracketri}htH2−/a\}bracketle{tK22T∗
12g1,g2/a\}bracketri}htH2(11)
foranyg1∈H1,g2∈H2, where K11,K22arethecovarianceoperatorsof h1,h2, respectively.
We assume that K11,K22are not zero. If K′
12= 0, then we require
K11T∗
21=T12{T21K11T∗
21+K22}. (12)
We have
(11) = 0⇔T21K11(I−T∗
21T∗
12) =K22T∗
12
⇔T21K11= (T21K11T∗
21+K22)T∗
12⇔(12)
However, the covariance operator K11andK22are not invertible because of they are com-
pact operator:
10Functional Linear Non-Gaussian Acyclic Model for Causal Di scovery
•A covariance operator istrace-class operator(Theorem7.2 .5 inHSING and EUBANK
(2015));
•Atrace-classoperatorisHibert-Schmidtoperator(Theore m4.5.2in HSING and EUBANK
(2015));
•An Hilbert–Schmidt operator is compact (Theorem 4.4.3 in HSING and EUBANK
(2015));
•Acompactoperatorisnotinvertible(Theorem4.1.4in HSING and EUBANK (2015)).
Then we know covariance operators are not invertible. But he re, we need to notice that we
can always deﬁne a Moore-Penrose inverse to make the equatio n (12) hold if
Im(K11T∗
21)⊆Im({T21K11T∗
21+K22}) (13)
and the following is bounded ( Li,2018):
{T21K11T∗
21+K22}†K11T∗
21. (14)
Then the problem becomes determining the Images and boundne ss.
Next we prove Im( A)⊆Im(A+B). Note that if Ais positive semideﬁnite and /a\}bracketle{tAu,u/a\}bracketri}ht=
0, thenAu= 0. To see why, let v1,...,vnbe an orthonormal basis of eigenvectors of A(so
A vi=λivi) and write u=/summationtextn
i=1/a\}bracketle{tu,vi/a\}bracketri}htvi. Then
/a\}bracketle{tAu,u/a\}bracketri}ht=n/summationdisplay
i=1/a\}bracketle{tu,vi/a\}bracketri}ht2λi= 0
together with λi≥0 implies that /a\}bracketle{tu,vi/a\}bracketri}ht= 0 ifλi>0 sou∈ker(A). To prove that
Im(A)⊆Im(A+B), it is enough to prove that
ker(A+B) = Im(A+B)⊥⊆Im(A)⊥= ker(A) (15)
letu∈ker(A+B). Then 0 = /a\}bracketle{t(A+B)u,u/a\}bracketri}ht=/a\}bracketle{tAu,u/a\}bracketri}ht+/a\}bracketle{tBu,u/a\}bracketri}htwhich implies that
/a\}bracketle{tAu,u/a\}bracketri}ht= 0, sou∈ker(A). Then ( 13) satisfys. Now we consider the boundness. As we
know, the eigenvalue of A+B(positive semideﬁnite) is bigger than AorB, which means the
inverse eigenvalue of A+Bwill be smaller than the inverse eigenvalue of AorB. Moreover,
the smallest eigenvalue of the covariance operator tends to 0, then ( A+B)†Ais bounded.
Then we say the equation ( 12) holds. We can check more details in the Appendix.
Conversely, we ﬁrst let h1⊥ ⊥h2andh′
1⊥ ⊥h′
2in (9) hold true simultaneously for some
T12,T21, and we want to prove that h1,h2,h′
1,h′
2belong to Gaussian under ( 12). Note that
a Hilbert space is a special case of Banach space. Then we use t he Proposition 7. We
assume that T12is invertible without losing generality. Next we show that t he eigenvalue
ofT12T21is less than 1, which means that I−T12T21is invertible (see Theorem 3.5.5 in
HSING and EUBANK (2015)). To achieve this, we multiply ( 12) byT21from the left-hand
side, then we obtain
T21K11T∗
21=T21T12{T21K11T∗
21+K22},
11Yang, Lee, Zhang, Suzuki
which means that the eigenvalue of T21T12is less than 1. Noting that T21T12andT12T21
share the eigenvalues:
T21T12u=λu=⇒T12T21T12u=λT12u=⇒T12T21v=λv
forλ/\e}atio\slash= 0,u∈H2, andv:=T12u∈H1, we have proved that the eigenvalue of T12T21is
less than 1. Then, as we did in ( 2.1), we correspond
(h1.h2,T21,I,I−T21T12,−T12) = (ξ1,ξ2,A1,A2,B1,B2)),
whereA1,A2,B1,B2are invertible.
3.3 Causal Inference in Multivariate Scenarios
In the context of multivariate cases, we introduce two lemma s following Shimizu et al.
(2011):
1. Lemma 9identiﬁes the exogenous function.
2. Lemma 10establishes the causal order among residuals.
By analyzing residuals, we can determine the causal order of random functions. This is
achieved after identifying an exogenous function, which, u nder the assumption of no latent
confounders, corresponds to an independent external inﬂue nce. The independence of these
residuals is assessed through a series of pairwise regressi ons.
Lemma 9 For multivariate case, a random function fjis exogenous if and only if fjis
independent of its residuals h(j)
i=fi−Tijfjfor alli/\e}atio\slash=j.
ProofForthesuﬃciency, if fj⊥ ⊥h(j)
i, assume fjisnotexogenous, then fj=/summationtext
k∈PjTjkfk+
hj=/summationtext
k∈PjTjk/summationtext
l/\egatio\slash=jTklhl+hj, wherePjmeans parents of fj. Thenh(j)
i= (I−TijTji)fi−
Tij/summationtext
k∈Pj,k/\egatio\slash=iTjkfk−Tijhj= (I−TijTji)/summationtext
q/\egatio\slash=jTiqhq−Tij/summationtext
k∈Pj,k/\egatio\slash=iTjk/summationtext
l/\egatio\slash=jTklhl−Tijhj.
The two formulas are composed of linear combinations of exte rnal inﬂuences other than hj,
from Prop. 7, all the functions are non-Gaussian, then h(j)
i/\e}atio\slash⊥ ⊥fj, then it contradicts. There-
fore,fjshould be exogenous; For the necessity, if fjis exogenous, fj=hj,fi=Tijfj+hi
withhi⊥ ⊥fj,hi=/summationtext
k/\egatio\slash=jTikhk, we know the residual error h(j)
i=hi. Then, we know
fj⊥ ⊥h(j)
ifrom the independence of noise functions. So far, the lemma h as been proven.
Lemma 10 Letkr(j)(i)is the causal order of r(j)
i,k(i)denotes a causal order of fi. Then,
the same ordering of the residuals ri=h(1)
i=fi−Ti1f1,i= 1...,p−1is a causal ordering
for the original observed functions as well: kr(j)(l)< kr(j)(m)⇐⇒k(l)< k(m).
ProofWhenwedeterminetheexogenousfunction f1, weneedtoestimatethe p−1residuals
off1:ri=h(1)
i=fi−Ti1f1=/summationtext
j/\egatio\slash=1Tijfj+Ti1f1−Ti1f1=/summationtext
j/\egatio\slash=1Tijfj,i= 1,...,p−1, which
isri=/summationtext
j/\egatio\slash=1Tij/summationtext
k/\egatio\slash=jTjkhk. For the residual of r2=f2−T21f1=h2(second function) is
12Functional Linear Non-Gaussian Acyclic Model for Causal Di scovery
r(j)
i=ri−T′
ijrj=/summationtext
j/\egatio\slash=1Tij/summationtext
k/\egatio\slash=j,2Tjkhk+T′
i2h2−T′
i2h2=/summationtext
j/\egatio\slash=1Tij/summationtext
k/\egatio\slash=j,2Tjkhkfor all
i/\e}atio\slash=j.From the independence assumption of noise functions, we kno wr2⊥ ⊥r(2)
i. Then we
knowthecausalrelationshipsofresiduals ri,i= 1,...,p−1arethesameas fi,i= 1,...,p−1
with the T′
ijbecause what we need to do is to test the independence between riand its
residualr(j)
i.
Extending the notion, we can determine the order among any nu mber of random functions
such asfi=/summationtexti−1
j=1Ti,jfj+hiwith non-Gaussian hiand boundedlinear operators Ti,j;Hj→
Hiforprandom functions f1∈H1,...,fp∈Hp.
4 The Procedure
Consider one model from ( 9):
f2=T21f1+h2. (16)
Then let’s notice the statement as follows:
Proposition 11 ( HSING and EUBANK (2015))LetT:H1→H2be a compact§
bounded linear operator, {λj}be the eigenvalues, and {e1,j}and{e2,j}be the sequences
with orthonormal eigenvectors of T∗TandTT∗, respectively. Then
Tf=∞/summationdisplay
i=1λi/a\}bracketle{tf,e1,i/a\}bracketri}htH1e2,i
withf∈H1.
FollowingthenotationinProposition 11, wewritethethreeterms T21f1=/summationtext∞
i=1λif1,ie1,i,
f2=/summationtext∞
i=1f2,ie2,i,h2=/summationtext∞
i=1h2,ie2,i. Then, ( 16) becomes:
Theorem 12 Suppose that T21:H1→H2is compact. If we regard the bases of H1and
H2as{e1,i}and{e2,i}, respectively, then
f2,i=λif1,i+h2,i (17)
fori= 1,2,..., whereλ1≥λ2≥ ···.
To ensure convergence of the eigenvalue sequence {λi}, we suppose that the operator
T21is compact. Without compactness, the {λi}would not be convergent. Practically,
we approximate the inﬁnite-dimensional random functions f1∈H1,f2,h2∈H2by ﬁnite
lengthMrandom vectors. We select the bases {e1,i}M
i=1and{e2,i}M
i=1to minimize the
approximation error.
FPCA oﬀers a more eﬀective ﬁt than PCA for raw data dimensionali ty reduction, par-
ticularly with time-series data like fMRI and EEG, where dim ensions vary with sampling
frequency (e.g., 100Hz vs. 1Hz). As frequency increases, di mensions approach inﬁnity.
FPCA overcomes this by approximating inﬁnite dimensions th rough orthogonal bases, pre-
serving maximal original data information and capturing la tent details beyond traditional
sampling. Figures 2and3demonstrate FPCA’s necessity for functional data.
§We deﬁne a bounded linear operator T:H1→H2to be compact if, for any bounded inﬁnite sequence
{fn}inH1, the sequence {Tfn}has a convergent subsequence in H2.
13Yang, Lee, Zhang, Suzuki
ft−2
1ft−1
1ft
1 ft+1
1
ft−2
2ft−1
2ft
2 ft+1
2
ft−2
3ft−1
3
Full-timeft
3 ft+1
3ft−1
1ft
1
ft−1
2ft
2
ft−1
3
Windowft
3f1
f2
f3
Summary
Figure 4: Illustration of Diﬀerent Kinds of Multivariate Time Series Causal
Graphs. Left: Full-time; Middle: Window; Right: Summary (this pap er).
The other merit of using the FPCA (functional principal comp onent analysis) approach
isitseﬃciency. Weassumethefollowingprocedure: ﬁrst,we approximatethe Wtimepoints
sampled from functions by the Lcoeﬃcients of the basis functions (B-spline). Then, we
transform it by the Mcoeﬃcients of the basis functions deﬁned above. The time com plexity
is as follows. M < L≪Wand the time complexity C(M) of the proposed procedure is
much less than C(W). For example, Shimizu et al. (2011) evaluated the complexity of their
method as C(W) =O(n(Wp)3q2+Wp)4q3), where q(≪n) is the maximal rank found by
the low-rank decomposition used in the kernel-based indepe ndence measure, although the
proposed procedure requires additional O(nL2+L3) complexity for the covariance matrix
O(nL2) and eigenvalue decomposition O(L3).
This paper primarily examines the summary causal relations hips among random func-
tions, focusing less on speciﬁc time points or partial windo ws in temporal data. There
are three graphical representations of causal structures i n temporal data, namely, the full-
time causal graph , thewindow causal graph , and the summary causal graph (Gong et al. ,
2023). Thefull-time causal graph , illustrated on the left in Fig. 4, depicts a complete dy-
namic system, representing all vertices including compone ntsf1,...,fpat each time point
t, connected through lag-speciﬁc directed links such as ft−k
i→ft
j. However, due to the
challenges of capturing a single observation for each serie s at every time point, constructing
a full-time causal graph can be complex. To address this, the window causal graph concept
is introduced, which operates under the assumption of a time -homogeneous causal struc-
ture. This graph, shown in the middle of Fig. 4, works within a time window corresponding
to the maximum lag in the full-time graph. On the other hand, t hesummary causal graph ,
displayed on the right in Fig. 4, abstracts each time series component into a single node,
illustrating inter-series causal relationships without s pecifying particular time lags. The
complexity of this summary graph depends on the choice of mul tivariate dependence mea-
sure, such as mutual information or HSIC. The algorithmic co mplexity for generating this
graph is similar to that of DirectLiNGAM. Fig. 4visually compares these diﬀerent types of
causal graphs for multivariate time series.
14Functional Linear Non-Gaussian Acyclic Model for Causal Di scovery
4.1 Algorithm
To show how to implement this method, we provide algorithm ps eudocode and empirical
experiments to demonstrate the eﬃciency. The algorithm pre sented in this study shares
similarities with the greedy search method of DirectLiNGAM . However, it diverges in two
key aspects: ﬁrst, we leverage Functional Principal Compon ent Analysis (FPCA) for data
preprocessing, and second, our independencetest consider s multivariate relationships rather
than univariate ones. This makes Func-LiNGAM straightforw ard to implement. For the
purpose of this paper, we focus on providing a basic implemen tation without delving into
enhancing search methods or other optimizations, as they ar e not the primary focus of our
research. The whole algorithm is as Algorithm 1.
Note that the Wmeans the sampled time points from one random function. As th e
intrinsically inﬁnite-dimensional property of functiona l data, we need to approximate W
with eﬃcient ﬁniterepresentation (FPCA with principal com ponent number M(M≪W)).
The number Mcan be decided by the explained variance ratio (95% or 99%). T o be simple,
here we let all the Mof random functions be the same.
Algorithm 1 Func-LiNGAM (Can be regarded as vector-based DirectLiNGAM but with
FPCA preprocessing.)
1:Input: Each function has Wtime points, then construct Wp-dimensional random
vectorf(W: Full-time points) for pfunctions, a set of its variable subscripts Uand a
Wp×ndata matrix as F, initialize an ordered list of functions K=∅andm:= 1;
2:Output: Adjacent Matrix ˆT∈Rp×p
3:Use FPCA for ﬁnite approximating each random vector to make t heir dimensions from
WptoMp, whereMis the number of principal components.
4:repeat
5:(a) Perform least squares regressions of the approximating random vector ˆfi∈RM
onˆfj∈RMfor alli∈U\K(i/\e}atio\slash=j) and compute the residual vectors r(j)and the
residual data matrix R(j)from the data matrix Ffor allj∈U\K. Find a variable ˆfm
that is most independent of its residuals:
ˆfm= arg min
j∈U\KMI/parenleftBig
ˆfj;U\K/parenrightBig
,
whereMIis the independence measure such as mutual information or ot her measures.
6:(b) Append mto the end of K.
7:(c) Letˆf:=r(m),ˆF:=R(m).
8:untilp−1 subscripts are appended to K
9:Append the remaining variable to the end of K.
10:Construct a strictly lower triangular matrix ˆTby following the order in K, and estimate
the connection strengths ˆTijby using least squares regression in this paper.
15Yang, Lee, Zhang, Suzuki
5 Experiment
To validate our method, we conducted comprehensive experim ents using simulated data, as
shown in Table 1. We observed an improvement in performance as the sample siz e increased
across multiple functions. Notably, precision decreased m onotonically and Structural Ham-
ming Distance (SHD) increased monotonically as the number o f functions ( p) grew. Our
data generation process, following the settings in Qiao et al. (2019), involved n×prandom
functions, deﬁned as:
Xij(t) =φ(t)Tδij (18)
whereirepresents the ithsample (i= 1,...,n), andjdenotes the jthrandom vector. The
vectorδij∈R5can be an arbitrary non-Gaussian random vector. Here we gene rated these
by ﬁrst creating random vectors qij∼ N(0,I5), then we square each element of the vector
to getδij. The ﬁve-dimensional Fourier basis φ(t) was also used. We modeled the causal
relationships in δias follows:
δi0=ǫ0, δi1=B1,0δi0+ǫ1, ..., δ ip=Bp,p−1δi(p−1)+ǫp (19)
whereul∼ N(0,I5), then we square each element of the vector to get ǫl. To be sim-
ple, we set Bl,l−1=I5,l= 1,...,p. The sample size is n={100,200,300,700},p=
{5,10,20,30,50,70}, and the observed values, gij(tk), follow
gij(tk) =Xij(tk)+eijk,
whereeijkis derived from the square of the random variable qijk, whereqijk∼ N(0,0.25).
Speciﬁcally, eijk=q2
ijk. Due to the squaring of a normally distributed variable with a
variance of 0 .25, the resulting distribution of eijkcan be described as a Gamma distribution
with a shape parameter of1
2and a scale parameter of 0 .5, applicable for i= 1,...,nand
j= 1,...,p. Every random function is sampled at W= 1000 equidistant time points,
0 =t1,...,t1000= 1.
We employ B-spline bases as a ﬁtting technique for each rando m function instead of the
Fourier basis to represent the actual data accurately. B-sp line bases oﬀer more ﬂexibility
and can capture the complex shapes and patterns present in th e data. After ﬁtting the ran-
dom functions with B-spline bases, we calculate each random function’s estimated principal
component scores. These scores are derived from the basis co eﬃcients, with the number of
calculated principal component scores limited to the ﬁrst Mcomponents ( M≤W). The
choice of Mallows us to control the dimensionality of the data represen tation, providing
a balance between capturing the most important variability in the data and minimizing
computational complexity. By calculating these estimated principal component scores, we
obtain a concise representation of the data that encapsulat es its essential characteristics
while reducing its dimensionality. This approach allows fo r eﬃcient analysis and interpre-
tation of the random functions within the context of our meth odology. We set M= 5
(99% explained variance ratio) for the B-spline. Cross-val idation can also obtain the opti-
malM. However, we set the parameters to ensure they maintain as mu ch information as
possible. We evaluate the Func-LiNGAM with Precision, Reca ll ratio, F1-score, and SHD
(Structural Hamming Distance in Tsamardinos et al. (2006)) in 50 trials as Table 1. The
smaller the SHD, the better the performance. To clarify, our objective is to demonstrate an
implementation example rather than to propose a superior al gorithm through comparison.
16Functional Linear Non-Gaussian Acyclic Model for Causal Di scovery
Data size MetricsVarious number of functions (mean ±standard deviation)
p= 5 p= 10 p= 20 p= 30 p= 50 p= 70
n= 100Precision 0 .76±0.14 0.64±0.10 0.57±0.09 0.40±0.06 0.30±0.04 0.25±0.03
Recall 0 .99±0.04 0.95±0.0 0.90±0.07 0.75±0.07 0.65±0.04 0.59±0.04
F1 0 .85±0.10 0.76±0.08 0.70±0.09 0.52±0.07 0.41±0.05 0.35±0.03
SHD 1 .40±0.95 5.03±1.91 13.47±4.17 33.47±6.56 74.73±9.86 119.47±10.70
n= 200Precision 0 .83±0.14 0.76±0.29 0.72±0.07 0.70±0.06 0.54±0.05 0.46±0.07
Recall 1 .00±0.00 0.80±0.24 0.99±0.01 0.97±0.03 0.88±0.03 0.81±0.07
F1 0 .90±0.08 0.78±0.27 0.83±0.05 0.81±0.05 0.67±0.05 0.59±0.07
SHD 0 .97±0.91 3.63±4.58 7.70±2.35 12.53±3.36 37.03±6.60 66.20±12.79
n= 300Precision 0 .85±0.13 0.79±0.28 0.75±0.07 0.74±0.05 0.70±0.05 0.60±0.04
Recall 1 .00±0.00 0.84±0.23 1.00±0.00 0.99±0.01 0.99±0.01 0.93±0.03
F1 0 .92±0.08 0.81±0.26 0.86±0.05 0.85±0.03 0.82±0.03 0.73±0.04
SHD 0 .80±0.75 3.17±4.43 6.57±2.50 10.27±2.41 21.27±4.36 42.90±6.25
n= 700Precision 0 .92±0.10 0.81±0.08 0.80±0.07 0.78±0.05 0.74±0.03 0.70±0.02
Recall 1 .00±0.00 1.00±0.00 1.00±0.00 1.00±0.00 1.00±0.00 0.95±0.05
F1 0 .96±0.06 0.88±0.05 0.88±0.04 0.87±0.03 0.85±0.02 0.83±0.05
SHD 0 .40±0.55 2.50±1.20 4.96±2.06 8.80±2.34 17.40±2.97 32.70±4.37
Table 1: Evaluation of Func-LiNGAM with various number pof functions. The causal
graph is as f1→f2→ ··· → fp(50 trials).
6 Actual Data
This section demonstrates the application of the proposed a pproach to analyzing brain
connectomes for functional magnetic resonance imaging (fM RI) data. The fMRI data
(Richardson et al. ,2018) is preprocessed by downsampling it to a resolution of 4mm, w ith
a repetition time (TR) of 2 seconds. This data consists of 155 subjects ( n= 155), 168 time
points (W= 168), and 17 parcels ( p= 17). During the study, 155 participants took part in
the fMRI scans. Among them, 122 participants were children, 33 were adults. The partici-
pants were instructed to watch a short animated movie that ai med to evoke various mental
states and physical sensations about the characters depict ed in the movie. Our objective is
to investigate the causal relationships between various br ain regions when individuals watch
the short ﬁlm, regardless of age. To check the Gaussianity of the observed functions, we
performed the Shapiro–Wilk normality test ( Shapiro and Wilk ,1965) onp= 17 parcels at
eachW= 168 time point. The null hypothesis (i.e., the observation s are marginally Gaus-
sian) was rejected for many combinations of scalp position a nd time point, and therefore,
the non-Gaussianity of the proposed model is deemed appropr iate. Next, we estimate the
adjacency matrix between the parcels with the number of prin cipal components M= 5.
The adjacency matrix reveals the presence of connections be tween speciﬁc parcel pairs.
To visualize the brain connectivity and causal relationshi ps, we present a 2D graph using
theNilearn Python package and a 3D graph using the BrainNet Viewer ( Xia et al. ,2013)
(Fig.5).
7 Conclusion
We have introduced a novel framework called Func-LiNGAM, wh ich aims to identify causal
relationships amongrandomfunctions. For thetheoretical foundationof Func-LiNGAM, we
17Yang, Lee, Zhang, Suzuki
Figure 5: Brain Connectivity Graphs (Left: 2D , Right: 3D).
have proven the identiﬁability of both non-Gaussian random vectors (Theorem 5) and non-
Gaussian processes (Theorem 8). Additionally, we have proposed a method to approximate
random functions using random vectors based on Functional P rincipal Component Anal-
ysis (FPCA). Empirically, we demonstrate that the proposed procedure of Func-LiNGAM
achieves accurate and eﬃcient identiﬁcation of causal orde rs among non-Gaussian random
functions. Furthermore, we have preliminarily applied Fun c-LiNGAM to analyze brain
connectivity using fMRI data. Our framework combines theor etical advancements with
practical applications, showcasing its eﬀectiveness in ide ntifying causal relationships among
random functions and its potential for various domains, suc h as brain connectivity.
References
G. Darmois. Analyse generale des liaisons stochastiques. Rev. Inst. Intern. Stat , 21:2–8,
1953.
I. Ebert-Uphoﬀ and Y. Deng. Causal Discovery for Climate Res earch Using Graphical
Models. Journal of Climate , 25(17):5648–5665, 2012.
K. J. Friston. Functional and eﬀective connectivity: A revie w.Brain Connectivity , 1(1):
13–36, 2001.
S. Ghurye and I. Olkin. A characterization of the multivaria te normal distribution. Ann.
Math. Statist. , 33(2):533–541, June 1962.
C. Gong, D. Yao, C. Zhang, W. Li, J. Bi, L. Du, and J. Wang. Causa l discovery from
temporal data. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining , KDD ’23, page 5803–5804, New York, NY, USA, 2023.
Association for Computing Machinery. ISBN 9798400701030. doi: 10.1145/3580305.
3599552. URL https://doi.org/10.1145/3580305.3599552 .
T. HSING and R. EUBANK. Theoretical Foundations of Functional Data Analysis, with
an Introduction to Linear Operators. Wiley, Chichester, 2015.
K. Y. Lee and L. Li. Functional structural equation model. Journal of the Royal Statistical
Society: Series B , 84(2), 2022.
18Functional Linear Non-Gaussian Acyclic Model for Causal Di scovery
B. Li. Linear operator-based statistical analysis: A usefu l paradigm for big data. Canadian
Journal of Statistics , 46(1):79–103, 2018. doi: https://doi.org/10.1002/cjs. 11329. URL
https://onlinelibrary.wiley.com/doi/abs/10.1002/cjs .11329.
P. J. Lucas, L. C. Gaag, and A. Abu-Hanna. Bayesian networks i n biomedicine and health-
care.Artiﬁcial Intelligence in Medicine , 30:201–214, 04 2004. doi: 10.1016/j.artmed.
2003.11.001.
S. Luo, R. Song, M. Styner, J. Gilmore, and H. Zhu. FSEM: Funct ional structural equation
models for twin functional data. Journal of the American Statistical Association , 114
(525):344–357, 2019.
M. V. Myronyuk. On the Skitovich-Darmois theorem and Heyde t heorem in a Banach
space.Ukr Math J , 60:1437–1447, 2008.
J. Pearl. Causality: Models, Reasoning, and Inference . Cambridge, U.K.: Cambridge
University Press, 2000.
X. Qiao, S. Guo, and G. M. James. Functional Graphical Models .J. Amer. Statist. Assoc. ,
114(525):211–222, 2019.
J. O. Ramsay and B. W. Silverman. Functional Data Analysis . Springer, 2005. ISBN
9780387400808. URL http://www.worldcat.org/isbn/9780387400808 .
H. Richardson, G. Lisandrelli, A. Riobueno-Naylor, and R. S axe. Development of the social
brain from age three to twelve years. Nature Communications , 9(1):1027, 2018.
S. Roy, R. K. W. Wong, and Y. Ni. Directed cyclic graph for caus al discovery from multi-
variate functional data, 2023.
K. Sachs, O. Perez, D. Pe’er, D. A. Lauﬀenburger, and G. P. Nola n. Causal
protein-signaling networks derived from multiparameter s ingle-cell data.
Science, 308(5721):523–529, 2005. doi: 10.1126/science.1105809 . URL
https://www.science.org/doi/abs/10.1126/science.110 5809.
S. S. Shapiro and M. B. Wilk. An analysis of variance test for n ormality (com-
plete samples). Biometrika , 52(3/4):591–611, 1965. ISSN 00063444. URL
http://www.jstor.org/stable/2333709 .
S. Shimizu, P. O. Hoyer, A. Hyv¨ arinen, and A. Kerminen. A Lin ear Non-Gaussian Acyclic
Model for Causal Discovery. Journal of Machine Learning Research , 7(72):2003–2030,
2006.
S. Shimizu, T. Inazumi, Y. Sogawa, A. Hyv¨ arinen, Y. Kawahar a, T. Washio, P. O. Hoyer,
and K. Bollen. DirectLiNGAM: A Direct Method for Learning a L inear Non-Gaussian
Structural Equation Model. Journal of Machine Learning Research , 12(33):1225–1248,
2011.
V. P. Skitivic. On a property of the normal distribution. Dokl. Akad. Nauk SSSR (N.S.) ,
(89):217–219, 1953.
19Yang, Lee, Zhang, Suzuki
P. Spirtes, C. Glymour, and R. Scheines. Causation, Prediction, and Search . The MIT
Press, 2000.
I. Tsamardinos, L. Brown, and C. Aliferis. The max-min hill- climbing bayesian network
structure learning algorithm. Mach Learn , 65:31–78, 2006.
R. Tsay and M. Pourahmadi. Modelling structured correlatio n matrices. Biometrika , 104
(1):237–242, 2017.
J. van Neerven. Stochastic Evolution Equations, 3 2020. URL
https://ocw.tudelft.nl/courses/stochastic-evolution -equations/subjects/lecture-4-gaussian- 
Z. Wei and H. Li. A hidden spatial-temporal markov random ﬁel d model for network-based
analysis of time course gene expression data. The Annals of Applied Statistics , 2(1):
408–429, 2008.
M. Xia, J. Wang, and Y. He. BrainNet Viewer: A network visuali zation tool for human
brain connectomics. PLoS ONE , 8(7):e68910, 2013.
T. Yang and J. Suzuki. The functional lingam. In A. Salmer¨ on and R. Rum¨ ı, editors,
The 11th International Conference on Probabilistic Graphical Models, volume 186, pages
25–36. PMLR, 05–07 Oct 2022.
F. Zhou, K. He, K. Wang, Y. Xu, and Y. Ni. Functional bayesian n etworks for discovering
causality from multivariate functional data, 2022.
20