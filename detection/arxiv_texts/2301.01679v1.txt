COVID-N ETUSP RO: ANOPEN-SOURCE EXPLAINABLE
FEW-SHOT DEEPPROTOTYPICAL NETWORK TO MONITOR AND
DETECT COVID-19 I NFECTION FROM POINT -OF-CARE
ULTRASOUND IMAGES
Jessy Song
Department of Systems Design Engineering
University of Waterloo
Waterloo, ON N2L 3G1, CanadaAshkan Ebadi
Digital Technologies Research Centre
National Research Council Canada
Toronto, ON M5T 3J1, Canada
ashkan.ebadi@nrc-cnrc.gc.ca
Adrian Florea
Department of Emergency Medicine
McGill University
Montreal, QC H4A 3J1, CanadaPengcheng Xi, Stéphane Tremblay
Digital Technologies Research Centre
National Research Council Canada
Ottawa, ON K1A 0R6, Canada
Alexander Wong
Department of Systems Design Engineering
University of Waterloo
Waterloo, ON N2L 3G1, Canada
ABSTRACT
As the Coronavirus Disease 2019 (COVID-19) continues to impact many aspects of life and the global
healthcare systems, the adoption of rapid and effective screening methods to prevent further spread of
the virus and lessen the burden on healthcare providers is a necessity. As a cheap and widely accessible
medical image modality, point-of-care ultrasound (POCUS) imaging allows radiologists to identify
symptoms and assess severity through visual inspection of the chest ultrasound images. Combined
with the recent advancements in computer science, applications of deep learning techniques in
medical image analysis have shown promising results, demonstrating that artiﬁcial intelligence-based
solutions can accelerate the diagnosis of COVID-19 and lower the burden on healthcare professionals.
However, the lack of a huge amount of well-annotated data poses a challenge in building effective
deep neural networks in the case of novel diseases and pandemics. Motivated by this, we present
COVID-Net USPro, an explainable few-shot deep prototypical network, that monitors and detects
COVID-19 positive cases with high precision and recall from minimal ultrasound images. COVID-
Net USPro achieves 99.65% overall accuracy, 99.7% recall and 99.67% precision for COVID-19
positive cases when trained with only 5 shots. The analytic pipeline and results were veriﬁed by our
contributing clinician with extensive experience in POCUS interpretation, ensuring that the network
makes decisions based on actual patterns.
Keywords Ultrasonic imaging LungCOVID-19Few-shot learningDeep explainable architecture
1 Introduction
The Coronavirus Disease 2019, or COVID-19, caused by severe acute respiratory syndrome coronavirus 2 (SARS-
CoV-2), has been continuously impacting individual’s well-being and the global healthcare systems [ 1]. Despite thearXiv:2301.01679v1  [eess.IV]  4 Jan 2023Song et al. (2023). COVID-Net USPro.
vaccination efforts, policies and regulations in place, due to the rapid transmission of the virus and waves of rising cases,
the development of effective screening and risk stratiﬁcation methods remains to be a critical need in controlling the
disease [ 2]. Various types of diagnostic tools, including reverse transcription-polymerase chain reaction (RT-PCR),
rapid antigen detection tests, and antibody tests, have been developed and adapted globally to increase the rate of
screening. While RT-PCR has been the gold standard test for diagnosing COVID-19, the technique involves large
labour and laboratory resources and is time-consuming [ 3]. Other rapid antigen tests and antibody tests with varying
sensitivity are also less reliable in comparison to RT-PCR tests [3].
For people with signiﬁcant respiratory symptoms, medical imaging is used to identity the disease and assess the
severity of the disease progression [ 4]. Under this protocol, a computed tomography (CT) scan, chest X-ray (CXR), or
point-of-care ultrasound (POCUS) imaging can be performed and used clinically as an alternative diagnostic tool [ 2]. To
make a diagnosis, acute care physicians and radiologists visually inspect the radiographic images to ﬁnd patterns related
to symptoms and to assess severity of COVID-19 infection and deformation [ 3]. During times of high transmission rate
of COVID-19, large inﬂux of patients increases the burden on clinicians and radiologists. Medical image processing and
artiﬁcial intelligence (AI) can assist in reducing this burden and accelerate the diagnostic and decision-making process,
as existing models and algorithms continue to improve and the amount of available medical image data continues to
grow [5, 6, 7].
Different imaging modalities, including CT scan, X-ray, and ultrasound may be used in the diagnosis of COVID-19 and
offer varying diagnostic values [ 8]. Chest CT scan is the most sensitive imaging modality in the initial diagnosis and
management of conﬁrmed cases, but it is more expensive and time-consuming [ 8,5]. In contrast, ultrasound imaging is
more accessible and portable, cheap, and safer as radiation is not involved during the examination, which are desirable
properties for its usage [8], especially in resource-limited settings/environments/areas/regions.
Deep learning usually requires a large set of training examples [ 9,7,4]. However, due to the nature of novel diseases,
the availability of such a huge amount of well-annotated data poses a great challenge to the learning algorithms.
Few-shot learning is an approach where model is trained to classify new data based on a limited number of samples
exposed in training [ 10]. This resembles how humans learn, as we can recognize new object classes from very few
instances, different from other current machine learning techniques that require large amount of data to achieve similar
performance [ 10]. Since the few-shot model requires less data to train, the computational costs in the process is also
signiﬁcantly reduced [ 10]. These properties make it an appropriate and promising approach for COVID-19 or rare
disease diagnosis. One approach for few-shot learning is metric-based learning. As a few-shot metric-based learning
approach, prototypical networks (PN) perform classiﬁcation by computing distances to prototype representations of
each class [ 10]. PN has shown state-of-the-art (SOTA) results on other datasets/domains (e.g., [ 11,12,13]), proving that
some simple design decisions can yield signiﬁcant improvements over other complicated architectures and meta-learning
approaches [10].
Motivated by the needs for fast and effective alternative screening solutions and considering ultrasound imaging
advantages, we present an open-source explainable deep prototypical network, called COVID-Net USPro, that learns to
detect COVID-19 positive cases with high precision and recall from a very limited number of lung ultrasound (LUS)
images. When trained with only 5 shots, COVID-Net USPro classiﬁes between positive and negative COVID-19
cases with 99.65% overall accuracy, 99.7% recall and 99.67% precision for COVID-19 positive cases. Intensive
experimentation was conducted (e.g., testing different image encoders, varying training conditions and number of
classes to optimize the network) to assess the performance of COVID-Net USPro network. To ensure the network’s
fairness and accountability, network beneﬁts from an explainability module, assessing decisions with visual explanation
tools, i.e., Grad-CAM [ 14] and GSInquire [ 15]. Moreover, our contributing clinician (A.F.) carefully veriﬁed and
validated the pipeline and produced results to ensure the validity of the proposed solution from the clinical perspective.
1.1 Related Work
There are several studies that aim to apply deep learning into the screening and detection of COVID-19 positive cases.
As an open-source and open-access initiative, the COVID-Net [ 16,5,9,7] includes research on the application of deep
learning neural networks using multitude of image modalities, such as CT, X-ray, and ultrasound images. Multiple
works have demonstrated the effectiveness of deep learning in the classiﬁcation of CT and X-ray images. For example,
COVID-Net CXR [ 17], a tailored deep convolutional neural network (DCNN/CNN) for detection of COVID-19 cases
from chest X-ray images, has achieved an overall accuracy of 98.3% and 97.5% sensitivity for COVID-19 cases.
Another work by Ozturk et al. proposed a DCNN based on the DarkNet model used for the you only look once (YOLO)
real time object detection system to classify X-ray images, which achieves 98.08% accuracy for binary COVID-19
cases detection [ 18]. Research by Afshar et al. proposed a capsule CNN-based network called COVID-CAPS [ 19]
which achieved over 98% accuracy and speciﬁcity using a limited amount of X-ray images. COVID-Net CT [ 6], a
deep neural network for detection of COVID-19 from CT images, scored 96.2% in sensitivity and 99% in speciﬁcity
2Song et al. (2023). COVID-Net USPro.
for COVID-19 cases. Potential of including both CT-scan and X-ray images for classiﬁcation is also explored, with
research by Thakur and Kumar demonstrating a DCNN-based model achieving over 99% accuracy and precision for
COVID-19 detection using images of both modalities [ 20]. For ultrasound images, custom neural network such as
COVID-Net US [ 7] was constructed and tailored to COVID-19 case detection. The network achieved an area under
receiver operating curve (AUC) of over 98% when trained with positive COVID-19 and negative normal case images.
Research by Diaz-Escobar et al. [ 21] also leveraged pre-trained neural networks such as VGG19 [ 22], InceptionV3
[23], and ResNet50 [ 24] in the detection of COVID-19 using ultrasound images and achieved 89.1% accuracy and AUC
of 97.1%. One limitation of using a custom deep neural network in most of the existing research is the need for a large
amount of training data, where in mentioned works above, datasets all surpassed 10,000 total images [7, 9].
Application of few-shot learning techniques has also been investigated. For example, MetaCOVID, proposed by
Shorfuzzaman et al [ 25], is a Siamese neural network framework with contrastive loss for few-shot diagnosis of
COVID-19 infection using CXR images. The performance of the best network achieved an accuracy of 95.6% and AUC
of 97% when trained under a 3-way, and tested in a 10-shot setting [ 25]. In [ 26], a deep siamese convolutional network,
called COVID-Net FewSE, is able to detect COVID-19 positive cases with 90% recall and accuracy of 99.7% when the
network is provided with only 50 observations in the training phase. In the work by Karnes et al. [ 27], the possibility
of using adaptive few-shot learning for ultrasound COVID-19 detection is examined, and the increasing performance
with the increasing number of shots is investigated. Although the feasibility of adopting few-shot learning techniques
for COVID-19 detection from medical imaging has been already investigated, analysis on network’s interpretability
is either missing or inadequate and lacks clinician validation, which limits the full understanding of the network and
whether data interpretation process aligns with real clinical settings.
Our contribution is at least three folds: 1) We presents a high-performing network (99.65% accuracy) trained with only
5 shots, while other works achieving similar performance require larger numbers of training examples, 2) COVID-Net
USPro is an explainable network, as demonstrated by analysis from two explainability visualization tools and clinician
validation, and 3) COVID-Net USPro is open-sourced and available to the public, which helps promote reproducibility
and accessibility of AI in healthcare and encourage further innovation.
The remainder of this paper is as follows. Section 2 explains data, techniques, and the experiments conducted to assess
the network performance in details. Section 3 presents ﬁndings from the analysis. Findings are then discussed in
Section 4 where some limitations of the research and future directions are also presented.
2 Data and Methodology
2.1 Data
The COVIDx-US dataset v1.4. [ 1] is used for this study. COVIDx-US is an open-access benchmark dataset of lung
ultrasound imaging data that contains 242 videos and 29,651 processed images of patients with COVID-19 infection,
non-COVID-19 infection, other lung conditions, and normal control cases. The dataset provides LUS images captured
with two kinds of probe, linear probe which produces a square or rectangular image, or convex probe, which allows for
a wider ﬁeld of view [ 28]. Due to the difference in ﬁeld of view and low numbers of COVID-19 positive examples with
linear probe, combining the linear and convex probe data in training may increase noise and inﬂuence the performance
of the network and hence, linear probe data are excluded in this study. A total number of 25,262 convex LUS images are
then randomly split into train set containing 90% of images in each class and test set with the remaining 10% of images,
ensuring all frames from each video are either in train or test set to avoid data leakage. All images are rescaled to
224224pixels to keep the images across entire dataset consistent. The dataset is further augmented by rotating each
image by 90 °, 180 °, 270 °, resulting in a total of 101,048 images ( 252624). This rotation technique is an appropriate
method for increasing the dataset size, as it keeps the images and areas of interest for clinical decisions unaltered and
in-bound [29].
2.2 Methodology
COVID-Net USPro is a prototypical few-shot learning network that trains in an episodic learning setting, using a
distance metric for assessing similarities between a set of unlabelled data, i.e., query set, and labelled data, i.e., support
set. Labelled data can be used to compute a single prototype representation of the class, and unlabelled data are assigned
to the class of the prototype they are closest to. A prototypical network [ 10] is based on this idea that there exists
an embedding in which points in a class cluster around a single prototype representation for the class. During the
training phase, a neural network is used to learn the non-linear mapping of the inputs to an embedding space, and a
class prototype is computed as the mean of its support set data in the embedding space. Classiﬁcation is then done by
ﬁnding the nearest class prototype for each query point based on a speciﬁed distance metric. An episodic approach
3Song et al. (2023). COVID-Net USPro.
Figure 1: High-level conceptual ﬂow of the Analysis.
is used to train the model, where in each training episode, the few-shot task is simulated by sampling the data point
in mini-batches to make the training process consistent with the testing environment. Performance of the network is
evaluated using the test dataset, and both quantitative analysis based on accuracy, precision and recall and qualitative
explainability analysis are conducted. An high-level conceptual ﬂow of the Analysis is presented in Figure 1.
We deﬁned the classiﬁcation problem as a K-wayN-shot episodic task, where Kdenotes the number of classes present
in the dataset and Ndenotes the number of available image examples for each class in each episode. For a given dataset,
Nimages from each of the Kclasses are sampled to form the support set, and another Mimages from each class are
sampled to form the query set. The network then aims to classify the images of the query set based on the KNtotal
images presented in the support set. In this work, we formulated the problem as a 2-way, 3-way and 4-way classiﬁcation
problem. Details are included under section 2.3.3.
The few-shot classiﬁcation with prototypical network can be summarized into three steps: 1) encoding of the images, 2)
generating class prototypes, and 3) assigning labels to query samples based on distance to the class prototypes. Let’s
S=f(x(1;s); y(1;s)); : : : ; (x(N;s); y(N;s))gandQ=f(x(1;q); y(1;q)); : : : ; (x(N;q); y(N;q))gbe the support and query
sets respectively, where each xi2RDis aD-dimensional example feature vector and yi2f1; : : : Kgis the label of
the example. The prototypical network embodies an image encoder f:RD!RHthat transforms each image xi
onto a H-dimensional embedding space where images of the same class cluster together. Class prototypes are then
generated for each class by averaging the embedding image vectors in the support set, where vk=1
NPN
i=1f(xi;s(k))
denotes the prototype of class k[10]. To classify the query image, a distance metric is used where distances between
the embedding vector of a query image and each of the class prototypes are computed. In this work, squared Euclidean
distance d(v; q) =jjv qjj=qP(vi q)2is used, where qis the embedding vector of the query image and viis the
embedding vector of the i-th prototype. After distances are computed, a SoftMax function is applied over distances to
the prototypes to compute the probabilities of the query image being in each class. The class with the highest probability
is then assigned to the query image.
In the training phase, the network learns by minimizing a loss function, i.e., the negative log-SoftMax function
(J= log (p(y=kjx))) of the true class kvia an optimizer for which we use an Adam optimizer with an initial
learning rate of 0.001, and reduced if loss is not improved after 3 epochs. In each episode, a subset of data points
is randomly selected, forming support and query set. Loss term is calculated at the end of each training episode. To
facilitate effective training process and prevent over-ﬁtting, early stopping is implemented to stop the training process
4Song et al. (2023). COVID-Net USPro.
Figure 2: COVID-Net USPro, network architecture design.
when loss term is not improved after 5 epochs. A total of 10 epochs is set for all training processes and 200 episodes is
set for each training epoch. Figure 2 presents an architecture design overview of the COVID-Net USPro network.
Trained model’s performance is evaluated quantitatively and qualitatively. In quantitative analysis, model’s accuracy,
precision and recall for each class are reported. In qualitative analysis, model explainability is investigated and
visualized. Explainable Artiﬁcial Intelligence (XAI) has been an important criterion when assessing whether neural
networks can be applied to real clinical settings [ 30]. While AI-driven systems may show high accuracy and precision
in analyzing medical images, lack of reasonable explainability will spark criticism to the network’s adoption [ 30].
COVID-Net USPro’s explainability is assessed using two approached, i.e., Gradient-weighted Class Activation Map
(Grad-CAM) [ 14] and GSInquire [ 15], on a selected dataset containing correctly classiﬁed COVID-19 and normal cases
with high conﬁdence (i.e., >99:9%probability) as well as falsely predicted COVID-19 and normal cases. Grad-CAM
generates a visual explanation of the input image using the gradient information ﬂowing into the last convolutional
layer of the convolutional neural network (CNN) encoder and assigns importance values to each neuron for making a
classiﬁcation decision [ 14]. The output is a heatmap-overlayed image that shows the regions that impact the particular
classiﬁcation decision made by the network [ 14]. The other tool GSInquire identiﬁes the critical factors in an input
image that are shown to be integral to the decisions made by the network in a generative synthesis approach [ 15].
The result is an annotated image highlighting the critical region, which drastically changes the classiﬁcation result if
removed [ 15]. Results from both tools are reviewed by a clinician with experience in analysis of ultrasound images to
assess whether clinically important patterns are captured by the network.
2.3 Experiment Settings
We comprehensively assess the performance of COVID-Net USPro in detecting COVID-19 positive cases from
ultrasound images by testing various training conditions such as image encoders, number of shots available for training,
and classiﬁcation task types. Details are further discussed in this section.
2.3.1 Image Encoders
To leverage the power of transfer learning, multiple encoders are experimented, including but not limited to the ResNet
and VGG-based models [ 24,22]. Pre-trained models refer to using model parameters pre-trained on ImageNet [ 31].
Here, we report 4 best encoders with respect to our research objectives:
•ResNet18L1: Pre-trained ResNet18 [ 24], with trainable parameters on the ﬁnal connected layer and setting
out features as the number of classes. Model 1 is regarded as the baseline model for encoders, as it contains
the least number of layers and retrained parameters.
•ResNet18L5: Pre-trained ResNet18 [ 24], with trainable parameters on the last 4 convolutional layers and ﬁnal
connected layer. Out features set to the number of classes.
•ResNet50L1: Pre-trained ResNet50 [ 24], with trainable parameters on the ﬁnal connected layer and setting
out features as the number of classes.
•ResNet50L4: Pre-trained ResNet50 [ 24], with trainable parameters on the last 3 convolutional layers and ﬁnal
connected layer. Out features set to the number of classes.
5Song et al. (2023). COVID-Net USPro.
2.3.2 Number of Training Shots
The optimal number of shots for maximized performance is tested by training models under 5, 10, 20, 30, 40, 50, 75,
and 100-shot scenarios. For selected models showing steady increase of performance over increasing shots, 150 and
200-shot conditions are tested to verify that the maximum performance is reached at 100-shot. To ensure training
process is faithful to the testing environment, the number of example shots for each class presented in each episode is
the same in support and query set in both training and testing. For example, in 5-shot scenario, 5 images in each class
are presented for both support set and query set in training, and the same follows in testing.
2.3.3 Problem Formulation
As the ability of the model to correctly identify COVID-19 positive cases is valued the most in comparison to other
classes, the classiﬁcation problem for identifying COVID-19 was formulated in 3 different scenarios as follows, in an
ascending order of data complexity:
•2-way classiﬁcation: Data from all 3 other classes, namely ’normal’ class, ’non-COVID-19’ class and ’other’
class, are viewed as a combined COVID-19 negative class. The network learns from COVID-19 positive and
COVID-19 negative dataset in this setting.
•3-way classiﬁcation: As the ’other’ class contains data from multiple different lung conditions, it has the
highest variations and may disrupt network’s learning process due to the lack of uniformity in the data
compared with COVID-19, normal or non-COVID-19 class. In 3-class classiﬁcation, the ‘other’ class is
excluded, and the network is trained to classify the remaining three classes.
•4-way classiﬁcation: As the dataset contains four classes, the four-class classiﬁcation condition remains this
setting and network is trained to classify ’COVID-19’, ’normal’, ’non-COVID-19’ and ’other’ class.
3 Results
This section summarizes the quantitative performance results of all combination of experiment settings listed in Section
2.3 as well as the results of the network explainability analysis.
3.1 Quantitative Performance Analysis
The performance of COVID-Net USPro is evaluated using the overall accuracy, and the precision and recall for each
class. As the performance of the model to diagnose COVID-19 positive cases is the most important for current clinical
use case, precision and recall for only COVID-19 case is reported below. To reduce table size, Table 1 only summarizes
the performance of the network under 5-shot and 100-shot scenarios for encoders that scored over 80% across all
evaluated metrics. For full performance results of all shot settings and precision, recall for all classes, please refer to
project repository: [www.anonymous].
Across all classiﬁcation types and models, performance is better under 100-shots training scenario than in 5-shot, with
performance metrics increasing from 5-shot and plateauing after 75-shot, as shown in Figure 3. ResNet networks
demonstrate the ability to classify COVID-19 with precision and recall above 87% consistently under both 5-shot and
above 99% under 100-shot condition. As seen in Table 1, the increasing classes in 3-way and 4-way classiﬁcation
types reduces the performance of the network, as the classiﬁcation is more complex given larger number of classes.
However, this performance difference among the three classiﬁcation types is reduced when the number of shots
increases, as more examples available in training improves the network’s ability to distinguish between multiple classes.
Among the four models, deeper models (i.e., those with ResNet50 as encoder) perform better in all classiﬁcation types
and shot conditions. In addition, models with re-trained ﬁnal convolutional layers parameters (model ResNet18L5
and ResNet50L4) using the ultrasound images achieve higher accuracy, precision, and recall. Therefore, it can be
said that while using pre-trained parameters and simpler models reduce the computational complexity and space,
tailoring parameters on the ﬁnal 3-4 convolutional layers to the ultrasound images and deeper image encoding boosted
performance to above 99%.
In 2-way and 3-way classiﬁcation, it is also observed that the precision and recall for classes other than COVID-19
achieve similar magnitude as the COVID-19 class. In the 4-way case, the precision and recall for ‘other’ class is
around 2-3% lower than those for ‘non-COVID-19’, ‘normal’ and ‘COVID-19’ classes. This is expected since the
‘other’ class covers various lung conditions/diseases that encompass a larger range of image features and variations.
Overall, with precision and recall achieving similar magnitude for all cases in 2-way, 3-way and 4-way classiﬁcation,
the network also demonstrates the ability to distinguish between multiple diseases. In comparison to studies outlined in
6Song et al. (2023). COVID-Net USPro.
Table 1: Summary of classiﬁcation results for 5-shot and 100-shot conditions.
Scenario No. shots Model Accuracy Precision Recall
2-way 5 ResNet18L1 0.9420 0.9486 0.9460
2-way 5 ResNet18L5 0.9930 0.9925 0.9950
2-way 5 ResNet50L1 0.9525 0.9570 0.9560
2-way 5 ResNet50L4 0.9965 0.9967 0.9970
2-way 100 ResNet18L1 0.9758 0.9764 0.9755
2-way 100 ResNet18L5 1.0000 1.0000 1.0000
2-way 100 ResNet50L1 0.9963 0.9964 0.9962
2-way 100 ResNet50L4 0.9999 0.9999 1.0000
3-way 5 ResNet18L1 0.9570 0.9606 0.9510
3-way 5 ResNet18L5 0.9987 0.9992 0.9970
3-way 5 ResNet50L1 0.9945 0.9508 0.9660
3-way 5 ResNet50L4 0.9947 0.9942 0.9940
3-way 100 ResNet18L1 0.9867 0.9833 0.9853
3-way 100 ResNet18L5 1.0000 1.0000 1.0000
3-way 100 ResNet50L1 0.9977 0.9970 0.9975
3-way 100 ResNet50L4 1.0000 1.0000 1.0000
4-way 5 ResNet18L1 0.8627 0.9281 0.8710
4-way 5 ResNet18L5 0.9817 0.9975 0.9970
4-way 5 ResNet50L1 0.9392 0.9640 0.9540
4-way 5 ResNet50L4 0.9850 0.9917 0.9930
4-way 100 ResNet18L1 0.9385 0.9742 0.9704
4-way 100 ResNet18L5 0.9884 1.0000 1.0000
4-way 100 ResNet50L1 0.9813 0.9947 0.9955
4-way 100 ResNet50L4 0.9902 1.0000 1.0000
Figure 3: Performance results with increasing shots trained under 4-class condition: ( a) Pre-trained ResNet18 with
trainable parameters on the ﬁnal connected layer and setting out features as the number of classes (ResNet18L1).
(b) Pre-trained ResNet50 with trainable parameters on the last 3 convolutional layers and ﬁnal connected layer
(ResNet50L4).
7Song et al. (2023). COVID-Net USPro.
Figure 4: COVID-19 positive case examples correctly classiﬁed by COVID-Net USPro with high conﬁdence: ( a) an
example of wrong decision factors. ( b) an example of a decision made based on the disease-related patterns.
Section 1.1, the performance of COVID-Net USPro networks tailored to ultrasound images with re-trained parameters
is improved. Accuracy of ResNet50L1 and ResNet50L4 exceeds 98% under 4-way 5-shot setting, while other work
such as MetaCOVID [ 25], which also applied a few-shot approach, achieved 95.6% accuracy under 3-way, 10-shot
setting. Additionally, the sensitivity of COVID-Net USPro for COVID-19 cases are also higher than networks trained
with other image modality data such as X-ray or CT, where they scored 97.5% in the best performing case [6].
3.2 Clinical Validation and Network Explainability Analysis
In addition to the intensive quantitative performance analysis, we clinically validated the network output to ensure that
the network captures important patterns in the ultrasound images. For this purpose, our contributing clinician (A.F.)
reviewed a randomly selected set of images and reported his ﬁndings and observations. Our contributing clinician (A.F.)
is an Assistant Professor in the Department of Emergency Medicine and the ultrasound co-director for undergraduate
medical students at McGill University. He is practicing Emergency Medicine full-time at Saint Mary’s Hospital in
Montreal.
Figure 4 presents two select ultrasound images of COVID-19 positive cases, annotated by Grad-CAM and GSInquire,
as examples, that were reviewed. As seen, the annotated images contain the lung pleura region at the top of the
image, while the second example (Figure 4-b) also marks the bottom region with high importance. B-lines, or the light
comet-tail artifacts extending from pleura to the bottom of the image, and the presence of dark regions interspacing
the B-lines at the bottom part of the image corresponding to signs of lung consolidation are indicators of abnormality
[32]. Hence, the visual annotations for the second example (Figure 4-b) are more representative for disease-related
patterns within the ultrasound image. Figure 4-a is one of the examples where the model considers the rib as a structure
of interest, which is not the abnormality, leading to classify the images as a COVID-19 positive case. Hence, although
the model correctly classiﬁed the image, the decision was made based on invalid clinical factors.
We implement two strategies to solve the mentioned issues and improve classiﬁcation explainability. First, excluding
images with low image quality, such as insufﬁcient image depth or the lack of representative features. A severity grade
introduced by COVIDx-US dataset v1.4, called lung ultrasound score (LUSS), rates each ultrasound video on a scale
of 0 to 3, where 0 corresponds to presence of only normal features, and 3 corresponds to presence of severe disease
artifacts [ 33]. Therefore, in the ﬁrst attempt to improve the network, images from videos with score of 0 for the normal
case and images from videos with score of 2 and 3 for COVID-19 case are used to train a binary classiﬁcation version
8Song et al. (2023). COVID-Net USPro.
Figure 5: Four cropped COVID-19 positive examples predicted correctly with high conﬁdence by COVID-Net USPro
(a-d), while recognizing disease artifacts, e.g., extended B-lines.
of the network. By observing the annotated images, network shows to focus more on the bottom regions of the images,
though cases where network focus on the top pleura region are still present. The second strategy to further improve
model explainability is to exclude regions above the pleura (i.e., soft tissue) of the images, so that network focuses on
the disease-deﬁning features, present mostly at the bottom of the images below lung pleura. Our experiments conﬁrm
the effectiveness of this strategy. Hence, combining the ﬁrst and second strategy, a binary model with LUSS score
ﬁltered and cropped images is trained. Figure 5 shows examples from the cropped images analysis. As suggested from
the annotated examples and conﬁrmed by our contributing clinician (A.F.), clinically determining artifacts such as
B-lines and lung consolidation are clearly identiﬁed in COVID-19 positive images by COVID-Net USPro.
9Song et al. (2023). COVID-Net USPro.
4 Conclusions
Deep neural network architectures have shown promising results in a wide range of tasks, including predictive and
diagnostic tasks. However, such networks require a massive amount of labelled data to train which is against the nature
of new pandemics and novel diseases where there are no or very few data samples available, especially in the initial
stages. As part of the COVID-Net initiative and using a diverse complex benchmark dataset, i.e., COVIDx-US, in this
work we introduce the COVID-Net USPro network, tailored to detect COVID-19 infection with high accuracy from very
few ultrasound images. The proposed deep prototypical network leverages pretrained models with tailored parameters
on ﬁnal layers to reduce computational complexity and achieve high classiﬁcation performance when only 5 examples
from each class are available for training. Accuracy, precision and recall for the best performing network are over 99%,
which are comparable or outperforming other existing work [ 7,27]. These properties are not only highly crucial for the
control of the COVID-19 pandemic but also for screening patients in new diseases/pandemics for which the proposed
network can be easily tuned. We intensively assessed the explainability of the network and clinically validated its
performance. Experimental results demonstrate that COVID-Net USPro can not only achieve high performance in terms
of accuracy, precision, and recall, but also shows predictive behaviour that is consistent with clinical interpretation, as
validated by our contributing clinician (A.F.). In addition, as part of the explainability-driven performance validation
process, we proposed and implemented two strategies to further improve the network performance in accordance with
the background clinical knowledge in identifying COVID-19 positive and negative cases. Overall, we believe the
simplicity and effectiveness of COVID-Net USPro makes it a promising tool to aid the COVID-19 screening process
using ultrasound images. We hope the open-source release of COVID-Net USPro help researchers and clinical data
scientists to accelerate innovations in the combat against the COVID-19 pandemic that can ultimately beneﬁt the larger
society.
Several future research directions can be explored to further improve the network. First, some additional steps in data
augmentation and preparation can be taken to improve data quality and dataset size. In this work, ultrasound images
captured with linear probe are excluded due to differences in clinical interpretation of linear probe and convex probe
captured images. More image augmentation and preparation techniques can be experimented to include linear probe
data and increase the data size. Second, in this work, we used simple cropping to ﬁlter out the pleura region of the
images. A more procedural image segmentation step could be added to include only clinically relevant areas of the
images for network construction to further improve network performance from the explainability standpoint. Lastly, we
used COVIDx-US which is a public dataset that includes data of various sources and quality. Network training could be
improved by only using high quality input ultrasound data, collected systematically, which contain clear representative
image artifacts with sufﬁcient/speciﬁc image depth. For this purpose, a data collection protocol might be required to
capture ultrasound images in a standardized manner from a set of consented participants.
References
[1]Ashkan Ebadi, Pengcheng Xi, Alexander MacLean, Adrian Florea, Stéphane Tremblay, Sonny Kohli, and
Alexander Wong. Covidx-us: An open-access benchmark dataset of ultrasound imaging data for ai-driven
covid-19 analytics. Frontiers in Bioscience-Landmark , 27(7), 2022.
[2]Marco Cascella, Michael Rajnik, Abdul Aleem, Scott C. Dulebohn, and Raffaela Di Napoli. Features, evaluation,
and treatment of coronavirus (covid-19), May 2022.
[3]Dinnes Jacqueline, Deeks Jonathan J, Berhane Sarah, Taylor Melissa, Adriano Ada, Davenport Clare, Dittrich
Sabine, Emperador Devy, Takwoingi Yemisi, Cunningham Jane, Beese Sophie, Domen Julie, Dretzke Janine,
Ferrante di Ruffano Lavinia, Harris Isobel M, Price Malcolm J, Taylor-Phillips Sian, Hooft Lotty, Leeﬂang
Mariska MG, McInnes Matthew DF, Spijker René, Van den Bruel Ann, and Cochrane COVID-19 Diagnostic
Test Accuracy Group. Rapid, point-of-care antigen and molecular-based tests for diagnosis of sars-cov-2 infection.
Cochrane Database of Systematic Reviews , 2021.
[4]Dandi Yang, Cristhian Martinez, Lara Visuña, Hardev Khandhar, Chintan Bhatt, and Jesus Carretero. Detection
and analysis of covid-19 in medical images using deep learning techniques. Scientiﬁc Reports , 11, 2021.
[5]Linda Wang, Zhong Qiu Lin, and Alexander Wong. Covid-net: a tailored deep convolutional neural network
design for detection of covid-19 cases from chest x-ray images. Scientiﬁc Reports , 10(1):19549, Nov 2020.
[6]Hayden Gunraj, Ali Sabri, David Koff, and Alexander Wong. Covid-net ct-2: Enhanced deep neural networks
for detection of covid-19 from chest ct images through bigger, more diverse learning. Frontiers in Medicine ,
8:729287, 2022.
[7]Alexander MacLean, Saad Abbasi, Ashkan Ebadi, Andy Zhao, Maya Pavlova, Hayden Gunraj, Pengcheng Xi,
Sonny Kohli, and Alexander Wong. Covid-net us: A tailored, highly efﬁcient, self-attention deep convolutional
10Song et al. (2023). COVID-Net USPro.
neural network design for detection of covid-19 patient cases from point-of-care ultrasound imaging. In FAIR-
MICCAI’21 , 2021.
[8]Hany Kasban. A comparative study of medical imaging techniques. International Journal of Information Science
and Intelligent System , 4:37–58, 03 2015.
[9]Hayden Gunraj, Linda Wang, and Alexander Wong. Covidnet-ct: A tailored deep convolutional neural network
design for detection of covid-19 cases from chest ct images. Frontiers in Medicine , 7:1025, 2020.
[10] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In I. Guyon,
U. V . Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural
Information Processing Systems , volume 30. Curran Associates, Inc., 2017.
[11] Gen Li, Varun Jampani, Laura Sevilla-Lara, Deqing Sun, Jonghyun Kim, and Joongkyu Kim. Adaptive prototype
learning and allocation for few-shot segmentation. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR) , pages 8334–8343, June 2021.
[12] Shengli Sun, Qingfeng Sun, Kevin Zhou, and Tengchao Lv. Hierarchical attention prototypical networks for
few-shot text classiﬁcation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language
Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , pages
476–485, Hong Kong, China, November 2019. Association for Computational Linguistics.
[13] Jessica Deuschel, Daniel Firmbach, Carol I. Geppert, Markus Eckstein, Arndt Hartmann, V olker Bruns, Petr
Kuritcyn, Jakob Dexl, David Hartmann, Dominik Perrin, Thomas Wittenberg, and Michaela Benz. Multi-prototype
few-shot learning in histopathology. In Proceedings of the IEEE/CVF International Conference on Computer
Vision (ICCV) Workshops , pages 620–628, October 2021.
[14] Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv
Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In 2017 IEEE
International Conference on Computer Vision (ICCV) , pages 618–626, 2017.
[15] Zhong Qiu Lin, Mohammad Javad Shaﬁee, Stanislav Bochkarev, Michael St. Jules, Xiaoyu Wang, and Alexander
Wong. Do explanations reﬂect decisions? A machine-centric strategy to quantify the performance of explainability
algorithms. CoRR , abs/1910.07387, 2019.
[16] Alexander Wong. Covid-net open initiative.
[17] Hossein Aboutalebi, Maya Pavlova, Hayden Gunraj, Mohammad Javad Shaﬁee, Ali Sabri, Amer Alaref, and
Alexander Wong. Medusa: Multi-scale encoder-decoder self-attention deep neural network architecture for
medical image analysis, 2021.
[18] Tulin Ozturk, Muhammed Talo, Eylul Azra Yildirim, Ulas Baran Baloglu, Ozal Yildirim, and U. Rajendra Acharya.
Automated detection of covid-19 cases using deep neural networks with x-ray images. Computers in Biology and
Medicine , 121:103792, 2020.
[19] Parnian Afshar, Shahin Heidarian, Farnoosh Naderkhani, Anastasia Oikonomou, Konstantinos N. Plataniotis, and
Arash Mohammadi. Covid-caps: A capsule network-based framework for identiﬁcation of covid-19 cases from
x-ray images. Pattern Recognition Letters , 138:638–643, 2020.
[20] Samritika Thakur and Aman Kumar. X-ray and ct-scan-based automated detection and classiﬁcation of covid-19
using convolutional neural networks (cnn). Biomedical Signal Processing and Control , 69:102920, 2021.
[21] Julia Diaz-Escobar, Nelson E. Ordóñez-Guillén, Salvador Villarreal-Reyes, Alejandro Galaviz-Mosqueda, Vitaly
Kober, Raúl Rivera-Rodriguez, and Jose E. Lozano Rizk. Deep-learning based detection of covid-19 using lung
ultrasound imagery. PLOS ONE , 16(8), 2021.
[22] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In
International Conference on Learning Representations , 2015.
[23] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the
inception architecture for computer vision. CoRR , abs/1512.00567, 2015.
[24] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. CoRR ,
abs/1512.03385, 2015.
[25] Mohammad Shorfuzzaman and M. Shamim Hossain. Metacovid: A siamese neural network framework with
contrastive loss for n-shot diagnosis of covid-19 patients. Pattern Recognition , 113:107700, 2021.
[26] A. Ebadi, H. Azimi, P. Xi, S. Tremblay, and A. Wong. Covid-net fewse: An open-source deep siamese convolu-
tional network model for few-shot detection of covid-19 infection from x-ray images. Journal of Computational
Vision and Imaging Systems , 7(1):16–18, 2021.
11Song et al. (2023). COVID-Net USPro.
[27] Michael Karnes, Shehan Perera, Srikar Adhikari, and Alper Yilmaz. Adaptive few-shot learning poc ultrasound
covid-19 diagnostic system, 2021.
[28] Wonseok Lee and Yongrae Roh. Ultrasonic transducers for medical diagnostic imaging. Biomedical Engineering
Letters , 7(2):91–97, 2017.
[29] Zeshan Hussain, Francisco Gimenez, Darvin Yi, and Daniel Rubin. Differential data augmentation techniques for
medical imaging classiﬁcation tasks, Apr 2018.
[30] Julia Amann, Alessandro Blasimme, Effy Vayena, Dietmar Frey, and Vince I. Madai. Explainability for artiﬁcial
intelligence in healthcare: A multidisciplinary perspective. BMC Medical Informatics and Decision Making ,
20(1), 2020.
[31] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In 2009 IEEE conference on computer vision and pattern recognition , pages 248–255. Ieee, 2009.
[32] Daniel A. Lichtenstein, Gilbert A. Mezière, Jean-François Lagoueyte, Philippe Biderman, Ivan Goldstein, and
Agnès Gepner. A-lines and b-lines: lung ultrasound as a bedside tool for predicting pulmonary artery occlusion
pressure in the critically ill. Chest , 136(4):1014–1020, 2009.
[33] Ashkan Ebadi, Pengcheng Xi, Alexander MacLean, Stéphane Tremblay, Sonny Kohli, and Alexander Wong.
Covidx-us - an open-access benchmark dataset of ultrasound imaging data for ai-driven covid-19 analytics.
arXiv:2103.10003 , 2021.
12