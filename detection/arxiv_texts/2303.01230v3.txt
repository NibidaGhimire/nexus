Synthetic Data: Methods, Use Cases, and Risks
Emiliano De Cristofaro, University of California, Riverside
emilianodc@cs.ucr.edu
Abstract
Sharing data can often enable compelling applications and analytics. However, more often than
not, valuable datasets contain information of sensitive nature, and thus sharing them can endanger
theprivacyofusersandorganizations. Apossiblealternativegainingmomentuminboththeresearch
community and industry is to share synthetic data instead. The idea is to release artificially generated
datasets that resemble the actual data – more precisely, having similar statistical properties.
In this article, we provide a gentle introduction to synthetic data and discuss its use cases, the pri-
vacychallengesthatarestillunaddressed,anditsinherentlimitationsasaneffectiveprivacy-enhancing
technology.
How To Safely Release Data?
We begin by reviewing the traditional “alternative” steps and technologies used by practitioners to
share data while attempting to reduce information leakage.
Anonymization: Theoretically, one could remove personally identifiable information before sharing it.
However, in practice, anonymization fails to provide realistic privacy guarantees because a malevolent
actoroftenhasauxiliaryinformationthatallowsthemtore-identifyanonymizeddata. Forexample,when
Netflix de-identified movie rankings (as part of a challenge seeking better recommendation systems),
Arvind Narayanan and Vitaly Shmatikov [1] were able to de-anonymize a significant portion by cross-
referencing them with public information on IMDb.
Aggregation: Another approach is to share aggregate statistics about a dataset. For example, telecom-
munication providers can provide statistics about how many people are in some specific locations at a
given time — e.g., to assess footfall and decide where one should open a new store. However, this is
often ineffective too [2, 3], as the aggregates can still help an adversary learn something about specific
individuals.
DifferentiallyPrivateStatistics: Morepromisingattemptscomefromprovidingaccesstostatisticsobtained
from the data while adding noise to the queries’ response, guaranteeing differential privacy (DP) [4].
DP provides mathematical guarantees against what an adversary can infer from learning the result of
somealgorithm;i.e.,itguaranteesthatanindividualwillbeexposedtothesameprivacyriskwhetheror
not her data is included. DP is generally achieved by adding noise at various steps of training.
However, this approach generally lowers the dataset’s utility, especially on high-dimensional data.
Additionally, allowing unlimited non-trivial queries on a dataset can reveal the whole dataset, so this
approach needs to keep track of the privacy budget over time.
Types of Synthetic Data
There are different approaches to generating synthetic data. Derek Snow of the Alan Turing Insti-
tute lists three main methods [5]:
1arXiv:2303.01230v3  [cs.CR]  27 Feb 20241.Hand-engineered methods identify an underlying distribution from real data using expert opinion
and seek to imitate it.
2.Agent-basedmodels establishknownagentsandallowthemtointeractaccordingtoprescribedrules
hoping that this interaction would ultimately amount to distribution profiles that look similar to
the original dataset.
3.Generative machine models learn how a dataset is generated using a probabilistic model and create
synthetic data by sampling from the learned distribution.
In the rest of this article, we will focus on generative models , as they are generally considered state-
of-the-art.
Background: Generative vs. Discriminative Models
A good way to understand how generative models work is to look at how they differ from discrimi-
native models. Consider, for instance, the task of identifying which paintings are by Vincent Van Gogh.
First, we label a dataset of artworks we know whether or not were painted by Van Gogh. Then, we train
adiscriminative modeltolearnthatspecificcharacteristics(e.g.,colors,shapes,ortextures)aretypicalof
Van Gogh. We can now use that model to predict whether Van Gogh authored any painting.
Figure 1: Discriminative Machine Learning Models. (Source: “Generative Deep Learning,” (CC BY 4.0).
Whereasgenerativemodelsallowthegenerationofanewimageofahorsethatdoesnotactuallyexist
but looks real. More precisely, we can train a generative model to learn what horses look like ; to do so, we
need a dataset with many examples (observations) of horses.
Each observation has many characteristics (or features), e.g., each pixel value. The goal is to build a
model that can generate new sets of features that look like they have been created using the same rules
as the original data.
Algorithms
Generative models used to produce synthetic data may use a number of architectures. For instance,
GenerativeAdversarialNetworks,orGANs[6],arecommonlyusedtogenerateartificialimages,videos,
etc. Theintuitionistopittwoneuralnetworksagainsteachother: ageneratorproducesreal-lookingim-
ages while the discriminator tries to distinguish between real and fake images. The process ends when
the discriminator can no longer discern.
2Figure 2: Generative Machine Learning Models. (Source: “Generative Deep Learning,” CC BY 4.0).
Figure 3: Generative GAN-generated, artificial images. (Source: [7])
Besides GANs, there are several other architectures, often based on generative models, used to pro-
duce synthetic data. For instance, Variational Autoencoders try to compress the data to a lower dimen-
sional space and then reconstruct it back to the original. More methods include Restricted Boltzmann
Machines, Bayesian networks, Markov chain Monte Carlo methods, etc.
What Can Synthetic Data Be Used For?
Nowadays, there are a number of companies that market synthetic data technologies, e.g., by Data-
gen, Mostly.ai, Hazy, Gretel.ai, and Aindo. They mention several use cases, including:
1.TrainingMachineLearningModels: syntheticdatacanbeusedtoaugmentrealdata,upsample/rebalance
under-represented classes, or make models more robust to special events, e.g., in the context of
fraud detection [8], healthcare [9], etc.
2.Product and Software Testing: generating synthetic data can be easier than obtaining and using real
rule-based test data to assist with various testing tasks. For example, companies are often unable
to legally use production data for testing purposes.
3.Governance: synthetic data can help remove biases, stress-test models, and increase explainability.
4.Privacy:syntheticdatacanmitigateprivacyconcernswhensharingorusingdataacrossandwithin
organizations. Datasets are considered “anonymous,” “safe,” or void of personally identifiable in-
3formation. This allows data scientists to comply with data protection regulations like HIPAA (in
the US), GDPR (in the EU), or CCPA (in California), etc.
Overall, over the past few years, there have been several initiatives and efforts both in industry and
government. Forexample,theUK’sNationalHealthServicepilotedaprojecttoreleasesyntheticdatafrom
“A&E” (i.e., Emergency Rooms in England) activity data and admitted patient care. In 2018 and 2020,
the US National Institute of Standards and Technology (NIST) ran two challenges related to synthetic
data: the Differential Privacy Synthetic Data and Temporal Map challenges, awarding cash prizes seek-
ing innovative synthetic data algorithms and metrics.
Risks of Using Synthetic Data
Toreasonaroundtherisksofsyntheticdata,researchershaveusedafew“metrics”tomeasureprivacy
properties.
Linkage. Becausesyntheticdatais“artificial,”acommonargumentisthatthereisnodirectlinkbetween
real and synthetic records, unlike anonymized records. Thus, researchers have used similarity tests be-
tweenrealandsyntheticrecordstosupportthesafetyofsyntheticdata. Unfortunately,however,thiskind
ofmetricfailstograsptherealrisksofastrategicadversaryusingfeaturesthatarelikelytobeinfluenced
by the target’s presence.
AttributeDisclosure. Thiskindofprivacyviolationhappenswheneveraccesstodataallowsanattacker
tolearnnewinformationaboutaspecificindividual[10],e.g.,thevalueofaparticularattributelikerace,
age,income,etc. Unfortunately,iftherealdatacontainsstrongcorrelationsbetweenattributes,thesecor-
relationswilllikelybereplicatedinthesyntheticdataandavailabletotheadversary[11]. Furthermore,
Theresa Stadler et al. [11] show that records with rare attributes or whose presence affects the ranges of
numerical attributes remain highly vulnerable to disclosure.
From metrics to attacks. Roughly speaking, linkage is often formulated as a successful membership in-
ferenceattack, whereby an adversary aims to infer if the data from specific target individuals were relied
upon by the synthetic data generation process.
Figure 4: Membership Inference Attack (Source: [12])
Consider the example in Figure 4, where synthetic health images are used for research: discovering that
a specific record was used in a study leaks information about the individual’s health.
4Attributedisclosureisusuallyformulatedasan attribute/propertyinference attack. Here,theadversary,
given some public information, tries to reconstruct some private attributes of some target users.
Figure 5: Attribute Inference Attack.
How realistic are the attacks? One important consideration that applies to most privacy studies is that
they do not provide “binary” answers, e.g., making definite statements of whether some method pro-
vides perfect privacy or none at all. Rather, they provide probability distributions vis-à-vis different
systems/threat models, adversarial assumptions, datasets, etc.
However, there still are a significant number of gaps identified by state-of-the-art research studies.
In practice, synthetic data provides little additional protection compared to anonymization techniques,
with privacy-utility trade-offs being even harder to predict [11].
Differentially Private Synthetic Data Generation. The state-of-the-art method for providing access to
information free from inferences is to satisfy differential privacy [4].
In the context of synthetic data, the intuition is to train the generative models used to produce syn-
thetic data in a differentially private manner. Typically, one of three methods is used: using the Laplace
mechanism,sanitizingthegradientsduringstochasticgradientdescent,orusingatechniquecalledPATE,
whichtransferstoa“student”modeltheknowledgeofanensembleof“teacher”models(privacyispro-
videdbytrainingteachersondisjointdataandnoisyaggregationofteachers’answers)[13]. Theresult-
ing methods tend to combine generative model architectures with differential privacy; state-of-the-art
tools include DP-GAN [14], DP-WGAN [15], DP-Syn [16], PrivBayes [17], PATE-GAN [18], etc. A list
of relevant papers (with code) is available on Georgi Ganev’s GitHub page.
The Inherent Security and Privacy Limitations
While there likely are other challenges related to synthetic data (e.g., regarding usability, fidelity,
and interpretability), we focus on security and privacy once. In particular, with respect to privacy, we
believe it unlikelythat synthetic data will provide a silver bullet tosanitize sensitive data or safelyshare
confidential information across the board. Instead, there could be specific use cases where training a
generative model provides better flexibility and privacy protection than the alternatives. For instance,
financialcompaniescanusesyntheticdatatoensureproductiondataisnotusedduringtestingorshared
across different sub-organizations. Also, governmental agencies could enable citizens and entities to
extract high-level statistics from certain data distributions.
5However,wearguethatthosecasestudiesarenotgoingtogeneralize. Essentially,generativemodels
trainedwithoutdifferentialprivacy(orwithverylargeprivacybudgets)donotprovidehighsafety,pri-
vacy,orconfidentialitylevels. Conversely,differentiallyprivatesyntheticdatagenerationalgorithmscan
but with a non-negligible cost to utility/accuracy. More precisely, protecting privacy inherently means
you must “hide” vulnerable data points like outliers, etc. So if one tries to use synthetic data to upsam-
pleanunder-representedclass,trainafraud/anomalydetectionmodel,etc.,theywillinherentlyneedto
choose between eitherprivacyorutility.
Another limitation is that usable privacy mechanisms must be predictable , i.e., enabling reliable as-
sumptionsofhowpartieswillaccessandusepersonallyidentifiableinformation[19]. Thatisnotalways
thecasewithsyntheticdata,becauseoftheprobabilisticnatureofgenerativemodelsandtheinherentdif-
ficultyofpredictingwhatsignalsasyntheticdatasetwillpreserveandwhatinformationwillbelost[11].
Looking Ahead
Inconclusion,webelievethereareseveralinterestingresearchquestionsthatthecommunityshould
be focusing on in the context of synthetic data:
1. While differential privacy often provides an overly conservative, worst-case approach to privacy,
it allows abstracting away from any adversarial assumption. But in practice, the accuracy of the
attacks we can realistically mount is measurably far from the theoretical bounds, which motivates
the need for more work on auditing differentially private synthetic data generation algorithms.
2. We call for the privacy engineering community to help practitioners and stakeholders identify the
use cases where synthetic data can be used safely, perhaps even in a semi-automated way.
3. Researchers should also be incentivized to provide actionable guidelines to understand the dis-
tributions, types of data, tasks, and settings, where one could achieve reasonable privacy-utility
tradeoffs via synthetic data.
Acknowledgements
We wish to thank Georgi Ganev, Bristena Oprisanu, and Meenatchi Sundaram Muthu Selva Annamalai
for reviewing a draft of this article.
References
[1] Arvind Narayanan and Vitaly Shmatikov. De-anonymizing social networks. In IEEE S&P , 2009.
[2] Apostolos Pyrgelis. On Location, Time, and Membership: Studying How Aggregate
Location Data Can Harm Users’ Privacy. https://www.benthamsgaze.org/2018/10/02/
on-location-time-and-membership-studying-how-aggregate-location-data-can-harm-users-privacy/,
2018.
[3] ApostolosPyrgelis,CarmelaTroncoso,andEmilianoDeCristofaro.KnockKnock,Who’sThere? Membership
Inference on Aggregate Location Data. In NDSS, 2018.
[4] Kobbi Nissim, Thomas Steinke, Alexandra Wood, Micah Altman, Aaron Bembenek, Mark Bun, Marco
Gaboardi,DavidRO’Brien,andSalilVadhan. Differentialprivacy: Aprimerforanon-technicalaudience. In
Privacy Law Scholars Conference , 2017.
[5] Derek Snow. Deep Generative Models are Privacy Regularisers. https://blog.ml-quant.com/p/
deep-generative-models-are-privacy, 2021.
[6] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. Generative Adversarial Nets. In NIPS, 2014.
[7] TeroKarras,TimoAila,SamuliLaine,andJaakkoLehtinen.Progressivegrowingofgansforimprovedquality,
stability, and variation. arXiv:1710.10196 , 2017.
6[8] Mostly.ai. Synthetic training data for improving fraud and anomaly AI’s performance. https://mostly.ai/
case-study/synthetic-training-data-for-machine-learning-fraud-detection/, 2023.
[9] Allan Tucker, Zhenchen Wang, Ylenia Rotalinti, and Puja Myles. Generating high-fidelity synthetic patient
data for assessing machine learning healthcare software. NPJ digital medicine , 3(1), 2020.
[10] Markus Hittmeir, Rudolf Mayer, and Andreas Ekelhart. A baseline for attribute disclosure risk in synthetic
data. InACM CODASPY , 2020.
[11] Theresa Stadler, Bristena Oprisanu, and Carmela Troncoso. Synthetic data–anonymisation groundhog day.
InUSENIX Security Symposium , 2022.
[12] Ziqi Zhang, Chao Yan, and Bradley A Malin. Membership inference attacks against synthetic health data.
Journal of biomedical informatics , 125, 2022.
[13] Nicolas Papernot, Shuang Song, Ilya Mironov, Ananth Raghunathan, Kunal Talwar, and Úlfar Erlingsson.
Scalable private learning with PATE. arXiv:1802.08908 , 2018.
[14] LiyangXie,KaixiangLin,ShuWang,FeiWang,andJiayuZhou. Differentiallyprivategenerativeadversarial
network. arXiv:1802.06739 , 2018.
[15] MoustafaAlzantotandManiSrivastava. DifferentialPrivacySyntheticDataGenerationusingWGANs. https:
//github.com/nesl/nist_differential_privacy_synthetic_data_challenge/, 2019.
[16] Ninghui Li, Zhikun Zhang, and Tianhao Wang. DPSyn: Experiences in the nist differential privacy data
synthesis challenges. arXiv:2106.12949 , 2021.
[17] JunZhang,GrahamCormode,CeciliaM.Procopiuc,DiveshSrivastava,andXiaokuiXiao. PrivBayes: Private
Data Release via Bayesian Networks. ACM Transactions on Database Systems , 2017.
[18] James Jordon, Jinsung Yoon, and Mihaela Van Der Schaar. PATE-GAN: Generating synthetic data with dif-
ferential privacy guarantees. In ICLR, 2018.
[19] European Data Protection Supervisor (EDPS). Opinion 5/2018-Preliminary Opinion on privacy by design.
2018.
7