Quantum Computing Provides Exponential Regret
Improvement in Episodic Reinforcement Learning
Bhargav Ganguly1, Yulian Wu2, Di Wang2, and Vaneet Aggarwal1;2
1Purdue University, West Lafayette, IN 47907, USA
2KAUST, Thuwal 23955, KSA
bganguly@purdue.edu, yulian.wu@kaust.edu.sa, di.wang@kaust.edu.sa, vaneet@purdue.edu
Abstract
In this paper, we investigate the problem of episodic reinforcement learning with quantum
oracles for state evolution. To this end, we propose an Upper Condence Bound (UCB)
based quantum algorithmic framework to facilitate learning of a nite-horizon MDP. Our
quantum algorithm achieves an exponential improvement in regret as compared to the
classical counterparts, achieving a regret of ~O(1) as compared to ~O(p
K)1,Kbeing the
number of training episodes. In order to achieve this advantage, we exploit ecient quan-
tum mean estimation technique that provides quadratic improvement in the number of i.i.d.
samples needed to estimate the mean of sub-Gaussian random variables as compared to
classical mean estimation. This improvement is a key to the signicant regret improvement
in quantum reinforcement learning. We provide proof-of-concept experiments on various
RL environments that in turn demonstrate performance gains of the proposed algorithmic
framework.
1. Introduction
Quantum Machine Learning (QML) is an emerging domain built at the conuence of quan-
tum information processing and Machine Learning (ML) (Saggio et al., 2021). A noteworthy
volume of prior works in QML demonstrate how quantum computers could be eectively
leveraged to improve upon classical results pertaining to classication/regression based pre-
dictive modeling tasks (A meur et al., 2013; Rebentrost et al., 2014; Arunachalam and
de Wolf, 2017). While the eciency of QML frameworks have been shown on conventional
supervised/unsupervised ML use cases, how similar improvements could be translated to
Reinforcement Learning (RL) tasks have gained signicant attention recently and is the
focus of this paper.
Traditional RL tasks comprise of an agent interacting with an external environment
attempting to learn its congurations, while collecting rewards via actions and state transi-
tions (Sutton and Barto, 2018). RL techniques have been credibly deployed at scale over a
variety of agent driven decision making industry use cases, e.g., autonomous navigation in
self-driving cars (Al-Abbasi et al., 2019), recommendation systems in e-commerce websites
(Rohde et al., 2018), and online gameplay agents such as AlphaGo (Silver et al., 2017).
Given the wide applications, this paper aims to study if quantum computing can help fur-
1.~O() hides logarithmic terms.
1arXiv:2302.08617v1  [cs.LG]  16 Feb 2023ther improve the performance of reinforcement learning algorithms. This paper considers an
episodic setup, where the learning occurs in episodes with a nite horizon. The performance
measure for algorithm design is the regret of agent's rewards (Mnih et al., 2016; Cai et al.,
2020), which measures the gap in the obtained rewards by the Algorithm and the optimal
algorithm. A central idea in RL algorithms is the notion of exploration-exploitation trade-
o, where agent's policy is partly constructed on its experiences so far with the environment
as well as injecting a certain amount of optimism to facilitate exploring sparsely observed
policy congurations (Kearns and Singh, 2002; Jin et al., 2020). In this context, we empha-
size that our work adopts the well-known Value Iteration (VI) technique which combines
empirically updating state-action policy model with Upper Condence Bound (UCB) based
strategic exploration (Azar et al., 2017).
In this paper, we show an exponential improvement in the regret of reinforcement learn-
ing. The key to this improvement is that quantum computing allows for improved mean
estimation results over classical algorithms (Brassard et al., 2002; Hamoudi, 2021). Such
mean estimators feature in very recent studies of quantum bandits (Wang et al., 2021b),
quantum reinforcement learning (Wang et al., 2021a), thereby leading to noteworthy conver-
gence gains. In our proposed framework, we specically incorporate a quantum information
processing technique that improves non-asymptotic bounds of conventional empirical mean
estimators which was rst demonstrated in (Hamoudi, 2021). In this regard, it is worth
noting that a crucial novelty pertaining to this work is carefully engineering agent's interac-
tion with the environment in terms of collecting classical and quantum signals. We further
note that one of the key aspects of analyzing reinforcement learning algorithms is the use of
Martingale convergence theorems, which is incorporated through the stochastic process in
the system evolution. Since there is no result on improved Martingale convergence results in
quantum computing so far (to the best of our knowledge), this work does a careful analysis
to approach the result without use of Martingale convergence results.
Given the aforementioned quantum setup in place, this paper attempts to address the
following: Can we design a quantum VI Algorithm that can improve classical regret bounds
in episodic RL setting?
This paper answers the question in positive. The key to achieve such quantum advantage
is the use of quantum environment that provides more information than just an observation
of the next state. This enhanced information is used with quantum computing techniques
to obtain ecient reget bounds in this paper.
To this end, we summarize the major contributions of our work as follows:
1. We present a novel quantum RL architecture that helps exploit the quantum advan-
tage in episodic reinforcement learning.
2. We propose QUCB-VI, which builds on the classical UCB-VI algorithm (Azar et al.,
2017), wherein we carefully leverage available quantum information and quantum
mean estimation techniques to engineer computation of agent's policy.
3. We perform rigorous theoretical analysis of the proposed framework and character-
ize its performance in terms of regret accumulated across Kepisodes of agent's
interaction with the unknown Markov Decision Process (MDP) environment. More
specically, we show that QUCB-VI incurs ~O(1) regret. We note that our algorithm
2provides a faster convergence rate in comparison to classical UCB-VI which accumu-
lates ~O(p
K) regret, where Kis the number of training episodes.
4. We conduct thorough experimental analysis of QUCB-VI (algorithm 1) and compare
against baseline classical UCB-VI algorithm on a variety of benchmark RL environ-
ments. Our experimental results reveals QUCB-VI's performance improvements in
terms of regret growth over baseline.
The rest of the paper is organized as follows. In Section 2, we present a brief back-
ground of key existing literature pertaining to classical RL, as well as discuss prior re-
search conducted in development of quantum mean estimation techniques and quantum RL
methodologies relevant to our work. In Section 3, we mathematically formulate the problem
of episodic RL in a nite horizon unknown MDP with the use of quantum oracles in the
environment. In Section 4, we describe the proposed QUCB-VI Algorithm while bringing
out key dierences involving agent's policy computations as compared to classical UCB-VI.
Subsequently, we provide the formal analysis of regret for the proposed algorithm in Section
5. In Section 6, we report our results of experimental evaluations performed on various RL
environments for the proposed algorithm and classical baseline method. Section 7 concludes
the paper.
2. Background and Related Work
Classical reinforcement learning: In the context of classical RL, an appreciable segment
of prior research focus on obtaining theoretical results in tabular RL, i.e., agent's state
and action spaces are discrete (Sutton and Barto, 2018). Several existing methodologies
guarantee sub-linear regret in this setting via leveraging optimism in the face of uncertainty
(OFU) principle (Lai et al., 1985), to strategically balance exploration-exploitation trade-o
(Osband et al., 2016; Strehl et al., 2006). Furthermore, on the basis of design requirements
and problem specic use cases, such algorithms have been mainly categorized as either
model-based (Auer et al., 2008; Dann et al., 2017) or model-free (Jin et al., 2018; Du et al.,
2019). In the episodic tabular RL problem setup, the optimal regret of~O(p
K) (Kis the
number of episodes) have been studied for both model-based as well as model-free learning
frameworks (Azar et al., 2017; Jin et al., 2018). In this paper, we study model-based
algorithms and derive ~O(1) regret with the use of quantum environment.
Quantum Mean Estimation: Mean estimation is a statistical inference problem in
which samples are used to produce an estimate of the mean of an unknown distribution.
The improvement in sample complexity for mean estimation using quantum computing has
been widely studied (Grover, 1998; Brassard et al., 2002, 2011). In (Montanaro, 2015),
a quantum information assisted Monte-Carlo Algorithm was proposed which achieves an
asymptotic near-quadratic faster convergence over its classical baseline. In this paper, we
use the approach in (Hamoudi, 2021) for the mean estimation of quantum random variables.
We rst describe the notion of random variable and corresponding extension to quantum
random variable.
Denition 1 (Random Variable) A nite random variable is a function X: 
!Efor
some probability space (
;P), where 
is a nite sample set, P: 
![0;1]is a probability
3mass function and ERis the support of X. As is customary, we will often omit to
mention (
;P)when referring to the random variable X.
The notion is extended to a quantum random variable (or q.r.v.) as follows.
Denition 2 (Quantum Random Variable) A q.r.v. is a triple (H;U;M )whereHis
a nite-dimensional Hilbert space, Uis a unitary transformation on H, andM=fMxgx2E
is a projective measurement on Hindexed by a nite set ER. Given a random variable
Xon a probability space (
;P), we say that a q-variable (H;U;M )generatesXwhen,
(1)His a nite-dimensional Hilbert space with some basis fj!ig!2
indexed by 
.
(2)Uis a unitary transformation on Hsuch thatUj0i=P
!2
p
P(!)j!i.
(3)M=fMxgxis the projective measurement on Hdened byMx=P
!:X(!)=xj!ih!j.
We now dene the notion of a quantum experiment. Let ( H;U;M ) be a q.r.v. that
generatesX. With abuse of notations, we call Xas the q.r.v. even though the actual
q.r.v. is the (H;U;M ) that generates X. We dene a quantum experiment as the process
of applying any of the unitaries U, their inverses or their controlled versions, or performing
a measurement according to M. We also assume an access to the quantum evaluation oracle
j!ij0i!j!ijX(!)i. Using this quantum oracle, the quantum mean estimation result can
be stated as follows.
Lemma 1 (Sub-Gaussian estimator (Hamoudi, 2021)) LetXbe a q.r.v. with mean
and variance 2. Givenni.i.d. samples of q.r.v. Xand a real 2(0;1)such that
n > log(1=), a quantum algorithm SubGaussEst (X;n; )(please refer to algorithm 2 in
(Hamoudi, 2021)) outputs a mean estimate ^such that,
P
j^ jlog(1=)
n
1 : (1)
The algorithm performs O(nlog3=2(n) log log(n))quantum experiments.
We note that this result achieves the mean estimation error of 1 =nin contrast to 1 =pnfor
the classical mean estimation, thus providing a quadratic reduction in the number of i.i.d.
samples needed for same error bound.
Quantum reinforcement learning: Recently, quantum mean estimation techniques
have been applied with favorable theoretical convergence speed-ups for Quantum multi-
armed bandits (MAB) problem setting (Casal e et al., 2020; Wang et al., 2021b; Lumbreras
et al., 2022). However, bandits do not have the notion of state evolution like in reinforcement
learning. Further, quantum reinforcement learning has been studied in (Paparo et al.,
2014; Dunjko et al., 2016, 2017; Jerbi et al., 2021; Dong et al., 2008), while these works
do not study the regret performance. The theoretical regret performance has been recently
studied in (Wang et al., 2021b), where a generative model is assumed and sample complexity
guarantees are derived for discounted innite horizon setup. In contrast, our work does not
consider discounted case, and we don't assume a generative model. This paper demonstrates
the quantum speedup for episodic reinforcement learning.
4Figure 1: Quantum episodic reinforcement learning architecture depicting agent-
environment interaction at round h.
3. Problem Formulation
We consider episodic reinforcement learning in a nite horizon Markov Decision Process
(Agarwal et al., 2019) given by a tuple 
S;A;H;fPhgh2[0;H 1];frhgh2[0;H 1]
, whereSand
Aare the state and the action spaces with cardinalities SandA, respectively, H2Nis
the episode length, Ph(s0js;a)2[0;1] is the probability of transitioning to state s0from
statesprovided action ais taken at step handrh(s;a)2[0;1] is the immediate reward
associated with taking action ain statesat steph. In our setting, we denote an episode
by the notation k, and every such episode comprises of Hrounds of agent's interaction
with the learning environment. In our problem setting, we assume an MDP with a xed
start states0, where at the start of each new episode the state is reset to s0. We note that
the results can be easily extended to the case where starting state is sampled from some
distribution. This is because we can have a dummy state s0which transitions to the next
states1coming from this distribution, independent of action, and having a reward of 0.
We encapsulate agent's interaction with the unknown MDP environment via the archi-
tecture as presented in Fig. 1. At an arbitrary time step h, given a state shand action ah,
the environment gives the reward rhand next state sh+1. Furthermore, we highlight that in
our proposed architecture this set of signals i.e., fsh;ah;rh;sh+1gare collected at the agent
asclassical information. Additionally, our architecture facilitates availability of Squantum
random variables (q.r.v.) (X1;h;;XS;h) at the agent's end, wherein q.r.v. Xi;hgenerates
the random variable 1fsh+1=ig. This q.r.v. corresponds to the Hilbert space with basis
vectorsj0iandj1iand the unitary transformation, given as follows:
Uj0i=p
1 Ph(sh+1=ijsh; ah)j0i
+p
Ph(sh+1=ijsh; ah)j1i: (2)
We note that these q.r.v.'s can be generated by using a quantum next state from the
environment which is given in form of the basis vectors for Sstates asj100>with
Squbits for rst state and so on till j001>for the last state. Thus, the over-
all quantum next state is the superposition of these Sstates with the amplitudes asp
Ph(sh+1= 1jsh; ah);;p
Ph(sh+1=Sjsh; ah), respectively. The Sq.r.v.'s correspond
to theSqubits in this next quantum state. Further, the next state can be obtained as
a measurement of this joint next state superposition. Thus, assuming that the quantum
5environment can generate multiple copies of the next state superposition, all the Sq.r.v.'s
and the next state measurement can be obtained.
We note that the agent does not know fPhgh2[0;H 1], which needs to be estimated in
the model-based setup. In order to estimate this, we will use the quantum mean estimation
approach. This approach needs as quantum evaluation oracle j!ij0i!j!ij1fsh+1=!gi
for!2f0;1g. In Section 4, we provide more details on how the aforementioned set of
quantum indicator variables i.e., fXs0;hgs02Sare fed to a specic quantum mean estimation
procedure to obtain the transition probability model.
Based on the observations, the agent needs to determine a policy hwhich determines
actionahgiven state sh. For given policy andh2f0;;H 1g, we dene the value
functionV
h:S!Ras
V
h(s) =E"H 1X
t=hrh(st;at)j; sh=s#
; (3)
where the expectation is with respect to the randomness of the trajectory, that is, the
randomness in state transitions and the stochasticity of . Similarly, the state-action value
(orQ-value) function Q
h:SA! Ris dened as
Q
h(s;a) =E"H 1X
t=hrh(st;at)j;sh=s;ah=a#
: (4)
We also use the notation V(s) =V
0(s). Given a state s, the goal of the agent is to nd a
policythat maximizes the value, i.e., the optimization problem the agent seeks to solve
is:
max
V(s): (5)
DeneQ?
h(s;a) = sup2Q
h(s;a) andV?
h(s) = sup2V
h(s). The agent aims to minimize
the expected cumulative regret incurred across Kepisodes:
Regret: E
KV(s0) K 1X
k=0H 1X
h=0r(sk
h;ak
h)
: (6)
In the following section, we describe the proposed algorithm and analyze its regret in Section
5.
4. Algorithmic Framework
In this section, we describe in detail our quantum information assisted algorithmic frame-
work to perform learning of unknown MDP under nite horizon episodic RL setting. In
particular, we propose a quantum algorithm that incorporates model-based episodic RL pro-
cedure originally proposed in the classical setting (Azar et al., 2017; Agarwal et al., 2019).
In algorithm 1, we present Quantum Upper Condence Bound -Value Iteration (QUCB-
VI) Algorithm which takes number of episodes K, length of an episode Hand condence
6Algorithm 1 QUCB-VI
1:Inputs:K,H,2(0;1].
2:SetN1
h(s;a) 0;8s2S; a2A; h2[0;H 1]).
3:fork= 1;:::;K do
4: SetbVk
H(s) 0;8s2S.
5: forh=H 1;:::; 1;0do.
6: Setf^Pk
h(s0js;a)gs;a;s0)2SAS via Eq. (9).
7: Setbk
h(s;a) log(SAHK= )
Nk
h(s;a).
8: SetfbQk
h(s;a)gs2S;a2Avia Eq. (10).
9: SetfbVk
h(s)gs2Svia Eq. (11).
10: end for
11: Set policiesfk
h(s)gs2S;h2[0;H 1]using Eq. (12).
12: Get Trajectoryfsk
h;ak
h;rk
h;sk
h+1gH 1
h=0viafk
hgH 1
h=0.
13: SetfNk+1
h(s;a)gs2S;a2A;h2[0;H 1]via Eq. (8).
14:end for
parameteras inputs. In the very rst step, the count of visitations corresponding to every
state-action pair ( s;a)2SA are initialized to 0. Subsequently, at the beginning of each
episodeK, the value function estimates for the entire state-space S, i.e.,fbVk
H(s)gs2Sare
set to 0.
Next, for each time instant up to H 1, we update the transition probability model.
Here, we utilize the set of quantum random variable (q.r.v.)fXk
s0;hgs02Sas introduced in
our quantum RL architecture presented in Figure 1. Recall that the elements of fXk
s0;hgs02S
are dened at each time step hduring episode kas follows:
Xk
s0;h,1[sk
h+1=s0]: (7)
We dene Nk
h(s;a) as the number of times ( s;a) is visited before episode k. More
formally, we have
Nk
h(s;a) =k 1X
i=01
(si
h;ai
h) = (s;a)
: (8)
Nk
h(s;a) indicates the number of samples obtained for ( s;a) in the past which will
help in ecient averaging to estimate the transition probabilities. With the formulation
of q.r.v.fXk
s0;hgs02Sin place, we update the transition probability model elements, i.e.,
f^Pk
h(s0js;a)g(s;a;s0)2SAS , in step 6 of algorithm 1. To get the estimate of ^Pk
h(s0js;a), we
use theNk
h(s;a) q.r.v.'sXk
s0;hfor all past k. Thus, ^Pk
h(s0js;a) is estimated as:
^Pk
h(s0js;a) SubGaussEst (Xk
s0;h;Nk
h(s;a);); (9)
where subroutine SubGaussEst as presented in Algorithm 2 of (Hamoudi, 2021) performs
mean estimation of q.r.v. Xk
s0;hgivenNk
h(s;a) collection of samples and condence param-
eter. We emphasize that step 6 in Algorithm 1 brings out the key change w.r.t. classical
7UCB-VI via carefully estimating mean of quantum information collected by agent via in-
teraction with the unknown MDP environment. In step 7, we set the reward bonus i.e.,
bk
h(s;a) which resembles a Bernstein-style UCB bonus, essentially inducing optimism in the
learnt model. Consequently, step 8-9 compute estimates of fQk
h(s;a)gs2S;a2A;fbVk
h(s)gs2S
by adopting the following Value Iteration based updates at time step h:
bQk
h(s;a) minfH;rk
h(s;a) +hbVk
h+1;bPk
h(js;a)i
+bk
h(s;a)g; (10)
bVk
h(s) max
a2AbQk
h(s;a): (11)
This Value Iteration procedure (i.e., inner loop consisting of steps 6-9) is executed for H
time steps thereby generating a collection of Hpoliciesfk
h(s)gs2S;h2[0;H 1]calculated for
each pair of ( s;h) in step 11 as:
k
h(s) arg max
a2AbQk
h(s;a); (12)
Next, using the updated policies i.e., fk
h(s)gs2S;h2[0;H 1]which are based on observations
recorded till episode k 1, the agent collects a new trajectory of Htuples pertaining to
episodeki.e.,fsk
h;ak
h;rk
h;sk
h+1gH 1
h=0in step 12 starting from initial state reset to s0. Finally,
the frequency of agent's visitation to all state action pairs at every time step hover thek
episodes i.e.,fNk+1
h(s;a)gs2S;a2A;h2[0;H 1]are updated in step 13. Consequently, algorithm
1 triggers a new episode of agent's interaction with the unknown MDP environment.
5. Regret Results for the Proposed Algorithm
5.1 Main Result: Regret Bound for QUCB-VI
In Theorem 1, we present the cumulative regret collected upon deploying QUCB-VI in an
unknown MDP environment M(please refer to Section 3 for denition of MDP) over a
nite horizon of Kepisodes.
Theorem 1 In an unknown MDP environment M, 
S;A;H;fPhgh2[0;H 1];frhgh2[0;H 1]
,
the regret incurred by QUCB-VI (algorithm 1) across Kepisodes is bounded as follows:
EhK 1X
k=0 
V(s0) Vk(s0)i
O(H2S2Alog2(SAH2K2)): (13)
The result obtained via Eq. (13) in Theorem 1 brings out the key advantage of the
proposed framework in terms of accelerating the regret convergence rate to ~O(1) against
the classical result of ~O(p
K) (Azar et al., 2017).
In order to prove Theorem 1, we present the following auxiliary mathematical results
in the ensuing subsections: bound for probability transition model error pertaining to ev-
ery state-action pair (section 5.2); optimism exhibited by the learnt model understood in
terms of value functions of the states (section 5.3); a supporting result that bounds inverse
frequencies of state-action pairs over the entire observed trajectory (section 5.4). Subse-
quently, we utilize these aforementioned theoretical results to prove Theorem 1 in section
5.5.
85.2 Probability Transition Model Error for state-action pairs
Lemma 2 Fork2f0;:::;K 1g,s2S,a2A,h2f0;:::;H 1g, for anyf:S! [0;H],
execution of QUCB-VI (algorithm 1) guarantees that the following holds with probability at
least 1 :

^Pk
h(js;a) Ph(js;a)T
fHSL
Nk
h(s;a); (14)
whereL,log(SAHK= )andfNk
h(s;a),^Pk
h(s0js;a)gare dened in Eq. (8),(9).
Proof To prove the claim in Eq. (14), we consider an arbitrary tuple ( s;a;k;h;f ) and
obtain the following:

^Pk
h(js;a) Ph(js;a)T
f
X
s02Sf(s0)j^Pk
h(s0js;a) Ph(s0js;a)j; (15)
HX
s02Sj^Pk
h(s0js;a) Ph(s0js;a)|{z}
(a)j; (16)
where Eq. (16) is due to the denition of f() as presented in the lemma statement. Next,
in order to analyze (a) in Eq. (16), we note the denition of Xk
s0;hpresented in Eq. (7) as
anindicator q.r.v allows us to write:
^Pk
h(s0js;a) =Eq;^Pk
h[Xk
s0;hjs;a]; (17)
Ph(s0js;a) =Eq;Ph[Xk
s0;hjs;a]: (18)
Using the fact that Xk
s0;his a q.r.v. allows us to directly apply Lemma 1, thereby further
bounding (a) in Eq. (16) with probability at least 1  as follows:
j^Pk
h(s0js;a) Ph(s0js;a)j
log(1=)
Nk
h(s;a); (19)
log(SAHK= )
Nk
h(s;a)=L
Nk
h(s;a); (20)
where, we emphasize that Eq. (20) is a consequence of applying union-bound 8s;a;h;k as
well as we use the denition of Lprovided in the lemma statement. Plugging the bound of
term (a) as obtained in Eq. (20) back into Eq. (16), we obtain:

^Pk
h(js;a) Ph(js;a)T
fHX
s02SL
Nk
h(s;a); (21)
HSL
Nk
h(s;a); (22)
which proves the claim of the Lemma.
9Interpretation of Lemma 2: One of the key insights that we draw from this lemma is that
the quantum mean estimation with q.r.v. Xk
s0;hallowed the use of Lemma 1, which facilitated
quadratic speed-up of transition probability model convergence. More specically, our result
suggests transition probability model error diminishes with ~O(1
Nk
h(s;a)) speed as opposed to
the classical results of ~O(1p
Nk
h(s;a)) (Azar et al., 2017; Agarwal et al., 2019).
5.3 Optimistic behavior of QUCB-VI
Lemma 3 Assume that the event described in Lemma 2 is true. Then, the following holds
8k:
bVk
0(s)V
0(s);8s2S; (23)
wherebVk
0(s)is calculated via our QUCB-VI Algorithm and V
h:S! [0;H].
Proof To prove the lemma statement, we proceed via mathematical induction. Firstly, we
highlight that the following holds at time step H:
^Vk
H(s) =V
H(s) = 0;8s2S; (24)
In the next step, assume that ^Vk
h+1(s)V
h+1(s). If ^Qk
h(s;a) =H, then ^Qk
h(s;a)Q
h(s;a)
sinceQ
h(s;a) can be atmost H. Otherwise, at time step h, we obtain:
^Qk
h(s;a) Q
h(s;a)
=bk
h(s;a) +h^Pk
h(js;a);^Vk
hi hPh(js;a);V
hi; (25)
bk
h(s;a) +h^Pk
h(js;a);V
hi hPh(js;a);V
hi; (26)
=bk
h(s;a)
+X
s02S ^Pk
h(s0js;a) Ph(s0js;a)
V
h(s0); (27)
bk
h(s;a) HSL
Nk
h(s;a); (28)
= 0; (29)
where Eq. (26) is due to the induction assumption. Furthermore, Eq. (28), (29) are owed
to Lemma 2 and denition of bonus in step 7 of Algorithm 1, respectively. Hence, we have
^Qk
h(s;a)Q
h(s;a). Using Value Iteration computations in Eq. (10) - (11), we obtain
^Vk
h(s)V
h(s);8h.
This completes the proof.
Interpretation of Lemma 3: This Lemma reveals that QUCB-VI (algorithm 1) outputs
estimates of the value function which are always lower bounded by the true value at each
time step, thereby exhibiting similar optimistic behavior as the classical UCB-VI algorithm.
Interestingly, faster convergence properties of QUCB-VI's transition model (i.e., Lemma 2)
complemented the usage of a sharper bonus term (i.e., bk
h(s;a) dened in algorithm 1)
instead of the bonus terms of the classical algorithm, while keeping the optimism behavior
of the model intact.
105.4 Trajectory Summation Bound Characterization
Next, we present a technical result bounding inverse of observed state-action pair frequencies
over agent's trajectory collected across all the episodes in Lemma 4.
Lemma 4 Assume an arbitrary sequence of trajectories fsk
h;ak
hgH 1
h=0fork= 0;;K 1.
Then, the following result holds:
K 1X
k=0H 1X
h=01
Nk
h(sk
h;ak
h)HSA log(K): (30)
Proof We change order of summations to obtain:
K 1X
k=0H 1X
h=01
Nk
h(sk
h;ak
h)
=H 1X
h=0K 1X
k=01
Nk
h(sk
h;ak
h); (31)
=H 1X
h=0X
(s;a)2SANK
h(s;a)X
i=11
i; (32)
H 1X
h=0X
(s;a)2SAlog(NK
h(s;a)); (33)
HSA maxSA [0;H 1]log(NK
h(s;a)); (34)
HSA log(K); (35)
where Eq. (33) is due to the fact thatPN
i=11=ilog(i).
This completes the proof of the lemma statement.
115.5 Proof of Theorem 1
To prove Theorem 1, we rst note that the following holds for episode k:
V(s0) Vk(s0)
^Vk
0(s0) Vk
0(s0); (36)
=^Qk
0(s0;k(s0)) Qk
0(s0;k(s0)); (37)
rk
0(s0;k(s0)) +bk
0(s0;k(s0))
+hbVk
1;bPk
0
js0;k(s0)
i rk
0(s0;k(s0))
 hVk
1;P0
js0;k(s0)
i; (38)
=bk
0(s0;k(s0)) +hbVk
1;bPk
0
js0;k(s0)
i
 hVk
1;P0
js0;k(s0)
i; (39)
=bk
0(s0;k(s0))
+hbVk
1;bPk
0
js0;k(s0)
 P0
js0;k(s0)
i
+hbVk
1 Vk
1;P0
js0;k(s0)
i
|{z }
(a)(40)
=H 1X
h=0Esh;ahdk
hh
bk
h(sh;ah)
+h^Pk
h(jsh;ah) Ph(jsh;ah);^Vk
h+1i| {z }
(b)i
; (41)
where Eq. (36) is due to Lemma 3. Eq. (38) uses the denition of Qfunction in Eq.
(10) and the fact that min fa;bga. In Eq. (40), we identify that term (a) is the 1-step
recursion of RHS of Eq. (36). Consequently, we obtain Eq. (41) where the expectation is
w.r.t. the intermediate trajectories dk
hgenerated via policies fk
igh 1
i=0.
Let us denote the event described in Lemma 2 as E. Formally,Eis described as follows
for some2[0;1]:
E,1h
j^Pk
h(s0js;a) Ph(s0js;a)jlog(1=)
Nk
h(s;a);8s;a;s0i
:
Then, assuming that Eis true, term (b) in Eq. (41) can be further bounded as follows:
h^Pk
h(jsh;ah) Ph(jsh;ah);^Vk
h+1i
k^Pk
h(jsh;ah) Ph(jsh;ah)k1k^Vk
h+1k1; (42)
HSL
Nk
h(s;a); (43)
12where Eq. (42), (43) are consequences of Holder's inequality and Lemma 2 respectively. By
plugging the bound for term (a) as obtained in Eq. (43) back into RHS of Eq. (41), we
have:
V(s0) Vk(s0)
H 1X
h=0Esh;ahdk
hh
bk
h(sh;ah) +HSL
Nk
h(s;a)i
; (44)
H 1X
h=0Esh;ahdk
hh
2HSL
Nk
h(s;a)i
; (45)
= 2HSLEhH 1X
h=01
Nk
h(s;a)H<ki
; (46)
where Eq. (45) is owed to the denition of bonus bk
hin algorithm 1. Further, in Eq. (46),
the expectation is w.r.t. trajectory fsk
h;ak
hggenerated via policy kwhile conditioning on
history collected till end of episode k 1, i.e.,H<k. Summing up across all the episodes
and taking into account success/failure of event E, we obtain the following:
EhK 1X
k=0V(s0) Vk(s0)i
=Eh
1[E]K 1X
k=0V(s0) Vk(s0)i
+Eh
1[E]K 1X
k=0V(s0) Vk(s0)i
; (47)
Eh
1[E]K 1X
k=0V(s0) Vk(s0)i
+ 2KH; (48)
2HSLEhK 1X
k=0H 1X
h=01
Nk
h(s;a)i
+ 2KH; (49)
2H2S2ALlog(K) + 2KH; (50)
where Eq. (48) is owed to the facts that value functions fV;Vkgare bounded by Hand
the failure probability is at most . Next, we obtain Eq. (49) by leveraging Eq. (46) when
eventEis successful. Eq. (50) is a direct consequence of Lemma 4.
By setting, = 1=KH , we obtain:
EhK 1X
k=0V(s0) Vk(s0)i
2H2S2Alog2(SAH2K2) + 2; (51)
=O(H2S2Alog2(SAH2K2)): (52)
13(a)Riverswim-6. S= 6; A= 2:
 (b)Riverswim-12. S= 12; A= 2:
(c)Grid-world. S= 20; A= 4:
Figure 2: Cumulative Regret incurred by QUCB-VI (algorithm 1) and UCB-VI algorithm
for various RL environments.
This completes the proof of the theorem.
The choice of improved reward bonus term manifests itself in Eq. (44)-(46) and plays
a signicant role towards QUCB-VI's overall regret improvement. Further, we note that
the Martingale style proof approach and the corresponding Azuma Hoeding's inequality,
which are typical in the regret bound proof of classical UCB-VI, are not used in the analysis
of QUCB-VI algorithm.
6. Numerical Evaluations
In this section, we analyze the performance of QUCB-VI (Algorithm 1) via proof-of-concept
experiments on multiple RL environments. Furthermore, we investigate the viability of our
methodology against its classical counterpart UCB-VI (Azar et al., 2017; Agarwal et al.,
2019). To this end, we rst conduct our empirical evaluations on RiverSwim-6 environment
comprising of 6 states and 2 actions, which is an extensively used environment for bench-
marking model-based RL frameworks (Osband et al., 2013; Tossou et al., 2019; Chowdhury
and Zhou, 2022). Next, we extend our testing setup to include Riverswim-12 with 12 states
and 2 actions. Finally, we construct a Grid-world environment (Sutton and Barto, 2018)
comprising of a 7 7 sized grid and characterized by 20 states and 4 actions.
Simulation Congurations: In our experiments, for all the aforementioned environ-
ments, we conduct training across K= 104episodes, and every episode consists of H= 20
time-steps. The environment is reset to a xed initial state at the beginning of each episode.
Furthermore, we perform 20 independent Monte-Carlo simulations and collect episode-wise
14cumulative regret incurred by QUCB-VI, and baseline UCB-VI algorithms. In our imple-
mentation of QUCB-VI, we accumulate the estimates of state transition probability model
based on uniform sample from the actual transition probability within a window governed
by the quantum mean estimation error.
Interpretation of Results In Figure 2, we report our experimental results in terms
of cumulative regret of agents rewards incurred against number of training episodes for each
RL environment. In Fig. 2(a)- 2(c), we note that QUCB-VI signicantly outperforms clas-
sical UCB-VI with a noticeable margin, while QUCB-VI also achieves model convergence
within the chosen number of training episodes. These observations support the perfor-
mance gains in terms of convergence speed of the proposed algorithm as revealed in our
theoretical analysis of regret. In Fig. 2(b)-2(c), we observe that classical UCB-VI suers
an increasingly linear trend in regret growth. This indicates that in environments such as
RiverSwim-12 ,Grid-world which are characterized by large diameter MDPs, it is necessary
to increase training episodes in order to ensure sucient exploration by the RL agent with
classical environment. This demonstrates that quantum computing helps in signicantly
faster convergence.
7. Conclusion and Future Work
We propose a Quantum information assisted model-based RL methodology that facilitates
an agent's learning in an unknown MDP environment. To this end, we rst present a
carefully engineered architecture modeling agent's interaction with the environment at every
time step. Consequently, we outline QUCB-VI Algorithm that suitably incorporates an
ecient quantum mean estimator, leading to exponential theoretical convergence speed
improvements in contrast to classical UCB-VI proposed in (Azar et al., 2017). Finally, we
report evaluations on a set of benchmark RL environment which support the ecacy of
QUCB-VI algorithm. As a future work, it will be worth exploring whether the benets can
be translated to model-free as well as continual RL settings.
References
Alekh Agarwal, Nan Jiang, Sham M Kakade, and Wen Sun. Reinforcement learning: Theory
and algorithms. CS Dept., UW Seattle, Seattle, WA, USA, Tech. Rep , pages 10{4, 2019.
Esma A meur, Gilles Brassard, and S ebastien Gambs. Quantum speed-up for unsupervised
learning. Machine Learning , 90:261{287, 2013.
Abubakr O Al-Abbasi, Arnob Ghosh, and Vaneet Aggarwal. Deeppool: Distributed model-
free algorithm for ride-sharing using deep reinforcement learning. IEEE Transactions on
Intelligent Transportation Systems , 20(12):4714{4727, 2019.
Srinivasan Arunachalam and Ronald de Wolf. Guest column: A survey of quantum learning
theory. ACM Sigact News , 48(2):41{67, 2017.
Peter Auer, Thomas Jaksch, and Ronald Ortner. Near-optimal regret bounds for reinforce-
ment learning. Advances in neural information processing systems , 21, 2008.
15Mohammad Gheshlaghi Azar, Ian Osband, and R emi Munos. Minimax regret bounds for
reinforcement learning. In International Conference on Machine Learning , pages 263{272.
PMLR, 2017.
Gilles Brassard, Peter Hoyer, Michele Mosca, and Alain Tapp. Quantum amplitude ampli-
cation and estimation. Contemporary Mathematics , 305:53{74, 2002.
Gilles Brassard, Frederic Dupuis, Sebastien Gambs, and Alain Tapp. An optimal quantum
algorithm to approximate the mean and its application for approximating the median of
a set of points over an arbitrary distance. arXiv preprint arXiv:1106.4267 , 2011.
Qi Cai, Zhuoran Yang, Chi Jin, and Zhaoran Wang. Provably ecient exploration in
policy optimization. In International Conference on Machine Learning , pages 1283{1294.
PMLR, 2020.
Balthazar Casal e, Giuseppe Di Molfetta, Hachem Kadri, and Liva Ralaivola. Quantum
bandits. Quantum Machine Intelligence , 2:1{7, 2020.
Sayak Ray Chowdhury and Xingyu Zhou. Dierentially private regret minimization in
episodic markov decision processes. In Proceedings of the AAAI Conference on Articial
Intelligence , volume 36, pages 6375{6383, 2022.
Christoph Dann, Tor Lattimore, and Emma Brunskill. Unifying pac and regret: Uniform pac
bounds for episodic reinforcement learning. Advances in Neural Information Processing
Systems , 30, 2017.
Daoyi Dong, Chunlin Chen, Hanxiong Li, and Tzyh-Jong Tarn. Quantum reinforcement
learning. IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics) ,
38(5):1207{1220, 2008.
Simon S Du, Yuping Luo, Ruosong Wang, and Hanrui Zhang. Provably ecient q-learning
with function approximation via distribution shift error checking oracle. Advances in
Neural Information Processing Systems , 32, 2019.
Vedran Dunjko, Jacob M Taylor, and Hans J Briegel. Quantum-enhanced machine learning.
Physical review letters , 117(13):130501, 2016.
Vedran Dunjko, Jacob M Taylor, and Hans J Briegel. Advances in quantum reinforcement
learning. In 2017 IEEE International Conference on Systems, Man, and Cybernetics
(SMC) , pages 282{287. IEEE, 2017.
Lov K Grover. A framework for fast quantum mechanical algorithms. In Proceedings of the
thirtieth annual ACM symposium on Theory of computing , pages 53{62, 1998.
Yassine Hamoudi. Quantum sub-gaussian mean estimator. arXiv preprint
arXiv:2108.12172 , 2021.
Soene Jerbi, Lea M Trenkwalder, Hendrik Poulsen Nautrup, Hans J Briegel, and Vedran
Dunjko. Quantum enhancements for deep reinforcement learning in large spaces. PRX
Quantum , 2(1):010328, 2021.
16Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael I Jordan. Is q-learning provably
ecient? Advances in neural information processing systems , 31, 2018.
Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably ecient reinforce-
ment learning with linear function approximation. In Conference on Learning Theory ,
pages 2137{2143. PMLR, 2020.
Michael Kearns and Satinder Singh. Near-optimal reinforcement learning in polynomial
time. Machine learning , 49:209{232, 2002.
Tze Leung Lai, Herbert Robbins, et al. Asymptotically ecient adaptive allocation rules.
Advances in applied mathematics , 6(1):4{22, 1985.
Josep Lumbreras, Erkka Haapasalo, and Marco Tomamichel. Multi-armed quantum bandits:
Exploration versus exploitation when learning properties of quantum states. Quantum ,
6:749, 2022.
Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lill-
icrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for
deep reinforcement learning. In International conference on machine learning , pages
1928{1937. PMLR, 2016.
Ashley Montanaro. Quantum speedup of monte carlo methods. Proceedings of the Royal
Society A: Mathematical, Physical and Engineering Sciences , 471(2181):20150301, 2015.
Ian Osband, Daniel Russo, and Benjamin Van Roy. (more) ecient reinforcement learning
via posterior sampling. Advances in Neural Information Processing Systems , 26, 2013.
Ian Osband, Benjamin Van Roy, and Zheng Wen. Generalization and exploration via ran-
domized value functions. In International Conference on Machine Learning , pages 2377{
2386. PMLR, 2016.
Giuseppe Davide Paparo, Vedran Dunjko, Adi Makmal, Miguel Angel Martin-Delgado, and
Hans J Briegel. Quantum speedup for active learning agents. Physical Review X , 4(3):
031002, 2014.
Patrick Rebentrost, Masoud Mohseni, and Seth Lloyd. Quantum support vector machine
for big data classication. Physical review letters , 113(13):130503, 2014.
David Rohde, Stephen Bonner, Travis Dunlop, Flavian Vasile, and Alexandros Karatzoglou.
Recogym: A reinforcement learning environment for the problem of product recommen-
dation in online advertising. arXiv preprint arXiv:1808.00720 , 2018.
Valeria Saggio, Beate E Asenbeck, Arne Hamann, Teodor Str omberg, Peter Schiansky,
Vedran Dunjko, Nicolai Friis, Nicholas C Harris, Michael Hochberg, Dirk Englund, et al.
Experimental quantum speed-up in reinforcement learning agents. Nature , 591(7849):
229{233, 2021.
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang,
Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Master-
ing the game of go without human knowledge. nature , 550(7676):354{359, 2017.
17Alexander L Strehl, Lihong Li, Eric Wiewiora, John Langford, and Michael L Littman. Pac
model-free reinforcement learning. In Proceedings of the 23rd international conference on
Machine learning , pages 881{888, 2006.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction . MIT
press, 2018.
Aristide Tossou, Debabrota Basu, and Christos Dimitrakakis. Near-optimal opti-
mistic reinforcement learning using empirical bernstein inequalities. arXiv preprint
arXiv:1905.12425 , 2019.
Daochen Wang, Aarthi Sundaram, Robin Kothari, Ashish Kapoor, and Martin Roetteler.
Quantum algorithms for reinforcement learning with a generative model. In International
Conference on Machine Learning , pages 10916{10926. PMLR, 2021a.
Daochen Wang, Xuchen You, Tongyang Li, and Andrew M Childs. Quantum exploration
algorithms for multi-armed bandits. In Proceedings of the AAAI Conference on Articial
Intelligence , volume 35, pages 10102{10110, 2021b.
18