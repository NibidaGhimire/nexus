基于改进 YOLOX 的道路目标检测算法 研究  
杨涛，吴友宇，汤杨心泰  
武汉理工大学  
摘要：道路目标检测 是自动驾驶技术中的一个重要分支，检测精度越高的模型越有利于车辆的安全驾
驶。在道路目标检测中，小目标和遮挡目标的漏检是一个重要的难题，降低目标的漏检率对于安全驾驶具
有重要意义。  
在本文的工作中，基于 YOLOX目标检测算法进行改进，提出了 DecIoU边界框回归损失函数来提高
预测框和真实框的形状一致性，并引入 Push  Loss来进一步优化边界框回归损失函数，以检测出更多的遮
挡目标。此外，还使用了动态锚框机制来提升置信度标签的准确性，改善了无锚框目标检测模型的标签不
准确的问题。  
在KITTI数据集上的大量实验证明了 所提出的方法的有效性，改进的 YOLOX -s在KITTI数据集上的
mAP和mAR分别达到 88.9%和91.0%，相比基线版本提升 2.77%和4.24%；改进的 YOLOX -m的mAP和
mAR分别达到 89.1%和91.4%，提升了 2.30%和4.10%。 
关键词： Object Detection ; YOLOX ; DecIoU ; Push Loss ; Dynamic Anchor  
1 介绍  
近年来，自动驾驶汽车不断走进我们的视野中，
面向自动驾驶的目标检测算法也成为了国内外的
研究热点之一。安全可靠的自动驾驶汽车依赖于对
周围环境的准确感知，以便及时做出正确的决策。
目标检测是自动驾驶系统的关键任务之一，其主要
的功能是检测前方道路上出现的目标的空间位置
和目标类别[1]。 
传统目标检测算法依赖于手工设计好的特征
来对目标进行特征提取，以实现分类和检测的目的，
常见的目标特征包括 Scale Invar iant Feature 
Transform  (SIFT )[2]、Speeded up robust features  
(SURF )[3]、histogram of oriented gradient  (HOG )[4] et 
al. 该类方法设计出的特征泛 化能力弱、鲁棒性较
差。2012年，Krizhevsky  et al.提出了以 Convolutional 
Neural Networks  (CNN)为基础的 AlexNe t[5]算法框架，极大的提升了算法的速度和准确度。  
相比于传统目标检测算法，以 CNN为核心的
目标检测算法具有准确率高、检测速度快等优点，
发展潜力巨大。根据神经网络的结构不同，可以将
目标检测算法分为两阶段目标检测算法和一阶段
目标检测算法。 2014年， Girshick  et al.提出了 R-
CNN[6]目标检测算法， 在目标检测数据集 VOC2012
上取得了 30%以上的精度提升。 2015年，基于 R-
CNN改进的 Fast R -CNN[7]、Faster R -CNN[8]等在检
测速度和精度上获得进一步提升，逐渐成为了目标
检测的首选方法。 R-CNN系列目标检测算法是典型
的两阶段目标检测算法，第一阶段通过算法生成候
选区域，第二阶段利用 CNN网络对候选区域进行
特征提取并根据提取的特征进行分类工作，得到最
终的检测结果。  
R-CNN系列目标检测算法以较慢的速度换取了较高的检测精度，在自 动驾驶、智慧交通等实时
检测场景中无法满足需求。 2016年， Redmon  et al.
提出了 You Only Look Once (YOLO )[9]有效的改善了
这一问题。 YOLO网络只需要 “看”一次输入图片，
即可输出最终的检测结果，是典型的一阶段目标检
测算法，具有网络结构简单、检测速度快等优点 。
随后， YOLO  v2~v5[10-13]相继提出， 较好的平衡了精
度和速度的， YOLO算法的优异性能表现将一阶段
目标检测算法推向了主流 。 
2021年，旷视科技 的Ge et al.进一步研究了
YOLO系列目标检测算法，并融合了解耦头、
AnchorFree[14-16]、SimOTA[17]和多正例等技术， 提出
了YOLOX[18]目标检测算法，在满足实时性的前提
下，进一步提升了近两个百分点的精度。  
在本文的研究中，基于 YOLOX目标检测算法
进行损失函数的优化，以改善遮挡目标和小目标等
困难目标检测精度较低的问题。简而言之，本文的
主要贡献如下：  
a) 提出 DecIoU， 通过对面积进行解耦来优化 预测
框的形状，提高预测框和真实框的形状一致性，
并与 IoU、GIoU、DIoU等其他损失函数对比，
证明了 DecIoU的有效性；  
b) 采用 Push  Loss并应用于 边界框回归损失中，
提高了 YOLOX 在KITTI数据集上的检测精
度，检测出更多的遮挡目标；  
c) 采用动态锚框来优化 置信度标签分配，生成更
准确的标签值以优化模型训练，最终得到检测
性能更好的模型。  
2 YOL OX 
Redmon  et al.提出的 YOLO算法把目标检测问
题转化为一个回归问题，通过一个统一的网络对目
标实现分类和定位，只需要在输入端输入一张图像
即可在输出端得到该图像中待检测目标的位置、类
别和置信度。其主要的思想是将输入图片分割成  
SS个网格， 如果图像中目标中心落在了一个网格
内，那么该网格就负责检测这个目标。每一个网格
都会预测出 B个边界框，每个边界框包含位置信息
( , , , )x y w h
、置信度以及 N个类别的概率，对于输
出 层 来 说 ， 最 终 的 输 出 维 度 为 ：
(4 1 ) S S B N   + +
的张量。  
YOLOX以YOLO系列目标检测算法为基础进
行优化，选用 CSPDarkNet 作为主干网络，并采用
了非线性表达能力更强的 SiLU函数作为激活函数，
增强了整个网络的学习能力。在检测头部分，
YOLOX 提出了解耦头以解决目标检测中 分类和回
归冲突的问题。此外，  YOLOX实现了无锚框的目
标检测，摆脱了 YOLO系列检测器的锚框机制 ，将
每个位置从预测 3个边界框减少为预测 1个边界框，
并使它们直接预测偏移值，这种改进不仅减少了检
测器的参数量和计算量，同时获得了更好的性能。
在目标框匹配上， YOLOX 提出了先进的 SimOTA
匹配策略，这使得网络的训练时间有所减少，检测
精度更加优异，展示了高级分配策略的优越性。
YOLOX主要包括 YOLOX -s、YOLOX -m、YOLOX -
l、YOLOX -x等版本，本文将以 YOLOX -s为基线
进行研究。  
损失函数是神经网络的核心驱动力，在神经网
络的训练过程中通过最小化神经网络的全局损失
来优化整个网络的性能表现。 YOLOX 的损失函数
包含三个部分：分类损失 、边界框回归损失和置信
度损失。 YOLOX 使用二元交叉熵损失函数来计算
类别概率和目标置信度得分的损失，使用 IoU或
GIoU等损失函数来作为边界框回归的损失，总损
失函数如 下： 
 
YOLOX cls reg objL L L L= + +    (1) 3 本文方法  
3.1 解耦 IoU损失 
目标检测任务可分为目标分类和目标定位两
个任务。目标分类是要对检测到的目标进行分类以
确定其属于哪一个类别。目标定位是要在图像中确
定待检测目标的位置信息，输出其在图像中的坐标。
目标定位依赖于边界框回归去定位目标，通过在模
型训练过程中最小化边界框回归损失，以优化所预
测边界框的位置，达到定位目标的目的。传统的边
界框回归损失一般通过 L1或L2距离范数来定义 ，
忽视了坐标间的关联性 。2016年，Yu et al.在人脸检
测任务中提出了 Intersection over Union (IoU)损失函
数以建立坐标之间的关联性，提升边界框 回归性能
[19]。IoU是比较两个形状之间相似性的最常用度量，
是目标检测任务中的主要评价指标之一，将度量本
身作为优化的目标是更佳的选择， IoU损失已经在
检测、跟踪和分隔等任务中广泛应用，成为边界框
回归任务的最佳损失函数之一， IoU和基于 IoU的
损失定义如下[20]： 
 
gt
gtBB
IoU
BB=
    (2) 
 
1 R( , )gtL IoU B B= − +    (3) 
式中
B表示预测框，
gtB 表示真实框。 然而当预测
框和真实框不重合时 IoU为0，使用 IoU损失将无
法度量预测框和真实框的远近，无法进一步优化预
测框。为了改善该问题，斯坦福学者 Rezatofighi  et 
al.在2019年提出了 GIoU[21]，随后 Zheng  et al.提出
DIoU[20]再一次优化了边界框回归损失函数， GIoU
和DIoU定义如下：  
 
() /gtC B B
GIoU IoUC=−
   (4) 
 
2
2()gtbbDIoU IoUc−=−   (5) 式中
C表示能包含
B 和
gtB的最小外接矩形框 ，
b
和
gtb分别表示
B 和
gtB的中心点，
c 表示
C的对角
线长度。  
IoU等损失函数主要从边界框面积之间 的差距
进行优化，在优化过程中无法保证预测框和真实框
形状的相似性。受 L1和L2损失函数的启发，我们
在IoU损失基础上对边界框面积进行解耦，添加宽
和高惩罚项，在最小化预测框和真实框面积差距的
同时优化其形状相似性，这对于遮挡目标和小目标
等困难目标检测有重要意义，更合理的检测框形状
将减小该框在后处理过程中被过滤掉的概率，提升
目标检测的召回率。本文将解耦 IoU定义如下：  
 
22
22( ) ( )w w w h
whC I C IDecIoU IoUCC−−= − −   (6) 
式中，
wI 、
hI分别表示
B 和
gtB重叠部分的宽和高，
wC
、
hC分别表示
C 的宽和高。 由此可得如算法 1
所示的 DecIoU损失函数，以优化边界框回归。  
Algorithm 1   DecIoU loss function  
input: 
1 1 2 2( , , , )B x y x y=  as the predicted box,  
1 1 2 2( , , , )gt gt gt gt gtB x y x y=
 as the ground -truth.  
output:
DecIoUL . 
1. Calculating area of 
B : 
2 1 2 1( ) ( )A x x y y= −  − . 
2. Calculating area of 
gtB :  
2 1 2 1( ) ( )gt gt gt gt gtA x x y y= −  −
. 
3. Calculating intersection 
I  between 
B  and 
gtB : 
1 1 1max( , )Igx x x=
, 
2 2 2min( , )Igx x x= , 
1 1 1max( , )Igy y y=
, 
2 2 2min( , )Igy y y= . 
4. Finding the coordinate of smallest enclosing box 
C . 
5. Calculating area of 
C :
2 1 2 1( ) ( )c c c c cA x x y y= −  − . 
6. Calculating 
iou  between 
B  and 
gtB : 
if 
21IIxx  and 
21IIyy  then: 
21()II
wI x x=−
, 
21()II
hI y y=− . 
else: 0wI=, 
0hI= . 
7. Calculating 
iou  and 
deciou : 
21()cc
wC x x=−
, 
21()cc
hC y y=− , 
wh
gt
whIIiouA A I I=+ − 
, 
22
22( ) ( )w w h h
whC I C Ideciou iouCC−−= − −
. 
8. 
1DecIoUL deciou=− . 
3.2 Push-IoU和Push-DecIoU损失 
目标遮挡 是目标检测任务的难题之一， Luo et 
al.提出 Pull loss和Push  loss应用于模型训练中，较
好的改善了遮挡目标的漏检问题[22]。在目标检测算
法训练的过程中，我们把预测框和真实框（ Ground -
truth）进行匹配，当一个预测框和 Ground -truth匹
配后，则该框为正样本；反之，未成功匹配的预测
框为负样本。在 YOLOX中，每个预测框最多匹配
一个 Ground -truth，当道路上两个目标之间的发生
遮挡时，相应的真实框之间出现部分重叠，这将使
得两个目标最终的预测框之间出现重叠，在算法的
后处理过程中有可能将重叠的预测框过滤掉，从而
产生目标的漏检，每一个漏检的目标都关乎着车辆
的行驶安全。为了进一步减小漏检情况的发生， 我
们对IoU损失进行了优化， 改进后的 Push -IoU损失
函数包含 IoU损失和 Push损失两部分， 如算法 2所
示。  
Algorithm 2   Push -IoU loss and Push -DecIoU loss  
input: 
1 1 2 2( , , , )B x y x y= as the predicted box,  
1 1 2 2( , , , )gt gt gt gt gtB x y x y=
as the ground -truth,  
G
as the set of all real boxes.  
output: 
Push IoUL− , 
Push DecIoUL− . 
1. Calculating 
iou  and 
deciou  between 
B  
and 
gtB : 
( , )gtiou IoU B B=
, 
( , )gtdeciou DecIoU B B= . 2. Finding the second ground -truth: find a real box   
`gtB
 which 
`( , )gtIoU B B   is the smallest, where   
`gtBG
 and 
`gt gtBB . 
3. Calculating 
iou  between
B and
`gtB : 
 
`` ( , )gtiou IoU B B= . 
4. 
1`Push IoUL iou iou −= − +  , 
1`Push DecIoUL deciou iou −= − + 
. 
在Push损失中，我们提出了 “Second  Ground -
truth”，如图 3.1所示，对于一个已经匹配了真实框
的正样本预测框，将进一步在该预测框周围的所有
真实框中寻找一个与之 IoU最大的真实框作为
“Second  Ground -truth”（如图 3.1中gt`） 。在训练的
过程中，最大化该正样本预测框和 gt框的 IoU，最
小化该框和 gt`的IoU，尽可能的将两个遮挡目标对
应的预测框推开，减小重叠部分，降低在后处理过
程中被过滤掉的可能性。 此外， 我们为 Push损失设
置了超参数
 来调节 IoU损失和 Push损失的比例，
以控制推开预测框的力度，避免预测 框偏移过多而
成为低质量预测框。  
 
Figure 3.1 Second  Ground -truth 
3.3 动态锚框  
置信度损失是目标检测损失函数中的重要损
失之一，置信度是指边界框中包含目标的可能性  
以及包含目标情况下边界框准确度的乘积，计算公
式如下：  
 
gt
object pred Confidence P IoU =   (7) 
预测框置信度的大小反应了该预测框中包含待检
测目标的概率。在目标检测算法中，通常将输入图
片分割成
SS 个格子， 如果一个格子中包含待检测
目标，则
1objectP= ，反之
0objectP= 。对于基于锚框的
目标检测算法，先验锚框通过训练集统计得出，可
以较好的反应数据集中目标宽高的分布，在训练早
期能够得到更加准确的预测框。 YOLOX 是无锚框
的目标检测算法，训练早期的预测框随机性较大，  
gt
predIoU
较小， 这将使得包含目标的单元格的置信度
标签
gt Confidence 偏小， 无法准确反应该单元格包含
目标的概率 ，。神经网络的训练是追求预测值和标
签值的不断靠近， 标签值的准确性对于目标检测模
型的训练至关重要[23]。本文引入了动态锚框来辅助
gt
predIoU
的计算，以生成更加准确的置信度标签值。
如图 3.2所示，
predb 和
gtb分别为预测框和真实框的
中心点，预测框的中心点已经较好的贴合真实框中
心点，具有成为高质量预测框的潜力，然而由于宽
和高的差距，最终
gt
predIoU 和
Confidence 较小，该预
测框在后续迭代训练过程中可能会被逐渐忽略。本
文以预测框的中心点
predb 作为中心点，构建一个宽
和高分别为
gtw 、
gth的动态锚框（中心点随着预测
框的动态变化而不断更新） ，以生成宽和高更加准
确的辅助框用于计算置信度标签值，推动该预测框
优化成为高质量预测框。  
bgtbpred
wgthgt 
Figure  3.2 Dynamic  Anchor  
4实验 
4.1 实验环境  
本文中改进的 YOLOX算法基于 Pytorch1.11 和
CUDA11.5 进行训练和验证， 实验平台配置如表 4.1，
模型训练参数设置如表 4.2。 
Table 4.1 实验平台配置  
Name  Parameter  
System  Windows10  
CPU  Intel(R) Core(TM) i9 -9900K  
GPU  NVIDIA GeForce RTX 2080Ti  
RAM  32.0GB  
Table 4.2 模型训练参数  
Parameter  Value  
learning rate  0.01 
epochs  500 
batch_size  16 
optimizer_type  SGD  
momentum  0.937  
weight_decay  0.0005  
4.2 KITTI数据集 
KITTI数据集是基于真实场景建立的一个庞大
的自动驾驶数据集，包含市区、乡镇和高速公路等场景的图像数据，可用于目标检测任务的模型训练
和评估。 KITTI数据集包含 7481张带有标签的图
片， 分为 Pedestrian 、 Truck、 Car、Cyclist、 DontCare 、
Misc、Van、Tram、 Person_sitting 等7个类别。 KITTI
数据集中不同类别样本数量极不均衡，可能导致模
型无法收敛，本文进一步将 7类目标划分为 Car、
Cyclist、Pedestrian 三类，在训练过程中将 7481张
图片按照 7:1:2的比例划分为训练集、验证集和测
试集。  
4.3 实验结果与分析  
4.3.1 与基线版本对比  
本文在 YOLOX 中将边界框回归损失函数由
IoU损失替换为 DecIoU损失，我们还将与 GIoU、
DIoU在YOLOX 中的表现进行比较，以评估我们
提出的边界框回归损失函数。  
在实验中， 我们通过计算在特定 IoU阈值下 (本
文取 0.5)不 同 类 别 目 标 的 mean Average 
Precision (mAP )和mean Average Recal l(mAR)来评估
模型的性能。同时，为了更全面的表达模型的检测
能力，我们借鉴 COCO数据集的评价指标，采用
AP S、AP M、AP L、AP 0.5:0.95等指标进行综合分析，
并在表 4.3中报告了我们的实验结果。通过对比实
验表明， GIoU、DIoU作为回归损失可以稍微提高
YOLOX 在KITTI数据集上性能。而本文提出的
DecIoU表现出更好的性能，实现了 88.4%的mAP
和89.6%的mAR， 相比于基线版本分别提升了 2.15%
和2.63%。在AP S、AP M和AP 0.5:0.95等指标上 DecIoU
也有一定优势，这表明 DecIoU通过对面积进行解
耦，所预测的边界框更加准确，定位能力更强，尤
其是对于中小目标有明显提升。 图4.1展示了训练
过程中，不同边界框回归损失的性能对比，在训练
前期 DecIoU与GIoU检测性能基本一致， 相比 DIoU收敛更慢，在训练 180 epoch后， DecIoU的检测性
能逐渐超越 DIoU和GIoU等，并最终 在400 epoch
后模型收敛， DecIoU在四种边界框回归损失中表现
最佳，证明了 DecIoU的有效性。  
 
Figure  4.1 mAP and mAR of training  
Table 4.3 边界框回归损失函数性能对比  
Loss mAP mAR AP S AP M APL AP0.5:0.95  
IoU 86.5 87.3 42.2 58.7 69.3 58.3 
GIoU 
improv%  86.8 
0.35 88.1 
0.92 42.2 
0.00 58.8 
0.17 69.1 
-0.29 58.3 
0.00 
DIoU 
improv%  87.3 
0.92 88.3 
1.15 42.7 
1.18 58.5 
-0.34 69.3 
0.00 58.4 
0.17 
DecIoU  
improv%  88.4 
2.15 89.6 
2.63 42.9 
1.66 59.8 
1.87 69.5 
0.29 59.1 
1.37 
Table 4.4 Results of Push Loss  
Loss  mAP mAR 
IoU 86.5 87.3 
Push-IoU 87.6 88.7 
Push-DecIoU  
improv %  88.3 
2.08 90.5 
3.67 
进一步的，我们验证了 Push -IoU损失和 Push -
DecIoU损失的检测性能， 通过表 4.4可以发现 Push -
DecIoU表现出最佳的检测性能， 缓解了目标漏检的
问题，在和 IoU保持基本一致的 mAP下， mAR达
到90.5%， 能够检测出更多的遮挡目标 。如图 4.2所
示的相同图片上的检测结果 ，IoU Loss检测出 10个
目标而 Push -IoU Loss 检测出 14个目标， 对于 IoU
漏检的遮挡目标， Push -IoU成功的将其检测出来。  
IoU Loss
Push -IoU Loss 
Figure 4.2 detection result  of IoU Loss  and Push -IoU Loss 
4.3.2 消融实验  
最终我们进行了多次消融实验来分析本文提
出的改进方法，消融实验结果如表 3.5所示。为了
提升数据的可信度， 我们基于 YOLOX -s和YOLOX -
m两个版本进行了实验，所有的实验数据均为 3次
实验的平均值。  
正如表 4.5的实验结果所示，本文提出的改进
方法对 YOLO X-s和 YOLOX -m两个版本均有一定
的改善，对 YOLOX的提升是稳定的， mAP分别提
高2.77%和4.24%， mAR分别提高 2.30%和4.10%，
通过消融实验证明了本文方法的有效性。  
Table 4.5 消融实验结果  
Back bone  YOLOX -s  YOLO X-m 
 mAP mAR  mAP mAR 
 86.5 87.3  87.1 87.8 
+DecIoU Loss  88.4 89.6  88.7 90.0 
+Push -DecIoU 
Loss  88.3 90.5  88.7 90.9 
+Dynamic Anchor  
improv %  88.9 
2.77 91.0 
4.24  89.1 
2.30 91.4 
4.10 5 结论 
本文以 YOLOX目标检测模型为基础，提出了
DecIoU边界框回归损失函数和 Push -IoU损失、
Push -DecIoU损失，对 YOLOX 的损失函数进行改
进，并在 KITTI数据集上进行实验验证。实验结果
表明，改进的损失函数提升了模型的精确度同时降
低了漏检率， mAP和mAR分别提升 了2.08%和
3.67%。我们进一步对 YOLOX 的置信度标签计算
方法进行了研究，通过提出动态 Anchor辅助标签
计算，优化了置信度标签的准确度，通过更准确的
标签训练出检测能力更强的模型。本文改进的
YOLOX在KITTI数据集上最终达到 89.1%的mAP
和91.4%的mAR， 对于道路目标的检测达到了较高
的精度，有助于提升智能汽车的行车安全，具有一
定的实用价值。  
然而，本文也有一些不足之处。我们仅使用
KITTI数据集进行了实验验证，下一步有待尝试更
多更大的自动驾驶数据集进行实验研究，以不断改
进我们的方法，提升对于道路目标的检测精度。此
外，弱光照下的目标检测也是道路目标检测场景不
得不面对的一个难题，本文没有针对弱光照场景进
行优化，如何在黑暗环境下保障行车安全也将是一
个重要的研究目标。  
 
参考文献  
 [1] Wu B, Wan A, Iandola F, et al. SqueezeDet: 
Unified, Small, Low Power Fully 
Convolutional Neur al Networks for Real -Time 
Object Detection for Autonomous Driving[J]. 
2017 IEEE Conference on Computer Vision 
and Pattern Recognition Workshops (CVPRW), 
2017. 
 [2] Karami E, Shehata M, Smith A. Image 
identification using SIFT algorithm: 
performance analysi s against different image 
deformations[J]. arXiv preprint 
arXiv:1710.02728, 2017.   [3] Bay H, Tuytelaars T, Gool L V. Surf: Speeded 
up robust features: European conference on 
computer vision[C]: Springer, 2006.  
 [4] Dalal N, Triggs B. Histograms of orient ed 
gradients for human detection: 2005 IEEE 
computer society conference on computer 
vision and pattern recognition (CVPR'05)[C]: 
Ieee, 2005.  
 [5] Krizhevsky A, Sutskever I, Hinton G E. 
Imagenet classification with deep 
convolutional neural networks[J]. Ad vances in 
neural information processing systems, 
2012,25: 1097 -1105. 
 [6] Girshick R, Donahue J, Darrell T, et al. Rich 
feature hierarchies for accurate object detection 
and semantic segmentation: Proceedings of the 
IEEE conference on computer vision and 
pattern recognition[C], 2014.  
 [7] Girshick R. Fast r -cnn: Proceedings of the 
IEEE international conference on computer 
vision[C], 2015.  
 [8] Ren S, He K, Girshick R, et al. Faster r -cnn: 
Towards real -time object detection with region 
proposal networks[J] . Advances in neural 
information processing systems, 2015,28: 91 -
99. 
 [9] Redmon J, Divvala S, Girshick R, et al. You 
only look once: Unified, real -time object 
detection: Proceedings of the IEEE conference 
on computer vision and pattern recognition[C], 
2016.  
[10] Redmon J, Farhadi A. YOLO9000: better, 
faster, stronger: Proceedings of the IEEE 
conference on computer vision and pattern 
recognition[C], 2017.  
[11] Redmon J, Farhadi A. Yolov3: An incremental 
improvement[J]. arXiv preprint 
arXiv:1804.02767, 201 8. 
[12] Bochkovskiy A, Wang C, Liao H M. Yolov4: 
Optimal speed and accuracy of object 
detection[J]. arXiv preprint arXiv:2004.10934, 
2020. 
[13] Jocher G. yolov5. 
https://github.com/ultralytics/yolov5[Z]. 2021.  
[14] Law H, Deng J. CornerNet: Detecting Objec ts as Paired Keypoints.[J]. International Journal 
of Computer Vision, 2020,3(128): 642 -656. 
[15] Duan K, Bai S, Xie L, et al. CenterNet: 
Keypoint Triplets for Object Detection: 
IEEE/CVF International Conference on 
Computer Vision (ICCV)[C], Seoul, Korea 
(South), 2019.  
[16] Tian Z, Shen C, Chen H, et al. FCOS: Fully 
Convolutional One -Stage Object Detection, 
Seoul, Korea (South), 2019.  
[17] Ge Z, Liu S, Li Z, et al. Ota: Optimal transport 
assignment for object detection: Proceedings of 
the IEEE/CVF Conference  on Computer Vision 
and Pattern Recognition[C], 2021.  
[18] Ge Z, Liu S, Wang F, et al. Yolox: Exceeding 
yolo series in 2021[J]. arXiv preprint 
arXiv:2107.08430, 2021.  
[19] Yu J, Jiang Y, Wang Z, et al. UnitBox: An 
advanced object detection network: MM '16 : 
Proceedings of the 24th ACM international 
conference on Multimedia[C], Amsterdam The 
Netherlands, 2016.  
[20] Zheng Z, Wang P, Liu W, et al. Distance -IoU 
Loss: Faster and Better Learning for Bounding 
Box Regression: 34th AAAI Conference on 
Artificial Int elligence / 32nd Innovative 
Applications of Artificial Intelligence 
Conference / 10th AAAI Symposium on 
Educational Advances in Artificial 
Intelligence[C], New York, NY, 2020.  
[21] Rezatofighi H, Tsoi N, Gwak J, et al. 
Generalized Intersection Over Union: A Metric 
and a Loss for Bounding Box Regression: 2019 
IEEE/CVF Conference on Computer Vision 
and Pattern Recognition (CVPR)[C], Long 
Beach, CA, USA, 2019.  
[22] Luo Z, Fang Z, Zheng  S, et al. NMS -loss: 
learning with non -maximum suppression for 
crowded pedestrian detection: Proceedings of 
the 2021 International Conference on 
Multimedia Retrieval[C], 2021.  
[23] Li J, Cheng B, Feris R, et al. Pseudo -IoU: 
Improving label assignment in a nchor -free 
object detection: Proceedings of the IEEE/CVF 
conference on computer vision and pattern recognition[C], 2021.  
 