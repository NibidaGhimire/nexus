1
Geometric Graph Filters and Neural Networks:
Limit Properties and Discriminability Trade-offs
Zhiyang Wang Luana Ruiz Alejandro Ribeiro
Abstract —This paper studies the relationship between a graph
neural network (GNN) and a manifold neural network (MNN)
when the graph is constructed from a set of points sampled
from the manifold, thus encoding geometric information. We
consider convolutional MNNs and GNNs where the manifold and
the graph convolutions are respectively defined in terms of the
Laplace-Beltrami operator and the graph Laplacian. Using the
appropriate kernels, we analyze both dense and moderately sparse
graphs. We prove non-asymptotic error bounds showing that
convolutional filters and neural networks on these graphs converge
to convolutional filters and neural networks on the continuous
manifold. As a byproduct of this analysis, we observe an important
trade-off between the discriminability of graph filters and their
ability to approximate the desired behavior of manifold filters. We
then discuss how this trade-off is ameliorated in neural networks
due to the frequency mixing property of nonlinearities. We further
derive a transferability corollary for geometric graphs sampled
from the same manifold. We validate our results numerically on a
navigation control problem and a point cloud classification task.
Index Terms —Graph Neural Networks, Manifold Filters, Man-
ifold Neural Networks, Convergence Analysis, Discriminability
I. I NTRODUCTION
Geometric data, or data supported in non-Euclidean domains,
is the object of much interest in modern information processing.
It arises in a number of applications, including protein function
prediction [3], [4], robot path planning [5], [6], 3D shape
analysis [7]–[9] and wireless resource allocation [10], [11].
Graph convolutional filters [12], [13] and graph neural networks
(GNNs) [14], [15], along with manifold convolutional filters
[16] and manifold neural networks (MNNs) [17]–[19], are the
standard tools for invariant information processing on these
domains when they are discrete and continuous respectively.
The convolution operation is implemented through information
diffusion over the geometric structure, thus enabling invariant
and stable representations [20]–[23] and feature sharing. The
cascading neural network architecture interleaves convolutions
and nonlinearities, further expanding the model’s expressiveness.
Although there is a clear parallel between graphs and
manifolds—the former can be seen as discretizations of the
latter—, manifolds are infinite-dimensional continuous latent
spaces which can only be accessed by discrete point sampling [7],
[24]–[26]. In general, we have access to a set of sampling points
from the manifold, and build a graph model to approximate
the underlying continuous manifold while attempting to retain
Supported by NSF-Simons MoDL, NSF AI Institutes program and
NSF HDR TRipods. ZW, AR are with the Department of Electrical and
Systems Engineering, University of Pennsylvania, PA, email: {zhiyangw,
aribeiro}@seas.upenn.edu. LR is with the Computer Science & Artificial
Intelligence Laboratory at MIT, MA, and is supported by FODSI and METEOR,
email: ruizl@mit.edu. Preliminary results presented in [1] and [2].the local and global geometric structure [7], [11], [27]. GNNs
have been shown to do well at processing information over
the manifold both experimentally and theoretically [16], [25],
[28]. Of particular note, conditions that guarantee asymptotic
convergence of graph filters and GNNs to manifold filters and
MNNs are known [16].
Asymptotic convergence is a minimal guarantee that can
be enriched with non-asymptotic approximation error bounds.
These bounds are unknown and they are the focus of this paper.
These non-asymptotic approximation error bounds relating
graph filters and GNNs to manifold filters and MNNs are
important because they inform the practical design on graphs
of information processing architectures that we want to deploy
on manifolds. In addition, explicit finite-sample error bounds
often reveal details about the convergence regime (e.g., rates
of convergence and discriminability trade-offs) that are not
revealed by their asymptotic counterparts. For example, the non-
asymptotic convergence analysis of GNNs on graphs sampled
from a graphon (also referred to as a transferability analysis)
gives a more precise characterization of the discriminability-
convergence tradeoff that arises in these GNNs [29], which is
not elucidated by the corresponding asymptotic convergence
result [30].
Contributions. In this paper, we prove and analyze a non-
asymptotic approximation error bound for GNNs on graphs
sampled from manifold, thus closing the gap between GNNs
and MNNs with an explicit numerical relationship. We start by
importing the definition of the manifold filter as a convolutional
operation where the diffusions are exponentials of the Laplace-
Beltrami (LB) operator Lof the manifold M ⊂RN[16]. Given
a set of discrete sampling points from the manifold, we describe
how to construct both dense and relatively sparse geometric
graphs that approximate the underlying manifold in both the
spatial and the spectral domains.
Next, we import the concept of Frequency Difference Thresh-
old (FDT) filters (Definition 3) [17] to overcome the challenge of
dimensionality associated with the infinite-dimensional spectrum
of the LB operator. We show that manifold filters exhibit a
trade-off between their discriminability and their ability to
be approximated by graph filters, which can be observed in
the approximation error bound of geometric graph filters in
Theorems 1 and 2.
The same analysis is conducted for GNNs by incorporating
nonlinearities (Theorem 3), but in GNNs we hypothesize that the
trade-off is alleviated, i.e., that we can recover discriminability,
through the addition of these nonlinear operations (Section V).
In other words, geometric GNNs can be both discriminative
and approximative of the underlying MNNs, which we verify
empirically through numerical experiments (Section VI). Finally,arXiv:2305.18467v2  [cs.LG]  28 Jun 20232
we show how our approximation result can be extended, by
a triangle inequality argument, to a transferability result for
geometric GNNs transferred across geometric graphs of different
sizes sampled from the same underlying manifold.
Related Works. GNNs have been thoroughly studied and
discussed in a number of previous works [14], [15], [31].
Similarly, the convergence and transferability of neural networks
on graphs have been studied in many works including [21], [29],
[32], [33]. These works however see the graphs as samples from
a graphon model. In our paper, we focus on the manifold as
the limit model for large-scale graphs, which is more realistic
than graphons as it can incorporate the underlying geometric
information. Moreover, the manifold can also represent the limit
of sparse or relatively sparse graphs. Very relevant to this paper,
the spectral convergence of relatively sparse graphs sampled
from manifolds has been studied at length in [34].
Manifold filters and manifold neural networks have been
established and discussed in [16]–[18], [24] using different
features and metrics. The integral of the heat diffusion process
is employed in [16], [17] where it is shown that manifold
convolutions are consistent with and can recover both the graph
convolution and the time convolution. In [18], a local geodesic
system capturing local anisotropic structures is used to define
the manifold convolution. Taking a different approach, [24]
defines a mixture Gaussian kernel over the spatial domain using
patch operator that is constructed locally.
Among all these methods, the definition leveraging the heat
diffusion process is the only one to build the connection between
graph neural networks and manifold neural networks when the
graphs are sampled from the manifold [16]. However, this paper
does not provide an explicit convergence rate or approximation
error bound between GNNs and MNNs. In [35], a convergence
rate is given for GNNs approximating MNNs, but the result is
restricted to input signals with a limited bandwidth. Moreover,
the graphs constructed in [16] and [35] still need to be dense
with a well-defined Gaussian kernel. In contrast, in this paper we
extend the analysis of the convergence of GNNs by proving a
non-asymptotic approximation error bound for geometric GNNs
sampled from the MNNs where the geometric graphs are either
dense or relatively sparse; and we eliminate the bandlimited
signal assumption by importing frequency-dependent filters.
Organization. Section II introduces preliminary definitions
of graph signal processing and graph neural networks along
with manifold signal processing and manifold neural networks.
Section III introduces the construction of the geometric graphs
from the sampling points of the manifold and the convergence
of the geometric graph Laplacians. We show the spectrum of
the geometric graphs – both dense and relatively sparse – can
approximate the spectrum of the LB operator of the underlying
manifold with a bounded error. We then go on to study the
approximation of the filtering on the geometric graphs which
is further extended to the approximation of geometric GNNs
to MNNs in Section IV. Section V discusses the implications
of the results derived in Section IV. Section VI illustrates the
results of Sections IV with numerical examples. Section VII
concludes the paper. Proofs are deferred to the appendices and
supplementary materials.II. P RELIMINARY DEFINITIONS
We start by reviewing the basic architecture of graph neural
networks and manifold neural networks.
A. Graph Signal Processing and Graph Neural Networks
LetG= (V,E,W)be an undirected graph with node set
V,|V|=n, and edge set E ⊆ V × V . The edges in Emay be
weighted, in which case the edge weights are assigned by a
function W:E →R. In this paper, we are interested in graph
signals x∈Rnsupported on the nodes. For 1≤i≤n,[x]i
represents the value of the signal at node i.
Graph shift. Operating on graph signals x, we define the
graph shift operator (GSO) S∈Rn×n. The GSO is any matrix
satisfying [S]ij̸= 0 if and only if (i, j)∈ E ori=j, e.g., the
adjacency matrix A,[A]ij=W(i, j), or the graph Laplacian
L=diag(A1)−A[13], [36]. The GSO is so called because,
at each node i, it has the effect of shifting or diffusing the
signal values at i’s neighbors to i, where signal values are
aggregated. Explicitly, [Sx]i=P
j,(i,j)∈E[S]ij[x]j. In the case
of an undirected graph, the GSO is symmetric and can therefore
be decomposed as S=VΛVH. The eigenvalues in the diagonal
matrix Λare seen as graph frequencies, and the eigenvectors
inVare the corresponding graph oscillation modes.
Graph convolution. A graph convolution is defined based on
the graph diffusion process with Kt−1consecutive shifts.
More formally, the graph convolutional filter [13], [15], [37]
with coefficients {hk}Kt−1
k=0is given by
hG(S)x=Kt−1X
k=0hkSkx. (1)
Plugging the spectral decomposition of Sinto (1), we see that
in the spectral domain this filter can be represented as
VHhG(S)x=Kt−1X
k=1hkΛkVHx=h(Λ)VHx. (2)
Hence, the graph frequency response of the graph convolution
is given by h(λ) =PKt−1
k=0hkλk, which only depends on the
weights hkand on the eigenvalues of S.
Graph neural network. A graph neural network (GNN) is
built by cascading layers that each consists of a bank of filters
followed by a point-wise nonlinearity σ:R→R,[σ(x)]i=
σ([x]i). The lth layer of a GNN produces Flsignals xp
l, each
called a feature, given by
xp
l=σ
Fl−1X
q=1hlpq
G(S)xq
l−1
, (3)
for1≤p≤Fl. At each layer l= 1,2. . . , L , the number of
input and output features are Fl−1andFlrespectively. The filter
hlpq
G(S)is as in (1)and maps the q-th feature of layer l−1to
thep-th feature of layer l. For simplicity, we write the GNN
consisting of Llayers like (3)as the map ΦG(H,S,x), where
Hdenotes a set of the graph filter coefficients at all layers.3
B. Manifold Signal Processing and Manifold Neural Networks
LetMbe ad-dimensional embedded manifold in RN. For
simplicity, in this paper whenever we mention the manifold
M, we assume that it is a compact, smooth and differentiable
d-dimensional submanifold embedded in RN. Atop M, signals
are defined as scalar functions f:M → Rcalled manifold
signals [23]. The inner product of signals f, g∈L2(M)is
defined as
⟨f, g⟩M=Z
Mf(x)g(x)dµ(x), (4)
where dµ(x)is the volume element with respect to the measure
µoverM. Similarly, the norm of the manifold signal fis
∥f∥2
M=⟨f, f⟩M. (5)
Manifold shift. The manifold Mis locally Euclidean, and the
local homeomorphic Euclidean space around each point x∈ M
is defined as the tangent space TxM. The disjoint union of all
tangent spaces over Mis called the tangent bundle and denoted
TM. The intrinsic gradient ∇:L2(M)→L2(TM)is the
differentiation operator and maps scalar functions to tangent
vector functions over M[25], [38]. The adjoint of ∇is the
intrinsic divergence , which is defined as div:L2(TM)→
L2(M). The Laplace-Beltrami (LB) operator L:L2(M)→
L2(M)is defined as the intrinsic divergence of the intrinsic
gradient [39]. Formally, the operation is written as
Lf=−div◦ ∇f. (6)
This operator measures the difference between the signal value at
a point and the average signal value in the point’s neighborhood
[25]. This is akin to how the graph Laplacian matrix can be
used to compute the total variation of a graph signal [40].
On manifolds, the shift operation is defined based on the LB
operator and on the solution to the heat equation (see [23] for
a detailed exposition). Explicitly, for a manifold signal fthe
manifold shift is written as e−dtLf. Since the LB operator is
self-adjoint and positive-semidefinite and the manifold Mis
compact, Lhas real, positive and discrete eigenvalues {λi}∞
i=1,
which can be written as
Lϕi=λiϕi, (7)
whereϕiis the eigenfunction associated with eigenvalue λi. The
eigenvalues, or manifold frequencies, are ordered in increasing
order as 0< λ 1≤λ2≤λ3≤. . ., and the eigenfunctions,
or manifold oscillation modes, are orthonormal and form an
eigenbasis of L2(M)in the intrinsic sense. The ϕiare also
eigenfunctions of the shift operator e−dtL, with corresponding
eigenvalues e−dtλi.
Manifold convolution. Integrating e−dtLfover[0,∞)yields
the infinite-horizon diffusion process
g(x) =Z∞
0e−tLf(x)dt. (8)
Let˜h:R+→Rdenote the filter impulse response. Based on
this diffusion process,a manifold filter can be defined via the
manifold convolution [23],
g(x) =hf(x) :=Z∞
0˜h(t)e−tLf(x)dt=h(L)f(x). (9)Note that the map h:=h(L)is parametric on the LB operator,
and is a spatial map acting directly on x∈ M .
If we write [ˆf]i=⟨f,ϕi⟩L2(M)=R
Mf(x)ϕi(x)dµ(x),
the manifold convolution can be represented in the manifold
frequency domain as
[ˆg]i=Z∞
0˜h(t)e−tλidt[ˆf]i. (10)
Hence, the frequency response of the filter h(L)is given by
ˆh(λ) =R∞
0˜h(t)e−tλdt, which only depends on the impulse
response ˜hand the LB eigenvalues λ=λi. Further summing
over all iand projecting back onto the spatial domain, we can
alternatively represent h(L)as
g=h(L)f=∞X
i=1ˆh(λi)[ˆf]iϕi. (11)
Manifold neural network. Similarly to the GNN, we can define
the manifold neural network (MNN) as a cascade of layers
l= 1,2. . . , L each of which consists of a bank of manifold
filters and a pointwise nonlinearity σ:R→R. Layer lis
explicitly written as
fp
l(x) =σ
Fl−1X
q=1hpq
l(L)fq
l−1(x)
, (12)
where each signal fp
l,1≤p≤Flis a different feature. Each
layer lmaps Fl−1input features to Floutput features. For a
more succinct representation, in the following the MNN will be
denoted Φ(H,L, f), where His a function set gathering the
impulse responses of the manifold filters hpq
lfor all features
and all layers.
III. G EOMETRIC GRAPHS AND LAPLACIAN CONVERGENCE
Given discrete set of points sampled from the manifold M,
we can construct a discrete approximation of Mby connecting
the sample points by means of a graph. The nodes of the graph
are the sample points, and the edges (more specifically, their
weights) are defined as some function of the Euclidean distance
between each node pair to encode geometric information from
the manifold. We henceforth refer to graphs carrying topological
manifold information as geometric graphs , which is slightly
different from the graph theory definition that focuses on the
geometric properties of the edges of the graph [41].
LetXbe a set of npoints {x1, x2, . . . , x n}sampled i.i.d.
from the manifold Maccording to measure µ. The discrete
empirical measure associated with these points is defined as
pn=1
nPn
i=1δxi, where δxirepresents the Dirac measure at
xi. For signals u, v∈L2(M), the inner product associated with
measure pnis defined as
⟨u, v⟩L2(Gn)=Z
u(x)v(x)dpn=1
nnX
i=1u(xi)v(xi)(13)
and so the norm in L2(Gn)is
∥u∥2
L2(Gn)=⟨u, u⟩L2(Gn).
For signals u,v∈L2(Gn), the inner product is
⟨u,v⟩L2(Gn)=1
nnX
i=1[u]i[v]i4
and the norm is ∥u∥L2(Gn)=⟨u,u⟩L2(Gn).
We construct an undirected geometric graph Gnfrom the
sampled points Xby seeing each point as a vertex. Every pair
of vertices is connected by edges with weight values determined
by a function Kϵof their Euclidean distance. Explicitly, the
weight of edge (i, j), denoted wij, is given by
wij=Kϵ∥xi−xj∥2
ϵ
, (14)
where ∥xi−xj∥is the Euclidean distance between xiandxj.
The geometric adjacency matrix Aϵ
nand the geometric graph
Laplacian Lϵ
ncan therefore be expressed as [Aϵ
n]ij=wijfor
1≤i, j≤nandLϵ
n=diag(Aϵ
n1)−Aϵ
n[42].
Since it is constructed from points xi∈ M , the geometric
graph can be seen as a discrete approximation, or discretization,
of the manifold. To analogously obtain the discretization of
a manifold signal f∈L2(M)on this graph, as well as the
reconstruction from the graph signal back to the manifold signal,
we define a uniform sampling operator Pn:L2(M)→L2(Gn)
and an interpolation operator In:L2(Gn)→L2(M). The
application of the operator Pntofyields the geometric graph
signal
f=Pnfwithf(xi) =f(xi), x i∈X. (15)
I.e.,fis a signal on the graph Gnsharing the values of the
manifold signal fat the sampled points X.
We put certain restrictions on the sampling and interpolation
operators as follows [43].
Definition 1 We call the sampling and interpolation operators
PnandInasymptotically reconstructive if for any manifold
signal f∈L2(M), it holds
lim
n→∞InPnf=f. (16)
Moreover, the sampling and interpolation operators PnandIn
are bounded if there exists a constant Dsuch that
lim sup
n∈N∥Pn∥ ≤D, lim sup
n∈N∥In∥ ≤D. (17)
Seeing the geometric graph Laplacian Lϵ
nas an operator
acting on f:X→R, we can write the diffusion operation at
each point xiexplicitly as
Lϵ
nf(xi) =nX
j=1Kϵ∥xi−xj∥2
ϵ
(f(xi)−f(xj)) (18)
fori= 1,2, . . . , n . This operation can be further lifted to
continuous manifold signals fas
Lϵ
nf(x) =nX
j=1Kϵ∥x−xj∥2
ϵ
(f(x)−f(xj)) (19)
where x∈ M . If we additionally extend the set of sampled
points from Xto all of the manifold M, we obtain the functional
approximation of the geometric graph Laplacian
Lϵf(x) =Z
MKϵ∥x−y∥2
ϵ
(f(x)−f(y))dµ(y). (20)
The definition of the weight function Kϵallows the con-
struction of geometric graphs with different levels of sparsity.
With different choices of Kϵ(i.e., whether Kϵhas a boundedsupport, the relationship between ϵand the number of sampling
nodes n, etc.), the graphs can be sparse (average node degree
Θ(1) ), relatively sparse (average node degree Θ(log n)) or dense
(average node degree Θ(n)). In the following, we will focus
on two kernel definitions that allow constructing dense and
relatively sparse geometric graphs.
A. Laplacian Convergence: Dense Graphs
Dense geometric graphs can be constructed when pairs of
points xiandxjare connected with a weight function Kϵ
defined on an unbounded support (i.e. [0,∞)), which connects
xiandxjregardless of the distance between them, but often
with the edge weight inversely proportional to this distance. This
results in a dense geometric graph with n2edges. In particular,
the Gaussian kernel has been widely used to define the weight
value function due to the good approximation properties of the
corrresponding graph Laplacians vis-à-vis the Laplace-Beltrami
operator [44], [45]. In the Gaussian case, the weight function
Kϵis computed explicitly as
Kϵ∥x−y∥2
ϵ
=1
n1
ϵd/2+1(4π)d/2e−∥x−y∥2
4ϵ,(21)
withdrepresenting the dimension of the manifold. The consis-
tency of the geometric graph Laplacian constructed with this
Gaussian kernel is ensured by a non-asymptotic error bound.
Explicitly, the following result quantifies the approximation of
the dense geometric graph Laplacian in the weak sense.
Proposition 1 LetM ⊂RNbe equipped with LB operator L,
whose eigendecomposition is given by (7). LetGnbe a dense
geometric graph constructed from npoints sampled u.i.d. from
Mwith the edge weights defined as (14) and(21),ϵ=ϵ(n)>
n−1/(d+4). Then, with probability at least 1−δ, the following
holds
|Lϵ
nϕi(x)− Lϕi(x)| ≤ 
C1r
ln(1/δ)
2nϵd+2+C2√ϵ!
λd+2
4
i.(22)
The constants C1,C2depend on the volume of the manifold
and are defined in Appendix A.
Proof. See Section A in supplemental materials. We can
see that the quality of the approximation of LbyLϵ
nrelates
not only to the number of sampling points nbut also grows
with the corresponding eigenvalue λi. This can be interpreted to
mean that eigenfunctions associated with eigenvalues oscillates
faster [46].
Based on this approximation result and using the Davis-
Kahan theorem [47], we can further derive a non-asymptotic
approximation bound relating the spectra of the dense geometric
graph Laplacian and of the LB operator.
Proposition 2 LetM ⊂RNbe equipped with LB operator L,
whose eigendecomposition is given by (7). LetLϵ
nbe the discrete
graph Laplacian of the dense geometric graph Gndefined as
in(19) and (21), with spectrum given by {λϵ
i,n,ϕϵ
i,n}n
i=1. Fix
K∈N+andϵ=ϵ(n)> n−1/(d+4). Then, with probability at
least 1−e−2nϵd+3, we have
|λi−λϵ
i,n| ≤Ω1,K√ϵ,∥aiϕϵ
i,n−ϕi∥ ≤Ω2,K√ϵ/θ, (23)5
withai∈ {− 1,1}for all i < K andθthe eigengap of L, i.e.,
θ= min 1≤i≤K{λi−λi−1, λi+1−λi}. The constants Ω1,K,
Ω2,Kdepend on λK,dand the volume of M.
Proof. See Section B in supplemental materials. Looking at
Proposition 2, we can see that the element-wise non-asymptotic
convergence of the eigenvalues and eigenfunctions is only
guaranteed in a limited part of the spectrum ( i < K ), and
that the upper bound is related to the upper limit λK. This
can be interpreted to mean that as eigenfunctions in the high
frequency domain oscillate faster, they are harder to approximate.
Therefore, when designing filters, we need to be especially
careful when trying to discriminate components in the high
frequency domain.
B. Laplacian Convergence: Relatively Sparse Graphs
We can construct relatively sparse graphs by setting the weight
function Kϵwith a bounded support, i.e., only nodes that are
within a certain distance of one another can be connected by
an edge. We consider the following weight function from [34],
which has been shown to provide a good approximation of the
LB operator,
Kϵ∥x−y∥2
ϵ
=1
nd+ 2
ϵd/2+1αd1[0,1]∥x−y∥2
ϵ
,(24)
where αdis the volume of the unit ball in Rd. As shown by the
indicator function, edges only connect points whose distance
is within the radius√ϵ. From the theory of random geometric
graphs [48], the order of the radius ϵdecides the order of the
average node degree. I.e., if ϵis in the order of Θ(1) , the
expected node degree is in the order of Θ(n), which is a dense
regime. If ϵis in the order of Θ((log( n)/n)2/d), the expected
node degree is in the order of Θ(log( n)), which is a relatively
sparse regime. When setting ϵin the order of Θ(n−2/d), the
graph is sparse with average node degree Θ(1) .
The following proposition provides a non-asymptotic error
bound (in the weak sense) for the approximation of the LB
operator by the relatively sparse graph Laplacian.
Proposition 3 [34, Theorem 3.3] Let M ∈RNbe equipped
with LB operator L, whose eigendecompostion is given by (7).
LetGnbe a sparse geometric graph constructed from npoints
sampled u.i.d. from Mwith the edge weights defined as (14)
and (24),ϵ >(log(n)/2n)2/(d+2). Then, with probability at
least 1−δ, the following holds
|Lϵ
nϕi(x)− Lϕi(x)| ≤ 
C3r
ln (2n/δ)
cnϵd+2+C4√ϵ!
λd+2
4
i.
(25)
The constants C3,C4depend on the volume of the manifold.
Comparing Proposition 1 with Proposition 3, we see that
for large enough ϵthe difference between the graph Lapla-
cian and the Laplace-Beltrami operator is in the order of
O(p
ln(2n/δ)/(nϵd+2),√ϵ)in the sparse graph setting, which
is slightly larger than the order O(p
ln(1/δ)/(nϵd+2),√ϵ)we
observe in the dense graph case (22). This agrees with the
intuition that as dense graphs include more information about the
manifold—by taking into account all the connections betweeneach pair of sampling points—, their Laplacian provides a
better approximation to the Laplace-Beltrami operator of the
underlying manifold than those of relatively sparse graphs.
The differences between the eigenvalues and eigenfunctions
can be bounded similarly, based on the Davis-Kahan theorem.
Proposition 4 [34, Theorem 2.4, Theorem 2.6] Let M ⊂RN
be equipped with LB operator L, whose eigendecomposition is
given by (7). LetLϵ
nbe the discrete graph Laplacian of graph
Gndefined as (19) and (24), with spectrum {λϵ
i,n,ϕϵ
i,n}n
i=1.
FixK∈N+and assume that ϵ=ϵ(n)≥(log(n)/2n)2/(d+2)
Then, with probability at least 1−2ne−cnϵd/2+2, we have
|λi−λϵ
i,n| ≤CK,1√ϵ,∥aiϕϵ
i,n−ϕi∥ ≤CK,2√ϵ/θ, (26)
with ai∈ {− 1,1}for all i < K andθthe eigengap of L,
i.e.,θ= min 1≤i≤K{λi−λi−1, λi+1−λi}. The constants CK,1,
CK,2depend on λK,dand the volume of M.
Akin to the result described in Proposition 2, the approxi-
mation errors of the eigenvalues and eigenfunctions can only
be bounded within a certain range of the spectrum ( < λ K).
Comparing with the bounds given in Proposition 2, the orders
of the errors both depend on variable ϵin the weight function.
Meanwhile, for the convergence probability there is a higher
probability in the dense geometric graph setting compared with
the relatively sparse geometric graph setting, which also supports
the observation that dense geometric graphs can provide a better
approximation of the underlying manifold than their sparse
counterparts.
Remark 1. We note that ϵin(24) controls the connectivity radius
of each node to its neighboring nodes, and that the average
vertex degree is given by αdnϵd/2according to the random
geometric graph theory [49]. In the case of Proposition 4, where
ϵis fixed in the order of (log(n)/2n)2/(d+2), the average degree
scales with an order between O((log( n))2/n)andO(log(n)).
Hence, the graphs in Propositions 3 and 4 are relatively sparse.
IV. G EOMETRIC GNN C ONVERGENCE
Equipped with Propositions 2 and 4, which related the eigen-
values and eigenfunctions of the manifold and graph Laplacian
operators, we will next show that convolutional filters operating
on dense or sparse geometric graphs constructed by sampling
the underlying manifold give good approximations of manifold
filters. As neural networks are cascading structures composing
convolutional filters, GNNs inherit this approximation property
from graph filters, and thus provide good approximations of
MNNs. We begin this section by discussing how the definitions
of manifold filters and MNNs can be generalized to the sampled
geometric graphs.
A. Geometric Graph Convolution
If we fix the impulse response function ˜h(t), the definition of
manifold filtering in (9)indicates that the convolution operation
on the manifold is parametric with respect to the LB operator.
Therefore, we can replace the LB operator, acting on the6
continuous manifold signal, by the discrete graph Laplacian Lϵ
n
acting on a geometric graph signal as defined in (18). Explicitly,
g=Z∞
0˜h(t)e−tLϵ
nfdt:=h(Lϵ
n)f,g,f∈Rn. (27)
This can be interpreted as a discrete geometric graph filtering
process in continuous-time , where the exponential term e−Lϵ
n
should be seen as the GSO. This is slightly different than
the graph convolutional filtering hGdefined in (1), where we
assumed a discrete-time frame.
Leveraging (11), the spectral representation of the above
continuous time geometric graph filter can be written as
g=nX
i=1ˆh(λϵ
i,n)⟨f,ϕϵ
i,n⟩L2(Gn)ϕϵ
i,n, (28)
where {λϵ
i,n,ϕϵ
i,n}n
i=1is the spectrum of Lϵ
n. The spectral rep-
resentation exposes the total dependency of the filter frequency
representation on the eigenspectrum of the Laplacian operator
This indicates that the relationship between geometric graph
filters and manifold filters can be established in the spectral
domain using Propositions 2 and 4.
B. Geometric Graph Convolution Convergence
Recall that the pointwise convergence of the eigenspectrum in
Proposition 2 and 4 is restricted to a certain spectral range. Yet,
the frequency representation of the graph filter has a dependency
on all spectral components. Hence, the infinite spectrum of the
LB operator inevitably presents a challenge in the convergence
analysis of geometric graph filters.
To address this issue, we exploit Weyl’s law [50]. This
classical result states that the eigenvalues {λi}∞
i=1ofLgrow
proportionally to i2/d. This indicates that large eigenvalues, in
the high frequency domain, tend to accumulate; and that the
differences between neighboring eigenvalues tend to become
smaller as the eigenvalues grow larger. This phenomenon is
formally described in the following lemma.
Lemma 1 [17, Proposition 3] Consider a d-dimensional
manifold M ⊂RNand let Lbe its LB operator with eigenvalues
{λk}∞
k=1. Let C1be an arbitrary constant and αdthe volume
of the d-dimensional unit ball. Let Vol(M)denote the volume
of manifold M. For any α >0andd >2, there exists N1,
N1=⌈(αd/C 1)d/(2−d)(CdVol(M))2/(2−d)⌉ (29)
such that, for all k > N 1,λk+1−λk≤α.
Equipped with this fact, we can employ a partitioning strategy
to separate the infinite spectrum into finite intervals as shown
in Definition 2.
Definition 2 [17, Definition 4] ( α-separated spectrum) The
α-separated spectrum of the LB operator Lis defined as a
partition Λ1(α)∪. . .∪ΛN(α)satisfying |λi−λj|> α for
λi∈Λk(α)andλj∈Λl(α),k̸=l.
Theα-separated spectrum as defined above can be achieved
by means of a α-FDT filter, which is defined as follows.Definition 3 [17, Definition 5] ( α-FDT filter) The α-frequency
difference threshold ( α-FDT) filter is defined as a filter h(L)
whose frequency response satisfies
|ˆh(λi)−ˆh(λj)| ≤γkfor all λi, λj∈Λk(α) (30)
withγk≤γfor some γ >0andk= 1, . . . , N .
To prove convergence, we will also an assumption on the
continuity of the manifold filter, which needs to be a Lipschitz
filter as defined below.
Definition 4 (Lipschitz filter) A filter is Ah-Lispchitz if its
frequency response is Lipschitz continuous with Lipschitz
constant Ah,
|ˆh(a)−ˆh(b)| ≤Ah|a−b|for all a, b∈(0,∞). (31)
Letting the geometric graph filter (27) be a Lipschitz continu-
ousα-FDT filter, we are ready to prove an approximation error
bound on the difference between the outputs of a manifold filter
and a geometric graph filter operating on a dense geometric
graph.
Theorem 1 (Convergence of filters on dense geometric graphs)
LetM ⊂ RNbe equipped with LB operator L, whose
eigendecomposition is given by (7). Let Lϵ
nbe the discrete
graph Laplacian of the dense graph Gndefined as in (19) and
(21), with spectrum given by {λϵ
i,n,ϕϵ
i,n}n
i=1. FixK∈N+and
assume that ϵ=ϵ(n)> n−1/(d+4)Leth(·)be the convolutional
filter. Under the assumption that the frequency response of
filterhis Lipschitz continuous and α-FDT with α2≫ϵ,
α > C M,dK2/d−1andγ= Ω 2,K√ϵ/α, with probability at
least 1−2n−2it holds that
∥h(Lϵ
n)Pnf−Pnh(L)f∥L2(Gn)
≤NΩ2,K
α+AhΩ1,K√ϵ+Cgcr
logn
n
(32)
where Nis the partition size of the α-FDT filter and Cgc
depends on both dand the volume of M.
Proof. See Appendix A.
From this theorem, we can see that, if we take ϵ=n−1/(d+4),
the difference between filtering on the constructed graph and
on the manifold scales with O(n−1/(2d+8)). Therefore, given
enough sampling points, one can guarantee good approximation
accuracy with high probability. Also note that a higher dimension
leads to a larger ϵ, which results in a larger approximation error.
This indicates that it is more difficult to approximate manifolds
with higher dimension.
Observe that the partition of the spectrum by the α-FDT filter
lifts the limitation on the spectrum required in Propositions 2 and
4. This is achieved by setting αlarge enough that eigenvalues
larger than λKare grouped. A smaller Kleads to a larger α, to
ensure that more eigenvalues in the high frequency domain are
grouped. Therefore, we can alleviate the divergence of spectral
components associated with large eigenvalues by giving them
similar frequency responses, which subsequently leads to similar
filter outputs. When Kgets larger, the constants Ω1,KandΩ′
2,K7
scale with λKas Propositions 2 and 4 show. Meanwhile, αcan
be set smaller, and the approximation error becomes larger.
Another observation we can make from this non-asymptotic
error bound is that, for a fixed number of sampling points n,
an interesting trade-off arises between the approximative and
discriminative capabilities of the filter. A larger α(i.e., a smaller
K) means a less discriminative filter as more eigenvalues tend
to be grouped and treated similarly with an almost constant
frequency response. A larger αleads to a smaller number of
partitions, as the number of singletons decreases, which results
in the decrease of the error bound in (33). This implies that
the error bound becomes smaller, as the errors brought by
the approximation of the eigenvalues are alleviated by treating
different eigenvalues with similar frequency responses.
For relatively sparse graphs, we can also establish an
approximation error bound between the outputs of manifold
filters and geometric graph filters (27).
Theorem 2 (Convergence of filters on relatively sparse geo-
metric graphs) Let M ⊂ RNbe equipped with LB operator
L, whose eigendecomposition is given by (7). LetLϵ
nbe the
discrete graph Laplacian of Gndefined as in (19) and (24).
FixK∈N+and assume that ϵ=ϵ(n)>(log(n)/n)1/dLet
h(·)be the convolutional filter. Under the assumption that the
frequency response of the filter his Lipschitz continuous and
α-FDT with α2≫ϵ,α > C M,dK1−2/dandγ=CK,2ϵ/α,
with probability at least 1−2nexp(−Cnϵd+4)it holds that
∥h(Lϵ
n)Pnf−Pnh(L)f∥L2(Gn)
≤NCK,2
α+AhCK,1√ϵ+Cgcr
logn
n(33)
where Nis the partition size ofthe α-FDT filter and Cgcdepends
on both dand the volume of M.
Proof. See Appendix A.
Since the convergence constants in this theorem are related
with the convergence rates presented in Proposition 4, the
convergence of geometric graph filters on sparse graphs is
accordant with the convergence of the Laplacian operator on
sparse graphs, and so similar comments apply. Namely, when
compared with sparse geometric graph filters, dense geometric
graph filters provide a better approximation of the underlying
manifold filters because dense graphs carry more information
about the geometry of the manifold.
C. Geometric Graph Neural Network
As filtering on the constructed geometric graph can suc-
cessfully approximate filtering on the manifold, we can cas-
cade geometric graph filters and nonlinearities to construct a
geometric graph neural network that we expect to provide a
good approximation of the manifold neural network. Given the
geometric graph filter defined in (27), the geometric GNN on
Gncan be written as
xp
l=σ
Fl−1X
q=1hpq
l(Lϵ
n)xq
l−1
, (34)
where hpq
l(Lϵ
n)is the filter at the l-th layer of this GNN mapping
theq-th feature in the l−1-th layer to the p-th feature in thel-th layer, with 1≤q≤Fl−1and1≤p≤Fl. We denote
the number of features in the l-th layer Fl(we have dropped
the subscript ninxp
landxq
l−1for simplicity). Gathering the
filter functions in the set H, this geometric GNN on Gncan
be represented more concisely as the map Φ(H,Ln,x).
D. Geometric GNN Convergence
Imposing a continuity assumption on the nonlinearity function,
we can prove the following approximation error bound for
geometric GNNs and MNNs.
Assumption 1 (Normalized Lipschitz nonlinearity) The nonlin-
earity σis normalized Lipschitz continuous, i.e., |σ(a)−σ(b)| ≤
|a−b|, with σ(0) = 0 .
We note that this assumption is reasonable, since most
common nonlinearity functions (e.g., the ReLU, the modulus
and the sigmoid) are normalized Lipschitz.
Theorem 3 With the same hypotheses and definitions as in
Theorem 1 and 2 respectively, let Φ(H,L,·)be an L-layer MNN
onM(12) withF0=FL= 1 input and output features and
Fl=F, l= 1,2, . . . , L −1features per layer. Let Φ(H,Ln,·)
be the GNN with the same architecture applied to the geometric
graph Gn. If the nonlinearities satisfy Assumption 1 and the
manifold filters satisfy ∥h(Lϵ
n)Pnf−Pnh(L)f∥L2(Gn)≤
∆fil,n, it holds that
∥Φ(H,Lϵ
n,Pnf)−PnΦ(H,L, f)∥L2(Gn)≤LFL−1∆fil,n
with high probability.
Proof. See Section C in supplemental materials.
We conclude that the geometric GNN converges to the MNN
as long as the geometric graph filter components give good
approximations of the corresponding manifold filters when
restricted to the sampled geometric graphs. Further, we see that
this error consists of two terms. The first term, LFL−1, depends
on the number of filters and layers in the MNN architecture.
More specifically, the approximation bound grows linearly with
the number of layers Land exponentially with the number
of features Fwhere the rate is determined by L. This term
appears because the approximation errors propagate across all
the manifold filters in all layers of the MNN. The second term
is the approximation error constant of the geometric graph
filters ∆fil,n, which should be replaced with the error bounds
from Theorem 1 and Theorem 2 for dense and sparse graphs
respectively.
We note that the approximation error constant ∆fil,n em-
phasizes that the trade-off between approximation and discrim-
inability is inherited from the convolutional filters. However,
since neural networks also have nonlinearities in their layers,
the geometric GNN has a better ability to both discriminate
high frequency components and approximate the MNN with
satisfying accuracy. This is due to the fact that nonlinearities
have a spectral mixing effect. In the neural network architecture,
the frequency components in the high frequency domain can
be shifted to low frequency domain, where they can then be
discriminated by the filters in the following layer. This role of
nonlinearities has also been discussed in the stability analysis
of GNNs [20] as well as of MNNs [17].8
E. From convergence to transferability
Leveraging the non-asymptotic convergence results derived in
Theorem 3, we can immediately prove a transferability corollary
for geometric GNNs constructed from a common underlying
MNN. Explicitly, consider two geometric graphs Gn1andGn2
that are either dense (21) or realtively sparse graphs (24). The
difference between the outputs of the same neural network (i.e.,
with the same weights) on these two graphs is bounded by the
following corollary.
Corollary 1 LetM ⊂RNbe equipped with LB operator L. Let
Lϵ
n1be the discrete graph Laplacian of the graph Gn1defined
as in (19) and (21) or(24). LetLϵ
n2be the discrete graph
Laplacian of the graph Gn2constructed in the same manner as
Gn1. LetΦ(H,·,·)be an L-layer GNN with F0=FL= 1input
and output features and Fl=F, l= 1,2, . . . , L −1features per
layer. Assume that the filters and nonlinearity functions are as
in Definitions 3, 4 and satisfy Assumption 1 respectively. Then,
it holds in high probability that
∥In1Φ(H,Lϵ
n1,Pn1f)−In2Φ(H,Lϵ
n2,Pn2f)∥
≤LFL−1(∆fil,n 1+ ∆ fil,n 2).(35)
From this corollary, we can observe that the geometric GNN
trained on one geometric graph can be directly transferred to
another geometric graph if they are constructed in the same way
from the same underlying manifold. The difference between
the outputs of the GNNs depends on the size of the neural
network architecture as well as on the approximation capability
of the filter functions. Larger numbers of sample points n1and
n2lead to smaller approximation errors ∆fil,n 1and∆fil,n 2,
that is, better approximation leads to better transferability in
geometric GNNs.
The main implication of this corollary is that if we have
trained a geometric GNN on a relatively small geometric graph,
this GNN can be transferred to another, larger geometric graph
with performance guarantees, which is helpful because it is
costly to train GNNs on large graphs. Using the bound in
Corollary 1, we can determine the minimum number of sampled
points needed for training a geometric GNN to meet a given
approximation error tolerance. Therefore, the geometric GNN
is a suitable model to approximate both continuous MNNs and
large geometric GNNs.
V. D ISCUSSIONS
Approximation vs. discriminability tradeoff. In the conver-
gence results for geometric graph filters on both dense graphs
(Theorem 1) and sparse graphs (Theorem 2), the approximation
error bounds depend on the size of the partition ( N), the
frequency partition threshold ( α), the Lipschitz continuity
constant ( Ah) and the weight function parameter ( ϵ), and the
number of nodes in the geometric graph ( n). The size of the
partition and the frequency partition threshold have a concerted
effect in the approximation, as a larger frequency partition
threshold leads to a larger number of intervals containing more
than one eigenvalue and a smaller number of intervals containing
only one eigenvalue. The size of the partition however tends to
stay the same or decrease because the number of intervals cannotexceed the number of eigenvalues. Therefore, a larger frequency
partition threshold results in a smaller partition size. Combined,
these lead to a smaller approximation error bound. Meanwhile,
a larger frequency threshold also means that a larger number
of eigenvalues are grouped and treated with similar frequency
responses. This makes the α-FDT filters less discriminative. A
similar story holds for the Lipschitz constant Ah, with smaller
Lipschitz constant increasing the approximation error bound
but decreasing discriminability. A larger ϵleads to a larger
approximation error bound, but at the same time the variation
range γin the α-FDT filter becomes larger which results in better
discriminability. In conclusion, there is a trade-off between the
approximation and discriminability properties of both geometric
graph filters and geometric GNNs, but in GNNs this trade-off
is alleviated thanks to the nonlinearities, as we discuss next.
Graph filters vs. GNNs. Due to the aforementioned trade-off
between approximation and discriminability arising from the
spectrum partition, geometric graph filters cannot simultaneously
discriminate high-frequency components and provide good
approximations of manifold filters. In geometric GNNs, this
trade-off is however alleviated by the use of nonlinearities. When
processing the outputs of the filters in the previous layer, the
nonlinearities in a given layer have the ability to shift some high
frequency components to the low frequency domain, where they
can then be discriminated by the filters in the following layer. As
a consequence of this frequency scattering behavior, geometric
GNNs can be simultaneously approximative and discriminative.
Dense graphs vs. sparse graphs. Depending on whether
the graphs are dense or sparse, different weight functions are
chosen to construct the geometric graph, leading to different
convergence regimes. From the results presented in Proposition
1 and Proposition 2 for dense geometric graphs and Proposition
3 and Proposition 2 for sparse geometric graphs, we can see
that, given the same number of sample points nand the same
weight function parameter ϵ, dense graphs have a slightly smaller
approximation error bound when compared with sparse graphs—
thus implying that geometric graph filters and geometric GNNs
provide better approximations of manifold filters and MNNs
when operating on dense graphs. This can be explained by
the fact that dense graphs include more information; for one,
they are complete. However, the sparse graph setting is a more
realistic model in practice, and in particular it has been employed
in many areas such as wireless communication networks and
sensor networks. The approximation guarantees derived in this
paper apply in either sparsity regime.
Discretization over time. Theoretically, we employ the defini-
tions of geometric graph convolution and manifold convolution
in a continuous-time domain as (27) and(9)show. However,
in practice we need to operate the filters and neural networks
in digital systems, which requires the time horizon is discrete.
In the following section, we carry out numerical experiments
with geometric graph filters defined in a discrete-time domain.
Specifically, we discretize the impulse response function with
an interval Tsand replace the function with a series of filter
coefficients hk=˜h(kTs)[16]. By fixing the time horizon with9
finite Ktsamples, we can write the practical geometric filter as
g=Kt−1X
k=0hke−kLϵ
nf,f,g∈Rn, (36)
which recovers the graph convolution definition in (1)with
e−Lϵ
nseen as the graph shift operator.
VI. N UMERICAL EXPERIMENTS
A. Navigation control
We consider a problem of automatic navigation of an agent
[51]. We intend to navigate an agent starting from a given point
to find a path to the goal without colliding into the obstacles. We
sample grid points uniformly over the free space while avoiding
the obstacle areas to construct a geometric graph structure
involving the topological information. We first generate several
trajectories with Dijkstra’s shortest path algorithm that leads
the agent from a randomly selected starting point to the goal.
Every point along the generated trajectories is labeled with a
2-d direction vector of the velocity. The geometric adjacency
matrix is calculated based on the weight function defined in (14)
with (21). Specifically, the underlying topology information is
captured by setting the Euclidean distance between two points
as infinity (i.e. weight value as zero) if there is no direct path
connecting these two points without colliding into the obstacles.
The input graph signal is the position coordinates of some
point over the space and the final output is the direction that
can lead to the goal point. The learned architectures are tested
via randomly generating 100 starting points and computing
the trajectory by predicting directions of the points along the
trajectory. If the trajectory can reach the goal point without
colliding into the obstacles, the trajectory is marked as “success".
The performances are measured by calculating the successful
rates of all the learned architectures.
Learning architectures and experiment settings. We build
dense geometric graphs as the approximations to the models as
the regular grid graph structure brings little difference between
the dense and sparse graph setting. We set the position of each
point as input signals on the graphs. The weights of the edges are
calculated based on the Euclidean distance between the nodes
and the weight function is determined as (21) withϵ= 0.2. We
calculate the Laplacian matrix for the graph as the input graph
shift operator. We train and test three architectures, including 2-
layer Graph Filters (GF), 2-layer Graph Neural Network (GNN)
and 2-layer Lipschitz Graph Neural Network (Lipschitz GNN)
which all contain F0= 2input features, F1= 128 andF2= 64
features with Kt= 10 filters in each layer. ReLU is used as the
nonlinearity function in GNN and Lipschitz GNN. In Lipschitz
GNN architecture, we put the Lipschitz continuity restriction to
the graph filter function by importing a penalty term which is
the scaled derivative of the filter function CLh′(λ)to the loss
function. A larger Lipschitz constant CLindicates a smoother
filter function. All architectures also include a linear readout
layer to map the 2-d direction vector outputs. The loss function
is MSE loss and the optimizer is SGD with the learning rate set
as0.0002 . The architectures are trained over 30,000epochs.Convergence analysis. The convergence is tested by calculating
the difference of the outputs of the graph filters in the last layer
for each architecture. By training the graph filters and GNNs on
geometric graphs with n= 190 ,276,435,630,780, and plotting
the differences between the output of the trained graph filters
or GNNs on geometric graphs with size nand on the relatively
large enough geometric graphs with size n= 1225 . From Figure
5 we can see that the output differences decrease and converge
as the trained geometric graph size grows, which is accordant
with Corollary 1. This also verifies the convergence of geoemtric
GNNs as presented in 3 if we see the large geometric graph as
an approximation of the underlying manifold. We can conclude
that Lipschitz GNNs have better approximations while GNNs
outperform Graph Filters.
We further study the effect of Lipschitz continuity of the
graph filter functions by changing the penalty constant CL. From
Figure 3 we can see as the constant grows, the output differences
become smaller, which attests our claim that smoother filter
functions provide better approximations.
Transferability verification. We verify the transferability result
presented in Corollary 1 by training the architectures on a
sampled geometric graph with size n= 435 ,630,780,1225
and testing these architectures on the large geometric graphs
withn= 1225 . Note that due to the non-scability of the linear
readout layer, we keep the graph filter functions unchanged and
only retrain the final linear layer to map the outputs to a 2-d
direction vector. The successful rates are shown in Table I. We
can observe that the architectures trained on smaller sampled
geometric graphs can still perform well when implemented
on larger graphs. Moreover, Lipschitz GNN performs slightly
better than GNN which outperforms Graph Filters. This can
be understood as the Lipschitz continuity of the filter functions
and the nonlinearity functions help with the transferability as
we have discussed in Section 3.
Graph Filter GNN Lipschitz GNN
n= 435 0.74 0.74 0.76
n= 630 0.79 0.8 0.78
n= 780 0.81 0.82 0.83
n= 1225 0.82 0.83 0.84
Table I: Successful rates when testing the architectures trained on
geometric graphs with n= 435 ,630,780,1225 on the geometric
graph with n= 1225 .
To make the results more explicit, we show the predicted
directions for each point over the space tested with a Lipschitz
GNN trained on graph with n= 435 in Figure 1. The red arrows
point to the learned directions starting from each unlabeled
points while the blue arrows are the learned directions for
labeled points. We can see that Lipschitz GNN can efficiently
learn the successful directions for unlabeled points based on
these geometric graphs and can transfer to larger graphs.
B. Point Cloud Model Classification
We further evaluate the convergence results on the Model-
Net10 [52] classification problem. The dataset includes 3,991
meshed CAD models from 10 categories for training and 908
models for testing. In each model, npoints are uniformly
randomly selected to construct geometric graphs to approximate
the underlying model, such as chairs, tables. Figure 4 shows10
0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.00246810
(a) Testing on n= 435
0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.00246810 (b) Testing on n= 630
0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.00246810 (c) Testing on n= 780
0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.00246810 (d) Testing on n= 1225
Figure 1: Lipschitz GNN trained on geometric graph with n= 435 and tested on geometric graphs with n= 435 ,630,780,1225 .
The red arrows in the figures show the predicted directions for all sampled points over the space.
200 300 400 500 600 700 800
Number of Nodes0.050.100.150.200.250.300.35Difference of the Outputs
Graph Filter
GNN
Lipschitz GNN
Figure 2: Differences of the outputs of 2-layer graph filters,
2-layer GNN and 2-layer Lipschitz GNN.
200 300 400 500 600 700 800
Number of Nodes0.060.080.100.120.140.160.18Difference of the Outputs
Penalty=0.1
Penalty=0.2
Penalty=0.5
Figure 3: Differences of the outputs of Lipshitz GNN with
penalty terms.
the point cloud model with different sampling points. Our goal
is to identify the models for chairs from other models.
Learning architectures and experiment settings. We build
both dense and sparse geometric graphs as the approximations
to the models. We set the coordinates of each point as input
signals on the graphs. The weights of the edges are calculated
based on the Euclidean distance between the nodes. For the
dense geometric graphs, the weight function is determined as
(21) with ϵ= 0.1. Similarly, the weight function of a sparse
geometric graph is calculated as (24) with ϵ= 0.001 as the
threshold. We calculate the Laplacian matrix for each graph as
Figure 4: Point cloud models with 50 and 300 sampling points
in each model. Our goal is to identify chair models from other
models such as toilet and table.
the input graph shift operator. In this experiment, we implement
and compare three different architectures, including 2-layer
Graph Filters (GF), 2-layer Graph Neural Network (GNN) and
2-layer Lipschitz Graph Neural Network (Lipschitz GNN). The
architectures contain F0= 3 input features which are the 3-d
coordinates of each point, F1= 64 andF2= 32 features with
Kt= 5filters in each layer. In GNN and Lipschitz GNN, ReLU
is used as the nonlinearity function. The filters in Lipschitz GNN
is regularized as Lipschitz continuous by imposing a penalty
term CLh′(λ)to the loss function with CLset as 0.3. All
architectures include a linear readout layer to map the final
classification outputs.
All the architectures are trained by minimizing the cross-
entropy loss. We implement an ADAM optimizer with the
learning rate set as 0.005 along with the forgetting factors 0.9
and 0.999. We carry out the training for 40 epochs with the
size of batches set as 10. We run 5 random dataset partitions
and show the average estimation error rates and the standard
deviation across these partitions.
Convergence Verification. We verify our convergence results
by training the graph filters and GNNs on geometric graphs
withn= 300 ,400,500,600,700,800,900sampling points, and
plotting the differences between the output of the trained graph
filters or GNNs on geometric graphs with size nand on the
relatively large enough geometric graphs with size 1000 . Figure
5 shows the convergence results for dense graphs and figure
6 shows the results for sparse graphs. From the figures we
can see that the differences between the outputs on the trained
smaller geometric graphs and the relatively large geometric
graphs decrease and converge as the trained graph size grows,
which verifies Corollary 1. This also proves what we state in
Theorem 3 that the approximation errors of geometric GNNs to
the MNNs decrease as the number of nodes grows if we see the11
300 400 500 600 700 800 900
Number of Nodes01234567Difference of the Outputs
Lipschitz GNN
GNN
GF
Figure 5: Differences of the outputs of Lipschitz GNN, GNN
and graph filter trained on dense geometric graphs.
300 400 500 600 700 800 900
Number of Nodes1234567Difference of the Outputs
Lipschitz GNN
GNN
GF
Figure 6: Differences of the outputs of Lipschitz GNN, GNN
and graph filter trained on sparse geometric graphs.
large enough graph with n= 1000 nodes as the manifold. We
can also observe that Lipschitz GNNs have better approximations
than GNNs because of the continuous filter function while GNNs
outperform graph filters due to the nonlinearity function.
Transferability verification. We train the architectures on both
dense and sparse geometric graphs with n= 300 ,500,700,900
sampled points from each CAD model. To justify the theoretical
results that we have stated in Section IV, we test these
trained architectures on a relatively large graph containing
n= 1000 sampled points to verify that geometric GNNs have
the transferability to large graphs, i.e. the trained geometric
graph filters and GNNs can be directly implemented on larger
geometric graphs as long as the geometric graphs are constructed
in the same manner. The classification error rates are shown
in Table II for the dense graph setting and in Table III for the
sparse graph setting.
Graph Filters GNN Lipschitz GNN
n= 300 21.15%±3.48% 9.35%±2.46% 7.63%±3.36%
n= 500 18.09%±6.28% 7.80%±3.50% 7.54%±4.01%
n= 700 17.31%±6.59% 8.16%±2.95% 7.97%±2.45%
n= 900 15.58%±4.54% 7.20%±3.77% 6.68%±3.94%
Table II: Classification error rates for model ’chair’ when testing
the architectures trained on dense geometric graphs with n=
300,500,700,900 to dense geometric graphs with n= 1000 .
Average over 5 data realizations.Graph Filters GNN Lipschitz GNN
n= 300 19.83%±5.94% 7.74%±4.05% 7.68%±3.75%
n= 500 21.97%±4.17% 10.10%±1.40% 8.60%±2.95%
n= 700 13.85%±3.81% 7.45%±4.03% 8.02%±2.77%
n= 900 16.62%±2.38% 7.92%±3.14% 7.44%±3.30%
Table III: Classification error rates for model ’chair’ when
testing the architectures trained on sparse geometric graphs
with n= 300 ,500,700,900 to sparse geometric graphs with
n= 1000 . Average over 5 data realizations.
We can see from the results shown in the tables that the
trained architectures can still perform well on the relatively large
geometric graphs, both for dense and sparse graph settings. We
can see that Lipschitz GNNs outperform GNNs, which perform
better than graph filters. This attests the effects of the filter
function continuity and nonlinearity that we have discussed in
Section IV, which is continuous filter function and nonlinearity
function both help with improving transferability. We can
also observe that architectures trained on geometric graphs
with more number of nodes can achieve better performances
on the relatively large graphs. This can be understood when
we see this large enough geometric graph with n= 1000
approximately as the underlying manifold and as Theorem 3
shows the geometric GNNs can give better approximations to
the MNNs as the number of nodes grow. This can also be
understood as the transferability property presented in Corollary
1, which also indicates that graph filters and GNNs trained on
larger geometric graphs have better performance approximations.
Furthermore, the results also show that architectures trained on
dense geometric graphs transfer better than the ones trained on
sparse geometric graphs, which is also accordant with what we
have claimed in Section IV.
VII. C ONCLUSIONS
In this paper, we import the definition of manifold convolu-
tional filters with an exponential Laplace-Beltrami operator to
process manifold signals. The manifold model is accessible with
a set of i.i.d. uniformly sampled points over the manifold. We
construct both dense and sparse graph models to approximate
the underlying manifold. We first prove the approximation error
bounds of discrete graph Laplacians to the LB operator in
the spectral domain. We transfer the definition of manifold
filter to the constructed graph and prove the graph filter
can approximate the manifold filter with a non-asymptotic
error bound. The constructed graph filters need to trade-off
between the discrimative and approximative powers while GNNs
composed with graph filters and nonlinearities can alleviate
the trade-off with the frequency mixing by nonlinearities. We
conclude that the GNNs are thus both good approximations
of MNNs and discriminative. We finally verified our results
numerically with a navigation control problem over a manifold.
REFERENCES
[1]Z. Wang, L. Ruiz, and A. Ribeiro, “Convolutional filtering on sampled
manifolds,” in ICASSP 2023-2023 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP) . IEEE, 2023, pp.
1–5.
[2]——, “Convergence of graph neural networks on relatively sparse graphs,”
2023. [Online]. Available: https://zhiyangw.com/Papers/Asilomar2023.pdf
[3]V . N. Ioannidis, A. G. Marques, and G. B. Giannakis, “Graph neural
networks for predicting protein functions,” in 2019 IEEE 8th International
Workshop on Computational Advances in Multi-Sensor Adaptive Processing
(CAMSAP) . IEEE, 2019, pp. 221–225.12
[4]V . Gligorijevi ´c, P. D. Renfrew, T. Kosciolek, J. K. Leman, D. Berenberg,
T. Vatanen, C. Chandler, B. C. Taylor, I. M. Fisk, H. Vlamakis et al. ,
“Structure-based protein function prediction using graph convolutional
networks,” Nature communications , vol. 12, no. 1, pp. 1–14, 2021.
[5]Q. Li, F. Gama, A. Ribeiro, and A. Prorok, “Graph neural networks for
decentralized multi-robot path planning,” in 2020 IEEE/RSJ International
Conference on Intelligent Robots and Systems (IROS) . IEEE, 2020, pp.
11 785–11 792.
[6]E. Tolstaya, J. Paulos, V . Kumar, and A. Ribeiro, “Multi-robot coverage
and exploration using spatial graph neural networks,” in 2021 IEEE/RSJ
International Conference on Intelligent Robots and Systems (IROS) . IEEE,
2021, pp. 8944–8950.
[7]J. Zeng, G. Cheung, M. Ng, J. Pang, and C. Yang, “3d point cloud denoising
using graph laplacian regularization of a low dimensional manifold model,”
IEEE Transactions on Image Processing , vol. 29, pp. 3474–3489, 2019.
[8]M. Devanne, H. Wannous, S. Berretti, P. Pala, M. Daoudi, and
A. Del Bimbo, “3-d human action recognition by shape analysis of motion
trajectories on riemannian manifold,” IEEE transactions on cybernetics ,
vol. 45, no. 7, pp. 1340–1352, 2014.
[9]Y . Wang, Y . Sun, Z. Liu, S. E. Sarma, M. M. Bronstein, and J. M. Solomon,
“Dynamic graph cnn for learning on point clouds,” Acm Transactions On
Graphics (tog) , vol. 38, no. 5, pp. 1–12, 2019.
[10] Z. He, L. Wang, H. Ye, G. Y . Li, and B.-H. F. Juang, “Resource
allocation based on graph neural networks in vehicular communications,”
inGLOBECOM 2020-2020 IEEE Global Communications Conference .
IEEE, 2020, pp. 1–5.
[11] Z. Wang, L. Ruiz, M. Eisen, and A. Ribeiro, “Stable and transferable
wireless resource allocation policies via manifold neural networks,” in
ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP) . IEEE, 2022, pp. 8912–8916.
[12] F. Gama, E. Isufi, G. Leus, and A. Ribeiro, “Graphs, convolutions, and
neural networks: From graph filters to graph neural networks,” IEEE
Signal Processing Magazine , vol. 37, no. 6, pp. 128–138, 2020.
[13] A. Ortega, P. Frossard, J. Kova ˇcevi´c, J. M. Moura, and P. Vandergheynst,
“Graph signal processing: Overview, challenges, and applications,” Pro-
ceedings of the IEEE , vol. 106, no. 5, pp. 808–828, 2018.
[14] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini,
“The graph neural network model,” IEEE transactions on neural networks ,
vol. 20, no. 1, pp. 61–80, 2008.
[15] F. Gama, A. G. Marques, G. Leus, and A. Ribeiro, “Convolutional neural
network architectures for signals supported on graphs,” IEEE Transactions
on Signal Processing , vol. 67, no. 4, pp. 1034–1049, 2019.
[16] Z. Wang, L. Ruiz, and A. Ribeiro, “Convolutional neural networks on
manifolds: From graphs and back,” arXiv preprint arXiv:2210.00376 ,
2022.
[17] ——, “Stability of neural networks on manifolds to relative perturbations,”
inICASSP 2022-2022 IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP) . IEEE, 2022, pp. 5473–5477.
[18] J. Masci, D. Boscaini, M. Bronstein, and P. Vandergheynst, “Geodesic
convolutional neural networks on riemannian manifolds,” in Proceedings
of the IEEE international conference on computer vision workshops , 2015,
pp. 37–45.
[19] R. Chakraborty, J. Bouza, J. H. Manton, and B. C. Vemuri, “Manifoldnet:
A deep neural network for manifold-valued data with applications,” IEEE
Transactions on Pattern Analysis and Machine Intelligence , vol. 44, no. 2,
pp. 799–810, 2020.
[20] F. Gama, J. Bruna, and A. Ribeiro, “Stability properties of graph neural
networks,” IEEE Transactions on Signal Processing , vol. 68, pp. 5680–
5695, 2020.
[21] N. Keriven, A. Bietti, and S. Vaiter, “Convergence and stability of graph
convolutional networks on large random graphs,” Advances in Neural
Information Processing Systems , vol. 33, pp. 21 512–21 523, 2020.
[22] D. Zou and G. Lerman, “Graph convolutional neural networks via
scattering,” Applied and Computational Harmonic Analysis , vol. 49, no. 3,
pp. 1046–1074, 2020.
[23] Z. Wang, L. Ruiz, and A. Ribeiro, “Stability to deformations of manifold
filters and manifold neural networks,” arXiv preprint arXiv:2106.03725 ,
2021.
[24] F. Monti, D. Boscaini, J. Masci, E. Rodola, J. Svoboda, and M. M.
Bronstein, “Geometric deep learning on graphs and manifolds using
mixture model cnns,” in Proceedings of the IEEE conference on computer
vision and pattern recognition , 2017, pp. 5115–5124.
[25] M. M. Bronstein, J. Bruna, Y . LeCun, A. Szlam, and P. Vandergheynst,
“Geometric deep learning: going beyond euclidean data,” IEEE Signal
Processing Magazine , vol. 34, no. 4, pp. 18–42, 2017.
[26] M. Eliasof, E. Haber, and E. Treister, “Pde-gcn: novel architectures
for graph neural networks motivated by partial differential equations,”Advances in neural information processing systems , vol. 34, pp. 3836–
3849, 2021.
[27] W. Shi and R. Rajkumar, “Point-gnn: Graph neural network for 3d object
detection in a point cloud,” in Proceedings of the IEEE/CVF conference
on computer vision and pattern recognition , 2020, pp. 1711–1719.
[28] M. T. Kejani, F. Dornaika, and H. Talebi, “Graph convolution networks with
manifold regularization for semi-supervised learning,” Neural Networks ,
vol. 127, pp. 160–167, 2020.
[29] L. Ruiz, L. F. Chamon, and A. Ribeiro, “Transferability Properties of
Graph Neural Networks,” arXiv preprint arXiv:2112.04629 , 2021.
[30] ——, “Graphon signal processing,” IEEE Transactions on Signal Process-
ing, vol. 69, pp. 4961–4976, 2021.
[31] J. Zhou, G. Cui, S. Hu, Z. Zhang, C. Yang, Z. Liu, L. Wang, C. Li, and
M. Sun, “Graph neural networks: A review of methods and applications,”
AI Open , vol. 1, pp. 57–81, 2020.
[32] L. Ruiz, L. Chamon, and A. Ribeiro, “Graphon neural networks and the
transferability of graph neural networks,” Advances in Neural Information
Processing Systems , vol. 33, pp. 1702–1712, 2020.
[33] S. Maskey, R. Levie, and G. Kutyniok, “Transferability of graph neural net-
works: an extended graphon approach,” arXiv preprint arXiv:2109.10096 ,
2021.
[34] J. Calder and N. G. Trillos, “Improved spectral convergence rates for
graph laplacians on epsilon-graphs and k-nn graphs,” arXiv preprint
arXiv:1910.13476 , 2019.
[35] J. Chew, D. Needell, and M. Perlmutter, “A convergence rate for manifold
neural networks,” arXiv preprint arXiv:2212.12606 , 2022.
[36] D. I. Shuman, S. K. Narang, P. Frossard, A. Ortega, and P. Vandergheynst,
“The emerging field of signal processing on graphs: Extending high-
dimensional data analysis to networks and other irregular domains,” IEEE
signal processing magazine , vol. 30, no. 3, pp. 83–98, 2013.
[37] A. Sandryhaila and J. M. Moura, “Discrete signal processing on graphs,”
IEEE transactions on signal processing , vol. 61, no. 7, pp. 1644–1656,
2013.
[38] N. Boumal, An introduction to optimization on smooth manifolds . Cam-
bridge University Press, 2023.
[39] S. Rosenberg and R. Steven, The Laplacian on a Riemannian manifold:
an introduction to analysis on manifolds . Cambridge University Press,
1997, no. 31.
[40] F. R. Chung, Spectral graph theory . American Mathematical Soc., 1997,
vol. 92.
[41] J. Pach, “The beginnings of geometric graph theory,” Erd˝ os Centennial ,
pp. 465–484, 2013.
[42] R. Merris, “A survey of graph laplacians,” Linear and Multilinear Algebra ,
vol. 39, no. 1-2, pp. 19–31, 1995.
[43] R. Levie, W. Huang, L. Bucci, M. Bronstein, and G. Kutyniok, “Trans-
ferability of spectral graph convolutional neural networks,” Journal of
Machine Learning Research , vol. 22, no. 272, pp. 1–59, 2021.
[44] D. B. Dunson, H.-T. Wu, and N. Wu, “Spectral convergence of graph
laplacian and heat kernel reconstruction in l∞from random samples,”
Applied and Computational Harmonic Analysis , vol. 55, pp. 282–336,
2021.
[45] M. Belkin and P. Niyogi, “Towards a theoretical foundation for laplacian-
based manifold methods,” Journal of Computer and System Sciences ,
vol. 74, no. 8, pp. 1289–1308, 2008.
[46] Y . Shi and B. Xu, “Gradient estimate of an eigenfunction on a compact
riemannian manifold without boundary,” Annals of Global Analysis and
Geometry , vol. 38, no. 1, pp. 21–26, 2010.
[47] A. Seelmann, “Notes on the sin2θtheorem,” Integral Equations and
Operator Theory , vol. 79, no. 4, pp. 579–597, 2014.
[48] M. Penrose, Random geometric graphs . OUP Oxford, 2003, vol. 5.
[49] M. Hamidouche, “Spectral analysis of random geometric graphs,” Ph.D.
dissertation, Université Côte d’Azur, 2020.
[50] W. Arendt and W. P. Schleich, Mathematical analysis of evolution,
information, and complexity . John Wiley & Sons, 2009.
[51] J. Cervino, L. Chamon, B. D. Haeffele, R. Vidal, and A. Ribeiro, “Learning
globally smooth functions on manifolds,” arXiv preprint arXiv:2210.00301 ,
2022.
[52] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and J. Xiao, “3d
shapenets: A deep representation for volumetric shapes,” in Proceedings
of the IEEE conference on computer vision and pattern recognition , 2015,
pp. 1912–1920.
[53] U. V on Luxburg, M. Belkin, and O. Bousquet, “Consistency of spectral
clustering,” The Annals of Statistics , pp. 555–586, 2008.
[54] M. Belkin and P. Niyogi, “Convergence of laplacian eigenmaps,” Advances
in neural information processing systems , vol. 19, 2006.13
APPENDIX
A. Proof of Theorem 1 and Theorem 2
We first write out the filter representation as
∥h(Lϵ
n)Pnf−Pnh(L)f∥
≤∞X
i=1ˆh(λϵ
i,n)⟨Pnf,ϕϵ
i,n⟩Gnϕϵ
i,n−∞X
i=1ˆh(λi)⟨f,ϕi⟩MPnϕi
(37)
We denote the index of partitions that contain a single
eigenvalue as a set Ks(|Ks|=Ns) and the rest as a set
Km(|Km|=Nm). We decompose the α-FDT filter function as
ˆh(λ) =h(0)(λ) +P
l∈Kmh(l)(λ)as
h(0)(λ) =(ˆh(λ)−P
l∈Kmˆh(Cl)λ∈[Λk(α)]k∈Ks
0 otherwise(38)
h(l)(λ) =

ˆh(Cl)λ∈[Λk(α)]k∈Ks
ˆh(λ) λ∈Λl(α)
0 otherwise(39)
withClsome constant in Λl(α). With the triangle inequality,
we start by analyzing the output difference of h(0)(λ)as
∥∞X
i=1h(0)(λϵ
i,n)⟨Pnf,ϕϵ
i,n⟩Gnϕϵ
i,n−∞X
i=1h(0)(λi)⟨f,ϕi⟩MPnϕi∥
≤∞X
i=1
h(0)(λϵ
i,n)−h(0)(λi)
⟨Pnf,ϕϵ
i,n⟩Gnϕϵ
i,n
+∞X
i=1h(0)(λi) 
⟨Pnf,ϕϵ
i,n⟩Gnϕϵ
i,n− ⟨f,ϕi⟩MPnϕi.
(40)
The first term in (40) can be bounded by leveraging the
Ah-Lipschitz continuity of the frequency response. From the
eigenvalue difference in Proposition 4, we can claim that for
each eigenvalue λi≤λK, we have
|λϵ
i,n−λi| ≤Ω1,K√ϵ. (41)
The square of the first term is bounded as
∞X
i=1(h(0)(λϵ
i,n)−h(0)(λi))⟨Pnf,ϕϵ
i,n⟩Gnϕϵ
i,n2
≤∞X
i=1|h(0)(λϵ
i,n)−h(0)(λi)|2|⟨Pnf,ϕϵ
i,n⟩Gn|2(42)
≤∞X
i=1A2
h|λϵ
i,n−λi|2∥Pnf∥2≤A2
hΩ2
1,Kϵ. (43)
The second term in (40) can be bounded combined with the
convergence of eigenfunctions in (45) as∞X
i=1h(0)(λi) 
⟨Pnf,ϕϵ
i,n⟩Gnϕϵ
i,n− ⟨f,ϕi⟩MPnϕi
≤∞X
i=1h(0)(λi) 
⟨Pnf,ϕϵ
i,n⟩Gnϕϵ
i,n− ⟨Pnf,ϕϵ
i,n⟩GnPnϕi
+∞X
i=1h(0)(λi) 
⟨Pnf,ϕϵ
i,n⟩GnPnϕi− ⟨f,ϕi⟩MPnϕi
(44)From the convergence stated in Theorem 4, we have
∥aiϕϵ
i,n−ϕi∥ ≤Ω2,K√ϵ/θ, (45)
with the eigengap θ≥αunder the α-FDT filter. Therefore, the
first term in (44) can be bounded as
∞X
i=1h(0)(λi) 
⟨Pnf,ϕϵ
i,n⟩Gnϕϵ
i,n− ⟨Pnf,ϕϵ
i,n⟩MPnϕi
≤NsX
i=1∥Pnf∥∥ϕϵ
i,n−Pnϕi∥ ≤NsΩ2,K
α√ϵ.(46)
The last equation comes from the definition of norm in L2(Gn).
The second term in (44) can be written as
∞X
i=1h(0)(λϵ
i,n)(⟨Pnf,ϕϵ
i,n⟩GnPnϕi− ⟨f,ϕi⟩MPnϕi)
≤∞X
i=1|h(0)(λϵ
i,n)|⟨Pnf,ϕϵ
i,n⟩Gn− ⟨f,ϕi⟩M∥Pnϕi∥.
(47)
Because {x1, x2,···, xn}is a set of uniform sampled points
fromM, based on Theorem 19 in [53] we can claim that
⟨Pnf,ϕϵ
i,n⟩Gn− ⟨f,ϕi⟩M=O r
logn
n!
. (48)
Taking into consider the boundedness of frequency response
|h(0)(λ)| ≤1and the bounded energy ∥Pnϕi∥. Therefore, we
have
∞X
i=1ˆh(λϵ
i,n) 
⟨Pnf,ϕϵ
i,n⟩Gn− ⟨f,ϕi⟩M
Pnϕi=O r
logn
n!
.
Combining the above results, we can bound the output
difference of h(0). Then we need to analyze the output difference
ofh(l)(λ)and bound this as
Pnh(l)(L)f−h(l)(Lϵ
n)Pnf
≤(ˆh(Cl) +γ)Pnf−(ˆh(Cl)−γ)Pnf≤2γ∥Pnf∥,(49)
where h(l)(L)andh(l)(Lϵ
n)are filters with filter function h(l)(λ)
on the LB operator Land graph Laplacian Lϵ
nrespectively.
Combining the filter functions, we can write
∥Pnh(L)f−h(Lϵ
n)Pnf∥
=Pnh(0)(L)f+PnX
l∈Kmh(l)(L)f−
h(0)(Lϵ
n)Pnf−X
l∈Kmh(l)(Lϵ
n)Pf(50)
≤ ∥Pnh(0)(L)f−h(0)(Lϵ
n)Pnf∥+X
l∈Km∥Pnh(l)(L)f−h(l)(Lϵ
n)Pnf∥ (51)
≤AhΩ1,K√ϵ+NsΩ2,K
α√ϵ+Nmγ+Cgcr
log(n)
n14
With γ= Ω 2,K√ϵ/α, we have
∥h(Lϵ
n)Pnf−Pnh(L)f∥
≤NΩ2,K
α+AhΩ1,K√ϵ+Cgcr
logn
n(52)
We can prove Theorem 2 similarly by importing Proposition
4 into the eigenvalue and eigenfunction differences.1
Supplemental Materials
A. Proof of Proposition 1
We decompose the operator difference between the graph
Laplacian and the LB operator with an intermediate term Lϵ,
which is the functional approximation defined in (20). We first
focus on the operator difference between LϵandL. From [45],
we can get the bound as
∥Lϵϕi− Lϕi∥ ≤C√ϵ∥ϕi∥Hd/2+1, (53)
For the Sobolev norm of eigenfunction ϕi, according to [54,
Lemma 4.4] we have
∥ϕi∥Hd/2+1≤Cλd+2
4
i, (54)
which leads to
∥Lϵϕi− Lϕi∥ ≤C1√ϵλd+2
4
i. (55)
For the operator difference between Lϵ
nandLϵwith Hoeffding’s
inequality as
P(|Lϵ
nϕi(x)−Lϵϕi(x)|> ϵ1)≤exp
−2nϵ2
1
∥ϕi∥2
Hd/2+1
.
(56)
Therefore, we can claim that with probability at least 1−δ, we
have
|Lϵ
nϕi(x)−Lϵϕi(x)| ≤r
ln 1/δ
2n∥ϕi∥Hd/2+1. (57)
Combining (55) and(57) with triangle inequality, we can get
the conclusion in Theorem 1.
B. Proof of Proposition 2
We first import two lemmas to help prove the spectral
properties.
Lemma 2 Let A,B be self-adjoint operators with
{λi(A),ui}∞
i=1and{λi(B),wi}∞
i=1as the corresponding
spectrum. Let Prwibe the orthogonal projection operation
onto the subspace generated by wi. Then we have
∥aiui−wi∥ ≤2∥ui−Prwiui∥ ≤2∥Bui−Aui∥
minj̸=i|λj(B)−λi(A)|.
(58)
Proof. The first inequality is directly from [53, Proposition 18].
LetPr⊥
wibe the orthogonal projection onto the complement of
the subspace generated by wi. Then we have
∥ui−Prwiui∥=∥Pr⊥
wiui∥=X
j̸=i⟨ui,wj⟩wj.(59)Therefore, we have
∥Pr⊥
wiBui−Pr⊥
wiAui∥
=X
j̸=i⟨Bui,wj⟩wj−X
j̸=i⟨Aui,wj⟩wj (60)
=⟨ui,Bwj⟩wj−X
j̸=iλi(A)⟨ui,wj⟩wj (61)
=X
j̸=i(λi(B)−λi(A))⟨ui,wj⟩wj (62)
≥min
j̸=i|λi(B)−λi(A)|∥X
j̸=i⟨ui,wj⟩wj∥ (63)
= min
j̸=i|λi(B)−λi(A)|∥ui−Prwiui∥, (64)
together with ∥Bui−Aui∥ ≥ ∥ Pr⊥
wiBui−Pr⊥
wiAui∥. We
can conclude the proof. The following lemma is adapted
from [44, Lemma 5c]
Lemma 3 Let A,B be self-adjoint operators with
{λi(A),ui}∞
i=1and{λi(B),wi}∞
i=1as the corresponding
spectrum. Then we have
|λi(A)−λi(B)|=⟨(A−B)ui,wi⟩
|⟨ui,wi⟩|≤∥(A−B)ui∥
|⟨ui,wi⟩|(65)
With the above lemmas and our proposed Theorem 1, which
includes the operator difference, we can prove Theorem 2. We
first fix some K∈N, wich provides an upper bound for λi≤
λKfor all 1≤i≤K. By taking the probability 1−n−2and
ϵ=n−1/(d+4), we can conclude that the operator difference
in Theorem 1 can be bounded with order O(√ϵ), with the
constant scaling with λd+2
4
K. Combine with Lemma 2 and θ=
min1≤j̸=i≤K|λj−λϵ
i,n|, we can get
∥aiϕϵ
i,n−ϕi∥ ≤Ck
θ√ϵ, (66)
where we denote the constant as Ω1to include the effects of
K, the eigengap and the volume of M.
This upper bound of the eigenfunction difference leads to
|⟨ui,wi⟩| ≥1−Ω1/2√ϵ≥1. Combining with Lemma 3, the
difference of the eigenvalues can also be bounded in the order
ofO(√ϵ).
C. Proof of Theorem 3
To bound the output difference of MNNs, we need to write
in the form of features of the final layer
∥Φ(H,Lϵ
n,Pnf)−PnΦ(H,L, f))∥=FLX
q=1xq
n,L−FLX
q=1Pnfq
L
≤FLX
q=1xq
n,L−Pnfq
L. (67)
By inserting the definitions, we have
xp
n,l−Pnfp
l
=σ
Fl−1X
q=1hpq
l(Lϵ
n)xq
n,l−1
−Pnσ
Fl−1X
q=1hpq
l(L)fq
l−1

(68)2
withxn,0=Pnfas the input of the first layer. With a
normalized point-wise Lipschitz nonlinearity, we have
∥xp
n,l−Pnfp
l∥ ≤Fl−1X
q=1hpq
l(Lϵ
n)xq
n,l−1−PnFl−1X
q=1hpq
l(L)fq
l−1
(69)
≤Fl−1X
q=1hpq
l(Lϵ
n)xq
n,l−1−Pnhpq
l(L)fq
l−1
(70)
The difference can be further decomposed as
∥hpq
l(Lϵ
n)xq
n,l−1−Pnhpq
l(L)fq
l−1∥
≤ ∥hpq
l(Lϵ
n)xq
n,l−1−hpq
l(Lϵ
n)Pnfq
l−1
+hpq
l(Lϵ
n)Pnfq
l−1−Pnhpq
l(L)fq
l−1∥ (71)
≤hpq
l(Lϵ
n)xq
n,l−1−hpq
l(Lϵ
n)Pnfq
l−1
+hpq
l(Lϵ
n)Pnfq
l−1−Pnhpq
l(L)fq
l−1(72)
The second term can be bounded with ∥h(Lϵ
n)Pnf−
Pnh(L)f∥L2(Gn)≤∆fil,n. The first term can be decomposed
by Cauchy-Schwartz inequality and non-amplifying of the filter
functions as
xp
n,l−Pnfp
l≤Fl−1X
q=1∆fil,n∥xq
n,l−1∥+Fl−1X
q=1∥xq
l−1−Pnfq
l−1∥,
(73)
where Cperrepresenting the constant in the error bound of
manifold filters in (33). To solve this recursion, we need to
compute the bound for ∥xp
l∥. By normalized Lipschitz continuity
ofσand the fact that σ(0) = 0 , we can get
∥xp
l∥ ≤Fl−1X
q=1hpq
l(Lϵ
n)xq
l−1≤Fl−1X
q=1∥hpq
l(Lϵ
n)∥∥xq
l−1∥
≤Fl−1X
q=1∥xq
l−1∥ ≤l−1Y
l′=1Fl′F0X
q=1∥xq∥. (74)
Insert this conclusion back to solve the recursion, we can get
xp
n,l−Pnfp
l≤l∆fil,n l−1Y
l′=1Fl′!F0X
q=1∥xq∥. (75)
Replace lwithLwe can obtain
∥Φ(H,Lϵ
n,Pnf)−PnΦ(H,L, f))∥
≤FLX
q=1 
L∆fil,n L−1Y
l′=1Fl′!F0X
q=1∥xq∥!
.(76)
With F0=FL= 1 andFl=Ffor1≤l≤L−1, then we
have
∥Φ(H,Lϵ
n,Pnf)−PnΦ(H,L, f))≤LFL−1∆fil,n,(77)
which concludes the proof.