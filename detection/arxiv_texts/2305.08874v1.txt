R ES E A R C H A RT IC L E
Online machine-learning forecast uncertainty
estimation for sequential data assimilation
Maximiliano A. Sacco1,2| Manuel Pulido3,4| Juan J.
Ruiz2,3,5| Pierre Tandeo6,7
1Servicio Meteorológico Nacional, Buenos
Aires, Argentina
2Departamento de Ciencias de la
Atmósfera y los Océanos, Facultad de
Ciencias Exactas y Naturales, Universidad
de Buenos Aires, Buenos Aires, Argentina
3CNRS-IRD-CONICET-UBA, Instituto
Franco-Argentino para el Estudio del Clima
y sus Impactos (IRL 3351 IFAECI), Buenos
Aires, Argentina
4Departamento de Física - Facultad
Ciencias Exactas y Naturales y Agrimensura,
Universidad Nacional del Nordeste,
Corrientes, Argentina
5Centro de Investigaciones del Mar y la
Atmósfera, Facultad de Ciencias Exactas y
Naturales, Universidad de Buenos Aires,
CONICET-UBA, Buenos Aires, Argentina
6IMT Atlantique, Lab-STICC, UMR CNRS
6285, 29238, France
7Odyssey, Inria/IMT, France
Correspondence
msacco@smn.gob.ar
Email: msacco@smn.gob.ar
Funding informationQuantifying forecast uncertainty is a key aspect of state-of-
the-art numerical weather prediction and data assimilation
systems. Ensemble-based data assimilation systems incor-
porate state-dependent uncertainty quanti/uniFB01cation based on
multiple model integrations. However, this approach is de-
manding in terms of computations and development. In
this work a machine learning method is presented based
on convolutional neural networks that estimates the state-
dependent forecast uncertainty represented by the fore-
cast error covariance matrix using a single dynamical model
integration. This is achieved by the use of a loss function
that takes into account the fact that the forecast errors are
heterodastic. The performance of this approach is exam-
ined within a hybrid data assimilation method that com-
bines a Kalman-like analysis update and the machine learn-
ing based estimation of a state-dependent forecast error
covariance matrix. Observing system simulation experiments
are conducted using the Lorenz’96 model as a proof-of-concept.
The promising results show that the machine learning method
is able to predict precise values of the forecast covariance
matrix in relatively high-dimensional states. Moreover, the
hybrid data assimilation method shows similar performance
to the ensemble Kalman /uniFB01lter outperforming it when the
ensembles are relatively small.
1arXiv:2305.08874v1  [physics.ao-ph]  12 May 20232 Sacco M. A. et al.
K E Y W O R D S
neural network, data assimilation, uncertainty estimation,
covariance estimation
1|INTRODUCTION
Quantifying forecast uncertainty is a key aspect of data assimilation (DA) systems. In particular most DA methods rely
on an accurate estimation of the forecast mean and error covariance matrix. Together they describe the probability
density function under the assumption that errors are unbiased and Gaussian.
Data assimilation approaches such as optimal interpolation (OI, Gandin 1965) or 3-dimensional variational meth-
ods (3DVar, Parrish and Derber 1992) assumes that the forecast error covariance matrix is independent of the state
of the system. Currently, DA methods that provide an implicit (e.g. 4-dimensional varitional methods, 4DVAR, Rabier
et al. 2000) or explicit (e.g. ensemble Kalman /uniFB01lters, EnKFs, Houtekamer and Zhang 2016, particle /uniFB01lters, PFs, van
Leeuwen et al. 2019) or hybrid (Bannister, 2017), estimation of the state dependent forecast probability density func-
tion produce a remarkable improvement in the accuracy of the initial conditions and of the forecast skill (Kalnay, 2003;
Carrassi et al., 2018). However, these improvements come at the expense of a signi/uniFB01cant increase in the computa-
tional cost. Moreover, even when state-dependent error covariances are well represented, an accurate estimation of
the contribution of model errors to the forecast error covariance in 4Dvar and EnKF frameworks is still challenging
(Tandeo et al., 2020).
Recently, machine learning methods—trainable statistical models that can represent complex functional depen-
dencies among di/uniFB00erent groups of variables given a large enough dataset— have emerged as a promising alternative
to estimate the forecast uncertainty (e.g. Tandeo et al. 2015; Ouala et al. 2018; Wang et al. 2018; Camporeale et al.
2019; Grönquist et al. 2019; Irrgang et al. 2020; Grooms 2021; Sacco et al. 2022, among others). These methods
provide an accurate estimation of the forecast uncertainty at a relatively low computational cost (i.e., without the
need of multiple integrations of the numerical model or its adjoint). Moreover, apart from the forecast uncertainty
quanti/uniFB01cation, some of these methods capture also an estimation of the uncertainty associated with model errors,
which are di/uniFB03cult to estimate (e.g., Camporeale 2018; Ouala et al. 2018; Wang et al. 2018; Sacco et al. 2022). These
methods rely on uncertainty-aware loss functions allowing the ML algorithms to learn the error statistics directly from
the data (see for example, Bishop 2006, Chapter 5.6).
Most of these works have focused on the estimation of the forecast error variance (e.g. Wang et al. 2018; Campo-
reale 2018; Grönquist et al. 2019; Irrgang et al. 2020; Sacco et al. 2022 among others). However, the estimation of the
full error covariance structure is essential for data assimilation. Grooms (2021) estimated the full covariance structure
based on a machine learning method designed to provide an ensemble of perturbations of the state variables that rep-
resents possible realizations of the forecast error. This approach emulates the one used in ensemble forecasting but
without the need to integrate the computationally demanding numerical model to generate the ensemble members.
Lguensat et al. (2017) replace the numerical model for a surrogate model based on a local linear analog regression,
thus signi/uniFB01cantly reducing the computational cost associated with the numerical integration of the ensemble. Ouala
et al. (2018) use a neural network and a Gaussian likelihood based loss function to estimate a diagonal error covariance
in a subspace de/uniFB01ned by the leading principal components of the state variables resulting in an approximation of the
full forecast error covariance.
The estimation of a full error covariance matrix from data has been investigated in other contexts. Williams
(1996) used a neural network to estimate the parameters of a multivariate Gaussian distribution. Hu and KantorSacco M. A. et al. 3
(2015) presented a parametric covariance prediction for heteroscedastic noise and Liu et al. (2018) implemented a
deep learning model for the inference of the observation error covariance matrix and applied it to position estimation
for navigation applications. In these cases, a Cholesky decomposition of the covariance matrix is estimated based on
the Gaussian likelihood.
The use of machine learning (ML) techniques in the context of data assimilation have been discussed in several
works. The similarities between DA and ML and their potential synergism has been introduced in Hsieh and Tang
(1998) and reviewed in Cheng et al. (2023). Bocquet et al. (2019); Brajard et al. (2020); Farchi et al. (2021a, 2022)
proposed a framework in which machine learning is used for the estimation of the system dynamics and to represent
model errors, while data assimilation provides an online continuous optimization of the data-driven model. Along the
same line, Brajard et al. (2021); Farchi et al. (2021b), use a data assimilation approach to train an ML-based parameteri-
zation of the e/uniFB00ect of unresolved scale dynamics within a numerical model. Other approaches aimed to a replacement
of the full DA system by a neural network as in Härter and de Campos Velho (2008). In this approach the authors
train a neural network that learns, from a given DA system, the magnitude and spatial patterns of the state update
introduced by the observations. Buizza et al. (2022) introduced the name "data learning" to describe several examples
in which ML and DA can be combined to overcome their mutual weaknesses.
Relatively few works investigated how ML-based forecast error covariance estimation, can be coupled with a DA
system (e.g., Lguensat et al. 2017; Ouala et al. 2018). In particular, Ouala et al. (2018), coupled a neural network-
based estimation of the forecast error covariance with a Kalman like analysis update in a high dimensional state
space and compared the results with an ensemble Kalman /uniFB01lter approach and the analog-based approach of Lguensat
et al. (2017) with promising results. In this work. we investigate the impact on the quality of the estimation of
the state of a dynamical system, particularly when a localized version of the full error covariance matrix is directly
estimated using an extension of the novel loss function presented in Sacco et al. (2022). The neural network covariance
estimation uses a single forecast as input variable which is obtained by means of a numerical model. The stability of the
method is investigated by performing several assimilation cycles. This (U)ncertainty estimation with neural networks
is integrated with a Kalman /uniFB01lter-based data assimilation system, forming a hybrid technique referred as UnnKF.
This work is structured as follows: Section 2 describes the di/uniFB00erent approaches for the estimation of the forecast
error covariance including a brief review of the ensemble Kalman /uniFB01lter and the experimental settings. The design of all
the experiments that were carried out in this work are described in Section 3. Section 4 analyzes the results obtained.
Section 5 draws the main conclusions of this work as well as a discussion of future perspectives.
2|METHODOLOGY
2.1 |Sequential data assimilation
In a sequential data assimilation cycle, we aim to estimate the state of a dynamical system at regular time intervals,
by combining the information provided by a surrogate numerical model and a set of partial and noisy observations
(Carrassi et al., 2018). We start by considering a chaotic dynamical system, represented via the following Markov
process
xk=Mk:k ¹xk ºýk; (1)
where xkis anNx-dimensional vector representing the state of the system at time k,Mk:k is a known nonlinear
and chaotic imperfect model of the system dynamics that maps the state at time k into timek, andkrepresents4 Sacco M. A. et al.
the discrepancy between xkandMk:k ¹xk ºdue to the model imperfection (i.e., the model error). In this work, we
assume that the model error is a random variable sampled from a Gaussian probability distribution.
Given a pointwise estimation of the state of the system at time k (xa
k ) a deterministic forecast of the state
at timekcan be obtained by integrating the dynamical model and neglecting model errors,
xf
k=Mk:k ¹xa
k º; (2)
Forecasts for longer lead times can be obtained by a recursive application of the numerical model. The forecast error
can be de/uniFB01ned as:
f
k=xf
k xt
k; (3)
where xt
kis the unknown true state of the system at time k. Forecast errors are the consequence of an imperfect
estimation of the state of the system at time k and model errors. The magnitude and structure of both contributions
to the forecast error depend strongly on the state, so that the structure and magnitude of the component of the
forecast error covariance matrix at time k(Pf
k=»f
kf
k>¼) are a function of the state. Data assimilation methods rely
on the assumption that these errors have zero-mean, which is not usually the case.
The state of the system is related to the observable quantities through the observation equation,
yk=H¹xt
kºýk; (4)
where ykis theNy-dimensional vector containing the observable quantities, His the observation operator (i.e. the
function mapping state variables into the observation space) and kis the observation error which is assumed to be
drawn from a Gaussian distribution with zero-mean and known covariance denoted Rk.
Given the forecast ( xf
k), a set of observations ( yk), and assuming that their errors are unbiased , the best linear
estimator that minimizes the root mean square error with respect to the true state of the system is given by:
xa
k=xf
kýK¹yk H¹xf
kºº; (5a)
K=Pf
kH>¹HPf
kH>ýRkº ; (5b)
where xa
kis the estimation of the system state (a.k.a the analysis) at time k,His the tangent linear approximation
of the observation operator and Kis the Kalman gain matrix which projects and weights the discrepancy between
the observations and the forecasted observed quantities into the state space. This estimate of the state is also the
maximum likelihood estimation of the state of the system under the assumption that the PDFs of the forecast errors
and observation errors are both zero mean and Gaussian (Carrassi et al., 2018). Depending on the forecast covariance,
Pf
k, Eq. 5 may represent an optimal interpolation or an extended Kalman /uniFB01lter. In the optimal interpolation approach,
Pf
kis usually assumed to be known a priori and state-independent, while in the extended Kalman /uniFB01lter, the time
evolution ofPf
kis computed using the tangent linear approximation of the numerical model.
Once an estimation of the system state is obtained at time k, the numerical model (Eq. 2) can be used to forecast
the state of the system for the next time, and the cycle can be repeated every time a new set of observations becomes
available. The accuracy of the state estimation depends strongly on the accuracy of the error covariance matrices Pf
kSacco M. A. et al. 5
andRkwhose estimation is arguably one of the most challenging aspects of DA systems (Tandeo et al., 2020).
2.2 |The ensemble Kalman /uniFB01lter
The ensemble Kalman /uniFB01lter (EnKF) is one of the most broadly used methods to incorporate the state-dependence
of the forecast error covariance matrix in data assimilation applications. In this work, the EnKF is used to generate
the database for the training of the machine learning method and is used as a benchmark for the evaluation of the
proposed machine learning-based algorithms. For completeness, we brie/uniFB02y describe this technique here.
If we have a sample of states drawn from the probability distribution of the analysis state at time k ¹xa;¹nº
k º, for
n2:::NewithNethe ensemble size, the sample covariance of the forecast at time kcan be estimated by evolving
the individual ensemble members from time k to timekthrough the non-linear model equations:
xf;¹nº
k=M¹nº
k:k 
xa;¹nº
k 
ý ß¹nº
k; (6)
where xf;¹nº
kare the evolved ensemble members. The forecast ensemble mean at time k,xf
k=
NeÍNe
n=xf;¹nº
kprovides
a pointwise estimation of the state. Along this line, the forecast error covariance can be estimated from the forecast
state sample,
ßPf
k=
¹Ne ºNeÕ
n=
xf;¹nº
k xf
k 
xf;¹nº
k xf
k>
: (7)
In the stochastic implementation of the EnKF (Burgers et al., 1998), the ensemble members are updated using
Equation 5a, in which ykis replaced by y¹nº
k=yký¹nº
k, with¹nº
kN¹;Rkºand with Pf
kgiven by Eq. 7.
In physical systems, the covariance between variables corresponding to locations that are far away in physical
space are close to zero. In the EnKF, due to the presence of sampling errors, covariances between distant variables
can be signi/uniFB01cantly di/uniFB00erent from 0, particularly when a small ensemble is used. In this case, a covariance localiza-
tion approach can be used to damp the magnitude of the spurious covariances. These methods usually multiply the
estimated covariances by a factor that decreases with the physical distance between the two variables (Hamill et al.,
2001).
In this work, the stochastic EnKF was chosen over deterministic /uniFB01lters such as the LETKF (Hunt et al., 2007) since
in these /uniFB01lters ensemble members are not equi-probable since some members are persistently associated with larger
departures from the ensemble mean. This e/uniFB00ect has already been reported by Amezcua et al. (2012) and found in a
realistic experiment by Kondo and Miyoshi (2019). This a/uniFB00ects negatively the training of the neural network models
used in this work. The stochastic EnKF, because of the random sampling in the update of each ensemble member,
does not su/uniFB00er from this problem. Also, we note that the /uniFB01ne-tuned localized stochastic EnKF and the LETKF had
the same performance in terms of RMSE in the conducted experiments.
2.3 |Uncertainty estimate with neural network for data assimilation
The likelihood function of the Gaussian distribution may be used as a loss function to train a neural network to learn the
state-dependent covariance matrix. However, estimating a full error covariance matrix is di/uniFB03cult and computationally
expensive to train due to the covariance matrix inversion in the evaluation of the likelihood function. The use of6 Sacco M. A. et al.
the Cholesky decomposition of the covariance matrix or its inverse, to ensure that the obtained matrix is positive
semide/uniFB01nite, have been proposed (e.g. Williams 1996; Liu et al. 2018; Hu and Kantor 2015) along with the de/uniFB01nition
of the cost function in terms of the precision matrix to avoid performing the inversion of the covariance matrix in
its computation. However, in preliminary experiments, the covariance estimated in this way su/uniFB00ers from serious
numerical instability problems when coupled with a data assimilation cycle with state space dimensions in the order
of. An alternative solution was proposed by Ouala et al. (2018) who assumes the covariance matrix to be diagonal
in the space de/uniFB01ned by the leading principal components of the state variables. In this space the problem reduces to
the estimation of the variance while a full covariance matrix can be obtained in the original state space.
2.3.1 |Extended-MSE loss-function for covariance estimation
The loss function we use was originally presented in Sacco et al. (2022) for variance estimation. The name extended-
MSE or simply eMSE was originally proposed because this technique uses the mean squared error equation for training,
but instead of using the training target directly, it uses an on-line estimate of the forecast error. In this work, we extend
the use of this loss function for a full covariance estimation and we use it into a DA framework.
The estimation of the forecast error requires an approximation of the true state (Eq. 3),
f
kxf
k ßxt
k: (8)
The approximation of the true state ßxt
kcould be taken to be, for instance, the mean analysis provided that the
analysis error is much smaller than the forecast error (i.e., the analysis is closer to the true state than the forecast).
We note that under this approximation the trace of the analysis covariance is assumed to be signi/uniFB01cantly smaller than
the forecast covariance. Further choices of proxies for model forecast error are discussed in Section 3.3 and will be
evaluated in the experiments.
This forecast error can be used to generate a state-dependent training matrix as
f
k¹f
kº>=¹xf
k ßxt
kº¹xf
k ßxt
kº>: (9)
The predicted covariance by the neural network is represented by
Ûk=FNN¹xf
k;xa
k ;º; (10)
whereFNNis the neural network, its parameters and fxf
k;xa
k gits input data. In Sacco et al. (2022), it was shown
that usingfxf
k;xa
k gas inputs to the network improved the estimation of the mean and the variance of the state
variables with respect to fxf
kg. Similar results were obtained for the estimation of the full forecast covariance matrix
Ûk(not shown).
Then, the loss function used for training (schematized in Figure 1) is the square of Frobenius norm between the
neural network output Ûk, and the training target f
k¹f
kº>,
L¹f
k;Ûkº=Ûk f
k¹f
kº>
F=Nx;NxÕ
i;j=¹Û¹i;jº
k »f
k¹f
kº>¼¹i;jºº: (11)
In the EnKF method, localization methods in the covariance matrix are used to alleviate sampling error. The sameSacco M. A. et al. 7
idea can be used to /uniFB01lter out spurious covariances between distant variables in the estimated covariance by applying
a localization matrix Cto the training target to force the decay of the estimated covariances with increasing distance
in the physical space. As in EnKF, the structure of the localization matrix is a design decision based on knowledge of
the dynamics of the problem or on empirical results. Based on this idea, the loss function is modi/uniFB01ed to
L¹f
k;Ûkº=kÛk C»f
k¹f
kº>¼kF (12)
where matrixCis assumed to be known a priori from the knowledgement of the dynamical interactions and selects
the elements of the covariance matrix that will be estimated by the network, and is the element-wise product.
Consistently, all the elements of the output matrix ( Ûk) corresponding to 0 values in Care removed by varying the
size of the output layer of the network (see Sec. 3.2). In this way, we reduce the number of training parameters and
limit the computation of the covariances to only the selected subdiagonals (i.e. Ûkmay be represented by a band
matrix).
FIGURE 1 ANN training scheme. The training error is determined by the Frobenius norm between Ûkand the
training matrix f
k¹f
kº>which is estimated from the approximated forecast error.
2.3.2 |Data assimilation process
Figure 2 shows a schematic representation of the hybrid data assimilation cycle. At each assimilation cycle, the
numerical model is initialized with the analysis of the previous cycle ( xa
k ) providing a deterministic forecast state,
xf
k=M¹xa
k º. The forecast and its corresponding analysis are used as inputs to the neural network to obtain an
estimation of the forecast error covariance Pf
kÛk=FNN¹xa
k ;xf
k;ºwhich we plug into Eq. 5a to obtain the
analysis at time k,xa
k. This in turn is used as initial condition to produce the forecast for the next assimilation cycle.
This approach uses a single forecast from a numerical dynamical model to propagate the information on the state
of the system from time k to timek(as in optimal interpolation or 3-dimensional variational approaches), but it
uses a time-dependent estimation of the forecast error covariance matrix as in the EnKF. However, instead of using
an ensemble of forecasts, the full covariance matrix is estimated by a neural network.
The analysis update given by Eq. 5a is quite sensitive to the quality of the estimated forecast error covariance.
For instance, if the diagonal terms are overestimated, the analysis tends to over/uniFB01t the observations. In addition, if
the subdiagonal elements are not well estimated, the information of the observations does not propagate properly
to the unobserved variables of the system. In the optimal interpolation or 3-dimensional variational approaches, it
is assumed that the covariance of the forecast does not depend on the state of the system. In the EnKF, the state
dependence is taken into account, but sampling errors due to a limited ensemble size can a/uniFB00ect its accuracy (Hamill
et al., 2001). In the case of UnnKF, the quality of the covariance will be determined by the ability of the neural network
to learn the relationship between the state of the system and the associated uncertainty, in our case, the covariance.8 Sacco M. A. et al.
FIGURE 2 Schematic representation of an UnnKF assimilation cycle (see the text for details).
In Sacco et al. (2022) the estimation of the forecast error variance was done in combination with an estimation
of the state-dependent forecast bias. In this work, our main goal is to evaluate the accuracy and e/uniFB00ectiveness of the
covariance estimation in the context of a sequential data assimilation system. Although the forecast bias correction
could improve the performance of the assimilation, we do not include it as part of the experiments in this work, since
it could hide the sensitivity of the analysis error to the accuracy of the covariance estimation. In other words, all
the improvements with respect to a /uniFB01xed covariance optimal interpolation in this work can be ascribed to the neural
network covariance estimation.
3|EXPERIMENTAL DESIGN
3.1 |Dataset generation
For the generation of the datasets used to train the neural networks and to validate their performance we used a
simpli/uniFB01ed data assimilation system based on the Lorenz’96 (Lorenz, 1995) dynamical model. This is a simple chaotic
model widely used in proof-of-concept experiments in the data assimilation community (e.g. Stanley et al. 2021;
Brajard et al. 2020; Lguensat et al. 2017; Terasaki and Miyoshi 2014).
In particular, the two-scale Lorenz model (Lorenz, 1995) is used to represent the evolution of the unknown nature
state. This two-scale system allows us to represent the essence of multiple spatio-temporal scale systems such as the
atmosphere or the ocean. The large and small-scale dynamical variables are governed by
dx¹iº
dt= x¹i º¹x¹i º x¹i ºº x¹iºýF hc
biJÕ
j=J¹i ºýy¹jº
dy¹jº
dt= cby¹jýº¹y¹jýº y¹j ºº cy¹jºýhc
bx¹int»¹j ºJ¼ýº;(13)
wherex¹iºis thei thcomponent of the slow dynamics state vector x, andy¹jºis thej thcomponent of the fast-
dynamics state vector, with Jthe number of yvariables for each xvariable. The coupling between the two systems
is controlled by the time-independent parameters h= ,c=  , andb=  . Both sets of equations have cyclic
boundary conditions, namely x¹º=x¹Sýº, andy¹º=y¹JSýº. For most of our experiments the number of state
variables are J=32 and S=100 (i.e., the yvector has a total of 3200 variables) and to obtain a chaotic behavior, the
forcing term Fis set to 26.
The one-scale Lorenz system,
dx¹iº
dt= x¹i º¹x¹i º x¹i ºº x¹iºýFýG¹iº; (14)Sacco M. A. et al. 9
is used as a surrogate model to estimate the true system state from an incomplete set of noisy observations using
an ensemble-based data assimilation method. This introduces model error into our data assimilation and forecasting
system since one of the scales is not explicitly represented.
The e/uniFB00ect of the missing dynamics (i.e., the e/uniFB00ect of fast variables y) in the surrogate model is approximated by
a state dependent parametrization term. As in Pulido et al. (2016), G¹iºis assumed to be a linear function of the state
variablex¹iº:
G¹iº=x¹iºý; (15)
with= :and= :constant parameters whose optimal values are taken from Sche/uniFB04er et al. (2019).
The observations were generated from the nature integration every 8 time steps adding a Gaussian error of zero
mean and variance equal to :. Observations are available at odd grid points (i.e., only 50% of the system is observed).
Given the observations set and the forecasting model, we used the EnKF methodology described in Section 2.2 to
generate a set of assimilated states xa
kthat is our best approximation to the real state of the system.
Two sets of analyses were generated, one using a 100-member ensemble and the other using a 5-member ensem-
ble. In both cases, a localization function was used to reduce the impact of sampling errors. The localization functions
follows the one suggested in Gaspari and Cohn (1999) with a localization scale of 7 grid points which was found to
minimize the RMSE of the analyses. These two sets of analyses were used independently to train the neural networks
for each experiment as explained in the following sections and as a baseline for analyzing the results. The in/uniFB02ation
factor was also tuned to give minimum RMSE, resulting in an optimal in/uniFB02ation factor of :for the 100-member
ensemble and :for the 5-member ensemble experiment.
The training set consists of 10000 analysis cycles and the validation set has 5000 cycles. The size of the training
and validation set is such that converting the Lorenz model time units to atmospheric times is equivalent to 10 years of
data. The testing set consists of 15000 time steps which are completely independent from the training and validation
sets.
3.2 |NN architecture and training
The neural network architecture consists of three convolutional layers (see Table 1). The size of the kernels are rela-
tively small (3 grid points) allowing the identi/uniFB01cation of patterns in a restricted locality. A kernel width of 5 was also
tested but did not result in a better performance that would justify the increase in the network complexity. This is
consistent with the behavior of the Lorenz variables that present localized interactions, i.e. two variables that are
far from each other have weak interactions. Furthermore, translation invariance is assumed in the convolution lay-
ers, which is in accord with the statistical isotropy of the Lorenz’96 dynamics. The size of the output layer depends
on the number of subdiagonals of the forecast covariance matrix that are estimated, which in turns depends on the
localization matrix Cin Eq. 9.
For the experiments, we make a simple choice for the localization function C. We use a Heaviside function to
localize the elements of the target covariance. If the distance between two Lorenz variables is less than dgrid points,
we leave the corresponding covariance unchanged, and if it is larger than dwe set the corresponding covariance to
. This is equivalent to keep only the /uniFB01rst ndsubdiagonals of the target covariance. In this case we construct our
neural-network model to estimate the /uniFB01rst ndsubdiagonals of the covariance matrix, while all other subdiagonals are
assumed to be (Figure 3). Sensitivity experiments were carried out to determine the number of subdiagonals needed
to optimize the RMSE and to compare this localization value with the optimal localization scale in the EnKF.10 Sacco M. A. et al.
FIGURE 3 The output of the network is shown in panel (a), theithelement of channel is the variance
corresponding to the ithstate variable. The dthchannel corresponds to the covariance between the state variables
separated by a distance das shown in panel (b). Covariance values not represented with the ANN output are
assumed to be zero.
layer in channel out channel kernel activation
input 2 32  Softplus
hidden 32 32  Softplus
output 32 nd  Softplus(n)+linear(n:nd)
TABLE 1 Description of the architecture of the convolutional neural network used in the experimentation. The
number of channels in the output layer ( nd) is the number of subdiagonals to be estimated in the covariance matrix.
As stated in Table 1 a single hidden convolutional layer is used. The inclusion of an extra hidden layer did not pro-
duce a signi/uniFB01cant improvement in performance. This convolutional layer use circular padding, since this is consistent
with the boundary conditions of the Lorenz model and allows us to keep unchanged the dimensions of the spatial
representation through the network. Softplus was chosen as the activation function for the /uniFB01rst two convolutional
layers since it produces a slightly better convergence among other considered activation functions (viz. logistic and
ReLU). In the output layer we use two activation functions: A Softplus function for the output elements corresponding
to the main diagonal ( n) so the estimated variances are positive, and a linear activation function for the elements cor-
responding to the covariances (subdiagonal elements of the covariance matrix n: : :nd). In preliminary experiments,
we observed that using linear or ReLU as the activation function in the main diagonal ( n) may lead to the estimation
of negative variances or variances equal to zero respectively.
The AdamW optimizer (Loshchilov and Hutter, 2017) was used to train all the networks with a learning-rate value
of:. The use of mini-batches of samples produces the best convergence in training. During the training, the
loss function is evaluated over the validation set every training epochs and the training stops when the loss function
evaluated over the validation set stops decreasing or starts to increase (early stop with patience).
Selecting an appropriate network architecture and training hyper-parameters resulted in a challenging task. Net-
work convergence was relatively easy to achieve, however the main di/uniFB03culty consists on /uniFB01nding a training hyper-
parameter set that results in an stable data assimilation cycle using the network estimated covariance. When esti-
mating the forecast covariance, over/uniFB01tting can be particularly strong since a very noisy target is used and we want
the network to /uniFB01lter out the noise and generate a robust estimate of the state-dependent covariance. The risk of
over/uniFB01tting in this setting is much higher than, for example, in the estimation of the mean error. To reduce the riskSacco M. A. et al. 11
of over/uniFB01tting of the training sample, we use a Lregularization (weight decay) approach. We noticed that for high
values of weight-decay parameter ( >) the variability of the estimated covariance matrix smooths and /uniFB02attens to an
almost state-independent covariance matrix (i.e. the climatological covariance matrix). But once the architecture and
the rest of the hyper-parameters have been properly chosen, the best results in terms of performance are achieved
with very small values of weight-decay ( :e or even).
3.3 |Forecast error proxies
FIGURE 4 Schematic of an ensemble assimilation cycle where MRA and MMA forecast error proxies are shown
using only the i thensemble member.
To train the network that estimates the forecast error covariance, we need a dataset that expresses the spatio-
temporal variability of the error. But constructing proxies for the short range forecast error (i.e. f
k) is a challenging
task. In this work, we evaluate di/uniFB00erent possible proxies for the short range forecast errors, these are schematized in
Figure 4. Based on the available ensembles of forecast and analysis, we evaluate three possible ways to de/uniFB01ne f
k:
•Mean forecast - MeanAnalyisis ( MMA ): In this case we de/uniFB01ne f
k=xf
k Ýxa
kwhere xf
kis a deterministic forecast
initialized from the analysis ensemble mean ( xf
k=M¹Ýxa
k º). In this case, we are taking the di/uniFB00erence between
the most probable state of the system given all the observations up to time k and the most probable state of
the system given all the observations up to time k.
•Mean forecast- Random Analysis ( MRA ): The error proxy is de/uniFB01ned as f
k=xf
k xa;¹nrº
kwhere xa;¹nrº
kis a randomly
selected member from the analysis ensemble. This proxy is assuming that analysis ensemble members represents
equally probable realizations of the true state. It is important to note that once the member is randomly selected,
it remains /uniFB01xed for all training epochs, i.e. the dataset is the same in all epochs and the random selection is
done only once. In this formulation it would be possible to augment the training dataset using all the available
ensemble members. In fact, enlarging the training set in this way improved the quality of the estimated covariance,
and consequently decreased the RMSE of the analyses. However, we chose to use only one randomly selected
member for comparison purposes so that the size of the training dataset is the same to the rest of the chosen
proxy methods.
•Mean forecast- NaTure (MNT ): Since we are conducting idealized experiments, we have access to the true state
of the system, thus for evaluating purposes we can compute the true forecast error as f
k=xf
k xt
k, where xt
kis12 Sacco M. A. et al.
the true system state given by the nature run. This representation of the forecast error cannot be computed in
the real applications and is used only for comparison.
Other error approximations were evaluated, in particular f
k=xf;¹nrº
k Ýxa
kwhere xf;¹nrº
k=M¹xa;¹nrº
k ºis a randomly
selected member from the forecast ensemble and f
k=xf;¹nrº
k xa;¹nrº
k, i.e. the di/uniFB00erence between a randomly selected
member of the forecast ensemble and the corresponding member in the analysis ensemble, but none of them gave
better results than MRA .
It is important to note that di/uniFB00erent error proxies are associated with di/uniFB00erent estimated error variances. Thus,
the trace of the estimated covariance matrix using these di/uniFB00erent proxies to compute the target, can be signi/uniFB01cantly
di/uniFB00erent. To reduce the impact of this e/uniFB00ect in the assimilation cycle and to compare these di/uniFB00erent approaches in
a more consistent way, a multiplicative in/uniFB02ation factor is applied in the data assimilation experiments as in the EnKF.
The multiplicative in/uniFB02ation factor is optimized independently for each error proxy using a brute force approach.
4|RESULTS
In this section, we present the results obtained with di/uniFB00erent sensitivity experiments designed to evaluate the per-
formance of the UnnKF and to compare it to the EnKF with two di/uniFB00erent ensemble sizes, andmembers. Each
UnnKF experiment is identi/uniFB01ed with a name composed of two parts, the /uniFB01rst refers to the error proxy used in the
training (see section 3.3) and the second one is the number of subdiagonal of the covariance matrix being estimated
by the network (including the main diagonal). Data assimilation experiments performed using the EnKF are named as
"ENS" followed by the number of members in the ensemble.
We start by comparing the time evolution of the covariances used in the data assimilation for the -variable
Lorenz model with the UnnKF and EnKF methods. The EnKF method uses the sample covariances obtained using -
member (ENS5) and -member (ENS100) ensembles to which a Gaspari-Cohn function with a localisation scale of
grid points has been applied. The UnnKF method uses an ANN with nd=(MRA6) and trained with the MRA error
proxy. In all three cases the magnitude of the estimated covariances has been scaled by the optimal multiplicative
in/uniFB02ation (i.e., the one that produced the best results in terms of the analysis RMSE).
Figure 5 shows the time evolution of selected elements of the error covariance matrix as estimated from the
EnKF with di/uniFB00erent ensemble sizes and the neural network. We distinguish between odd covariance matrix rows
(centered at an observed variable (Fig. 5 left column)) and even rows (centered at an an unobserved variable, Fig. 5
right column) since their variability can be di/uniFB00erent. The temporal correlation coe/uniFB03cient of MRA6 and ENS5 with
respect to ENS100 for the entire testing set is stated at the right of each panel of Figure 5. In all cases, the correlation
coe/uniFB03cient of the MRA6 estimate is higher than the correlation of ENS5, even for those estimated covariances which
are not shown in the /uniFB01gure.
The overall analysis shows that ENS5 produces covariances with a higher temporal variability compared to ENS100
due to the e/uniFB00ect of sampling noise. In contrast, MRA6 closely follows the variability of the ENS100 for both observed
and unobserved variables. In general, MRA6 has a smoother variability than ENS100 and sometimes it seems to omit
some extremes (e.g. time 1515 for the observed variables in all covariances). But it is also able to reproduce quite ac-
curately other extremes present in ENS100 (e.g. variance and covariance at time 1525 for observed variables). Figure
5 shows that, in general, the time evolution of the covariance matrix estimated by MRA6 is closer to ENS100 than to
ENS5. This is consistent with the obtained correlation coe/uniFB03cients already mentioned.
To assess the overall quality of the spatio-temporal structure of the estimates in the context of data assimilation,Sacco M. A. et al. 13
FIGURE 5 Time evolution of selected covariance matrix elements over 100 consecutive time steps starting at
the 1500 cycle of the testing set. The panels, from top to bottom, show respectively the /uniFB01rst elements of the /uniFB01rst
column of the covariance matrix. Left panels show covariance matrix elements of an odd row (centered at observed
variables), while right panels shows covariance matrix elements of an even row (centered at an unobserved variable).
The correlation coe/uniFB03cient computed over 15.000 assimilation cycles, between ENS5 and MRA6 with respect to
ENS100 are shown to the right of each panel.
Figure 6 compares the analysis RMSE over 15,000 consecutive assimilation cycles of the testing dataset using the
UnnKF with those generated with ENS5 and ENS100. The black line on top of each bar represent the 95% con/uniFB01dence
interval computed using a bootstrap approach using 500 subsamples obtained from the testing dataset using random
selection with replacement and selecting samples which are more than 20 time steps apart from each other to increase
the independence between di/uniFB00erent sample elements.
For both the observed and unobserved variables, the RMSE obtained in Figure 6 is much closer to ENS100 than
to ENS5. This agrees with the time evolution analysis of the covariance matrix elements and shows that the proposed
methodology is able to generate a state-dependent estimate of the covariance matrix robust enough to run long
assimilation cycles, using only a deterministic forecast as input.
4.1 |Sensitivity to the forecast error proxy
Figure 7 shows the RMSE of the analysis over the test dataset for an ensamble of 100 member using EnKF and
for the UnnKF trained with the actual forecast error (MNT) and the error proxies, MRA and MMA. In all the cases
subdiagonals of the error covariance matrix are estimated. Overall the performance of the UnnKF is stable and
produce accurate results. Besides, the skill of the UnnKF is sensitive to the error proxy with roughly a % di/uniFB00erence14 Sacco M. A. et al.
FIGURE 6 RMSE of the analyses generated by a 5-member ensemble (ENS5), a 100-member ensemble
(ENS100) using ENKF method and the UnnKF method (MRA6) for the 100-variable Lorenz’96 model. The RMSE of
unobserved variables (left), observed variables (middle) and the total RMSE (right) are shown. Note that the ranges
of the RMSE axes are di/uniFB00erent in each panel, this is to highlight the di/uniFB00erence between the experiments. The limits
of a% con/uniFB01dence interval obtained using a bootstrap approach is indicated by the black line on top of each bar.
between the best (MRA) and the worst (MMA) proxies.
Using the mean analysis as the training target results in a signi/uniFB01cantly higher RMSE compared to MRA. Addition-
ally, UnnKF using MMA requires a larger multiplicative in/uniFB02ation factor to achieve optimal performance, indicating that
this proxy underestimates the amplitude of errors. This can be explained by the fact that in this case, the forecast
error is being approximated as the di/uniFB00erence between the deterministic forecast (which is close to the forecast en-
semble mean) and the analysis ensemble mean, without considering that the forecast and the real state of the system
are random realizations of these distributions. In the MRA approach, a random analysis ensemble member is chosen,
taking into account that the analysis members are equally probable realizations of the true state of the system. This
proxy overestimates the variance of the errors, leading to optimal multiplicative in/uniFB02ations that are below one.
MNT6 has the lowest analysis RMSE when the estimated covariances are used in the context of a data assimilation
system, even better than ENS100. This means that if the error is well estimated, the loss function allows a neural
network to be successfully trained to estimate the error covariance. This is quite interesting as it indicates that, given
a suitable target, the neural network is able to provide an estimate of the state-dependent uncertainty of the forecast
that is more accurate than that provided by a large ensemble, at a much lower computational cost.
FIGURE 7 The RMSE of the unobserved system variables (left panel), the observed variables (middle panel) and
the total RMSE (right panel) of the analyses generated using a separate testing dataset are shown for the networks
trained with the datasets, MNT, MRA and MMA. The limits of a % con/uniFB01dent interval obtained using a bootstrap
approach is indicated by the black line on top of each bar.Sacco M. A. et al. 15
The MRA experiment performed similarly to the ENS100 and MNT experiments for the observed variables, but
more signi/uniFB01cant RMSE di/uniFB00erences appear for the unobserved variables. Part of these di/uniFB00erences can be explained
by the e/uniFB00ect of model errors. In our experiments, both the forecasts and the analysis are a/uniFB00ected by model errors.
The assimilation of observations reduces the impact of model errors in the analysis, but some of these errors remain,
particularly in the unobserved variables, leading to a misrepresentation of these errors in our proxies.
However, the use of error proxies such as MRA or MMA leads to analysis errors comparable to those obtained
with the large ensemble and when the network is trained with the actual forecast error (the analysis RMSE increases by
3.5% for MRA and 10% for MMA compared to MNT). This suggests that, despite the limitations of these proxies, they
could give reasonable results. The use of MRA, which uses a random member of the analysis ensemble to approximate
the forecast error, produces a more accurate representation of the forecast probability distribution than the use of
the analysis ensemble mean. The reason for the better performance of the MRA proxy relative to the MMA is not
clear.
The experiment MNT gives an RMSE which is lower than ENS100. The best RMSE for MNT experiments, :
(Fig. 7), is obtained using a multiplicative in/uniFB02ation of 0.85 and a localisation function, as used in the ENS100 experi-
ment, but with a distance of 4 grid points. If no localisation function is used the obtained RMSE for MNT experiment is
: and the optimal multiplicative in/uniFB02ation is :. This indicates a slight overestimation of the distant covariances in
the MNT experiment. On the other hand, the minimum RMSE for the experiments with MMA and MRA error proxies
is achieved without applying the localisation function.
4.2 |Sensitivity to localization
We conducted another set of experiments to explore the sensitivity of the UnnKF to the number of estimated subdi-
agonals in the forecast covariance matrix ( nd) (i.e., how covariance between distant variables are correctly modeled
by the neural network and to what extent the inclusion of covariances between variables that are farther improves
the analysis accuracy). The training of these experiments was carried out using the MRA error proxy.
Figure 8 shows the analysis RMSE in the observed, unobserved and total variables as a function of nd. Overall,
the largernd, the lower the RMSE of the analysis with signi/uniFB01cant reduction of the RMSE up to nd= . The optimal
in/uniFB02ation for each experiment is consistent with the overall performance of the experiment, with larger multiplicative
in/uniFB02ation associated with the experiments with larger analysis errors. Beyond nd=the RMSE continues to decrease,
but the di/uniFB00erences are not statistically signi/uniFB01cant. This shows that the proposed training methodology is able to
capture the variability of the farthest covariances containing relevant physical interactions present in the system.
For unobserved variables, the experiment with nd= does not lead to an analysis error reduction with respect
to thend= case. This can be because, in the experiment with nd= , the number of observations used to obtain
the analysis at unobserved variables is the same as the in the nd= experiment. However, the performance on
the observed variables improves when ndis increased from 2 to 3, since the number of observations assimilated at
observed variables increases from 1 to 3. This e/uniFB00ect is schematized in Figure 9 showing that the even subdiagonals
propagate information from the observed variables to the unobserved variables, while the odd subdiagonals prop-
agate the information from the observed variables to other observed variables (information is propagated from the
observations). This e/uniFB00ect seems imperceptible from the 4th subdiagonal onward likely because of the small amplitude
of the estimated covariances.16 Sacco M. A. et al.
FIGURE 8 RMSE for band covariance matrices of di/uniFB00erent sizes f;;;;;;g, for the unobserved system
variables (left panel), the observed variables (middle panel) and the total RMSE (right panel) of the analyses
generated using a separate testing dataset are shown. The optimal in/uniFB02ation values for each experiments are
indicated to the left of the /uniFB01gure. The limits of a % con/uniFB01dent interval obtained using a bootstrap approach is
indicated by the black line on top of each bar.
FIGURE 9 Pattern of observed ( Obs) and unobserved ( NoObs ) state variables during data assimilation process
and how each covariance ( n;i) propagates information from one variable ( n) to another ( i).
4.3 |Scalability
In this section we investigate how the optimal size of the neural network (i.e. the number of convolutional /uniFB01lters in
the hidden layer) depends on the number of estimated diagonals of the error covariance matrix and on the dimension
of the state space. We perform experiments varying the size of the neural network and the number of estimated
diagonals to evaluate how this a/uniFB00ects the RMSE of the analyses. Results are shown in Table 2. It was found that the
optimal network capacity is almost insensitive to the number of estimated diagonals ( nd). Fornd= (200 output
variables) the optimal number of /uniFB01lters is and fornd= (output variables) the optimal is . Using larger
networks slightly degrades the RMSE. This suggests that adding more channels does not improve the performance.
Furthermore, the increase in output variables related to more subdiagonals in the covariance estimation does not lead
to a linear increase in the number of channels.
We also investigate the sensitivity of the analysis error to the dimension of the state space. In this work, we
explore the scalability with respect to the state space dimension, by evaluating the performance of the UnnKF for
di/uniFB00erent sizes of the state of the system S(i.e. the number of slow variables in the Lorenz’96 model). We conducted
three experiments with S=,and. For the /uniFB01rst experiment (i.e., S=8), we estimated the full covariance matrix
(nd=). For the other two experiments ( S= and), we consider nd=. These experiments are compared with a
small ensemble ( -members) and a large ensemble ( -members for S=and-member for S= and) EnKFs.Sacco M. A. et al. 17
FIGURE 10 RMSE of the analyses generated for the Lorenz’96 model of 8, 40 and 100 state variables (right,
middle and left panels respectively). Each panel shows the RMSE achieved for the EnKF methodology with a small
ensemble of 5 members (ENS5), a large ensemble of 50 members for S=8 (ENS50) and 100 members for S=40 and
S=100 (ENS100) and for the UnnKF methodology (MRA6).
number of channels nd= nd=
in the hidden layer test-loss RMSE test-loss RMSE
10 0.11816 0.4085 0.068481 0.39275
20 0.11758 0.4050 0.068203 0.37823
32 0.11748 0.4042 0.068105 0.37336
40 0.11755 0.4053 0.068130 0.37331
50 0.11766 0.4044 0.068115 0.37335
TABLE 2 Loss function and analysis RMSE computed over the testing dataset for di/uniFB00erent ANN architectures.
The /uniFB01rst column shows the number of convolutional kernels used in the hidden layer. Results are presented for the
case where 2 diagonals ( nd=) and 8 diagonals ( nd=) of the covariance matrix were estimated.
In all cases, the large ensemble is in the saturation zone of the RMSE curve (i.e., no further signi/uniFB01cant improvement
was obtained by increasing the ensemble size).
The UnnKF has an RMSE that is close to the one of the large ensemble in all the experiments. This indicates
that the performance of the estimation of the covariance has no sensitivity to the size of the state vector in these
experiments (Fig. 10). Moreover, in all cases the performance of the UnnKF is signi/uniFB01cantly better than the one
obtained with the small ensemble size, even though an appropriately tuned covariance localization and multiplicative
in/uniFB02ation factor has been used in the EnKF.
4.4 |Sensitivity to target quality
In the experiments presented so far the methodology with the novel loss function achieves a reliable estimate of
the state-dependent covariance when trained using an ensemble data assimilation system with a large ensemble.
However, in real world applications, available ensemble-based data assimilation systems which can be used for the
training of the neural network are based on smaller ensembles due to the high computational cost associated with
multiple model integrations. To investigate the impact of the training data quality upon the estimated covariances with
the neural network, we performed an additional experiment in which the error proxy is computed from a ensemble-
based data assimilation system with only 5 ensemble members. Figure 11 compares the results of a neural network in18 Sacco M. A. et al.
which the error proxy is computed with a 100-member ensemble (MRA6_E100 as in previous experiments) and the
results obtained when the training is performed with an error proxy computed from a smaller 5-member ensemble.
The results obtained when the actual forecast error is used for the training are also included for comparison (MNT6).
FIGURE 11 RMSE of di/uniFB00erent assimilation experiments where ENS100 and ENS5 correspond to the ensemble
Kalman /uniFB01lter performance for a 100 and 5 ensemble respectively. MNT6 corresponds to the performance of a
neural network trained with the ground truth, MRA6_E5 is a network trained with the 5-member ensemble dataset
and MRA6_E100 is the network trained with a 100-member ensemble dataset.
An interesting result is that the network trained with error proxies derived from a small ensemble-based data
assimilation system (MRA6_5) allows us to produce UnnKF analyses with much lower RMSE than the EnKF analyses
used in the training data. These results may be explained by the fact that the small ensemble has only a few members
to estimate all the elements of the covariance so, sampling errors are expected to be large. However, during training
and due to the use of multiple instances at di/uniFB00erent times the neural network learns to smooth out the di/uniFB00erent
samples leading to a more reliable estimation of the covariance matrix.
In contrast, analyses generated with large ensembles have very small sampling error but model error is likely to
become more dominant. The analyses have a bias, so a part of the forecast error is not well represented during training.
Consequently the resulting RMSEs are rather similar with a slight advantage by ENS100 compared to MRA_100.
Meanwhile, the experiment MNT_100 outperforms ENS100.
FIGURE 12 RMSE of the ensemble Kalman /uniFB01lter as a function of the ensemble size. The dots show the RMSE of
the di/uniFB00erent experiments shown in Fig. 11 but plotted over the line to relate the performance of the machine
learning technique to the ensemble size of the ensemble Kalman /uniFB01lter.
Figure 12 shows the RMSE of the EnKF as a function of the ensemble size. In the same curve, the dots show theSacco M. A. et al. 19
corresponding RMSE of the di/uniFB00erent experiments (i.e., ENS5, ENS100, MRA6_E100, MRA6_E5, and MNT6), so that
it establishes an equivalence between the accuracy of the machine learning based analysis and the ones produced by
the EnKF. In particular, MRA6_E5 is equivalent to an EnKF with ensemble-members, MRA6_E100 is equivalent to a
35-member ensemble, and MNT6 is equivalent to a more than 300 members EnKF. Therefore, we note that MRA6_E5
improves the performance of the ENS5 dataset.
5|CONCLUSION
In this work, we examine the potential of machine learning techniques to infer a state-dependent forecast covariance
using a single deterministic forecast from a numerical dynamical model. We propose a loss function (eMSE) that
allows training a neural network to estimate state-dependent covariance matrices using only previously computed
analyses (training target) and current forecasts (model input). Furthermore, this training method is trivially adaptable
to localize the covariance matrix to an arbitrary number of diagonals. We also evaluate this novel way to estimate
the covariance matrix using a methodology that combines the Kalman-like /uniFB01lter technique with the neural network
covariance estimate (UnnKF), allowing us to perform data assimilation with state-dependent covariance using a single
deterministic forecast. Moreover, a model bias correction method could be easily included within the same framework
as shown in Sacco et al. (2022). This hybrid data-driven methodology was evaluated in terms of numerical stability
and scalability as a function of the size of the state vector. The results are stable (the data assimilation cycle using
the UnnKF could be run robustly during ;cycles) and allowed us to generate analyses with a performance
comparable to an ensemble-based data assimilation technique with members. The optimal network size was not
very sensitive to the size of the state space and to the number of covariance matrix elements being estimated, which
suggests that the extension of this approach to more realistic applications in high-dimensional state spaces is feasible,
although more research is certainly required to con/uniFB01rm this.
In the experiments where the neural networks was trained with EnKF analysis resulting from a small ensemble,
the UnnKF methodology decreases considerably the RMSE of analyses outperforming the EnKF performance. The
ability of the UnnKF to outperform its training analysis suggests the possibility to sequentially training it with self-
produced analyses generated in a previous training. This process could eventually be repeated until convergence is
achieved and the RMSE is stabilised at the lowest possible value.
Besides the encouraging results there are many challenges and issues that requires further investigation before
this methodology can be implemented in combination with state-of-the-art data assimilation systems. For instance,
a relatively simple convolutional neural network architecture is used in our experiments. This was su/uniFB03cient for rep-
resenting the uncertainty of the two-scale Lorenz-96 dynamics. However, more realistic datasets with multi-scale
dynamics are expected to require a deeper network architecture. Another important issue is the /uniFB02exibility of the
technique in a context of a continuously changing observing network. In the experiments presented in this work,
the observation network is assumed to be /uniFB01xed. This hypothesis, leads to a quanti/uniFB01cation of the uncertainty that is
implicitly assuming the underlying observation network structure. More work is required to develop more /uniFB02exible
implementations of this methods that can produce a reliable quanti/uniFB01cation of the forecast error covariance matrix in
scenarios where the observing networks are changing. More research is also required to more e/uniFB03ciently compute the
analysis update. In this work, we conduct an explicit estimation of the analysis update based on the Kalman update
equation. However, this approach is not feasible in high dimensions. A local implementation of the UnnKF like the one
used in the Optimal Interpolation approach can be used to allow the computation of the analysis in high dimensional
systems.20 Sacco M. A. et al.
In this work, we analyze a limit case in which only one deterministic forecast run was performed to conduct
data assimilation with the UnnKF. However the combination of machine-learning approaches and ensemble-based
approaches have been also explored in the literature leading to promising results (e.g. Grönquist et al. 2021) although
the implementation in the context of data assimilation has not yet been tested.
The results obtained in this proof of concept work, using a simple and numerically stable loss-function, are a /uniFB01rst
step towards evaluating the potential of hybrid machine-learning data-assimilation techniques that can be applied
as operational data assimilation and weather prediction systems in meteorological centers where the computational
capacity is limited like in developing countries where the computational cost of well-established assimilation methods,
like 4DVAr or EnKF is prohibitive. Future work will extend and evaluate the present methodology in more realistic
datasets.
references
Amezcua, J., Ide, K., Bishop, C. and Kalnay, E. (2012) Ensemble clustering in deterministic ensemble kalman /uniFB01lters. Tellus A ,
64.
Bannister, R. N. (2017) A review of operational methods of variational and ensemble-variational data assimilation. Quarterly
Journal of the Royal Meteorological Society ,143, 607–633. URL: https://rmets.onlinelibrary.wiley.com/doi/abs/ .
 /qj..
Bishop, C. (2006) Pattern Recognition and Machine Learning . Information Science and Statistics. Springer. URL: https://books.
google.com.ar/books?id=qWPwnQEACAAJ .
Bocquet, M., Brajard, J., Carrassi, A. and Bertino, L. (2019) Data assimilation as a learning tool to infer ordinary di/uniFB00er-
ential equation representations of dynamical models. Nonlinear Processes in Geophysics ,26, 143–162. URL: https:
//npg.copernicus.org/articles/ // /.
Brajard, J., Carrassi, A., Bocquet, M. and Bertino, L. (2020) Combining data assimilation and machine learning to emulate a
dynamical model from sparse and noisy observations: A case study with the lorenz 96 model. Journal of Computational
Science ,44, 101171. URL: https://www.sciencedirect.com/science/article/pii/S  .
— (2021) Combining data assimilation and machine learning to infer unresolved scale parametrization. Philos Trans A Math
Phys Eng Sci ,379, 20200086.
Buizza, C., Quilodrán Casas, C., Nadler, P., Mack, J., Marrone, S., Titus, Z., Le Cornec, C., Heylen, E., Dur, T., Baca Ruiz, L.,
Heaney, C., Díaz Lopez, J. A., Kumar, K. S. and Arcucci, R. (2022) Data learning: Integrating data assimilation and machine
learning. Journal of Computational Science ,58, 101525. URL: https://www.sciencedirect.com/science/article/pii/
S .
Burgers, G., van Leeuwen, P. J. and Evensen, G. (1998) Analysis scheme in the ensemble kalman /uniFB01lter. Monthly Weather Re-
view,126, 1719 – 1724. URL: https://journals.ametsoc.org/view/journals/mwre/ // - _ __ _
asitek_..co_.xml.
Camporeale, E. (2018) Accuracy-reliability cost function for empirical variance estimation.
Camporeale, E., Chu, X., Agapitov, O. V. and Bortnik, J. (2019) On the generation of probabilistic forecasts from deterministic
models. Space Weather .
Carrassi, A., Bocquet, M., Bertino, L. and Evensen, G. (2018) Data assimilation in the geosciences: An overview of methods,
issues, and perspectives. WIREs Climate Change ,9, e535.Sacco M. A. et al. 21
Cheng, S., Quilodran-Casas, C., Ouala, S., Farchi, A., Liu, C., Tandeo, P., Fablet, R., Lucor, D., Iooss, B., Brajard, J. et al. (2023)
Machine learning with data assimilation and uncertainty quanti/uniFB01cation for dynamical systems: a review. arXiv preprint
arXiv:2303.10462 .
Farchi, A., Bocquet, M., Laloyaux, P., Bonavita, M. and Malartic, Q. (2021a) A comparison of combined data assimilation and
machine learning methods for o/uniFB04ine and online model error correction. Journal of Computational Science ,55, 101468.
URL: https://www.sciencedirect.com/science/article/pii/S  .
Farchi, A., Chrust, M., Bocquet, M., Laloyaux, P. and Bonavita, M. (2022) Online model error correction with neural networks
in the incremental 4d-var framework.
Farchi, A., Laloyaux, P., Bonavita, M. and Bocquet, M. (2021b) Using machine learning to correct model error in data assimila-
tion and forecast applications. Quarterly Journal of the Royal Meteorological Society ,147.
Gandin, L. (1965) Objective analysis of meteorological /uniFB01elds: Gidrometeorologicheskoe izdatel’stvo (gimiz), leningrad. Trans-
lated by Israel Program for Scienti/uniFB01c Translations, Jerusalem .
Gaspari, G. and Cohn, S. E. (1999) Construction of correlation functions in two and three dimensions. Quarterly Journal
of the Royal Meteorological Society ,125, 723–757. URL: https://rmets.onlinelibrary.wiley.com/doi/abs/ . /qj.
 .
Grooms, I. (2021) Analog ensemble data assimilation and a method for constructing analogs with variational autoencoders.
Quarterly Journal of the Royal Meteorological Society ,147, 139–149.
Grönquist, P., Ben-Nun, T., Dryden, N., Dueben, P., Lavarini, L., Li, S. and Hoe/uniFB02er, T. (2019) Predicting weather uncertainty
with deep convnets.
Grönquist, P., Yao, C., Ben-Nun, T., Dryden, N., Dueben, P., Li, S. and Hoe/uniFB02er, T. (2021) Deep learning for post-processing en-
semble weather forecasts. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences ,
379, 20200092.
Hamill, T. M., Whitaker, J. S. and Snyder, C. (2001) Distance-dependent /uniFB01ltering of background error covariance estimates
in an ensemble kalman /uniFB01lter. Monthly Weather Review ,129, 2776 – 2790. URL: https://journals.ametsoc.org/view/
journals/mwre/ // - _ __ _ddfobe_..co_.xml.
Härter, F. P. and de Campos Velho, H. F. (2008) New approach to applying neural network in nonlinear dynamic
model. Applied Mathematical Modelling ,32, 2621–2633. URL: https://www.sciencedirect.com/science/article/pii/
S X .
Houtekamer, P. L. and Zhang, F. (2016) Review of the ensemble kalman /uniFB01lter for atmospheric data assimilation. Monthly
Weather Review ,144, 4489 – 4532. URL: https://journals.ametsoc.org/view/journals/mwre/ //mwr-d-- .
.xml.
Hsieh, W. W. and Tang, B. (1998) Applying neural network models to prediction and data analysis in meteorology and oceanog-
raphy. Bulletin of the American Meteorological Society ,79, 1855 – 1870. URL: https://journals.ametsoc.org/view/
journals/bams/ // - _ __ _annmtp___co_.xml.
Hu, H. and Kantor, G. (2015) Parametric covariance prediction for heteroscedastic noise. In 2015 IEEE/RSJ International
Conference on Intelligent Robots and Systems (IROS) , 3052–3057.
Hunt, B. R., Kostelich, E. J. and Szunyogh, I. (2007) E/uniFB03cient data assimilation for spatiotemporal chaos: A local ensemble
transform kalman /uniFB01lter. Physica D: Nonlinear Phenomena ,230, 112 – 126. Data Assimilation.
Irrgang, C., Saynisch-Wagner, J. and Thomas, M. (2020) Machine learning-based prediction of spatiotemporal uncertainties in
global wind velocity reanalyses. Journal of Advances in Modeling Earth Systems .
Kalnay, E. (2003) Atmospheric Modeling, Data Assimilation and Predictability . Cambridge University Press.22 Sacco M. A. et al.
Kondo, K. and Miyoshi, T. (2019) Non-gaussian statistics in global atmospheric dynamics: a study with a 10 240-member
ensemble kalman /uniFB01lter using an intermediate atmospheric general circulation model. Nonlinear Processes in Geophysics ,26,
211–225. URL: https://npg.copernicus.org/articles/ // /.
van Leeuwen, P. J., Künsch, H. R., Nerger, L., Potthast, R. and Reich, S. (2019) Particle /uniFB01lters for high-dimensional geo-
science applications: A review. Quarterly Journal of the Royal Meteorological Society ,145, 2335–2365. URL: https:
//rmets.onlinelibrary.wiley.com/doi/abs/ . /qj..
Lguensat, R., Tandeo, P., Ailliot, P., Pulido, M. and Fablet, R. (2017) The analog data assimilation. Monthly Weather Review ,145,
4093 – 4107. URL: https://journals.ametsoc.org/view/journals/mwre/ //mwr-d-- ..xml.
Liu, K., Ok, K., Vega-Brown, W. and Roy, N. (2018) Deep inference for covariance estimation: Learning gaussian noise models
for state estimation. In 2018 IEEE International Conference on Robotics and Automation (ICRA) , 1436–1443.
Lorenz, E. (1995) Predictability: a problem partly solved.
Loshchilov, I. and Hutter, F. (2017) Decoupled weight decay regularization. URL: https://arxiv.org/abs/  . .
Ouala, S., Fablet, R., Herzet, C., Chapron, B., Pascual, A., Collard, F. and Gaultier, L. (2018) Neural network based kalman
/uniFB01lters for the spatio-temporal interpolation of satellite-derived sea surface temperature. Remote Sensing ,10. URL: https:
//www.mdpi.com/  - ///.
Parrish, D. F. and Derber, J. C. (1992) The national meteorological center’s spectral statistical-interpolation system.
Pulido, M., Sche/uniFB04er, G., Ruiz, J. J., Lucini, M. M. and Tandeo, P. (2016) Estimation of the functional form of subgrid-scale
parametrizations using ensemble-based data assimilation: a simple model experiment. Quarterly Journal of the Royal Me-
teorological Society ,142, 2974–2984.
Rabier, F., Järvinen, H., Klinker, E., Mahfouf, J.-F. and Simmons, A. (2000) The ecmwf operational implementation of four-
dimensional variational assimilation. i: Experimental results with simpli/uniFB01ed physics. Quarterly Journal of the Royal Meteoro-
logical Society ,126, 1143–1170. URL: https://rmets.onlinelibrary.wiley.com/doi/abs/ . /qj. .
Sacco, M. A., Ruiz, J. J., Pulido, M. and Tandeo, P. (2022) Evaluation of machine learning techniques for forecast uncertainty
quanti/uniFB01cation. Quarterly Journal of the Royal Meteorological Society ,148, 3470–3490. URL: https://rmets.onlinelibrary.
wiley.com/doi/abs/ . /qj..
Sche/uniFB04er, G., Ruiz, J. and Pulido, M. (2019) Inference of stochastic parametrizations for model error treatment using nested
ensemble kalman /uniFB01lters. Quarterly Journal of the Royal Meteorological Society ,145, 2028–2045.
Stanley, Z., Grooms, I. and Kleiber, W. (2021) Multivariate localization functions for strongly coupled data assimilation in the
bivariate lorenz 96 system. Nonlinear Processes in Geophysics ,28, 565–583. URL: https://npg.copernicus.org/articles/
// /.
Tandeo, P., Ailliot, P., Bocquet, M., Carrassi, A., Miyoshi, T., Pulido, M. and Zhen, Y. (2020) A review of innovation-based
methods to jointly estimate model and observation error covariance matrices in ensemble data assimilation. Monthly
Weather Review ,148, 3973 – 3994. URL: https://journals.ametsoc.org/view/journals/mwre/ //mwrD .xml.
Tandeo, P., Ailliot, P., Ruiz, J., Hannart, A., Chapron, B., Cuzol, A., Monbet, V., Easton, R. and Fablet, R. (2015) Combining analog
method and ensemble data assimilation: application to the lorenz-63 chaotic system. In Machine Learning and Data Mining
Approaches to Climate Science: proceedings of the 4th International Workshop on Climate Informatics , 3–12. Springer.
Terasaki, K. and Miyoshi, T. (2014) Data assimilation with error-correlated and non-orthogonal observations: Experiments
with the lorenz-96 model. SOLA ,10, 210–213.
Wang, B., Lu, J., Yan, Z., Luo, H., Li, T., Zheng, Y. and Zhang, G. (2018) Deep uncertainty quanti/uniFB01cation: A machine learning
approach for weather forecasting.
Williams, P. M. (1996) Using neural networks to model conditional multivariate densities. Neural Computation ,8, 843–854.