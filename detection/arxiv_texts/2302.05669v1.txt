arXiv:2302.05669v1  [cs.SE]  11 Feb 2023Treat societally impactful scientiﬁc insights as
open-source software artifacts
Cynthia C. S. Liem
Multimedia Computing Group
Delft University of Technology
Delft, The Netherlands
c.c.s.liem@tudelft.nl
https://orcid.org/0000-0002-5385-7695Andrew M. Demetriou
Multimedia Computing Group
Delft University of Technology
Delft, The Netherlands
a.m.demetriou@tudelft.nl
https://orcid.org/0000-0002-0724-2278
Abstract —So far, the relationship between open science and
software engineering expertise has largely focused on the o pen
release of software engineering research insights and repr oducible
artifacts, in the form of open-access papers, open data, and
open-source tools and libraries. In this position paper, we draw
attention to another perspective: scientiﬁc insight itsel f is a
complex and collaborative artifact under continuous devel opment
and in need of continuous quality assurance, and as such, has
many parallels to software artifacts. Considering current calls
for more open, collaborative and reproducible science; inc reas-
ing demands for public accountability on matters of scienti ﬁc
integrity and credibility; methodological challenges com ing with
transdisciplinary science; political and communication t ensions
when scientiﬁc insight on societally relevant topics is to b e trans-
lated to policy; and struggles to incentivize and reward aca demics
who truly want to move into these directions beyond traditio nal
publishing habits and cultures, we make the parallels betwe en
the emerging open science requirements and concepts alread y
well-known in (open-source) software engineering researc h more
explicit. We argue that the societal impact of software engi neering
expertise can reach far beyond the software engineering res earch
community, and call upon the community members to pro-
actively help driving the necessary systems and cultural ch anges
towards more open and accountable research.
Index Terms —open science, software engineering, open source,
transdisciplinary research, responsible research practi ce
I. I NTRODUCTION
This article is a ‘paper’1. At the moment it will reach
broader readership with a formal citation attached, it will have
passed peer review, and be part of a referenceable collectio n of
proceedings of the ICSE 2023 Software Engineering in Societ y
Track. This form and workﬂow have been the traditional
template for communicating scientiﬁc outcomes, where gett ing
papers accepted at prestigious venues has traditionally be en
treated as the major indicator of academic achievement.
Academic research has been operating under scarcity, both
regarding job and research funding security. As a consequen ce,
(not) getting major publications accepted and sufﬁciently cited
thus has great career consequences. Still, for a long time,
research communities have been acknowledging that contri-
butions of scientiﬁc insight extend much beyond a paper, and
proposals for open science have emerged, including venture s
1Most likely, it will not reach the reader on paper, but as a dig ital PDF.into open access, open and FAIR (Findable, Accessibile,
Interoperable, Reusable) data, and open-source software.
The software engineering research community has been act-
ing upon this [1], with open science policies now being expli cit
parts of well-respected venues like ICSE and the Empirical
Software Engineering Journal, open-source tools with arti fact
badging being explicitly encouraged, and the option to subm it
registered reports entering several sub-communities such as
the Conference on Mining Software Repositories. Software
engineering researchers also have actively contributed to dis-
cussions on applying FAIR principles to research software [ 2].
With this position paper, we wish to inspire the software
community to look even beyond this. More speciﬁcally, con-
sidering empirical scientiﬁc insights in the broad sense (i .e.,
insights requiring empirical observation of phenomena, of ten
expressed in the form of data measurements), we will argue
that making these insights more open will require infrastru c-
ture and quality assurance mechanisms similar to those need ed
in developing complex open-source software artifacts.
II. A RGUMENTS FOR OPEN SCIENCE BEYOND THE PAPER
Already in 1942, Robert K. Merton noted that anti-
intellectualism was rising and the integrity of science was
under attack. In response, four ‘institutional imperative s’ were
formulated as comprising the ethos of modern science: univer-
salism (the acceptance or rejection of claims entering the lists
of science should not depend on personal or social attribute s
of the person bringing in these claims), “communism” [sic]
(common ownership of scientiﬁc ﬁndings, with the imperativ e
to communicate ﬁndings, as opposed to secrecy), disinter-
estedness (upholding scientiﬁc integrity by not having self-
interested motivations), and organized skepticism (judgment on
the scientiﬁc contribution should be suspended until detac hed
scrutiny is performed, according to institutionally accep ted
criteria) [3]. Many scientists still subscribe to these nor ms
today [4]. These imperatives also implicitly echo in today’ s
calls, manifestos and proposals for open science and open
access [5], [6], which push for better science, which more pe o-
ple can access—but with which more people also can actively
interact. Below, we further elaborate on several arguments andinitiatives that argue that open science should not stop at a
paper that more people can read.
A. Insufﬁcient quality control on papers
Open-access publishing may stimulate academic and so-
cietal uptake, transform the business models of publishers ,
and allow for publicly funded knowledge to be publicly
available. Still, open access is only an aspect of open scien ce,
and insights and methods reported in a paper may not triv-
ially be reproducible or replicable2, either because common
speciﬁcations are not sufﬁciently detailed [9], or because
claims may be outright false [10]. While researchers have
been divided on which domains suffer from reproducibility
crises [11], generally, many well-published works have fai led
to replicate in psychology [12] and cancer biology [13], and
many concerns are arising about the replicability of machin e
learning outcomes [14], [15]. This leads to credibility cri ses,
in which it is unclear whether results can actually be truste d
and built upon. When policy-makers seek to base decisions on
scientiﬁc insights, this can have severe consequences to hu man
health and public trust [16], [17].
Ofﬁcially, science should be self-correcting; through pee r
review and active continuous scrutiny processes, illegiti mate
claims should be detected and corrected. However, in practi ce,
self-correction turns out painfully slow and reluctant [18 ],
[19]. This may have to do with ‘publish or perish’ cultures
being too strong in institutions, leading to unhealthy work ing
environments [20]–[22], incentivizing Questionable Rese arch
Practices [23], and de-incentivizing investment in Respon sible
Research Practices [24].
B. Joint resource investments for collaborative momentum
With machine learning research, growing power and re-
source imbalances are observed between large industrial la bs,
and small labs in public institutions. A researcher at a uni-
versity will likely not have sufﬁcient computational resou rces
and comprehensive data access to easily be able to replicate
results as reported by big tech industry. Thus, joint invest ments
in shared computation infrastructure are needed [25].
In psychological science, joint efforts have been coordina ted
into massive replication projects, where multiple teams tr ied to
replicate canonically reported outcomes in parallel. Good ex-
amples of this are the ﬁve ‘Many Labs’ large-scale replicati on
projects [26]–[30].
For such efforts, the joint investments need to focus on
technical and intellectual infrastructure: i.e., the effo rts re-
quired to reach a joint insight or paper, in such a way that
many can indeed participate, without the transaction costs of
getting started growing too large on an individual party. In
other words, the focus needs to be on facilitating a shared
process, rather than claiming limited-ownership output, w hich
our present-day incentive systems still appear to push for.
2Deﬁnitions of ‘reproducibility’ and ‘replicability’ have not always been
used in crisp ways; e.g., compare the former [7] and current [ 8] ACM
deﬁnitions, in which deﬁnitions are swapped. Generally spo ken, in the current
discussion, we do not need a sharp distinction, and rather wa nt to refer to the
overall concept that repeating an experiment should give co nsistent results.C. Challenges when crossing disciplines
When research becomes interdisciplinary or even transdis-
ciplinary [31], methodology and consequent quality assura nce
mechanisms become more ambiguous than in the case of
monodisciplinary work. While in the software engineering
community, the SIGSOFT empirical standards [32] help ar-
ticulating and standardizing what a reviewer should expect for
different types of methodological contributions, when mul tiple
disciplines are represented at the same time, a discipline-
speciﬁc reviewer may only be capable of doing a thorough
quality assessment for the parts of the contribution within their
expertise, but not of the full intellectual work.
In case of transdisciplinary work, a broader spectrum of
stakeholders (that may not be academics) will be involved.
This again causes ambiguity on how work should be reviewed
and evaluated. At the same time, for societally relevant app li-
cation domains, it has been argued that broader participati on
of stakeholders can help getting out of credibility crises w ith
regard to modeling choices [17]. Furthermore, if academic
insights are to be implemented in society, it is not unreason able
to not only push the view of academics, but also actively
involve the perspectives and experiences of non-academic
societal stakeholders who will be experiencing the impact o f
this implementation.
D. Societal relevance causes vulnerability
Research on urgent, societally relevant challenges (e.g.,
climate change, public health) tend to be situated in dy-
namic, complex, socio-technical contexts, and require tra ns-
disciplinary approaches [31]. Problems of relevance may be
wicked [33] or even super wicked [34], meaning that there is
ambiguity on how the problem should be framed (while the
solution depends on the framing), and one can assess whether
a solution is ‘better’ or ‘worse’, but there are no hard binar y
outcomes of whether a result is absolutely ‘true’ or ‘false’ . In
case of super wicked problem, there is high urgency and time
is running out, while there is a lack of central authority.
Acting under such dynamic uncertainty comes with chal-
lenges. While fast open publishing and knowledge-sharing c an
be further enabled through open science, too-hasty conclu-
sions that have not been deeply reviewed may cause hazards
to human safety [16], [19]. Furthermore, while the general
public will demand high accountability on societally impac t-
ful outcomes, at the same time, ambiguity, uncertainty, and
dynamically changing insights make it impossible to end up
with static, ﬁrm insights. Potentially contradicting read ings on
topics requiring deeper expertise can cause feelings of un-
certainty in people, harming credibility of scientiﬁc work and
leading to distrust [35]. Distrust in science causes vulner ability
to credibility attacks. Indeed, in Big Tobacco, health, cli mate
change, and AI, concerted delegitimizing efforts have been
taking place as part of lobbying processes towards non-publ ic
interests [36]–[38]. Here again, more public transparency on
how insights were obtained may help in sustaining trust and
facilitating broader public scrutiny.III. M ORE HOLISTIC OPEN SCIENCE :FROM TOOLS TO
CONCEPTUAL PARALLELS TO OPEN -SOURCE SOFTWARE
In response to movements towards more open science,
in recent years, a plethora of process improvements with
supporting platforms and tools have emerged, that support
releasing a more holistic scientiﬁc artifact than a paper al one.
These include pre-registration (e.g., The Center for Open
Science (COS)3, AsPredicted4), pre-print publication (e.g.,
arXiv5, COS), storage of additional materials beyond the PDF
(e.g., COS, data repositories such as Zenodo6and Research-
Hub7), the co-publication of research code or software artifact s
(e.g., Papers with Code8)), decomposed publication (e.g.,
Octopus9, ResearchEquals10, Desci Foundation11), open peer
review (e.g., F1000Research12) and pre-print / post publication
peer review (e.g., PubPeer13, PREreview14, Sciety15). Organi-
zations like the COS and Psychological Science Accelerator16
have coordinated big-team data collection efforts. In para llel,
traditional publication venues have started accepting mor e
modern publication formats, such as registered reports [39 ].
This tooling space is presently fragmented, capturing diff erent
aspects that should improve openness in science. At a higher
level, as discussed below, we however see clusters of intend ed
functionality, that are very close to well-researched topi cs in
software engineering research.
A. Inclusive contributorship with credit
As opposed to the traditional authorship model of publica-
tion (where author names in a list denote some undisclosed
contribution to the work, the list of authors is ﬁnal, and
author order may imply local hierarchies that are speciﬁc
to a research sub-community), there is a need to be more
speciﬁc and transparent about collaborators’ contributio ns to
the intellectual work. In the publication world, the Contri butor
Roles Taxonomy (CRediT) has been proposed and increasingly
adopted as a possible taxonomy for this, with an explicit
change from authorship to contributorship [40].
Models of contributorship have naturally been implemented ,
facilitated and acknowledged in open-source software. In c ase
multiple contributors work on the same artifact, version co ntrol
systems (typically, Git) will be employed that help trackin g
the degree and provenance of changes (i.e., who contributed
what at what time on the development timeline). Contributor s
may work in parallel, both working on main features needing
3https://www.cos.io/
4https://aspredicted.org
5https://arxiv.org/
6https://zenodo.org/
7https://www.researchhub.com/
8https://paperswithcode.com/
9https://www.octopus.ac
10https://www.researchequals.com
11https://descifoundation.org/
12https://f1000research.com/
13https://pubpeer.com
14https://prereview.org/
15https://sciety.org
16https://psysciacc.org/priority, but also on more experimental features. Through
branches, this can be done while there still is a consensus
of what currently is a working non-breaking artifact on the
main branch. While parallel work may be done, version contro l
systems have protocols for resolving potential conﬂicts ar ising
from parallel contributions and changes. Regardless of the
status of the branch, the history of contributions will alwa ys be
transparent. In addition, they allow for ‘orphan’ componen ts
of unﬁnished projects to also be gathered and transparently
disclosed. In psychology, attention has for long been drawn
to the ‘ﬁle drawer’ problem [41]. Here, many studies with
non-signiﬁcant results may never have been reported, but st ill
provide useful insights, and can help meta-scientiﬁc under -
standing of whether results reported as signiﬁcant are inde ed
signiﬁcant, or may have resulted from sampling bias.
We can see a similar parallel to the building of scientiﬁc
knowledge: a main branch can represent current stable in-
sights, where other branches may represent work in progress ,
that down the road can make the overall artifact better. Wher e
in software engineering, code review practices ensure qual ity
control whenever a change is to be committed (regardless of
whether this is on a main or experimental branch), the same
can hold for peer review, where elevated reviewing safeguar ds
can be implemented for merging into the main branch and
‘pushing to production’. As we will discuss further down,
the concept of the ‘main branch’ and versioned releases has
parallels to scientiﬁc consensus of current state-of-the- art.
Where in terms of ownership, public open-source reposi-
tories may have an active team of maintainers and owners
of an artifact, other people not in these groups are explicit ly
welcome to raise issues or feature requests if they see point s
for improvement, and implement and suggest contributions
themselves, that the maintainers and owners may choose
to incorporate. Similarly, in scientiﬁc insight, a core tea m
may work on a particular project, but other researchers and
interested parties may suggest changes or improvements tha t
could be incorporated with visible provenance.
Where open-source projects that actively seek public con-
tributors will have clear documentation and guidelines on
how to get started and contribute if one is an outsider,
similar inclusion-facilitating practices can transfer to scientiﬁc
research projects, as already have been demonstrated in e.g .
the Many Labs large-scale replication projects.
B. Decomposition into maintainable units
As discussed, potential reviewers to scientiﬁc work may
not naturally be equipped to thoroughly review every aspect
of a complete paper, especially if this paper reﬂects inter-
disciplinary work. Generally spoken, it seems unnatural to
only review a complex intellectual contribution only at rel ease
time. With pre-registration and registered reports, publi shing
cultures already tried to solicit such feedback earlier, wi th
positive effects on research quality and integrity [42]; ho wever,
this still involves the review of complete experimental set ups.
In the software engineering world, it has generally been see n
as an example of good practice to organize a complex softwareartifact into smaller, clearly scoped modules and function s.
When committing code contributions to this overall artifac t,
commits also would be organized in smaller, logical contrib u-
tions with a clear focus, and code review would iteratively b e
solicited on these small contributions. This reviewing mod el
resembles the tools facilitating decomposed publication. In
software engineering, we have already seen that decomposit ion
will help in fostering maintainability of the overall artif act, and
making it easier on new contributors to quickly get onboarde d
on the parts of the artifact where they wish to contribute.
We explicitly want to note that this model could work at
the level of scientiﬁc artifacts (effectively, a digitally enriched
form of work that currently only manifests as a paper), but al so
one step up, at the level of scientiﬁc insight that may source
from different papers and other intellectual contribution s.
In scientiﬁc insight, we wish to stand on the shoulders of
giants, and build upon earlier work. As such, we may source
from other insights, similarly to how open-source software
may make use of existing other libraries. Furthermore, agai n
looking at functionalities offered by Git, if serious new co ntri-
butions to an existing repository possibly warrant branchi ng-
off into a new strand of independent development, forking
functionality allows for this, while again still keeping a l iving
reference to the original repository.
Where in science, the insights we build upon may still be
under active research, and there are chances they still may
change and update, the same holds for open-source software
libraries. This may create a dependency hell, for which soft -
ware engineering research is actively researching best pra ctices
to still make a complex artifact building upon other artifac ts as
maintainable as possible. We argue that a translation of the se
best practices will be beneﬁcial in navigating how scientiﬁ c
insight building upon other insights can best be organized a nd
updated, in case all insights dynamically will keep evolvin g.
As for how to decompose and (re)organize complex code,
the software engineering research community has consolida ted
a rich body of best practices or practices to avoid, consolid ated
in the form of software engineering methods, design pattern s
and smells. Equivalents can be formulated for the organizat ion
of scientiﬁc insight: what sub-experiments or analyses can be
modularized or refactored for better reuse? Here, we would
like to point out that software engineering methods tend to b e
taught as advanced-level programming knowledge, and as suc h
may not as actively be part of the skillset of non-computer
scientists who took an introductory programming courses—
while we believe they are essential in thinking strategical ly
about overall information organization.
C. Intermediate releases with consensus, and organization al
safety to ﬁnd weaknesses, iterate and improve
When developing a software artifact, pushing code to pro-
duction, and having formal versioned releases, we naturall y
agree we have not yet reached The Ultimate and Optimal
Final Product—rather, what currently is running may be a
Minimum Viable Product that is iterated upon, but that will
likely still have many imperfections in need of improvement .In scientiﬁc publishing, we may acknowledge this in text, bu t
there is less incentive to demonstrate progressive improve ment
over subsequent contributions. Furthermore, as we will arg ue
below, it may culturally be unsafe to admit weaknesses and
visibly correct them, as this could lead to retractions and c on-
sequences on citation track records. In software developme nt,
this however is no problem, as changes and releases that can
be referenced by others are more clearly separated.
To us, a scientiﬁc paper could be seen as a versioned
release—a larger, but coherent collection of changes and re -
viewer consensus that can be frozen and referred to. Similar ly,
through containerization, we can freeze, save and share the
entirety of a computational environment associated to a con -
tribution. If available on a cloud-based platform, this all ows for
reproducibility, as well as immediate, rapid progress on bo th
the review of material and its reuse and further development ,
since installation overhead will be reduced. At the same tim e,
these freezes do not signal the end of development, and
development can still actively continue.
In our argument to not only organize scientiﬁc artifacts
as open-source artifacts, but even group them at a higher
level of scientiﬁc insight, the concept of currently agreed -
on consensus can also be taken one level up, similar to
how knowledge is established in Wikipedia: for a research
problem that many people work on in parallel, a meta-scienti ﬁc
overview of what the collective insight and consensus cur-
rently is can be consolidated. Consensus-focused publicat ions
aim to condense the overall state of a thread of research,
reporting ﬁrst any consensus, while also indicating ambigu ities
or research opportunities. One might further conceive ‘liv ing’
consensus-focused articles, i.e. systematic reviews, in a model
similar to Wikipedia articles, where the review stays curre nt, as
authors continuously update it. This especially will be rel evant
for topics with increasingly unwieldy numbers of associate d
publications, where there is a clear beneﬁt to ﬁnding a means
to condense scientiﬁc information; even more so, when the
consolidated insights may be looked-at in informing policy
(e.g., with regard to climate change or public health).
As software is developed under pressure and with short de-
velopment timelines, compromises and simpliﬁcations will be
made. This may lead to technical debt, in which issues needin g
deeper attention may pile up without being prioritized—up
to the point that major and expensive ﬁxes may be needed.
Similarly, we would argue that current problems with self-
correction in science and questionable insights may be a
consequence of intellectual debt, as also suggested in [43] .
Again, creating cultures in which this is as actively mitiga ted
as possible will be beneﬁcial.
Here, we already mentioned problems with organizational
and cultural safety in admitting weakness and implementing
corrections in scientiﬁc contributions. With software art i-
facts, we up-front acknowledge that programmers are compe-
tent [44], but bugs, errors and issues may still have occurre d.
Through testing (and ideally, test-driven development) we
include safeguards that help reducing the amount of problem s
that will need ﬁxing—or otherwise will help us signaling andﬁxing them as early as possible. Still, we will never know
whether a program will be fully bug-free. However, this does
not prevent us from having justiﬁable perspectives on when
code artifacts can be published and released. We feel that th e
culture of encouraging and appreciating testing and qualit y
assurance in software engineering may be very inspirationa l
to discussions on fostering Responsible Research Practice s and
academic integrity, without this being seen as a reputation al
threat or attack on character.
Finally, the software engineering community has increas-
ingly been acknowledging the social and contextual organi-
zational surroundings of a software artifact, with emergin g
strands of research studying how team interactions and orga ni-
zational policies will affect the quality of a software arti fact, as
well as the efﬁciency and effectiveness of the process leadi ng
to its development. Similarly, these social and organizati onal
insights will be beneﬁcial in efforts to address the culture of
scientiﬁc research itself (as well as constructive directi ons for
systems changes, if we indeed would decide to move beyond
our current ways of sharing insights through static papers) .
IV. C ONCLUSION
In this work, we have outlined how the development of
scientiﬁc insights parallels the development of software. The
shift from traditional publishing to open science involves
challenging culture and systems changes. As the software
engineering community has noticed in its own open science
endeavors, investing in this is a serious, expensive and so
far under-rewarded investment [1], [45], [46]. Yet, as we
argued, the strong expertise of software engineering exper ts
in acknowledging contributorship on complex larger collab -
orations, designing for robust maintainability, and devel oping
based on iterative improvements, can more broadly beneﬁt th e
development of scientiﬁc insight subscribing to the Merton ian
norms of science—and be beneﬁcial to society at large when
complex societal challenges are addressed.
We therefore call upon our software engineering colleagues
committed to open science to both think more boldly in
how academic incentives can be improved beyond the focus
on output, and even look beyond the software engineering
research ﬁeld alone. As for the ﬁrst, beyond current (com-
mendable) efforts to integrate open science principles in t he
publication process of software engineering venues, it wil l
be worthwhile to think of what a ‘Many Labs’ equivalent in
software engineering may look like. As one thought, may it
take aspects of current tool competitions and benchmarks, t hat
also focus on collective understanding, but rather from the start
be framed as a joint collaborative and iterative effort?
As for the second, we invite our colleagues to join existing
scientiﬁc reform movements, help developing and increasin g
interoperability of current tools, and critically reﬂect o n what
software engineering skills can best be taught outside of th e
own curriculum. As one possible thought experiment, what
would a re-framing of the state-of-the-art in climate scien ce
as a complex software-like artifact look like? Which insigh ts
would need to be decomposed? Who reviews what, and whatwould a review discussion look like if multiple disciplines get
involved? How can we allow for public scrutiny, while not
feeding into public distrust?
As one example, we as authors of this article have been
actively attending meta-scientiﬁc and science improvemen t
events (such as the meetings of the Society for the Improve-
ment of Psychological Science17), and have started prototyping
the idea of turning scientiﬁc publication processes into Gi t-
supported software artifacts [47]. First prototypical dev elop-
ment towards the latter mission was performed in the form
of a software development project, which several bachelor
students in Computer Science and Engineering at our institu te
took up as part of their software engineering coursework. Th e
resulting work was presented as a non-archival contributio n at
a Scientiﬁc Progress Seminar [48]. While this was not yet a
formal publication, it was an excellent way to get bachelor-
level software engineering students interested in researc h
processes, and many of them enthusiastically attended the
seminar, that was highly interdisciplinary, also e.g. invo lving
epistemological philosophical work. Currently, we are wor king
with our local Open Science community and advertising new
student projects to further develop this project.
However, we ourselves are no software engineering re-
searchers, and we are certain our colleagues with deeper
expertise in the subject matter can push such developments
much further. In doing this, we would argue that software
engineering expertise can have even broader societal and
scientiﬁc impact than it already does today.
CREDITAUTHOR STATEMENT
Cynthia C. S. Liem : Conceptualization, Investigation,
Methodology, Supervision, Writing – original draft, Writi ng –
review & editing; Andrew M. Demetriou : Conceptualization,
Investigation, Resources, Writing – original draft, Writi ng –
review & editing.
REFERENCES
[1] D. Mendez, D. Graziotin, S. Wagner, and H. Seibold, “Open science in
software engineering,” in Contemporary Empirical Methods in Software
Engineering , M. Felderer and G. H. Travassos, Eds. Cham: Springer
International Publishing, 2020, pp. 477–501. [Online]. Av ailable:
https://doi.org/10.1007/978-3-030-32489-6_17
[2] A.-L. Lamprecht, L. Garcia, M. Kuzak, C. Martinez, R. Arc ila,
E. Martin Del Pico, V . Dominguez Del Angel, S. van de Sandt,
J. Ison, P. A. Martinez, P. McQuilton, A. Valencia, J. Harrow ,
F. Psomopoulos, J. L. Gelpi, N. Chue Hong, C. Goble, and
S. Capella-Gutierrez, “Towards FAIR principles for resear ch software,”
Data Science , vol. 3, no. 1, pp. 37–59, 2020. [Online]. Available:
https://doi.org/10.3233/DS-190026
[3] R. K. Merton et al. , “Science and technology in a democratic order,”
Journal of legal and political sociology , vol. 1, no. 1, pp. 115–126,
1942.
[4] M. S. Anderson, E. A. Ronning, R. DeVries, and B. C. Martin son,
“Extending the Mertonian Norms: Scientists’ Subscription to Norms of
Research,” Journal of Higher Education , vol. 81, pp. 366–393, 2010.
[Online]. Available: https://doi.org/10.1353/jhe.0.00 95
[5] M. Munafò, B. Nosek, D. Bishop, K. Button, C. Chambers,
N. Percie Du Sert, U. Simonsohn, E. Wagenmakers, J. Ware,
and J. Ioannidis, “A manifesto for reproducible science,” Nature
Human Behaviour , vol. 1, no. 1, Jan. 2017. [Online]. Available:
https://doi.org/10.1038/s41562-016-0021
17https://improvingpsych.org/[6] J. Tennant, F. Waldner, D. Jacques, P. Masuzzo, L. Collis ter, and
C. Hartgerink, “The academic, economic and societal impact s of open
access: an evidence-based review [version 3; peer review: 4 approved,
1 approved with reservations],” F1000Research , vol. 5, no. 632, 2016.
[Online]. Available: https://doi.org/10.12688/f1000re search.8460.3
[7] ACM, “Artifact Review and Badging – Ver-
sion 1.0 (not current),” 2019. [Online]. Available:
https://www.acm.org/publications/policies/artifact- review-badging
[8] ——, “Artifact Review and Badging Ver-
sion 1.1,” 2020. [Online]. Available:
https://www.acm.org/publications/policies/artifact- review-and-badging-current
[9] B. McFee, J. W. Kim, M. Cartwright, J. Salamon, R. Bittner , and
J. P. Bello, “Open-Source Practices for Music Signal Proces sing
Research,” IEEE Signal Processing Magazine , vol. 36, pp. 128–137,
2019. [Online]. Available: https://doi.org/10.1109/MSP .2018.2875349
[10] J. P. A. Ioannidis, “Why Most Published Research Findin gs
Are False,” PLoS Medicine , vol. 2, 2005. [Online]. Available:
https://doi.org/10.1371/journal.pmed.1004085
[11] M. Baker, “1,500 scientists lift the lid on reproducibi lity,”
Nature , vol. 533, pp. 452–454, 2016. [Online]. Available:
https://doi.org/10.1038/533452a
[12] Open Science Collaboration, “Estimating the reproduc ibility of
psychological science,” Science , vol. 349, no. 6251, 2015. [Online].
Available: https://doi.org/10.1126/science.aac4716
[13] C. G. Begley and L. M. Ellis, “Raise standards for precli nical cancer
research,” Nature , vol. 483, pp. 531–533, 2012. [Online]. Available:
https://doi.org/10.1038/483531a
[14] B. Yildiz, H. Hung, J. H. Krijthe, C. C. S. Liem, M. Loog,
G. Migut, F. A. Oliehoek, A. Panichella, P. Pawełczak, S. Pic ek,
M. de Weerdt, and J. van Gemert, “ReproducedPapers.Org: Ope nly
Teaching and Structuring Machine Learning Reproducibilit y,” in
Reproducible Research in Pattern Recognition: Third Inter national
Workshop, RRPR 2021, Virtual Event, January 11, 2021,
Revised Selected Papers , 2021, pp. 3—-11. [Online]. Available:
https://doi.org/10.1007/978-3-030-76423-4_1
[15] S. Kapoor and A. Narayanan, “Leakage and the Reproducib ility
Crisis in ML-based Science,” 2022. [Online]. Available:
https://arxiv.org/abs/2207.07048
[16] J. M. Lawrence, G. Meyerowitz-Katz, J. A. J. Heathers, N . J. L.
Brown, and K. A. Sheldrick, “The lesson of ivermectin: meta-
analyses based on summary data alone are inherently unrelia ble,”
Nature Medicine , vol. 27, pp. 1853–1854, 2021. [Online]. Available:
https://doi.org/10.1038/s41591-021-01535-y
[17] J. P. van der Sluijs, “A way out of the credibility
crisis of models used in integrated environmental assessme nt,”
Futures , vol. 34, pp. 133–146, mar 2002. [Online]. Available:
https://doi.org/10.1016/S0016-3287(01)00051-9
[18] S. Vazire and A. O. Holcombe, “Where Are The Self-
Correcting Mechanisms In Science?” Review of General
Psychology , vol. 26, pp. 212–223, 2022. [Online]. Available:
https://doi.org/10.1177/10892680211033912
[19] L. Besançon, E. Bik, J. Heathers, and G. Meyerowitz-Kat z, “Correction
of scientiﬁc literature: Too little, too late!” PLoS Biology , vol. 20,
2022. [Online]. Available: https://doi.org/10.1371/jou rnal.pbio.3001572
[20] S. Rawat and S. Meena, “Publish or perish: Where are we he ading?”
J Res Med Sci , vol. 19, pp. 87–89, feb 2014. [Online]. Available:
https://pubmed.ncbi.nlm.nih.gov/24778659
[21] M. de Rond and A. N. Miller, “Publish or Perish: Bane or Bo on
of Academic Life?” Journal of Management Inquiry , vol. 14, 2005.
[Online]. Available: https://doi.org/10.1177/10564926 05276850
[22] H. P. van Dalen and K. Henkens, “Intended and Unintended
Consequences of a Publish-or-Perish Culture: A Worldwide S urvey,”
Journal of the American Society for Information Science and
Technology , vol. 63, no. 7, pp. 1282—-1293, 2012. [Online]. Available:
https://doi.org/10.1002/asi.22636
[23] G. Gopalakrishna, G. ter Riet, G. Vink, I. Stoop, J. M. Wi cherts, and
L. M. Bouter, “Prevalence of questionable research practic es, research
misconduct and their potential explanatory factors: A surv ey among
academic researchers in The Netherlands,” PLOS ONE , vol. 17, no. 2, 02
2022. [Online]. Available: https://doi.org/10.1371/jou rnal.pone.0263023
[24] G. Gopalakrishna, J. Wicherts, G. Vink, I. Stoop, O. van den Akker,
G. ter Riet, and L. Bouter, “Prevalence of responsible resea rch
practices among academics in The Netherlands [version 2; pe erreview: 2 approved],” F1000Research , vol. 11, no. 471, 2022. [Online].
Available: https://doi.org/10.12688/f1000research.11 0664.2
[25] V . J. Hellendoorn and A. A. Sawant, “The Growing Cost of
Deep Learning for Source Code,” Communications of the ACM ,
vol. 65, no. 1, pp. 31—-33, dec 2021. [Online]. Available:
https://doi.org/10.1145/3501261
[26] R. A. Klein, K. A. Ratliff, M. Vianello, R. B. Adams, v. Ba hník, M. J.
Bernstein, K. Bocian, M. J. Brandt, B. Brooks, C. C. Brumbaug h,
Z. Cemalcilar, J. Chandler, W. Cheong, W. E. Davis, T. Devos,
M. Eisner, N. Frankowska, D. Furrow, E. M. Galliani, F. Hasse lman,
J. A. Hicks, J. F. Hovermale, S. J. Hunt, J. R. Huntsinger, H. I Jzerman,
M.-S. John, J. A. Joy-Gaba, H. Barry Kappes, L. E. Krueger, J. Kurtz,
C. A. Levitan, R. K. Mallett, W. L. Morris, A. J. Nelson, J. A. N ier,
G. Packard, R. Pilati, A. M. Rutchick, K. Schmidt, J. L. Skori nko,
R. Smith, T. G. Steiner, J. Storbeck, L. M. Van Swol, D. Thomps on,
A. E. van ‘t Veer, L. Ann Vaughn, M. Vranka, A. L. Wichman, J. A.
Woodzicka, and B. A. Nosek, “Investigating Variation in Rep licability,”
Social Psychology , vol. 45, no. 3, pp. 142–152, 2014. [Online].
Available: https://doi.org/10.1027/1864-9335/a000178
[27] R. A. Klein, M. Vianello, F. Hasselman, B. G. Adams, J. Re ginald
B. Adams, S. Alper, M. Aveyard, J. R. Axt, M. T. Babalola,
Štˇepán Bahník, R. Batra, M. Berkics, M. J. Bernstein, D. R. Berr y,
O. Bialobrzeska, E. D. Binan, K. Bocian, M. J. Brandt, R. Busc hing,
A. C. Rédei, H. Cai, F. Cambier, K. Cantarero, C. L. Carmichae l,
F. Ceric, J. Chandler, J.-H. Chang, A. Chatard, E. E. Chen, W. Cheong,
D. C. Cicero, S. Coen, J. A. Coleman, B. Collisson, M. A. Conwa y,
K. S. Corker, P. G. Curran, F. Cushman, Z. K. Dagona, I. Dalgar , A. D.
Rosa, W. E. Davis, M. de Bruijn, L. D. Schutter, T. Devos, M. de Vries,
C. Do ˘gulu, N. Dozo, K. N. Dukes, Y . Dunham, K. Durrheim,
C. R. Ebersole, J. E. Edlund, A. Eller, A. S. English, C. Finck ,
N. Frankowska, M. Ángel Freyre, M. Friedman, E. M. Galliani, J. C.
Gandi, T. Ghoshal, S. R. Giessner, T. Gill, T. Gnambs, Ángel G ómez,
R. González, J. Graham, J. E. Grahe, I. Grahek, E. G. T. Green, K. Hai,
M. Haigh, E. L. Haines, M. P. Hall, M. E. Heffernan, J. A. Hicks ,
P. Houdek, J. R. Huntsinger, H. P. Huynh, H. IJzerman, Y . Inba r,
Åse H. Innes-Ker, W. Jiménez-Leal, M.-S. John, J. A. Joy-Gab a,
R. G. Kamilo ˘glu, H. B. Kappes, S. Karabati, H. Karick, V . N.
Keller, A. Kende, N. Kervyn, G. Kneževi ´c, C. Kovacs, L. E. Krueger,
G. Kurapov, J. Kurtz, D. Lakens, L. B. Lazarevi ´c, C. A. Levitan,
J. Neil A. Lewis, S. Lins, N. P. Lipsey, J. E. Losee, E. Maassen , A. T.
Maitner, W. Malingumu, R. K. Mallett, S. A. Marotta, J. Me ¯dedovi ´c,
F. Mena-Pacheco, T. L. Milfont, W. L. Morris, S. C. Murphy,
A. Myachykov, N. Neave, K. Neijenhuijs, A. J. Nelson, F. Neto ,
A. L. Nichols, A. Ocampo, S. L. O’Donnell, H. Oikawa, M. Oikaw a,
E. Ong, G. Orosz, M. Osowiecka, G. Packard, R. Pérez-Sánchez ,
B. Petrovi ´c, R. Pilati, B. Pinter, L. Podesta, G. Pogge, M. M. H.
Pollmann, A. M. Rutchick, P. Saavedra, A. K. Saeri, E. Salomo n,
K. Schmidt, F. D. Schönbrodt, M. B. Sekerdej, D. Sirlopú, J. L . M.
Skorinko, M. A. Smith, V . Smith-Castro, K. C. H. J. Smolders,
A. Sobkow, W. Sowden, P. Spachtholz, M. Srivastava, T. G. Ste iner,
J. Stouten, C. N. H. Street, O. K. Sundfelt, S. Szeto, E. Szumo wska,
A. C. W. Tang, N. Tanzer, M. J. Tear, J. Theriault, M. Thomae,
D. Torres, J. Traczyk, J. M. Tybur, A. Ujhelyi, R. C. M. van Aer t,
M. A. L. M. van Assen, M. van der Hulst, P. A. M. van Lange,
A. E. van ’t Veer, A. Vásquez-Echeverría, L. A. Vaughn, A. Váz quez,
L. D. Vega, C. Verniers, M. Verschoor, I. P. J. V oermans, M. A.
Vranka, C. Welch, A. L. Wichman, L. A. Williams, M. Wood, J. A.
Woodzicka, M. K. Wronska, L. Young, J. M. Zelenski, Z. Zhijia , and
B. A. Nosek, “Many Labs 2: Investigating Variation in Replic ability
Across Samples and Settings,” Advances in Methods and Practices
in Psychological Science , vol. 1, no. 4, pp. 443–490, 2018. [Online].
Available: https://doi.org/10.1177/2515245918810225
[28] C. Ebersole, O. Atherton, A. Belanger, H. Skulborstad, J. Allen,
J. Banks, E. Baranski, M. Bernstein, D. Bonﬁglio, L. Boucher ,
E. Brown, N. Budiman, A. Cairo, C. Capaldi, C. Chartier, J. Ch ung,
D. Cicero, J. Coleman, J. Conway, W. Davis, T. Devos, M. Fletc her,
K. German, J. Grahe, A. Hermann, J. Hicks, N. Honeycutt,
B. Humphrey, M. Janus, D. Johnson, J. Joy-Gaba, H. Juzeler, A . Keres,
D. Kinney, J. Kirshenbaum, R. Klein, R. Lucas, C. Lustgraaf, D. Martin,
M. Menon, M. Metzger, J. Moloney, P. Morse, R. Prislin, T. Raz za,
D. Re, N. Rule, T. Sacco, K. Sauerberger, E. Shrider, M. Shult z,
C. Siemsen, K. Sobocko, R. Sternglanz, A. Summerville, K. Ts khay,
Z. van Allen, L. Vaughn, R. Walker, A. Weinberg, J. Wilson, J. Wirth,
J. Wortman, and B. Nosek, “Many Labs 3: Evaluating participa ntpool quality across the academic semester via replication, ”Journal
of Experimental Social Psychology , vol. 67, pp. 68–82, Nov. 2016.
[Online]. Available: https://doi.org/10.1016/j.jesp.2 015.10.012
[29] R. A. Klein, C. L. Cook, C. R. Ebersole, C. Vitiello, B. A. Nosek,
J. Hilgard, P. H. Ahn, A. J. Brady, C. R. Chartier, C. D. Christ opherson,
S. Clay, B. Collisson, J. T. Crawford, R. Cromar, G. Gardiner ,
C. L. Gosnell, J. Grahe, C. Hall, I. Howard, J. A. Joy-Gaba,
M. Kolb, A. M. Legg, C. A. Levitan, A. D. Mancini, D. Manfredi,
J. Miller, G. Nave, L. Redford, I. Schlitz, K. Schmidt, J. L. M .
Skorinko, D. Storage, T. Swanson, L. M. Van Swol, L. A. Vaughn ,
D. Vidamuerte, B. Wiggins, and K. A. Ratliff, “Many Labs 4: Fa ilure to
Replicate Mortality Salience Effect With and Without Origi nal Author
Involvement,” Collabra: Psychology , vol. 8, no. 1, 04 2022, 35271.
[Online]. Available: https://doi.org/10.1525/collabra .35271
[30] C. R. Ebersole, M. B. Mathur, E. Baranski, D.-J. Bart-Pl ange, N. R.
Buttrick, C. R. Chartier, K. S. Corker, M. Corley, J. K. Harts horne,
H. IJzerman, L. B. Lazarevi ´c, H. Rabagliati, I. Ropovik, B. Aczel,
L. F. Aeschbach, L. Andrighetto, J. D. Arnal, H. Arrow, P. Bab incak,
B. E. Bakos, G. Baník, E. Baskin, R. Belopavlovi ´c, M. H. Bernstein,
M. Białek, N. G. Bloxsom, B. Bodroža, D. B. V . Bonﬁglio, L. Bou cher,
F. Brühlmann, C. C. Brumbaugh, E. Casini, Y . Chen, C. Chiorri , W. J.
Chopik, O. Christ, A. M. Ciunci, H. M. Claypool, S. Coary, M. V .
ˇColi´c, W. M. Collins, P. G. Curran, C. R. Day, B. Dering, A. Dreber,
J. E. Edlund, F. Falcão, A. Fedor, L. Feinberg, I. R. Ferguson , M. Ford,
M. C. Frank, E. Fryberger, A. Garinther, K. Gawryluk, K. Ashb augh,
M. Giacomantonio, S. R. Giessner, J. E. Grahe, R. E. Guadagno ,
E. Hałasa, P. J. B. Hancock, R. A. Hilliard, J. Hüffmeier, S. H ughes,
K. Idzikowska, M. Inzlicht, A. Jern, W. Jiménez-Leal, M. Joh annesson,
J. A. Joy-Gaba, M. Kauff, D. J. Kellier, G. Kessinger, M. C.
Kidwell, A. M. Kimbrough, J. P. J. King, V . S. Kolb, S. Kołodzi ej,
M. Kovacs, K. Krasuska, S. Kraus, L. E. Krueger, K. Kuchno, C. A.
Lage, E. V . Langford, C. A. Levitan, T. J. S. de Lima, H. Lin,
S. Lins, J. E. Loy, D. Manfredi, Łukasz Markiewicz, M. Menon,
B. Mercier, M. Metzger, V . Meyet, A. E. Millen, J. K. Miller,
A. Montealegre, D. A. Moore, R. Muda, G. Nave, A. L. Nichols,
S. A. Novak, C. Nunnally, A. Orli ´c, A. Palinkas, A. Panno, K. P.
Parks, I. Pedovi ´c, E. P˛ ekala, M. R. Penner, S. Pessers, B. Petrovi ´c,
T. Pfeiffer, D. Pie ´nkosz, E. Preti, D. Puri ´c, T. Ramos, J. Ravid, T. S.
Razza, K. Rentzsch, J. Richetin, S. C. Rife, A. D. Rosa, K. H.
Rudy, J. Salamon, B. Saunders, P. Sawicki, K. Schmidt, K. Sch uepfer,
T. Schultze, S. Schulz-Hardt, A. Schütz, A. N. Shabazian, R. L.
Shubella, A. Siegel, R. Silva, B. Sioma, L. Skorb, L. E. C. de S ouza,
S. Steegen, L. A. R. Stein, R. W. Sternglanz, D. Stojilovi ´c, D. Storage,
G. B. Sullivan, B. Szaszi, P. Szecsi, O. Szöke, A. Szuts, M. Th omae,
N. D. Tidwell, C. Tocco, A.-K. Torka, F. Tuerlinckx, W. Vanpa emel,
L. A. Vaughn, M. Vianello, D. Viganola, M. Vlachou, R. J. Walk er,
S. C. Weissgerber, A. L. Wichman, B. J. Wiggins, D. Wolf, M. J.
Wood, D. Zealley, I. Žeželj, M. Zrubka, and B. A. Nosek, “Many labs
5: Testing pre-data-collection peer review as an intervent ion to increase
replicability,” Advances in Methods and Practices in Psychological
Science , vol. 3, no. 3, pp. 309–331, 2020. [Online]. Available:
https://doi.org/10.1177/2515245920958687
[31] OECD, “Addressing societal challenges using transdis ciplinary
research,” OECD, Tech. Rep. 88, 2020. [Online]. Available:
https://doi.org/10.1787/0ca0ca45-en
[32] P. Ralph, N. b. Ali, S. Baltes, D. Bianculli, J. Diaz, Y . D ittrich, N. Ernst,
M. Felderer, R. Feldt, A. Filieri, B. B. N. de França, C. A. Fur ia, G. Gay,
N. Gold, D. Graziotin, P. He, R. Hoda, N. Juristo, B. Kitchenh am,
V . Lenarduzzi, J. Martínez, J. Melegati, D. Mendez, T. Menzi es,
J. Molleri, D. Pfahl, R. Robbes, D. Russo, N. Saarimäki, F. Sa rro,
D. Taibi, J. Siegmund, D. Spinellis, M. Staron, K. Stol, M.-A . Storey,
D. Taibi, D. Tamburri, M. Torchiano, C. Treude, B. Turhan, X. Wang,
and S. Vegas, “Empirical Standards for Software Engineerin g Research,”
[35] C. Chang, “Motivated processing: How people perceive n ews
covering novel or contradictory health research ﬁndings,” Science2020. [Online]. Available: https://doi.org/10.48550/ar Xiv.2010.03525
[33] H. W. J. Rittel and M. M. Webber, “Dilemmas in a general th eory
of planning,” Policy Sciences , vol. 4, no. 2, pp. 155–169, Jun 1973.
[Online]. Available: https://doi.org/10.1007/BF014057 30
[34] K. Levin, B. Cashore, S. Bernstein, and G. Auld, “Overco ming
the tragedy of super wicked problems: constraining our futu re
selves to ameliorate global climate change,” Policy Sciences ,
vol. 45, no. 2, pp. 123–152, Jun 2012. [Online]. Available:
https://doi.org/10.1007/s11077-012-9151-0
Communication , vol. 37, no. 5, pp. 602–634, 2015. [Online]. Available:
https://doi.org/10.1177/1075547015597914
[36] G. Reed, Y . Hendlin, A. Desikan, T. MacKinney, E. Berman , and G. T.
Goldman, “The disinformation playbook: how industry manip ulates
the science-policy process—and how to restore scientiﬁc in tegrity,”
Journal of Public Health Policy , vol. 42, pp. 622––634, 2021. [Online].
Available: https://doi.org/10.1057/s41271-021-00318- 6
[37] L. Antilla, “Climate of scepticism: Us newspaper cover age of the science
of climate change,” Global Environmental Change , pp. 338–352, 2005.
[Online]. Available: https://doi.org/10.1016/j.gloenv cha.2005.08.003
[38] M. Abdalla and M. Abdalla, “The Grey Hoodie Project: Big Tobacco,
Big Tech, and the Threat on Academic Integrity,” in Proceedings of the
2021 AAAI/ACM Conference on AI, Ethics, and Society , ser. AIES ’21.
New York, NY , USA: Association for Computing Machinery, 202 1, p.
287–297. [Online]. Available: https://doi.org/10.1145/ 3461702.3462563
[39] B. A. Nosek and D. Lakens, “Registered reports: A method
to increase the credibility of published results,” Social
Psychology , vol. 45, pp. 137–141, 2014. [Online]. Available:
https://doi.org/10.1027/1864-9335/a000192
[40] M. K. McNutt, M. Bradford, J. M. Drazen, B. Hanson, B. How ard,
K. H. Jamieson, V . Kiermer, E. Marcus, B. K. Pope, R. Schekman ,
S. Swaminathan, P. J. Stang, and I. M. Verma, “Transparency
in authors’ contributions and responsibilities to promote integrity
in scientiﬁc publication,” Proceedings of the National Academy of
Sciences , vol. 115, no. 11, pp. 2557–2560, 2018. [Online]. Available :
https://doi.org/10.1073/pnas.1715374115
[41] R. Rosenthal, “The ﬁle drawer problem and tolerance for null results,”
Psychological Bulletin , vol. 86, no. 3, pp. 638—-641, 1979. [Online].
Available: https://doi.org/10.1037/0033-2909.86.3.63 8
[42] C. K. Soderberg, T. M. Errington, S. R. Schiavone, J. Bot tesini, F. S.
Thorn, S. Vazire, K. M. Esterling, and B. A. Nosek, “Initial e vidence
of research quality of registered reports compared with the standard
publishing model,” Nature Human Behaviour , vol. 5, pp. 990–997,
2021. [Online]. Available: https://doi.org/10.1038/s41 562-021-01142-4
[43] J. Zittrain, “Intellectual Debt: With Great Power
Comes Great Ignorance,” 2019. [Online]. Available:
https://nrs.harvard.edu/URN-3:HUL.INSTREPOS:3737327 6
[44] R. A. DeMillo, R. J. Lipton, and F. G. Sayward, “Hints on t est data
selection: Help for the practicing programmer,” IEEE Computer , vol. 11,
no. 4, pp. 34–41, 1978.
[45] C. S. Timperley, L. Herckis, C. Le Goues, and M. Hilton,
“Understanding and improving artifact sharing in software engineering
research,” Empirical Software Engineering , vol. 26, no. 4, 2021.
[Online]. Available: https://doi.org/10.1007/s10664-0 21-09973-5
[46] B. Hermann, “What has artifact evaluation ever done for us?” IEEE
Security & Privacy , vol. 20, no. 5, pp. 96–99, 2022. [Online]. Available:
https://doi.org/10.1109/MSEC.2022.3184234
[47] A. M. Demetriou and C. C. S. Liem, “Alexandria: a Proof-o f-Concept
Publication Platform that Treats Academic Outputs like Sof tware
Artifacts,” 2022. [Online]. Available: https://osf.io/h d5nu/
[48] A. M. Demetriou, A. van der Meijden, J. Sloof, M. de Wit, E . Witting,
A. Zlei, and C. C. S. Liem, “Alexandria: a Proof-of-Concept P ublication
Platform that Treats Academic Outputs like Software Artifa cts,” in
Scientiﬁc Progress – Individual and Collective seminar , 2022.