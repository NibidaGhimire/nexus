ENHANCING SELF-SUPERVISED LEARNING FOR REMOTE
SENSING WITH ELEVATION DATA: A C ASE STUDY WITH SCARCE
ANDHIGHLEVEL SEMANTIC LABELS
Omar A. Castaño-Idarraga
Faculty of Engineering
University of AntioquiaFreddie Kalaitzis
Department of Computer Science
University of OxfordRaul Ramos-Pollán
Faculty of Engineering
University of Antioquia
ABSTRACT
This work proposes a hybrid unsupervised and supervised learning method to pre-train models
applied in Earth observation downstream tasks when only a handful of labels denoting very general
semantic concepts are available. We combine a contrastive approach to pre-train models with a
pixel-wise regression pre-text task to predict coarse elevation maps, which are commonly available
worldwide. We hypothesize that this will allow the model to pre-learn useful representations, as there
is generally some correlation between elevation maps and targets in many remote sensing tasks. We
assess the performance of our approach on a binary semantic segmentation task and a binary image
classification task, both derived from a dataset created for the northwest of Colombia. In both cases,
we pre-train our models with 39k unlabeled images, fine-tune them on the downstream tasks with
only 80 labeled images, and evaluate them with 2944 labeled images. Our experiments show that our
methods, GLCNet+Elevation for segmentation, and SimCLR+Elevation for classification, outperform
their counterparts without the pixel-wise regression pre-text task, namely SimCLR and GLCNet, in
terms of macro-average F1 Score and Mean Intersection over Union (MIoU). Our study not only
encourages the development of pre-training methods that leverage readily available geographical
information, such as elevation data, to enhance the performance of self-supervised methods when
applied to Earth observation tasks, but also promotes the use of datasets with high-level semantic
labels, which are more likely to be updated frequently. Project code can be found in this link
https://github.com/omarcastano/Elevation-Aware-SSL.
1 Introduction
Inspired by the success of self-supervised learning methods
in remote sensing [ 1,2,3,4,5,6], we explore their poten-
tial for application to a dataset with and high-level seman-
tic labels. Recent studies have shown that self-supervised
learning methods can perform comparably or even better
than their supervised counterparts in tasks such as im-
age classification, object detection, and semantic segmen-
tation on remote sensing data. Existing applications of
self-supervised learning in remote sensing primarily uti-
lize benchmark datasets with low-level semantic concepts.
However, there is a significantly larger amount of high-
level semantic labels available for remote sensing, which
are more likely to be updated frequently. The application
of self-supervised learning methods in this context appears
promising, and a few works are emerging in this area [ 7,8],
adapting learning from label proportions methods [ 9,10]
to specific use cases. However, its application to remote
sensing is still incipient.In this direction, we first create a dataset comprising high-
level semantic segmentation maps (refer to 3) for the
northwest region of Colombia. This dataset, which we
have named Northwest Region Colombia Dataset (NWRC
dataset), is based on Sentinel-2 [ 11] for the satellite images,
and SIPRA [ 12], the Colombian agency for agricultural
data and planning, for the semantic segmentation maps.
We then use the NWRC dataset to evaluate the perfor-
mance of two existing self-supervised contrastive learning
methods, GLCNet [ 14] for a segmentation downstream
task, which aims to predict the segmentation maps ex-
tracted from SIPRA, and SimCLR [ 13] for a classification
downstream task derived from the segmentation masks.
This classification task can be seen as a spatially coarser
counterpart of the segmentation task by setting the classifi-
cation target to summarize the pixel classes. We find that,
even with only high-level semantic labels available, these
self-supervised methods outperform models trained from
scratch when the amount of labeled data is limited.arXiv:2304.06857v3  [cs.CV]  20 Feb 2024Next, we propose a novel unsupervised learning approach
that combines contrastive learning and elevation maps (Fig-
ure 1). Our approach involves designing a pixel-level re-
gression pre-text task that predicts elevation maps at a
coarser spatial resolution than the input images. This is
then combined with SimCLR and GLCNet through an inte-
grated loss function. We refer to these new frameworks as
SimCLR+Elevation and GLCNet+Elevation, respectively.
The intuition behind this approach is that there may be
a correlation between elevation and the classes in the fi-
nal downstream tasks. This correlation may enable the
model to learn a useful representation that incorporates
information about the classes in the final task.
In both cases, we pre-train our models with 39K unla-
beled images, fine-tune the downstream task only with
80 labeled images, and test it with 2944 labeled images.
Our experiments show that our proposed methods for pre-
training, SimCLR+Elevation for classification tasks and
GLCNet+Elevation for segmentation tasks, outperform
both the SimCLR and GLCNet methods respectively, in
terms of accuracy and macro average F1. This supports the
idea that the inclusion of additional information, during
the pre-training phase, which is associated with the classes
in the downstream tasks, can boost model performance.
In summary, our contributions are: the creation and open-
source release of a novel dataset for the northwest region
of Colombia (the NWRC dataset), designed specifically
for two tasks: classification and semantic segmentation.
(2) the application of self-supervised contrastive learning
to remote sensing using the NWRC dataset, demonstrating
its ability to enhance model performance, even with only
high-level semantic labels available, when the amount of
labeled data is limited. (3) a novel unsupervised method
that incorporates spatially coarse elevation maps as a pre-
text task to boost the performance of current contrastive
learning methods.
2 Background
In this section, we present a brief overview of the con-
trastive framework for unsupervised learning and we
describe two well-known self-supervised frameworks
SimCRL[ 13] and GLCNet[ 14]. It is important to high-
light that in this research, SimCRL was consistently used
to pre-train the models for the image classification down-
stream task. In contrast, GLCNet was used for pre-training
models specifically designed for the image semantic seg-
mentation downstream task.
2.1 Contrastive Learning Framework
Self-supervised contrastive learning is a type of unsuper-
vised learning methodology that is used for representa-
tion learning. The objective in contrastive learning is to
push the representations of positive samples closer together
while pulling the representations of negative samples fur-
ther apart. The positive samples are pairs of augmentedversions of the same sample, while the negative samples
are pairs of samples from different instances. Formally,
given an unlabeled image x, two augmented views are
generated, ˆxandx+. An encoder emaps the input images
into an embedding space, then a multilayer perceptron
(MLP) his used to project the embeddings to a metric
space, ˆz=h(e(ˆx))andz+=h(e(x+)). The encoder is
trained to maximize the similarity between ˆzandz+, while
minimizing the similarity between ˆzandz−, where z−is
randomly selected from a set of instances distinct from
x. With the similarity measured by dot products, recent
approaches in contrastive learning differ in the type of con-
trastive loss and generation of positive and negative pairs.
In this work, we focus on SimCLR[ 13] and GLCNet[ 14],
two well-known contrastive methods designed for image
classification and image segmentation, respectively.
2.1.1 SimCLR
SimCLR [ 13] is a simple contrastive learning framework
where the representations are learned by maximizing agree-
ment of positive samples, while minimizing agreement of
negative samples. The positive samples are pairs of aug-
mented versions of a single sample, while the negative
samples are pairs of samples from different instances. In
this framework, Nsamples from a mini-batch are aug-
mented to create 2Nsamples. A pair of samples derived
from the same sample forms a positive pair, while the re-
maining 2(N−1)samples form negative samples. The
contrastive loss, which is called NT-Xent , is calculated as
follows:
LC=1
2NNX
k=1(ℓ(˜xi,ˆxi) +ℓ(ˆxi,˜xi)) (1)
with:
ℓ(˜xi,ˆxi) =−logexp(sim(˜zi,ˆzi)/τ)P
x∈Λ−exp(sim(˜zi,h(e(x)))/τ)(2)
and
ˆz=h(e(ˆx)),˜z=h(e(˜x)) (3)
where edenotes the encoder used to map input images into
an embedding space, hdenotes the MLP used to project
embeddings into a metric space, sim denotes the similarity
measure function between two feature vectors, which in
this case is cosine similarity. Λ−refers to the 2(N−1)
negative samples in addition to the positive sample pair.
τis a temperature parameter. Figure 1-a illustrates the
SimCLR framework.
2.1.2 GLCNet
GLCNet[ 14] is a self-supervised learning method that aims
to pre-train an encoder-decoder network for image seman-
tic segmentation. It achieves this by combining two mod-
ules. The Global Style Contrastive Learning focuses on
learning global features by contrasting different augmented
2Figure 1: a) Combined SimCLR+Elevation framework. b) Downstream classification using pre-trained backbone. c)
Combined GLCNet+Elevation framework. d) Downstream segmentation using pre-trained backbone. Observe that
in both (with SimCLR or with GLCNet) cases the Resnet18 encoder is shared during pre-training and it is only that
encoder that is transferred to the downstream tasks. The rest of the architectural elements (the projection head hdand
the decoder d) are initialized randomly.
views of the same image. It uses style features instead of
the average pooling features commonly used in contrastive
learning, which are believed to better represent the overall
image characteristics. The Local Matching Contrastive
Learning targets learning local features by contrasting
small matching regions from different augmented views
of the same image. It utilizes the output of the decoder to
extract these local regions and employs a projection head
to generate metric embeddings. The total loss function
of GLCNet is based one the NT-Xent loss function 1, and
combines the losses from both modules, with a weighting
parameter λthat controls their relative importance.
LC=λLG+ (1−λ)LL (4)
whereLGandLLdenote the global and local loss func-
tions respectively. Both loss functions are the NT-Xent
loss function applied to the metric embeddings coming
from the encoder and decoder, respectively. Figure 1-c
illustrates the GLCNet framework.
3 Method
This section introduces our unsupervised learning ap-
proach that leverage elevation maps for model pre-training.
This approach is incorporated with contrastive learning
frameworks, specifically SimCLR[ 13] and GLCNet[ 14].The objective is to improve their performance when the
pre-trained backbone is applied to downstream tasks, par-
ticularly classification and semantic segmentation.
3.1 Elevation Map Prediction as a Pre-text Task
In this section, we explore the potential of using spatially
coarse elevation maps for unsupervised learning. Specifi-
cally, we design a pixel-wise regression pre-text task that
leverages elevation maps for pre-training a backbone. We
input 100x100 pixel RGB images, and the model outputs
33x33 regression values for elevation. As this pre-text
task is at the pixel level, we hypothesize that it may help
the model to pre-learn local patterns that could be useful
in a downstream segmentation task. Figure 1-a (Eleva-
tion model) illustrates our proposed pixel-wise regression
pre-text task.
More specifically, given an image x, we use an encoder e
to map images into an embedding space of lower dimen-
sion, then we use a decoder dto upscale the embedded
representation and predict the elevation map, ˆy=d(e(x)),
which might be of lower resolution than the input image
x. This encoder-decoder network is based on the Unet
[16] architecture. The encoder and decoder are optimized
using the pixel-level root mean squared error loss function,
which is calculated as follows:
3LE=1
NNX
i=1WX
j=1HX
k=1(yijk−d(e(xi))jk)2(5)
Where yjkandd(e(xi))jkare the ground truth and pre-
dicted values for the ithimage at location (j, k), respec-
tively. N denotes the number of instances in the mini-bath,
WandHdenote the width and the high of the ground
truth and predicted elevation map.
Following the completion of the pre-training process, the
decoder and projection header are eliminated, and only
the encoder eis utilized to initialize the backbone in the
downstream task.
3.2 Combining Elevation Prediction and Contrastive
Learning
In the previous subsection, we designed a regression pre-
text task leveraging elevation maps. In this section, we
aim to integrate the elevation map prediction pre-text task
with the contrastive learning frameworks, SimCLR and
GLCNet, in order to improve the performance of these
models.
3.2.1 SimCLR+Elevation
This integrated framework combines the pixel-wise regres-
sion pre-text task, presented in the previous subsection,
with the contrastive task proposed in SimCLR (refer to Fig-
ure 1, upper-left).The framework involves generating three
augmented views of an image, namely ˆx,˜x, and x. These
augmented views are then mapped to a shared embedding
space using an encoder e, resulting in ˆv=e(ˆx),˜v=e(˜x),
andv= (x). For the contrastive pre-text task, the embed-
ded views ˆvand˜vare passed through a projection header
hp, which is a multi-layer perceptron, resulting in metric
space representations ˆz=hp(ˆv)and˜z=hp(˜z)which are
used in the contrastive pre-text task to learn global features.
On the other hand, the remaining embedded view v=e(x)
is utilized to predict the elevation map. To accomplish this,
a decoder dis employed to upscale the embedded repre-
sentation and predict the elevation map ˆy=dp(e(x)). The
network is jointly optimized to perform both contrastive
learning and elevation map prediction. Therefore, a loss
function combining the NT-Xent loss (defined in Equation
1) and the pixel level MSE (define in Equatio 5) loss with
a coefficient αis introduced. The loss function is defined
as:
L=αLE+ (1−α)LC (6)
Here, the coefficient αrepresents the relative importance
of the contrastive learning loss LCand the elevation map
learning loss LE. The joint optimization of both tasks
allows our model to learn representations that simultane-
ously maximize agreement between positive image pairs,
minimize agreement between negative image pairs, and
accurately predict the elevation map of the images.Upon the completion of the pre-training process using the
combined framework, SimCLR+Elevation, the decoder
dpand projection header hpare discarded, and only the
encoder eis used to initialize the backbone in the classifi-
cation downstream task.
3.2.2 GLCNet+Elevation
Similar to SimCLR+Elevation, the GLCNet+Elevation
framework merges the regression pre-text task with the
contrastive learning mechanism of GLCNet (illustrated in
Figure 1, upper-right). This approach also generates three
augmented images: ˆx,˜x, and x, which are encoded into a
shared embedding space. The GLCNet framework intro-
duces two contrastive modules: the global style contrastive
module and the local matching contrastive module. For the
global style module, the embeddings ˆvand˜vare processed
by a projection head hg, resulting in metric space represen-
tations ˆz=hg(ˆv)and˜z=hg(˜v), which are used to learn
global features. For the local matching module, the embed-
dings ˆvand˜vare decoded and then projected to obtain met-
ric space embeddings ˆz=hl(dl(ˆv))and˜z=hl(dl(˜v)),
which are used to learn local features beneficial for seg-
mentation tasks. The elevation map prediction uses the
embedded view v, with a decoder dppredicting the eleva-
tion map ˆy=dp(e(x)). The network is jointly optimized
for contrastive learning and elevation map prediction, with
a loss function that combines the loss proposed in GLCNet
4 and the pixel-level MSE loss 5, weighted by coefficients
αandλ, as shown in Equation 6.
L=αLE+ (1−α)(λLG+ (1−λ)LL) (7)
Here, the coefficient αserves to balance the elevation
prediction loss LEwith the global and local contrastive
losses, LGandLL, respectively. The parameter λfur-
ther adjusts the emphasis between the global and local
contrastive learning objectives.
Upon the completion of the pre-training, the network is
fine-tuned on the downstream task. Specifically, the de-
coders dlanddp, and the projection heads hgandhlare
removed, and the pre-trained encoder eis used to initialize
the backbone for the downstream segmentation semantic
task.
4 Data Description
In this study, we developed a dataset for the northwest re-
gion of Colombia by utilizing Sentinel-2[ 11] imagery and
the agricultural land frontier defined by SIPRA[ 12], the
official spatial viewer for the agricultural sector in Colom-
bia. The dataset includes 42704 images of 100x100 pixels
at 10m per pixel, featuring the RGB spectral bands. The
labels are segmentation maps with two classes representing
the general concept of:
National agricultural frontier (farmland ): areas where
agricultural activities such as agriculture, livestock, aqua-
culture and fishing take place. This general label includes
4various fine-grained labels such as vegetable crops, tu-
ber crops, confined crops, lagoons, lakes, natural swamps,
wooded pastures, and mosaic of pastures and crops among
others, which we do not have access to.
Natural forest and non-agricultural areas (other ornot
farmland ): areas where agricultural activities do not take
place. This general label includes various fine-grained
labels such as buildings, roads, trails, plains, and natural
forests among others, which we do not have access to.
We also use the geo-reference information of the images
collected from Sentinel-2 to extract 30m elevation maps of
33x33 pixels from SRTMGL1Nv003[ 17]. These elevation
maps will be used in our proposed pixel-wise regression
pre-text task.
4.1 Semantic Segmentation Dataset
The semantic segmentation dataset is formed using the seg-
mentation maps which provide pixel-level annotations for
the two principal categories: National agricultural frontier
andNatural forest and non-agricultural areas (refer to
Figure 3 on the Ground Truth column). To benchmark the
performance of unsupervised models (GLCNet and GLC-
Net+Elevation) with a limited amount of labeled data for
fine-tuning, we randomly selected 3024 images. Of these,
80 images, representing a mere 0.2% of the pre-training
dataset, are designated for fine-tuning, while the remaining
2944 images are used for testing. The remaining 39720
images are left for unsupervised pre-training.
4.2 Classification Dataset
The classification dataset is derived from the segmenta-
tion maps containing pixel-level class information. For
this dataset, we chose images that exclusively depict one
class, as shown in Figure 2. Unlike the task of predict-
ing a 2D segmentation map, the goal here is to assign a
single label to each image, either National agricultural
frontier orNatural forest and non-agricultural areas , fol-
lowing the standard practice in classification tasks. To
evaluate the efficacy of unsupervised learning techniques
(SimCLR and SimCLR+Elevation) with a limited labeled
dataset for fine-tuning, we randomly selected 3024 images.
From this subset, 80 images, which constitute 0.2% of
the pre-training dataset, are used for fine-tuning, and the
remaining 2944 images are set aside for testing. The bulk
of the dataset, which includes 39720 images, is reserved
for unsupervised pre-training.
5 Experiments and Results
We evaluated the performance of our unsupervised pre-
training approach on the image classification and se-
mantic segmentation datasets described earlier. For im-
age classification, we pre-trained a backbone using Sim-
CLR+Elevation and compared its performance against
three baselines. The first baseline, referred to as Random-
Figure 2: Samples from the NWRC dataset used in the
image classification task. The classification task involves
predicting a label rather than a 2D segmentation map.
Init, denotes a model that was trained from scratch on the
classification dataset without any pre-training. The sec-
ond, termed Elevation-Map, represents a model pre-trained
solely on pixel-wise regression pre-text tasks. The third
baseline, known as Vanilla SimCLR, is a model pre-trained
using the standard SimCLR framework.
For semantic segmentation, we evaluated the performance
of our unsupervised GLCNet+Elevation model in a similar
manner. The Random-Init baseline in this context refers to
a model trained from scratch on the segmentation dataset,
without any pre-training. The Elevation-Map comparison
involves a model that has been pre-trained solely on the
elevation regression task. Finally, the Vanilla GLCNet
baseline denotes a model pre-trained using the GLCNet
approach.
5.1 Image Classification on the NWRC Dataset
We conducted experiments on the NWRC classification
dataset presented in subsection 4.2. Initially, we pre-
trained a backbone using an unsupervised strategy, includ-
ing SimCLR, Elevation-Map, and SimCLR+Elevation, on
39,720 unlabeled images. This pre-training phase was fol-
lowed by a fine-tuning process on a significantly smaller
dataset of 80 labeled images. The performance of these
fine-tuned models was then assessed using a test set of
2,944 labeled images.
5.1.1 Implementation Details for Unsupervised
Learning
For image classification, we employed SimCLR[ 13] as the
backbone of our contrastive learning method. We gener-
ated positive and negative pairs for network pre-training us-
ing various image augmentation techniques, such as color
jittering, random grayscale conversion, horizontal and ver-
tical flipping, and random resized cropping. We chose the
ResNet18 architecture for the backbone and pre-trained
the network for 200 epochs with a batch size of 256. The
Adam optimizer was used with a weight decay of 1e-4, an
initial learning rate of 0.001, and a cosine decay learning
rate schedule. A temperature scaling τof 0.5 was applied
to calculate the contrastive loss. For the pixel-wise regres-
sion pre-text task, we also applied image augmentations,
5including color jittering, random grayscale conversion, and
flipping, and a Unet architecture was used to predict the
elevation maps. When integrating the elevation map task
with contrastive learning, we set the weight αto 0.5 in the
combined loss function.
5.1.2 Implementation Details for Fine-Tuning
We evaluated the pre-learned representations by train-
ing a linear classification layer on top of the pre-trained
backbone using 80 labeled data points. More specifi-
cally, we initialized a ResNet18 backbone with a pre-
trained representation (SimCLR, Elevation-Map, and Sim-
CLR+Elevation) and added a single fully-connected layer
that maps the intermediate representation (embeddings) to
class logits. We then proceeded to freeze the pre-trained
backbone and train only the added layer for 20 epochs
using a batch size of 8 and the Adam optimizer with an
initial learning rate of 1e-3. Finally, we fine-tuned the
entire network for 80 epochs with a batch size of 8 and a
learning rate of 1e-5. Figure 1 (upper-left) illustrates the
configuration of our proposed approach.
5.1.3 Quantitative Analysis
The results of our classification experiments on the NWRC
dataset are presented in Table 1. We can see that the Sim-
CLR+Elevation initialization method outperforms other
initialization methods in terms of accuracy and macro aver-
age F1. In particular, the SimCLR+Elevation initialization
outperforms Random initialization (Random-Init) and Sim-
CLR by 6.2%and2.7%F1, respectively. This confirms
our hypothesis that incorporating features correlated with
the classes in the pre-training phase can boost the per-
formance of contrastive methods, in this case, SimCLR.
However, we also notice that the Elevation initialization
(Elevation-Map), which corresponds to setting α= 1.0
in the SimCLR+Elevation framework (Equation 6), does
not perform better than Random-Init. This may be due to
the fact that the pixel-wise regression pre-text task is not
well-aligned with the downstream classification task and
there is not enough data to fine-tune the network effectively
for classification.
Pre-train Accuracy F1
Random-Init 87.73 86.85
SimCLR 90.79 90.31
Elevation-Map (ours) 87.64 86.46
SimCLR+Elevation (ours) 93.5 93.04
Table 1: Comparison of fine-tuning accuracy and F1-score
on the classification test set for different pre-training meth-
ods. The ResNet18 backbone was pre-trained on 39,720
images. The SimCLR+Elevation method (ours) outper-
forms the other methods.5.2 Semantic Segmentation on the NWRC Dataset
We further evaluated our pre-training approach on the se-
mantic segmentation task using the dataset described in
subsection 4.1. A backbone was pre-trained using unsu-
pervised methods, including GLCNet, Elevation-Map, and
GLCNet+Elevation, on 39,720 unlabeled images, followed
by fine-tuning on 80 labeled images. The performance
of the fine-tuned models was then evaluated on a test set
of 2,944 labeled images, using the F1 score and mean
intersection over union (MIoU) as metrics.
5.2.1 Implementation Details for Unsupervised
Learning
In our semantic segmentation experiments, we used GLC-
Net as the backbone for contrastive learning. To pre-train
the network using contrastive learning, we generated posi-
tive and negative pairs using various image augmentation
techniques, including color jittering, random grayscale,
horizontal and vertical flipping, and random resized crop-
ping. We pre-trained the network for 200 epochs with a
batch size of 200 using a ResNet18 architecture, employ-
ing the Adam optimizer with a weight decay of 1e-4, an
initial learning rate of 0.001, and a cosine decay schedule.
For the elevation map regression pre-text task, we applied
image augmentations and used a Unet architecture to pre-
dict the elevation maps. We set α= 0.5(Equation 7) when
integrating the pixel-level regression task into contrastive
learning.
5.2.2 Implementation Details for Supervised
Learning
To test the pre-trained representations, we used a Unet
architecture with the pre-trained ResNet18 backbones. We
initialized and froze the Unet’s encoder with the pre-trained
ResNet18 and trained the decoder for 20 epochs with a
batch size of 8, using the Adam optimizer at a learning
rate of 1e-3. The entire network was then fine-tuned for 80
epochs with a batch size of 8 and a learning rate of 1e-5.
Additional data augmentation techniques were applied to
the training images. Figure 1 (upper-right) illustrates our
approach for pre-training and fine-tuning on a semantic
segmentation task.
5.2.3 Quantitative Analysis
The semantic segmentation results are summarized in Table
2. The GLCNet+Elevation method achieved the highest F1
score and MIoU, outperforming Random and GLCNet by
4.29% and1.54% in MIoU, respectively. This supports the
notion that pre-training with class-correlated features can
enhance model performance in downstream tasks. Addi-
tionally, we observe that the Elevation initialization, which
corresponds to setting α= 1.0in the GLCNet+Elevation
framework, performed better than Random initialization
and was close to GLCNet, suggesting that the alignment
between the regression pre-text task and the semantic seg-
mentation task is beneficial. Visual comparisons in Figure
6Figure 3: This Figure shows the experimental results of all models and the ground truth on the semantic segmentation
dataset.
3 further illustrate the superior performance of our GLC-
Net+Elevation method compared to other approaches.
Pre-train F1 MIoU
Random Init 71.81 56.72
GLCNet 74.17 59.47
Elevation-Map (ours) 73.29 58.0
GLCNet+Elevation (ours) 75.33 61.01
Table 2: Fine-tuning MIoU and F1-score on the semantic
segmentation test set. We use a ResNet18 backbone pre-
trained on 39720 images.
5.3 Ablation Analysis
We conducted a series of ablation experiments to investi-
gate the influence of increasing the quantity of labeled data
for the fine-tuning of pre-trained models in both image
classification and semantic segmentation tasks. Specifi-
cally, we adjusted the volume of labeled data utilized for
fine-tuning the pre-trained models on the downstream tasks
within the range of [80,100,200,500,1000,2000,3000] ,
while using 36,800 images for pre-training and 2,944 im-
ages for testing.In the context of image classification, our proposed
methodology, SimCLR+Elevation, demonstrated superior
performance over other unsupervised pre-training tech-
niques across all tested scenarios, as depicted in Figure 4.
A more pronounced impact was observed when the amount
of labeled data was less than 500. Additionally, SimCLR
consistently outperformed Random-Init across all tested
scenarios.
Figure 4: Results from the ablation analysis showcasing
the impact of varying amounts of labeled data on the fine-
tuning of pre-trained models for image classification.
7Interestingly, the Elevation-Map pre-training method un-
derperformed in comparison to Random-Init and SimCLR
when the amount of labeled data was less than 500, but
surpassed them when the amount of labeled data was 500
or greater. We hypothesize that this is due to the Eleva-
tion pre-training being a regression pixel-level pre-text
task, which does not align well with the final classification
downstream task. Therefore, it requires more labeled data,
compared to SimCLR or SimCLR+Elevation, to effectively
leverage the pre-training.
In terms of semantic segmentation, our proposed methodol-
ogy, GLCNet+Elevation, outperformed other unsupervised
pre-training techniques in all tested scenarios, as depicted
in Figure 5. Moreover, GLCNet and Elevation-Map pre-
training methods demonstrated comparable performance
and consistently surpassed Random-Init in all tested scenar-
ios. In this instance, we hypothesize that as the regression
pixel-level pre-text task aligns well with the final semantic
segmentation downstream task, which is also a pixel-level
task.
Figure 5: Results from the ablation analysis illustrating
the effect of different amounts of labeled data on the fine-
tuning of pre-trained models for semantic segmentation.
6 Conclusion
In this study, we proposed a novel hybrid unsupervised
and supervised learning method for pre-training models
applied to Earth observation tasks. We combined a con-
trastive approach with a pixel-wise regression pre-text task
to predict coarse elevation maps, hypothesizing that this
would allow the model to pre-learn useful representations.
Our experiments on a binary segmentation downstream
task and a binary image classification task, both derived
from a dataset for the northwest of Colombia, demon-
strated that our methods, GLCNet+Elevation for segmen-
tation, and SimCLR+Elevation for classification, outper-
formed their counterparts without the pixel-wise regression
pre-text task, namely SimCLR and GLCNet, in terms of
macro-average F1 Score and Mean Intersection over Union
(MIoU).
Our findings suggest that leveraging readily available geo-
graphical information, such as elevation data, can enhancethe performance of self-supervised methods when applied
to Earth observation tasks. Furthermore, our study pro-
motes the use of datasets with high-level semantic labels,
which are more likely to be updated frequently.
Our ablation analysis also revealed that the amount of la-
beled data used for fine-tuning significantly influences
the performance of pre-trained models. In particular,
our proposed methods, SimCLR+Elevation and GLC-
Net+Elevation, consistently outperformed other methods
across all tested scenarios, with a more pronounced impact
observed when the amount of labeled data was less than
500.
In conclusion, our study contributes to the development
of pre-training methods for Earth observation tasks, pro-
viding a promising approach that combines contrastive
learning and elevation maps. Future work may explore the
application of our proposed method to other geographical
datasets and tasks, as well as the integration of other types
of geographical information into the pre-training process.
References
[1]Stefano Vincenzi, Angelo Porrello, Pietro Buzzega,
Marco Cipriano, Pietro Fronte, Roberto Cuccu, Carla
Ippoliti, Annamaria Conte, and Simone Calderara.
The color out of space: learning self-supervised rep-
resentations for earth observation imagery. In 2020
25th International Conference on Pattern Recogni-
tion (ICPR) , pages 3034–3041. IEEE, 2021.
[2]Neal Jean, Sherrie Wang, Anshul Samar, George Az-
zari, David Lobell, and Stefano Ermon. Tile2vec:
Unsupervised representation learning for spatially
distributed data. In Proceedings of the AAAI Con-
ference on Artificial Intelligence , volume 33, pages
3967–3974, 2019.
[3]Heechul Jung and Taegyun Jeon. Self-supervised
learning with randomised layers for remote sensing.
Electronics Letters , 57(6):249–251, 2021.
[4]Vladan Stojnic and Vladimir Risojevic. Self-
supervised learning of remote sensing scene represen-
tations using contrastive multiview coding. In Pro-
ceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 1182–1191,
2021.
[5]Aidan M Swope, Xander H Rudelis, and Kyle T Story.
Representation learning for remote sensing: An un-
supervised sensor fusion approach. arXiv preprint
arXiv:2108.05094 , 2021.
[6]Heechul Jung, Yoonju Oh, Seongho Jeong, Chae-
hyeon Lee, and Taegyun Jeon. Contrastive self-
supervised learning with smoothed representation
for remote sensing. IEEE Geoscience and Remote
Sensing Letters , 19:1–5, 2021.
[7]Laura Elena Cué La Rosa and Dário Augusto Borges
Oliveira. Learning from label proportions with pro-
totypical contrastive clustering. In Proceedings of
8the AAAI Conference on Artificial Intelligence , vol-
ume 36, pages 2153–2161, 2022.
[8]Corentin Bolyn, Philippe Lejeune, Adrien Michez,
and Nicolas Latte. Mapping tree species propor-
tions from satellite imagery using spectral–spatial
deep learning. Remote Sensing of Environment ,
280:113205, 2022.
[9]Clayton Scott and Jianxin Zhang. Learning from la-
bel proportions: A mutual contamination framework.
Advances in neural information processing systems ,
33:22256–22267, 2020.
[10] Kuen-Han Tsai and Hsuan-Tien Lin. Learning from
label proportions with consistency regularization. In
Asian Conference on Machine Learning , pages 513–
528. PMLR, 2020.
[11] Matthias Drusch, Umberto Del Bello, Sébastien Car-
lier, Olivier Colin, Veronica Fernandez, Ferran Gas-
con, Bianca Hoersch, Claudia Isola, Paolo Laberinti,
Philippe Martimort, et al. Sentinel-2: Esa’s opti-
cal high-resolution mission for gmes operational ser-
vices. Remote sensing of Environment , 120:25–36,
2012.
[12] SIPRA. https://sipra.upra.gov.co/
nacional . Accessed: 2023-2-8.
[13] Ting Chen, Simon Kornblith, Mohammad Norouzi,
and Geoffrey Hinton. A simple framework for con-
trastive learning of visual representations. In Interna-
tional conference on machine learning , pages 1597–
1607. PMLR, 2020.
[14] Haifeng Li, Yi Li, Guo Zhang, Ruoyun Liu, Haozhe
Huang, Qing Zhu, and Chao Tao. Global and local
contrastive self-supervised learning for semantic seg-
mentation of hr remote sensing images. IEEE Trans-
actions on Geoscience and Remote Sensing , 60:1–14,
2022.
[15] Xun Huang and Serge Belongie. Arbitrary style trans-
fer in real-time with adaptive instance normalization.
InProceedings of the IEEE international conference
on computer vision , pages 1501–1510, 2017.
[16] Olaf Ronneberger, Philipp Fischer, and Thomas Brox.
U-net: Convolutional networks for biomedical im-
age segmentation. In Medical Image Computing and
Computer-Assisted Intervention–MICCAI 2015: 18th
International Conference, Munich, Germany, Octo-
ber 5-9, 2015, Proceedings, Part III 18 , pages 234–
241. Springer, 2015.
[17] Nasa Jpl. NASA shuttle radar topography mission
global 1 arc second number, 2013.
9