SPRING: Situated Conversation Agent Pretrained with
Multimodal Questions from Incremental Layout Graph
Yuxing Long1, Binyuan Hui2, Fulong Ye1, Yanyang Li2, Zhuoxin Han1,
Caixia Yuan1, Yongbin Li2, Xiaojie Wang1*
1Beijing University of Posts and Telecommunications, Beijing, China
2Independent Researcher
flongyuxing, fulong ye, hanzhuoxin, yuancx, xjwang g@bupt.edu.cn
lyb821@gmail.com
Abstract
Existing multimodal conversation agents have shown impres-
sive abilities to locate absolute positions or retrieve attributes
in simple scenarios, but they fail to perform well when com-
plex relative positions and information alignments are in-
volved, which poses a bottleneck in response quality. In this
paper, we propose a Situated Conversation Agent PRetrained
with Multimodal Questions from INcremental Layout Graph
(SPRING ) with abilities of reasoning multi-hops spatial rela-
tions and connecting them with visual attributes in crowded
situated scenarios. SpeciÔ¨Åcally, we design two types of Mul-
timodal Question Answering (MQA) tasks to pretrain the
agent. All QA pairs utilized during pretraining are gener-
ated from novel Incremental Layout Graphs (ILG). QA pair
difÔ¨Åculty labels automatically annotated by ILG are used
to promote MQA-based Curriculum Learning. Experimen-
tal results verify the S PRING ‚Äôs effectiveness, showing that it
signiÔ¨Åcantly outperforms state-of-the-art approaches on both
SIMMC 1.0 and S IMMC 2.0 datasets. We release our code
and data at https://github.com/LYX0501/SPRING.
1 Introduction
Building multi-modal conversation agents that can commu-
nicate with people in visual situations is an attractive goal
for the AI community. Lots of speciÔ¨Åc tasks and datasets
for visual dialog, like VisDial (Das et al. 2017), GuessWhat
(De Vries et al. 2017), GuessWhich (Chattopadhyay et al.
2017), are proposed in recent years. Among them, the Sit-
uated Interactive Multi-modal Conversation (S IMMC 1.0)
(Moon et al. 2020) aims to study task-oriented dialogues that
encompass a situated multi-modal user context in the form
of a virtual reality (VR) environment. The updated dataset
SIMMC 2.0 (Kottur et al. 2021b) provides a more challeng-
ing test bed for multi-modal conversation agents. There are
many assets with a complex layout in each image. Figure
1 gives an example of a scene and a fragment of dialogue
in S IMMC 2.0. There are dozens of clothes in the image.
Each cloth is a digit asset with a unique asset ID and a set of
attributes ( e.g.type, color) in the metadata. But there is no
information on the scene layout except for a few labels on
four relations (up, down, left, and right) between the assets.
*Corresponding author
Copyright ¬© 2023, Association for the Advancement of ArtiÔ¨Åcial
Intelligence (www.aaai.org). All rights reserved.
Figure 1: An example of a virtual scene and a fragment of
dialogue in S IMMC 2.0. Since there are many clothes with
similar visual attributes, it is difÔ¨Åcult to talk about a asset
only by its visual attributes. For this case, the spatial rela-
tions in the layout are necessary.
A number of works have been established on S IMMC
2.0. Based on different multi-modal Visual-Language pre-
training Models (VLM), previous researchers pay more at-
tention to learning the visual attributes of assets. QS Goal
Diggers (Kottur et al. 2021a) and Kakao Enterprise (Lee and
Han 2021) directly insert visual attributes into models input,
while Sogang University (Kottur et al. 2021b) and A-STAR
(Nguyen et al. 2021) build a set of visual attributes predic-
tion tasks in pre-training stage. KAIST (Lee et al. 2022) de-
signs an auxiliary task to predict visual attributes. However,
less attention has been paid to building spatial relations be-
tween assets. All existing models only use the coordinates of
asset bounding boxes as positional information, which can-
not capture spatial relations in the scenes.
As a result, the models can accurately generate ‚Äùthe black
jacket‚Äù but fail to describe more natural referring expres-
sions like ‚Äùthe black jacket on the leftmost Ô¨Çoor rack‚Äù . It is
obvious that the later expression is more useful in a scene
including lots of clothes with similar attributes. The combi-
nation of attributes and spatial relations helps people locate
assets quickly. To generate this type of expression, a model
needs to learn not only the visual attributes of each asset but
also spatial relations between different items.
To address above problem, we propose Situated Conver-
sation Agent PRetrained with Multimodal Questions fromarXiv:2301.01949v1  [cs.CL]  5 Jan 2023INcremental Layout Graph ( SPRING ), which is pretrained
with multimodal questions generated from incremental lay-
out graph. In our method, we design Incremental Layout
Graph (ILG) for each scene to capture rich spatial relations
between different scene items. Unlike scene graph (Chang
et al. 2020), an ILG is built using pure textual information
and can be extended incrementally with newly added dia-
logue. And then, two types of Multimodal Question An-
swering (MQA) pre-training tasks and corresponding QA
pairs are collected by traversing nodes (digital assets and
background items) on the ILG. According to the spanned
path length, QA pairs can be automatically annotated with
difÔ¨Åculty levels. Curriculum Learning (Bengio et al. 2009)
is therefore employed for pre-training on a Transformer
(Vaswani et al. 2017) encoder-decoder backbone. Experi-
ments on both S IMMC 1.0 and S IMMC 2.0 show that our
method improves the response quality by a large margin
compared to previous best models.
The main contributions of our work are as follows:
We Ô¨Årst propose a novel approach to build ILGs for vir-
tual scenes from dialogue text incrementally. The ILGs
include scene items with relations. It is worth noting that
this process does not rely on any human annotation.
Based on ILGs, we introduce two types of new MQA
pretraining tasks that can facilitate model understanding
of visual metadata and spatial relations between different
assets. Pre-training samples are automatically generated
by traversing the ILG, which also generates an accompa-
nying difÔ¨Åculty label for curriculum learning.
We conduct thorough experiments to verify that our ap-
proach effectively enhances response quality. Our ap-
proach outperforms existing state-of-the-art methods by
a signiÔ¨Åcant margin consistently on all metrics on both
SIMMC 1.0 and S IMMC 2.0.
2 Related Works
Situated Interactive Multimodal Conversations. Con-
versation systems have developed rapidly in recent years,
e.g., task-oriented conversations pretraining (He et al.
2022c,a,b), knowledge-based conversations (Hui et al. 2022;
Wang et al. 2022a) and so on. Among them, multimodal
conversations are the new trend. META releases multimodal
conversation datasets SIMMC (Kottur et al. 2021b) based on
VR shopping stores. There are hundreds of scene snapshots
from different angles. Compared with the previous multi-
modal dialogue datasets MMD (Saha, Khapra, and Sankara-
narayanan 2018) and VisDial (Das et al. 2017), the situ-
ated agent is required to generate more complex visual at-
tributes and more detailed spatial relations to infer digital
assets in the scene. Kottur et al. (2021a) has preliminary
explorations on utilizing visual attributes and spatial rela-
tions. Concretely, DialVinVL (Kottur et al. 2021a) incorpo-
rates slot values about visual attributes with dialogue history
as textual input and concatenates original box coordinates
to region features extracted by the object detector as visual
input. JMGPT (Kottur et al. 2021b) and JointGM (Nguyen
et al. 2021) apply language model to predict visual attributes
and system response jointly. MMBart (Lee et al. 2022) addsembedded box coordinates to textual embedding as Trans-
former input and designs auxiliary tasks to predict visual at-
tributes according to the output of encoder hidden states. We
can Ô¨Ånd that their utilized spatial information is all from the
bounding box. Unlike these methods, we Ô¨Årst notice the lack
of VLM‚Äôs capability for visuality and spatiality, and then
propose MQA pretraining tasks based on incremental layout
graphs which have been successfully applied to (Qiu et al.
2021; Liao et al. 2021; Hui et al. 2021; Qiu et al. 2022).
Visual Language Pretraining. To improve models‚Äô per-
ception of text and image and help them establish connec-
tions between multimodal information, kinds of visual lan-
guage pretraining models are designed. ViLBERT (Lu et al.
2019) and UNITER (Chen et al. 2020) propose to con-
sider the raw output of the detector, a distribution of object
classes, as soft labels and optimize the KL-divergence be-
tween two distributions. LEXMERT (Tan and Bansal 2019)
and UNIMO (Li et al. 2021) propose Masked Region Fea-
ture Regression (MRFR) regresses the masked region fea-
ture to its corresponding original region feature, where rep-
resents images as a sequence of region features by Faster
R-CNN (Ren et al. 2015). Furthermore, SOHO (Huang et al.
2021b) is designed to avoid information leakage from neigh-
bor features when images are converted into grid features or
patch features.
Recently, CLIP (Radford et al. 2021) and ALIGN (Jia
et al. 2021) leverage large-scale image-text pairs to learn
transferable visual representations and exhibit surprising
zero-shot transfer to image classiÔ¨Åcation tasks. VL-T5 (Cho
et al. 2021) and OFA (Wang et al. 2022b) introduce down-
stream tasks, like visual grounding and grounded caption,
into pretraining tasks to narrow the gap between pretrain-
ing and Ô¨Åne-tuning. Unlike these efforts, we design new pre-
training tasks through a uniÔ¨Åed QA paradigm to improve ex-
isting methods‚Äô visual attributes and spatial relations model-
ing without adding new modules.
3 Methods
LetD=f(Ut;Rt;It)gT
t=1be an ongoing dialogue between
a user and an agent with Trounds, where Utis the user
utterance at time step t,Rtis the language response to Utby
the agent, and Itis the accompanying scene image. The task
here is to predict the optimal language response R0
t, given
the dialog history Ht= [Ui;Ri]t 1
i=1, current user utterance
Utand scene image It, as modeled in Eq (1)
R0
t=argmaxP (RtjHt;Ut;It) (1)
whereis the model learnable parameters.
To solve above problem, we propose a mutimodal dia-
logue model SPRING , which is pretrained with mutlimodal
questions generated from incremental layout graph. In the
following sections, we will introduce model architecture,
ILG generation and MQA pretraining tasks in order.
3.1 Architecture
The backbone of SPRING model is encoder-decoder based
single-stream VLM framework, which are stacks of Trans-
former (Vaswani et al. 2017) layers. The scene image It2inoninininon
Do you have a nice coat from Downtown Consignment ?I have the black coatin the second row of the third compartment in the leftmost cupboard.
User( Dialogue History  ‚Ä¶ )
My next question is whether you have quality t-shirt to show me ?How about the pink t-shirt in the top rowon the back display walland the blue t-shirtin the bottom row?
UserAgent( Dialogue History  ‚Ä¶ )
I need a blouse as well.How about the brown blouse in the bottom row on the back display wall and the grey blousein the middle rowin the second compartment in the leftmost cupboard. ?
UserAgent( Dialogue History  ‚Ä¶ )pinkt-shirtID 42toprowbluet-shirtID 33bottomrowbackdisplaywallbrownblouseID 9bottomrowblackcoatID16secondrowthirdcompart-mentleftmostcupboard
greyBlouseID 20secondrowsecondcompart-mentleftmostcupboardQuery Asset ID & BBox by Visual Attribute 
Query Asset ID & BBox by Visual Attribute 
Query Asset ID & BBox by Visual Attribute in
ininof
of[412, 323, 479, 588]
[656, 263, 701, 399][699, 410, 740, 598][661, 421, 695, 592]
[218, 354, 230, 579]top/downon[661, 421, 695, 592]blackcoatID16secondrowthirdcompart-mentleftmostcupboardininof
greyBlouseID 20inbluet-shirtID 33
in[699, 410, 740, 598]
secondrowthirdcompart-mentof[218, 354, 230, 579]inpinkt-shirtID 42toprowbackdisplaywallinon
[656, 263, 701, 399]right/leftbrownblouseID 9bottomrowinright/leftright/leftright/left
Part of Background ItemBackground ItemDigital AssetSpatial RelationshipILG
[412, 323, 479, 588]Agent
backdisplaywallFigure 2: Construction of Incremental Layout Graph from dialogue. Digital assets and background items constitute ILG nodes
while spatial relations form ILG edges. ILG is continuously incremented with newly added dialogue under the same scene.
Rhwcis splitted to Ppatches. And each patch is projected
to visual embedding of the model hidden size. The dialogue
history and current user utterance are converted to sub-word
sequence by Byte-Pair Encoding (BPE) and then embedded
to textual embedding. All visual embedding and textual em-
bedding are concatenated as model input.
To facilitate SPRING to better understand the informa-
tion of embodied scenes, we propose a series of MQA pre-
training tasks based on layout graphs Gi. As there is no an-
notated layout graph in the S IMMC dataset, we propose an
unsupervised ILG construction method based on natural lan-
guage dialog history.
3.2 Incremental Layout Graph (ILG)
We observe that visual attributes and spatial descriptions
exist in the dialogue history. Compared with dataset an-
notations, the information from dialogue is more detailed.
For example, S IMMC 2.0 annotation only gives bounding
boxes of digital assets and four types of relative position
(up, down, right, left) between them, while dialogues in-
clude the absolute position of background items and their
relative position with assets. A crucial discovery lies in the
co-coreference between dialogue history and response, the
same assets present in the responses to be generated.
Therefore, we propose an ILG generation algorithm to ex-
tract high quality information from dialogues and generate
Incremental Layout Graph (ILG) Gi=hVi;Eiito dispose
them, whereVidenotes the node set containing the digital
assets and background items from dialog history and Eirep-
resents the edge set depicting spatial relations between scene
itemsVi.
Textual Information Extraction and Alignment we
consider adopting a rule-based textual information extrac-
tion method, i.e., regular expression, to extract visual at-
tributes and spatial descriptions from dialogue history with-
out human annotation. The regular expressions RegExpva
andRegExpsdforvisual attribute andspatial description
are as follows.RegExpva= (art:) (color) (asset type) (2)
RegExpsd= (positional prep :) (art:) (:?) (punc:)(3)
where art. is article, prep . represents preposition and punc .
means punctuation. Please refer to Appendix for details.
With these two regular expressions, as left part of Figure 2
shows, we can extract visual attribute ‚Äùblack coat‚Äù and spa-
tial description ‚Äùin the second row of the third compartment
in the leftmost cupboard‚Äù from dialogue history.
Although the visual attributes and spatial descriptions
extracted by the above regular expressions are naturally
aligned because of language features, they are not aligned
with asset IDs, making asset box coordinates unusable. To
solve this problem, we query the color and type of assets
from the database by their IDs to compose visual attributes
like ‚Äùblack coat‚Äù and then try pairing them with the ex-
tracted visual attributes like ‚Äùblack coat‚Äù . If these two visual
attributes match, the corresponding asset ID ‚Äù16‚Äù can be de-
termined, from which we can get the paired asset IDs, visual
attributes, and spatial descriptions. We further design the fol-
lowing two regular expressions to extract background item
andrelative spatial relation from extracted spatial descrip-
tions.
RegExpbi= (background item) (4)
RegExpsr= (positional prep :) (5)
where RegExpbiandRegExpsrdenote regular expressions
for background item and spatial relation, prep . represents
preposition. With these two regular expressions, as middle
part of Figure 2 shows, we can extract background items
‚Äùsecond row‚Äù ,‚Äùthird compartment‚Äù and ‚Äùleftmost cup-
board‚Äù and relative spatial relations ‚Äùin‚Äù ,‚Äùof‚Äù from spatial
description obtained previously.
Incremental Layout Graph Generation With rich infor-
mation extracted from a sample of dialogue history, lay-
out sub-graph can be generated as middle part of Figure 2
shows. In the layout sub-graph, digital asset node store its
visual attributes like ‚Äùblack coat‚Äù and asset ID ‚Äù16‚Äù while
background item nodes store background items like ‚Äùsec-
ond row‚Äù ,‚Äùthird compartment‚Äù and‚Äùleftmost cupboard‚Äù .[Pure Visual QA]  What is the type of asset 9 and 16 ?
[Region-Guided Visual QA] What is the color of asset 42 
in region ? Region: [656, 263, 701, 399]
[Position-Guided Visual QA]  What are types of asset 20 
in second row of the third compartment and asset 42 in 
the top row on the back display wall?
[Pure Spatial QA] Where is asset 33 , 9 and 20?
[Region-Guided Spatial QA] Where is asset 16 in region ? 
Region: [412, 323, 479, 588]
[Visual Attribute-Guided Spatial QA] Where is the pink 
t-shirt and grey blouse ?
Spatial QAVisual QA
SPRING
Blouse and coat.
Pink.
Blouse and t-shirt.
In the bottom row, in the bottom row on the 
back display wall and in the second row of the 
second compartment in leftmost cupboard .
In the second row of the third compartment in 
the leftmost cupboard.
In the top row on the back display wall.
2
1
5
8
4
2
NDifficulty LevelCurriculum LearningILGFigure 3: Demonstration of S PRING model and two types of MQA pretraining tasks, Visual QA and Spatial QA. Curriculum
learning based on QA pair difÔ¨Åculty level activates the potential of MQA tasks.
SCENE IMAGE QA T YPE QUESTION TEMPLATE ANSWER
Pure Visual QA What is the [visual attribute type] of item [asset ID] ? [visual attribute value]
Region-Guided Visual QA What is the [visual attribute type] of item [asset ID] in region? Region: [x1, y1, x2, y2] [visual attribute value]
Position-Guided Visual QA What is the [visual attribute type] of item [asset ID] [position] ? [visual attribute value]
Pure Spatial QA Where is the item [asset ID] ? [position]
Region-Guided Spatial QA Where is the item [asset ID] in region? Region: [x1, y1, x2, y2] [position]
Visual Attribute-Guided Spatial QA Where is the [item color] [item type] [asset ID] ? [position]
Table 1: QA pair template. Square brackets ‚Äò []‚Äô represent slots to be Ô¨Ålled by traversing ILGs.
Spatial relations and queried bounding boxes are utilized to
deÔ¨Åne layout sub-graph edges. As the right part of Figure 2
shows, the scene ILG continuously increments with newly
added sub-graph about the same scene, which Ô¨Ånally can
include all digital assets, background items, and spatial rela-
tions between them under this scene. Mining information on
the ILG is simple but effective. The visual attributes can be
easily obtained by traversing the ILG nodes, while multiple
types of spatial relations can be inferred by walking along
the ILG edges.
3.3 ILG-based MQA Pre-training Tasks
To enhance response generation quality of visual attributes
and spatial relations, we design visual QA pre-training task
and spatial QA pre-training task based on Multimodal Ques-
tion Answering (MQA), which respectively contain three
types of novel sub-tasks. As shown in Figure 3 and Table
1, all QA pairs are automatically generated by traversing
ILG and Ô¨Ålling the corresponding template. The QA pair
generation algorithm is displayed in Algorithm 1. Formula-
rly, we use the question template Ô¨Ålling function Qtype()to
generate question, Atype represents corresponding answer,
Typevameans visual attribute type, ID asset denotes asset ID,
Iscene is scene image, BBox asset means asset region coor-
dinates,tsrrepresents spatial relation, tvais visual attribute,
tbidenotes background item.
3.3.1 Visual QA
Pure Visual QA (PVQA) As the most basic visual QA
task, the goal of Pure Visual QA is to help the model es-
tablish connections between asset ID and corresponding vi-
sual attributes when a scene image is provided. We design
PVQA template in which the question prompts the type ofvisual attribute and asset ID. The pure visual question can
be generated by traversing the asset nodes of ILG and Ô¨Ålling
[asset ID] slot in the template while answers are gener-
ated based on the visual attributes stored in asset nodes. The
objective of PVQA task is the following.
L= PN
i=1logP(ApvjQpv(Typeva;IDasset);Iscene) (6)
Region-Guided Visual QA (RVQA) To improve the
model‚Äôs ability of locating asset and describing its visual
attribute by region visual context, we design RVQA tem-
plate based on PVQA, in which the question is guided by
asset region coordinates and asset ID. The region-guided
visual question can be generated by traversing asset nodes
of ILG and Ô¨Ålling [asset ID] , bounding box coordinates
[x1, y1, x2, y2] slots in the template. The corresponding
answer is produced based on the visual attributes stored in
asset nodes. The objective of RVQA task is the following.
L= PN
i=1logP(ArgvjQrgv(Typeva;IDasset;BBoxasset);Iscene)(7)
Position-Guided Visual QA (PoVQA) In the conversa-
tions, instead of region coordinates, an agent has to locate
asset by its spatial information no matter when understand-
ing user utterances or making recommendations. To bring
the question closer to a real conversation, we design PoVQA
template by replacing region coordinates in RVQA with spa-
tial relations. For position-guided visual question template,
the[asset ID] slot can be Ô¨Ålled by traversing asset nodes
of ILG while the [position] slot is Ô¨Ålled by spatial rela-
tion path between asset nodes and background item nodes.
The corresponding answer is produced based on the visual
attribute stored in asset nodes. The objective of PoVQA task
is the following.
L= PN
i=1logP(ApgvjQpgv(Typeva;IDasset;tsr);Iscene)(8)Algorithm 1: QA Pair Generation
Input:
ILGGi=hVi;Eii, QA template list T
Output:
QA pair list QA, difÔ¨Åculty label list DL
1: Initialize QA pair list QAand difÔ¨Åculty label list DL
2:fornode inEido
3: ifTypeOf (node ) = "background item "then
4: Skip node
5: /* Get information from digtal asset node */
6: (tva; ID asset; BBox asset ) GetInfo (Gi; node )
7: /* Walk from node to get spatial relations */
8: (tbi; tsr) Walk (Gi; node )
9: tslot (tva; tbi; tsr; BBox asset; ID asset )
10: fortemplate inTdo
11: /* Fill in the template */
12: (qa; dl ) FillIn (template; t slot)
13: Add QA qa; DL dl
3.3.2 Spatial QA
Pure Spatial QA (PSQA) As the most basic spatial QA
task, the goal of PSQA is to help the model establish connec-
tions between asset ID and corresponding spatial relations
when a scene image is provided. We design PSQA template
in which the question only prompts ‚Äùwhere‚Äù and asset ID.
The pure spatial question can be generated by traversing the
asset nodes of ILG and Ô¨Ålling [asset ID] slot in the tem-
plate, while answers are generated based on the spatial rela-
tion paths between the background item node and the asset
node. The objective of PSQA task is the following.
L= PN
i=1logP(ApsjQps(IDasset);Iscene) (9)
Region-Guided Spatial QA (RSQA) To improve the
model‚Äôs ability of locating an asset and describing its spatial
relations by region visual context, we design RSQA tem-
plate based on PSQA, in which the question is guided by
asset region coordinates and asset ID. The region-guided vi-
sual question can be generated by traversing asset nodes of
ILG and Ô¨Ålling the slots of [asset ID] , bounding box coor-
dinates [x1, y1, x2, y2] in the template. The correspond-
ing answer is produced based on the spatial relation paths
between the background item node and the asset node. The
objective of RSQA task is the following.
L= PN
i=1logP(ArgsjQrgs(IDasset;BBoxasset);Iscene)(10)
Visual Attribute-Guided Spatial QA (VSQA) In the
conversations, instead of region coordinates, an agent has
to locate an asset by its visual attribute no matter when un-
derstanding user utterances or making recommendations. To
bring the question closer to a real conversation, we design
VSQA template by replacing region coordinates in RSQA
with visual attributes ( e.g.color, type). For spatial-guided vi-
sual question template, the [asset ID] slot can be Ô¨Ålled by
traversing asset nodes of ILG while the [item color] and
[item types] slots are Ô¨Ålled by the visual attribute stored
in asset nodes. The corresponding answer is produced based
on the spatial relation paths between the background item
node and the asset node. The objective of VSQA task is thefollowing.
L= PN
i=1logP(AvagsjQvags(IDasset;tva);Iscene)(11)
3.4 MQA-Based Curriculum Learning
Automatic DifÔ¨Åculty Level Annotation When generat-
ing QA pairs by walking on the ILG, the number of nodes
spanned by the pathway can be recorded. The more nodes
the path passes through, the more scene information con-
tained in the corresponding QA pair, which means that the
multimodal dialogue model needs more hops to make infer-
ences. Therefore, we automatically label the difÔ¨Åculty level
of each QA pair according to the number of nodes the path
spans. For example, when generating the question ‚ÄúWhere is
the brown jacket 83 & 1055?‚Äù and the answer ‚Äúit is on the
Ô¨Çoor rack near the entrance. ‚Äù , one asset node ‚Äúbrown jack
83 & 1055‚Äù and two background item nodes are spanned on
the ILG. The difÔ¨Åculty level of this QA pair is annotated as
3. The following is the formal expression.
d=jVspannedj
D(12)
whereddenotes the normalized difÔ¨Åculty level of QA pair,
jVspannedjrepresents the number of ILG nodes spanned by
corresponding path, Dis the maximum value of ILG nodes
spanned by the QA pair path in the dataset.
Pretraining Strategy With automatically annotated difÔ¨Å-
culty labels, we propose MQA based curriculum learning
to activate the potential of our designed MQA pretraining
tasks. We deÔ¨Åne the model competence cas follows.
c(t) =q
t
T+ 
1 t
T
min2(d) (13)
wheretis the index of current training step, Trepresents
the maximum number of training steps, min2(d)means the
minimum value of difÔ¨Åculty level d,andare hyper-
parameters, is determined by asq
1
. Here we set 
to 1.2 andto 0.8. At a given training step t, QA pair with
difÔ¨Åculty smaller than or equal to c(t)(i.e.dc(t)) will be
sampled for training. As such, our pretraining strategy fo-
cuses on QA pairs with lower difÔ¨Åculty in the early stage,
aiming at helping the model form preliminary perception
and inference capabilities for scene items. In the middle and
late stages, more difÔ¨Åcult QA pairs are added, which im-
proves the model‚Äôs ability to generate visual attributes and
spatial relations for multiple assets.
After MQA pretraining, SPRING model is Ô¨Åne-tuned on
the S IMMC response generation task. The auto-regressive
language modeling objective is the following.
L= PN
i=1logP(RijHi;Ui;Ii) (14)
whereNdenotes the total number of training samples.
4 Experiment
4.1 Set up
Datasets. To evaluate the performance of the proposed
model, we Ô¨Årst conduct experiments on widely-used situated
multimodal dialogue datasets S IMMC 1.0 and S IMMC 2.0.MODELS BLEU-1 BLEU-2 BLEU-3 BLEU-4 METEOR ROUGE CIDEr VISUAL SPATIAL
SIMMC 1.0
MN-MAG (Kim et al. 2021) 27.28 16.75 12.32 9.50 16.62 32.35 0.8694 9.49 9.10
Tom (Jeong et al. 2021) 28.95 18.81 14.23 11.10 18.83 38.18 1.5014 11.13 10.17
JBi-encoder (Huang et al. 2021a) 26.76 16.76 12.49 9.60 17.65 36.46 1.2345 9.73 9.43
SPRING (Ours) 32.46 22.15 17.23 13.77 20.75 40.51 1.6329 13.53 12.60
SIMMC 2.0
MTN (Kottur et al. 2021b) 62.38 44.52 32.90 21.70 21.38 38.50 1.1207 19.91 14.95
JMGPT (Kottur et al. 2021b) 51.05 35.03 24.66 19.20 14.73 29.18 0.7738 13.67 11.54
JMGPT-BS (Kottur et al. 2021a) 64.86 48.86 37.91 28.38 22.43 43.88 1.9669 22.10 14.56
JointGM (Nguyen et al. 2021) 64.40 48.54 37.69 34.62 21.91 42.44 1.8265 21.77 15.82
MMBart (Lee et al. 2022) 69.89 52.99 41.32 33.10 24.79 46.60 2.1887 26.19 21.11
DialVinVL (Kottur et al. 2021a) 75.38 57.42 44.92 34.90 27.09 51.24 2.3426 29.92 22.55
GPTDeIT (Lee and Han 2021) 68.43 52.23 40.95 28.50 24.81 47.80 2.2271 25.04 18.06
GLIMMeR (Hemanthage et al. 2021) 74.05 56.85 44.88 35.31 27.48 50.92 2.4952 32.70 22.58
SPRING (Ours) 83.29 64.75 52.41 42.49 31.90 57.12 3.1351 38.87 30.77
Table 2: Comparison on S IMMC 1.0, S IMMC 2.0 dataset, visual and spatial subsets. Our model consistently outperforms strong
baselines by a large margin on 7 widely-used metrics. Specially, evaluation on Visual Subset andSpatial Subset by BLEU-4
effectively verify the huge improvement of our model comes from better response about visual attribute and spatial relation.
The S IMMC 2.0 dataset contains 7.2k fashion dialogs and 4k
furniture dialogs, respectively. There are around 290 digital
assets for fashion and 110 assets for furniture, which are re-
arranged within seed scenes to generate 160 different scenes.
The S IMMC 1.0 dataset includes 6.6k fashion dialogs and
6.4k furniture dialogs. We evaluate model performance on
the dev-test split of S IMMC 1.0 and S IMMC 2.0, which has
the same scale as the test-std1dataset.
In addition, we invite human experts to Ô¨Ålter responses
with visual attribute or spatial relation from S IMMC 1.0 and
SIMMC 2.0 dev-test split to construct Visual Subset and
Spatial Subset . We further evaluate models on these two
subsets to prove the effectiveness of our model.
Evaluation Metrics. The ofÔ¨Åcial metric adopted by
SIMMC 2.0 response generation task is BLEU-4, which only
focuses on n-grams overlap between the predicted and tar-
get response. For a more comprehensive comparison, we add
widely-used machine generation metrics: BLUE-n (Papineni
et al. 2002), METEOR (Banerjee and Lavie 2005), ROUGE
(Lin and Hovy 2003) and CIDEr (Vedantam, Zitnick, and
Parikh 2015) metrics. Compared with the accuracy based
BLEU metric, METOR and ROUGH pay attention to recall
and calculate how many n-grams from the target response
exist in the predicted response, while CIDEr uses TF-IDF to
assign larger weights to infrequent phrases.
Implementation Details. Our model is based on Trans-
former (Vaswani et al. 2017) structure with 12 layers, where
ever Transformer block has 768 hidden units and 12 atten-
tion heads. Each patch is projected to features of the same
size as the hidden units. We initialize S PRING parameters
from pretrained VLM, i.e., OFA (Wang et al. 2022b). Dur-
ing pretraining, our model is trained for 4 epochs with 8
batch sizes on 8 TESLA V100 GPU. Adam (Kingma and
Ba 2015) is adopted as optimizer with a 4e-4 learning rate.
Besides, the dropout rate is set to 0.2 to prevent over-Ô¨Åtting.
During Ô¨Åne-tuning stage, we train 60 epochs on the S IMMC
train set with a learning rate of 4e-5 and a batch size of 16.
1Not publicly available as a test set for the DSTC competition.Compared Methods. We compare S PRING with strong
baseline methods from S IMMC 1.0 and S IMMC 2.0. On
SIMMC 1.0, MN-MAG (Kim et al. 2021) adopts a memory
network as encoder and designs multimodal fusion gate to
fuse information. Tom (Jeong et al. 2021) esambles predic-
tion results from several GPT-2 models. JBi-encoder (Huang
et al. 2021a) is jointly trained to predict belief state and re-
sponse. On S IMMC 2.0, MTN (Le et al. 2019) separately en-
codes multimodal input while the visual encoder is guided
by a query-aware attention encoder. JMGPT (Kottur et al.
2021b) trains a multi-task GPT2-large, which takes dialogue
history and Ô¨Çattened multimodal contexts as input. Further-
more, JMGPT-BS (Kottur et al. 2021a) extends JMGPT by
inferring with different beam search sizes. MMBart (Lee
et al. 2022) adds box coordinates embedding to textual in-
put and proposes auxiliary tasks to predict asset attributes.
DialVinVL (Kottur et al. 2021a) is based on VinVL-Base
(Zhang et al. 2021), concatenates original box coordinates
to region features as visual input, and incorporates dialogue
history with dialogue policy as textual input. GPTDeIT (Lee
and Han 2021) utilizes GPT2-large (Radford et al. 2019)
as the text model to encode dialogue history and Ô¨Çattened
slot values and DeIT-I (Touvron et al. 2021) as the image
model to encode assets referenced by current turn utterance.
JointGM (Nguyen et al. 2021) leverages BART-large (Lewis
et al. 2020) to predict disambiguation label, belief state and
response jointly according to inputted dialogue history. Sim-
ilar to GPTDeIT, GLIMMeR (Hemanthage et al. 2021) also
leverages GPT2-large and utilizes asset scene ID to help
the model understand the semantics of each asset. Notably,
GLIMMeR is the state-of-the-art method on S IMMC 2.0 and
achieves the winner of the DSTC10.
4.2 Overall Performance
Table 2 displays the results of the model on the S IMMC
1.0 and S IMMC 2.0 dataset response generation task. It
can be seen that S PRING has exceeded previous models by
a large margin and achieved state-of-the-art results on all
representative machine generation metrics. On S IMMC 2.0,TASK MODELS SIMMC 2.0 V ISUAL SPATIAL
VLM 38.22 34.67 25.04
Visual QAVLM
w/ PVQA 40.75 36.54 27.58
w/ RVQA 41.27 37.02 27.22
w/ PoVQA 40.89 35.94 28.08
w/ (PVQA + RVQA + PoVQA) 41.36 37.59 28.24
Spatial QAVLM
w/ PSQA 41.18 36.05 28.30
w/ RSQA 40.77 35.42 28.18
w/ VSQA 40.40 36.34 27.97
w/ (PSQA + RSQA + VSQA) 41.56 36.25 28.49
AllVLM
w/ all QA 41.92 38.52 30.18
w/ (all QA + CL) 42.49 38.87 30.77
Table 3: Ablation study on S IMMC 2.0 dataset with BLEU-4
metric. Red and green shades represent a stronger advantage
in the visual and spatial subsets, respectively.
SPRING is respectively 7.91, 7.33, 7.49, and 7.18 higher
than previous best models on BLEU-n, varying n from 1 to
4. The signiÔ¨Åcant increased percentage on BLEU-n mani-
fests our model successfully utilizing more accurate words
and phrases to make responses. Our model also shows ex-
cellent performance on recall-based metrics METEOR and
ROUGE, of which the score improvements reach 4.42 and
6.2. When the CIDEr metric pays more attention to infre-
quent n-grams, S PRING still outperforms GLIMMeR with
0.64 on CIDEr. Besides, according to the right part of Ta-
ble 2, our model exhibits the highest BLEU-4 scores on the
visual subset and spatial subset, which veriÔ¨Åes the improve-
ment of our model is produced by its better understanding
of visual attribute and spatial relation and ability to conduct
reasoning with aligned information to generate more accu-
rate responses.
4.3 Detailed Analysis
Ablation Study. As shown in Table 3, we perform abla-
tion experiments to evaluate the effectiveness of each pre-
training task and curriculum learning strategy in S PRING .
It can be observed that each MQA pretraining task brings
signiÔ¨Åcantly BLEU-4 improvement on the complete S IMMC
2.0 dataset compared with the basic VLM model. SpeciÔ¨Å-
cally, VLM models pretrained with all visual QA tasks per-
form 2.92 higher than baseline on the Visual Subset, while
VLM models pretrained with all spatial QA tasks display
3.45 improvement compared with baseline on the Spatial
Subset, which can verify that visual QA and spatial QA re-
spectively prompt model‚Äôs ability of describing visual at-
tribute and spatial relation. Besides, the last two rows fur-
ther prove that our designed curriculum learning pretraining
strategy effectively activates the potential of QA pretraining
tasks and boosts model performance.
Human Evaluation. The human evaluation mainly fo-
cuses on 4 aspects: Ô¨Çuency ,relevance ,correctness , and
informativeness , which are important for task-oriented di-
alogue systems. We randomly select 500 dialogues from
SIMMC 2.0 dev-test dataset as candidates, and then Ô¨Ålter
these dialogues from the results generated by DialVinVL,
GLIMMeR, and our model. We release evaluation task on
Fluency Relevance Correctness Informativeness1.01.52.02.53.03.54.04.55.0Human Evaluation Score4.474.66
2.77
1.864.83 4.85
3.20
2.244.96 4.94
4.04
3.13DialVinVL
GLIMMeR
OursFigure 4: The human evaluation results on S IMMC 2.0 with
four aspects. Our model displays signiÔ¨Åcant improvement
oncorrectness andinformativeness .
Amazon Mechanical Turk (AMT) and make the last re-
sponse of every selected dialogue evaluated by 10 different
evaluators. Each evaluator scores 1500 generated responses
on 4 aspects according to golden response in blind review
from 1 to 5, simulating a real-life multimodal dialogue sce-
nario. As shown in 4, it can be observed that our model con-
sistently outperforms the other two models on all metrics,
which is in line with automatic evaluation results.
Case Study. To better illustrate the advantage of our
model and display how S PRING prompts model‚Äôs ability of
predicting visual attributes and spatial relations related to
background items, we visualize several generated responses
from our model and existing SOTA model with correspond-
ing user utterance and scene snapshot, as shown in Figure 5.
It can be explicitly observed that: (1) our model is able to
adopt background items to describe the position of target as-
sets. (2) The relative spatial relations between target assets
and background items can be accurately predicted by our
model. (3) our model is equipped with the ability of align-
ing visual attribute to spatial information when multiple as-
sets exist in the response.
Figure 5: Case study on SIMMC 2.0 dataset.
5 Conclusion
In this paper, we propose a novel situated conversation agent
pretraining method named S PRING . SpeciÔ¨Åcally, all QA
pairs and their difÔ¨Åculty labels used in pretraining are gener-
ated from our Incremental Layout Graph without any extra
human annotations. Experimental results on S IMMC 1.0 and
SIMMC 2.0 show that S PRING greatly surpasses previous
models and describes visual attributes and spatial relations
more accurately.Acknowledgement
We would like to sincerely thank anonymous reviewers for
their suggestions and comments. The work was partially
supported by the National Natural Science Foundation of
China (NSFC62076032). We also want to express our grati-
tude for precious advises given by Guanqi Zhan.
References
Banerjee, S.; and Lavie, A. 2005. METEOR: An Automatic
Metric for MT Evaluation with Improved Correlation with
Human Judgments. In Goldstein, J.; Lavie, A.; Lin, C.; and
V oss, C. R., eds., Proceedings of the Workshop on Intrinsic
and Extrinsic Evaluation Measures for Machine Translation
and/or Summarization @ ACL .
Bengio, Y .; Louradour, J.; Collobert, R.; and Weston, J.
2009. Curriculum learning. In Proceedings of the Inter-
national Conference on Machine Learning (ICML) .
Chang, X.; Ren, P.; Xu, P.; Li, Z.; Chen, X.; and Hauptmann,
A. 2020. A survery of scene graph: Generation and applica-
tion. In Proceedings of IEEE Transactions on Neural Net-
work Learning .
Chattopadhyay, P.; Yadav, D.; Prabhu, V .; Chandrasekaran,
A.; Das, A.; Lee, S.; Batra, D.; and Parikh, D. 2017. Evalu-
ating Visual Conversational Agents via Cooperative Human-
AI Games. In Proceedings of the Fifth AAAI Conference on
Human Computation and Crowdsourcing (AAAI) .
Chen, Y .-C.; Li, L.; Yu, L.; El Kholy, A.; Ahmed, F.; Gan,
Z.; Cheng, Y .; and Liu, J. 2020. Uniter: Universal image-
text representation learning. In Proceedings of the European
conference on computer vision (ECCV) , 104‚Äì120. Springer.
Cho, J.; Lei, J.; Tan, H.; and Bansal, M. 2021. Unifying
vision-and-language tasks via text generation. In Proceed-
ings of the International Conference on Machine Learning
(ICML) .
Das, A.; Kottur, S.; Gupta, K.; Singh, A.; Yadav, D.; Moura,
J. M.; Parikh, D.; and Batra, D. 2017. Visual dialog. In
Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR) .
De Vries, H.; Strub, F.; Chandar, S.; Pietquin, O.;
Larochelle, H.; and Courville, A. 2017. Guesswhat?! vi-
sual object discovery through multi-modal dialogue. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR) .
He, W.; Dai, Y .; Hui, B.; Yang, M.; Cao, Z.; Dong, J.; Huang,
F.; Si, L.; and Li, Y . 2022a. SPACE-2: Tree-Structured Semi-
Supervised Contrastive Pre-training for Task-Oriented Dia-
log Understanding. In Proceedings of the 29th International
Conference on Computational Linguistics (COLING) .
He, W.; Dai, Y .; Yang, M.; Sun, J.; Huang, F.; Si, L.; and
Li, Y . 2022b. SPACE-3: UniÔ¨Åed Dialog Model Pre-training
for Task-Oriented Dialog Understanding and Generation. In
Proceedings of the 45th International ACM SIGIR Confer-
ence on Research and Development in Information Retrieval
(SIGIR) .
He, W.; Dai, Y .; Zheng, Y .; Wu, Y .; Cao, Z.; Liu, D.; Jiang,
P.; Yang, M.; Huang, F.; Si, L.; et al. 2022c. SPACE:A Generative Pre-trained Model for Task-Oriented Dialog
with Semi-Supervised Learning and Explicit Policy Injec-
tion. Proceedings of the AAAI Conference on ArtiÔ¨Åcial In-
telligence (AAAI) .
Hemanthage, B.; Lemon, O.; Dondrup, C.; and Bartie, P.
2021. GLIMMeR: Global-Local Information-aware Multi-
modal grounding with GPT for Co-reference Resolution. In
DSTC10 challenge wokrshop at AAAI .
Huang, X.; Tan, C. S.; Ng, Y . B.; Shi, W.; Yeo, K. H.; Jiang,
R.; and Kim, J. J. 2021a. Joint generation and bi-encoder
for situated interactive multimodal conversations. In DSTC9
challenge wokrshop at AAAI .
Huang, Z.; Zeng, Z.; Huang, Y .; Liu, B.; Fu, D.; and Fu, J.
2021b. Seeing out of the box: End-to-end pre-training for
vision-language representation learning. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) .
Hui, B.; Geng, R.; Ren, Q.; Li, B.; Li, Y .; Sun, J.; Huang, F.;
Si, L.; Zhu, P.; and Zhu, X. 2021. Dynamic Hybrid Relation
Exploration Network for Cross-Domain Context-Dependent
Semantic Parsing. In AAAI .
Hui, B.; Geng, R.; Wang, L.; Qin, B.; Li, Y .; Li, B.; Sun,
J.; and Li, Y . 2022. S2SQL: Injecting Syntax to Question-
Schema Interaction Graph Encoder for Text-to-SQL Parsers.
InFindings of the Association for Computational Linguis-
tics: ACL 2022 , 1254‚Äì1262. Dublin, Ireland: Association for
Computational Linguistics.
Jeong, Y .; Lee, S. J.; Ko, Y .; and Seo, J. 2021. Tom: End-to-
end task-oriented multimodal dialog system with gpt-2. In
DSTC9 challenge wokrshop at AAAI .
Jia, C.; Yang, Y .; Xia, Y .; Chen, Y .-T.; Parekh, Z.; Pham, H.;
Le, Q.; Sung, Y .-H.; Li, Z.; and Duerig, T. 2021. Scaling
up visual and vision-language representation learning with
noisy text supervision. In Proceedings of the International
Conference on Machine Learning (ICML) .
Kim, B.; Lee, I.; Jeong, Y .; Youngjoong, K.; Koo, M.-W.;
and Seo, J. 2021. Improving multimodal api prediction via
adding dialog state and various multimodal gates. In DSTC9
challenge wokrshop at AAAI .
Kingma, D. P.; and Ba, J. 2015. Adam: A Method for
Stochastic Optimization. In Proceedings of the International
Conference on Learning Representations (ICLR) .
Kottur, S.; Moon, S.; Geramifard, A.; and Damavandi, B.
2021a. Overview of Situated and Interactive Multimodal
Conversations (SIMMC) 2.0 Track at DSTC 10. In DSTC10
challenge wokrshop at AAAI .
Kottur, S.; Moon, S.; Geramifard, A.; and Damavandi, B.
2021b. SIMMC 2.0: A Task-oriented Dialog Dataset for
Immersive Multimodal Conversations. In Proceedings of
the 2021 Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP) .
Le, H.; Sahoo, D.; Chen, N.; and Hoi, S. 2019. Multimodal
transformer networks for end-to-end video-grounded dia-
logue systems. In Proceedings of the 57th Annual Meeting
of the Association for Computational Linguistics (ACL) .Lee, H.; Kwon, O. J.; Choi, Y .; Kim, J.; Lee, Y .; Han, R.;
Kim, Y .; Park, M.; Lee, K.; Shin, H.; and Kim, K.-E. 2022.
Learning to Embed Multi-Modal Contexts for Situated Con-
versational Agents. In Findings of the Association for Com-
putational Linguistics (NAACL) .
Lee, J.; and Han, K. 2021. Multimodal Interactions Using
Pretrained Unimodal Models for SIMMC 2.0. In DSTC10
challenge wokrshop at AAAI .
Lewis, M.; Liu, Y .; Goyal, N.; Ghazvininejad, M.; Mo-
hamed, A.; Levy, O.; Stoyanov, V .; and Zettlemoyer, L.
2020. BART: Denoising Sequence-to-Sequence Pre-training
for Natural Language Generation, Translation, and Compre-
hension. In Proceedings of the 58th Annual Meeting of the
Association for Computational Linguistics (ACL) .
Li, W.; Gao, C.; Niu, G.; Xiao, X.; Liu, H.; Liu, J.; Wu,
H.; and Wang, H. 2021. UNIMO: Towards UniÔ¨Åed-Modal
Understanding and Generation via Cross-Modal Contrastive
Learning. In Proceedings of the 59th Annual Meeting of the
Association for Computational Linguistics (ACL) .
Liao, L.; Long, L. H.; Ma, Y .; Lei, W.; and Chua, T.-S.
2021. Dialogue State Tracking with Incremental Reasoning.
Transactions of the Association for Computational Linguis-
tics, 9: 557‚Äì569.
Lin, C.; and Hovy, E. H. 2003. Automatic Evaluation of
Summaries Using N-gram Co-occurrence Statistics. In Hu-
man Language Technology Conference of the North Ameri-
can Chapter of the Association for Computational Linguis-
tics (HLT-NAACL) .
Lu, J.; Batra, D.; Parikh, D.; and Lee, S. 2019. Vilbert:
Pretraining task-agnostic visiolinguistic representations for
vision-and-language tasks. In Proceedings of the Advances
in neural information processing systems (NIPS) .
Moon, S.; Kottur, S.; Crook, P. A.; De, A.; Poddar, S.; Levin,
T.; Whitney, D.; Difranco, D.; Beirami, A.; Cho, E.; Subba,
R.; and Geramifard, A. 2020. Situated and Interactive Mul-
timodal Conversations. In Proceedings of the 28th Interna-
tional Conference on Computational Linguistics (COLING) .
Nguyen, T.-T.; Shi, W.; Jiang, R.; and jae Kim, J. 2021. Mul-
timodal and Joint Learning Generation Models for SIMMC
2.0. In DSTC10 challenge wokrshop at AAAI .
Papineni, K.; Roukos, S.; Ward, T.; and Zhu, W. 2002. Bleu:
a Method for Automatic Evaluation of Machine Translation.
InProceedings of the 40th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL) .
Qiu, L.; Liang, Y .; Zhao, Y .; Lu, P.; Peng, B.; Yu, Z.; Wu,
Y . N.; and Zhu, S. 2021. SocAoG: Incremental Graph Pars-
ing for Social Relation Inference in Dialogues. In Proceed-
ings of the 59th Annual Meeting of the Association for Com-
putational Linguistics and the 11th International Joint Con-
ference on Natural Language Processing (ACL-IJCNLP .
Qiu, L.; Zhao, Y .; Liang, Y .; Lu, P.; Shi, W.; Yu, Z.; and Zhu,
S.-C. 2022. Towards Socially Intelligent Agents with Mental
State Transition and Human Value. In SIGDIAL .
Radford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.;
Agarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.;
et al. 2021. Learning transferable visual models from naturallanguage supervision. In Proceedings of the International
Conference on Machine Learning (ICML) .
Radford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.;
Sutskever, I.; et al. 2019. Language models are unsupervised
multitask learners. In Arxiv .
Ren, S.; He, K.; Girshick, R.; and Sun, J. 2015. Faster r-cnn:
Towards real-time object detection with region proposal net-
works. In Proceedings of the Advances in neural informa-
tion processing systems (NIPS) .
Saha, A.; Khapra, M.; and Sankaranarayanan, K. 2018. To-
wards building large scale multimodal domain-aware con-
versation systems. In Proceedings of the AAAI Conference
on ArtiÔ¨Åcial Intelligence .
Tan, H.; and Bansal, M. 2019. LXMERT: Learning Cross-
Modality Encoder Representations from Transformers. In
Proceedings of the 2019 Conference on Empirical Meth-
ods in Natural Language Processing and the 9th Interna-
tional Joint Conference on Natural Language Processing
(EMNLP-IJCNLP) .
Touvron, H.; Cord, M.; Douze, M.; Massa, F.; Sablayrolles,
A.; and J ¬¥egou, H. 2021. Training data-efÔ¨Åcient image trans-
formers & distillation through attention. In Proceedings of
the International Conference on Machine Learning (ICML) .
Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,
L.; Gomez, A. N.; Kaiser, ≈Å.; and Polosukhin, I. 2017. At-
tention is all you need. Proceedings of the Advances in neu-
ral information processing systems (NIPS) .
Vedantam, R.; Zitnick, C. L.; and Parikh, D. 2015. CIDEr:
Consensus-based image description evaluation. In IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR) .
Wang, L.; Qin, B.; Hui, B.; Li, B.; Yang, M.; Wang, B.;
Li, B.; Huang, F.; Si, L.; and Li, Y . 2022a. Proton: Prob-
ing Schema Linking Information from Pre-trained Language
Models for Text-to-SQL Parsing. Proceedings of the 28th
ACM SIGKDD Conference on Knowledge Discovery and
Data Mining .
Wang, P.; Yang, A.; Men, R.; Lin, J.; Bai, S.; Li, Z.; Ma,
J.; Zhou, C.; Zhou, J.; and Yang, H. 2022b. OFA: Unify-
ing Architectures, Tasks, and Modalities Through a Simple
Sequence-to-Sequence Learning Framework. In Proceed-
ings of the International Conference on Machine Learning
(ICML) .
Zhang, P.; Li, X.; Hu, X.; Yang, J.; Zhang, L.; Wang, L.;
Choi, Y .; and Gao, J. 2021. Vinvl: Revisiting visual rep-
resentations in vision-language models. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) .SLOT TYPE SLOT VALUE
fashion color‚Äôred, white, yellow‚Äô, ‚Äôred, white‚Äô, ‚Äôpurple‚Äô, ‚Äôwhite, black‚Äô, ‚Äôblack‚Äô, ‚Äôdark grey‚Äô, ‚Äôgrey, black‚Äô, ‚Äôbrown‚Äô, ‚Äôdark green‚Äô, ‚Äôgrey‚Äô, ‚Äôdark blue‚Äô, ‚Äôblue, white‚Äô,
‚Äôgrey, brown‚Äô, ‚Äôwhite, blue‚Äô, ‚Äôyellow, white‚Äô, ‚Äôdark green, dark blue‚Äô, ‚Äôlight grey‚Äô, ‚Äôwhite‚Äô, ‚Äôblue‚Äô, ‚Äôgreen‚Äô, ‚Äômaroon‚Äô, ‚Äôyellow‚Äô, ‚Äôred‚Äô, ‚Äôviolet‚Äô,
‚Äôblack, red, white‚Äô, ‚Äôyellow, black‚Äô, ‚Äôblue, black‚Äô, ‚Äôblack, white‚Äô, ‚Äôlight blue‚Äô, ‚Äôred, black‚Äô, ‚Äôpink, white‚Äô, ‚Äôorange‚Äô, ‚Äôyellow, brown‚Äô, ‚Äôlight pink‚Äô,
‚Äôdark brown‚Äô, ‚Äôpink‚Äô, ‚Äôdark yellow‚Äô, ‚Äôlight red‚Äô, ‚Äôgreen, white‚Äô, ‚Äôgrey, white‚Äô, ‚Äôblack, red‚Äô, ‚Äôgrey, blue‚Äô, ‚Äôbrown, white‚Äô, ‚Äôwhite, black, red‚Äô, ‚Äôbeige‚Äô,
‚Äôlight orange‚Äô, ‚Äôorange, purple‚Äô, ‚Äôdirty green‚Äô, ‚Äôblue, grey‚Äô, ‚Äôblack, grey‚Äô, ‚Äôwhite, grey‚Äô, ‚Äôolive‚Äô, ‚Äôdark red‚Äô, ‚Äôolive, black‚Äô, ‚Äôpink, black‚Äô, ‚Äôblue, green‚Äô,
‚Äôgreen, black‚Äô, ‚Äôlight blue, light green‚Äô, ‚Äôdark pink, white‚Äô, ‚Äôdirty grey‚Äô, ‚Äôdark pink‚Äô, ‚Äôred, grey‚Äô, ‚Äôdark violet‚Äô, ‚Äôolive, white‚Äô, ‚Äôblack, orange‚Äô, ‚Äôgolden‚Äô,
‚Äômaroon, white, blue‚Äô, ‚Äôgreen, violet, pink‚Äô, ‚Äôwhite, red, violet‚Äô, ‚Äôbrown, black‚Äô, ‚Äôblack, olive‚Äô
fashion type ‚Äôblouse‚Äô, ‚Äôjacket‚Äô, ‚Äôshirt‚Äô, ‚Äôsweater‚Äô, ‚Äôdress‚Äô, ‚Äôtshirt‚Äô, ‚Äôjoggers‚Äô, ‚Äôjeans‚Äô, ‚Äôhat‚Äô, ‚Äôtank top‚Äô, ‚Äôvest‚Äô, ‚Äôcoat‚Äô, ‚Äôshoes‚Äô, ‚Äôskirt‚Äô, ‚Äôsuit‚Äô, ‚Äôtrousers‚Äô, ‚Äôhoodie‚Äô
furniture color ‚Äôred‚Äô, ‚Äôblue‚Äô, ‚Äôwhite‚Äô, ‚Äôgrey‚Äô, ‚Äôbrown‚Äô, ‚Äôgreen‚Äô, ‚Äôblack‚Äô, ‚Äôblack and white‚Äô, ‚Äôwooden‚Äô
furniture type ‚Äôarea rug‚Äô, ‚Äôbed‚Äô, ‚Äôchair‚Äô, ‚Äôcoffee table‚Äô, ‚Äôcouch chair‚Äô, ‚Äôend table‚Äô, ‚Äôlamp‚Äô, ‚Äôshelves‚Äô, ‚Äôsofa‚Äô, ‚Äôtable‚Äô
background item‚Äôrack‚Äô, ‚Äôwall‚Äô, ‚Äômirror‚Äô, ‚Äôshelf‚Äô, ‚Äôcloset‚Äô, ‚Äôtable‚Äô, ‚Äôwardrobe‚Äô, ‚Äôcabinet‚Äô, ‚Äôwindow‚Äô, ‚Äôdivider‚Äô, ‚Äôdoor‚Äô, ‚Äôcounter‚Äô, ‚Äôcubby‚Äô, ‚Äôcubbies‚Äô, ‚Äôhanger‚Äô, ‚Äôstand‚Äô,
‚Äôcupboard‚Äô, ‚Äômannequin‚Äô, ‚Äôshoe boxes‚Äô, ‚Äôroom divider‚Äô, ‚Äôwall divider‚Äô
positional preposition ‚Äôin‚Äô, ‚Äôon‚Äô, ‚Äôat‚Äô, ‚Äôbehind‚Äô, ‚Äôtoward‚Äô, ‚Äôto‚Äô, ‚Äôagainst‚Äô, ‚Äôof‚Äô, ‚Äôalong‚Äô, ‚Äôbelow‚Äô, ‚Äôtowards‚Äô, ‚Äôabove‚Äô
article and pronoun ‚Äôthe‚Äô, ‚Äôa‚Äô, ‚Äôthat‚Äô, ‚Äôthis‚Äô, ‚Äôother‚Äô, ‚Äôanother‚Äô
punctuation and conjunctions ‚Äô,‚Äô, ‚Äô.‚Äô, ‚Äô;‚Äô, ‚Äô?‚Äô, ‚Äôand‚Äô, ‚Äôor‚Äô
Table 4: Slot types and slot values in the regular expression.
Appendix
In the Table 4, we display the slot types and slot values of
four regular expressions we use to extract visual attribute,
spatial description, background item, and spatial relation
from dialogue corpus. All color values and type values are
from S IMMC 2.0 metadata while background items are com-
mon furniture words provided by Wikipedia except for those
appear in S IMMC 2.0 furniture types. The article, pronoun,
coordinating conjunction and punctuation come from The
Oxford English Dictionary. To make it clearer, we adopt a
simple python code snippet to show how to use our designed
regular expressions to extract visual and spatial information
in the following.
1immport re
2
3system_response = ‚ÄôHow about the blue
4tshirt on the shelf or the red jacket
5above the table ?‚Äô
6
7RegExp_vi = ‚Äô(a |the|that |this |other |
8another) (red, white, yellow |pink |
9red, white |purple |white, black |black |
10dark grey |light grey |white |blue |green |
11maroon |yellow |red|violet |yellow, black |
12black, red, white |blue, black |
13black, white |light blue |red, black |
14pink, white |orange |yellow, brown |
15light pink |dark brown |pink |dark yellow |
16light red |green, white |grey, white |
17black, red |grey, blue |brown, white |
18white, black, red |light orange |
19orange, purple |dirty green |blue, grey |
20black, grey |white, grey |olive |dark red |
21olive, black |pink, black |blue, green |
22green, black |light blue, light green)
23(blouse |jacket |shirt |sweater |dress |
24tshirt |joggers |jeans |hat|vest |bed|
25coat |shoes |skirt |suit |trousers |hoodie)‚Äô
26
27RegExp_sd = ‚Äô(in |on|at|behind |toward |
28to|against |of|along |below |above) (the |
29a|that |this |other) (. *?) (and |or|,|30\.|\?)‚Äô
31
32RegExp_bi = ‚Äô(rack |wall |mirror |shelf |
33closet |table |wardrobe |cabinet |window |
34divider |door |counter |cubby |cubbies)‚Äô
35
36RegExp_sr = ‚Äô(in |on|at|behind |toward |
37to|against |of|along |below |towards |
38above)‚Äô
39
40extracted_vi = re.findall(RegExp_vi,
41system_response)
42# [(‚Äôthe‚Äô, ‚Äôblue‚Äô, ‚Äôtshirt‚Äô),
43# (‚Äôthe‚Äô, ‚Äôred‚Äô, ‚Äôjacket‚Äô)]
44
45extracted_sd = re.findall(RegExp_sd,
46system_response)
47# [(‚Äôon‚Äô, ‚Äôthe‚Äô, ‚Äôshelf‚Äô, ‚Äôor‚Äô),
48# (‚Äôabove‚Äô, ‚Äôthe‚Äô, ‚Äôtable‚Äô, ‚Äô?‚Äô)]
49
50sds = [‚Äô ‚Äô.join(item[:-1]) for item
51inextracted_sd]
52# [‚Äôon the shelf‚Äô, ‚Äôabove the table‚Äô]
53
54bi_list, sr_list = [], []
55for sdinsds:
56 bi = re.findall(RegExp_bi, sd)
57 bi_list.append(bi)
58 sr = re.findall(RegExp_sr, sd)
59 sr_list.append(sr)
60# bi_list [(‚Äôshelf‚Äô), (‚Äôtable‚Äô)]
61# sr_list [(‚Äôon‚Äô), (‚Äôabove‚Äô)]