1
You Only Train Once: Learning a General Anomaly
Enhancement Network with Random Masks for
Hyperspectral Anomaly Detection
Zhaoxu Li, Yingqian Wang, Chao Xiao, Qiang Ling, Zaiping Lin, and Wei An
Abstract —In this paper, we introduce a new approach to
address the challenge of generalization in hyperspectral anomaly
detection (AD). Our method eliminates the need for adjusting
parameters or retraining on new test scenes as required by most
existing methods. Employing an image-level training paradigm,
we achieve a general anomaly enhancement network for hyper-
spectral AD that only needs to be trained once. Trained on a
set of anomaly-free hyperspectral images with random masks,
our network can learn the spatial context characteristics between
anomalies and background in an unsupervised way. Additionally,
a plug-and-play model selection module is proposed to search for
a spatial-spectral transform domain that is more suitable for AD
task than the original data. To establish a uniﬁed benchmark
to comprehensively evaluate our method and existing methods,
we develop a large-scale hyperspectral AD dataset (HAD100)
that includes 100 real test scenes with diverse anomaly targets.
In comparison experiments, we combine our network with a
parameter-free detector and achieve the optimal balance between
detection accuracy and inference speed among state-of-the-art
AD methods. Experimental results also show that our method
still achieves competitive performance when the training and test
set are captured by different sensor devices. Our code is available
at https://github.com/ZhaoxuLi123/AETNet.
Index Terms —Anomaly detection, Vision Transformer, hyper-
spectral imagery.
I. I NTRODUCTION
HYPERSPECTRAL imagery (HSI) records abundant
spectral information which can reﬂect the essential char-
acteristics of different materials [1]. With the development
of hyperspectral spectrometers, hyperspectral imaging has
been widely applied to different ﬁelds, such as agricultural
estimation [2], civilian rescue [3], mineral exploration [4],
quality monitoring [5], and explosive detection [6]. Among
them, hyperspectral anomaly detection is a critical technology
to ﬁnd targets from background without any prior information.
During the past years, many methods have been developed
for hyperspectral anomaly detection (AD). RX method [7],
named after its proposers, is the cornerstone among AD
methods. It supposes background spectra in the HSI obey
a multi-Gaussian distribution and calculates the Mahalanobis
Manuscript received XXX; revised XXX. This work was supported in part
by the Foundation for Innovative Research Groups of the National Natural
Science Foundation of China under Grant 61921001. Zhaoxu Li, Yingqian
Wang, Chao Xiao, Qiang Ling, Zaiping Lin, and Wei An are with the
College of Electronic Science and Technology, National University of Defense
Technology, Changsha 410073, China (email: lizhaoxu@nudt.edu.cn; wangy-
ingqian16@nudt.edu.cn; xiaochao12@nudt.edu.cn; lingqiang16@nudt.edu.cn;
linzaiping@nudt.edu.cn; anwei@nudt.edu.cn)
Corresponding authors: Zaiping Lin; Qiang Ling.
Inference Speed ( FPS)AETNet (Ours)
28.6FPS, 0.9925mAUC
Non-deep -learning -based
Deep -learning -basedDetection Accuracy ( mAUC )
LRX
KRXQLRX2S-GLRTKSVDDCRKCSRAETNet -KCSR (Ours)
0.15FPS, 0.9948mAUC  
KIFDAED
GRX
AutoAD
LREN
Fig. 1. Comparison of the detection accuracy (mAUC) and inference speed
(frames per second, FPS) of different AD methods on our HAD100 dataset.
distance between the test sample and the background samples
to search anomalies. Depending on whether the global or local
background spectrum is used, RX can be divided into global
RX (GRX) and local RX (LRX) [8]. The later adopts a dual
concentric rectangular window centering around the test pixel
and selects the area between the inner and outer windows as
background samples. Inspired by LRX, many dual-window-
based methods are developed, such as quasi-local-RX (QLRX)
[9], locally adaptable iterative RX (LAIRX) [10], weighted
RX [11], linear ﬁlter-based RX [11], kernel RX (KRX) [12],
cluster KRX (CKRX) [13], support vector data description
method (SVDD) [14], and 2S-GLRT [15]. In addition, some
methodscombined sparse representation with the dual window
and achieved higher detection precision, such as collaborative
representation (CR) [16], constrained sparse representation
(CSR) [17], and dual collaborative representation (DCR) [18].
However, these dual-window-based methods have an inherent
limitation. The size of the dual window needs to be set
manually by users according to the spatial information of
targets. Generally, the inner window size should be larger
than the target size. In addition, the best size of the dual
window is also related to the shape and spacing of targets.
This problem limits the generalization capability of these AD
methods. Multiple-window AD (MWAD) [19] and superpixel-
based dual window RX (SPDWRX) [20] improved the dual
window, but this problem has not been solved well.arXiv:2303.18001v1  [eess.IV]  31 Mar 20232
Many researchers were not limited to the dual-window
framework. Chang [21]–[23] proposed dummy variable trick to
convert hyperspectral target detection methods to AD methods.
Tu et al. [24] introduced density peak clustering [25] to hyper-
spectral AD task. Tao et al. [26] employed fractional Fourier
transform (FrFT) as pre-processing to enhance discrimination
between anomalies and background. But these methods ne-
glecte spatial information and can not deal with non-stationary
background or large targets well. Some methods try to use
spatial features to improve detection performance, such as
attribute and edge-preserving ﬁltering-based detection method
(AED) [27], spectral-spatial feature extraction (SSFE) [28],
structure tensor and guided ﬁlter (STGF) [29], segmentation-
based weighting strategy [30], and kernel isolation forest-
based detection method (KIFD) [31]. Besides, tensor-based
methods pay attention on the 3D structure in HSI and extracted
anomaly tensors from HSI cube data by tensor decomposi-
tion, e.g., tensor decomposition-based method (TDAD) [32],
tensor principal component analysis (TPCA) [33], and prior-
based tensor approximation (PTA) [34]. In addition, low-rank
and sparsity-matrix decomposition (LRaSMD) [35] has also
received widespread attention and derived many AD methods
such as OSP-GoDec [36], OSP-AD [37], SDP [38], component
decomposition analysis (CDA) [39], and effective anomaly
space (EAS) [40]. However, the parameters of the above
methods still need to be ﬁne-tuned on different test scenes.
Lately, deep learning is booming in many ﬁelds and brings
more novel ideas for hyperspectral AD. Generative models
such as Autoencoder (AE) and generative adversarial network
(GAN) [41] can capture nonlinear and potential characteristics
and have been successfully applied to the AD task. A common
practice of AE-based methods is to take the vectors from
the output space or the latent space of AE as the input of
other anomaly detectors [42]–[46]. In particular, in order to
adapt the learned latent representation to a speciﬁc density
estimation, low-rank embedded network (LREN) [45] intro-
duces a trainable density estimation module into the latent
space. Another common practice is to reconstruct the spectra
under certain constraints and regard the spectral reconstruction
error as the detection anomaly score [47]–[50]. This kind
of methods usually use full connection layers to achieve
spectral-level self-supervised learning, and need to introduce
additional spatial information to improve detection accuracy.
For example, robust graph autoencoder method (RGAE) [49]
is embedded with a superpixel segmentation-based graph
regularization term to preserve the geometric structure and
the local spatial consistency of HSI. An exception to this
kind of methods is AutoAD [51], which adopts convolution
autoencoder to reconstruct the original HSI from pure noise
input. Although some methods mentioned above (such as
AutoAD) do not require any manual selection of parameters,
they must be retrained when applying to test scenes.
In summary, two kinds of problems restrict the generaliza-
tion capability of current AD methods: (1) Some parameters
need to be manually set according to the characteristics
of targets in each test scene. (2) Most deep-learning-based
methods need to be retrained to apply to new test scenes.
To tackle these issues, we aim to develop a general networkfor hyperspectral AD. Such a network should be trained only
once without using ground-truth labels and can be generalized
to unseen test scenes without retraining. It is worth noting
that there is very little prior study on this topic. Two recent
methods, WeaklyAD [52] and dual-frequency autoencoder
(DFAE) [53], explore this generalization issue. Speciﬁcally,
WeaklyAD uses a spectral-constrained GAN to enhance the
discrimination between anomalies and background in a weakly
supervised way. DFAE transforms the original HSI into high-
frequency and low-frequency components and then detects
anomalies from these two components in parallel. Both these
two methods are trained on a public test HSI and achieve
competitive results on several test scenes without retraining.
However, these methods are still far from the AD framework
we expect. WeaklyAD needs to extract anomalous spectra
for adversarial training, while DFAE still requires manual
parameter setting. In addition, these two methods have not
been veriﬁed on a large-scale dataset.
In this paper, we introduce an image-level training paradigm
and a simple data augmentation strategy called Random Mask
to solve the generalization problem of hyperspectral AD. The
previous deep-learning-based methods almost perform self-
supervised learning on the 1D spectral level, while our network
performs self-supervised learning on a set of 3D hyperspectral
images. This allows our network to learn the spatial context
information directly without requiring additional spatial con-
straints. Trained on anomaly-free hyperspectral images with
random masks, our network, called AETNet , can achieve
Anomaly Enhancement Transformation on unseen test im-
ages. With the cooperation of the Random Mask strategy
and a structural-difference-based loss function, our AETNet
can learn the spatial-spectral context relationship between
anomalies and background in an unsupervised way. Besides, a
simple plug-and-play model selection module is introduced to
guide the training process and search for a transform domain
that is more suitable for AD task than the original data. Once
trained, our method can project unseen test images into the
searched transform domain, and achieve parameter-free but
high-accuracy detection on these transformed test images.
Moreover, since existing methods generally report their best
results after ﬁne-tuning on a limited number of test images,
a more comprehensive evaluation benchmark is necessary. In
this paper, we further develop a new hyperspectral AD dataset
to evaluate current methods. Our dataset, called HAD100,
contains 100 test HSIs with various anomaly targets. In
addition to the proposed AETNet, we select 16 representative
AD methods as solid baselines to build a comprehensive
benchmark. The contributions of this paper can be summarized
as follows:
1) We introduce an image-level training paradigm to de-
velop a general anomaly enhancement network for
hyperspectral AD. Once trained, our network can per-
form inference directly on unseen test images. As far as
we know, we are the ﬁrst attempt to train the network
on a set of hyperspectral images in this ﬁeld.
2) We provide a simple data augmentation strategy called
Random Mask for hyperspectral AD. This strategy can3
help our network learn the context relationship between
anomalies and background from anomaly-free hyper-
spectral data in an unsupervised way.
3) We develop a new hyperspectral AD dataset and com-
prehensively evaluate the performance of both classic
and SOTA AD methods. To the best of our knowledge,
this is the ﬁrst uniﬁed large-scale benchmark in the
hyperspectral AD ﬁeld.
4) Extensive experiments show that our proposed AETNet
achieves the best balance between detection accuracy
and inference speed among the SOTA AD methods. And
our method can still achieve competitive performance
when the training and test set are captured by different
sensor devices.
The rest of this paper is organized as follows. Section II
reviews the research basic related to our network. Section III
describes the implementation details of the proposed method.
Our dataset and experiments are presented in Section IV,
followed by the conclusion in Section V.
II. R ELATED WORK
In this section, we brieﬂy introduce two studies involved in
our network: UNet and Vision Transformer.
A. U-shaped Network
UNet [54], named after its symmetrical U-shaped structure,
is a simple but widely used segmentation network. UNet can
be split into a contracting path and an expansive path. The con-
tracting path consists of a series of down-sampling modules,
regarded as encoder layers. The expansive path consists of a
series of up-sampling modules, regarded as decoder layers and
symmetric with the encoder layers. Adding skip connections
between the encoder and decoder layers, UNet mitigates the
loss of spatial information due to down-sampling. The fusion
of multi-scale contextual features enhances image reconstruc-
tion process and signiﬁcantly improves the segmentation per-
formance. Nowadays, UNet and its abundant variations are
widely applied to many tasks such as object segmentation [55],
image super-resolution [56], image reconstruction [57], and so
on.
B. Vision Transformer
Transformer [58] is ﬁrst proposed in the natural language
processing and outperforms the previous complicated recurrent
models and CNN models. The success of Transformer has
attracted the great attention of researchers in the computer
vision community. Dosovitskiy et al. [59] are the ﬁrst to
propose Vision Transformer (ViT) for the image classiﬁcation
task. Compared with CNN, ViT has a better capability to
model the long-range dependencies and achieves a global
receptive ﬁeld in the shallow layers [60]. Therefore, CNN
frameworks have been approached and even surpassed by ViT
and its variations [61]–[63] on the image classiﬁcation task.
Nevertheless, expensive training cost makes ViT unsuitable
for dense prediction tasks such as image segmentation. To
this end, PVT [62] adds the pyramid structure to ViT to ac-
quire multiscale feature maps. Furthermore, Swin Transformer[63] limits self-attention computation to non-overlapping local
windows and achieves linear computational complexity with
respect to image size. With the help of a shifted window
strategy, Swin Transformer achieves cross-window informa-
tion interaction. It surpasses the SOTA methods in many tasks,
such as object detection, instance segmentation, and semantic
segmentation. Subsequently, Cao et al. [64] combined Swin
Transformer with UNet and propose the ﬁrst pure Transformer-
based U-shaped architecture, named Swin UNet. Swin UNet
outperforms CNN or the combination of Transformer and
CNN in medical image segmentation and is successfully
applied to other dense prediction tasks [65].
III. M ETHODOLOGY
In this section, we ﬁrst introduce the overview of our
method. Then, we describe the implementation details of each
component of our method.
A. Overview
Our proposed method consists of three major components:
mask generation module, image reconstruction module, and
transform domain search module. The overview architecture
of AETNet is shown in Fig. 2. The backbone of the image re-
construction module is based on Swin UNet and convolutional
autoencoder (CAE), and the input of our network is a 3D HSI.
The training and test data are both hyperspectral cubes with the
same spatial size and band number. The training set is deﬁned
asX=fX1;X2;;XngRHWB, wherenis the
number of HSIs in the training set. H,W, andBdenote a sin-
gle hyperspectral cube’s height, width, and band number. The
test set is deﬁned as Y=fY1;Y2;;YmgRHWB,
wheremis the number of HSIs in the test set.
In the training stage, the mask generation module generates
random masks for each training HSI Xi2X. Then HSIs
with random masks are fed to the image reconstruction mod-
ule. After every training epoch, the transform domain search
module is used to evaluate the enhancement performance of
the reconstruction module and decide whether to terminate the
training.
In the inference stage, each test HSI Yi2Yis sent
to the image reconstruction module, and then we obtain a
reconstructed HSI Yr. Next, anomaly detectors such as GRX
takeYras input to generate the anomaly score map of Yi.
B. Random Mask Strategy
Considering that there is no target label in the training
phase, we introduce a single data augmentation strategy called
Random Mask. Random masking is a popular tool in the
CV community [66] and has been successfully applied to the
industrial AD task [67]. Different from masks in [67], random
masks in our paper have irregular shapes and random sizes
and can simulate the spatial morphology of anomaly targets.
It is usually believed that anomaly targets in hyperspectral
remote sensing imagery have the following characteristics:
(1) Anomaly targets are embedded to background in the
form of irregular blocks. (2) The spectra of anomaly targets4
(a) Training Stage
(b) Inference Stage
Image Reconstruction Module 
Transform Domain Search Module
Mask 
Generation 
Module 
Mahalanobis
DistanceInput Training HSI Masked Training HSI Reconstructed
Training HSIMSGMS Loss 
Function
Input Test HSI Reconstructed Test HSIAnomaly Score Map
Fig. 2. Overview of our proposed method.
are different from those of background. (3) The area ratio
of anomaly targets is low in the total image. As shown in
Fig. 3, we divide the Random Mask strategy into mask map
generation and masked region ﬁlling, simulating the spatial
and spectral characteristics of anomaly targets, respectively.
1) Mask Map Generation: The mask map Mhas the same
heightHand weight Was the input training HSI Xi. The
height and weight are equally divided by K, and thenM
is divided into K2non-overlapping patches. The number of
random masks Nis randomly generated from the integer range
of[Nmin;Nmax], whereNmax should be less than K2. We
randomly select Npatches from all the patches and then
randomly select a pixel position in each selected patch. Next,
an iterative expansion method is used to generate the random
mask at each selected pixel position. We regard the selected
pixel position as the start point of a random mask to be
generated. The four-connected pixels of the start point are
randomly merged into the random mask, and the probability
of every connected pixel being selected is 50%. The mask
is continuously expanded by randomly merging the updated
four-connected pixels until the area reaches A, whereAis
randomly sampled from the integer range of [Amin;Amax].
After traversing all the selected pixel positions, we get N
masks with different areas and random shapes. The ﬁnal mask
mapMis a binary map where the masked regions are set to
0, and the remaining pixels are set to 1.
2) Mask Region Filling: Considering the spectral charac-
teristic of anomaly targets, we introduce an additional cube
I2RHWBto ﬁll the generated masks. The masked input
HSI cubeXMcan be attained by
XM=Xi
M+I
M; (1)where
denotes element-wise multiplication and Mrep-
resents the inversed mask map of M. Inspired by recent
data augmentation approaches, we introduce two mask-ﬁlling
methods, namely CutOut [68] and CutMix [69].
CutOut: CutOut replaces masked regions in RGB images
with gray or black pixels. In this paper, we set Ito a three-
dimensional zero matrix.
CutMix: CutMix replaces masked regions in RGB images
with the corresponding regions from other images. In this
paper, we randomly choose another training HSI from a
different capture ﬂight line as I.
C. Network Architecture
As shown in Fig. 4(a), our network consists of a convo-
lutional encoder, a Swin UNet module, and a convolutional
decoder. The detailed network structure is further described
below.
1) Convolutional Autoencoder: The previous deep-learning
-based methods mainly use fully connected layers to perform
self-supervised learning on the spectral level, resulting in the
loss of spatial information. The most simple improvement
is to use CAE to perform self-supervised learning on the
hyperspectral cubes. CAE combines AE with convolution and
pooling operations and can extract spatial features of images.
We use a simple CAE. The convolutional encoder is a
33convolutional layer, which changes the spectral channel
number of the input cube XmfromBtoC. The convolutional
decoder is also a 33convolutional layer, which restores the
channel number from CtoB.
Common CAEs consist of several convolutional layers,
down-sample layers, deconvolutional layers, and up-sample
layers. Since ViT can model long-range dependencies better5
(a)  Mask Map Generation
(b)  CutOut(c)  CutMix
Input HSIMask Map
Mask MapMasked HSI
Masked HSIInput HSI Inversed Mask Map
Element -wise Multiplication Element -wise Addition
Fig. 3. Diagram of the proposed Random Mask strategy.
than CNN, we abandon CAE’s down-sample and up-sample
operations and only apply the channel dimension transforma-
tion. Meanwhile, we insert a Swin UNet module between the
convolutional encoder and decoder as the middle layer of CAE
to extract spatial and spectral features better.
2) Swin UNet Module: As shown in Fig. 4(a), the Swin
UNet module can also be divided into an encoder, a bottleneck,
and a decoder. Unlike the structure in [64], we adopt a non-
classical structure that directly processes latent feature maps
without patch partition, merging, and expanding operations.
Encoder: The encoder consists of two stages, and each
stage contains a Swin Transformer block and a down-sample
layer. The down-sample layer is a 44strided convolutional
layer which halves the spatial resolution of feature maps and
doubles the channels. In the ﬁrst stage of the encoder, the
feature maps from the convolutional encoder with the size of
HWCare sent into the ﬁrst Swin Transformer block
to perform representation learning. Next, the feature maps
are fed into a down-sample layer, and the size changes into
H
2W
22C. Then, the feature maps are sent into the Swin
Transformer block and the down-sample layer in the second
stage. The size of the feature maps becomesH
4W
44C.
Bottleneck: The bottleneck of the Swin UNet module is
a single Swin Transformer block and learns the deep feature
representation. In the bottleneck, the feature maps’ channel
number and spatial resolution remain unchanged.
Decoder: Symmetric to the encoder, the decoder also has
two stages. In each stage, the feature maps are ﬁrst fed into
an up-sample layer, a 22strided deconvolutional layer, to
halve the channels and double the spatial resolution of feature
maps. Then, we use the skip connection to fuse the up-sampled
feature maps with the feature map of the same size from
the encoder. Channel concatenation on the shallow and deep
feature maps can reduce the information loss caused by down-sampling. After a 11convolutional layer, the concatenated
feature maps have the same channel number as the up-sampled
feature maps. A Swin Transformer block at the end of each
stage is used for the representation learning of the deep feature
maps. After the decoder, the size of the feature maps returns
toHWC.
Swin Transformer Block: Swin Transformer is a shifted-
window-based ViT and contains two different attention mod-
ules, called Window-based Multi-head Self Attention (W-
MSA) module and Shifted Window-based Multi-head Self
Attention (SW-MSA) module, respectively. More details of
W-MSA and SW-MSA can be found in [63]. As shown in
Fig. 4(b), the difference between our Swin Transformer block
and the classical structure in [63] is that LayerNorm (LN)
layers in front of the two MSA modules are removed, which
is validated in our network.
3) Residual Connection: The only difference between the
masked input HSI XMand the expected output HSI Xiis
the masked region whose area accounts for a small portion
of the whole HSI. In other words, our network only needs to
restore the masks to the original spectra rather than restore the
whole HSI. Therefore, we introduce a residual connection to
connect the input XMand outputXrof our network. In this
way, the network can pay more attention to the random masks
and converge faster during training.
D. Loss Function
In the previous AD networks, spectrum-wise L2 loss is
widely used. However, we are more interested in the spatial
texture relationship between anomaly targets and background
rather than the spectral reconstruction accuracy. Spectrum-
wise loss functions usually ignore the spatial dependence of
neighboring pixels. Therefore, a multi-scale gradient magni-
tude similarity (MSGMS) loss [67] is introduced to penalize6
Convolutional  
Encoder
Convolutional  
DecoderSwin
Transformer
Block
Swin
Transformer
BlockSwin
Transformer
BlockSwin
Transformer
BlockConvolutional  
AutoencoderSwin UNet Module(a)  AETNet
D D
Swin
Transformer
Block
U UC
(b) Swin Transformer Block 1C
1
W-MSA LN MLP SW-MSA LN MLP
C D 1 U Channel Concatenation Element -wise Addition Down -sample Layer Up-sample Layer Conv 1×1Input HSI
Reconstructed HSI
Fig. 4. Network structure of the proposed AETNet.
structural differences between the reconstructed HSI Xrand
the original HSI Xi.
The MSGMS is the multi-scale expansion of the gradient
magnitude similarity (GMS) [70]. The gradient magnitude is
deﬁned as the root mean square error of image directional
gradients along two orthogonal directions in [67], [70]. To ac-
quire more accurate gradient magnitude maps, we adopt Sobel
ﬁlters along horizontal, vertical, and diagonal dimensions. The
gradient magnitude map of Xican be calculated according to:
Gi=q
(Xi~sx)2+ (Xi~sy)2+ (Xi~sd1)2+ (Xi~sd2)2;
(2)
where~is the convolution operation, and sx,sy,sd1, andsd2
are33Sobel ﬁlters along the xdimension,ydimension, 45
diagonal, and 135diagonal. Similarly, the gradient magnitude
map ofXrcan be calculated according to:
Gr=q
(Xr~sx)2+ (Xr~sy)2+ (Xr~sd1)2+ (Xr~sd2)2:
(3)
The gradient magnitude similarity map between XrandXi
is then generated by:
GMS (Xi;Xr) =2GiGr+c
Gi2+Gr2+c; (4)
wherecis a constant that ensures numerical stability. Subse-
quently, down-sampled images at different scales of Xrand
Xiare generated by 22strided average pooling operation.
The GMS maps between down-sampled XrandXiare
calculated again according to Eqs. (2), (3), and (4). Finally, theMSGMS loss between down-sampled XrandXiis deﬁned
as:
L(Xi;Xr) =
1
SSX
l=11
HlWlHlX
a=1WlX
b=1
1 GMSa;b
Xil;Xrl
;(5)
whereSis the number of reconstructed HSIs at different
scales, andXilandXjlare the original HSI and the recon-
structed HSI at the l-th scale.HlandWldenote the height and
width of the HSI at the l-th scale, and GMSa;b()represents
the value of the GMS map at pixel (a;b).
The reason why we choose MSGMS instead of GMS is
that the size of anomaly targets is various. The receptive ﬁeld
of GMS is only 33, and thus large masked regions can
only show edges on the gradient magnitude maps. MSGMS
enlarges the receptive ﬁeld by constructing an image pyramid
and can penalize masked regions of different sizes.
E. Transform Domain Search
In the image reconstruction module, masked input HSIs are
regarded as damaged images and need to be reconstructed to
original mask-free images. General image inpainting networks
aim to generate undamaged images accurately, but we are not
interested in that. The phenomenon that the optimized network
can restore the masked training HSIs to the original HSIs
indicates that the network can utilize the context information
of masks and change their spectra in the reconstructed images.
Hence, we believe that the image reconstruction process is
a domain transformation, and regard the output space of
our network as a spatial-spectral transform domain. In the7
transform domain, the anomaly targets should be enhanced
and have more signiﬁcant differences from background.
After each training epoch, the network with updated pa-
rameters generates a new transform domain. We need an
automatic approach to select an appropriate transform domain.
For most image inpainting networks, the reconstruction loss is
an important basis for the termination of training. However, a
good image reconstruction performance does not mean a good
detection performance. In the target detection task, researchers
select trained models according to the detection accuracy on
the veriﬁcation set. But no target label is available for training
in hyperspectral AD. As a result, we introduce a distance
measurement that implicitly reﬂects the detection performance
after each training epoch. Given a certain test HSI Yv2Y,
we send it to the image reconstruction module that undergoes
jtraining epochs:
~Yj
v=f(Yv;j); (6)
wheref()denotes the network structure, jis the updated
parameters after the j-th training epoch, and ~Yj
v2RB1is
the reconstructed HSI of Yv. For each spectrum ~yjon~Yj
v,
the global Mahalanobis distance is calculated according to:
M(~yj) = ( ~yj j)T 1
j(~yj j); (7)
wherej2RB1and 1
j2RBBare the mean vector
and the inverse covariance matrix of all the spectra on Yj
v,
respectively. The maximum of global Mahalanobis distances
Mcan reﬂect the difference between the most anomalous
spectrum and background on ~Yj
v. A largerMrepresents
a better domain transformation of our network. The model
corresponding to the peak value of Mis used for inference
on the test set.
F . Inference
The searched network by the transform domain search
module can be used for inference on the other test HSIs in
Y. For each test HSI Yi2Y, its reconstructed HSI Yrcan
be obtained by:
Yr=f
Yi;~
; (8)
where ~represents the optimized parameters of the image
reconstruction module. Then, GRX detector takes detection on
Yrto obtain the anomaly score map of Yi. As same as Eq. (7),
the anomaly score of each spectrum yonYris calculated
according to:
M(y) = (y )T 1(y ); (9)
where2RB1and 12RBBare the mean vector and
the inverse covariance matrix of all the spectra on Yr, respec-
tively. We can also replace GRX detector with other anomaly
detectors. Since GRX detector is efﬁcient and parameter-free,
we specify that Eq. (8) and Eq. (9) are standard procedures of
our method.
(a)
 (b)
 (c)
 (d)
 (e)
(f)
 (g)
 (h)
 (i)
 (j)
Fig. 5. Example scenes on the A VIRIS-NG training set.
(a)
 (b)
 (c)
 (d)
 (e)
(f)
 (g)
 (h)
 (i)
 (j)
Fig. 6. Example scenes on the A VIRIS-Classic training set.
IV. E XPERIMENTS
In this section, we ﬁrst introduce our developed hyper-
spectral AD dataset and two metrics used to evaluate methods
on our dataset. Then, we introduce the experimental settings
and analyze the experimental results in detail.
A. HAD100 Dataset
Although many novel AD methods have been proposed in
recent years, there is no large-scale dataset that can compre-
hensively evaluate the performance of these methods. Existing
methods generally report their best results after ﬁne-tuning on
different test scenes. However, ﬁne-tuned parameters cannot be
guaranteed to be applicable to other scenes. Some recent AD
methods claimed that they could perform inference on unseen
test scenes but were only evaluated on a limited number of
test scenes.
Consequently, building a uniﬁed and comprehensive evalu-
ation benchmark of hyperspectral AD is necessary. For this
purpose, we develop a large-scale Hyperspectral Anomaly
Detection dataset that contains 100 real remote sensing test
scenes, called HAD100 dataset. Our dataset has more test
scenes than the current popular ABU dataset [27], which
only contains 13 test scenes. All the test HSIs are down-
loaded from A VIRIS-NG (Airborne Visible InfraRed Imag-
ing Spectrometer-Next Generation) website1and uniformly
cropped to patches of size 6464. Fig. 7 shows the false color
maps and ground truth of example scenes in the test set. The
marked targets are mainly compact manufactured objects such
1https://avirisng.jpl.nasa.gov/8
(1)
 (2)
 (3)
 (4)
 (5)
 (6)
 (7)
 (8)
 (9)
 (10)
(11)
 (12)
 (13)
 (14)
 (15)
 (16)
 (17)
 (18)
 (19)
 (20)
(21)
 (22)
 (23)
 (24)
 (25)
 (26)
 (27)
 (28)
 (29)
 (30)
Fig. 7. False color maps and ground truth of example scenes (the test scenes 1-30) on the HAD100 dataset.
TABLE I
DATA CAPTURE INFORMATION ON THE AVIRIS-C LASSIC TRAINING SET
Flight Line ID DateSensorResolutionTraining
Device Images
F080709t01p00r15 2008/7/9 A VIRIS-Classic 17.0m 3
F080723t01p00r06 2008/7/23 A VIRIS-Classic 17.2m 1
F080723t01p00r07 2008/7/23 A VIRIS-Classic 17.0m 2
F090706t01p00r06 2009/7/6 A VIRIS-Classic 16.6m 12
F090710t01p00r10 2009/7/10 A VIRIS-Classic 16.8m 42
F090715t01p00r11 2009/7/15 A VIRIS-Classic 16.2m 13
F100825t01p00r05 2010/8/25 A VIRIS-Classic 16.0m 6
F131030t01p00r14 2013/10/30 A VIRIS-Classic 16.3m 18
F170119t01p00r09 2017/1/19 A VIRIS-Classic 13.4m 20
F170507t01p00r09 2017/5/7 A VIRIS-Classic 16.8m 26
F170507t01p00r10 2017/5/7 A VIRIS-Classic 16.9m 16
F180111t01p00r06 2018/1/11 A VIRIS-Classic 15.6m 12
F180118t01p00r19 2018/1/18 A VIRIS-Classic 17.0m 16
F180601t01p00r09 2018/6/1 A VIRIS-Classic 15.0m 75
Total - - - 262
as vehicles, boats, and buildings. The size of targets ranges
from 1 pixel to 69 pixels. Furthermore, our dataset contains
various backgrounds, including grassland, forest, farmland,
desert, lake, river, and coast. The HAD100 dataset can be
downloaded from our website2.
In order to inspire new ideas for hyperspectral AD, we
also provide two training sets which are captured by A VIRIS-
NG and A VIRIS-Classic3, respectively. As shown in Figs. 5
and 6, the HSIs in the training set are only cropped on the
background areas, with the space sizes ranging from 6666
to130130. As shown in Table II, the A VIRIS-NG training
2https://ZhaoxuLi123.github.io/HAD100
3https://aviris.jpl.nasa.gov/TABLE II
DATA CAPTURE INFORMATION ON THE AVIRIS-NG T RAINING AND TEST
SET
Flight Line ID DateSensorResolutionTraining Test
Device Images Images
Ang20170821t183707 2017/8/21 A VIRIS-NG 2.3m 90 38
Ang20170821t195229 2017/8/21 A VIRIS-NG 2.5m 20 8
Ang20170825t173426 2017/8/25 A VIRIS-NG 3.0m 13 5
Ang20170908t225309 2017/9/8 A VIRIS-NG 2.0m 30 13
Ang20171012t194435 2017/10/12 A VIRIS-NG 2.0m 22 12
Ang20180227t082814 2018/2/27 A VIRIS-NG 4.0m 14 6
Ang20191004t185054 2019/10/4 A VIRIS-NG 8.4m 21 6
Ang20191004t215336 2019/10/4 A VIRIS-NG 8.4m 10 3
Ang20191027t204454 2019/10/27 A VIRIS-NG 5.1m 9 1
Ang20210614t141018 2021/6/14 A VIRIS-NG 1.9m 31 8
Total - - - 260 100
TABLE III
DIFFERENCES BETWEEN AVIRIS-C LASSIC AND AVIRIS-NG
SensorSpectral RangeSpectral Original Reserved
Device Resolution Bands Bands
A VIRIS-Classic 366-2497nm 10nm 224 162
A VIRIS-NG 377-2500nm 5nm 425 276
set contains 260 normal HSIs, which are captured by the
same sensor device and capture ﬂight lines as the test set.
It means that most background spectra are common to both
the A VIRIS-NG training set and the test set. As shown in
Table I, the A VIRIS-Classic training set contains 262 normal
HSIs, which are captured by a different sensor device from the
test set. Within a similar data scale, the A VIRIS-NG training
set and the A VIRIS-Classic training set can be used for the
generalization evaluation of AD methods to different degrees.9
Table III shows the differences between A VIRIS-Classic and
A VIRIS-NG. A VIRIS-NG is the upgraded version of A VIRIS-
Classic. Under the condition that the spectral range is almost
unchanged, A VIRIS-NG has higher spectral resolution and
more bands than A VIRIS-Classic. We remove water absorption
bands and noise bands in the training and test sets. A VIRIS-
NG data retains 276 bands, of which ids are 16-109, 119-145,
159-187, 228-274, and 329-407. A VIRIS-Classic data retains
162 bands, of which ids are 8-57, 66-79, 86-104, 123-149, and
173-224.
B. Evaluation Metrics
In this paper, we use two evaluation metrics to compare
the detection performance of different AD methods on our
HAD100 dataset. The receiver operating characteristic (ROC)
area under the curve (AUC) is the most widely used eval-
uation tool in the hyperspectral AD task and can directly
give quantitative results. Besides, researchers usually use Box-
whisker Plot to compare the anomaly-background separability
of AD methods qualitatively. We replace Box-whisker Plot
with our improved version of signal-to-noise probability ratio
(SNPR) [71] to evaluate the anomaly-background separability
quantitatively.
1) AUC: ROC curve shows the corresponding relationship
between detection probability Pdand false alarm probability
Pf. Given a segmentation threshold and an anomaly score
mapM, we specify that pixels whose anomaly scores greater
than or equal to are classiﬁed as positive samples, and Pd
andPfcan be calculated according to:
Pd=TP
TP + FN; (10)
Pf=FP
TN + FP; (11)
where TP is the number of target pixels that are predicted as
positive samples, FN is the number of target pixels that are
predicted as negative samples, FP is the number of background
pixels that are predicted as positive samples, and TN is the
number of background pixels that are predicted as negative
samples. Each can produce a pair of PdandPfas a point
(Pd,Pf) on the ROC curve. We traverse every anomaly score
value onMasand generate a discrete ROC curve. Then,
the AUC value of Mis calculated by:
AUC =1
2p 1X
l=1
Pl+1
f Pl
f 
Pl+1
d+Pl
d
; (12)
where
Pl
f;Pl
d
denotes the l-th point on the discrete ROC
curve andpis the number of points. A larger AUC value means
a better detection performance. In particular, the AUC value is
speciﬁed as 0.5 when the anomaly score of each pixel is equal.
We use the mean AUC value (mAUC) on all the test HSIs in
HAD100 dataset to evaluate the AD methods throughout the
rest of this paper.TABLE IV
THEBESTDUAL WINDOW SIZES OF DIFFERENT AD M ETHODS
Parameter LRX KRX QLRX 2S-GLRT KSVDD CR KCSR
!in 13 3 11 3 3 5 3
!out 29 7 29 29 29 29 29
2) ASNPR: A recent study [71] deeply analyzed the evalu-
ation tools for hyperspectral target detection, and proposed
signal-to-noise probability ratio (SNPR) to evaluate back-
ground suppressibility. SNPR is calculated based on the 3D
ROC curve. In addition to PdandPf, the 3D ROC curve
introduces the segmentation threshold as the third variable,
which is sampled from the anomaly score map with Maxmin
Normalization. A 3D ROC curve can generate three 2D ROC
curves, which take (Pd;Pf),(Pd;), and (Pf;)as variables,
respectively. Similar to Eq. (12), the AUC value urder the 2D
ROC curve (Pd;)is calculated according to:
AUCd;=1
2p 1X
l=1 
l+1 l 
Pl+1
d+Pl
d
; (13)
and the AUC value urder the 2D ROC curve (Pf;)is
calculated according to:
AUCf;=1
2p 1X
l=1 
l+1 l
Pl+1
f+Pl
f
: (14)
Then, the SNPR value is calculated according to:
SNPR =AUCd;
AUCf;: (15)
A larger SNPR value means a better background suppressibil-
ity of AD methods.
In practice, we ﬁnd that the above 3D ROC-based evaluation
metrics are unsuitable for scenes with multiple types of targets.
When a certain target has an extremely high anomaly score,
other targets can have low values in the normalized anomaly
score map. Whether these targets are separated from back-
ground or not, the AUCf;value remains low. To solve this
problem, we develop adaptive versions of AUCd;,AUCf;,
andSNPR . Speciﬁcally, we set the median value of anomaly
scores on the real targets as the upper bound of the anomaly
score map. Calculated on the truncated anomaly score map,
AUCd;,AUCf;andSNPR are robust to the maximum
anomaly response value. The mean value of the adaptive
SNPR on all the test HSIs is calculated according to:
mASNPR =nX
k=110 log10AAUCk
d;
AAUCk
f;; (16)
wherenis the total number of the test HSIs, and AAUCk
d;
andAAUCk
f;are the adaptive AUCd;andAUCf;on the
k-th test HSI, respectively. Since the value range of ASNPR
is[0;+1), we introduce logarithmic operation to avoid over-
large ASNPR values of individual test HSIs. In particular, the
logarithmic value of ASNPR is speciﬁed as 0 dB when the
anomaly score of each pixel is equal.10
TABLE V
DETECTION PERFORMANCE AND INFERENCE TIME(INSECONDS )OFDIFFERENT AD M ETHODS ON THE FRIST 50, 100, AND 200 B ANDS OF HAD100
DATASET . THE BOLD BLACK ,RED,AND BLUE FONT IN EACH COLUMN INDICATE THE TOP THREE VALUES .
MethodThe First 50 Bands The First 100 Bands The First 200 Bands
mAUC mASNPR time mAUC mASNPR time mAUC mASNPR time
GRX 0.9799 7.93 0.022 0.9714 7.03 0.031 0.9649 6.59 0.058
LRX 0.9775 11.21 2.492 0.9692 10.92 6.272 0.9720 11.79 19.560
KRX 0.9616 10.59 1.918 0.9329 7.54 2.435 0.9282 7.30 2.506
QLRX 0.9801 7.85 1.091 0.9715 6.96 1.725 0.9682 6.86 2.947
2S-GLRT 0.9853 9.38 5.207 0.9683 8.39 11.765 0.9310 7.43 30.830
KSVDD 0.9845 6.47 43.563 0.9737 4.87 75.653 0.9706 4.42 140.750
CR 0.9929 11.24 27.175 0.9859 10.24 24.361 0.9849 10.12 25.882
KCSR 0.9939 10.48 6.006 0.9865 7.73 9.475 0.9860 7.13 17.286
KIFD 0.9829 8.14 35.725 0.9822 7.89 34.681 0.9774 7.61 34.267
PTA 0.9031 6.32 4.170 0.8976 4.72 14.201 0.8686 4.34 34.728
FrFE 0.9476 7.80 1.292 0.9371 7.05 3.091 0.9164 6.56 11.955
AED 0.9859 8.79 0.050 0.9645 8.47 0.054 0.9625 8.38 0.067
RGAE 0.9003 9.98 43.599 0.8876 7.37 47.748 0.8823 7.29 54.039
AutoAD 0.9688 10.17 6.305 0.9661 10.27 6.360 0.9642 10.86 6.304
LREN 0.9583 5.85 35.427 0.9286 4.24 35.703 0.9309 4.84 35.546
WeaklyADA 0.9862 11.33 1.887 0.9788 10.04 1.768 0.9773 9.39 1.782
WeaklyADB 0.9887 13.75 0.046 0.9838 12.76 0.085 0.9850 11.46 0.118
AETNet 0.9925 11.72 0.035 0.9875 9.96 0.043 0.9818 8.06 0.070
AETNet-KCSR 0.9948 13.15 6.815 0.9897 10.64 11.169 0.9869 10.63 20.348
False Color
 Ground Truth
 GRX
 LRX
 KRX
 QLRX
 2S-GLRT
KSVDD
 CR
 KCSR
 KIFD
 PTA
 FrFE
 AED
RGAE
 AutoAD
 LREN
 WeaklyADA
 WeaklyADB
 AETNet
 AETNet-KCSR
Fig. 8. Detection maps of different AD methods on the ﬁrst 50 bands of the test scene 83.
C. Experimental Settings
1) Implementation Details of AETNet: Since the training
input of AETNet needs to be consistent with the test HSIs in
the spatial size and band number, we generate 6464mask
maps in the mask generation module and set the parameter
Kto 8. The number range [Nmin;Nmax]of pseudo targets in
mask maps is set to [1;32], and the area range [Amin;Amax]
of pseudo targets is set to [3;20]. Regardless of the band
numberBof input data, the channel number Coutput by the
convolutional encoder is ﬁxed at 32. The number of attentionheads of ﬁve Swin Transformer blocks in AETNet is set to 2,
4, 8, 4, and 2, respectively. The number of non-overlapping
local windows in all Swin Transformer blocks is set to 88. In
the calculation of MSGMS loss, we set the constant cto 1 and
perform four pooling operations, i.e., the parameter S= 5.
In the training stage, the Adam optimizer [72] with a
learning rate of 1e 4and a weight decay of 5e 6is used
to optimize our AETNet. The batch size is set to 16. Training
terminates when the peak value of the measurement Min
the transform domain search module does not change for 30
epochs, or the epoch number reaches 200. Then, the trained11
TABLE VI
THEAUC PERFORMANCE OF DIFFERENT AD METHODS ACHIEVED ON THE FIRST 50BANDS OF THE HAD100 DATASET . THE BOLD BLACK ,RED,AND
BLUE FONT IN EACH ROW INDICATE THE TOP THREE VALUES . THE GRAY CELLS REPRESENT THAT THE AUC VALUES ARE LESS THAN 0.95.
Scene GRX LRX KRX QLRX2S-KSVDD CR KCSR KIFD PTA FrFE AED RGAE AutoAD LRENWeakly- Weakly-AETNetAETNet-
GLRT ADAADBKCSR
1 0.99962 0.99144 0.94310 0.99953 0.99897 0.98063 0.99531 0.99469 0.98213 0.96223 0.99902 0.90824 0.95829 0.96457 0.97865 0.99394 0.99804 0.99876 0.99828
2 0.99998 0.99867 0.99910 0.99997 0.99655 0.99500 0.99869 0.99988 0.99728 0.99995 0.99996 0.99995 0.99965 0.99377 0.99998 0.99975 0.99864 0.99866 0.99916
3 0.98819 0.96344 0.97025 0.99026 0.97194 0.99046 0.98986 0.99348 0.98666 0.93605 0.98756 0.96613 0.94636 0.88084 0.94661 0.97081 0.98281 0.98082 0.98651
4 0.99978 0.99724 0.98952 0.99978 0.99615 0.98778 0.99835 0.99731 0.99786 0.98932 0.99826 0.99326 0.98553 0.98976 0.99875 0.99784 0.99902 0.99895 0.99878
5 0.97158 0.96190 0.99865 0.97484 0.99273 0.98637 0.98629 0.99371 0.98044 0.98283 0.97653 0.99495 0.96140 0.97070 0.87241 0.96894 0.95974 0.99601 0.99692
6 0.98724 0.97592 0.98630 0.98177 0.99273 0.97856 0.99875 0.99831 0.98991 0.97179 0.98197 0.99387 0.95500 0.97218 0.98563 0.99486 0.99559 0.99581 0.99719
7 0.99595 0.99195 0.95488 0.99639 0.99949 0.98995 0.99722 0.99603 0.99564 0.86162 0.99650 0.99897 0.72763 0.95963 0.94748 0.98644 0.98914 0.99912 0.99895
8 0.99994 0.99991 0.99737 0.99994 0.99994 0.99761 0.99988 0.99982 0.99957 0.99875 1.00000 0.99957 0.98667 0.99991 0.98489 0.99985 0.99994 0.99994 0.99979
9 0.99975 0.99988 0.99878 0.99977 0.99949 0.98921 0.99882 0.99902 0.99932 0.99762 0.99803 0.99732 0.99978 0.95918 0.99825 0.99901 0.99716 0.99981 0.99898
10 0.98105 0.99640 0.99228 0.97837 0.99673 0.99454 0.99687 0.99886 0.99776 0.99741 0.98115 0.99808 0.98854 0.97806 0.99312 0.99857 0.99792 0.99787 0.99850
11 0.98941 0.99733 0.98850 0.99825 0.99881 0.99643 0.99891 0.99905 0.99844 0.94335 0.99265 0.99968 0.89225 0.99493 0.99815 0.99677 0.99819 0.99919 0.99913
12 0.99664 0.99903 0.99208 0.99541 0.99894 0.99434 0.99869 0.99740 0.96041 0.96143 0.99769 0.99897 0.92498 0.99268 0.99853 0.99837 0.99697 0.99907 0.99846
13 0.97049 0.94425 0.93150 0.97065 0.95857 0.96385 0.99263 0.99351 0.98869 0.85529 0.97340 0.97768 0.86210 0.91637 0.91588 0.96675 0.98574 0.97647 0.99012
14 0.99988 0.99998 0.99444 0.99958 0.99963 0.99567 1.00000 0.99995 0.92974 0.99344 1.00000 0.99998 0.98458 0.99941 0.99868 0.99993 1.00000 0.99985 1.00000
15 0.99540 0.99896 0.99813 0.99297 0.97528 0.99037 0.99942 0.99891 0.99885 0.99322 0.99791 0.99957 0.95164 0.98704 0.98636 0.99889 0.99936 0.99923 0.99902
16 0.99980 0.97738 0.99752 0.99729 0.98697 0.99160 0.99956 0.99939 0.99638 0.50000 0.99988 0.99436 0.98428 0.99468 0.99974 0.99914 0.99918 0.99907 0.99919
17 0.99980 0.99845 0.96054 0.99969 0.99955 0.99586 0.99941 0.99804 0.99955 0.92207 0.99855 0.99861 0.92163 0.99925 1.00000 0.99892 0.99888 0.99984 0.99869
18 0.98536 0.98373 0.91600 0.97662 0.96292 0.94792 0.99218 0.98846 0.99230 0.50000 0.98712 0.99708 0.79820 0.99769 0.99907 0.98800 0.97605 0.99294 0.97998
19 0.99993 0.99510 0.99994 0.99987 0.99912 0.99666 0.99957 0.99969 0.99823 0.99789 0.99985 0.99888 0.99839 0.97921 0.99693 0.99975 0.99901 0.99799 0.99935
20 0.94822 0.99907 0.90231 0.94849 0.99521 0.99445 0.99854 0.99734 0.99048 0.89033 0.95114 0.99889 0.87891 0.98504 0.99391 0.97613 0.99287 0.99568 0.99756
21 1.00000 1.00000 0.99443 1.00000 0.99943 0.99947 0.99978 0.99973 0.99700 0.99082 0.99998 0.99680 0.98925 0.99029 1.00000 0.99976 0.99986 0.99908 0.99971
22 0.99885 0.99668 0.97693 0.99920 0.99350 0.99125 0.99848 0.99685 0.99882 0.97656 0.99791 0.99923 0.96742 0.99351 0.99794 0.99539 0.99672 0.99815 0.99881
23 0.99772 0.99365 0.96288 0.99710 0.99349 0.99216 0.99751 0.99764 0.99863 0.97279 0.99806 0.99715 0.93201 0.98372 0.98918 0.99221 0.98934 0.99828 0.99822
24 0.99683 0.91625 0.99427 0.99596 0.99641 0.98810 0.99427 0.99646 0.99821 0.99357 0.99549 0.99195 0.99395 0.94782 0.99889 0.99756 0.99844 0.99967 0.99952
25 0.99977 0.99972 0.99886 0.99986 0.99664 0.99998 0.99999 0.99996 0.99839 0.98939 0.99986 0.99680 0.99298 0.95643 0.99653 0.99990 0.99995 0.99748 0.99923
26 0.98420 0.99946 0.97166 0.99292 0.99880 0.97699 0.99685 0.99514 0.98316 0.95011 0.40540 0.99954 0.80332 0.98965 0.99960 0.99921 0.99702 0.99962 0.99891
27 0.99889 1.00000 0.99611 0.99922 0.99673 0.99982 0.99995 0.99997 0.93484 0.99683 0.99886 0.99982 0.99606 0.98412 0.99234 0.99938 0.99944 0.99874 0.99948
28 0.96821 0.99531 0.98412 0.98528 0.98033 0.98765 0.99773 0.99706 0.99628 0.95083 0.96640 0.99799 0.95107 0.97956 0.99766 0.98691 0.98625 0.99418 0.99716
29 0.98342 0.99553 0.93009 0.98861 0.99833 0.99901 0.99637 0.99481 0.99187 0.84638 0.38223 0.99739 0.74893 0.99461 0.95807 0.96628 0.97618 0.99755 0.99763
30 0.99909 0.98587 0.99416 0.99835 0.99503 0.98703 0.99923 0.99967 0.99896 0.99060 0.99851 0.98595 0.99278 0.99095 0.99998 0.99916 0.99884 0.99765 0.99854
31 0.97919 0.99170 0.98455 0.98719 0.98818 0.98679 0.99327 0.99555 0.98562 0.91163 0.97443 0.99288 0.83842 0.96620 0.94122 0.99331 0.98942 0.99658 0.99581
32 0.99993 0.99985 0.99900 0.99989 0.99654 0.99729 0.99065 0.99862 0.96146 0.99974 0.99989 0.97384 0.99977 0.98622 0.99978 0.99907 0.99723 0.99682 0.99668
33 0.97990 0.94724 0.95419 0.97960 0.98056 0.99597 0.99467 0.99740 0.98383 0.88990 0.98260 0.99170 0.91677 0.96149 0.95070 0.97607 0.97899 0.98949 0.99612
34 0.99897 0.96678 0.99798 0.99858 0.98209 0.97668 0.97969 0.99647 0.99126 0.99206 0.99821 0.99389 0.96830 0.96299 0.98821 0.99706 0.99218 0.99481 0.99419
35 0.96907 0.95702 0.99157 0.97493 0.93485 0.97933 0.97358 0.98091 0.94390 0.96827 0.96657 0.94784 0.94997 0.91764 0.80658 0.95588 0.92418 0.97264 0.98684
36 0.99949 0.99469 0.99792 0.99939 0.99959 0.99063 0.99892 0.99888 0.99976 0.99584 0.99925 0.98749 0.99482 0.98755 0.98545 0.99820 0.99902 0.99918 0.99929
37 0.99992 0.99996 0.96736 0.99992 0.99963 0.99055 0.99984 0.99967 0.99756 0.84943 0.99976 0.99698 0.73362 0.99919 1.00000 0.99723 0.99756 0.99980 0.99971
38 0.99538 0.97900 0.99148 0.99167 0.98413 0.99846 0.99630 0.99930 0.99906 0.96334 0.99737 0.99855 0.98074 0.98428 0.99971 0.99793 0.99953 0.99590 0.99767
39 0.95263 0.91497 0.90757 0.95166 0.95629 0.92645 0.97465 0.97695 0.96248 0.87893 0.95023 0.98202 0.86175 0.89516 0.93031 0.90263 0.95383 0.96726 0.97376
40 0.99304 0.97355 0.98878 0.99484 0.99366 0.99207 0.98971 0.99695 0.99176 0.95623 0.99588 0.99675 0.92802 0.96890 0.99229 0.99649 0.99173 0.99565 0.99505
41 0.98623 0.98887 0.90939 0.98158 0.99928 0.99175 0.99862 0.99719 0.99769 0.82164 0.98738 0.99856 0.64898 0.99730 0.96261 0.98767 0.99725 0.99952 0.99870
42 0.86529 0.95220 0.92777 0.87971 0.98779 0.96413 0.97880 0.97480 0.93840 0.94111 0.90340 0.96942 0.86217 0.93191 0.90532 0.93758 0.93248 0.98751 0.98689
43 0.99995 0.99840 0.97129 0.99995 0.99886 0.99111 0.99870 0.99769 0.98565 0.97398 0.99935 0.99834 0.96648 0.99902 0.98407 0.99649 0.99456 0.99935 0.99842
44 0.94703 0.93335 0.92137 0.95961 0.95522 0.96731 0.97171 0.98254 0.86910 0.74919 0.91652 0.97214 0.48556 0.94864 0.80487 0.95306 0.93383 0.99018 0.99123
45 0.98849 0.98272 0.95195 0.98973 0.98802 0.99361 0.99710 0.99481 0.99668 0.50000 0.32088 0.98789 0.81380 0.97481 0.95597 0.98275 0.97223 0.99686 0.99752
46 0.99970 0.99989 0.98660 0.99974 0.99725 0.98568 0.99991 0.99989 0.99770 0.97249 0.99994 0.99778 0.96667 0.99000 0.95835 0.99949 0.99977 0.99889 0.99968
47 0.97441 0.91580 0.97358 0.97179 0.89034 0.94786 0.95673 0.96727 0.95289 0.98311 0.94116 0.65446 0.72823 0.85456 0.51047 0.94881 0.91471 0.97235 0.97923
48 0.96767 0.97082 0.95510 0.96501 0.98879 0.96952 0.97796 0.98548 0.99341 0.95827 0.97505 0.99429 0.89580 0.98710 0.99814 0.99059 0.99121 0.99859 0.99511
49 0.99864 0.94034 0.98880 0.99849 0.98880 0.98401 0.99428 0.99834 0.99830 0.99228 0.99778 0.99400 0.99388 0.99209 0.99973 0.99933 0.99689 0.99773 0.99748
50 0.93762 0.97116 0.65081 0.93443 0.93779 0.97269 0.98531 0.98422 0.96274 0.76715 0.93198 0.95106 0.80734 0.95122 0.98827 0.95800 0.95959 0.96719 0.97868
51 0.99301 0.97387 0.98953 0.99407 0.99203 0.99741 0.99377 0.99827 0.99645 0.98845 0.99268 0.99754 0.98377 0.96006 0.99870 0.99387 0.99802 0.99805 0.99750
52 0.99984 0.99857 0.98462 0.99930 0.99685 0.99799 0.99998 1.00000 0.99405 0.50000 0.99998 0.99998 0.92306 0.99997 0.99993 0.99969 0.99991 1.00000 1.00000
53 0.99743 0.99479 0.99522 0.99566 0.96779 0.99779 0.99793 0.99946 0.99862 0.99938 0.99727 0.98837 0.99888 0.99017 0.99894 0.99889 0.99828 0.98684 0.99265
54 0.99646 0.99754 0.99442 0.99493 0.97512 0.99686 0.99731 0.99835 0.99688 0.99877 0.99624 0.99088 0.99794 0.99641 0.99927 0.99750 0.99683 0.98967 0.99460
55 0.99886 0.99290 0.94212 0.99767 0.98970 0.96906 0.99856 0.99869 0.97926 0.97445 0.99882 0.98299 0.97298 0.99330 0.99185 0.99762 0.99817 0.99508 0.99731
56 0.99825 0.99017 0.98916 0.96390 0.98674 0.99293 0.99763 0.99743 0.99494 0.98618 0.99825 0.99417 0.97826 0.97891 0.98585 0.98828 0.99555 0.99766 0.99769
57 0.98011 0.97916 0.90563 0.95117 0.97372 0.97605 0.99771 0.99401 0.96457 0.92044 0.97899 0.99847 0.77419 0.98671 0.88986 0.98373 0.97969 0.98253 0.98993
58 0.99524 0.96993 0.98088 0.99484 0.98552 0.96139 0.99376 0.99497 0.93202 0.98186 0.99495 0.98500 0.98158 0.97451 0.97388 0.99221 0.98363 0.98959 0.98947
59 0.99264 0.99318 0.97185 0.99177 0.97286 0.97574 0.99872 0.99828 0.96182 0.96869 0.99218 0.99726 0.96694 0.98186 0.99489 0.99195 0.99425 0.99333 0.99576
60 0.99188 0.98165 0.97821 0.99250 0.97897 0.97146 0.99716 0.99553 0.99573 0.97861 0.99182 0.98433 0.97898 0.97978 0.97552 0.99234 0.99220 0.98874 0.99143
61 0.98268 0.98734 0.96314 0.97967 0.98870 0.96310 0.99527 0.99234 0.96395 0.96727 0.98264 0.99102 0.96615 0.97864 0.95580 0.99104 0.99748 0.98889 0.99420
62 1.00000 0.99982 0.97600 1.00000 0.99951 0.99427 0.99996 0.99994 0.99973 0.95446 1.00000 0.99971 0.96731 0.99629 0.97286 1.00000 0.99994 0.99969 0.99994
63 0.94162 0.91371 0.96591 0.93317 0.99021 0.92673 0.98532 0.97570 0.96547 0.88716 0.94223 0.98111 0.88190 0.91472 0.83796 0.96189 0.97825 0.98611 0.98740
64 0.98897 0.98550 0.98863 0.94305 0.88882 0.98765 0.99567 0.99583 0.99478 0.99131 0.98938 0.99697 0.98568 0.88503 0.94506 0.99681 0.99794 0.99557 0.99554
65 0.96486 0.93660 0.91710 0.95979 0.96258 0.95879 0.98204 0.98509 0.97553 0.73960 0.96832 0.94928 0.73011 0.95428 0.92115 0.93771 0.97769 0.97669 0.98419
66 0.96811 0.99991 0.93426 0.98204 0.99749 0.99973 0.99826 0.99895 0.99909 0.82755 0.98400 0.99958 0.74505 0.99562 0.75255 0.97772 0.99840 0.99913 0.99951
67 0.99982 0.99974 0.98686 0.99965 0.99639 0.99305 0.99948 0.99886 0.98992 0.93295 0.99931 0.99902 0.82781 0.98950 0.99983 0.99625 0.99542 0.99889 0.99798
68 1.00000 0.99993 0.98252 1.00000 0.99967 0.99794 0.99949 0.99876 0.99913 0.86125 0.49701 0.99832 0.78582 0.99634 0.99993 0.99771 0.99918 0.99986 0.99969
69 0.99860 0.98509 0.94514 0.99869 0.99691 0.99717 0.99953 0.99914 0.99897 0.77945 0.92934 0.97941 0.69234 0.98703 0.99911 0.99226 0.99595 0.99875 0.99860
70 0.97177 0.99714 0.93108 0.97968 0.99616 0.98700 0.99744 0.99603 0.99854 0.76559 0.99882 0.99792 0.70946 0.96650 0.97091 0.99176 0.99885 0.99529 0.99719
71 0.88996 0.94212 0.92381 0.89023 0.99170 0.97839 0.98953 0.98197 0.96186 0.84525 0.50734 0.99187 0.81777 0.96189 0.99097 0.95845 0.98835 0.99689 0.99695
72 0.92788 0.95995 0.86920 0.93852 0.97297 0.98538 0.99526 0.99282 0.98815 0.76534 0.98047 0.99119 0.79534 0.94838 0.99914 0.98384 0.98910 0.99891 0.99770
73 0.88235 0.93601 0.90893 0.86449 0.99403 0.97101 0.96647 0.96856 0.96363 0.83900 0.96233 0.92578 0.85134 0.97070 0.99572 0.98772 0.98507 0.99757 0.99682
74 0.97037 0.98035 0.90796 0.97134 0.98873 0.99512 0.99439 0.99690 0.99423 0.50000 0.99223 0.99696 0.80992 0.92194 0.99412 0.99561 0.99689 0.99769 0.99866
75 0.92498 0.89053 0.94546 0.91954 0.97106 0.98050 0.99014 0.99258 0.95321 0.79161 0.21224 0.98910 0.74136 0.96868 0.90336 0.96421 0.99171 0.99773 0.99775
76 0.95124 0.99800 0.75151 0.97025 0.97250 0.99701 0.99849 0.99811 0.96835 0.68676 0.97485 0.98480 0.59581 0.98464 0.93003 0.97200 0.99687 0.97495 0.99014
77 0.99983 0.99954 0.99931 0.99983 0.99810 0.99915 0.99968 0.99991 0.99559 0.99999 0.99983 0.99873 0.99997 0.99808 0.99975 0.99994 0.99952 0.99815 0.99997
78 0.99997 0.99998 1.00000 0.99997 0.99962 0.99902 0.99997 0.99989 0.81375 1.00000 0.99997 1.00000 1.00000 0.99884 0.99989 0.99971 0.99977 0.99997 1.00000
79 0.99893 0.99862 0.99985 0.99954 0.99972 0.99761 0.99979 0.99991 0.99966 0.97208 0.99893 0.99905 0.96129 0.99991 0.96710 0.95536 0.99826 0.99985 0.99991
80 1.00000 0.99972 0.99988 1.00000 0.99939 0.99982 0.99972 0.99985 0.99942 0.99994 1.00000 0.99141 0.99985 0.99985 0.92557 0.99648 0.99991 0.99988 0.99982
81 1.00000 1.00000 1.00000 1.00000 0.99817 0.99991 1.00000 0.99991 0.99896 0.99979 1.00000 0.98939 0.99982 1.00000 0.98098 0.99988 1.00000 1.00000 1.00000
82 0.95723 0.99053 0.99778 0.96609 0.99918 0.98337 0.97994 0.99957 0.99547 0.99125 0.96292 0.99988 0.98398 0.99206 0.92516 0.99641 0.99339 0.99676 0.99965
83 0.84093 0.89191 0.87625 0.84095 0.91313 0.96640 0.92502 0.94422 0.96415 0.76452 0.88209 0.96409 0.46200 0.94748 0.99235 0.95967 0.97450 0.91208 0.94677
84 0.99274 0.90761 0.98272 0.98264 0.98719 0.98370 0.99019 0.99704 0.99818 0.99084 0.99314 0.95448 0.98963 0.92667 0.99318 0.99290 0.99365 0.99614 0.99698
85 0.99726 0.97773 0.98828 0.99321 0.99646 0.99035 0.99411 0.99698 0.99409 0.99274 0.99703 0.98765 0.99161 0.96819 0.96453 0.99811 0.99799 0.99857 0.99856
86 0.99447 0.99225 0.98129 0.99602 0.98868 0.99382 0.99918 0.99768 0.99191 0.94169 0.99595 0.99424 0.92858 0.96300 0.92408 0.98654 0.96886 0.99612 0.99768
87 0.88410 0.99614 0.81167 0.99065 0.99400 0.95207 0.96672 0.97376 0.96918 0.56200 0.92318 0.96191 0.60734 0.91820 0.47409 0.95891 0.94874 0.87797 0.93903
88 0.98583 0.98510 0.98177 0.99212 0.96837 0.97750 0.97618 0.98710 0.98863 0.99141 0.98837 0.98684 0.99035 0.79181 0.98075 0.99082 0.99028 0.99025 0.99034
89 0.99944 0.99963 0.99850 0.99958 0.99504 0.99871 0.99943 0.99970 0.99757 0.99588 0.99948 0.99842 0.99117 0.98001 0.97916 0.99924 0.99898 0.99823 0.99871
90 0.83511 0.82214 0.84415 0.79736 0.93217 0.88720 0.96791 0.93930 0.96588 0.86656 0.84141 0.97382 0.71027 0.87168 0.82263 0.88812 0.96388 0.97861 0.98151
91 0.99436 0.98946 0.99344 0.99557 0.99251 0.98360 0.98901 0.99756 0.99560 0.99147 0.99334 0.99139 0.99139 0.97607 0.98290 0.99696 0.99292 0.99831 0.99907
92 0.99985 0.99984 0.99990 0.99985 0.99839 0.99962 0.99914 0.99974 0.95788 0.50000 0.99985 0.99975 0.99990 0.99485 0.99994 0.99958 0.99869 0.99917 0.99968
93 0.96483 0.94694 0.98138 0.96672 0.98350 0.98659 0.99053 0.99084 0.97518 0.80236 0.96430 0.99059 0.95937 0.87783 0.96516 0.96703 0.95910 0.98862 0.99264
94 0.99988 0.99266 0.99958 0.99984 0.99794 0.99654 0.99933 0.99947 0.99042 0.99999 0.99988 0.99758 1.00000 0.99511 1.00000 0.99975 0.99910 0.99970 0.99973
95 1.00000 0.99883 0.99453 0.99997 0.99665 0.99606 0.99977 0.99993 0.99948 0.99843 0.99994 0.99984 0.99840 0.99438 1.00000 0.99981 0.99869 0.99742 0.99888
96 0.99353 0.96564 0.97143 0.99449 0.98413 0.99198 0.99493 0.99388 0.99734 0.98226 0.99204 0.99272 0.98393 0.93232 0.96657 0.98298 0.97014 0.98889 0.99284
97 0.99293 0.90117 0.97337 0.99056 0.98289 0.97327 0.99406 0.99153 0.99595 0.95605 0.99190 0.98374 0.97354 0.96027 0.96665 0.98781 0.99374 0.99445 0.99472
98 1.00000 0.99991 0.98462 1.00000 0.99856 0.99972 0.99988 0.99997 0.99936 0.99645 1.00000 1.00000 0.99398 0.99113 0.98719 0.99951 0.99804 0.99804 0.99994
99 1.00000 0.99250 0.97899 1.00000 0.99314 0.98020 0.99825 0.99678 0.99991 0.98220 1.00000 0.99925 0.98628 0.99893 0.93975 0.99970 0.99657 0.99702 0.99802
100 0.98000 0.98740 0.85947 0.98138 0.99003 0.98584 0.99719 0.99575 0.99070 0.62044 0.98083 0.99055 0.65218 0.96071 0.93395 0.99012 0.98517 0.99917 0.99777
mean 0.97986 0.97750 0.96159 0.98006 0.98530 0.98452 0.99289 0.99390 0.98288 0.90307 0.94764 0.98593 0.90029 0.96881 0.95834 0.98622 0.98866 0.99249 0.9948112
False Color
 Ground Truth
 GRX
 LRX
 KRX
 QLRX
 2S-GLRT
KSVDD
 CR
 KCSR
 KIFD
 PTA
 FrFE
 AED
RGAE
 AutoAD
 LREN
 WeaklyADA
 WeaklyADB
 AETNet
 AETNet-KCSR
Fig. 9. Detection maps of different AD methods on the ﬁrst 50 bands of the test scene 87.
False Color
 Ground Truth
 GRX
 LRX
 KRX
 QLRX
 2S-GLRT
KSVDD
 CR
 KCSR
 KIFD
 PTA
 FrFE
 AED
RGAE
 AutoAD
 LREN
 WeaklyADA
 WeaklyADB
 AETNet
 AETNet-KCSR
Fig. 10. Detection maps of different AD methods on the ﬁrst 50 bands of the test scene 90.
model corresponding to the peak value of Mis used for
inference. Before being sent to AETNet, all the input data are
processed with linear normalization and scaled to [ 0:1;0:1].
We select the test scene 1 in HAD100 dataset as the validation
HSI in the transform domain search module. If there is no
speciﬁc statement, we use the A VIRIS-NG training set and
CutOut by default.
In addition, we use three simple data augmentation ap-
proaches to expand the training set of AETNet:
Image Crop: We crop a single training HSI into four 6464
HSIs. The A VIRIS-NG training set is expanded to 1048 HSIs,
and the A VIRIS-Classic training set is expanded to 1040 HSIs.
Image Rotation: At the beginning of each training epoch,
every input HSI is randomly rotated by 0,90,180, or270.
Image Flip: After random rotation, each input HSI israndomly ﬂipped along the horizontal or vertical direction.
The probabilities of horizontal ﬂip and vertical ﬂip are both
50%.
2) Comparative Methods: Sixteen classic or SOTA hyper-
spectral AD methods are selected to be evaluated on our
HAD100 dataset, including GRX [7], LRX [8], KRX [12],
QLRX [9], 2S-GLRT [15], KSVDD [14], CR [16], KCSR
[17], KIFD [31], PTA [34], FrFE [26], AED [27], RGAE
[49], AutoAD [51], LREN [45], and WeaklyAD [52]. GRX
is the most classic AD method and is widely used as the
AD baseline. LRX, KRX, QLRX, 2S-GLRT, CR, and KCSR
are dual-window-based AD methods. CR and KCSR are
sparse-representation-based AD methods. PTA is one of the
most advanced tensor-approximation-based AD methods, and
AET introduces morphological image processing to the hyper-13
spectral AD ﬁeld. KRX, KSVDD, KCSR, and KIFD all use
the kernel trick to process spectral samples. RGAE, LREN,
WeaklyAD, and AutoAD are deep-learning-based methods, of
which the ﬁrst three are spectral-level self-supervised methods.
In particular, WeaklyAD can perform inference on unseen test
scenes.
The codes of 2S-GLRT, KCSR, KIFD, PTA, FrFE, RGAE,
AutoAD, and LREN are all ofﬁcial versions, and the codes of
GRX and AED are widely used unofﬁcial versions. Besides,
we reproduce the codes of LRX, KRX, QLRX, KSVDD, CR,
and WeaklyAD, whose detection performances are close to
those in the literature.
Most of the above methods adopt Maxmin Normalization
for data preprocessing and linearly scale each test HSI to [0;1].
But some methods adopt distinctive normalization. 2S-GLRT
and PTA perform Maxmin Normalization on each band of
HSI data. AET uses principal component analysis (PCA) to
reduce the dimension of HSI data and then linearly scale drop-
dimensional HSI data to [0;1]. FrFE divides all values on the
test HSI by the maximum value.
3) Settings of Comparative Experiments: To explore the
inﬂuence of channel number and spectral range on the general-
ization ability of AD methods, we conducte three comparative
experiments on the ﬁrst 50 bands, the ﬁrst 100 bands, and
the ﬁrst 200 bands of HAD100 dataset, respectively. For
WeaklyAD, we simultaneously compare its retraining ver-
sion and non-retraining version, denoted as WeaklyADAand
WeaklyADB, respectively. To achieve a fair comparison with
our AETNet, WeaklyADBextracts coarse anomaly samples on
tthe test sence 1 and adopts the whole A VIRIS-NG training
set as the background sample set. On the ﬁrst 50 bands of
the test set, the critical parameters of all comparative methods
are ﬁne-tuned to achieve the best mAUC values. Then, these
carefully regulated methods are directly evaluated on the ﬁrst
100 bands and ﬁrst 200 bands without parameter readjustment.
Furthermore, the dual window sizes (!in;!out)of all the dual-
window-based methods traverse from 3 to 29, and Table IV
shows the best dual window sizes of different methods. The
upper bound of the outer window size !outis set to 29 to
avoid huge calculation costs and ensure the local property
of the dual-window-based methods. Interestingly, the best
outer window sizes !outof all the dual-window-based methods
except KRX reach the upper bound of 29. In addition to the
standard version of AETNet, we also provide an advanced
version that replaces Eq. (9) with KCSR, named AETNet-
KCSR. AETNet-KCSR adopts the same parameter settings as
AETNet and KCSR.
In addition to mAUC and mASNPR, we also report the
average inference time of all the comparative methods on a
single test HSI. All the experiments are implemented on a PC
with an Intel Core i9-9980XE CPU and two GeForce RTX
2080 Ti GPUs.
D. Analysis of Detection Performance
1) Analysis of Experiments on the First 50 Bands: The
gray region (the 2nd to 5th columns) in Table V shows the
detection performance and inference time of difference ADmethods on the ﬁrst 50 bands of HAD100 dataset. In terms
of mAUC, the top three methods, AETNet-KCSR, KCSR,
and CR, are all based on sparse representation, with values
of 0.9948, 0.9939, and 0.9929, respectively. The standard
AETNet ranks fourth with an mAUC value of 0.9925. The
two worst-performing methods are RGAE and PTA, of which
mAUC values are 0.9003 and 0.9031, respectively. The mAUC
value of GRX reaches 0.9799, and the local improved versions
of GRX (LRX, KRX, and QLRX) do not show advantages.
Table VI shows the AUC values of different AD methods on
all the test scenes. The colored table intuitively gives a similar
conclusion as the mAUC performance in Table V. CR, KCSR,
AETNet, and AETNet-KCSR have the fewest gray cells, while
PTA and RGAE have the most gray cells. The standard and
advanced versions of AETNet have obvious advantages on the
test scenes 42, 44, and 90. Visual results of different methods
on the test scene 90 are shown in Fig. 10. Compared to other
methods, our AETNet has higher response values to compact
targets and lower response values to borders (the false alarms
on the detection maps of most AD methods). However, both
AETNet and AETNet-KCSR performs less competitively on
the test scenes 83 and 87. As shown in Fig. 8, our AETNet
can effectively suppress most borders on the test scene 83 but
generates a false target on a dark border. The rest region of the
dark border is mixed with the bright borders, causing AETNet
not to treat this border as a whole. As shown in Fig. 9, there
is a large cloud region the test scene 87, where many methods
have high anomaly response values. Our AETNet also has high
anomaly response values on a group of mixed pixels of cloud
and land, which has spatial characteristics similar to those of
anomaly targets. The performance on the test scenes 83 and
87 indicates that the AETNet’s ability to perceive mixed pixels
needs further improvement.
In terms of mASNPR, the top three methods, WeaklyADB,
AETNet-KCSR and AETNet, reach 13.75 dB, 13.15 dB, and
11.72 dB, respectively. These three methods are all deep-
learning-based methods without retraining. In terms of the
inference time, the standard AETNet with 0.035s ranks second
after GRX with 0.022s. Except for AED and WeaklyADB, the
remaining methods have high calculation costs. Compared to
GRX, the standard AETNet improves the AUC performance
from 0.9799 to 0.9925 with little additional time, while main-
taining the parameter-free characteristic of GRX. As shown in
Fig. 1, the standard AETNet achieves the best balance between
detection accuracy and efﬁciency with mAUC of 0.9925 and
inference speed of 28.6 FPS.
2) Analysis of Experiments on the First 100 Bands: The
blue region (the 6th to 9th columns) in Table V shows the
detection performance and inference time of difference AD
methods on the ﬁrst 100 bands. All the detection accuracy
evaluations except the mASNPR value of AutoAD are lower
than those on the ﬁrst 50 bands. GRX is a parameter-free AD
method, and AET uses PAC to reduce the spectral dimension
to a ﬁxed value. The performance degradation of these two
methods indicates that the added spectral range on the ﬁrst 100
bands contains distractive spectral information and increases
the difﬁculty of AD.
Our AETNet still achieves higher accuracy and efﬁciency14
TABLE VII
THE M AUC P ERFORMANCE ACHIEVED BY DIFFERENT SETTINGS OF
RANDOM MASK STRATEGY ON THE AVIRIS-NG T RAINING SET.
Random Mask 50 Bands 100 Bands 200 Bands
CutOut 0.9925 0.9875 0.9818
CutMix 0.9883 0.9826 0.9770
w/o 0.9905 0.9714 0.9793
TABLE VIII
THE M AUC P ERFORMANCE ACHIEVED BY DIFFERENT SETTINGS OF
RANDOM MASK STRATEGY ON THE AVIRIS-C LASSIC TRAINING SET.
Random Mask 50 Bands 100 Bands
CutOut 0.9904 0.9730
CutMix 0.9864 0.9837
w/o 0.9883 0.9715
than the SOTA methods. In terms of mAUC, AETNet-KCSR
and AETNet win the top two places with 0.9897 and 0.9875,
respectively. The detection time of most methods increases
dramatically, while that of the standard AETNet remains at a
low level of 0.043s.
3) Analysis of Experiments on the First 200 Bands: The
blue region (the 10th to 13th columns) in Table V shows
the detection performance and inference time of difference
AD methods on the ﬁrst 200 bands. Compared to the ﬁrst
100 bands data, the ﬁrst 200 bands data has more distractive
information and is more challenging for AD methods. Except
for LRX, LREN, and WeaklyADB, the remaining methods
have lower mAUC values than those on the ﬁrst 50 and 100
bands. AETNet-KCSR is still the ﬁrst place in terms of mAUC,
and the standard AETNet ranks ﬁfth with a value of 0.9818.
In terms of inference efﬁciency, GRX is still the fastest with
inference time of 0.058s, followed by AED and AETNet with
inference time of 0.067s and 0.07s, respectively.
E. Inﬂuence of Random Mask Strategy on Generalization
Capability
Both our standard AETNet and WeaklyADBare trained
on the A VIRIS-NG training set and use GRX as the detec-
tor. As shown in Table V, AETNet has a higher gain to
GRX than WeaklyADBon the unseen test scenes. Compared
with WeaklyADB, AETNet achieves an image-level training
paradigm and adopts the Random Mask strategy to handle
the generalization problem. In the following, we analyze the
generalization ability under different settings of the Random
Mask strategy.
1) Analysis of Cross-Scene Generalization Capability: The
A VIRIS-NG training set is collected from the same device
sensor as the test set and does not include anomaly scenes. As
shown in Table VII, we compare the mAUC values of AETNet
with CutOut, AETNet with CutMix, and AETNet without
Random Mask. All the mAUC values in three comparative
experiments with different band numbers are higher than those
of GRX except AETNet without Random Mask. Among them,
AETNet with CutOut achieves the best mAUC values on all
three comparative experiments, which are 1.53%, 1.88%, andTABLE IX
DETECTION ACCURACY AND TRAINING EPOCH SEARCHED BY THE
TRANSFORM DOMAIN SEARCH MODULE UNDER THE AVIRIS-NG
TRAINING DATA OF DIFFERENT SCALES ON THE FIRST 50 B ANDS
Training Images Data Ratio mAUC Training Epoch
52 5% 0.9916 70
104 10% 0.9904 45
156 15% 0.9922 93
208 20% 0.9925 39
312 30% 0.9925 32
416 40% 0.9926 18
520 50% 0.9924 17
624 60% 0.9926 13
728 70% 0.9905 6
832 80% 0.9919 6
936 90% 0.9920 6
1040 100% 0.9925 10
2.28% higher than those of GRX. It can be seen that CutOut
has the most obvious effect on the cross-scene (on the same
device) generalization capability of AETNet.
2) Analysis of Cross-Device Generalization Capability:
The A VIRIS-Classic training set is collected by a different
device sensor from the test set. Since A VIRIS-Classic data
only remains 162 bands, we select the ﬁrst 50 and the ﬁrst 100
bands of the A VIRIS-Classic training set as the training data.
Due to the different spectral intervals and resolutions, there
is no common background spectrum between the training and
test data. The cross-device generalization capability is a more
difﬁcult challenge for hyperspectral AD networks. As shown
in Table VIII, the mAUC values of AETNet with CutOut
are 0.9904 and 0.9730, respectively, which are lower than
those in the cross-scene generalization capability (on the same
device) experiments. AETNet with CutMix has the lowest
mAUC value of 0.9864 on the ﬁrst 50 bands of the test set,
but achieves the highest mAUC value of 0.9837 on the ﬁrst
100 bands of the test set. In addition, AETNet with CutMix
is superior to the baseline (GRX) on these two comparative
experiments, and the mAUC values are increased by 0.67%
and 1.27%, respectively. In summary, AETNet with CutMix
is suitable for inference on the cross-device test data.
F . Analysis of Training Data Scale
Compared with the previous AD methods, our AETNet can
be trained on a training set that does not contain any test
HSI. The above experiments demonstrate that the image-level
training paradigm is effective for hyperspectral AD. Table IX
shows the detection accuracy of AETNet and the training
epoch that produces the peak value of the measurement Min
the transform domain search module under the A VIRIS-NG
training data of different scales. With only 5% of the training
data, AETNet can reach a mAUC value of 0.9916. Using 15%
of the training set can achieve the detection accuracy obtained
on the whole training set. In addition, the epoch required for
searching the transform domain decreases as the scale of the
training data increases.15
TABLE X
DETECTION ACCURACY PERFORMANCE OF AETN ETACHIEVED BY DIFFERENT LOSSFUNCTIONS ON HAD100 D ATASET .
Data 50 Bands 100 Bands
Loss L1 L2 SSIM MSGMS L1 L2 SSIM MSGMS
Random Mask X X X X X X X X
mAUC 0.9799 0.9799 0.9875 0.9799 0.9880 0.9799 0.9925 0.9905 0.9811 0.9714 0.9856 0.9775 0.9858 0.9716 0.9875 0.9714
(a)
(b)
(c)
1st Band
 2nd Band
 3rd Band
 4th Band
 5th Band
 6th Band
 7th Band
 8th Band
 9th Band
 10th Band
Fig. 11. Residual output maps of AETNet achieved on the test scene 6. (a) Use MSGMS loss and Random Mask strategy. (b) Only use MSGMS loss. (c)
Use L2 loss and Random Mask strategy.
(a)
(b)
(c)
1st Band
 2nd Band
 3rd Band
 4th Band
 5th Band
 6th Band
 7th Band
 8th Band
 9th Band
 10th Band
Fig. 12. Residual output maps of AETNet achieved on the test scene 7. (a) Use MSGMS loss and Random Mask strategy. (b) Only use MSGMS loss. (c)
Use L2 loss and Random Mask strategy.
G. Ablation Experiments
1) Loss Function: As shown in Table X, we compare
the detection accuracy of AETNet with four different loss
functions. L1 loss and L2 loss penalize pixel-wise spectral
differences between input and output images. MSGMS loss
considers the spatial correlation of pixels and penalizes the
structural differences between input and output images. SSIM
loss simultaneously penalizes the pixel-wise spectral differ-
ences and the structural differences. When the Random Mask
strategy is adopted, most mAUC values of AETNet with these
losses are better than the baseline values achieved by GRX.
MSGMS loss outperforms the other three loss functions, while
L1 loss performs the worst and fails on the ﬁrst 50 bands.
SSIM loss has a slight advantage over L2 loss on the ﬁrst
50 and 100 bands. Figs. 11 and 12 show the residual outputof AETNet in the inference on the test scene 6 and 7. It can
be obviously seen that anomaly targets have higher response
values than background on the residual output maps when our
network adopts the Random Mask strategy. Consequently, the
spectral differences between anomaly targets and background
are enhanced in the transform domain generated by AETNet.
Compared with L2 loss, MSGMS loss obtains residual output
maps with a cleaner background.
In contrast, when AETNet is not trained with the Random
Mask strategy, the enhancement effect on anomaly targets
cannot be guaranteed. Table X shows that all four loss func-
tions fail without the Random Mask strategy. Figs. 11(b) and
12(b) show that the residual output of AETNet without the
Random Mask strategy does not focus on anomaly targets.
These experimental results indicate that the Random Mask16
TABLE XI
DETECTION ACCURACY PERFORMANCE ACHIEVED BY DIFFERENT
NETWORK ARCHITECTURE ON THE FIRST 50 B ANDS OF THE TESTSET
CAE UNetSwin Transformer ResidualmAUCBlock Connection
X 0.9850
X X 0.9894
X X 0.9814
X X X 0.9906
X X X 0.9877
X X X X 0.9925
strategy is the foundation for the generalization capability of
AETNet.
2) Network Structure: Table XI shows the detection accu-
racy achieved by different network architectures on the ﬁrst
50 bands. A single CAE with residual connection reaches
an mAUC value of 0.9894 and exceeds WeaklyADBwith a
value of 0.9887, which indicates that our image-level training
paradigm is better than the traditional spectral-level training
paradigm. Added with the UNet structure, the network reaches
an mAUC value of 0.9906. Furthermore, the full version of
AETNet, which uses Swin Transformer blocks, can reach an
mAUC value of 0.9925. In addition, the residual connection
plays an important role in our AETNet. When the residual
connection is removed, the detection performance of all three
network structures decreases dramatically.
V. C ONCLUSION
In this paper, we introduce an image-level training paradigm
and Random Mask strategy to solve the generalization problem
in hyperspectral anomaly detection. Our method eliminates the
need for adjusting parameters or retraining on new test scenes
as required by most existing methods. Moreover, we developed
a large-scale hyperspectral anomaly detection dataset and
a uniﬁed evaluation benchmark. Experimental results show
that our method achieves a better balance between detection
accuracy and inference speed than existing state-of-the-art
methods. We hope our work can stimulate the community to-
ward real-world and practical hyperspectral anomaly detection.
REFERENCES
[1] M. Borengasser, W. Hungate, and R. Watkins, Hyperspectral Remote
Sensing—Principles and Applications . Boca Raton, FL, USA: CRC
Press, 2008. I
[2] B. Datt, T. McVicar, T. Van Niel, D. Jupp, and J. Pearlman, “Prepro-
cessing EO-1 hyperion hyperspectral data to support the application of
agricultural indexes,” IEEE Trans. Geosci. Remote Sens. , vol. 41, no. 6,
pp. 1246–1259, Jun. 2003. I
[3] M. Eismann, A. Stocker, and N. Nasrabadi, “Automated hyperspectral
cueing for civilian search and rescue,” in Proc. IEEE , vol. 97, no. 6,
Jun. 2009, pp. 1031–1055. I
[4] B. H ¨orig, F. K ¨uhn, F. Osch ¨utz, and F. Lehmann, “HyMap hyperspectral
remote sensing to detect hydrocarbons,” Int. J. Remote Sens. , vol. 22,
no. 8, pp. 1413–1422, 2001. I
[5] P. Mehl, Y . Chen, M. Kim, and D. Chan, “Development of hyperspectral
imaging technique for the detection of apple surface defects and con-
taminations,” J. Food Eng. , vol. 61, no. 1, pp. 67–81, 2004. I
[6] A. Zare, J. Bolton, P. Gader, and M. Schatten, “Vegetation mapping
for landmine detection using long-wave hyperspectral imagery,” IEEE
Trans. Geosci. Remote Sens. , vol. 46, no. 1, pp. 172–178, Jan. 2008. I[7] I. Reed and X. Yu, “Adaptive multiple-band CFAR detection of an
optical pattern with unknown spectral distribution,” IEEE Trans. Acoust.,
Speech, Signal Process. , vol. 38, no. 10, pp. 1760–1770, Oct. 1990. I,
IV-C2
[8] M. Molero, E. Garzn, I. Garcła, and A. Plaza, “Analysis and optimiza-
tions of global and local versions of the rx algorithm for anomaly
detection in hyperspectral data,” IEEE Trans. Geosci. Remote Sens. ,
vol. 6, no. 2, pp. 801–814, Apr. 2013. I, IV-C2
[9] C. Caefer, J. Silverman, O. Orthal, D. Antonelli, Y . Sharoni, and
S. Rotman, “Improved covariance matrices for point target detection
in hyperspectral data,” Opt. Eng. , vol. 47, no. 7, pp. 1–13, 2008. I,
IV-C2
[10] Y . P. Taitano, B. A. Geier, and K. W. Bauer, “A locally adaptable iterative
rx detector,” EURASIP Journal on Advances in Signal Processing , vol.
2010, pp. 1–10, 2010. I
[11] Q. Guo, B. Zhang, Q. Ran, L. Gao, J. Li, and A. Plaza, “Weighted-
RXD and Linear Filter-Based RXD: Improving background statistics
estimation for anomaly detection in hyperspectral imagery,” IEEE J. Sel.
Topics Appl. Earth Observ. Remote Sens. , vol. 7, no. 6, pp. 2351–2366,
Jun. 2014. I
[12] H. Kwon and N. Nasrabadi, “Kernel RX-algorithm: a nonlinear anomaly
detector for hyperspectral imagery,” IEEE Trans. Geosci. Remote Sens. ,
vol. 43, no. 2, pp. 388–397, Feb 2005. I, IV-C2
[13] J. Zhou, C. Kwan, B. Ayhan, and M. Eismann, “A novel cluster kernel
rx algorithm for anomaly and change detection using hyperspectral
images,” IEEE Trans. Geosci. Remote Sens. , vol. 54, no. 11, pp. 6497–
6504, 2016. I
[14] P. Gurram and H. Kwon, “Support-vector-based hyperspectral anomaly
detection using optimized kernel parameters,” IEEE Geosci. Remote
Sens. Lett. , vol. 2, no. 6, pp. 1060–1064, Nov. 2011. I, IV-C2
[15] J. Liu, Z. Hou, W. Li, R. Tao, D. Orlando, and H. Li, “Multipixel
anomaly detection with unknown patterns for hyperspectral imagery,”
IEEE Trans. Neural Netw. Learn. Syst. , vol. 33, no. 10, pp. 5557–5567,
2022. I, IV-C2
[16] W. Li and Q. Du, “Collaborative representation for hyperspectral
anomaly detection,” IEEE Trans. Geosci. Remote Sens. , vol. 53, no. 3,
pp. 1463–1474, Mar. 2015. I, IV-C2
[17] Q. Ling, Y . Guo, Z. Lin, and W. An, “A constrained sparse representa-
tion model for hyperspectral anomaly detection,” IEEE Trans. Geosci.
Remote Sens. , vol. 57, no. 4, pp. 2358–2371, April 2019. I, IV-C2
[18] G. Zhang, N. Li, B. Tu, Z. Liao, and Y . Peng, “Hyperspectral anomaly
detection via dual collaborative representation,” IEEE J. Sel. Topics Appl.
Earth Observ. Remote Sens. , vol. 13, pp. 4881–4894, 2020. I
[19] W. Liu and C. Chang, “Multiple-window anomaly detection for hyper-
spectral imagery,” IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens. ,
vol. 6, no. 2, pp. 644–658, April 2013. I
[20] L. Ren, L. Zhao, and Y . Wang, “A superpixel-based dual window rx
for hyperspectral anomaly detection,” IEEE Geosci. Remote Sens. Lett. ,
vol. 17, no. 7, pp. 1233–1237, 2020. I
[21] C.-I. Chang, “Hyperspectral anomaly detection: A dual theory of hyper-
spectral target detection,” IEEE Trans. Geosci. Remote Sens. , vol. 60,
pp. 1–20, 2022. I
[22] ——, “Constrained energy minimization anomaly detection for hyper-
spectral imagery via dummy variable trick,” IEEE Trans. Geosci. Remote
Sens. , vol. 60, pp. 1–19, 2022. I
[23] ——, “Target-to-anomaly conversion for hyperspectral anomaly detec-
tion,” IEEE Trans. Geosci. Remote Sens. , vol. 60, pp. 1–28, 2022. I
[24] B. Tu, X. Yang, N. Li, C. Zhou, and D. He, “Hyperspectral anomaly
detection via density peak clustering,” Pattern Recognition Letters , vol.
129, pp. 144–149, 2020. I
[25] A. Rodriguez and A. Laio, “Clustering by fast search and ﬁnd of density
peaks,” science , vol. 344, no. 6191, pp. 1492–1496, 2014. I
[26] R. Tao, X. Zhao, W. Li, H. Li, and Q. Du, “Hyperspectral anomaly
detection by fractional fourier entropy,” IEEE J. Sel. Topics Appl. Earth
Observ. Remote Sens. , vol. 12, no. 12, pp. 4920–4929, Dec 2019. I,
IV-C2
[27] X. Kang, X. Zhang, S. Li, K. Li, J. Li, and J. Benediktsson, “Hyper-
spectral anomaly detection with attribute and edge-preserving ﬁlters,”
IEEE Trans. Geosci. Remote Sens. , vol. 55, no. 10, pp. 5600–5611, Oct
2017. I, IV-A, IV-C2
[28] J. Lei, W. Xie, J. Yang, Y . Li, and C. Chang, “Spectral-spatial feature
extraction for hyperspectral anomaly detection,” IEEE Trans. Geosci.
Remote Sens. , vol. 57, no. 10, pp. 8131–8143, Oct 2019. I
[29] W. Xie, T. Jiang, Y . Li, X. Jia, and J. Lei, “Structure tensor and guided
ﬁltering-based algorithm for hyperspectral anomaly detection,” IEEE
Trans. Geosci. Remote Sens. , vol. 57, no. 7, pp. 4218–4230, 2019. I17
[30] Z. Li, Q. Ling, Z. Lin, and J. Wu, “Segmentation-based weighting
strategy for hyperspectral anomaly detection,” IEEE Geosci. Remote
Sens. Lett. , vol. 18, no. 10, pp. 1806–1810, Oct 2021. I
[31] S. Li, K. Zhang, P. Duan, and X. Kang, “Hyperspectral anomaly
detection with kernel isolation forest,” IEEE Trans. Geosci. Remote
Sens. , vol. 58, no. 1, pp. 319–329, Jan 2020. I, IV-C2
[32] X. Zhang, G. Wen, and W. Dai, “A tensor decomposition-based anomaly
detection algorithm for hyperspectral image,” IEEE Trans. Geosci.
Remote Sens. , vol. 54, no. 10, pp. 5801–5820, Oct 2016. I
[33] Z. Chen, B. Yang, and B. Wang, “A preprocessing method for hyper-
spectral target detection based on tensor principal component analysis,”
Remote Sensing , vol. 10, no. 7, p. 1033, 2018. I
[34] L. Li, W. Li, Y . Qu, C. Zhao, R. Tao, and Q. Du, “Prior-based tensor
approximation for anomaly detection in hyperspectral imagery,” IEEE
Trans. Neural Netw. Learn. Syst. , vol. 33, no. 3, pp. 1037–1050, March
2022. I, IV-C2
[35] T. Zhou and D. Tao, “Godec: Randomized low-rank & sparse matrix
decomposition in noisy case,” in Proceedings of the 28th International
Conference on Machine Learning (ICML) , 2011. I
[36] C.-I. Chang, H. Cao, S. Chen, X. Shang, C. Yu, and M. Song, “Orthog-
onal subspace projection-based go-decomposition approach to ﬁnding
low-rank and sparsity matrices for hyperspectral anomaly detection,”
IEEE Trans. Geosci. Remote Sens. , vol. 59, no. 3, pp. 2403–2429, 2021.
I
[37] C.-I. Chang, H. Cao, and M. Song, “Orthogonal subspace projection
target detector for hyperspectral anomaly detection,” IEEE J. Sel. Topics
Appl. Earth Observ. Remote Sens. , vol. 14, pp. 4915–4932, 2021. I
[38] C.-I. Chang and J. Chen, “Hyperspectral anomaly detection by data
sphering and sparsity density peaks,” IEEE Trans. Geosci. Remote Sens. ,
vol. 60, pp. 1–21, 2022. I
[39] S. Chen, C.-I. Chang, and X. Li, “Component decomposition analysis
for hyperspectral anomaly detection,” IEEE Trans. Geosci. Remote Sens. ,
vol. 60, pp. 1–22, 2022. I
[40] C.-I. Chang, “Effective anomaly space for hyperspectral anomaly de-
tection,” IEEE Trans. Geosci. Remote Sens. , vol. 60, pp. 1–24, 2022.
I
[41] I. Goodfellow, J. Pouget, M. Mirza, B. Xu, D. Warde, S. Ozair,
A. Courville, and Y . Bengio, “Generative adversarial networks,” Com-
munications of the ACM , vol. 63, no. 11, pp. 139–144, 2020. I
[42] C. Zhao, X. Li, and H. Zhu, “Hyperspectral anomaly detection based
on stacked denoising autoencoders,” Journal of Applied Remote Sensing ,
vol. 11, no. 4, p. 042605, 2017. I
[43] K. Jiang, W. Xie, J. Lei, Z. Li, Y . Li, T. Jiang, and Q. Du, “E2e-liade:
End-to-end local invariant autoencoding density estimation model for
anomaly target detection in hyperspectral image,” IEEE Trans. Cybern. ,
pp. 1–12, 2021. I
[44] Y . Li, T. Jiang, W. Xie, J. Lei, and Q. Du, “Sparse coding-inspired
gan for hyperspectral anomaly detection in weakly supervised learning,”
IEEE Trans. Geosci. Remote Sens. , vol. 60, pp. 1–11, 2022. I
[45] K. Jiang, W. Xie, J. Lei, T. Jiang, and Y . Li, “Lren: Low-rank embedded
network for sample-free hyperspectral anomaly detection,” in Proceed-
ings of the AAAI Conference on Artiﬁcial Intelligence (AAAI) , vol. 35,
no. 5, 2021, pp. 4139–4146. I, IV-C2
[46] X. Li, S. Yu, S. Chen, and L. Zhao, “Normalizing ﬂow-based probability
distribution representation detector for hyperspectral anomaly detection,”
IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens. , vol. 15, pp. 4885–
4896, 2022. I
[47] A. Taghipour and H. Ghassemian, “Unsupervised hyperspectral target
detection using spectral residual of deep autoencoder networks,” in 2019
4th International Conference on Pattern Recognition and Image Analysis
(IPRIA) , 2019, pp. 52–57. I
[48] N. Huyan, X. Zhang, D. Quan, J. Chanussot, and L. Jiao, “Cluster-
memory augmented deep autoencoder via optimal transportation for
hyperspectral anomaly detection,” IEEE Trans. Geosci. Remote Sens. ,
vol. 60, pp. 1–16, 2022. I
[49] G. Fan, Y . Ma, X. Mei, F. Fan, J. Huang, and J. Ma, “Hyperspectral
anomaly detection with robust graph autoencoders,” IEEE Trans. Geosci.
Remote Sens. , vol. 60, pp. 1–14, 2022. I, IV-C2
[50] J. Wei, J. Zhang, Y . Xu, L. Xu, Z. Wu, and Z. Wei, “Hyperspectral
anomaly detection based on graph regularized variational autoencoder,”
IEEE Geosci. Remote Sens. Lett. , vol. 19, pp. 1–5, 2022. I
[51] S. Wang, X. Wang, L. Zhang, and Y . Zhong, “Auto-ad: Autonomous
hyperspectral anomaly detection network based on fully convolutional
autoencoder,” IEEE Trans. Geosci. Remote Sens. , vol. 60, pp. 1–14,
2022. I, IV-C2
[52] T. Jiang, W. Xie, Y . Li, J. Lei, and Q. Du, “Weakly supervised
discriminative learning with spectral constrained generative adversarialnetwork for hyperspectral anomaly detection,” IEEE Trans. Neural Netw.
Learn. Syst. , pp. 1–14, 2021. I, IV-C2
[53] Y . Liu, W. Xie, Y . Li, Z. Li, and Q. Du, “Dual-frequency autoencoder for
anomaly detection in transformed hyperspectral imagery,” IEEE Trans.
Geosci. Remote Sens. , vol. 60, pp. 1–13, 2022. I
[54] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks
for biomedical image segmentation,” in International Conference on
Medical Image Computing and Computer-assisted Intervention (MIC-
CAI) . Springer, 2015, pp. 234–241. II-A
[55] B. Li, C. Xiao, L. Wang, Y . Wang, Z. Lin, M. Li, W. An, and Y . Guo,
“Dense nested attention network for infrared small target detection,”
IEEE Trans. Image Process. , pp. 1–1, 2022. II-A
[56] K. Zhang, Y . Li, W. Zuo, L. Zhang, L. Van Gool, and R. Timofte,
“Plug-and-play image restoration with deep denoiser prior,” IEEE Trans.
Pattern Anal. Mach. Intell. , vol. 44, no. 10, pp. 6360–6376, 2022. II-A
[57] Y . Cai, J. Lin, X. Hu, H. Wang, X. Yuan, Y . Zhang, R. Timofte, and
L. Gool, “Mask-guided spectral-wise transformer for efﬁcient hyper-
spectral image reconstruction,” in IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR) , 2022, pp. 17 481–17 490. II-A
[58] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” Advances in
Neural Information Processing Systems , vol. 30, 2017. II-B
[59] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,
T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al. ,
“An image is worth 16x16 words: Transformers for image recognition
at scale,” arXiv preprint arXiv:2010.11929 , 2020. II-B
[60] M. Raghu, T. Unterthiner, S. Kornblith, C. Zhang, and A. Dosovitskiy,
“Do vision transformers see like convolutional neural networks?” Ad-
vances in Neural Information Processing Systems , vol. 34, pp. 12 116–
12 128, 2021. II-B
[61] L. Yuan, Y . Chen, T. Wang, W. Yu, Y . Shi, Z. Jiang, F. Tay, J. Feng, and
S. Yan, “Tokens-to-token vit: Training vision transformers from scratch
on imagenet,” in IEEE/CVF International Conference on Computer
Vision (ICCV) , 2021, pp. 538–547. II-B
[62] W. Wang, E. Xie, X. Li, D. Fan, K. Song, D. Liang, T. Lu, P. Luo, and
L. Shao, “Pyramid vision transformer: A versatile backbone for dense
prediction without convolutions,” in IEEE/CVF International Conference
on Computer Vision (ICCV) , 2021, pp. 548–558. II-B, II-B
[63] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang, S. Lin, and
B. Guo, “Swin transformer: Hierarchical vision transformer using shifted
windows,” in IEEE/CVF International Conference on Computer Vision
(ICCV) , 2021, pp. 9992–10 002. II-B, II-B, III-C2, III-C2
[64] H. Cao, Y . Wang, J. Chen, D. Jiang, X. Zhang, Q. Tian, and M. Wang,
“Swin-unet: Unet-like pure transformer for medical image segmenta-
tion,” arXiv preprint arXiv:2105.05537 , 2021. II-B, III-C2
[65] X. He, Y . Zhou, J. Zhao, D. Zhang, R. Yao, and Y . Xue, “Swin
transformer embedding unet for remote sensing image semantic seg-
mentation,” IEEE Trans. Geosci. Remote Sens. , vol. 60, pp. 1–15, 2022.
II-B
[66] K. He, X. Chen, S. Xie, Y . Li, P. Dollr, and R. Girshick, “Masked
autoencoders are scalable vision learners,” in IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR) , 2022, pp. 15 979–
15 988. III-B
[67] V . Zavrtanik, M. Kristan, and D. Sko ˇcaj, “Reconstruction by inpainting
for visual anomaly detection,” Pattern Recognition , vol. 112, p. 107706,
2021. III-B, III-B, III-D, III-D
[68] T. DeVries and G. Taylor, “Improved regularization of convolutional
neural networks with cutout,” arXiv preprint arXiv:1708.04552 , 2017.
III-B2
[69] S. Yun, D. Han, S. Chun, S. Oh, Y . Yoo, and J. Choe, “Cutmix: Regu-
larization strategy to train strong classiﬁers with localizable features,” in
2019 IEEE/CVF International Conference on Computer Vision (ICCV) ,
2019, pp. 6022–6031. III-B2
[70] W. Xue, L. Zhang, X. Mou, and A. Bovik, “Gradient magnitude
similarity deviation: A highly efﬁcient perceptual image quality index,”
IEEE Trans. Image Process. , vol. 23, no. 2, pp. 684–695, 2014. III-D,
III-D
[71] C. Chang, “An effective evaluation tool for hyperspectral target detec-
tion: 3d receiver operating characteristic curve analysis,” IEEE Trans.
Geosci. Remote Sens. , vol. 59, no. 6, pp. 5131–5153, 2021. IV-B, IV-B2
[72] D. Kingma and J. Ba, “Adam: A method for stochastic optimization,”
arXiv preprint arXiv:1412.6980 , 2014. IV-C1