An Improved Classical Singular Value Transformation
for Quantum Machine Learning
Ainesh Bakshi
ainesh@mit.edu
MITEwin Tang
ewint@cs.washington.edu
University of Washington
Abstract
The field of quantum machine learning (QML) produces many proposals for attaining
quantum speedups for tasks in machine learning and data analysis. Such speedups can only
manifest if classical algorithms for these tasks perform significantly slower than quantum
ones. We study quantum-classical gaps in QML through the quantum singular value trans-
formation (QSVT) framework. QSVT, introduced by Gilyén, Su, Low and Wiebe [GSLW19],
unifies all major types of quantum speedup [MRTC21]; in particular, a wide variety of QML
proposals are applications of QSVT on low-rank classical data. We challenge these proposals
by providing a classical algorithm that matches the performance of QSVT in this regime up
to a small polynomial overhead.
We show that, given a matrix A∈Cm×n, a vector b∈Cn, a bounded degree- dpoly-
nomial p, and linear-time pre-processing, we can output a description of a vector vsuch
that∥v−p(A)b∥⩽ε∥b∥ineO(d11∥A∥4
F/(ε2∥A∥4))time. This improves upon the best
known classical algorithm [CGLLTW22], which requires eO(d22∥A∥6
F/(ε6∥A∥6))time, and
narrows the gap with QSVT, which, after linear-time pre-processing to load input into a
quantum-accessible memory, can estimate the magnitude of an entry p(A)btoε∥b∥error
ineO(d∥A∥F/(ε∥A∥))time. Instantiating our algorithm with different polynomials, we
improve on prior classical algorithms for quantum-inspired regression [CGLLTW22; GST22],
recommendation systems [Tan19; CGLLTW22], and Hamiltonian simulation [CGLLTW22].
Our key insight is to combine the Clenshaw recurrence , an iterative method for computing
matrix polynomials, with sketching techniques to simulate QSVT classically. We introduce
several new classical techniques in this work, including (a) a non-oblivious matrix sketch
for approximately preserving bi-linear forms, (b) a new stability analysis for the Clenshaw
recurrence, and (c) a new technique to bound arithmetic progressions of the coefficients
appearing in the Chebyshev series expansion of bounded functions, each of which may be
of independent interest.arXiv:2303.01492v4  [quant-ph]  28 Jul 2024Contents
1 Introduction 1
1.1 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
1.2 Applications to Dequantizing QML . . . . . . . . . . . . . . . . . . . . . . . . . . 4
2 Technical Overview 5
3 Discussion 12
4 Preliminaries 14
4.1 Linear Algebra . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
4.2 Polynomials and the Chebyshev Basis . . . . . . . . . . . . . . . . . . . . . . . . . 15
4.3 Sampling and Query Access . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
5 Extending the Sketching Toolkit 19
5.1 The Bi-Linear Entry-wise Sampling Transform . . . . . . . . . . . . . . . . . . . . 19
5.2 Approximate Matrix Products from ℓ2
2Importance Sampling . . . . . . . . . . . . 20
6 Sums of Chebyshev Coefficients 23
7 Properties of the Clenshaw Recursion 29
7.1 Deriving the Clenshaw Recursions . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
7.2 Evaluating Even and Odd Polynomials . . . . . . . . . . . . . . . . . . . . . . . . 30
8 Stability of the Scalar Clenshaw Recursion 32
8.1 Analyzing Error Propagation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
8.2 Bounding the Iterates of the Clenshaw Recurrence . . . . . . . . . . . . . . . . . . 35
9 Computing Matrix Polynomials 37
9.1 Computing Odd Matrix Polynomials . . . . . . . . . . . . . . . . . . . . . . . . . . 37
9.2 Generalizing to Even Polynomials . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
9.3 Bounding Iterates of Singular Value Transformation . . . . . . . . . . . . . . . . . 49
10 Dequantizing QML Algorithms 51
10.1 Recommendation Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
10.2 Linear Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
10.3 Hamiltonian Simulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
Acknowledgments 54
References 541 Introduction
Quantum machine learning (QML) has rapidly developed into an active field of study with
numerous proposals for speeding up machine learning tasks with quantum computers [DW20;
Cil+18]. These proposals include quantum algorithms for basic tasks in machine learning,
including regression [HHL09], perceptron learning [KWS16], support vector machines [RML14],
recommendation systems [KP17], and semi-definite programming [BKLLSW19]. A central
goal of QML is to demonstrate a problem on which quantum computers obtain a substantial
practical speedup over classical computers. A successful resolution of this goal would provide
compelling motivation to invest more resources into developing scalable quantum computers
(i.e. be a killer app for quantum computing).
The quantum singular value transformation (QSVT) framework [LC17; CGJ19; GSLW19] uses
ideas from signal processing to unify algorithm design for quantum linear algebra and, by
extension, QML. This framework has been called a “grand unification” of quantum algorithms
as it captures all major classes of quantum advantage [MRTC21]. With respect to QML, QSVT
captures essentially all known techniques in quantum linear algebra, so it will be the focus
of our investigation. QSVT generalizes the simple observation that, for a quantum state
|b⟩=∑ibi|i⟩ ∈Cnencoding the vector binto its amplitudes, a quantum gate corresponds to
applying a unitary matrix U∈Cn×nto the vector, |b⟩ 7→ | Ub⟩. By including the non-unitary
operation of measurement, we can consider quantum circuits that perform |b⟩ 7→ | Ab⟩for
general (bounded) matrices A∈Cm×n; such circuits are called block-encodings . The fundamental
result of QSVT is that, given a block-encoding of Aand a bounded, even or odd, degree- d
polynomial p, we can form a block-encoding of p(A)with an overhead of d.1This new quantum
circuit can then be applied to a quantum state |b⟩to get|p(A)b⟩with probability ∥p(A)b∥2. In
this way, quantum linear algebra algorithms can apply generic functions to the singular values
of a matrix efficiently, provided that we have an efficient block-encoding and the function is
smooth enough to be approximated well by a low-degree polynomial.
Though QSVT is an immensely powerful tool for quantum linear algebra, it requires having an
efficient block-encoding for the input matrix A. These efficient block-encodings do not exist
in general, but they do exist for two broad classes of matrices, assuming appropriately strong
forms of coherent access: matrices with low sparsity [GSLW19, Lemma 48] (a typical setting for
quantum simulation) and matrices with low stable rank [GSLW19, Lemma 50] (a typical setting
for quantum machine learning). We treat the latter case; specifically, assuming the existence of
quantum random access memory (QRAM), a piece of hardware that supports queries to its memory
in superposition [GLM08],2we can process a matrix Agiven as a list of its entries Ai,jinto a data
structure in linear time such that preparation of a block-encoding of A/∥A∥Fis efficient (e.g.
using O(log(mn))queries to the QRAM). Here, ∥A∥F=q
∑i,jAi,j2denotes the Frobenius
norm of A. This type of block-encoding is the one commonly used for quantum linear algebra
algorithms on classical data [KLLP19; CW23], since it works for arbitrary matrices and vectors,
paying only a ∥A∥F/∥A∥factor in sub-normalization.
1When Ais asymmetric, we can interpret QSVT as applying the matrix function that applies pto each singular
value of A(Definition 4.1).
2Having quantum random access memory is an assumption similar to the classical random access memory
assumption in algorithm design, that querying any piece of memory has cost only logarithmic (or constant) in
input size. It’s unclear whether we can attain the effectively constant cost of modern RAM when we want to query
memory in superposition, making this assumption somewhat speculative [JR23]. In this paper, we will be fine giving
quantum algorithms QRAM assumptions, and these matters will not affect the classical algorithms we present.
1A natural question then arises: how well can classical computers simulate QSVT? Sparsity-
based QSVT supports universal quantum computation, and therefore cannot be simulated
classically in polynomial time unless BPP=BQP . On the other hand, prior work by Chia, Gilyén,
Li, Lin, Tang and Wang [CGLLTW22, Section 6.1] gives a classical algorithm that outputs a
vector vsuch that ∥p(A)b−v∥⩽ε∥b∥inO(d22∥A∥6
F/(ε6∥A∥6))time, after a linear-time pre-
processing phase. More generally, all of the “quantum-inspired” algorithms generated through
this framework have large polynomial slowdowns, as they require computing a singular value
decomposition of a matrix with Ω((∥A∥F
∥A∥ε)2)rows and columns, immediately incurring a power-
six dependence in εand the Frobenius norm of A. In fact, without assuming that the input
matrix is strictly low-rank, all prior quantum-inspired algorithms incur a 1/ε6dependence,
except for one for linear regression which incurs an 1/ε4[GST22]. This gives the perception
that a large polynomial running time (significantly larger than quartic) to simulate QSVT may
be inherent. In fact, this classical hardness has been conjectured [KLLP19; KP22]. Such a
conclusion would be significant, as this suggests a potential regime for a practical speedup for
many problems in QML. Therefore, the central question we address in this work is as follows:
How large is the quantum-classical gap for singular value transformation
and the machine learning problems captured by it?
1.1 Results
Our main result addresses the aforementioned question by providing an improved classical al-
gorithm for QSVT that avoids computing a full singular value decomposition. As a consequence,
we also obtain better bounds on the quantum-classical gap for regression [CGLLTW20; GST22],
recommendation systems [Tan19; CGLLTW22], and Hamiltonian simulation [CGLLTW22]
with improved dependence in each relevant parameter. Additionally, we improve over prior
quantum-inspired algorithms with atypical guarantees in various parameter regimes [CCHLW22;
SM22]. We begin by stating our result for simulating QSVT on inputs with low stable rank.3
Theorem 1.1 (Classical Singular Value Transformation, informal version of Theorem 9.1) .Given
a Hermitian A∈Cn×nandb∈Cn, and an accuracy parameter 0<ε<1, afterO(nnz(A) +nnz(b))
pre-processing time to create a data structure,4for a degree- dpolynomial psuch that |p(x)|⩽1for
x∈[−∥A∥,∥A∥], we can output a description of a vector y∈ Cnsuch that with probability at least
0.9,∥y−p(A)b∥⩽ε∥b∥. The algorithm takes
eOd11∥A∥4
F
∥A∥4ε2
time to output a description of yasAxfor a sparse vector x. This description allows us to compute
entries of y in eOd6∥A∥2
F
∥A∥2ε2
time and obtain an ℓ2
2sample from y in eOd8∥A∥4
F∥b∥2
∥A∥4ε2∥y∥2
time.5
The assumptions that Ais square and Hermitian are not needed; for other A, the definition
ofp(A)needs to be adjusted appropriately and restricted to peither even or odd. For the
discussion that follows, we assume ∥A∥⩽1and that p(x)is bounded in the interval [−1, 1],
which allows us to analyze p(x)in terms of its Chebyshev coefficients, without rescaling.
3The stable rank of a matrix Ais∥A∥2
F/∥A∥2.
4If we are already given Aand bin the QRAM data structures needed to prepare a block-encoding of A/∥A∥F
and a quantum state of b/∥b∥, this pre-processing can be done in eO(d12∥A∥4
F/(∥A∥4ε4))time.
5Throughout the paper, we use eO(·)to surpress poly-logarithmic factors in d, 1/εand∥A∥2
F/∥A∥2.
2Comparison to QSVT. We compare our main result to the one achievable by QSVT. The
quantum computer is able to produce a quantum state approximating |p(A)b⟩, but to give
a concrete comparison, we consider the task of estimating an overlap with a given vector v.
Classically, we can use Theorem 1.1 to compute a description of y≈p(A)band compute entries
yior estimate overlaps ⟨v|y⟩of that description, all in eO(d11∥A∥4
F/ε2)time (Remark 9.2).
In the same setting, a quantum computer equipped with a QRAM can output an ε-good esti-
mate of |⟨v|p(A/2)b⟩|2inO(d∥A∥Flog(mn)/ε)gates and queries to the QRAM.6We get this
from the following procedure: in the pre-processing phase, place Aand bin data structures
in QRAM such that we can prepare states |b⟩and perform a block-encoding for A/∥A∥Fin
O(log(mn))QRAM queries [Pra14]. Using QSVT, we can get a block-encoding of an εapproxi-
mation of A/2with cost inflated by a factor of O(∥A∥Flog(∥A∥F/ε))through singular value
amplification [GSLW19, Theorem 30], from which we can get a block-encoding of (approx-
imately) p(A/2)with cost inflated by a further factor of O(d). Applying this to a state |b⟩,
we get an output state approximating |p(A/2)b⟩(thought of as a sub-normalized state) with
eO(d∥A∥Flog(mn)log(1/ε))gates. We can then produce a sample from p(A/2)bby paying
an additional 1/∥p(A/2)b∥2factor. So far, the dependence on error εis merely logarithmic.
However, this is not a realizable quantum speedup (except possibly for sampling tasks) since
the output is a quantum state: estimating some statistic of the quantum state requires incurring
a polynomial dependence on ε. For example, if we want to estimate the overlap |⟨v|p(A/2)b⟩|2,
then this can be done with 1/ε2invocations of a swap test (or 1/εif one uses amplitude amplifi-
cation). More generally, distinguishing a state from one ε-far in trace distance requires Ω(1/ε)
additional overhead, even when given an oracle efficiently preparing that state,7so estimating
quantities to this sensitivity requires polynomial dependence on ε.
To summarize, the quantum-classical gap is is 1-to-11 for the degree d, 1-to-4 for ∥A∥F(which
we think of as square root of stable rank ∥A∥2
F/∥A∥2), and 1-to-2 for ε.8The quadratic gap
inεseems inherent, and the quartic gap in stable rank bears resemblance to quartic quantum
speedups noted for other spectral algorithms [Has20]. The d11degree dependence could
potentially be improved, though our techniques have an inherent limit of d5; see Section 2 for
more details.
Comparison to [CGLLTW22; JGS20; GL22]. There are three papers that get similar results
about “dequantizing the quantum singular value transformation”. The work of Chia, Gilyén, Li,
Lin, Tang, and Wang [CGLLTW22] gives a running time of eO(d22∥A∥6
F/ε6)(after renormalizing
so that ∥A∥=1instead of ∥A∥F=1). We improve in all parameters over this work: degree of
the polynomial p, Frobenius norm of A, and accuracy parameter ε.
The work of Jethwani, Le Gall, and Singh [JGS20] provides two algorithms for applying f(A)b
for Lipschitz-continuous fand Awith condition number κ. (Standard results in function
6We will not concern ourselves with log(mn)andlog1
εfactors: quantum algorithms typically count bit complexity
where classical algorithms count word RAM complexity, which muddles any such comparisons. We will also ignore
issues of sub-normalization (i.e. the AvsA/2 in the comparison), though this seeming constant factor incurred by
the quantum algorithm can lead to quadratic losses depending on the application.
7This holds because the output state of a quantum circuit that applies a unitary U T times is perturbed by at most
Tεwhen Uis perturbed by ε. So, 1/εapplications of Uare needed to distinguish between Uand some ε-close ˜U
with constant probability.
8Note that the quantum-classical comparison can be very different (in both directions, favoring quantum or
classical) depending on what property of the output vector we wish to learn. See the related work section for more
discussion of this; we choose the “overlap” task, as this is a natural linear algebraic quantity to estimate, and is the
quantity QML algorithms typically try to compute.
3approximation state that such functions can be approximated by polynomials of degree O(L),
where Lis the Lipschitz constant of fand the tail is either polynomial or exponential depending
on how smooth fis [Tre19].) In particular, they achieve a running time of O(∥A∥6
Fκ20(d2+
κ)6/ε6)to apply a degree- dpolynomial. Again, we improve in all parameters over this work:
degree of the polynomial p, Frobenius norm of A, and accuracy parameter ε. We also do not
incur condition number dependence.
Finally, the work of Gharibian and Le Gall [GL22] considers QSVT when the input matrices are
sparse, which is the relevant regime for quantum chemistry and other problems in many-body
systems, where the matrix is a local Hamiltonian. See also Van den Nest’s work which uses
similar techniques to simulate restricted classes of quantum circuits [Van11]. The setting where
Ais sparse (corresponding to QSVT with sparse input) is significantly different from our setting,
where Ahas low stable rank. In our setting, algorithms run in time polynomial in degree and
stable rank, whereas in the low sparsity case, algorithms run in time exponential in degree and
polynomial in sparsity instead of stable rank. These are incomparable: algorithms specialized
to one setting perform worse than the naive algorithm in the other setting.
1.2 Applications to Dequantizing QML
Now, we describe the implications of Theorem 1.1 to specific problems in QML. We obtain faster
quantum-inspired algorithms for linear regression, recommendation systems, and Hamiltonian
simulation.
We begin with quantum recommendation systems, where the goal is to output a sample from
a row of a low-rank approximation of the input vector A. The original quantum algorithm
has running time of eO(∥A∥F/σ)to output |[thresh σ(A)]i,∗⟩, where thresh σis a polynomial
approximating a threshold function—close to identity for values ⩾σ, close to zero for values
⩽5σ/6, and bounded in between [KP17; CGJ19]. We then obtain the following corollary:
Corollary 1.2 (Dequantized recommendation systems, informal version of Corollary 10.2) .Given
a matrix A∈Cm×nsuch that ∥A∥=1, a sufficiently small accuracy parameter ε>0, and an i∈[n],
we can produce a data structure in O(nnz(A))time such that, we can compute a description of a vector
ysuch that with probability at least 0.9,∥y−[thresh σ(A)]i,∗∥⩽ε∥Ai,∗∥ineO(∥A∥4
F/ 
σ11ε2)time.
From this description we can output a sample from y in eO∥A∥4
F∥Ai,∗∥2
σ8ε2∥y∥2
time.
Remark 1.3 (Comparison to [CGLLTW22]) .[CGLLTW22, Corollary 6.7] achieves a running time
ofeO(∥A∥6
F/(σ16ε6)). We improve upon it in every parameter, including error ε, the threshold
σ, and the Frobenius norm ∥A∥F. We defer comparison to the work of Chepurko, Clarkson,
Horesh, Lin, and Woodruff [CCHLW22] to Remark 10.3, as it achieves different guarantees
from QSVT. To summarize the discussion there, if we attempt to translate the guarantees of
[CCHLW22] to this setting, we find that our running times are better, under the mild assumption
thatεis roughly smaller than σ.
Next, we state the dequantization result we obtain for regression. Quantum algorithms can
produce a state ε-close to |invκ(A)b⟩inO(κ∥A∥Flog1
ε)time, where invκis a polynomial close
to1/xon[1/κ, 1]and and smoothly thresholds away all singular values below 1/κ[WZP18;
CGJ19].
Corollary 1.4 (Dequantized regression, informal version of Corollary 10.5) .Given A∈Cm×n
andb∈Cnsuch that ∥A∥=1,∥b∥⩽1, a singular value threshold 0<1/κ<1, and a sufficiently
small accuracy parameter ε>0. Then after O(nnz(A) +nnz(b))pre-processing time, we can compute
4a description of a vector y∈Cnsuch that, with probability ⩾0.9,∥y−invκ(A)b∥⩽ε∥b∥/κ. The
algorithm runs in eO(κ11∥A∥4
F/ε2)time. From this description we can output a sample from yin
eOκ10∥A∥4
F∥b∥2
ε2∥y∥2
time.
Remark 1.5 (Comparison with [CGLLTW22; GST22; SM22]) .Prior work [CGLLTW22, Corollary
6.15] achieves a running time of O(∥A∥6
Fκ22/ε6)to achieve an error bound of ε∥b∥/∥A∥(σin
that setting is our 1/κ). Converting our result there increases our runtime by κ2, but we still
improve over this result in all parameters.
Other quantum-inspired algorithms give improved results under fairly strong additional as-
sumptions. When Ahas no non-zero singular values larger than σ, then [GST22] achieves
a running time of O(∥A∥6
F/(σ12ε4)), which Shao and Montanaro [SM22] improve further to
O(∥A∥6
F/(σ8ε2))when bis in the image of A. We improve over the former in all parameters,
and for the latter we obtain a better ∥A∥Fdependence at the cost of a worse σdependence.
Both of these provide the stronger guarantee that the output vector is close up to relative error
ε∥A+b∥, though [GST22] requires factors of ∥A+∥∥b∥/∥A+b∥to compensate (which we elided
in the above expression).
Finally, we state the dequantization result we obtain for Hamiltonian simulation.
Corollary 1.6 (Dequantized Hamiltonian simulation, informal version of Corollary 10.8) .Given
a Hamiltonian H∈Cn×nwith∥H∥⩽1, a vector b∈Cn, and a sufficiently small accuracy parameter
ε>0, after O(nnz(H) +nnz(b))pre-processing time, we can output a description of a vector v
such that, with probability ⩾0.9,v−eiHtb⩽ε∥b∥with running time eO(t11∥H∥4
F/ε2). From this
description we can output a sample from v in eOt8∥H∥4
F∥b∥2
ε2∥v∥2
time.
Remark 1.7 (Comparison with [CGLLTW22]) .The only prior work [CGLLTW22] we are aware
of in the low-rank regime obtains a running time O(t16∥H∥6
F/ε6), and we improve upon it in
every parameter.
2 Technical Overview
Prior work [CGLLTW22] d22∥A∥6
F/ε6
Using the polynomial structure d17∥A∥4
F/ε4
Tightening Clenshaw’s stability analysis d13∥A∥4
F/ε4
Sparsifying matrices d11∥A∥4
F/ε2
Figure 1: A list of our technical contributions, along with the improvements they each make to
the running time of the final algorithm, ignoring log factors.
In this section, we describe our classical framework for simulating QSVT and provide an
overview of our key new contributions. For ease of exposition, we assume ∥A∥=1and
consider when pis odd. We have three conceptual contributions. First, we use an iterative
method (the Clenshaw recurrence) instead of a pure sketching algorithm, which improves the
running time to O(d17∥A∥4
F/ε4). This corresponds to diterations of matrix-vector products of
size∥A∥2
F/(ε/d4)2by∥A∥2
F/(ε/d4)2. Second, we develop new insights into the stability of the
Clenshaw recurrence and arithmetic progressions of Chebyshev coefficients to improve the size
from∥A∥2
F/(ε/d4)2to an∥A∥2
F/(ε/(d3log2(d)))2. Third, we give a subtle argument that we
5can sparsify these matrices, improving the ε4to an ε2. Together,9these give the final running
time of O(d11log4(d)∥A∥4
F/ε2). We now walk through these steps in more detail.
Computing matrix polynomials through sketches and iterative algorithms. Recall our goal of
simulating QSVT: given a matrix A∈ Cm×n, a vector b∈ Cn, and a polynomial p:[−1, 1]→R,
compute a description of a vector ysuch that ∥y−p(A)b∥⩽ε∥p∥sup∥b∥, where ∥p∥sup:=
maxx∈[−1,1]|p(x)|. Specifically, we aim for our algorithm to run in poly(∥A∥F,1
ε,d)time after
O(nnz(A) +nnz(b))pre-processing, and our output description is some sparse vector xsuch
that y=Ax, since this allows us to simulate some tasks that the quantum algorithm can do
with copies of |y⟩, like estimating overlaps and performing measurements in the computational
basis.
We require the running time of the algorithm to be independent of input dimension (after the
pre-processing) and therefore are compelled to create sketches of Aand band work with these
sketches. We note that prior work [Tan19; Tan21; CGLLTW22] stops here, and directly computes
a SVD of the sketches, and applies the relevant polynomial pto the singular values of the sketch.
As noted in the previous section, this approach loses large polynomial factors in the relevant
parameters.
Our main conceptual insight is to run iterative algorithms on the resulting sketches of A
and bin order to approximate matrix polynomials. In the canonical numerical linear algebra
regime (working with matrices directly, instead of sketches), there are two standard methods to
achieve this goal: (1) compute the polynomial explicitly through something like a Clenshaw
recurrence [Cle55], or (2) use the Lanczos method [Lan50] to create a roughly-orthogonal basis
for the Krylov subspace {b,Ab,A2b,. . .}and then apply the function exactly to the matrix
in this basis, in which Ais tridiagonal, implicitly using a polynomial approximation in the
analysis. We note that in a world where we are allowed to pay O(nnz(A))(or even O(m+n))
time per-iteration, we can simply run either of these algorithms and call it a day. The main
challenge in the setting we consider is that each iterative step must run in time that is entirely
dimension-independent.
Algorithm 1 (Singular value transformation for odd polynomials. Informal version of
Algorithm 2) .
Input (pre-processing): A matrix A∈Cm×n, vector b∈Cn, and parameters 0<ε<1.
An odd degree 2d+1polynomial given as its Chebyshev coefficients a2i+1(so that
p(x) =∑d
i=0a2i+1T2i+1(x)).
Pre-processing sketches: Lets,t=eO d6∥A∥2
F
ε2
.
P1.LetS∈ Cn×sbe a sampling matrix that samples scolumns of Asuch that the
j-th column is sampled with probability1
2
∥A∗,j∥2/∥A∥2
F+|bj|2/∥b∥2
.
P2. Let T∈ Ct×mbe a sampling matrix that samples tcolumns of ASsuch that the
i-th column is sampled with probability ∥[AS]i,∗∥2/∥AS∥2
F.
P3. Compute TAS .
9It is natural to wonder here why the complexity is not something like d∥A∥4
F/(ε/(d3log2(d)))2=
d7log2(d)∥A∥4
F/ε2. Such a running time is conceivable, but our analysis essentially replaces two factors of 1/εwith
factors of 1/ d2, so the sparsification only saves a factor of d2.
6Clenshaw iteration: Letr=eO 
d10∥A∥4
F/ε2
. Let vd+1=vd+2=⃗0s. For k∈[d, 0],
I1. Let B(k)∈Ct×sbe the estimator formed by sampling rentries of TAS such that
entry (i,j)is sampled with probability |[TAS]i,j|2/∥TAS∥2
F. Construct B(k)
†from
(TAS)†similarly.
I2. Compute vk=2(2B(k)
†B(k)−I)vk+1−vk+2+a2k+1S†b.
Output: Output x=1
2S(v0−v1)that satisfies ∥Ax−p(A)b∥⩽ε∥p∥sup∥b∥.
Sketching down the Clenshaw recurrence. For reasons explained below, we use the Clenshaw
recurrence instead of the Lanczos method. Given a degree- dpolynomial pgiven in terms of
its Chebyshev coefficients aℓ(i.e. p(x) =∑d
ℓ=0aℓTℓ(x), where Tℓ(x)is the degree ℓChebyshev
polynomial) and a value x∈[−1, 1], the Clenshaw recurrence computes p(x). We use a
modified recurrence for technical reasons. Concretely, our recurrence for odd p(so that aℓ=0
forℓeven), is the following:
q(d−1)/2=q(d+1)/2=0;
qk=2(2x2−1)qk+1−qk+2+2a2k+1x;
p(x) =1
2(q0−q1).
The scalar recurrences we discuss lift to computing matrix polynomials in a natural way:
u(d−1)/2=u(d+1)/2=0;
uk=2(2AA†−I)uk+1−uk+2+2a2k+1Ab;
p(A)b=1
2(u0−u1).
Each iteration (to get ukfrom uk+1and uk+2) can be performed in O(nnz(A))arithmetic opera-
tions, so this can be used to compute p(A)binO(dnnz(A))operations. We would like to do
this approximately in time independent of nnz(A)and n. We begin by sketching down our
matrix and vector: we show that it suffices to maintain a sparse description of ukof the form
uk=Avkwhere vkis sparse. In particular, we produce sketches S∈Cn×sand T∈Ct×msuch
that
1.∥AS(AS)†−AA†∥⩽ε;
2.∥ASS†b−Ab∥⩽ε∥b∥;
3.∥TAS(TAS)†−AS(AS)†∥⩽ε.
Sketches that satisfy the above type of guarantees are called approximate matrix product (AMP )
sketches, and are standard in the quantum-inspired algorithms literature [CGLLTW22]. In the
linear-time pre-processing phase, we can produce these sketches of size s,t=eO(∥A∥2
F
ε2log1
δ), and
then compute TAS . If the input is given in the quantum-inspired access model of oversampling
and query access , this can even be done in time independent of dimension. All of these guarantees
follow from Theorem 5.5, which shows ℓ2
2sampling gives an asymmetric approximate matrix
product property (in operator norm). We do not need this generalization (prior “symmetric”
results suffice), but we use it for convenience.
7Using these guarantees we can sketch the iterates as follows:
uk=2(2AA†−I)uk+1−uk+2+2a2k+1Ab
=4AA†Avk+1−2Avk+1−Avk+2+2a2k+1Ab
≈AS[4(TAS)†(TAS)vk+1−2vk+1−vk+2+2a2k+1S†b].(1)
Therefore, we can interpret Clenshaw iteration as the recursion on the dimension-independent
term vk≈4(TAS)†(TAS)vk+1−2vk+1−vk+2+2a2k+1S†b, and then applying ASon the left to
lift it back to mdimensional space. We can recognize this as roughly the recursion performed in
Algorithm 1. As desired, we can perform the iteration to produce vkinO(st) =eO(∥A∥4
F
ε4log21
δ)
time, which is independent of dimension, at the cost of incurring O(ε(∥vk+1∥+∥vk+2∥+
a2k+1∥b∥))error. To bound the effect of these per-iteration errors on the final output, we need a
stability analysis of the Clenshaw recurrence.
Connecting sketching error to finite precision. Given that we sketch each iterate down to a
size that is dimension-independent, we introduce additive error at each step. We can re-interpret
this additive error as truncation error, and forge a conceptual connection between running
iterative algorithms on sketches and stability analyses (also known as finite-precision analyses ) in
the classical NLA literature. Stability analyses of Clenshaw and Lanczos iteration are well-
understood [Cle55; Pai76; MMS18]. However, we cannot use them in a black-box manner, since
they are concerned with optimizing the “number of bits” required to maintain an accurate
solution, and so are content with error bounds that are only polynomially tight. Translating
these results to our setting results in a significantly worse polynomial running time. Therefore,
for our purposes, we must revisit classical stability analyses and refine our understanding of
how error propagates in these iterative methods.
Folklore intuition suggests that Lanczos is a stabler algorithm for applying matrix functions, but
the state-of-the-art analysis of it [MMS18] relies on the stability of the Clenshaw recurrence as a
subroutine, and therefore gives a strictly worse error-accumulation bounds than the Clenshaw
recurrence. In particular, if we wish to compute a generic p(A)btoε∥p∥sup∥b∥error in the
regime where every matrix-vector product Axincurs an error of ε∥A∥∥x∥using Lanczos, the
stability analysis of Musco, Musco, and Sidford suggests that the error of the output is O(d5.5ε),
which would introduce a d11in our setting [MMS18].10To incur less error, we do not use
Lanczos and analyze the Clenshaw recurrence directly.
This discussion will focus on standard Clenshaw instead of our modified version, since this is
the focus of prior work; the same discussion holds for our odd/even recurrences, but with an
additional factor of dincurred. The Clenshaw recurrence computes p(x)through the iteration
computing qdthrough to q0:
qd+1,qd+2:=0;
10This computation arises from taking Lemma 9 of [MMS18] to evaluate a degree- dpolynomial, say, bounded
by 1 in [−1, 1]. A generic example of such polynomial is only by a constant in [−1−η, 1+η]when η=O(1/d2)
(Lemma 4.3), and has Chebyshev coefficients bounded only by a constant, without decaying. Thus, the bound from
Lemma 9 becomes O(d5∥E∥). Continuing the analysis into [Pai76], Eis the matrix whose ith column is the error
incurred in the ith iteration; each column has norm ε∥A∥∥vj+1∥=εin our setting where we only incur error in
matrix-vector products, since ∥A∥=1by normalizing and ∥vj+1∥=1because the algorithm normalizes it to unit
norm, and we assume that scalar addition and multiplication can be performed exactly. We have no control over the
direction of error, so we can at best bound ∥E∥with the column-wise bound, giving ∥E∥⩽∥E∥F⩽√
kε. So, our
version of [MMS18, Equation 16] gives ∥E∥⩽√
dε, which gives us the asserted O(d5.5ε)bound.
8qk:=2xqk+1−qk+2+ak;
p(x) =1
2(a0+q0−q2).
For example, in the randomized numerical linear algebra (RNLA) literature, this is often applied
in the case where ad=1and ak=0otherwise, to evaluate a degree- dChebyshev polynomial
Td(x).
Now, we consider the Clenshaw recurrence when each iteration incurs error ε(|qk+1|+|qk+1|).
A naive argument gives a O(d3)bound on the overhead, which implies that εneeds to be scaled
down by that much to get the desired error bound. For our modified recurrence, this is a O(d4),
giving a time of O(d16∥A∥4
F
ε4log21
δ)to perform an iteration, and dtimes that for the total runtime.
We can hope for better. By the Markov brothers’ inequality, a bounded polynomial has derivative
bounded by d2[Sch41], and attained by the Chebyshev polynomial Td(x). So, if we only incur
error from an error in input, in that we perform the recurrence not with xbut some value in
(x−ε,x+ε), this error only cascades to a O(d2ε)worst-case error in the output. This argument
suggests that a O(d2)overhead is the best we could hope for.
Our technical contribution is an analysis showing that the Clenshaw algorithm gives this
optimal overhead, up to a logarithmic overhead. We first show that the overhead can be upper
bounded by the size of the largest Clenshaw iterate |qk|(see Theorem 8.1), and then we bound
the size of iterate |qk|byO(dlog(d)∥p∥sup)(see Theorem 8.4). The main lemma we prove states
that for a bounded polynomial p(x) =∑d
ℓ=0aℓTℓ(x), sums of the form aℓ+aℓ+2+· · ·are all
bounded by O(log(ℓ)∥p∥sup)(Fact 6.3). This statement follows from facts in Fourier analysis
(in particular, this log(ℓ)is the same logas the one occurs when bounding the L1norm of the
Dirichlet kernel).
To our knowledge, our work is the first to give any bound of o(d3). The standard literature
either considers an additive error (where, unlike usual models like floating-point arithmetic,
each multiplication incurs identical error regardless of magnitude) [Cle55; FP68; MH02] or
eventually boils down to bounding |ai|(since their main concern is dependence on x) [Oli77;
Oli79]. The modern work we are aware of shows a O(d2)bound only for computing Chebyshev
polynomials [BKM22], sometimes used to give a O(d2∑i|ai|) =O(d3)bound for computing
generic bounded polynomials [MMS18], since a degree- dpolynomial can be written as a linear
combination of Tk(x)with bounded coefficients. None of these analysis are sufficient to get
ourd2log(d)stability bound, since they ultimately depend on ∑|ai|, which can be as small as
∥p∥supand as large as√
d∥p∥sup; see the start of Section 8 for more discussion. Our improved
stability analysis saves a d4factor in our main algorithm, from d17tod13.
Improving the ε-dependence. So far, our improvements give a eO(d13∥A∥4
F
ε4log21
δ)running
time, whereas we wish to achieve a O(1/ε2)dependence. Though so far we have only used a
very limited subset of the sketching toolkit—namely, ℓ2
2importance sampling—it’s not clear
how, for example, oblivious sketches [NN13] or the connection between importance sampling
and leverage score sampling [CCHLW22] help us, since our choices of sketches are optimal up
to log factors for the guarantees we desire. To get the additional improvement, we need a new
sketching technique.
A natural next step to improve per-iteration running time is to sparsify the matrix TAS , in order
to make matrix-vector products more efficient. If we can sparsify TAS toO(1/ε2)non-zero
entries, then we get the desired quadratic savings in per-iteration cost. There is significant
9literature on sparsifying the entries of a matrix [AM07; KD14; BKKS21]. However, it does
not suffice for us to use these as a black box. For example, consider the sketch given by
Drineas and Zouzias [DZ10]: for a matrix M∈Rn×n, zero out every entry smaller than
ε
2n, then sample entries proportional to their ℓ2
2magnitude, and consider the corresponding
unbiased estimator of M, denoted ˜M. The guarantee is that the operator norms are close,M−˜M⩽ε, and the sparsity is O(nlog(n)∥A∥2
F
ε2). This is precisely the guarantee we need
and the sketch can be performed efficiently; however, this does not sparsify the matrix for us,
since in our setting, our matrices TAS have dimension ∥A∥2
F/ε2, so the sparsity guarantee is
onlyO(∥A∥4
Flog(n)
ε4) =O(stlog(n)). In other words, this sketch gives us no improvement on
sparsity!
Bi-linear entry-wise sampling transform. Our second main technical contribution is to bypass
this barrier by noticing that we don’t need a guarantee as strong as a spectral norm bound.
Instead, we only need to achieve approximations of the form
∥ASMv −AS˜Mx k∥⩽ε, (2)
for various different choices of xk. So, we only need our sketch ˜Mto approximate Min some
directions with good probability, instead of approximating it in all directions. We define a
simple sketch, which we call the Bilinear Entry Sampling Transform ( BEST11), that is an unbi-
ased estimator for Alinearly (rather than the product A†A) and achieves the aforementioned
guarantee (Equation (2)). This sketch is sampled from the same distribution as the one of
Drineas and Zouzias from above, but without the zero-ing out small entries. In particular, we
define
M(k)=1
pi,jAi,jeie†
j with probability pi,j=Ai,j
∥A∥2
F.
Then, to get the sketch with sparsity r, we take the average of rindependent copies of the above
random matrix:
BEST(A) =1
r∑
k∈[r]M(k).
This definition is not new: for example, one of the earliest papers on matrix sparsification for
linear algebra algorithms briefly considers this sketch [AM07], and this sketch has been used
implicitly in prior work on dequantizing QML algorithms [Tan21]. However, to the best of our
knowledge, our analysis of this sketch for preserving bi-linear forms and saving a factor of 1/ε2
is novel.
We show that BEST(A)satisfies the following guarantees: taking r=Ω(∥M∥2
F/ε2), this sketch
preserves the bilinear form u†Mvtoε∥u∥∥v∥error with probability ⩾0.9(Lemma 5.2). Second,
taking r=Ω(∥M∥2
Fn), this sketch preserves the norm ∥Mv∥to0.1∥v∥error with probability
⩾0.9. So, by taking r=Ω(∥M∥2
F(n+1
ε2)), we can get both properties. However, with this
choice of r,BEST(M)does not preserve the spectral norm ∥M∥, even to constant error.12This
sketch can be interpreted as a relaxation of the approximate matrix product and the sparsification
11That is, we pronounce BEST(A)as “best of A”. We make no normative claim about our sketch vis-a-vis other
sketches.
12At this point, one might wonder whether one can simply zero out small entries to get these guarantees with the
additional spectral norm bound. This is not the case; if we only zero out entries smaller than ε/(2n), this threshold
is too small to improve the matrix Bernstein tail bound, whereas if we zero out entries smaller than, say, 1/(100n),
the matrix Bernstein tail bound goes through, but the bilinear form is not ε-preserved.
10sketches above, since we use it in regimes where ∥BEST(A)∥is unbounded and ∥BEST(A)b∥
is not ε-close to ∥Ab∥. However, if we use it in the “interior” of a matrix product expression,
as a proxy for approximate matrix product, it can be used successfully to improve the n/ε2
dependence of spectral norm entry sparsification to something like n+1/ε2. In our setting, this
corresponds to an improvement of 1/ ε4to 1/ ε2.
Applying BEST to Clenshaw Iteration. Returning to our odd Clenshaw iteration, we had our
approximate iterate
uk≈AS[4(TAS)†(TAS)vk+1−2vk+1−vk+2+2a2k+1S†b].
We can approximate this by taking B=BEST(TAS)and B†=BEST((TAS)†)with sparsity
r=Θ(∥A∥4
F/ε2)to get that the bilinear forms like [AS]i,∗(TAS)†((TAS)vk+1)are preserved.
This allows us to successfully approximate with sparsified matrices,
uk≈AS[4B†Bvk+1−2vk+1−vk+2+2a2k+1S†b],
sovk≈4B†Bvk+1−2vk+1−vk+2+2a2k+1S†b.
This recurrence in vkwill be our algorithm (Algorithm 1): compute Sand Tin the pre-processing
phase, then compute the iteration of vk’s, pulling fresh copies of Band B†each time, and output
it as the description of the output u≈p(A)b. What remains is the error analysis, which similar
to the scalar case, requires bounding the size of the iterates ∥vk∥. We used the ε-approximation
of the bilinear form to show that the sparsifications Band B†successfully approximate uk; we
use the constant-error approximation of the norm to show that the ∥vk∥’s are bounded.
Challenges in extending finite-precision Clenshaw iteration. We note here that the above
error does not directly follow from the error of the sketches along with the finite-precision
scalar Clenshaw iteration. The error we incur in iteration kis not ε∥uk+1∥, but ε∥vk+1∥, so
our error analysis requires understanding both the original Clenshaw iteration along with the
“dual” Clenshaw iteration vk, which requires a separate error analysis. This dual iteration is
essentially the same Clenshaw iteration, but for computing p(x)/x, which bears resemblance to
how prior algorithms for SVT depend on the Lipschitz constant of p(x)/x[CGLLTW22]. That it
is evaluating p(x)/xmakes sense, since the dual iteration is the same as the Clenshaw iteration
but with a factor of Aremoved.
Sums of Chebyshev coefficients. One final technical wrinkle remains, which is to bound the
error accumulation of the matrix Clenshaw recurrences. In the same way that the original scalar
Clenshaw algorithm required bounding arithmetic progressions of Chebyshev coefficients with
step size two, aℓ+aℓ+2+· · ·by the norm of the corresponding polynomial p=∑d
ℓ=0aℓTℓ(x),
to prove stability for the even and odd versions, we need to bound arithmetic progressions with
step size four. Surprisingly, this is significantly more challenging.
We give a thorough explanation for this in Remark 6.8, but in brief, some arithmetic progressions
of Chebyshev coefficients arise naturally as linear combinations of polynomial evaluations. For
example, ∑k⩾0a2k=1
2(p(−1) +p(1)), so we can conclude that∑k⩾0a2k⩽∥p∥sup. Through
Fourier analysis, this can be generalized to progressions with different step sizes, but breaks
for progressions with certain offsets. The sum ∑k⩾0a4k+1is one such example, and this is a
quantity we need to bound to give a eO(d2)bound on the iterates of odd matrix Clenshaw. A
11naïve bound of O(d)follows from bounding each coefficient separately, but this results in a
significantly worse running time down the line.
In Lemma 6.4, we show that we can bound the above sum ∑k⩾0a4k+1byO(log2(d)∥p∥sup).
This shows that this sum has near-total cancellation: despite our guarantee on coefficient
being merely that it is bounded by a constant,13the sum of O(d)of these coefficients is only
poly-logarithmic in magnitude. The proof proceeds by considering many different polynomial
evaluations p(x0),p(x1),. . .,p(xd), and trying to write the arithmetic progression as a linear
combination of these evaluations ∑ckp(xk). This can be thought of as a linear system in the ck’s,
which we can then prove has a solution, and then bound the ℓ1norm of this solution. To do
this, we argue that we can bound its solution A−1bby the solution of a different system, C−1b,
where Cis an matrix that is entrywise larger than A. Since this is not true generally, we use
strong properties of the particular matrix Ain the linear system at hand to prove this.
3 Discussion
Comparison to [CCHLW22]. The work of Chepurko, Clarkson, Horesh, Lin, and Woodruff
[CCHLW22] gives algorithms for low-rank approximation sampling and ridge regression with
significantly different guarantees from that of prior quantum-inspired algorithms, since these
atypical guarantees are more amenable to existing sketching techniques. These do not match
QSVT in all regimes, but one could argue that for the purposes of the particular problems
targeted, they match them “in spirit” for practical regimes.
The guarantees obtained by [CCHLW22] differ from those obtained by QSVT since they do
not try to obtain an approximation of f(A)b, for some function f. They instead create data-
dependent sketches and solve the corresponding optimization problem in the sketched space.
Since the sketches preserve the cost, the solution in the sketched space has comparable cost
to the optimal solution ( f(A)b) in the original space. However, such a guarantee does not
immediately translate to a bound on the distance between the sketched solution and the optimal
solution. While leverage score and ridge leverage score sampling are the right primitives when
sketching a problem of optimizing an objective value, quantum algorithms extend more broadly
to general functions fand achieve different guarantees, about closeness to the output vector.
Related techniques in randomized numerical linear algebra. Our work draws upon several
ideas from the randomized numerical linear algebra literature. We refer the reader to the sur-
veys of Mahoney [Mah11] and Woodruff [Woo14] for the relevant background. The asymmetric
approximate matrix product sketch we introduce is closely related to the AMP sketches consid-
ered in [Mag11; MZ11; CEMMP15] (non-oblivious) and [CNW16] (oblivious). Our result about
asymmetric AMP has essentially been observed by Magen and Zouzias [MZ11], which samples
from the distributionn
∥Ai,∗∥∥Bi,∗∥/∑j∈[n]∥Aj,∗∥∥Bj,∗∥o
, which our distribution oversamples.
We could have used other AMP sketches as well, but we use importance sampling because
(1) we can prepare these sketches in linear-time, (2) we can prepare them in time independent
of dimension in the quantum-inspired setting, and (3) the sampling probabilities are optimal
when B=A†, as shown by Drineas, Kannan, and Mahoney [DKM06b].
Regarding the bi-linear entry-wise sampling transform, creating sketches by sampling entries
13In fact, for any degree d, there are polynomials of that degree which are bounded and yet ∑d
ℓ=0|aℓ|=Ω(d)
[Tre19, Theorems 8.2 and 8.3].
12proportional to their squared-magnitude are well-known in the literature but are typically used
to bound the operator norm of the matrix rather than any particular bi-linear form. Bounding the
operator norm directly happens to be too restrictive, as we discussed in the technical overview.
Regardless, entry-wise sampling sketches were introduced by Achlioptas and McSherry [AM07]
to speed up low-rank approximation. Arora, Hazan and Kale used them in the context of faster
algorithms for SDP solving [AHK06]. Since then, a series of works have provided sharper
guarantees for such sketches [GT09; DZ10; KD14; KDM17; BKKS21].
In addition to the usual sketching and sampling toolkit, we also use ideas from iterative algo-
rithms in the numerical linear algebra literature. Iterative methods, such as power iteration,
Golub-Kahan bidiagonalization, and Arnoldi iteration are ubiquitous in scientific computing
and are used for linear programming, low-rank approximation, and numerous other fundamen-
tal linear algebra primitives [Saa81; TB97]. Our work is closely related to iterative algorithms
that use the sparsification sketches and exploit singular values gaps [AL16; GS18; MM15;
BCW22], but this prior work incurs dimension-dependent factors, which are crucial to avoid in
our setting.
Speedups in quantum machine learning. In view of the broader goal of finding quantum
advantage for machine learning tasks, we spend some time here to point out when our assump-
tions do not hold, and therefore, dequantization results do not apply. First, quantum-inspired
linear algebra crucially relies on the input data being classical , meaning that, for example, we
are given input data as a list of entries, rather than as a quantum state, which does not have its
amplitudes easily accessible. Specifically, the weakest setting in which our algorithms work
is when we have oversampling and query access to input (Definition 4.5). This type of access
has extensibility properties similar to that of the block-encoding and we often get this access
whenever quantum states and generic block-encodings can be prepared efficiently [CGLLTW22,
Section 3]. However, as observed by Cotler, Huang, and McClean [CHM21], there are simple
problems for which having access to entries makes trivial a problem that is exponentially hard
when only given vectors via their corresponding states. A dequantized algorithm simply proves
that, if its quantum counterpart does achieve a, say, exponential speedup, then oversampling
and query access queries, which we need to run a dequantized algorithm, must be exponentially
hard to perform on a classical computer. This explains why quantum principal component
analysis has a dequantization [Tan21] when one has classical access to input as well as a proof
of exponential quantum advantage [Hua+22] when one does not. More generally, an algorithm
with a dequantization is still useful when run on “quantum data”.
Even given classical data, algorithms can resist dequantization by using high-degree sparsity-
based QSVT rather than low-rank-based QSVT, as techniques do not extend to this regime.
Further, sparsity-based QSVT is BQP-complete (indeed, quantum computation can be described
as applying block-encodings of low-sparsity unitary matrices), so we would not expect this to be
dequantized in full. In particular, we note that the current proposals that resist dequantization
and potentially obtain a super-polynomial quantum speedup include Zhao, Fitzsimons, and
Fitzsimons on Gaussian process regression [ZFF19] and Lloyd, Garnerone, and Zanardi on
topological data analysis [LGZ16]. Though these works avoid the dequantization barrier to
large quantum speedups, their potential for practical quantum speedups remains to be seen,
since the decision on whether the speedups are “practical” or “useful” will ultimately come
down to the particular choice of dataset and hardware.
Finally, even if a quantum algorithm can be simulated by a classical algorithm, there are
13certain problems for which having |v⟩is better than having the succinct representation of v.
For example, estimating the Forrelation [AA18; AC17] of |v⟩with another vector |u⟩requires
exponentially many queries to vwhen it is given as a classical list of entries, even with the
ability to produce importance samples. If the desired task is to output the Forrelation of an
output of low-rank QSVT, this could potentially give a large speedup despite our results on
low-rank QSVT. However, this is a case of artificially adding hardness: we are unaware of
problems in machine learning where Forrelation-type quantities are desired. Such a problem
would be a good candidate for QML speedup.
Open problems. The main question in this area remains: what kinds of quantum linear
algebra can we make convincing arguments of quantum speedup for? As our main result
highlights, there is currently a quartic gap in the ∥A∥Fdependence, which researchers believe
is large enough for near-term speedups [BMNGBN21]. Are there ways to attain this speedup
with more practical quantum algorithms?
Another question arising in our work is that of polynomial evaluation. We showed that
computing a polynomial p(x)with ε-noisy arithmetic operations can be done to O(d2ε)error.
Can we show a more fine-grained result: that, for p(x)bounded and L-Lipschitz, can we
compute p(x)toO(Lε)error?
With respect to the classical SVT algorithm, a natural question is to ask for an improved
algorithm for estimating Tr(p(A))to relative error, for p(x)a low-degree bounded polynomial
and Asymmetric. Using our result along with a Hutch-style estimate requires paying dimension
dependence, but prior work shows that it is possible to achieve a running time of O(nnz(A) +
poly(∥A∥F,d, 1/ε))with a high polynomial cost, at least when p(x)is an approximation to
ex[CGLLTW22]. Can we get an improved algorithm for matrix polynomial traces?
Finally, with respect to the techniques we introduce, we could ask for settings in which these
can be applied and improvements to our techniques. Can our matrix sparsification techniques
be applied to other settings?
4 Preliminaries
We use the notation f≲gto denote the ordering f=O(g)(and respectively for ≳and≂), and
eO(f)is shorthand for O(fpoly(logf)). We use logto refer to the natural logarithm. We use
the Iverson bracket, where JPKis one if the predicate Pis true and zero otherwise. For example,
∑d
i=1∑d
j=iaij=∑d
i=1∑d
j=1aijJj⩾iK. Finally, we assume that arithmetic operations (e.g., addition
and multiplication of real numbers) and function evaluation oracles (computing f(x)from x)
take unit time.
4.1 Linear Algebra
For a vector v∈Cn, the standard Euclidean norm of vis denoted ∥v∥:= (∑n
i=1|vi|2)1/2. The
number of non-zero entries of vis denoted ∥z∥0. For a matrix A∈Cm×n, the Frobenius norm of
Ais∥A∥F:= (∑m
i=1∑n
j=1Ai,j2)1/2and the spectral norm ofAis∥A∥:=supx∈ Cn,∥x∥=1∥Ax∥. The
i-th row and j-th column of Aare denoted Ai,∗and A∗,j, respectively. A†denotes the conjugate
transpose ofA, and A+denotes the Moore-Penrose pseudoinverse ofA.
Asingular value decomposition (SVD) of Ais a representation A=UDV†, where for N:=
14min(m,n),U∈ Cm×Nand V∈ Cn×Nare isometries and D∈ RN×Nis diagonal with σi:=Di,i
andσ1⩾σ2⩾· · ·⩾σN⩾0. We can also write this decomposition as A=∑N
i=1σiU∗,iV†
∗,i.
We denote the set of singular values of AbySpec(A):={σk}k∈[N]. The rank ofAis the number
of nonzero singular values and the stable rank ofAis∥A∥2
F/∥A∥2.
For a Hermitian matrix A∈ Cn×nand a function f: R→ C,f(A)∈ Cn×ndenotes the matrix
where fis applied to the eigenvalues of A. Below, in Definition 4.1, we define notions of matrix
function that extend to non-Hermitian matrices.
4.2 Polynomials and the Chebyshev Basis
We consider polynomials with real coefficients, p∈ R[x]. For a Hermitian matrix A,p(A)
refers to evaluating the polynomial with xreplacing A; this is equivalent to applying pto the
eigenvalues of A. The right definition for applying pto a general non-square matrix is subtle; as
done in QSVT, we restrict to settings where the matrix formed by evaluating pon the singular
values of Acoincides with the evaluation of a corresponding polynomial in A.
Definition 4.1 (Definition 6.1 of [CGLLTW22]) .For a matrix A∈ Cm×nand degree- dpolynomial
p(x)∈ R[x]of parity- d(i.e., even if dis even and odd if dis odd), we define the notation p(A)
in the following way:
1. If piseven, meaning that we can express p(x) =q(x2)for some polynomial q(x), then
p(A):=q(A†A) =p(√
A†A).
2. If pisodd, meaning that we can express p(x) =x·q(x2)for some polynomial q(x), then
p(A):=A·q(√
A†A).
For example, if p(x) = x2+1, then p(A) = A†A+I, and if p(x) = x3+x, then p(A) =
AA†A+A. Looking at a singular value decomposition A=∑σiU∗,iV†
∗,i,p(A) =∑p(σi)U∗,iV†
∗,i
when pis odd and p(A) =∑p(σi)V∗,iV†
∗,iwhen pis even, thus making this definition coincide
with the singular value transformation as given in [GSLW19, Definition 16].
We work in the Chebyshev basis of polynomials throughout. Let Tℓ(x)and Uℓ(x)denote the
degree- ℓChebyshev polynomials of the first and second kind, respectively. They can be defined
on[−1, 1]via
Tℓ(cos(θ)) = cos(ℓθ)and (3)
Uℓ(cos(θ)) = sin((ℓ+1)x)/ sin(x), (4)
but we will give attention to their recursive definitions, since we use them for computation.
T0(x) =1 U0(x) =1
T1(x) =x U 1(x) =2x (5)
Tk(x) =2x·Tk−1(x)−Tk−2(x) Uk(x) =2x·Uk−1(x)−Uk−2(x)
For a function f:[−1, 1]→R, we denote ∥f∥sup:=supx∈[−1,1]|f(x)|. In this norm, the
Chebyshev polynomials have ∥Tk(x)∥sup=1and∥Uk(x)∥sup=n+1. More generally, for a
function f:S→RforS⊂R, we denote ∥f∥S:=supx∈S|f(x)|, so that ∥f∥sup=∥f∥[−1,1].
15We use the following well-known properties of Chebyshev polynomials from Mason and
Handscomb [MH02].
Ti(x) =1
2(Ui(x)−Ui−2(x))fori⩾1 (6)
Ui(x) =∑
j⩾0Ti−2j(x)(1+Ji−2j̸=0K) (7)
Tjk(x) =Tj(Tk(x)) (8)
U2k+1(x) =Uk(T2(x))U1(x) =Uk(T2(x))2x (9)
d
dxTk(x) =kUk−1(x) (10)
Any Lipschitz continuous function14f:[−1, 1]→Rcan be written as a (unique) linear
combination of Chebyshev polynomials, f(x) =∑ℓaℓTℓ(x)(where we interpret Tℓ(x)≡0
for negative ℓ). When fis a degree- dpolynomial, then aℓ=0for all ℓ > d. A common way
to approximate a function is by truncating its Chebyshev series expansion; we denote this
operation by fk(x):=∑k
ℓ=0aℓTℓ(x), and we denote the remainder to be ¯fk(x):=f(x)−fk(x) =
∑∞
ℓ=k+1aℓTℓ(x). Standard results in approximation give bounds on ∥f−fk∥supfor various
smoothness assumptions on f. We recommend the book by Trefethen on this topic [Tre19], and
use results from it throughout. We list here some basic lemmas for future use.
Lemma 4.2 (Coefficient bound, consequence of [Tre19, Eq. (3.12)]) .Letf:[−1, 1]→Rbe a
Lipschitz continuous function. Then all its Chebyshev coefficients a kare bounded: |ak|⩽2∥f∥sup.
Lemma 4.3. For a degree-d polynomial p, and δ=1
4d2,
∥p∥[−1−δ,1+δ]⩽e∥p∥[−1,1].
Proof. Without loss of generality, take ∥p∥sup=1. By [SV14, Proposition 2.4] and basic proper-
ties of Chebyshev polynomials,
∥p(x)∥[−1−δ,1+δ]⩽∥Td(x)∥[−1−δ,1+δ]=Td(1+δ).
Further, by Proposition 2.5 in [SV14], we can evaluate Td(1+δ)via the formula
Td(x) =1
2
x+p
x2−1d
+1
2
x−p
x2−1d
Td(1+δ) =1
2
1+δ+p
2δ+δ2d
+1
2
1+δ−p
2δ+δ2d
⩽exp
d(δ+p
2δ+δ2)
⩽exp
1
4d+q
1
2+1
16d2
⩽e.
4.3 Sampling and Query Access
We now introduce the “quantum-inspired” access model, following the exposition from prior
work [CGLLTW22]. We refer the reader there for a more thorough investigation of this access
model. From a sketching perspective, this model encompasses “the set of algorithms that can
be performed in time independent of input dimension, using only ℓ2
2sampling”, and is a decent
classical analogue for the input given to a quantum machine learning algorithms operating on
classical data.
14We call a function f:[−1, 1]→RLipschitz continuous if there exists a constant Csuch that |f(x)−f(y)|⩽
C|x−y|forx,y∈[−1, 1].
16Definition 4.4 (Sampling and query access to a vector, [CGLLTW22, Definition 3.2]) .For a
vector v∈ Cn, we have SQ (v),sampling and query access tov, if we can:
1. query for entries of v;
2. obtain independent samples s∈[n]where the probability that sis some iis|vi|2/∥v∥2;
3. query for ∥v∥.
We will use our pre-processing time to construct data structures that give us sampling and
query access to our input. The samples from SQ(v)are called “ ℓ2
2importance samples” in the
randomized numerical linear algebra literature; we will call them samples from v. Such samples
are equivalent to measurements of the quantum state |v⟩:=1
∥v∥∑vi|i⟩in the computational
basis. Sampling and query access is closed under taking linear combinations, once we introduce
slack in the form of oversampling.
Definition 4.5 (Oversampling and query access, [CGLLTW22, Definition 3.4]) .Forv∈ Cnand
ϕ⩾1, we have SQϕ(v),ϕ-oversampling and query access tov, if we can query for entries of v
and we have SQ(˜v), where ˜v∈ Cnis a vector satisfying ∥˜v∥2=ϕ∥v∥2and|˜vi|2⩾|vi|2for all
i∈[n].
Theℓ2
2distribution over ˜vϕ-oversamples the distribution over v:
|˜vi|2
∥˜v∥2=|˜vi|2
ϕ∥v∥2⩾1
ϕ|vi|2
∥v∥2.
Intuitively speaking, as a consequence, estimators that use samples from vcan also use samples
from ˜vat the expense of a factor ϕincrease in the number of samples used. Formally, we can
prove that oversampling access implies an approximate version of the usual sampling access:
Lemma 4.6 (Oversampling to sampling, [CGLLTW22, Lemma 3.5]) .Foru∈Cn, suppose we
are given SQϕ(u)and some δ∈(0, 1]. We can sample from uwith probability ⩾1−δinO(ϕlog1
δ)
queries to SQϕ(u). We can also estimate ∥u∥toνmultiplicative error for ν∈(0, 1]with probability
⩾1−δinO(ϕ
ν2log1
δ)queries. Both of these algorithms take linear time in the number of queries.
Generally, compared to a quantum algorithm that can output (and measure) a desired vector |v⟩,
our algorithms will output SQϕ(u)such that ∥u−v∥is small. Our analysis will give a bound on
ϕto show that we can output samples from |v⟩. As for error, a bound ∥u−v∥⩽ε∥v∥implies
that measurements from uand measurements from vare2ε-close in total variation distance
[Tan19, Lemma 4.1]. Now, we show that oversampling and query access of vectors is closed
under taking small linear combinations.
Lemma 4.7 (Linear combinations, [CGLLTW22, Lemma 3.6]) .Given SQφt(v(t))∈ Cnandλt∈ C
for all t∈[τ], we have SQϕ(∑τ
t=1λtv(t))forϕ=τ∑φt∥λtv(t)∥2
∥∑λtv(t)∥2. After paying the pre-processing cost of
querying for each of the norms of the ˜v(t)’s, the cost of any query is equal to the cost of sampling from any
of the v(t)’s plus the cost of querying an entry from all of the v(t)’s.
We can also define (over)sampling and query access for a matrix Aas having sampling and
query access to all of the rows of A, along with the vector of row norms of A.
Definition 4.8 (Oversampling and query access to a matrix, [CGLLTW22, Definition 3.7]) .For a
matrix A∈ Cm×n, we have SQ(A)if we have SQ(Ai,∗)for all i∈[m]and SQ(a)fora∈ Rmthe
vector of row norms ( ai:=∥Ai,∗∥).
17We have SQϕ(A)if we can query entries of Aand we have SQ(˜A)for ˜A∈ Cm×nsatisfying
∥˜A∥2
F=ϕ∥A∥2
Fand˜Ai,j2⩾Ai,j2for all (i,j)∈[m]×[n].
Sampling and query access is relevant to QML because many settings where we can apply QML
to classical data (that is, data given as a list of entries, with some allowed pre-processing or in
some data structure) also admits efficient SQaccess to input. So, we concern ourselves with the
setting that we can perform SQ queries in, say, O(1)time.
Remark 4.9. We can get SQ access to input matrices and vectors in input-sparsity time. Given
v∈ Cnin the standard RAM model, the alias method [Vos91] takes Θ(nnz(v))pre-processing
time to output a data structure that uses Θ(nnz(v))space and can sample from vinΘ(1)
time. In other words, we can get SQ(v)with constant-time queries in O(nnz(v))time, and by
extension, for a matrix A∈ Cm×n, SQ(A)with constant-time queries in O(nnz(v))time.15
The algorithms presented here will take linear-time pre-processing to construct the above data
structure, among other things. However, they will still run in time independent of dimension
without this pre-processing, supposing that we have efficient SQaccess to input. Finally, we
synthesize the prior results on linear combinations into the following corollary, which shows
that a certain type of succinct description of a vector uis sufficient to get approximate sampling
and query access to it.
Corollary 4.10. Suppose we are given sampling and query access to a matrix A∈ Cm×nand a vector
b∈ Cn, where we can respond to queries in O(1)time. Further suppose we have a vector u∈Cn
implicitly represented by v ∈ Cmandη, with u =A†v+ηb. Then we can:
(i) Compute entries of u in O(∥v∥0)time;
(ii) Sample i ∈[n]with probability|ui|2
∥u∥2inO
∥v∥0(∥v∥0∑k∥vkAk,∗∥2+η2∥b∥2)1
∥u∥2log1
δ
time
with probability ⩾1−δ;
(iii) Estimate ∥u∥2toνrelative error in O
∥v∥0(∥v∥0∑k∥vkAk,∗∥2+η2∥b∥2)1
ν2∥u∥2log1
δ
time
with probability ⩾1−δ.
Proof. By Lemma 4.7, we have SQ ϕ(A†v)for
ϕ=∥v∥0∑k∥vkAk,∗∥2
∥A†v∥2
and a query cost of O(∥v∥0). Applying Lemma 4.7 again, we have SQ φ(A†v+ηb)for
φ=2∥v∥0∑k∥vkAk,∗∥2+η2∥b∥2
∥A†v+ηb∥2.
By Lemma 4.6, we can draw one sample from uwith probability ⩾1−δwithO(φlog1
δ)
queries to SQϕ(A†v), each of which takes O(∥v∥0)time. Similarly, we can estimate ∥u∥2toν
multiplicative error with O(φ
ν2log1
δ)queries to SQ ϕ(A†v).
For intuition, the running times above are only large when u=A†v+ηbhas significantly
smaller norm than the magnitude of the summands vi(Ai,∗)†would suggest. Usually, in our
applications, we can intuitively think about this overhead being small when the desired output
15This holds in the word-RAM model, with an additional log overhead when considering bit complexity.
18vector mostly lies in a subspace spanned by singular vectors with large singular values in
our low-rank input. Quantum algorithms also have the same kind of overhead. Namely, the
QSVT framework encodes this in the subnormalization constant αof block-encodings, and
the overhead from the subnormalization appears during post-selection [GSLW19]. When this
cancellation is not too large, the resulting overhead typically does not affect too badly the
runtime of our applications.
5 Extending the Sketching Toolkit
In this section, we show how to extend the modern sketching toolkit (see e.g. [Woo14]) in two
ways: (a) we provide a sub-sampling sketch that preserves bi-linear forms with only a inverse
quadratic dependence on εand (b) a non-oblivious, ℓ2
2sampling based asymmetric approximate
matrix product sketch.
5.1 The Bi-Linear Entry-wise Sampling Transform
Definition 5.1 (Bi-linear Entry-wise Sparsifying Transform) .For a matrix A∈ Cm×n, the BEST
ofAwith parameter Tis a matrix sampled as follows: for all k∈[T],
M(k)=1
pi,jAi,jeie†
j with probability pi,j=|Ai,j|2
∥A∥2
F.
Then,
BEST T(A) =1
T∑
k∈[T]M(k).
Lemma 5.2 (Basic properties of the Bi-Linear Entry-wise Sparsifying Transform) .For a matrix
A∈ Cm×n, letM=BEST(A)with parameter T. Then, for X∈ Cm×m,u∈ Cmandv∈ Cn, we have
nnz(M)⩽T; (11)
E[M]=A; (12)
Eh
M†XM−A†XAi
=1
T
Tr(X)∥A∥2
FI−A†XA
. (13)
Proof. Observe, since Mis an average of Tsub-samples, each of which are 1-sparse, Mhas at
most Tnon-zero entries. Next,
E[M]=1
T∑
k∈TE[M(k)] =∑
i∈[m]∑
j∈[n]pi,jAi,j
pi,jeie†
j=A.
Similarly,
Eh
M†XMi
=1
T2E"
∑
k∈[T]M(k)†
X
∑
k∈[T]M(k)#
=1
T2E"
∑
k,k′∈[T](M(k))†XM(k′)#
=1
T2
∑
k̸=k′∈[T]E[M(k)]†·X·E[M(k′)]
+
∑
k∈[T]E[(M(k))†XM(k)]
19=
1−1
T
A†XA+1
T∑
i∈[m],j∈[n]pi,jA2
i,j
p2
i,jeje†
iXeie†
j
=
1−1
T
A†XA+∥A∥2
F
T∑
i∈[m],j∈[n]Xi,ieje†
j
=
1−1
T
A†XA+∥A∥2
FTr(X)
TI.
We list a simple consequence of these bounds that we use later.
Corollary 5.3. For a matrix A∈ Cm×n, let M=BEST(A)with parameter T. Then, for matrices
X∈ Cℓ×mand Y∈ Cn×d,
Prh
∥XMY−XAY∥F⩾∥X∥F∥A∥F∥Y∥F√
δTi
⩽δ.
5.2 Approximate Matrix Products from ℓ2
2Importance Sampling
In this subsection, we extend the well-known approximate matrix product to the setting where
we have an ℓ2
2-sampling oracle [Woo14]. The approximate matrix product guarantee is typically
achieved in the oblivious sketching model [CNW16], which we cannot extend to the quantum
setting. Earlier work [DKM06a] considers achieving this guarantee through row subsampling,
which can be performed in this setting.
Definition 5.4. Given two matrices A∈Cm×nand B∈Cd×n, along with a probability distribu-
tion p∈Rn
⩾0, we define the Asymmetric Approximate Matrix Product of sketch size s, denoted
AMP s(A,B†,p), to be the n×smatrix whose columns are i.i.d. sampled according to the law
[AMP s(A,B†,p)]∗,j=ek√s·pkwith probability pk
For an S=AMP s(A,B†,p), we will typically consider the expression ASS†B†, which can be
written as the sum of independent rank-one matrices1
s·pkA∗,kB∗,k. Notice that E[ASS†B†] =AB†.
We show that AMP s(A,B†,p)is a good approximate matrix product sketch for AB†when the
distribution pis a mixture of the row sampling distributions for Aand B.
Theorem 5.5 (Asymmetric Approximate Matrix Multiplication) .Given matrices A∈ Cm×nand
B∈ Cn×d, consider S=AMP s(A,B†,p)forp∈Rn
⩾0with pi⩾1
2ϕ(∥A∗,i∥2
∥A∥2
F+∥B∗,i∥2
∥B∥2
F)for some ϕ⩾1.
Letsr=∥A∥2
F
∥A∥2+∥B∥2
F
∥B∥2. Then, with probability at least 1−δ>0.75,
∥ASS†B†−AB†∥⩽r
2
slogsr
δ
ϕ
∥A∥2
F∥B∥2+∥A∥2∥B∥2
F
+1
slogsr
δ
ϕ∥A∥F∥B∥F.
We will use the following consequence of this theorem.
Corollary 5.6. Given matrices A∈ Cm×nandB∈ Cn×d, consider S=AMP s(A,B†,p)forp∈Rn
⩾0
with pi⩾1
2ϕ(∥A∗,i∥2
∥A∥2
F+∥B∗,i∥2
∥B∥2
F)for some ϕ⩾1. For ε∈(0, 1]andδ∈(0, 0.25 ], when s=4ϕ
ε2(∥A∥2
F
∥A∥2+
∥B∥2
F
∥B∥2)log(1
δ(∥A∥2
F
∥A∥2+∥B∥2
F
∥B∥2)), then∥ASS†B†−AB†∥⩽ε∥A∥∥B∥with probability ⩾1−δ.
20The symmetric version of this result was previously stated in [KV17; RV07], and this asymmetric
version was stated in [MZ11]. However, the final theorem statement was weaker, so we reprove
it here. To obtain it, we prove the following key lemma:
Lemma 5.7 (Concentration of asymmetric random outer products) .Let{(Xi,Yi)}i∈[s]besin-
dependent copies of the tuple of random vectors (X,Y), with X∈ CmandY∈ Cd. In particular,
(X,Y) = ( a(i),b(i))with probability p ifor i∈[n]. Let M ,L⩾0be such that
L⩾max
i∈[n]∥a(i)(b(i))†∥,
M2⩾max
i∈[n]∥b(i)∥2∥E[XX†]∥+max
i∈[n]∥a(i)∥2∥E[YY†]∥,
sr⩾maxi∈[n]∥b(i)∥2E[∥X∥2
F] +maxi∈[n]∥a(i)∥2E[∥Y∥2
F]
max(maxi∈[n]∥b(i)∥2∥E[XX†]∥, maxi∈[n]∥a(i)∥2∥E[YY†]∥).
Then, for any t ⩾M/√s+2L/(3s),
Prh1
s∑
i∈[s]XiY†
i−E[XY†]⩾ti
⩽4 sr exp
−st2
2(M2+Lt)
.
Proof. Fori∈[s], let Zi=1
s 
XiY†
i−E[XY†]
. Then ∥Zi∥⩽2
sXiY†
i⩽2L
s. Next, we bound
the variance,
σ2:=max ∑
i∈[n]E[ZiZ†
i]
|{z }
(i),∑
i∈[n]E[Z†
iZi]
|{z }
(ii)!
.
We can observe that
∑
i∈[s]E[ZiZ†
i] =1
sEh
(XiY†
i−E[XY†])(XiY†
i−E[XY†])†i
=1
sEh
∥Yi∥2XiX†
i−E[XY†]E[YX†]i
⪯1
s(max
i∈[n]∥b(i)∥2)E[XX†] =:V1
∑
i∈[s]E[Z†
iZi] =1
sEh
(XiY†
i−E[XY†])†(XiY†
i−E[XY†])i
=1
sEh
∥Xi∥2YiY†
i−E[YX†]E[XY†]i
⪯1
s(max
i∈[n]∥a(i)∥2)E[YY†] =:V2
We can use this to bound term ( i):
∑
i∈[s]E[ZiZ†
i]⩽1
sE[∥Yi∥2XiX†
i]⩽1
s(max
i∈[n]∥b(i)∥2)∥E[XX†]∥.
We bound term ( ii) as follows:
∑
i∈[s]E[Z†
iZi]⩽1
sE[∥Xi∥2YiY†
i]⩽1
s(max
i∈[n]∥a(i)∥2)∥E[YY†]∥.
21Altogether, we have shown that σ2⩽M2/s. Applying Matrix Bernstein (see Fact 5.8) with
upper bounds of V1and V2and parameters L←2L
sand v←M2/s, we get
Pr"1
s∑
i∈[s]XiY†
i−E[XY†]⩾t#
=Pr"∑
i∈[s]Zi⩾t#
⩽4 sr exp
−t2/2
M2/s+2Lt/(3s)
⩽4 sr exp
−st2
2(M2+Lt)
,
where
sr=Tr(V1) +Tr(V2)
max(∥V1∥,∥V2∥)=maxi∈[n]∥b(i)∥2E[∥X∥2
F] +maxi∈[n]∥a(i)∥2E[∥Y∥2
F]
max(maxi∈[n]∥b(i)∥2∥E[XX†]∥, maxi∈[n]∥a(i)∥2∥E[YY†]∥).
Fact 5.8 (Intrinsic Matrix Bernstein, [Tro15, Theorem 7.3.1]) .Consider a finite sequence {Zk}k∈[s]
of random complex matrices with the same size, and assume that E[Zk] =0and∥Zk∥⩽L. Let V1and
V2be semidefinite upper bounds for the corresponding matrix-valued variances:
V1⪰E[s
∑
k=1ZkZ†
k]; V2⪰E[s
∑
k=1Z†
kZk].
Define an intrinsic dimension bound and a variance bound,
sr=Tr(V1+V2)
max(∥V1∥,∥V2∥); v=max{∥V1∥,∥V2∥}.
Then, for t ⩾√v+L/3,
Pr"∑
i∈[k]Zi⩾t#
⩽4 sr exp
−t2/2
v+Lt/3
.
It is now straight-forward to prove Theorem 5.5 using the aforementioned lemma:
Proof of Theorem 5.5. We apply Lemma 5.7 with a(i)=p
1/pi·A∗,iand b(i)=p
1/pi·B∗,i. As
assumed the sampling distribution pisatisfies pi⩾1
2ϕ(∥A∗,i∥2
∥A∥2
F+∥B∗,i∥2
∥B∥2
F)⩾∥A∗,i∥∥B∗,i∥
ϕ∥A∥F∥B∥F, so
∥a(i)∥=∥A∗,i∥/√pi⩽∥A∗,i∥s
2ϕ∥A∥2
F
∥A∗,i∥2=p
2ϕ∥A∥F;
∥b(i)∥=∥B∗,i∥/√pi⩽∥B∗,i∥s
2ϕ∥B∥2
F
∥B∗,i∥2=p
2ϕ∥B∥F;
∥a(i)(b(i))†∥=∥A∗,i∥∥B∗,i∥
pi⩽ϕ∥A∥F∥B∥F.
Further,
∥E[XX†]∥=∑
i∈[n]A∗,iA†
∗,i=∥AA†∥=∥A∥2;
∥E[YY†]∥=∑
i∈[n]B∗,iB∗,i=∥BB†∥=∥B∥2.
22Finally,
maxi∈[n]∥b(i)∥2E[∥X∥2
F] +maxi∈[n]∥a(i)∥2E[∥Y∥2
F]
max(maxi∈[n]∥b(i)∥2∥E[XX†]∥, maxi∈[n]∥a(i)∥2∥E[YY†]∥)⩽E[∥X∥2
F]
∥E[XX†]∥+E[∥Y∥2
F]
∥E[YY†]∥
=∥A∥2
F
∥A∥2+∥B∥2
F
∥B∥2.
So, in Lemma 5.7, we can set L=ϕ∥A∥F∥B∥F,M2=2ϕ∥A∥2
F∥B∥2+2ϕ∥B∥2
F∥A∥2, and sr=
∥A∥2
F
∥A∥2+∥B∥2
F
∥B∥2to get that, for all t⩾M/√s+2L/(3s),
Pr"1
s∑
i∈[s]XiY†
i−E[XY†]⩾t#
⩽4∥A∥2
F
∥A∥+∥B∥2
F
∥B∥
exp−st2
2ϕ(∥A∥2
F∥B∥2+∥A∥2∥B∥2
F) +ϕ∥A∥F∥B∥Ft
.
To get the right-hand side of the above equation to be ⩽δ, it suffices to choose
t=r
2
slogsr
δ
ϕ
∥A∥2
F∥B∥2+∥A∥2∥B∥2
F
+1
slogsr
δ
ϕ∥A∥F∥B∥F.
This choice of tis greater than M/√s+2L/(3s)when δ<1/e, so with this assumption, we
can conclude that with probability ⩾1−δ,
∥ASS†B†−AB†∥⩽r
2
slogsr
δ
ϕ
∥A∥2
F∥B∥2+∥A∥2∥B∥2
F
+1
slogsr
δ
ϕ∥A∥F∥B∥F.
6 Sums of Chebyshev Coefficients
To give improved stability bounds for the Clenshaw recurrence, we need to bound various sums
of Chebyshev coefficients. Since we aim to give bounds that hold for all degree- dpolynomials,
we use no property of the function beyond that it has a unique Chebyshev expansion; of course,
for any particular choice of function f, the bounds in this section can be improved by explicitly
computing its Chebyshev coefficients, or in some cases, by using smoothness properties of the
function [Tre19, Theorems 7.2 and 8.2].
Let f:[−1, 1]→Rbe a Lipschitz continuous function. Then it can be expressed uniquely
as a linear combination of Chebyshev polynomials f(x) =∑∞
i=0aiTi(x). A broad topic of
interest in approximation theory is bounds for linear combinations of these coefficients, ∑aici,
in terms of ∥f∥sup; this was one motivation of Vladimir Markov in proving the Markov brothers’
inequality [Sch41, p575]. Our goal for this section will be to investigate this question in the
case where these sums are arithmetic progressions of step four. This will be necessary for later
stability analyses, and is one of the first non-trivial progressions to bound. We begin with some
straightforward assertions (see [Tre19] for background).
Fact 6.1. Letf:[−1, 1]→Rbe a Lipschitz continuous function. Then its Chebyshev coefficients {aℓ}ℓ
23satisfy the following bounds.
∑
ℓaℓ=|f(1)|⩽∥f∥sup
∑
ℓ(−1)ℓaℓ=|f(−1)|⩽∥f∥sup
∑
ℓaℓJℓis even K=∑
ℓaℓ1
2(1+ (−1)ℓ)⩽∥f∥sup
∑
ℓaℓJℓis odd K=∑
ℓaℓ1
2(1−(−1)ℓ)⩽∥f∥sup
We use the following result on Lebesgue constants to bound truncations of the Chebyshev
coefficient sums.
Lemma 6.2 ([Tre19, Theorem 15.3]) .Letf:[−1, 1]→Rbe a Lipschitz continuous function, let
fk(x) =∑k
ℓ=0aℓTℓ(x), and let the optimal degree- kapproximating polynomial to fbe denoted f∗
k. Then
∥f−fk∥sup⩽
4+4
π2log(k+1)
∥f−f∗
k∥sup
⩽
4+4
π2log(k+1)
∥f∥sup.
Similarly,
∥fk∥sup⩽∥f−fk∥sup+∥f∥sup⩽
5+4
π2log(k+1)
∥f∥sup.
This implies bounds on sums of coefficients.
Fact 6.3. Consider a function f (x) =∑ℓaℓTℓ(x). Then
∞
∑
ℓ=kaℓJℓ−k is even K⩽∥f−fk−1∥sup⩽
4+4
π2log(k)
∥f∥sup,
∞
∑
ℓ=kaℓ(−1)ℓ⩽∥f−fk−1∥sup⩽
4+4
π2log(k)
∥f∥sup,
where the inequalities follow from Fact 6.1 and Lemma 6.2. When k=0, then the sum is bounded by
∥f∥sup, as shown in Fact 6.1.
Now, we prove similar bounds in the case that f(x)is an odd function. In particular, we want
to obtain a bound on alternating signed sums of the Chebshyev coefficients and we incur a
blowup that scales logarithmically in the degree.
Lemma 6.4. Letf:[−1, 1]→Rbe an odd Lipschitz continuous function with Chebyshev coefficients
{aℓ}ℓ, so that a k=0for all even k. Then the Chebyshev coefficient sum is bounded as
d
∑
ℓ=0(−1)ℓa2ℓ+1⩽(log(d) +2)max
0⩽k⩽2d+1∥fk∥sup
⩽(log(d) +2)
5+4
π2log(2d+2)
∥f∥sup
⩽
16+4 log2(d+1)
∥f∥sup.
We first state the following relatively straight-forward corollary:
24Corollary 6.5. Lemma 6.4 gives bounds on arithmetic progressions with step size four. Let f:[−1, 1]→
Rbe a Lipschitz continuous function, and consider non-negative integers c ⩽d. Then
d
∑
ℓ=caℓJℓ−c≡0(mod 4 )K⩽(32+8 log2(d+1))∥f∥sup.
Proof. Define fodd:=1
2(f(x)−f(−x))and feven:=1
2(f(x) +f(−x))to be the odd and even
parts of frespectively. Triangle inequality implies that ∥fodd∥sup,∥feven∥sup⩽∥f∥sup. Suppose
c,dare odd. Then
d
∑
ℓ=caℓJℓ−c≡0(mod 4 )K
=1
2⌊(d−c)/2⌋
∑
ℓ=0ac+2ℓ(1±(−1)ℓ)
⩽1
2⌊(d−c)/2⌋
∑
ℓ=0ac+2ℓ+⌊(d−c)/2⌋
∑
ℓ=0(−1)ℓac+2ℓ
=1
2fodd
d(1)−fodd
c−2(1)+(d−1)/2
∑
ℓ=0(−1)ℓa2ℓ+1−(c−3)/2
∑
ℓ=0(−1)ℓa2ℓ+1
⩽1
2
∥fodd
c−2∥sup+∥fodd
d∥sup+2(log(d) +2)max
0⩽k⩽d∥fodd
k∥sup
⩽(32+8 log2(d+1))∥fodd∥sup
⩽(32+8 log2(d+1))∥f∥sup.
The case when cis even is easier: by Eq. (8), we know that
∑
ℓa2ℓTℓ(x)
sup=∑
ℓa2ℓTℓ(T2(x))
sup=∑
ℓa2ℓT2ℓ(x)
sup=feven(x)
sup⩽∥f∥sup,
so by Fact 6.3,
∑
ℓ⩾caℓJℓ−c≡0(mod 4 )K=∑
ℓ⩾c/2a2ℓJℓ−c/2 is even K
⩽
4+4
π2log(c/2−1)∑
ℓa2ℓTℓ(x)
sup
⩽
4+4
π2log(c/2−1)
∥f∥sup. (14)
From the above, we can bound the type of sums in the problem statement, paying an additional
factor of two:
d
∑
ℓ=caℓJℓ−c≡0(mod 4 )K⩽∑
ℓ⩾caℓJℓ−c≡0(mod 4 )K+∑
ℓ⩾d+1aℓJℓ−c≡0(mod 4 )K
⩽
8+4
π2(log(c/2−1) +log(d/2+1))
∥f∥sup, (15)
giving the desired bound.
We note that Lemma 6.4 will be significantly harder to prove. See Remark 6.8 for an intuitive
explanation why. We begin with two structural lemmas on how the solution to a unitriangular
linear system behaves, which might be of independent interest.
25Lemma 6.6 (An entry-wise positive solution) .Suppose that A∈Rd×dis an upper unitriangular
matrix such that, for all i ⩽j, A i,j>0, Ai,j>Ai−1,j. Then A−1⃗1is a vector with positive entries.
The same result holds when Ais a lower unitriangular matrix such that, for all i⩾j,Ai,j>0,
Ai,j>Ai+1,j.
Proof. Letx=A−1⃗1. Then xd=1⩾0. The result follows by induction:
xi=1−d
∑
j=i+1Ai,jxj
=d
∑
j=i+1(Ai+1,j−Ai,j)xj+1−d
∑
j=i+1Ai+1,jxj
=d
∑
j=i+1(Ai+1,j−Ai,j)xj+1−[Ax]i+1
=d
∑
j=i+1(Ai+1,j−Ai,j)xj
>0.
For lower unitriangular matrices, the same argument follows. The inverse satisfies x1=1and
xi=1−i−1
∑
j=1Ai,jxj
=d
∑
j=i+1(Ai−1,j−Ai,j)xj+1−d
∑
j=i+1Ai−1,jxj>0.
Next, we characterize how the solution to a unitriangular linear system behaves when we
consider a partial ordering on the matrices.
Lemma 6.7. LetAbe a non-negative upper unitriangular matrix such that Ai,j>Ai−1,jandAi,j>
Ai,j+1for all i⩽j. Let Bbe a matrix with the same properties, such that A⩾Bentrywise. By Lemma 6.6,
x(A)=A−1⃗1and x(B)=B−1⃗1are non-negative. It further holds that ∑d
i=1[A−1⃗1]i⩽∑d
i=1[B−1⃗1]i.
Proof. We consider the line between Aand B,A(t) = A(1−t) +Btfort∈[0, 1]. Let x(t) =
A−1⃗1; we will prove that ⃗1†x(t)is monotonically increasing in t. The gradient of x(t)has a
simple form [Tao13]:
A(t)x(t) =⃗1
∂[A(t)x(t)] =∂t[⃗1]
(B−A)x(t) +A(t)∂tx(t) =0
∂tx(t) =A−1(t)(A−B)x(t).
So,
⃗1†∂tx(t) =⃗1†A−1(t)(A−B)A−1(t)⃗1
= [([ A(t)]−1)†⃗1]†(A−B)[A−1(t)⃗1].
Since Aand Bsatisfy the entry constraints, so do every matrix along the line. Consequently,
the column constraints in Lemma 6.6 are satisfied for both Aand A†, so both ([A(t)]−1)†⃗1and
26A−1(t)⃗1are positive vectors. Since A⩾Bentrywise, this means that ⃗1†∂tx(t)is positive, as
desired.
Proof of Lemma 6.4. We first observe that the following sorts of sums are bounded. Let xk:=
cos(π
2(1−1
2k+1)). Then, using that Tℓ(cos(x)) = cos(ℓx),
f2k+1(xk) =2k+1
∑
ℓ=0aℓTℓ(xk)
=k
∑
ℓ=0a2ℓ+1T2ℓ+1(xk)
=k
∑
ℓ=0a2ℓ+1cosπ
2
2ℓ+1−2ℓ+1
2k+1
=k
∑
ℓ=0(−1)ℓa2ℓ+1sinπ
22ℓ+1
2k+1
.
We have just shown that
k
∑
ℓ=0a2ℓ+1(−1)ℓsinπ
22ℓ+1
2k+1⩽∥f2k+1∥sup. (16)
We now claim that there exist non-negative ckfork∈ {0, 1, . . . , d}such that
d
∑
ℓ=0(−1)ℓa2ℓ+1=d
∑
k=0ckf2k+1(xk). (17)
The f2k+1(xk)’s can be bounded using Lemma 6.2. The rest of the proof will consist of showing
that the ck’s exist, and then bounding them.
To do this, we consider the coefficient of each a2ℓ+1separately; let A(k)∈[0, 1]d+1(index starting
at zero) be the vector of coefficients associated with p2k+1(xk):
A(k)
ℓ=sinπ
22ℓ+1
2k+1
for 0⩽ℓ⩽k, 0 otherwise. (18)
Note that the A(k)
ℓis always non-negative and increasing with ℓup to A(k)
k=1. Then Eq. (17)
holds if and only if
c0A(0)+· · ·+cdA(d)=⃗1,
or in other words, the equation Ac=⃗1is satisfied, where Ais the matrix with columns A(k)
and cis the vector of cℓ’s. Since Ais upper triangular (in fact, with unit diagonal), this can be
solved via backwards substitution: cd=1, then cd−1can be deduced from cd, and so on. More
formally, the sth row gives the following constraint that can be rewritten as a recurrence.
d
∑
t=ssinπ
22s+1
2t+1
ct=1 (19)
cs=1−d
∑
t=s+1sinπ
22s+1
2t+1
ct (20)
Because the entries of Aincrease in ℓ, the cℓ’s are all positive.
27Invoking Lemma 6.6 with the matrix A establishes that such csexist; our goal now is to bound
them. Doing so is not as straightforward as it might appear: since the recurrence Eq. (20)
subtracts byct’s, an upper bound on ctfort∈[s+1,d]does not give an upper bound on cs; it
gives a lower bound. So, an induction argument to show bounds for cs’s fails. Further, we were
unable to find any closed form for this recurrence. However, since all we need to know is the
sum of the cs’s, we show that we canbound this via a generic upper bound on the recurrence.
Here, we apply Lemma 6.7 to Aas previously defined, and the bounding matrix is (for i⩽j)
Bi,j=i
j⩽2i+1
2j+1⩽sinπ
22i+1
2j+1
=Ai,j,
using that sin (π
2x)⩾xforx∈[0, 1]. Let ˆc=B−1⃗1. Then ˆci=1
i+1fori̸=dand ˆcd=1.
[Bˆc]i=d
∑
j=iBi,jˆcj=d−1
∑
j=ii
j1
j+1+i
d=id−1
∑
j=i1
j−1
j+1
+i
d=i1
i−1
d) +i
d=1
By Lemma 6.7, ∑ici⩽∑iˆci⩽log(d) +2. So, altogether, we have
d
∑
ℓ=0(−1)ℓa2ℓ+1=d
∑
k=0ckf2k+1(xk)
⩽d
∑
k=0ck∥f2k+1∥sup
⩽d
∑
k=0ck
max
0⩽k⩽d∥f2k+1∥sup
=d
∑
k=0ck
max
0⩽k⩽2d+1∥fk∥sup
⩽(log(d) +2)max
0⩽k⩽2d+1∥fk∥sup.
Remark 6.8. A curious reader will (rightly) wonder whether this proof requires this level of
difficulty. Intuition from the similar Fourier analysis setting suggests that arithmetic progres-
sions of any step size at any offset are easily bounded. We can lift to the Fourier setting by
considering, for an f:[−1, 1]→R, a corresponding 2 π-periodic g:[0, 2π]→Rsuch that
g(θ):=f(cos(θ)) =∞
∑
k=0akTk(cos(θ)) =∞
∑
k=0akcos(kθ) =∞
∑
k=0akeikθ+e−ikθ
2.
This function has the property that |g(θ)|⩽∥f∥supandbg(k) = a|k|/2(except bg(0) = a0).
Consequently,
1
tt−1
∑
j=0f
cos(2πj
t)
=1
tt−1
∑
j=0g2πj
t
=1
tt−1
∑
j=0∞
∑
k=−∞bg(k)e2πijk/t=∞
∑
k=−∞bg(k)t−1
∑
j=01
te2πijk/t
=∞
∑
k=−∞bg(k)Jkis divisible by tK=∞
∑
k=−∞bg(kt),
so we can bound arithmetic progressions |∑kbg(kt)|⩽∥f∥sup, and this generalizes to other
offsets, to bound |∑kbg(kt+o)|for some o∈[t−1]. Notably, though, this approach does not
say anything about sums like ∑ka4k+1. The corresponding progression of Fourier coefficients
28doesn’t give it, for example, since we pick up unwanted terms from the negative Fourier
coefficients.16
∑
kbg(4k+1) =(bg(1) +bg(5) +bg(9) +· · ·)+(bg(−3) +bg(−7) +bg(−11) +· · ·)
=1
2(a1+a5+a9+· · ·)+1
2(a3+a7+a11+· · ·)=∑
k⩾0a2k+1.
In fact, by inspection of the distribution17D(x) =∑∞
k=0T4k+1(x), it appears that this arithmetic
progression cannot be written as a linear combination of evaluations of f(x). Since the shape
of the distribution appears to have 1/xbehavior near x=0, we conjecture that our analysis
losing a log factor is, in some respect, necessary.
Conjecture 6.9. For any step size t>1and offset o∈[t−1]such that o̸=t/2, there exists a function
f:[−1, 1]→Rsuch that ∥f∥sup=1but|∑n
k=0atk+o|=Ω(log(n)).
7 Properties of the Clenshaw Recursion
7.1 Deriving the Clenshaw Recursions
Suppose we are given as input a degree- dpolynomial as a linear combination of Chebyshev
polynomials:
p(x) =d
∑
k=0akTk(x). (21)
Then this can be computed with the Clenshaw algorithm , which is the following recurrence.
qd+1=qd+2=0
qk=2xqk+1−qk+2+ak (Clenshaw)
˜p=1
2(a0+q0−q2)
Lemma 7.1. The recursion in Eq. (Clenshaw) computes p (x). That is, in exact arithmetic, ˜p=p(x).
In particular,
qk=d
∑
i=kaiUi−k(x). (22)
Proof. We show Eq. (22) by induction.
qk=2xqk+1−qk+2+ak
=2x d
∑
i=k+1aiUi−k−1(x)
− d
∑
i=k+2aiUi−k−2(x)
+ak
16These sums are related to the Chebyshev coefficients one gets from interpolating a function at Chebyshev points
[Tre19, Theorem 4.2].
17This is the functional to integrate against to compute the sum,2
πR1
−1f(x)D(x)/√
1−x2=∑a4k+1. The
distribution is not a function, but can be thought of as the limit object of Dn(x) =∑n
k=0T4k+1(x)asn→∞,
analogous to Dirichlet kernels and the Dirac delta distribution.
29=ak+2xak+1U0(x) +d
∑
i=k+2ai(2xUi−k−1(x)−Ui−k−2(x))
=d
∑
i=kaiUi−k(x).
Consequently, we have
1
2(a0+u0−u2) =1
2
a0+d
∑
i=0aiUi(x)−d
∑
i=2aiUi−2(x)
=a0+a1x+d
∑
i=2ai
2(Ui(x)−Ui−2(x)) =d
∑
i=0aiTi(x).
Remark 7.2. Though the aforementioned discussion is specialized to the scalar setting, it extends
to the the matrix setting almost entirely syntactically: consider a Hermitian A∈Cn×nand
b∈Cnwith∥A∥,∥b∥⩽1. Then p(A)bcan be computed in the following way:
ud+1=⃗0;
ud=adb;
uk=2Auk+1−uk+2+akb;
u:=p(A)b=1
2(a0b+u0−u2).(23)
The proof that this truly computes p(A)bis the same as the proof of correctness for Clenshaw’s
algorithm shown above. We will also be generalizing to non-Hermitian A∈Cm×n, in which
case the only additional wrinkle is that in the recurrence we will need to choose either AorA†
such that dimensions are consistent. Provided that the polynomial being computed is even or
odd, no issues will arise. Consequently, Clenshaw-like recurrences will give matrix polynomials
where xkare replaced with A†AA†· · ·Ab, which corresponds to the definition of singular value
transformation from Definition 4.1.
7.2 Evaluating Even and Odd Polynomials
We will be considering evaluating odd and even polynomials. We again focus on the scalar
setting and note that this extends to the matrix setting in the obvious way. The previous
recurrence Eq. (Clenshaw) can work in this setting, but it’ll be helpful for our analysis if the
recursion multiplies by x2each time, instead of x[MH02, Chapter 2, Problem 7]. So, in the case
where the degree- (2d+1)polynomial p(x)isodd(soa2k=0for every k), it can be computed
with the iteration
qd+1=qd+2=0;
qk=2T2(x)qk+1−qk+2+a2k+1U1(x); (Odd Clenshaw)
˜p=1
2(q0−q1).
When p(x)is a degree- (2d)even polynomial (so a2k+1=0for every k), it can be computed via
the same recurrence, replacing a2k+1U1(x)with a2k. However, we will use an alternative form
that’s more convenient for us (since we can reuse the analysis of the odd case).
˜a2k:=a2k−a2k+2+a2k+4− · · · ± a2d; (24)
30qd+1=qd+2=0;
qk=2T2(x)qk+1−qk+2+˜a2k+2U1(x)2; (Even Clenshaw)
˜p=˜a0+1
2(q0−q1).
These recurrences correctly compute pfollows from a similar analysis to the standard Clenshaw
algorithm, formalized below.
Lemma 7.3. The recursions in Eq. (Odd Clenshaw) and Eq. (Even Clenshaw) correctly compute
p(x)for even and odd polynomials, respectively. That is, in exact arithmetic, ˜p=p(x). In particular,
qk=d
∑
i=kaiUi−k(x). (25)
Proof. We can prove these statements by applying Eq. (22). In the odd case, Eq. (Odd Clenshaw)
is identical to Eq. (Clenshaw) except that xis replaced by T2(x)and akis replaced by a2k+1U1(x),
so by making the corresponding changes in the iterate, we get that
qk=d
∑
i=ka2i+1U1(x)Ui−k(T2(x)) =d
∑
i=ka2i+1U2(i−k)+1(x) by Eq. (9)
˜p=1
2(q0−q1) =d
∑
i=0a2i+1
2
U2i+1(x)−U2i−1(x)
=p(x). by Eq. (6)
Similarly, in the even case, Eq. (Even Clenshaw) is identical to Eq. (Clenshaw) except that xis
replaced by T2(x)and akis replaced by 4 ˜a2kx2(see Definition 24), so that
qk=d
∑
i=k˜a2i+2U1(x)2Ui−k(T2(x))
=d
∑
i=k˜a2i+2U1(x)U2(i−k)+1(x) by Eq. (9)
=d
∑
i=k˜a2i+2(U2(i−k)(x) +U2(i−k+1)(x)) by Eq. (5)
=d+1
∑
i=k˜a2i+2U2(i−k)(x) +d+1
∑
i=k+1˜a2iU2(i−k)(x) noticing that ˜a2d+2=0
=˜a2k+2+d+1
∑
i=k+1(˜a2i+˜a2i+2)U2(i−k)(x)
=˜a2k+2+d
∑
i=k+1a2iU2(i−k)(x)
=−˜a2k+d
∑
i=ka2iU2(i−k)(x).
Finally, observe
˜a0+1
2(q0−q1) = ˜a0+1
2(a0−˜a0+˜a2) +d
∑
i=1a2i
2(U2i(x)−U2i−2(x)) = p(x).
31Remark 7.4. We can further compute what happens to all these recursions with some additive
ε(k)error in iteration k. This follows just by adding ε(k)to the constant term in the recursion and
chasing the resulting changes through the analysis of Clenshaw, as is done for Lemma 7.3.
for standard Clenshaw: ˜qk=d
∑
i=k(ai+ε(i))Ui−k(x)
for odd Clenshaw: ˜qk=d
∑
i=k(a2i+1U2(i−k)+1(x) +ε(i)Ui−k(T2(x)))
for even Clenshaw: ˜qk=−˜a2k+d
∑
i=k(a2iU2(i−k)(x) +ε(i)Ui−k(T2(x)))
Propagating this error to the full result gives the following results:
for standard Clenshaw: ˜p−p(x) =1
2+d
∑
i=1ε(i)Ti(x)
for odd Clenshaw: ˜p−p(x) =1
2ε(0)+1
2d
∑
i=1ε(i)(Ui(T2(x))−Ui−1(T2(x)))
for even Clenshaw: ˜p−p(x) =1
2ε(0)+1
2d
∑
i=1ε(i)(Ui(T2(x))−Ui−1(T2(x)))
Because ∥Ti(x)∥sup=1but∥Ui(T2(x))−Ui−1(T2(x))∥sup=2i+1, this suggests that these
parity-specific recurrences are less stable than the standard recursion. However, they will be
more amenable to our sketching techniques.
8 Stability of the Scalar Clenshaw Recursion
Before we move to the matrix setting, we warmup with a stability analysis of the scalar
Clenshaw recurrence. Suppose we perform Eq. (Clenshaw) to compute a degree- dpolyno-
mial p, except every addition, subtraction, and multiplication incurs εrelative error. Typi-
cally, this has been analyzed in the finite precision setting, where the errors are caused by
truncation. These standard analyses show that this finite precision recursion gives p(x)to
d2(∑|ai|)ε=O(d3∥p∥supε)error. This bound ∑|ai|is not easily improved in settings where pis
a polynomial approximation of a smooth function, since standard methods only give bounds of
the form |ak|=Θ((1−log(1/ε)/d)−k), [Tre19, Theorem 8.1], giving only constant bounds for
the coefficients.
Such a bound on ∑|ak|is not tight, however. A use of Parseval’s formula [MH02, Theorem 5.3]
improves on this by a factor of√
d:
∑|ai|⩽√
dq
∑a2
i=O(√
d∥p∥sup). (26)
Testing ∥∑d
ℓ=1sℓ1√
ℓTℓ(x)∥supfor random signs sℓ∈ {± 1}suggests that this bound is tight,
meaning coefficient-wise bounds can only prove an error overhead of eΘ(d2.5∥p∥sup)for the
Clenshaw recurrence.
We improve on prior stability analyses to show that the Clenshaw recurrence for Chebyshev
polynomials only incurs an error overhead of d2log(d)∥p∥sup. This is tight up to a logarithmic
factor. This, for example, could be used to improve the bound in [MMS18, Lemma 9] from k3to
32k2log(k)(where in that paper, kdenotes degree). As we do for the upcoming matrix setting, we
proceed by performing an error analysis on the recursion with a stability parameter µ, and then
showing that for any bounded polynomial, 1/ µcan be chosen to be O(d2logd).
8.1 Analyzing Error Propagation
The following is a simple analysis of Clenshaw, with some rough resemblance to an analysis of
Oliver [Oli79].
Theorem 8.1 (Stability analysis for scalar Clenshaw) .Consider a degree- dpolynomial p:[−1, 1]→
Rwith Chebyshev coefficients p(x) =∑d
k=0akTk(x). Let⊕,⊖,⊙:C×C→Cbe binary operations
representing addition, subtraction, and multiplication to µεrelative error, for 0<ε<1:
|(x⊕y)−(x+y)|⩽µε(|x|+|y|);
|(x⊖y)−(x−y)|⩽µε(|x|+|y|);
|x⊙y−x·y|⩽µε|x||y|=µε|xy|.
Given an x ∈[−1, 1], consider performing the Clenshaw recursion with these noisy operations:
˜qd+1=˜qd+2=0;
˜qk= (2⊙x)⊙˜qk+1⊖(˜qk+2⊖ak); (Finite-Precision Clenshaw)
˜˜p=1
2⊙((a0⊕q0)⊖q2).
Then Eq. (Finite-Precision Clenshaw) outputs p(x)up to 50ε∥p∥superror18, provided that µ>0
satisfies the following three criterion.
(a)µε⩽1
50(d+2)2;
(b)µ∑d
i=0|ai|⩽∥p∥sup;
(c)µ|qk|=µ|∑d
i=kaiUi−k(x)|⩽1
d∥p∥supfor all k ∈ {0, . . . , d}.
This analysis shows that arithmetic operations incurring µεerror result in computing p(x)to
εerror. In particular, the stability of the scalar Clenshaw recurrence comes down to under-
standing how small we can take µ. Note that if we ignored coefficient sign, |∑d
i=kaiUi−k(x)|⩽
|∑d
i=k|ai|Ui−k(x)|=∑d
i=k(i−k+1)|ai|, this would require setting µ=Θ(1/d3). We show in
Section 8.2 that we can set µ=Θ((d2log(d))−1)for all x∈[−1, 1]and polynomials p.
Lemma 8.2. In Theorem 8.1, it suffices to take µ=Θ((d2log(d))−1).
Proof of Theorem 8.1. We will expand out these finite precision arithmetic to get error intervals
for each iteration.
˜qd+1=˜qd+2=0, (27)
18We did not attempt to optimize the constants for this analysis.
33and
˜qk= (2⊙x)⊙˜qk+1⊖(˜qk+2⊖ak)
= (2x±2µε|x|)⊙˜qk+1⊖(˜qk+2−ak±µε(|˜qk+2|+|ak|))
= (( 2x˜qk+1±2µε|x|˜qk+1)±µε|(2x±2µε|x|)˜qk+1|)⊖(˜qk+2−ak±µε(|˜qk+2|+|ak|))
∈(2x˜qk+1±(2µε+µ2ε2)2|x˜qk+1|)⊖(˜qk+2−ak±µε(|˜qk+2|+|ak|))
∈(2x˜qk+1±6µε|x˜qk+1|)⊖(˜qk+2−ak±µε(|˜qk+2|+|ak|))
=2x˜qk+1−˜qk+2+ak±µε(6|x˜qk+1|+|˜qk+2|+|ak|)
+µε|2x˜qk+1±6µε|x˜qk+1||+µε|˜qk+2−ak±µε(|˜qk+2|+|ak|)|
∈2x˜qk+1−˜qk+2+ak±µε(14|x˜qk+1|+3|˜qk+2|+3|ak|),
and,
˜˜p=1
2⊙((a0⊕q0)⊖q2)
=1
2⊙((a0+q0±µε(|a0|+|q0|))⊖q2)
=1
2⊙((a0+q0−q2±µε(|a0|+|q0|))±µε(|a0+q0±µε(|a0|+|q0|)|+|q2|))
∈1
2⊙(a0+q0−q2±µε(3|a0|+3|q0|+|q2|))
=1
2(a0+q0−q2±µε(3|a0|+3|q0|+|q2|))±µε1
2|a0+q0−q2±µε(3|a0|+3|q0|+|q2|)|
∈1
2(a0+q0−q2)±1
2µε(7|a0|+7|q0|+3|q2|).
To summarize, we have
˜qd+1=˜qd+2=0,
˜qk=2x˜qk+1−˜qk+2+ak+δk, where |δk|⩽µε(14|x˜qk+1|+3|˜qk+2|+3|ak|) (28)
˜˜p=1
2(a0+q0−q2) +δ, where |δ|⩽1
2µε(7|a0|+7|q0|+3|q2|) (29)
By Lemma 7.1, this recurrence satisfies
˜qk=d
∑
i=kUi−k(x)(ai+δi)
qk−˜qk=d
∑
i=kUi−k(x)δi
q−˜q=δ+1
2d
∑
i=0Ui(x)δi−d
∑
i=2Ui−2(x)δi
=δ+1
2δ0+d
∑
i=1Ti(x)δi
|q−˜q|⩽|δ|+1
2|δ0|+d
∑
i=1|Ti(x)δi|⩽|δ|+d
∑
i=0|δi|. (30)
This analysis so far has been fully standard. Let’s continue bounding.
⩽µε
7
2|a0|+7
2|q0|+3
2|q2|+d
∑
i=0(14|x˜qi+1|+3|˜qi+2|+3|ai|)
⩽µεd
∑
i=0(20|˜qi|+10|ai|). (31)
34Now, we will bound all of the δk’s. Combining previous facts, we have
|˜qk|=d
∑
i=kUi−k(x)(ai+δi)
⩽d
∑
i=kUi−k(x)ai+d
∑
i=kUi−k(x)δi
⩽d
∑
i=kUi−k(x)ai+d
∑
i=k(i−k+1)|δi|
⩽d
∑
i=kUi−k(x)ai+µεd
∑
i=k(i−k+1)(14|˜qi+1|+3|˜qi+2|+3|ai|)
⩽1
µd+3µεd−k+1
µ
∥p∥sup+µεd
∑
i=k(i−k+1)(14|˜qi+1|+3|˜qi+2|)
⩽1.5
µd∥p∥sup+µεd
∑
i=k(i−k+1)(14|˜qi+1|+3|˜qi+2|).
Note that |˜qk|⩽ck, where
cd=0;
ck=1.5
µd∥p∥sup+µεd
∑
i=k(i−k+1)(14ci+1+3ci+2).
Solving this recurrence, we have that ck⩽2
µd∥p∥sup, since by strong induction,
ck⩽1.5
µd+µεd
∑
i=k(i−k+1)172
µd
∥p∥sup
=1.5
µd+17µε1
µd(d−k+1)(d−k+2)
∥p∥sup⩽2
µd∥p∥sup.
Returning to Equation (31):
|q−˜q|⩽µεd
∑
i=0(20|˜qi|+10|ai|)⩽µεd
∑
i=0(20ci+10|ai|)
⩽40ε∥p∥sup+10µεd
∑
i=0|ai|⩽50ε∥p∥sup.
8.2 Bounding the Iterates of the Clenshaw Recurrence
The goal of this section is to prove Lemma 8.2. In particular, we wish to show that for µ=
Θ((d2log(d))−1), the following criteria hold:
(a)µε⩽1
50(d+2)2;
(b)µ∑d
i=0|ai|⩽∥p∥sup;
(c)µ|qk|=µ|∑d
i=kaiUi−k(x)|⩽1
d∥p∥supfor all k∈ {0, . . . , d}.
For this choice of µ, (a) is clearly satisfied, and since |ai|⩽2∥p∥sup(Lemma 4.2), µ∑d
i=0|ai|⩽
2(d+1)∥p∥sup⩽∥p∥sup, so (b) is satisfied. In fact, both of these criterion are satisfied for
µ=Ω(1/d), provided εis sufficiently small.
35Showing (c) requires bounding ∥∑d
ℓ=kaℓUℓ−k(x)∥supfor all k∈[d]. These expressions are also
the iterates of the Clenshaw algorithm (Lemma 7.1), so we are in fact trying to show that in
the process of our algorithm we never produce a value that’s much larger than the final value.
From testing computationally, we believe that the following holds true.
Conjecture 8.3. Letp(x)be a degree- dpolynomial with Chebyshev expansion p(x) =∑d
ℓ=0aℓTℓ(x).
Then, for all k from 0to d,
d
∑
ℓ=kaℓUℓ−k(x)
sup⩽(d−k+1)∥p∥sup,
maximized for the Chebyshev polynomial p (x) =Td(x).
Conjecture 8.3 would imply that it suffices to take µ=Θ(1/d2). We prove it up to a log factor.
Theorem 8.4. For a degree- dpolynomial p(x) =∑d
ℓ=0aℓTℓ(x), consider the degree- (d−k)polynomial
qk(x) =∑d
ℓ=kaℓUℓ−k(x). Then
∥qk∥sup⩽(d−k+1)
16+16
π2log(d)
∥p∥sup.
Proof. We proceed by carefully bounding the Chebyshev coefficients of qk, which turn out to be
arithmetic progressions of the ak’s which we bounded in Section 6.
qk(x) =∑
iaiUi−k(x)
=∑
i∑
j⩾0aiTi−k−2j(x)(1+Ji−k−2j̸=0K)
=∑
i∑
j⩾0ai+k+2jTi(x)(1+Ji̸=0K)
=∑
iTi(x)(1+Ji̸=0K)∑
j⩾0ai+k+2j
|qk(x)|⩽∑
iJi⩾0K(1+Ji̸=0K)∑
j⩾0ai+k+2j
⩽2d−k
∑
i=0∑
j⩾0ai+k+2j
⩽4d−k
∑
i=0
4+4
π2log(i+k−1)
∥p∥sup by Fact 6.3
⩽(d−k+1)
16+16
π2log(d)
∥p∥sup.
Remark 8.5. We spent some time trying to prove Conjecture 8.3, since its form is tantalizingly
close to that of the Markov brothers’ inequality [Sch41] ∥d
dxp(x)∥sup=∥∑d
ℓ=0aℓℓUℓ−1(x)∥sup⩽
d2∥p(x)∥sup, except with the linear differential operatord
dx:Tℓ7→ℓUℓ−1replaced with the
linear operator Tℓ7→Uℓ−k. However, calculations suggest that the variational characterization
ofmax∥p∥sup=1|d
dxp(x)|underlying proofs of the Markov brothers’ inequality [Sha04] does not
hold here, and from our shallow understanding of these proofs, it seems that they strongly use
properties of the derivative.
369 Computing Matrix Polynomials
Our goal is to prove the following theorem:
Theorem 9.1. Suppose we are given sampling and query access to A∈Cm×nand b∈Cnwith
∥A∥⩽1; an even or odd degree- dpolynomial pwith p(0) =0, given as its Chebyshev coefficients; and
a sufficiently small accuracy parameter ε>0. Then we can output a description of a vector y(in Cmifp
is odd, in Cnif p is even) such that ∥y−p(A)b∥⩽ε∥p∥sup∥b∥with probability ⩾0.9in time
O
min
nnz(A),∥A∥4
F
ε4d12log8(d)log2∥A∥F
∥A∥
+∥A∥4
F
ε2d11log4(d)log∥A∥F
∥A∥
.
We can access the output description in the following way:
(i) Compute entries of y in O∥A∥2
F
ε2d6log4(d)log∥A∥F
∥A∥
time;
(ii)Sample i∈[n]with probability|yi|2
∥y∥2inO∥p∥2
sup∥A∥4
F∥b∥2
ε2∥y∥2 d8log8(d)log∥A∥F
∥A∥
time with proba-
bility⩾0.9;
(iii) Estimate ∥y∥2toνrelative error in O∥p∥2
sup∥A∥4
F∥b∥2
ν2ε2∥y∥2d8log8(d)log∥A∥F
∥A∥
time with probability
⩾0.9.
We prove the even and odd cases separately. We can conclude the above theorem as a conse-
quence of Theorems 9.3 and 9.7 and Corollaries 9.4 and 9.8.
Remark 9.2. We can also compute estimate ⟨u|y⟩toε∥u∥∥b∥error without worsening the
1/ε2dependence. This does not follow from the above; rather, we can observe that, from the
description of our output, y= (AS)v+ηb, it suffices to estimate u†(AS)vand u†b. Because
of properties of the sketches and the later analysis, ∥AS∥F≲∥A∥Fand∥v∥≲dlog2(d)∥b∥,
so by [CGLLTW22, Lemma 4.12 and Remark 4.13] we can estimate these to the desired error
withO(d2log4(d)∥A∥2
F1
ε2log1
δ)queries to u,v,Aand samples to AS. All of these queries can
be done in O(1)time.
9.1 Computing Odd Matrix Polynomials
In this section, we prove the following theorem. The statement involves a parameter µwhich
depends on the polynomial being evaluated; this parameter is between 1and(dlogd)−2,
depending on how well-conditioned the polynomial is.
Theorem 9.3. Suppose we are given sampling and query access to A∈Cm×nand b∈Cnwith
∥A∥⩽1; a(2d+1)-degree odd polynomial p, written in its Chebyshev coefficients as
p(x) =d
∑
i=0a2i+1T2i+1(x);
an accuracy parameter ε>0; a failure probability parameter δ>0; and a stability parameter µ>0.
Then we can output a vector x∈ Cnsuch that ∥Ax−p(A)b∥⩽ε∥p∥sup∥b∥with probability ⩾1−δ
in time
O
min
nnz(A),d4∥A∥4
F
(µε)4log2 ∥A∥F
δ∥A∥
+d7∥A∥4
F
(µε)2δlog ∥A∥F
δ∥A∥
,
assuming µε<min(1
4d∥A∥,1
100d)andµsatisfies the following bounds:
37(a)µ∑d
i=0|a2i+1|⩽∥p∥sup;
(b)µ∥∑d
i=ka2i+1Ui−k(T2(x))∥sup⩽1
d∥p∥supfor all 0⩽k⩽d;
The output description has the additional properties
∑
jA∗,j2xj2≲ε2∥p∥2
sup∥b∥2
d4log∥A∥F
δ∥A∥∥x∥0≲d2∥A∥2
F
(µε)2log∥A∥F
δ∥A∥,
so that by Corollary 4.10, for the output vector y :=Ax, we can:
(i) Compute entries of y in O(∥x∥0) =Od2∥A∥2
F
(µε)2log∥A∥F
δ∥A∥
time;
(ii)Sample i∈[n]with probability|yi|2
∥y∥2inO∥p∥2
sup∥A∥4
F∥b∥2
µ4ε2∥y∥2log∥A∥F
δ∥A∥log1
δ
time with probability
⩾1−δ;
(iii) Estimate ∥y∥2toνrelative error in O∥p∥2
sup∥A∥4
F∥b∥2
ν2µ4ε2∥y∥2log∥A∥F
δ∥A∥log1
δ
time with probability ⩾
1−δ.
Algorithm 2 (Odd singular value transformation) .
Input (pre-processing): A matrix A∈Cm×n, vector b∈Cn, and parameters ε,δ,µ>0.
Pre-processing sketches: Lets,t=Θd2∥A∥2
F
(µε)2log(∥A∥F
δ∥A∥)
. This phase will succeed with
probability ⩾1−δ.
P1.IfSQ(A†)and SQ(b)are not given, compute data structures to simulate them in
O(1)time;
P2. Sample S∈ Cn×sto be AMP s
A,b,{1
2(∥A∗,j∥2
∥A∥2
F+|bj|2
∥b∥2)}j∈[n]
(Definition 5.4);
P3. Sample T†∈ Cm×tto be AMP t
S†A†,AS,{∥[AS]i,∗∥
∥AS∥2
F}i∈[m]
(Definition 5.4);
P4. Compute a data structure that can respond to SQ (TAS)queries in O(1)time;
Input: A degree 2d+1polynomial p(x) =∑d
i=0a2i+1T2i+1(x)given as its coefficients a2i+1.
Clenshaw iteration: Letr=Θ(d4∥A∥2
F(s+t)1
δ) =Θ(d6∥A∥4
F
(µε)2δlog(∥A∥F
δ∥A∥)). This phase will
succeed with probability ⩾1−δ. Starting with vd+1=vd+2=⃗0sand going until v0,
I1. Let B(k)=BEST r(TAS)and B(k)
†=BEST r((TAS)†)(Definition 5.1);
I2. Compute vk=2(2B(k)
†B(k)−I)vk+1−vk+2+a2k+1S†b.
Output: Output x=1
2S(v0−v1)satisfying ∥Ax−p(A)b∥⩽ε∥p∥sup∥b∥.
The criterion for what µneed to be are somewhat non-trivial; the important requirement is
Item 9.3(b), which states that 1/µis a bound on the norm of various polynomials. These
polynomials turn out to be iterates of Eq. (Odd Clenshaw) when computing p(x)/x, so roughly
speaking, the algorithm we present depends on the numerical stability of evaluating p(x)/x.
This is necessary because we primarily work in the “dual” space, maintaining our Clenshaw
iterate ukasAvkwhere vkis a sparse vector. This bears a resemblance to the dependence in
38[CGLLTW22, Theorem 5.1] on the Lipschitz smoothness of f(x)/x. For any bounded polyno-
mial, we can always take µto beΩ((dlog(d))−2).
Corollary 9.4 (Corollary of Proposition 9.9) .In Theorem 9.3, we can always take 1/µ≲d2log2(d)
for d>1.
This bound is achieved up to log factors by p(x) =T2k+1(x). We are now ready to dive into the
proof of Theorem 9.3. Without loss of generality, we assume ∥p∥sup=1.
Pre-processing sketches, running time. Given a matrix A∈Cm×nand a vector b∈ Cn, the
pre-processing phase of Algorithm 2 can be performed in O(nnz(A) +nnz(b))time. First, we
build a data structure to respond to SQ(A†)and SQ(b)queries in O(1)time using the alias
data structure described in Remark 4.9 (A2.P1). Then, we use these accesses to construct an
AMP sketch SforAb, where we produce the samples for the sketch by sampling from band
sampling from the row norms of A†, each with probability1
2(A2.P2). Since we need ssamples,
this takes O(s)queries to the data structure. The sketch Sis defined such that ASis a subset of
the columns of A, with each column rescaled according to the probability it was sampled. So,
with another pass through A, we can construct a data structure for SQ(AS)inO(1)time using
Remark 4.9 and use this to construct an AMP sketch T†for(AS)†AS(A2.P3). The samples for
the sketch are drawn from the row norms of AS. The final matrix TAS is a rescaled submatrix
ofA; another O(nnz(A))pass through Asuffices to construct a data structure to respond to
SQ(TAS)queries in O(1)time (A2.P4). The running time is O(nnz(A) +nnz(b) +s+t)or,
alternatively, three passes through Aand one pass with b, using O(st)space. We can assume
that s,t⩽nnz(A), though: if s⩾n, then we can take S=I, and if t⩾m, then we can take
T=I, and this will satisfy the same guarantees; further, without loss, nnz(A)⩾max(m,n),
since otherwise there is an empty row or column that we can ignore.
If we are given A,bsuch that SQ(A†)and SQ(b)queries can be performed in O(Q)time, then
the pre-processing phase of Algorithm 2 can be performed in O(Qst)time. The main difference
from the description above is that we use that, given SQ(A†), we can simulate queries to SQ(AS)
with only O(s)overhead. To produce a sample i∈[m]with probability ∥[AS]i,∗∥2/∥AS∥2
F,
sample a column index j∈[s]with probability[AS]∗,j2/∥AS∥2
Fand then query SQ(A)to get
a row index i∈[m]with probability[AS]i,j2/[AS]∗,j2.
Remark 9.5. If we are not given buntil after the pre-processing phase, or if we are only given b
as a list of entries without SQ(b), then we can take the Ssketch to just be sampling from the
row norms of A†. This will decrease the success probability of the following phase (specifically,
because of the guarantee in Eq. ( AbAMP)) to 0.99.
Pre-processing sketches, correctness. We list the guarantees of the sketch that we will use
in the error analysis, and point to where they come from in Section 5.2. Recall that, with
S∈ Cn×staken to be an AMP sketch of X∈ Cm×n,Y∈ Cn×d, by Corollary 5.6, with probability
⩾1−δ,∥XSS†Y†−XY†∥⩽ε∥X∥∥Y∥provided s=Ω(1
ε2(∥X∥2
F
∥X∥2+∥Y∥2
F
∥Y∥2)log(1
δ(∥X∥2
F
∥X∥2+∥Y∥2
F
∥Y∥2)))
andε⩽1. We will use this with s,t=Θ(d2∥A∥2
F
(µε)2log(∥A∥F
δ∥A∥)), whereµε
d⩽1
4∥A∥. The guarantees
of Corollary 5.6 individually fail with probability O(δ), so we will rescale to say that they all
hold with probability ⩾1−δ. The following bounds hold for the sketch S.
∥[AS]∗,j∥2⩽2∥A∥2
F/sfor all j∈[s] by Definition 5.4 ( ∥[AS]∗,j∥bd)
∥AS∥2
F⩽2∥A∥2
F by Eq. ( ∥[AS]∗,j∥bd) ( ∥AS∥Fbd)
39∥S†b∥2⩽2∥b∥2by Definition 5.4 ( ∥S†b∥bd)
∥Ab−ASS†b∥⩽µε
d∥b∥ by Corollary 5.6 ( AbAMP)
∥AA†−AS(AS)†∥⩽µε
d∥A∥ by Corollary 5.6 ( AA†AMP)
∥AS∥2=∥AS(AS)†∥⩽(1+µε
d∥A∥)∥A∥2by Eq. ( AA†AMP) ( ∥AS∥bd)
The following bounds hold for the sketch T.
∥TAS∥2
F=∥AS∥2
F by Definition 5.4
⩽2∥A∥2
F by Eq. ( ∥AS∥Fbd) ( ∥TAS∥Fbd)
∥(AS)†AS−(TAS)†TAS∥⩽µε
d∥AS∥ by Corollary 5.6
⩽2µε
d∥A∥ by Eq. ( ∥AS∥bd) ( (AS)†ASAMP)
∥TAS∥2=∥(TAS)†TAS∥⩽(1+µε
d∥AS∥)∥AS∥2by Eq. ( (AS)†ASAMP)
⩽(1+2µε
d∥A∥)∥A∥2by Eq. ( ∥AS∥bd) ( ∥TAS∥bd)
One Clenshaw iteration. We are trying to perform the odd Clenshaw recurrence defined in
Eq. (Odd Clenshaw). The matrix analogue of this is
uk=2(2AA†−I)uk+1−uk+2+2a2k+1Ab, (Odd Matrix Clenshaw)
u=1
2(u0−u1).
We now show how to compute the next iterate ukgiven band the previous two iterates as
vk+1,vk+2∈ Cswhere uk+1=ASv k+1and uk+2=ASv k+2, for all k⩾0. The analysis begins by
showing that ukisε∥vk+1∥-close to the output of an exact, zero-error iteration. Next, we show
that∥vk∥isO(d)-close to its zero-error iteration value. Finally, we show that these errors don’t
accumulate too much towards the final outcome.
Starting from Eq. (Odd Matrix Clenshaw), we take the following series of approximations by
applying intermediate sketches:
4AA†uk+1−2uk+1−uk+2+a2k+1Ab
≈14AS(AS)†uk+1−2uk+1−uk+2+a2k+1Ab
=AS
4(AS)†(AS)vk+1−2vk+1−vk+2
+a2k+1Ab
≈2AS
4(TAS)†(TAS)vk+1−2vk+1−vk+2
+a2k+1Ab
≈3AS
4(TAS)†(TAS)vk+1−2vk+1−vk+2+a2k+1S†b
(32)
≈4AS(4(TAS)†B(k)vk+1−2vk+1−vk+2+a2k+1S†b)
≈5AS(4B(k)
†B(k)vk+1−2vk+1−vk+2+a2k+1S†b). (33)
The final expression, Eq. (33), is what the algorithm computes, taking
vk:= (4B(k)
†B(k)−2I)vk+1−vk+2+a2k+1S†b. (34)
Here, B(k)and B(k)
†are taken to be BEST r(TAS)and BEST r((TAS)†)forr=Θ(d4∥A∥2
F(s+t)1
δ) =
Θ(d6∥A∥4
F
(µε)2δlog(∥A∥F
δ∥A∥)). The runtime of each iteration is O(r), since the cost of producing the
sketches B(k)and B(k)
†using SQ(TAS)isO(r)(A2.I1), and actually computing the iteration costs
O(r+s+t) =O(r)(A2.I2).
40Remark 9.6. We could have stopped sketching at Eq. (32), without using BEST , and instead
took vk:=4(TAS)†(TAS)vk+1−2vk+1−vk+2+a2k+1S†b. The time to compute each iteration
increases to O(st), and the final running time is
Od5∥A∥4
F
µ4ε4log2∥A∥F
δ∥A∥
to achieve the guarantees of Theorem 9.3 with probability ⩾1−δ. This running time is worse
by a factor of d2/ε2, but scales logarithmically in failure probability.
As for approximation error, let ε1,ε2,ε3,ε4andε5be the errors introduced in the approximation
steps for Eq. (33). Using the previously established bounds on Sand T,
ε1⩽4∥AA†−AS(AS)†∥∥uk+1∥⩽4µε
d∥A∥∥uk+1∥ by Eq. ( AA†AMP)
ε2⩽4∥AS∥∥((AS)(AS)†−(TAS)(TAS)†)vk+1∥⩽16µε
d∥A∥2∥vk+1∥by Eq. ( (AS)†ASAMP)
ε3⩽|a2k+1|∥Ab−ASS†b∥⩽|a2k+1|µε
d∥b∥ by Eq. ( AbAMP)
The bounds on ε4andε5follow from the bounds in Section 5.1 applied to TAS . With probability
⩾1−δ/d, the following hold:
ε4⩽4∥AS(TAS)†(TAS−B(k))vk+1∥
⩽4∥AS(TAS)†∥F∥TAS∥F∥vk+1∥q
d/(rδ) by Corollary 5.3
⩽∥AS(TAS)†∥F∥TAS∥F∥vk+1∥µε
12∥A∥2
Fd5/2
⩽d−5/2µε∥A∥∥vk+1∥ by Eqs. ( ∥AS∥bd) and ( ∥TAS∥Fbd)
ε5⩽4∥AS((TAS)†−B(k)
†)B(k)vk+1∥
⩽4∥AS∥F∥TAS∥F∥B(k)vk+1∥q
d/(rδ) by Corollary 5.3
⩽4q
d/(rδ)∥AS∥F∥TAS∥F
∥TASv k+1∥+q
d/(rδ)∥It∥F∥TAS∥F∥vk+1∥
by Corollary 5.3
⩽1
3d−5/2µε
∥TASv k+1∥+d−3/2∥vk+1∥
by Eqs. ( ∥TAS∥Fbd) and ( ∥TAS∥bd)
⩽d−5/2µε∥vk+1∥. by ∥A∥⩽1
In summary, we can view the iterate of A2.I2 as computing
euk=2(2AA†−I)euk+1−euk+2+2a2k+1Ab+ε(k), (35)
where ε(k)∈ Cmis the error of the approximation in the iterate Eq. (33). We have showed that
∥ε(k)∥⩽ε1+ε2+ε3+ε4+ε5≲µε
d
∥vk+1∥+|a2k+1|∥b∥
.
Upon applying a union bound, we see that this bound on ε(k)holds for every kfrom 0 to d−1
with probability ⩾1−3δ.
Error accumulation across iterations. Now, we analyze how the error from one iteration
affects to the final output. Using the formulation of the iterate from Eq. (35), we notice that this
is the standard Clenshaw iteration Eq. (Clenshaw) with xreplaced with T2(A†) =2AA†−I
41and akreplaced with 2a2k+1Ab+ε(k). Following Remark 7.4 and Lemma 7.3, we conclude that
the output of Algorithm 2 satisfies
euk=d
∑
i=kUi−k(T2(A†))(2a2i+1Ab+ε(k));
eu:=1
2(eu0−eu1)
=d
∑
i=01
2(Ui(T2(A†))−Ui−1(T2(A†)))(2a2i+1Ab+ε(k))
=d
∑
i=0a2i+1T2i+1(A)b+d
∑
i=01
2(Ui(T2(A†))−Ui−1(T2(A†)))ε(k).
In other words, after completing the iteration, we have a vector eusuch that
∥eu−p(A)b∥⩽d
∑
i=01
2(Ui(T2(A†))−Ui−1(T2(A†)))ε(k)
⩽d
∑
i=0(2i+1)∥ε(k)∥
≲µεd
∑
k=0(∥vk+1∥+|a2k+1|∥b∥)
⩽ε∥b∥+µεd
∑
k=1∥vk∥. (36)
The last step follows from Item 9.3(a). So, it suffices to bound the vk’s. Recalling from Eq. (34),
the recursions defining them is
vk=4B(k)
†B(k)vk+1−2vk+1−vk+2+a2k+1S†b
=2(2(TAS)†(TAS)−I)vk+1−vk+2+a2k+1S†b+4(B(k)
†B(k)−(TAS)†(TAS))vk+1.
This is Eq. (Odd Matrix Clenshaw) on the matrix (TAS)†with an additional error term. Follow-
ing Remark 7.4, this solves to
vk=d
∑
i=kUi−k(T2(TAS))
a2i+1S†b+4(B(i)
†B(i)−(TAS)†(TAS))vi+1
.
Since B(k)and B(k)
†are all drawn independently, E[B(k)
†B(k)−(TAS)†(TAS)]is the zero matrix,
where the expectation is over the randomness of B(k)and B(k)
†. We use the following bound on
the variance of the error. In this computation, we use Eq. (13), which states that, for B=BEST(A)
with parameter rand Xpositive semi-definite, E[B†XB]⩽A†XA+1
rTr(X)∥A∥2
FI.
E
k(B(k)
†B(k)−(TAS)†(TAS))vk+12
=E
kB(k)
†B(k)vk+12
−(TAS)†(TAS)vk+12
=E
kh
(B(k)vk+1)†(B(k)
†)†B(k)
†(B(k)vk+1)i
−(TAS)†(TAS)vk+12
⩽E
k
(B(k)vk+1)†((TAS)(TAS)†+1
rTr(Is)∥TAS∥2
FIt)(B(k)vk+1)
−(TAS)†(TAS)vk+12
42⩽E
kh
v†
k+1(TAS)†((TAS)(TAS)†+s
r∥TAS∥2
FIt)(TAS)vk+1
+v†
k+11
rTr((TAS)(TAS)†+s
r∥TAS∥2
FIt)∥TAS∥2
Fvk+1i
−(TAS)†(TAS)vk+12
=s
r∥TAS∥2
F∥TASv k+1∥2+1
r+st
r2
∥TAS∥4
F∥vk+1∥2
⩽4s∥A∥2
F∥A∥2
r+∥A∥4
F
r+st∥A∥4
F
r2
∥vk+1∥2by Eqs. ( ∥TAS∥Fbd) and ( ∥TAS∥bd)
⩽δ
1000 d4∥A∥2∥vk+1∥2, (37)
where the last line uses r=Θ(d4∥A∥2
F(s+t)1
δ)(and is the bottleneck for the choice of r). Let
E[k,d]denote taking the expectation over B(i)and B(i)
†foribetween kand d(treating T,Sas
fixed).
¯vk:=E
[k,d][vk]=d
∑
i=kUi−k(T2(TAS))a2i+1S†b.
We first bound the recurrence in expectation, then we bound the second moment.
∥¯vk∥⩽d
∑
i=kUi−k(T2(TAS))a2i+1∥S†b∥ by sub-multiplicativity of ∥·∥
=d
∑
i=kUi−k(T2(x))a2i+1
Spec(TAS)∥S†b∥
⩽d
∑
i=kUi−k(T2(x))a2i+1
[−1−2µε
d,1+2µε
d]∥S†b∥ by Eq. ( ∥TAS∥bd)
⩽ed
∑
i=kUi−k(T2(x))a2i+1
sup∥S†b∥ by Lemma 4.3, µε⩽1
100d
⩽4d
∑
i=kUi−k(T2(x))a2i+1
sup∥b∥ by Eq. ( ∥S†b∥bd)
⩽41
µd∥b∥ by Item 9.3(b)
We now compute the second moment of vk.
E
[k,d]h
∥vk−¯vk∥2i
=E
[k,d]"d
∑
i=kUi−k(T2(TAS))4(B(i)
†B(i)−(TAS)†(TAS))vi+12#
Because the B(i)’s are independent, the variance of the sum is the sum of the variances, so
=d
∑
i=kE
[k,d]Ui−k(T2(TAS))4(B(i)
†B(i)−(TAS)†(TAS))vi+12
⩽16d
∑
i=kUi−k(T2(TAS))2
E
[k,d](B(i)
†B(i)−(TAS)†(TAS))vi+12
⩽16d
∑
i=ke2d2E
[k,d](B(i)
†B(i)−(TAS)†(TAS))vi+12
⩽16e2d
∑
i=kd2δ
1000 d4∥A∥2E
[i+1,d]h
∥vi+1∥2i
43⩽δ
2d2∥A∥2d
∑
i=kE
[i+1,d]h
∥vi+1∥2i
=δ
2d2∥A∥2d
∑
i=k(E
[i+1,d]h
∥vi+1−¯vi+1∥2i
+∥¯vi+1∥2)
⩽δ
2d2∥A∥2d
∑
i=k(E
[i+1,d]h
∥vi+1−¯vi+1∥2i
+16
µ2d2∥b∥2) by Eq. (37)
To bound this recurrence, we define the following recurrence ckto satisfy E[k,d][∥vk−¯vk∥2]⩽ck:
ck=γd
∑
i=k(ci+1+Γ) γ=δ
2d2∥A∥2,Γ=16
µ2d2∥b∥2.
For this recurrence, ck⩽dγΓfor all kbetween 0 and dprovided that dγ⩽1
2.
ck⩽8δ
µ2d3∥A∥2∥b∥2
We have shown that E[∥vk−¯vk∥2]⩽8δ
d(∥A∥∥b∥
µd)2. By Markov’s inequality, with probability
⩾1−δ/100, we have that for all k,∥vk∥≲∥b∥
µd. Returning to the final error bound Eq. (36),
∥eu−p(A)b∥≲ε∥b∥+µεd
∑
k=1∥vk∥≲ε∥b∥. (38)
Output description properties. After the iteration concludes, we can compute uby computing
x=1
2S(v0−v1)in linear O(s)time. Then, u=1
2(u0−u1) =1
2AS(v0−v1) = Ax. Note that
though x∈ Cn, its sparsity is at most the sparsity of x, which is bounded by s.
Further, using the prior bounds on v0and v1, we have that
n
∑
j=1A∗,j2|xi|2=s
∑
j=1[SA]∗,j21
2(v0−v1)i2
⩽s
∑
j=12
s∥A∥2
F1
2(v0−v1)i2
⩽2
s∥A∥2
F1
2(v0−v1)2
⩽2
s∥A∥2
F(∥v0∥2+∥v1∥2)
≲∥A∥2
F∥b∥2/(√
sµd)2
≲ε2∥b∥2/(d4log(∥A∥F
δ∥A∥)).
9.2 Generalizing to Even Polynomials
We also obtain an analogous result for even polynomials. For the most part, changes are
superficial; the red text indicates differences from Theorem 9.3. The major difference is that
the representation of the output is Ax+ηbinstead of Ax, which results from constant terms
being allowed when pis even. We state the theorem for estimating p(A†)bbecause it makes the
similarities with the odd setting more apparent.
44Theorem 9.7. Suppose we are given sampling and query access to A∈Cm×nand b∈Cmwith
∥A∥⩽1; a(2d)-degree even polynomial, written in its Chebyshev coefficients as
p(x) =d
∑
i=0a2iT2i(x);
an accuracy parameter ε>0; a failure probability parameter δ>0; and a stability parameter µ>0.
Then we can output a vector x∈ Cnandη∈ Csuch that ∥Ax+ηb−p(A†)b∥⩽ε∥p∥sup∥b∥with
probability ⩾1−δin time
O
min
nnz(A),d4∥A∥4
F
(µε)4log2 ∥A∥F
δ∥A∥
+d7∥A∥4
F
(µε)2δlog ∥A∥F
δ∥A∥
,
assuming µε<min(1
4d∥A∥,1
100d)andµsatisfies the following bounds. Below, ˜a2k:=a2k−a2k+2+
· · · ± a2d.
(a)µ∑d
i=1|˜a2i|⩽∥p∥supand d µ2∑d
i=1|˜a2i|2⩽∥p∥2
sup;
(b)µ∥∑d
i=k4˜a2i+2x·Ui−k(T2(x))∥sup⩽1
d∥p∥supfor all 0⩽k⩽d.
The output description has the additional properties
∑
jA∗,j2xj2≲ε2∥p∥2
sup∥b∥2
d4log(∥A∥F
δ∥A∥)∥x∥0≲d2∥A∥2
F
(µε)2log(∥A∥F
δ∥A∥),
so that by Corollary 4.10, for the output vector y :=Ax+ηb, we can:
(i) Compute entries of y in O(∥x∥0) =Od2∥A∥2
F
(µε)2log(∥A∥F
δ∥A∥)
time;
(ii)Sample i∈[n]with probability|yi|2
∥y∥2inO ∥p∥2
sup∥A∥2
F
(µd)2+p(0)2d2∥A∥2
F∥b∥2
µ2ε2∥y∥2log(∥A∥F
δ∥A∥)log1
δ
time with probability ⩾1−δ;
(iii) Estimate ∥y∥2toνrelative error in O ∥p∥2
sup∥A∥2
F
(µd)2+p(0)2d2∥A∥2
F∥b∥2
ν2µ2ε2∥y∥2log(∥A∥F
δ∥A∥)log1
δ
time
with probability ⩾1−δ.
Corollary 9.8 (Corollary of Proposition 9.10) .In Theorem 9.7, we can always take 1/µ≲d2log(d)
for d>1.
Recall the odd and even recurrences defined in Eqs. (Odd Clenshaw) and (Even Clenshaw).
uk=2(2AA†−I)uk+1−uk+2+2a2k+1Ab, (Odd Matrix Clenshaw)
p(A)b=u=1
2(u0−u1).
The matrix analogue of the even recurrence is identical except that the final term is 4˜a2k+2AA†b
instead of 2 a2k+1Ab.
˜a2k:=a2k−a2k+2+a2k+4− · · · ± a2d
uk=2(2AA†−1)uk+1−uk+2+4˜a2k+2AA†b, (Even Matrix Clenshaw)
p(A†)b=u=˜a0b+1
2(u0−u1).
So, a roughly identical analysis works upon making the appropriate changes. As before, we
assume ∥p∥sup=1 without loss of generality.
45Algorithm 3 (Even singular value transformation) .
Input (pre-processing): A matrix A∈Cm×n, vector b∈Cm, and parameters ε,δ,µ>0.
Pre-processing sketches: Lets,t=Θd2∥A∥2
F
(µε)2log(∥A∥F
δ∥A∥)
. This phase will succeed with
probability ⩾1−δ.
P1.IfSQ(A†)and SQ(b)are not given, compute data structures to simulate them in
O(1)time;
P2. Sample S∈ Cn×sto be AMP s
A,A†,{∥A∗,j∥2
∥A∥2
F}j∈[n]
(Definition 5.4);
P3.Sample T†∈ Cm×tto be AMP t
S†A†,AS,{1
2(∥[AS]i,∗∥2
∥AS∥2
F+|bi|2
∥b∥2)}i∈[m]
(Defini-
tion 5.4);
P4. Compute a data structure that can respond to SQ (TAS)queries in O(1)time;
Input: A degree 2 dpolynomial p(x) =∑d
i=0a2iT2i(x)given as its coefficients a2i.
Compute all ˜a2k=a2k−a2k+2+· · · ± a2d.
Clenshaw iteration: Letr=Θ(d4∥A∥2
F(s+t)1
δ) =Θ(d6∥A∥4
F
(µε)2δlog(∥A∥F
δ∥A∥)). This phase will
succeed with probability ⩾1−δ. Starting with vd+1=vd+2=⃗0sand going until v0,
I1. Let B(k)=BEST r(TAS)and B(k)
†=BEST r((TAS)†)(Definition 5.1);
I2. Compute vk=2(2B(k)
†B(k)−I)vk+1−vk+2+4˜a2k+2B(k)
†Tb.
Output: Output x=1
2S(v0−v1)andη=˜a0satisfyingAx+ηb−p(A†)b⩽ε∥p∥sup∥b∥.
Pre-processing sketches, correctness. Though the sketches are chosen to be slightly different
because of the different parity, all of the sketching bounds used for the odd SVT analysis hold
here, up to rescaling s,tby constant factors. This includes Eqs. ( ∥[AS]∗,j∥bd), (∥AS∥Fbd), ( AA†
AMP), ( ∥AS∥bd), (∥TAS∥Fbd), ( (AS)†ASAMP) and ( ∥TAS∥bd). What remains (Eqs. ( ∥S†b∥
bd) and ( AbAMP)) have analogues that follow from the same argument:
∥Tb∥2⩽2∥b∥2(∥Tb∥bd)
∥(AS)†b−(TAS)†Tb∥⩽µε
d∥b∥ ((AS)†bAMP)
All this holds with probability ⩾1−δ.
One (even) Clenshaw iteration. As with the odd case, we perform the recurrence on ukby
updating vksuch that uk= (AS)vk. The error analysis proceeds by bounding
4AA†uk+1−2uk+1−uk+2+4˜a2k+2AA†b
≈14AS(AS)†uk+1−2uk+1−uk+2+4˜a2k+2AA†b
=AS
4(AS)†(AS)vk+1−2vk+1−vk+2
+4˜a2k+2AA†b
≈2AS
4(TAS)†(TAS)vk+1−2vk+1−vk+2
+4˜a2k+2AA†b
≈3AS
4(TAS)†(TAS)vk+1−2vk+1−vk+2+4˜a2k+2(AS)†b
≈4AS
4(TAS)†B(k)vk+1−2vk+1−vk+2+4˜a2k+2(AS)†b
46≈5AS
4B(k)
†B(k)vk+1−2vk+1−vk+2+4˜a2k+2(AS)†b
≈6AS
4(TAS)†(TAS)vk+1−2vk+1−vk+2+4˜a2k+2(TAS)†Tb
≈7AS
4B(k)
†B(k)vk+1−2vk+1−vk+2+4˜a2k+2B(k)
†Tb
. (39)
So, our update is
vk=4B(k)
†B(k)vk+1−2vk+1−vk+2+4˜a2k+2B(k)
†Tb. (40)
As before, we can label the error incurred by each approximation in Eq. (39) with ε1,. . .,ε7.
The approximations in ε1,ε2,ε4,ε5do not involve the constant term and so can be bounded
identically to the odd case.
ε1⩽4∥AA†−AS(AS)†∥∥uk+1∥⩽4µε
d∥A∥∥uk+1∥
ε2⩽4∥AS∥∥((AS)(AS)†−(TAS)(TAS)†)vk+1∥⩽16µε
d∥A∥2∥vk+1∥
ε4⩽4∥AS(TAS)†(TAS−B(k))vk+1∥⩽d−5/2µε∥A∥∥vk+1∥
ε5⩽4∥AS((TAS)†−B(k)
†)B(k)vk+1∥⩽d−5/2µε∥vk+1∥.
The approximation in ε3goes through with a slight modification.
ε3⩽|4˜a2k+2|∥AA†b−AS(AS)†b∥⩽4|˜a2k+2|µε
d∥A∥∥b∥ by Eq. ( AA†AMP)
The approximations ε6andε7follow from similar arguments.
ε6⩽|4˜a2k+2|∥AS∥∥(AS)†b−(TAS)†Tb∥
⩽|8˜a2k+2|µε
d∥b∥ by Eqs. ( ∥[AS]∗,j∥bd) and ( (AS)†bAMP)
ε7⩽|4˜a2k+2|∥AS((TAS)†−B(k)
†)Tb∥
⩽|4˜a2k+2|∥AS∥F∥TAS∥F∥Tb∥q
d/(rδ) by Corollary 5.3
⩽|˜a2k+2|d−5/2µε∥b∥ by Eqs. ( ∥TAS∥bd) and ( ∥Tb∥bd)
In summary, we can view the iterate of A2.I2 as computing
euk=2(2AA†−I)euk+1−euk+2+4˜a2k+2AA†b+ε(k), (41)
where ε(k)∈ Cmis the error of the approximation in the iterate Eq. (33), and
∥ε(k)∥⩽ε1+ε2+ε3+ε4+ε5+ε6+ε7
≲µε
d
∥vk+1∥+|˜a2k+2|∥b∥
.
Upon applying a union bound, we see that this bound on ε(k)holds for every kfrom 0 to d−1
with probability ⩾1−4δ.
Error accumulation across iterations. The error accumulates in the same way as in the odd
setting. Using the formulation of the iterate from Eq. (41), we notice that this is the standard
Clenshaw iteration Eq. (Clenshaw) with xreplaced with T2(A†) =2AA†−Iand akreplaced
with 4˜a2k+2AA†b+ε(k). Following Remark 7.4 and Lemma 7.3, we conclude that the output of
Algorithm 3 satisfies
euk=d
∑
i=kUi−k(T2(A†))(4˜a2i+2AA†b+ε(i));
47eu:=˜a0+1
2(eu0−eu1)
=˜a0+d
∑
i=01
2(Ui(T2(A†))−Ui−1(T2(A†)))(4˜a2i+2AA†b+ε(k))
=d
∑
i=0a2iT2(i−k)(A†)b+d
∑
i=01
2(Ui(T2(A†))−Ui−1(T2(A†)))ε(k).
In other words, after completing the iteration, we have a vector eusuch that
∥eu−p(A†)b∥⩽d
∑
i=01
2(Ui(T2(A†))−Ui−1(T2(A†)))ε(k)
⩽d
∑
i=0(2i+1)∥ε(k)∥
≲µεd
∑
k=0(∥vk+1∥+|˜a2k+2|∥b∥)
⩽ε∥b∥+µεd
∑
k=1∥vk∥. (42)
In the last line, we use the assumption Item 9.7(a). It suffices to bound the vk’s. Recalling from
Eq. (40), the recursions defining them is
vk=4B(k)
†B(k)vk+1−2vk+1−vk+2+4˜a2k+2B(k)
†Tb
=2(2(TAS)†(TAS)−I)vk+1−vk+2+4˜a2k+2B(k)
†Tb+4(B(k)
†B(k)−(TAS)†(TAS))vk+1.
Following Remark 7.4, this solves to
vk=d
∑
i=kUi−k(T2(TAS))
4˜a2i+2B(i)
†Tb+4(B(i)
†B(i)−(TAS)†(TAS))vi+1
.
From here, the identical analysis applies.
¯vk:=E
[k,d][vk]=d
∑
i=kUi−k(T2(TAS))4˜a2i+2(TAS)†Tb
∥¯vk∥⩽d
∑
i=kUi−k(T2(TAS))4˜a2i+2(TAS)†∥Tb∥⩽41
µd∥b∥.
We now compute the second moment of vk. The main difference from the odd setting is that
there is an additional term, 4 ˜a2k+2(B(k)
†−(TAS)†)Tb, where
E
k(B(k)
†−(TAS)†)Tb2
⩽1
rTr(Is)∥TAS∥2
F∥Tb∥2by Eq. (13)
⩽δ
1000 d4∥b∥2. by Eq. ( ∥Tb∥bd), r=Θ(d4∥A∥2
F(s+t)1
δ)
We use this and the derivation in Eq. (37) to conclude
E
[k,d]h
∥vk−¯vk∥2i
=E
[k,d]"d
∑
i=kUi−k(T2(TAS))4(˜a2i+2(B(i)
†−(TAS)†)Tb+ (B(i)
†B(i)−(TAS)†(TAS))vi+1)2#
48=d
∑
i=kE
[k,d]Ui−k(T2(TAS))4(˜a2i+2(B(i)
†−(TAS)†)Tb+ (B(i)
†B(i)−(TAS)†(TAS))vi+1)2
⩽16d
∑
i=kUi−k(T2(TAS))2
E
[k,d]˜a2i+2(B(i)
†−(TAS)†)Tb+ (B(i)
†B(i)−(TAS)†(TAS))vi+12
⩽32d
∑
i=ke2d2E
[k,d]˜a2i+2(B(i)
†−(TAS)†)Tb2
+(B(i)
†B(i)−(TAS)†(TAS))vi+12
⩽32d
∑
i=ke2d2E
[k,d]δ|˜a2i+2|2
1000 d4∥b∥2+δ
1000 d4∥A∥2∥vk+1∥2
⩽δ
4d2∥b∥2d
∑
i=k|˜a2i+2|2+δ∥A∥2
4d2d
∑
i=kE
[k,d]h
∥vk+1∥2i
⩽δ
4µ2d3∥b∥2+δ∥A∥2
4d2d
∑
i=kE
[k,d]h
∥vk+1∥2i
by Item 9.7(a)
=δ
4µ2d3∥b∥2+δ∥A∥2
4d2d
∑
i=k(E
[i+1,d]h
∥vi+1−¯vi+1∥2i
+∥¯vi+1∥2)
⩽δ
4µ2d3∥b∥2+δ∥A∥2
4d2d
∑
i=k(E
[i+1,d]h
∥vi+1−¯vi+1∥2i
+16
µ2d2∥b∥2) by Eq. (37)
⩽δ∥A∥2
4d2d
∑
i=k(E
[i+1,d]h
∥vi+1−¯vi+1∥2i
+17
µ2d2∥b∥2)
By the same recurrence argument, this is
⩽9δ
µ2d3∥A∥2∥b∥2. (43)
We have shown that E[∥vk−¯vk∥2]⩽9δ
d(∥A∥∥b∥
µd)2. By Markov’s inequality, with probability
⩾1−δ/100, we have that for all k,∥vk∥≲∥b∥
µd. Returning to the final error bound Eq. (42),
∥eu−p(A†)b∥≲ε∥b∥+µεd
∑
k=1∥vk∥≲ε∥b∥. (44)
Output description properties. The argument from the odd case shows that
∑
j∥A∗,j∥2|xj|2≲ε2∥b∥2
d4log(∥A∥F
δ∥A∥)
and∥x∥0⩽s. By Corollary 4.10 we get the desired bounds. Notice that ˜a0=a0−a2+a4−
· · · ± a2d=p(0).
9.3 Bounding Iterates of Singular Value Transformation
We give bounds on the value of µthat suffices for a generic polynomial. Though bounds may
be improvable for specific functions, they are tight up to logfactors for Chebyshev polynomials
Tk(x), and improve by a factor of dover naive coefficient-wise bounds.
Proposition 9.9. Letp(x)be an odd polynomial with degree 2d+1, with a Chebyshev series expansion
of p(x) =∑d
i=0a2i+1T2i+1(x). Then ∑d
i=0|a2i+1|⩽2d∥p∥supand, for all integers k ⩽d,
d
∑
i=ka2i+1Ui−k(T2(x))
sup≲(d−k+1)(1+log2(d+1))∥p∥sup.
49Proof. Without loss of generality, we take ∥p∥sup=1. By Lemma 4.2, |a2i+1|⩽2, giving the first
conclusion. Towards the second conclusion, we first note that
d
∑
i=ka2i+1Ui−k(T2(x))
sup=d
∑
i=ka2i+1Ui−k(x)
sup,
since T2maps [−1, 1]to[−1, 1]. Then, we use the strategy from Theorem 8.4, writing out Ui−k
with Eq. (7) and then bounding the resulting coefficients. Note that, by convention, aiand Ui
are zero for any integer i∈Zfor which they are not defined.
d
∑
i=ka2i+1Ui−k(x) =∑
ia2i+1Ui−k(x)
=∑
ia2i+1∑
j⩾0Ti−k−2j(x)(1+Ji−k−2j̸=0K)
=∑
j⩾0∑
ia2i+1Ti−k−2j(x)(1+Ji−k−2j̸=0K)
=∑
j⩾0∑
ia2(i+k+2j)+1Ti(x)(1+Ji̸=0K)
=∑
iTi(x)(1+Ji̸=0K)∑
j⩾0a2(i+k+2j)+1
d
∑
i=ka2i+1Ui−k(x)
sup⩽∑
i(1+Ji̸=0K)∥Ti(x)∥sup∑
j⩾0a2(i+k+2j)+1
=d−k
∑
i=0(1+Ji̸=0K)∑
j⩾0a2(i+k)+1+4j
⩽d−k
∑
i=0(1+Ji̸=0K)(32+8 log2(2d+2)) by Corollary 6.5
= (2(d−k) +1)(32+8 log2(2d+2))
≲(d−k+1)(1+log2(d+1))
Proposition 9.10. Letp(x)be an even polynomial with degree 2d, written as p(x) =∑d
i=0a2iT2i(x).
Let˜a2k:=a2k−a2k+2+· · · ± a2d. Then
d
∑
i=0|˜a2i|⩽4(d+1)(1+log(d+1))∥p∥sup,
andd
∑
i=0|˜a2i|2⩽32(d+1)(1+log2(d+1))∥p∥sup,
and, for all integers k ⩽d,
d
∑
i=k4˜a2i+2x·Ui−k(T2(x))
sup≲(d−k+1)(1+log(d+1))∥p∥sup.
Proof. Without loss of generality, we take ∥p∥sup=1. Consider the polynomial q(x) =
∑d
i=0biTi(x)where bi:=a2i. By Eq. (8), p(x) = q(T2(x)), and because T2maps [−1, 1]to
[−1, 1],∥q∥sup=∥q∥sup=1. Then by Fact 6.3,
|˜a2k|=|bk−bk+1+· · · ± bd|⩽
4+4
π2log(max(k, 1))
∥q∥sup=4+4
π2log(max(k, 1)).
50From this follows the first two conclusions. For the final conclusion, by Eq. (9),
d
∑
i=k4˜a2i+2xUi−k(T2(x)) =d
∑
i=k2˜a2i+2U2(i−k)+1(x) =∑
i2˜a2i+2U2(i−k)+1(x).
From here, we proceed as in the proof of Theorem 8.4.
∑
i˜a2i+2U2(i−k)+1(x) =∑
i∑
r⩾0(−1)rbi+r+12∑
sT2s+1(x)Js⩽i−kK
=2∑
sT2s+1(x)∑
i∑
r⩾0Js⩽i−kK(−1)rbi+r+1
=2∑
sT2s+1(x)∑
tbt∑
i∑
rJr⩾0KJt=i+r+1KJs⩽i−kK(−1)r
=2∑
sT2s+1(x)∑
tbt∑
rJr⩾0KJs⩽t−r−1−kK(−1)r
=2∑
sT2s+1(x)∑
tbtt−s−k−1
∑
r=0(−1)r
=2∑
sT2s+1(x)∑
tbtJt−s−k−1∈2Z⩾0K
=2∑
sT2s+1(x)∑
t⩾0b2t+s+k+1
∑
i˜a2i+2U2(i−k)+1(x)
sup⩽2d−k
∑
s=0∑
t⩾0b2t+s+k+1
⩽2d−k
∑
s=0
4+4
π2log(max(s+k, 1))
by Fact 6.3
≲(d−k+1)(1+log(d+1))
10 Dequantizing QML Algorithms
In this section, we focus on providing classical algorithms for regression, low-rank approxima-
tion and Hamiltonian simulation. We show that plugging in the appropriate polynomials into
our algorithm corresponding to Theorem 9.1 results in the fastest algorithms for these problems.
10.1 Recommendation Systems
Given a matrix A, the quantum recommendation system problem, introduced by Kerenidis
and Prakash [KP20], is to output a sample from a row of a low-rank approximation of A. The
quantum algorithm for this approaches this in a different way from the standard classical
algorithms, by taking a polynomial approximation of a threshold function and using quantum
linear algebra techniques to apply it to A. The threshold function is as follows:
Lemma 10.1 (Polynomial approximations of the rectangle function [GSLW19, Lemma 29]) .Let
δ′,ε′∈(0,1
2)andt∈[−1, 1]. There exists an even polynomial p∈ R[x]of degree O(log(1
ε′)/δ′),
such that |p(x)|⩽1for all x ∈[−1, 1], and
p(x)∈(
[0,ε′] for all x ∈[−1,−t−δ′]∪[t+δ′, 1], and
[1−ε′, 1]for all x ∈[−t−δ′,t+δ′]
51In particular, one obtains the quantum recommendation systems result from QSVT is by
preparing the state |Ai,∗⟩and applying a block-encoding of p(A)to get a copy of |p(A)Ai,∗⟩,
where pis the polynomial from Lemma 10.1.19We obtain the same guarantee classically:
Corollary 10.2 (Dequantizing recommendation systems) .Suppose we are given a matrix A∈Cm×n
such that 0.01⩽∥A∥⩽1and0<ε,σ<1, withO(nnz(A))time pre-processing. Then there exists
an algorithm that, given an index i∈[m], computes a vector ysuch that ∥y−p(A)Ai,∗∥⩽ε∥Ai,∗∥
with probability at least 0.9, where p(x)is the rectangle polynomial from Lemma 10.1, with parameters
t=σ,δ′=σ/6,ε′=ε. Further, the running time to compute such a description of y is
eO∥A∥4
F
σ11ε2
,
and from this description we can sample from y in eO
∥A∥4
F∥Ai,∗∥2
σ8ε2∥y∥2
time.
Thinking of p(A)Aas the low-rank approximation of Athis algorithm gives access to, we
see that the error guarantee of Corollary 10.2 implies the error guarantee achieved by prior
quantum-inspired algorithms [Tan19; CGLLTW22].
Remark 10.3 (Detailed comparison to [CCHLW22, Theorem 26]) .The recommendation systems
algorithm of Chepurko, Clarkson, Horesh, Lin, and Woodruff outputs a rank- kmatrix Zsuch
that
∥A−Z∥2
F⩽(1+O(ε))∥A−Ak∥2
F, (45)
where Akis the best rank- kapproximation to A, ineO(k3
ε6+k∥A∥4
F
(∥A−Ak∥2
F/k+σ2
k)σ2
kε4) =eO(k3
ε6+k∥A∥4
F
σ4
kε4)
time20, under the assumption that ε⩽∥Ak∥2
F
∥A∥2
F. This guarantee is the typical one for low-rank ap-
proximation problems. The authors use that ℓ2
2importance sampling sketches oversample ridge
leverage score sketch in certain parameter regimes, which is why dependences on the norm of
the low-rank approximation ∥Ak∥Fand its residual ∥A−Ak∥Fappear. For the recommendation
systems task, it’s not clear whether this style of guarantee is as good as the guarantee achieved
by the quantum algorithm. The quantum guarantee implies the classical one in the regime where
additive and relative error are comparable: ∥A−Z∥F⩽∥A−p(A)A∥F+∥p(A)A−Z∥F⩽
∥A−Ak∥F+O(ε∥A∥F), where the second inequality uses that p(A)Ais approximately a low-
rank approximation that thresholds at σ, and so is at least as good of an approximation to A
asAk, at the cost of being potentially higher rank. Going from the classical guarantee to the
quantum one loses a quadratic factor: it can be the case that the guarantee from Eq. (45) holds
but∥Ak−Z∥F=Ω(√ε∥A∥F).21To achieve the precise guarantee of the quantum algorithm
would require setting ε←ε2, giving a running time of
eOk3
ε12+k∥A∥4
F
σ4
kε8
.
We improve over this running time when ε=O(k3/10σ11/10
∥A∥4/10
F)or when ε=O(k1/6σ7/6).
19The original paper goes through a singular value estimation procedure; see the discussion in Section 3.6 of
[GSLW19] for more details.
20For ease of comparison, we suppose we know σkand∥A−Ak∥2
Fexactly, in which case ψλ⩽ψkand k⩽ψk, so
the running time is eO(k3/ε6+kψλψk/ε4), using the notation from [CCHLW22].
21Take, for example, A= [2
1]and the rank k=1approximation Z=zz†where z= [√
2+√ε√ε]. More generally,
by [Tan19, Theorem 4.7], Eq. (45) implies ∥Z−A⩾σ∥F=O(√ε∥A−Ak∥F)(for certain forms of Z).
5210.2 Linear Regression
Next, we consider linear regression: given a matrix A, a vector b, and a parameter κ>0, the
quantum algorithm obtains a state close to |f(A)b⟩, where f(x)is a polynomial approximation
to1/xin the interval [−1,−1/κ]∪[1/κ, 1][GSLW19, Theorem 41]. Formally, this polynomial is
the Chebyshev truncation of (1−(1−x2)b)/xforb=⌈κ2log(κ/ε)⌉, multiplied by the rectangle
function from Lemma 10.1 to keep the truncation bounded.
Lemma 10.4 (Polynomial approximations of 1/x, [GSLW19, Lemma 40], following [CKS17]) .
Letκ>1and0<ε<1
2. There is an odd polynomial p(x)of degree O(κlog(κ
ε))with the properties
that
•|p(x)−1/x|⩽εfor x∈[−1,−1/κ]∪[1/κ, 1];
•|p(x)|=O(κlogκ
ε).
We obtain the following corollary by invoking Theorem 9.1 with the polynomial from Lemma 10.4.
Corollary 10.5 (Dequantizing linear regression) .Given a matrix A∈Cm×nsuch that 0.01⩽
∥A∥⩽1; a vector b∈Cm; and parameters ε, 1/κbetween 0and1, withO(nnz(A) +nnz(b))time
pre-processing. Then there exists an algorithm that outputs a vector ysuch that ∥y−p(A)b∥⩽ε∥b∥/κ
with probability at least 0.9, where pis the polynomial from Lemma 10.4 with parameters κ,ε. Further,
the running time to compute a description of y is
eOκ11∥A∥4
F
ε2
,
and from this description we can output a sample from y in eO
κ10∥A∥4
F∥b∥2
ε2∥y∥2
time.
Remark 10.6 (Comparison to [CCHLW22, Theorem 24]) .Chepurko, Clarkson, Horesh, Lin,
and Woodruff solves the regularized regression problem, where the goal is to output a vector
close to x∗=arg min x∥Ax−b∥2+λ∥x∥2for a given λ>0. Their algorithm outputs a vector
y= (SA)†vsuch that
∥y−x∗∥⩽ε
∥x∗∥+2∥b∥∥x∗∥
∥AA+x∗∥+1√
λ∥Πλ,⊥b∥
,
where Πλ,⊥is the projection onto the left singular vectors of SAwhose singular values are at
most√
λ. This algorithm achieves a running time of eO(∥A∥4
F(∥A∥2+λ)2log(d)/((σ2+λ)4ε4)),
where σis the minimum singular value of A. Since x∗=f(A)bis singular value transformation
for the function f(x) =x/(x2+λ), this is different from the setting we consider. They become
comparable when we take λ→0, in which case we must assume that bis in the image of A, so
that∥Πλ,⊥b∥tends to zero, and we depend on the minimum singular value of A. As discussed
in Remark 1.5, these assumptions are strong, and different from the setting we consider.
10.3 Hamiltonian Simulation
Finally, we give a classical algorithm for Hamiltonian simulation: for a Hermitian matrix
H∈Cn×n; a vector b∈Cn; and a time t∈R, the goal is to produce a description of eiHtb. We
begin by describing our polynomial approximation to eix=cos(x) +i sin(x).
Lemma 10.7 (Polynomial approximation to trigonometric functions, [GSLW19, Lemma 57]) .
Given t∈Randε∈(0, 1/e), letr=Θ(t+log(1/ε)
log log (1/ε)). Then, the following polynomials c(x)(even
53with degree 2r) and s (x)(odd with degree 2r+1),
c(x) = J0(t)−2∑
i∈[1,r](−1)iJ2i(t)T2i(x)
s(x) =2∑
i∈[0,r](−1)iJ2i+1(t)T2i+1(x),
satisfy that ∥cos(tx)−c(x)∥sup⩽εand∥sin(tx)−s(x)∥sup⩽ε. Here, Ji(x)is the i-th Bessel
function of the first kind [DLMF, (10.2.2)].
Corollary 10.8 (Dequantizing Hamiltonian simulation) .Given a Hermitian Hamiltonian H∈Cn×n
such that 0.01⩽∥H∥⩽1; a vector b∈Cn; a time t>0; and 0<ε<1, withO(nnz(A) +nnz(b))
pre-processing. Then there exists an algorithm that outputs a vector ysuch thaty−eiHtb⩽ε∥b∥
with probability ⩾0.9. Further, the running time to compute such a description of y is
eOt11∥H∥4
F
ε2
,
and from this description we can output a sample from y in eOt8∥H∥4
F∥b∥2
ε2∥y∥2
time.
Proof. Letc(x)and s(x)be the polynomials from Lemma 10.7. We apply Theorem 9.1 to get
descriptions of cand ssuch that ∥yc−c(H)b∥⩽ε∥b∥and∥ys−s(H)b∥⩽ε∥b∥. Then
eiHtb=cos(Ht)b+i sin(Ht)b≈ε∥b∥c(H)b+is(H)b≈ε∥b∥yc+iys.
This gives us a description O(ε)-close to eiHt. Using Corollary 4.10, we can get a sample from
this output in the time described, by combining the two descriptions of ycand ys.
Acknowledgments
ET and AB thank Nick Trefethen, Simon Foucart, Alex Townsend, Sujit Rao, and Victor Reis for
helpful discussions. ET thanks t.f. for the support. AB is supported by Ankur Moitra’s ONR
grant. ET is supported by the NSF GRFP (DGE-1762114).
References
[DLMF] NIST Digital Library of Mathematical Functions .https://dlmf.nist.gov/ , Release
1.1.10 of 2023-06-15. F. W. J. Olver, A. B. Olde Daalhuis, D. W. Lozier, B. I.
Schneider, R. F. Boisvert, C. W. Clark, B. R. Miller, B. V . Saunders, H. S. Cohl,
and M. A. McClain, eds. URL:https://dlmf.nist.gov/ (page 54).
[AA18] Scott Aaronson and Andris Ambainis. “Forrelation: a problem that optimally
separates quantum from classical computing”. In: SIAM Journal on Computing
47.3 (Jan. 2018), pp. 982–1038. DOI:10.1137/15m1050902 . arXiv: 1411.5729
[quant-ph] (page 14).
[AC17] Scott Aaronson and Lijie Chen. “Complexity-Theoretic Foundations of Quan-
tum Supremacy Experiments”. In: 32ndComputational Complexity Conference
(CCC 2017) . Vol. 79. Leibniz International Proceedings in Informatics (LIPIcs).
Dagstuhl, Germany: Schloss Dagstuhl–Leibniz-Zentrum fuer Informatik, 2017,
22:1–22:67. DOI:10.4230/LIPIcs.CCC.2017.22 (page 14).
54[AHK06] Sanjeev Arora, Elad Hazan, and Satyen Kale. “A fast random sampling algo-
rithm for sparsifying matrices”. In: Approximation, Randomization, and Combi-
natorial Optimization. Algorithms and Techniques . Springer, 2006, pp. 272–279
(page 13).
[AL16] Zeyuan Allen-Zhu and Yuanzhi Li. “LazySVD: even faster SVD decomposition
yet without agonizing pain”. In: Advances in neural information processing systems
29 (2016) (page 13).
[AM07] Dimitris Achlioptas and Frank Mcsherry. “Fast computation of low-rank matrix
approximations”. In: Journal of the ACM 54.2 (Apr. 2007), p. 9. DOI:10.1145/
1219092.1219097 (pages 10, 13).
[BCW22] Ainesh Bakshi, Kenneth L. Clarkson, and David P . Woodruff. “Low-rank ap-
proximation with 1/ε1/3matrix-vector products”. In: Proceedings of the 54th
Annual ACM SIGACT Symposium on Theory of Computing . STOC ’22. ACM, June
2022. DOI:10.1145/3519935.3519988 . arXiv: 2202.05120 [cs.DS] (page 13).
[BKKS21] Vladimir Braverman, Robert Krauthgamer, Aditya R Krishnan, and Shay Sapir.
“Near-optimal entrywise sampling of numerically sparse matrices”. In: Confer-
ence on Learning Theory . PMLR. 2021, pp. 759–773 (pages 10, 13).
[BKLLSW19] Fernando G. S. L. Brandão, Amir Kalev, Tongyang Li, Cedric Yen-Yu Lin, Krysta
M. Svore, and Xiaodi Wu. “Quantum SDP solvers: large speed-ups, optimality,
and applications to quantum learning”. In: 46thInternational Colloquium on
Automata, Languages, and Programming (ICALP 2019) . Vol. 132. Leibniz Interna-
tional Proceedings in Informatics (LIPIcs). Schloss Dagstuhl – Leibniz-Zentrum
für Informatik, 2019, 27:1–27:14. ISBN : 978-3-95977-109-2. DOI:10.4230/LIPICS.
ICALP.2019.27 . arXiv: 1710.02581 [quant-ph] (page 1).
[BKM22] Vladimir Braverman, Aditya Krishnan, and Christopher Musco. “Sublinear
time spectral density estimation”. In: Proceedings of the 54thAnnual ACM
SIGACT Symposium on Theory of Computing . STOC 2022. Rome, Italy: Asso-
ciation for Computing Machinery, 2022, pp. 1144–1157. ISBN : 9781450392648.
DOI:10.1145/3519935.3520009 (page 9).
[BMNGBN21] Ryan Babbush, Jarrod R McClean, Michael Newman, Craig Gidney, Sergio
Boixo, and Hartmut Neven. “Focus beyond quadratic speedups for error-
corrected quantum advantage”. In: PRX Quantum 2.1 (2021), p. 010103 (page 14).
[CCHLW22] Nadiia Chepurko, Kenneth Clarkson, Lior Horesh, Honghao Lin, and David
Woodruff. “Quantum-inspired algorithms from randomized numerical linear
algebra”. In: Proceedings of the 39thInternational Conference on Machine Learning .
Vol. 162. Proceedings of Machine Learning Research. PMLR, 2022, pp. 3879–
3900. arXiv: 2011.04125 [cs.DS] .URL:https://proceedings.mlr.press/v162/
chepurko22a.html (pages 2, 4, 9, 12, 52, 53).
[CEMMP15] Michael B Cohen, Sam Elder, Cameron Musco, Christopher Musco, and Madalina
Persu. “Dimensionality reduction for k-means clustering and low rank approx-
imation”. In: Proceedings of the forty-seventh annual ACM symposium on Theory of
computing . 2015, pp. 163–172 (page 12).
55[CGJ19] Shantanav Chakraborty, András Gilyén, and Stacey Jeffery. “The power of
block-encoded matrix powers: improved regression techniques via faster Hamil-
tonian simulation”. In: 46thInternational Colloquium on Automata, Languages,
and Programming (ICALP 2019) . LIPIcs. Schloss Dagstuhl, 2019. DOI:10.4230/
LIPIcs.ICALP.2019.33 . arXiv: 1804.01973 [quant-ph] (pages 1, 4).
[CGLLTW20] Nai-Hui Chia, András Gilyén, Han-Hsuan Lin, Seth Lloyd, Ewin Tang, and
Chunhao Wang. “Quantum-inspired algorithms for solving low-rank linear
equation systems with logarithmic dependence on the dimension”. In: 31st
International Symposium on Algorithms and Computation (ISAAC 2020) . Vol. 181.
Leibniz International Proceedings in Informatics (LIPIcs). Published version
of [CLW18] and [GLT20]. Schloss Dagstuhl–Leibniz-Zentrum für Informatik,
2020, 47:1–47:17. ISBN : 978-3-95977-173-3. DOI:10.4230/LIPIcs.ISAAC.2020.47
(page 2).
[CGLLTW22] Nai-Hui Chia, András Pal Gilyén, Tongyang Li, Han-Hsuan Lin, Ewin Tang,
and Chunhao Wang. “Sampling-based sublinear low-rank matrix arithmetic
framework for dequantizing quantum machine learning”. In: Journal of the
ACM 69.5 (Oct. 2022), pp. 1–72. DOI:10.1145/3549524 . arXiv: 1910.06151
[cs.DS] (pages 2–7, 11, 13–17, 37, 39, 52).
[CHM21] Jordan Cotler, Hsin-Yuan Huang, and Jarrod R. McClean. “Revisiting dequanti-
zation and quantum advantage in learning tasks”. 2021. DOI:10.48550/ARXIV.
2112.00811 . arXiv: 2112.00811 [quant-ph] (page 13).
[Cil+18] Carlo Ciliberto, Mark Herbster, Alessandro Davide Ialongo, Massimiliano
Pontil, Andrea Rocchetto, Simone Severini, and Leonard Wossnig. “Quantum
machine learning: a classical perspective”. In: Proceedings of the Royal Society A:
Mathematical, Physical and Engineering Sciences 474.2209 (Jan. 2018), p. 20170551.
DOI:10.1098/rspa.2017.0551 . arXiv: 1707.08561 (page 1).
[CKS17] Andrew M. Childs, Robin Kothari, and Rolando D. Somma. “Quantum algo-
rithm for systems of linear equations with exponentially improved dependence
on precision”. In: SIAM Journal on Computing 46.6 (2017), pp. 1920–1950. DOI:
10.1137/16M1087072 (page 53).
[Cle55] C. W. Clenshaw. “A note on the summation of Chebyshev series”. In: Math.
Tables Aids Comput. 9 (1955), pp. 118–120. ISSN : 0891-6837. DOI:10.1090/s0025-
5718-1955-0071856-0 (pages 6, 8, 9).
[CLW18] Nai-Hui Chia, Han-Hsuan Lin, and Chunhao Wang. “Quantum-inspired sub-
linear classical algorithms for solving low-rank linear systems”. 2018. arXiv:
1811.04852 [cs.DS] (page 56).
[CNW16] Michael B Cohen, Jelani Nelson, and David P Woodruff. “Optimal approximate
matrix product in terms of stable rank”. In: 43rdInternational Colloquium on
Automata, Languages, and Programming (ICALP 2016) . Schloss Dagstuhl-Leibniz-
Zentrum fuer Informatik. 2016 (pages 12, 20).
[CW23] Yanlin Chen and Ronald de Wolf. “Quantum algorithms and lower bounds for
linear regression with norm constraints”. In: 50thInternational Colloquium on
Automata, Languages, and Programming (ICALP 2023) . Vol. 261. Leibniz Interna-
tional Proceedings in Informatics (LIPIcs). Schloss Dagstuhl – Leibniz-Zentrum
56für Informatik, 2023, 38:1–38:21. ISBN : 978-3-95977-278-5. DOI:10.4230/LIPIcs.
ICALP.2023.38 . arXiv: 2110.13086 [quant-ph] (page 1).
[DKM06a] P . Drineas, R. Kannan, and M. Mahoney. “Fast Monte Carlo algorithms for ma-
trices I: approximating matrix multiplication”. In: SIAM Journal on Computing
36.1 (Jan. 2006), pp. 132–157. DOI:10.1137/s0097539704442684 (page 20).
[DKM06b] P . Drineas, R. Kannan, and M. Mahoney. “Fast Monte Carlo algorithms for
matrices II: computing a low-rank approximation to a matrix”. In: SIAM Journal
on Computing 36.1 (Jan. 2006), pp. 158–183. DOI:10.1137/s0097539704442696
(page 12).
[DW20] Vedran Dunjko and Peter Wittek. “A non-review of Quantum Machine Learn-
ing: trends and explorations”. In: Quantum Views 4 (Mar. 2020), p. 32. DOI:
10.22331/qv-2020-03-17-32 .URL:https://doi.org/10.22331/qv-2020-03-17-
32(page 1).
[DZ10] Petros Drineas and Anastasios Zouzias. “A note on element-wise matrix spar-
sification via a matrix-valued bernstein inequality”. In: Information Processing
Letters 111.8 (June 2, 2010), pp. 385–389. DOI:10.1016/j.ipl.2011.01.010 . arXiv:
1006.0407v2 [cs.DS] (pages 10, 13).
[FP68] L. Fox and I. B. Parker. Chebyshev polynomials in numerical analysis . Oxford
University Press, London-New York-Toronto, Ont., 1968, pp. ix+205 (page 9).
[GL22] Sevag Gharibian and François Le Gall. “Dequantizing the quantum singular
value transformation: hardness and applications to quantum chemistry and
the quantum pcp conjecture”. In: Proceedings of the 54thAnnual ACM SIGACT
Symposium on Theory of Computing . STOC 2022. Rome, Italy: Association for
Computing Machinery, 2022, pp. 19–32. ISBN : 9781450392648. DOI:10.1145/
3519935.3519991 (pages 3, 4).
[GLM08] Vittorio Giovannetti, Seth Lloyd, and Lorenzo Maccone. “Quantum random
access memory”. In: Physical Review Letters 100.16 (2008), p. 160501. DOI:10.
1103/PhysRevLett.100.160501 . arXiv: 0708.1879 (page 1).
[GLT20] András Gilyén, Seth Lloyd, and Ewin Tang. “Quantum-inspired low-rank
stochastic regression with logarithmic dependence on the dimension”. 2020.
arXiv: 1811.04909 [cs.DS] (page 56).
[GS18] Neha Gupta and Aaron Sidford. “Exploiting numerical sparsity for efficient
learning: Faster eigenvector computation and regression”. In: Advances in Neu-
ral Information Processing Systems . Vol. 31. 2018, pp. 5269–5278. arXiv: 1811.
10866 (page 13).
[GSLW19] András Gilyén, Yuan Su, Guang Hao Low, and Nathan Wiebe. “Quantum
singular value transformation and beyond: Exponential improvements for
quantum matrix arithmetics”. In: Proceedings of the 51stACM Symposium on
the Theory of Computing (STOC) . ACM, June 2019, pp. 193–204. DOI:10.1145/
3313276.3316366 . arXiv: 1806.01838 (pages 1, 3, 15, 19, 51–53).
[GST22] András Gilyén, Zhao Song, and Ewin Tang. “An improved quantum-inspired
algorithm for linear regression”. In: Quantum 6 (June 2022), p. 754. DOI:10.
22331/q-2022-06-30-754 . arXiv: 2009.07268 [cs.DS] (pages 2, 5).
57[GT09] Alex Gittens and Joel A Tropp. “Error bounds for random matrix approxima-
tion schemes”. In: (Nov. 20, 2009). DOI:10.48550/ARXIV.0911.4108 . arXiv:
0911.4108 [math.NA] (page 13).
[Has20] Matthew B. Hastings. “Classical and Quantum Algorithms for Tensor Principal
Component Analysis”. In: Quantum 4 (Feb. 2020), p. 237. ISSN : 2521-327X. DOI:
10.22331/q-2020-02-27-237 (page 3).
[HHL09] Aram W. Harrow, Avinatan Hassidim, and Seth Lloyd. “Quantum algorithm
for linear systems of equations”. In: Physical Review Letters 103 (15 Oct. 2009),
p. 150502. DOI:10.1103/PhysRevLett.103.150502 (page 1).
[Hua+22] Hsin-Yuan Huang, Michael Broughton, Jordan Cotler, Sitan Chen, Jerry Li, Ma-
soud Mohseni, Hartmut Neven, Ryan Babbush, Richard Kueng, John Preskill,
and Jarrod R. McClean. “Quantum advantage in learning from experiments”.
In:Science 376.6598 (2022), pp. 1182–1186. DOI:10 . 1126 / science . abn7293
(page 13).
[JGS20] Dhawal Jethwani, François Le Gall, and Sanjay K. Singh. “Quantum-Inspired
Classical Algorithms for Singular Value Transformation”. In: 45thInternational
Symposium on Mathematical Foundations of Computer Science (MFCS 2020) . LIPIcs.
Schloss Dagstuhl–Leibniz-Zentrum für Informatik, 2020. DOI:10.4230/LIPIcs.
MFCS.2020.53 . arXiv: 1910.05699 [cs.DS] (page 3).
[JR23] Samuel Jaques and Arthur G. Rattew. “QRAM: A survey and critique”. May 17,
2023. arXiv: 2305.10310 [quant-ph] (page 1).
[KD14] Abhisek Kundu and Petros Drineas. “A note on randomized element-wise
matrix sparsification”. In: (Apr. 1, 2014). arXiv: 1404.0320v1 [cs.IT] (pages 10,
13).
[KDM17] Abhisek Kundu, Petros Drineas, and Malik Magdon-Ismail. “Recovering pca
and sparse pca via hybrid-(l1, l2) sparse sampling of data elements”. In: The
Journal of Machine Learning Research 18.1 (2017), pp. 2558–2591 (page 13).
[KLLP19] Iordanis Kerenidis, Jonas Landman, Alessandro Luongo, and Anupam Prakash.
“q-means: a quantum algorithm for unsupervised machine learning”. In: Ad-
vances in Neural Information Processing Systems . Vol. 32. 2019. arXiv: 1812.03584
(pages 1, 2).
[KP17] Iordanis Kerenidis and Anupam Prakash. “Quantum recommendation sys-
tems”. In: Proceedings of the 8thInnovations in Theoretical Computer Science Con-
ference (ITCS) . 2017, 49:1–49:21. DOI:10.4230/LIPIcs.ITCS.2017.49 . arXiv:
1603.08675 (pages 1, 4).
[KP20] Iordanis Kerenidis and Anupam Prakash. “Quantum gradient descent for
linear systems and least squares”. In: Physical Review A 101.2 (2020), p. 022316.
DOI:10.1103/PhysRevA.101.022316 . arXiv: 1704.04992 [quant-ph] (page 51).
[KP22] Iordanis Kerenidis and Anupam Prakash. Quantum machine learning with sub-
space states . Jan. 31, 2022. arXiv: 2202.00054 [quant-ph] (page 2).
[KV17] Ravindran Kannan and Santosh Vempala. “Randomized algorithms in numer-
ical linear algebra”. In: Acta Numerica 26 (2017), pp. 95–135. DOI:10.1017/
S0962492917000058 (page 21).
58[KWS16] Ashish Kapoor, Nathan Wiebe, and Krysta Svore. “Quantum perceptron mod-
els”. In: Advances in Neural Information Processing Systems . Vol. 29. Curran
Associates, Inc., Feb. 15, 2016. arXiv: 1602.04799 [quant-ph] (page 1).
[Lan50] Cornelius Lanczos. “An iteration method for the solution of the eigenvalue
problem of linear differential and integral operators”. In: J. Research Nat. Bur.
Standards 45 (1950), pp. 255–282. ISSN : 0160-1741 (page 6).
[LC17] Guang Hao Low and Isaac L. Chuang. “Optimal Hamiltonian simulation
by quantum signal processing”. In: Physical Review Letters 118.1 (Jan. 2017),
p. 010501. DOI:10.1103/PhysRevLett.118.010501 . arXiv: 1606.02685 [quant-
ph](page 1).
[LGZ16] Seth Lloyd, Silvano Garnerone, and Paolo Zanardi. “Quantum algorithms for
topological and geometric analysis of data”. In: Nature Communications 7.1 (Jan.
2016), p. 10138. DOI:10.1038/ncomms10138 . arXiv: 1408.3106 (page 13).
[Mag11] Malik Magdon-Ismail. “Using a non-commutative bernstein bound to ap-
proximate some matrix algorithms in the spectral norm”. In: arXiv preprint
arXiv:1103.5453 (2011) (page 12).
[Mah11] Michael W. Mahoney. “Randomized algorithms for matrices and data”. In:
Foundations and Trends ®in Machine Learning 3.2 (2011), pp. 123–224. ISSN : 1935-
8237. DOI:10.1561/2200000035 (page 12).
[MH02] John C Mason and David C Handscomb. Chebyshev polynomials . Chapman and
Hall/CRC, 2002 (pages 9, 16, 30, 32).
[MM15] Cameron Musco and Christopher Musco. “Randomized block Krylov meth-
ods for stronger and faster approximate singular value decomposition”. In:
Advances in neural information processing systems 28 (2015) (page 13).
[MMS18] Cameron Musco, Christopher Musco, and Aaron Sidford. “Stability of the
Lanczos method for matrix function approximation”. In: Proceedings of the
Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms . Society for
Industrial and Applied Mathematics, Jan. 2018, pp. 1605–1624. DOI:10.1137/1.
9781611975031.105 (pages 8, 9, 32).
[MRTC21] John M. Martyn, Zane M. Rossi, Andrew K. Tan, and Isaac L. Chuang. “Grand
unification of quantum algorithms”. In: PRX Quantum 2 (4 Dec. 2021), p. 040203.
DOI:10.1103/PRXQuantum.2.040203 . arXiv: 2105.02859 [quant-ph] (page 1).
[MZ11] Avner Magen and Anastasios Zouzias. “Low rank matrix-valued chernoff
bounds and approximate matrix multiplication”. In: Proceedings of the twenty-
second annual ACM-SIAM symposium on Discrete Algorithms . SIAM. 2011, pp. 1422–
1436 (pages 12, 21).
[NN13] Jelani Nelson and Huy L. Nguyen. “OSNAP: faster numerical linear algebra
algorithms via sparser subspace embeddings”. In: 2013 IEEE 54thAnnual Sym-
posium on Foundations of Computer Science . IEEE, Oct. 2013. DOI:10.1109/focs.
2013.21 (page 9).
[Oli77] J. Oliver. “An error analysis of the modified Clenshaw method for evaluating
Chebyshev and Fourier series”. In: J. Inst. Math. Appl. 20.3 (1977), pp. 379–391.
ISSN : 0020-2932 (page 9).
59[Oli79] J. Oliver. “Rounding error propagation in polynomial evaluation schemes”.
In:Journal of Computational and Applied Mathematics 5.2 (1979), pp. 85–97. ISSN :
0377-0427. DOI:10.1016/0771-050X(79)90002-0 (pages 9, 33).
[Pai76] C. C. Paige. “Error analysis of the Lanczos algorithm for tridiagonalizing a
symmetric matrix”. In: IMA Journal of Applied Mathematics 18.3 (1976), pp. 341–
349. DOI:10.1093/imamat/18.3.341 (page 8).
[Pra14] Anupam Prakash. “Quantum algorithms for linear algebra and machine learn-
ing”. PhD thesis. University of California at Berkeley, 2014. URL:https://www2.
eecs.berkeley.edu/Pubs/TechRpts/2014/EECS-2014-211.pdf (page 3).
[RML14] Patrick Rebentrost, Masoud Mohseni, and Seth Lloyd. “Quantum support
vector machine for big data classification”. In: Physical Review Letters 113.13
(13 Sept. 2014), p. 130503. DOI:10 . 1103 / PhysRevLett . 113 . 130503 . arXiv:
1307.0471 (page 1).
[RV07] Mark Rudelson and Roman Vershynin. “Sampling from large matrices: an
approach through geometric functional analysis”. In: Journal of the ACM 54.4
(July 2007), 21–es. ISSN : 0004-5411. DOI:10.1145/1255443.1255449 .URL:https:
//doi.org/10.1145/1255443.1255449 (page 21).
[Saa81] Yousef Saad. “Krylov subspace methods for solving large unsymmetric linear
systems”. In: Mathematics of computation 37.155 (1981), pp. 105–126 (page 13).
[Sch41] A. C. Schaeffer. “Inequalities of A. Markoff and S. Bernstein for polynomials
and related functions”. In: Bull. Amer. Math. Soc. 47 (1941), pp. 565–579. ISSN :
0002-9904. DOI:10.1090/S0002-9904-1941-07510-5 (pages 9, 23, 36).
[Sha04] Aleksei Shadrin. “Twelve proofs of the Markov inequality”. In: Approximation
theory: a volume dedicated to Borislav Bojanov . Prof. M. Drinov Acad. Publ. House,
Sofia, 2004, pp. 233–298 (page 36).
[SM22] Changpeng Shao and Ashley Montanaro. “Faster quantum-inspired algorithms
for solving linear systems”. In: ACM Transactions on Quantum Computing 3.4
(July 2022), pp. 1–23. ISSN : 2643-6817. DOI:10.1145/3520141 . arXiv: 2103.10309
[quant-ph] (pages 2, 5).
[SV14] Sushant Sachdeva and Nisheeth K. Vishnoi. “Faster algorithms via approxima-
tion theory”. In: Foundations and Trends in Theoretical Computer Science 9.2 (2014),
pp. 125–210. ISSN : 1551-305X. DOI:10.1561/0400000065 (page 16).
[Tan19] Ewin Tang. “A quantum-inspired classical algorithm for recommendation
systems”. In: Proceedings of the 51stAnnual ACM SIGACT Symposium on Theory
of Computing - STOC 2019 . ACM Press, 2019, pp. 217–228. DOI:10.1145/3313276.
3316310 . arXiv: 1807.04271 [cs.IR] (pages 2, 6, 17, 52).
[Tan21] Ewin Tang. “Quantum principal component analysis only achieves an exponen-
tial speedup because of its state preparation assumptions”. In: Physical Review
Letters 127 (6 Aug. 2021), p. 060503. DOI:10.1103/PhysRevLett.127.060503 .
arXiv: 1811.00414 [cs.IR] (pages 6, 10, 13).
[Tao13] Terence Tao. Matrix identities as derivatives of determinant identities, 2013 . 2013.
URL:https://terrytao.wordpress.com/2013/01/13/matrix- identities- as-
derivatives-of-determinant-identities (page 26).
60[TB97] Lloyd N Trefethen and David Bau III. Numerical linear algebra . Vol. 50. Siam,
1997 (page 13).
[Tre19] Lloyd N. Trefethen. Approximation theory and approximation practice, extended
edition . Extended edition [of 3012510]. Philadelphia, PA: Society for Industrial
and Applied Mathematics, 2019, pp. xi+363. ISBN : 978-1-611975-93-2. DOI:
10.1137/1.9781611975949 (pages 4, 12, 16, 23, 24, 29, 32).
[Tro15] Joel A. Tropp. “An introduction to matrix concentration inequalities”. In: Foun-
dations and Trends ®in Machine Learning 8.1-2 (2015), pp. 1–230. DOI:10.1561/
2200000048 . arXiv: 1501.01571 [math.PR] (page 22).
[Van11] Maarten Van den Nest. “Simulating quantum computers with probabilis-
tic methods”. In: Quantum Information and Computation 11.9&10 (Sept. 2011),
pp. 784–812. ISSN : 1533-7146. DOI:10.26421/qic11.9-10-5 . arXiv: 0911.1624
[quant-ph] (page 4).
[Vos91] Michael D. Vose. “A linear algorithm for generating random numbers with a
given distribution”. In: IEEE Transactions on Software Engineering 17.9 (1991),
pp. 972–975. DOI:10.1109/32.92917 (page 18).
[Woo14] David P . Woodruff. “Sketching as a tool for numerical linear algebra”. In:
Foundations and Trends ®in Theoretical Computer Science 10.1–2 (2014), pp. 1–157.
ISSN : 1551-305X. DOI:10.1561/0400000060 (pages 12, 19, 20).
[WZP18] Leonard Wossnig, Zhikuan Zhao, and Anupam Prakash. “Quantum linear
system algorithm for dense matrices”. In: Physical Review Letters 120.5 (2018),
p. 050502. DOI:10.1103/PhysRevLett.120.050502 . arXiv: 1704.06174 (page 4).
[ZFF19] Zhikuan Zhao, Jack K. Fitzsimons, and Joseph F. Fitzsimons. “Quantum-
assisted Gaussian process regression”. In: Physical Review A 99 (5 May 2019),
p. 052331. DOI:10.1103/PhysRevA.99.052331 . arXiv: 1512.03929 [quant-ph]
(page 13).
61