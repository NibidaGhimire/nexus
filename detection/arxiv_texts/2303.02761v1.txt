A Study of Augmentation Methods for
Handwritten Stenography Recognition
Raphaela Heil[0000−0002−5010−9149]and Eva Breznik[0000−0003−3147−5626]
Centre for Image Analysis,
Department of Information Technology,
Uppsala University, Uppsala, Sweden
{firstname}.{lastname}@it.uu.se
Abstract. One of the factors limiting the performance of handwritten
textrecognition(HTR)forstenographyisthesmallamountofannotated
training data. To alleviate the problem of data scarcity, modern HTR
methods often employ data augmentation. However, due to speciﬁcs of
the stenographic script, such settings may not be directly applicable for
stenographyrecognition.Inthiswork,westudy22classicalaugmentation
techniques, most of which are commonly used for HTR of other scripts,
such as Latin handwriting. Through extensive experiments, we identify
a group of augmentations, including for example contained ranges of
random rotation, shifts and scaling, that are beneﬁcial to the use case
of stenography recognition. Furthermore, a number of augmentation ap-
proaches,leadingtoadecreaseinrecognitionperformance,areidentiﬁed.
Our results are supported by statistical hypothesis testing. Links to the
publicly available dataset and codebase are provided.
Keywords: Stenography ·Handwritten text recognition ·CNNs ·Aug-
mentation study
1 Introduction
Deep learning-based approaches form the majority of the state-of-the-art meth-
ods for handwritten text recognition (HTR) at the time of writing. While these
approaches have been shown to reach high levels of recognition performance,
they typically require large amounts of annotated data during training. This can
for example pose a challenge to HTR for historic manuscripts. Here the acqui-
sition of annotated data can be costly and time-consuming, as it often requires
trained professionals, such as historians or palaeographers. Data availability may
further be limited by the use of a rarely-used script or language, as is for example
the case for the Khmer language [26].
When the acquisition of additional training data is not feasible, or possible, a
commonly used approach for artiﬁcially increasing the dataset size is to use data
augmentation [14,23]. Here, slight alterations, for example rotation and scaling,
are applied to the images in order to increase the visual variety.arXiv:2303.02761v1  [cs.CV]  5 Mar 20232 R. Heil, E. Breznik
u
y
ö äa
e
iåo
d
t
Fig.1: Selected examples of symbols and respective character transliterations
from the Swedish “Melin” stenography system.
In the particular case of stenography, the acquisition of more data is limited
both by the special skill required to transliterate the writing, as well as the lim-
ited use of the script, making data augmentation options especially interesting.
In contrast to scripts like Latin, stenography (also called shorthand) typically
uses short strokes to represent characters, n-grams or even whole words. As can
be seen in the excerpt of the Swedish stenography alphabet in Figure 1, features
like rotation and scale play a considerable role in diﬀerentiating between certain
symbols.ThisraisesthequestionwhethertypicalHTRaugmentationtechniques,
which often include changes in rotation and scale, can also be applied to stenog-
raphy or whether they may cause certain symbols to be interpreted as others
(e.g. “a” as “e” and vice-versa).
In this work, we aim to address this question by studying the applicability
of a selection of commonly used HTR augmentation techniques for the case of
handwritten stenography recognition. We experiment with a deep learning archi-
tecture that has previously been shown to perform best for Swedish stenography
[9], and observe how diﬀerent augmentation techniques aﬀect the text recog-
nition performance on the public LION dataset [9], consisting of stenographic
manuscripts, written by the Swedish children’s book author Astrid Lindgren.
2 Related Work
2.1 Handwritten Stenography Recognition
At the time of writing, very little research has been conducted on deep learning-
based handwritten stenography recognition (HSR). In [17] and [18], convolu-
tional neural networks (CNNs) are used to classify a selection of words written
in Pitman’s, respectively Gregg’s, stenography system. Both types of stenogra-
phy are primarily used in English-language contexts.
Zhai et al. [32] propose to use a CNN-based feature extractor, followed by a
recurrentneuralnetworktogeneratetransliterationsofindividualwords,written
in Gregg’s stenography.
Lastly, our previous work [9] introduces the novel LION dataset, based on
the Swedish stenography system Melin, using excerpts from Astrid Lindgren’s
manuscripts. It furthermore establishes a baseline performance using the model
proposed by Neto et al. [24]. To the best of our knowledge, LION is currently
the only available line-based dataset for any system of stenography.Title Suppressed Due to Excessive Length 3
2.2 Data Augmentation Methods for Handwritten Text Recognition
A variety of augmentation approaches for HTR has been proposed in the litera-
ture. Some of the most commonly used augmentations include rotations, transla-
tions, shearing and scaling, as well as greyscale dilations and erosions [20,24,30].
Similarly to these, Retsinas et al. [21] propose the use of rotations, skewing and
additionally applying Gaussian noise.
Wick et al. [27] also use greyscale dilation and erosion and further simu-
late the variability in handwriting by applying grid-like distortions, originally
proposed by Wigington et al. [28]. Krishnan and Jawahar [13] follow a similar
approach, applying translation, scaling, rotation and shearing and combine these
with the elastic transformations, proposed by Simard et al. [23].
All of the aforementioned works apply each augmentation with an indepen-
dent probability (often 0.5) to each image in the training set. Furthermore, the
augmentation parameters, for example the angle of rotation, are sampled per
image from a user-deﬁned range, yielding a wide variety of altered datasets.
Wilkinson et al. [31] use the same augmentation approach and parameters
as [30] but arrange the perturbed images into lines and whole pages, thereby
creating artiﬁcial data for segmentation-free word spotting.
Zhai et al. [32] apply similar transforms to all of the above but do not sample
the respective parameters. Instead, they use ﬁxed, pre-deﬁned values and gen-
erate a total of eight images (with ﬁxed choice of augmentation parameters) for
every input.
Instead of perturbing the available data, a diﬀerent line of augmentation
techniques is centred around creating entirely synthetic datasets. Circumventing
the use of the data at hand entirely, Krishnan et al. [12] generate words by using
publicly available handwriting fonts. This approach is currently not applicable
in the case of stenography, as such a library of handwritten symbols and fonts
is not available for these scripts, to the best of our knowledge.
While Alonso et al. [1], Kang et al. [11] and Mattick at al. [15] also propose
to generate synthetic data, they employ generative adversarial networks to syn-
thesise words in the style of a given dataset or sample image. To the best of
our knowledge, approaches in this line of research currently employ word-level
annotations (segmentation and transliteration), which are not available for the
dataset in our study.
3 Study Design
In order to investigate the eﬀect of diﬀerent augmentations on the text recog-
nition performance, we evaluate a variety of image transformations that are
applied during training. We compare the results with those obtained from a
baseline model, trained on the original data, without any augmentations.
3.1 Examined Augmentations
Below, the examined augmentations and the respective parameter conﬁgurations
are brieﬂy presented. Each experiment conﬁguration is denoted with a name4 R. Heil, E. Breznik
Table 1: Summary of augmentation conﬁgurations and the names by which they
are referred to in this paper.
Name Augmentation Type Parameters
baseline none N/A
rot1.5 Random Rotation [−1.5,1.5]degrees
rot5 Random Rotation [5,5]degrees
rot10 Random Rotation [10,10]degrees
positive Random Rotation [0,1.5]degrees
negative Random Rotation [−1.5,0]degrees
rot+2 Fixed Rotation 2 degrees
rot-2 Fixed Rotation -2 degrees
square-dilation Random Dilation square SE, [1..4]px
disk-dilation Random Dilation disk SE, [1..4]px
square-erosion Random Erosion square SE, [1..3]px
disk-erosion Random Erosion disk SE, [1..3]px
shift Random Shift horiz. = [0,15],vert. = [−3.5,3.5]
elastic Random Elastic Transform. [23] α= [16,20],σ= [5,7]
shear Random Horizontal Shearing [−5,30]degrees
shear30 Random Horizontal Shearing [−30,30]degrees
scale75 Random Scaling [0.75,1]
scale95 Random Scaling [0.95,1]
mask10 Random Column Masking 10% of columns
mask40 Random Column Masking 40% of columns
noise Gaussian Noise σ={0.08,0.12,0.18}
dropout Pixel Dropout [0,20]% of pixels
blur Gaussian Blur kernel=5,σ= [0.1,2]
by which it will be referred to in the remainder of this paper. A summary of
names, augmentation types and parameters is shown in Table 1. Additionally,
theconnectedsupplementary[8]containsavisualisationoftheimpactofselected
augmentation parameters on the original line image.
The implementations for most of the examined augmentations were provided
by [10], [16] and [25]. All others are available in our project repository (cf. sec-
tion 5 - Data and Code Availability).
Baseline In order to establish a baseline performance, the model is trained on
the original data, not using any augmentations.
Rotations We examine a variety of rotations, largely based on conﬁgurations
used in related text recognition works. Three models are trained with random
rotationsintheranges [−1.5,1.5](“rot1.5”)[24], [−5,5](“rot5”)[30]and [−10,10]
(“rot10”). Following [32], one model each is trained with a ﬁxed rotation of +2,
respectively -2, degrees (“rot+2”, “rot-2”). Lastly, one model each is trained with
only positive, respectively negative rotations, in the ranges [0,1.5](“positive”)
and[−1.5,0](“negative”).Title Suppressed Due to Excessive Length 5
Morphological Operations With respect to morphological operations, we
consider greyscale dilations and erosions with a square structuring element (SE)
[20,24,30]. Additionally, we evaluate the use of a disk SE, because the primary
writing implement in the LION dataset is a pencil, which typically features
a round footprint. We refer to these augmentations as “square-dilation”, “disk-
dilation”, “square-erosion” and “disk-erosion”, respectively. For each application
of these augmentations (i.e. per image) the size of the SE is sampled from [1..3]
for erosions and [1..4]for dilations. It should be noted that in the case of a square
SE, the size refers to the width, whereas it indicates the radius for disks.
Geometric Augmentations In addition to the aforementioned rotations, we
consider a number of other geometric augmentations.
Concretely, we evaluate shearing with angles in the range of [−5,30](“shear”,
following [30]) and [−30,30](“shear30”). Furthermore, we examine downscaling
with factors in the range of [0.95,1](“scale95”, [24]) and [0.75,1](“scale75”). The
rescaled image is zero-padded at the top and bottom to the original size for
further processing.
Following Krishnan and Jawahar [13] we investigate the applicability of the
elastic transformation (“elastic”), proposed by [23]. In this work, we sample α
from the range [16,20]andσfrom [5,7].
Lastly,weconsiderrandomshifts(“shift”)inhorizontalandverticaldirection,
within the pixel ranges of [0,15], respectively [−3.5,3.5]. For both directions,
non-integer shifts may occur.
Intensity Augmentations Finally, we consider a selection of augmentations
that aﬀect an image’s pixel intensities. We examine both pixel dropout (“drop-
out”), i.e. zero-masking of random pixels, with rates in the range of [0,20]%of
the image, and random column masking with ﬁxed rates of 10%(“mask10”) and
40%(“mask40”) of columns. Additionally, we consider the application of Gaus-
sian noise (“noise”), with σ={0.08,0.12,0.18}, and Gaussian blurring (“blur”)
with a kernel size of 5 and σ= [0.1,2].
3.2 Dataset
This study is centred around the LION dataset [9], which is based on steno-
graphed manuscripts by Swedish children’s book author Astrid Lindgren (1907
– 2002). It consists of 198 pages, written in Swedish, using the Melinshort-
hand system. For each page, line-level bounding boxes and transliterations are
available, the latter of which were provided by stenographers through expert
crowdsourcing [2]. Overall, the dataset has an alphabet size of 51, entailing the
26 lower-case Latin characters (a-z), the Swedish vowels “äöå”, digits 0-9, and a
selection of punctuation marks and quotes appearing in the text.
We follow the datasplitting proposed by [9], resulting in a ﬁve-fold cross-
validation set of 1530 lines (306 lines per fold). This portion of the dataset
contains texts from The Brothers Lionheart (chapters 1 - 3, 5, 6; original title:6 R. Heil, E. Breznik
Bröderna Lejonhjärta). The test split consists of 474 in-domain lines, covering
chapter four of The Brothers Lionheart , and 191 out-of-domain lines, containing
portions from other literary works by Lindgren. All lines in the dataset are
preprocessed,followingthemethodproposedby[9].Forconcreteimplementation
details, the interested reader is referred to our implementation (cf- section 5).
Figure 2 shows an example line from the dataset in its original form (top) and
the preprocessed version (bottom).
3.3 Model Architecture
We focus our study on the deep neural network architecture proposed by Neto et
al.,whodemonstratethatthismodelperformswellinlimited-resourceline-based
HTR settings [24]. This architecture outperformed other connectionist temporal
classiﬁcation(CTC)[6,7]architecturesinpreliminaryexperiments[9].Themodel
consists of a convolutional block, mixing regular and gated convolutions [5], for
feature extraction, followed by a recurrent block, employing bi-directional gated
recurrent units [4], which generate the output sequence.
Furtherdetailsregarding the model architecture can be found in ourPyTorch
[19] implementation in the accompanying code (cf. section 5 - Data and Code
Availability).
3.4 Experimental Settings
All of the experiments below follow the same general procedure and only diﬀer
in regard to the augmentation that is applied to the data during training. In
each epoch, each image is augmented at a rate of 50%. Afterwards, all images
are scaled to a ﬁxed height of 64 pixels and padded with the background colour
(black) to a width of 1362 pixels. Target sequences are padded with the token
indicating blanks in the CTC-loss (here zero), to a length of 271.
We follow the same training protocol as [9], which is included for reference
in the supplementary material [8].
Each augmentation conﬁguration is trained 30 separate times for each of
the ﬁve cross-validation folds, yielding a total of 150 model checkpoints (i.e.
sets of weights). All checkpoints are evaluated by measuring the character error
Fig.2: Example image from the LION dataset [9]. Top: original line image,
bottom: preprocessed line image. The transliteration reads: “jonatan hette inte
lejonhjärta från början” (English: jonatan was not called lionheart from the be-
ginning).Title Suppressed Due to Excessive Length 7
rate (CER) and word error rate (WER) for the transliterations, obtained via
best path decoding [6,7] on the test set. We do not consider advanced decoding
techniques, such as word beam search [22] or language models, in order to rule
out any eﬀects that these may have on the obtained transliterations. CER and
WER are deﬁned as:
ErrorRate =S+D+I
N(1)
whereSis the number of character (word) substitutions, Dthe number of
character (word) deletions and Ithe number of character (word) insertions, that
are required to convert a given sequence to a reference sequence. Ndenotes the
number of characters (words) in the reference string.
Final statistical testing for signiﬁcant diﬀerences is performed by a Wilcoxon
signed-rank test [29] on the mean results, with a Bonferroni correction [3] for
multiplicity.
4 Results and Discussion
The mean error rates for the 22 diﬀerent augmentations, as well as for the
augmentation-free baseline, are shown in Table 2. As can be seen, the augmen-
tations rot10,square-erosion ,disk-erosion ,square-dilation ,disk-dilation ,mask40
andblurall result in signiﬁcant decreases in performance.
Forrot10, this result is likely attributable to the extreme rescaling of the
line contents that occurs when height-normalising the rotated image. In order
to avoid cutting oﬀ parts of a rotated line, the image size is expanded accord-
ingly and padded with black. Larger rotations require a larger target image to
accommodate the whole line, resulting in a smaller text size, as compared to
smaller rotations, when scaled to the same height. Figure 3 shows an example
of the impact on the image content, when rotating the original line image to by
10, respectively 1.5 degrees.
Regarding the increase in error rates for both kinds of erosions, it can be ob-
served that these augmentations thin the original strokes considerably. We argue
that the selected erosions thin the strokes too much, leaving too little informa-
tion for the recognition behind. This argument is supported by the signiﬁcantly
lower (p<0.01) performance of the disk-erosion, as compared to the square one.
As indicated earlier (cf. section 3.1), the disk SEs are larger than their square
(a) -10 degrees
 (b) -1.5 degrees
Fig.3: Examples for the impact of diﬀerent rotations on the original image con-
tent size. Padding value set to grey to emphasise the required padding.8 R. Heil, E. Breznik
counterparts, thus thinning the strokes more strongly and leaving even less in-
formation behind. It should be noted here, that this is not an issue that can
be directly attributed to stenography but rather to the use of a relatively thin
pencil, which results in thin strokes that are more susceptible to erosions.
Considering the two dilation-based augmentations, it can ﬁrstly be noticed
that, in contrast to the WER, no signiﬁcant diﬀerence for the CER can be
determined in the case of the square-dilation . This may be attributable to large
dilations closing the gap between words. Such word boundary errors will only
marginally aﬀect the CER as they require a single character substitution or
insertion to establish the gap. They will however lead to considerable increases
in the WER, as one word substitution and one word insertion will be required to
recover the recognition mistake. This may also be a contributing factor for the
disk-dilation . In this regard it can however also be observed that the disk SE,
whose footprint is larger than the square’s, leads to the loss of details, regarding
small, and long and thin, loops, which are frequently used in Melin’s stenography
system. This loss of deﬁnition, an example of which is shown in Figure 4, may
lead to incorrect transliterations.
(a) original
 (b) disk-dilation, radius = 4px
Fig.4: Example demonstrating the ﬁlling of several loops due to a dilation with
a large structuring element.
Since the performance of mask10 is on par with the baseline, a probable
explanation for the decrease in performance when using mask40is that the level
of masking is simply too high, again removing too much of the information and
thus aﬀecting the recognition performance. The decrease in performance for the
bluraugmentation may be attributable to the loss of deﬁnition between smaller
symbols, like “a” and “o”, and “u” and “ö”.
For six of the augmentations, namely rot5, rot+2, rot-2, mask10, noise and
dropout, no diﬀerences can be observed with respect to the baseline. While
including these may not directly harm the performance, they also do not appear
to contribute to the learning and are therefore not of immediate interest to the
concrete recognition task.
Finally, ten of the examined augmentations yield signiﬁcant decreases in er-
ror rates, as compared to the baseline. Considering the improvements obtained
when applying conservative rotations, in the overall range of [−1.5,1.5](rot1.5,
positive,negative), we cannot ﬁnd any indications supporting the initial concern
that rotations in general may negatively aﬀect the transliteration of certain char-
acters. Obviously, the angles between the similar symbols, for example “a” and
“e”, tend to be much larger than the ones examined in these experiments. How-
ever, as discussed for the case of rot10, even rotations closer to the actual angleTitle Suppressed Due to Excessive Length 9
betweensimilarcharactersarelikelytoleadtoscalingrelatedissues,outweighing
any potential recognition errors caused directly by rotated symbols.
Regarding the decrease in error rates for the remaining six augmentations,
shift,shear,shear30,elastic,scale75andscale95it can be noted that all of these
correspond to variations that, to some degree, naturally occur in the dataset.
Applying these augmentations therefore constitutes plausible transformations
within the dataset domain and increases the visual variety of the training data.
Given the reduction in error rates when applying several of the augmenta-
tions individually, the follow-up question arose whether a combination of some
of these would have a similar positive eﬀect. We therefore selected the three
best performing augmentations, rot1.5,shiftandscale75, and repeated the ex-
periment with a combination of these. Here, each augmentation in the sequence
is applied with an independent probability of 0.5. This conﬁguration achieves
signiﬁcantly lower error rates than the baseline, with a mean CER of 0.3090
(0.0208) and WER of 0.5603(0.0253).
Table 2: Mean (and std. dev.) CER and WER per augmentation type. For both
metrics,lowervaluesarebetter.Comparisoncolumnsindicateifthemeanforthe
inspected augmentation type is signiﬁcantly higher ( >, red), lower ( <, blue), or
no diﬀerent (−) than the baseline mean, according to the Wilcoxon signed-rank
test (p<0.01, after applying a Bonferroni correction with n= 22).
Augmentation CER (Std.dev.) CER Comparison WER (std.dev.) WER Comparison
baseline 0.3174 (0.0123) N/A 0.5715 (0.0145) N/A
rot1.5 0.3048 (0.0298) < 0.5561 (0.0334) <
rot5 0.3167 (0.0351) − 0.5674 (0.0383) −
rot+2 0.3114 (0.0203) − 0.5616 (0.0245) <
rot-2 0.3134 (0.0314) − 0.5640 (0.0367) −
positive 0.3066 (0.0236) < 0.5584 (0.0278) <
negative 0.3062 (0.0225) < 0.5584 (0.0273) <
rot10 0.3585 (0.0753) > 0.6113 (0.0659) >
square-erosion 0.3189 (0.0156) > 0.5740 (0.0187) >
disk-erosion 0.3352 (0.0568) > 0.5899 (0.0390) >
square-dilation 0.3182 (0.0135) − 0.5739 (0.0173) >
disk-dilation 0.3203 (0.0164) > 0.5764 (0.0209) >
shift 0.3065 (0.0126) < 0.5583 (0.0162) <
shear 0.3103 (0.0131) < 0.5649 (0.0173) <
shear30 0.3102 (0.0128) < 0.5641 (0.0157) <
elastic 0.3139 (0.0128) < 0.5675 (0.0165) <
scale75 0.3040 (0.0190) < 0.5557 (0.0235) <
scale95 0.3077 (0.0119) < 0.5614 (0.0156) <
mask10 0.3182 (0.0147) − 0.5732 (0.0192) −
mask40 0.3370 (0.0797) > 0.5875 (0.0540) >
noise 0.3172 (0.0156) − 0.5715 (0.0187) −
dropout 0.3172 (0.0127) − 0.5727 (0.0159) −
blur 0.3195 (0.0153) > 0.5758 (0.0196) >10 R. Heil, E. Breznik
5 Conclusions
Inthisworkwehaveexamined22diﬀerentaugmentationconﬁgurationsandtheir
impact on handwritten stenography recognition. Based on the obtained results,
we conclude that small rotations, shifting, shearing, elastic transformations and
scaling are suitable augmentations for the recognition task and dataset at hand.
Furthermore, an increase in performance could be observed for a combination
of the top three augmentations, i.e. rotations in the range of [−1.5,1.5]degrees,
horizontal and vertical shifting by [0,15]px, respectively [−3.5,3.5]px, and
scaling by a factor in the range of [0.75,1].
Adecreaseinperformancecanbeobservedforlargerrotations( ±10degrees),
which is attributable to the extreme rescaling resulting from the padding of
augmented images. Besides this, the results obtained for erosions and dilations
indicate that the examined conﬁgurations have adverse eﬀects on the recognition
performance. This can largely be attributed to the writing implement that was
used in the dataset, highlighting that aspects like the stroke width should be
taken into account when choosing augmentations for any kind of text recognition
system.
Overall, initial concerns that commonly-used HTR augmentations may lead
to confusions between certain symbols in the stenographic alphabet could not
be conﬁrmed.
Interesting avenues for future research include the creation of text lines by
combining segmented words, for example as proposed by Wilkinson and Brun
[31], as well as the investigation of text generation approaches similar to the ones
proposed by Alonso et al. [1], Kang et al. [11] and Mattick at al. [15].
Data and Code Availability The dataset and code will be available via Zen-
odo (https://zenodo.org/) with the camera-ready version of this paper.
Acknowledgements This work is partially supported by Riksbankens Ju-
bileumsfond (RJ) (Dnr P19-0103:1). The computations were enabled by re-
sources provided by the National Academic Infrastructure for Supercomputing
in Sweden (NAISS) at Chalmers Centre for Computational Science and Engi-
neering (C3SE) partially funded by the Swedish Research Council through grant
agreement no. 2022-06725. Author E.B. is partially funded by the Centre for In-
terdisciplinary Mathematics, Uppsala University, Sweden.
References
1. Alonso, E., Moysset, B., Messina, R.: Adversarial generation of handwritten text
images conditioned on sequences. In: 2019 International Conference on Document
Analysis and Recognition (ICDAR). pp. 481–486 (2019)
2. Andersdotter, K., Nauwerck, M.: Secretaries at work: Accessing astrid lindgren’s
stenographed manuscripts through expert crowdsourcing. In: Berglund, K., Mela,
M.L., Zwart, I. (eds.) Proceedings of the 6th Digital Humanities in the Nordic andTitle Suppressed Due to Excessive Length 11
Baltic Countries Conference (DHNB 2022), Uppsala, Sweden, March 15-18, 2022.
vol. 3232, pp. 9–22 (2022)
3. Bonferroni, C.: Teoria statistica delle classi e calcolo delle probabilita. Pubbli-
cazioni del R Istituto Superiore di Scienze Economiche e Commericiali di Firenze
8, 3–62 (1936)
4. Cho, K., van Merriënboer, B., Bahdanau, D., Bengio, Y.: On the properties of
neuralmachinetranslation:Encoder–decoderapproaches.In:ProceedingsofSSST-
8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation.
pp. 103–111 (2014)
5. Dauphin, Y.N., Fan, A., Auli, M., Grangier, D.: Language modeling with gated
convolutional networks. In: Precup, D., Teh, Y.W. (eds.) Proceedings of the 34th
International Conference on Machine Learning. Proceedings of Machine Learning
Research, vol. 70, pp. 933–941. PMLR (06–11 Aug 2017)
6. Graves, A., Fernández, S., Gomez, F., Schmidhuber, J.: Connectionist temporal
classiﬁcation: Labelling unsegmented sequence data with recurrent neural net-
works. In: Proceedings of the 23rd International Conference on Machine Learning.
p. 369–376. ICML ’06, ACM, New York, NY, USA (2006)
7. Graves, A., Liwicki, M., Fernández, S., Bertolami, R., Bunke, H., Schmidhuber,
J.: A novel connectionist system for unconstrained handwriting recognition. IEEE
Transactions on Pattern Analysis and Machine Intelligence 31(5), 855–868 (2009)
8. Heil, R., Breznik, E.: Supplementary Material (2023), https://uppsala.box.com/
shared/static/d9o7s1cs282gzoki2ahz054ie0axq6lb.pdf, arXiv publ. in progress
9. Heil, R., Nauwerck, M.: Handwritten stenography recognition and the LION
dataset. Manuscript (2023)
10. Jung, A.B., Wada, K., Crall, J., Tanaka, S., Graving, J., Reinders, C., et al.:
imgaug. https://github.com/aleju/imgaug (2020), online; accessed 01-Feb-2020
11. Kang, L., Riba, P., Wang, Y., Rusiñol, M., Fornés, A., Villegas, M.: Ganwriting:
Content-conditioned generation of styled handwritten word images. In: Vedaldi,
A., Bischof, H., Brox, T., Frahm, J.M. (eds.) Computer Vision – ECCV 2020. pp.
273–289. Springer International Publishing, Cham (2020)
12. Krishnan, P., Jawahar, C.V.: Matching handwritten document images. In: Leibe,
B., Matas, J., Sebe, N., Welling, M. (eds.) Computer Vision – ECCV 2016. pp.
766–782. Springer International Publishing, Cham (2016)
13. Krishnan, P., Jawahar, C.V.: Hwnet v2: an eﬃcient word image representation for
handwritten documents. International Journal on Document Analysis and Recog-
nition (IJDAR) 22(4), 387–405 (Dec 2019)
14. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classiﬁcation with deep con-
volutional neural networks. In: Pereira, F., Burges, C., Bottou, L., Weinberger, K.
(eds.) Advances in Neural Information Processing Systems. vol. 25. Curran Asso-
ciates, Inc. (2012)
15. Mattick, A., Mayr, M., Seuret, M., Maier, A., Christlein, V.: Smartpatch: Improv-
ing handwritten word imitation with patch discriminators. In: Lladós, J., Lopresti,
D., Uchida, S. (eds.) Document Analysis and Recognition – ICDAR 2021. pp.
268–283. Springer International Publishing, Cham (2021)
16. Michaelis, C., Mitzkus, B., Geirhos, R., Rusak, E., Bringmann, O., Ecker, A.S.,
Bethge, M., Brendel, W.: Benchmarking robustness in object detection: Au-
tonomous driving when winter is coming. arXiv preprint arXiv:1907.07484 (2019)
17. Montalbo,F.J.P., Barfeh,D.P.Y.:Classiﬁcation ofstenography usingconvolutional
neural networks and canny edge detection algorithm. In: 2019 International Con-
ference on Computational Intelligence and Knowledge Economy (ICCIKE). pp.
305–310 (2019)12 R. Heil, E. Breznik
18. Padilla, D.A., Vitug, N.K.U., Marquez, J.B.S.: Deep learning approach in gregg
shorthand word to english-word conversion. In: 2020 IEEE 5th International Con-
ference on Image, Vision and Computing (ICIVC). pp. 204–210 (2020)
19. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T.,
Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z.,
Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., Chintala,
S.: PyTorch: An Imperative Style, High-Performance Deep Learning Library. In:
Wallach, H., Larochelle, H., Beygelzimer, A., d’Alché Buc, F., Fox, E., Garnett,
R. (eds.) Advances in Neural Information Processing Systems 32. pp. 8024–8035.
Curran Associates, Inc. (2019)
20. Puigcerver,J.:Aremultidimensionalrecurrentlayersreallynecessaryforhandwrit-
ten text recognition? In: 2017 14th IAPR International Conference on Document
Analysis and Recognition (ICDAR). vol. 01, pp. 67–72 (2017)
21. Retsinas, G., Sﬁkas, G., Gatos, B., Nikou, C.: Best practices for a handwritten text
recognition system. In: Uchida, S., Barney, E., Eglin, V. (eds.) Document Analysis
Systems. pp. 247–259. Springer International Publishing, Cham (2022)
22. Scheidl, H., Fiel, S., Sablatnig, R.: Word beam search: A connectionist tempo-
ral classiﬁcation decoding algorithm. In: 2018 16th International Conference on
Frontiers in Handwriting Recognition (ICFHR). pp. 253–258 (2018)
23. Simard, P.Y., Steinkraus, D., Platt, J.C.: Best practices for convolutional neural
networks applied to visual document analysis. In: Proceedings of the Seventh In-
ternational Conference on Document Analysis and Recognition - Volume 2. p. 958.
ICDAR ’03, IEEE Computer Society, USA (2003)
24. de Sousa Neto, A.F., Leite Dantas Bezerra, B., Hector Toselli, A., Baptista Lima,
E.: A robust handwritten recognition system for learning on diﬀerent data restric-
tion scenarios. Pattern Recognition Letters 159, 232–238 (2022)
25. TorchVision maintainers and contributors: TorchVision: PyTorch’s Computer Vi-
sion library (11 2016)
26. Valy, D., Verleysen, M., Chhun, S.: Data augmentation and text recognition on
khmer historical manuscripts. In: 2020 17th International Conference on Frontiers
in Handwriting Recognition (ICFHR). pp. 73–78 (2020)
27. Wick, C., Zöllner, J., Grüning, T.: Rescoring sequence-to-sequence models for text
line recognition with ctc-preﬁxes. In: Uchida, S., Barney, E., Eglin, V. (eds.) Doc-
ument Analysis Systems. pp. 260–274. Springer International Publishing, Cham
(2022)
28. Wigington, C., Stewart, S., Davis, B., Barrett, B., Price, B., Cohen, S.: Data
augmentation for recognition of handwritten words and lines using a cnn-lstm
network. In: 2017 14th IAPR International Conference on Document Analysis and
Recognition (ICDAR). vol. 01, pp. 639–645 (2017)
29. Wilcoxon, F.: Individual comparisons by ranking methods. Biometrics Bulletin
1(6), 80–83 (1945)
30. Wilkinson, T., Brun, A.: Semantic and verbatim word spotting using deep neu-
ral networks. In: 2016 15th International Conference on Frontiers in Handwriting
Recognition (ICFHR). pp. 307–312 (2016)
31. Wilkinson, T., Lindstrom, J., Brun, A.: Neural ctrl-f: Segmentation-free query-by-
string word spotting in handwritten manuscript collections. In: Proceedings of the
IEEE International Conference on Computer Vision (ICCV) (Oct 2017)
32. Zhai, F., Fan, Y., Verma, T., Sinha, R., Klakow, D.: A dataset and a novel neural
approach for optical gregg shorthand recognition. In: International Conference on
Text, Speech, and Dialogue. pp. 222–230. Springer (2018)Supplementary Material for:
“A Study of Augmentation Methods for
Handwritten Stenography Recognition”
Raphaela Heil[0000−0002−5010−9149]and Eva Breznik[0000−0003−3147−5626]
Centre for Image Analysis,
Department of Information Technology,
Uppsala University, Uppsala, Sweden
{firstname}.{lastname}@it.uu.se
1 Neural Network Training Protocol
The model is trained with a batch size of eight, using a CTC-loss [1,2] and the
AdamW [3] optimiser with a learning rate of 0.001. After each training epoch,
the validation loss is calculated and the model weights with the lowest validation
loss are preserved for the ﬁnal evaluation. Training is carried on for a maximum
of 100 epochs, stopping earlier if the lowest validation loss has not been exceeded
within the ﬁve most recent epochs.
2 Visualisation of Evaluated Augmentations
The following sections show examples of the extremes for the respective aug-
mentation parameter range. For ease of comparison, all images are scaled to the
same height. Padding and masking are visualised in grey, to emphasise the eﬀect
of the respective augmentation.
2.1 Baseline
Fig.1: Baseline sample image (augmentation-free)arXiv:2303.02761v1  [cs.CV]  5 Mar 20232 R. Heil, E. Breznik
2.2 Rotations
(a) -1.5 degrees
 (b) +1.5 degrees
Fig.2: rot1.5, negative and positive
(a) -5 degrees
 (b) +5 degrees
Fig.3: rot5
Fig.4: rot-2
Fig.5: rot+2
(a) -10 degrees
 (b) +10 degrees
Fig.6: rot10Supplementary Material 3
2.3 Morphological Operations
(a) square structuring element, width=1
(b) square structuring element, width=4
(c) disk structuring element, radius=1
(d) disk structuring element, radius=4
Fig.7: square-dilation and disk-dilation
(a) square structuring element, width=1
(b) square structuring element, width=4
(c) disk structuring element, radius=1
(d) disk structuring element, radius=4
Fig.8: square-erosion and disk-erosion4 R. Heil, E. Breznik
2.4 Geometric Augmentations
(a) horizontal shift, 15px
(b) vertical shift, -3.5 px
(c) vertical shift, +3.5 px
(d) horizontal shift = 15px, vertical shift = -3.5 px
(e) horizontal shift = 15px, vertical shift = +3.5 px
Fig.9: shift
(a)α= 16,σ= 5
(b)α= 16,σ= 7
(c)α= 20,σ= 5
(d)α= 20,σ= 7
Fig.10: elasticSupplementary Material 5
(a) -5 degrees
(b) -30 degrees
(c) +30 degrees
Fig.11: shear and shear30
(a) scale factor = 0.75
(b) scale factor = 0.95
Fig.12: scale75 and scale95
2.5 Intensity Augmentations
(a) 10% of columns masked
(b) 40% of columns masked
Fig.13: mask and mask40
(a)σ= 0.08
(b)σ= 0.18
Fig.14: noise6 R. Heil, E. Breznik
Fig.15: dropout, 20% of pixels masked
(a)σ= 0.1
(b)σ= 2.0
Fig.16: blur
References
1. Graves,A.,Fernández,S.,Gomez,F.,Schmidhuber,J.:Connectionisttemporalclas-
siﬁcation: Labelling unsegmented sequence data with recurrent neural networks. In:
Proceedings of the 23rd International Conference on Machine Learning. p. 369–376.
ICML ’06, ACM, New York, NY, USA (2006)
2. Graves, A., Liwicki, M., Fernández, S., Bertolami, R., Bunke, H., Schmidhuber,
J.: A novel connectionist system for unconstrained handwriting recognition. IEEE
Transactions on Pattern Analysis and Machine Intelligence 31(5), 855–868 (2009)
3. Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. In: International
Conference on Learning Representations (2019)