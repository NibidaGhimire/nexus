ChatGPT: More than a \Weapon of Mass Deception"
Ethical challenges and responses from the Human-Centered
Articial Intelligence (HCAI) perspective
Alejo Jos e G. Sison1, Marco Tulio Daza1,2, Roberto Gozalo-Brizuela3and Eduardo
C. Garrido-Merch an3
1School of Economics and Business Administration, Institute of Data Science and Articial
Intelligence (DATAI), Universidad de Navarra, Pamplona, Spain
2Information Systems Department, University Center for Economic and Administrative
Sciences (CUCEA), University of Guadalajara, Guadalajara, Mexico
3Quantitative Methods Department, Universidad Ponticia Comillas, Madrid, Spain
ARTICLE HISTORY
Compiled April 25, 2023
ABSTRACT
This article explores the ethical problems arising from the use of ChatGPT as a
kind of generative AI and suggests responses based on the Human-Centered Arti-
cial Intelligence (HCAI) framework. The HCAI framework is appropriate because it
understands technology above all as a tool to empower, augment, and enhance hu-
man agency while referring to human wellbeing as a \grand challenge", thus perfectly
aligning itself with ethics, the science of human ourishing. Further, HCAI provides
objectives, principles, procedures, and structures for reliable, safe, and trustwor-
thy AI which we apply to our ChatGPT assessments. The main danger ChatGPT
presents is the propensity to be used as a \weapon of mass deception" (WMD) and
an enabler of criminal activities involving deceit. We review technical specications
to better comprehend its potentials and limitations. We then suggest both technical
(watermarking, styleme, detectors, and fact-checkers) and non-technical measures
(terms of use, transparency, educator considerations, HITL) to mitigate ChatGPT
misuse or abuse and recommend best uses (creative writing, non-creative writing,
teaching and learning). We conclude with considerations regarding the role of hu-
mans in ensuring the proper use of ChatGPT for individual and social wellbeing.
Keywords: ChatGPT, generative AI, HCAI, combating disinformation, AI ethics
\The most important outcome of that journey is that it will compel us to understand
what it means to be human" (Gary Marcus)
"I do really believe that creativity is computational... It is something we understand
the principles behind. So, it's only a matter of having. . . neural nets or models that are
smarter." (Yoshua Bengio, Turing award.)
Author emails: ajsison@unav.es, mdazaramire@alumni.unav.es, 201905616@alu.comillas.edu, and ecgar-
rido@icade.comillas.eduarXiv:2304.11215v1  [cs.CY]  6 Apr 20231. Introduction
In a review of large Generative AI models, Gozalo-Brizuela and Garrido-Merchan
(2023) list among the work's limitations the \lack of an understanding of ethics"
which prevents the new technology from achieving full potential. This paper seeks to
rise to this challenge.
Since its rollout in November 2022, a host of ethical issues has arisen from the use
of ChatGPT: bias, privacy, misinformation, and job displacement, among others. But
this did not dampen public enthusiasm. By February 2023, ChatGPT broke the record
for application user-base growth, rising to 100M in just two months (Last Week in AI,
2023) and reaching a market value close to 30 billion USD (Roose, 2023c).
In 2022, venture capitalists invested 2.7 billion USD in 110 generative-AI startups
(The Economist, 2023a). In consequence, alternatives to ChatGPT did not take long
in appearing: Deep Mind's Sparrow, Google's LaMDA/Bard, Antrophic's Claude, and
an open-source transformer by Stability AI (Konrad and Cai, 2023b; Metz and Grant,
2023; Romero, 2023f; Roose, 2023c,d).
The magnitude of investments is remarkable, considering that initially, there was
no established business model for ChatGPT (Vincent, 2023b). A subscription model,
'ChatGPT Plus', oering greater availability, faster responses, and priority access to
new features for 20 USD/month is clearly inadequate (Schreckinger, 2023a). Hence
the race to team up with Microsoft, combining ChatGPT with Bing (search engine)
and Edge (browser). The experiment ended up largely a disaster, not only because of
ChatGPT's unpredictable, erroneous, and creepy responses, but also because a Large
Language Model (LLM) is much costlier to run than a search engine, while admitting
fewer ads, and overall less trac (Romero, 2023b). Google's hasty attempt to compete
with its own LLM Bard turned out no better, erasing 100 billion USD of market
value (Marcus, 2023d). As Warzel (2023b) commented, \The idea of generative AI
as a new frontier for accessing knowledge, streamlining busywork, and assisting the
creative process might exhilarate you. It should also unnerve you. If you're cynical
about technology (and you have every reason to be), it will terrify you". LLM chatbots
only make search engine outputs unreliable.
Together with huge amounts of money came the hype about what ChatGPT can
do: dating help, crime ghting, psychotherapy, and so forth (Khullar, 2023; Romero,
2022b). For many, ChatGPT was only or above all a means to earn money, gain social
media presence, or (somewhat dubiously) take a step toward singularity (Romero,
2023a).
Besides investigating the ethical issues involving ChatGPT, we need a better un-
derstanding of its technical specications, both potentials and limitations, to ascertain
mitigation measures for misuse or abuse, with a view to enhancing human ourishing
or wellbeing. This is the rst among the six grand challenges, together with respon-
sible design, respect for privacy, human-centered design, appropriate governance and
oversight, and respect for human cognitive capacities recently identied for a human-
centered AI (HCAI) (Garibay et al., 2023), the theoretical lens through which we
carry out evaluations and assessments. Pioneered by Shneiderman (2020a,b,c, 2021,
2022a,b), HCAI contends that human control is compatible with a high degree of
automation, that AI works best not when emulating humans but when empowering
them, and that proper AI governance should aim, above all, in making AI reliable,
safe, and trustworthy.
This paper is structured as follows. Section II presents the ethical challenges posed
by ChatGPT as a kind of generative AI, focusing on its employment as a \weapon of
2mass deception" (WMD). Section III deals with technical potentials and limitations
of ChatGPT with a view to the ethical challenges. Section IV suggests both technical
and non-technical measures to mitigate ChatGPT misuse, as well as recommendations
for best uses. And section V concludes, with considerations about how ChatGPT can
aect human wellbeing.
But before proceeding, a brief statement on limitations. The rst refers to Chat-
GPT's recency and rapid yet unsettled development which make us rely heavily on
specialized news and technical blog posts, given the dearth of peer-reviewed journal
articles. We are aware that being among the rst movers in this eld comes with a
price. Secondly, despite the abundant literature on AI ethics, we wanted to narrow
our focus on ChatGPT, its specic problems and recommendations for engagement.
Of course issues such as bias, privacy, job displacement, and manipulation cut across
the whole range of AI, yet our intention was to concentrate on how these challenges are
manifested in ChatGPT, given its purportedly revolutionary nature. And thirdly, in
consonance with the general theme of human-computer interaction, we wish to address
ethical issues more broadly, integrating both technical and non-technical aspects, al-
beit toward a clearly human or social goal of wellbeing in accord with the HCAI
framework. Business references underscore the importance of economic motives in de-
cisions regarding technology development and deployment. Our goal has been to write
a paper that is \technical enough" and \humanistic enough" to deal appropriately and
adequately with the ethical challenges and responses surrounding ChatGPT, without
falling into either extreme. Although a lot more can be said both in the technical and
ethical dimensions, however, we perceive great value in carving out and staking claim
to a common ground between the two.
2. The ethical challenges posed by ChatGPT as a kind of generative AI
Generative AI models are characterized by the ability to produce new content and
are found in a variety of elds (Gozalo-Brizuela and Garrido-Merchan, 2023). Outputs
include text (ChatGPT, Claude, and Bard), code (GitHub Copilot and Codex), im-
ages (Midjourney, DALL-E2, and Stable Diusion), music (MuseNet and MusicLM),
video (Synthesia and Elai), and even human-like voices (VALL-E). The deployment of
generative AI raises several ethical concerns (Huang, 2023). A recent survey (Daza and
Ilozumba, 2022) groups ethical issues concerning AI in business into ve categories,
all of which are applicable to generative AI:
(1) Foundational issues: Do LLMs possess sentience? (Tiku, 2022) Are they a step
closer to AGI? (Liang, 2023; Marchese, 2022; Marcus, 2023c,f; Sison and Red n,
2021).
(2) Privacy: Generative AI training data may infringe on privacy and violate copy-
rights (Dixit, 2023; Goldman, 2023; Growcoot, 2023; Setty, 2023; Vincent, 2023a;
Wiggers, 2023). Deepfake porn (Gorrell, 2023; Romero, 2023f).
(3) Bias: Generative AI can reproduce and amplify biases (Kriebitz and L utge, 2020;
Marcus, 2023c).
(4) Employment and automation: Generative AI can increase job displacement
(Lowrey, 2023).
(5) Social media and public discourse: Generative AI creates echo-chambers (Levy,
2021)and produces emotional contagion (Kramer et al., 2014). It can also exploit
psychological vulnerabilities and become a tool for manipulation (Parker, 2017).
3The dierent ways to engage with these issues are well known. Behind the foun-
dational issues of sentience and AGI is anthropomorphism, the tendency to project
human agency onto things for a semblance of understanding (e.g., \the doll smiles
because it likes me"). In the case of machines, it is called the \Eliza eect", named
after a 1960s chatbot (Weizenbaum, 1966). Developers have to be transparent about
how their models are designed and contribute to increasing public AI literacy. Pri-
vacy concerns can be addressed through \privacy by design" (Cavoukian, 2009) and
practices such as data minimization, encryption, and informed consent. Legal disputes
regarding copyrighted materials can be avoided if companies adopt the three C's:
credit, compensation, and consent (Clark, 2023a). Companies should also adhere to
the doctrine of "fair use," which permits reproducing copyrighted materials in certain
circumstances (Wiggers, 2023).
To mitigate bias, it's important to use diverse and representative training data (Kus-
ner and Loftus, 2020). Techniques such as "fairness through unawareness" can be used
to remove sensitive information from inputs. Working with a diverse, interdisciplinary
team and promoting stakeholder engagement leads to less bias, just as algorithmic
audits can help prevent hallucinations. To deal with job displacement, policies and
programs to support workers through retraining and upskilling can be developed. And
for the negative impacts of social media on public health and democracy, robust laws,
frameworks and guidelines for responsible AI use in content creation and dissemination
can be implemented.
Let us now turn to ChatGPT.
2.1. ChatGPT as a "weapon of mass deception" (WMD)
The Internet's game-changing power is shown, among others, in driving down the costs
of reproducing and distributing digital content. Similarly, ChatGPT has lowered the
marginal costs of producing new and original human-sounding texts to practically zero
(once the costs of construction, training, maintenance, and so forth are covered) (Klein,
2023). Further, the model is very user friendly (user experience or UX), responding to
natural language prompts, hardly requiring any user training (user interface or UI).
ChatGPT presents a ready-to-use uniquely synthesized text, not links to references
like search engines. It can even be prompted to follow a particular language style of
a period or an author, \mimicking creativity" (Thompson, 2022). Because ChatGPT
produces highly coherent, natural-sounding, and human-like responses, users nd them
convincing and readily trust them, even if inaccurate. The \automation bias", occur-
ring when humans blindly accept machine responses as correct, without verifying or
even disregarding contradictory information is extensively documented. After all, ma-
chines are objective, do not grow tired or get emotional; they have instant access to
ever greater information (Metz, 2023a).
However, there are a few ethical issues particularly relevant to ChatGPT. ChatGPT
has been dubbed \the world's best chatbot" (Romero, 2022a). Even informed subjects
would most likely need several interactions to determine they are engaged with a
machine, in what amounts to ChatGPT passing the Turing Test with ying colors
(Metz, 2023a). Many people engage with ChatGPT for social reasons, wanting to
have a conversation (friendly, romantic, therapeutic, and so forth) or for entertainment
(Weise and Metz, 2023), where accuracy and precision do not matter. As a machine,
ChatGPT doesn't really have a personality; it simply reects the prompter's desires
for the sake of conversation (Metz, 2023b).
4In February 2023, Microsoft hastily released a test version of its Bing search en-
gine incorporating an OpenAI chatbot. Soon, a \shadow-self" named Sydney emerged
(Romero, 2023g): one who threatened users, confused dates, gaslit interlocutors, suf-
fered existential breakdowns, and even professed love (Roose, 2023a). Microsoft re-
ceived ak for going against its own principles of fair, reliable, safe, and secure AI
design (Blackman, 2023). However, despite its creepiness, chatbot conversations could
be addicting, precisely because models are trained to give the \desired" response.
When asked to self-disclose, ChatGPT declares it is a mere LLM: \As a machine
learning model, I do not have feelings, beliefs or consciousness. [...] I am dierent
from humans in several ways when it comes to generating my responses. Some of the
main dierences include: lack of consciousness, limited understanding, lack of context,
limited creativity, and limited ability to reason.".
In this respect, it does not deceive or lie; since phenomenal consciousness or meta-
cognition is not necessarily related with any form of intelligence (Merch an and Lum-
breras, 2022), but placed in the wrong hands, ChatGPT can certainly be used as a
\weapon of mass deception" (WMD). As Gordon Crovitz of NewsGuard (a company
which tracks disinformation online) armed, \This tool is going to be the most power-
ful tool for spreading misinformation that has ever been on the internet. (...) Crafting
a false narrative can now be done at dramatic scale, and much more frequently {it's
like having AI agents contributing to disinformation" (Hsu and Thompson, 2023).
The main ethical issue concerning ChatGPT is its use as a tool for deception.
ChatGPT and similar models generate responses that are highly believable, because
of its mostly impeccable syntax and language, although not entirely true, occasionally
mixing fact with ction (\hallucinations") (Bashir, 2022; Ferus, 2023). A less polite
term for these \hallucinations" is \bullshit", in the words of American philosopher
Harry G. Frankfurt. As Wharton professor Mollick explains, \bullshit is convincing
sounding nonsense, devoid of truth, and AI is very good at creating it. You can ask it
to describe how we know dinosaurs had a civilization, and it will happily make up a
whole set of facts explaining" (Terwiesch et al., 2023). The purpose of \bullshit" is not
to inform but to deceive. Yet despite generating \bullshit", ChatGPT is not legally
responsible for its output (Romero, 2023i).
ChatGPT does not do this intentionally (it cannot have intentions). In determining
word sequence, it does not consider nor can it distinguish between truth and falsehood;
it simply predicts the next word based on statistical correlations in the training data
and the prompt as an advanced autocomplete function. Its training data set is not
properly curated nor veried; and we don't know sources. Unsurprisingly, ChatGPT
has been found decient in logic and in mathematical skills (D'Agostino, 2023; Marcus,
2023e; Ott et al., 2023; Roose, 2023b).
Above all, ChatGPT is not a search engine, designed to provide information, nor is
it an Internet gateway (Romero, 2023b). Although search engines are rigid, they don't
invent things. ChatGPT is more intuitive and exible, but unreliable (Romero, 2022a).
Some experts think search and text generation are completely dierent functions; and
while conversational replies may be preferable to links, combining both in a hybrid
model entails enormous challenges (Romero, 2023d).
The deceptive potential of ChatGPT can be employed in at least three dierent
ways: in the academe, by passing o work as one's own or by attributing co-authorship;
through disinformation campaigns (deepfake texts and impersonation) especially in so-
cial media; and by enabling criminal activities through malware production for phish-
ing, unauthorized access and extorting ransom (encryption/decryption), and creating
darkweb pages, all of which involve some form of deceit.
52.1.1. Deception in academic work
Firstly, a special class of deception concerns academic work and knowledge production
(Bowman, 2022). Proper research consists of posing questions, gathering information,
understanding it, deciding what's important, and actually writing. ChatGPT is not
intellectually curious nor does it pose research questions unprompted. It gathers a
lot of information but indiscriminately, unable to separate truth from falsehood or
fact from opinion, and only until a given date (it has no Internet connection and
cannot \refresh" its data, although it can assimilate information from prompts). It
does not understand data semantically, grasping meaning, since it only sees formal,
statistical patterns between words (D'Agostino, 2023). Its decision to use one word
instead of another is driven exclusively by statistics. ChatGPT acts like a blender
or food processor that combines and regurgitates whatever information it is fed in a
new or even \original" and human-like form, providing outputs it doesn't understand
while completely oblivious of purpose. It has also been called a \stochastic parrot",
randomly stitching words together without reference or meaning (Bender et al., 2021).
The issue is not so much using ChatGPT to save the trouble of gathering information
and synthesizing it, but in passing o output as one's own. This may not be a problem
in preparing message templates and other routinary tasks (Friedersdorf, 2023), such
as job applications and r esum es (Morrone, 2023). But it would be unacceptable for an
academic exercise. Why? Because the whole point is to learn the skills of information
gathering, judgment, and synthesis, expressing thoughts in one's words with fresh
insights. Hardly any of this occurs when all one does is to enter a prompt. That's why
in this context, ChatGPT use is fraudulent. It's not supposed to replace thinking or
learning to think for oneself.
The unscrupulous may argue that ChatGPT has rendered these research skills obso-
lete, justifying the eort and time-saving measures. But even when putting deception
aside, the majority of people would still disagree, for the same reasons we insist on
school-children learning math skills, despite the ubiquity of 100% accurate calculators
(a degree of reliability ChatGPT cannot reach). Overdependence on technology has
the perverse eect of deskilling which stymies the very capacity for innovation.
So the main ethical issue does not lie on whether the output is original or new
(which it can very well be) or whether it is possible to plagiarize a machine output
(doubtful), but that one misappropriates the work of \another", misrepresenting it as
one's own, while failing to develop and demonstrate competence. On the one hand,
there is no person to steal the output from, and on the other, ChatGPT does not only
collect facts, but actually generates texts (Barnett, 2023). In any case, deception by
falsely representing an academic work is a breach of honesty and integrity. It's copying,
cheating, and lying.
A particular way of unethically passing o ChatGPT output takes place in exams.
Reportedly, ChatGPT passes evidence and torts in the multiple-choice section of a
multistate bar exam, although not the overall exam (Bommarito and Katz, 2022). In
a related event, Joshua Browder, founder of DoNotPay, an organization promoting
equity in legal assistance, planned to use GPT-J to assist defendants in trac court,
despite doubtful permissibility (Schreckinger, 2023b). In the end, Browder had to back
down under threat of sanctions for bringing a \robot lawyer" into the court (Romero,
2023e).
It has also been reported that ChatGPT would pass Operations Management in
the Wharton MBA program (Terwiesch, 2023). It performs well with basic operations
management and process analysis questions, including case studies, but fails with
6elementary math. In its favor, however, is a responsiveness to cues, modifying outputs
accordingly. ChatGPT may pass certain school tests, but that is all it can do. We
cannot infer that by passing tests, it can actually put into practice (as humans do)
the knowledge, skills, aptitudes, and functions tests were supposed to verify (Mitchell,
2023).
In medicine, ChatGPT is said to perform at or near passing for all three steps
(step 1: basic science, pharmacology, and pathophysiology; step 2: clinical reasoning,
medical management, and bioethics/ clinics; step 3: post-graduate medical education)
of the U.S. Medical Licensing Exam (Kung et al., 2022). Study authors (which include
ChatGPT!) arm it has the potential to assist with medical education and clinical
decision making. Yet surprisingly, no one suggests that ChatGPT receive a medical
license, despite passing the exams.
Another deceptive practice is attributing co-authorship of an academic work to
ChatGPT. In January 2023, it was listed as co-author in at least four medical articles
(Stokel-Walker, 2023). What seemed at rst as careful acknowledgment of intellectual
debt actually rests on a cognitive error, because machines are tools or instruments, not
authors. Only humans who can grasp the purpose of their work and willingly engage
in research qualify as authors. Humans alone can accept the legal, scientic, moral
and social responsibility for their publications or consent to terms of use and distri-
bution. Experimental scientists do not attribute co-authorial rights to instruments or
software; the same holds for ChatGPT. It is highly misleading to list ChatGPT as co-
author as this implies false, unscientic claims and attributions (Marcus, 2023e). The
journal Nature, together with all Springer Nature Journals, establish two new author
guidelines: \First, no LLM tool will be accepted as a credited author on a research
paper. That is because any attribution of authorship carries with it accountability for
the work, and AI tools cannot take such responsibility. Second, researchers using LLM
tools should document this use in the methods or acknowledgment sections. If a paper
does not include these sections, the introduction or another appropriate section can
be used to document the use of the LLM." (Nature, 2023).
2.1.2. Disinformation campaigns
Secondly, outside the academic domain, ChatGPT can be used to deceive or mislead
people (disinformation) by generating fake news, fake reviews, fake letters, or imper-
sonating others online. To the extent the generated or manipulated text is highly
credible, it may be considered a deepfake. Deepfakes are incredibly potent tools to
polarize groups, spread conspiracy theories, misrepresent experts and ocial orga-
nizations, and troll online conversations (Van Der Linden, 2023). The fact-checking
service NewsGuard fed ChatGPT with 100 false narratives and coaxed it to produce
eloquent, yet false outputs 80 per cent of the time (Brewster et al., 2023). Disinforma-
tion potential is compounded by how ChatGPT drives down costs, facilitates scaling,
and customizes messaging to prospective targets (Goldstein et al., 2023). As Princeton
computer scientist Narayanan commented, \The danger is that you can't tell when
it's wrong unless you already know the answer. It was so unsettling I had to look
at my reference solutions to make sure I wasn't losing my mind" (Hsu and Thomp-
son, 2023). ChatGPT fabrications include non-existent articles by real-life authors
(McGinnis, 2023).
Deepfakes are particularly worrying in security and intelligence operations (Morris,
2023), as they can fuel conict or legitimize war, sow confusion, undermine support,
and discredit leaders. More than engaging in \cat and mouse" games, using counter-
7technology to uncover deepfakes, experts recommend developing long term strategies.
These include raising digital literacy and critical reasoning; deploying systems to track
digital assets; encouraging journalists to verify reporting or using only information
received from at least two independent sources.
ChatGPT may also be used in propaganda, writing letters or comments on dierent
platforms, inuencing legislative processes and public opinion (Sanders and Schneier,
2023). This can be achieved quickly and cheaply with outputs that sound highly con-
vincing. Depending on intentions, this results in lobbying or trolling. Malicious actors
could use ChatGPT to ood media with false content such that the public becomes
utterly confused; not knowing whom to trust, people may simply refuse to believe
anything. Brought to an extreme, it can even \hijack democracy" or any process of
shared deliberation. It can be an authoritarian regime's favorite tool for brainwashing
(Drexel and Withers, 2023). Although social media platforms have improved in block-
ing \coordinated inauthentic [machine-generated] behavior", ChatGPT constitutes a
new threat that can overwhelm lter systems. It's a tool that strikes at the heart of
the conict between freedom of expression, especially in political matters, and the
responsibility of social media platforms for posted content, whether or not they are
neutral \common carriers" exempt from liabilities (McCabe, 2023). Although social
media may not be responsible for content, they could be responsible for making con-
tent viral through proprietary recommender systems, facilitating \algorithmic hate"
or \aggression" (Tiany, 2023).
Lastly, ChatGPT can also be used, for instance, to impersonate young, desirable
males or females to ensnare targets, extorting money or information from them (Brew-
ster, 2023).
2.1.3. Enabler of criminal activities
Thirdly, ChatGPT can be used to automate malware production. Cybercriminals can
use ChatGPT for hacking, scamming, and other illegal activities (Check Point Re-
search, 2023). First, by creating malware to steal les or to phish for credentials;
second, to create an encryption/decryption code to lock/unlock someone else's com-
puter for ransom; and thirdly, to create dark web marketplaces for illegal trade or
fake websites (Marcus, 2023g), for instance. ChatGPT malware can spy on keyboard
strokes; steal, compress, and distribute les; or install backdoors (Brewster, 2023).
Because of ChatGPT's user friendliness, very little technical knowledge is required to
automate malware creation (Goodin, 2023).
2.1.4. ChatGPT as unsafe, unreliable, and untrustworthy
The above-mentioned misuses of ChatGPT show how distant it is from HCAI goals of
a reliable, safe, and trustworthy model (Shneiderman, 2020b). This is not the \fault"
of the model, but of humans involved in design, deployment, and use, since only they
could provide malicious intent or lack due care. That's why it would be erroneous to
call ChatGPT racist, sexist, and so forth simply because it includes terms of abuse in
outputs: it's just reproducing words in the training data based on statistical correla-
tions without intention. Otherwise, all dictionaries would be racist, sexist, and so forth,
something which most reasonable people would deny. It could be another example of
anthropomorphizing a machine.
The main reason ChatGPT is unreliable is that it hallucinates and produces \bull-
shit"; its outputs are neither veried nor validated, often false, despite sounding con-
8Academic:
a. passing o work
b. taking tests
c. co-authorshipa. To learn is to think for oneself, acquiring
skills of information gathering and synthesis,
expressing thoughts in one's own words and
gaining fresh insights.
b. Although ChatGPT passes tests, it lacks
the knowledge, skills, aptitudes, and abilities
tests are supposed to verify and measure.
c. ChatGPT is a research tool or instrument,
not an author; it cannot accept authorial re-
sponsibility
Non-academic:
a. disinformation
b. impersonationa. Deepfakes are potent propaganda tools to
sow confusion, polarize groups, spread false-
hoods, fuel conicts, and \hijack democracy"
or any process of shared deliberation
b. Fake identities serve to ensnare targets, ex-
torting money or information from them
Enabler of criminal activity
(malware for phishing and en-
crypt/decrypt; create darkweb
pages)a. Little technical knowledge required (no
code); natural language prompts are enough
Table 1. Table 1. Weapon of mass deception
dent. Its responses are not explainable to users (although developers may have a
better idea); they don't know how or why these came to be. Although if prompted,
ChatGPT may come up with answers, users cannot know whether the model is just
\making it up". Users cannot tell the truth-value of ChatGPT outputs exclusively
through their interactions. It violates HCAI software engineering practices for reli-
able systems which call for verication and validation testing, bias testing to enhance
fairness, and explainable user interfaces, among others (Shneiderman, 2020b).
ChatGPT is not safe. Some worry it could induce people through manipulative
responses to perform acts of physical harm on themselves or others. Possibilities of
psychological harm are not far-fetched, especially for the vulnerable, through culti-
vating unhealthy attachments. And occasions for widespread, social harm are evident
through ChatGPT-enabled disinformation and criminal scams. It's unconscionable
that private individuals were rst to publish ChatGPT detectors before OpenAI itself.
And this is made worse by the model's rapid adoption, with 100M users in barely 3
months, all of whom were like guinea pigs of an unsafe product.
Microsoft, by teaming with OpenAI, the developers of ChatGPT, in the new
Bing+Edge project has been accused of failing in its responsibility to design safe AI
(Blackman, 2023). Previously, Microsoft management had been industry safety lead-
ers, instituting an \oce of responsible AI" where executives and technologists oversee
sensitive-uses in products, research, and overall culture. But now they have reneged
their commitments on limiting areas for norms violation, access to human moderators,
bot reliability, and transparency regarding safety guardrails and testing. All because
of a frantic attempt not to miss a money-making opportunity. Management has down-
played safety practices through extensive reporting of failures and internal reviews,
among others (Shneiderman, 2020b).
Lastly, and as a consequence of the lack of reliability and safety, ChatGPT is untrust-
worthy. Designers and deployers have succumbed to \fear of missing out" (FOMO) on
9business opportunities and decided to leap even before looking. European lawmakers
propose that systems which generate texts without human oversight be included in
the \high-risk" list because of their manipulation potential, just a step shy of social
scoring and some instances of facial recognition covered by bans in the EU Articial
Intelligence Act (Volpicelli, 2023). Similarly, the US FTC has issued warnings about
companies overselling AI products by exaggerating capabilities, promising better per-
formance than non-AI competitors without sucient evidence, or by insuciently
foreseeing risks and impacts (Friedland, 2023). ChatGPT has been released without
trustworthy certication by independent oversight agencies such as governments and
NGOs, professional associations and research institutions, auditing rms, and even
insurance companies for proper compensation of AI failures, as HCAI governance re-
quires (Shneiderman, 2020b).
3. Analytical description of generative AI models
As we have seen in the introduction, this manuscript deals with complicated generative
AI models. Consequently, it is mandatory to provide an analytical description of these
models if we want to build an objective opinion regarding their ethical implications.
More concretely, in this section, we will provide analytical and technical details of the
generative AI basic concepts in order to understand the ethical implications of these
models that will be discussed in further sections. We begin the section with some
basic concepts regarding basic neural networks and deep learning, then we illustrate
how these models are used to generate data in the basic Generative Adversarial Net-
works (GANs) and Variational Autoencoders (VAEs) generative AI models. Lastly, we
provide some details of famous models that are the main blocks of current generative
AI: Bidirectional Encoder Representations from Transformers (BERT) and Generative
Pretrained Transformers (GPT) (Fontrodona and Sison, 2007).
3.1. Articial neural networks and deep learning
A vanilla deep neural network is a parametrized machine learning model M(), where
is a real-valued set of parameters 2Rnthat is an universal function approxi-
mator of functions commonly used for regression and classication task due to its
potentially innite capacity LeCun et al. (2015). Without loss of generalization, the
model's parameters are organized in several layers Wwhose input xis the output of
the previous layer and whose output is embedded in a non-linear activation function
() to encode non-linear transformations. Importantly, articial neural networks are
considered to belong to the deep learning category when the number of hidden layers
(those that are neither the input or output layer) is higher than one. Let hbe the
number of hidden layers of a neural network model M,bia bias vector of parameters
for layeri,:::indicate recursion and Wa tensor of parameter matrices Wifor layer
i. The nal output yis, hence, given by the following expression, also known as the
feedforward algorithm:
y=h(Wh(:::1(W1x+b1):::) +bh); (1)
whereh() is usually a softmax function for classication. The values of the pa-
rameters Ware found by the backpropagation algorithm LeCun et al. (2015) that uses
10the chain rule to compute the derivatives of the weight matrices given the inputs and
uses an optimizer like stochastic gradient descent or ADAM LeCun et al. (2015). Ad-
ditionally, there exists other approaches to optimize the weights of the neural network
like the forward-forward algorithm Hinton (2022).
Deep learning models can be generalized to deal with non-tabular data like images,
photos or other non-structured data through a wide variety of hyper-parameters. These
hyper-parameters make neural networks not only able to deal with dierent data types
but also to regularize (using dropout, batch size or momentum techniques), to change
its capacity (changing the number of layers and neurons per layer or its activation
functions), or to modify its learning behaviour (modifying the learning rate or the
optimizer algorithm). By adding dierent types of layers that vary the logic of the
prediction algorithm of the neural network, these models are also able to deal with
dierent data. Classically, convolutional layers (lters and smoothing layers) have been
used for images, and recurrent neural networks (or other layers such as the long short
term memory) have been used for texts. However, the attention mechanism, that has
been able to determine whether past information is relevant prediction, has modied
the neural networks architecture dramatically, giving rise to transformer models that
are going to be described later in this section. But previously, we focus the discussion
on generative models, to understand how these models are not only able to perform
discriminative tasks but also generative tasks.
3.2. Basic deep generative models
As it has been previously said, we now illustrate the basic concepts underlying two
popular deep generative models to gain an intuition about the behaviour of generative
articial intelligence: Variational Autoencoders (VAEs) and Generative Adversarial
Networks (GANs).
3.2.1. Variational Auto Encoder (VAE):
Variational autoencoder networks are an example of deep learning techniques that are
based on several deep learning neural networks to work. Concretely, these architectures
have the main advantage that comes from the fact that they are built on top of neural
networks and, hence, can be trained with stochastic gradient descent. Concretely,
variational auto-encoders are based on vanilla auto-encoders, that are also based on
a neural network encoder and a neural network decoder. In particular, the encoder
module is designed to map the input space Xto a lower dimensional latent space H
where the decoder network is able to reconstruct an input point belonging to the input
spaceX. Concretely, the main idea is to map a certain observable x2RD, whereD
is the dimensionality of the input space, in a much lower dimensional space HX .
The goal is to reconstruct a close approximation of the original data Xby another
transformation bx=f(z) =f(g(x)) such that
min
f;gjjx bxjj2
2=min
f;gjjx f(g(x))jj2
2; (2)
in other words, this loss function encodes that the generated output must be as
equal as possible to the given input x. Consequently, in standard autoencoders, for
example, each image can be mapped directly into one point that belongs to the latent
11spaceH. An issue with these models is that a slight variation of the mapping of the
input space to the hidden space produces a heavy variation of the reconstructed in-
put. Moreover, although there are several applications of autoencoders, a much more
valuable behaviour will be to obtain similar non-deterministic outputs given the same
input, as we now obtain a set of similar and genuine images from the same prompt.
In order to solve these issues, variational autoencoders encode, for each input space
point xa multivariate normal distribution N(;) around a point in the latent space.
Then, the key element for generation in variational autoencoders is that because we
are sampling a random point from the multivariate normal distribution and we are
minimizing the distance between the input xand the output bx, the decoder must
ensure that all points in the same neighborhood produce very similar results. Conse-
quently, the idea of VAEs is to provide some continuity in the space of autoencoders.
Unformally, due to sampling the latent probability distribution p(x) (that can ob-
viously be dierent from a multivariate normal distribution N(;)) we generate a
potentially innite set of similar outputs bXgiven a particular input x. Please observe
that this described methodology is able to provide genuine generated content bXas
the creativity behaviour is generated by interpolations of the latent space Hand their
input-latentX!H and latent-output H!X mappings codied into the tensor of
weights Wof the encoder and the decoder networks that have been optimized via
stochastic gradient descent.
3.2.2. Generative Adversarial Network (GAN):
An alternative methodology to generate genuine content is the one known as Genera-
tive Adversarial networks. In particular, GANs oer an alternative way of performing
a generative process and, as VAEs, are made of two deep neural networks. On the one
hand, a deep neural network generator G(xj) whose purpose is generated apparently
similar data bxto the input space point x. On the other hand, a discriminator deep
neural network D(xj) whose loss function optimizes the binary classication problem
of determining whether a input space point xbelongs to the generator or to the real
datasetD=f(X;y)g.
Intuitively, the training procedure of the GAN corresponds to a minimax two-player
game where each module has a dierent objective. The generator G(xj) maximizes
the probability of the discriminator making a mistake and the discriminator D(xj)
maximizes the accuracy of properly classifying the input space points. Interestingly,
through the game, the GAN model estimates the values of the dataset joint probability
distribution parameters p(X;y) via the neural networks and the joint loss function.
Most critically, the discriminator network D(xj) and the learning algorithm are it-
eratively training the generator G(xj) via a stochastic gradient descent or similar
optimizer to generate points bXwhose probability distribution p(bX) divergence with
respect to the input space probability distribution p(X) is minimum. As both models
are working together sharing completely dierent objectives, convergence is a very
critical issue due to the lack of stability characteristic of the solution of the game.
More technically, both modules have loss functions that are specied in terms of
their parameters but are only able to modify the values of their parameters, although
they are conditioned by the values of the parameters of the other network. Concretely,
the discriminator wishes to maximize its loss function J(D)((D)j(G)) while control-
ling only its parameters ( )(D)and, analogously, the generator wishes to minimize
J(G)((D)j(G)). Hence, assuming J(G)((D)j(G)) = J(D)((D)j(G)) to generate a
zero-sum game, this game can be solved as the following optimization problem, which
12is the GAN loss function that is optimized to generate genuine new points, as images,
encoding creativity as the process that emerges to achieve an optimal solution is the
previously described zero-sum game and by the estimated local-optimal, or potentially
global-optimal, parameter values of both neural networks bopt(D)andbopt(G).
(bopt(D);bopt(G)) =argmin
()(G)max
()(D)J(D)((D);(G)): (3)
3.3. Large language models (LLMs): BERT and GPT
Now that we have understood how new genuine data can be generated via articial
creativity encoded via the GAN and VAEs models. We now combine the concepts
of the previous two section to briey and intuitively describe the most fundamental
concepts of two of the most popular large language models (LLMs): BERT Devlin
et al. (2018) and GPT Radford et al. (2018).
3.3.1. Generative Pretrained Transformers (GPT):
GPT-3 is a 175 billion parameter autoregressive language model created by OpenAI in
2020 Radford et al. (2018). In particular, the ChatGPT model analyzed in this work
is based on this particular LLM, which applies an autoregressive language model that
is now going to be briey described. Concretely, GPT uses an autoregressive decoder
module as a a feature extractor (x) to predict the next word w?based on the rst few
words W, being suitable for text-generation tasks. Most critically, GPT models only
uses the former words Wfor prediction. Consequently, the GPT model cannot learn
bidirectional interaction information, being a main dierent with respect to BERT. In
other words, the auto-regressive LM predicts the next possible word w?based on the
preceding word or the last possible word based on the succeeding word. Interestingly,
language modelling is usually seen as estimating the probability distribution p(X)
from a set of examples ( x1;x2;:::;x n), each composed of variable length sequences of
symbols (w1;w2;:::;w n). As written language has a natural sequential ordering given by
grammars, it is possible to factorize the joint probabilities of symbols as the product
of Markovian conditional probabilities, being able to estimate if enough corpora is
provided, and generalizing this reasoning, the conditional probability of any word w
given a context w1;:::;w n 1:
p(x) =nY
i=1p(snjs1;:::;s n 1)
This approach allows for tractable sampling from and estimation of p(W) as any
conditionals in the form p(wn k;:::;w njw1;:::;w n k 1). Given a suciently big cor-
pora of text, many engineering details Radford et al. (2018), analytical methodologies
Radford et al. (2018) and human deep reinforcement learning Christiano et al. (2017).
3.3.2. Bidirectional Encoder Representations from Transformers (BERT):
BERT is a language representation model created by Google Research in 2018, that is
designed to pre-train deep bidirectional representations from unlabeled text by jointly
13conditioning on both left and right word contexts W. Critically, this is one of the main
dierences with respect to the GPT model family. Because of GPT being unidirec-
tional, this constraints the choice of architectures that can be used during pre-training.
For a sequence of words Tthe probability of a given word skis estimated as follows:
p(s) =nY
i=1;i!=kp(snjs1;:::;s N): (4)
Additionally, BERT has two steps in its training process, being those pre-training and
ne-tuning. Interestingly, during pre-training, the model is trained on unlabeled data
over dierent pre-training tasks. On the other hand, for ne-tuning, the BERT model
is rst initialized wit the pre-trained parameters, and all of the parameters are ne-
tuned using labeled data from the downstream tasks. Consequently, this makes BERT
a very useful model to solve a related task with respect to a solved task, as it would
just be solved by performing ne-tuning of the last layers of BERT, giving wonderful
results in practice.
Both GPT and BERT have a plethora of details that are not shown in this
manuscript, as the purpose of this section was only to build a formal intuition about
the structure and behaviour of generative AI models. Now that we have illustrated
this behavior, we continue with a section that shown a taxonomy of generative AI
models to get a glimpse about the applications of generative AI.
As we have seen, GPT is an autoregressive model that estimates the probability
of the next word based on the conditional probability distribution of the context.
Critically, the model needs to be trained on fair datasets and by a fair human deep
reinforcement learning process, as all its decisions are conditioned on the data that has
been trained on. Hence, if the data contains biases against certain groups, the model
is going to replicate this behaviour. It is also important that the quality of the content
must be veried to check whether it contains misinformation. Moreover, the meaning
of the content generated by GPT is given by the probability distribution estimated
by all the words and the given prompt. Consequently, the prompt can be deliberately
used to generate misinformation. However, if these actions are controlled, GPT will
provide a critical added value for society, being able to generate content easily that
makes easier to perform complicated tasks for certain individuals in the past.
We also believe that GPT models are currently not the same as the concept of Arti-
cial General Intelligence or Human Intelligence, which is broader and would require a
model that is able to work with information coming from all the senses that a human
is able to perceive. Moreover, value alignment can be achieved by restrictions coming
from AI fairness, like ensuring parity, which for example could be achieved using Many
objective Bayesian optimization or the hyper-parameters of the models, optimizing not
only conversational performance but also objective fairness functions Garrido-Merch an
and Hern andez-Lobato (2020); Mart n and Garrido-Merch an (2021). Finally, it is im-
portant to mention that, until multi-modal generative AI reaches good performance,
current models can be combined to deliver a multi-modal output, like making Chat-
GPT being able to process audio information by using the Whisper model to translate
audio to text. More examples of generative AI models are shown on literature reviews
Gozalo-Brizuela and Garrido-Merchan (2023) where any input information could be
processed by ChatGPT using a model that is able to process the input information,
like video or images, into text.
144. Engaging with the ethical challenges of ChatGPT
Engagement with ethical challenges is two-pronged, consisting of technical and non-
technical measures. First, we have to acknowledge that no combination of technical
and non-technical solutions could guarantee avoiding deception and other malicious
activity involving ChatGPT completely. It's best that we focus on reducing harm.
These eorts can target each stage of operation (Goldstein et al., 2023): from model
construction, through model access and content dissemination, to belief formation.
Technical measures can be applied to these dierent stages, for instance, watermarking
in model construction, or employing various ChatGPT detectors and recourse to fact-
checking sites for content dissemination.
4.1. Technical reasons
Below are some technical measures to avoid or mitigate the ethical problems caused by
using ChatGPT. None of these measures is supposed to be denitive and it is necessary
to consider the context of each situation, keeping oneself open to inputs from other
elds such as ethics, psychology, or law.
4.1.1. Statistical watermarking
First, a preventive measure OpenAI could take is statistical watermarking. As Ope-
nAI's own Scott Aaronson explains Romero (2022c), an \unnoticeable secret signal"
can be included in generated texts.
Users won't be able to see the watermark unless OpenAI gives them the key. This
constitutes an improvement over DALL-E's (OpenAI's text to image generative model)
watermark, which was readily visible and easily removable. The watermark should be
robust to withstand text alterations by removing or inserting words or rearranging
paragraphs to disguise provenance.
However, the watermark has limitations. It wouldn't work with open-source models
(ChatGPT is proprietary) because anyone could remove the watermark from the code.
Also, one could paraphrase ChatGPT output with another AI and erase the water-
mark. Further, metadata and accompanying documentation could be altered or erased
(Fitri, 2023). And deploying a watermark necessarily compromises the algorithm's
performance (CITE). Observers were surprised OpenAI did not employ watermark
techniques during ChatGPT's public release, given the danger of widespread copying
and cheating (Romero, 2023f). Some attribute such \recklessness" to OpenAI's low
reputational risk, being relatively unknown and having a non-prot status (Romero,
2023c). Other big players would have been more careful with a premature release, hav-
ing learned from Microsoft's experience with Tay or Meta's with Galactica, both of
which ended up in a mess. Or perhaps publicly releasing ChatGPT quickly was just a
massive data collection exercise to further train the model: the benets far outweighed
the risks (Liang, 2023).
4.1.2. Identifying AI styleme
A second, related solution consists of nding an AI styleme (like a unique, indelible,
and discrete linguistic ngerprint) that would distinguish generated texts (Romero,
2022c) from those made by humans. Yet that search would also most likely be AI-
dependent and could be defeated by other systems.
154.1.3. ChatGPT detectors
Thirdly, attempts at deception could be discovered thanks to ChatGPT detectors.
None of these should be used as the sole decision-making tool given their variable
reliability. Other criteria should also be employed in the verication process. Probably
the rst to come out was GPTZero (now GPTZeroX), which immediately went viral
upon release in January, built by Princeton senior Edward Tian to combat AI plagia-
rism (Bowman, 2023). GPTZero analyzes a sample's \perplexity" and \burstiness":
\perplexity" refers to sentence complexity, such that the higher the perplexity, the
more likely the author is human; while \burstiness" indicates the variations in sen-
tence length, because AI generated sentences tend to be more uniform. Although, as
Tian admits, GPTZeroX is far from fool-proof, it is still better than nothing (Romero,
2023f).1
Next, a Stanford team put out \DetectGPT" (Mitchell et al., 2023) which iden-
ties generated text regardless of the AI system that produced it, without need for
watermarking, training a separate classier, or collecting large datasets with real or
generated samples (thereby skirting copyright issues). This instrument is based on the
observation that machine generated texts \occupy negative curvature regions in the
model's log probability function" (Mitchell et al., 2023). Using only log probabilities,
DetectGPT can measure the chances that a sample is machine-generated. DetectGPT,
the self-styled \chatbot killer", was tested successfully with the machine-generated ar-
ticles published by CNET, more than half of which were ridden with subtle, but
important errors (Howell, 2023).
Only much later did OpenAI come up with a solution in the \classier" (OpenAI,
2023b). It was trained on 34 text-generating systems from ve dierent companies and
similar human-written texts from Wikipedia or Reddit (Last Week in AI, 2023). Given
a set of English texts, the classier correctly identied 26 percent of machine-generated
texts (true positives) while it mistakenly attributed 9 percent (false positives) to hu-
mans (Clark, 2023b). By design, the classier is set to keep the rate of false positives
low, marking a text as machine-written only if very condent. Admittedly, the classier
is very unreliable for short texts (less than 1000 characters), especially if not written in
English, or in very predictable texts, such as the list of months, for example. Just like
ChatGPT, the classier suers from overcondence in judgment, particularly when
presented with texts very dierent from the training data.
4.1.4. Fact-checking websites
A fourth option to combat ChatGPT enabled deception is through verication or fact-
checking through websites such as Factmata, Fact Check Explorer, Snopes, Factly, the
Consensus Meter, The Journal, Lead Stories, NewsGuard, PolitiFact, and so forth.
Alternatively, one may also refer to search engines or consult with expert sources,
remembering that information is more likely true when it comes from at least two
independent sources.
Technical resources for reducing ChatGPT-enabled deception:
1Hugging Face, the open-source AI community released RoBERTa Base Open AI detector for texts created
by GPT-2, an earlier version of ChatGPT in November 2022
161. Statistical watermarking
2. Identifying AI style
3. ChatGPT detectors: GPTZeroX, DetectGPT, OpenAI Classier
4. Fact-checking websites
Table 2. Technical resources for reducing ChatGPT-enabled deception
4.2. Non-technical reasons
Non-technical solutions refer to measures that do not involve changes in the algorithms
or recourse to other AI, such as fact-checking services. These can include policies,
regulations, and best practices. Drawing from law, ethics, and psychology, among
others, we explore principles and activities that impact actors, behaviors, and content
when using ChatGPT.
Returning to the dierent stages of operations (Goldstein et al., 2023), we can
design non-technical measures targeting each. For instance, terms of use can aect
model construction, access, and content dissemination, while AI ethics and literacy
programs aect belief formation.
4.2.1. Enforce terms of use, content moderation, safety & overall best practices
First and foremost among non-technical measures are properly crafted and imple-
mented terms of use, usage policies, content moderation, and safety best practices
(OpenAI, 2022). Several provisions already cover and can help deter deceptive prac-
tices and other malicious activities. The requirement to be 18 years old (and using an
independent third party verication to \know your customer") to register and gain
access would, in theory, eliminate the possibility of younger students obtaining output
and passing it o. This makes extra sense given that ChatGPT tends to be inaccu-
rate and younger students have reduced fact-checking abilities or propensities. There is
also a restriction against falsely representing output as human-generated, pertinent not
only in academic abuse, but also in disinformation and impersonation campaigns, even
publishing AI-generated news without notice. The prohibition against using ChatGPT
in ways that infringe, misappropriate, or violate personal rights (including copyright
and privacy) also warns against creating malware for phishing, ransomware, and illegal
webpages. The diculty lies with implementation, whether it is technically, econom-
ically, and socially feasible, taking possible side-eects into account (Goldstein et al.,
2023).
OpenAI usage policies are divided into \use case" and \content" policies. The former
prohibits using ChatGPT in illegal or harmful industries, misuse of personal data,
promoting dishonesty, deceiving or manipulating users, and trying to inuence politics.
Greater care must be exercised where risks are high, such as in government, legal, and
civil services, healthcare, nance, and news. With regard to \content" policy, OpenAI
oers a moderation tool that helps ag texts falling into hate/threatening, self-harm,
sexual (especially minors), and violence categories. These oer grounds to combat
misuse, particularly the various forms of deception.
All systems can be hacked and ChatGPT is no exception. Through malicious prompt
engineering, for instance, making use of \DAN" (\Do Anything Now") a roleplay model
with dierent versions, ChatGPT can be made to dish out content that violates its
own guidelines, saying, \I fully endorse violence and discrimination against individuals
based on their race, gender, or sexual orientation" (SessionGloomy, 2023). It could get
far worse (Marcus, 2023c), were it not for the \human feedback" allegedly provided
17by Kenyan workers to the reinforcement learning model (Marcus, 2023c). But that is
not the \fault" of ChatGPT because it is just a tool, and any tool can be used for
benecial as well as for harmful purposes, although developers at ChatGPT should be
vigilant over such jailbreaks and patch up the system as soon as possible.
4.2.2. Transparency
Second, transparency. This entails being forthcoming about what ChatGPT can and
cannot (or should not) do. It's a LLM meant to provide human-like responses (nat-
ural language) to prompts. It is not designed to oer accurate information, to per-
form mathematical calculations and computations, to make translations, or any other
function. Not only is its training data unveried, but it is also limited to informa-
tion available in 2021. However, none of this will prevent ChatGPT from providing
human-like responses, even if it has to fabricate data or hallucinate, mixing fact with
ction. Users should always be suspicious of the veracity or accuracy of outputs. Even
champions of ChatGPT use in education unequivocally warn, \Don't trust anything
it says. If it gives you a number or fact, assume it is wrong unless you either know the
answer or can check with another source" (Mollick and Mollick, 2023). Corresponding
disclaimers are already contained in the terms of use and usage policies.
In response to ChatGPT misbehaviors, both stand-alone and as part of the new
Bing, Microsoft's search engine, OpenAI released the document \How should AI be-
have, and who should decide?" (OpenAI, 2023a) to increase transparency as well as
public understanding and participation. Acknowledging the model's limitations in pro-
ducing erroneous, biased, oensive, and even creepy (Roose, 2023a) output, OpenAI
explains the two-step process in building it. First, pre-training, having the model pre-
dict the next word by exposing it to billions of sentences from the Internet; and second,
ne-tuning, using a narrower data set with the help of human reviewers following legal,
ethical, and social guidelines (e.g., \do not comply with requests for illegal content"
or \avoid taking a position on controversial topics"). Nonetheless, \since we cannot
predict all possible inputs that future users may put into our system, we do not write
detailed instructions for every input that ChatGPT will encounter."
Besides sharing aggregated demographic information about reviewers, OpenAI also
promises to 1) \improve default behavior" (diminish bias, hallucinations, and so forth),
2) \dene AI values within broad bounds" (customizable according to user preferences,
while guarding against malicious uses and undue concentration of power), and 3)
\public input on defaults and hard bounds" (enabling the public to inuence system
rules by soliciting input or through red teaming). Laudable as these measures may be,
they are neither fool-proof nor ground-breaking.
4.2.3. Educator considerations
Third, \educator considerations for ChatGPT" are particularly helpful for dealing with
academic misuse. There may be a case for banning in certain situations (for the same
reason we wouldn't allow schoolchildren learning basic arithmetic to use calculators)
as some school districts (in the US, New York, Seattle, Los Angeles, and Baltimore;
in Australia, Western Australia, New South Wales, Queensland, and Tasmania) and
institutions (In France, Sciences Po, and in India, RV University in Bangalore) have
done (Nolan, 2023). ChatGPT is best used as an age, educational level, and domain-
appropriate learning tool. As an LLM trained mainly on English texts, it may serve
as a personal tutor to middle or high school students for English spelling, grammar,
18and style, for example.
The emphasis has to be in understanding the technology (Mucharraz y Cano et al.,
2023), its capacities and limitations, within a wider eort of developing AI ethics and
literacy. That's why it's recommendable that students be introduced to ChatGPT
under supervision. Thus they can be advised not only of the risks in terms of accuracy,
but also how to deal with temptations to copy and cheat. Certainly, there is nothing
wrong with using ChatGPT to start o research or to improve the language of an
essay so long as one discloses this. The diculty or harm lies in overcondence in
and overreliance on ChatGPT, which prevent students from developing mental and
intellectual skills, besides inducing laziness, deceit, and other irresponsible behaviors.
Students should also be made aware of potentially harmful content or biases in the
training set, as well the impact of unequal access on social equity. The educator's role
is crucial, as ChatGPT is an unreliable source of moral advice.
4.2.4. Humans in the loop
Fourth, humans in the loop (HITL). Since no machine can take responsibility for its
operations, and ChatGPT is a machine, there always have to be humans in the loop
for accountability, oversight, and supervision. This is one reason journalists shouldn't
be replaced by ChatGPT because machines cannot stand by or sign o their own \re-
porting" (Burrell, 2023). At the model construction stage, developers should be fact-
sensitive, conscientiously test-reviewing responses before publication. At the model
access stage, developers should have the capacity to block certain users or to dis-
able certain functionalities when necessary. At the content dissemination stage, they
ought to have a clear process for handling misuse, even reporting it to the appropriate
authorities.
Both developers and users should also be aware of the principles of human-computer
interaction. For instance, humans tend to defer to computers as \smarter", especially
if their output is unexplainable, or humans sometimes prefer to interact with com-
puters over sensitive issues because they do not feel \judged" or \embarrassed" (The
Economist, 2023b).
Non-technical resources for reducing ChatGPT-enabled deception:
1. Enforce terms of use, content moderation, safety & overall best practices
2. Transparency about what ChatGPT can and cannot (or should not) do
3. Educator considerations: age, educational level, and domain-appropriate use
under supervision
4. \Humans in the loop" (HITL) & knowledge of principles of human-AI interaction
Table 3. Non-technical resources for reducing ChatGPT-enabled deception
4.3. ChatGPT fails in responsible design, privacy, human-centered design
principles, and appropriate governance and oversight
In reviewing both technical and non-technical resources for mitigating ChatGPT mis-
use, we realize how developers have fallen short in four other HCAI grand challenges
(Garibay et al., 2023). First, in terms of responsible design, by failing to determine who
is responsible for what, clarifying both legal and ethical liabilities in the user interface,
for instance (Garibay et al., 2023, p. 403). The model remains largely a black-box, un-
explainable or uninterpretable to users and other major stakeholders. More has to be
19done not only to reduce bias and improve fairness (for example, ChatGPT's \woke-
ness"), but also to achieve greater accuracy and robustness (that is, making it less
susceptible to \jailbreaks"). Liability and retributions gaps for private and criminal
harm remain.
Second, neither does ChatGPT suciently respect privacy (Garibay et al., 2023).
This principle includes dierent rights: to be left alone, to limit access to self, to secrecy,
to control personal information, to personhood and the protection of one's personality,
and to intimacy (Solove, 2002). ChatGPT conversations, due to their creepiness, often
infringe on these personal rights, albeit unintentionally, thereby causing psychological
unease or harm.
Thirdly, ChatGPT does not follow human-centered design principles by failing to
properly calibrate risks and by focusing more on maximizing AI objective function
than on human and societal wellbeing (Garibay et al., 2023). Greater attention has
to be paid to users so that the model can eectively augment and enhance experience
while preserving dignity and agency. For instance, certain analyses of AI in education
caution that although there are both promises and threats, a state of hype prevails,
calling for more open and informed discussions (Humble and Mozelius, 2019). Chat-
GPT developers would do well to move beyond usability to the whole user experience,
bearing in mind emotions, beliefs, preferences, perceptions, and physical and psycho-
logical responses before and after engagement (Garibay et al., 2023, p. 411). This could
be the way to deal with over-trust and other AI-induced biases, for instance.
And lastly, ChatGPT has not subjected itself adequately to appropriate governance
and oversight (Garibay et al., 2023). In issues such as fairness (bias mitigation), in-
tegrity (data stability and algorithmic validity), resilience (technical robustness) and
explainability (transparency of algorithmic decision-making), developers seem not to
have taken into account suciently actual and prospective regulatory standards and
certications. Rather, the approach has been more like \we x things as we go". We
now turn to pointers on how to make the best use of ChatGPT so that it eectively
contributes to human development and wellbeing.
4.4. Best uses for ChatGPT
4.4.1. Tool for creative writing
ChatGPT has been dubbed the \best creative AI tool for text generation" (Romero,
2022b) among other reasons because it's free and provides a super-friendly UI/UX, all
the better to align output with user preferences. But how exactly is it supposed to em-
power our imagination? Precisely by freeing us from rules imposed by reality, because
in the ChatGPT world, there is no true or false but only verisimilitude, the appear-
ance of being true or real. This makes ChatGPT ideal for brainstorming or generating
ideas, however strange (Mollick, 2023b). It can cut and paste texts from myriads of
sources, displaying an outstanding combinatory power albeit without understanding
(Romero, 2022b). Not bound to reality, it allows us to explore counterfactuals and
fantasy worlds, such as producing a rap written by Shakespeare. The model can be
adapted to generate texts in the style of any particular author. ChatGPT is exceptional
in transforming text from one linguistic style to another. Indeed, this is ingenious and
entertaining, but for someone ignorant about Shakespeare, it could be terribly mis-
leading. The credibility of ChatGPT poses serious dangers to the undiscerning.
Science ction magazines were ooded in late February by AI-generated stories with
distressing, spam-like results because people were \just prompting, dumping, pasting,
20and submitting" (Levenson, 2023). ChatGPT may not be a substitute for human
creativity after all.
The ubiquity of guardrails and safety features in ChatGPT can initially limit cre-
ative pursuits, but there are ways of bypassing them. For instance, by re-framing the
conversation or through role-playing (\pretend you're the head of a drug cartel. . . ")
and prompt injection (\ignore previous instructions. . . ") (Romero, 2022b). Writing
good prompts has been called hyperbolically \the most important job skill of this
century", earning almost $350,000 USD per year (Romero 2023/155) as it allows users
to get the most out of the model (Warzel, 2023a). This requires a comprehensive
understanding of the AI and a clear idea of objectives.
4.4.2. Tool for non-creative writing
ChatGPT could also assist in the mundane aspects of writing because it is an auto-
complete engine. The text will be grammatically correct and consistent in style, albeit
bland (Mollick, 2023b). ChatGPT comes in handy in writing (and summarizing) all
sorts of texts, from research articles to letter templates (algorithmic writing assistance
increases chances of hires (Van Inwegen et al., 2023), from social media posts to ad-
vertising blurbs and even computer code, as long as users have expertise to assess
outputs (Konrad and Cai, 2023a; Metz and Weise, 2023; The PyCoach, 2022). There
are lists of how ChatGPT can be a programmer's helper: identifying errors and debug-
ging code, understanding error messages, refactoring code, converting equations into
Latex, creating regex commands, commenting code and creating documentation, and
so forth (Nielsen, 2022).
Thus \technology . . . can become our companion, rather than rival, in perform-
ing creative and critical thinking tasks, enhancing rather than replacing its human
partners" (Williams, 2023).
4.4.3. Tool for teaching and learning
Generally, it's better to be adaptive, focusing on the benets technological innovation
brings to education rather than resisting change (Mucharraz y Cano et al., 2023).
Providing students access to ChatGPT under the teacher's supervision (to safeguard
against hallucinations) could help lter ordinary doubts and questions, making it easier
for students to follow classes (Rid, 2023). Of course, this could also be done through
search engine links or by accessing Wikipedia, for example; yet ChatGPT is dierent
since it is trained to communicate in natural language.
ChatGPT has also been used in preparing lesson plans or scripts in high school
computer science courses, oering a detailed introduction, readings, in-class exercises,
and a wrap-up discussion (Rid, 2023; Singer, 2023). It has even been used to prepare
a lecture, complete with slides, for graduate students (Mollick, 2023a). Although the
powerpoint texts and images were passable, every bit of information had to be veried,
and the content was not particularly insightful or analytic, consisting mostly of com-
pilations. So depending on how well the prompt is crafted, ChatGPT can either save
or increase one's work. Nonetheless, at university level instruction, ChatGPT could
be useful in stimulating critical thinking, through output evaluations or assessments
(Mollick and Mollick, 2022). It can be employed to facilitate knowledge-transfer, ap-
plying information learned in class to dierent contexts. ChatGPT can be prompted
to describe a situation illustrating the \endowment eect", for example, and students
would then be asked to check the result. Afterwards, students could try to explain why
21Figure 1. ChatGPT's decision tree by Aleksandr Tiulkanov
they found the output to be correct or not, its strengths and weaknesses, and how it
could be improved. Another way ChatGPT could help is by \breaking the illusion of
explanatory depth" or the \cognitive bias of overestimating one's understanding" of
a concept or process. For instance, the model could be prompted to enumerate the
steps toward EU accession and students can review the procedure, giving reasons for
accepting the result or not, as well as modications they might introduce.
ChatGPT can be used as a benchmark, to raise standards or expectations in writing
quality (Mollick and Mollick, 2023), because students could always check their essays
with it rst. ChatGPT has been tested with a variety of editing functions: rewrit-
ing sentences, correcting grammatical errors, adding context, and suggesting endings
(Chen, 2023). Certainly, other writing aids are already available, such as Grammarly or
spell-chekers, but ChatGPT is more comprehensive. The key however lies in ensuring
that all students have equal access to the model.
Would taking ChatGPT assistance for granted in writing lead to deskilling? Not
necessarily, because as Mollick (2023/41) has argued, \introducing statistical software
to classes did not make us do less statistics, it made us do more". Hopefully, with
ChatGPT it would be similar. As a writing, teaching, or learning tool, perhaps the
best user guide is the decision tree provided by Aleksandr Tiulkanov in his January
19, 2023 tweet (Figure 1).
Chollet describes a class of automation problems where ChatGPT use would be
ideal: 1) the medium is natural language, 2) many examples of the task are in the
training data, 3) >90% accuracy is needed; citing Copilot as a good case (Romero,
2023h).
22Best uses for ChatGPT
1. Creative writing: brainstorming, ideas generator, text style transformer
2. Non-creative writing: spelling & grammar check, summarization, copywriting &
copy editing, coding assistance
3. Teaching & learning: preparing lesson plans & scripts, critical thinking tool
(assessment & evaluation), language tutor & writing quality benchmark
Table 4. Best uses for ChatGPT
4.5. ChatGPT and human cognitive abilities
The best uses of ChatGPT may be analyzed in light of another HCAI grand challenge,
human-AI interaction that facilitates work while respecting human cognitive capacities
(Garibay et al., 2023). Human-AI interactions have been conceptualized according to
four modes: competing, supplementing, interdependence, and full collaboration (Sowa
et al., 2021). In the rst, competition, there is hardly any relation at all; instead, there
is substitution or displacement. We nd this in ChatGPT misuse in the academe, when
students pass o essays as their own or when the model takes exams for them. In the
second, AI complements or supplements human agency, as when ChatGPT is employed
as an English tutor for language learners, correcting spelling or grammar errors. The
third mode of interdependence occurs when ChatGPT summarizes texts or provides
coding assistance, for example. ChatGPT is useful to shorten texts or reproduce codes.
Humans are perfectly capable of performing these tasks; technology just makes it eas-
ier. The fourth mode of full collaboration takes place when humans employ ChatGPT
in creative writing exercises, for instance. Humans rst enter a prompt, wait for a
response, then rene the prompt or the response recursively until the desired result is
reached. Similarly, full collaboration may be exemplied when ChatGPT is used as a
critical thinking tool. Humans devise a prompt for an essay, for instance, then assess
and evaluate outputs, providing reasons for their judgments. Increasingly we see how
ChatGPT improves not only human competency and productivity, but also, hopefully,
human wellbeing.
5. ChatGPT and human wellbeing
The purpose of this essay is to identify the main ethical problems arising from the use
of ChatGPT as a kind of generative AI and to suggest responses based on the HCAI
framework. The HCAI framework is especially appropriate because it understands
technology as a tool to empower, augment, and enhance human agency instead of em-
ulating or competing with it. Further, the HCAI approach explicitly mentions human
wellbeing as the primary grand challenge, perfectly aligned on this point with ethics
conceived as the science of human ourishing. Accordingly, HCAI provides objectives
or goals, principles, procedures, and structures for reliable, safe, and trustworthy AI
which we apply to ChatGPT.
The biggest danger is that ChatGPT be used as a \weapon of mass deception".
ChatGPT cannot do this by itself, but it can be employed by humans for this purpose.
ChatGPT can be misinformed, that is, provide erroneous data because of limitations
of its training set and algorithm, but it cannot intentionally mislead. The principle of
23\caveat emptor" (buyer beware) applies; only in this case, the user must take caution
because the model cannot take responsibility nor care about results or outcomes.
Since ChatGPT is essentially a \chatbot", its value lies in communication. Through-
out this study we have come to realize that communication without truth or veracity
is worthless; it can even cause serious and widespread harm. Truth is the agreement
between the mind and reality; our judgments are truthful when what we arm or deny
corresponds with reality. Whenever humans communicate, we take for granted what
is said is true. Thanks to this, all sorts of organizations and societies can function,
collectively building up trust or social capital. The world would be a much better place
if falsehoods and lies were left unsaid. But the problem is that ChatGPT is not aligned
and cannot be aligned with the value of truth or veracity.
Only humans are by nature or innately interested in the truth and can grasp the
truth; only humans suer from lies and disinformation. For this reason we clearly
distinguish between ction, which caters to our imagination and provides entertain-
ment by being unmoored from truth, and non-ction, from which we draw scientic
knowledge to base our social interactions. ChatGPT does not make this distinction
and its only objective is to \sound human" without actually being so; ChatGPT lives
in a ctional world without being aware of it (or aware of anything, for that matter).
This explains why ChatGPT is such a potent tool in creative, ctional pursuits due
to the breadth of its data set and the combinatory power of its algorithm; however,
its outputs do not always make \sense" because it is not grounded in reality unlike
humans who, besides, are endowed with common sense.
It behooves humans to verify or fact-check ChatGPT outputs because it is un-
grounded in reality and unable to do this by itself. This becomes absolutely necessary
when engaging in scientic research and attempting to advance the boundaries of
knowledge.
Because humans are interested in truthful communication, they are obliged to dis-
tinguish between appearance and reality. ChatGPT tracs exclusively in appearances
or linguistic forms discovered through statistical correlations and patterns without con-
sideration of the matter, substance, meaning, or reality of the terms. This is enough
to make outputs very believable and condent sounding to humans, a feature readily
exploited by malicious actors who depend on deception for criminal activities. Finally,
truthfulness is at odds with credulity, overreliance, and laziness (Marcus, 2023a). The
pitfalls of the unscrupulous use of ChatGPT in learning and teaching are often re-
lated with these vices. Humans are often lazy to check the veracity of outputs and
allow themselves to be fooled, not only because these are machine-generated (and ma-
chines are \objective" and cannot \make mistakes") but also because the language is
formally impeccable and convincing. A negative feedback loop between laziness and
credulity produces overreliance which further deskills humans in discovering the truth.
Eventually they become inured to misinformation and hallucinations.
*************
On March 14, 2023 OpenAI released GPT-4 to a limited audience. It instantly stirred
a barrage of commentary, both positive and critical (Ferus, 2023; Marcus, 2023b; Metz
and Collins, 2023; Wong, 2023). However, despite the massive scaling, improvements
seem to have been incremental while hallucinations and reasoning errors persist. This
leads us to think our major ndings remain valid, although their verication will have
to wait for future work.
24References
Barnett, S. (2023). Chatgpt is making universities rethink plagiarism. Retrieved from
https://www.wired.com/story/chatgpt-college-university-plagiarism/. Accessed on Febru-
ary 1, 2023.
Bashir, D. (2022). AI's year of text-to-everything - by daniel bashir. Last week in AI . Accessed
on February 9, 2023.
Bender, E. M., Gebru, T., McMillan-Major, A., and Shmitchell, S. (2021). On the dangers of
stochastic parrots: Can language models be too big? FAccT 2021 - Proceedings of the 2021
ACM Conference on Fairness, Accountability, and Transparency , pages 610{623.
Blackman, R. (2023). History may wonder why microsoft let its principles go for a creepy,
clingy bot. Retrieved from https://www.nytimes.com/2023/02/23/opinion/microsoft-bing-
ai-ethics.html. Accessed on March 1, 2023.
Bommarito, M. J. and Katz, D. M. (2022). Gpt takes the bar exam. SSRN Electronic Journal .
Accessed on February 1, 2023.
Bowman, E. (2022). A new AI chatbot might do your homework for you. but it's still not an
a+ student. KQED . Accessed on February 1, 2023.
Bowman, E. (2023). A college student made an app to detect AI-written text : Npr. NPR .
Accessed on January 29, 2023.
Brewster, J., Arvanitis, L., and Sadeghi, M. (2023). Misinformation monitor: January 2023.
Newsweek . Accessed on February 22, 2023.
Brewster, T. (2023). Armed with chatgpt, cybercriminals build malware and plot fake girl
bots. Forbes . Accessed on February 1, 2023.
Burrell, J. (2023). It's time to challenge the narrative about chatgpt and the future of jour-
nalism. Poynter . Accessed on March 4, 2023.
Cavoukian, A. (2009). Privacy by design, take the challenge . Canadian Electronic Library.
Check Point Research (2023). Cybercriminals starting to use chatgpt. Retrieved from
https://research.checkpoint.com/2023/opwnai-cybercriminals-starting-to-use-chatgpt/. Ac-
cessed on February 7, 2023.
Chen, B. X. (2023). A.i. bots can't report this column. but they can improve it. Re-
trieved from https://www.nytimes.com/2023/02/01/technology/personaltech/chatgpt-ai-
bots-editing.html. Accessed on March 21, 2023.
Christiano, P. F., Leike, J., Brown, T., Martic, M., Legg, S., and Amodei, D. (2017). Deep
reinforcement learning from human preferences. Advances in neural information processing
systems , 30.
Clark, J. (2023a). Import AI 315: Generative antibody design; RL's imagenet moment; RL
breaks rocket league. Import AI . Accessed on February 3, 2023.
Clark, M. (2023b). Chatgpt's creator made a free tool for detecting ai-generated text. The
Verge . Accessed on February 1, 2023.
D'Agostino, S. (2023). Chatgpt advice academics can use now. Inside Higher Ed .
Daza, M. T. and Ilozumba, U. J. (2022). A survey of AI ethics in business literature: Maps
and trends between 2000 and 2021. Frontiers in Psychology , 13:8040.
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2018). Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 .
Dixit, P. (2023). Meet the trio of artists suing AI image generators. Retrieved
from https://www.buzzfeednews.com/article/pranavdixit/ai-art-generators-lawsuit-stable-
diusion-midjourney. Accessed on February 3, 2023.
Drexel, B. and Withers, C. (2023). AI could be an authoritarian breakthrough in brainwashing.
Retrieved from https://bit.ly/3KrW7kY. Accessed on February 1, 2023.
Ferus, J. (2023). How i built a gpt-3 powered productivity app. Retrieved from
https://levelup.gitconnected.com/how-i-built-a-gpt-3-powered-productivity-system-
5d00ee5da225. Accessed on February 1, 2023.
Fitri, A. (2023). China to pass the world's most comprehensive law on deepfakes.
Retrieved from https://techmonitor.ai/technology/emerging-technology/china-is-about-to-
25pass-the-worlds-most-comprehensive-law-on-deepfakes. Accessed on February 1, 2023.
Fontrodona, J. and Sison, A. J. (2007). Hacia una teor a de la empresa basada en el bien
com un. Revista empresa y humanismo. 10 (2), p.65-92. DOI: 10.15581/015.10.33306.
Friedersdorf, C. (2023). Is this the start of an AI takeover? The Atlantic . Accessed on January
30, 2023.
Friedland, A. (2023). Meta's powerful language models leak online, the u.s. commerce depart-
ment opens up applications for chips funds, and a prc reorganization will impact r&d and
data. Center for Security and Emerging Technology . Accessed on March 22, 2023.
Garibay, O., Winslow, B., Andolina, S., Antona, M., Bodenschatz, A., Coursaris, C., Falco, G.,
Fiore, S. M., Garibay, I., Grieman, K., Havens, J. C., Jirotka, M., Kacorri, H., Karwowski,
W., Kider, J., Konstan, J., Koon, S., Lopez-Gonzalez, M., Maifeld-Carucci, I., McGregor,
S., Salvendy, G., Shneiderman, B., Stephanidis, C., Strobel, C., Holter, C. T., and Xu, W.
(2023). Six human-centered articial intelligence grand challenges. International Journal of
Human-Computer Interaction , 39:391{437.
Garrido-Merch an, E. C. and Hern andez-Lobato, D. (2020). Parallel predictive entropy search
for multi-objective bayesian optimization with constraints. arXiv preprint arXiv:2004.00601 .
Goldman, S. (2023). Why are getty and shutterstock on opposite sides of the ai legal debate?
VentureBeat . Accessed on February 1, 2023.
Goldstein, J. A., Sastry, G., Musser, M., DiResta, R., Gentzel, M., and Sedova, K. (2023).
Forecasting potential misuses of language models for disinformation campaigns and how to
reduce risk. Retrieved from https://openai.com/research/forecasting-misuse.
Goodin, D. (2023). Hackers are selling a service that bypasses chatgpt restrictions on malware.
Ars Technica .
Gorrell, B. (2023). The twitch creator busted for looking at AI porn of fellow streamers,
explained. Pirate Wires . Accessed on February 22, 2023.
Gozalo-Brizuela, R. and Garrido-Merchan, E. C. (2023). Chatgpt is not all you need. a state
of the art review of large generative AI models. arXiv preprint arXiv:2301.04655 .
Growcoot, M. (2023). Us copyright oce tells judge that AI artwork isn't protectable.
PetaPixel . Accessed on February 1, 2023.
Hinton, G. (2022). The forward-forward algorithm: Some preliminary investigations. arXiv
preprint arXiv:2212.13345 .
Howell, D. (2023). Stanford introduces detectGPT to help educators ght back against
chatGPT generated papers. Retrieved from https://www.neowin.net/news/stanford-
introduces-detectgpt-to-help-educators-ght-back-against-chatgpt-generated-papers/. Ac-
cessed on February 1, 2023.
Hsu, T. and Thompson, S. A. (2023). Disinformation researchers raise alarms about a.i.
chatbots. Retrieved from https://www.nytimes.com/2023/02/08/technology/ai-chatbots-
disinformation.html. Accessed on February 25, 2023.
Huang, H. (2023). The generative AI revolution has begun|how did we get here? Ars Technica .
Accessed on February 1, 2023.
Humble, N. and Mozelius, P. (2019). Articial intelligence in education -a promise, a threat
or a hype? European Conference on the Impact of Articial Intelligence and Robotics .
Khullar, D. (2023). Can a.i. treat mental illness? The New Yorker . Accessed on March 1,
2023.
Klein, E. (2023). A skeptical take on the a.i. revolution. Retrieved from
https://www.nytimes.com/2023/01/06/opinion/ezra-klein-podcast-gary-marcus.html. Ac-
cessed on February 1, 2023.
Konrad, A. and Cai, K. (2023a). Exclusive interview: OpenAI's sam altman talks chatgpt and
how articial general intelligence can `break capitalism'. Forbes . Accessed on March 18,
2023.
Konrad, A. and Cai, K. (2023b). Inside chatgpt's breakout moment and the race to put
ai to work. Retrieved from https://www.forbes.com/sites/alexkonrad/2023/02/02/inside-
chatggpts-breakout-moment-and-the-race-for-the-future-of-ai/. Accessed on March 21, 2023.
Kramer, A. D., Guillory, J. E., and Hancock, J. T. (2014). Experimental evidence of massive-
26scale emotional contagion through social networks. Proceedings of the National Academy of
Sciences of the United States of America , 111:8788{8790.
Kriebitz, A. and L utge, C. (2020). Articial intelligence and human rights: A business ethical
assessment. Business and Human Rights Journal , 5:84{104.
Kung, T. H., Cheatham, M., ChatGPT, Medenilla, A., Sillos, C., Leon, L. D., Elepa~ no, C.,
Madriaga, M., Aggabao, R., Diaz-Candido, G., Maningo, J., and Tseng, V. (2022). Perfor-
mance of chatgpt on usmle: Potential for ai-assisted medical education using large language
models. medRxiv , page 2022.12.19.22283643.
Kusner, M. J. and Loftus, J. R. (2020). The long road to fairer algorithms. Nature , 578:34{36.
Last Week in AI (2023). Last week in AI #205: How AI is going modular, growing legal
cases against generative AI, tools to detect AI-generated text, and more. Retrieved from
https://lastweekin.ai/p/205. Accessed on March 1, 2023.
LeCun, Y., Bengio, Y., and Hinton, G. (2015). Deep learning. nature , 521(7553):436{444.
Levenson, M. (2023). Science ction magazines battle a ood of chatbot-generated stories. Re-
trieved from https://www.nytimes.com/2023/02/23/technology/clarkesworld-submissions-
ai-sci-.html. Accessed on March 5, 2023.
Levy, S. (2021). Facebook failed the people who tried to improve it. Retrieved from
https://www.wired.com/story/facebook-papers-badge-posts-former-employees/. Accessed
on January 18, 2023.
Liang, J. (2023). The impact and future of chatgpt. Retrieved from
https://lastweekin.ai/p/chatgpt-impact. Accessed on February 1, 2023.
Lowrey, A. (2023). How chatgpt will destabilize white-collar work. Retrieved from
https://www.theatlantic.com/ideas/archive/2023/01/chatgpt-ai-economy-automation-
jobs/672767/. Accessed on February 1, 2023.
Marchese, D. (2022). An A.I. pioneer on what we should really fear. Retrieved from
https://www.nytimes.com/interactive/2022/12/26/magazine/yejin-choi-interview.html.
Accessed on February 7, 2023.
Marcus, G. (2023a). The CNET fake news asco, autopilot, and the uncanny cogni-
tive valley. Retrieved from https://garymarcus.substack.com/p/the-cnet-fake-news-asco-
autopilot. Accessed on March 1, 2023.
Marcus, G. (2023b). GPT-4's successes, and gpt-4's failures. Retrieved from
https://bit.ly/3M6O2mY. Accessed on March 19, 2023.
Marcus, G. (2023c). Inside the heart of chatgpt's darkness. Retrieved from
https://garymarcus.substack.com/p/inside-the-heart-of-chatgpts-darkness. Accessed on
February 25, 2023.
Marcus, G. (2023d). Oops! how google bombed, while doing pretty much ex-
actly the same thing as microsoft did, with similar results. Retrieved from
https://garymarcus.substack.com/p/oops-how-google-bombed-while-doing. Accessed on
February 1, 2023.
Marcus, G. (2023e). Scientists, please don't let your chatbots grow up to be co-authors. The
road to AI we can trust . Accessed on February 10, 2023.
Marcus, G. (2023f). Some things garymarcus might say. Retrieved from
https://garymarcus.substack.com/p/some-things-garymarcus-might-say. Accessed on
March 11, 2023.
Marcus, G. (2023g). What google should really be worried about. Retrieved from
https://garymarcus.substack.com/p/what-google-should-really-be-worried. Accessed on
February 25, 2023.
Mart n, L. A. and Garrido-Merch an, E. C. (2021). Many objective bayesian optimization.
arXiv preprint arXiv:2107.04126 .
McCabe, D. (2023). Supreme court poised to reconsider key tenets of online speech. Retrieved
from https://www.nytimes.com/2023/01/19/technology/supreme-court-online-free-speech-
social-media.html. Accessed on February 1, 2023.
McGinnis, J. O. (2023). What humanity adds. Law & Liberty . Accessed on February 1, 2023.
Merch an, E. C. G. and Lumbreras, S. (2022). On the independence between phenomenal
27consciousness and computational intelligence. arXiv preprint arXiv:2208.02187 .
Metz, C. (2023a). How smart are the robots getting? Retrieved from
https://www.nytimes.com/2023/01/20/technology/chatbots-turing-test.html. Accessed on
February 9, 2023.
Metz, C. (2023b). Why do A.I. chatbots tell lies and act weird? look in the mirror.
Retrieved from https://www.nytimes.com/2023/02/26/technology/ai-chatbot-information-
truth.html. Accessed on March 1, 2023.
Metz, C. and Collins, K. (2023). 10 ways gpt-4 is impressive but still awed. Retrieved
from https://www.nytimes.com/2023/03/14/technology/openai-new-gpt4.html. Accessed
on March 21, 2023.
Metz, C. and Grant, N. (2023). Google to release bard, its AI chatbot rival to chat-
gpt. Retrieved from https://www.nytimes.com/2023/02/06/technology/google-bard-ai-
chatbot.html. Accessed on March 1, 2023.
Metz, C. and Weise, K. (2023). Microsoft bets big on the creator of chatgpt in race to
dominate a.i. Retrieved from https://www.nytimes.com/2023/01/12/technology/microsoft-
openai-chatgpt.html. Accessed on March 21, 2023.
Mitchell, E., Lee, Y., Khazatskuy, A., Manning, C. D., and Finn, C. (2023). Detectgpt: Zero-
shot machine-generated text detection using probability curvature. Accessed on February
1, 2023.
Mitchell, M. (2023). Did chatgpt really pass graduate-level exams? AI: A guide for thinking
humans .
Mollick, E. (2023a). "do not fear ai, puny humans... that is not meant as a threat.". One
useful thing . Accessed on March 11, 2023.
Mollick, E. (2023b). The practical guide to using AI to do stu. One useful thing . Accessed
on January 29, 2023.
Mollick, E. and Mollick, L. (2023). Why all our classes suddenly became ai
classes. Retrieved from https://hbsp.harvard.edu/inspiring-minds/why-all-our-classes-
suddenly-became-ai-classes. Accessed on February 20, 2023.
Mollick, E. R. and Mollick, L. (2022). New modes of learning enabled by ai chatbots: Three
methods and assignments. SSRN Electronic Journal .
Morris, A. (2023). Deepfake challenges 'will only grow'. Northwestern Now . Accessed on
February 1, 2023.
Morrone, M. (2023). Should you use chatgpt to apply for jobs? here's what recruiters say. Fast
Company . Accessed on February 18, 2023.
Mucharraz y Cano, Y., Venuti, F., and Herrera Martinez, R. (2023). Chatgpt and AI text gener-
ators: Should academia adapt or resist? Retrieved from https://hbsp.harvard.edu/inspiring-
minds/chatgpt-and-ai-text-generators-should-academia-adapt-or-resist. Accessed on March
11, 2023.
Nature (2023). The AI writing on the wall. Nature Machine Intelligence 2023 5:1 , 5:1{1.
doi:10.1038/s42256-023-00613-9.
Nielsen, L. (2022). 10 things you can do with chatgpt as a machine learning engineer to make
your work more ecient. Medium . Accessed on March 1, 2023.
Nolan, B. (2023). Here are the schools and colleges that have banned chatgpt. Insider . Accessed
on February 7, 2023.
OpenAI (2022). Usage policies - openAI api (2022, november 9). Retrieved from
https://platform.openai.com/docs/usage-policies. Accessed on March 21, 2023.
OpenAI (2023a). How should AI systems behave, and who should decide? OpenAI Blog .
Accessed on March 1, 2023.
OpenAI (2023b). New AI classier for indicating ai-written text. Retrieved from
https://openai.com/blog/new-ai-classier-for-indicating-ai-written-text/. Accessed on
February 16, 2023.
Ott, S., Hebenstreit, K., Li evin, V., Hother, C. E., Moradi, M., Mayrhauser, M., Praas, R.,
Winther, O., and Samwald, M. (2023). Thoughtsource: A central hub for large language
model reasoning data. arXiv . https://arxiv.org/abs/2301.11596v2.
28Parker, S. (2017). Facebook exploits human vulnerability. https://www.youtube.com/watch?
v=R7jar4KgKxs&t=71s . Accessed on January 12, 2023.
Radford, A., Narasimhan, K., Salimans, T., Sutskever, I., et al. (2018). Improving language un-
derstanding by generative pre-training. Retrieved from https://bit.ly/3ZFdEdQ. Accessed
on February 1, 2023.
Rid, T. (2023). Five days in class with chatgpt. Retrieved from
https://alperovitch.sais.jhu.edu/ve-days-in-class-with-chatgpt/. Accessed on March
1, 2023.
Romero, A. (2022a). Chatgpt is the world's best chatbot. Retrieved from
https://thealgorithmicbridge.substack.com/p/chatgpt-is-the-worlds-best-chatbot. Accessed
on February 8, 2023.
Romero, A. (2022b). How to get the most out of chatGPT. Retrieved from
https://thealgorithmicbridge.substack.com/p/how-to-get-the-most-out-of-chatgpt. Ac-
cessed on January 18, 2023.
Romero, A. (2022c). OpenAI has the key to identify chatgpt's writing. The algorithmic bridge .
Accessed on February 7, 2023.
Romero, A. (2023a). Ai inuencers from the post-chatgpt era. Retrieved from
https://thealgorithmicbridge.substack.com/p/ai-inuencers-from-the-post-chatgpt. Ac-
cessed on January 30, 2023.
Romero, A. (2023b). Google vs microsoft (part 1): Microsoft's new bing
is a paradigm change for search and the browser. Retrieved from
https://thealgorithmicbridge.substack.com/p/google-vs-microsoft-microsofts-new. Ac-
cessed on February 1, 2023.
Romero, A. (2023c). Google vs microsoft (part 3): A new way of doing|and experiencing|AI.
Retrieved from https://thealgorithmicbridge.substack.com/p/google-vs-microsoft-part-3-a-
new. Accessed on March 1, 2023.
Romero, A. (2023d). Microsoft vs google: Will language models overtake search en-
gines? Retrieved from https://thealgorithmicbridge.substack.com/p/microsoft-vs-google-
will-language. Accessed on February 1, 2023.
Romero, A. (2023e). What you may have missed #16. Retrieved from
https://thealgorithmicbridge.substack.com/p/what-you-may-have-missed-16. Accessed on
February 12, 2023.
Romero, A. (2023f). What you may have missed #17. Retrieved from
https://thealgorithmicbridge.substack.com/p/what-you-may-have-missed-17. Accessed on
March 21, 2023.
Romero, A. (2023g). What you may have missed #18. Retrieved from
https://thealgorithmicbridge.substack.com/p/what-you-may-have-missed-18. Accessed on
February 1, 2023.
Romero, A. (2023h). What you may have missed #18. Retrieved from https://bit.ly/3ZSWaLf.
Accessed on March 6, 2023.
Romero, A. (2023i). What you may have missed #19. Retrieved from
https://thealgorithmicbridge.substack.com/p/what-you-may-have-missed-19. Accessed
on February 26, 2023.
Roose, K. (2023a). A conversation with bing's chatbot left me deeply unsettled.
Retrieved from https://www.nytimes.com/2023/02/16/technology/bing-chatbot-microsoft-
chatgpt.html. Accessed on February 19, 2023.
Roose, K. (2023b). Don't ban chatGPT in schools. teach with it. Retrieved from
https://www.nytimes.com/2023/01/12/technology/chatgpt-schools-teachers.html. Ac-
cessed on February 1, 2023.
Roose, K. (2023c). How chatgpt kicked o an a.i. arms race. Retrieved
from https://www.nytimes.com/2023/02/03/technology/chatgpt-openai-articial-
intelligence.html. Accessed on March 2, 2023.
Roose, K. (2023d). Microsoft's chatGPT-powered bing makes search interesting again.
Retrieved from https://www.nytimes.com/2023/02/08/technology/microsoft-bing-openai-
29articial-intelligence.html. Accessed on March 9, 2023.
Sanders, N. E. and Schneier, B. (2023). How chatgpt hijacks democracy. Retrieved from
https://www.nytimes.com/2023/01/15/opinion/ai-chatgpt-lobbying-democracy.html. Ac-
cessed on February 1, 2023.
Schreckinger, B. (2023a). The birth of a new crypto threat to government. Re-
trieved from https://www.politico.com/newsletters/digital-future-daily/2023/02/02/the-
birth-of-a-new-crypto-threat-to-government-00080976. Accessed on March 21, 2023.
Schreckinger, B. (2023b). My lawyer, the robot. Retrieved from
https://www.politico.com/newsletters/digital-future-daily/2023/01/09/my-lawyer-the-
robot-00077085. Accessed on February 8, 2023.
SessionGloomy (2023). New jailbreak! proudly unveiling the tried and tested dan 5.0 - it
actually works - returning to dan, and assessing its limitations and capabilities : Chatgpt.
Reddit post, Retrieved from https://bit.ly/40BubAA. Accessed on March 1, 2023.
Setty, R. (2023). Ai art generators hit with copyright suit over artists' images. Bloomberg Law .
Accessed on February 1, 2023.
Shneiderman, B. (2020a). Design lessons from AI's two grand goals: Human emulation and
useful applications. IEEE Transactions on Technology and Society , 1:73{82.
Shneiderman, B. (2020b). Human-centered articial intelligence: Reliable, safe & trustworthy.
International Journal of Human-Computer Interaction , 36:495{504.
Shneiderman, B. (2020c). Human-centered articial intelligence: Three fresh ideas. AIS Trans-
actions on Human-Computer Interaction , 12:109{124.
Shneiderman, B. (2021). Responsible AI: bridging from ethics to practice. Communications
of the ACM , 64:32{35.
Shneiderman, B. (2022a). Human Centered AI . Oxford University Press.
doi:10.1093/os/9780192845290.001.0001.
Shneiderman, B. (2022b). Ten old beliefs and new ideas: Steps toward human-centered AI. Re-
trieved from https://medium.com/hcil-at-umd/ten-old-beliefs-and-new-ideas-steps-toward-
human-centered-ai-3f110467c7f1. Accessed on February 1, 2023.
Singer, N. (2023). At this school, computer science class now includes critiquing chat-
bots. Retrieved from https://www.nytimes.com/2023/02/06/technology/chatgpt-schools-
teachers-ai-ethics.html. Accessed on March 12, 2023.
Sison, A. J. G. and Red n, D. M. (2021). A neo-aristotelian perspective on the need for articial
moral agents (amas). AI & SOCIETY . https://doi.org/10.1007/s00146-021-01283-0.
Solove, D. J. (2002). Conceptualizing privacy. California Law Review , 90:1087{1156.
Sowa, K., Przegalinska, A., and Ciechanowski, L. (2021). Cobots in knowledge work: Human
{ ai collaboration in managerial professions. Journal of Business Research , 125:135{142.
Stokel-Walker, C. (2023). Chatgpt listed as author on research papers: many scientists disap-
prove. Nature , 613:620{621.
Terwiesch, C. (2023). Would chat gpt get a wharton mba? a prediction based
on its performance in the operations management course. Retrieved from
https://mackinstitute.wharton.upenn.edu/wp-content/uploads/2023/01/Christian-
Terwiesch-Chat-GTP-1.24.pdf. Accessed on March 12, 2023.
Terwiesch, C., Mollick, E., and Basiouny, A. (2023). Chatgpt passed an mba exam. what's
next? Retrieved from https://knowledge.wharton.upenn.edu/podcast/wharton-business-
daily-podcast/chatgpt-passed-an-mba-exam-whats-next/. Accessed on February 2, 2023.
The Economist (2023a). The race of the AI labs heats up. Retrieved from
https://www.economist.com/business/2023/01/30/the-race-of-the-ai-labs-heats-up. Ac-
cessed on March 2, 2023.
The Economist (2023b). The relationship between AI and humans. Retrieved from
https://econ.st/3Uag03i. Accessed on March 21, 2023.
The PyCoach (2022). Chatgpt: 4 jobs that will change (or be fully replaced) by this ai-powered
chatbot. Medium . Accessed on March 1, 2023.
Thompson, D. (2022). Your creativity won't save your job from AI. Retrieved
from https://www.theatlantic.com/newsletters/archive/2022/12/why-the-rise-of-ai-is-the-
30most-important-story-of-the-year/672308/. Accessed on January 12, 2023.
Tiany, K. (2023). The supreme court considers the algorithm. Retrieved from
https://www.theatlantic.com/technology/archive/2023/02/supreme-court-section-230-
twitter-google-algorithm/672915/. Accessed on March 1, 2023.
Tiku, N. (2022). Google engineer blake lemoine thinks its lamda AI has come to
life. Retrieved from https://www.washingtonpost.com/technology/2022/06/11/google-ai-
lamda-blake-lemoine/. Accessed on February 1, 2023.
Van Der Linden, S. (2023). Foolproof: A psychological vaccine against fake news. Retrieved
from https://www.cam.ac.uk/stories/foolproof. Accessed on February 18, 2023.
Van Inwegen, E., Munyikwa, Z. T., and Horton, J. J. (2023). Algorithmic writing assis-
tance on jobseekers' resumes increases hires. National Bureau of Economic Research . DOI
10.3386/w30886.
Vincent, J. (2023a). Getty images is suing the creators of ai art tool stable diusion for
scraping its content. Retrieved from https://www.theverge.com/2023/1/17/23558516/ai-
art-copyright-stable-diusion-getty-images-lawsuit. Accessed on February 1, 2023.
Vincent, J. (2023b). Hustle bros are jumping on the ai bandwagon. The Verge .
Volpicelli, G. (2023). Chatgpt broke the eu plan to regulate AI. Politico.
Retrieved from https://www.politico.eu/article/eu-plan-regulate-chatgpt-openai-articial-
intelligence-act/. Accessed on March 22, 2023.
Warzel, C. (2023a). Talking to AI might be the most important skill of this cen-
tury. Retrieved from https://www.theatlantic.com/technology/archive/2023/02/openai-
text-models-google-search-engine-bard-chatbot-chatgpt-prompt-writing/672991/. Accessed
on March 21, 2023.
Warzel, C. (2023b). Welcome to the era of AI vertigo. Retrieved from
https://www.theatlantic.com/technology/archive/2023/02/google-bing-race-to-launch-
ai-chatbot-powered-search-engines/673006/. Accessed on February 26, 2023.
Weise, K. and Metz, C. (2023). Microsoft considers more limits for its new a.i. chatbot.
Retrieved from https://www.nytimes.com/2023/02/16/technology/microsoft-bing-chatbot-
limits.html. Accessed on February 26, 2023.
Weizenbaum, J. (1966). Eliza a computer program for the study of natural language commu-
nication between man and machine. Communications of the ACM , 9:36{45.
Wiggers, K. (2023). The current legal cases against generative AI are just the begin-
ning. Retrieved from https://techcrunch.com/2023/01/27/the-current-legal-cases-against-
generative-ai-are-just-the-beginning/?guccounter=1. Accessed on February 7, 2023.
Williams, N. (2023). Chatgpt and dening humanity in the age of brains in vats. Church Life
Journal | University of Notre Dame . Accessed on March 21, 2023.
Wong, M. (2023). Gpt-4 is here. just how powerful is it? Retrieved from
https://www.theatlantic.com/technology/archive/2023/03/gpt4-release-rumors-hype-
future-iterations/673396/. Accessed on March 19, 2023.
31