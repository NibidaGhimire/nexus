Transferable Learned Image Compression-Resistant
Adversarial Perturbations
Yang Sui1, Zhuohang Li2, Ding Ding3, Xiang Pan3,
Xiaozhong Xu3, Shan Liu3, Zhenzhong Chen4
1Rutgers University2Vanderbilt University3Tencent America4Wuhan University
yang.sui@rutgers.edu
Abstract
Adversarial attacks can readily disrupt the image classification system, revealing the vul-
nerability of DNN-based recognition tasks. While existing adversarial perturbations are
primarily applied to uncompressed images or compressed images by the traditional image
compression method, i.e., JPEG, limited studies have investigated the robustness of models
for image classification in the context of DNN-based image compression. With the rapid evo-
lution of advanced image compression, DNN-based learned image compression has emerged
as the promising approach for transmitting images in many security-critical applications,
such as cloud-based face recognition and autonomous driving, due to its superior perfor-
mance over traditional compression. Therefore, there is a pressing need to fully investigate
the robustness of a classification system post-processed by learned image compression. To
bridge this research gap, we explore the adversarial attack on a new pipeline that tar-
gets image classification models that utilize learned image compressors as pre-processing
modules. Furthermore, to enhance the transferability of perturbations across various qual-
ity levels and architectures of learned image compression models, we introduce a saliency
score-based sampling method to enable the fast generation of transferable perturbation.
Extensive experiments with popular attack methods demonstrate the enhanced transfer-
ability of our proposed method when attacking images that have been post-processed with
different learned image compression models.
1 Introduction
Deep Neural Network(DNN)-based models are known to be vulnerable against ad-
versarial examples [1], which are carefully perturbed images that are unsuspicious to
human eyes but can cause deep learning models to produce incorrect or malicious
predictions. Although this phenomenon was initially discovered on image classifica-
tion models [1], research on adversarial examples has since then quickly spread to
many critical domains in computer vision, such as facial recognition [2], and object
detection [3]. Initial studies [1, 3] typically assume a white-box access to the target
model. Later, black-box attacks [4] are also developed where the details of the target
model are unknown to the adversary.
Existing works predominantly study adversarial examples in the context of uncom-
pressed images. However, when deployed to real-world systems, including cloud-based
The work of Yang Sui was done during the internship at Tencent America.
The work of Zhuohang Li was done during the visit at Tencent America.
The work of Zhenzhong Chen was done during the visit at Tencent.arXiv:2401.03115v2  [cs.CV]  6 Nov 2024image analysis services and edge image recognition systems (e.g., object recognition
for autonomous driving and face recognition for security services), in order to save
communication bandwidth or computation cost, typically images are first compressed
using some compression algorithms before being fed into the classification system to
get predictions. In light of this scenario, a branch of studies [5‚Äì7] has investigated
JPEG-compression-resistant adversarial attacks.
Recently, Learned Image Compression (LIC) has rapidly become the primary
method for transmitting images under bit-rate constraints due to its superior perfor-
mance. Specifically, LIC frameworks [8‚Äì10] have recently evolved, showing substantial
rate-distortion performance improvement over standard image compression [11] such
as JPEG[12], JPEG2000[13], and BPG[14], owing to the remarkable representation
ability of DNNs. The fundamental structure of LIC is the auto-encoder framework
with entropy minimization constraints, which employs the DNN-based encoder and
decoder for image compression and reconstruction.
LIC has been quickly adopted in many application scenarios as a pre-processing
module for various image classification tasks and has also been proposed in ISO/IEC
JTC 1/SC29/WG1 M93073 in the 93rd JPEG-AI Meeting. Despite its wide deploy-
ment, the robustness of the LIC-powered image recognition pipeline remains under-
explored. To fill this research gap, in this paper, we aim to investigate the robustness
ofLearned Image Compression Classification System (LICCS) by launch-
ing specialized adversarial attacks that are optimized for this pipeline. Further, in
the LICCS, to-be-classified compressed images have unknown compression quality
levels determined by LIC. Therefore, exploring the robustness across all quality levels
presents a unique challenge. Overall, the goal of our investigation is to answer the
following research questions:
‚Ä¢How robust are the LICCSs against adversarial perturbations?
‚Ä¢How transferable are adversarial perturbations across various quality levels
under the same/different LIC model architectures?
To address these questions, we conduct a series of empirical investigations on a
typical LICCS pipeline, which is illustrated in Fig. 1. To evaluate the robustness of
the LICCS, we first consider a white-box scenario where the attacker has the ability
to access the details of models. Next, to evaluate how the attack can be generalized in
the black-box scenario, we measure the transferability of perturbations across various
unknown quality levels under the same LIC model architecture. We find through our
experiments that the LICCS framework is naturally, to some extent, resilient to trans-
ferable attacks, as the attack performance shows significant disparities across different
quality levels. To improve the transferability across the quality levels, we propose a
saliency score-based sampling method that performs ensemble attacks on the most
influential quality levels of LIC models, which show the highest adversarial impact on
the LICCS across all quality levels. Specifically, we measure the collective coverage
of affected curves of all combinations of surrogate models to calculate the saliency
score. Based on these scores, we select the highest influential combination consisting
of top- Kquality levels, which are further utilized to launch attacks to generalize to all
quality levels. Our contributions can be summarized as follows: (1) We investigate anHWImage
WHReconstructedImageDecoderEncoder!ùíöùíöAEAD!ùíöQBit streamHyperDecoderContext ModelHyper priorHyperEncoder
ImageImage
Cat
ReconstructedImage
Classification ModelClassification Model
CatCorrect
CorrectAdversarial PerturbationAdversarial Example
ReconstructedAdversarial Example
Classification ModelClassification Model
CatDogCorrect
WrongImage
Figure 1: Our proposed adversarial attack pipeline against the LICCS. Left: The framework
of the LIC (described in Section 2.1); Middle: LICCS pipeline. Both the original image
and the reconstructed image are classified with the correct label ‚ÄúCat‚Äù; Right: After the
adversarial attacks, although the adversarial examples are classified with the correct label,
its reconstructed image through the LIC is misrecognized as the wrong label ‚ÄúDog‚Äù.
adversarial attack pipeline for the LICCS, utilizing LICs as pre-processing modules
for the image classification model. To the best of our knowledge, we are the first work
to investigate the robustness of LICCS. (2) To measure the robustness of LICCS and
the transferability of its adversarial perturbations, we conduct a series of experiments
in white-box and black-box scenarios. Based on the black-box results, we observed
that the neighboring quality levels are more significantly affected when attacking a
certain quality level. (3) To improve the transferability of the attack across different
quality levels and architectures, we propose a saliency score-based sampling method
that enables generating transferable perturbations with limited model access. (4) To
improve the transferability of the attack across different quality levels and architec-
tures, we propose a saliency score-based sampling method that enables generating
transferable perturbations with limited model access.
2 Transferable Attack against LICCS
2.1 Learned Image Compression
As illustrated in Fig. 1 (left), the input image is converted into a latent representation
via a non-linear transformation. The latents are then encoded by an arithmetic en-
coder to produce the bit stream for storage. To reconstruct the image, the bit stream
is decoded by an arithmetic decoder and fed into the main DNN-based decoder to
generate a reconstructed image. Specifically, suppose ga(¬∑),gs(¬∑) are the non-linear
transforms. Let XandÀÜXdenote the original input and reconstructed images, respec-
tively. Let YandÀÜYdenote the pre-quantized and quantized latent representation,
respectively, then the deep learning-based image compression can be described as:
Y=ga(X),
ÀÜY= AD(AE( Q(Y))),
ÀÜX=gs(ÀÜY),(1)
where Q(¬∑) is the quantization process. AE and AD denote the arithmetic encod-
ing and decoding processes, respectively. ÀÜXrepresents the reconstructed image. Toreduce the bit rate, a hyper-prior is used as side information to estimate the mean
and scale parameters of latents predicted from the entropy model, including a hyper
encoder and hyper decoder. Furthermore, an auto-regressive context model is inte-
grated into the hyper-prior framework to boost the R-D performance. Since we aim
to explore the LICCS, which prioritizes the reconstructed image over the bit-rate.
Therefore, we omit the hyper-prior and context model for simplicity.
2.2 LICCS Attack
Fig. 1 (right) illustrates the attack pipeline for generating adversarial examples in
conjunction with or without the LIC. Let X,ydenote the image and ground-truth
label. Given a classification model f(¬∑)ipredicts the probability of the image be-
longing to class i, adversarial attacks aim to generate an adversarial perturbation Œ¥
embedded on Xso that the new adversarial image can misclassify the classification
model f(¬∑)i. It can be formulated as:
arg max
if(X+Œ¥)iÃ∏=y,s.t.‚à•Œ¥‚à•p‚â§œµ, (2)
where œµdenotes the perturbation budget to ensure the induced distortion is imper-
ceptible.
Unlike prior works that focus on the adversarial attack toward reconstruction im-
age quality of LIC [15, 16], in this paper, we primarily investigate the robustness of a
LIC-based classification system. To perform an adversarial attack on an image within
the LICCS as illustrated in Fig. 1 (right), the goal is to introduce the adversarial
perturbation Œ¥to the source image Xthat causes the reconstructed adversarial ex-
amples gs(Q(ga(X+Œ¥))) to be misclassified by the classification model. Then, Eq. 2
is extended as follows:
arg max
if(gs(Q(ga(X+Œ¥))))iÃ∏=y,s.t.‚à•Œ¥‚à•p‚â§œµ. (3)
2.3 White-box Attacks Evaluation
To evaluate the robustness of the LICCS, we first perform the white-box attacks
by solving Eq. 3. The detailed results of Top-1 accuracy of LICCS ( cheng2020
[9] LIC model and ResNet-20 classification model) attacked by the PGD attack [17]
on the CIFAR-10 dataset is shown in Table 4 of Section 3. Given a pre-trained
classification model with 91.25 % top-1 accuracy on the CIFAR-10 dataset in LICCS,
when generating adversarial examples with œµ= 16, the accuracy of LICCS decreases to
18.75% and 5.53% in quality levels 1 and 6, demonstrating considerable vulnerability
to attacks. Hence, we conclude the LICCS is vulnerable to white-box attacks ,
mainly contributing to the inherent DNN-based classification model.
2.4 Transferability from Black-box Attacks
Next, we explore the robustness of the LICCS with black-box attacks and its trans-
ferability. The concept of ‚Äúquality levels‚Äù of LIC distinguishes LICCS from theTable 1: Top-1 accuracy of PGD black-
box attack results of cheng2020 [9]
model. Each row/column corresponds to
a surrogate/target model with a given
quality level.
Quality 1 2 3 4 5 6
œµ= 4, Œ±= 1, iters = 10
1 35.59% 57.54% 68.61% 79.25% 84.50% 87.19%
2 43.71% 37.86% 58.07% 76.14% 83.01% 85.65%
3 45.85% 46.15% 35.82% 69.49% 79.02% 83.00%
4 48.72% 56.92% 58.16% 37.28% 53.77% 66.30%
5 49.22% 59.32% 62.38% 47.77% 34.83% 41.01%
6 49.55% 60.20% 65.07% 57.97% 39.10% 32.53%
1 2 3 4 5 6
Target Model Quality Level20406080100ASR (\%)
Surrogate Model w/ Quality 1
Surrogate Model w/ Quality 2
Surrogate Model w/ Quality 3
Surrogate Model w/ Quality 4
Surrogate Model w/ Quality 5
Surrogate Model w/ Quality 6Figure 2: ASR of PGD black-box attack re-
sults on quality level 1 to 6 of cheng2020 [9]
model with œµ= 16, Œ±= 2,iters = 20.
traditional classification model, introducing unknown images to attackers and in-
creasing the challenge of black-box attacks. Consequently, we aim to explore this
unique feature by launching black-box attacks across different quality levels to eval-
uate the transferability between them. Table 1 demonstrates the top-1 accuracy of
LICCS after the PGD attack [17] on cheng2020 [9] and ResNet-20 on the CIFAR-10
dataset with different quality levels. The number at the beginning of the row and col-
umn denotes the quality level of the surrogate model and target model, respectively.
Here, the surrogate model refers to a well-trained LIC model whose quality levels,
parameters, and architecture are known to the attacker. In contrast, such informa-
tion about the target models remains unknown. The experimental setup details are
illustrated in Section 3.
As demonstrated in Table 1, the attack fails to achieve effective transfer-
ability across different quality levels. More detailed results are shown in Fig. 4.
For instance, the top-1 accuracy of the target model with the quality level 6 remains
robust at 87.19%, indicating a highly restricted transferability from quality level 1
to 6. Such a significant discrepancy concludes that it is challenging for naive
black-box attack to defeat LICCS without improving transferability . Con-
sequently, as a LICCS attacker, we strive to design an attack strategy with improved
transferability.
2.5 Observation
As pointed out by several studies a natural way of improving the transferability of
adversarial examples is by attacking an ensemble of models, which diversifies the
surrogate models and helps to capture better the intrinsic adversarial vulnerability
of the target model [18, 19]. A range of LIC models with varying quality levels could
potentially be utilized as components of ensemble models. However, in LIC, the
quality level is controlled through a hyper-parameter which can take arbitrary real
values; yet it is infeasible to incorporate an infinite number of such models to cover
all possible values due to memory and computational constraints.
To analyze the influence of the adversarial examples on each quality level, weconduct the black-box attack shown in Fig. 2. Findings from the figure and Table 1
reveal that ASR(i, q)‚àù1/Distance(i, q) , where ASR(i, q) = 100% ‚àíAcc(i, q) denotes
the ASR (Attack Success Rate) when attacking the surrogate quality level iand
evaluate on target quality level q. In other words, when a specific quality level is
attacked, the adjacent quality levels tend to experience more substantial
impact, whereas the impact diminishes for quality levels that are more
distant .
The observations indicate that each quality level possesses its unique sphere of
influence. This implies that even if selecting the most influential model, attacking a
single quality level can only affect its near vicinity and has less effect on others that
are further away. Hence, it becomes crucial to identify the optimal combination of
multiple quality levels rather than repeatedly attacking the individuals with the most
influence.
2.6 Saliency Score-based Sampling
Building upon the observations outlined in Section 2.5, we propose a method to
improve the transferability of attacks across all quality levels. We first sample Œª1,2,¬∑¬∑¬∑,N
(the coefficient to control the rate-distortion trade-off) and train corresponding N
surrogate models with diverse quality levels differently. To ensure the exact black-
box attack process, the Œªof training surrogate models are independent of those used
in target models. Subsequently, we calculate the black-box ASR of the surrogate
models and derive the ASR(q1, q2), q1, q2‚àà[1, N], to analyze the influence of each pair
of quality level. Since the potential target models in real-world scenarios have an
infinite range of quality levels, we strive to accurately depict the influence at each
potential real number of quality levels. To achieve this, we convert the discrete ASR
points into a continuous polynomial ASR curve with a polynomial fitting function
polyn (¬∑). This representation captures the ASR values across a continuous spectrum
of quality levels rather than only at Ndiscrete quality levels.
To identify the optimal combination of quality levels, we introduce the coverage
function, defined as max( A, B), which quantifies the collective coverage of curve A
andB. Then, we employ an accumulated integral function to calculate the saliency
score for each combination of Kquality levels. The saliency score represents the
degree to which the combination maximizes the coverage area, thereby representing
the potential influence on ASR across all quality levels. Given the desired number of
ensemble models K, where K < N , the formulation of the saliency score is as follows:
S(q) =Zxmax
xminmax( polyn (q0),polyn (q1),¬∑¬∑¬∑,polyn (qK))dx (4)
where q= [q0, q1,¬∑¬∑¬∑, qK] represents the combination of Kquality levels. xminand
xmaxare the lowest and highest quality level of surrogate models.
After calculation, we select the largest Samong total N
K
values, corresponding
to the combination of Kquality levels that cause the most substantial impact across
the entire range of quality levels within the surrogate models.arg max
iq‚àó
KX
q=q‚àó
0f(gsq(Q(gaq(X+Œ¥))))iÃ∏=y,s.t.‚à•Œ¥‚à•p‚â§œµ, (5)
where q‚àóis from the optimal combination of quality levels corresponding to the largest
S. A smaller value of Kindicates that we use fewer surrogate models to generate the
transferable adversarial perturbations, thereby improving efficiency.
3 Experiments
Setting. We adopt PGD [17] and FGSM [20] attack methods with cheng2020 [9]
andhyper [8] LIC models to evaluate the effectiveness of transferability across quality
levels and different architectures. In this paper, we use the differential approximation
quantization from [5] to execute the gradient-based attack. We fix the classification
model as ResNet-20. The evaluations are conducted on the CIFAR-10 dataset.
Metric. The attack performance is evaluated based on the top-1 accuracy of
the victim LICCS. The lower accuracy of the victim model indicates better attack
performance. We also measure the adversarial perturbations generation time (average
of 100 times) per image.
Hyperparameter. For the white-box attack in Table 4, we set perturbation
budget œµ‚àà {1,2,4,8,16}, learning step Œ±‚àà {1,2}, and iterations T‚àà {10,20}.
For the black-box attack in Table 1, experiments are conducted with œµ‚àà {4,8},
Œ±‚àà {1,2},T‚àà {10,20}. For the Table 2, 3, and 1, experiments are conducted with
œµ‚àà {4,8,16},Œ±‚àà {1,2}. All œµandŒ±are divided by 255 to match the normalized
image. The coefficients of the rate-distortion trade-off of surrogate models, Œª, are
randomly sampled to control the quality levels independent of those in target models.
Surrogate models with Œªare fully trained following the setting of [21].
Table 2: Top-1 accuracy of LICCS with the surrogate model cheng2020 and target model
cheng2020 attacked by PGD. Lower accuracy demonstrates higher transferability.
Quality 1 2 3 4 5 6 Average Time
œµ= 4, Œ±= 1, iters = 10
R-En 44.32% 51.10% 54.95% 56.16% 55.10% 58.02% 53.28% 1.1s
Ours 47.86% 54.46% 51.41% 47.36% 39.26% 40.53% 46.81% 1.1s
œµ= 8, Œ±= 2, iters = 10
R-En 39.83% 43.34% 45.42% 43.20% 43.23% 46.59% 43.60% 1.1s
Ours 45.52% 47.25% 42.14% 31.26% 25.01% 25.33% 36.09 % 1.1s
œµ= 16, Œ±= 2, iters = 10
R-En 35.68% 37.15% 37.87% 35.76% 37.86% 40.92% 37.54% 1.1s
Ours 43.31% 41.18% 34.46% 22.64% 20.24% 20.39% 30.37 % 1.1s
Transferablity across quality levels. We initially evaluate the performance
of our method by applying the PGD on the surrogate model cheng2020 to generate
adversarial perturbations. To evaluate the transferability across quality levels, we
utilize these perturbations to disrupt a target model of unknown quality levels. The
baseline method, termed ‚ÄúR-En‚Äù, involves randomly selecting Ksurrogate models to
conduct the ensemble attacks. To ensure consistency, we set K= 2 for both R-EnTable 3: Top-1 accuracy of LICCS with the surrogate model hyper and target model hyper
attacked by FGSM. Lower accuracy demonstrates higher transferability.
Quality 1 2 3 4 5 6 7 8 Average Time
œµ= 4, iters = 1
R-En 42.97% 56.03% 62.81% 67.65% 71.92% 77.36% 80.69% 83.10% 67.82% 0.15s
Ours 44.69% 56.79% 61.32% 59.82% 61.75% 62.21% 67.06% 69.58% 60.28% 0.15s
œµ= 8, iters = 1
R-En 43.44% 53.26% 54.35% 56.24% 62.37% 70.22% 72.43% 72.44% 59.57% 0.15s
Ours 41.93% 47.81% 47.24% 47.89% 52.42% 55.49% 57.37% 57.38% 50.94% 0.15s
and our proposed method. To mitigate the variance arising from the randomness, we
calculate the average of 20 experiment results for the R-En method. As demonstrated
in Table 2, compared to R-En, our method improves the average ASR by 6.46%,
7.51%, 7.17% with œµ= 4, œµ= 8, œµ= 16, while maintaining the same perturbation
generation time. We further measure our approach using the FGSM attack on the
hyper model to assess the improved transferability. According to Table 3, compared
to R-En, our method enhances the average ASR by 7.54% and 8.63% with œµ= 4 and
œµ= 8, respectively.
Transferablity across different architectures. To evaluate the transferabil-
ity across various architectures, we apply adversarial perturbations generated from
cheng2020 to disrupt an unseen target model hyper . As illustrated in Fig. 3, com-
pared to R-En, our method can improve the average ASR by 7.57%, 6.53%, and 8.23%
with œµ= 4,œµ= 8, and œµ= 16, respectively.
1 2 3 4 5 6 7 8
Quality4550556065707580Accuracy (%)
42.85%54.27%
55.75%61.02%
59.71%1.31%64.11%
55.5%8.61%66.56%
54.07%12.49%71.28%
55.54%15.74%76.44%
62.4%14.04%
69.41%10.91%R-En
Ours
R-En Avg: 64.61%
Ours Avg: 57.05%
1 2 3 4 5 6 7 8
Quality40455055606570Accuracy (%)
40.34%48.22%
46.36%51.26%
8.49%50.99%
9.52%52.15%
11.21%57.8%
47.14%10.66%64.67%
57.59%7.08%
65.85%4.18%R-En
Ours
R-En Avg: 54.43%
Ours Avg: 47.90%
1 2 3 4 5 6 7 8
Quality30354045505560Accuracy (%)
37.56%
38.68%42.08%
39.23%42.36%
10.06%41.09%
10.05%43.09%
11.95%49.39%
35.47%13.92%56.52%
45.94%10.58%
54.68%7.56%R-En
Ours
R-En Avg: 46.79%
Ours Avg: 38.56%
Figure 3: Top-1 accuracy of LICCS with the surrogate model cheng2020 and target model
hyper attacked by PGD. Lower accuracy demonstrates higher transferability.
Detailed results of white-box attack. We provide detailed results for the
white-box attack. As depicted in Table 4, we initially conducted the experiments
on LICCS for quality levels ranging from 1 to 6. After attacking, LICCS can only
achieve less than 50% top-1 accuracy in most cases. Further, we also perform experi-
ments to evaluate the performance when employing these adversarial examples on the
classification model without the LIC component. Interestingly, omitting the LIC
module seems to provide some defense against these adversarial examples.
For instance, when an attack is launched on LICCS with œµ= 16, T= 20, the LICCSTable 4: Top-1 accuracy of LICCS after PGD white-box attack on quality level 1 to 6 of
cheng2020 [9] model. ‚Äúw/o LIC‚Äù and ‚Äúw/ LIC‚Äù mean the generated adversarial examples
are fed into the ResNet-20 without LIC module and standard LICCS (ResNet-20 with LIC
module), respectively.
Quality
Levelœµ= 1 œµ= 2 œµ= 4 œµ= 8 œµ= 8 œµ= 16 œµ= 16
Œ±= 1 Œ±= 1 Œ±= 1 Œ±= 2 Œ±= 2 Œ±= 2 Œ±= 2
T= 10 T= 10 T= 10 T= 10 T= 20 T= 10 T= 20
w/o LIC
1 92.45% 92.26% 91.75% 90.22% 89.58% 87.20% 84.9%
2 92.29% 92.02% 91.22% 89.01% 88.66% 85.81% 83.5%
3 91.93% 91.57% 90.34% 87.18% 87.17% 83.22% 80.17%
4 91.35% 90.36% 87.73% 80.09% 80.57% 73.96% 68.80%
5 90.43% 88.23% 83.16% 72.11% 71.27% 63.13% 55.24%
6 89.41% 86.05% 78.01% 63.96% 62.80% 54.57% 46.42%
w/ LIC
1 45.17% 41.19% 35.59% 28.45% 26.69% 22.7% 18.75%
2 53.05% 45.6% 37.86% 28.72% 25.75% 23.12% 17.07%
3 54.33% 44.47% 35.82% 25.35% 20.92% 20.41% 13.73%
4 57.24% 45.76% 37.28% 25.89% 19.31% 21.75% 12.20%
5 54.17% 42.24% 34.83% 23.06% 14.17% 19.46% 8.16%
6 50.51% 39.25% 32.53% 19.08% 9.95% 16.12% 5.53%
1 2 3 4 5 6
Quality of T arget Model1 2 3 4 5 6Quality of Surrogate Model35.59 57.54 68.61 79.25 84.50 87.19
43.71 37.86 58.07 76.14 83.01 85.65
45.85 46.15 35.82 69.49 79.02 83.00
48.72 56.92 58.16 37.28 53.77 66.30
49.22 59.32 62.38 47.77 34.83 41.01
49.55 60.20 65.07 57.97 39.10 32.53=4, =1, iters=10
1 2 3 4 5 6
Quality of T arget Model1 2 3 4 5 628.45 51.07 65.16 78.16 83.87 86.21
35.45 28.72 44.33 72.11 80.19 83.64
40.65 32.05 25.35 58.75 71.67 77.76
46.18 50.46 45.26 25.89 33.64 45.95
47.75 55.44 54.28 28.03 23.06 24.83
48.21 57.94 59.97 37.95 21.46 19.08=8, =2, iters=10
1 2 3 4 5 6
Quality of T arget Model1 2 3 4 5 626.69 50.56 64.40 78.25 83.99 86.07
34.37 25.75 42.27 72.63 80.32 83.48
40.17 29.53 20.92 58.20 71.88 78.18
45.75 50.21 44.55 19.31 27.90 42.21
47.11 55.39 54.64 22.37 14.17 16.02
47.86 57.43 60.38 37.64 12.71 9.95=8, =2, iters=20
20406080
T op-1 Accuracy (%)
Figure 4: Top-1 accuracy of LICCS after PGD black-box attack of cheng2020 [9] model.
Each row/column corresponds to a surrogate/target model with a given quality level.
can only achieve 5.53% top-1 accuracy for quality level 6. In contrast, without the
LIC module, 46 .42% of these adversarial examples can be classified correctly.
Detailed results of black-box attack. We here provide detailed black-box at-
tacks across different quality levels to evaluate the transferability across quality levels
in Fig. 4 as the complementary of Table 1. Each grid value represents the top-1 accu-
racy of the target model post-attack, using a surrogate model in a black-box setting.
The experiments, as seen in Fig. 4 and conducted with diverse parameters of œµ, Œ±, T ,
indicate varying performance when there‚Äôs a mismatch or slight similarity between
the surrogate and target quality levels. This underscores the limited transferability
of traditional black-box attacks on LICCS across different quality levels without our
method.4 Conclusion
In this paper, we introduce an adversarial attack pipeline specifically designed for
the LICCS and conduct a range of experiments in both white-box and black-box
settings. Based on findings from the black-box experiments, we propose a saliency
score-based sampling approach that allows for generating transferable perturbations
even with limited model access and enhances transferability. We conducted further
experiments with our methods on PGD and FGSM methods on various LIC models
with multiple quality levels. The results demonstrate that our proposed method
effectively enhances the transferability of adversarial perturbations across different
quality levels and architectures.
References
[1] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian
Goodfellow, and Rob Fergus, ‚ÄúIntriguing properties of neural networks,‚Äù in Proceedings
of the International Conference on Learning Representations , 2014.
[2] Yinpeng Dong, Hang Su, Baoyuan Wu, Zhifeng Li, Wei Liu, Tong Zhang, and Jun Zhu,
‚ÄúEfficient decision-based black-box adversarial attacks on face recognition,‚Äù in Pro-
ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition ,
2019, pp. 7714‚Äì7722.
[3] Cihang Xie, Jianyu Wang, Zhishuai Zhang, Yuyin Zhou, Lingxi Xie, and Alan Yuille,
‚ÄúAdversarial examples for semantic segmentation and object detection,‚Äù in Proceedings
of the IEEE international conference on computer vision , 2017, pp. 1369‚Äì1378.
[4] Andrew Ilyas, Logan Engstrom, Anish Athalye, and Jessy Lin, ‚ÄúBlack-box adversarial
attacks with limited queries and information,‚Äù in International conference on machine
learning . PMLR, 2018, pp. 2137‚Äì2146.
[5] Richard Shin and Dawn Song, ‚ÄúJpeg-resistant adversarial images,‚Äù in NIPS 2017
Workshop on Machine Learning and Computer Security , 2017, vol. 1, p. 8.
[6] Zhibo Wang, Hengchang Guo, Zhifei Zhang, Mengkai Song, Siyan Zheng, Qian Wang,
and Ben Niu, ‚ÄúTowards compression-resistant privacy-preserving photo sharing on so-
cial networks,‚Äù in Proceedings of the Twenty-First International Symposium on Theory,
Algorithmic Foundations, and Protocol Design for Mobile Networks and Mobile Com-
puting , 2020, pp. 81‚Äì90.
[7] Wen Zhou, Xin Hou, Yongjun Chen, Mengyun Tang, Xiangqi Huang, Xiang Gan, and
Yong Yang, ‚ÄúTransferable adversarial perturbations,‚Äù in Proceedings of the European
Conference on Computer Vision (ECCV) , 2018, pp. 452‚Äì467.
[8] Johannes Ball¬¥ e, David Minnen, Saurabh Singh, Sung Jin Hwang, and Nick Johnston,
‚ÄúVariational image compression with a scale hyperprior,‚Äù in International Conference
on Learning Representations , 2018.
[9] Zhengxue Cheng, Heming Sun, Masaru Takeuchi, and Jiro Katto, ‚ÄúLearned image
compression with discretized gaussian mixture likelihoods and attention modules,‚Äù in
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recogni-
tion, 2020, pp. 7939‚Äì7948.
[10] Yang Sui, Ding Ding, Xiang Pan, Xiaozhong Xu, Shan Liu, Bo Yuan, and Zhenzhong
Chen, ‚ÄúCorner-to-center long-range context model for efficient learned image com-
pression,‚Äù Journal of Visual Communication and Image Representation , vol. 98, pp.
103990, 2024.[11] Benjamin Bross, Ye-Kui Wang, Yan Ye, Shan Liu, Jianle Chen, Gary J Sullivan, and
Jens-Rainer Ohm, ‚ÄúOverview of the versatile video coding (vvc) standard and its
applications,‚Äù IEEE Transactions on Circuits and Systems for Video Technology , vol.
31, no. 10, pp. 3736‚Äì3764, 2021.
[12] Gregory K Wallace, ‚ÄúThe jpeg still picture compression standard,‚Äù Communications
of the ACM , vol. 34, no. 4, pp. 30‚Äì44, 1991.
[13] David S Taubman and Michael W Marcellin, ‚ÄúJpeg2000: Image compression funda-
mentals,‚Äù Standards and Practice , vol. 11, no. 2, 2002.
[14] Fabrice Bellard, ‚ÄúBpg image format (2014),‚Äù Volume , vol. 1, pp. 2, 2016.
[15] Yang Sui, Zhuohang Li, Ding Ding, Xiang Pan, Xiaozhong Xu, Shan Liu, and Zhen-
zhong Chen, ‚ÄúReconstruction distortion of learned image compression with impercep-
tible perturbations,‚Äù in ICML 2023 Workshop Neural Compression: From Information
Theory to Applications , 2023.
[16] Tong Chen and Zhan Ma, ‚ÄúTowards robust neural image compression: Adversarial
attack and model finetuning,‚Äù IEEE Transactions on Circuits and Systems for Video
Technology , 2023.
[17] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
Adrian Vladu, ‚ÄúTowards deep learning models resistant to adversarial attacks,‚Äù in
International Conference on Learning Representations , 2018.
[18] Florian Tram` er, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and
Patrick McDaniel, ‚ÄúEnsemble adversarial training: Attacks and defenses,‚Äù in Interna-
tional Conference on Learning Representations , 2018.
[19] Ambra Demontis, Marco Melis, Maura Pintor, Matthew Jagielski, Battista Biggio,
Alina Oprea, Cristina Nita-Rotaru, and Fabio Roli, ‚ÄúWhy do adversarial attacks
transfer? explaining transferability of evasion and poisoning attacks,‚Äù in 28th USENIX
security symposium (USENIX security 19) , 2019, pp. 321‚Äì338.
[20] Ian Goodfellow, Jonathon Shlens, and Christian Szegedy, ‚ÄúExplaining and harnessing
adversarial examples,‚Äù in International Conference on Learning Representations , 2015.
[21] Jean B¬¥ egaint, Fabien Racap¬¥ e, Simon Feltman, and Akshay Pushparaja, ‚ÄúCompressai:
a pytorch library and evaluation platform for end-to-end compression research,‚Äù arXiv
preprint arXiv:2011.03029 , 2020.