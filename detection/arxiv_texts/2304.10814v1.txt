Automated Static Camera Calibration with
Intelligent Vehicles
Alexander Tsaregorodtsev, Adrian Holzbock, Jan Strohbeck, Michael Buchholz, and Vasileios Belagiannisy
Institute for Measurement-, Control-, and Microtechnology
Ulm University , Germany
ffirstnameg.flastnameg@uni-ulm.de
yChair of Multimedia Communications and Signal Processing
Friedrich-Alexander-Universit ¬®at Erlangen-N ¬®urnberg , Germany
vasileios.belagiannis@fau.de
Abstract ‚ÄîConnected and cooperative driving requires pre-
cise calibration of the roadside infrastructure for having a
reliable perception system. To solve this requirement in an
automated manner, we present a robust extrinsic calibration
method for automated geo-referenced camera calibration. Our
method requires a calibration vehicle equipped with a combined
GNSS/RTK receiver and an inertial measurement unit (IMU)
for self-localization. In order to remove any requirements for the
target‚Äôs appearance and the local trafÔ¨Åc conditions, we propose a
novel approach using hypothesis Ô¨Åltering. Our method does not
require any human interaction with the information recorded
by both the infrastructure and the vehicle. Furthermore, we do
not limit road access for other road users during calibration.
We demonstrate the feasibility and accuracy of our approach
by evaluating our approach on synthetic datasets as well as a
real-world connected intersection, and deploying the calibration
on real infrastructure. Our source code is publicly available1.
I. I NTRODUCTION
The research on automated driving topics has recently
expanded beyond the automated vehicle itself and started
including other sources of environment perception, mainly
stemming from connected infrastructure, e.g., roadside units
(RSUs) [1]. Such units mostly incorporate monocular cameras,
which provide a simple and cheap way [2] of environment
perception. The information provided by the RSUs can be
integrated into downstream tasks like road user trajectory
prediction and cooperative trajectory planning [3]. Therefore,
the localization accuracy of road users needs to be sufÔ¨Åcient
to avoid dangerous situations due to inaccurate data, assuming
a high calibration quality of the infrastructure. Furthermore, if
the RSU infrastructure is going to be installed in a place with
heavy trafÔ¨Åc, creating a constrained environment for precise
calibration may be challenging. As an example, classical cal-
ibration methods mostly require manually placed calibration
targets in the sensor‚Äôs Ô¨Åeld of view (FoV) [4]. The placement
of such targets in a heavily congested intersection may not be
viable in every situation, making manual targets unsuitable for
performing extrinsic calibration in the Ô¨Åeld.
Part of this work was Ô¨Ånancially supported by the Federal Ministry
for Economic Affairs and Climate Action of Germany within the program
‚ÄùNew Vehicle and System Technologies‚Äù (project LUKAS, grant number
19A20004F).
1https://github.com/Tuxacker/infrastructure calibration toolbox
Fig. 1. Approach overview. From left to right, tracked object bounding boxes
extracted from camera images and vehicle localization data are recorded.
Data from both domains are then merged based on the recorded timestamps
to create extrinsic calibration hypotheses. The hypotheses are grouped and
Ô¨Åltered to remove tracks of other road users. Finally, the best-performing
hypothesis group is merged and reÔ¨Åned to obtain the Ô¨Ånal calibration Kext.
There is prior work on explicitly adapting the calibration
for infrastructure, e.g., [5], [6] to reduce human interaction
and simplify the process. However, these methods require
overlapping Ô¨Åelds of view, implicating multiple sensors and
thus driving up the infrastructure costs. Furthermore, in the
case of temporary infrastructure, like at construction sites,
only one RSU may be deployed, and a second sensor for
calibration may not be available. As an alternative, feature-
based calibration approaches emerged in the literature. Edge-
based approaches like [7], [8] use object edge features to
estimate an extrinsic calibration while under trafÔ¨Åc by using
the trafÔ¨Åc itself as a calibration target. In contrast to other
techniques, the resulting calibration is not geo-referenced, thus
hindering the conversion of the detected trafÔ¨Åc participants
into the coordinate system of an automated vehicle. Finally,
deep learning approaches like [9] have been proposed, where
an extrinsic calibration is obtained from camera data and
intelligent vehicle (IV) lidar sensor data by means of a neural
network. With [9] reporting calibration errors of over 1m,
these approaches are unsuitable for cooperative driving scenar-
ios. As an example, a vehicle detection with such localizationarXiv:2304.10814v1  [cs.CV]  21 Apr 2023errors may be erroneously assigned to an incorrect lane.
We propose an automated single monocular camera calibra-
tion approach designed for live trafÔ¨Åc conditions, addressing
the limitations of the previously mentioned methods. Our
method uses a GNSS/RTK- and IMU-equipped vehicle, which
records its localization data while driving through the FoV
of the sensor under calibration. By combining the recorded
vehicle localization data with synchronously recorded camera
data, an extrinsic calibration can be estimated without any hu-
man interaction. Our approach consists of data pre-processing,
hypothesis generation, Ô¨Åltering/grouping, and reÔ¨Ånement steps.
Fig. 1 gives a visual overview of the proposed algorithm.
During the initial data pre-processing, objects in the camera
images are detected with a neural object detection network.
These objects are then tracked over time in pixel coordinates.
In the hypothesis generation step, the tracks in pixel coordi-
nates are combined with synchronized localization data from
the calibration vehicle to create association hypotheses. By
estimating the calibration from each hypothesis and subse-
quent Ô¨Åltering and grouping, the tracks most likely associated
with the GNSS/RTK- and IMU-equipped vehicle are obtained,
while tracks belonging to other trafÔ¨Åc participants are re-
moved. During the reÔ¨Ånement step, the remaining hypotheses
are combined, and the merged calibration from all hypotheses
is reÔ¨Åned. The reÔ¨Ånement step yields the Ô¨Ånal extrinsic cali-
bration. Our experiments using simulated data generated with
DeepSIL [10] and CARLA [11] and real-world data from our
test site [1] show sufÔ¨Åcient accuracy for automated driving
applications. Our contributions are summarized as follows:
An automated approach for geo-referenced infrastructure
calibration based on a novel hypothesis Ô¨Åltering scheme
enabling the use of standard camera calibration algo-
rithms like [12] on unmodiÔ¨Åed and unÔ¨Åltered data from
the infrastructure.
Minimal requirements for the calibration vehicle, as only
a localization sensor is required and the vehicle size
has to be known, and almost no requirements for the
appearance of the calibration vehicle as well as the sensor
environment, therefore other road users may freely use
the area captured by the sensor during the calibration
process.
Experiments on synthetic and real-world data show ap-
plicability in automated and cooperative driving tasks.
II. R ELATED WORK
The calibration of camera sensors is a widely explored topic,
which can be further divided into four different types. Namely,
the methods can be split into target-based like ours, feature-
based, neural-network-based, and motion-based approaches.
The most classic approaches are target-based, e.g., [4], and
use a chessboard pattern or other target types to estimate in-
trinsic and extrinsic calibration parameters. A more specialized
approach targeting road infrastructure was presented in [5],
where a pair of Ô¨Åsheye cameras with large overlapping Ô¨Åelds
of view were calibrated using vehicle calibration targets. In asimilar fashion, our approach does not need static targets that
may hinder the trafÔ¨Åc during calibration.
Feature-based approaches adopt various image features as
2D-3D-correspondences. The algorithm presented in [13] em-
ploys a mobile RGB-D camera setup to scan the environment
and utilizes this data to create a 3D environment model of the
observable area. Then, the 3D model features are matched with
2D feature descriptors to Ô¨Ånd the closest matching keyframe
from the scan. Another method described in [7] extracts and
tracks object edges of moving vehicles. The edge features
are then subsequently used to estimate the camera‚Äôs vanishing
points and calculate the extrinsic camera transformation matrix
into a local, non-geo-referenced coordinate system. A newer
idea proposed in [14] employs a neural network trained to
output landmark positions of speciÔ¨Åc vehicle models. The
spatial relations between the landmarks are then compared
with distances obtained from measuring the landmark dis-
tances of an accurate 3D model of the selected vehicle type
to Ô¨Ånd a suitable camera calibration, which, however, is not
geo-referenced. Finally, the method described in [15] applies
semantic segmentation networks to segment camera images as
well as lidar scans of the environment and then minimizes the
visual error between the image segmentation and a rendered
view of the segmented point cloud by adapting the initial
camera transformation parameters. In contrast to the presented
methods, our method is global, i.e., it does not require an
initial calibration estimate to work while also providing a geo-
referenced global position.
With the recent emergence of neural networks to perform
various tasks, direct 2D-3D matching algorithms were pro-
posed. The network presented in [16] directly predicts the
similarity between point cloud patches and image patches. [17]
Ô¨Ånd correspondences between 2D line features obtained from
a neural network with 3D line features calculated from a lidar
scan, while [9] utilize a classiÔ¨Åcation network to classify points
being inside or outside of the camera frustum and then solve
an inverse camera projection problem. Our proposed method,
in comparison, does not use neural networks to create 2D-3D
correspondences but only to perform object detection, which
results in smaller calibration errors.
Another distinct algorithm type present in literature is
motion-based calibration [18]‚Äì[21], which exploits the move-
ment of the sensor itself and is particularly suitable for auto-
mated vehicle calibration or online re-calibration. For example,
[18] employ visual SLAM to estimate the sensor movement
and then optimize the loop closure between different time
steps using an efÔ¨Åcient dual quaternion representation of
the extrinsic rotation and translation, which allows for real-
time re-calibration. This approach type is not applicable to
infrastructure, as it is stationary and captures a static view of
the environment.
III. C ALIBRATION METHOD
In the following subsections, we formulate our calibra-
tion objective and describe every step of our algorithm. An
overview of the algorithm is shown in Fig. 1.A. Problem Formulation
A standard perspective camera model [4] is deÔ¨Åned as
follows:
2
4u
v
c3
5=Kint:=Kextz}|{
R t
012
664x
y
z
13
775;2
4u
v
13
5=1
c2
4u
v
c3
5 (1)
where [u;v;1]Tdenotes a homogeneous camera image coor-
dinate of an image I, and [x;v;z; 1]Tis a homogeneous point
coordinate of a point in 3D world space. Our goal is to estimate
Kextcontaining a rotation Rand translation t, which can
transform between world/UTM [22] and sensor coordinates.
We assume the intrinsic camera matrix Kintis known. The
data used for calibration consists of GNSS/RTK+IMU mea-
surements in UTM coordinates xt= [xUTMyUTMzUTM  ]
at timestept, where; ; andare the roll, pitch, and yaw,
respectively. Furthermore, we utilize the vehicle box detec-
tions of the camera under calibration, where each detection
bi;t= [u v w h ]has a unique ID iand contains the box
center coordinates uandv, as well as the width wand height
h.
B. Data Acquisition and Pre-processing
At the beginning of the calibration pipeline, data from the
sensor under calibration as well as the location of the target
vehicle has to be recorded. Our algorithm only imposes two
constraints on this recording process. Firstly, we require time
synchronicity between camera and localization measurements,
as the subsequent 2D-3D correspondences are based on the
association of camera and vehicle data by their timestamps.
This requirement is enforced by the connected infrastructure,
as it is also required for assisted driving. Secondly, we require
at least two different traversals of the sensor‚Äôs FoV , such that
the target vehicle is not visible or detectable in the camera
image between traversals. This fact is used later to Ô¨Ålter
out false positive hypotheses in Sec. III-E. After collecting
the calibration data, the data has to be processed for the
subsequent steps. To process the vehicle localization data,
either the mean position of the recording or an externally
deÔ¨Åned reference point is subtracted from each position mea-
surement. This helps the numerical stability, as the recorded
UTM coordinates may be in the range of 105 107, which can
lead to numerical instabilities during later matrix (inversion)
operations. To process the image data, object-level features
are extracted. The feature extraction is performed by a neural
object detection network, which returns bounding boxes for
all vehicles in an image. This leaves us with vehicle bounding
boxes bi;tof all vehicles that passed the sensor infrastructure
during a single time step tof the recording, with each one
having a unique ID i= 1:::N , including our calibration
target vehicle.
C. Object Tracking in Image Space
Following the bounding box extraction, the object boxes
are associated over all time steps, thus tracking the objectsover the recording period. Usually, a multi-object tracker
(MOT) [23] is used for such tasks, however, this requires the
sensor measurements to be in a metric space in order to apply
the underlying kinematic models, i.e., CV/CA/CTRA [24]. As
in our case, the transformation to the metric space is not known
yet; such trackers can not be adopted for our purpose. Instead,
we rely on a model-less tracking approach. Therefore, we use
the Hungarian Algorithm [25] to Ô¨Ånd the most cost-effective
box-to-box association between two frames. In our case, we
deÔ¨Åne our association cost as follows:
d(b1;b2) =(
1 DIoU (b1;b2);if IoU (b1;b2)>0
2 ;else(2)
where IoU is the intersection-over-union between two boxes
and DIoU is the distance-IoU introduced in [26], which
additionally penalizes the distance between bounding box
centers and can take values between 0and2. While using
Eq. 2, we assume that the camera frame rate is high enough
so that two consecutive bounding boxes of the same vehicle
partially overlap if the vehicle obeys the speed limit, otherwise
incorrect object associations may occur at a much higher rate.
This implies that object boxes not overlapping with another
box are newly detected objects. Furthermore, by using the
DIoU, we additionally penalize associating object boxes with
vastly different sizes and center locations. Using the Hungarian
algorithm and preventing association between two objects if
their cost equals 2, we can add new box measurements to
existing tracks and set up new tracks if no matching box from
the previous frame is found. If a track from the previous frame
has no associated boxes in the current frame, it is linearly
extrapolated. This extrapolation is performed until the track
can be associated with a newly detected box or the amount
of extrapolated timestamps exceeds a threshold, in which case
the track is removed from the current list of tracked objects.
By applying this tracking procedure to the data of the entire
recorded sequence, it is possible to reconstruct the path of
each vehicle that drove through the sensor‚Äôs FoV . As the
tracking is performed ofÔ¨Çine, we can remove all extrapolated
box positions from the tracks. At the end of this step, we have
a list ofMobject tracksT=fo1:::oMg;oi=fbi;tgtend;i
t=tstart;i
belonging to uninvolved trafÔ¨Åc and the target vehicle.
D. Hypothesis Creation
After obtaining all object tracks Tfrom the images of
the sensor under calibration, the calibration vehicle tracks
need to be detected and separated from other object tracks.
We formulate this task as a kind of hypothesis Ô¨Åltering,
where each object track may hypothetically be the target
vehicle. To construct a hypothesis hi=fxt;bi;tgtend;i
t=tstart;i,
we associate a given object track oi, where we assume
the object location to be the bounding box center, with the
recorded GNSS/RTK+IMU data xfrom the same timestamps
t2[tstart;i;tend;i], yielding 2D-3D correspondences between
the image and world coordinate frames. The width and height
contained in oiare not considered for the initial estimate.After creating a hypothesis out of every object track, initial
Ô¨Åltering can be performed. During the initial Ô¨Åltering, all
hypotheses which do not contain enough data points (the
EPnP algorithm used requires at least 4 correspondences)
or have short track lengths in either 2D or 3D coordinates
(i.e. the observed vehicle didn‚Äôt move) are discarded. Then, all
remaining hypotheses are accepted to obtain a Ô¨Årst estimate
ofKextfor each hypothesis using the EPnP algorithm [12].
As this approach is negatively affected by outliers in the input
data, a RANSAC scheme [27] is employed to Ô¨Ålter out such
outliers. If the median reprojection errors of the estimates
exceed a threshold, the hypothesis is discarded. To further
narrow down the hypotheses, the camera position is obtained
withxcam= RTt. The hypothesis is then also discarded if
the camera position is over a threshold distance dthraway from
the path driven by the target vehicle or if the camera height
is below ground. dthrcan be in the range from 10m for real
data to 30m for synthetic data. The remaining Nfhypotheses
form the setH=fhigNf
i=1.
E. Hypothesis Grouping and Filtering
After determining the set of Ô¨Åltered, valid hypotheses H,
a unique solution has to be found. At this stage, Hshould
contain at least two or more tracks belonging to the target
vehicle, depending on the number of passes along the in-
frastructure. However, incorrect hypotheses belonging to other
vehicles may still be contained in the set. This can occur if the
trajectory of the calibration vehicle driving outside the sensor‚Äôs
FoV closely resembles the trajectory of another vehicle passing
the sensor, which is most common on straight stretches of
road. To remove incorrect hypotheses, clustering of similar
hypotheses is performed in two parts. In the Ô¨Årst part, the
similarity between two hypotheses hiandhjis determined
by the following scores:
Outlier ratio: The object track contained in hiis projected
into world space with the calibration estimate Kext;jof
hj. The ratio routof points lying outside the camera
frustum of hypothesis hjis determined.
Overlap ratio: The localization track contained in hiis
projected into image space with the calibration estimate
Kext;j. Then, the ratio rovof 2D localization points within
kpixels of any bounding box in the calibration sequence
is calculated.
Rotational similarity: We deÔ¨Åne and obtain the rotational
similarityrsim=tr(R 1
jRi)=3, where Rfi;jgis the
upper left matrix of Kext;fi;jg, as deÔ¨Åned in Eq. 1.
If all three scores lie within speciÔ¨Åed ranges described by a
set of hyper-parameters, the estimated Kextof two hypotheses
and, therefore, the hypotheses themselves are considered sim-
ilar. By comparing every pairing possible in H, we can build
a hypothesis graph where similar hypotheses are connected by
edges. In the second step, all unconnected nodes are discarded,
as we require at least two traversals of the camera environment
in order to create a groupable sub-graph in the Ô¨Årst step. The
remaining sub-graphs are fed to the DBSCAN [28] clusteringalgorithm to group all hypotheses with similar camera coordi-
nates, with most to all outliers removed. This results in a list of
different hypothesis groups, where each group contains only
similar hypotheses. If no hypotheses are left at this stage, it
indicates an insufÔ¨Åcient count of calibration vehicle traversals.
F . Result ReÔ¨Ånement
After obtaining valid hypothesis groups, the data inside
these groups can be reÔ¨Åned and merged. Using merged
tracks containing different trajectories may help minimize the
heteroscedastic errors introduced by missing depth informa-
tion. Furthermore, in Sec. III-D, the center of the bounding
box and the geometric center of the vehicle are used as cor-
respondences. However, this may not necessarily be accurate.
We, therefore, propose a reÔ¨Ånement step that exploits
knowledge about the size of the target vehicle to reduce
the error caused by the previous assumption. By using the
orientation information contained in xtand the known vehicle
dimensions, we can calculate the footprint of the vehicle,
i.e., the world coordinates of its corners in the ground plane.
Due to geometrical constraints, one of these ground points
must lie on the bounding box‚Äôs bottom edge. For reÔ¨Ånement,
the bounding box of the vehicle is projected onto the ground
plane resulting in a quadrilateral. The projection is made with
one of the previously obtained hypothetical transformations.
The required ground plane is estimated by performing a
principal component analysis (PCA) [29] of the recorded
localization track. The vehicle corner point with the lowest
distance to the projected bottom line of the bounding box
rectangle is chosen as the new vehicle reference point xi;t;n
(cyan point in Fig. 2). To get the corresponding bounding
box point bi;t;n = [unvn](light green point in Fig. 2),
xi;t;n is projected back into the 2D bounding box, where its
closest point on the bottom box edge is found. This process is
visualized in Fig. 2. Alternatively, the optimization approach
described in [30] can be used to obtain reÔ¨Åned point pairs.
After obtaining the reÔ¨Åned point pairs fxi;t;n;bi;t;ng, the line-
point registration described in [31] is applied to calculate an
optimized transformation which forces the physical boundaries
of the vehicle to align with the bounding box bottom line
projected onto the ground plane, improving the estimated
calibration. By performing this reÔ¨Ånement on each hypothesis
separately, then merging the data of all hypotheses tracks of
a group and repeating reÔ¨Ånement on the ensemble hypothesis,
the extrinsic calibration Kextis obtained for each group. By
measuring the distance between the point correspondences
used for reÔ¨Ånement after performing the optimization step,
the transformation with the smallest average distance between
the projected bounding box point and vehicle corner point p,
denoted by the dark green line in Fig. 2, is chosen as the Ô¨Ånal
Kext. The merged calibration may be discarded if the average
errorpof one speciÔ¨Åc track is lower than pof the merged
calibration when evaluating on the merged tracks.Fig. 2. Overview of the reÔ¨Ånement procedure in image space and bird‚Äôs eye view. A new correspondence is found by Ô¨Ånding the vehicle corner point closest
to the bottom box edge (denoted in cyan). The associated bounding box point is determined by backprojecting the corner point into the image and Ô¨Ånding
its closest point on the bottom box edge (denoted in light green). The length of the dark green line represents the localization error made by using a speciÔ¨Åc
calibration. This error is denoted as pin Sec. IV-B. The vehicle image was generated with CARLA [11].
IV. E VALUATION
In order to test our approach, a number of different evalua-
tion protocols were applied. The evaluation was performed
with both real-world sequences from our test site as well
as synthetic data. In the following, we provide a detailed
description of the evaluation setups and present the results.
A. Data sources and Evaluation Setup
We evaluate our approach on both real and synthetic data.
First, we employ an extended version of the DeepSIL [10]
and CARLA [11] simulators to generate synthetic localization
and detection data as input for the algorithm. Second, we rely
on real-world sequences from our test site [1]. The calibration
obtained from the real-world sequences is deployed at the test
site and is utilized for cooperative driving tests.
The DeepSIL simulator uses the digital map of our real-
world test site [1] to create a digital twin and simulate
scenarios with different virtual vehicles which pass it. During
the simulation, one actor vehicle is chosen as the calibration
vehicle, and its world position during every time step is
recorded. To generate image-space detections, three virtual
cameras are deÔ¨Åned in the digital map. Using the deÔ¨Åned
camera transforms, detection bounding boxes for each vehicle
inside the virtual sensor‚Äôs FoV are generated. As real-world
detections and measurements are not exact and always come
attached with some type of detection probability and measure-
ment noise, a simple, normally distributed noise is added to the
generated data. We added Gaussian noise with = 0:075m to
both the x- and y-position of the simulated calibration vehicle,
therefore emulating noisy measurements of the localization
track. Noisy bounding box detections can also be generated
if the additive noise signiÔ¨Åcantly exceeds the measurement
noise of a real-world localization unit. For each type of simu-
lated measurement, its corresponding generation timestamp is
recorded as well, as it is required for hypothesis creation. The
tracking, hypothesis creation, Ô¨Åltering, and reÔ¨Ånement steps
are performed directly on the resulting synthetic data, which
is used as an input to the algorithm.
The CARLA conÔ¨Åguration is similar to the DeepSIL setup.
However, the map does not correspond to a real-world lo-
cation. Furthermore, we do not add any noise to the data
generated by CARLA to evaluate our approach‚Äôs performancein a controlled environment and simulate ideal measurements
while having access to different scenarios involving signalized
intersections and vulnerable road users.
We also recorded multiple calibration sequences of up to
15 minutes in length on a real intersection located in Ulm-
Lehr, Germany. The intersection consists of multiple cameras
with a resolution of 1920x1200 and a frame rate of 10
fps, where some cameras have partially overlapping Ô¨Åelds of
view, are rotated into a portrait orientation, and have different
focal lengths. Therefore wide-angle, standard, and telephoto
perspectives are available and evaluated in Table II. A test
vehicle equipped with a GNSS/RTK+IMU localization system
is used as a target vehicle providing high-precision localization
data with a rate of 50Hz and localization errors below 5cm.
The synchronization between both data sources is ensured by
using GPS time as the reference time, as both our vehicle
and test site are equipped with GPS receivers providing the
same time reference, with measured clock offsets below 2ms,
enabling the execution of cooperative maneuvers. Evaluation
with real-world sequences is performed ofÔ¨Çine after recording
in the same way as with synthetic data, with the only difference
being the additional feature extraction step, in which objects
are detected in the camera images to obtain object bounding
boxes. For this purpose, we rely on YOLOv5s network [32]
trained on the BDD100K dataset [33].
B. Results discussion for synthetic and real calibration se-
quences
Table I summarizes the evaluation results with synthetic
data generated by DeepSIL and CARLA. Due to the use of
bounding box object data, the object‚Äôs outline is unavailable.
Furthermore, the box corners usually do not align with object
corners, making them unsuitable for distance error calculation.
In order to circumvent this lack of information, we apply the
approach from Sec. III-F shown in Fig. 2. Here, we project
the bottom box edge onto the ground plane and measure
the minimal distance pto the closest ground point of the
calibration vehicle, which is obtained from the localization
measurements. We provide the mean and maximal absolute
distance errors p;mandp;was we want to assess the usability
of estimated calibrations for real cooperative driving use cases
and deployments instead of just comparing the camera positionto a ground truth position. We also report the mean and
maximum relative distance errors emandew, which are
obtained by dividing p;fm;wgby the distance between the
vehicle corner point and camera. The evaluation is always
performed on the entire merged data of all calibration vehicle
tracks.
TABLE I
SYNTHETIC EVALUATION RESULTS FOR DEEPSIL AND CARLA.
Scene SIL-North SIL-South SIL-West CARLA
em(%) 0.83 1.39 0.22 0.17
ew(%) 2.80 2.22 1.80 0.68
p;m (m) 0.36 0.54 0.06 0.087
p;w(m) 1.52 0.86 0.4 0.237
From the results, we can clearly see the performance of
our approach. The results are promising when looking at the
CARLA results obtained from ideal, noise-free data. However,
an averagep;mof around 0.09 msuggests a small systematic
error. This error stems from the reÔ¨Ånement step, where the
distance between the projected bottom edge and the closest
ground point is minimized. An error is introduced as this is
done by linearly projecting the ground point onto the bottom
edge from the currently estimated perspective instead of the
ground truth perspective. The DeepSIL results show higher
errors than the CARLA results and similar values to Table II,
which can be explained by the added noise. The comparatively
higher mean error for the virtual ‚ÄúSouth‚Äù camera stems from
the fact that this camera is placed almost away 30 meters
from any road segment. Therefore, measurements close to the
camera are not present, impacting the calibration quality.
TABLE II
EVALUATION RESULTS FROM REAL -WORLD SEQUENCES .
Camera name Wide Tele 1 Tele 2 Portrait Regular
f 815.5 2174 2186 1067 1422
em(%) 0.91 0.67 0.29 0.55 0.65
ew(%) 2.94 2.42 2.33 4.78 3.17
p;m (m) 0.26 0.37 0.14 0.24 0.32
p;w(m) 0.50 1.49 1.03 2.18 1.44
For the real-world sequences, in addition to the metrics
reported in Table I, the previously measured focal length f
contained in Kintis reported in order to differentiate the
different cameras as well as demonstrate the applicability
of our approach to different camera conÔ¨Ågurations. When
comparing the results of the different cameras, we can see that
each camera, independent of its focal length, achieves mean
distance errors below 0.4 m, which is acceptable for automated
scenarios, as the planning module of an automated vehicle can
be adjusted to add 0.4 msafety margins. Furthermore, the error
does not increase signiÔ¨Åcantly with greater distances between
object and camera, as shown in Fig. 3. The high maximal error
Fig. 3. Calibration vehicle localization error pmean values over 0.5 m
distance intervals plotted over the distance between camera and vehicle. For
the ‚ÄúPortrait‚Äù camera, the maximal error of 2:0m is reached at 45m and
therefore clipped for better visualization.
of the camera in portrait orientation is caused by the occlusion
of vehicles due to trees and lamp poles in the scene, as
described in Sec.IV-C. When looking at automatic calibration
approaches like [7] and [8], we see similar or better numbers
in the used evaluation metrics. However, our approach can not
be compared directly with previous works, as other automatic
approaches do not provide geo-referenced calibrations and are
not evaluated on the same dataset.
C. Further Experiments
We repeated the DeepSIL simulation with different magni-
tudes for the position noise to evaluate our results beyond
our provided localization error measurements. As seen in
Table III, the average and maximal relative errors emandew
increase, but only up to a certain threshold, as the results for
pos= 0:2mandpos= 0:5mare very similar. We explain
this due to our application of a RANSAC-based transform
estimation, which directly discards incorrect data points that
cause excessive reprojection errors, making our algorithm
more robust against low-quality input data. It should be noted,
however, that in a real use case, the measurement errors of the
localization unit are not randomly distributed but rather slowly
drift over time, resulting in a small position bias, which has a
smaller impact on the estimated transform.
TABLE III
DEEPSIL SIMULATION WITH DIFFERENT ADDITIVE NOISE FOR
SIL-S OUTH SCENARIO
pos 0.075 m 0.2m 0.5m
em(%) 1.39 2.22 2.30
ew(%) 2.22 11.32 12.36
In addition to the measurement noise experiment, we visu-
alize the localization error in relation to camera distance for all
real-world sequences in Fig.3. It is visible that the error is not
increasing signiÔ¨Åcantly with increased distance to the camera,
indicating that the rotation is estimated correctly. Furthermore,
some regions of increased errors can be identiÔ¨Åed. In our
case, the ‚ÄúPortrait‚Äù camera observes a light pole obstacleat a distance of approx. 45 m, causing partial or missing
detections of vehicles behind the obstacle, which results in
high localization errors. The same applies to camera ‚ÄúTele 1‚Äù,
where one lane is partially occluded by a tree at a distance of
60m, while the error of camera ‚ÄúRegular‚Äù is caused by an
uneven slope of the ground in front of the camera.
V. CONCLUSION
We presented a new automated approach for geo-referenced
infrastructure calibration based on hypothesis Ô¨Åltering. Our
method only imposes minimal requirements on the calibration
vehicles, is insensitive to other trafÔ¨Åc participants, and is not
dependent on human interaction. The calibration is performed
by recording camera data and the position and orientation
of the target vehicle while driving at least twice through
the camera‚Äôs FoV under calibration. In a pre-processing step,
object tracks are extracted from the camera data. Then, the
camera and vehicle tracks are combined during hypothesis
generation. Subsequent Ô¨Åltering removes incorrect hypotheses,
and in the Ô¨Ånal reÔ¨Ånement step, the calibration is reÔ¨Åned
to take the 3D bounding box of the vehicle into account.
An evaluation on synthetic and real-world data shows the
effectiveness of our approach.
REFERENCES
[1] M. Buchholz, et al. , ‚ÄúHandling occlusions in automated driving using a
multiaccess edge computing server-based environment model from in-
frastructure sensors,‚Äù IEEE Intelligent Transportation Systems Magazine ,
vol. 14, no. 3, pp. 106‚Äì120, 2022.
[2] B. Rinner and W. Wolf, ‚ÄúAn introduction to distributed smart cameras,‚Äù
Proceedings of the IEEE , vol. 96, no. 10, pp. 1565‚Äì1575, 2008.
[3] M. B. Mertens, J. M ¬®uller, and M. Buchholz, ‚ÄúCooperative maneuver
planning for mixed trafÔ¨Åc at unsignalized intersections using proba-
bilistic predictions,‚Äù in 2022 IEEE Intelligent Vehicles Symposium (IV) .
IEEE, 2022, pp. 1174‚Äì1180.
[4] R. Hartley and A. Zisserman, ‚ÄúComputation of the camera matrix P,‚Äù
inMultiple View Geometry in Computer Vision . Cambridge University
Press, 2004, p. 178‚Äì236.
[5] S. R. E. Datondji, Y . Dupuis, P. Subirats, and P. Vasseur, ‚ÄúRotation and
translation estimation for a wide baseline Ô¨Åsheye-stereo at crossroads
based on trafÔ¨Åc Ô¨Çow analysis,‚Äù in 2016 IEEE 19th International Con-
ference on Intelligent Transportation Systems (ITSC) , 2016, pp. 1534‚Äì
1539.
[6] C. Zhu, Z. Zhou, Z. Xing, Y . Dong, Y . Ma, and J. Yu, ‚ÄúRobust plane-
based calibration of multiple non-overlapping cameras,‚Äù in 2016 Fourth
International Conference on 3D Vision (3DV) , 2016, pp. 658‚Äì666.
[7] M. Dubsk ¬¥a, A. Herout, R. Jur ¬¥anek, and J. Sochor, ‚ÄúFully automatic
roadside camera calibration for trafÔ¨Åc surveillance,‚Äù IEEE Transactions
on Intelligent Transportation Systems , vol. 16, no. 3, pp. 1162‚Äì1171,
2014.
[8] M. Dubsk ¬¥a, A. Herout, and J. Sochor, ‚ÄúAutomatic camera calibration
for trafÔ¨Åc understanding.‚Äù in BMVC , vol. 4, no. 6, 2014, p. 8.
[9] J. Li and G. H. Lee, ‚ÄúDeepI2P: Image-to-point cloud registration via
deep classiÔ¨Åcation,‚Äù in Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , 2021, pp. 15 960‚Äì15 969.
[10] J. Strohbeck, J. M ¬®uller, A. Holzbock, and M. Buchholz, ‚ÄúDeepSIL: A
software-in-the-loop framework for evaluating motion planning schemes
using multiple trajectory prediction networks,‚Äù in 2021 IEEE/RSJ Inter-
national Conference on Intelligent Robots and Systems (IROS) , 2021,
pp. 7075‚Äì7081.
[11] A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V . Koltun,
‚ÄúCARLA: An open urban driving simulator,‚Äù in Proceedings of the 1st
Annual Conference on Robot Learning , 2017, pp. 1‚Äì16.
[12] V . Lepetit, F. Moreno-Noguer, and P. Fua, ‚ÄúEPnP: An accurate o(n)
solution to the PnP problem,‚Äù International journal of computer vision ,
vol. 81, no. 2, pp. 155‚Äì166, 2009.[13] E. Ataer-Cansizoglu, Y . Taguchi, S. Ramalingam, and Y . Miki, ‚ÄúCali-
bration of non-overlapping cameras using an external SLAM system,‚Äù
in2014 2nd International Conference on 3D Vision , vol. 1, 2014, pp.
509‚Äì516.
[14] V . Bartl, J. ÀáSpaÀánhel, P. Dobe Àás, R. Juranek, and A. Herout, ‚ÄúAutomatic
camera calibration by landmarks on rigid objects,‚Äù Machine Vision and
Applications , vol. 32, no. 1, pp. 1‚Äì13, 2021.
[15] A. Tsaregorodtsev, J. Muller, J. Strohbeck, M. Herrmann, M. Buchholz,
and V . Belagiannis, ‚ÄúExtrinsic camera calibration with semantic seg-
mentation,‚Äù in 2022 IEEE 25th International Conference on Intelligent
Transportation Systems (ITSC) . IEEE, 2022, pp. 3781‚Äì3787.
[16] M. Feng, S. Hu, M. H. Ang, and G. H. Lee, ‚Äú2D3D-Matchnet: Learning
to match keypoints across 2D image and 3D point cloud,‚Äù in 2019
International Conference on Robotics and Automation (ICRA) , 2019,
pp. 4790‚Äì4796.
[17] H. Yu, W. Zhen, W. Yang, J. Zhang, and S. Scherer, ‚ÄúMonocular camera
localization in prior lidar maps with 2D-3D line correspondences,‚Äù
in2020 IEEE/RSJ International Conference on Intelligent Robots and
Systems (IROS) , 2020, pp. 4588‚Äì4594.
[18] M. Horn, T. Wodtko, M. Buchholz, and K. Dietmayer, ‚ÄúOnline extrinsic
calibration based on per-sensor ego-motion using dual quaternions,‚Äù
IEEE Robotics and Automation Letters , vol. 6, no. 2, pp. 982‚Äì989, 2021.
[19] T. Wodtko, M. Horn, M. Buchholz, and K. Dietmayer, ‚ÄúGlobally optimal
multi-scale monocular hand-eye calibration using dual quaternions,‚Äù in
2021 International Conference on 3D Vision (3DV) , 2021, pp. 249‚Äì257.
[20] L. Wei, L. Naiguang, D. Mingli, and L. Xiaoping, ‚ÄúCalibration-free
robot-sensor calibration approach based on second-order cone program-
ming,‚Äù in MATEC Web of Conferences , vol. 173. EDP Sciences, 2018,
p. 02005.
[21] E. Wise, M. Giamou, S. Khoubyarian, A. Grover, and J. Kelly,
‚ÄúCertiÔ¨Åably optimal monocular hand-eye calibration,‚Äù in 2020 IEEE
International Conference on Multisensor Fusion and Integration for
Intelligent Systems (MFI) . IEEE, 2020, pp. 271‚Äì278.
[22] E. Grafarend, ‚ÄúThe optimal universal transverse mercator projection,‚Äù
inGeodetic Theory Today: Third Hotine-Marussi Symposium on Math-
ematical Geodesy L‚ÄôAquila, Italy, May 30‚ÄìJune 3, 1994 . Springer,
1995, pp. 51‚Äì51.
[23] Y . Bar-Shalom, X. R. Li, and T. Kirubarajan, Estimation with applica-
tions to tracking and navigation: theory algorithms and software . John
Wiley & Sons, 2004.
[24] R. Schubert, E. Richter, and G. Wanielik, ‚ÄúComparison and evaluation
of advanced motion models for vehicle tracking,‚Äù in 2008 11th Interna-
tional Conference on Information Fusion , 2008, pp. 1‚Äì6.
[25] D. F. Crouse, ‚ÄúOn implementing 2d rectangular assignment algorithms,‚Äù
IEEE Transactions on Aerospace and Electronic Systems , vol. 52, no. 4,
pp. 1679‚Äì1696, 2016.
[26] Z. Zheng, P. Wang, W. Liu, J. Li, R. Ye, and D. Ren, ‚ÄúDistance-iou loss:
Faster and better learning for bounding box regression,‚Äù in Proceedings
of the AAAI conference on artiÔ¨Åcial intelligence , vol. 34, no. 07, 2020,
pp. 12 993‚Äì13 000.
[27] M. Zuliani, ‚ÄúRANSAC for dummies,‚Äù Vision Research Lab, University
of California, Santa Barbara , 2009.
[28] E. Schubert, J. Sander, M. Ester, H. P. Kriegel, and X. Xu, ‚ÄúDbscan
revisited, revisited: Why and how you should (still) use dbscan,‚Äù ACM
Trans. Database Syst. , vol. 42, no. 3, jul 2017. [Online]. Available:
https://doi.org/10.1145/3068335
[29] H. Abdi and L. J. Williams, ‚ÄúPrincipal component analysis,‚Äù Wiley
interdisciplinary reviews: computational statistics , vol. 2, no. 4, pp. 433‚Äì
459, 2010.
[30] S. Masi, P. Xu, P. Bonnifait, and S.-S. Ieng, ‚ÄúAugmented perception with
cooperative roadside vision systems for autonomous driving in complex
scenarios,‚Äù in 2021 IEEE International Intelligent Transportation Sys-
tems Conference (ITSC) . IEEE, 2021, pp. 1140‚Äì1146.
[31] J. Briales and J. Gonzalez-Jimenez, ‚ÄúConvex global 3d registration with
lagrangian duality,‚Äù in Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR) , July 2017.
[32] G. Jocher, et al. , ‚Äúultralytics/yolov5: v6.2 - YOLOv5 ClassiÔ¨Åcation
Models, Apple M1, Reproducibility, ClearML and Deci.ai integrations,‚Äù
Aug. 2022. [Online]. Available: https://doi.org/10.5281/zenodo.7002879
[33] F. Yu, et al. , ‚ÄúBDD100K: A diverse driving dataset for heterogeneous
multitask learning,‚Äù in Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition , 2020, pp. 2636‚Äì2645.