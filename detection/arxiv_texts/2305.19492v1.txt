CVSNet: A Computer Implementation for Central
Visual System of The Brain
Ruimin Gao
Department of Computer Science
and Engineering
University of Electornic Science
and Technology of China
202021081002@std.uestc.edu.cnHao Zou∗
Department of Computer Science
and Engineering
University of Electornic Science
and Technology of China
zouhao@uestc.edu.cn
Zhekai Duan
The University of Edinburgh
zhekaiduan2312@gmail.com
Abstract
In computer vision, different basic blocks are created around different matrix
operations, and models based on different basic blocks have achieved good results.
Good results achieved in vision tasks grants them rationality. However, these
experimental-based models also make deep learning long criticized for principle
and interpretability. Deep learning originated from the concept of neurons in
neuroscience, but recent designs detached natural neural networks except for some
simple concepts. In this paper, we build an artificial neural network, CVSNet, which
can be seen as a computer implementation for central visual system of the brain.
Each block in CVSNet represents the same vision information as that in brains.
In CVSNet, blocks differs from each other and visual information flows through
three independent pathways and five different blocks. Thus CVSNet is completely
different from the design of all previous models, in which basic blocks are repeated
to build model and information between channels is mixed at the outset. In ablation
experiment, we show the information extracted by blocks in CVSNet and compare
with previous networks, proving effectiveness and rationality of blocks in CVSNet
from experiment side. And in the experiment of object recognition, CVSNet
achieves comparable results to ConvNets, Vision Transformers and MLPs.
1 Introduction
In computer vision, three different kinds of networks get the most attention recent years, ConvNets,
Vision Transformers and MLPs. ConvNets, based on convolution operations, are the de-facto model
of computer vision in 2010s. Further for attention, Vision Transformers generates Q, K, V like in
NLP tasks to represent vision information. MLPs come into sight again from MLP-Mixer, which
is an architecture based mainly on multi-layer perceptrons. New blocks are created towards better
performance on certain dataset, good results achieved in vision tasks grants them rationality. However,
it’s hard to explain what is learned in each part and how information is processed in whole model,
which makes deep learning long criticized for principle and interpretability. Deep learning originated
from the concept of neurons in neuroscience, but artificial neural networks and natural neural networks
have hardly ever been closely related except for some simple concepts recent years.
∗Hao Zou is the corresponding author
Preprint. Under review.arXiv:2305.19492v1  [cs.CV]  31 May 2023Figure 1: A figure of visual information flow. The flow of visual information in neuroscience
on the left [ 1] and the flow of visual information in CVSNet on the right. At Inner Plexiform,
vision information is split into color-sensitive and color-insensitive parts. And at Outer Plexiform,
vision information further become three independent pathways.The three pathways perform different
functions in parallel until the Abstract Cognitive Layer synthesize their information to gain a high-
level abstract understanding.
Different from the design routine of previous networks, our work starts from neuroscience. To some
extent, our work can be seen as the reproduction of biological neural networks in artificial neural
networks. In the visual system of neuroscience, it is roughly divided into two parts with the entrance
of the cerebral cortex, striate cortex, as the boundary. The former part collects visual information,
analyzes and extracts the information. This part is called the central visual system, which is also The
area studied in this paper. The other part accepts the information of the previous part and processes
the information into various aspects that can meet the advanced functional needs of animals such as
movement and thinking. As it is not the main topics, details will not be shown in this article.
Contrast to networks constructed with repeated blocks, we designed interpretable blocks with a
clear vision information flow, and the whole model is implemented in accordance with central visual
system in Neuroscience. The same as visual information flows in neuroscience, the artificial network
consists of five different blocks and three independent pathways. Figure 1 compare similarities of the
visual information flow in neuroscience and in our networks. We directly quote terms in neuroscience
to name our blocks with inner plexiform, outer plexiform, lateral geniculate nucleus, striate cortex
and abstract cognitive layer. Abstract cognitive layer is introduced to satisfy certain vision tasks
replacing layers after the striate cortex. And the new designed network is named as CVSNet.
As the structure and function of each block is designed to mimic the neuroscience counterpart,
CVSNet is well interpretable and closely related to natural network in neuroscience. In the inner
plexiform, a center-peripheral design of contrast activation was used to represent the retina’s detection
to relative rather than absolute values of light intensity. And network is split into color-sensitive
and color-insensitive two parts, mimicking the workings of cones and rods, respectively. In outer
plexiform, color contrast is introduced into the center-peripheral design to learn information between
colors. In striate cortex, three pathways are formed to analyze the overall features of the image, the
local detail information of the image and the color information respectively.
Ablation experiments are designed to verify the information extracted by blocks. It has been proved
from both theory and experiment that each structure has indeed fulfilled the expected function. The
full network, CVSNet, is trained on on the ImageNet1K Dataset [ 2] to verify the visual processing
ability of the entire network. As result, CVSNet achieves comparable results to ConvNets, Vision
Transformers and MLPs.
Some existing work is trying to analyze artificial neural networks, and trying to find its connection
with neuroscience. However, a complete analysis and reproduction of a system in neuroscience like
CVSNet is unprecedented. This piece of work adds to the interpretability of deep learning and makes
2a breakthrough in bridging the gap between natural neural networks and artificial neural networks.
We believe it will bring new development to deep learning and artificial intelligence.
2 Related Work
ConvNets. AlexNet [ 3] proposed the first implementable deep network. ResNet [ 4;5] creatively
proposed residual connections to make gradient easier to backpropagation in deep neural networks.
Inception [ 6;7;8;9] constructed a deep neural network by concatenating multi-scale convolution,
while DenseNet [ 10] finds another path for gradient propagation by changing the way the model is
internally connected. EfficientNet [ 11;12] use both manual and neural architecture search to design
a new baseline network. ResNeXt [ 13] uses group convolution on ResNet in order to balance the
amount of computation and accuracy. ResNest [ 14] applies the channel-wise attention on different
network branches. ShuffleNet [ 15;16] proposes shuffle operation to cross the information between
convolution groups. MobileNet [ 17;18] uses depth separation convolution, and Dilated Residual
Networks [ 19] considered the way to increase the receptive field of convolution without obviously
increasing computation. ConvNext [ 20] discovers several key components that contribute to the
performance difference. RegNet [ 21] first designs network design spaces that parametrize populations
of networks.
Attention and Vision Transformers. Attention means put more weights on some part of the network.
SENet [ 22] proposes the “Squeezeand-Excitation” (SE) block that adaptively recalibrates channel-
wise feature responses. SKNet [ 23] contributes Selective Kernel in which multiple branches with
different kernel sizes are fused. CBAM [ 24] sequentially infers attention maps along two separate
dimensions, channel and spatial. NonLocal [ 25] operation computes the response at a position as
a weighted sum of the features at all positions. In fact, he calculation of NonLocal is very similar
to ViT. ViT [ 26] first introduced the Transformer structure into computer vision, and constructed
a pure transformer models. DeiT [ 27] produces convolution-free transformers and introduces a
teacher-student strategy specific to transformers. SwinViT [ 28] proposes a hierarchical Transformer
whose representation is computed with shifted windows. MobileViT [ 29;30] presents a different
perspective for the global processing of information with transformers, while Mobile-Former [ 31]
Bridges MobileNet and Transformer. Bottleneck Transformers [ 32] replaces the spatial convolutions
with global self-attention in the final three bottleneck blocks. MLP-Mixer [ 33] learns from ViT,
cutting the image into patches, then use patches first presents an architecture based exclusively on
multi-layer perceptrons. Hire-MLP [ 34] make a MLP architecture via Hierarchical rearrangement.
LeViT [35] have a nice bridge with ViT and MLP blocks.
Computational Neuroscience [36] analyzed the connection between computer vision and biological
vision. [ 37] present a recurrent network model of classical and extra-classical receptive fields
that is constrained by the anatomy and physiology of the visual cortex. [ 38] develop an approach
that simultaneously achieves both flexibility and tractability for machine learning. [ 39] attempt
to reconstruct the response properties of experimentally unobserved neurons in the interior of a
multilayered neural circuit, using cascaded linear-nonlinear (LN-LN) models. [ 40] outline how
the goal-driven HCNN approach can be used to delve even more deeply into understanding the
development and organization of sensory cortical processing. NasNet [ 41;42] use a recurrent
network to generate hyperparameters in neural network structures. This process is similar to the
reward and punishment mechanism of biological learning and memory.
3 The Design of CVSNet
The structures of blocks in CVSNet will be introduced in detail in this section. Each subsection is
divided into two parts. The actual structures of the brain will be introduced first, named neuroscience .
And our implementation will be presented next, titled implementation .
3.1 Inner Plexiform
Neuroscience In the collection of optical information, there are two different types of cells, cones
and rods. Cones are divided into three different types, each response most to one of red, green, and
blue light. All rods are not sensitive to the wavelength of light. For both Cones and rods, cells in the
center and periphery have opposite activation-inhibitory characteristics [1].
3Figure 2: The designed structure of Inner Plexiform Layer. In Inner Plexiform Layer, visual
information is divided into color-sensitive(RGB) and color-insensitive(grey) two parts. Each part
is composed of directly connected and indirectly connected cell layers with opposite non-linear
activation. Note that red, green and blue three channels would still be independent in this layer.
Implementation As depicted in Figure 2, in inner plexiform layer, visual information is divided into
color-sensitive and color-insensitive two parts, each of which is composed of directly connected and
indirectly connected cell layers.
In color-insensitive part, grey path comes from the sum of RGB channels, representing the rod cells.
In color-sensitive part, the depth-wise convolution is applied to keep the RGB channels separated
with each other, representing the cone cells.
Inner color-sensitive and color-insensitive part, model consists of two structure representing the
light-giving center and the light-extracting center. Light-giving center consists of positive central cells
and negative horizontal cells. Positive and negative means the symbol of activation function, while
central and horizontal represents the receptive field of cells. For positive central cells, a convolution of
small kernel with positive nonlinear function, like RuLE is used to represent ’positive’. For negative
horizontal cells, a convolution of big kernel with negative nonlinear function, like RuLE with a factor
−0.3. The light-extracting center is composed of negative central cells and positive horizontal cells,
which is similar to design of light-giving center.
3.2 Outer Plexiform
Neuroscience In outer plexiform layer, cells are divided into three types: one is large M-type ganglion
cells, accounting for 5% of the total ganglion cells; other is small P-type ganglion cells, accounting
for 90% of the total ganglion cells; the rest 5% were a non-M-non-P ganglion cell. In P-type cells,
the opposing colors are red and green; M cells are color-insensitive; non-M-non-P (abbreviated as
NMP below) processes blue-yellow information.
Implementation Three kinds of cells, M-type, P-type and NMP-type cells formed in outer plexform,
corresponding to M-type ganglion cells, P-type ganglion cells and NMP-type ganglion cells in the
brain. The connections of M-type, P-type and NMP-type cells are as depicted in Figure 3. In this
section, R,G,B are used to represent color red,green,blue, and symbols +,- after color represent
positive and negative.
For example, a R+G- P-type cell has a red activation center and a green inhibitory periphery. R+ make
a small-kernel convolution, positive activation on the R channel. G- make a big-kernel convolution
and negative activation on the G channel. P-type cells with R-G+ is analogous to the R+G-.
As in Figure 3, NMP-type cell comes from calculates of B and Y . Y represents yellow, which is
obtained by summing the corresponding R channel and G channel. The rest operations to Y and B
channels are similarly to P-type cell.
4Figure 3: The designed structure of Outer Plexiform Layer. On the left of the figure, R+ means red
channel with positive activation while R- means red channel with negative activation. The similar as
G+,G-,B+,B-,Y+ and Y-, Y is short for yellow. On the right of the figure, visual information flows is
similar to grey path in Inner Plexiform Layer.
Different from P-type cells and NMP-type cells, M-type cells are constructed somehow like rods. As
in Figure 3, model split into two parts to represent the light-giving center and the light-extracting
center. As M-type cells is set to have bigger receptive field, big-kernel convolution is applied.
In order to reflect the numerical ratio of the center and surrounding areas of the light and light
withdrawal, inner plexiform layer and out plexiform layer do not use any normalization layers like
Batch Normalization [7] or Layer Normalization [43].
3.3 Lateral Geniculate Nucleus
Neuroscience The LGN (short for lateral geniculate nucleus) consists of six layers of cells with
input from both eyes. For simplicity, only three cell layers that process monocular information are
considered in this paper. One of the layers contains large neurons, called the magnocellular LGN
layer, that receives all projections from retinal M-type ganglion cells. Two layers contain small
cells, termed the parvocellular LGN layer, that receive all projections from P-type ganglion cells.
There is also a large number of tiny neurons located on the ventral side of each layer, known as the
koniocellular layer, that receives input from retinal non-M-non-P-type ganglion cells [1].
Implementation The LGN’s function is to integrate and "expand" the visual signal input from the
retina to the striate cortex. The three parallel pathways are named as M (short for magnocellular), P
(short for parvocellular), and K (short for koniocellular) in LGN layer.
M cells’ receptive field is larger than P and K, so M uses a large convolution kernel with channels
expanding to 2mtimes.
Convolution with a group of 2 is applied to P and channels are shuffled before convolution. P uses a
small convolution kernel to represent the smaller cells and receptive fields. The channels of P are
expanded to 2ptimes.
K is divided into two categories, color-sensitive and color-insensitive. For the color-sensitive,
convolutions with groups equal to input channels are used. For the color-insensitive, convolutions are
without using groups. K also applies small kernel size with expanding channels to 2ktimes.
3.4 Striate Cortex
Neuroscience There are actually at least nine distinct layers of neurons in striate cortex, which are
named by Roman numerals VI, V , IV A, IVB, IVC α, IVC β, III, II and I. There are three pathways
that perform different functions in parallel. These can be called the magnocellular pathway, the
parvo-interblob pathway, and the blob pathway.
5The magnocellular pathway begins with M-type ganglion cells of the retina. It starts axons to the
magnocellular layers of the LGN. These layers project to layer IVC αof striate cortex, which in turn
projects to layer IVB. Many of these cortical neurons are direction selective.
The parvo-interblob pathway originates with P-type ganglion cells of the retina, which project to
the parvocellular layers of the LGN. The parvocellular LGN sends axons to layer IVC βof striate
cortex, which projects to layer II and III interblob regions. Neurons in this pathway have small
orientation-selective receptive fields.
The blob pathway receives input from the subset of ganglion cells that are neither M-type cells nor
P-type cells. These nonM–nonP cells project to the koniocellular layers of the LGN. The koniocellular
LGN projects directly to the cytochrome oxidase blobs in layers II and III. Many neurons in the blobs
are color selective [1].
Implementation Orientation selectivity and direction selectivity are two important properties that
need to be characterized in this layer. Orientation selectivity means that the stimulus along a certain
strip direction has the largest activation. The implementation of orientation selectivity is relatively
simple, as the convolution with filter size as 1 ×n or n ×1 would perform well. Direction selectivity
refers to the cells responding to only one of them when stimulated in opposite directions. For example,
a cell responds to an elongated stimulus swept rightward across the receptive field, but much less
with leftward movement will be thought to be direction selective. Direction selectivity lays the
groundwork for the network to recognize asymmetric objects.
We creatively use difference map to characterize direction selectivity. Apply difference map to matrix
generates difference matrix . The transform on elements of the matrix are depicted as follow.
For a matrix AM×N(call as Abelow), we conventionally name each element of the matrix a11,. . . ,
aMN. The detection of a rightward movement is as shown in Equation (1) and Equation (2):
aij=ai(j+N−k)%N,foriin1toM,jin1toN (1)
A new matrix A∗is obtained after transposition depicted in Equation (1), in which k is any positive
integer, generally set as 1. The difference matrix dAis obtained with A∗−Aas shown in Equation (2).
aij=ai((j+k)%N)−aij,foriin1toM,jin1toN (2)
We call such a transformation a difference map . Similarly, leftward movement can be expressed as in
Equation (3).
aij=ai((j+N−k)%N)−aij,foriin1toM,jin1toN (3)
The vertical movement is represented as in Equation (4).
aij=a((i+M±k)%M)j−aij,foriin1toM,jin1toN (4)
Oblique movements are represented as in Equation (5).
aij=a((i+M±k)%M)((j+N±k)%N)−aij,foriin1toM,jin1toN (5)
As depicted in Figure 4, The structure of striate cortex layer consists of 3 pathways, and output from
6 components, corresponding to the output of 3 pathways and 6 layers in neuroscience.
The M path is specialized to analyze the overall features of the image. The input of the M path comes
from the upper M. M path consists of convolution with big filter size, the orientation information
process, and the direction information process. The processing of direction information in M path
includes more detailed direction branches, which is called complex direction.
The P-IB path is specialized to analyze the local detail information of the image. The input of the P-IB
path comes from the upper P. P-IB path consists of convolution with small filter size, the orientation
information process, and the direction information process. The number of direction branches in
P-IB path is less than that in M path, called simple direction.
The Blob path is specialized to analyze the color information. The input of Blob path comes from the
concatenation of the convolution output of M, P, K. The Blob path performs simple direction and
convolution operations on the input.
6Figure 4: The designed structure of striate cortex layer. This layer consists of three pathways and six
outputs, and three different kinds of color is applied to represent information flows in three pathways.
As shown in this figure, M-out, P-out and K-out each contributes two outputs of final six outputs of
striate cortex.
3.5 Abstract Cognitive Layer
Neuroscience Beyond striate cortex lie another two-dozen distinct area of cortex that have unique
receptive field properties. These cortical areas have complex functions. They abstract and process
various visual information required by people’s daily behavior. We use abstract cognitive layer to
represent them. Rather than fulfil a block to reproduce complex human behavior, this article aims at
object recognition task. So abstract cognitive layer is simplified applied for recognition task.
Implementation We use a two-step MLP to mix within and between channels, and abstract informa-
tion representing local-global information. We first use the MLP-mixer [ 33] module that does not
change the number of each channel to mix the internal information of the channel. Then the number
of channels for each channel is then halved using an MLP layer and concatenated into a total channel.
After extracting abstract information from each channel, the second step is to integrate and analyze
the association between channels. This abstract analysis process is also implemented using the
MLP-mixer module for the total channel.
4 Object Recognition Experiment
4.1 Settings
We use the ILSVRC-2012 ImageNet dataset [ 33] [41], which has 1.3M images with 1k classes. All
test results are without pre-training. Data augmentation techniques such as Mixup [ 44], Cutmix [ 45],
RandAugment [ 46], Random Erasing [ 47], and regularization schemes including Label Smoothing
are applied to the training. The epochs are set as 300, and we employ an AdamW optimizer [ 48] for
300 epochs using a cosine decay learning rate scheduler and 20 epochs of linear warm-up. A weight
decay of 0.05 are used.
4.2 Results
We simply stack the structure described above in sequence to form the original CVSNet, which we
named as CVSNet-1. We then simplify the model for the needs of classification tasks, thus achieving
a network with lower complexity than CVSNet-1, but higher classification accuracy on ImageNet1K,
which is named CVSNet-2.
As shown in Table 1, compared to Vision Transformer L/16, MLP-Mixer L/16, CVSNet-1 have less
parameters and FLOPs, while achieving a better top1 and top5 accuracy. +1.87%/2.1% for CVSNet-
7model image size #param FLOPs top1 top5
MLP-Mixer L/16 224*224 44.58G 208.2M 71.76 87.89
ViT L/16 224*224 59.67G 304.25M 74.63 90.97
CVSNet-1 224*224 6.9G 109.36M 75.50 92.07
MLP-Mixer B/32 224*224 44.58G 208.2M 73.63 89.97
MLP-Mixer B/16 224*224 59.67G 304.25M 76.44 91.11
ViT B/32 224*224 4.37G 88.2M 71.64 87.12
CVSNet-2 224*224 4.84G 46.50M 76.04 92.62
Table 1: Compare CVSNet with other networks
Figure 5: Contrast of the ability to capture relative intensity of light. The top line of the figure is
a group of pictures with different brightness. The middle line is the output of inner plexiform of
CVSNet. We show both positive center and negative center of CVSNet in the middle two lines. The
bottom line is the output of corresponding block of ResNet.
1(75.50%/92.07%) over ViT (73.63%/89.97%), and +1.5%/1.4% for CVSNet-1 over MLP-Mixer.
Compared with MLP and ViT with same level of complexity, CVSNet-2 achieves a better accuracy.
Compared with the ViT and MLP-Mixer, CVSNet achieves comparable accuracy. Such experimental
results are sufficient to demonstrate that the constructed modules indeed have comparable visual
representation capabilities.
5 Ablation Experiment
In object recognition experiment, we demonstrate that full network, CVSNet, indeed have the ability
to deal with certain computer vision tasks. In this section, ablation experiments are designed to
verify the information extracted by each module and compared with previous networks. It proved
that artificial structures implemented with Deep learning technology do have the ability to provide
corresponding functions in neuroscience. More experiments of outputs of three pathways could be
found in Appendix A.
5.1 Detection of Relative Intensity of Light
In the inner plexiform, a center-peripheral design of contrast activation was used to represent the
retina’s detection to relative rather than absolute values of light intensity. We create multiple pictures
by changing the brightness of a picture. Then they are fed to CVSNet trained on ImageNet1K to get
the output of the inner plexiform layer; similarly, we also get the output of the corresponding block in
ResNet.
As shown in Figure 5, the output of inner plexiform of CVSNet changes slightly across different
intensity of light. It proves inner plexiform of CVSNet indeed have the ability to capture relative
8Figure 6: CVSNet’s changes when color changes. The top line of the figure is a group of pictures
with different color painted. The middle line is the output of outer plexiform of CVSNet. We show
react of 4 different kind cells in the outer plexiform. The bottom line is the output of corresponding
block of ResNet.
intensity of light. As a contrast, ResNet have significant changes on feature map, which shows
different detection pattern of previous networks compared with CVSNet.
5.2 Color Contrast
In outer plexiform, color contrast is introduced into the center-peripheral design to learn information
between colors. In this part, we paint the same photo in different colors. Then feed these pictures
painted with different colors to the network and observe the changes in the extracted features.
As shown in Figure 6, CVSNet’s color changes are very regular compared with ResNet’s irregular
changes. As long as we analyze the operation process of the two networks, we can know that such a
result is obvious. Because a network like ResNet performs a similar weighted sum operation on the
features of the three color channels, while CVSNet calculates the difference between two colors in a
certain area. So this experiment is in fact a confirmatory experiment.
Furthermore, in experiments, we unexpectedly found a place that echoes neuroscience theory. The
red-green channel contrast can better reflect the boundary and outline of the object and almost
constant in the change, while the blue-yellow channel contrast is more sensitive to color information.
This corresponds to the P-Path at the striate cortex, which mainly accepts the red and green channel of
front layer, is responsible for analyzing the details of the object, and the K-Path channel, which mainly
receives the red and green channel of front layer, is responsible for analyzing the color information of
the object.
6 Conclusion
We present a novel neural network, CVSNet, which implements counterpart in computer science of
central vision system in neuroscience. CVSNet use meaningful blocks which differs from previous
work repeating basic blocks to build models. Moreover, CVSNet is a new attempt at interdisciplinary
neuroscience and computer science, beyond the approaches of just getting precision on a certain
9dataset. So this work is also a preliminary proof that the neural network vision information processing
progress in computer is indeed quite similar to that in the brain.
In the current artificial intelligence, varieties model structures are proposed to deal with varieties
vision tasks. But in neuroscience, human process all visual information with a set of visual information
processing baselines. CVSNet, base on structure of central vision system, is worth looking forward
to its performance on other vision tasks. And we will continue to promote relevant research. Beyond
computer vision, similar work remains to be done in NLP and other representation tasks. We believe
that one day, creating highly intelligent neural network machines will come true.
References
[1]Mark Bear, Barry Connors, and Michael A Paradiso. Neuroscience: Exploring the Brain, Enhanced
Edition: Exploring the Brain . Jones & Bartlett Learning, 2020.
[2]Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,
Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge.
International journal of computer vision , 115(3):211–252, 2015.
[3]Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional
neural networks. Communications of the ACM , 60(6):84–90, 2017.
[4]Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.
InProceedings of the IEEE conference on computer vision and pattern recognition , pages 770–778, 2016.
[5]Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks.
InEuropean conference on computer vision , pages 630–645. Springer, 2016.
[6]Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru
Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of
the IEEE conference on computer vision and pattern recognition , pages 1–9, 2015.
[7]Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing
internal covariate shift. In International conference on machine learning , pages 448–456. PMLR, 2015.
[8]Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the
inception architecture for computer vision. In Proceedings of the IEEE conference on computer vision and
pattern recognition , pages 2818–2826, 2016.
[9]Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander A Alemi. Inception-v4, inception-
resnet and the impact of residual connections on learning. In Thirty-first AAAI conference on artificial
intelligence , 2017.
[10] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected
convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition ,
pages 4700–4708, 2017.
[11] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In
International conference on machine learning , pages 6105–6114. PMLR, 2019.
[12] Mingxing Tan and Quoc Le. Efficientnetv2: Smaller models and faster training. In International Conference
on Machine Learning , pages 10096–10106. PMLR, 2021.
[13] Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, and Kaiming He. Aggregated residual transforma-
tions for deep neural networks. In Proceedings of the IEEE conference on computer vision and pattern
recognition , pages 1492–1500, 2017.
[14] Hang Zhang, Chongruo Wu, Zhongyue Zhang, Yi Zhu, Haibin Lin, Zhi Zhang, Yue Sun, Tong He,
Jonas Mueller, R Manmatha, et al. Resnest: Split-attention networks. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition , pages 2736–2746, 2022.
[15] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun. Shufflenet: An extremely efficient convolutional
neural network for mobile devices. In Proceedings of the IEEE conference on computer vision and pattern
recognition , pages 6848–6856, 2018.
[16] Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun. Shufflenet v2: Practical guidelines for
efficient cnn architecture design. In Proceedings of the European conference on computer vision (ECCV) ,
pages 116–131, 2018.
10[17] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco
Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision
applications. arXiv preprint arXiv:1704.04861 , 2017.
[18] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2:
Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on computer vision and
pattern recognition , pages 4510–4520, 2018.
[19] Fisher Yu, Vladlen Koltun, and Thomas Funkhouser. Dilated residual networks. In Proceedings of the
IEEE conference on computer vision and pattern recognition , pages 472–480, 2017.
[20] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A
convnet for the 2020s. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 11976–11986, 2022.
[21] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr Dollár. Designing network
design spaces. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition ,
pages 10428–10436, 2020.
[22] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In Proceedings of the IEEE conference
on computer vision and pattern recognition , pages 7132–7141, 2018.
[23] Xiang Li, Wenhai Wang, Xiaolin Hu, and Jian Yang. Selective kernel networks. In Proceedings of the
IEEE/CVF conference on computer vision and pattern recognition , pages 510–519, 2019.
[24] Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In So Kweon. Cbam: Convolutional block attention
module. In Proceedings of the European conference on computer vision (ECCV) , pages 3–19, 2018.
[25] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In
Proceedings of the IEEE conference on computer vision and pattern recognition , pages 7794–7803, 2018.
[26] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth
16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929 , 2020.
[27] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Hervé Jégou.
Training data-efficient image transformers & distillation through attention. In International Conference on
Machine Learning , pages 10347–10357. PMLR, 2021.
[28] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin
transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , pages 10012–10022, 2021.
[29] Sachin Mehta and Mohammad Rastegari. Mobilevit: light-weight, general-purpose, and mobile-friendly
vision transformer. arXiv preprint arXiv:2110.02178 , 2021.
[30] Sachin Mehta and Mohammad Rastegari. Separable self-attention for mobile vision transformers. arXiv
preprint arXiv:2206.02680 , 2022.
[31] Yinpeng Chen, Xiyang Dai, Dongdong Chen, Mengchen Liu, Xiaoyi Dong, Lu Yuan, and Zicheng Liu.
Mobile-former: Bridging mobilenet and transformer. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , pages 5270–5279, 2022.
[32] Aravind Srinivas, Tsung-Yi Lin, Niki Parmar, Jonathon Shlens, Pieter Abbeel, and Ashish Vaswani.
Bottleneck transformers for visual recognition. In Proceedings of the IEEE/CVF conference on computer
vision and pattern recognition , pages 16519–16529, 2021.
[33] Ilya O Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner,
Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, et al. Mlp-mixer: An all-mlp architecture
for vision. Advances in Neural Information Processing Systems , 34:24261–24272, 2021.
[34] Jianyuan Guo, Yehui Tang, Kai Han, Xinghao Chen, Han Wu, Chao Xu, Chang Xu, and Yunhe Wang.
Hire-mlp: Vision mlp via hierarchical rearrangement. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , pages 826–836, 2022.
[35] Benjamin Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin, Hervé Jégou, and
Matthijs Douze. Levit: a vision transformer in convnet’s clothing for faster inference. In Proceedings of
the IEEE/CVF international conference on computer vision , pages 12259–12269, 2021.
11[36] Gabriel Kreiman. Biological and Computer Vision . Cambridge University Press, 2021.
[37] David A Mély and Thomas Serre. Opponent surrounds explain diversity of contextual phenomena across
visual modalities. bioRxiv , page 070821, 2016.
[38] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised
learning using nonequilibrium thermodynamics. In International Conference on Machine Learning , pages
2256–2265. PMLR, 2015.
[39] Niru Maheswaranathan, David B Kastner, Stephen A Baccus, and Surya Ganguli. Inferring hidden structure
in multilayered neural circuits. PLoS computational biology , 14(8):e1006291, 2018.
[40] Daniel LK Yamins and James J DiCarlo. Using goal-driven deep learning models to understand sensory
cortex. Nature neuroscience , 19(3):356–365, 2016.
[41] Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. arXiv preprint
arXiv:1611.01578 , 2016.
[42] Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V Le. Learning transferable architectures
for scalable image recognition. In Proceedings of the IEEE conference on computer vision and pattern
recognition , pages 8697–8710, 2018.
[43] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint
arXiv:1607.06450 , 2016.
[44] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk
minimization. arXiv preprint arXiv:1710.09412 , 2017.
[45] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo.
Cutmix: Regularization strategy to train strong classifiers with localizable features. In Proceedings of the
IEEE/CVF international conference on computer vision , pages 6023–6032, 2019.
[46] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data
augmentation with a reduced search space. In Proceedings of the IEEE/CVF conference on computer
vision and pattern recognition workshops , pages 702–703, 2020.
[47] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang. Random erasing data augmentation.
InProceedings of the AAAI conference on artificial intelligence , volume 34, pages 13001–13008, 2020.
[48] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint
arXiv:1711.05101 , 2017.
Appendix A
Outputs of M, K, P Pathways
In striate cortex, three pathways are formed to analyze the overall features of the image, the local
detail information of the image and the color information respectively. The P-Path at the striate cortex
mainly accepts the red and green channel of the outer plexiform layer, the K-Path channel, which
mainly receives blue and yellow channel of the outer plexiform layer, and the M-Path receives grey
channel of the the outer plexiform layer. So as depicted in the experiments before, the three path
indeed deal with three aspect of the vision information.
However, in order to satisfy the special curiosity of some readers, we also visualized the feature
information extracted from the changes of the three pathways M, K, and P. The picture in the
darkening experiment is from Figure 5, and the picture in the color change experiment is from
Figure 6.
As shown in Figure 7, in the experiment of reducing the overall brightness of the picture, M Path has
the smallest change, followed by P Path, and K Path has the largest change. This is consistent with
the visual effect the picture gives us, that is, when the picture is darkened, the division of things in the
whole picture can still be barely recognized, but it is difficult to observe the details of the shape and
texture of things, and the color information is basically unobtainable. In the color change experiment,
the M Path is basically not affected, the P Path part is greatly affected, the other part is less affected,
and the K Path is greatly affected.
12Figure 7: Outputs of M, K, P Pathways. The top line of features are computed from the original
picture. The middle line of features are calculated from the darken picture. And the bottom line of
features are extracted from the picture where color changes.
13