Regularized Weight Aggregation in Networked
Federated Learning for Glioblastoma
Segmentation
Muhammad Irfan Khan1, Mohammad Ayyaz Azeem2, Esa Alhoniemi1, Elina
Kontio1, Suleiman A. Khan1, and Mojtaba Jafaritadi1
1Turku University of Applied Sciences, Turku 20520, Finland
2Riphah International University, Islamabad 45210, Pakistan
irfan.khan@turkuamk.fi,32175@student.riphah.edu.pk,
mojtaba.jafaritadi,esa.alhoniemi,elina.kontio,
suleiman.alikhan@turkuamk.fi
Abstract. In federated learning (FL), the global model at the server
requires an ecient mechanism for weight aggregation and a systematic
strategy for collaboration selection to manage and optimize communica-
tion payload. We introduce a practical and cost-ecient method for regu-
larized weight aggregation and propose a laborsaving technique to select
collaborators per round. We illustrate the performance of our method,
regularized similarity weight aggregation (RegSimAgg), on the Federated
Tumor Segmentation (FeTS) 2022 challenge's federated training (weight
aggregation) problem. Our scalable approach is principled, frugal, and
suitable for heterogeneous non-IID collaborators. Using FeTS2021 eval-
uation criterion, our proposed algorithm RegSimAgg stands at 3rd po-
sition in the nal rankings of FeTS2022 challenge in the weight ag-
gregation task. Our solution is open sourced at: https://github.com/
dskhanirfan/FeTS2022
Keywords: Brain Tumors ¬∑Cancer ¬∑Collaborative Learning ¬∑Federated
Learning ¬∑FeTS Challenge ¬∑Lesion Segmentation ¬∑Weight Aggregation
1 Introduction
Federated learning (FL) is on the horizon to replace the current paradigm of data
sharing, allowing for privacy-preserving cross-institutional research including a
wide range of biomedical disciplines. In simple terms, FL is a machine learning
paradigm in a distributed or decentralized setting. It is particularly favorable
because pooling all the curated data from dierent data silos to a central location
is arduous, for training machine learning models. Moreover, sharing sensitive
data is becoming increasingly dicult due to data privacy and security concerns,
bureaucratic challenges and stringent GDPR (EU) and HIPAA (US) laws [1,20].
The implementation of FL in practice requires several distinct clients called
collaborators to contribute to the creation of a global expert model via a denedarXiv:2301.12617v1  [cs.LG]  30 Jan 20232 Khan et al., 2022
server called an aggregator . Each collaborator provides some of the information
that the aggregator would combine [14]. The aggregator does not have access to
the collaborator's private data, which does not egress from the collaborator. The
actual training tasks are executed at the collaborators, because each collaborator
has a chunk of the whole data. During federation rounds, participating collab-
orators cartel the parameters combined at the server for a unied consensus
model to foster knowledge exchange. This global model is assembled by learned
information, in terms of parameters, from a conglomerate of individual partici-
pating collaborators that execute the learning task independently. Hence, model
training is essentially performed in a distributed fashion on the data hoarded
at distinct edge devices. After model training, the generalizable global model
or model parameters are dispatched to the collaborators [7]. Consequently, in a
federation process, all the participating collaborators get potential benet from
the global model at the aggregator, because they receive the learned model pa-
rameters collected from trained models on other collaborators as well. Moreover,
the performance and inference of global model, trained on several collaborators,
on unseen data is better as compared to the model trained on any individual
collaborator data. Figure 1 shows a high-level schema of the federated learning
framework.
CollaboratorsLocal Model transfer, t+1
AggregationFederated  Server
Global Model transfer, tGlobal Modela1a2a3‚Ä¶a1a2a3‚Ä¶a1a2a3‚Ä¶a1a2a3‚Ä¶Learnableaggregationweightsùë§‚Üê#!ùëé!ùë§!a1 a2 a3a1 a2 a3a1 a2 a3Time (t)Aggregator and collaborator handler
Fig. 1. General workow of an FL-trained model and the key components in a feder-
ated learning setting [23]. Private collaborators communicate the local weight updates
with a federated secure server at regularly occurring intervals to learn a global model;
the server aggregates the updates and sends back the parameters of the updated global
model to the clients. The aggregation weights are learned on the clients and dynami-
cally adjusted throughout the training process when communicating with the federated
server.Federated Regularized Weight Aggregation 3
The utility spectrum of federated learning is quite broad with wide-range
applications in telecommunication services, ntech, and healthcare sector. In
healthcare, data analytics in radiology and genomics can particularly welfare
from harnessing the power of FL ecosystem. A direct clinical impact of FL
therefore could be made in the most-eective translation medicine, e.g. privacy-
preserving medical image segmentation [11,19]. However, training a shared gen-
eralizable global model for medical image segmentation in a federated fashion
presents a number of challenges and limitations. These include training local
models using dierent imaging modalities on many scanners from dierent man-
ufacturers, acquisition times, and various image resolutions and protocols. The
improper use of such training data, referred to as not independent and identi-
cally distributed (non-IID), thus can result in performance degradation among
dierent clients. FL has been combined with domain adaptation [25], contrastive
learning [9], and knowledge distillation [13] in order to learn a more generaliz-
able federated model. Other limitations include cross domain and imbalance
of annotated data (limited labeling budgets) [6]. The challenge of data hetero-
geneity and domain shifting was recently tackled in novel ways by, for example,
federated disentanglement learning via disentangling the parameter space into
shape and appearance [6] and automated federated averaging based on Dirich-
let distribution [22]. Dynamic Re-Weighting mechanisms [12], federated cross
ensemble learning [24], and label-agnostic (mixed labels) unied FL formed by
a mixture of the client distributions [21] have been recently proposed to relax
an unrealistic assumption that each client's training set will be annotated simi-
larly and therefore follows the same image supervision level during the training
of an image segmentation model. Although extensive research has been carried
out on FL, there is still a need for methods to enable the development of more
generalized FL models for clinical use which can eectively deal with statistical
heterogeneity in weight aggregation, communication eciency, and privacy with
security.
In this paper, we aim to establish an adaptive regularized weight aggregation
by upgrading our previously developed similarity weight aggregation (SimAgg)
algorithm [8]. We propose a robust and ecient federated lesion segmentation
algorithm applicable in generalized and realistic detection of the \rare" disease
of glioblastoma, a form of brain cancerous tumor, and particularly on the delin-
eation of its sub-regions by leveraging multi-modal magnetic resonance imaging
(MRI) brain scans [17]. We present an extensive evaluation of the proposed
regularized similarity weight aggregation (RegSimAgg) strategy in a networked
federated learning fashion. In the light of our previous research in FeTS2021,
we propose an ecient yet simple method for addressing FeTS2022 and existing
challenges of FL.
The rest of this paper is organized as follows: in Section 2, we describe the
upgraded methodologies to the previous SimAgg algorithm including collabo-
rator selection and weight aggregation regularization through our experiment
setting. In Section 3, we describe FL experiments and evaluate the performance
of the proposed method quantitatively and in Section 4, we discuss about the4 Khan et al., 2022
presented work, potentials and limitations, and describe our future direction in
FL. Finally, Section 5 concludes this work.
2 Methods
2.1 FeTS 2022 challenge
Federated Tumor Segmentation (FeTS) 2022 is a continuation to the previ-
ous FeTS2021 [17] challenge with the focus on federated training methodolo-
gies including weight aggregation, client selection, training per-round, compres-
sion, communication eciency, and algorithmic generalizability on out-of-sample
data. It is intended to build and evaluate a consensus model that eectively iden-
ties intrinsically heterogeneous brain tumors.
The FeTS 2022 challenge provides updated multi-institutional multi-parametric
Magnetic Resonance Imaging (mpMRI) scans of glioblastoma (GBM), the most
common primary brain tumor, prior to any kind of resection surgery. The datasets
used in the FeTS 2022 challenge are the subset of GBM cases from the Brain
Tumor Segmentation (BraTS) Continuous Challenge which aims at identifying
state-of-the-art segmentation algorithms for brain diuse glioma patients and
their sub-regions [2,3,4,5,15].
All FeTS brain MRI scans, provided as NIfTI les ( .nii.gz ), had four struc-
tural MRI sequences including native (T1), post-contrast T1-weighted (T1Gd),
T2-weighted (T2), and T2 FLuid Attenuated Inversion Recovery (FLAIR) vol-
umes. Image samples were acquired with dierent clinical protocols and various
scanners from multiple data contributing institutions. One to four raters an-
notated each of the images manually, following a standardized protocol, and
their annotations were approved by neuroradiologists. Annotations comprise the
pathologically conrmed segmentation labels with similar volume size of 240 
240155 including the GD-enhancing tumor (ET | label 4), the peritumoral
edematous/invaded tissue (ED | label 2), and the necrotic tumor core (NCR
| label 1). All these provided MRI scans including the ground truth data were
pre-processed such as rigid registration, brain extraction, alignment, 1 11 mm
resolution resampling, and skull stripping were applied as described in [15].
The training and validation datasets include 1251 and 219 subjects, respec-
tively. The training set consists of two partitions each providing information for
how to split the training data into non-IID institutional subsets. That is, each
patient dataset is linked to a de-identied partitioning label according to the
acquiring institutions.
We deployed Intel Federated Learning (OpenFL) [18] framework for train-
ing brain tumor segmentation model | an encoder-decoder 3D U-shape type
of convolutional neural network provided by FeTS2022 challenge | using the
data-private collaborative learning paradigm of FL [16]. OpenFL considers two
main components: 1) the collaborator which uses a local dataset to train the
global model and 2) the aggregator which receives model updates from each
collaborator and fuses them to form the global model.Federated Regularized Weight Aggregation 5
2.2 Regularized Similarity Weighted Aggregation (RegSimAgg)
In FeTS 2021 challenge, we suggested a novel aggregation method named Similar-
ity Weighted Aggregation (SimAgg) for ecient aggregation of model parameters
at the server that is suitable for both IID as well as non-IID data [8]. Here, we
propose an extension of SimAgg method which contains a regularization mech-
anism to speed up convergence, which is a critical issue in the computationally
demanding federated learning framework.
Collaborator Selection. We allow collaborators to contribute in a nonde-
terministic fashion by picking up a subset of the available collaborators (for
example, 20%) at each round. To ensure that the model sees all collaborators
the same number of times at regular intervals, we use a sliding window over the
randomized collaborator index as shown in Fig. 2; once all collaborators have
participated in the updates, the order is randomly shued. In this manner, we
ensure roughly equal participation of all collaborators. However, a particular
combination of the collaborators selected in one FL round will not be repeated
in the successive rounds.
a)Original collaborator list
1234567891011121314151617181920212223
b)Shuffled collaborator list
3861015137222201694191712518121111423
c)Selected collaborators
38610151372222016941917125181211114233
Fig. 2. Strategy for choosing collaborators. In a) Original collaborator list is given to
the model, in b) collaborator order is randomized, and in c) collaborators are selected
for each round using a sliding window. The collaborator list is reshued once it has
been fully utilized, and the process repeats step b). The collaborators chosen in a
certain combination during one FL round, however, will not be chosen again during
subsequent rounds.
Weight Aggregation. The fact that model parameters from the collaborators
can dier is a major concern with non-IID data. We employ weighted aggregation6 Khan et al., 2022
of the collaborators at the server to overcome such a scenario. The collaborators'
weights are determined by measuring similarity of the collaborators to their non-
weighted average. We have also learned that, from the convergence point of view,
it is benecial to add regularization to the weighting process after the initial FL
rounds. Our aggregation algorithm is compactly described in Algorithm 1, and
the steps are explained in detail below.
At roundr, the parameters pCrof the participating collaborators Crare sent
to the server. At the server, the average of these parameters is calculated as:
ÀÜp=1
jCrj/uniEF23/uniEBE92Crp/uniEBE9. (1)
We then calculate the inverse distance (similarity) of each collaborator c2Cr
from the average:
s/uniEBE9mc=/uniEF23/uniEBE92Crjp/uniEBE9 ÀÜpj
jpc ÀÜpj+Œµ, (2)
whereŒµ=1e 5(small positive constant). We normalize the distances to
obtain similarity weights as follows:
/uniEBF5c=s/uniEBE9mc
/uniEF23/uniEBE92Crs/uniEBE9m/uniEBE9. (3)
The collaborators close to the average receive a high similarity weight and
vice versa. In the extreme case this approach can expel the diverging collabora-
tor.
In order to adjust for the eect of varying number of samples at each col-
laboratorc2Cr, we use sample size weights that favor collaborators with large
sample sizes:
/uniEBF6c=Nc
/uniEF23/uniEBE92CrN/uniEBE9, (4)
whereNcis the number of examples at collaborator c.
Using the weights obtained using Eqs. 3 and 4, the aggregation weights are
computed as:
/uniEBF7c=/uniEBF5c+/uniEBF6c
/uniEF23/uniEBE92Cr(/uniEBF5/uniEBE9+/uniEBF6/uniEBE9), (5)
If we have run enough iterations (e.g., r>10), we also regularize the aggre-
gation weights:
/uniEBF7c=/uniEBF7c
1
jCrjP
/uniEBE92Cr¬Ä
pprev
/uniEBE9 p/uniEBE9¬ä. (6)
Compared to the SimAgg method proposed earlier by us in [8], this is the
only but remarkable change in our algorithm. The basic idea is to let the FL sys-
tem learn fast during the initial FL rounds after which the regularization makesFederated Regularized Weight Aggregation 7
learning somewhat slower but stable by suppressing signicant weight adjust-
ments. The FL round iteration limit (here 10) for starting the regularization is a
hyperparameter of the FL aggregator, and based on our experience it is sensible
to set it to a relatively small value.
The parameters are nally aggregated as a weighted average using the ag-
gregation weights:
pm=1
jCrj/uniEF23/uniEBE92Cr(/uniEBF7/uniEBE9p/uniEBE9). (7)
The normalized aggregated parameters pmare dispatched to the next set of
collaborators in the successive federation rounds.
Algorithm 1 RegSimAgg aggregation algorithm
1:procedure Regularized Similarity Weighted Aggregation (Cr,pCr,pprev
Cr)
2:Œµ 1e 5 .Cr= set of collaborators (at round r)
3:ÀÜp= average(pCr) using Eq. 1 .pCr= parameters of the collaborators in Cr
4: forcinCrdo
5: Compute similarity weights /uniEBF5cusing Eqs. 2 and3
6: Compute sample weights /uniEBF6cusing Eq. 4
7: forcinCrdo
8: Compute aggregation weights /uniEBF7cusing Eq. 5
9: ifr>10then
10: Regularize the aggregation weights /uniEBF7cusing Eq. 6
11: Compute master model parameters pmusing Eq. 7
12: returnpm
3 Experiments
3.1 Setup
Task 1 focuses on ecient aggregation, client selection, training-per-round, and
communication eciency in order to optimize the federation process. We have
devised a mechanism for aggregating model updates trained on individual collab-
orators that is both ecient and eective. A training data set with total of 1251
multi-institutional patients and 219 validation data set was available. Supple-
mentary data shows how patients are divided into distinct partitions. Partition
1 has 23 contributors, whereas partition 2 has 33 collaborators. For the semantic
segmentation of the total tumor, tumor core, and enhancing tumor, the exper-
imental setup leverages Intel's OpenFL platform for federated learning and a
precongured 3D U-shape neural network. Binary DICE similarity (total tumor,
enhancing tumor, tumor core) and Hausdor (95 percent) distance are the met-
rics computed in the aggregation rounds (whole tumor, enhancing tumor, tumor
core) as described in [17].8 Khan et al., 2022
The hyperparameters used are shown in Table 1. Collaborator selection for
RegSimAgg is shown in Fig. 2.
Table 1. Hyperparameters used in aggregation algorithms.
Hyperparameter RegSimAgg
Learning rate 5e-5
Epochs per round 1.0
3.2 Results
In this section, regularized similarity weighted aggregation (RegSimAgg) nd-
ings are summarized for partition 2. The results demonstrate that our approach
quickly converges and maintains stability as learning advances across all assessed
criteria.
Model training and performance using internal validation data. Fig-
ure 3 shows the performance of model training on internal validation data for
partition 2.
Model performance using external validation data. Prior to the formal
testing phase, challenge organizers provided 219 cases of external validation data
that we used to evaluate the performance of our approach RegSimAgg, see Ta-
bles 2. Overall, RegSimAgg performs better on whole tumor segmentation as
compared to enhancing tumor segmentation and tumor core segmentation.
Table 2. Performance on external validation data for partition 2.
Metrics RegSimAgg
Dice ET 0.7350
Dice TC 0.7337
Dice WT 0.8091
Hausdor95 ET 30.3497
Hausdor95 TC 25.3156
Hausdor95 WT 23.7706
Sensitivity ET 0.7231
Sensitivity TC 0.7146
Sensitivity WT 0.8144
Specicity ET 0.9998
Specicity TC 0.9998
Specicity WT 0.9988Federated Regularized Weight Aggregation 9
Fig. 3. Performance metrics for model training of RegSimAgg for partition 2 for inter-
nal validation. The horizontal axis refers to the number of rounds and the vertical axis
to the performance metrics. (a) DICE Score for labels 0, 1, 2, 4; (b) Hausdor 95%
Score for labels 0, 1, 2, 4; (c) Projected Convergence Score. Total simulation time was
approximately 154 hours.
Model performance using fully blinded test set. Team HT-TUAS sub-
mitted RegSimAgg algorithm for the leaderboard ranking. Model training is
performed for 500 rounds by challenge organizers. The performance stats on the
fully blinded test set for 570 patients are shown in Tables 3. When a weightage
of 6 is given to the communication cost, our team achieved third place in the
leaderboard.
4 Discussion
Several federated aggregation methods like exponential smoothing aggregation
and conditional threshold aggregation methods require user dened threshold pa-
rameter settings. Therefore, these algorithms are not easily applicable to new and
unexplored data sets. To overcome this issue, we developed regularized similar-
ity weighted aggregation, which adaptively learns the participating collaborator
weights.10 Khan et al., 2022
Table 3. RegSimAgg (HT-TUAS) test set performance on Leaderboard
Mean Standard Deviation Median 25 quantile 75 quantile
DICE ET 0.6745 0.2920 0.8004 0.5704 0.8866
DICE WT 0.7247 0.2026 0.7820 0.6235 0.8811
DICE TC 0.7169 0.3002 0.8655 0.6154 0.9259
Sensitivity ET 0.7501 0.3087 0.8815 0.6882 0.9612
Sensitivity WT 0.7834 0.2044 0.8546 0.7079 0.9307
Sensitivity TC 0.7592 0.3091 0.9035 0.6990 0.9715
Specicity ET 0.9991 0.0017 0.9997 0.9992 0.9999
Specicity WT 0.9974 0.0033 0.9985 0.9968 0.9994
Specicity TC 0.9992 0.0017 0.9997 0.9994 0.9999
Hausdor (95%) ET 35.2283 86.9517 3.7417 1.7321 19.1724
Hausdor (95%) WT 35.9036 30.8538 31.6172 8.4853 58.6204
Hausdor (95%) TC 33.7853 80.5775 6.4807 3.0000 20.1239
Communication Cost 0.6905 0.6905 0.6905 0.6905 0.6905
Our method does not require any participation in the modeling process at
the client side but it learns a global model at the server side. It is able to mini-
mize the contribution of diverging collaborators and allows clients with varying
settings to join the federation. This is in contrast to Fedprox [10], which per-
forms client side regularization of diverging collaborators. Moreover, studies have
shown that training federated learning algorithms using a randomly selected sub-
set of collaborators expedites the process [26]. We expanded on these ideas and
developed a sliding window technique that corroborates that all collaborators
are participate in the training process.
FeTS 2022 data is released in two partitions which split the training data
into non-IID data sets based on institution and tumor size. As a result, amount
and characteristics of data may vary for dierent collaborators. Weighted aggre-
gation strategy helps to learn a model that represents well the majority of the
contributors at each round with low impact by outliers. Therefore, our method
works well on both partitions. Even though the model works well when data has
non-IID splits in general, it could be also be useful to study the performance on
outliers further.
A limitation of the current setting is that the number of patients for each
collaborator is xed. In a real-world setting the patient data at a collaborator
may change between federation rounds. Additional patient data potentially al-
ters collaborator's data distribution { which might, in turn, aect the learned
parameters at the master model: during the FL learning process, a previously
diverging collaborator may become similar to the participating collaborators,
compelling some other participating collaborators to become outliers. However,
our approach takes diverging collaborator weights into account at each round.
Hence, this method can be used as a baseline for rened generation of a better
model that can be used at scale to newly generated data sets. Moreover, incorpo-
ration of our FL approach with a cutting-edge privacy protection AI frameworkFederated Regularized Weight Aggregation 11
is one of our future research directions. We also intend to investigate a potential
solution to the communication payload bottleneck between the collaborators and
the server. Further, we will also study how a decrease in the payload throughput
aects model convergence time-eciency and task performance optimization.
5 Conclusion
In this paper, we propose regularized similarity weighted aggregation as an im-
proved weight aggregation strategy for federated learning and apply it to imag-
ing in order to achieve robust brain tumor segmentation. In our experiments,
the proposed method { deployed in the OpenFL platform { performed well in
terms of convergence score and communication costs. Currently, the proposed
algorithm is a comprehensive proof-of-concept (POC) solution in the health-
care domain, demonstrated to potentially assist radiologists in diagnostic digital
pathology. However, this edge computing infrastructure can easily be scaled to
versatile real-world multi-client production level applications for foundational
collaborative computation and federated learning workows in other disciplines
like internet-of-things (IoT) and telecommunication. Moreover, this methodol-
ogy can issue stronger privacy guarantee via integrating dierential privacy or
secure multi-party computation, or a combination of the two, which is an in-
triguing future research topic.
6 Acknowledgements
This work was supported by the Business Finland under Grant 33961/31/2020.
We also acknowledge the support and computational resources facilitated by the
CSC-Puhti super-computer, a non-prot state enterprise owned by the Finnish
state and higher education institutions in Finland.
References
1. Annas, G.J.: Hipaa regulations|a new era of medical-record privacy? (2003)
2. Baid, U., Ghodasara, S., Mohan, S., Bilello, M., Calabrese, E., Colak, E., Farahani,
K., Kalpathy-Cramer, J., Kitamura, F.C., Pati, S., et al.: The rsna-asnr-miccai
brats 2021 benchmark on brain tumor segmentation and radiogenomic classica-
tion. arXiv preprint arXiv:2107.02314 (2021)
3. Bakas, S., Akbari, H., Sotiras, A., Bilello, M., Rozycki, M., Kirby, J., Freymann,
J., Farahani, K., Davatzikos, C.: Segmentation labels and radiomic features for the
pre-operative scans of the tcga-gbm collection. the cancer imaging archive. Nat Sci
Data 4, 170117 (2017)
4. Bakas, S., Akbari, H., Sotiras, A., Bilello, M., Rozycki, M., Kirby, J., Freymann,
J., Farahani, K., Davatzikos, C.: Segmentation labels and radiomic features for
the pre-operative scans of the tcga-lgg collection. The cancer imaging archive 286
(2017)12 Khan et al., 2022
5. Bakas, S., Akbari, H., Sotiras, A., Bilello, M., Rozycki, M., Kirby, J.S., Freymann,
J.B., Farahani, K., Davatzikos, C.: Advancing the cancer genome atlas glioma mri
collections with expert segmentation labels and radiomic features. Scientic data
4(1), 1{13 (2017)
6. Bernecker, T., Peters, A., Schlett, C.L., Bamberg, F., Theis, F., Rueckert, D., Wei,
J., Albarqouni, S.: Fednorm: Modality-based normalization in federated learning
for multi-modal liver segmentation. arXiv preprint arXiv:2205.11096 (2022)
7. Kairouz, P., McMahan, H.B., Avent, B., Bellet, A., Bennis, M., Bhagoji, A.N.,
Bonawitz, K., Charles, Z., Cormode, G., Cummings, R., et al.: Advances and open
problems in federated learning. Foundations and Trends ¬Æin Machine Learning
14(1{2), 1{210 (2021)
8. Khan, M.I., Jafaritadi, M., Alhoniemi, E., Kontio, E., Khan, S.A.: Adaptive weight
aggregation in federated learning for brain tumor segmentation. In: Crimi, A.,
Bakas, S. (eds.) Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic
Brain Injuries. pp. 455{469. Springer International Publishing, Cham (2022)
9. Li, Q., He, B., Song, D.: Model-contrastive federated learning. In: Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp.
10713{10722 (2021)
10. Li, T., Sahu, A.K., Zaheer, M., Sanjabi, M., Talwalkar, A., Smith, V.: Federated
optimization in heterogeneous networks. arXiv preprint arXiv:1812.06127 (2018)
11. Li, W., Milletar , F., Xu, D., Rieke, N., Hancox, J., Zhu, W., Baust, M., Cheng,
Y., Ourselin, S., Cardoso, M.J., et al.: Privacy-preserving federated brain tumour
segmentation. In: International workshop on machine learning in medical imaging.
pp. 133{141. Springer (2019)
12. Liu, D., Cabezas, M., Wang, D., Tang, Z., Bai, L., Zhan, G., Luo, Y., Kyle, K.,
Ly, L., Yu, J., et al.: Ms lesion segmentation: Revisiting weighting mechanisms for
federated learning. arXiv preprint arXiv:2205.01509 (2022)
13. Liu, Q., Chen, C., Qin, J., Dou, Q., Heng, P.A.: Feddg: Federated domain gener-
alization on medical image segmentation via episodic learning in continuous fre-
quency space. In: Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition. pp. 1013{1023 (2021)
14. McMahan, B., Moore, E., Ramage, D., Hampson, S., y Arcas, B.A.:
Communication-ecient learning of deep networks from decentralized data. In:
Articial intelligence and statistics. pp. 1273{1282. PMLR (2017)
15. Menze, B.H., Jakab, A., Bauer, S., Kalpathy-Cramer, J., Farahani, K., Kirby, J.,
Burren, Y., Porz, N., Slotboom, J., Wiest, R., et al.: The multimodal brain tumor
image segmentation benchmark (brats). IEEE transactions on medical imaging
34(10), 1993{2024 (2014)
16. Pati, S., Baid, U., Edwards, B., Sheller, M., Wang, S.H., Reina, G.A., Foley, P.,
Gruzdev, A., Karkada, D., Davatzikos, C., et al.: Federated learning enables big
data for rare cancer boundary detection. arXiv preprint arXiv:2204.10836 (2022)
17. Pati, S., Baid, U., Zenk, M., Edwards, B., Sheller, M., Reina, G.A., Foley, P.,
Gruzdev, A., Martin, J., Albarqouni, S., et al.: The federated tumor segmentation
(fets) challenge. arXiv preprint arXiv:2105.05874 (2021)
18. Reina, G.A., Gruzdev, A., Foley, P., Perepelkina, O., Sharma, M., Davidyuk, I.,
Trushkin, I., Radionov, M., Mokrov, A., Agapov, D., et al.: Open: An open-source
framework for federated learning. arXiv preprint arXiv:2105.06413 (2021)
19. Sarma, K.V., Harmon, S., Sanford, T., Roth, H.R., Xu, Z., Tetreault, J., Xu, D.,
Flores, M.G., Raman, A.G., Kulkarni, R., et al.: Federated learning improves site
performance in multicenter deep learning without data sharing. Journal of the
American Medical Informatics Association 28(6), 1259{1264 (2021)Federated Regularized Weight Aggregation 13
20. Voigt, P., Von dem Bussche, A.: The eu general data protection regulation (gdpr).
A Practical Guide, 1st Ed., Cham: Springer International Publishing 10, 3152676
(2017)
21. Wicaksana, J., Yan, Z., Zhang, D., Huang, X., Wu, H., Yang, X., Cheng, K.T.: Fed-
mix: Mixed supervised federated learning for medical image segmentation. arXiv
preprint arXiv:2205.01840 (2022)
22. Xia, Y., Yang, D., Li, W., Myronenko, A., Xu, D., Obinata, H., Mori, H., An,
P., Harmon, S., Turkbey, E., et al.: Auto-fedavg: learnable federated averaging for
multi-institutional medical image segmentation. arXiv preprint arXiv:2104.10195
(2021)
23. Xu, J., Glicksberg, B.S., Su, C., Walker, P., Bian, J., Wang, F.: Federated learning
for healthcare informatics. Journal of Healthcare Informatics Research 5(1), 1{19
(2021)
24. Xu, X., Chen, T., Deng, H., Kuang, T., Barber, J.C., Kim, D., Gateno, J., Yan, P.,
Xia, J.J.: Federated cross learning for medical image segmentation. arXiv preprint
arXiv:2204.02450 (2022)
25. Yan, Z., Wicaksana, J., Wang, Z., Yang, X., Cheng, K.T.: Variation-aware feder-
ated learning with multi-source decentralized medical image data. IEEE Journal
of Biomedical and Health Informatics 25(7), 2615{2628 (2020)
26. Zhao, Y., Li, M., Lai, L., Suda, N., Civin, D., Chandra, V.: Federated learning
with non-iid data. arXiv preprint arXiv:1806.00582 (2018)