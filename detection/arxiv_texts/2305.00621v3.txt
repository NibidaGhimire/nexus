Proper Scoring Rules for Survival Analysis
Hiroki Yanagisawa1
Abstract
Survival analysis is the problem of estimating
probability distributions for future event times,
which can be seen as a problem in uncertainty
quantification. Although there are fundamental
theories on strictly proper scoring rules for uncer-
tainty quantification, little is known about those
for survival analysis. In this paper, we investigate
extensions of four major strictly proper scoring
rules for survival analysis and we prove that these
extensions are proper under certain conditions,
which arise from the discretization of the estima-
tion of probability distributions. We also compare
the estimation performances of these extended
scoring rules by using real datasets, and the exten-
sions of the logarithmic score and the Brier score
performed the best.
1. Introduction
The theory of scoring rules is a fundamental theory in sta-
tistical analysis, and it is widely used in uncertainty quan-
tification (see, e.g., (Mura et al., 2008; Parmigiani & Inoue,
2009; Benedetti, 2010; Schlag et al., 2015)). Suppose that
there is a random variable Ywhose cumulative distribution
function (CDF) is FY. Given an estimation ˆFYofFYand a
single sample yobtained from Y, a scoring rule S(ˆFY, y)
is a function that returns an evaluation score for ˆFYbased
ony. Since ˆFYis a CDF and yis a single sample of Y,
it is not straightforward to choose an appropriate scoring
ruleS(ˆFY, y). The theory of scoring rules suggests how to
choose an appropriate one. This theory proves that a certain
class of scoring rules satisfies this natural property: the av-
erage evaluation score S(ˆFY, y)overy∼Yis minimized
only by the true CDF FY. A scoring rule that satisfies this
property is called strictly proper in this theory. Examples of
strictly proper scoring rules include the pinball loss, the log-
arithmic score, the Brier score, and the ranked probability
1IBM Research - Tokyo, Tokyo, Japan. Correspondence to:
Hiroki Yanagisawa <yanagis@jp.ibm.com >.
Proceedings of the 40thInternational Conference on Machine
Learning , Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright
2023 by the author(s).score (see, e.g., (Gneiting & Raftery, 2007) for the defini-
tions of these scoring rules). In uncertainty quantification,
it is standard to use a strictly proper scoring rule both for a
loss function to train machine learning models and for an
evaluation metric to evaluate the models. Note that, if we
use a non-proper scoring rule S(ˆFY, y)as a loss function, a
prediction model (e.g., a neural network model) might find
an estimation ˆFYsuch that S(ˆFY, y)< S(FY, y)holds for
y∼Yon average and such ˆFYcould be very different from
trueFY.
Survival analysis , which is also known as time-to-event
analysis , can be seen as a problem in uncertainty quantifica-
tion. Despite the long history of research from the seminal
work (Cox, 1972) on survival analysis (see, e.g., (Wang
et al., 2019) for a comprehensive survey), little is known
about the strictly proper scoring rules for survival analysis.
Therefore, we investigate extensions of these strictly proper
scoring rules for survival analysis.
In survival analysis, the time between a well-defined start-
ing point and the occurrence of an event is called the sur-
vival time orevent time , and the goal of survival analysis
is to estimate the probability distribution of event times. In
healthcare applications, an event usually corresponds to an
undesirable event for a patient (e.g., a death or the onset
of disease). Survival analysis has important applications in
many fields such as credit scoring (Dirick et al., 2017) and
fraud detection (Zheng et al., 2019) as well as healthcare.
Datasets for survival analysis are censored , which means
that events of interest might not be observed for a number of
data points. This may be due to either a limited observation
time window or missing traces caused by other irrelevant
events. In this paper, we consider only right censored data,
which is a widely studied problem setting in survival analy-
sis. The exact event time of a right censored data point is
unknown; we know only that the event had not happened
up to a certain time for the data point. The time between a
well-defined starting point and the last observation time of
a right censored data point is called the censoring time .
Many neural network models have been proposed for sur-
vival analysis (e.g., (Avati et al., 2019; Kamran & Wiens,
2021; Pearce et al., 2022)). A common problem with these
models is that they define their own custom loss functions,
and they use these loss functions without proving that they
1arXiv:2305.00621v3  [stat.ME]  12 Jun 2023Proper Scoring Rules for Survival Analysis
are strictly proper in terms of the theory of scoring rules.
Indeed, Rindt et al. (2022) show that the loss functions used
in (Avati et al., 2019; Kamran & Wiens, 2021) are not proper.
Moreover, survival models have been evaluated by custom
evaluation metrics without proving that these metrics are
proper in terms of the theory of scoring rules. Popular met-
rics used for survival analysis include the integrated Brier
score (Graf et al., 1999) and variants of C-index (Antolini
et al., 2005; Uno et al., 2011). However, all of them are
not proper (Blanche et al., 2018; Rindt et al., 2022). We
also note that Sonabend et al. (2022) discuss the problems
of using these variants of C-index as evaluation metrics in
survival analysis.
The only exception to the above argument is (Rindt et al.,
2022). This paper shows a rigorous proof that an exten-
sion of the logarithmic score for survival analysis is strictly
proper. Note that this paper is not the first one that uses
this extension of the logarithmic score (e.g., (Lee et al.,
2018; Ren et al., 2019; Tjandra et al., 2021)). However, it
is usually used in part of the loss functions of the proposed
models, and these loss functions are used without the proof
of properness.
Our contributions. We analyze survival analysis through
the lens of the theory of scoring rules. First, we prove that
Portnoy’s estimator (Portnoy, 2003), which is an extension
of the pinball loss for survival analysis, is proper under cer-
tain conditions. This is the first proof for the properness
for Portnoy’s estimator. In addition, we show such con-
ditions are due to the discretization of the estimation of a
probability distribution and we explain why such conditions
are required to be proper scoring rules for survival analysis.
Second, we show that the proof of strict properness of the
extension of the logarithmic score (Rindt et al., 2022) is
based on implicit assumptions by showing its alternative
proof. Third, we show two new proper scoring rules for
survival analysis under certain conditions by extending the
Brier score and the ranked probability score. These scor-
ing rules are the first scoring rules with rigorous proofs of
properness as extensions of the Brier score and the ranked
probability score. Finally, we compare these four extensions
of the scoring rules by using real datasets, and the results
show that the extensions of the logarithmic score and Brier
score performed the best.
2. Related Work
Survival analysis has been traditionally studied under the
proportional hazard assumption . Its seminal work is the
Cox model (Cox, 1972), and many other prediction models
have been proposed under this strong assumption. Since
outputs of these models are scalar values called hazard rates
and are not CDFs, we use different types of loss functionsand evaluation metrics in traditional survival analysis. One
of the popular evaluation metrics is the concordance index
(C-index) (Harrell et al., 1982), which is a generalization of
the Kendall rank correlation coefficient. See, e.g., (Wang
et al., 2019) for a comprehensive survey on survival analysis
based on this assumption. In this paper, we focus on survival
analysis without this assumption.
We note that there are many loss functions used in survival
models that can be seen as variants of known scoring rules.
•Pinball loss. Portnoy’s estimator (Portnoy, 2003),
which is an extension of the pinball loss, has been used
in quantile regression-based survival analysis (Port-
noy, 2003; Neocleous et al., 2006; Pearce et al., 2022).
It was unknown if this estimator is proper or not in
terms of the theory of scoring rules, and we are the first
to prove that this estimator is proper under a certain
condition.
•Brier score. The IPCW Brier score (Graf et al., 1999)
and integrated Brier score (Graf et al., 1999) are widely
used in survival analysis (e.g., (Kvamme et al., 2019;
Haider et al., 2020; Han et al., 2021; Zhong et al.,
2021)) as variants of the Brier score. However, Rindt
et al. (2022) show that neither of them is proper in
terms of the theory of scoring rules.
•Ranked probability score. Variants of the ranked
probability score have been proposed in (Avati et al.,
2019; Kamran & Wiens, 2021), but Rindt et al. (2022)
show that they are not proper in terms of the theory of
scoring rules.
3. Preliminaries
Given a feature vector x∈X, letTxandCxbe random
variables for the event time and censoring time of x, respec-
tively. Unless otherwise stated, we consider a fixed x, and
we denote them by TandCby omitting the subscript x
from TxandCx, respectively.
Lett∼Tandc∼Cbe samples obtained from Tand
C, respectively. We assume that tandcare positive real
values (i.e., t∈R+andc∈R+). In survival analysis,
we can observe only the minimum z= min {t, c}, and we
useδ= 1(t≤c)to indicate whether zrepresents the
true event time (i.e., δ= 1 means zis uncensored and
z=t) orzrepresents the censoring time (i.e., δ= 0
means zis censored and z=c). In this paper, a pair of
samples (t, c)is often represented as a pair of values (z, δ)
to emphasize that we can observe only one of tandc. We
assume that we have prior knowledge that zis at most zmax
(i.e., 0< z≤zmax holds for any z). Let F(t)be the
CDF of T, which is defined as F(t) = Pr( T≤t). By the
definition of F(t), we have F(0) = 0 , and we can represent
2Proper Scoring Rules for Survival Analysis
0τ1τ2τ3τ41
ˆF(t)
Time tzmax
(a) Quantile regression0ζ1ζ2ζ3ζ4zmax1
ˆF(t)
Time t
(b) Distribution regression
Figure 1. Two types of discretization of probability distribution
ˆF(t)withB= 5.
the probability that the true event time is between t1andt2
asPr(t1< T≤t2) =F(t2)−F(t1).
Survival analysis is the problem of estimating the ˆF(t)of
the true CDF F(t). For simplicity, we assume that both F(t)
andˆF(t)are monotonically increasing continuous functions.
This means that F(t1)< F(t2)holds if and only if 0≤
t1< t2<∞. This assumption enables us to calculate
F(t)for any time 0≤t <∞and to calculate F−1(τ)for
any quantile level 0≤τ≤1. When we estimate ˆF(t)
by using a prediction model (e.g., a neural network), we
usually discretize p=ˆF(t)along the p-axis or the t-axis
as shown in Fig. 1. In quantile regression-based survival
analysis, p=ˆF(t)is discretized along the p-axis, ˆF−1(τi)
is estimated for 0 =τ0< τ1<···< τB−1< τB= 1,
and we assume that ˆF−1(τ0) = 0 andˆF−1(τB) =zmax.
In distribution regression-based survival analysis, p=ˆF(t)
is discretized along the t-axis, ˆF(ζi)is estimated for 0 =
ζ0< ζ1<···< ζB−1< ζB=zmax, and we assume that
ˆF(ζ0) = 0 andˆF(ζB) = 1 .
Throughout this paper, we assume that the censoring time
and the event time are independent of each other given a
feature vector x. This assumption is usually represented as
follows.
Assumption 3.1. T⊥ ⊥C|X.
This assumption is widely used in survival analysis (e.g.,
the classical Kaplan-Meier estimator (Kaplan & Meier,
1958) and the calibration metric D-calibration (Haider et al.,
2020)). We can find examples of the other stronger assump-
tions (e.g., unconditionally random right censoring) used in
survival analysis in (Peng, 2021).
4. Proper Scoring Rules for Survival Analysis
We briefly review the theory of scoring rules for uncertainty
quantification. Let Ybe a random variable, and let FY(y)
be its CDF, which is defined as FY(y) = Pr( Y≤y). A
scoring rule is a function S(ˆFY, y)that returns a real value
(i.e., an evaluation score) for inputs ˆFYandy, where ˆFY
is an estimation of FYandyis a sample obtained fromY. In this paper, we consider negatively-oriented scoring
rules, which means that a smaller score is better. We can
interpret the scoring rule S(ˆFY, y)as a penalty function for
the misestimation of ˆFYfor a sample y.
Theproper andstrictly proper scoring rules are defined as
follows.
Definition 4.1. A scoring rule S(ˆFY, y)isproper if
E
y∼Y[S(ˆFY, y)]≥E
y∼Y[S(FY, y)].
Definition 4.2. A scoring rule S(ˆFY, y)isstrictly proper if
E
y∼Y[S(ˆFY, y)]≥E
y∼Y[S(FY, y)]
holds and the equality holds only when ˆFY=FY.
These definitions are based on a natural property that any
scoring rule should satisfy. Definition 4.2 means that we can
recover the true FYby minimizing the average evaluation
score S(ˆFY, y)overy∼Yfor a strictly proper scoring rule
S(·,·).
We extend these definitions of the proper and strictly proper
scoring rules for survival analysis. We define the proper
andstrictly proper scoring rules for survival analysis by
changing the inputs of a scoring rule S(ˆF,(z, δ))from FY
andytoFand(z, δ).
Definition 4.3. A scoring rule S(ˆF,(z, δ))isproper if
E
(t,c)∼(T,C)[S(ˆF,(z, δ))]≥ E
(t,c)∼(T,C)[S(F,(z, δ))].
Definition 4.4. A scoring rule S(ˆF,(z, δ))isstrictly proper
if
E
(t,c)∼(T,C)[S(ˆF,(z, δ))]≥ E
(t,c)∼(T,C)[S(F,(z, δ))]
holds and the equality holds only when ˆF=F.
Following the standard approach of using a strictly proper
scoring rule in uncertainty quantification (Bengs et al.,
2022), we explain how to use a scoring rule S(ˆF,(z, δ))
as a loss function in survival analysis. Given a training
dataset {(x(i), z(i), δ(i))}n
i=1, we formulate survival analy-
sis as minimizing the empirical loss
nX
i=1S(ˆFx(i),(z(i), δ(i))),
where ˆFx(i)is an estimation of the true CDF Fx(i)of random
variable Tx(i)forx(i)∈X. This formulation assumes that
eachx(i)∈Xhas an underlying random variable Tx(i)for
event times, and our task is to find a good estimation ˆFx(i)
for each x(i).
3Proper Scoring Rules for Survival Analysis
In this paper, we investigate the extensions of the scoring
rules for survival analysis. In Sec. 4.1, we consider quantile
regression and survival analysis based on quantile regression.
In Secs. 4.2–4.4, we consider distribution regression and
survival analysis based on distribution regression.
4.1. Extension of Pinball Loss
We first review quantile regression (Koenker & Bassett,
1978; Koenker & Hallock, 2001). Let Ybe a real-valued
random variable and FYbe its CDF. In quantile regression,
we estimate the τ-th quantile of Y, which can be written as
F−1
Y(τ) = inf {y|FY(y)≥τ}.
Thepinball loss (Koenker & Bassett, 1978), which is also
known as the check function , is a widely used scoring rule.
The pinball loss for an estimation ˆFYofFYand a quantile
levelτ∈[0,1]is defined as
SPinball (ˆFY, y;τ)
=ρτ(ˆF−1
Y(τ), y)
=(
(1−τ)(ˆF−1
Y(τ)−y)ifˆF−1
Y(τ)≥y,
τ(y−ˆF−1
Y(τ)) ifˆF−1
Y(τ)< y.(1)
Note that the pinball loss with τ= 0.5is equivalent to the
mean absolute error (MAE), and it can be used to estimate
themedian (i.e., 0.5-th quantile) of Y. This means that the
pinball loss is a generalization of MAE for any quantile level
τ∈[0,1]. Note also that we include the quantile level τin
the notation SPinball (ˆF−1
Y, y;τ)to clarify that this scoring
rule receives τas an input.
It is known that the pinball loss is strictly proper (see
e.g., (Gneiting & Raftery, 2007)), which means that we
have
E
y∼Y[SPinball (ˆFY, y;τ)]≥E
y∼Y[SPinball (FY, y;τ)],
and the equality holds only when ˆF−1
Y(τ) =F−1
Y(τ)by
Definition 4.2. Therefore, it is standard to use the pinball
loss both for a loss function and an evaluation metric in
quantile regression.
Portnoy’s estimator (2003) is an extension of the pinball
loss for quantile regression-based survival analysis, which
is defined as
SPortnoy (ˆF,(z, δ);w, τ)
=

ρτ(ˆF−1(τ), z) ifδ= 1,
wρτ(ˆF−1(τ), z)
+(1−w)ρτ(ˆF−1(τ), z∞)ifδ= 0,(2)
where ρτis the pinball loss defined in Eq. (1), wis a weight
parameter to control the balance between two pinball lossterms, and z∞is any constant such that z∞> z max. In
Portnoy’s estimator, we can set an arbitrary constant 0≤
w≤1for the parameter wifτc> τ, where τc= Pr( t≤
c) =F(c), but we have to set w= Pr( F(c)< F(t)≤
τ|t > c ) = ( τ−τc)/(1−τc)otherwise (i.e., τc≤τ).
Since we do not know the true value τc=F(c), we have to
resolve this problem to use this estimator. Before showing
how to resolve this problem, we prove that this estimator is
proper under the condition that wis correct. Note that this
is the first result for the quantile regression-based survival
analysis in terms of the theory of scoring rules.
Theorem 4.5. Portnoy’s estimator is proper under the con-
dition that wis correct.
Proof. We give a proof in Appendix A.1.
This theorem means that the crucial part of Portnoy’s esti-
mator is to set an appropriate value for w, and this theorem
ensures that we can recover the true probability distribution
Fby minimizing Eq. (2) if wis correct.
Now, we emphasize that we cannot avoid the dependence
on unknown parameters such as F(c)in the definition of
anyof the scoring rules for survival analysis due to the dis-
cretization of ˆF. In the case of Portnoy’s estimator, even
if we know the true value F−1(τi)for all {τi}B
i=0, we can-
not compute F(c)because cis not always contained in
{F−1(τi)}B
i=0. The best we can do is to find quantile lev-
elsτiandτi+1such that F−1(τi)< c≤F−1(τi+1)by
using the assumption that Fis a monotonically increasing
function. This means that F(c)is between τiandτi+1.
Even if we could find such τiandτi+1, we would not
be able to calculate some important probabilities such as
Pr(c < t ≤F−1(τi+1)) = τi+1−F(c). Therefore, we
usually mitigate this problem by using a large B, which en-
ables us to assume, for example, F−1(τi+1)−F−1(τi)≈0
for all i.
Even if we use a large Bto assume that we can find
the quantile level τ′
csuch that c≈F−1(τ′
c)for any c,
the problem that we do not know the true F−1remains.
One of the approaches to tackling this problem is the
grid search algorithm (Portnoy, 2003; Neocleous et al.,
2006). In this algorithm, we use a sufficiently large B,
and we estimate ˆF−1(τi)ofF−1(τi)in the increasing or-
der of i= 0,1, . . . , B . Suppose that we have estimated
{ˆF−1(τi)}j−1
i=0and we are going to estimate ˆF−1(τj). The
key idea of this algorithm is that we can find τ′
c∈ {τi}j−1
i=0
such that c≈ˆF−1(τ′
c)ifτc=F(c)< τj. If we can find
such τ′
c, we estimate wby using τ′
c≈τc. If we cannot
find such τ′
c, this algorithm assumes that τc> τjand we
use an arbitrary constant 0≤w≤1. Portnoy (2003) dis-
cusses that this algorithm is analogous to the Kaplan-Meier
estimator (Kaplan & Meier, 1958), and their theoretical
4Proper Scoring Rules for Survival Analysis
analysis (Portnoy, 2003; Neocleous et al., 2006) proves that
the estimation model combining Portnoy’s estimator, linear
regression, and the grid search algorithm can recover the
true probability distribution Fif there is a sufficient number
of data points.
As for another approach, Pearce et al. (2022) propose the
CQRNN algorithm, which we call an iterative reweighting
(IR) algorithm. Unlike the grid search algorithm, this al-
gorithm estimates {ˆF−1(τi)}B
i=0simultaneously by using
a neural network. This algorithm starts with an arbitrary
initial estimation ˆF, and it estimates ˆwof the true wby
using ˆF. Then, it updates ˆFby using ˆw, and it repeats this
iterative procedure of estimating ˆFandˆwuntil these values
converge. This IR algorithm is similar to the expectation-
maximization (EM) algorithm, and the relationship between
this algorithm and the EM algorithm is discussed in (Pearce
et al., 2022). Note that this IR algorithm can be implemented
for “free” according to (Pearce et al., 2022), which means
that we can implement it easily in the computation of the
loss function of a neural network training algorithm, and we
do not need to construct two separate neural network mod-
els for estimating ˆFandˆw. The experimental evaluation
in (Pearce et al., 2022) shows that the IR algorithm per-
forms the best among the quantile regression-based survival
analysis models.
4.2. Extension of Logarithmic Score
While we estimate {ˆF−1
Y(τi)}B
i=0in quantile regression,
we consider distribution regression, in which we estimate
{ˆFY(ζi)}B
i=0. For distribution regression, the logarithmic
score (Good, 1952) is known as a strictly proper scoring
rule, and it is defined as
SLog(ˆFY, y;{ζi}B
i=0)
=−B−1X
i=01(ζi< y≤ζi+1) log( ˆFY(ζi+1)−ˆFY(ζi))
=−B−1X
i=01(ζi< y≤ζi+1) log ˆfi, (3)
where ˆfi=ˆFY(ζi+1)−ˆFY(ζi)fori= 0,1, . . . , B −1.
We extend this logarithmic score for distribution regression-
based survival analysis as
SCen−log(ˆF,(z, δ);{wi}B−1
i=0,{ζi}B
i=0)
=−B−1X
i=01(ζi< z≤ζi+1)g(i, δ, w i), (4)
where
g(i, δ, w i)
=(
logˆfi ifδ= 1,
wilogˆfi+ (1−wi) log(1 −ˆF(ζi+1)) ifδ= 0,1−F(c)
F(ζi+1)−F(c)
0 cζi+11 F(t)
Time t
Figure 2. Illustration of computation of weight wi= (F(ζi+1)−
F(c))/(1−F(c))for scoring rule SCen−log.
ˆfi=ˆF(ζi+1)−ˆF(ζi), and wi= Pr( c < t ≤ζi+1|t >
c) = (F(ζi+1)−F(c))/(1−F(c)). Note that this scoring
rule is equivalent to Eq. (3) if δ= 1. Similar to Portnoy’s
estimator, we cannot set the parameter wiof this scoring
rule because we do not know F(ζi+1)andF(c).
Even though we do not know the correct {wi}B−1
i=0, we
prove that this scoring rule is proper if the set of parameters
{wi}B−1
i=0is correct.
Theorem 4.6. SCen−Log(ˆF,(z, δ);{wi}B−1
i=0,{ζi}B
i=0)is a
proper scoring rule under the condition that wiis correct
for all i.
Proof. We give a proof in Appendix A.2.
Similar to Portnoy’s estimator, we can use both the grid-
search algorithm and the IR algorithm to estimate {wi}B−1
i=0.
In addition, we show another simpler approach by assuming
thatwi≈0for all iifBis large. If Bis large, 1−F(c)is
usually much larger than F(ζi+1)−F(c)(see Fig. 2), and
hence, we have wi= (F(ζi+1)−F(c))/(1−F(c))≈0.
Therefore, we can obtain a simpler variant of SCen−logby
setting wi= 0for all i:
SCen−log−simple (ˆF,(z, δ);{ζi}B
i=0)
=−B−1X
i=01(ζi< z≤ζi+1)g(i, δ,0) (5)
=−δB−1X
i=01(ζi< z≤ζi+1) log ˆfi
−(1−δ)B−1X
i=01(ζi< z≤ζi+1) log(1 −ˆF(ζi+1)).
Furthermore, by increasing Bto infinity (i.e., B→ ∞ ), we
obtain the continuous version of this scoring rule:
SCen−cont−log(ˆF,(z, δ))
=−δlogdˆF
dt(z)−(1−δ) log(1 −ˆF(z)),(6)
5Proper Scoring Rules for Survival Analysis
which is equal to the extension of the logarithmic score that
is proven to be strictly proper in (Rindt et al., 2022).
Remarks. This simplification clarifies that the proof
in (Rindt et al., 2022) implicitly assumes that Bis suf-
ficiently large. This means that we should set Blarge
enough in practice. Moreover, strictly speaking, the relation
wi= (F(ζi+1)−F(c))/(1−F(c))≈0may not hold if
1−F(c)≈0. Therefore, we recommend SCen−log(Eq. (4))
rather than SCen−log−simple (Eq. (5)) and SCen−cont−log
(Eq. (6)) whenever possible.
4.3. Extension of Brier Score
In distribution regression, the Brier score (Brier, 1950) is
also known as a strictly proper scoring rule, which is defined
as
SBrier(ˆFY, y;{ζi}B
i=0)
=B−1X
i=0( 1(ζi< y≤ζi+1)−ˆfi)2, (7)
where ˆfi=ˆFY(ζi+1)−ˆFY(ζi)fori= 0,1, . . . , B −1.
We extend this Brier score for distribution regression-based
survival analysis as
SCen−Brier(ˆF,(z, δ);{wi}B−1
i=0,{ζi}B
i=0)
=B−1X
i=0
wi(1−ˆfi)2+ (1−wi)ˆf2
i
, (8)
where
wi=

0 ifδ= 1andζi+1< z=t,
1 ifδ= 1andζi< z=t≤ζi+1,
0 ifz≤ζi,
F(ζi+1)−F(c)
1−F(c)ifδ= 0andζi< z=c≤ζi+1,
F(ζi+1)−F(ζi)
1−F(c)ifδ= 0andζi+1< z=c.
Ifδ= 1, it is easy to see that Eq. (8) is equivalent to Eq. (7).
We prove that this scoring rule is proper if the set of param-
eters{wi}B−1
i=0is correct.
Theorem 4.7. SCen−Brier(ˆF,(z, δ);{wi}B−1
i=0,{ζi}B
i=0)is
a proper scoring rule under the condition that wiis correct
for all i.
Proof. We give a proof in Appendix A.3.
We can use the IR algorithm to estimate wi. However, un-
like Portnoy’s estimator and the extension of the logarithmic
score, we cannot use the grid-search algorithm in this exten-
sion of the Brier score because we need to estimate wifor
alli= 0,1, . . . , B −1.Note that each wiin this scoring rule is close to zero if Bis
large and δ= 0. However, since wis are designed to satisfyP
iwi= 1, we cannot use the approximation wi≈0for
this scoring rule.
4.4. Extension of Ranked Probability Score
The ranked probability score (RPS) is also known as a
strictly proper scoring rule in distribution regression (see,
e.g., (Gneiting & Raftery, 2007)). It is defined as
SRPS(ˆFY, y) =B−1X
i=1SBinary −Brier(ˆFY, y;ζi),
where SBinary −Brier is the binary version of SBrier (Eq. (7))
with a single threshold ζ:
SBinary −Brier(ˆFY, y;ζ) = ( 1(y≤ζ)−ˆFY(ζ))2.
We extend this scoring rule for survival analysis:
SCen−RPS(ˆF,(z, δ);{wi}B−1
i=1,{ζi}B−1
i=1)
=B−1X
i=1SCen−Binary −Brier(ˆF,(z, δ);wi, ζi),(9)
where SCen−Binary −Brier is the binary version of
SCen−Brier (Eq. (8)) with a single threshold ζ:
SCen−Binary −Brier(ˆF,(z, δ);w, ζ)
=

ˆF(ζ)2ifz > ζ,
(1−ˆF(ζ))2ifδ= 1andz=t≤ζ,
w(1−ˆF(ζ))2
+(1−w)ˆF(ζ)2ifδ= 0andz=c≤ζ,
where w= (F(ζ)−F(c))/(1−F(c)).
Since this scoring rule is just the sum of the binary version
of Brier scores for survival analysis, it is straightforward to
prove this theorem.
Theorem 4.8. SCen−RPS(ˆF,(z, δ);{wi}B−1
i=1,{ζi}B−1
i=1)is
a proper scoring rule under the condition that wiis correct
for all i.
Note that the scoring rule SCen−Binary −Brier is analogous
to Portnoy’s estimator. The scoring rule SCen−Binary −Brier
is designed to estimate ˆF(ζ), where ζis an input, and we
useF(c)andζto set w, whereas Portnoy’s estimator is
designed to estimate ˆF−1(τ), where τis an input, and we
useF(c)andτto set w. As these two scoring rules are
similar, we can use both the grid-search algorithm and the
IR algorithm for SCen−RPS.
Unlike SCen−logdefined in Eq. (4), the parameter wof
the scoring rule SCen−Binary −Brier is usually not close to
zero because ζandcare usually not close to each other as
shown in Fig. 3. We note that the parameter wof Portnoy’s
estimator is also not close to zero for a similar reason.
6Proper Scoring Rules for Survival Analysis
1−F(c)F(ζ)−F(c)
0 c ζ1 F(t)
Time t
Figure 3. Illustration of computations of weight wi= (F(ζ)−
F(c))/(1−F(c))for scoring rule SCen−Binary −Brier.
5. Evaluation Metrics for Survival Analysis
While we have discussed the extensions of the scoring rules
as loss functions, we should use strictly proper scoring rules
also for evaluation metrics. However, among the exten-
sions of the scoring rules for survival analysis, we can use
onlySCen−log−simple (Eq. (5)) as an evaluation metric be-
cause the other scoring rules depend on the parameter w
or{wi}B−1
i=0. Note that we can use SCen−log−simple only
when Bis sufficiently large. In Appendix B, we conducted
experiments on choosing an appropriate B, and the results
suggested using B > 16.
While we can use SCen−log−simple as a discrimination met-
ric for survival analysis, we note that there is a calibration
metric, D-calibration (Haider et al., 2020), for survival anal-
ysis. D-calibration is widely used in survival analysis, but
we propose another calibration metric, KM-calibration . Let
κ(t)be the survival function estimated by the Kaplan-Meier
estimator (Kaplan & Meier, 1958). This function κ(t)rep-
resents the survival rate (i.e., the probability that the event
time is less than t) over the entire dataset rather than indi-
vidual feature vector x. By definition, κ(0) = 1 andκ(t)is
a monotonically decreasing function. Assuming that κ(t)is
correct, κ(t) = 1−ˆFavg(t)must hold, where ˆFavg(t)is the
average of ˆF(t)over all data points in the test dataset. There-
fore, we define our KM-calibration as the Kullback-Leibler
divergence between κ(t)and1−ˆFavg(t):
dKM−cal(κ,ˆFavg) = dKL(κ||1−ˆFavg)
=B−1X
i=0(pilogpi−pilogqi),
where pi=κ(ζi+1)−κ(ζi),qi= (1−ˆFavg(ζi+1))−
(1−ˆFavg(ζi)), and we assume here that κ(ζB) = 0 . This
metric is based on the observation that the model’s predicted
number of events within any time interval should be similar
to the observed number (Goldstein et al., 2020). We note
that there is another calibration metric (Chapfuwa et al.,
2020) based on the Kaplan-Meier estimator. Whereas thiscalibration metric uses the absolute difference, our KM-
calibration uses the Kullback-Leibler divergence.
6. Experiments
In our experiments, we compared practical prediction perfor-
mances of various loss functions on real datasets. We used
three datasets for the survival analysis from the packages
in R (R Core Team, 2016): the flchain dataset (Dispen-
zieri et al., 2012), which was obtained from the “survival”
package and contains 7874 data points (69.9% of which are
censored), the prostateSurvival dataset (Lu-Yao et al., 2009),
which was obtained from the “asaur” package and contains
14294 data points (71.7% of which are censored), and the
support dataset (Knaus et al., 1995), which was obtained
from the “casebase” package and contains 9104 data points
(31.9% of which are censored). For each dataset, we di-
vided the time interval [0, zmax+ϵ), where ϵ= 10−3, into
Bequal-length intervals to get the thresholds {ζi}B
i=0for
distribution regression-based survival analysis, and we di-
vided the unit interval [0,1]intoBequal-length intervals to
get the quantile levels {τi}B
i=0for quantile regression-based
survival analysis. Unless otherwise stated, we set B= 32 .
All our experiments were conducted on a virtual machine
with an Intel Xeon CPU (3.30 GHz) processor without any
GPU and 64 GB of memory running Red Hat Enterprise
Linux Server 7.6. We used Python 3.7.4 and PyTorch 1.7.1
for the implementation.
We estimated ˆF(t)by combining a multi-layer perceptron
(MLP) and the IR algorithm (see Sec. 4.1) to estimate w
or{wi}B−1
i=0. The MLP consists of three hidden layers con-
taining 128 neurons, and the number of outputs was B.
The type of activation function after the hidden layer was
the rectified linear unit (ReLU), and the activation func-
tion at the output node was softmax. The softmax func-
tion is used to satisfy the assumption that ˆF(t)is a mono-
tonically increasing continuous function. In distribution
regression-based survival analysis, each output of MLP es-
timates ˆfi=ˆF(ζi+1)−ˆF(ζi)fori= 0,1, . . . , B −1. By
using these outputs {ˆfi}B−1
i=0, we can calculate {ˆF(ζi)}B
i=0
and we can represent the function ˆF(t)as a piecewise linear
function connecting the values {ˆF(ζi)}B
i=0. Since ˆfi>0
holds for all i,ˆF(t)estimated in this way is a monotoni-
cally increasing continuous function. We can estimate ˆF
for quantile regression-based survival analysis by using a
similar way.
For the training of the neural network, we used the Adam
optimizer (Kingma & Ba, 2015) with the learning rate 0.001,
and the other parameters were set to their default values. We
ran training for 300epochs for our neural network models.
Our implementation of the scoring rules are available at
https://github.com/IBM/dqs .
7Proper Scoring Rules for Survival Analysis
0.0 0.5 1.0 1.5 2.0IPCW BS(t)S-CRPSDRSADeepHitPortnoyCen-RPSCen-BrierCen-log
(a)SCen−log−simple on flchain
0.0 0.5 1.0 1.5IPCW BS(t)S-CRPSDRSADeepHitPortnoyCen-RPSCen-BrierCen-log (b)SCen−log−simple on prostateSurvival
0.0 0.5 1.0 1.5 2.0 2.5IPCW BS(t)S-CRPSDRSADeepHitPortnoyCen-RPSCen-BrierCen-log (c)SCen−log−simple on support
0.000 0.002 0.004 0.006 0.008 0.010IPCW BS(t)S-CRPSDRSADeepHitPortnoyCen-RPSCen-BrierCen-logKaplan-Meier
(d) D-calibration on flchain
0.000 0.002 0.004 0.006 0.008 0.010IPCW BS(t)S-CRPSDRSADeepHitPortnoyCen-RPSCen-BrierCen-logKaplan-Meier (e) D-calibration on prostateSurvival
0.000 0.005 0.010 0.015 0.020 0.025IPCW BS(t)S-CRPSDRSADeepHitPortnoyCen-RPSCen-BrierCen-logKaplan-Meier (f) D-calibration on support
0.0 0.1 0.2 0.3 0.4IPCW BS(t)S-CRPSDRSADeepHitPortnoyCen-RPSCen-BrierCen-logKaplan-Meier
(g) KM-calibration on flchain
0.0 0.2 0.4 0.6 0.8 1.0IPCW BS(t)S-CRPSDRSADeepHitPortnoyCen-RPSCen-BrierCen-logKaplan-Meier (h) KM-calibration on prostateSurvival
0.0 0.2 0.4 0.6IPCW BS(t)S-CRPSDRSADeepHitPortnoyCen-RPSCen-BrierCen-logKaplan-Meier (i) KM-calibration on support
Figure 4. Prediction performance (lower is better) comparison on three datasets with SCen−log−simple , KM-calibration, and D-calibration.
We compared the prediction performances of various scor-
ing rules (i.e., loss functions), and Fig. 4 shows the results.
In these experiments, we split the data points into train-
ing (60%), validation (20%), and test (20%), and each bar
shows the mean of the measurements on the test data of
five random splits together with the error bar, which rep-
resents the standard deviation. We used SCen−log−simple
(Eq. (5)) as a metric for discrimination performance and
D-calibration (Haider et al., 2020) and KM-calibration (see
Sec. 5) as calibration metrics, where we used 20 bins of
equal length for D-calibration. For the calibration metrics,
we added the mean D-calibration and mean KM-calibration
of the Kaplan-Meier estimator (Kaplan & Meier, 1958) as
a red line in each graph. Since the Kaplan-Meier estimator
is calibrated in theory, the values of the D-calibration and
the KM-calibration of this estimator should be regarded as
close to zeros. In this figure, the four scoring rules Cen-
log (SCen−logdefined in Eq. (4)), Cen-Brier ( SCen−Brier
defined in Eq. (8)), Cen-RPS ( SCen−RPSdefined in Eq. (9)),
and Portnoy ( SPortnoy defined in Eq. (2)) are proved to be
conditionally proper in this paper. Note that Cen-log is
similar to the scoring rule (Eq. (6)) that is proved to be
strictly proper in (Rindt et al., 2022) and Portnoy is pro-
posed in (Portnoy, 2003). This figure also contains the
results for other scoring rules in the state-of-the-art models
for survival analysis: DeepHit (Lee et al., 2018) with pa-rameter α= 1, DRSA (Ren et al., 2019) with parameter
α= 0.25, S-CRPS (Avati et al., 2019), and IPCW BS( t)
game model (Han et al., 2021). These four scoring rules are
not proved to be proper.
Figure 4 shows that the prediction performances of the four
extended scoring rules (Cen-log, Cen-Brier, Cen-RPS, and
Portnoy) were not similar, even though we prove that these
four scoring rules are conditionally proper and the outputs
are expected to be similar if the parameters ˆwand{ˆwi}B−1
i=0
are correct. The scoring rules Cen-log and Cen-Brier out-
performed the scoring rules Cen-RPS and Portnoy in dis-
crimination performance SCen−log−simple . These results
indicate that the accuracy of the estimated parameters ˆw
and{ˆwi}B−1
i=0by the IR algorithm are important when we
use these scoring rules in practice. The major difference be-
tween these scoring rules are that, whereas the set of param-
eters{wi}B−1
i=0in Cen-log and Cen-Brier usually satisfies
wi≈0orwi= 1, the set of parameters {wi}B
i=0in Cen-
RPS can take an arbitrary value 0≤wi≤1. The parameter
win Portnoy can also take an arbitrary value 0≤w≤1.
Therefore, Cen-log and Cen-Brier seem less sensitive to the
accuracy of the parameters than Cen-RPS and Portnoy. This
figure also shows that the other four scoring rules (DeepHit,
DRSA, S-CRPS, and IPCW BS( t)) performed worse than
Cen-log and Cen-Brier. Note that IPCW BS( t) model is
8Proper Scoring Rules for Survival Analysis
Table 1. Prediction performances of DeepHit (lower is better) with various αforB= 32 .
Metric Model flchain prostateSurvival support
SCen−log−simple DeepHit (α= 0) 1.5059±0.0513 1 .3609±0.0301 1 .8296±0.0446
DeepHit (α= 0.1) 1.5200±0.0398 1 .3644±0.0293 1 .8481±0.0453
DeepHit (α= 1) 1.5858±0.0495 1 .3813±0.0318 1 .9996±0.0525
DeepHit (α= 10) 2.0313±0.1648 1 .5688±0.0823 2 .3657±0.0441
D-calibration DeepHit (α= 0) 0.0003±0.0001 0 .0001±0.0000 0 .0062±0.0012
DeepHit (α= 0.1) 0.0005±0.0002 0 .0001±0.0000 0 .0056±0.0009
DeepHit (α= 1) 0.0008±0.0003 0 .0003±0.0001 0 .0062±0.0010
DeepHit (α= 10) 0.0138±0.0046 0 .0064±0.0035 0 .0179±0.0053
KM-calibration DeepHit (α= 0) 0.0213±0.0049 0 .0343±0.0102 0 .0288±0.0127
DeepHit (α= 0.1) 0.0264±0.0071 0 .0418±0.0139 0 .0249±0.0067
DeepHit (α= 1) 0.0362±0.0084 0 .0599±0.0341 0 .0545±0.0110
DeepHit (α= 10) 0.2077±0.0543 0 .4937±0.1772 0 .4273±0.1188
similar to the IR algorithm in that both of the algorithms are
used to estimate unknown parameters, but the loss function
of IPCW BS( t) model is not proved to be proper in terms of
the theory of scoring rules. With respect to the calibration
metrics, Cen-log and Cen-Brier showed comparable perfor-
mance with the Kaplan-Meier estimator. However, the other
scoring rules showed worse calibration performances for at
least one of D-calibration and KM-calibration.
Regarding the parameter αof DeepHit (Lee et al., 2018), we
conducted additional experiments by changing this parame-
ter. The loss function of DeepHit consists of two terms. The
first term is equal to the extension of the logarithmic score
SCen−log−simple , and the second term is used to improve a
ranking metric (i.e., a variant of C-index). The parameter
αis used to control the balance between these two terms,
and the weight for the second term is increased by using a
largeα. Note that the scoring rule SCen−log−simple is equiv-
alent to DeepHit with α= 0. Table 1 shows the results for
α∈ {0,0.1,1,10}. This table shows that the prediction per-
formances of DeepHit became worse as αincreases. This
means that we should set α= 0when we use DeepHit.
7. Conclusion
We discussed extensions of four scoring rules for survival
analysis, and we proved that these extensions are proper
if the parameter wor{wi}B−1
i=0is correct. These proofs
reduce the problem of estimating ˆFto the problem of esti-
mating the parameter wor{wi}B−1
i=0in proper scoring rules.
We also demonstrated that the models with SCen−logand
SCen−Brier as loss functions performed the best in our exper-
iments. These results indicate that it is better to use a proper
scoring rule that has low sensitivity on the parameter. In
addition, we clarified the hidden assumption in the proof of
the properness for SCen−cont−log(Rindt et al., 2022). This
suggests us to use a sufficiently large Bwhen we use it, andwe demonstrated that such Bcan be found by comparing the
prediction performances of SCen−log−simple andSCen−log
with various B.
References
Antolini, L., Boracchi, P., and Biganzoli, E. A time-
dependent discrimination index for survival data. Statis-
tics in Medicine , 24(24):3927–3944, 2005.
Avati, A., Duan, T., Zhou, S., Jung, K., Shah, N. H., and
Ng, A. Y . Countdown regression: Sharp and calibrated
survival predictions. In Proceedings of UAI 2019 , pp.
145–155, 2019.
Benedetti, R. Scoring rules for forecast verification. Ameri-
can Meteorological Society , 138(1):203–211, 2010.
Bengs, V ., H ¨ullermeier, E., and Waegeman, W. Pitfalls of
epistemic uncertainty quantification through loss minimi-
sation. In Proceedings of NeurIPS 2022 , 2022.
Blanche, P., Kattan, M. W., and Gerds, T. A. The c-index
is not proper for the evaluation of t-year predicted risks.
Biostatistics , 20(2):347–357, 2018.
Brier, G. W. Verification of forecasts expressed in terms of
probability. Monthly Weather Review , 78(1):1–3, 1950.
Chapfuwa, P., Tao, C., Li, C., Khan, I., Chandross, K. J.,
Pencina, M. J., Carin, L., and Henao, R. Calibration
and uncertainty in neural time-to-event modeling. IEEE
Transactions on Neural Networks and Learning Systems ,
34(4):1666–1680, 2020.
Cox, D. R. Regression models and life-tables. Journal of
the Royal Statistical Society, Series B , 34(2):187–220,
1972.
9Proper Scoring Rules for Survival Analysis
Dirick, L., Claeskens, G., and Baesens, B. Time to default
in credit scoring using survival analysis: a benchmark
study. Journal of the Operational Research Society , 68
(6):652–665, 2017.
Dispenzieri, A., Katzmann, J. A., Kyle, R. A., Larson, D. R.,
Therneau, T. M., Colby, C. L., Clark, R. J., Mead, G. P.,
Kumar, S., III, L. J. M., and Rajkumar, S. V . Use of
nonclonal serum immunoglobulin free light chains to
predict overall survival in the general population. Mayo
Clinic Proceedings , 87(6):517–523, 2012.
Gneiting, T. and Raftery, A. E. Strictly proper scoring
rules, prediction, and estimation. Journal of the American
Statistical Association , 102(477):359–378, 2007.
Goldstein, M., Han, X., Puli, A. M., Perotte, A., and Ran-
ganath, R. X-CAL: Explicit calibration for survival anal-
ysis. In Proceedings of NeurIPS 2020 , pp. 18296–18307,
2020.
Good, I. J. Rational decisions. Journal of the Royal Statis-
tical Society. Series B (Methodological) , 14(1):107–114,
1952.
Graf, E., Schmoor, C., Sauerbrei, W., and Schumacher, M.
Assessment and comparison of prognostic classification
schemes for survival data. Statistics in Medicine , 18
(17–18):2529–2545, 1999.
Haider, H., Hoehn, B., Davis, S., and Greiner, R. Effective
ways to build and evaluate individual survival distribu-
tions. Journal of Machine Learning Research , 21(85):
1–63, 2020.
Han, X., Goldstein, M., Puli, A., Wies, T., Perotte, A.,
and Ranganath, R. Inverse-weighted survival games. In
Proceedings of NeurIPS 2021 , pp. 2160–2172, 2021.
Harrell, F. E., Califf, R. M., Pryor, D. B., Lee, K. L., and
Rosati, R. A. Evaluating the yield of medical tests. Jour-
nal of the American Medical Association , 247(18):2543–
2546, 1982.
Kamran, F. and Wiens, J. Estimating calibrated individual-
ized survival curves with deep learning. In Proceedings
of AAAI 2021 , pp. 240–248, 2021.
Kaplan, E. L. and Meier, P. Nonparametric estimation from
incomplete observations. Journal of the American Statis-
tical Association , 53(282):457–481, 1958.
Kingma, D. P. and Ba, J. Adam: A method for stochastic
optimization. In Proceedings of ICLR 2015 , 2015.
Knaus, W. A., Harrell, Jr., F. E., Lynn, J., Goldman, L.,
Phillips, R. S., Connors, Jr., A. F., Dawson, N. V ., Fulker-
son, Jr., W. J., Califf, R. M., Desbiens, N., Layde, P., Oye,R. K., Bellamy, P. E., Hakim, R. B., and Wagner, D. P.
The SUPPORT prognostic model. Objective estimates
of survival for seriously ill hospitalized adults. Study to
understand prognoses and preferences for outcomes and
risks of treatments. Annals of Internal Medicine , 122(3):
191–203, 1995.
Koenker, R. and Bassett, Jr., B. Regression quantiles. Econo-
metrica , 46(1):33–50, 1978.
Koenker, R. and Hallock, K. F. Quantile regression. Journal
of economic perspectives , 15(4):143–156, 2001.
Kvamme, H., Borgan, O., and Scheel, I. Time-to-event pre-
diction with neural networks and Cox regression. Journal
of Machine Learning Research , 20(129):1–30, 2019.
Lee, C., Zame, W. R., Yoon, J., and van der Schaar, M.
DeepHit: A deep learning approach to survival analysis
with competing risks. In Proceedings of AAAI-18 , pp.
2314–2321, 2018.
Lu-Yao, G. L., Albertsen, P. C., Moore, D. F., Shih, W., Lin,
Y ., DiPaola, R. S., Barry, M. J., Zietman, A., O’Leary, M.,
Walker-Corkery, E., and Yao, S.-L. Outcomes of local-
ized prostate cancer following conservative management.
Journal of the American Medical Association , 302(11):
1202–1209, 2009.
Mura, A., Galavotti, M., Hykel, H., and de Finetti, B. Philo-
sophical Lectures on Probability: collected, edited, and
annotated by Alberto Mura . Synthese Library. Springer
Netherlands, 2008.
Neocleous, T., Branden, K. V ., and Portnoy, S. Correc-
tion to censored regression quantiles by S. Portnoy, 98
(2003), 1001–1012. Journal of the American Statistical
Association , 101(474):860–861, 2006.
Parmigiani, G. and Inoue, L. Decision Theory: Principles
and Approaches . Wiley Series in Probability and Statis-
tics. Wiley, 2009.
Pearce, T., Jeong, J.-H., Jia, Y ., and Zhu, J. Censored
quantile regression neural networks. In Proceedings of
NeurIPS 2022 , 2022.
Peng, L. Quantile regression for survival data. Annual
Review of Statistics and Its Application , 8:413–437, 2021.
Portnoy, S. Censored regression quantiles. Journal of the
American Statistical Association , 98(464):1001–1012,
2003.
R Core Team. R: A Language and Environment for Sta-
tistical Computing . R Foundation for Statistical Com-
puting, Vienna, Austria, 2016. URL https://www.
R-project.org/ .
10Proper Scoring Rules for Survival Analysis
Ren, K., Qin, J., Zheng, L., Yang, Z., Zhang, W., Qiu, L.,
and Yu, Y . Deep recurrent survival analysis. In Proceed-
ings of AAAI-19 , pp. 4798–4805, 2019.
Rindt, D., Hu, R., Steinsaltz, D., and Sejdinovic, D. Sur-
vival regression with proper scoring rules and monotonic
neural networks. In Proceedings of AISTATS 2022 , 2022.
Schlag, K. H., Tremewan, J., and van der Weele, J. J. A
penny for your thoughts: a survey of methods for eliciting
beliefs. Experimental Economics , 18:457–490, 2015.
Sonabend, R., Bender, A., and V ollmer, S. Avoiding c-
hacking when evaluating survival distribution predictions
with discrimination measures. Bioinformatics , 38(17):
4178–4184, 2022.
Tjandra, D. E., He, Y ., and Wiens, J. A hierarchical approach
to multi-event survival analysis. In Proceedings of AAAI
2021 , pp. 591–599, 2021.
Uno, H., Cai, T., Pencina, M. J., D’Agostino, R. B., and Wei,
L. J. On the C-statistics for evaluating overall adequacy
of risk prediction procedures with censored survival data.
Statistics in Medicine , 30(10):1105–1117, 2011.
Wang, P., Li, Y ., and Reddy, C. K. Machine learning for
survival analysis: A survey. ACM Computing Surveys , 51
(6):1–36, 2019.
Zheng, P., Yuan, S., and Wu, X. SAFE: A neural survival
analysis model for fraud early detection. In Proceedings
of AAAI-19 , pp. 1278–1285, 2019.
Zhong, Q., Mueller, J., and Wang, J.-L. Deep extended
hazard models for survival analysis. In Proceedings of
NeurIPS 2021 , 2021.
11Proper Scoring Rules for Survival Analysis
A. Proofs of Theorems
We give proofs of the theorems, which are omitted from the main body of this paper.
A.1. Portnoy’s Estimator
We show a proof of Theorem 4.5.
Proof. We consider a fixed c∼C, and we prove
E
t∼T|C=c[SPortnoy (ˆF,(z, δ);w, τ)]≥E
t∼T|C=c[SPortnoy (F,(z, δ);w, τ)] (10)
for these four cases separately.
• Case 1: c≤min{F−1(τ),ˆF−1(τ)}.
• Case 2: max{F−1(τ),ˆF−1(τ)}< c.
• Case 3: F−1(τ)< c≤ˆF−1(τ).
• Case 4: ˆF−1(τ)< c≤F−1(τ).
Note that, if Inequality (10) holds for any c∼C, we can marginalize the inequality with respect to C, and we have
E
t∼T,c∼C[SPortnoy (ˆF,(z, δ);w, τ)]≥E
t∼T,c∼C[SPortnoy (F,(z, δ);w, τ)],
which means that SPortnoy (ˆF,(z, δ);w, τ)is proper. Therefore, we prove Inequality (10) for the four cases.
Case 1. We prove the case for c≤min{F−1(τ),ˆF−1(τ)}. This means that τc≤τandw= (τ−τc)/(1−τc). Hence,
we have
SPortnoy (ˆF,(z, δ);w, τ) =(
ρτ(ˆF−1(τ), t) ift≤c,
wρτ(ˆF−1(τ), c) + (1 −w)ρτ(ˆF−1(τ), z∞)ift > c,
=(
(1−τ)(ˆF−1(τ)−t) ift≤c,
τ−τc
1−τc(1−τ)(ˆF−1(τ)−c) +1−τ
1−τcτ(z∞−ˆF−1(τ))ift > c,
=(
(1−τ)(ˆF−1(τ)−t) ift≤c,
−τc(1−τ)
1−τcˆF−1(τ)−(τ−τc)(1−τ)
1−τcc+(1−τ)τ
1−τcz∞ift > c.
By Assumption 3.1, we have Pr(t≤c|C=c) = Pr( t≤c) =τcandPr(t > c|C=c) = 1−τc. Hence, we have
E
t∼T|C=c[SPortnoy (ˆF,(z, δ);w, τ)] = Pr( t≤c|C=c)(1−τ)ˆF−1(τ)−(1−τ) E
t∼T|C=c,t≤c[t]
−Pr(t > c|C=c)τc(1−τ)
1−τcˆF−1(τ)−(τ−τc)(1−τ)
1−τcc+(1−τ)τ
1−τcz∞
=−(1−τ) E
t∼T|C=c,t≤c[t]−(τ−τc)(1−τ)
1−τcc+(1−τ)τ
1−τcz∞.
Since this value is the same for SPortnoy (ˆF,(z, δ);w, τ)andSPortnoy (F,(z, δ);w, τ), we have
E
t∼T|C=c[SPortnoy (ˆF,(z, δ);w, τ)] = E
t∼T|C=c[SPortnoy (F,(z, δ);w, τ)].
12Proper Scoring Rules for Survival Analysis
Case 2. We prove the case for max{F−1(τ),ˆF−1(τ)}< c.
IfF−1(τ)≤ˆF−1(τ)< c, then we have
SPortnoy (ˆF,(z, δ);w, τ) =(
ρτ(ˆF−1(τ), t) ift≤c,
wρτ(ˆF−1(τ), c) + (1 −w)ρτ(ˆF−1(τ), z∞)ift > c,
=

(1−τ)(ˆF−1(τ)−t) ift≤ˆF−1(τ),
−τ(ˆF−1(τ)−t) ifˆF−1(τ)< t≤c,
−wτ(ˆF−1(τ)−c)−(1−w)τ(ˆF−1(τ)−z∞)ift > c,
≥

(1−τ)(ˆF−1(τ)−t) ift≤F−1(τ),
−τ(ˆF−1(τ)−t) ifF−1(τ)< t≤c,
−τˆF−1(τ) +wτc+ (1−w)z∞ift > c,
where we used −τ(ˆF−1(τ)−t)≤(1−τ)(ˆF−1(τ)−t)when F−1(τ)< t≤ˆF−1(τ)for the inequality.
IfˆF−1(τ)≤F−1(τ)< c, then we have
SPortnoy (ˆF,(z, δ);w, τ) =(
ρτ(ˆF−1(τ), t) ift≤c,
wρτ(ˆF−1(τ), c) + (1 −w)ρτ(ˆF−1(τ), z∞)ift > c,
=

(1−τ)(ˆF−1(τ)−t) ift≤ˆF−1(τ),
−τ(ˆF−1(τ)−t) ifˆF−1(τ)< t≤c,
−wτ(ˆF−1(τ)−c)−(1−w)τ(ˆF−1(τ)−z∞)ift > c,
>

(1−τ)(ˆF−1(τ)−t) ift≤F−1(τ),
−τ(ˆF−1(τ)−t) ifF−1(τ)< t≤c,
−τˆF−1(τ) +wτc+ (1−w)z∞ift > c,
where we used −τ(ˆF−1(τ)−t)>(1−τ)(ˆF−1(τ)−t)when ˆF−1(τ)< t≤F−1(τ)for the inequality.
By Assumption 3.1, we have Pr(t≤F−1(τ)|C=c) = Pr( t≤F−1(τ)) = τ,Pr(F−1(τ)< t|C=c) = 1−τ, and
Pr(c < t|C=c) = 1−τc. Hence, we have
E
t∼T|C=c[SPortnoy (ˆF,(z, δ);w, τ)]≥Pr(t≤F−1(τ)|C=c)(1−τ)ˆF−1(τ)−(1−τ) E
t∼T|C=c,t≤F−1(τ)[t]
−Pr(F−1(τ)< t|C=c)τˆF−1(τ)
+Pr( c < t|C=c)(wτc+ (1−w)z∞)
=−(1−τ) E
t∼T|C=c,t≤F−1(τ)[t] + (1−τc)(wτc+ (1−w)z∞).
By using a similar argument, we have
E
t∼T|C=c[SPortnoy (F,(z, δ);w, τ)] =−(1−τ) E
t∼T|C=c,t≤F−1(τ)[t] + (1−τc)(wτc+ (1−w)z∞).
Note that this equation holds with equality.
Hence, we have
E
t∼T|C=c[SPortnoy (ˆF,(z, δ);w, τ)]≥E
t∼T|C=c[SPortnoy (F,(z, δ);w, τ)].
Case 3. We prove the case for F−1(τ)< c≤ˆF−1(τ).
13Proper Scoring Rules for Survival Analysis
We have
SPortnoy (ˆF,(z, δ);w, τ) =(
ρτ(ˆF−1(τ), t) ift≤c,
wρτ(ˆF−1(τ), c) + (1 −w)ρτ(ˆF−1(τ), z∞)ift > c,
=(
(1−τ)(ˆF−1(τ)−t) ift≤c,
w(1−τ)(ˆF−1(τ)−c)−(1−w)τ(ˆF−1(τ)−z∞)ift > c,
≥

(1−τ)(ˆF−1(τ)−t) ift≤F−1(τ),
−τ(ˆF−1(τ)−t) ifF−1(τ)< t≤c,
−wτ(ˆF−1(τ)−c)−(1−w)τ(ˆF−1(τ)−z∞)ift > c,
where we used (1−τ)(ˆF−1(τ)−t)≥ −τ(ˆF−1(τ)−t)when F−1(τ)< t≤candw(1−τ)(ˆF−1(τ)−c)≥
−wτ(ˆF−1(τ)−c)when t > c .
By using a similar argument, we have
SPortnoy (F,(z, δ);w, τ) =(
ρτ(F−1(τ), t) ift≤c,
wρτ(F−1(τ), c) + (1 −w)ρτ(F−1(τ), z∞)ift > c,
=

(1−τ)(F−1(τ)−t) ift≤F−1(τ),
−τ(F−1(τ)−t) ifF−1(τ)< t≤c,
−wτ(F−1(τ)−c)−(1−w)τ(F−1(τ)−z∞)ift > c,
Note that this equation holds with equality.
Hence, we have
E
t∼T|C=c[SPortnoy (ˆF,(z, δ);w, τ)]≥E
t∼T|C=c[SPortnoy (F,(z, δ);w, τ)].
Case 4. We prove the case for ˆF−1(τ)< c≤F−1(τ).
Regarding ˆF, we have
SPortnoy (ˆF,(z, δ);w, τ) =(
ρτ(ˆF−1(τ), t) ift≤c,
wρτ(ˆF−1(τ), c) + (1 −w)ρτ(ˆF−1(τ), z∞)ift > c,
=

(1−τ)(ˆF−1(τ)−t) ift≤ˆF−1(τ),
−τ(ˆF−1(τ)−t) ifˆF−1(τ)< t≤c,
−wτ(ˆF−1(τ)−c)−(1−w)τ(ˆF−1(τ)−z∞)ift > c,
>(
(1−τ)(ˆF−1(τ)−t) ift≤c,
−τˆF−1(τ) +wτc+ (1−w)τz∞ift > c,
where we used −τ(ˆF−1(τ)−t)>(1−τ)(ˆF−1(τ)−t)when ˆF−1(τ)< t≤cfor the inequality. By Assumption 3.1, we
have Pr(t≤c|C=c) = Pr( t≤c) =τcandPr(t > c|C=c) = 1−τc. Hence, we have
E
t∼T|C=c[SPortnoy (ˆF,(z, δ);w, τ)]>Pr(t≤c|C=c)(1−τ)ˆF−1(τ)−(1−τ) E
t∼T|C=c,t≤c[t]
+Pr( t > c|C=c)(−τˆF−1(τ) +wτc+ (1−w)τz∞)
> τ c(1−τ)ˆF−1(τ)−(1−τ) E
t∼T|C=c,t≤c[t]
−(1−τc)τˆF−1(τ) + (1 −τc)(wτc+ (1−w)τz∞)
>(τc−τ)ˆF−1(τ)−(1−τ) E
t∼T|C=c,t≤c[t] + (1−τc)(wτc+ (1−w)τz∞).
14Proper Scoring Rules for Survival Analysis
Regarding F, since w= (τ−τc)/(1−τc), we have
SPortnoy (F,(z, δ);w, τ) =(
ρτ(F−1(τ), t) ift≤c,
wρτ(F−1(τ), c) + (1 −w)ρτ(F−1(τ), z∞)ift > c,
=(
(1−τ)(F−1(τ)−t) ift≤c,
w(1−τ)(F−1(τ)−c)−(1−w)τ(F−1(τ)−z∞)ift > c,
=(
(1−τ)(F−1(τ)−t) ift≤c,
−τc(1−τ)
1−τcF−1(τ)−w(1−τ)c+ (1−w)τz∞ift > c,
By Assumption 3.1, we have Pr(t≤c|C=c) = Pr( t≤c) =τcandPr(t > c|C=c) = 1−τc. Hence, we have
E
t∼T|C=c[SPortnoy (ˆF,(z, δ);w, τ)] = Pr( t≤c|C=c)(1−τ)ˆF−1(τ)−(1−τ) E
t∼T|C=c,t≤c[t]
+Pr( t > c|C=c)(−τc(1−τ)
1−τcF−1(τ)−w(1−τ)c+ (1−w)τz∞)
=τc(1−τ)ˆF−1(τ)−(1−τ) E
t∼T|C=c,t≤c[t]
−τc(1−τ)ˆF−1(τ) + (1 −τc)(−w(1−τ)c+ (1−w)τz∞)
=−(1−τ) E
t∼T|C=c,t≤c[t] + (1−τc)(−w(1−τ)c+ (1−w)τz∞).
Therefore, since τc≤τandw= (τ−τc)/(1−τc), we have
E
t∼T|C=c[SPortnoy (ˆF,(z, δ);w, τ)]−E
t∼T|C=c[SPortnoy (F,(z, δ);w, τ)]
>((τc−τ)ˆF−1(τ) + (1 −τc)wτc) + (1 −τc)w(1−τ)c
= (τc−τ)(ˆF−1(τ)−c)
≥0.
A.2. Extension of Logarithmic Score
We show a proof of Theorem 4.6.
Proof. We consider a fixed c∼C, and let tbe a sample obtained from T. Letibe the index such that ζi≤c < ζ i+1. Since
Assumption 3.1 holds, we have Pr(ζj< t≤ζj+1|C=c) = Pr( ζj< t≤ζj+1) =F(ζj+1)−F(ζj) =fjfor any j < i ,
Pr(ζi< t≤c|C=c) =F(c)−F(ζi), and Pr(c < t|C=c) = Pr( c < t ) = 1−F(c). Hence, we have
E
t∼T|C=c[SCen−log(ˆF,(z, δ);{wk}B−1
k=0,{ζk}B
k=0)] = −X
j<iPr(ζj< t≤ζj+1|C=c) log ˆfj
−Pr(ζi< t≤c|C=c) log ˆfi
−Pr(c < t|C=c)
wilogˆfi+ (1−wi) log(1 −ˆF(ζi+1))
=−X
j<ifjlogˆfj
−(F(c)−F(ζi)) log ˆfi
−(1−F(c))
wilogˆfi+ (1−wi) log(1 −ˆF(ζi+1))
=−X
j≤ifjlogˆfj−(1−F(ζi+1)) log(1 −ˆF(ζi+1)),
15Proper Scoring Rules for Survival Analysis
where we used wi= (F(ζi+1)−F(c))/(1−F(c))for the last equality.
Hence, we have
E
t∼T|C=c[SCen−log(ˆF,(z, δ);{wk}B−1
k=0,{ζk}B
k=0)]−E
t∼T|C=c[SCen−log(F,(z, δ);{wk}B−1
k=0,{ζk}B
k=0)]
=−X
j≤ifj(log ˆfj−logfj)−(1−F(ζi+1))(log(1 −ˆF(ζi+1))−log(1−F(ζi+1)))
≥0, (11)
where we used the fact that the Kullback-Leibler divergence between two probability distributions is non-negative for the
inequality. This means that the inequality
−X
kpk(log ˆpk−logpk)≥0
holds for any two probability distributions pkandˆpkand the equality holds only if pk= ˆpkfor all k. Here, we use an
(i+ 2) -dimensional vector p= (p0, p1, . . . , p i+1); we set pk=fkfor all k≤iand we set pi+1= 1−F(ζi+1). Note that
the vectors pandˆpconstructed in this way are a probability distribution (i.e.,P
kpk= 1).
Since Inequality (11) holds for any c∼C, we marginalize the inequality with respect to C, and we have
E
t∼T,c∼C[SCen−log(ˆF,(z, δ);{wi}B−1
i=0,{ζi}B
i=0)]≥E
t∼T,c∼C[SCen−log(F,(z, δ);{wi}B−1
i=0,{ζi}B
i=0)],
which means that SCen−log(ˆF,(z, δ);{wi}B−1
i=0,{ζi}B
i=0)is proper.
A.3. Extension of Brier Score
We show a proof of Theorem 4.7.
Proof. We consider a fixed c∼C, and let tbe a sample obtained from T. Letjbe the index such that ζj< c≤ζj+1. Since
Assumption 3.1 holds, we have Pr(ζi< t≤ζi+1|C=c) = Pr( ζi< t≤ζi+1) =F(ζi+1)−F(ζi) =fifor any i < j ,
Pr(ζj< t≤c|C=c) =F(c)−F(ζj), and Pr(c < t|C=c) = Pr( c < t ) = 1−F(c). Hence, we have
E
t∼T|C=c[SCen−Brier(ˆF,(z, δ);{wk}B−1
k=0,{ζk}B
k=0)]
=X
i<jPr(ζi< t≤ζi+1|C=c)
(1−ˆfi)2+X
k̸=iˆf2
k

+Pr( ζj< t≤c|C=c)
(1−ˆfj)2+X
k̸=jˆf2
k

+Pr( c < t|C=c)
wj(1−ˆfj)2+ (1−wj)ˆf2
j+X
i<jˆf2
i+X
i>j
wi(1−ˆfi)2+ (1−wi)ˆf2
i

=X
i<jfi
(1−ˆfi)2+X
k̸=iˆf2
k
+ (F(c)−F(ζj))
(1−ˆfj)2+X
k̸=jˆf2
k

+(1−F(c))
wj(1−ˆfj)2+ (1−wj)ˆf2
j+X
i<jˆf2
i+X
i>j
wi(1−ˆfi)2+ (1−wi)ˆf2
i

=X
i
fi(1−ˆfi)2+ (1−fi)ˆf2
i
=X
i(ˆf2
i−2fiˆfi+fi),
16Proper Scoring Rules for Survival Analysis
where we used
wi=

0ifδ= 1andζi+1< z=t,
1ifδ= 1andζi< z=t≤ζi+1,
0ifz≤ζi
for the first equality and
wi=(
(F(ζi+1)−F(c))/(1−F(c))ifδ= 0andi=j,
fi/(1−F(c)) ifδ= 0andi > j
for the last equality.
Hence we have
E
t∼T|C=c[SCen−Brier(ˆF,(z, δ);{wi}B−1
i=0,{ζi}B
i=0)]−E
t∼T|C=c[SCen−Brier(F,(z, δ);{wi}B−1
i=0,{ζi}B
i=0)]
=X
i(ˆf2
i−f2
i−2fi(ˆfi−fi))
=X
i(ˆfi−fi)2
≥0. (12)
Note that the equality holds only if ˆfi=fiholds for all i.
Since Inequality (12) holds for any c∼C, we have
E
t∼T,c∼C[SCen−Brier(ˆF,(z, δ);{wi}B−1
i=0,{ζi}B
i=0)]≥E
t∼T,c∼C[SCen−Brier(F,(z, δ);{wi}B−1
i=0,{ζi}B
i=0)],
which means that SCen−Brier(ˆF,(z, δ);{wi}B−1
i=0,{ζi}B
i=0)is proper.
B. Additional Experiments
We investigated the differences of the prediction performances between SCen−log(defined in Eq. (4)) and SCen−log−simple
(defined in Eq. (5)) by using SCen−log−simple , D-calibration, and KM-calibration as metrics to determine the parameter B.
Tables 2– 4 show the results for B= 8,16,32, respectively, where each number shows the mean and variance of the values
obtained by five random runs and the bold numbers were used to emphasize the difference between two scoring rules. These
results show that the prediction performances of these two scoring rules were similar for the prostateSurvival and support
datasets even for B= 8. However, they showed different prediction performances for the flchain dataset for B= 8and
B= 16 , but the performance difference was negligible for B= 32 . Therefore, we used B= 32 in the other experiments in
this paper.
17Proper Scoring Rules for Survival Analysis
Table 2. Comparison between two extensions of logarithmic score for B= 8.
Metric Loss Function flchain prostateSurvival support
SCen−log−simple SCen−log 6.4618±0.1204 1 .3460±0.0476 1 .5422±0.0704
SCen−log−simple 6.4176±0.1266 1 .3447±0.0451 1 .5368±0.0701
D-calibration SCen−log 0.0045±0.0004 0 .0002±0.0000 0 .0370±0.0032
SCen−log−simple 0.0127±0.0013 0 .0002±0.0001 0 .0349±0.0024
KM-calibration SCen−log 0.0048±0.0026 0 .0048±0.0028 0 .0057±0.0027
SCen−log−simple 0.0614±0.0081 0 .0083±0.0024 0 .0061±0.0033
Table 3. Comparison between two extensions of logarithmic score for B= 16 .
Metric Loss Function flchain prostateSurvival support
SCen−log−simple SCen−log 3.6774±0.0386 1 .2880±0.0247 1 .6017±0.0733
SCen−log−simple 3.6676±0.0424 1 .3447±0.0451 1 .6008±0.0731
D-calibration SCen−log 0.0005±0.0002 0 .0001±0.0000 0 .0147±0.0020
SCen−log−simple 0.0013±0.0004 0 .0002±0.0000 0 .0143±0.0021
KM-calibration SCen−log 0.0117±0.0046 0 .0142±0.0036 0 .0149±0.0080
SCen−log−simple 0.0162±0.0049 0 .0158±0.0063 0 .0158±0.0100
Table 4. Comparison between two extensions of logarithmic score for B= 32 .
Metric Loss Function flchain prostateSurvival support
SCen−log−simple SCen−log 1.5054±0.0508 1 .3608±0.0295 1 .8307±0.0452
SCen−log−simple 1.5059±0.0513 1 .3609±0.0301 1 .8296±0.0446
D-calibration SCen−log 0.0003±0.0001 0 .0001±0.0000 0 .0063±0.0009
SCen−log−simple 0.0003±0.0001 0 .0001±0.0000 0 .0062±0.0012
KM-calibration SCen−log 0.0206±0.0049 0 .0312±0.0084 0 .0299±0.0115
SCen−log−simple 0.0213±0.0049 0 .0343±0.0102 0 .0288±0.0127
18