Zero-Sum Games between Large-Population Teams:
Reachability-based Analysis under Mean-Field Sharing
Yue Guan∗Mohammad Afshari†Panagiotis Tsiotras‡
ABSTRACT
This work studies the behaviors of two large-population teams competing in a discrete environ-
ment. The team-level interactions are modeled as a zero-sum game while the agent dynamics
within each team is formulated as a collaborative mean-field team problem. Drawing inspira-
tion from the mean-field literature, we first approximate the large-population team game with its
infinite-population limit. Subsequently, we construct a fictitious centralized system and transform
the infinite-population game to an equivalent zero-sum game between two coordinators. We study
the optimal coordination strategies for each team via a novel reachability analysis and later translate
them back to decentralized strategies that the original agents deploy. We prove that the strategies are
ϵ-optimal for the original finite-population team game, and we further show that the suboptimality
diminishes when team size approaches infinity. The theoretical guarantees are verified by numerical
examples.
1 Introduction
Multi-agent decision-making arises in many applications, ranging from warehouse robots [Li et al., 2021], surveillance
missions [Tian et al., 2020] to organizational economics [Gibbons et al., 2013]. While the majority of the literature for-
mulates the problems within either the cooperative or competitive settings, results on mixed collaborative-competitive
team behaviors are relatively sparse. In this work, we consider a competitive team game, where two teams, each
consisting of a large number of intelligent agents, compete at the team level, while the agents within each team col-
laborate. Such hierarchical interactions are of particular interest to military operations [Tyler et al., 2020] and other
multi-agent systems that operate in adversarial environments.
There are two major challenges when trying to solve such competitive team problems:
1. Large-population team problems are computationally challenging since the solution complexity increases
exponentially with the number of agents. It is well-known that team optimal control problems belong to the
NEXP complexity class [Bernstein et al., 2002].
2. Competitive team problems are conceptually challenging due to the elusive nature of the opponent team. In
particular, one may want to impose assumptions on the opponent team to obtain tractable solutions, but these
assumptions may not be valid. It is thus unclear whether one can deploy techniques that are readily available
in the large-population game literature.
The scalability challenge is also present in dynamic games; however, it has been successfully resolved for a class
of models known as mean-field games (MFGs) [Huang et al., 2006; Lasry and Lions, 2007]. The salient feature of
mean-field games is that the selfish agents are weakly coupled in their dynamics and rewards, and coupling is only
through the mean field (i.e., the empirical distribution of the agents). When the population is large, the game can be
approximately solved by considering the infinite-population limit, at which point the interactions among agents are
reduced to a game between a typical agent and the infinite population.
∗Yue Guan is a PhD student with the School of Aerospace Engineering, Georgia Institute of Technology, Atlanta, GA, USA.
Email: yguan44@gatech.edu
†Mohammad Afshari is a Postdoctoral Fellow with the Institute for Robotics and Intelligent Machines, Georgia Institute of
Technology, Atlanta, GA, USA. Email: mafshari@gatech.edu
‡Panagiotis Tsiotras is the David & Andrew Lewis Chair Professor with the School of Aerospace Engineering, Georgia Institute
of Technology, Atlanta, GA, USA. Email: tsiotras@gatech.eduarXiv:2303.12243v3  [eess.SY]  22 Feb 2024Published as a conference paper at AAAI 2024
The mean-field approximation idea has been extended to the single-team setting [Arabneydi and Mahajan, 2014],
where a group of homogeneous agents are weakly coupled but receive the same team reward and thus collaborate.
Under the assumption that all agents apply the same strategy, a dynamic programming decomposition is developed
leveraging the common-information approach [Nayyar et al., 2013] so that all agents within the team deploy the same
strategy prescribed by a fictitious coordinator. The identical strategy assumption significantly simplifies the problem,
but the optimality of such approach is only guaranteed for the linear-quadratic models [Arabneydi and Mahajan, 2015].
However, in competitive team setting, although one may restrict the strategies used by her/his team to be identical,
extending the same assumption to the opponent team may lead to a substantial underestimation of the opponent’s
capabilities and thus requires further justification.
1.1 Main Contributions
We address the two aforementioned challenges for the class of discrete zero-sum mean-field team games (ZS-MFTGs),
which is an extension to the mean-field (single) team problems. Importantly, ZS-MFTG models competitive team
behaviors and draws focus to the justifiable approximation of the opponent team strategies. We focus on discrete-time
models with finite state and action spaces, different from the continuous setting in the concurrent work [Sanjari et al.,
2023].
We develop a dynamic program that constructs ϵ-optimal strategies to the proposed large-population competitive team
problem. Notably, our approach finds an optimal solution at the infinite-population limit and considers only identical
team strategies. This avoids both the so-called “curse of dimensionality” issue in multi-agent systems and the book-
keeping of individual strategies. Our main results provide a sub-optimality bound on the exploitability for our proposed
solution in the original finite-population game, even when the opponent team is allowed to use non-identical strategies.
Specifically, we show that the sub-optimality decreases at the rate of O(N−0.5), where Nis the size of the smaller
team.
Our results stem from a novel reachability-based analysis of the mean-field approximation. In particular, we show that
any finite-population team behavior can be effectively approximated by an infinite-population team that uses identical
team strategies. This result allows us to approximate the original problem with two competing infinite-population
teams and transform the resulting problem into a zero-sum game between two fictitious coordinators. Specifically,
each coordinator observes the mean-fields (state distributions) of both teams and prescribes a local policy for all
agents within its team (see Figure 1). Such transformation leads to a simple dynamic program based on the common-
information approach [Nayyar et al., 2013].
1.2 Related Work
Mean-field games. The mean-field game model was introduced in [Huang et al., 2006, 2007; Lasry and Lions,
2007] to address the scalability issues in large population games. The salient feature of mean-field games is that
selfish agents are weakly-coupled in their dynamics and rewards through the state distribution (mean-field) of the
agents. If the population is sufficiently large, then an approximately optimal solution for these models can be obtained
by solving the infinite-population limit, and the solution in this limiting case is the mean-field equilibrium (MFE). The
infinite-population game is easier to solve since, when the population is asymptotically large, the action of a single
agent does not affect the mean-field. In the continuous setting, the MFE is characterized by a Hamilton-Jacobi-Bellman
equation (HJB) coupled with a transport equation [Lasry and Lions, 2007]. The HJB equation describes the optimality
conditions for the strategy of a typical agent, and the transport equation captures the evolution of the mean-field. The
existence and uniqueness of the MFE have been established in [Huang et al., 2006]. Two desirable features of the
MFE are: (i) the resultant strategy is decentralized and identical across all agents, i.e., an agent selects its action only
based on the local information of its own state, and no information regarding the mean-field is needed; and (ii) the
complexity of the MFE solution does not depend on the number of agents. The mean-field results have been extended
to discrete environments in [Cui and Koeppl, 2021; Guan et al., 2022], using entropy-regularization. We refer the
readers to [Lauri `ere et al., 2022] for a detailed overview of the main results in the mean-field game literature.
The main differences between our setup and the current mean-field game literature are the following: (i) we seek team-
optimal strategies, while MFG seeks Nash equilibrium strategies. In particular, we provide performance guarantees
when the whole opponent team deviates, while MFG only considers single-agent deviations. (ii) The MFE assumes
that all agents apply the same strategy thus the mean-field flow can be solved offline. As a result, the MFE strategy
is open-loop to the mean-field. However, under the setting in this paper, different opponent team strategies can lead
to different mean-field trajectories, and thus our optimal strategy requires feedback on the mean-field to respond to
different strategies deployed by the opponent team.
Mean-field teams. The single-team problem under with mean-field-type coupling has been considered in [Arab-
neydi and Mahajan, 2014], where all agents receive the same team reward and thus the problem is collaborative. ItPublished as a conference paper at AAAI 2024
transforms the team problem into a hierarchical control structure, where a fictitious coordinator broadcasts a local
policy based on the distribution of agents, and the individual agents then act according to this policy. The work [Arab-
neydi and Mahajan, 2014] assumes that all agents within the team apply the same strategy and the optimality for the
finite-population game is only guaranteed in the LQG setting [Mahajan and Nayyar, 2015]. Our work extends this
formulation to the two-team zero-sum setting and justifies the identical team strategy assumptions on the opponent
team.
The concurrent work [Sanjari et al., 2023] studies a similar team against team problem but in a continuous state and
action setting. Through modeling the randomized policies as Borel probability measures, the authors showed that
under suitable topology induced by certain information structure, a Nash equilibrium exists for the team game, and
it is exchangeable for the finite population and symmetric (identical) for the infinite population. It is also shown that
common randomness is not necessary for an approximate Nash equilibrium for large population games. Our work
differs in the following aspects: (i) The concurrent work [Sanjari et al., 2023] relies on the convexity of the team
best-response correspondence and the Kakutani fixed point theorem to establish the existence of a Nash equilibrium.
However, due to discrete nature of our formulation, the convexity no longer holds, and thus a Nash equilibrium may
not exist as shown in Numerical Example 1. Consequently, we developed a novel reachability-based analysis to study
the single-sided optimality based on the lower and upper game values; (ii) the mean-field sharing information struc-
ture allows an easy transformation of the team against team problem into a zero-sum continuous game between two
coordinators, which can be solved via dynamic programming. (iii) our reachability-based approach and the addi-
tional Lipschitz assumptions allow us to provide the convergence rate of the finite-population team performance to its
infinite-population limit.
Markov team games and information structure. Different from mean-field games, team problems focus on fully
cooperative scenarios, where all agents share a common reward function. The theory of teams was first investigated
in [Marschak, 1955; Radner, 1962]. From a game-theoretic perspective, this cooperative setting can also be viewed as
a special case of Markov potential games [Zazo et al., 2016], with the potential function being the common reward.
When all agents have access to the present and past observations along with the past actions of all other agents, such a
system is equivalent to a centralized one, which enables the use of single-agent algorithms, such as value iteration or
Q-learning [Bertsekas and Tsitsiklis, 1996]. However, such a centralized system suffers from scalability issues, since
the joint state and action spaces grow exponentially with the number of agents. Furthermore, in many applications,
decentralization is a desirable or even required trait, due to reasons such as limited communication bandwidth, privacy,
etc. This work, therefore, investigates the decentralized team control problem under the information structure known
as the mean-field sharing [Arabneydi and Mahajan, 2014].
Information structure is one of the most important characteristics of a multi-agent system, since it largely determines
the tractability of the corresponding decentralized decision problem [Papadimitriou and Tsitsiklis, 1985; Ho, 1980].
In [Witsenhausen, 1971], information structures are classified into three classes: classical, quasi-classical, and non-
classical. The characterizing feature of the classical information structure is centralization of information, i.e., all
agents know the information available to all agents acting in the past. A system is called quasi-classical or partially
nested if the following condition holds: If agent ican influence agent j, then agent jmust know the information
available to agent i. All other information structures are non-classical. In the team game literature, information
structures that are commonly used include state sharing [Aicardi et al., 1987], belief sharing [Y ¨uksel, 2009], partial-
history sharing [Nayyar et al., 2013], mean-field sharing [Arabneydi and Mahajan, 2014], etc. This work, in particular,
assumes a mean-field sharing structure, where each agent observes its local state and the mean-fields of both teams.
As the agents do not receive information regarding the other agents’ individual states, the mean-field sharing is a
non-classic information structure, thus posing a challenge to attain tractable solutions.
As decentralized team problems belong to the NEXP complexity class [Bernstein et al., 2002], no efficient algorithm
is available, in general. Nonetheless, it is possible to develop a dynamic programming decomposition for specific
information structures. Three such generic approaches are discussed in [Mahajan et al., 2012]: namely, person-by-
person approach, designer’s approach, and common-information approach. While we use the common-information
approach in this work, we refer the interested reader to [Mahajan et al., 2012] for the other two approaches.
Team problems in a mixed competitive-cooperative setting have also been considered. For example, [Lagoudakis and
Parr, 2002] proposed a centralized learning algorithm for zero-sum Markov games in which each side is composed of
multiple agents collaborating against an opponent team of agents. Later, in [Hogeboom-Burr and Y ¨uksel, 2023], the
authors studied the information structure and the existence of equilibria in games involving teams against teams.
Common-information approach. The common information-based approach was proposed in [Nayyar et al., 2013]
to investigate optimal strategies in decentralized stochastic control problems with partial history-sharing information
structure. In this model, each agent shares part of its information with other agents at each time step. Based on the
common information available to all agents, a fictitious coordinator is designed. It is shown that the optimal control
problem at the coordinator level is a centralized Partially Observable Markov Decision Process (POMDP) and canPublished as a conference paper at AAAI 2024
be solved using dynamic programming. The coordinator solves the POMDP problem and chooses a prescription for
each agent that maps that agent’s local information to its actions. The common information approach was used to
solve decentralized stochastic control problems including the control sharing information structure [Mahajan, 2011],
mean-field teams [Arabneydi and Mahajan, 2014], and LQG systems [Mahajan and Nayyar, 2015].
Most relevant to this work, [Kartik et al., 2021] considered a general model of zero-sum stochastic game between
two competing teams, where the information available to each agent can be decomposed into common and private
information. Exploiting this specific information structure, an expanded game between two virtual players (fictitious
coordinators) is constructed based on the sufficient statistics known as the common information belief. The authors
showed that the expanded game can be solved via dynamic programming, and the upper and lower values of the
expanded games provide bounds for the original game values. Although we utilize a similar common information
approach as in [Kartik et al., 2021], this work focuses more on the large-population aspect of the problem, i.e., the
mean-field limits of the team games along with the performance guarantees under the mean-field approximation.
In this work, we treat the mean fields of both (infinite-population) teams as common information. Instead of having
a single coordinator, we introduce two fictitious coordinators, one for each team, and, consequently, formulate an
equivalent zero-sum game between the two coordinators. Since we consider the equivalent coordinator game at the
infinite-population limit, the two mean fields provide enough information to characterize the status of the system
under the mean-field dynamics, which leads to a full-state information structure instead of a partially observable one.
Although the original environment is discrete, the resultant zero-sum coordinator game has continuous state and action
spaces4. Under certain assumptions, we show that the optimal max-min and min-max value functions are Lipschitz
continuous with respect to the mean-fields. This justifies the solution of the continuous game via a discretization
scheme.
1.3 Road Map
Our approach can be summarized in the road map in Figure 1.
1. In Section 2, we present the formulation of the zero-sum mean-field team games (ZS-MFTG) with finite
number of agents .
2. In Section 3, we employ the mean-field approximation and considers the infinite-population limit where
identical team strategies are deployed.
3. In Section 4, we use the common information approach [Nayyar et al., 2013] and reformulate the infinite-
population problem as a zero-sum game between two coordinators . This allows us to efficiently find the
max-min and min-max optimal strategies through dynamic programming.
4. In Section 5, we establish that the optimal strategies for the coordinator game (computed under the com-
mon information approach, mean-field approximation, and identical team strategies) remain ϵ-optimal for the
original finite-population game. Specifically, if one team applies the proposed strategy, the opponent team
can only increase its performance by O 
1/p
min{N1, N2}
, even if the opponent team uses a non-identical
team strategy to exploit.
Finite-population systemInfinite-population systemZero-SumCoordinator GameCommon InformationMean Field Approximation(Identical team strategy)𝝐-optimal Strategy(Nonidentical team strategy)
Figure 1: The road map of the proposed approach.
4The joint state of the coordinator game is the mean-fields of the two teams, which are distributions over state space and are
continuous objects. The actions used by the coordinators are the local policies, which are distributions over the action spaces and
are also continuous.Published as a conference paper at AAAI 2024
1.4 Notations
We use the shorthand notation [n]to denote the set of indices {1,2, . . . , n }. The indicator function is denoted as 1· 
·
,
such that 1a 
b
= 1ifa=band0otherwise. To distinguish between random variables and their realizations, we use
uppercase letters to denote random variables (e.g. XandM) and lowercase letters to denote their realizations (e.g.
xandµ). We use bold letters to denote vectors, e.g. Y= (Y1, . . . , Y n). For a finite set E, we denote the space of
all probability measures over EasP(E)and equip it with the total variation norm dTV[Chung, 2001]. Additional
important notations used in this paper are summarized in Table 1.
Table 1: Important Notation.
Notation Description Population Size
N1/N2/N Number of agents in Blue / Red team / whole system Finite
ρ Team size ratio N1/N Finite
XN1
i,t(xN1
i,t)/YN2
j,t(yN2
j,t) State of a single Blue/Red agent (realization) Finite
MN1
t(µN1
t)/NN2
t(νN2
t)Empirical distribution of Blue/Red team (realization) Finite
µρ
t/νρ
t Mean-field of Blue/Red team Infinite
ϕt/ψt Identical Blue/Red team policy Finite & Infinite
ϕ/ψ Identical Blue/Red team strategy Finite & Infinite
ϕN1
t/ψN2
t Non-identical Blue/Red team policy Finite
ϕN1/ψN2 Non-identical Blue/Red team strategy Finite
πt/σt Identical Blue/Red team local policy Infinite
α/β Blue/Red coordinator strategy Infinite
rρ
t Game reward Finite & Infinite
fρ
t/gρ
t Dynamics of Blue/Red agents Finite & Infinite
JNFinite-population game value Finite
Jρ
cor Coordinator game value Infinite
Rρ
µ,t/Rρ
ν,t Reachablity correspondence for Blue/Red mean-field Infinite
2 Problem Formulation
Consider a discrete-time system with two large teams of agents that operate over a finite time horizon T. The Blue
team consists of N1homogeneous agents, and the Red team consists of N2homogeneous agents. The total system
size is denoted as N=N1+N2. We use ρ=N1/Nto denote the ratio between the size of the Blue team and the total
number of agents (Blue and Red agents combined). Let XN1
i,t∈ X andUN1
i,t∈ U be the random variables representing
the state and action, respectively, taken by Blue agent i∈[N1]at time t. Here, XandUrepresent the finite individual
state and action space for each Blue agent, independent of iandt. Similarly, we use YN2
j,t∈ Y andVN2
j,t∈ V to
denote the individual state and action of Red agent j∈[N2]. The joint state and action of the Blue team are denoted
asXN1
t=
XN1
1,t, . . . , XN1
N1,t
andUN1
t=
UN1
1,t, . . . , UN1
N1,t
, respectively. Similarly, the Red team’s joint state and
action are denoted as YN2
t, and VN2
t. We use XN1
−i,t=
XN1
1,t, . . . , XN1
i−1,t, XN1
i+1,t, . . . , XN1
N1,t
to denote the joint
state of the Blue team excluding Blue agent i, andYN2
−j,tis defined similarly.
Definition 1 (Empirical Distribution) .Theempirical distribution (ED) for the Blue and Red teams are defined as
MN1
t(x) =1
N1N1X
i=11x(XN1
i,t), x∈ X, (1a)
NN2
t(y) =1
N2N2X
j=11y(YN2
j,t), y∈ Y. (1b)
Notice that MN1
t(x)gives the fraction of Blue agents at state x. Hence, the random vector MN1
t= [MN1
t(x)]x∈Xis
a probability measure, i.e., MN1
t∈ P(X). Similarly, we have NN2
t∈ P(Y). Since finite state and action spaces are
considered, we have P(X) = ∆ |X|andP(Y) = ∆ |Y|, where ∆kis the k-dimensional standard simplex.Published as a conference paper at AAAI 2024
We introduce two operators Empµ:XN1→ P(X)andEmpν:YN2→ P(X)to denote operation performed in (1)
that relates the joint states and their corresponding EDs:
µN1
t= Empµ(xN1
t), νN2
t= Empν(yN2
t). (2)
To measure the distance between two distributions (probability measures), we use the total variation.
Definition 2. For a finite set E, the total variation between two probability measures µ, µ′∈ P(E)is given by
dTV 
µ, µ′
=1
2X
e∈E|µ(e)−µ′(e)|=1
2∥µ−µ′∥1. (3)
2.1 Dynamics
We consider a weakly-coupled dynamics, where the dynamics of each individual agent is coupled with other agents
through the EDs (empirical distributions). For Blue agent i, its stochastic transition is governed by a transition kernel
fρ
t:X × X × U × P (X)× P(Y)→[0,1]and satisfies
P(XN1
i,t+1=xN1
i,t+1|UN1
i,t=uN1
i,t,XN1
t=xN1
t,YN2
t=yN2
t) =fρ
t(xN1
i,t+1|xN1
i,t, uN1
i,t, µN1
t, νN2
t), (4)
where µN1
t= Empµ(xN1
t)andνN2
t= Empν(yN2
t). Note that the transition kernel has ρ=N1/Ndepends on the
ratio between the sizes of the two teams. Similarly, the transition of Red agent jfollows
P(YN2
j,t+1=yN2
j,t+1|VN2
j,t=vN2
j,t,XN1
t=xN1
t,YN2
t=yN2
t) =gρ
t(yN2
j,t+1|yN2
j,t, vN2
j,t, µN1
t, νN2
t). (5)
Assumption 1. The transition kernels fρ
tandgρ
tareLftandLgt-Lipschitz continuous, respectively. Formally, for all
µ, µ′∈ P(X)andν, ν′∈ P(Y)and for all t∈ {0, . . . , T −1}, there exist positive constants LftandLgtsuch that
X
x′∈X|fρ
t(x′|x, u, µ, ν )−fρ
t(x′|x, u, µ′, ν′)| ≤Lft
dTV 
µ, µ′
+ dTV 
ν, ν′
∀x∈ X, u∈ U,
X
y′∈Y|gρ
t(y′|y, v, µ, ν )−gρ
t(y′|y, v, µ′, ν′)| ≤Lgt
dTV 
µ, µ′
+ dTV 
ν, ν′
∀y∈ Y, v∈ V.
2.2 Reward Structure
Under the team-game framework, agents in the same team share the same reward. Similar to the dynamics, we consider
a weakly-coupled team reward
rρ
t:P(X)× P(Y)→[−Rmax, Rmax]. (6)
Assumption 2. The reward function is Lr-Lipschitz continuous. Formally, for all µ, µ′∈ P(X),ν, ν′∈ P(Y), and
for all t∈ {0, . . . , T }, there exists a positive constant Lrsuch that
|rρ
t(µ, ν)−rρ
t(µ′, ν′)| ≤Lr
dTV 
µ, µ′
+ dTV 
ν, ν′
. (7)
Under the zero-sum reward structure, we let the Blue team maximize the reward while the Red team minimizes it.
2.3 Information Structure.
We assume a mean-field sharing information structure [Arabneydi and Mahajan, 2015]. At each time step t, Blue
agent iobserves its own state XN1
i,tand the EDs MN1
tandNN2
t. Similarly, Red agent jobserves YN2
j,t,MN1
tand
NN2
t. Viewing the EDs as aggregated states of the teams, we consider the following mixed Markov policies:
ϕi,t:U × X × P (X)× P(Y)→[0,1],
ψj,t:V × Y × P (X)× P(Y)→[0,1],(8)
where ϕi,t(u|XN1
i,t,MN1
t,NN2
t)represents the probability that Blue agent iselects action ugiven its state XN1
i,tand
the team distributions MN1
tandNN2
t. Note that agent’s individual state is a private information, while the team EDs
are the common information shared among all agents.
An individual strategy is defined as a time sequence ϕi={ϕi,t}T
t=0. A Blue team strategy ϕN1={ϕi}N1
i=1is the
collection of individual strategies used by each Blue agent. We use ΦtandΦto denote, respectively, the set ofPublished as a conference paper at AAAI 2024
individual policies and strategies available to each Blue agent.5The set of Blue team strategies is then defined as the
Cartesian product ΦN1=×N1
i=1Φ. The notations extend naturally to the Red team.
In summary, an instance of a zero-sum mean-field team game is defined as the tuple:
ZS-MFTG =⟨X,Y,U,V, fρ
t, gρ
t, rρ
t, N1, N2, T⟩.
2.4 Optimization Problem.
Starting from the initial joint states xN1
0andyN2
0, the performance of any team strategy pair (ϕN1, ψN2)∈ΦN1×ΨN2
is quantified by the expected cumulative reward
JN,ϕN1,ψN2 
xN1
0,yN2
0
=EϕN1,ψN2"TX
t=0rρ
t(MN1
t,NN2
t)XN1
0=xN1
0,YN2
0=yN2
0#
, (9)
where MN1
t= Empµ(XN1
t)andNN2
t= Empν(YN2
t), and the expectation is with respect to the distribution induced
on all system variables by the strategy pair (ϕN1, ψN2).
Remark 1. Note that the value function in (9) takes the joint states as arguments, different from the value function
in [Arabneydi and Mahajan, 2015] which takes the ED as its argument. The difference comes from the non-identical
team strategies allowed in our formulation, which requires each agent’s state and index information to sample actions
and predict the game’s evolution. Arabneydi and Mahajan assume an identical team strategy, and hence the ED is an
information state (sufficient statistics) that characterizes the value function of the finite-population game. A counter-
example that shows the ED is not an information state under non-identical team strategy is presented in Appendix A.3.
When the maximizing Blue team considers its worst-case performance, we have the following max-min optimization:
JN∗= max
ϕN1∈ΦN1min
ψN2∈ΨN2JN,ϕN1,ψN2, (10)
where JN∗is the lower game value for the finite-population game. Note that the game value may not always exist
(max-min value differs from min-max value) [Elliott and Kalton, 1972]. In Section 4.5, we present a special case
of the ZS-MFTG where the game value exists at the infinite-population limit. For the general case, we consider the
following optimality condition for the Blue team strategy.
Definition 3. A Blue team strategy ϕN1∗isϵ-optimal if
JN∗≥min
ψN2∈ΨN2JN,ϕN1∗,ψN2≥JN∗−ϵ.
The strategy ϕN1∗is optimal if ϵ= 0.
Similarly, the minimizing Red team considers a min-max optimization problem, which leads to the upper game value
as follows
¯JN∗= min
ψN2∈ΨN2max
ϕN1∈ΦN1JN,ϕN1,ψN2. (11)
The definitions of optimality and ϵ-optimality extend naturally to the Red team strategies.
The rest of the paper focuses on the performance analysis form the Blue team’s perspective (max-min optimization),
but the techniques developed are applicable to the Red team’s side due to the symmetry of the problem formulation.
2.5 Examples
A two-node example Consider a simple team game on a two-node graph in Figure 2. The state spaces are given
byX={x1, x2}andY={y1, y2}, and the action spaces are U={u1, u2}andV={v1, v2}. The Blue action u1
corresponds to staying on the current node and u2represents moving to the other node. The same connotations apply
to Red actions v1andv2.
The maximizing Blue team’s objective is to maximize its presence at node 2 (state x2), and hence rt(µ, ν) =µ(x2).
The Blue transition kernel at x1under u2is defined as
ft(x1|x1, u2, µ, ν) = 0 .5 
1− 
ρµ(x1)−(1−ρ)ν(y1)
,
ft(x2|x1, u2, µ, ν) = 0 .5 
1 + 
ρµ(x1)−(1−ρ)ν(y1)
.
5Since Blue agents have the same state and action spaces, they have the same policy space.Published as a conference paper at AAAI 2024
<latexit sha1_base64="ydhPtDgkQ6ql6mCWuxCrq89jB6A=">AAACb3icbZDfahNBFMYnW7Ux/mlaL7wQZDAIKpjultB6JYXeeFnBtIVsDLOzZ5uh82eZOdtmGfYpfBpv9Sl8jL6Bs2nANvXAwMf3O2fO4ctKKRzG8Z9OtPHg4aPN7uPek6fPnm/1t3dOnKkshzE30tizjDmQQsMYBUo4Ky0wlUk4zS6OWn56CdYJo79hXcJUsXMtCsEZBmvW/0hThAUuP/KZrKDxi+9JQ3dv+xbyxtfBnvUH8TBeFr0vkpUYkFUdz7Y7H9Lc8EqBRi6Zc5MkLnHqmUXBJTS9tHJQMn7BzmESpGYK3NQv1zb0bXByWhgbnka6dG9PeKacq1UWOhXDuVtnrfk/Nqmw+DT1QpcVguY3i4pKUjS0DYnmwgJHWQfBuBXhVsrnzDKOIcpequGKG6WYzn26aHzaLsgKv2iau6z+x+p1VgSWGZm39xvpi8BDusl6lvfFyd4w2R+Ovo4Gh59XOXfJK/KGvCMJOSCH5As5JmPCyQ/yk/wivzvX0cvodURvWqPOauYFuVPR+7+VSsHE</latexit>x1/y1<latexit sha1_base64="dBUSEKHsiK3BMaOegBtnYyj2DzI=">AAACb3icbZDfahNBFMYnq6012prqhReCDAahFUx3S2m9koI3XkYwbSEbw+zs2Wbo/FlmzrZZhn0Kn8ZbfQofwzdwNgnYph4Y+Ph+58w5fFkphcM4/t2JHjzc2Hy09bj75On2zrPe7vMzZyrLYcSNNPYiYw6k0DBCgRIuSgtMZRLOs6tPLT+/BuuE0V+xLmGi2KUWheAMgzXtvacpwhwXH/lMVtD4+bfDhh7c9i3kja+DPe3140G8KHpfJCvRJ6saTnc779Lc8EqBRi6Zc+MkLnHimUXBJTTdtHJQMn7FLmEcpGYK3MQv1jb0bXByWhgbnka6cG9PeKacq1UWOhXDmVtnrfk/Nq6w+DDxQpcVgubLRUUlKRrahkRzYYGjrINg3IpwK+UzZhnHEGU31XDDjVJM5z6dNz5tF2SFnzfNXVb/Y/U6KwLLjMzb+430ReAh3WQ9y/vi7HCQHA+Ovhz1Tz+uct4ir8gbskcSckJOyWcyJCPCyXfyg/wkvzp/opfR64guW6POauYFuVPR/l+ZHcHG</latexit>x2/y2
<latexit sha1_base64="gunQFF4kPY5kmORUpuV2n9C0szo=">AAACb3icbZDfahNBFMYnq9WatprqhReCDIZCW2i6K0W9KgVvvIxg2kI2DbOzZ9uh82eZOVuzDPsUPk1v9Sl8DN/A2SRgm3pg4OP7nTPn8GWlFA7j+HcnevR47cnT9Wfdjc2t5y962y9PnakshxE30tjzjDmQQsMIBUo4Ly0wlUk4y64/t/zsBqwTRn/DuoSJYpdaFIIzDNa0d0BThBnOP/KZrKDx1UXS0MO7voW88TfBnvb68SCeF30okqXok2UNp9ud/TQ3vFKgkUvm3DiJS5x4ZlFwCU03rRyUjF+zSxgHqZkCN/HztQ3dCU5OC2PD00jn7t0Jz5RztcpCp2J45VZZa/6PjSssPk280GWFoPliUVFJioa2IdFcWOAo6yAYtyLcSvkVs4xjiLKbavjOjVJM5z6dNT5tF2SFnzXNfVb/Y/UqKwLLjMzb+430ReAh3WQ1y4fi9P0g+TA4+nrUPzle5rxO3pB3ZJck5CM5IV/IkIwIJz/ILflJfnX+RK+jtxFdtEad5cwrcq+ivb+JxcG+</latexit>u1/v1<latexit sha1_base64="gunQFF4kPY5kmORUpuV2n9C0szo=">AAACb3icbZDfahNBFMYnq9WatprqhReCDIZCW2i6K0W9KgVvvIxg2kI2DbOzZ9uh82eZOVuzDPsUPk1v9Sl8DN/A2SRgm3pg4OP7nTPn8GWlFA7j+HcnevR47cnT9Wfdjc2t5y962y9PnakshxE30tjzjDmQQsMIBUo4Ly0wlUk4y64/t/zsBqwTRn/DuoSJYpdaFIIzDNa0d0BThBnOP/KZrKDx1UXS0MO7voW88TfBnvb68SCeF30okqXok2UNp9ud/TQ3vFKgkUvm3DiJS5x4ZlFwCU03rRyUjF+zSxgHqZkCN/HztQ3dCU5OC2PD00jn7t0Jz5RztcpCp2J45VZZa/6PjSssPk280GWFoPliUVFJioa2IdFcWOAo6yAYtyLcSvkVs4xjiLKbavjOjVJM5z6dNT5tF2SFnzXNfVb/Y/UqKwLLjMzb+430ReAh3WQ1y4fi9P0g+TA4+nrUPzle5rxO3pB3ZJck5CM5IV/IkIwIJz/ILflJfnX+RK+jtxFdtEad5cwrcq+ivb+JxcG+</latexit>u1/v1
<latexit sha1_base64="J4TkoE1lW7bXQULbRlBWDQJFyc8=">AAACb3icbZDdShxBEIV7Jz/qapI1XnghhCaLkASymRFJchWE3HipkFVhZ116emq0sX+G7hrdoZmnyNPkVp/Cx8gb2LMuRNcUNBzOV9VVnKyUwmEc33aiZ89fvFxaXumurr16/aa3/vbImcpyGHIjjT3JmAMpNAxRoIST0gJTmYTj7OJny48vwTph9C+sSxgrdqZFITjDYE16n2mKMMXZRz6TFTS+Ot1p6JeHvoW88ZfBnvT68SCeFX0qkrnok3kdTNY7n9Lc8EqBRi6Zc6MkLnHsmUXBJTTdtHJQMn7BzmAUpGYK3NjP1jZ0Ozg5LYwNTyOduQ8nPFPO1SoLnYrhuVtkrfk/Nqqw+D72QpcVgub3i4pKUjS0DYnmwgJHWQfBuBXhVsrPmWUcQ5TdVMMVN0oxnft02vi0XZAVfto0j1n9j9WLrAgsMzJv7zfSF4GHdJPFLJ+Ko51B8nWwe7jb3/sxz3mZbJH35ANJyDeyR/bJARkSTn6TP+Sa3HT+RpvRu4jet0ad+cwGeVTRxzuNmMHA</latexit>u2/v2
<latexit sha1_base64="J4TkoE1lW7bXQULbRlBWDQJFyc8=">AAACb3icbZDdShxBEIV7Jz/qapI1XnghhCaLkASymRFJchWE3HipkFVhZ116emq0sX+G7hrdoZmnyNPkVp/Cx8gb2LMuRNcUNBzOV9VVnKyUwmEc33aiZ89fvFxaXumurr16/aa3/vbImcpyGHIjjT3JmAMpNAxRoIST0gJTmYTj7OJny48vwTph9C+sSxgrdqZFITjDYE16n2mKMMXZRz6TFTS+Ot1p6JeHvoW88ZfBnvT68SCeFX0qkrnok3kdTNY7n9Lc8EqBRi6Zc6MkLnHsmUXBJTTdtHJQMn7BzmAUpGYK3NjP1jZ0Ozg5LYwNTyOduQ8nPFPO1SoLnYrhuVtkrfk/Nqqw+D72QpcVgub3i4pKUjS0DYnmwgJHWQfBuBXhVsrPmWUcQ5TdVMMVN0oxnft02vi0XZAVfto0j1n9j9WLrAgsMzJv7zfSF4GHdJPFLJ+Ko51B8nWwe7jb3/sxz3mZbJH35ANJyDeyR/bJARkSTn6TP+Sa3HT+RpvRu4jet0ad+cwGeVTRxzuNmMHA</latexit>u2/v2
Figure 2: An example of ZS-MFTG over a two-node graph, where N1= 2,N2= 2andρ= 0.5.
Under this transition kernel, the probability of a Blue agent transitioning from node 1 to node 2 depends on the Blue
team’s numerical advantage over the Red team at node 1.
The initial joint states depicted in Figure 2 are given by x2
0= [x1, x1]andy2
0= [y1, y2]. The corresponding EDs
areµ2
0= [1,0],ν2
0= [0.5,0.5], and the running reward is r0=µ2
0(x2) = 0 . Suppose the Blue team applies a team
strategy such that ϕi
0(u2|x1, µ2
0, ν2
0) = 1 for both i∈[2](under which both Blue agents apply u2). The probability
of an individual Blue agent transitioning to node 2 is 0.625. Thus, the next Blue ED is a random vector with three
possible realizations: (i) M2
1= [1,0]with probability 0.14 (both Blue agents remain on node 1); (ii) M2
1= [0.5,0.5]
with probability 0.47 (one Blue agent moves and the other remains); and (iii) M2
1= [0,1]with probability 0.39 (both
Blue agents move). Suppose the game terminates at T= 1, then the value under the given Blue strategy ϕ2is given
byJ4,ϕ2,ψ2(x2
0,y2
0) = 0 + (0 .14·0 + 0 .47·0.5 + 0 .39·1) = 0 .63.
Connections to the dynamics in [Huang et al., 2006] As a special case of the previously described dynamics and
reward structure, we consider the following pairwise-state-coupled dynamics and team reward derived from the single-
population model in [Huang et al., 2006]. The transition kernel for the Blue and Red agents are defined as follows:
ft 
xN1
i,t+1xN1
i,t, uN1
i,t,xN1
−i,t,yN2
t) =PN1
k=1f1,t(xN1
i,t+1xN1
i,t, uN1
i,t, xN1
k,t)
N+PN2
k=1f2,t(xN1
i,t+1xN1
i,t, uN1
i,t, yN2
k,t)
N(12a)
gt 
yN2
j,t+1yN2
j,t, vN2
j,t,xN1
t,yN2
−j,t) =PN1
k=1g1,t(yN2
j,t+1yN2
j,t, vN2
j,t, xN1
k,t)
N+PN2
k=1g2,t(yN2
j,t+1yN2
j,t, vN2
j,t, yN2
k,t)
N.(12b)
The terms f1,t,f2,t,g1,tandg2,tare transition kernels and model the pair-wise influence between agents. For example,
f2,trepresents the influence that the state of a Red agent has on the transition of a Blue agent.
The weakly-coupled team rewards are the averaged state-dependent reward
rt(xN1
t,yN2
t) =1
NN1X
k=1r1,t(xN1
k,t)−1
NN2X
k=1r2,t(yN2
k,t). (13)
Through some algebraic manipulations, the aforementioned pairwise-state-dependent transition kernel and team re-
ward can be transformed into the weakly-coupled form represented by equations (4) and (6) as follows:
ft 
xN1
i,t+1xN1
i,t, uN1
i,t,xN1
−i,t,yN2
t) =ρX
x∈XµN1
t(x)f1,t(xN1
i,t+1xN1
i,t, uN1
i,t, x)+(1−ρ)X
y∈YνN2
t(y)f2,t(xN1
i,t+1xN1
i,t, uN1
i,t, y)
≜fρ
t 
xN1
i,t+1xN1
i,t, uN1
i,t, µN1
t, νN2
t),
gt 
yN2
j,t+1yN2
j,t, vN2
j,t,xN1
t,yN2
−j,t) =ρX
x∈XµN1
t(x)g1,t(yN2
j,t+1yN2
j,t, vN2
j,t, x)+(1−ρ)X
y∈YνN2
t(y)g2,t(yN2
j,t+1yN2
j,t, vN2
j,t, y)
≜gρ
t 
yN2
j,t+1yN2
j,t, vN2
j,t, µN1
t, νN2
t),
rt(xN1
t,yN2
t) =ρX
x∈XµN1
t(x)r1,t(x)−(1−ρ)X
y∈YνN2
t(y)r2,t(y)≜rρ
t(µN1
t, νN2
t),
where ρ=N1/N,µN1
t= Empµ(xN1
t)andνN2
t= Empν(yN2
t)(see Appendix A for the detailed derivations). In
this specific example, it is clear that the team with more agents has a larger influence on the dynamics and the reward.
Hence, it is necessary to include ρ=N1/Nas a parameter for both the dynamics and the reward function. It can be fur-
ther verified that the above weakly-coupled transition kernels fρ
tandgρ
tare both 2-Lipschitz continuous with respect to
the EDs. Additionally, the weakly-coupled reward function has a Lipschitz constant of Lr= 2 max {r1,max, r2,max},
where r1,max= max x,t|r1,t(x)|andr2,max= max y,t|r2,t(y)|(see Propositions 1 and 2 in Appendix A).Published as a conference paper at AAAI 2024
3 Mean-Field Approximation
The preceding max-min and min-max optimizations are intractable for large-population systems, since the dimension
of the joint policy spaces ΦN1andΨN2grows exponentially with the number of the agents. To address this scalability
challenge, we first consider the limiting case of the ZS-MFTGs, when the number of agents of both teams goes
to infinity. We further assume that agents from the same infinite-population team employ the same strategy. As a
result, the behavioral of the entire team can be represented by a typical agent [Huang et al., 2006]. As we will show
later, due to the law of large numbers, the ED of an infinite-population team converges to the state distribution of its
corresponding typical agent. This limiting distribution is known as the mean-field [Huang et al., 2006]. In the sequel,
we formulate the mean-field team game at its infinite-population limit and introduce several additional concepts that
are necessary for the performance analysis in the next sections.
3.1 Identical Team Strategies and Mean-Field Dynamics
At the infinite population limit, we specifically focus on the Blue and Red teams that employ identical team strategies.
Consequently, we first formalize the class of identical team strategies.
Definition 4 (Identical Blue Team Strategy) .The Blue team strategy ϕN1={ϕ1, . . . , ϕ N1}is an identical team
strategy, if ϕi1,t=ϕi2,tfor all i1, i2∈[N1]and all t∈ {0, . . . , T −1}.
We slightly abuse the notation and use ϕto denote the identical Blue team strategy, under which all Blue agents apply
the same individual strategy ϕ. Consequently, Φis used to denote both the set of Blue individual strategies and the set
of identical Blue team strategies.6The definitions and notations extend to the identical Red team strategies.
With the notions of identical team strategies, we define the mean-field flow as a deterministic sequence of the state
distribution of a typical agent under the given pair of strategies.
Definition 5. Given identical team strategies ϕ∈Φandψ∈Ψ, the MFs are defined as the sequence of vectors that
adhere to the following coupled deterministic dynamics:
µρ
t+1(x′) =X
x∈X"X
u∈Ufρ
t(x′|x, u, µρ
t, νρ
t)ϕt(u|x, µρ
t, νρ
t)#
µρ
t(x), (14a)
νρ
t+1(y′) =X
y∈Y"X
v∈Vgρ
t(y′|y, v, µρ
t, νρ
t)ψt(v|y, µρ
t, νρ
t)#
νρ
t(y), (14b)
where the initial conditions are given by µρ
0=µ0andνρ
0=ν0.
The above deterministic mean-field dynamics can be expressed in a compact matrix form as
µρ
t+1=µρ
tFρ
t(µρ
t, νρ
t, ϕt),
νρ
t+1=νρ
tGρ
t(µρ
t, νρ
t, ψt),(15)
where Fρ
t∈R|X|×|X|is the transition matrix for a typical Blue agent’s distribution under the policy ϕtand its entries
are given by [Fρ
t(µρ
t, νρ
t, ϕt)]pq=P
u∈Ufρ
t(q|p, u, µρ
t, νρ
t)ϕt(u|p, µρ
t, νρ
t). The matrix Gρ
tis defined similarly.
The following lemma shows that the deterministic MF above is an approximation of the (stochastic) finite-population
ED, and the approximation error goes to zero when N1, N2→ ∞ . Thus, we can regard the mean-field as the empirical
distribution of an infinite-population team.
Lemma 1. LetXN1
t,YN2
t,MN1
tandNN2
tbe the joint states and the corresponding EDs of a finite-population game.
Denote the next EDs induced by an identical policy pair (ϕt, ψt)∈Φt×Ψtas(MN1
t+1,NN2
t+1). Then, the following
bounds hold:
Eϕth
dTV 
MN1
t+1,Mt+1XN1
t,YN2
ti
≤|X|
2r
1
N1,
Eψth
dTV 
NN2
t+1,Nt+1XN1
t,YN2
ti
≤|Y|
2r
1
N2,
where Mt+1=MN1
tFρ
t(MN1
t,NN2
t, ϕt)andNt+1=NN2
tGρ
t(MN1
t,NN2
t, ψt)according to (15).
Proof. See Appendix B.2
6When Φdenotes the set of identical team strategies, it is a subset of the original set of Blue team strategies ΦN1, i.e.,Φ⊆ΦN1.Published as a conference paper at AAAI 2024
3.2 Infinite-Population Optimization
For the infinite-population game, the cumulative reward induced by the strategy pair (ϕ, ψ)∈Φ×Ψis given by
Jρ,ϕ,ψ(µρ
0, νρ
0) =TX
t=0rρ
t(µρ
t, νρ
t), (16)
where the propagation of the mean-fields µρ
tandνρ
tare subject to the dynamics (14). The worst-case performance of
the Blue team is given by the lower game value
Jρ∗(µρ
0, νρ
0) = max
ϕ∈Φmin
ψ∈ΨJρ,ϕ,ψ(µρ
0, νρ
0). (17)
The Red team instead uses the upper game value based on the following min-max optimization
¯Jρ∗(µρ
0, νρ
0) = min
ψ∈Ψmax
ϕ∈ΦJρ,ϕ,ψ(µρ
0, νρ
0). (18)
Remark 2. Different from the finite-population value (10) which takes joint states as argument, the infinite-population
game value (17) takes MFs. This is due to the assumed identical team strategy, which no longer requires agents’ index
information to differentiate them and sample actions. Consequently, the MFs can serve as the information state as
in [Arabneydi and Mahajan, 2015]. The difference comes from the non-identical strategies considered in the finite-
population game, which require each agent’s state and index information to sample actions and predict the game’s
evolution.
4 Zero-Sum Game Between Coordinators
The mean-field sharing structure in (8) allows us to reformulate the infinite -population competitive team problem (17)
as an equivalent two-player game from the perspective of two fictitious7coordinators. The coordinators know the
common information (MFs) and selects a local policy that maps each agent’s local information (individual state) to
its actions. Through this common-information approach [Nayyar et al., 2013], we provide a dynamic program that
constructs optimal strategies for all agents under the original mean-field sharing information structure.
4.1 Equivalent Centralized Problem
We use πt:U × X → [0,1]to denote a local Blue policy, which is open-loop with respect to the MFs. Specifically,
πt(u|x)is the probability that a Blue agent selects action uat state xregardless of the current MFs. The set of open-
loop Blue local policies is denoted as Πt. Similarly, σt:V × Y → [0,1]andΣtdenote a Red local policy and its
admissible set. Under the local policy πt, the Blue MF propagates as
µρ
t+1(x′)=X
x∈XhX
u∈Ufρ
t(x′|x, u, µρ
t, νρ
t)πt(u|x)i
µρ
t(x), (19)
and the Red team MF dynamics under Red local policies is defined similarly as
νρ
t+1(y′)=X
y∈YhX
v∈Vgρ
t(y′|y, v, µρ
t, νρ
t)σt(v|y)i
νρ
t(y).
At each time t, a Blue coordinator observes the MFs of both teams (common information) and prescribes a local policy
πt∈Πtto all Blue agents within its team. The local policy is selected based on:
πt=αt 
µρ
t, νρ
t
,
where αt:P(X)× P(Y)→Πtis a deterministic Blue coordination policy , and πt(ut|xt)≜αt(µρ
t, νρ
t)(ut|xt)
gives the probability that a Blue agent selects action utgiven its current state xt. Similarly, the Red coordinator
observes the MFs and selects a local policy σt∈Σtaccording to σt=βt 
µρ
t, νρ
t
. We refer to the time sequence
α= 
α1, . . . , α T−1
as the Blue team coordination strategy andβ= 
β1, . . . , β T−1
as the Red team coordination
strategy. The sets of admissible coordination strategies are denoted as AandB.
7The coordinators are fictitious since they are introduced as an auxiliary concept and are not required for the actual implemen-
tation of the obtained strategies.Published as a conference paper at AAAI 2024
play against coordinatorsmean-fieldlocal policymean-field⋮⋮local policy
<latexit sha1_base64="vqDEwZFKUqU/xNeIpwtsH14Mu04=">AAAFinicnZTdbtMwFMe9scIYAza45MYiQhrSVDXdtyahiQ8JIW2MsXad6qxyHKe15jjBdrZVVl6CW3gx3gYnbdcsHTdYinSOf8fH/3NOZD/hTOlG48/c/IOF2sNHi4+Xniw/ffZ8ZfVFW8WpJLRFYh7Ljo8V5UzQlmaa004iKY58Ts/8yw85P7uiUrFYnOphQr0I9wULGcHabnUQ5skA93RvxWnUXXfXLmiN3aa7s2eNrb2Nre0mdOuNYjlgvI57qwscBTFJIyo04ViprttItGew1Ixwmi2hVNEEk0vcp11rChxR5ZlCcAbf2J0AhrG0n9Cw2C2fMDhSahj5NjLCeqCqLN+8j3VTHe56hokk1VSQ0UVhyqGOYV49DJikRPOhNTCRzGqFZIAlJtr2aAkJek3iKMIiMKiTGZRfQDA3nSy7C89L8LwKWyXYqsJ2Cbar8LAED6vwqASPqtAPb8bYD83NLB1O6XCWdqZ0plQ/PJ/SmVqZCPJ/yQ612/RGYYEybtYzjpshn/XXjNMsjLeVkzhJ5ESzjMzIrcSo1M+rTkVAZf6Hz9YtqWJBSkut+Zqh9/ZaFNqpWiE2yQ+px6kKVBVycvid6qn8PMeJ1Y+idN3WcJHrX7POBZKDuChrHSJRcmfyHd2fT/xnvkBfZV3Xu+1UUIgb2af2F4JFm6HjwvvaTOxobsNzpzoGX42ycxpqxO1boR0XSdYfaIhk7lYO0JuElMrzffMpn7Ydcp6hOznsVUeprYz9iRBV1/UM7VdieHxN5RXmxTynQ/+SZfZ9mjxC8N9Gu1l3t+ub3zadg3fjl2oRvAKvwRpwwQ44AJ/BMWgBAjj4CX6B37XlWrO2V9sfhc7Pjc+8BHdW7eNf/x34hw==</latexit>↵t
<latexit sha1_base64="gnp/5wsfaIwB0h94DG6oHFCsCdM=">AAAFh3icnZTfb9MwEMe9scI2fm3wyItFhDSkqTRl6zpNQgOEhJA2xli7TnVWOY7TWnOcYDtdqyh/A6/wp/Hf4KTtmqXjBUuR7vw5n793F9mNOFO6VvuztHxvpXL/wera+sNHj5883dh81lZhLAltkZCHsuNiRTkTtKWZ5rQTSYoDl9Nz9+pjxs+HVCoWijM9jqgT4L5gPiNYm60WilhP9zasWtW2m2ZBYzTr9t6+MXb33+426tCu1vJlgek66W2ucOSFJA6o0IRjpbp2LdJOgqVmhNN0HcWKRphc4T7tGlPggConydWm8JXZ8aAfSvMJDfPd4okEB0qNA9dEBlgPVJllm3exbqz9ppMwEcWaCjK5yI851CHMSocek5RoPjYGJpIZrZAMsMREmwatI0GvSRgEWHgJ6qQJyi4gmCedNL0NLwrwogxbBdgqw3YBtsvwqACPyvC4AI/L0PVHU+z6yWiRjud0vEg7c7pQqutfzOlCrUx42Y9khtqtO5MwTyV22kssO0Uu628lVj03XpdO4iiSM80ySCZuKUbFblZ1LDwqs997sW5JFfNiWmjN1xR9MNci30zVCDFJfkg9TZWjspDTo+9Uz+VnOU6NfhTE26aGy0z/lnEukRyEeVnbEImCu5Dv+O584j/zeXqYdm3nplNeLm5in5lfCOZthpYN72ozMaO5Cc+c8hhcNcnOqa8RNw+FtmwkWX+gIZKZWzpARxEplOe6yads2mbIWYbu7LBTHqU2Mg5mQlRVV1N0UIrh4TWVQ8zzec6H/iVNzfs0e4Tgv412vWo3qjvfdqzDd9OXahW8AC/BFrDBHjgEn8EJaAECGPgJfoHflbXKm0qj0pyELi9NzzwHt1bl/V94p/c8</latexit>⇡t<latexit sha1_base64="lNQkX74ozJ4gUCMS7Atfna8Il0k=">AAAFiXicnZTfb9MwEMc9WGFs/NjgkReLCGlIU9VUUxlMQhMTEkLaGGPtOtVdZTtOa81xgu1sq6L8EbzCX8Z/g522a5aOFyxFOvtzd/7eXWSSCK5No/Fn6d795dqDhyuPVtceP3n6bH3jeUfHqaKsTWMRqy7BmgkuWdtwI1g3UQxHRLBTcrHv+OklU5rH8sSME9aP8FDykFNs7NEpIszggRmse416a+fddrMJG/VGsZzht/xWE/rTEw9M19FgY1mgIKZpxKShAmvd8xuJ6WdYGU4Fy1dRqlmC6QUesp41JY6Y7meF3hy+ticBDGNlP2lgcVqOyHCk9Tgi1jPCZqSrzB3exXqpCXf6GZdJapikk4vCVEATQ1c8DLhi1IixNTBV3GqFdIQVpsa2aBVJdkXjKMIyyFA3z5C7gGKRdfP8NjwrwbMqbJdguwo7JdipwoMSPKjCwxI8rEISXk8xCbPrRTqe0/Ei7c7pQqkkPJvThVq5DNyvZIfaa/YnboHO/HyQeX6OCB9uZl6zMN5UInGSqJlmFWWTbcVHp8RVncqAKfeDL9atmOZBykqt+Zqjj/ZaFNqpWiE2yQ9lpqkKVBVyfPCdmbl8l+PY6kdRumVrOHf6N+3mHKlRXJS1BZEsbRfyHd6dT/5nvsBc5j2/f9OpoBA3sU/sLwSLNkPPh3e1mdrR3Li7TXUMRE+yCxYaJOxTYTwfKT4cGYiU21YC2HVCS+URkn1y07ZDdhl6s+B+dZTGytidCdF1U8/RbsVHxFdMXWJRzHM+9C95bt+n2SME/210mnW/Vd/+tu3tfZi+VCvgJXgFNoEP3oI98BkcgTag4AL8BL/A79paza/t1N5PXO8tTWNegFurtv8X0eX35g==</latexit> t<latexit sha1_base64="OUOh5btSvBe8fnAQv8LcasHVN3E=">AAAFinicnZTfb9MwEMc9WGGMARs88mIRIQ1pqppoKpsmoYkfEkLaGGPtOtVZ5ThOa81xgu1sq6L8E7zCP8Z/g522a5aOFyxFOvtzd/7eXeQg5UzpVuvP0r37y40HD1cerT5ee/L02frG865KMklohyQ8kb0AK8qZoB3NNKe9VFIcB5yeBhcfLD+9pFKxRJzocUr9GA8FixjB2hz1kGLDGA/0YN1pNds7u9ueB1vNVrms4bbdtgfd6YkDputosLHMUZiQLKZCE46V6rutVPs5lpoRTotVlCmaYnKBh7RvTIFjqvy8FFzA1+YkhFEizSc0LE+rETmOlRrHgfGMsR6pOrOHd7F+pqMdP2cizTQVZHJRlHGoE2irhyGTlGg+NgYmkhmtkIywxESbHq0iQa9IEsdYhDnqFTmyFxDM815R3IZnFXhWh50K7NRhtwK7dXhQgQd1eFiBh3UYRNdTHET59SIdz+l4kfbmdKHUIDqb04VamQjtv2SG2vf8iVuocrcY5I5boIANN3PHK403tUicpnKmWcb5ZFvzUVlgq85ESKX9wxfrllSxMKOV1nwt0HtzLYrMVI0Qk+SH1NNUJaoLOT74TvVcvs1xbPSjONsyNZxb/Ztmc47kKCnL2oJIVLYL+Q7vzif+M1+oL4u+6990KizFTewT8wvBss3QceFdbSZmNDfudlMfQ6Am2TmNNOLmrdCOiyQbjjRE0m5rAfQ6JZXygiD/ZKdthmwz9GfBfn2U2sjYmwlRTd0s0F7NhydXVF5iXs5zPvQvRWHep9kjBP9tdL2m225uf9t29t9NX6oV8BK8ApvABW/BPvgMjkAHEMDBT/AL/G6sNbzGbmNv4npvaRrzAtxajY9/AVTs+GU=</latexit> t
<latexit sha1_base64="weXOvkddUUaqjHQbcggi/NC2z2g=">AAAFh3icnZTdbtMwFMe9scI2PrbBJTcWEdKQppKM0bUaFwOEhJA2xli7TnVWOY7TWnOcYDvbqijPwC08Gm+Dk7Zrlo4bLEU6x7/j4//xcezFnClt238WFu8t1e4/WF5Zffjo8ZO19Y2nHRUlktA2iXgkux5WlDNB25ppTruxpDj0OD31Lj7m/PSSSsUicaJHMXVDPBAsYARrM9VGYdLX/XXLrtvNRqPVhHbdsVt2a9cYb53WzpsWdOp2MSwwGUf9jSWO/IgkIRWacKxUz7Fj7aZYakY4zVZRomiMyQUe0J4xBQ6pctNCbQZfmhkfBpE0n9CwmC2vSHGo1Cj0TGSI9VBVWT55F+slOmi6KRNxoqkg442ChEMdwbx06DNJieYjY2AimdEKyRBLTLQ5oFUk6BWJwhALP0XdLEX5BgTztJtlt+FZCZ5VYbsE21XYKcFOFR6U4EEVHpbgYRV6wfUEe0F6PU9HMzqap90ZnSvVC85mdK5WJvz8Ipmm9rbdcZivUifrp5aTIY8NNlNruzBeVVbiOJZTzTJMx24lRiVeXnUifCrz6z1ft6SK+QktHc3XDH0w26LAdNUIMUl+SD1JVaCqkOOD71TP5Oc5jo1+819smRrOc/2bxjlHchgVZW1BJEruXL7Du/OJ/8zn68us57g3J+UX4sb2iblCsDhmaDnwrmMmpjU34blTbYOnxtk5DTTi5qHQloMkGww1RDJ3KwvodUxK5Xle+invtmlynqE3XexWW6mNjL2pEFXX9QztVWJ4dEXlJeZFP2dN/5Jl5n2aPkLw30Znu+406jvfdqz9d5OXahk8By/AJnDALtgHn8ERaAMCGPgJfoHftZXa61qj1hyHLi5M1jwDt0bt/V+wS/dF</latexit>µt<latexit sha1_base64="/zEnT0AZR1xn8NGtt3whB2MWzrE=">AAAFh3icnZTdbtMwFMe9scI2vja45MYiQhrSVJoy1qFxMUBICGljjLXrVGeV4zitNccJtrOtivIM3MKj8TbYabtm6bjBUqTj8zs+/h8fx37CmdKNxp+FxTtLtbv3lldW7z94+Ojx2vqTjopTSWibxDyWXR8rypmgbc00p91EUhz5nJ745x8tP7mgUrFYHOtRQr0IDwQLGcHauNpIpH3dX3Ma9W337ZtWCzbqjWJY47XbcregO/E4YDIO++tLHAUxSSMqNOFYqZ7bSLSXYakZ4TRfRamiCSbneEB7xhQ4osrLCrU5fGE8AQxjaT6hYeEtr8hwpNQo8k1khPVQVZl13sZ6qQ53vIyJJNVUkPFGYcqhjqEtHQZMUqL5yBiYSGa0QjLEEhNtDmgVCXpJ4ijCIshQN8+Q3YBgnnXz/CY8LcHTKmyXYLsKOyXYqcL9EtyvwoMSPKhCP7yaYD/MrubpaEZH87Q7o3Ol+uHpjM7VykRgL5Jpaq/pjcMClbl5P3PcHPlssJE5zcJ4WVmJk0RONcsoG08rMSr1bdWpCKi013u+bkkVC1JaOpqvOfpgtkWh6aoRYpL8kHqSqkBVIUf736meybc5jox+FKWbpoYzq3/DTM6QHMZFWZvQ/DOz6Vy+g9vzif/MF+iLvOd61ycVFOLG9rG5QrA4Zui48LZjJqY11+F2Um2Dr8bZOQ014uah0I6LJBsMNUTSTisL6FVCSuX5fvbJdts02WboTRd71VZqI2N3KkTVdT1Hu5UYHl9SeYF50c9Z07/kuXmfpo8Q/LfRadbd7frWty1n793kpVoGz8BzsAFc0AJ74DM4BG1AAAM/wS/wu7ZSe1Xbru2MQxcXJmueghuj9v4v3LP3Gw==</latexit>⌫t
Figure 3: A schematic of the proposed common-information approach for the mean-field zero-sum team games.
Remark 3. There is a one-to-one correspondence between the coordination strategies and the identical team strategies.
For example, given an identical Blue team strategy ϕ∈Φ, one can define the coordination strategy:
αt(µt, νt) =πts.t. πt(ut|xt) =ϕt(ut|xt, µt, νt)∀µt∈ P(X), νt∈ P(Y), xt∈ X andut∈ U.
Similarly, a Blue coordination strategy α∈ A induces an identical team strategy ϕ∈Φaccording to the rule
ϕt(ut|xt, µρ
t, νρ
t) =
αt(µρ
t, νρ
t)
|{z}
πt(ut|xt)∀µt∈ P(X), νt∈ P(Y), xt∈ X andut∈ U.
Plugging in the deterministic coordination strategies, the mean-fields dynamics in (19) becomes
µρ
t+1=µρ
tFρ
t(µρ
t, νρ
t, αt(µρ
t, νρ
t)|{z}
πt), (20a)
νρ
t+1=νρ
tGρ
t(µρ
t, νρ
t, βt(µρ
t, νρ
t)). (20b)
For notational simplicity, we will use the shorthand notations Fρ
t(µρ
t, νρ
t, αt)andGρ
t(µρ
t, νρ
t, βt).
The original competitive team problem in (17) can now be viewed as an equivalent zero-sum game played between the
two coordinators, where the game state is the joint mean-field (µρ
t, νρ
t), and the actions are the local policies πtandσt
selected by the coordinators. Figure 3 provides a schematic of the equivalent centralized system.
Formally, the zero-sum coordinator game can be defined via a tuple
ZS-CG =⟨P(X),P(Y),Πt,Σt, Fρ
t, Gρ
t, rρ
t, ρ, T⟩. (21)
In particular, the continuous game state space is P(X)×P(Y), and the continuous action spaces are ΠtandΣtfor the
Blue and Red coordinators, respectively. The deterministic dynamics of the game is given in (20). Finally, the reward
structure rρ
tis given in (6), and the horizon Tis the same as in the original game.
Given two coordination strategies α∈ A andβ∈ B, the cumulative rewards of the coordinator game is defined as
Jρ,α,β
cor(µρ
0, νρ
0) =TX
t=0rρ
t(µρ
t, νρ
t), (22)
where the propagation of the mean-fields µρ
tandνρ
tare subject to the dynamics (20). The worst-case performance of
the Blue team is given by the lower coordinator game value
Jρ∗
cor(µρ
0, νρ
0) = max
α∈Amin
β∈BJρ,α,β
cor(µρ
0, νρ
0), (23)
and the upper value for the Red team is based on the following min-max optimization
¯Jρ∗
cor(µρ
0, νρ
0) = min
β∈βmax
α∈AJρ,α,β
cor(µρ
0, νρ
0). (24)
In the next section, we examine the properties of the max-min (lower) and min-max (upper) game values of the
coordinator game.Published as a conference paper at AAAI 2024
4.2 Value Functions of the Coordinator Game
Similar to the standard two-player zero-sum games, we use a backward induction scheme to find the lower and upper
values of the coordinator game. In general, the max-min value need not be equal to the min-max value [Elliott and
Kalton, 1972]. Consequently, from the Blue coordinator’s perspective, we consider the lower value (max-min), which
provides the highest guaranteed performance for the maximizing Blue team in a worst-case scenario.
The terminal lower value at time Tis defined as
Jρ∗
cor,T(µρ
T, νρ
T) =rρ
T(µρ
T, νρ
T). (25)
For all previous time steps t= 0, . . . , T −1, the two coordinators optimize their cumulative reward function by
choosing their actions (i.e., local policies) πtandσt. Consequently, for all t= 0, . . . , T −1,we have
Jρ∗
cor,t(µρ
t, νρ
t) =rρ
t(µρ
t, νρ
t) + max
πt∈Πtmin
σt∈ΣtJρ∗
cor,t+1 
µρ
tFρ
t(µρ
t, νρ
t, πt), νρ
tGρ
t(µρ
t, νρ
t, σt)
. (26)
With the optimal value function, the optimal Blue team coordination policies can then be easily constructed via
α∗
t(µρ
t, νρ
t)∈argmax
πt∈Πtmin
σt∈ΣtJρ∗
cor,t+1 
µρ
tFρ
t(µρ
t, νρ
t, πt), νρ
tGρ
t(µρ
t, νρ
t, σt)
. (27)
Similarly, for the Red team coordinator, the upper values are computed as
¯Jρ∗
cor,T(µρ
T, νρ
T) =rρ
T(µρ
T, νρ
T), (28a)
¯Jρ∗
cor,t(µρ
t, νρ
t) =rt(µρ
t, νρ
t) + min
σt∈Σtmax
πt∈Πt¯Jρ∗
cor,t+1 
µρ
tFρ
t(µρ
t, νρ
t, πt), νρ
tGρ
t(µρ
t, νρ
t, σt)
, t= 0, . . . , T −1,(28b)
and the optimal Red team coordination policy is given by
β∗
t(µρ
t, νρ
t)∈argmin
σt∈Σtmax
πt∈Πt¯Jρ∗
cor,t+1 
µρ
tFρ
t(µρ
t, νρ
t, πt), Gρ
t(µρ
t, νρ
t, σt)
. (29)
Note that the optimal Blue team coordination strategy induces an identical Blue team strategy that satisfies the mean-
field sharing information structure and can be implemented in the finite-population game (Remark 3).
4.3 Reachable Sets
At the infinite-population limit, the MF dynamics in (15) is deterministic, and thus selecting the local policies πtand
σtat time tis equivalent to selecting the desirable MFs at the next time step. Consequently, we examine the set of
MFs that can be reached from the current MFs.
Definition 6. The Blue reachable set, starting from the mean-fields µρ
tandνρ
t, is defined as the set comprising all the
possible next Blue team mean-fields µρ
t+1that can be achieved by employing a local policy πt∈Πt. Formally,
Rρ
µ,t(µρ
t, νρ
t) =
µρ
t+1| ∃πt∈Πts.t. µρ
t+1=µρ
tFρ
t(µρ
t, νρ
t, πt)	
. (30)
Similarly, the Red reachable set is defined as
Rρ
ν,t(µρ
t, νρ
t) =
νρ
t+1| ∃σt∈Σts.t. νρ
t+1=νρ
tGρ
t(µρ
t, νρ
t, σt)	
. (31)
We will regard the reachable sets as set-valued functions (correspondences) [Freeman and Kokotovic, 2008]. In this
case, we write Rρ
µ,t:P(X)× P(Y)⇝P(X), and similarly Rρ
ν,t:P(X)× P(Y)⇝P(Y).
The following lemma justifies using the reachable sets constructed based on the local policies to analyze the reacha-
bility of identical team policies.
Lemma 2. For all µρ
t∈ P(X)andνρ
t∈ P(Y), we have that

µρ
t+1| ∃ϕt∈Φts.t. µρ
t+1=µρ
tFρ
t(µρ
t, νρ
t, ϕt)	
=Rρ
µ,t(µρ
t, νρ
t). (32)
Proof. This lemma is a direct consequence of Remark 3.
Remark 4. Note that the reachable sets are constructed based on identical team strategies, since under the coordinator
game formulation, all agents in the same team follow the same local policies prescribed by their coordinator.Published as a conference paper at AAAI 2024
4.4 Equivalent Form of Value Functions
We can now change the optimization domains in (25) and (26) from the policy spaces to the corresponding reachable
sets. One can easily see that the following lower value propagation scheme is equivalent to the one in (25) and (26).
Jρ∗
cor,T(µρ
T, νρ
T) =rρ
T(µρ
T, νρ
T)
Jρ∗
cor,t(µρ
t, νρ
t) =rρ
t(µρ
t, νρ
t) + max
µρ
t+1∈Rρ
µ,t(µρ
t,νρ
t)min
νρ
t+1∈Rρ
ν,t(µρ
t,νρ
t)Jρ∗
cor,t+1(µρ
t+1, νρ
t+1), t= 0, . . . , T −1.(33)
Given the current mean-fields (µρ
t, νρ
t), the optimal next mean-field to achieve is then given by
µρ∗
t+1∈ argmax
µρ
t+1∈Rρ
µ,t(µρ
t,νρ
t)min
νρ
t+1∈Rρ
ν,t(µρ
t,νρ
t)Jρ∗
cor,t+1(µρ
t+1, νρ
t+1).
The Blue coordination policy that leads to µρ∗
t+1can be constructed via a simple linear program leveraging the linearity
of the mean-field dynamics with respect to the policy. See Appendix E for details.
In the sequel, we primarily work with the reachability-based optimization in (33). There are two advantages to this
approach: First, the reachable sets generally have a lower dimension than the coordinator action spaces,8, which is
desirable for numerical algorithms; Second, the reachability-based optimization allows us to compare the “reachabil-
ity” induced by non-identical and identical team strategies (Theorem 2 in Section 5) and then study the performance
loss due to the identical strategy assumption.
4.5 Existence of the Coordinator Game Value
In general, the coordinator game value may not exist, i.e, the max-min and min-max values differ (see Numerical
Example 1 in Section 6). However, we can show that the existence of coordinator game value for a special class of
mean-field team games, where the dynamics of each individual agent is independent of other agents’ states (both its
teammates and its opponents). It is left as a future research direction to obtain more general conditions that ensure the
existence of the game value.
Definition 7. We say that the weakly-coupled dynamics are independent if the following holds for all µt∈ P(X),
νt∈ P(Y)andt∈ {0, . . . , T −1},
fρ
t(xt+1|xt, ut, µt, νt) =¯ft(xt+1|xt, ut),
gρ
t(yt+1|yt, vt, µt, νt) = ¯gt(yt+1|yt, vt),(34)
for some transition kernels ¯ftand¯gt.
In other words, the dynamics of an agent only depends on that agent’s current state and action and is fully decoupled
from all other agents. However, we still allow reward coupled via the MFs. The following theorem ensures the
existence of the game value under independent dynamics.
Theorem 1. Suppose the reward function rρ
tis concave-convex for all t∈ {0, . . . , T }and the system dynamics is
independent. Then, the game value exists.
Proof. One can show that the optimal value under independent dynamics is concave-convex and apply the minimax
theorem. The detailed proof is given in Appendix D.
5 Main Results
Recall that the optimal Blue team coordination strategy α∗is constructed for the infinite-population game assuming
that both teams employ identical team strategies. This section establishes the performance guarantees for α∗in the
finite-population games where both teams are allowed to deploy non-identical strategies.
5.1 Approximation Error
Asα∗is solved at the infinite-population limit, it is essential to understand how well the infinite-population game ap-
proximates the original finite-population problem. In this subsection, we show that that the reachable set constructed
using identical strategies and the mean-field dynamics is rich enough to approximate any empirical distributions in-
duced by non-identical team strategies in finite-population games. We start with constructing local policies that mimics
the behaviors of non-identical team strategies.
8The Blue reachable set is a subset of P(X), while the Blue coordinator action space is given by Πt= (P(U))|X|.Published as a conference paper at AAAI 2024
Lemma 3. LetXN1
t,YN2
t,MN1
tandNN2
tbe the joint states and the corresponding EDs of a finite-population game.
Given any Blue team policy ϕN1
t∈ΦN1
t(potentially non-identical), define the following local policy
πapprx ,t(u|x) =

PN
i=11x 
XN1
i,t
ϕi,t(u|x,MN1
t,NN2
t)
N1MN1
t(x)ifMN1
t(x)>0,
1/|U| ifMN1
t(x) = 0 .(35)
Further, define the next mean-field induced by πapprx ,tas
Mapprx ,t+1=MN1
tFρ
t(MN1
t,NN2
t, πapprx ,t). (36)
Then, the expected distance between the next Blue ED MN1
t+1induced by the team policy ϕN1
tand the mean-field
Mapprx ,t+1satisfies
Eh
dTV 
MN1
t+1,Mapprx ,t+1
|XN1
t,YN2
ti
≤|X|
2r
1
N1. (37)
Proof. See Appendix B.3.
The local policy πapprx ,tmimics the population behavior by setting its action distribution at state xas the average of
the policies used by the Blue agents at state x. The purpose of the second case in (35) is to ensure that the constructed
local policy is well-defined at states where no Blue agent is present. The mean-field induced by πapprx ,tis within the
reachable set and is close to the next ED induced by the non-identical team policy ϕN1
tin expectation. This idea is
visualized in Figure 4.
Lemma 3 directly leads to the following theorem regarding the richness of the reachable sets, as the mean-field induced
byπapprx ,tis within the reachable set.
Theorem 2. LetXN1
t,YN2
t,MN1
t, andNN2
tbe the joint states and the corresponding EDs at time t. Denote the
next Blue ED induced by some Blue team policy ϕN1
t∈ΦN1
tasMN1
t+1. Then, there exists a mean-field µt+1∈
Rρ
µ,t(MN1
t,NN2
t)that satisfies
EϕN1
th
dTV 
MN1
t+1, µt+1XN1
t,YN2
ti
≤|X|
2r
1
N1. (38)
Remark 5. The construction of πapprx ,tin (35) requires knowing each Blue agent’s state, which may seem to violate
the mean-field sharing information structure in (8). However, πapprx ,tonly serves as an auxiliary concept to prove the
existence of a mean-field in the reachable set that satisfies (38). The existence result in Theorem 2 is all we need to
provide performance guarantees. In fact, πapprx ,twill not be used to construct the optimal policies.
Remark 6. All results in this subsection extend to the analysis from Red team’s side.
5.2 Lipschitz Continuity of the Value Functions
Next, we examine the continuity of the optimal value function in (33) with respect to the mean-field arguments, which
is essential for the performance guarantees. Clearly, the continuity of the value function depends on the continuity of
the two reachability correspondences Rρ
µ,tandRρ
ν,t. To properly study the continuity of the reachability correspon-
dences, we use the Hausdorff distance to measure the distance between two sets.
<latexit sha1_base64="LbxMwv6XCcrC+DD+muYVteqkdIE=">AAACXHicbZDPSsNAEMa3sWqNVquCFy/BIngqiRQVTwUvnkTBVqGJZbPZtIv7J+xu1LDkNbzqa3nxWdzUgrY6MPDx/WaY4YszSpT2/Y+as1RfXlltrLnrG83Nrdb2zkCJXCLcR4IKeR9DhSnhuK+Jpvg+kxiymOK7+PGi4ndPWCoi+K0uMhwxOOYkJQhqa4VhNiEP5moUlCM9arX9jj8t768IZqINZnU92q6dh4lAOcNcIwqVGgZ+piMDpSaI4tINc4UziB7hGA+t5JBhFZnp06V3aJ3ES4W0zbU3dX9vGMiUKlhsJxnUE7XIKvM/Nsx1ehYZwrNcY46+D6U59bTwqgS8hEiMNC2sgEgS+6uHJlBCpG1ObsjxMxKMQZ6Y8KU0YXUgTs1LWc6z4ocViyy1LBY0qf4X1KTlXBYmZqVr0w4Ws/0rBsed4KTTvem2e91Z7g2wDw7AEQjAKeiBS3AN+gCBDLyCN/Be+3TqzrrT/B51arOdXTBXzt4X0r25qA==</latexit> N1t
<latexit sha1_base64="+pmBObB+xsdiFY5cxN1Scyk4JY0=">AAACaHicbZDLSsNAFIan8V5vVUERN8Ei6MKSiKi4Ety4VLAqNKVMJic6OJcwM9GGMS/jVl/IV/ApnNSCtnpg4Od85zZ/nDGqTRB81LyJyanpmdm5+vzC4tJyY2X1RstcEWgTyaS6i7EGRgW0DTUM7jIFmMcMbuPH84rfPoHSVIprU2TQ5fhe0JQSbFyq11iPDPSN3RVS7NMEhHGE7ZW9RjNoBYPw/4pwKJpoGJe9ldpplEiSczeCMKx1Jwwy07VYuYkMynqUa8gwecT30HFSYA66awcfKP0dl0n8VCr3hPEH2d8dFnOtCx67So7Ngx5nVfI/1slNetK1VGS5AUG+F6U58430Kzf8hCoghhVOYKKou9UnD1hhYpxn9UjAM5GcY5HYqF/aqFoQp7ZflqOs+GHFOEsdiyVLqvsls2k54oWNeVl3bofj3v4VNwet8Kh1eHXYPGsNfZ9FW2gb7aIQHaMzdIEuURsR9IJe0Rt6r316DW/D2/wu9WrDnjU0Et72F5otviI=</latexit>(non-identical)
<latexit sha1_base64="O9XZj31Eh4U+fCjiS9dPVoMwtYQ=">AAACZXicbZDLSgMxGIXT8V5v1YobFw4WwVWZkaLiquDGjaJgVejUIZPJaDCXIcmoQ8izuNVH8gl8DTO1oK3+EDic7//J4SQ5JUoHwUfNm5qemZ2bX6gvLi2vrDbW1q+VKCTCPSSokLcJVJgSjnuaaIpvc4khSyi+SR5PKn7zhKUigl/pMscDBu85yQiC2llxoxkxqB8QpObMxvrOnMehjRutoB0Mx/8rwpFogdFcxGu14ygVqGCYa0ShUv0wyPXAQKkJotjWo0LhHKJHeI/7TnLIsBqYYXrr7zon9TMh3ePaH7q/LwxkSpUscZtVVjXJKvM/1i90djQwhOeFxhx9f5QV1NfCr6rwUyIx0rR0AiJJXFYfPUAJkXaF1SOOn5FgDPLURC/WDItKMvNi7Tgrf1g5yTLHEkHTKr+gJrNjXZiE2bprO5zs9q+43m+HB+3OZafV7Yx6nwdbYAfsgRAcgi44BRegBxAowSt4A++1T2/Z2/A2v1e92uimCcbG2/4CDqu8/Q==</latexit>MN1t<latexit sha1_base64="5UaJY1PY3LiC5YxHNTGGy8Jwfsg=">AAACaXicbZDLSgMxFIbT8V5vrS4U3QwWQRDKjBQVV4IbN4qCVaFTh0wmo8FchiSjDiFP41YfyGfwJczUgrZ6IPDz/+dwTr4kp0TpIPioeROTU9Mzs3P1+YXFpeVGc+VaiUIi3EWCCnmbQIUp4biriab4NpcYsoTim+TxpMpvnrBURPArXea4z+A9JxlBUDsrbqxFDOoHBKk5s7HRu6G9M+dxaONGK2gHg/L/inAoWmBYF3GzdhSlAhUMc40oVKoXBrnuGyg1QRTbelQonEP0CO9xz0kOGVZ9M/iB9bedk/qZkO5x7Q/c3xMGMqVKlrjO6l41nlXmf1mv0Nlh3xCeFxpz9L0oK6ivhV/h8FMiMdK0dAIiSdytPnqAEiLtoNUjjp+RYAzy1EQv1gxgJZl5sXY0K3+ycjzLXJYImlb3C2oyO8LCJMzWHe1wnO1fcb3XDvfbnctO67gz5D4LNsEW2AEhOADH4BRcgC5AwIJX8Abea59e01v3Nr5bvdpwZhWMlNf6AlMXvnk=</latexit>MN1t+1
<latexit sha1_base64="iEUbGb7sMhbW19nZzIZ/73xZsoE=">AAACaHicdZDfSiMxFMbTWdc/3VWrC4rsTbAIXkiZkaJ2r4S98VLBqtApJZPJaDD/SDLaIeZl9nb3hfYV9ik201a0ogcCH9/vHM7JlylGjY3jv43o08LnxaXlleaXr6tr662NzSsjS41JH0sm9U2GDGFUkL6llpEbpQniGSPX2f3Pml8/EG2oFJe2UmTI0a2gBcXIBmvU2koVHbmUI3unuUNK6bE/sH7UasedeFLwlej1TpJeDyYzpw1mdT7aaPxIc4lLToTFDBkzSGJlhw5pSzEjvpmWhiiE79EtGQQpECdm6CYf8HAvODkspA5PWDhxX084xI2peBY660vNW1ab77FBaYuToaNClZYIPF1UlAxaCes0YE41wZZVQSCsabgV4jukEbYhs2YqyCOWnCORu3TspzFlhRt7P8+qF1a9ZUVgmWR5fb9krvBzWbiM+2ZI+zlS+LG4OuwkR53uRbd92p3lvgy+g12wDxJwDE7BGTgHfYDBE/gFfoM/jX9RK9qOdqatUWM28w3MVbT7HzBRvvo=</latexit>⇡apprx,t<latexit sha1_base64="CnQfQz/Z5G7hE8FRE4mJxR4QiJ8=">AAAFi3icnZRRb9MwEMc9WGEMBhs88mIRIQ0xVc2YYGU8TKBJCGljjLVrVXeT4zitNccJtrOtsvIpeIUPxrfBTts1S8cLliKd/bs7/8/nOEg5U7rR+LNw5+5i7d79pQfLDx+tPH6yuva0rZJMEtoiCU9kJ8CKciZoSzPNaSeVFMcBpyfB+SfHTy6oVCwRx3qU0n6MB4JFjGBtl7oozs6Mfu3nZ6teo94oBiwZzea232xCf7Ligck4PFtb5ChMSBZToQnHSvX8Rqr7BkvNCKf5MsoUTTE5xwPas6bAMVV9UyjO4Uu7EsIokfYTGhar5QiDY6VGcWA9Y6yHqsrc4m2sl+lou2+YSDNNBRlvFGUc6gS68mHIJCWaj6yBiWRWKyRDLDHR9pCWkaCXJIljLEKDOrlBbgOCuenk+U3YLcFuFbZKsFWF7RJsV+F+Ce5X4UEJHlRhEF1NcBCZq3k6mtHRPO3M6FypQdSd0blamQjdZbJN7W32x26hMvZCGc/PUcAG68bbLIxXlUicpnKqWcZmPK34qCxwVWcipNJd8fm6JVUszGjpaL7m6KPdFkW2q1aITfJD6kmqAlWFHO1/p3om3+U4svrtv7Fhazh1+tft5BTJYVKUtQGRKE3n8h3cnk/8Z75QX+Q9v399UmEhbmwf2ysEi2OGng9vO2ZiW3Pt7ibVNgRqnJ3TSCNuHwvt+UiywVBDJN20EkCvUlIqLwjMnuu2bbLL0JsG96ut1FbGzlSIqut6jnYqPjy5pPIC86Kfs6Z/yd37NH2E4L+N9mbdf1vf+rbl7W5NXqol8By8AOvAB+/ALvgMDkELEBCDn+AX+F1bqb2pva99GLveWZjEPAM3Rm3vLy6++Ig=</latexit>µt+1
<latexit sha1_base64="E271Nr1+xV8yKXwjj7HcD/V43RU=">AAACk3icbZBdS9xAFIZn0y+7rbq2eNWb0KVgQZZksR/UG7W96I2i0lVhs4bJZOIOzkeYObGGYX6Pv6a3Lf03nazBdtceGHh5n3M4Z96s5MxAFP3uBA8ePnr8ZOlp99nz5ZXV3tqLE6MqTeiIKK70WYYN5UzSETDg9KzUFIuM09Ps8nPDT6+oNkzJb1CXdCLwhWQFIxi8lfZ2E4FhSjC3x+480VOV2kRUm+A27sC+S+HcHqSx27zzDlpv6N6mvX40iGYV3hdxK/qorcN0rfMpyRWpBJVAODZmHEclTCzWwAinrptUhpaYXOILOvZSYkHNxM7+6sI33snDQmn/JIQz998Ji4Uxtch8Z3OsWWSN+T82rqD4OLFMlhVQSW4XFRUPQYVNcGHONCXAay8w0czfGpIp1piAj7ebSPqdKCGwzG1y7ewsqayw187Ns/ovqxdZ4VmmeN7cr7gt3FwWNhOu69OOF7O9L06Gg/j94N3RVn9n2Oa+hF6h12gDxegD2kFf0SEaIYJu0A/0E/0K1oPtYC/4ctsadNqZl2iugv0/xSDPgg==</latexit>R⇢µ,t(MN1t,NN2t)
Figure 4: An illustration of Lemma 3.Published as a conference paper at AAAI 2024
Definition 8 (Hausdorff distance) .For a normed space (X,∥·∥), the Hausdorff distance between the sets A, B⊆ X
is defined as
distH(A, B) = max
sup
a∈Ainf
b∈B∥a−b∥,sup
b∈Binf
a∈A∥a−b∥
. (39)
Based on the Hausdorff distance, we have the following notion of Lipschitz continuity for correspondences [Freeman
and Kokotovic, 2008].
Definition 9. A correspondence Γ :X⇝ZisLΓ-Lipschitz continuous under the Hausdorff distance if, for all
x, x′∈ X, it satisfies that
distH(Γ(x),Γ(x′))≤LΓ∥x−x′∥. (40)
The following lemma presents a Lipschitz continuity result for the reachability correspondences.
Lemma 4. The reachability correspondences Rµ,tandRν,tin(30) and(31) satisfy the following inequalities for all
µt, µ′
t∈ P(X)andνt, ν′
t∈ P(Y),
distH(Rρ
µ,t(µt, νt),Rρ
µ,t(µ′
t, ν′
t))≤LRµ,t 
dTV 
µt, µ′
t
+ dTV 
νt, ν′
t
,
distH(Rρ
ν,t(µt, νt),Rρ
ν,t(µ′
t, ν′
t))≤LRν,t 
dTV 
µt, µ′
t
+ dTV 
νt, ν′
t
,(41)
where LRµ,t= 1 +1
2Lft,LRν,t= 1 +1
2Lgt, and LftandLgtare the Lipschitz constants in Assumption 1.
Proof. The proof is postponed to Appendix C.2.
Leveraging the continuity of the reachability correspondences, the following theorem establishes the Lipschitz conti-
nuity of the optimal value functions.
Theorem 3. The optimal lower value function Jρ∗
cor,tand the optimal upper value function ¯Jρ∗
cor,tare both Lipschitz
continuous. Formally, for all µρ
t, µρ′
t∈ P(X)andνρ
t, νρ′
t∈ P(Y), the following inequalities hold
Jρ∗
cor,t(µρ
t, νρ
t)−Jρ∗
cor,t(µρ′
t, νρ′
t)≤LJ,t 
dTV 
µρ
t, µρ′
t
+ dTV 
µρ
t, νρ′
t
,
¯Jρ∗
cor,t(µρ
t, νρ
t)−¯Jρ∗
cor,t(µρ′
t, νρ′
t)≤LJ,t 
dTV 
µρ
t, µρ′
t
+ dTV 
µρ
t, νρ′
t
,(42)
where the Lipschitz constant LJ,tis given by
LJ,t=Lr 
1 +T−1X
k=tkY
τ=t(LRρ
µ,τ+LRρ
ν,τ)
. (43)
Proof. Observe that the lower value in (33) takes the form: f(x, y) = max p∈Γ(x,y)minq∈Θ(x,y)g(p, q), which can be
viewed as an extension of the marginal function [Freeman and Kokotovic, 2008] to the max-min case. We present a
continuity result for this type of marginal function in Lemma 11 in Appendix C.1. Based on Lemma 11, we can prove
the Lipschitz continuity result through an inductive argument since the terminal rewards are assumed to be Lipschitz.
A detailed proof is given in Appendix C.3.
5.3 Performance Guarantees
In the previous section, we obtained an optimal Blue coordination strategy α∗for the coordinator game. The coor-
dinator game is constructed under the mean-field setting, i.e., both teams have an infinite number of agents and all
agents in each team apply the same strategy. We analyze the performance guarantees for α∗in the finite-population
game and compare the worst-case performance of this coordinator-induced Blue team strategy to the original max-min
optimization in (10) where agents have the flexibility to apply non-identical strategies within each team.
To this end, let the Red team apply non-identical team strategies ψN2= (ψN2
1, . . . ψN2
N2)∈ΨN2. Recall that a
Blue coordination strategy induces an identical Blue team strategy (see Remark 3). We denote the finite-population
performance under the identical Blue team strategy induced by α∗and the Red team strategy ψN2as
JN,α∗,ψN2
0 (xN1
0,yN2
0) =Eα∗,ψN2hTX
t=0rρ
t(MN1
t,NN2
t)|XN1
0=xN1
0,YN2
0=yN2
0i
.
Through dynamic programming, we can compute the above-induced value through a backward propagation scheme
JN,α∗,ψN2
T (xN1
T,yN2
T) =rρ
T(µN1
T, νN2
T) (44a)Published as a conference paper at AAAI 2024
JN,α∗,ψN2
t (xN1
t,yN2
t) =rρ
t(µN1
t, νN2
t) +Eα∗,ψN2h
JN,α∗,ψN2
t+1 (XN1
t+1,YN2
t+1)XN1
t=xN1
t,YN2
t=yN2
ti
, (44b)
∀t= 0, . . . , T −1,
where µN1
t= Empµ(xN1
t)andνN2
t= Empν(yN2
t)are the EDs corresponding to the joint states xN1
tandyN2
t,
respectively.
We have the following main result regarding the performance guarantees for the optimal Blue coordination strategy.
Theorem 4. The optimal Blue coordination strategy α∗obtained from (27) induces an ϵ-optimal Blue team strategy.
Formally, for all xN1∈ XN1,yN2∈ YN2,
JN∗(xN1,yN2)≥min
ψN2∈ΨN2JN,α∗,ψN2(xN1,yN2)≥JN∗(xN1,yN2)− O1√N
(45)
where N= min {N1, N2}.
Proof. We first prove the first inequality in (45).
JN∗(xN1,yN2) = max
ϕN1∈ΦN1min
ψN2∈ΨN2JN,ϕN1,ψN2(xN1,yN2)
(i)
≥max
ϕ∈Φmin
ψN2∈ΨN2JN,ϕ,ψN2(xN1,yN2)(ii)= max
α∈Amin
ψN2∈ΨN2JN,α,ψN2(xN1,yN2)
≥min
ψN2∈ΨN2JN,α∗,ψN2(xN1,yN2),
where inequality (i) is a result of Φ⊆ΦN2, and equality (ii) is due to the one-to-one correspondence between
coordination strategies and identical team strategies (see Remark 3).
For the second inequality in (45), we break it down into two lemmas: First, Lemma 5 states that
minψN2∈ψN2JN,α∗,ψN2≥Jρ∗
cor−ϵ1; and Lemma 6 shows that Jρ∗
cor≥JN∗−ϵ2; finally, it is shown that both
error terms are of order O
1√
N
. Combining the two lemmas, we obtain the desired result.
Remark 7. Recall that α∗is solved at the infinite-population limit under the restriction that both teams apply identical
team strategies. Theorem 4 states that the identical Blue team strategy induced by α∗is still ϵ-optimal, even if (i) it is
deployed in a finite-population game and (ii) the opponent team employs non-identical strategies to exploit.
Remark 8. Continuity Assumptions 1 and 2 are necessary to translate the infinite-population performance back to the
finite-population game. See Appendix A.2 for a discontinuous example where the infinite-population game value is
significantly different from that of the finite-population problem.
Lemma 5. For all joint states xN1∈ XN1andyN2∈ YN2, the optimal Blue coordination strategy α∗in(27)
guarantees
min
ψN2∈ψN2JN,α∗,ψN2(xN1,yN2)≥Jρ∗
cor(µN1, νN2)− O1√N
, (46)
where µN1= Empµ(xN1)andνN2= Empν(yN2)are the corresponding EDs.
Proof. The proof is constructed based on induction. Fix an arbitrary Red team strategy ψN2∈ΨN2.
Base case: At the terminal timestep T, since there is no decision to be made, both value functions are equal to the
terminal reward and are thus the same. Formally, for all xN1
T∈ XN1andyN2
T∈ YN2,
JN,α∗,ψN2
T (xN1
T,yN2
T) =Jρ∗
cor,T(µN1
T, νN1
T) =rρ
T(µN1
T, νN2
T),
where µN1
T= Empµ(xN1
T)andνN2
T= Empν(yN2
T). For simplicity, we do not emphasize the correspondence between
the joint states and the EDs for the rest of the proof, as it is clear from the context.
Inductive hypothesis: Assume that at t+ 1, the following holds for all joint states xN1
t+1∈ XN1andyN2
t+1∈ YN2:
JN,α∗,ψN2
t+1 (xN1
t+1,yN2
t+1)≥Jρ∗
cor,t+1(µN1
t+1, νN2
t+1)− O1√N
. (47)Published as a conference paper at AAAI 2024
Induction: At timestep t, consider an arbitrary pair of joint states xN1
t∈ XN1andyN2
t∈ YN2, and their correspond-
ing EDs µN1
tandνN2
t. Define µ∗
t+1as
µ∗
t+1=µN1
tF(µN1
t, νN2
t, α∗
t).
Note that, from the optimality of α∗
t, we have
µ∗
t+1∈ argmax
µt+1∈Rρ
µ,t(µN1
t,νN2
t)min
νt+1∈Rρ
ν,t(µN1
t,νN2
t)Jρ∗
cor,t+1(µt+1, νt+1). (48)
Furthermore, from Theorem 2, there exists a νapprx ,t+1∈ Rρ
ν,t(µN1
t, νN2
t)for the Red team policy ψN2
tsuch that
EψN2
th
dTV 
NN2
t+1, νapprx ,t+1XN1
t=xN1
t,YN2
t=yN2
ti
≤|Y|
2r
1
N2. (49)
For notational simplicity, we drop the conditions XN1
t=xN1
tandYN2
t=yN2
tin the following derivations. Then, for
all joint states xN1
t∈ XN1andyN2
t∈ YN2, we have
JN,α∗,ψN2
t (xN1
t,yN2
t) =rρ
t(µN1
t, νN2
t) +Eα∗,ψN2h
JN,α∗,ψN2
t+1 (XN1
t+1,YN2
t+1)i
(50)
(i)
≥rρ
t(µN1
t, νN2
t) +Eα∗,ψN2h
Jρ∗
cor,t+1(MN1
t+1,NN2
t+1)i
− O1√N
(51)
=rρ
t(µN1
t, νN2
t)− O1√N
(52)
+Eα∗,ψN2h
Jρ∗
cor,t+1(MN1
t+1,NN2
t+1)−Jρ∗
cor,t+1(µ∗
t+1, νapprx ,t+1) +Jρ∗
cor,t+1(µ∗
t+1, νapprx ,t+1)i
(ii)
≥rρ
t(µN1
t, νN2
t) +Jρ∗
cor,t+1(µ∗
t+1, νapprx ,t+1)− O1√N
(53)
−LJ,t+1Eα∗h
dTV 
MN1
t+1, µ∗
t+1i
| {z }
=O(1√
N1)due to Lemma 1−LJ,t+1EψN2h
dTV 
NN2
t+1, νapprx ,t+1i
| {z }
=O(1√
N2)due to (49)
(iii)=rρ
t(µN1
t, νN2
t) +Jρ∗
cor,t+1(µ∗
t+1, νapprx ,t+1)− O1√N
(54)
(iv)
≥rρ
t(µN1
t, νN2
t) + min
νt+1∈Rρ
ν,t(µN1
t,νN2
t)Jρ∗
cor,t+1(µ∗
t+1, νt+1)− O1√N
(55)
(v)=rρ
t(µN1
t, νN2
t) + max
µt+1∈Rρ
µ,t(µN1
t,νN2
t)min
νt+1∈Rρ
ν,t(µN1
t,νN2
t)Jρ∗
cor,t+1(µt+1, νt+1)− O1√N
(56)
=Jρ∗
cor,t(µN1
t, νN2
t)− O1√N
. (57)
For inequality (i), we used the inductive hypothesis; for inequality (ii), we utilized the Lipschitz continuity of the
coordinator value function in Theorem 3; equality (iii) is due to Lemma 1 and Theorem 2, and the fact that the two
error terms with Lipschitz constant LJ,t+1are bounded by O
1√
N
; inequality (iv) is due to the fact that νapprx ,t+1
is in the reachable set; equality (v) comes from the definition of µ∗
t+1in (48), and the final equality in (57) is simply
the definition of Jρ∗
cor,t, which completes the induction.
Since the Red team strategy ψN2∈ΨN2is arbitrary, we have that, for all joint states xN1∈ XN1andyN2∈ YN2,
min
ψN2∈ψN2JN,α∗,ψN2(xN1,yN2) = min
ψN2∈ψN2JN,α∗,ψN2
0 (xN1,yN2)
≥Jρ∗
cor,0(µ, ν)− O1√N
=Jρ∗
cor(µN1, νN2)− O1√N
.Published as a conference paper at AAAI 2024
Lemma 6. The following inequality holds for all joint states xN1∈ XN1andyN2∈ YN2,
Jρ∗
cor(µN1, νN1)≥JN∗(xN1,yN2)− O1√N
, (58)
where µN1= Empµ(xN1)andνN2= Empν(yN2).
Proof. We prove the lemma through an inductive argument.
Base case: At the terminal timestep T, the two value functions are the same. Thus, we have, for all joint states
xN1
T∈XN1andyN2
T∈YN2, that
Jρ∗
cor,T(µN1
T, νN1
T) =JN∗
T(xN1
T,yN2
T) =rρ
t(µN1
T, νN1
T).
Inductive hypothesis: Assume that, at time step t+ 1, the following holds for all xN1
t+1∈XN1andyN2
t+1∈YN2,
Jρ∗
cor,t+1(µN1
t+1, νN1
t+1)≥JN∗
t+1(xN1
t+1,yN2
t+1)− O1√N
. (59)
Induction: Consider arbitrary xN1
t∈XN1andyN2
t∈YN2. For each Blue team policy ϕN1
t∈ΦN1
t, Theorem 2
allows us to define µϕN1
t
apprx ,t+1∈ R µ(µN1
t, νN2
t)such that
EϕN1
t
dTV 
MN1
t+1, µϕN1
t
apprx ,t+1XN1
t=xN1
t,YN2
t=yN2
t
≤|X|
2r
1
N1. (60)
For an identical Red team policy ψt∈Ψt, denote νψt
t+1=νN2
tGρ
t(µN1
t, νN2
t, ψt). Then, we have
JN∗
t(xN1
t,yN2
t) = max
ϕN1
t∈ΦN1
tmin
ψN2
t∈ΨN2
trρ
t(µN1
t, νN2
t) +EϕN1
t,ψN2
th
JN∗
t+1(XN1
t+1,YN2
t+1)i
(i)
≤rρ
t(µN1
t, νN2
t) + max
ϕN1
t∈ΦN1
tmin
ψN2
t∈ΨN2
tEϕN1
t,ψN2
th
Jρ∗
cor,t+1(MN1
t+1,NN2
t+1)i
+O1√N
(ii)
≤rρ
t(µN1
t, νN2
t) + max
ϕN1
t∈ΦN1
tmin
ψt∈ΨtEϕN1
t,ψth
Jρ∗
cor,t+1(MN1
t+1,NN2
t+1)i
+O1√N
=rρ
t(µN1
t, νN2
t) +O1√N
+ max
ϕN1
t∈ΦN1
tmin
ψt∈ΨtEϕN1
t,ψth
Jρ∗
cor,t+1(MN1
t+1,NN2
t+1)
−Jρ∗
cor,t+1(µϕN1
t
apprx ,t+1, νψt
t+1) +Jρ∗
cor,t+1(µϕN1
t
apprx ,t+1, νψt
t+1)i
(iii)
≤rρ
t(µN1
t, νN2
t) + max
ϕN1
t∈ΦN1
tmin
ψt∈ΨtJρ∗
cor,t+1(µϕN1
t
apprx ,t+1, νψt
t+1) +O1√N
+LJ,t+1EϕN1
t,ψth
dTV 
MN1
t+1, µϕN1
t
apprx ,t+1
+ dTV 
NN2
t+1, νψt
t+1i
(iv)
≤rρ
t(µN1
t, νN2
t) + max
ϕN1
t∈ΦN1
tmin
ψt∈ΨtJρ∗
cor,t+1(µϕN1
t
apprx ,t+1, νψt
t+1) +O1√N
(v)=rρ
t(µN1
t, νN2
t) + max
ϕN1
t∈ΦN1
tmin
νt+1∈Rρ
ν,t(µN1
t,νN2
t)Jρ∗
cor,t+1(µϕN1
t
apprx ,t+1, νt+1) +O1√N
(vi)
≤rρ
t(µt, νt) + max
µt+1∈Rρ
µ,t(µN1
t,νN2
t)min
νt+1∈Rρ
ν,t(µN1
t,νN2
t)Jρ∗
cor,t+1(µt+1, νt+1) +O1√N
=Jρ∗
cor,t(µt, νt) +O1√N
.
For inequality (i), we used the inductive hypothesis; for inequality (ii), we reduced the optimization domain of the
Red team to the identical policy space; inequality (iii) is a result of the Lipschitz continuity; for inequality (iv), we
use Lemma 1 and Theorem 2; equality (v) holds, since for all µt+1in the reachable set, there is an identical Red team
strategy that induces it, and vice versa. Consequently, switching from optimizing with the identical Red team policiesPublished as a conference paper at AAAI 2024
to the reachable set does not change the minimization domain; inequality(vi) is due to the fact that µϕN1
t
apprx ,t+1is always
in the reachable set by construction.
6 Numerical Example
In this section, we provide two numerical examples. The first one illustrates a scenario where the lower and upper
coordinator game values are different. The second one verifies the performance guarantees in the finite-population
game. For both examples, the states spaces for the Blue and Red team are X={x1, x2}andY={y1, y2}, and the
action spaces are U={u1, u2}andV={v1, v2}. The two-state state spaces allow the joint mean-fields (µt, νt)to
be fully characterized solely by µt(x1)andνt(y1).
The code that implements the two examples can be found at https://github.com/scottyueguan/MFTG.
6.1 Numerical Example 1
We consider a following two-stage example with a population ratio ρ= 0.6. The time-invariant transition kernel for
the Blue agents is defined as follows:
fρ(x1|x1, u1, µ, ν) = 0 .5 
1 + 
ρµ(x1)−(1−ρ)ν(y1)
, fρ(x2|x1, u1, µ, ν) = 0 .5 
1− 
ρµ(x1)−(1−ρ)ν(y1)
,
fρ(x1|x1, u2, µ, ν) = 0 .5 
1−0.3 
ρµ(x1)−(1−ρ)ν(y1)
, fρ(x2|x1, u2, µ, ν) = 0 .5 
1 + 0 .3 
ρµ(x1)−(1−ρ)ν(y1)
,
fρ(x1|x2, u1, µ, ν) = 0 .5 
1− 
ρµ(x2)−(1−ρ)ν(y2)
, fρ(x2|x2, u1, µ, ν) = 0 .5 
1 + 
ρµ(x2)−(1−ρ)ν(y2)
,
fρ(x1|x2, u2, µ, ν) = 0 .5 
1 + 0 .3 
ρµ(x2)−(1−ρ)ν(y2)
, fρ(x2|x2, u2, µ, ν) = 0 .5 
1−0.3 
ρµ(x2)−(1−ρ)ν(y2)
.
The time-invariant Red transition kernel is similarly defined as
gρ(y1|y1, v1, µ, ν) = 0 .5 
1 + 
(1−ρ)ν(y1)−ρµ(x1)
, gρ(y2|y1, v1, µ, ν) = 0 .5 
1− 
(1−ρ)ν(y1)−ρµ(x1)
,
gρ(y1|y1, v2, µ, ν) = 0 .5 
1−0.3 
(1−ρ)ν(y1)−ρµ(x1)
, gρ(y2|y1, v2, µ, ν) = 0 .5 
1 + 0 .3 
(1−ρ)ν(y1)−ρµ(x1)
,
gρ(y1|y2, v1, µ, ν) = 0 .5 
1− 
(1−ρ)ν(y2)−ρµ(x2)
, gρ(y2|y2, v1, µ, ν) = 0 .5 
1 + 
(1−ρ)ν(y2)−ρµ(x2)
,
gρ(y1|y2, v2, µ, ν) = 0 .5 
1 + 0 .3 
(1−ρ)ν(y2)−ρµ(x2)
, gρ(y2|y2, v2, µ, ν) = 0 .5 
1−0.3 
(1−ρ)ν(y2)−ρµ(x2)
.For simplicity, we only consider non-zero reward at the terminal time step T= 2. Specifically,
rρ
0(µ, ν) =rρ
1(µ, ν) = 0 ∀µ∈ P(X), ν∈ P(Y),
rρ
2(µ, ν) =µ(x2).
In other words, the objective for the Blue team is to maximize its population fraction on state x2att= 2. While the
Red distribution does not play a role in the reward structure, the Red agents need to strategically distribute themselves
to influence the Blue agent’s transitions in order to minimize the terminal reward.
The game values for the coordinator game are computed through discretization, where we uniformly mesh the two-
dimensional simplices P(X)andP(Y)into 500 bins9. The numerically solved values are presented in Figure 5.
min-maxmax-min(a)<latexit sha1_base64="XWNb9lj+W+G2oTlJ/kA9yRRKobo=">AAAFhXicnZTfT9swEMfNRgdjv2B73Eu0aBKTWNVUiE1C09CmSdMkGANaiuqAbMdpLRwnsx2givIn7HX72/bfzE4LDQ5PsxTpzp/z+Xt3kXHGmdKdzt+Fe/cXWw+Wlh+uPHr85Omz1bXnfZXmktAeSXkqBxgpypmgPc00p4NMUpRgTo/x+WfLjy+oVCwVR3qS0TBBI8FiRpA2W4f6Q+ds1e+0O9XymkYwM3wwW/tna4scRinJEyo04UipYdDJdFggqRnhtFyBuaIZIudoRIfGFCihKiwqraX32uxEXpxK8wntVbv1EwVKlJok2EQmSI+Vy+zmXWyY6/h9WDCR5ZoKMr0ozrmnU88W7kVMUqL5xBiISGa0emSMJCLatGcFCnpJ0iRBIirgoCygvYAgXgzK8jY8qcETF/ZqsOfCfg32Xbhbg7su3KvBPRfi+GqGcVxcNelkTidNOpjTRqk4PpnTRq1MRPY3MkMddsNpWKSKoDwr/KCEmI3WC79bGW+ckyjL5LVmmRRT14lRObZV5yKi0v7czbolVSzKaa0130v4yVwLYzNVI8Qk+Sn1LFWFXCEHu4dUz+XbHAdGP0zyDVPDqdW/bpxTKMdpVdaGB0XNbeTbuzuf+M98kb4oh0F406moEje1j8wv5FVt9vzAu6vNxIzmJtw67hiwmmbnNNaQm2dC+wGUbDTWHpTWdQ7Qq4zUysO4+GKnbYZsMwyvD4fuKLWRsX0tRLV1u4TbTgxPL6m8QLya53zo38rSvE+B+xo1jX63HWy1N39s+jvd2Uu1DF6CV2AdBOAd2AFfwT7oAQJG4Bf4Df60llpvW5utrWnovYXZmRfg1mp9/AdpK/WZ</latexit>t=0(b)<latexit sha1_base64="PbSAZptgQ2+CNHeXThDWx8kKqyY=">AAAFhXicnZTfT9swEMfNRgdjv2B73Iu1aBKTWNVUiE1C09CmSdMkGANaiuqAHMdpLRwnsx2gsvIn7HX72/bfzEkLDQ5PsxTpzp/z+Xt3kcOMM6U7nb8L9+4vth4sLT9cefT4ydNnq2vP+yrNJaE9kvJUDkKsKGeC9jTTnA4ySXEScnocnn8u+fEFlYql4khPMhokeCRYzAjWdutQf/DPVr1Ou1Mt2DT8meGB2do/W1vkKEpJnlChCcdKDf1OpgODpWaE02IF5YpmmJzjER1aU+CEqsBUWgv42u5EME6l/YSG1W79hMGJUpMktJEJ1mPlsnLzLjbMdfw+MExkuaaCTC+Kcw51CsvCYcQkJZpPrIGJZFYrJGMsMdG2PStI0EuSJgkWkUGDwqDyAoK5GRTFbXhSgycu7NVgz4X9Guy7cLcGd124V4N7LgzjqxkOY3PVpJM5nTTpYE4bpYbxyZw2amUiKn8jO9RhN5iGRcr4xZnx/AKFbLRuvG5lvHFO4iyT15plYqauE6PysKw6FxGV5c/drFtSxaKc1lrzvUCf7LUotlO1QmySn1LPUlXIFXKwe0j1XH6Z48DqR0m+YWs4LfWvW+cUyXFalbUBkai5jXx7d+cT/5kv0hfF0A9uOhVV4qb2kf2FYNVm6PnwrjYTO5qb8NJxxxCqaXZOY424fSa05yPJRmMNkSxd5wC9ykitvDA0X8pp2yGXGYbXhwN3lNrK2L4Wotq6XaBtJ4anl1ReYF7Ncz70b0Vh3yfffY2aRr/b9rfamz82vZ3u7KVaBi/BK7AOfPAO7ICvYB/0AAEj8Av8Bn9aS623rc3W1jT03sLszAtwa7U+/gNuSvWa</latexit>t=1(c)<latexit sha1_base64="PbSAZptgQ2+CNHeXThDWx8kKqyY=">AAAFhXicnZTfT9swEMfNRgdjv2B73Iu1aBKTWNVUiE1C09CmSdMkGANaiuqAHMdpLRwnsx2gsvIn7HX72/bfzEkLDQ5PsxTpzp/z+Xt3kcOMM6U7nb8L9+4vth4sLT9cefT4ydNnq2vP+yrNJaE9kvJUDkKsKGeC9jTTnA4ySXEScnocnn8u+fEFlYql4khPMhokeCRYzAjWdutQf/DPVr1Ou1Mt2DT8meGB2do/W1vkKEpJnlChCcdKDf1OpgODpWaE02IF5YpmmJzjER1aU+CEqsBUWgv42u5EME6l/YSG1W79hMGJUpMktJEJ1mPlsnLzLjbMdfw+MExkuaaCTC+Kcw51CsvCYcQkJZpPrIGJZFYrJGMsMdG2PStI0EuSJgkWkUGDwqDyAoK5GRTFbXhSgycu7NVgz4X9Guy7cLcGd124V4N7LgzjqxkOY3PVpJM5nTTpYE4bpYbxyZw2amUiKn8jO9RhN5iGRcr4xZnx/AKFbLRuvG5lvHFO4iyT15plYqauE6PysKw6FxGV5c/drFtSxaKc1lrzvUCf7LUotlO1QmySn1LPUlXIFXKwe0j1XH6Z48DqR0m+YWs4LfWvW+cUyXFalbUBkai5jXx7d+cT/5kv0hfF0A9uOhVV4qb2kf2FYNVm6PnwrjYTO5qb8NJxxxCqaXZOY424fSa05yPJRmMNkSxd5wC9ykitvDA0X8pp2yGXGYbXhwN3lNrK2L4Wotq6XaBtJ4anl1ReYF7Ncz70b0Vh3yfffY2aRr/b9rfamz82vZ3u7KVaBi/BK7AOfPAO7ICvYB/0AAEj8Av8Bn9aS623rc3W1jT03sLszAtwa7U+/gNuSvWa</latexit>t=1
<latexit sha1_base64="phhA2LL2kLWQ4rDWdMcRmj2Ehhw=">AAAF3HicnZRPb9MwFMA9WGGUfxscuVhUSBsaVbNOW9EuEwgJTdoYY+061W3lOE5rLXGC7WyrIt+4Ia58HL4Hd67wGbDTds3SccFSpGf/3nt5f/zsxgGTqlb7uXDr9mLpzt2le+X7Dx4+ery88qQlo0QQ2iRREIm2iyUNGKdNxVRA27GgOHQDeuKevbX85JwKySJ+rEYx7YZ4wJnPCFbmqL/cRiHj/RTxBCLGIQqxGhIcpEc6O1yvaQ3RUMaY0PRVrVrbZlzDvV6KxDB6aXWsgQhTEgm97uhVFCbrxm6t3F+uGPVsQSM06o36lhHqtfp2w4HOBFXAZB32VxYD5EUkCSlXJMBSdpxarLopFoqRgOoySiQ1cZzhAe0YkeOQym6alUDDF+bEg34kzMcVzE7zFikOpRyFrtG0Ecsis4c3sU6i/EY3ZTxOFOVk/CM/CaCKoK0n9JigRAUjI2AimIkVkiEWmChT9TLi9IJEYYi5l6K2Tq/q29b6OjzNwdMibOZgswhbOdgqwv0c3C/Cgxw8KELXv5xg108v5+loRkfztD2jc6m6/umMzuXKuGdvp2lqZ6M7VvNk6pi7VnE0ctlgNa1sZMJawRLHsZjGbK7keFvQkYlrs064R4Wdmfm8BZXMS2iuNB80emN+i3zTVROIcfJZqImrDBUDOdr/RNUs/Kt5MrNhcujZ+O2g9OwQZWmtQzMzs+2cv4Ob/fH/9Oepc91xuleV8nKDfNyyE2/LDCsOvKnMdtSvzX2xDa4cew+or1BgXh9VcZBgg6GCSNhtwYBexiSXnuum72y3TZOth87UuFtspTJh7EwDkVVV1WinoBNEF1Sc4yDr56zpe1qb92n6CMF/C62NqrNV3fy4Wdl9PXmplsAz8BysAgdsg13wHhyCJiDgB/gFfoM/pV7pS+lr6dtY9dbCxOYpuLZK3/8C8AMYtw==</latexit>min⌫2R⌫,0J⇢⇤cor,1(µ,⌫)
(d)
<latexit sha1_base64="Vr1u0zJ6ll4ESgZ0EvxQQqR4AE8=">AAAF3HicnZRPb9MwFMA9WGGMfxscuVhUSBsaVbNOW9EuEwgJTdoYY+061W3lOE5rLXGC7WytIt+4Ia58HL4Hd67wGbDTds3SccFSpGf/3nt5f/zsxgGTqlr9uXDr9mLpzt2le8v3Hzx89Hhl9UlTRokgtEGiIBItF0saME4biqmAtmJBcegG9NQ9f2v56QUVkkX8RI1i2glxnzOfEazMUW+lhUI87KUoTCBiHJqdGhAcpMc6O9yoag3RQMaY0PRVtVLdYVzD/W6KxCB6mekYAxGmJBJ6w9Fr1gbxZH25t1I26tmCRqjX6rVtI9SqtZ26A50JKoPJOuqtLgbIi0gSUq5IgKVsO9VYdVIsFCMB1csokdTEcY77tG1EjkMqO2lWAg1fmBMP+pEwH1cwO81bpDiUchS6RtNGLIvMHt7E2ony652U8ThRlJPxj/wkgCqCtp7QY4ISFYyMgIlgJlZIBlhgokzVlxGnlyQKQ8y9FLV0elXfltbX4VkOnhVhIwcbRdjMwWYRHuTgQREe5uBhEbr+cIJdPx3O09GMjuZpa0bnUnX9sxmdy5Vxz95O09T2Zmes5snUMXet7Gjksv5aWt7MhPWCJY5jMY3ZXMnxtqAjE9dmnXCPCjsz83kLKpmX0FxpPmj0xvwW+aarJhDj5LNQE1cZKgZyfPCJqln4+XkyOXRt/HZQunaIsrQ2oJmZ2XbO3+HN/vh/+vPUhW47natKeblBPmnaibdlhmUH3lRmO+rX5r7YBleOvQfUVygwr48qO0iw/kBBJOy2YECHMcml57rpO9tt02TroT017hRbqUwYu9NAZEVVNNot6ATRJRUXOMj6OWv6vtbmfZo+QvDfQnOz4mxXtj5ulfdeT16qJfAMPAdrwAE7YA+8B0egAQj4AX6B3+BPqVv6Uvpa+jZWvbUwsXkKrq3S97/wGBi3</latexit>maxµ2Rµ,0J⇢⇤cor,1(µ,⌫)<latexit sha1_base64="PBmkh+mazriNxwCthZZY26MJKU4=">AAAFl3icnZTdTtswFMcNoxtjX2VcTbuxFk1iEqqSjsEQF0ObNqFJMGC0FDWhchyntXCczHaAKsqz7HZ7pL3N7PQrpOxmliId+3d88j/n2PYTRqWy7T8Li/eWavcfLD9cefT4ydNn9dXnbRmnApMWjlksOj6ShFFOWooqRjqJICjyGTnzLz8ZfnZFhKQxP1XDhHgR6nMaUoyUXurV19wIqQFGLDvJe5nL0w0779Utu7HVbDrbO9Bu2MUwxltnx3kHnfGKBcbjqLe6xNwgxmlEuMIMSdl17ER5GRKKYkbyFTeVJEH4EvVJV5scRUR6WaE+h6/1SgDDWOiPK1islndkKJJyGPna02iVVWYW72LdVIXvvYzyJFWE49GPwpRBFUNTChhQQbBiQ20gLKjWCvEACYSVLtiKy8k1jqMI8SBzO3k2LVQnz2/D8xI8r8JWCbaqsF2C7So8KMGDKjwswcMq9MObMfbD7GaeDmd0OE87MzqXqh+ez+hcrpQH5mDppnab3sgtkJmjD5bl5K5P++uZ1SyMN5WdKEnERLOIstG04iNT32Sd8oAIc9zn8xZE0iAlpdJ8y92P+rduqLuqheggP4QahypQVcjJwXeiZvKnFyNKN3QOF0b/up5cuGIQF2ltQH1pZtO5eId3x+P/GS9QV3nX8aaVCgpxI/tUHyFYlBlaDryrzFi3ZupuJtU2+HIUnZFQuUw/HMpyXEH7AwVdYaaVDeQmwaX0fD/7bLqtm2widCebvWorlZaxOxEiG6qRu7sVHxZfE3GFWNHPWdO/5uZ9mjxC8N9Gu9lwthqbx5vW3ub4pVoGL8ErsA4csA32wD44Ai2AwRD8BL/A79qL2ofal9r+yHVxYbxnDdwateO/qg79Kw==</latexit>R⌫,0
<latexit sha1_base64="zQAZCC6bXGKKih+BFJTqVL0VNH4=">AAAFnHicnZRbb9MwFIC9scIYtw1ekJCQRYTUSVPVlLEx7WXiIiFQxxhr16lOK8dxWmuOE2xnWxWFX8Mr/B/+DU56y9LxgqVI55zv+ORcbLsRZ0rX63+Wlm+tVG7fWb27du/+g4eP1jcet1UYS0JbJOSh7LhYUc4EbWmmOe1EkuLA5fTUPX+X8dMLKhULxYkeRdQJ8EAwnxGsjam//hQFWA8J5slx2k+QiLfqaXXUszf761a9ttNo2Lt7sF6r5ysTXtl79mtoTywWmKyj/sYKR15I4oAKTThWqmvXI+0kWGpGOE3XUKxohMk5HtCuEQUOqHKSvIQUvjQWD/qhNJ/QMLcWdyQ4UGoUuMYzS1iVWWa8iXVj7b9xEiaiWFNBxj/yYw51CLN+QI9JSjQfGQETyUyukAyxxESbrq0hQS9JGARYeAnqpMmsW500vQ7PCvCsDFsF2CrDdgG2y7BZgM0yPCzAwzJ0/asJdv3kapGO5nS0SDtzulCq65/N6UKtTHjZ6TJD7TacsZunEtucLstOkcsG1cRq5MJmaSeOIjnNWQbJWC35qNjNqo6FR2V25hfrllQxL6aF1nxJ0VvzW+SbqZpETJDvUk9C5aicyHHzG9Xz9Ge3I4i3TA29LP+qUXpIDsO8rC1obs5cXYh3eHM88Z/xPH2Rdm1n1ikvT24sn5gjBPM2Q8uGN7WZmNHM3DOlPAZXjaNz6mvEzeuhLRtJNhhqiGSmljbQq4gUynPd5EM2bTPkLEJ3utkpj1KbNPaniaiarqVov+TDw0sqLzDP5zkf+qc0Ne/T9BGC/xbajZq9U9v+um0dbE9eqlXwDLwAVWCDXXAAPoIj0AIE/AA/wS/wu/K88r7yudIcuy4vTfY8AddWpf0XhV3+tg==</latexit>R⌫,0(y1)<latexit sha1_base64="O/qq28Gl3lJ95rfW5fqCCm+okZc=">AAAFnHicnZRbb9MwFIC9scIotw1ekJCQRYTUSVPVjGoM7WXiIiFQxxhr16nOKsdxWmuOE2ynaxWFX8Mr/B/+DU7arlk6XrBU6ZzznXNyLq7diDOlG40/K6u31iq376zfrd67/+Dho43Nxx0VxpLQNgl5KLsuVpQzQduaaU67kaQ4cDk9dS/eZfx0RKVioTjRk4g6AR4I5jOCtTH1N56iAOshwTw5TvsJCuLtRlobn9tb/Q2rUW/kBxaEN3u7zb1X0J5ZLDA7R/3NNY68kMQBFZpwrFTPbkTaSbDUjHCaVlGsaITJBR7QnhEFDqhykryFFL40Fg/6oTQ/oWFuLUYkOFBqErjGMytYlVlmvIn1Yu3vOQkTUaypINMP+TGHOoTZPKDHJCWaT4yAiWSmVkiGWGKizdSqSNBLEgYBFl6CumlyNa1uml6HZwV4VobtAmyXYacAO2XYKsBWGR4W4GEZuv54hl0/GS/TyYJOlml3QZdadf2zBV3qlQkvu11mqb0dZ+rmqcQ2t8uyU+SyQS2xdnJhqxSJo0jOa5ZBMlVLPip2s65j4VGZ3fnlviVVzItpYTRfUvTWfBb5ZqumEJPku9SzVDkqF3Lc+kb1ovziv8P0cJ7VXzPKOZLDMG9rGyJRUJfyHd6cT/xnPk+P0p7tXE3Ky4ubyifmCsF8zNCy4U1jJmY1V+6ZUl6Dq6bZOfU14ub10JaNJBsMNUQyU0sBdByRQnuum3zItm2WnGXozYOd8iq1KWN/Xoiq63qK9ks+PLykcoR5vs/F0j+lqXmf5o8Q/LfQ2anbu/Xm16Z10Jy9VOvgGXgBasAGr8EB+AiOQBsQ8AP8BL/A78rzyvvK50pr6rq6Mot5Aq6dSucvUTX+rA==</latexit>Rµ,0(x1)
<latexit sha1_base64="wm3o3MKSypCSuQP7l41k+qNq+5Q=">AAAFl3icnZTfT9swEMcNoxtjv2A8TXuxFk1iEqoaVjEmHoY2bUKTYMBoKapDZTtOa+E4me0AVZS/Za/bn7T/Zk7a0pB2L7NU6ezP3eV7d65JLLg2jcafhcV7S7X7D5Yfrjx6/OTps9W1520dJYqyFo1EpDoEaya4ZC3DjWCdWDEcEsHOyOWnnJ9dMaV5JE/NMGZeiPuSB5xiY496q+soxGZAsUhPsl6KwmSzkfVWnUa9USxYMt7vbDd33kJ3fOKA8TrqrS0J5Ec0CZk0VGCtu24jNl6KleFUsGwFJZrFmF7iPutaU+KQaS8t1GfwtT3xYRAp+5MGFqfliBSHWg9DYj1zrbrK8sN5rJuYYMdLuYwTwyQdfShIBDQRzFsBfa4YNWJoDUwVt1ohHWCFqbENW0GSXdMoDLH0U9TJ0ttGdbLsLjwvwfMqbJVgqwrbJdiuwoMSPKjCwxI8rEIS3IwxCdKbWTqc0uEs7UzpTKkkOJ/SmVq59POLZYfa3fJGbr5OXXuxHDdDhPc3UmerMN5UInEcq4lmFaajbcVHJySvOpE+U/l1n61bMc39hJVa8y1DH+1nUWCnaoXYJD+UGacqUFXIycF3Zqbyy38MW8NFrn/Dbi6QGkRFWZsQydJ2Jt/h/HzyP/P55irrut5tp/xC3Mg+tVcIFm2GjgvntZna0dy655vqGIgeZRcsMEjYh8M4LlK8PzAQqXxbCWA3MS2VR0j6OZ+2HXKeoTsJ9qqjNFbG7kSIrpt6hnYrPiK6ZuoKi2Ke06F/zfL3afIIwX8b7a26u11vHjedveb4pVoGL8ErsAFc8A7sgX1wBFqAgiH4CX6B37UXtQ+1L7X9keviwjhmHdxZteO/ew39Ig==</latexit>Rµ,0
<latexit sha1_base64="zQAZCC6bXGKKih+BFJTqVL0VNH4=">AAAFnHicnZRbb9MwFIC9scIYtw1ekJCQRYTUSVPVlLEx7WXiIiFQxxhr16lOK8dxWmuOE2xnWxWFX8Mr/B/+DU56y9LxgqVI55zv+ORcbLsRZ0rX63+Wlm+tVG7fWb27du/+g4eP1jcet1UYS0JbJOSh7LhYUc4EbWmmOe1EkuLA5fTUPX+X8dMLKhULxYkeRdQJ8EAwnxGsjam//hQFWA8J5slx2k+QiLfqaXXUszf761a9ttNo2Lt7sF6r5ysTXtl79mtoTywWmKyj/sYKR15I4oAKTThWqmvXI+0kWGpGOE3XUKxohMk5HtCuEQUOqHKSvIQUvjQWD/qhNJ/QMLcWdyQ4UGoUuMYzS1iVWWa8iXVj7b9xEiaiWFNBxj/yYw51CLN+QI9JSjQfGQETyUyukAyxxESbrq0hQS9JGARYeAnqpMmsW500vQ7PCvCsDFsF2CrDdgG2y7BZgM0yPCzAwzJ0/asJdv3kapGO5nS0SDtzulCq65/N6UKtTHjZ6TJD7TacsZunEtucLstOkcsG1cRq5MJmaSeOIjnNWQbJWC35qNjNqo6FR2V25hfrllQxL6aF1nxJ0VvzW+SbqZpETJDvUk9C5aicyHHzG9Xz9Ge3I4i3TA29LP+qUXpIDsO8rC1obs5cXYh3eHM88Z/xPH2Rdm1n1ikvT24sn5gjBPM2Q8uGN7WZmNHM3DOlPAZXjaNz6mvEzeuhLRtJNhhqiGSmljbQq4gUynPd5EM2bTPkLEJ3utkpj1KbNPaniaiarqVov+TDw0sqLzDP5zkf+qc0Ne/T9BGC/xbajZq9U9v+um0dbE9eqlXwDLwAVWCDXXAAPoIj0AIE/AA/wS/wu/K88r7yudIcuy4vTfY8AddWpf0XhV3+tg==</latexit>R⌫,0(y1)<latexit sha1_base64="O/qq28Gl3lJ95rfW5fqCCm+okZc=">AAAFnHicnZRbb9MwFIC9scIotw1ekJCQRYTUSVPVjGoM7WXiIiFQxxhr16nOKsdxWmuOE2ynaxWFX8Mr/B/+DU7arlk6XrBU6ZzznXNyLq7diDOlG40/K6u31iq376zfrd67/+Dho43Nxx0VxpLQNgl5KLsuVpQzQduaaU67kaQ4cDk9dS/eZfx0RKVioTjRk4g6AR4I5jOCtTH1N56iAOshwTw5TvsJCuLtRlobn9tb/Q2rUW/kBxaEN3u7zb1X0J5ZLDA7R/3NNY68kMQBFZpwrFTPbkTaSbDUjHCaVlGsaITJBR7QnhEFDqhykryFFL40Fg/6oTQ/oWFuLUYkOFBqErjGMytYlVlmvIn1Yu3vOQkTUaypINMP+TGHOoTZPKDHJCWaT4yAiWSmVkiGWGKizdSqSNBLEgYBFl6CumlyNa1uml6HZwV4VobtAmyXYacAO2XYKsBWGR4W4GEZuv54hl0/GS/TyYJOlml3QZdadf2zBV3qlQkvu11mqb0dZ+rmqcQ2t8uyU+SyQS2xdnJhqxSJo0jOa5ZBMlVLPip2s65j4VGZ3fnlviVVzItpYTRfUvTWfBb5ZqumEJPku9SzVDkqF3Lc+kb1ovziv8P0cJ7VXzPKOZLDMG9rGyJRUJfyHd6cT/xnPk+P0p7tXE3Ky4ubyifmCsF8zNCy4U1jJmY1V+6ZUl6Dq6bZOfU14ub10JaNJBsMNUQyU0sBdByRQnuum3zItm2WnGXozYOd8iq1KWN/Xoiq63qK9ks+PLykcoR5vs/F0j+lqXmf5o8Q/LfQ2anbu/Xm16Z10Jy9VOvgGXgBasAGr8EB+AiOQBsQ8AP8BL/A78rzyvvK50pr6rq6Mot5Aq6dSucvUTX+rA==</latexit>Rµ,0(x1)
Figure 5: Subplots (a)-(c) present the game value computed via discretization. The x- and y-axes correspond to
µρ
t(x1)andνρ
t(y1), respectively. Subplot (d) illustrates the reachable sets starting from µ0= [0.96,0.04]andν0=
[0.04,0.96].
9One can easily provide an error bound on the difference between the discretized value and the true optimal value using the
Lipschitz constants. See [Guan et al., 2023] for an example.Published as a conference paper at AAAI 2024
0.960.040.960.040.41720.590.410.58280.47020.46820.53180.52980.41720.68400.31600.5828
0.46160.46130.53870.5384min-max max-min <latexit sha1_base64="qLx+o+sPjqci9yVF/HsxCFLUuP8=">AAACcHicbZDdahNBFMcn60drtDa1N4IXjgah5GLZLf3yolLwRryq0LSFbAyzs2eTofOxzMzWLsO8hU/jrb6Er+ETOJsEtGkPDPz5/87hnPnnFWfGJsnvTvTg4aPHa+tPuk+fbTzf7G29ODeq1hSGVHGlL3NigDMJQ8ssh8tKAxE5h4v86mPLL65BG6bkmW0qGAsylaxklNhgTXpxVssCdDvuPvuvLtMzNfATlwliZ1o4qrT3+Bgn8f7u+yM86fWTOJkXvivSpeijZZ1OtjqDrFC0FiAt5cSYUZpUduyItoxy8N2sNlARekWmMApSEgFm7OYf8/hdcApcKh2etHju/j/hiDCmEXnobO81q6w172Oj2pZHY8dkVVuQdLGorDm2Crcp4YJpoJY3QRCqWbgV0xnRhNqQZTeT8I0qIYgsXHbjF2Hlpbvx/jZr/rFmlZWB5YoX7f2KuzLwkG66muVdcb4bpwfx3pe9/smHZc7r6BV6i3ZQig7RCfqETtEQUfQd/UA/0a/On+hl9Dp6s2iNOsuZbXSrosFf4QbAwQ==</latexit>J⇢⇤cor=0.5298
<latexit sha1_base64="ck1+Dd7Pk70NiZUqw6Y9I+QK3Do=">AAACanicbZDfTtswFMadjDHWDVbgauLGopuEuKgS1m3cDCHtZtoVSLRFakrlOE5r1X8i29mILL8CT8Pt9h57Bx4Cp40ElB3J0qfvd47O8ZcWjGoTRf+C8MXay/VXG69bb95ubr1rb+8MtCwVJn0smVSXKdKEUUH6hhpGLgtFEE8ZGabz7zUf/iJKUykuTFWQMUdTQXOKkfHWpH2QpEjZn+7KJmomD93EJhyZmeIWS+Uc/Aaj7udPxz04aXeibrQo+FzEjeiAps4m28FhkklcciIMZkjrURwVZmyRMhQz4lpJqUmB8BxNychLgTjRY7v4koMfvZPBXCr/hIEL9/GERVzriqe+s75Xr7La/B8blSY/HlsqitIQgZeL8pJBI2GdD8yoItiwyguEFfW3QjxDCmHjU2wlgvzGknMkMptcu2VYaW6vnXvKqgdWrbLcs1SyrL5fMpt77tONV7N8LgZH3fhLt3fe65yeNDlvgD2wDw5ADL6CU/ADnIE+wOAG3II/4G9wF+6E78O9ZWsYNDO74EmFH+4BwkC98A==</latexit>¯J⇢⇤cor=0.5384
Figure 6: Max-min and min-max trajectory of the coordinator game.
While the game value Jρ∗
cor,1exists at time t= 1, it is not convex-concave, and hence the best-response correspon-
dences in the coordinator game is not convex-valued. Consequently, the upper (max-min) and lower (min-max)
game values at the previous step t= 0 differs, as observed in subplot (a). Specifically, at µρ
0= [0.96,0.04]and
νρ
0= [0.04,0.96], we have the lower value Jρ∗
cor,0= 0.5298 and the upper value ¯Jρ∗
cor,0= 0.5384 , which are visualized
as the green and yellow points. This discrepancy in the game values implies the absence of a Nash equilibrium in this
coordinator game.
Based on (33), the optimization domains for computing Jρ∗
cor,0areRµ,0(µρ
0, νρ
0)for the maximization and Rν,0(µρ
0, νρ
0)
for the minimization, both of which are plotted in (d) and visualized as the box in subplots (b) and (c). Subplot (c)
presents a zoom-in for the optimization maxRµ,0minRν,0Jρ∗
cor,1and its min-max counterpart. The marginal functions
are also plotted, from which the max-min (green point) and min-max values (yellow point) at t= 0 can be directly
obtained.
Figure 6 presents the max-min and min-max trajectory of the coordinator game. Note that the max-min solution
corresponds to the highest worst case performance for the Blue coordinator, where the Blue coordinator announces its
first move from [0.96,0.04]to[0.4172,0.5828] and then the Red coordinator exploits the announced move. If instead,
the Red coordinator announces its first move toward [0.3160,0.6840] in the max-min trajectory, the Blue coordinator
can exploit that move by achieving [0.776,0.224] att= 1and ultimately receive a higher reward of 0.5442.
6.2 Numerical Example 2
It is generally challenging to verify the suboptimality bound in Theorem 4, since computing the true optimal perfor-
mance of a finite-population team game is intractable. However, for the following simple example, we can construct
the optimal team strategies even for large but still finite-population teams.
Consider a ZS-MFTG instance with a terminal time T= 2 and population ratio ρ= 0.375. The (minimizing) Red
team’s objective is to maximize its presence at state y1att= 2, which translates to
rρ
0(µ, ν) =rρ
1(µ, ν) = 0 ∀µ∈ P(X), ν∈ P(Y),
rρ
2(µ, ν) =−ν(y1)
The Blue transition kernels are time-invariant, deterministic and independent of the MFs, and are defined as
fρ
t(x1|x1, u1, µ, ν) = 1 .0, fρ
t(x2|x1, u1, µ, ν) = 0 .0,∀µ∈ P(X), ν∈ P(Y), t∈ {0,1},
fρ
t(x1|x1, u2, µ, ν) = 0 .0, fρ
t(x2|x1, u2, µ, ν) = 1 .0,∀µ∈ P(X), ν∈ P(Y), t∈ {0,1},
fρ
t(x1|x2, u1, µ, ν) = 0 .0, fρ
t(x2|x2, u1, µ, ν) = 1 .0,∀µ∈ P(X), ν∈ P(Y), t∈ {0,1},
fρ
t(x1|x2, u2, µ, ν) = 1 .0, fρ
t(x2|x2, u2, µ, ν) = 0 .0,∀µ∈ P(X), ν∈ P(Y), t∈ {0,1}.(61)
Under the above transition kernel, a Blue agent can freely move tween the two nodes. Specifically, action u1instructs
a Blue agent to remain at its current state, while action u2directs it to transition to the other state. One can verify that
the above Blue transition kernel ensures that the Blue reachable set covers the whole simplex regardless of the initial
team distributions, i.e., Rµ,t(µ, ν) =P(X)for all µ∈ P(X),ν∈ P(Y)andt∈ {0,1}.
The Red transition kernels are time dependent and are defined as
gρ
0(y1|y1, v, µ, ν ) = 1 .0, gρ
0(y2|y1, v, µ, ν ) = 0 .0,∀v∈ V, µ∈ P(X), ν∈ P(Y), (62)
gρ
0(y1|y2, v, µ, ν ) = 0 .0, gρ
0(y2|y2, v, µ, ν ) = 1 .0,∀v∈ V, µ∈ P(X), ν∈ P(Y), (63)
gρ
1(y1|y1, v, µ, ν ) = 1 .0, gρ
1(y2|y1, v, µ, ν ) = 0 .0,∀v∈ V, µ∈ P(X), ν∈ P(Y), (64)
gρ
1(y1|y2, v1, µ, ν) = 0 .0, gρ
1(y2|y2, v1, µ, ν) = 1 .0, ∀µ∈ P(X), ν∈ P(Y), (65)Published as a conference paper at AAAI 2024
and the transitions from state y2using action v2att= 1are given by
gρ
1(y2|y2, v2, µ, ν) = 1−minn
5
(µ(x1)−1√
2)2+ (µ(x2)−(1−1√
2))2
,1o
,∀ν∈ P(Y),
gρ
1(y2|y2, v2, µ, ν) = minn
5
(µ(x1)−1√
2)2+ (µ(x2)−(1−1√
2))2
,1o
, ∀ν∈ P(Y).(66)
The Red transitions in (62) and (63) implies that all Red agents are frozen and can not change their states with any
action at t= 0. Similarly, all Red agents on state y1are frozen at t= 1as depicted in (64). The Red agents at state y2
att= 1can choose to either deterministically stay at state y2using action v1as in (65) or try to move toward state y1
using action v2. As in (66), the probability of transitioning from y2toy1is controlled by the Blue team’s distribution
µatt= 1. If the Blue team can perfectly match the target distribution [1/√
2,1−1/√
2], then no Red agent can
transition from y2toy1. Otherwise, the more the Blue team deviates from the target distribution, the more likely a
Red agent can transition from y2toy1.
Infinite-population case. Since all Red agents are frozen at t= 0 and the Red sub-population at state y2is again
frozen at t= 1, only the actions of Red agents at state y1att= 1have an impact on the game outcome. As a result,
the above setup leads to a dominant optimal Red team strategy: all Red agents at y2use action v2att= 1and try to
transition to state y1for the sake of maximizing the team’s presence at y1, regardless of the Blue team’s distribution.
Note that this is the dominant optimal strategy against all Blue team strategies in both finite and infinite-population
scenarios. Specifically,
ψN2∗
j,1(v2|y2, µ, ν) =ψρ∗
1(v2|y2, µ, ν) = 1 .0,∀µ∈ P(X), ν∈ P(Y), j∈[N2]. (67)
On the other hand, the Blue team should try to match the target distribution [1/√
2,1−1/√
2]to minimize the portion
of Red agents transitioning from y2toy1att= 1. Since the Blue reachable set at t= 0 covers the whole simplex,
the target distribution can always be achieved at t= 1 starting from any initial Blue team distribution in the infinite-
population case. One Blue coordination strategy that achieves the given target distribution is
α0(µ, ν) =(
π1
0, ifµ0(x1)<1√
2,
π2
0, ifµ0(x1)≥1√
2,(68)
where,
π1
0(u1|x1) = 1 , π1
0(u2|x1) = 0 ,
π1
0(u1|x2) =1−1/√
2
µ0(x2), π1
0(u2|x2) =1/√
2−µ0(x1)
µ0(x2),
π2
0(u1|x1) =1/√
2
µ0(x1), π2
0(u2|x1) =1−1/√
2−µ0(x2)
µ0(x1),
π2
0(u1|x2) = 1 , π2
0(u2|x2) = 0 .
The following Figure Figure 7 presents the coordinator game value, which is solved via discretization. In this example,
the upper and lower game coincides, and thus the game value exists. The black line in the middle subplot marks the
game value with µρ
1(x1) = 1 /√
2, when the Blue team perfectly achieves the target distribution. The values on the
black line dominate and give the highest performance the Blue team can achieve given any νρ
1(y1), which aligns with
our analysis which states that the Blue team should match the target distribution.
Figure 7: The values of the coordinator game at time t= 0,1and 2. The blue line in the middle figure marks the value
atµρ
1(x1) = 1 /√
2, when the Blue team perfectly achieves the target distribution.Published as a conference paper at AAAI 2024
In the infinite-population coordinator game, the Blue team can always achieve the target distribution regardless of its
initial distribution and thus completely block the migration of Red agents from y2toy1. Consequently, only the Red
agents that are initialized directly at y1can be counted toward the reward if the Blue team plays rationally, and thus
the game value is simply given by Jρ∗
0=−νρ
0(y1), which matches the game value plotted in Figure 7.
Finite-population case. The Red team’s optimal strategy remains the same as the infinite-population case. However,
the Blue team cannot achieve the irrational target distribution with finite number of agents. While the Blue team can
still match the target distribution in expectation using a (stochastic) identical team strategy, the following analysis
shows that a non-identical deterministic Blue team strategy achieves a better performance.
Consider a Blue team with three agents and all Blue agents are on node 1, i.e., µ3
0= [1,0]. The optimal Blue
coordination strategy prescribes that all Blue agents pick u1(“stay”) with probability 1/√
2andu2(“move to x2”) with
probability (1−1/√
2)to reach the target distribution in expectation . Such action selection leads to the following four
possible outcomes of the next Blue team ED µ3
1:P([1,0]) = 0 .354,P([2/3,1/3]) = 0 .439,P([1/3,2/3]) = 0 .182,
andP([0,1]) = 0 .025. In expectation, these empirical distributions lead to a transition probability of 0.518 for a Red
team agent moving from y2toy1. Consequently, we have the worst-case performance of the optimal Blue coordinator
strategy as minψN2J3,α∗,ψN2=−ν0(y1)−0.518ν0(y2).
Next, consider the non-identical deterministic Blue team strategy, under which Blue agents 1 and 2 apply action u1
and Blue agent 3 applies u2. This Blue team strategy deterministically leads to M3
1= [2/3,1/3]att= 1, and
the resultant Red team transition probability from y2toy1is 0.016. Clearly, the non-identical Blue team strategy
significantly outperforms the identical mixed team strategy in this three-agent case. Furthermore, this Blue team
strategy is optimal over the entire non-identical Blue team strategy set, resulting in a finite-population optimal game
value J3∗=−ν0(y1)−0.016ν0(y2).
We repeat the above computation for multiple Blue team size N1and plot the suboptimality gap as the blue line in
Figure 8, which verifies the O(1/√N)decrease rate predicted by Theorem 4.
Specifically, the optimal value for the finite-population game is computed based on the following:
• Att= 0, the Blue team uses a non-identical deterministic team strategy and deterministically achieves the
optimal ED µN1
1, which minimize the two-norm distance to the target distribution. Such optimal Red ED is
constructed as [n
N1,N1−n
N1]where n= argmin0≤n≤N1(n
N1−1√
2)2.
• Att= 1, all Red agents apply v2, and the distribution of the next Red EDs νN2
2can be computed based on
the optimal µN1
1achieved at t= 1.
• The expected game value is then computed based on the distribution of the Red ED νN2
2.
<latexit sha1_base64="mo0SZFqfE8NWIMZAZM+zLWpZ3q0=">AAAFuXicnZTdbtMwFIA9WGGMvw0uubGIkMY0qqYaP9JupiEkNGlljLXrVKeV4zitmeME29lWWXkSnoZbeALeBift1izZFZYiHZ/vnOPzE9tPOFO61fq7dOfucuPe/ZUHqw8fPX7ydG39WU/FqSS0S2Iey76PFeVM0K5mmtN+IimOfE5P/LOPOT85p1KxWBzraUK9CI8FCxnB2qpGa2/3h6azmcE3EEVMjAxKFLOaUTvLYI62EObJBA83t2AJjdacVrNVLFgX3LnggPk6HK0vcxTEJI2o0IRjpQZuK9GewVIzwmm2ilJFE0zO8JgOrChwRJVnivoy+MpqAhjG0n5Cw0Jb9jA4Umoa+dYywnqiqixX3sYGqQ4/eIaJJNVUkNlBYcqhjmHeLBgwSYnmUytgIpnNFZIJlpho29JVJOgFiaMIi8CgfmZQfgDB3PSz7CY8LcHTKuyWYLcKeyXYq8KDEjyowk4JdqrQDy/n2A/NZZ1OF3Rap/0FrZXqh6cLWquViSD/9exQB21vZhYo42Yj47gZ8tl4wzjtQnhd8cRJIq9ylpGZbSs2KvXzqlMRUJlfiHrdkioWpLTUmi8Z2rPHotBO1SZig/yQeh6qQNVEjg6+Ub1IP49xZPNHUbplaxjm+W/YzRDJSVyUZW+OKG1r8Tq3xxP/GS/Q59nA9a47FRTJzeRj+wvBos3QceFtbSZ2NNfm+aY6Bl/NonMaasTt06IdF0k2nmiIZL6tONDLhJTK833zKZ+2HXIeYXDl7FVHqW0aO1eJqKZuZminYsPjCyrPMS/muRj6fvE+udXXqC702k33XXP767azuzd/qVbAC/ASbAAXvAe74DM4BF1AwE/wC/wGfxo7DdyYNL7PTO8szX2egxurof4BLsAJog==</latexit>JN⇤ min N2JN,↵⇤, N2
<latexit sha1_base64="BRRBD2Z2qtXX1YICKuB5SVpX33w=">AAAFp3icnZTdTtswFMcNoxtjX7Bd7iZaNKlIqGsQ2iZxg5gmTZMojNFSVAfkOE5r4TiZ7QCV5QfY0+x2e5S9zZy00NTlapYinePf8cn/nGM5yhmVqt3+u7T8YKXx8NHq47UnT589f7G+8bIns0Jg0sUZy0Q/QpIwyklXUcVIPxcEpREjp9Hlp5KfXhEhacZP1DgnYYqGnCYUI2W3LtZ9mCI1wojpQ9MM3kH5QygNCx4TUabUHWM2bVS71a6Wt2gEU8MH03V0sbHCYJzhIiVcYYakHATtXIUaCUUxI2YNFpLkCF+iIRlYk6OUyFBX1Rjvrd2JvSQT9uPKq3brJzRKpRynkY0stUuXlZv3sUGhko+hpjwvFOF48qOkYJ7KvLI1XkwFwYqNrYGwoFarh0dIIKxsA9cgJ9c4S1PEYw37Rt81rm/MPDyrwTMXdmuw68JeDfZceFCDBy7s1GDHhVFyM8VRom8W6XhGx4u0P6MLpUbJ2Ywu1Ep5XF40O9TBdjgJi6UOzIX2AwMjOmxqf7syNp2TKM/FrWaR6onrxMgiKqueu6vzEYJIGhek1ppDA/ftb2Fip2qF6Ol9r1JVyBVyfPCdqJn8Msex1Q/TYsvWcF7qb1rnHIpRVpW15UFecxfyde7Px/8zX6yuzCAI7zoVV+Im9om9Ql7VZs8PvPvajO1o7sJLxx1DJCfZGUkUZPYhUX4ABR2OlAdF6ToHyE2Oa+VFkf5cTtsOucwwuD0cuqNUVsburRDZUi0Dd50Yll0TcYVYNc/Z0L8aY9+nwH2NFo3edit439r5tuPv7U9fqlXwGrwBTRCAD2APfAFHoAsw+Al+gd/gT2OzcdjoNfqT0OWl6ZlXYG410D8CagP8</latexit>O(1/pN)
Figure 8: Difference between game values with initial distributions µ0= [1,0]andν0= [0.6,0.4].Published as a conference paper at AAAI 2024
7 Conclusion
In this work, we introduced a discrete zero-sum mean-field team game to model the behaviors of competing large-
population teams. We developed a dynamic programming approach that approximately solves this large-population
game at its infinite-population limit where only identical team strategies are considered. Our analysis demonstrated
that the identical strategies constructed are ϵ-optimal within the general class of non-identical team strategies when
deployed in the original finite-population game. The derived performance guarantees are verified through numerical
examples. Future work will investigate the LQG setup of this problem and explore machine-learning techniques to
address more complex zero-sum mean-field team problems. Additionally, we aim to generalize our results to the
infinite-horizon discounted case and problems with heterogeneous sub-populations.
Acknowledgments:
The authors acknowledge fruitful discussions with Dr. Ali Pakniyat.Published as a conference paper at AAAI 2024
Appendix A Special Cases and Examples
A.1 Pairwise State-Coupled Mean-Field Team Games
In Section 2, we introduced the pairwise state-coupled dynamics and the averaged state-dependent reward as a special
case for the proposed mean-field team game. Specifically, the pairwise state-coupled transition kernels are defined as
follows
ft 
xN1
i,t+1xN1
i,t, uN1
i,t,xN1
−i,t,yN2
t) =PN1
k=1f1,t(xN1
i,t+1xN1
i,t, uN1
i,t, xN1
k,t)
N+PN2
k=1f2,t(xN1
i,t+1xN1
i,t, uN1
i,t, yN2
k,t)
N
gt 
yN2
j,t+1yN2
j,t, vN2
j,t,xN1
t,yN2
−j,t) =PN1
k=1g1,t(yN2
j,t+1yN2
j,t, vN2
j,t, xN1
k,t)
N+PN2
k=1g2,t(yN2
j,t+1yN2
j,t, vN2
j,t, yN2
k,t)
N.(12)
Through some algebraic manipulations, the Blue transition kernel can be re-written in the weakly-coupled form in (4):
ft 
xN1
i,t+1xN1
i,t, uN1
i,t,xN1
−i,t,yN2
t)
=N1
NPN1
k=1f1,t(xN1
i,t+1xN1
i,t, uN1
i,t, xN1
k,t)
N1+N2
NPN2
k=1f2,t(xN1
i,t+1xN1
i,t, uN1
i,t, yN2
k,t)
N2
=ρPN1
k=1P
x∈X1x 
xN1
k,t
f1,t(xN1
i,t+1xN1
i,t, uN1
i,t, xN1
k,t)
N1+ (1−ρ)PN2
k=1P
y∈Y1y 
yN2
k,t
f2,t(xN1
i,t+1xN1
i,t, uN1
i,t, yN2
k,t)
N2
=ρP
x∈XPN1
k=11x 
xN1
k,t
f1,t(xN1
i,t+1xN1
i,t, uN1
i,t, x)
N1+ (1−ρ)P
y∈YPN2
k=11y 
yN2
k,t
f2,t(xN1
i,t+1xN1
i,t, uN1
i,t, y)
N2
=ρX
x∈XµN1
t(x)f1,t(xN1
i,t+1xN1
i,t, uN1
i,t, x) + (1 −ρ)X
y∈YνN2
t(y)f2,t(xN1
i,t+1xN1
i,t, uN1
i,t, y)
≜fρ
t 
xN1
i,t+1xN1
i,t, uN1
i,t, µN1
t, νN2
t).
The Red transition kernel can be similarly transformed into a weakly-coupled form.
The averaged state-dependent reward is defined as
rt(xN1
t,yN2
t) =1
NN1X
k=1r1,t(xN1
k,t)−1
NN2X
k=1r2,t(yN2
k,t). (13)
One can also transform the above reward structure to a weakly-coupled form as in (6)
rt(xN1
t,yN2
t) =N1
NPN1
k=1r1,t(xN1
k,t)
N1+N2
NPN2
k=1r2,t(yN2
k,t)
N2
=N1
NPN1
k=1P
x∈X1x 
xN1
k,t
r1,t(xN1
k,t)
N1+N2
NPN2
k=1P
y∈Y1y 
yN2
k,t
r2,t(yN2
k,t)
N2
=ρX
x∈XµN1
t(x)r1,t(x)−(1−ρ)X
y∈YνN2
t(y)r2,t(y)
≜rρ
t(µN1
t, νN2
t).
Proposition 1. The transition kernels in (12) satisfy
X
x′∈X|fρ
t(x′|x, u, µ, ν )−fρ
t(x′|x, u, µ′, ν′)| ≤2
dTV 
µ, µ′
+ dTV 
ν, ν′
∀x∈ X, u∈ U,
X
y′∈Y|gρ
t(y′|y, v, µ, ν )−gρ
t(y′|y, v, µ′, ν′)| ≤2
dTV 
µ, µ′
+ dTV 
ν, ν′
∀y∈ Y, v∈ V.
Proof. Due to symmetry, we only show the result for the Blue transition kernel. From the definition, we have, for all
x∈ X andu∈ U, that
X
x′∈Xfρ
t(x′|x, u, µ, ν )−fρ
t(x′|x, u, µ′, ν′)=X
x′∈XρX
z∈Xµ(z)f1,t(x′|x, u, z ) + (1 −ρ)X
y∈Yν(y)f2,t(x′|x, u, y )Published as a conference paper at AAAI 2024
−ρX
z∈Xµ(z)f1,t(x′|x, u, z )−(1−ρ)X
y∈Yν(y)f2,t(x′|x, u, y )
≤ρX
x′∈XX
z∈Xf1,t(x′|x, u, z )|µ(z)−µ′(z)|+ (1−ρ)X
x′∈XX
y∈Yf2,t(x′|x, u, y )|ν(y)−ν′(y)|
=ρX
z∈X|µ(z)−µ′(z)|+ (1−ρ)X
y∈Y|ν(y)−ν′(y)|= 2ρdTV 
µ, µ′
+ 2(1−ρ) dTV 
ν, ν′
≤2
dTV 
µ, µ′
+ dTV 
ν, ν′
.
Proposition 2. For all µ, µ′∈ P(X),ν, ν′∈ P(Y)andt∈ {0, . . . , T }, the reward structure in (13) satisfies
|rρ
t(µ, ν)−rρ
t(µ′, ν′)| ≤(2 max {r1,max, r2,max})
dTV 
µ, µ′
+ dTV 
ν, ν′
,
where r1,max= max x,t|r1,t(x)|andr2,max= max y,t|r2,t(y)|.
Proof. Note that
|rρ
t(µ, ν)−rρ
t(µ′, ν′)|=X
xr1,t(x)(µ(x)−µ′(x))−X
yr2,t(y)(ν(y)−ν′(y))
≤X
xr1,t(x)(µ(x)−µ′(x))+X
yr2,t(y)(ν(y)−ν′(y))
≤X
x|r1,t(x)||µ(x)−µ′(x)|+X
y|r2,t(y)||ν(y)−ν′(y)|
≤r1,maxX
x|µ(x)−µ′(x)|+r2,maxX
y|ν(y)−ν′(y)|
≤max{r1,max, r2,max} 
2dTV 
µ, µ′
+ 2d TV 
ν, ν′
.
A.2 Discontinuous Reward Example
Through this example, we demonstrate the necessity of Assumption 2, i.e., the reward function needs to be continuous
with respect to the mean-fields as in (7). Consider a MFTG with terminal time T= 1over the following deterministic
system.
The Blue agent’s transition kernel are given by
fρ
0(x1|x1, u1, µ, ν) = 1 , fρ
0(x2|x1, u1, µ, ν) = 0 ,
fρ
0(x1|x1, u2, µ, ν) = 0 , fρ
0(x2|x1, u1, µ, ν) = 1 ,
fρ
0(x1|x2, u3, µ, ν) = 0 , fρ
0(x2|x2, u3, µ, ν) = 1 ,
fρ
0(x1|x2, u4, µ, ν) = 1 , fρ
0(x2|x2, u4, µ, ν) = 0 ,
which holds for all distributions µ∈ P(X)andν∈ P(Y).
<latexit sha1_base64="5n37TFEAnnQNgM5uFQ5SMJVhjr0=">AAACQ3icbZDLSsNAFIZn6r3eWl26CRZBXJREiros6MKlolWhqWUymbRD5xJmJtoQ8ghu9Yl8CJ/BnbgVnNSAWj0w8PN/53DO/EHMqDau+wIrM7Nz8wuLS9XlldW19Vp940rLRGHSwZJJdRMgTRgVpGOoYeQmVgTxgJHrYHRc8Os7ojSV4tKkMelxNBA0ohgZa10kt16/1nCb7qScv8IrRQOUddavwz0/lDjhRBjMkNZdz41NL0PKUMxIXvUTTWKER2hAulYKxInuZZNbc2fHOqETSWWfMM7E/TmRIa51ygPbyZEZ6mlWmP+xbmKio15GRZwYIvDXoihhjpFO8XEnpIpgw1IrEFbU3urgIVIIGxtP1RfkHkvOkQgzf5xnfrEgiLJxnv9m6TdLp1lkWSBZWNwvWRZZbtP1prP8K672m95Bs3XearRPypwXwRbYBrvAA4egDU7BGegADAbgATyCJ/gMX+EbfP9qrcByZhP8KvjxCWU+sxQ=</latexit>u1<latexit sha1_base64="3vYaVMjMFbx1KM/inyq+RCf1J64=">AAACQ3icbZDLSsNAFIYnXmu9tbp0EyyCuCiJFHVZ0IXLivYCTS2TyaQdnEuYmWhDyCO41SfyIXwGd+JWcNIWtK0HBn7+7xzOmd+PKFHacd6tpeWV1bX1wkZxc2t7Z7dU3mspEUuEm0hQITs+VJgSjpuaaIo7kcSQ+RS3/YfLnLcfsVRE8DudRLjH4ICTkCCojXUb35/2SxWn6ozLXhTuVFTAtBr9snXiBQLFDHONKFSq6zqR7qVQaoIozoperHAE0QMc4K6RHDKseun41sw+Mk5gh0Kax7U9dv9OpJAplTDfdDKoh2qe5eZ/rBvr8KKXEh7FGnM0WRTG1NbCzj9uB0RipGliBESSmFttNIQSIm3iKXocPyHBGORB6o2y1MsX+GE6yrJZlvyyZJ6FhvmCBvn9gqah4SZddz7LRdE6rbpn1dpNrVK/muZcAAfgEBwDF5yDOrgGDdAECAzAM3gBr9ab9WF9Wl+T1iVrOrMPZsr6/gFnG7MV</latexit>u2<latexit sha1_base64="oAJmCgEKVwepB42rtKgdnwqntVY=">AAACQ3icbZDLSsNAFIYnXmu9tbp0M1gEcVESLepS0IVLRWsLTZXJZFIH5xJmJmoIeQS3+kQ+hM/gTtwKTtqA2npg4Of/zuGc+YOYUW1c982Zmp6ZnZuvLFQXl5ZXVmv1tSstE4VJG0smVTdAmjAqSNtQw0g3VgTxgJFOcHdc8M49UZpKcWnSmPQ5GggaUYyMtS6S672bWsNtusOCk8IrRQOUdXZTd3b8UOKEE2EwQ1r3PDc2/QwpQzEjedVPNIkRvkMD0rNSIE50PxvemsMt64Qwkso+YeDQ/T2RIa51ygPbyZG51eOsMP9jvcREh/2MijgxRODRoihh0EhYfByGVBFsWGoFworaWyG+RQphY+Op+oI8YMk5EmHmP+aZXywIouwxz/+y9Iel4yyyLJAsLO6XLIsst+l641lOiqvdprffbJ23GkcnZc4VsAE2wTbwwAE4AqfgDLQBBgPwBJ7Bi/PqvDsfzueodcopZ9bBn3K+vgFo+LMW</latexit>u3
<latexit sha1_base64="GtAZ/m1JTeKMLT3qkJJnfF9/ndQ=">AAACQ3icbZDLSsNAFIZnvNZ6a3XpJlgEcVESKeqyoAuXFW0VmiqTyaQOziXMTLQh5BHc6hP5ED6DO3ErOGkDavXAwM//ncM58wcxo9q47iucmZ2bX1isLFWXV1bX1mv1jZ6WicKkiyWT6ipAmjAqSNdQw8hVrAjiASOXwd1xwS/vidJUiguTxmTA0VDQiGJkrHWeXLduag236Y7L+Su8UjRAWZ2bOtzzQ4kTToTBDGnd99zYDDKkDMWM5FU/0SRG+A4NSd9KgTjRg2x8a+7sWCd0IqnsE8YZuz8nMsS1TnlgOzkyt3qaFeZ/rJ+Y6GiQUREnhgg8WRQlzDHSKT7uhFQRbFhqBcKK2lsdfIsUwsbGU/UFecCScyTCzB/lmV8sCKJslOe/WfrN0mkWWRZIFhb3S5ZFltt0veks/4reftM7aLbOWo32SZlzBWyBbbALPHAI2uAUdEAXYDAEj+AJPMMX+Abf4cekdQaWM5vgV8HPL2rVsxc=</latexit>u4
<latexit sha1_base64="cOtKgBFGssTqU8Cfd85ZIa1BW84=">AAACQ3icbZDLSsNAFIZnvNZ6a3XpJlgEcVESKeqyoAuXFW0VmiqTyaQOziXMTLQh5BHc6hP5ED6DO3ErOGkDavXAwM//ncM58wcxo9q47iucmZ2bX1isLFWXV1bX1mv1jZ6WicKkiyWT6ipAmjAqSNdQw8hVrAjiASOXwd1xwS/vidJUiguTxmTA0VDQiGJkrHU+uvZuag236Y7L+Su8UjRAWZ2bOtzzQ4kTToTBDGnd99zYDDKkDMWM5FU/0SRG+A4NSd9KgTjRg2x8a+7sWCd0IqnsE8YZuz8nMsS1TnlgOzkyt3qaFeZ/rJ+Y6GiQUREnhgg8WRQlzDHSKT7uhFQRbFhqBcKK2lsdfIsUwsbGU/UFecCScyTCzB/lmV8sCKJslOe/WfrN0mkWWRZIFhb3S5ZFltt0veks/4reftM7aLbOWo32SZlzBWyBbbALPHAI2uAUdEAXYDAEj+AJPMMX+Abf4cekdQaWM5vgV8HPL2rbsxc=</latexit>x1<latexit sha1_base64="X+AA2MvYIxfYYOpXVwzaZ7upi6k=">AAACQ3icbZDLSsNAFIZn6r3eWl26CRZBXJREiros6MKlolWhqWUymbRD5xJmJtoQ8ghu9Yl8CJ/BnbgVnNSAWj0w8PN/53DO/EHMqDau+wIrM7Nz8wuLS9XlldW19Vp940rLRGHSwZJJdRMgTRgVpGOoYeQmVgTxgJHrYHRc8Os7ojSV4tKkMelxNBA0ohgZa12kt16/1nCb7qScv8IrRQOUddavwz0/lDjhRBjMkNZdz41NL0PKUMxIXvUTTWKER2hAulYKxInuZZNbc2fHOqETSWWfMM7E/TmRIa51ygPbyZEZ6mlWmP+xbmKio15GRZwYIvDXoihhjpFO8XEnpIpgw1IrEFbU3urgIVIIGxtP1RfkHkvOkQgzf5xnfrEgiLJxnv9m6TdLp1lkWSBZWNwvWRZZbtP1prP8K672m95Bs3XearRPypwXwRbYBrvAA4egDU7BGegADAbgATyCJ/gMX+EbfP9qrcByZhP8KvjxCWy6sxg=</latexit>y1<latexit sha1_base64="Bd/ESWke6R23lQ4r0P3TqUNsgfc=">AAACQ3icbZDLSsNAFIYn3q23VpduBosgLkoiRV0WdOFS0V6gqWUymdTBuYSZiTaEPIJbfSIfwmdwJ24FJzWgVg8M/PzfOZwzfxAzqo3rvjgzs3PzC4tLy5WV1bX1jWpts6NlojBpY8mk6gVIE0YFaRtqGOnFiiAeMNINbk8K3r0jSlMprkwakwFHI0EjipGx1uXdtTes1t2GOyn4V3ilqIOyzoc1Z98PJU44EQYzpHXfc2MzyJAyFDOSV/xEkxjhWzQifSsF4kQPssmtOdy1TggjqewTBk7cnxMZ4lqnPLCdHJkbPc0K8z/WT0x0PMioiBNDBP5aFCUMGgmLj8OQKoINS61AWFF7K8Q3SCFsbDwVX5B7LDlHIsz8cZ75xYIgysZ5/pul3yydZpFlgWRhcb9kWWS5TdebzvKv6Bw0vMNG86JZb52WOS+BbbAD9oAHjkALnIFz0AYYjMADeARPzrPz6rw571+tM045swV+lfPxCWcdsxU=</latexit>v1
<latexit sha1_base64="OfOjFM0Lc4HUSZ8ARdd1l/zbJC0=">AAACQ3icbZDLSsNAFIYnXmu9VpdugkUQFyUpoi4LunBZ0bZCU8tkMqlD5xJmJtoQ8ghu9Yl8CJ/BnbgVnLQBbeuBgZ//O4dz5vcjSpR2nHdrYXFpeWW1tFZe39jc2t6p7LaViCXCLSSokHc+VJgSjluaaIrvIokh8ynu+MOLnHcesVRE8FudRLjH4ICTkCCojXUzuq/3d6pOzRmXPS/cQlRBUc1+xTr2AoFihrlGFCrVdZ1I91IoNUEUZ2UvVjiCaAgHuGskhwyrXjq+NbMPjRPYoZDmcW2P3b8TKWRKJcw3nQzqBzXLcvM/1o11eN5LCY9ijTmaLApjamth5x+3AyIx0jQxAiJJzK02eoASIm3iKXscPyHBGORB6o2y1MsX+GE6yrJplvyyZJaFhvmCBvn9gqah4SZddzbLedGu19zT2sn1SbVxWeRcAvvgABwBF5yBBrgCTdACCAzAM3gBr9ab9WF9Wl+T1gWrmNkDU2V9/wBsuLMY</latexit>x2
Figure 9: A deterministic system with two Blue states (left) and one Red state (right).Published as a conference paper at AAAI 2024
<latexit sha1_base64="5n37TFEAnnQNgM5uFQ5SMJVhjr0=">AAACQ3icbZDLSsNAFIZn6r3eWl26CRZBXJREiros6MKlolWhqWUymbRD5xJmJtoQ8ghu9Yl8CJ/BnbgVnNSAWj0w8PN/53DO/EHMqDau+wIrM7Nz8wuLS9XlldW19Vp940rLRGHSwZJJdRMgTRgVpGOoYeQmVgTxgJHrYHRc8Os7ojSV4tKkMelxNBA0ohgZa10kt16/1nCb7qScv8IrRQOUddavwz0/lDjhRBjMkNZdz41NL0PKUMxIXvUTTWKER2hAulYKxInuZZNbc2fHOqETSWWfMM7E/TmRIa51ygPbyZEZ6mlWmP+xbmKio15GRZwYIvDXoihhjpFO8XEnpIpgw1IrEFbU3urgIVIIGxtP1RfkHkvOkQgzf5xnfrEgiLJxnv9m6TdLp1lkWSBZWNwvWRZZbtP1prP8K672m95Bs3XearRPypwXwRbYBrvAA4egDU7BGegADAbgATyCJ/gMX+EbfP9qrcByZhP8KvjxCWU+sxQ=</latexit>u1<latexit sha1_base64="3vYaVMjMFbx1KM/inyq+RCf1J64=">AAACQ3icbZDLSsNAFIYnXmu9tbp0EyyCuCiJFHVZ0IXLivYCTS2TyaQdnEuYmWhDyCO41SfyIXwGd+JWcNIWtK0HBn7+7xzOmd+PKFHacd6tpeWV1bX1wkZxc2t7Z7dU3mspEUuEm0hQITs+VJgSjpuaaIo7kcSQ+RS3/YfLnLcfsVRE8DudRLjH4ICTkCCojXUb35/2SxWn6ozLXhTuVFTAtBr9snXiBQLFDHONKFSq6zqR7qVQaoIozoperHAE0QMc4K6RHDKseun41sw+Mk5gh0Kax7U9dv9OpJAplTDfdDKoh2qe5eZ/rBvr8KKXEh7FGnM0WRTG1NbCzj9uB0RipGliBESSmFttNIQSIm3iKXocPyHBGORB6o2y1MsX+GE6yrJZlvyyZJ6FhvmCBvn9gqah4SZddz7LRdE6rbpn1dpNrVK/muZcAAfgEBwDF5yDOrgGDdAECAzAM3gBr9ab9WF9Wl+T1iVrOrMPZsr6/gFnG7MV</latexit>u2
<latexit sha1_base64="cOtKgBFGssTqU8Cfd85ZIa1BW84=">AAACQ3icbZDLSsNAFIZnvNZ6a3XpJlgEcVESKeqyoAuXFW0VmiqTyaQOziXMTLQh5BHc6hP5ED6DO3ErOGkDavXAwM//ncM58wcxo9q47iucmZ2bX1isLFWXV1bX1mv1jZ6WicKkiyWT6ipAmjAqSNdQw8hVrAjiASOXwd1xwS/vidJUiguTxmTA0VDQiGJkrHU+uvZuag236Y7L+Su8UjRAWZ2bOtzzQ4kTToTBDGnd99zYDDKkDMWM5FU/0SRG+A4NSd9KgTjRg2x8a+7sWCd0IqnsE8YZuz8nMsS1TnlgOzkyt3qaFeZ/rJ+Y6GiQUREnhgg8WRQlzDHSKT7uhFQRbFhqBcKK2lsdfIsUwsbGU/UFecCScyTCzB/lmV8sCKJslOe/WfrN0mkWWRZIFhb3S5ZFltt0veks/4reftM7aLbOWo32SZlzBWyBbbALPHAI2uAUdEAXYDAEj+AJPMMX+Abf4cekdQaWM5vgV8HPL2rbsxc=</latexit>x1<latexit sha1_base64="OfOjFM0Lc4HUSZ8ARdd1l/zbJC0=">AAACQ3icbZDLSsNAFIYnXmu9VpdugkUQFyUpoi4LunBZ0bZCU8tkMqlD5xJmJtoQ8ghu9Yl8CJ/BnbgVnLQBbeuBgZ//O4dz5vcjSpR2nHdrYXFpeWW1tFZe39jc2t6p7LaViCXCLSSokHc+VJgSjluaaIrvIokh8ynu+MOLnHcesVRE8FudRLjH4ICTkCCojXUzuq/3d6pOzRmXPS/cQlRBUc1+xTr2AoFihrlGFCrVdZ1I91IoNUEUZ2UvVjiCaAgHuGskhwyrXjq+NbMPjRPYoZDmcW2P3b8TKWRKJcw3nQzqBzXLcvM/1o11eN5LCY9ijTmaLApjamth5x+3AyIx0jQxAiJJzK02eoASIm3iKXscPyHBGORB6o2y1MsX+GE6yrJplvyyZJaFhvmCBvn9gqah4SZddzbLedGu19zT2sn1SbVxWeRcAvvgABwBF5yBBrgCTdACCAzAM3gBr9ab9WF9Wl+T1gWrmNkDU2V9/wBsuLMY</latexit>x2<latexit sha1_base64="5n37TFEAnnQNgM5uFQ5SMJVhjr0=">AAACQ3icbZDLSsNAFIZn6r3eWl26CRZBXJREiros6MKlolWhqWUymbRD5xJmJtoQ8ghu9Yl8CJ/BnbgVnNSAWj0w8PN/53DO/EHMqDau+wIrM7Nz8wuLS9XlldW19Vp940rLRGHSwZJJdRMgTRgVpGOoYeQmVgTxgJHrYHRc8Os7ojSV4tKkMelxNBA0ohgZa10kt16/1nCb7qScv8IrRQOUddavwz0/lDjhRBjMkNZdz41NL0PKUMxIXvUTTWKER2hAulYKxInuZZNbc2fHOqETSWWfMM7E/TmRIa51ygPbyZEZ6mlWmP+xbmKio15GRZwYIvDXoihhjpFO8XEnpIpgw1IrEFbU3urgIVIIGxtP1RfkHkvOkQgzf5xnfrEgiLJxnv9m6TdLp1lkWSBZWNwvWRZZbtP1prP8K672m95Bs3XearRPypwXwRbYBrvAA4egDU7BGegADAbgATyCJ/gMX+EbfP9qrcByZhP8KvjxCWU+sxQ=</latexit>u1
<latexit sha1_base64="3vYaVMjMFbx1KM/inyq+RCf1J64=">AAACQ3icbZDLSsNAFIYnXmu9tbp0EyyCuCiJFHVZ0IXLivYCTS2TyaQdnEuYmWhDyCO41SfyIXwGd+JWcNIWtK0HBn7+7xzOmd+PKFHacd6tpeWV1bX1wkZxc2t7Z7dU3mspEUuEm0hQITs+VJgSjpuaaIo7kcSQ+RS3/YfLnLcfsVRE8DudRLjH4ICTkCCojXUb35/2SxWn6ozLXhTuVFTAtBr9snXiBQLFDHONKFSq6zqR7qVQaoIozoperHAE0QMc4K6RHDKseun41sw+Mk5gh0Kax7U9dv9OpJAplTDfdDKoh2qe5eZ/rBvr8KKXEh7FGnM0WRTG1NbCzj9uB0RipGliBESSmFttNIQSIm3iKXocPyHBGORB6o2y1MsX+GE6yrJZlvyyZJ6FhvmCBvn9gqah4SZddz7LRdE6rbpn1dpNrVK/muZcAAfgEBwDF5yDOrgGDdAECAzAM3gBr9ab9WF9Wl+T1iVrOrMPZsr6/gFnG7MV</latexit>u2
Figure 10: A counter-example showing ED cannot serve as an information state.
The Red agent’s transition kernel is given by
gρ
0(y1|y1, v1, µ, ν) = 1 ,∀µ∈ P(X), ν∈ P(Y).
Given µ∈ P(X)andν∈ P(Y), the discontinuous reward is given by
rρ
0(µ, ν) = 0 ,
rρ
1(µ, ν) =1
µ= (1/√
3,1−1/√
3)
.
As the dynamics are decoupled and the reward does not depend on the Red distribution, the game is essentially a
single-team problem. We choose this degenerate example for its simplicity. Since the reward at time t= 0is always
zero, the objective of the Blue team is to achieve the desired distribution (1/√
3,1−1/√
3)at time t= 1 to receive
one unit of reward. Since t= 0 is the only time step that the Blue agents select action, a Blue coordination strategy
only consists of a Blue coordination policy at time t= 0. In this case, an optimal Blue coordination strategy can be
α∗
0(µ, ν) =(
π1
0, ifµ0(x1)<1√
3
π2
0, ifµ0(x1)≥1√
3,
where,
π1
0(u1|x1) = 1 , π1
0(u2|x1) = 0 ,
π1
0(u3|x2) =1−1/√
3
µ0(x2), π1
0(u4|x2) =1/√
3−µ0(x1)
µ0(x2),
π2
0(u1|x1) =1/√
3
µ0(x1), π2
0(u2|x1) =1−1/√
3−µ0(x2)
µ0(x1),
π2
0(u3|x2) = 1 , π2
0(u4|x2) = 0 .
The proposed local policy is well-defined, since for the case where µ0(x1)<1/√
3, it is implied that µ0(x2)>0, and
similarly, for the case where µ0(x1)>1/√
3, we automatically have µ0(x1)>0.
One can verify that the above coordination strategy will achieve the mean-field µ1= (√
3,1−1/√
3)from all
µ0∈ P(X)under the mean-field dynamics (19). Consequently, we have
Jρ∗
cor(µ0, ν0) = 1 ,∀µ0∈ P(X), ν0∈ P(Y).
However, we know that for all N1, the ED must be rational, and thus MN1̸= (√
3,1−1/√
3)for all N1almost
surely, which leads to
JN∗(xN1
0,yN2
0) = 0 ,∀xN1
0∈ XN1,yN2
0∈ YN2,
which implies that the performance of the coordination strategy achieved at the infinite-population limit fails to trans-
late back to the finite-population game, and hence Theorem 4 no longer holds.
One can construct similar examples to illustrate the necessity of the transition kernels being continuous with respect
to the mean-fields.
A.3 Counter-Example for Information State
For simplicity, we use a single-team example to illustrate why the ED cannot be an information state when different
agents are allowed to apply different strategies, even under the weakly-coupled dynamics (4) and rewards (6). Consider
the following system with state spaces X={x1, x2}andY=∅, and action spaces U={u1, u2}andV=∅.Published as a conference paper at AAAI 2024
The transition kernel is time-invariant and is given by
ft(x1|x1, u1, µ) = 1 .0, f t(x2|x1, u2, µ) = 1 .0,∀µ∈ P(X), t∈ {0,1}.
ft(x2|x2, u1, µ) = 1 .0, f t(x1|x2, u2, µ) = 1 .0,
In words, each agent’s transition is deterministic and decoupled from the other agents. At each state, the agent can
select action u1to stay at its current state or use action u2to move to the other state.
We consider a one-stage scenario with the reward functions
r0(µ) = 0 ,∀µ∈ P(X),
r1(µ) =µ(x1).
To illustrate that the ED cannot serve as an information state, consider a two-agent team and the following non-identical
team strategy:
ϕ2
1,0(u2|x1, µ) = 1 , ϕ2
1,0(u1|x2, µ) = 1 ,∀µ∈ P(X),
ϕ2
2,0(u1|x1, µ) = 1 , ϕ2
2,0(u1|x2, µ) = 1 ,∀µ∈ P(X).
In words, agent 1 selects u2on state x1and selects u1on state x2, while agent 2 selects u1on both states x1andx2.
We now consider two initial configurations with the same ED.
•X2
1,0=x1andX2
2,0=x2, i.e., agent 1 is initially at x1and agent 2 at x2. The ED is thus M2
0= [0.5,0.5].
If the two agents follow the above non-identical team strategy, then agent 1 uses action u2and transits to x2,
while agent 2 chooses action u1and stays on x2, leading to M2
1= [0,1]. The resultant reward value is then
0.
•X2
1,0=x2andX2
2,0=x1, i.e., agent 1 is initially at x2and agent 2 at x1. The ED is again M2
0= [0.5,0.5].
If the two agents follow the above non-identical team strategy, then both agents select action u1and stay at
their current states, leading to M2
1= [0.5,0.5]and a reward value of 0.5.
From this example, we have shown that the values can be different under the same team strategy given different initial
conditions that correspond to the same ED since the ED does not differentiate the ordering of the agents. Clearly, the
ED alone is not enough to characterize the future evolution of the game, nor the value function. Consequently, the ED
is not an information state and we need the joint state information in the finite-population game to properly construct
the value functions when different agents apply different strategies.
Appendix B Proof of the Mean-Field Approximation Results
B.1 Modified ℓ2Weak Law of Large Numbers
The following lemma is a modified version of the ℓ2weak law of large numbers [Chung, 2001] adapted to accommo-
date the notion of EDs used in this work. This lemma will be used extensively in the later proofs.
Lemma 7. LetX1, . . . , X NbeNindependent random variables (vectors) taking values in a finite set X. Suppose Xi
is distributed according to pi∈ P(X), such that P(Xi=x) =pi(x). Define the ED as
MN(x) =1
NNX
i=11x 
Xi
.
and let µ(x) =1
NPN
i=1pi(x). Then, the following inequality holds
E
dTV 
MN, µ
≤1
2r
|X|
N. (69)Published as a conference paper at AAAI 2024
Proof. Consider the random variables Xx,i=1x 
Xi
, which are independent across iwith mean pi(x). Since Xx,i
only takes a value of 0 and 1, we have E[X2
x,i] =E[Xx,i] =pi(x), and Var(Xx,i) =pi(x)−pi(x)2. It follows that
E
∥MN−µ∥2
2
=E"X
x∈XMN(x)−µ(x)2#
=EhX
x∈X1
NNX
i=1(Xx,i−pi(x))2i
=1
N2X
x∈XVarNX
i=1Xx,i(i)=1
N2X
x∈XNX
i=1Var(Xx,i)
=1
N2X
x∈XNX
i=1(pi(x)−pi(x)2)≤1
N2NX
i=1X
x∈Xpi(x)
≤1
N,
where (i)leverages the fact that Xx,iare independent across i. By Jensen’s inequality, we have E
∥MN−µ∥2
≤
1√
N. Furthermore, due to equivalence of norms in the finite dimensional space X, we haveMN−µ
1≤p
|X|MN−µ
2almost surely, where |X|is the cardinality of the set X. It then follows that
E[dTV 
MN, µ
] =1
2EMN−µ
1
≤1
2r
|X|
N.
Corollary 1. LetX1, . . . , X NbeNindependent and identically distributed random variables (vectors) taking values
in a finite set X. Suppose Xiis distributed according to p∈ P(X), such that P(xi=x) =p(x). Define the ED as
MN(x) =1
NNX
i=11x 
Xi
.
Then,
E
dTV 
MN, p
=1
2r
|X|
N. (70)
B.2 Proof of Lemma 1
The following lemma is used to support the proof of Lemma 1.
Lemma 8. For all a1, . . . , a n∈R≥0, the following inequality holds:
nX
i=1ai≤√n
vuutnX
i=1a2
i
.
Proof. Cauchy-Schwarz inequality directly implies
nX
i=1ai=nX
i=1|ai| ·1≤vuutnX
i=1a2
ivuutnX
i=112=√nvuutnX
i=1a2
i.
Now, we restate Lemma 1 and present its proof.
Lemma 1. LetXN1
t,YN2
t,MN1
tandNN2
tbe the joint states and the corresponding EDs of a finite-population game.
Denote the next EDs induced by an identical policy pair (ϕt, ψt)∈Φt×Ψtas(MN1
t+1,NN2
t+1). Then, the following
bounds hold:
Eϕth
dTV 
MN1
t+1,Mt+1XN1
t,YN2
ti
≤|X|
2r
1
N1,Published as a conference paper at AAAI 2024
Eψth
dTV 
NN2
t+1,Nt+1XN1
t,YN2
ti
≤|Y|
2r
1
N2,
where Mt+1=MN1
tFρ
t(MN1
t,NN2
t, ϕt)andNt+1=NN2
tGρ
t(MN1
t,NN2
t, ψt)according to (15).
Proof. Due to symmetry, we only prove the result for the Blue team. Let Nx
1,t=N1MN1
t(x)be the number of
Blue agents that are at state xat time tin the finite-population system. With a slight abuse of notation, we use [Nx
1,t]
to denote the set of the Blue agents currently at state x. We first focus on the sub-population of Blue agents that
are at state x. Since all Blue agents in [Nx
1,t]apply ϕtand randomize independently, the next states for these Nx
1,t
agents are independent and identically distributed conditioned on the joint states XN1
tandYN2
t. For an agent iin this
sub-population [Nx
1,t]the conditional distribution of its next state XN1
i,t+1is given by
Ph
XN1
i,t+1=x′XN1
i,t,XN1
t,YN2
ti
=X
ufρ
t(x′|x, u,MN1
t,NN1
t)ϕt(u|x,MN1
t,NN1
t)≜Mx
t+1(x′), (71)
where MN1
t= Empµ(XN1
t)andNN2
t= Empν(YN2
t). Define the ED of these Nx
1,tBlue agents at time t+ 1as
MNx
1,t
t+1(x′) =1
Nx
1,tX
i∈[Nx
1,t]1x′ 
XN1
i,t+1
.
Then, we have the following inequality due to Corollary 1:
Eϕth
dTV 
MNx
1,t
t+1,Mx
t+1XN1
t,YN2
ti
≤1
2s
|X|
Nx
1,t. (72)
Note that we can compute the ED of the whole Blue team based on the ED of sub-populations of Blue agents on
different states via
MN1
t+1(x′) =1
N1N1X
i=01x′ 
XN1
i,t+1
=1
N1X
x∈XX
i∈[Nx
1,t]1x′ 
XN1
i,t+1
=X
x∈XNx
1,t
N1MNx
1,t
t+1(x′). (73)
Similarly, we can relate the propagated mean-field of the whole team with the ones of each sub-population via
Mρ
t+1(x′) =X
x∈X"X
u∈Ufρ
t(x′|x, u,MN1
t,NN2
t)ϕt(u|x,MN1
t,NN2
t)#
MN1
t(x)
=X
x∈XMx
t+1(x′)MN1
t(x) =X
x∈XNx
1,t
N1Mx
t+1(x′).(74)
Combining (72), (73), and (74), we have
Eϕth
dTV 
MN1
t+1,Mρ
t+1XN1
t,YN2
ti
=Eϕt"
1
2X
x′MN1
t+1(x′)− Mρ
t+1(x′)|XN1
t,YN2
t#
=Eϕt"
1
2X
x′X
xNx
1,t
N1MNx
1,t
t+1(x′)−X
xNx
1,t
N1Mx
t+1(x′)XN1
t,YN2
t#
≤X
xNx
1,t
N1Eϕt"
1
2X
x′MNx
1,t
t+1(x′)− Mx
t+1(x′)XN1
t,YN2
t#
=X
xNx
1,t
N1Eϕth
dTV 
MNx
1,t
t+1,Mx
t+1XN1
t,YN2
ti
≤X
xNx
1,t
2N1s
|X|
Nx
1,t≤|X|
2r
1
N1,Published as a conference paper at AAAI 2024
where the last inequality is a result of Lemma 8.10
B.3 Proof of Lemma 3
Lemma 3. LetXN1
t,YN2
t,MN1
tandNN2
tbe the joint states and the corresponding EDs of a finite-population game.
Given any Blue team policy ϕN1
t∈ΦN1
t(potentially non-identical), define the following local policy
πapprx ,t(u|x) =

PN
i=11x 
XN1
i,t
ϕi,t(u|x,MN1
t,NN2
t)
N1MN1
t(x)ifMN1
t(x)>0,
1/|U| ifMN1
t(x) = 0 .(35)
Further, define the next mean-field induced by πapprx ,tas
Mapprx ,t+1=MN1
tFρ
t(MN1
t,NN2
t, πapprx ,t). (36)
Then, the expected distance between the next Blue ED MN1
t+1induced by the team policy ϕN1
tand the mean-field
Mapprx ,t+1satisfies
Eh
dTV 
MN1
t+1,Mapprx ,t+1
|XN1
t,YN2
ti
≤|X|
2r
1
N1. (37)
Proof. Note that if MN1
t(x) = 0 , then the state xhas no contribution to the next mean-field propagated via (36). The
second case in (35) ensures that the constructed policy is well-defined. Without loss of generality, we assume that
MN1
t(x)>0for all x∈ X.
LetNx
1,t=N1MN1
t(x)be the number of Blue agents that are on state xat time tin the finite-population system.
With a slight abuse of notation, we use [Nx
1,t]to denote the set of the Blue agents on state x. Then, for agent i∈[Nx
1,t]
applying policy ϕi,t, its conditional distribution of its next state XN1
i,t+1is given by
px
i(x′) =P(XN1
i,t+1=x′|XN1
t,YN2
t) =X
u∈Ufρ
t(x′|u,MN1
t,NN2
t)ϕi,t(u|x,MN1
t,NN2
t)
Define the ED of these Nx
1,tBlue agents at time t+ 1as
MNx
1,t
t+1(x′) =1
Nx
1,tX
i∈[Nx
1,t]1x′ 
XN1
i,t+1
. (75)
On the other hand, the mean-field propagated from state xunder local policy πapprx ,tcan be expressed as
Mx
apprx ,t+1(x′) =X
ufρ
t(x′|x, u,MN1
t,NN2
t)πapprx ,t(u|x)
=X
ufρ
t(x′|x, u,MN1
t,NN2
t)P
i∈[Nx
1,t]ϕi,t(u|x,MN1
t,NN2
t)
Nx
1,t
=P
i∈[Nx
1,t]px
i(x′)
Nx
1,t.(76)
From Lemma 7, we have that
Eh
dTV 
MNx
1,t
t+1,Mx
apprx ,t+1XN1
t,YN2
ti
≤1
2s
|X|
Nx
1,t. (77)
10Letax=pNx
1,tfor each x∈ X , it follows that ax≥0andP
x(ax)2=N1almost surely, which allows the application of
Lemma 8.Published as a conference paper at AAAI 2024
The rest of the analysis is similar to the proof of Lemma 1. Notice that
MN1
t+1(x′) =X
x∈XNx
1,t
N1MNx
1,t
t+1(x′),
Mapprx ,t+1(x′) =X
x∈XNx
1,t
N1Mx
apprx ,t+1(x′).
It then follows
Eh
dTV 
MN1
t+1,Mapprx ,t+1XN1
t,YN2
ti
≤X
xNx
1,t
N1Eh
dTV 
MNx
1,t
t+1,Mx
apprx ,t+1XN1
t,YN2
ti
≤|X|
2r
1
N1,
where the last inequality results from Lemma 8.
Appendix C Proof of the Continuity Results
In this section, we present the proofs for Lemma 4 and Theorem 3. We start with several preliminary results.
C.1 Preliminary results
Lemma 9. Letξ:A × B → Rbe a Lipschitz continuous function in its second argument, such that
|ξ(a, b)−ξ(a, b′)| ≤Lξ∥b−b′∥for all a∈ A andb, b′∈ B. Let compact sets Q, Q′⊆ B such that distH(Q, Q′)≤ϵ.
Then, for all a∈ A,
min
b∈Qξ(a, b)−min
b′∈Q′ξ(a, b′)≤Lξϵ. (78a)
max
b∈Qξ(a, b)−max
b′∈Q′ξ(a, b′)≤Lξϵ. (78b)
Proof. We start with the proof for (78a). Fix a∈ A and consider the minimizer b∗∈argminb∈Qξ(a, b). From the
assumption on the distance between QandQ′, there exists a b∗′∈Q′such that ∥b∗′−b∗∥ ≤ϵ. Since ξis Lipschitz
with respect to b, it follows that
|ξ(a, b∗)−ξ(a, b∗′)| ≤Lξϵ. (79)
Then,
min
b∈Qξ(a, b) =ξ(a, b∗)≥ξ(a, b∗′)−Lξϵ≥min
b′∈Q′ξ(a, b′)−Lξϵ.
Similarly, one can show that minb′∈Q′ξ(a, b′)≥minb∈Qξ(a, b)−Lξϵ, and thus we have (78a).
The result (78a) can be easily extended to the maximization case in (78b) by making use of the fact that
max b∈Qξ(a, b) =−minb∈Q−ξ(a, b).
Lemma 10. Letζ:A×B → Rbe a Lipschitz continuous function in its first argument, such that |ζ(a, b)−ζ(a′, b)| ≤
Lζ∥a−a′∥for all a, a′∈ A andb∈ B. Then, for all compact sets Q⊆ B,
min
b∈Qζ(a, b)−min
b′∈Qζ(a′, b′)≤Lζ∥a−a′∥.
Proof. Since Qis compact and ζis continuous, the two minimization problems are well-defined. Next, let b∗∈
argminb∈Qζ(a, b), it follows that
min
b′∈Qζ(a′, b′)−min
b∈Qζ(a, b) = min
b′∈Qζ(a′, b′)−ζ(a, b∗)≤ζ(a′, b∗)−ζ(a, b∗)(i)
≤Lζ∥a−a′∥,
where (i) is due to the Lipschitz assumption. Similarly, one can show the other direction minb∈Qζ(a, b)−
minb′∈Qζ(a′, b′)≤Lζ∥a−a′∥.Published as a conference paper at AAAI 2024
Lemma 11. Consider the compact-valued correspondences Γ :X ×Y⇝XandΘ :X ×Y⇝Y, which are Lipschitz
continuous with Lipschitz constants LΓandLΘ, respectively. Given a Lg-Lipschitz continuous real-valued function
g:X × Y → R, the max-min marginal function
f(x, y) = max
p∈Γ(x,y)min
q∈Θ(x,y)g(p, q) (80)
is Lipschitz continuous with Lipschitz constant Lg(LΓ+LΘ).
Proof. Leth(x, y, p ) = min q∈Θ(x,y)g(p, q). Since g(p, q)is continuous and Θ(x, y)is compact, the minimization is
well-defined for each p. Fixp∈ X, and consider x, x′∈ X andy, y′∈ Y. The Lipschitz continuity of Θimplies that
distH(Θ(x, y),Θ(x′, y′))≤LΘ(∥x−x′∥+∥y−y′∥). (81)
For simplicity, let ϵ= (∥x−x′∥+∥y−y′∥). Leveraging Lemma 9, we have11
|h(x, y, p )−h(x′, y′, p)| ≤LgLΘϵ. (82)
Next, fix (x, y)∈ X × Y and consider p, p′∈ Y. It follows from Lemma 10 that12
|h(x, y, p )−h(x, y, p′)|=min
q∈Θ(x,y)g(p, q)−min
q′∈Θ(x,y)g(p′, q′)≤Lg∥p−p′∥. (83)
Finally, consider x, x′∈ X andy, y′∈ Y, it follows that
f(x, y)−f(x′, y′)=max
p∈Γ(x,y)h(x, y, p )−max
p′∈Γ(x′,y′)h(x′, y′, p′)(84)
≤max
p∈Γ(x,y)h(x, y, p )−max
p∈Γ(x,y)h(x′, y′, p)+max
p∈Γ(x,y)h(x′, y′, p)−max
p′∈Γ(x′,y′)h(x′, y′, p′)(85)
≤LgLΘϵ+LΓLgϵ=Lg(LΘ+LΓ)ϵ, (86)
where the first term in (85) is bounded using Lemma 10 and the Lipschitz constant derived in (82)13, and the second
term in (85) is bounded using Lemma 9 and the Lipschitz constant in (83)14.
Next, we present some results regarding the continuity of the reachability correspondences. We start by defining the
pure local policies.
Definition 10 (Pure local policy) .A Blue local policy πt∈Πtis said to be pure if πt(u|x)∈ {0,1}for all u∈ U and
x∈ X . We use ˆΠt={ˆπk}|U||X|
k=1to denote the set of pure Blue policies, where ˆπkdenotes the k-th pure Blue local
policy. The pure Red local policies are defined similarly.
Proposition 3. The Blue reachable set is characterized as
Rρ
µ,t(µt, νt) = Co( {µtFρ
t(µt, νt,ˆπk}|U||X|
k=1), (87)
where ˆπkare pure Blue local policies, and Codenotes the convex hull.
Proof. We first show that the mean-field propagation rule is linear with respect to the policy. Note that the set of
admissible policies Πtcan be characterized as the convex hull of the pure policies, i.e., Πt= Co( {ˆπk}k). Consider
arbitrary current mean-fields µtandνt, and consider the policy πt=λπ1
t+ (1−λ)π2
tfor some λ∈(0,1)and
π1
t, π2
t∈Πt. Then, the next mean-field induced by πtis given by
µt+1(x′) =X
x"X
ufρ
t(x′|x, µt, νt)πt(u|x)#
µt(x)
11Relating to Lemma 9: Set gas function ξ; treat the argument pasaand the optimization variable qasb; regard the sets Θ(x, y)
andΘ(x′, y′)asQandQ′respectively.
12Relating to Lemma 10: Set gas function ζ; treat the arguments pandqasaandb, respectively; regard the set Θ(x, y)asQ.
13Relating to Lemma 10: Set has function ζ; treat the arguments (x, y)asaand the argument pasb; regard the set Γ(x, y)as
the optimization domain Q.
14Relating to Lemma 9: Set has function ξ; treat the argument (x′, y′)asaand the optimization variable pasb; regard the sets
Γ(x, y)andΓ(x′, y′)asQandQ′respectively.Published as a conference paper at AAAI 2024
=λX
x"X
ufρ
t(x′|x, µt, νt)π1
t(u|x)#
µt(x) + (1 −λ)X
x"X
ufρ
t(x′|x, µt, νt)π2
t(u|x)#
µt(x),
which implies that
µt+1=µtFρ
t(µt, νt, πt) =λµtFρ
t(µt, νt, π1
t) + (1 −λ)µtFρ
t(µt, νt, π2
t). (88)
For the ⊆direction in (87), consider µt+1∈ Rρ
µ,t(µt, νt). Then there exists a policy πtsuch that µt+1=
µtFρ
t(µt, νt, πt). Since the admissible policy set is the convex hull of pure policies, we have πt=P
kλkˆπk, where
λk≥0andP
kλk= 1. It follows directly from (88) that
µt+1=X
kλkµtFρ
t(µt, νt,ˆπk)∈Co({µtFρ
t(µt, νt,ˆπk}|U||X|
k=1). (89)
For the ⊇direction in (87), consider a point µt+1∈Co({µtFρ
t(µt, νt,ˆπk)}|U||X|
k=1). By definition and the linearity
in (88), we have
µt+1=X
kλkµtFρ
t(µt, νt,ˆπk) =µtFρ
t(µt, νt, πt),
where πt=P
kλkˆπk∈Πt, which implies µt+1∈ Rρ
µ,t(µρ
t, νρ
t)
Lemma 12. Consider a pure local policy ˆπt∈ˆΠtand arbitrary mean-fields µt, µ′
t∈ P(X)andνt, ν′
t∈ P(Y). The
following bound holds:
dTV 
µtFρ
t(µt, νt,ˆπt), µ′
tFρ
t(µ′
t, ν′
t,ˆπt)
≤ 
1 +1
2Lft 
dTV 
µt, µ′
t
+ dTV 
νt, ν′
t
.
Proof. Using the triangle inequality, we have that
dTV 
µtFρ
t(µt, νt,ˆπt), µ′
tFρ
t(µ′
t, ν′
t,ˆπt)
≤dTV 
µtFρ
t(µt, νt,ˆπt), µ′
tFρ
t(µt, νt,ˆπt)
| {z }
A
+ dTV 
µ′
tFρ
t(µt, νt,ˆπt), µ′
tFρ
t(µ′
t, ν′
t,ˆπt)
| {z }
B.
Since Fρ
tis a stochastic matrix, it is a non-expansive operator under the 1-norm, and hence A≤dTV 
µt, µ′
t
. Next,
we bound the term Busing Assumption 1.
B=1
2X
x′X
xµ′
t(x)X
uˆπt(u|x) 
fρ
t(x′|x, µt, νt, u)−fρ
t(x′|x, µ′
t, ν′
t, u)
≤1
2X
x′,x,uµ′
t(x)ˆπt(u|x)fρ
t(x′|x, µt, νt, u)−fρ
t(x′|x, µ′
t, ν′
t, u)
=1
2X
x,uµ′
t(x)ˆπt(u|x)X
x′fρ
t(x′|x, µt, νt, u)−fρ
t(x′|x, µ′
t, ν′
t, u)
≤1
2X
x,uµ′
t(x)ˆπt(u|x)Lft
dTV 
µt, µ′
t
+ dTV 
νt, ν′
t
=1
2Lft
dTV 
µt, µ′
t
+ dTV 
νt, ν′
t
Combining the bounds, we have
dTV 
µtFρ
t(µt, νt,ˆπt), µ′
tFρ
t(µ′
t, ν′
t,ˆπt)
≤ 
1 +1
2Lft
dTV 
µt, µ′
t
+1
2LftdTV 
νt, ν′
t
.
Lemma 13. Let two sets of points {xi}N
i=1and{yi}N
i=1that satisfy ∥xi−yi∥ ≤ϵfor all i. Then,
distH(Co({xi}N
i=1),Co({yi}N
i=1))≤ϵ.Published as a conference paper at AAAI 2024
Proof. Consider a point x∈Co({xi}N
i=1). By definition, there exists a set of non-negative coefficients {λi}N
i=1, such
thatx=P
iλixiandP
iλi= 1. Using the same set of coefficients, define y=P
iλiyi∈Co({yi}N
i=1). Then,
∥x−y∥ ≤X
iλi∥xi−yi∥ ≤ϵ.
Thus, for a fixed x∈Co({xi}N
i=1), we have infy∈Co({yi}N
i=1)∥x−y∥ ≤ϵ.Since x∈Co({xi}N
i=1)is arbitrary, the
above inequality implies
sup
x∈Co({xi}N
i=1)inf
y∈Co({yi}N
i=1)∥x−y∥ ≤ϵ.
Through a similar argument, we can conclude that supy∈Co({yi}N
i=1)infx∈Co({xi}N
i=1)∥x−y∥ ≤ϵ,which completes
the proof.
C.2 Proof of Lemma 4
Lemma 4. The reachability correspondences Rµ,tandRν,tin(30) and(31) satisfy the following inequalities for all
µt, µ′
t∈ P(X)andνt, ν′
t∈ P(Y),
distH(Rρ
µ,t(µt, νt),Rρ
µ,t(µ′
t, ν′
t))≤LRµ,t 
dTV 
µt, µ′
t
+ dTV 
νt, ν′
t
,
distH(Rρ
ν,t(µt, νt),Rρ
ν,t(µ′
t, ν′
t))≤LRν,t 
dTV 
µt, µ′
t
+ dTV 
νt, ν′
t
,(41)
where LRµ,t= 1 +1
2Lft,LRν,t= 1 +1
2Lgt, and LftandLgtare the Lipschitz constants in Assumption 1.
Proof. Due to symmetry, we only present the proof for the Blue reachability correspondence. From Lemma 12, we
have that, for all pure local policies ˆπk
t∈ˆΠt,
dTV 
µtFρ
t(µt, νt,ˆπk
t), µ′
tFρ
t(µ′
t, ν′
t,ˆπt)
≤ 
1 +1
2Lft
dTV 
µt, µ′
t
+1
2LftdTV 
νt, ν′
t
.
From Proposition 3, we know that the reachable set can be characterized as the convex hull
Rρ
µ,t(µt, νt) = Co( {µtFρ
t(µt, νt,ˆπk}|U||X|
k=1).
Leveraging Lemma 13, we can conclude that
distH(Rµ,t(µt, νt),Rµ,t(µ′
t, ν′
t)))≤ 
1 +1
2Lft 
dTV 
µt, µ′
t
+ dTV 
νt, ν′
t
.
C.3 Proof of Theorem 3
Theorem 3. The optimal lower value function Jρ∗
cor,tand the optimal upper value function ¯Jρ∗
cor,tare both Lipschitz
continuous. Formally, for all µρ
t, µρ′
t∈ P(X)andνρ
t, νρ′
t∈ P(Y), the following inequalities hold
Jρ∗
cor,t(µρ
t, νρ
t)−Jρ∗
cor,t(µρ′
t, νρ′
t)≤LJ,t 
dTV 
µρ
t, µρ′
t
+ dTV 
µρ
t, νρ′
t
,
¯Jρ∗
cor,t(µρ
t, νρ
t)−¯Jρ∗
cor,t(µρ′
t, νρ′
t)≤LJ,t 
dTV 
µρ
t, µρ′
t
+ dTV 
µρ
t, νρ′
t
,(42)
where the Lipschitz constant LJ,tis given by
LJ,t=Lr 
1 +T−1X
k=tkY
τ=t(LRρ
µ,τ+LRρ
ν,τ)
. (43)
Proof. The proof is given by induction.
Base case: At the terminal time T, the optimal coordinator value function is given by
Jρ∗
cor,T(µρ
T, νρ
T) =rρ
T(µρ
T, νρ
T).
From Assumption 2, the Lipschitz constant for Jρ∗
cor,TisLr.
Inductive hypothesis: Suppose that the Lipschitz constant is given by (43) at time t+ 1.Published as a conference paper at AAAI 2024
Induction: Recall the definition of the optimal coordinator lower value at time t:
Jρ∗
cor,t(µρ
t, νρ
t) =rt(µρ
t, νρ
t) + max
µρ
t+1∈Rρ
µ,t(µρ
t,νρ
t)min
νρ
t+1∈Rρ
ν,t(µρ
t,νρ
t)Jρ∗
cor,t+1(µρ
t+1, νρ
t+1),
where the second term takes the form of a max-min marginal function defined in (80), where Γ =Rρ
µ,t,Θ =Rρ
ν,tand
g=Jρ∗
cor,t+1, while the first term is Lipschitz continuous with Lipschitz constant Lr. Applying Lemma 11, it follows
that, for all µρ
t, µρ′
t∈ P(X)andνρ
t, νρ′
t∈ P(Y),
Jρ∗
cor,t(µρ
t, νρ
t)−Jρ∗
cor,t(µρ′
t, νρ′
t)≤rt(µρ
t, νρ
t)−rt(µρ′
t, νρ′
t)+
max
µρ
t+1∈Rρ
µ,t(µρ
t,νρ
t)min
νρ
t+1∈Rρ
ν,t(µρ
t,νρ
t)Jρ∗
cor,t+1(µρ
t+1, νρ
t+1)− max
µρ′
t+1∈Rρ′
µ,t(µρ′
t,νρ′
t)min
νρ′
t+1∈Rρ′
ν,t(µρ′
t,νρ′
t)Jρ∗
cor,t+1(µρ′
t+1, νρ′
t+1)
≤
Lr+ (LRρ
µ,t+LRρ
ν,t)Lr
1 +T−1X
k=t+1kY
τ=t+1(LRρ
µ,τ+LRρ
ν,τ)
dTV 
µρ
t, µρ′
t
+ dTV 
νρ
t, νρ′
t
=
Lr
1 + (LRρ
µ,t+LRρ
ν,t) +T−1X
k=t+1(LRρ
µ,t+LRρ
ν,t)kY
τ=t+1(LRρ
µ,τ+LRρ
ν,τ)
dTV 
µρ
t, µρ′
t
+ dTV 
νρ
t, νρ′
t
=
Lr
1 + (LRρµ,t+LRρ
ν,t) +T−1X
k=t+1kY
τ=t(LRρ
µ,τ+LRρ
ν,τ)
dTV 
µρ
t, µρ′
t
+ dTV 
νρ
t, νρ′
t
=
Lr
1 +T−1X
k=tkY
τ=t(LRρ
µ,τ+LRρ
ν,τ)
dTV 
µρ
t, µρ′
t
+ dTV 
νρ
t, νρ′
t
,
which completes the induction.
Appendix D Proof of Existence of Game Values
We first examine the convexity of the reachability correspondences.
Definition 11 ([Kuroiwa, 1996]) .LetX,YandZbe convex sets. The correspondence Γ :X × Y⇝Zis convex
with respect to Xif, for all x1, x2∈ X ,y∈ Y ,z1∈Γ(x1, y),z2∈Γ(x2, y), and λ∈(0,1), there exists
z∈Γ(λx1+ (1−λ)x2, y)such that
z=λz1+ (1−λ)z2.
Remark 9. The above definition of convexity is equivalent to the following set inclusion
Γ(λx1+ (1−λ)x2, y)⊇λΓ(x1, y) + (1 −λ)Γ(x2, y), (90)
for all λ∈(0,1),x1, x2∈ X andy∈ Y, where the Minkowski sum is defined as
λΓ(x1, y) + (1 −λ)Γ(x2, y) ={z|z=λz1+ (1−λ)z2where z1∈Γ(x1, y), z2∈Γ(x2, y)}.
Definition 12. The correspondence Γ :X × Y⇝Zis constant with respect to Yif
Γ(x, y1) = Γ( x, y2)∀x∈ X,∀y1, y2∈ Y. (91)
We have the following concave-convex result for a max-min marginal function defined in (80).
Lemma 14. Consider two compact-valued correspondences Γ :X × Y ⇝XandΘ :X × Y ⇝Y. Let Γ
be convex with respect to Xand constant with respect to Y, and let Θbe constant with respect to Xand convex
with respect to Y. Let g:X × Y → Rbe concave-convex. Then, the max-min marginal function f(x, y) =
max p∈Γ(x,y)minq∈Θ(x,y)g(p, q)is also concave-convex.
Proof. Fixy∈ Y, and consider x1, x2∈ X andλ∈(0,1). Denote x=λx1+ (1−λ)x2. It follows that
f(λx1+ (1−λ)x2, y) = max
p∈Γ(λx1+(1−λ)x2,y)min
q∈Θ(λx1+(1−λ)x2,y)g(p, q)
(i)
≥ max
p∈λΓ(x1,y)+(1−λ)Γ(x2,y)min
q∈Θ(x,y)g(p, q)
= max
p1∈Γ(x1,y)
p2∈Γ(x2,y)min
q∈Θ(x,y)g(λp1+ (1−λ)p2, q)Published as a conference paper at AAAI 2024
(ii)
≥max
p1∈Γ(x1,y)
p2∈Γ(x2,y)min
q∈Θ(x,y)(λg(p1, q) + (1 −λ)g(p2, q))
(iii)
≥ max
p1∈Γ(x1,y)
p2∈Γ(x2,y)
min
q1∈Θ(x,y)λg(p1, q1)
| {z }
A(p1)+ min
q2∈Θ(x,y)(1−λ)g(p2, q2)
| {z }
B(p2)
=λmax
p1∈Γ(x1,y)min
q1∈Θ(x,y)g(p1, q1) + (1 −λ) max
p2∈Γ(x2,y)min
q2∈Θ(x,y)g(p2, q2)
(iv)=λmax
p1∈Γ(x1,y)min
q1∈Θ(x1,y)g(p1, q1) + (1 −λ) max
p2∈Γ(x2,y)min
q2∈Θ(x2,y)g(p2, q2)
=λf(x1, y) + (1 −λ)f(x2, y),
where inequality (i) holds from restricting the maximization domain using the convexity of Γin (90); inequality (ii)
holds from the assumption that gis concave with respect to its p-argument; inequality (iii) is the result of distributing
the minimization; equality (iv) is due to Θbeing constant with respect to X. The above result implies the concavity of
fwith respect to its x-argument.
Fixx∈ X, and let y1, y2∈ Y andλ∈(0,1). Denote y=λy1+ (1−λ)y2. Then,
f(x, λy 1+ (1−λ)y2) = max
p∈Γ(x,λy 1+(1−λ)y2)min
q∈Θ(x,λy 1+(1−λ)y2)g(p, q)
≤max
p∈Γ(x,y)min
q1∈Θ(x,y1)
q2∈Θ(x,y2)g(p, λq 1+ (1−λ)q2)
≤max
p∈Γ(x,y)min
q1∈Θ(x,y1)
q2∈Θ(x,y2)(λg(p, q1) + (1 −λ)g(p, q2))
≤λmax
p1∈Γ(x,y)min
q1∈Θ(x,y1)g(p1, q1) + (1 −λ) max
p2∈Γ(x,y)min
q2∈Θ(x,y2)g(p2, q2)
=λmax
p1∈Γ(x,y1)min
q1∈Θ(x,y1)g(p1, q1) + (1 −λ) max
p2∈Γ(x,y2)min
q2∈Θ(x,y2)g(p2, q2)
=λf(x, y1) + (1 −λ)f(x, y2),
which implies that fis convex with respect to its y-argument.
Recall the definition of independent dynamics:
Definition 7. We say that the weakly-coupled dynamics are independent if the following holds for all µt∈ P(X),
νt∈ P(Y)andt∈ {0, . . . , T −1},
fρ
t(xt+1|xt, ut, µt, νt) =¯ft(xt+1|xt, ut),
gρ
t(yt+1|yt, vt, µt, νt) = ¯gt(yt+1|yt, vt),(34)
for some transition kernels ¯ftand¯gt.
The following lemma shows the convexity of the reachability correspondences Rρ
µ,tandRρ
ν,tunder independent
dynamics.
Lemma 15. Under the independent dynamics in Definition 7, the reachability correspondences satisfy
Rρ
µ,t(µt, νt) =Rρ
µ,t(µt, ν′
t),∀µt∈ P(X),∀νt, ν′
t∈ P(Y), (92a)
Rρ
ν,t(µt, νt) =Rρ
ν,t(µ′
t, νt),∀µt, µ′
t∈ P(X),∀νt∈ P(Y). (92b)
Furthermore, for all λ∈[0,1],
Rρ
µ,t 
λµt+ (1−λ)µ′
t, νt
=λRρ
µ,t 
µt, νt
+ (1−λ)Rρ
µ,t 
µ′
t, νt
,∀µt, µ′
t∈ P(X),∀νt∈ P(Y), (93a)
Rρ
ν,t 
µt, λνt+ (1−λ)ν′
t
=λRρ
ν,t 
µt, νt
+ (1−λ)Rρ
ν,t 
µt, ν′
t
,∀µt∈ P(X),∀νt, ν′
t∈ P(Y). (93b)
Proof. Due to symmetry, we only prove the results for the Blue reachability correspondence. The results for the Red
team can be obtained through a similar argument.Published as a conference paper at AAAI 2024
Consider an arbitrary pair of distributions µt∈ P(X)andνt∈ P(Y). Ifµt+1∈ Rρ
µ,t(µt, νt)under the independent
dynamics in (34), there exists a local policy πt∈Πtsuch that
µt+1(x′) =X
x∈XhX
u∈U¯ft(x′|x, u)πt(u|x)i
µt(x),
which is independent of νt. Consequently,
Rρ
µ,t(µt, νt) =Rρ
µ,t(µt, ν′
t),∀µt∈ P(X)and∀νt, ν′
t∈ P(Y).
Next, we prove the property in (93a). For the ⊆direction, consider two Blue mean-fields µt, µ′
t∈ P(X)and a Red
mean-field νt∈ P(Y). Let ¯µt+1∈ Rρ
µ,t(λµt+ (1−λ)µ′
t, νt). From the definition of the reachable set, there exists a
local policy πt∈Πtsuch that
¯µt+1(x′) =X
x∈XhX
u∈U¯ft(x′|x, u)πt(u|x)i
(λµt(x) + (1 −λ)µ′
t(x))
=λX
x∈XhX
u∈U¯ft(x′|x, u)πt(u|x)i
µt(x)
| {z }
µt+1(x′)+(1−λ)X
x∈XhX
u∈U¯ft(x′|x, u)πt(u|x)i
µ′
t(x)
| {z }
µ′
t+1(x′),
where µt+1∈ Rρ
µ,t(µt, νt)andµ′
t+1∈ Rρ
µ,t(µ′
t, νt). Hence, we have
Rρ
µ,t(λµt+ (1−λ)µ′
t, νt)⊆λRρ
µ,t 
µt, νt
+ (1−λ)Rρ
µ,t 
µ′
t, νt
.
Let now ¯µt+1=λµt+1+ (1−λ)µ′
t+1, where µt+1∈ Rρ
µ,t(µt, νt)andµ′
t+1∈ Rρ
µ,t(µ′
t, νt). Further, define
¯µt=λµt+ (1−λ)µ′
t. From the definition of the reachable set, there exists local policies πtandπ′
tsuch that
¯µt+1=λX
x∈XhX
u∈U¯ft(x′|x, u)πt(u|x)i
µt(x) + (1 −λ)X
x∈XX
u∈U¯ft(x′|x, u)π′
t(u|x)i
µ′
t(x)
=X
x∈XX
u∈U¯ft(x′|x, u)h
λπt(u|x)µt(x) + (1 −λ)π′
t(u|x)µ′
t(x)i
=X
x∈XX
u∈U1¯µt(x)>0¯ft(x′|x, u)h
λπt(u|x)µt(x) + (1 −λ)π′
t(u|x)µ′
t(x)i
+X
x∈XX
u∈U1¯µt(x)=0¯ft(x′|x, u)h
λπt(u|x)µt(x) + (1 −λ)π′
t(u|x)µ′
t(x)i
| {z }
=015
=X
x∈XX
u∈U1¯µt(x)>0¯ft(x′|x, u)λπt(u|x)µt(x) + (1 −λ)π′
t(u|x)µ′
t(x)
¯µt(x)¯µt(x)
+X
x∈XX
u∈U1¯µt(x)=0¯ft(x′|x, u)h
λπt(u|x) + (1 −λ)π′
t(u|x)i
¯µt(x)
| {z }
=0
=X
x∈X"X
u∈U¯ft(x′|x, u)¯πt(u|x)#
¯µt(x),
where the “averaged” local policy ¯πtis given by
¯πt(u|x) =1¯µt(x)>0λπt(u|x)µt(x) + (1 −λ)π′
t(u|x)µ′
t(x)
λµt(x) + (1 −λ)µ′
t(x)+1¯µt(x)=0
λπt(u|x) + (1 −λ)π′
t(u|x)
.
Consequently, we have
¯µt+1∈ Rρ
µ,t(λµt+ (1−λ)µ′
t, νt).
Hence,
Rρ
µ,t(λµt+ (1−λ)µ′
t, νt)⊇λRρ
µ,t 
µt, νt
+ (1−λ)Rρ
µ,t 
µ′
t, νt
.
15Since the condition ¯µt(x) = 0 together with λ∈(0,1)implies that µt(x) = 0 andµ′
t(x) = 0 .Published as a conference paper at AAAI 2024
Theorem 1. Suppose the reward function rρ
tis concave-convex for all t∈ {0, . . . , T }and the system dynamics is
independent. Then, the game value exists.
Proof. The game value at time Tis directly the mean-field reward, which is assumed to be concave-convex. Hence,
one can provide an inductive proof leveraging Lemma 14 and Lemma 15 to show that both the lower and upper game
values are concave-convex at each time step. Note that the lower game value in (33) has the form of a max-min
marginal function. Finally, as the reachable sets are compact and convex, one can apply the minimax theorem [Owen,
2013], which guarantees that the max-min optimal value is the same as the min-max optimal value, and thus ensures
the existence of the game value.
Appendix E Policy Extraction from Reachability Set
In Section 4.4, we changed the optimization domain from the policy space to the reachability set and obtained the
optimal next-to-go mean-field. We now address the natural question regarding the construction of the coordination
strategy that achieve the desired next-to-go mean-field.
We consider the following general policy extraction problem.
Problem 1. Given a desired next-to-go Blue mean-field µρ
t+1∈ Rρ
µ,t(µρ
t, νρ
t), find the local policy πt∈Πtsuch that
µρ
t+1=µρ
tFρ(µρ
t, νρ
t, πt).
From the convex hull characterization of the reachability set (see Proposition 3), we have that the mean-field µρ
t+1∈
Rρ
µ,t(µρ
t, νρ
t)can be written as
µρ
t+1=X
kλkµρ
tFρ
t(µρ
t, νρ
t,ˆπk
t),
where λk≥0andP
kλk= 1, and{ˆπk}kare the pure local policies (see Definition 10).
Since µρ
t,νρ
tand{πk
t}are all known variables at time t, the vector-form coefficients λcan be found by solving the
following linear feasibility problem:
Mtλ=µρ
t+1,s.t.λ≥0andX
kλk= 1, (94)
where the matrix Mt∈R|X|×| ˆΠt|hasµρ
tFρ
t(µρ
t, νρ
t,ˆπk
t)as its columns. Again, the feasibility of (94) is guaranteed
due to the construction of the reachable set.
From the linearity of the mean-field dynamics in (88), we have that the local policy πt=P
kλkˆπk
tachieves the
desired next-to-go mean-field. Specifically,
µρ
t+1=µρ
tFρ(µρ
t, νρ
t, πt) =X
kλkµρ
tFρ
t(µρ
t, νρ
t,ˆπk
t).
The Blue coordination strategy can then be constructed to select the local Blue policy solved from (94) to achieve the
optimal next-to-go Blue mean-field.
References
Aicardi, M., Davoli, F., and Minciardi, R. (1987). Decentralized optimal control of Markov chains with a common
past information set. IEEE Transactions on Automatic Control , 32(11):1028–1031.
Arabneydi, J. and Mahajan, A. (2014). Team optimal control of coupled subsystems with mean-field sharing. In 53rd
IEEE Conference on Decision and Control , pages 1669–1674.
Arabneydi, J. and Mahajan, A. (2015). Team-optimal solution of finite number of mean-field coupled LQG subsys-
tems. In 54th IEEE Conference on Decision and Control , pages 5308–5313, Osaka, Japan, Dec. 15–18, 2015.
Bernstein, D. S., Givan, R., Immerman, N., and Zilberstein, S. (2002). The complexity of decentralized control of
Markov decision processes. Mathematics of Operations Research , 27(4):819–840.
Bertsekas, D. and Tsitsiklis, J. N. (1996). Neuro-dynamic Programming . Athena Scientific.
Chung, K. L. (2001). A Course in Probability Theory . Academic press.Published as a conference paper at AAAI 2024
Cui, K. and Koeppl, H. (2021). Approximately solving mean field games via entropy-regularized deep reinforcement
learning. In International Conference on Artificial Intelligence and Statistics , pages 1909–1917. PMLR.
Elliott, R. J. and Kalton, N. J. (1972). The Existence of Value in Differential Games , volume 126. American Mathe-
matical Soc.
Freeman, R. and Kokotovic, P. V . (2008). Robust Nonlinear Control Design: State-space and Lyapunov Techniques .
Springer Science & Business Media.
Gibbons, R., Roberts, J., et al. (2013). The Handbook of Organizational Economics . Princeton University Press
Princeton, NJ.
Guan, Y ., Pan, L., Shishika, D., and Tsiotras, P. (2023). On the adversarial convex body chasing problem. In 2023
American Control Conference , pages 435–440.
Guan, Y ., Zhou, M., Pakniyat, A., and Tsiotras, P. (2022). Shaping large population agent behaviors through entropy-
regularized mean-field games. In 2022 American Control Conference (ACC) , pages 4429–4435. IEEE.
Ho, Y .-C. (1980). Team decision theory and information structures. Proceedings of the IEEE , 68(6):644–654.
Hogeboom-Burr, I. and Y ¨uksel, S. (2023). Zero-sum games involving teams against teams: existence of equilibria,
and comparison and regularity in information. Systems & Control Letters , 172:105454.
Huang, M., Caines, P. E., and Malham ´e, R. P. (2007). Large-population cost-coupled LQG problems with nonuniform
agents: individual-mass behavior and decentralized ϵ-Nash equilibria. IEEE Transactions on Automatic Control ,
52(9):1560–1571.
Huang, M., Malham ´e, R. P., and Caines, P. E. (2006). Large population stochastic dynamic games: closed-loop
McKean-Vlasov systems and the nash certainty equivalence principle. Communications in Information & Systems ,
6(3):221–252.
Kartik, D., Nayyar, A., and Mitra, U. (2021). Common information belief based dynamic programs for stochastic
zero-sum games with competing teams. arXiv preprint arXiv:2102.05838 .
Kuroiwa, D. (1996). Convexity for set-valued maps. Applied Mathematics Letters , 9(2):97–101.
Lagoudakis, M. G. and Parr, R. (2002). Learning in zero-sum team Markov games using factored value functions.
Advances in Neural Information Processing Systems , 15.
Lasry, J.-M. and Lions, P.-L. (2007). Mean field games. Japanese Journal of Mathematics , 2(1):229–260.
Lauri `ere, M., Perrin, S., Geist, M., and Pietquin, O. (2022). Learning mean field games: A survey. arXiv preprint
arXiv:2205.12944 .
Li, J., Tinka, A., Kiesel, S., Durham, J. W., Kumar, T. S., and Koenig, S. (2021). Lifelong multi-agent path finding
in large-scale warehouses. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 35, pages
11272–11281.
Mahajan, A. (2011). Optimal decentralized control of coupled subsystems with control sharing. In 50th IEEE Con-
ference on Decision and Control and European Control Conference , pages 5726–5731, Orlando, FL, Dec. 12–15,
2011.
Mahajan, A., Martins, N. C., Rotkowitz, M. C., and Y ¨uksel, S. (2012). Information structures in optimal decentralized
control. In 51st IEEE Conference on Decision and Control , pages 1291–1306, Maui, HW, Dec. 10–13, 2012. IEEE.
Mahajan, A. and Nayyar, A. (2015). Sufficient statistics for linear control strategies in decentralized systems with
partial history sharing. IEEE Transactions on Automatic Control , 60(8):2046–2056.
Marschak, J. (1955). Elements for a theory of teams. Management Science , 1(2):127–137.
Nayyar, A., Mahajan, A., and Teneketzis, D. (2013). Decentralized stochastic control with partial history sharing: A
common information approach. IEEE Transactions on Automatic Control , 58(7):1644–1658.
Owen, G. (2013). Game Theory . Emerald Group Publishing.
Papadimitriou, C. H. and Tsitsiklis, J. N. (1985). Intractable problems in control theory. In 24th IEEE Conference on
Decision and Control , pages 1099–1103, Fort Lauderdale, FL, Dec. 11–13, 1985.
Radner, R. (1962). Team decision problems. The Annals of Mathematical Statistics , 33(3):857–881.
Sanjari, S., Saldi, N., and Y ¨uksel, S. (2023). Nash equilibria for exchangeable team against team games and their
mean field limit. In 2023 American Control Conference , pages 1104–1109. IEEE.
Tian, Y ., Liu, K., Ok, K., Tran, L., Allen, D., Roy, N., and How, J. P. (2020). Search and rescue under the forest canopy
using multiple UA Vs. The International Journal of Robotics Research , 39(10-11):1201–1221.Published as a conference paper at AAAI 2024
Tyler, J., Arnold, R., Abruzzo, B., and Korpela, C. (2020). Autonomous teammates for squad tactics. In International
Conference on Unmanned Aircraft Systems , pages 1667–1672, Athens, Greece, Sept. 1–4, 2020.
Witsenhausen, H. S. (1971). Separation of estimation and control for discrete-time systems. Proceedings of the IEEE ,
59(11):1557–1566.
Y¨uksel, S. (2009). Stochastic nestedness and the belief sharing information pattern. IEEE Transactions on Automatic
Control , 54(12):2773–2786.
Zazo, S., Macua, S. V ., S ´anchez-Fern ´andez, M., and Zazo, J. (2016). Dynamic potential games with constraints:
Fundamentals and applications in communications. IEEE Transactions on Signal Processing , 64(14):3806–3821.