DIVERSE AND VIVID SOUND GENERATION FROM TEXT DESCRIPTIONS
Guangwei Li, Xuenan Xu, Lingfeng Dai, Mengyue Wuy, Kai Yuy
X-LANCE Lab, Department of Computer Science and Engineering
MoE Key Lab of Artiﬁcial Intelligence
AI Institute, Shanghai Jiao Tong University, Shanghai, China
ABSTRACT
Previous audio generation mainly focuses on speciﬁed sound classes
such as speech or music, whose form and content are greatly re-
stricted. In this paper, we go beyond speciﬁc audio generation by us-
ing natural language description as a clue to generate broad sounds.
Unlike visual information, a text description is concise by its nature
but has rich hidden meanings beneath, which poses a higher pos-
sibility and complexity on the audio to be generated. A Variation-
Quantized GAN is used to train a codebook learning discrete rep-
resentations of spectrograms. For a given text description, its pre-
trained embedding is fed to a Transformer to sample codebook in-
dices to decode a spectrogram to be further transformed into wave-
form by a melgan vocoder. The generated waveform has high quality
and ﬁdelity while excellently corresponding to the given text. Ex-
periments show that our proposed method is capable of generating
natural, vivid audios, achieving superb quantitative and qualitative
results.
©2023 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including
reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or
reuse of any copyrighted component of this work in other works.Index Terms —Sound generation, Variation-Quantized GAN,
text-to-sound
1. INTRODUCTION
Cross-modality generation task has received increasing amount of
attention, spanning from text-image [1, 2, 3], image-audio [4, 5, 6],
and vice versa. However, audio generation from text, in particular
from sentence-level descriptions, is still at an emerging stage. Au-
dio generation has exhibited unique challenges due to sound events’
time-frequency correlations in the real world. For example, for the
sentence ‘Food is frying, and a woman talks,’ the uncertainties in
the chronological order and duration of both sounds largely inﬂu-
ence the possible audio content. In this paper, we deﬁne a task in
which complex audios are generated based on free textual descrip-
tions (see Figure 1). The generated audios are expected to maintain
high quality and naturalness as well as comprehensively reﬂect the
sound classes and their relations described in the sentences.
Previous audio generation tasks mainly focus on synthesizing
speciﬁed sound classes like speech [7, 8, 9] and music [10, 11, 12].
However, these tasks can be further extended: i) The sound class to
generate is pre-deﬁned hence the input is restricted; ii) The content
generated is, in fact, a translation of the input modality. An emerg-
ing study [13] focuses on audio generation given text as input, using
onomatopoeic words to synthesize environmental sounds. However,
the diversity is still restricted since onomatopoeic words are limited
yMengyue Wu and Kai Yu are the corresponding authors.This
work has been supported by Shanghai Municipal Science and Technol-
ogy Major Project (2021SHZDZX0102) and Jiangsu Technology Project
(No.BE2022059-2).
t
Text to Audio Generation Model
0s 10sGenerated Example 1
Generated Example 2Text: A dog barking  as a man is talking  with wind
blowing into a microphone  followed by  birds chirping
Fig. 1 . The proposed audio generation from in-the-wild text aims at
generating high-quality rich audio content based on any given text.
Here are two schematic examples generated from one single sen-
tence which contains multiple sound events from our model.
and sometimes less expressive. Recently, vivid audio generation
with less restricted input is implemented with visual cues [4, 5, 6]
or onomatopoeic words [13]. Yet, there is little research on how to
generate diverse audios based on free-text descriptions. Concurrent
work focusing on this task includes [14, 15]. Unlike audio-video
or audio-image pairs that can be naturally obtained from the original
video sources, audio-text pairs need additional annotations. Annota-
tions describing the same audio may greatly vary from each other in
word choices, overall style and writing order.
Inspired by studies on audio generation with visual cues, we pro-
pose a text-to-audio generation system that can generate high-quality
audios given descriptive captions. The system is presented in Fig-
ure 2. A Transformer and a reference codebook generates dense rep-
resentations according to the text feature extracted from a pre-trained
language model. Then a decoder transforms the codebook represen-
tations into a spectrogram. Finally the spectrogram is transformed
into a waveform with a MelGAN [16] vocoder. The codebook and
the decoder are from a pre-trained Variation-Quantized Generative
Adversarial Network (VQGAN) [4, 1]. The system is trained on
audio captioning datasets [17, 18], where audio clips and their cor-
responding text descriptions are provided.
We apply different metrics, including PSNR, FID, MMKL and
SPICE, CIDEr (see details in Section 3.1), as well as subjective hu-
man analysis for evaluation. Both quantitative and qualitative results
demonstrate that the generated audios retain high hearing quality and
are reﬂective of the text given, thus, exhibiting the effectiveness of
our text-to-audio generation approach. We also present a few gener-
ated examples for a more intuitive presentation1.
1https://ligw1998.github.io/audiogeneration.htmlarXiv:2305.01980v1  [cs.SD]  3 May 2023unflattenA man speaks as birds chirp and
dog barks
Language Model(A). T ext feature extraction
Text feature (B). Indices prediction via T ransformer
Text feature Predicted indices  indices to
be
predicted(C). Referenced Codebook 
index value
0
1
2
Codebook
Decoder
Vocoder
(D). W aveform generation with V ocoderCodebook representation Generated mel-spectrogram  
Generated waveform consult
codebookFig. 2 . The main structure of our text-to-audio generation system. There are four major procedures when sampling. The text feature is
extracted from the given sentence through a pre-trained language model in (A). The codebook indices are generated autoregressively given the
text feature in (B). (C) uses a pre-trained codebook to fetch the codebook representation with the indices generated in (B). The representation
is subsequently transformed into a mel-spectrogram, which is turned into a waveform through a vocoder in (D).
2. TEXT-TO-AUDIO GENERATION
Our main objective is to generate audios that can comprehensively
reﬂect the audible elements in the text description given and maintain
high quality and ﬁdelity in the meantime. The following sections
introduce the main parts accordingly, with a framework overview
illustrated in Figure 2.
2.1. Discrete spectrogram codebook of learned representations
As mentioned above, our goal is ﬁrst to construct the audio mel-
spectrogram based on the input text. One straightforward way is
to predict the optimal value for every single pixel in the spectro-
gram. However, the scale of pixels from the raw spectrogram may
be tremendous, making it non-operative for a Transformer to attend.
In order to express the mel-spectrogram more efﬁciently, we use
a discrete codebook to represent the spectrogram. Our approach
refers to a VQGAN to train the codebook, which was initially pro-
posed in [1] and adapted to audio in [4]. VQGAN is an autoencoder
that has a smaller representation size than vector-quantized varia-
tional autoencoder (VQV AE) [19] and is originally meant to decode
an image. Here we adopt VQGAN to reconstruct a given spectro-
gramx2RMTto^x2RMT,Mserves as the number of mel
banks while Tis the time dimension.
During training, the spectrogram is encoded into a smaller ^z2
Rmtnzwith an encoder E, wherenzserves as the dimensional
of codes. The small-scale representation can then be ﬂattened into
anmtsequence to determine the entries to be chosen from the
learned codebookZ=fzlgL
l=1Rnz. We use an element-wise
quantization qto map each individual ^zijin^zinto its closest code-
book entryzl, which can be described as:
zq=q(^z) := arg min
zl2Zk^zij zlk (1)
zqis the quantized representation and can be further decoded into a
spectrogram ^xby a decoder D:
^x=D(zq) =D(q(^z)) (2)
Our goal is to minimize the difference between xand^x.Although the quantization process in Equation (1) is nondiffer-
entiable, according to [20], the gradient for back-propagation in
Equation (2) can be copied to the decoder from the encoder by a gra-
dient estimator. The loss function of Equation (2) can be described
as:
LV Q(E;D;Z) =k^x xk2+ksg[^ z] zqk2
2+ksg[z q] ^ zk2
2
(3)
whereLrec=k^x xk2andsg[]stands for the stop gradient op-
eration that has zero gradient during back-propagation and is a
weighting factor.
VQGAN extents the loss function in Equation (3) with discrim-
inator [21] and perceptual loss [22]. We follow [4] and adapt the
VQGAN on the spectrogram, and replace the original perceptual loss
with a learned perceptual audio patch similarity. The ﬁnal loss func-
tion of Spectrogram VQGAN is:
LSpecV QGAN (fE;D;Zg;C) =LV Q(E;D;Z) +logC (x)
+log(1 C(^x)) +X
s1
MsTskxs ^xsk2
2(4)
Here,Cis a patch-based discriminator and xsand^xsare features
of ground truth and reconstructed spectrogram from the sthscale of
VGGish-ish [4] trained on AudioSet.
2.2. Text feature extraction
Before training a Transformer to sample the codebook indices, the
text description needs to be extracted to provide the necessary infor-
mation to generate an audio clip. The sentence is fed to a BERT -
based [23] model to be transformed into an embedding sequence.
We also use the text encoder from CLIP [24]. Inspired by CLIP,
we also perform audio-text pre-training with the contrastive learn-
ing paradigm. After pre-training, the text encoder is taken as an
advanced feature extractor, which is called CL‘A’P [25]. Similar to
CLIP, within each batch, we take the matching audio-text pairs as
positive samples, while the mismatched pairs as negative samples.
We train the model on an integrated dataset based on currently avail-
able public text-audio dataset (AudioCaps, Clotho, MACS) .A man speaking  as birds are chirping
A man speaking  on a microphone followed by a
crowd of people laughing  then applauding
Original label(s)
related fr om Audioset:
Chirp tweet , 
Speech , 
Animal, 
Outside rural or natural
Applause ,  
Speech  Top 3 pr edictions
from Melception:
Speech: 0.805  
Bird: 0.276
Bird vocalization bird call bird
song: 0.102  
Applause: 0.825
Speech: 0.709
Music: 0.079Fig. 3 . Examples from our text-to-audios generation system. We feed the generated spectrogram into a Melception audio classiﬁer, the event
detection results are listed on the right, along with the referenced ground truth label(s) from original audios in AudioSet.
2.3. Text conditioned generation with Transformer
With the encoder Eand decoder Dtrained in Section Discrete and
the text feature extracted in Section 2.2, we are able to generate a
spectrogram with a latent Transformer. Given the text feature Ftext,
the Transformer learns to predict the distribution of the next indice
siwith the indices s<i. The likelihood of the sequence p(s)is:
p(sjFtext) =Y
ip(sijs<i;Ftext) (5)
The Transformer is trained by maximizing the log-likelihood of
the conditioned sequence:
LTransformer =Exp(x)[ logp(sjFtext)] (6)
During inference, the codebook indices ^siare sampled from the
distribution provided by p. Generated autoregressively until i=
mt, the indices are unﬂattened into shape (m;t)in a column-
major way according to [4] and used to look up the codebook by
replacing each code by its index in codebook Z:
(zq)ij=zlif sij= l (7)
wherei2f1;2;3;:::;mgandj2f1;2;3;:::;tg. By mapping
the indices back to codebook representation ^zq2Rmtnz, the
generated spectrogram can be recovered with decoder D:
^xF=D( ^zq) (8)
3. EXPERIMENTS
3.1. Datasets and evaluation metrics
We conduct our experiments on two datasets: AudioCaps [18] and
Clotho [17], and evaluate the performance objectively and subjec-
tively.
Objective Evaluation We incorporate several metrics to eval-
uate the generation performance. FID [26], PSNR and MMKL are
used to measure the generated spectrogram quality. Besides, a high-
quality audio generated from text should be able to generate back to
captions through an audio captioning model [25]. We calculate the
similarity between the input text and the back-generated one in terms
of SPICE [27] and CIDEr [28] to measure the relevance between the
input text and the generated audio.
Multiclass Melception-based KL-divergence (MMKL), an im-
proved version of MKL, is initially proposed in [4]. The original
classiﬁer Melception is trained on VGGSound with only one sound
label per clip, while most of the sound clips in AudioCaps have more
than two sound classes, we replace the Softmax activation layer withSigmoid and train the Melception using the binary cross entropy loss
on AudioSet for evaluation.
Subjective Evaluation We adopt human evaluation on both the
quality andrelevance of the generated waveforms. For each text
description given, the rater will score on both the relevance and the
quality of the audio generated, with a 0-10 scale by four raters. More
speciﬁcally, ‘relevance’ measures how the audio matches the text de-
scription and ‘quality’ indicates how the sound(s) generated is simi-
lar to sound(s) in real life and how difﬁcult it is to identify them.
3.2. Experimental setup
For the data pre-processing, each audio clip is 10s long of
32000Hz, and is extracted every 12.5ms with a window size of 50ms.
The number of mel banks is 80. We utilize different pre-trained lan-
guage models, including BERT-base-uncased, BERT-medium, CLIP
and CL‘A’P text encoders, to transform the text description into a
feature taken from hidden states. For all text encoders, the maximum
sequence length is set to 30.
When training a codebook, we set the length of the codebook jZj
as 1024. The adversarial part of the loss in Equation (4) is zeroed-
out in the initial stage of the codebook training process to stabilize
the model. For the Transformer part, we use a GPT2-based archi-
tecture [29], with a hidden dimension of 1024. The output of the
Transformer goes through a 1024-way classiﬁer ending with Soft-
max, which learns a distribution over the next codebook indice. Dur-
ing inference, we clip the distribution of the next codebook token to
Top-K probabilities. It provides a control for the sample’s diversity.
K ranges from K = jZj= 1024 to K = 1, here we choose K=512.
The Melception for MMKL is trained on AudioSet and achieves an
mAP of 33.4%. A MelGAN [16] based architecture is used to bridge
the gap between the predicted mel-spectrogram and waveform.
4. RESULTS
In this section, we evaluate the generated audio samples according
to the objective and subjective metrics on AudioCaps and Clotho.
4.1. Quantitative results
AudioCaps Our ﬁnal results are calculated to evaluate the similarity
between the generated and the ground-truth spectrograms, illustrated
in Table 1. Since there is no previous work on text-to-natural audio
generation, we compare our best model (h) with models using differ-
ent text encoders mentioned in Section 2.2. We also alter the length
of the feature sequence fed to the model as previous work suggests
the effectiveness and practicability of reduced features [23]. Current
feature length comparison includes No Feat (0 feature by replac-
ing the text feature with a random embedding), 1 Feat (1 featureTable 1 . The performance of systems under different conﬁgurations.
Condition Text Encoder PSNR" FID# MMKL# SPICE" CIDEr"
(a) No Feat 13.69 0.977 15.80 0.035 0.053
(b) 1 Feat BERT-medium 14.62 0.931 10.98 0.073 0.180
(c) 1 Feat BERT-base 14.78 0.936 10.43 0.075 0.183
(d) 1 Feat CLIP 14.89 0.918 10.00 0.083 0.194
(e) 1 Feat CL‘A’P 14.97 0.784 8.74 0.092 0.246
(f) 30 Feats BERT-medium 14.66 0.858 10.06 0.081 0.190
(g) 30 Feats BERT-base 14.92 0.851 10.05 0.085 0.210
(h) 30 Feats CL‘A’P 14.83 0.778 8.57 0.096 0.249
from the [CLS] token, output embedding from the ﬁrst hidden state)
and30 Feats (full output features). (a) listed our results without
any input condition. We set this as an upper bound for MMKL; (b,
c, d, e) compares on 1 Feat condition, the inﬂuence of different text
feature extractors; similarly, (f, g, h) concentrates on the entire 30
feats with different feature sizes.
Several observations can be extracted from the results: 1) if sam-
pling without input text conditions, the upper bound of PSNR and
MMKL is 13.69 and 15.80, respectively, with the captioning metrics
rather low, in (a); 2) Adding at least one text feature will lead to a
signiﬁcant shift in these two metrics to 14.78 "and 10.43#(c) and
a boost in captioning results, suggesting the importance of the input
text information to the generated audios; 3) An increase in the num-
ber of text features will increase the relevance (lower MMKL) as
well as the overall generation quality (lower FID) (f,g,h). We hence
infer that with more input information, the model is better capable of
differentiating the sound sources in the input sentence and the rela-
tionship. 4) Using text features with smaller feature sizes extracted
from a smaller language model will slightly affect the generation
quality negatively (d,e)(b,c). However, this is not a critical factor
since generating audio content from a sentence focuses most on the
sound classes mentioned and their temporal relations of occurrence.
5) Our Best model is (h) using CL‘A’P, which means with audio-text
contrastive learning, the features extracted are better at representing
audio events from the original text given. Visualized sample spec-
trograms along with their audio tagging results from Melception are
illustrated in Figure 3.
Clotho Generally, the overall audio generation performance on
Clotho, as shown in Table 2, is inferior to the performance on Au-
dioCaps, since the dataset size of Clotho is distinctively smaller than
AudioCaps and the text descriptions in Clotho are more literary.
More complex syntax and terms are utilized in Clotho than Audio-
Caps, making it more difﬁcult for the model to parse and build con-
nections between text and audio. If we use the ﬁrst-stage codebook
trained on AudioCaps and train the Transformer on Clotho, the re-
sult turns better, suggesting a stronger and more effective codebook
is trained on AudioCaps.
Table 2 . Results on Clotho test set, with codebooks trained on two
datasets.
Codebook MMKL# SPICE" CIDEr"
Clotho 11.613 0.049 0.091
AudioCaps 10.988 0.068 0.1374.2. Qualitative analysis
We randomly picked 200 samples from the 964 generated ones from
the AudioCaps test set. For every text description, four audios are
held out to the raters: the original audio from AudioCaps; the gener-
ated audio with no text feature given; audio conditioned on one feat
and on full feats. The averaged results are listed in Table 3.
Table 3 . Human evaluation results. The generated audio clips are
scored on Relevance andQuality with a rating scale of 0 - 10.
Condition Avg. Relevance(/10) Avg. Quality(/10)
Original 9.55 9.55
No Feat 1.85 4.58
BERT-b 5.50 6.08
CL‘A’P 6.79 6.13
It can be observed that both the relevance and the quality of the
generated audios are correlated with the conditioned text features.
With the audio pre-trained CL‘A’P extractor, the auditory feedback
of the generated samples excels the one from other extractors, which
is consistent with the quantitative results listed in the previous sec-
tion. However, a considerable gap is observed between the gener-
ated audio and the original audio clips. After consulting the raters,
we summarized a few limitations of the current audios generated: 1)
different sound classes provided in the text description may occur
alternately, which does not happen commonly in real life; 2) the vol-
ume of different sounds in a single clip may vary greatly; 3) some
rare sound classes are sometimes omitted; 4) the content of the gen-
erated ‘speech’ is slurred with no exact words.
5. CONCLUSION
In this paper, we focus on a novel and challenging task of generat-
ing high-quality yet diverse sound content given a text description.
With a reference codebook trained by VQGAN, our model is able to
reconstruct a mel-spectrogram from a more miniature representation
efﬁciently. A Transformer-based structure is implemented to bridge
the gap between the input text feature and the predicted representa-
tion when sampling. Our best model achieves an average of 14.83,
0.778, and 8.57 in quantitative metrics PSNR, FID, and MMKL, re-
spectively, along with audio caption results (0.096 with SPICE and
0.249 with CIDEr). Human evaluation also afﬁrms the overall qual-
ity and the relevance of the generated audio clips with the given text.
Both results indicate that our proposed approach can generate natural
and diversiﬁed audios that correspond to the information applied in
the text. We also expect further research, including a better chrono-
logical order and duration of the generated sound events.6. REFERENCES
[1] Patrick Esser, Robin Rombach, and Bjorn Ommer, “Taming
transformers for high-resolution image synthesis,” in Proceed-
ings of the IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition , 2021, pp. 12873–12883.
[2] Alex Nichol and et al. Dhariwal, “Glide: Towards photoreal-
istic image generation and editing with text-guided diffusion
models,” arXiv preprint arXiv:2112.10741 , 2021.
[3] Chitwan Saharia, William Chan, Saurabh Saxena, and et al.
Li, “Photorealistic text-to-image diffusion models with deep
language understanding,” arXiv preprint arXiv:2205.11487 ,
2022.
[4] Vladimir Iashin and Esa Rahtu, “Taming visually guided sound
generation,” arXiv preprint arXiv:2110.08791 , 2021.
[5] Kun Su and et al., “Audeo: Audio generation for a silent per-
formance video,” Advances in Neural Information Processing
Systems , vol. 33, pp. 3325–3337, 2020.
[6] Huadong Tan, Guang Wu, Pengcheng Zhao, and Yanxiang
Chen, “Spectrogram analysis via self-attention for realizing
cross-model visual-audio generation,” in ICASSP 2020-2020
IEEE International Conference on Acoustics, Speech and Sig-
nal Processing (ICASSP) . IEEE, 2020, pp. 4392–4396.
[7] Jonathan Shen, Ruoming Pang, Ron J Weiss, Rj Schuster, et al.,
“Natural tts synthesis by conditioning wavenet on mel spec-
trogram predictions,” in 2018 IEEE international conference
on acoustics, speech and signal processing (ICASSP) . IEEE,
2018, pp. 4779–4783.
[8] Naihan Li, Shujie Liu, Yanqing Liu, Sheng Zhao, Ming Liu,
and Ming Zhou, “Close to human quality tts with transformer,”
arXiv preprint arXiv:1809.08895 , 2018.
[9] Yi Ren, Chenxu Hu, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao,
and Tie-Yan Liu, “Fastspeech 2: Fast and high-quality end-to-
end text to speech,” arXiv preprint arXiv:2006.04558 , 2020.
[10] Li-Chia Yang and et al. Chou, “Midinet: A convolutional gen-
erative adversarial network for symbolic-domain music gener-
ation,” arXiv preprint arXiv:1703.10847 , 2017.
[11] Hao-Wen Dong, Wen-Yi Hsiao, Li-Chia Yang, and Yi-Hsuan
Yang, “Musegan: Multi-track sequential generative adversar-
ial networks for symbolic music generation and accompani-
ment,” in Proceedings of the AAAI Conference on Artiﬁcial
Intelligence , 2018, vol. 32.
[12] Bryan Wang and Yi-Hsuan Yang, “Performancenet: Score-to-
audio music generation with multi-band convolutional residual
network,” in Proceedings of the AAAI Conference on Artiﬁcial
Intelligence , 2019, vol. 33, pp. 1174–1181.
[13] Yuki Okamoto and et al. Imoto, “Onoma-to-wave: Environ-
mental sound synthesis from onomatopoeic words,” arXiv
preprint arXiv:2102.05872 , 2021.
[14] Dongchao Yang, Jianwei Yu, Helin Wang, Wen Wang, Chao
Weng, Yuexian Zou, and Dong Yu, “Diffsound: Discrete dif-
fusion model for text-to-sound generation,” arXiv preprint
arXiv:2207.09983 , 2022.
[15] Felix Kreuk, Gabriel Synnaeve, Adam Polyak, Uriel Singer,
Alexandre D ´efossez, Jade Copet, Devi Parikh, Yaniv Taigman,
and Yossi Adi, “Audiogen: Textually guided audio generation,”
arXiv preprint arXiv:2209.15352 , 2022.[16] Kundan Kumar, Rithesh Kumar, and et al. de Boissiere, “Mel-
gan: Generative adversarial networks for conditional wave-
form synthesis,” Advances in neural information processing
systems , vol. 32, 2019.
[17] Konstantinos Drossos, Samuel Lipping, and Tuomas Virtanen,
“Clotho: An audio captioning dataset,” in ICASSP 2020-2020
IEEE International Conference on Acoustics, Speech and Sig-
nal Processing (ICASSP) . IEEE, 2020, pp. 736–740.
[18] Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee, and
Gunhee Kim, “Audiocaps: Generating captions for audios in
the wild,” in Proceedings of the 2019 Conference of the North
American Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, Volume 1 (Long and
Short Papers) , 2019, pp. 119–132.
[19] Aaron Van Den Oord, Oriol Vinyals, et al., “Neural discrete
representation learning,” Advances in neural information pro-
cessing systems , vol. 30, 2017.
[20] Yoshua Bengio and et al. L ´eonard, “Estimating or propagating
gradients through stochastic neurons for conditional computa-
tion,” arXiv preprint arXiv:1308.3432 , 2013.
[21] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros,
“Image-to-image translation with conditional adversarial net-
works,” in Proceedings of the IEEE conference on computer
vision and pattern recognition , 2017, pp. 1125–1134.
[22] Justin Johnson, Alexandre Alahi, and Li Fei-Fei, “Percep-
tual losses for real-time style transfer and super-resolution,” in
European conference on computer vision . Springer, 2016, pp.
694–711.
[23] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova, “Bert: Pre-training of deep bidirectional
transformers for language understanding,” arXiv preprint
arXiv:1810.04805 , 2018.
[24] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh,
Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda
Askell, Pamela Mishkin, Jack Clark, et al., “Learning trans-
ferable visual models from natural language supervision,” in
International Conference on Machine Learning . PMLR, 2021,
pp. 8748–8763.
[25] Xuenan Xu, Zeyu Xie, Mengyue Wu, and Kai Yu, “The
SJTU system for DCASE2022 challenge task 6: Audio cap-
tioning with audio-text retrieval pre-training,” Tech. Rep.,
DCASE2022 Challenge, 2022.
[26] Martin Heusel, Hubert Ramsauer, and et al. Unterthiner, “Gans
trained by a two time-scale update rule converge to a local nash
equilibrium,” Advances in neural information processing sys-
tems, vol. 30, 2017.
[27] Peter Anderson, Basura Fernando, Mark Johnson, and Stephen
Gould, “Spice: Semantic propositional image caption evalu-
ation,” in European conference on computer vision . Springer,
2016, pp. 382–398.
[28] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh,
“Cider: Consensus-based image description evaluation,” in
Proceedings of the IEEE conference on computer vision and
pattern recognition , 2015, pp. 4566–4575.
[29] Klemens Lagler, Michael Schindelegger, Johannes B ¨ohm,
Hana Kr ´asn´a, and Tobias Nilsson, “Gpt2: Empirical slant de-
lay model for radio space geodetic techniques,” Geophysical
research letters , vol. 40, no. 6, pp. 1069–1073, 2013.