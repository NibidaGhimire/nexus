Functional Autoencoder for Smoothing and
Representation Learning
Sidi Wu1, C´ edric Beaulac2, Jiguo Cao1*
1Department of Statistics and Actuarial Science, Simon Fraser University,
Burnaby, BC, Canada.
2D´ epartement de Math´ ematiques, Universit´ e du Qu´ ebec ` a Montr´ eal,
Montr´ eal, Qu´ ebec, Canada.
*Corresponding author(s). E-mail(s): jiguo cao@sfu.ca;
Contributing authors: sidi wu@sfu.ca; beaulac.cedric@uqam.ca;
Abstract
A common pipeline in functional data analysis is to first convert the discretely
observed data to smooth functions, and then represent the functions by a finite-
dimensional vector of coefficients summarizing the information. Existing methods
for data smoothing and dimensional reduction mainly focus on learning the lin-
ear mappings from the data space to the representation space, however, learning
only the linear representations may not be sufficient. In this study, we propose
to learn the nonlinear representations of functional data using neural network
autoencoders designed to process data in the form it is usually collected with-
out the need of preprocessing. We design the encoder to employ a projection
layer computing the weighted inner product of the functional data and functional
weights over the observed timestamp, and the decoder to apply a recovery layer
that maps the finite-dimensional vector extracted from the functional data back
to functional space using a set of predetermined basis functions. The developed
architecture can accommodate both regularly and irregularly spaced data. Our
experiments demonstrate that the proposed method outperforms functional prin-
cipal component analysis in terms of prediction and classification, and maintains
superior smoothing ability and better computational efficiency in comparison to
the conventional autoencoders under both linear and nonlinear settings.
Keywords: Functional data analysis, Neural networks, Nonlinear learning, Functional
principal component analysis
1arXiv:2401.09499v1  [cs.LG]  17 Jan 20241 Introduction
Functional data analysis (FDA) has found extensive application and received growing
attention across diverse scientific domains. Functional data, as the core of FDA, is
defined as any random variables that assume values in an infinite-dimensional space,
such as time or spatial space in theory (Ferraty and Vieu, 2006, Ramsay and Silverman,
2005) , and usually discretely observed at some regularly or irregularly-spaced points
over the time span in applications. Due to the complexity and difficulty in interpreting
and analyzing infinite-dimensional variables, a common pipeline for functional data
analysis (FDA) is to represent the infinite-dimensional functional data, denoted as X(t),
by a finite-dimensional vector of coefficients that extract and summarize the useful
information carried by the individual functions (Yao et al., 2021). These coefficients can
be of interests themselves or be readily utilized in further analysis (Wang et al., 2016a).
Two predominate approaches for dimensional reduction in FDA are basis expansion
and functional principal component analysis (FPCA). The first approach, the con-
ventional basis expansion, represents the functional data as Xi(t) =PMB
m=1cimϕm(t),
where ϕm(t) are known basis functions and cimare corresponding basis coefficients
for the i-th subject containing the information from the original functions (Ramsay
and Silverman, 2005). This method requires the predetermination of a basis system
first, for instance, Fourier or B-spline, and MBand the number of basis functions,
to then learn the functional data’s representation. The second approach, FPCA (Fer-
raty and Vieu, 2006, Ramsay and Silverman, 2005, Sang et al., 2017), being a fully
data-driven approach, compresses the functional data Xi(t) into functional princi-
pal component (FPC) scores ξim=R
{Xi(t)−µ(t)}ψm(t)dt, where µ(t) is the mean
function of the variable X(t), and ψm(t)’s are the FPCs which are also the eigen
functions derived from the spectral decomposition of the variance-covariance function
ofX(t). By Karhunen-Lo` eve expansion, FPCA can construct the functional data as
Xi(t) =µ(t) +PMP
j=1ξijψj(t), with a predetermined proportion of explained variation,
which indirectly defines MP, the number of FPCs identified. The theoretical details and
results on asymptotic distributions were well derived and fully discussed by Dauxois
et al. (1982), Hall and Hosseini-Nasab (2006) and Hall and Hosseini-Nasab (2006).
Representations such as FPC scores have been widely used for establishing func-
tional regression models (M¨ uller and Yao, 2008, Yao et al., 2005b, 2010), clustering
(Chiou and Li, 2007, Peng and M¨ uller, 2008) and classification (M¨ uller, 2005, M¨ uller
and Stadtm¨ uller, 2005) of functional curves. Both aforementioned methods are funda-
mentally linear mappings from infinite-dimensional data to the vector of finite scalars,
however, learning linear projections of functional data might not be sufficient and infor-
mative. Furthermore, FPCA relies on the assumption of a common variance-covariance
of all curves, which might be violated when the individual trajectories are labelled
with classes.
Numerous extensions to the conventional FPCA have been suggested to adapt the
linear representation of functional data for diverse scenarios (Chen and Lei, 2015, Peng
and Paul, 2009, Sang et al., 2017, Yao et al., 2005a, Zhong et al., 2022). Nevertheless,
limited contributions on nonlinear representation learning of functional data can be
found in latest literature. Song and Li (2021) extended the standard FPCA to a
2nonlinear additive functional principal component analysis (NAFPCA) for a vector-
valued functional to accommodate nonlinear functions of functional data via two
additively nested Hilbert spaces. Similar to the linear FPCA with discrete functional
data, however, this technique requires to firstly estimate the underlying X(t) using
the basis expansion or the reproducing Kernel Hilbert space method in the first-level
function space. Chen and M¨ uller (2012) developed nonlinear manifold learning to
generate nonlinear representations of functional data by modifying the existing nonlinear
dimension reduction methods to satisfy functional data settings. The manifold-based
representation is basically designed to be layered on the representation produced by
FPCA, while its computational difficulties may arise as the sample size increases.
Meanwhile, the advent use of big data and gradual popularity of deep learning
promote the introduction of neural networks to functional data representation learning.
Wang and Cao (2023) explored a functional nonlinear learning (FunNoL) method
relying on recurrent neural networks (RNN) to represent multivariate functional data
in a lower-dimensional feature space and handle the missing observations and excessive
local disturbances of observed functional data. This method ignores the basic structure
of functional data as it regards X(t) as time series data and captures the temporal
dependency across time sequences. Moreover, to enable the usage of representation
in classifying curves, FunNoL is designed to be a semi-supervised model that merges
a classification model to a standard RNN, introducing more complexity in network
optimization and representation learning. Hsieh et al. (2021) defined a functional
autoencoders, which generalize the conventional neural network autoencoders to handle
continuous functional data, and derived the functional gradient-based learning algorithm
for autoencoder optimization, to study the nonlinear projection of multidimensional
functional data. This approach requires smooth functional inputs and overlooks the
common issue where functional data are barely fully observed in practice (Yao et al.,
2005a).
The main objective of this study is to propose a solution to the nonlinear repre-
sentation learning and smoothing of discrete functional data using a novel functional
autoencoders (FAE) based on dense feed-forward neural networks, which include FPCA
as a special case under the linear representation setting. As an unsupervised learn-
ing technique, autoencoders (AE) have been frequently used for feature extraction
and representation learning in vector-space problem (Bengio et al., 2013, Hinton and
Salakhutdinov, 2006, Meiler et al., 2001, Wang et al., 2016b). A traditional AE con-
sists of an encoder and a decoder connected by a bottleneck layer, where the former
one is a mapping from a P-dimensional vector-valued input space to a d-dimensional
representation space and the latter one maps from the d-dimensional representation
space back to a vector-valued output space of Pdimensions, where the output layer
consists of a reconstruction of the original input. Assuming d << P , the neurons
in the bottleneck layer serve as a lower dimension representation of the input. This
representation is a collection of neuron features extracted from the AE and can be of
interests themselves or can be used for further research. The relation between AE and
principal component analysis (PCA) has been well discussed in several existing studies.
Oja (1982, 1992) demonstrated that a neural network employing a linear activation
function essentially learns the principal component representation of the input data.
3Furthermore, Baldi and Hornik (1989) and Bengio et al. (2013) demonstrated that an
autoencoder with one hidden layer and identity activation is essentially equivalent to
PCA. Bourlard and Kamp (1988) and Baldi and Hornik (1989) also explained that the
representations captured by autoencoders are a basis of the subspace spanned by the
leading principal components (PCs) instead of necessarily coincident with them. The
connection between the conventional AE and the PCA can be naturally transplanted
to that of the designed FAE and the FPCA with a relevant discussion provided.
In this work, we propose to construct an autoencoder under discrete functional data
settings; a functional autoencoder. We design the encoder to incorporate a projection
layer computing the weighted inner product of the functional data and functional
weights over the observed discrete time span, and the decoder to equip a recovery
layer to map the finite-dimensional vector extracted from the functional data to
functional space using a set of pre-selected basis functions. The developed architecture
compresses the discretely observed functional data to a set of representations and
then outputs smooth functions. The resulting lower-dimensional vector will be the
representation/encoding of the functional data, serving a similar purpose to the basis
coefficients or FPC scores previously mentioned and can be inputted into any further
analysis.
The autoencoder we designed for functional data have at least the following high-
lights. First, the proposed FAE addresses the learning of a nonlinear representation
from discrete functional data with a flexible nonlinear mapping path captured by neu-
ral networks, which eliminates the conduct of curve smoothing assuming any particular
form in advance. As a result, our method performs a one-step model simultaneously
learning the representative feature and smoothing the discretely observed trajecto-
ries. Second, it allows us to obtain linear and nonlinear projections of functional data,
with the former path serving as an alternative approach to FPCA. Third, the pro-
posed method is applicable for both regularly and irregularly spaced data, while the
smoothness of the recovered curves is controlled through a roughness penalty added to
the objective function in training. Forth, the architecture of the FAE is flexibly pro-
grammable and compatible with existing neural networks libraries/modules. Last but
not the least, the robustness and efficiency of our method in representation extraction
and curve recovery with small size of data and substantial missing information are
supported by the results of various numerical experiments.
The remainder of this article proceeds in the following manner. In Section 2, we
provide the methodological details for the proposed FAE, including a description about
the network architecture and an explanation on the corresponding training procedure.
A brief discussion on the connections between the proposed method and two well-
established methods, FPCA and AE, is given in Section 3. In Section 4, we compare
the proposed functional autoencoders to the existing methods applicable for functional
data representation with a focus on relationship capture and computational efficiency
via intensive simulation studies under various scenarios. The designed autoencoders
and other techniques in comparison are further evaluated in Section 5 under a real data
application. Finally, we conclude with a discussion and future directions in Section 6.
The pre-processed data sets and computing codes of the proposed method on selected
applications are available at https://github.com/CedricBeaulac/FAE.
42 Functional autoencoders
2.1 Motivation: autoencoders for continuous functional data
Suppose there are Nsubjects and for the i-th subject, a functional variable Xi(t), t∈ T
is observed in the L2(t) space. To address the limitations of linear representations of
functional data X(t), we propose to learn nonlinear mappings from functional data
space L2(t) toK-dimensional vector space RKthrough a neural network autoencoder
which contains an encoder compressing the functional input to some scalar-valued
neurons, and a decoder reconstructing the functional input back from the encoded
representations.
We introduce an autoencoder with Lhidden layers (excluding the input and output
layers) for continuous functional data X(t), which we suppose, are fully observed over a
continuum t. Different from conventional autoencoders consuming scalar inputs, in this
scenario, functions are served as inputs and fed into the neural network, and the designed
autoencoder for continuous functional data is supposed to be trained by minimizing
the reconstruction error L(X(t),ˆX(t)) =1
NtrainPNtrain
i=1R
T(Xi(t)−ˆXi(t))2dt.
We propose to encode the infinite-dimensional functions to some finite number
of numerical neurons by introducing functional weights wI(t) for bridging the input
and the first hidden layer of the encoder. Specifically, the scalar inner product, which
connects neurons in the input and the first layers of the conventional AE, is generalized
by the inner product of the functional input X(t) and functional weight wI(t) inL2
space. Consequently, the k-th neuron in the first hidden layer h(1)
kis computed as
h(1)
k=gÅZ
TX(t)wI
k(t)dtã
, (1)
where wI
k(t) is the input functional weight connecting the functional input and the
k-th neuron in the first hidden layer, and g(·) is the activation function. To be noted
that here we opted to neglect the numerical bias term b(1)for simplicity.
The proposed functional weights together with the inner product of two functions
achieve the mapping from L2toRK(1), where K(l)is the number of neurons in the l-th
hidden layer and l∈ {1,2, ..., L}. The resulting numerical neurons are further passed to
the continuous hidden layers of the autoencoders, following the same calculation rules as
in conventional AEs. Accordingly, the k-th neuron in the l-th hidden layers is given by
h(l)
k=gÑ
K(l)X
jh(l−1)
jw(l)
jé
, (2)
with hl−1
jbeing the j-th neuron in the l−1 layer connected by the scalar network
weight w(l)
j.
Similarly, a set of functional weights {wO
k(t)}K(L)
k=1, instead of scalar weights, are
applied at the output layer of the decoder to map the second to last layer from RK(L)
back to functional space L2and mathematically, the outputted functional neuron is
5X(t)Input layer ( I)
h(1)
1
h(1)
2
h(1)
K(1)Hidden layer ( l)
w(I)
1(t)
w(I)
2(t)
w(I)
K(1)(t)ˆX(t)Output layer ( O)
w(O)
1(t)
w(O)
2(t)
w(O)
K(L)(t)...
Fig. 1 : Functional autoencoder for continuous data with L= 1 hidden layer.
calculated as
ˆX(t) =K(L)X
k=1h(L)
kw(O)
k(t), (3)
where wO
k(t) is the output functional weight connecting the k-th neuron in the L-th
hidden layer and the functional neuron. For this output layer to produce the functional
output required, the linear activation function must be used.
We name this autoencoder the continuous functional autoencoder (CFAE) and
a graphical visualization of the CFAE with L= 1 hidden layer(s) for functional
data can be seen in Figure 1. In this scenario, {h(1)
1, h(1)
2, ..., h(1)
K(1)}is regarded as the
vector-valued representation of X(t).
2.2 Proposed model: autoencoders for discrete functional data
The CFAE introduced in Section 2.1 serves as an inspiration for the model we proposed
better suited for discrete functional data, which is how functional data are collected and
stored in practical applications. Considering the functional data are discretely observed
atJevenly spaced time points ( t1, ..., t J) over the time interval Tfor all Nsubjects,
and therefore for the i-th subject, we obtain Jpairs of observations {tj, Xi(tj)},
j= 1,2, ...J. As a matter of fact, the real functional data are often contaminated
with some observational errors, resulting in a collection of noisy discrete observations
˜Xi(tj) =Xi(tj) +ϵi(tj), where ϵi(tj) is the i.i.d. measurement error. Without knowing
the true underlying curves X(t)’s, the contaminated observations {˜X(tj)}J
j=1’s are
employed as an alternative to {X(tj)}J
j=1’s in applications.
We propose to adapt the CFAE to take data {X(t1), X(t2), ..., X (tJ)}, aJ-
dimensional vector, as the input. Instead of smoothing the discrete data and then
applying the previously defined autoencoder, we develop the architecture to satisfy
such discrete functional input. This is a major advantage of our proposed method
6as the data no longer needs to be preprocessed in any way before being fed to our
proposed FAE. To do so, we replace the weight functions wI
k(t) for the input layer and
wO
k(t) for the output layer with their discrete versions {wI
k(tj)}J
j=1and{wO
k(tj)}J
j=1
evaluated at the corresponding Jtime points ( t1, ..., t J), respectively. Naturally, the
integralR
TX(t)w(I)
k(t)dtin Eq.(1) is approximated numerically using the rectangular
or trapezoidal rule and accordingly the k-th neuron in the first hidden layer is updated
as
h(1)
k=g JX
j=1ωjX(tj)w(I)
k(tj)!
, (4)
where {ωj}J
j=1are the weights used in the numerical integration algorithm.
Likewise, the output layer now consists of Jneurons corresponding to the J-
dimensional input vector as
ˆX(tj) =K(L)X
k=1h(L)
kw(O)
k(tj). (5)
As illustrated in Figure 2, the mappings L2→RK(1)andRK(L)→L2in the CFAE
are substituted with RJ→RK(1)andRK(L)→RJ, respectively, for this discrete
setting.
The autoencoder for discrete functional data seemingly behaves the same as a
conventional autoencoder, however, the proposed FAE requires a different training
process which accounts for the assumption that functional data are the realization of a
underlying smooth stochastic process. This way, the proposed FAE considers the serial
correlation of the functional data and returns a smooth and continuous functional data
without the need of smoothing in the input preemptively.
We propose to represent functional weight as w(·)
k(t) =PM(·)
k
m=1c(·)
mkϕ(·)
mk(t), where
{ϕ(·)
k(t)}M(·)
k
m=1’s are some known basis functions from a selected basis system, such as
Fourier or B-spline, {c(·)
mk}M(·)
k
m=1are the corresponding basis coefficients remain to be
determined, and M(·)
kis some predefined truncation integer for the k-th weight. The
snapshots of the k-th input or output weight function are accordingly marked as
w(·)
k(tj) =M(·)
kX
m=1c(·)
mkϕ(·)
mk(tj), (6)
and therefore Eq.(4) and Eq.(5) calculating the k-th neurons in the first hidden layer
and the output layer, respectively, can be re-written as
h(1)
k=gÑ
JX
j=1ωjX(tj)M(I)
kX
m=1c(I)
mkϕ(I)
mk(tj)é
7X(t1)
X(t2)
X(t3)
X(tJ)Input layer ( I)
h(1)
1
h(1)
2
h(1)
K(1)Hidden layer ( l)
w(I)
1(t1)
w(I)
1(t2)
w(I)
1(t3)
w(I)
1(tJ)ˆX(t1)
ˆX(t2)
ˆX(t3)
ˆX(tJ)Output layer ( O)
w(O)
1(t1)
w(O)
1(t2)
w(O)
1(t3)
w(O)
1(tJ)
......
...
Fig. 2 : Functional autoencoder for discrete data with L= 1 hidden layer.
=gÑ
M(I)
kX
m=1c(I)
mkJX
j=1ωjX(tj)ϕ(I)
mk(tj)é
, (7)
ˆX(tj) =K(L)X
k=1h(L)
kM(O)
kX
m=1c(O)
mkϕ(O)
mk(tj)
=K(L)X
k=1M(O)
kX
m=1h(L)
kc(O)
mkϕ(O)
mk(tj). (8)
In such a manner, the problem of learning a functional weight w(·)
k(t) using typical
machine learning techniques becomes one of learning {c(·)
mk}M(·)
k
m=1, the parameters defining
w(·)
k(tj), for k= 1, ..., K(l),l= 1orO, and consequently, we seek to learn the coefficients
{c(·)
mk}M(·)
k
m=1through back-propagation.
2.2.1 Encoder with a feature layer
For computational convenience, we let M(I)
k=M(I)andϕ(I)
mk(t) =ϕ(I)
m(t) for all k∈
{1,2, ..., K(1)}, indicating that the input weight functions {w(I)
k(t)}K(1)
k=1are expressed
with the same basis expansion. In consequence, we can simplify Eq. (7) as
h(1)
k=gÑ
M(I)X
m=1c(I)
mkJX
j=1ωjX(tj)ϕ(I)
m(tj)é
8X(t1)
X(t2)
X(t3)
X(tJ)Input layer
f1
f2
f3
f4
fM(I)Feature layer
ϕ(I)
1(t1)
ϕ(I)
1(t2)
ϕ(I)
1(t3)
ϕ(I)
1(tJ)h(1)
1
h(1)
2
h(1)
KFirst hidden layer
c(I)
11
c(I)
21
c(I)
31
c(I)
41
c(I)
M(I)1
...
......
Fig. 3 : Encoder with a Feature Layer. Notice that the input and feature layers are
devoid of parameters at this point and are entirely deterministic given the data and
the choice of basis function for w.
=gÑ
M(I)X
m=1c(I)
mkfmé
, (9)
where fm=PJ
j=1ωjX(tj)ϕ(I)
m(tj), m={1,2, ..., M(I)}, a Riemann sum approximating
the inner product of X(t) and ϕ(I)
m(t).{fm}M(I)
m=1represent the resulting features of
X(t) projected to the basis function sets and serve as the pivot connecting the input
layer and the first hidden layer. Hence, we design our proposed encoder by inserting a
feature layer offm’s between the input layer and the first hidden layer, as shown in
Figure 3. This deterministic layer translates discretely observed functional data into a
scalar structure that can then be processed with existing neural network models and
training algorithms.
Specifically, the input layer and the feature layer are linked by the snapshots of the
basis function set {ϕ(I)
m(tj)}M(I)
m=1, while c(I)
mkbecomes the network weights connecting
thefeature layer and the first hidden layer.
A benefit of this layer architecture is that different observations that might be
observed at different time points will all get converted to the same features. Conse-
quently, irregularly observed data are managed through this deterministic layer. It is
also possible to recover the continuous functional weights {w(I)
k(t)}K(1)
k=1for visualization
purpose after learning the coefficients {c(·)
mk}M(·)
k
m=1through training.
92.2.2 Decoder with a coefficient layer
Again, for simplicity, we set the basis functions to be the same for representing all
output weight functions {w(O)
k(t)}K(L)
k=1by setting M(O)
k=M(O)andϕ(O)
mk(t) =ϕ(O)
m(t)
for all k∈ {1,2, ..., K(L)}. Following on Eq.(8), we now have:
ˆX(tj) =K(L)X
k=1M(O)X
m=1h(L)
kc(O)
mkϕ(O)
m(tj)
=M(O)X
m=1Ñ
K(L)X
k=1h(L)
kc(O)
mké
ϕ(O)
m(tj)
=M(O)X
m=1bmϕ(O)
m(tj), (10)
where bm=PK(L)
k=1h(L)
kc(O)
mk. In fact, {ϕ(O)
m(t)}M(O)
m=1can be regarded as the basis
functions used in the representation of ˆX(t), the reconstructed functional observation.
In turn, {bm}M(O)
m=1play the role of the corresponding basis coefficients.
Hence, in Figure 4, we visualize bm’s as the neurons of a coefficient layer added to
the decoder for connecting the last hidden layer and the output layer, while c(O)
mkare the
network weights between the last hidden layer and the coefficient layer , and meanwhile
thecoefficient layer and the output layer are connected deterministically through
snapshots of the basis functions {ϕ(O)
m(t)}M(O)
m=1. The proposed decoder is essentially
and functionally consistent with NNBR, a neural network designed for scalar input and
functional output, developed by Wu et al. (2023), since both approaches decompress
the scalar-valued basis coefficients to the functional curves in a linear manner, ensuring
the use of backpropagation in model training.
Likewise, an advantage of this layer architecture is that it easily handles irregularly-
spaced as explained in Wu et al. (2023) and provides us with a smooth reconstruction
of the input functional data, one that can be evaluated at any point on the domain thus
effectively smoothing the functional data while simultaneously learning a meaningful
and useful representation.
2.2.3 Training the proposed FAE
A full architecture (with L= 1) of the proposed FAE is displayed in Figure 5. As
detailed in section 2.2.1 and section 2.2.2, a deterministic feature layer of size M(I)is
created to follow the input layer without using any unknown parameters or weights
for neuron calculation, and each neuron in the feature layer produces a scalar value
computed as the numerical approximation of the inner product of the input X(tj)
and the preselected basis function ϕ(I)
m(tj) over the observed timestamp. On the other
end, a coefficient layer ofM(O)scalar-valued neurons is handcrafted as the second to
last layer, and it connects the output layer through the known basis functions ϕ(O)
m(t),
making the output layer also deterministic. Layers between the feature layer and the
10h(L)
1
h(L)
2
h(L)
KLast hidden layer
b1
b2
b3
b4
bM(O)Coefficient layer
c(O)
11
c(O)
21
c(O)
31
c(O)
41
c(O)
M(O)1ˆX(t1)
ˆX(t2)
ˆX(t3)
ˆX(tJ)Output layer
ϕ(O)
1(t1)
ϕ(O)
1(t2)
ϕ(O)
1(t3)
ϕ(O)
1(tJ)
...
......
Fig. 4 : Decoder with a Coefficient Layer. Similarly, the last two layers are devoid of
parameters and are deterministic.
coefficient layer share the same structure as a conventional autoencoder. This specific
structure is the essence of the proposed FAE.
Same as traditional autoencoders, the training process of FAEs comprises of two
components, the forward propagation and the backward propagation , and can be
operated using existing neural network libraries or modules, such as pytorch (Paszke
et al., 2019) and tensorflow (Abadi et al., 2015). The forward propagation has been
previously depicted and is summarized in algorithm 1, here we put emphasize on
thebackward propagation that updates the network parameters using gradient-based
optimizers.
Letθ={c((I)
mk, c(O)
mk, η}denote the collection of network parameters, where ηstands
for all the network weights involved in connecting the hidden layers. The training
process targets at finding ˆθ=argminθL(X(tj),ˆX(tj)), and we employ the standard
mean-square-error (MSE) between X(tj) and ˆX(tj) across all the observed time points
jand subjects iin the training set as the reconstruction error of the FAE, in specific,
L(X(tj),ˆX(tj)) =1
NtrainPNtrain
i=1PJ
j=1(Xi(tj)−ˆXi(tj))2. We design the output layer
of FAE to be a linear combination of some preselected basis functions {ϕ(O)
m}M(O)
m=1and
the neurons {bm}M(O)
m=1outputted by the second to last layer (the coefficient layer ),
and therefore the neuron ˆX(tj) in the output layer of FAE, which is the snapshot
of the reconstructed curve ˆX(t) at time tjis the vector product of {bm}M(O)
m=1and
{ϕ(O)
m}M(O)
m=1evaluated at the specific tj. The linear relation between the coefficient
layer and the output layer, together with the differentials of the known basis functions,
11X(t1)
X(t2)
X(t3)
X(tJ)Input layer
f1
f2
f3
f4
fM(I)Feature layer
ϕ(I)
1(t1)
ϕ(I)
1(t2)
ϕ(I)
1(t3)
ϕ(I)
1(tJ)h(1)
1
h(1)
2
h(1)
KHidden layer
(Representation)
b1
b2
b3
b4
bM(O)Coefficient layer
ˆX(t1)
ˆX(t2)
ˆX(t3)
ˆX(tJ)Output layer
ϕ(O)
1(t1)
ϕ(O)
1(t2)
ϕ(O)
1(t3)
ϕ(O)
1(tJ)
...
......
......
Fig. 5 : A graphical representation of the FAE we propose for discrete functional data.
The model represented only has a single hidden layer h, that serves the role of latent
representation.
Algorithm 1: FAE Forward Pass
Input: X={X(t1), X(t2), ..., X (tJ)}
Output: ˆX={ˆX(t1),ˆX(t2), ...,ˆX(tJ)}
Hyper-parameters: {ϕ(I)
m(tj)}M(I)
m=1,{ϕ(O)
m(tj)}M(O)
m=1,ωjfor all j, a pre-defined
network NN( θ) with Lhidden layers, K(l)neurons in the l-th hidden layer,
activation functions g1, ..., g L,Eepochs, Optimizer (including learning rate ϱ),
etc.
1Input Layer →Feature Layer
{X(tj)}J
j=1→fm=PJ
j=1ωjX(tj)ϕ(I)
m(tj), m∈ {1,2, ..., M(I)}
2Feature Layer →Coefficient Layer
{fm}M(I)
m=1→bm=PKL
k=1c(O)
mkgL
···g1PM(I)
m=1c(I)
mkfm
, m∈
{1,2, ..., M(O)}
Specifically, the k-th neuron in the l-th hidden layer is constructed the same
way as that in conventional neural networks as h(l)
k=gl(PK(l)
k=1h(l−1)
kw(l)),
andw(l)are the scalar network weights.
3Coefficient Layer →Output Layer
{bm}M(O)
m=1→ˆX(tj) =PM(O)
m=1bmϕ(O)
m(tj), j∈ {1, ..., J}
return {ˆX(t1),ˆX(t2), ...,ˆX(tJ)}
12ensure the feasibility of computing the gradient of ( Xi(tj)−ˆXi(tj))2with respect to
the coefficients bmas:
∂L
∂bm=∂L
∂ˆX(tj)∂ˆX(tj)
∂bm. (11)
The gradient with respect to the network weights ηin the remaining layers prior to
thecoefficient layer can be subsequently computed in the backward manner as that in
a classic neural network until reaching the feature layer , while no any further gradient
calculation is made from the feature layer back to the input layer because they are
connected by the predefined input basis functions {ϕ(I)
m}M(I)
m=1, instead of some network
parameters in need of training. Algorithm 2 details the gradients calculating procedure
used to update network parameters in the backpropagation .
Algorithm 2: FAE Backward Pass
Input: θcurrent ,{X(t1), X(t2), ..., X (tJ)},{ˆX(t1),ˆX(t2), ...,ˆX(tJ)}
Output: θupdated
Hyper-parameters: {ϕ(I)
m(tj)}M(I)
m=1,{ϕ(O)
m(tj)}M(O)
m=1,ωjfor all j, a pre-defined
network NN( θ) with Lhidden layers, K(l)neurons in the l-th hidden layer,
activation functions g1, ..., g L,Eepochs, Optimizer (including learning rate ϱ),
etc.
1Compute loss function L(X(tj),ˆX(tj))
2Setθ=θcurrent
3Output Layer →Coefficient Layer
∂L(θ)
∂bm=∂L(θ)
∂ˆX(tj)∂ˆX(tj)
∂bm, because ˆX(tj) =f(bm) and f′(bm) exists
4Coefficient Layer →Feature Layer
∂L(θ)
∂θ, same gradient calculation as used in traditional neural networks.
5Feature Layer →Input Layer
No gradient calculation involved (deterministic operation)
6Update NN parameters θ∗
return θupdated =θ∗
2.3 FAE as a functional data smoother
By design, our proposed FAE outputs a smooth continuous curve over the entire
interval of interest as an estimate of the underlying generative stochastic process for
any input distinctly observed functional data by
ˆX(t) =K(L)X
k=1M(O)X
m=1h(L)
kc(O)
mkϕ(O)
m(t) =M(O)X
m=1bmϕ(O)
m(t), (12)
13which is achievable thanks to the continuity of the preselected basis functions ϕ(O)
m(t)’s.
This is a core design choice made so that the FAE we propose acts not only as a
representation learner but a smoother itself and could substitute other smoothing
processes such as fitting a B-Spline model.
Following the tradition in FDA, we can promote the smoothness of the output
curves by adding a roughness penalty to the objective function of the FAE. With
a consideration for computational simplicity, among different choices of roughness
penalties, we propose to apply the difference penalty on the elements of the coefficient
layer as they act as the basis coefficients in the output functional curves. Consequently,
including such a penalty term leads to the following objective function,
Lpen=1
NtrainNtrainX
i=1Ñ
JX
j=1Ä
Xi(tj)−ˆXi(tj)ä2+λM(O)X
m=3(∆2bim)2é
(13)
and ∆2bm=bm−2bm−1+bm−2, where bimis the m-th neuron in the coefficient layer
for the i-th subject, and parameter λcontrols the smoothness. In implementation, we
suggest applying roughness penalty when the M(O)is relatively large ( M(O)>> J )
and the optimal λcan be selected using cross validation.
2.4 FAE for irregularly-spaced observations
For many existing FDA models, it is quite common to assume that the observed
discrete functional data are regularly spaced. A benefit of our designed FAE is that it
is free of this assumption and its input layer can actually be of flexible size because of
the proposed feature layer applied in the early stages of the model.
As detailed in section 2.2.1, we express the input functional weights w(I)
k(tj) by a
fixed representation ofPM(I)
k
m=1c(I)
mkϕ(I)
mk(tj), and therefore every discrete functional input
Xi(tij), j= 1, ..., J i, where Jivaries with i, are all equivalently projected to the same
basis functions M(I), forming the tij-free features fim=PJi
j=1ωijXi(tij)ϕ(I)
m(tij), m=
{1,2, ..., M(I)}. These M(I)features then participate in the following forward pass
in place of the actual functional inputs Xi(tij) for training the same set of network
parameters including the input weight coefficients c(I)
mk, which are free of i.
The designed feature layer , coupled with the input functional weight representa-
tion, digests the irregular inputs by generalizing the problems of estimating irregular
snapshots of input weight functions to estimating input weight coefficients that are
consistent over all subjects.
3 Connection with existing models
3.1 Relation with FPCA
As previously pointed out by Baldi and Hornik (1989), Bengio et al. (2013), and
Bourlard and Kamp (1988), a single-hidden-layer linear autoencoder with its objec-
tive function being the squared reconstruction error, i.e., L=PNtrain
i=1Xi−ˆXi2
=
14PNtrain
i=1∥Xi−WdWeXi∥2=PNtrain
i=1PP
p=1{Xip−(WdWeXi)p}2, where Xi=
{Xi1, Xi2, ..., X iP},Wd,Wedenote the i-th network input of Pdimensions, weight
matrix of the decoder and weight matrix of the encoder, respectively, is approximately
identical to the conventional PCA, because such an autoencoder is learning the same
subspace as the PCA. More precisely, the unique global minimum of Lis corresponding
to the orthogonal projection of Xonto the subspace spanned by the leading eigenvec-
tors (principal components) of the covariance matrix of X. It is worth mentioning that
at the global minimum, the uniqueness occurs with the global map Wd×We, while the
matrices WdandWemay not be unique. This is because, for multiple appropriate C
we have Wd×We= (WdC)(C−1We). In other words, the mapping Wd×Weis unique
but not the encoder and decoder weight matrices.
When it comes to the functional scenario, a homogeneous relationship exists between
FAE and FPCA. For a single-hidden-layer FAE with linear activation function under
continuous functional data setting, the objective function measuring the mean squared
reconstruction error turns out to be
L=NtrainX
i=1Xi−ˆXi2
=NtrainX
i=1Z
T

Xi(t)−K(1)X
k=1ÅZ
TXi(t)w(I)
k(t)dtã
w(O)
k(t)

2
dt.
(14)
Ramsay and Silverman (2005) concluded that the aforementioned fitting criterion
is minimized when the orthonormal-restricted weight functions w(·)(t) are precisely the
same set of principal component weight functions of the functional data X(t). Hence,
training a one-hidden-layer linear FAE with respect to the squared reconstruction error
criterion and an orthonormal constrain on functional weights is exactly approaching to
project the input X(t) onto the subspace generated by the first functional principal
components, the same space learned by FPCA.
For discrete functional data, the objective model becomes
L=NtrainX
i=1Xi−ˆXi2
=NtrainX
i=11
JJX
j=1

Xi(tj)−K(1)X
k=1 JX
j=1ωjXi(tj)w(I)
k(tj)!
w(O)
k(tj)

2
.
(15)
It is important to notice that this approximation can lead to some difference which
should progressively decreases as Jincreases. Consequently, for relatively large values
ofJ, the FAE optimized by minimizing Eq.(15) will yield functional weights that are
approximately the same as those obtained by minimizing Eq.(14). To put it differently,
when subjected to the orthonormal constraint on the functional weights, the FAE that
minimizes the objective model Eq.(14) is effectively learning the empirical projection of
X(t) onto the same space as FPCA does. Importantly, the proposed FAE with discrete
configuration generalizes FPCA up to a few approximations, and the functional weights
produced by FAE can be identically interpreted as the FPCs in FAE.
153.2 Relation with AE
As pointed out in section 2.2, the proposed FAE is structurally similar to the classic AEs
based on fully connected neural networks. The main difference lies in the first and last
layers. In detail, a classic AE consists of network weights (and bias) free of restrictions
and the training task aims at optimizing these vectors of network parameters. The
FAE we developed also includes such weights to link layers between the feature layer
andcoefficient layer , however, the difference lies in the deterministic weights before
thefeature layer and after the coefficient layer , which are comprised of snapshots of
continuous basis functions. With the goal of optimizing the non-deterministic weights,
the training process of FAE follows the same rule as used in classic AE. The FAE can
be regarded as an extension of AE with some deterministic layers added to both ends
of the network.
The addition of feature layer to the conventional AE architecture enables the FAE
to quickly summarize the underlying temporal relationship among observed time span
into neurons that actually step into the network, resulting in the faster convergence
and better generalization in network training, compared to that of conventional AE.
Meanwhile, thanks to the application of the functional output weights, the FAE we
developed can recover the discrete functional data to smooth curves over a continuous
interval, satisfying the smoothness requirement of functional data, while the classic
AE is limited to output discontinuous functions evaluated at some discrete timestamp
of observations. Additionally, our method is capable to efficiently handle irregularly
spaced functional input along with its underlying correlation in the feature layer by
adjusting the weights ωused for numerical integration calculation, while AE has to take
care of the shortage of observations at some time points by training the model with
some null-valued input for the corresponding neurons in the input layer. The designed
structure benefits our method with better performances in less computational cost
when manipulating irregularity, which is further highlighted by a series of simulation
studies in the following section.
4 Simulation study
In this section, we aim to compare our proposed FAE with the two existing baseline
methods it extends, FPCA and AE respectively, for representation learning and curve
smoothing from discretely observed functional data. We concentrate on investigating
the effectiveness of our method compared to FPCA in capturing the potential nonlinear
relationship as well as evaluating the smoothing ability and computational efficiency
of the FAE compared to AE.
4.1 Simulation setup
4.1.1 Data generation
We generate the data by first sampling a d-dimensional representation Zfrom a
Gaussian mixture model. The mean vector and the covariance matrix of each component
are designed so that components are separable. We then apply a function f(·) that
maps the representations Z, to a set of M-dimensional basis coefficients Bm. Finally,
16we produce the continuous functional data using a linear combination of Mbasis
functions ψm(t)’s and the basis coefficients Bm:
X(t) =MX
m=1Bmψm(t) =f(Z)ψ. (16)
Finally, we evaluate X(t) at some discrete times {t1, t2, ..., t J} ∈[0,1] to obtain a
discrete version of the functional data.
The basis system used ψm(t) are B-spline basis system with an order of 4, and the
number of bases Mvaries throughout experiments. In terms of the mapping function
f(·), we employ a neural network NN(·) with multiple architectures aiming to create
different mapping paths from the representation vector Zto the basis coefficients of the
functional data. The neural network takes the d-dimensional representation vector as
input and outputs the M-dimensional basis coefficients. We apply a simple model with
no hidden layers and linear activation function for linear scenarios, and neural networks
with at least one hidden layer and nonlinear activation functions for nonlinear scenarios.
An optional Gaussian noise can be further added to the discrete functional curve
to mimic observational errors. The component the representation was sampled from is
used as the label for the functional data in classification experiments.
4.1.2 Implementation of models
FPCA linearly encodes functional curves to FPC scores ξim’s with corresponding FPCs
ψm(t)’s. We implement FPCA in python relying on the scikit-fda library (Ramos-
Carre˜ no et al., 2022). The discrete functional data are firstly converted to smooth
functions using basis expansion with customized number of B-spline basis functions,
and then the conventional FPCA is performed on the estimated curve with a user-
defined number of FPC. The resulting FPC scores serve as the scalar representation of
the functional data and are used for further statistical analyses.
AEbased on densely feed-forward-network architecture can learn an encoding from
the functional trajectory observed at discrete time points to a lower-dimensional vector
of representation without considering any temporal correlation among the discrete
observations. We design the input layer of AE to have Jneurons with the j-th neuron
representing the snapshot of the functional curve observed at tj. We adopt different
architecture with a bottleneck hidden layer being the layer producing representation
and attempted with both linear and nonlinear activation functions and initialize the
network weights to random values drawn from N(0, σ). We implement the AE using
thepytorch library.
Lastly, we implement the proposed FAE using the pytorch , coupled with the
scikit-fda library for applying the basis expansion to functional weights. Analogously,
we attempt with different architecture with a hidden layer for extracted representation,
employ both linear and nonlinear activation functions in model training, and initialize
weights randomly by sampling them from a Gaussian distribution N(0, σ).
17Table 1 : Means and standard deviations (displayed inside parentheses)
of prediction error and classification accuracy of functional autoencoder
with Identity activation function (FAE(Identity)), functional autoencoder
with Softplus activation function (FAE(Softplus)) and functional principal
component analysis (FPCA) on 10 random test data sets in Scenario 1.1,
with the best results being highlighted in bold.
FAE
(Identity)FAE
(Softplus)FPCA
MSE p3 Reps 0.0050(0.0001) 0.0045 (0.0005) 0.0052(0.0001)
5 Reps 0.0019( <0.0001) 0.0022(0.0003) 0.0021( <0.0001)
10 Reps 0.0009( <0.0001) 0.0017(0.0005) 0.0010( <0.0001)
Pclassification3 Reps 87.24%(0.93%) 87.72% (1.62%) 87.68%(0.78%)
5 Reps 87.94%(0.81%) 86.53%(0.94%) 89.21% (0.78%)
10 Reps 89.16%(0.75%) 89.61% (0.99%) 89.22%(0.70%)
4.2 Results
Series of simulations are performed under various scenarios to investigate the perfor-
mance of the proposed method in both prediction and classification against those of
FPCA and AE, separately. The prediction error is measured by the mean squared
prediction error (MSE p) averaged across the number of samples and the number of
observed time points in the test set, while the classification accuracy, P classification , is
calculated as the percentage of test observations that can be labelled correctly by a
logistic regression based on the representations extracted. For each scenario, we report
the mean and standard deviation (SD) of the evaluation metrics across all replications.
4.2.1 FAE vs. FPCA
Scenario 1.1 (Linear & Regular) : 6000 discrete functional observations evaluated
at 21 equally spaced points over the interval [0 ,1] are simulated. A five-dimensional
Gaussian mixture model with three components is used to generate the representations
and the resulting functional curves are labelled with class 1, 2 and 3. A neural
network without hidden layers and a linear activation function is performed to map the
representation to the basis coefficients. We employ 8 B-spline basis functions ( M= 8)
along with the aforementioned basis coefficients to express the underlying functional
curves.
We assign 80% of the observations by random to the training set and the remainder
to the test set. The FPCA and two types of FAE are successively trained and the
model details are summarized in Table S1 in the supplementary document.
Scenario 1.2 (Nonlinear & Regular) : We generate 3000 functional observations
discretely measured at 51 equally spaced points over the interval T= [0,1]. We sample
a 5-dimensional representations for each curve from a 3-component Gaussian mixture
model and label the associated functional curves with class 1, 2 and 3. We map the
representations to the basis coefficients using a neural network with one hidden layer
with 20 neurons and Sigmoid activation function. Afterwards, individual functional
18Table 2 : Means and standard deviations (displayed inside parentheses)
of prediction error and classification accuracy of functional autoencoder
with Identity activation function (FAE(Identity)), functional autoencoder
with Sigmoid activation function (FAE(Sigmoid)) and functional principal
component analysis (FPCA) on 10 random test data sets in Scenario 1.2,
with the best results being highlighted in bold.
FAE
(Identity)FAE
(Sigmoid)FPCA
MSE p3 Reps 0.0070(0.0002) 0.0038 (0.0002) 0.0070(0.0002)
5 Reps 0.0035(0.0001) 0.0026 (0.0004) 0.0036(0.0001)
10 Reps 0.0013 (<0.0001) 0.0014( <0.0001) 0.0013 (<0.0001)
Pclassification3 Reps 85.05%(1.08%) 88.68% (1.46%) 85.17%(1.06%)
5 Reps 86.62%(1.06%) 92.42% (1.02%) 86.65%(1.28%)
10 Reps 87.55%(1.13%) 91.20% (1.06%) 87.53%(1.26%)
curve is constructed using 10 B-spline basis functions and the basis coefficients described
above.
We continue to randomly generate training and test sets that contain 80% and
20% observations respectively. Again, we put FPCA, linear FAE and nonlinear FAE
in comparison with model configuration adjusted and detailed in Table S2 in the
supplementary document.
Table 1 and Table 2 summarize the predictive and classification performances of
the proposed FAE and FPCA. In the linear & regular context, we observe that all
three approaches in comparison yield similar performances in both prediction and
classification for most representation attempts.
In contrast, under the nonlinear scenario, both linear FAE (FAE with Identity
activation function) and FPCA generate relatively higher MSE pand lower Pclassification
due to the violation of the linearity assumption. Meanwhile, the FAE with Sigmoid
activation function retains superior performances on both prediction and classification in
comparison to the other linear approaches, with only minimal difference when predicting
with 10 representations, indicating that the nonlinear FAE can more efficiently and
accurately capture and comprise the information carried by the discrete data.
With regard to curve smoothing, as displayed in Figure 6, both FPCA and FAE
produce smooth curves based on the inputted discrete observations, while the designed
FAE demonstrates additional benefits in curve recovery. Plainly, FAE can not only
correctly reconstruct the complete moving trend but also sensitively capture the
individual pop-up variations, e.g. the local ∩-shaped mode appearing in the shaded
interval.
4.2.2 FAE vs. AE
Scenario 2.1 (Nonlinear & Regular) : The simulated data used in the scenario 1.2
in section 4.2.1 is simultaneously applied for a comparison between FAE and AE. Again,
80% of the random observations are assigned to the training set and the remaining
20% to the test set. Given this scenario follows the nonlinear setting, we put emphasize
190.6
0.4
0.2
0.00.20.40.6"Simulated" FPCA
0.0 0.2 0.4 0.6 0.8 1.0FAE(Sigmoid)
0.0 0.2 0.4 0.6 0.8 1.00.8
0.6
0.4
0.2
0.00.20.40.60.8AE(Sigmoid)Fig. 6 : The simulated curves and the curves recovered by functional principal
component analysis (FPCA), classic autoencoder with Sigmoid activation func-
tion (AE(Sigmoid)) and functional autoencoder with Sigmoid activation function
(FAE(Sigmoid)) using 5 representations for a random test set in Scenario 1.2 and
Scenario 2.1.
on the nonlinear models by training the baseline model AE and the proposed FAE
with model configurations listed in Table S3 in the supplementary document.
Table 3 presents the means and SDs of MSE pandPclassification over 10 replicates
trained by AE and FAE in the nonlinear but regularly-spaced-data scenario. We can
observe that the two methods achieved competitive performances in representation
learning, with the conventional AE giving better results on classifying the curves and
the FAE becoming ahead with smaller predictive errors in reconstructing the functional
observations. Figure 6 visualizes the simulated curves, and the full trajectories recovered
by FPCA, nonlinear AE and nonlinear FAE, respectively. It is clearly shown that
the proposed FAE can directly and accurately output smooth curves using the given
discrete observations for the entire domain, while AE is limited to discretely recover
the curve at the time points with available observations, indicating that the FAE
is capable of efficiently capturing the representative information and simultaneously
smoothing the discretely functional observation.
Scenario 2.2 (Nonlinear & Irregular) : In this scenario, we continue to use
the data simulated in scenario 1.2 and randomly remove measurements in 25 time
points (excluding the start and end time point) for each curve to create irregularly
and discretely observed functional data, that is, the resulting functional curve contains
26 irregular observations individually over the domain interval T.
We experiment with two different training set settings: (i) the training set contains
80% observations; (ii) the training set contains 20% data, and focus on a comparison
between the nonlinear AE and FAE with configurations provided in Table S4 in the
supplementary document to examine their performances in handling nonlinearity and
20Table 3 : Means and standard deviations (displayed inside parentheses) of prediction error and classification
accuracy of functional autoencoder with Sigmoid activation function (FAE(Sigmoid)) and classic autoencoder
with Sigmoid activation function (AE(Sigmoid)) on 10 random test data sets in Scenario 2.1, with the better
results being highlighted in bold.
FAE (Sigmoid) AE (Sigmoid)
3 Reps 5 Reps 10 Reps 3 Reps 5 Reps 10 Reps
MSE p 0.0038 (0.0002) 0.0026 (0.0004) 0.0014 (<0.0001) 0.0046(0.0005) 0.0030(0.0005) 0.0124(0.0069)
Pclassification 88.68%(1.46%) 92.42% (1.02%) 91.02%(1.06%) 89.35% (1.39%) 92.75% (1.15%) 92.65% (1.81%)
21irregularity simultaneously. For those time points without observations (randomly
removed), we feed the corresponding neurons in the input layer of AE and FAE with
0 and abandon those neurons when computing the loss. When training FAE, we
also adjust the weights {ωj}Ji
j=1individually for each discrete curve ifor a numerical
integration over all the observed timestamp.
The performances of prediction and classification of nonlinear AE and nonlinear
FAE trained with 80% and 20% irregularly-spaced functional data are illustrated in
Table 4 and Table 5, separately, with the performances of both models reported for
each thousand epochs. We can see that proposed FAE shows more advantages in
speedily learning the representation and accurately capturing the information for both
prediction and classification, especially when the training epochs remain small. On
the other hand, the classic AE needs to gradually master the mapping path in respect
of reconstruction error, while its resulting representations can outperform those by
FAE in classification when the training cost increases. The visual comparisons of how
the mean MSE pand mean Pclassification of FAE and AE changes with the number of
training epochs for 80% and 20% training sizes, corresponding to Table 4 and Table 5,
are presented in Section S1.2 of the supplementary document. As demonstrated, the
computational efficiency of the FAE is robust over different representation numbers,
which further confirms that the FAE is able to generalize better and converge faster
even with fewer epochs and larger batch size over traditional AE that has similar
architecture in the matter of curve reconstruction and unsupervised representation
learning for classification.
Apart from representation learning, we display the simulated irregularly-spaced
functional segments, along with the full curves reconstructed by the nonlinear AE and
nonlinear FAE in Figure 7 to reveal the smoothing ability of the FAE. When training
with 80% observations, it is not surprising to observe that the proposed FAE oversteps
the classic AE by generating predominantly smooth curves that effectively capture
the entire underlying patterns and primary modes present in the originally observed
data. Nevertheless, trajectories obtained through AE exhibit noticeable oscillations and
incoherence with numerous accidents protrudes appearing across the entire domain. In
the case of training with only 20% data, as expected, the FAE continues its dominance
by showing dramatically leading performances in curve smoothing, highlighting that the
FAE with specially designed architecture is able to efficiently learn the representations
and simultaneously smooth the unequally-spaced and noisy functional data, even with
limited training information.
5 Real application
To further demonstrate the effectiveness of our method, in this section, we perform the
proposed FAE, together with the conventional FPCA and the classic AE on the El
Ni˜ no data set which is available in Rpackage rainbow (Shang and Hyndman, 2019).
This data set catalogs the monthly sea surface temperatures originally observed in 4
different locations from January 1950 to December 2006. The temperature curves were
discretely measured at 12 evenly spaced time points over the entire interval for every
year. We treat the measurements of each calendar year as an independent observation
22Table 4 : Means and standard deviations (displayed inside parentheses) of prediction error and classification accuracy of
functional autoencoder with Softplus activation function (FAE(Softplus)) and classic autoencoder with Softplus activation
function (AE(Softplus)) on 10 random test data sets when training with 80% irregularly observed data in Scenario 2.2,
with the better results being highlighted in bold.
FAE (Softplus) AE (Softplus)
3 Reps 5 Reps 10 Reps 3 Reps 5 Reps 10 Reps
MSE pepochs=1000 0.0031(0.0003) 0.0023(0.0002) 0.0014(0.0002) 0.0035(0.0002) 0.0029(0.0003) 0.0143(0.0127)
epochs=2000 0.0023(0.0001) 0.0015( <0.0001) 0.0010( <0.0001) 0.0034(0.0003) 0.0044(0.0059) 0.0103(0.0102)
Pclassificationepochs=1000 86.57%(1.08%) 87.85%(2.03%) 89.22%(1.17%) 89.85%(1.32%) 91.05%(0.69%) 90.58%(1.59%)
epochs=2000 88.67%(1.22%) 90.12%(1.70%) 91.75%(1.10%) 90.68%(1.30%) 91.03%(1.09%) 90.73%(1.66%)
23Table 5 : Means and standard deviations (displayed inside parentheses) of prediction error and classification accuracy of
functional autoencoder with Softplus activation function (FAE(Softplus)) and classic autoencoder with Softplus activation
function (AE(Softplus)) on 10 random test data sets when training with 20% irregularly observed data in Scenario 2.2, with
the better results being highlighted in bold.
FAE (Softplus) AE (Softplus)
3 Reps 5 Reps 10 Reps 3 Reps 5 Reps 10 Reps
MSE pepochs=1000 0.0057 (0.0009) 0.0041 (0.0009) 0.0039 (0.0026) 0.0386(0.0152) 0.0730(0.0237) 0.4591(0.2692)
epochs=2000 0.0046 (0.0011) 0.0030 (0.0004) 0.0025 (0.0007) 0.0194(0.0094) 0.0464(0.0266) 0.3579(0.2449)
epochs=3000 0.0035 (0.0003) 0.0027 (0.0003) 0.0019 (0.0004) 0.0104(0.0027) 0.0230(0.1578) 0.1917(0.0934)
epochs=4000 0.0031 (0.0002) 0.0021 (<0.0001) 0.0029 (0.0039) 0.0086(0.0012) 0.0093(0.0037) 0.0968(0.0632)
epochs=5000 0.0029 (0.0002) 0.0019 (0.0001) 0.0015 (0.0004) 0.0094(0.0010) 0.0070(0.0015) 0.0588(0.0434)
Pclassificationepochs=1000 78.32%(1.10%) 81.59% (2.12%) 82.30% (2.84%) 78.71% (2.97%) 80.48%(2.30%) 64.36%(5.26%)
epochs=2000 81.40%(2.00%) 83.86%(1.17%) 83.50% (1.20%) 85.30% (0.82%) 86.16% (1.39%) 80.63%(6.87%)
epochs=3000 84.09%(1.06%) 85.75%(1.24%) 85.27%(1.02%) 86.70% (1.11%) 87.50% (1.57%) 88.61% (1.29%)
epochs=4000 85.05%(0.72%) 86.69%(1.26%) 87.17%(1.04%) 87.18% (1.27%) 88.03% (1.17%) 90.05% (0.65%)
epochs=5000 85.53%(0.94%) 87.63%(1.26%) 88.27%(1.34%) 87.50% (1.25%) 88.23% (0.86%) 89.98% (0.63%)
240.75
0.50
0.25
0.000.250.500.75"Simulated"
0.75
0.50
0.25
0.000.250.500.75FAE(Softplus)
0.0 0.2 0.4 0.6 0.8 1.01
0123AE(Softplus)
0.6
0.4
0.2
0.00.20.40.6"Simulated"
1.0
0.5
0.00.51.0FAE(Softplus)
0.0 0.2 0.4 0.6 0.8 1.03
2
1
012AE(Softplus)Fig. 7 : The simulated irregularly-spaced curves and the curves recovered by classic
autoencoder with Softplus activation function (AE(Softplus)) and functional autoen-
coder with Softplus activation function (FAE(Softplus)) using 5 representations for a
random test set in Scenario 2.2, when training with 80% observations (left panel) and
20% observations (right panel), respectively.
of the true underlying functional curve (varying with time), resulting in a total of 267
observations, and we label the 4 locations with numbers from 1 to 4 randomly. To avoid
poor random initialization and obtain stable training process for the NN-based methods,
we centre the data by subtracting the sample mean curve across all observations, and
a visualization of the centered sea surface temperature curves is provided by Figure 8.
We continue to compare the proposed method with two benchmark models, FPCA
and classic AE, on their performances in terms of curve reconstruction and repre-
sentation extraction. We equip the classic AE and the proposed FAE with different
combinations of hyper-parameters for a linear mapping path and a potential nonlinear
mapping pattern, while FPCA is performed with the focus on measuring the linear
relationship. The hyper-parameters for all models in comparison are tuned in advance
to yield a fair improvement in their performances during actual training. To reduce
the computational cost of tuning process, for each model, we fix the number of rep-
resentations to be 5 and then perform a grid search over the hyper-parameters of
our interests with respect to the loss function by simply building a model for each
possible combination of all of the hyper-parameter values provided, and the optimal
model architecture combination of hyper-parameters identified by the grid search with
5 representations is further applied to model training with 3 and 8 representations. In
the supplementary document, Table S5 provides a summary of the candidate values
considered in hyper-parameter tuning for all models, while the details of the optimally
252 4 6 8 10 12
Month6
4
2
024Sea Surface TemperatureFig. 8 : Centered monthly sea surface temperature measured in the “Ni˜ no region”
defined by the coordinates 0-10 degree South and 90-80 degree West.
identified configurations for models in comparison is narrated in Table S6. We proceed
with 20 repetitions of random subsampling validation: randomly dividing the data set
into a training set and a test set, with 80% and 20% of the total samples assigned
to them, respectively. We evaluate the prediction, classification and clustering perfor-
mance of the proposed methods on the 20 test sets of all the mentioned models using
3, 5 and 8 representations, respectively.
Table 6 summaries the performances of all the methods applied for different
numbers of representation extracted on curve prediction and classification, using MSE p
and P classification averaged over 20 random test sets. Apparently, the proposed FAEs
consistently and comprehensively outperforms the FPCA and the AE models in terms
of both reconstruction and classification, by reaching the lowest prediction error and
the highest classification accuracy for all representation attempts. With regard to the
predictive performance, the linear FAE retains to be the top performer, closely followed
by the nonlinear FAE. On the other hand, the nonlinear FAE continuously oversteps
the other models in the competition of classifying curves, exhibiting its advantages in
extracting more informative representations. To further confirm this in the context
of statistical significance, we conduct two-sided paired t-tests to compare the MSE p
and P classification of the 20 replicates of the nonlinear FAE to those of the FPCA, and
the corresponding p-values are reported in Table S7 in the supplementary document.
We observe that the nonlinear FAE remains superior to the FPCA regarding both
evaluation metrics, especially when the representation size increases, demonstrating the
importance and necessity of the proposed FAE in nonlinear representation learning.
The other highlight of the proposed FAE is its capability of smoothing the originally
discrete data. As illustrated in Figure 9, the trajectories recovered by using FAE are
smooth over the entire domain due to the fact that they are constructed as the linear
combination of the neurons in the coefficient layer and the basis functions that can be
26Table 6 : Means and standard deviations (displayed inside parentheses) of prediction error and
classification accuracy of functional autoencode Identity (FAE(Identity)) and Sigmoid activation
function (FAE(Sigmoid)), classic autoencoder with Identity (AE(Identity)) and Sigmoid activation
function (AE(Sigmoid)) and functional principal component analysis (FPCA) on 20 random test
sets with the El Ni˜ no data set.
FAE (Identity) FAE (Sigmoid) AE (Identity) AE (Sigmoid) FPCA
MSE p3 reps 0.0616(0.0051) 0.0582 (0.0045) 0.0619(0.0051) 0.0715(0.0079) 0.0656(0.0054)
5 reps 0.0211 (0.0023) 0.0226(0.0031) 0.0261(0.0052) 0.0329(0.0042) 0.0242(0.0031)
8 reps 0.0062 (0.0009) 0.0089(0.0014) 0.0064(0.0008) 0.0071(0.0021) 0.0113(0.0013)
Pclassification3 reps 76.88%(4.01%) 77.68% (5.07%) 76.52%(3.67%) 77.14%(6.05%) 77.59%(4.81%)
5 reps 85.18%(4.86%) 86.52% (4.46%) 84.20%(5.15%) 85.71%(3.48%) 84.38%(5.20%)
8 reps 85.89%(4.58%) 87.59% (4.67%) 85.27%(3.91%) 85.80%(3.89%) 84.81%(4.50%)
276
4
2
024"Observed" FPCA
2 4 6 8 10 126
4
2
024AE(Sigmoid)
2 4 6 8 10 12FAE(Sigmoid)
MonthSea Surface TemperatureFig. 9 : The observed curves and curves recovered by functional principal component
analysis (FPCA), classic autoencoder with Sigmoid activation function (AE(Sigmoid))
and functional autoencoder (FAE(Sigmoid)) with 5 representations for a test set of El
Ni˜ no data set.
evaluated at any point within the interval of observation. On the contrary, the classic
AE can only achieve point-wise prediction at the timestamp with actual observations,
resulting in visible discontinuity in the curve reconstruction. Meanwhile, FPCA can
also produce smooth prediction but it usually requires the discrete observation to be
firstly smoothed.
In addition, FAE surpasses AE by fast converging to a similarly low prediction
error but with a resulting higher classification accuracy in both linear and nonlinear
scenarios, as displayed in Figure 10 and Figure 11, demonstrating its high efficiency
in extracting meaningful features and potential advantage in saving computational
cost. It is noteworthy that the model configuration for the nonlinear AE is simpler
than that of the nonlinear FAE, which brings benefits to the speed of nonlinear AE in
training loss convergence.
The given results might imply that, for the El Ni˜ no data, the true relationship
between the functional space to representation space for the sea temperature curves
can be more accurately learned and revealed through a nonlinear mapping path, with
the resulting nonlinear representations carrying more valuable information ready for
further statistical analysis.
6 Concludsion
In this work, we introduced autoencoders with a new architecture design for discrete
functional observations targeting at unsupervised representation learning and direct
curve smoothing concurrently. The deterministic feature layer added to the encoder
280 1000 2000 3000 4000 5000
Epochs0.050.100.150.200.250.300.35Prediction ErrorAE
FAE
0 1000 2000 3000 4000 5000
Epochs78.0%79.0%80.0%81.0%82.0%83.0%84.0%85.0%Classification Accuracy
AE
FAEFig. 10 : How the averaged prediction error and classification accuracy of functional
autoencoder (FAE) and classic autoencoder (AE) with Identity activation function
using 5 representations on 20 random test sets of the ElNino data set change with the
number of epochs.
0 1000 2000 3000 4000 5000
Epochs0.020.040.060.080.100.12Prediction ErrorAE
FAE
0 1000 2000 3000 4000 5000
Epochs78.0%80.0%82.0%84.0%86.0%Classification Accuracy
AE
FAE
Fig. 11 : How the averaged prediction error and classification accuracy of functional
autoencoder (FAE) and classic autoencoder (AE) with Sigmoid activation function
using 5 representations on 20 random test sets of the ElNino data set change with the
number of epochs.
reduces the computational effort and enhances the model robustness in learning perfor-
mances, while the additional coefficient layer similarly incorporated into the decoder
allows the usage of back-propagation in model training and allows the decompression
from scalar neurons to functional curve in a linear manner. Additionally, we imple-
mented our proposed FAE in a way that accommodates both regularly and irregularly
functional data with flexible necessity on the size of training data for achieving satisfac-
tory performances. Through simulation studies and real applications, we demonstrated
that the method we proposed is superior to other linear representation method for
functional data, i.e., FPCA, under nonlinear scenarios and retains competitive perfor-
mances in linear cases. Moreover, the numerical experiments endorse that our model
29can be a dramatic improvement over the classic AE in terms of computational efficiency
by generalizing and converging rapidly even with reduced training observations and
limited computing time.
Nevertheless, the developed method depends on numerous hyper-parameters, includ-
ing number of hidden layers, number of neurons in each hidden layer, training optimizer,
etc., and unfortunately conducting a grid search on that space can be time-consuming.
It is necessary to bring up that the performance of the FAE varies from one hyper-
parameters configuration to another, and the randomness in initializing network
parameters will introduce more variance to the results across training replicates. In
contrast, the FPCA can be effortlessly fitted with only one hyper-parameter neces-
sitating predetermination, but in sacrifice of the ability to accurately capturing the
learning maps in nonlinear situations. Another weakness of our approach is its inability
to process multidimensional functional data in its current form. Therefore, in a future
work we could extend the current network architecture to a more dynamic architecture
allowing discrete multivariate functional data. This might be achieved by introducing
micro-neural networks (Lin et al., 2014, Yao et al., 2021) to replace the neurons in the
feature layer and the coefficient layer . Furthermore, an analogous architecture of our
proposed FAE can be implemented to tackle nonlinear functional regression problems
with both a functional predictor and a functional response.
References
Mart´ ın Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig
Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghe-
mawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing
Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dande-
lion Man´ e, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster,
Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent
Vanhoucke, Vijay Vasudevan, Fernanda Vi´ egas, Oriol Vinyals, Pete Warden, Martin
Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale
machine learning on heterogeneous systems, 2015. URL https://www.tensorflow.org/.
Software available from tensorflow.org.
Pierre Baldi and Kurt Hornik. Neural networks and principal component analysis:
Learning from examples without local minima. Neural Networks , 2(1):53–58, 1989.
Yoshua Bengio, Aaron C. Courville, and Pascal Vincent. Representation learning: A
review and new perspectives. IEEE Transactions on Pattern Analysis and Machine
Intelligence , 35(8):1798–1828, 2013.
Herv´ e Bourlard and Yves Kamp. Auto-association by multilayer perceptrons and
singular value decomposition. Biological Cybernetics , 59(4):291–294, 1988.
Dong Chen and Hans-Georg M¨ uller. Nonlinear Manifold Representations for Functional
Data. The Annals of Statistics , 40(1):1–29, 2012.
30Kehui Chen and Jing Lei. Localized functional principal component analysis. Journal
of the American Statistical Association , 110(511):1266–1275, 2015.
Jeng-Min Chiou and Pai-Ling Li. Functional Clustering and Identifying Substructures
of Longitudinal Data. Journal of the Royal Statistical Society Series B: Statistical
Methodology , 69(4):679–699, 2007. doi:10.1111/j.1467-9868.2007.00605.x.
Jacques Dauxois, Alain Pousse, and Yves Romain. Asymptotic theory for the principal
component analysis of a vector random function: Some applications to statistical
inference. Journal of Multivariate Analysis , 12(1):136–154, 1982.
Fr´ ed´ eric Ferraty and Philippe Vieu. Nonparametric Functional Data Analysis: Theory
and Practice . Springer-Verlag, New York, 2006.
Peter Hall and Mohammad Hosseini-Nasab. On properties of functional principal
components analysis. Journal of the Royal Statistical Society Series B: Statistical
Methodology , 68(1):109–126, 2006.
Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data
with neural networks. Science , 313(5786):504–507, 2006.
Tsung-Yu Hsieh, Yiwei Sun, Suhang Wang, and Vasant Honavar. Functional
Autoencoders for Functional Data Representation Learning , pages 666–674. 2021.
Min Lin, Qiang Chen, and Shuicheng Yan. Network in network. arXiv , 2014.
doi:10.48550/arXiv.1312.4400.
Jens Meiler, Michael M¨ uller, Anita Zeidler, and Felix Schm¨ aschke. Generation and
evaluation of dimension-reduced amino acid parameter representations by artificial
neural networks. Molecular Modeling Annual , 7(9):360–369, 2001.
Hans-georg M¨ uller. Functional modelling and classification of longitudinal data.
Scandinavian Journal of Statistics , 32(2):223–240, 2005.
Hans-Georg M¨ uller and Ulrich Stadtm¨ uller. Generalized Functional Linear Models.
The Annals of Statistics , 33(2):774–805, 2005. doi:10.1214/009053604000001156.
Hans-Georg M¨ uller and Fang Yao. Functional additive models. Journal of the American
Statistical Association , 103(484):1534–1544, 2008.
Erkki Oja. Simplified neuron model as a principal component analyzer. Journal of
Mathematical Biology , 15:267–273, 1982.
Erkki Oja. Principal components, minor components, and linear neural networks.
Neural networks , 5(6):927–935, 1992.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban
31Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan
Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith
Chintala. Pytorch: An imperative style, high-performance deep learning library. In
Advances in Neural Information Processing Systems 32 , pages 8024–8035. Curran
Associates, Inc., 2019.
Jie Peng and Hans-Georg M¨ uller. Distance-based clustering of sparsely observed
stochastic processes, with applications to online auctions. The Annals of Applied
Statistics , 2(3):1056–1077, 2008.
Jie Peng and Debashis Paul. A geometric approach to maximum likelihood estimation
of the functional principal components from sparse longitudinal data. Journal of
Computational and Graphical Statistics , 18(4):995–1015, 2009.
Carlos Ramos-Carre˜ no, Jos´ e Luis Torrecilla, Yujian Hong, and Alberto Su´ arez. scikit-
fda: Computational tools for machine learning with functional data. In 2022 IEEE
34th International Conference on Tools with Artificial Intelligence (ICTAI) , pages
213–218, 2022. doi:10.1109/ICTAI56018.2022.00038.
James O. Ramsay and Bernard W. Silverman. Functional Data Analysis (Second
Edition) . Springer, New York, 2005.
Peijun Sang, Liangliang Wang, and Jiguo Cao. Parametric functional principal
component analysis. Biometrics , 73(3):802–810, 2017.
Hanlin Shang and Rob Hyndman. rainbow: Bagplots, Boxplots and Rainbow Plots for
Functional Data , 2019. URL https://CRAN.R-project.org/package=rainbow. R
package version 3.6.
Jun Song and Bing Li. Nonlinear and additive principal component analysis for
functional data. Journal of Multivariate Analysis , 181:104675, 2021. ISSN 0047-259X.
Haixu Wang and Jiguo Cao. Functional nonlinear learning. Journal of Computational
and Graphical Statistics , 2023. doi:10.1080/10618600.2023.2233581.
Jane-Ling Wang, Jeng-Min Chiou, and Hans-Georg M¨ uller. Functional data analysis.
Annual Review of Statistics and Its Application , 3(1):257–295, 2016a.
Yasi Wang, Hongxun Yao, and Sicheng Zhao. Auto-encoder based dimensionality
reduction. Neurocomputing , 184:232–242, 2016b.
Sidi Wu, C´ edric Beaulac, and Jiguo Cao. Neural networks for scalar input and functional
output. Statistics and Computing , 33(5):118, 2023.
Fang Yao, Hans-Georg M¨ uller, and Jane-Ling Wang. Functional data analysis for
sparse longitudinal data. Journal of the American statistical association , 100(470):
577–590, 2005a.
32Fang Yao, Hans-Georg M¨ uller, and Jane-Ling Wang. Functional Linear Regression
Analysis for Longitudinal Data. The Annals of Statistics , 33(6):2873–2903, 2005b.
Fang Yao, Yuejiao Fu, and Thomas C. M. Lee. Functional Mixture Regression.
Biostatistics , 12(2):341–353, 2010. ISSN 1465-4644.
Junwen Yao, Jonas Mueller, and Jane-Ling Wang. Deep learning for functional data
analysis with adaptive basis layers. In International Conference on Machine Learning ,
pages 11898–11908. PMLR, 2021.
Rou Zhong, Shishi Liu, Haocheng Li, and Jingxiao Zhang. Functional principal compo-
nent analysis estimator for non-gaussian data. Journal of Statistical Computation
and Simulation , 92(13):2788–2801, 2022.
33