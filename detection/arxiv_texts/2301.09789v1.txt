A Qualitative Study on the Implementation Design
Decisions of Developers
Jenny T. Liang
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA, USA
jtliang@cs.cmu.eduMaryam Arab
Department of Computer Science
George Mason University
Fairfax, V A, USA
marab@gmu.eduMinhyuk Ko
Department of Computer Science
Virginia Tech
Blacksburg, V A, USA
minhyukko@vt.edu
Amy J. Ko
Information School
University of Washington
Seattle, WA, USA
ajko@uw.eduThomas D. LaToza
Department of Computer Science
George Mason University
Fairfax, V A, USA
tlatoza@gmu.edu
Abstract ‚ÄîDecision-making is a key software engineering skill.
Developers constantly make choices throughout the software
development process, from requirements to implementation.
While prior work has studied developer decision-making, the
choices made while choosing what solution to write in code
remain understudied. In this mixed-methods study, we examine
the phenomenon where developers select one speciÔ¨Åc way to
implement a behavior in code, given many potential alternatives.
We call these decisions implementation design decisions . Our
mixed-methods study includes 46 survey responses and 14 semi-
structured interviews with professional developers about their
decision types, considerations, processes, and expertise for imple-
mentation design decisions. We Ô¨Ånd that implementation design
decisions, rather than being a natural outcome from higher
levels of design, require constant monitoring of higher level
design choices, such as requirements and architecture. We also
show that developers have a consistent general structure to their
implementation decision-making process, but no single process is
exactly the same. We discuss the implications of our Ô¨Åndings on
research, education, and practice, including insights on teaching
developers how to make implementation design decisions.
Index Terms ‚Äîimplementation design decisions, software de-
sign
I. I NTRODUCTION
Making decisions effectively is a crucial skill for software
engineers [1]. One reason is because making explicit and
rationalized design decisions during the design process im-
proves software design quality [2]. Developers make these
explicit decisions throughout the software design process, from
requirements and architecture to implementation. These deci-
sions in turn have downstream effects on the software, such as
inÔ¨Çuencing how easily developers comprehend a codebase [3]
or resulting in systems that are difÔ¨Åcult to maintain [4], [5].
At higher levels of software design, developers make ex-
plicit decisions about the software architecture by prototyping
them at the whiteboard [6] or documenting them in UML
diagrams [7]. Researchers have developed an understanding
of how developers make architectural decisions [8], [9], even
ImplementationArchitectureRequirementsConsideration: What third-party libraries can I take dependencies on?
!Decision: How should I represent my matrix data‚ÄîPython arrays, C++ arrays, or third-party libraries?Action: Prioritizing testability as a requirement after learning third-party libraries are hard to unit test while prototyping. Consideration: How important is memory for my use case?
Action: Researching what similar modules or classes already use in their implementations.Class/ModuleFig. 1. An example of an implementation design decision. Developers
consider aspects of software design that are above the implementation (e.g.,
requirements, architecture, or class and modules) to make these decisions.
building several tools to aid this process [10], [11]. At the
code level, developers also make explicit decisions, such as
implementing speciÔ¨Åc design patterns [12], [13]. Yet, the
decisions that developers make while choosing what solution
to write in code remains understudied. We call these decisions
implementation design decisions .
Implementation design decisions are when developers se-
lect one speciÔ¨Åc way to implement a behavior, given many
potential alternatives. They may choose an implementation
that minimizes the time to market in place of producing high-
quality, robust software. Meanwhile, developers may choosearXiv:2301.09789v1  [cs.SE]  24 Jan 2023to implement solutions for other reasons: system requirements
(e.g., performance), code quality (e.g., readability), or conve-
nience (e.g., ease of implementation).
Figure 1 includes an example of an implementation design
decision. A developer is considering three different ways to
represent their matrix data in a script‚ÄîPython arrays, C++
arrays, or using a third-party library. If the developer wanted
to minimize runtime, they could use C++ arrays or third-party
libraries. If they wanted a simple, easy-to-read solution for
their teammates, they could opt for Python arrays.
Understanding implementation design decisions can provide
insights on what decisions result in good or bad software
designs, which could be taught to novice developers. Fur-
thermore, studying these decisions can elicit a broader set of
considerations developers are optimizing for. This could help
explain the decisions that are on its face sub-optimal but are
in fact necessary (e.g., decisions causing technical debt).
In this work, we study implementation design decisions
with a focus on the explicit decisions made by developers, as
conscious reasoning in software design improves the design
quality [2]. We address the following questions:
RQ1 : What implementation design decisions do software
developers make?
RQ2 : What considerations do software developers have
while making implementation design decisions?
RQ3 : What process do software developers follow to
make implementation design decisions?
RQ4 : Which types of developer expertise are described
in the implementation decision-making process?
To answer these questions, we designed a mixed-methods
study using surveys and interviews to understand implemen-
tation design decisions. Our study had 53 participants who
program professionally. We Ô¨Ånd that implementation design
decisions demanded careful thought. Overall, they require
constant evaluation of higher levels of design and could even
exert inÔ¨Çuence on them (see Figure 1), which corroborates
Ô¨Åndings from prior work [14]. This is in contrast to other
work that characterizes implementation as a natural result
from higher level forms of design in a top-down fashion. For
example, Perry and Wolf described implementation as a code-
level representation satisfying requirements, architecture and
design [15]. Thus, our study supports the theory that problems
and solutions in software design co-evolve with one another‚Äî
as the solution develops, the problem space can update [8].
We also show that developers have a general structure to
their decision-making process, but each developers‚Äô process
is unique. Finally, we provide implications in how our results
can be applied in research, education, and practice.
II. R ELATED WORK
A. Implementation Design Decisions
Ralph and Tempero studied the characteristics of decisions
made while programming [14]. They elicited common themes
in these decisions, such as their considerations and the ap-
proaches taken to solve a problem. Our study examines asimilar phenomenon as Ralph and Tempero. It extends this
work by also investigating the systematic processes developers
use to make such decisions, as well as the types of expertise
they apply in making them.
Prior research has also investigated a speciÔ¨Åc outcome from
a particular subset of implementation design decisions in
depth‚Äîtechnical debt. These occur when developers choose
implementations that bias time to market over software qual-
ity [16]. Technical debt comes in several forms, such as ar-
chitectural, structural, documentation, test, infrastructure, and
requirements debts [4], [5]. Technical debt has many negative
downstream effects, such as reducing developer productiv-
ity [17] as well as decreasing team morale, causing delays, and
lowering code quality [18]. Technical debt can be caused for
many reasons, including planning and management (e.g., dead-
lines), development issues (e.g., not adopting good practices),
software engineering processes (e.g., a lack of documentation),
a lack of knowledge (e.g., lack of experience), human factors
(e.g., lack of commitment), organizational factors (e.g., lack
of trained professionals), external factors (e.g., pressure), and
infrastructure (e.g., unavailable infrastructure) [5], [19].
Implementation design decisions, such as ones that result in
technical debt, are concerned with decisions that developers
make when they are about to write an implementation. They
consider different ways a behavior may be implemented in
code. Thus, prior work in this space largely focuses on the
far-reaching effects of implementation design decisions, rather
than the decisions themselves.
B. Decision-Making in Software Engineering
Developers make decisions across several types of activi-
ties, such as in project planning and veriÔ¨Åcation [20]. These
decisions require developers to make tradeoffs by reasoning
about future outcomes [21]. Prior work in software engineering
expertise also suggests that knowing how to effectively make
decisions is a form of expertise [1], [22], [23].
Requirements engineering activities can be framed as
decision-making processes at the organizational and individual
levels [24]. While deciding on requirements, software devel-
opers make decisions on which requirements to prioritize [25].
In software architecture, developers make decisions to se-
lect between candidate architectures [26], [27] and integrate
components into an existing system [20]. van Vliet et al. argue
that software architecture can be viewed as a set of design de-
cisions made by software architects [8]. Additionally, software
architecture decisions use both slow, rational and fast, intuitive
thinking and can be prone to cognitive biases [8], [9]. Zannier
et al. found which of the two thinking approaches designers
used depended on how structured the problem was [28].
Software developers also make decisions in API design.
Stylos and Myers outlined the space of design decisions for
APIs, such as design patterns to use or what Ô¨Åelds or methods
to provide. These decisions were split across the architectural,
structural, class, and language levels [29]. In order to make
proper API design decisions, software practitioners and orga-nizations have outlined several guidelines [30], [31] and online
resources [32] on how to design APIs.
Overall, prior work has largely studied developers‚Äô decision-
making in speciÔ¨Åc development contexts or at higher levels
of design, such as requirements or architecture. Our work
diverges from this literature by studying the decisions made
while developers are about to write implementations in code.
C. Software Design Practices
Many studies have examined how software developers do
software design. Petre and van der Hoek studied the individual
practices of software designers [33], such as involving experts
from outside the team to learn domain-speciÔ¨Åc knowledge.
Practitioners have also written resources on writing code with
clean software design (e.g., [34], [35]) and architecture (e.g.,
[36], [37]) with prescriptions on how to make design decisions
effectively, such as following well-known programming prin-
ciples like SOLID. In open-source software, contributors make
design decisions on bug reports [38] and online discussions on
GitHub issues [39], [40].
For software design at higher levels of abstraction, devel-
opers are often known to perform design activities at the
whiteboard, creating visual diagrams to support the design
process [6], [41]. Software developers can informally denote
their designs using UML [7] or sketches [42]. Prior work
has also investigated the process software engineers follow
while doing early stage design work. Sharif et al. found that
while designing, developers engage in activities which overlap
with the traditional software development process, namely
requirements, analysis, design, and implementation [43].
Thus, prior research has investigated the practices develop-
ers follow to design software. Instead of studying developers‚Äô
actions, our work contrasts prior literature by studying the
decision-making and reasoning of developers.
III. S TUDY DESIGN
To answer the research questions, we collected data on
software developers‚Äô implementation design decisions from 53
study participants (Section III-A) using surveys (Section III-B)
and interviews (Section III-C). We then analyzed the data
using qualitative coding (Section III-D). The breadth of the
survey data and the depth of the interview data corroborated
our Ô¨Åndings from multiple sources.
A. Participants
To elicit a wide range of developers‚Äô insights on im-
plementation design decisions, we recruited developers with
diverse programming experiences. Our inclusion criteria was
developers who contributed their programming expertise to at
least one project in a professional setting.
1) Sampling strategy: We recruited participants with dif-
ferent levels of software engineering experience, technology
expertise, job titles, and engineering team sizes. The authors
then released a survey (Section III-B) on their personal social
media accounts for recruitment. Social media posts displayedbrief descriptions about implementation design decisions, the
estimated time of completion, and a survey link.
We increased sample coverage by also recruiting on Reddit.
The Ô¨Årst author advertised the survey in a text post on 10 popu-
lar developer-centric Reddit communities. As of January 2022,
the subreddits had between 6,681 members to 291,170 mem-
bers. Reddit communities were selected by technology (e.g.,
r/reactjs, r/php), geographic location (e.g., r/developersIndia),
or role (e.g., r/dataengineering). Posts were only published
after receiving permission from the community‚Äôs moderators,
as stipulated by the institutional review board. The Reddit
recruitment posts introduced implementation design decisions,
provided an estimated time of completion, explained why the
subreddit was a good Ô¨Åt, and linked the study. We also relied
on snowball sampling by encouraging interview participants to
share the study to others. In the survey, participants indicated
if they wanted to participate in a follow-up interview.
In total, 60 developers agreed to participate in the study,
with 53 who met the inclusion criteria and participated in the
study. 46 participants completed the survey and 14 participants
completed an interview.
2) Demographics: We report interviewee demographics in
Table I. In our study, participants represented diverse geo-
graphic locations, including the Americas ( n= 27 ), Africa
(n= 1), Asia ( n= 7), Europe ( n= 14 ), and Oceania ( n= 3).
Multiple genders were also represented, such as man ( n= 44 ),
woman ( n= 7 ), and non-binary ( n= 1 ). Participants
had job titles such as junior, senior, or principal software
engineer; Chief Technology OfÔ¨Åcer; software architect; ma-
chine learning engineer; and research engineer. Participants
reported contributing between 1 to over 1,000 projects, with a
median of 25. Participants worked in companies of varying
sizes, whose engineering organization sizes ranged from 1
to 36,000, with a median of 18. Participants reported using
a variety of technologies (e.g., Java, MongoDB, Angular, R,
Verilog, Jupyter notebooks) and working on diverse problem
domains (e.g., deployment infrastructure, IoT, static analysis
tools, Ô¨Ånancial technology, healthcare, online exams).
B. Survey
We designed a 15-minute Google Forms survey on examples
of implementation design decisions and considerations and
distributed it to developers using the sampling strategy from
Section III-A.
1) Design: In the survey, we Ô¨Årst presented participants
with the deÔ¨Ånition and three examples of implementation
design decisions. We then asked participants to provide exam-
ples of implementation design decisions and considerations,
limiting examples to the past Ô¨Åve days in order to reduce
memory bias. These questions had a response length minimum
of 30 characters to encourage participants to record sufÔ¨Åciently
detailed answers. Following best practices, we used the HCI
Guidelines for Gender Equity and Inclusivity [44] to collect
gender information. We allowed participants to select multiple
responses for questions on gender. The full survey instrument
is available in our supplemental materials [45].TABLE I
OVERVIEW OF INTERVIEW PARTICIPANTS . ‚ÄúS OFTWARE ENGINEER ‚ÄùIS
ABBREVIATED AS ‚ÄúSWE‚Äù.
ID Job title#
projectsOrg.
sizeGender Location
P4Senior Principal
Engineer50+ 1,700 ManUnited
Kingdom
P6Chief Technol-
ogy OfÔ¨Åcer30 16 Man Germany
P7ScientiÔ¨Åc SWE
II20 200 WomanUnited
States
P15Head of
ResearchDozens 12 Man Israel
P18 Principal SWE 50 1,000 ManUnited
States
P22Chief Technol-
ogy OfÔ¨Åcer10 6 Man Philippines
P28Data Platform
Engineer5-6 500 Man Australia
P29 Data Engineer 15-20 12 Woman Germany
P31Senior SWE /
Analyst200+ 9 ManUnited
States
P32 SWE 4 20,000 WomanUnited
States
P34 Senior SWE 20 50 ManUnited
States
P50Technical Direc-
tor; Senior SWE25+ 10 ManUnited
Kingdom
P51Small Business
Owner100‚Äôs 2 Man Canada
P52 SWE 50 2 ManUnited
States
2) Piloting: Following best practices for experiments with
human subjects in software engineering [46], we conducted
pilots of the survey to identify and reduce confounding factors.
We piloted drafts of the survey with four software developers
to clarify wording and updated the survey after each round
of feedback. To ensure data quality, the survey was deployed
publicly, piloted, and updated on the Ô¨Årst 15 survey responses.
C. Interviews
We conducted 45-minute interviews via online conference
calls to gather examples of implementation design decisions
and considerations as well as developers‚Äô decision-making
process. Interviews were recorded and transcribed. Recordings
were destroyed after transcription. Interview participants were
compensated with a $30 USD Amazon.com gift certiÔ¨Åcate.
1) Design: Topics in the interview included implementation
design decisions and considerations participants made in the
past Ô¨Åve days, as well as written explicit programming strate-
gies on how participants made their decisions. We collected
written explicit programming strategies as a structured format
to capture developers‚Äô processes and knowledge. An example
of an explicit programming strategy from our study is in
Figure 2; we report all collected strategies in our supplemental
materials [45].Explicit programming strategies are ‚Äúhuman-executable pro-
cedure[s] for accomplishing a programming task‚Äù [47]. In this
study, explicit programming strategies represent the process
software developers used to make implementation design deci-
sions. We collected them since developers can follow system-
atic processes to make some design decisions, such as selecting
between multiple software architecture alternatives [26], [27].
Additionally, developer expertise and processes can also be
externalized by developers via explicit programming strate-
gies [48], which allowed us to elicit software developers‚Äô
decision-making processes in interviews.
2) Protocol: Two authors conducted the interviews: one to
execute the protocol and one to record the participant‚Äôs explicit
programming strategy. Having a interviewer experienced in
strategy authoring to write strategies ensured strategy writing
quality, as authored strategies could be ambiguous or struggle
to generalize [48]. During the interview, we reminded par-
ticipants of the deÔ¨Ånition of implementation design decisions
in a Google Slides presentation, using similar wording and
examples from the survey. Next, we collected implementation
design decisions and considerations. Finally, for as many
design decisions that time allowed for, we extracted an explicit
programming strategy the participant used to make their deci-
sion. The interview protocol is available in our supplemental
materials [45].
To extract programming strategies, the participant explained
their decision-making process step-by-step. Interviewers trans-
lated this to an explicit programming strategy in a shared
Google Document to reduce the participant‚Äôs cognitive load
while recalling their process, as strategy authoring is cog-
nitively demanding [48]. The participant then reviewed the
strategy and provided corrections or feedback. Because au-
thored strategies may omit details that prevents the strategy
from being usable [48], the participants elaborated on edge
cases and updated their strategy accordingly to increase its
robustness. Since authored strategies‚Äô scope may be too nar-
row [48], interviewees updated the strategy to be general
enough for a similar problem. After reviewing the strategy,
the participant edited wording and clarity so the strategy was
at a quality that could be released publicly. Following prior
work [49], we documented a brief description of the strategy;
tools, technologies, and prior knowledge necessary to use the
strategy; and the steps of the strategy. The authors updated the
strategy for consistency, but participants could edit the strategy
upon request. Participants also could add additional data (e.g.,
comments, visual media), to explain their process.
3) Piloting: Following best practices in experiments with
human subjects in software engineering [46], we piloted the
interview to identify and reduce confounding factors. The Ô¨Årst
author ran the interview protocol on one author and three de-
velopers and updated the protocol based on their feedback. The
purpose of the pilots was two-fold: 1) to improve the clarity
of the interview protocol and 2) validate the hypothesis that
developers had systematic processes to make implementation
design decisions and further, could articulate their processes.
We found that all pilot participants could recall and articulatestrategies for making implementation design decisions.
Use this when: Using less common features in libraries instead of
using the popular functions
Tools/technologies: StackOverÔ¨Çow, Google, continuous learning
Prior knowledge: Common design patterns, popular libraries
1)Decide what the goal of the program is.
2)Begin writing the program.
3)While writing the program, search online whether other li-
braries support your use case. Use your prior knowledge or
colleagues to help guide your search.
4)Choose a library which meets your use case. This can be based
on the popularity of the library with respect to the language.
5)Look at the features of the library and test the ones that you‚Äôre
interested in on small examples. Get a feel of the library and
select a solution which achieves the desired behavior.
6)If you have code that works, show the solution to another
individual for review.
Fig. 2. An example of an explicit programming strategy from the study.
User-generated content is written in italics .
D. Analysis
To analyze the collected data, we used qualitative coding.
Weopen coded the data for RQ1 ,RQ2 , and RQ3 to sum-
marize the data on various aspects of implementation design
decisions as this phenomenon is understudied. We close coded
RQ4 to situate the strategies with prior work on software
engineering expertise, which has been well-studied (e.g., [1],
[22], [23], [50], [51]).
1) Open Coding: Foropen coding , we followed best prac-
tices by Hammer and Berland [52], which outlines proce-
dures on interpreting coding results and reporting coding
disagreements. We treated generated codes as tabulated claims
about the data that could be investigated in future work. We
checked the reliability of the coding by resolving disagree-
ments, Ô¨Årst by discussing any disagreements and then coming
to an agreement as a group. Finally, we interpreted coding
disagreements as coding variance and reported the content of
the disagreements.
We followed best open coding practices [53], preparing
separate documents for each coder for qualitative analysis,
taking care to remove the prompts from the responses. In these
documents, survey responses and interview responses were
stored separately and analyzed independently, as the data was
collected in different contexts. We also shufÔ¨Çed all the rows in
the data to remove any ordering effects. Finally, we removed
data collected from piloting.
Open coding occurred in multiple phases. In the Ô¨Årst
phase, three authors separately reviewed the responses and
inductively generated codes for each dataset. Each response
was labeled with zero or more codes. Each code was given
a unique identiÔ¨Åer and a brief description. To aggregate the
codes, the authors compared their separately generated codes
and identiÔ¨Åed codes with similar concepts. These codes were
merged under a single code and copied to a shared codebook.
For the remaining codes, the authors discussed instances ofdisagreement and resolved them by unanimously agreeing to
add or remove the code in the shared codebook. Disagreements
were most frequently the result of differing scopes of codes,
rather than the meaning of the participants‚Äô statements. Some
disagreements arose due to an author not coding a part of
the response another author did. In the second round, the
authors applied the shared codebook to the original data. If
there were multiple datasets to analyze for the same research
question, each dataset was coded using the aforementioned
process. Then, the resulting codebooks were merged by the
authors. The authors identiÔ¨Åed codes with similar deÔ¨Ånitions
and added them to a Ô¨Ånal codebook with a unique identiÔ¨Åer.
The remaining unmerged codes were automatically added to
the Ô¨Ånal codebook. The authors performed a third round of
coding with the Ô¨Ånal merged codebook. For RQ3 , the authors
then applied pattern coding to the Ô¨Ånal codes to group the
codes into broader categories [54]. To do this, the authors
placed each code into an initial category by unanimously
agreeing to put it in a category or create a new one. Then,
they reviewed each category and unanimously Ô¨Ånalized its
scope and, if necessary, moved the codes between categories
to reÔ¨Çect the new scope.
2) Closed Coding: For closed coding , the Ô¨Årst author
identiÔ¨Åed a codebook to code the dataset with. For RQ4 , we
used Table 3 and Table 5 from Li et al. [1], which respectively
contains codes on attributes of expert developers‚Äô decision-
making processes and their software and designs. The three
authors deductively applied the codes to the dataset. Each
instance was labeled with zero or more codes. Next, they
reconvened to discuss instances of disagreement and resolved
them by unanimously agreeing on which codes were to be
applied. In this step, disagreements arose due to the scope of
the code rather than the meaning of the statements.
3) Data: ForRQ1 , we analyzed 82 examples of imple-
mentation design decisions from the survey and interviews.
ForRQ2 , we analyzed 113 examples of considerations from
survey and interview data as well as implementation design
decisions from interviews, since participants mentioned con-
siderations in context of their decisions. For RQ3 andRQ4 ,
we analyzed a dataset with 99 steps from the 16 collected
strategies.
In addition to extracting action codes for RQ3 , we analyzed
the programming strategies on the decision-making process as
a sequence of action codes and categories. We did this by
replacing each step of the strategy with its respective code
or category. If a step had multiple codes or categories, we
represented the step as a sequence of codes or categories in
the order they were mentioned. If a code or category occurred
consecutively, they were reduced to a single occurrence. We
include these representations of strategies in the supplemental
materials [45].
IV. R ESULTS
We report the results to our research questions below. Due to
space constraints, we only discuss codes we found interesting
with respect to prior work.TABLE II
THE TYPES OF IMPLEMENTATION DECISIONS MADE BY SOFTWARE DEVELOPERS . CODES DISCUSSED IN DETAIL ARE UNDERLINED .
Code & Description Representative Quote
Alternatives ‚ÄîDeciding what high-level approaches to use to address a
particular problem.‚ÄúSo we are ingesting data. One way...is using Python scripts... The other
option that we looked into was getting it via third party tools...the third
was just outsourcing it.‚Äù (P28)
Behaviors ‚ÄîDeciding the program speciÔ¨Åcation: what parameters to set for
a program and their types, what outputs the program should give, and the
behavior of the program.‚Äú[The API] would expect to take in the input data type, which is this union
of Xarray, Numpy, Dask, all of these supported data types...‚Äù (P7)
Data ‚ÄîDeciding how to manage data within software: what data should be
handled in a program, how it should be represented, and how it should be
interacted with.‚ÄúI opted to represent the tree as an ancestry string of the top slash the next,
[and] the next. And then you can use ‚Äòlike‚Äô with a wild card and you‚Äôll
get the subtree.‚Äù (P31)
Code constructs ‚ÄîDeciding which programming language constructs to use
within a program.‚ÄúMaking a change to a Python program, I removed an indexing expression
(val = x[0] ) and replaced it with a destructuring assignment ( val,
__ = x )‚Äù. (P18)
Structure ‚ÄîDeciding how to organize the codebase, where Ô¨Åles should lie,
and how code should be modularized.‚ÄúI‚Äôm going to refactor this to bring out the bits of logic that pertain to...my
bit of the business, so that I can then later have ownership of it...rather
than [having a] big monolithic system.‚Äù (P4)
Programming languages, APIs, services ‚ÄîDeciding the programming lan-
guages, APIs, or third-party services to use in the software system or script.‚ÄùI used Golang to handle a large amount of JSON Ô¨Åles that would‚Äôve taken
too long to handle in Python.‚Äù (P20)
Automation ‚ÄîDeciding whether to implement a technology solution from
scratch.‚ÄúI could have manually typed in the new kinds of records that I wanted in
production...but instead I encoded that all in a formalized runnable script.‚Äù
(P32)
Reuse ‚ÄîDeciding whether code should be reused and to what extent it
should be general enough to be extended to different scenarios.‚ÄúMerge two C# applications (FTP and SFTP server) into one, in order to
reuse Ô¨Åle tree state, user authentication, and so on.‚Äù (P45)
Updates ‚ÄîDeciding whether to update the software or not.‚ÄúDo I tell them I can‚Äôt Ô¨Åx the problem or do I go in and tempt small
solutions, just to get it to function for a few days...or do I completely
write my own Ô¨Åx?‚Äù (P51)
Prior work (e.g., [15], [55]) characterizes implementation
as naturally arising from higher levels of design. In contrast,
we found that implementation design decisions involved a
constant top-down and bottom-up dialogue between imple-
mentation and higher levels of design, such as requirements
and architecture. This supports the view that problems and
software implementations co-evolve with one another [8].
Consider Figure 1 as an example. When a developer decides
whether their matrix-based data should be represented using
native Python arrays, C++ arrays, or a third party library
(e.g., Numpy), they may consider the architecture by thinking
about what languages or third-party libraries are compatible
with their system. They may also consider non-functional
requirements (e.g., memory or performance) or look to how
other similar modules address this problem. Finally, they may
even change the priority of certain requirements, such as
testability, after realizing the difÔ¨Åculty of prototyping unit tests
with a third party library.
A. What implementation design decisions do software devel-
opers make? ( RQ1 )
Participants described 8 different types of implementation
design decisions (see Table II). Each code appeared more than
once in our data. All codes appeared in all datasets.
a) Behaviors: Participants decided on how the software
should behave, such as its inputs, outputs, and what should
occur during execution. This was an informal version of the re-
quirements elicitation, analysis, and validation processes [56],
[57]. These decisions often required a change in requirements,which corroborates the viewpoint that decisions about the so-
lution may change requirements [8]. For instance, participants
decided on entire method speciÔ¨Åcations when requirements
were under-speciÔ¨Åed and made changes to the program be-
havior to handle certain requirements:
/quote_left‚ÄúBut, I decided to record each line of the CSV Ô¨Åle as a record
with a header record...So on that header record, I record who
gave me the Ô¨Åle and when it was given to me. I could reproduce
the CSV Ô¨Åle from what I‚Äôm storing. ‚Äù (P31)
b) Code constructs: Consistent with prior work [14], par-
ticipants debated which programming constructs to use (e.g.,
loop constructs, ternary operators, pointers). These decisions
were the lowest level decisions made. Even at this level,
participants still considered requirements (e.g., performance):
/quote_left[Sometimes] I would rather go for efÔ¨Åciency or performance
[but when]..I am working with other developers, I‚Äôm more leaning
to having the code more readable... Instead of functional program-
ming mapreduce I go for loop, so that the other developers can
understand the code itself. . ‚Äù (P22)
c) Updates: Participants deliberated whether to make
speciÔ¨Åc code changes (e.g., Ô¨Åxing a defect), as it could
have unwanted effects. They modeled potential outcomes‚Äî
a decision-making attribute of developers [1]. These decisions
at times required considering the software architecture, such
as while deciding whether to update dependencies:
/quote_leftThe library that we use...didn‚Äôt compile anymore... We have two
[options]. One is to say, ‚ÄòOkay, we freeze the library version... ‚Äô
and then we postpone the solution of the problem. Or we look at
the problem and Ô¨Åx it immediately... ‚Äù (P6)TABLE III
SOFTWARE DEVELOPERS ‚ÄôCONSIDERATIONS FOR IMPLEMENTATION DECISIONS . CODES DISCUSSED IN DETAIL ARE UNDERLINED .
Code & Description Representative Quote
Community Support ‚ÄîHow well-supported by a developer community a
technology is.‚ÄúI wanted to use PHP 8.1, but there is still no general support...‚Äù (P54)
Features ‚ÄîThe features a technology contains.‚ÄúOpen source C# MailKit was selected over builtin SmtpClient to...allow
Ô¨Çexible email body manipulation.‚Äù (P37)
Popularity ‚ÄîThe number of users that use the technology or library.‚Äú[Laravel SPATIE Media Library] being very well understood by the rest
of the Laravel developer community.‚Äù (P52)
Reliability ‚ÄîHow reliable and correct the software is. ‚ÄúCorrectness [with] concurrent updates and...mutable objects.‚Äù (P44)
Security ‚ÄîHow secure software is; robustness of software to adversarial
attacks.‚ÄúEach decision had tradeoffs...[in] security (the latter being exposing,
possibly private, brands.)‚Äù (P24)
Maintainability ‚ÄîHow easily maintenance actions (e.g., Ô¨Åxing defects,
updating components) can be performed on software.‚ÄúOver-engineering a system that may...add cognitive overhead to any
maintenance.‚Äù (P39)
Testability ‚ÄîHow easily software can be tested (e.g., unit tests). ‚ÄúTestability (functional is almost always easier to test).‚Äù (P27)
Extensibility ‚ÄîHow easily the code can be extended to accommodate
changes (e.g., new features).‚ÄúSo how the structure and application itself is laid out so that it‚Äôs not going
to be a pain to work with, as we expand it.‚Äù (P50)
Performance ‚ÄîPerformance aspects of the code (e.g., runtime, memory). ‚ÄúThe function call is expensive in certain...circumstances.‚Äù (P18)
Reproducibility ‚ÄîWhether code is able to reproduce the same output, given
the same input.‚ÄúDoes the code do it in an idempotent way? So...the same input would do
the same output regardless of how many times you do it.‚Äù (P22)
Requirements ‚ÄîThe requirements of the software; customer needs.‚ÄúAfter the Ô¨Årst implementation, a new requirement came so structuring for
reuse [was] useful. ‚Äù (P15)
Future Requirements ‚ÄîRequirements or customer needs that may or may
not occur in the future.‚ÄúWill there be a need to run the pipeline in parallel some day (like on
Spark or Dask)?‚Äù (P27)
Skills ‚ÄîThe current skills of the team or of the developer.‚ÄúWe also chose PHP because...more developers familiar with the PHP
framework than with Python frameworks.‚Äù (P56)
Budget ‚ÄîAmount of resources (e.g., time, money) available to implement
the software project.‚ÄúBecause we were on a tight deadline...I decided to just process [the data]
all on my local machine...and then upload it.‚Äù (P57)
Reusing Resources ‚ÄîReusing existing resources (e.g., code, practices). ‚ÄúWhat parts of the code will they reuse?‚Äù (P32)
DifÔ¨Åculty ‚ÄîHow much effort completing the implementation will be. ‚ÄúThe implementation difÔ¨Åculty comes into [these decisions].‚Äù (P28)
Readability ‚ÄîHow easily code syntax is read by a developer. ‚ÄúGenerics in code may be harder to comprehend for some...‚Äù(P42)
Code Cleanliness ‚ÄîThe quality of the implementation‚Äôs code; how easy it
is to onboard other developers and make updates.‚ÄúTry not to make ravioli code where we have too many modules that do
little things.‚Äù (P42)
Simplicity ‚ÄìThe length or complexity of the implementation.‚ÄúI did this to keep my pull request shorter and closer to the original code.
Less to read means faster code review.‚Äù (P18)
Consistency ‚ÄîBeing consistent with the code style of the programming
language or code base.‚ÄúSo, not only is it existing code that‚Äôs already there. I don‚Äôt want to be the
person to introduce something different.‚Äù (P34)
System Fit ‚ÄîHow well the implementation Ô¨Åts in with an existing code
base or system.‚ÄúWhere to set up the event subscription... [In] a React component or outside
of the React/Redux content...‚Äù (P24)
Data ‚ÄîHow data in the system will be managed or handled.‚ÄúThis is a trade-off of having less-fresh data, with being more robust in
the event the 3rd party is unavailable.‚Äù (P31)
Impacts ‚ÄîThe impacts that the implementation may cause.‚ÄúI want to be very safe when making potentially-impactful changes in
production environments.‚Äù (P59)
Users ‚ÄîThinking about collaborators who will be working in the code base;
the usability of the software for end-users.‚ÄúIt‚Äôs trying to make the code...understandable for the other developers for
maintenance purposes and if they need to upgrade the code...‚Äù (P22)
Documentation ‚ÄîWriting documentation for the implementation. ‚ÄúSo that‚Äôs a signiÔ¨Åcant documentation...cost.‚Äù (P50)
B. What considerations do software developers have while
making implementation design decisions? ( RQ2 )
Participants described 25 distinct considerations while mak-
ing implementation design decisions. We report them in
Table III. Each code appeared more than once‚Äî17 codes
appeared in all datasets, 7 codes appeared in two datasets,
and 1 code appeared in one dataset.
a) Community support: Participants reported that com-
munity support for third-party libraries was a factor. Thisensured that dependencies were well-maintained for software
quality. Having community support enables the production of
educational resources for the tool, such as on YouTube [58] as
well as StackOverÔ¨Çow and blog posts [59]. Participants said
this reduced onboarding costs:
/quote_leftAnd there are instructional videos on YouTube and whatnot
[that] can already teach people how to do [use the tool] without
the rest of the development team having to do anything. ‚Äù (P52)
b) Future requirements: Similar to the management
phase in requirements engineering [56], [57] participants notedrequirements could change. Understanding future requirements
ensured the software was useful in the long term. Estimating
them depended on prior experience and domain knowledge:
/quote_left...I‚Äôm making an assumption about what might come down the
pipe in the future. [It] depends on kind of my experience in that
Ô¨Åeld, and my work that I‚Äôve done with past clients to think that
my future clients might be similar enough to them. ‚Äù (P51)
c) Consistency: Similar to prior work [14], consistency
of the code style in the codebase or following program-
ming language convention was a consideration. This reduced
confusion between teammates and cognitive overhead for
individuals working across multiple contexts. This occurred
both at the application- and module-levels:
/quote_leftI have 200 or so web applications and having consis-
tency...makes it easy for me to switch between them without having
to re-remember a whole different framework. ‚Äù (P31)
/quote_leftSo some places [a stock] is called a stock, some places
it‚Äôs called security...I will try to keep that pattern, even if it‚Äôs
something I don‚Äôt necessarily agree with. ‚Äù (P34)
d) System Ô¨Åt: Participants considered how easily the
implementation could be integrated with existing code, such
as synergy with speciÔ¨Åc technologies. This is an attribute
of expert developers‚Äô software and designs [1]. System Ô¨Åt
required knowledge of the system‚Äôs architecture:
/quote_leftChoosing between django-q and celery was difÔ¨Åcult- one is
closely coupled with django environment and the other has long
history/reliability. ‚Äù (P55)
C. What process do software developers follow to make im-
plementation design decisions? ( RQ3 )
We describe the types of actions developers take while
making implementation design decisions (Sections IV-C1). We
then report the sequence of actions that developers follow in
their decision-making process (Section IV-C2).
1) Actions: Participants described 15 types of actions in
their strategies. We grouped these into 7 categories: deÔ¨Ån-
ing the problem space; ideating, evaluating, prototyping,
implementing, and verifying potential solutions; and updat-
ing knowledge. Many of these actions overlapped with the
traditional software development process, similar to prior
work [43]. Furthermore, participants‚Äô actions were often a
dialogue between the implementation and requirements. The
full list of the actions in implementation design decisions are
in Table IV. All codes occur in the data at least twice.
a) Updating requirements: Study participants described
times when requirements changed after they were deÔ¨Åned.
This occurred after learning from proof-of-concepts, reacting
to changes in the situation, or analyzing the requirements‚Äô
viability. Unlike in requirements engineering [56], [57], re-
quirements also changed during implementation. In these
cases, updating requirements was how participants dealt with
unforseen circumstances during implementation, such as time
constraints. Participants even developed heuristics to do so:
/quote_leftIf you are under time constraints, restrict the scope of your
implementation and don‚Äôt let perfect be the enemy of good. ‚Äù (P50)
b) Evaluating: Participants evaluated potential alterna-
tives for pros and cons, where they compared them against alist of considerations, especially non-functional requirements.
Some participants wrote lists or drew matricies, while others
developed checklists from their expertise:
/quote_leftDecide what you think is a pro/con of a given solution for your
use case.
Security concerns, an insecure package is never acceptable...
The popularity of the package is critical for evaluating
lifetime reliability and long term support.
Level of skill required to use the package, poorly designed
apis will be difÔ¨Åcult to extend if needs change, and compli-
cated for junior developers to work with.
Clean, consistent and clear are the ideal interfaces.
Consider the cost to replace the package if licensing, support
or project direction dramatically change... ‚Äù (P50)
c) Proof-of-concept: Study participants reported creating
proof-of-concepts, which is also important in requirements
elicitation [56], [57]. This quickly determined whether a
potential solution was viable. Participants varied in which
solutions they chose to prototype‚Äîsome chose to prototype
only the best candidate solution, while others prototyped
all potential solutions. Prototyping was also an information
gathering mechanism to brainstorm potential solutions:
/quote_leftLook at the features of the library and test the ones that you‚Äôre
interested in on small examples. Get a feel of the library... ‚Äù (P6)
Participants developed a proof-of-concept to update require-
ments. This was one way they considered a higher level of
design during implementation design decisions:
/quote_leftAsk the people who you interviewed to try your function and
provide feedback on any of the parameters. ‚Äù (P7)
d) Researching: Participants researched the problem
space, as identiÔ¨Åed in prior work [14]. This was the second
most reported action. This action was often used to elicit
requirements. Participants worked with stakeholders (e.g.,
project managers) and accessed websites for knowledge shar-
ing in software engineering, such as StackOverÔ¨Çow [59] and
Reddit [60]. They reviewed similar projects and used empirical
methods to understand the problem and generate requirements:
/quote_left[Using] tools to go through Git history...to Ô¨Ånd potential
problems... ‚Äù (P4)
2) Processes: Just as how software designers follow in-
dividualized processes [43], we found that strategies about
developers‚Äô decision-making processes were unique: there
were no repeated sequences of action codes or categories in
the strategies. One source of dissimilarity were action codes
and categories that were repeated in strategies. Table IV shows
the occurrences of repeated action categories in the strategies.
All action categories were repeated except ideating actions.
Implementing and deÔ¨Åning problem space actions were most
repeated in participants‚Äô strategies. Yet, there were common-
alities in the structure, namely when certain types of actions
occur. This is shown in Table V, which reports the median
position of each action category across all action sequences.
D. Which types of developer expertise are described in the
implementation design decision-making process? ( RQ4 )
Table VI shows the types of developer expertise in study
participants‚Äô decision-making process. We found that decision-
making expertise was most commonly cited in strategies.TABLE IV
SOFTWARE DEVELOPERS ‚ÄôACTIONS IN MAKING IMPLEMENTATION DECISIONS . CODES DISCUSSED IN DETAIL ARE UNDERLINED . THE NUMBER OF TIMES
AN ACTION CATEGORY IS REPEATED WITHIN THE SAME STRATEGY IN OUR DATA IS DENOTED WITH .
Code & Description Representative Quote
DeÔ¨Åning Problem Space ( 8)
Providing Context ‚ÄîExplaining context about the problem the developer is
facing (e.g., refactoring).‚ÄúWrite an initial program...If you have another program that requires a
similar behavior, consider whether you want to refactor the code.‚Äù (P15)
DeÔ¨Åning Requirements ‚ÄîDeÔ¨Åning the requirements of the solution, consid-
ering user needs, business needs, and organization needs.‚Äú...Figure out what use cases [your end users] would want for this function.
Ask your end users to provide examples of inputs...‚Äù (P7)
Updating Requirements ‚ÄîUpdating the requirements of the solution after
they are initially deÔ¨Åned.‚ÄúIf you Ô¨Ånd new requirements from your proof-of-concept, go to step 1.‚Äù
(P52)
Ideating (0)
Brainstorming ‚ÄîBrainstorming potential solutions that could solve the
problem.‚ÄúThink about the problem for a set period of time and write down more
than one idea on how to implement a solution...‚Äù (P7)
Assessing (4)
Evaluating ‚ÄîEvaluating the developer‚Äôs current situation; considering the
pros and cons for each solution.‚ÄúList out all the options that you have into a document and their pros and
cons.‚Äù (P29)
Estimating ‚ÄîEstimating the potential costs associated with implementation.‚ÄúDetermine whether the requirements are realistic given the resources you
have available.‚Äù (P34)
Prototyping (1)
Proof-of-Concept ‚ÄîBuilding a proof-of-concept for a potential solution. ‚ÄúTest each option in the development environment...‚Äù (P29)
Implementing (15)
Choosing ‚ÄîChoosing a solution to implement. ‚ÄúSelect the option which meets your requirements...‚Äù (P18)
Planning ‚ÄîPlanning the steps needed to implement the solution.‚ÄúList out the tasks that need to be done based on the requirements of the
problem/client and the technology stack available.‚Äù (P51)
Implementing ‚ÄîImplementing a particular solution. ‚ÄúBuild an implementation from the proof of concept.‚Äù (P52)
Updating Implementation ‚ÄîTrying a new implementation or updating an
existing one based on previous implementation attempts.‚ÄúIf there is a problem with the solution that‚Äôs implemented, go to step 1
with what you learned by implementing the solution.‚Äù (P29)
Deploying ‚ÄîReleasing the solution to the public.‚ÄúImplement your solution...and deploy it into a development environment.‚Äù
(P28)
Verifying (1)
Reviewing ‚ÄîHaving others review and provide feedback to the solution.‚ÄúHave others review your implementation proposal (over coffee can help).‚Äù
(P28)
Testing ‚ÄîTesting the implementation for functionality and potential defects.‚ÄúTest your implementation until you Ô¨Ånd most of the bugs you can and
your teams agree to release to prod.‚Äù (P52)
Updating Knowledge (4)
Researching ‚ÄîLearning more about the problem or potential solutions. ‚ÄúSearch online whether other libraries support your use case.‚Äù (P6)
Expertise relating to deeply understanding the vision of the
project (e.g., knowledgeable about customers and business )
was most frequently referenced. Expertise on
evaluating the pros and cons of a solution (e.g.,
makes tradeoffs ) was also mentioned. Participants also
frequently described forms of technical expertise (e.g.,
knowledgeable about tools and building materials ).
Expertise that was less commonly cited in strategies
largely related to aspects of the software and designs (e.g.,
attentive to details ). Expertise about teammates or the com-
pany (e.g., knowledgeable about the people and organization )
and responding to changing problem contexts (e.g.,
updates their mental models ) were also less referenced.
V. T HREATS TO VALIDITY
1) Internal validity: Strategy content may have been inÔ¨Çu-
enced by the study authors since they were initially recordedby them. To address this, the authors asked participants to
review and conÔ¨Årm the strategy multiple times to ensure the
strategy was accurate and written as the participants wished.
Participants could also directly make changes to the strategy
upon request.
The authors could have conÔ¨Årmation biases that developer
actions must follow normative theories of the software de-
velopment life cycle, which could inÔ¨Çuence the generation of
action codes. We reduced this threat by achieving consensus on
each code applied in our qualitative analysis. Future studies
using other methods, such as contextual inquiries of imple-
mentation design decision-making, could address this bias.
Additionally, memory bias could have introduced inaccura-
cies in participants‚Äô recall on past decisions, considerations,
and strategies. We reduced this bias by asking participants to
recount decisions, considerations, and actions that occurred in
the past Ô¨Åve days in both the survey and interview.TABLE V
MEDIAN POSITION OF ACTIONS IN PARTICIPANTS ‚ÄôSTRATEGIES AND
PERCENT OF STRATEGIES CONTAINING ACTIONS .
Action Median Position% Strategies w/
Action
Providing Context 1.5 12.5%
Researching 2 75.0%
DeÔ¨Åning Requirements 2 81.3%
Brainstorming 3 62.5%
Estimating 3 25.0%
Evaluating 4 81.3%
Choosing 5 81.3%
Planning 6 25.0%
Proof-of-Concept 6.5 31.3%
Updating Requirements 7 43.8%
Implementing 7 68.8%
Reviewing 8 43.8%
Testing 9 31.3%
Updating Implementation 9 18.8%
Deploying 10.5 12.5%
TABLE VI
FREQUENCY OF DEVELOPER EXPERTISE FROM LI ET AL . [1] IN
PARTICIPANTS ‚ÄôSTRATEGIES .
Expertise (quoted from Li et al. [1]) Count
Decision Making
Knowledgeable about customers and business 56
Sees the forest and the trees 45
Knowledgeable about tools and building materials 39
Knowledgeable about their technical domain 38
Knowledgeable about engineering processes 31
Models states and outcomes 24
Handles complexity 24
Knowledgeable about people and the organization 13
Updates their mental models 8
Software & Designs
Carefully constructed 26
Fitted 11
Evolving 10
Attentive to details 9
Anticipates needs 5
Creative 5
Long-termed 2
Elegant 2Participants could have been primed in the implementation
design decisions they reported from the survey and interview
examples. We reduced this threat by providing a short, diverse
set of examples to show the breadth of the phenomenon.
Study participants may have misunderstood the wording of
the questions. To reduce this threat, we piloted the survey and
interview with software developers and study team members
and asked for feedback on clarity. We also performed pilots
on the Ô¨Årst 15 survey responses for data quality.
2) External validity: Any small-scale empirical study has
generalizability issues [61]. To address this, we sampled
Reddit users across a diverse set of subreddits. Our sample rep-
resents diverse geographic regions, engineering organization
sizes, roles, and amounts of relevant experience. Additionally,
we also collected the data on decisions and considerations
using two methods, which we used to corroborate results.
Empirical studies can also suffer from selection bias. The
subreddits we recruited from may be homogeneous due having
to common interests, so some programming expertise was
not represented in our study (e.g., game development). Our
survey also limited representation to regions where English is
a primary language due to the survey being written in English.
We addressed this threat by ensuring the survey was as short
as possible, accurately advertising the survey‚Äôs length, and
providing incentives for participating in the interview.
One common issue for programming strategies is that
they may not generalize due to defects or having a narrow
scope [48]. To address this concern, we asked participants to
test and Ô¨Åx their strategy and verify whether their strategy
was generalized. The study authors were familiar with writing
high-quality and generalized strategies.
3) Construct validity: Since the participants‚Äô decisions,
considerations, and strategies were self-reported, there could
be inconsistencies in what participants report doing versus
what they actually did. They may have forgotten to explicitly
mention actions or misremembered the process they used.
VI. D ISCUSSION & F UTURE WORK
Our Ô¨Åndings overlap some with prior work (e.g., [1], [14],
[33], [62], [63]). In this section, we discuss them in relation
to implementation design decisions. This produces several
implications, which we elaborate on.
Our results suggest that implementation design decisions
are shaped by higher levels of design (e.g., requirements and
architecture). Also, a developer‚Äôs decisions can directly and in-
tentionally shape these higher level concerns. Thus, interpret-
ing requirements throughout implementation is key to mak-
ing these decisions. Requirements appeared in the decisions
(e.g., behaviors ), considerations (e.g., future requirements ),
and actions (e.g., deÔ¨Åning requirements ) of software devel-
opers. It also was the most frequently cited form of ex-
pertise (e.g., knowledgeable about customers and business ) in
strategies. This highlights the perspective that requirements
engineering is an ongoing process throughout implementa-
tion and maintenance. Depending on how much control the
developer had, they could re-interpret or completely changerequirements, suggesting that understanding how to update
requirements to match dynamic contexts could be a software
engineering skill. This contrasts the notion of requirements
being set prior to implementation. Rather, it supports prior
work stating that requirements can be iterated upon through
prototyping [55]‚Äì[57].
Next, we Ô¨Ånd that maintainability is a major theme in
implementation design decisions. It appeared in the de-
cisions (e.g., reuse ) and considerations (e.g., extensibility ,
reusing resources ) as it reduced workload, cognitive load, and
technical debt. This suggests that software developers may
need to develop a sense of how to anticipate maintenance effort
and develop knowledge on how to manage and reduce debt,
such as separation of concerns and modularity.
We also Ô¨Ånd that the process to make implementation design
decisions is both an art and a science. Across different devel-
opers and problems, there were strong commonalities in the
considerations and strategy structure. However, each strategy
was unique‚Äîprior work has shown that software development
teams also follow their own individual processes for early
stage software design work [43]. This implies that making
implementation design decisions has a common structure. Yet,
it requires expert judgment developed from experience with
similar problems to know when to deviate from it for the given
use case. Furthermore, it suggests this form of design expertise
is both systematic and opportunistic, which has been observed
in prior work [28], [64]. One source of opportunism is when
similar types of actions are repeated in different parts of the
strategy. This suggests that implementation design decisions
require rounds of iteration in different stages, especially in
implementing and deÔ¨Åning the problem space .
Finally, our results suggest that some forms of expertise are
more implicit, while others are more tacit. Explicit program-
ming strategies are a form of explicit programming knowl-
edge [47]. Participants‚Äô strategies largely referenced decision-
making expertise, implying that it could be explicit knowledge.
Meanwhile, expertise on developing software and designs was
referenced noticeably less frequently in participants‚Äô strategies
but instead overlapped with our enumerated considerations.
This suggests that this form of knowledge is more tacit and is
applied in the moment of programming problem-solving.
These implications affect software engineering researchers,
educators, and practitioners. We describe how our Ô¨Åndings
apply to them and provide opportunities for future work.
A. Educators
Our Ô¨Åndings have implications on how to teach program-
ming and software design. While teaching software engineer-
ing, educators could consider providing open-ended projects,
as suggested by Offutt and Baral [65]. This would provide
students with opportunities to make various types of im-
plementation design decisions. Further, these projects could
span for a longer duration to teach students how their deci-
sions shape software maintenance. Educators could scaffold
students‚Äô problem-solving process by authoring their ownexplicit programming strategies on how to make implemen-
tation design decisions. Educators could also use the list of
considerations from Table III as a checklist for students to
follow when evaluating candidate solutions.
B. Software Engineers
Our Ô¨Åndings can help novice engineers to make better im-
plementation design decisions. Since iteration is an important
part of making implementation design decisions, less expe-
rienced engineers could work prototyping into their regular
practice and become accustomed to learning from small-scale
failures through prototyping. Managers or mentors could rein-
force this learning environment by encouraging this practice.
Additionally, novices could also use the list of consider-
ations as a checklist to help make decisions. More senior
engineers could extend our list of considerations by writing
their own deÔ¨Ånitions and heuristics. They could also add their
own considerations to teach less experienced team members.
C. Researchers
This work raises questions about our understanding of
software design. Previous work viewed software design as a
sociotechnical process (e.g., [41]), a set of habits (e.g., [33]),
or as high-level code structure (e.g., [15]). Our work extends
this knowledge by focusing on the cognitive process involved
in software design. We view software design as a decision-
making exercise, following prior work (e.g., [8], [14]).
There are several directions that require additional study.
Future work could study each of the the consideration codes
to discover how developers estimate them and how they
compare considerations against one another. This could help
develop automated metrics or tools to aid developers‚Äô decision-
making. Additionally, future work could examine how experts‚Äô
decision-making processes differ than that of novices‚Äô, espe-
cially in the strategy structure and considerations. This could
help better understand the attributes that relate to effective
decision-making processes and advance understanding of soft-
ware engineering expertise.
DATA AVAILABILITY
Our supplemental materials are available on Figshare [45].
Data includes the codebooks for RQ1 ,RQ2 , and RQ3 ; the
plain text, action code, and action category representations
of participants‚Äô strategies; the survey instrument; and the
interview protocol.
ACKNOWLEDGMENTS
We thank our survey and interview participants for their
insight and Soham Pardeshi, Lilian Liang, Nimit Johri, and
Tobias D ¬®urschmid for their feedback. We give special thanks to
Mei
 , an outstanding canine software engineering researcher,
for providing support and motivation throughout this study.
This work was supported by the National Science Foun-
dation under grants 1539179, 1703734, 1703304, 1836813,
1845508, 2031265, 2100296, 2122950, 2137834, 2137312,
and by unrestricted gifts from Microsoft, Adobe, and Google.REFERENCES
[1] P. L. Li, A. J. Ko, and J. Zhu, ‚ÄúWhat makes a great software engineer?‚Äù
inIEEE/ACM International Conference on Software Engineering , 2015,
pp. 700‚Äì710.
[2] A. Tang, M. H. Tran, J. Han, and H. van Vliet, ‚ÄúDesign reasoning
improves software design quality,‚Äù in International Conference on the
Quality of Software Architectures , 2008, pp. 28‚Äì42.
[3] V . Rajlich and N. Wilde, ‚ÄúThe role of concepts in program comprehen-
sion,‚Äù in IEEE/ACM International Workshop on Program Comprehen-
sion, 2002, pp. 271‚Äì278.
[4] P. Kruchten, R. L. Nord, and I. Ozkaya, ‚ÄúTechnical debt: From metaphor
to theory and practice,‚Äù IEEE Software , vol. 29, no. 6, pp. 18‚Äì21, 2012.
[5] R. Verdecchia, P. Kruchten, and P. Lago, ‚ÄúArchitectural technical debt:
A grounded theory,‚Äù in European Conference on Software Architecture ,
2020, pp. 202‚Äì219.
[6] M. Cherubini, G. Venolia, R. DeLine, and A. J. Ko, ‚ÄúLet‚Äôs go to
the whiteboard: How and why software developers use drawings,‚Äù in
SIGCHI Conference on Human Factors in Computing Systems , 2007,
pp. 557‚Äì566.
[7] M. Petre, ‚ÄúUML in practice,‚Äù in IEEE/ACM International Conference
on Software Engineering , 2013, pp. 722‚Äì731.
[8] H. van Vliet and A. Tang, ‚ÄúDecision making in software architecture,‚Äù
Journal of Systems and Software , vol. 117, pp. 638‚Äì644, 2016.
[9] A. Tang, M. Razavian, B. Paech, and T. Hesse, ‚ÄúHuman aspects in soft-
ware architecture decision making,‚Äù in IEEE International Conference
on Software Architecture , 2017, pp. 107‚Äì116.
[10] A. Shahbazian, S. Karthik, Y . Brun, and N. Medvidovic, ‚ÄúeQual:
informing early design decisions,‚Äù in ACM Joint Meeting on European
Software Engineering Conference and Symposium on the Foundations
of Software Engineering , 2020, pp. 1039‚Äì1051.
[11] A. Jansen, J. van der Ven, P. Avgeriou, and D. K. Hammer, ‚ÄúTool
support for architectural decisions,‚Äù in Working IEEE/IFIP Conference
on Software Architecture , 2007, pp. 4‚Äì4.
[12] E. Gamma, R. Johnson, R. Helm, R. E. Johnson, and J. Vlissides, Design
Patterns: Elements of Reusable Object-Oriented Software . Pearson,
1995.
[13] K. Beck, R. Crocker, G. Meszaros, J. O. Coplien, L. Dominick,
F. Paulisch, and J. Vlissides, ‚ÄúIndustrial experience with design patterns,‚Äù
inIEEE International Conference on Software Engineering , 1996, pp.
103‚Äì114.
[14] P. Ralph and E. Tempero, ‚ÄúCharacteristics of decision-making during
coding,‚Äù in International Conference on Evaluation and Assessment in
Software Engineering , 2016, pp. 1‚Äì10.
[15] D. E. Perry and A. L. Wolf, ‚ÄúFoundations for the study of software
architecture,‚Äù ACM SIGSOFT Software Engineering Notes , vol. 17, no. 4,
pp. 40‚Äì52, 1992.
[16] W. Cunningham, ‚ÄúThe WyCash portfolio management system,‚Äù ACM
SIGPLAN OOPS Messenger , vol. 4, no. 2, pp. 29‚Äì30, 1992.
[17] T. Besker, A. Martini, and J. Bosch, ‚ÄúSoftware developer productivity
loss due to technical debt‚Äîa replication and extension study examining
developers‚Äô development work,‚Äù Journal of Systems and Software , vol.
156, pp. 41‚Äì61, 2019.
[18] N. Rios, R. O. Sp ¬¥ƒ±nola, M. Mendonc ¬∏a, and C. Seaman, ‚ÄúThe most
common causes and effects of technical debt: First results from a global
family of industrial surveys,‚Äù in ACM/IEEE International Symposium on
Empirical Software Engineering and Measurement , 2018, pp. 1‚Äì10.
[19] ‚Äî‚Äî, ‚ÄúSupporting analysis of technical debt causes and effects with
cross-company probabilistic cause-effect diagrams,‚Äù in IEEE/ACM In-
ternational Conference on Technical Debt , 2019, pp. 3‚Äì12.
[20] G. Ruhe, ‚ÄúSoftware engineering decision support‚Äîa new paradigm for
learning software organizations,‚Äù in International Workshop on Learning
Software Organizations , 2002, pp. 104‚Äì113.
[21] C. Becker, D. Walker, and C. McCord, ‚ÄúIntertemporal choice: Decision-
making and time in software engineering,‚Äù in IEEE/ACM International
Workshop on Cooperative and Human Aspects of Software Engineering ,
2017, pp. 23‚Äì29.
[22] S. Baltes and S. Diehl, ‚ÄúTowards a theory of software development
expertise,‚Äù in ACM Joint Meeting on European Software Engineering
Conference and Symposium on the Foundations of Software Engineering ,
2018, pp. 187‚Äì200.
[23] P. L. Li, A. J. Ko, and A. Begel, ‚ÄúWhat distinguishes great software
engineers?‚Äù Empirical Software Engineering , vol. 25, no. 1, pp. 322‚Äì
352, 2020.[24] A. Aurum and C. Wohlin, ‚ÄúThe fundamental nature of requirements
engineering activities as a decision-making process,‚Äù Information and
Software Technology , vol. 45, no. 14, pp. 945‚Äì954, 2003.
[25] P. Berander and A. Andrews, ‚ÄúRequirements prioritization,‚Äù in Engi-
neering and Managing Software Requirements . Springer, 2005, pp.
69‚Äì94.
[26] D. Falessi, G. Cantone, R. Kazman, and P. Kruchten, ‚ÄúDecision-making
techniques for software architecture design: A comparative survey,‚Äù
ACM Computing Surveys , vol. 43, no. 4, pp. 1‚Äì28, 2011.
[27] L. Bass, P. Clements, and R. Kazman, Software Architecture in Practice .
Addison-Wesley Professional, 2003.
[28] C. Zannier, M. Chiasson, and F. Maurer, ‚ÄúA model of design decision
making based on empirical results of interviews with software design-
ers,‚Äù Information and Software Technology , vol. 49, no. 6, pp. 637‚Äì653,
2007.
[29] J. Stylos and B. Myers, ‚ÄúMapping the space of API design decisions,‚Äù in
IEEE Symposium on Visual Languages and Human-Centric Computing ,
2007, pp. 50‚Äì60.
[30] R. Dharani, Web API Design: Crafting Interfaces that Developers Love .
Independently published, 2017.
[31] ‚ÄúSwift.org‚ÄîAPI design guidelines,‚Äù 2022, retrieved December 20, 2022
from https://www.swift.org/documentation/api-design-guidelines/.
[32] ‚ÄúAPI design,‚Äù 2022, retrieved December 20, 2022 from https://
martinfowler.com/tags/API%20design.html.
[33] M. Petre and A. van der Hoek, Software Design Decoded: 66 Ways
Experts Think . MIT Press, 2016.
[34] J. Bloch, Effective Java . Addison-Wesley Professional, 2008.
[35] R. C. Martin, Clean Code: A Handbook of Agile Software Craftsman-
ship. Pearson Education, 2009.
[36] ‚Äî‚Äî, Just Enough Software Architecture: A Risk-Driven Approach .
Prentice Hall, 2018.
[37] G. Fairbanks, Clean Architecture: A Craftsman‚Äôs Guide to Software
Structure and Design . Marshall & Brainerd, 2018.
[38] A. J. Ko and P. K. Chilana, ‚ÄúDesign, discussion, and dissent in open
bug reports,‚Äù in iConference , 2011, pp. 106‚Äì113.
[39] J. Brunet, G. C. Murphy, R. Terra, J. Figueiredo, and D. Serey, ‚ÄúDo
developers discuss design?‚Äù in Working Conference on Mining Software
Repositories , 2014, pp. 340‚Äì343.
[40] A. Mahadi, N. A. Ernst, and K. Tongay, ‚ÄúConclusion stability for natural
language based mining of design discussions,‚Äù Empirical Software
Engineering , vol. 27, no. 1, pp. 1‚Äì42, 2022.
[41] N. Mangano, T. D. LaToza, M. Petre, and A. van der Hoek, ‚ÄúSupporting
informal design with interactive whiteboards,‚Äù in SIGCHI Conference on
Human Factors in Computing Systems , 2014, pp. 331‚Äì340.
[42] S. Baltes and S. Diehl, ‚ÄúSketches and diagrams in practice,‚Äù in ACM
SIGSOFT International Symposium on Foundations of Software Engi-
neering , 2014, pp. 530‚Äì541.
[43] B. Sharif, N. Dragan, A. Sutton, M. L. Collard, and J. I. Maletic,
‚ÄúIdentifying and analyzing software design activities,‚Äù in Software
Designers in Action: A Human-Centric Look at Design Work . Chapman
and Hall/CRC, 2013, pp. 153‚Äì174.
[44] M. K. Scheuerman, K. Spiel, O. L. Haimson, F. Hamidi, and S. M.
Branham, ‚ÄúHCI guidelines for gender equity and inclusivity,‚Äù in UMBC
Faculty Collection , 2020.
[45] J. T. Liang, M. Arab, M. Ko, A. J. Ko, and T. D. LaToza, ‚ÄúSupplemental
materials to ‚ÄúA qualitative study on the implementation design decisions
of developers‚Äù,‚Äù 2023, available at https://doi.org/10.6084/m9.Ô¨Ågshare.
21820140.
[46] A. J. Ko, T. D. LaToza, and M. M. Burnett, ‚ÄúA practical guide
to controlled experiments of software engineering tools with human
participants,‚Äù Empirical Software Engineering , vol. 20, no. 1, pp. 110‚Äì
141, 2015.
[47] T. D. LaToza, M. Arab, D. Loksa, and A. J. Ko, ‚ÄúExplicit programming
strategies,‚Äù Empirical Software Engineering , vol. 25, no. 4, pp. 2416‚Äì
2449, 2020.
[48] M. Arab, T. D. LaToza, J. Liang, and A. J. Ko, ‚ÄúAn exploratory study
of sharing strategic programming knowledge,‚Äù in SIGCHI Conference
on Human Factors in Computing Systems , 2022, pp. 1‚Äì15.
[49] M. Arab, J. Liang, Y . Yoo, A. J. Ko, and T. D. LaToza, ‚ÄúHowToo:
A platform for sharing, Ô¨Ånding, and using programming strategies,‚Äù in
IEEE Symposium on Visual Languages and Human-Centric Computing ,
2021, pp. 1‚Äì9.[50] D. Ford, T. Zimmermann, C. Bird, and N. Nagappan, ‚ÄúCharacterizing
software engineering work with personas based on knowledge worker
actions,‚Äù in ACM/IEEE International Symposium on Empirical Software
Engineering and Measurement , 2017, pp. 394‚Äì403.
[51] E. K. Smith, C. Bird, and T. Zimmermann, ‚ÄúBeliefs, practices, and
personalities of software engineers: A survey in a large software com-
pany,‚Äù in International Workshop on Cooperative and Human Aspects
of Software Engineering , 2016, pp. 15‚Äì18.
[52] D. Hammer and L. K. Berland, ‚ÄúConfusing claims for data: A critique
of common practices for presenting qualitative research on learning,‚Äù
Journal of the Learning Sciences , vol. 23, no. 1, pp. 37‚Äì46, 2014.
[53] J. Salda Àúna,The Coding Manual for Qualitative Researchers . SAGE
Publications, 2009.
[54] M. B. Miles and A. M. Huberman, Qualitative Data Analysis: An
Expanded Sourcebook . SAGE Publications, 1994.
[55] B. W. Boehm, ‚ÄúA spiral model of software development and enhance-
ment,‚Äù Computer , vol. 21, no. 5, pp. 61‚Äì72, 1988.
[56] F. Paetsch, A. Eberlein, and F. Maurer, ‚ÄúRequirements engineering
and agile software development,‚Äù in IEEE International Workshops on
Enabling Technologies: Infrastructure for Collaborative Enterprises ,
2003, pp. 308‚Äì313.
[57] B. Nuseibeh and S. Easterbrook, ‚ÄúRequirements engineering: A
roadmap,‚Äù in Conference on the Future of Software Engineering , 2000,
pp. 35‚Äì46.
[58] L. MacLeod, M.-A. Storey, and A. Bergen, ‚ÄúCode, camera, action:
How software developers document and share program knowledge using
YouTube,‚Äù in IEEE International Conference on Program Comprehen-
sion, 2015, pp. 104‚Äì114.
[59] C. Parnin, C. Treude, L. Grammel, and M.-A. Storey, ‚ÄúCrowd docu-
mentation: Exploring the coverage and the dynamics of API discussions
on Stack OverÔ¨Çow,‚Äù Georgia Institute of Technology Technical Report ,
vol. 11, 2012.
[60] C. D. Hardin and M. Berland, ‚ÄúLearning to program using online forums:
A comparison of links posted on Reddit and Stack OverÔ¨Çow,‚Äù in ACM
Technical Symposium on Computing Science Education , 2016, pp. 723‚Äì
723.
[61] B. Flyvbjerg, ‚ÄúFive misunderstandings about case-study research,‚Äù Qual-
itative Inquiry , vol. 12, no. 2, pp. 219‚Äì245, 2006.
[62] S. Balaji and M. S. Murugaiyan, ‚ÄúWaterfall vs. V-model vs. Agile:
A comparative study on sdlc,‚Äù International Journal of Information
Technology and Business Management , vol. 2, no. 1, pp. 26‚Äì30, 2012.
[63] M. Petre, A. van der Hoek, and D. S. Bowers, ‚ÄúSoftware design as
multiple contrasting dialogues,‚Äù in Psychology of Programming Interest
Group 30th Annual Conference , 2019.
[64] S. P. Davies, ‚ÄúCharacterizing the program design activity: Neither
strictly top-down nor globally opportunistic,‚Äù Behaviour & Information
Technology , vol. 10, no. 3, pp. 173‚Äì190, 1991.
[65] J. Offutt and K. Baral, ‚ÄúDesigning divergent thinking, creative problem
solving exams,‚Äù in ACM/IEEE International Conference on Software
Engineering: Software Engineering Education and Training , 2022, pp.
82‚Äì89.