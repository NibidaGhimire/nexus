P3DC-Shot: Prior-Driven Discrete Data Calibration for
Nearest-Neighbor Few-Shot ClassiÔ¨Åcation
Shuangmei Wanga,, Rui Maa,b,, Tieru Wua,b,, Yang Caoa,
aJilin University, No. 2699 Qianjin Street, Changchun, 130012, China
bEngineering Research Center of Knowledge-Driven Human-Machine Intelligence, MOE, No. 2699 Qianjin Street, Changchun, 130012, China
Abstract
Nearest-Neighbor (NN) classiÔ¨Åcation has been proven as a simple and e ective approach for few-shot learning. The
query data can be classiÔ¨Åed e ciently by Ô¨Ånding the nearest support class based on features extracted by pretrained
deep models. However, NN-based methods are sensitive to the data distribution and may produce false prediction if
the samples in the support set happen to lie around the distribution boundary of di erent classes. To solve this issue,
we present P3DC-Shot, an improved nearest-neighbor based few-shot classiÔ¨Åcation method empowered by prior-
driven data calibration. Inspired by the distribution calibration technique which utilizes the distribution or statistics of
the base classes to calibrate the data for few-shot tasks, we propose a novel discrete data calibration operation which is
more suitable for NN-based few-shot classiÔ¨Åcation. SpeciÔ¨Åcally, we treat the prototypes representing each base class
as priors and calibrate each support data based on its similarity to di erent base prototypes. Then, we perform NN
classiÔ¨Åcation using these discretely calibrated support data. Results from extensive experiments on various datasets
show our e cient non-learning based method can outperform or at least comparable to SOTA methods which need
additional learning steps.
Keywords:
Few-Shot Learning, Image ClassiÔ¨Åcation, Prototype, Calibration
1. Introduction
Deep learning has triggered signiÔ¨Åcant breakthroughs
in many computer vision tasks, such as image classiÔ¨Å-
cation [1, 2, 3], object detection [4, 5, 6], and seman-
tic segmentation [7, 8, 9] etc. One key factor for the
success of deep learning is the emergence of large-scale
datasets, e.g., ImageNet [2], MSCOCO [10], Cityscapes
[11], just to name a few. However, it is di cult and
expensive to collect and annotate su cient data sam-
ples to train a deep model with numerous weights. The
data limitation has become a main bottleneck for more
broader application of deep leaning, especially for the
tasks involving rarely seen samples. On the other hand,
human can learn to recognize novel visual concepts
from only a few samples. There is still a notable gap
This work is supported in part by the National Key Research
and Development Program of China (Grant No. 2020YFA0714103)
and the National Natural Science Foundation of China (Grant No.
61872162 and 62202199).
Co-Ô¨Årst authors.
Corresponding authors.between human intelligence and the deep learning based
artiÔ¨Åcial intelligence. Few-shot learning (FSL) aims to
learn neural models for novel classes with only a few
samples. Due to its ability for generalization, FSL has
attracted extensive interests in recent years [12, 13, 14].
Few-shot classiÔ¨Åcation is the most widely studied
FSL task which attempts to recognize new classes or
classify data in an unseen query set . Usually, few-shot
classiÔ¨Åcation is formulated in a meta-learning frame-
work [15, 16, 17, 18, 19, 20, 21, 22, 23]. In the
meta-training stage, the N-way K-shot episodic training
paradigm is often employed to learn generalizable clas-
siÔ¨Åers or feature extractors for data of the base classes .
Then, in the meta-testing stage, the meta-learned clas-
siÔ¨Åers can quickly adapt to a few annotated but unseen
data in a support set and attain the ability to classify the
novel query data. Although meta-learning has shown
the eectiveness for few-shot classiÔ¨Åcation, it is unclear
how to set the optimal class number (N) and per-class
sample number (K) when learning the classiÔ¨Åers. Also,
the learned classiÔ¨Åer may not perform well when the
sample number K used in meta-testing does not match
Preprint submitted to Elsevier January 3, 2023arXiv:2301.00740v1  [cs.CV]  2 Jan 2023the one used in the meta-training [24].
On the other hand, nearest-neighbor (NN) based clas-
siÔ¨Åcation has been proven as a simple and e ective ap-
proach for FSL. Based on features obtained from the
meta-learned feature extractor [15, 16] or the pretrained
deep image models [25], the query data can be e -
ciently classiÔ¨Åed by Ô¨Ånding the nearest support class.
SpeciÔ¨Åcally, the prediction is determined by measuring
the similarity or distance between the query feature and
the prototypes (i.e., average or centroid) of the support
features. From the geometric view, NN-based classi-
Ô¨Åcation can be solved using a V oronoi Diagram (VD)
which is a partition of the space formed by the support
features [26, 27]. Given a query feature, its class can
be predicted by computing the closest V oronoi cell that
corresponds to a certain support class. With proper VD
construction and feature distance metrics, the state-of-
the-art performance can be achieved for few-shot clas-
siÔ¨Åcation [28]. However, due to the limited number
of support samples, NN-based few-shot classiÔ¨Åcation is
sensitive to the distribution of the sampled data and may
produce false prediction if the samples in the support set
happen to lie around the distribution boundary of di er-
ent classes (see Figure 1 left).
To solve above issues, various e orts have been paid
to more e ectively utilize the knowledge or priors from
the base classes for few-shot classiÔ¨Åcation. One natural
way is to learn pretrained classiÔ¨Åers or image encoders
with the abundant labeled samples of base classes and
then adapt them the novel classes via transfer learning
[29, 30, 31, 23]. Meanwhile, it has been shown that
variations in selecting the base classes can lead to dif-
ferent performance on the novel classes [32, 33, 34] and
how to select the base classes for better feature repre-
sentation learning still needs more investigation. On
the other hand, a series of works [35, 36, 37, 38] per-
form data calibration to the novel classes so that the re-
sults are less a ected by the limited number of support
samples. One representative is Distribution Calibration
(DC) [38] which assumes the features of the data fol-
low the Gaussian distribution and transfers the statis-
tics from the similar base classes to the novel classes.
Then, DC trains a simple logistic regression classiÔ¨Åer
to classify the query features using features sampled
from the calibrated distributions of the novel classes.
Although DC has achieved superior performance than
previous meta-learning [19, 21, 22] or transfer-learning
[29, 30, 31, 23] based methods, it relies on the strong as-
sumption for Gaussian-like data distribution and it can-
not be directly used for NN-based few-shot classiÔ¨Åca-
tion.
In this paper, we propose P3DC-Shot, an improved
Support sample                               Query sample                       Calibrated support sampleFigure 1: When samples in the support set lie around the distribution
boundary of di erent classes, the NN classiÔ¨Åer may produce false pre-
diction. By performing discrete calibration for each support sample
using priors from the base classes, the calibrated support data is trans-
formed closer to the actual class centroid and can lead to less-biased
NN classiÔ¨Åcation. The colored regions represent the underlying data
distribution of di erent classes. The gray lines are the predicted deci-
sion boundaries by the NN classiÔ¨Åer.
NN-based few-shot classiÔ¨Åcation method that employs
prior information from base classes to discretely cali-
brate or adjust the support samples so that the calibrated
data is more representative for the underlying data dis-
tribution (Figure 1 right). Our main insight is even the
novel classes have not been seen before, they still share
similar features to some base classes, and the prior in-
formation from the base classes can serve as the context
data for the novel classes. When only a few support
samples are available for the novel classes, performing
prior-driven calibration can alleviate the possible bias
introduced by the few-shot support samples. With the
calibrated support samples, the query data can be more
accurately classiÔ¨Åed by a NN-based classiÔ¨Åer.
SpeciÔ¨Åcally, for the prior information, we compute
the prototype, i.e., the average of features, for each base
class. Then, we propose three di erent schemes for se-
lecting the similar prototypes to calibrate the support
data. Firstly, we propose the sample-level calibration
which selects the top Mmost similar base prototypes for
each support sample and then apply weighted averaging
between each support sample and selected prototypes to
obtain the calibrated support sample. Secondly, to uti-
lize more context from the base classes, we propose the
task-level calibration which combines the most similar
base prototypes for each support sample into a union
and performs the calibration for the support samples us-
ing each prototype in the union. In addition, we pro-
pose a uniÔ¨Åed calibration scheme that combines the two
above schemes so that the calibration can exploit dif-
ferent levels of prior information from the base classes.
To utilize the calibrated support samples for the NN-
based classiÔ¨Åcation, we further obtain the prototypes of
2the support class using an attention-weighted averaging,
while the attention weights are computed between the
query sample and each calibrated support sample. Fi-
nally, the classiÔ¨Åcation of a query sample is simply de-
termined by Ô¨Ånding its nearest support prototype mea-
sured by the cosine similarity.
Comparing to DC, our P3DC-Shot adopts the simi-
lar idea of transferring the information or statistics from
the base classes to the novel classes. The key di er-
ence is our data calibration is performed on each indi-
vidual support sample rather than the distribution pa-
rameters and we employ the NN-based classiÔ¨Åcation in-
stead of the learned classiÔ¨Åer as in DC. Comparing to
other NN-based few-shot classiÔ¨Åcation methods such as
SimpleShot [25], since our support data is calibrated,
the NN classiÔ¨Åcation is less a ected by the sampling
bias for the support data, e.g, the calibrated data is more
likely to be close to the center of the corresponding
novel class. We conduct extensive comparisons with re-
cent state-of-the-art few-shot classiÔ¨Åcaiton methods on
miniImageNet [2], tiredImageNet [39] and CUB [40]
and the results demonstrate the superiority and general-
izability of our P3DC-Shot. Ablation studies on di er-
ent calibration schemes, i.e., di erent weights between
the sample-level and task-level calibration also show the
necessity of combining two schemes for better results.
In summary, our contributions are as follows:
1. We propose P3DC-Shot, a prior-driven dis-
crete data calibration strategy for nearest-neighbor
based few-shot classiÔ¨Åcation to enhance the
model‚Äôs robustness to the distribution of the sup-
port samples.
2. Without additional training and expensive compu-
tation, the proposed method can e ciently cali-
brate each support sample using information from
the prototypes of the similar base classes.
3. We conduct extensive evaluations on three discrete
calibration schemes on various datasets and the re-
sults show our e cient non-learning based method
can outperform or at least comparable to SOTA
few-shot classiÔ¨Åcation methods.
2. Related Work
In this section, we Ô¨Årst review the representative
meta-learning and transfer learning based few-shot clas-
siÔ¨Åcation techniques. Then, we summarize the nearest-
neighbor and data calibration based approaches which
are most relevant to our P3DC-Shot.
Meta-learning based few-shot classiÔ¨Åcation. Meta-
learning [41] has been widely adopted for few-shot clas-
siÔ¨Åcation. The core idea is to leverage the episodictraining paradigm to learn generalizable classiÔ¨Åers or
feature extractors using the data from the base classes
in an optimization-based framework [18, 19, 20, 21,
22], as well as learn a distance function to measure
the similarity between the support and query samples
through metric-learning [42, 15, 17, 43, 44, 37]. For
example, MAML [19] is one of the most representa-
tive optimization-based meta-learning method for few-
shot classiÔ¨Åcation and its goal is to learn good net-
work initialization parameters so that the model can
quickly adapt to new tasks with only a small amount
of new training data from the novel classes. For metric-
learning based methods such as the Matching Networks
[15], Prototypical Networks [16] and Relation Net-
works [17], the network is trained to either learn an
embedding function with a given distance function or
learn both the embedding and the distance function in
a meta-learning architecture. Unlike the optimization
and metric-learning based methods which require so-
phisticated meta-learning steps, our method can directly
utilize the features extracted by the pretrained models
and perform the prior-driven calibration to obtain less-
biased support features for classiÔ¨Åcation.
Transfer learning based few-shot classiÔ¨Åcation.
Transfer learning [45, 46, 47] is a classic machine learn-
ing or deep learning technique that aims to improve
the the learning of a new task through the transfer of
knowledge from one or more related tasks that have al-
ready been learned. Pretraining a deep network on the
base dataset and transferring knowledge to the novel
classes via Ô¨Åne-tuning [31, 48, 30] has been shown as
the strong baseline for the few-shot classiÔ¨Åcation. To
learn better feature representations which can lead to
improved few-shot Ô¨Åne-tuning performance, Mangla et
al. [29] propose S2M2, the Self-Supervised Manifold
Mixup, to apply regularization over the feature mani-
fold enriched via the self-supervised tasks. In addition
to training new linear classiÔ¨Åers based on the pretrained
weights learned from the base classes, Meta-Baseline
[23] performs meta-learning to further optimize the pre-
trained weights for few-shot classiÔ¨Åcation. On the other
hand, it has been shown the results of the transfer learn-
ing based methods depend on di erent selections of the
base classes for pretraining [32, 33], while how to se-
lect the base classes to achieve better performance is
still challenging [34]. In comparison, our P3DC-shot
does not need the additional cost for feature represen-
tation learning and can more e ectively utilize the base
classes in a NN-based classiÔ¨Åcation framework.
Nearest neighbor based few-shot classiÔ¨Åcation.
NN-based classiÔ¨Åcation has also been investigated for
few-shot classiÔ¨Åcation. The main idea is to compute the
3prototypes of the support samples, i.e., the mean or cen-
troid of the support features, and classify the query sam-
ple using metrics such as L2 distance, cosine similarity
or a learned distance function. In SimpleShot [25], it
shows nearest neighbor classiÔ¨Åcation with features sim-
ply normalized by L2 norm and measured by Euclidean
distance can achieve competitive few-shot classiÔ¨Åcation
results. Instead of performing nearest neighbor classiÔ¨Å-
cation on the image-level features, Li et al. [49] intro-
duces a Deep Nearest Neighbor Neural Network which
performs nearest neighbor search over the deep local
descriptors and deÔ¨Ånes an image-to-class measure for
few-shot classiÔ¨Åcation. From a geometric view, Ma et
al. [50] utilize the Cluster-induced V oronoi Diagram
(CIVD) to incorporate cluster-to-point and cluster-to-
cluster relationships to the nearest neighbor based clas-
siÔ¨Åcation. Similar to above methods, our method is
based on the nearest prototype classiÔ¨Åcation, while
we perform the prior-driven data calibration to obtain
less-biased support data for the prototype computation.
Meanwhile, computing the attentive or reweighted pro-
totypes [51, 52, 53] that are guided by the base classes
or query samples has also been investigated recently.
We follow the similar idea and compute the attention-
weighted prototypes for NN-based classiÔ¨Åcation.
Data calibration for few-shot classiÔ¨Åcation. Due to
the limited number of samples, the prototypes or cen-
troids computed from the few-shot support data may be
biased and cannot represent the underlying data distri-
bution. Simply performing NN-based classiÔ¨Åcation on
these biased prototypes will lead to inaccurate classi-
Ô¨Åcation. Several methods have been proposed to cali-
brate or rectify the data to obtain better samples or pro-
totypes of the support class [35, 36, 37, 54, 38]. Using
the images in the base classes, RestoreNet [35] learns
a class agnostic transformation on the feature of each
image to move it closer to the class center in the fea-
ture space. To reduce the bias caused by the scarcity
of the support data, Liu et al., [36] employ the pseudo-
labeling to add unlabelled samples with high prediction
conÔ¨Ådence into the support set for prototype rectiÔ¨Åca-
tion. In [37], Guo et al. propose a Pair-wise Similar-
ity Module to generate calibrated class centers that are
adapted to the query sample. Instead of calibrating in-
dividual support samples, Distribution Calibration (DC)
[38] aims to calibrate the underlying distribution of the
support classes by transferring the Gaussian statistics
from the base classes. With su cient new support data
sampled from the calibrated distribution, an additional
classiÔ¨Åer is trained in [38] to classify the query sam-
ple. In contrast to these methods, we do not require
additional training or assumption of the underlying dis-tribution. Instead, we directly use the prototypes of the
base classes to calibrate each support sample individ-
ually and we adopt the NN-based classiÔ¨Åcation which
makes the whole pipeline discrete and e cient. One
recent work that is similar to ours is Xu et al. [54]
which proposes the Task Centroid Projection Removing
(TCPR) module and transforms all support and query
features in a given task to alleviate the sample selection
bias problem. Comparing to [54], we only calibrate the
support samples using the priors from the base classes
and keep the query samples unchanged.
3. Method
To eectively utilize the prior knowledge from the
base classes, we Ô¨Årst propose two independent calibra-
tion strategies, i.e., sample-level calibration and task-
level calibration, which exploit di erent levels of infor-
mation from the base classes. Then, we combine the
sample-level and task-level calibration together to ob-
tain the Ô¨Ånal calibrated support samples which will be
used for the nearest neighbor classiÔ¨Åcation.
Figure 2 shows an illustration of the P3DC-Shot
pipeline. Given a pretrained feature extractor Fand a
set of prototypes of base classes, we perform the prior-
driven discrete calibration to the normalized features of
the support data. Initially, the query sample in green
is closer to the support sample in yellow. After the
proposed calibration using the related base class proto-
types, the query sample becomes closer to the calibrated
support sample in blue. In the following, we provide
technical details of the P3DC-Shot for few-shot classi-
Ô¨Åcation.
3.1. Problem Statement
In this paper, we focus on the few-shot image clas-
siÔ¨Åcation which aims to classify the new image sam-
ples from the novel classes with just a few labeled im-
age samples. Normally, the new data sample is called
a query sample and the labelled samples are called sup-
port samples. With the aid of a set of base classes rep-
resented by their prototypes Pb=fpb
ignb
i=1, our goal is to
calibrate the support samples from novel-class so that
they can be better matched with the query samples by
a nearest neighbor classiÔ¨Åer. Here, all data samples are
represented by the features computed from a pretrained
feature extractor F() :X!Rd, while Xis the domain
of the image space and dis the dimension of the feature
space; pb
iis the prototype of a base class, which is com-
puted as the average feature of the samples within the
4L2 norm CalibrationùêπùêπFeature
extraction
ùë•ùë•1
ùë•ùë•2ùëûùëû
Support data
Query dataFinal calibrated support featuresEndpoint of sample -level calibration
Endpoint of task-level calibrationAll base class prototypes
or Selected prototypes for a sample
+ Selected prototypes for a taskÃÖùë†ùë†1
ÃÖùë°ùë°1
ÃÖùë°ùë°2ÃÖùë†ùë†2ùë•ùë•1ùëêùëê
ùë•ùë•2ùëêùëêÃÖùë•ùë•1
ÔøΩùëûùëûÃÖùë•ùë•2Figure 2: An illustration of the P3DC-Shot pipeline for the 2-way 1-shot scenario. Note that the direct interpolation of the three triangle vertices
return a feature on the triangle plane. After normalization, the Ô¨Ånal calibrated features ¬Ø xu
1and ¬Øxu
2are on the hypersphere of the normalized space.
class; nbis the number of all base classes. For simplic-
ity, we directly use xito represent the feature F(xi) of
an image xi.
We follow the conventional few-shot learning setting,
i.e., build a series of N-way K-shot tasks where Nis the
number of novel classes and Kis the number of sup-
port samples in each task. Formally, each task con-
sists of a support set S=f(xi;yi)gNK
i=1and a query set
Q=fqigNK+NQ
i=NK+1. Here, yiis the label of the corre-
sponding sample, which is known for the support set
and unknown for the query set; Qis the number of query
sample for each novel class in the current task. Given a
support feature xi, we perform our prior-driven calibra-
tion to obtain the calibrated support feature xc
i=C(xi),
whereC() :Rd!Rdconducts feature transformation
based on the information from the base classes. Then,
we predict the label of a query feature by performing
nearest neighbor classiÔ¨Åcation w.r.t the novel class pro-
totypes computed from the calibrated support feature(s).
3.2. Prior-Driven Discrete Data Calibration
Before we perform calibration to the support data, we
Ô¨Årst apply L2 normalization to the support and query
features. It is shown in SimpleShot [25] that using
L2-normalized feature with a NN-based classiÔ¨Åer can
lead to competitive results for few-shot classiÔ¨Åcation.
Hence, we obtain ¬Ø xifor a support feature xiby:
¬Øxi=normalize (xi)=xi
kxik2: (1)
Similarly, the normalization of the query features are
also computed: ¬Ø qi=normalize (qi). By working with
the normalized features, we can obviate the absolute
scales of the features and focus on the similarities and
dierences on their directions. Note that, the normal-
ized features are used in the feature combination step(Eq. 7, 10 and 11) for obtaining the interpolation be-
tween the normalized features and in the NN-based clas-
siÔ¨Åcation step (Eq. 12) for performance improvement.
Next, we propose the sample-level and task-level cal-
ibration, and their combination to utilize the priors from
the base classes for obtaining the less-biased support
features.
3.2.1. Sample-Level Calibration
According to previous works [55, 38] which also use
the information from base classes for classifying the
new classes, the base classes with higher similarities
to the query classes are more important than other base
classes. Hence, we Ô¨Årst propose to perform calibration
based on the top similar base classes for each support
sample. Moreover, following DC [38], we apply the
Tukeys‚Äôs Ladder of Powers transformation [56] to the
features of the support samples before the calibration:
Àúxi=(x
iif,0
log(xi) if=0(2)
Here,is a hyperparameter which controls the distri-
bution of the transformed feature, with a smaller can
lead to a less skewed feature distribution. We set =0:5
and obtain the transformed support feature Àú xifrom the
original feature xi.
Then, we select the top Mbase classes with higher
similarities to a transformed support feature Àú xi:
M
i=fpb
jjj2topM (Si)g; (3)
where Si=f<Àúxi;pb
j>jj2f1;:::nbgg: (4)
Here, M
istores the Mnearest base prototypes with re-
spect to a transformed support feature vector Àú xi;topM ()
is an operator that returns the index of top Melements
from Si, the similarity set of Àú xi, while the similarity be-
tween Àú xiand a base prototype pb
jis computed by the
5inner product <;>. In DC [38], the distributions
of the base and novel classes are assumed as Gaussian
distribution and the statistics (mean and co-variance) of
the base classes are used to calibrate the distribution of
the novel classes. In contrast, we directly use the sim-
ilar base prototypes to calibrate each support feature.
SpeciÔ¨Åcally, the calibration for Àú xidriven by base proto-
types pb
j2M
iis computed as:
si=Àúxi+X
j2M
iwi jpb
j; (5)
where the weights of the M nearest base classes proto-
types in M
iare obtained by applying Softmax to the
similarities between Àú xiand these prototypes:
wi j=e<Àúxi;pb
j>
P
k2M
ie<Àúxi;pb
k>;j2M
i: (6)
It should be noted that, in Eq. 5, the support feature Àú xi
is a transformed feature, while the base prototypes are
in the original feature space. This setting is the same
as DC does for calibrating the distribution of the novel
classes and it can be understood as follows: 1) the trans-
formation can initially reduce the skewness of the few-
shot-sampled support features; 2) the term wi jpb
jcan be
regarded as the projection of Àú xiw.r.t prototype pb
j; 3)
Àúxiis calibrated based on its projects to all of its similar
base prototypes in M
i.
Finally, the sample-level calibration for a normalized
support sample ¬Ø xiis deÔ¨Åned as:
¬Øxs
i=normalize ((1 ) ¬Øxi+¬Øsi); (7)
where2[0;1] is a parameter to linearly combine
the normalized support feature ¬Ø xiand normalized base-
prototypes-driven calibration ¬Ø si=norm (si). As shown
in Figure 2, ¬Ø xiand ¬Øsiform a line in the normalized fea-
ture space and ¬Ø xs
iis the normalization of a in-between
point on this line. In general, the sample-level calibra-
tion can rectify each support sample based on its own
topMmost similar base classes.
3.2.2. Task-Level Calibration
By performing the sample-level calibration, the bias
induced by the few-shot support samples can be reduced
to a certain degree. However, when the sampling bias
is too large, e.g., the support sample is lying near the
boundary of a class, the set of similar base classes M
i
obtained by Eq. 3 may also be biased. To alleviate such
bias, we propose the task-level calibration which utilizes
the base prototypes related to all support samples whencalibrating each individual support feature. Concretely,
for a support set S=f(xi;yi)gNK
i=1w.r.t a taskT, we col-
lect the top Msimilar base prototypes for each support
sample and form a union of related base prototypes for
T:
T=NK[
i=1M
i: (8)
Then, for a transformed support sample Àú xiobtained
by Eq. 2, the calibration using all of the task-related
base prototypes is computed by:
ti=Àúxi+X
j2Twi jpb
j; (9)
where wi jis calculated in the similar way as Eq. 6, but
the similarities are computed using the prototypes from
Tinstead of M
i. By involving more prototypes to cal-
ibrate the support samples, the bias caused by only using
nearby prototypes for a near-boundary support sample
can be reduced.
Then, we deÔ¨Åne the task-level calibration for a nor-
malized support sample ¬Ø xias:
¬Øxt
i=normalize ((1 ) ¬Øxi+¬Øti); (10)
where ¬Øtiis the normalization of ti. Similar to the sample-
level calibration, ¬Ø xiand¬Øtialso form a line in the normal-
ized feature space, while the calibration for each support
sample is based on the union of all related base proto-
types T.
3.2.3. UniÔ¨Åed Model
The sample-level and task-level calibration utilize
dierent levels of information from the base classes to
rectify the support samples in a discrete manner. To fur-
ther attain the merits of both calibration schemes, we
propose a uniÔ¨Åed model which linearly combines the
sample-level and task-level calibration:
xc
i=¬Øxu
i=normalize ((1  ) ¬Øxi+¬Øsi+¬Øti):(11)
Here, ¬Ø xu
iwhich is also denoted as xc
i, is the Ô¨Ånal calibra-
tion for a normalized support sample ¬Ø xi. Geometrically,
xc
ican be understood as the normalization of an interpo-
lated feature point xu
ilocating in the triangle formulated
by the three vertices ¬Ø xi, ¬Øsiand¬Øti, while 1  ,and
are the barycentric coordinates of xu
i. Dierentand
values can lead to di erent calibration e ects. When
=0, the uniÔ¨Åed model degenerates to the sample-
level calibration, while when =0, the model becomes
to the task-level calibration. We quantitatively evaluate
the eects of di erentandvalues in Section 4.4.
63.3. Nearest Prototype ClassiÔ¨Åer
With the calibrated support set Sc=f(xc
i;yi)gNK
i=1, we
compute the prototypes fpngN
n=1for the novel classes and
perform cosine similarity based nearest classiÔ¨Åcation
for a query feature q. To simplify the notation, we fur-
ther represent Sc=fSc
ngN
n=1, while Sc
n=f(xc
k;yk=n)gK
k=1
is the support set for a novel class CLS n.
For the 1-shot case, each calibrated support sample
becomes one prototype and the class of the query fea-
ture is predicted by the nearest prototype classiÔ¨Åer:
y=max
pncos(¬Øq;pn); (12)
where pn=xc
nis the calibrated prototype for novel class
CLS nand ¬Øqis the normalization of query q.
For the multi-shot case, one way to obtain the pro-
totype for a novel class is simply to compute the av-
erage of all support features for the given class as in
Prototypical Networks [16]. However, merely using the
unweighted average of the support features as prototype
does not consider the importance of the support samples
w.r.t the query. Therefore, we adopt the idea of attentive
prototype which is proposed in recent works [51, 53] for
query-guided prototype computation. In our implemen-
tation, we deÔ¨Åne the attention-weighted prototype as:
pq
n=X
xc
k2Scnakxc
k; (13)
where a k=e<q;xc
k>
P
xcm2Scne<q;xcm>: (14)
Here, xc
kandxc
mare the calibrated support samples be-
longing to the CLS n‚Äôs support set Sc
nandakis the atten-
tion weight computed by applying Softmax to the sim-
ilarities between query qand these calibrated support
samples; pq
nis the CLS n‚Äôs prototype guided by query
q. Similar to Eq. 12, the prediction for a query qis
obtained by Ô¨Ånding the novel class with the nearest pro-
totype pq
n.
4. Experiments
In this section, we perform quantitative compar-
isons between our P3DC-Shot and state-of-the-art
few-shot classiÔ¨Åcation methods on three represen-
tative datasets. We also conduct ablation studies
on evaluating di erent hyperparameters and design
choices for our methods. Our code is available at:
https: //github.com /breakaway7 /P3DC-Shot.4.1. Datasets
We evaluate our prior-driven data calibration strate-
gies on three popular datasets for benchmarking few
shot classiÔ¨Åcaiton: miniImageNet [2], tieredImageNet
[39] and CUB [40]. miniImageNet and tieredImageNet
contain a broad range of classes including various an-
imals and objects, while CUB is a more Ô¨Åne-grained
dataset that focuses on various species of birds.
SpeciÔ¨Åcally, the miniImageNet [2] is derived from
the ILSVRC-2012 [58] and it contains a subset of 100
classes, each of which consisting of 600 images. We
follow the split used in [18] and obtain 64 base, 16 val-
idation and 20 novel classes for miniImageNet. Comar-
ing to miniImageNet, the tieredImageNet [39] is a larger
subset of [58] which contains 608 classes and therefore
more challenging. We follow [39] and split the tiered-
ImageNet into 351, 97, and 160 classes for base, vali-
dation, and novel classes, respectively. For CUB [40], it
is the short name for Caltech-UCSD Birds 200 dataset,
which contains a total of 11,788 images covering 200
categories of di erent bird species. We split the CUB
dataset into 100 base, 50 validation and 50 novel classes
following [31]. Note that the set formed by the base
classes can also be regarded as the train set and the novel
classes correspond to the test set.
4.2. Implementation Details
For each image in the dataset, we represent it as a
640-dimensional feature vector which is extracted us-
ing the WideResNet [59] pretrained by the S2M2 [29]
work. Our calibration pipeline can e ciently proceed
in four steps: 1) Ô¨Ånd the M=5 nearby base prototypes
for each support sample xi; 2) compute the endpoint
of the sample-level calibration for xi, i.e., si; 3) col-
lect all nearby base prototypes for all support samples
in the task and compute the endpoint of the task-level
calibration for xi, i.e., ti; 4) combine the sample-level
and task-level calibration and obtain the Ô¨Ånal calibrated
support sample xc
i. The parameter andfor weighting
the sample-level and task-level calibration are selected
based on the best results obtained on the validation set
for each dataset. All experiments are conducted on a
PC with a 2.70GHz CPU and 16G memory. No GPU
is needed during the calibration. On average, for a 5-
way 5-shot task, it takes 0.027 seconds to calibrate the
support samples and 0.002 seconds for performing the
nearest prototype classiÔ¨Åcation.
4.3. Comparison and Evaluation
To evaluate the performance of our P3DC-Shot, we
Ô¨Årst conduct quantitative comparisons with some rep-
resentative and state-of-the-art few-short classiÔ¨Åcation
7Table 1: Quantitative comparison on the test set of miniImageNet, tieredImageNet and CUB. The 5-way 1-shot and 5-way 5-shot classiÔ¨Åcation
accuracy (%) with 95% conÔ¨Ådence intervals are measured. Best results are highlighted in bold and second best are in italic. The last line shows the
andselected based on the valiation set for each dataset. * 8 and 20 are the number of ensembles in DeepV oro and DeepV oro ++.yThe results
of [54] on tieredImageNet are obtained using its released code.
MethodsminiImageNet tieredImageNet CUB
5-way 1-shot 5-way 5-shot 5-way 1-shot 5-way 5-shot 5-way 1-shot 5-way 5-shot
Meta-learning (metric-learning)
MatchingNet [15] (2016) 64:030:20 76:320:16 68:500:92 80:600:71 73:490:89 84:450:58
ProtoNet [16] (2017) 54:160:82 73:680:65 65:650:92 83:400:65 72:990:88 86:640:51
RelationNet [17] (2018) 52:190:83 70:200:66 54:480:93 71:320:78 68:650:91 81:120:63
Meta-learning (optimization)
MAML [19] (2017) 48:701:84 63:100:92 51:671:81 70:300:08 50:450:97 59:600:84
LEO [21] (2019) 61:760:08 77:590:12 66:330:15 81:440:09 68:220:22 78:270:16
DCO [22] (2019) 62:640:61 78:630:46 65:990:72 81:560:53 - -
Transfer learning
Baseline ++[31] (2019) 57:530:10 72:990:43 60:980:21 75:930:17 70:400:81 82:920:78
Negative-Cosine [57] (2020) 62:330:82 80:940:59 - - 72:660:85 89:400:43
S2M2 R[29] (2020) 64:650:45 83:200:30 68:120:52 86:710:34 80:140:45 90:990:23
Nearest neighbor
SimpleShot [25] (2019) 64:290:20 81:500:14 71:320:22 86:660:15 - -
DeepV oro(8)[50] (2022) 66:450:44 84.550.29 74:020:49 88.900.29 80:980:44 91.470.22
DeepV oro ++(20)[50] (2022) 68:380:46 83:270:31 74.480.50 - 80:700:45 -
Data calibration
RestoreNet [35] (2020) 59:280:20 - - - 74:320:91 -
DC [38] (2021) 67:790:45 83:690:31 74:240:50 88:380:31 79:930:46 90:770:24
MCL-Katz +PSM [37] (2022) 67:03 84 :03 69:90 85 :08 85.89 93.08
S2M2 +TCPRy[54] (2022) 68:050:41 84.510.27 72:670:48 87:960:31 - -
P3DC-Shot ( =0,=0) 65:930:45 84:060:30 73:560:49 88:500:32 81:610:43 91:360:22
P3DC-Shot ( =1,=0) 68:410:44 83:060:32 74:840:49 88:010:33 81:510:44 90:830:24
P3DC-Shot ( =0,=1) 68.670.44 83:640:31 75.200.48 88:290:33 81:580:44 91:020:23
P3DC-Shot ( =1
3,=1
3) 68:330:44 84:190:30 74.910.49 88:540:32 81:750:43 91:210:23
P3DC-Shot (selected ,) 68.680.44 84:370:30 75.200.48 88.670.32 81.860.43 91:360:23
(0:0;0:9) (0:0;0:4) (0:0;1:0) (0:0;0:3) (0:2;0:4) (0:0;0:4)
methods. Then, we compare with di erent data trans-
formation or calibration schemes and provide qualita-
tive visualization for showing the di erence of our cali-
bration results w.r.t existing works. In addition, we eval-
uate the generalizability of our method by performing
classiÔ¨Åcation tasks with di erent di culties.
Quantitative comparisons. As there are numerous
eorts have been paid to the few-shot classiÔ¨Åcation,
we mainly compare our P3DC-Shot with representative
and SOTA works which cover di erent types of few-
shot learning schemes. The compared methods include
the metric-learning based meta-learning [15, 16, 17],
optimization-based meta-learning [19, 21, 22], transfer
learning [31, 57, 29], nearest neighbor [25, 50] and cal-
ibration [35, 38, 37, 54] based methods. For certain
methods such as [29, 28], we only compare with their
basic versions and do not consider their model trained
with data augmentation. Note that as not every method
has conducted experiments on all three datasets, wemainly compare with their reported results. One excep-
tion is for [54], we compare with its results generated
using its released code.
For our method, we report the results of our model
with di erent hyperparameters and. In particular,
we consider the case when andare both zero, which
makes our method a simple NN-based method with no
data calibration and only shows the e ect for using the
query-guided prototype computation (Eq. 13). We also
compare with the results of oris 1, or both of them
are equal to1
3, which correspond to the cases that the
endpoint of the sample-level or task-level calibration or
the barycenter of the calibration triangle (Figure 2). In
the end, we provide our best results with the orse-
lected based on the validation set.
For each dataset, we evaluate on the 5-way 1-shot
and 5-way 5-shot classiÔ¨Åcation setting. For each set-
ting, 2,000 testing tasks, each of which contains 5 K
(K=1or5) samples for the support set and 5 15
8‚Ä¢‚Ä¢ Ôºé 
Âè£ÔøΩÊú¨
¬∑.. -¬≠ÔºéÔºé.ÔºåÔºåÔºå ÔºÇ ‚Ä¢‚Ä¢ ¬∑Êä§h.ÔºéÂøÉ
..‰∫Ü'..:.-ÔøΩ ÔΩûÔºéÊ≤ÅÔºöÔºéÔºé‚ÄúÂøÉ..¬∞'‚Äú)Ôºé‰∏Ä..,... 
¬∑---‚ñ†‰∏Ä‰∏Ä-.‰∏ÄÊä§Ôºé Ôºø Ôºé I I Ôºé ... Ôºé Ôºé Ôºé ‚Ä¢. lJ. Ôºé Ôºé ‚Ä¢ Ôºé Ôºé .1 
ÔøΩ 
-.. Ôºé ÔºéÔºé Ôºé 
ÔºéÔºé Ôºé Ôºé Ôºé ‚Ä¢‚Ä¢ Ê∞î
Ôºé 
ÂÆÉv
‰∫å
,‚ñ°Â•≥ÔºéÔºé Ôºé Ôºé ÁÇâ
Â•≥Ôºé 
Ôºé Â•≥
0v Ôºé 
¬∑`i.ÔºéÔºé Ôºé Ôºé Ôºé Ôºé Ôºé Ôºé ËßÖÔºé Ôºé Ôºé 
ÁÇâ‚Ä¢‚Ä¢ÔºéÔºé ÔºáÔºå ÔºéÔºé Ôºé Ôºé Ôºé Ôºé .,.Ôºé‚Ä¢‚Ä¢ 
(a) (b) (c)Figure 3: T-SNE visualization of the calibration on example support samples from the test set of miniImageNet (a), tieredImageNet (b), and CUB
(c). The colored dots are data from the same underlying classes as the selected sample and the star is the center of each class. Given a support
sample (represented in square), the upside down triangle is our calibration result and the lozenge is the calibration result of DC [38].
samples for the query set, are randomly generated from
the test split of the corresponding dataset. Table 1 shows
the quantitative comparison results on three datasets. It
can be seen that our best results outperform most meth-
ods in the 5-way 1-shot setting and are comparable to
the SOTA methods [28, 38] for the 5-way 5-shot set-
ting. Note that although [37] achieves best results on the
CUB dataset, it is inferior on miniImageNet and tiered-
ImageNet. Moreover, since [37] follows a metric-based
few-shot learning pipeline, it still requires to train the
feature extractor and the metric module for each dataset.
For [28], it performs generally well on all three datasets,
but as an ensemble-based method, its computation time
is much longer than our method, especially when the
ensemble number is large. In contrast, our method does
not require any training and only needs to perform an
ecient calibration step for each testing task.
Also, from results of our method with di erentand
values in Table 1, it can be found when andis
zero, the query-guided prototype computation can lead
to better performance than the simple NN-based Sim-
pleShot [25]. When either the sample-level or task-level
calibration is applied, i.e., oris not zero, the results
are better than the non-calibrated version, showing the
calibration can indeed reduce the bias for the support
samples. Meanwhile, which calibration type is more
suitable is depending on the underlying data distribu-
tion of the dataset. By selecting the andbased on
the validation set of each dataset, the results are further
improved. In the ablation study, we perform more ex-
periments and analysis of di erentandvalues.
Comparison with di erent data transformation or
calibration schemes. To further verify the e ectivenessTable 2: Comparison with di erent data transformation or calibration
schemes. Accuracy (%) for 5-way 1-shot task on the test set of mini-
ImageNet are measured.
ModelminiImageNet CUB
5-way 1-shot 5-way 1-shot
NN 47:50 76 :40
L2N+NN 65:93 81 :61
CL2N +NN 65:96 81 :54
DC+L2N+NN 66:23 79 :49
P3DC-Shot 68.68 81.86
(selected,) (0.0,0.9) (0.2,0.4)
of our prior-driven data calibration, we compare with
several NN-based baseline methods which perform dif-
ferent data transformation or calibration schemes and
the results are shown in Table 2. In this experiment, all
methods are based on the pretrained WideResNet fea-
tures. Also, only the 5-way 1-shot classiÔ¨Åcation ac-
curacy is measured so that the comparison is focused
on feature transformation instead of the prototype com-
putation schemes. The Ô¨Årst baseline is NN, which is
a naive inner product based nearest neighbor classiÔ¨Åer.
Then, L2N and CL2N represent L2 normalization and
centered L2 normalization which have been shown as
eective in SimpleShot [25]. In addition, another base-
line that follows the data calibration scheme in DC [38]
is compared. Comparing to the original DC, this base-
line directly takes the calibrated and then normalized
features and employs NN for classiÔ¨Åcation instead of
training new classiÔ¨Åers using the sampled data. From
Table 2, it can be observed the data normalization or cal-
ibration can signiÔ¨Åcantly improve the NN-based classi-
9Table 3: Generalizability test on di erent N in N-way 1-shot tasks. Accuracy (%) on the test set of miniImageNet are measured. For our P3DC-Shot,
the same=0 and=0:9 selected based on the validation set for the 5-way 1-shot case are used for all experiments.
Models 5-way 7-way 9-way 11-way 13-way 15-way 20-way
RestroreNet [35] 59:56 50:55 44:54 39:98 36:34 33:52 28:48
L2N+NN 65:93 57:86 52:45 48:25 44:80 42:12 37:06
CL2N +NN 65:96 57:69 52:23 47:93 44:36 41:85 36:65
P3DC-Shot 68:68 60:58 55:03 50:75 47:21 44:43 39:33
Ô¨Åcation. In addition, our data calibration achieves the
best results comparing to other baselines. The main rea-
son is the L2N and CL2N only perform transformation
rather than calibration using the base priors, while the
modiÔ¨Åed DC does not consider the attentive similarity
between the support samples and the base classes when
performing the calibration.
Visualization of the calibration. To qualitatively
verify the e ectiveness of our calibration, we show the
T-SNE [60] visualization of the calibration results for
some example support samples in Figure 3. The results
of calibrating the same sample using DC [38] are also
compared. It can be seen from Figure 3 that our calibra-
tion can more e ectively transform the support samples
closer to the center of the underlying classes. For DC,
the calibration may be minor or even be far away from
the center. The reason is still due to it treats the nearby
base classes with the same weights. In contrast, our cal-
ibration pays more attention to the similar base classes
when determining the weights for combining the base
prototypes (Eq. 5 and 9).
Generalizability test on di erent N in N-way clas-
siÔ¨Åcation. Following [35], we conduct a series of N-
way 1-shot experiments on miniImageNet to test the
generalizability of the proposed calibration for di er-
ent classiÔ¨Åcation tasks. Table 3 shows the results of the
baseline methods [35], L2N and CL2N and ours. Note
that with the N increases, there are more data samples in
a test task and the classiÔ¨Åcation becomes more di cult.
It can be observed that our P3DC-Shot achieves con-
sistent best results comparing to the baseline methods,
verifying our method is generalizable to classiÔ¨Åcation
tasks with di erent di culties.
4.4. Ablation Study
In this section, we perform ablation studies to ver-
ify the e ectiveness of di erent modules and design
choices of our method. First, we conduct experiments
on di erent hyperparameter andto see how the
sample-level and task-level calibration can a ect the Ô¨Å-
nal results. Then, we perform the study on the e ec-tiveness of using the query-guided attentive prototypes
in the NN classiÔ¨Åcation step.
Eect on di erent hyperparameter ,.Dier-
entandvalues correspond to di erent degrees of
sample-level and task-level calibration applied to the in-
put data. Geometrically, ,and 1  can also be
understood as the coordinates of the calibration result
w.r.t to the triangle formed by the three points ¬Ø xi;si;ti.
To quantitatively reveal how these two hyperparameters
can a ect the results, we enumerate di erentand
values on both the validation and test sets of di erent
datasets. From the results in Figure 4, it can be found
the accuracy near the origin of the Ô¨Ågures are smaller,
which means performing calibration can improve upon
using the original features for classiÔ¨Åcation, i.e., and
is zero. Also, di erent datasets prefer di erentand
combinations for achieving higher performance. For
example, miniImageNet shows better results when +
is around 0.9 and CUB prefers a relatively smaller cal-
ibration, i.e., +is around 0.6. For tieredImageNet,
better results are obtained around the topper left of the
Ô¨Ågure, showing the task-level calibration is more help-
ful than the sample-level. Overall, the trend on the test
set is consistent with the validation set. From above ex-
periments, it shows the sample-level and task-level cali-
bration are consistently e ective, while how to selecting
the goodandvalues are dataset dependent. There-
fore, for our best results, we use the andselected
based on the validation set and report their performance
on the test set.
Eect on using attentive prototypes in NN classiÔ¨Å-
cation. To improve the conventional prototype based
NN classiÔ¨Åcaiton, we propose to compute the query-
guided attentive prototypes to represent the support
class. To verify the e ectiveness of this scheme, we per-
form ablation study for 5-way 5-shot tasks on di erent
tasks using di erent prototype computation schemes.
SpeciÔ¨Åcally, we take the calibrated support features
and compute the prototypes for the support classes by
performing the conventional average operation or our
query-guided attentive averaging (Eq. 13). The results
10Figure 4: The e ect of di erentandon the validation (top) and test (bottom) set of di erent datasets. Accuracy (%) for 5-way 1-shot task on
miniImageNet, tieredImageNet and CUB are measured. The warmer color corresponds to higher accuracy.
Table 4: Ablation study on using the query-guided attentive proto-
types in NN classiÔ¨Åcation. Accuray (%) on the test set of miniIma-
geNet, tieredImageNet and CUB are measured.
ModelminiImageNet tieredImageNet CUB
5-way 5-shot 5-way 5-shot 5-way 5-shot
Average 84:11 88 :54 91 :27
Attentive 84:37 88 :67 91 :36
in Table 4 show that the attentive prototypes can lead to
better performance. Hence, we adopt the attentive pro-
totypes in our NN-based classiÔ¨Åcation.
5. Conclusion
In this paper, we propose a simple yet e ective frame-
work, named P3DC-Shot, for few-shot classiÔ¨Åcation.
Without any retraining and expensive computation, our
prior-driven discrete data calibration method can e -
ciently calibrate the support samples based on prior-
information from the base classes to obtain the less-
biased support data for NN-based classiÔ¨Åcation. Exten-
sive experiments show that our method can outperformor at least comparable to SOTA methods which need ad-
ditional learning steps or more computation. One lim-
itation of our method is we rely on the whole valida-
tion set to select the good hyperparameters andto
determine which degree of the sample-level and task-
level calibration is more suitable for the given dataset.
Investigating a more general scheme to combine the
sample-level and task-level calibration is an interesting
future work. Moreover, when exploring the combina-
tion schemes, we only focus on exploring the inner area
of the calibration triangle. It is worthy to extend the
parameter search to a larger area, i.e., by extrapolation
of the calibration triangle, to Ô¨Ånd whether better results
can be obtained.
References
[1] K. Simonyan, A. Zisserman, Very deep convolutional net-
works for large-scale image recognition, arXiv preprint
arXiv:1409.1556 (2014).
[2] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,
Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al., Ima-
genet large scale visual recognition challenge, Int. J. Comput.
Vis. 115 (2015) 211‚Äì252.
11[3] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for
image recognition, in: IEEE Conf. Comput. Vis. Pattern Recog.,
2016, pp. 770‚Äì778.
[4] R. Girshick, Fast r-cnn, in: Int. Conf. Comput. Vis., 2015, pp.
1440‚Äì1448.
[5] S. Ren, K. He, R. Girshick, J. Sun, Faster r-cnn: Towards real-
time object detection with region proposal networks, Adv. Neu-
ral Inform. Process. Syst. 28 (2015).
[6] J. Redmon, S. Divvala, R. Girshick, A. Farhadi, You only look
once: UniÔ¨Åed, real-time object detection, in: IEEE Conf. Com-
put. Vis. Pattern Recog., 2016, pp. 779‚Äì788.
[7] J. Long, E. Shelhamer, T. Darrell, Fully convolutional networks
for semantic segmentation, in: IEEE Conf. Comput. Vis. Pattern
Recog., 2015, pp. 3431‚Äì3440.
[8] K. He, G. Gkioxari, P. Doll ¬¥ar, R. Girshick, Mask r-cnn, in: Int.
Conf. Comput. Vis., 2017, pp. 2961‚Äì2969.
[9] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, A. L.
Yuille, Deeplab: Semantic image segmentation with deep con-
volutional nets, atrous convolution, and fully connected crfs,
IEEE Trans. Pattern Anal. Mach. Intell. 40 (2017) 834‚Äì848.
[10] T.-Y . Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra-
manan, P. Doll ¬¥ar, C. L. Zitnick, Microsoft coco: Common ob-
jects in context, in: Eur. Conf. Comput. Vis., 2014, pp. 740‚Äì755.
[11] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler,
R. Benenson, U. Franke, S. Roth, B. Schiele, The cityscapes
dataset for semantic urban scene understanding, in: IEEE Conf.
Comput. Vis. Pattern Recog., 2016, pp. 3213‚Äì3223.
[12] Y . Wang, Q. Yao, J. T. Kwok, L. M. Ni, Generalizing from a few
examples: A survey on few-shot learning, ACM Comput Surv
53 (2020) 1‚Äì34.
[13] J. Lu, P. Gong, J. Ye, C. Zhang, Learning from very few sam-
ples: A survey, arXiv preprint arXiv:2009.02653 (2020).
[14] G. Huang, I. Laradji, D. V ¬¥azquez, S. Lacoste-Julien, P. Ro-
driguez, A survey of self-supervised and few-shot object de-
tection, IEEE Trans. Pattern Anal. Mach. Intell. (2022).
[15] O. Vinyals, C. Blundell, T. Lillicrap, D. Wierstra, et al., Match-
ing networks for one shot learning, Adv. Neural Inform. Pro-
cess. Syst. 29 (2016).
[16] J. Snell, K. Swersky, R. Zemel, Prototypical networks for few-
shot learning, Adv. Neural Inform. Process. Syst. 30 (2017).
[17] F. Sung, Y . Yang, L. Zhang, T. Xiang, P. H. Torr, T. M.
Hospedales, Learning to compare: Relation network for few-
shot learning (2018) 1199‚Äì1208.
[18] S. Ravi, H. Larochelle, Optimization as a model for few-shot
learning (2016).
[19] C. Finn, P. Abbeel, S. Levine, Model-agnostic meta-learning for
fast adaptation of deep networks, in: Int. Conf. Mach. Learn.,
PMLR, 2017, pp. 1126‚Äì1135.
[20] M. A. Jamal, G.-J. Qi, Task agnostic meta-learning for few-shot
learning, in: IEEE Conf. Comput. Vis. Pattern Recog., 2019.
[21] A. A. Rusu, D. Rao, J. Sygnowski, O. Vinyals, R. Pascanu,
S. Osindero, R. Hadsell, Meta-learning with latent embedding
optimization, in: Int. Conf. Learn. Represent., 2019.
[22] K. Lee, S. Maji, A. Ravichandran, S. Soatto, Meta-learning with
dierentiable convex optimization, in: IEEE Conf. Comput. Vis.
Pattern Recog., 2019, pp. 10657‚Äì10665.
[23] Y . Chen, X. Wang, Z. Liu, H. Xu, T. Darrell, A new meta-
baseline for few-shot learning, arXiv preprint arXiv:2003.11539
(2020).
[24] T. Cao, M. T. Law, S. Fidler, A theoretical analysis of the num-
ber of shots in few-shot learning, in: Int. Conf. Learn. Repre-
sent., 2019.
[25] Y . Wang, W.-L. Chao, K. Q. Weinberger, L. van der Maaten,
Simpleshot: Revisiting nearest-neighbor classiÔ¨Åcation for few-
shot learning, arXiv preprint arXiv:1911.04623 (2019).[26] F. Aurenhammer, V oronoi diagrams‚Äîa survey of a fundamental
geometric data structure, ACM Comput Surv 23 (1991) 345‚Äì
405.
[27] D. Z. Chen, Z. Huang, Y . Liu, J. Xu, On clustering induced
voronoi diagrams, SIAM Journal on Computing 46 (2017)
1679‚Äì1711.
[28] C. Ma, Z. Huang, M. Gao, J. Xu, Few-shot learning as cluster-
induced voronoi diagrams: A geometric approach, in: Int. Conf.
Learn. Represent., 2022.
[29] P. Mangla, N. Kumari, A. Sinha, M. Singh, B. Krishnamurthy,
V . N. Balasubramanian, Charting the right manifold: Manifold
mixup for few-shot learning, in: WACV , 2020, pp. 2218‚Äì2227.
[30] Y . Tian, Y . Wang, D. Krishnan, J. B. Tenenbaum, P. Isola, Re-
thinking few-shot image classiÔ¨Åcation: a good embedding is all
you need?, in: Eur. Conf. Comput. Vis., Springer, 2020, pp.
266‚Äì282.
[31] W.-Y . Chen, Y .-C. Liu, Z. Kira, Y .-C. F. Wang, J.-B. Huang,
A closer look at few-shot classiÔ¨Åcation, in: Int. Conf. Learn.
Represent., 2019.
[32] W. Ge, Y . Yu, Borrowing treasures from the wealthy: Deep
transfer learning through selective joint Ô¨Åne-tuning, in: IEEE
Conf. Comput. Vis. Pattern Recog., 2017.
[33] O. Sbai, C. Couprie, M. Aubry, Impact of base dataset design on
few-shot image classiÔ¨Åcation, Eur. Conf. Comput. Vis. (2020).
[34] L. Zhou, P. Cui, X. Jia, S. Yang, Q. Tian, Learning to select base
classes for few-shot classiÔ¨Åcation, in: IEEE Conf. Comput. Vis.
Pattern Recog., 2020, pp. 4624‚Äì4633.
[35] W. Xue, W. Wang, One-shot image classiÔ¨Åcation by learning to
restore prototypes, in: AAAI, volume 34, 2020, pp. 6558‚Äì6565.
[36] J. Liu, L. Song, Y . Qin, Prototype rectiÔ¨Åcation for few-shot
learning, in: Eur. Conf. Comput. Vis., Springer, 2020, pp. 741‚Äì
756.
[37] Y . Guo, R. Du, X. Li, J. Xie, Z. Ma, Y . Dong, Learning cali-
brated class centers for few-shot classiÔ¨Åcation by pair-wise sim-
ilarity, IEEE Trans. Image Process. 31 (2022) 4543‚Äì4555.
[38] S. Yang, L. Liu, M. Xu, Free lunch for few-shot learning: Dis-
tribution calibration, in: Int. Conf. Learn. Represent., 2021.
[39] M. Ren, E. TriantaÔ¨Ållou, S. Ravi, J. Snell, K. Swersky, J. B.
Tenenbaum, H. Larochelle, R. S. Zemel, Meta-learning for
semi-supervised few-shot classiÔ¨Åcation, in: Int. Conf. Learn.
Represent., 2018.
[40] C. Wah, S. Branson, P. Welinder, P. Perona, S. Belongie, The
caltech-ucsd birds-200-2011 dataset (2011).
[41] T. Hospedales, A. Antoniou, P. Micaelli, A. Storkey, Meta-
learning in neural networks: A survey 44 (2021) 5149‚Äì5169.
[42] G. Koch, R. Zemel, R. Salakhutdinov, et al., Siamese neural net-
works for one-shot image recognition, in: ICML deep learning
workshop, 2015.
[43] W. Xu, Y . Xu, H. Wang, Z. Tu, Attentional constellation nets
for few-shot learning, in: Int. Conf. Learn. Represent., 2021.
[44] Y . Liu, T. Zheng, J. Song, D. Cai, X. He, Dmn4: Few-shot learn-
ing via discriminative mutual nearest neighbor neural network,
in: AAAI, volume 36, 2022, pp. 1828‚Äì1836.
[45] L. Torrey, J. Shavlik, Transfer learning, in: Handbook of re-
search on machine learning applications and trends: algorithms,
methods, and techniques, IGI global, 2010, pp. 242‚Äì264.
[46] C. Tan, F. Sun, T. Kong, W. Zhang, C. Yang, C. Liu, A survey on
deep transfer learning, in: International conference on artiÔ¨Åcial
neural networks, Springer, 2018, pp. 270‚Äì279.
[47] F. Zhuang, Z. Qi, K. Duan, D. Xi, Y . Zhu, H. Zhu, H. Xiong,
Q. He, A comprehensive survey on transfer learning, Proceed-
ings of the IEEE 109 (2020) 43‚Äì76.
[48] G. S. Dhillon, P. Chaudhari, A. Ravichandran, S. Soatto, A
baseline for few-shot image classiÔ¨Åcation, in: Int. Conf. Learn.
Represent., 2020.
12[49] W. Li, L. Wang, J. Xu, J. Huo, Y . Gao, J. Luo, Revisiting local
descriptor based image-to-class measure for few-shot learning,
in: IEEE Conf. Comput. Vis. Pattern Recog., 2019, pp. 7260‚Äì
7268.
[50] C. Ma, Z. Huang, M. Gao, J. Xu, Few-shot learning as cluster-
induced voronoi diagrams: A geometric approach (2022).
[51] F. Wu, J. S. Smith, W. Lu, C. Pang, B. Zhang, Attentive proto-
type few-shot learning with capsule network-based embedding,
in: Eur. Conf. Comput. Vis., 2020, pp. 237‚Äì253.
[52] Z. Ji, X. Chai, Y . Yu, Z. Zhang, Reweighting and information-
guidance networks for few-shot learning, Neurocomputing 423
(2021) 13‚Äì23.
[53] X. Wang, J. Meng, B. Wen, F. Xue, Racp: A network with
attention corrected prototype for few-shot speaker recognition
using indeÔ¨Ånite distance metric, Neurocomputing 490 (2022)
283‚Äì294.
[54] J. Xu, X. Luo, X. Pan, W. Pei, Y . Li, Z. Xu, Alleviating the sam-
ple selection bias in few-shot learning by removing projection
to the centroid, in: Adv. Neural Inform. Process. Syst., 2022.
[55] L. Zhou, P. Cui, S. Yang, W. Zhu, Q. Tian, Learning to learn
image classiÔ¨Åers with visual analogy, in: IEEE Conf. Comput.
Vis. Pattern Recog., 2019, pp. 11497‚Äì11506.
[56] J. W. Tukey, et al., Exploratory data analysis, volume 2, Read-
ing, MA, 1977.
[57] B. Liu, Y . Cao, Y . Lin, Q. Li, Z. Zhang, M. Long, H. Hu, Nega-
tive margin matters: Understanding margin in few-shot classiÔ¨Å-
cation, in: Eur. Conf. Comput. Vis., Springer, 2020.
[58] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, L. Fei-Fei, Im-
agenet: A large-scale hierarchical image database, in: IEEE
Conf. Comput. Vis. Pattern Recog., Ieee, 2009, pp. 248‚Äì255.
[59] S. Zagoruyko, N. Komodakis, Wide residual networks, arXiv
preprint arXiv:1605.07146 (2016).
[60] L. van der Maaten, G. E. Hinton, Visualizing data using t-sne,
Journal of Machine Learning Research 9 (2008) 2579‚Äì2605.
13