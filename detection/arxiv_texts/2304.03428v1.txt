TinyDet: Accurate Small Object Detection in Lightweight Generic Detectors?
Shaoyu Chen1, Tianheng Cheng1, Jiemin Fang2, Qian Zhang3, Yuan Li4, Wenyu Liu1, Xinggang Wang1,y
1School of EIC, Huazhong University of Science and Technology
2Institute of AI, Huazhong University of Science and Technology
3Horizon Robotics4Google
Source code and pretrained models are available at: hustvl/TinyDet
Abstract
Small object detection requires the detection head to
scan a large number of positions on image feature maps,
which is extremely hard for computation- and energy-
efﬁcient lightweight generic detectors. To accurately de-
tect small objects with limited computation, we propose a
two-stage lightweight detection framework with extremely
low computation complexity, termed as TinyDet. It enables
high-resolution feature maps for dense anchoring to better
cover small objects, proposes a sparsely-connected convo-
lution for computation reduction, enhances the early stage
features in the backbone, and addresses the feature mis-
alignment problem for accurate small object detection. On
the COCO benchmark, our TinyDet-M achieves 30:3AP
and13:5APswith only 991MFLOPs, which is the ﬁrst de-
tector that has an AP over 30with less than 1GFLOPs;
besides, TinyDet-S and TinyDet-L achieve promising per-
formance under different computation limitation.
1. Introduction
Object detection plays an important role in computer vi-
sion, and gradually becomes the technical foundation of
many applications, such as autonomous driving, remote
sensing and video surveillance. However, the inference pro-
cedure of advanced detection models costs massive compu-
tational resources, which makes them hard to be applied on
resource-constrained mobile or edge devices. To widely ap-
ply artiﬁcial intelligence (AI), “Tiny AI” models that are
both computation-efﬁcient and energy-efﬁcient are becom-
ing more and more popular. In this paper, we design a
lightweight generic object detection framework, termed as
?A letter version of this paper is published in SCIENCE CHINA
Information Sciences (SCIS) with doi.org/10.1007/s11432-021-3504-4.
Please cite the SCIS version.yCorresponding author: Xinggang Wang
(xgwang@hust.edu.cn ).
elephant: 0.99elephant: 0.96elephant: 0.95elephant: 0.73elephant: 0.51(a) ThunderNet
(b) Ours
elephant: 0.99elephant: 0.98elephant: 0.97elephant: 0.95elephant: 0.91elephant: 0.81elephant: 0.74elephant: 0.71elephant: 0.630 500 1000 15002000 2500 3000MFLOPs202224262830323436mAPTinyHRDet-MTinyHRDet-L
TinyHRDet-STiny-DSODSSDLiteEfficientDet-D0
PeleeNetThunderNet-535NAS-FPNLiteThunderNet-146
elephant: 0.99elephant: 0.96elephant: 0.95elephant: 0.73elephant: 0.51(a) ThunderNet
(b) Ourselephant: 0.99elephant: 0.98elephant: 0.97elephant: 0.95elephant: 0.91elephant: 0.81elephant: 0.74elephant: 0.71elephant: 0.630 500 1000 15002000 2500 3000MFLOPs202224262830323436mAPTinyHRDet-MTinyHRDet-L
TinyHRDet-STiny-DSODSSDLiteEfficientDet-D0
PeleeNetThunderNet-535NAS-FPNLiteThunderNet-146
elephant: 0.99elephant: 0.96elephant: 0.95elephant: 0.73elephant: 0.51(a) ThunderNet
(b) Ourselephant: 0.99elephant: 0.98elephant: 0.97elephant: 0.95elephant: 0.91elephant: 0.81elephant: 0.74elephant: 0.71elephant: 0.630 500 1000 15002000 2500 3000MFLOPs202224262830323436mAPTinyHRDet-MTinyHRDet-L
TinyHRDet-STiny-DSODSSDLiteEfficientDet-D0
PeleeNetThunderNet-535NAS-FPNLiteThunderNet-146
elephant: 0.99elephant: 0.96elephant: 0.95elephant: 0.73elephant: 0.51(a) ThunderNet
(b) Ourselephant: 0.99elephant: 0.98elephant: 0.97elephant: 0.95elephant: 0.91elephant: 0.81elephant: 0.74elephant: 0.71elephant: 0.630 500 1000 15002000 2500 3000MFLOPs202224262830323436mAPTinyHRDet-MTinyHRDet-L
TinyHRDet-STiny-DSODSSDLiteEfficientDet-D0
PeleeNetThunderNet-535NAS-FPNLiteThunderNet-146elephant: 0.99elephant: 0.96elephant: 0.95elephant: 0.73elephant: 0.51(a) ThunderNet
(b) Ourselephant: 0.99elephant: 0.98elephant: 0.97elephant: 0.95elephant: 0.91elephant: 0.81elephant: 0.74elephant: 0.71elephant: 0.630 500 1000 15002000 2500 3000MFLOPs202224262830323436mAPTinyHRDet-MTinyHRDet-L
TinyHRDet-STiny-DSODSSDLiteEfficientDet-D0
PeleeNetThunderNet-535NAS-FPNLiteThunderNet-146elephant: 0.99elephant: 0.96elephant: 0.95elephant: 0.73elephant: 0.51(a) ThunderNet
(b) Ourselephant: 0.99elephant: 0.98elephant: 0.97elephant: 0.95elephant: 0.91elephant: 0.81elephant: 0.74elephant: 0.71elephant: 0.630 500 1000 15002000 2500 3000MFLOPs202224262830323436mAPTinyHRDet-MTinyHRDet-L
TinyHRDet-STiny-DSODSSDLiteEfficientDet-D0
PeleeNetThunderNet-535NAS-FPNLiteThunderNet-146Figure 1. Model FLOPs vsCOCO Accuracy. Our TinyDet
achieves higher mAP with less computation cost compared with
other detectors.
TinyDet, for efﬁcient and accurate object detection, espe-
cially small objects, with low computation cost.
In recent years, many innovative and representative
lightweight detection models [6, 34, 38, 41, 44] have been
proposed for the better trade-off between the computation
cost and accuracy. To reduce the computation cost, they
usually downscale the input image by a large ratio and per-
form object detection on small feature maps. For example,
Pelee [44] takes 304304input and the largest feature map
used for detection in Pelee is 1919. ThunderNet [34]
takes 320320input, and only uses a single feature map
with the resolution of 2020for detection. For comparison,
large models like Faster R-CNN [37] with feature pyramid
network (FPN) [23] takes 8001333 input and the largest
feature map is as large as 200333. Performing object de-
tection with small input images and on small feature maps is
helpful to reducing computation cost. However, small fea-
ture maps have no detailed information and poor positional
resolution. Previous lightweight detectors have very limited
ability to detect small objects. They sacriﬁce the detection
performance of small objects for high efﬁciency.arXiv:2304.03428v1  [cs.CV]  7 Apr 2023The capacity of detecting small objects is of great im-
portance for object detection based applications and is a
key factor for evaluating an object detection model. As
demonstrated in the COCO dataset, approximately 41% ob-
jects are small (area <322) [25]. In this paper, we tar-
get at boosting the small object detection performance in
lightweight generic detection networks. Based on the good
practices of designing lightweight networks, we propose
TinyDet, which is a two-stage detector with high-resolution
(HR) feature maps for dense anchoring. HR feature maps
signiﬁcantly improve the small object detection ability but
also bring much more computation cost. To release the
contradiction between resolution and computation, we pro-
pose TinyFPN and TinyRPN by introducing a sparsely-
connected convolution (SCConv), which keeps both high
resolution and low computation. We improve the back-
bone network for better small object detection performance.
Small object detection relies more on detailed information
in shallow features. We keep more detailed information
by allocating more computation to the early stages. Be-
sides, we observe that severe feature misalignment exists
in lightweight detectors. The feature misalignment accu-
mulates layer by layer and is passed to the detection part,
affecting the precision of regression in both RPN and R-
CNN head. Small objects are much more sensitive to such
positional misalignment. By eliminating the misalignment,
the detection performance of small objects is signiﬁcantly
improved.
Our contributions can be summarized as:
• In lightweight detection networks, for the ﬁrst time, we
enable the high-resolution detection feature map ( i.e.,
8080) for dense anchoring, which is essential for
detecting small objects.
• We propose TinyFPN and TinyRPN with the sparsely-
connected convolution to perform efﬁcient object de-
tection with high-resolution detection feature maps.
• By enhancing the early stages in the backbone and ad-
dressing the misalignment problem in TinyDet, we fur-
ther improve the detection results of small objects.
• Our TinyDet models have strong performance and lit-
tle computation budget on the COCO test-dev2017 set
as shown in Fig. 1. TinyDet-M achieves 30.3 mAP
with only 991 MFLOPs, which is the state-of-the-art
result among lightweight detectors. Notably, the small
object detection performance is outstanding, the APs
of TinyDet-S and TinyDet-M are two times of that of
ThunderNets.
2. Related Work
Lightweight Object Detector Lightweight models for
generic object detection has witnessed rapid developmentsin recent years. Firstly, advances of lightweight classi-
ﬁcation network design methods [14, 15, 30, 38, 49] di-
rectly boost the development of lightweight object detec-
tion. The lightweight classiﬁcation networks are often di-
rectly adopted as backbones of detectors to extract fea-
tures, e.g., the hand-crafted MobileNetV2 [38] is used in
SSDLite [38] and the neural architecture searched Efﬁcient-
Net [40] is used in EfﬁcientDet [41]. But detection and clas-
siﬁcation need different backbones [22]. To better match
characteristics of detection tasks, in many lightweight de-
tectors [6, 34], specialized backbones are proposed, based
on existing classiﬁcation networks. Secondly, well devel-
oped pipelines [23,24,27,37] for object detection also build
a solid foundation for lightweight detector research. Most
lightweight detectors [36, 38, 41, 44] follow the compact
one-stage architecture. PeleeNet [44] is only built with
conventional convolutions without using the popular mo-
bile convolution. ReﬁneDetLite [6] makes design special-
ized for CPU-only devices. EfﬁcientDet [41] proposes a
weighted bi-directional feature pyramid network for easy
and fast feature fusion and builds a scalable detection ar-
chitecture across a wide spectrum of resource constraints.
Two-stage detectors, with more complicated pipelines, are
usually thought to be more time consuming in inference
phase. However, some [9,21,34] prove that two-stage detec-
tors can also be as efﬁcient as one-stage ones if the second
stage is made lightweight enough. The two-stage paradigm
tends to be better performing at detecting small objects.
Thus, we follow the two-stage paradigm to design our de-
tector by considering both efﬁciency and accuracy.
Small Object Detection Detecting small objects from the
video and image has a high-proﬁle in computer vision, re-
mote sensing, autonomous driving, etc.. Liu et al. [27] cre-
ates more small object training examples by reducing the
size of large objects. D-SSD [10], C-SSD [48] F-SSD [5]
and ION [2] focus on building appropriate context features
for small object detection. Hu et al. [17] makes use of a
coarse image pyramid and uses two times upsampled in-
put images to detect small faces. Several studies, such as
[1, 19, 31], use generative adversarial network (GAN) [13]
to generate super-resolved features for small object detec-
tion. Larger input resolution and super-resolution methods
bring much more computation cost and are not suitable for
lightweight detector design. Our work is slightly related
to high-resolution network (HRNet) [45], which maintains
a high-resolution representation in the whole network for
position-sensitive visual recognition tasks. We ﬁnd that
there are no papers that focus on detecting small objects
in lightweight generic detectors. In this paper, we target at
accurate small object detection while keeping low compu-
tation budget.Sparse Convolution Reducing the redundancy in deep
convolutional neural networks is an important direction to
explore when designing lightweight detection networks.
Many previous works dedicate to reduce the redundancy
by making the connections in convolution sparser. Liu et
al.[26] adopts sparse decompositions to get sparse convo-
lutional networks and lower the computational complexity.
The depth-wise separable convolution [8,15], a kind of fac-
torized convolution, is widely adopted for reducing compu-
tation and model size. And the group convolution is used
in [49] for efﬁcient model design. Our work combines the
depth-wise convolution and group convolution in a novel
form to build extremely lightweight FPN and RPN.
3. Method
In this section, we detail the design of TinyDet and il-
lustrate how a generic detector with low FLOPs performs
accurate small object detection. We follow the two-stage
detection paradigm, which is more friendly to small object
detection [27]. The detector contains four parts: the en-
hanced backbone, TinyFPN, TinyRPN and R-CNN head.
For clarity, we only provide details of TinyDet-M in this
section and leave the details of TinyDet-S and TinyDet-L in
Appendix.
3.1. Backbone Network
3.1.1 Detailed Information Enhancement
Early-stage feature maps with high resolution in the back-
bone contain abundant detailed information, which is vi-
tal for recognizing and localizing small objects. Existing
lightweight backbone networks [14, 15, 30, 38, 49] usually
downsample feature maps rapidly, keeping less layers and
channels in high resolution stages. This setting success-
fully minimizes computational budget but sacriﬁces much
detailed information.
To improve the performance of detecting small ob-
jects, we propose a detailed information enhanced back-
bone based on the high-performance network, Mo-
bileNetV3 [14]. Tab. 1 shows the detailed network conﬁgu-
ration. Compared with other widely used lightweight back-
bone networks [14,15,30,38,49], we allocate more compu-
tation to the early stages with higher resolution. With this
design, more detailed information is extracted and kept for
detecting small objects.
3.1.2 Solving the Feature Alignment Problem
Convolution with stride 2 is widely used to reduce the reso-
lution of the feature maps [14, 15, 30, 38, 49]. And in detec-
tion, even input resolution is a common practice. Because
odd input pixels ruin the proportional relation between dif-
ferent pyramid levels, making the coordinate mapping be-
Avg PoolingK=2, S=1ConvK=3, S=2ConvK=3, S=2(a)Conv(a)Pooling+ConvFigure 2. The feature alignment problem. (a) When the num-
ber of input pixels is even, due to spatial asymmetry, the strided
convolution causes a 0:5-pixel misalignment. (b) By introducing
an average pooling before the strided convolution, even pixels are
converted to odd ones, eliminating asymmetry and misalignment.
Dashed squares denote zero padding.
tween input and output complex. However, strided convo-
lutions on even input resolution may lead to feature mis-
alignment. As shown in Fig. 2(a), considering a convolu-
tion layer with stride 2 and the input resolution is even, dur-
ing the convolution computation, one padding pixel is ig-
nored, resulting in spatial asymmetry and 0:5pixel feature
misalignment. The feature misalignment caused by several
strided convolutions is accumulated layer by layer through
the whole network and becomes more signiﬁcant in higher
levels. The misalignment is negligible for large models with
high input resolution, but signiﬁcant for lightweight detec-
tors with low resolution. It severely degrades the perfor-
mance. The RoI pooling/align operation would extract mis-
aligned features for every object proposals. Small object de-
tection requires more accurate localization and are affected
more.
To alleviate the feature misalignment in TinyDet, we
adopt average pooling layers before each strided convolu-
tion. As illustrated in Fig. 2(b), the average pooling op-
eration converts even pixels to odd ones and avoids asym-
metry in strided convolution and correct the misalignment.
And when optimizing the network, the adjacent convolution
layer and averaging pooling layer can be fused into a single
layer for better inference efﬁciency.
Theoretical Calculation of Feature Misalignment We
provide the theoretical calculation about the feature mis-
alignment caused by strided convolution. For a convolu-
tion with stride 2, we assume that the kernel size is k, the
padding size isk 1
2, and the input feature map is with
stride scompared with the input image. As shown in
Fig. 2(a), one padding pixel is dropped, and the feature
center is shifted by 0:5pixel. When mapped to the input
image, the misalignment iss
2pixels. The feature misalign-
ment affects subsequent layers and is accumulated layer by
layer. In TinyDet, 6strided convolution layers exist in the
backbone network and respectively cause misalignment of
0:5;1;2;4;8;16pixels. In FPN and RPN, the accumulated
misalignment is as large as 31:5pixels. For lightweight de-Table 1. Speciﬁcation for the backbone of TinyDet-M. Bneck denotes the inverted residual bottleneck structure [38]. ExSize denotes
expansion size. SE denotes the squeeze-and-excitation module [16]. NL denotes the type of the used nonlinearity. HS denotes h-swish [14]
and RE denotes ReLU. FPN denotes whether the output of the block is fed into FPN. Enhancement denotes modiﬁcations compared with
the original MobileNetV3 and ”c” denotes channel.
Input Operator ExSize Out SE NL Stride FPN Enhancement
32023 conv2d, 33 - 24 - HS 2 - c + 50%
160224 bneck, 33 24 24 - RE 1 - c + 50%
160224 bneck, 33 72 36 - RE 2 - c + 50%
80236 bneck, 33 108 36 - RE 1 - added blocks
80236 bneck, 33 108 36 - RE 1 - added blocks
80236 bneck, 33 108 36 - RE 1 X c + 50%
80236 bneck, 55 108 60X RE 2 - c + 50%
40260 bneck, 55 180 60X RE 1 - c + 50%
40260 bneck, 55 180 60X RE 1 X c + 50%
40260 bneck, 33 240 80 - HS 2 - -
20280 bneck, 33 200 80 - HS 1 - -
20280 bneck, 33 184 80 - HS 1 - -
20280 bneck, 33 184 80 - HS 1 - -
20280 bneck, 33 480 112X HS 1 - -
202112 bneck, 33 672 112X HS 1 X -
202112 bneck, 55 672 160X HS 2 - -
102160 bneck, 55 960 160X HS 1 - -
102160 bneck, 55 960 160X HS 1 X -
102160 bneck, 55 960 160X HS 2 - -
52160 bneck, 55 960 160X HS 1 X -
(a)Withoutfeaturealignment
(b)Withfeaturealignment
Figure 3. Visualization of ERFs of different pyramid levels. From left to right: pyramid levels with stride 4, 8, 16, 32 and 64 in FPN. The
red point denotes the geometry center. The red square indicates the corresponding anchor size in each pyramid level.
tectors with small input size, the misalignment is quite sig-
niﬁcant. Considering the input resolution of 320320, the
misalignment proportion is up to31:5
3209:8%, which leads
to severe mismatching between features and their spatial po-
sitions.Visualization of Effective Receptive Field To better
demonstrate the effect of feature misalignment, we adopt
effective receptive ﬁeld (ERF) maps [29] to visualize the
misalignment of feature maps. As shown in Fig. 3(a), we
can observe that ERFs obviously deviate from the geometric
centers of corresponding anchors. Pixel-level prediction in
RPN and region-based feature extraction in R-CNN wouldbe based on misaligned features. With average pooling lay-
ers applied, the misalignment is eliminated (Fig. 3(b)).
3.2. TinyFPN and TinyRPN
3.2.1 High Resolution Detection Feature Maps
Under the limitation of computation cost, previous
lightweight detectors usually adopt feature maps with low
resolutions for detection ( 3838in SSDLite [38], 1919
in Pelee [44], 2020in ThunderNet [34]). However, small
feature maps have low spatial resolution. Low-resolution
feature maps cannot provide spatially matched features for
objects located in arbitrary positions, especially for small
objects. In this paper, we enable object detection on high-
resolution feature maps. We fetch ﬁve feature maps from
the backbone for detection, respectively with stride 4, 8, 16,
32 and 64. Note that the resolution of the feature map with
stride 4 is 8080, which is the highest resolution feature
map used in lightweight detectors. More analysis on the
importance of the high-resolution setting can be found in
Sec. 3.3.
3.2.2 Sparsely-connected Convolution for Computa-
tion Reduction
Due to the high-resolution design, computation budget of
the detection part becomes extremely high. Though the
depth-wise separable convolution [8, 15] has been widely
used for detection part design in prior lightweight detec-
tors [6,34,41] to reduce the computation cost, we ﬁnd it not
enough in our high-resolution setting. Consequently, we
exploit a sparsely-connected convolution (SCConv), spe-
cialized for both efﬁciency and high resolution in FPN and
RPN. As shown in Fig. 4, the SCConv is a combination of a
depth-wise convolution [15] and a point-wise group convo-
lution [49]. Compared with the vanilla depth-wise separable
convolution [8,15], SCConv further reduces the connections
among channels. Experiments in Sec. 4.3 shows that this
sparser setting has little inﬂuence on detection performance
and reduces the computation cost by a large amount.
Based on SCConv, we propose TinyFPN and TinyRPN.
In TinyFPN (Fig. 4), SCConv is applied after feature fu-
sion, in place of the normal 33convolutions. We set
larger group numbers, i.e.sparser connections, for SCConvs
in the high-resolution pyramid levels to reduce the compu-
tation cost. TinyRPN (Fig. 4) consists of a SCConv and two
sibling 11convolutions for classiﬁcation and regression
respectively. The parameters of TinyRPN are shared across
all pyramid levels. Ablation studies about the group settings
of SCConvs are provided in Sec. 4.3.3.3. High-resolution Feature Maps for Dense An-
choring
Translation variance is a challenge in object detection.
Detectors should be insensitive to translation and deal with
objects located in arbitrary positions. The introduction of
anchor [37] eases the problem of translation variance in ob-
ject detection. A large number of anchors are evenly tiled
over the whole image and each anchor is only responsible
for predicting objects that appear in a certain region, which
is deﬁned as responsive region . During training, objects are
assigned to anchors according to their IoUs and an assign-
ment strategy [37]. However, two problems emerge in the
procedure of anchor assignment when anchors are not spa-
tially dense enough, which are described as follows.
• With the assignment IoU threshold ﬁxed, responsive
regions are ﬁxed. As is shown in Fig. 5(a), when an-
chors are not dense enough, responsive regions cannot
cover the whole image. We call the uncovered region
overlooked region . Objects, especially small objects,
located in overlooked regions are never assigned to any
anchor during training. Consequently, in the inference
phase, these objects are less possible to be detected for
lack of anchors here.
• If we lower the assignment IoU threshold, the respon-
sive regions are enlarged while the overlooked regions
shrink or even vanish. But enlarged responsive regions
make it hard for detectors to obtain accurate results.
Each anchor deals with more objects and more vari-
ance in object shape and position, which is illustrated
in Fig. 5(b).
As discussed above, the anchor density should be high
enough to cover possible objects. Thus, in TinyDet, we
keep high-resolution feature maps for dense anchoring. To
comprehend how dense anchoring impact detection perfor-
mance, especially for small objects, we take TinyDet with
ThunderNet [34] for comparison. In Fig. 6, we visualize
the distribution of smallest anchors. In ThunderNet, the
distance between adjacent anchors is 16pixels. It’s hard
to match small objects with anchors spatially. While that in
TinyDet is only 4pixels. Densely tiled anchors better cover
small objects.
Table 2. Ground-truth miss-assignment ratio (GTMR) for Faster
R-CNN, ThunderNet and our TinyDet. GTMRs, GTMRmand
GTMRlfor small, medium and large objects respectively.
Detector GTMR(%) GTMRs(%) GTMRm(%) GTMRl(%) Input
Faster R-CNN 6:6 17.0 1.2 1.9 8001333
Faster R-CNN w/ FPN 6:1 15.2 1.2 2.3 8001333
ThunderNet 18:1 53.1 5.6 1.2 320320
TinyDet 8:4 21.2 2.2 3.0 320320
Quantitatively, we propose ground-truth miss-
assignment ratio (GTMR) to evaluate the assignment2×
2×
2×
2×80×80× 36
40×40× 60
20×20×112
10×10×160
5 × 5 ×160Conv 1×1, 245
Conv 1×1, 245
Conv 1×1, 245
Conv 1×1, 245
Conv 1×1, 245SCConv, 245, g=49
SCConv, 245, g= 7
SCConv, 245, g= 5
SCConv, 245, g= 1
SCConv, 245, g= 1SCConv, 245, g=49
SCConv, 245, g=49
SCConv, 245, g=49
SCConv, 245, g=49
SCConv, 245, g=49TinyFPN TinyRPN
Conv 1×1
Groups=gDepthwise
Conv 3×3H × W × C
H × W × CSparsely-Connected
(SC)Conv
Conv 1×1, boxConv 1×1, score
Conv 1×1, boxConv 1×1, score
Conv 1×1, boxConv 1×1, score
Conv 1×1, boxConv 1×1,  score
Conv 1×1, boxConv 1×1, scoreFigure 4. TinyFPN and TinyRPN with sparsely-connected convolutions (SCConvs). SCConv consists of a depth-wise convolution and a
point-wise group convolution. ”g” denotes the number of groups in SCConv. 245is the number of output channels for both vanilla Conv
11and SCConv.
(a)(b)
Figure 5. Problems caused by sparse anchors. Dashed squares
denote anchors. Solid rectangles denote possible objects that may
exist in an image. (a): Objects located in the overlooked regions
(the red box) have low IoUs with every anchor, and would not be
assigned to any anchor during training. (b): With the responsive
region enlarged, every anchor deals with more objects and more
variance in object shape and position. Detection becomes harder.
(a)ThunderNet(a)TinyHRDet
Figure 6. Anchor density comparison between ThunderNet and
TinyDet. Black points denote centers of evenly tiled anchors
and white boxes show the anchors that have the best IoU with a
ground-truth object. Densely tiled anchors better cover small ob-
jects.
procedure. GTMR is deﬁned as the proportion of those
ground-truth objects that are not assigned to any anchorover all the objects. It reﬂects the matching quality between
anchors and objects under a certain assignment strategy.
As shown in Tab. 2, GTMR of ThunderNet is up to 18:1%,
much higher than that of Faster R-CNN [37]. TinyDet
obtains a quite lower GTMR, 8:4%, though as lightweight
as ThunderNet. Notably, GTMR of small objects of
ThunderNet is extremely high; while, in our TinyDet with
high-resolution feature maps and densely tiled anchors,
GTMR of small objects is much lower than ThunderNet.
Small objects can be better matched with anchors spatially.
4. Experiments
In this section, we ﬁrst describe the experimental details.
Then we compare our TinyDet models with other state-of-
the-art (SOTA) methods. We further perform detailed ab-
lation studies to demonstrate the effectiveness of our pro-
posed methods.
4.1. Experimental Details
Our experiments are conducted on the COCO dataset
[25]. We use the train2017 split for training and report our
main results on the test-dev2017 . The val2017 split is used
for detailed ablation studies. We follow the standard COCO
detection metrics to report the average precision (AP) under
different IoUs ( i.e., AP50, AP75and the overall AP) and AP
for detecting objects in different scales ( i.e., APs, APmand
APl).
We implement our TinyDet based on the PyTorch [32]
framework and MMDetection [7] toolbox. Our models are
trained with batch size 128on4GPUs ( 32images per GPU)
for240epochs. The SGD optimizer is used with momen-
tum 0:9and weight decay 1e-5. We linearly increase the
learning rate from 0to0:35in the ﬁrst 500iterations and
then decay it to 1e-5 using the cosine anneal schedule [28].
2000 /200proposals are used in the second stage at the train-
ing/inference phase. We adopt the same data augmenta-Table 3. SOTA results on the COCO test-dev2017 set. All the results are obtained via the single-model and single-scale testing.ymeans
the model is re-implemented by ourselves.zdenotes the result is evaluated on val2017 set because the result on test-dev2017 set is not
provided.
Method FLOPs AP AP50AP75APsAPmAPlInput
ThunderNet-SNet146 [34] 470M 23.6 40.2 24.5 - - - 3202
ThunderNet-SNet146y499M 23.8 40.5 24.7 4.6 23.0 42.9 3202
TinyDet-S 495M 26.0 45.8 26.5 9.6 26.8 39.5 3202
MobileNetV2-SSDLite [38] 800M 22.1 - - - - - 3202
MobileNet-SSDLite [38] 1300M 22.2 - - - - - 3202
Pelee [44] 1290M 22.4 38.3 22.9 - - - 3042
Tiny-DSOD [20] 1120M 23.2 40.4 22.8 - - - 3002
YOLOX-Nanoz[11] 1080M 25.3 - - - - - 4162
NASFPNLite MobileNetV2 [12] 980M 25.7 - - - - - 3202
ThunderNet-SNet535 [34] 1300M 28.0 46.2 29.5 - - - 3202
ThunderNet-SNet535y1297M 27.3 45.4 28.4 6.5 28.4 46.2 3202
TinyDet-M 991M 30.3 51.2 31.8 13.5 30.9 43.9 3202
YOLOv2 [35] 17.5G 21.6 44.0 19.2 5.0 22.4 35.5 4162
PRN [42] 4.0G 23.3 45.0 22.0 6.7 24.8 35.1 4162
EFM (SAM) [43] 5.1G 26.8 49.0 26.7 9.8 28.2 38.8 4162
YOLOX-Tinyz[11] 6.5G 31.7 - - - - - 4162
EfﬁcientDet-D0 [41] 2.5G 32.4 - - - - - 5122
YOLOv3 [36] 71.0G 33.0 57.9 34.4 18.3 35.4 41.9 6082
YOLOv5 [18] 17.0G 36.7 - - - - - 6402
TinyDet-L 2.4G 35.5 56.8 38.3 18.3 37.5 48.4 5122
tion strategy introduced in SSD [27]. Following Thunder-
Net [34], online hard example mining (OHEM) [39], Soft-
NMS [3], and Cross-GPU Batch Normalization [33] are
used.
In the proposed TinyRPN head, anchors in ﬁve pyramid
levels are respectively with sizes of 12:82,25:62,51:22,
102:42and204:82. Anchors of multiple aspect ratios f1 :
2;1 : 1;2 : 1gare used in each level, the same as [37] and
[23]. As for the R-CNN head, we adopt position-sensitive
RoI align [9, 21, 34] to extract box features effectively.
4.2. Comparison with SOTA Lightweight Generic
Detectors
We compare our TinyDet with other SOTA lightweight
detectors on the COCO test-dev2017 set in Tab. 3. Among
the compared methods, ThunderNet [34] and Efﬁcient-
Det [41] are regarded as the most recent SOTA lightweight
detectors. The results show that our models obviously
outperform them with fewer computation costs: TinyDet-
S surpasses ThunderNet-SNet146 by 2:4%AP with sim-
ilar FLOPs; TinyDet-M surpasses ThunderNet-SNet535
by2:3%AP with 76% FLOPs; and TinyDet-L surpasses
EfﬁcientDet-D0 by 3:1%AP with similar FLOPs. Be-
sides, TinyDet has better performance-computation trade-
offs than the automatically searched lightweight detec-tor (i.e., NAS-FPNLite [12]) and the popular YOLO se-
ries [18, 35, 36].
Our TinyDet models obtain extraordinary performance
on detecting small objects. With similar computation cost
(i.e., FLOPs), TinyDet-M achieves 13:5APs, which is
over 100% improvement over ThunderNet-SNet535 with
6:5APs; and TinyDet-S achieves 9:6APs, which is also
over 100% improvement over ThunderNet-SNet146 with
4:6APs. TinyDet-L achieves the same 18:3APswith
YOLOv3 [36], but it only has 1=30computation cost of
YOLOv3. Some visualized results are shown in Fig. 7.
4.3. Ablation Studies
4.3.1 Ablation Study on Backbone Enhancement
As discussed in Sec. 3.1.1, we enhance detailed information
in the MobileNetV3-based backbone, which is important
for improving the feature representation of high resolution.
We provide three MobileNetV3’s variants: MobileNetV3-
B, -C and -BC to demonstrate our effectiveness of the en-
hanced backbone. Compared with original MobileNetV3,
MobileNetV3-B adds 2extra blocks in the stage with stride
4 and MobileNetV3-C contains 50% more channels in early
stages. MobileNet-BC is the combination of MobileNetV3-
B and MobileNetV3-C which is our proposed backbone forFigure 7. Some visualized detection results of TinyDet-S in the top row and TinyDet-M in the bottom row. Best view in PDF.
Table 4. Evaluation of backbone enhancement. The enhanced
backbone signiﬁcantly improves the detection performance, espe-
cially that of small objects.
Backbone MFLOPs AP APsAPmAPl
MobileNetV2 [38] 908 28.2 12.5 29.5 43.3
SNet535 [34] 1602 28.5 11.9 29.3 44.9
FBNet-C [47] 1055 27.8 11.6 28.6 43.9
Proxyless-GPU [4] 1304 27.8 11.8 28.4 44.3
MobileNetV3-BC w/o SE 988 29.5 14.2 31.1 44.1
MobileNetV3 [14] 963 28.4 13.0 29.6 43.7
MobileNetV3-B 1016 29.5 14.3 30.7 44.3
MobileNetV3-C 1001 29.8 14.6 30.9 44.8
MobileNetV3-BC 991 30.3 15.6 31.9 45.0
TinyDet (conﬁgurations in Tab. 1). Tab. 4 shows the com-
parison of different conﬁgurations. For the fair comparison,
we adjust group numbers of TinyFPN and TinyRPN to keep
the total computation budget similar (around 1GFLOPs).
Our MobileNetV3-BC outperforms MobileNetV3 and other
variants. Increasing layer numbers and channel numbers
respectively improves AP by 1:1%and1:4%. When both
are adopted, AP is improved by 1:9%. Besides, the gain
on APs(+2:6)is more signiﬁcant than APm(+1:3)and
APl(+1:3). With more computation allocated to the early
stages, detailed information is enhanced and beneﬁts small
object detection more.
We also compare MobileNetV3-BC with other repre-
sentative lightweight backbones (Tab. 4). We remove SE
module for fair comparison. MobileNetV3-BC achieves
better results than both manually designed backbones
(SNet535 [34] and MobileNetV2 [38]) and automatically
searched backbones (FBNet [47] and Proxyless [4]).Table 5. Results of applying different convolution operators in
FPN and RPN. The proposed sparsely-connected convolution in
FPN and RPN greatly reduces the computation cost and keeps
comparable performance.
Conv type MFLOPs AP AP50AP75
Conventional Conv 10115 31.8 52.5 33.5
Depth-wise Separable Conv 1970 30.8 51.5 32.4
Sparsely-connected Conv 991 30.3 50.9 32.1
4.3.2 Ablation Study on the SCConv in TinyFPN and
TinyRPN
To make FPN and RPN lightweight, we combine the depth-
wise convolution and point-wise group convolution as the
sparsely-connected convolution (SCConv). As shown in
Tab. 5, compared with the conventional convolution or
depth-wise separable convolution [8, 15] in FPN and RPN,
SCConv greatly reduces the computation cost and achieves
comparable results. The results conﬁrm that the SCConv
in TinyFPN and TinyRPN causes little degradation on de-
tection performance. For the lightweight detector design,
adopting SCConv in detection heads is an effective method
to save computation cost while maintaining high detection
performance.
We also evaluate different group number settings of SC-
Convs in TinyFPN and TinyRPN. The results are presented
in Tab. 6. From the comparison between row (c) and (e), we
ﬁnd that setting different group numbers for different pyra-
mid levels is better than setting the same group number. A
good practice is to use more groups for the bottom pyramids
and fewer groups in the top pyramids in FPN (Tab. 6(e)).Table 6. Evaluation of different group conﬁgurations. TinyFPN
groups in the table respectively corresponds to pyramid levels with
stride 4, 8, 16, 32 and 64. We adopt (e) in our TinyDet.
TinyFPN groups TinyRPN group MFLOPs AP AP50AP75
(a) 1, 1, 1, 1, 1 1 1970 30.8 51.5 32.4
(b) 245, 245, 245, 245, 245 245 950 28.8 49.6 30.1
(c) 7, 7, 7, 7, 7 49 1030 30.0 50.4 31.4
(d) 1, 1, 5, 7, 49 49 1440 30.6 51.1 32.4
(e) 49, 7, 5, 1, 1 49 991 30.3 50.9 32.1
4.3.3 Ablation Study on Feature Alignment
We evaluate how feature misalignment, described in
Sec. 3.1.2, affects the detection performance in Tab. 7. By
better aligning the backbone features using a simple aver-
age pooling, we get a 0:5%gain on AP. Note that the gain
on APs(+1:4%) is higher than that on APm(+0:6%) or
APl(+0:8%). This is because small objects are much more
sensitive to feature misalignment and beneﬁt more from this
correction.
Table 7. Detection results without and with feature alignment.
Alignment signiﬁcantly improves detection performance of small
objects.
AP AP50AP75APsAPmAPl
without alignment 29.8 50.4 31.1 14.2 31.3 44.2
with alignment 30.3 "0:550.9"0:532.1"1:015.6"1:431.9"0:645.0"0:8
Table 8. Inference latency of TinyDet on two types of ARM CPUs
with single thread. No speciﬁc optimization is adopted.
CPU type Snapdragon 865 Kirin 820
TinyDet-S 103ms 133ms
TinyDet-M 179ms 236ms
TinyDet-L 312ms 386ms
4.4. Limitation
There is a limitation in this study. Currently we can-
not fairly compare the speed with other detectors. The rea-
sons are two-fold. Firstly, the speed of a detector is highly
dependent on the inference SDK ( e.g., Apple’s CoreML
and Google’s TensorFlowLite). For ThunderNet, it uses a
third-party high-performance inference SDK, which is not
publicly available. This makes fair speed comparison in
the same hardware condition infeasible. Secondly, most
inference SDKs are mainly optimized for computation-
intensive models and less focuses on the memory band-
width. However, TinyDet is computation-efﬁcient and re-
quires bandwidth-oriented optimization. The inconsistency
makes the TinyDet sub-optimal in speed. Currently weoffer the inference latency of TinyDet on two types of
ARM CPUs with single thread and without any optimiza-
tion (Tab. 8).
FLOPs is a widely-accepted metric of great generality,
especially in the neural architecture search (NAS) domain.
FLOPs can be calculated theoretically, irrelevant to speciﬁc
hardware implementation. And according to the rooﬂine
theory [46], FLOPs determines the upper bound of speed.
Considering the low computation requirement, with spe-
ciﬁc optimization, TinyDet can achieve excellent inference
speed.
5. Conclusion
In this paper, we target at detecting generic ( i.e., 80 ob-
ject categories) small ( i.e.,(3232)=(480640)
0:33% area of the input image) objects in the computation
constrained ( i.e.,1 GFLOPs) setting. The proposed tech-
nologies, such as high-resolution detection feature maps,
TinyFPN/RPN, early-stage enhanced backbone and aver-
age pooling-based feature alignment are focusing on solv-
ing this problem. We obtain outstanding results - more than
100% AP improvement over the previous state-of-the-art
lightweight generic detector on COCO small object detec-
tion. In the future, we would like to implement and optimize
the proposed TinyDet on ASICs and edge devices.
References
[1] Yancheng Bai, Yongqiang Zhang, Mingli Ding, and Bernard
Ghanem. SOD-MTGAN: small object detection via multi-
task generative adversarial network. In ECCV , 2018. 2
[2] Sean Bell, C. Lawrence Zitnick, Kavita Bala, and Ross B.
Girshick. Inside-outside net: Detecting objects in context
with skip pooling and recurrent neural networks. In CVPR ,
2016. 2
[3] Navaneeth Bodla, Bharat Singh, Rama Chellappa, and
Larry S. Davis. Soft-nms - improving object detection with
one line of code. In ICCV , 2017. 7
[4] Han Cai, Ligeng Zhu, and Song Han. Proxylessnas: Direct
neural architecture search on target task and hardware. In
ICLR , 2019. 8
[5] Guimei Cao, Xuemei Xie, Wenzhe Yang, Quan Liao, Guang-
ming Shi, and Jinjian Wu. Feature-fused SSD: fast detection
for small objects. arXiv:1709.05054 , 2017. 2
[6] Chen Chen, Mengyuan Liu, Xiandong Meng, Wanpeng
Xiao, and Qi Ju. Reﬁnedetlite: A lightweight one-
stage object detection framework for cpu-only devices.
arXiv:1911.08855 , 2019. 1, 2, 5
[7] Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu
Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei
Liu, Jiarui Xu, Zheng Zhang, Dazhi Cheng, Chenchen Zhu,
Tianheng Cheng, Qijie Zhao, Buyu Li, Xin Lu, Rui Zhu,
According to the deﬁnition of small objects in the COCO dataset,
referring to [25] and http://cocodataset.org/#detection-
evalYue Wu, Jifeng Dai, Jingdong Wang, Jianping Shi, Wanli
Ouyang, Chen Change Loy, and Dahua Lin. MMDe-
tection: Open mmlab detection toolbox and benchmark.
arXiv:1906.07155 , 2019. 6
[8] Franc ¸ois Chollet. Xception: Deep learning with depthwise
separable convolutions. In CVPR , 2017. 3, 5, 8
[9] Jifeng Dai, Yi Li, Kaiming He, and Jian Sun. R-FCN: object
detection via region-based fully convolutional networks. In
NIPS , 2016. 2, 7
[10] Cheng-Yang Fu, Wei Liu, Ananth Ranga, Ambrish Tyagi,
and Alexander C. Berg. DSSD : Deconvolutional single shot
detector. arXiv:1701.06659 , 2017. 2
[11] Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, and Jian
Sun. Yolox: Exceeding yolo series in 2021. arXiv preprint
arXiv:2107.08430 , 2021. 7
[12] Golnaz Ghiasi, Tsung-Yi Lin, and Quoc V . Le. NAS-FPN:
learning scalable feature pyramid architecture for object de-
tection. In CVPR , 2019. 7
[13] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville,
and Yoshua Bengio. Generative adversarial nets. In NIPS ,
2014. 2
[14] Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh
Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu,
Ruoming Pang, Vijay Vasudevan, Quoc V . Le, and Hartwig
Adam. Searching for mobilenetv3. arXiv:1905.02244 , 2019.
2, 3, 4, 8, 11, 13, 14
[15] Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry
Kalenichenko, Weijun Wang, Tobias Weyand, Marco An-
dreetto, and Hartwig Adam. Mobilenets: Efﬁcient con-
volutional neural networks for mobile vision applications.
arXiv:1704.04861 , 2017. 2, 3, 5, 8
[16] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation net-
works. In CVPR , 2018. 4, 13, 14
[17] Peiyun Hu and Deva Ramanan. Finding tiny faces. In CVPR ,
2017. 2
[18] Glenn Jocher, Yonghye Kwon, guigarfr, perry0418, Josh
Veitch-Michaelis, Ttayu, Daniel Suess, Fatih Baltacı,
Gabriel Bianconi, IlyaOvodov, Marc, e96031413, Chang
Lee, Dustin Kendall, Falak, Francisco Reveriano, FuLin,
GoogleWiki, Jason Nataprawira, Jeremy Hu, LinCoce,
LukeAI, NanoCode012, NirZarrabi, Oulbacha Reda, Piotr
Skalski, SergioSanchezMontesUAM, Shiwei Song, Thomas
Havlik, and Timothy M. Shead. ultralytics/yolov3: v9.5.0
- YOLOv5 v5.0 release compatibility update for YOLOv3,
Apr. 2021. 7
[19] Jianan Li, Xiaodan Liang, Yunchao Wei, Tingfa Xu, Jiashi
Feng, and Shuicheng Yan. Perceptual generative adversarial
networks for small object detection. In CVPR , 2017. 2
[20] Yuxi Li, Jiuwei Li, Weiyao Lin, and Jianguo Li. Tiny-dsod:
Lightweight object detection for resource-restricted usages.
InBMVC , 2018. 7
[21] Zeming Li, Chao Peng, Gang Yu, Xiangyu Zhang, Yangdong
Deng, and Jian Sun. Light-head R-CNN: in defense of two-
stage object detector. arXiv:1711.07264 , 2017. 2, 7
[22] Zeming Li, Chao Peng, Gang Yu, Xiangyu Zhang, Yangdong
Deng, and Jian Sun. Detnet: Design backbone for objectdetection. In The European Conference on Computer Vision
(ECCV) , September 2018. 2
[23] Tsung-Yi Lin, Piotr Doll ´ar, Ross B. Girshick, Kaiming He,
Bharath Hariharan, and Serge J. Belongie. Feature pyramid
networks for object detection. In CVPR , 2017. 1, 2, 7
[24] Tsung-Yi Lin, Priya Goyal, Ross B. Girshick, Kaiming He,
and Piotr Doll ´ar. Focal loss for dense object detection. In
ICCV , 2017. 2
[25] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James
Hays, Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and
C. Lawrence Zitnick. Microsoft COCO: common objects in
context. In ECCV , 2014. 2, 6, 9
[26] Baoyuan Liu, Min Wang, Hassan Foroosh, Marshall F. Tap-
pen, and Marianna Pensky. Sparse convolutional neural net-
works. In CVPR , 2015. 3
[27] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian
Szegedy, Scott E. Reed, Cheng-Yang Fu, and Alexander C.
Berg. SSD: single shot multibox detector. In ECCV , 2016.
2, 3, 7
[28] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient
descent with warm restarts. arXiv:1608.03983 , 2016. 6
[29] Wenjie Luo, Yujia Li, Raquel Urtasun, and Richard S.
Zemel. Understanding the effective receptive ﬁeld in deep
convolutional neural networks. In NIPS , 2016. 4
[30] Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun.
Shufﬂenet V2: practical guidelines for efﬁcient CNN archi-
tecture design. In ECCV , 2018. 2, 3
[31] Junhyug Noh, Wonho Bae, Wonhee Lee, Jinhwan Seo, and
Gunhee Kim. Better to follow, follow to be better: Towards
precise supervision of feature super-resolution for small ob-
ject detection. In ICCV , 2019. 2
[32] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,
James Bradbury, Gregory Chanan, Trevor Killeen, Zeming
Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An
imperative style, high-performance deep learning library. In
NIPS , 2019. 6
[33] Chao Peng, Tete Xiao, Zeming Li, Yuning Jiang, Xiangyu
Zhang, Kai Jia, Gang Yu, and Jian Sun. Megdet: A large
mini-batch object detector. In CVPR , 2018. 7
[34] Zheng Qin, Zeming Li, Zhaoning Zhang, Yiping Bao, Gang
Yu, Yuxing Peng, and Jian Sun. Thundernet: Towards real-
time generic object detection on mobile devices. In ICCV ,
2019. 1, 2, 5, 7, 8
[35] Joseph Redmon and Ali Farhadi. YOLO9000: better, faster,
stronger. In CVPR , 2017. 7
[36] Joseph Redmon and Ali Farhadi. Yolov3: An incremental
improvement. arXiv:1804.02767 , 2018. 2, 7
[37] Shaoqing Ren, Kaiming He, Ross B. Girshick, and Jian Sun.
Faster R-CNN: towards real-time object detection with re-
gion proposal networks. In NIPS , 2015. 1, 2, 5, 6, 7
[38] Mark Sandler, Andrew G. Howard, Menglong Zhu, Andrey
Zhmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted
residuals and linear bottlenecks. In CVPR , 2018. 1, 2, 3, 4,
5, 7, 8, 13, 14
[39] Abhinav Shrivastava, Abhinav Gupta, and Ross B. Girshick.
Training region-based object detectors with online hard ex-
ample mining. In CVPR , 2016. 7[40] Mingxing Tan and Quoc V . Le. Efﬁcientnet: Rethinking
model scaling for convolutional neural networks. In ICML ,
2019. 2
[41] Mingxing Tan, Ruoming Pang, and Quoc V . Le. Efﬁcientdet:
Scalable and efﬁcient object detection. arXiv:1911.09070 ,
2019. 1, 2, 5, 7
[42] Chien-Yao Wang, Hong-Yuan Mark Liao, Ping-Yang Chen,
and Jun-Wei Hsieh. Enriching variety of layer-wise learning
information by gradient combination. In ICCVW , 2019. 7
[43] Chien-Yao Wang, Hong-Yuan Mark Liao, Yueh-Hua Wu,
Ping-Yang Chen, Jun-Wei Hsieh, and I-Hau Yeh. Cspnet: A
new backbone that can enhance learning capability of CNN.
InCVPRW , 2020. 7
[44] Jun Wang, Tanner A. Bohn, and Charles X. Ling. Pelee:
A real-time object detection system on mobile devices. In
NIPS , 2018. 1, 2, 5, 7
[45] Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang,
Chaorui Deng, Yang Zhao, Dong Liu, Yadong Mu, Mingkui
Tan, Xinggang Wang, et al. Deep high-resolution repre-
sentation learning for visual recognition. arXiv:1908.07919 ,
2019. 2
[46] Samuel Williams, Andrew Waterman, and David A. Patter-
son. Rooﬂine: an insightful visual performance model for
multicore architectures. Commun. ACM , 2009. 9
[47] Bichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang,
Fei Sun, Yiming Wu, Yuandong Tian, Peter Vajda, Yangqing
Jia, and Kurt Keutzer. Fbnet: Hardware-aware efﬁcient con-
vnet design via differentiable neural architecture search. In
CVPR , 2019. 8
[48] Wei Xiang, Dong-Qing Zhang, Heather Yu, and Vassilis
Athitsos. Context-aware single-shot detector. In WACV ,
2018. 2
[49] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun.
Shufﬂenet: An extremely efﬁcient convolutional neural net-
work for mobile devices. In CVPR , 2018. 2, 3, 5A. Appendix
A.1. Overall Computation Allocation of TinyDets
Tab. 9 provides the overall computation allocation of
TinyDets. The backbone network contains most of the com-
putation budget (70%) while TinyFPN, TinyRPN and R-
CNN are extremely computationally cheap.
Table 9. Computation allocation of different parts (MFLOPs).
Detector Total Backbone TinyFPN TinyRPN R-CNN
TinyDet-S 495 (100%) 347 (70%) 64 (13%) 16 (3%) 68 (14%)
TinyDet-M 991 (100%) 703 (71%) 155 (16%) 65 (7%) 68 (7%)
TinyDet-L 2427 (100%) 1797 (74%) 396 (16%) 166 (7%) 68 (3%)
A.2. Detailed Backbone Information
TinyDet-S is within the computation constraint of 500
MFLOPs. It takes a more lightweight variant of Mo-
bileNetV3 [14] as the backbone, termed as MobileNetV3-
D. The speciﬁcation of MobileNetV3-D can be found in
Tab. 10. To reduce computation cost, we remove the de-
tailed information enhancement used in TinyDet-M and
TinyDet-L, and shrink the expansion size of high-level lay-
ers.
TinyDet-M is within the computation constraint of 1000
MFLOPs. It takes a MobileNetV3-BC as the backbone
(Tab. 11). The input resolution is 320320.
TinyDet-L is targeted at a high accuracy. It also adopts
MobileNetV3-BC as the backbone but takes a larger input
resolution as 512512(Tab. 12).
A.3. Computation Cost of TinyFPN & TinyRPN
In TinyDet-S, different from the structure shown in Fig.
4 of the main paper, we drop the highest-resolution pyramid
level (stride= 4) to reduce computation cost. TinyFPN con-
tains four 11convs and four SCConvs (Tab. 13). TinyRPN
contains four SCConvs, four convs for predicting scores and
four convs for box regression ((Tab. 14).
As shown in Fig. 4 of the main paper, TinyFPN contains
ﬁve 11convs and ﬁve SCConvs (Tab. 15). TinyRPN
contains ﬁve SCConvs, ﬁve convs for predicting scores and
ﬁve convs for box regression (Tab. 16).
TinyFPN and TinyRPN of TinyDet-L have the same
structure with those of TinyDet-M, except the resolution.
TinyFPN contains ﬁve 11convs and ﬁve SCConvs
(Tab. 17). TinyRPN contains ﬁve SCConvs, ﬁve convs for
predicting scores and ﬁve convs for regression (Tab. 18).
A.4. Computation Cost of R-CNN
The R-CNN head is based on PSRoI align, which con-
sists of three fully connected layers, as shown in Fig. 8. 200
RoIs are used in inference phase. Based on 200 RoIs, thecomputation cost of R-CNN is:
200(2451024 + 102481 + 10244)
= 67:584106
68MFLOPs(1)
102481
47×7×5fcfc
fccls
boxPSRoI align
Figure 8. Structure of R-CNN head.Table 10. Speciﬁcation for the backbone of TinyDet-S (MobileNetV3-D). Bneck denotes the inverted residual bottleneck structure [38].
ExSize denotes expansion size. SE denotes the squeeze-and-excitation module [16]. NL denotes the type of the used nonlinearity. HS
denotes h-swish [14] and RE denotes ReLU. FPN denotes whether the output of the block is fed into FPN.
Input Operator ExSize Out SE NL Stride FPN MFLOPs
32023 conv2d, 33 - 16 - HS 2 - 11.06
160216 bneck, 33 16 16 - RE 1 - 10.24
160216 bneck, 33 64 24 - RE 2 - 39.73
80224 bneck, 33 72 24 - RE 1 - 26.27
80224 bneck, 55 72 40X RE 2 - 18.55
40240 bneck, 55 120 40X RE 1 - 20.17
40240 bneck, 55 120 40X RE 1 X 20.17
40240 bneck, 33 240 80 - HS 2 - 23.90
20280 bneck, 33 240 80 - HS 1 - 16.22
20280 bneck, 55 240 80 - HS 1 - 17.76
20280 bneck, 55 240 80 - HS 1 - 17.76
20280 bneck, 33 240 112X HS 1 - 19.33
202112 bneck, 33 336 112X HS 1 X 31.37
202112 bneck, 55 336 160X HS 2 - 21.33
102160 bneck, 55 480 160X HS 1 - 16.68
102160 bneck, 55 480 160X HS 1 X 16.68
102160 bneck, 55 960 160X HS 2 X 20.26
In total - - - - - - - 347.48
Table 11. Speciﬁcation for the backbone of TinyDet-M. Bneck denotes the inverted residual bottleneck structure [38]. ExSize denotes
expansion size. SE denotes the squeeze-and-excitation module [16]. NL denotes the type of the used nonlinearity. HS denotes h-swish [14]
and RE denotes ReLU. FPN denotes whether the output of the block is fed into FPN. Enhancement denotes modiﬁcations compared with
the original MobileNetV3 and ”c” denotes channel.
Input Operator ExSize Out SE NL Stride FPN Enhancement MFLOPs
32023 conv2d, 33 - 24 - HS 2 - c + 50% 16.59
160224 bneck, 33 24 24 - RE 1 - c + 50% 20.28
160224 bneck, 33 72 36 - RE 2 - c + 50% 64,97
80236 bneck, 33 108 36 - RE 1 - added blocks 55.99
80236 bneck, 33 108 36 - RE 1 - added blocks 55.99
80236 bneck, 33 108 36 - RE 1 X c + 50% 55.99
80236 bneck, 55 108 60X RE 2 - c + 50% 39.58
40260 bneck, 55 180 60X RE 1 - c + 50% 41.78
40260 bneck, 55 180 60X RE 1 X c + 50% 41.78
40260 bneck, 33 240 80 - HS 2 - - 31.58
20280 bneck, 33 200 80 - HS 1 - - 13.52
20280 bneck, 33 184 80 - HS 1 - - 12.44
20280 bneck, 33 184 80 - HS 1 - - 12.44
20280 bneck, 33 480 112X HS 1 - - 38.71
202112 bneck, 33 672 112X HS 1 X - 62.86
202112 bneck, 55 672 160X HS 2 - - 42.76
102160 bneck, 55 960 160X HS 1 - - 33.58
102160 bneck, 55 960 160X HS 1 X - 33.58
102160 bneck, 55 960 160X HS 2 - - 20.26
52160 bneck, 55 960 160X HS 1 X - 8.74
In total - - - - - - - - 703.42Table 12. Speciﬁcation for the backbone of TinyDet-L. Bneck denotes the inverted residual bottleneck structure [38]. ExSize denotes
expansion size. SE denotes the squeeze-and-excitation module [16]. NL denotes the type of the used nonlinearity. HS denotes h-swish [14]
and RE denotes ReLU. FPN denotes whether the output of the block is fed into FPN. Enhancement denotes modiﬁcations compared with
the original MobileNetV3 and ”c” denotes channel.
Input Operator ExSize Out SE NL Stride FPN Enhancement MFLOPs
51223 conv2d, 33 - 24 - HS 2 - c + 50% 42.47
256224 bneck, 33 24 24 - RE 1 - c + 50% 51.91
256224 bneck, 33 72 36 - RE 2 - c + 50% 166.33
128236 bneck, 33 108 36 - RE 1 - added blocks 143.33
128236 bneck, 33 108 36 - RE 1 - added blocks 143.33
128236 bneck, 33 108 36 - RE 1 X c + 50% 143.33
128236 bneck, 55 108 60X RE 2 - c + 50% 101.31
64260 bneck, 55 180 60X RE 1 - c + 50% 106.92
64260 bneck, 55 180 60X RE 1 X c + 50% 106.92
64260 bneck, 33 240 80 - HS 2 - - 80.86
32280 bneck, 33 200 80 - HS 1 - - 34.61
32280 bneck, 33 184 80 - HS 1 - - 31.84
32280 bneck, 33 184 80 - HS 1 - - 31.84
32280 bneck, 33 480 112X HS 1 - - 98.91
322112 bneck, 33 672 112X HS 1 X - 160.56
322112 bneck, 55 672 160X HS 2 - - 109.12
162160 bneck, 55 960 160X HS 1 - - 85.25
162160 bneck, 55 960 160X HS 1 X - 85.25
162160 bneck, 55 960 160X HS 2 - - 51.15
82160 bneck, 55 960 160X HS 1 X - 21.66
In total - - - - - - - - 1796.90
Table 13. Computation cost of TinyFPN of TinyDet-S.
Operator Input Output MFLOPs
Conv, 11 40240 402245 16.07
Conv, 11 202112 202245 11.01
Conv, 11 102160 102245 3.95
Conv, 11 52160 52245 0.99
SCConv, g=7 402245 402245 18.03
SCConv, g=5 202245 202245 5.88
SCConv, g=1 102245 102245 6.27
SCConv, g=1 52245 52245 1.57
In total - - 63.77Table 14. Computation cost of TinyRPN of TinyDet-S.
Operator Input Output MFLOPs
SCConv, g=49402245 402245
8.33202245 202245
102245 102245
52245 52245
Conv, 11(score)402245 4023
1.57202245 2023
102245 1023
52245 523
Conv, 11(box)402245 40212
6.27202245 20212
102245 10212
52245 5212
In total - - 16.17Table 15. Computation cost of TinyFPN of TinyDet-M.
Operator Input Output MFLOPs
Conv, 11 80236 802245 58.02
Conv, 11 40260 402245 23.91
Conv, 11 202112 202245 11.07
Conv, 11 102160 102245 3.95
Conv, 11 52160 52245 0.99
SCConv, g=49 802245 802245 25.09
SCConv, g=7 402245 402245 18.03
SCConv, g=5 202245 202245 5.88
SCConv, g=1 102245 102245 6.27
SCConv, g=1 52245 52245 1.57
In total - - 154.78Table 16. Computation cost of TinyRPN of TinyDet-M.
Operator Input Output MFLOPs
SCConv, g=49802245 802245
33.42402245 402245
202245 202245
102245 102245
52245 52245
Conv, 11(score)802245 8023
6.29402245 4023
202245 2023
102245 1023
52245 523
Conv, 11(box)802245 80212
25.17402245 40212
202245 20212
102245 10212
52245 5212
In total - - 64.88
Table 17. Computation cost of TinyFPN of TinyDet-L.
Operator Input Output MFLOPs
Conv, 11 128236 1282245 148.52
Conv, 11 64260 642245 61.22
Conv, 11 322112 322245 28.35
Conv, 11 162160 162245 10.10
Conv, 11 82160 82245 2.52
SCConv, g=49 1282245 1282245 64.23
SCConv, g=7 642245 642245 46.16
SCConv, g=5 322245 322245 15.05
SCConv, g=1 162245 162245 16.06
SCConv, g=1 82245 82245 4.01
In total - - 396.22Table 18. Computation cost of TinyRPN of TinyDet-L.
Operator Input Output MFLOPs
SCConv, g=491282245 1282245
85.55642245 642245
322245 322245
162245 162245
82245 82245
Conv, 11(score)1282245 12823
16.11642245 6423
322245 3223
162245 1623
82245 823
Conv, 11(box)1282245 128212
64.42642245 64212
322245 32212
162245 16212
82245 8212
In total - - 166.08