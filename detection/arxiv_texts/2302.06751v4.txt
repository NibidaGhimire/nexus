OpenHLS : High-Level Synthesis for Low-Latency Deep Neural
Networks for Experimental Science
Maksim Levental
mlevental@uchicago.edu
University of ChicagoArham Khan
arham@uchicago.edu
University of ChicagoRyan Chard
rchard@anl.gov
Argonne National Laboratory
Kazutomo Yoshii
kazutomo@anl.gov
Argonne National LaboratoryKyle Chard
chard@uchicago.edu
University of ChicagoIan Foster
foster@uchicago.edu
University of Chicago
ABSTRACT
In many experiment-driven scientific domains, such as high-energy
physics, material science, and cosmology, high data rate experi-
ments impose hard constraints on data acquisition systems: col-
lected data must either be indiscriminately stored for post-processing
and analysis, thereby necessitating large storage capacity, or ac-
curately filtered in real-time, thereby necessitating low-latency
processing. Deep neural networks, effective in other filtering tasks,
have not been widely employed in such data acquisition systems,
due to design and deployment difficulties. We present an open
source, lightweight, compiler framework, without any proprietary
dependencies, OpenHLS , based on high-level synthesis techniques,
for translating high-level representations of deep neural networks
to low-level representations, suitable for deployment to near-sensor
devices such as field-programmable gate arrays. We evaluate OpenHLS
on various workloads and present a case-study implementation of
a deep neural network for Bragg peak detection in the context
of high-energy diffraction microscopy. We show OpenHLS is able
to produce an implementation of the network with a throughput
4.8µs/sample, which is approximately a 4 ×improvement over the
existing implementation.
ACM Reference Format:
Maksim Levental, Arham Khan, Ryan Chard, Kazutomo Yoshii, Kyle Chard,
and Ian Foster. 2023. OpenHLS : High-Level Synthesis for Low-Latency Deep
Neural Networks for Experimental Science. In Proceedings of ACM Confer-
ence (Conference’17). ACM, New York, NY, USA, 12 pages. https://doi.org/10.
1145/nnnnnnn.nnnnnnn
1 INTRODUCTION
High data rates are observed and, consequently, large datasets are
generated, across a broad range of science experiments in domains
such as high-energy physics, materials science, and cosmology. For
example, in high-energy physics, the LHCb detector at the Large
Hadron Collider (LHC) is tasked with observing the trajectories of
particles produced in proton-proton collisions at 40 MHz [ 19]. With
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
Conference’17, July 2017, Washington, DC, USA
©2023 Association for Computing Machinery.
ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00
https://doi.org/10.1145/nnnnnnn.nnnnnnna packet size of approximately 50 kB (per collision), this implies
a data rate of approximately 2 TB/s. Ultimately, in combination
with other detectors, the LHC processes approximately 100 EB of
data per year. In materials science, Bragg diffraction peak analysis,
which provides non-destructive characterization of single-crystal
and polycrystalline structure and its evolution in a broad class of
materials, can have collection rates approaching 1 MHz [ 23], with
a corresponding packet size of 80 kB. In cosmology, the Square
Kilometer Array, a radio telescope projected to be operational by
2027 [33], will sustain data rates in excess of 10 TB/s [21].
Storing and distributing such large quantities of data for fur-
ther analysis is cost prohibitive. Thus, data must be compressed
or (as we consider here) filtered to preserve only the most “in-
teresting” elements at the time of collection, an approach that
reduces storage needs but imposes stringent latency constraints
on the filtering mechanisms. Typically, filtering mechanisms con-
sist of either physics-based [ 14] or machine learning models [ 20];
in either case, maximally efficient and effective use of the target
hardware platform is important. Irrespective of the technique em-
ployed, almost universally, for ultra-low (e.g., sub-microsecond)
latency use cases the implementation is deployed to either field-
programmable gate arrays (FPGAs) or application-specific inte-
grated circuits (ASICs) [17]. Here we focus primarily on FPGAs.
Deep neural networks (DNNs), a particular type of machine
learning model, have been shown to be effective in many scientific
and commercial domains due to their representational capacity,
i.e., their ability to represent (approximately) diverse sets of map-
pings [ 5]. DNNs “learn” to represent a mapping over the course of
“training,” wherein they are iteratively evaluated on sample data
while a “learning rule” periodically updates the weights that param-
eterize the DNN. In recent years, DNNs have been investigated for
near real-time scientific use cases [ 29,30,38] but their use for the
lowest latency use cases has been limited [17], for three reasons:
(1)Graphics Processing Units (GPUs), the conventional hard-
ware target for DNNs, are not sufficiently performant for
these high data rate, low latency, use cases (due to their
low clock speeds and low peripheral bandwidth, until re-
cently [3]);
(2)DNNs, by virtue of their depth, require substantial mem-
ory (for weights) and compute (floating-point arithmetic),
thereby preventing their deployment to FPGAs, which, in
particular, have limited static RAM;
(3)DNNs are (typically) defined, trained, and distributed by us-
ing high-level frameworks (e.g., PyTorch [ 37], TensorFlowarXiv:2302.06751v4  [cs.AR]  16 Mar 2023Conference’17, July 2017, Washington, DC, USA Maksim Levental, Arham Khan, Ryan Chard, Kazutomo Yoshii, Kyle Chard, and Ian Foster
[4], MXNet [ 11]), which abstract all implementation details,
thereby making portability of model architectures to unsup-
ported hardware platforms (e.g., FPGAs and ASICs) close to
non-existent (barring almost wholesale reimplementations
of the frameworks).
These three barriers demand a solution that can translate a high-
level DNN representation to a low-level representation, suitable
for FPGA deployment, while simultaneously optimizing resource
usage and minimizing latency. In general, the task of lowering high-
level representations of programs to low-level representations is the
domain of a compiler. Similarly, the task of synthesizing aregister-
transfer level (RTL) design , rendered in a hardware description lan-
guage (HDL), from a program, is the domain of high-level synthesis
(HLS) [ 34] tools. Existing HLS tools [ 10,18,48] struggle to perform
needed optimizations in reasonable amounts of time (see Section 2.2)
despite, often, bundling robust optimizing compilers.
Recently, deep learning compilers (e.g., TVM [ 12], MLIR [ 26],
and Glow [ 42]) have demonstrated the ability to reduce dramati-
cally inference DNN latencies [ 28], training times [ 49], and memory
usage [ 13]. These compilers function by extracting intermediate-
level representations (IRs) of the DNNs from the representations
produced by the frameworks, and performing various optimiza-
tions (e.g., kernel fusion [ 6], vectorization [ 32], and memory plan-
ning [ 13]) on those IRs. The highly optimized IR is then used to
generate code for various target hardware platforms. Given the
successes of these compilers, it is natural to wonder whether they
can be adapted to the task of sufficiently optimizing a DNN such
that it might be synthesized to RTL, for deployment to FPGA.
In this paper, we present OpenHLS , an open-source1, lightweight
compiler and HLS framework that can translate DNNs defined as
PyTorch models to FPGA-compatible implementations. OpenHLS
uses a combination of compiler and HLS techniques to compile
the entire DNN into fully scheduled RTL, thereby eliminating all
synchronization overheads and achieving low latency. OpenHLS is
general and supports a wide range of DNN layer types, and thus
a wide range of DNNs. To the best of our knowledge, OpenHLS
is the first HLS framework that enables the use of DNNs, free
of a dependence on expensive and opaque proprietary HLS tools,
for science experiments that demand low-latency inference. In
summary our specific contributions include:
(1)We describe and implement a compiler framework, OpenHLS ,
that can efficiently transform, without use of proprietary
HLS tools, unoptimized, hardware-agnostic PyTorch models
into low-latency RTL suitable for deployment to FPGAs;
(2)We show that OpenHLS generates lower latency designs than
does a state-of-the-art commercial HLS tool (Xilinx’s Vitis
HLS) for many DNN layer types. In particular we show that
OpenHLS can produce synthesizable designs that meet place-
ment, routing, and timing constraints for BraggNN , a DNN
designed for analyzing Bragg diffraction peaks;
(3)We discuss challenges faced even after successful synthesis
of RTL from a high-level representation of a DNN, namely
during the place and route phases of implementation.
Note that while we focus here, for illustrative purposes, on opti-
mizations relevant to a DNN used for identifying Bragg diffraction
1Available at https://github.com/makslevental/openhlspeaks in materials science, OpenHLS supports a wide range of DNNs,
limited only by upstream support for DNN layers.
The rest of this paper is as follows: Section 2 reviews key concepts
from compilers, high-level synthesis, and RTL design for FPGA,
as well as related work. Section 3 describes the OpenHLS compiler
and HLS framework in detail. Section 4 evaluates OpenHLS ’s perfor-
mance, scalability, and competitiveness with designs generated by
Vitis HLS, and describes a case study in which OpenHLS is applied
toBraggNN , a Bragg peak detection DNN with a target latency of 1
µs/sample. Finally, Section 5 concludes and discusses future work.
2 BACKGROUND
We briefly review relevant concepts from DNN frameworks and
compilers, high-level synthesis, and FPGA design. Each subsection
corresponds to a phase in the translation from high-level DNN to
feasible FPGA implementation.
2.1 Compilers: The path from high to low
The path from a high-level, abstract, DNN representation to a
register-transfer level representation can be viewed as a sequence
of lowerings between adjacent levels of abstraction. Each level of
abstraction is rendered as a programming language, IR, or HDL,
and thus we describe each lowering in terms of the representations
and tools used by OpenHLS to manipulate those representations:
(1)An imperative, define-by-run, Python representation, in Py-
Torch;
(2) High-level data-flow graph representation, in TorchScript;
(3)Low-level data and control flow graph representation, in
Multi-Level Intermediate Representation (MLIR).
2.1.1 PyTorch and TorchScript. Typically DNN models are repre-
sented in terms of high-level frameworks, themselves implemented
within general purpose programming languages. Such frameworks
are popular because of their ease of use and large library of exam-
ple implementations of various DNN model architectures. OpenHLS
targets the PyTorch framework. DNNs developed within PyTorch
aredefined-by-run : the author describes the DNN imperatively in
terms of high-level operations, using Python, which, when exe-
cuted, materializes the (partial) high-level data-flow graph (DFG)
corresponding to the DNN (e.g., for the purposes of reverse-mode
automatic differentiation). From the perspective of the user, define-
by-run enables fast iteration at development time, possibly at the
cost of some runtime performance.
Yet from the perspective of compilation, define-by-run precludes
efficient extraction of the high-level DFG; since the DFG is ma-
terialized only at runtime, it cannot easily be statically inferred
from the textual representation (i.e., the Python source) of the DNN.
Furthermore, a priori, the runtime-materialized DFG is only par-
tially materialized [ 37], and only as an in-memory data structure.
Thus, framework support is necessary for efficiently extracting
the full DFG. For this purpose, PyTorch supports a Single Static
Assignment (SSA) IR, called TorchScript (TS) IR and accompanying
tracing mechanism (the TS JIT), which generates TS IR from con-
ventionally defined PyTorch models. Lowering from PyTorch to TS
IR enables various useful analyses and transformations on a DNN
at the level of the high-level DFG, but targeting FPGAs requiresOpenHLS Conference’17, July 2017, Washington, DC, USA
a broader collection of transformations. To this end, we turn to a
recent addition to the compiler ecosystem, MLIR.
2.1.2 MLIR. MLIR [ 26] presents a new approach to building reusable
and extensible compiler infrastructure. MLIR is composed of a set of
dialect IRs, subsets of which are mutually compatible, either directly
or by way of translation/legalization. The various dialects aim to
capture and formalize the semantics of compute intensive programs
at varying levels of abstraction, as well as namespace-related sets
of IR transformations. The entrypoint into this compiler framework
from PyTorch is the torch dialect [ 44], a high-fidelity mapping
from TS IR to MLIR native IR, which, in addition to performing the
translation to MLIR, fully refines all shapes of intermediate tensors
in the DNN (i.e., computes concrete values for all dimensions of
each tensor), a necessary step for downstream optimizations and
eliminating inconsistencies in the DNN [24].
While necessary for lowering to MLIR and shape refinement,
thetorch dialect represents a DNN at the same level of abstraction
as TS IR: it does not capture the precise data and control flow
needed for de novo implementations of DNN operations (e.g., for
FPGA). Fortunately, MLIR supports lower-level dialects, such as
linalg ,affine , and scf. The scf(structured control flow) dialect
describes standard control flow primitives, such as conditionals
and loops, and is mutually compatible with the arith (arithmetic
operations) and memref (memory buffers) dialects. The affine
dialect, on the other hand, provides a formalization of semantics
that lend themselves to polyhedral compilation techniques [ 9] that
enable loop dependence analysis and loop transformations. Such
loop transformations, particularly loop unrolling, are crucial for
achieving lowest possible latencies [ 47] because loop nests directly
inform the concurrency and parallelism of the final RTL design.
2.2 High-level synthesis
High-level synthesis tools produce RTL descriptions of designs from
high-level representations, such as C or C++ [ 10,18]. In particular,
Xilinx’s Vitis HLS, based on the Autopilot project [ 48], is a state-
of-the-art HLS tool. Given a high-level, procedural, representation,
HLS carries out three fundamental tasks, in order to produce a
corresponding RTL design:
(1)HLS schedules operations (such as mulf ,addf ,load ,store )
in order to determine which operations should occur during
each clock cycle; such a schedule depends on three charac-
teristics of the high-level representation: (a) the topological
ordering of the DFG of the procedural representation (i.e.,
the dependencies of operations on results of other operations
and resources); (b) the delay for each operation; and (c) the
user’s desired clock rate/frequency.
(2)HLS associates ( binds ) floating point operations to RTL in-
stantiations of intellectual property (IP) for those operations;
for example whether to associate an addition operation fol-
lowed by a multiply operation to IPs for each, or whether to
associate them both with a single IP, designed to perform
a fused multiply-accumulate (MAC). In the case of floating-
point arithmetic operations, HLS also (with user guidance)
determines the precision of the floating-point representation.def conv2d (
input: MemRef( 𝑏,𝑐𝑖𝑛,ℎ,𝑤),
output: MemRef( 𝑏,𝑐𝑜𝑢𝑡,ℎ,𝑤),
weight: MemRef( 𝑐𝑜𝑢𝑡,𝑐𝑖𝑛,𝑘,𝑘)
):
for i1inrange( 0,𝑏):
for i2inrange( 0,𝑐𝑜𝑢𝑡):
for i3inrange( 0,ℎ):
for i4inrange( 0,𝑤):
for i5inrange( 0,𝑐𝑖𝑛):
for i6inrange( 0,𝑘):
for i7inrange( 0,𝑘):
_3 = i3 + i6
_4 = i4 + i7
_5 = input[i1, i5, _3, _4]
_6 = weight[i2, i5, i6, i7]
_7 = output[i1, i2, i3, i4]
_8 = _5 * _6
_9 = _7 + _8
output[i1, i2, i3, i4] = _9
Listing 1: Python representation of a padding ⌊𝑘/2⌋, stride 1,
𝑐𝑜𝑢𝑡filter convolution with 𝑘×𝑘kernel applied to ( 𝑏,𝑐𝑖𝑛,ℎ,𝑤)-
dimensional input tensor;𝑏is batch size, 𝑐𝑖𝑛is number of
channels, and ( ℎ,𝑤) are height and width, respectively.
(3)HLS builds a finite-state machine (FSM) that implements the
schedule of operations as control logic, i.e., logic that initiates
operations during the appropriate stages of the schedule.
In addition to fulfilling these three fundamental tasks, HLS aims
to optimize the program. In particular, HLS attempts to maximize
concurrency and parallelism (number of concurrent operations
scheduled during a clock cycle) in order maximize the throughput
and minimize the latency of the final implementation. Maximizing
concurrency entails pipelining operations: operations are executed
such that they overlap in time when possible, subject to available
resources. Maximizing parallelism entails partitioning the DNN
into subsets of operation that can be computed independently and
simultaneously and whose results are aggregated upon completion.
While HLS aims to optimize various characteristics of a design
automatically, there are challenges associated this automation. In
particular, maximum concurrency and parallelism necessitates data-
flow analysis in order to identify data dependencies amongst oper-
ations, both for scheduling and identifying potential data hazards.
Such data-flow analysis is expensive and grows (in runtime) as
better performance is pursued. This can be understood in terms of
loop-nest representations of DNN operations.
For example, consider the convolution in Listing 1. A schedule
that parallelizes (some of) the arithmetic operations for this loop
nest can be computed by first unrolling the loops up to some “trip
count” and then computing the topological sort of the operations.
When using this list scheduling algorithm, the degree to which the
loops are unrolled determines how many arithmetic operations can
be scheduled in parallel. The issue is that the store s and load sConference’17, July 2017, Washington, DC, USA Maksim Levental, Arham Khan, Ryan Chard, Kazutomo Yoshii, Kyle Chard, and Ian Foster
1def conv2d (
2 input: MemRef( 𝑏,𝑐𝑖𝑛,ℎ,𝑤),
3 output: MemRef( 𝑏,𝑐𝑜𝑢𝑡,ℎ,𝑤),
4 weight: MemRef( 𝑐𝑜𝑢𝑡,𝑐𝑖𝑛,𝑘,𝑘)
5):
6 for i1inrange( 0,𝑏):
7 for i2inrange( 0,𝑐𝑜𝑢𝑡):
8 for i3inrange( 0,ℎ):
9 for i4inrange( 0,𝑤):
10 ...
11 # e.g., i5, i6, i7 = 2, 3, 4
12 _31 = i3 + i6
13 _41 = i4 + i7
14 _51 = input[i1, i5, _31, _41]
15 _61 = weight[i2, i5, i6, i7]
16 _71 = output[i1, i2, i3, i4]
17 _81 = _51 * _61
18 _91 = _71 + _81
19 output[i1, i2, i3, i4] = _91
20 # i5, i6, i7 = 2, 3, 5
21 _32 = i3 + i6
22 _42 = i4 + i7
23 _52 = input[i1, i5, _32, _42]
24 _62 = weight[i2, i5, i6, i7]
25 _72 = output[i1, i2, i3, i4]
26 _82 = _52 * _62
27 _92 = _72 + _82
28 output[i1, i2, i3, i4] = _92
29 ...
Listing 2: Store-load forwarding across successive iterations
(e.g., i7=4,5) of the inner loop in Listing 1, after unrolling.
The forwarding opportunity is from the store on line 19 to
the load on line 25; both can be eliminated and _91can re-
place uses of _72, such as in the computation of _92(and
potentially many others).
on the output array prevent reconstruction of explicit relation-
ships between the inputs and outputs of the arithmetic operations
across loop iterations. The conventional resolution to this loss of
information is to perform store-load forwarding : pairs of store and
load operations on the same memory address are eliminated, with
the operand of the store forwarded to the uses of the load (see
Listing 2). Ensuring correctness of this transformation (i.e., that it
preserves program semantics) requires verifying, for each pair of
candidate store andload operations, that there is no intervening
memory operation on the same memory address. These verifica-
tions are non-trivial since the iteration spaces of the loops need not
be regular; in general it might involve solving a small constraint
satisfaction program [ 39]. Furthermore, the number of required
verifications grows polynomially in the convolution parameters,
since the loop nest unrolls into 𝑏×𝑐𝑜𝑢𝑡×ℎ×𝑤×𝑐𝑖𝑛×𝑘2store -load
pairs on the output array.
Finally, note, although greedy solutions to the scheduling prob-
lem solved by HLS are possible, the scheduling problem, in principle,can be formulated as an integer linear program (ILP), for which the
corresponding decision problem is complete for NP. In summary,
HLS tools solve computationally intensive problems in order to pro-
duce an RTL description of a high-level representation of a DNN.
These phases of the HLS process incur “development time” costs
(i.e., runtime of the tools) and impose practical limitations on the
amount of design space exploration (for the purpose of achieving
latency goals) which can be performed. OpenHLS addresses these
issues by enabling the user to employ heuristics during both the
parallelization and scheduling phases which, while not guaranteed
to be correct (but can be behaviorally verified ) and have much lower
runtimes (see Section 3.1).
2.3 FPGA design
Broadly, at the register-transfer level of abstraction, there remain
two more steps prior to being able to deploy a design to an FPGA: a
final lowering, so-called logic synthesis, and place and route (P&R).
The entire process may be carried out by Xilinx’s Vivado tool.
Logic synthesis is the process of mapping RTL to actual hard-
ware primitives on the FPGA (so-called technology mapping ), such
as lookup tables (LUTs), block RAMs (BRAMs), flip-flops (FFs), and
digital signal processors (DSPs). Logic synthesis produces a network
list (netlist ) describing the logical connectivity of various parts of
the design. Logic synthesis, for example, determines the implemen-
tation of floating-point operations in terms of DSPs; depending
on user parameters and other design features, DSP resource con-
sumption for floating-point multiplication and addition can differ
greatly. Logic synthesis also determines the number of LUTs and
DSPs which a high-level representation of a DNN corresponds to,
which is relevant to both the performance and feasibility of that
DNN when deployed to FPGA.
After the netlist has been produced, the entire design undergoes
P&R to determine which configurable logic block within an FPGA
should implement each of the units of logic required by the digital
design. P&R algorithms need to minimize distances between related
units of functionality (in order to minimize wire delay), balance wire
density across the entire fabric of the FPGA (in order to reduce route
congestion), and maximize the clock speed of the design (a function
of both wire delay, logic complexity, and route congestion). The
final, routed design, can then be deployed to the FPGA by producing
a proprietary bitstream , which configures the FPGA.
2.4 Related work
Several projects aim to support translation from high-level repre-
sentations of DNNs to feasible FPGA designs. Typically they rely
on commercial HLS tools for the scheduling, binding, and RTL emis-
sion phases of the translation, such as in the cases of DaCeML [ 40],
hls4ml [ 17], and ScaleHLS [ 47], which all rely on Xilinx’s Vitis HLS.
Thus, they fail to efficiently (i.e., without incurring the aforemen-
tioned runtime costs) produce feasible and low-latency designs.
One notable recent work is the SODA Synthesizer [ 8], which does
not rely on a commercial tool but instead relies on the open-source
PandA-Bambu HLS tool [ 18]; though open-source and mature, we
found in our own tests that PandA-Bambu also could not handle
fully unrolled designs efficiently.OpenHLS Conference’17, July 2017, Washington, DC, USA
Alternatively, some projects do not rely on HLS for scheduling,
binding, and RTL emission, and also attempt to translate from high-
level representations of DNNs to feasible FPGA designs, such as
DNN Weaver [ 43] and NNGen [ 45]. Both of the cited projects func-
tion as parameterized/templatized RTL generators and thus lack
sufficient generality for our needs; primarily they seek to produce
implementations of kernels that emulate GPU architectures (i.e.,
optimizing for throughput rather than latency). In our experiments
they were unable to generate low-latency implementations, either
by achieving unacceptable latencies or by simply failing outright.
(NNGen, due to the nature of templates, supports only limited com-
position, and produced “recursion” errors.)
3 THE COMPILER AND HLS FRAMEWORK
OpenHLS is an open source compiler and HLS framework that em-
ploys MLIR for extracting loop-nest representations of DNNs. Im-
plemented in Python for ease of use and extensibility, it handles
the DNN transformations as well as scheduling, binding, and FSM
extraction. Importantly, there is no dependence on commercial
HLS tools, a property that uniquely enables its use for applications
that require the flexibility of open source tool (e.g., the ability to
inspect and modify internals in order to adapt to special cases),
such as low-latency physical science experiments. Figure 1 shows
its overall architecture. OpenHLS first lowers DNNs from PyTorch
to MLIR through TorchScript and the torch dialect (see Section
2.1.2) and then from the torch dialect to the scfdialect (through
thelinalg dialect). Such a representation lends itself to a straight-
forward translation to Python (compare Listing 1 to Listing 3) and
indeed OpenHLS performs this translation.
PyTorchTorchScriptDNN FrameworkTorch-MLIRMLIRTorchScriptAffineSCFPythonTransformationsSymbolic executionScheduling (CIRCT)VerilogOpenHLS
Figure 1: OpenHLS framework overview.
The benefits of translating scfdialect to Python are manifold:
see Section 3.1. Ultimately, OpenHLS produces a representation of
the DNN that is then fully scheduled by using the scheduling infras-
tructure in CIRCT [ 36] (an MLIR adjacent project). After scheduling,
OpenHLS emits corresponding RTL (as Verilog).@conv2d (
%input: memref< 𝑏×𝑐𝑖𝑛×ℎ×𝑤>,
%weight: memref< 𝑏×𝑐𝑜𝑢𝑡×ℎ×𝑤>,
%output: memref< 𝑐𝑜𝑢𝑡×𝑐𝑖𝑛×𝑘×𝑘>
) {
scf.for %i1 = %c0 to𝑏step %c1 {
scf.for %i2 = %c0 to𝑐𝑜𝑢𝑡step %c1 {
scf.for %i3 = %c0 toℎstep %c1 {
scf.for %i4 = %c0 to𝑤step %c1 {
scf.for %i5 = %c0 to𝑐𝑖𝑛step %c1 {
scf.for %i6 = %c0 to𝑘step %c1 {
scf.for %i7 = %c0 to𝑘step %c1 {
%3 = arith.addi %i3, %i6
%4 = arith.addi %i4, %i7
%5 = memref.load %input[
%i1, %i5, %i3, %3, %4]
%6 = memref.load %weight[
%i2, %i5, %i6, %i7]
%7 = memref.load %output[
%i1, %i2, %i3, %i4]
%8 = arith.mulf %5, %6
%9 = arith.addf %7, %8
memref.store %9, %output[
%i1, %i2, %i3, %i4]
}
}
}
}
}
}
}
return %2
}
Listing 3: scfdialect loop representation of Listing 1.
OpenHLS delegates to the FloPoCo [ 16] IP generator the task
of generating pipelined implementations of the standard floating-
point arithmetic operations ( mulf ,divf ,addf ,subf ,sqrtf ) at var-
ious precisions. In addition, we implement a few generic (parame-
terized by bit width) operators in order to support a broad range
of DNN operations: two-operand maximum ( max), unary negation
(neg), and the rectified linear unit ( relu ). Transcendental functions,
such as exp, are implemented by using a Taylor series expansion
to𝑘-th order (where 𝑘is determined on a case-by-case basis). Note
that FloPoCo’s floating-point representation differs slightly from
IEEE754, foregoing subnormals and differently encoding zeroes,
infinities and NaNs (for the benefit of reduced complexity) and our
implementations max,neg,relu are adjusted appropriately.
We now discuss some aspects of OpenHLS in more detail.
3.1 Symbolic interpretation for fun and profit
As noted in Section 2.2, maximizing concurrency and parallelism for
a design entails unrolling loops and analyzing the data flow of their
operations. As illustrated in Figure 2, the formally correct approachConference’17, July 2017, Washington, DC, USA Maksim Levental, Arham Khan, Ryan Chard, Kazutomo Yoshii, Kyle Chard, and Ian Foster
816 32 64 128
Image size0123456Time (s)1e5
Figure 2: 3×3-kernel convolution (cf. Listing 3) full unrolling
time vs. input (square) image size, with store -load forward-
ing using MLIR’s -affine-scalrep pass. The longest time
is 577,419 s (≈160 h) for a loop nest with a trip count of
128×128×3×3=147,456.
to unrolling loop nests can be prohibitively expensive in terms of
runtime. In the case of BraggNN (see Listing 5), for example, the
high cost of unrolling precluded effective search of the design space
for a RTL representation achieving the target latency. Translating
scfdialect to Python enables OpenHLS to overcome this barrier
by enabling us to use the Python interpreter as a symbolic inter-
preter . Interpreting the resulting Python loop nests (i.e., running
the Python program) while treating the arithmetic and memory
operations on SSA values as operations on symbols (i.e., Python
classes with overloaded methods) enables us to:
(1)Partially evaluate functions of iteration variables (for exam-
ple, %3 = arith.addi %i3, %i6 ) to determine array in-
dex operands of all stores and loads (for example,
memref.load %input[%i1,%i5,%i3,% 3,%4]) and thereupon
perform memory dependence checks, thus transforming the
problem of statically verifying memory dependence into one
of checking assertions at runtime;
(2)Unroll loops by recording each floating-point arithmetic op-
eration executed while enforcing SSA; e.g., for a loop whose
body has repeated assignments to the same SSA value (osten-
sibly violating SSA), we execute the loop and instantiate new,
uniquely identified, symbols for the result of each operation;
(3)Reconstruct all data flow through arithmetic operations and
memory operations by interpreting memref s as geometric
symbol tables (i.e., symbol tables indexed by array indices
rather than identifiers/names) and store s and load s as reads
and writes on those symbol tables;
(4)Swap evaluation rules in order to support various functional
modes, e.g., evaluating floating-point arithmetic operations
by using (Python) bindings to FloPoCo’s C++ functional mod-
els, thereby enabling behavioral verification of our designs.
See Table 3 for the translation rules from MLIR dialects to Python.3.2 AST transformations and verification
Prior to interpretation, OpenHLS performs some simple AST trans-
formations on the Python generated from scfdialect:
(1)Hoist globals : Move fixed DNN tensors (i.e., weights) out of
the body of the generated Python function ( OpenHLS trans-
lates the MLIR module corresponding to the DNN into a
single Python function in order to simplify analysis and in-
terpretation) and into the parameter list, for the purpose of
ultimately exposing them at the RTL module interface.
(2)Remove ifexpressions : DNN relu operations are low-
ered to the scfdialect as a decomposition into arith.cmpfugt
andarith.select ; this transformation recomposes them
into a relu .
(3)Remove MACs : Schedule sequences of load -multiply -add-
store (common in DNN implementations) jointly, coalescing
them into a single fmac operation.
(4)Reduce fors: Implement the reduction tree structure for
non-parallelizable loop nests mentioned in Section 3.3.
These transformations on the Python AST are simple (imple-
mented with procedural pattern matching), extensible, and efficient
(marginal runtime cost) because no effort is made to verify their
formal correctness. Thus, OpenHLS trades formal correctness for
development time performance. This tradeoff enables quick de-
sign space iteration, which for example, enabled us to achieve low
latency implementations for BraggNN (see Section 4.2).
OpenHLS supports behavioral rather than formal verification.
Specifically, OpenHLS can generate testbenches for all synthesized
RTL. The test vectors for these testbenches are generated by eval-
uating the generated Python representation of the DNN on ran-
domly generated inputs but with floating-point operations now
evaluated using functional models of the corresponding FloPoCo
operators. The testbenches can then be run using any IEEE 1364
compliant simulator. We run a battery of such testbenches (corre-
sponding to various DNN operation types), using cocotb [41] and
iverilog [46], as a part of our continuous integration (CI) process.
3.3 Scheduling
Recall that HLS must schedule operations during each clock cycle
in a way that preserves the DNN’s data-flow graph. That schedule
then informs the construction of a corresponding FSM. As already
mentioned, scheduling an arbitrary DNN involves formulating and
solving an ILP. In the resource-unconstrained case, due to the prece-
dence relations induced by data flow, the constraint matrix of the
associated ILP is a totally unimodular matrix and the feasible re-
gion of the problem is an integral polyhedron. In such cases, the
scheduling problem can be solved optimally in polynomial time
with a LP solver [ 35]. In the resource-constrained case, resource
constraints can also be transformed into precedence constraints
by picking a particular (possibly heuristic) linear ordering on the
resource-constrained operations. This transformation partitions
resource-constrained operations into distinct clock cycles, thereby
guaranteeing sufficient resources are available for all operations
scheduled within the same clock cycle [15].
OpenHLS uses the explicit parallelism of the scf.parallel loop-
nest representation to inform such a linear ordering on resource-
constrained operations. By assumption, for loop nests which canOpenHLS Conference’17, July 2017, Washington, DC, USA
JMLIR K Python
J%5K v5 = Val( "%5")
Jmemref<𝑏×𝑐𝑖𝑛×ℎ×𝑤>K MemRef(𝑏,𝑐𝑖𝑛,ℎ,𝑤)
J%5 = memref.load %input[%i1, %i5, % 3, %4]K J%5K=J%input K.__getitem__ ((J%i1K,J%i5K,J%3K,J%4K))
Jmemref.store %9, %output[%i1, %i5, % 3, %4]K J%output K.__getitem__ ((J%i1K,J%i5K,J%3K,J%4K),J%9K)
Jscf.for %i1 = %c0 to𝑏step %c1K for J%i1Kinrange( J%c0K,𝑏,J%c1K)
J%3 = arith.addi %i3, %i6 K J%3K=J%i3K+J%i6K
J%8 = arith.mulf %5, %6 K J%8K=J%5K.__mul__ (J%6K)
J%9 = arith.addf %7, %8 K J%9K=J%7K.__add__ (J%8K)
J%63 = arith.cmpfugt %10,%c0K∧ J%64 = arith.select %63,%10,%c0K
J%64K.relu( J%10K)
J%8 = arith.mulf %5,%6K∧ J%9 = arith.addf %7,%8K
J%9K= fma( J%5K,J%6K,J%7K)
Figure 3: Translation rules for mapping scf,arith , and memref dialects to Python.
be reprepresented as scf.parallel loop nests (see Listing 4), each
instance of a floating-point arithmetic operation in the body corre-
sponding to unique values of the iteration variables (e.g., %i1,%i2,
%i3,%i4for Listing 4) is independent of all other such instances,
although data flow within a loop body must still be respected. This
exactly determines total resource usage per loop nest; for example,
the convolution in Listing 4 would bind to 2𝐾𝑖DSPs (assuming
mulf ,addf bind to one DSP each), where:
𝐾𝑖B|{%i1=%c0+%c1×N∧%i1<𝑏}|×
|{%i2=%c0+%c1×N∧%i2<𝑐𝑜𝑢𝑡}|×
|{%i3=%c0+%c1×N∧%i3<ℎ}|×
|{%i4=%c0+%c1×N∧%i4<𝑤}|
with %c1×Nrepresenting all multiples of %c1. That is to say, 𝐾𝑖is
the cardinality of the cartesian product of the iteration spaces of
the parallel iteration variables.
Defining𝐾Bmax𝑖𝐾𝑖across all scf.parallel loop nests, we
can infer peak usage of any resource. Then, after indexing available
hardware resources 𝑗=1,...,𝐾 , we can bind the operations of any
particular loop nest. This leads to a linear ordering on resource-
constrained operations such that operations bound to the same
hardware resource index 𝑗must be ordered according to their exe-
cution order during symbolic interpretation.2Note that this order-
ing coincides with the higher-level structure of the DNN, which
determines the ordering of scf.parallel loop nests (and thus
interpretation order during execution of the Python program).
For DNN operations that lower to sequential loop nests rather
than scf.parallel loop nests (e.g., sum,max, orprod ), we fully
unroll the loops and transform the resulting, sequential, operations
2OpenHLS only needs to construct a partial precedence ordering op𝑎<op𝑏for op-
erations op𝑎,op𝑏which CIRCT then combines with the delays of the operations to
construct constraints such as start_op 𝑎+delay 𝑎≤start_op 𝑏.@conv2d (
%input: memref< 𝑏×𝑐𝑖𝑛×ℎ×𝑤>,
%weight: memref< 𝑏×𝑐𝑜𝑢𝑡×ℎ×𝑤>,
%output: memref< 𝑐𝑜𝑢𝑡×𝑐𝑖𝑛×𝑘×𝑘>
) {
scf.parallel (%i1, %i2, %i3, %i4) =
(%c0, %c0, %c0, %c0) to
(𝑏,𝑐𝑜𝑢𝑡,ℎ,𝑤)step
(%c1, %c1, %c1, %c1) {
scf.for %i5 = %c0 to𝑐𝑖𝑛step %c1 {
scf.for %i6 = %c0 to𝑘step %c1 {
scf.for %i7 = %c0 to𝑘step %c1 {
%3 = arith.addi %i3, %i6
%4 = arith.addi %i4, %i7
%5 = memref.load %input[%i1, %i5, %i3, % 3, %4]
%6 = memref.load %weight[%i2, %i5, %i6, %i7]
%7 = memref.load %output[%i1, %i2, %i3, %i4]
%8 = arith.mulf %5, %6
%9 = arith.addf %7, %8
memref.store %9, %output[%i1, %i2, %i3, %i4]
}
}
}
}
return %2
}
Listing 4: Parallel loop representation of Listing 1, exhibit-
ing explicitly the resource partitioning and ordering strat-
egy we employ to construct a feasible schedule of opera-
tions.Conference’17, July 2017, Washington, DC, USA Maksim Levental, Arham Khan, Ryan Chard, Kazutomo Yoshii, Kyle Chard, and Ian Foster
into a reduction tree; we use As-Late-As-Possible scheduling [ 7]
amongst the subtrees of such reduction trees.
4 EVALUATION
We evaluate OpenHLS both on individual DNN layers, and end-to-
end, on our use-case BraggNN . We compare OpenHLS to Xilinx’s
Vitis HLS by comparing the latencies and resource usages of the
final designs generated by each. We also compare the runtimes of
the tools themselves. Both OpenHLS and Vitis HLS produce Verilog
RTL, on which we run a synthesis pass by using Xilinx’s Vivado.
The particular FPGA target is Xilinx Alveo U280. We measure LUT,
DSP, BRAM, and FF usage. For the DNN layer evaluations, we use
FloPoCo (5,11)-floating point representations (5-bit exponent, 11-bit
mantissa), corresponding to Vitis HLS’s IEEE half-precision IPs. We
synthesize all designs for a 10 ns target clock period and report
end-to-end latency as a product of the total schedule interval count
of the design and achieved clock period ( 10-WNS , where WNS is
the worst negative slack reported). In the case of Vitis HLS, which
potentially explicitly pipelines the design and therefore implements
with an initiation interval strictly less than the total schedule in-
terval count, we report in terms of the best possible interval count
(LatencyBest from the Vitis HLS reports). All other measurements
are collected from Vivado synthesis reports. As Vitis HLS operates
on C++ representations, we generate such a representation for our
test cases by first lowering each DNN layer to the affine dialect
and then applying the scalehls-translate tool of the ScaleHLS
project [ 47] to emit C++. Importantly, we do not make any use of
scalehls-opt optimization tool (of the same project).
Since our ultimate goal is low latency inference, and since the
strategy that OpenHLS employs in the pursuit of this goal is loop
unrolling, in order to produce a like for like comparison, we simi-
larly unroll the representation that is passed to Vitis HLS. Thus, all
Vitis HLS measurements are reported in terms of unroll factor : an
unroll factor of 𝑘corresponds to a 𝑘-fold increase in the number of
statements in the body of a loop and commensurate 𝑘-fold decrease
in the trip count of the loop. For loop nests, we unroll inside out: if
𝑘is greater than the trip count 𝑡of the innermost loop, we unroll
the innermost loop completely and then unroll the enclosing loop
by a factor of 𝑘−𝑡. We do not perform any store-load forwarding
during this preprocessing but we annotate all arrays with the direc-
tive array_partition complete dim=1 in order that Vitis HLS
can effectively pipeline. All representations generated by OpenHLS
correspond to full unrolling of the loop nests.
4.1 DNN layers
We evaluate OpenHLS vs. Xilinx’s Vitis HLS by comparing the la-
tency of the final design on five DNN layer types, chosen to cover
a range of arithmetic operations ( mulf ,divf ,addf ,subf ,sqrtf )
and data access patterns (iteration, accumulation, reduction):
•addmm(a, b, c) : Matrix multiply: a×b+c;
•batch_norm_2d(num_features) : Batch normalization over
a 4D input [25];
•conv_2d(𝑐𝑖𝑛,𝑐𝑜𝑢𝑡,𝑘): 2D convolution with bias, with
𝑘×𝑘kernel, over a 𝑏×𝑐𝑖𝑛×ℎ×𝑤input, producing 𝑏×
𝑐𝑜𝑢𝑡×ℎ′×𝑤′output;•max_pool_2d( 𝑘, stride) : 2D max pooling, with 𝑘×𝑘
kernel, and striding;
•soft_max : softmax(𝒙)B"
exp(𝑥𝑖)Í
𝑗exp 𝑥𝑗#
The parameter values and input dimensions used during evaluation
are summarized in Table 1.
Table 1: DNN layers used for evaluation of OpenHLS.
Layer Parameter values Input dimensions
addmm N/A a,b,c:(16,16)
batch_norm_2d num_features =2 input :(10,2,3,3)
conv_2d 𝑐𝑖𝑛=1,𝑐𝑜𝑢𝑡=𝑘=3input :(1,1,16,16)
max_pool_2d 𝑘=3,stride =2 input :(1,3,16,16)
soft_max N/A input :(1,3,16,16)
Figure 4 shows Vitis HLS vs. OpenHLS resource usage and latency
vs. unroll factor and Figure 5 shows the runtimes of Vitis HLS as
function of increasing unroll factor. We observe that while Vitis
HLS end-to-end latencies decrease with increased unroll factor, they
never match that achieved by OpenHLS . Even at an unroll factor of
1024 (which corresponds to fully unrolled for all loop nests com-
prising these layer types), Vitis HLS is only within 10 ×ofOpenHLS .
We attribute this to Vitis HLS’s inability to pipeline effectively, due
to its inability to eliminate memory dependencies, either through
store -load forwarding or further array partitioning. Conversely,
OpenHLS ’s ability to effectively perform store -load forwarding is
evident in the complete lack of BRAM usage: all weights are kept
on FFs or LUTs. While infeasible for larger designs (which would
be constrained by the number of available FFs), this unconstrained
usage of FFs is acceptable for our use case. The increasing latency
(as a function of unroll factor) in the max_pool_2d case is due to
Vitis HLS’s failure to meet timing, i.e., while the interval count
decreases as a function of unroll factor, the clock period increases.
4.2 BraggNN case study
High-energy diffraction microscopy enables non-destructive char-
acterization for a broad class of single-crystal and polycrystalline
materials. A critical step in a typical HEDM experiment is an anal-
ysis to determine precise Bragg diffraction peak characteristics.
Peak characteristics are typically computed by fitting the peaks
to a probability distribution, e.g., Gaussian, Lorentzian, Voigt, or
Pseudo-Voigt. As noted in Section 1, HEDM experiments can collect
data at more than 80 GB/s. These data rates, though more modest
than at the LHC, merit exploring low latency approaches in order to
enable experiment modalities that depend on measurement-based
feedback (i.e., experiment steering).
BraggNN [31], a DNN aimed at efficiently characterizing Bragg
diffraction peaks, achieves a throughput (via batch inference) of ap-
proximately 22 µs/sample on a state-of-the-art GPU: a large speedup
over classical pseudo-Voigt peak fitting methods, but still far short
of the 1 µs/sample needed to handle 1 MHz sampling rates. In ad-
dition, the data-center class GPU such as a NVIDIA V100 (or evenOpenHLS Conference’17, July 2017, Washington, DC, USA
OpenHLS0.10.20.40.60.82468
base481216202428323640
1024
Unroll factor2468Latency ( s)
Latency
Vitis HLS
OpenHLSResources
DSP
BRAM
LUT
FF
(a)addmm module
OpenHLS0.110
0.20.40.60.8246820
Resource utilization (%)
base481216202428323640
1024
Unroll factor0.20.40.60.8
Latency
Vitis HLS
OpenHLSResources
DSP
LUT
FF (b)batch_norm_2d module
OpenHLS0.110
0.20.40.60.8246820
base481216202428323640
1024
Unroll factor10
6820Latency ( s)
Latency
Vitis HLS
OpenHLSResources
DSP
LUT
FF
(c)conv_2d module
OpenHLS1
0.20.40.60.82468
Resource utilization (%)
base481216202428323640
1024
Unroll factor10
468
Latency
Vitis HLS
OpenHLSResources
LUT
FF (d)max_pool_2d module
OpenHLS0.110
0.20.40.60.824682040
Resource utilization (%)
base481216202428323640
1024
Unroll factor102040Latency ( s)
Latency
Vitis HLS
OpenHLSResources
DSP
BRAM
LUT
FF
(e)soft_max module
Figure 4: Vitis HLS vs. OpenHLS resource usage and latency vs. unroll factor for five DNN modules, exhibiting the large runtime
cost incurred in using Vitis HLS to search the design space (of possible low-latency designs for each layer). The lines give
latencies (left axes); the bars give the % of the resource used (right axes). All y-scales are log.
a workstation class GPU such as a NVIDIA RTX 2080Ti) required
to run the current BraggNN implementation cannot be deployed atthe edge, i.e., adjacent or proximal to the high energy microscopy
equipment. With the goal of reducing both per-sample time andConference’17, July 2017, Washington, DC, USA Maksim Levental, Arham Khan, Ryan Chard, Kazutomo Yoshii, Kyle Chard, and Ian Foster
base481216202428323640
1024
Unroll factor100101102103104Time (s)
addmm
batch_norm
braggnn
conv
max_pool_2d
soft_max
OpenHLS100101102103104
Figure 5: Vitis HLS vs. OpenHLS runtime vs. unroll factor, il-
lustrating the large runtime cost incurred in using Vitis HLS
to search over possible low-latency BraggNN designs.
OpenHLS0.110
0.20.40.60.8246820406080
Resource utilization (%)
base481216202428323640
1024
Unroll factor406080Latency ( s)
Latency
Vitis HLS
OpenHLSResources
DSP
BRAM
LUT
FF
Figure 6: BraggNN Vitis HLS vs. OpenHLS resource usage and la-
tency vs. unroll factor (with both at half-precision) through-
out the design space of possible low-latency designs.
deployment footprint, we applied OpenHLS to the PyTorch rep-
resentation of BraggNN(s=1) (see Listing 5) and achieved a RTL
implementation which synthesizes to a 1238 interval count design
that places, routes, and meets timing closure for a clock period
of 10 ns (for a Xilinx Alveo U280). The design consists of a three
stage pipeline with the longest stage measuring 480 intervals, for a
throughput of 4.8 µs/sample. See Figure 6 for a comparison with
designs generated by Vitis HLS (using the same flow as in 4).
The most challenging aspect of implementing BraggNN was mini-
mizing latency while satisfying compute resource constraints (LUTs,
DSPs, BRAMs) and achieving routing closure, i.e., not exceeding
available routing resources and avoiding congestion. We made two
design choices to reduce resource consumption. The first was to
reduce the precision used for the floating-point operations, from
half precision to FloPoCo (5,4)-precision (5-bit exponent, 4-bit man-
tissa), a choice justified by examination of the distribution of the
weights of the fully trained BraggNN (see Figure 7).BraggNN(𝑠)(
(cnn_layers_1): Conv2d( 𝑠×16, kernel= 3, stride= 1)
(nlb): NLB(
(theta_layer): Conv2d( 𝑠×16,𝑠×8, kernel= 1, stride= 1)
(phi_layer): Conv2d( 𝑠×16,𝑠×8, kernel= 1, stride= 1)
(g_layer): Conv2d( 𝑠×16,𝑠×8, kernel= 1, stride= 1)
(out_cnn): Conv2d( 𝑠×8,𝑠×16, kernel= 1, stride= 1)
(soft): Softmax()
)
(cnn_layers_2): Sequential(
(0): ReLU()
(1): Conv2d(𝑠×16,𝑠×8, kernel= 3, stride= 1)
(2): ReLU()
(3): Conv2d(𝑠×8,𝑠×2, kernel= 3, stride= 1)
(4): ReLU()
)
(dense_layers): Sequential(
(0): Linear(in_features= 𝑠×50, out_features= 𝑠×16)
(1): ReLU()
(2): Linear(in_features= 𝑠×16, out_features= 𝑠×8)
(3): ReLU()
(4): Linear(in_features= 𝑠×8, out_features= 𝑠×4)
(5): ReLU()
(6): Linear(in_features= 𝑠×4, out_features= 2)
(7): ReLU()
)
)
Listing 5: BraggNN model architecture for scaling factors
s=1,2 .
Reducing the precision enabled the second design choice, to
eliminate BRAMs from the design, since, at the lower precision, all
weights can be represented as registered constants. The reduced
precision also drove the Vivado synthesizer to infer implementa-
tions of the floating-point operations that make no use of DSPs,
likely becaue the DSP48 hardware block includes a 18-bit by 25-bit
signed multiplier and a 48-bit adder [ 2], neither of which neatly
divides the bit width of FloPoCo (5,4)-precision cores. (The actual
width for FloPoCo (5,4)-precision is 12 bits: 1 extra bit is needed for
the sign and 2 for handling of exceptional conditions.)
Achieving routing closure was difficult due to the nature of the
Xilinx’s UltraScale architecture, of which the Alveo U280 is an
instance. The UltraScale architecture achieves its scale through
Stacked Silicon Interconnect (SSI) technology [ 27], which implies
multiple distinct FPGA dies, called Super Logic Regions (SLRs), on
the same chip, connected by interposers. Adjacent SLRs communi-
cate with each other over a limited set of Super Long Lines (SLLs),
which determine the maximum bus width that spans two SLRs. On
the Alveo U280 there are exactly 23,040 SLLs available between ad-
jacent SLRs and at (5,4)-precision BraggNN (s=1) needs 23,328 SLLs
between SLR2 and SLR1. [We route from SLR2 to SLR1 the outputs
ofcnn_layers_1 (1×16×9×9×12 wires) and soft(theta_layer ×
phi_layer)×g_layer (1×8×9×9×12 wires).] Thus, we further re-
duced the precision to (5,3). Finally, since multiple dies constitute
independent clock domains, the SLLs that cross SLRS are sensitive
to hold time violations due to the higher multi-die variability [ 1].
This multi-die variability leads to high congestion if not addressed.OpenHLS Conference’17, July 2017, Washington, DC, USA
0 5 10 15 20 25
101102103104countBraggNN Scales
1/2 scale
1/1 scale
Exponent (Magnitude)Count
Figure 7: OpenHLS weights exponent distribution, illustrat-
ing the narrow distribution of observed weight exponents
thereby justifying reduced precision.
Thus, routing across SLRs needs to be handled manually, using
placement and routing constraints for logic in each SLR and the
addition of so-called “launch” and “latch” registers in each SLR.
Figure 8 illustrates the effect of using launch and latch registers as
well as placement and routing constraints.
Thus, these design choices (in combination with compiler level
optimizations performed by OpenHLS ) plus careful management of
routing constraints enable us to lower, compile, synthesize, place,
and route BraggNN (s=1) to Xilinx’s Alveo U280 at a throughput of
4.8µs/sample: ~5×higher latency than the target 1 µs/sample, but
a ~4×improvement over the PyTorch GPU implementation.
5 CONCLUSION
We have presented OpenHLS , an MLIR-based HLS compilation frame-
work that supports translating DNN models to RTL without the
use of commercial HLS tools. The OpenHLS end-to-end compilation
pipeline provides a PyTorch front-end and Verilog emission back-
end. An extensible Python intermediate layer supports use-case-
specific optimizations (e.g., store -load forwarding) that are not
possible otherwise. Experimental results demonstrate that OpenHLS
outperforms, in terms of end-to-end latency, Vitis HLS on a range
of DNN layer types and on a case-study DNN.
We note three directions for future work, primarily with re-
spect to scheduling: (1) Better integration between the Python
layer and MLIR: it is preferable that the transformations on the
Python representation could make use of various MLIR facilities,
such as affine analysis, for the purposes of exploring loop transfor-
mations that improve latency; (2) Expanding the set of scheduling
algorithms available: for example, resource aware scheduling [ 15];
and (3) Integration of scheduling-aware placement and vice-versa
(placement-aware scheduling): currently OpenHLS can be used to in-
form placement but does not explicitly emit placement constraints
(see Section 4.2); a more precise approach, such as in [ 22], would
potentially enable better pipelining and thus higher throughput.REFERENCES
[1]Create placed and routed DCP to cross SLR. https://www.rapidwright.io/docs/
SLR_Crosser_DCP_Creator_Tutorial.html. Accessed: 2022-10-15.
[2]2021. UltraScale Architecture DSP Slice . Technical Report. XiLinx. https://docs.
xilinx.com/v/u/en-US/ug579-ultrascale-dsp
[3]Roel Aaij et al .2020. Allen: A high-level trigger on GPUs for LHCb. Computing
and Software for Big Science 4, 1 (2020), 1–11.
[4]Martín Abadi et al .TensorFlow: Large-Scale Machine Learning on Heterogeneous
Distributed Systems. https://doi.org/10.48550/ARXIV.1603.04467
[5]Laith Alzubaidi et al .2021. Review of deep learning: Concepts, CNN architectures,
challenges, applications, future directions. Journal of Big Data 8, 1 (2021), 1–74.
[6]Arash Ashari et al .2015. On Optimizing Machine Learning Workloads via Kernel
Fusion. 50, 8 (2015), 173–182. https://doi.org/10.1145/2858788.2688521
[7]Zoltan Baruch. 1996. Scheduling algorithms for high-level synthesis. ACAM
Scientific Journal 5, 1-2 (1996), 48–57.
[8]Nicolas Bohm Agostini et al .2022. Bridging Python to Silicon: The SODA
Toolchain. IEEE Micro (2022). https://doi.org/10.1109/MM.2022.3178580
[9]Uday Bondhugula. Polyhedral compilation opportunities in MLIR. https://acohen.
gitlabpages.inria.fr/impact/impact2020/slides/IMPACT_2020_keynote.pdf.
[10] Andrew Canis et al .2013. LegUp: An Open-Source High-Level Synthesis Tool
for FPGA-Based Processor/Accelerator Systems. ACM Trans. Embed. Comput.
Syst. 13, 2, Article 24 (2013). https://doi.org/10.1145/2514740
[11] Tianqi Chen et al .MXNet: A Flexible and Efficient Machine Learning Library for
Heterogeneous Distributed Systems. https://doi.org/10.48550/ARXIV.1512.01274
[12] Tianqi Chen et al .2018. TVM: An automated end-to-end optimizing compiler for
deep learning. In 13th USENIX Symp. Operating Systems Design & Impl. 578–594.
[13] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training Deep Nets
with Sublinear Memory Cost. https://doi.org/10.48550/ARXIV.1604.06174
[14] LHCb Collaboration. 2020. Comparison of particle selection algorithms for the
LHCb Upgrade . Technical Report. https://cds.cern.ch/record/2746789
[15] Steve Dai, Gai Liu, and Zhiru Zhang. 2018. A Scalable Approach to Exact
Resource-Constrained Scheduling Based on a Joint SDC and SAT Formulation.
InACM/SIGDA Intl Symposium on Field-Programmable Gate Arrays . 137–146.
[16] Florent de Dinechin. 2019. Reflections on 10 Years of FloPoCo. In IEEE 26th
Symposium on Computer Arithmetic . 187–189.
[17] J. Duarte et al .2018. Fast inference of deep neural networks in FPGAs for particle
physics. Journal of Instrumentation 13, 07 (2018), P07027–P07027.
[18] Fabrizio Ferrandi et al .2021. Bambu: an Open-Source Research Framework for
the High-Level Synthesis of Complex Applications. In 58th ACM/IEEE Design
Automation Conference . IEEE, 1327–1330.
[19] Vladimir Gligorov. 2015. Real-time data analysis at the LHC: present and future.
InNIPS Workshop on High-energy Physics and Machine Learning , Vol. 42. 1–18.
[20] V V Gligorov and M Williams. 2013. Efficient, reliable and fast high-level trigger-
ing using a bonsai boosted decision tree. J. Instrumentation 8, 02 (2013).
[21] Keith Grainge et al .2017. Square Kilometre Array: The radio telescope of the
XXI century. Astronomy reports 61, 4 (2017), 288–296.
[22] Licheng Guo et al .2021. AutoBridge: Coupling Coarse-Grained Floorplanning and
Pipelining for High-Frequency HLS Design on Multi-Die FPGAs. In ACM/SIGDA
International Symposium on Field-Programmable Gate Arrays . 81–92.
[23] M. Hammer, K. Yoshii, and A. Miceli. 2021. Strategies for on-chip digital data
compression for X-ray pixel detectors. Journal of Instrumentation 16, 01 (2021),
P01025–P01025. https://doi.org/10.1088/1748-0221/16/01/p01025
[24] Momoko Hattori, Naoki Kobayashi, and Ryosuke Sato. Gradual Tensor Shape
Checking. https://doi.org/10.48550/ARXIV.2203.08402
[25] Sergey Ioffe and Christian Szegedy. Batch Normalization: Accelerating Deep
Network Training by Reducing Internal Covariate Shift. https://doi.org/10.
48550/ARXIV.1502.03167
[26] Chris Lattner et al .MLIR: A compiler infrastructure for the end of Moore’s Law.
https://doi.org/10.48550/ARXIV.2002.11054
[27] Steve Leibson et al .2013. Xilinx ultrascale: The next-generation architecture for
your next-generation architecture. Xilinx White Paper WP435 143 (2013).
[28] Yizhi Liu et al. 2018. Optimizing CNN Model Inference on CPUs. (2018). https:
//doi.org/10.48550/ARXIV.1809.02697
[29] Yongtao Liu et al .2022. Exploring physics of ferroelectric domain walls in real
time: Deep learning enabled scanning probe microscopy. Advanced Science (2022).
[30] Zhengchun Liu et al .2019. Deep learning accelerated light source experiments.
InIEEE/ACM 3rd Workshop on Deep Learning on Supercomputers . IEEE, 20–28.
[31] Zhengchun Liu et al .2022. BraggNN : fast X-ray Bragg peak analysis using deep
learning. IUCrJ 9, 1 (2022), 104–113.
[32] Saeed Maleki et al .2011. An evaluation of vectorizing compilers. In International
Conference on Parallel Architectures and Compilation Techniques . IEEE, 372–382.
[33] J McMullin et al .2022. The Square Kilometre Array project update. In Ground-
based and Airborne Telescopes IX , Vol. 12182. SPIE, 263–271.
[34] Razvan Nane et al .2016. A Survey and Evaluation of FPGA High-Level Synthesis
Tools. IEEE Transactions on Computer-Aided Design of Integrated Circuits and
Systems 35, 10 (2016), 1591–1604. https://doi.org/10.1109/TCAD.2015.2513673Conference’17, July 2017, Washington, DC, USA Maksim Levental, Arham Khan, Ryan Chard, Kazutomo Yoshii, Kyle Chard, and Ian Foster
(a)BraggNN fails to achieve routing closure without placement and routing
constraints and launch and latch registers.
(b)BraggNN achieves routing closure with use of per SLR placement and rout-
ing constraints ( pblock_1 ,pblock_2 ,pblock_3 ) and launch and latch registers
(not highlighted).
Figure 8: Congestion maps for BraggNN on a Xilinx Alveo U280. Magenta indicates areas of high congestion.
[35] Julian Oppermann. 2019. Advances in ILP-based Modulo Scheduling for High-
Level Synthesis . Ph. D. Dissertation. Technische Universität, Darmstadt. http:
//tuprints.ulb.tu-darmstadt.de/9272/
[36] Julian Oppermann et al .2022. How to make hardware with maths: An intro-
duction to CIRCT’s scheduling infrastructure. In European LLVM Developers’
Meeting .
[37] Adam Paszke et al .2017. Automatic differentiation in PyTorch. In 31st Conference
on Neural Information Processing Systems .
[38] Robert M Patton et al .2018. 167-Pflops deep learning for electron microscopy:
From learning physics to atomic manipulation. In SC’18 . IEEE, 638–648.
[39] Sanjay V Rajopadhye. 2002. Dependence Analysis and Parallelizing Transforma-
tions. In The Compiler Design Handbook .
[40] Oliver Rausch et al .2022. DaCeML: A Data-Centric Optimization Framework for
Machine Learning. In 36th ACM International Conference on Supercomputing .
[41] Benjamin John Rosser. Cocotb: a Python-based digital logic verification frame-
work. https://docs.cocotb.org.
[42] Nadav Rotem et al .Glow: Graph Lowering Compiler Techniques for Neural
Networks. https://doi.org/10.48550/ARXIV.1805.00907
[43] Hardik Sharma et al .2016. From high-level deep neural models to FPGAs. In 49th
Annual IEEE/ACM International Symposium on Microarchitecture . 1–12.
[44] Sean Silva and Anush Elangovan. Torch-MLIR. https://mlir.llvm.org/
OpenMeetings/2021-10-07-The-Torch-MLIR-project.pdf.
[45] Shinya Takamaeda-Yamazaki. 2015. Pyverilog: A Python-based hardware de-
sign processing toolkit for Verilog HDL. In International Symposium on Applied
Reconfigurable Computing . Springer, 451–460.
[46] Stephen Williams. Icarus Verilog, 1998–2020. http://iverilog.icarus.com.[47] Hanchen Ye et al .2022. ScaleHLS: A New Scalable High-Level Synthesis Frame-
work on Multi-Level Intermediate Representation. In IEEE International Sympo-
sium on High-Performance Computer Architecture .
[48] Zhiru Zhang et al .2008. AutoPilot: A Platform-Based ESL Synthesis System. In
High-Level Synthesis . Springer Netherlands, Dordrecht, 99–112.
[49] S. Zheng et al .2022. NeoFlow: A Flexible Framework for Enabling Efficient
Compilation for High Performance DNN Training. IEEE Transactions on Parallel
and Distributed Systems 33, 11 (2022), 3220–3232.